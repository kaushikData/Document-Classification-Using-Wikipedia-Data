<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>116 (number)</title>
    <ns>0</ns>
    <id>498596</id>
    <revision>
      <id>867367711</id>
      <parentid>867367652</parentid>
      <timestamp>2018-11-05T07:29:45Z</timestamp>
      <contributor>
        <username>LucasBrown</username>
        <id>11487766</id>
      </contributor>
      <minor/>
      <comment>/* In mathematics */ Link to factorial prime page</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2569">{{Infobox number
| number = 116
| divisor = 1, 2, 4, 29, 58, 116
}}
'''116''' ('''one hundred [and] sixteen''') is the [[natural number]] following [[115 (number)|115]] and preceding [[117 (number)|117]].

==In mathematics==
116 is a [[noncototient]], meaning that there is no solution to the equation {{math|1=''m'' &amp;minus; ''φ''(''m'') = ''n''}}, where {{mvar|φ}} stands for [[Euler's totient function]].&lt;ref&gt;{{Cite OEIS|sequencenumber=A005278 |name=Noncototients: n such that x-phi(x)=n has no solution}}.&lt;/ref&gt;

116! + 1 is a [[factorial prime]].&lt;ref&gt;{{Cite OEIS|sequencenumber=A002981 |name=Numbers n such that n! + 1 is prime}}.&lt;/ref&gt;

There are 116 ternary [[Lyndon word]]s of length six, and 116 [[irreducible polynomial]]s of degree six over a three-element field, which form the basis of a [[free Lie algebra]] of dimension 116.&lt;ref&gt;{{Cite OEIS|sequencenumber=A027376 |name=Number of ternary irreducible polynomials of degree n; dimensions of free Lie algebras}}.&lt;/ref&gt;

There are 116 different ways of partitioning the numbers from 1 through 5 into subsets in such a way that, for every ''k'', the union of the first ''k'' subsets is a consecutive sequence of integers.&lt;ref&gt;{{Cite OEIS|sequencenumber=A007052 |name=Number of order-consecutive partitions of n}}.&lt;/ref&gt;

There are 116 different 6×6 [[Costas array]]s.&lt;ref&gt;{{Cite OEIS|sequencenumber=A008404 |name=Number of Costas arrays of order n, counting rotations and flips as distinct}}.&lt;/ref&gt;

==In other fields==
'''One hundred sixteen''' is also:
* The prefix for several EU-wide telephone helplines designated as [[harmonised service of social value]]
* The [[atomic number]] of [[livermorium]]
* The number of years that the [[Hundred Years' War]] between [[France]] and [[England]], from 1337 to 1453, actually lasted
* The [[fire]] [[emergency telephone number]] in [[Peru]]
* The record for number of wins in a single season of [[Major League Baseball]] achieved by the [[Chicago Cubs]] in 1906 and the [[Seattle Mariners]] in 2001.
* The number of pages in the [[Lost 116 pages]], the original manuscript of what [[Joseph Smith]] said was the translation of the Book of Lehi, of the [[Golden plates]] revealed to him in 1827

==See also==
* [[List of highways numbered 116]]
* [[116th Street (Manhattan)]]
* [[116th Street Crew]], a mafia crew named after the Manhattan street
* The [[116 Clique]], a group of Christian rappers from [[Dallas]], [[Texas|TX]] named after the Bible verse Romans 1:16

==References==
{{reflist}}

{{Integers|1}}

{{DEFAULTSORT:116 (Number)}}
[[Category:Integers]]</text>
      <sha1>qj0k0om0226cjwqg1emtea916ks2k9p</sha1>
    </revision>
  </page>
  <page>
    <title>158 (number)</title>
    <ns>0</ns>
    <id>2695751</id>
    <revision>
      <id>863956729</id>
      <parentid>859261294</parentid>
      <timestamp>2018-10-14T05:19:34Z</timestamp>
      <contributor>
        <username>Kinu</username>
        <id>206667</id>
      </contributor>
      <comment>Remove inappropriate [[WP:EL]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3914">{{example farm|date=September 2018}}
{{Infobox number
| number = 158
| divisor=1, 2, 79, 158
}}

'''158''' ('''one hundred [and] fifty-eight''') is the [[natural number]] following [[157 (number)|157]] and preceding [[159 (number)|159]].

==In mathematics==

158 is a [[nontotient]], since there is no integer with 158 [[coprime]]s below it. 158 is a [[Perrin number]], appearing after 68, 90, 119.&lt;ref&gt;{{Cite web|url=https://oeis.org/A001608|title=Sloane's A001608 : Perrin sequence|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-28}}&lt;/ref&gt;

158 is the number of digits in the [[decimal]] expansion of 100[[factorial|!]], the product of all the natural numbers up to and including 100.

==In the military==
* {{USS|Amador|AK-158}} was a United States Navy {{sclass-|Alamosa|cargo ship|1}} during World War II
* {{USS|Caution|AM-158}} was a United States Navy {{sclass-|Admirable|minesweeper|1}} during World War II
* {{USS|Chase|DE-158}} was a United States Navy {{sclass-|Buckley|destroyer escort|1}} during World War II
* {{USS|General W. G. Haan|AP-158}} was a United States Navy {{sclass-|General G. O. Squier|transport|1}} following World War II
* {{USS|Leary|DD-158}} was a United States Navy {{sclass-|Wickes|destroyer|1}} during World War II
* {{USS|Limestone|IX-158}} was a United States Navy ''Trefoil''-class concrete barge during World War II
* {{USS|Newberry|APA-158}} was a United States Navy {{sclass-|Haskell|attack transport|1}} during World War II
* {{USS|Wadena|SP-158}} was a United States Navy converted yacht patrol vessel during World War I

==In music==
* The song 158 by the [[Indie-rock]] band [[Blackbud]]
* The song "Here We Go" (1998) from [[The Bouncing Souls]]’ ''[[Tie One On]]'' CD includes the lyrics "Me, Shal Pete and Lamar thumbed down the ramp of Exit 158"

==In transportation==
* The [[Alfa Romeo 158/159 Alfetta|Alfa Romeo 158]] [[racecar]]
* The [[Ferrari 158]] [[racecar]] produced between 1964 and 1965
*The [[British Rail Class 158]] [[Sprinter (train)|Express Sprinter]] is a [[diesel multiple unit]] (DMU) train, built for [[British Rail]] between 1989 and 1992

==In other fields==
'''158''' is also:
* The year [[158|AD 158]] or [[158 BC]]
* [[List of highways numbered 158|One of a number of highways]]
* The [[atomic number]] of an element temporarily called [http://www.flw.com/datatools/periodic/001.php?id=158 unpentoctium]
* [[158 Koronis]] is a [[Asteroid belt|Main belt]] [[asteroid]]
* In the [[Israel]]i [[satire|satirical]] [[comedy]] ''[[Operation Grandma]]'' ("Mivtza Safta", מבצע סבתא), the number 158 is implied to be a [[Classified information|classified]] high-rank officer position (Alon says: "Since you've became 158, you became all that?")
* [[Township 158-30, Lake of the Woods County, Minnesota|Township 158-30]] is a small [[township (United States)|township]] in [[Lake of the Woods County, Minnesota]]
* [[Edenwold No. 158, Saskatchewan]] is a [[rural municipality]] in [[Saskatchewan]], Canada
* [[John Irving]]'s third novel, ''[[The 158-Pound Marriage]]''
* [[Financial Accounting Standards Board]] summary of statement No. 158 requires an employer to recognize the overfunded or underfunded status of a defined benefit postretirement plan

==See also==
* [[List of highways numbered 158]]
* [[United Nations Security Council Resolution 158]]
* [[List of United States Supreme Court cases, volume 158|United States Supreme Court cases, Volume 158]]
* [[Pennsylvania House of Representatives, District 158]]
* [[Consolidated School District 158]], [[Illinois]]
* [[Marie Curie Middle School 158]], [[Bayside, New York]]
* [[P.S. 158]], [[Manhattan, New York City]]

==References==
{{Reflist}}

==External links==
{{Commons cat|158 (number)}}
* [http://www.numdic.com/158 The Number 158]

{{Integers|1}}

{{DEFAULTSORT:158 (Number)}}
[[Category:Integers]]</text>
      <sha1>1wk0lx2h0xfnc4v3s0qb5cwpv6drz52</sha1>
    </revision>
  </page>
  <page>
    <title>1961 Census of India</title>
    <ns>0</ns>
    <id>44751671</id>
    <revision>
      <id>841559846</id>
      <parentid>836921563</parentid>
      <timestamp>2018-05-16T15:46:11Z</timestamp>
      <contributor>
        <username>175518 Stand</username>
        <id>33744318</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2215">{{Infobox census
| name =  1961 Census of India
| logo = 
| logo_size = 
| logo_caption = 
| image = 
| image_size = 
| image_caption = 
| country = [[India]]
| date =            &lt;!--{{start date|YYYY|MM|DD|df=y}}--&gt;
| population = 438,936,918
| percent_change =21.62 
| annual_percent_change =8.33 
| region_type =     &lt;!--state, department, region, ...--&gt;
| most_populous = [[Uttar Pradesh]]
| least_populous = [[Sikkim]]
}}
The '''1961 Census of India''' was the 10th in a series of [[census of India|censuses]] held in [[India]] every decade since [[1871 India Census|1871]].{{cn|date=December 2014}}

The population of India was counted as 438,936,918 people.&lt;ref name="Census1961"&gt;{{cite web |author=Mallikarjun, B. | date=5 August 2002 |publisher=M. S. Thirumalai |journal=Languages in India |url=http://www.languageinindia.com/aug2002/indianmothertongues1961aug2002.html |title=Mother Tongues of India According to the 1961 Census | volume= 2 | issn=1930-2940 |accessdate=11 December 2014}}&lt;/ref&gt;

==Language data==
The 1961 census recognized 1,652 ''mother tongues'', counting all declarations made by any individual at the time when the census was conducted.&lt;ref name="Census1961"/&gt; However, the declaring individuals often mixed names of languages with those of dialects, sub-dialects and dialect clusters or even castes, professions, religions, localities, regions, countries and nationalities.&lt;ref name="Census1961"/&gt; The list therefore includes "languages" with barely a few individual speakers as well as 530 unclassified "mother tongues" and more than 100 idioms that are non-native to India, including linguistically unspecific [[demonym]]s such as "African", "Canadian" or "Belgian".&lt;ref name="Census1961"/&gt; Modifications were done by bringing in two additional components- place of birth i.e. village or town and duration of stay ( if born elsewhere).

==See also==
*[[Demographics of India]]

== References ==
{{reflist}}

==External links==
*{{official website|http://censusindia.gov.in/}}

{{Census of India}}
[[Category:1961 in India|Census Of India, 1991]]
[[Category:Censuses in India]]
[[Category:Political history of India]]
[[Category:1961 censuses|India]]

{{Census-stub}}{{India-stub}}</text>
      <sha1>mhd1eb9i25c2w0r5x6zpmma4ovqbq5q</sha1>
    </revision>
  </page>
  <page>
    <title>Abraham Adrian Albert</title>
    <ns>0</ns>
    <id>3326054</id>
    <revision>
      <id>857770383</id>
      <parentid>857491905</parentid>
      <timestamp>2018-09-02T22:29:26Z</timestamp>
      <contributor>
        <username>Suslindisambiguator</username>
        <id>12329968</id>
      </contributor>
      <comment>added Daniel Zelinsky to list of doctoral students</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12423">{{Infobox scientist
|name              = A. A. Albert
|image             = 
|caption           = 
|birth_date        = {{Birth date |1905|11|09}}
|birth_place       = [[Chicago]]
|death_date        = {{Death date and age|1972|06|06|1905|11|09}}
|death_place       = Chicago
|residence         = 
|citizenship       = 
|nationality       = [[United States|American]]
|ethnicity         = 
|fields            = [[mathematics]]
|workplaces        = [[Columbia University]]&lt;br&gt;[[University of Chicago]]
|alma_mater        = [[University of Chicago]]
|doctoral_advisor  = [[L. E. Dickson]]
|academic_advisors = 
|doctoral_students = [[Richard Earl Block|Richard Block]]&lt;br&gt;[[Nathan Divinsky]]&lt;br&gt;[[Murray Gerstenhaber]]&lt;br&gt;[[Anatol Rapaport]]&lt;br&gt;[[Richard D. Schafer]]&lt;br&gt;[[Daniel Zelinsky]]
|notable_students  = 
|known_for         = [[Albert algebra]]s
|author_abbrev_bot = 
|author_abbrev_zoo = 
|influences        = 
|influenced        = 
|awards            = [[Cole Prize]] &lt;small&gt;(1939)&lt;/small&gt;
|religion          = 
|signature         =  &lt;!--(filename only)--&gt;
|footnotes         = 
}}
'''Abraham Adrian Albert''' (November 9, 1905 &amp;ndash; June 6, 1972) was an [[United States|American]] [[mathematician]].&lt;ref&gt;http://www.jinfo.org/Mathematics_Comp.html&lt;/ref&gt; In 1939, he received the American Mathematical Society's [[Cole Prize]] in Algebra for his work on [[Riemann matrices]].&lt;ref&gt;[http://www.jinfo.org/Cole_Mathematics.html Jewish recipients of the Frank Nelson Cole Prizes in algebra and number theory (43% of recipients)]&lt;/ref&gt;  He is best known for his work on the [[Albert–Brauer–Hasse–Noether theorem]]  on finite-dimensional division algebras over [[number fields]] and as the developer of [[Albert algebra]]s, which are also known as [[exceptional object|exceptional]] [[Jordan algebra]]s.

==Professional overview==
A second generation [[United States|American]], he was born in [[Chicago]] and most associated with that city. He received his [[Bachelor of Science]] in 1926, [[Master's degree|Masters]] in 1927, and [[Doctor of Philosophy|PhD]] in 1928, at the age of 22.  All degrees were obtained from the [[University of Chicago]].  He married around the same time as his graduation. He spent his postdoctoral year at [[Princeton University]] and then from 1929 to 1931 he was an instructor at [[Columbia University]]. During this period he worked on [[Abelian varieties]] and their endomorphism algebras. He returned to Princeton for the opening year of the [[Institute for Advanced Study]] in 1933-34 and spent another year in Princeton in 1961-62 as the first Director of the Communications Research Division of IDA (the Institute for Defense Analyses).

From 1931 to 1972, he served on the mathematics faculty at the University of Chicago, where he became chair of the Mathematics Department in 1958 and Dean of the Physical Sciences Division in 1961.

As a research mathematician, he is primarily known for his work as one of the principal developers of the theory of [[linear associative algebra]]s and as a pioneer in the development of [[linear]] non-[[associative]] [[algebra over a field|algebra]]s, although all of this grew out of his work on [[endomorphism]] algebras of Abelian varieties.

As an [[applied mathematician]], he also did work for the military during [[World War II]] and thereafter.  One of his most notable achievements was his groundbreaking work on [[cryptography]]. He prepared a manuscript, "Some Mathematical Aspects of Cryptography," for his invited address at a meeting of the [[American Mathematical Society]] in November 1941. The theory that developed from this work can be seen in [[digital communications]] technologies.

After WWII, he became a forceful advocate favoring government support for research in mathematics on a par with other physical sciences. He served on policy-making bodies at the [[Office of Naval Research]], the [[United States National Research Council]], and the [[National Science Foundation]] that funneled research grants into mathematics, giving many young mathematicians career opportunities previously unavailable.  Due to his success in helping to give mathematical research a sound financial footing, he earned a reputation as a "statesman for mathematics." Albert was elected a Fellow of the [[American Academy of Arts and Sciences]] in 1968.&lt;ref name=AAAS&gt;{{cite web|title=Book of Members, 1780-2010: Chapter A|url=http://www.amacad.org/publications/BookofMembers/ChapterA.pdf|publisher=American Academy of Arts and Sciences|accessdate=6 April 2011| archiveurl= https://web.archive.org/web/20110510021801/http://www.amacad.org/publications/BookofMembers/ChapterA.pdf| archivedate= 10 May 2011 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;

==Publications==

===Books===
* A. A. Albert, ''Algebras and their radicals, and division algebras'', 1928.
* {{Citation | last1=Albert | first1=A. Adrian | title=Modern higher algebra | url={{Google books|iVwZCgAAQBAJ|Modern higher algebra|plainurl=yes}} | publisher=[[Cambridge University Press]] | isbn=978-1-107-54462-8 | year=2015 |origyear=1938 }}.&lt;ref&gt;{{cite journal|author=Brinkmann, H. W.|title=Review: ''Modern Higher Algebra'' by A. Adrian Albert|journal=Bull. Amer. Math. Soc.|year=1938|volume=44|issue=7|pages=471–473|url=http://www.ams.org/journals/bull/1938-44-07/S0002-9904-1938-06758-4/S0002-9904-1938-06758-4.pdf|doi=10.1090/s0002-9904-1938-06758-4}}&lt;/ref&gt;
* A. A. Albert, ''Structure of algebras'', 1939.&lt;ref&gt;{{cite journal|author=Baer, Reinhold|authorlink=Reinhold Baer|title=Review: A. Adrian Albert, ''Structure of Algebras''|journal=Bull. Amer. Math. Soc.|year=1940|volume=46|issue=7|pages=587–591|url=http://projecteuclid.org/euclid.bams/1183502777|doi=10.1090/s0002-9904-1940-07233-7}}&lt;/ref&gt; Colloquium publications '''24''', [[American Mathematical Society]], 2003, {{ISBN|0-8218-1024-3}}.
* {{Citation |title=Introduction to algebraic theories|year=1941}}
* {{Citation |title=College algebra|year=1946}}
* {{Citation |title=Solid analytic geometry|year=1949}}
* {{Citation |title=Fundamental concepts of higher algebra|year=1956}}&lt;ref&gt;{{cite journal|author=Mattuck, Arthur|title=Review: ''Fundamental concepts of higher algebra'' by A. Adrian Albert|journal=Bull. Amer. Math. Soc.|year=1957|volume=63|issue=5|pages=323–325|url=http://www.ams.org/journals/bull/1957-63-05/S0002-9904-1957-10130-X/S0002-9904-1957-10130-X.pdf|doi=10.1090/s0002-9904-1957-10130-x}}&lt;/ref&gt;
* with Rebeun Sandler: {{cite book|title=Introduction to finite projective plans|year=1968}}
*{{Citation | last1=Albert | first1=A. Adrian | editor1-last=Block | editor1-first=Richard E. | editor2-last=Jacobson | editor2-first=Nathan | editor2-link=Nathan Jacobson | editor3-last=Osborn | editor3-first=J. Marshall | editor4-last=Saltman | editor4-first=David J. | editor5-last=Zelinsky | editor5-first=Daniel | title=Collected mathematical papers. Part 1. Associative algebras and Riemann matrices.| url=https://books.google.com/books?isbn=0821800051 | publisher=[[American Mathematical Society]] | location=Providence, R.I. | isbn=978-0-8218-0005-8 | year=1993 | mr=1213451}}
*{{Citation | last1=Albert | first1=A. Adrian | editor1-last=Block | editor1-first=Richard E. | editor2-last=Jacobson | editor2-first=Nathan | editor2-link=Nathan Jacobson | editor3-last=Osborn | editor3-first=J. Marshall | editor4-last=Saltman | editor4-first=David J. | editor5-last=Zelinsky | editor5-first=Daniel | title=Collected mathematical papers. Part 2. Nonassociative algebras and miscellany | url=https://books.google.com/books?isbn=0821800078 | publisher=[[American Mathematical Society]] | location=Providence, R.I. | isbn=978-0-8218-0007-2 | year=1993 | mr=1213452}}

===Articles in PNAS===
*{{cite journal|title=The Norm Form of a Rational Division Algebra|journal=Proc Natl Acad Sci U S A|pmc=528485 | pmid=16590045|volume=43|year=1957|pages=506–9|doi=10.1073/pnas.43.6.506 }}
*{{cite journal|title=On Hermitian Operators over the Cayley Algebra|journal=Proc Natl Acad Sci U S A|pmc=528152 | pmid=16589719|volume=41|year=1955|pages=639–40|doi=10.1073/pnas.41.9.639 }}
*{{cite journal|title=A Note on the Exceptional Jordan Algebra|journal=Proc Natl Acad Sci U S A|pmc=1063206 | pmid=15430315|volume=36|year=1950|pages=372–4|doi=10.1073/pnas.36.7.372 }}
*{{cite journal|title=A Theory of Trace-Admissible Algebras|journal=Proc Natl Acad Sci U S A|pmc=1063026 | pmid=16588897|volume=35|year=1949|pages=317–22|doi=10.1073/pnas.35.6.317}}
*{{cite journal|title=The Minimum Rank of a Correlation Matrix|journal=Proc Natl Acad Sci U S A|pmc=1078686 | pmid=16588638|volume=30|year=1944|pages=144–6|doi=10.1073/pnas.30.6.144 }}
*{{cite journal|title=The Matrices of Factor Analysis|journal=Proc Natl Acad Sci U S A|pmc=1078675 | pmid=16578117|volume=30|year=1944|pages=90–5|doi=10.1073/pnas.30.4.90 }}
*{{cite journal|title=On the Structure of Pure Riemann Matrices with Non-commutative Multiplication Algebras|journal=Proc Natl Acad Sci U S A|pmc=526637 | pmid=16587573|volume=16|year=1930|pages=308–12|doi=10.1073/pnas.16.4.308}}
*{{cite journal|title=The Group of the Rank Equation of Any Normal Division Algebra|journal=Proc Natl Acad Sci U S A|pmc=1085796 | pmid=16587420|volume=14|year=1928|pages=906–7|doi=10.1073/pnas.14.12.906 }}
*{{cite journal|title=On the Nuclei of a Simple Jordan Algebra|journal=Proc Natl Acad Sci U S A|pmc=221198 | pmid=16578544|volume=50|year=1963|pages=446–7|doi=10.1073/pnas.50.3.446 }}
*{{cite journal|title=A Property of Special Jordan Algebras|journal=Proc Natl Acad Sci U S A|pmc=534263 | pmid=16589918|volume=42|year=1956|pages=624–5|doi=10.1073/pnas.42.9.624 }}
*{{cite journal|title=On Involutorial Algebras|journal=Proc Natl Acad Sci U S A|pmc=528119 | pmid=16589700|volume=41|year=1955|pages=480–2|doi=10.1073/pnas.41.7.480 }}
*{{cite journal|title=Involutorial Simple Algebras and Real Riemann Matrices|journal=Proc Natl Acad Sci U S A|pmc=1076512 | pmid=16587930|volume=20|year=1934|pages=676–81|doi=10.1073/pnas.20.12.676 }}
*{{cite journal|title=Normal Division Algebras of 2&lt;sup&gt;2m &lt;/sup&gt;|journal=Proc Natl Acad Sci U S A|pmc=1076070 | pmid=16587641|volume=17|year=1931|pages=389–92|doi=10.1073/pnas.17.6.389 }}
*{{cite journal|title=On Direct Products, Cyclic Division Algebras, and Pure Riemann Matrices|journal=Proc Natl Acad Sci U S A|pmc=526638 | pmid=16587574|volume=16|year=1930|pages=313–5|doi=10.1073/pnas.16.4.313}}
*{{cite journal|title=The Rank Function of Any Simple Algebra|journal=Proc Natl Acad Sci U S A|pmc=522469 | pmid=16587486|volume=15|year=1929|pages=372–6|doi=10.1073/pnas.15.4.372 }}
*{{cite journal|title=Normal Division Algebras Satisfying Mild Assumptions|journal=Proc Natl Acad Sci U S A|pmc=1085795 | pmid=16587419|volume=14|year=1928|pages=904–6|doi=10.1073/pnas.14.12.904 }}

==References==
{{Reflist}}

== Further reading ==
* Nancy E. Albert, ''A&lt;sup&gt;3&lt;/sup&gt; and His Algebra: How a Boy from Chicago's West Side Became a Force in American Mathematics'', iUniverse, Lincoln, NE, 2005. {{ISBN|978-0-595-32817-8}}.

== External links ==
* {{MathGenealogy |id=6130}}
* {{MacTutor Biography|id=Albert_Abraham}}
* [http://www.nasonline.org/publications/biographical-memoirs/memoir-pdfs/albert-abraham.pdf Abraham Adrian Albert 1905–1972], A Biographical Memoir by [[Irving Kaplansky]]
*[http://www.nasonline.org/publications/biographical-memoirs/memoir-pdfs/albert-abraham.pdf National Academy of Sciences Biographical Memoir]
*[https://scholar.google.com/scholar?as_q=&amp;as_epq=&amp;as_oq=&amp;as_eq=&amp;as_occt=any&amp;as_sauthors=abraham+adrian+albert&amp;as_publication=&amp;as_ylo=&amp;as_yhi=&amp;btnG=&amp;hl=en&amp;as_sdt=0%2C11 search on author Abraham Adrian Albert] from [[Google Scholar]]

{{AMS Presidents}}

{{Authority control}}

{{DEFAULTSORT:Albert, Abraham Adrian}}
[[Category:1905 births]]
[[Category:1972 deaths]]
[[Category:20th-century American mathematicians]]
[[Category:American Jews]]
[[Category:Algebraists]]
[[Category:Fellows of the American Academy of Arts and Sciences]]
[[Category:Institute for Advanced Study visiting scholars]]
[[Category:Members of the United States National Academy of Sciences]]
[[Category:Presidents of the American Mathematical Society]]
[[Category:Princeton University faculty]]
[[Category:University of Chicago alumni]]
[[Category:University of Chicago faculty]]
[[Category:Columbia University faculty]]
[[Category:People from Chicago]]
[[Category:Mathematicians from Illinois]]</text>
      <sha1>to4mqzp3su8sevgd0rsrvo5zufbw9xo</sha1>
    </revision>
  </page>
  <page>
    <title>Artin–Mazur zeta function</title>
    <ns>0</ns>
    <id>1342156</id>
    <revision>
      <id>862708137</id>
      <parentid>852434816</parentid>
      <timestamp>2018-10-06T05:10:19Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Removing category Eponymous scientific concepts per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2018 September 22]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2819">In [[mathematics]], the '''Artin&amp;ndash;Mazur [[zeta function]]''', named after [[Michael Artin]] and [[Barry Mazur]], is a function that is used for studying the [[iterated function]]s that occur in [[dynamical systems]] and [[fractals]].

It is defined as the [[formal power series]]

:&lt;math&gt;\zeta_f(z)=\exp \left(\sum_{n=1}^\infty \operatorname{card} 
\left(\operatorname{Fix} (f^n)\right) \frac {z^n}{n}\right),&lt;/math&gt;

where Fix(''&amp;fnof;''&lt;sup&gt;&amp;nbsp;''n''&lt;/sup&gt;) is the set of [[Fixed point (mathematics)|fixed point]]s of the ''n''th iterate of the function ''&amp;fnof;'', and card(Fix(''&amp;fnof;''&lt;sup&gt;&amp;nbsp;''n''&lt;/sup&gt;)) is the number of fixed points (i.e. the [[cardinality]] of that set).

Note that the zeta function is defined only if the set of fixed points is finite for each ''n''. This definition is formal in that the series does not always have a positive [[radius of convergence]].

The Artin&amp;ndash;Mazur zeta function is invariant under [[topological conjugacy|topological conjugation]].

The [[Milnor–Thurston kneading theory|Milnor&amp;ndash;Thurston theorem]] states that the Artin&amp;ndash;Mazur zeta function is the inverse of the [[kneading determinant]] of ''&amp;fnof;''.

==Analogues==

The Artin&amp;ndash;Mazur zeta function is formally similar to the [[local zeta function]], when a [[diffeomorphism]] on a compact manifold replaces the [[Frobenius mapping]] for an [[algebraic variety]] over a [[finite field]].

The [[Ihara zeta function]] of a graph can be interpreted as an example of the Artin&amp;ndash;Mazur zeta function.

==See also==

*[[Lefschetz number]]
*[[Lefschetz zeta function|Lefschetz zeta-function]]

== References ==
* {{Citation | doi=10.2307/1970384 | last1=Artin | first1=Michael | author1-link=Michael Artin | last2=Mazur | first2=Barry | author2-link=Barry Mazur | title=On periodic points | mr=0176482 | year=1965 | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=81 | pages=82–99 | issue=1 | publisher=Annals of Mathematics | jstor=1970384}}
* [[David Ruelle]], [http://www.maths.ex.ac.uk/~mwatkins/zeta/ruelle.pdf Dynamical Zeta Functions and Transfer Operators] (2002) (PDF)
* {{cite journal | first1=Motoko |last1=Kotani | first2=Toshikazu | last2=Sunada |  author2-link=Toshikazu Sunada | title=Zeta functions of finite graphs | journal=J. Math. Sci. Univ. Tokyo | volume=7 | year=2000 | pages=7–25 }}
* {{citation | title=Zeta Functions of Graphs: A Stroll through the Garden | volume=128 | series=Cambridge Studies in Advanced Mathematics | first=Audrey | last=Terras | authorlink=Audrey Terras | publisher=[[Cambridge University Press]] | year=2010 | isbn=0-521-11367-9 | zbl=1206.05003 }}

{{DEFAULTSORT:Artin-Mazur zeta function}}
[[Category:Zeta and L-functions]]
[[Category:Dynamical systems]]
[[Category:Fixed points (mathematics)]]</text>
      <sha1>2xsnt95ixyc05woiow35qszs2an9pz5</sha1>
    </revision>
  </page>
  <page>
    <title>Basic hypergeometric series</title>
    <ns>0</ns>
    <id>2233526</id>
    <revision>
      <id>849205137</id>
      <parentid>849202369</parentid>
      <timestamp>2018-07-07T08:44:16Z</timestamp>
      <contributor>
        <username>MathXplore</username>
        <id>31773483</id>
      </contributor>
      <minor/>
      <comment>/* Matrix version */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10932">In [[mathematics]], '''basic hypergeometric series''', or '''hypergeometric ''q''-series''', are [[q-analog|''q''-analogue]] generalizations of [[generalized hypergeometric series]], and are in turn generalized by [[elliptic hypergeometric series]].  
A series ''x''&lt;sub&gt;''n''&lt;/sub&gt; is called hypergeometric if the ratio of successive terms ''x''&lt;sub&gt;''n''+1&lt;/sub&gt;/''x''&lt;sub&gt;''n''&lt;/sub&gt; is a [[rational function]] of ''n''.  If the ratio of successive terms is a rational function of ''q''&lt;sup&gt;''n''&lt;/sup&gt;, then the series is called a basic hypergeometric series. The number ''q'' is called the base. 

The basic hypergeometric series &lt;sub&gt;2&lt;/sub&gt;φ&lt;sub&gt;1&lt;/sub&gt;(''q''&lt;sup&gt;α&lt;/sup&gt;,''q''&lt;sup&gt;β&lt;/sup&gt;;''q''&lt;sup&gt;γ&lt;/sup&gt;;''q'',''x'') was first considered by {{harvs|txt|authorlink=Eduard Heine|first=Eduard|last= Heine|year=1846}}. It becomes the hypergeometric series ''F''(α,β;γ;''x'') in the limit when the base ''q'' is 1.

==Definition==
There are two forms of basic hypergeometric series,  the '''unilateral basic hypergeometric series''' φ, and the more general '''bilateral basic hypergeometric series''' ψ.
The '''unilateral basic hypergeometric series''' is defined as

:&lt;math&gt;\;_{j}\phi_k \left[\begin{matrix} 
a_1 &amp; a_2 &amp; \ldots &amp; a_{j} \\ 
b_1 &amp; b_2 &amp; \ldots &amp; b_k \end{matrix} 
; q,z \right] = \sum_{n=0}^\infty  
\frac {(a_1, a_2, \ldots, a_{j};q)_n} {(b_1, b_2, \ldots, b_k,q;q)_n} \left((-1)^nq^{n\choose 2}\right)^{1+k-j}z^n&lt;/math&gt;
where 
:&lt;math&gt;(a_1,a_2,\ldots,a_m;q)_n = (a_1;q)_n (a_2;q)_n \ldots (a_m;q)_n&lt;/math&gt;
and 
:&lt;math&gt;(a;q)_n = \prod_{k=0}^{n-1} (1-aq^k)=(1-a)(1-aq)(1-aq^2)\cdots(1-aq^{n-1})&lt;/math&gt;
is the [[q-shifted factorial|''q''-shifted factorial]].
The most important special case is when ''j'' = ''k'' + 1, when it becomes
:&lt;math&gt;\;_{k+1}\phi_k \left[\begin{matrix} 
a_1 &amp; a_2 &amp; \ldots &amp; a_{k}&amp;a_{k+1} \\ 
b_1 &amp; b_2 &amp; \ldots &amp; b_{k} \end{matrix} 
; q,z \right] = \sum_{n=0}^\infty  
\frac {(a_1, a_2, \ldots, a_{k+1};q)_n} {(b_1, b_2, \ldots, b_k,q;q)_n} z^n.&lt;/math&gt;
This series is called ''balanced'' if ''a''&lt;sub&gt;1&lt;/sub&gt; ... ''a''&lt;sub&gt;''k'' + 1&lt;/sub&gt; = ''b''&lt;sub&gt;1&lt;/sub&gt; ...''b''&lt;sub&gt;''k''&lt;/sub&gt;''q''.
This series is called ''well poised'' if ''a''&lt;sub&gt;1&lt;/sub&gt;''q'' = ''a''&lt;sub&gt;2&lt;/sub&gt;''b''&lt;sub&gt;1&lt;/sub&gt; = ... = ''a''&lt;sub&gt;''k'' + 1&lt;/sub&gt;''b''&lt;sub&gt;''k''&lt;/sub&gt;, and ''very well poised'' if in addition  ''a''&lt;sub&gt;2&lt;/sub&gt; = −''a''&lt;sub&gt;3&lt;/sub&gt; = ''qa''&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;1/2&lt;/sup&gt;. 
The unilateral basic hypergeometric series is a q-analog of the hypergeometric series since
:&lt;math&gt;\lim_{q\to 1}\;_{j}\phi_k \left[\begin{matrix} 
q^{a_1} &amp; q^{a_2} &amp; \ldots &amp; q^{a_j} \\ 
q^{b_1} &amp; q^{b_2} &amp; \ldots &amp; q^{b_k} \end{matrix} 
; q,(q-1)^{1+k-j} z \right]=\;_{j}F_k \left[\begin{matrix} 
a_1 &amp; a_2 &amp; \ldots &amp; a_j \\ 
b_1 &amp; b_2 &amp; \ldots &amp; b_k \end{matrix} 
;z \right]&lt;/math&gt;
holds ({{harvtxt|Koekoek|Swarttouw|1996}}).&lt;br&gt;
The '''bilateral basic hypergeometric series''', corresponding to the [[bilateral hypergeometric series]], is defined as

:&lt;math&gt;\;_j\psi_k \left[\begin{matrix} 
a_1 &amp; a_2 &amp; \ldots &amp; a_j \\ 
b_1 &amp; b_2 &amp; \ldots &amp; b_k  \end{matrix} 
; q,z \right] = \sum_{n=-\infty}^\infty  
\frac {(a_1, a_2, \ldots, a_j;q)_n} {(b_1, b_2, \ldots, b_k;q)_n}  \left((-1)^nq^{n\choose 2}\right)^{k-j}z^n.&lt;/math&gt;

The most important special case is when ''j'' = ''k'', when it becomes
:&lt;math&gt;\;_k\psi_k \left[\begin{matrix} 
a_1 &amp; a_2 &amp; \ldots &amp; a_k \\ 
b_1 &amp; b_2 &amp; \ldots &amp; b_k  \end{matrix} 
; q,z \right] = \sum_{n=-\infty}^\infty  
\frac {(a_1, a_2, \ldots, a_k;q)_n} {(b_1, b_2, \ldots, b_k;q)_n} z^n.&lt;/math&gt;

The unilateral series can be obtained as a special case of the bilateral one by setting one of the ''b'' variables equal to ''q'', at least when none of the ''a'' variables is a power of ''q'', as all the terms with ''n'' &lt; 0 then vanish.

==Simple series==
Some simple series expressions include

:&lt;math&gt;\frac{z}{1-q} \;_{2}\phi_1 \left[\begin{matrix} 
q \; q \\ 
q^2  \end{matrix}\;  ; q,z \right] = 
\frac{z}{1-q}
+ \frac{z^2}{1-q^2}
+ \frac{z^3}{1-q^3}
+ \ldots &lt;/math&gt;

and 

:&lt;math&gt;\frac{z}{1-q^{1/2}} \;_{2}\phi_1 \left[\begin{matrix} 
q \; q^{1/2} \\ 
q^{3/2}  \end{matrix}\;  ; q,z \right] = 
\frac{z}{1-q^{1/2}}
+ \frac{z^2}{1-q^{3/2}}
+ \frac{z^3}{1-q^{5/2}}
+ \ldots &lt;/math&gt;

and 

:&lt;math&gt;\;_{2}\phi_1 \left[\begin{matrix} 
q \; -1 \\ 
-q  \end{matrix}\;  ; q,z \right] = 1+
\frac{2z}{1+q}
+ \frac{2z^2}{1+q^2}
+ \frac{2z^3}{1+q^3}
+ \ldots. &lt;/math&gt;

==The ''q''-binomial theorem==
The ''q''-binomial theorem (first published in 1811 by [[Heinrich August Rothe]])&lt;ref&gt;{{citation
 | last = Bressoud | first = D. M.
 | doi = 10.1017/S0305004100058114
 | issue = 2
 | journal = Mathematical Proceedings of the Cambridge Philosophical Society
 | mr = 600238
 | pages = 211–223
 | title = Some identities for terminating ''q''-series
 | volume = 89
 | year = 1981| bibcode = 1981MPCPS..89..211B
 }}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last = Benaoum | first = H. B.
 | arxiv = math-ph/9812011
 | doi = 10.1088/0305-4470/31/46/001
 | issue = 46
 | journal = Journal of Physics A: Mathematical and General
 | pages = L751–L754
 | title = ''h''-analogue of Newton's binomial formula
 | volume = 31| bibcode = 1998JPhA...31L.751B
 }}.&lt;/ref&gt; states that

:&lt;math&gt;\;_{1}\phi_0 (a;q,z) =\frac{(az;q)_\infty}{(z;q)_\infty}= \prod_{n=0}^\infty 
\frac {1-aq^n z}{1-q^n z}&lt;/math&gt;

which follows by repeatedly applying the identity
:&lt;math&gt;\;_{1}\phi_0 (a;q,z) = 
\frac {1-az}{1-z} \;_{1}\phi_0 (a;q,qz).&lt;/math&gt;

The special case of ''a''&amp;nbsp;=&amp;nbsp;0 is closely related to the [[q-exponential]].
===Cauchy binomial theorem===
Cauchy binomial theorem is a special case of the q-binomial theorem&lt;ref&gt;[http://mathworld.wolfram.com/CauchyBinomialTheorem.html Wolfram Mathworld: Cauchy Binomial Theorem]&lt;/ref&gt;.
: &lt;math&gt;\sum_{n=0}^{N}y^nq^{n(n+1)/2}\begin{bmatrix}N\\n\end{bmatrix}_q=\prod_{k=1}^{N}\left(1+yq^k\right)\qquad(|q|&lt;1)&lt;/math&gt;

==Ramanujan's identity==
[[Srinivasa Ramanujan]] gave the identity

:&lt;math&gt;\;_1\psi_1 \left[\begin{matrix} a \\ b \end{matrix} ; q,z \right] 
= \sum_{n=-\infty}^\infty \frac {(a;q)_n} {(b;q)_n} z^n
= \frac {(b/a,q,q/az,az;q)_\infty }
{(b,b/az,q/a,z;q)_\infty} &lt;/math&gt;

valid for |''q''|&amp;nbsp;&amp;lt;&amp;nbsp;1 and |''b''/''a''|&amp;nbsp;&amp;lt;&amp;nbsp;|''z''|&amp;nbsp;&amp;lt;&amp;nbsp;1. Similar identities for &lt;math&gt;\;_6\psi_6&lt;/math&gt; have been given by Bailey. Such identities can be understood to be generalizations of the [[Jacobi triple product]] theorem, which can be written using q-series as

:&lt;math&gt;\sum_{n=-\infty}^\infty q^{n(n+1)/2}z^n = 
(q;q)_\infty \; (-1/z;q)_\infty \; (-zq;q)_\infty.&lt;/math&gt;

[[Ken Ono]] gives a related [[formal power series]]&lt;ref&gt; Gwynneth H. Coogan and [[Ken Ono]], ''[http://www.ams.org/journals/proc/2003-131-03/S0002-9939-02-06649-2/S0002-9939-02-06649-2.pdf A q-series identity and the Arithmetic of Hurwitz Zeta  Functions]'', (2003) Proceedings of the [[American Mathematical Society]] '''131''', pp.&amp;nbsp;719–724
&lt;/ref&gt;

:&lt;math&gt;A(z;q) \stackrel{\rm{def}}{=} \frac{1}{1+z} \sum_{n=0}^\infty 
\frac{(z;q)_n}{(-zq;q)_n}z^n = 
\sum_{n=0}^\infty (-1)^n z^{2n} q^{n^2}.&lt;/math&gt;

==Watson's contour integral==
As an analogue of the [[Barnes integral]] for the hypergeometric series, [[G. N. Watson|Watson]] showed that
:&lt;math&gt;
{}_2\phi_1(a,b;c;q,z) = \frac{-1}{2\pi i}\frac{(a,b;q)_\infty}{(q,c;q)_\infty}
\int_{-i\infty}^{i\infty}\frac{(qq^s,cq^s;q)_\infty}{(aq^s,bq^s;q)_\infty}\frac{\pi(-z)^s}{\sin \pi s}ds
&lt;/math&gt;
where the poles of &lt;math&gt;(aq^s,bq^s;q)_\infty&lt;/math&gt; lie to the left of the contour and the remaining poles lie to the right. There is a similar contour integral for &lt;sub&gt; ''r''+1&lt;/sub&gt;φ&lt;sub&gt;''r''&lt;/sub&gt;. This contour integral gives an analytic continuation of the basic hypergeometric function in ''z''.
==Matrix version==
The basic hypergeometric matrix function can be defined as follows:
:&lt;math&gt;
{}_2\phi_1(A,B;C;q,z):= \sum_{n=0}^\infty\frac{(A;q)_n(B;q)_n}{(C;q)_n(q;q)_n}z^n,\quad (A;q)_0:=1,\quad(A;q)_n:=\prod_{k=0}^{n-1}(1-Aq^k).&lt;/math&gt;
The ratio test shows that this matrix function is absolutely convergent&lt;ref&gt; Ahmed Salem (2014) The basic Gauss hypergeometric matrix function
and its matrix q-difference equation, Linear and Multilinear Algebra, 62:3, 347-361, DOI:
10.1080/03081087.2013.777437&lt;/ref&gt;.

==See also==
*[[Dixon's identity]]
*[[Rogers-Ramanujan identities]]
==Notes==
{{reflist}}
==External links==
* [http://mathworld.wolfram.com/q-HypergeometricFunction.html Wolfram Mathworld – q-Hypergeometric Functions].
==References==
*{{dlmf|id=17|first=G. E.|last=Andrews|title=q-Hypergeometric and Related Functions}}
* W.N. Bailey, ''Generalized Hypergeometric Series'', (1935) Cambridge Tracts in Mathematics and Mathematical Physics, No.32, Cambridge University Press, Cambridge.
* William Y. C. Chen and Amy Fu, ''[https://web.archive.org/web/20050530142121/http://cfc.nankai.edu.cn/publications/04-accepted/Chen-Fu-04A/semi.pdf Semi-Finite Forms of Bilateral Basic Hypergeometric Series]'' (2004)
* [[Harold Exton|Exton]], H. (1983), ''q-Hypergeometric Functions and Applications'', New York:  Halstead Press, Chichester: Ellis Horwood, {{ISBN|0853124914}},  {{ISBN|0470274530}}, {{ISBN|978-0470274538}}
* [[Sylvie Corteel]] and Jeremy Lovejoy, ''[https://web.archive.org/web/20050410204356/http://www.labri.fr/Perso/~lovejoy/1psi1.pdf  Frobenius Partitions and the Combinatorics of Ramanujan's &lt;math&gt;\,_1\psi_1&lt;/math&gt; Summation]''
*{{Citation | last1=Fine | first1=Nathan J. | title=Basic hypergeometric series and applications | url=http://www.ams.org/bookstore?fn=20&amp;arg1=survseries&amp;ikey=SURV-27 | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys and Monographs | isbn=978-0-8218-1524-3 | mr=956465 | year=1988 | volume=27}}
*{{Citation | last1=Gasper | first1=George | last2=Rahman | first2=Mizan | title=Basic hypergeometric series | publisher=[[Cambridge University Press]] | edition=2nd | series=Encyclopedia of Mathematics and its Applications | isbn=978-0-521-83357-8 | doi=10.2277/0521833574 | mr=2128719 | year=2004 | volume=96}}
*{{citation|first=Eduard |last=Heine|year=1846|journal= Journal für die reine und angewandte Mathematik|pages=210–212|volume=32|title=Über die Reihe &lt;math&gt;1+\frac{(q^\alpha-1)(q^\beta-1)}{(q-1)(q^\gamma-1)}x + \frac{(q^\alpha-1)(q^{\alpha+1}-1)(q^\beta-1)(q^{\beta+1}-1)}{(q-1)(q^2-1)(q^\gamma-1)(q^{\gamma+1}-1)}x^2+\cdots&lt;/math&gt;|url=http://resolver.sub.uni-goettingen.de/purl?GDZPPN002145391}}
*[[Victor Kac]], Pokman Cheung, Quantum calculus'',  Universitext, Springer-Verlag, 2002. {{isbn|0-387-95341-8}}

* Andrews, G. E., Askey, R. and Roy, R. (1999). Special Functions, Encyclopedia of Mathematics and its Applications, volume 71, [[Cambridge University Press]].
* [[Eduard Heine]], ''Theorie der Kugelfunctionen'', (1878) ''1'', pp 97–125.
* Eduard Heine, ''Handbuch die Kugelfunctionen. Theorie und Anwendung'' (1898) Springer, Berlin.

[[Category:Q-analogs]]
[[Category:Hypergeometric functions]]</text>
      <sha1>ql8kyfmpp03rexhjdbrfwwvwur2nlhn</sha1>
    </revision>
  </page>
  <page>
    <title>Benjamin Abram Bernstein</title>
    <ns>0</ns>
    <id>50635919</id>
    <revision>
      <id>842799367</id>
      <parentid>842727716</parentid>
      <timestamp>2018-05-24T18:51:10Z</timestamp>
      <contributor>
        <username>BrownHairedGirl</username>
        <id>754619</id>
      </contributor>
      <minor/>
      <comment>remove [[:Category:Jewish mathematicians]]. [[WP:G4]] per [[:WP:Categories for discussion/Log/2007 May 14#Category:Jewish_mathematicians]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8974">'''Benjamin Abram Bernstein''' (20 May 1881, [[Pasvalys]], [[Lithuania]] – 25 September 1964, [[Berkeley, California]]) was an American mathematician, specializing in [[mathematical logic]].&lt;ref name=BerkeleyObit&gt;[http://texts.cdlib.org/view?docId=hb338nb1j4&amp;doc.view=frames&amp;chunk.id=div00004&amp;toc.depth=1&amp;toc.id= Benjamin Abram Bernstein, University of California: In Memoriam, April 1965]&lt;/ref&gt;

==Biography==
With his family, Bernstein immigrated as a child to the United States. After completing public primary education in 1897 in Baltimore, he completed in 1902 his secondary education at [[Baltimore City College]], and then received in 1905 his A.B. degree from [[Johns Hopkins University]]. After completing two years of graduate study at Johns Hopkins University, he became in 1907 an instructor and continuing graduate student in mathematics at the [[University of California, Berkeley]]. There he received in 1913, with supervisor [[Mellen W. Haskell]], his Ph.D.&lt;ref name=MathGenRef&gt;{{MathGenealogy|id=23961}}&lt;/ref&gt; At Berkeley, Bernstein became in 1918 an assistant professor, in 1923 an associate professor, and in 1928 a full professor of mathematics, retiring in 1951 as professor emeritus.&lt;ref name=BerkeleyObit/&gt;

{{blockquote|When Professor Bernstein began his studies in mathematics, the subject of algebra of logic, or mathematical logic, was just beginning to receive intensive scholarly study, mainly from philosophers. Along with [[Edward Vermilye Huntington|E. V. Huntington]] of Harvard, Professor Bernstein was a pioneer in this field from the mathematical point of view. ... During this early period in the history of mathematical logic, a notable event for the subject was the publication of ''[[Principia Mathematica]]'', by [[Alfred North Whitehead|Whitehead]] and [[Bertrand Russell|Russell]]. Professor Bernstein was an intensive and critical student of this ''Principia'' and discussed it in many papers.&lt;ref name=BerkeleyObit/&gt;}}

He was an Invited Speaker at the [[International Congress of Mathematicians|ICM]] in 1924 in Toronto.&lt;ref&gt;Bernstein, B. A. [http://www.mathunion.org/ICM/ICM1924.1/Main/icm1924.1.0207.0216.ocr.pdf "Modular representations of finite algebras."] In Proc. 7th Int. Cong. Math, vol. 1, pp. 207-216. 1924.&lt;/ref&gt; His doctoral students include Robert Levit&lt;ref name=MathGenRef/&gt;&lt;ref&gt;[http://www.legacy.com/obituaries/sfgate/obituary.aspx?n=robert-levit&amp;pid=143215693 Robert Levit Obituary, San Francisco Chronicle, 30 May 2010]&lt;/ref&gt; and [[J.C.C. McKinsey]].

In June 1920 in New York City, Professor Bernstein married Rose Davidson; her brother was the famous sculptor [[Jo Davidson]]. Bernstein was predeceased by his wife and upon his death was survived by a daughter and a granddaughter.&lt;ref name=BerkeleyObit/&gt;

==Selected publications==
*with [[Armin Otto Leuschner|A. O. Leuschner]]: {{cite journal|title=Note on the graphical solutions of the fundamental equations in the short methods of determining orbits|journal=Bull. Amer. Math. Soc.|volume=18|year=1912|pages=294–298|mr=1559207|doi=10.1090/s0002-9904-1912-02211-5}}
*{{cite journal|title=A simplification of the Whitehead-Huntington set of postulates for boolean algebras|journal=Bull. Amer. Math. Soc.|volume=22|year=1916|pages=458–459|mr=1559829|doi=10.1090/s0002-9904-1916-02831-x}}
*{{cite journal|title=A set of four independent postulates for Boolean algebras|journal=Trans. Amer. Math. Soc.|volume=17|year=1916|pages=50–52|mr=1501029|doi=10.1090/s0002-9947-1916-1501029-5}}
*{{cite journal|title=The complete existential theory of Hurwitz's postulates for abelian groups and fields|journal=Bull. Amer. Math. Soc.|volume=28|year=1922|pages=397–399|mr=1560594|doi=10.1090/s0002-9904-1922-03600-2}}
*{{cite journal|title=Operations with respect to which the elements of a Boolean algebra form a group|journal=Trans. Amer. Math. Soc.|volume=26|year=1924|pages=171–175|mr=1501271|doi=10.1090/s0002-9947-1924-1501271-8}}
** Errata for 1924 Trans. Amer. Math. Soc. vol. 26, pages 171–175: published Trans. Amer. Math. Soc. 27 (1925) 600. {{mr|1500497}}
*{{cite journal|title=Representation of three-element algebras|journal=American Journal of Mathematics|volume=46|issue=2|year=1924|pages=110–116|doi=10.2307/2370826}}
*{{cite journal|title=Sets of postulates for the logic of propositions|journal=Trans. Amer. Math. Soc.|volume=28|year=1926|pages=472–478|mr=1501359|doi=10.1090/s0002-9947-1926-1501359-3}}
*{{cite journal|title=On the existence of fields in Boolean algebras|journal=Trans. Amer. Math. Soc.|volume=28|year=1926|pages=654–657|mr=1501369|doi=10.1090/s0002-9947-1926-1501369-6}}
*[http://www.ams.org/journals/bull/1926-32-05/S0002-9904-1926-04256-7/S0002-9904-1926-04256-7.pdf "On the Serial Relations in Boolean Algebras"] ''Bulletin of the American Mathematical Society'' 32(5) 523,4 1926
*{{cite journal|title=Whitehead and Russell's theory of deduction as a mathematical science|journal=Bull. Amer. Math. Soc.|volume=37|year=1931|pages=480–488|mr=1562173|doi=10.1090/s0002-9904-1931-05191-0}}
*{{cite journal|title=Application of Boolean algebra to proving consistency and independence of postulates|journal=Bull. Amer. Math. Soc.|volume=37|year=1931|pages=715–719|mr=1562241|doi=10.1090/s0002-9904-1931-05242-3}}
*with Nemo Debely: {{cite journal|title=A practical method for the modular representation of finite operations and relations|journal=Bull. Amer. Math. Soc.|volume=38|year=1932|pages=110–114|mr=1562337|doi=10.1090/s0002-9904-1932-05338-1}}
*{{cite journal|title=On proposition *4.78 of Principia Mathematica|journal=Bull. Amer. Math. Soc.|volume=38|year=1932|pages=388–391|mr=1562404|doi=10.1090/s0002-9904-1932-05400-3}}
*{{cite journal|title=Relation of Whitehead and Russell's theory of deduction to the Boolean logic of propositions|journal=Bull. Amer. Math. Soc.|volume=38|year=1932|pages=589–593|mr=1562463|doi=10.1090/s0002-9904-1932-05464-7}}
*{{cite journal|title=On unit-zero Boolean representations of operations and relations|journal=Bull. Amer. Math. Soc.|volume=38|year=1932|pages=707–712|mr=1562493|doi=10.1090/s0002-9904-1932-05507-0}}
*{{cite journal|title=Remarks on Propositions *1.1 and *3.35 of Principia Mathematica|journal=Bull. Amer. Math. Soc.|volume=39|year=1933|pages=111–114|mr=1562562|doi=10.1090/s0002-9904-1933-05576-3}}
*{{cite journal|title=Simplification of the set of four postulates for Boolean algebras in terms of rejection|journal=Bull. Amer. Math. Soc.|volume=39|year=1933|pages=783–787|mr=1562729|doi=10.1090/s0002-9904-1933-05738-5}}
*{{cite journal|title=On Section A of Principia Mathematica|journal=Bull. Amer. Math. Soc.|volume=39|year=1933|pages=788–792|mr=1562730|doi=10.1090/s0002-9904-1933-05740-3}}
*{{cite journal|title=A set of four postulates for Boolean algebra in terms of the "implicative" operation|journal=Trans. Amer. Math. Soc.|volume=36|year=1934|pages=876–884|mr=1501773|doi=10.1090/s0002-9947-1934-1501773-0}}
*{{cite journal|title=Postulates for Boolean algebra involving the operation of complete disjunction|journal=Annals of Mathematics|year=1936|pages=317–325|doi=10.2307/1968444}}
*{{cite journal|title=Remarks on Nicod's reduction of ''Principia Mathematica''|journal=Journal of Symbolic Logic|volume=2|issue=4|pages=165–166|date=December 1937|doi=10.2307/2268282}}
*{{cite journal|title=Postulates for abelian groups and fields in terms of non-associative operations|journal=Trans. Amer. Math. Soc.|volume=43|year=1938|pages=1–6|mr=1501932|doi=10.1090/s0002-9947-1938-1501932-0}}
*with [[Alfred Foster (mathematician)|Alfred L. Foster]]: {{cite journal|title=Symmetric Approach to Commutative Rings with Duality Theorem: Boolean Duality as Special Case|date=September 1944|journal=Duke Mathematical Journal|volume=11|pages=603–616|url=http://projecteuclid.org/download/pdffirstpage_1/euclid.dmj/1077472668|doi=10.1215/s0012-7094-44-01153-1}}
*{{cite journal|title=Postulate-sets for Boolean rings|journal=Trans. Amer. Math. Soc.|volume=55|year=1944|pages=393–400|mr=0009944|doi=10.1090/s0002-9947-1944-0009944-6}}
*{{cite journal|url=http://projecteuclid.org/download/pdffirstpage_1/euclid.dmj/1077474143|title=Weak definitions of a field|journal=Duke Mathematical Journal|volume=14|year=1947|pages=475–482|doi=10.1215/s0012-7094-47-01439-7}}

==References==
{{reflist}}

==External links==
*[http://www.oac.cdlib.org/findaid/ark:/13030/tf600005dh/ Guide to the Benjamin Abram Bernstein papers, 1901–1963, Online Archive of California]

{{Authority control}}

{{DEFAULTSORT:Bernstein, Benjamin Abram}}
[[Category:1881 births]]
[[Category:1964 deaths]]
[[Category:Mathematical logicians]]
[[Category:20th-century American mathematicians]]
[[Category:Baltimore City College alumni]]
[[Category:Johns Hopkins University alumni]]
[[Category:University of California, Berkeley alumni]]
[[Category:University of California, Berkeley faculty]]
[[Category:American people of Lithuanian-Jewish descent]]</text>
      <sha1>nmancuyymrbzi7fjyoecgn08kjttjor</sha1>
    </revision>
  </page>
  <page>
    <title>Binomial theorem</title>
    <ns>0</ns>
    <id>4677</id>
    <revision>
      <id>870665499</id>
      <parentid>870665477</parentid>
      <timestamp>2018-11-26T07:58:00Z</timestamp>
      <contributor>
        <username>GorillaWarfare</username>
        <id>4968133</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/101.50.123.24|101.50.123.24]] ([[User talk:101.50.123.24|talk]]) ([[WP:HG|HG]]) (3.4.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="33224">[[File:Pascal's triangle 5.svg|right|thumb|200px|The [[binomial coefficients]] appear as the entries of [[Pascal's triangle]] where each entry is the sum of the two above it.]]
In [[elementary algebra]], the '''binomial theorem''' (or '''binomial expansion''') describes the algebraic expansion of [[exponentiation|powers]] of a [[binomial (polynomial)|binomial]]. According to the theorem, it is possible to expand the polynomial {{math|(''x''&amp;nbsp;+&amp;nbsp;''y'')&lt;sup&gt;''n''&lt;/sup&gt;}} into a [[summation|sum]] involving terms of the form {{math|''a x''&lt;sup&gt;''b''&lt;/sup&gt; ''y''&lt;sup&gt;''c''&lt;/sup&gt;}}&lt;!-- U+200A (hair space) --&gt;, where the exponents {{math|''b''}} and {{math|''c''}} are [[nonnegative integer]]s with {{math|''b'' + ''c'' {{=}} ''n''}}, and the [[coefficient]] {{math|''a''}} of each term is a specific [[positive integer]] depending on {{math|''n''}} and {{math|''b''}}.  For example (for {{math| ''n'' {{=}} 4}}),

:&lt;math&gt;(x+y)^4 = x^4 + 4 x^3y + 6 x^2 y^2 + 4 x y^3 + y^4.&lt;/math&gt;

The coefficient {{math|''a''}} in the term of {{math|''a x''&lt;sup&gt;''b''&lt;/sup&gt; ''y''&lt;sup&gt;''c''&lt;/sup&gt;}} is known as the [[binomial coefficient]] &lt;math&gt;\tbinom nb&lt;/math&gt; or &lt;math&gt;\tbinom nc&lt;/math&gt; (the two have the same value). These coefficients for varying {{math|''n''}} and {{math|''b''}} can be arranged to form [[Pascal's triangle]]. These numbers also arise in [[combinatorics]], where &lt;math&gt;\tbinom nb&lt;/math&gt; gives the number of different [[combinations]] of {{math|''b''}} [[element (mathematics)|elements]] that can be chosen from an {{math|''n''}}-element [[set (mathematics)|set]].

==History==
Special cases of the binomial theorem were known since at least the 4th century B.C. when [[Greek mathematics|Greek mathematician]] [[Euclid]] mentioned the special case of the binomial theorem for exponent&amp;nbsp;2.&lt;ref name=wolfram&gt;{{cite web|url=http://mathworld.wolfram.com/BinomialTheorem.html|title=Binomial Theorem|website=Wolfram MathWorld|last=Weisstein|first=Eric W.}}&lt;/ref&gt;&lt;ref name="Coolidge"&gt;{{cite journal|title=The Story of the Binomial Theorem|first=J. L.|last=Coolidge|journal=The American Mathematical Monthly|volume=56|issue=3|date=1949|pp=147–157|doi=10.2307/2305028|jstor = 2305028}}&lt;/ref&gt;  There is evidence that the binomial theorem for cubes was known by the 6th century in India.&lt;ref name=wolfram /&gt;&lt;ref name="Coolidge" /&gt;

Binomial coefficients, as combinatorial quantities expressing the number of ways of selecting ''k'' objects out of ''n'' without replacement, were of interest to ancient Indian mathematicians.  The earliest known reference to this combinatorial problem is the ''Chandaḥśāstra'' by the Indian lyricist [[Pingala]] (c. 200 B.C.), which contains a method for its solution.&lt;ref name=Chinese&gt;{{cite book|title=A history of Chinese mathematics|author1=Jean-Claude Martzloff|author2=S.S. Wilson|author3=J. Gernet|author4=J. Dhombres|publisher=Springer|year=1987}}&lt;/ref&gt;{{rp|230}}  The commentator [[Halayudha]] from the 10th century A.D. explains this method using what is now known as [[Pascal's triangle]].&lt;ref name=Chinese /&gt;  By the 6th century A.D., the Indian mathematicians probably knew how to express this as a quotient &lt;math&gt;\frac{n!}{(n-k)!k!}&lt;/math&gt;,&lt;ref name="Biggs"&gt;{{cite journal|last=Biggs|first=N. L.|title=The roots of combinatorics|journal=Historia Math.|volume=6|date=1979|issue=2|pages=109–136|doi=10.1016/0315-0860(79)90074-0}}&lt;/ref&gt; and a clear statement of this rule can be found in the 12th century text ''Lilavati'' by [[Bhāskara II|Bhaskara]].&lt;ref name="Biggs" /&gt;

The first formulation of the binomial theorem and the table of binomial coefficients, to our knowledge, can be found in a work by [[Al-Karaji]], quoted by [[Al-Samaw'al]] in his "al-Bahir".&lt;ref&gt;{{Cite journal|last=|first=|date=|title=Taming the unknown. A history of algebra from antiquity to the early ttwentieth century|url=http://www.ams.org/journals/bull/2015-52-04/S0273-0979-2015-01491-6/S0273-0979-2015-01491-6.pdf|journal=Bulletin of the American Mathematical Society|volume=|pages=|via=|page=727|quote=However, algebra advanced in other respects. Around 1000, al-Karaji stated the binomial theorem}}&lt;/ref&gt;&lt;ref&gt;{{Cite book|url=https://books.google.com/?id=vSkClSvU_9AC&amp;pg=PA62#v=onepage&amp;q&amp;f=true|title=The Development of Arabic Mathematics: Between Arithmetic and Algebra|last=Rashed|first=R.|date=1994-06-30|publisher=Springer Science &amp; Business Media|isbn=9780792325659|language=en|page=63}}&lt;/ref&gt; [[Al-Karaji]] described the triangular pattern of the binomial coefficients&lt;ref name=Karaji&gt;{{MacTutor|id=Al-Karaji|title=Abu Bekr ibn Muhammad ibn al-Husayn Al-Karaji}}&lt;/ref&gt; and also provided a [[mathematical proof]] of both the binomial theorem and Pascal's triangle, using an early form of [[mathematical induction]].&lt;ref name=Karaji /&gt;   The Persian poet and mathematician [[Omar Khayyam]] was probably familiar with the formula to higher orders, although many of his mathematical works are lost.&lt;ref name="Coolidge" /&gt;  The binomial expansions of small degrees were known in the 13th century mathematical works of [[Yang Hui]]&lt;ref&gt;{{cite web
| last = Landau
| first = James A.
| title =Historia Matematica Mailing List Archive: Re: [HM] Pascal's Triangle
| work = Archives of Historia Matematica
| format = mailing list email
| accessdate = 2007-04-13
| date = 1999-05-08
| url = http://archives.math.utk.edu/hypermail/historia/may99/0073.html
}}&lt;/ref&gt; and also [[Chu Shih-Chieh]].&lt;ref name="Coolidge" /&gt;  Yang Hui attributes the method to a much earlier 11th century text of [[Jia Xian]], although those writings are now also lost.&lt;ref name=Chinese /&gt;{{rp|142}}

In 1544, [[Michael Stifel]] introduced the term "binomial coefficient" and showed how to use them to express &lt;math&gt;(1+a)^n&lt;/math&gt; in terms of &lt;math&gt;(1+a)^{n-1}&lt;/math&gt;, via "Pascal's triangle".&lt;ref name=Kline&gt;{{cite book|title=History of mathematical thought|first=Morris|last=Kline|page=273|publisher=Oxford University Press|year=1972}}&lt;/ref&gt;  [[Blaise Pascal]] studied the eponymous triangle comprehensively in the [[treatise]] ''Traité du triangle arithmétique'' (1653). However, the pattern of numbers was already known to the European mathematicians of the late Renaissance,  including Stifel, [[Niccolò Fontana Tartaglia]], and [[Simon Stevin]].&lt;ref name=Kline /&gt;

[[Isaac Newton]] is generally credited with the generalized binomial theorem, valid for any rational exponent.&lt;ref name=Kline /&gt;&lt;ref&gt;{{cite book|title=Elements of the History of Mathematics Paperback|date= 18 November 1998|first= N.|last=Bourbaki|others= J. Meldrum (Translator)|isbn=978-3-540-64767-6}}&lt;/ref&gt;

== Theorem statement ==
According to the theorem, it is possible
to expand any power of ''x''&amp;nbsp;+&amp;nbsp;''y'' into a sum of the form
:&lt;math&gt;(x+y)^n = {n \choose 0}x^n y^0 + {n \choose 1}x^{n-1}y^1 + {n \choose 2}x^{n-2}y^2 + \cdots + {n \choose n-1}x^1 y^{n-1} + {n \choose n}x^0 y^n,
&lt;/math&gt;
where each &lt;math&gt; \tbinom nk &lt;/math&gt; is a specific positive integer known as a [[binomial coefficient]]. (When an exponent is zero, the corresponding power expression is taken to be 1 and this multiplicative factor is often omitted from the term.  Hence one often sees the right side written as &lt;math&gt;\binom{n}{0} x^n + \ldots&lt;/math&gt;.) This formula is also referred to as the '''binomial formula''' or the '''binomial identity'''. Using [[Capital-sigma notation|summation notation]], it can be written as
:&lt;math&gt;(x+y)^n = \sum_{k=0}^n {n \choose k}x^{n-k}y^k = \sum_{k=0}^n {n \choose k}x^{k}y^{n-k}.
&lt;/math&gt;
The final expression follows from the previous one by the symmetry of ''x'' and ''y'' in the first expression, and by comparison it follows that the sequence of binomial coefficients in the formula is symmetrical.
A simple variant of the binomial formula is obtained by [[substitution (algebra)|substituting]] 1 for ''y'', so that it involves only a single [[Variable (mathematics)|variable]]. In this form, the formula reads
:&lt;math&gt;(1+x)^n = {n \choose 0}x^0 + {n \choose 1}x^1 + {n \choose 2}x^2 + \cdots + {n \choose {n-1}}x^{n-1} + {n \choose n}x^n,&lt;/math&gt;
or equivalently
:&lt;math&gt;(1+x)^n = \sum_{k=0}^n {n \choose k}x^k.&lt;/math&gt;

== Examples ==
[[File:Pascal triangle small.png|thumb|right|300px|Pascal's triangle]]
The most basic example of the binomial theorem is the formula for the [[Square (algebra)|square]] of {{math|''x'' + ''y''}}:

:&lt;math&gt;(x + y)^2 = x^2 + 2xy + y^2.&lt;/math&gt;

The binomial coefficients 1, 2, 1 appearing in this expansion correspond to the second row of Pascal's triangle.  (Note that the top "1" of the triangle is considered to be row 0, by convention.) The coefficients of higher powers of {{math|''x'' + ''y''}} correspond to lower rows of the triangle:

:&lt;math&gt;
\begin{align}
(x+y)^3 &amp; = x^3 + 3x^2y + 3xy^2 + y^3, \\[8pt]
(x+y)^4 &amp; = x^4 + 4x^3y + 6x^2y^2 + 4xy^3 + y^4, \\[8pt]
(x+y)^5 &amp; = x^5 + 5x^4y + 10x^3y^2 + 10x^2y^3 + 5xy^4 + y^5, \\[8pt]
(x+y)^6 &amp; = x^6 + 6x^5y + 15x^4y^2 + 20x^3y^3 + 15x^2y^4 + 6xy^5 + y^6, \\[8pt]
(x+y)^7 &amp; = x^7 + 7x^6y + 21x^5y^2 + 35x^4y^3 + 35x^3y^4 + 21x^2y^5 + 7xy^6 + y^7.
\end{align}
&lt;/math&gt;
Several patterns can be observed from these examples. In general, for the expansion {{math|(''x'' + ''y'')&lt;sup&gt;''n''&lt;/sup&gt;}}:
# the powers of {{math|''x''}} start at {{mvar|n}} and decrease by 1 in each term until they reach 0 (with {{math|1=''x''&lt;sup&gt;0&lt;/sup&gt; = 1}}, often unwritten);
# the powers of {{math|''y''}} start at 0 and increase by 1 until they reach {{math|''n''}}; 
# the {{mvar|n}}th row of Pascal's Triangle will be the coefficients of the expanded binomial when the terms are arranged in this way;
# the number of terms in the expansion before like terms are combined is the sum of the coefficients and is equal to {{math|2&lt;sup&gt;''n''&lt;/sup&gt;}}; and
# there will be {{math|''n'' + 1}} terms in the expression after combining like terms in the expansion.

The binomial theorem can be applied to the powers of any binomial. For example,

:&lt;math&gt;\begin{align}
(x+2)^3 &amp;= x^3 + 3x^2(2) + 3x(2)^2 + 2^3 \\
&amp;= x^3 + 6x^2 + 12x + 8.\end{align}&lt;/math&gt;

For a binomial involving subtraction, the theorem can be applied by using the form {{math|1=(''x'' − ''y'')&lt;sup&gt;''n''&lt;/sup&gt; = (''x'' + (−''y''))&lt;sup&gt;''n''&lt;/sup&gt;}}. This has the effect of changing the sign of every other term in the expansion:
:&lt;math&gt;(x-y)^3 = (x+(-y))^3 = x^3 + 3x^2(-y) + 3x(-y)^2 + (-y)^3 = x^3 - 3x^2y + 3xy^2 -y^3.&lt;/math&gt;

=== Geometric explanation ===
[[File:binomial_theorem_visualisation.svg|thumb|300px|Visualisation of binomial expansion up to the 4th power]]
For positive values of ''a'' and ''b'', the binomial theorem with ''n''&amp;nbsp;=&amp;nbsp;2 is the geometrically evident fact that a square of side {{nowrap|''a'' + ''b''}} can be cut into a square of side ''a'', a square of side ''b'', and two rectangles with sides ''a'' and ''b''. With ''n''&amp;nbsp;=&amp;nbsp;3, the theorem states that a cube of side {{nowrap|''a'' + ''b''}} can be cut into a cube of side ''a'', a cube of side ''b'', three ''a''×''a''×''b'' rectangular boxes, and three ''a''×''b''×''b'' rectangular boxes.

In [[calculus]], this picture also gives a geometric proof of the [[derivative]] &lt;math&gt;(x^n)'=nx^{n-1}:&lt;/math&gt;&lt;ref name="barth2004"&gt;{{cite journal | last = Barth | first = Nils R.| title = Computing Cavalieri's Quadrature Formula by a Symmetry of the ''n''-Cube | doi = 10.2307/4145193 | jstor = 4145193 | journal = The American Mathematical Monthly| issn = 0002-9890| volume = 111| issue = 9| pages = 811–813 | date=2004 | pmid =  | pmc =| postscript = , [http://nbarth.net/math/papers/barth-01-cavalieri.pdf author's copy], [http://nbarth.net/math/papers/ further remarks and resources]}}&lt;/ref&gt; if one sets &lt;math&gt;a=x&lt;/math&gt; and &lt;math&gt;b=\Delta x,&lt;/math&gt; interpreting ''b'' as an [[infinitesimal]] change in ''a,'' then this picture shows the infinitesimal change in the volume of an ''n''-dimensional [[hypercube]], &lt;math&gt;(x+\Delta x)^n,&lt;/math&gt; where the coefficient of the linear term (in &lt;math&gt;\Delta x&lt;/math&gt;) is &lt;math&gt;nx^{n-1},&lt;/math&gt; the area of the ''n'' faces, each of dimension &lt;math&gt;(n-1):&lt;/math&gt;
:&lt;math&gt;(x+\Delta x)^n = x^n + nx^{n-1}\Delta x + \tbinom{n}{2}x^{n-2}(\Delta x)^2 + \cdots.&lt;/math&gt;
Substituting this into the [[definition of the derivative]] via a [[difference quotient]] and taking limits means that the higher order terms, &lt;math&gt;(\Delta x)^2&lt;/math&gt; and higher, become negligible, and yields the formula &lt;math&gt;(x^n)'=nx^{n-1},&lt;/math&gt; interpreted as
:"the infinitesimal rate of change in volume of an ''n''-cube as side length varies is the area of ''n'' of its &lt;math&gt;(n-1)&lt;/math&gt;-dimensional faces".
If one integrates this picture, which corresponds to applying the [[fundamental theorem of calculus]], one obtains [[Cavalieri's quadrature formula]], the integral &lt;math&gt;\textstyle{\int x^{n-1}\,dx = \tfrac{1}{n} x^n}&lt;/math&gt; – see [[Cavalieri's quadrature formula#Proof|proof of Cavalieri's quadrature formula]] for details.&lt;ref name="barth2004" /&gt;

{{clear}}

== Binomial coefficients ==
{{Main|Binomial coefficient}}
The coefficients that appear in the binomial expansion are called '''binomial coefficients'''. These are usually written &lt;math&gt; \tbinom nk &lt;/math&gt;, and pronounced “''n'' choose ''k''”.

=== Formulae ===
The coefficient of ''x''&lt;sup&gt;''n''−''k''&lt;/sup&gt;''y''&lt;sup&gt;''k''&lt;/sup&gt; is given by the formula

:&lt;math&gt;{n \choose k} = \frac{n!}{k! (n-k)!}&lt;/math&gt;

which is defined in terms of the [[factorial]] function ''n''!. Equivalently, this formula can be written

:&lt;math&gt;{n \choose k} = \frac{n (n-1) \cdots (n-k+1)}{k (k-1) \cdots 1} = \prod_{\ell=1}^k \frac{n-\ell+1}{\ell} = \prod_{\ell=0}^{k-1} \frac{n-\ell}{k - \ell}&lt;/math&gt;

with ''k'' factors in both the numerator and denominator of the [[Fraction (mathematics)|fraction]]. Note that, although this formula involves a fraction, the binomial coefficient &lt;math&gt; \tbinom nk &lt;/math&gt; is actually an [[integer]].

=== Combinatorial interpretation ===
The binomial coefficient &lt;math&gt; \tbinom nk &lt;/math&gt; can be interpreted as the number of ways to choose ''k'' elements from an ''n''-element set. This is related to binomials for the following reason: if we write (''x''&amp;nbsp;+&amp;nbsp;''y'')&lt;sup&gt;''n''&lt;/sup&gt; as a [[Product (mathematics)|product]]
:&lt;math&gt;(x+y)(x+y)(x+y)\cdots(x+y),&lt;/math&gt;
then, according to the [[distributive law]], there will be one term in the expansion for each choice of either ''x'' or ''y'' from each of the binomials of the product. For example, there will only be one term ''x''&lt;sup&gt;''n''&lt;/sup&gt;, corresponding to choosing ''x'' from each binomial. However, there will be several terms of the form ''x''&lt;sup&gt;''n''−2&lt;/sup&gt;''y''&lt;sup&gt;2&lt;/sup&gt;, one for each way of choosing exactly two binomials to contribute a ''y''. Therefore, after [[combining like terms]], the coefficient of ''x''&lt;sup&gt;''n''−2&lt;/sup&gt;''y''&lt;sup&gt;2&lt;/sup&gt; will be equal to the number of ways to choose exactly 2 elements from an ''n''-element set.

== Proofs ==

=== Combinatorial proof ===

==== Example ====
The coefficient of ''xy''&lt;sup&gt;2&lt;/sup&gt; in

:&lt;math&gt;\begin{align}
(x+y)^3 &amp;= (x+y)(x+y)(x+y) \\
&amp;= xxx + xxy + xyx + \underline{xyy} + yxx + \underline{yxy} + \underline{yyx} + yyy \\
&amp;= x^3 + 3x^2y + \underline{3xy^2} + y^3.
\end{align}&lt;/math&gt;

equals &lt;math&gt;\tbinom{3}{2}=3&lt;/math&gt; because there are three ''x'',''y'' strings of length 3 with exactly two ''y'''s, namely,

:&lt;math&gt;xyy, \; yxy, \; yyx,&lt;/math&gt;

corresponding to the three 2-element subsets of {&amp;nbsp;1,&amp;nbsp;2,&amp;nbsp;3&amp;nbsp;}, namely,

:&lt;math&gt;\{2,3\},\;\{1,3\},\;\{1,2\}, &lt;/math&gt;

where each subset specifies the positions of the ''y'' in a corresponding string.

==== General case ====
Expanding (''x''&amp;nbsp;+&amp;nbsp;''y'')&lt;sup&gt;''n''&lt;/sup&gt; yields the sum of the 2&lt;sup&gt;&amp;nbsp;''n''&lt;/sup&gt; products of the form ''e''&lt;sub&gt;1&lt;/sub&gt;''e''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;...&amp;nbsp;''e''&lt;sub&gt;&amp;nbsp;''n''&lt;/sub&gt; where each ''e''&lt;sub&gt;&amp;nbsp;''i''&lt;/sub&gt; is ''x'' or&amp;nbsp;''y''. Rearranging factors shows that each product equals ''x''&lt;sup&gt;''n''−''k''&lt;/sup&gt;''y''&lt;sup&gt;''k''&lt;/sup&gt; for some ''k'' between 0 and&amp;nbsp;''n''. For a given ''k'', the following are proved equal in succession:
* the number of copies of ''x''&lt;sup&gt;''n''&amp;nbsp;−&amp;nbsp;''k''&lt;/sup&gt;''y''&lt;sup&gt;''k''&lt;/sup&gt; in the expansion
* the number of ''n''-character ''x'',''y'' strings having ''y'' in exactly ''k'' positions
* the number of ''k''-element subsets of {&amp;nbsp;1,&amp;nbsp;2,&amp;nbsp;...,&amp;nbsp;''n''}
* &lt;math&gt;{n \choose k}&lt;/math&gt; (this is either by definition, or by a short combinatorial argument if one is defining &lt;math&gt;{n \choose k}&lt;/math&gt; as &lt;math&gt;\frac{n!}{k! (n-k)!}&lt;/math&gt;).
This proves the binomial theorem.

=== Inductive proof ===
[[mathematical induction|Induction]] yields another proof of the binomial theorem. When ''n''&amp;nbsp;=&amp;nbsp;0, both sides equal 1, since ''x''&lt;sup&gt;0&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;1 and &lt;math&gt;\tbinom{0}{0}=1&lt;/math&gt;.
Now suppose that the equality holds for a given ''n''; we will prove it for ''n''&amp;nbsp;+&amp;nbsp;1.
For ''j'',&amp;nbsp;''k''&amp;nbsp;≥&amp;nbsp;0, let [''ƒ''(''x'',&amp;nbsp;''y'')]&lt;sub&gt;&amp;nbsp;''j,k''&lt;/sub&gt; denote the coefficient of ''x''&lt;sup&gt;''j''&lt;/sup&gt;''y''&lt;sup&gt;''k''&lt;/sup&gt; in the polynomial ''ƒ''(''x'',&amp;nbsp;''y'').
By the inductive hypothesis, (''x''&amp;nbsp;+&amp;nbsp;''y'')&lt;sup&gt;''n''&lt;/sup&gt; is a polynomial in ''x'' and ''y'' such that [(''x''&amp;nbsp;+&amp;nbsp;''y'')&lt;sup&gt;''n''&lt;/sup&gt;]&lt;sub&gt;&amp;nbsp;''j,k''&lt;/sub&gt; is &lt;math&gt;\tbinom{n}{k}&lt;/math&gt; if ''j''&amp;nbsp;+&amp;nbsp;''k''&amp;nbsp;=&amp;nbsp;''n'', and 0 otherwise.
The identity

:&lt;math&gt; (x+y)^{n+1} = x(x+y)^n + y(x+y)^n&lt;/math&gt;

shows that (''x''&amp;nbsp;+&amp;nbsp;''y'')&lt;sup&gt;''n'' + 1&lt;/sup&gt; also is a polynomial in ''x'' and ''y'', and

:&lt;math&gt; [(x+y)^{n+1}]_{j,k} = [(x+y)^n]_{j-1,k} + [(x+y)^n]_{j,k-1},&lt;/math&gt;

since if ''j''&amp;nbsp;+&amp;nbsp;''k''&amp;nbsp;=&amp;nbsp;''n''&amp;nbsp;+&amp;nbsp;1, then (''j''&amp;nbsp;−&amp;nbsp;1)&amp;nbsp;+&amp;nbsp;''k''&amp;nbsp;=&amp;nbsp;''n'' and ''j''&amp;nbsp;+&amp;nbsp;(''k''&amp;nbsp;−&amp;nbsp;1)&amp;nbsp;=&amp;nbsp;''n''. Now, the right hand side is

:&lt;math&gt; \binom{n}{k} + \binom{n}{k-1} = \binom{n+1}{k},&lt;/math&gt;

by [[Pascal's identity]].&lt;ref&gt;[http://proofs.wiki/Binomial_theorem Binomial theorem] – inductive proofs {{webarchive |url=https://web.archive.org/web/20150224130932/http://proofs.wiki/Binomial_theorem |date=February 24, 2015 }}&lt;/ref&gt; On the other hand, if ''j''&amp;nbsp;+''k''&amp;nbsp;≠&amp;nbsp;''n''&amp;nbsp;+&amp;nbsp;1, then (''j''&amp;nbsp;–&amp;nbsp;1)&amp;nbsp;+&amp;nbsp;''k''&amp;nbsp;≠&amp;nbsp;''n'' and ''j''&amp;nbsp;+(''k''&amp;nbsp;–&amp;nbsp;1)&amp;nbsp;≠&amp;nbsp;''n'', so we get 0&amp;nbsp;+&amp;nbsp;0&amp;nbsp;=&amp;nbsp;0. Thus

:&lt;math&gt;(x+y)^{n+1} = \sum_{k=0}^{n+1} \tbinom{n+1}{k} x^{n+1-k} y^k,&lt;/math&gt;

which is the inductive hypothesis with ''n''&amp;nbsp;+&amp;nbsp;1 substituted for ''n'' and so completes the inductive step.

== Generalizations ==

=== Newton's generalized binomial theorem ===
{{Main|Binomial series}}
Around 1665, [[Isaac Newton]] generalized the binomial theorem to allow real exponents other than nonnegative integers. (The same generalization also applies to [[complex number|complex]] exponents.) In this generalization, the finite sum is replaced by an [[infinite series]]. In order to do this, one needs to give meaning to binomial coefficients with an arbitrary upper index, which cannot be done using the usual formula with factorials.  However, for an arbitrary number ''r'', one can define

:&lt;math&gt;{r \choose k}=\frac{r\,(r-1) \cdots (r-k+1)}{k!} =\frac{(r)_k}{k!},&lt;/math&gt;
&lt;!--
This is not the same as \frac{r!}{k!\,(r−k)!}. Factorials are typically only defined on natural number arguments, but even if you are using factorials generalized (e.g. by the \Gamma function) to non-integer values, they are still undefined on the negative integers. To get the usual binomial theorem as a special case of this so-called generalization, we had better define the binomial coefficient when ''r'' is an integer, but in that case ''r''−''k'' will be a negative integer for sufficiently large ''k'', so one cannot use any formula involving the factorial &lt;math&gt;(r−k)!&lt;/math&gt;.

This negative comment about "not the same as…" seems to be needed. People keep coming along and completing this formula with this expression involving factorials, missing the point of this section.
~~~~perhaps someone could put a better explanation in! Here is an attempt!.
The problem with substituting \frac{r!}{k!\,(r−k)!} is that the ! ends up being used for negative numbers which doesn't work with the definition of !. Consequently, the notation here is used because if you look at it for a negative value of n, the value is still defined with this notation. That being said, many text books are careless about it.
--&gt;
where &lt;math&gt;(\cdot)_k&lt;/math&gt; is the [[Pochhammer symbol]], here standing for a [[falling factorial]].  This agrees with the usual definitions when ''r'' is a nonnegative integer.  Then, if ''x'' and ''y'' are real numbers with |''x''|&amp;nbsp;&gt;&amp;nbsp;|''y''|,&lt;ref name=convergence group=Note&gt;This is to guarantee convergence. Depending on ''r'', the series may also converge sometimes when |''x''|&amp;nbsp;=&amp;nbsp;|''y''|.&lt;/ref&gt; and ''r'' is any complex number, one has

:&lt;math&gt;
\begin{align}
(x+y)^r &amp; =\sum_{k=0}^\infty {r \choose k} x^{r-k} y^k \\
&amp; = x^r + r x^{r-1} y + \frac{r(r-1)}{2!} x^{r-2} y^2 + \frac{r(r-1)(r-2)}{3!} x^{r-3} y^3 + \cdots.
\end{align}
&lt;/math&gt;
When ''r'' is a nonnegative integer, the binomial coefficients for ''k''&amp;nbsp;&gt;&amp;nbsp;''r'' are zero, so this equation reduces to the usual binomial theorem, and there are at most  ''r''&amp;nbsp;+&amp;nbsp;1 nonzero terms. For other values of ''r'', the series typically has infinitely many nonzero terms.

For example, ''r''&amp;nbsp;=&amp;nbsp;1/2 gives the following series for the square root:
:&lt;math&gt;\sqrt{1+x} = \textstyle 1 + \frac{1}{2}x - \frac{1}{8}x^2 + \frac{1}{16}x^3 - \frac{5}{128}x^4 + \frac{7}{256}x^5 - \cdots&lt;/math&gt;

Taking &lt;math&gt;r=-1&lt;/math&gt;, the generalized binomial series gives the [[Geometric series#Formula|geometric series formula]], valid for &lt;math&gt;|x| &lt; 1&lt;/math&gt;:

:&lt;math&gt;(1+x)^{-1} = \frac{1}{1+x} = 1 - x + x^2 - x^3 + x^4 - x^5 + \cdots&lt;/math&gt;

More generally, with ''r''&amp;nbsp;=&amp;nbsp;−''s'':

:&lt;math&gt;\frac{1}{(1-x)^s} = \sum_{k=0}^\infty {s+k-1 \choose k} x^k \equiv \sum_{k=0}^\infty {s+k-1 \choose s-1} x^k.&lt;/math&gt;

So, for instance, when &lt;math&gt;s=1/2&lt;/math&gt;,
:&lt;math&gt;\frac{1}{\sqrt{1+x}} = \textstyle 1 -\frac{1}{2}x + \frac{3}{8}x^2 - \frac{5}{16}x^3 + \frac{35}{128}x^4 - \frac{63}{256}x^5 + \cdots&lt;/math&gt;

=== Further generalizations ===
The generalized binomial theorem can be extended to the case where ''x'' and ''y'' are complex numbers. For this version, one should again assume |''x''|&amp;nbsp;&gt;&amp;nbsp;|''y''|&lt;ref name=convergence group=Note /&gt; and define the powers of ''x''&amp;nbsp;+&amp;nbsp;''y'' and ''x'' using a [[Holomorphic function|holomorphic]] [[complex logarithm|branch of log]] defined on an open disk of radius |''x''| centered at ''x''.
The generalized binomial theorem is valid also for elements ''x'' and ''y'' of a [[Banach algebra]] as long as ''xy''&amp;nbsp;=&amp;nbsp;''yx'', ''x''&amp;nbsp;is invertible, and&amp;nbsp;||''y/x''||&amp;nbsp;&lt;&amp;nbsp;1.

A version of the binomial theorem is valid for the following [[Pochhammer symbol]]-like family of polynomials: for a given real constant ''c'', define &lt;math&gt; x^{(0)} = 1 &lt;/math&gt; and &lt;math&gt; x^{(n)} = \prod_{k=1}^{n}[x+(k-1)c]&lt;/math&gt; for &lt;math&gt; n &gt; 0&lt;/math&gt;.  Then&lt;ref name="Sokolowsky"&gt;{{cite journal|url=https://cms.math.ca/crux/backfile/Crux_v5n02_Feb.pdf#page=26|title=Problem 352|first1=Dan|last1=Sokolowsky|first2=Basil C.|last2=Rennie|journal=Crux Mathematicorum|volume=5|issue=2|date=February 1979|pp=55-56}}&lt;/ref&gt;
&lt;math display = "block"&gt; (a + b)^{(n)} = \sum_{k=0}^{n}\binom{n}{k}a^{(n-k)}b^{(k)}.&lt;/math&gt;
The case {{math|''c'' {{=}} 0}} recovers the usual binomial theorem.

More generally, a sequence &lt;math&gt;\{p_n\}_{n=0}^\infty&lt;/math&gt; of polynomials is said to be '''binomial''' if
* &lt;math&gt; \deg p_n = n &lt;/math&gt; for all &lt;math&gt;n&lt;/math&gt;,
* &lt;math&gt; p_0(0) = 1 &lt;/math&gt;, and
* &lt;math&gt; p_n(x+y) = \sum_{k=0}^n \binom{n}{k} p_k(x) p_{n-k}(y) &lt;/math&gt; for all &lt;math&gt;x&lt;/math&gt;, &lt;math&gt;y&lt;/math&gt;, and &lt;math&gt;n&lt;/math&gt;.
An operator &lt;math&gt;Q&lt;/math&gt; on the space of polynomials is said to be the ''basis operator'' of the sequence &lt;math&gt;\{p_n\}_{n=0}^\infty&lt;/math&gt; if &lt;math&gt;Qp_0 = 0&lt;/math&gt; and &lt;math&gt; Q p_n = n p_{n-1} &lt;/math&gt; for all &lt;math&gt; n \geqslant 1 &lt;/math&gt;. A sequence &lt;math&gt;\{p_n\}_{n=0}^\infty&lt;/math&gt; is binomial if and only if its basis operator is a [[Delta operator]].&lt;ref&gt;{{cite book |last1=Aigner |first1=Martin |title=Combinatorial Theory |orig-year=Reprint of the 1979 Edition |date=1997 |publisher=Springer |isbn=3-540-61787-6 |page=105}}&lt;/ref&gt; Writing &lt;math&gt; E^a &lt;/math&gt; for the shift by &lt;math&gt; a &lt;/math&gt; operator, the Delta operators corresponding to the above "Pochhammer" families of polynomials are the backward difference &lt;math&gt; I - E^{-c} &lt;/math&gt; for &lt;math&gt; c&gt;0 &lt;/math&gt;, the ordinary derivative for &lt;math&gt; c=0 &lt;/math&gt;, and the forward difference &lt;math&gt; E^{-c} - I &lt;/math&gt; for &lt;math&gt; c&lt;0 &lt;/math&gt;.

=== Multinomial theorem ===
{{Main|Multinomial theorem}}
The binomial theorem can be generalized to include powers of sums with more than two terms. The general version is

:&lt;math&gt;(x_1 + x_2 + \cdots + x_m)^n
 = \sum_{k_1+k_2+\cdots +k_m = n} {n \choose k_1, k_2, \ldots, k_m}
 x_1^{k_1} x_2^{k_2} \cdots x_m^{k_m}. &lt;/math&gt;

where the summation is taken over all sequences of nonnegative integer indices ''k''&lt;sub&gt;1&lt;/sub&gt; through ''k''&lt;sub&gt;''m''&lt;/sub&gt; such that the sum of all ''k''&lt;sub&gt;''i''&lt;/sub&gt; is&amp;nbsp;''n''. (For each term in the expansion, the exponents must add up to&amp;nbsp;''n''). The coefficients &lt;math&gt; \tbinom n{k_1,\cdots,k_m} &lt;/math&gt; are known as multinomial coefficients, and can be computed by the formula

:&lt;math&gt; {n \choose k_1, k_2, \ldots, k_m}
 = \frac{n!}{k_1! \cdot k_2! \cdots k_m!}.&lt;/math&gt;

Combinatorially, the multinomial coefficient &lt;math&gt;\tbinom n{k_1,\cdots,k_m}&lt;/math&gt; counts the number of different ways to [[Partition of a set|partition]] an ''n''-element set into [[Disjoint sets|disjoint]] [[subset]]s of sizes ''k''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;...,&amp;nbsp;''k''&lt;sub&gt;''m''&lt;/sub&gt;.

=== {{anchor|multi-binomial}} Multi-binomial theorem ===
It is often useful when working in more dimensions, to deal with products of binomial expressions. By the binomial theorem this is equal to

:&lt;math&gt; (x_{1}+y_{1})^{n_{1}}\dotsm(x_{d}+y_{d})^{n_{d}} = \sum_{k_{1}=0}^{n_{1}}\dotsm\sum_{k_{d}=0}^{n_{d}} \binom{n_{1}}{k_{1}}\, x_{1}^{k_{1}}y_{1}^{n_{1}-k_{1}}\;\dotsc\;\binom{n_{d}}{k_{d}}\, x_{d}^{k_{d}}y_{d}^{n_{d}-k_{d}}. &lt;/math&gt;

This may be written more concisely, by [[multi-index notation]], as

:&lt;math&gt; (x+y)^\alpha = \sum_{\nu \le \alpha} \binom{\alpha}{\nu} x^\nu y^{\alpha - \nu}.&lt;/math&gt;

=== General Leibniz rule ===
{{Main|General Leibniz rule}}

The general Leibniz rule gives the {{mvar|n}}th derivative of a product of two functions in a form similar to that of the binomial theorem:&lt;ref&gt;{{cite book |last=Seely |first=Robert T. |title=Calculus of One and Several Variables |location=Glenview |publisher=Scott, Foresman |year=1973 |isbn=978-0-673-07779-0 }}&lt;/ref&gt;

:&lt;math&gt;(fg)^{(n)}(x) = \sum_{k=0}^n \binom{n}{k} f^{(n-k)}(x) g^{(k)}(x).&lt;/math&gt;

Here, the superscript {{math|(''n'')}} indicates the {{mvar|n}}th derivative of a function.  If one sets {{math|1=''f''(''x'') = ''e''{{sup|''ax''}}}} and {{math|1=''g''(''x'') = ''e''{{sup|''bx''}}}}, and then cancels the common factor of {{math|''e''{{sup|(''a'' + ''b'')''x''}}}} from both sides of the result, the ordinary binomial theorem is recovered.

== Applications ==

=== Multiple-angle identities ===
For the [[complex numbers]] the binomial theorem can be combined with [[De Moivre's formula]] to yield [[List of trigonometric identities#Multiple-angle formulae|multiple-angle formulas]] for the [[sine]] and [[cosine]]. According to De Moivre's formula,
:&lt;math&gt;\cos\left(nx\right)+i\sin\left(nx\right) = \left(\cos x+i\sin x\right)^n.&lt;/math&gt;

Using the binomial theorem, the expression on the right can be expanded, and then the real and imaginary parts can be taken to yield formulas for cos(''nx'') and sin(''nx''). For example, since
:&lt;math&gt;\left(\cos x+i\sin x\right)^2 = \cos^2 x + 2i \cos x \sin x - \sin^2 x,&lt;/math&gt;
De Moivre's formula tells us that
:&lt;math&gt;\cos(2x) = \cos^2 x - \sin^2 x \quad\text{and}\quad\sin(2x) = 2 \cos x \sin x,&lt;/math&gt;
which are the usual double-angle identities. Similarly, since
:&lt;math&gt;\left(\cos x+i\sin x\right)^3 = \cos^3 x + 3i \cos^2 x \sin x - 3 \cos x \sin^2 x - i \sin^3 x,&lt;/math&gt;
De Moivre's formula yields
:&lt;math&gt;\cos(3x) = \cos^3 x - 3 \cos x \sin^2 x \quad\text{and}\quad \sin(3x) = 3\cos^2 x \sin x - \sin^3 x.&lt;/math&gt;
In general,
:&lt;math&gt;\cos(nx) = \sum_{k\text{ even}} (-1)^{k/2} {n \choose k}\cos^{n-k} x \sin^k x&lt;/math&gt;
and
:&lt;math&gt;\sin(nx) = \sum_{k\text{ odd}} (-1)^{(k-1)/2} {n \choose k}\cos^{n-k} x \sin^k x.&lt;/math&gt;

=== Series for e ===
The [[e (mathematical constant)|number ''e'']] is often defined by the formula

:&lt;math&gt;e = \lim_{n\to\infty} \left(1 + \frac{1}{n}\right)^n.&lt;/math&gt;

Applying the binomial theorem to this expression yields the usual [[infinite series]] for ''e''. In particular:

:&lt;math&gt;\left(1 + \frac{1}{n}\right)^n = 1 + {n \choose 1}\frac{1}{n} + {n \choose 2}\frac{1}{n^2} + {n \choose 3}\frac{1}{n^3} + \cdots + {n \choose n}\frac{1}{n^n}.&lt;/math&gt;

The ''k''th term of this sum is

:&lt;math&gt;{n \choose k}\frac{1}{n^k} = \frac{1}{k!}\cdot\frac{n(n-1)(n-2)\cdots (n-k+1)}{n^k}&lt;/math&gt;

As ''n''&amp;nbsp;→&amp;nbsp;∞, the rational expression on the right approaches one, and therefore

:&lt;math&gt;\lim_{n\to\infty} {n \choose k}\frac{1}{n^k} = \frac{1}{k!}.&lt;/math&gt;

This indicates that ''e'' can be written as a series:

:&lt;math&gt;e=\sum_{k=0}^\infty\frac{1}{k!}=\frac{1}{0!} + \frac{1}{1!} + \frac{1}{2!} + \frac{1}{3!} + \cdots.&lt;/math&gt;

Indeed, since each term of the binomial expansion is an [[Monotonic function|increasing function]] of ''n'', it follows from the [[monotone convergence theorem]] for series that the sum of this infinite series is equal to&amp;nbsp;''e''.

=== Probability ===
The binomial theorem is closely related to the probability mass function of the [[negative binomial distribution]]. The probability of a (countable) collection of independent Bernoulli trials &lt;math&gt;\{X_t\}_{t\in S}&lt;/math&gt; with probability of success &lt;math&gt;p\in [0,1]&lt;/math&gt; all not happening is  &lt;math display="block"&gt; P\left(\bigcap_{t\in S} X_t^C\right) = (1-p)^{|S|} = \sum_{n=0}^{|S|} {|S| \choose n} (-p)^n&lt;/math&gt;A useful upper bound for this quantity is &lt;math&gt; e^{-pn}&lt;/math&gt;. &lt;ref&gt;{{Cite book|title=Data Compression|last=Cover|first=Thomas M.|last2=Thomas|first2=Joy A.|date=2001-01-01|publisher=John Wiley &amp; Sons, Inc.|isbn=9780471200611|pages=320|language=en|doi=10.1002/0471200611.ch5}}&lt;/ref&gt;

== The binomial theorem in abstract algebra ==

Formula (1) is valid more generally for any elements ''x'' and ''y'' of a [[semiring]] satisfying ''xy''&amp;nbsp;=&amp;nbsp;''yx''. The [[theorem]] is true even more generally: [[alternativity]] suffices in place of [[associativity]].

The binomial theorem can be stated by saying that the [[polynomial sequence]] {&amp;nbsp;1,&amp;nbsp;''x'',&amp;nbsp;''x''&lt;sup&gt;2&lt;/sup&gt;,&amp;nbsp;''x''&lt;sup&gt;3&lt;/sup&gt;,&amp;nbsp;...&amp;nbsp;} is of [[binomial type]].

== In popular culture ==
* The binomial theorem is mentioned in the [[Major-General's Song]] in the comic opera [[The Pirates of Penzance]].
* [[Professor Moriarty]] is described by Sherlock Holmes as having written [[A Treatise on the Binomial Theorem|a treatise on the binomial theorem]].
* The Portuguese poet [[Fernando Pessoa]], using the heteronym [[Álvaro de Campos]], wrote that "Newton's Binomial is as beautiful as the [[Venus de Milo]]. The truth is that few people notice it."&lt;ref&gt;{{cite web|url=http://arquivopessoa.net/textos/224|title=Arquivo Pessoa: Obra Édita - O binómio de Newton é tão belo como a Vénus de Milo.|publisher=arquivopessoa.net}}&lt;/ref&gt;
* In the 2014 film [[The Imitation Game]], Alan Turing makes reference to Isaac Newton's work on the Binomial Theorem during his first meeting with Commander Denniston at Bletchley Park.

== See also ==
{{portal|Mathematics}}
* [[Binomial approximation]]
* [[Binomial distribution]]
* [[Binomial inverse theorem]]
* [[Stirling's approximation]]

== Notes ==
{{reflist|group=Note}}

== References ==
{{reflist|30em}}

== Further reading ==
* {{cite journal|last=Bag|first=Amulya Kumar|year=1966|title=Binomial theorem in ancient India|journal=Indian J. History Sci|volume=1|issue=1|pages=68–74}}
* {{cite book|last1=Graham|first1=Ronald|first2=Donald |last2=Knuth|first3= Oren|last3= Patashnik|title=Concrete Mathematics|publisher=Addison Wesley|year=1994|edition=2nd|pages=153–256|chapter=(5) Binomial Coefficients|isbn=978-0-201-55802-9|oclc=17649857}}

== External links ==
{{Wikibooks|Combinatorics|Binomial Theorem|The Binomial Theorem}}
* {{SpringerEOM|id=Newton_binomial|first=E.D.|last= Solomentsev|title=Newton binomial}}
* [http://demonstrations.wolfram.com/BinomialTheorem/ Binomial Theorem] by [[Stephen Wolfram]], and [http://demonstrations.wolfram.com/BinomialTheoremStepByStep/ "Binomial Theorem (Step-by-Step)"] by Bruce Colletti and Jeff Bryant, [[Wolfram Demonstrations Project]], 2007.

{{PlanetMath attribution|id=338|title=inductive proof of binomial theorem}}

{{Authority control}}

[[Category:Factorial and binomial topics]]
[[Category:Theorems in algebra]]
[[Category:Articles containing proofs]]</text>
      <sha1>4q174m3m98lfte0ufv24rqpedyghz7v</sha1>
    </revision>
  </page>
  <page>
    <title>Byte</title>
    <ns>0</ns>
    <id>3365</id>
    <revision>
      <id>870401529</id>
      <parentid>870241165</parentid>
      <timestamp>2018-11-24T15:22:07Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 2 sources and tagging 0 as dead. #IABot (v2.0beta10)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="38283">{{hatnote|This article is about the unit of information. For other uses, see [[Byte (disambiguation)]].}}
{{Infobox unit
| name         = byte
| standard     = [[Units of information#Units derived from bit|units derived from bit]]
| quantity     = digital information, data size
| symbol       = B
| symbol2      = (when referring to exactly 8&amp;nbsp;bits) [[octet (computing)|o]]
}}
{{anchor|4 bit|6 bit|8 bit|9 bit}}The '''byte''' is a [[units of information|unit of digital information]] that most commonly consists of eight [[bit]]s, representing a [[binary number]]. Historically, the byte was the number of bits used to encode a single [[character (computing)|character]] of text in a computer&lt;ref name="Buchholz_1962"/&gt;&lt;ref name="Bemer_1959"/&gt; and for this reason it is the smallest [[address space|addressable]] unit of [[Computer memory|memory]] in many [[computer architecture]]s.

The size of the byte has historically been hardware dependent and no definitive standards existed that mandated the size – byte-sizes from 1&lt;ref name="Buchholz_1956_1"/&gt; to 48 bits&lt;ref name="CDC_1965_3600"/&gt; are known to have been used in the past.&lt;ref name="Rao_1989"/&gt;&lt;ref name="Tafel_1971"/&gt; Early character encoding systems often used [[Six-bit character code|six bits]], and machines using six-bit and nine-bit bytes were common into the 1960s. These machines most commonly had [[Word (computer architecture)|memory words]] of 12, 24, 36, 48 or 60 bits, corresponding to two, four, six, eight or 10 six-bit bytes. In this era, bytes in the instruction stream were often referred to as ''[[Syllable (computing)|syllables]]'', before the term byte became common.

The modern [[de facto standard|''de-facto'' standard]] of eight bits, as documented in ISO/IEC 2382-1:1993, is a convenient [[power of two]] permitting the values 0 through 255 for one byte (2 in power of 8 = 256, where zero signifies number as well).&lt;ref name="ISO_IEC_2382-1_1993"/&gt; The international standard [[IEC 80000-13]] codified this common meaning. Many types of applications use information representable in eight or fewer bits and processor designers optimize for this common usage. The popularity of major commercial computing architectures has aided in the ubiquitous acceptance of the eight-bit size.&lt;ref name="CHM_1964"/&gt; Modern architectures typically use 32- or 64-bit words, built of four or eight bytes.

The unit symbol for the byte was designated as the upper-case letter ''B'' by the [[International Electrotechnical Commission]] (IEC) and [[Institute of Electrical and Electronics Engineers]] (IEEE)&lt;ref name="MIXF"/&gt; in contrast to the bit, whose IEEE symbol is a lower-case ''b''. Internationally, the unit ''[[Octet (computing)|octet]]'', symbol ''o'', explicitly denotes a sequence of eight bits, eliminating the ambiguity of the byte.&lt;ref name="TCPIP"/&gt;&lt;ref name="ISO_2382-4"/&gt;

==History==
The term ''byte'' was coined by [[Werner Buchholz]] in June 1956,&lt;ref name="Buchholz_1956_1"/&gt;&lt;ref name="Buchholz_1977"/&gt;&lt;ref name="Timeline_1956"/&gt;{{efn|{{anchor|Note-Dates}}Many sources erroneously indicate a birthday of the term ''byte'' in July 1956, but [[Werner Buchholz]] claimed that the term would have been coined in [[#Buchholz-1977|June 1956]]. In fact, the [[#Buchholz-1956-1|earliest document]] supporting this dates from 1956-06-11. Buchholz stated that the transition to 8-bit bytes was conceived in [[#Buchholz-1977|August 1956]], but the earliest document found using this notion dates from [[#Buchholz-1956-3|September 1956]].}} during the early design phase for the [[IBM 7030|IBM Stretch]]&lt;ref name="Buchholz_1956_2"/&gt;&lt;ref name="Buchholz_1956_3"/&gt;&lt;ref name="Buchholz_1962"/&gt;&lt;ref name="Buchholz_1977"/&gt;&lt;ref name="Timeline_1956"/&gt;&lt;ref name="ESR"/&gt;&lt;ref name="Bemer_2000"/&gt; computer, which had addressing to the bit and [[variable field length]] (VFL) instructions with a byte size encoded in the instruction.&lt;ref name="Buchholz_1977"/&gt;
It is a deliberate respelling of ''[[bite]]'' to avoid accidental mutation to ''bit''.&lt;ref name="Buchholz_1962"/&gt;&lt;ref name="Buchholz_1977"/&gt;&lt;ref name="Blaauw_1959"/&gt;

Another origin of ''byte'' for bit groups smaller than a machine's word size (and in particular groups of [[Nibble (computing)|four bits]]) is on record by Louis G. Dooley, who claimed he coined the term while working with [[Jules Schwartz]] and Dick Beeler on an air defense system called [[Experimental SAGE Subsector|SAGE]] at [[MIT Lincoln Laboratory]] in ca. 1956/1957, which was jointly developed by [[Rand Corporation|Rand]], MIT, and IBM.&lt;ref name="Dooley_1995_Byte"/&gt;&lt;ref name="Ram_Byte"/&gt; Later on, Schwartz's language [[JOVIAL]] actually used the term, but he recalled vaguely that it was derived from [[AN/FSQ-31]].&lt;ref name="Schwartz_Brooks_ACM"/&gt;&lt;ref name="Ram_Byte"/&gt;

Early computers used a variety of four-bit [[binary coded decimal]] (BCD) representations and the [[Sixbit|six-bit]] codes for printable graphic patterns common in the [[U.S. Army]] ([[FIELDATA]]) and [[United States Navy|Navy]]. These representations included alphanumeric characters and special graphical symbols. These sets were expanded in 1963 to seven bits of coding, called the [[American Standard Code for Information Interchange]] (ASCII) as the [[Federal Information Processing Standard]], which replaced the incompatible teleprinter codes in use by different branches of the U.S. government and universities during the 1960s. ASCII included the distinction of upper- and lowercase alphabets and a set of [[control character]]s to facilitate the transmission of written language as well as printing device functions, such as page advance and line feed, and the physical or logical control of data flow over the transmission media.&lt;ref name="Bemer_2000"/&gt; During the early 1960s, while also active in ASCII standardization, IBM simultaneously introduced in its product line of [[System/360]] the eight-bit [[Extended Binary Coded Decimal Interchange Code]] (EBCDIC), an expansion of their [[six-bit binary-coded decimal]] (BCDIC) representation used in earlier card punches.&lt;ref name="ibmebcdic"/&gt;
The prominence of the System/360 led to the ubiquitous adoption of the eight-bit storage size,&lt;ref name="Bemer_2000"/&gt;&lt;ref name="Buchholz_1956_3"/&gt;&lt;ref name="Buchholz_1977"/&gt; while in detail the EBCDIC and ASCII encoding schemes are different.

In the early 1960s, [[AT&amp;T]] introduced [[digital telephony]] first on long-distance [[trunk line]]s. These used the eight-bit [[µ-law algorithm|µ-law encoding]]. This large investment promised to reduce transmission costs for eight-bit data.

The development of [[eight-bit]] [[microprocessor]]s in the 1970s popularized this storage size. Microprocessors such as the [[Intel 8008]], the direct predecessor of the [[Intel 8080|8080]] and the [[Intel 8086|8086]], used in early personal computers, could also perform a small number of operations on the [[4bit|four-bit]] pairs in a byte, such as the decimal-add-adjust (DAA) instruction. A four-bit quantity is often called a [[nibble]], also ''nybble'', which is conveniently represented by a single [[hexadecimal]] digit.

The term ''[[Octet (computing)|octet]]'' is used to unambiguously specify a size of eight bits.&lt;ref name="Bemer_2000"/&gt;&lt;ref name="ISO_2382-4"/&gt; It is used extensively in [[Protocol (computing)|protocol]] definitions.

Historically, the term ''octad'' or ''octade'' was used to denote eight bits as well at least in Western Europe;&lt;ref name="Williams_1969"/&gt;&lt;ref name="Philips_1971"/&gt; however, this usage is no longer common. The exact origin of the term is unclear, but it can be found in British, Dutch, and German sources of the 1960s and 1970s, and throughout the documentation of [[Philips]] mainframe computers.

==Unit symbol==
{{Bit and byte prefixes}}
The unit symbol for the byte is specified in [[IEC 80000-13]], [[IEEE 1541]] and the Metric Interchange Format&lt;ref name="MIXF"/&gt; as the upper-case character ''B''. In contrast, IEEE 1541 specifies the lower case character ''b'' as the symbol for the [[bit]], but IEC 80000-13 and Metric-Interchange-Format specify the symbol as ''bit'', providing disambiguation from B for byte.

In the [[International System of Quantities]] (ISQ), B is the symbol of the ''[[bel (acoustics)|bel]]'', a unit of logarithmic power ratios named after [[Alexander Graham Bell]], creating a conflict with the IEC specification. However, little danger of confusion exists, because the bel is a rarely used unit. It is used primarily in its decadic fraction, the [[decibel]] (dB), for [[signal strength]] and [[sound pressure level]] measurements, while a unit for one tenth of a byte, the decibyte, and other fractions, are only used in derived units, such as transmission rates.

The lowercase letter o for [[Octet (computing)|octet]] is defined as the symbol for octet in IEC 80000-13 and is commonly used in languages such as [[French language|French]]&lt;ref name="IEC_Binary"/&gt; and [[Romanian language|Romanian]], and is also combined with metric prefixes for multiples, for example ko and Mo.

The usage of the term ''octad(e)'' for eight bits is no longer common.&lt;ref name="Williams_1969"/&gt;&lt;ref name="Philips_1971"/&gt;

==Unit multiples==
[[File:Binaryvdecimal.svg|thumb|right|275px|Percentage difference between decimal and binary interpretations of the unit prefixes grows with increasing storage size]]

Despite standardization efforts, ambiguity still exists in the meanings of the [[SI prefix|SI (or metric) prefixes]] used with the unit byte, especially concerning the prefixes ''kilo'' (k or K), ''mega'' (M), and ''giga'' (G). Computer memory has a binary architecture in which multiples are expressed in [[power of two|powers of 2]]. In some fields of the software and computer hardware industries a [[binary prefix]] is used for bytes and bits, while producers of computer storage devices practice adherence to decimal SI multiples. For example, a computer disk drive capacity of 100&amp;nbsp;gigabytes is specified when the disk contains 100&amp;nbsp;billion bytes (93&amp;nbsp;gibibytes) of storage space.

While the numerical difference between the decimal and binary interpretations is relatively small for the prefixes [[Kilo-|kilo]] and [[Mega-|mega]], it grows to over 20% for prefix [[yotta]]. The linear-log graph at right illustrates the difference versus storage size up to an [[exa]]byte.

==Common uses==
Many [[programming language]]s defined the [[data type]] ''byte''.

The [[C (programming language)|C]] and [[C++]] programming languages define ''byte'' as an "''addressable unit of data storage large enough to hold any member of the basic character set of the execution environment''" (clause 3.6 of the C standard). The C standard requires that the integral data type ''unsigned char'' must hold at least 256 different values, and is represented by at least eight bits (clause 5.2.4.2.1). Various implementations of C and C++ reserve 8, 9, 16, 32, or 36 bits for the storage of a byte.&lt;ref name="Cline_Bytes"/&gt;&lt;ref name="Klein_2008"/&gt;{{efn|The actual number of bits in a particular implementation is documented as &lt;code&gt;CHAR_BIT&lt;/code&gt; as implemented in the file [[limits.h]].}} In addition, the C and C++ standards require that there are no "gaps" between two bytes. This means every bit in memory is part of a byte.&lt;ref name="Cline_FAQ"/&gt;

[[Java (programming language)|Java's]] primitive &lt;code&gt;byte&lt;/code&gt; data type is always defined as consisting of 8 bits and being a signed data type, holding values from −128 to 127.

.NET programming languages, such as C#, define both an unsigned &lt;code&gt;byte&lt;/code&gt; and a signed &lt;code&gt;sbyte&lt;/code&gt;, holding values from 0 to 255, and −128 to 127, respectively.

In data transmission systems, the byte is defined as a contiguous sequence of bits in a serial data stream representing the smallest distinguished unit of data. A transmission unit might include start bits, stop bits, or [[parity bit]]s, and thus could vary from 7 to 12 bits to contain a single 7-bit [[ASCII]] code.&lt;ref name="NWU"/&gt;

==See also==
* [[data]]
* [[Data hierarchy]]
* [[JBOB]], Just a Bunch Of Bytes
* [[Nibble]]
* [[Primitive data type]]
* [[Tryte]]
* [[Qubyte]] (quantum byte)
* [[Word (computer architecture)]]
* [[Octet (computing)]]

==Notes==
{{notelist}}

==References==
{{reflist|refs=
&lt;ref name="Buchholz_1956_1"&gt;{{anchor|Buchholz-1956-1}}{{cite book |title=The Link System |chapter=7. The Shift Matrix |author-first=Werner |author-last=Buchholz |author-link=Werner Buchholz |date=1956-06-11 |id=[[IBM Stretch|Stretch]] Memo No. 39G |publisher=[[IBM]] |pages=5–6 |url=http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/06-07/102632284.pdf |access-date=2016-04-04 |dead-url=no |archive-url=https://web.archive.org/web/20170404152534/http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/06-07/102632284.pdf |archive-date=2017-04-04 |quote=[…] Most important, from the point of view of editing, will be the ability to handle any characters or digits, from 1 to 6 bits long.&lt;br /&gt;Figure 2 shows the Shift Matrix to be used to convert a 60-bit [[word (computer architecture)|word]], coming from Memory in parallel, into [[character (computing)|characters]], or 'bytes' as we have called them, to be sent to the [[serial adder|Adder]] serially. The 60 bits are dumped into [[magnetic core]]s on six different levels. Thus, if a 1 comes out of position 9, it appears in all six cores underneath. Pulsing any diagonal line will send the six bits stored along that line to the Adder. The Adder may accept all or only some of the bits.&lt;br /&gt;Assume that it is desired to operate on 4 bit [[decimal digit]]s, starting at the right. The 0-diagonal is pulsed first, sending out the six bits 0 to 5, of which the Adder accepts only the first four (0–3). Bits 4 and 5 are ignored. Next, the 4 diagonal is pulsed. This sends out bits 4 to 9, of which the last two are again ignored, and so on.&lt;br /&gt;It is just as easy to use all six bits in [[alphanumeric]] work, or to handle bytes of only one bit for logical analysis, or to offset the bytes by any number of bits. All this can be done by pulling the appropriate shift diagonals. An analogous matrix arrangement is used to change from serial to parallel operation at the output of the adder. […]}}&lt;/ref&gt;
&lt;ref name="Buchholz_1956_2"&gt;{{cite book |title=Memory Word Length |chapter=5. Input-Output |author-first=Werner |author-last=Buchholz |author-link=Werner Buchholz |date=1956-07-31 |id=[[IBM Stretch|Stretch]] Memo No. 40 |publisher=[[IBM]] |page=2 |url=http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/06-08/102632289.pdf |access-date=2016-04-04 |dead-url=no |archive-url=https://web.archive.org/web/20170404160423/http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/06-08/102632289.pdf |archive-date=2017-04-04 |quote=[…] 60 is a multiple of 1, 2, 3, 4, 5, and 6. Hence bytes of length from 1 to 6 bits can be packed efficiently into a 60-bit [[word (computer architecture)|word]] without having to split a byte between one word and the next. If longer bytes were needed, 60 bits would, of course, no longer be ideal. With present applications, 1, 4, and 6 bits are the really important cases.&lt;br /&gt;With 64-bit words, it would often be necessary to make some compromises, such as leaving 4 bits unused in a word when dealing with 6-bit bytes at the input and output. However, the LINK Computer can be equipped to edit out these gaps and to permit handling of bytes which are split between words. […]}}&lt;/ref&gt;
&lt;ref name="Buchholz_1956_3"&gt;{{anchor|Buchholz-1956-3}}{{cite book |title=Memory Word Length and Indexing |chapter=2. Input-Output Byte Size |author-first=Werner |author-last=Buchholz |author-link=Werner Buchholz |date=1956-09-19 |id=[[IBM Stretch|Stretch]] Memo No. 45 |publisher=[[IBM]] |page=1 |url=http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/06-08/102632292.pdf |access-date=2016-04-04 |dead-url=no |archive-url=https://web.archive.org/web/20170404161611/http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/06-08/102632292.pdf |archive-date=2017-04-04 |quote=[…] The maximum input-output byte size for serial operation will now be 8 bits, not counting any error detection and correction bits. Thus, the Exchange will operate on an 8-bit byte basis, and any input-output units with less than 8 bits per byte will leave the remaining bits blank. The resultant gaps can be edited out later by programming […]}}&lt;/ref&gt;
&lt;ref name="Buchholz_1962"&gt;{{anchor|Buchholz-1962}}{{cite |title=Planning a Computer System – Project Stretch |author-first1=Gerrit Anne |author-last1=Blaauw |author-link1=Gerrit Anne Blaauw |author-first2=Frederick Phillips |author-last2=Brooks, Jr. |author-link2=Frederick Phillips Brooks, Jr. |author-first3=Werner |author-last3=Buchholz |author-link3=Werner Buchholz |editor-first=Werner |editor-last=Buchholz |editor-link=Werner Buchholz |publisher=[[McGraw-Hill Book Company, Inc.]] / The Maple Press Company, York, PA. |lccn=61-10466 |year=1962 |chapter=4: Natural Data Units |format=PDF |pages=39–40 |url=http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/Buchholz_102636426.pdf |access-date=2017-04-03 |dead-url=no |archive-url=https://web.archive.org/web/20170403014651/http://archive.computerhistory.org/resources/text/IBM/Stretch/pdfs/Buchholz_102636426.pdf |archive-date=2017-04-03 |quote=[…] Terms used here to describe the structure imposed by the machine design, in addition to ''[[bit]]'', are listed below.&lt;br /&gt;''Byte'' denotes a group of bits used to encode a character, or the number of bits transmitted in parallel to and from input-output units. A term other than ''[[character (computing)|character]]'' is used here because a given character may be represented in different applications by more than one code, and different codes may use different numbers of bits (i.e., different byte sizes). In input-output transmission the grouping of bits may be completely arbitrary and have no relation to actual characters. (The term is coined from ''[[bite]]'', but respelled to avoid accidental mutation to ''bit''.)&lt;br /&gt;A ''[[Word (unit)|word]]'' consists of the number of data bits transmitted in parallel from or to memory in one memory cycle. [[Word size]] is thus defined as a structural property of the memory. (The term ''[[catena (unit)|catena]]'' was coined for this purpose by the designers of the [[Groupe Bull|Bull]] {{ill|Bull Gamma 60{{!}}GAMMA 60|fr|Gamma 60}} computer.)&lt;br /&gt;''[[Block (data storage)|Block]]'' refers to the number of words transmitted to or from an input-output unit in response to a single input-output instruction. Block size is a structural property of an input-output unit; it may have been fixed by the design or left to be varied by the program. […]}}&lt;/ref&gt;
&lt;ref name="Bemer_1959"&gt;{{cite |author-first=Robert William |author-last=Bemer |author-link=Robert William Bemer |title=A proposal for a generalized card code of 256 characters |journal=[[Communications of the ACM]] |volume=2 |number=9 |pages=19–23 |year=1959 |doi=10.1145/368424.368435}}&lt;/ref&gt;
&lt;ref name="CHM_1964"&gt;{{cite web |title=Computer History Museum – Exhibits – Internet History – 1964: Internet History 1962 to 1992 |publisher=[[Computer History Museum]] |date=2017 |orig-year=2015 |url=http://www.computerhistory.org/internet_history/#1964 |access-date=2017-04-03 |dead-url=no |archive-url=https://web.archive.org/web/20170403115211/http://www.computerhistory.org/internethistory/ |archive-date=2017-04-03}}&lt;/ref&gt;
&lt;ref name="MIXF"&gt;{{cite web |title=Metric-Interchange-Format |author-first=Aubrey |author-last=Jaffer |author-link=Aubrey Jaffer |date=2011 |orig-year=2008 |url=http://people.csail.mit.edu/jaffer/MIXF |access-date=2017-04-03 |dead-url=no |archive-url=https://web.archive.org/web/20170403121705/https://people.csail.mit.edu/jaffer/MIXF/ |archive-date=2017-04-03}}&lt;/ref&gt;
&lt;ref name="TCPIP"&gt;{{cite web |title=The TCP/IP Guide – Binary Information and Representation: Bits, Bytes, Nibbles, Octets and Characters – Byte versus Octet |version=3.0 |date=2005-09-20 |orig-year=2001 |author-first=Charles M. |author-last=Kozierok |url=http://www.tcpipguide.com/free/t_BinaryInformationandRepresentationBitsBytesNibbles-3.htm |access-date=2017-04-03 |dead-url=no |archive-url=https://web.archive.org/web/20170403122042/http://www.tcpipguide.com/free/t_BinaryInformationandRepresentationBitsBytesNibbles-3.htm |archive-date=2017-04-03}}&lt;/ref&gt;
&lt;ref name="Timeline_1956"&gt;{{cite web |title=Timeline of the IBM Stretch/Harvest era (1956–1961) |publisher=[[Computer History Museum]] |date=June 1956 |url=http://archive.computerhistory.org/resources/text/IBM/Stretch/102636400.txt |access-date=2017-04-03 |dead-url=no |archive-url=https://web.archive.org/web/20160429212717/http://archive.computerhistory.org/resources/text/IBM/Stretch/102636400.txt |archive-date=2016-04-29 |quote=1956 Summer: [[Gerrit Blaauw]], [[Fred Brooks]], [[Werner Buchholz]], [[John Cocke]] and Jim Pomerene join the [[IBM Stretch|Stretch]] team. Lloyd Hunter provides [[transistor]] leadership.&lt;br /&gt;{{sic|1956 July|expected=1956 June}}: In a report Werner Buchholz lists the advantages of a 64-bit word length for Stretch. It also supports [[NSA]]'s requirement for 8-bit bytes. Werner's term "Byte" first popularized in this memo.}} (NB. This timeline erroneously specifies the birth date of the term "byte" as ''[[#Note-Dates|July 1956]]'', while Buchholz actually used the term as early as ''[[#Buchholz-1956-1|June 1956]]''.)&lt;/ref&gt;
&lt;ref name="ESR"&gt;{{cite web |title=byte definition |author-first=Eric Steven |author-last=Raymond |author-link=Eric Steven Raymond |date=2017 |orig-year=2003 |url=http://catb.org/~esr/jargon/html/B/byte.html |access-date=2017-04-03 |dead-url=no |archive-url=https://web.archive.org/web/20170403120304/http://catb.org/~esr/jargon/html/B/byte.html |archive-date=2017-04-03}}&lt;/ref&gt;
&lt;ref name="Blaauw_1959"&gt;{{anchor|Blaauw-1959}}{{cite journal |title=Processing Data in Bits and Pieces |author-first1=Gerrit Anne |author-last1=Blaauw |author-link1=Gerrit Anne Blaauw |author-first2=Frederick Phillips |author-last2=Brooks, Jr. |author-link2=Frederick Phillips Brooks, Jr. |author-first3=Werner |author-last3=Buchholz |author-link3=Werner Buchholz |journal=[[IRE Transactions on Electronic Computers]] |date=June 1959 |page=121}}&lt;/ref&gt;
&lt;ref name="Buchholz_1977"&gt;{{anchor|Buchholz-1977}}{{cite journal |author-last=Buchholz |author-first=Werner |author-link=Werner Buchholz |title=The Word 'Byte' Comes of Age... |journal=[[Byte Magazine]] |date=February 1977 |volume=2 |issue=2 |page=144 |url=https://archive.org/stream/byte-magazine-1977-02/1977_02_BYTE_02-02_Usable_Systems#page/n145/mode/2up |quote=&lt;!-- The Word "Byte" Comes of Age...&lt;br /&gt;''We received the following from W Buchholz, one of the individuals who was working on IBM's Project Stretch in the mid 1950s. His letter tells the story.''&lt;br /&gt;Not being a regular reader of your magazine, I heard about the question in the November 1976 issue regarding the origin of the term "byte" from a colleague who knew that I had prepetrated this piece of jargon ''[see page 77 of November 1976 BYTE, "Olde Englishe"]''. I searched my files and could not locate a birth certificate. But I am sure that "byte" is coming of age in 1977 with its 21st birthday. Many have assumed that byte, meaning 8 bits, originated with the IBM System/360, which spread such bytes far and wide in the mid-1960s. The editor is correct in pointing out that the term goes back to the earlier Stretch computer (but incorrect in that Stretch was the first, not the last, of IBM's second-generation transistorized computers to be developed). --&gt;[…] The first reference found in the files was contained in an internal memo written in June 1956 during the early days of developing [[IBM Stretch|Stretch]]. A byte was described as consisting of any number of parallel bits from one to six. Thus a byte was assumed to have a length appropriate for the occasion. Its first use was in the context of the input-output equipment of the 1950s, which handled six bits at a time. The possibility of going to 8 bit bytes was considered in [[#Note-Dates|August 1956]] and incorporated in the design of Stretch [[#Buchholz-1956-3|shortly thereafter]]. The first published reference to the term occurred in 1959 in a paper '[[#Blaauw-1959|Processing Data in Bits and Pieces]]' by [[Gerrit Anne Blaauw|G&amp;nbsp;A&amp;nbsp;Blaauw]], [[Frederick Phillips Brooks, Jr.|F&amp;nbsp;P&amp;nbsp;Brooks&amp;nbsp;Jr]] and [[Werner Buchholz|W&amp;nbsp;Buchholz]] in the ''[[IRE Transactions on Electronic Computers]]'', June 1959, page 121. The notions of that paper were elaborated in Chapter 4 of ''[[#Buchholz-1962|Planning a Computer System (Project Stretch)]]'', edited by W&amp;nbsp;Buchholz, [[McGraw-Hill Book Company]] (1962). The rationale for coining the term was explained there on page 40 as follows:&lt;br /&gt;Byte ''denotes a group of bits used to encode a character, or the number of bits transmitted in parallel to and from input-output units. A term other than ''character'' is used here because a given character may be represented in different applications by more than one code, and different codes may use different numbers of bits (ie, different byte sizes). In input-output transmission the grouping of bits may be completely arbitrary and have no relation to actual characters. (The term is coined from ''[[bite]]'', but respelled to avoid accidental mutation to ''bit''.)''&lt;br /&gt;[[System/360]] took over many of the Stretch concepts, including the basic byte and word sizes, which are powers of 2. For economy, however, the byte size was fixed at the 8 bit maximum, and addressing at the bit level was replaced by byte addressing. […]&lt;!-- Since then the term byte has generally meant 8 bits, and it has thus passed into the general vocabulary. Are there any other terms coined especially for the computer field which have found their way into general dictionaries of English language?&lt;br /&gt;W. Buchholz&lt;br /&gt;24 Edge Hill Rd&lt;br /&gt;Wappingers Fall NY 12590 --&gt;}}&lt;/ref&gt;
&lt;ref name="Bemer_2000"&gt;{{cite web |title=Why is a byte 8 bits? Or is it? |author-first=Robert William |author-last=Bemer |author-link=Robert William Bemer |date=2000-08-08 |work=Computer History Vignettes |url=http://www.bobbemer.com/BYTE.HTM |access-date=2017-04-03 |dead-url=yes |archive-url=https://web.archive.org/web/20170403130829/http://www.bobbemer.com/BYTE.HTM# |archive-date=2017-04-03 |quote=[…] I came to work for [[IBM]], and saw all the confusion caused by the 64-character limitation. Especially when we started to think about word processing, which would require both upper and lower case. […]&lt;!-- Add 26 lower case letters to 47 existing, and one got 73 -- 9 more than 6 bits could represent. --&gt; I even made a proposal (in view of [[IBM Stretch|STRETCH]], the very first computer I know of with an 8-bit byte) that would extend the number of [[punch card]] character codes to 256 […].&lt;!-- [1]. Some folks took it seriously. I thought of it as a spoof. --&gt; So some folks started thinking about 7-bit characters, but this was ridiculous. With IBM's STRETCH computer as background, handling 64-character words divisible into groups of 8 (I designed the character set for it, under the guidance of Dr. [[Werner Buchholz]], the man who DID coin the term 'byte' for an 8-bit grouping). […]&lt;!-- [2] --&gt; It seemed reasonable to make a universal 8-bit character set, handling up to 256. In those days my mantra was 'powers of 2 are magic'. And so the group I headed developed and justified such a proposal […]&lt;!-- [3]. That was a little too much progress when presented to the standards group that was to formalize ASCII, so they stopped short for the moment with a 7-bit set, or else an 8-bit set with the upper half left for future work. --&gt; The [[IBM System 360|IBM 360]] used 8-bit characters, although not ASCII directly. Thus Buchholz's 'byte' caught on everywhere. I myself did not like the name for many reasons. The design had 8 bits moving around in parallel. But then came a new IBM part, with 9 bits for self-checking, both inside the CPU and in the [[tape drive]]s. I exposed this 9-bit byte to the press in 1973. But long before that, when I headed software operations for [[Cie. Bull]] in France in 1965–66, I insisted that 'byte' be deprecated in favor of '[[octet (computing)|octet]]'. […]&lt;!-- You can notice that my preference then is now the preferred term. --&gt; It is justified by new communications methods that can carry 16, 32, 64, and even 128 bits in parallel. But some foolish people now refer to a '16-bit byte' because of this parallel transfer, which is visible in the [[UNICODE]] set. I'm not sure, but maybe this should be called a '[[hextet]]'. […]&lt;!-- But you will notice that I am still correct. Powers of 2 are still magic! --&gt; |df= }}&lt;/ref&gt;
&lt;ref name="ibmebcdic"&gt;{{cite web |title=IBM confirms the use of EBCDIC in their mainframes as a default practice |year=2008 |publisher=[[IBM]] |url=http://publib.boulder.ibm.com/infocenter/zos/v1r9/index.jsp?topic=/com.ibm.zos.r9.adms700/adms7a05158.htm |access-date=2008-06-16 }} {{dead link |date=November 2016 |bot=InternetArchiveBot |fix-attempted=yes}}&lt;/ref&gt;
&lt;ref name="IEC_Binary"&gt;{{cite web |url=http://www.iec.ch/si/binary.htm |title=When is a kilobyte a kibibyte? And an MB an MiB? |work=The International System of Units and the IEC |publisher=[[International Electrotechnical Commission]] |access-date=2010-08-30}})&lt;/ref&gt;
&lt;ref name="Williams_1969"&gt;{{cite web |title=British Commercial Computer Digest: Pergamon Computer Data Series |author-first=R. H. |author-last=Williams |publisher=[[Pergamon Press]] |date=1969-01-01 |isbn=1483122107|id=978-1483122106 |url=https://www.amazon.de/British-Commercial-Computer-Digest-Pergamon/dp/1483122107 |access-date=2015-08-03}}&lt;/ref&gt;
&lt;ref name="Philips_1971"&gt;{{cite web |title=Philips – Philips Data Systems' product range – April 1971 |publisher=[[Philips]] |date=April 1971 |url=http://www.intact-reunies.nl/pdf/product1971.pdf |access-date=2015-08-03 |dead-url=yes |archive-url=https://web.archive.org/web/20160304072023/http://www.intact-reunies.nl/pdf/product1971.pdf |archive-date=2016-03-04}}&lt;/ref&gt;
&lt;ref name="Cline_Bytes"&gt;{{cite web |author-first=Marshall |author-last=Cline |url=https://isocpp.org/wiki/faq/intrinsic-types#very-large-bytes |title=I could imagine a machine with 9-bit bytes. But surely not 16-bit bytes or 32-bit bytes, right?}}&lt;/ref&gt;
&lt;ref name="Klein_2008"&gt;{{Citation |author-last=Klein |author-first=Jack |year=2008 |title=Integer Types in C and C++ |url=http://home.att.net/~jackklein/c/inttypes.html#char |archive-url=https://web.archive.org/web/20100327225121/http://home.att.net/~jackklein/c/inttypes.html#char |archive-date=2010-03-27 |access-date=2015-06-18}}&lt;/ref&gt;
&lt;ref name="Cline_FAQ"&gt;{{cite web |author-first=Marshall |author-last=Cline |url=https://isocpp.org/wiki/faq/intrinsic-types#bytes-review |title=C++ FAQ: the rules about bytes, chars, and characters}}&lt;/ref&gt;
&lt;ref name="NWU"&gt;{{cite web |publisher=Northwestern University |url=http://www.ece.northwestern.edu/local-apps/matlabhelp/techdoc/matlab_external/ch_seri8.html |title=External Interfaces/API}}&lt;/ref&gt;
&lt;ref name="CDC_1965_3600"&gt;{{cite book |title=3600 Computer System – Reference Manual |date=1966-10-11 |orig-year=1965 |version=K |publisher=[[Control Data Corporation]] (CDC) |location=St. Paul, Minnesota, USA |id=60021300 |url=http://bitsavers.org/pdf/cdc/3x00/48bit/60021300K_3600_SysRef_Oct66.pdf |access-date=2017-04-05 |dead-url=yes |archive-url=https://web.archive.org/web/20170405154001/http://bitsavers.informatik.uni-stuttgart.de/pdf/cdc/3x00/48bit/60021300K_3600_SysRef_Oct66.pdf# |archive-date=2017-04-05 |quote=Byte – A partition of a computer word. |df= }} (NB. Discusses 12-bit, 24-bit and 48-bit bytes.)&lt;/ref&gt;
&lt;ref name="Dooley_1995_Byte"&gt;{{cite journal |title=Byte: The Word |author-first=Louis G. |author-last=Dooley |date=February 1995 |journal=[[BYTE]] |location=Ocala, FL, USA |url=http://www.byte.com/art/9502/sec2/art12.htm |dead-url=yes |archive-url=https://web.archive.org/web/19961220122258/http://www.byte.com/art/9502/sec2/art12.htm |archive-date=1996-12-20 |quote=&lt;!-- I would like to get the following on record: --&gt;[…] The word byte was coined around 1956 to 1957 at [[MIT Lincoln Laboratory|MIT Lincoln Laboratories]] within a project called [[Experimental SAGE Subsector|SAGE]] (the North American Air Defense System), which was jointly developed by [[Rand Corporation|Rand]], Lincoln Labs, and [[IBM]]. In that era, computer memory structure was already defined in terms of [[word size (computing)|word size]]. A word consisted of x number of [[bit]]s; a bit represented a binary notational position in a word. Operations typically operated on all the bits in the full word.&lt;br /&gt;We coined the word byte to refer to a logical set of bits less than a full word size. At that time, it was not defined specifically as x bits but typically referred to as a set of [[Nibble (computing)|4 bits]], as that was the size of most of our coded data items. Shortly afterward, I went on to other responsibilities that removed me from SAGE. After having spent many years in Asia, I returned to the U.S. and was bemused to find out that the word byte was being used in the new microcomputer technology to refer to the basic addressable memory unit.}} (NB. According to his son, Dooley wrote to him:&lt;!-- See: https://en.wikipedia.org/wiki/Talk:Byte#Not_.22to_bite.22_but_.22a_bite.22 --&gt; "On good days, we would have the [[IBM XD-1|XD-1]] up and running and all the programs doing the right thing, and we then had some time to just sit and talk idly, as we waited for the computer to finish doing its thing. On one such occasion, I coined the word "byte", they ([[Jules Schwartz]] and Dick Beeler) liked it, and we began using it amongst ourselves. The origin of the word was a need for referencing only a part of the word length of the computer, but a part larger than just one bit...Many programs had to access just a specific [[Nibble (computing)|4-bit]] segment of the full word...I wanted a name for this smaller segment of the fuller word. The word "[[bit]]" lead to "[[bite]]" (meaningfully less than the whole), but for a unique spelling, "i" could be "y", and thus the word "byte" was born.")&lt;/ref&gt;
&lt;ref name="Schwartz_Brooks_ACM"&gt;{{cite |title=Origin of the term "byte", 1956 |url=http://www.xent.com/FoRK-archive/july99/0646.html |access-date=2017-04-10 |dead-url=no |archive-url=https://web.archive.org/web/20170410125522/http://www.xent.com/FoRK-archive/july99/0646.html |archive-date=2017-04-10 |quote=A question-and-answer session at an [[ACM conference]] on the history of programming languages included this exchange:&lt;br /&gt;[[John Goodenough|JOHN GOODENOUGH]]: You mentioned that the term "byte" is used in [[JOVIAL]]. Where did the term come from?&lt;br /&gt;[[Jules Schwartz|JULES SCHWARTZ]] (inventor of JOVIAL): As I recall, the [[AN/FSQ-31]], a totally different computer than the [[IBM 709|709]], was byte oriented. I don't recall for sure, but I'm reasonably certain the description of that computer included the word "byte," and we used it.&lt;br /&gt;[[Fred Brooks|FRED BROOKS]]: May I speak to that? [[Werner Buchholz]] coined the word as part of the definition of [[IBM STRETCH|STRETCH]], and the AN/FSQ-31 picked it up from STRETCH, but Werner is very definitely the author of that word.&lt;br /&gt;SCHWARTZ: That's right. Thank you.}}&lt;/ref&gt;
&lt;ref name="Ram_Byte"&gt;{{cite web |author-first=Stefan |author-last=Ram |title=Erklärung des Wortes "Byte" im Rahmen der Lehre binärer Codes |language=German |publisher=[[Freie Universität Berlin]] |location=Berlin, Germany |url=http://userpage.fu-berlin.de/~ram/pub/pub_jf47ht81Ht/code_byte_de |access-date=2017-04-10}}&lt;/ref&gt;
&lt;ref name="ISO_IEC_2382-1_1993"&gt;{{cite book |title=ISO/IEC 2382-1: 1993, Information technology – Vocabulary – Part 1: Fundamental terms |date=1993 |quote=byte&lt;br /&gt;A string that consists of a number of bits, treated as a unit, and usually representing a character or a part of a character.&lt;br /&gt;NOTES&lt;br /&gt;1 The number of bits in a byte is fixed for a given data processing system.&lt;br /&gt;2 The number of bits in a byte is usually 8.}}&lt;/ref&gt;
&lt;ref name="ISO_2382-4"&gt;{{cite book |title=ISO 2382-4, Organization of data |edition=2 |quote=byte, octet, 8-bit byte: A string that consists of eight bits.}}&lt;/ref&gt;
&lt;ref name="Rao_1989"&gt;{{cite book |title=Error-Control Coding for Computer Systems |author-first1=Thammavaram R. N. |author-last1=Rao |author-first2=Eiji |author-last2=Fujiwara |editor-first=Edward J. |editor-last=McCluskey |series=Prentice Hall Series in Computer Enginering |date=1989 |lccn=88-17892 |edition=1 |isbn=0-13-283953-9 |publisher=[[Prentice Hall]] |location=Englewood Cliffs, NJ, USA}} (NB. Example of the usage of a code for "4-bit bytes".)&lt;/ref&gt;
&lt;ref name="Tafel_1971"&gt;{{cite book |title=Einführung in die digitale Datenverarbeitung |language=German |trans-title=Introduction to digital information processing |author-first=Hans Jörg |author-last=Tafel |publisher=[[Carl Hanser Verlag]] |date=1971 |location=[[RWTH]], Aachen, Germany |publication-place=Munich, Germany |isbn=3-446-10569-7 |page=300 |quote=Byte = zusammengehörige Folge von i.a. neun Bits; davon sind acht Datenbits, das neunte ein Prüfbit}} (NB. Defines a byte as a group of typically 9 bits; 8 data bits plus 1 parity bit.)&lt;/ref&gt;
}}

==Further reading==
* {{cite book |title=Programming with the PDP-10 Instruction Set |series=PDP-10 System Reference Manual |date=August 1969 |volume=1 |publisher=[[Digital Equipment Corporation]] (DEC) |url=http://bitsavers.org/pdf/dec/pdp10/1970_PDP-10_Ref/1970PDP10Ref_Part1.pdf |access-date=2017-04-05 |dead-url=no |archive-url=https://web.archive.org/web/20170405154620/http://bitsavers.informatik.uni-stuttgart.de/pdf/dec/pdp10/1970_PDP-10_Ref/1970PDP10Ref_Part1.pdf |archive-date=2017-04-05}}

{{Computer Storage Volumes}}
{{Data types}}

[[Category:Data types]]
[[Category:Units of information]]
[[Category:Binary arithmetic]]
[[Category:Computer memory]]
[[Category:Data unit]]
[[Category:Primitive types]]
[[Category:Words coined in the 1950s]]
[[Category:8 (number)]]</text>
      <sha1>d1bx7jjra6qqvoroa4317pu5iov9h0k</sha1>
    </revision>
  </page>
  <page>
    <title>CS-BLAST</title>
    <ns>0</ns>
    <id>22405720</id>
    <revision>
      <id>867232501</id>
      <parentid>829612644</parentid>
      <timestamp>2018-11-04T14:17:39Z</timestamp>
      <contributor>
        <username>Nemo bis</username>
        <id>2584239</id>
      </contributor>
      <comment>Added free to read link in citations with [[WP:OABOT|OAbot]] #oabot</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15532">{{Infobox Software
| name                   = CS-BLAST
| developer              = Angermueller C, Biegert A, and Soeding J
| latest_release_version = 2.2.3
| latest_release_date    = {{release date|2013|12|07}}
| latest preview version = 1.1
| latest preview date    = {{Start date and age|2009|04|14}}
| programming language   = [[C++]]
| language               = English
| genre                  = [[Bioinformatics]] tool
| license                = [[GNU GPL| GNU GPL v3]]
| website                = http://wwwuser.gwdg.de/~compbiol/data/csblast/releases/, https://github.com/soedinglab/csblast
}}

'''CS-BLAST'''
&lt;ref name="csdis"&gt;{{Cite journal  
| last1 = Angermüller 
| first1 = C. 
| last2 = Biegert 
| first2 = A. 
| last3 = Söding 
| first3 = J. 
| title = Discriminative modelling of context-specific amino acid substitution probabilities. 
| journal = Bioinformatics 
| volume = 28 
| issue = 24 
| pages = 3240–7
|date=Dec 2012 
| doi = 10.1093/bioinformatics/bts622 
| PMID = 23080114 }}&lt;/ref&gt;
&lt;ref name="csgen"&gt;{{Cite journal  
| last1 = Biegert 
| first1 = A. 
| last2 = Söding 
| first2 = J. 
| title = Sequence context-specific profiles for homology searching. 
| journal = Proc Natl Acad Sci U S A 
| volume = 106 
| issue = 10 
| pages = 3770–5 
|date=Mar 2009 
| doi = 10.1073/pnas.0810767106 
| PMID = 19234132
| pmc=2645910| url = http://pubman.mpdl.mpg.de/pubman/item/escidoc:1944232/component/escidoc:1949958/1944232.pdf 
}}&lt;/ref&gt; 
&lt;ref name="betterseqSD"&gt;{{cite web
|url=https://www.sciencedaily.com/releases/2009/02/090223131125.htm
|title=Better Sequence Searches Of Genes And Proteins Devised|date=Mar 7, 2009
|publisher=ScienceDaily 
|accessdate=2009-08-14}}
&lt;/ref&gt; 
(Context-Specific BLAST) is a tool that searches a [[protein]] sequence that extends [[BLAST|BLAST (Basic Local Alignment Search Tool)]],&lt;ref&gt;{{ cite journal
  |vauthors=Altschul SF, Gish W, Miller W, Myers EW, Lipman DJ | title = Basic local alignment search tool.
  | journal = J Mol Biol
  | year =  1990 
  | volume = 215
  | issue = 3
  | pages = 403–410
  | pmid = 2231712
  | doi = 10.1016/S0022-2836(05)80360-2}}
&lt;/ref&gt; (Basic Local Alignment Search Tool) using context-specific mutation probabilities. More specifically, CS-BLAST derives context-specific [[amino-acid]] similarities on each query sequence from short windows on the query sequences [4]. Using CS-BLAST doubles sensitivity and significantly improves alignment quality without a loss of speed in comparison to BLAST. '''CSI-BLAST''' (Context-Specific Iterated) BLAST is the context-specific analog of [[PSI-BLAST]] &lt;ref&gt;{{cite journal
  |author1=Altschul SF |author2=Madden TL |author3=Schäffer AA |author4=Zhang J |author5=Zhang Z |author6=Miller W |author7=Lipman DJ. | title = Gapped BLAST and PSI-BLAST: a new generation of protein database search programs.
  | journal = Nucleic Acids Res
  | year = 1997
  | volume = 25
  | issue = 17
  | pages = 3389–3402
  | pmid = 9254694
  | doi = 10.1093/nar/25.17.3389
  | pmc = 146917
}}
&lt;/ref&gt; (Position-Specific Iterated) BLAST which computes the mutation profile with substitution probabilities and mixes it with the query profile [2]. CSI-BLAST (Context-Specific Iterated BLAST) is the context specific analog of PSI-BLAST (Position-Specific Iterated) BLAST. Both of these programs are available as web-server and are available for free download.

== Background ==
Homology is the relationship between biological structures or sequences derived from a common ancestor. Homologous proteins (proteins who have common ancestry) are inferred from their sequence similarity. Inferring homologous relationships involves calculating scores of aligned pairs minus penalties for gaps. Aligning pairs of proteins identify regions of similarity indicating a relationship between the two, or more, proteins. In order to have a homologous relationship, the sum of scores over all the aligned pairs of amino acids or nucleotides must be sufficiently high [2]. Standard methods of sequence comparisons use a [[substitution matrix]] to accomplish this [4]. Similarities between amino acids or nucleotides are quantified in these substitution matrices. The substitution score (&lt;math&gt;S&lt;/math&gt;) of amino acids &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; can we written as follows:

&lt;math&gt;S(a,b) = const \times \log \left ( \frac{P(a|b)}{P(a)} \right )&lt;/math&gt;

where &lt;math&gt;P(a|b)&lt;/math&gt; denotes the probability of amino acid &lt;math&gt;a&lt;/math&gt; mutating into amino acid &lt;math&gt;b&lt;/math&gt; [2]. In a large set of sequence alignments, counting the number of amino acids  as well as the number of aligned pairs &lt;math&gt;(a, b)&lt;/math&gt; will allow you to derive the probabilities &lt;math&gt;P(a|b)&lt;/math&gt; and &lt;math&gt;P(a)&lt;/math&gt;.

Since protein sequences need to maintain a stable structure, a residue’s substitution probabilities are largely determined by the structural context of where it is found. As a result, substitution matrices are trained for structural contexts. Since context information is encoded in transition probabilities between states, mixing mutation probabilities from substitution matrices weighted for corresponding states achieves improved alignment qualities when compared to standard substitution matrices. CS-BLAST improves further upon this concept. The figure illustrates the sequence to sequence and profile to sequence equivalence with the alignment matrix. The query profile results from the artificial mutations in which the bar heights are proportional to the corresponding amino acid probabilities [4].

(A FIGURE NEEDS TO GO HERE THIS IS THE CAPTION) “Sequence search/alignment algorithms find the path that maximizes the sum of similarity scores (color-coded blue to red). Substitution matrix scores are equivalent to profile scores if the sequence profile (colored histogram) is generated from the query sequence by adding artificial  mutations with the substitution matrix pseudocount scheme. Histogram bar heights represent the fraction of amino acids in profile columns” [4].

== Performance ==
CS-BLAST greatly improves alignment quality over the entire range of sequence identities and especially for difficult alignments in comparison to regular BLAST and PSI-BLAST. PSI-BLAST (Position-Specific Iterated BLAST) runs at about the same speed per iteration as regular BLAST, but is able to detect weaker sequence similarities that are still biologically relevant [3]. Alignment quality is based on alignment sensitivity and alignment precision [4].

=== Alignment Quality ===
Alignment sensitivity is measured by correctly comparing predicted alignments of residue pairs to the total number of possible alignable pairs. This is calculated with the fraction: (pairs correctly aligned)/(pairs structurally alignable)

Alignment precision is measured by the correctness of aligned residue pairs. This is calculated with the fraction: (pairs correctly aligned)/(pairs aligned)

=== Search Performance ===
The graph is the benchmark Biegert and Söding used to evaluate homology detection. The benchmark compares CS-BLAST to BLAST using true positives from the same superfamily versus false positive of pairs from different folds [4]. (A GRAPH NEEDS TO GO HERE)

The other graph uses detects true positives (with a different scale than the previous graph) and false positives of PSI-BLAST and CSI-BLAST and compares the two for one to five iterations [4]. (A DIFFERENT GRAPH NEEDS TO GO HERE)

CS-BLAST offers improved sensitivity and alignment quality in sequence comparison. Sequence searches with CS-BLAST are more than twice as sensitive as BLAST [4]. It produces higher quality alignments and generates reliable E-values without a loss of speed. CS-BLAST detects 139% more homologous proteins at a cumulative error rate of 20% [2]. At a 10% error rate, 138% more homologs are detected, and for the easiest cases at a 1% error rate, CS-BLAST was still 96% more effective than BLAST [2]. Additionally, CS-BLAST in 2 iterations is more sensitive than 5 iterations of PSI-BLAST. About 15% more homologs were detected in comparison [4].

== Method ==
The CS-BLAST method derives similarities between sequence context-specific amino acids for 13 residue windows centered on each residue. CS-BLAST works by generating a sequence profile for a query sequence by using context-specific mutations and then jumpstarting a profile-to-sequence search method.

CS-BLAST starts by predicting the expected mutation probabilities for each position. For a certain residue, a sequence window of ten total surrounding residues is selected as seen in the image. Then, Biegert and Söding compared the sequence window to a library with thousands of context profiles. The library is generated by clustering a representative set of sequence profile windows. The actual predicting of mutation probabilities is achieved by weighted mixing of the central columns of the most similar context profiles [4]. This aligns short profiles that are nonhomologous and ungapped which gives higher weight to better matching profiles, making them easier to detect [4]. A sequence profile represents a multiple alignment of homologous sequences and describes what amino acids are likely to occur at each position in related sequences. With this method substitution matrices are unnecessary. In addition, there is no need for transition probabilities as a result of the fact that context information is encoded within the context profiles. This makes computation simpler and allows for runtime to be scaled linearly instead of quadratically.

(A FIGURE GOES HERE AND THIS IS THE CAPTION) “Computation of context-specific pseudocounts. The Expected mutations (i.e., pseudocounts) for a residue (highlighted in yellow) are calculated based on the sequence context around it (red box). Library profiles contribute to the context-specific sequence profile with weights determined by their similarity to the sequence context. The resulting profile can be used to jump-start PSI-BLAST, which will then perform a sequence-to-sequence search with context-specific amino acid similarities” [4].

The context specific mutation probability, the probability of observing a specific amino acid in a homologous sequence given a context, is calculated by a weighted mixing of the amino acids in the central columns of the most similar context profiles. The image illustrates the calculation of expected mutation probabilities for a specific residue at a certain position. As seen in the image, the library of context profiles all contribute based on similarity to the context specific sequence profile for the query sequence [4].

== Models  ==
In predicting substitution probabilities using only the amino acid’s local sequence context, you gain the advantage of not needing to know the structure of the query protein while still allowing for the detection of more homologous proteins than standard substitution matrices [4]. Bigert and Söding’s approach to predicting substitution probabilities was based on a generative model. In another paper in collaboration with Angermüller, they develop a discriminative machine learning method that improves prediction accuracy [2].

=== Generative Model ===
Given an observed variable &lt;math&gt;x&lt;/math&gt; and a target variable &lt;math&gt;y&lt;/math&gt;, a generative model defines the probabilities &lt;math&gt;P(x, y)&lt;/math&gt; and &lt;math&gt; P(y)&lt;/math&gt; separately. In order to predict the unobserved target variable, &lt;math&gt;y&lt;/math&gt;,  Bayes’ theorem, &lt;math&gt;P(y|x) = \left ( \frac{P(x|y)P(y)}{[\textstyle \sum_{y}P(x|y)P(y) \displaystyle]} \right )&lt;/math&gt;

is used. A generative model, as the name suggests, allows one to generate new data points &lt;math&gt;(x, y)&lt;/math&gt;. The joint distribution is described as &lt;math&gt;P(x,y) = P(x|y)P(y)&lt;/math&gt;. To train a generative model, the following equation is used to maximize the joint probability &lt;math&gt;\prod\left ( \frac{P(x_n,y_n)}{trainingData(x_n,y_n)} \right )&lt;/math&gt;.

=== Discriminative Model ===
The discriminative model is a logistic regression maximum entropy classifier. With the discriminative model, the goal is to predict a context specific substitution probability given a query sequence. The discriminative approach for modeling substitution probabilities,&lt;math&gt;P(a|C_l)&lt;/math&gt; where &lt;math&gt;C_l&lt;/math&gt; describes a sequence of amino acids around position &lt;math&gt;l&lt;/math&gt; of a sequence, is based on &lt;math&gt;K&lt;/math&gt; context states. Context states are characterized by parameters emission weight (&lt;math&gt;v_k(a)&lt;/math&gt;), bias weight (&lt;math&gt;\pi_k&lt;/math&gt;), and context weight (&lt;math&gt;\lambda_k(j,a)&lt;/math&gt;) [2]. Emission probabilities from a context state are given by the emission weights as follows for &lt;math&gt;d=1&lt;/math&gt; to &lt;math&gt;20&lt;/math&gt;: &lt;math&gt;P(a|k) = \left ( \frac{exp(v_k(a))}{\sum exp(v_k(a'))} \right )&lt;/math&gt;

where &lt;math&gt;P(a|k)&lt;/math&gt; is the emission probability and  is the context state. In the discriminative approach, probability for a context state &lt;math&gt;k&lt;/math&gt; given context &lt;math&gt;C_l&lt;/math&gt; is modeled directly by the exponential of an affine function of the context account profile where &lt;math&gt;C_l(j,a)&lt;/math&gt; is the context count profile with a normalization constant &lt;math&gt;Z(C_l)&lt;/math&gt; normalizes the probability to 1. This equation is as follows where the first summation takes &lt;math&gt;j=-d&lt;/math&gt; to &lt;math&gt;d&lt;/math&gt; and the second summation takes &lt;math&gt;a=1&lt;/math&gt; to &lt;math&gt;20&lt;/math&gt;: &lt;math&gt;P(k|C_l) = \left ( \frac{1}{Z(C_l)}exp(\pi_k + \pi\sum\sum \lambda_k(j,a)(C_l(j,a)) \right)&lt;/math&gt;.

As with the generative model, target distribution is obtained by mixing the emission probabilities of each context state weighted by the similarity.

== Using CS-BLAST ==
The MPI Bioinformatics toolkit in an interactive website and service that allows anyone to do comprehensive and collaborative protein analysis with a variety of different tools including CS-BLAST as well as PSI-BLAST [1]. This tool allows for input of a protein and select options for you to customize your analysis. It also can forward the output to other tools as well.

==See also==
*[[Sequence alignment software]]
*[[Multiple sequence alignment]]
*[[Position-specific scoring matrix]]
*[[BLAST| BLAST (Basic Local Alignment Search Tool)]]
*[[HH-suite|HH-suite software package]]

==References==
{{reflist}}[1] Alva, Vikram, Seung-Zin Nam, Johannes Söding, and Andrei N. Lupas. “The MPI Bioinformatics Toolkit as an Integrative Platform for Advanced Protein Sequence and Structure Analysis.” ''Nucleic Acids Research'' 44.Web server Issue (2016): W410-415. ''NCBI''. Web. 2 Nov. 2016.

[2] Angermüller, Christof, Andreas Biegert, and Johannes Söding. “Discriminative Modelling of Context-specific Amino Acid Substitution Properties” ''BIOINFORMATICS'' 28.24 (2012): 3240-247. ''Oxford Journals''. Web. 2 Nov. 2016.

[3] Astschul, Stephen F., et al. “Gapped BLAST and PSI-BLAST: A New Generation of Protein Database Search Programs.” ''Nucleic Acids Research'' 25.17 (1997): 3389-402. ''Oxford University Press.'' Print

[4] Bigert, A., and J. Söding. “Sequence Context-specific Profiles for Homology Searching.” ''Proceedings of the National Academy of Sciences'' 106.10 (2009): 3770-3775. PNAS. Web. 23 Oct. 2016.

==External links==
*[http://toolkit.lmb.uni-muenchen.de/cs_blast CS-BLAST] &amp;mdash; free server at University of Munich (LMU)
*[http://toolkit.tuebingen.mpg.de/cs_blast CS-BLAST] &amp;mdash; free server at Max-Planck Institute in Tuebingen
*[https://github.com/soedinglab/csblast CS-BLAST source code]

[[Category:Bioinformatics software]]
[[Category:Computational science]]</text>
      <sha1>005vwf4qektrft6j38gea4v3su9wgxq</sha1>
    </revision>
  </page>
  <page>
    <title>Carl Wilhelm Borchardt</title>
    <ns>0</ns>
    <id>482522</id>
    <revision>
      <id>809094565</id>
      <parentid>703096827</parentid>
      <timestamp>2017-11-07T02:42:37Z</timestamp>
      <contributor>
        <username>NihlusBOT</username>
        <id>31996569</id>
      </contributor>
      <minor/>
      <comment>Bot: removed invalid image syntax from file parameters ([[Wikipedia:Bots/Requests for approval/NihlusBOT 7|Task 7]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3276">{{Infobox scientist
| name = Carl Borchardt
| image = Carl Wilhelm Borchardt.jpg
| image_size = 220px
| caption = Carl Wilhelm Borchardt (1817-1880)
| birth_date  = {{birth date|1817|2|22|df=y}} 
| birth_place = [[Berlin]], [[Kingdom of Prussia]]
| death_date  = {{death date and age|1880|6|27|1817|2|22|df=y}})
| death_place = [[Rüdersdorf]], [[German Empire]]
| residence = [[Germany]]
| nationality = [[Germany|German]]
| field = [[Mathematician]]
| work_institution = [[Humboldt University of Berlin|University of Berlin]]
| alma_mater = [[Humboldt University of Berlin|University of Berlin]] &lt;br&gt; [[University of Königsberg]]
| doctoral_advisor = [[Peter Gustav Lejeune Dirichlet]]
| doctoral_students = 
| known_for  = [[matrix (mathematics)|Diagonalising symmetric matrices]]
| prizes = 
| religion = [[Judaism]]
| footnotes =
}}

'''Carl Wilhelm Borchardt''' (22 February 1817 &amp;ndash; 27 June 1880) was a [[Germany|German]] [[mathematics|mathematician]].

Borchardt was born to a [[Jew]]ish family in [[Berlin]].&lt;ref name = "MacTutor"&gt;{{cite web
| url = http://www-groups.dcs.st-and.ac.uk/~history/Biographies/Borchardt.html
| title = Carl Wilhelm Borchardt
| accessdate = 2007-03-12
| last = O’Connor
| first = J.J.
|author2=E.F. Robertson
|date=August 2006
| work = [[The MacTutor History of Mathematics archive]]
| publisher = [[University of St Andrews]]
}}
&lt;/ref&gt; His father, Moritz, was a respected merchant, and his mother was Emma Heilborn.&lt;ref name = "MacTutor" /&gt; Borchardt studied under a number of tutors, including [[Julius Plücker]] and [[Jakob Steiner]]. He studied at the [[Humboldt University of Berlin|University of Berlin]] under [[Peter Gustav Lejeune Dirichlet|Lejeune Dirichlet]] in 1836 and at the [[University of Königsberg]] in 1839.&lt;ref name = "MacTutor" /&gt; In 1848 he began teaching at the University of Berlin.

He did research in the area of [[arithmetic-geometric mean]], continuing work by [[Carl Friedrich Gauss|Gauss]] and [[Joseph-Louis Lagrange|Lagrange]]. He generalised the results of [[Ernst Kummer|Kummer]] diagonalising symmetric [[matrix (mathematics)|matrices]], using [[determinant]]s and [[Sturm's theorem|Sturm functions]]. He was also an editor of ''[[Crelle's Journal]]'' from 1856&amp;ndash;80, during which time it was known as ''Borchardt's Journal''.

He died in [[Rüdersdorf]], [[Germany]]. His grave is preserved in the [[Protestant]] ''Friedhof III der Jerusalems- und Neuen Kirchengemeinde'' (Cemetery No. III of the congregations of [[Jerusalem's Church]] and [[Deutscher Dom|New Church]]) in [[Kreuzberg|Berlin-Kreuzberg]], south of [[Hallesches Tor (Berlin U-Bahn)|Hallesches Tor]].

== See also ==
* [[Cayley's formula]]

== References==
{{Reflist}}

{{Authority control}}

{{DEFAULTSORT:Borchardt, Carl Wilhelm}}
[[Category:1817 births]]
[[Category:1880 deaths]]

&lt;!--Categories--&gt;
[[Category:Algebraists]]
[[Category:19th-century German mathematicians]]
[[Category:German Jews]]
[[Category:Members of the Prussian Academy of Sciences]]
[[Category:Scientists from Berlin]]
[[Category:People from the Province of Brandenburg]]
[[Category:Humboldt University of Berlin alumni]]
[[Category:Humboldt University of Berlin faculty]]
[[Category:University of Königsberg alumni]]


{{Germany-mathematician-stub}}</text>
      <sha1>0v3rlt49founmhf554eokc1igb78xk5</sha1>
    </revision>
  </page>
  <page>
    <title>Chebfun</title>
    <ns>0</ns>
    <id>29095224</id>
    <revision>
      <id>830777201</id>
      <parentid>786632970</parentid>
      <timestamp>2018-03-16T21:46:51Z</timestamp>
      <contributor>
        <username>Tom.Reding</username>
        <id>9784415</id>
      </contributor>
      <minor/>
      <comment>Fix [[:Category:Pages using deprecated image syntax]]; [[WP:GenFixes]] on, using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5169">{{Infobox software
| name                   = Chebfun
| logo                   = Cheblogo.png
| logo size              = 250px
| developer              = The Chebfun Team,University of Oxford
| programming language   = [[MATLAB]]
| genre                  = [[Numerical software]]
| license                = [[BSD licenses|BSD]]
| website                = {{URL|http://www.chebfun.org/}}
| latest_release_version = v5.7.0
| latest_release_date    = 02 June 2017
}}

'''Chebfun''' is a [[Free and open source software|free/open-source]] software system written in [[MATLAB]] for numerical computation with functions of a real variable.  It is based on the idea of overloading MATLAB's commands for vectors and matrices to analogous commands for functions and operators.  Thus, for example, whereas the SUM command in MATLAB adds up the elements of a vector, the SUM command in Chebfun evaluates a definite integral.  Similarly the backslash command in MATLAB becomes a Chebfun command for solving differential equations.&lt;ref name="Battles2004"&gt;{{Cite journal | last1 = Battles | first1 = Zachary| last2 = Trefethen | first2 = Lloyd N.| doi = 10.1137/S1064827503430126 | title = An Extension of MATLAB to Continuous Functions and Operators| url = http://www.chebfun.org/publications/chebfun_paper.pdf| journal = SIAM Journal on Scientific Computing | volume = 25 | issue = 5 | pages = 1743–1770| year = 2004 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref name="Trefethen2007"&gt;{{Cite journal | last1 = Trefethen | first1 = Lloyd N. | doi = 10.1007/s11786-007-0001-y | title = Computing Numerically with Functions Instead of Numbers| url = http://www.chebfun.org/publications/trefethen_functions.pdf| journal = Mathematics in Computer Science | volume = 1 | pages = 9–19 | year = 2007 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref name="Pachon2009"&gt;{{Cite journal | last1 = Pachón | first1 = Ricardo | last2 = Platte | first2 = Rodrigo B. | last3 = Trefethen | first3 = Lloyd N. | doi = 10.1093/imanum/drp008 | title = Piecewise-smooth chebfuns| url = http://people.maths.ox.ac.uk/trefethen/publication/PDF/2010_134.pdf| journal = IMA Journal of Numerical Analysis | volume = 30 | issue = 4 | pages = 898&amp;ndash;916 | date = October 2010 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref name="Driscoll2008"&gt;{{Cite journal | last1 = Driscoll | first1 = Tobin A. | last2 = Bornemann | first2 = Folkmar| last3 = Trefethen | first3 = Lloyd N.| doi = 10.1007/s10543-008-0198-4 | title = The chebop system for automatic solution of differential equations| url = http://www.chebfun.org/publications/driscoll_born_tref.pdf| journal = BIT Numerical Mathematics | volume = 48 | issue = 4 | pages = 701&amp;ndash;723| date = December 2008 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref name="Townsend2013"&gt;{{Cite journal | last1 = Townsend | first1 = Alex| last2 = Trefethen | first2 = Lloyd N. | doi = 10.1137/130908002 | title = An Extension of Chebfun to Two Dimensions| url = http://www.chebfun.org/publications/Chebfun2paper.pdf| journal = SIAM Journal on Scientific Computing | volume = 35 | issue = 6 | pages = C495–C518| year = 2013 | pmid =  | pmc = }}&lt;/ref&gt;

The mathematical basis of Chebfun is numerical algorithms involving piecewise polynomial interpolants and [[Chebyshev polynomials]], and this is where the name "Cheb" comes from.  The package aims to combine the feel of symbolic computing systems like [[Maple (software)|Maple]] and [[Mathematica]] with the speed of floating-point numerics.&lt;ref name="Trefethen2007" /&gt;&lt;ref name="Pachon2009" /&gt;

The Chebfun project is based in the Mathematical Institute at the [[University of Oxford]] and was initiated in 2002 by [[Lloyd N. Trefethen]] and his student Zachary Battles.&lt;ref name="Battles2004" /&gt;  The most recent version, Version 5.7.0, was released on June 2, 2017.

Chebfun2, a software system that extends Chebfun to two dimensions, was made publicly available on the 4th of March 2013. Following Chebfun2, Spherefun (extension to the unit sphere) and Chebfun3 (extension to three dimensions) were made publicly available in May and July 2016.

==Features==

* Approximation of functions in 1D, including functions with jumps
* Approximation of smooth bivariate functions (Chebfun2)
* Approximation of smooth trivariate functions (Chebfun3)
* Approximation of smooth functions on the unit sphere (Spherefun)
* Quadrature
* Rootfinding
* 1D global optimisation
* Bivariate and trivariate rootfinding
* Ordinary differential equations
* Partial differential equations
* Vector calculus

==Example usage==

A user may begin by initialising the variable x, on the interval [0,10], say.

&lt;source lang="matlab"&gt;
&gt;&gt; x = chebfun('x',[0,10]); 
&lt;/source&gt;

This variable can now be used to perform further computations, for example, computing and plotting roots of a function:

&lt;source lang="matlab"&gt;
&gt;&gt; f = sin(x) + sin(x.^2);  plot(f)
&gt;&gt; r = roots(f); hold on, plot(r,f(r),'.r'), hold off 
&lt;/source&gt;

[[Image:ChebExampleRoots.png|500px]]

The definite integral can be computed with:

&lt;source lang="matlab"&gt;
&gt;&gt; sum(f) 
ans 
   = 2.422742429006079 
&lt;/source&gt;

== References ==
{{reflist}}

== External links ==
* {{Official|http://www.chebfun.org/}}

[[Category:Mathematical software]]</text>
      <sha1>ezj6fcz0hc16q8yx9xa2ifjy21xwq5h</sha1>
    </revision>
  </page>
  <page>
    <title>Constructive set theory</title>
    <ns>0</ns>
    <id>5042360</id>
    <revision>
      <id>850512505</id>
      <parentid>804818028</parentid>
      <timestamp>2018-07-16T10:25:16Z</timestamp>
      <contributor>
        <ip>31.209.33.141</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11934">'''Constructive set theory''' is an approach to [[constructivism (mathematics)|mathematical constructivism]] following the program of [[axiomatic set theory]]. That is, it uses the usual [[first-order logic|first-order]] language of classical set theory, and although of course the logic is [[constructive logic|constructive]], there is no explicit use of [[constructive type theory|constructive types]]. Rather, there are just [[Set (mathematics)|sets]], thus it can look very much like classical mathematics done on the most common [[foundation of mathematics|foundations]], namely the [[Zermelo–Fraenkel axioms]] (ZFC).

== Intuitionistic Zermelo–Fraenkel ==
In 1973, [[John Myhill]] proposed a system of set theory based on [[intuitionistic logic]]&lt;ref&gt;Myhill, '' "Some properties of Intuitionistic Zermelo-Fraenkel set theory"'', Proceedings of the 1971 Cambridge Summer School in Mathematical Logic (Lecture Notes in Mathematics 337) (1973) pp 206-231&lt;/ref&gt; taking the most common foundation, ZFC, and throwing away the [[axiom of choice]] (AC) and the [[law of the excluded middle]] (LEM), leaving everything else as is. However, different forms of some of the ZFC axioms which are equivalent in the classical setting are inequivalent in the constructive setting, and some forms imply LEM.

The system, which has come to be known as IZF, or Intuitionistic Zermelo–Fraenkel (ZF refers to ZFC without the axiom of choice), has the usual axioms of [[axiom of extensionality|extensionality]], [[axiom of pairing|pairing]], [[axiom of union|union]], [[axiom of infinity|infinity]], [[axiom schema of separation|separation]] and [[axiom of power set|power set]]. The [[axiom of regularity]] is stated in the form of an [[epsilon-induction|axiom schema of set induction]]. Also, while Myhill used the [[axiom schema of replacement]] in his system,  IZF usually stands for the version with  [[Axiom schema of replacement#Axiom schema of collection|collection]].

While the axiom of replacement requires the relation ''φ'' to be a [[function (set theory)|function]] over the set ''A'' (that is, for every ''x'' in ''A'' there is associated exactly one ''y''), the axiom of collection does not: it merely requires there be associated at least one ''y'', and it asserts the existence of a set which collects at least one such ''y'' for each such ''x''. The [[axiom of regularity]] as it is normally stated implies LEM, whereas the form of set induction does not. The formal statements of these two schemata are:

&lt;math&gt;\forall A \; ([\forall x \in A \; \exists y \; \phi(x,y)] \to \exists B \; \forall x \in A \; \exists y \in B \; \phi(x,y))&lt;/math&gt;

&lt;math&gt;[\forall y \; ([\forall x \in y \; \phi(x)] \to \phi(y))] \to \forall y \; \phi(y)&lt;/math&gt;

Adding LEM back to IZF results in ZF, as LEM makes collection equivalent to replacement and set induction equivalent to regularity. Even without LEM, IZF's proof-theoretical power equals that of ZF.

=== Predicativity ===
While IZF is based on intuitionistic rather than classical logic, it is considered [[impredicative]]. It allows formation of sets using the [[axiom of separation]] with any proposition, including ones which contain [[quantifier (logic)|quantifier]]s which are not bounded. Thus new sets can be formed in terms of the universe of all sets. Additionally the power set axiom implies the existence of a set of [[truth value]]s. In the presence of LEM, this set exists and has two elements. In the absence of it, the set of truth values is also considered impredicative.

== Myhill's constructive set theory ==
The subject was begun by [[John Myhill]] to provide a formal foundation for [[Errett Bishop]]'s program of constructive mathematics. As he presented it, Myhill's system CST is a constructive first-order logic with three [[many-sorted logic|sorts]]: [[natural numbers]], [[function (mathematics)|function]]s, and sets. The system is:

* Constructive first-order predicate logic with identity, and basic axioms related to the three sorts.
* The usual [[Peano axioms]] for natural numbers.
* The usual [[axiom of extensionality]] for sets, as well as one for functions, and the usual [[axiom of union]].
* A form of the [[axiom of infinity]] asserting that the collection of natural numbers (for which he introduces a constant '''N''') is in fact a set.
* Axioms asserting that the [[domain (mathematics)|domain]] and [[range (mathematics)|range]] of a function are both sets. Additionally, an [[axiom of non-choice]] asserts the existence of a choice function in cases where the choice is already made. Together these act like the usual [[axiom schema of replacement|replacement axiom]] in classical set theory.
* The [[axiom of exponentiation]], asserting that for any two sets, there is a third set which contains all (and only) the functions whose domain is the first set, and whose range is the second set. This is a greatly weakened form of the [[axiom of power set]] in classical set theory, to which Myhill, among others, objected on the grounds of its [[impredicative|impredicativity]].
* The [[axiom schema of predicative separation|axiom of restricted, or predicative, separation]], which is a weakened form of the [[axiom schema of separation|separation axiom]] in classical set theory, requiring that any [[quantification (logic)|quantification]]s be bounded to another set.
* An [[axiom of dependent choice]], which is much weaker than the usual [[axiom of choice]].

== Aczel's constructive Zermelo–Fraenkel ==
[[Peter Aczel]]'s ''constructive Zermelo-Fraenkel'',&lt;ref&gt;Peter Aczel and Michael Rathjen, [https://web.archive.org/web/20130619020751/https://www.mittag-leffler.se/preprints/files/IML-0001-40.pdf ''Notes on Constructive Set Theory''], Reports Institut Mittag-Leffler, Mathematical Logic - 2000/2001, No. 40&lt;/ref&gt; or '''CZF''', is essentially IZF with its impredicative features removed. It strengthens the collection scheme, and then drops the impredicative power set axiom and replaces it with another collection scheme. Finally the separation axiom is restricted, as in Myhill's CST. This theory has a relatively simple interpretation in a version of [[constructive type theory]] and has modest proof theoretic strength as well as a fairly direct constructive and predicative justification, while retaining the language of set theory. Adding LEM to this theory also recovers full ZF.

The collection axioms are:

'''Strong collection schema''': This is the constructive replacement for the [[axiom schema of replacement]]. It states that if φ is a [[binary relation]] between sets which is ''[[Functional relation|total]]'' over a certain domain set (that is, it has at least one image of every element in the domain), then there exists a set which contains at least one image under φ of every element of the domain, and only images of elements of the domain. Formally, for any formula φ:

&lt;math&gt;\forall a ((\forall x \in a \; \exists y \; \phi(x,y)) \to \exists b \; (\forall x \in a \; \exists y \in b \; \phi(x,y)) \wedge (\forall y \in b \; \exists x \in a \; \phi(x,y)))&lt;/math&gt;

'''Subset collection schema''': This is the constructive version of the [[power set axiom]]. Formally, for any formula φ:

&lt;math&gt;\forall a,b \; \exists u \; \forall z ((\forall x \in a \; \exists y \in b \; \phi(x,y,z)) \to \exists v \in u \; (\forall x \in a \; \exists y \in v \; \phi(x,y,z)) \wedge (\forall y \in v \; \exists x \in a \; \phi(x,y,z)))&lt;/math&gt;

This is equivalent to a single and somewhat clearer '''axiom of fullness''': between any two sets ''a'' and ''b'', there is a set ''c'' which contains a total subrelation of any  total relation between ''a'' and ''b'' that can be encoded as a set of [[ordered pair]]s. Formally:

&lt;math&gt;\forall a,b \; \exists c \subseteq P(a,b) \; \forall R \in P(a,b) \; \exists S \in c \; S \subseteq R&lt;/math&gt;

where the references to ''P(a,b)'' are defined by:

&lt;math&gt;R \in P(a,b) \iff (\forall u \in R \; \exists x \in a \; \exists y \in b \; \langle x,y \rangle = u) \wedge (\forall x \in a \; \exists y \in b \; \langle x,y \rangle \in R)&lt;/math&gt;

&lt;math&gt;c \subseteq P(a,b) \iff \forall R \in c \; R \in P(a,b)&lt;/math&gt;

and some set-encoding of the ordered pair &lt;x,y&gt; is assumed.

The axiom of fullness implies CST's axiom of exponentiation: given two sets, the collection of all total functions from one to the other is also in fact a set.

The remaining axioms of CZF are: the axioms of [[axiom of extensionality|extensionality]], [[axiom of pairing|pairing]], [[axiom of union|union]], and [[axiom of infinity|infinity]] are the same as in ZF; and [[epsilon-induction|set induction]] and [[axiom schema of predicative separation|predicative separation]] are the same as above.

== Interpretability in type theory ==
{{expand section|date=June 2008}}
In 1977 Aczel showed that CZF can be interpreted in [[Martin-Löf type theory]],&lt;ref&gt;Aczel, Peter: 1978. The type theoretic interpretation of constructive set theory. In: A. MacIntyre et al. (eds.), Logic Colloquium '77, Amsterdam: North-Holland, 55–66.&lt;/ref&gt; (using the [[propositions-as-types]] approach) providing what is now seen a standard model of CZF in type theory.&lt;ref&gt;{{citation|editor1-first=Godehard |editor1-last=Link|title=One Hundred Years of Russell ́s Paradox: Mathematics, Logic, Philosophy|year=2004|publisher=Walter de Gruyter|isbn=978-3-11-019968-0| chapter =Predicativity, Circularity, and Anti-Foundation | first = M. | last= Rathjen| url=http://www1.maths.leeds.ac.uk/~rathjen/russelle.pdf}}
&lt;/ref&gt; In 1989 Ingrid Lindström showed that [[non-well-founded set]]s obtained by replacing the [[axiom of foundation]] in CZF with [[Aczel's anti-foundation axiom]] (CZFA) can also be interpreted in Martin-Löf type theory.&lt;ref&gt;Lindström, Ingrid: 1989. A construction of non-well-founded sets within Martin-Löf type theory. Journal of Symbolic Logic 54: 57–64.&lt;/ref&gt;

== Interpretability in category theory ==
{{expand section|date=November 2012}}
[[Presheaf (category theory)|Presheaf]] models for constructive set theory were introduced by Nicola Gambino in 2004. They are analogous to the Presheaf models for intuitionistic set theory developed by [[Dana Scott]] in the 1980s (which remained unpublished).&lt;ref&gt;{{Cite book| last1 = Gambino | first1 = N. | chapter = PRESHEAF MODELS FOR CONSTRUCTIVE SET THEORIES | doi = 10.1093/acprof:oso/9780198566519.003.0004 | title = From Sets and Types to Topology and Analysis | pages = 62–96 | year = 2005 | isbn = 9780198566519 | pmid =  | pmc = | url = http://www.math.unipa.it/~ngambino/Research/Papers/presheaf.pdf| editor = Laura Crosilla and Peter Schuster }}&lt;/ref&gt;&lt;ref&gt;Scott, D. S. (1985). Category-theoretic models for Intuitionistic Set Theory. Manuscript slides of a talk given at Carnegie-Mellon University&lt;/ref&gt;

==References==
{{Reflist}}

== Further reading ==
* {{cite book|isbn=0-444-70358-6|title=Constructivism in Mathematics, Vol. 2|last1=Troelstra|first1=Anne|authorlink1=Anne Sjerp Troelstra|last2=van Dalen|first2=Dirk|authorlink2=Dirk van Dalen|series=Studies in Logic and the Foundations of Mathematics|year=1988|page=619}}
* Aczel, P. and Rathjen, M. (2001). [https://web.archive.org/web/20070204153712/http://www.ml.kva.se/preprints/meta/AczelMon_Sep_24_09_16_56.rdf.html Notes on constructive set theory]. Technical Report 40, 2000/2001. Mittag-Leffler Institute, Sweden.

== External links ==
* Laura Crosilla, [http://plato.stanford.edu/entries/set-theory-constructive/ Set Theory: Constructive and Intuitionistic ZF], [[Stanford Encyclopedia of Philosophy]], Feb 20, 2009
* Benno van den Berg, [http://www.illc.uva.nl/KNAW/Heyting/uploaded_files/inlineitem/vdberg-slides.pdf Constructive set theory – an overview], slides from Heyting dag, Amsterdam, 7 September 2012

{{Non-classical logic}}

{{DEFAULTSORT:Constructive Set Theory}}
[[Category:Systems of set theory]]
[[Category:Constructivism (mathematics)]]
[[Category:Intuitionism]]</text>
      <sha1>c3pp0754kjnqpk1ijh68l6xwykx3j8n</sha1>
    </revision>
  </page>
  <page>
    <title>Counter-machine model</title>
    <ns>0</ns>
    <id>7599249</id>
    <revision>
      <id>853988606</id>
      <parentid>814196249</parentid>
      <timestamp>2018-08-08T06:03:56Z</timestamp>
      <contributor>
        <username>Ira Leviton</username>
        <id>25046916</id>
      </contributor>
      <minor/>
      <comment>Deleted 'interestingly' - see [[Wikipedia:Manual_of_Style/Words_to_watch#Editorializing]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="43294">''This page supplements [[counter machine]].''

Although some authors use the name "[[register machine]]" synonymously with "counter machine", this article will give details and examples of only of the most primitive species{{spaced ndash}}the "counter machine"{{spaced ndash}}of the genus "register machine."

Within the species "counter machine" there are a number of varieties: the models of [[Hans Hermes|Hermes]] (1954), Kaphengst (1957), [[Andrey Ershov|Ershov]] (1958), [[Rózsa Péter|Péter]] (1958), [[Marvin Minsky|Minsky]] (1961) and Minsky (1967), Melzak (1961), [[Joachim Lambek|Lambek]] (1961), Shepherdson and Sturgis (1963), and [[Arnold Schönhage|Schönhage]] (1980). These models will be described in more detail in the following.

== The models in more detail ==

=== 1954: Hermes' model ===
Shepherdson and Sturgis observe that "the proof of this universality [of digital computers to Turing machines] ... seems to have been first written down by Hermes, who showed in [7--their reference number] how an idealized computer could be programmed to duplicate the behavior of any Turing machine" (Shepherdson and Sturgis, p.&amp;nbsp;219).

Shepherdson and Sturgis observe that:
:"Kaphengst's approach is interesting in that it gives a direct proof of the universality of present-day digital computers, at least when idealized to the extent of admitting an infinity of storage registers each capable of storing arbitrarily long words" (Shepherdson and Sturgis, p. 219)

The only two ''arithmetic'' instructions are
*(1) Successor operation
*(2) Testing two numbers for equality

The rest of the operations are transfers from register-to-accumulator or accumulator-to-register or test-jumps.

Kaphengst's paper is written in German; Sheperdson and Sturgis' translation results in strange words such as "mill" and "orders".

The machine contains "a mill" (accumulator). Kaphengst designates his mill/accumulator with the "infinity" symbol but we will use "A" in the following description. It also contains an "order register" ("order" as in "instruction", not as in "sequence"). (This usage came from the report of Burks-Goldstine-von Neumann (1946) description of '...an Electronic Computing Instrument'.) The order/instruction register is register "0". And, although not clear from Sheperdson and Sturgis' exposition the model contains an "extension register" designated by Kaphengst "infinity-prime"; we will use "E". 
   
The instructions are stored in the registers:
:"...so the machine, like an actual computer, is capable of doing arithmetic operations on its own program" (p. 244).

Thus this model is actually a [[random-access machine]]. In the following, "[ r ]" indicates "contents of" register r, etc.

{|class="wikitable"
|- style="font-size:9pt"
! width="21.6" Height="9.6" align="center" valign="bottom" | 
! width="41.4" valign="bottom" | 
! width="216" align="center" valign="bottom" | Action:
! width="306.6" align="center" valign="bottom" | Description

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | D1:
| valign="bottom" | C(r, A)
| valign="bottom" | [ r ] → A, [ r ] → r
| valign="bottom" | Copy contents of register r to accumulator A

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | D2:
| valign="bottom" | C(A, r)
| valign="bottom" | [ A ] → r, [ A ] → A
| valign="bottom" | Copy contents of  accumulator A to register r

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | C1:
| valign="bottom" | O(A)
| valign="bottom" | 0  →  A
| valign="bottom" | Zero (clear) accumulator A

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | A1:
| valign="bottom" | P(A)
| valign="bottom" | [ A ] + 1 → A
| valign="bottom" | Increment (add 1 to) contents of accumulator A

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | F1:
| valign="bottom" | J(A) [E1]
| valign="bottom" | IF [ A ] = 0 THEN jump to "Exit 1"
| valign="bottom" | Jump if contents of accumulator A = 0

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | G1:
| valign="bottom" | On(A) 
| valign="bottom" | IF [ A ] = [ r ] THEN 0 → &lt; A &gt; ELSE 1 →  A 
| valign="bottom" | Clear contents of A if contents of A = contents of r, else "set" A=1

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | G2:
| valign="bottom" | O'(A)
| valign="bottom" | 1 → A
| valign="bottom" | "Set" contents of A = 1

|}

Shepherdson and Sturgis remove the mill/accumulator A and reduce the Kaphengst instructions to register-to-register "copy", arithmetic operation "Increment", and "register-to-register compare". ''Observe that there is no decrement''. This model, almost verbatim, is to be found in Minsky (1967); see more in the section below.

{|class="wikitable"
|- style="font-size:9pt"
! width="21.6" Height="9.6" align="center" valign="bottom" | 
! width="64.8" valign="bottom" | 
! width="252.6" align="center" valign="bottom" | Action:
! width="306.6" align="center" valign="bottom" | Description:

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | a:
| valign="bottom" | P(A)
| valign="bottom" | [ A ] + 1 → A
| valign="bottom" | Increment (add 1 to) contents of accumulator A

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | d.
| valign="bottom" | C(r&lt;sub&gt;j&lt;/sub&gt;, r&lt;sub&gt;k&lt;/sub&gt;)
| valign="bottom" | [ r&lt;sub&gt;j&lt;/sub&gt; ] → r&lt;sub&gt;k&lt;/sub&gt;, [ r&lt;sub&gt;j&lt;/sub&gt; ] → r&lt;sub&gt;j&lt;/sub&gt;
| valign="bottom" | Copy contents of register r&lt;sub&gt;j&lt;/sub&gt; to register r&lt;sub&gt;k&lt;/sub&gt;

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | f:
| valign="bottom" | J(r) [E1]
| valign="bottom" | IF [ r ] = 0 THEN jump to "Exit 1" ELSE next instruction
| valign="bottom" | Jump if contents of register r = 0

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | c:
| valign="bottom" | E(r&lt;sub&gt;j&lt;/sub&gt;, r&lt;sub&gt;k&lt;/sub&gt;) 
| valign="bottom" | IF [ r&lt;sub&gt;j&lt;/sub&gt; ] = [ r&lt;sub&gt;k&lt;/sub&gt; ] THEN 0 → E ELSE 1 → E
| valign="bottom" | Clear contents of register E if contents of r&lt;sub&gt;j&lt;/sub&gt; = contents of r&lt;sub&gt;k&lt;/sub&gt;, else "set" E=1

|}

=== 1958: Ershov's class of operator algorithms ===
Shepherdson and Sturgis (1963) observe that Ersov's model allows for storage of the program in the registers. They assert that Ersov's model is as follows:

{|class="wikitable"
|- style="font-size:9pt"
! width="21.6" Height="9.6" align="center" valign="bottom" | 
! width="64.8" valign="bottom" | 
! width="252.6" align="center" valign="bottom" | Action:
! width="306.6" align="center" valign="bottom" | Description:

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | d.
| valign="bottom" | C(r&lt;sub&gt;j&lt;/sub&gt;,r&lt;sub&gt;k&lt;/sub&gt;)
|  valign="bottom" | [ r&lt;sub&gt;j&lt;/sub&gt; ] → r&lt;sub&gt;k&lt;/sub&gt;, [ r&lt;sub&gt;j&lt;/sub&gt; ] → r&lt;sub&gt;j&lt;/sub&gt;
| valign="bottom" | Copy contents of register r&lt;sub&gt;j&lt;/sub&gt; to register r&lt;sub&gt;k&lt;/sub&gt;

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | d'.
| valign="bottom" | C' (r&lt;sub&gt;j&lt;/sub&gt;,r&lt;sub&gt;k&lt;/sub&gt;)
|  valign="bottom" | [ r&lt;sub&gt;j&lt;/sub&gt; ] +1 → r&lt;sub&gt;k&lt;/sub&gt;, [ r&lt;sub&gt;j&lt;/sub&gt; ] → r&lt;sub&gt;j&lt;/sub&gt;
| valign="bottom" | Copy incremented contents of register r&lt;sub&gt;j&lt;/sub&gt; to register r&lt;sub&gt;k&lt;/sub&gt;

|- style="font-size:9pt"
| Height="9.6" valign="bottom" | e.
| valign="bottom" | J[E1]
|  valign="bottom" | Jump to "Exit 1"
| valign="bottom" | Unconditional jump to "Exit #1"

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | f*:
| valign="bottom" | J(r&lt;sub&gt;j&lt;/sub&gt;, r&lt;sub&gt;k&lt;/sub&gt;)[E1, E2] 
|  valign="bottom" | IF [ r&lt;sub&gt;j&lt;/sub&gt; ] ≤ [ r&lt;sub&gt;k&lt;/sub&gt; ] THEN jump to "Exit 1" ELSE jump to "Exit 2"
| valign="bottom" | Jump to exit E1 if contents of register r&lt;sub&gt;j&lt;/sub&gt; is less than or equal to contents of r&lt;sub&gt;k&lt;/sub&gt;, else jump to E=2

|}

=== 1958: Péter's "treatment" ===
Shepherdson and Sturgis (1963) observe that Péter's "treatment" (they are not too specific here) has an equivalence to the instructions shown in the following table. They comment specifically about these instructions, that:
:"from the point of view of proving as quickly as possible the computability of all [[partial recursive function]]s Péter's is perhaps the best; for proving their computability by [[Turing machine]]s a further analysis of the copying operation is necessary along the lines we have taken above." (Shepherdson and Sturgis (1963) p. 246)

{|class="wikitable"
|- style="font-size:9pt"
! width="21.6" Height="9.6" align="center" valign="bottom" | 
! width="64.8" valign="bottom" | 
! width="252.6" align="center" valign="bottom" | Action:
! width="306.6" align="center" valign="bottom" | Description:

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | c:
| valign="bottom" | O(n)
| valign="bottom" | 0  → [ n ]
| valign="bottom" | Zero (clear) register n

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | d.
| valign="bottom" | C(m,n)
|  valign="bottom" | [ m ] →  n, [ m ] → [ m ]
| valign="bottom" | Copy contents of register m to register n

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | d'.
| valign="bottom" | C'(m,n)
|  valign="bottom" | [ m ] + 1  → [ n ], [ m ] → [ m ]
| valign="bottom" | Copy incremented contents of register m to register n

|- style="font-size:9pt"
| Height="9.6" valign="bottom" | e.
| valign="bottom" | J(m, n)[E1, E2]
|  valign="bottom" | IF [m]=[n] jump to E1 ELSE jump to E2
| valign="bottom" | Conditional jump to E1 if contents of m equals contents of n, else jump to E2.
|}

=== 1961: Minsky's model of a partial recursive function reduced to a "program" of only two instructions ===
In his inquiry into problems of [[Emil Post]] (the [[tag system]]) and [[David Hilbert|Hilbert]]'s 10th problem ([[Hilbert's problems]], [[Diophantine equation]]) led Minsky to the following definition of:
:"an interesting basis for recursive function theory involving programs of only the simplest arithmetic operations" (Minsky (1961) p. 437).

His "Theorem Ia" asserts that any partial recursive function is represented by "a program operating on ''two'' integers S1 and S2 using instructions Ij of the forms (cf Minsky (1961) p. 449):

{|class="wikitable"
|- style="font-size:9pt"
! width="25" Height="9.6" align="center" valign="bottom" | 
! width="69" valign="bottom" | 
! width="277.8" align="center" valign="bottom" | Action:
! width="385.2" align="center" valign="bottom" | Description:

|- style="font-size:9pt" valign="bottom"
| Height="9.6" | a.
 | ADD ( r, I&lt;sub&gt;j1&lt;/sub&gt; )
 | [ r ] + 1 → r; go to instruction I&lt;sub&gt;j1&lt;/sub&gt;.
 | Increment (add 1 to) contents of register r and go to instruction I&lt;sub&gt;j1&lt;/sub&gt;.

|- style="font-size:9pt" valign="bottom"
| Height="9.6" | b.
 | SUB (r, I&lt;sub&gt;j1&lt;/sub&gt;, I&lt;sub&gt;j2&lt;/sub&gt;)
 | If [ r ] ≤ 0 THEN go to instr. I&lt;sub&gt;j2&lt;/sub&gt; ELSE [ r ] -1 → r and go to instr. I&lt;sub&gt;j1&lt;/sub&gt;
 | IF contents of register r equals zero THEN jump to instruction I&lt;sub&gt;j2&lt;/sub&gt;; ELSE decrement (subtract 1 from) contents of register r and jump to instr. I&lt;sub&gt;j1&lt;/sub&gt;.

|}

The first theorem is the context of a second "Theorem IIa" that 
: "...represents any partial recursive function by a program operating on one integer S [contained in a single register r1] using instructions I&lt;sub&gt;j&lt;/sub&gt; of the forms":
{|class="wikitable"
|- style="font-size:9pt"
! width="25" Height="9.6" align="center" valign="bottom" | 
! width="69" valign="bottom" | 
! width="277.8" align="center" valign="bottom" | Action:
! width="385.2" align="center" valign="bottom" | Description:

|- style="font-size:9pt" valign="bottom"
| Height="9.6" | a.
 | MULT (K&lt;sub&gt;j&lt;/sub&gt;, I&lt;sub&gt;j1&lt;/sub&gt;)
 | [ r1 ]*K&lt;sub&gt;j&lt;/sub&gt; → r1; go to instruction I&lt;sub&gt;j1&lt;/sub&gt;.
 | Multiply contents of register r1 by constant K&lt;sub&gt;j&lt;/sub&gt;

|- style="font-size:9pt" valign="bottom"
| Height="9.6" | b.
 | DIV (K&lt;sub&gt;j&lt;/sub&gt;, I&lt;sub&gt;j1&lt;/sub&gt;, I&lt;sub&gt;j2&lt;/sub&gt;)
 | [ r1 ]/Kj = 0 then go to instruction I&lt;sub&gt;j2&lt;/sub&gt; else go to I&lt;sub&gt;j1&lt;/sub&gt;. 
 | If division of contents of register 1 by constant K&lt;sub&gt;j&lt;/sub&gt; has no remainder then instr. I&lt;sub&gt;j1&lt;/sub&gt; else instr. I&lt;sub&gt;j2&lt;/sub&gt;

|}

In this second form the machine uses [[Gödel number]]s to process "the integer S". He asserts that the first machine/model does not need to do this if it has 4 registers available to it.

=== 1961: Melzak model: a single ternary instruction with addition and proper subtraction ===
:"It is our object to describe a primitive device, to be called a Q-machine, which arrives at effective computability via arithmetic rather than via logic. Its three operations are keeping tally, comparing non-negative integers, and transferring" (Melzak (1961) p. 281)

If we use the context of his model, "keeping tally" means "adding by successive increments" (throwing a pebbles into) or "subtracting by successive decrements"; transferring means moving (not copying) the contents from hole A to hole B, and comparing numbers is self-evident. This appears to be a blend of the three base models.

Melzak's physical model is holes { X, Y, Z, etc. } in the ground together with an unlimited supply of pebbles in a special hole '''S''' (Sink or Supply or both? Melzak doesn't say).

:"The Q-machine consists of an '''indefinitely large number of locations''': S, A1, A2, ..., an indefinitely large supply of counters distributed among these locations, a program, and an operator whose sole purpose is to carry out the instructions. Initially all but a finite number from among the locations ... are empty and each of the remaining ones contains a '''finite number of counters'''" (p. 283, boldface added)

The phrases '''indefinitely large number of locations''' and '''finite number of counters''' here are important. This model is different than the Minsky model that allows for a ''finite'' number of locations with ''unbounded'' (effectively infinite) capacity for "markers".

The instruction is a ''single'' "ternary operation" he calls "XYZ": 
:"XYZ" denotes the operation of
::(i) Count the number of pebbles in hole '''Y''',
::(ii) put them back into '''Y''',
::(iii) attempt to remove this same amount from hole '''X'''. IF this is not possible because it will empty hole '''X''' THEN do nothing and jump to instruction #I; ELSE,
::(iv) remove the Y-amount from '''X''' and (iv) transfer them to, i.e. ''add'' them to, the quantity in hole '''Z'''.

Of all the possible operations, some are not allowed, as shown in the table below:

{|class="wikitable"
|- style="font-size:9pt" align="center" valign="bottom"
! width="36" Height="9.6" | Allowed
! width="50.4" | Instruction
! width="115.2" | Hole "X"
! width="100.8" | Hole "Y" 
! width="95.4" | Hole "Z"
! width="234" | Meaning of Instruction

|- style="font-size:9pt" align="center" valign="bottom"
| Height="9.6" | NO
 | XXX
 | 
 | 
 | 
 |

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 
| align="center" valign="bottom" | XXY
| align="center" valign="bottom" | ([ X ] - [ X ])=0 → X 
| align="center" valign="bottom" | [ Y ] + [ X ]  → Y
| align="center" valign="bottom" |  [ Z  ] → Z
| valign="bottom" | All of X's pebbles taken from X and added to Y

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 
| align="center" valign="bottom" | XXS
| align="center" valign="bottom" | ([ X ] - [ X ])=0 → X 
| align="center" valign="bottom" |  [ Y ] → Y
| align="center" valign="bottom" |  [ Z ] → Z
| valign="bottom" | All of X's pebbles taken from X and put into sink/source S

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| valign="bottom" |

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | NO
| align="center" valign="bottom" | XYX
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| valign="bottom" |

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 
| align="center" valign="bottom" | XYY
| align="center" valign="bottom" | [ X ] - [ Y ]  → X
| align="center" valign="bottom" | [ Y ] + [ Y ]  → Y 
| align="center" valign="bottom" |  [ Z ] → Z
| valign="bottom" | Count of Y's pebbles taken from X and placed in Y, doubling count of Y

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 
| align="center" valign="bottom" | XYS
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| valign="bottom" |

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| valign="bottom" |

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | NO
| align="center" valign="bottom" | XSX
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| valign="bottom" |

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | NO
| align="center" valign="bottom" | XSY
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| valign="bottom" |

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | NO
| align="center" valign="bottom" | XSS
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| valign="bottom" |

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| valign="bottom" |

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 
| align="center" valign="bottom" | XYZ
| align="center" valign="bottom" | [ X ] - [ Y ]  → X
| align="center" valign="bottom" |  [ Y ] → Y
| align="center" valign="bottom" | [ Z ] + [ Y ]  → Z
| valign="bottom" | Count of Y's pebbles taken from X and added to Z,

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 
| align="center" valign="bottom" | SYY
| align="center" valign="bottom" |  [ X ] → X
| align="center" valign="bottom" | [ Y ] + [ Y ]  → Y
| align="center" valign="bottom" |  [ Z ] → Z
| valign="bottom" | Count of Y's pebbles taken from S and added to Y, doubling Y's count

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 
| align="center" valign="bottom" | SYZ
| align="center" valign="bottom" |  [ X ] → X
| align="center" valign="bottom" |  [ Y ] → Y
| align="center" valign="bottom" | [ Z ] + [ Y ]  → [ Z ]
| valign="bottom" | Count of Y's pebbles taken from S and added to Z

|}

'''Some observations about the Melzak model''':
:(a) If all the holes start with 0, then how do we increment? Apparently this is not possible; one hole must contain a single pebble.
:(b) The conditional "jump" occurs on every instance of XYZ type because: if it cannot be performed because X does not have enough counters/pebbles then the jump occurs; otherwise if it can be performed it will be and the instructions continue to the next in sequence.
:(c) Neither SXY nor XXY can cause a jump because both can always be performed.
:(d) Melzak adds indirection to his model (see [[Random-access machine]]) and gives two examples of its use. But he does not pursue this further. This is the first verified instance of "indirection" to appear in the literature.
:(e) Both papers{{spaced ndash}}that of [[Z. Alexander Melzak]] ([[William Lowell Putnam Mathematical Competition]], winner 1950) was received 15 May 1961 and [[Joachim Lambek]] received a month later on 15 June 1961{{spaced ndash}}are contained in the same volume, one after the other.
:(f) Is Melzak's assertion true?{{spaced ndash}}that this model is "so simple that its working could probably be understood by an average school-child after a few minutes's explanation" (p. 282)? The reader will have to decide.

=== 1961: Lambek "abacus" model: atomizing Melzak's model to X+, X- with test ===
'''Original "abacus" model of Lambek (1962):'''

Lambek references Melzak's paper. He atomizes Melzak's single 3-parameter operation (really 4 if we count the instruction addresses) into a 2-parameter increment "X+" and 3-parameter decrement "X-". He also provides both an informal and ''formal'' definition of "a program". This form is virtually identical to the Minsky (1961) model, and has been adopted by Boolos-Burgess-Jeffrey (2002).

{|class="wikitable"
|- style="font-size:9pt"
! width="16.8" Height="9.6" align="center" valign="bottom" | 
! width="69" valign="bottom" | 
! width="277.8" align="center" valign="bottom" | Action:
! width="385.2" align="center" valign="bottom" | Description:

|- style="font-size:9pt" valign="bottom"
| Height="9.6" | a.
 | X+ (r, I&lt;sub&gt;a&lt;/sub&gt;)
 | [ r ] + 1 → r; go to instruction I&lt;sub&gt;a&lt;/sub&gt;.
 | Increment (add 1 to) contents of register r

|- style="font-size:9pt" valign="bottom"
| Height="9.6" | b.
 | X- (r, I&lt;sub&gt;a&lt;/sub&gt;, I&lt;sub&gt;b&lt;/sub&gt;)
 | If [ r ] ≤ 0, go to instr.I&lt;sub&gt;b&lt;/sub&gt; else [ r ]-1 → r and go to instr. I&lt;sub&gt;a&lt;/sub&gt;
 | First test for zero, then decrement (subtract 1 from) contents of register r
|}

'''Abacus model of Boolos-Burgess (1970, etc.), Boolos-Burgess-Jeffrey (2002)''':

The various editions beginning with 1970 the authors use the Lambek (1961) model of an "infinite abacus". This series of Wikipedia articles is using their symbolism, e.g. " [ r ] +1 →  r" "the contents of register identified as number 'r', plus 1, replaces the contents of [is put into] register number 'r' ".

They use Lambek's name "abacus" but follow Melzak's pebble-in-holes model, modified by them to a 'stones-in-boxes' model. Like the original abacus model of Lambek, their model retains the Minsky (1961) use of non-sequential instructions{{spaced ndash}}unlike the "conventional" computer-like default sequential instruction execution, the next instruction I&lt;sub&gt;a&lt;/sub&gt; is contained within the instruction.

Observe, however, that B-B and B-B-J do not use a variable "X" in the mnemonics with a specifying parameter (as shown in the Lambek version) --i.e. "X+" and "X-"{{spaced ndash}}but rather the instruction mnemonics specifies the registers themselves, e.g. "2+", or "3-":

{|class="wikitable"
|- style="font-size:9pt"
! width="16.8" Height="9.6" align="center" valign="bottom" | 
! width="69" valign="bottom" | 
! width="277.8" align="center" valign="bottom" | Action:
! width="385.2" align="center" valign="bottom" | Description:

|- style="font-size:9pt" valign="bottom"
| Height="9.6" | a1.
 | 1+ (I&lt;sub&gt;a&lt;/sub&gt;)
 | [ r1 ] + 1 → r1 then go to instruction I&lt;sub&gt;a&lt;/sub&gt;.
 | Increment (add 1 to) contents of register #1

|- style="font-size:9pt" valign="bottom"
| Height="9.6" | b1.
 | 1- (I&lt;sub&gt;a&lt;/sub&gt;, I&lt;sub&gt;b&lt;/sub&gt;)
 | If [ r1 ]  ≤ 0 THEN go to I&lt;sub&gt;b&lt;/sub&gt; else [ r1 ] -1 → r1 and go to I&lt;sub&gt;a&lt;/sub&gt;.
 | Jump to instruction I&lt;sub&gt;b&lt;/sub&gt; if contents of register r1 is zero ELSE decrement (subtract 1 from) contents of register #1
|}

=== 1963: Shepherdson and Sturgis' model ===
On page 218 Shepherdson and Sturgis references Minsky (1961) as it appeared for them in the form of an [[MIT Lincoln Laboratory]] report:
: In Section 10 we show that theorems (including Minsky's results [21, their reference]) on the computation of partial recursive functions by one or two tapes can be obtained rather easily from one of our intermediate forms (p. 218).

Their model is strongly influenced by the model and the spirit of [[Hao Wang (academic)|Hao Wang]] (1957) and his [[Wang B-machine]] (also see [[Post–Turing machine]]). They "sum up by saying":
:"...we have tried to carry a step further the 'rapprochement' between the practical and theoretical aspects of computation suggested and started by Wang."

'''Unlimited Register Machine URM''': This, their "most flexible machine... consists of a denumerable sequence of registers numbered 1, 2, 3, ..., each of which can store any natural number...Each particular program, however involves only a finite number of these registers" (p.&amp;nbsp;219). In other words, the number of registers is potentially infinite, and each register's "size" is infinite.

They offer the following instruction set (p.&amp;nbsp;219), and the following "Notes":

{|class="wikitable"
|- style="font-size:9pt"
! width="21.6" Height="12" align="center" valign="bottom" | 
! width="64.8" valign="bottom" | URM model:
! width="252.6" align="center" valign="bottom" | Action:
! width="306.6" align="center" valign="bottom" | Description:

|- style="font-size:9pt" valign="bottom"
| Height="9.6" | a.
 | P(n)
 | [ r ] + 1 → r
 | Increment (add 1 to) contents of register r

|- style="font-size:9pt" valign="bottom"
| Height="9.6" | b.
 | D(n)
 | [ r ] - 1 → r

 | Decrement (subtract 1 from) contents of register r

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | c:
| valign="bottom" | O(n)
| valign="bottom" | 0  → r
| valign="bottom" | Zero (clear) register r

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | d.
| valign="bottom" | C(m,n)
|  valign="bottom" | [ r&lt;sub&gt;j&lt;/sub&gt; ] → r&lt;sub&gt;k&lt;/sub&gt;, [ r&lt;sub&gt;j&lt;/sub&gt; ] → r&lt;sub&gt;j&lt;/sub&gt;, 
| valign="bottom" | Copy contents of register r&lt;sub&gt;j&lt;/sub&gt; to register r&lt;sub&gt;k&lt;/sub&gt;

|- style="font-size:9pt"
| Height="9.6" valign="bottom" | e.
| valign="bottom" | J[E1]
|  valign="bottom" | Jump to "Exit 1"
| valign="bottom" | Unconditional jump to "Exit #1"

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | f:
| valign="bottom" | J(r) [E1]
| valign="bottom" | IF [ r&lt;sub&gt;j&lt;/sub&gt; ] = 0 THEN jump to "Exit 1" ELSE next instruction
| valign="bottom" | IF contents of register r = 0 then jump to instruction "Exit 1" else next 
instruction
|}
  
"Notes.
:"(1) This set of instructions is chosen for ease of programming the computation of partial recursive functions rather than economy; it is shown in Section 4 that this set is equivalent to a smaller set.
:"(2) There are infinitely many instructions in this list since m, n [ contents of r&lt;sub&gt;j&lt;/sub&gt;, etc] range over all positive integers.
:(3) In instructions a, b, c, d the contents of all registers except n are supposed to be left unchanged; in instructions e, f, the contents of all registers are unchanged (p. 219).

Indeed, they show how to reduce this set further, to the following (for an infinite number of registers each of infinite size):

{|class="wikitable"
|- style="font-size:9pt"
! width="21.6" Height="9.6" align="center" valign="bottom" | 
! width="64.8" valign="bottom" | Reduced URM:
! width="252.6" align="center" valign="bottom" | Action:
! width="306.6" align="center" valign="bottom" | Description:

|- style="font-size:9pt" valign="bottom"
| Height="9.6" | a1.
 | P(r)
 | [ r ] + 1 → r
 | Increment (add 1 to) contents of register r

|- style="font-size:9pt" valign="bottom"
| Height="9.6" | b1.
 | D(n)
 | [ r ] - 1 → r
 | Decrement (subtract 1 from) contents of register r

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | ~f1:
| valign="bottom" | J(r) [E1]
| valign="bottom" | IF [ r ] ≠ 0 THEN jump to "Exit 1"
| valign="bottom" | If contents of register m ≠ 0 THEN jump to instruction "Exit 1" ELSE continue

|}

'''Limited Register Machine LRM''': Here they restrict the machine to a finite number of registers N, but they also allow more registers to "be brought in" or removed if empty (cf p.&amp;nbsp;228). They show that the remove-register instruction need not require an empty register.

'''Single-Register Machine SRM''': Here they are implementing the [[tag system]] of [[Emil Post]] and thereby allow only writing to the end of the string and erasing from the beginning. This is shown in their Figure 1 as a tape with a read head on the left and a write head on the right, and it can only move the tape right. "A" is their "word" (p.&amp;nbsp;229):
:a. P(i)    ;add ai to the end of A
:b. D       ;delete the first letter of A
:f'. Ji[E1]  ;If A begins with ai jump to exit 1.

They also provide a model as "a stack of cards" with the symbols { 0, 1 } (p.&amp;nbsp;232 and Appendix C p.&amp;nbsp;248):
:(1) add card at top printed 1
:(2) add card at top printed 1
:(3) remove bottom card; if printed 1 jump to instruction m, else next instruction.

=== 1967: Minsky's "Simple Universal Base for a Program Computer" ===
Ultimately, in Problem 11.7-1 Minsky observes that many bases of computation can be formed from a tiny collection:
:"Many other combinations of operation types [ 0 ], [ ' ], [ - ], [ O- ], [ → ] and [ RPT ] form universal basis. Find some of these basis. Which combinations of three operations are not universal basis? Invent some other operations..."(p. 214)

The following are definitions of the various instructions he treats:
{|class="wikitable"
|- style="font-size:9pt"
! width="16.8" Height="9.6" align="center" valign="bottom" | 
! width="39" valign="bottom" | 
! width="277.8" align="center" valign="bottom" | Action:
! width="385.2" align="center" valign="bottom" | Description:

|- style="font-size:9pt"
| Height="9.6" valign="bottom" | a.
| align="center" valign="bottom" | [ 0 ]
| valign="bottom" | 0 → r
|  valign="bottom" | Zero (clear) register r

|- style="font-size:9pt"
| Height="9.6" valign="bottom" | b.
| align="center" valign="bottom" | [ ' ]
| valign="bottom" | [ r ] + 1 → r
|  valign="bottom" | Increment (add 1 to) contents of register r ( apostrophe ' signifies "successor" )

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | c.
| align="center" valign="bottom" | [ - ]
| valign="bottom" | IF [ r ] = 0 THEN jump to instruction z ELSE next instruction
|  valign="bottom" | Test register r and jump to instruction z if contents is zero; if not, decrement (subtract 1 from) contents of register r

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | d.
| align="center" valign="bottom" | [ O- ]
| valign="bottom" | If [ r ] ≠ 0 THEN [ r ] -1 → r  ELSE next instruction 
|  valign="bottom" | IF contents of register r not zero decrement contents of register r and jump to zth instruction, else if 0 then next instruction

|- style="font-size:9pt"
| Height="9.6" valign="bottom" | e.
| align="center" valign="bottom" | [ → ]
| valign="bottom" | [ r&lt;sub&gt;j&lt;/sub&gt; ] → r&lt;sub&gt;k&lt;/sub&gt;, [ r&lt;sub&gt;j&lt;/sub&gt; ] → r&lt;sub&gt;j&lt;/sub&gt; 
|  valign="bottom" | Copy contents of register r&lt;sub&gt;j&lt;/sub&gt; to register r&lt;sub&gt;k&lt;/sub&gt;

|- style="font-size:9pt"
| Height="9.6"  valign="bottom" | f.
| align="center" valign="bottom" | [ RPT]
| valign="bottom" | RPT a:[m,n].  Repeat cannot operate within its own range.
|  valign="bottom" | Do until contents of register [ r ] = 0: Repeat instructions m thru n. When [ r ] = 0, go to next instruction.

|- style="font-size:9pt"
| Height="9.6" valign="bottom" | g.
| align="center" valign="bottom" | [ H ]
| valign="bottom" | HALT
|  valign="bottom" |

|- style="font-size:9pt"
| Height="9.6" valign="bottom" | h.
| align="center" valign="bottom" | goto(z)
| valign="bottom" | Jump to instruction z
|  valign="bottom" | Unconditional jump to instruction z

|- style="font-size:9pt"
| Height="9.6" valign="bottom" | i.
| align="center" valign="bottom" | [ ≠ ] 
| valign="bottom" | If [ r&lt;sub&gt;j&lt;/sub&gt; ] ≠ [ r&lt;sub&gt;k&lt;/sub&gt; ] THEN jump to zth instruction ELSE next instruction 
|  valign="bottom" | Conditional jump: if contents of register r&lt;sub&gt;j&lt;/sub&gt; not equal to contents of register r&lt;sub&gt;k&lt;/sub&gt; THEN jump to instruction z ELSE next instruction

|- style="font-size:9pt"ucti
| Height="9.6" valign="bottom" | j.
| align="center" valign="bottom" | [ RPT]*
| valign="bottom" | RPT a:[m,n].  Repeat can operate within its own range.
| align="center" valign="bottom" | *Note: RPT must be in an infinite register

|}

Minsky (1967) begins with a model that consists of the three operations plus HALT:
:{ [ 0 ], [ ' ], [ - ], [ H ] }

He observes that we can dispense with [ 0 ] if we allow for a specific register e.g. '''w''' already "empty" (Minsky (1967) p.&amp;nbsp;206). Later (pages 255ff) he compresses the three { [ 0 ], [ ' ], [ - ] }, into two { [ ' ], [ - ] }.

But he admits the model is easier if he adds some [pseudo]-instructions [ O- ] (combined [ 0 ] and [ - ]) and "go(n)". He builds "go(n)" out of the register '''w''' pre-set to 0, so that [O-] ('''w''', (n)) is an unconditional jump.

In his section 11.5 "The equivalence of Program Machines with General-recursive functions" he introduces two new subroutines:
:f. [ → ]

:j. [ ≠ ]
::Jump unless equal": IF [ r&lt;sub&gt;j&lt;/sub&gt; ] ≠ [ r&lt;sub&gt;k&lt;/sub&gt; ] THEN jump to zth instruction ELSE next instruction

He proceeds to show how to replace the "successor-predecessor" set { [ 0 ], [ ' ], [ - ] } with the "successor-equality" set { [ 0 ], [ ' ], [ ≠ ] }. And then he defines his "REPEAT" [RPT] and shows that we can define any [[primitive recursive function]] by the "successor-repeat" set { [ 0 ], [ ' ], [RPT] } (where the range of the [ RPT ] cannot include itself. If it does, we get what is called the [[mu operator]] (see also [[mu recursive function]]s ) (p.&amp;nbsp;213)):

:Any general recursive function can be computed by a program computer using only operations [ 0 ], [ ' ], [ RPT ] if we permit a RPT operation to lie in its own range ... [however] in general a RPT operation could not be an instruction in the finite-state part of the machine...[if it were] this might exhaust any particular amount of storage allowed in the finite part of the machine. RPT operations require infinite registers of their own, in general... etc." (p. 214)

=== 1980: Schönhage's 0-parameter model RAM0 ===
Schönhage (1980) developed his computational model in context of a "new" model he called the Storage Machine Modification model (SMM), his variety of [[pointer machine]]. His development described a RAM ([[Random-access machine]]) model with a remarkable instruction set requiring no operands at all, excepting, perhaps, the "conditional jump" (and even that could be achieved without an operand):

:"...the RAM0 version deserves special attention for its extreme simplicity; its instruction set consists of only a few one-letter codes, without any (explicit) addressing" (p. 494)

The way Schönhage did this is of interest. He (i) atomizes the conventional register "address:datum" into its two parts: "address", and "datum", and (ii) generates the "address" in a specific register '''n''' to which the finite-state machine instructions (i.e. the "machine code") would have access, and (iii) provides an "accumulator" register '''z''' where all arithmetic operations are to occur.

In his particular RAM0 model has only two "arithmetic operations"{{spaced ndash}}"Z" for "set contents of register '''z''' to zero", and "A" for "add one to contents of register '''z'''". The only access to address-register '''n''' is via a copy-from-A-to-N instruction called "set address '''n'''". To store a "datum" in accumulator '''z''' in a given register, the machine uses the contents of '''n''' to specify the register's address and register '''z''' to supply the datum to be sent to the register.
         
'''Peculiarities:''' A first peculiarity of the Schönhage RAM0 is how it "loads" something into register '''z''': register '''z''' first supplies the register-address and then secondly, receives the datum from the register{{spaced ndash}}a form of indirect "load". The second peculiarity is the specification of the COMPARE operation. This is a "jump if accumulator-register '''z'''=''zero'' (not, for example, "compare the contents of '''z''' to the contents of the register pointed to by '''n'''). Apparently if the test fails the machine skips over the next instruction which always must be in the form of "goto λ" where "λ" is the jump-to address. The instruction{{spaced ndash}}"compare contents of '''z''' to ''zero''" is unlike the Schonhage successor-RAM1 model (or any other known successor-models) with the more conventional "compare contents of register '''z''' to contents of register a for equality".

Primarily for reference{{spaced ndash}}this is a RAM model, not a counter-machine model{{spaced ndash}}the following is the Schönhage RAM0 instruction set:

{|class="wikitable"
|- style="font-size:9pt"
! width="16.8" Height="9.6" align="center" valign="bottom" |
! width="48.6" valign="bottom" | Instruction
! width="227.4" align="center" valign="bottom" | Action:
! width="429" align="center" valign="bottom" | Description:

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 1
| valign="bottom" | Z
| valign="bottom" | 0 → z
| valign="bottom" | Clear accumulator-register z

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 2
| valign="bottom" | A
| valign="bottom" | [ z ] + 1 → z
| valign="bottom" | Increment the contents of accumulator-register z

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 3
| valign="bottom" | N 
| valign="bottom" | [ z ] → n, [ z ] → z  
| valign="bottom" | "Set address n": copy contents of accumulator z into address-register n

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 4
| valign="bottom" | L 
| valign="bottom" | [ [ z ] ] → z
| valign="bottom" | Indirectly copy into accumulator z the contents of the register pointed to by accumulator z

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 5
| valign="bottom" | S
| valign="bottom" | [ z ] → [ n ] 
| valign="bottom" | Indirectly store the contents of accumulator z into the register pointed to by the contents of address-register n

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 6
| valign="bottom" | C
| valign="bottom" |  If [ z ] = 0 skip the next instruction (which must be a goto instruction I&lt;sub&gt;λ&lt;/sub&gt;) 
| valign="bottom" | If contents of accumulator z = 0 THEN skip next instruction else continue

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 7
| valign="bottom" | goto I&lt;sub&gt;λ&lt;/sub&gt;
| valign="bottom" | Unconditional goto (jump to) instruction I&lt;sub&gt;λ&lt;/sub&gt;
| valign="bottom" | Unconditional goto (jump to) instruction I&lt;sub&gt;λ&lt;/sub&gt;

|}

Again, the above instruction set is for a ''random-access machine'', a ''RAM''{{spaced ndash}}a counter machine with indirect addressing; instruction "N" allows for indirect storage of the accumulator, and instruction "L" allows for indirect load of the accumulator.

While peculiar, Schönhage's model shows how the conventional counter-machine's "register-to-register" or "read-modify-write" instruction set can be atomized to its simplest 0-parameter form. &lt;!-- One possible instruction set (now using more conventional mnemonics) might be the following set 1A through 8. This set is not minimimal{{spaced ndash}}we can dispense with at least one instruction. But the arithmetic operations are "symmetric" in the sense that what we do to A (e.g. 1A, 2A, 3A) we can do to N (e.g. 1N, 2N, 3N), with the exception of 8 and 9 which use the contents of N to address the register that provides the datum to A (LDAN) or accepts the datum from A (STAN): 
{|class="wikitable"
|- style="font-size:9pt"
! width="16.8" Height="9.6" align="center" valign="bottom" | 
! width="48.6" valign="bottom" | Instruction
! width="246" align="center" valign="bottom" | Action:
! width="390.6" align="center" valign="bottom" | Description:

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 1A
| valign="bottom" | DECA
| valign="bottom" | &lt; A &gt; - 1  →  &lt; A &gt;
| valign="bottom" | Decrement the contents of accumulator-register A

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 2A
| valign="bottom" | INCA
| valign="bottom" | &lt; A &gt; + 1  →  &lt; A &gt;
| valign="bottom" | Increment the contents of address-register A

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 3A
| valign="bottom" | JAZ xxx
| valign="bottom" |  If &lt; A &gt; = 0 jump to instruction xxx else continue in sequence
| valign="bottom" | Conditional jump if contents of accumlator equals 0

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 1N
| valign="bottom" | DECN
| valign="bottom" | &lt; N &gt; - 1 → &lt; N &gt;
| valign="bottom" | Decrement the contents of address-register N

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 2N
| valign="bottom" | INCN
| valign="bottom" | &lt; N &gt; + 1  →  &lt; A &gt;
| valign="bottom" | Increment the contents of accumulator-register A

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 3N
| valign="bottom" | JNZ xxx
| valign="bottom" |  If &lt; N &gt; = 0 jump to instruction xxx else continue in sequence
| valign="bottom" | Conditional jump if contents of address-register equals 0

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 7
| valign="bottom" | LDAN
| valign="bottom" | &lt;&lt; N &gt;&gt; → &lt; A &gt; 
| valign="bottom" | Load into accumulator A the contents of register x pointed to by the contents of address-register N

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | 8
| valign="bottom" | STAN
| valign="bottom" | &lt; A &gt; → &lt;&lt; N &gt;&gt; 
| valign="bottom" | Store the contents of accumulator A into register x pointed to by the contents of address-register n

|- style="font-size:9pt"
| Height="3" align="center" valign="bottom" | 
| valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" |

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | cnv
| valign="bottom" | CLRA
| valign="bottom" | 0 →  &lt; A &gt;
| valign="bottom" | Clear accumulator-register A to 0

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | cnv
| valign="bottom" | CLRN
| valign="bottom" | 0 → &lt; N &gt;
| valign="bottom" | Clear address-register N to 0

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | cnv
|style="background-color:#FFFF99" valign="bottom" | LDN k
| valign="bottom" | immediate-load constant "k" into N
| valign="bottom" | Load constant "k" into N for purposes of direct addressing of registers

|- style="font-size:9pt"
| Height="3" align="center" valign="bottom" | 
| valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" |

|- style="font-size:9pt"
| Height="9.6" align="center" valign="bottom" | indr
| valign="bottom" | CPYAN
| valign="bottom" | &lt; A &gt; → &lt; N &gt;
| valign="bottom" | For purposes of indirect addressing: copy contents of A into N

|}

Shepherdson-Sturgis (1963) have shown how similar instruction sets, expanded with "convenience instructions" such as "CLRA" and "CLRN", are possible. The very convenient instruction "LDN k"{{spaced ndash}}LoaD N with "immediate" constant k (actually a register address) obtained from the instruction"{{spaced ndash}}is useful when the number of registers is known and bounded to less than some k&lt;sub&gt;max&lt;/sub&gt;.

As discussed above, the CPYAN instruction is required for indirect addressing.

If the jump-to address were in its own register (or even in N) then the jump-to address/parameter could be dispensed with. But all of this atomization comes at a significant cost{{spaced ndash}}the program must generate a register-address by successive INCN instructions, or a register-address by successive "INC_Jaddress", for example. --&gt;

[[Category:Models of computation]]</text>
      <sha1>ia1mejdqrrclrobo22w1dtd1dye78qq</sha1>
    </revision>
  </page>
  <page>
    <title>DEVS</title>
    <ns>0</ns>
    <id>14093130</id>
    <revision>
      <id>793902562</id>
      <parentid>784110347</parentid>
      <timestamp>2017-08-04T17:23:20Z</timestamp>
      <contributor>
        <username>Un11imig</username>
        <id>6233136</id>
      </contributor>
      <comment>/* Other Formalisms */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19538">'''DEVS''' abbreviating '''Discrete Event System Specification''' is a modular and hierarchical formalism for modeling and analyzing general systems that can be discrete event systems which might be described by [[state transition table]]s, and continuous state systems which might be described by [[differential equation]]s, and hybrid continuous state and discrete event systems. DEVS is a [[timed event system]].

==History==
DEVS is a formalism for modeling and analysis of discrete event systems (DESs). The DEVS formalism was invented by [[Bernard P. Zeigler]], who is emeritus professor at the [[University of Arizona]]. DEVS was introduced to the public in Zeigler's first book, [http://acims.asu.edu/publications/books-proceedings-and-manuscripts/theory-of-modeling-and-simulation-2nd-edition-academic-press ''Theory of Modeling and Simulation''], in 1976, while Zeigler was an associate professor at [[University of Michigan]]. DEVS can be seen as an extension of the [[Moore machine]] formalism,&lt;ref&gt;automata were the mathematical models of Dr. Zeigler's Ph.D. thesis [[DEVS#References|[Zeigler68]]]&lt;/ref&gt; which is a finite state automaton where the outputs are determined by the current state alone (and do not depend directly on the input). The extension was done by   
# associating a lifespan with each state [[DEVS#References|[Zeigler76]]],
# providing a hierarchical concept with an operation, called ''coupling'' [[DEVS#References|[Zeigler84]]].

Since the lifespan of each state is a real number (more precisely, non-negative real) or infinity, it is distinguished from discrete time systems, sequential machines, and [[Moore machine]]s, in which time is determined by a tick time multiplied by non-negative integers. Moreover, the lifespan can be a [[random variable]]; for example the lifespan of a given state can be distributed [[Exponential distribution|exponentially]] or [[Uniform distribution (continuous)|uniformly]]. The state transition and output functions of DEVS can also be [[stochastic]].

Zeigler proposed a hierarchical algorithm for DEVS model simulation in 1984 [[DEVS#References|[Zeigler84]]] which was published in ''Simulation'' journal in 1987. Since then, many extended formalism from DEVS have been introduced with their own purposes: DESS/DEVS for combined continuous and discrete event systems, P-DEVS for parallel DESs, G-DEVS for piecewise continuous state trajectory modeling of DESs, RT-DEVS for realtime DESs, Cell-DEVS for cellular DESs, Fuzzy-DEVS for fuzzy DESs, Dynamic Structuring DEVS for DESs changing their coupling structures dynamically, and so on. In addition to its extensions, there are some subclasses such as [[SP-DEVS]] and [[FD-DEVS]] have been researched for achieving decidability of system properties.

Due to the modular and hierarchical modeling views, as well as its simulation-based analysis capability, the DEVS formalism and its variations have been used in many application of engineering (such as hardware design, hardware/software codesign, [[communications system]]s, [[manufacturing]] systems) and science (such as [[biology]], and [[sociology]])

==Formalism==
[[File:FD-DEVS PINGPONG.JPG|thumb|right|400px|Fig. 1. A DEVS Model for Ping-Pong Game]]
; Intuitive Example
DEVS defines system behavior as well as system structure. System behavior in DEVS formalism is described using input and output events as well as states. For example, for the ping-pong player of Fig. 1, the input event is ''?receive'', and the output event is ''!send''. Each player, ''A'', ''B'', has its states: ''Send'' and ''Wait''. ''Send'' state takes 0.1 seconds to send back the ball that is the output event ''!send'', while the ''Wait'' state lasts until the player receives the ball that is the input event ''?receive''.

The structure of ping-pong game is to connect two players: Player ''A'' 's output event ''!send'' is transmitted to Player ''B'' 's input event ''?receive'', and vice versa.

In the classic DEVS formalism, ''Atomic DEVS'' captures the system behavior, while ''Coupled DEVS'' describes the structure of system.

The following formal definition is for Classic DEVS [[DEVS#References|[ZKP00]]]. In this article, we will use the time base, &lt;math&gt; \mathbb{T}=[0,\infty)&lt;/math&gt; that is the set of non-negative real numbers; the extended time base,&lt;math&gt; \mathbb{T}^\infty=[0,\infty]&lt;/math&gt; that is the set of non-negative real numbers plus infinity.

===Atomic DEVS===
An atomic DEVS model is defined as a 7-[[tuple]] &lt;br /&gt;
   &lt;center&gt;&lt;math&gt;M=&lt;X,Y,S,ta, \delta_{ext}, \delta_{int}, \lambda&gt;&lt;/math&gt; &lt;/center&gt;

where 
&lt;blockquote&gt;
*&lt;math&gt;X&lt;/math&gt; is ''the set of input events'';
*&lt;math&gt;Y&lt;/math&gt; is ''the set of output events'';
*&lt;math&gt;S&lt;/math&gt; is ''the set of sequential states'' (or also called ''the set of partial states'');
*&lt;math&gt;s_0\in S&lt;/math&gt; is ''the initial state'';
*&lt;math&gt;ta:S \rightarrow \mathbb{T}^\infty&lt;/math&gt; is ''the time advance function'' which is used to determine the lifespan of a state;
*&lt;math&gt;\delta_{ext}:Q \times X \rightarrow  S &lt;/math&gt; is ''the external transition function'' which defines how an input event changes a state of the system, where &lt;math&gt;Q=\{(s,t_e)|s \in S, t_e \in (\mathbb{T} \cap [0, ta(s)])\}&lt;/math&gt; is the ''set of total states'', and &lt;math&gt;t_e&lt;/math&gt; is the ''elapsed time'' since ''the last event'';
&lt;ref&gt;We can also define the external transition function as &lt;math&gt;\delta_{ext}:Q \times X \rightarrow S \times \{0,1\} &lt;/math&gt; where &lt;math&gt; Q = S \times \mathbb{T}^\infty \times \mathbb{T} &lt;/math&gt; such that for a total state &lt;math&gt;(s, t_s, t_e) \in Q&lt;/math&gt;, &lt;math&gt;s \in S&lt;/math&gt; is a partial state, &lt;math&gt; t_s \in \mathbb{T}^\infty&lt;/math&gt; is the lifespan of &lt;math&gt;s&lt;/math&gt;, and &lt;math&gt; t_e \in (\mathbb{T} \cap [0,t_s])&lt;/math&gt; is the  elapsed time since ''last update'' of &lt;math&gt; t_s&lt;/math&gt;. For more how to understand this function, refer to the article, [[Behavior of DEVS]].&lt;/ref&gt;

&lt;br /&gt;
*&lt;math&gt;\delta_{int}:S \rightarrow S &lt;/math&gt; is ''the internal transition function'' which defines how a state of the system changes internally (when the elapsed time reaches to the lifetime of the state);
*&lt;math&gt;\lambda:S \rightarrow  Y^\phi&lt;/math&gt; is ''the output function'' where &lt;math&gt;Y^\phi=Y \cup \{\phi\}&lt;/math&gt; and &lt;math&gt; \phi \not\in Y&lt;/math&gt; is a ''silent'' event or an ''unobserved'' event. This function defines how a state of the system generates an output event (when the elapsed time reaches to the lifetime of the state);  &lt;/blockquote&gt;

; The atomic DEVS Model for Ping-Pong Players
The atomic DEVS model for player A of Fig. 1 is given 
Player=&lt;math&gt;&lt;X,Y,S,s_0,ta,\delta_{ext}, \delta_{int}, \lambda&gt;&lt;/math&gt; 
such that 
&lt;blockquote&gt;
&lt;math&gt;
\begin{align}
 X &amp;= \{?\textit{receive}\}\\
 Y &amp;= \{!\textit{send}\}\\
 S &amp;= \{(d,\sigma)| d \in \{\textit{Wait},\textit{Send}\}, \sigma \in \mathbb{T}^\infty\}\\
 s_0 &amp;= (\textit{Send}, 0.1)\\
 t_a(s) &amp;=\sigma \text{ for all } s \in S\\
\delta_{ext}(((\textit{Wait},\sigma),t_e),?\textit{receive})&amp;=(\textit{Send},0.1)\\
\delta_{int}(\textit{Send},\sigma)&amp;=(\textit{Wait},\infty)\\
\delta_{int}(\textit{Wait},\sigma)&amp;=(\textit{Send},0.1)\\
\lambda(\textit{Send},\sigma)&amp;=!\textit{send}\\
\lambda(\textit{Wait},\sigma)&amp;=\phi
\end{align}
&lt;/math&gt;
&lt;/blockquote&gt;

Both Player A and Player B are atomic DEVS models.

==== Behavior of Atomic DEVS ====
Simply speaking, there are two cases that an atomic DEVS model &lt;math&gt; M &lt;/math&gt; can change its state &lt;math&gt;s \in S&lt;/math&gt;: (1) when an external input &lt;math&gt; x \in X &lt;/math&gt; comes into the system &lt;math&gt; M &lt;/math&gt;; (2) when the elapsed time &lt;math&gt; t_e &lt;/math&gt; reaches the lifespan of &lt;math&gt;s&lt;/math&gt; which is defined by &lt;math&gt; ta(s) &lt;/math&gt;. (At the same time of (2), &lt;math&gt; M &lt;/math&gt; generates an output &lt;math&gt; y \in Y&lt;/math&gt; which is defined by &lt;math&gt; \lambda(s) &lt;/math&gt;.) .

For formal behavior description of given an Atomic DEVS model, refer to the page [[Behavior of DEVS]]. Computer algorithms to implement the behavior of a given Atomic DEVS model are available at [[Simulation Algorithms for Atomic DEVS]].

=== Coupled DEVS ===
The coupled DEVS defines which sub-components belong to it and how they are connected with each other. A coupled DEVS model is defined as an 8-[[tuple]]

&lt;center&gt;  &lt;math&gt;N=&lt;X,Y,D,\{M_i\},C_{xx}, C_{yx}, C_{yy}, Select&gt; &lt;/math&gt; &lt;/center&gt;

where
&lt;blockquote&gt;
*&lt;math&gt;X&lt;/math&gt; is ''the set of input events'';
*&lt;math&gt;Y&lt;/math&gt; is ''the set of output events'';
*&lt;math&gt;D&lt;/math&gt; is ''the name set of sub-components'';
*&lt;math&gt;\{M_i\}&lt;/math&gt; is ''the set of sub-components'' where for each &lt;math&gt;i \in D, M_i&lt;/math&gt; can be either an atomic DEVS model or a coupled DEVS model.
*&lt;math&gt;C_{xx}\subseteq X \times \bigcup_{i \in D} X_i&lt;/math&gt; is ''the set of external input couplings'';
*&lt;math&gt;C_{yx}\subseteq \bigcup_{i \in D} Y_i \times \bigcup_{i \in D} X_i&lt;/math&gt; is ''the set of internal couplings'';
*&lt;math&gt;C_{yy}: \bigcup_{i \in D} Y_i \rightarrow Y^\phi&lt;/math&gt; is ''the external output coupling function'';
*&lt;math&gt;Select:2^D \rightarrow D&lt;/math&gt; is ''the tie-breaking function'' which defines how to select the event from the set of simultaneous events;
&lt;/blockquote&gt;

; The coupled DEVS model for Ping-Pong Game
The ping-pong game of Fig. 1 can be modeled as a coupled DEVS model &lt;math&gt; N=&lt;X,Y,D,\{M_i\},C_{xx}, C_{yx}, C_{yy}, Select&gt;&lt;/math&gt; where &lt;math&gt;X=\{\} &lt;/math&gt;;&lt;math&gt;Y=\{\} &lt;/math&gt;;&lt;math&gt;D=\{A,B\} &lt;/math&gt;; &lt;math&gt;M_A \text{ and } M_B &lt;/math&gt; is described as above; &lt;math&gt;C_{xx}=\{\}&lt;/math&gt;; &lt;math&gt;C_{yx}=\{(A.!send, B.?receive), (B.!send, A.?receive)\}&lt;/math&gt;; and &lt;math&gt;C_{yy}(A.!send)=\phi, C_{yy}(B.!send)=\phi&lt;/math&gt;.

==== Behavior of Coupled DEVS ====
Simply speaking, like the behavior of the atomic DEVS class, a coupled DEVS model &lt;math&gt; N &lt;/math&gt; changes its components' states (1) when an external event &lt;math&gt;x \in X&lt;/math&gt; comes into &lt;math&gt; N &lt;/math&gt;; (2) when one of components &lt;math&gt; M_i &lt;/math&gt; where &lt;math&gt; i \in D &lt;/math&gt; executes its internal state transition and generates its output &lt;math&gt; y_i \in Y_i&lt;/math&gt;. In both cases (1) and (2), a triggering event is transmitted to all influencees which are defined by coupling sets &lt;math&gt; C_{xx}, C_{yx},&lt;/math&gt; and &lt;math&gt; C_{yy} &lt;/math&gt;.

For formal definition of behavior of the coupled DEVS, you can refer to [[Behavior of Coupled DEVS]]. Computer algorithms to implement the behavior of a given coupled DEVS mode are available at [[Simulation Algorithms for Coupled DEVS]].

==Analysis Methods==

=== Simulation for Discrete Event Systems ===
The simulation algorithm of DEVS models considers two issues: time synchronization and message propagation. ''Time synchronization'' of DEVS is to control all models to have the identical current time. However, for an efficient execution, the algorithm makes the current time jump to the most urgent time when an event is scheduled to execute its internal state transition as well as its output generation. ''Message propagation'' is to transmit a triggering message which can be either an input or output event along the associated couplings which are defined in a coupled DEVS model. For more detailed information, the reader can refer to [[Simulation Algorithms for Atomic DEVS]] and [[Simulation Algorithms for Coupled DEVS]].

===Simulation for Continuous State Systems===
By introducing a quantization method which abstracts a continuous segment as a piecewise const segment, DEVS can simulate behaviors of continuous state systems which are described by networks of [[differential algebraic equation]]s. This research has been initiated by Zeigler in 90's&lt;ref&gt;the use of quantized values in order to simulate continuous systems by means of a [[Discrete event simulation|discrete event]] method was empirically tried out a few years sooner - in the early 90's - by a [[Engineering education#France|French]] [[engineer]] &lt;We need any reference for this argument&gt;. He was then working for a company spun off from [[University of Valenciennes and Hainaut-Cambresis]], and now part of the [[Schneider Electric]]. This ''quantization'' is a feature of a [[simulation]] [[Application software|software]] of which this engineer is the conceptor and main [[Software developer|developer]], that is used for [[Programmable logic controller|PLC]] programs checking and operator training.&lt;/ref&gt; and many properties have been clarified by Prof. Kofman in 2000's and Dr. Nutaro. In 2006, Prof. Cellier who is the author of ''Continuous System Modeling''[[DEVS#References|[Cellier91]]], and Prof. Kofman wrote a text book, ''Continuous System Simulation''[[DEVS#References|[CK06]]] in which Chapters 11 and 12 cover how DEVS simulates continuous state systems. Dr. Nutaro's book [[DEVS#References|[Nutaro10]]], covers the discrete event simulation of continuous state systems too.

=== Verification for Discrete Event Systems ===
As an alternative analysis method against the sampling-based simulation method, an exhaustive generating behavior approach, generally called ''verification'' has been applied for analysis of DEVS models. It is proven that infinite states of a given DEVS model (especially a coupled DEVS model ) can be abstracted by behaviorally isomorphic finite structure, called a ''reachability graph'' when the given DEVS model is a sub-class of DEVS such as Schedule-Preserving DEVS ([[SP-DEVS]]), Finite &amp; Deterministic DEVS ([[FD-DEVS]]) [[DEVS#References|[HZ09]]], and Finite &amp; Real-time DEVS (FRT-DEVS) [[DEVS#References|[Hwang12]]]. As a result, based on the rechability graph, (1) dead-lock and live-lock freeness as qualitative properties are decidable with SP-DEVS [[DEVS#References|[Hwang05]]], FD-DEVS [[DEVS#References|[HZ06]]], and FRT-DEVS [[DEVS#References|[Hwang12]]]; and (2) min/max processing time bounds as a quantitative property are decidable with SP-DEVS so far by 2012.

==Variations of DEVS==

===Extensions (Superclassing)===
{{Expand section|date=November 2007}}
Numerous extensions of the classic DEVS formalism have been developed in the last decades.
Among them formalisms which allow to have changing model structures while the simulation time evolves.

G-DEVS [[DEVS#References|[Giambiasi01]]][[DEVS#References|[Zacharewicz08]]], Parallel DEVS, Dynamic Structuring DEVS, Cell-DEVS [[DEVS#References|[Wainer09]]], dynDEVS, Fuzzy-DEVS, GK-DEVS, ml-DEVS, Symbolic DEVS, Real-Time DEVS, rho-DEVS

===Restrictions (Subclassing)===
There are some sub-classes known as Schedule-Preserving DEVS ([[SP-DEVS]]) and Finite and Deterministic DEVS ([[FD-DEVS]]) which were designated to support verification analysis.
[[SP-DEVS]] and [[FD-DEVS]] whose expressiveness are ''E''([[SP-DEVS]]) &lt;math&gt;\subset&lt;/math&gt; ''E''([[FD-DEVS]])&lt;math&gt;\subset&lt;/math&gt; ''E''(DEVS) where ''E''(''formalism'') denotes the expressiveness of ''formalism''.

== See also ==

===DEVS Related Articles ===
*[[Event Segment]]
*[[Timed Event System]]
* Verifiable sub-classes of DEVS: [[SP-DEVS]], [[FD-DEVS]]
*[[Behavior of Atomic DEVS]]
*[[Behavior of Coupled DEVS]]
*[[Simulation Algorithms for Atomic DEVS]]
*[[Simulation Algorithms for Coupled DEVS]]

===Other Formalisms===
*[[Automata Theory]]: a formal method for state transition systems
*[[Finite State Machine]]: a state transition machine with finite sets of events and states
*[[Petri Nets]]: a graphical representation of state and transition relations
*[[Markov Chain]]: a [[stochastic]] process in which the future will be determined by the current state
*[[Specification and Description Language]]: SDL, a formal complete and unambiguous language to represent graphically simulation models.

== Footnotes ==
&lt;references /&gt;

==References==
* [Cellier91] {{cite book|author = Francois E. Cellier | year = 1991| title = Continuous System Modeling| publisher = Springer| isbn =978-0-387-97502-3  |edition=first}}
* [CK06] {{cite book|author1=Francois E. Cellier |author2=Ernesto Kofman | year = 2006| title = Continuous System Simulation| publisher = Springer| isbn = 978-0-387-26102-7 |edition=first}}
* [Giambiasi01] Giambiasi N., Escude B. Ghosh S. “Generalized Discrete Event Simulation of Dynamic Systems”, in: Issue 4 of SCS Transactions: Recent Advances in DEVS Methodology-part II, Vol. 18, pp.&amp;nbsp;216–229, dec 2001
* [Hwang05] M.H. Hwang, "Tutorial: Verification of Real-time System Based on Schedule-Preserved DEVS", ''Proceedings of 2005 DEVS Symposium'', San Diego, Apr. 2-8, 2005, {{ISBN|1-56555-293-8}},
* [HZ06] M.H. Hwang and B. P. Zeigler, "A Modular Verification Framework using Finite and Deterministic DEVS", ''Proceedings of 2006 DEVS Symposium'', pp57–65, Huntsville, Alabama, USA,
* [HZ09] M.H. Hwang and B.P. Zeigler, "Reachability Graph of Finite and Deterministic DEVS Networks", ''IEEE Transactions on Automation Science and Engineering'', Volume 6, Issue 3, 2009, pp.&amp;nbsp;454–467,
* [Hwang12] M.H. Hwang, "Qualitative verification of finite and real-time DEVS networks", ''Proceedings of the 2012 Symposium on Theory of Modeling and Simulation - DEVS Integrative M&amp;S Symposium'', Article No. 43,
* [Mittal13] {{cite book|author1=Saurabh Mittal |author2=Jose L. Risco Martin | year = 2013| title = Netcentric System of Systems Engineering with DEVS Unified Process| publisher = CRC Press|edition=first|isbn = 978-1439827062}}
* [Nutaro10] {{cite book|author = James Nutaro| year = 2010| title = Building Software for Simulation: Theory, Algorithms, and Applications in C++| publisher = Wiley|edition=first|isbn = 0-470-41469-3}}
* [Sarjoughian09] {{cite journal|author1=Hessam S. Sarjoughian |author2=Vignesh Elamvazhuthi | year = 2009| title = CoSMoS: A Visual Environment for Component-Based Modeling, Experimental Design, and Simulation| publisher = Proceedings of the International Conference on Simulation Tools and Techniques}}
* [Wainer09] {{cite book|author = Gabriel A. Wainer | year = 2009| title= Discrete-Event Modeling and Simulation: A Practitioner's Approach | publisher = CRC Press | isbn = 978-1-4200-5336-4 | edition = first}}
* [Wainer10] {{cite book|author = Gabriel A. Wainer and Pieter Mosterman Eds. | year = 2010| title= Discrete-Event Modeling and Simulation: Theory and Applications | publisher = CRC Press | isbn = 978-1-4200-7233-4 | edition = first}}
* [Zacharewicz08] Gregory Zacharewicz, Claudia Frydman, and Norbert Giambiasi (2008) G-DEVS/HLA Environment for Distributed Simulations of Workflows, SIMULATION May 2008 84: 197-213, doi:10.1177/0037549708092833.
* [Zeiger68] {{cite book|author = Bernard Zeigler | year = 1968| title = On the Feedback Complexity of Automata| publisher = University of Michigan | id = |edition=Ph.D. Thesis}}
* [Zeigler76] {{cite book|author = Bernard Zeigler | year = 1976| title = Theory of Modeling and Simulation| publisher = Wiley Interscience, New York  | id = |edition=first|isbn = 0-12-778455-1}}
* [Zeigler84] {{cite book|author = Bernard Zeigler | year = 1984| title = Multifacetted Modeling and Discrete Event Simulation | publisher = Academic Press, London; Orlando | isbn = 978-0-12-778450-2}}
* [Zeigler87] {{cite journal|author = Bernard Zeigler|year =1987| title= Hierarchical, modular discrete-event modelling in an object-oriented environment | journal = Simulation | volume = 49|issue = 5 | pages=219&amp;ndash;230| doi= 10.1177/003754978704900506}}
* [ZKP00] {{cite book|author1=Bernard Zeigler |author2=Tag Gon Kim |author3=Herbert Praehofer | year = 2000| title = Theory of Modeling and Simulation| publisher = Academic Press, New York  | isbn= 978-0-12-778455-7 |edition=second}}

{{DEFAULTSORT:Devs}}
[[Category:Automata (computation)]]
[[Category:Formal specification languages]]</text>
      <sha1>mb3zvexxybw9ecj4fxqpigheroamrw2</sha1>
    </revision>
  </page>
  <page>
    <title>Excess-2048</title>
    <ns>0</ns>
    <id>57914837</id>
    <redirect title="Offset binary" />
    <revision>
      <id>850519261</id>
      <timestamp>2018-07-16T11:35:34Z</timestamp>
      <contributor>
        <username>Matthiaspaul</username>
        <id>13467261</id>
      </contributor>
      <comment>[[WP:AES|←]]Redirected page to [[Offset binary#Excess-2048]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="181">#REDIRECT [[Offset binary#Excess-2048]]

{{Redirect category shell|1=
{{R to related topic}}
{{R with possibilities}}
}}

[[Category:Binary arithmetic]]
[[Category:Numeral systems]]</text>
      <sha1>hoyqglttbfw4fu8q8d1uy8xrg6elh6w</sha1>
    </revision>
  </page>
  <page>
    <title>Fangcheng (mathematics)</title>
    <ns>0</ns>
    <id>52815653</id>
    <revision>
      <id>856503637</id>
      <parentid>853406506</parentid>
      <timestamp>2018-08-25T18:04:09Z</timestamp>
      <contributor>
        <username>Runawayangel</username>
        <id>7340759</id>
      </contributor>
      <minor/>
      <comment>/* Additional readings */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8415">'''Fangcheng''' (sometimes written as '''fang-cheng''' or  '''fang cheng''') ({{zh|c=方程|p=fāng chéng}}) is the title of the eighth chapter of the [[Chinese mathematics|Chinese mathematical]] classic [[Jiuzhang suanshu]]  (The Nine Chapters on the Mathematical Art) composed by several generations of scholars who flourished during the period from the 10th to the 2nd century BCE. This text is one of the earliest surviving mathematical texts from China. Several historians of Chinese mathematics have observed that the term ''fangcheng'' is not easy to translate exactly.&lt;ref name="Hist01"&gt;{{cite book |author=Jean-Clause Martzloff |title=A History of Chinese Mathematics |date=2006 |publisher=Springer |page=250}}&lt;/ref&gt;&lt;ref name="Hart01"&gt;{{cite book |author=Roger Hart |title=The Chinese Roots of Linear Algebra |date=2011 |publisher=The Johns Hopkins University Press |url=https://muse.jhu.edu/chapter/322683 |accessdate=6 December 2016}}&lt;/ref&gt; However, as a first approximation it has been translated as "[[Matrix (mathematics)|rectangular arrays]]" or "square arrays".&lt;ref name=Hist01/&gt; The term is also used to refer to a particular procedure for solving a certain class of problems discussed in the Chapter 8 of The Nine Chapters book.&lt;ref name=Hart01/&gt;

The procedure referred to by the term ''fangcheng'' and explained in the eighth chapter of The Nine Chapters, is essentially a procedure to find the solution of systems of ''n'' equations in ''n'' unknowns and is equivalent to certain similar procedures in modern [[linear algebra]]. The earliest recorded ''fangcheng'' procedure is similar to what we now call [[Gaussian elimination]].

The ''fangcheng'' procedure was popular in ancient China and was transmitted to [[Japan]]. It is possible that this procedure was transmitted to [[Europe]] also and served as precursors of the modern theory of [[Matrix (mathematics)|matrices]], [[Gaussian elimination]], and [[determinant]]s.&lt;ref name="Hart02"/&gt; It is well known that there was not much work on linear algebra in [[Greece]] or [[Europe]] prior to [[Gottfried Leibniz]]’s studies of [[Elimination theory|elimination]] and determinants, beginning in 1678. Moreover Leibniz was a [[Sinophile]] and was interested in the translations of such Chinese texts as were available to him.&lt;ref name="Hart02"&gt;{{cite book |author=Roger Hart|title=The Chinese Roots of Linear Algebra |date=2011 |publisher=The Johns Hopkins University Press |url=https://muse.jhu.edu/chapter/322679 |accessdate=6 December 2016}}&lt;/ref&gt;

==On the meaning of ''fangcheng''==

There is no ambiguity in the meaning of the first character ''fang''. It means “rectangle” or “square.” But different interpretations are given to the second character ''cheng'':&lt;ref name=Hart01/&gt;

#The earliest extant commentary, by [[Liu Hui]], dated 263 CE defines ''cheng'' as "measures,” citing the non-mathematical term ''kecheng'', which means “collecting taxes according to tax rates.” Liu then defines ''fangcheng'' as a “rectangle of measures.” The term ''kecheng'', however, is not a mathematical term and it appears nowhere else in the Nine Chapters.  Outside of mathematics, ''kecheng'' is a term most commonly used for collecting taxes. 
#Li Ji’s "Nine Chapters on the Mathematical Arts: Pronunciations and Meanings" also glosses ''cheng'' as "measure," again using a nonmathematical term, ''kelü'', commonly used for taxation. This is how Li Ji defines ''fangcheng'': "''Fang'' means [on the] left and right. ''Cheng'' means terms of a ratio. Terms of a ratio [on the] left and right, combining together numerous objects, therefore [it] is called a "rectangular array"." 
#[[Yang Hui]]’s "Nine Chapters on the Mathematical Arts with Detailed Explanations" defines ''cheng'' as a general term for measuring weight, height, and length. Detailed Explanations states:  What is called “rectangular” (''fang'') is the shape of the numbers; “measure” (''cheng'') is the general term for [all forms of] measurement, also a method for equating weights, lengths, and volumes, especially referring to measuring clearly and distinctly the greater and lesser.

Since the end of the 19th century, in Chinese mathematical literature the term ''fangcheng'' has been used to denote an "equation." However, as already been noted, the traditional meaning of the term is very different from "equation."

==Contents of the chapter titled ''Fangcheng''==

The eighth chapter titled ''Fangcheng'' of the ''Nine Chapters'' book contains 18 problems. (There are a total of 288 problems in the whole book.) Each of these 18 problems reduces to a problem of solving a system of simultaneous linear equations. Except for one problem, namely Problem 13, all the problems are determinate in the sense that the number of unknowns is same as the number of equations.  There are problems involving 2, 3, 4 and 5 unknowns. The table below shows how many unknowns are there in the various problems:

&lt;center&gt;
'''Table showing the number of unknowns and number of equations &lt;br&gt; in the various problems in Chapter 8 of ''Nine Chapters'' '''
{| class="wikitable" style="Text-align: center"
|-
! Number of unknowns&lt;br&gt; in the problem !! Number of equations&lt;br&gt; in the problem !! Serial numbers of problems !! Number of problems || Determinacy
|-
| 2 || 2 ||2, 4, 5, 6, 7, 9, 10, 11  || 8 ||Determinate
|-
| 3 || 3 || 1, 3, 8, 12, 15, 16 || 6 ||Determinate
|-
| 4 || 4 || 14, 17 || 2 ||Determinate
|-
| 5 || 5 || 18 || 1 ||Determinate
|-
| 6 || 5 || 13 || 1 ||[[Indeterminate system|Indeterminate]]
|-
|  || || Total || 18
|}
&lt;/center&gt;

The presentations of all the 18  problems  (except Problem 1 and Problem 3) follow a common pattern:

*First the problem is stated.
*Then the answer to the problem is given.
*Finally the method of obtaining the answer is indicated.

===On Problem 1===
The presentation of Problem 1 contains a description (not a crisp indication)  of the procedure for obtaining the solution. The procedure has been referred to as ''fangcheng shu'', which means "''fangcheng'' procedure." The remaining problems all give the instruction "follow the ''fangcheng''" procedure sometimes followed by the instruction to use the "procedure for positive and negative numbers".

===On Problem 3===
There is also a special procedure, called "procedure for positive and negative numbers" (''zhenh fu shu'') for handling negative numbers. This procedure is explained as part of the method for solving Problem 3.

===On Problem 13=== 
In the collection of these 18 problems Problem 13 is very special. In it there are 6 unknowns but only 5 equations and so Problem 13 is indeterminate and does not have a unique solution.  This is the earliest known reference to a system of linear equations in which the number of unknowns exceeds the number of equations. As per a suggestion of Jean-Claude Martzloff, a historian of Chinese mathematics, Roger Hart has named this problem "the well problem."

==References==
{{reflist}}

==Further reading==
*{{cite journal|author=Christine Andrews-Larson|title=Roots of Linear Algebra: An Historical Exploration of Linear Systems|journal=PRIMUS|date=2015|volume=25|issue=6|pages=507–528|url=https://dx.doi.org/10.1080/10511970.2015.1027975|accessdate=6 December 2016}}
*{{cite book|author=Kangshen Shen|author2=John N. Crossley|author3=Anthony Wah-Cheung Lun, Hui Liu|title=The Nine Chapters on the Mathematical Art: Companion and Commentary|date=1999|publisher=Oxford University Press|isbn=9780198539360|pages=386–440|url=https://books.google.co.in/books?id=eiTJHRGTG6YC&amp;pg=PA1&amp;lpg=PA1&amp;dq=nine+chapters+of+mathematical+art&amp;source=bl&amp;ots=i5RaRayT5L&amp;sig=tO2daShj5Ey0TlOw5YD_Dl5LIGk&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiJtOejvOLQAhWsIMAKHSjHDrY4FBDoAQgeMAE#v=onepage&amp;q=nine%20chapters%20of%20mathematical%20art&amp;f=false|accessdate=7 December 2016}}
*For an investigation into the possibility of teaching ''fangcheng'' to European children: {{cite journal|author=Cecília Costa|title=Potentialities on the Western Education of the Ancient Chinese Method to Solve Linear Systems of Equations|journal=Applied Mathematical Sciences|date=2014|volume=8|issue=36|pages=1789–1798|url=http://www.m-hikari.com/ams/ams-2014/ams-33-36-2014/costaAMS33-36-2014.pdf|accessdate=15 December 2016}}

[[Category:Chinese mathematics]]
[[Category:Linear algebra]]
[[Category:Numerical linear algebra]]
[[Category:Han dynasty texts]]</text>
      <sha1>m2kg3wjrsd6mqio1efox6ss4t5se48d</sha1>
    </revision>
  </page>
  <page>
    <title>Fractional-order integrator</title>
    <ns>0</ns>
    <id>1548123</id>
    <revision>
      <id>605807330</id>
      <parentid>542263854</parentid>
      <timestamp>2014-04-25T21:23:01Z</timestamp>
      <contributor>
        <ip>128.205.21.101</ip>
      </contributor>
      <comment>/* Analog devices */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3581">{{further|Fractional calculus}}
{{Unreferenced|date=June 2009}}
{{calculus|expanded=Specialized calculi}}
A '''fractional-order integrator''' or just simply '''fractional integrator''' is an [[integrator]] device that calculates the fractional-order integral or derivative (usually called a [[differintegral]]) of an input. Differentiation or integration is a real or complex parameter. The fractional integrator is useful in [[fractional-order control]] where the history of the system under control is important to the control system output.

== Overview ==
The [[differintegral]] function,

:&lt;math&gt;{}_a \mathbb{D}^q_t \left( f(x) \right)&lt;/math&gt;

includes the integer order differentiation and integration functions, and allows a continuous range of functions around them. The differintegral parameters are ''a'', ''t'', and ''q''. The parameters ''a'' and ''t'' describe the range over which to compute the result. The differintegral parameter ''q'' may be any real number or complex number. If ''q'' is greater than zero, the differintegral computes a derivative. If ''q'' is less than zero, the differintegral computes an integral.
The integer order integration can be computed as a [[Riemann–Liouville differintegral]], where the weight of each element in the sum is the constant unit value 1, which is equivalent to the [[Riemann sum]]. To compute an integer order derivative, the weights in the summation would be zero, with the exception of the most recent data points, where (in the case of the first unit derivative) the weight of the data point at ''t''&amp;nbsp;−&amp;nbsp;1 is −1 and the weight of the data point at ''t'' is&amp;nbsp;1. The sum of the points in the input function using these weights results in the difference of the most recent data points.
These weights are computed using ratios of the [[Gamma function]] incorporating the number of data points in the range [''a'',''t''], and the parameter&amp;nbsp;''q''.

== Digital devices ==
Digital devices have the advantage of being versatile, and are not susceptible to unexpected output variation due to heat or noise. The discrete nature of a computer however, does not allow for all of history to be computed. Some finite range [a,t] must exist. Therefore, the number of data points that can be stored in memory (''N''), determines the oldest data point in memory, so that the value a is never more than ''N'' samples old. The effect is that any history older than a is ''completely'' forgotten, and no longer influences the output.

A solution to this problem is the [[Coopmans approximation]], which allows old data to be forgotten more gracefully (though still with exponential decay, rather than with the power law decay of a purely [[analog device]]).

== Analog devices ==
Analog devices have the ability to retain history over longer intervals. This translates into the parameter a staying constant, while ''t'' increases. 

There is no [[Round-off error|error due to round-off]], as in the case of digital devices, but there may be error in the device due to [[Leakage (electronics)|leakage]]s, and also unexpected variations in behavior caused by heat and noise.

An example fractional-order integrator is a modification of the standard [[integrator circuit]], where a [[capacitor]] is used as the [[feedback impedance]] on an [[opamp]]. By replacing the capacitor with an [[RC Ladder]] circuit, a half order integrator, that is, with

:&lt;math&gt;q = -\frac{1}{2},&lt;/math&gt;

can be constructed.

==See also==

*[[Signal analysis]]
*[[Fourier series]]

[[Category:Cybernetics]]
[[Category:Fractional calculus]]</text>
      <sha1>smu63ylgqqfjknnv74hr8rz539u49t3</sha1>
    </revision>
  </page>
  <page>
    <title>French Institute for Research in Computer Science and Automation</title>
    <ns>0</ns>
    <id>318413</id>
    <revision>
      <id>870671723</id>
      <parentid>867701262</parentid>
      <timestamp>2018-11-26T09:17:26Z</timestamp>
      <contributor>
        <ip>128.93.83.119</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5980">{{primary sources|date=September 2016}}
{{Infobox organization 
 |name= Institut national de recherche en informatique et en automatique
 |formation= {{Start date and age|1967|01|03|df=yes|p=yes|br=yes}}
 |headquarters=[[Rocquencourt]], [[France]]
 |type=Public
 |purpose= Research 
 |image= File:INRIA logo.png
 |fields=[[Computer science]]&lt;br /&gt;[[Applied mathematics]]
 |languages=[[French language|French]], [[English language|English]]
 |budget=[[euro|€]]235 million (2013)
 |leader_title=President
 |leader_name=Bruno Sportisse
 |staff=1,772 researchers
 |website=[http://www.inria.fr/en/ inria.fr]
}}

The '''National Institute for Research in Computer Science and Automation''' ('''INRIA''') ({{lang fr|Institut national de recherche en informatique et en automatique}}) is a [[France|French]] national research institution focusing on [[computer science]] and [[applied mathematics]].
It was created under the name ''Institut de recherche en informatique et en automatique'' ('''IRIA''') in 1967 at [[Rocquencourt]] near [[Paris]], part of [[Plan Calcul]]. Its first site was the historical premises of [[Supreme Headquarters Allied Powers Europe|SHAPE]] (central command of [[NATO]] military forces). In 1979 IRIA became INRIA.&lt;ref&gt;[http://www.inria.fr/institut/inria-en-bref/histoire-d-inria Des balbutiements de l’informatique au règne du tout numérique - Inria].&lt;/ref&gt; Since 2011, it has been styled ''inria''.

Inria is a [[Public Scientific and Technical Research Establishment]] (EPST) under the double supervision of the French [[Minister of National Education (France)|Ministry of National Education, Advanced Instruction and Research]] and the [[Ministry of Economy, Finance and Industry]].

==Administrative status==
[[File:INRIA 2007-02-05.jpg|thumb|alt=Two chairs on a concrete porch of a rectilinear building overlooking hills and green forest|Part of INRIA in [[Valbonne]]]]
Inria has 8 research centers (in [[Bordeaux]], [[Grenoble]]-[[Inovallée]], [[Lille]], [[Nancy, France|Nancy]], [[Paris]]-[[Rocquencourt]], [[Rennes]], [[Plateau de Saclay|Saclay]], and [[Sophia Antipolis]]) and also contributes to academic research teams outside of those centers.

Before December 2007, the three centers of Bordeaux, Lille and Saclay formed a single research center called INRIA Futurs.

In October 2010, INRIA, with [[Pierre and Marie Curie University]] and [[Paris Diderot University]] started [[IRILL]], a center for innovation and research initiative for free software.

Inria employs 3800 people. Among them are 1300 researchers, 1000 Ph.D. students and 500 postdoctorates.

== Research ==
Inria does both [[Theoretical computer science|theoretical]] and applied research in computer science. In the process, it has produced many widely used programs, such as 

* [[Bigloo]], a [[Scheme (programming language)|Scheme]] implementation
* [[CADP]], a tool box for the verification of asynchronous [[concurrent systems]]
* [[Caml]], a language from the [[ML programming language|ML]] family
** [[Caml Light]] and [[OCaml]] implementations
* [[ChorusOS]], distributed operating system
* [[Contrail (software)|Contrail]]&lt;ref name="ISGTW"&gt;{{cite web |url = http://www.isgtw.org/announcement/contrail-project-proud-present-its-first-complete-set-interoperable-cloud-federation-to |title = The Contrail project is proud to present its first complete set of interoperable Cloud federation tools |archive-date = 2013-10-17 |archive-url = https://web.archive.org/web/20131017190902/http://www.isgtw.org/announcement/contrail-project-proud-present-its-first-complete-set-interoperable-cloud-federation-to |last = Versweyveld |first = Leslie |date = 30 October 2012 |website = International Science Grid This Week (ISGTW) |accessdate = 17 October 2013 |deadurl = yes |df =  }} &lt;/ref&gt;
* [[Coq]], a [[proof assistant]]
* [[Eigen (C++ library)]]
* [[Esterel]], a [[programming language]] for State Automata
* Geneauto — code-generation from model&lt;ref&gt;{{cite web|url=http://forge.scilab.org/index.php/p/geneauto-p/|title=Geneauto / P toolset - The P toolset includes a code generation and verification framework for the languages supported by the TOPCASED environment|publisher=[[Scilab]]}}&lt;/ref&gt;
* Graphite, a research platform for computer graphics, 3D modeling and numerical geometry
* medInria, a medical image processing software, popularly used for MRI images.&lt;ref&gt;{{cite web|url=https://med.inria.fr/|title=medInria}}&lt;/ref&gt;
* [[OpenVibe|OpenViBE]], a [[software platform]] dedicated to designing, testing and using [[brain-computer interfaces]].
* [[Pharo]], an open-source dynamic and reflective language influenced by Smalltalk [http://www.pharo.org].
* [[Scilab]], a numerical computation software package
* [[SimGrid]]
* [[SmartEiffel]], a free Eiffel compiler
* [[Simulation_Open_Framework_Architecture | SOFA]], an open source framework for multi-physics simulation with an emphasis on medical simulation.
* [[Tom (pattern matching language)|TOM]], a pattern matching language
* [https://visp.inria.fr/ ViSP], an open source visual servoing platform library 
* [[XtreemFS]]&lt;ref name="ISGTW" /&gt;
* [[XtreemOS]]

==References==
{{Reflist}}

==Further reading==
* {{cite book |first1= Alain |last1= Beltran |first2= Pascal |last2= Griset |title= Histoire d'un pionnier de l'informatique: 40 ans de recherche à l'Inria |trans-title= Story of a computer pioneer: 40 years of research at INRIA |publisher= EDP Sciences  |date= 2007 |isbn= 2-86883-806-5 |language= French }}

==External links==
*{{Official website|www.inria.fr/en/}} {{en icon}}

{{Scientific research in France}}

[[Category:Computer science research organizations]]
[[Category:Scientific agencies of the government of France]]
[[Category:Theoretical computer science]]
[[Category:Computer science institutes in France]]
[[Category:Members of the European Research Consortium for Informatics and Mathematics]]
[[Category:Information technology research institutes]]


{{France-university-stub}}</text>
      <sha1>hjdhiqx2lpp1awiftpipvjmuvioeaiv</sha1>
    </revision>
  </page>
  <page>
    <title>Hilbert's nineteenth problem</title>
    <ns>0</ns>
    <id>2336226</id>
    <revision>
      <id>840752011</id>
      <parentid>819046897</parentid>
      <timestamp>2018-05-11T23:00:08Z</timestamp>
      <contributor>
        <username>OAbot</username>
        <id>28481209</id>
      </contributor>
      <comment>[[Wikipedia:OABOT|Open access bot]]: add pmc identifier to citation with #oabot.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="27921">'''Hilbert's nineteenth problem''' is one of the 23 [[Hilbert problems]], set out in a list compiled in 1900 by [[David Hilbert]].&lt;ref&gt;See {{harv|Hilbert|1900}} or, equivalently, one of its translations.&lt;/ref&gt; It asks whether the solutions of regular problems in the calculus of variations are always [[analytic function|analytic]].&lt;ref&gt;"''Sind die Lösungen regulärer Variationsprobleme stets notwending analytisch?''" (English translation by [[Mary Frances Winston Newson]]:-"''Are the solutions of regular problems in the calculus of variations always necessarily analytic?''"), formulating the problem with the same words of {{harvtxt|Hilbert|1900|p=288}}.&lt;/ref&gt; Informally, and perhaps less directly, since Hilbert's concept of a "''regular variational problem''" identifies precisely a [[Calculus of variation|variational problem]] whose [[Euler–Lagrange equation]] is an [[elliptic partial differential equation]] with analytic coefficients.&lt;ref&gt;See {{harv|Hilbert|1900|pp=288–289}}, or the corresponding section on the nineteenth problem in any of its translation or reprint, or the subsection "[[Hilbert's nineteenth problem#The origins of the problem|The origins of the problem]]" in the historical section of this entry.&lt;/ref&gt; Hilbert's nineteenth problem, despite its seemingly technical statement, simply asks whether, in this class of [[partial differential equation]]s, any solution function inherits the relatively simple and well understood structure from the solved equation.

==History==

===The origins of the problem===
{{quote
|text= Eine der begrifflich merkwürdigsten Thatsachen in den Elementen der Theorie der analytischen Funktionen erblicke ich darin, daß es Partielle Differentialgleichungen giebt, deren Integrale sämtlich notwendig analytische Funktionen der unabhängigen Variabeln sind, die also, kurz gesagt, nur analytischer Lösungen fähig sind.&lt;ref&gt;English translation by Mary Frances Winston Newson:-"''One of the most remarkable facts in the elements of the theory of analytic functions appears to me to be this: that there exist partial differential equations whose integrals are all of necessity analytic functions of the independent variables, that is, in short, equations susceptible of none but analytic solutions''".&lt;/ref&gt;
|sign= [[David Hilbert]]
|source= {{harv|Hilbert|1900|p=288}}.
}}

David Hilbert presented the now called Hilbert's nineteen problem in his speech at the second [[International Congress of Mathematicians]].&lt;ref&gt;For a detailed historical analysis, see the relevant entry "[[Hilbert's problems]]".&lt;/ref&gt; In {{harv|Hilbert|1900|p=288}} he states that, in his opinion, one of the most remarkable facts of the theory of analytic functions is that there exist classes of partial differential equations which admit only such kind of functions as solutions, adducing [[Laplace's equation]], [[Liouville's equation]],&lt;ref&gt;Hilbert does not cite explicitly [[Joseph Liouville]] and considers the constant [[Gaussian curvature]] {{math|''K''}} as equal to {{math|-1/2}}: compare the relevant entry with {{harv|Hilbert|1900|p=288}}.&lt;/ref&gt; the [[minimal surface equation]] and a class of linear partial differential equations studied by [[Émile Picard]] as examples.&lt;ref&gt;Contrary to Liouville's work, Picard's work is explicitly cited by {{harvtxt|Hilbert|1900|loc=p. 288 and footnote 1 in the same page}}.&lt;/ref&gt; He then notes the fact that most of the partial differential equations sharing this property are the Euler–Lagrange equation of a well defined kind of variational problem, featuring the following three properties:&lt;ref name="Hilbertp288"&gt;See {{harv|Hilbert|1900|p=288}}.&lt;/ref&gt;
:{{EquationRef|1|(1){{spaces|5}}}}&lt;math&gt;{\iint F(p,q,z;x,y) dx dy} = \text{Minimum} \qquad 
\left[ \frac{\partial z}{\partial x}=p \quad;\quad \frac{\partial z}{\partial y}=q \right]&lt;/math&gt;,
:{{EquationRef|2|(2){{spaces|5}}}}&lt;math&gt;\frac{\partial^2 F}{\partial^2 p}\cdot\frac{\partial^2 F}{\partial^2 q} - \left(\frac{\partial^2 F}{{\partial p}{\partial q}}\right)^2 &gt; 0&lt;/math&gt;,
:{{EquationRef|3|(3){{spaces|5}}}} {{math|''F''}} is an analytic function of all its arguments {{math|''p'', ''q'', ''z'', ''x''}} and {{math|''y''}}.
Hilbert calls this kind of variational problem a "''regular variational problem''":&lt;ref&gt;"''Reguläres Variationsproblem''", in his exact words. Hilbert's definition of a regular variational problem is stronger than the currently used one, found, for example, in {{harv|Gilbarg|Trudinger|2001|p=289}}.&lt;/ref&gt; property {{EquationNote|(1)}} means that such kind of variational problems are [[minimum|minimum problems]], property {{EquationNote|(2)}} is the [[Elliptic partial differential equation|ellipticity condition]] on the Euler–Lagrange equations associated to the given [[Functional (mathematics)|functional]], while property {{EquationNote|(3)}} is a simple regularity assumption the function {{math|''F''}}.&lt;ref&gt;Since Hilbert considers all [[derivative]]s in the "classical", i.e. not in the [[Weak derivative|weak]] but in the [[strong derivative|strong]], sense, even before the statement of its analyticity in {{EquationNote|(3)}}, the function {{math|''F''}} is assumed to be at least {{math|{{SubSup|C||2}}}}, as the use of the [[Hessian determinant]] in {{EquationNote|(2)}} implies.&lt;/ref&gt; Having identified the class of problems to deal with, he then poses the following question:-"''... does every Lagrangian partial differential equation of a regular variation problem have the property of admitting analytic integrals exclusively?''"&lt;ref&gt;English translation by Mary Frances Winston Newson: [[#{{harvid|Hilbert|1900}}|Hilbert's (1900]], p. 288) precise words are:-"''... d. h. ob jede Lagrangesche partielle Differentialgleichung eines reguläres Variationsproblem die Eigenschaft at, daß sie nur analytische Integrale zuläßt''" ([[Italic type|Italics emphasis]] by Hilbert himself).&lt;/ref&gt; and asks further if this is the case even when the function is required to assume, as it happens for Dirichlet's problem on the [[potential theory|potential function]], boundary values which are continuous, but not analytic.&lt;ref name="Hilbertp288" /&gt;

===The path to the complete solution===
Hilbert stated his nineteenth problem as a [[regularity problem]] for a class of elliptic partial differential equation with analytic coefficients,&lt;ref name="Hilbertp288" /&gt; therefore the first efforts of the researchers who sought to solve it were directed to study the regularity of [[classical solution]]s for equations belonging to this class. For [[Continuously differentiable function|{{math|{{SubSup|C||3}}}}]] solutions Hilbert's problem was answered positively by {{harvs|txt|first=Sergei|last=Bernstein|authorlink=Sergei Natanovich Bernstein|year=1904}} in his thesis: he showed that {{math|{{SubSup|C||3}}}} solutions of nonlinear elliptic analytic equations in 2 variables are analytic. Bernstein's result was improved over the years by several authors, such as {{harvtxt|Petrowsky|1939}}, who reduced the differentiability requirements on the solution needed to prove that it is analytic. On the other hand, direct methods in the calculus of variations showed the existence of solutions with very weak differentiability properties. For many years  there was a gap between these results: the solutions that could be constructed were known to have square integrable second derivatives, which was not quite strong enough to feed into the machinery that could prove they were analytic, which needed continuity of first derivatives. This gap was filled independently by {{harvs|txt|author-link=Ennio de Giorgi|first=Ennio |last= De Giorgi|year1=1956|year2= 1957}}, and {{harvs|txt|first=John Forbes |last=Nash|author-link=John Forbes Nash|year1=1957|year2=1958}}. They were able to show the solutions had first derivatives that were [[Hölder continuous]], which by previous results implied that the solutions are analytic whenever the differential equation has analytic coefficients,  thus completing the solution of Hilbert's nineteenth problem.

===Counterexamples to various generalizations of the problem===
The affirmative answer to Hilbert's nineteenth problem given by Ennio De Giorgi and John Forbes Nash raised the question if the same conclusion holds also for Euler-lagrange equations of more general [[Functional (mathematics)|functional]]s: at the end of the [[sixties]], {{harvtxt|Maz'ya|1968}},&lt;ref&gt;See {{harv|Giaquinta|1983|p=59}}, {{harv|Giusti|1994|loc=p. 7 footnote 7 and p. 353}}, {{harv|Gohberg|1999|p=1}}, {{harv|Hedberg|1999|pp=10–11}}, {{harv|Kristensen|Mingione|2011|loc=p. 5 and p. 8}}, and {{harv|Mingione|2006|p=368}}.&lt;/ref&gt; {{harvtxt|De Giorgi|1968}} and {{harvtxt|Giusti|Miranda|1968}} constructed independently several [[counterexample]]s,&lt;ref&gt;See {{harv|Giaquinta|1983|pp=54–59}}, {{harv|Giusti|1994|loc=p. 7 and pp. 353}}.&lt;/ref&gt; showing that in general there is no hope to prove such kind of regularity results without adding further hypotheses.

Precisely, {{harvtxt|Maz'ya|1968}} gave several counterexamples involving a single elliptic equation of order greater than two with analytic coefficients:&lt;ref&gt;See {{harv|Hedberg|1999|pp=10–11}}, {{harv|Kristensen|Mingione|2011|loc=p. 5 and p. 8}} and {{harv|Mingione|2006|p=368}}.&lt;/ref&gt; for experts, the fact that such kind of equations could have nonanalytic and even nonsmooth solutions created a sensation.&lt;ref&gt;According to {{harv|Gohberg|1999|p=1}}.&lt;/ref&gt;

{{harvtxt|De Giorgi|1968}} and {{harvtxt|Giusti|Miranda|1968}} gave counterexamples showing that in the case when the solution is vector-valued rather than scalar-valued, it need not be analytic: the example of De Giorgi consists of an elliptic system with bounded coefficients, while the one of Giusti and Miranda has analytic coefficients.&lt;ref&gt;See {{harv|Giaquinta|1983|pp=54–59}} and {{harv|Giusti|1994|loc=p. 7, pp. 202–203 and pp. 317–318}}.&lt;/ref&gt; Later on, {{harvtxt|Nečas|1977}} provided other, more refined, examples for the vector valued problem.&lt;ref&gt;For more information about the work of [[Jindřich Nečas]] see the work of {{harvtxt|Kristensen|Mingione|2011|loc=§3.3, pp. 9–12}} and {{harv|Mingione|2006|loc=§3.3, pp. 369–370}}.&lt;/ref&gt;

==De Giorgi's theorem==
The key theorem proved by De Giorgi is an [[a priori estimate]] stating that if ''u'' is a solution of a suitable linear second order strictly elliptic PDE of the form
:&lt;math&gt; D_i(a^{ij}(x)D_ju)=0&lt;/math&gt;
and ''u'' has square integrable first derivatives, then ''u'' is Hölder continuous.

==Application of De Giorgi's theorem to Hilbert's problem==
Hilbert's problem asks whether the minimizers &lt;math&gt;w&lt;/math&gt; of an energy functional such as
:&lt;math&gt;\int_UL(Dw)\mathrm{d}x&lt;/math&gt;
are analytic. Here &lt;math&gt;w&lt;/math&gt; is a function on some compact set &lt;math&gt;U&lt;/math&gt; of '''R'''&lt;sup&gt;''n''&lt;/sup&gt;, &lt;math&gt;Dw&lt;/math&gt; is its [[gradient]] vector, and &lt;math&gt;L&lt;/math&gt; is the Lagrangian, a function of the derivatives of &lt;math&gt;w&lt;/math&gt; that satisfies certain growth, smoothness, and convexity conditions. The smoothness of &lt;math&gt;w&lt;/math&gt; can be shown using De Giorgi's theorem
as follows. The Euler–Lagrange equation for this variational problem is the non-linear equation
:&lt;math&gt; \Sigma_i(L_{p_i}(Dw))_{x_i} = 0&lt;/math&gt;
and differentiating this with respect to ''x''&lt;sub&gt;''k''&lt;/sub&gt; gives
:&lt;math&gt; \Sigma_i(L_{p_ip_j}(Dw)w_{x_jx_k})_{x_i} = 0&lt;/math&gt;
This means that ''u''=''w''&lt;sub&gt;''x''&lt;sub&gt;''k''&lt;/sub&gt;&lt;/sub&gt; satisfies the linear equation
:&lt;math&gt; D_i(a^{ij}(x)D_ju)=0&lt;/math&gt;
with 
:&lt;math&gt;a^{ij} = L_{p_ip_j}(Dw)&lt;/math&gt;
so by De Giorgi's result the solution ''w'' has Hölder continuous first derivatives.

Once ''w'' is known to have Hölder continuous (''n''+1)st derivatives for some ''n'' ≥ 0, then the coefficients ''a''&lt;sup&gt;''ij''&lt;/sup&gt; have Hölder continuous ''n''th derivatives, so a theorem of Schauder implies that the (''n''+2)nd derivatives are also Hölder continuous, so repeating this infinitely often shows that the solution ''w'' is smooth.

==Nash's theorem==

Nash gave a continuity estimate for solutions of the parabolic equation
:&lt;math&gt; D_i(a^{ij}(x)D_ju)=D_t(u)&lt;/math&gt;
where ''u'' is a bounded function of ''x''&lt;sub&gt;1&lt;/sub&gt;,...,''x''&lt;sub&gt;''n''&lt;/sub&gt;, ''t'' defined for ''t'' ≥ 0. From his estimate Nash was able to deduce a continuity estimate for solutions of the elliptic equation 
:&lt;math&gt; D_i(a^{ij}(x)D_ju)=0&lt;/math&gt; by considering the special case when ''u'' does not depend on ''t''.

==Notes==
{{reflist|29em}}

==References==
*{{Citation
 | last = Bernstein
 | first = S.
 | author-link = Sergei Natanovich Bernstein
 | title = Sur la nature analytique des solutions des équations aux dérivées partielles du second ordre
 | journal = [[Mathematische Annalen]]
 | issn = 0025-5831
 | volume = 59
 | pages = 20–76
 | year = 1904
 | language = French
 | url = http://www.digizeitschriften.de/dms/resolveppn/?PPN=GDZPPN00225977X
 | doi = 10.1007/BF01444746
 | jfm = 35.0354.01
}}.
*{{Citation
 | last=Bombieri
 | first=Enrico
 | author-link=Enrico Bombieri
 | editor-last =
 | editor-first =
 | editor2-last =
 | editor2-first =
 | contribution = Variational problems and elliptic equations
 | contribution-url = http://www.mathunion.org/ICM/ICM1974.1/Main/icm1974.1.0053.0064.ocr.pdf
 | title=Proceedings of the International Congress of Mathematicians, Vancouver, B.C., 1974, Vol. 1
 | series =ICM Proceedings
 | year=1975
 | pages=53–63
 | place= Montreal
 | publisher = Canadian Mathematical Congress
 | url = http://www.mathunion.org/ICM/ICM1974.1/
 | mr=0509259
 | zbl=0344.49002
}}. Reprinted in {{Citation
 | last=Bombieri
 | first=Enrico 
 | author-link=Enrico Bombieri
 | editor-last=Browder
 | editor-first=Felix E.
 | editor-link= Felix Browder
 | contribution = Variational problems and elliptic equations
 | contribution-url = 
 | title=Mathematical developments arising from Hilbert problems
 | series=[[Proceedings of Symposia in Pure Mathematics]]
 | volume=XXVIII
 | year=1976
 | pages=525–535
 | place=Providence, Rhode Island
 | publisher=[[American Mathematical Society]]
 | url=https://books.google.com/books?isbn=0821814281
 | isbn=978-0-8218-1428-4
 | mr=0425740
 | zbl=0347.35032
}}.
*{{Citation
 | last=De Giorgi
 | first=Ennio
 | author-link=Ennio De Giorgi
 | title=Sull'analiticità delle estremali degli integrali multipli
 | journal=Atti della Accademia Nazionale dei Lincei. Rendiconti. Classe di Scienze Fisiche, Matematiche e Naturali
 | series=Serie VIII,
 | volume=20
 | pages=438–441
 | year=1956
 | language=Italian
 | mr=0082045
 | zbl=0074.31503
}}. "''On the analyticity of extremals of multiple integrals''" (English translation of the title) is a short research announcement disclosing the results detailed later in {{harv|De Giorgi|1957}}. While, according to the [[#{{harvid|De Giorgi|2006}}|Complete list of De Giorgi's scientific publication (De Giorgi 2006]], p.&amp;nbsp;6), an English translation should be included in {{harv|De Giorgi|2006}}, it is unfortunately missing.
*{{Citation
 | last=De Giorgi
 | first=Ennio
 | title=Sulla differenziabilità e l'analiticità delle estremali degli integrali multipli regolari
 | journal=Memorie della Accademia delle Scienze di Torino. Classe di Scienze Fisiche, Matematicahe e Naturali.
 | series = Serie III,
 | volume=3
 | pages=25–43
 | year=1957
 | language=Italian
 | mr=0093649
 | zbl=0084.31901
}}. Translated in English as "''On the differentiability and the analyticity of extremals of regular multiple integrals''" in {{harv|De Giorgi|2006|pp=149–166}}.
*{{Citation
 | last=De Giorgi
 | first=Ennio
 | title=Un esempio di estremali discontinue per un problema variazionale di tipo ellittico
 | journal=[[Bollettino dell'Unione Matematica Italiana]] (4)
 | series = Serie IV, 
 | volume=1
 | pages=135–137
 | year=1968
 | language=Italian
 | mr=0227827
 | zbl=0084.31901
}}. Translated in English as "''An example of discontinuous extremals for a variational problem of elliptic type''" in {{harv|De Giorgi|2006|pp=285–287}}.
*{{Citation
 | last=De Giorgi
 | first=Ennio
 | editor-last=Ambrosio
 | editor-first=Luigi
 | editor-link=Luigi Ambrosio
 | editor2-last=Dal Maso
 | editor2-first=Gianni
 | editor2-link=Gianni Dal Maso
 | editor3-last=Forti
 | editor3-first=Marco
 | editor4-last=Miranda
 | editor4-first=Mario
 | editor4-link=Mario Miranda (mathematician)
 | editor5-last=Spagnolo
 | editor5-first=Sergio
 | title=Selected papers
 | place=Berlin–New York
 | publisher=[[Springer-Verlag]]
 | year=2006
 | pages=x+889
 | url=https://www.springer.com/mathematics/analysis/book/978-3-540-26169-8
 | isbn=978-3-540-26169-8
 | mr=2229237
 | zbl=1096.01015
}}.
*{{Citation
 | last = Giaquinta
 | first = Mariano
 | author-link = Mariano Giaquinta
 | title = Multiple integrals in the calculus of variations and nonlinear elliptic systems
 | place = Princeton, New Jersey
 | publisher =Princeton University Press
 | series = [[Annals of Mathematics Studies]]
 | volume = 105
 | year = 1983
 | pages = vii+297
 | url = https://books.google.com/books?id=JwSAewaYsdMC&amp;printsec=frontcover&amp;hl=it#v=onepage&amp;q&amp;f=true
 | isbn = 0-691-08330-4
 | mr = 0717034
 | zbl = 0516.49003
}}.
*{{Citation
 | last = Gilbarg
 | first = David
 | author-link = David Gilbarg
 | last2 = Trudinger
 | first2 = Neil S. 
 | author2-link = Neil Trudinger
 | title = Elliptic partial differential equations of second order
 | place = Berlin – Heidelberg – New York
 | publisher = Springer Verlag
 | series = Classics in Mathematics
 | origyear = 1998
 | year = 2001
 | edition = Revised 3rd printing of 2nd 
 | pages = xiv+517
 | url = https://books.google.com/books?id=eoiGTf4cmhwC&amp;printsec=frontcover&amp;hl=it&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=true
 | doi =
 | id =
 | isbn = 3-540-41160-7
 | mr = 1814364 
 | zbl = 1042.35002
}}.
*{{Citation
| last = Giusti
| first = Enrico
| author-link = Enrico Giusti
| title = Metodi diretti nel calcolo delle variazioni
| place = [[Bologna]]
| publisher = [[Unione Matematica Italiana]]
| year = 1994
| language = Italian
| series = Monografie Matematiche
| pages = VI+422
| url = 
| isbn = 
| mr = 1707291
| zbl = 0942.49002}}, translated in English as {{Citation
| title = Direct Methods in the Calculus of Variations
| place = [[River Edge, New Jersey]] – London – Singapore
| publisher = World Scientific Publishing
| year = 2003
| pages = viii+403
| url = https://books.google.com/books?id=FofhcvUZo9YC&amp;printsec=frontcover&amp;hl=it#v=onepage&amp;q&amp;f=true
| isbn = 981-238-043-4
| mr = 1962933
| zbl = 1028.49001
}}.
*{{Citation
 | last=Giusti
 | first=Enrico
 | author-link=Enrico Giusti
 | last2=Miranda
 | first2=Mario
 | author2-link=Mario Miranda (mathematician)
 | title=Un esempio di soluzioni discontinue per un problema di minimo relativo ad un integrale regolare del calcolo delle variazioni
 | journal=[[Bollettino dell'Unione Matematica Italiana]]
 | series = Serie IV, 
 | volume=2
 | pages=1–8
 | year=1968
 | language=Italian
 | mr=0232265
 | zbl=0155.44501
}}.
*{{Citation
| first = Israel
| last = Gohberg
| author-link = Israel Gohberg
| editor-last = Rossman
| editor-first = Jürgen
| editor2-last = Takáč
| editor2-first = Peter
| editor3-last = Wildenhain
| editor3-first = Günther
| contribution = Vladimir Maz'ya: Friend and Mathematician. Recollections
| contribution-url = 
| title = The Maz'ya anniversary collection. Vol. 1: On Maz'ya's work in functional analysis, partial differential equations and applications. Based on talks given at the conference, Rostock, Germany, August 31 – September 4, 1998
| series = Operator Theory. Advances and Applications
| volume = 109
| year = 1999
| pages = 1–5
| place = Basel
| publisher = Birkhäuser Verlag
| url = https://books.google.com/?id=9xPz9Mg2c_EC&amp;printsec=frontcover#v=onepage&amp;q
| mr = 1747861 
| zbl = 0939.01018
| isbn = 978-3-7643-6201-0
}}.
*{{Citation
 | first = Lars Inge 
 | last =Hedberg
 | author-link =
 | editor-last =Rossmann
 | editor-first =Jürgen
 | editor2-last =Takáč
 | editor2-first =Peter
 | editor3-last =Wildenhain
 | editor3-first =Günther
 | contribution =On Maz'ya's work in potential theory and the theory of function spaces
 | contribution-url =
 | title =The Maz'ya Anniversary Collection. Volume 1: On Maz'ya's work in functional analysis, partial differential equations and applications
 | series =109
 | volume =Operator Theory: Advances and Applicationsǘ
 | year =1999
 | pages =7–16
 | place =[[Basel]]
 | publisher =Birkhäuser Verlag
 | url =
 | doi =10.1007/978-3-0348-8675-8_2
 | mr =1747862
 | zbl =0939.31001
}}
*{{Citation
 | last = Hilbert
 | first = David
 | author-link = David Hilbert
 | title = Mathematische Probleme
 | journal = [[Nachrichten von der Königlichen Gesellschaft der Wissenschaften zu Göttingen, Mathematisch-Physikalische Klasse]]
 | issue = 3
 | pages = 253–297
 | year = 1900
 | language = German
 | url = http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=PPN252457811_1900&amp;DMDID=DMDLOG_0037
 | jfm =31.0068.03
}} (reprinted as {{Citation
 | title = Mathematische Probleme
 | journal = [[Archiv der Mathematik und Physik]]
 | series = dritte reihe
 | volume = 1
 | pages = 44–63 and 253–297
 | year = 1900
 | language = German
 | url = https://archive.org/stream/archivdermathem02unkngoog#page/n61/mode/1up
 | jfm = 32.0084.05
}}), translated in English by [[Mary Frances Winston Newson]] as {{Citation
 | last = Hilbert
 | first = David
 | author-link = David Hilbert
 | title = Mathematical Problems
 | journal = [[Bulletin of the American Mathematical Society]]
 | volume = 8
 | issue = 10
 | pages = 437–479
 | year = 1902
 | url = http://www.ams.org/journals/bull/1902-08-10/S0002-9904-1902-00923-3/
 | doi = 10.1090/S0002-9904-1902-00923-3
 | jfm = 33.0976.07
 | mr = 1557926
}} (reprinted as {{Citation
 | last = Hilbert
 | first = David
 | author-link = David Hilbert
 | title = Mathematical Problems
 | journal = [[Bulletin of the American Mathematical Society]]
 | series = New Series
 | volume = 37
 | issue = 4
 | pages = 407–436
 | year = 2000
 | url = http://www.ams.org/journals/bull/2000-37-04/S0273-0979-00-00881-8/
 | doi = 10.1090/S0273-0979-00-00881-8 
 | mr = 1779412
 | zbl = 0979.01028
}}), and in French (with additions of Hilbert himself) by M. L. Laugel as {{Citation
 | last = Hilbert
 | first = David
 | author-link = David Hilbert
 | editor-last = Duporcq
 | editor-first = E.
 | contribution = Sur les problèmes futurs des Mathématiques
 | contribution-url = http://www.mathunion.org/ICM/ICM1900/Main/icm1900.0058.0114.ocr.pdf
 | title = Compte Rendu du Deuxième Congrès International des Mathématiciens, tenu à Paris du 6 au 12 août 1900. Procès-Verbaux et Communications
 | series = ICM Proceedings
 | year = 1902
 | pages = 58–114
 | place = Paris
 | publisher = Gauthier-Villars
 | url = http://www.mathunion.org/ICM/ICM1900/
 | jfm = 32.0084.06}}. There exists also an earlier (and shorter) resume of Hilbert's original talk, translated in French and published as {{Citation
 | last = Hilbert
 | first = D.
 | author-link = David Hilbert
 | title =Problèmes mathématiques
 | journal =[[L'Enseignement Mathématique]]
 | volume =2
 | pages =349–355
 | year =1900
 | language =French
 | url =
 | doi =10.5169/seals-3575
 | jfm = 31.0905.03}}.
*{{Citation
 |last         = Kristensen
 |first        = Jan
 |author-link  = 
 |last2        = Mingione
 |first2       = Giuseppe
 |author2-link = Giuseppe Mingione
 |title        = Sketches of Regularity Theory from The 20th Century and the Work of Jindřich Nečas
 |volume       = Report no. OxPDE-11/17
 |place        = Oxford
 |publisher    = Oxford Centre for Nonlinear PDE
 |pages        = 1–30
 |date         = October 2011
 |url          = http://www.maths.ox.ac.uk/system/files/attachments/OxPDE_11-17.pdf
 |doi          = 
 |id           = 
 |mr           = 
 |zbl          = 
 |deadurl      = yes
 |archiveurl   = https://web.archive.org/web/20140107114055/http://www.maths.ox.ac.uk/system/files/attachments/OxPDE_11-17.pdf
 |archivedate  = 2014-01-07
 |df           = 
}}.
*{{Citation
 | last = Maz'ya
 | first = V. G.
 | author-link = Vladimir Gilelevich Maz'ya
 | script-title=ru:Примеры нерегулярных решений квазилинейных эллиптических уравнений с аналитическими коэффициентами
 | journal =[[Funktsional’nyĭ Analiz i Ego Prilozheniya]]
 | volume =2
 | issue = 3
 | pages = 53–57
 | year =1968
 | language = Russian
 | url = http://mi.mathnet.ru/eng/faa/v2/i3/p53
 | mr =0237946 
 | zbl = 
 }}, translated in English as {{Citation
 | last = Maz'ya
 | first = V. G.
 | author-link = Vladimir Gilelevich Maz'ya
 | title = Examples of nonregular solutions of quasilinear elliptic equations with analytic coefficients 
 | journal = [[Functional Analysis and Its Applications]]
 | volume = 2
 | issue = 3
 | pages = 230–234
 | year =1968
 | doi =10.1007/BF01076124
 | zbl = 0179.43601
}}.
*{{Citation
 | last =Mingione
 | first =Giuseppe
 | author-link =Giuseppe Mingione
 | title = Regularity of minima: an invitation to the Dark Side of the Calculus of Variations.
 | journal =[[Applications of Mathematics]]
 | volume =51
 | issue =4
 | pages =355–426
 | year =2006
 | url =http://dml.cz/dmlcz/134645
 | mr =2291779
 | zbl =1164.49324
 | doi=10.1007/s10778-006-0110-3
}}.
*{{Citation
| last=Morrey 
| first=Charles B. 
| author-link=Charles B. Morrey, Jr.
| title= Multiple integrals in the calculus of variations 
| url=https://books.google.com/books?id=-QNKm1PBohsC 
| place = Berlin–Heidelberg–New York
| publisher = Springer-Verlag
| series = Die Grundlehren der mathematischen Wissenschaften
| volume=130
| year=1966 
| pages = xii+506
| isbn=978-3-540-69915-6  
| mr=0202511 
| zbl=0142.38701 
}}.
*{{Citation
 | last=Nash
 | first=John
 | author-link=John Forbes Nash
 | title=Parabolic equations
 | journal=[[Proceedings of the National Academy of Sciences of the United States of America]]
 | year=1957
 | volume=43
 | issue=8
 | pages=754–758
 | url=http://www.pnas.org/content/43/8/754.full.pdf+html?sid=db030833-a739-437a-8ce0-be81f750b3a7
 | issn=0027-8424
 | jstor=89599
 | mr=0089986
 | zbl=0078.08704
 | doi=10.1073/pnas.43.8.754
| pmc=528534
 }}.
*{{Citation
 | last1=Nash
 | first1=John
 | author1-link=John Forbes Nash
 | title=Continuity of solutions of parabolic and elliptic equations
 | year=1958
 | journal=[[American Journal of Mathematics]]
 | volume=80
 | issue=4
 | pages=931–954
 | issn=0002-9327
 | jstor=2372841
 | mr=0100158
 | zbl=0096.06902
 | doi=10.2307/2372841
}}.
*{{Citation
 | first =Jindřich 
 | last = Nečas
 | author-link =Jindřich Nečas
 | editor-last =Kluge
 | editor-first =Reinhard
 | editor2-last =Müller
 | editor2-first =Wolfdietrich
 | contribution = Example of an irregular solution to a nonlinear elliptic system with analytic coefficients and conditions for regularity
 | contribution-url =
 | title = Theory of nonlinear operators: constructive aspects. Proceedings of the fourth international summer school, held at Berlin, GDR, from September 22 to 26, 1975 
 | series = Abhandlungen der Akademie der Wissenschaften der DDR
 | volume = Nr. 1N
 | year = 1977
 | pages = 197–206
 | place = Berlin
 | publisher = Akademie-Verlag
 | url =
 | doi =
 | id = 
 | mr =0509483
 | zbl=0372.35031
}}.
*{{Citation
 | last1=Petrowsky
 | first1=I. G.
 | author-link=Ivan Georgievich Petrovsky
 | title= Sur l'analyticité des solutions des systèmes d'équations différentielles
 | url= http://mi.mathnet.ru/eng/msb5769
 | year=1939
 | language = French
 | journal = [[Matematicheskii Sbornik|Recueil Mathématique (Matematicheskii Sbornik)]] 
 | volume = 5(47)
 | issue = 1
 | pages = 3–70
 | jfm = 65.0405.02
 | mr = 0001425
 | zbl = 0022.22601
}}.

{{Hilbert's problems}}

[[Category:Hilbert's problems|#19]]
[[Category:Partial differential equations]]
[[Category:Calculus of variations]]</text>
      <sha1>4isixphe2g20o4htfsostcunoln656q</sha1>
    </revision>
  </page>
  <page>
    <title>Hilbert–Burch theorem</title>
    <ns>0</ns>
    <id>33800100</id>
    <revision>
      <id>802061444</id>
      <parentid>646638430</parentid>
      <timestamp>2017-09-23T19:06:29Z</timestamp>
      <contributor>
        <username>Conformancenut347</username>
        <id>3663405</id>
      </contributor>
      <comment>/* Statement */ cite regular element, Fitting ideal; notatoin</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2634">In [[mathematics]], the '''Hilbert–Burch theorem''' describes the structure of some [[Resolution (algebra)|free resolutions]] of a [[Quotient ring|quotient]] of a [[Local ring|local]] or [[Graded ring|graded]] [[Ring (mathematics)|ring]] in the case that the quotient has [[projective dimension]]&amp;nbsp;2. {{harvs|txt|last=Hilbert|authorlink=David Hilbert|year=1890}} proved a version of this theorem for [[polynomial ring]]s, and {{harvs|txt|last=Burch|year1=1968|loc=p.944}} proved a more general version. Several other authors later rediscovered and published variations of this theorem. {{harvtxt|Eisenbud|1995|loc=theorem 20.15}} gives a statement and proof.

==Statement==
If ''R'' is a local ring with an [[ideal (ring theory)|ideal]] ''I'' and 
:&lt;math&gt; 0 \rightarrow R^m\stackrel{f}{\rightarrow} R^n \rightarrow R \rightarrow R/I\rightarrow 0&lt;/math&gt; 
is a free resolution of the ''R''-[[module (mathematics)|module]] ''R''/''I'', then ''m''&amp;nbsp;=&amp;nbsp;''n''&amp;nbsp;–&amp;nbsp;1 and the ideal ''I'' is ''aJ'' where ''a'' is a [[zero divisor|regular]] element of ''R'' and ''J'', a depth-2 ideal, is the first [[Fitting ideal]] &lt;math&gt;\mathrm{Fitt}_1 I&lt;/math&gt; of ''I'', i.e., the ideal generated by the [[determinant]]s of the minors of size ''m'' of the matrix of ''f''.

==References==

*{{Citation | last1=Burch | first1=Lindsay | title=On ideals of finite homological dimension in local rings | doi=10.1017/S0305004100043620 | mr=0229634 | year=1968 | journal=Proc. Cambridge Philos. Soc. | volume=64 | pages=941–948 | zbl=0172.32302 | issn=0008-1981 }}
*{{Citation | last1=Eisenbud | first1=David | author1-link=David Eisenbud | title=Commutative algebra. With a view toward algebraic geometry | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=[[Graduate Texts in Mathematics]] | isbn=3-540-94268-8 | mr=1322960 | year=1995 | volume=150 | zbl=0819.13001 }}
*{{citation |authorlink=David Eisenbud |first=David |last=Eisenbud |title=The Geometry of Syzygies.  A second course in commutative algebra and algebraic geometry | series=[[Graduate Texts in Mathematics]] | volume=229 | year=2005 |location=New York, NY |publisher=[[Springer-Verlag]] | isbn=0-387-22215-4 | zbl=1066.14001 }}
*{{Citation | last1=Hilbert | first1=David | author1-link=David Hilbert | title=Ueber die Theorie der algebraischen Formen | language=German | doi=10.1007/BF01208503 | year=1890 | journal=[[Mathematische Annalen]] | issn=0025-5831 | volume=36 | issue=4 | pages=473–534 | jfm=22.0133.01 }}

{{DEFAULTSORT:Hilbert-Burch theorem}}
[[Category:Commutative algebra]]
[[Category:Theorems in algebra]]

{{abstract-algebra-stub}}</text>
      <sha1>qzzm6xy0ukxpmbgt94fsu3r45fnq99a</sha1>
    </revision>
  </page>
  <page>
    <title>Ideal number</title>
    <ns>0</ns>
    <id>523968</id>
    <revision>
      <id>858016334</id>
      <parentid>858011450</parentid>
      <timestamp>2018-09-04T14:47:01Z</timestamp>
      <contributor>
        <username>Magidin</username>
        <id>228772</id>
      </contributor>
      <minor/>
      <comment>Undid revision 858011450 by [[Special:Contributions/Jobu0101|Jobu0101]] ([[User talk:Jobu0101|talk]]) part of the point is that this is done symbolically, not with specific values of the root</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6862">In [[number theory]] an '''ideal number''' is an [[algebraic integer]] which represents an [[ideal (ring theory)|ideal]] in the [[ring (mathematics)|ring]] of integers of a [[number field]]; the idea was developed by [[Ernst Kummer]], and led to [[Richard Dedekind]]'s definition of [[ideal (ring theory)|ideal]]s for rings. An ideal in the ring of integers of an algebraic number field is ''principal'' if it consists of multiples of a single element of the ring, and ''nonprincipal'' otherwise. By the [[principal ideal theorem]] any nonprincipal ideal becomes principal when extended to an ideal of the [[Hilbert class field]]. This means that there is an element of the ring of integers of the Hilbert class field, which is an ideal number, such that the original nonprincipal ideal is equal to the collection of all multiples of this ideal number by elements of this [[ring of integers]] that lie in the original field's ring of integers.

==Example==
For instance, let ''y'' be a root of ''y''&lt;sup&gt;2&lt;/sup&gt; + ''y'' + 6 = 0, then the ring of integers of the field &lt;math&gt;\Bbb{Q}(y)&lt;/math&gt; is &lt;math&gt;\Bbb{Z}[y]&lt;/math&gt;, which means all ''a'' + ''by'' with ''a'' and ''b'' integers form the ring of integers. An example of a nonprincipal ideal in this ring is the set of all 2''a'' + ''yb'' where ''a'' and ''b'' are integers; the cube of this ideal is principal, and in fact the [[class group]] is cyclic of order three. The corresponding class field is obtained by adjoining an element ''w'' satisfying ''w''&lt;sup&gt;3&lt;/sup&gt; − ''w'' − 1 = 0 to &lt;math&gt;\Bbb{Q}(y)&lt;/math&gt;, giving &lt;math&gt;\Bbb{Q}(y,w)&lt;/math&gt;. An ideal number for the nonprincipal ideal 2''a'' + ''yb'' is &lt;math&gt;\iota = (-8-16y-18w+12w^2+10yw+yw^2)/23&lt;/math&gt;. Since this satisfies the equation
&lt;math&gt;\iota^6-2\iota^5+13\iota^4-15\iota^3+16\iota^2+28\iota+8 = 0&lt;/math&gt; it is an algebraic integer.

All elements of the ring of integers of the class field which when multiplied by ι give a result in &lt;math&gt;\Bbb{Z}[y]&lt;/math&gt; are of the form ''a''α&amp;nbsp;+&amp;nbsp;''b''β, where

:&lt;math&gt;\alpha = (-7+9y-33w-24w^2+3yw-2yw^2)/23&lt;/math&gt;

and

:&lt;math&gt;\beta = (-27-8y-9w+6w^2-18yw-11yw^2)/23.&lt;/math&gt;

The coefficients α and β are also algebraic integers, satisfying

:&lt;math&gt;\alpha^6+7\alpha^5+8\alpha^4-15\alpha^3+26\alpha^2-8\alpha+8=0&lt;/math&gt;

and

:&lt;math&gt;\beta^6+4\beta^5+35\beta^4+112\beta^3+162\beta^2+108\beta+27=0&lt;/math&gt;

respectively. Multiplying ''a''α + ''b''β by the ideal number ι gives 2''a'' + ''by'', which is the nonprincipal ideal.

==History==
Kummer first published the failure of unique factorization in [[cyclotomic fields]] in 1844 in an obscure journal; it was reprinted in 1847 in [[Joseph Liouville|Liouville's]] journal. In subsequent papers in 1846 and 1847 he published his main theorem, the unique factorization into (actual and ideal) primes.

It is widely believed that Kummer was led to his "ideal complex numbers" by his interest in [[Fermat's Last Theorem]]; there is even a story often told that Kummer, like [[Gabriel Lamé|Lamé]], believed he had proven Fermat's Last Theorem until [[Peter Gustav Lejeune Dirichlet|Lejeune Dirichlet]] told him his argument relied on unique factorization; but the story was first told by [[Kurt Hensel]] in 1910 and the evidence indicates it likely derives from a confusion by one of Hensel's sources. [[Harold Edwards (mathematician)|Harold Edwards]] says the belief that Kummer was mainly interested in Fermat's Last Theorem "is surely mistaken" (Edwards 1977, p. 79). Kummer's use of the letter λ to represent a prime number, α to denote a λth root of unity, and his study of the factorization of prime number &lt;math&gt;p\equiv 1 \pmod{\lambda}&lt;/math&gt; into "complex numbers composed of &lt;math&gt;\lambda&lt;/math&gt;th roots of unity" all derive directly from a paper of [[Carl Gustav Jakob Jacobi|Jacobi]] which is concerned with [[higher reciprocity laws]]. Kummer's 1844 memoir was in honor of the jubilee celebration of the University of Königsberg and was meant as a tribute to Jacobi. Although Kummer had studied Fermat's Last Theorem in the 1830s and was probably aware that his theory would have implications for its study, it is more likely that the subject of Jacobi's (and [[Carl Friedrich Gauss|Gauss's]]) interest, higher reciprocity laws, held more importance for him. Kummer referred to his own partial proof of Fermat's Last Theorem for [[regular prime]]s as "a curiosity of number theory rather than a major item" and to the higher reciprocity law (which he stated as a conjecture) as "the principal subject and the pinnacle of contemporary number theory." On the other hand, this latter pronouncement was made when Kummer was still excited about the success of his work on reciprocity and when his work on Fermat's Last Theorem was running out of steam, so it may perhaps be taken with some skepticism.

The extension of Kummer's ideas to the general case was accomplished independently by Kronecker and Dedekind during the next forty years. A direct generalization encountered formidable difficulties, and it eventually led Dedekind to the creation of the theory of [[module (mathematics)|modules]] and [[ideal (ring theory)|ideals]]. Kronecker dealt with the difficulties by developing a theory of forms (a generalization of [[quadratic forms]]) and a theory of [[divisor (algebraic geometry)|divisors]]. Dedekind's contribution would become the basis of [[ring theory]] and [[abstract algebra]], while Kronecker's would become major tools in [[algebraic geometry]].

==References==
*[[Nicolas Bourbaki]], ''Elements of the History of Mathematics.'' Springer-Verlag, NY, 1999.
*[[Harold M. Edwards]], ''Fermat's Last Theorem. A genetic introduction to number theory.'' Graduate Texts in Mathematics vol. 50, Springer-Verlag, NY, 1977.
*C.G. Jacobi, ''Über die complexen Primzahlen, welche in der theori der Reste der 5ten, 8ten, und 12ten Potenzen zu betrachten sind,'' Monatsber. der. Akad. Wiss. Berlin (1839) 89-91.
*E.E. Kummer, ''De numeris complexis, qui radicibus unitatis et numeris integris realibus constant,'' Gratulationschrift der Univ. Breslau zur Jubelfeier der Univ. Königsberg, 1844; reprinted in ''Jour. de Math.'' 12 (1847) 185-212.
*E.E. Kummer, ''Über die Zerlegung der aus Wurzeln der Einheit gebildeten complexen Zahlen in ihre Primfactoren,'' Jour. für Math. (Crelle) 35 (1847) 327-367.
*[[John Stillwell]], introduction to ''Theory of Algebraic Integers'' by Richard Dedekind. Cambridge Mathematical Library, Cambridge University Press, Great Britain, 1996.

==External links==
* [http://fermatslasttheorem.blogspot.com/2006/07/cyclotomic-integers-ideal-numbers_25.html Ideal Numbers], Proof that the theory of ideal numbers saves unique factorization for cyclotomic integers at [http://fermatslasttheorem.blogspot.com Fermat's Last Theorem Blog].

[[Category:Number theory]]
[[Category:Numbers]]</text>
      <sha1>n1o8vgyq4902f8sfkp7fvyf2nu8033n</sha1>
    </revision>
  </page>
  <page>
    <title>Index of fractal-related articles</title>
    <ns>0</ns>
    <id>506177</id>
    <revision>
      <id>865938141</id>
      <parentid>589996660</parentid>
      <timestamp>2018-10-27T05:02:14Z</timestamp>
      <contributor>
        <username>Hyacinth</username>
        <id>17171</id>
      </contributor>
      <comment>s</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1391">This is a '''list of [[fractal]] topics''', by Wikipedia page, See also [[list of dynamical systems and differential equations topics]].

*[[1/f noise]]
*[[Apollonian gasket]]
*[[Attractor]]
*[[Box-counting dimension]]
*[[Cantor distribution]]
*[[Cantor dust]]
*[[Cantor function]]
*[[Cantor set]]
*[[Cantor space]]
*[[Chaos theory]]
*[[Coastline]]
*[[Constructal theory]]
*[[Dimension]]
*[[Dimension theory]]
*[[Dragon curve]]
*[[Fatou set]]
*[[Fractal antenna]]
*[[Fractal art]]
*[[Fractal compression]]
*[[Fractal flame]]
*[[Fractal landscape]]
*[[Fractal transform]]
*[[Fractint]]
*[[Graftal]]
*[[Iterated function system]]
*[[Horseshoe map]]
*[[How Long Is the Coast of Britain? Statistical Self-Similarity and Fractional Dimension]]
*[[Julia set]]
*[[Koch snowflake]]
*[[L-system]]
*[[Lebesgue covering dimension]]
*[[Lévy C curve]]
*[[Lévy flight]]
*[[List of fractals by Hausdorff dimension]]
*[[Lorenz attractor]]
*[[Lyapunov fractal]]
*[[Mandelbrot set]]
*[[Menger sponge]]
*[[Minkowski–Bouligand dimension]]
*[[Multifractal analysis]]
*[[Olbers' paradox]]
*[[Perlin noise]]
*[[Power law]]
*[[Rectifiable curve]]
*[[Scale-free network]]
*[[Self-similarity]]
*[[Sierpinski carpet]]
*[[Sierpiński curve]]
*[[Sierpinski triangle]]
*[[Space-filling curve]]
*[[T-square (fractal)]]
*[[Topological dimension]]

[[Category:Mathematics-related lists|Fractals]]
[[Category:Fractals| ]]</text>
      <sha1>dw7kdbkn1vor7owh0vbkvd3q9i3804u</sha1>
    </revision>
  </page>
  <page>
    <title>International Symposium on Graph Drawing</title>
    <ns>0</ns>
    <id>13516635</id>
    <revision>
      <id>844698311</id>
      <parentid>824416594</parentid>
      <timestamp>2018-06-06T14:50:43Z</timestamp>
      <contributor>
        <username>Arthur MILCHIOR</username>
        <id>9557656</id>
      </contributor>
      <minor/>
      <comment>Moving a category to a more precise subcategory</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4616">[[File:GD09 Poster Session.jpg|thumb|300px|The poster session at Graph Drawing 2009 in [[Chicago]].]]
The '''International Symposium on Graph Drawing (GD)''' is an annual [[academic conference]] in which researchers present [[peer review]]ed papers on [[graph drawing]], [[information visualization]] of [[Network theory|network]] information, [[geometric graph theory]], and related topics.

==Significance==
The Graph Drawing symposia have been central to the growth and development of graph drawing as a research area: as Herman et al. write, "the Graph Drawing community grew around the yearly Symposia."&lt;ref&gt;{{citation|first1=Ivan|last1=Herman|first2=Guy|last2=Melançon|first3=M. Scott|last3=Marshall|title=Graph visualization and navigation in information visualization: A survey|journal=IEEE Transactions in Information Visualization and Computer Graphics|volume=6|issue=1|year=2000|pages=24–43|doi=10.1109/2945.841119}}.&lt;/ref&gt; Nguyen&lt;ref&gt;{{citation|first=Quang Ving|last=Nguyen|title=Space-Efficient Visualization of Large Hierarchies|series=Ph.D. thesis|publisher=Univ. of Technology, Sydney|year=2005|hdl=2100/315}}.&lt;/ref&gt; lists Graph Drawing as one of "several good conferences which directly or indirectly concern with information visualization", and Wong et al.&lt;ref&gt;{{citation|first1=Pak Chung|last1=Wong|last2=Chin|first2=G.|last3=Foote|first3=H.|last4=Mackey|first4=P.|last5=Thomas|first5=J.|contribution=Have Green – A Visual Analytics Framework for Large Semantic Graphs|title=IEEE Symposium On Visual Analytics Science And Technology|year=2006|pages=67–74|doi=10.1109/VAST.2006.261432|url=http://www.purdue.edu/discoverypark/vaccine/publications/pdf/Have%20Green.pdf}}.&lt;/ref&gt; report that its proceedings "provide a wealth of information". In a 2003 study the symposium was among the top 30% of computer science research publication venues, ranked by [[impact factor]].&lt;ref&gt;[http://citeseer.ist.psu.edu/impact.html Estimated impact of publication venues in Computer Science], CiteSeer, May 2003.&lt;/ref&gt;

==History==
The first symposium was held in Marino, near [[Rome]], Italy, in 1992, organized by Giuseppe Di Battista, [[Peter Eades]], [[Pierre Rosenstiehl]], and [[Roberto Tamassia]]. The first two symposia did not publish proceedings, but reports are available online.&lt;ref&gt;[http://graphdrawing.org/literature/gd92-report.pdf Report from 1992 symposium] and [http://graphdrawing.org/literature/gd93-v2.pdf 1993 symposium].&lt;/ref&gt; Since 1994, the proceedings of the symposia have been published by [[Springer-Verlag]]'s [[Lecture Notes in Computer Science]] series.&lt;ref&gt;[http://www.informatik.uni-trier.de/~ley/db/conf/gd/index.html Listing of GD conference proceedings] in DB&amp;amp;LP.&lt;/ref&gt;

Countries in which the conference has been held include Australia, Austria, Canada, the [[Czech Republic]], France, Germany (twice), Greece, Ireland, Italy (three times), and the United States (five times).

==Citation data and its analysis==
A [[citation graph]] having vertices representing the papers in the 1994–2000 Graph Drawing symposia and having edges representing citations between these papers was made available as part of the graph drawing contest associated with the 2001 symposium.&lt;ref&gt;{{citation|first1=T. C.|last1=Biedl|author1-link=Therese Biedl|first2=F. J.|last2=Brandenburg|contribution=Graph-drawing contest report|title=Graph Drawing, 9th Int. Symp., GD 2001|volume=2265|series=Lecture Notes in Computer Science|pages=513–522|publisher=Springer-Verlag|year=2002}}.&lt;/ref&gt; 
The largest connected component of this graph consists of 249 vertices and 642 edges; clustering analysis reveals several prominent subtopics within graph drawing that are more tightly connected, including three-dimensional graph drawing and orthogonal graph drawing.&lt;ref&gt;{{citation|first1=U.|last1=Brandes|first2=T.|last2=Willhalm|contribution=Visualization of bibliographic networks with a reshaped landscape metaphor|title=Proc. Symp. Data Visualisation 2002|year=2002|publisher=Eurographics Association|pages=159–164|url=http://portal.acm.org/citation.cfm?id=509765}}.&lt;/ref&gt;

== See also ==
* The [[list of computer science conferences]] contains other academic conferences in computer science.

==References==
{{reflist|2}}

==External links==
* [http://graphdrawing.org/ graphdrawing.org], the official web site of the conference series.
* [http://www.informatik.uni-trier.de/~ley/db/conf/gd/ the DBLP entry]  (with list of articles).

[[Category:Theoretical computer science conferences]]
[[Category:Mathematics conferences]]
[[Category:Graph drawing]]
[[Category:Visualization (research)]]</text>
      <sha1>1w7wh3z3f03uvyq7zmpxy9c7gucrf6p</sha1>
    </revision>
  </page>
  <page>
    <title>Johannes Widmann</title>
    <ns>0</ns>
    <id>1232678</id>
    <revision>
      <id>870438761</id>
      <parentid>868432934</parentid>
      <timestamp>2018-11-24T20:31:46Z</timestamp>
      <contributor>
        <username>Bartleby08</username>
        <id>7749462</id>
      </contributor>
      <minor/>
      <comment>/* Sources */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4316">{{Infobox scientist
   |name              = Johannes Widmann
   |image             = Johannes_Widmann-Mercantile_Arithmetic_1489.jpg
   |image_size        =
   |caption           = The first use of the [[plus and minus signs]] in print
   |birth_date        = c. 1460
   |birth_place       = [[Cheb|Eger]], [[Holy Roman Empire]]
   |death_date        = after 1498
   |death_place       = [[Leipzig]]
   |nationality       = German
   |residence         =
   |citizenship       =
   |ethnicity         =
   |field             = [[Mathematics]]
   |work_institution  = [[University of Leipzig]]
   |alma_mater        = [[University of Leipzig]]
   |doctoral_advisor  =
   |doctoral_students =
   |known_for         = [[Plus and minus signs]], first university lecture on [[algebra]]
   |prizes            =
}}

'''Johannes Widmann''' (c. 1460 – after 1498) was a [[Germany|German]] [[mathematics|mathematician]]. The [[Plus and minus signs|+ and - symbols]] first appeared in print in his book ''Mercantile Arithmetic or Behende und hüpsche Rechenung auff allen Kauffmanschafft'' published in Leipzig in 1489 in reference to surpluses and deficits in business problems.&lt;ref name="Cajori2007"&gt;{{cite book|author=[[Florian Cajori]]|title=A History of Mathematical Notations -|url=https://books.google.com/books?id=rhEh8jPGQOcC|accessdate=6 July 2013|date=1 June 2007|publisher=Cosimo, Inc.|isbn=978-1-60206-684-7|page=128}}&lt;/ref&gt;

Born in [[Cheb|Eger]], [[Bohemia]], Widmann attended the [[University of Leipzig]] in the 1480s. In 1482 he earned his "[[Bachelor of Arts|Baccalaureus]]" (Bachelor of Art degree) and in 1485 his "[[Magister degree|Magister]]" (doctorate).

Widman published ''Behende und hübsche Rechenung auff allen Kauffmanschafft'' ([[German language|German]]; i.e. Nimble and neat calculation in all trades), his work making use of the signs, in [[Leipzig]] in 1489.&lt;ref name="Cajori2007"/&gt; Further editions were published in [[Pforzheim]], [[Hagenau]], and [[Augsburg]].
Handwritten entries in a surviving collection show that after earning his "Magister" Widman announced holding lectures on e.g. calculating on the lines of a calculating board and on algebra. There is evidence that the lecture on algebra actually took place, making it the first known university lecture on this topic.{{cn|date=July 2016}}

Around 1495 Widmann published the Latin writings ''Algorithmus integrorum cum probis annexis'', ''Algorithmus linealis'', ''Algorithmus minutiarum phisicarum'', ''Algorithmus minutiarum vulgarium'', ''Regula falsi apud philosophantes augmenti et decrementi appellata und Tractatus proportionum plusquam aureus''.

He died in [[Leipzig]].

When [[Adam Ries]] was in [[Erfurt]] between 1518 and 1522 he got to know Widmann's algebra lecture script (today in the Saxon State Library) wherefrom he took examples for his own writings.

==References==
&lt;references/&gt;

==Sources==
* Barbara Gärtner, ''Johannes Widmanns „Behende und hübsche Rechenung“. Die Textsorte „Rechenbuch“ in der Frühen Neuzeit'', Tübingen 2000. (Germanistische Linguistik. 222.)
*[[Moritz Cantor|M. Cantor]], ''Vorlesungen über Geschichte der Mathematik''  II (Leipzig, 1913), pp. 228s.
*K. Fogel, ''Merchants' aids in practical arithmetic from the Middle Ages'' (Russian), Istor.-Mat. Issled. No. 23 (1978), pp. 235-249; 359.
*W. Kaunzner and H. Wussing (eds.), ''Adam Ries, Coss'' (B.G. Teubner Verlagsgesellschaft mbH, Stuttgart, 1992).
*Karl Röttel, ''Johannes Widmann – Am Wendepunkt der Mathematikgeschichte''. In: Schatzkammer der Rechenkunst. Annaberg-Buchholz 2008.
*K. Vogel,'' Biography in Dictionary of Scientific Biography''  (New York 1970-1990).
* Franz Xaver Wilhelm, ''Zur Biographie des Mathematikers Johann Widmann von Eger. In: Mitteilungen des Vereins für Geschichte der Deutschen in Böhmen'', Volume 45 (1907), pp. 429–430.

{{Authority control}}
{{DEFAULTSORT:Widmann, Johannes}}
[[Category:1460s births]]
[[Category:People from Cheb]]
[[Category:German mathematicians]]
[[Category:Medieval German mathematicians]]
[[Category:Algebraists]]
[[Category:15th-century German mathematicians]]
[[Category:15th-century German writers]]
[[Category:15th-century Latin writers]]
[[Category:Leipzig University faculty]]
[[Category:Year of death unknown]]

{{Germany-mathematician-stub}}</text>
      <sha1>6xhjnxk2vk6hqx2amqkmarmby1vrx9e</sha1>
    </revision>
  </page>
  <page>
    <title>John Reif</title>
    <ns>0</ns>
    <id>10804313</id>
    <revision>
      <id>864854828</id>
      <parentid>846506734</parentid>
      <timestamp>2018-10-19T22:56:48Z</timestamp>
      <contributor>
        <ip>140.247.136.210</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7289">{{for|the Oklahoma Supreme Court justice|John F. Reif}}
'''John H. Reif''' (born 1951) is an [[United States|American]] academic, and Professor of Computer Science at [[Duke University]], who has made contributions to large number of fields in [[computer science]]: ranging from [[algorithms]] and [[computational complexity theory]] to [[robotics]] and to [[game theory]].

== Biography ==
John Reif received a B.S. (magna cum laude) from Tufts University in 1973, a M.S. from Harvard University in 1975 and a Ph.D. from Harvard University in 1977.&lt;ref&gt;[http://www.cs.duke.edu/~reif/vita/vita.html Reif's Vita] {{webarchive|url=https://web.archive.org/web/20080517083556/http://www.cs.duke.edu/~reif/vita/vita.html |date=2008-05-17 }}&lt;/ref&gt;

From 1983 to 1986 he was Associate Professor of Harvard University, and since 1986 he has been Professor of Computer Science at [[Duke University]]. Currently he holds the Hollis Edens Distinguished Professor, Trinity College of Arts and Sciences, [[Duke University]].  From 2011-2014 he was Distinguished Adjunct Professor, Faculty of Computing and Information Technology (FCIT), King Abdulaziz University (KAU), Jeddah, Saudi Arabia.

John Reif is President of Eagle Eye Research, Inc.,&lt;ref&gt;[http://www.manta.com/coms2/dnbcompany_hdxvxr Eagle Eye Research, Inc.]&lt;/ref&gt; which specializes in defense applications of DNA biotechnology. He has also contributed to bringing together various disjoint research communities working in different areas of nano-sciences by organizing (as General Chairman) annual Conferences on "Foundations of Nanoscience: Self-assembled architectures and devices" (FNANO&lt;ref&gt;[http://www.cs.duke.edu/~reif/FNANO FNANO]&lt;/ref&gt;) for last 15 years.

He has been awarded Fellow of the following organizations: [[American Association for the Advancement of Science]], [[IEEE]], [[Association for Computing Machinery|ACM]], and the Institute of Combinatorics.

He is the son of [[Arnold E. Reif]].

== Research contributions ==
John Reif has made contributions to large number of fields in [[computer science]]: ranging from [[algorithms]] and [[computational complexity theory]] to [[robotics]] and to [[game theory]]. He developed efficient [[randomized algorithms]] and [[parallel algorithms]] for a wide variety of [[Graph algorithm#Graph algorithms|graph]], [[Computational geometry|geometric]], numeric, algebraic, and logical problems. His [https://scholar.google.com/citations?hl=en&amp;user=Er-gEaoAAAAJ Google Scholar H-index]&lt;ref&gt;[[h-index]]&lt;/ref&gt; is 68.

In the area of robotics, he gave the first hardness proofs for [[Motion planning|robotic motion planning]] as well as efficient algorithms for a wide variety of motion planning problems.

He also has led applied research projects: parallel programming languages (Proteus System for parallel programming), parallel architectures (Blitzen, a massively parallel machine), data compression (massively parallel loss-less compression hardware), and [[optical computing]] (free-space holographic routing). His papers on these algorithmic topics can be downloaded [http://www.cs.duke.edu/~reif/vita/papertopics.html here].

=== Research in nanoscience ===
More recently, he has centered his research in [[nanoscience]] and in particular [[DNA nanotechnology]], [[DNA computing]], and DNA [[nanorobotics]]. In the last dozen years his group at Duke has designed and experimentally demonstrated in the lab a variety of novel self-assembled DNA nanostructures and DNA lattices, including the first experimental demonstrations of molecular scale computation and patterning using DNA assembly. His group also experimentally demonstrated various molecular robotic devices composed of DNA, including one of the first autonomous unidirectional DNA walker that walked on a DNA track. He also has done significant work on controlling errors in self-assembly and the stochastic analysis of self-assembly.&lt;ref&gt;His papers on these topics can be downloaded [http://www.cs.duke.edu/~reif/vita/topics/biomolecular.html here].&lt;/ref&gt;

== See also ==
* [[Kinodynamic planning]]

== Publications ==
He is the author of over 200 publications.&lt;ref&gt;Publications:
* [https://users.cs.duke.edu/~reif/vita/papertopics.htm Reif's publications organized by research area]
* [https://users.cs.duke.edu/~reif/vita/papers.htm Reif's publications chronographically ordered]
* [http://fds.duke.edu/db/aas/cs/faculty/reif/publications.html Reif's publications listed on Duke Faculty Website]
* [https://scholar.google.com/citations?hl=en&amp;user=Er-gEaoAAAAJ Reif's publications listed on Google Scholar Website]&lt;/ref&gt; A selection:
* 2003. Hao Yan, Thomas H. LaBean, Liping Feng, and John H. Reif, [http://www.pnas.org/content/100/14/8103.abstract Directed Nucleation Assembly of Barcode Patterned DNA Lattices], Proceedings of the National Academy of Sciences, Volume 100, No. 14, pp.&amp;nbsp;8103&amp;ndash;8108 (July 8, 2003).
* 2004. Peng Yin, Hao Yan, Xiaoju G. Daniel, Andrew J. Turberfield, John H. Reif, [http://www3.interscience.wiley.com/journal/109605254/abstract A Unidirectional DNA Walker Moving Autonomously Along a Linear Track], Angewandte Chemie, Volume 43, Number 37, pp.&amp;nbsp;4906&amp;ndash;4911 (Sept. 20, 2004).
* 2007. John H. Reif and Thomas H. LaBean, [http://portal.acm.org/citation.cfm?id=1284621.1284647&amp;coll=portal&amp;dl=ACM&amp;idx=J79&amp;part=magazine&amp;WantType=Magazines&amp;title=Communications%20of%20the%20ACM&amp;CFID=11223344&amp;CFTOKEN=44332211 Autonomous Programmable Biomolecular Devices Using Self-Assembled DNA Nanostructures], Communications of the ACM (CACM), Volume 50, Issue 9, pp.&amp;nbsp;46&amp;ndash;53 (Sept 2007).
* 2008. Peng Yin, Rizal F. Hariadi, Sudheer Sahu, Harry M.T.Choi, Sung Ha Park, Thomas H. LaBean, John H. Reif, [http://www.sciencemag.org/cgi/content/abstract/321/5890/824 Programming DNA Tube Circumferences], Science, Vol. 321. no. 5890, pp.&amp;nbsp;824–826, (August 8, 2008).

== Books ==
*''[[Parallel Algorithm Derivation and Program Transformation]]'', (with Robert Paige and Ralph Wachter), Kluwer Academic Publishers, Boston, MA 1993.
*''[[Handbook of Randomized Computing]]'', (with Sanguthevar Rajasekaran, Panos M. Pardalos and José Rolim), Springer, New York, NY, 2001.
*''Synthesis of Parallel Algorithms'', Morgan Kaufmann Publishers, San Francisco, CA, 1993.
*''[[DNA-based Self-assembly and Nanorobotics]]'', (with [[Sudheer Sahu|S. Sahu]]), VDM Verlag, Saarbrücken, Germany, 2008.

== References ==
{{reflist}}

==External links==
* [http://www.cs.duke.edu/~reif Reif's Personal Web page]
* [http://fds.duke.edu/db/aas/cs/faculty/reif Reif's Duke Web page]
* [http://www.cs.duke.edu/~reif/bio/ Reif's Family, Schooling, Work and Play]

{{Authority control}}

{{DEFAULTSORT:Reif, John}}
[[Category:1951 births]]
[[Category:Duke University faculty]]
[[Category:Harvard University alumni]]
[[Category:Harvard University faculty]]
[[Category:Living people]]
[[Category:Researchers in geometric algorithms]]
[[Category:Theoretical computer scientists]]
[[Category:Tufts University alumni]]
[[Category:Tufts University School of Engineering alumni]]
[[Category:Fellows of the American Association for the Advancement of Science]]
[[Category:Fellows of the Association for Computing Machinery]]
[[Category:Fellow Members of the IEEE]]
[[Category:DNA nanotechnology people]]</text>
      <sha1>1rn0q0utsoamr62qf8d21760m7ukxk2</sha1>
    </revision>
  </page>
  <page>
    <title>Kummer's function</title>
    <ns>0</ns>
    <id>1516033</id>
    <revision>
      <id>542560379</id>
      <parentid>528966054</parentid>
      <timestamp>2013-03-07T10:37:27Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 2 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q971651]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1188">In [[mathematics]], there are several functions known as '''Kummer's function'''. One is known as the [[confluent hypergeometric function]] of Kummer. Another one, defined below, is related to the [[polylogarithm]]. Both are named for [[Ernst Kummer]].

Kummer's function is defined by

:&lt;math&gt;\Lambda_n(z)=\int_0^z \frac{\log^{n-1}|t|}{1+t}\;dt.&lt;/math&gt;

The [[duplication formula]] is

:&lt;math&gt;\Lambda_n(z)+\Lambda_n(-z)= 2^{1-n}\Lambda_n(-z^2)&lt;/math&gt;.

Compare this to the duplication formula for the polylogarithm:

:&lt;math&gt;\operatorname{Li}_n(z)+\operatorname{Li}_n(-z)= 2^{1-n}\operatorname{Li}_n(z^2).&lt;/math&gt;

An explicit link to the polylogarithm is given by

:&lt;math&gt;\operatorname{Li}_n(z)=\operatorname{Li}_n(1)\;\;+\;\;
\sum_{k=1}^{n-1} (-)^{k-1} \;\frac{\log^k |z|} {k!} \;\operatorname{Li}_{n-k} (z) \;\;+\;\;
\frac{(-)^{n-1}}{(n-1)!} \;\left[ \Lambda_n(-1) - \Lambda_n(-z) \right].&lt;/math&gt;

==References==
*{{citation|editor-first=Leonard|editor-last=Lewin|title=Structural Properties of Polylogarithms|year=1991|location=Providence, RI|publisher=American Mathematical Society|isbn=0-8218-4532-2}}.

[[Category:Special functions]]

{{mathanalysis-stub}}

[[hu:Kummer-függvény]]</text>
      <sha1>kraip0t6e3nrildwt42au2nllz32mxh</sha1>
    </revision>
  </page>
  <page>
    <title>Malmquist's theorem</title>
    <ns>0</ns>
    <id>42074715</id>
    <revision>
      <id>793627268</id>
      <parentid>793626922</parentid>
      <timestamp>2017-08-02T23:45:00Z</timestamp>
      <contributor>
        <username>Headbomb</username>
        <id>1461430</id>
      </contributor>
      <comment>fix</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2702">{{no footnotes|date=June 2017}}
In [[mathematics]], '''Malmquist's theorem''', is the name of any of the three theorems proved by {{harvs|txt|authorlink=Axel Johannes Malmquist|first=Axel Johannes |last=Malmquist|year1=1913|year2=1920|year3=1941}}. These theorems restrict the forms of first order algebraic [[differential equation]]s which have transcendental [[meromorphic]] or algebroid solutions.

==Statement of the theorems==
Theorem (1913). If the differential equation
:&lt;math&gt;\frac{dw}{dz}=R(z,w)&lt;/math&gt;
where ''R''(''z'',''w'') is a [[rational function]], has a transcendental [[meromorphic function|meromorphic]] solution, then ''R'' is a [[polynomial]] of degree at most 2 with respect to ''w''; in other words the differential equation is a [[Riccati equation]], or linear.

Theorem (1920). If an irreducible differential equation
:&lt;math&gt;F\left(\frac{dw}{dz},w,z\right)=0&lt;/math&gt;
where ''F'' is a polynomial, has a transcendental meromorphic solution, then the equation has no [[movable singularity|movable singularities]]. Moreover, it can be algebraically reduced either to a Riccati equation or to
:&lt;math&gt;\left(\frac{dw}{dz}\right)^2=a(z)P(z,w),&lt;/math&gt;
where ''P'' is a polynomial of degree ''3'' with respect to ''w''.

Theorem (1941). If an irreducible differential equation
:&lt;math&gt;F\left(\frac{dw}{dz},w,z\right)=0,&lt;/math&gt;
where ''F'' is a polynomial, has a transcendental [[algebroid function|algebroid]] solution, then it can be algebraically reduced to an equation that has no movable singularities.

A modern account of theorems 1913, 1920 is given in the paper of [[Alexandre Eremenko|A. Eremenko]](1982)

==References==
*{{citation |journal=Acta Mathematica |year=1913 |volume=36 |issue=1 |pages=297–343 |title=Sur les fonctions à un nombre fini de branches définies par les équations différentielles du premier ordre |first=J. |last=Malmquist |doi=10.1007/BF02422385}}
*{{citation |journal=Acta Mathematica |year=1920 |volume=42 |issue=1 |pages=317–325  |title=Sur les fonctions à un nombre fini de branches satisfaisant à une équation différentielle du premier ordre |first=J. |last=Malmquist |doi=10.1007/BF02404413}}
*{{citation |MR=0005974 |journal=Acta Mathematica |year=1941 |volume=74 |issue=1 |pages=175–196 |title=Sur les fonctions à un nombre fini de branches satisfaisant à une équation différentielle du premier ordre|first=J. |last=Malmquist |doi=10.1007/BF02392253}}
*{{citation |mr=0667974 |journal=Russian Mathematical Surveys |year=1982 |volume=37 |issue=4 |pages=61–95 |title=Meromorphic solutions of algebraic differential equations |first=A. |last=Eremenko}}

[[Category:Ordinary differential equations]]
[[Category:Theorems in analysis]]</text>
      <sha1>qm38hmy6krn9j4rwobmqukpck2zxqcr</sha1>
    </revision>
  </page>
  <page>
    <title>Marginal model</title>
    <ns>0</ns>
    <id>13398693</id>
    <revision>
      <id>750295618</id>
      <parentid>415682794</parentid>
      <timestamp>2016-11-18T22:31:48Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Statistical models]]; added [[Category:Regression models]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1519">In [[statistics]], '''marginal models''' (Heagerty &amp; Zeger, 2000) are a technique for obtaining regression estimates in [[multilevel model]]ing, also called [[hierarchical linear models]].
People often want to know the effect of a predictor/explanatory variable ''X'', on a response variable ''Y''. One way to get an estimate for such effects is through [[regression analysis]].

==Why the name marginal model?==
In a typical multilevel model, there are level 1 &amp; 2 residuals (R and U variables). The two variables form a [[joint distribution]] for the response variable (&lt;math&gt;Y_{ij}&lt;/math&gt;). In a marginal model, we collapse over the level 1 &amp; 2 residuals and thus ''marginalize'' (see also [[conditional probability]]) the joint distribution into a univariate [[normal distribution]]. We then fit the marginal model to data.

For example, for the following hierarchical model,

:level 1: &lt;math&gt;Y_{ij} = \beta_{0j} + R_{ij}&lt;/math&gt;, the residual is &lt;math&gt;R_{ij}&lt;/math&gt;, and &lt;math&gt;var(R_{ij}) = \sigma^2&lt;/math&gt;

:level 2: &lt;math&gt;\beta_{0j} = \gamma_{00} + U_{0j}&lt;/math&gt;, the residual is &lt;math&gt;U_{0j}&lt;/math&gt;, and &lt;math&gt;var(U_{0j}) = \tau_0^2&lt;/math&gt;

Thus, the marginal model is,

:&lt;math&gt;Y_{ij} \sim N(\gamma_{00},(\tau_0^2+\sigma^2))&lt;/math&gt;

This model is what is used to fit to data in order to get regression estimates.

==References==
Heagerty, P. J., &amp; Zeger, S. L. (2000). Marginalized multilevel models and likelihood inference. ''Statistical Science, 15(1)'', 1-26.

[[Category:Regression models]]


{{stats-stub}}</text>
      <sha1>favvsvl9khrtpqzepf9e04rbc17yeog</sha1>
    </revision>
  </page>
  <page>
    <title>Markov brothers' inequality</title>
    <ns>0</ns>
    <id>9318403</id>
    <revision>
      <id>687633558</id>
      <parentid>647500129</parentid>
      <timestamp>2015-10-26T20:06:05Z</timestamp>
      <contributor>
        <username>Doctormatt</username>
        <id>1213465</id>
      </contributor>
      <minor/>
      <comment>less clunky wording</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2125">In [[mathematics]], the '''Markov brothers' inequality''' is an [[inequality (mathematics)|inequality]] proved in the 1890s by brothers [[Andrey Markov]] and [[Vladimir Andreevich Markov|Vladimir Markov]], two Russian mathematicians. This inequality bounds the maximum of the [[derivative]]s of a polynomial on an interval in terms of the maximum of the polynomial.&lt;ref&gt;{{cite book|last=Achiezer|first=N.I.|authorlink=Naum Akhiezer|title=Theory of approximation|publisher=Dover Publications, Inc.|location=New York|year=1992}}&lt;/ref&gt; For ''k'' = 1 it was proved by Andrey Markov,&lt;ref&gt;{{cite journal|last=Markov|first=A.A.|authorlink=Andrey Markov|title=On a question by D. I. Mendeleev|journal=Zap. Imp. Akad. Nauk SPb.|volume=62|year=1890|pages=1&amp;ndash;24}}&lt;/ref&gt; and for ''k'' = 2,3,... by his brother Vladimir Markov.&lt;ref&gt;{{cite journal|last=Markov|first=V.A.|authorlink=Vladimir Andreevich Markov|title=О функциях, наименее уклоняющихся от нуля в данном промежутке (On Functions of Least Deviation from Zero in a Given Interval)|year=1892}} Appeared in German with a foreword by [[Sergei Bernstein]] as {{cite journal|last=Markov|first=V.A.|authorlink=Vladimir Andreevich Markov|title=Über Polynome, die in einem gegebenen Intervalle möglichst wenig von Null abweichen|journal=Math. Ann.|volume=77|year=1916|pages=213–258|doi=10.1007/bf01456902}}&lt;/ref&gt;

==The statement==
Let ''P'' be a polynomial of degree ≤ ''n''.  Then

: &lt;math&gt; \max_{-1 \leq x \leq 1} |P^{(k)}(x)| \leq \frac{n^2 (n^2 - 1^2) (n^2 - 2^2) \cdots (n^2 - (k-1)^2)}{1 \cdot 3 \cdot 5 \cdots (2k-1)} \max_{-1 \leq x \leq 1} |P(x)|. &lt;/math&gt;

Equality is attained for [[Chebyshev polynomials]] of the first kind.

==Related inequalities==
* [[Bernstein's inequality (mathematical analysis)]]
* [[Remez inequality]]

==Applications==
Markov's inequality is used to obtain lower bounds in [[computational complexity theory]] via the so-called [http://www.scottaaronson.com/talks/polymeth.ppt "Polynomial Method"].

==References==
{{Reflist}}

[[Category:Theorems in analysis]]
[[Category:Inequalities]]</text>
      <sha1>okx2zbsythwmk3vmgpjnl92um5q9n1f</sha1>
    </revision>
  </page>
  <page>
    <title>Matsumoto zeta function</title>
    <ns>0</ns>
    <id>26702096</id>
    <revision>
      <id>862709844</id>
      <parentid>852434709</parentid>
      <timestamp>2018-10-06T05:24:12Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Removing category Eponymous scientific concepts per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2018 September 22]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="652">In [[mathematics]], '''Matsumoto zeta functions''' are a type of [[zeta function]] introduced by [[Kohji Matsumoto]] in 1990. They are functions of the form
:&lt;math&gt;\phi(s)=\prod_{p}\frac{1}{A_p(p^{-s})}&lt;/math&gt;
where ''p'' is a [[prime]] and ''A''&lt;sub&gt;''p''&lt;/sub&gt; is a [[polynomial]].

==References==
*{{Citation | last1=Matsumoto | first1=Kohji | title=Analytic number theory ({T}okyo, 1988) | doi=10.1007/BFb0097134 | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Lecture Notes in Math. | mr=1071754 | year=1990 | volume=1434 | chapter=Value-distribution of zeta-functions}}

[[Category:Zeta and L-functions]]


{{numtheory-stub}}</text>
      <sha1>qtta2gtza8kzuuq917az3g305nd32kw</sha1>
    </revision>
  </page>
  <page>
    <title>Method of steepest descent</title>
    <ns>0</ns>
    <id>27082137</id>
    <revision>
      <id>857550482</id>
      <parentid>848853347</parentid>
      <timestamp>2018-09-01T11:57:53Z</timestamp>
      <contributor>
        <ip>2605:E000:9149:A600:1DC2:1921:F704:7FAB</ip>
      </contributor>
      <comment>sp</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="30240">{{for|the optimization algorithm|Gradient descent}}
In mathematics, the '''method of steepest descent''' or '''stationary-phase method''' or '''saddle-point method''' is an extension of [[Laplace's method]] for approximating an integral, where one deforms a contour integral in the complex plane to pass  near a stationary point ([[saddle point]]), in roughly the direction of steepest descent or stationary phase. The saddle-point approximation is used with integrals in the complex plane, whereas Laplace’s method is used with real integrals.

The integral to be estimated is often of the form
:&lt;math&gt;\int_Cf(z)e^{\lambda g(z)}\,dz,&lt;/math&gt;
where ''C'' is a contour, and λ is large. One version of the method of steepest descent deforms the contour of integration ''C'' into a new path integration ''C′'' so that the following conditions hold:
# ''C′'' passes through one or more zeros of the derivative ''g''′(''z''),
# the imaginary part of ''g''(''z'') is constant on ''C′''.

The method of steepest descent was first published by {{harvtxt|Debye|1909}}, who used it to estimate [[Bessel functions]] and pointed out that it occurred in the unpublished note {{harvtxt|Riemann|1863}} about [[hypergeometric functions]]. The contour of steepest descent has a minimax property, see {{harvtxt|Fedoryuk|2001}}. {{harvtxt|Siegel|1932}} described some other unpublished notes of Riemann, where he used this method to derive the [[Riemann–Siegel formula]].

==A simple estimate&lt;ref&gt;A modified version of Lemma 2.1.1 on page 56 in {{harvtxt|Fedoryuk|1987}}.&lt;/ref&gt;==
Let {{math|&amp;thinsp;''f'', ''S'' : '''C'''&lt;sup&gt;''n''&lt;/sup&gt; → '''C'''}} and {{math|''C'' ⊂  '''C'''&lt;sup&gt;''n''&lt;/sup&gt;}}. If

:&lt;math&gt; M = \sup_{x \in C} \Re (S(x)) &lt; \infty,&lt;/math&gt;

where &lt;math&gt;\Re (\cdot)&lt;/math&gt; denotes the real part, and there exists a positive real number {{math|''λ''&lt;sub&gt;0&lt;/sub&gt;}} such that

:&lt;math&gt;\int_{C} \left| f(x) e^{\lambda_0 S(x)}  \right| dx &lt; \infty,&lt;/math&gt;

then the following estimate holds:
:&lt;math&gt;\left| \int_{C} f(x) e^{\lambda S(x)}  dx \right| \leqslant \text{const}\cdot e^{\lambda M}, \qquad \forall \lambda \in \mathbb{R}, \quad \lambda \geqslant \lambda_0.&lt;/math&gt;

&lt;div style="clear:both;width:80%;" class="NavFrame collapsed"&gt;
&lt;div class="NavHead" style="background-color:#CCCCFF; text-align:left; font-size:normal;"&gt;Proof of the simple estimate &lt;/div&gt;
&lt;div class="NavContent" style="text-align:left;"&gt;
:&lt;math&gt;\begin{align}
\left| \int_{C} f(x) e^{\lambda S(x)} dx \right| &amp;\leqslant \int_C |f(x)| \left|e^{\lambda S(x)} \right| dx \\ 
&amp;\equiv \int_{C} |f(x)| e^{\lambda M} \left | e^{\lambda_0 (S(x)-M)} e^{(\lambda-\lambda_0)(S(x)-M)} \right| dx  \\
&amp;\leqslant \int_C |f(x)| e^{\lambda M} \left| e^{\lambda_0 (S(x)-M)} \right| dx &amp;&amp; \left| e^{(\lambda-\lambda_0)(S(x) - M)} \right| \leqslant 1 \\
&amp;= \underbrace{e^{-\lambda_0 M} \int_{C} \left| f(x) e^{\lambda_0 S(x)} \right| dx}_{\text{const}} \cdot e^{\lambda M}.
\end{align}&lt;/math&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;!-- END OF A SIMPLE ESTIMATE --&gt;

== The case of a single non-degenerate saddle point ==

=== Basic notions and notation ===
Let {{mvar|x}} be a complex {{mvar|n}}-dimensional vector, and

:&lt;math&gt;S''_{xx}(x) \equiv \left( \frac{\partial^2 S(x)}{\partial x_i \partial x_j} \right), \qquad 1\leqslant i,\, j\leqslant n,&lt;/math&gt;

denote the [[Hessian matrix]] for a function {{math|''S''(''x'')}}. If

:&lt;math&gt;\boldsymbol{\varphi}(x) = (\varphi_1(x), \varphi_2(x), \ldots, \varphi_k(x))&lt;/math&gt;

is a vector function, then its [[Jacobian matrix and determinant|Jacobian matrix]] is defined as

:&lt;math&gt;\boldsymbol{\varphi}_x' (x) \equiv \left( \frac{\partial \varphi_i (x)}{\partial x_j} \right), \qquad 1 \leqslant i \leqslant k, \quad 1 \leqslant j \leqslant n.&lt;/math&gt;

A '''non-degenerate saddle point''', {{math|''z''&lt;sup&gt;0&lt;/sup&gt; ∈ '''C'''&lt;sup&gt;''n''&lt;/sup&gt;}}, of a holomorphic function {{math|''S''(''z'')}} is a critical point of the function (i.e., {{math|∇''S''(''z''&lt;sup&gt;0&lt;/sup&gt;) {{=}} 0}}) where the function's Hessian matrix has a non-vanishing determinant (i.e., &lt;math&gt;\det S''_{zz}(z^0) \neq 0&lt;/math&gt;).

The following is the main tool for constructing the asymptotics of integrals in the case of a non-degenerate saddle point:

===Complex Morse lemma===
The [[Morse theory#The Morse lemma|Morse lemma]] for real-valued functions generalizes as follows&lt;ref&gt;Lemma 3.3.2 on page 113 in {{harvtxt|Fedoryuk|1987}}&lt;/ref&gt; for [[holomorphic function]]s: near a non-degenerate saddle point {{math|''z''&lt;sup&gt;0&lt;/sup&gt;}} of a holomorphic function {{math|''S''(''z'')}}, there exist coordinates in terms of which {{math|''S''(''z'') − ''S''(''z''&lt;sup&gt;0&lt;/sup&gt;)}} is exactly quadratic. To make this precise, let {{mvar|S}} be a holomorphic function with domain {{math|''W'' ⊂ '''C'''&lt;sup&gt;''n''&lt;/sup&gt;}}, and let {{math|''z''&lt;sup&gt;0&lt;/sup&gt;}} in {{mvar|W}} be a non-degenerate saddle point of {{mvar|S}}, that is, {{math|∇''S''(''z''&lt;sup&gt;0&lt;/sup&gt;) {{=}} 0}} and &lt;math&gt;\det S''_{zz}(z^0) \neq 0&lt;/math&gt;. Then there exist neighborhoods {{math|''U'' ⊂ ''W''}} of {{math|''z''&lt;sup&gt;0&lt;/sup&gt;}} and {{math|''V'' ⊂ '''C'''&lt;sup&gt;''n''&lt;/sup&gt;}} of {{math|''w'' {{=}} 0}}, and a [[One-to-one function|bijective]] holomorphic function {{math|'''''φ''''' : ''V'' → ''U''}} with {{math|'''''φ'''''(0) {{=}} ''z''&lt;sup&gt;0&lt;/sup&gt;}} such that

:&lt;math&gt;\forall w \in V: \qquad S(\boldsymbol{\varphi}(w)) = S(z^0) + \frac{1}{2} \sum_{j=1}^n \mu_j w_j^2, \quad \det\boldsymbol{\varphi}_w'(0) = 1,
&lt;/math&gt;

Here, the {{math|''μ&lt;sub&gt;j&lt;/sub&gt;''}} are the [[Eigenvalues and eigenvectors|eigenvalues]] of the matrix &lt;math&gt;S_{zz}''(z^0)&lt;/math&gt;.

[[File:Complex Morse Lemma Illustration.pdf|thumb|center|An illustration of Complex Morse lemma]]

&lt;div style="clear:both;width:80%;" class="NavFrame collapsed"&gt;
&lt;div class="NavHead" style="background-color:#CCCCFF; text-align:left; font-size:normal;"&gt;Proof of complex Morse lemma&lt;/div&gt;
&lt;div class="NavContent" style="text-align:left;"&gt;

The following proof is a straightforward generalization of the proof of the real [[Morse theory#The Morse lemma|Morse Lemma]], which can be found in.&lt;ref&gt;{{harvtxt|Poston|Stewart|1978}}, page 54; see also the comment on page 479 in {{harvtxt|Wong|1989}}.
&lt;/ref&gt; We begin by demonstrating

:'''Auxiliary statement.''' Let {{math|&amp;thinsp;''f''&amp;thinsp; : '''C'''&lt;sup&gt;''n''&lt;/sup&gt; → '''C'''}} be [[Holomorphic function|holomorphic]] in a neighborhood of the origin and {{math|&amp;thinsp;''f''&amp;thinsp;(0) {{=}} 0}}. Then in some neighborhood, there exist functions {{math|''g&lt;sub&gt;i&lt;/sub&gt;'' : '''C'''&lt;sup&gt;''n''&lt;/sup&gt; → '''C'''}} such that
::&lt;math&gt;f(z) = \sum_{i=1}^n z_i g_i(z),&lt;/math&gt; 
:where each {{math|''g&lt;sub&gt;i&lt;/sub&gt;''}} is [[Holomorphic function|holomorphic]] and 
::&lt;math&gt;g_i(0) = \left. \tfrac{\partial f(z)}{\partial z_i} \right|_{z=0}.&lt;/math&gt;
 
&lt;div style="clear:both;width:80%;" class="NavFrame collapsed"&gt;
&lt;div class="NavHead" style="background-color:#CCCCCF; text-align:center; font-size:normal;"&gt;Proof of auxiliary statement&lt;/div&gt;
&lt;div class="NavContent" style="text-align:left;"&gt;
From the identity

:&lt;math&gt;f(z) = \int_0^1 \frac{d}{dt} f \left (t z_1,\cdots, t z_n \right ) dt = \sum_{i=1}^n z_i \int_0^1 \left. \frac{\partial f(z)}{\partial z_i}\right|_{z=(t z_1, \ldots, t z_n)} dt,&lt;/math&gt;

we conclude that

:&lt;math&gt;g_i(z) = \int_0^1 \left. \frac{\partial f(z)}{\partial z_i}\right|_{z=(t z_1, \ldots, t z_n)} dt&lt;/math&gt;

and

:&lt;math&gt;g_i(0) = \left. \frac{\partial f(z)}{\partial z_i} \right|_{z=0}.&lt;/math&gt;
&lt;/div&gt;
&lt;/div&gt;

Without loss of generality, we translate the origin to {{math|''z''&lt;sup&gt;0&lt;/sup&gt;}}, such that {{math|''z''&lt;sup&gt;0&lt;/sup&gt; {{=}} 0}} and {{math|''S''(0) {{=}} 0}}. Using the Auxiliary Statement, we have
:&lt;math&gt;S(z) = \sum_{i=1}^n z_i g_i (z).&lt;/math&gt;
Since the origin is a saddle point,
:&lt;math&gt;\left. \frac{\partial S(z)}{\partial z_i} \right|_{z=0} = g_i(0) = 0,&lt;/math&gt;
we can also apply the Auxiliary Statement to the functions {{math|''g&lt;sub&gt;i&lt;/sub&gt;''(''z'')}} and obtain 
:&lt;math&gt;S(z) = \sum_{i,j=1}^n z_i z_j h_{ij}(z).&lt;/math&gt;&lt;div style="text-align: right;"&gt; '''(1)''' &lt;/div&gt;
Recall that an arbitrary matrix {{mvar|A}} can be represented as a sum of symmetric {{math|''A''&lt;sup&gt;(''s'')&lt;/sup&gt;}} and anti-symmetric {{math|''A''&lt;sup&gt;(''a'')&lt;/sup&gt;}} matrices, 
:&lt;math&gt;A_{ij} = A_{ij}^{(s)} + A_{ij}^{(a)}, \qquad A_{ij}^{(s)} = \tfrac{1}{2}\left( A_{ij} + A_{ji} \right), \qquad A_{ij}^{(a)} = \tfrac{1}{2}\left( A_{ij} - A_{ji} \right).&lt;/math&gt;
The contraction of any symmetric matrix ''B'' with an arbitrary matrix {{mvar|A}} is
:&lt;math&gt;\sum_{i,j} B_{ij} A_{ij} = \sum_{i,j} B_{ij} A_{ij}^{(s)},&lt;/math&gt;
&lt;div style="text-align: right;"&gt; '''(2)''' &lt;/div&gt;
i.e., the anti-symmetric component of {{mvar|A}} does not contribute because
:&lt;math&gt;\sum_{i,j} B_{ij} C_{ij} = \sum_{i,j} B_{ji} C_{ji} = - \sum_{i,j} B_{ij} C_{ij} = 0.&lt;/math&gt;
Thus, {{math|''h&lt;sub&gt;ij&lt;/sub&gt;''(''z'')}} in equation (1) can be assumed to be symmetric with respect to the interchange of the indices {{mvar|i}} and {{mvar|j}}. Note that
:&lt;math&gt;\left. \frac{\partial^2 S(z)}{\partial z_i \partial z_j} \right|_{z=0} = 2h_{ij}(0);&lt;/math&gt;
hence, {{math|det(''h&lt;sub&gt;ij&lt;/sub&gt;''(0)) ≠ 0}} because the origin is a non-degenerate saddle point.

Let us show by [[Mathematical induction|induction]] that there are local coordinates {{math|''u'' {{=}} (''u''&lt;sub&gt;1&lt;/sub&gt;, ... ''u&lt;sub&gt;n&lt;/sub&gt;''), ''z'' {{=}} '''''ψ'''''(''u''), 0 {{=}} '''''ψ'''''(0)}}, such that
:&lt;math&gt;S(\boldsymbol{\psi}(u)) = \sum_{i=1}^n u_i^2.&lt;/math&gt;&lt;div style="text-align: right;"&gt; '''(3)''' &lt;/div&gt;
First, assume that there exist local coordinates {{math|''y'' {{=}} (''y''&lt;sub&gt;1&lt;/sub&gt;, ... ''y&lt;sub&gt;n&lt;/sub&gt;''), ''z'' {{=}} '''''φ'''''(''y''), 0 {{=}} '''''φ'''''(0)}}, such that
:&lt;math&gt;S(\boldsymbol{\phi}(y)) = y_1^2 + \cdots + y_{r-1}^2 + \sum_{i,j = r}^n y_i y_j H_{ij} (y),&lt;/math&gt;&lt;div style="text-align: right;"&gt; '''(4)''' &lt;/div&gt;
where {{math|''H&lt;sub&gt;ij&lt;/sub&gt;''}} is symmetric due to equation (2). By a linear change of the variables {{math|(''y&lt;sub&gt;r&lt;/sub&gt;'', ... ''y&lt;sub&gt;n&lt;/sub&gt;'')}}, we can assure that {{math|''H&lt;sub&gt;rr&lt;/sub&gt;''(0) ≠ 0}}. From the [[chain rule]], we have

:&lt;math&gt;\frac{\partial^2 S (\boldsymbol{\phi}(y))}{\partial y_i \partial y_j} = \sum_{l,k=1}^n \left. \frac{\partial^2 S (z)}{\partial z_k \partial z_l} \right|_{z=\boldsymbol{\phi}(y)} \frac{\partial \phi_k}{\partial y_i} \frac{\partial \phi_l}{\partial y_j} + \sum_{k=1}^n \left. \frac{\partial S (z)}{\partial z_k } \right|_{z=\boldsymbol{\phi}(y)} \frac{\partial^2 \phi_k}{\partial y_i \partial y_j}&lt;/math&gt;

Therefore:

:&lt;math&gt;S''_{yy} (\boldsymbol{\phi}(0)) = \boldsymbol{\phi}'_y(0)^T S''_{zz}(0) \boldsymbol{\phi}'_y(0), \qquad \det \boldsymbol{\phi}'_y(0) \neq 0;&lt;/math&gt;

whence,

:&lt;math&gt;0 \neq \det  S''_{yy} (\boldsymbol{\phi}(0)) = 2^{r-1} \det \left( 2H_{ij}(0) \right).&lt;/math&gt;

The matrix {{math|(''H&lt;sub&gt;ij&lt;/sub&gt;''(0))}} can be recast in the [[Jordan normal form]]: {{math|(''H&lt;sub&gt;ij&lt;/sub&gt;''(0)) {{=}} ''LJL''&lt;sup&gt;−1&lt;/sup&gt;}}, were {{mvar|L}} gives the desired non-singular linear transformation and the diagonal of {{mvar|J}} contains non-zero [[Eigenvalues and eigenvectors|eigenvalues]] of {{math|(''H&lt;sub&gt;ij&lt;/sub&gt;''(0))}}. If {{math|''H&lt;sub&gt;ij&lt;/sub&gt;''(0) ≠ 0}} then, due to continuity of {{math|''H&lt;sub&gt;ij&lt;/sub&gt;''(''y'')}}, it must be also non-vanishing in some neighborhood of the origin. Having introduced &lt;math&gt;\tilde{H}_{ij}(y) = H_{ij}(y)/H_{rr}(y)&lt;/math&gt;, we write

:&lt;math&gt;\begin{align}
S(\boldsymbol{\varphi}(y)) =&amp; y_1^2 + \cdots + y_{r-1}^2 + H_{rr}(y) \sum_{i,j = r}^n y_i y_j \tilde{H}_{ij} (y) \\
=&amp; y_1^2 + \cdots + y_{r-1}^2 + H_{rr}(y)\left[ y_r^2 + 2y_r \sum_{j=r+1}^n y_j \tilde{H}_{rj} (y) +  \sum_{i,j = r+1}^n y_i y_j \tilde{H}_{ij} (y) \right] \\
=&amp; y_1^2 + \cdots + y_{r-1}^2 + H_{rr}(y)\left[ \left( y_r + \sum_{j=r+1}^n y_j \tilde{H}_{rj} (y)\right)^2 - \left( \sum_{j=r+1}^n y_j \tilde{H}_{rj} (y)\right)^2  \right] + H_{rr}(y) \sum_{i,j = r+1}^n y_i y_j \tilde{H}_{ij}(y) 
\end{align}&lt;/math&gt;

Motivated by the last expression, we introduce new coordinates {{math|''z'' {{=}} '''''η'''''(''x''), 0 {{=}} '''''η'''''(0),}}

:&lt;math&gt;x_r = \sqrt{ H_{rr}(y) } \left( y_r + \sum_{j=r+1}^n y_j \tilde{H}_{rj} (y)\right), \qquad x_j = y_j, \quad \forall j \neq r.&lt;/math&gt;

The change of the variables {{math|''y'' ↔ ''x''}} is locally invertible since the corresponding [[Jacobian matrix and determinant|Jacobian]] is non-zero,

:&lt;math&gt;\left. \frac{\partial x_r}{\partial y_k} \right|_{y=0} = \sqrt{H_{rr}(0)} \left[ \delta_{r,\, k} + \sum_{j=r+1}^n \delta_{j, \, k} \tilde{H}_{jr}(0) \right].&lt;/math&gt;

Therefore,

:&lt;math&gt;S(\boldsymbol{\eta}(x)) = {x}_1^2 + \cdots + {x}_r^2 + \sum_{i,j = r+1}^n {x}_i {x}_j W_{ij} (x).&lt;/math&gt;&lt;div style="text-align: right;"&gt; '''(5)''' &lt;/div&gt;

Comparing equations (4) and (5), we conclude that equation (3) is verified. Denoting the [[Eigenvalues and eigenvectors|eigenvalues]] of &lt;math&gt;S''_{zz}(0)&lt;/math&gt; by {{math|''μ&lt;sub&gt;j&lt;/sub&gt;''}}, equation (3) can be rewritten as

:&lt;math&gt;S(\boldsymbol{\varphi}(w)) = \frac 12 \sum_{j=1}^n \mu_j w_j^2.&lt;/math&gt;&lt;div style="text-align: right;"&gt; '''(6)''' &lt;/div&gt;

Therefore,

:&lt;math&gt;S''_{ww} (\boldsymbol{\varphi}(0)) = \boldsymbol{\varphi}'_w(0)^T S''_{zz}(0) \boldsymbol{\varphi}'_w(0),&lt;/math&gt;&lt;div style="text-align: right;"&gt; '''(7)''' &lt;/div&gt;

From equation (6), it follows that &lt;math&gt;\det S''_{ww} (\boldsymbol{\varphi}(0)) = \mu_1 \cdots \mu_n&lt;/math&gt;. The [[Jordan normal form]] of &lt;math&gt;S''_{zz}(0)&lt;/math&gt; reads &lt;math&gt;S''_{zz}(0) = P J_z P^{-1}&lt;/math&gt;, where {{math|''J&lt;sub&gt;z&lt;/sub&gt;''}} is an upper diagonal matrix containing the [[Eigenvalues and eigenvectors|eigenvalues]] and {{math|det ''P'' ≠ 0}}; hence, &lt;math&gt;\det S''_{zz} (0) = \mu_1 \cdots \mu_n&lt;/math&gt;. We obtain from equation (7)

:&lt;math&gt;\det S''_{ww} (\boldsymbol{\varphi}(0)) = \left[\det \boldsymbol{\varphi}'_w(0) \right]^2 \det S''_{zz}(0) \Longrightarrow  \det \boldsymbol{\varphi}'_w(0) = \pm 1.&lt;/math&gt;	

If &lt;math&gt;\det \boldsymbol{\varphi}'_w(0) = -1&lt;/math&gt;, then interchanging two variables assures that &lt;math&gt;\det \boldsymbol{\varphi}'_w(0) = +1&lt;/math&gt;. 
&lt;/div&gt;
&lt;/div&gt;
&lt;!-- END OF COMPLEX MORSE LEMMA --&gt;

=== The asymptotic expansion in the case of a single non-degenerate saddle point ===
Assume
# {{math|&amp;thinsp;''f''&amp;thinsp;(''z'')}} and {{math|''S''(''z'')}} are [[Holomorphic function|holomorphic]] functions in an [[Open set|open]], [[Bounded set (topological vector space)|bounded]], and [[Simply connected space|simply connected]] set {{math|Ω''&lt;sub&gt;x&lt;/sub&gt;'' ⊂ '''C'''&lt;sup&gt;''n''&lt;/sup&gt;}} such that the {{math|''I&lt;sub&gt;x&lt;/sub&gt;'' {{=}} Ω''&lt;sub&gt;x&lt;/sub&gt;'' ∩ '''R'''&lt;sup&gt;''n''&lt;/sup&gt;}} is [[Connected space|connected]];
# &lt;math&gt;\Re(S(z))&lt;/math&gt; has a single maximum: &lt;math&gt;\max_{z \in I_x} \Re(S(z)) = \Re(S(x^0))&lt;/math&gt; for exactly one point {{math|''x''&lt;sup&gt;0&lt;/sup&gt; ∈ ''I&lt;sub&gt;x&lt;/sub&gt;''}};
# {{math|''x''&lt;sup&gt;0&lt;/sup&gt;}} is a non-degenerate saddle point (i.e., {{math|∇''S''(''x''&lt;sup&gt;0&lt;/sup&gt;) {{=}} 0}} and &lt;math&gt;\det S''_{xx}(x^0) \neq 0&lt;/math&gt;).

Then, the following asymptotic holds
:&lt;math&gt;I(\lambda) \equiv \int_{I_x} f(x) e^{\lambda S(x)} dx = \left( \frac{2\pi}{\lambda}\right)^{\frac{n}{2}} e^{\lambda S(x^0)} \left(f(x^0)+ O\left(\lambda^{-1}\right) \right) \prod_{j=1}^n (-\mu_j)^{-\frac{1}{2}}, \qquad \lambda \to \infty,&lt;/math&gt;&lt;div style="text-align: right;"&gt; '''(8)''' &lt;/div&gt;
where {{math|''μ&lt;sub&gt;j&lt;/sub&gt;''}} are eigenvalues of the [[Hessian matrix|Hessian]] &lt;math&gt;S''_{xx}(x^0)&lt;/math&gt; and &lt;math&gt;(-\mu_j)^{-\frac{1}{2}}&lt;/math&gt; are defined with arguments
:&lt;math&gt;\left | \arg\sqrt{-\mu_j} \right| &lt; \tfrac{\pi}{4}.&lt;/math&gt;
&lt;div style="text-align: right;"&gt; '''(9)''' &lt;/div&gt;

This statement is a special case of more general results presented in Fedoryuk (1987).&lt;ref&gt;{{harvtxt|Fedoryuk|1987}}, pages 417-420.&lt;/ref&gt;

&lt;div style="clear:both;width:80%;" class="NavFrame collapsed"&gt;
&lt;div class="NavHead" style="background-color:#CCCCFF; text-align:left; font-size:normal;"&gt;Derivation of equation (8)&lt;/div&gt;
&lt;div class="NavContent" style="text-align:left;"&gt;

[[File:Illustration To Derivation Of Asymptotic For Saddle Point Integration.pdf|thumb|center|An illustration to the derivation of equation (8)]]

First, we deform the contour {{math|''I&lt;sub&gt;x&lt;/sub&gt;''}} into a new contour &lt;math&gt;I'_x \subset \Omega_x&lt;/math&gt; passing through the saddle point {{math|''x''&lt;sup&gt;0&lt;/sup&gt;}} and sharing the boundary with {{math|''I&lt;sub&gt;x&lt;/sub&gt;''}}. This deformation does not change the value of the integral {{math|''I''(''λ'')}}. We employ the [[Method of steepest descent#Complex Morse Lemma|Complex Morse Lemma]] to change the variables of integration. According to the lemma, the function {{math|'''''φ'''''(''w'')}} maps a neighborhood {{math|''x''&lt;sup&gt;0&lt;/sup&gt; ∈ ''U'' ⊂ Ω''&lt;sub&gt;x&lt;/sub&gt;''}} onto a neighborhood {{math|Ω''&lt;sub&gt;w&lt;/sub&gt;''}} containing the origin.  The integral {{math|''I''(''λ'')}} can be split into two: {{math|''I''(''λ'') {{=}} ''I''&lt;sub&gt;0&lt;/sub&gt;(''λ'') + ''I''&lt;sub&gt;1&lt;/sub&gt;(''λ'')}}, where {{math|''I''&lt;sub&gt;0&lt;/sub&gt;(''λ'')}} is the integral over &lt;math&gt;U\cap I'_x&lt;/math&gt;, while {{math|''I''&lt;sub&gt;1&lt;/sub&gt;(''λ'')}} is  over &lt;math&gt;I'_x \setminus (U\cap I'_x)&lt;/math&gt; (i.e., the remaining part of the contour {{math|''I′&lt;sub&gt;x&lt;/sub&gt;''}}). Since the latter region does not contain the saddle point {{math|''x''&lt;sup&gt;0&lt;/sup&gt;}}, the value of {{math|''I''&lt;sub&gt;1&lt;/sub&gt;(''λ'')}} is exponentially smaller than {{math|''I''&lt;sub&gt;0&lt;/sub&gt;(''λ'')}} as {{math|''λ'' → ∞}};&lt;ref&gt;This conclusion follows from a comparison between the final asymptotic for {{math|''I''&lt;sub&gt;0&lt;/sub&gt;(''λ'')}}, given by equation (8), and [[Method of steepest descent#A simple estimate .5B1.5D|a simple estimate]] for the discarded integral {{math|''I''&lt;sub&gt;1&lt;/sub&gt;(''λ'')}}.&lt;/ref&gt; thus, {{math|''I''&lt;sub&gt;1&lt;/sub&gt;(''λ'')}} is ignored. Introducing the contour {{math|''I&lt;sub&gt;w&lt;/sub&gt;''}} such that &lt;math&gt;U\cap I'_x = \boldsymbol{\varphi}(I_w)&lt;/math&gt;, we have

:&lt;math&gt;I_0(\lambda) = e^{\lambda S(x^0)} \int_{I_w} f[\boldsymbol{\varphi}(w)] \exp\left( \lambda \sum_{j=1}^n \tfrac{\mu_j}{2} w_j^2 \right) \left |\det\boldsymbol{\varphi}_w'(w) \right | dw.&lt;/math&gt;&lt;div style="text-align: right;"&gt; '''(10)''' &lt;/div&gt;

Recalling that {{math|''x''&lt;sup&gt;0&lt;/sup&gt; {{=}} '''''φ'''''(0)}} as well as &lt;math&gt;\det \boldsymbol{\varphi}_w'(0) = 1&lt;/math&gt;, we expand the pre-exponential function into a Taylor series and keep just the leading zero-order term

:&lt;math&gt;I_0(\lambda) \approx f(x^0) e^{\lambda S(x^0)} \int_{\mathbf{R}^n} \exp \left( \lambda \sum_{j=1}^n \tfrac{\mu_j}{2} w_j^2 \right) dw = f(x^0)e^{\lambda S(x^0)} \prod_{j=1}^n \int_{-\infty}^{\infty} e^{\frac{1}{2}\lambda \mu_j y^2} dy.&lt;/math&gt;&lt;div style="text-align: right;"&gt; '''(11)''' &lt;/div&gt;

Here, we have substituted the integration region {{math|''I&lt;sub&gt;w&lt;/sub&gt;''}} by {{math|'''R'''&lt;sup&gt;''n''&lt;/sup&gt;}} because both contain the origin, which is a saddle point, hence they are equal up to an exponentially small term.&lt;ref&gt;This is justified by comparing the integral asymptotic over {{math|'''R'''&lt;sup&gt;''n''&lt;/sup&gt;}} [see equation (8)] with [[Method of steepest descent#A simple estimate .5B1.5D|a simple estimate]] for the altered part.&lt;/ref&gt; The integrals in the r.h.s. of equation (11) can be expressed as

:&lt;math&gt;\mathcal{I}_j = \int_{-\infty}^{\infty} e^{\frac{1}{2} \lambda \mu_j y^2} dy = 2\int_0^{\infty} e^{-\frac{1}{2} \lambda \left(\sqrt{-\mu_j} y\right)^2} dy = 2\int_0^{\infty} e^{-\frac{1}{2} \lambda \left |\sqrt{-\mu_j} \right|^2 y^2\exp\left(2i\arg\sqrt{-\mu_j}\right)} dy.&lt;/math&gt;&lt;div style="text-align: right;"&gt; '''(12)''' &lt;/div&gt;

From this representation, we conclude that condition (9) must be satisfied in order for the r.h.s. and l.h.s. of equation (12) to coincide. According to assumption 2, &lt;math&gt;\Re \left( S_{xx}''(x^0) \right)&lt;/math&gt; is a [[Definite bilinear form|negatively defined quadratic form]] (viz., &lt;math&gt;\Re(\mu_j)&lt;0&lt;/math&gt;) implying the existence of the integral &lt;math&gt;\mathcal{I}_j&lt;/math&gt;, which is readily calculated

:&lt;math&gt;\mathcal{I}_j = \frac{2}{\sqrt{-\mu_j}\sqrt{\lambda}} \int_0^{\infty} e^{-\frac{\xi^2}{2}} d\xi = \sqrt{\frac{2\pi}{\lambda}} (-\mu_j)^{-\frac{1}{2}}.&lt;/math&gt;

&lt;/div&gt;
&lt;/div&gt;
&lt;!-- END OF PROOF --&gt;

Equation (8) can also be written as

:&lt;math&gt;I(\lambda) = \left( \frac{2\pi}{\lambda}\right)^{\frac{n}{2}} e^{\lambda S(x^0)} \left ( \det (-S_{xx}''(x^0)) \right )^{-\frac{1}{2}} \left (f(x^0) + O\left(\lambda^{-1}\right) \right),&lt;/math&gt;&lt;div style="text-align: right;"&gt; '''(13)''' &lt;/div&gt;

where the branch of

:&lt;math&gt;\sqrt{\det \left (-S_{xx}''(x^0) \right)}&lt;/math&gt;

is selected as follows

:&lt;math&gt;\begin{align}
\left (\det \left (-S_{xx}''(x^0) \right ) \right)^{-\frac{1}{2}} &amp;= \exp\left( -i \text{ Ind} \left (- S_{xx}''(x^0) \right ) \right) \prod_{j=1}^n \left| \mu_j \right|^{-\frac{1}{2}}, \\
\text{Ind} \left (-S_{xx}''(x^0) \right) &amp;= \tfrac{1}{2} \sum_{j=1}^n \arg (-\mu_j), &amp;&amp; |\arg(-\mu_j)| &lt; \tfrac{\pi}{2}.
\end{align}&lt;/math&gt;

Consider important special cases:

* If {{math|''S''(''x'')}} is real valued for real {{mvar|x}} and {{math|''x''&lt;sup&gt;0&lt;/sup&gt;}} in {{math|'''R'''&lt;sup&gt;''n''&lt;/sup&gt;}} (aka, the '''multidimensional Laplace method'''), then&lt;ref&gt;See equation (4.4.9) on page 125 in {{harvtxt|Fedoryuk|1987}}&lt;/ref&gt;
::&lt;math&gt;\text{Ind} \left(-S_{xx}''(x^0) \right ) = 0.&lt;/math&gt;

* If {{math|''S''(''x'')}} is purely imaginary for real {{mvar|x}} (i.e., &lt;math&gt;\Re(S(x)) = 0&lt;/math&gt; for all {{mvar|x}} in {{math|'''R'''&lt;sup&gt;''n''&lt;/sup&gt;}}) and {{math|''x''&lt;sup&gt;0&lt;/sup&gt;}} in {{math|'''R'''&lt;sup&gt;''n''&lt;/sup&gt;}} (aka, the '''multidimensional stationary phase method'''),&lt;ref&gt;Rigorously speaking, this case cannot be inferred from equation (8) because [[Method of steepest descent#The asymptotic expansion in the case of a single non-degenerate saddle point|the second assumption]], utilized in the derivation, is violated. To include the discussed case of a purely imaginary phase function, condition (9) should be replaced by &lt;math&gt; \left | \arg\sqrt{-\mu_j} \right | \leqslant \tfrac{\pi}{4}.&lt;/math&gt;&lt;/ref&gt; then&lt;ref&gt;See equation (2.2.6') on page 186 in {{harvtxt|Fedoryuk|1987}}&lt;/ref&gt;

::&lt;math&gt;\text{Ind} \left (-S_{xx}''(x^0) \right ) = \frac{\pi}{4} \text{sign }S_{xx}''(x_0),&lt;/math&gt;

:where &lt;math&gt;\text{sign }S_{xx}''(x_0)&lt;/math&gt; denotes [[Sylvester's law of inertia#Statement of the theorem|the signature of matrix]] &lt;math&gt;S_{xx}''(x_0)&lt;/math&gt;, which equals to the number of negative eigenvalues minus the number of positive ones. It is noteworthy that in applications of the stationary phase method to the multidimensional WKB approximation in quantum mechanics (as well as in optics), {{math|Ind}} is related to the [[Maslov index]] see, e.g., {{harvtxt|Chaichian|Demichev|2001}} and {{harvtxt|Schulman|2005}}.

== The case of multiple non-degenerate saddle points ==
If the function {{math|''S''(''x'')}} has multiple isolated non-degenerate saddle points, i.e.,

:&lt;math&gt;\nabla S \left (x^{(k)} \right ) = 0, \quad \det S''_{xx} \left (x^{(k)} \right ) \neq 0, \quad x^{(k)} \in \Omega_x^{(k)},&lt;/math&gt;

where

:&lt;math&gt;\left \{ \Omega_x^{(k)} \right \}_{k=1}^K&lt;/math&gt;

is an [[open cover]] of {{math|Ω&lt;sub&gt;''x''&lt;/sub&gt;}}, then the calculation of the integral asymptotic is reduced to the case of a single saddle point by employing the [[partition of unity]]. The [[partition of unity]] allows us to construct a set of continuous functions {{math|''ρ&lt;sub&gt;k&lt;/sub&gt;''(''x'') : Ω&lt;sub&gt;''x''&lt;/sub&gt; → [0, 1], 1 ≤ ''k'' ≤ ''K'',}} such that

:&lt;math&gt;\begin{align} 
\sum_{k=1}^K \rho_k(x) &amp;= 1, &amp;&amp; \forall x \in \Omega_x, \\
\rho_k(x) &amp;= 0 &amp;&amp; \forall x \in \Omega_x\setminus \Omega_x^{(k)}.
\end{align}&lt;/math&gt;

Whence,

:&lt;math&gt;\int_{I_x \subset \Omega_x} f(x) e^{\lambda S(x)} dx \equiv \sum_{k=1}^K \int_{I_x \subset \Omega_x} \rho_k(x) f(x) e^{\lambda S(x)} dx.&lt;/math&gt;

Therefore as {{math|''λ'' → ∞}} we have:

:&lt;math&gt;\sum_{k=1}^K  \int_{\text{a neighborhood of }x^{(k)}} f(x) e^{\lambda S(x)} dx = \left(\frac{2\pi}{\lambda}\right)^{\frac{n}{2}} \sum_{k=1}^K e^{\lambda S \left (x^{(k)} \right )} \left ( \det \left(-S_{xx}'' \left (x^{(k)} \right )\right) \right)^{-\frac{1}{2}} f \left (x^{(k)} \right ),&lt;/math&gt;

where equation (13) was utilized at the last stage, and the pre-exponential function {{math|&amp;thinsp;''f''&amp;thinsp;(''x'')}} at least must be continuous.

== The other cases ==
When {{math|∇''S''(''z''&lt;sup&gt;0&lt;/sup&gt;) {{=}} 0}} and &lt;math&gt;\det S''_{zz}(z^0) = 0&lt;/math&gt;, the point {{math|''z''&lt;sup&gt;0&lt;/sup&gt; ∈ '''C'''&lt;sup&gt;''n''&lt;/sup&gt;}} is called a '''degenerate saddle point''' of a function {{math|''S''(''z'')}}.

Calculating the asymptotic of

:&lt;math&gt; \int f(x) e^{\lambda S(x)} dx,&lt;/math&gt;

when {{math|''λ'' → ∞, &amp;thinsp;''f''&amp;thinsp;(''x'')}} is continuous, and {{math|''S''(''z'')}} has a degenerate saddle point, is a very rich problem, whose solution heavily relies on the [[catastrophe theory]]. Here, the catastrophe theory replaces the [[Method of steepest descent#Complex Morse Lemma|Morse lemma]], valid only in the non-degenerate case, to transform the function {{math|''S''(''z'')}} into one of the multitude of canonical representations. For further details see, e.g., {{harvtxt|Poston|Stewart|1978}} and {{harvtxt|Fedoryuk|1987}}.

Integrals with degenerate saddle points naturally appear in many applications including [[Caustic (optics)|optical caustics]] and the multidimensional [[WKB approximation]] in quantum mechanics.

The other cases such as, e.g., {{math|&amp;thinsp;''f''&amp;thinsp;(''x'')}} and/or {{math|''S''(''x'')}} are discontinuous or when an extremum of {{math|''S''(''x'')}} lies at the integration region's boundary, require special care (see, e.g., {{harvtxt|Fedoryuk|1987}} and {{harvtxt|Wong|1989}}).

==Extensions and generalizations==
An extension of the steepest descent method is the so-called ''nonlinear stationary phase/steepest descent method''. Here, instead of integrals, one needs to evaluate asymptotically solutions of [[Riemann&amp;ndash;Hilbert factorization]] problems.

Given a contour ''C'' in the [[complex sphere]], a function ''f'' defined on that contour and a special point, say infinity, one seeks a function ''M'' holomorphic away from the contour ''C'', with prescribed jump across ''C'', and with a given normalization at infinity. If ''f'' and hence ''M'' are matrices rather than scalars this is a problem that in general does not admit an explicit solution.

An asymptotic evaluation is then possible along the lines of the linear stationary phase/steepest descent method. The idea is to reduce asymptotically the solution of the given Riemann&amp;ndash;Hilbert problem to that of a simpler, explicitly solvable, Riemann&amp;ndash;Hilbert problem. Cauchy's theorem is used to justify deformations  of the jump contour.

The nonlinear stationary phase was introduced by Deift and Zhou in 1993, based on earlier work of the Russian mathematician Alexander Its. A (properly speaking) nonlinear steepest descent method was introduced by Kamvissis, K. McLaughlin and P. Miller in 2003, based on previous work of Lax, Levermore, Deift, Venakides and Zhou. As in the linear case, steepest descent contours solve a min-max problem. In the nonlinear case they turn out to be "S-curves" (defined in a different context back in the 80s by Stahl, Gonchar and Rakhmanov).

The nonlinear stationary phase/steepest descent method has applications to the theory of [[soliton]] equations and [[integrable model]]s, [[random matrices]] and [[combinatorics]].

==See also==
[[Pearcey integral]]

==Notes==
{{reflist}}

==References==
*{{Citation
 | last1=Chaichian
 | first1=M.
 | last2=Demichev
 | first2=A.
 | title=Path Integrals in Physics Volume 1: Stochastic Process and Quantum Mechanics
 | publisher=Taylor &amp; Francis
 | year=2001
 | page=174
 | ISBN=075030801X
}}
*{{Citation
 | last1=Debye
 | first1=P.
 | author1-link=Peter Debye
 | title=Näherungsformeln für die Zylinderfunktionen für große Werte des Arguments und unbeschränkt veränderliche Werte des Index
 | doi=10.1007/BF01450097
 | year=1909
 | journal=Mathematische Annalen
 | volume=67
 | issue=4
 | pages=535–558}} English translation in {{Citation | last1=Debye | first1=Peter J. W. | title=The collected papers of Peter J. W. Debye | publisher=Interscience Publishers, Inc., New York | isbn=978-0-918024-58-9 |mr=0063975 | year=1954}}
*{{Citation
 | last=Deift
 | first=P.
 | last2=Zhou
 | first2=X.
 | year=1993
 | title=A steepest descent method for oscillatory Riemann-Hilbert problems. Asymptotics for the MKdV equation
 | periodical=Ann. of Math.
 | volume=137
 | issue=2
 | pages=295–368
 | doi=10.2307/2946540
 | publisher=The Annals of Mathematics, Vol. 137, No. 2
 | jstor=2946540
| arxiv=math/9201261
 }}.
*{{Citation
 | last=Erdelyi
 | first=A.
 | year=1956
 | title=Asymptotic Expansions
 | publisher=Dover
}}.
*{{eom|id=Saddle_point_method|first=M V|last= Fedoryuk}}.
*{{Citation
 | last1=Fedoryuk
 | first1=M. V. 
 | title=Asymptotic: Integrals and Series
 | publisher =Nauka, Moscow
 | year=1987}} [in Russian].
*{{Citation
 | last=Kamvissis
 | first=S.
 | last2=McLaughlin
 | first2=K. T.-R.
 | last3=Miller
 | first3=P.
 | year=2003
 | title=Semiclassical Soliton Ensembles for the Focusing Nonlinear Schrödinger Equation
 | periodical=Annals of Mathematics Studies
 | volume=154
 | publisher=Princeton University Press
}}.
*{{Citation
 |title=Sullo svolgimento del quoziente di due serie ipergeometriche in frazione continua infinita
 |first=B.
 |last=Riemann
 |year=1863}} (Unpublished note, reproduced in Riemann's collected papers.)
*{{Citation
 |authorlink=Carl Ludwig Siegel
 |last=Siegel
 |first= C. L.
 |title=Über Riemanns Nachlaß zur analytischen Zahlentheorie
 |journal= Quellen Studien zur Geschichte der Math. Astron. und Phys. Abt. B: Studien 2
 |pages= 45–80
 |year= 1932}} Reprinted in Gesammelte Abhandlungen, Vol. 1. Berlin: Springer-Verlag, 1966.
*{{Citation
 |last1=Poston
 |first1=T.
 |last2=Stewart
 |first2=I.
 |title=Catastrophe Theory and Its Applications
 |publisher=Pitman|year=1978
}}.
*{{Citation
 |last1=Schulman
 |first1=L. S.
 |title=Techniques and Applications of Path Integration
 |publisher=Dover
 |year=2005
 |ISBN=0486445283
 |chapter=Ch. 17: The Phase of the Semiclassical Amplitude 
}} 
*{{Citation
 |last1=Wong
 |first1=R.
 |title=Asymptotic approximations of integrals
 |publisher=Academic Press
 |year=1989
}}.

[[Category:Asymptotic analysis]]
[[Category:Perturbation theory]]</text>
      <sha1>gt5920fxvjic2i3ygf5t8ehpbvoe53r</sha1>
    </revision>
  </page>
  <page>
    <title>Natural pseudodistance</title>
    <ns>0</ns>
    <id>16593557</id>
    <revision>
      <id>811493431</id>
      <parentid>597437125</parentid>
      <timestamp>2017-11-22T00:09:45Z</timestamp>
      <contributor>
        <username>Siddharthist</username>
        <id>28122572</id>
      </contributor>
      <comment>Add {{math-stub}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2769">In [[size theory]], the '''natural pseudodistance''' between two [[size pair]]s &lt;math&gt;(M,\varphi:M\to \mathbb{R})\ &lt;/math&gt;, &lt;math&gt;(N,\psi:N\to \mathbb{R})\ &lt;/math&gt; is the value &lt;math&gt;\inf_h \|\varphi-\psi\circ h\|_\infty\ &lt;/math&gt;, where &lt;math&gt;h\ &lt;/math&gt; varies in the set of all [[homeomorphism]]s from the manifold &lt;math&gt;M\ &lt;/math&gt; to the manifold &lt;math&gt;N\ &lt;/math&gt; and &lt;math&gt;\|\cdot\|_\infty\ &lt;/math&gt; is the [[supremum norm]]. If &lt;math&gt;M\ &lt;/math&gt; and &lt;math&gt;N\ &lt;/math&gt; are not homeomorphic, then the natural pseudodistance is defined to be &lt;math&gt;\infty\ &lt;/math&gt;.
It is usually assumed that &lt;math&gt;M\ &lt;/math&gt;, &lt;math&gt;N\ &lt;/math&gt; are &lt;math&gt;C^1\ &lt;/math&gt; [[closed manifold]]s and the [[measuring function]]s &lt;math&gt;\varphi,\psi\ &lt;/math&gt; are &lt;math&gt;C^1\ &lt;/math&gt;. Put another way, the natural pseudodistance  measures the infimum of the change of the measuring function induced by the homeomorphisms from &lt;math&gt;M\ &lt;/math&gt; to &lt;math&gt;N\ &lt;/math&gt;.

The concept of natural pseudodistance can be easily extended to [[size pair]]s where the measuring function &lt;math&gt;\varphi\ &lt;/math&gt; takes values in &lt;math&gt;\mathbb{R}^m\ &lt;/math&gt;
.&lt;ref name="FroMu99"&gt;Patrizio Frosini, Michele Mulazzani, ''Size homotopy groups for computation of natural size distances'', [[Bulletin of the Belgian Mathematical Society]], 6:455-464, 1999.&lt;/ref&gt;

==Main properties==
It can be proved &lt;ref name="DoFro04"&gt;Pietro Donatini, Patrizio Frosini, ''Natural pseudodistances between closed manifolds'', Forum Mathematicum, 16(5):695-715, 2004.&lt;/ref&gt;
that the natural pseudodistance always equals the Euclidean distance between two critical values of the measuring functions (possibly, of the ''same'' measuring function) divided by a suitable positive integer &lt;math&gt;k\ &lt;/math&gt;.
If &lt;math&gt;M\ &lt;/math&gt; and &lt;math&gt;N\ &lt;/math&gt; are surfaces, the number &lt;math&gt;k\ &lt;/math&gt; can be assumed to be &lt;math&gt;1\ &lt;/math&gt;, &lt;math&gt;2\ &lt;/math&gt; or &lt;math&gt;3\ &lt;/math&gt;.&lt;ref&gt;Pietro Donatini, Patrizio Frosini, ''Natural pseudodistances between closed surfaces'',
Journal of the European Mathematical Society, 9(2):231–253, 2007.&lt;/ref&gt; If &lt;math&gt;M\ &lt;/math&gt; and &lt;math&gt;N\ &lt;/math&gt; are curves, the number &lt;math&gt;k\ &lt;/math&gt; can be assumed to be &lt;math&gt;1\ &lt;/math&gt; or &lt;math&gt;2\ &lt;/math&gt;.&lt;ref&gt;Pietro Donatini, Patrizio Frosini, ''Natural pseudodistances between closed curves'', Forum Mathematicum, 21(6):981–999, 2009.&lt;/ref&gt;
If an optimal homeomorphism &lt;math&gt;\bar h\ &lt;/math&gt; exists (i.e., &lt;math&gt;\|\varphi-\psi\circ \bar h\|_\infty=\inf_h \|\varphi-\psi\circ h\|_\infty\ &lt;/math&gt;), then &lt;math&gt;k\ &lt;/math&gt; can be assumed to be &lt;math&gt;1\ &lt;/math&gt;.&lt;ref name="DoFro04"/&gt;

==See also==
* [[Fréchet surface|Fréchet distance]]
* [[Size function]]
* [[Size functor]]
* [[Size homotopy group]]

==References==
{{reflist}}

{{math-stub}}

[[Category:Differential geometry]]</text>
      <sha1>ombtxwwrqhouzg91jh4mz9ia8zhk64r</sha1>
    </revision>
  </page>
  <page>
    <title>Nielsen–Schreier theorem</title>
    <ns>0</ns>
    <id>18401725</id>
    <revision>
      <id>844730787</id>
      <parentid>830425678</parentid>
      <timestamp>2018-06-06T19:25:24Z</timestamp>
      <contributor>
        <ip>2601:807:8000:75F1:6D2A:8480:A6C6:8FAE</ip>
      </contributor>
      <comment>/* Proof */ "Schreier" was misspelled.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11616">In [[group theory]], a branch of mathematics, the '''Nielsen–Schreier theorem''' states that every [[subgroup]] of a [[free group]] is itself free.&lt;ref name="stillwell"/&gt;&lt;ref&gt;{{citation|Magnus|Karass|Solitar|1976}}, Corollary 2.9, p. 95.&lt;/ref&gt;&lt;ref name="j80"/&gt; It is named after [[Jakob Nielsen (mathematician)|Jakob Nielsen]] and [[Otto Schreier]].

==Statement of the theorem==
A free group may be defined from a [[Presentation of a group|group presentation]] consisting of a [[generating set of a group|set of generators]] and the [[empty set]] of relations (equations that the generators satisfy). That is, it is the unique group in which every element is a product of some sequence of generators and their inverses, and in which there are no equations between group elements that do not follow in a trivial way from the equations {{math|''gg''&lt;sup&gt;&amp;minus;1&lt;/sup&gt;}} describing the relation between a generator and its inverse. The elements of a free group may be described as all of the possible [[Word (group theory)|reduced words]]; these are [[string (computer science)|strings]] of generators and their inverses, in which no generator is adjacent to its own inverse. Two reduced words may be multiplied by [[concatenation|concatenating]] them and then removing any generator-inverse pairs that result from the concatenation.

The '''Nielsen–Schreier theorem''' states that if {{mvar|G}} is a subgroup of a free group, then {{mvar|G}} is itself [[group isomorphism|isomorphic]] to a free group. That is, there exists a subset {{mvar|S}} of elements of {{mvar|G}} such that every element in {{mvar|G}} is a product of members of {{mvar|S}} and their inverses, and such that {{mvar|S}} satisfies no nontrivial relations.

The '''Nielsen–Schreier formula''', or '''Schreier index formula''', quantifies the result in the case where the subgroup has finite index: if {{mvar|G}} is a free group on {{mvar|n}} generators, and {{mvar|H}} is a subgroup of finite [[Index of a subgroup|index]] {{mvar|e}}, then {{mvar|H}} is free of rank&lt;ref&gt;{{harvp|Fried|Jarden|2008|p=355}}&lt;/ref&gt;
:&lt;math&gt; 1 + e(n-1) \ . &lt;/math&gt;

==Example==
Let {{mvar|G}} be the free group with two generators, {{mvar|a}} and {{mvar|b}}, and let {{mvar|E}} be the subgroup consisting of all reduced words that are products of evenly many generators or their inverses. Then {{mvar|E}} is itself generated by the six elements {{math|1=''p'' = ''aa''}},  {{math|1=''q'' = ''ab''}},  {{math|1=''r'' = ''ab''&lt;sup&gt;&amp;minus;1&lt;/sup&gt;}}, {{math|1=''s'' = ''ba''}}, {{math|1=''t'' = ''ba''&lt;sup&gt;&amp;minus;1&lt;/sup&gt;}}, and {{math|1=''u'' = ''bb''}}. A factorization of any reduced word in {{mvar|E}} into these generators and their inverses may be constructed simply by taking consecutive pairs of symbols in the reduced word. However, this is not a free presentation of {{mvar|E}} because it satisfies the relations {{math|1=''p'' = ''qr''&lt;sup&gt;&amp;minus;1&lt;/sup&gt; = ''rq''&lt;sup&gt;&amp;minus;1&lt;/sup&gt;}} and {{math|1=''s'' = ''tu''&lt;sup&gt;&amp;minus;1&lt;/sup&gt; = ''ut''&lt;sup&gt;&amp;minus;1&lt;/sup&gt;}}. Instead, {{mvar|E}} is generated as a free group by the three elements {{math|1=''p'' = ''aa''}},  {{math|1=''q'' = ''ab''}}, and {{math|1=''s'' = ''ba''}}. Any factorization of a word into a product of generators from the six-element generating set {{math|{''p'', ''q'', ''r'', ''s'', ''t'', ''u''}}} can be transformed into a product of generators from this smaller set by replacing {{mvar|r}} with {{math|''ps''&lt;sup&gt;&amp;minus;1&lt;/sup&gt;}}, replacing {{mvar|t}} with {{math|''sp''&lt;sup&gt;&amp;minus;1&lt;/sup&gt;}}, and replacing {{mvar|u}} with {{math|''sp''&lt;sup&gt;&amp;minus;1&lt;/sup&gt;''q''}}. There are no additional relations satisfied by these three generators, so {{mvar|E}} is the free group generated by {{mvar|p}}, {{mvar|q}}, and {{mvar|s}}.&lt;ref&gt;{{harvtxt|Johnson|1997}}, ex. 15, p. 12.&lt;/ref&gt; The Nielsen–Schreier theorem states that this example is not a coincidence: like {{mvar|E}}, every subgroup of a free group can be generated as a free group, possibly with a larger set of generators.

==Proof==
It is possible to prove the Nielsen–Schreier theorem using [[algebraic topology]].&lt;ref name="stillwell"&gt;{{harvtxt|Stillwell|1993}}, Section 2.2.4, The Nielsen–Schreier Theorem, pp. 103–104.&lt;/ref&gt; A free group {{mvar|G}} on a set of generators is the [[fundamental group]] of a [[rose (topology)|bouquet of circles]], a [[topological graph]] with a single vertex and with an edge for each generator.&lt;ref name="still-bouquet"/&gt; Any subgroup {{mvar|H}} of the fundamental group is itself a fundamental group of a [[covering space]] of the bouquet, a (possibly infinite) topological [[Schreier coset graph]] that has one vertex for each [[coset]] of the subgroup.&lt;ref&gt;{{harvtxt|Stillwell|1993}}, Section 2.2.2, The Subgroup Property, pp. 100–101.&lt;/ref&gt; And in any topological graph, it is possible to shrink the edges of a [[spanning tree]] of the graph, producing a bouquet of circles that has the same fundamental group {{mvar|H}}. Since {{mvar|H}} is the fundamental group of a bouquet of circles, it is itself free.&lt;ref name="still-bouquet"&gt;{{harvtxt|Stillwell|1993}}, Section 2.1.8, Freeness of the Generators, p. 97.&lt;/ref&gt; This proof is due to {{harvs|first1=Reinhold|last1=Baer|author1-link=Reinhold Baer|first2=Friedrich|last2=Levi|author2-link=Friedrich Wilhelm Levi|year=1936|txt}}; the original proof by Schreier forms the Schreier graph in a different way as a quotient of the [[Cayley graph]] of {{mvar|G}} modulo the action of {{mvar|H}}.

According to [[Schreier's subgroup lemma]], a set of generators for a free presentation of {{mvar|H}} may be constructed from [[cycle (graph theory)|cycles]] in the covering graph formed by concatenating a spanning tree path from a base point (the coset of the identity) to one of the cosets, a single non-tree edge, and an inverse spanning tree path from the other endpoint of the edge back to the base point.&lt;ref&gt;{{harvtxt|Stillwell|1993}}, Section 2.2.6, Schreier Transversals, pp. 105–106.&lt;/ref&gt;

==Axiomatic foundations==
Although several different proofs of the Nielsen–Schreier theorem are known, they all depend on the [[axiom of choice]]. In the proof based on fundamental groups of bouquets, for instance, the axiom of choice appears in the guise of the statement that every connected graph has a spanning tree. The use of this axiom is necessary, as there exist models of [[Zermelo–Fraenkel set theory]] in which the axiom of choice and the Nielsen–Schreier theorem are both false. The Nielsen–Schreier theorem in turn implies a weaker version of the axiom of choice, for finite sets.&lt;ref&gt;{{harvtxt|Läuchli|1962}}&lt;/ref&gt;&lt;ref&gt; {{harvtxt|Howard|1985}}.&lt;/ref&gt;

==History==
The Nielsen–Schreier theorem is a [[Nonabelian group|non-abelian]] analogue of an older result of [[Richard Dedekind]], that every subgroup of a [[free abelian group]] is free [[abelian group|abelian]].&lt;ref name="j80"&gt;{{harvtxt|Johnson|1980}}, Section 2, The Nielsen–Schreier Theorem, pp. 9–23.&lt;/ref&gt;

{{harvs|first=Jakob|last=Nielsen|year=1921|txt}} originally proved a restricted form of the theorem, stating that any finitely-generated subgroup of a free group is free. His proof involves performing a sequence of [[Nielsen transformation]]s on the subgroup's generating set that reduce their length (as reduced words in the free group from which they are drawn).&lt;ref name="stillwell"/&gt;&lt;ref&gt;{{citation|Magnus|Karass|Solitar|1976}}, Section 3.2, A Reduction Process, pp. 121–140.&lt;/ref&gt; Otto Schreier proved the Nielsen–Schreier theorem in its full generality in his 1926 [[habilitation]] [[thesis]], ''Die Untergruppen der freien Gruppe'', also published in 1927 in ''Abh. math. Sem. Hamburg. Univ.''&lt;ref&gt;{{MacTutor|name=Otto Schreier|id=Schreier}}&lt;/ref&gt;&lt;ref&gt;{{citation|page=117|title=Jakob Nielsen, Collected Mathematical Papers: 1913-1932|first=Vagn Lundsgaard|last=Hansen|publisher=Birkhäuser|year=1986|isbn=978-0-8176-3140-6}}.&lt;/ref&gt;

The topological proof based on fundamental groups of bouquets of circles is due to {{harvs|last1=Baer|first1=Reinhold|author1-link=Reinhold Baer|last2=Levi|first2=Friedrich|author2-link=Friedrich Wilhelm Levi|year=1936|txt}}. Another topological proof, based on the [[Bass–Serre theory]] of [[group action]]s on [[Real tree|trees]], was published by {{harvs|authorlink=Jean-Pierre Serre|first=Jean-Pierre|last=Serre|year=1970|txt}}.&lt;ref name="rotman"&gt;{{harvtxt|Rotman|1995}}, The Nielsen–Schreier Theorem, pp. 383–387.&lt;/ref&gt;

==See also==
*[[Fundamental theorem of cyclic groups]], a similar result for [[cyclic group]]s that in the infinite case may be seen as a special case of the Nielsen–Schreier theorem

==Notes==
{{reflist|colwidth=40em}}

==References==
*{{citation|last1=Baer|first=Reinhold|authorlink=Reinhold Baer|title=Freie Produkte und ihre Untergruppen|first2=Friedrich|last2=Levi|author2-link=Friedrich Wilhelm Levi|journal=Compositio Mathematica |volume=3|year=1936|pages=391–398}}.
*{{citation | last1=Fried | first1=Michael D. | author-link1=Michael D. Fried | last2=Jarden | first2=Moshe | author-link2=Moshe Jarden | title=Field arithmetic | edition=3rd | series=Ergebnisse der Mathematik und ihrer Grenzgebiete. 3. Folge | volume=11 | publisher=[[Springer-Verlag]] | year=2008 | isbn=978-3-540-77269-9 | zbl=1145.12001 | page=70 }}.
*{{citation
 | last = Howard | first = Paul E.
 | doi = 10.2307/2274234
 | mr = 793126
 | issue = 2
 | journal = The Journal of Symbolic Logic
 | pages = 458–467
 | title = Subgroups of a free group and the axiom of choice
 | volume = 50
 | year = 1985}}.
*{{citation|title=Topics in the Theory of Group Presentations|volume=42|series=London Mathematical Society lecture note series|first=D. L.|last=Johnson|publisher=Cambridge University Press|year=1980|isbn=978-0-521-23108-4}}.
*{{citation|title=Presentations of Groups|volume=15|series=London Mathematical Society student texts|first=D. L.|last=Johnson|edition=2nd|publisher=Cambridge University Press|year=1997|isbn=978-0-521-58542-2}}.
*{{citation
 | last = Läuchli | first = Hans
 | mr = 0143705
 | journal = Commentarii Mathematici Helvetici
 | pages = 1–18
 | title = Auswahlaxiom in der Algebra
 | volume = 37
 | year = 1962
 | doi=10.1007/bf02566957}}.
*{{Citation | last1=Magnus | first1=Wilhelm | author1-link=Wilhelm Magnus | last2= Karrass | first2 =Abraham | first3=Donald |last3=Solitar | edition=2nd revised | title=Combinatorial Group Theory | publisher=[[Dover Publications]] | year=1976}}.
*{{Citation | last1=Nielsen | first1=Jakob | author1-link=Jakob Nielsen (mathematician) | title=Om regning med ikke-kommutative faktorer og dens anvendelse i gruppeteorien | language=Danish | jfm=48.0123.03 | year=1921 | journal=Math. Tidsskrift B | volume=1921 | pages=78–94}}.
*{{citation|title=An Introduction to the Theory of Groups|volume=148|series=Graduate Texts in Mathematics|first=Joseph J.|last=Rotman|author-link=Joseph J. Rotman|edition=4th|publisher=Springer-Verlag|year=1995|isbn=978-0-387-94285-8}}.
*{{citation|first=J.-P.|last=Serre|authorlink=Jean-Pierre Serre|title=Groupes Discretes|series=Extrait de I'Annuaire du College de France|location=Paris|year=1970}}.
*{{citation|first=J.-P.|last=Serre|authorlink=Jean-Pierre Serre|title=Trees|publisher=Springer-Verlag|year=1980|isbn=3-540-10103-9}}.
*{{citation|title=Classical Topology and Combinatorial Group Theory|last=Stillwell|first=John|authorlink=John Stillwell|series=Graduate Texts in Mathematics|volume=72|edition=2nd|year=1993|publisher=Springer-Verlag}}.

{{DEFAULTSORT:Nielsen-Schreier theorem}}
[[Category:Properties of groups]]
[[Category:Axiom of choice]]
[[Category:Theorems in algebra]]</text>
      <sha1>tagjsly35rcgxhewsak7rm9ubn6oae5</sha1>
    </revision>
  </page>
  <page>
    <title>Paul Benioff</title>
    <ns>0</ns>
    <id>57028478</id>
    <revision>
      <id>852196857</id>
      <parentid>849535205</parentid>
      <timestamp>2018-07-27T08:21:30Z</timestamp>
      <contributor>
        <username>Josvebot</username>
        <id>14967932</id>
      </contributor>
      <minor/>
      <comment>Fixing [[WP:CHECKWIKI]] #16: unicode contol character (and other minor general edits caused by AWB), replaced: →</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11374">{{Multiple issues|
{{BLP sources|date=April 2018}}
{{More footnotes|date=April 2018}}
}}
'''Paul A. Benioff''' &lt;ref&gt;Date of birth and career information from ''American Men and Women of Science'', Thomson Gale 2004&lt;/ref&gt; is an American physicist who helped pioneer the field of [[quantum computing]]. Benioff is best known for his research in [[quantum information]] theory that demonstrated the theoretical possibility of quantum computers.

==Early life and education==
Benioff was born on May 1, 1930, in Pasadena, California.&lt;ref&gt;Date of birth and career information from ''American Men and Women of Science'', Thomson Gale 2004&lt;/ref&gt; His father was a professor of [[seismology]] at the [[California Institute of Technology]], and his mother received a master’s degree in English from the [[University of California, Berkeley]].

Benioff also attended Berkeley, where he earned an undergraduate degree in [[botany]], in 1951. After a two-year stint working in nuclear chemistry for Tracerlab, he returned to Berkeley, in 1959, to obtain his Ph.D. in nuclear chemistry.

==Career and research==
In 1960, Benioff spent a year at the [[Weizmann Institute of Science]] in Israel as a postdoctoral fellow. He then spent six months at the [[Niels Bohr Institute]] in Copenhagen, as a Ford Fellow. In 1961, he began a long career at [[Argonne National Laboratory]], first with its Chemistry Division and later in 1978 in the lab’s Environmental Impact Division. Benioff remained at Argonne until he retired in 1995. He continues to conduct research at the laboratory as a post-retirement research participant for the Physics Division. In 1979, Benioff taught the foundations of quantum mechanics as a visiting professor at [[Tel Aviv University]], and worked as a visiting scientist at CNRS Marseilles, in 1979 and 1982.

In the 1970s, Benioff began to research the theoretical feasibility of [[quantum computing]]. His early research culminated in a paper, published in 1980 that described a quantum mechanical model of [[Turing Machines]].&lt;ref&gt;"The Computer as a Physical System: A Microscopic Quantum Mechanical Hamiltonian Model of Computers as Represented by Turing Machines", Paul Benioff, ''Journal of Statistical Physics'', 22, 563, 1980.&lt;/ref&gt; This work was based on a classical description in 1973 of reversible Turing machines by Bennett.&lt;ref&gt;"Logical reversibility of computation", C. H. Bennett, ''IBM Journal of  Research and  Development'',  Vol. 17, 525, (1973).&lt;/ref&gt; Benioff’s model of a quantum computer was reversible, and did not dissipate energy.&lt;ref&gt;"Quantum Mechanical Models of Turing Machines That Dissipate No Energy", Paul Benioff, ''Physical Review Letters'', 48, 1581 (1982).&lt;/ref&gt; At the time, there were several papers arguing that the creation of a reversible model of quantum computing was impossible. Benioff’s paper was the first to show that reversible quantum computing was theoretically possible. This work and other later work by several authors initiated the field of quantum computing. After publishing several more papers on quantum computers, the idea began to gain traction with industry, banking, and government agencies. The field is now a fast-growing area of research.

Throughout his career at Argonne, Benioff conducted research in many fields, including [[mathematics]], [[physics]] and [[chemistry]]. While in the Chemistry Division, he conducted research on nuclear reaction theory, as well as the relationship between the foundations of physics and mathematics.

After joining Argonne’s Environmental Impact Division in 1978, Benioff continued work on quantum computing and on foundational issues. This included descriptions of quantum robots, quantum mechanical models of different types of numbers, and other topics. More recently, his work has been on the effects of number scaling and local mathematics on physics and geometry. As an emeritus, he continues to work on these and other foundational topics.

==Awards and recognition==
In 2000, Benioff received the Quantum Communication Award of the International Organization for Quantum Communication, Computing, and Measurement, as well as the Quantum Computing and Communication Prize from [[Tamagawa University]] in Japan. He became a fellow of the [[American Physical Society]] in 2001. The following year, he was awarded the Special University of Chicago Medal for Distinguished Performance at Argonne National Laboratory. In 2016, Argonne held a conference in honor of his quantum computing work.

==Selected scientific works==
*”Cosmic-ray production rate and mean removal time of beryllium-7 from the atmosphere," ''Physical Review'', Vol. 104, 1956, pp.&amp;nbsp;1122–1130.
*”Information theory in quantum statistical mechanics," ''Physical Letters'', Vol. 14, 1965, pp.&amp;nbsp;196–197.
*”Some aspects of the relationship between mathematical logic and physics. I," ''Journal of Mathematical Physics'', Vol. 11, 1970, pp.&amp;nbsp;2553–2569.
*”Some aspects of the relationship between mathematical logic and physics. II," ''Journal of Mathematical Physics'', Vol. 12, 1971, pp.&amp;nbsp;360–376.
*”Operator valued measures in quantum mechanics: finite and infinite processes," ''Journal of Mathematical Physics'', Vol. 13, 1972, pp.&amp;nbsp;231–242. 
*”Decision procedures in quantum mechanics," ''Journal of Mathematical Physics'', Vol. 13, 1972, pp.&amp;nbsp;908–915.
*”Procedures in quantum mechanics without Von Neumann's projection axiom," ''Journal of Mathematical Physics'', Vol. 13, 1972, pp.&amp;nbsp;1347–1355.
*”Some consequences of the strengthened interpretative rules of quantum mechanics," ''Journal of Mathematical Physics'', Vol. 15, 1974, pp.&amp;nbsp;552–559. 
*”Models of Zermelo Frankel set theory as carriers for the mathematics of physics. I", ''Journal of Mathematical Physics'', Vol. 17, 1976, pp.&amp;nbsp;618–628. 
*”Models of Zermelo Frankel set theory as carriers for the mathematics of physics. II," ''Journal of Mathematical Physics'', Vol. 17, 1976, pp.&amp;nbsp;629–640.
*”Finite and infinite measurement sequences in quantum mechanics and randomness: The Everett interpretation," ''Journal of Mathematical Physics'', Vol. 18, 1977, pp.&amp;nbsp;2289–2295.
*"The computer as a physical system: A microscopic quantum mechanical Hamiltonian model of computers as represented by Turing machines", ''Journal of Statistical Physics'', Vol. 22, 1980, pp.&amp;nbsp;563–591.
*"Quantum mechanical hamiltonian models of turing machines", ''Journal of Statistical Physics'', Vol. 29, 1982, pp.&amp;nbsp;515–546. 
*"Quantum Mechanical Models of Turing Machines That Dissipate No Energy", ''Phys. Rev. Lett.'', Vol. 48, 1982, pp.&amp;nbsp;1581–1585.
*"Quantum mechanical Hamiltonian models of discrete processes that erase their own histories: Application to Turing machines, Int. J". ''Theor. Phys.'', Vol. 21, 1982, pp.&amp;nbsp;177–201.&lt;ref&gt;Contribution to a 1981 MIT conference concerning quantum computing&lt;/ref&gt;
*"Comment on 'Dissipation in Computation'," ''Physical Review Letters'', Vol. 53, 1984, pp.&amp;nbsp;1203.
*"Quantum Mechanical Hamiltonian Models of Computers", ''Annals New York Academy of Sciences'', Vol. 480, 1986, pp.&amp;nbsp;475–486.
*"Quantum ballistic evolution in quantum mechanics: Application to quantum computers", ''Phys. Rev. A'', Vol. 54, 1996, pp.&amp;nbsp;1106–1123, [https://arxiv.org/abs/quant-ph/9610026 Arxiv].
*"Tight binding Hamiltonians and Quantum Turing Machines", ''Phys. Rev. Lett.'', Vol. 78, 1997, pp.&amp;nbsp;590–593.
*"Transmission and spectral aspects of tight binding hamiltonians for the counting quantum turing machine," ''Physical Review B'', Vol. 55, 1997, pp.&amp;nbsp;9482–9493. 
*"Models of Quantum Turing Machines", ''Fortschritte der Physik'', Vol. 46, 1998, pp.&amp;nbsp;423–441, [https://arxiv.org/abs/quant-ph/9708054 Arxiv].
*"Quantum robots and environments", ''Phys. Rev. A'', Vol. 58, 1998, pp.&amp;nbsp;893–904, [https://arxiv.org/abs/quant-ph/9802067 Arxiv].
*"Quantum Robots and Quantum Computers", in: A. J. G. Hey (Hrsg.), ''Feynman and Computation'', Perseus Books 1999, pp.&amp;nbsp;155–176, [https://arxiv.org/abs/quant-ph/9706012 Arxiv].
*"A simple example of definitions of truth, validity, consistency, and completeness in quantum mechanics," ''Physical Review A'', Vol. 59, 1999, pp.&amp;nbsp;4223–4252.
*"The Representation of Natural Numbers in Quantum Mechanics", ''Phys. Rev. A'', Vol. 63, 2001, 032305, [https://arxiv.org/abs/quant-ph/0003063 Arxiv].
*"Efficient Implementation and the Product State Representation of Numbers", ''Phys. Rev. A'', Vol. 64, 2001, pp.&amp;nbsp;052310, [https://arxiv.org/abs/quant-ph/0104061 Arxiv].
*"Language is physical," ''Quantum Information Proceedings'', Vol. 1, 2002, pp.&amp;nbsp;495–509.
*"Use of mathematical logical concepts in quantum mechanics: an example," ''Journal of Physics A: Mathematical and General'', Vol. 35, 2002, pp.&amp;nbsp;5843–5857. 
*"Towards a Coherent Theory of Physics and Mathematics", ''Found. Phys.'', Vol. 32, 2002, pp.&amp;nbsp;989–1029, [https://arxiv.org/abs/quant-ph/0201093 Arxiv].
*"The Representation of Numbers in Quantum Mechanics", ''Algorithmica'', Vol. 34, 2002, pp.&amp;nbsp;529–559, [https://arxiv.org/abs/quant-ph/0103078 Arxiv].
*"Towards a Coherent Theory of Physics and Mathematics: The Theory-Experiment Connection", ''Foundations of Physics'', Vol. 35, 2005, pp.&amp;nbsp;1825–1856, [https://arxiv.org/abs/quant-ph/0403209 Arxiv].
*"Representation of complex rational numbers in quantum mechanics", ''Phys. Rev. A'', Vol. 72, 2005, pp.&amp;nbsp;032314, [https://arxiv.org/abs/quant-ph/0503154 Arxiv].
*“Fields of quantum reference frames based on different representations of rational numbers as states of qubit strings.” Submitted to proceedings, 3rd Feynman Festival, University of Maryland, 2006, ''Journal of Physics'': Conference Series 70 (2007) 012003.
*"A representation of real and complex numbers in quantum theory," ''International Journal of Pure and Applied Mathematics'', Vol. 39, 2007, pp.&amp;nbsp;297–339. 
*"Reference frame fields based on quantum theory representations of real and complex numbers," ''Advances in Quantum Computation'', Vol. 482, 2009, pp.&amp;nbsp;125–163.
*“Effects on quantum physics of the local availability of mathematics and space time dependent scaling factors for number systems.” Chapter 2, in ''Advances in Quantum Theory'', I. I. Cotaescu (Ed.), Intech open access publisher, 2012.
*“Gauge theory extension to include number scaling by boson field: Effects on some aspects of physics and geometry.” Chapter in ''Recent Developments in Bosons Research'', Ignace Tremblay (Ed.), Nova Press, 2013.
*"Fiber bundle description of number scaling in gauge theory and geometry," ''Quantum Studies: Mathematics and Foundations'', Vol. 2, 2015, pp.&amp;nbsp;289–313.
*"Effects of a scalar scaling field on quantum mechanics," ''Quantum Information Processing'', Vol. 15(7), 2016, pp.&amp;nbsp;3005–3034. 
*"The no information at a distance principle and local mathematics: some effects on physics and geometry," ''Theoretical Information Studies'', submitted.

==External links==
*[https://www.phy.anl.gov/theory/staff/Benioff_P.html Homepage]

== Notes ==
{{Reflist}}

{{DEFAULTSORT:Benioff, Paul}}
[[Category:20th-century physicists]]
[[Category:20th-century scientists]]
[[Category:Quantum computing]]
[[Category:1930 births]]
[[Category:Living people]]</text>
      <sha1>q2jvh3v623u8eavskn5fjei410bjvq4</sha1>
    </revision>
  </page>
  <page>
    <title>Per capita</title>
    <ns>0</ns>
    <id>357788</id>
    <revision>
      <id>871150350</id>
      <parentid>871147184</parentid>
      <timestamp>2018-11-29T07:28:55Z</timestamp>
      <contributor>
        <username>Girth Summit</username>
        <id>6225634</id>
      </contributor>
      <minor/>
      <comment>Reverted 1 edit by [[Special:Contributions/172.117.160.135|172.117.160.135]] identified as test/vandalism using [[WP:STiki|STiki]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1624">'''''Per capita''''' is a Latin prepositional phrase: ''per'' (preposition, taking the accusative case, meaning "by means of") and ''capita'' (accusative plural of the noun ''caput'', "head"). The phrase thus means "by heads" or "for each head", i.e., per individual/person.  The term is used in a wide variety of [[social sciences]] and statistical research contexts, including government statistics, [[economic indicator]]s, and built environment studies.

It is commonly and usually used in the [[discipline (academia)|field]] of [[statistics]] in place of saying "per person"&lt;ref&gt;{{cite web|url=http://dictionary.reference.com/browse/Per%20capita |title=Per capita &amp;#124; Define Per capita at Dictionary.com |publisher=Dictionary.reference.com |date= |accessdate=2017-04-08}}&lt;/ref&gt; (although ''per caput'' is the Latin for "per head"&lt;ref name="Economist"&gt;[http://www.economist.com/style-guide/per-caput-per-capita "Per caput, per capita."] at ''[[The Economist]]'' [http://www.economist.com/styleguide/introduction style guide].  Retrieved 15 July 2017.&lt;/ref&gt;).
It is also used in [[will (law)|will]]s to indicate that each of the named [[beneficiaries]] should receive, by devise or [[bequest]], equal shares of the estate.&lt;ref name="Economist"/&gt;  This is in contrast to a ''[[per stirpes]]'' division, in which each branch (Latin ''stirps'', plural ''stirpes'') of the [[inheriting]] family inherits an equal share of the [[estate (law)|estate]].

==See also==
* [[Per capita income]]

==References==
{{Wiktionary}}
{{Reflist}}

[[Category:Latin words and phrases]]
[[Category:Social statistics]]


{{Statistics-stub}}</text>
      <sha1>o8pv28mjn6una4vo4c98v6gisll6aa9</sha1>
    </revision>
  </page>
  <page>
    <title>Pfister's sixteen-square identity</title>
    <ns>0</ns>
    <id>40216580</id>
    <revision>
      <id>790740883</id>
      <parentid>769515199</parentid>
      <timestamp>2017-07-15T19:59:17Z</timestamp>
      <contributor>
        <username>Deacon Vorbis</username>
        <id>29330520</id>
      </contributor>
      <minor/>
      <comment>/* top */LaTeX spacing clean up, replaced: \,&lt;/math&gt; → &lt;/math&gt; (2) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7776">In [[algebra]], '''Pfister's sixteen-square identity''' is a non-[[bilinear map|bilinear]] identity of form

:&lt;math&gt;
(x_1^2+x_2^2+x_3^2+\cdots+x_{16}^2)\,(y_1^2+y_2^2+y_3^2+\cdots+y_{16}^2) = z_1^2+z_2^2+z_3^2+\cdots+z_{16}^2
&lt;/math&gt;

It was first proven to exist by [[Hans Zassenhaus|H. Zassenhaus]] and W. Eichhorn in the 1960s,&lt;ref&gt;H. Zassenhaus and W. Eichhorn, "Herleitung von Acht- und Sechzehn-Quadrate-Identitäten mit Hilfe von Eigenschaften der verallgemeinerten Quaternionen und der Cayley-Dicksonchen Zahlen," Arch. Math. 17 (1966), 492-496&lt;/ref&gt; and independently by Pfister&lt;ref&gt;A. Pfister, Zur Darstellung von -1 als Summe von Quadraten in einem Körper," J. London Math. Soc. 40 (1965), 159-165&lt;/ref&gt; around the same time.  There are several versions, a concise one of which is

:&lt;math&gt;\,^{z_1 = {\color{blue}{x_1 y_1 - x_2 y_2 - x_3 y_3 - x_4 y_4 - x_5 y_5 - x_6 y_6 - x_7 y_7 - x_8 y_8}} + u_1 y_9 - u_2 y_{10} - u_3 y_{11} - u_4 y_{12} - u_5 y_{13} - u_6 y_{14} - u_7 y_{15} - u_8 y_{16}}&lt;/math&gt;
:&lt;math&gt;\,^{z_2 = {\color{blue}{x_2 y_1 + x_1 y_2 + x_4 y_3 - x_3 y_4 + x_6 y_5 - x_5 y_6 - x_8 y_7 + x_7 y_8}} + u_2 y_9 + u_1 y_{10} + u_4 y_{11} - u_3 y_{12} + u_6 y_{13} - u_5 y_{14} - u_8 y_{15} + u_7 y_{16}}&lt;/math&gt;
:&lt;math&gt;\,^{z_3 = {\color{blue}{x_3 y_1 - x_4 y_2 + x_1 y_3 + x_2 y_4 + x_7 y_5 + x_8 y_6 - x_5 y_7 - x_6 y_8}} + u_3 y_9 - u_4 y_{10} + u_1 y_{11} + u_2 y_{12} + u_7 y_{13} + u_8 y_{14} - u_5 y_{15} - u_6 y_{16}}&lt;/math&gt;
:&lt;math&gt;\,^{z_4 = {\color{blue}{x_4 y_1 + x_3 y_2 - x_2 y_3 + x_1 y_4 + x_8 y_5 - x_7 y_6 + x_6 y_7 - x_5 y_8}} + u_4 y_9 + u_3 y_{10} - u_2 y_{11} + u_1 y_{12} + u_8 y_{13} - u_7 y_{14} + u_6 y_{15} - u_5 y_{16}}&lt;/math&gt;
:&lt;math&gt;\,^{z_5 = {\color{blue}{x_5 y_1 - x_6 y_2 - x_7 y_3 - x_8 y_4 + x_1 y_5 + x_2 y_6 + x_3 y_7 + x_4 y_8}} + u_5 y_9 - u_6 y_{10} - u_7 y_{11} - u_8 y_{12} + u_1 y_{13} + u_2 y_{14} + u_3 y_{15} + u_4 y_{16}}&lt;/math&gt;
:&lt;math&gt;\,^{z_6 = {\color{blue}{x_6 y_1 + x_5 y_2 - x_8 y_3 + x_7 y_4 - x_2 y_5 + x_1 y_6 - x_4 y_7 + x_3 y_8}} + u_6 y_9 + u_5 y_{10} - u_8 y_{11} + u_7 y_{12} - u_2 y_{13} + u_1 y_{14} - u_4 y_{15} + u_3 y_{16}}&lt;/math&gt;
:&lt;math&gt;\,^{z_7 = {\color{blue}{x_7 y_1 + x_8 y_2 + x_5 y_3 - x_6 y_4 - x_3 y_5 + x_4 y_6 + x_1 y_7 - x_2 y_8}} + u_7 y_9 + u_8 y_{10} + u_5 y_{11} - u_6 y_{12} - u_3 y_{13} + u_4 y_{14} + u_1 y_{15} - u_2 y_{16}}&lt;/math&gt;
:&lt;math&gt;\,^{z_8 = {\color{blue}{x_8 y_1 - x_7 y_2 + x_6 y_3 + x_5 y_4 - x_4 y_5 - x_3 y_6 + x_2 y_7 + x_1 y_8}} + u_8 y_9 - u_7 y_{10} + u_6 y_{11} + u_5 y_{12} - u_4 y_{13} - u_3 y_{14} + u_2 y_{15} + u_1 y_{16}}&lt;/math&gt;
:&lt;math&gt;\,^{z_9  =  x_9 y_1 - x_{10} y_2 - x_{11} y_3 - x_{12} y_4 - x_{13} y_5 - x_{14} y_6 - x_{15} y_7 - x_{16} y_8 + x_1 y_9 - x_2 y_{10} - x_3 y_{11} - x_4 y_{12} - x_5 y_{13} - x_6 y_{14} - x_7 y_{15} - x_8 y_{16}}&lt;/math&gt;
:&lt;math&gt;\,^{z_{10} = x_{10} y_1 + x_9 y_2 + x_{12} y_3 - x_{11} y_4 + x_{14} y_5 - x_{13} y_6 - x_{16} y_7 + x_{15} y_8 + x_2 y_9 + x_1 y_{10} + x_4 y_{11} - x_3 y_{12} + x_6 y_{13} - x_5 y_{14} - x_8 y_{15} + x_7 y_{16}}&lt;/math&gt;
:&lt;math&gt;\,^{z_{11} = x_{11} y_1 - x_{12} y_2 + x_9 y_3 + x_{10} y_4 + x_{15} y_5 + x_{16} y_6 - x_{13} y_7 - x_{14} y_8 + x_3 y_9 - x_4 y_{10} + x_1 y_{11} + x_2 y_{12} + x_7 y_{13} + x_8 y_{14} - x_5 y_{15} - x_6 y_{16}}&lt;/math&gt;
:&lt;math&gt;\,^{z_{12} = x_{12} y_1 + x_{11} y_2 - x_{10} y_3 + x_9 y_4 + x_{16} y_5 - x_{15} y_6 + x_{14} y_7 - x_{13} y_8 + x_4 y_9 + x_3 y_{10} - x_2 y_{11} + x_1 y_{12} + x_8 y_{13} - x_7 y_{14} + x_6 y_{15} - x_5 y_{16}}&lt;/math&gt;
:&lt;math&gt;\,^{z_{13} = x_{13} y_1 - x_{14} y_2 - x_{15} y_3 - x_{16} y_4 + x_9 y_5 + x_{10} y_6 + x_{11} y_7 + x_{12} y_8 + x_5 y_9 - x_6 y_{10} - x_7 y_{11} - x_8 y_{12} + x_1 y_{13} + x_2 y_{14} + x_3 y_{15} + x_4 y_{16}}&lt;/math&gt;
:&lt;math&gt;\,^{z_{14} = x_{14} y_1 + x_{13} y_2 - x_{16} y_3 + x_{15} y_4 - x_{10} y_5 + x_9 y_6 - x_{12} y_7 + x_{11} y_8 + x_6 y_9 + x_5 y_{10} - x_8 y_{11} + x_7 y_{12} - x_2 y_{13} + x_1 y_{14} - x_4 y_{15} + x_3 y_{16}}&lt;/math&gt;
:&lt;math&gt;\,^{z_{15} = x_{15} y_1 + x_{16} y_2 + x_{13} y_3 - x_{14} y_4 - x_{11} y_5 + x_{12} y_6 + x_9 y_7 - x_{10} y_8 + x_7 y_9 + x_8 y_{10} + x_5 y_{11} - x_6 y_{12} - x_3 y_{13} + x_4 y_{14} + x_1 y_{15} - x_2 y_{16}}&lt;/math&gt;
:&lt;math&gt;\,^{z_{16} = x_{16} y_1 - x_{15} y_2 + x_{14} y_3 + x_{13} y_4 - x_{12} y_5 - x_{11} y_6 + x_{10} y_7 + x_9 y_8 + x_8 y_9 - x_7 y_{10} + x_6 y_{11} + x_5 y_{12} - x_4 y_{13} - x_3 y_{14} + x_2 y_{15} + x_1 y_{16}}&lt;/math&gt;

If all &lt;math&gt;x_i&lt;/math&gt; and &lt;math&gt;y_i&lt;/math&gt; with &lt;math&gt;i&gt;8&lt;/math&gt; are set equal to zero, then it reduces to [[Degen's eight-square identity]] (in blue). The &lt;math&gt;u_i&lt;/math&gt; are

:&lt;math&gt;u_1 = \tfrac{(ax_1^2+x_2^2+x_3^2+x_4^2+x_5^2+x_6^2+x_7^2+x_8^2)x_9 - 2x_1(bx_1 x_9 +x_2 x_{10} +x_3 x_{11} +x_4 x_{12} +x_5 x_{13} +x_6 x_{14} +x_7 x_{15} +x_8 x_{16})}{c}&lt;/math&gt;
:&lt;math&gt;u_2 = \tfrac{(x_1^2+ax_2^2+x_3^2+x_4^2+x_5^2+x_6^2+x_7^2+x_8^2)x_{10} - 2x_2(x_1 x_9 +bx_2 x_{10} +x_3 x_{11} +x_4 x_{12} +x_5 x_{13} +x_6 x_{14} +x_7 x_{15} +x_8 x_{16})}{c}&lt;/math&gt;
:&lt;math&gt;u_3 = \tfrac{(x_1^2+x_2^2+ax_3^2+x_4^2+x_5^2+x_6^2+x_7^2+x_8^2)x_{11} - 2x_3(x_1 x_9 +x_2 x_{10} +bx_3 x_{11} +x_4 x_{12} +x_5 x_{13} +x_6 x_{14} +x_7 x_{15} +x_8 x_{16})}{c}&lt;/math&gt;
:&lt;math&gt;u_4 = \tfrac{(x_1^2+x_2^2+x_3^2+ax_4^2+x_5^2+x_6^2+x_7^2+x_8^2)x_{12} - 2x_4(x_1 x_9 +x_2 x_{10} +x_3 x_{11} +bx_4 x_{12} +x_5 x_{13} +x_6 x_{14} +x_7 x_{15} +x_8 x_{16})}{c}&lt;/math&gt;
:&lt;math&gt;u_5 = \tfrac{(x_1^2+x_2^2+x_3^2+x_4^2+ax_5^2+x_6^2+x_7^2+x_8^2)x_{13} - 2x_5(x_1 x_9 +x_2 x_{10} +x_3 x_{11} +x_4 x_{12} +bx_5 x_{13} +x_6 x_{14} +x_7 x_{15} +x_8 x_{16})}{c}&lt;/math&gt;
:&lt;math&gt;u_6 = \tfrac{(x_1^2+x_2^2+x_3^2+x_4^2+x_5^2+ax_6^2+x_7^2+x_8^2)x_{14} - 2x_6(x_1 x_9 +x_2 x_{10} +x_3 x_{11} +x_4 x_{12} +x_5 x_{13} +bx_6 x_{14} +x_7 x_{15} +x_8 x_{16})}{c}&lt;/math&gt;
:&lt;math&gt;u_7 = \tfrac{(x_1^2+x_2^2+x_3^2+x_4^2+x_5^2+x_6^2+ax_7^2+x_8^2)x_{15} - 2x_7(x_1 x_9 +x_2 x_{10} +x_3 x_{11} +x_4 x_{12} +x_5 x_{13} +x_6 x_{14} +bx_7 x_{15} +x_8 x_{16})}{c}&lt;/math&gt;
:&lt;math&gt;u_8 = \tfrac{(x_1^2+x_2^2+x_3^2+x_4^2+x_5^2+x_6^2+x_7^2+ax_8^2)x_{16} - 2x_8(x_1 x_9 +x_2 x_{10} +x_3 x_{11} +x_4 x_{12} +x_5 x_{13} +x_6 x_{14} +x_7 x_{15} +bx_8 x_{16})}{c}&lt;/math&gt;

and,

:&lt;math&gt;a=-1,\;\;b=0,\;\;c=x_1^2+x_2^2+x_3^2+x_4^2+x_5^2+x_6^2+x_7^2+x_8^2\,.&lt;/math&gt;

The identity shows that, in general, the product of two sums of sixteen squares is the sum of sixteen [[rational number|rational]] squares. Incidentally, the &lt;math&gt;u_i&lt;/math&gt; also obey,

:&lt;math&gt;u_1^2+u_2^2+u_3^2+u_4^2+u_5^2+u_6^2+u_7^2+u_8^2 = x_{9}^2+x_{10}^2+x_{11}^2+x_{12}^2+x_{13}^2+x_{14}^2+x_{15}^2+x_{16}^2&lt;/math&gt;

No sixteen-square identity exists involving only bilinear functions since [[Hurwitz's theorem (normed division algebras)|Hurwitz's theorem]] states an identity of the form

:&lt;math&gt;(x_1^2+x_2^2+x_3^2+\cdots+x_n^2)(y_1^2+y_2^2+y_3^2+\cdots+y_n^2) = z_1^2+z_2^2+z_3^2+\cdots+z_n^2&lt;/math&gt;

with the &lt;math&gt;z_i&lt;/math&gt; [[bilinear map|bilinear]] functions of the &lt;math&gt;x_i&lt;/math&gt; and &lt;math&gt;y_i&lt;/math&gt; is possible only for ''n'' &amp;isin; {1, 2, 4, 8} . However, the more general [[Pfister's theorem]] (1965)  shows that if the &lt;math&gt;z_i&lt;/math&gt; are [[rational functions]] of one set of variables, hence has a [[denominator]], then it is possible for all &lt;math&gt;n = 2^m&lt;/math&gt;.&lt;ref&gt;Pfister's Theorem on Sums of Squares, Keith Conrad, http://www.math.uconn.edu/~kconrad/blurbs/linmultialg/pfister.pdf&lt;/ref&gt; There are also non-bilinear versions of [[Euler's four-square identity|Euler's four-square]] and [[Degen's eight-square identity|Degen's eight-square]] identities.

==See also==
* [[Brahmagupta–Fibonacci identity]]
* [[Euler's four-square identity]]
* [[Degen's eight-square identity]]
* [[Sedenions]]

==References==
&lt;references/&gt;

==External links==
*[http://sites.google.com/site/tpiezas/0021c Pfister's 16-Square Identity]

[[Category:Analytic number theory]]
[[Category:Mathematical identities]]</text>
      <sha1>5zcf5zn4dgf8x50d3zw2evccx6cnl0r</sha1>
    </revision>
  </page>
  <page>
    <title>Random forest</title>
    <ns>0</ns>
    <id>1363880</id>
    <revision>
      <id>869972790</id>
      <parentid>869966944</parentid>
      <timestamp>2018-11-21T15:43:43Z</timestamp>
      <contributor>
        <ip>146.23.30.234</ip>
      </contributor>
      <comment>Undid revision 869966944 by [[Special:Contributions/145.5.245.122|145.5.245.122]] ([[User talk:145.5.245.122|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="39591">{{short description|An ensemble machine learning method}}
 
{{About|the machine learning technique|other kinds of random tree|Random tree}}
{{machine learning bar}}

'''Random forests''' or '''random decision forests''' are an [[ensemble learning]] method for [[statistical classification|classification]], [[regression analysis|regression]] and other tasks, that operate by constructing a multitude of [[decision tree learning|decision trees]] at training time and outputting the class that is the [[mode (statistics)|mode]] of the classes (classification) or mean prediction (regression) of the individual trees.&lt;ref name="ho1995"/&gt;&lt;ref name="ho1998"/&gt; Random decision forests correct for decision trees' habit of [[overfitting]] to their [[Test set|training set]].{{r|elemstatlearn}}{{rp|587–588}}

The first algorithm for random decision forests was created by [[Tin Kam Ho]]&lt;ref name="ho1995"&gt;{{cite conference
 |first        = Tin Kam
 |last         = Ho
 |title        = Random Decision Forests
 |conference   = Proceedings of the 3rd International Conference on Document Analysis and Recognition, Montreal, QC, 14–16 August 1995
 |year         = 1995
 |pages        = 278–282
 |url          = http://ect.bell-labs.com/who/tkh/publications/papers/odt.pdf
 |access-date  = 5 June 2016
 |archive-url  = https://web.archive.org/web/20160417030218/http://ect.bell-labs.com/who/tkh/publications/papers/odt.pdf
 |archive-date = 17 April 2016
 |dead-url     = yes
 |df           = dmy-all
}}&lt;/ref&gt; using the [[random subspace method]],&lt;ref name="ho1998"&gt;{{cite journal | first = Tin Kam | last = Ho | name-list-format = vanc | title = The Random Subspace Method for Constructing Decision Forests | journal = IEEE Transactions on Pattern Analysis and Machine Intelligence | year = 1998 | volume = 20 | issue = 8 | pages = 832–844 | doi = 10.1109/34.709601 | url = http://ect.bell-labs.com/who/tkh/publications/papers/df.pdf }}&lt;/ref&gt; which, in Ho's formulation, is a way to implement the "stochastic discrimination" approach to classification proposed by Eugene Kleinberg.&lt;ref name="kleinberg1990"&gt;{{cite journal |first=Eugene |last=Kleinberg | name-list-format = vanc |title=Stochastic Discrimination |journal=[[Annals of Mathematics and Artificial Intelligence]] |year=1990 |volume=1 |issue=1–4 |pages=207–239 |url=https://pdfs.semanticscholar.org/faa4/c502a824a9d64bf3dc26eb90a2c32367921f.pdf |doi=10.1007/BF01531079|citeseerx=10.1.1.25.6750 }}&lt;/ref&gt;&lt;ref name="kleinberg1996"&gt;{{cite journal |first=Eugene |last=Kleinberg | name-list-format = vanc |title=An Overtraining-Resistant Stochastic Modeling Method for Pattern Recognition |journal=[[Annals of Statistics]] |year=1996 |volume=24 |issue=6 |pages=2319–2349 |doi=10.1214/aos/1032181157 |mr=1425956}}&lt;/ref&gt;&lt;ref name="kleinberg2000"&gt;{{cite journal|first=Eugene|last=Kleinberg| name-list-format = vanc |title=On the Algorithmic Implementation of Stochastic Discrimination|journal=IEEE Transactions on PAMI|year=2000|volume=22|issue=5|url=https://pdfs.semanticscholar.org/8956/845b0701ec57094c7a8b4ab1f41386899aea.pdf}}&lt;/ref&gt;

An extension of the algorithm was developed by [[Leo Breiman]]&lt;ref name="breiman2001"&gt;{{cite journal | first = Leo | last = Breiman | authorlink = Leo Breiman | name-list-format = vanc | title = Random Forests | journal = [[Machine Learning (journal)|Machine Learning]] | year = 2001 | volume = 45 | issue = 1 | pages = 5–32 | doi = 10.1023/A:1010933404324 }}&lt;/ref&gt; and Adele Cutler,&lt;ref name="rpackage"/&gt; and "Random Forests" is their [[trademark]].&lt;ref&gt;U.S. trademark registration number 3185828, registered 2006/12/19.&lt;/ref&gt; The extension combines Breiman's "[[Bootstrap aggregating|bagging]]" idea and random selection of features, introduced first by Ho&lt;ref name="ho1995"/&gt; and later independently by Amit and [[Donald Geman|Geman]]&lt;ref name="amitgeman1997"&gt;{{cite journal | last = Amit | first = Yali | last2 = Geman | first2 = Donald | authorlink2 = Donald Geman | name-list-format = vanc | title = Shape quantization and recognition with randomized trees | journal = [[Neural Computation (journal)|Neural Computation]] | year = 1997 | volume = 9 | issue = 7 | pages = 1545–1588 | doi = 10.1162/neco.1997.9.7.1545 | url = http://www.cis.jhu.edu/publications/papers_in_database/GEMAN/shape.pdf | citeseerx = 10.1.1.57.6069 }}&lt;/ref&gt; in order to construct a collection of decision trees with controlled variance.

== History ==
The general method of random decision forests was first proposed by Ho in 1995.&lt;ref name="ho1995"/&gt; Ho established that forests of trees splitting with oblique hyperplanes can gain accuracy as they grow without suffering from overtraining, as long as the forests are randomly restricted to be sensitive to only selected [[Feature (machine learning)|feature]] dimensions.  A subsequent work along the same lines&lt;ref name="ho1998"/&gt; concluded that other splitting methods, as long as they are randomly forced to be insensitive to some feature dimensions, behave similarly.  Note that this observation of a more complex classifier (a larger forest) getting more accurate nearly monotonically is in sharp contrast to the common belief that the complexity of a classifier can only grow to a certain level of accuracy before being hurt by overfitting.  The explanation of the forest method's resistance to overtraining can be found in Kleinberg's theory of stochastic discrimination.&lt;ref name="kleinberg1990"/&gt;&lt;ref name="kleinberg1996"/&gt;&lt;ref name="kleinberg2000"/&gt;
 
The early development of Breiman's notion of random forests was influenced by the work of Amit and
Geman&lt;ref name="amitgeman1997"/&gt; who introduced the idea of searching over a random subset of the
available decisions when splitting a node, in the context of growing a single
[[Decision tree|tree]].  The idea of random subspace selection from Ho&lt;ref name="ho1998"/&gt; was also influential in the design of random forests.  In this method a forest of trees is grown,
and variation among the trees is introduced by projecting the training data
into a randomly chosen [[Linear subspace|subspace]] before fitting each tree or each node.  Finally, the idea of
randomized node optimization, where the decision at each node is selected by a
randomized procedure, rather than a deterministic optimization was first
introduced by Dietterich.&lt;ref&gt;{{cite journal | first = Thomas | last = Dietterich | title = An Experimental Comparison of Three Methods for Constructing Ensembles of Decision Trees: Bagging, Boosting, and Randomization | journal = [[Machine Learning (journal)|Machine Learning]] | year = 2000 | pages = 139–157 }}&lt;/ref&gt;

The introduction of random forests proper was first made in a paper
by [[Leo Breiman]].&lt;ref name="breiman2001"/&gt;  This paper describes a method of building a forest of
uncorrelated trees using a [[Classification and regression tree|CART]] like procedure, combined with randomized node
optimization and [[Bootstrap aggregating|bagging]].  In addition, this paper combines several
ingredients, some previously known and some novel, which form the basis of the
modern practice of random forests, in particular:

# Using [[out-of-bag error]] as an estimate of the [[generalization error]].
# Measuring variable importance through permutation.

The report also offers the first theoretical result for random forests in the
form of a bound on the [[generalization error]] which depends on the strength of the
trees in the forest and their [[correlation]].

==Algorithm==

===Preliminaries: decision tree learning===
{{main article|Decision tree learning}}
Decision trees are a popular method for various machine learning tasks. Tree learning "come[s] closest to meeting the requirements for serving as an off-the-shelf procedure for data mining", say [[Trevor Hastie|Hastie]] ''et al.'', "because it is invariant under scaling and various other transformations of feature values, is robust to inclusion of irrelevant features, and produces inspectable models. However, they are seldom accurate".&lt;ref name="elemstatlearn"&gt;{{ElemStatLearn}}&lt;/ref&gt;{{rp|352}}

In particular, trees that are grown very deep tend to learn highly irregular patterns: they [[overfitting|overfit]] their training sets, i.e. have [[Bias–variance tradeoff|low bias, but very high variance]]. Random forests are a way of averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance.&lt;ref name="elemstatlearn"/&gt;{{rp|587–588}} This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance in the final model.

===Bagging===
{{main article|Bootstrap aggregating}}
The training algorithm for random forests applies the general technique of [[bootstrap aggregating]], or bagging, to tree learners. Given a training set {{mvar|X}} = {{mvar|x&lt;sub&gt;1&lt;/sub&gt;}}, ..., {{mvar|x&lt;sub&gt;n&lt;/sub&gt;}} with responses {{mvar|Y}} = {{mvar|y&lt;sub&gt;1&lt;/sub&gt;}}, ..., {{mvar|y&lt;sub&gt;n&lt;/sub&gt;}}, bagging repeatedly (''B'' times) selects a [[Sampling_(statistics)#Replacement_of_selected_units|random sample with replacement]] of the training set and fits trees to these samples:

: For {{mvar|b}} = 1, ..., {{mvar|B}}:
:# Sample, with replacement, {{mvar|n}} training examples from {{mvar|X}}, {{mvar|Y}}; call these {{mvar|X&lt;sub&gt;b&lt;/sub&gt;}}, {{mvar|Y&lt;sub&gt;b&lt;/sub&gt;}}.
:# Train a classification or regression tree {{mvar|f&lt;sub&gt;b&lt;/sub&gt;}} on {{mvar|X&lt;sub&gt;b&lt;/sub&gt;}}, {{mvar|Y&lt;sub&gt;b&lt;/sub&gt;}}.

After training, predictions for unseen samples {{mvar|x'}} can be made by averaging the predictions from all the individual regression trees on {{mvar|x'}}:

:&lt;math&gt;\hat{f} = \frac{1}{B} \sum_{b=1}^Bf_b (x')&lt;/math&gt;

or by taking the majority vote in the case of classification trees.

This bootstrapping procedure leads to better model performance because it decreases the [[Bias–variance dilemma|variance]] of the model, without increasing the bias. This means that while the predictions of a single tree are highly sensitive to noise in its training set, the average of many trees is not, as long as the trees are not correlated. Simply training many trees on a single training set would give strongly correlated trees (or even the same tree many times, if the training algorithm is deterministic); bootstrap sampling is a way of de-correlating the trees by showing them different training sets.

Additionally, an estimate of the uncertainty of the prediction can be made as the standard deviation of the predictions from all the individual regression trees on {{mvar|x'}}:

:&lt;math&gt;\sigma = \sqrt{\frac{\sum_{b=1}^B (f_b(x')  - \hat{f})^2}{B-1} }.&lt;/math&gt;

The number of samples/trees, {{mvar|B}}, is a free parameter. Typically, a few hundred to several thousand trees are used, depending on the size and nature of the training set. An optimal number of trees {{mvar|B}} can be found using [[Cross-validation_(statistics)|cross-validation]], or by observing the ''[[out-of-bag error]]'': the mean prediction error on each training sample {{mvar|xᵢ}}, using only the trees that did not have {{mvar|xᵢ}} in their bootstrap sample.&lt;ref name="islr"&gt;{{cite book |author1=Gareth James |author2=Daniela Witten |author3=Trevor Hastie |author4=Robert Tibshirani |title=An Introduction to Statistical Learning |publisher=Springer |year=2013 |url=http://www-bcf.usc.edu/~gareth/ISL/ |pages=316–321}}&lt;/ref&gt;
The training and test error tend to level off after some number of trees have been fit.

===From bagging to random forests===
{{main article|Random subspace method}}
The above procedure describes the original bagging algorithm for trees. Random forests differ in only one way from this general scheme: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a [[Random subspace method|random subset of the features]]. This process is sometimes called "feature bagging". The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few [[Feature (machine learning)|features]] are very strong predictors for the response variable (target output), these features will be selected in many of the {{mvar|B}} trees, causing them to become correlated. An analysis of how bagging and random subspace projection contribute to accuracy gains under different conditions is given by Ho.&lt;ref name="ho2002"&gt;
{{cite journal | first = Tin Kam | last = Ho | title = A Data Complexity Analysis of Comparative Advantages of Decision Forest Constructors | journal = Pattern Analysis and Applications | year = 2002 | pages = 102–112 | url = http://ect.bell-labs.com/who/tkh/publications/papers/compare.pdf }}&lt;/ref&gt;

Typically, for a classification problem with {{mvar|p}} features, {{sqrt|{{mvar|p}}}} (rounded down) features are used in each split.&lt;ref name="elemstatlearn"/&gt;{{rp|592}}  For regression problems the inventors recommend {{mvar|p/3}} (rounded down) with a minimum node size of 5 as the default.&lt;ref name="elemstatlearn"/&gt;{{rp|592}}

===ExtraTrees===
Adding one further step of randomization yields ''extremely randomized trees'', or ExtraTrees. While similar to ordinary random forests in that they are an ensemble of individual trees, there are two main differences: first, each tree is trained using the whole learning sample (rather than a bootstrap sample), and second, the top-down splitting in the tree learner is randomized. Instead of computing the locally ''optimal'' cut-point for each feature under consideration (based on, e.g., [[information gain]] or the [[Gini impurity]]), a ''random'' cut-point is selected. This value is selected from a uniform distribution within the feature's empirical range (in the tree's training set). Then, of all the randomly generated splits, the split that yields the highest score is chosen to split the node. Similar to ordinary random forests, the number of randomly selected features to be considered at each node can be specified. Default values for this parameter are &lt;math&gt;\sqrt{n}&lt;/math&gt; for classification and &lt;math&gt;n&lt;/math&gt; for regression, where &lt;math&gt;n&lt;/math&gt; is the number of features in the model. &lt;ref&gt;{{Cite journal | doi = 10.1007/s10994-006-6226-1| title = Extremely randomized trees| journal = Machine Learning| volume = 63| pages = 3–42| year = 2006| vauthors = Geurts P, Ernst D, Wehenkel L | url = http://orbi.ulg.ac.be/bitstream/2268/9357/1/geurts-mlj-advance.pdf}}&lt;/ref&gt;

==Properties==

=== Variable importance ===

Random forests can be used to rank the importance of variables in a regression or classification problem in a natural way.  The following technique was described in Breiman's original paper&lt;ref name=breiman2001/&gt; and is implemented in the [[R (programming language)|R]] package randomForest.&lt;ref name="rpackage"&gt;{{cite web |url=https://cran.r-project.org/web/packages/randomForest/randomForest.pdf |title=Documentation for R package randomForest |first1=Andy |last1=Liaw | name-list-format = vanc | date=16 October 2012 |access-date=15 March 2013}}
&lt;/ref&gt;

The first step in measuring the variable importance in a data set &lt;math&gt;\mathcal{D}_n =\{(X_i, Y_i)\}_{i=1}^n&lt;/math&gt; is to fit a random forest to the data.  During the fitting process the [[out-of-bag error]] for each data point is recorded and averaged over the forest (errors on an independent test set can be substituted if bagging is not used during training).

To measure the importance of the &lt;math&gt;j&lt;/math&gt;-th feature after training, the values of the &lt;math&gt;j&lt;/math&gt;-th feature are permuted among the training data and the out-of-bag error is again computed on this perturbed data set.  The importance score for the &lt;math&gt;j&lt;/math&gt;-th feature is computed by averaging the difference in out-of-bag error before and after the permutation over all trees.  The score is normalized by the standard deviation of these differences.

Features which produce large values for this score are ranked as more important than features which produce small values. The statistical definition of the variable importance measure was given and analyzed by Zhu ''et al.''&lt;ref&gt;{{cite journal | vauthors = Zhu R, Zeng D, Kosorok MR | title = Reinforcement Learning Trees | journal = Journal of the American Statistical Association | volume = 110 | issue = 512 | pages = 1770–1784 | date = 2015 | pmid = 26903687 | pmc = 4760114 | doi = 10.1080/01621459.2015.1036994 }}&lt;/ref&gt;

This method of determining variable importance has some drawbacks.  For data including categorical variables with different number of levels, random forests are biased in favor of those attributes with more levels. Methods such as [[partial permutation]]s&lt;ref&gt;{{cite conference
|author=Deng,H.|author2=Runger, G. |author3=Tuv, E.
 |title=Bias of importance measures for multi-valued attributes and solutions
|conference=Proceedings of the 21st International Conference on Artificial Neural Networks (ICANN)
|year=2011|pages=293–300
|url=https://www.researchgate.net/profile/Houtao_Deng/publication/221079908_Bias_of_Importance_Measures_for_Multi-valued_Attributes_and_Solutions/links/0046351909faa8f0eb000000/Bias-of-Importance-Measures-for-Multi-valued-Attributes-and-Solutions.pdf
}}&lt;/ref&gt;&lt;ref&gt;{{cite journal | vauthors = Altmann A, Toloşi L, Sander O, Lengauer T | title = Permutation importance: a corrected feature importance measure | journal = Bioinformatics | volume = 26 | issue = 10 | pages = 1340–7 | date = May 2010 | pmid = 20385727 | doi = 10.1093/bioinformatics/btq134 | url = http://bioinformatics.oxfordjournals.org/content/early/2010/04/12/bioinformatics.btq134.abstract }}&lt;/ref&gt;
and growing unbiased trees&lt;ref&gt;{{cite journal | last = Strobl | first = Carolin | last2 = Boulesteix | first2 = Anne-Laure | last3 = Augustin | first3 = Thomas | name-list-format = vanc | title = Unbiased split selection for classification trees based on the Gini index | journal = Computational Statistics &amp; Data Analysis | year = 2007 | pages = 483–501 | url = https://epub.ub.uni-muenchen.de/1833/1/paper_464.pdf }}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last1=Painsky|first1=Amichai|last2=Rosset|first2=Saharon| name-list-format = vanc |title=Cross-Validated Variable Selection in Tree-Based Methods Improves Predictive Performance|journal=IEEE Transactions on Pattern Analysis and Machine Intelligence|date=2017|volume=39|issue=11|pages=2142–2153|doi=10.1109/tpami.2016.2636831|pmid=28114007|arxiv=1512.03444}}&lt;/ref&gt;can be used to solve the problem.  If the data contain groups of correlated features of similar relevance for the output, then smaller groups are favored over larger groups.&lt;ref&gt;{{cite journal | vauthors = Tolosi L, Lengauer T | title = Classification with correlated features: unreliability of feature ranking and solutions | journal = Bioinformatics | volume = 27 | issue = 14 | pages = 1986–94 | date = July 2011 | pmid = 21576180 | doi = 10.1093/bioinformatics/btr300 | url = http://bioinformatics.oxfordjournals.org/content/27/14/1986.abstract }}&lt;/ref&gt;

=== Relationship to nearest neighbors ===
A relationship between random forests and the [[K-nearest neighbor algorithm|{{mvar|k}}-nearest neighbor algorithm]] ({{mvar|k}}-NN) was pointed out by Lin and Jeon in 2002.&lt;ref name="linjeon02"&gt;{{Cite techreport  |first1=Yi |last1=Lin |first2=Yongho |last2=Jeon |title=Random forests and adaptive nearest neighbors |series=Technical Report No. 1055 |year=2002 |institution=University of Wisconsin |url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.153.9168}}&lt;/ref&gt; It turns out that both can be viewed as so-called ''weighted neighborhoods schemes''. These are models built from a training set &lt;math&gt;\{(x_i, y_i)\}_{i=1}^n&lt;/math&gt; that make predictions &lt;math&gt;\hat{y}&lt;/math&gt; for new points {{mvar|x'}} by looking at the "neighborhood" of the point, formalized by a weight function {{mvar|W}}:

:&lt;math&gt;\hat{y} = \sum_{i=1}^n W(x_i, x') \, y_i.&lt;/math&gt;

Here, &lt;math&gt;W(x_i, x')&lt;/math&gt; is the non-negative weight of the {{mvar|i}}'th training point relative to the new point {{mvar|x'}} in the same tree. For any particular {{mvar|x'}}, the weights for points &lt;math&gt;x_i&lt;/math&gt; must sum to one. Weight functions are given as follows:

* In {{mvar|k}}-NN, the weights are &lt;math&gt;W(x_i, x') = \frac{1}{k}&lt;/math&gt; if {{mvar|x&lt;sub&gt;i&lt;/sub&gt;}} is one of the {{mvar|k}} points closest to {{mvar|x'}}, and zero otherwise.
* In a tree, &lt;math&gt;W(x_i, x') = \frac{1}{k'}&lt;/math&gt; if {{mvar|x&lt;sub&gt;i&lt;/sub&gt;}} is one of the {{mvar|k'}} points in the same leaf as {{mvar|x'}}, and zero otherwise.

Since a forest averages the predictions of a set of {{mvar|m}} trees with individual weight functions &lt;math&gt;W_j&lt;/math&gt;, its predictions are

:&lt;math&gt;\hat{y} = \frac{1}{m}\sum_{j=1}^m\sum_{i=1}^n W_{j}(x_i, x') \, y_i = \sum_{i=1}^n\left(\frac{1}{m}\sum_{j=1}^m W_{j}(x_i, x')\right) \, y_i.&lt;/math&gt;

This shows that the whole forest is again a weighted neighborhood scheme, with weights that average those of the individual trees. The neighbors of {{mvar|x'}} in this interpretation are the points &lt;math&gt;x_i&lt;/math&gt; sharing the same leaf in any tree &lt;math&gt;j&lt;/math&gt;. In this way, the neighborhood of {{mvar|x'}} depends in a complex way on the structure of the trees, and thus on the structure of the training set. Lin and Jeon show that the shape of the neighborhood used by a random forest adapts to the local importance of each feature.&lt;ref name="linjeon02"/&gt;

== Unsupervised learning with random forests ==
As part of their construction, random forest predictors naturally lead to a dissimilarity measure among the observations. One can also define a random forest dissimilarity measure between unlabeled data: the idea is to construct a random forest predictor that distinguishes the “observed” data from suitably generated synthetic data.&lt;ref name=breiman2001/&gt;&lt;ref&gt;{{cite journal |authors=Shi, T., Horvath, S. |year=2006 |title=Unsupervised Learning with Random Forest Predictors |journal=Journal of Computational and Graphical Statistics |volume=15 |issue=1 |pages=118–138  |doi=10.1198/106186006X94072 |jstor=27594168|citeseerx=10.1.1.698.2365 }}&lt;/ref&gt;
The observed data are the original unlabeled data and the synthetic data are drawn from a reference distribution. A random forest dissimilarity can be attractive because it handles mixed variable types very well, is invariant to monotonic transformations of the input variables, and is robust to outlying observations. The random forest dissimilarity easily deals with a large number of semi-continuous variables due to its intrinsic variable selection; for example, the "Addcl 1" random forest dissimilarity weighs the contribution of each variable according to how dependent it is on other variables. The random forest dissimilarity has been used in a variety of applications, e.g. to find clusters of patients based on tissue marker data.&lt;ref&gt;{{cite journal | vauthors = Shi T, Seligson D, Belldegrun AS, Palotie A, Horvath S | title = Tumor classification by tissue microarray profiling: random forest clustering applied to renal cell carcinoma | journal = Modern Pathology | volume = 18 | issue = 4 | pages = 547–57 | date = April 2005 | pmid = 15529185 | doi = 10.1038/modpathol.3800322 | url = http://www.nature.com/modpathol/journal/v18/n4/full/3800322a.html }}&lt;/ref&gt;

== Variants ==
Instead of decision trees, linear models have been proposed and evaluated as base estimators in random forests, in particular [[multinomial logistic regression]] and [[naive Bayes classifier]]s.&lt;ref&gt;{{cite journal |authors=Prinzie, A., Van den Poel, D. |year=2008 |title=Random Forests for multiclass classification: Random MultiNomial Logit |journal=Expert Systems with Applications |volume=34 |issue=3 |pages=1721–1732 |doi=10.1016/j.eswa.2007.01.029}}&lt;/ref&gt;&lt;ref&gt;{{Cite book | doi = 10.1007/978-3-540-74469-6_35 | title=Random Multiclass Classification: Generalizing Random Forests to Random MNL and Random NB | year=2007 | journal=Lecture Notes in Computer Science | volume=4653 | pages=349–358 | last1 = Prinzie | first1 = Anita| isbn=978-3-540-74467-2 }}&lt;/ref&gt;

==Kernel random forest==
In machine learning, kernel random forests establish the connection between [[random forest]]s and [[kernel method]]s. By slightly modifying their definition, [[random forests]] can be rewritten as [[kernel method]]s, which are more interpretable and easier to analyze.&lt;ref name="scornet2015random"&gt;{{cite arXiv
 |first=Erwan|last=Scornet
 |title=Random forests and kernel methods
   |year= 2015|eprint=1502.03836
|class=math.ST
 }}&lt;/ref&gt;

=== History ===
[[Leo Breiman]]&lt;ref name="breiman2000some"&gt;{{cite journal | first = Leo | last = Breiman | authorlink = Leo Breiman | title = Some infinity theory for predictor ensembles | institution = Technical Report 579, Statistics Dept. UCB | year = 2000 | url = http://oz.berkeley.edu/~breiman/some_theory2000.pdf }}{{dead link|date=May 2017 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; was the first person to notice the link between [[random forest]] and [[kernel methods]]. He pointed out that [[random forests]] which are grown using [[i.i.d.]] random vectors in the tree construction are equivalent to a kernel acting on the true margin. Lin and Jeon&lt;ref name="lin2006random"&gt;{{cite journal | first = Yi | last = Lin | first2 = Yongho | last2 = Jeon | title = Random forests and adaptive nearest neighbors | journal = Journal of the American Statistical Association | volume = 101 | number = 474 | pages = 578–590 | year = 2006 | doi = 10.1198/016214505000001230 | citeseerx = 10.1.1.153.9168 }}&lt;/ref&gt; established the connection between [[random forests]] and adaptive nearest neighbor, implying that [[random forests]] can be seen as adaptive kernel estimates. Davies and Ghahramani&lt;ref name="davies2014random"&gt;{{cite arXiv |first=Alex |last=Davies |first2=Zoubin|last2=Ghahramani |title=The Random Forest Kernel and other kernels for big data from random partitions |eprint=1402.4293 |year= 2014 |class=stat.ML }}&lt;/ref&gt; proposed Random Forest Kernel and show that it can empirically outperform state-of-art kernel methods. Scornet&lt;ref name="scornet2015random"/&gt; first defined KeRF estimates and gave the explicit link between KeRF estimates and [[random forest]]. He also gave explicit expressions for kernels based on centered random forest&lt;ref name="breiman2004consistency"&gt;{{cite journal | first = Leo | last = Breiman | first2 = Zoubin | last2 = Ghahramani | name-list-format = vanc | title = Consistency for a simple model of random forests | journal = Statistical Department, University of California at Berkeley. Technical Report | number = 670 | year = 2004 }}&lt;/ref&gt; and uniform random forest,&lt;ref name="arlot2014analysis"&gt;{{cite arXiv |first=Sylvain |last=Arlot  | first2 = Robin | last2 = Genuer | name-list-format = vanc |title=Analysis of purely random forests bias |eprint=1407.3939 |year= 2014 |class=math.ST  }}&lt;/ref&gt; two simplified models of [[random forest]]. He named these two KeRFs Centered KeRF and Uniform KeRF, and proved upper bounds on their rates of consistency.

=== Notations and definitions ===
==== Preliminaries: Centered forests ====
Centered forest&lt;ref name="breiman2004consistency"/&gt; is a simplified model for Breiman's original [[random forest]], which uniformly selects an attribute among all attributes and performs splits at the center of the cell along the pre-chosen attribute. The algorithm stops when a fully binary tree of level &lt;math&gt;k&lt;/math&gt; is built, where &lt;math&gt;k \in\mathbb{N} &lt;/math&gt; is a parameter of the algorithm.

==== Uniform forest ====
Uniform forest&lt;ref name="arlot2014analysis"/&gt; is another simplified model for Breiman's original [[random forest]], which uniformly selects a feature among all features and performs splits at a point uniformly drawn on the side of the cell, along the preselected feature.

==== From random forest to KeRF ====
Given a training sample  &lt;math&gt;\mathcal{D}_n =\{(\mathbf{X}_i, Y_i)\}_{i=1}^n&lt;/math&gt; of &lt;math&gt;[0,1]^p\times\mathbb{R}&lt;/math&gt;-valued independent random variables distributed as the independent prototype pair &lt;math&gt;(\mathbf{X}, Y)&lt;/math&gt;, where &lt;math&gt;\operatorname{E}[Y^2]&lt;\infty&lt;/math&gt;. We aim at predicting the response &lt;math&gt;Y&lt;/math&gt;, associated with the random variable &lt;math&gt;\mathbf{X}&lt;/math&gt;, by estimating the regression function &lt;math&gt;m(\mathbf{x})=\operatorname{E}[Y \mid \mathbf{X} = \mathbf{x}]&lt;/math&gt;. A random regression forest is an ensemble of &lt;math&gt;M&lt;/math&gt; randomized regression trees. Denote &lt;math&gt;m_n(\mathbf{x},\mathbf{\Theta}_j)&lt;/math&gt; the predicted value at point &lt;math&gt;\mathbf{x}&lt;/math&gt; by the &lt;math&gt;j&lt;/math&gt;-th tree, where &lt;math&gt;\mathbf{\Theta}_1,\ldots,\mathbf{\Theta}_M &lt;/math&gt; are independent random variables, distributed as a generic random variable &lt;math&gt;\mathbf{\Theta}&lt;/math&gt;, independent of the sample &lt;math&gt;\mathcal{D}_n&lt;/math&gt;. This random variable can be used to describe the randomness induced by node splitting and the sampling procedure for tree construction. The trees are combined to form the finite forest estimate &lt;math&gt;m_{M, n}(\mathbf{x},\Theta_1,\ldots,\Theta_M) = \frac{1}{M}\sum_{j=1}^M m_n(\mathbf{x},\Theta_j)&lt;/math&gt;.
For regression trees, we have &lt;math&gt;m_n = \sum_{i=1}^n\frac{Y_i\mathbf{1}_{\mathbf{X}_i\in A_n(\mathbf{x},\Theta_j)}}{N_n(\mathbf{x}, \Theta_j)}&lt;/math&gt;, where &lt;math&gt;A_n(\mathbf{x},\Theta_j)&lt;/math&gt; is the cell containing &lt;math&gt;\mathbf{x}&lt;/math&gt;, designed with randomness &lt;math&gt;\Theta_j&lt;/math&gt; and dataset &lt;math&gt;\mathcal{D}_n&lt;/math&gt;, and &lt;math&gt; N_n(\mathbf{x}, \Theta_j) = \sum_{i=1}^n \mathbf{1}_{\mathbf{X}_i\in A_n(\mathbf{x}, \Theta_j)}&lt;/math&gt;.

Thus random forest estimates satisfy, for all &lt;math&gt;\mathbf{x}\in[0,1]^d&lt;/math&gt;, &lt;math&gt; m_{M,n}(\mathbf{x}, \Theta_1,\ldots,\Theta_M) =\frac{1}{M}\sum_{j=1}^M \left(\sum_{i=1}^n\frac{Y_i\mathbf{1}_{\mathbf{X}_i\in A_n(\mathbf{x},\Theta_j)}}{N_n(\mathbf{x}, \Theta_j)}\right)&lt;/math&gt;. Random regression forest has two level of averaging, first over the samples in the target cell of a tree, then over all trees. Thus the contributions of observations that are in cells with a high density of data points are smaller than that of observations which belong to less populated cells. In order to improve the random forest methods and compensate the misestimation, Scornet&lt;ref name="scornet2015random"/&gt; defined KeRF by

: &lt;math&gt; \tilde{m}_{M,n}(\mathbf{x}, \Theta_1,\ldots,\Theta_M) = \frac{1}{\sum_{j=1}^M N_n(\mathbf{x}, \Theta_j)}\sum_{j=1}^M\sum_{i=1}^n Y_i\mathbf{1}_{\mathbf{X}_i\in A_n(\mathbf{x}, \Theta_j)},&lt;/math&gt;

which is equal to the mean of the &lt;math&gt;Y_i&lt;/math&gt;'s falling in the cells containing &lt;math&gt;\mathbf{x}&lt;/math&gt; in the forest. If we define the connection function of the &lt;math&gt;M&lt;/math&gt; finite forest as &lt;math&gt;K_{M,n}(\mathbf{x}, \mathbf{z}) = \frac{1}{M} \sum_{j=1}^M \mathbf{1}_{\mathbf{z} \in A_n (\mathbf{x}, \Theta_j)}&lt;/math&gt;, i.e. the proportion of cells shared between &lt;math&gt;\mathbf{x}&lt;/math&gt; and &lt;math&gt;\mathbf{z}&lt;/math&gt;, then almost surely we have &lt;math&gt;\tilde{m}_{M,n}(\mathbf{x}, \Theta_1,\ldots,\Theta_M) =
\frac{\sum_{i=1}^n Y_i K_{M,n}(\mathbf{x}, \mathbf{x}_i)}{\sum_{\ell=1}^n K_{M,n}(\mathbf{x}, \mathbf{x}_{\ell})}&lt;/math&gt;, which defines the KeRF.

==== Centered KeRF ====
The construction of Centered KeRF of level &lt;math&gt;k&lt;/math&gt; is the same as for centered forest, except that predictions are made by &lt;math&gt;\tilde{m}_{M,n}(\mathbf{x}, \Theta_1,\ldots,\Theta_M) &lt;/math&gt;, the corresponding kernel function, or connection function is

: &lt;math&gt;
\begin{align}
K_k^{cc}(\mathbf{x},\mathbf{z}) = \sum_{k_1,\ldots,k_d, \sum_{j=1}^d k_j=k} &amp;
\frac{k!}{k_1!\cdots k_d!} \left(\frac 1 d \right)^k
\prod_{j=1}^d\mathbf{1}_{\lceil2^{k_j}x_j\rceil=\lceil2^{k_j}z_j\rceil}, \\
&amp; \text{ for all } \mathbf{x},\mathbf{z}\in[0,1]^d.
\end{align}
&lt;/math&gt;

==== Uniform KeRF ====
Uniform KeRF is built in the same way as uniform forest, except that predictions are made by &lt;math&gt;\tilde{m}_{M,n}(\mathbf{x}, \Theta_1,\ldots,\Theta_M) &lt;/math&gt;, the corresponding kernel function, or connection function is
:&lt;math&gt;K_k^{uf}(\mathbf{0},\mathbf{x}) =
\sum_{k_1,\ldots,k_d, \sum_{j=1}^d k_j=k}
\frac{k!}{k_1!\ldots k_d!}\left(\frac{1}{d}\right)^k
\prod_{m=1}^d\left(1-|x_m|\sum_{j=0}^{k_m-1}\frac{(-\ln|x_m|)^j}{j!}\right) \text{ for all } \mathbf{x}\in[0,1]^d.&lt;/math&gt;

=== Properties ===
==== Relation between KeRF and random forest ====
Predictions given by KeRF and [[random forests]] are close if the number of points in each cell is controlled:

&lt;blockquote&gt;
Assume that there exist sequences &lt;math&gt; (a_n),(b_n) &lt;/math&gt; such that, almost surely,
: &lt;math&gt; a_n\leq N_n(\mathbf{x},\Theta)\leq b_n \text{ and } a_n\leq \frac 1 M \sum_{m=1}^M N_n {\mathbf{x},\Theta_m}\leq b_n.
&lt;/math&gt;
Then almost surely,
:&lt;math&gt;|m_{M,n}(\mathbf{x}) - \tilde{m}_{M,n}(\mathbf{x})| \le\frac{b_n-a_n}{a_n} \tilde{m}_{M,n}(\mathbf{x}).
&lt;/math&gt;
&lt;/blockquote&gt;

==== Relation between infinite KeRF and infinite random forest ====
When the number of trees &lt;math&gt;M&lt;/math&gt; goes to infinity, then we have infinite [[random forest]] and infinite KeRF. Their estimates are close if the number of observations in each cell is bounded:

&lt;blockquote&gt;
Assume that there exist sequences &lt;math&gt;(\varepsilon_n), (a_n),(b_n)&lt;/math&gt; such that, almost surely
* &lt;math&gt;\operatorname{E}[N_n(\mathbf{x},\Theta)] \ge 1,&lt;/math&gt;
* &lt;math&gt;\operatorname{P}[a_n\le N_n(\mathbf{x},\Theta) \le b_n\mid \mathcal{D}_n] \ge 1-\varepsilon_n/2,&lt;/math&gt;
* &lt;math&gt;\operatorname{P}[a_n\le \operatorname{E}_\Theta [N_n(\mathbf{x},\Theta)] \le b_n\mid \mathcal{D}_n] \ge 1-\varepsilon_n/2,&lt;/math&gt;
Then almost surely,
: &lt;math&gt; |m_{\infty,n}(\mathbf{x})-\tilde{m}_{\infty,n}(\mathbf{x})| \le
\frac{b_n-a_n}{a_n}\tilde{m}_{\infty,n}(\mathbf{x}) + n \varepsilon_n \left( \max_{1\le i\le n} Y_i \right).&lt;/math&gt;
&lt;/blockquote&gt;

=== Consistency results ===
Assume that &lt;math&gt;Y = m(\mathbf{X}) + \varepsilon&lt;/math&gt;, where &lt;math&gt;\varepsilon&lt;/math&gt; is a centered Gaussian noise, independent of &lt;math&gt;\mathbf{X}&lt;/math&gt;, with finite variance &lt;math&gt;\sigma^2&lt;\infty&lt;/math&gt;. Moreover, &lt;math&gt;\mathbf{X}&lt;/math&gt; is uniformly distributed on &lt;math&gt;[0,1]^d&lt;/math&gt; and &lt;math&gt;m&lt;/math&gt; is [[Lipschitz]]. Scornet&lt;ref name="scornet2015random"/&gt; proved upper bounds on the rates of consistency for centered KeRF and uniform KeRF.

==== Consistency of centered KeRF ====
Providing &lt;math&gt;k\rightarrow\infty&lt;/math&gt; and &lt;math&gt;n/2^k\rightarrow\infty&lt;/math&gt;, there exists a constant &lt;math&gt;C_1&gt;0&lt;/math&gt; such that, for all &lt;math&gt;n&lt;/math&gt;,
&lt;math&gt; \mathbb{E}[\tilde{m}_n^{cc}(\mathbf{X}) - m(\mathbf{X})]^2 \le C_1 n^{-1/(3+d\log 2)}(\log n)^2&lt;/math&gt;.

==== Consistency of uniform KeRF ====
Providing &lt;math&gt;k\rightarrow\infty&lt;/math&gt; and &lt;math&gt;n/2^k\rightarrow\infty&lt;/math&gt;, there exists a constant &lt;math&gt;C&gt;0&lt;/math&gt; such that,
&lt;math&gt;\mathbb{E}[\tilde{m}_n^{uf}(\mathbf{X})-m(\mathbf{X})]^2\le Cn^{-2/(6+3d\log2)}(\log n)^2&lt;/math&gt;.

== RF in scientific works ==
The algorithm is often used in scientific works because of its advantages. For example, it can be used for quality assessment of [[Wikipedia]] articles.&lt;ref&gt;{{Cite book |last1=Węcel |first1=Krzysztof |last2=Lewoniewski |first2=Włodzimierz | name-list-format = vanc |date=2015-12-02 |title=Modelling the Quality of Attributes in Wikipedia Infoboxes |journal= Lecture Notes in Business Information Processing |volume=228 |issue= |pages=308–320 |doi=10.1007/978-3-319-26762-3_27 |isbn=978-3-319-26761-6 }}&lt;/ref&gt;&lt;ref&gt;{{Cite book |last1=Lewoniewski |first1=Włodzimierz |last2=Węcel |first2=Krzysztof |last3=Abramowicz |first3=Witold | name-list-format = vanc |date=2016-09-22 |title=Quality and Importance of Wikipedia Articles in Different Languages |journal=Information and Software Technologies. ICIST 2016. Communications in Computer and Information Science |volume=639 |issue= |pages=613–624 |doi=10.1007/978-3-319-46254-7_50 |series=Communications in Computer and Information Science |isbn=978-3-319-46253-0 }}&lt;/ref&gt;&lt;ref&gt;{{Cite book |last1=Warncke-Wang |first1=Morten |last2=Cosley |first2=Dan | first3=John | last3=Riedl | name-list-format = vanc |date=2013  |title=Tell me more: An actionable quality model for wikipedia |url=https://dl.acm.org/citation.cfm?id=2491063 |journal=WikiSym '13 Proceedings of the 9th International Symposium on Open Collaboration |volume= |issue= |pages= 8:1–8:10|doi=10.1145/2491055.2491063 |access-date=2018-01-25 |isbn=9781450318525 |series=WikiSym '13 }}&lt;/ref&gt;

== Open source implementations ==
* [http://www.stat.berkeley.edu/~breiman/RandomForests/cc_software.htm The Original RF] by Breiman and Cutler written in Fortran 77.
* [http://www.alglib.net/dataanalysis/decisionforest.php ALGLIB] contains a modification of the random forest in C#, C++, Pascal, VBA.
* [http://cran.r-project.org/web/packages/party/index.html party] Implementation based on the conditional inference trees in [[R (programming language)|R]].
* [http://cran.r-project.org/web/packages/randomForest/index.html randomForest] for classification and regression in [[R (programming language)|R]].
* [http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html Python implementation] with examples in [[scikit-learn]].
* [[Orange (software)|Orange data mining]] suite includes random forest learner and can visualize the trained forest.
* [http://code.google.com/p/randomforest-matlab Matlab] implementation.
* [http://sqp.upf.edu SQP] software uses random forest algorithm to predict the quality of survey questions, depending on formal and linguistic characteristics of the question.
* [http://weka.sourceforge.net/doc.dev/weka/classifiers/trees/RandomForest.html Weka RandomForest] in Java library and GUI.
* [https://github.com/imbs-hl/ranger ranger] A C++ implementation of random forest for classification, regression, probability and survival. Includes interface for [[R (programming language)|R]].

== See also ==
*[[Decision tree learning]]
*[[Gradient boosting]]
*[[Randomized algorithm]]
*[[Ensemble learning]]
*[[Boosting (machine learning)|Boosting]]
*[[Non-parametric statistics]]
*[[Kernel random forest]]

== References ==
{{Reflist|32em}}

== Further reading ==
{{refbegin}}
* {{cite conference |doi = 10.1007/978-3-540-74469-6_35 |chapter = Random Multiclass Classification: Generalizing Random Forests to Random MNL and Random NB |chapter-url = https://www.researchgate.net/profile/Dirk_Van_den_Poel/publication/225175169_Random_Multiclass_Classification_Generalizing_Random_Forests_to_Random_MNL_and_Random_NB/links/02e7e5278a0a7b8e7f000000.pdf |title = Database and Expert Systems Applications |series = [[Lecture Notes in Computer Science]] |year = 2007 |last1 = Prinzie |first1 = Anita |last2 = Poel |first2 = Dirk | name-list-format = vanc |isbn = 978-3-540-74467-2 |volume = 4653 |pages = 349}}
* {{cite journal | vauthors = Denisko D, Hoffman MM | title = Classification and interaction in random forests | journal = Proceedings of the National Academy of Sciences of the United States of America | volume = 115 | issue = 8 | pages = 1690–1692 | date = February 2018 | pmid = 29440440 | doi = 10.1073/pnas.1800256115 | pmc=5828645}}
{{refend}}

== External links ==
* [https://www.stat.berkeley.edu/~breiman/RandomForests/cc_home.htm Random Forests classifier description] (Leo Breiman's site)
* [https://cran.r-project.org/doc/Rnews/Rnews_2002-3.pdf Liaw, Andy &amp; Wiener, Matthew "Classification and Regression by randomForest" R News (2002) Vol. 2/3 p. 18] (Discussion of the use of the random forest package for [[R programming language|R]])

[[Category:Classification algorithms]]
[[Category:Ensemble learning]]
[[Category:Decision trees]]
[[Category:Decision theory]]
[[Category:Computational statistics]]
[[Category:Machine learning]]</text>
      <sha1>352h6moh1axdzcfgrkgik40lcjehv3y</sha1>
    </revision>
  </page>
  <page>
    <title>Shortlex order</title>
    <ns>0</ns>
    <id>2660383</id>
    <revision>
      <id>665917248</id>
      <parentid>665917095</parentid>
      <timestamp>2015-06-07T17:34:12Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <minor/>
      <comment>RAIRO</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2841">In [[mathematics]], and particularly in the theory of [[formal language]]s, '''shortlex''' is a [[total ordering]] for [[finite sequence]]s of objects that can themselves be totally ordered. In the shortlex ordering, sequences are primarily sorted by [[cardinality]] (length) with the shortest sequences first, and sequences of the same length are sorted into [[lexicographical order]].&lt;ref&gt;{{cite book
| last = Sipser | first = Michael | author-link = Michael Sipser
| isbn = 978-1133187790
| mr = 
| page = 14
| publisher = Cengage Learning
| location = Boston, MA
| title = Introduction to the Theory of Computation
| year = 2012
| edition = 3}}&lt;/ref&gt; Shortlex ordering is also called '''radix''', '''length-lexicographic''', '''military''', or '''genealogical''' ordering.&lt;ref&gt;{{citation
 | last = Bárány | first = Vince
 | doi = 10.1051/ita:2008008
 | issue = 3
 | journal = RAIRO Theoretical Informatics and Applications
 | mr = 2434027
 | pages = 417–450
 | title = A hierarchy of automatic ω-words having a decidable MSO theory
 | volume = 42
 | year = 2008}}.&lt;/ref&gt;

In the context of [[Formal language|strings]] on a totally ordered alphabet, the '''shortlex order''' is identical to the lexicographical order, except that shorter strings precede longer strings. E.g., the shortlex order of the set of strings on the English alphabet (in its usual order) is ''[&amp;epsilon;, a, b, c, ..., z, aa, ab, ac, ..., zz, aaa, aab, aac, ..., zzz, ...]'', where ε denotes the empty string.

The strings in this ordering over a fixed alphabet can be placed into one-to-one correspondence with the non-negative [[integer]]s, giving the [[bijective numeration]] system for representing numbers.&lt;ref&gt;{{citation
 | last = Smullyan | first = R. | author-link = Raymond Smullyan
 | contribution = 9. Lexicographical ordering; ''n''-adic representation of integers
 | contribution-url = https://books.google.com/books?id=O4wZLfMnd74C&amp;pg=PA34
 | pages = 34–36
 | publisher = Princeton University Press
 | series = Annals of Mathematics Studies
 | title = Theory of Formal Systems
 | volume = 47
 | year = 1961}}.&lt;/ref&gt; The shortlex ordering is also important in the theory of [[automatic group]]s.&lt;ref&gt;{{citation
 | last1 = Epstein | first1 = David B. A. | author1-link = David B. A. Epstein
 | last2 = Cannon | first2 = James W. | author2-link = James W. Cannon
 | last3 = Holt | first3 = Derek F.
 | last4 = Levy | first4 = Silvio V. F.
 | last5 = Paterson | first5 = Michael S. | author5-link = Mike Paterson
 | last6 = Thurston | first6 = William P. | author6-link = William Thurston
 | isbn = 0-86720-244-0
 | location = Boston, MA
 | mr = 1161694
 | page = 56
 | publisher = Jones and Bartlett Publishers
 | title = Word processing in groups
 | year = 1992}}.&lt;/ref&gt;

==References==
{{reflist}}

[[Category:Order theory]]


{{algebra-stub}}</text>
      <sha1>ofjdugtuuksqkfuyy7cyw0j2rss1vm0</sha1>
    </revision>
  </page>
  <page>
    <title>Side-approximation theorem</title>
    <ns>0</ns>
    <id>25824263</id>
    <revision>
      <id>841148627</id>
      <parentid>841148527</parentid>
      <timestamp>2018-05-14T08:51:48Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <comment>/* References */ 10.2307/1970057</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="818">In geometric topology, the '''side-approximation theorem''' was proved by {{harvtxt|Bing|1963}}. It implies that a 2-sphere in '''R'''&lt;sup&gt;3&lt;/sup&gt; can be approximated by polyhedral 2-spheres.

==References==
*{{Citation | last1=Bing | first1=R. H. | author-link=R. H. Bing | title=Approximating surfaces with polyhedral ones | jstor=1970057 | mr=0087090  | year=1957 | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=65 | pages=465–483 | doi=10.2307/1970057}}
*{{Citation | last1=Bing | first1=R. H. | title=Approximating surfaces from the side | jstor=1970203 | mr=0150744  | year=1963 | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=77 | pages=145–192 | doi=10.2307/1970203}}

[[Category:Geometric topology]]
[[Category:Theorems in topology]]</text>
      <sha1>g5a6ufj3a5y2akc68xil61dhnsk6hub</sha1>
    </revision>
  </page>
  <page>
    <title>Sign extension</title>
    <ns>0</ns>
    <id>3349272</id>
    <revision>
      <id>860192703</id>
      <parentid>860192625</parentid>
      <timestamp>2018-09-18T23:38:47Z</timestamp>
      <contributor>
        <username>Jeh</username>
        <id>341372</id>
      </contributor>
      <comment>/* Zero extension */ add instruction interpretation</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3093">'''Sign extension''' is the operation, in [[computer]] [[arithmetic]], of increasing the number of [[bit]]s of a [[binary number]] while preserving the number's [[sign (mathematics)|sign (positive/negative)]] and value. This is done by appending digits to the [[most significant bit|most significant]] side of the number, following a procedure dependent on the particular [[signed number representation]] used.

For example, if six bits are used to represent the number "&lt;code&gt;00 1010&lt;/code&gt;" (decimal positive 10) and the sign extend operation increases the [[Word (data type)|word length]] to 16 bits, then the new representation is simply "&lt;code&gt;0000 0000 0000 1010&lt;/code&gt;".  Thus, both the value and the fact that the value was positive are maintained.

If ten bits are used to represent the value "&lt;code&gt;11 1111 0001&lt;/code&gt;" (decimal negative 15) using [[two's complement]], and this is sign extended to 16 bits, the new representation is "&lt;code&gt;1111 1111 1111 0001&lt;/code&gt;".  Thus, by padding the left side with ones, the negative sign and the value of the original number are maintained.

In the [[Intel]] [[x86 instruction listings|x86 instruction set]], for example, there are two ways of doing sign extension:
* using the instructions &lt;tt&gt;cbw&lt;/tt&gt;, &lt;tt&gt;cwd&lt;/tt&gt;, &lt;tt&gt;cwde&lt;/tt&gt;, and &lt;tt&gt;cdq&lt;/tt&gt;: convert byte to word, word to doubleword, word to extended doubleword, and doubleword to quadword, respectively (in the x86 context a byte has 8 bits, a word 16 bits, a doubleword and extended doubleword 32 bits, and a quadword 64 bits);
* using one of the sign extended moves, accomplished by the &lt;tt&gt;movsx&lt;/tt&gt; ("move with sign extension") family of instructions.

==Zero extension==
A similar concept is zero extension. In a move or convert operation, zero extension refers to setting the high bits of the destination to zero, rather than setting them to a copy of the most significant bit of the source. If the source of the operation is an unsigned number, then zero extension is usually the correct way to move it to a larger field while preserving its numeric value, while sign extension is correct for signed numbers. 

In the x86 and x64 instruction sets, the &lt;code&gt;movzx&lt;/code&gt; instruction ("move with zero extension") performs this function. For example, &lt;code&gt;movzx ebx, al&lt;/code&gt; copies a byte from the &lt;code&gt;al&lt;/code&gt; register to the low-order byte of &lt;code&gt;ebx&lt;/code&gt; and then fills the remaining bytes of &lt;code&gt;ebx&lt;/code&gt; with zeroes.   

On x64, most instructions that write to the lower 32 bits of any of the general-purpose registers will zero the upper half of the destination register. For example, the instruction &lt;code&gt;mov eax, 1234&lt;/code&gt; will clear the upper 32 bits of the &lt;code&gt;rax&lt;/code&gt; register.

==References==
* Mano, Morris M.; Kime, Charles R. (2004). ''Logic and Computer Design Fundamentals'' (3rd ed.), pp 453. Pearson Prentice Hall. {{ISBN|0-13-140539-X}}.

[[Category:Binary arithmetic]]

[[de:Zweierkomplement#Vorzeichenerweiterung]]
[[ru:Дополнительный_код_(представление_числа)#Расширение_знака]]</text>
      <sha1>o77vbcuu5j72an2t25k5kt27y47h3pq</sha1>
    </revision>
  </page>
  <page>
    <title>Solid partition</title>
    <ns>0</ns>
    <id>41305278</id>
    <revision>
      <id>832876096</id>
      <parentid>822478579</parentid>
      <timestamp>2018-03-28T13:26:22Z</timestamp>
      <contributor>
        <username>DePiep</username>
        <id>199625</id>
      </contributor>
      <minor/>
      <comment>use {{OEIS}} templates where possible (via [[WP:JWB]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7569">In mathematics, '''solid partitions''' are natural generalizations of [[Partition (number theory)|partitions]] and [[plane partition]]s defined by [[Percy Alexander MacMahon]].&lt;ref&gt;P. A. MacMahon, Combinatory Analysis. Cambridge Univ. Press, London and New York, Vol. 1, 1915 and Vol. 2, 1916; see vol. 2, p 332.&lt;/ref&gt; A solid partition of &lt;math&gt; n &lt;/math&gt; is a three-dimensional array, &lt;math&gt; n_{i,j,k}&lt;/math&gt;, of non-negative integers (the indices &lt;math&gt; i,\ j,\  k\geq 1&lt;/math&gt;) such that 
:&lt;math&gt; \sum_{i,j,k} n_{i,j,k}=n&lt;/math&gt; 
and
:&lt;math&gt;
n_{i+1,j,k} \leq n_{i,j,k}\quad,\quad
n_{i,j+1,k} \leq n_{i,j,k}\quad\text{and}\quad
n_{i,j,k+1} \leq n_{i,j,k}\quad,\quad \forall\ i,\ j \text{ and } k\ .
&lt;/math&gt;
Let &lt;math&gt;p_3(n)&lt;/math&gt; denote the number of solid partitions of &lt;math&gt;n&lt;/math&gt;. As the definition of solid partitions involves three-dimensional arrays of numbers, they are also called '''three-dimensional partitions''' in notation where plane partitions are two-dimensional partitions and partitions are one-dimensional partitions. Solid partitions and their higher-dimensional generalizations are discussed in the book by [[George Andrews (mathematician)|Andrews]].&lt;ref&gt;G. E. Andrews, ''The theory of partitions'', Cambridge University Press, 1998.&lt;/ref&gt;

== Ferrers diagrams for solid partitions ==

Another representation for solid partitions is in the form of [[Norman Macleod Ferrers|Ferrers]] diagrams. The Ferrers diagram of a solid partition of &lt;math&gt;n&lt;/math&gt; is a collection of &lt;math&gt;n&lt;/math&gt; points or ''nodes'', &lt;math&gt;\lambda=(\mathbf{y}_1,\mathbf{y}_2,\ldots,\mathbf{y}_n)&lt;/math&gt;, with &lt;math&gt;\mathbf{y}_i\in \mathbb{Z}_{\geq0}^4&lt;/math&gt; satisfying the condition:&lt;ref name="Atkin1967"&gt;A. O. L. Atkin, P. Bratley, I. G. McDonald and J. K. S. McKay, Some computations for'' ''m-dimensional partitions, Proc. Camb. Phil. Soc., 63 (1967), 1097–1100.&lt;/ref&gt;

:'''Condition FD:''' If  the node &lt;math&gt;\mathbf{a}=(a_1,a_2,a_3, a_4)\in \lambda&lt;/math&gt;, then so do all the nodes &lt;math&gt;\mathbf{y}=(y_1,y_2,y_3,y_4)&lt;/math&gt; with &lt;math&gt;0\leq y_i\leq a_i&lt;/math&gt; for all &lt;math&gt;i=1,2,3,4&lt;/math&gt;.

For instance, the Ferrers diagram
:&lt;math&gt;
\left( \begin{smallmatrix} 0\\ 0\\  0 \\ 0 \end{smallmatrix}
\begin{smallmatrix} 0\\ 0\\  1 \\ 0  \end{smallmatrix}
\begin{smallmatrix} 0\\ 1\\ 0  \\ 0 \end{smallmatrix}
\begin{smallmatrix}1 \\ 0 \\ 0  \\ 0 \end{smallmatrix}
 \begin{smallmatrix} 1 \\ 1\\  0 \\ 0 \end{smallmatrix}
\right) \ ,
&lt;/math&gt;
where each column is a node, represents  a solid partition of &lt;math&gt;5&lt;/math&gt;. There is a natural action of the permutation group &lt;math&gt;S_4&lt;/math&gt; on a Ferrers diagram – this corresponds to permuting the four coordinates of all nodes. This generalises the operation denoted by  conjugation on usual partitions.

=== Equivalence of the two representations ===

Given a Ferrers diagram, one constructs the solid partition (as in the main definition) as follows.
:Let &lt;math&gt;n_{i,j,k}&lt;/math&gt; be the number of nodes in the Ferrers diagram with coordinates of the form &lt;math&gt;(i-1,j-1,k-1,*)&lt;/math&gt; where &lt;math&gt;*&lt;/math&gt; denotes an arbitrary value. The collection &lt;math&gt;n_{i,j,k}&lt;/math&gt; form a solid partition. One can verify that condition FD implies that the conditions for a solid partition are satisfied.

Given a set of &lt;math&gt;n_{i,j,k}&lt;/math&gt; that form a solid partition, one obtains the corresponding Ferrers diagram as follows.
:Start with the Ferrers diagram with no nodes. For every non-zero &lt;math&gt;n_{i,j,k}&lt;/math&gt;, add &lt;math&gt;n_{i,j,k}&lt;/math&gt; nodes &lt;math&gt;(i-1,j-1,k-1,y_4)&lt;/math&gt; for &lt;math&gt;0\leq y_4&lt; n_{i,j,k}&lt;/math&gt; to the Ferrers diagram. By construction, it is easy to see that condition FD is satisfied.

For example, the Ferrers diagram with &lt;math&gt;5&lt;/math&gt; nodes  given above corresponds to the solid partition with 
:&lt;math&gt;n_{1,1,1}=n_{2,1,1}=n_{1,2,1}=n_{1,1,2}=n_{2,2,1}=1&lt;/math&gt; 
with all other &lt;math&gt;n_{i,j,k}&lt;/math&gt; vanishing.

== Generating function ==

Let &lt;math&gt;p_3(0)\equiv 1&lt;/math&gt;. Define the generating function of solid partitions, &lt;math&gt;P_3(q)&lt;/math&gt;, by
:&lt;math&gt;
P_3(q) :=\sum_{n=0}^\infty p_3(n)\ q^n = 1+q+4\ q^2+10\ q^3+26\ q^4+59\ q^5+140\ q^6+\cdots 
&lt;/math&gt;
The generating functions of [[Partition (number theory)|partitions]] and [[plane partition]]s have simple formulae due to [[Leonhard Euler|Euler]] and [[Percy Alexander MacMahon|MacMahon]] respectively. However, a guess of [[Percy Alexander MacMahon|MacMahon]] fails to correctly reproduce the solid partitions of 6 as shown by Atkin et al.&lt;ref name="Atkin1967" /&gt; It appears that there is no simple formula for the generating function of solid partitions. Somewhat confusingly, Atkin et al. refer to solid partitions as four-dimensional partitions as that is the dimension of the Ferrers diagram.&lt;ref name="Atkin1967" /&gt;

== Exact enumeration using computers ==

Given the lack of an explicitly known generating function, the enumerations of the numbers of solid partitions for larger integers have been carried out numerically. There are two algorithms that are used to enumerate solid partitions and their higher-dimensional generalizations. The work of Atkin. et al. used an algorithm due to Bratley and McKay.&lt;ref&gt;P. Bratley and J. K. S. McKay, "Algorithm 313: Multi-dimensional partition generator", Comm. ACM, 10 (Issue 10, 1967), p. 666.&lt;/ref&gt; In 1970, Knuth proposed a different algorithm to enumerate topological sequences that he used to evaluate numbers of solid partitions of all integers &lt;math&gt;n\leq 28&lt;/math&gt;.&lt;ref&gt;D. E. Knuth, "A note on solid partitions", Math. Comp., 24 (1970), 955–961.&lt;/ref&gt; Mustonen and Rajesh extended the enumeration for all integers &lt;math&gt;n\leq 50&lt;/math&gt;.&lt;ref name="Mustonen"&gt;Ville Mustonen and R. Rajesh, "Numerical Estimation of the Asymptotic Behaviour of Solid Partitions of an Integer", J. Phys. A: Math. Gen. '''36''' (2003), no. 24, 6651.[https://arxiv.org/abs/cond-mat/0303607 cond-mat/0303607]&lt;/ref&gt; In 2010, S. Balakrishnan proposed a parallel version of Knuth's algorithm that has been used to extend the enumeration to all integers &lt;math&gt;n\leq 72&lt;/math&gt;.&lt;ref&gt;Srivatsan Balakrishnan, Suresh Govindarajan and Naveen S. Prabhakar, "On the asymptotics of higher-dimensional partitions", J.Phys. A: Math. Gen. '''45''' (2012) 055001 [https://arxiv.org/abs/1105.6231 arXiv:1105.6231].&lt;/ref&gt; One finds 
:&lt;math&gt; p_3(72)=3464 27497 40651 72792\ ,&lt;/math&gt;
which is a 19 digit number illustrating the difficulty in carrying out such exact enumerations.

== Asymptotic behavior ==

It is known that from the work of Bhatia et al. that&lt;ref&gt;D P Bhatia, M A Prasad and D Arora, "Asymptotic results for the number of multidimensional partitions of an integer and directed compact lattice animals", J. Phys. A: Math. Gen. '''30''' (1997) 2281&lt;/ref&gt;
:&lt;math&gt;
\lim_{n\rightarrow\infty} n^{-3/4}
 \ln p_3(n) \rightarrow \text{a constant}.
&lt;/math&gt;
The value of this constant was estimated using Monte-Carlo simulations by Mustonen and Rajesh to be &lt;math&gt;1.79\pm 0.01&lt;/math&gt;.&lt;ref name="Mustonen" /&gt;

== References ==

&lt;!--- See [[Wikipedia:Footnotes]] on how to create references using&lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist}}

== External links ==
* {{OEIS el|1=A000293|2=Solid (i.e., three-dimensional) partitions|formalname=a(n) = number of solid (i.e., three-dimensional) partitions of n}}
* [http://boltzmann.wikidot.com/solid-partitions The Solid Partitions Project of IIT Madras]
* [http://mathworld.wolfram.com/SolidPartition.html The Mathworld entry for Solid Partitions]

[[Category:Enumerative combinatorics]]
[[Category:Number theory]]
[[Category:Combinatorics]]</text>
      <sha1>dyj4ec9dprcfwgb96lwlezo0782w7eb</sha1>
    </revision>
  </page>
  <page>
    <title>Solvency ratio</title>
    <ns>0</ns>
    <id>21669817</id>
    <revision>
      <id>812590432</id>
      <parentid>812589488</parentid>
      <timestamp>2017-11-28T18:42:52Z</timestamp>
      <contributor>
        <ip>199.166.227.141</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1942">{{local}}
A '''solvency ratio''' measures the extent to which assets cover commitments for future payments, the liabilities. 

The '''solvency ratio''' of an [[insurance]] company is the size of its [[Capital requirement|capital]] relative to all risks it has taken. The solvency ratio is most often defined as: 

:&lt;math&gt;net.assets \div net.premium.written &lt;/math&gt;

The solvency ratio is a measure of the [[risk]] an insurer faces of claims that it cannot absorb. The amount of premium written is a better measure than the total amount insured because the level of premiums is linked to the likelihood of claims. 

Different countries use different methodologies to calculate the solvency ratio, and have different requirements.  For example, in [[India]] insurers are required to maintain a minimum ratio of 1.5.&lt;ref&gt;http://www.irdaindia.org/Finance_And_Analysis/Annexure221107.xls&lt;/ref&gt;

For pension plans, the '''solvency ratio''' is the ratio of pension plan assets to liabilities (the pensions to be paid). Another measure of the pension plan's ability to pay all pensions in perpetuity is the going concern ratio, which measures the cost of pensions if the pension plan continues to operate. For the solvency ratio, the pension liabilities are measured using stringent rules including the assumption that the plan will be close immediately so must purchase of annuities to transfer responsibility of the pensions to another party. This is more expensive so the solvency ratio is usually lower than the going concern ratio, which measures the pension plan's ability to pay pensions if it continues to operate.

In finance, the solvency ratio measures a company's cash flow compared to its liabilities:

Solvency ratio = (net income + depreciation) / liabilities

==See also ==
* [[Current Ratio]]
* [[Solvency]]
* [[Solvency II Directive 2009]] the EU requirement on the ratio
== References ==
{{reflist}}

[[Category:Actuarial science]]</text>
      <sha1>m84kt84ue03lu5hyix9ivda950qdbtf</sha1>
    </revision>
  </page>
  <page>
    <title>Tomaž Pisanski</title>
    <ns>0</ns>
    <id>84539</id>
    <revision>
      <id>838028534</id>
      <parentid>832931328</parentid>
      <timestamp>2018-04-24T14:01:56Z</timestamp>
      <contributor>
        <username>Tom.Reding</username>
        <id>9784415</id>
      </contributor>
      <minor/>
      <comment>+{{[[Template:Authority control|Authority control]]}} (5 sources from Wikidata), [[WP:GenFixes]] on, using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5993">{{Infobox scientist
| name              = Tomaž Pisanski
| image             = 
| image_size        = 
| alt               = 
| caption           = 
| birth_date        = {{Birth date and age|1949|05|24}}
| birth_place       = [[Ljubljana]], [[Slovenia]]
| residence         = 
| nationality       = [[Slovenes|Slovene]]
| fields            = [[Topological graph theory]], [[Algebraic graph theory]], [[Discrete mathematics]], [[Configuration (geometry)]].
| workplaces        = [[University of Primorska]]
| alma_mater        = [[University of Ljubljana]], [[Pennsylvania State University]]
| doctoral_advisor  = [[Torrence Parsons]]
| doctoral_students = 
| notable_students  = 
| website           = 
}}
'''Tomaž (Tomo) Pisanski''' (born May 24, 1949 in [[Ljubljana, Slovenia]]) is a [[Slovenia]]n [[mathematician]] working mainly in [[discrete mathematics]] and [[graph theory]].
In 1980 he calculated the [[genus]] of the [[Cartesian product of graphs|Cartesian product]] of any pair of connected, bipartite, ''d''-valent graphs using a method that was later called the ''White–Pisanski method''.&lt;ref&gt;J.L. Gross and [[Thomas W. Tucker|T.W. Tucker]], Topological graph theory, Wiley Interscience, 1987&lt;/ref&gt;
In 1982 [[Vladimir Batagelj]] and Pisanski proved that the [[Cartesian product of graphs|Cartesian product]] of a [[tree (mathematics)|tree]] and a [[cycle graph|cycle]] is Hamiltonian if and only if no degree of the tree exceeds the length of the cycle. They also proposed a conjecture concerning ''cyclic Hamiltonicity'' of graphs. Their conjecture was proved in 2005.&lt;ref name="DPP2005"&gt;
{{cite journal
|title=On the Hamiltonicity of the Cartesian product
|journal=Inf. Proc. Lett
|doi=10.1016/j.ipl.2005.05.016
|first1=Vassilios V.
|last1=Dimakopoulos
|first2=Leonidas
|last2=Palios
|first3=Athanasios S.
|last3=Poulakidas
|year=2005
|volume=96
|issue=2
|pages=49&amp;ndash;53
}}&lt;/ref&gt;

==Biography==
Pisanski studied at the [[University of Ljubljana]] where he obtained a B.Sc., M.Sc and Ph.D. in mathematics. He competed in the 1966 and 1967 [[International Mathematical Olympiad]]s, winning a bronze medal in 1967.&lt;ref&gt;[http://www.imocompendium.com/index.php?options=gl%7Cimotres&amp;imetakm=Pisanski,%20Tomo&amp;p=3999c32]&lt;/ref&gt; His Ph.D. thesis from [[topological graph theory]] was written under the guidance of [[Torrence Parsons]]. During his undergraduate studies he was an exchange student at the [[University of Nancy]], [[France]]. He also obtained a M.Sc. in [[computer science]] from the [[Pennsylvania State University]].

Currently, Pisanski is a professor of [[discrete mathematics|discrete]] and [[computation|computational mathematics]] at Head of the Department of Information Sciences and Technology at [[University of Primorska]] in [[Koper]]. He has taught undergraduate and graduate courses in mathematics and computer science at the [[University of Ljubljana]], [[University of Zagreb]], [[University of Udine]], [[University of Leoben]], [[California State University, Chico]], [[Simon Fraser University]], [[University of Auckland]] and [[Colgate University]].  His students include [[John Shawe-Taylor]] (BSc in Ljubljana), [[Vladimir Batagelj]], [[Bojan Mohar]], [[Sandi Klavžar]], [[Fisher–Yates shuffle|Sandra Sattolo]] (MSc in Udine).

From 1998-1999 he was chairman of the [[Society of Mathematicians, Physicists and Astronomers of Slovenia]].  In 2005 he was [[civil decoration|decorated]] with the '''Order of Merit (Slovenia)'''
&lt;ref name=order&gt;{{cite web
|url=http://www.up-rs.si/up-rs/uprs.nsf/objave/Vrste-odlikovanj?OpenDocument
|title= Order of Merit (Red za zasluge)
}}&lt;/ref&gt;
. He is a founding member of the [[International Academy of Mathematical Chemistry]] and from 2007-2011 its Vice President. He is a founding editor of the journal ''[[Ars Mathematica Contemporanea]]''. In 2012 he was elected to the [[Academia Europaea]].&lt;ref&gt;[http://www.ae-info.org/ae/User/Pisanski_Tomaž at Academia Europaea], retrieved 2012-10-09.&lt;/ref&gt;

With [[Brigitte Servatius]] he is the co-author of a book on [[Configuration (geometry)|configurations]].&lt;ref name="PS2013"&gt;
{{citation
 | last1 = Pisanski | first1 = Tomaž 
 | last2 = Servatius | first2 = Brigitte
 | isbn = 9780817683641
 | publisher = Springer
 | title = Configurations from a Graphical Viewpoint
 | url = https://books.google.com/books?id=bnh2zkuTZr4C
 | year = 2013}}.&lt;/ref&gt;

==Awards and honors==

In 2015 Pisanski received the Zois award for exceptional contributions to discrete mathematics and its applications.

==References==
 &lt;references /&gt;

==External links==
*[https://web.archive.org/web/20041130062818/http://www.ijp.si/ Pisanski's CV]
* {{cite web
|title=prof. dr. Tomaz Pisanski
|url=https://www.famnit.upr.si//en/about-faculty/staff/pisanski/
}}
*{{MathGenealogy|id=20387}}
*[http://www.iamc-online.org/members/index.htm International Academy of Mathematical Chemistry - List of Members]
*[http://en.ias.si/membership/regular-and-associate-members/ Slovenian Academy of Engineering - List of Members]

{{Authority control}}

{{DEFAULTSORT:Pisanski, Tomaz}}
[[Category:1949 births]]
[[Category:20th-century mathematicians]]
[[Category:21st-century mathematicians]]
[[Category:Graph theorists]]
[[Category:Living people]]
[[Category:Pennsylvania State University alumni]]
[[Category:Slovenian mathematicians]]
[[Category:Slovenian computer scientists]]
[[Category:People from Ljubljana]]
[[Category:Mathematical chemistry]]
[[Category:University of Ljubljana alumni]]
[[Category:Members of Academia Europaea]]
[[Category:University of Ljubljana faculty]]
[[Category:University of Primorska faculty]]
[[Category:University of Zagreb faculty]]
[[Category:Montanuniversität Leoben faculty]]
[[Category:California State University, Chico faculty]]
[[Category:Simon Fraser University faculty]]
[[Category:University of Auckland faculty]]
[[Category:Colgate University faculty]]
[[Category:International Mathematical Olympiad participants]]
[[Category:Computational chemists]]</text>
      <sha1>8es3hohmvfx5qbkhi5949his3vypu0w</sha1>
    </revision>
  </page>
  <page>
    <title>Tucker's lemma</title>
    <ns>0</ns>
    <id>17633708</id>
    <revision>
      <id>847551955</id>
      <parentid>811886833</parentid>
      <timestamp>2018-06-26T04:50:37Z</timestamp>
      <contributor>
        <username>Turgidson</username>
        <id>1747755</id>
      </contributor>
      <comment>/* Proofs */ tex</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5451">[[File:TuckerLemExample.png|thumb|350px|In this example, where n=2, the red 1-simplex has vertices which are labelled by the same number with opposite signs. Tucker's lemma states that for such a triangulation at least one such 1-simplex must exist.]]

In [[mathematics]], '''Tucker's lemma''' is a [[combinatorics|combinatorial]] analog of the [[Borsuk&amp;ndash;Ulam theorem]], named after [[Albert W. Tucker]].

Let T be a [[Triangulation (topology)|triangulation]] of the closed ''n''-dimensional [[Ball (mathematics)|ball]] &lt;math&gt;B_n&lt;/math&gt;. Assume T is antipodally symmetric on the boundary [[sphere]] &lt;math&gt;S_{n-1}&lt;/math&gt;. That means that the subset of [[Simplex|simplices]] of T which are in &lt;math&gt;S_{n-1}&lt;/math&gt; provides a triangulation of &lt;math&gt;S_{n-1}&lt;/math&gt; where if σ is a simplex then so is −σ.
Let &lt;math&gt;L:V(T)\to\{+1,-1,+2,-2,...,+n,-n\}&lt;/math&gt; be a labeling of the vertices of T which is an [[odd function]] on &lt;math&gt;S_{n-1}&lt;/math&gt;, i.e, &lt;math&gt;L(-v) = -L(v)&lt;/math&gt; for every vertex &lt;math&gt;v\in S_{n-1}&lt;/math&gt;.
Then Tucker's lemma states that T contains a ''complementary edge'' - an edge (a 1-simplex) whose vertices are labelled by the same number but with opposite signs.&lt;ref&gt;{{citation
 | last = Matoušek | first = Jiří | author-link = Jiří Matoušek (mathematician)
 | isbn = 3-540-00362-2
 | pages = 34–45
 | publisher = [[Springer-Verlag]]
 | title = Using the Borsuk–Ulam Theorem
 | year = 2003}}&lt;/ref&gt;

== Proofs ==
The first proofs were non-constructive, by way of contradiction.&lt;ref&gt;{{citation | last = Tucker | first = Albert W. | author-link = Albert W. Tucker | contribution = Some topological properties of disk and sphere | location = Toronto | mr = 0020254 | pages = 285–309 | publisher = [[University of Toronto Press]] | title = Proc. First Canadian Math. Congress, Montreal, 1945 | year = 1946}}&lt;/ref&gt;

Later, constructive proofs were found, which also supplied algorithms for finding the complementary edge.&lt;ref&gt;{{citation | last1 = Freund | first1 = Robert M. | last2 = Todd | first2 = Michael J. | doi = 10.1016/0097-3165(81)90027-3 | issue = 3 | journal = [[Journal of Combinatorial Theory]] | mr = 618536 | pages = 321–325 | series = Series A | title = A constructive proof of Tucker's combinatorial lemma | volume = 30 | year = 1981}}&lt;/ref&gt;&lt;ref&gt;{{citation | last1 = Freund | first1 = Robert M. | last2 = Todd | first2 = Michael J. | title = A constructive proof of Tucker's combinatorial lemma | year = 1980|url=http://oai.dtic.mil/oai/oai?verb=getRecord&amp;metadataPrefix=html&amp;identifier=ADA090797}}&lt;/ref&gt; Basically, the algorithms are path-based: they start at a certain point or edge of the triangulation, then go from simplex to simplex according to prescribed rules, until it is not possible to proceed any more. It can be proved that the path must end in a simplex which contains a complementary edge.

An easier proof of Tucker's lemma uses the more general [[Ky Fan lemma]], which has a simple algorithmic proof.

The following description illustrates the algorithm for &lt;math&gt;n=2&lt;/math&gt;.&lt;ref name="Meunier10"&gt;{{citation | url=http://pretty.structures.free.fr/talks/Meunier.pdf | title=Sperner and Tucker lemmas | publisher=Algorithms and Pretty Theorems Blog | date=2010 | accessdate=25 May 2015 | author=Meunier, Frédéric | pages=46–64}}&lt;/ref&gt; Note that in this case &lt;math&gt;B_n&lt;/math&gt; is a disc and there are 4 possible labels: &lt;math&gt;-2, -1, 1, 2&lt;/math&gt;, like the figure at the top-right.

Start outside the ball and consider the labels of the boundary vertices. Because the labeling is an odd function on the boundary, the boundary must have both positive and negative labels:
* If the boundary contains only &lt;math&gt;\pm 1&lt;/math&gt; or only &lt;math&gt;\pm 2&lt;/math&gt;, there must be a complementary edge on the boundary. Done.
* Otherwise, the boundary must contain &lt;math&gt;(+1,-2)&lt;/math&gt; edges. Moreover, the number of &lt;math&gt;(+1,-2)&lt;/math&gt; edges on the boundary must be odd.

Select an &lt;math&gt;(+1,-2)&lt;/math&gt; edge and go through it. There are three cases:
* You are now in a &lt;math&gt;(+1,-2,+2)&lt;/math&gt; simplex. Done.
* You are now in a &lt;math&gt;(+1,-2,-1)&lt;/math&gt; simplex. Done.
* You are in a simplex with another &lt;math&gt;(+1,-2)&lt;/math&gt; edge. Go through it and continue.

The last case can take you outside the ball. However, since the number of &lt;math&gt;(+1,-2)&lt;/math&gt; edges on the boundary must be odd, there must be a new, unvisited &lt;math&gt;(+1,-2)&lt;/math&gt; edge on the boundary. Go through it and continue.

This walk must end inside the ball, either in a &lt;math&gt;(+1,-2,+2)&lt;/math&gt; or in a &lt;math&gt;(+1,-2,-1)&lt;/math&gt; simplex. Done.

== Run-time ==
The run-time of the algorithm described above is polynomial in the triangulation size. This is considered bad, since the triangulations might be very large. It would be desirable to find an algorithm which is logarithmic in the triangulation size. However, the problem of finding a complementary edge is [[PPA (complexity)|PPA]]-complete even for &lt;math&gt;n=2&lt;/math&gt; dimensions. This implies that there is not too much hope for finding a fast algorithm.&lt;ref&gt;{{citation
 | last1 = Aisenberg | first1 = James
 | last2 = Bonet | first2 = Maria Luisa
 | last3 = Buss | first3 = Sam
 | id = {{ECCC|2015|15|163}}
 | title = 2-D Tucker is PPA complete
 | year = 2015}}&lt;/ref&gt;

== Equivalent results ==
{{Analogous fixed-point theorems}}

== See also ==
* [[Topological combinatorics]]

==References==
{{reflist}}

[[Category:Combinatorics]]
[[Category:Topology]]
[[Category:Lemmas]]</text>
      <sha1>20n2hw83bfa5qel3org9a8lbsjw2xnb</sha1>
    </revision>
  </page>
  <page>
    <title>Twisted Poincaré duality</title>
    <ns>0</ns>
    <id>32182870</id>
    <revision>
      <id>641444788</id>
      <parentid>641444429</parentid>
      <timestamp>2015-01-07T17:01:56Z</timestamp>
      <contributor>
        <ip>2600:1006:B121:B69B:9ABE:BCC1:DEB0:76BC</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2414">{{Technical|date=January 2015}}
In mathematics, the '''twisted Poincaré duality''' is a theorem removing the restriction on [[Poincaré duality]] to [[oriented manifold]]s. The existence of a global orientation is replaced by carrying along local information, by means of a [[local coefficient system]].

==Twisted Poincaré duality for de Rham cohomology==
Another version of the theorem with real coefficients features the [[de Rham cohomology]] with values in the '''orientation bundle'''. This is the '''[[flat vector bundle|flat]]''' real [[line bundle]] denoted &lt;math&gt; o(M)&lt;/math&gt;, that is trivialized by coordinate charts of the manifold ''NM'', with transition maps the sign of the [[Jacobian determinant]] of the charts transition maps.  As a [[flat vector bundle|flat line bundle]], it has a de Rham cohomology, denoted by
:&lt;math&gt;H^* (M; \mathbb R^w)&lt;/math&gt; or &lt;math&gt;H^* (M; o(M))&lt;/math&gt;.

For ''M'' a ''compact''  manifold, the top degree cohomology is equipped with a so-called '''trace morphism'''
:&lt;math&gt;\theta: H^d (M; o(M)) \to \mathbb R&lt;/math&gt;,
that is to be interpreted as integration on ''M'', ''ie.'' evaluating against the fundamental class.

The Poincaré duality for differential forms is then the conjunction, for ''M'' connected, of the following two statements:
* The trace morphism is a linear isomorphism,
* The cup product, or [[exterior product]] of differential forms
:&lt;math&gt;\cup : H^* (M; \mathbb R)\otimes H^{d-*}(M, o(M)) \to H^d(M, o(M)) \simeq \mathbb R&lt;/math&gt;
is non-degenerate.

The oriented [[Poincaré duality]] is contained in this statement, as understood from the fact that the orientation bundle ''o(M)'' is trivial if the manifold is oriented, an orientation being a global trivialization, ''ie.'' a nowhere vanishing parallel section.

==See also==
*[[Local system]]
*[[Dualizing sheaf]]
*[[Verdier duality]]

==References==
*Some references are provided in [http://mathoverflow.net/questions/61194/non-oriented-version-of-poincare-duality the answers to this thread] on [[MathOverflow]]
*The online book [http://www.maths.ed.ac.uk/~aar/books/surgery.pdf ''Algebraic and geometric surgery''] by Andrew Ranicki
*[[Raoul Bott|R. Bott]]-L. Tu. ''Differential forms in algebraic topology'', a classic reference

{{DEFAULTSORT:Twisted Poincare duality}}
[[Category:Algebraic topology]]
[[Category:Manifolds]]
[[Category:Duality theories]]
[[Category:Theorems in topology]]</text>
      <sha1>3d92338oymamuojlp0mjfpxt9348l5t</sha1>
    </revision>
  </page>
  <page>
    <title>Type safety</title>
    <ns>0</ns>
    <id>602650</id>
    <revision>
      <id>862348706</id>
      <parentid>849957922</parentid>
      <timestamp>2018-10-03T20:26:45Z</timestamp>
      <contributor>
        <ip>63.80.210.198</ip>
      </contributor>
      <comment>/* C++ */ C++ strongly-typed enumerations, explicit constructors, and explicit conversion operators</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="27048">{{Type systems}}
In [[computer science]], '''type safety''' is the extent to which a [[programming language]] discourages or prevents '''[[type error]]s'''. A type error is erroneous or undesirable program behaviour caused by a discrepancy between differing [[data type]]s for the program's constants, variables, and methods (functions), e.g., treating an integer ('''int''') as a floating-point number ('''float'''). Type safety is sometimes alternatively considered to be a property of a computer program rather than the language in which that program is written; that is, some languages have type-safe facilities that can be circumvented by programmers who adopt practices that exhibit poor type safety. The formal [[type theory|type-theoretic]] definition of type safety is considerably stronger than what is understood by most programmers.

[[Type enforcement]] can be static, catching potential errors at [[compile time]], or dynamic, associating type information with values at [[Run time (program lifecycle phase)|run-time]] and consulting them as needed to detect imminent errors, or a combination of both.

The behaviors classified as type errors by a given programming language are usually those that result from attempts to perform operations on [[value (computer science)|values]] that are not of the appropriate [[data type]].  This classification is partly based on opinion; it may imply that any operation not leading to program crashes, security flaws or other obvious failures is legitimate and need not be considered an error, or it may imply that any contravention of the programmer's explicit intent (as communicated via typing annotations) to be erroneous and not "type-safe".

In the context of static (compile-time) type systems, type safety usually involves (among other things) a guarantee that the eventual value of any [[expression (programming)|expression]] will be a legitimate member of that expression's static type. The precise requirement is more subtle than this — see, for example, [[subtype]] and [[polymorphism (computer science)|polymorphism]] for complications.

Type safety is closely linked to ''[[memory safety]]'', a restriction on the ability to copy arbitrary bit patterns from one memory location to another.  For instance, in an implementation of a language that has some type &lt;math&gt;t&lt;/math&gt;, such that some sequence of bits (of the appropriate length) does not represent a legitimate member of &lt;math&gt;t&lt;/math&gt;, if that language allows data to be copied into a [[Variable (programming)|variable]] of type &lt;math&gt;t&lt;/math&gt;, then it is not type-safe because such an operation might assign a non-&lt;math&gt;t&lt;/math&gt; value to that variable.  Conversely, if the language is type-unsafe to the extent of allowing an arbitrary integer to be used as a [[pointer (computer programming)|pointer]], then it is not memory-safe.

Most statically typed languages provide a degree of type safety that is strictly stronger than memory safety, because their type systems enforce the proper use of [[abstract data type]]s defined by programmers even when this is not strictly necessary for memory safety or for the prevention of any kind of catastrophic failure.

==Definitions==
Type-safe code accesses only the memory locations it is authorized to access. (For this discussion, type safety specifically refers to memory type safety and should not be confused with type safety in a broader respect.) For example, type-safe code cannot read values from another object's private fields.

[[Robin Milner]] provided the following slogan to describe type safety:
:Well-typed programs cannot "go wrong".&lt;ref&gt;{{Citation
  | last1 = Milner | first1 = Robin
  | title = A Theory of Type Polymorphism in Programming
  | journal = Jcss
  | pages = 348–375
  | volume = 17
  | year = 1978
  | doi=10.1016/0022-0000(78)90014-4
 }}&lt;/ref&gt; 
The appropriate formalization of this slogan depends on the style of formal semantics used for a particular language.  In the context of [[denotational semantics]], type safety means that the value of an expression that is well-typed, say with type τ, is a ''bona fide'' member of the set corresponding to τ.

In 1994, Andrew Wright and [[Matthias Felleisen]] formulated what is now the standard definition and proof technique for type safety in languages defined by [[operational semantics]].  Under this approach, type safety is determined by two properties of the semantics of the programming language:

;(Type-) preservation or [[subject reduction]]: "Well typedness" ("typability") of programs remains invariant under the transition rules (i.e. evaluation rules or reduction rules) of the language.
;Progress: A well typed (typable) program never gets "stuck", which means the [[Expression (computer science)|expressions]] in the program will either be evaluated to a [[Value (computer science)|value]], or there is a transition rule for it; in other words, the program never gets into an undefined state where no further transitions are possible.

These properties do not exist in a vacuum; they are linked to the semantics of the programming language they describe, and there is a large space of varied languages that can fit these criteria, since the notion of "well typed" program is part of the static semantics of the programming language and the notion of "getting stuck" (or "going wrong") is a property of its [[dynamic semantics]].

Vijay Saraswat provides the following definition:
:"A language is type-safe if the only operations that can be performed on data in the language are those sanctioned by the type of the data." &lt;ref name=saraswat-java&gt;{{cite web|last=Saraswat |first=Vijay | date=1997-08-15 |title=Java is not type-safe |url=http://www.cis.upenn.edu/~bcpierce/courses/629/papers/Saraswat-javabug.html |accessdate=2008-10-08 }}&lt;/ref&gt;

==Relation to other forms of safety==
Type safety is ultimately aimed at excluding other problems, e.g.:-
* Prevention of illegal operations. For example, we can identify an expression &lt;code&gt;3 / "Hello, World"&lt;/code&gt; as invalid, because the rules of [[arithmetic]] do not specify how to divide an [[integer]] by a [[string (computer science)|string]].
* [[Memory safety]]
** [[Wild pointer]]s can arise when a pointer to one type object is treated as a pointer to another type. For instance, the size of an object depends on the type, so if a pointer is incremented under the wrong credentials, it will end up pointing at some random area of memory.
** [[Buffer overflow]] - Out-of bound writes can corrupt the contents of objects already present on the heap. This can occur when a larger object of one type is crudely copied into smaller object of another type.
* [[Logic error]]s originating in the [[semantics]] of different types. For instance, inches and millimeters may both be stored as integers, but should not be substituted for each other or added. A type system can enforce two different types of integer for them.

==Type-safe and type-unsafe languages==
Type safety is usually a requirement for any [[toy language]] proposed in academic programming language research. Many languages, on the other hand, are too big for human-generated type safety proofs, as they often require checking thousands of cases. Nevertheless, some languages such as [[Standard ML]], which has rigorously defined semantics, have been proved to meet one definition of type safety.&lt;ref&gt;[http://www.smlnj.org/sml.html Standard ML]. Smlnj.org. Retrieved on 2013-11-02.&lt;/ref&gt; Some other languages such as [[Haskell (programming language)|Haskell]] are ''believed''{{discuss}} to meet some definition of type safety, provided certain "escape" features are not used (for example Haskell's &lt;tt&gt;unsafePerformIO&lt;/tt&gt;, used to "escape" from the usual restricted environment in which I/O is possible, circumvents the type system and so can be used to break type safety.&lt;ref&gt;{{cite web|url=http://www.haskell.org/ghc/docs/latest/html/libraries/base/System-IO-Unsafe.html#v:unsafePerformIO |title=System.IO.Unsafe |work=GHC libraries manual: base-3.0.1.0 |accessdate=2008-07-17 }}&lt;/ref&gt;) [[Type punning]] is another example of such an "escape" feature. Regardless of the properties of the language definition, certain errors may occur at [[Run time (program lifecycle phase)|run-time]] due to bugs in the implementation, or in linked [[library (computer science)|libraries]] written in other languages; such errors could render a given implementation type unsafe in certain circumstances. An early version of Sun's [[Java virtual machine]] was vulnerable to this sort of problem.&lt;ref name=saraswat-java /&gt;

== Strong and weak typing ==
{{Main|Strong and weak typing}}
Programming languages are often colloquially classified as strongly typed or weakly typed (also loosely typed) to refer to certain aspects of type safety. In 1974, [[Barbara Liskov|Liskov]] and Zilles defined a strongly-typed language as one in which "whenever an object is passed from a calling function to a called function, its type must be compatible with the type declared in the called function."&lt;ref&gt;{{cite journal | citeseerx = 10.1.1.136.3043 | title = Programming with abstract data types | first1 = B | last1 = Liskov | first2 = S | last2 = Zilles | journal = ACM SIGPLAN Notices | year = 1974 | doi=10.1145/942572.807045}}&lt;/ref&gt;
In 1977, Jackson wrote, "In a strongly typed language each data area will have a distinct type and each process will state its communication requirements in terms of these types."&lt;ref&gt;{{cite conference | title = Parallel processing and modular software construction | first1 = K. | last1 = Jackson | journal = Design and Implementation of Programming Languages | year = 1977 | volume = 54 | pages = 436–443 | doi = 10.1007/BFb0021435 | url = http://www.springerlink.com/content/wq02703237400667/ | series = Lecture Notes in Computer Science | isbn = 3-540-08360-X }}&lt;/ref&gt;
In contrast, a weakly typed language may produce unpredictable results or may perform implicit type conversion.&lt;ref&gt;{{cite web | url =http://www.cs.cornell.edu/ | title =CS1130. Transition to OO programming. – Spring 2012 --self-paced version | date = 2005 | work = | publisher = Cornell University, Department of Computer Science | archiveurl = http://www.cs.cornell.edu/courses/CS1130/2012sp/1130selfpaced/module1/module1part4/strongtyping.htm| archivedate = 2005 | accessdate = 2015-11-23}}&lt;/ref&gt;

==Type safety in object oriented languages==
In [[object oriented]] languages type safety is usually intrinsic in the fact that a [[type system]] is in place. This is expressed in terms of class definitions.

A [[Class (computer science)|class]] essentially defines the structure of the objects derived from it and an [[Api#API in object-oriented languages|API]] as a ''contract'' for handling these objects.
Each time a new object is created it will ''comply'' with that contract.

Each function that exchanges objects derived from a specific class, or implementing a specific [[Interface (computer science)|interface]], will adhere to that contract: hence in that function the operations permitted on that object will be only those defined by the methods of the class the object implements.
This will guarantee that the object integrity will be preserved.&lt;ref&gt;Type safety is hence also a matter of good class definition: public methods that modify the internal state of an object shall preserve the object itegrity&lt;/ref&gt;

Exceptions to this are object oriented languages that allow dynamic modification of the object structure, or the use of reflection to modify the content of an object to overcome the constraints imposed by the class methods definitions.

==Type safety issues in specific languages==

=== Ada ===

{{wikibooks|Ada Programming|Type System}}
[[Ada programming language|Ada]] was designed to be suitable for [[embedded system]]s, [[device driver]]s and other forms of [[system programming]], but also to encourage type-safe programming.  To resolve these conflicting goals, Ada confines type-unsafety to a certain set of special constructs whose names usually begin with the string &lt;tt&gt;Unchecked_&lt;/tt&gt;. Unchecked_Deallocation can be effectively banned from a unit of Ada text by applying &lt;tt&gt;pragma Pure&lt;/tt&gt; to this  unit. It is expected that programmers will use &lt;tt&gt;Unchecked_&lt;/tt&gt; constructs very carefully and only when necessary; programs that do not use them are type-safe.

The [[SPARK programming language]] is a subset of Ada eliminating all its potential ambiguities and insecurities while at the same time adding [[design by contract|statically checked contracts]] to the language features available. SPARK avoids the issues with [[dangling pointer]]s by disallowing allocation at run time entirely.

Ada2012 adds [[design by contract|statically checked contracts]] to the language itself (in form of pre-, and post-conditions, as well as type invariants).

=== C ===

{{Wikibooks|C Programming}}
The [[C (programming language)|C programming language]] is type-safe in limited contexts; for example, a compile-time error is generated when an attempt is made to convert a pointer to one type of structure to a pointer to another type of structure, unless an explicit cast is used.  However, a number of very common operations are non-type-safe; for example, the usual way to print an integer is something like &lt;code&gt;printf("%d", 12)&lt;/code&gt;, where the &lt;code&gt;%d&lt;/code&gt; tells &lt;code&gt;printf&lt;/code&gt; at run-time to expect an integer argument. (Something like &lt;code&gt;printf("%s", 12)&lt;/code&gt;, which erroneously tells the function to expect a pointer to a character-string, may be accepted by compilers, but will produce undefined results.) This is partially mitigated by some compilers (such as gcc) checking type correspondences between printf arguments and format strings.

In addition, C, like Ada, provides unspecified or undefined explicit conversions; and unlike in Ada, idioms that use these conversions are very common, and have helped to give C a type-unsafe reputation. For example, the standard way to allocate memory on the heap is to invoke a memory allocation function, such as &lt;code&gt;[[malloc]]&lt;/code&gt;, with an argument indicating how many bytes are required. The function returns an untyped pointer (type &lt;code&gt;void *&lt;/code&gt;), which the calling code must explicitly or implicitly cast to the appropriate pointer type. Pre-standardized implementations of C required an explicit cast to do so, therefore the code &lt;code&gt;(struct foo *) malloc(sizeof(struct foo))&lt;/code&gt; became the accepted practice.&lt;ref name="knr"&gt;{{cite book | last = Kernighan | author1-link = Brian Kernighan | author2 = Dennis M. Ritchie | authorlink2 = Dennis Ritchie | title = The C Programming Language | edition = 2nd | publisher = [[Prentice Hall]] | date = March 1988 | location = [[Englewood Cliffs, NJ]] | url = http://cm.bell-labs.com/cm/cs/cbook/ | isbn = 0-13-110362-8 | quote = In C, the proper method is to declare that malloc returns a pointer to void, then explicitly coerce the pointer into the desired type with a cast. | page = 116 | deadurl = yes | archiveurl = https://web.archive.org/web/20081106175456/http://cm.bell-labs.com/cm/cs/cbook/ | archivedate = 2008-11-06 | df =  }}&lt;/ref&gt; However, this practice is discouraged in ISO C as it can mask a failure to include the header file in which &lt;code&gt;malloc&lt;/code&gt; is defined, resulting in downstream errors on machines where the int and pointer types are of different sizes, such as most common implementations of C for the now-ubiquitous [[x86-64]] architecture.&lt;ref name="stackdiscussion"&gt;{{cite web |url= https://stackoverflow.com/questions/605845/do-i-cast-the-result-of-malloc/605858#605858|title= Do I cast the result of malloc?|date=7 November 2012|publisher=[[Stack Overflow]]|accessdate=21 February 2013}}&lt;/ref&gt; A conflict arises in code that is required to compile as C++, since the cast is necessary in that language.

===C++===
Some features of C++ that promote more type-safe code:
* The [[new (C++)|new]] operator returns a pointer of type based on operand, whereas [[malloc]] returns a void pointer.
* C++ code can use virtual functions and [[Template (programming)|templates]] to achieve polymorphism without void pointers.
* Safer casting operators, such as [[dynamic cast]] that performs run-time type checking.
* [[C++11#Strongly_typed_enumerations|C++11 strongly typed enumerations]] cannot be implicitly converted to or from integers or other enumeration types.
* [[C++11#Explicit_conversion_operators|C++ explicit constructors and C++11 explicit conversion operators]] prevent implicit type conversions.

===C#===
[[C Sharp (programming language)|C#]] is type-safe (but not statically type-safe). It has support for untyped pointers, but this must be accessed using the "unsafe" keyword which can be prohibited at the compiler level. It has inherent support for run-time cast validation. Casts can be validated by using the "as" keyword that will return a null reference if the cast is invalid, or by using a C-style cast that will throw an exception if the cast is invalid. See [[C Sharp Syntax#Conversion operators|C Sharp conversion operators]].

Undue reliance on the [[C Sharp syntax#object class|object]] type (from which all other types are derived) runs the risk of defeating the purpose of the C# type system. It is usually better practice to abandon object references in favour of [[Generic programming#Generic programming in .NET|generics]], similar to templates in C++ and [[generics in Java]].

===Java===
{{Wikibooks|Java Programming}}
The [[Java language]] is designed to enforce type safety. 
Anything in Java ''happens'' inside an [[Object (computer science)|object]]
and each object is an instance of a [[Class (computer programming)|class]].

To implement the ''type safety'' enforcement, each object, before usage, needs to be [[Memory allocation|allocated]].
Java allows usage of [[Primitive data type|primitive types]] but only inside properly allocated objects.

Sometimes a part of the type safety is implemented indirectly: e.g. the class BigDecimal represents a floating point number of arbitrary precision, but handles only numbers that can be expressed with a finite representation.
The operation BigDecimal.divide() calculates a new object as the division of two numbers expressed as BigDecimal.

In this case if the division has no finite representation, as when one computes e.g. 1/3=0.33333..., the divide() method can raise an exception if no rounding mode is defined for the operation.
Hence the library, rather than the language, guarantees that the object respects the contract implicit in the class definition.

===Standard ML===
{{Wikibooks|Standard ML Programming}}
[[Standard ML|SML]] has rigorously defined semantics and is known to be type-safe.  However, some implementations of SML, including [[Standard ML of New Jersey]] (SML/NJ), its syntactic variant [[Mythryl]] and [[Mlton]], provide libraries that offer certain unsafe operations.  These facilities are often used in conjunction with those implementations' [[foreign function interface]]s to interact with non-ML code (such as C libraries) that may require data laid out in specific ways.  Another example is the SML/NJ [[read-eval-print loop|interactive toplevel]] itself, which must use unsafe operations to execute ML code entered by the user.

=== Modula-2 ===
Modula-2 is a strongly typed language with a design philosophy to require any unsafe facilities to be explicitly marked as unsafe. This is achieved by "moving" such facilities into a built-in pseudo-library called SYSTEM from where they must be imported before they can be used. The import thus makes it visible when such facilities are used. Unfortunately, this was not consequently implemented in the original language report and its implementation.&lt;ref&gt;{{cite book|author=Niklaus Wirth|title=Programming in Modula-2|publisher=Springer Verlag|date=1985}}&lt;/ref&gt; There still remained unsafe facilities such as the type cast syntax and variant records (inherited from Pascal) that could be used without prior import.&lt;ref name="knightsoftype.blogspot.com"&gt;{{cite web|title=The Separation of Safe and Unsafe Facilities|url=http://knightsoftype.blogspot.com/2013/12/the-separation-of-safe-and-unsafe.html|accessdate=24 March 2015}}&lt;/ref&gt; The difficulty in moving these facilities into the SYSTEM pseudo-module was the lack of any identifier for the facility that could then be imported since only identifiers can be imported, but not syntax.

&lt;source lang="modula2"&gt;
IMPORT SYSTEM; (* allows the use of certain unsafe facilities: *)
VAR word : SYSTEM.WORD; addr : SYSTEM.ADDRESS;
addr := SYSTEM.ADR(word);

(* but type cast syntax can be used without such import *)
VAR i : INTEGER; n : CARDINAL;
n := CARDINAL(i); (* or *) i := INTEGER(n);
&lt;/source&gt;

The ISO Modula-2 standard corrected this for the type cast facility by changing the type cast syntax into a function called CAST which has to be imported from pseudo-module SYSTEM. However, other unsafe facilities such as variant records remained available without any import from pseudo-module SYSTEM.&lt;ref&gt;{{cite web|title=ISO Modula-2 Language Reference|url=http://www.excelsior-usa.com/doc/xds/isom2.html|accessdate=24 March 2015}}&lt;/ref&gt;

&lt;source lang="modula2"&gt;
IMPORT SYSTEM;
VAR i : INTEGER; n : CARDINAL;
i := SYSTEM.CAST(INTEGER, n); (* Type cast in ISO Modula-2 *)
&lt;/source&gt;

A recent revision of the language applied the original design philosophy rigorously. First, pseudo-module SYSTEM was renamed to UNSAFE to make the unsafe nature of facilities imported from there more explicit. Then all remaining unsafe facilities where either removed altogether (for example variant records) or moved to pseudo-module UNSAFE. For facilities where there is no identifier that could be imported, enabling identifiers were introduced. In order to enable such a facility, its corresponding enabling identifier must be imported from pseudo-module UNSAFE. No unsafe facilities remain in the language that do not require import from UNSAFE.&lt;ref name="knightsoftype.blogspot.com"/&gt;

&lt;source lang="modula2"&gt;
IMPORT UNSAFE;
VAR i : INTEGER; n : CARDINAL;
i := UNSAFE.CAST(INTEGER, n); (* Type cast in Modula-2 Revision 2010 *)

FROM UNSAFE IMPORT FFI; (* enabling identifier for foreign function interface facility *)
&lt;*FFI="C"*&gt; (* pragma for foreign function interface to C *)
&lt;/source&gt;

===Pascal===
{{Wikibooks|Pascal Programming}}
[[Pascal (programming language)|Pascal]] has had a number of type safety requirements, some of which are kept in some compilers.  Where a Pascal compiler dictates "strict typing", two variables cannot be assigned to each other unless they are either compatible (such as conversion of integer to real) or assigned to the identical subtype.  For example, if you have the following code fragment:

&lt;source lang="pascal"&gt;
type
  TwoTypes = record
     I: Integer;
     Q: Real;
  end;

  DualTypes = record
    I: Integer;
    Q: Real;
  end;

var
  T1, T2:  TwoTypes;
  D1, D2:  DualTypes;
&lt;/source&gt;

Under strict typing, a variable defined as &lt;tt&gt;TwoTypes&lt;/tt&gt; is ''not compatible'' with &lt;tt&gt;DualTypes&lt;/tt&gt; (because they are not identical, even though the components of that user defined type are identical) and an assignment of &lt;tt&gt; T1 := D2; &lt;/tt&gt; is illegal.  An assignment of &lt;tt&gt;T1 := T2; &lt;/tt&gt; would be legal because the subtypes they are defined to ''are'' identical.  However, an assignment such as &lt;tt&gt;T1.Q := D1.Q;&lt;/tt&gt; would be legal.

===Common Lisp===
In general, [[Common Lisp]] is a type-safe language. A Common Lisp compiler is responsible for inserting dynamic checks for operations whose type safety cannot be proven statically. However, a programmer may indicate that a program should be compiled with a lower level of dynamic type-checking.&lt;ref&gt;{{cite web|title=Common Lisp HyperSpec|url=http://www.lispworks.com/documentation/HyperSpec/Body/d_optimi.htm|accessdate=26 May 2013}}&lt;/ref&gt; A program compiled in such a mode cannot be considered type-safe.

== C++ Examples ==
The following examples illustrates how C++ cast operators can break type safety when used incorrectly. The first example shows how basic data types can be incorrectly casted:
&lt;source lang="cpp"&gt;
#include &lt;iostream&gt;
using namespace std;

int main () {
    int   ival = 5;                              // integer value
    float fval = reinterpret_cast&lt;float&amp;&gt;(ival); // reinterpret bit pattern
    cout &lt;&lt; fval &lt;&lt; endl;                        // output integer as float
    return 0;
}
&lt;/source&gt;
In this example, &lt;code&gt;reinterpret_cast&lt;/code&gt; explicitly prevents the compiler from performing a safe conversion from integer to floating-point value.&lt;ref&gt;http://en.cppreference.com/w/cpp/language/reinterpret_cast&lt;/ref&gt; When the program runs it will output a garbage floating-point value. The problem could have been avoided by instead writing &lt;code&gt;float fval = ival;&lt;/code&gt;

The next example shows how object references can be incorrectly downcasted:
&lt;source lang="cpp"&gt;
#include &lt;iostream&gt;
using namespace std;

class Parent {
public:
    virtual ~Parent() {} // virtual destructor for RTTI
};

class Child1 : public Parent {
public:
    int a;
};

class Child2 : public Parent {
public:
    float b;
};

int main () {
    Child1 c1;
    c1.a = 5;
    Parent &amp; p = c1;                     // upcast always safe
    Child2 &amp; c2 = static_cast&lt;Child2&amp;&gt;(p); // invalid downcast
    cout &lt;&lt; c2.b &lt;&lt; endl;          // will output garbage data
    return 0;
}
&lt;/source&gt;
The two child classes have members of different types. When downcasting a parent class pointer to a child class pointer, then the resulting pointer may not point to a valid object of correct type. In the example, this leads to garbage value being printed. The problem could have been avoided by replacing &lt;code&gt;static_cast&lt;/code&gt; with &lt;code&gt;dynamic_cast&lt;/code&gt; that throws an exception on invalid casts.&lt;ref&gt;http://en.cppreference.com/w/cpp/language/dynamic_cast&lt;/ref&gt;

==See also==
*[[Type theory]]

==Notes==
{{reflist}}

==References==
*{{Cite book|authorlink=Benjamin C. Pierce |last=Pierce |first=Benjamin C. |title=Types and Programming Languages |publisher=MIT Press |year=2002 |isbn=0-262-16209-1 |url=http://www.cis.upenn.edu/~bcpierce/tapl/}}
*{{cite web|title=Type Safe |work=Portland Pattern Repository Wiki |url=http://c2.com/cgi/wiki?TypeSafe}}
*{{Cite journal|last=Wright |first=Andrew K. |author2=[[Matthias Felleisen]]  |title=A Syntactic Approach to Type Soundness |journal=Information and Computation |volume=115 |issue=1 |pages=38–94 |year=1994 |url=http://citeseer.ist.psu.edu/wright92syntactic.html |doi=10.1006/inco.1994.1093}}
*{{Cite journal|first=Stavros |last=Macrakis |title=Safety and power |journal=ACM SIGSOFT Software Engineering Notes |volume=7 |issue=2 |pages=25–26 |date=April 1982 |url=http://portal.acm.org/citation.cfm?id=1005937.1005941 |format=requires subscription |doi=10.1145/1005937.1005941}}

{{DEFAULTSORT:Type Safety}}
[[Category:Programming language topics]]
[[Category:Type theory]]
[[Category:Articles with example Pascal code]]</text>
      <sha1>iykrnrqbjajrdm1zy7vjgzys8ejxgbi</sha1>
    </revision>
  </page>
  <page>
    <title>Value (computer science)</title>
    <ns>0</ns>
    <id>338331</id>
    <revision>
      <id>803728831</id>
      <parentid>801649724</parentid>
      <timestamp>2017-10-04T07:33:39Z</timestamp>
      <contributor>
        <username>Liberalartist</username>
        <id>3784662</id>
      </contributor>
      <comment>Clarify that values are different from expressions</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6103">{{refimprove|date=August 2009}}
In [[computer science]], a '''value''' is the representation of some entity that can be manipulated by a program. The members of a [[type (computer science)|type]] are the values of that type.{{sfn|Mitchell|1996|p=9}} 

The "value of a variable" is given by the corresponding [[Map (computer science)|mapping]] in the [[environment (type theory)|environment]].{{Citation needed|date=March 2011}} In languages with [[assignable variable]]s it becomes necessary to distinguish between the ''r-value'' (or contents) and the ''l-value'' (or location) of a variable.{{sfn|Mitchell|1996|pp=389–390}}

In [[Declarative programming|declarative]] (high-level) languages, values have to be [[referential transparency (computer science)|referentially transparent]]. This means that the resulting value is independent of the location in which a (sub-)expression needed to compute the value is stored. Only the contents of the location (the bits, whether they are 1 or 0) and their interpretation are significant.{{Citation needed|date=March 2011}}

=={{Anchor|lrvalue}}Assignment: l-values and r-values==
Some languages use the idea of '''l-values''' and '''r-values''', deriving from the typical mode of evaluation on the left and right hand side of an assignment statement. An lvalue refers to an object that persists beyond a single expression. An rvalue is a temporary value that does not persist beyond the expression that uses it.&lt;ref&gt;{{cite web|title=Lvalues and Rvalues (Visual C++)|url=https://msdn.microsoft.com/en-us/library/f90831hc.aspx|website=Microsoft Developer Network|accessdate=3 September 2016}}&lt;/ref&gt;

The notion of l-values and r-values was introduced by [[Combined Programming Language]] (CPL). The notions in an expression of r-value, l-value, and r-value/l-value are analogous to the [[Parameter (computer programming)|parameter]] modes of input parameter (has a value), [[output parameter]] (can be assigned), and input/output parameter (has a value and can be assigned), though the technical details differ between contexts and languages.

===R-values and addresses===
In many languages, notably the [[C programming language|C family]], l-values have [[memory address|storage address]]es that are programmatically accessible to the running program (e.g., via some address-of operator like "&amp;" in C/C++), meaning that they are variables or dereferenced references to a certain memory location. R-values can be l-values (see below) or non-l-values—a term only used to distinguish from l-values. Consider the C expression &lt;code&gt;4 + 9&lt;/code&gt;. When executed, the computer generates an integer value of 13, but because the program has not explicitly designated where in the computer this 13 is stored, the expression is a non l-value. On the other hand, if a C program declares a variable x and assigns the value of 13 to x, then the expression &lt;code&gt;x&lt;/code&gt; has a value of 13 and is an l-value.

In C, the term l-value originally meant something that could be assigned to (hence the name, indicating it is on the left side of the assignment operator), but since the reserved word &lt;code&gt;const&lt;/code&gt; (constant) was added to the language, the term is now 'modifiable l-value'. In [[C++11]] a special semantic-glyph &lt;code&gt;&amp;&amp;&lt;/code&gt; exists, to denote the ''use/access of the expression's address for the ''compiler'' only''; i.e., the address cannot be retrieved using the address-of, &lt;code&gt;&amp;&lt;/code&gt;, operator during the ''run-time'' of the program (see [[C++11#Rvalue references and move constructors|the use of move semantics]]).

This type of reference can be applied to ''all'' r-values including non-l-values as well as l-values. Some processors provide one or more instructions which take an '''immediate value''', sometimes referred to as "immediate" for short. An immediate value is stored as part of the instruction which employs it, usually to load into, add to, or subtract from, a register. The other parts of the instruction are the [[opcode]], and destination. The latter may be implicit. (A non-immediate value may reside in a register, or be stored elsewhere in memory, requiring the instruction to contain a direct or indirect address [e.g., index register address] to the value.)

The l-value expression designates (refers to) an object. A non-modifiable l-value is addressable, but not assignable. A modifiable l-value allows the designated object to be changed as well as examined. An r-value is any expression, a non-l-value is any expression that is not an l-value. One example is an "immediate value" (look below) and consequently not addressable..

== In assembly language ==
A value can be virtually any kind of data by a given [[data type]], for instance a string, a digit, a single letter.

Processors often support more than one size of immediate data, e.g. 8 or 16 bit, employing a unique opcode and mnemonic for each instruction variant. If a programmer supplies a data value that will not fit, the assembler issues an "Out of range" error message. Most assemblers allow an immediate value to be expressed as [[ASCII]], [[decimal]], [[hexadecimal]], [[octal]], or [[Binary code|binary]] data. Thus, the ASCII character &lt;tt&gt;'A'&lt;/tt&gt; is the same as &lt;tt&gt;65&lt;/tt&gt; or &lt;tt&gt;0x41&lt;/tt&gt;. The [[byte order]] of strings may differ between processors, depending on the assembler and computer architecture.

== Notes ==
{{reflist|2}}

== References ==
* {{cite book|first=John C.|last=Mitchell|authorlink=John C. Mitchell|title=Foundations for Programming Languages|year=1996|publisher=The MIT Press|isbn=0-262-13321-0|ref=harv}}
* {{cite journal|first=Christopher|last=Strachey|authorlink=Christopher Strachey|title=Fundamental Concepts in Programming Languages|journal=[[Higher-Order and Symbolic Computation]]|volume=13|pages=11–49|year=2000|doi=10.1023/A:1010000313106|ref=harv}}

==External links==
*[http://c2.com/cgi/wiki?ValueObject Value Object]
*[http://www.oracle.com/technetwork/java/transferobject-139757.html Transfer Object Pattern]

[[Category:Computer data]]
[[Category:Programming language concepts]]
[[Category:Type theory]]</text>
      <sha1>bk7k21q55up2kzzrmfnuxt4xbe0h94z</sha1>
    </revision>
  </page>
  <page>
    <title>Visual modeling</title>
    <ns>0</ns>
    <id>5734612</id>
    <revision>
      <id>741255929</id>
      <parentid>741255471</parentid>
      <timestamp>2016-09-26T10:28:54Z</timestamp>
      <contributor>
        <username>TimJay</username>
        <id>29263655</id>
      </contributor>
      <comment>added Reactive Blocks</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1838">'''Visual modeling''' is the graphic representation of objects and systems of interest using [[Graphical modeling language|graphical language]]s.  Visual modeling languages may be [[General-Purpose Modeling]] (GPM) languages (e.g., [[Unified Modeling Language|UML]], [[Southbeach Notation]], [[IDEF]]) or [[Domain-Specific Modeling]] (DSM) languages (e.g., [[SysML]]). They include industry open standards (e.g., UML, SysML, [[Modelica]]), as well as proprietary standards, such as the visual languages associated with [[VisSim]], [[MATLAB]] and [[Simulink]], [[OPNET]], [[NetSim]], [[NI Multisim]], and [[Reactive Blocks]]. Both VisSim and Reactive Blocks provide a royalty-free, downloadable viewer that lets anyone open and interactively simulate their models. The community edition of Reactive Blocks also allows full editing of the models as well as compilation, as long as the work is published under the [[Eclipse Public License]].  Visual modeling languages are an area of active research that continues to evolve, as evidenced by increasing interest in DSM languages, visual [[requirements]], and visual OWL ([[Web Ontology Language]]).&lt;ref name="faq"&gt;{{cite web|author=|title=Visual OWL|url=http://www.visualmodeling.com/VisualOWL.htm}}&lt;/ref&gt;

==See also==
* [[Service-oriented modeling#Discipline-specific modeling|Discipline-Specific Modeling]] 
* [[Domain-Specific Modeling]] 
* [[Model Driven Engineering]] 
* [[Modeling language]]

== References ==
{{reflist}}

==External links==
*[http://www.VisualModelingForum.com Visual Modeling Forum] A web community dedicated to visual modeling languages and tools.
[[Category:Programming language topics]]
[[Category:Unified Modeling Language]]
[[Category:Simulation programming languages]]

{{comp-sci-stub}}&lt;!-- should be information visualization stub or something like that --&gt;</text>
      <sha1>rgdp0fm1k6botgb6ymc2xr4zr9b33jo</sha1>
    </revision>
  </page>
  <page>
    <title>Well-formed formula</title>
    <ns>0</ns>
    <id>404582</id>
    <revision>
      <id>870614674</id>
      <parentid>841699491</parentid>
      <timestamp>2018-11-25T23:43:31Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <minor/>
      <comment>linking</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15754">In [[mathematical logic]], [[propositional logic]] and [[predicate logic]], a '''well-formed formula''', abbreviated '''WFF''' or '''wff''', often simply '''formula''', is a finite [[sequence]] of [[symbol (formal)|symbols]] from a given [[alphabet (computer science)|alphabet]] that is part of a [[formal language]].&lt;ref&gt;Formulas are a standard topic in introductory logic, and are covered by all introductory textbooks, including Enderton (2001), Gamut (1990), and Kleene (1967)&lt;/ref&gt; A formal language can be identified with the set of formulas in the language.

A formula is a [[syntax (logic)|syntactic]] object that can be given a semantic [[Formal semantics (logic)|meaning]] by means of an interpretation. Two key uses of formulas are in propositional logic and predicate logic.

==Introduction==
A key use of formulas is in [[propositional logic]] and [[predicate logic]]s such as [[first-order logic]].  In those contexts, a formula is a string of symbols φ for which it makes sense to ask "is &amp;phi; true?", once any [[free variable]]s in φ have been instantiated.  In formal logic, [[Mathematical proof|proof]]s can be represented by sequences of formulas with certain properties, and the final formula in the sequence is what is proven.

Although the term "formula" may be used for written marks (for instance, on a piece of paper or chalkboard), it is more precisely understood as the sequence of symbols being expressed, with the marks being a [[type-token distinction|token]] instance of formula. Thus the same formula may be written more than once, and a formula might in principle be so long that it cannot be written at all within the physical universe.

Formulas themselves are syntactic objects. They are given meanings by interpretations. For example, in a propositional formula, each propositional variable may be interpreted as a concrete proposition, so that the overall formula expresses a relationship between these propositions. A formula need not be interpreted, however, to be considered solely as a formula.

==Propositional calculus==
{{Main article|Propositional calculus}}
The formulas of [[propositional calculus]], also called [[propositional formula]]s,&lt;ref&gt;First-order logic and automated theorem proving, Melvin Fitting, Springer, 1996 [https://books.google.com/books?id=T8OMqvXvWWQC&amp;lpg=PA11&amp;dq=%22propositional%20formula%22&amp;hl=en&amp;pg=PA11#v=onepage&amp;q=%22propositional%20formula%22&amp;f=false]&lt;/ref&gt; are expressions such as &lt;math&gt;(A \land (B \lor C))&lt;/math&gt;. Their definition begins with the arbitrary choice of a set ''V'' of  [[propositional variable]]s. The alphabet consists of the letters in ''V'' along with the symbols for the [[logical connective|propositional connective]]s and parentheses "(" and ")", all of which are assumed to not be in ''V''.  The formulas will be certain expressions (that is, strings of symbols) over this alphabet.

The formulas are [[inductive definition|inductively]] defined as follows:
* Each propositional variable is, on its own, a formula.
* If φ is a formula, then &amp;not;φ is a formula.
* If φ and ψ are formulas, and • is any binary connective, then ( φ • ψ) is a formula. Here • could be (but is not limited to) the usual operators ∨, ∧, →, or ↔.

This definition can also be written as a formal grammar in [[Backus–Naur form]], provided the set of variables is finite:
{{#tag:syntaxhighlight|&lt;alpha set&gt; ::= p {{!}} q {{!}} r {{!}} s {{!}} t {{!}} u {{!}} ... (the arbitrary finite set of propositional variables)
&lt;form&gt; ::= &lt;alpha set&gt; {{!}} ¬&lt;form&gt; {{!}} (&lt;form&gt;∧&lt;form&gt;) {{!}} (&lt;form&gt;∨&lt;form&gt;) {{!}} (&lt;form&gt;→&lt;form&gt;) {{!}} (&lt;form&gt;↔&lt;form&gt;)|lang="bnf"}}
Using this grammar, the sequence of symbols
:(((''p'' &amp;rarr; ''q'') &amp;and; (''r'' &amp;rarr; ''s'')) &amp;or; (&amp;not;''q'' &amp;and; &amp;not;''s''))
is a formula, because it is grammatically correct. The sequence of symbols
:((''p'' &amp;rarr; ''q'')&amp;rarr;(''qq''))''p''))
is not a formula, because it does not conform to the grammar.

A complex formula may be difficult to read, owing to, for example, the proliferation of parentheses. To alleviate this last phenomenon, precedence rules (akin to the [[Order of operations|standard mathematical order of operations]]) are assumed among the operators, making some operators more binding than others. For example, assuming the precedence (from most binding to least binding) 1. &amp;not; &amp;nbsp; 2. &amp;rarr;&amp;nbsp; 3. &amp;and;&amp;nbsp; 4. &amp;or;. Then the formula
:(((''p'' &amp;rarr; ''q'') &amp;and; (''r'' &amp;rarr; ''s'')) &amp;or; (&amp;not;''q'' &amp;and; &amp;not;''s''))
may be abbreviated as
:''p'' &amp;rarr; ''q'' &amp;and; ''r'' &amp;rarr; ''s'' &amp;or; &amp;not;''q'' &amp;and; &amp;not;''s''
This is, however, only a convention used to simplify the written representation of a formula. If the precedence was assumed, for example, to be left-right associative, in following order: 1. &amp;not; &amp;nbsp; 2. &amp;and;&amp;nbsp; 3. &amp;or;&amp;nbsp; 4. &amp;rarr;, then the same formula above (without parentheses) would be rewritten as
:(''p'' &amp;rarr; (''q'' &amp;and; ''r'')) &amp;rarr; (''s'' &amp;or; ((&amp;not;''q'') &amp;and; (&amp;not;''s'')))

==Predicate logic==
The definition of a formula in [[first-order logic]] &lt;math&gt;\mathcal{QS}&lt;/math&gt; is relative to the [[Signature (mathematical logic)|signature]] of the theory at hand. This signature specifies the constant symbols, relation symbols, and function symbols of the theory at hand, along with the [[Arity|arities]] of the function and relation symbols.

The definition of a formula comes in several parts. First, the set of '''[[Term (logic)|terms]]''' is defined recursively. Terms, informally, are expressions that represent objects from the domain of discourse.
#Any variable is a term.
#Any constant symbol from the signature is a term
#an expression of the form ''f''(''t''&lt;sub&gt;1&lt;/sub&gt;,...,''t''&lt;sub&gt;''n''&lt;/sub&gt;), where ''f'' is an ''n''-ary function symbol, and ''t''&lt;sub&gt;1&lt;/sub&gt;,...,''t''&lt;sub&gt;''n''&lt;/sub&gt; are terms, is again a term.

The next step is to define the [[atomic formula]]s.
#If ''t''&lt;sub&gt;1&lt;/sub&gt; and ''t''&lt;sub&gt;2&lt;/sub&gt; are terms then ''t''&lt;sub&gt;1&lt;/sub&gt;=''t''&lt;sub&gt;2&lt;/sub&gt; is an atomic formula
#If ''R'' is an ''n''-ary relation symbol, and ''t''&lt;sub&gt;1&lt;/sub&gt;,...,''t''&lt;sub&gt;''n''&lt;/sub&gt; are terms, then ''R''(''t''&lt;sub&gt;1&lt;/sub&gt;,...,''t''&lt;sub&gt;''n''&lt;/sub&gt;) is an atomic formula

Finally, the set of formulas is defined to be the smallest set containing the set of atomic formulas such that the following holds:
#&lt;math&gt;\neg\phi&lt;/math&gt; is a formula when &lt;math&gt;\ \phi&lt;/math&gt; is a formula
#&lt;math&gt;(\phi \land \psi)&lt;/math&gt; and &lt;math&gt;(\phi \lor \psi)&lt;/math&gt; are formulas when &lt;math&gt;\ \phi&lt;/math&gt; and &lt;math&gt;\ \psi&lt;/math&gt; are formulas;
#&lt;math&gt;\exists x\, \phi&lt;/math&gt; is a formula when &lt;math&gt;\ x&lt;/math&gt; is a variable and &lt;math&gt;\ \phi&lt;/math&gt; is a formula;
#&lt;math&gt;\forall x\, \phi&lt;/math&gt; is a formula when &lt;math&gt;\ x&lt;/math&gt; is a variable and &lt;math&gt;\ \phi&lt;/math&gt; is a formula (alternatively, &lt;math&gt;\forall x\, \phi&lt;/math&gt; could be defined as an abbreviation for &lt;math&gt;\neg\exists x\, \neg\phi&lt;/math&gt;).

If a formula has no occurrences of &lt;math&gt;\exists x&lt;/math&gt; or &lt;math&gt;\forall x&lt;/math&gt;, for any variable &lt;math&gt;\ x&lt;/math&gt;, then it is called ''quantifier-free''. An ''existential formula'' is a formula starting with a sequence of existential quantification followed by a quantifier-free formula.

==Atomic and open formulas==
{{Main article|Atomic formula}}

An ''atomic formula'' is a formula that contains no [[logical connective]]s nor [[Quantification (logic)|quantifiers]], or equivalently a formula that has no strict subformulas.
The precise form of atomic formulas depends on the formal system under consideration; for [[propositional logic]], for example, the atomic formulas are the [[propositional variable]]s. For [[predicate logic]], the atoms are predicate symbols together with their arguments, each argument being a [[Formation rules|term]].

According to some terminology, an ''open formula'' is formed by combining atomic formulas using only logical connectives, to the exclusion of quantifiers.&lt;ref&gt;Handbook of the history of logic, (Vol 5, Logic from Russell to Church), Tarski's logic by Keith Simmons, D. Gabbay and J. Woods Eds, p568 [https://books.google.com/books?id=K5dU9bEKencC&amp;lpg=PA568&amp;dq=%22open%20formula%22&amp;pg=PA568#v=onepage&amp;q=open%20formula&amp;f=false].&lt;/ref&gt; This has not to be confused with a formula which is not closed.

==Closed formulas==
{{Main article|Sentence (logic)}}

A ''closed formula'', also ''[[ground expression|ground]] formula'' or ''sentence'', is a formula in which there are no [[Free and bound variables|free occurrences]] of any [[variable (mathematics)|variable]]. If '''A''' is a formula of a first-order language in which the variables ''v&lt;sub&gt;1&lt;/sub&gt;'', ..., ''v&lt;sub&gt;n&lt;/sub&gt;'' have free occurrences, then '''A''' preceded by {{all}}''v&lt;sub&gt;1&lt;/sub&gt;'' ... {{all}}''v&lt;sub&gt;n&lt;/sub&gt;'' is a closure of '''A'''.

==Properties applicable to formulas==

* A formula '''A''' in a language &lt;math&gt;\mathcal{Q}&lt;/math&gt; is ''[[satisfiability and validity|valid]]'' if it is true for every [[interpretation (logic)|interpretation]] of &lt;math&gt;\mathcal{Q}&lt;/math&gt;.
* A formula '''A''' in a language &lt;math&gt;\mathcal{Q}&lt;/math&gt; is ''[[satisfiability and validity|satisfiable]]'' if it is true for some [[interpretation (logic)|interpretation]] of &lt;math&gt;\mathcal{Q}&lt;/math&gt;.
* A formula '''A''' of the language of [[Peano arithmetic|arithmetic]] is ''decidable'' if it represents a [[decidable set]], i.e. if there is an [[effective method]] which, given a [[substitution of variables|substitution]] of the free variables of '''A''', says that either the resulting instance of '''A''' is provable or its negation is.

==Usage of the terminology==
In earlier works on mathematical logic (e.g. by [[Alonzo Church|Church]]&lt;ref&gt;Alonzo Church, [1996] (1944), Introduction to mathematical logic, page 49&lt;/ref&gt;), formulas referred to any strings of symbols and among these strings, well-formed formulas were the strings that followed the formation rules of (correct) formulas.

Several authors simply say formula.&lt;ref&gt;[[David Hilbert |Hilbert, David]]; [[Wilhelm Ackermann |Ackermann, Wilhelm]] (1950) [1937], Principles of Mathematical Logic, New York: Chelsea&lt;/ref&gt;&lt;ref&gt;Hodges, Wilfrid (1997), A shorter model theory, Cambridge University Press, {{isbn|978-0-521-58713-6}}&lt;/ref&gt;&lt;ref&gt;[[Jon Barwise|Barwise, Jon]], ed. (1982), Handbook of Mathematical Logic, Studies in Logic and the Foundations of Mathematics, Amsterdam: North-Holland, {{isbn|978-0-444-86388-1}}&lt;/ref&gt;&lt;ref&gt;Cori, Rene; Lascar, Daniel (2000), Mathematical Logic: A Course with Exercises, Oxford University Press, {{isbn|978-0-19-850048-3}}&lt;/ref&gt; Modern usages (especially in the context of computer science with mathematical software such as [[List of model checking tools|model checkers]], [[automated theorem prover]]s, [[Interactive theorem proving|interactive theorem provers]]) tend to retain of the notion of formula only the algebraic concept and to leave the question of [[well-formedness]], i.e. of the concrete string representation of formulas (using this or that symbol for connectives and quantifiers, using this or that [[order of operations|parenthesizing convention]], using [[Polish notation|Polish]] or [[infix notation|infix]] notation, etc.) as a mere notational problem.

While the expression ''well-formed formula'' is still in use,&lt;ref&gt;Enderton, Herbert [2001] (1972), A mathematical introduction to logic (2nd ed.), Boston, MA: Academic Press, {{isbn|978-0-12-238452-3}}&lt;/ref&gt;&lt;ref&gt;R. L. Simpson (1999), Essentials of Symbolic Logic, page 12&lt;/ref&gt;&lt;ref&gt;Mendelson, Elliott [2010] (1964), An Introduction to Mathematical Logic (5th ed.), London: Chapman &amp; Hall&lt;/ref&gt; these authors do not necessarily{{Weasel inline|date=July 2016}} use it in contradistinction to the old sense of ''formula'', which is no longer common in mathematical logic.{{Citation needed|date=July 2016}}

The expression "well-formed formulas" (WFF) also crept into popular culture. ''WFF'' is part of an esoteric pun used in the name of the academic game "[[Academic Games#WFF .27N Proof|WFF 'N PROOF: The Game of Modern Logic]]," by Layman Allen,&lt;ref&gt;Ehrenburg 2002&lt;/ref&gt; developed while he was at [[Yale Law School]] (he was later a professor at the [[University of Michigan]]). The suite of games is designed to teach the principles of symbolic logic to children (in [[Polish notation]]).&lt;ref&gt;More technically, [[Propositional calculus|propositional logic]] using the [[Fitch-style calculus]].&lt;/ref&gt; Its name is an echo of ''[[whiffenpoof]]'', a [[nonsense word]] used as a [[Cheering|cheer]] at [[Yale University]] made popular in ''The Whiffenpoof Song'' and [[The Whiffenpoofs]].&lt;ref&gt;Allen (1965) acknowledges the pun.&lt;/ref&gt;

==See also==
{{Portal|Logic}}
* [[Ground expression]]

==Notes==
{{Reflist}}

==References==
*{{citation
  |first1=Layman E.
  |last1= Allen
  |title=Toward Autotelic Learning of Mathematical Logic by the WFF 'N PROOF Games
  | journal= Mathematical Learning: Report of a Conference Sponsored by the Committee on Intellective Processes Research of the Social Science Research Council
  |series= Monographs of the Society for Research in Child Development
  |volume=30
  |issue=1
  |year=1965
  |pages=29–41
}}
* {{Citation
  | last1=Boolos
  | first1=George
  | author1-link=George Boolos
  | last2=Burgess
  | first2=John
  | last3=Jeffrey
  | first3=Richard
  | author3-link=Richard Jeffrey
  | title=Computability and Logic
  | publisher=[[Cambridge University Press]]
  | edition=4th
  | isbn=978-0-521-00758-0 
  | year=2002}}
* {{cite news
  | first=Rachel
  | last=Ehrenberg
  | title=He's Positively Logical
  | date=Spring 2002
  | publisher=University of Michigan
  | url=http://www.umich.edu/~newsinfo/MT/02/Spr02/mt9s02.html
  | work=Michigan Today
  | accessdate=2007-08-19
}}
* {{Citation
  | last1=Enderton
  | first1=Herbert
  | title=A mathematical introduction to logic
  | publisher=[[Academic Press]]
  | location=Boston, MA
  | edition=2nd
  | isbn=978-0-12-238452-3
  | year=2001
}}
* {{Citation
  | last1=Gamut
  | first1=L.T.F.
  | title=Logic, Language, and Meaning, Volume 1: Introduction to Logic
  |publisher= University Of Chicago Press
  | year= 1990
  | isbn=0-226-28085-3
}}
*{{Citation
 | last=Hodges
 | first=Wilfrid
 | section=Classical Logic I: First-Order Logic
 | editor1-last=Goble
 | editor1-first=Lou
 | title=The Blackwell Guide to Philosophical Logic
 | publisher=Blackwell
 | isbn=978-0-631-20692-7
 | year=2001
}}
*{{Citation
  | last1=Hofstadter
  | first1=Douglas
  | author1-link=Douglas Hofstadter
  | title=Gödel, Escher, Bach: An Eternal Golden Braid
  | publisher=[[Penguin Books]]
  | isbn=978-0-14-005579-5
  | year=1980
}}
* {{Citation
  | last1=Kleene
  | first1=Stephen Cole
  | author1-link=Stephen Kleene
  | title=Mathematical logic
  | origyear=1967
  | publisher=[[Dover Publications]]
  | location=New York
  | isbn=978-0-486-42533-7
  | mr=1950307 
  | year=2002
}}
* {{Citation|last=Rautenberg|first=Wolfgang|authorlink=Wolfgang Rautenberg|doi=10.1007/978-1-4419-1221-3|title=A Concise Introduction to Mathematical Logic|url=http://www.springerlink.com/content/978-1-4419-1220-6/|publisher=[[Springer Science+Business Media]]|location=[[New York City|New York]]|edition=3rd|isbn=978-1-4419-1220-6|year=2010}}

==External links==
*[http://www.cs.odu.edu/~toida/nerzic/content/logic/pred_logic/construction/wff_intro.html Well-Formed Formula for First Order Predicate Logic] - includes a short [[Java (programming language)|Java]] quiz.
*[http://www.apronus.com/provenmath/formulas.htm Well-Formed Formula at ProvenMath]

{{Mathematical logic}}

{{DEFAULTSORT:Well-Formed Formula}}
[[Category:Formal languages]]
[[Category:Metalogic]]
[[Category:Syntax (logic)]]
[[Category:Mathematical logic]]
[[Category:Logical expressions| ]]</text>
      <sha1>eb68r4e35sg724psz9zq7tcqt7a7wy3</sha1>
    </revision>
  </page>
</mediawiki>
