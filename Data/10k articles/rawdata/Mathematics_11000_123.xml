<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>132 (number)</title>
    <ns>0</ns>
    <id>654760</id>
    <revision>
      <id>870093838</id>
      <parentid>870093784</parentid>
      <timestamp>2018-11-22T09:39:41Z</timestamp>
      <contributor>
        <username>Crystallizedcarbon</username>
        <id>22150306</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/223.231.64.178|223.231.64.178]] ([[User talk:223.231.64.178|talk]]) ([[WP:HG|HG]]) (3.1.22)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4556">{{Example farm|date=September 2010}}
{{Infobox number
| number = 132
| divisor = 1, 2, 3, 4, 6, 11, 12, 22, 33, 44, 66, 132
}}
'''132''' ('''one hundred [and] thirty-two''') is the [[natural number]] following [[131 (number)|131]] and preceding [[133 (number)|133]].

==In mathematics==
'''132''' is the sixth [[Catalan number]].&lt;ref&gt;{{Cite web|url=https://oeis.org/A000108|title=Sloane's A000108 : Catalan numbers|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-27}}&lt;/ref&gt; It is a [[pronic number]], the product of 11 and 12. As it has 12 divisors total, 132 is a [[refactorable number]].

If you take the sum of all 2-digit numbers you can make from 132, you get 132: &lt;math&gt;12 + 13 + 21 + 23 + 31 + 32 = 132&lt;/math&gt;. 132 is the smallest number with this property,&lt;ref&gt;Wells, D. ''[[The Penguin Dictionary of Curious and Interesting Numbers]]'' London: Penguin Group. (1987): 138&lt;/ref&gt; which is shared by 264, 396 and 35964 (see [[digit-reassembly number]]).&lt;ref&gt;{{Cite OEIS|A241754|name=Numbers ''n'' equal to the sum of all numbers created from permutations of ''d'' digits sampled from ''n''}}&lt;/ref&gt;


But there is no number that, when added to the sum of its own digits, sums to 132, therefore 132 is a [[self number]]. 132 is also a [[Harshad number]], divisible by the sum of its [[decimal numbers|base-ten]] digits.

==In the military==
* [[132 Battery (The Bengal Rocket Troop) Royal Artillery]] of the [[Royal Artillery]]
* [[AIM-132 ASRAAM|AIM-132 Advanced Short Range Air-to-Air Missile]] is a [[United Kingdom|British]] [[infrared homing]] ("heat seeking") [[air-to-air missile]] in service in the [[Royal Air Force]]
* {{USNS|Mission Santa Clara|T-AO-132}} was a ''Mission Buenaventura''-class [[fleet oiler]]s during [[World War II]]
* {{USS|Aaron Ward|DD-132}} was a [[United States Navy]] {{sclass-|Wickes|destroyer}}
* {{USS|Balduck|APD-132}} was a United States Navy {{sclass-|Crosley|high speed transport}} ship
* {{USS|Barnwell|APA-132}} was a United States Navy {{sclass-|Haskell|attack transport}} during World War II
* {{USS|General J. R. Brooke|AP-132}} was a United States Navy {{sclass-|General G. O. Squier|transport ship}} during World War II
* {{USS|Macon|CA-132}} was a United States Navy {{sclass-|Baltimore|cruiser}} during World War II
* {{USS|Propus|AK-132}} was a United States Navy {{sclass-|Crater|cargo ship}} during World War II
* {{USS|Robert E. Peary|DE-132}} was a United States Navy {{sclass-|Edsall|destroyer escort}} during World War II
* {{USS|S-27|SS-132}} was a United States Navy [[United States S-class submarine|S-class]] [[submarine]]
* [[VFA-132|Strike Fighter Squadron VFA-132]] was a [[United States Navy]] Strike Fighter Squadron once based at [[Naval Air Station Cecil Field]], [[Florida]]

==In transportation==
* The diesel locomotive [[DR Class 132]] was introduced in 1973
* [[London Buses route 132]] is a [[Transport for London]] contracted bus route in [[London]]
* The [[Fiat 132/Argenta]] was a [[large family car]]
* The [[SEAT 132]] was a four-door rear wheel drive [[notchback]] car produced between 1973 and 1982
* The [[BMW 132]] was a nine-cylinder radial aircraft engine introduced in 1933
* [[STS-132]] was a [[Space Shuttle Atlantis]] mission to the [[International Space Station]] that occurred in May 2010.
* The 132 Bus Line, the most used line in [[Buenos Aires]], [[Argentina]]

==In other fields==
'''132''' is also:
* The year [[132|AD 132]] or [[132 BC]]
* 132 AH is a year in the [[Islamic calendar]] that corresponds to 749 &amp;ndash; 750 [[Common Era|CE]]
* [[OGLE-TR-132]] is a [[apparent magnitude|magnitude]] 15.72 [[star]] in the star fields of the [[constellation]] [[Carina (constellation)|Carina]]
* [[132 Aethra]] is a [[M-type asteroid|M-type]] [[main belt]] [[asteroid]]
* [[Sonnet 132]] by [[William Shakespeare]]
* 132 is the fire [[emergency telephone number]] in [[Chile]]
* [[132nd Street (Manhattan)|132 Street]] is a [[thoroughfare]] in [[Harlem]], [[New York City]]
* The number of columns of a [[Line printer]] printing in landscape mode on 14-inch paper.
* Refers to the [[Yo Soy 132]] movement to vote in [[Mexican general election, 2012|2012 Mexican elections]] against [[Institutional Revolutionary Party|PRI]] candidate [[Enrique Peña Nieto]].

==See also==
* [[List of highways numbered 132]]
* [[United Nations Security Council Resolution 132]]
* [[House at 132 Baltic Circle]]

==References==
&lt;references/&gt;

{{Integers|1}}

{{DEFAULTSORT:132 (Number)}}
[[Category:Integers]]</text>
      <sha1>5dxxhfr3ls5ilklm0gzlags2knnl6yx</sha1>
    </revision>
  </page>
  <page>
    <title>8</title>
    <ns>0</ns>
    <id>208174</id>
    <revision>
      <id>871124591</id>
      <parentid>871085113</parentid>
      <timestamp>2018-11-29T02:38:06Z</timestamp>
      <contributor>
        <username>Arthur Rubin</username>
        <id>374195</id>
      </contributor>
      <comment>Reverted [[WP:AGF|good faith]] edits by [[Special:Contributions/JustANinjaOwl|JustANinjaOwl]] ([[User talk:JustANinjaOwl|talk]]): Trivia . ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="33230">{{about|the number|the year|AD 8|other uses|8 (disambiguation)|and|Number Eight (disambiguation)}}
{{redirect|8th|other uses|Eighth (disambiguation)}}
{{in popular culture|date=June 2016}}
{{Infobox number
|number=8
|numeral=[[octal]]
|divisor=1, 2, 4,
|roman unicode=Ⅷ, ⅷ
|greek prefix=[[Wikt:octa-|octa-]]/[[Wikt:oct-|oct-]]
|latin prefix=[[Wikt:octo-|octo-]]/[[Wikt:oct-|oct-]]
|lang1=[[Greek numerals|Greek]]
|lang1 symbol=η (or Η)
|lang2=[[Arabic language|Arabic]] &amp; [[Central Kurdish|Kurdish]]
|lang2 symbol=&lt;span style="font-size:150%;"&gt;٨&lt;/span&gt;
|lang3=[[Urdu]]
|lang3 symbol={{Urdu numeral|8|20}}
|lang4=[[Amharic]]
|lang4 symbol=&lt;span style="font-size:100%;"&gt;፰&lt;/span&gt;
|lang5=[[Bengali language|Bengali]]
|lang5 symbol=&lt;span style="font-size:150%;"&gt;৮&lt;/span&gt;
|lang6=[[Chinese numeral]]
|lang6 symbol=八,捌
|lang7=[[Devanāgarī]]
|lang7 symbol=&lt;span style="font-size:150%;"&gt;८&lt;/span&gt;
|lang8=[[Kannada]]
|lang8 symbol=&lt;span style="font-size:150%;"&gt;೮&lt;/span&gt;
|lang9=[[Telugu language|Telugu]]
|lang9 symbol=&lt;span style="font-size:150%;"&gt;౮&lt;/span&gt;
|lang10=[[Tamil language|Tamil]]
|lang10 symbol=&lt;span style="font-size:150%;"&gt;௮&lt;/span&gt;
|lang11=[[Biblical Hebrew|Hebrew]]
|lang11 symbol=&lt;span style="font-size:150%;"&gt;ח&lt;/span&gt;
|lang13=[[Khmer numerals|Khmer]]
|lang13 symbol=៨
|lang14=[[Korean numerals|Korean]]
|lang14 symbol=팔
|lang15=[[Thai numerals|Thai]]
|lang15 symbol=๘
|lang16=[[Armenian language|Armenian]]
|lang16 symbol=Ը ը}}
'''8''' ('''eight''') is the [[natural number]] following [[7]] and preceding [[9]].

==In mathematics==
8 is:
* a [[composite number]], its [[proper divisor]]s being {{num|1}}, {{num|2}}, and {{num|4}}. It is twice 4 or four times 2.
* a [[power of two]], being 2{{sup|3}} (two cubed), and is the first number of the form {{mvar|p&lt;sup&gt;3&lt;/sup&gt;}}, {{mvar|p}} being an integer greater than 1.
* the first number which is neither [[prime]] nor [[semiprime]].
* the base of the [[octal]] number system, which is mostly used with [[computer]]s. In octal, one digit represents three [[bit]]s. In modern computers, a [[byte]] is a grouping of eight bits, also called an [[wikt:octet|octet]].
* a [[Fibonacci number]], being {{num|3}} plus {{num|5}}. The next Fibonacci number is {{num|13}}. 8 is the only positive Fibonacci number, aside from 1, that is a perfect cube.&lt;ref&gt;Bryan Bunch, ''The Kingdom of Infinite Number''. New York: W. H. Freeman &amp; Company (2000): 88&lt;/ref&gt;
* the only nonzero [[perfect power]] that is one less than another perfect power, by [[Catalan conjecture|Mihăilescu's Theorem]].
* the order of the smallest non-abelian group all of whose subgroups are normal.
* the dimension of the [[octonion]]s and is the highest possible dimension of a [[normed division algebra]].
* the first number to be the aliquot sum of two numbers other than itself; the discrete biprime {{num|10}}, and the square number {{num|49}}.

A number is divisible by 8 if its last three digits, when written in [[decimal]], are also divisible by 8, or its last three digits are 0 when written in [[binary number|binary]].

There are a total of eight convex [[deltahedron|deltahedra]].

A [[polygon]] with eight sides is an [[octagon]]. [[Figurate number]]s representing octagons (including eight) are called [[octagonal number]]s.

A [[polyhedron]] with eight faces is an [[octahedron]]. A [[cuboctahedron]] has as faces six equal squares and eight equal regular triangles.

A [[cube]] has eight [[vertex (geometry)|vertices]].

[[Sphenic number]]s always have exactly eight divisors.

The number 8 is involved with a number of interesting mathematical phenomena related to the notion of [[Bott periodicity theorem|Bott periodicity]]. For example, if ''O''(∞) is the direct limit of the inclusions of real orthogonal groups
:&lt;math&gt;O(1)\hookrightarrow O(2)\hookrightarrow\ldots\hookrightarrow O(k)\hookrightarrow\ldots&lt;/math&gt;,
then
:&lt;math&gt;\pi_{k+8}(O(\infty))\cong\pi_{k}(O(\infty))&lt;/math&gt;.

Clifford algebras also display a periodicity of 8. For example, the algebra ''Cl''(''p'' + 8,''q'') is isomorphic to the algebra of 16 by 16 matrices with entries in ''Cl''(''p'',''q''). We also see a period of 8 in the [[K-theory]] of spheres and in the [[group representation|representation theory]] of the [[special orthogonal group|rotation group]]s, the latter giving rise to the 8 by 8 [[spinor]]ial chessboard. All of these properties are closely related to the properties of the [[octonion]]s.

The [[spin group]] Spin(8) is the unique such group that exhibits the phenomenon of [[triality]].

The lowest-dimensional even [[unimodular lattice]] is the 8-dimensional [[E8 (mathematics)|E&lt;sub&gt;8&lt;/sub&gt;]] lattice. Even positive definite unimodular lattices exist only in dimensions divisible by 8.

A figure 8 is the common name of a [[geometry|geometric]] [[shape]], often used in the context of sports, such as skating. Figure-eight turns of a rope or cable around a cleat, pin, or bitt are used to belay something.

===List of basic calculations===
{|class="wikitable" style="text-align: center; background: white"
|-
! style="width:105px;"|[[Multiplication]]
!1
!2
!3
!4
!5
!6
!7
!8
!9
!10
!11
!12
!13
!14
!15
|-
|'''8 × ''x'''''
|'''8'''
|[[16 (number)|16]]
|[[24 (number)|24]]
|[[32 (number)|32]]
|[[40 (number)|40]]
|[[48 (number)|48]]
|[[56 (number)|56]]
|[[64 (number)|64]]
|[[72 (number)|72]]
|[[80 (number)|80]]
|[[88 (number)|88]]
|[[96 (number)|96]]
|[[104 (number)|104]]
|[[112 (number)|112]]
|[[120 (number)|120]]
|}

{|class="wikitable" style="text-align: center; background: white"
|-
! style="width:105px;"|[[Division (mathematics)|Division]]
!1
!2
!3
!4
!5
!6
!7
!8
!9
!10
! style="width:5px;"|
!11
!12
!13
!14
!15
|-
|'''8 ÷ ''x'''''
|'''8'''
|4
|2.{{overline|6}}
|2
|1.6
|1.{{overline|3}}
|1.{{overline|142857}}
|1
|0.{{overline|8}}
|0.8
!
|0.{{overline|72}}
|0.{{overline|6}}
|0.{{overline|615384}}
|0.{{overline|571428}}
|0.5{{overline|3}}
|-
|'''''x'' ÷ 8'''
|0.125
|0.25
|0.375
|0.5
|0.625
|0.75
|0.875
|1
|1.125
|1.25
!
|1.375
|1.5
|1.625
|1.75
|1.875
|}

{|class="wikitable" style="text-align: center; background: white"
|-
! style="width:105px;"|[[Exponentiation]]
!1
!2
!3
!4
!5
!6
!7
!8
!9
!10
! style="width:5px;"|
!11
!12
!13
|-
|'''8{{sup|''x''}}'''
|'''8'''
|64
|512
|4096
|32768
|262144
|2097152
|16777216
|134217728
|1073741824
!
|8589934592
|68719476736
|549755813888
|-
|'''''x''{{sup|8}}'''
|1
|256
|6561
|65536
|390625
|1679616
|5764801
|16777216
|43046721
|100000000
!
|214358881
|429981696
|815730721
|}

==Etymology==
English ''eight'', from Old English ''eahta, æhta'', [[Proto-Germanic]] ''*ahto''
is a direct continuation of [[Proto-Indo-European numerals|Proto-Indo-European]] ''[[:wikt:Appendix:Proto-Indo-European/oḱtṓw|*oḱtṓ(w)-]]'', and as such cognate with Greek {{lang|grc|ὀκτώ}} and Latin ''octo-'', both of which stems are reflected by the English prefix [[:wikt:oct-|oct(o)-]], as in the ordinal adjective ''octaval'' or ''octavary'', the distributive adjective is ''[[octonary]]''.
The adjective ''octuple'' (Latin ''octu-plus'') may also be used as a noun, meaning "a set of eight items"; the diminutive ''[[octuplet]]'' is mostly used to refer to eight sibling delivered in one birth.

The [[Semitic numerals|Semitic numeral]] is based on a root ''*θmn-'', whence Akkadian ''smn-'', Arabic ''ṯmn-'', Hebrew ''šmn-'' etc.
&lt;!--possibly but not certainly related to Egyptioan ''χmn-''--&gt;
The [[Chinese numeral]], written {{lang|zh|八}} ([[Standard Mandarin|Mandarin]]: ''bā''; [[Cantonese language|Cantonese]]: ''baat''), is from [[Old Chinese]] ''*priāt-'', ultimately from Sino-Tibetan [[:wikt:Appendix:Proto-Sino-Tibetan/b-r-gjat ~ b-g-rjat|''b-r-gyat'' or ''b-g-ryat'']] which also yielded Tibetan ''[[:wikt:བརྒྱད|brgyat]]''.
&lt;!--https://books.google.ch/books?id=nIvqAC7FNBQC&amp;q=eight#v=onepage&amp;q&amp;f=false--&gt;

It has been argued that, as the cardinal number {{num|7}} is the highest number of item that can universally be [[The Magical Number Seven, Plus or Minus Two|cognitively processed]] as a single set, the etymology of the numeral ''eight'' might be the first to be considered composite, either as "twice four" or as "two short of ten", or similar. 
The [[Turkic languages|Turkic]] words for "eight" are from a [[Proto-Turkic]] stem ''*sekiz'', which has been suggested as originating as a negation of ''eki'' "two", as in "without two fingers" (i.e., "two short of ten; two fingers are not being held up");&lt;ref&gt;
''Etymological Dictionary of Turkic Languages: Common Turkic and Interturkic stems starting with letters «L», «M», «N», «P», «S»'', Vostochnaja Literatura RAS,  2003, 241f. ([http://altaica.ru/LIBRARY/e_edtl.htm altaica.ru])&lt;/ref&gt;
this same principle is found in [[Finnic languages|Finnic]] ''[[:wikt:Appendix:Proto-Finnic/kakteksa|*kakte-ksa]]'', which conveys a meaning of "two before (ten)". The Proto-Indo-European reconstruction ''[[:wikt:Appendix:Proto-Indo-European/oḱtṓw|*oḱtṓ(w)-]]'' itself has been argued as representing an old dual, which would correspond to an original meaning of "twice four". 
Proponents of this "quaternary hypothesis" adduce the numeral ''{{num|9}}'', which might be built on the stem ''new-'', meaning "new" (indicating the beginning of a "new set of numerals" after having counted to eight).&lt;ref&gt;the hypothesis is discussed critically (and rejected as "without sufficient support") by 
Werner Winter, 'Some thought about Indo-European numerals' in: Jadranka Gvozdanović (ed.), 
''Indo-European Numerals'', Walter de Gruyter, 1992, 14f.&lt;/ref&gt;

==Glyph==
[[Image:Evo8glyph.svg|thumb|Evolution of the numeral 8 from the Indians to the Europeans]]
The modern 8 glyph, like all modern [[Arabic numerals]] (other than zero) originates with the [[Brahmi numerals]]. 
The Brahmi numeral for ''eight'' by the 1st&amp;nbsp;century was written in one stroke as a curve └┐ looking like an uppercase H with the bottom half of the left line and the upper half of the right line removed.
However the ''eight'' glyph used in India in the early centuries of the Common Era developed considerable variation, and in some cases took the shape of a single wedge, which was adopted into the Perso-Arabic tradition as [[:wikt:٨|٨]] (and also gave rise to the later Devanagari numeral [[:wikt:८|८]]; the alternative curved glyph also existed as a variant in Perso-Arabic tradition, where it came to look similar to our glyph ''5''.{{year needed|date=October 2014}}

The numerals as used in [[Al-Andalus]] by the 10th century were a distinctive western variant of the glyphs used in the Arabic-speaking world, known as ''ghubār'' numerals (''ghubār'' translating to "[[sand table]]"). In these numerals, the line of the ''5''-like glyph used in Indian manuscripts for eight came to be formed in ghubār as a closed loop, which was the ''8''-shape that became adopted into European use in the 10th&amp;nbsp;century.&lt;ref&gt;Georges Ifrah, ''The Universal History of Numbers: From Prehistory to the Invention of the Computer'' transl. David Bellos et al. London: The Harvill Press (1998): 395, Fig. 24.68.&lt;/ref&gt;

Just as in most modern [[typeface]]s, in typefaces with [[text figures]] the 8 character usually has an [[ascender (typography)|ascender]], as, for example, in [[Image:TextFigs148.svg]].

The [[infinity symbol]] ∞, described as a "sideways figure eight" is unrelated to the ''8'' glyph in origin; it is first used (in the mathematical meaning "infinity") in the 17th&amp;nbsp;century, and it may be derived from the [[Roman numeral]] for "one thousand" CIƆ, or alternatively from the final Greek letter, [[ω]].

The numeral eight in [[Greek numerals]], developed in [[Classical Greece]] by the 5th century BC, was written as [[Η]], the eighth letter of the Greek alphabet.

The [[Chinese numeral]] eight is written in two strokes, {{lang|zh|[[:wikt:八|八]]}}; the glyph is also the [[Radical 12|12th]] [[Kangxi radical]].

==In science==
===Physics===
* In nuclear physics, the second [[Magic number (physics)|magic number]].
* In [[particle physics]], the [[eightfold Way (physics)|eightfold way]] is used to classify sub-atomic particles.
* In [[statistical mechanics]], the [[eight-vertex model]] has 8 possible configurations of arrows at each vertex.

===Astronomy===
* [[Messier object]] [[Lagoon Nebula|M8]], a magnitude 5.0 [[nebula]] in the [[constellation]] of [[Sagittarius (constellation)|Sagittarius]].
* The [[New General Catalogue]] [http://www.ngcic.org/ object] [[NGC 8]], a double star in the constellation [[Pegasus (constellation)|Pegasus]].
* Since the demotion of [[Pluto]] to a [[dwarf planet]] on August 24, 2006, in our [[Solar System]], eight of the bodies orbiting the Sun are considered to be [[planet]]s.

===Chemistry===
* The [[atomic number]] of [[oxygen]].
* The number of [[allotropes of carbon]].
* The most stable allotrope of a [[sulfur]] molecule is made of eight sulfur atoms arranged in a rhombic form.
* The maximum number of electrons that can occupy a [[electron shell#Valence shells|valence shell]].
* The red pigment [[lycopene]] consists of eight [[isoprene]] units.

===Geology===
* A [[disphenoid]] crystal is bounded by eight scalene triangles arranged in pairs. A ditetragonal prism in the [[tetragonal crystal system]] has eight similar faces whose alternate interfacial angles only are equal.

===Biology===
* All [[spider]]s, and more generally all [[arachnid]]s, have eight legs. [[Orb-weaver spider]]s of the cosmopolitan family Areneidae have eight similar eyes.
* The [[octopus]] and its cephalopod relatives in genus [[Argonaut (animal)|Argonauta]] have eight arms (tentacles).
* Compound coelenterates of the subclass or order [[octocorallia|Alcyonaria]] have polyps with eight-branched tentacles and eight septa.
* Sea anemones of genus ''[[Edwardsia]]'' have eight [[Mesentery#Invertebrate anatomy|mesenteries]].
* Animals of phylum [[Ctenophora]] swim by means of eight meridional bands of transverse ciliated plates, each plate representing a row of large modified cilia.
* The [[Alypia octomaculata|eight-spotted forester]] (genus ''Alypia'', family [[Zygaenidae]]) is a diurnal moth having black wings with brilliant white spots.
* The [[ascus]] in fungi of the class [[Ascomycota#Ascomycetes versus Ascomycetes|Ascomycetes]], following nuclear fusion, bears within it typically eight ascospores.
* Herbs of genus ''[[Coreopsis]]'' (tickseed) have showy flower heads with involucral bracts in two distinct series of eight each.
* [[Timothy Leary]] identified a [[8-Circuit Model of Consciousness|hierarchy of eight levels of consciousness]].
* In human [[adult dentition]] there are eight teeth in each quadrant. The eighth tooth is the so-called [[wisdom tooth]].
* There are eight [[cervical nerves]] on each side in man and most mammals.

==In technology==
[[Image:ICS Eight.svg|right|thumb|100px|[[Naval flag signalling#Numerals|NATO signal flag]] for 8]]
* A [[byte]] is eight [[bit]]s.
* Many (mostly historic) computer architectures are eight-bit, among them the [[Nintendo Entertainment System]].
* [[Standard 8 mm film|Standard-8]] and [[Super 8 mm film|Super-8]] are 8&amp;nbsp;mm [[List of film formats|film formats]].
* Video8, Hi8 and Digital8 are related [[8 mm video format]]s.
* On most phones, the 8 key is associated with the letters [[T]], [[U]], and [[V]], but on the [[BlackBerry]] it is the key for [[B]], [[N]], and [[X]].
* An eight may refer to an eight-cylinder engine or automobile. A [[V8 engine]] is an [[internal combustion engine]] with eight cylinders configured in two banks (rows) of four forming a "V" when seen from the end.
* A [[figure-eight knot]] (so named for its configuration) is a kind of [[stopper knot]].
* The number eight written in parentheses is the code for the musical note in [[Windows Live Messenger]].
* In a [[seven-segment display]], when an 8 is illuminated, all the display bulbs are on.

===In measurement===
* The [[SI prefix]] for 1000&lt;sup&gt;8&lt;/sup&gt; is yotta (Y), and for its reciprocal, yocto (y).
* In liquid measurement ([[United States customary units]]), there are eight [[fluid ounce]]s in a [[Cup (volume)|cup]], eight [[pint]]s in a [[gallon]] and eight [[tablespoon]]fuls in a [[gill (volume)|gill]].
* There are eight [[furlong]]s in a mile.
* The clove, an old [[English units#Avoirdupois|English unit]] of weight, was equal to eight pounds when measuring cheese.
* An eight may be an article of clothing of the eighth [[clothing sizes|size]].
* Force eight is the first [[wind]] strength attributed to a [[gale]] on the [[Beaufort scale]] when announced on a [[Shipping Forecast]].

==In culture==
===Architecture===
* Various types of buildings are usually eight-sided (octagonal), such as single-roomed [[gazebo]]s and multi-roomed [[pagoda]]s (descended from stupas; see religion section below).
* Eight [[Glossary of architecture#C|caulicoles]] rise out of the leafage in a [[Corinthian order|Corinthian capital]], ending in leaves that support the [[volutes]].

===In religion, folk belief and divination===
====Hinduism====
* Also known as [[Astha]] in [[Sanskrit]],it is the number of wealth and abundance. 
* The Goddess of wealth and prosperity [[Lakshmi]] has eight forms which is known as [[Ashta Lakshmi]] and worshipped as: &lt;br&gt;"''Maha-lakshmi,Dhana-lakshmi,Dhanya-lakshmi,Gaja-lakshmi,&lt;br&gt;Santana-lakshmi,Veera-lakshmi,Vijaya-lakshmi and Vidhya-lakshmi''" 
*There are eight ''nidhi'', or seats of wealth according to [[Hinduism]]. 
*There are eight [[Guardians of the directions]] known as ''Astha-dikpalas''.
*There are eight [[Hindu]] monasteries established by saint [[Madhvacharya]] in [[Udupi]],[[India]] popularly known as the ''[[Ashta Mathas of Udupi]]''

====Buddhism====
[[Image:Dharma Wheel.svg|thumb|right|In Buddhism, the 8-spoked [[Dharmacakra]] represents the [[Noble Eightfold Path]]]]
* The [[Dharmacakra]], a [[Buddhism|Buddhist]] symbol, has eight spokes. The Buddha's principal teaching—the [[Four Noble Truths]]—ramifies as the [[Noble Eightfold Path]] and the Buddha emphasizes the importance of the eight attainments or jhanas.
* In [[Mahayana]] Buddhism, the branches of the Eightfold Path are embodied by the Eight Great Bodhisattvas: ([[Manjusri]], [[Vajrapani]], [[Avalokiteśvara]], [[Maitreya]], [[Ksitigarbha]], Nivaranavishkambhi, [[Akasagarbha]], and [[Samantabhadra]]). These are later (controversially) associated with the Eight Consciousnesses according to the [[Yogacara]] school of thought: consciousness in the five senses, thought-consciousness, self-consciousness, and unconsciousness-"consciousness" or "store-house consciousness" (alaya-vijñana). The "irreversible" state of enlightenment, at which point a Bodhisattva goes on "autopilot", is the Eight Ground or ''bhūmi''. In general, "eight" seems to be an auspicious number for Buddhists, e.g., the "eight auspicious symbols" (the jewel-encrusted parasol; the goldfish (always shown as a pair, e.g., the glyph of Pisces); the self-replenishing amphora; the white ''kamala'' lotus-flower; the white conch; the eternal (Celtic-style, infinitely looping) knot; the banner of imperial victory; the eight-spoked wheel that guides the ship of state, or that symbolizes the Buddha's teaching). Similarly, [[Buddha's Birthday|Buddha's birthday]] falls on the 8th day of the 4th month of the [[Chinese calendar]].

====Judaism====
* The religious rite of [[brit milah]] (commonly known as [[circumcision]]) is held on a baby boy's eighth day of life.
* [[Hanukkah]] is an eight-day Jewish holiday that starts on the 25th day of [[Kislev]].
* [[Shemini Atzeret]] ([[Hebrew language|Hebrew]]: "Eighth Day of Assembly") is a one-day Jewish holiday immediately following the seven-day holiday of [[Sukkot]].

====Christianity====
* The spiritual [[The eighth day (Christian)|Eighth Day]], because the number 7 refers to the days of the week (which repeat themselves).
* The number of [[Beatitudes]].
* [[wikisource:Bible (King James)/1 Peter#3:20|1 Peter 3:20]] states that there were eight people on [[Noah's Ark]].
* The [[Antichrist]] is the eighth king in the [[Book of Revelation]].&lt;ref&gt;[https://books.google.com/books?id=sMxBttwNnEMC "Life Application New Testament Commentary"], Bruce B. Barton. Tyndale House Publishers, Inc., 2001. {{ISBN|0-8423-7066-8}}, {{ISBN|978-0-8423-7066-0}}. p. 1257&lt;/ref&gt;

====Islam====
* In [[Islam]], eight is the number of [[angel]]s carrying the [[Throne of God|throne]] of [[Allah]] in heaven.
* The number of gates of heaven.

====Taoism====
* [[Ba Gua]]
* [[Ba Xian]]
* [[baduanjin|Ba Duan Jin]]

====Other====
* The [[Eight Immortals]] are Chinese demigods.
* In [[Wicca]], there are eight Sabbats, festivals, seasons, or spokes in the [[Wheel of the Year]].
* In [[Taoism]] and Chinese cosmology, the eight trigrams of the [[Bagua]]. "Bagua" literally means "eight symbols".
* In [[Ancient Egyptian mythology]], the [[Ogdoad (Egyptian)|Ogdoad]] represents the [[Ancient Egyptian creation myths|eight primordial deities of creation]].
* In [[Scientology]] there are eight dynamics of existence

====As a lucky number====&lt;!--This section is linked from [[2008 Summer Olympics]]--&gt;
* The number eight is considered to be a [[Numbers in Chinese culture|lucky number in Chinese]] and other Asian cultures.&lt;ref&gt;{{cite journal |last=Ang |first=Swee Hoon |title=Chinese consumers' perception of alpha-numeric brand names |journal=Journal of Consumer Marketing |year=1997 |volume=14 |issue=3 |pages=220–233 |url=http://www.emeraldinsight.com/journals.htm?articleid=856257&amp;show=abstract |doi=10.1108/07363769710166800}}&lt;/ref&gt; Eight ({{lang|zh-Hani|八}}; [[Chinese numerals#Numeral characters|accounting]] {{lang|zh-Hani|捌}}; [[pinyin]] ''bā'') is considered a [[Numbers in Chinese culture#Eight|lucky number in Chinese culture]] because it sounds like the word meaning to generate wealth ({{lang|zh-Hant|發(T) 发(S)}}; [[Pinyin]]: ''fā''). Property with the number 8 may be valued greatly by Chinese. For example, a Hong Kong [[Vehicle registration plate|number plate]] with the number 8 was sold for $640,000.&lt;ref&gt;{{Cite journal |url=http://www.umac.edu.mo/fba/irer/papers/past/vol2_pdf/079-093LN-NZ.pdf |journal=International Real Estate Review |year=1999 |volume=2 |pages=79–93 |title=Hedonic Prices and House Numbers: The Influence of Feng Shui |author1=Steven C. Bourassa |author2=Vincent S. Peng |issue=1 |postscript= |deadurl=yes |archiveurl=https://www.webcitation.org/6XlGD7PlK?url=http://www.umac.edu.mo/fba/irer/papers/past/vol2_pdf/079-093LN-NZ.pdf |archivedate=13 April 2015 |df=dmy-all }}&lt;/ref&gt; The opening ceremony of the [[2008 Summer Olympics|Summer Olympics in Beijing]] started at 8 seconds and 8 minutes past 8 pm (local time) on 8 August 2008.&lt;ref name="game"&gt;[https://www.theguardian.com/sport/2008/aug/09/olympics2008.openingceremony "Patriot games: China makes its point with greatest show"] by Richard Williams, ''[[The Guardian]]'', published 9 August 2008&lt;/ref&gt;
* {{Nihongo|Eight|八|hachi, ya}} is also considered a lucky number in [[Japan]], but the reason is different from that in Chinese culture. Eight gives an idea of growing prosperous, because the letter ({{nihongo2|八}}) broadens gradually.
* The Japanese thought of {{Nihongo|eight|や|ya}} as a holy number in the ancient times. The reason is less well-understood, but it is thought that it is related to the fact they used eight to express large numbers vaguely such as {{Nihongo|manyfold|やえはたえ|Yae Hatae}} (literally, eightfold and twentyfold), {{Nihongo|many clouds|やくも|Yakumo}} (literally, eight clouds), {{Nihongo|millions and millions of Gods|やおよろずのかみ|Yaoyorozu no Kami}} (literally, eight millions of Gods), etc. It is also guessed that the ancient Japanese gave importance to pairs, so some researchers guess twice as {{Nihongo|four|よ|yo}}, which is also guessed to be a holy number in those times because it indicates the world (north, south, east, and west) might be considered a very holy number.
* In [[numerology]], 8 is the number of building, and in some theories, also the number of destruction.

====In astrology====
* In [[astrology]], [[Scorpius|Scorpio]] is the 8th [[astrological sign]] of the [[Zodiac]].
* In the [[Middle Ages]], 8 was the number of "unmoving" stars in the sky, and symbolized the perfection of incoming planetary energy.

===In music and dance===
* A note played for one-eighth the duration of a whole note is called an [[eighth note]], or quaver.
* An [[octave]], the interval between two [[musical note]]s with the same letter name (where one has double the frequency of the other), is so called because there are eight notes between the two on a standard major or minor [[diatonic scale]], including the notes themselves and without chromatic deviation. The ecclesiastical [[musical mode|mode]]s are ascending diatonic musical scales of eight notes or tones comprising an octave.
* There are eight notes in the [[octatonic scale]].
* There are eight musicians in a double quartet or an [[octet (music)|octet]]. Both terms may also refer to a musical composition for eight voices or instruments.
* Caledonians is a square dance for eight, resembling the [[quadrille]].
* [[Album]]s with the number eight in their title include ''8'' by the Swedish band [[Arvingarna]], ''[[8 (Incubus album)|8]]'' by the American rock band [[Incubus (band)|Incubus]] and ''[[The Meaning of 8]]'' by Minnesota indie rock band [[Cloud Cult]].
* [[Dream Theater]]'s eighth album ''[[Octavarium (album)|Octavarium]]'' contains many different references to the number 8, including the number of songs and various aspects of the music and cover artwork.
* "Eight maids a-milking" is the gift on the eighth day of Christmas in the carol "[[The Twelve Days of Christmas (song)|The Twelve Days of Christmas]]".
* The [[8-track tape|8-track cartridge]] is a musical recording format.
* "#8" is the stage name of [[Slipknot (band)|Slipknot]] vocalist [[Corey Taylor]].
* "Too Many Eights" is a song by Athens, Georgia's [[Supercluster (band)|Supercluster]].
* [[Eight Seconds]], a Canadian musical group popular in the 1980s with their most notable song "Kiss You (When It's Dangerous)".
* "[[Eight Days a Week (song)|Eight Days a Week]]" is a #1 single for the music group [[The Beatles]].
* [[Figure 8 (album)|''Figure 8'']] is the fifth studio album by singer-songwriter [[Elliott Smith]], released in the year 2000.
* Ming Hao from the k-pop group [[Seventeen (band)#Performance Unit|Seventeen]] goes by the name "The8".
* "8 (circle)" is the eighth song on the album [[22, A Million]] by the American band [[Bon Iver]].

===In film and television===
* ''[[8 Guys]]'' is a 2003 short film written and directed by [[Dane Cook]].
* ''[[8 Man]]'' (or ''[[Eightman]]''): 1963 Japanese manga and anime superhero.
* [[8 Mile (film)|''8 Mile'']] is a 2002 film directed by [[Curtis Hanson]].
* [[8mm (film)|''8mm'']] is a 1999 film directed by [[Joel Schumacher]].
* ''[[8 Women]]'' (Original French title: 8 femmes) is a 2002 film directed by [[François Ozon]].
* ''[[Eight Below]]'' is a 2006 film directed by [[Frank Marshall (film producer)|Frank Marshall]].
* ''[[Eight Legged Freaks]]'' is a 2002 film directed by [[Ellory Elkayem]].
* ''[[Eight Men Out]]'' is a 1988 film directed by [[John Sayles]].
* ''[[Jennifer Eight]]'', also known as ''Jennifer 8'', is a 1992 film written and directed by [[Bruce Robinson]].
* ''[[Eight Is Enough]]'' is an American television comedy-drama series.
* In ''[[Stargate SG-1]]'' and ''[[Stargate Atlantis]]'', dialing an 8-chevron address will open a [[wormhole]] to another galaxy.
* ''[[The Hateful Eight]]'' is a 2015 American western mystery film written and directed by [[Quentin Tarantino]].
* ''[[Kate Plus 8]]'' is an American reality television show.
* [[The Weather Channel]]'s segment ''[[Local on the 8s]]'' features daily and weekly forecasts for regions and cities all over the United States.
* ''[[SBS 8 News]]'' is a South Korean primetime news program broadcast on SBS.

===In sports and other games===
[[File:8-Ball.jpg|thumb|An 8-ball in billiards]]
* [[Eight-ball]] [[pocket billiards]] is played with a cue ball and 15 numbered balls, the black ball numbered 8 being the middle and most important one, as the winner is the player or side that legally pockets it after first pocketing its numerical group of 7 object balls (for other meanings see ''[[Eight ball (disambiguation)]]'').
* Balklines divide a billiards table into eight outside compartments or divisions called balks. In [[Balkline and straight rail#The ascendancy of balkline|balkline billiards]] the table also has eight anchor spaces.
* In [[chess]], each side has eight pawns and the board is made of 64 squares arranged in an eight by eight lattice. The [[eight queens puzzle]] is a challenge to arrange eight queens on the board so that none can capture any of the others.
* In the game of eights or [[Crazy Eights]], each successive player must play a card either of the same suit or of the same rank as that played by the preceding player, or may play an eight and call for any suit. The object is to get rid of all one's cards first.
* In [[association football]], the number 8 has historically been the number of the Central Midfielder.
* in [[baseball]] scorekeeping, the [[center fielder]] is designated as number 8.
* In [[rugby union]], the only position without a proper name is the [[Number eight (rugby union)|Number 8]], a forward position.
* In most [[rugby league]] competitions (though not the European [[Super League]], which uses static squad numbering), one of the two starting props wears the number 8.
* In [[Rowing (sport)|rowing]], an "eight" refers to a sweep-oar racing boat with a crew of eight rowers plus a [[coxswain]].
* In the [[2008 Summer Olympics|2008 Games of the XXIX Olympiad]], the official opening was on 08/08/08 at 8:08:08&amp;nbsp;p.m. local time in [[Beijing, China]].
* In the [[The Stanley Parable|Stanley Parable Demonstration]], there is an eight button, that, when pressed, says the word eight.
* In the racing video game ''[[Mario Kart 8]]'', there is an item called the Crazy Eight.

===In foods===
* [[Nestlé]] sells a brand of chocolates filled with peppermint-flavoured cream called ''[[After Eight]]'', referring to the time 8 p.m.
* There are eight vegetables in [[V8 (beverage)|V8]] juice.
* In cooking recipes, there are approximately 8 pinches to a [[teaspoon]].

===In literature===
* Eights may refer to [[octosyllable|octosyllabic]], usually [[Choliamb|iambic]], [[Meter (poetry)|lines]] of verse.
* The drott-kvaett, an [[Old Icelandic]] verse, consisted of a stanza of eight regular lines.
* In [[Terry Pratchett]]'s ''[[Discworld]]'' series, eight is a magical number and is considered taboo. Eight is not safe to be said by wizards on the [[Discworld (world)|Discworld]] and is the number of [[Discworld gods#Bel-Shamharoth|Bel-Shamharoth]]. Also, there are eight days in a Disc week and eight colours in a Disc spectrum, the eighth one being [[octarine]].
* [[Lewis Carroll]]'s poem [[The Hunting of the Snark]] has 8 "fits" (cantos), which is noted in the full name "The Hunting of the Snark – ''An Agony, in Eight Fits''
* Eight apparitions appear to [[Macbeth (character)|Macbeth]] in Act 4 scene 1 of Shakespeare's [[Macbeth]] as representations of the eight descendants of [[Banquo]].

===In slang===
* An "eighth" is a common measurement of [[cannabis (drug)|marijuana]], meaning an eighth of an [[ounce]]. It is also a common unit of sale for [[psilocybin mushrooms]]. Also, an eighth of an ounce of [[cocaine]] is commonly referred to as an "8-ball."&lt;ref&gt;{{cite web |url=http://www.thegooddrugsguide.com/cocaine/faq.htm |title=Cocaine - Frequently Asked Questions |website=thegooddrugsguide.com}}&lt;/ref&gt;
* The numeral "8" is sometimes used in informal writing and [[Internet slang]] to represent the syllable "ate", as in writing "H8" for "hate", or "congratul8ions" for "congratulations". [[Avril Lavigne]]'s song "[[Sk8er Boi]]" uses this convention in the title.
* "Section 8" is common U.S. slang for "crazy", based on the [[United States armed forces|U.S. military's]] [[Section 8 (military)|Section 8]] discharge for [[Mental disorder|mentally unfit]] personnel.
* The [[Section 8 (housing)|Housing Choice Voucher Program]], operated by the [[United States Department of Housing and Urban Development]], is commonly referred to as the Section 8 program, as this was the original section of the Act which instituted the program.
* In [[Colombia]] and [[Venezuela]], "volverse un ocho" (meaning to tie oneself in a figure 8) refers to getting in trouble or contradicting oneself.
* In China, "8" is used in chat speak as a term for parting. This is due to the closeness in pronunciation of "8" (bā) and the English word "bye".

==See also==
* [[The Magical Number Seven, Plus or Minus Two]]

==References==
{{Reflist}}

==External links==
* [https://web.archive.org/web/20090421054044/http://math.ucr.edu/home/baez/octonions/octonions.html The Octonions], John C. Baez

{{Integers|zero}}

{{Use dmy dates|date=June 2013}}

{{DEFAULTSORT:8 (Number)}}
[[Category:Integers]]
[[Category:8 (number)]]</text>
      <sha1>mlzonf8ptgsoeqvd03ftxgy39i6jhkh</sha1>
    </revision>
  </page>
  <page>
    <title>Algebraic interior</title>
    <ns>0</ns>
    <id>30823252</id>
    <revision>
      <id>808066789</id>
      <parentid>753095520</parentid>
      <timestamp>2017-10-31T17:49:15Z</timestamp>
      <contributor>
        <username>Tom.Reding</username>
        <id>9784415</id>
      </contributor>
      <minor/>
      <comment>/* Relation to interior */WL 1 first-publisher; [[WP:GenFixes]] on; using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4420">In [[functional analysis]], a branch of mathematics, the '''algebraic interior''' or '''radial kernel''' of a subset of a [[vector space]] is a refinement of the concept of the [[Interior (topology)|interior]].  It is the subset of points contained in a given set with respect to which it is [[absorbing set|absorbing]], i.e. the [[radial set|radial]] points of the set.&lt;ref name="coherent"&gt;{{cite journal|title=Coherent Risk Measures, Valuation Bounds, and (&lt;math&gt;\mu,\rho&lt;/math&gt;)-Portfolio Optimization|first1=Stefan|last1=Jaschke|first2=Uwe|last2=Kuchler|date=2000}}&lt;/ref&gt;  The elements of the algebraic interior are often referred to as '''internal points'''.&lt;ref name="aliprantis+border"&gt;{{cite book|last=Aliprantis|first=C.D.|last2=Border|first2=K.C.|title=Infinite Dimensional Analysis: A Hitchhiker's Guide|edition=3rd|publisher=Springer|date=2007|isbn=978-3-540-32696-0|doi=10.1007/3-540-29587-9|pages=199–200}}&lt;/ref&gt;&lt;ref name="cook"&gt;{{cite web|url=http://www.johndcook.com/SeparationOfConvexSets.pdf | accessdate=November 14, 2012 |format=pdf |title=Separation of Convex Sets in Linear Topological Spaces |author=John Cook |date=May 21, 1988}}&lt;/ref&gt;

Formally, if &lt;math&gt;X&lt;/math&gt; is a [[linear space]] then the algebraic interior of &lt;math&gt;A \subseteq X&lt;/math&gt; is
: &lt;math&gt;\operatorname{core}(A) := \left\{x_0 \in A: \forall x \in X, \exists t_x &gt; 0, \forall t \in [0,t_x], x_0 + tx \in A\right\}.&lt;/math&gt;&lt;ref&gt;{{cite book|author=Nikolaĭ Kapitonovich Nikolʹskiĭ|title=Functional analysis I: linear functional analysis|date=1992|publisher=Springer|isbn=978-3-540-50584-6}}&lt;/ref&gt;

Note that in general &lt;math&gt;\operatorname{core}(A) \neq \operatorname{core}(\operatorname{core}(A))&lt;/math&gt;, but if &lt;math&gt;A&lt;/math&gt; is a [[convex set]] then &lt;math&gt;\operatorname{core}(A) = \operatorname{core}(\operatorname{core}(A))&lt;/math&gt;.  If &lt;math&gt;A&lt;/math&gt; is a convex set then if &lt;math&gt;x_0 \in \operatorname{core}(A), y \in A, 0 &lt; \lambda \leq 1&lt;/math&gt; then &lt;math&gt;\lambda x_0 + (1 - \lambda)y \in \operatorname{core}(A)&lt;/math&gt;.

== Example ==
If &lt;math&gt;A = \{x \in \mathbb{R}^2: x_2 \geq x_1^2 \text{ or } x_2 \leq 0\} \subseteq \mathbb{R}^2&lt;/math&gt; then &lt;math&gt;0 \in \operatorname{core}(A)&lt;/math&gt;, but &lt;math&gt;0 \not\in \operatorname{int}(A)&lt;/math&gt; and &lt;math&gt;0 \not\in \operatorname{core}(\operatorname{core}(A))&lt;/math&gt;.

== Properties ==
Let &lt;math&gt;A,B \subset X&lt;/math&gt; then:
* &lt;math&gt;A&lt;/math&gt; is [[Absorbing set|absorbing]] if and only if &lt;math&gt;0 \in \operatorname{core}(A)&lt;/math&gt;.&lt;ref name="coherent" /&gt;
* &lt;math&gt;A + \operatorname{core}B \subset \operatorname{core}(A + B)&lt;/math&gt;&lt;ref name="Zalinescu"&gt;{{cite book|last=Zălinescu|first=C.|title=Convex analysis in general vector spaces|publisher=World Scientific Publishing  Co., Inc|location= River Edge, NJ |date= 2002|pages=2–3|isbn=981-238-067-1|mr=1921556}}&lt;/ref&gt;
* &lt;math&gt;A + \operatorname{core}B = \operatorname{core}(A + B)&lt;/math&gt; if &lt;math&gt;B = \operatorname{core}B&lt;/math&gt;&lt;ref name="Zalinescu" /&gt;

=== Relation to interior ===
Let &lt;math&gt;X&lt;/math&gt; be a [[topological vector space]], &lt;math&gt;\operatorname{int}&lt;/math&gt; denote the interior operator, and &lt;math&gt;A \subset X&lt;/math&gt; then: 
* &lt;math&gt;\operatorname{int}A \subseteq \operatorname{core}A&lt;/math&gt;
* If &lt;math&gt;A&lt;/math&gt; is nonempty convex and &lt;math&gt;X&lt;/math&gt; is finite-dimensional, then &lt;math&gt;\operatorname{int}A = \operatorname{core}A&lt;/math&gt;&lt;ref name="aliprantis+border" /&gt;
* If &lt;math&gt;A&lt;/math&gt; is convex with non-empty interior, then &lt;math&gt;\operatorname{int}A = \operatorname{core}A&lt;/math&gt;&lt;ref name="kantorovitz"&gt;{{cite book|title=Introduction to Modern Analysis |author=Shmuel Kantorovitz |publisher=[[Oxford University Press]] |date=2003 |isbn=9780198526568 |page=134}}&lt;/ref&gt;
* If &lt;math&gt;A&lt;/math&gt; is a closed convex set and &lt;math&gt;X&lt;/math&gt; is a [[complete metric space]], then &lt;math&gt;\operatorname{int}A = \operatorname{core}A&lt;/math&gt;&lt;ref&gt;{{citation|title=Perturbation Analysis of Optimization Problems|series=Springer series in operations research|first1=J. Frederic|last1=Bonnans|first2=Alexander|last2=Shapiro|publisher=Springer|date=2000|isbn=9780387987057|at=Remark 2.73, p.&amp;nbsp;56|url=https://books.google.com/books?id=ET70F9HgIpIC&amp;pg=PA56}}.&lt;/ref&gt;

== See also ==
* [[Interior (topology)]]
* [[Relative interior]]
* [[Quasi-relative interior]]
* [[Order unit]]
* [[Bounding point]]

== References ==
{{Reflist}}

{{Functional Analysis}}

[[Category:Topology]]
[[Category:Mathematical analysis]]
[[Category:Functional analysis]]</text>
      <sha1>oy6ibfs5043wr5chhkkapmtx9a6f0b2</sha1>
    </revision>
  </page>
  <page>
    <title>Automatic semigroup</title>
    <ns>0</ns>
    <id>17655204</id>
    <revision>
      <id>847354156</id>
      <parentid>847353968</parentid>
      <timestamp>2018-06-24T18:49:01Z</timestamp>
      <contributor>
        <username>Pintoch</username>
        <id>16990030</id>
      </contributor>
      <comment>/* References */ add pdf</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5884">In [[mathematics]], an '''automatic semigroup''' is a finitely generated [[semigroup]] equipped with several [[regular languages]] over an alphabet representing a generating set. One of these languages determines "canonical forms" for the elements of the semigroup, the other languages determine if two canonical forms represent elements that differ by multiplication by a generator.

Formally, let &lt;math&gt;S&lt;/math&gt; be a semigroup and &lt;math&gt;A&lt;/math&gt; be a finite set of generators.  Then an ''automatic structure'' for &lt;math&gt;S&lt;/math&gt; with respect to &lt;math&gt;A&lt;/math&gt; consists of a regular language &lt;math&gt;L&lt;/math&gt; over &lt;math&gt;A&lt;/math&gt; such that every element of &lt;math&gt;S&lt;/math&gt; has at least one representative in &lt;math&gt;L&lt;/math&gt; and such that for each &lt;math&gt;a \in A \cup \{\varepsilon\}&lt;/math&gt;, the relation consisting of pairs &lt;math&gt;(u,v)&lt;/math&gt; with &lt;math&gt;ua = v&lt;/math&gt; is regular.

The concept of an automatic semigroup was generalized from [[automatic group]]s by Campbell et al. (2001)

Unlike automatic groups (see Epstein et al. 1992), a semigroup may have an automatic structure with respect to one generating set, but not with respect to another. However, if an automatic semigroup has an identity, then it has an automatic structure with respect to any generating set (Duncan et al. 1999).

==Decision problems==

Like automatic groups, automatic semigroups have [[word problem for groups|word problem]] solvable in quadratic time. Kambites &amp; Otto (2006) showed that it is undecidable whether an element of an automatic monoid possesses a right inverse.

Cain (2006) proved that both cancellativity and left-cancellativity are undecidable for automatic semigroups. On the other hand, right-cancellativity is decidable for automatic semigroups (Silva &amp; Steinberg 2004).

==Geometric characterization==

Automatic structures for groups have an elegant geometric characterization called the ''fellow traveller property'' (Epstein et al. 1992, ch. 2). Automatic structures for semigroups ''possess'' the fellow traveller property but are not in general characterized by it (Campbell et al. 2001). However, the characterization can be generalized to certain '[[Group (mathematics)|group]]-like' classes of semigroups, notably [[completely simple semigroup]]s (Campbell et al. 2002) and group-embeddable semigroups (Cain et al. 2006).

==Examples of automatic semigroups==
*[[Bicyclic monoid]]
*Finitely generated subsemigroups of a [[free semigroup]]

== References ==

*{{citation
 | last1 = Cain | first1 = Alan J.
 | title = Cancellativity is undecidable for automatic semigroups
 | journal = Quarterly Journal of Mathematics
 | volume = 57 | year = 2006 | issue = 3 | pages = 285–295
 | doi = 10.1093/qmath/hai023| citeseerx = 10.1.1.106.6068}}

*{{citation
 | last1 = Cain | first1 = Alan J.
 | last2 = Robertson | first2 = Edmund F.
 | last3 = Ruskuc | first3 = Nik
 | title = Subsemigroups of groups: presentations, Malcev presentations, and automatic structures
 | journal = Journal of Group Theory
 | volume = 9 | year = 2006 | issue = 3 | pages = 397–426
 | doi = 10.1515/JGT.2006.027}}.

*{{citation
 | last1 = Campbell | first1 = Colin M.
 | last2 = Robertson | first2 = Edmund F.
 | last3 = Ruskuc | first3 = Nik
 | last4 = Thomas | first4 = Richard M.
 | title = Automatic semigroups
 | journal = Theoretical Computer Science
 | volume = 250 | year = 2001 | issue = 1–2 | pages = 365–391
 | url = https://core.ac.uk/download/pdf/82399070.pdf
 | doi = 10.1016/S0304-3975(99)00151-6}}.

*{{citation
 | last1 = Campbell | first1 = Colin M.
 | last2 = Robertson | first2 = Edmund F.
 | last3 = Ruskuc | first3 = Nik
 | last4 = Thomas | first4 = Richard M.
 | title = Automatic completely simple semigroups
 | journal = Acta Mathematica Hungarica
 | volume = 95 | year = 2002 | issue = 3 | pages = 201–215
 | doi = 10.1023/A:1015632704977}}.

*{{citation
 | last1 = Duncan | first1 = A. J.
 | last2 = Robertson | first2 = E. F.
 | last3 = Ruskuc | first3 = N.
 | title = Automatic monoids and change of generators
 | journal = Mathematical Proceedings of the Cambridge Philosophical Society
 | volume = 127 | year = 1999 | issue = 3 | pages = 403–409
 | doi = 10.1017/S0305004199003722}}.

*{{citation
 | last1 = Epstein | first1 = David B. A. | authorlink1 = David B. A. Epstein
 | last2 = Cannon | first2 = James W.
 | last3 = Holt | first3 = Derek F.
 | last4 = Levy | first4 = Silvio V. F.
 | last5 = Paterson | first5 = Michael S.
 | last6 = Thurston | first6 = William P. | authorlink6 = William Thurston
 | title = Word Processing in Groups
 | publisher = Jones and Bartlett Publishers | location = Boston, MA | year = 1992 | isbn = 0-86720-244-0}}.

*{{citation
 | last1 = Kambites | first1 = Mark
 | last2 = Otto
 | first2 = F
 | title = Uniform decision problems for automatic semigroups
 | journal = Journal of Algebra
 | volume = 303 | year = 2006 | issue = 2 | pages = 789–809
 | doi = 10.1016/j.jalgebra.2005.11.028}}

*{{citation
 | last1 = Silva | first1 = P.V.
 | last2 = Steinberg | first2 = B.
 | title = A geometric characterization of automatic monoids
 | journal = Quarterly Journal of Mathematics
 | volume = 55 | issue = 3 | year = 2004 | pages = 333–356
 | doi = 10.1093/qmath/hah006| citeseerx = 10.1.1.36.1681}}

==Further reading==
* {{citation | last1=Hoffmann | first1=Michael | last2=Kuske | first2=Dietrich | last3=Otto | first3=Friedrich | last4=Thomas | first4=Richard M. | chapter=Some relatives of automatic and hyperbolic groups | zbl=1031.20047 | editor1-last=Gomes | editor1-first=Gracinda M. S.  | title=Semigroups, algorithms, automata and languages. Proceedings of workshops held at the International Centre of Mathematics, CIM, Coimbra, Portugal, May, June and July 2001 | location=Singapore | publisher= World Scientific | pages= 379–406 | year=2002 }}

[[Category:Semigroup theory]]
[[Category:Computability theory]]</text>
      <sha1>opttwdf33u432243cbldyuas3vtipsx</sha1>
    </revision>
  </page>
  <page>
    <title>Barrier resilience</title>
    <ns>0</ns>
    <id>43640244</id>
    <revision>
      <id>848834416</id>
      <parentid>848834322</parentid>
      <timestamp>2018-07-04T17:22:26Z</timestamp>
      <contributor>
        <username>Adam majewski</username>
        <id>441224</id>
      </contributor>
      <minor/>
      <comment>improved link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8351">[[File:Barrier resilience.svg|thumb|300px|Barrier resilience. This instance has resilience&amp;nbsp;=&amp;nbsp;1 (it is possible to connect the green regions by a path that crosses only one barrier, the blue one) but the barrier must be crossed twice by the path.]]
'''Barrier resilience''' is an [[algorithm]]ic [[optimization problem]] in [[computational geometry]] motivated by the design of [[wireless sensor network]]s, in which one seeks a path through a collection of barriers (often modeled as [[unit disk]]s) that passes through as few barriers as possible.

==Definitions==
The barrier resilience problem was introduced by {{harvtxt|Kumar|Lai|Arora|2005}} (using different terminology) to model the ability of [[wireless sensor network]]s to detect intruders robustly when some sensors may become faulty.
In this problem, the region under surveillance from each sensor is modeled as a unit disk in the [[Euclidean plane]]. An intruder can reach a target region of the plane without detection, if there exists a path in the plane connecting a given start region to the target region without crossing any of the sensor disks. The ''barrier resilience'' of a sensor network is defined to be the minimum, over all paths from the start region to the target region, of the number of sensor disks intersected by the path. The barrier resilience problem is the problem of computing this number by finding an optimal path through the barriers.{{sfnp|Bereg|Kirkpatrick|2009}}

A simplification of the problem, which encapsulates most of its essential features, makes the target region be the [[Origin (mathematics)|origin]] of the plane, and the start region be the set of points outside the [[convex hull]] of the sensor disks. In this version of the problem, the goal is to connect the origin to points arbitrarily far from the origin by a path through as few sensor disks as possible.

Another variation of the problem counts the number of times a path crosses the boundary of a sensor disk. If a path crosses the same disk multiple times, each crossing counts towards the total. The ''barrier thickness'' of a sensor network is the minimum number of crossings of a path from the start region to the target region.{{sfnp|Bereg|Kirkpatrick|2009}}

==Computational complexity==
Barrier thickness may be computed in [[polynomial time]] by constructing the [[Arrangement of lines|arrangement]] of the barriers (the subdivision of the plane formed by overlaying all barrier boundaries) and computing a [[shortest path]] in the [[dual graph]] of this subdivision.{{sfnp|Bereg|Kirkpatrick|2009}}

The complexity of barrier resilience for unit disk barriers is an open problem. It may be solved by a [[Parameterized complexity|fixed-parameter tractable algorithm]] whose time is cubic in the total number of barriers and exponential in the square of the resilience, but it is not known whether it has a fully polynomial time solution.{{sfnp|Korman|Löffler|Silveira|Strash|2014}}
The corresponding problem for barriers of some other shapes, including unit-length [[line segment]]s or axis-aligned [[rectangle]]s of [[aspect ratio]] close to&amp;nbsp;1, is known to be [[NP-hard]].&lt;ref&gt;{{harvtxt|Alt|Cabello|Giannopoulos|Knauer|2011}}; {{harvtxt|Tseng|Kirkpatrick|2012}}; {{harvtxt|Korman|Löffler|Silveira|Strash|2014}}.&lt;/ref&gt;

A variation of the barrier resilience problem, studied by {{harvtxt|Kumar|Lai|Arora|2005}}, restricts both the sensors and the escape path to a [[rectangle]] in the plane. In this variation, the goal is to find a path from the top side of the rectangle to the bottom side that passes through as few of the sensor disks as possible. By applying [[Menger's theorem]] to the [[unit disk graph]] defined from the barriers, this minimal number of disks can be shown to equal the maximum number of subsets into which all of the disks can be partitioned, such that each subset contains a chain of disks passing all the way from the left to the right side of the rectangle. As {{harvtxt|Kumar|Lai|Arora|2005}} showed, this characterization allows the optimal resilience to be computed in polynomial time by transforming the problem into an instance of the [[maximum flow problem]].

For unit disks with bounded ''ply'' (the maximum number of disks that have a common intersection) there exists a [[polynomial-time approximation scheme]] for the resilience, that can be generalized to barrier shapes of the same size as each other with bounded aspect ratios.{{sfnp|Korman|Löffler|Silveira|Strash|2014}} For unit disks without assuming bounded ply, the problem of computing the resilience may be approximated to within a constant factor, using the fact that for this shape of barrier the optimal path can only cross each barrier a constant number of times, so the barrier thickness and barrier resilience are within a constant factor of each other.{{sfnp|Bereg|Kirkpatrick|2009}} Similar methods can be generalized to non-uniform sensors of approximately equal size.{{sfnp|Chan|Kirkpatrick|2013}}

==Notes==
{{reflist|30em}}

==References==
{{refbegin|30em}}
*{{citation
 | last1 = Alt | first1 = Helmut
 | last2 = Cabello | first2 = Sergio
 | last3 = Giannopoulos | first3 = Panos
 | last4 = Knauer | first4 = Christian
 | arxiv = 1104.4618
 | title = Minimum cell connection and separation in line segment arrangements
 | year = 2011| bibcode = 2011arXiv1104.4618A}}.
*{{citation
 | last1 = Bereg | first1 = Sergey
 | last2 = Kirkpatrick | first2 = David | author2-link = David G. Kirkpatrick
 | contribution = Approximating barrier resilience in wireless sensor networks
 | doi = 10.1007/978-3-642-05434-1_5
 | pages = 29–40
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = Algorithmic Aspects of Wireless Sensor Networks: 5th International Workshop, ALGOSENSORS 2009, Rhodes, Greece, July 10-11, 2009, Revised Selected Papers
 | volume = 5804
 | year = 2009| bibcode = 2009LNCS.5304...29B}}.
*{{citation
 | last1 = Chan | first1 = David Yu Cheng
 | last2 = Kirkpatrick | first2 = David | author2-link = David G. Kirkpatrick
 | contribution = Approximating barrier resilience for arrangements of non-identical disk sensors
 | doi = 10.1007/978-3-642-36092-3_6
 | pages = 42–53
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = Algorithms for Sensor Systems: 8th International Symposium on Algorithms for Sensor Systems, Wireless Ad Hoc Networks and Autonomous Mobile Entities, ALGOSENSORS 2012, Ljubljana, Slovenia, September 13-14, 2012, Revised Selected Papers
 | volume = 7718
 | year = 2013}}.
*{{citation
 | last1 = Korman | first1 = Matias
 | last2 = Löffler | first2 = Maarten
 | last3 = Silveira | first3 = Rodrigo I.
 | last4 = Strash | first4 = Darren
 | contribution = On the complexity of barrier resilience for fat regions
 | doi = 10.1007/978-3-642-45346-5_15
 | pages = 201–216
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = Algorithms for Sensor Systems: 9th International Symposium on Algorithms and Experiments for Sensor Systems, Wireless Networks and Distributed Robotics, ALGOSENSORS 2013, Sophia Antipolis, France, September 5-6, 2013, Revised Selected Papers
 | volume = 8243
 | year = 2014| arxiv = 1302.4707}}.
*{{citation
 | last1 = Kumar | first1 = Santosh
 | last2 = Lai | first2 = Ten H.
 | last3 = Arora | first3 = Anish
 | contribution = Barrier coverage with wireless sensors
 | doi = 10.1145/1080829.1080859
 | isbn = 1-59593-020-5
 | location = New York, NY, USA
 | pages = 284–298
 | publisher = ACM
 | title = Proceedings of the 11th Annual International Conference on Mobile Computing and Networking (MobiCom '05, Cologne, Germany)
 | year = 2005}}.
*{{citation
 | last1 = Tseng | first1 = Kuan-Chieh Robert
 | last2 = Kirkpatrick | first2 = David | author2-link = David G. Kirkpatrick
 | contribution = On barrier resilience of sensor networks
 | doi = 10.1007/978-3-642-28209-6_11
 | pages = 130–144
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = Algorithms for Sensor Systems: 7th International Symposium on Algorithms for Sensor Systems, Wireless Ad Hoc Networks and Autonomous Mobile Entities, ALGOSENSORS 2011, Saarbrücken, Germany, September 8-9, 2011, Revised Selected Papers
 | volume = 7111
 | year = 2012}}.
{{refend}}

[[Category:Computational geometry]]</text>
      <sha1>1doy45zpb8v8zsfik9zl7v7f2jt30nw</sha1>
    </revision>
  </page>
  <page>
    <title>Beatrice Mabel Cave-Browne-Cave</title>
    <ns>0</ns>
    <id>31735347</id>
    <revision>
      <id>857495232</id>
      <parentid>812362254</parentid>
      <timestamp>2018-09-01T01:17:41Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>add authority control, test</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2805">{{Use dmy dates|date=January 2017}}
{{Use British English|date=January 2017}}
'''Beatrice Mabel Cave-Browne-Cave, [[Order of the British Empire|MBE]]''' (30 May 1874 – 9 July 1947) was an [[English people|English]] [[mathematician]] who undertook pioneering work in the mathematics of aeronautics.

==Birth and education==
Beatrice Cave-Browne-Cave was the daughter of Sir Thomas Cave-Browne-Cave (1835–1924; see [[Cave-Browne-Cave baronets]] for earlier history of the family) and Blanche Matilda Mary Ann (née Milton). One of six siblings, one of her brothers was [[Henry Cave-Browne-Cave]], a [[Royal Air Force]] officer. She was educated at home in [[Streatham]] and entered [[Girton College]], [[University of Cambridge|Cambridge]] with her younger sister [[Frances Cave-Browne-Cave|Frances]] in 1895. Gaining a second-class degree in the [[Cambridge Mathematical Tripos|mathematical tripos]], part one (1898), she took part two a year later (1899), and was placed in the third class.&lt;ref&gt;{{Cite ODNB|id= 61586 |title=Cave, Beatrice Mabel Cave-Browne- (1874–1947), applied mathematician}}&lt;/ref&gt;

==Career==
After eleven years teaching mathematics to girls at a high school in Clapham in south London, in the years just before the First World War she worked under Professor [[Karl Pearson]]&lt;ref&gt;David Alan Grier, ''When Computers Were Human'', Princeton University Press, 2005, pp. 111-12.&lt;!-- ISBN needed --&gt;&lt;/ref&gt; in the [[Galton Laboratory]] at [[University College, London]].

During World War I, she carried out original research for the government on the mathematics of aeronautics which remained classified under the [[Official Secrets Act]] for fifty years. Elected an associate fellow of the [[Royal Aeronautical Society]] in 1919 and awarded an [[Order of the British Empire|MBE]] in 1920, she later worked as an assistant to Sir [[Leonard Bairstow]], the Zaharoff Professor of Aviation at Imperial College, London. She retired in 1937, continuing to live in Streatham&lt;ref&gt;A.E.L. Davis, entry on Beatrice and Evelyn Cave-Browne-Cave, ''Oxford Dictionary of National Biography'', Vol. 10, Oxford University Press, 2004, pp. 594-95.&lt;!-- ISSN/ISBN needed --&gt;&lt;/ref&gt;

==Death==
Beatrice Mabel Cave-Browne-Cave died on 9 July 1947 at age 73, unmarried.&lt;ref name="peerage"&gt;[http://www.thepeerage.com/p21059.htm Profile], peerage.com; accessed 31 October 2014.&lt;/ref&gt;

==References==
{{Reflist}}

{{authority control}}

{{DEFAULTSORT:Cave-Brown-Cave, Beatrice Mabel}}
[[Category:1874 births]]
[[Category:1947 deaths]]
[[Category:Women mathematicians]]
[[Category:English mathematicians]]
[[Category:English aerospace engineers]]
[[Category:Place of birth missing]]
[[Category:Place of death missing]]
[[Category:Members of the Order of the British Empire]]
[[Category:British women engineers]]</text>
      <sha1>pbxg8x26hg0k2v4cxlckwzge6dilx9k</sha1>
    </revision>
  </page>
  <page>
    <title>Beta (finance)</title>
    <ns>0</ns>
    <id>897558</id>
    <revision>
      <id>860344375</id>
      <parentid>859448816</parentid>
      <timestamp>2018-09-20T01:03:53Z</timestamp>
      <contributor>
        <username>ImageRemovalBot</username>
        <id>4851336</id>
      </contributor>
      <comment>Removing links to deleted file [[:File:SecMktLine.png]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="30675">{{other uses|Beta (disambiguation)}}
{{Redirect|Beta coefficient|the general statistical concept|Standardized coefficient}}
{{lead too long|date=May 2016}}

In [[finance]], the '''beta''' (β or '''beta coefficient''') of an [[investment]] indicates whether the investment is more or less volatile than the market as a whole.  

Beta is a measure of the risk arising from exposure to general [[Market (economics)|market]] movements as opposed to idiosyncratic factors. The [[market portfolio]] of all investable [[asset]]s has a beta of exactly 1. A beta below 1 can indicate either an investment with lower [[volatility (finance)|volatility]] than the market, or a volatile investment whose price movements are not highly [[correlation and dependence|correlated]] with the market. An example of the first is a [[United States Treasury security|treasury]] bill: the price does not go up or down a lot, so it has a low beta. An example of the second is [[gold]]. The price of gold does go up and down a lot, but not in the same direction or at the same time as the market.&lt;ref&gt;{{cite book
 | last = Sharpe
 | first = William
 | year = 1970
 | title = Portfolio Theory and Capital Markets
 | publisher = McGraw-Hill Trade
 | isbn = 978-0071353205
}}&lt;/ref&gt;

A beta greater than 1 generally means that the asset both is volatile and tends to move up and down with the market. An example is a stock in a big technology company. Negative betas are possible for investments that tend to go down when the market goes up, and vice versa. There are few fundamental investments with consistent and significant negative betas, but some [[derivative (finance)|derivatives]] like [[put option]]s can have large negative betas.&lt;ref&gt;{{cite book
 | last = Markowitz
 | first = Harry
 | year = 1958
 | title = Portfolio Selection
 | publisher = John Wiley &amp; Sons
 | isbn = 978-1557861085
}}&lt;/ref&gt;

Beta is important because it measures the risk of an investment that cannot be reduced by [[diversification (finance)|diversification]]. It does not measure the risk of an investment held on a stand-alone basis, but the amount of risk the investment adds to an already-diversified portfolio. In the [[Capital Asset Pricing Model]] (CAPM), beta risk is the only kind of risk for which investors should receive an expected return higher than the [[risk-free interest rate|risk-free rate of interest]].&lt;ref&gt;{{cite book
 | last = Fama
 | first = Eugene
 | year = 1976
 | title = Foundations of Finance: Portfolio Decisions and Securities Prices
 | publisher = Basic Books
 | isbn = 978-0465024995
}}&lt;/ref&gt;

The definition above covers only theoretical beta. The term is used in many related ways in finance. For example, the betas commonly quoted in [[mutual fund]] analyses generally measure the risk of the fund arising from exposure to a benchmark for the fund, rather than from exposure to the entire market portfolio. Thus they measure the amount of risk the fund adds to a diversified portfolio of funds of the same type, rather than to a portfolio diversified among all fund types.&lt;ref&gt;{{cite book
 | last = Ilmanen
 | first = Antti
 | year = 2011
 | title = Expected Returns: An Investor's Guide to Harvesting Market Rewards
 | publisher = John Wiley &amp; Sons
 | isbn = 978-1119990727
}}&lt;/ref&gt;

Beta decay refers to the tendency for a company with a high beta coefficient (β &gt; 1) to have its beta coefficient decline to the market beta. It is an example of [[regression toward the mean]].

==Statistical estimation==
A statistical estimate of beta is calculated by a regression method. For a given asset and a benchmark, the goal is to find an approximate formula

:&lt;math&gt;r_a \approx \alpha + \beta r_b&lt;/math&gt;

where ''r''&lt;sub&gt;a&lt;/sub&gt; is the return of the asset, [[alpha (finance)|alpha]] (&amp;alpha;) is the active return, and ''r''&lt;sub&gt;b&lt;/sub&gt; is return of the benchmark.

Since practical data are typically available as a discrete [[time series]] of samples, the statistical model is

:&lt;math&gt;r_{a,t} = \alpha + \beta r_{b,t} + \varepsilon_t,&lt;/math&gt;

where &amp;epsilon;&lt;sub&gt;''t''&lt;/sub&gt; is an error term (the unexplained return).
The best (in the sense of least squared error) estimates for &amp;alpha; and &amp;beta; are those such that &amp;Sigma;&amp;epsilon;&lt;sub&gt;''t''&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; is as small as possible.

A common expression for beta is

:&lt;math&gt;\beta = \frac {\mathrm{Cov}(r_a,r_b)}{\mathrm{Var}(r_b)},&lt;/math&gt;

where Cov and Var are the [[covariance]] and [[variance]] operators.

By using the relationships between standard deviation, variance and correlation: &lt;math&gt;\sigma_a = \sqrt{\mathrm{Var}(r_a)}&lt;/math&gt;, &lt;math&gt;\sigma_b = \sqrt{\mathrm{Var}(r_b)}&lt;/math&gt;, &lt;math&gt;\rho_{a,b} = \mathrm{Cov}(r_a, r_b)/\sqrt{ \mathrm{Var}(r_a) \mathrm{Var}(r_b) }&lt;/math&gt;, the above expression can also be written as

:&lt;math&gt;\beta = \rho_{a,b}\frac{\sigma_a}{\sigma_b}&lt;/math&gt;

where &amp;rho;&lt;sub&gt;a,b&lt;/sub&gt; is the correlation of the two returns, and &amp;sigma;&lt;sub&gt;a&lt;/sub&gt; and &amp;sigma;&lt;sub&gt;b&lt;/sub&gt; are the respective volatilities. If ''a'' refers to the investment and ''b'' refers to the market, it now becomes clear that the interpretation of beta as 'the volatility of an investment relative to the market volatility' is inconsistent with how beta is calculated; this is due to the presence of the correlation in the above formula.&lt;ref name=":0" /&gt;

Beta can be computed for prices in the past, where the data is known, which is historical beta. However, what most people are interested in is ''future beta'', which relates to risks going forward. Estimating future beta is a difficult problem. One guess is that future beta equals historical beta.

From this, we find that beta can be explained as "correlated relative volatility". This has three components:
*correlated
*relative
*volatility

Beta is also referred to as '''financial [[elasticity (economics)|elasticity]]''' or correlated relative [[Volatility (finance)|volatility]], and can be referred to as a measure of the sensitivity of the [[asset]]'s returns to market returns, its non-diversifiable [[risk]], its [[systematic risk]], or market risk.  On an individual asset level, measuring beta can give clues to [[Volatility (finance)|volatility]] and [[liquidity]] in the marketplace. In fund management, measuring beta is thought to separate a manager's skill from his or her willingness to take risk.

The portfolio of interest in the CAPM formulation is the market portfolio that contains all risky assets, and so the ''r''&lt;sub&gt;b&lt;/sub&gt; terms in the formula are replaced by ''r''&lt;sub&gt;m&lt;/sub&gt;, the rate of return of the market. The regression line is then called the [[security characteristic line]] ('''SCL''').

:&lt;math&gt; \mathrm{SCL}: r_{a,t} = \alpha_a + \beta_a r_{m,t} + \varepsilon_{a,t}&lt;/math&gt;

&lt;math&gt;\alpha_a&lt;/math&gt; is called the asset's [[Alpha (finance)|alpha]] and &lt;math&gt;\beta_a&lt;/math&gt; is called the asset's '''beta coefficient'''. Both coefficients have an important role in [[modern portfolio theory]].

For example, in a year where the broad market or [[Benchmark (surveying)|benchmark]] index returns 25% above the risk free rate, suppose two managers gain 50% above the risk free rate. Because this higher return is theoretically possible merely by taking a [[Leverage (finance)|leverage]]d position in the broad market to double the beta so it is exactly 2.0, we would expect a skilled portfolio manager to have built the outperforming portfolio with a beta somewhat less than 2, such that the [[Alpha (finance)|excess return]] not explained by the beta is positive. If one of the managers' portfolios has an average beta of 3.0, and the other's has a beta of only 1.5, then the CAPM simply states that the extra return of the first manager is not sufficient to compensate us for that manager's risk, whereas the second manager has done more than expected given the risk.  Whether investors can expect the second manager to duplicate that performance in future periods is of course a different question.

===Security market line===
{{Main|Security market line}}
{| style="float:right;"
|-
|&lt;!-- Deleted image removed: [[File:SecMktLine.png|thumb|center|The security market line]] --&gt;
|}

The [[Security market line|SML]] graphs the results from the capital asset pricing model (CAPM) formula. The ''x''-axis represents the risk (beta), and the ''y''-axis represents the expected return. The market risk premium is determined from the slope of the SML.

The relationship between β and required return is plotted on the ''security market line'' (SML) which shows expected return as a function of β. The intercept is the nominal risk-free rate ''R&lt;sub&gt;f&lt;/sub&gt;'' available for the market, while the slope is E(''R''&lt;sub&gt;''m''&lt;/sub&gt;)&amp;minus;&amp;nbsp;''R''&lt;sub&gt;''f''&lt;/sub&gt; (for market return ''R&lt;sub&gt;m&lt;/sub&gt;''). The security market line can be regarded as representing a single-factor model of the asset price, where beta is exposure to changes in value of the market. The equation of the SML, giving the expected value of the return on asset ''i'', is thus:

:&lt;math&gt; \mathrm{SML}: E(R_i) - R_f = \beta_i (E(R_M) - R_f).~ &lt;/math&gt;

It is a useful tool in determining if an asset being considered for a portfolio offers a reasonable expected return for risk. Individual securities are plotted on the SML graph. If the security's risk versus expected return is plotted above the SML, it is undervalued because the investor can expect a greater return for the inherent risk. A security plotted below the SML is overvalued because the investor would be accepting a lower return for the amount of risk assumed.

==Choice of benchmark==

In the U.S., published betas typically use a [[stock market index]] such as the [[S&amp;P 500]] as a benchmark. The S&amp;P 500 is a popular index of U.S. large-cap stocks. Other choices may be an international index such as the [[MSCI EAFE]]. The benchmark is often chosen to be similar to the assets chosen by the investor. For example, for a person who owns S&amp;P 500 index funds and gold bars, the index would combine the S&amp;P 500 and the price of gold. In practice a standard index is used.

The choice of the index need not reflect the portfolio under question; e.g., beta for gold bars compared to the S&amp;P 500 may be low or negative carrying the information that gold does not track stocks and may provide a mechanism for reducing risk.  The restriction to stocks as a benchmark is somewhat arbitrary. A model portfolio may be stocks plus bonds. Sometimes the market is defined as "all investable assets" (see [[Roll's critique]]); unfortunately, this includes lots of things for which returns may be hard to measure.

==Investing==

By definition, the market itself has a beta of 1, and individual stocks are ranked according to how much they deviate from the macro market (for simplicity purposes, the [[S&amp;P 500]] is sometimes used as a proxy for the market as a whole). A stock whose returns vary more than the market's returns over time can have a beta whose [[absolute value]] is greater than 1.0 (whether it is, in fact, greater than 1.0 will depend on the correlation of the stock's returns and the market's returns). A stock whose returns vary less than the market's returns has a beta with an absolute value less than 1.0.

A stock with a beta of 2 has returns that change, on average, by twice the magnitude of the overall market; when the market's return falls or rises by 3%, the stock's return will fall or rise (respectively) by 6% on average. (However, because beta also depends on the correlation of returns, there can be considerable variance about that average; the higher the correlation, the less variance; the lower the correlation, the higher the variance.) Beta can also be negative, meaning the stock's returns tend to move in the opposite direction of the market's returns. A stock with a beta of −3 would see its return ''decline'' 9% (on average) when the market's return goes up 3%, and would see its return ''climb'' 9% (on average) if the market's return falls by 3%.

Higher-beta stocks tend to be more volatile and therefore riskier, but provide the potential for higher returns. Lower-beta stocks pose less risk but generally offer lower returns. Some have challenged this idea, claiming that the data show little relation between beta and potential reward, or even that lower-beta stocks are both less risky and more profitable (contradicting CAPM).&lt;ref&gt;McAlpine, Chad (2010). [http://business.financialpost.com/2010/06/22/low-risk-tsx-stocks-have-outearned-riskiest-peers-over-30-year-period-analyst/ "Low-risk TSX stocks have outearned riskiest peers over 30-year period"], ''The Financial Post Trading Desk'', June 22, 2010&lt;/ref&gt; In the same way a stock's beta shows its relation to market shifts, it is also an indicator for required [[Rate of return|returns on investment]] (ROI). Given a [[risk-free rate]] of 2%, for example, if the market (with a beta of 1) has an [[expected return]] of 8%, a stock with a beta of 1.5 should return 11% (=&amp;nbsp;2%&amp;nbsp;+&amp;nbsp;1.5(8%&amp;nbsp;−&amp;nbsp;2%)) in accordance with the financial CAPM model.

== Adding to a portfolio ==
Suppose an investor has all his money in an asset class X and wishes to move a small amount to an asset class Y. For example, X could be U.S. stocks, while Y could be stocks of a different country, or bonds. Then the new portfolio, Z, can be expressed symbolically

:&lt;math&gt;Z = (1 - \delta) X + \delta Y.&lt;/math&gt;

The variance can be computed as

:&lt;math&gt;\operatorname{Var}(Z) = (1 - \delta)^2 \operatorname{Var}(X) + 2 \delta (1 - \delta) \operatorname{Cov}(X, Y) + \delta^2 \operatorname{Var}(Y)&lt;/math&gt;

which can be simplified by ignoring &amp;delta;&lt;sup&gt;2&lt;/sup&gt; terms:

:&lt;math&gt;\operatorname{Var}(Z) \approx (1 - 2 \delta) \operatorname{Var}(X) + 2 \delta \operatorname{Cov}(X, Y).&lt;/math&gt;

The first formula is exact, while the second one is only valid for small &amp;delta;. Using the formula for &amp;beta; of Y relative to X,

:&lt;math&gt;\beta = \operatorname{Cov}(X, Y) / \operatorname{Var}(X),&lt;/math&gt;

we can compute

:&lt;math&gt;\operatorname{Var}(Z) / \operatorname{Var}(X) \approx 1 + 2 \delta (\beta - 1).&lt;/math&gt;

This suggests that an asset with &amp;beta; greater than 1 will increase variance, while an asset with &amp;beta; less than 1 will decrease variance, if added in the right amount. This assumes that variance is an accurate measure of risk, which is usually good. However, the beta does need to be computed with respect to what the investor currently owns.

==Academic theory==

Academic theory claims that higher-risk investments should have higher returns over the ''long-term''. Wall Street has a saying that "higher return requires higher risk", not that a risky investment will automatically do better. Some things may just be poor investments (e.g., playing [[roulette]]). Further, highly rational investors should consider correlated volatility (beta) instead of simple volatility (sigma). Theoretically, a negative beta equity is possible; for example, an [[Inverse exchange-traded fund|inverse ETF]] should have negative beta to the relevant index. Also, a [[Short (finance)|short]] position should have opposite beta.

This expected return on equity, or equivalently, a firm's [[cost of equity]], can be estimated using the [[capital asset pricing model]] (CAPM).  According to the model, the expected return on equity is a function of a firm's equity beta (β&lt;sub&gt;E&lt;/sub&gt;) which, in turn, is a function of both leverage and asset risk (β&lt;sub&gt;A&lt;/sub&gt;):

: &lt;math&gt;K_E = R_F + \beta_E (R_M - R_F)&lt;/math&gt;

where:

* ''K''&lt;sub&gt;E&lt;/sub&gt; = firm's cost of equity
* ''R''&lt;sub&gt;F&lt;/sub&gt; = [[risk-free interest rate|risk-free rate]] (the rate of return on a "risk free investment"; e.g., U.S. Treasury Bonds)
* ''R''&lt;sub&gt;M&lt;/sub&gt; = return on the market portfolio
* &lt;math&gt;\beta_E = \beta =\left[ \beta_A - \beta_D \left(\frac {D}{V}\right) \right]   \frac {V}{E}&lt;/math&gt;

because:

: &lt;math&gt;\beta_A = \beta_D \left(\frac {D}{V}\right) + \beta_E \left(\frac {E}{V}\right) &lt;/math&gt;

and

: Firm value (''V'') + cash and risk-free securities = debt value (''D'') + equity value (''E'')

An indication of the systematic riskiness attaching to the returns on ordinary shares. It equates to the asset Beta for an ungeared firm, or is adjusted upwards to reflect the extra riskiness of shares in a geared firm., i.e. the Geared Beta.&lt;ref&gt;{{cite web|url=http://www.lse.co.uk/financeglossary.asp?searchTerm=equity&amp;iArticleID=1688&amp;definition=equity_beta |title=Click here definition of Equity Beta, what is Equity Beta, what does Equity Beta mean? Finance Glossary - Search our financial terms for a definition - London South East |publisher=Lse.co.uk |accessdate=2012-12-03}}&lt;/ref&gt;

==Multiple beta model==
The [[arbitrage pricing theory]] (APT) has multiple betas in its model.  In contrast to the CAPM that has only one [[Risk factor (finance)|risk factor]], namely the overall market, APT has multiple risk factors.  Each risk factor has a corresponding beta indicating the responsiveness of the asset being priced to that risk factor.

Multiple-factor models contradict CAPM by claiming that some other factors can influence return, therefore one may find two stocks (or funds) with equal beta, but one may be a better investment.

==Estimation==
To estimate beta, one needs a list of returns for the asset and returns for the index; these returns can be daily, weekly or any period. Then one uses standard formulas from [[linear regression]]. The slope of the fitted line from the [[linear least squares (mathematics)|linear least-squares]] calculation is the estimated Beta.  The ''y''-intercept is the [[Alpha (finance)|alpha]].

[[Myron Scholes]] and [[Joseph Williams (economist)|Joseph Williams]] (1977) provided a model for estimating betas from nonsynchronous data.&lt;ref&gt;{{cite journal |last=Scholes |first=Myron |authorlink= |author2=Williams, Joseph |year=1977 |title=Estimating betas from nonsynchronous data |journal=Journal of Financial Economics |volume=5 |issue=3 |pages=309–327 |doi=10.1016/0304-405X(77)90041-1 |url= |accessdate= |quote= }}&lt;/ref&gt;

Beta specifically gives the volatility ratio multiplied by the correlation of the plotted data. To take an extreme example, something may have a beta of zero even though it is highly volatile, provided it is uncorrelated with the market. Tofallis (2008) provides a discussion of this,&lt;ref name=":0"&gt;{{cite journal |last=Tofallis |first=Chris |authorlink= |year=2008 |title=Investment Volatility: A Critique of Standard Beta Estimation and a Simple Way Forward |journal=European Journal of Operational Research |volume=187 |issue=3 |pages=1358–1367 |doi=10.1016/j.ejor.2006.09.018 |url= |accessdate= |quote= |arxiv=1109.4422}}&lt;/ref&gt; together with a real example involving [[AT&amp;T Inc.]] The graph showing monthly returns from AT&amp;T is visibly more volatile than the
index and yet the standard estimate of beta for this is less than one.

The relative volatility ratio described above is actually known as Total Beta (at least by appraisers who practice business valuation).  Total beta is equal to the identity: beta/''R'' or the standard deviation of the stock/standard deviation of the market (note: the relative volatility).  Total beta captures the security's risk as a stand-alone asset (because the correlation coefficient, R, has been removed from beta), rather than part of a well-diversified portfolio.  Because appraisers frequently value closely held companies as stand-alone assets, total beta is gaining acceptance in the business valuation industry.  Appraisers can now use total beta in the following equation: total cost of equity (TCOE) = risk-free rate + total beta&amp;middot;equity risk premium.  Once appraisers have a number of TCOE benchmarks, they can compare/contrast the risk factors present in these publicly traded benchmarks and the risks in their closely held company to better defend/support their valuations.

==Interpretations==
Some interpretations of beta are explained in the following table:&lt;ref name=WikinvestBeta&gt;[[wikinvest:Beta|Definition of Beta Definition via Wikinvest]]&lt;/ref&gt;
{| class="wikitable"
|-
! Value of Beta !! Interpretation !! Example
|-
| β &amp;lt; -1 || Asset moves in the opposite direction, and in a greater amount than the negative of the benchmark || ¿¿¿ Maybe similar to β &amp;gt; 1, but inversed ???
|-
| -1 &amp;lt; β &amp;lt; 0 || Asset movement is in the opposite direction of the benchmark || An [[inverse exchange-traded fund]] or a short position
|-
| β = 0 || Asset movement is uncorrelated to the benchmark || Fixed-yield asset, whose growth is unrelated to the movement of the stock market
|-
| 0 &lt; β &lt; 1 ||  Asset moves in the same direction, but in a lesser amount than the benchmark || Stable, "staple" stock such as a company that makes soap.  Moves in the same direction as the market at large, but less susceptible to day-to-day fluctuation.
|-
| β = 1 ||  Asset moves in the same direction and in the same amount as the benchmark || A representative stock, or a stock that is a strong contributor to the index itself.
|-
| β &amp;gt; 1 ||  Asset moves in the same direction, but in a greater amount than the benchmark || Stocks which are very strongly influenced by day-to-day market news, or by the general health of the economy.
|}

It measures the part of the asset's statistical [[variance]] that cannot be removed by the [[Diversification (finance)|diversification]] provided by the portfolio of many risky assets, because of the [[correlation]] of its returns with the returns of the other assets that are in the portfolio.  Beta can be estimated for individual companies using [[regression analysis]] against a [[stock market index]]. An alternative to standard beta is [[downside beta]].

Beta is always measured in respect to some benchmark. Therefore, an asset may have different betas depending on which benchmark is used. Just a number is useless if the benchmark is not known.

==Extreme and interesting cases==
* Beta has no upper or lower bound, and betas as large as 3 or 4 will occur with highly volatile stocks. 
* Beta can be zero. Some zero-beta assets are risk-free, such as [[United States Treasury security|treasury bonds]] and [[Money market fund|cash]]. However, simply because a beta is zero does ''not'' mean that it is risk-free. A beta can be zero simply because the correlation between that item's returns and the market's returns is zero. An example would be betting on horse racing. The correlation with the market will be zero, but it is certainly not a risk-free endeavor.
* On the other hand, if a stock has a moderately low but positive correlation with the market, but a high volatility, then its beta may still be high. 
* A negative beta simply means that the stock is inversely correlated with the market.
* A negative beta might occur even when both the benchmark index and the stock under consideration have positive returns. It is possible that lower positive returns of the index coincide with higher positive returns of the stock, or vice versa. The slope of the regression line in such a case will be negative.
* Using beta as a measure of relative risk has its own limitations. Most analyses consider only the magnitude of beta. Beta is a statistical variable and should be considered with its statistical significance ([[R square]] value of the regression line). Closer to 1 [[R square]] value implies higher [[correlation]] and a stronger relationship between returns of the asset and benchmark index.
* If beta is a result of regression of one stock against the market where it is quoted, betas from different countries are not comparable.
* Utility stocks commonly show up as examples of low beta. These have some similarity to bonds, in that they tend to pay consistent dividends, and their prospects are not strongly dependent on economic cycles. They are still stocks, so the market price will be affected by overall stock market trends, even if this does not make sense.
* Staple stocks are thought to be less affected by cycles and usually have lower beta. [[Procter &amp; Gamble]], which makes soap, is a classic example. Other similar ones are [[Altria Group|Philip Morris]] (tobacco) and [[Johnson &amp; Johnson]] (Health &amp; Consumer Goods).
* 'Tech' stocks are commonly equated with higher beta. This is based on experience of the [[dot-com bubble]] around year 2000. Although tech did very well in the late 1990s, it also fell sharply in the early 2000s, much worse than the decline of the overall market. More recently, this is not a good example.
* During the 2008 market fall, finance stocks did very poorly, much worse than the overall market. Then in the following years they gained the most, although not to make up for their losses.
* Foreign stocks may provide some diversification. World benchmarks such as [[S&amp;P Global 100]] have slightly lower betas than comparable US-only benchmarks such as [[S&amp;P 100]]. However, this effect is not as good as it used to be; the various markets are now fairly correlated, especially the US and Western Europe.{{Citation needed|date=November 2008}}
* [[Derivative (finance)|Derivatives]] and other non-linear assets.  Beta relies on a linear model.  An out of the money option may have a distinctly non-linear payoff.  The change in price of an option relative to the change in the price of the underlying asset (for example a stock) is not constant.  For example, if one purchased a put option on the S&amp;P 500, the beta would vary as the price of the underlying index (and indeed as volatility, time to expiration and other factors) changed. (see [[options pricing]], and [[Black–Scholes model]]).

==Criticism==
[[Seth Klarman]] of the Baupost group wrote  in ''[[Margin of Safety]]'':
"I find it preposterous that a single number reflecting past price fluctuations could be thought to completely describe the risk in a security. Beta views risk solely from the perspective of market prices, failing to take into consideration specific business
fundamentals or economic developments. The price level is also ignored, as if IBM selling at 50 dollars per share would
not be a lower-risk investment than the same IBM at 100 dollars per share. Beta fails to allow for the influence that investors
themselves can exert on the riskiness of their holdings through such efforts as [[proxy contests]], shareholder resolutions, communications with management, or the ultimate purchase of sufficient stock to gain corporate control and with it direct access to underlying value. Beta also assumes that the upside potential and [[downside risk]] of any investment are essentially equal,
being simply a function of that investment's volatility compared with that of the market as a whole. This too is inconsistent
with the world as we know it. The reality is that past security price volatility does not reliably predict future investment
performance (or even future volatility) and therefore is a poor measure of risk."&lt;ref&gt;{{cite journal |last=Klarman |first=Seth |authorlink=  |year=1991 |title=Beta|page=117 |doi=10.1016/0304-405X(77)90041-1 |url= |accessdate= |quote=|author8 = Klarman, Seth |journal=Journal of Financial Economics |volume=5 |last2=Williams |first2=Joseph |issue=3 }}&lt;/ref&gt;

At the industry level, beta tends to underestimate [[downside beta]] two-thirds of the time (resulting in value overestimation) and overestimate [[upside beta]] one-third of the time resulting in value underestimation.&lt;ref name="The Entrepreneur's Cost of Capital"&gt;{{cite web|title=The Entrepreneur's Cost of Capital: Incorporating Downside Risk in the Buildup Method|url=http://www.macrorisk.com/wp-content/uploads/2013/04/MRA-WP-2013-e.pdf|accessdate=25 June 2013|author=James Chong|author2=Yanbo Jin|author3= Michael Phillips|date=April 29, 2013}}&lt;/ref&gt;

Another weakness of beta can be illustrated through an easy example by considering two hypothetical stocks, A and B. The returns on A, B and the market follow the probability distribution below:
{| class="wikitable"
|-
! Probability !! Market !! Stock A !! Stock B
|-
| 0.25 || −30% || −15% || −60%
|-
| 0.25 || −15% || −7.5% || −30%
|-
| 0.25 || 15% || 30% || 7.5%
|-
| 0.25 || 30% || 60% || 15%
|}

The table shows that stock A goes down half as much as the market when the market goes down and up twice as much as the market when the market goes up. Stock B, on the other hand, goes down twice as much as the market when the market goes down and up half as much as the market when the market goes up. Most investors would label stock B as more risky. In fact, stock A has better return in every possible case. However, according to the [[capital asset pricing model]], stock A and B would have the same beta, meaning that theoretically, investors would require the same [[rate of return]] for both stocks. Of course it is entirely expected that this example could break the CAPM as the CAPM relies on certain assumptions one of the most central being the nonexistence of arbitrage, However, in this example buying stock A and selling stock B is an example of an arbitrage as stock A is worth more in every scenario. This is an illustration of how using standard beta might mislead investors. The [[dual-beta]] model, in contrast, takes into account this issue and differentiates [[downside beta]] from [[upside beta]], or [[downside risk]] from [[upside risk]], and thus allows investors to make better informed investing decisions.&lt;ref name="The Entrepreneur's Cost of Capital" /&gt;

==See also==
{{Div col|colwidth=22em|small=yes}}
*[[Alpha (finance)]]
*[[wikinvest:Beta|Beta Coefficient]] via [[Wikinvest]]
*[[Capital structure substitution theory#Beta|CSS Theory - Beta]]
*[[Cost of capital]]
*[[Financial risk]]
*[[Hamada's equation]]
*[[Macro risk]]
*[[Risk factor (finance)]]
*[[Treynor ratio]]
*[[Weighted average cost of capital|WACC]]
{{div col end}}

==References==
{{Reflist|30em}}

==External links==
*[http://www.etf.com/sections/research/5911-etfs-a-diversification.html?iu=1 ETFs &amp; Diversification: A Study of Correlations] 
*[http://rdcohen.50megs.com/IDRHEqabstract.htm Leverage and diversification effects of public companies]
*[http://investexcel.net/367/calculate-stock-beta-with-excel Calculate Beta in a Spreadsheet]
*[https://unicornbay.com/tools/beta-calculator Free Beta Calculator for any Asset-Index pair]
*[https://marketxls.com/calculate-sharpe-ratio-of-portfolio-in-excel/ Calculate Sharpe Ratio in Excel]
*[https://marketxls.com/beta-formula-in-excel/ Calculate Beta in Excel]

{{Stock market}}
{{Hedge funds}}

{{DEFAULTSORT:Beta (Finance)}}
[[Category:Mathematical finance]]
[[Category:Fundamental analysis]]
[[Category:Financial ratios]]
[[Category:Statistical ratios]]

[[ja:資本コスト#β値（ベータ値）]]</text>
      <sha1>m4fgps2wzrjhbg26ocp5aw73ewx9t7l</sha1>
    </revision>
  </page>
  <page>
    <title>Bisimulation</title>
    <ns>0</ns>
    <id>397571</id>
    <revision>
      <id>846949053</id>
      <parentid>841962834</parentid>
      <timestamp>2018-06-21T22:03:32Z</timestamp>
      <contributor>
        <username>Rgdboer</username>
        <id>92899</id>
      </contributor>
      <minor/>
      <comment>/* Formal definition */  lk Converse relation</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11847">{{refimprove|date=February 2017}}
In [[theoretical computer science]] a '''bisimulation''' is a [[binary relation]] between [[state transition system]]s, associating systems that behave in the same way in the sense that one system simulates the other and vice versa.

Intuitively two systems are '''bisimilar''' if they match each other's moves. In this sense, each of the systems cannot be distinguished from the other by an observer.

== Formal definition ==
Given a [[state transition system|labelled state transition system]] (&lt;math&gt;S&lt;/math&gt;, &amp;Lambda;, &amp;rarr;), a ''bisimulation'' [[Relation (mathematics)|relation]] is a [[binary relation]] &lt;math&gt;R&lt;/math&gt; over &lt;math&gt;S&lt;/math&gt; (i.e., &lt;math&gt;R&lt;/math&gt; &amp;sube; &lt;math&gt;S&lt;/math&gt; &amp;times; &lt;math&gt;S&lt;/math&gt;) such that both &lt;math&gt;R&lt;/math&gt; and its [[converse relation|converse]] &lt;math&gt;R^T&lt;/math&gt; are [[simulation preorder|simulation]]s.

Equivalently &lt;math&gt;R&lt;/math&gt; is a bisimulation if for every pair of elements &lt;math&gt;p, q&lt;/math&gt; in &lt;math&gt;S&lt;/math&gt; with &lt;math&gt;(p,q)&lt;/math&gt; in &lt;math&gt;R&lt;/math&gt;, for all &amp;alpha; in &amp;Lambda;:

for all &lt;math&gt;p'&lt;/math&gt; in &lt;math&gt;S&lt;/math&gt;,

::&lt;math&gt; 
p \overset{\alpha}{\rightarrow} p'
  &lt;/math&gt;

:implies that there is a &lt;math&gt;q'&lt;/math&gt; in &lt;math&gt;S&lt;/math&gt; such that

::&lt;math&gt; 
q \overset{\alpha}{\rightarrow} q'
  &lt;/math&gt;

:and &lt;math&gt;(p',q') \in R&lt;/math&gt;;

and, symmetrically, for all &lt;math&gt;q'&lt;/math&gt; in &lt;math&gt;S&lt;/math&gt;

::&lt;math&gt;
q \overset{\alpha}{\rightarrow} q' 
  &lt;/math&gt;

:implies that there is a &lt;math&gt;p'&lt;/math&gt; in &lt;math&gt;S&lt;/math&gt; such that

::&lt;math&gt;
p \overset{\alpha}{\rightarrow} p'
  &lt;/math&gt;

:and &lt;math&gt;(p',q') \in R&lt;/math&gt;.

Given two states &lt;math&gt;p&lt;/math&gt; and &lt;math&gt;q&lt;/math&gt; in &lt;math&gt;S&lt;/math&gt;, &lt;math&gt;p&lt;/math&gt; is '''bisimilar''' to &lt;math&gt;q&lt;/math&gt;, written &lt;math&gt;p \, \sim \, q&lt;/math&gt;, if there is a bisimulation &lt;math&gt;R&lt;/math&gt; such that &lt;math&gt;(p, q)&lt;/math&gt; is in &lt;math&gt;R&lt;/math&gt;.

The bisimilarity relation &lt;math&gt; \, \sim \, &lt;/math&gt; is an [[equivalence relation]]. Furthermore, it is the largest bisimulation relation over a given transition system.

Note that it is not always the case that if &lt;math&gt;p&lt;/math&gt; simulates &lt;math&gt;q&lt;/math&gt; and &lt;math&gt;q&lt;/math&gt; simulates &lt;math&gt;p&lt;/math&gt; then they are bisimilar. For &lt;math&gt;p&lt;/math&gt; and &lt;math&gt;q&lt;/math&gt; to be bisimilar, the simulation between &lt;math&gt;p&lt;/math&gt; and &lt;math&gt;q&lt;/math&gt; must be the [[converse relation|converse]] of the simulation between &lt;math&gt;q&lt;/math&gt; and &lt;math&gt;p&lt;/math&gt;. Counter-example (in [[Calculus of Communicating Systems|CCS]], describing a coffee machine) : &lt;math&gt;M=p.\overline{c}.M+p.\overline{t}.M+p.(\overline{c}.M+\overline{t}.M)&lt;/math&gt; and &lt;math&gt;M'=p.(\overline{c}.M'+\overline{t}.M')&lt;/math&gt; simulate each other but are not bisimilar.

== Alternative definitions ==

=== Relational definition ===
Bisimulation can be defined in terms of [[composition of relations]] as follows.

Given a [[state transition system|labelled state transition system]] &lt;math&gt;(S, \Lambda, \rightarrow)&lt;/math&gt;, a ''bisimulation'' [[Relation (mathematics)|relation]] is a [[binary relation]] &lt;math&gt;R&lt;/math&gt; over &lt;math&gt;S&lt;/math&gt; (i.e., &lt;math&gt;R&lt;/math&gt; &amp;sube; &lt;math&gt;S&lt;/math&gt; &amp;times; &lt;math&gt;S&lt;/math&gt;) such that &lt;math&gt;\forall\alpha\in\Lambda&lt;/math&gt;

::&lt;math&gt;R\ ;\ \overset{\alpha}{\rightarrow}\quad {\subseteq}\quad \overset{\alpha}{\rightarrow}\ ;\ R&lt;/math&gt;
:and
::&lt;math&gt;R^{-1}\ ;\ \overset{\alpha}{\rightarrow}\quad {\subseteq}\quad \overset{\alpha}{\rightarrow}\ ;\ R^{-1}&lt;/math&gt;

From the monotonicity and continuity of relation composition, it follows immediately that the set of the bisimulations is closed under unions (joins in the poset of relations), and a simple algebraic calculation shows that the relation of bisimilarity—the join of all bisimulations—is an equivalence relation. This definition, and the associated treatment of bisimilarity, can be interpreted in any involutive [[quantale]].

=== Fixpoint definition ===

Bisimilarity can also be defined in [[Order theory|order theoretical]] fashion, in terms of [[Knaster&amp;ndash;Tarski theorem|fixpoint theory]], more precisely as the greatest fixed point of a certain function defined below.

Given a [[state transition system|labelled state transition system]] (&lt;math&gt;S&lt;/math&gt;, &amp;Lambda;, &amp;rarr;), define &lt;math&gt;F:\mathcal{P}(S \times S) \to \mathcal{P}(S \times S)&lt;/math&gt; to be a function from binary relations over &lt;math&gt;S&lt;/math&gt; to binary relations over &lt;math&gt;S&lt;/math&gt;, as follows:

Let &lt;math&gt;R&lt;/math&gt; be any binary relation over &lt;math&gt;S&lt;/math&gt;. &lt;math&gt;F(R)&lt;/math&gt; is defined to be the set of all pairs &lt;math&gt;(p,q)&lt;/math&gt; in &lt;math&gt;S&lt;/math&gt; &amp;times; &lt;math&gt;S&lt;/math&gt; such that:

:&lt;math&gt;
\forall \alpha \in \Lambda. \, \forall p' \in S. \,
p \overset{\alpha}{\rightarrow} p' \, \Rightarrow \, 
\exists q' \in S. \, q \overset{\alpha}{\rightarrow} q' \,\textrm{ and }\, (p',q') \in R
&lt;/math&gt;

and

:&lt;math&gt;
\forall \alpha \in \Lambda. \, \forall q' \in S. \,
q \overset{\alpha}{\rightarrow} q' \, \Rightarrow \, 
\exists p' \in S. \, p \overset{\alpha}{\rightarrow} p' \,\textrm{ and }\, (p',q') \in R
&lt;/math&gt;

Bisimilarity is then defined to be the [[greatest fixed point]] of &lt;math&gt;F&lt;/math&gt;.

=== Game theoretical definition ===
Bisimulation can also be thought of in terms of a game between two players:  attacker and defender.

"Attacker" goes first and may choose any valid transition, &lt;math&gt;\alpha&lt;/math&gt;, from &lt;math&gt;(p,q)&lt;/math&gt;. I.e.:

&lt;math&gt; 
(p,q) \overset{\alpha}{\rightarrow} (p',q)
  &lt;/math&gt;
or 
&lt;math&gt; 
(p,q) \overset{\alpha}{\rightarrow} (p,q')
  &lt;/math&gt;

The "Defender" must then attempt to match that transition, &lt;math&gt;\alpha&lt;/math&gt; from either &lt;math&gt;(p',q)&lt;/math&gt; or &lt;math&gt;(p,q')&lt;/math&gt; depending on the attacker's move.
I.e., they must find an  &lt;math&gt;\alpha&lt;/math&gt; such that:

&lt;math&gt; 
(p',q) \overset{\alpha}{\rightarrow} (p',q')
  &lt;/math&gt;
or 
&lt;math&gt; 
(p,q') \overset{\alpha}{\rightarrow} (p',q')
 &lt;/math&gt;

Attacker and defender continue to take alternating turns until:

* The defender is unable to find any valid transitions to match the attacker's move.  In this case the attacker wins.
* The game reaches states &lt;math&gt;(p,q)&lt;/math&gt; that are both 'dead' (i.e., there are no transitions from either state) In this case the defender wins
* The game goes on forever, in which case the defender wins.
* The game reaches states &lt;math&gt;(p,q)&lt;/math&gt;, which have already been visited.  This is equivalent to an infinite play and counts as a win for the defender.

By the above definition the system is a bisimulation if and only if there exists a winning strategy for the defender.

=== Coalgebraic definition ===

A bisimulation for state transition systems is a special case of [[F-coalgebra|coalgebraic]] bisimulation for the type of covariant powerset [[functor]].
Note that every state transition system &lt;math&gt;(S, \Lambda, \rightarrow)&lt;/math&gt; is [[Bijection|bijectively]] a function &lt;math&gt;\xi_{\rightarrow} &lt;/math&gt; from &lt;math&gt;S&lt;/math&gt; to the [[Power set|powerset]] of &lt;math&gt;S&lt;/math&gt; indexed by &lt;math&gt;\Lambda&lt;/math&gt; written as &lt;math&gt;\mathcal{P}(\Lambda \times S)&lt;/math&gt;, defined by
::&lt;math&gt; p \mapsto \{ (\alpha, q) \in \Lambda \times S : p \overset{\alpha}{\rightarrow} q \}.&lt;/math&gt;

Let &lt;math&gt;\pi_i \colon S \times S \to S&lt;/math&gt; be &lt;math&gt;i&lt;/math&gt;-th [[Product (category theory)|projection]] mapping
&lt;math&gt;(p, q)&lt;/math&gt; to &lt;math&gt;p&lt;/math&gt; and &lt;math&gt;q&lt;/math&gt; respectively for &lt;math&gt;i = 1, 2&lt;/math&gt;; and
&lt;math&gt;\mathcal{P}(\Lambda \times \pi_1)&lt;/math&gt; the forward image of &lt;math&gt;\pi_1&lt;/math&gt; defined by dropping the third component
::&lt;math&gt; P \mapsto \{ (\alpha, p) \in \Lambda \times S : \exists q . (\alpha, p, q) \in P \}&lt;/math&gt;
where &lt;math&gt;P&lt;/math&gt; is a subset of &lt;math&gt;\Lambda \times S \times S&lt;/math&gt;. Similarly for &lt;math&gt;\mathcal{P}(\Lambda \times \pi_2)&lt;/math&gt;.

Using the above notations, a relation &lt;math&gt;R \subseteq S \times S &lt;/math&gt; is a '''bisimulation''' on a transition system &lt;math&gt;(S, \Lambda, \rightarrow)&lt;/math&gt; if and only if there exists a transition system &lt;math&gt;\gamma \colon R \to \mathcal{P}(\Lambda \times R)&lt;/math&gt; on the relation &lt;math&gt;R&lt;/math&gt; such that the [[Commutative diagram|diagram]]

[[Image:Coalgebraic bisimulation.svg|frameless|upright=1.5]]

commutes, i.e. for &lt;math&gt;i = 1, 2&lt;/math&gt;, the equations
:: &lt;math&gt; \xi_\rightarrow \circ \pi_i = \mathcal{P}(\Lambda \times \pi_i) \circ \gamma &lt;/math&gt;
hold
where &lt;math&gt;\xi_{\rightarrow}&lt;/math&gt; is the functional representation of &lt;math&gt;(S, \Lambda, \rightarrow)&lt;/math&gt;.

== Variants of bisimulation ==
In special contexts the notion of bisimulation is sometimes refined by adding additional requirements or constraints. For example, if the state transition system includes a notion of ''silent'' (or ''internal'') action, often denoted with &lt;math&gt;\tau&lt;/math&gt;, i.e. actions that are not visible by external observers, then bisimulation can be relaxed to be ''weak bisimulation'', in which if two states &lt;math&gt;p&lt;/math&gt; and &lt;math&gt;q&lt;/math&gt; are bisimilar and there is some number of internal actions leading from &lt;math&gt;p&lt;/math&gt; to some state &lt;math&gt;p'&lt;/math&gt; then there must exist state &lt;math&gt;q'&lt;/math&gt; such that there is some number (possibly zero) of internal actions leading from &lt;math&gt;q&lt;/math&gt; to &lt;math&gt;q'&lt;/math&gt;. A relation &lt;math&gt;\mathcal{R}&lt;/math&gt; on processes is a weak bisimulation if the following holds (with &lt;math&gt;\mathcal{S} \in \{ \mathcal{R}, \mathcal{R}^{-1} \}&lt;/math&gt;, and &lt;math&gt;a,\tau&lt;/math&gt; being an observable and mute transition respectively):

&lt;math&gt;\forall p, q. \quad (p,q) \in \mathcal{S} \Rightarrow p \stackrel{\tau}{\rightarrow} p' \Rightarrow \exists q' . \quad q \stackrel{\tau^\ast}{\rightarrow} q' \wedge (p',q') \in \mathcal{S} &lt;/math&gt;

&lt;math&gt;\forall p, q. \quad (p,q) \in \mathcal{S} \Rightarrow p \stackrel{a}{\rightarrow} p' \Rightarrow \exists q' . \quad q \stackrel{\tau^\ast a \tau^\ast}{\rightarrow} q' \wedge (p',q') \in \mathcal{S} &lt;/math&gt;

This is closely related to bisimulation [[Up to#Computer science|up to]] a relation.

Typically, if the [[state transition system]] gives the [[operational semantics]] of a [[programming language]], then the precise definition of bisimulation will be specific to the restrictions of the programming language. Therefore, in general, there may be more than one kind of bisimulation, (bisimilarity resp.) relationship depending on the context.

== Bisimulation and modal logic ==

Since [[Kripke semantics|Kripke models]] are a special case of (labelled) state transition systems, bisimulation is also a topic in [[modal logic]]. In fact, modal logic is the fragment of [[first-order logic]] invariant under bisimulation ([[Johan van Benthem (logician)|van Benthem's theorem]]).

== See also ==
* [[Simulation preorder]]
* [[Congruence relation]]
* [[Probabilistic bisimulation]]

== References ==
# {{Cite conference
 | first = David
 | last = Park
 | year = 1981
 | title = Concurrency and Automata on Infinite Sequences
 | conference = Proceedings of the 5th GI-Conference, Karlsruhe
 | booktitle = Theoretical Computer Science
 | series = [[Lecture Notes in Computer Science]]
 | editor = Deussen, Peter
 | pages = 167–183
 | volume = 104
 | publisher = [[Springer-Verlag]]
 | isbn = 978-3-540-10576-3
 | doi = 10.1007/BFb0017309
}}
# {{Cite book
 | last = Milner
 | first = Robin
 | title = Communication and Concurrency
 | year = 1989
 | publisher = [[Prentice Hall]]
 | isbn = 0-13-114984-9
 }}

==Further reading==
* Davide Sangiorgi. (2011). ''Introduction to Bisimulation and Coinduction''. Cambridge University Press. {{ISBN|9781107003637}}

==External links==

=== Software tools ===
* [[CADP]]: [http://cadp.inria.fr tools to minimize and compare finite-state systems according to various bisimulations]
* [[mCRL2]] tools to minimize and compare finite-state systems according to various bisimulations]
* [http://www.brics.dk/bisim/ The Bisimulation Game Game]

{{Authority control}}

[[Category:Theoretical computer science]]
[[Category:Formal methods]]
[[Category:Logic in computer science]]</text>
      <sha1>6365t0vbrfljlxyh639tl9m8k2v2rad</sha1>
    </revision>
  </page>
  <page>
    <title>Cantor–Dedekind axiom</title>
    <ns>0</ns>
    <id>1595873</id>
    <revision>
      <id>830951667</id>
      <parentid>776548558</parentid>
      <timestamp>2018-03-17T22:01:27Z</timestamp>
      <contributor>
        <username>RJGray</username>
        <id>8268674</id>
      </contributor>
      <minor/>
      <comment>/* top */ Changed : to .</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1669">In [[mathematical logic]], the '''Cantor–Dedekind axiom''' is the thesis that the [[Real number#Axiomatic approach|real number]]s are order-[[isomorphic]] to the [[linear continuum]] of [[geometry]]. In other words, the axiom states that there is a one-to-one correspondence between real numbers and points on a line.

This axiom is the cornerstone of [[analytic geometry]]. The [[Cartesian coordinate system]] developed by [[René Descartes]] implicitly assumes this axiom by blending the distinct concepts of real number system with the geometric line or plane into a [[conceptual metaphor]]. This is sometimes referred to as the ''real number line'' blend.&lt;ref&gt;{{cite book | author = George Lakoff and Rafael E. Núñez | title = Where Mathematics Comes From: How the embodied mind brings mathematics into being | publisher=Basic Books|year=2000|isbn= 0-465-03770-4}}&lt;/ref&gt;

A consequence of this axiom is that [[Alfred Tarski|Alfred Tarski's]] proof of the [[decidability (logic)|decidability]] of the ordered real field could be seen as an [[algorithm]] to solve any problem in [[Euclidean geometry]].

==Notes==
{{reflist}}

== References ==

* [[Philip Ehrlich|Ehrlich, P.]] (1994). "General introduction". ''Real Numbers, Generalizations of the Reals, and Theories of Continua'', vi–xxxii. Edited by P. Ehrlich, Kluwer Academic Publishers, Dordrecht
* Bruce E. Meserve (1953) {{Google books|qtuySdnGDtoC|Fundamental Concepts of Algebra|page=32}}
* B.E. Meserve (1955) {{Google books|Y6jDAgAAQBAJ|Fundamental Concepts of Geometry|page=86}}

{{DEFAULTSORT:Cantor-Dedekind axiom}}
[[Category:Real numbers]]
[[Category:Mathematical axioms]]


{{mathlogic-stub}}</text>
      <sha1>4sbksnxufjmz5kxicblh2iw5dia09ld</sha1>
    </revision>
  </page>
  <page>
    <title>Cartesian tensor</title>
    <ns>0</ns>
    <id>1824845</id>
    <revision>
      <id>868794649</id>
      <parentid>868793953</parentid>
      <timestamp>2018-11-14T13:50:38Z</timestamp>
      <contributor>
        <username>JivanP</username>
        <id>31075022</id>
      </contributor>
      <minor/>
      <comment>[[WP:CE]]; rephrasing to avoid potentially ambiguous use of "repeated"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="64795">[[File:Rectangular coordinate systems index lowered.svg|thumb|Two different 3d [[Orthonormal basis|orthonormal bases]]: each basis consists of unit vectors that are mutually perpendicular.]]

In [[geometry]] and [[linear algebra]], a '''Cartesian tensor''' uses an [[orthonormal basis]] to [[Representation (mathematics)|represent]] a [[tensor]] in a [[Euclidean space]] in the form of components. Converting a tensor's components from one such basis to another is through an [[orthogonal transformation]].

The most familiar coordinate systems are the [[Two-dimensional space|two-dimensional]] and [[Three-dimensional space|three-dimensional]] [[Cartesian coordinate]] systems. Cartesian tensors may be used with any Euclidean space, or more technically, any finite-dimensional [[vector space]] over the [[field (mathematics)|field]] of [[real number]]s that has an [[inner product]].

Use of Cartesian tensors occurs in [[physics]] and [[engineering]], such as with the [[Cauchy stress tensor]] and the [[moment of inertia]] tensor in [[rigid body dynamics]]. Sometimes general [[curvilinear coordinates]] are convenient, as in high-deformation [[continuum mechanics]], or even necessary, as in [[general relativity]]. While orthonormal bases may be found for some such coordinate systems (e.g. [[tangent]] to [[Spherical coordinate system|spherical coordinates]]), Cartesian tensors may provide considerable simplification for applications in which rotations of rectilinear coordinate axes suffice. The transformation is a [[passive transformation]], since the coordinates are changed and not the physical system.

==Cartesian basis and related terminology==

===Vectors in three dimensions===

In [[three-dimensional space|3d]] [[Euclidean space]], ℝ&lt;sup&gt;3&lt;/sup&gt;, the [[standard basis]] is '''e'''&lt;sub&gt;x&lt;/sub&gt;, '''e'''&lt;sub&gt;y&lt;/sub&gt;, '''e'''&lt;sub&gt;z&lt;/sub&gt;. Each basis vector points along the x-, y-, and z-axes, and the vectors are all [[unit vector]]s (or normalized), so the basis is [[orthonormal]].

Throughout, when referring to [[Cartesian coordinates]] in [[three dimensions]], a right-handed system is assumed and this is much more common than a left-handed system in practice, see [[orientation (vector space)]] for details.
simi
For Cartesian tensors of order 1, a Cartesian vector '''a''' can be written algebraically as a [[linear combination]] of the basis vectors '''e'''&lt;sub&gt;x&lt;/sub&gt;, '''e'''&lt;sub&gt;y&lt;/sub&gt;, '''e'''&lt;sub&gt;z&lt;/sub&gt;:

:&lt;math&gt;\mathbf{a} = a_\text{x}\mathbf{e}_\text{x} + a_\text{y}\mathbf{e}_\text{y} + a_\text{z}\mathbf{e}_\text{z} &lt;/math&gt;

where the [[coordinate vector|coordinate]]s of the vector with respect to the Cartesian basis are denoted ''a''&lt;sub&gt;x&lt;/sub&gt;, ''a''&lt;sub&gt;y&lt;/sub&gt;, ''a''&lt;sub&gt;z&lt;/sub&gt;. It is common and helpful to display the basis vectors as [[column vector]]s

:&lt;math&gt; \mathbf{e}_\text{x} = \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix} \,,\quad \mathbf{e}_\text{y} = \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix} \,,\quad \mathbf{e}_\text{z} = \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} &lt;/math&gt;

when we have a [[coordinate vector]] in a column vector representation:

:&lt;math&gt;\mathbf{a} = \begin{pmatrix} a_\text{x} \\ a_\text{y} \\ a_\text{z} \end{pmatrix} &lt;/math&gt;

A [[row vector]] representation is also legitimate, although in the context of general curvilinear coordinate systems the row and column vector representations are used separately for specific reasons – see [[Einstein notation]] and [[covariance and contravariance of vectors]] for why.

The term "component" of a vector is ambiguous: it could refer to:

*a specific coordinate of the vector such as ''a''&lt;sub&gt;z&lt;/sub&gt; (a scalar), and similarly for ''x'' and ''y'', or
*the coordinate scalar-multiplying the corresponding basis vector, in which case the "y-component" of '''a''' is ''a''&lt;sub&gt;y&lt;/sub&gt;'''e'''&lt;sub&gt;y&lt;/sub&gt; (a vector), and similarly for ''x'' and ''z''.

A more general notation is [[tensor index notation]], which has the flexibility of numerical values rather than fixed coordinate labels. {{anchor|indices replace labels}}&lt;!---PLEASE DON'T DELETE THIS ANCHOR, IT'S USED LATER---&gt;The Cartesian labels are replaced by tensor indices in the basis vectors '''e'''&lt;sub&gt;x&lt;/sub&gt; ↦ '''e'''&lt;sub&gt;1&lt;/sub&gt;, '''e'''&lt;sub&gt;y&lt;/sub&gt; ↦ '''e'''&lt;sub&gt;2&lt;/sub&gt;, '''e'''&lt;sub&gt;z&lt;/sub&gt; ↦ '''e'''&lt;sub&gt;3&lt;/sub&gt; and coordinates ''A''&lt;sub&gt;x&lt;/sub&gt; ↦ ''A''&lt;sub&gt;1&lt;/sub&gt;, ''A''&lt;sub&gt;y&lt;/sub&gt; ↦ ''A''&lt;sub&gt;2&lt;/sub&gt;, ''A''&lt;sub&gt;z&lt;/sub&gt; ↦ ''A''&lt;sub&gt;3&lt;/sub&gt;. In general, the notation '''e'''&lt;sub&gt;1&lt;/sub&gt;, '''e'''&lt;sub&gt;2&lt;/sub&gt;, '''e'''&lt;sub&gt;3&lt;/sub&gt; refers to ''any'' basis, and ''A''&lt;sub&gt;1&lt;/sub&gt;, ''A''&lt;sub&gt;2&lt;/sub&gt;, ''A''&lt;sub&gt;3&lt;/sub&gt; refers to the corresponding coordinate system; although here they are restricted to the Cartesian system. Then:

:&lt;math&gt;\mathbf{a} = a_1\mathbf{e}_1 + a_2\mathbf{e}_2 + a_3\mathbf{e}_3 = \sum_{i=1}^3 a_i\mathbf{e}_i&lt;/math&gt;

It is standard to use the [[Einstein notation]]—the summation sign for summation over an index that is present exactly twice within a term may be suppressed for notational conciseness:

:&lt;math&gt;\mathbf{a} = \sum_{i=1}^3 a_i\mathbf{e}_i \equiv a_i\mathbf{e}_i &lt;/math&gt;

An advantage of the index notation over coordinate-specific notations is the independence of the dimension of the underlying vector space, i.e. the same expression on the right hand side takes the same form in higher dimensions (see below). Previously, the Cartesian labels x, y, z were just labels and ''not'' indices. (It is informal to say "''i'' = x, y, z").

===Second order tensors in three dimensions===

A [[dyadic tensor]] '''T''' is an order 2 tensor formed by the [[tensor product]] &amp;otimes; of two Cartesian vectors '''a''' and '''b''', written '''T''' = '''a''' &amp;otimes; '''b'''. Analogous to vectors, it can be written as a linear combination of the tensor basis {{nowrap|'''e'''&lt;sub&gt;x&lt;/sub&gt; &amp;otimes; '''e'''&lt;sub&gt;x&lt;/sub&gt; ≡ '''e'''&lt;sub&gt;xx&lt;/sub&gt;}}, {{nowrap|'''e'''&lt;sub&gt;x&lt;/sub&gt; &amp;otimes; '''e'''&lt;sub&gt;y&lt;/sub&gt; ≡ '''e'''&lt;sub&gt;xy&lt;/sub&gt;}}, ..., {{nowrap|'''e'''&lt;sub&gt;z&lt;/sub&gt; &amp;otimes; '''e'''&lt;sub&gt;z&lt;/sub&gt; ≡ '''e'''&lt;sub&gt;zz&lt;/sub&gt;}} (the right hand side of each identity is only an abbreviation, nothing more):

:&lt;math&gt;\begin{array}{ccl}
\mathbf{T} &amp; = &amp; \left(a_\text{x}\mathbf{e}_\text{x} + a_\text{y}\mathbf{e}_\text{y} + a_\text{z}\mathbf{e}_\text{z}\right)\otimes\left(b_\text{x}\mathbf{e}_\text{x} + b_\text{y}\mathbf{e}_\text{y} + b_\text{z}\mathbf{e}_\text{z}\right) \\
&amp;  &amp; \\
&amp; = &amp; a_\text{x} b_\text{x} \mathbf{e}_\text{x} \otimes \mathbf{e}_\text{x} + a_\text{x} b_\text{y}\mathbf{e}_\text{x} \otimes \mathbf{e}_\text{y} + a_\text{x} b_\text{z}\mathbf{e}_\text{x} \otimes \mathbf{e}_\text{z} \\
&amp; &amp; {} + a_\text{y} b_\text{x}\mathbf{e}_\text{y} \otimes \mathbf{e}_\text{x} + a_\text{y} b_\text{y}\mathbf{e}_\text{y} \otimes \mathbf{e}_\text{y} + a_\text{y} b_\text{z}\mathbf{e}_\text{y} \otimes \mathbf{e}_\text{z} \\
&amp; &amp; {} + a_\text{z} b_\text{x} \mathbf{e}_\text{z} \otimes \mathbf{e}_\text{x} + a_\text{z} b_\text{y}\mathbf{e}_\text{z} \otimes \mathbf{e}_\text{y} + a_\text{z} b_\text{z}\mathbf{e}_\text{z} \otimes \mathbf{e}_\text{z}  \\
\end{array}&lt;/math&gt;

Representing each basis tensor as a matrix:

:&lt;math&gt; 
{\mathbf{e}_\text{x} \otimes \mathbf{e}_\text{x}} \equiv \mathbf{e}_\text{xx} = \begin{pmatrix} 
1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0
\end{pmatrix}\,,\quad
{\mathbf{e}_\text{x} \otimes \mathbf{e}_\text{y}} \equiv \mathbf{e}_\text{xy} = \begin{pmatrix} 
0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0
\end{pmatrix}\,,\cdots \quad {\mathbf{e}_\text{z} \otimes \mathbf{e}_\text{z}} \equiv \mathbf{e}_\text{zz} = \begin{pmatrix} 
0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1
\end{pmatrix}&lt;/math&gt;

then '''T''' can be represented more systematically as a matrix:

:&lt;math&gt; \mathbf{T} = \begin{pmatrix} 
a_\text{x} b_\text{x} &amp; a_\text{x} b_\text{y} &amp; a_\text{x} b_\text{z} \\ 
a_\text{y} b_\text{x} &amp; a_\text{y} b_\text{y} &amp; a_\text{y} b_\text{z} \\
a_\text{z} b_\text{x} &amp; a_\text{z} b_\text{y} &amp; a_\text{z} b_\text{z}
\end{pmatrix}&lt;/math&gt;

See [[matrix multiplication#The inner and outer products|matrix multiplication]] for the notational correspondence between matrices and the dot and tensor products.

More generally, whether or not '''T''' is a tensor product of two vectors, it is always a linear combination of the basis tensors with coordinates ''T''&lt;sub&gt;xx&lt;/sub&gt;, ''T''&lt;sub&gt;xy&lt;/sub&gt;, ... ''T''&lt;sub&gt;zz&lt;/sub&gt;:

:&lt;math&gt;\begin{array}{ccl}
\mathbf{T}&amp; = &amp; T_\text{xx}\mathbf{e}_\text{xx} + T_\text{xy}\mathbf{e}_\text{xy} + T_\text{xz}\mathbf{e}_\text{xz} \\
&amp; &amp; {} + T_\text{yx}\mathbf{e}_\text{yx} + T_\text{yy}\mathbf{e}_\text{yy} + T_\text{yz}\mathbf{e}_\text{yz} \\
&amp; &amp; {} + T_\text{zx}\mathbf{e}_\text{zx} + T_\text{zy}\mathbf{e}_\text{zy} + T_\text{zz}\mathbf{e}_\text{zz}  
\end{array}&lt;/math&gt;

while in terms of tensor indices:

:&lt;math&gt;\mathbf{T} = T_{ij} \mathbf{e}_{ij} \equiv \sum_{ij} T_{ij} \mathbf{e}_i \otimes \mathbf{e}_j \,,&lt;/math&gt;

and in matrix form:

:&lt;math&gt; \mathbf{T} = \begin{pmatrix} 
T_\text{xx} &amp; T_\text{xy} &amp; T_\text{xz} \\ 
T_\text{yx} &amp; T_\text{yy} &amp; T_\text{yz} \\
T_\text{zx} &amp; T_\text{zy} &amp; T_\text{zz}
\end{pmatrix}&lt;/math&gt;

Second order tensors occur naturally in physics and engineering when physical quantities have directional dependence in the system, often in a "stimulus-response" way. This can be mathematically seen through one aspect of tensors - they are [[multilinear function]]s. A second order tensor '''T''' which takes in a vector '''u''' of some magnitude and direction will return a vector '''v'''; of a different magnitude and in a different direction to '''u''', in general. The notation used for [[function (mathematics)|function]]s in [[mathematical analysis]] leads us to write {{nowrap|'''v''' {{=}} '''T'''('''u''')}},&lt;ref name="MTW notation"&gt;{{cite book|author=[[Charles W. Misner|C.W. Misner]], [[Kip S. Thorne|K.S. Thorne]], [[John A. Wheeler|J.A. Wheeler]]|title=[[Gravitation (book)|Gravitation]]|page=|isbn=0-7167-0344-0}}, used throughout&lt;/ref&gt; while the same idea can be expressed in matrix and index notations&lt;ref name="Kibble notation"&gt;{{cite book|title=classical mechanics|year=1973|publisher=McGraw Hill|edition=2nd|series=European physics series|author=T. W. B. Kibble|isbn=978-0-07-084018-8}}, see Appendix C.&lt;/ref&gt; (including the summation convention), respectively:

:&lt;math&gt; \begin{pmatrix} 
v_\text{x} \\ 
v_\text{y} \\
v_\text{z}
\end{pmatrix} = \begin{pmatrix} 
T_\text{xx} &amp; T_\text{xy} &amp; T_\text{xz} \\ 
T_\text{yx} &amp; T_\text{yy} &amp; T_\text{yz} \\
T_\text{zx} &amp; T_\text{zy} &amp; T_\text{zz}
\end{pmatrix}\begin{pmatrix} 
u_\text{x} \\ 
u_\text{y} \\
u_\text{z}
\end{pmatrix}\,,\quad v_i = T_{ij}u_j &lt;/math&gt;

By "linear", if {{nowrap|'''u''' {{=}} ''ρ'''''r''' + ''σ'''''s'''}} for two scalars ''ρ'' and ''σ'' and vectors '''r''' and '''s''', then in function and index notations:

:&lt;math&gt; \mathbf{v} = \mathbf{T}(\rho\mathbf{r} + \sigma\mathbf{s}) = \rho\mathbf{T}(\mathbf{r}) + \sigma\mathbf{T}(\mathbf{s}) &lt;/math&gt;
:&lt;math&gt; v_i = T_{ij}(\rho r_j + \sigma s_j) = \rho T_{ij} r_j + \sigma T_{ij} s_j &lt;/math&gt;

and similarly for the matrix notation. The function, matrix, and index notations all mean the same thing. The matrix forms provide a clear display of the components, while the index form allows easier tensor-algebraic manipulation of the formulae in a compact manner. Both provide the physical interpretation of ''directions''; vectors have one direction, while second order tensors connect two directions together. One can associate a tensor index or coordinate label with a basis vector direction.

The use of second order tensors are the minimum to describe changes in magnitudes and directions of vectors, as the [[dot product]] of two vectors is always a scalar, while the [[cross product]] of two vectors is always a pseudovector perpendicular to the plane defined by the vectors, so these products of vectors alone cannot obtain a new vector of any magnitude in any direction. (See also below for more on the dot and cross products). The tensor product of two vectors is a second order tensor, although this has no obvious directional interpretation by itself.

The previous idea can be continued: if '''T''' takes in two vectors '''p''' and '''q''', it will return a scalar ''r''. In function notation we write ''r'' = '''T'''('''p''', '''q'''), while in matrix and index notations (including the summation convention) respectively:

:&lt;math&gt; r = \begin{pmatrix} 
p_\text{x} &amp; 
p_\text{y} &amp;
p_\text{z}
\end{pmatrix}\begin{pmatrix} 
T_\text{xx} &amp; T_\text{xy} &amp; T_\text{xz} \\ 
T_\text{yx} &amp; T_\text{yy} &amp; T_\text{yz} \\
T_\text{zx} &amp; T_\text{zy} &amp; T_\text{zz}
\end{pmatrix}\begin{pmatrix} 
q_\text{x} \\ 
q_\text{y} \\
q_\text{z}
\end{pmatrix}\,,\quad r = p_i T_{ij} q_j &lt;/math&gt;

The tensor '''T''' is linear in both input vectors. When vectors and tensors are written without reference to components, and indices are not used, sometimes a dot · is placed where summations over indices (known as [[tensor contraction]]s) are taken. For the above cases:&lt;ref name="MTW notation"/&gt;&lt;ref name="Kibble notation"/&gt;

:&lt;math&gt;\mathbf{v} = \mathbf{T}\cdot\mathbf{u}&lt;/math&gt;
:&lt;math&gt; r = \mathbf{p}\cdot\mathbf{T}\cdot\mathbf{q}&lt;/math&gt;

motivated by the dot product notation:

:&lt;math&gt;\mathbf{a}\cdot\mathbf{b} \equiv a_i b_i&lt;/math&gt;

More generally, a tensor of order ''m'' which takes in ''n'' vectors (where ''n'' is between 0 and ''m'' inclusive) will return a tensor of order {{nowrap|''m'' − ''n''}}, see [[tensor#As multilinear maps|Tensor: As multilinear maps]] for further generalizations and details. The concepts above also apply to pseudovectors in the same way as for vectors. The vectors and tensors themselves can vary within throughout space, in which case we have [[vector field]]s and [[tensor field]]s, and can also depend on time.

Following are some examples:

:{| class="wikitable"
|-
! scope="col" width="200" | An applied or given...
! scope="col" width="200" | ...to a material or object of...
! scope="col" width="200" | ...results in...
! scope="col" width="200" | ...in the material or object, given by:
|-
| [[unit vector]] '''n''' || [[Cauchy stress tensor]] '''σ''' || a traction force '''t''' || &lt;math&gt;\mathbf{t}=\boldsymbol{\sigma}\cdot\mathbf{n}&lt;/math&gt;
|-
|scope="row" rowspan="2"| [[angular velocity]] '''ω''' || [[moment of inertia]] '''I''' || an [[angular momentum]] '''J''' || &lt;math&gt;\mathbf{J}=\mathbf{I}\cdot\boldsymbol{\omega}&lt;/math&gt;
|-
|| [[moment of inertia]] '''I''' || a rotational [[kinetic energy]] ''T'' || &lt;math&gt;T=\frac{1}{2}\boldsymbol{\omega}\cdot\mathbf{I}\cdot\boldsymbol{\omega}&lt;/math&gt;
|-
|scope="row" rowspan="2"| [[electric field]] '''E''' || [[electrical conductivity]] '''σ''' || a [[current density]] flow '''J''' || &lt;math&gt;\mathbf{J}=\boldsymbol{\sigma}\cdot\mathbf{E}&lt;/math&gt;
|-
 || [[polarizability]] '''α''' (related to the [[permittivity]] '''ε''' and [[electric susceptibility]] '''χ'''&lt;sub&gt;E&lt;/sub&gt;) || an induced [[polarization density|polarization]] field '''P''' || &lt;math&gt;\mathbf{P}=\boldsymbol{\alpha}\cdot\mathbf{E}&lt;/math&gt;
|-
| [[magnetic field|magnetic '''H''' field]] || [[magnetic permeability]] '''μ''' || a [[magnetic field|magnetic '''B''' field]] || &lt;math&gt;\mathbf{B}=\boldsymbol{\mu}\cdot\mathbf{H}&lt;/math&gt;
|-
|}

For the electrical conduction example, the index and matrix notations would be:

:&lt;math&gt;J_i = \sigma_{ij}E_j \equiv \sum_{j} \sigma_{ij}E_j &lt;/math&gt;
:&lt;math&gt;\begin{pmatrix} J_\text{x} \\ J_\text{y} \\ J_\text{z} \end{pmatrix} = \begin{pmatrix} 
\sigma_\text{xx} &amp; \sigma_\text{xy} &amp; \sigma_\text{xz} \\ 
\sigma_\text{yx} &amp; \sigma_\text{yy} &amp; \sigma_\text{yz} \\
\sigma_\text{zx} &amp; \sigma_\text{zy} &amp; \sigma_\text{zz}
\end{pmatrix} \begin{pmatrix} E_\text{x} \\ E_\text{y} \\ E_\text{z} \end{pmatrix} &lt;/math&gt;

while for the rotational kinetic energy ''T'':

:&lt;math&gt;T = \frac{1}{2} \omega_i I_{ij} \omega_j \equiv \frac{1}{2} \sum_{ij} \omega_i I_{ij} \omega_j \,,&lt;/math&gt;
:&lt;math&gt;T = \frac{1}{2} \begin{pmatrix} \omega_\text{x} &amp; \omega_\text{y} &amp; \omega_\text{z} \end{pmatrix} \begin{pmatrix} 
I_\text{xx} &amp; I_\text{xy} &amp; I_\text{xz} \\ 
I_\text{yx} &amp; I_\text{yy} &amp; I_\text{yz} \\
I_\text{zx} &amp; I_\text{zy} &amp; I_\text{zz}
\end{pmatrix} \begin{pmatrix} \omega_\text{x} \\ \omega_\text{y} \\ \omega_\text{z} \end{pmatrix} \,.&lt;/math&gt;

See also [[constitutive equation]] for more specialized examples.

===Vectors and tensors in ''n'' dimensions===

In ''n''-dimensional Euclidean space over the real numbers, ℝ&lt;sup&gt;''n''&lt;/sup&gt;, the standard basis is denoted '''e'''&lt;sub&gt;1&lt;/sub&gt;, '''e'''&lt;sub&gt;2&lt;/sub&gt;, '''e'''&lt;sub&gt;3&lt;/sub&gt;, ... '''e'''&lt;sub&gt;''n''&lt;/sub&gt;. Each basis vector '''e'''&lt;sub&gt;''i''&lt;/sub&gt; points along the positive ''x&lt;sub&gt;i&lt;/sub&gt;'' axis, with the basis being orthonormal. Component ''j'' of '''e'''&lt;sub&gt;''i''&lt;/sub&gt; is given by the [[Kronecker delta]]:

:&lt;math&gt;(\mathbf{e}_i)_j = \delta_{ij} &lt;/math&gt;

A vector in ℝ&lt;sup&gt;''n''&lt;/sup&gt; takes the form:

:&lt;math&gt;\mathbf{a} = a_i\mathbf{e}_i \equiv \sum_i a_i\mathbf{e}_i \,.&lt;/math&gt;

Similarly for the order 2 tensor above, for each vector '''a''' and '''b''' in ℝ&lt;sup&gt;''n''&lt;/sup&gt;:

:&lt;math&gt;\mathbf{T} = a_i b_j \mathbf{e}_{ij} \equiv \sum_{ij} a_i b_j \mathbf{e}_i \otimes \mathbf{e}_j \,,&lt;/math&gt;

or more generally:

:&lt;math&gt; \mathbf{T} = T_{ij} \mathbf{e}_{ij} \equiv \sum_{ij} T_{ij} \mathbf{e}_i \otimes \mathbf{e}_j \,.&lt;/math&gt;

==Transformations of Cartesian vectors (any number of dimensions)==

[[File:Rectangular coordinate system position vector index lowered.svg|thumb|The same [[position vector]] '''x''' represented in two 3d rectangular coordinate systems each with an [[orthonormal basis]], the cuboids illustrate the [[parallelogram law]] for adding vector components.]]

===Meaning of "invariance" under coordinate transformations===

The [[position vector]] '''x''' in ℝ&lt;sup&gt;''n''&lt;/sup&gt; is a simple and common example of a vector, and can be represented in ''any'' [[coordinate system]]. Consider the case of [[rectangular coordinate system]]s with orthonormal bases only. It is possible to have a coordinate system with rectangular geometry if the basis vectors are all mutually perpendicular and not normalized, in which case the basis is ortho''gonal'' but not ortho''normal''. However, orthonormal bases are easier to manipulate and are often used in practice. The following results are true for orthonormal bases, not orthogonal ones.

In one rectangular coordinate system, '''x''' as a contravector has coordinates ''x&lt;sup&gt;i&lt;/sup&gt;'' and basis vectors '''e'''&lt;sub&gt;''i''&lt;/sub&gt;, while as a covector it has coordinates ''x&lt;sub&gt;i&lt;/sub&gt;'' and basis covectors '''e'''&lt;sup&gt;''i''&lt;/sup&gt;, and we have:

:&lt;math&gt;\mathbf{x}=x^i\mathbf{e}_i\,,\quad \mathbf{x}=x_i\mathbf{e}^i&lt;/math&gt;

In another rectangular coordinate system, '''x''' as a contravector has coordinates ''{{overline|x}}&lt;sup&gt;i&lt;/sup&gt;'' and bases {{overline|'''e'''}}&lt;sub&gt;''i''&lt;/sub&gt;, while as a covector it has coordinates ''{{overline|x}}&lt;sub&gt;i&lt;/sub&gt;'' and bases {{overline|'''e'''}}&lt;sup&gt;''i''&lt;/sup&gt;, and we have:

:&lt;math&gt;\mathbf{x}=\bar{x}^i\bar{\mathbf{e}}_i\,,\quad \mathbf{x}=\bar{x}_i\bar{\mathbf{e}}^i&lt;/math&gt;

Each new coordinate is a function of all the old ones, and vice versa for the [[inverse function]]:

:&lt;math&gt;\bar{x}{}^i=\bar{x}{}^i\left(x^1,x^2,\cdots\right) \quad \rightleftharpoons \quad x{}^i = x{}^i\left(\bar{x}^1,\bar{x}^2,\cdots\right)&lt;/math&gt;

:&lt;math&gt;\bar{x}{}_i=\bar{x}{}_i\left(x_1,x_2,\cdots\right) \quad \rightleftharpoons \quad x{}_i = x{}_i\left(\bar{x}_1,\bar{x}_2,\cdots\right)&lt;/math&gt;

and similarly each new basis vector is a function of all the old ones, and vice versa for the inverse function:

:&lt;math&gt;\bar{\mathbf{e}}{}_j = \bar{\mathbf{e}}{}_j\left(\mathbf{e}_1,\mathbf{e}_2\cdots\right) \quad \rightleftharpoons \quad \mathbf{e}{}_j = \mathbf{e}{}_j \left(\bar{\mathbf{e}}_1,\bar{\mathbf{e}}_2\cdots\right)&lt;/math&gt;

:&lt;math&gt;\bar{\mathbf{e}}{}^j = \bar{\mathbf{e}}{}^j\left(\mathbf{e}^1,\mathbf{e}^2\cdots\right) \quad \rightleftharpoons \quad \mathbf{e}{}^j = \mathbf{e}{}^j \left(\bar{\mathbf{e}}^1,\bar{\mathbf{e}}^2\cdots\right)&lt;/math&gt;

for all ''i'', ''j''.

A vector is invariant under any change of basis, so if coordinates transform according to a [[transformation matrix]] '''L''', the bases transform according to the [[matrix inverse]] '''L'''&lt;sup&gt;−1&lt;/sup&gt;, and conversely if the coordinates transform according to inverse '''L'''&lt;sup&gt;−1&lt;/sup&gt;, the bases transform according to the matrix '''L'''. The difference between each of these transformations is shown conventionally through the indices as superscripts for contravariance and subscripts for covariance, and the coordinates and bases are [[linear transformation|linearly transformed]] according to the following rules:

:{| class="wikitable"
|-
! Vector elements
! Contravariant transformation law
! Covariant transformation law
|-
! Coordinates
| &lt;math&gt;\bar{x}^j = x^i (\boldsymbol{\mathsf{L}})_i{}^j = x^i \mathsf{L}_i{}^j&lt;/math&gt; 
| &lt;math&gt;\bar{x}_j = x_k(\boldsymbol{\mathsf{L}}^{-1})_j{}^k&lt;/math&gt;
|-
! Basis
| &lt;math&gt;\bar{\mathbf{e}}_j = (\boldsymbol{\mathsf{L}}^{-1})_j{}^k\mathbf{e}_k&lt;/math&gt;
| &lt;math&gt;\bar{\mathbf{e}}^j = (\boldsymbol{\mathsf{L}})_i{}^j \mathbf{e}^i = \mathsf{L}_i{}^j \mathbf{e}^i&lt;/math&gt;
|-
! Any vector
| &lt;math&gt;\bar{x}^j \bar{\mathbf{e}}_j = x^i \mathsf{L}_i{}^j (\boldsymbol{\mathsf{L}}^{-1})_j{}^k \mathbf{e}_k = x^i \delta_i{}^k \mathbf{e}_k = x^i \mathbf{e}_i &lt;/math&gt;
| &lt;math&gt;\bar{x}_j \bar{\mathbf{e}}^j = x_i (\boldsymbol{\mathsf{L}}^{-1})_j{}^i \mathsf{L}_k{}^j \mathbf{e}^k = x_i \delta^i{}_k \mathbf{e}^k = x_i \mathbf{e}^i&lt;/math&gt;
|-
|}

where L&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;''j''&lt;/sup&gt; represents the entries of the [[transformation matrix]] (row number is ''i'' and column number is ''j'') and ('''L'''&lt;sup&gt;−1&lt;/sup&gt;)&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;''k''&lt;/sup&gt; denotes the entries of the [[inverse matrix]] of the matrix L&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;''k''&lt;/sup&gt;.

If '''L''' is an [[orthogonal transformation]] ([[orthogonal matrix]]), the objects transforming by it are defined as '''Cartesian tensors'''. This geometrically has the interpretation that a rectangular coordinate system is mapped to another rectangular coordinate system, in which the [[Norm (mathematics)|norm]] of the vector '''x''' is preserved (and distances are preserved).

The [[determinant]] of '''L''' is det('''L''') = ±1, which corresponds to two types of orthogonal transformation: (+1) for [[rotation (mathematics)|rotations]] and (−1) for [[improper rotation]]s (including [[reflection (mathematics)|reflection]]s).

There are considerable algebraic simplifications, the [[matrix transpose]] is the [[inverse matrix|inverse]] from the definition of an orthogonal transformation:

:&lt;math&gt;\boldsymbol{\mathsf{L}}^{\mathrm{T}}=\boldsymbol{\mathsf{L}}^{-1}\Rightarrow(\boldsymbol{\mathsf{L}}^{-1})_i{}^j=(\boldsymbol{\mathsf{L}}^{\mathrm{T}})_i{}^j=(\boldsymbol{\mathsf{L}})^j{}_i=\mathsf{L}^j{}_i&lt;/math&gt;

From the previous table, orthogonal transformations of covectors and contravectors are identical. There is no need to differ between [[raising and lowering indices]], and in this context and applications to physics and engineering the indices are usually all subscripted to remove confusion for [[exponent]]s. All indices will be lowered in the remainder of this article. One can determine the actual raised and lowered indices by considering which quantities are covectors or contravectors, and the relevant transformation rules.

Exactly the same transformation rules apply to any vector '''a''', not only the position vector. If its components ''a''&lt;sub&gt;''i''&lt;/sub&gt; do not transform according to the rules, '''a''' is not a vector.

Despite the similarity between the expressions above, for the change of coordinates such as {{nowrap|''{{overline|x}}&lt;sup&gt;j&lt;/sup&gt;'' {{=}} '''L'''&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;''j''&lt;/sup&gt;''x&lt;sup&gt;i&lt;/sup&gt;''}}, and the action of a tensor on a vector like {{nowrap|''b&lt;sub&gt;i&lt;/sub&gt;'' {{=}} ''T&lt;sub&gt;ij&lt;/sub&gt;a&lt;sub&gt;j&lt;/sub&gt;''}}, '''L''' is not a tensor, but '''T''' is. In the change of coordinates, '''L''' is a ''matrix'', used to relate two rectangular coordinate systems with orthonormal bases together. For the tensor relating a vector to a vector, the vectors and tensors throughout the equation all belong to the same coordinate system and basis.

===Derivatives and Jacobian matrix elements===

The entries of '''L''' are [[partial derivative]]s of the new or old coordinates with respect to the old or new coordinates, respectively.

Differentiating ''{{overline|x}}&lt;sub&gt;i&lt;/sub&gt;'' with respect to ''x&lt;sub&gt;k&lt;/sub&gt;'':

:&lt;math&gt;\frac{\partial\bar{x}_i}{\partial x_k}=\frac{\partial}{\partial x_k}(x_j \mathsf{L}_{ji})=\mathsf{L}_{ji}\frac{\partial x_j}{\partial x_k}= \delta_{kj}\mathsf{L}_{ji} = \mathsf{L}_{ki}&lt;/math&gt;

so

:&lt;math&gt;\mathsf{L}_i{}^j \equiv \mathsf{L}_{ij} = \frac{\partial\bar{x}_j}{\partial x_i} &lt;/math&gt;

is an element of the [[Jacobian matrix]]. There is a (partially mnemonical) correspondence between index positions attached to '''L''' and in the partial derivative: ''i'' at the top and ''j'' at the bottom, in each case, although for Cartesian tensors the indices can be lowered.

Conversely, differentiating ''x&lt;sub&gt;j&lt;/sub&gt;'' with respect to ''{{overline|x}}&lt;sub&gt;i&lt;/sub&gt;'':

:&lt;math&gt;\frac{\partial x_j}{\partial\bar{x}_k}=\frac{\partial}{\partial\bar{x}_k}(\bar{x}_i(\boldsymbol{\mathsf{L}}^{-1})_{ij})=\frac{\partial\bar{x}_i}{\partial\bar{x}_k}(\boldsymbol{\mathsf{L}}^{-1})_{ij}=\delta_{ki} (\boldsymbol{\mathsf{L}}^{-1})_{ij}=(\boldsymbol{\mathsf{L}}^{-1})_{kj}&lt;/math&gt;
 
so

:&lt;math&gt;(\boldsymbol{\mathsf{L}}^{-1})_i{}^j \equiv (\boldsymbol{\mathsf{L}}^{-1})_{ij}=\frac{\partial x_j}{\partial\bar{x}_i}&lt;/math&gt;

is an element of the inverse Jacobian matrix, with a similar index correspondence.
 
Many sources state transformations in terms of the partial derivatives:

{{Equation box 1
|indent =:
|equation = &lt;math&gt;\begin{array}{c}
\bar{x}_j = x_i\frac{\partial\bar{x}_j}{\partial x_i} \\
\upharpoonleft\downharpoonright\\
x_j = \bar{x}_i\frac{\partial x_j}{\partial\bar{x}_i}
\end{array}
&lt;/math&gt;
|cellpadding= 6
|border = 1
|border colour = black
|background colour=white}}

and the explicit matrix equations in 3d are:

:&lt;math&gt;\bar{\mathbf{x}}=\boldsymbol{\mathsf{L}}\mathbf{x}&lt;/math&gt;

:&lt;math&gt;\begin{pmatrix}\bar{x}_1\\
\bar{x}_2\\
\bar{x}_3
\end{pmatrix}=\begin{pmatrix}\frac{\partial\bar{x}_1}{\partial x_1} &amp; \frac{\partial\bar{x}_1}{\partial x_2} &amp; \frac{\partial\bar{x}_1}{\partial x_3}\\
\frac{\partial\bar{x}_2}{\partial x_1} &amp; \frac{\partial\bar{x}_2}{\partial x_2} &amp; \frac{\partial\bar{x}_2}{\partial x_3}\\
\frac{\partial\bar{x}_3}{\partial x_1} &amp; \frac{\partial\bar{x}_3}{\partial x_2} &amp; \frac{\partial\bar{x}_3}{\partial x_3}
\end{pmatrix}\begin{pmatrix}x_1\\
x_2\\
x_3
\end{pmatrix}
&lt;/math&gt;

similarly for

:&lt;math&gt;\mathbf{x}=\boldsymbol{\mathsf{L}}^{-1}\bar{\mathbf{x}}=\boldsymbol{\mathsf{L}}^{\mathrm{T}}\bar{\mathbf{x}}&lt;/math&gt;

===Projections along coordinate axes===

[[File:Rectangular coordinate systems angles index lowered.svg|500px|thumb|'''Top:''' Angles from the ''x&lt;sub&gt;i&lt;/sub&gt;'' axes to the ''{{overline|x}}&lt;sub&gt;i&lt;/sub&gt;'' axes. '''Bottom:''' Vice versa.]]

As with all linear transformations, '''L''' depends on the basis chosen. For two orthonormal bases

:&lt;math&gt;\bar{\mathbf{e}}_i\cdot\bar{\mathbf{e}}_j=\mathbf{e}_i\cdot\mathbf{e}_j=\delta_{ij}\,,\quad\left|\mathbf{e}_i\right|=\left|\bar{\mathbf{e}}_i\right|=1\,,&lt;/math&gt;
 
* projecting '''x''' to the ''{{overline|x}}'' axes: &lt;math&gt;\bar{x}_i=\bar{\mathbf{e}}_i\cdot\mathbf{x}=\bar{\mathbf{e}}_i\cdot x_j\mathbf{e}_j=x_i \mathsf{L}_{ij} \,, &lt;/math&gt;
* projecting '''x''' to the ''x'' axes: &lt;math&gt;x_i=\mathbf{e}_i\cdot\mathbf{x}=\mathbf{e}_i\cdot\bar{x}_j\bar{\mathbf{e}}_j=\bar{x}_j(\boldsymbol{\mathsf{L}}^{-1})_{ji} \,.&lt;/math&gt;

Hence the components reduce to [[direction cosine]]s between the ''{{overline|x}}&lt;sub&gt;i&lt;/sub&gt;'' and ''x&lt;sub&gt;j&lt;/sub&gt;'' axes:

:&lt;math&gt;\mathsf{L}_{ij} = \bar{\mathbf{e}}_i\cdot\mathbf{e}_j=\cos\theta_{ij}&lt;/math&gt;
:&lt;math&gt;(\boldsymbol{\mathsf{L}}^{-1})_{ij} = \mathbf{e}_i\cdot\bar{\mathbf{e}}_j=\cos\theta_{ji}&lt;/math&gt;

where ''θ&lt;sub&gt;ij&lt;/sub&gt;'' and ''θ&lt;sub&gt;ji&lt;/sub&gt;'' are the angles between the ''{{overline|x}}&lt;sub&gt;i&lt;/sub&gt;'' and ''x&lt;sub&gt;j&lt;/sub&gt;'' axes. In general, ''θ&lt;sub&gt;ij&lt;/sub&gt;'' is not equal to ''θ&lt;sub&gt;ji&lt;/sub&gt;'', because for example ''θ''&lt;sub&gt;12&lt;/sub&gt; and ''θ''&lt;sub&gt;21&lt;/sub&gt; are two different angles.

The transformation of coordinates can be written:

{{Equation box 1
|indent =:
|equation = &lt;math&gt;\begin{array}{c}
\bar{x}_j = x_i \left(\bar{\mathbf{e}}_i\cdot\mathbf{e}_j \right) = x_i\cos\theta_{ij}\\
\upharpoonleft\downharpoonright\\
x_j = \bar{x}_i \left( \mathbf{e}_i\cdot\bar{\mathbf{e}}_j \right) = \bar{x}_i\cos\theta_{ji}
\end{array}
&lt;/math&gt;
|cellpadding= 6
|border = 1
|border colour = black
|background colour=white}}

and the explicit matrix equations in 3d are:

:&lt;math&gt;\bar{\mathbf{x}}=\boldsymbol{\mathsf{L}}\mathbf{x}&lt;/math&gt;

:&lt;math&gt;\begin{pmatrix}
\bar{x}_1\\
\bar{x}_2\\
\bar{x}_3
\end{pmatrix}=\begin{pmatrix}\bar{\mathbf{e}}_1\cdot\mathbf{e}_1 &amp; \bar{\mathbf{e}}_1\cdot\mathbf{e}_2 &amp; \bar{\mathbf{e}}_1\cdot\mathbf{e}_3\\
\bar{\mathbf{e}}_2\cdot\mathbf{e}_1 &amp; \bar{\mathbf{e}}_2\cdot\mathbf{e}_2 &amp; \bar{\mathbf{e}}_2\cdot\mathbf{e}_3\\
\bar{\mathbf{e}}_3\cdot\mathbf{e}_1 &amp; \bar{\mathbf{e}}_3\cdot\mathbf{e}_2 &amp; \bar{\mathbf{e}}_3\cdot\mathbf{e}_3
\end{pmatrix}\begin{pmatrix}x_1\\
x_2\\
x_3
\end{pmatrix}=\begin{pmatrix}\cos\theta_{11} &amp; \cos\theta_{12} &amp; \cos\theta_{13}\\
\cos\theta_{21} &amp; \cos\theta_{22} &amp; \cos\theta_{23}\\
\cos\theta_{31} &amp; \cos\theta_{32} &amp; \cos\theta_{33}
\end{pmatrix}\begin{pmatrix}x_1\\
x_2\\
x_3
\end{pmatrix}
&lt;/math&gt;

similarly for

:&lt;math&gt;\mathbf{x}=\boldsymbol{\mathsf{L}}^{-1}\bar{\mathbf{x}}=\boldsymbol{\mathsf{L}}^{\mathrm{T}}\bar{\mathbf{x}}&lt;/math&gt;

The geometric interpretation is the ''{{overline|x}}&lt;sub&gt;i&lt;/sub&gt;'' components equal to the sum of projecting the ''x&lt;sub&gt;j&lt;/sub&gt;'' components onto the ''{{overline|x}}&lt;sub&gt;j&lt;/sub&gt;'' axes.

The numbers '''e'''&lt;sub&gt;''i''&lt;/sub&gt;&amp;sdot;'''e'''&lt;sub&gt;''j''&lt;/sub&gt; arranged into a matrix would form a [[symmetric matrix]] (a matrix equal to its own transpose) due to the symmetry in the dot products, in fact it is the [[metric tensor]] '''g'''. By contrast '''e'''&lt;sub&gt;''i''&lt;/sub&gt;&amp;sdot;{{overline|'''e'''}}&lt;sub&gt;''j''&lt;/sub&gt; or {{overline|'''e'''}}&lt;sub&gt;''i''&lt;/sub&gt;&amp;sdot;'''e'''&lt;sub&gt;''j''&lt;/sub&gt; do ''not'' form symmetric matrices in general, as displayed above. Therefore, while the '''L''' matrices are still orthogonal, they are not symmetric.

Apart from a rotation about any one axis, in which the ''x&lt;sub&gt;i&lt;/sub&gt;'' and ''{{overline|x}}&lt;sub&gt;i&lt;/sub&gt;'' for some ''i'' coincide, the angles are not the same as [[Euler angle]]s, and so the '''L''' matrices are not the same as the [[rotation matrix|rotation matrices]].

==Transformation of the dot and cross products (three dimensions only)==

The [[dot product]] and [[cross product]] occur very frequently, in applications of vector analysis to physics and engineering, examples include:

*[[power (physics)|power]] transferred ''P'' by an object exerting a force '''F''' with velocity '''v''' along a straight-line path: 
::&lt;math&gt;P = \mathbf{v} \cdot \mathbf{F}&lt;/math&gt;
*tangential [[velocity]] '''v''' at a point '''x''' of a rotating [[rigid body]] with [[angular velocity]] '''ω''': 
::&lt;math&gt;\mathbf{v} = \boldsymbol{\omega} \times \mathbf{x}&lt;/math&gt;
*[[potential energy]] ''U'' of a [[magnetic dipole]] of [[magnetic moment]] '''m''' in a uniform external [[magnetic field]] '''B''': 
::&lt;math&gt;U=-\mathbf{m}\cdot\mathbf{B}&lt;/math&gt;
*[[angular momentum]] '''J''' for a particle with [[position vector]] '''r''' and [[momentum]] '''p''': 
::&lt;math&gt;\mathbf{J} = \mathbf{r}\times \mathbf{p}&lt;/math&gt;
*[[torque]] '''τ''' acting on an [[electric dipole]] of [[electric dipole moment]] '''p''' in a uniform external [[electric field]] '''E''': 
::&lt;math&gt;\boldsymbol{\tau} = \mathbf{p}\times\mathbf{E}&lt;/math&gt;
*induced surface [[current density]] '''j'''&lt;sub&gt;S&lt;/sub&gt; in a magnetic material of [[magnetization]] '''M''' on a surface with [[unit normal]] '''n''': 
::&lt;math&gt;\mathbf{j}_\mathrm{S} = \mathbf{M} \times \mathbf{n}&lt;/math&gt;

How these products transform under orthogonal transformations is illustrated below.

===Dot product, Kronecker delta, and metric tensor===

The [[dot product]] &amp;sdot; of each possible pairing of the basis vectors follows from the basis being orthonormal. For perpendicular pairs we have

:&lt;math&gt;\begin{array}{cccc}
\mathbf{e}_\text{x}\cdot\mathbf{e}_\text{y} &amp; = \mathbf{e}_\text{y}\cdot\mathbf{e}_\text{z} &amp; =\mathbf{e}_\text{z}\cdot\mathbf{e}_\text{x}\\
\mathbf{e}_\text{y}\cdot\mathbf{e}_\text{x} &amp; =\mathbf{e}_\text{z}\cdot\mathbf{e}_\text{y} &amp; =\mathbf{e}_\text{x}\cdot\mathbf{e}_\text{z} &amp; =0
\end{array}
&lt;/math&gt;

while for parallel pairs we have

:&lt;math&gt;\mathbf{e}_\text{x}\cdot\mathbf{e}_\text{x}=\mathbf{e}_\text{y}\cdot\mathbf{e}_\text{y}=\mathbf{e}_\text{z}\cdot\mathbf{e}_\text{z}=1 .&lt;/math&gt;

Replacing Cartesian labels by index notation as shown [[#indices replace labels|above]], these results can be summarized by

:&lt;math&gt;\mathbf{e}_i\cdot\mathbf{e}_j = \delta_{ij} &lt;/math&gt;

where ''δ&lt;sub&gt;ij&lt;/sub&gt;'' are the components of the [[Kronecker delta]]. The Cartesian basis can be used to represent ''δ'' in this way.

In addition, each [[metric tensor]] component ''g&lt;sub&gt;ij&lt;/sub&gt;'' with respect to any basis is the dot product of a pairing of basis vectors:

:&lt;math&gt;g_{ij} = \mathbf{e}_i\cdot\mathbf{e}_j .&lt;/math&gt;

For the Cartesian basis the components arranged into a matrix are:

:&lt;math&gt;\mathbf{g} = \begin{pmatrix} 
g_\text{xx} &amp; g_\text{xy} &amp; g_\text{xz} \\
g_\text{yx} &amp; g_\text{yy} &amp; g_\text{zz} \\
g_\text{zx} &amp; g_\text{zy} &amp; g_\text{zz} \\
\end{pmatrix} = \begin{pmatrix} 
\mathbf{e}_\text{x}\cdot\mathbf{e}_\text{x} &amp; \mathbf{e}_\text{x}\cdot\mathbf{e}_\text{y} &amp; \mathbf{e}_\text{x}\cdot\mathbf{e}_\text{z} \\
\mathbf{e}_\text{y}\cdot\mathbf{e}_\text{x} &amp; \mathbf{e}_\text{y}\cdot\mathbf{e}_\text{y} &amp; \mathbf{e}_\text{y}\cdot\mathbf{e}_\text{z} \\
\mathbf{e}_\text{z}\cdot\mathbf{e}_\text{x} &amp; \mathbf{e}_\text{z}\cdot\mathbf{e}_\text{y} &amp; \mathbf{e}_\text{z}\cdot\mathbf{e}_\text{z} \\
\end{pmatrix} = \begin{pmatrix} 
1 &amp; 0 &amp; 0 \\
0 &amp; 1 &amp; 0 \\
0 &amp; 0 &amp; 1 \\
\end{pmatrix}&lt;/math&gt;

so are the simplest possible for the metric tensor, namely the [[identity matrix|''δ'']]:

:&lt;math&gt;g_{ij} = \delta_{ij}&lt;/math&gt;

This is ''not'' true for general bases: [[orthogonal coordinates]] have [[diagonal matrix|diagonal]] metrics containing various scale factors (i.e. not necessarily 1), while general [[curvilinear coordinates]] could also have nonzero entries for off-diagonal components.

The dot product of two vectors '''a''' and '''b''' transforms according to

:&lt;math&gt;\mathbf{a}\cdot\mathbf{b} = \bar{a}_j \bar{b}_j = a_i \mathsf{L}_{ij} b_k(\boldsymbol{\mathsf{L}}^{-1})_{jk} = a_i \delta_i{}_k b_k = a_i b_i &lt;/math&gt;

which is intuitive, since the dot product of two vectors is a single scalar independent of any coordinates. This also applies more generally to any coordinate systems, not just rectangular ones; the dot product in one coordinate system is the same in any other.

===Cross and product, Levi-Civita symbol, and pseudovectors===

{{multiple image
|width=170
|image1=Epsilon volume cyclic permutations.svg
|image2=Epsilon volume anticyclic permutations.svg
|caption1=Cyclic permutations of index values and positively oriented cubic volume.
|caption2=Anticyclic permutations of index values and negatively oriented cubic volume.
|footer=Non-zero values of the [[Levi-Civita symbol]] ''ε&lt;sub&gt;ijk&lt;/sub&gt;'' as the volume {{nowrap|'''e'''&lt;sub&gt;''i''&lt;/sub&gt; · '''e'''&lt;sub&gt;''j''&lt;/sub&gt; × '''e'''&lt;sub&gt;''k''&lt;/sub&gt;}} of a cube spanned by the 3d orthonormal basis.}}

For the [[cross product]] × of two vectors, the results are (almost) the other way round. Again, assuming a right-handed 3d Cartesian coordinate system, [[cyclic permutation]]s in perpendicular directions yield the next vector in the cyclic collection of vectors:

:&lt;math&gt;\mathbf{e}_\text{x}\times\mathbf{e}_\text{y} = \mathbf{e}_\text{z}\,\quad \mathbf{e}_\text{y}\times\mathbf{e}_\text{z} = \mathbf{e}_\text{x}\,\quad \mathbf{e}_\text{z}\times\mathbf{e}_\text{x} = \mathbf{e}_\text{y}&lt;/math&gt;

:&lt;math&gt;\mathbf{e}_\text{y}\times\mathbf{e}_\text{x} = - \mathbf{e}_\text{z}\,\quad \mathbf{e}_\text{z}\times\mathbf{e}_\text{y} = -\mathbf{e}_\text{x}\,\quad \mathbf{e}_\text{x}\times\mathbf{e}_\text{z} = -\mathbf{e}_\text{y}&lt;/math&gt;

while parallel vectors clearly vanish:

:&lt;math&gt;\mathbf{e}_\text{x}\times\mathbf{e}_\text{x}=\mathbf{e}_\text{y}\times\mathbf{e}_\text{y}=\mathbf{e}_\text{z}\times\mathbf{e}_\text{z} = \boldsymbol{0}&lt;/math&gt;

and replacing Cartesian labels by index notation as [[#indices replace labels|above]], these can be summarized by:

:&lt;math&gt;\mathbf{e}_i\times\mathbf{e}_j=\left [ \begin{array}{cc}
+\mathbf{e}_k &amp; \text{cyclic permutations: } (i,j,k) = (1,2,3), (2,3,1), (3,1,2) \\
-\mathbf{e}_k &amp; \text{anticyclic permutations: } (i,j,k) = (2,1,3), (3,2,1), (1,3,2) \\
\boldsymbol{0} &amp; i=j
\end{array}\right.
&lt;/math&gt;

where ''i'', ''j'', ''k'' are indices which take values 1, 2, 3. It follows that:

:&lt;math&gt;{\mathbf{e}_k\cdot\mathbf{e}_i\times\mathbf{e}_j}=\left[ \begin{array}{cc}
+1 &amp; \text{cyclic permutations: } (i,j,k) = (1,2,3), (2,3,1), (3,1,2) \\
-1 &amp; \text{anticyclic permutations: } (i,j,k) = (2,1,3), (3,2,1), (1,3,2) \\
0 &amp; i=j\text{ or }j=k\text{ or }k=i
\end{array}\right.
&lt;/math&gt;

These permutation relations and their corresponding values are important, and there is an object coinciding with this property: the [[Levi-Civita symbol]], denoted by ''ε''. The Levi-Civita symbol entries can be represented by the Cartesian basis:

:&lt;math&gt;\varepsilon_{ijk} = \mathbf{e}_i\cdot \mathbf{e}_j\times\mathbf{e}_k&lt;/math&gt;

which geometrically corresponds to the [[volume]] of a [[cube]] spanned by the orthonormal basis vectors, with sign indicating [[orientation (vector space)|orientation]] (and ''not'' a "positive or negative volume"). Here, the orientation is fixed by ''ε''&lt;sub&gt;123&lt;/sub&gt; = +1, for a right-handed system. A left-handed system would fix ''ε''&lt;sub&gt;123&lt;/sub&gt; = −1 or equivalently ''ε''&lt;sub&gt;321&lt;/sub&gt; = +1.

The [[scalar triple product]] can now be written:

:&lt;math&gt;\mathbf{c} \cdot \mathbf{a} \times \mathbf{b} = c_i\mathbf{e}_i \cdot a_j\mathbf{e}_j \times b_k\mathbf{e}_k = \varepsilon_{ijk} c_i a_j b_k  &lt;/math&gt;

with the geometric interpretation of volume (of the [[parallelepiped]] spanned by '''a''', '''b''', '''c''') and algebraically is a [[determinant]]:&lt;ref&gt;{{cite book|edition=2nd|author1=M. R. Spiegel |author2=S. Lipcshutz |author3=D. Spellman | title=Vector analysis| series=Schaum’s Outlines|publisher=McGraw Hill |page=23|year=2009 | isbn=978-0-07-161545-7}}&lt;/ref&gt;

:&lt;math&gt;\mathbf{c} \cdot \mathbf{a} \times \mathbf{b} = \begin{vmatrix} c_\text{x} &amp; a_\text{x} &amp; b_\text{x} \\ c_\text{y} &amp; a_\text{y} &amp; b_\text{y} \\ c_\text{z} &amp; a_\text{z} &amp; b_\text{z} \end{vmatrix} &lt;/math&gt;

This in turn can be used to rewrite the [[cross product]] of two vectors as follows:

:&lt;math&gt;\begin{array}{ll}
&amp; (\mathbf{a} \times \mathbf{b})_i = {\mathbf{e}_i \cdot \mathbf{a} \times \mathbf{b}} = \varepsilon_{\ell jk} {(\mathbf{e}_i)}_\ell a_j b_k = \varepsilon_{\ell jk} \delta_{i \ell} a_j b_k = \varepsilon_{ijk} a_j b_k \\
\Rightarrow &amp; {\mathbf{a} \times \mathbf{b}} = (\mathbf{a} \times \mathbf{b})_i \mathbf{e}_i = \varepsilon_{ijk} a_j b_k \mathbf{e}_i 
\end{array}&lt;/math&gt;

Contrary to its appearance, the Levi-Civita symbol is ''not a tensor'', but a [[pseudotensor]], the components transform according to:

:&lt;math&gt;\bar{\varepsilon}_{pqr} = \det(\boldsymbol{\mathsf{L}}) \varepsilon_{ijk} \mathsf{L}_{ip}\mathsf{L}_{jq}\mathsf{L}_{kr} \,.&lt;/math&gt;

Therefore, the transformation of the cross product of '''a''' and '''b''' is:
&lt;!---PLEASE LEAVE THE SPACES \;\; FOR CLARITY, AND LEAVE IN ALL THE STEPS - CLARITY IS NOT "TEXTBOOKY" - THANKS---&gt;
:&lt;math&gt;\begin{align}
(\bar{\mathbf{a}} \times \bar{\mathbf{b}})_i &amp; = \bar{\varepsilon}_{ijk} \bar{a}_j \bar{b}_k \\
&amp; = \det(\boldsymbol{\mathsf{L}}) \;\; \varepsilon_{pqr} \mathsf{L}_{pi}\mathsf{L}_{qj} \mathsf{L}_{rk} \;\; a_m \mathsf{L}_{mj} \;\; b_n \mathsf{L}_{nk} \\
&amp; = \det(\boldsymbol{\mathsf{L}}) \;\; \varepsilon_{pqr} \;\; \mathsf{L}_{pi} \;\; \mathsf{L}_{qj} (\boldsymbol{\mathsf{L}}^{-1})_{jm} \;\; \mathsf{L}_{rk} (\boldsymbol{\mathsf{L}}^{-1})_{kn} \;\; a_m \;\; b_n \\
&amp; = \det(\boldsymbol{\mathsf{L}}) \;\; \varepsilon_{pqr} \;\; \mathsf{L}_{pi}  \;\;  \delta_{qm} \;\; \delta_{rn} \;\; a_m \;\; b_n \\
&amp; = \det(\boldsymbol{\mathsf{L}}) \;\; \mathsf{L}_{pi}  \;\;  \varepsilon_{pqr} a_q b_r \\
&amp; = \det(\boldsymbol{\mathsf{L}}) \;\; (\mathbf{a}\times\mathbf{b})_p  \mathsf{L}_{pi}
\end{align}&lt;/math&gt;

and so '''a''' × '''b''' transforms as a [[pseudovector]], because of the determinant factor.

The [[tensor index notation]] applies to any object which has entities that form [[multidimensional array]]s – not everything with indices is a tensor by default. Instead, tensors are defined by how their coordinates and basis elements change under a transformation from one coordinate system to another.

Note the cross product of two vectors is a pseudovector, while the cross product of a pseudovector with a vector is another vector.

===Applications of the ''δ'' tensor and ''ε'' pseudotensor===

Other identities can be formed from the ''δ'' tensor and ''ε'' pseudotensor, a notable and very useful identity is one that converts two Levi-Civita symbols adjacently contracted over two indices into an antisymmetrized combination of Kronecker deltas:

:&lt;math&gt;\varepsilon_{ijk}\varepsilon_{pqk} = \delta_{ip}\delta_{jq} - \delta_{iq}\delta_{jp} &lt;/math&gt;

The index forms of the dot and cross products, together with this identity, greatly facilitate the manipulation and derivation of other [[vector calculus identities|identities in vector calculus]] and algebra, which in turn are used extensively in physics and engineering. For instance, it is clear the dot and cross products are distributive over vector addition:

:&lt;math&gt;\mathbf{a}\cdot(\mathbf{b} + \mathbf{c} ) =  a_i ( b_i + c_i ) =  a_i b_i + a_i c_i = \mathbf{a}\cdot\mathbf{b} + \mathbf{a}\cdot\mathbf{c} &lt;/math&gt;
:&lt;math&gt; \mathbf{a}\times(\mathbf{b}+\mathbf{c}) = \mathbf{e}_i\varepsilon_{ijk} a_j ( b_k + c_k ) = \mathbf{e}_i \varepsilon_{ijk} a_j b_k + \mathbf{e}_i \varepsilon_{ijk} a_j c_k = \mathbf{a}\times\mathbf{b} + \mathbf{a}\times\mathbf{c}&lt;/math&gt;

without resort to any geometric constructions - the derivation in each case is a quick line of algebra. Although the procedure is less obvious, the vector triple product can also be derived. Rewriting in index notation:

:&lt;math&gt; \left[ \mathbf{a}\times(\mathbf{b}\times\mathbf{c})\right]_i = \varepsilon_{ijk} a_j ( \varepsilon_{k \ell m} b_\ell c_m ) = (\varepsilon_{ijk} \varepsilon_{k \ell m} ) a_j b_\ell c_m &lt;/math&gt;

and because cyclic permutations of indices in the ''ε'' symbol does not change its value, cyclically permuting indices in ''ε&lt;sub&gt;k{{ell}}m&lt;/sub&gt;'' to obtain ''ε&lt;sub&gt;{{ell}}mk&lt;/sub&gt;'' allows us to use the above ''δ''-''ε'' identity to convert the ''ε'' symbols into ''δ'' tensors:

:&lt;math&gt; \begin{align}
\left[ \mathbf{a}\times(\mathbf{b}\times\mathbf{c})\right]_i &amp; = ( \delta_{i\ell} \delta_{jm} - \delta_{im} \delta_{j\ell} ) a_j b_\ell c_m \\
&amp; = \delta_{i\ell} \delta_{jm} a_j b_\ell c_m - \delta_{im} \delta_{j\ell} a_j b_\ell c_m \\ 
&amp; = a_j b_i c_j -  a_j b_j c_i \\
\end{align}&lt;/math&gt;

thusly:

:&lt;math&gt;\mathbf{a}\times(\mathbf{b}\times\mathbf{c}) = (\mathbf{a}\cdot\mathbf{c})\mathbf{b} -  (\mathbf{a}\cdot\mathbf{b})\mathbf{c}&lt;/math&gt;

Note this is antisymmetric in '''b''' and '''c''', as expected from the left hand side. Similarly, via index notation or even just cyclically relabelling '''a''', '''b''', and '''c''' in the previous result and taking the negative:

:&lt;math&gt;(\mathbf{a}\times \mathbf{b})\times\mathbf{c} = (\mathbf{c}\cdot\mathbf{a})\mathbf{b} - (\mathbf{c}\cdot\mathbf{b})\mathbf{a} &lt;/math&gt;

and the difference in results show that the cross product is not associative. More complex identities, like quadruple products;

:&lt;math&gt;(\mathbf{a}\times \mathbf{b})\cdot(\mathbf{c}\times\mathbf{d}), \quad (\mathbf{a}\times \mathbf{b})\times(\mathbf{c}\times\mathbf{d}) ,\ldots&lt;/math&gt;

and so on, can be derived in a similar manner.

==Transformations of Cartesian tensors (any number of dimensions)==

Tensors are defined as quantities which transform in a certain way under linear transformations of coordinates.

===Second order===
Text below contradics introduced above contravariant and covariant vectors (tensors)

Let '''a''' = ''a&lt;sub&gt;i&lt;/sub&gt;'''''e'''&lt;sub&gt;''i''&lt;/sub&gt; and '''b''' = ''b&lt;sub&gt;i&lt;/sub&gt;'''''e'''&lt;sub&gt;''i''&lt;/sub&gt; be two vectors, so that they transform according to ''{{overline|a}}&lt;sub&gt;j&lt;/sub&gt;'' = ''a&lt;sub&gt;i&lt;/sub&gt;L&lt;sub&gt;i&lt;/sub&gt;&lt;sub&gt;j&lt;/sub&gt;'', ''{{overline|b}}&lt;sub&gt;j&lt;/sub&gt;'' = ''b&lt;sub&gt;i&lt;/sub&gt;L&lt;sub&gt;i&lt;/sub&gt;&lt;sub&gt;j&lt;/sub&gt;''.

Taking the tensor product gives:

:&lt;math&gt;\mathbf{a}\otimes\mathbf{b}=a_i\mathbf{e}_i\otimes b_j\mathbf{e}_j=a_i b_j\mathbf{e}_i\otimes\mathbf{e}_j&lt;/math&gt;

then applying the transformation to the components

:&lt;math&gt;\bar{a}_p\bar{b}_q= a_i \mathsf{L}_i{}_p b_j \mathsf{L}_j{}_q = \mathsf{L}_i{}_p\mathsf{L}_j{}_q a_i b_j &lt;/math&gt;

and to the bases

:&lt;math&gt;\bar{\mathbf{e}}_p\otimes\bar{\mathbf{e}}_q = (\boldsymbol{\mathsf{L}}^{-1})_{pi}\mathbf{e}_i\otimes(\boldsymbol{\mathsf{L}}^{-1})_{qj}\bar{\mathbf{e}}_j = (\boldsymbol{\mathsf{L}}^{-1})_{pi}(\boldsymbol{\mathsf{L}}^{-1})_{qj}\mathbf{e}_i\otimes\bar{\mathbf{e}}_j = \mathsf{L}_{ip} \mathsf{L}_{jq} \mathbf{e}_i\otimes\bar{\mathbf{e}}_j&lt;/math&gt;

gives the transformation law of an order-2 tensor. The tensor '''a'''&amp;otimes;'''b''' is invariant under this transformation:

:&lt;math&gt;\begin{array}{cl}
\bar{a}_p\bar{b}_q\bar{\mathbf{e}}_p\otimes\bar{\mathbf{e}}_q &amp; = \mathsf{L}_{kp} \mathsf{L}_{\ell q} a_k b_{\ell} \, (\boldsymbol{\mathsf{L}}^{-1})_{pi} (\boldsymbol{\mathsf{L}}^{-1})_{qj} \mathbf{e}_i\otimes\mathbf{e}_j \\
 &amp; = \mathsf{L}_{kp} (\boldsymbol{\mathsf{L}}^{-1})_{pi} \mathsf{L}_{\ell q} (\boldsymbol{\mathsf{L}}^{-1})_{q j} \, a_k b_{\ell} \mathbf{e}_i\otimes\mathbf{e}_j \\
 &amp; = \delta_k{}_i \delta_{\ell j}  \, a_k b_{\ell} \mathbf{e}_i\otimes\mathbf{e}_j \\
 &amp; = a_ib_j\mathbf{e}_i\otimes\mathbf{e}_j
\end{array}
&lt;/math&gt;

More generally, for any order-2 tensor

:&lt;math&gt;\mathbf{R}=R_{ij}\mathbf{e}_i\otimes\mathbf{e}_j\,,&lt;/math&gt;

the components transform according to;

:&lt;math&gt;\bar{R}_{pq}=\mathsf{L}_i{}_p\mathsf{L}_j{}_q R_{ij}&lt;/math&gt;,

and the basis transforms by:

:&lt;math&gt;\bar{\mathbf{e}}_p\otimes\bar{\mathbf{e}}_q = (\boldsymbol{\mathsf{L}}^{-1})_{ip}\mathbf{e}_i\otimes (\boldsymbol{\mathsf{L}}^{-1})_{jq}\mathbf{e}_j&lt;/math&gt;

If '''R''' does not transform according to this rule - whatever quantity '''R''' may be, it's not an order 2 tensor.

===Any order===

More generally, for any order ''p'' tensor

:&lt;math&gt;\mathbf{T} = T_{j_1 j_2 \cdots j_p} \mathbf{e}_{j_1}\otimes\mathbf{e}_{j_2}\otimes\cdots\mathbf{e}_{j_p}&lt;/math&gt;

the components transform according to;

:&lt;math&gt;\bar{T}_{j_1j_2\cdots j_p} = \mathsf{L}_{i_1 j_1} \mathsf{L}_{i_2 j_2}\cdots \mathsf{L}_{i_p j_p} T_{i_1 i_2\cdots i_p}&lt;/math&gt;

and the basis transforms by:

:&lt;math&gt;\bar{\mathbf{e}}_{j_1}\otimes\bar{\mathbf{e}}_{j_2}\cdots\otimes\bar{\mathbf{e}}_{j_p}=(\boldsymbol{\mathsf{L}}^{-1})_{j_1 i_1}\mathbf{e}_{i_1}\otimes(\boldsymbol{\mathsf{L}}^{-1})_{j_2 i_2}\mathbf{e}_{i_2}\cdots\otimes(\boldsymbol{\mathsf{L}}^{-1})_{j_p i_p}\mathbf{e}_{i_p}&lt;/math&gt;

For a [[pseudotensor]] '''S''' of order ''p'', the components transform according to;

:&lt;math&gt;\bar{S}_{j_1j_2\cdots j_p} = \det(\boldsymbol{\mathsf{L}}) \mathsf{L}_{i_1 j_1} \mathsf{L}_{i_2 j_2}\cdots \mathsf{L}_{i_p j_p} S_{i_1 i_2\cdots i_p}\,.&lt;/math&gt;

==Pseudovectors as antisymmetric second order tensors==

The antisymmetric nature of the cross product can be recast into a tensorial form as follows.&lt;ref&gt;{{cite book|title=classical mechanics|year=1973|publisher=McGraw Hill|edition=2nd|series=European physics series|pages=234–235|author=T. W. B. Kibble|isbn=978-0-07-084018-8}}, see Appendix C.&lt;/ref&gt; Let '''c''' be a vector, '''a''' be a pseudovector, '''b''' be another vector, and '''T''' be a second order tensor such that:

:&lt;math&gt;\mathbf{c} = \mathbf{a}\times\mathbf{b} = \mathbf{T}\cdot\mathbf{b} &lt;/math&gt;

As the cross product is linear in '''a''' and '''b''', the components of '''T''' can be found by inspection, and they are:

:&lt;math&gt;\mathbf{T} = \begin{pmatrix}
0 &amp; - a_\text{z} &amp; a_\text{y} \\
a_\text{z} &amp; 0 &amp; - a_\text{x} \\
- a_\text{y} &amp; a_\text{x} &amp; 0 \\
\end{pmatrix}&lt;/math&gt;

so the pseudovector '''a''' can be written as an antisymmetric tensor. This transforms as a tensor, not a pseudotensor. For the mechanical example above for the tangential velocity of a rigid body, given by {{nowrap|'''v''' {{=}} '''ω''' × '''x'''}}, this can be rewritten as {{nowrap|'''v''' {{=}} '''Ω''' · '''x'''}} where '''Ω''' is the tensor corresponding to the pseudovector '''ω''':

:&lt;math&gt;\boldsymbol{\Omega} = \begin{pmatrix}
0 &amp; - \omega_\text{z} &amp; \omega_\text{y} \\
\omega_\text{z} &amp; 0 &amp; - \omega_\text{x} \\
- \omega_\text{y} &amp; \omega_\text{x} &amp; 0 \\
\end{pmatrix}&lt;/math&gt;

For an example in [[electromagnetism]], while the [[electric field]] '''E''' is a [[vector field]], the [[magnetic field]] '''B''' is a pseudovector field. These fields are defined from the [[Lorentz force]] for a particle of [[electric charge]] ''q'' traveling at velocity '''v''':

:&lt;math&gt;\mathbf{F} = q(\mathbf{E} + \mathbf{v} \times \mathbf{B}) = q(\mathbf{E} - \mathbf{B} \times \mathbf{v})&lt;/math&gt;

and considering the second term containing the cross product of a pseudovector '''B''' and velocity vector '''v''', it can be written in matrix form, with '''F''', '''E''', and '''v''' as column vectors and '''B''' as an antisymmetric matrix:

:&lt;math&gt; \begin{pmatrix}
F_\text{x} \\
F_\text{y} \\
F_\text{z} \\
\end{pmatrix} = q\begin{pmatrix}
E_\text{x} \\
E_\text{y} \\
E_\text{z} \\
\end{pmatrix} - q \begin{pmatrix}
0 &amp; - B_\text{z} &amp; B_\text{y} \\
B_\text{z} &amp; 0 &amp; - B_\text{x} \\
- B_\text{y} &amp; B_\text{x} &amp; 0 \\
\end{pmatrix} \begin{pmatrix}
v_\text{x} \\
v_\text{y} \\
v_\text{z} \\
\end{pmatrix}&lt;/math&gt;

If a pseudovector is explicitly given by a cross product of two vectors (as opposed to entering the cross product with another vector), then such pseudovectors can also be written as antisymmetric tensors of second order, with each entry a component of the cross product. The angular momentum of a classical pointlike particle orbiting about an axis, defined by {{nowrap|'''J''' {{=}} '''x''' × '''p'''}}, is another example of a pseudovector, with corresponding antisymmetric tensor:

:&lt;math&gt;\mathbf{J} = \begin{pmatrix}
0 &amp; - J_\text{z} &amp; J_\text{y} \\
J_\text{z} &amp; 0 &amp; - J_\text{x} \\
- J_\text{y} &amp; J_\text{x} &amp; 0 \\
\end{pmatrix} = \begin{pmatrix}
0 &amp; - (x p_\text{y} - y p_\text{x}) &amp; (z p_\text{x} - x p_\text{z}) \\
(x p_\text{y} - y p_\text{x}) &amp; 0 &amp; - (y p_\text{z} - z p_\text{y}) \\
- (z p_\text{x} - x p_\text{z}) &amp; (y p_\text{z} - z p_\text{y}) &amp; 0 \\
\end{pmatrix}&lt;/math&gt;

Although Cartesian tensors do not occur in the theory of relativity; the tensor form of orbital angular momentum '''J''' enters the spacelike part of the [[relativistic angular momentum]] tensor, and the above tensor form of the magnetic field '''B''' enters the spacelike part of the [[electromagnetic tensor]].

==Vector and tensor calculus==

It should be emphasized the following formulae are only so simple in Cartesian coordinates - in general curvilinear coordinates there are factors of the metric and its determinant - see [[tensors in curvilinear coordinates]] for more general analysis.

===Vector calculus===

Following are the differential operators of [[vector calculus]]. Throughout, left Φ('''r''', ''t'') be a [[scalar field]], and

:&lt;math&gt;\mathbf{A}(\mathbf{r},t) = A_\text{x}(\mathbf{r},t)\mathbf{e}_\text{x} + A_\text{y}(\mathbf{r},t)\mathbf{e}_\text{y} + A_\text{z}(\mathbf{r},t)\mathbf{e}_\text{z}&lt;/math&gt;
:&lt;math&gt;\mathbf{B}(\mathbf{r},t) = B_\text{x}(\mathbf{r},t)\mathbf{e}_\text{x} + B_\text{y}(\mathbf{r},t)\mathbf{e}_\text{y} + B_\text{z}(\mathbf{r},t)\mathbf{e}_\text{z}&lt;/math&gt;

be [[vector field]]s, in which all scalar and vector fields are functions of the [[position vector]] '''r''' and time ''t''.

The [[gradient]] operator in Cartesian coordinates is given by:

:&lt;math&gt;\nabla = \mathbf{e}_\text{x}\frac{\partial}{\partial x} +  \mathbf{e}_\text{y}\frac{\partial}{\partial y} + \mathbf{e}_\text{z}\frac{\partial}{\partial z} &lt;/math&gt;

and in index notation, this is usually abbreviated in various ways:

:&lt;math&gt;\nabla_i \equiv \partial_i \equiv \frac{\partial}{\partial x_i} &lt;/math&gt;

This operator acts on a scalar field Φ to obtain the vector field directed in the maximum rate of increase of Φ:

:&lt;math&gt;\left(\nabla\Phi\right)_i = \nabla_i \Phi &lt;/math&gt;

The index notation for the dot and cross products carries over to the differential operators of vector calculus.&lt;ref&gt;{{cite book|edition=2nd|author1=M. R. Spiegel |author2=S. Lipcshutz |author3=D. Spellman | title=Vector analysis| series=Schaum’s Outlines|publisher=McGraw Hill |page=197| year=2009 | isbn=978-0-07-161545-7}}&lt;/ref&gt;

The [[directional derivative]] of a scalar field Φ is the rate of change of Φ along some direction vector '''a''' (not necessarily a [[unit vector]]), formed out of the components of '''a''' and the gradient:

:&lt;math&gt;\mathbf{a}\cdot(\nabla\Phi) = a_j (\nabla\Phi)_j &lt;/math&gt;

The [[divergence]] of a vector field '''A''' is:

:&lt;math&gt;\nabla\cdot\mathbf{A} = \nabla_i A_i &lt;/math&gt;

Note the interchange of the components of the gradient and vector field yields a different differential operator

:&lt;math&gt;\mathbf{A}\cdot\nabla =  A_i \nabla_i &lt;/math&gt;

which could act on scalar or vector fields. In fact, if '''A''' is replaced by the [[velocity field]] '''u'''('''r''', ''t'') of a fluid, this is a term in the [[material derivative]] (with many other names) of [[continuum mechanics]], with another term being the partial time derivative:

:&lt;math&gt; \frac{D}{D t} = \frac{\partial}{\partial t} + \mathbf{u}\cdot\nabla&lt;/math&gt;

which usually acts on the velocity field leading to the non-linearity in the [[Navier-Stokes equations]].

As for the [[Curl (mathematics)|curl]] of a vector field '''A''', this can be defined as a pseudovector field by means of the ''ε'' symbol:

:&lt;math&gt;\left(\nabla\times\mathbf{A}\right)_i = \varepsilon_{ijk} \nabla_j A_k &lt;/math&gt;

which is only valid in three dimensions, or an antisymmetric tensor field of second order via antisymmetrization of indices, indicated by delimiting the antisymmetrized indices by square brackets (see [[Ricci calculus]]):

:&lt;math&gt;\left(\nabla\times\mathbf{A}\right)_{ij} = \nabla_i A_j - \nabla_j A_i = 2\nabla_{[i} A_{j]}&lt;/math&gt;

which is valid in any number of dimensions. In each case, the order of the gradient and vector field components should not be interchanged as this would result in a different differential operator:

:&lt;math&gt; \varepsilon_{ijk} A_j \nabla_k &lt;/math&gt;
:&lt;math&gt;A_i \nabla_j - A_j \nabla_i = 2 A_{[i} \nabla_{j]} &lt;/math&gt;

which could act on scalar or vector fields.

Finally, the [[Laplacian operator]] is defined in two ways, the divergence of the gradient of a scalar field Φ:

:&lt;math&gt;\nabla\cdot(\nabla \Phi) = \nabla_i (\nabla_i \Phi) &lt;/math&gt;

or the square of the gradient operator, which acts on a scalar field Φ or a vector field '''A''':

:&lt;math&gt;(\nabla\cdot\nabla) \Phi = (\nabla_i \nabla_i) \Phi &lt;/math&gt;
:&lt;math&gt;(\nabla\cdot\nabla) \mathbf{A} = (\nabla_i \nabla_i) \mathbf{A} &lt;/math&gt;

In physics and engineering, the gradient, divergence, curl, and Laplacian operator arise inevitably in [[fluid mechanics]], [[Newtonian gravitation]], [[electromagnetism]], [[heat conduction]], and even [[quantum mechanics]].

Vector calculus identities can be derived in a similar way to those of vector dot and cross products and combinations. For example, in three dimensions, the curl of a cross product of two vector fields '''A''' and '''B''':

:&lt;math&gt;\begin{align}
\left[\nabla\times(\mathbf{A}\times\mathbf{B})\right]_i &amp; = \varepsilon_{ijk} \nabla_j (\varepsilon_{k\ell m} A_\ell B_m) \\
&amp; = (\varepsilon_{ijk} \varepsilon_{\ell m k} ) \nabla_j (A_\ell B_m) \\
&amp; = (\delta_{i\ell}\delta_{jm} - \delta_{im}\delta_{j\ell}) (B_m \nabla_j A_\ell + A_\ell \nabla_j B_m ) \\
&amp; = (B_j \nabla_j A_i  + A_i \nabla_j B_j ) - (B_i \nabla_j A_j + A_j \nabla_j B_i ) \\
&amp; = (B_j \nabla_j)A_i  + A_i(\nabla_j B_j ) - B_i (\nabla_j A_j ) - (A_j \nabla_j) B_i \\
&amp; = \left[(\mathbf{B} \cdot \nabla)\mathbf{A}  + \mathbf{A}(\nabla\cdot \mathbf{B}) - \mathbf{B}(\nabla\cdot \mathbf{A} ) - (\mathbf{A}\cdot \nabla) \mathbf{B} \right]_i \\
\end{align}&lt;/math&gt;

where the [[product rule]] was used, and throughout the differential operator was not interchanged with '''A''' or '''B'''. Thus:

:&lt;math&gt;\nabla\times(\mathbf{A}\times\mathbf{B}) = (\mathbf{B} \cdot \nabla)\mathbf{A}  + \mathbf{A}(\nabla\cdot \mathbf{B}) - \mathbf{B}(\nabla\cdot \mathbf{A} ) - (\mathbf{A}\cdot \nabla) \mathbf{B} &lt;/math&gt;

===Tensor calculus===

One can continue the operations on tensors of higher order. Let '''T''' = '''T'''('''r''', ''t'') denote a second order tensor field, again dependent on the position vector '''r''' and time ''t''.

For instance, the gradient of a vector field in two equivalent notations ("dyadic" and "tensor", respectively) is:

:&lt;math&gt;(\nabla \mathbf{A})_{ij} \equiv (\nabla \otimes \mathbf{A})_{ij} = \nabla_i A_j &lt;/math&gt;

which is a tensor field of second order.

The divergence of a tensor is:

:&lt;math&gt;(\nabla \cdot \mathbf{T})_j = \nabla_i T_{ij} &lt;/math&gt;

which is a vector field. This arises in continuum mechanics in [[Continuum mechanics#Governing equations|Cauchy's laws of motion]] - the divergence of the Cauchy stress tensor '''σ''' is a vector field, related to [[body force]]s acting on the fluid.

==Difference from the standard tensor calculus==

Cartesian tensors are as in [[tensor algebra]], but [[Euclidean space#Euclidean structure|Euclidean structure]] of and restriction of the basis brings some simplifications compared to the general theory.

The general tensor algebra consists of general [[mixed tensor]]s of type (''p'', ''q''):

:&lt;math&gt;\mathbf{T} = T_{j_1 j_2 \cdots j_q}^{i_1 i_2 \cdots i_p} \mathbf{e}_{i_1 i_2 \cdots i_p}^{j_1 j_2 \cdots j_q} &lt;/math&gt;

with basis elements:

:&lt;math&gt;\mathbf{e}_{i_1 i_2 \cdots i_p}^{j_1 j_2 \cdots j_q} = \mathbf{e}_{i_1}\otimes\mathbf{e}_{i_2}\otimes\cdots\mathbf{e}_{i_p}\otimes\mathbf{e}^{j_1}\otimes\mathbf{e}^{j_2}\otimes\cdots\mathbf{e}^{j_q}&lt;/math&gt;

the components transform according to:

:&lt;math&gt;\bar{T}_{\ell_1 \ell_2 \cdots \ell_q}^{k_1 k_2 \cdots k_p} = \mathsf{L}_{i_1}{}^{k_1} \mathsf{L}_{i_2}{}^{k_2} \cdots \mathsf{L}_{i_p}{}^{k_p} (\boldsymbol{\mathsf{L}}^{-1})_{\ell_1}{}^{j_1}(\boldsymbol{\mathsf{L}}^{-1})_{\ell_2}{}^{j_2} \cdots (\boldsymbol{\mathsf{L}}^{-1})_{\ell_q}{}^{j_q} T_{j_1 j_2 \cdots j_q}^{i_1 i_2 \cdots i_p}&lt;/math&gt;

as for the bases:

:&lt;math&gt;\bar{\mathbf{e}}_{k_1 k_2 \cdots k_p}^{\ell_1 \ell_2 \cdots \ell_q} = (\boldsymbol{\mathsf{L}}^{-1})_{k_1}{}^{i_1} (\boldsymbol{\mathsf{L}}^{-1})_{k_2}{}^{i_2} \cdots (\boldsymbol{\mathsf{L}}^{-1})_{k_p}{}^{i_p} \mathsf{L}_{j_1}{}^{\ell_1} \mathsf{L}_{j_2}{}^{\ell_2} \cdots \mathsf{L}_{j_q}{}^{\ell_q} \mathbf{e}_{i_1 i_2 \cdots i_p}^{j_1 j_2 \cdots j_q}&lt;/math&gt;

For Cartesian tensors, only the order {{nowrap|''p'' + ''q''}} of the tensor matters in a Euclidean space with an orthonormal basis, and all {{nowrap|''p'' + ''q''}} indices can be lowered.  A Cartesian basis does not exist unless the vector space has a positive-definite metric, and thus cannot be used in [[special relativity|relativistic]] contexts.

==History==

[[Dyadic tensor]]s were historically the first approach to formulating second-order tensors, similarly triadic tensors for third-order tensors, and so on. Cartesian tensors use [[tensor index notation]], in which the [[Covariance and contravariance of vectors|variance]] may be glossed over and is often ignored, since the components remain unchanged by [[raising and lowering indices]].

==See also==

*[[Tensor algebra]]
*[[Tensor calculus]]
*[[Tensors in curvilinear coordinates]]
*[[Rotation group]]

==References==

{{reflist}}

===Notes===

*{{cite book| author=D. C. Kay| title=Tensor Calculus| series=Schaum’s Outlines|publisher=McGraw Hill|pages=18–19, 31–32| year=1988 | isbn=0-07-033484-6}}
*{{cite book|edition=2nd|author1=M. R. Spiegel |author2=S. Lipcshutz |author3=D. Spellman | title=Vector analysis| series=Schaum’s Outlines|publisher=McGraw Hill |page=227| year=2009 | isbn=978-0-07-161545-7}}
*{{cite book|title=An introduction to tensor analysis for engineers and applied scientists
|author=J.R. Tyldesley|volume=|publisher=Longman|pages=5–13|year=1975|series=|isbn=0-582-44355-5|url=https://books.google.com/books/about/An_introduction_to_tensor_analysis_for_e.html?id=PODXAAAAMAAJ&amp;redir_esc=y}}

===Further reading and applications===

*{{cite book|edition=4th|author1=S. Lipcshutz |author2=M. Lipson | title=Linear Algebra| series=Schaum’s Outlines|publisher=McGraw Hill |page=| year=2009 | isbn=978-0-07-154352-1}}
*{{cite book|title=Elasticity: Tensor, Dyadic, and Engineering Approaches|author=Pei Chi Chou|page=|volume=|publisher=Courier Dover Publications|year=1992|series=|isbn=048-666-958-0|url=https://books.google.com/books?id=9-pJ7Kg5XmAC&amp;printsec=frontcover&amp;dq=cartesian+tensor&amp;hl=en&amp;sa=X&amp;ei=JjawUd6BO6qb0wWExYDIBA&amp;ved=0CDAQ6AEwADgK#v=onepage&amp;q=cartesian%20tensor&amp;f=false}}
*{{cite book|title=Vectors, Pure and Applied: A General Introduction to Linear Algebra|author=T. W. Körner|page=216|volume=|publisher=Cambridge University Press|year=2012|series=|isbn=11070-3356-X|url=https://books.google.com/books?id=RO_TMc7ETPEC&amp;dq=cartesian+tensor&amp;source=gbs_navlinks_s}}
*{{cite book|title=Relativity and Geometry|author=R. Torretti|publisher=Courier Dover Publications|year=1996 |page=103|isbn=0-4866-90466|url=https://books.google.com/books?id=vpW_sBxwr88C&amp;pg=PA103&amp;dq=cartesian+tensor&amp;hl=en&amp;sa=X&amp;ei=qDGwUau2K8KlO9S7gegF&amp;ved=0CFMQ6AEwBg#v=onepage&amp;q=cartesian%20tensor&amp;f=false}}
*{{cite book|title=Tensor Calculus|author1=J. J. L. Synge |author2=A. Schild |page=128|volume=|publisher=Courier Dover Publications|year=1978|series=|isbn=0-4861-4139-X|url=https://books.google.com/books?id=8vlGhlxqZjsC&amp;pg=PA127&amp;dq=cartesian+tensor&amp;hl=en&amp;sa=X&amp;ei=qDGwUau2K8KlO9S7gegF&amp;ved=0CEwQ6AEwBQ#v=onepage&amp;q=cartesian%20tensor&amp;f=false}}
*{{cite book|title=Dynamic Analysis of Robot Manipulators: A Cartesian Tensor Approach|author1=C. A. Balafoutis |author2=R. V. Patel |volume=131|publisher=Springer|year=1991|series=The Kluwer International Series in Engineering and Computer Science: Robotics: vision, manipulation and sensors|isbn=0792-391-454|url=https://books.google.com/books?id=7BcpyUjmLpUC&amp;printsec=frontcover&amp;dq=cartesian+tensor&amp;hl=en&amp;sa=X&amp;ei=qDGwUau2K8KlO9S7gegF&amp;ved=0CDEQ6AEwAA#v=onepage&amp;q=cartesian%20tensor&amp;f=false}}
*{{cite book|title=Robotic systems: advanced techniques and applications|author=S. G. Tzafestas|volume=|publisher=Springer|year=1992|series=|isbn=0-792-317-491|url=https://books.google.com/books?id=iG5W9IQhjd4C&amp;pg=PA45&amp;dq=cartesian+tensor&amp;hl=en&amp;sa=X&amp;ei=JjawUd6BO6qb0wWExYDIBA&amp;ved=0CFQQ6AEwBjgK#v=onepage&amp;q=cartesian%20tensor&amp;f=false}}
*{{cite book|title=Mathematical Methods In Classical And Quantum Physics|author1=T. Dass |author2=S. K. Sharma |volume=|publisher=Universities Press|page=144|year=1998|series=|isbn=817-371-0899|url=https://books.google.com/books?id=AQCsAxpZ7ToC&amp;pg=PA144&amp;dq=cartesian+tensor&amp;hl=en&amp;sa=X&amp;ei=JjawUd6BO6qb0wWExYDIBA&amp;ved=0CFgQ6AEwBzgK#v=onepage&amp;q=cartesian%20tensor&amp;f=false}}
*{{cite book|title=Cartesian Tensors: An Introduction|author=G. F. J. Temple|volume=|publisher=DOVER PUBN Incorporated|page=|year=2004|series=Dover Books on Mathematics Series|isbn=0-4864-3908-9|url=https://books.google.com/books?id=56WqzKbTMtMC&amp;dq=Cartesian+Tensors:+An+Introduction+temple&amp;hl=en&amp;sa=X&amp;ei=2aW0UYkYxIDQBen9gPAI&amp;ved=0CDQQ6AEwAA}}
*{{cite book|title=Cartesian Tensors|author= H. Jeffreys|authorlink=Harold Jeffreys|volume=|publisher=The University Press|page=|year=1961|series=|isbn=|url=https://books.google.com/books?id=oOYIAQAAIAAJ&amp;q=cartesian+tensors&amp;dq=cartesian+tensors&amp;hl=en&amp;sa=X&amp;ei=0jSyUeSaJOyS0QW95YD4DA&amp;ved=0CDkQ6AEwAQ}}

==External links==
*[http://www.mso.anu.edu.au/~geoff/HEA/A1_Tensors.pdf ''Cartesian Tensors'']
*[http://www.ce.udel.edu/faculty/kaliakin/appendix_tensors.pdf V. N. Kaliakin, ''Brief Review of Tensors'', University of Delaware]
*[http://www.damtp.cam.ac.uk/user/reh10/lectures/nst-mmii-chapter3.pdf R. E. Hunt, ''Cartesian Tensors'', University of Cambridge]

{{Tensors}}

[[Category:Linear algebra]]
[[Category:Tensors]]
[[Category:Applied mathematics]]</text>
      <sha1>bnhz4zty16expd732w1u95kbw8xnccc</sha1>
    </revision>
  </page>
  <page>
    <title>Cathetus</title>
    <ns>0</ns>
    <id>2568071</id>
    <revision>
      <id>805963370</id>
      <parentid>805959091</parentid>
      <timestamp>2017-10-18T19:26:28Z</timestamp>
      <contributor>
        <username>ScrapIronIV</username>
        <id>23177185</id>
      </contributor>
      <comment>Reverted 1 edit by [[Special:Contributions/LikeTheCats|LikeTheCats]] ([[User talk:LikeTheCats|talk]]): RVV. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2279">{{about|the geometric meaning|the plant|Phyllanthus}}

[[Image:Triangle Sides.svg|200px|frame|A right-angled triangle where ''c''&lt;sub&gt;1&lt;/sub&gt; and ''c''&lt;sub&gt;2&lt;/sub&gt; are the catheti and ''h'' is the hypotenuse]]

In a [[right triangle]], a '''cathetus''' (originally from the [[Greek language|Greek]] word {{lang|el|Κάθετος}}; plural: '''catheti'''), commonly known as a '''leg''', is either of the sides that are adjacent to the [[right angle]]. It is occasionally called a "side about the right angle". The side opposite the right angle is the [[hypotenuse]]. In the context of the hypotenuse, the catheti are sometimes referred to simply as "the other two sides". 

If the catheti of a right triangle have equal lengths, the triangle is [[isosceles]]. If they have different lengths, a distinction can be made between the minor (shorter) and major (longer) cathetus. The [[ratio]] of the lengths of the catheti defines the [[trigonometric functions]] tangent and cotangent &lt;!-- PLEASE DO NOT link [[tangent]] (that article is about a tangent line) or [[cotangent]] (that redirects to [[Trigonometric functions]] --&gt; of the [[acute angle]]s in the triangle: the ratio &lt;math&gt;c_1/c_2&lt;/math&gt;  is the tangent of the acute angle adjacent to &lt;math&gt;c_2&lt;/math&gt; and is also the cotangent of the acute angle adjacent to &lt;math&gt;c_1&lt;/math&gt;. 

In a right triangle, the length of a cathetus is the [[geometric mean]] of the length of the adjacent segment cut by the [[altitude (geometry)|altitude]] to the hypotenuse and the length of the whole hypotenuse.

By the [[Pythagorean theorem]], the sum of the squares of the lengths of the catheti is equal to the square of the length of the hypotenuse.

The term '''leg''', in addition to referring to a cathetus of a right triangle, is also used to refer to either of the equal sides of an [[isosceles triangle]] or to either of the non-parallel sides of a [[trapezoid]].

== References ==
* Bernhardsen, T. ''Geographic Information Systems: An Introduction'', 3rd ed.  New York: Wiley, p.&amp;nbsp;271, 2002.
* [http://www.encyclopediaofmath.org/index.php/Cathetus ''Cathetus'' at Encyclopaedia of Mathematics]
* {{MathWorld | urlname=Cathetus | title=Cathetus}}

{{Wiktionary}}

[[Category:Elementary geometry]]
[[Category:Triangle geometry]]</text>
      <sha1>r3jb1esvbe0zffjsyt6n08xh60hxjvc</sha1>
    </revision>
  </page>
  <page>
    <title>Chain-complete partial order</title>
    <ns>0</ns>
    <id>1012798</id>
    <revision>
      <id>787977473</id>
      <parentid>787977234</parentid>
      <timestamp>2017-06-28T18:44:13Z</timestamp>
      <contributor>
        <username>EmilJ</username>
        <id>94981</id>
      </contributor>
      <minor/>
      <comment>EmilJ moved page [[Chain complete]] to [[Chain-complete partial order]]: (1) Fix English grammar (hyphen needed). (2) Use full name of the concept, not just a bare adjective.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2996">In [[order theory|order-theoretic mathematics]], a [[partially ordered set]] is '''chain-complete''' if every [[chain (order theory)|chain]] in it has a [[least upper bound]]. It is '''ω-complete''' when every increasing sequence of elements (a type of [[countability|countable]] chain) has a least upper bound; the same notion can be extended to other cardinalities of chains.&lt;ref name="m76"&gt;{{citation
 | last = Markowsky | first = George
 | issue = 1
 | journal = Algebra Universalis
 | mr = 0398913
 | pages = 53–68
 | title = Chain-complete posets and directed sets with applications
 | volume = 6
 | year = 1976
 | doi=10.1007/bf02485815}}.&lt;/ref&gt;

==Examples==
Every [[complete lattice]] is chain-complete. Unlike complete lattices, chain-complete posets are relatively common. Examples include:

* The set of all [[linearly independent]] subsets of a [[vector space]] ''V'', ordered by [[inclusion (set theory)|inclusion]]. 
* The set of all [[partial function]]s on a set, ordered by [[Restriction (mathematics)|restriction]].
* The set of all partial [[choice function]]s on a collection of non-empty sets, ordered by restriction.
* The set of all [[prime ideals]] of a [[ring (mathematics)|ring]], ordered by inclusion.
* The set of all [[consistent]] theories of a [[first-order logic|first-order language]].

==Properties==
A poset is chain-complete if and only if it is a [[complete partial order|pointed dcpo]]&lt;ref name="m76"/&gt;. However, this equivalence requires the [[axiom of choice]].

[[Zorn's lemma]] states that, if a poset has an upper bound for every chain, then it has a [[maximal element]]. Thus, it applies to chain-complete posets, but is more general in that it allows chains that have upper bounds but do not have least upper bounds.

Chain-complete posets also obey the [[Bourbaki–Witt theorem]], a [[fixed point theorem]] stating that, if ''f'' is a function from a chain complete poset to itself with the property that, for all ''x'', ''f''(''x'')&amp;nbsp;≥&amp;nbsp;''x'', then ''f'' has a fixed point. This theorem, in turn, can be used to prove that Zorn's lemma is a consequence of the [[axiom of choice]].&lt;ref&gt;{{citation
 | last = Bourbaki | first = Nicolas | authorlink = Nicolas Bourbaki
 | journal = Archiv der Mathematik
 | mr = 0047739
 | pages = 434–437 (1951)
 | title = Sur le théorème de Zorn
 | volume = 2
 | year = 1949
 | doi=10.1007/bf02036949}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last = Witt | first = Ernst | authorlink = Ernst Witt
 | journal = [[Mathematische Nachrichten]]
 | mr = 0039776
 | pages = 434–438
 | title = Beweisstudien zum Satz von M. Zorn
 | volume = 4
 | year = 1951
 | doi=10.1002/mana.3210040138}}.&lt;/ref&gt;

By analogy with the [[Dedekind–MacNeille completion]] of a partially ordered set, every partially ordered set can be extended uniquely to a minimal chain-complete poset.&lt;ref name="m76"/&gt;

==See also==
*[[Completeness (order theory)]]

==References==
{{reflist}}

{{DEFAULTSORT:Chain Complete}}
[[Category:Order theory]]</text>
      <sha1>328v52dw1w3pk9jovi5506s31g1ym0e</sha1>
    </revision>
  </page>
  <page>
    <title>Complex plane</title>
    <ns>0</ns>
    <id>217628</id>
    <revision>
      <id>843164069</id>
      <parentid>837151153</parentid>
      <timestamp>2018-05-27T09:23:04Z</timestamp>
      <contributor>
        <username>Störm</username>
        <id>25154634</id>
      </contributor>
      <minor/>
      <comment>Moving from [[Category:Control theory]] to [[Category:Classical control theory]] using [[c:Help:Cat-a-lot|Cat-a-lot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="30355">[[Image:Complex conjugate picture.svg|right|thumb|Geometric representation of ''z'' and its conjugate ''z̅'' in the complex plane. The distance along the light blue line from the origin to the point ''z'' is the ''modulus'' or ''absolute value'' of ''z''. The angle φ is the ''argument'' of ''z''.]]

In [[mathematics]], the '''complex plane''' or '''''z''-plane''' is a geometric representation of the [[complex numbers]] established by the '''real axis''' and the [[perpendicular]] '''imaginary axis'''. It can be thought of as a modified [[Cartesian plane]], with the [[real part]] of a complex number represented by a displacement along the x-axis, and the [[imaginary part]] by a displacement along the y-axis.&lt;ref&gt;Although this is the most common mathematical meaning of the phrase "complex plane", it is not the only one possible. Alternatives include the [[split-complex number|split-complex plane]] and the [[dual number]]s, as introduced by [[Quotient ring#Alternative complex planes|quotient rings]].&lt;/ref&gt;

The concept of the complex plane allows a geometric interpretation of complex numbers. Under [[addition]], they add like [[vector (geometry)|vector]]s. The [[multiplication]] of two complex numbers can be expressed most easily in [[polar coordinates]]—the magnitude or ''modulus'' of the product is the product of the two [[absolute value]]s, or moduli, and the [[angle]] or ''argument'' of the product is the sum of the two angles, or arguments. In particular, multiplication by a complex number of modulus 1 acts as a rotation.

The complex plane is sometimes known as the '''Argand plane'''. 

==Notational conventions==
In [[complex analysis]], the complex numbers are customarily represented by the symbol ''z'', which can be separated into its real (''x'') and imaginary (''y'') parts:
:&lt;math&gt;
z = x + iy
&lt;/math&gt;
for example: ''z'' = 4 + 5''i'', where ''x'' and ''y'' are real numbers, and ''i'' is the [[imaginary unit]]. In this customary notation the complex number ''z'' corresponds to the point (''x'', ''y'') in the [[cartesian coordinate system|Cartesian plane]].

In the Cartesian plane the point (''x'', ''y'') can also be represented in [[polar coordinates]] as
:&lt;math&gt;
(x, y) = (r\cos\theta, r\sin\theta)\qquad(r, \theta) = \left(\sqrt{x^2+y^2}, \quad \arctan\frac{y}{x}\right).
&lt;/math&gt;

In the Cartesian plane it may be assumed that the [[inverse trigonometric function|arctangent]] takes values from &amp;minus;''π/2'' to ''π/2'' (in [[radian]]s), and some care must be taken to define the ''real'' arctangent function for points (''x'', ''y'') when ''x'' ≤ 0.&lt;ref&gt;A detailed definition of the complex argument in terms of the ''real'' arctangent can be found [[Complex number#Conversion from the Cartesian form to the polar form|here]].&lt;/ref&gt; In the complex plane these polar coordinates take the form
:&lt;math&gt;
z = x + iy = |z|\left(\cos\theta + i\sin\theta\right) = |z|e^{i\theta}
&lt;/math&gt;
where
:&lt;math&gt;
|z| =  \sqrt{x^2+y^2}; \quad \theta = \arg(z) = \frac{1}{i}\ln\frac{z}{|z|} = -i\ln\frac{z}{|z|}.\,
&lt;/math&gt;&lt;ref&gt;It can be shown (Whittaker &amp; Watson, 1927, ''Appendix'') that all the familiar properties of the complex exponential function, the trigonometric functions, and the complex logarithm can be deduced directly from the [[power series#Examples|power series]] for ''e''&lt;sup&gt;''z''&lt;/sup&gt;. In particular, the principal value of log''r'', where |''r''| = 1, can be calculated without reference to any geometrical or trigonometric construction.&lt;/ref&gt;

Here |''z''| is the ''absolute value'' or ''modulus'' of the complex number ''z'';  ''θ'', the ''argument'' of ''z'', is usually taken on the interval 0 ≤ ''θ'' &amp;lt; 2''π''; and the last equality (to |''z''|''e''&lt;sup&gt;''iθ''&lt;/sup&gt;) is taken from [[Euler's formula]]. Notice that without the constraint on the range of ''θ'', the argument of ''z'' is multi-valued, because the [[exponential function#On the complex plane|complex exponential function]] is periodic, with period 2''π i''. Thus, if ''θ'' is one value of arg(''z''), the other values are given by arg(''z'') = ''θ'' + 2''nπ'', where ''n'' is any integer ≠ 0.&lt;ref&gt;(Whittaker &amp; Watson, 1927, p. 10)&lt;/ref&gt; 

While seldom used explicitly, the geometric view of the complex numbers is implicitly based on its [[Euclidean space#Euclidean structure|structure of a Euclidean vector space]] of dimension&amp;nbsp;2, where the [[inner product]] of complex numbers {{math|''w''}} and {{math|''z''}} is given by &lt;math&gt;\Re(w\overline{z})&lt;/math&gt;; then for a complex number {{math|''z''}} its absolute value |{{math|''z''}}| coincides with its Euclidean norm, and its argument {{math|arg(''z'')}} with the angle turning from 1&amp;nbsp;to&amp;nbsp;{{math|''z''}}.

The theory of [[line integral#Complex line integral|contour integration]] comprises a major part of complex analysis. In this context the direction of travel around a closed curve is important&amp;nbsp;– reversing the direction in which the curve is traversed multiplies the value of the integral by &amp;minus;1. By convention the ''positive'' direction is counterclockwise. For example, the [[unit circle]] is traversed in the positive direction when we start at the point ''z'' = 1, then travel up and to the left through the point ''z'' = ''i'', then down and to the left through &amp;minus;1, then down and to the right through &amp;minus;''i'', and finally up and to the right to ''z'' = 1, where we started.

Almost all of complex analysis is concerned with [[complex analysis#Complex functions|complex functions]]&amp;nbsp;– that is, with functions that map some subset of the complex plane into some other (possibly overlapping, or even identical) subset of the complex plane. Here it is customary to speak of the [[domain (mathematics)|domain]] of ''f''(''z'') as lying in the ''z''-plane, while referring to the [[range (mathematics)|range]] or ''image'' of ''f''(''z'') as a set of points in the ''w''-plane. In symbols we write
:&lt;math&gt;
z = x + iy;\qquad f(z) = w = u + iv
&lt;/math&gt;
and often think of the function ''f'' as a transformation from the ''z''-plane (with coordinates (''x'', ''y'')) into the ''w''-plane (with coordinates (''u'', ''v'')).

== Argand diagram ==
[[File:Argandgaussplane.png|right|150px]]
'''Argand diagram''' refers to a geometric [[Plot (graphics)|plot]] of complex numbers as points z=x+iy using the x-axis as the real axis and y-axis as the imaginary axis.&lt;ref&gt;{{cite web|url=http://mathworld.wolfram.com/ArgandDiagram.html|title=Argand Diagram|first=Weisstein, Eric|last=W.|date=|website=mathworld.wolfram.com|accessdate=19 April 2018}}&lt;/ref&gt;. Such plots are named after [[Jean-Robert Argand]] (1768–1822), although they were first described by Norwegian–Danish land surveyor and mathematician [[Caspar Wessel]] (1745–1818).&lt;ref&gt;Wessel's memoir was presented to the Danish Academy in 1797; Argand's paper was published in 1806. (Whittaker &amp; Watson, 1927, p. 9)&lt;/ref&gt; Argand diagrams are frequently used to plot the positions of the [[zeros and poles]] of a function in the complex plane. 
{{clear}}

==Stereographic projections==
{{main|Stereographic projection}}
[[File:Stereographic projection in 3D.svg|thumb|right|[[Riemann sphere]] which maps all points on a sphere except one to all points on the complex plane]]

It can be useful to think of the complex plane as if it occupied the surface of a sphere. Given a [[sphere]] of unit radius, place its center at the origin of the complex plane, oriented so that the equator on the sphere coincides with the unit circle in the plane, and the north pole is "above" the plane.

We can establish a [[bijection|one-to-one correspondence]] between the points on the surface of the sphere minus the north pole and the points in the complex plane as follows. Given a point in the plane, draw a straight line connecting it with the north pole on the sphere. That line will intersect the surface of the sphere in exactly one other point. The point {{nowrap|1=''z'' = 0}} will be projected onto the south pole of the sphere. Since the interior of the unit circle lies inside the sphere, that entire region ({{nowrap|{{abs|''z''}} &amp;lt; 1}}) will be mapped onto the southern hemisphere. The unit circle itself ({{nowrap|1={{abs|''z''}} = 1}}) will be mapped onto the equator, and the exterior of the unit circle ({{nowrap|{{abs|''z''}} &amp;gt; 1}}) will be mapped onto the northern hemisphere, minus the north pole. Clearly this procedure is reversible&amp;nbsp;– given any point on the surface of the sphere that is not the north pole, we can draw a straight line connecting that point to the north pole and intersecting the flat plane in exactly one point.

Under this stereographic projection the north pole itself is not associated with any point in the complex plane. We perfect the one-to-one correspondence by adding one more point to the complex plane&amp;nbsp;– the so-called ''[[point at infinity]]''&amp;nbsp;– and identifying it with the north pole on the sphere. This topological space, the complex plane plus the point at infinity, is known as the [[Riemann sphere|extended complex plane]]. We speak of a single "point at infinity" when discussing complex analysis. There are two points at infinity (positive, and negative) on the [[real number line]], but there is only one point at infinity (the north pole) in the extended complex plane.&lt;ref&gt;(Flanigan, 1983, p.&amp;nbsp;305)&lt;/ref&gt;

Imagine for a moment what will happen to the lines of latitude and longitude when they are projected from the sphere onto the flat plane. The lines of latitude are all parallel to the equator, so they will become perfect circles centered on the origin {{nowrap|1=''z'' = 0}}. And the lines of longitude will become straight lines passing through the origin (and also through the "point at infinity", since they pass through both the north and south poles on the sphere).

This is not the only possible yet plausible stereographic situation of the projection of a sphere onto a plane consisting of two or more values. For instance, the north pole of the sphere might be placed on top of the origin {{nowrap|1=''z'' = −1}} in a plane that is tangent to the circle. The details don't really matter. Any stereographic projection of a sphere onto a plane will produce one "point at infinity", and it will map the lines of latitude and longitude on the sphere into circles and straight lines, respectively, in the plane.

==Cutting the plane==
When discussing functions of a complex variable it is often convenient to think of a '''cut''' in the complex plane. This idea arises naturally in several different contexts.

===Multi-valued relationships and branch points===
Consider the simple two-valued relationship
:&lt;math&gt;
w = f(z) = \pm\sqrt{z} = z^{1/2}.
&lt;/math&gt;

Before we can treat this relationship as a single-valued [[function (mathematics)|function]], the range of the resulting value must be restricted  somehow. When dealing with the square roots of non-negative real numbers this is easily done. For instance, we can just define
:&lt;math&gt;
y = g(x) = \sqrt{x}\ = x^{1/2}
&lt;/math&gt;
to be the non-negative real number ''y'' such that ''y''&lt;sup&gt;2&lt;/sup&gt; = ''x''. This idea doesn't work so well in the two-dimensional complex plane. To see why, let's think about the way the value of ''f''(''z'') varies as the point ''z'' moves around the unit circle. We can write
:&lt;math&gt;
z = re^{i\theta}\quad\mbox{and take}\quad w=z^{1/2} = \sqrt{r}\,e^{i\theta/2}\qquad(0\leq\theta\leq 2\pi).
&lt;/math&gt;

Evidently, as ''z'' moves all the way around the circle, ''w'' only traces out one-half of the circle. So one continuous motion in the complex plane has transformed the positive square root ''e''&lt;sup&gt;0&lt;/sup&gt; = 1 into the negative square root ''e''&lt;sup&gt;''iπ''&lt;/sup&gt; = &amp;minus;1.

This problem arises because the point ''z'' = 0 has just one square root, while every other complex number ''z'' ≠ 0 has exactly two square roots. On the real number line we could circumvent this problem by erecting a "barrier" at the single point ''x'' = 0. A bigger barrier is needed in the complex plane, to prevent any closed contour from completely encircling the [[branch point]] ''z'' = 0. This is commonly done by introducing a '''branch cut'''; in this case the "cut" might extend from the point ''z'' = 0 along the positive real axis to the point at infinity, so that the argument of the variable ''z'' in the cut plane is restricted to the range 0 ≤ arg(''z'') &amp;lt; 2''π''.

We can now give a complete description of ''w'' = ''z''&lt;sup&gt;½&lt;/sup&gt;. To do so we need two copies of the ''z''-plane, each of them cut along the real axis. On one copy we define the square root of 1 to be e&lt;sup&gt;0&lt;/sup&gt; = 1, and on the other we define the square root of 1 to be ''e''&lt;sup&gt;''iπ''&lt;/sup&gt; = &amp;minus;1. We call these two copies of the complete cut plane ''sheets''. By making a continuity argument we see that the (now single-valued) function ''w'' = ''z''&lt;sup&gt;½&lt;/sup&gt; maps the first sheet into the upper half of the ''w''-plane, where 0 ≤ arg(''w'') &amp;lt; ''π'', while mapping the second sheet into the lower half of the ''w''-plane (where ''π'' ≤ arg(''w'') &amp;lt; 2''π'').&lt;ref name="Moretti"&gt;(Moretti, 1964, pp. 113–119)&lt;/ref&gt;

The branch cut in this example doesn't have to lie along the real axis. It doesn't even have to be a straight line. Any continuous curve connecting the origin ''z'' = 0 with the point at infinity would work. In some cases the branch cut doesn't even have to pass through the point at infinity. For example, consider the relationship
:&lt;math&gt;
w = g(z) = \left(z^2 - 1\right)^{1/2}.
&lt;/math&gt;

Here the polynomial ''z''&lt;sup&gt;2&lt;/sup&gt; &amp;minus; 1 vanishes when ''z'' = ±1, so ''g'' evidently has two branch points. We can "cut" the plane along the real axis, from &amp;minus;1 to 1, and obtain a sheet on which ''g''(''z'') is a single-valued function. Alternatively, the cut can run from ''z'' = 1 along the positive real axis through the point at infinity, then continue "up" the negative real axis to the other branch point, ''z'' = &amp;minus;1.

This situation is most easily visualized by using the [[complex plane#Stereographic projections|stereographic projection described above]]. On the sphere one of these cuts runs longitudinally through the southern hemisphere, connecting a point on the equator (''z'' = &amp;minus;1) with another point on the equator (''z'' = 1), and passing through the south pole (the origin, ''z'' = 0) on the way. The second version of the cut runs longitudinally through the northern hemisphere and connects the same two equatorial points by passing through the north pole (that is, the point at infinity).

===Restricting the domain of meromorphic functions===
A [[meromorphic function]] is a complex function that is [[holomorphic function|holomorphic]] and therefore [[analytic function|analytic]] everywhere in its domain except at a finite, or [[countable set|countably infinite]], number of points.&lt;ref&gt;See also [[Proof that holomorphic functions are analytic]].&lt;/ref&gt; The points at which such a function cannot be defined are called the [[Pole (complex analysis)|pole]]s of the meromorphic function. Sometimes all these poles lie in a straight line. In that case mathematicians may say that the function is "holomorphic on the cut plane". Here's a simple example.

The [[gamma function]], defined by
:&lt;math&gt;
\Gamma (z) = \frac{e^{-\gamma z}}{z} \prod_{n=1}^\infty \left[\left(1+\frac{z}{n}\right)^{-1}e^{z/n}\right]
&lt;/math&gt;
where ''γ'' is the [[Euler–Mascheroni constant]], and has simple poles at 0, &amp;minus;1, &amp;minus;2, &amp;minus;3, ... because exactly one denominator in the [[infinite product]] vanishes when ''z'' is zero, or a negative integer.&lt;ref&gt;It can be shown that the infinite product for &amp;Gamma;(''z'') is [[uniform convergence|uniformly convergent]] on any bounded region where none of its denominators vanish; therefore it defines a meromorphic function on the complex plane. (Whittaker &amp; Watson, 1927, pp. 235–236)&lt;/ref&gt; Since all its poles lie on the negative real axis, from ''z'' = 0 to the point at infinity, this function might be described as "holomorphic on the cut plane, the cut extending along the negative real axis, from 0 (inclusive) to the point at infinity."

Alternatively, Γ(''z'') might be described as "holomorphic in the cut plane with &amp;minus;''π'' &amp;lt; arg(''z'') &amp;lt; ''π'' and excluding the point ''z'' = 0."

Notice that this cut is slightly different from the '''branch cut''' we've already encountered, because it actually ''excludes'' the negative real axis from the cut plane. The branch cut left the real axis connected with the cut plane on one side (0 ≤ ''θ''), but severed it from the cut plane along the other side (''θ'' &amp;lt; 2''π'').

Of course, it's not actually necessary to exclude the entire line segment from ''z'' = 0 to &amp;minus;∞ to construct a domain in which Γ(''z'') is holomorphic. All we really have to do is '''puncture''' the plane at a countably infinite set of points {0, &amp;minus;1, &amp;minus;2, &amp;minus;3, ...}. But a closed contour in the punctured plane might encircle one or more of the poles of Γ(''z''), giving a [[line integral|contour integral]] that is not necessarily zero, by the [[residue theorem]]. By cutting the complex plane we ensure not only that Γ(''z'') is holomorphic in this restricted domain&amp;nbsp;– we also ensure that the contour integral of Γ over any closed curve lying in the cut plane is identically equal to zero.

===Specifying convergence regions===
Many complex functions are defined by [[series (mathematics)|infinite series]], or by [[generalized continued fraction|continued fractions]]. A fundamental consideration in the analysis of these infinitely long expressions is identifying the portion of the complex plane in which they converge to a finite value. A cut in the plane may facilitate this process, as the following examples show.

Consider the function defined by the infinite series
:&lt;math&gt;
f(z) = \sum_{n=1}^\infty \left(z^2 + n\right)^{-2}.
&lt;/math&gt;

Since ''z''&lt;sup&gt;2&lt;/sup&gt; = (&amp;minus;''z'')&lt;sup&gt;2&lt;/sup&gt; for every complex number ''z'', it's clear that ''f''(''z'') is an [[even and odd functions|even function]] of ''z'', so the analysis can be restricted to one half of the complex plane. And since the series is undefined when
:&lt;math&gt;
z^2 + n = 0 \quad \Leftrightarrow \quad z = \pm i\sqrt{n},
&lt;/math&gt;
it makes sense to cut the plane along the entire imaginary axis and establish the convergence of this series where the real part of ''z'' is not zero before undertaking the more arduous task of examining ''f''(''z'') when ''z'' is a pure imaginary number.&lt;ref&gt;When Re(''z'') &amp;gt; 0 this sum converges uniformly on any bounded domain by comparison with ''&amp;zeta;''(2), where ''&amp;zeta;''(''s'') is the [[Riemann zeta function]].&lt;/ref&gt;

In this example the cut is a mere convenience, because the points at which the infinite sum is undefined are isolated, and the ''cut'' plane can be replaced with a suitably ''punctured'' plane. In some contexts the cut is necessary, and not just convenient. Consider the infinite periodic continued fraction
:&lt;math&gt;
f(z) = 1 + \cfrac{z}{1 + \cfrac{z}{1 + \cfrac{z}{1 + \cfrac{z}{\ddots}}}}.
&lt;/math&gt;

It [[convergence problem|can be shown]] that ''f''(''z'') converges to a finite value if and only if ''z'' is not a negative real number such that ''z'' &amp;lt; &amp;minus;¼. In other words, the convergence region for this continued fraction is the cut plane, where the cut runs along the negative real axis, from &amp;minus;¼ to the point at infinity.&lt;ref&gt;(Wall, 1948, p. 39)&lt;/ref&gt;

==Gluing the cut plane back together==
{{Main|Riemann surface}}

We have [[#Multi-valued relationships and branch points|already seen]] how the relationship
:&lt;math&gt;
w = f(z) = \pm\sqrt{z} = z^{1/2}
&lt;/math&gt;
can be made into a single-valued function by splitting the domain of ''f'' into two disconnected sheets. It is also possible to "glue" those two sheets back together to form a single '''Riemann surface''' on which {{nowrap|1=''f''(''z'') = ''z''&lt;sup&gt;1/2&lt;/sup&gt;}} can be defined as a holomorphic function whose image is the entire ''w''-plane (except for the point {{nowrap|1=''w'' = 0}}). Here's how that works.

Imagine two copies of the cut complex plane, the cuts extending along the positive real axis from {{nowrap|1=''z'' = 0}} to the point at infinity. On one sheet define {{nowrap|0 ≤ arg(''z'') &amp;lt; 2''π''}}, so that {{nowrap|1=1&lt;sup&gt;1/2&lt;/sup&gt; = ''e''&lt;sup&gt;0&lt;/sup&gt; = 1}}, by definition. On the second sheet define {{nowrap|2''π'' ≤ arg(''z'') &amp;lt; 4''π''}}, so that {{nowrap|1=1&lt;sup&gt;1/2&lt;/sup&gt; = ''e''&lt;sup&gt;''iπ''&lt;/sup&gt; = &amp;minus;1}}, again by definition. Now flip the second sheet upside down, so the imaginary axis points in the opposite direction of the imaginary axis on the first sheet, with both real axes pointing in the same direction, and "glue" the two sheets together (so that the edge on the first sheet labeled "{{nowrap|1=''θ'' = 0}}" is connected to the edge labeled "{{nowrap|''θ'' &amp;lt; 4''π''}}" on the second sheet, and the edge on the second sheet labeled "{{nowrap|1=''θ'' = 2''π''}}" is connected to the edge labeled "{{nowrap|''θ'' &amp;lt; 2''π''}}" on the first sheet). The result is the Riemann surface domain on which {{nowrap|1=''f''(''z'') = ''z''&lt;sup&gt;1/2&lt;/sup&gt;}} is single-valued and holomorphic (except when {{nowrap|1=''z'' = 0}}).&lt;ref name="Moretti"/&gt;

To understand why ''f'' is single-valued in this domain, imagine a circuit around the unit circle, starting with {{nowrap|1=''z'' = 1}} on the first sheet. When {{nowrap|0 ≤ ''θ'' &amp;lt; 2''π''}} we are still on the first sheet. When {{nowrap|1=''θ'' = 2''π''}} we have crossed over onto the second sheet, and are obliged to make a second complete circuit around the branch point {{nowrap|1=''z'' = 0}} before returning to our starting point, where {{nowrap|1=''θ'' = 4''π''}} is equivalent to {{nowrap|1=''θ'' = 0}}, because of the way we glued the two sheets together. In other words, as the variable ''z'' makes two complete turns around the branch point, the image of ''z'' in the ''w''-plane traces out just one complete circle.

Formal differentiation shows that
:&lt;math&gt;
f(z) = z^{1/2} \quad\Rightarrow\quad f^\prime (z) = {\textstyle \frac{1}{2}}z^{-1/2}
&lt;/math&gt;

from which we can conclude that the derivative of ''f'' exists and is finite everywhere on the Riemann surface, except when {{nowrap|1=''z'' = 0}} (that is, ''f'' is holomorphic, except when {{nowrap|1=''z'' = 0}}).

How can the Riemann surface for the function
:&lt;math&gt;
w = g(z) = \left(z^2 - 1\right)^{1/2},
&lt;/math&gt;
also discussed [[#Multi-valued relationships and branch points|above]], be constructed? Once again we begin with two copies of the ''z''-plane, but this time each one is cut along the real line segment extending from {{nowrap|1=''z'' = &amp;minus;1}} to {{nowrap|1=''z'' = 1}} &amp;ndash; these are the two branch points of ''g''(''z''). We flip one of these upside down, so the two imaginary axes point in opposite directions, and glue the corresponding edges of the two cut sheets together. We can verify that ''g'' is a single-valued function on this surface by tracing a circuit around a circle of unit radius centered at {{nowrap|1=''z'' = 1}}. Commencing at the point {{nowrap|1=''z'' = 2}} on the first sheet we turn halfway around the circle before encountering the cut at {{nowrap|1=''z'' = 0}}. The cut forces us onto the second sheet, so that when ''z'' has traced out one full turn around the branch point {{nowrap|1=''z'' = 1}}, ''w'' has taken just one-half of a full turn, the sign of ''w'' has been reversed (since {{nowrap|1=''e''&lt;sup&gt;''iπ''&lt;/sup&gt; = &amp;minus;1}}), and our path has taken us to the point {{nowrap|1=''z'' = 2}} on the '''second''' sheet of the surface. Continuing on through another half turn we encounter the other side of the cut, where {{nowrap|1=''z'' = 0}}, and finally reach our starting point ({{nowrap|1=''z'' = 2}} on the '''first''' sheet) after making two full turns around the branch point.

The natural way to label {{nowrap|1=''θ'' = arg(''z'')}} in this example is to set {{nowrap|&amp;minus;''π'' &amp;lt; ''θ'' ≤ ''π''}} on the first sheet, with {{nowrap|''π'' &amp;lt; ''θ'' ≤ 3''π''}} on the second. The imaginary axes on the two sheets point in opposite directions so that the counterclockwise sense of positive rotation is preserved as a closed contour moves from one sheet to the other (remember, the second sheet is ''upside down''). Imagine this surface embedded in a three-dimensional space, with both sheets parallel to the ''xy''-plane. Then there appears to be a vertical hole in the surface, where the two cuts are joined together. What if the cut is made from {{nowrap|1=''z'' = &amp;minus;1}} down the real axis to the point at infinity, and from {{nowrap|1=''z'' = 1}}, up the real axis until the cut meets itself? Again a Riemann surface can be constructed, but this time the "hole" is horizontal. [[Topology|Topologically speaking]], both versions of this Riemann surface are equivalent&amp;nbsp;– they are [[orientable]] two-dimensional surfaces of [[genus (mathematics)#Orientable surface|genus]] one.

==Use of the complex plane in control theory==
In [[control theory]], one use of the complex plane is known as the '[[S plane|s-plane]]'.  It is used to visualise the roots of the equation describing a system's behaviour (the characteristic equation) graphically.  The equation is normally expressed as a polynomial in the parameter 's' of the [[Laplace transform]], hence the name 's' plane. Points in the s-plane take the form &lt;math&gt;s=\sigma+j\omega&lt;/math&gt;, where ''&lt;nowiki/&gt;'j'&lt;nowiki/&gt;'' is used instead of the usual ''&lt;nowiki/&gt;'i''' to represent the imaginary component.

Another related use of the complex plane is with the [[Nyquist stability criterion]].  This is a geometric principle which allows the stability of a closed-loop feedback system to be determined by inspecting a [[Nyquist plot]] of its open-loop magnitude and phase response as a function of frequency (or loop [[transfer function]]) in the complex plane.

The 'z-plane' is a discrete-time version of the s-plane, where [[z-transform]]s are used instead of the Laplace transformation.

==Quadratic spaces==
The complex plane is associated with two distinct [[quadratic space]]s. For a point ''z'' = ''x'' + ''iy'' in the complex plane, the [[square (algebra)|squaring function]] ''z''&lt;sup&gt;2&lt;/sup&gt; and the norm-squared &lt;math&gt;x^2 + y^2 &lt;/math&gt; are both [[quadratic form]]s. The former is frequently neglected in the wake of the latter's use in setting a [[metric (mathematics)|metric]] on the complex plane. These distinct faces of the complex plane as a quadratic space arise in the construction of [[algebras over a field]] with the [[Cayley–Dickson process]]. That procedure can be applied to any [[field (mathematics)|field]], and different results occur for the fields ℝ and ℂ: when ℝ is the take-off field, then ℂ is constructed with the quadratic form &lt;math&gt;x^2 + y^2  ,&lt;/math&gt; but the process can also begin with ℂ and ''z''&lt;sup&gt;2&lt;/sup&gt;, and that case generates algebras that differ from those derived from ℝ. In any case, the algebras generated are [[composition algebra]]s; in this case the complex plane is the point set for two distinct composition algebras.

==Other meanings of "complex plane"==
The preceding sections of this article deal with the complex plane in terms of a geometric representation of the complex numbers. Although this usage of the term "complex plane" has a long and mathematically rich history, it is by no means the only mathematical concept that can be characterized as "the complex plane". There are at least three additional possibilities.
#Two-dimensional complex vector space, a "complex plane" in the sense that it is a two-dimensional vector space whose coordinates are ''complex numbers''. See also: {{section link|Complex affine space|Two dimensions}}.
#(1 + 1)-dimensional [[Minkowski space]], also known as the [[split-complex plane]], is a "complex plane" in the sense that the algebraic [[split-complex number]]s can be separated into two real components that are easily associated with the point {{nowrap|(''x'', ''y'')}} in the Cartesian plane.
#The set of [[dual number]]s over the reals can also be placed into one-to-one correspondence with the points {{nowrap|(''x'', ''y'')}} of the Cartesian plane, and represent another example of a "complex plane".

==Terminology==
While the terminology "complex plane" is historically accepted, the object could be more appropriately named "complex line" as it is a 1-dimensional [[complex vector space]].

==See also==
[[File:Mandelset hires.png|thumb|right|370px|[[Mandelbrot fractal]], imagined on a complex plane]]
* [[Constellation diagram]]
* [[Riemann sphere]]
* [[s-plane]]
* [[In-phase and quadrature components]]
* [[Real line]]

==Notes==
&lt;!--This article uses the Cite.php citation mechanism. If you would like more information on how to add references to this article, please see http://meta.wikimedia.org/wiki/Cite/Cite.php --&gt;
{{reflist}}

==References==
*{{cite book |first=Francis J. |last=Flanigan |title=Complex Variables: Harmonic and Analytic Functions |location= |publisher=Dover |year=1983 |isbn=0-486-61388-7 }}
*{{cite book |first=Gino |last=Moretti |title=Functions of a Complex Variable |publisher=Prentice-Hall |year=1964 }}
*{{cite book |first=H. S. |last=Wall |title=Analytic Theory of Continued Fractions |publisher=D. Van Nostrand Company |year=1948 }} Reprinted (1973) by Chelsea Publishing Company {{ISBN|0-8284-0207-8}}.
*{{cite book |authorlink=E. T. Whittaker |first=E. T. |last=Whittaker |authorlink2=G. N. Watson |first2=G. N. |last2=Watson |title=A Course in Modern Analysis |edition=Fourth |location= |publisher=Cambridge University Press |year=1927 }}

==External links==
{{Commons category}}
* {{MathWorld|urlname=ArgandDiagram|title=Argand Diagram}}
* Jean-Robert Argand, "Essai sur une manière de représenter des quantités imaginaires dans les constructions géométriques", 1806, online and analyzed on [https://www.bibnum.education.fr/mathematiques/geometrie/essai-sur-une-maniere-de-representer-des-quantites-imaginaires-dans-les-cons ''BibNum''] &lt;small&gt;[for English version, click 'à télécharger']&lt;/small&gt;

{{Complex numbers}}

[[Category:Complex analysis]]
[[Category:Complex numbers]]
[[Category:Classical control theory]]</text>
      <sha1>3zl1eyh65q5zs4ub15lzzk30ebwqdbb</sha1>
    </revision>
  </page>
  <page>
    <title>Dold–Kan correspondence</title>
    <ns>0</ns>
    <id>39431504</id>
    <revision>
      <id>829631383</id>
      <parentid>829247749</parentid>
      <timestamp>2018-03-09T20:29:43Z</timestamp>
      <contributor>
        <username>TakuyaMurata</username>
        <id>6707</id>
      </contributor>
      <minor/>
      <comment>/* top */ lk</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2619">In mathematics, more precisely, in the theory of [[simplicial set]]s, the '''Dold–Kan correspondence''' (named after [[Albrecht Dold]] and [[Daniel Kan]]) states&lt;ref&gt;{{harvs|txt|last1=Goerss|first1=Paul|last2=Jardine|first2=Rick|authorlink2=Rick Jardine|year=1999|loc=Ch 3. Corollary 2.3}}&lt;/ref&gt; that there is an equivalence between the category of (nonnegatively graded) [[chain complex]]es and the category of [[simplicial abelian group]]s. Moreover, under the equivalence, the &lt;math&gt;n&lt;/math&gt;th homology group of a chain complex is the &lt;math&gt;n&lt;/math&gt;th homotopy group of the corresponding simplicial abelian group, and a [[chain homotopy]] corresponds to a [[simplicial homotopy]]. (In fact, the correspondence preserves the respective standard [[model category|model structure]]s.) 

'''Example''': Let ''C'' be a chain complex that has an abelian group ''A'' in degree ''n'' and zero in other degrees. Then the corresponding simplicial group is the [[Eilenberg–MacLane space]] &lt;math&gt;K(A, n)&lt;/math&gt;.

There is also an [[∞-category]]-version of a Dold–Kan correspondence.&lt;ref&gt;{{harvnb|Lurie|2012|loc=§ 1.2.4.}}&lt;/ref&gt;

The book "Nonabelian Algebraic Topology" cited below has a Section 14.8 on [[cubical set|cubical]] versions of the Dold-Kan theorem, and relates them  to a previous equivalence of categories 
between cubical omega-groupoids and crossed complexes, which is fundamental to the work of that book.

== References ==
{{reflist}}
* {{Cite book | last1=Goerss | first1=Paul G. | last2=Jardine | first2=John F. | authorlink2=Rick Jardine | title=Simplicial Homotopy Theory | publisher=Birkhäuser | location=Basel, Boston, Berlin | series=Progress in Mathematics | isbn=978-3-7643-6064-1 | year=1999 | volume=174 | postscript=&lt;!--None--&gt;}}
*{{Lurie-HA}}
* A. Mathew, [http://people.fas.harvard.edu/~amathew/doldkan.pdf The Dold-Kan correspondence]
* {{Cite book | last1=Brown | first1=Ronald|authorlink1=Ronald Brown (mathematician)| last2=Higgins | first2=Philip J. | last3=Sivera | first3=Rafael |title=Nonabelian Algebraic Topology:  filtered spaces, crossed complexes, cubical homotopy groupoids | publisher=[[European Mathematical Society]] | location=Zurich | series=Tracts in Mathematics | isbn= 978-3-03719-083-8 | year=2011 | volume=15 | postscript=&lt;!--None--&gt;}}

== Further reading ==
*[[Jacob Lurie]], [http://www.math.harvard.edu/~lurie/papers/DAG-I.pdf DAG-I]

== External links ==
*{{nlab|id=Dold-Kan+correspondence|title=Dold-Kan correspondence}}

{{DEFAULTSORT:Dold-Kan correspondence}}
[[Category:Simplicial sets]]
[[Category:Theorems in abstract algebra]]

{{categorytheory-stub}}</text>
      <sha1>oaypahkpuq7ywbnxfsr48l2l8t9njvf</sha1>
    </revision>
  </page>
  <page>
    <title>Effect system</title>
    <ns>0</ns>
    <id>677451</id>
    <revision>
      <id>767820125</id>
      <parentid>749304551</parentid>
      <timestamp>2017-02-28T03:56:03Z</timestamp>
      <contributor>
        <username>Cedar101</username>
        <id>374440</id>
      </contributor>
      <minor/>
      <comment>{{code}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4332">{{No footnotes|date=September 2010}}
In [[computing]], an '''effect system''' is a [[formal system]] which describes the computational effects of computer programs, such as [[side effect (computer science)|side effect]]s. An effect system can be used to provide a compile-time check of the possible effects of the program.

The effect system extends the notion of type to have an "effect" component, which comprises an '''effect kind''' and a '''region'''. The effect kind describes ''what'' is being done, and the region describes ''with what'' it is being done.

An effect system is typically an extension of a [[type system]]. The term "type and effect system" is sometimes used in this case. Often, a type of a value is denoted together with its effect as ''type ! effect'', where both the type component and the effect component mention certain regions (for example, a type of a mutable memory cell is parameterized by the label of the memory region in which the cell resides).

Some examples of the behaviors that can be described by effect systems include:
* Reading, writing or allocating memory: the effect kind is ''read'', ''write'', ''allocate'' or ''free'', and the region is the point of the program where allocation was performed (i.e., each program point where allocation is performed is assigned a unique label, and region information is statically propagated along the dataflow). Most functions working with memory will actually be polymorphic in the region variable: for example, a function that swaps two locations in memory will have type {{code|forall r1 r2, unit ! {{mset|read r1, read r2, write r1, write r2}}}}.
* Working with resources, such as files: for example, the effect kind may be ''open'', ''read'' and ''close'', and again, the region is the point of the program where the resource is opened.
* Control transfers with continuations and long jumps: the effect kind may be ''[[goto]]'' (i.e. the piece of code may perform a jump) and ''[[comefrom]]'' (i.e. the piece of code may be the target of a jump), and the region denotes the point of the program from which or to which the jump may be performed.
* Java's [[checked exception]]s are an example of an effect system: the effect kind is ''throws'' and the region is the type of the exception being thrown.

Effect systems may be used to prove the external purity of certain internally impure definitions: for example, if a function internally allocates and modifies a region of memory, but the function's type does not mention the region, then the corresponding effect may be erased from the function's effect.

==References==
; Textbook chapters
*{{Cite book|last1=Hankin|first1=Chris|last2=Nielson|first2=Flemming|last3=Nielson|first3=Hanne Riis|title=Principles of Program Analysis|year=1999|publisher=Springer|location=Berlin|isbn=3-540-65410-0}}
*{{Cite book|last1=Gifford|first1=David|last2=Turbak|first2=Franklyn A.|last3=Sheldon|first3=Mark A.|title=Design Concepts in Programming Languages|year=2008|publisher=MIT Press|location=Cambridge, Mass|isbn=0-262-20175-5|chapter=16}}
; Overview papers
* {{cite journal|first1=Flemming|last1=Nielson|first2=Hanne Riis|last2=Nielson|title=Type and Effect Systems|journal=Correct System Design: Recent Insight and Advances|year=1999|pages=114&amp;ndash;136|publisher=Springer-Verlag|series=[[Lecture Notes in Computer Science]]|volume=1710|isbn=3-540-66624-9|url=http://www.cs.ucla.edu/~palsberg/tba/papers/nielson-nielson-csd99.pdf}}

==Further reading==
*{{Cite journal|last1=Marino|first1=Daniel|first2=Todd |last2=Millstein|year=2008|title=A Generic Type and Effect System|journal=Proceedings of the 4th international workshop on Types In Language Design And Implementation|publisher=[[Association for Computing Machinery|ACM]]|page=39|doi=10.1145/1481861.1481868|url=http://www.cs.ucla.edu/~todd/research/tldi09.pdf}}
*{{Cite journal|last1=Lucassen|first1=John M.|first2=David K. |last2=Gifford|year=1988|title=Polymorphic Effect Systems|journal=Proceedings of the 15th ACM [[SIGPLAN]]-[[SIGACT]] Symposium on [[Principles of Programming Languages]]|publisher=[[Association for Computing Machinery|ACM]]|pages=47–57|doi=10.1145/73560.73564|url=https://scholar.google.com/scholar?cluster=4860658690557317521}}

{{DEFAULTSORT:Effect System}}
[[Category:Program analysis]]
[[Category:Type theory]]</text>
      <sha1>dhk3055mj8jko9e1959b6s17pbzoyxc</sha1>
    </revision>
  </page>
  <page>
    <title>Eleazar Chisma</title>
    <ns>0</ns>
    <id>11354632</id>
    <revision>
      <id>805557186</id>
      <parentid>726534895</parentid>
      <timestamp>2017-10-16T04:22:37Z</timestamp>
      <contributor>
        <username>DGG</username>
        <id>2157954</id>
      </contributor>
      <minor/>
      <comment>lnk</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3990">''For other people named Eleazer. see:  [[Eleazar (name)]]''
{{Eras of the Halakha}}
'''Eleazar Chisma'''  (Ḥisma; {{lang-he|'''אלעזר חסמא'''}}, ''"Eleazar Chasma''", or '''אלעזר בן חסמא''', "''Eleazar ben Chasma''") was a [[Tannaim|tanna]] (sage) of the second and third generations (2nd century); he was a disciple of [[Joshua ben Hananiah]] and [[Gamaliel II]]. (Ḥag. 3a; Hor. 10a).

==Etymology==
In their use of the word "ben" in connection with his cognomen "Ḥisma" or "Ḥasma" (see Geiger, "Schriften," iv. 343, and Strack, "Einleitung in den Thalmud," 2d ed., p.&amp;nbsp;81), the sources are inconsistent; its insertion, however, seems justifiable. "Ḥisma," is not an adjectival cognomen (see Eleazar I.), but a locative, the place probably being identical with Hizmeh (see Luncz, "Jerusalem," vi. 67; {{cite encyclopedia |last=Selbie|first=J. A.|author=|authorlink=|coauthors= |editor=[[James Hastings]] |encyclopedia=[[Hastings' Dictionary of the Bible|A Dictionary of the Bible]] |title=Azmaveth |url=http://www.ccel.org/ccel/hastings/dictv1/Page_208.html |accessdate=  |edition= |year= 1898|month= |publisher= |volume= I|location= |id= |isbn= |doi= |pages= 208|quote= }}; hence "ben Ḥisma" means "son of [= "native of"] Ḥisma" (compare R. H. 17a; Meg. 19a; Ḳid. ii. 3).

Several ''[[halakot]]'' are preserved under Eleazar's name in the ''[[Mishnah]]'' (Ter. iii. 5; B. M. vii. 5), and he is met with in halakic controversies with [[Eleazar ben Azariah]] and [[Rabbi Akiva]] (Neg. vii. 2; Sifra, Tazria', i. 2), and with [[Eliezer ben Jacob I]] (Pes. 32a; Yalḳ., Lev. 638); and to him is ascribed the economic rule that the employee is not entitled to a proportion of his employer's produce greater than the amount of his wages (B. M. vii. 5, 92a; Sifre, Deut. 266).

==Exegesis==
Some ''[[haggadot]]'' also are ascribed to him (Mek., Beshallaḥ Wayassa', 4; ib., Amalek, 1; Yoma 19b). Conjointly with Rabbi Joshua, he gives an allegorical reason for [[Amalek]]'s attack on Israel (Ex. xvii. 8 et seq.) just at the time it occurred. Citing Job viii. 11, "Can a rush grow up without mire? Can the flag grow without water?" he remarks, "Even so is it impossible for Israel to flourish without the Law; and since they had neglected the Law [see Ex. xvii. 1-7], an enemy was ordered out to war against them" (compare Yalḳ. to Ex. l.c., § 262; anonymous in Yalḳ. to Job l.c., § 904). Again, he cites Isa. xliii. 22, "But thou hast not called on me, O Jacob," and applies it to those who are not devout in their prayers, but while reciting the "Shema'" communicate with their neighbors by sign language (compare Yalḳ. to Isa. l.c., § 318).

==Scientific knowledge==
Not only was Chizma possessed of wide rabbinic learning, but he was also an adept in the sciences. Joshua, introducing him and Johanan b. (Gudgada) Nuri to the notice of Patriarch Gamaliel II, remarked of them that they could approximately calculate the number of drops contained in the ocean (Hor. 10a). As they were very poor, Gamaliel appointed them to remunerative offices in the academy (Sifre, Deut. 14; Yalḳ., Deut. 902; Hor. l.c.). Probably it was here—because the academicians sought from him instruction in secular science—that Eleazar remarked, "The laws concerning birds' nests and those concerning the incipient uncleanness of woman are elements of the Law, while astronomy and geometry are only condiments of wisdom" (Ab. iii. 18; Ab. R. N. xxvii. 2).

==References==
{{reflist}}

*Bacher, Ag. Tan. i. 374;
*[[Brüll]], Mebo a-Mishnah, i. 149;
*Frankel, Darke a-Mishnah, p.&amp;nbsp;134;
*Geiger, Schriften, iv. 343;
*Heilprin, Seder a-Dorot, ii., s.v.;
*Weiss, Dor, ii. 122;
*Zacuto, Yuḥasin, ed. Filipowski, p.&amp;nbsp;41b.

==External links==
*[http://jewishencyclopedia.com/view.jsp?artid=147&amp;letter=E Jewish Encyclopedia]
{{JewishEncyclopedia}}

{{Tannaim}}

[[Category:Mishnah rabbis]]
[[Category:Ancient mathematicians]]
[[Category:2nd-century rabbis]]</text>
      <sha1>pdz4w0ws4gesnnxdac0ffq5u8d7oslz</sha1>
    </revision>
  </page>
  <page>
    <title>Exhaustion by compact sets</title>
    <ns>0</ns>
    <id>3521238</id>
    <revision>
      <id>784487722</id>
      <parentid>769807639</parentid>
      <timestamp>2017-06-08T16:11:54Z</timestamp>
      <contributor>
        <username>Magic links bot</username>
        <id>30707369</id>
      </contributor>
      <minor/>
      <comment>Replace [[Help:Magic links|magic links]] with templates per [[Special:Permalink/772743896#Future of magic links|local RfC]] and [[:mw:Requests for comment/Future of magic links|MediaWiki RfC]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1470">In [[mathematics]], especially [[mathematical analysis|analysis]], '''exhaustion by compact sets''' of an [[open set]] ''E'' in the [[Euclidean space]] '''R'''&lt;sup&gt;''n''&lt;/sup&gt; (or a [[manifold]] with [[Second countable|countable base]]) is an increasing [[sequence]] of [[compact set]]s &lt;math&gt;K_j&lt;/math&gt;, where by increasing we mean &lt;math&gt;K_j&lt;/math&gt; is a [[subset]] of &lt;math&gt;K_{j+1}&lt;/math&gt;, with the limit ([[set union|union]]) of the sequence being ''E''. 

Sometimes one requires the sequence of compact sets to satisfy one more property&amp;mdash;that &lt;math&gt;K_j&lt;/math&gt; is contained in the [[Interior (topology)|interior]] of &lt;math&gt;K_{j+1}&lt;/math&gt; for each &lt;math&gt;j&lt;/math&gt;. This, however, is dispensed in '''R'''&lt;sup&gt;''n''&lt;/sup&gt; or a manifold with countable base.

For example, consider a unit open disk and the concentric closed disk of each radius inside. That is let &lt;math&gt;E = \{ z; |z| &lt; 1 \}&lt;/math&gt; and &lt;math&gt;K_j = \{ z; |z| \le (1 - 1/j) \}&lt;/math&gt;. Then taking the limit (union) of the sequence &lt;math&gt;K_j&lt;/math&gt; gives ''E''. The example can be easily generalized in other dimensions.

==See also==
*[[σ-compact space]]

==References==

* [[Leon Ehrenpreis]], ''Theory of Distributions for Locally Compact Spaces'', [[American Mathematical Society]], 1982. {{ISBN|0-8218-1221-1}}.

==External links==

* {{planetmath reference|title=Exhaustion by compact sets|id=7103}}

[[Category:Compactness (mathematics)]]
[[Category:Mathematical analysis]]


{{Mathanalysis-stub}}</text>
      <sha1>tjno26uoi78nr63toijak0z7fz6xzfa</sha1>
    </revision>
  </page>
  <page>
    <title>Exponent bias</title>
    <ns>0</ns>
    <id>1069091</id>
    <revision>
      <id>830151563</id>
      <parentid>829361742</parentid>
      <timestamp>2018-03-13T01:55:32Z</timestamp>
      <contributor>
        <username>Ic727</username>
        <id>12364387</id>
      </contributor>
      <comment>This edit is a revision of a previous edit which was well intentioned but inadequate. The three statements dealing with the interpretation of the exponent should now be more accurate and clearer than their original versions.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2712">In [[IEEE 754]] [[floating point]] numbers, the [[exponent]] is [[bias]]ed in the engineering sense of the word – the value stored is offset from the actual value by the '''exponent bias'''. 
Biasing is done because exponents have to be signed values in order to be able to represent both tiny and huge values, but [[two's complement]], the usual representation for signed values, would make comparison harder.

To solve this problem the exponent is stored as an unsigned value suitable for comparison, and when being interpreted it is converted into an exponent within a signed range by subtracting the bias.

By arranging the fields such that the sign bit takes the most significant bit position, the biased exponent takes the middle position, then the [[significand|mantissa]] will be the least significant bits  and the resulting value will be ordered properly. This is the case whether or not it is interpreted as a floating point or integer value. The purpose of this is to enable high speed comparisons between floating point numbers using fixed point hardware.

To calculate the bias for an arbitrarily sized floating point number apply the formula 2&lt;sup&gt;k−1&lt;/sup&gt;&amp;nbsp;−&amp;nbsp;1 where k is the number of bits in the exponent.&lt;ref&gt;{{cite book|last=O'Hallaron|first=Randal E. Bryant, David R.|title=Computer systems : a programmer's perspective|year=2010|publisher=Prentice Hall|location=Boston|isbn=978-0-13-610804-7|edition=2nd}}&lt;/ref&gt;

When interpreting the floating-point number, the bias is subtracted to retrieve the actual exponent.

* For a [[single precision|single-precision]] number, the exponent is stored in the range 1 .. 254 (0 and 255 have special meanings), and is interpreted by subtracting the bias for an 8-bit exponent (127) to get an exponent value in the range −126 .. +127.
* For a [[double precision|double-precision]] number, the exponent is stored in the range 1 .. 2046 (0 and 2047 have special meanings), and is interpreted by subtracting the bias for an 11-bit exponent (1023) to get an exponent value in the range −1022 .. +1023.
* For a [[quad precision|quad-precision]] number, the exponent is stored in the range 1 .. 32766 (0 and 32767 have special meanings), and is interpreted by subtracting the bias for a 15-bit exponent (16383) to get an exponent value in the range −16382 .. +16383.

== History ==

The floating point format of the [[IBM 704]] introduced the use of a biased exponent in 1954.

==See also==
* [[Offset binary]]
* [[Excess-K]]
* [[Excess-15]]
* [[Excess-64]]
* [[Excess-127]]
* [[Excess-128]]
* [[Excess-129]]
* [[Excess-1023]]
* [[Excess-16383]]

==References==
{{Reflist}}

{{DEFAULTSORT:Exponent Bias}}
[[Category:Computer arithmetic]]</text>
      <sha1>kva29b2penqju9h1wx5j3geadxesqye</sha1>
    </revision>
  </page>
  <page>
    <title>Extractor (mathematics)</title>
    <ns>0</ns>
    <id>9309</id>
    <revision>
      <id>478974773</id>
      <parentid>321430226</parentid>
      <timestamp>2012-02-26T17:20:14Z</timestamp>
      <contributor>
        <username>Rcsprinter123</username>
        <id>12557839</id>
      </contributor>
      <minor/>
      <comment>Repairing links to disambiguation pages - [[Wikipedia:DPL|You can help!]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2154">An &lt;math&gt;(N,M,D,K,\epsilon)&lt;/math&gt; -'''extractor''' is a [[bipartite graph]] with &lt;math&gt;N&lt;/math&gt; nodes on the left and &lt;math&gt;M&lt;/math&gt; nodes on the right such that each node on the left has &lt;math&gt;D&lt;/math&gt; neighbors (on the right), which has the added property that
for any subset &lt;math&gt;A&lt;/math&gt; of the left vertices of size at least &lt;math&gt;K&lt;/math&gt;, the distribution on right vertices obtained by choosing a random node in &lt;math&gt;A&lt;/math&gt; and then following a random [[graph theory|edge]] to get a node x on the right side is &lt;math&gt;\epsilon&lt;/math&gt;-close to the [[Uniform distribution (continuous)|uniform distribution]] in terms of [[total variation distance]].

A [[disperser]] is a related graph. 

An equivalent way to view an extractor is as a bivariate function 

:&lt;math&gt;E : [N] \times [D] \rightarrow [M]&lt;/math&gt; 

in the natural way. With this view it turns out that the extractor property is equivalent to: for any source of randomness &lt;math&gt;X&lt;/math&gt; that gives &lt;math&gt;n&lt;/math&gt; [[bit]]s with [[min-entropy]] &lt;math&gt;\log K&lt;/math&gt;, the distribution &lt;math&gt; E(X,U_D) &lt;/math&gt; is &lt;math&gt;\epsilon&lt;/math&gt;-close to &lt;math&gt;U_M&lt;/math&gt;, where &lt;math&gt;U_T&lt;/math&gt; denotes the uniform distribution on &lt;math&gt;[T]&lt;/math&gt;.

Extractors are interesting when they can be constructed with small &lt;math&gt;K,D,\epsilon&lt;/math&gt; relative to &lt;math&gt;N&lt;/math&gt; and &lt;math&gt;M&lt;/math&gt; is as close to &lt;math&gt;KD&lt;/math&gt; (the total randomness in the input sources) as possible.

Extractor functions were originally researched as a way to ''extract'' [[randomness]] from weakly random sources. ''See'' [[randomness extractor]].

Using the [[probabilistic method]] it is easy to show that extractor graphs with really good parameters exist. The challenge is to find explicit or [[polynomial time]] computable examples of such graphs with good parameters. Algorithms that compute extractor (and disperser) graphs have found many applications in [[computer science]].

==References==
* Ronen Shaltiel, [http://www.cs.haifa.ac.il/~ronen/online_papers/survey.ps Recent developments in extractors] - a survey

[[Category:Graph families]]
[[Category:Pseudorandomness]]
[[Category:Theoretical computer science]]</text>
      <sha1>dvvjgzbkvh8ytplru0xvszje9ta4i5w</sha1>
    </revision>
  </page>
  <page>
    <title>Fermat's theorem (stationary points)</title>
    <ns>0</ns>
    <id>4142944</id>
    <revision>
      <id>808890216</id>
      <parentid>808887350</parentid>
      <timestamp>2017-11-05T21:10:21Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor/>
      <comment>Dating maintenance tags: {{No references}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15821">{{About|Fermat's theorem concerning the maximums and minimums of functions|other theorems also named after Pierre de Fermat|Fermat's theorem}}
{{no references|date=November 2017}}
In mathematics, '''Fermat's theorem''' (also known as '''interior extremum theorem''') is a method to find local [[maximum|maxima]] and [[minimum|minima]] of [[differentiable function]]s on [[open sets]] by showing that every local [[Maxima and minima|extremum]] of the function is a [[stationary point]] (the function [[derivative]] is zero at that point).  Fermat's theorem is a [[theorem]] in [[real analysis]], named after [[Pierre de Fermat]].

By using Fermat's theorem, the potential extrema of a function &lt;math&gt;\displaystyle f&lt;/math&gt;, with derivative &lt;math&gt;\displaystyle f'&lt;/math&gt;, are found by solving an [[equation]] in &lt;math&gt;\displaystyle f'&lt;/math&gt;. Fermat's theorem gives only a [[necessary condition]] for extreme function values, as some stationary points are [[inflection point]]s (not a maximum or minimum). The function's [[second derivative]], if it exists, can determine if any stationary point is a maximum, minimum, or inflection point.

==Statement==
One way to state Fermat's theorem is that, if a [[function (mathematics)|function]] has a local [[Maxima and minima|extremum]] at some point and is [[differentiable function|differentiable]] there, then the function's derivative at that point must be zero. In precise mathematical language:

:Let &lt;math&gt;f\colon (a,b) \rightarrow \mathbb{R}&lt;/math&gt; be a function and suppose that &lt;math&gt;x_0 \in (a,b)&lt;/math&gt; is a point where &lt;math&gt;f&lt;/math&gt; has a local extremum. If &lt;math&gt;f&lt;/math&gt; is [[differentiable function|differentiable]] at &lt;math&gt;\displaystyle x_0&lt;/math&gt;, then &lt;math&gt;f'(x_0) = 0&lt;/math&gt;.

Another way to understand the theorem is via the [[contrapositive]] statement: if the derivative of a function at any point is not zero, then there is not a local extremum at that point. Formally:
:If &lt;math&gt;f&lt;/math&gt; is [[differentiable function|differentiable]] at &lt;math&gt;x_0 \in (a,b)&lt;/math&gt;, and &lt;math&gt;f'(x_0) \neq 0&lt;/math&gt;, then &lt;math&gt;x_0&lt;/math&gt; is not a local extremum of &lt;math&gt;f&lt;/math&gt;.''

===Corollary===
The [[Global minimum|global extrema]] of a function ''f'' on a domain ''A'' occur only at [[Bounded function|boundaries]], non-differentiable points, and stationary points.
If &lt;math&gt;x_0&lt;/math&gt; is a global extremum of ''f'', then one of the following is true:
* '''boundary:''' &lt;math&gt;x_0&lt;/math&gt; is in the boundary of ''A''
* '''non-differentiable:''' ''f'' is not differentiable at &lt;math&gt;x_0&lt;/math&gt;
* '''stationary point:''' &lt;math&gt;x_0&lt;/math&gt; is a stationary point of ''f''

===Extension===

In higher dimensions, exactly the same statement holds; however, the proof is slightly more complicated. The complication is that in 1 dimension, one can either move left or right from a point, while in higher dimensions, one can move in many directions. Thus, if the derivative does not vanish, one must argue that there is ''some'' direction in which the function increases – and thus in the opposite direction the function decreases. This is the only change to the proof or the analysis.

The statement can also be extended to [[differentiable manifolds]]. If &lt;math&gt;f : M \to \mathbb{R}&lt;/math&gt; is a [[Differentiable manifold#Differentiable functions|differentiable function]] on a manifold &lt;math&gt;M&lt;/math&gt;, then its local extrema must be [[Critical point (mathematics)|critical points]] of &lt;math&gt;f&lt;/math&gt;, in particular points where the [[exterior derivative]] &lt;math&gt;df&lt;/math&gt; is [[Zero of a function|zero]].&lt;ref&gt;{{cite web|title=Is Fermat's theorem about local extrema true for smooth manifolds?|url=https://math.stackexchange.com/questions/1392981/is-fermats-theorem-about-local-extrema-true-for-smooth-manifolds|website=Mathematics Stack Exchange|accessdate=21 April 2017}}&lt;/ref&gt;

==Applications==
{{See also|maxima and minima}}
Fermat's theorem is central to the calculus method of determining [[maxima and minima]]: in one dimension, one can find extrema by simply computing the stationary points (by computing the zeros of the derivative), the non-differentiable points, and the boundary points, and then investigating this set to determine the extrema.

One can do this either by evaluating the function at each point and taking the maximum, or by analyzing the derivatives further, using the [[first derivative test]], the [[second derivative test]], or the [[higher-order derivative test]].

==Intuitive argument==
Intuitively, a differentiable function is approximated by its derivative – a differentiable function behaves infinitesimally like a [[linear function]] &lt;math&gt;a+bx,&lt;/math&gt; or more precisely, &lt;math&gt;f(x_0) + f'(x_0)\cdot (x-x_0).&lt;/math&gt; Thus, from the perspective that "if ''f'' is differentiable and has non-vanishing derivative at &lt;math&gt;x_0,&lt;/math&gt; then it does not attain an extremum at &lt;math&gt;x_0,&lt;/math&gt;" the intuition is that if the derivative at &lt;math&gt;x_0&lt;/math&gt; is positive, the function is ''increasing'' near &lt;math&gt;x_0,&lt;/math&gt; while if the derivative is negative, the function is ''decreasing'' near &lt;math&gt;x_0.&lt;/math&gt; In both cases, it cannot attain a maximum or minimum, because its value is changing. It can only attain a maximum or minimum if it "stops" – if the derivative vanishes (or if it is not differentiable, or if one runs into the boundary and cannot continue). However, making "behaves like a linear function" precise requires careful analytic proof.

More precisely, the intuition can be stated as: if the derivative is positive, there is ''some point'' to the right of &lt;math&gt;x_0&lt;/math&gt; where ''f'' is greater, and ''some point'' to the left of &lt;math&gt;x_0&lt;/math&gt; where ''f'' is less, and thus ''f'' attains neither a maximum nor a minimum at &lt;math&gt;x_0.&lt;/math&gt; Conversely, if the derivative is negative, there is a point to the right which is lesser, and a point to the left which is greater. Stated this way, the proof is just translating this into equations and verifying "how much greater or less".

The [[Intuition (knowledge)|intuition]] is based on the behavior of [[polynomial]] functions. Assume that function ''f'' has a maximum at ''x''&lt;sub&gt;0&lt;/sub&gt;, the reasoning being similar for a function minimum. If &lt;math&gt;\displaystyle x_0 \in (a,b)&lt;/math&gt; is a local maximum then, roughly, there is a (possibly small) [[neighbourhood (mathematics)|neighborhood]] of &lt;math&gt;\displaystyle x_0&lt;/math&gt; such as the function "is [[increasing function|increasing]] before" and "decreasing after"&lt;ref group="note"&gt;This intuition is only correct for [[continuously differentiable]] &lt;math&gt;\left(C^1\right)&lt;/math&gt; functions, while in general it is not literally correct&amp;mdash;a function need not be increasing up to a local maximum: it may instead be oscillating, so neither increasing nor decreasing, but simply the local maximum is greater than any values in a small neighborhood to the left or right of it. See details in the pathologies.&lt;/ref&gt; &lt;math&gt;\displaystyle x_0&lt;/math&gt;. As the derivative is positive for an increasing function and negative for a decreasing function, &lt;math&gt;\displaystyle f'&lt;/math&gt; is positive before and negative after &lt;math&gt;\displaystyle x_0&lt;/math&gt;. &lt;math&gt;\displaystyle f'&lt;/math&gt; doesn't skip values (by [[Darboux's theorem (analysis)|Darboux's theorem]]), so it has to be zero at some point between the positive and negative values. The only point in the neighbourhood where it is possible to have &lt;math&gt;\displaystyle f'(x) = 0&lt;/math&gt; is &lt;math&gt;\displaystyle x_0&lt;/math&gt;.

The theorem (and its proof below) is more general than the intuition in that it doesn't require the function to be differentiable over a neighbourhood around &lt;math&gt;\displaystyle x_0&lt;/math&gt;. It is sufficient for the function to be differentiable only in the extreme point.

==Proof==

=== Proof 1: Non-vanishing derivatives implies not extremum ===
Suppose that ''f'' is differentiable at &lt;math&gt;x_0 \in (a,b),&lt;/math&gt; with derivative ''K,'' and assume [[without loss of generality]] that &lt;math&gt;K &gt; 0,&lt;/math&gt; so the tangent line at &lt;math&gt;x_0&lt;/math&gt; has positive slope (is increasing). Then there is a neighborhood of &lt;math&gt;x_0&lt;/math&gt; on which the secant lines through &lt;math&gt;x_0&lt;/math&gt; all have positive slope, and thus to the right of &lt;math&gt;x_0,&lt;/math&gt; ''f'' is greater, and to the left of &lt;math&gt;x_0,&lt;/math&gt; ''f'' is lesser.

The schematic of the proof is:
* an infinitesimal statement about derivative (tangent line) ''at'' &lt;math&gt;x_0&lt;/math&gt; implies
* a local statement about difference quotients (secant lines) ''near'' &lt;math&gt;x_0,&lt;/math&gt; which implies
* a local statement about the ''value'' of ''f'' near &lt;math&gt;x_0.&lt;/math&gt;

Formally, by the definition of derivative, &lt;math&gt;f'(x_0) = K&lt;/math&gt; means that
:&lt;math&gt;\lim_{\varepsilon \to 0} \frac{f(x_0+\varepsilon)-f(x_0)}{\varepsilon} = K.&lt;/math&gt;
In particular, for sufficiently small &lt;math&gt;\varepsilon&lt;/math&gt; (less than some &lt;math&gt;\varepsilon_0&lt;/math&gt;), the fraction must be at least &lt;math&gt;K/2,&lt;/math&gt; by the definition of limit. Thus on the interval &lt;math&gt;(x_0-\varepsilon_0,x_0+\varepsilon_0)&lt;/math&gt; one has:
:&lt;math&gt;\frac{f(x_0+\varepsilon)-f(x_0)}{\varepsilon} &gt; K/2;&lt;/math&gt;
one has replaced the ''equality'' in the limit (an infinitesimal statement) with an ''inequality'' on a neighborhood (a local statement). Thus, rearranging the equation, if &lt;math&gt;\varepsilon &gt; 0,&lt;/math&gt; then:
:&lt;math&gt;f(x_0+\varepsilon) &gt; f(x_0) + (K/2)\varepsilon &gt; f(x_0),&lt;/math&gt;
so on the interval to the right, ''f'' is greater than &lt;math&gt;f(x_0),&lt;/math&gt; and if &lt;math&gt;\varepsilon &lt; 0,&lt;/math&gt; then:
:&lt;math&gt;f(x_0+\varepsilon) &lt; f(x_0) + (K/2)\varepsilon &lt; f(x_0),&lt;/math&gt;
so on the interval to the left, ''f'' is less than &lt;math&gt;f(x_0).&lt;/math&gt;

Thus &lt;math&gt;x_0&lt;/math&gt; is not a local or global maximum or minimum of ''f.''

=== Proof 2: Extremum implies derivative vanishes ===
Alternatively, one can start by assuming that &lt;math&gt;\displaystyle x_0&lt;/math&gt; is a local maximum, and then prove that the derivative is 0.

Suppose that &lt;math&gt;\displaystyle x_0&lt;/math&gt; is a local maximum (a similar proof applies if &lt;math&gt;\displaystyle x_0&lt;/math&gt; is a local minimum). Then there &lt;math&gt;\exists \, \delta &gt; 0 &lt;/math&gt; such that &lt;math&gt;(x_0 - \delta,x_0 + \delta) \subset (a,b)&lt;/math&gt; and such that we have &lt;math&gt;f(x_0) \ge f(x)\, \forall  x&lt;/math&gt; with &lt;math&gt;\displaystyle |x - x_0| &lt; \delta &lt;/math&gt;. Hence for any &lt;math&gt;h \in (0,\delta)&lt;/math&gt; we notice that it holds

:&lt;math&gt;\frac{f(x_0+h) - f(x_0)}{h} \le 0.&lt;/math&gt;

Since the [[Limit of a function|limit]] of this ratio as &lt;math&gt;\displaystyle h&lt;/math&gt; gets close to 0 from above exists and is equal to &lt;math&gt;\displaystyle f'(x_0)&lt;/math&gt; we conclude that &lt;math&gt;f'(x_0) \le 0&lt;/math&gt;. On the other hand for &lt;math&gt;h \in (-\delta,0)&lt;/math&gt; we notice that

:&lt;math&gt;\frac{f(x_0+h) - f(x_0)}{h} \ge 0&lt;/math&gt;

but again the limit as &lt;math&gt;\displaystyle h&lt;/math&gt; gets close to 0 from below exists and is equal to &lt;math&gt;\displaystyle f'(x_0)&lt;/math&gt; so we also have &lt;math&gt;f'(x_0) \ge 0&lt;/math&gt;.

Hence we conclude that &lt;math&gt;\displaystyle f'(x_0) = 0.&lt;/math&gt;

== Cautions ==
A subtle misconception that is often held in the context of Fermat's theorem is to assume that it makes a stronger statement about local behavior than it does. Notably, Fermat's theorem does ''not'' say that functions (monotonically) "increase up to" or "decrease down from" a local maximum. This is very similar to the misconception that a limit means "monotonically getting closer to a point". For "well-behaved functions" (which here mean continuously differentiable), some intuitions hold, but in general functions may be ill-behaved, as illustrated below. The moral is that derivatives determine ''infinitesimal'' behavior, and that ''continuous'' derivatives determine ''local'' behavior.

=== Continuously differentiable functions ===
If ''f'' is [[continuously differentiable]] &lt;math&gt;\left(C^1\right)&lt;/math&gt; on an [[open set|open]] [[Neighborhood (mathematics)|neighborhood]] of the point &lt;math&gt;x_0&lt;/math&gt;, then &lt;math&gt;f'(x_0) &gt; 0&lt;/math&gt; does mean that ''f'' is increasing on a neighborhood of &lt;math&gt;x_0,&lt;/math&gt; as follows.

If &lt;math&gt;f'(x_0) = K &gt; 0&lt;/math&gt; and &lt;math&gt;f \in C^1,&lt;/math&gt; then
by continuity of the derivative, there is some &lt;math&gt;\varepsilon_0&gt;0&lt;/math&gt; such that &lt;math&gt;f'(x_0) &gt; K/2&lt;/math&gt;  &lt;math&gt;\forall x\in\left(x_0-\varepsilon_0,x_0+\varepsilon_0\right)&lt;/math&gt;. Then ''f'' is increasing on this interval, by the [[mean value theorem]]: the slope of any secant line is at least &lt;math&gt;K/2,&lt;/math&gt; as it equals the slope of some tangent line.

However, in the general statement of Fermat's theorem, where one is only given that the derivative ''at'' &lt;math&gt;x_0&lt;/math&gt; is positive, one can only conclude that secant lines ''through'' &lt;math&gt;x_0&lt;/math&gt; will have positive slope, for secant lines between &lt;math&gt;x_0&lt;/math&gt; and near enough points.

Conversely, if the derivative of ''f'' at a point is zero (&lt;math&gt;x_0&lt;/math&gt; is a stationary point), one cannot in general conclude anything about the local behavior of ''f'' – it may increase to one side and decrease to the other (as in &lt;math&gt;x^3&lt;/math&gt;), increase to both sides (as in &lt;math&gt;x^4&lt;/math&gt;), decrease to both sides (as in &lt;math&gt;-x^4&lt;/math&gt;), or behave in more complicated ways, such as oscillating (as in &lt;math&gt;x^2(\sin(1/x))&lt;/math&gt;, as discussed below).

One can analyze the infinitesimal behavior via the [[second derivative test]] and [[higher-order derivative test]], if the function is differentiable enough, and if the first non-vanishing derivative at &lt;math&gt;x_0&lt;/math&gt; is a continuous function, one can then conclude local behavior (i.e., if &lt;math&gt;f^{(k)}(x_0) \neq 0&lt;/math&gt; is the first non-vanishing derivative, and &lt;math&gt;f^{(k)}&lt;/math&gt; is continuous, so &lt;math&gt;f\in C^k&lt;/math&gt;), then one can treat ''f'' as locally close to a polynomial of degree ''k,'' since it behaves approximately as &lt;math&gt;f^{(k)}(x_0) (x-x_0)^k,&lt;/math&gt; but if the ''k''th derivative is not continuous, one cannot draw such conclusions, and it may behave rather differently.

=== Pathological functions ===
Consider the function &lt;math&gt;\sin(1/x)&lt;/math&gt; – it oscillates increasingly rapidly between &lt;math&gt;-1&lt;/math&gt; and &lt;math&gt;1&lt;/math&gt; as ''x'' approaches 0. Consider then &lt;math&gt;f(x)=(1+\sin(1/x))x^2&lt;/math&gt; – this oscillates increasingly rapidly between 0 and &lt;math&gt;2x^2&lt;/math&gt; as ''x'' approaches 0. If one extends this function by &lt;math&gt;f(0) := 0,&lt;/math&gt; then the function is continuous and everywhere differentiable (it is differentiable at 0 with derivative 0), but has rather unexpected behavior near 0: in any neighborhood of 0 it attains 0 infinitely many times, but also equals &lt;math&gt;2x^2&lt;/math&gt; (a positive number) infinitely often.

Continuing in this vein, &lt;math&gt;f(x)=(2+\sin(1/x))x^2&lt;/math&gt; oscillates between &lt;math&gt;x^2&lt;/math&gt; and &lt;math&gt;3x^2,&lt;/math&gt; and &lt;math&gt;x=0&lt;/math&gt; is a local and global minimum, but on no neighborhood of 0 is it decreasing down to or increasing up from 0 – it oscillates wildly near 0.

This pathology can be understood because, while the function is everywhere differentiable, it is not ''continuously'' differentiable: the limit of &lt;math&gt;f'(x)&lt;/math&gt; as &lt;math&gt;x \to 0&lt;/math&gt; does not exist, so the derivative is not continuous at 0. This reflects the oscillation between increasing and decreasing values as it approaches 0.

== See also ==
* [[Optimization (mathematics)]]
* [[Maxima and minima]]
* [[Derivative]]
* [[Extreme value]]
* [[arg max]] &lt;!-- lower case intentional --&gt;
* [[Adequality]]

== Notes ==
{{Reflist|group=note}}

==External links==
* {{PlanetMath reference|id=4450|title=Fermat's Theorem (stationary points)}}
* {{PlanetMath reference|id=4452|title=Proof of Fermat's Theorem (stationary points)}}

{{DEFAULTSORT:Fermat's Theorem (Stationary Points)}}
[[Category:Theorems in real analysis]]
[[Category:Differential calculus]]
[[Category:Articles containing proofs]]
[[Category:Theorems in calculus]]</text>
      <sha1>eq4kknfs211yia58ll81b5g08uwxf2b</sha1>
    </revision>
  </page>
  <page>
    <title>Fibered manifold</title>
    <ns>0</ns>
    <id>3247635</id>
    <revision>
      <id>830799155</id>
      <parentid>795625405</parentid>
      <timestamp>2018-03-17T00:42:41Z</timestamp>
      <contributor>
        <username>TakuyaMurata</username>
        <id>6707</id>
      </contributor>
      <comment>/* See also */ +1</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9941">In [[differential geometry]], in the category of [[differentiable manifold]]s, a '''fibered manifold''' is a [[surjective]] [[submersion (mathematics)|submersion]]&lt;ref&gt;{{harvnb|Kolář|1993|p=11}}&lt;/ref&gt;

:&lt;math&gt;\pi \colon E \to B\,&lt;/math&gt;

i.e. a surjective differentiable mapping such that at each point {{math|''y'' ∈ ''E''}} the tangent mapping

:&lt;math&gt;T_y\pi \colon T_{y}E \to T_{\pi(y)}B&lt;/math&gt;

is surjective, or, equivalently, its rank equals dim {{math|''B''}}.

== History ==
In [[topology]], the words '''fiber''' ('''Faser''' in German) and '''fiber space''' ('''gefaserter Raum''') appeared for the first time in a paper by [[Herbert Seifert|Seifert]] in 1932,&lt;ref&gt;{{harvnb|Seifert|1932}}&lt;/ref&gt; but his definitions are limited to a very special case. The main difference from the present day conception of a fiber space, however, was that for Seifert what is now called the '''base space''' (topological space) of a fiber (topological) space ''E'' was not part of the structure, but derived from it as a quotient space of ''E''. The first definition of '''fiber space''' is given by [[Hassler Whitney]] in 1935 &lt;ref&gt;{{harvnb|Whitney|1935}}&lt;/ref&gt; under the name '''sphere space''', but in 1940 Whitney changed the name to '''sphere bundle'''.&lt;ref&gt;{{harvnb|Whitney|1940}}&lt;/ref&gt;

The theory of fibered spaces, of which [[vector bundle]]s, [[principal bundle]]s, topological [[fibration]]s and fibered manifolds are a special case, is attributed to [[Herbert Seifert|Seifert]], [[Heinz Hopf|Hopf]], [[Jacques Feldbau|Feldbau]],&lt;ref&gt;{{harvnb|Feldbau|1939}}&lt;/ref&gt; [[Hassler Whitney|Whitney]], [[Norman Steenrod|Steenrod]], [[Charles Ehresmann|Ehresmann]],&lt;ref&gt;{{harvnb|Ehresman|1947a}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Ehresman|1947b}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Ehresman|1955}}&lt;/ref&gt; [[Jean-Pierre Serre|Serre]],&lt;ref&gt;{{harvnb|Serre|1951}}&lt;/ref&gt; and others.

== Formal definition ==
A triple {{math|(''E'', ''π'', ''B'')}} where {{math|''E''}} and {{math|''B''}} are differentiable manifolds and {{math|''π'': ''E'' → ''B''}} is a surjective submersion, is called a '''fibered manifold'''.&lt;ref&gt;{{harvnb|Krupka|Janyška|1990|p=47}}&lt;/ref&gt; ''E'' is called the '''total space''', ''B'' is called the '''base'''.

== Examples ==
* Every differentiable [[fiber bundle]] is a '''fibered manifold'''.
* Every differentiable [[covering space]] is a '''fibered manifold''' with discrete fiber.
* In general, a fibered manifold needs not to be a fiber bundle: different fibers may have different topologies. An example of this phenomenon may be constructed by taking the trivial bundle {{math|('''S'''&lt;sup&gt;1&lt;/sup&gt; × ℝ, ''π''&lt;sub&gt;1&lt;/sub&gt;, '''S'''&lt;sup&gt;1&lt;/sup&gt;)}} and deleting two points in to two different fibers over the base manifold {{math|'''S'''&lt;sup&gt;1&lt;/sup&gt;}}.The result is a new fibered manifold where all the fibers except two are connected.

== Properties ==
*Any surjective submersion {{math|''π'': ''E'' → ''B''}} is open: for each open {{math|''V'' ⊂ ''E''}}, the set {{math|''π''(''V'') ⊂ ''B''}} is open in {{math|''B''}}.
*Each fiber {{math|''π''&lt;sup&gt;−1&lt;/sup&gt;(''b'') ⊂ ''E'', ''b'' ∈ ''B''}} is a closed embedded submanifold of {{mvar|E}} of dimension {{math|dim ''E'' − dim ''B''}}.&lt;ref&gt;{{harvnb|Giachetta|Mangiarotti|Sardanashvily|1997|p=11}}&lt;/ref&gt;
*A fibered manifold admits local sections: For each {{math|''y'' ∈ ''E''}} there is an open neighborhood {{math|''U''}} of {{math|''π''(''y'')}} in {{math|''B''}} and a smooth mapping {{math|''s'': ''U'' → ''E''}} with {{math|1=''π'' ∘ ''s'' = Id&lt;sub&gt;''U''&lt;/sub&gt;}} and {{math|''s''(''π''(''y'')) {{=}} ''y''}}.
*A surjection {{math|''π'' : ''E'' → ''B''}} is a fibered manifold if and only if there exists a local section {{math|''s'' : ''B'' → ''E''}} of {{mvar|π}}  (with {{math|''π'' ∘ ''s'' {{=}} Id&lt;sub&gt;''B''&lt;/sub&gt;}}) passing through each {{math|''y'' ∈ ''E''}}.&lt;ref&gt;{{harvnb|Giachetta|Mangiarotti|Sardanashvily|1997|p=15}}&lt;/ref&gt;

== Fibered coordinates ==
Let {{math|''B''}} (resp. {{math|''E''}}) be an {{math|''n''}}-dimensional (resp. {{math|''p''}}-dimensional) manifold. A fibered manifold {{math|(''E'', ''π'', ''B'')}} admits '''fiber charts'''. We say that a [[chart]] {{math|(''V'', ''ψ'')}} on {{math|''E''}} is a '''fiber chart''', or is '''adapted''' to the surjective submersion {{math|''π'': ''E'' → ''B''}} if there exists a chart {{math|(''U'', ''φ'')}} on {{math|''B''}} such that {{math|1=''U'' = ''π''(''V'')}} and

: &lt;math&gt;u^1=x^1\circ \pi,\,u^2=x^2\circ \pi,\,\dots,\,u^n=x^n\circ \pi\, ,&lt;/math&gt;

where

:&lt;math&gt;\begin{align}\psi &amp;= (u^1,\dots,u^n,y^1,\dots,y^{p-n}). \quad y_{0}\in V,\\
\varphi &amp;= (x^1,\dots,x^n), \quad \pi(y_{0})\in U.\end{align}&lt;/math&gt;

The above fiber chart condition may be equivalently expressed by

:&lt;math&gt;\varphi\circ\pi = \mathrm{pr}_1\circ\psi,&lt;/math&gt;

where

:&lt;math&gt;{\mathrm {pr}_1} \colon {\mathbb R^n}\times{\mathbb R^{p-n}} \to {\mathbb R^n}\,&lt;/math&gt;

is the projection onto the first {{math|''n''}} coordinates. The chart {{math|(''U'', ''φ'')}} is then obviously unique. In view of the above property, the '''fibered coordinates''' of a fiber chart {{math|(''V'', ''ψ'')}} are usually denoted by {{math|''ψ'' {{=}} (''x''&lt;sup&gt;''i''&lt;/sup&gt;, ''y''&lt;sup&gt;''σ''&lt;/sup&gt;)}} where {{math|''i'' ∈ &lt;nowiki&gt;{&lt;/nowiki&gt;1, ..., ''n''&lt;nowiki&gt;}&lt;/nowiki&gt;}},  {{math|''σ'' ∈ &lt;nowiki&gt;{&lt;/nowiki&gt;1, ..., ''m''&lt;nowiki&gt;}&lt;/nowiki&gt;}}, {{math|1=''m'' = ''p'' − ''n''}} the coordinates of the corresponding chart {{math|''U'', ''φ'')}} on {{math|''B''}} are then denoted, with the obvious convention, by {{math|1=''φ'' = (''x''&lt;sup&gt;''i''&lt;/sup&gt;)}} where {{math|''i'' ∈ &lt;nowiki&gt;{&lt;/nowiki&gt;1, ..., ''n''&lt;nowiki&gt;}&lt;/nowiki&gt;}}.

Conversely, if a surjection {{math|''π'': ''E'' → ''B''}} admits a '''fibered [[Atlas (topology)|atlas]]''', then {{math|''π'': ''E'' → ''B''}} is a fibered manifold.

== Local trivialization and fiber bundles ==
Let {{math|''E'' → ''B''}} be a fibered manifold and {{math|''V''}} any manifold. Then an open covering {{math|&lt;nowiki&gt;{&lt;/nowiki&gt;''U''&lt;sub&gt;''α''&lt;/sub&gt;&lt;nowiki&gt;}&lt;/nowiki&gt;}} of {{math|''B''}} together with maps&lt;ref&gt;{{harvnb|Giachetta|Mangiarotti|Sardanashvily|1997|p=13}}&lt;/ref&gt;

:&lt;math&gt;\psi: \pi^{-1}(U_\alpha) \rightarrow U_\alpha \times V,&lt;/math&gt;

called '''trivialization maps''', such that

:&lt;math&gt;\mathrm{pr}_1 \circ \psi_\alpha = \pi, \forall \alpha&lt;/math&gt;

is a '''local trivialization''' with respect to {{math|''V''}}.

A fibered manifold together with a manifold {{math|''V''}} is a [[fiber bundle]] with '''typical fiber''' (or just '''fiber''') {{math|''V''}} if it admits a local trivialization with respect to {{mvar|V}}. The atlas {{math|Ψ {{=}} &lt;nowiki&gt;{&lt;/nowiki&gt;(''U''&lt;sub&gt;''α''&lt;/sub&gt;, ''ψ''&lt;sub&gt;''α''&lt;/sub&gt;)&lt;nowiki&gt;}&lt;/nowiki&gt;}} is then called a '''bundle atlas'''.

== See also ==
* [[Covering space]]
* [[Fiber bundle]]
* [[Fibration]]
* [[Quasi-fibration]]
* [[Natural bundle]]
* [[Seifert fiber space]]
* [[Connection (fibred manifold)]]
* [[Algebraic fiber space]]

== Notes ==
{{Reflist|2}}

== References ==
* {{citation|last1 = Kolář|first1=Ivan|last2=Michor|first2=Peter|last3=Slovák|first3=Jan|url=http://www.emis.de/monographs/KSM/kmsbookh.pdf|format=PDF|title=Natural operators in differential geometry|year = 1993|publisher = Springer-Verlag}}
* {{citation|last1 = Krupka|first1=Demeter|last2=Janyška|first2=Josef|title=Lectures on differential invariants|year = 1990|publisher = Univerzita J. E. Purkyně V Brně|isbn=80-210-0165-8}}
* {{citation|last1 = Saunders|first1=D.J.|title=The geometry of jet bundles|year = 1989|publisher = Cambridge University Press|isbn=0-521-36948-7}}
*{{cite book|ref=harv|last1=Giachetta|first1=G.|last2=Mangiarotti|first2=L.|last3=Sardanashvily|first3=G.|authorlink3=Gennadi Sardanashvily|title=New Lagrangian and Hamiltonian Methods in Field Theory|publisher=[[World Scientific]]|year=1997|isbn=981-02-1587-8}}

=== Historical ===
*{{cite journal|ref=harv|title=Sur la théorie des espaces fibrés|first=C.|last=Ehresmann|authorlink=Charles Ehresmann|journal=Coll. Top. alg. Paris|volume=C.N.R.S.|year=1947a|pages=3–15|language=French}}
*{{cite journal|ref=harv|title=Sur les espaces fibrés différentiables|first=C.|last=Ehresmann|journal=C. R. Acad. Sci. Paris|volume=224|year=1947b|pages=1611–1612|language=French}}
*{{cite journal|ref=harv|title=Les prolongements d'un espace fibré différentiable|first=C.|last=Ehresmann|journal=C. R. Acad. Sci. Paris|volume=240|year=1955|pages=1755–1757|language=French}}
*{{cite journal|ref=harv|title=Sur la classification des espaces fibrés|first=J.|last=Feldbau|authorlink=Jacques Feldbau|journal=C. R. Acad. Sci. Paris|volume=208|year=1939|pages=1621–1623|language=French}}
*{{cite journal|ref=harv|title=Topologie dreidimensionaler geschlossener Räume|first=H.|last=Seifert|authorlink=Herbert Seifert|journal=Acta Math.|volume=60|year=1932|pages=147–238|doi=10.1007/bf02398271|language=French}}
*{{cite journal|ref=harv|title=Homologie singulière des espaces fibrés. Applications|first=J.-P.|last=Serre|authorlink=Jean-Pierre Serre|journal=Ann. of Math.|volume=54|year=1951|pages=425–505|doi=10.2307/1969485|language=French}}
*{{cite journal|ref=harv|title=Sphere spaces|first=H.|last=Whitney|authorlink=Hassler Whitney|journal=Proc. Natl. Acad. Sci. USA|volume=21|year=1935|pages=464–468|doi=10.1073/pnas.21.7.464|url=http://www.pnas.org/content/21/7.toc}} {{open access}}
*{{cite journal|ref=harv|title=On the theory of sphere bundles|first=H.|last=Whitney|journal=Proc. Natl. Acad. Sci. USA|volume=26|year=1940|pages=148–153|mr=0001338|url=http://www.pnas.org/content/26/2.toc|doi=10.1073/pnas.26.2.148}} {{open access}}

== External links ==
*{{cite web|title=A History of Manifolds and Fibre Spaces: Tortoises and Hares|url=http://pages.vassar.edu/mccleary/files/2011/04/history.fibre_.spaces.pdf|format=pdf|last=McCleary|first=J.}}

[[Category:Differential geometry]]
[[Category:Manifolds]]
[[Category:Fiber bundles]]</text>
      <sha1>7vt77woe50i36py4nmdetd9bu9ruipc</sha1>
    </revision>
  </page>
  <page>
    <title>Fundamental theorem of ideal theory in number fields</title>
    <ns>0</ns>
    <id>36099150</id>
    <revision>
      <id>780158360</id>
      <parentid>740295548</parentid>
      <timestamp>2017-05-13T10:55:41Z</timestamp>
      <contributor>
        <ip>163.1.88.6</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="770">In [[ideal theory]], the '''fundamental theorem of ideal theory in number fields''' states that every nonzero [[proper ideal]] in the ring of integers of a [[number field]] admits unique [[factorization]] into a product of nonzero [[prime ideal]]s.

==References==
* Keith Conrad, [http://www.math.uconn.edu/~kconrad/blurbs/gradnumthy/idealfactor.pdf Ideal factorization]
* {{cite book | last = Hilbert | first = D. | authorlink = David Hilbert | others = Trans. by Iain T. Adamson | title = The Theory of Algebraic Number Fields | publisher = Springer Verlag | url = https://books.google.com/books?id=_Q2h83Bm94cC&amp;printsec=frontcover&amp;hl=de#v=snippet&amp;q=%22fundamental%20theorem%20of%20ideal%20theory%22&amp;f=false | isbn = 3-540-62779-0}}

[[Category:Fundamental theorems]]</text>
      <sha1>h1li8v9mtgqaqknnmiti2s4bc1doq00</sha1>
    </revision>
  </page>
  <page>
    <title>G-structure on a manifold</title>
    <ns>0</ns>
    <id>1277699</id>
    <revision>
      <id>862007525</id>
      <parentid>846595934</parentid>
      <timestamp>2018-10-01T15:36:45Z</timestamp>
      <contributor>
        <ip>157.253.136.147</ip>
      </contributor>
      <comment>/* G-structures */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19550">{{DISPLAYTITLE:''G''-structure on a manifold}}
In [[differential geometry]], a '''''G''-structure''' on an ''n''-[[manifold]] ''M'', for a given [[structure group]]&lt;ref&gt;Which is a [[Lie group]] &lt;math&gt;G \to GL(n,\mathbf{R})&lt;/math&gt; mapping to the [[general linear group]] &lt;math&gt;GL(n,\mathbf{R})&lt;/math&gt;. This is often but not always a [[Lie subgroup]]; for instance, for a [[spin structure]] the map is a [[covering space]] onto its image.&lt;/ref&gt; ''G'', is a ''G''-[[subbundle]] of the [[frame bundle#Tangent frame bundle|tangent frame bundle]] F''M'' (or GL(''M'')) of ''M''.

The notion of ''G''-structures includes various classical structures that can be defined on manifolds, which in some cases are [[tensor field]]s. For example, for the [[orthogonal group]], an O(''n'')-structure defines a [[Riemannian metric]], and for the [[special linear group]] an SL(''n'','''R''')-structure is the same as a [[volume form]].  For the [[trivial group]], an {''e''}-structure consists of an [[parallelizable manifold|absolute parallelism]] of the manifold.

Generalising this idea to arbitrary [[principal bundle]]s on topological spaces, one can ask if a principal &lt;math&gt;G&lt;/math&gt;-bundle over a [[group (mathematics)|group]] &lt;math&gt;G&lt;/math&gt; "comes from" a [[subgroup]] &lt;math&gt;H&lt;/math&gt; of &lt;math&gt;G&lt;/math&gt;. This is called '''reduction of the structure group''' (to &lt;math&gt;H&lt;/math&gt;).

Several structures on manifolds, such as a [[Complex manifold|complex structure]], a [[symplectic structure]], or a [[Kähler manifold|Kähler structure]], are ''G''-structures with an additional [[integrability condition]].

==Reduction of the structure group==
One can ask if a principal &lt;math&gt;G&lt;/math&gt;-bundle over a [[group (mathematics)|group]] &lt;math&gt;G&lt;/math&gt; "comes from" a [[subgroup]] &lt;math&gt;H&lt;/math&gt; of &lt;math&gt;G&lt;/math&gt;. This is called '''reduction of the structure group''' (to &lt;math&gt;H&lt;/math&gt;), and makes sense for any map &lt;math&gt;H \to G&lt;/math&gt;, which need not be an [[inclusion map]] (despite the terminology).

=== Definition ===
In the following, let &lt;math&gt;X&lt;/math&gt; be a [[topological space]], &lt;math&gt;G, H&lt;/math&gt; topological groups and a group homomorphism &lt;math&gt;\phi\colon H \to G&lt;/math&gt;.

==== In terms of concrete bundles ====
Given a principal &lt;math&gt;G&lt;/math&gt;-bundle &lt;math&gt;P&lt;/math&gt; over &lt;math&gt;X&lt;/math&gt;, a ''reduction of the structure group'' (from &lt;math&gt;G&lt;/math&gt; to &lt;math&gt;H&lt;/math&gt;) is an ''&lt;math&gt;H&lt;/math&gt;''-bundle &lt;math&gt;Q&lt;/math&gt; and an isomorphism &lt;math&gt;\phi_Q\colon Q \times_H G \to P&lt;/math&gt; of the [[associated bundle]] to the original bundle.

==== In terms of classifying spaces ====
Given a map &lt;math&gt;\pi\colon X \to BG&lt;/math&gt;, where &lt;math&gt;BG&lt;/math&gt; is the [[classifying space]] for &lt;math&gt;G&lt;/math&gt;-bundles, a ''reduction of the structure group'' is a map &lt;math&gt;\pi_Q\colon X \to BH&lt;/math&gt; and a homotopy &lt;math&gt;\phi_Q\colon B\phi \circ \pi_Q \to \pi&lt;/math&gt;.

=== Properties and examples ===
Reductions of the structure group do not always exist. If they exist, they are usually not essentially unique, since the isomorphism &lt;math&gt;\phi&lt;/math&gt; is an important part of the data.

As a concrete example, every even-dimensional real [[vector space]] is isomorphic to the underlying real space of a complex vector space: it admits a [[linear complex structure]]. A real [[vector bundle]] admits an [[almost complex]] structure if and only if it is isomorphic to the underlying real bundle of a complex vector bundle. This is then a reduction along the inclusion ''GL''(''n'','''C''') → ''GL''(2''n'','''R''')

In terms of [[transition map]]s, a ''G''-bundle can be reduced if and only if the transition maps can be taken to have values in ''H''. Note that the term ''reduction'' is misleading: it suggests that ''H'' is a subgroup of ''G'', which is often the case, but need not be (for example for [[spin structure]]s): it's properly called a [[Homotopy lifting property|lifting]].

More abstractly, "''G''-bundles over ''X''" is a [[functor]]&lt;ref&gt;Indeed, it is a [[bifunctor]] in ''G'' and ''X''.&lt;/ref&gt; in ''G'': given a map ''H'' → ''G'', one gets a map from ''H''-bundles to ''G''-bundles by [[Induced representation|inducing]] (as above). Reduction of the structure group of a ''G''-bundle ''B'' is choosing an ''H''-bundle whose image is ''B''.

The inducing map from ''H''-bundles to ''G''-bundles is in general neither onto nor one-to-one, so the structure group cannot always be reduced, and when it can, this reduction need not be unique. For example, not every manifold is [[orientable]], and those that are orientable admit exactly two orientations.

If ''H'' is a closed subgroup of ''G'', then there is a natural one-to-one correspondence between reductions of a ''G''-bundle ''B'' to ''H'' and global sections of the [[fiber bundle]] ''B''/''H'' obtained by quotienting ''B'' by the right action of ''H''.  Specifically, the [[fibration]] ''B'' → ''B''/''H'' is a principal ''H''-bundle over ''B''/''H''.  If σ : ''X'' →  ''B''/''H'' is a section, then the [[pullback bundle]] ''B''&lt;sub&gt;H&lt;/sub&gt; = σ&lt;sup&gt;−1&lt;/sup&gt;''B'' is a reduction of ''B''.&lt;ref&gt;In [[classical field theory]], such a section &lt;math&gt;\sigma&lt;/math&gt; describes a classical [[Higgs field (classical)|Higgs field]] ({{cite journal|last1=Sardanashvily|first1=G.|year=2006|title=Geometry of Classical Higgs Fields|journal=International Journal of Geometric Methods in Modern Physics|volume=03|pages=139|arxiv=hep-th/0510168|doi=10.1142/S0219887806001065}}).
&lt;/ref&gt;

== ''G''-structures ==
Every [[vector bundle]] of dimension &lt;math&gt;n&lt;/math&gt; has a canonical &lt;math&gt;GL(n)&lt;/math&gt;-bundle, the [[frame bundle]]. In particular, every [[Differentiable manifold|smooth manifold]] has a canonical vector bundle, the [[tangent bundle]]. For a Lie group &lt;math&gt;G&lt;/math&gt; and a group homomorphism &lt;math&gt;\phi\colon G \to GL(n)&lt;/math&gt;, a &lt;math&gt;G&lt;/math&gt;-structure is a reduction of the structure group of the frame bundle to &lt;math&gt;G&lt;/math&gt;.

=== Examples ===
The following examples are defined for [[Real vector bundle|real vector bundles]], particularly the [[tangent bundle]] of a [[manifold|smooth manifold]].
{| class="wikitable"
!Group homomorphism
!Group &lt;math&gt;G&lt;/math&gt;
!&lt;math&gt;G&lt;/math&gt;-structure
!Obstruction
|-
|&lt;math&gt;GL^+(n) &lt; GL(n)&lt;/math&gt;
|[[General linear group#real case|General linear group of positive determinant]]
|[[Orientation (manifold)|Orientation]]
|Bundle must be orientable
|-
|&lt;math&gt;SL(n) &lt; GL(n)&lt;/math&gt;
|[[Special linear group]]
|[[Volume form]]
|Bundle must be orientable (&lt;math&gt;SL \to GL^+&lt;/math&gt; is a [[deformation retract]])
|-
|&lt;math&gt;SL^{\pm}(n) &lt; GL(n)&lt;/math&gt;
|Determinant &lt;math&gt;\pm 1&lt;/math&gt;
|Pseudo-[[volume form]]
|Always possible
|-
|&lt;math&gt;O(n) &lt; GL(n)&lt;/math&gt;
|[[Orthogonal group]]
|[[Riemannian metric]]
|Always possible (&lt;math&gt;O(n)&lt;/math&gt; is the [[maximal compact subgroup]], so the inclusion is a deformation retract)
|-
|&lt;math&gt;O(1,n-1) &lt; GL(n)&lt;/math&gt;
|[[Indefinite orthogonal group]]
|[[Pseudo Riemannian metric|Pseudo-Riemannian metric]]
|Topological obstruction&lt;ref&gt;It is a [[gravitational field]] in [[gauge gravitation theory]] ({{Cite journal|last1=Sardanashvily|first1=G.|year=2006|title=Gauge gravitation theory from the geometric viewpoint|journal=Int.J.Geom.Methods Mod.Phys.|volume=3|issue=1|pages=v-xx|arxiv=gr-qc/0512115|bibcode=2005gr.qc....12115S}})&lt;/ref&gt;
|-
|&lt;math&gt;GL(n,\mathbf{C}) &lt; GL(2n,\mathbf{R})&lt;/math&gt;
|[[Complex general linear group]]
|[[almost complex manifold|Almost complex structure]]
|Topological obstruction
|-
|&lt;math&gt;GL(n,\mathbf{H})\cdot Sp(1) &lt; GL(4n,\mathbf{R})&lt;/math&gt;
|
* &lt;math&gt;GL(n,\mathbf{H})&lt;/math&gt;: [[Quaternion|quaternionic]] general linear group acting on &lt;math&gt;\mathbf{H}^n \cong \mathbf{R}^{4n}&lt;/math&gt; from the left
* &lt;math&gt;Sp(1)=Spin(3)&lt;/math&gt;: group of unit quaternions acting on &lt;math&gt;\mathbf{H}^n&lt;/math&gt; from the right
|almost quaternionic structure&lt;ref name=":0"&gt;{{harvnb|Besse|1987|loc=§14.61}}&lt;/ref&gt;
|Topological obstruction&lt;ref name=":0" /&gt;
|-
|&lt;math&gt;GL(k) \times GL(n-k) &lt; GL(n)&lt;/math&gt;
|[[General linear group]]
|Decomposition as a [[Whitney sum]] (direct sum) of sub-bundles of rank &lt;math&gt;k&lt;/math&gt; and &lt;math&gt;n-k&lt;/math&gt;.
|Topological obstruction
|}
Some &lt;math&gt;G&lt;/math&gt;-structures are defined terms of others: Given a Riemannian metric on an oriented manifold, a &lt;math&gt;G&lt;/math&gt;-structure for the 2-fold [[covering space|cover]] &lt;math&gt;\mbox{Spin}(n) \to \mbox{SO}(n)&lt;/math&gt; is a [[spin manifold|spin structure]]. (Note that the group homomorphism here is ''not'' an inclusion.)

=== Principal bundles ===
Although the theory of [[principal bundle]]s plays an important role in the study of ''G''-structures, the two notions are different.  A ''G''-structure is a principal subbundle of the [[frame bundle#Tangent frame bundle|tangent frame bundle]], but the fact that the ''G''-structure bundle ''consists of tangent frames'' is regarded as part of the data.  For example, consider two Riemannian metrics on '''R'''&lt;sup&gt;''n''&lt;/sup&gt;.  The associated O(''n'')-structures are isomorphic if and only if the metrics are isometric.  But, since '''R'''&lt;sup&gt;''n''&lt;/sup&gt; is contractible, the underlying O(''n'')-bundles are always going to be isomorphic as principal bundles because the only bundles over contractible spaces are trivial bundles.

This fundamental difference between the two theories can be captured by giving an additional piece of data on the underlying ''G''-bundle of a ''G''-structure: the '''[[solder form]]'''.  The solder form is what ties the underlying principal bundle of the ''G''-structure to the local geometry of the manifold itself by specifying a canonical isomorphism of the tangent bundle of ''M'' to an [[associated bundle|associated vector bundle]].  Although the solder form is not a [[connection form]], it can sometimes be regarded as a precursor to one.

In detail, suppose that ''Q'' is the principal bundle of a ''G''-structure.  If ''Q'' is realized as a reduction of the frame bundle of ''M'', then the solder form is given by the [[pullback (differential geometry)|pullback]] of the [[frame bundle#Solder form|tautological form of the frame bundle]] along the inclusion.  Abstractly, if one regards ''Q'' as a principal bundle independently of its realization as a reduction of the frame bundle, then the solder form consists of a representation &amp;rho; of ''G'' on '''R'''&lt;sup&gt;n&lt;/sup&gt; and an isomorphism of bundles &amp;theta; : ''TM'' &amp;rarr; ''Q'' &amp;times;&lt;sub&gt;&amp;rho;&lt;/sub&gt; '''R'''&lt;sup&gt;n&lt;/sup&gt;.

== Integrability conditions and flat ''G''-structures ==
Several structures on manifolds, such as a complex structure, a [[symplectic structure]], or a [[Kähler manifold|Kähler structure]], are ''G''-structures (and thus can be obstructed), but need to satisfy an additional [[integrability condition]]. Without the corresponding integrability condition, the structure is instead called an "almost" structure, as in an [[almost complex structure]], an [[almost symplectic structure]], or an [[almost Kähler manifold|almost Kähler structure]].

Specifically, a [[symplectic manifold]] structure is a stronger concept than a ''G''-structure for the [[symplectic group]]. A symplectic structure on a manifold is a [[two-form]] ''&amp;omega;'' on ''M'' that is non-degenerate (which is an &lt;math&gt;Sp&lt;/math&gt;-structure, or [[almost symplectic structure]]), ''together with'' the extra condition that d''&amp;omega;'' = 0; this latter is called an [[integrability condition]].

Similarly, [[foliation]]s correspond to ''G''-structures coming from [[block matrix|block matrices]], together with integrability conditions so that the [[Frobenius theorem (differential topology)|Frobenius theorem]] applies.

A '''flat ''G''-structure''' is a ''G''-structure ''P'' having a global section (''V''&lt;sub&gt;1&lt;/sub&gt;,...,''V''&lt;sub&gt;n&lt;/sub&gt;) consisting of [[Lie derivative|commuting vector fields]].  A ''G''-structure is '''integrable''' (or ''locally flat'') if it is locally isomorphic to a flat ''G''-structure.

== Isomorphism of ''G''-structures ==
The set of [[diffeomorphism]]s of ''M'' that preserve a  ''G''-structure is called the ''[[automorphism group]]'' of that structure. For an O(''n'')-structure they are the group of [[isometry|isometries]] of the Riemannian metric and for an SL(''n'','''R''')-structure volume preserving maps.

Let ''P'' be a ''G''-structure on a manifold ''M'', and ''Q'' a ''G''-structure on a manifold ''N''.  Then an '''isomorphism''' of the ''G''-structures is a diffeomorphism ''f'' : ''M'' &amp;rarr; ''N'' such that the [[pushforward (differential)|pushforward]] of linear frames ''f''&lt;sub&gt;*&lt;/sub&gt; : ''FM'' &amp;rarr; ''FN'' restricts to give a mapping of ''P'' into ''Q''.  (Note that it is sufficient that ''Q'' be contained within the image of ''f''&lt;sub&gt;*&lt;/sub&gt;.)  The ''G''-structures ''P'' and ''Q'' are '''locally isomorphic''' if ''M'' admits a covering by open sets ''U'' and a family of diffeomorphisms ''f''&lt;sub&gt;U&lt;/sub&gt; : ''U'' &amp;rarr; ''f''(''U'') &amp;sub; ''N'' such that ''f''&lt;sub&gt;U&lt;/sub&gt; induces an isomorphism of ''P''|&lt;sub&gt;U&lt;/sub&gt; &amp;rarr; ''Q''|&lt;sub&gt;''f''(''U'')&lt;/sub&gt;.

An '''automorphism''' of a ''G''-structure is an isomorphism of a ''G''-structure ''P'' with itself.  Automorphisms arise frequently&lt;ref&gt;Kobayashi (1972).&lt;/ref&gt; in the study of [[transformation group]]s of geometric structures, since many of the important geometric structures on a manifold can be realized as ''G''-structures.

A wide class of [[Cartan's equivalence method|equivalence problems]] can be formulated in the language of ''G''-structures.  For example, a pair of Riemannian manifolds are (locally) equivalent if and only if their bundles of [[orthonormal frame]]s are (locally) isomorphic ''G''-structures.  In this view, the general procedure for solving an equivalence problem is to construct a system of invariants for the ''G''-structure which are then sufficient to determine whether a pair of ''G''-structures are locally isomorphic or not.

== Connections on ''G''-structures ==

Let ''Q'' be a ''G''-structure on ''M''.   A [[connection (principal bundle)|principal connection]] on the principal bundle ''Q'' induces a connection on any associated vector bundle: in particular on the tangent bundle.  A  [[connection (vector bundle)|linear connection]] &amp;nabla; on ''TM'' arising in this way is said to be '''compatible''' with ''Q''. Connections compatible with ''Q'' are also called '''adapted connections'''.

Concretely speaking, adapted connections can be understood in terms of a [[moving frame]].&lt;ref&gt;Kobayashi (1972) I.4.&lt;/ref&gt;  Suppose that ''V''&lt;sub&gt;i&lt;/sub&gt; is a basis of local sections of ''TM'' (i.e., a frame on ''M'') which defines a section of ''Q''.  Any connection &amp;nabla; determines a system of basis-dependent 1-forms &amp;omega; via

:&amp;nabla;&lt;sub&gt;X&lt;/sub&gt; V&lt;sub&gt;i&lt;/sub&gt; = &amp;omega;&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;j&lt;/sup&gt;(X)V&lt;sub&gt;j&lt;/sub&gt;

where, as a matrix of 1-forms, &amp;omega; &amp;isin; &amp;Omega;&lt;sup&gt;1&lt;/sup&gt;(M)&amp;otimes;'''gl'''(''n'').  An adapted connection is one for which &amp;omega; takes its values in the Lie algebra '''g''' of ''G''.

=== Torsion of a ''G''-structure ===
Associated to any ''G''-structure is a notion of torsion, related to the [[torsion (differential geometry)|torsion]] of a connection.  Note that a given ''G''-structure may admit many different compatible connections which in turn can have different torsions, but in spite of this it is possible to give an independent notion of torsion ''of the G-structure'' as follows.&lt;ref&gt;Gauduchon (1997).&lt;/ref&gt;

The difference of two adapted connections is a 1-form on ''M'' [[vector-valued differential form|with values in]] the [[adjoint bundle]] Ad&lt;sub&gt;''Q''&lt;/sub&gt;. That is to say, the space ''A''&lt;sup&gt;''Q''&lt;/sup&gt; of adapted connections is an [[affine space]]
for &amp;Omega;&lt;sup&gt;1&lt;/sup&gt;(Ad&lt;sub&gt;''Q''&lt;/sub&gt;).

The [[torsion of connection|torsion]] of an adapted connection defines a map

:&lt;math&gt;A^Q \to \Omega^2 (TM)\,&lt;/math&gt;

to 2-forms with coefficients in ''TM''. This map is linear; its linearization 

:&lt;math&gt;\tau:\Omega^1(\mathrm{Ad}_Q)\to \Omega^2(TM)\,&lt;/math&gt;

is called '''the algebraic torsion map'''. Given two adapted connections &amp;nabla; and &amp;nabla;&amp;prime;, their torsion tensors ''T''&lt;sub&gt;&amp;nabla;&lt;/sub&gt;, ''T''&lt;sub&gt;&amp;nabla;&amp;prime;&lt;/sub&gt; differ by &amp;tau;(&amp;nabla;&amp;minus;&amp;nabla;&amp;prime;). Therefore, the image of ''T''&lt;sub&gt;&amp;nabla;&lt;/sub&gt; in coker(&amp;tau;) is independent from the choice of &amp;nabla;.

The image of ''T''&lt;sub&gt;&amp;nabla;&lt;/sub&gt; in coker(&amp;tau;) for any adapted connection &amp;nabla; is called the '''torsion''' of the ''G''-structure. A ''G''-structure is said to be '''torsion-free''' if its torsion vanishes. This happens precisely when ''Q'' admits a torsion-free adapted connection.

=== Example: Torsion for almost complex structures ===

An example of a ''G''-structure is an [[almost complex structure]], that is, a reduction
of a structure group of an even-dimensional manifold to GL(''n'','''C'''). Such a reduction is uniquely determined by a ''C''&lt;sup&gt;&amp;infin;&lt;/sup&gt;-linear endomorphism ''J'' &amp;isin; End(''TM'') such that ''J''&lt;sup&gt;2&lt;/sup&gt; = &amp;minus;1. In this situation, the torsion can be computed explicitly as follows.

An easy dimension count shows that

:&lt;math&gt;\Omega^2(TM)= \Omega^{2,0}(TM)\oplus \mathrm{im}(\tau)&lt;/math&gt;,

where &amp;Omega;&lt;sup&gt;2,0&lt;/sup&gt;(''TM'') is a space of forms ''B'' &amp;isin; &amp;Omega;&lt;sup&gt;2&lt;/sup&gt;(''TM'') which satisfy

:&lt;math&gt;B(JX,Y) = B(X, JY) = - J B(X,Y).\,&lt;/math&gt;

Therefore, the torsion of an almost complex structure can be considered as an element in 
&amp;Omega;&lt;sup&gt;2,0&lt;/sup&gt;(''TM''). It is easy to check that the torsion of an almost complex structure is equal to its [[Nijenhuis tensor]].

== Higher order ''G''-structures ==

Imposing [[integrability condition]]s on a particular ''G''-structure (for instance, with the case of a symplectic form) can be dealt with via the process of [[Cartan's equivalence method|prolongation]].  In such cases, the prolonged ''G''-structure cannot be identified with a ''G''-subbundle of the bundle of linear frames.  In many cases, however, the prolongation is a principal bundle in its own right, and its structure group can be identified with a subgroup of a higher-order [[jet group]].  In which case, it is called a higher order ''G''-structure [Kobayashi].  In general, [[Cartan's equivalence method]] applies to such cases.

==See also==
* [[G2-structure|G&lt;sub&gt;2&lt;/sub&gt;-structure]]

==Notes==
&lt;references/&gt;

==References==
*{{cite journal | authorlink=S. S. Chern | last = Chern | first = S.S. | year = 1966 | title = The geometry of ''G''-structures | journal = Bull. Amer. Math. Soc. | volume = 72 | pages = 167&amp;ndash;219 | doi = 10.1090/S0002-9904-1966-11473-8 | issue=2}}
* {{cite conference | first = P. | last = Gauduchon | title = Canonical connections for almost-hypercomplex structures | booktitle = Complex Analysis and Geometry | series = Pitman Research Notes in Mathematics Series | publisher = Longman | year = 1997 | pages = 123&amp;ndash;136}}
* {{cite book | first = S. | last = Kobayashi | title = Transformation Groups in Differential Geometry | series = Classics in Mathematics | publisher = Springer | year = 1972 | isbn = 3-540-58659-8 | oclc = 31374337}}
*{{cite book | last = Sternberg | first = S. | year = 1983 | title = Lectures on Differential Geometry | edition = (2nd ed.) | publisher = Chelsea Publishing Co. | location = New York | isbn = 0-8218-1385-4 | oclc = 43032711}}
*{{cite journal|title=Reductive G-structures and Lie derivatives|author=Godina M. and Matteucci P.|journal=Journal of Geometry and Physics |volume=47|year=2003|pages=66–86 |doi=10.1016/S0393-0440(02)00174-2|arxiv=math/0201235|bibcode=2003JGP....47...66G}}

[[Category:Differential geometry]]
[[Category:Structures on manifolds]]</text>
      <sha1>ewaqgwxcak5h0ln4hb2vzd5h5evtnp2</sha1>
    </revision>
  </page>
  <page>
    <title>Genevieve Grotjan Feinstein</title>
    <ns>0</ns>
    <id>45667899</id>
    <revision>
      <id>844162225</id>
      <parentid>843312956</parentid>
      <timestamp>2018-06-03T01:44:33Z</timestamp>
      <contributor>
        <username>Aboutmovies</username>
        <id>2602832</id>
      </contributor>
      <comment>removed [[Category:People from Buffalo, New York]]; added [[Category:Scientists from Buffalo, New York]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4511">{{Infobox scientist
|name        = Genevieve Grotjan Feinstein
|image       =  Genevieve Grotjan Feinstein.jpg
|image_size  =
|caption     =
|birth_name  = Genevieve Marie Grotjan
|birth_date  = April 30, 1913&lt;ref&gt;''U.S., Social Security Death Index, 1935-2014''&lt;/ref&gt;
|birth_place = [[Buffalo, New York]]
|death_date  = {{dda|2006|8|10|1913|4|30}}
|death_place = [[Fairfax, Virginia]]
|death_cause =
|other_names = 
|residence   =
|citizenship = American
|nationality =
|fields      = [[Mathematics]]&lt;br /&gt;[[Cryptanalysis]]
|workplaces  = [[Signals Intelligence Service]]
|known_for   = Deciphering the [[Purple (cipher machine)|Purple]] machine
|spouse      =  Hyman Feinstein
}}
  
'''Genevieve Marie Grotjan Feinstein''' (April 30, 1913 – August 10, 2006) was an American [[mathematician]] and [[Cryptanalysis|cryptanalyst]]. She worked for the [[Signals Intelligence Service]] throughout World War II, during which time she played an important role in deciphering the Japanese cryptography machine [[Purple (cipher machine)|Purple]], and later worked on the Cold War-era [[Venona project]].

== Career ==
Feinstein discovered a passion for mathematics at a young age and aspired to become a math teacher until the beginning of World War II, when U.S. President [[Franklin D. Roosevelt]] made it possible for women to fulfill non-combat roles in the military. She passed the necessary tests to become a government mathematician in 1939, and was hired by [[William F. Friedman]] to work as a [[Cryptanalysis|cryptanalyst]] for the Army's [[Signals Intelligence Service]] (SIS). For eighteen months, she worked with other SIS codebreakers to decipher the code used by [[Purple (cipher machine)|Purple]], a Japanese cryptography machine, and ultimately played a key role in cracking the cipher in September 1940.&lt;ref name=war&gt;{{cite book|title=Encyclopedia of American Women at War: From the Home Front to the Battlefields|year=2013|publisher=[[ABC-CLIO]]|entry=Feinstein, Genevieve Grotjan (1912–2006)|first=Megan|last=Findling|isbn=978-1-59884-443-6|editor-first=Lisa|editor-last=Tendrich Frank|pages=215–217}}&lt;/ref&gt; This enabled the construction of an analog machine by the SIS which in turn enabled the interception of almost all messages exchanged between the Japanese government and its embassies in foreign countries.&lt;ref name=war /&gt;&lt;ref&gt;{{cite web|url=https://www.nsa.gov/about/cryptologic_heritage/women/honorees/feinstein.shtml|title=Genevieve Grotjan Feinstein|publisher=[[National Security Agency]]|date=January 15, 2009|accessdate=March 14, 2015}}&lt;/ref&gt;

After the conclusion of World War II, Feinstein continued to work at the SIS throughout the [[Cold War]], trying to decode encrypted messages sent by the Soviet [[KGB]] and [[Main Intelligence Directorate (Russia)|Main Intelligence Directorate]] (GRU).&lt;ref name=war /&gt; She made a significant breakthrough in the early stages of the [[Venona project]], which allowed American cryptographers to recognize when an individual cipher key was reused, but resigned from the SIS in 1947.&lt;ref name=nsa&gt;{{cite web|url=https://www.nsa.gov/about/cryptologic_heritage/hall_of_honor/2010/grotjan.shtml|title=Genevieve Grotjan Feinstein: 2010 Inductee|date=April 6, 2011|publisher=[[National Security Agency]]|accessdate=March 14, 2015}}&lt;/ref&gt; After resigning from government cryptanalysis, she began working in the faculty of [[George Mason University]], where she served as a professor of mathematics.&lt;ref name=war /&gt;

== Personal life ==
Genevieve Grotjan married the chemist Hyman Feinstein in 1943, and they had a son named Ellis.&lt;ref name=war /&gt; She died in 2006.&lt;ref name=nsa /&gt;

== Legacy ==
Feinstein's breakthrough in deciphering the Purple machine has been called, in the ''Encyclopedia of American Women at War'', "one of the greatest achievements in the history of U.S. codebreaking".&lt;ref name=war /&gt; She was posthumously inducted into the [[NSA Hall of Honor]] in 2010, and an award in cryptology was established at George Mason University in her honor.&lt;ref name=nsa /&gt;

== References ==
{{reflist}}
{{Authority control}}

{{DEFAULTSORT:Feinstein, Genevieve Grotjan}}
[[Category:1913 births]]
[[Category:2006 deaths]]
[[Category:20th-century American mathematicians]]
[[Category:Signals Intelligence Service cryptographers]]
[[Category:American women in World War II]]
[[Category:George Mason University faculty]]
[[Category:Women mathematicians]]
[[Category:20th-century women scientists]]
[[Category:Scientists from Buffalo, New York]]</text>
      <sha1>ppexoy69ih24w2ux38fss1ozoahv5yr</sha1>
    </revision>
  </page>
  <page>
    <title>Group (mathematics)</title>
    <ns>0</ns>
    <id>19447</id>
    <revision>
      <id>871696128</id>
      <parentid>871696038</parentid>
      <timestamp>2018-12-02T21:27:23Z</timestamp>
      <contributor>
        <username>Tornado chaser</username>
        <id>31083626</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contribs/2600:1700:C640:1750:6444:4E7E:3E60:3DE2|2600:1700:C640:1750:6444:4E7E:3E60:3DE2]] ([[User talk:2600:1700:C640:1750:6444:4E7E:3E60:3DE2|talk]]) to last version by ImTheIP</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="96226">{{about|basic notions of groups in mathematics|a more advanced treatment|Group theory}}
[[Image:Rubik's cube.svg|thumb|right|The manipulations of this [[Rubik's Cube]] form the [[Rubik's Cube group]].]]

In [[mathematics]], a '''group''' is an [[algebraic structure]] consisting of a [[set (mathematics)|set]] of [[element (mathematics)|elements]] equipped with a [[binary operation]] which combines any two elements to form a third element. To be a group, this operation must satisfy four conditions called the group [[axiom]]s, namely [[Closure (mathematics)|closure]], [[associativity]], [[identity element|identity]] and [[Inverse elements|invertibility]]. One of the most familiar examples of a group is the set of [[integer]]s together with the [[addition]] operation, but the abstract formalization of the group axioms, detached as it is from the concrete nature of any particular group and its operation, applies much more widely.  It allows entities with highly diverse mathematical origins in [[abstract algebra]] and beyond to be handled in a flexible way while retaining their essential structural aspects. The ubiquity of groups in numerous areas within and outside mathematics makes them a central organizing principle of contemporary mathematics.&lt;ref&gt;{{Harvard citations|last = Herstein|year = 1975|loc = §2, p. 26|nb = yes}}&lt;/ref&gt;&lt;ref&gt;{{Harvard citations|last = Hall|year = 1967|loc = §1.1, p. 1|nb = yes}}: "The idea of a group is one which pervades the whole of mathematics both pure and applied."&lt;/ref&gt;

Groups share a fundamental kinship with the notion of [[symmetry]]. For example, a [[symmetry group]] encodes symmetry features of a [[geometry|geometrical]] object: the group consists of the set of transformations that leave the object unchanged and the operation of combining two such transformations by performing one after the other. [[Lie group]]s are the symmetry groups used in the [[Standard Model]] of [[particle physics]]; [[Poincaré group]]s, which are also Lie groups, can express the physical symmetry underlying [[special relativity]]; and [[point group]]s are used to help understand [[Molecular symmetry|symmetry phenomena in molecular chemistry]].

The concept of a group arose from the study of [[polynomial equations]], starting with [[Évariste Galois]] in the 1830s.  After contributions from other fields such as [[number theory]] and geometry, the group notion was generalized and firmly established around 1870.  Modern [[group theory]]—an active mathematical discipline—studies groups in their own right.{{cref|a}} To explore groups, mathematicians have devised [[Glossary of group theory|various notions]] to break groups into smaller, better-understandable pieces, such as [[subgroup]]s, [[quotient group]]s and [[simple group]]s. In addition to their abstract properties, group theorists also study the different ways in which a group can be expressed concretely, both from a point of view of [[representation theory]] (that is, through the [[group representation|representations of the group]]) and of [[computational group theory]].  A theory has been developed for [[finite group]]s, which culminated with the [[classification of finite simple groups]], completed in 2004.{{cref|aa}} Since the mid-1980s, [[geometric group theory]], which studies [[finitely generated group]]s as geometric objects, has become a particularly active area in group theory.

{{Group theory sidebar |image_param= |style_param=}}
{{Algebraic structures |Group}}

{{TOClimit|limit=3}}

== Definition and illustration==

===First example: the integers===
One of the most familiar groups is the set of [[integers]] &lt;math&gt;\mathbb{Z}&lt;/math&gt; which consists of the numbers
:..., −4, −3, −2, −1, 0, 1, 2, 3, 4,&amp;nbsp;...,&lt;ref&gt;{{Harvard citations|last = Lang|year = 2005|loc = App. 2, p. 360|nb = yes}}&lt;/ref&gt; together with [[addition]].
The following properties of integer addition serve as a model for the group axioms given in the definition below.

*For any two integers ''a'' and ''b'', the [[Summation|sum]] ''a'' + ''b'' is also an integer. That is, addition of integers always yields an integer. This property is known as ''[[Closure (mathematics)|closure]]'' under addition.
*For all integers ''a'', ''b'' and ''c'', (''a'' + ''b'') + ''c'' = ''a'' + (''b'' + ''c'').   Expressed in words, adding ''a'' to ''b'' first, and then adding the result to ''c'' gives the same final result as adding ''a'' to the sum of ''b'' and ''c'', a property known as ''[[associativity]]''.
*If ''a'' is any integer, then 0 + ''a'' = ''a'' + 0 = ''a''. [[Zero]] is called the ''[[identity element]]'' of addition because adding it to any integer returns the same integer.
*For every integer ''a'', there is an integer ''b'' such that ''a'' + ''b'' = ''b'' + ''a'' = 0. The integer ''b''  is called the ''[[inverse element]]'' of the integer ''a'' and is denoted −''a''.

The integers, together with the operation +, form a mathematical object belonging to a broad class sharing similar structural aspects. To appropriately understand these structures as a collective, the following [[definition]] is developed.

===Definition===
{{quote box
|align = right
|width=33%
|quote=The axioms for a group are short and natural... Yet somehow hidden behind these axioms is the [[Monster group|monster simple group]], a huge and extraordinary mathematical object, which appears to rely on numerous bizarre coincidences to exist. The axioms for groups give no obvious hint that anything like this exists.
|source=[[Richard Borcherds]] in ''Mathematicians: An Outer View of the Inner World'' &lt;ref&gt;{{Citation |last=Cook |first=Mariana R. |year=2009 |title=Mathematicians: An Outer View of the Inner World |publisher=Princeton University Press |publication-place=Princeton, N.J. |page=24 | isbn=9780691139517 |url=https://books.google.com/books?id=06h8NT77OgMC&amp;vq=Richard%20Ewen%20Borcherds&amp;pg=PA24#v=onepage&amp;q=Richard%20Ewen%20Borcherds&amp;f=false |accessdate= }}&lt;/ref&gt;
}}

A group is a [[set (mathematics)|set]], ''G'', together with an [[Binary operation|operation]] • (called the ''group law'' of ''G'') that combines any two [[element (mathematics)|elements]] ''a'' and ''b'' to form another element, denoted {{nowrap|''a'' • ''b''}} or ''ab''. To qualify as a group, the set and operation, {{nowrap|(''G'', •)}}, must satisfy four requirements known as the ''group axioms'':&lt;ref&gt;{{Harvard citations|last = Herstein|year = 1975|loc = §2.1, p. 27|nb = yes}}&lt;/ref&gt;

;Closure: For all ''a'', ''b'' in ''G'', the result of the operation, ''a'' • ''b'', is also in ''G''.{{cref|b}}
;Associativity: For all ''a'', ''b'' and ''c'' in ''G'', (''a'' • ''b'') • ''c'' = ''a'' • (''b'' • ''c'').
;Identity element: There exists an element ''e'' in ''G'' such that, for every element ''a'' in ''G'', the equation {{nowrap|1=''e'' • ''a'' = ''a'' • ''e'' = ''a''}} holds. Such an element is unique ([[#Uniqueness of identity element and inverses|see below]]), and thus one speaks of ''the'' identity element. 
;Inverse element: For each ''a'' in ''G'', there exists an element ''b'' in ''G'', commonly denoted ''a''&lt;sup&gt;−1&lt;/sup&gt; (or −''a'', if the operation is denoted "+"), such that ''a'' • ''b'' = ''b'' • ''a'' = ''e'', where ''e'' is the identity element.

The result of an operation may depend on the order of the operands. In other words, the result of combining element ''a'' with element ''b'' need not yield the same result as combining element ''b'' with element ''a''; the equation
:{{nowrap|1=''a'' • ''b'' = ''b'' • ''a''}}
may not always be true. This equation always holds in the group of integers under addition, because {{nowrap|1=''a'' + ''b'' = ''b'' + ''a''}} for any two integers  ([[commutativity]] of addition). Groups for which the commutativity equation {{nowrap|1=''a'' • ''b'' = ''b'' • ''a''}} always holds are called ''[[abelian group]]s'' (in honor of [[Niels Henrik Abel]]). The symmetry group described in the following section is an example of a group that is not abelian.

The identity element of a group ''G'' is often written as 1 or 1&lt;sub&gt;''G''&lt;/sub&gt;,&lt;ref&gt;{{MathWorld |title=Identity Element |urlname=IdentityElement}}&lt;/ref&gt; a notation inherited from the [[multiplicative identity]]. If a group is abelian, then one may choose to denote the group operation by + and the identity element by 0; in that case, the group is called an additive group. The identity element can also be written as ''id''.

The set ''G'' is called the ''underlying set'' of the group {{nowrap|(''G'', •)}}. Often the group's underlying set ''G'' is used as a short name for the group {{nowrap|(''G'', •)}}. Along the same lines, shorthand expressions such as "a subset of the group ''G''" or "an element of group ''G''" are used when what is actually meant is "a subset of the underlying set ''G'' of the group {{nowrap|(''G'', •)}}" or "an element of the underlying set ''G'' of the group {{nowrap|(''G'', •)}}".  Usually, it is clear from the context whether a symbol like ''G'' refers to a group or to an underlying set.

An alternate (but equivalent) definition is to expand the structure of a group to define a group as a set equipped with three operations satisfying the same axioms as above, with the "there exists" part removed in the two last axioms, these operations being
the group law, as above, which is a [[binary operation]],
the ''inverse operation'', which is a [[unary operation]] and maps {{mvar|a}} to &lt;math&gt;a^{-1},&lt;/math&gt;
and the identity element, which is viewed as a [[0-ary function|0-ary operation]].

As this formulation of the definition avoids [[existential quantifier]]s, it is generally preferred for [[computational group theory|computing with groups]] and for [[computer-aided proof]]s. This formulation exhibits groups as a variety of [[universal algebra]]. It is also useful for talking of properties of the inverse operation, as needed for defining [[topological group]]s and [[group object]]s.

=== Second example: a symmetry group ===
Two figures in the plane are [[congruence (geometry)|congruent]] if one can be changed into the other using a combination of [[rotation (mathematics)|rotation]]s, [[reflection (mathematics)|reflection]]s, and [[translation (geometry)|translation]]s.  Any figure is congruent to itself.  However, some figures are congruent to themselves in more than one way, and these extra congruences are called [[symmetry|symmetries]]. A square has eight symmetries. These are:
{| class="wikitable" style="text-align:center;"
|+ The elements of the symmetry group of the square (D&lt;sub&gt;4&lt;/sub&gt;). Vertices are identified by color or number.
|-
| [[Image:group D8 id.svg|140px]] &lt;br /&gt; id (keeping it as it is) || [[Image:group D8 90.svg|140px]] &lt;br /&gt; r&lt;sub&gt;1&lt;/sub&gt; (rotation by 90° clockwise) || [[Image:group D8 180.svg|140px]] &lt;br /&gt; r&lt;sub&gt;2&lt;/sub&gt; (rotation by 180° clockwise) || [[Image:group D8 270.svg|140px]] &lt;br /&gt; r&lt;sub&gt;3&lt;/sub&gt; (rotation by 270° clockwise)
|-
| [[Image:group D8 fv.svg|140px]] &lt;br /&gt; f&lt;sub&gt;v&lt;/sub&gt; (vertical reflection) || [[Image:group D8 fh.svg|140px]] &lt;br /&gt; f&lt;sub&gt;h&lt;/sub&gt; (horizontal reflection)|| [[Image:group D8 f13.svg|140px]] &lt;br /&gt; f&lt;sub&gt;d&lt;/sub&gt; (diagonal reflection) || [[Image:group D8 f24.svg|140px]] &lt;br /&gt; f&lt;sub&gt;c&lt;/sub&gt; (counter-diagonal reflection)
|}
* the [[identity operation]] leaving everything unchanged, denoted id;
* rotations of the square around its center by 90° clockwise, 180° clockwise, and 270° clockwise, denoted by r&lt;sub&gt;1&lt;/sub&gt;, r&lt;sub&gt;2&lt;/sub&gt; and r&lt;sub&gt;3&lt;/sub&gt;, respectively;
* reflections about the vertical and horizontal middle line (f&lt;sub&gt;h&lt;/sub&gt; and f&lt;sub&gt;v&lt;/sub&gt;), or through the two [[diagonal]]s (f&lt;sub&gt;d&lt;/sub&gt; and f&lt;sub&gt;c&lt;/sub&gt;).
{{clear}}

These symmetries are represented by functions. Each of these functions sends a point in the square to the corresponding point under the symmetry. For example, r&lt;sub&gt;1&lt;/sub&gt; sends a point to its rotation 90° clockwise around the square's center, and f&lt;sub&gt;h&lt;/sub&gt; sends a point to its reflection across the square's vertical middle line. Composing two of these symmetry functions gives another symmetry function.  These symmetries determine a group called the [[dihedral group]] of degree 4 and denoted D&lt;sub&gt;4&lt;/sub&gt;.  The underlying set of the group is the above set of symmetry functions, and the group operation is [[function composition]].&lt;ref&gt;{{Harvard citations|last = Herstein|year = 1975|loc = §2.6, p. 54|nb = yes}}&lt;/ref&gt; Two symmetries are combined by composing them as functions, that is, applying the first one to the square, and the second one to the result of the first application.  The result of performing first ''a'' and then ''b'' is written symbolically ''from right to left'' as
:{{nowrap|''b'' • ''a''}} ("apply the symmetry ''b'' after performing the symmetry ''a''").
The right-to-left notation is the same notation that is used for composition of functions.

The [[group table]] on the right lists the results of all such compositions possible. For example, rotating by 270° clockwise (r&lt;sub&gt;3&lt;/sub&gt;) and then reflecting horizontally (f&lt;sub&gt;h&lt;/sub&gt;) is the same as performing a reflection along the diagonal (f&lt;sub&gt;d&lt;/sub&gt;). Using the above symbols, highlighted in blue in the group table:
:{{nowrap|1=f&lt;sub&gt;h&lt;/sub&gt; • r&lt;sub&gt;3&lt;/sub&gt; = f&lt;sub&gt;d&lt;/sub&gt;}}.

{| class="wikitable" style="float:right; text-align:center; margin:.5em 0 .5em 1em; width:40ex; height:40ex;"
|+ [[Cayley table|Group table]] of D&lt;sub&gt;4&lt;/sub&gt;
|-
!  style="width:12%; background:#fdd; border-top:solid black 2px; border-left:solid black 2px;"| •
!  style="background:#fdd; border-top:solid black 2px; width:11%;"| id
!  style="background:#fdd; border-top:solid black 2px; width:11%;"| r&lt;sub&gt;1&lt;/sub&gt;
!  style="background:#fdd; border-top:solid black 2px; width:11%;"| r&lt;sub&gt;2&lt;/sub&gt;
!  style="background:#fdd; border-right:solid black 2px; border-top:solid black 2px; width:11%;"| r&lt;sub&gt;3&lt;/sub&gt;
! style="width:11%;"| f&lt;sub&gt;v&lt;/sub&gt; !! style="width:11%;"| f&lt;sub&gt;h&lt;/sub&gt; !! style="width:11%;"| f&lt;sub&gt;d&lt;/sub&gt; !! style="width:11%;"| f&lt;sub&gt;c&lt;/sub&gt;
|-
!style="background:#FDD;  border-left:solid black 2px;" | id
|style="background:#FDD;"| id
|style="background:#FDD;"| r&lt;sub&gt;1&lt;/sub&gt;
|style="background:#FDD;" | r&lt;sub&gt;2&lt;/sub&gt;
|style="background:#FDD; border-right:solid black 2px;"| r&lt;sub&gt;3&lt;/sub&gt; || f&lt;sub&gt;v&lt;/sub&gt; || f&lt;sub&gt;h&lt;/sub&gt; || f&lt;sub&gt;d&lt;/sub&gt;
|style="background:#FFFC93; border-right:solid black 2px; border-left:solid black 2px; border-top:solid black 2px;"| f&lt;sub&gt;c&lt;/sub&gt;
|-
!style="background:#FDD;  border-left:solid black 2px;" | r&lt;sub&gt;1&lt;/sub&gt;
|style="background:#FDD;"| r&lt;sub&gt;1&lt;/sub&gt;
|style="background:#FDD;"| r&lt;sub&gt;2&lt;/sub&gt;
|style="background:#FDD;"| r&lt;sub&gt;3&lt;/sub&gt;
|style="background:#FDD; border-right:solid black 2px;"| id || f&lt;sub&gt;c&lt;/sub&gt; || f&lt;sub&gt;d&lt;/sub&gt; || f&lt;sub&gt;v&lt;/sub&gt;
|style="background:#FFFC93; border-right: solid black 2px; border-left: solid black 2px;"| f&lt;sub&gt;h&lt;/sub&gt;
|- style="height:10%"
!style="background:#FDD;  border-left:solid black 2px;" | r&lt;sub&gt;2&lt;/sub&gt;
|style="background:#FDD;"| r&lt;sub&gt;2&lt;/sub&gt;
|style="background:#FDD;"| r&lt;sub&gt;3&lt;/sub&gt;
|style="background:#FDD;"| id
|style="background:#FDD; border-right:solid black 2px;"| r&lt;sub&gt;1&lt;/sub&gt; || f&lt;sub&gt;h&lt;/sub&gt; || f&lt;sub&gt;v&lt;/sub&gt; || f&lt;sub&gt;c&lt;/sub&gt;
|style="background:#FFFC93; border-right: solid black 2px; border-left: solid black 2px;"| f&lt;sub&gt;d&lt;/sub&gt;
|- style="height:10%"
!style="background:#FDD; border-bottom:solid black 2px; border-left:solid black 2px;" | r&lt;sub&gt;3&lt;/sub&gt;
|style="background:#FDD; border-bottom:solid black 2px;"| r&lt;sub&gt;3&lt;/sub&gt;
|style="background:#FDD; border-bottom:solid black 2px;"| id
|style="background:#FDD; border-bottom:solid black 2px;"| r&lt;sub&gt;1&lt;/sub&gt;
|style="background:#FDD; border-right:solid black 2px; border-bottom:solid black 2px;"| r&lt;sub&gt;2&lt;/sub&gt; || f&lt;sub&gt;d&lt;/sub&gt; || f&lt;sub&gt;c&lt;/sub&gt; || f&lt;sub&gt;h&lt;/sub&gt;
|style="background:#FFFC93; border-right:solid black 2px; border-left:solid black 2px; border-bottom:solid black 2px;"| f&lt;sub&gt;v&lt;/sub&gt;
|- style="height:10%"
! f&lt;sub&gt;v&lt;/sub&gt;
| f&lt;sub&gt;v&lt;/sub&gt; || f&lt;sub&gt;d&lt;/sub&gt; || f&lt;sub&gt;h&lt;/sub&gt; || f&lt;sub&gt;c&lt;/sub&gt;|| id || r&lt;sub&gt;2&lt;/sub&gt; || r&lt;sub&gt;1&lt;/sub&gt; || r&lt;sub&gt;3&lt;/sub&gt;
|- style="height:10%"
! f&lt;sub&gt;h&lt;/sub&gt;
| f&lt;sub&gt;h&lt;/sub&gt; || f&lt;sub&gt;c&lt;/sub&gt; || f&lt;sub&gt;v&lt;/sub&gt; ||style="background:#DEF;border:solid black 2px;"| f&lt;sub&gt;d&lt;/sub&gt; || r&lt;sub&gt;2&lt;/sub&gt; || id || r&lt;sub&gt;3&lt;/sub&gt; || r&lt;sub&gt;1&lt;/sub&gt;
|- style="height:10%"
! f&lt;sub&gt;d&lt;/sub&gt;
| f&lt;sub&gt;d&lt;/sub&gt; || f&lt;sub&gt;h&lt;/sub&gt; || f&lt;sub&gt;c&lt;/sub&gt; || f&lt;sub&gt;v&lt;/sub&gt; || r&lt;sub&gt;3&lt;/sub&gt; || r&lt;sub&gt;1&lt;/sub&gt; || id || r&lt;sub&gt;2&lt;/sub&gt;
|- style="height:10%"
! f&lt;sub&gt;c&lt;/sub&gt;
|style="background:#9DFF93; border-left: solid black 2px; border-bottom: solid black 2px; border-top: solid black 2px;" | f&lt;sub&gt;c&lt;/sub&gt;
|style="background:#9DFF93; border-bottom: solid black 2px; border-top: solid black 2px;" | f&lt;sub&gt;v&lt;/sub&gt;
|style="background:#9DFF93; border-bottom: solid black 2px; border-top: solid black 2px;" | f&lt;sub&gt;d&lt;/sub&gt;
|style="background:#9DFF93; border-bottom:solid black 2px; border-top:solid black 2px; border-right:solid black 2px;" | f&lt;sub&gt;h&lt;/sub&gt; || r&lt;sub&gt;1&lt;/sub&gt; || r&lt;sub&gt;3&lt;/sub&gt; || r&lt;sub&gt;2&lt;/sub&gt; || id
|-
| colspan="9" style="text-align:left"| The elements id, r&lt;sub&gt;1&lt;/sub&gt;, r&lt;sub&gt;2&lt;/sub&gt;, and r&lt;sub&gt;3&lt;/sub&gt; form a [[subgroup]], highlighted in {{color box|#FDD}} red (upper left region). A left and right [[coset]] of this subgroup is highlighted in {{color box|#9DFF93}} green (in the last row) and {{color box|#FFFC93}} yellow (last column), respectively.
|}
Given this set of symmetries and the described operation, the group axioms can be understood as follows:
{{ordered list
|1= The closure axiom demands that the composition {{nowrap|''b'' • ''a''}} of any two symmetries ''a'' and ''b'' is also a symmetry. Another example for the group operation is
:{{nowrap|r&lt;sub&gt;3&lt;/sub&gt; • f&lt;sub&gt;h&lt;/sub&gt; {{=}} f&lt;sub&gt;c&lt;/sub&gt;,}}
i.e., rotating 270° clockwise after reflecting horizontally equals reflecting along the counter-diagonal (f&lt;sub&gt;c&lt;/sub&gt;). Indeed every other combination of two symmetries still gives a symmetry, as can be checked using the group table.

|2= The associativity constraint deals with composing more than two symmetries: Starting with three elements ''a'', ''b'' and ''c'' of D&lt;sub&gt;4&lt;/sub&gt;, there are two possible ways of using these three symmetries in this order to determine a symmetry of the square.  One of these ways is to first compose ''a'' and ''b'' into a single symmetry, then to compose that symmetry with ''c''.  The other way is to first compose ''b'' and ''c'', then to compose the resulting symmetry with ''a''.  The associativity condition
:{{nowrap|1=(''a'' • ''b'') • ''c'' = ''a'' • (''b'' • ''c'')}}
means that these two ways are the same, i.e., a product of many group elements can be simplified in any grouping.
For example, {{nowrap|1=(f&lt;sub&gt;d&lt;/sub&gt; • f&lt;sub&gt;v&lt;/sub&gt;) • r&lt;sub&gt;2&lt;/sub&gt; = f&lt;sub&gt;d&lt;/sub&gt; • (f&lt;sub&gt;v&lt;/sub&gt; • r&lt;sub&gt;2&lt;/sub&gt;)}} can be checked using the group table at the right
{{aligned table|cols=5|style=margin-left:1.2em
|(f&lt;sub&gt;d&lt;/sub&gt; • f&lt;sub&gt;v&lt;/sub&gt;) • r&lt;sub&gt;2&lt;/sub&gt;|&amp;nbsp;{{=}}&amp;nbsp;|r&lt;sub&gt;3&lt;/sub&gt; • r&lt;sub&gt;2&lt;/sub&gt;|&amp;nbsp;{{=}}&amp;nbsp;|r&lt;sub&gt;1&lt;/sub&gt;, which equals
|f&lt;sub&gt;d&lt;/sub&gt; • (f&lt;sub&gt;v&lt;/sub&gt; • r&lt;sub&gt;2&lt;/sub&gt;)|{{=}}|f&lt;sub&gt;d&lt;/sub&gt; • f&lt;sub&gt;h&lt;/sub&gt;|{{=}}|r&lt;sub&gt;1&lt;/sub&gt;.
}}
While associativity is true for the symmetries of the square and addition of numbers, it is not true for all operations.  For instance, subtraction of numbers is not associative: {{nowrap|(7 − 3) − 2 {{=}} 2}} is not the same as {{nowrap|7 − (3 − 2) {{=}} 6.}}

|3= The identity element is the symmetry id leaving everything unchanged: for any symmetry ''a'', performing id after ''a'' (or ''a'' after id) equals ''a'', in symbolic form,
: {{nowrap|1=id • ''a'' = ''a'',}}
: {{nowrap|1=''a'' • id = ''a''.}}

|4= An inverse element undoes the transformation of some other element. Every symmetry can be undone: each of the following transformations—identity id, the reflections f&lt;sub&gt;h&lt;/sub&gt;, f&lt;sub&gt;v&lt;/sub&gt;, f&lt;sub&gt;d&lt;/sub&gt;, f&lt;sub&gt;c&lt;/sub&gt; and the 180° rotation r&lt;sub&gt;2&lt;/sub&gt;—is its own inverse, because performing it twice brings the square back to its original orientation. The rotations r&lt;sub&gt;3&lt;/sub&gt; and r&lt;sub&gt;1&lt;/sub&gt; are each other's inverses, because rotating 90° and then rotation 270° (or vice versa) yields a rotation over 360° which leaves the square unchanged. In symbols,
:{{nowrap|f&lt;sub&gt;h&lt;/sub&gt; • f&lt;sub&gt;h&lt;/sub&gt; {{=}} id,}}
:{{nowrap|r&lt;sub&gt;3&lt;/sub&gt; • r&lt;sub&gt;1&lt;/sub&gt; {{=}} r&lt;sub&gt;1&lt;/sub&gt; • r&lt;sub&gt;3&lt;/sub&gt; {{=}} id.}}
}}

In contrast to the group of integers above, where the order of the operation is irrelevant, it does matter in D&lt;sub&gt;4&lt;/sub&gt;: {{nowrap|f&lt;sub&gt;h&lt;/sub&gt; • r&lt;sub&gt;1&lt;/sub&gt; {{=}} f&lt;sub&gt;c&lt;/sub&gt;}} but {{nowrap|r&lt;sub&gt;1&lt;/sub&gt; • f&lt;sub&gt;h&lt;/sub&gt; {{=}} f&lt;sub&gt;d&lt;/sub&gt;.}} In other words, D&lt;sub&gt;4&lt;/sub&gt; is not abelian, which makes the group structure more difficult than the integers introduced first.

== History ==
{{Main|History of group theory}}
The modern concept of an abstract group developed out of several fields of mathematics.&lt;ref&gt;{{Harvard citations|nb = yes|last = Wussing|year = 2007}}&lt;/ref&gt;&lt;ref&gt;{{Harvard citations|last = Kleiner|year = 1986|nb = yes}}&lt;/ref&gt;&lt;ref&gt;{{Harvard citations|last = Smith|year = 1906|nb = yes}}&lt;/ref&gt; The original motivation for group theory was the quest for solutions of [[polynomial equation]]s of degree higher than 4. The 19th-century French mathematician [[Évariste Galois]], extending prior work of [[Paolo Ruffini]] and [[Joseph-Louis Lagrange]], gave a criterion for the solvability of a particular polynomial equation in terms of the [[symmetry group]] of its [[root of a function|roots]] (solutions).  The elements of such a [[Galois group]] correspond to certain [[permutation]]s of the roots. At first, Galois' ideas were rejected by his contemporaries, and published only posthumously.&lt;ref&gt;{{Harvard citations|last = Galois|year = 1908|nb = yes}}&lt;/ref&gt;&lt;ref&gt;{{Harvard citations|last = Kleiner|year = 1986|loc = p. 202|nb = yes}}&lt;/ref&gt; More general [[permutation group]]s were investigated in particular by [[Augustin Louis Cauchy]]. [[Arthur Cayley]]'s ''On the theory of groups, as depending on the symbolic equation θ&lt;sup&gt;n&lt;/sup&gt; = 1'' (1854) gives the first abstract definition of a [[finite group]].&lt;ref&gt;{{Harvard citations|last=Cayley|year=1889|nb=yes}}&lt;/ref&gt;

Geometry was a second field in which groups were used systematically, especially symmetry groups as part of [[Felix Klein]]'s 1872 [[Erlangen program]].&lt;ref&gt;{{Harvard citations|last = Wussing|year = 2007|loc = §III.2|nb = yes}}&lt;/ref&gt; After novel geometries such as [[hyperbolic geometry|hyperbolic]] and [[projective geometry]] had emerged, Klein used group theory to organize them in a more coherent way. Further advancing these ideas, [[Sophus Lie]] founded the study of [[Lie group]]s in 1884.&lt;ref&gt;{{Harvard citations|last=Lie|year=1973|nb=yes}}&lt;/ref&gt;

The third field contributing to group theory was [[number theory]]. Certain [[abelian group]] structures had been used implicitly in [[Carl Friedrich Gauss]]' number-theoretical work ''[[Disquisitiones Arithmeticae]]'' (1798), and more explicitly by [[Leopold Kronecker]].&lt;ref&gt;{{Harvard citations|last = Kleiner|year = 1986|loc = p. 204|nb = yes}}&lt;/ref&gt; In 1847, [[Ernst Kummer]] made early attempts to prove [[Fermat's Last Theorem]]  by developing [[class group|groups describing factorization]] into [[prime number]]s.&lt;ref&gt;{{Harvard citations|last = Wussing|year = 2007|loc = §I.3.4|nb = yes}}&lt;/ref&gt;

The convergence of these various sources into a uniform theory of groups started with [[Camille Jordan]]'s ''Traité des substitutions et des équations algébriques'' (1870).&lt;ref&gt;{{Harvard citations|last=Jordan|year=1870|nb=yes}}&lt;/ref&gt; [[Walther von Dyck]] (1882) introduced the idea of specifying a group by means of generators and relations, and was also the first to give an axiomatic definition of an "abstract group", in the terminology of the time.&lt;ref&gt;{{Harvard citations|last=von Dyck|year=1882|nb=yes}}&lt;/ref&gt; As of the 20th century, groups gained wide recognition by the pioneering work of [[Ferdinand Georg Frobenius]] and [[William Burnside]], who worked on [[representation theory]] of finite groups, [[Richard Brauer]]'s [[modular representation theory]] and [[Issai Schur]]'s papers.&lt;ref&gt;{{Harvard citations|last = Curtis |year = 2003|nb = yes}}&lt;/ref&gt; The theory of Lie groups, and more generally [[locally compact group]]s was studied by [[Hermann Weyl]], [[Élie Cartan]] and many others.&lt;ref&gt;{{Harvard citations|last=Mackey|year=1976|nb=yes}}&lt;/ref&gt; Its algebraic counterpart, the theory of [[algebraic group]]s, was first shaped by [[Claude Chevalley]] (from the late 1930s) and later by the work of [[Armand Borel]] and [[Jacques Tits]].&lt;ref&gt;{{Harvard citations|last=Borel|year=2001|nb=yes}}&lt;/ref&gt;

The [[University of Chicago]]'s 1960–61 Group Theory Year brought together group theorists such as [[Daniel Gorenstein]], [[John G. Thompson]] and [[Walter Feit]], laying the foundation of a collaboration that, with input from numerous other mathematicians, led to the [[classification of finite simple groups]], with the final step taken by [[Michael Aschbacher|Aschbacher]] and Smith in 2004. This project exceeded previous mathematical endeavours by its sheer size, in both length of proof and number of researchers. Research is ongoing to simplify the proof of this classification.&lt;ref&gt;{{Harvard citations|last = Aschbacher|year = 2004|nb = yes}}&lt;/ref&gt; These days, group theory is still a highly active mathematical branch, impacting many other fields.{{cref|a}}

==Elementary consequences of the group axioms ==
Basic facts about all groups that can be obtained directly from the group axioms are commonly subsumed under ''elementary group theory''.&lt;ref&gt;{{Harvard citations|last = Ledermann|year = 1953|loc = §1.2, pp. 4–5|nb = yes}}&lt;/ref&gt; For example, [[Mathematical induction|repeated]] applications of the associativity axiom show that the unambiguity of
:''a'' • ''b'' • ''c'' = (''a'' • ''b'') • ''c'' = ''a'' • (''b'' • ''c'')
generalizes to more than three factors. Because this implies that parentheses can be inserted anywhere within such a series of terms, parentheses are usually omitted.&lt;ref&gt;{{Harvard citations|nb = yes|last = Ledermann|year = 1973|loc = §I.1, p. 3}}&lt;/ref&gt;

The axioms may be weakened to assert only the existence of a [[left identity]] and [[left inverse element|left inverse]]s. Both can be shown to be actually two-sided, so the resulting definition is equivalent to the one given above.&lt;ref&gt;{{Harvard citations|nb = yes|last = Lang|year = 2002|loc = §I.2, p. 7}}&lt;/ref&gt;

===Uniqueness of identity element and inverses===
Two important consequences of the group axioms are the uniqueness of the identity element and the uniqueness of inverse elements. There can be only one identity element in a group, and each element in a group has exactly one inverse element. Thus, it is customary to speak of ''the'' identity, and ''the'' inverse of an element.&lt;ref name="lang2005"&gt;{{Harvard citations|nb = yes|last = Lang|year = 2005|loc = §II.1, p. 17}}&lt;/ref&gt;

To prove the uniqueness of an inverse element of ''a'', suppose that ''a'' has two inverses, denoted ''b'' and ''c'', in a group (''G'', •). Then

:{|
|''b'' ||=||''b'' • ''e'' ||&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;||as ''e'' is the identity element
|-
| ||=||''b'' • (''a'' • ''c'') ||&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;||because ''c'' is an inverse of ''a'', so ''e'' = ''a'' • ''c''
|-
| ||=||(''b'' • ''a'') • ''c'' ||&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;||by associativity, which allows rearranging the parentheses
|-
| ||=||''e'' • ''c''||&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;||since ''b'' is an inverse of ''a'', i.e., ''b'' • ''a'' = ''e''
|-
| ||=||''c''||&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;|| for ''e'' is the identity element
|}

The term ''b'' on the first line above and the ''c'' on the last are equal, since they are connected by a chain of equalities. In other words, there is only one inverse element of ''a''.  Similarly, to prove that the identity element of a group is unique, assume ''G'' is a group with two identity elements ''e'' and ''f''. Then ''e'' = ''e'' • ''f'' = ''f'', hence ''e'' and ''f'' are equal.

===&lt;span id="translation"&gt;&lt;/span&gt;Division===
In groups, the existence of inverse elements implies that [[division (mathematics)|division]] is possible: given elements ''a'' and ''b'' of the group ''G'', there is exactly one solution ''x'' in ''G'' to the [[equation]] {{nowrap|1=''x'' • ''a'' = ''b''}}, namely {{nowrap|''b'' • ''a''&lt;sup&gt;−1&lt;/sup&gt;}}.&lt;ref name="lang2005"/&gt; In fact, we have 
:{{nowrap|1= (''b'' • ''a''&lt;sup&gt;−1&lt;/sup&gt;) • ''a'' =  ''b'' • (''a''&lt;sup&gt;−1&lt;/sup&gt; • ''a'') = ''b'' • ''e'' = ''b''.}}
Uniqueness results by multiplying the two sides of the equation {{nowrap|1=''x'' • ''a'' = ''b''}} by {{nowrap|''a''&lt;sup&gt;−1&lt;/sup&gt;}}. The element {{nowrap|''b'' • ''a''&lt;sup&gt;−1&lt;/sup&gt;}}, often denoted {{nowrap|''b'' / ''a''}}, is called the ''right quotient'' of ''b'' by ''a'', or the result of the ''right division'' of ''b'' by ''a''.

Similarly there is exactly one solution ''y'' in ''G'' to the equation {{nowrap|1=''a'' • ''y'' = ''b''}}, namely {{nowrap|1=''y'' = ''a''&lt;sup&gt;−1&lt;/sup&gt; • ''b''}}. This solution is the ''left quotient'' of ''b'' by ''a'', and is sometimes denoted {{nowrap|''a'' \ ''b''}}.

In general {{nowrap|''b'' / ''a''}} and {{nowrap|''a'' \ ''b''}} may be different, but, if the group operation is [[commutative]] (that is, if the group is [[abelian group|abelian]]), they are equal. In this case, the group operation is often denoted as an [[addition]], and one talks of ''subtraction'' and ''difference'' instead of division and quotient.

A consequence of this is that multiplication by a group element ''g'' is a [[bijection]]. Specifically, if ''g'' is an element of the group ''G'', the [[function (mathematics)]] from ''G'' to itself that maps {{nowrap|''h'' ∈ ''G''}} to {{nowrap|''g'' • ''h''}} is a bijection. This function is called the ''left translation'' by ''g'' . Similarly, the ''right translation'' by ''g'' is the bijection from ''G'' to itself, that maps ''h'' to {{nowrap|''h'' • ''g''}}. If ''G'' is abelian, the left and the right translation by a group element are the same.

== Basic concepts ==
:&lt;div class="dablink"&gt;''The following sections use [[table of mathematical symbols|mathematical symbols]] such as'' {{nowrap|1=''X'' = {''x'', ''y'', ''z''} }} ''to denote a [[set (mathematics)|set]]'' ''X'' ''containing [[element (mathematics)|elements]]'' ''x'', ''y'', ''and'' ''z'', ''or alternatively'' {{nowrap|''x'' ∈ ''X''}} ''to restate that'' ''x'' ''is an element of'' ''X''. ''The notation'' {{nowrap|''f'' : ''X'' → ''Y''}} ''means'' ''f'' ''is a [[function (mathematics)|function]] assigning to every element of'' ''X'' ''an element of'' ''Y''.&lt;/div&gt;

To understand groups beyond the level of mere symbolic manipulations as above, more structural concepts have to be employed.{{cref|c}} There is a conceptual principle underlying all of the following notions: to take advantage of the structure offered by groups (which sets, being "structureless", do not have), constructions related to groups have to be ''compatible'' with the group operation. This compatibility manifests itself in the following notions in various ways. For example, groups can be related to each other via functions called group homomorphisms. By the mentioned principle, they are required to respect the group structures in a precise sense. The structure of groups can also be understood by breaking them into pieces called subgroups and quotient groups. The principle of "preserving structures"—a recurring topic in mathematics throughout—is an instance of working in a [[category (mathematics)|category]], in this case the [[category of groups]].&lt;ref name="MacLane"/&gt;

===Group homomorphisms===
{{Main|Group homomorphism}}
''Group homomorphisms''{{cref|g}} are functions that preserve group structure.  A function {{nowrap|''a'': ''G'' → ''H''}} between two groups {{nowrap|(''G'', •)}} and {{nowrap|(''H'', ∗)}} is called a ''homomorphism'' if the equation
:{{nowrap|1=''a''(''g'' • ''k'') = ''a''(''g'') ∗ ''a''(''k'')}}
holds for all elements ''g'', ''k'' in ''G''. In other words, the result is the same when performing the group operation after or before applying the map ''a''. This requirement ensures that {{nowrap|1=''a''(1&lt;sub&gt;''G''&lt;/sub&gt;) = 1&lt;sub&gt;''H''&lt;/sub&gt;}}, and also {{nowrap|1=''a''(''g'')&lt;sup&gt;−1&lt;/sup&gt; = ''a''(''g''&lt;sup&gt;−1&lt;/sup&gt;)}} for all ''g'' in ''G''. Thus a group homomorphism respects all the structure of ''G'' provided by the group axioms.&lt;ref&gt;{{Harvard citations|nb = yes|last = Lang|year = 2005|loc = §II.3, p. 34}}&lt;/ref&gt;

Two groups ''G'' and ''H'' are called ''[[Group isomorphism|isomorphic]]'' if there exist group homomorphisms {{nowrap|''a'': ''G'' → ''H''}} and {{nowrap|''b'': ''H'' → ''G''}}, such that applying the two functions [[function composition|one after another]] in each of the two possible orders gives the [[identity function]]s of ''G'' and ''H''. That is, {{nowrap|1=''a''(''b''(''h'')) = ''h''}} and {{nowrap|1=''b''(''a''(''g'')) = ''g''}} for any ''g'' in ''G'' and ''h'' in ''H''. From an abstract point of view, isomorphic groups carry the same information. For example, proving that {{nowrap|1=''g'' • ''g'' = 1&lt;sub&gt;''G''&lt;/sub&gt;}} for some element ''g'' of ''G'' is [[Logical equivalence|equivalent]] to proving that {{nowrap|1=''a''(''g'') ∗ ''a''(''g'') = 1&lt;sub&gt;''H''&lt;/sub&gt;}}, because applying ''a'' to the first equality yields the second, and applying ''b'' to the second gives back the first.

===Subgroups===
{{Main|Subgroup}}
Informally, a ''subgroup'' is a group ''H'' contained within a bigger one, ''G''.&lt;ref&gt;{{Harvard citations|nb = yes|last = Lang|year = 2005|loc = §II.1, p. 19}}&lt;/ref&gt; Concretely, the identity element of ''G'' is  contained in ''H'', and whenever ''h''&lt;sub&gt;1&lt;/sub&gt; and ''h''&lt;sub&gt;2&lt;/sub&gt; are in ''H'', then so are {{nowrap|''h''&lt;sub&gt;1&lt;/sub&gt; •  ''h''&lt;sub&gt;2&lt;/sub&gt;}} and ''h''&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;−1&lt;/sup&gt;, so the elements of ''H'', equipped with the group operation on ''G'' restricted to ''H'', indeed form a group.

In the example above, the identity and the rotations constitute a subgroup {{nowrap|1=''R'' = {id, r&lt;sub&gt;1&lt;/sub&gt;, r&lt;sub&gt;2&lt;/sub&gt;, r&lt;sub&gt;3&lt;/sub&gt;},}} highlighted in red in the group table above: any two rotations composed are still a rotation, and a rotation can be undone by (i.e., is inverse to) the complementary rotations  270° for 90°, 180° for 180°, and 90° for 270° (note that rotation in the opposite direction is not defined). The [[subgroup test]] is a [[Necessary and sufficient conditions|necessary and sufficient condition]] for a nonempty subset ''H'' of a group ''G'' to be a subgroup: it is sufficient to check that {{nowrap|''g''&lt;sup&gt;−1&lt;/sup&gt;''h'' ∈ ''H''}} for all elements {{nowrap|''g'', ''h'' ∈ ''H''}}. Knowing [[lattice of subgroups|the subgroups]] is important in understanding the group as a whole.{{cref|d}}

Given any subset ''S'' of a group ''G'', the subgroup generated by ''S'' consists of products of elements of ''S'' and their inverses. It is the smallest subgroup of ''G'' containing ''S''.&lt;ref&gt;{{Harvard citations|nb = yes|last = Ledermann|year = 1973|loc = §II.12, p. 39}}&lt;/ref&gt; In the introductory example above, the subgroup generated by r&lt;sub&gt;2&lt;/sub&gt; and f&lt;sub&gt;v&lt;/sub&gt; consists of these two elements, the identity element id and {{nowrap|1=f&lt;sub&gt;h&lt;/sub&gt; = f&lt;sub&gt;v&lt;/sub&gt; • r&lt;sub&gt;2&lt;/sub&gt;}}. Again, this is a subgroup, because combining any two of these four elements or their inverses (which are, in this particular case, these same elements) yields an element of this subgroup.

===Cosets===
{{Main|Coset}}
In many situations it is desirable to consider two group elements the same if they differ by an element of a given subgroup. For example, in D&lt;sub&gt;4&lt;/sub&gt; above, once a reflection is performed, the square never gets back to the r&lt;sub&gt;2&lt;/sub&gt; configuration by just applying the rotation operations (and no further reflections), i.e., the rotation operations are irrelevant to the question whether a reflection has been performed. Cosets are used to formalize this insight: a subgroup ''H'' defines left and right cosets, which can be thought of as translations of ''H'' by arbitrary group elements ''g''. In symbolic terms, the ''left'' and ''right'' cosets of ''H'' containing ''g'' are

:{{nowrap|1=''gH'' = {''g'' • ''h'' : ''h'' ∈ ''H''} }} and {{nowrap|1=''Hg'' = {''h'' • ''g'' : ''h'' ∈ ''H''},}} respectively.&lt;ref&gt;{{Harvard citations|nb = yes|last = Lang|year = 2005|loc = §II.4, p. 41}}&lt;/ref&gt;

The left cosets of any subgroup ''H'' form a [[Partition of a set|partition]] of ''G''; that is, the [[Union (set theory)|union]] of all left cosets is equal to ''G'' and two left cosets are either equal or have an [[empty set|empty]] [[Intersection (set theory)|intersection]].&lt;ref&gt;{{Harvard citations|nb = yes|last = Lang|year = 2002|loc = §I.2, p. 12}}&lt;/ref&gt; The first case {{nowrap|1=''g''&lt;sub&gt;1&lt;/sub&gt;''H'' = ''g''&lt;sub&gt;2&lt;/sub&gt;''H''}} happens [[if and only if|precisely when]] {{nowrap|''g''&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;−1&lt;/sup&gt; • ''g''&lt;sub&gt;2&lt;/sub&gt; ∈ ''H''}}, i.e., if the two elements differ by an element of ''H''. Similar considerations apply to the right cosets of ''H''. The left and right cosets of ''H'' may or may not be equal. If they are, i.e., for all ''g'' in ''G'', {{nowrap|1=''gH'' = ''Hg''}}, then ''H'' is said to be a ''[[normal subgroup]]''.

In D&lt;sub&gt;4&lt;/sub&gt;, the introductory symmetry group, the left cosets ''gR'' of the subgroup ''R'' consisting of the rotations are either equal to ''R'', if ''g'' is an element of ''R'' itself, or otherwise equal to {{nowrap|1=''U'' = f&lt;sub&gt;c&lt;/sub&gt;''R'' = {f&lt;sub&gt;c&lt;/sub&gt;, f&lt;sub&gt;v&lt;/sub&gt;, f&lt;sub&gt;d&lt;/sub&gt;, f&lt;sub&gt;h&lt;/sub&gt;} }} (highlighted in green). The subgroup ''R'' is also normal, because {{nowrap|1=f&lt;sub&gt;c&lt;/sub&gt;''R'' = ''U'' = ''R''f&lt;sub&gt;c&lt;/sub&gt;}} and similarly for any element other than f&lt;sub&gt;c&lt;/sub&gt;. (In fact, in the case of D&lt;sub&gt;4&lt;/sub&gt;, observe that all such cosets are equal, such that {{nowrap|1=f&lt;sub&gt;h&lt;/sub&gt;''R'' = f&lt;sub&gt;v&lt;/sub&gt;''R'' = f&lt;sub&gt;d&lt;/sub&gt;''R'' = f&lt;sub&gt;c&lt;/sub&gt;''R''}}.)

===Quotient groups===
{{Main|Quotient group}}
In some situations the set of cosets of a subgroup can be endowed with a group law, giving a ''quotient group'' or ''factor group''. For this to be possible, the subgroup has to be [[normal subgroup|normal]]. Given any normal subgroup ''N'', the quotient group is defined by
:''G'' / ''N'' = {''gN'', ''g'' ∈ ''G''}, "''G'' modulo ''N''".&lt;ref&gt;{{Harvard citations|nb = yes|last = Lang|year = 2005|loc = §II.4, p. 45}}&lt;/ref&gt;

This set inherits a group operation (sometimes called coset multiplication, or coset addition) from the original group ''G'': {{nowrap|1=(''gN'') • (''hN'') = (''gh'')''N''}} for all ''g'' and ''h'' in ''G''. This definition is motivated by the idea (itself an instance of general structural considerations outlined above) that the map {{nowrap|''G'' → ''G'' / ''N''}} that associates to any element ''g'' its coset ''gN'' be a group homomorphism, or by general abstract considerations called [[universal property|universal properties]]. The coset {{nowrap|1=''eN '' = ''N''}} serves as the identity in this group, and the inverse of ''gN'' in the quotient group is {{nowrap|1=(''gN'')&lt;sup&gt;−1&lt;/sup&gt; = (''g''&lt;sup&gt;−1&lt;/sup&gt;)''N''}}.{{cref|e}}

{| class="wikitable" style="float:right; text-align:center; margin:.5em 0 .5em 1em; width:200px;"
|+ Group table of the quotient group {{nowrap|D&lt;sub&gt;4&lt;/sub&gt; / ''R''}}
|-
! style="width:30px;"| •
! style="width:33%;"| R
! style="width:33%;"| U
|-
! ''R''
| ''R'' || ''U''
|-
! ''U''
| ''U'' || ''R''
|}
The elements of the quotient group {{nowrap|D&lt;sub&gt;4&lt;/sub&gt; / ''R''}} are ''R'' itself, which represents the identity, and {{nowrap|1=''U'' = f&lt;sub&gt;v&lt;/sub&gt;''R''}}. The group operation on the quotient is shown at the right. For example, {{nowrap|1=''U'' • ''U'' = f&lt;sub&gt;v&lt;/sub&gt;''R'' • f&lt;sub&gt;v&lt;/sub&gt;''R'' = (f&lt;sub&gt;v&lt;/sub&gt; • f&lt;sub&gt;v&lt;/sub&gt;)''R'' = ''R''}}. Both the subgroup {{nowrap|1=''R'' = {id, r&lt;sub&gt;1&lt;/sub&gt;, r&lt;sub&gt;2&lt;/sub&gt;, r&lt;sub&gt;3&lt;/sub&gt;},}} as well as the corresponding quotient are abelian, whereas D&lt;sub&gt;4&lt;/sub&gt; is not abelian. Building bigger groups by smaller ones, such as D&lt;sub&gt;4&lt;/sub&gt; from its subgroup ''R'' and the quotient {{nowrap|D&lt;sub&gt;4&lt;/sub&gt; / ''R''}} is abstracted by a notion called [[semidirect product]].

Quotient groups and subgroups together form a way of describing every group by its ''[[presentation of a group|presentation]]'': any group is the quotient of the [[free group]] over the ''[[Generating set of a group|generators]]'' of the group, quotiented by the subgroup of ''relations''. The dihedral group D&lt;sub&gt;4&lt;/sub&gt;, for example, can be generated by two elements ''r'' and ''f'' (for example, ''r''&amp;nbsp;=&amp;nbsp;r&lt;sub&gt;1&lt;/sub&gt;, the right rotation and ''f''&amp;nbsp;=&amp;nbsp;f&lt;sub&gt;v&lt;/sub&gt; the vertical (or any other) reflection), which means that every symmetry of the square is a finite composition of these two symmetries or their inverses. Together with the relations
:''r ''&lt;sup&gt;4&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;''f ''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;(''r'' • ''f'')&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;1,&lt;ref&gt;{{Harvard citations|nb = yes|last = Lang|year = 2002|loc = §I.2, p. 9}}&lt;/ref&gt;
the group is completely described. A presentation of a group can also be used to construct the [[Cayley graph]], a device used to graphically capture [[discrete group]]s.

Sub- and quotient groups are related in the following way: a subset ''H'' of ''G'' can be seen as an [[injective]] map {{nowrap|''H'' → ''G''}}, i.e., any element of the target has at most one [[preimage|element that maps to it]]. The counterpart to injective maps are [[surjective]] maps (every element of the target is mapped onto), such as the canonical map {{nowrap|''G'' → ''G'' / ''N''}}.{{cref|y}} Interpreting subgroup and quotients in light of these homomorphisms emphasizes the structural concept inherent to these definitions alluded to in the introduction. In general, homomorphisms are neither injective nor surjective. [[Kernel (algebra)|Kernel]] and [[Image (mathematics)|image]] of group homomorphisms and the [[first isomorphism theorem]] address this phenomenon.

==Examples and applications==
{{Main|Examples of groups|Applications of group theory}}
{{multiple image
| align     = right
| direction = vertical
| width     = 180
| image1    = Wallpaper group-cm-6.jpg
| width1    = 150
| caption1  = A periodic wallpaper pattern gives rise to a [[wallpaper group]].
| image2    = Fundamental group.svg
| width2    = 180
| caption2  = The fundamental group of a plane minus a point (bold) consists of loops around the missing point. This group is isomorphic to the integers.
}}
Examples and applications of groups abound. A starting point is the group '''Z''' of integers with addition as group operation, introduced above. If instead of addition [[multiplication]] is considered, one obtains [[multiplicative group]]s. These groups are predecessors of important constructions in [[abstract algebra]].

Groups are also applied in many other mathematical areas. Mathematical objects are often examined by [[functor|associating]] groups to them and studying the properties of the corresponding groups. For example, [[Henri Poincaré]] founded what is now called [[algebraic topology]] by introducing the [[fundamental group]].&lt;ref&gt;{{Harvard citations|nb = yes|last = Hatcher|year = 2002|loc = Chapter I, p. 30}}&lt;/ref&gt; By means of this connection, [[Glossary of topology|topological properties]] such as [[Neighbourhood (mathematics)|proximity]] and [[continuous function|continuity]] translate into properties of groups.{{cref|i}} For example, elements of the fundamental group are represented by loops. The second image at the right shows some loops in a plane minus a point. The blue loop is considered [[null-homotopic]] (and thus irrelevant), because it can be [[homotopy|continuously shrunk]] to a point. The presence of the hole prevents the orange loop from being shrunk to a point. The fundamental group of the plane with a point deleted turns out to be infinite cyclic, generated by the orange loop (or any other loop [[winding number|winding once]] around the hole). This way, the fundamental group detects the hole.

In more recent applications, the influence has also been reversed to motivate geometric constructions by a group-theoretical background.{{cref|j}} In a similar vein, [[geometric group theory]] employs geometric concepts, for example in the study of [[hyperbolic group]]s.&lt;ref&gt;{{Harvard citations|nb = yes|last1 = Coornaert|last2 = Delzant|last3=Papadopoulos|year = 1990}}&lt;/ref&gt; Further branches crucially applying groups include [[algebraic geometry]] and [[number theory]].&lt;ref&gt;for example, [[class group]]s and [[Picard group]]s; see {{Harvard citations|nb = yes|last = Neukirch|year = 1999}}, in particular §§I.12 and I.13&lt;/ref&gt;

In addition to the above theoretical applications, many practical applications of groups exist. [[Cryptography]] relies on the combination of the abstract group theory approach together with [[algorithm]]ical knowledge obtained in [[computational group theory]], in particular when implemented for finite groups.&lt;ref&gt;{{Harvard citations|nb = yes|last = Seress|year = 1997}}&lt;/ref&gt; Applications of group theory are not restricted to mathematics; sciences such as [[physics]], [[chemistry]] and [[computer science]] benefit from the concept.

=== Numbers ===
Many number systems, such as the integers and the rationals enjoy a naturally given group structure. In some cases, such as with the rationals, both addition and multiplication operations give rise to group structures. Such number systems are predecessors to more general algebraic structures known as [[ring (mathematics)|rings]] and [[field (mathematics)|fields]]. Further [[abstract algebra]]ic concepts such as [[module (mathematics)|module]]s, [[vector space]]s and [[algebra over a field|algebras]] also form groups.

==== Integers ====
The group of integers &lt;math&gt;\mathbb{Z}&lt;/math&gt; under addition, denoted &lt;math&gt;\left(\mathbb{Z},+\right)&lt;/math&gt;, has been described above. The integers, with the operation of [[multiplication]] instead of addition, &lt;math&gt;\left(\mathbb{Z},\cdot\right)&lt;/math&gt; do ''not'' form a group. The closure, associativity and identity axioms are satisfied, but inverses do not exist: for example, {{nowrap|1=''a'' = 2}} is an integer, but the only solution to the equation {{nowrap|1=''a · b'' = 1}} in this case is {{nowrap|1=''b'' = 1/2}}, which is a rational number, but not an integer. Hence not every element of &lt;math&gt;\mathbb{Z}&lt;/math&gt; has a (multiplicative) inverse.{{cref|k}}

==== Rationals ====
The desire for the existence of multiplicative inverses suggests considering [[fraction (mathematics)|fractions]]
:&lt;math alt="a/b"&gt;\frac{a}{b}.&lt;/math&gt;
Fractions of integers (with ''b'' nonzero) are known as [[rational number]]s.{{cref|l}} The set of all such irreducible fractions is commonly denoted &lt;math&gt;\mathbb{Q}&lt;/math&gt;. There is still a minor obstacle for &lt;math&gt;\left(\mathbb{Q},\cdot\right)&lt;/math&gt;, the rationals with multiplication, being a group: because the rational number [[0 (number)|0]] does not have a multiplicative inverse (i.e., there is no ''x'' such that {{nowrap|1=''x'' · 0 = 1}}), &lt;math&gt;\left(\mathbb{Q},\cdot\right)&lt;/math&gt; is still not a group.

However, the set of all ''nonzero'' rational numbers &lt;math&gt;\mathbb{Q}\setminus\left\{0\right\}=\left\{q\in\mathbb{Q}\mid q\neq 0\right\}&lt;/math&gt; does form an abelian group under multiplication, generally denoted &lt;math&gt;\mathbb{Q}^*&lt;/math&gt;.{{cref|m}} Associativity and identity element axioms follow from the properties of integers. The closure requirement still holds true after removing zero, because the product of two nonzero rationals is never zero. Finally, the inverse of ''a''/''b'' is ''b''/''a'', therefore the axiom of the inverse element is satisfied.

The rational numbers (including 0) also form a group under addition. Intertwining addition and multiplication operations yields more complicated structures called [[ring (mathematics)|rings]] and—if division is possible, such as in &lt;math&gt;\mathbb{Q}&lt;/math&gt;—[[field (mathematics)|fields]], which occupy a central position in [[abstract algebra]]. Group theoretic arguments therefore underlie parts of the theory of those entities.{{cref|n}}

===Modular arithmetic===

[[File:Clock group.svg|thumb|right|The hours on a clock form a group that uses [[Modular arithmetic|addition modulo]] 12. Here 9 + 4 = 1.]]
In [[modular arithmetic]], two integers are added and then the sum is divided by a positive integer called the ''modulus.'' The result of modular addition is the [[remainder]] of that division. For any modulus, ''n'', the set of integers from 0 to {{nowrap|''n'' − 1}} forms a group under modular addition: the inverse of any element ''a'' is {{nowrap|''n'' − ''a''}}, and 0 is the identity element. This is familiar from the addition of hours on the face of a [[12-hour clock|clock]]: if the hour hand is on 9 and is advanced 4 hours, it ends up on 1, as shown at the right. This is expressed by saying that 9 + 4 equals 1 "modulo 12" or, in symbols,
:9 + 4 ≡ 1 modulo 12.
The group of integers modulo ''n'' is written &lt;math&gt;\mathbb{Z}_{n}&lt;/math&gt; or &lt;math&gt;\mathbb{Z}/n\mathbb{Z}&lt;/math&gt;.

For any [[prime number]] ''p'', there is also the [[multiplicative group of integers modulo n|multiplicative group of integers modulo ''p'']].&lt;ref&gt;{{Harvard citations|nb = yes|last = Lang|year = 2005|loc = Chapter VII}}&lt;/ref&gt; Its elements are the integers 1 to {{nowrap|''p'' − 1}}. The group operation is multiplication modulo ''p''. That is, the usual product is divided by ''p'' and the remainder of this division is the result of modular multiplication. For example, if {{nowrap|1=''p'' = 5}}, there are four group elements 1, 2, 3, 4. In this group, {{nowrap|1=4 · 4 = 1}}, because the usual product 16 is equivalent to 1, which divided by 5 yields a remainder of 1.  for 5 divides {{nowrap|1=16 − 1 = 15}}, denoted
:16 ≡ 1 (mod 5).
The primality of ''p'' ensures that the product of two integers neither of which is divisible by ''p'' is not divisible by ''p'' either, hence the indicated set of classes is closed under multiplication.{{cref|o}} The identity element is 1, as usual for a multiplicative group, and the associativity follows from the corresponding property of integers. Finally, the inverse element axiom requires that given an integer ''a'' not divisible by ''p'', there exists an integer ''b'' such that
:''a''&amp;nbsp;·&amp;nbsp;''b''&amp;nbsp;≡&amp;nbsp;1 (mod ''p''), i.e., ''p'' divides the difference {{nowrap|''a'' · ''b'' − 1}}.
The inverse ''b'' can be found by using [[Bézout's identity]] and the fact that the [[greatest common divisor]] {{nowrap|gcd(''a'', ''p'')}} equals 1.&lt;ref&gt;{{Harvard citations|nb = yes|last = Rosen|year = 2000|loc = p. 54 (Theorem 2.1)}}&lt;/ref&gt; In the case {{nowrap|1=''p'' = 5}} above, the inverse of 4 is 4, and the inverse of 3 is 2, as {{nowrap|1=3 · 2 = 6 ≡ 1 (mod 5)}}. Hence all group axioms are fulfilled. Actually, this example is similar to &lt;math&gt;\left(\mathbb{Q}\setminus\left\{0\right\},\cdot\right)&lt;/math&gt; above: it consists of exactly those elements in &lt;math&gt;\mathbb{Z}/p\mathbb{Z}&lt;/math&gt; that have a multiplicative inverse.&lt;ref&gt;{{Harvard citations|nb = yes|last = Lang|year = 2005|loc = §VIII.1, p. 292}}&lt;/ref&gt; These groups are denoted '''F'''&lt;sub&gt;''p''&lt;/sub&gt;&lt;sup&gt;×&lt;/sup&gt;. They are crucial to [[public-key cryptography]].{{cref|p}}

===Cyclic groups===
{{Main|Cyclic group}}
[[Image:Cyclic group.svg|right|thumb|upright|The 6th complex roots of unity form a cyclic group. ''z'' is a primitive element, but ''z''&lt;sup&gt;2&lt;/sup&gt; is not, because the odd powers of ''z'' are not a power of ''z''&lt;sup&gt;2&lt;/sup&gt;.]]
A ''cyclic group'' is a group all of whose elements are [[power (mathematics)|powers]] of a particular element ''a''.&lt;ref&gt;{{Harvard citations|nb = yes|last = Lang|year = 2005|loc = §II.1, p. 22}}&lt;/ref&gt; In multiplicative notation, the elements of the group are:
:..., ''a''&lt;sup&gt;−3&lt;/sup&gt;, ''a''&lt;sup&gt;−2&lt;/sup&gt;, ''a''&lt;sup&gt;−1&lt;/sup&gt;, ''a''&lt;sup&gt;0&lt;/sup&gt; = ''e'', ''a'', ''a''&lt;sup&gt;2&lt;/sup&gt;, ''a''&lt;sup&gt;3&lt;/sup&gt;, ...,
where ''a''&lt;sup&gt;2&lt;/sup&gt; means ''a'' • ''a'', and ''a&lt;sup&gt;−3&lt;/sup&gt;'' stands for ''a''&lt;sup&gt;−1&lt;/sup&gt; • ''a''&lt;sup&gt;−1&lt;/sup&gt; • ''a''&lt;sup&gt;−1&lt;/sup&gt; = (''a'' • ''a'' • ''a'')&lt;sup&gt;−1&lt;/sup&gt; etc.{{cref|h}} Such an element ''a'' is called a generator or a [[Primitive root modulo n|primitive element]] of the group. In additive notation, the requirement for an element to be primitive is that each element of the group can be written as
: ..., −''a''−''a'', −''a'', 0, ''a'', ''a''+''a'', ...

In the groups '''Z'''/''n'''''Z''' introduced above, the element 1 is primitive, so these groups are cyclic. Indeed, each element is expressible as a sum all of whose terms are 1.  Any cyclic group with ''n'' elements is isomorphic to this group. A second example for cyclic groups is the group of [[root of unity|''n''-th complex roots of unity]], given by [[complex number]]s ''z'' satisfying {{nowrap|1=''z''&lt;sup&gt;''n''&lt;/sup&gt; = 1}}. These numbers can be visualized as the vertices on a regular ''n''-gon, as shown in blue at the right for {{nowrap|1=''n'' = 6}}. The group operation is multiplication of complex numbers. In the picture, multiplying with ''z'' corresponds to a [[clockwise|counter-clockwise]] rotation by 60°.&lt;ref&gt;{{Harvard citations|nb = yes|last = Lang|year = 2005|loc = §II.2, p. 26}}&lt;/ref&gt; Using some [[field theory (mathematics)|field theory]], the group '''F'''&lt;sub&gt;''p''&lt;/sub&gt;&lt;sup&gt;×&lt;/sup&gt; can be shown to be cyclic: for example, if {{nowrap|1=''p'' = 5}}, 3 is a generator since {{nowrap|1=3&lt;sup&gt;1&lt;/sup&gt; = 3}}, {{nowrap|1=3&lt;sup&gt;2&lt;/sup&gt; = 9 ≡ 4}}, {{nowrap|3&lt;sup&gt;3&lt;/sup&gt; ≡ 2}}, and {{nowrap|3&lt;sup&gt;4&lt;/sup&gt; ≡ 1}}.

Some cyclic groups have an infinite number of elements.  In these groups, for every non-zero element ''a'', all the powers of ''a'' are distinct; despite the name "cyclic group", the powers of the elements do not cycle. An infinite cyclic group is isomorphic to {{nowrap|('''Z''', +)}}, the group of integers under addition introduced above.&lt;ref&gt;{{Harvard citations|nb = yes|last = Lang|year = 2005|loc = §II.1, p. 22 (example 11)}}&lt;/ref&gt; As these two prototypes are both abelian, so is any cyclic group.

The study of finitely generated abelian groups is quite mature, including the [[fundamental theorem of finitely generated abelian groups]]; and reflecting this state of affairs, many group-related notions, such as [[Center (group theory)|center]] and [[commutator]], describe the extent to which a given group is not abelian.&lt;ref&gt;{{Harvard citations|nb = yes|last = Lang|year = 2002|loc = §I.5, p. 26, 29}}&lt;/ref&gt;

===Symmetry groups===
{{Main|Symmetry group}}
{{see also|Molecular symmetry|Space group|Symmetry in physics}}
''Symmetry groups'' are groups consisting of [[symmetry|symmetries]] of given mathematical objects—be they of geometric nature, such as the introductory symmetry group of the square, or of algebraic nature, such as [[polynomial equation]]s and their solutions.&lt;ref&gt;{{Harvard citations|nb = yes|last = Weyl|year = 1952}}&lt;/ref&gt; Conceptually, group theory can be thought of as the study of symmetry.{{cref|t}} [[Symmetry in mathematics|Symmetries in mathematics]] greatly simplify the study of [[geometry|geometrical]] or [[analysis|analytical objects]]. A group is said to [[group action|act]] on another mathematical object ''X'' if every group element performs some operation on ''X'' compatibly to the group law. In the rightmost example below, an element of order 7 of the [[(2,3,7) triangle group]] acts on the tiling by permuting the highlighted warped triangles (and the other ones, too). By a group action, the group pattern is connected to the structure of the object being acted on.

[[Image:Sixteenth stellation of icosahedron.png|right|thumb|200px|Rotations and reflections form the symmetry group of a [[great icosahedron]].]]
In chemical fields, such as [[crystallography]], [[space group]]s and [[point group]]s describe [[molecular symmetry|molecular symmetries]] and crystal symmetries. These symmetries underlie the chemical and physical behavior of these systems, and group theory enables simplification of [[quantum mechanics|quantum mechanical]] analysis of these properties.&lt;ref&gt;{{Harvard citations|nb = yes|last1 = Conway|last2= Delgado Friedrichs|last3 = Huson|last4 = Thurston|year = 2001}}. See also {{Harvard citations|nb = yes|last = Bishop|year = 1993}}&lt;/ref&gt; For example, group theory is used to show that optical transitions between certain quantum levels cannot occur simply because of the symmetry of the states involved.

Not only are groups useful to assess the implications of symmetries in molecules, but surprisingly they also predict that molecules sometimes can change symmetry. The [[Jahn-Teller effect]] is a distortion of a molecule of high symmetry when it adopts a particular ground state of lower symmetry from a set of possible ground states that are related to each other by the symmetry operations of the molecule.&lt;ref name=Bersuker&gt;{{citation |title=The Jahn-Teller Effect |first= Isaac |last= Bersuker |page=2 |isbn=0-521-82212-2 |publisher=Cambridge University Press |year=2006  }}&lt;/ref&gt;&lt;ref&gt;{{Harvard citations|nb = yes|last1 = Jahn|last2=Teller|year = 1937}}&lt;/ref&gt;

Likewise, group theory helps predict the changes in physical properties that occur when a material undergoes a [[phase transition]], for example, from a cubic to a tetrahedral crystalline form. An example is [[ferroelectric]] materials, where the change from a paraelectric to a ferroelectric state occurs at the [[Curie temperature]] and is related to a change from the high-symmetry paraelectric state to the lower symmetry ferroelectric state, accompanied by a so-called soft [[phonon]] mode, a vibrational lattice mode that goes to zero frequency at the transition.&lt;ref name=Dove&gt;{{citation |title=Structure and Dynamics: an atomic view of materials |first=Martin T|last= Dove |page=265 |isbn=0-19-850678-3 |publisher=Oxford University Press |year=2003  }}&lt;/ref&gt;

Such [[spontaneous symmetry breaking]] has found further application in elementary particle physics, where its occurrence is related to the appearance of [[Goldstone boson]]s.

{| class="wikitable" style="text-align:center; margin:1em auto 1em auto;"
|-
|width=15%| [[Image:C60a.png|125px]]
|width=20%| [[Image:Ammonia-3D-balls-A.png|125px]]
|width=14%| [[Image:Cubane-3D-balls.png|125px]]
|width=36%| [[Image:Hexaaquacopper(II)-3D-balls.png|125px]]
|width=15%| [[Image:Uniform tiling 73-t2 colored.png|125px]]
|-
| [[Buckminsterfullerene]] displays&lt;br /&gt;[[icosahedral symmetry]], though the double bonds reduce this to [[pyritohedral symmetry]].
|[[Ammonia]], [[nitrogen|N]][[hydrogen|H&lt;sub&gt;3&lt;/sub&gt;]]. Its symmetry group is of order 6, generated by a 120° rotation and a reflection.
|[[Cubane]] [[Carbon|C&lt;sub&gt;8&lt;/sub&gt;]][[Hydrogen|H&lt;sub&gt;8&lt;/sub&gt;]] features&lt;br /&gt; [[octahedral symmetry]].
|Hexaaquacopper(II) [[complex ion]], [[copper|[Cu]][[oxygen|(O]]H&lt;sub&gt;2&lt;/sub&gt;)&lt;sub&gt;6&lt;/sub&gt;]&lt;sup&gt;2+&lt;/sup&gt;. Compared to a perfectly symmetrical shape, the molecule is vertically dilated by about 22% (Jahn-Teller effect).
|The (2,3,7) triangle group, a hyperbolic group, acts on this [[Tessellation|tiling]] of the [[hyperbolic geometry|hyperbolic]] plane.
|}

Finite symmetry groups such as the [[Mathieu group]]s are used in [[coding theory]], which is in turn applied in [[forward error correction|error correction]] of transmitted data, and in [[CD player]]s.&lt;ref&gt;{{Harvard citations|nb = yes|last = Welsh|year = 1989}}&lt;/ref&gt; Another application is [[differential Galois theory]], which characterizes functions having [[antiderivative]]s of a prescribed form, giving group-theoretic criteria for when solutions of certain [[differential equation]]s are well-behaved.{{cref|u}} Geometric properties that remain stable under group actions are investigated in [[geometric invariant theory|(geometric)]] [[invariant theory]].&lt;ref&gt;{{Harvard citations|nb = yes|last1 = Mumford|last2 = Fogarty|last3 = Kirwan|year = 1994}}&lt;/ref&gt;

===General linear group and representation theory===
{{Main|General linear group|Representation theory}}
[[Image:Matrix multiplication.svg|right|thumb|250px|Two [[vector (mathematics)|vectors]] (the left illustration) multiplied by matrices (the middle and right illustrations). The middle illustration represents a clockwise rotation by 90°, while the right-most one stretches the ''x''-coordinate by factor 2.]]
[[Matrix group]]s consist of [[Matrix (mathematics)|matrices]] together with [[matrix multiplication]]. The ''general linear group'' {{nowrap|GL(''n'', '''R''')}} consists of all [[invertible matrix|invertible]] ''n''-by-''n'' matrices with [[real number|real]] entries.&lt;ref&gt;{{Harvard citations|nb = yes|last = Lay|year = 2003}}&lt;/ref&gt; Its subgroups are referred to as ''matrix groups'' or ''[[linear group]]s''. The dihedral group example mentioned above can be viewed as a (very small) matrix group. Another important matrix group is the [[special orthogonal group]] SO(''n''). It describes all possible rotations in ''n'' dimensions. Via [[Euler angles]], [[Rotation matrix|rotation matrices]] are used in [[computer graphics]].&lt;ref&gt;{{Harvard citations|nb = yes|last = Kuipers|year = 1999}}&lt;/ref&gt;

''Representation theory'' is both an application of the group concept and important for a deeper understanding of groups.&lt;ref name=FultonHarris&gt;{{Harvard citations|last1 = Fulton|last2 = Harris|year = 1991|nb = yes}}&lt;/ref&gt;&lt;ref&gt;{{Harvard citations|nb = yes|last = Serre|year = 1977}}&lt;/ref&gt; It studies the group by its [[group action]]s on other spaces. A broad class of [[group representation]]s are linear representations, i.e., the group is acting on a [[vector space]], such as the three-dimensional [[Euclidean space]] '''R'''&lt;sup&gt;3&lt;/sup&gt;. A representation of ''G'' on an ''n''-[[dimension]]al real vector space is simply a group homomorphism
:''ρ'': ''G'' → GL(''n'', '''R''')
from the group to the general linear group. This way, the group operation, which may be abstractly given, translates to the multiplication of matrices making it accessible to explicit computations.{{cref|w}}

Given a group action, this gives further means to study the object being acted on.{{cref|x}} On the other hand, it also yields information about the group. Group representations are an organizing principle in the theory of finite groups, Lie groups, [[algebraic group]]s and [[topological group]]s, especially (locally) [[compact group]]s.&lt;ref name="FultonHarris"/&gt;&lt;ref&gt;{{Harvard citations|nb = yes|last = Rudin|year = 1990}}&lt;/ref&gt;

=== Galois groups ===
{{Main|Galois group}}
''Galois groups'' were developed to help solve [[polynomial equation]]s by capturing their symmetry features.&lt;ref&gt;{{Harvard citations|nb = yes|last = Robinson|year = 1996|loc = p. viii}}&lt;/ref&gt;&lt;ref&gt;{{Harvard citations|nb = yes|last = Artin|year = 1998}}&lt;/ref&gt; For example, the solutions of the [[quadratic equation]] {{nowrap|1=''ax''&lt;sup&gt;2&lt;/sup&gt; + ''bx'' + ''c'' = 0}} are given by
:&lt;math alt="x = (negative b plus or minus the squareroot of (b squared minus 4 a c)) over 2a"&gt;x = \frac{-b \pm \sqrt {b^2-4ac}}{2a}.&lt;/math&gt;
Exchanging "+" and "−" in the expression, i.e., permuting the two solutions of the equation can be viewed as a (very simple) group operation. Similar formulae are known for [[cubic equation|cubic]] and [[quartic equation]]s, but do ''not'' exist in general for [[quintic equation|degree 5]] and higher.&lt;ref&gt;{{Harvard citations|nb = yes|last = Lang|year = 2002|loc = Chapter VI}} (see in particular p. 273 for concrete examples)&lt;/ref&gt; Abstract properties of Galois groups associated with polynomials (in particular their [[solvable group|solvability]]) give a criterion for polynomials that have all their solutions expressible by radicals, i.e., solutions expressible using solely addition, multiplication, and [[Nth root|roots]] similar to the formula above.&lt;ref&gt;{{Harvard citations|nb = yes|last = Lang|year = 2002|loc = p. 292 (Theorem VI.7.2)}}&lt;/ref&gt;

The problem can be dealt with by shifting to [[field theory (mathematics)|field theory]] and considering the [[splitting field]] of a polynomial.  Modern [[Galois theory]] generalizes the above type of Galois groups to [[field extension]]s and establishes—via the [[fundamental theorem of Galois theory]]—a precise relationship between fields and groups, underlining once again the ubiquity of groups in mathematics.

== Finite groups ==
{{Main|Finite group}}
A group is called ''finite'' if it has a [[finite set|finite number of elements]]. The number of elements is called the [[order (group theory)|order]] of the group.&lt;ref&gt;{{Harvard citations|nb = yes|last1 = Kurzweil|last2= Stellmacher|year = 2004}}&lt;/ref&gt;  An important class is the ''[[symmetric group]]s'' S&lt;sub&gt;''N''&lt;/sub&gt;, the groups of [[permutation]]s of ''N'' letters. For example, the symmetric group on 3 letters [[dihedral group of order 6|S&lt;sub&gt;3&lt;/sub&gt;]] is the group consisting of all possible orderings of the three letters ''ABC'', i.e., contains the elements ''ABC'', ''ACB'', ''BAC'', ''BCA'', ''CAB'', ''CBA'', in total 6 ([[factorial]] of 3) elements. This class is fundamental insofar as any finite group can be expressed as a subgroup of a symmetric group S&lt;sub&gt;''N''&lt;/sub&gt; for a suitable integer ''N'', according to [[Cayley's theorem]]. Parallel to the group of symmetries of the square above, S&lt;sub&gt;3&lt;/sub&gt; can also be interpreted as the group of symmetries of an [[equilateral triangle]].

The order of an element ''a'' in a group ''G'' is the least positive integer ''n'' such that ''a''&lt;sup&gt;''n''&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;''e'', where ''a''&lt;sup&gt;''n''&lt;/sup&gt; represents

: &lt;math&gt;\underbrace{a \cdots a}_{n \text{ factors}},&lt;/math&gt;

i.e., application of the operation • to ''n'' copies of ''a''. (If • represents multiplication, then ''a''&lt;sup&gt;''n''&lt;/sup&gt; corresponds to the ''n''th power of ''a''.) In infinite groups, such an ''n'' may not exist, in which case the order of ''a'' is said to be infinity. The order of an element equals the order of the cyclic subgroup generated by this element.

More sophisticated counting techniques, for example counting cosets, yield more precise statements about finite groups: [[Lagrange's theorem (group theory)|Lagrange's Theorem]] states that for a finite group ''G'' the order of any finite subgroup ''H'' [[divisor|divides]] the order of ''G''. The [[Sylow theorems]] give a partial converse.

The [[dihedral group]] (discussed above) is a finite group of order 8. The order of r&lt;sub&gt;1&lt;/sub&gt; is 4, as is the order of the subgroup ''R'' it generates (see above). The order of the reflection elements f&lt;sub&gt;v&lt;/sub&gt; etc. is 2. Both orders divide 8, as predicted by Lagrange's theorem. The groups '''F'''&lt;sub&gt;''p''&lt;/sub&gt;&lt;sup&gt;×&lt;/sup&gt; above have order {{nowrap|''p'' − 1}}.

=== Classification of finite simple groups ===
{{Main|Classification of finite simple groups}}
Mathematicians often strive for a complete [[classification theorems|classification]] (or list) of a mathematical notion. In the context of finite groups, this aim leads to difficult mathematics. According to Lagrange's theorem, finite groups of order ''p'', a prime number, are necessarily cyclic (abelian) groups '''Z'''&lt;sub&gt;''p''&lt;/sub&gt;. Groups of order ''p''&lt;sup&gt;2&lt;/sup&gt; can also be shown to be abelian, a statement which does not generalize to order ''p''&lt;sup&gt;3&lt;/sup&gt;, as the non-abelian group D&lt;sub&gt;4&lt;/sub&gt; of order 8 = 2&lt;sup&gt;3&lt;/sup&gt; above shows.&lt;ref&gt;{{Harvard citations|nb = yes|last = Artin|year = 1991|loc = Theorem 6.1.14}}. See also {{Harvard citations|nb = yes|last = Lang|year = 2002|loc = p. 77}} for similar results.&lt;/ref&gt; [[Computer algebra system]]s can be used to [[List of small groups|list small groups]], but there is no classification of all finite groups.{{cref|q}} An intermediate step is the classification of finite simple groups.{{cref|r}} A nontrivial group is called ''[[simple group|simple]]'' if its only normal subgroups are the [[trivial group]] and the group itself.{{cref|s}} The [[Jordan–Hölder theorem]] exhibits finite simple groups as the building blocks for all finite groups.&lt;ref&gt;{{Harvard citations|nb = yes|last = Lang|year = 2002|loc = §I. 3, p. 22}}&lt;/ref&gt; [[List of finite simple groups|Listing all finite simple groups]] was a major achievement in contemporary group theory. 1998 [[Fields Medal]] winner [[Richard Borcherds]] succeeded in proving the [[monstrous moonshine]] conjectures, a surprising and deep relation between the largest finite simple [[sporadic group]]—the "[[monster group]]"—and certain [[modular function]]s, a piece of classical [[complex analysis]], and [[string theory]], a theory supposed to unify the description of many physical phenomena.&lt;ref&gt;{{Harvard citations|nb = yes|last = Ronan|year = 2007}}&lt;/ref&gt;

==Groups with additional structure==
Many groups are simultaneously groups and examples of other mathematical structures. In the language of [[category theory]], they are [[group object]]s in a [[category (mathematics)|category]], meaning that they are objects (that is, examples of another mathematical structure) which come with transformations (called [[morphism]]s) that mimic the group axioms. For example, every group (as defined above) is also a set, so a group is a group object in the [[category of sets]].

===Topological groups===
[[Image:Circle as Lie group2.svg|right|thumb|The [[unit circle]] in the [[complex plane]] under complex multiplication is a Lie group and, therefore, a topological group. It is topological since complex multiplication and division are continuous. It is a manifold and thus a Lie group, because every [[Neighbourhood (mathematics)|small piece]], such as the red arc in the figure, looks like a part of the [[real line]] (shown at the bottom). ]]
{{Main|Topological group}}
Some [[topological space]]s may be endowed with a group law. In order for the group law and the topology to interweave well, the group operations must be [[continuous function]]s, that is, {{nowrap|''g'' • ''h''}}, and ''g''&lt;sup&gt;−1&lt;/sup&gt; must not vary wildly if ''g'' and ''h'' vary only little. Such groups are called ''topological groups,'' and they are the group objects in the [[category of topological spaces]].&lt;ref&gt;{{Harvard citations|nb = yes| last = Husain|year = 1966}}&lt;/ref&gt; The most basic examples are the [[real number|reals]] '''R''' under addition, {{nowrap|('''R''' ∖ {0}, ·)}}, and similarly with any other [[topological field]] such as the [[complex number]]s or [[p-adic number|''p''-adic numbers]]. All of these groups are [[locally compact topological group|locally compact]], so they have [[Haar measure]]s and can be studied via [[harmonic analysis]]. The former offer an abstract formalism of invariant [[integral]]s. [[Invariant (mathematics)|Invariance]] means, in the case of real numbers for example:
: &lt;math&gt;\int_{\mathbb{R}} f(x)\,dx = \int_{\mathbb{R}} f(x+c)\,dx&lt;/math&gt;
for any constant ''c''. Matrix groups over these fields fall under this regime, as do [[adele ring]]s and [[adelic algebraic group]]s, which are basic to number theory.&lt;ref&gt;{{Harvard citations|nb = yes| last = Neukirch|year = 1999}}&lt;/ref&gt; Galois groups of infinite field extensions such as the [[absolute Galois group]] can also be equipped with a topology, the so-called [[Krull topology]], which in turn is central to generalize the above sketched connection of fields and groups to infinite field extensions.&lt;ref&gt;{{Harvard citations|nb = yes| last = Shatz|year = 1972}}&lt;/ref&gt; An advanced generalization of this idea, adapted to the needs of [[algebraic geometry]], is the [[étale fundamental group]].&lt;ref&gt;{{Harvard citations|nb = yes| last = Milne|year = 1980}}&lt;/ref&gt;

===Lie groups===
{{Main|Lie group}}
''Lie groups'' (in honor of [[Sophus Lie]]) are groups which also have a [[manifold]] structure, i.e., they are spaces [[diffeomorphism|looking locally like]] some [[Euclidean space]] of the appropriate [[dimension]].&lt;ref&gt;{{Harvard citations|nb = yes|last = Warner|year = 1983}}&lt;/ref&gt; Again, the additional structure, here the manifold structure, has to be compatible, i.e., the maps corresponding to multiplication and the inverse have to be [[smooth map|smooth]].

A standard example is the general linear group introduced above: it is an [[open subset]] of the space of all ''n''-by-''n'' matrices, because it is given by the inequality
:det (''A'') ≠ 0,
where ''A'' denotes an ''n''-by-''n'' matrix.&lt;ref&gt;{{Harvard citations|nb = yes|last = Borel|year = 1991}}&lt;/ref&gt;

Lie groups are of fundamental importance in modern physics: [[Noether's theorem]] links continuous symmetries to [[conserved quantities]].&lt;ref&gt;{{Harvard citations|nb = yes|last = Goldstein|year = 1980}}&lt;/ref&gt; [[Rotation]], as well as [[translation (geometry)|translations]] in [[space]] and [[time]] are basic symmetries of the laws of [[mechanics]]. They can, for instance, be used to construct simple models—imposing, say, axial symmetry on a situation will typically lead to significant simplification in the equations one needs to solve to provide a physical description.{{cref|v}} Another example are the [[Lorentz transformation]]s, which relate measurements of time and velocity of two observers in motion relative to each other. They can be deduced in a purely group-theoretical way, by expressing the transformations as a rotational symmetry of [[Minkowski space]]. The latter serves—in the absence of significant [[gravitation]]—as a model of [[space time]] in [[special relativity]].&lt;ref&gt;{{harvard citations|nb=yes|last=Weinberg|year=1972}}&lt;/ref&gt; The full symmetry group of Minkowski space, i.e., including translations, is known as the [[Poincaré group]]. By the above, it plays a pivotal role in special relativity and, by implication, for [[quantum field theories]].&lt;ref&gt;{{Harvard citations|nb = yes|last = Naber|year = 2003}}&lt;/ref&gt; [[Local symmetry|Symmetries that vary with location]] are central to the modern description of physical interactions with the help of [[gauge theory]].&lt;ref&gt;{{Harvard citations|nb = yes| last = Becchi|year = 1997}}&lt;/ref&gt;

==Generalizations==
{{Group-like structures}}
In [[abstract algebra]], more general structures are defined by relaxing some of the axioms defining a group.&lt;ref name="MacLane"&gt;{{Harvard citations|nb = yes|last = Mac Lane|year = 1998}}&lt;/ref&gt;&lt;ref&gt;{{Harvard citations|nb = yes|last1 = Denecke|last2 = Wismath|year = 2002}}&lt;/ref&gt;&lt;ref&gt;{{Harvard citations|nb = yes|last1 = Romanowska|last2 = Smith|year=2002}}&lt;/ref&gt; For example, if the requirement that every element has an inverse is eliminated, the resulting algebraic structure is called a [[monoid]]. The [[natural number]]s '''N''' (including 0) under addition form a monoid, as do the nonzero integers under multiplication {{nowrap|('''Z''' ∖ {0}, ·)}}, see above. There is a general method to formally add inverses to elements to any (abelian) monoid, much the same way as {{nowrap|('''Q''' ∖ {0}, ·)}} is derived from {{nowrap|('''Z''' ∖ {0}, ·)}}, known as the [[Grothendieck group]].
[[Groupoid]]s are similar to groups except that the composition {{nowrap|''a'' • ''b''}} need not be defined for all ''a'' and ''b''. They arise in the study of more complicated forms of symmetry, often in [[topology|topological]] and [[mathematical analysis|analytical]] structures, such as the [[fundamental groupoid]] or [[stack (mathematics)|stacks]]. Finally, it is possible to generalize any of these concepts by replacing the binary operation with an arbitrary [[arity|''n''-ary]] one (i.e., an operation taking ''n'' arguments). With the proper generalization of the group axioms this gives rise to an [[n-ary group|''n''-ary group]].&lt;ref&gt;{{Harvard citations|nb = yes|last = Dudek|year = 2001}}&lt;/ref&gt; The table gives a list of several structures generalizing groups.
{{clear}}

==See also==
* [[List of group theory topics]]

==Notes==
{{clear}}
{{Div col|colwidth=25em}}&lt;span class="small"&gt;
{{cnote|a|[[Mathematical Reviews]] lists 3,224 research papers on group theory and its generalizations written in 2005.}}
{{cnote|aa|The classification was announced in 1983, but gaps were found in the proof.  See [[classification of finite simple groups]] for further information.}}
{{cnote|b|The closure axiom is already implied by the condition that • be a binary operation. Some authors therefore omit this axiom. However, group constructions often start with an operation defined on a superset, so a closure step is common in proofs that a system is a group. {{Harvard citations|nb = yes|last = Lang|year = 2002}}}}
{{cnote|c|See, for example, the books of Lang (2002, 2005) and Herstein (1996, 1975).}}
{{cnote|d|However, a group is not determined by its lattice of subgroups. See {{Harvard citations|nb = yes|last = Suzuki|year = 1951}}.}}
{{cnote|e|The fact that the group operation extends this [[canonical form|canonically]] is an instance of a [[universal property]].}}
{{cnote|f|For example, if ''G'' is finite, then the [[order of a group|size]] of any subgroup and any quotient group divides the size of ''G'', according to Lagrange's theorem.}}
{{cnote|g|The word homomorphism derives from [[Ancient Greek|Greek]] ὁμός—the same and [[wikt:μορφή|μορφή]]—structure.}}
{{cnote|h|The additive notation for elements of a cyclic group would be {{nowrap|''t'' • ''a'', ''t'' in '''Z'''}}.}}
{{cnote|i|See the [[Seifert–van Kampen theorem]] for an example.}}
{{cnote|j|An example is [[group cohomology]] of a group which equals the [[singular cohomology]] of its [[classifying space]].}}
{{cnote|k|Elements which do have multiplicative inverses are called [[unit (ring theory)|units]], see {{Harvard citations|nb = yes|last = Lang|year = 2002|loc =§II.1, p. 84}}.}}
{{cnote|l|The transition from the integers to the rationals by adding fractions is generalized by the [[field of fractions]].}}
{{cnote|m|The same is true for any [[field (mathematics)|field]] ''F'' instead of '''Q'''. See {{Harvard citations|nb = yes|last = Lang|year = 2005|loc = §III.1, p. 86}}.}}
{{cnote|n|For example, a finite subgroup of the multiplicative group of a field is necessarily cyclic. See {{Harvard citations|nb = yes|last = Lang|year = 2002|loc = Theorem IV.1.9}}. The notions of [[Torsion (algebra)|torsion]] of a [[module (mathematics)|module]] and [[simple algebra]]s are other instances of this principle.}} {{cnote|o|The stated property is a possible definition of prime numbers. See [[prime element]].}}
{{cnote|p|For example, the [[Diffie-Hellman]] protocol uses the [[discrete logarithm]].}}
{{cnote|q|The groups of order at most 2000 are known. [[Up to]] isomorphism, there are about 49 billion. See {{Harvard citations|nb = yes|last1 = Besche|last2 = Eick|last3 = O'Brien|year = 2001}}.}}
{{cnote|r|The gap between the classification of simple groups and the one of all groups lies in the [[extension problem]], a problem too hard to be solved in general. See {{Harvard citations|nb = yes|last = Aschbacher|year = 2004|loc = p. 737}}.}}
{{cnote|s|Equivalently, a nontrivial group is simple if its only quotient groups are the trivial group and the group itself. See {{Harvard citations|nb = yes|last = Michler|year = 2006}}, {{Harvard citations|nb = yes|last = Carter|year = 1989}}.}}
{{cnote|t|More rigorously, every group is the symmetry group of some [[graph (discrete mathematics)|graph]]; see [[Frucht's theorem]], {{Harvard citations|nb = yes|last = Frucht|year = 1939}}.}}
{{cnote|u|More precisely, the [[monodromy]] action on the [[vector space]] of solutions of the differential equations is considered. See {{Harvard citations|nb = yes|last = Kuga|year=1993|loc = pp. 105–113}}.}}
{{cnote|v|See [[Schwarzschild metric]] for an example where symmetry greatly reduces the complexity of physical systems.}}
{{cnote|w|This was crucial to the classification of finite simple groups, for example. See {{Harvard citations|nb = yes|last = Aschbacher|year = 2004}}.}}
{{cnote|x|See, for example, [[Schur's Lemma]] for the impact of a group action on [[simple module]]s. A more involved example is the action of an [[absolute Galois group]] on [[étale cohomology]].}}
{{cnote|y|Injective and surjective maps correspond to [[monomorphism|mono-]] and [[epimorphism]]s, respectively. They are interchanged when passing to the [[dual category]].}}
&lt;/span&gt;
{{Div col end}}

==Citations==
{{Reflist|35em}}&lt;span class="small"&gt;

==References==

===General references===
* {{Citation
  | last1=Artin
  | first1=Michael
  | authorlink1=Michael Artin
  | title=Algebra
  | publisher=[[Prentice Hall]]
  | isbn=978-0-89871-510-1
  | year=1991
 }}, Chapter 2 contains an undergraduate-level exposition of the notions covered in this article.
* {{Citation | last1=Devlin | first1=Keith | authorlink1=Keith Devlin | title=The Language of Mathematics: Making the Invisible Visible | publisher=Owl Books | isbn=978-0-8050-7254-9 | year=2000}}, Chapter 5 provides a layman-accessible explanation of groups.
* {{Citation | authorlink=George G. Hall | last=Hall | first=G. G. | title=Applied group theory | publisher=American Elsevier Publishing Co., Inc., New York | mr=0219593 | year=1967}}, an elementary introduction.
* {{Citation | last1=Herstein | first1=Israel Nathan |authorlink1 = Israel Nathan Herstein | title=Abstract algebra | publisher=Prentice Hall Inc. | location=Upper Saddle River, NJ | edition=3rd | isbn=978-0-13-374562-7 | mr=1375019 | year=1996}}.
* {{Citation | last1=Herstein | first1=Israel Nathan | title=Topics in algebra | publisher=Xerox College Publishing | location=Lexington, Mass. | edition=2nd | mr=0356988 | year=1975}}.
* {{Lang Algebra}}&lt;!-- Don't add a fullstop here: it breaks the layout! --&gt;
* {{Citation | last1=Lang | first1=Serge | title=Undergraduate Algebra | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=3rd | isbn=978-0-387-22025-3 | year=2005}}.
* {{Citation | last1=Ledermann | first1=Walter | title=Introduction to the theory of finite groups | publisher=Oliver and Boyd, Edinburgh and London | mr=0054593 | year=1953}}.
* {{Citation | last1=Ledermann | first1=Walter | title=Introduction to group theory | publisher=Barnes and Noble | location=New York | oclc=795613 | year=1973}}.
* {{Citation | last1=Robinson | first1=Derek John Scott | title=A course in the theory of groups | publisher=Springer-Verlag | location=Berlin, New York | isbn=978-0-387-94461-6 | year=1996}}.

===Special references===
* {{Citation | last1=Artin | first1=Emil | author1-link=Emil Artin | title=Galois Theory | publisher=[[Dover Publications]] | location=New York | isbn=978-0-486-62342-9 | year=1998}}.
* {{Citation | last1=Aschbacher | first1=Michael | author1-link = Michael Aschbacher | title=The Status of the Classification of the Finite Simple Groups | url=http://www.ams.org/notices/200407/fea-aschbacher.pdf |format=PDF| year=2004 | journal=[[Notices of the American Mathematical Society]]   | volume=51 | issue=7 | pages=736–740}}.
* {{Citation| last = Becchi| first = C.|arxiv=hep-ph/9705211|title=Introduction to Gauge Theories|year = 1997|bibcode = 1997hep.ph....5211B| page = 5211 }}.
* {{Citation | last1=Besche | first1=Hans Ulrich | last2=Eick | first2=Bettina | last3=O'Brien | first3=E. A. | title=The groups of order at most 2000 | url=http://www.ams.org/era/2001-07-01/S1079-6762-01-00087-7/home.html | mr=1826989 | year=2001 | journal=Electronic Research Announcements of the American Mathematical Society | volume=7 | pages=1–4 | doi=10.1090/S1079-6762-01-00087-7}}.
* {{Citation | last1=Bishop | first1=David H. L. | title=Group theory and chemistry | publisher=Dover Publications | location=New York | isbn=978-0-486-67355-4 | year=1993}}.
* {{Citation | last1=Borel | first1=Armand | author1-link=Armand Borel | title=Linear algebraic groups | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | series=Graduate Texts in Mathematics | isbn=978-0-387-97370-8 | mr=1102012 | year=1991 | volume=126}}.
* {{Citation | last1=Carter | first1=Roger W. | author1-link=Roger Carter (mathematician) | title=Simple groups of Lie type | publisher=[[John Wiley &amp; Sons]] | location=New York | isbn=978-0-471-50683-6 | year=1989}}.
* {{Citation | last1=Conway | first1=John Horton | author1-link=John Horton Conway | last2=Delgado Friedrichs | first2=Olaf | last3=Huson | first3=Daniel H. | last4=Thurston | first4=William P. | author4-link=William Thurston | title=On three-dimensional space groups | arxiv=math.MG/9911185 | mr=1865535 | year=2001 | journal=Beiträge zur Algebra und Geometrie   | volume=42 | issue=2 | pages=475–507}}.
* {{Citation | last1=Coornaert | first1=M. | last2=Delzant | first2=T. | last3=Papadopoulos | first3=A. | title=Géométrie et théorie des groupes [Geometry and Group Theory]| publisher=Springer-Verlag | location=Berlin, New York | series=Lecture Notes in Mathematics | isbn=978-3-540-52977-4 | mr=1075994 | year=1990 | volume=1441|language=fr}}.
* {{Citation | last1=Denecke | first1=Klaus | last2=Wismath | first2=Shelly L. | title=Universal algebra and applications in theoretical computer science | publisher=[[CRC Press]] | location=London | isbn=978-1-58488-254-1 | year=2002}}.
* {{Citation| last=Dudek |first=W.A. |title=On some old problems in n-ary groups |journal=Quasigroups and Related Systems |year=2001 |volume=8 |pages= 15–36}}.
* {{Citation | authorlink=R. Frucht | last1=Frucht | first1=R. | title=Herstellung von Graphen mit vorgegebener abstrakter Gruppe [Construction of Graphs with Prescribed Group] | url=http://www.numdam.org/numdam-bin/fitem?id=CM_1939__6__239_0 | year=1939 | journal=Compositio Mathematica | volume=6 | pages=239–50 | language=de | deadurl=yes | archiveurl=https://web.archive.org/web/20081201083831/http://www.numdam.org/numdam-bin/fitem?id=CM_1939__6__239_0 | archivedate=2008-12-01 | df= }}.
* {{Citation| last1 = Fulton| first1 = William| author1-link = William Fulton (mathematician)| last2 = Harris| first2 = Joe| author2-link = Joe Harris (mathematician)| year         = 1991| title = Representation theory. A first course| publisher = Springer-Verlag| location = New York| series = [[Graduate Texts in Mathematics]], Readings in Mathematics| volume = 129| isbn  = 978-0-387-97495-8| mr =  1153249}}
* {{Citation| last = Goldstein | first = Herbert | author-link = Herbert Goldstein | year = 1980 | title = [[Classical Mechanics (textbook)|Classical Mechanics]] | edition = 2nd | publisher = Addison-Wesley Publishing | location = Reading, MA | isbn = 0-201-02918-9 | pages = 588–596}}.
* {{Citation | last1=Hatcher | first1=Allen | author-link=Allen Hatcher | title=Algebraic topology | url=http://www.math.cornell.edu/~hatcher/AT/ATpage.html | publisher=[[Cambridge University Press]] | isbn=978-0-521-79540-1 | year=2002}}.
* {{Citation | last1=Husain | first1=Taqdir | title=Introduction to Topological Groups | publisher=W.B. Saunders Company | location=Philadelphia | isbn=978-0-89874-193-3 | year=1966}}
* {{Citation | last1 = Jahn | first1=H.| author1-link=Hermann Arthur Jahn|last2=Teller|first2=E.|author2-link=Edward Teller| title = Stability of Polyatomic Molecules in Degenerate Electronic States. I. Orbital Degeneracy | year = 1937 | journal = [[Proceedings of the Royal Society A]] | volume = 161 | issue = 905 | pages = 220–235 | doi = 10.1098/rspa.1937.0142 | bibcode=1937RSPSA.161..220J}}.
* {{Citation | last1=Kuipers | first1=Jack B. | title=Quaternions and rotation sequences—A primer with applications to orbits, aerospace, and virtual reality | publisher=[[Princeton University Press]] | isbn=978-0-691-05872-6 | mr=1670862 | year=1999}}.
* {{Citation | last1=Kuga | first1=Michio | author-link=Michio Kuga | title=Galois' dream: group theory and differential equations | publisher=Birkhäuser Boston | location=Boston, MA | isbn=978-0-8176-3688-3 | mr=1199112 | year=1993}}.
* {{Citation | last1=Kurzweil | first1=Hans | last2=Stellmacher | first2=Bernd | title=The theory of finite groups | publisher=Springer-Verlag | location=Berlin, New York | series=Universitext | isbn=978-0-387-40510-0 | mr=2014408 | year=2004}}.
* {{Citation | last1=Lay | first1=David | title=Linear Algebra and Its Applications | publisher=[[Addison-Wesley]] | isbn=978-0-201-70970-4 | year=2003}}.
* {{Citation | last1=Mac Lane | first1=Saunders | author1-link=Saunders Mac Lane | title=[[Categories for the Working Mathematician]] | publisher=Springer-Verlag | location=Berlin, New York | edition=2nd | isbn=978-0-387-98403-2 | year=1998}}.
* {{Citation | last1=Michler | first1=Gerhard | title=Theory of finite simple groups | publisher=Cambridge University Press | isbn=978-0-521-86625-5 | year=2006}}.
* {{Citation | last1=Milne | first1=James S. | title=Étale cohomology | publisher=Princeton University Press | isbn=978-0-691-08238-7 | year=1980}}
* {{Citation | last1=Mumford | first1=David | author1-link=David Mumford | last2=Fogarty | first2=J. | last3=Kirwan | first3=F. | title=Geometric invariant theory | publisher=Springer-Verlag | location=Berlin, New York | edition=3rd | isbn=978-3-540-56963-3 | mr=1304906 | year=1994 | volume=34}}.
* {{Citation | last1=Naber | first1=Gregory L. | title=The geometry of Minkowski spacetime | publisher=Dover Publications | location=New York | isbn=978-0-486-43235-9 | mr=2044239 | year=2003}}.
* {{Citation | last = Neukirch| first = Jürgen| author-link = Jürgen Neukirch| title = Algebraic Number Theory| publisher = Springer-Verlag| location = Berlin| series = {{lang|de|Grundlehren der mathematischen Wissenschaften}}| isbn = 978-3-540-65399-8| mr = 1697859| zbl = 0956.11021 | year = 1999| volume = 322}}
* {{Citation | last1=Romanowska | first1=A.B. | last2=Smith | first2=J.D.H. | title=Modes | publisher=[[World Scientific]] | isbn=978-981-02-4942-7 | year=2002}}.
* {{Citation | last1=Ronan | first1=Mark | author1-link= Mark Ronan|title=Symmetry and the Monster: The Story of One of the Greatest Quests of Mathematics | publisher=[[Oxford University Press]] | isbn=978-0-19-280723-6 | year=2007}}.
* {{Citation | last1=Rosen | first1=Kenneth H. | title=Elementary number theory and its applications | publisher=Addison-Wesley | edition=4th | isbn=978-0-201-87073-2 | mr=1739433 | year=2000}}.
* {{Citation| last = Rudin | first = Walter | author-link = Walter Rudin | title = Fourier Analysis on Groups|publisher=Wiley-Blackwell|series=Wiley Classics|year=1990|isbn=0-471-52364-X}}.
* {{Citation | last1=Seress | first1=Ákos | title=An introduction to computational group theory | url=http://www.ams.org/notices/199706/seress.pdf | mr=1452069 | year=1997 | journal=Notices of the American Mathematical Society   | volume=44 | issue=6 | pages=671–679}}.
* {{Citation | last1=Serre | first1=Jean-Pierre | author1-link=Jean-Pierre Serre | title=Linear representations of finite groups | publisher=Springer-Verlag | location=Berlin, New York | isbn=978-0-387-90190-9 | mr=0450380 | year=1977}}.
* {{Citation | last1=Shatz | first1=Stephen S. | title=Profinite groups, arithmetic, and geometry | publisher=Princeton University Press | isbn=978-0-691-08017-8 | mr=0347778 | year=1972}}
* {{Citation|last = Suzuki|first= Michio|author-link = Michio Suzuki|title = On the lattice of subgroups of finite groups|journal = [[Transactions of the American Mathematical Society]]| volume = 70| issue = 2| year = 1951| pages = 345–371| doi = 10.2307/1990375|jstor = 1990375}}.
* {{Citation | last1=Warner | first1=Frank | title=Foundations of Differentiable Manifolds and Lie Groups | publisher=Springer-Verlag | location=Berlin, New York | isbn=978-0-387-90894-6 | year=1983}}.
* {{Citation | last1=Weinberg | first1=Steven | author1-link=Steven Weinberg | title=Gravitation and Cosmology | publisher=John Wiley &amp; Sons | location=New York | year=1972 |isbn=0-471-92567-5}}.
* {{Citation | last1=Welsh | first1=Dominic | title=Codes and cryptography | publisher=Clarendon Press | location=Oxford | isbn=978-0-19-853287-3 | year=1989}}.
* {{Citation | last1=Weyl | first1=Hermann | author1-link=Hermann Weyl | title=Symmetry | publisher=Princeton University Press | isbn=978-0-691-02374-8 | year=1952}}.

===Historical references===
{{See also|List of publications in mathematics#Group theory|l1=Historically important publications in group theory}}
* {{Citation | last1=Borel | first1=Armand | author1-link=Armand Borel | title=Essays in the History of Lie Groups and Algebraic Groups | publisher=[[American Mathematical Society]] | location=Providence, R.I. | isbn=978-0-8218-0288-5 | year=2001}}
* {{Citation | last1=Cayley | first1=Arthur | author1-link=Arthur Cayley | title=The collected mathematical papers of Arthur Cayley | url=http://www.hti.umich.edu/cgi/t/text/pageviewer-idx?c=umhistmath;cc=umhistmath;rgn=full%20text;idno=ABS3153.0001.001;didno=ABS3153.0001.001;view=image;seq=00000140 | publisher=[[Cambridge University Press]] | year=1889 | volume=II (1851–1860)}}.
* {{MacTutor | id=Development_group_theory | class=HistTopics | title = The development of group theory}}
* {{Citation | last1=Curtis | first1=Charles W. | authorlink = Charles W. Curtis | title=Pioneers of Representation Theory: Frobenius, Burnside, Schur, and Brauer | publisher=American Mathematical Society | location=Providence, R.I. | series=History of Mathematics | isbn=978-0-8218-2677-5 | year=2003}}.
* {{Citation | last1=von Dyck | year=1882 | first1=Walther | author1-link=Walther von Dyck | title=Gruppentheoretische Studien (Group-theoretical Studies) | doi=10.1007/BF01443322 | journal=[[Mathematische Annalen]] | volume=20 | issue=1 | pages=1–44 | url=http://gdz.sub.uni-goettingen.de/index.php?id=11&amp;PPN=PPN235181684_0020&amp;DMDID=DMDLOG_0007&amp;L=1 | language=de | deadurl=yes | archiveurl=https://web.archive.org/web/20140222213905/http://gdz.sub.uni-goettingen.de/index.php?id=11&amp;PPN=PPN235181684_0020&amp;DMDID=DMDLOG_0007&amp;L=1 | archivedate=2014-02-22 | df= }}.
* {{Citation | last1=Galois | first1=Évariste | author1-link=Évariste Galois | editor1-last=Tannery | editor1-first=Jules | title=Manuscrits de Évariste Galois [Évariste Galois' Manuscripts] | url=http://quod.lib.umich.edu/cgi/t/text/text-idx?c=umhistmath;idno=AAN9280 | publisher=Gauthier-Villars | location=Paris | year=1908|language=fr}} (Galois work was first published by [[Joseph Liouville]] in 1843).
* {{Citation | last1=Jordan | first1=Camille | author-link=Camille Jordan | title=Traité des substitutions et des équations algébriques [Study of Substitutions and Algebraic Equations] | url=https://archive.org/details/traitdessubstit00jordgoog | publisher=Gauthier-Villars | location=Paris | year=1870 |language=fr}}.
* {{Citation | doi=10.2307/2690312 | last1=Kleiner | first1=Israel | authorlink=Israel Kleiner (mathematician) | title=The Evolution of Group Theory: A Brief Survey | mr=863090 | year=1986 | journal=[[Mathematics Magazine]]   | volume=59 | issue=4 | pages=195–215 | jstor=2690312 }}.
* {{Citation | last1=Lie | first1=Sophus | author1-link=Sophus Lie | title=Gesammelte Abhandlungen. Band 1 [Collected papers. Volume 1] | publisher=Johnson Reprint Corp. | location=New York | mr=0392459 | year=1973|language=de}}.
* {{Citation | last1=Mackey | first1=George Whitelaw | author1-link=George Mackey | title=The theory of unitary group representations | publisher=[[University of Chicago Press]] | mr=0396826 | year=1976}}
* {{Citation | last1=Smith | first1=David Eugene | author1-link=David Eugene Smith | title=History of Modern Mathematics | url=http://www.gutenberg.org/etext/8746 | series=Mathematical Monographs, No. 1 | year=1906}}.
* {{Citation | last1=Wussing | first1=Hans | authorlink=Hans Wussing | title=The Genesis of the Abstract Group Concept: A Contribution to the History of the Origin of Abstract Group Theory | publisher=[[Dover Publications]] | location=New York | isbn=978-0-486-45868-7 | year=2007}}.
{{Authority control}}
{{Group navbox}}
{{Algebra}}
{{featured article}}

{{DEFAULTSORT:Group (Mathematics)}}
[[Category:Algebraic structures]]
[[Category:Group theory|*]]
[[Category:Symmetry]]</text>
      <sha1>gf9zj57y7opgjfjyqsi2n75qczc2z23</sha1>
    </revision>
  </page>
  <page>
    <title>Hausdorff density</title>
    <ns>0</ns>
    <id>11014228</id>
    <revision>
      <id>757851589</id>
      <parentid>711840689</parentid>
      <timestamp>2017-01-02T02:48:03Z</timestamp>
      <contributor>
        <username>Alan Chang</username>
        <id>49265</id>
      </contributor>
      <comment>link to David Preiss</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2865">In [[measure theory]], a field of mathematics, the '''Hausdorff density''' measures how concentrated a [[Radon measure]] is at some point.

== Definition ==

Let &lt;math&gt;\mu&lt;/math&gt; be a Radon measure and &lt;math&gt;a\in\mathbb{R}^{n}&lt;/math&gt; some point in [[Euclidean space]]. The ''s''-dimensional upper and lower Hausdorff densities are defined to be, respectively, 
:&lt;math&gt; \Theta^{*s}(\mu,a)=\limsup_{r\rightarrow 0}\frac{\mu(B_{r}(a))}{r^{s}}&lt;/math&gt;
and
:&lt;math&gt; \Theta_{*}^{s}(\mu,a)=\liminf_{r\rightarrow 0}\frac{\mu(B_{r}(a))}{r^{s}}&lt;/math&gt;
where &lt;math&gt; B_{r}(a)&lt;/math&gt; is the [[ball (mathematics)|ball]] of radius ''r'' &gt; 0 centered at ''a''. Clearly, &lt;math&gt;\Theta_{*}^{s}(\mu,a)\leq \Theta^{*s}(\mu,a)&lt;/math&gt; for all &lt;math&gt;a\in\mathbb{R}^{n}&lt;/math&gt;. In the event that the two are equal, we call their common value the '''s-density''' of &lt;math&gt;\mu&lt;/math&gt; at ''a'' and denote it &lt;math&gt;\Theta^{s}(\mu,a)&lt;/math&gt;.

== Marstrand's theorem ==

The following theorem states that the times when the ''s''-density exists are rather seldom.

: '''Marstrand's theorem:''' Let &lt;math&gt;\mu&lt;/math&gt; be a Radon measure on &lt;math&gt;\mathbb{R}^{d}&lt;/math&gt;. Suppose that the ''s''-density &lt;math&gt;\Theta^{s}(\mu,a)&lt;/math&gt; exists and is positive and finite for ''a'' in a set of positive &lt;math&gt;\mu&lt;/math&gt; measure. Then ''s'' is an integer.

== Preiss' theorem ==

In 1987 [[David Preiss]] proved a stronger version of Marstrand's theorem. One consequence is that sets with positive and finite density are [[rectifiable set]]s.

: '''Preiss' theorem:''' Let &lt;math&gt;\mu&lt;/math&gt; be a Radon measure on &lt;math&gt;\mathbb{R}^{d}&lt;/math&gt;. Suppose that ''m''&lt;math&gt;\geq 1&lt;/math&gt; is an integer and the ''m''-density &lt;math&gt;\Theta^{m}(\mu,a)&lt;/math&gt; exists and is positive and finite for &lt;math&gt;\mu&lt;/math&gt; almost every ''a'' in the [[Support (measure theory)|support]] of &lt;math&gt;\mu&lt;/math&gt;. Then &lt;math&gt;\mu&lt;/math&gt; is ''m''-rectifiable, i.e. &lt;math&gt;\mu\ll H^{m}&lt;/math&gt; (&lt;math&gt;\mu&lt;/math&gt; is [[absolutely continuous]] with respect to [[Hausdorff measure]] &lt;math&gt;H^m&lt;/math&gt;) and the support of &lt;math&gt;\mu&lt;/math&gt; is an ''m''-rectifiable set.

==External links==
* [http://www.encyclopediaofmath.org/index.php/Density_of_a_set Density of a set] at [http://www.encyclopediaofmath.org/ Encyclopedia of Mathematics]
* [http://www.encyclopediaofmath.org/index.php/Rectifiable_set Rectifiable set] at [http://www.encyclopediaofmath.org/ Encyclopedia of Mathematics]

== References ==
* [[Pertti Mattila]], ''Geometry of sets and measures in Euclidean spaces.'' Cambridge Press, 1995.
* {{cite journal
  | last = Preiss | first = David
  | author-link = David Preiss
  | title = Geometry of measures in &lt;math&gt;R^n&lt;/math&gt;: distribution, rectifiability, and densities
  | jstor = 1971410
  | journal = Ann. Math. | volume = 125
  | issue = 3 | pages = 537&amp;ndash;643 | year = 1987 | doi=10.2307/1971410
}}

[[Category:Measure theory]]</text>
      <sha1>ijjpba0uy9ewhu89g8ar5eaeeh1zpw0</sha1>
    </revision>
  </page>
  <page>
    <title>Hotelling's lemma</title>
    <ns>0</ns>
    <id>3394448</id>
    <revision>
      <id>871377757</id>
      <parentid>871375697</parentid>
      <timestamp>2018-11-30T17:17:33Z</timestamp>
      <contributor>
        <username>Gagarine</username>
        <id>11797131</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4326">{{Technical|date=September 2010}}

'''Hotelling's lemma''' is a result in [[microeconomic]]s that relates the supply of a good to the profit of the good's producer.  It was first shown by [[Harold Hotelling]], and is widely used in the [[theory of the firm]].

The [[Lemma (mathematics)|lemma]] can be stated as: ''The change in profits from a change in price is equal to the quantity produced.''

:&lt;math&gt;\frac {\partial \pi (p)}{\partial p} = y(p)&lt;/math&gt;

''For &lt;math&gt;\pi&lt;/math&gt; the profit function of the firm n terms of the good's price p and &lt;math&gt;y&lt;/math&gt;the production function in terms of the good's price p, assuming that &lt;math&gt;p &gt; 0&lt;/math&gt; and that derivative exists.''

==Proof For Hotelling's Lemma==
The proof of the theorem stems from the fact that for a profit-maximizing firm an under duality, the maximum of the firm's profit at some output &lt;math&gt;y^*(p)&lt;/math&gt; is given by the minimum of  &lt;math&gt; \pi (p^*) - p^* y^*(p) &lt;/math&gt;  (cost) at some price, &lt;math&gt;p^*&lt;/math&gt;, namely where &lt;math&gt;\frac {\partial \pi (p)}{\partial p} - y = 0&lt;/math&gt; holds.  Thus, &lt;math&gt;y(p)= \frac {\partial \pi (p)}{\partial p}&lt;/math&gt;; QED.

The proof is also a corollary of the [[envelope theorem]].

==Application of Hotelling's lemma==

This is not an application of Hotelling's lemma, it's the definition of partial derivatives.

Let the firm's profit function be:
:&lt;math&gt;\pi=py- wx&lt;/math&gt;

where:
&lt;ul&gt;
&amp;#9679;&lt;math&gt;\pi&lt;/math&gt; is profit.&lt;br&gt;&amp;#9679;&lt;math&gt;p&lt;/math&gt; is the price of output &lt;math&gt;y&lt;/math&gt;.&lt;br&gt;&amp;#9679;&lt;math&gt;y&lt;/math&gt; is output.&lt;br&gt;&amp;#9679;&lt;math&gt;w&lt;/math&gt; is input price for input &lt;math&gt;x&lt;/math&gt;&lt;br&gt;&amp;#9679;&lt;math&gt;x&lt;/math&gt; is the single input needed for producing &lt;math&gt;y&lt;/math&gt;
&lt;/ul&gt;

If a firm produces 10 units of &lt;math&gt;y&lt;/math&gt; using 5 units of input &lt;math&gt;x&lt;/math&gt; which cost 1 dollar each and sells each output for 2 dollars. the profit the firm makes is:

:&lt;math&gt;\pi_1=(2)(10)-(1)(5)=15&lt;/math&gt;

If the firm increases the price of the output to 3 dollars and still sells the same amount of &lt;math&gt;y&lt;/math&gt;, the firm's profits are now:
:&lt;math&gt;\pi_2=(3)(10)-(1)(5)=25&lt;/math&gt;

Taking the difference between &lt;math&gt;\pi_1&lt;/math&gt; and &lt;math&gt;\pi_2&lt;/math&gt;
:&lt;math&gt;\Delta \pi=\pi_2-\pi_1=25-15=10&lt;/math&gt;

The change in profits from a change in price is 10, which is exactly the same as the output produced. thus the statement of &lt;math&gt;y(p)= \frac {\partial \pi (p)}{\partial p}&lt;/math&gt; holds.

== Criticisms and empirical evidence ==
A number of criticisms have been made with regards to the use and application of Hotelling's lemma in empirical work.

C.Robert Taylor points out that the accuracy of Hotelling's lemma is dependent on the firm maximizing profits, meaning that it is producing  profit maximizing output &lt;math&gt;y^*&lt;/math&gt; and cost minimizing input &lt;math&gt;x^*&lt;/math&gt;. If a firm is not producing at these optimums, then Hotelling's lemma would not hold.&lt;ref&gt;{{Cite journal|last=Taylor|first=C. Robert|date=1989|title=Duality, Optimization, and Microeconomic Theory: Pitfalls for the Applied Researcher|url=http://www.jstor.org/stable/40988099|journal=Western Journal of Agricultural Economics|volume=14|issue=2|pages=200–212}}&lt;/ref&gt;

==See also==
*[[Hotelling's law]]
*[[Hotelling's rule]]
*[[Supply and demand]]
*[[Shephard's lemma]]

==References==
{{Reflist}}
* {{cite journal |last=Hotelling |first=H. |year=1932 |title=Edgeworth's taxation paradox and the nature of demand and supply functions |journal=[[Journal of Political Economy]] |volume=40 |issue=5 |pages=577–616 |doi= 10.1086/254387|jstor=1822600 }}
* {{cite journal |last=Sakai |first=Y. |authorlink=Yasuhiro Sakai |title=Substitution and Expansion Effects in Production Theory: The Case of Joint Production |journal=[[Journal of Economic Theory]] |volume=9 |issue=3 |year=1974 |pages=255–274 |doi=10.1016/0022-0531(74)90051-9 }}
* {{cite book |last=Takayama |first=A. |authorlink=Akira Takayama (economist) |title=Mathematical Economics |location=New York |publisher=Cambridge University Press |year=1985 |isbn=0-521-31498-4 |pages=141–144 |url=https://books.google.com/books?id=j6PLOBFotPQC&amp;pg=PA141 }}
* {{cite book |last=Varian |first=H. |authorlink=Hal Varian |year=1992 |title=Microeconomic Analysis |edition=3rd |location=New York |publisher=W. W Norton |isbn=0-393-95735-7 |pages=43–45 |url= }}

[[Category:Lemmas]]


{{microeconomics-stub}}</text>
      <sha1>7ctnogki7v8dp7jdiez5shfdfgqqnzj</sha1>
    </revision>
  </page>
  <page>
    <title>Hypothetical syllogism</title>
    <ns>0</ns>
    <id>177633</id>
    <revision>
      <id>868856990</id>
      <parentid>858180403</parentid>
      <timestamp>2018-11-14T22:07:03Z</timestamp>
      <contributor>
        <username>Texvc2LaTeXBot</username>
        <id>33995001</id>
      </contributor>
      <minor/>
      <comment>Replacing deprecated latex syntax [[mw:Extension:Math/Roadmap]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2820">{{Transformation rules}}
In [[classical logic]], '''hypothetical syllogism''' is a [[Validity (logic)|valid]] [[logical form|argument form]] which is a [[syllogism]] having a [[Material conditional|conditional statement]] for one or both of its [[premise]]s.

An example in [[English language|English]]:
:If I do not wake up, then I cannot go to work.
:If I cannot go to work, then I will not get paid.
:Therefore, if I do not wake up, then I will not get paid.

The term originated with [[Theophrastus]].&lt;ref&gt;[http://www.britannica.com/EBchecked/topic/346217/history-of-logic/65923/Theophrastus-of-Eresus "History of Logic: Theophrastus of Eresus"] in [[Encyclopædia Britannica Online]].&lt;/ref&gt;

==Propositional logic==
In [[propositional logic]], '''hypothetical syllogism''' is the name of a valid [[rule of inference]] (often abbreviated '''HS''' and sometimes also called the '''chain argument''', '''chain rule''', or the principle of '''transitivity of implication'''). Hypothetical syllogism is one of the rules in [[classical logic]] that is not always accepted in certain [[formal system|systems]] of [[non-classical logic]].{{Example needed}} The rule may be stated:

:&lt;math&gt;\frac{P \to Q, Q \to R}{\therefore P \to R}&lt;/math&gt;

where the rule is that whenever instances of "&lt;math&gt;P \to Q&lt;/math&gt;", and "&lt;math&gt;Q \to R&lt;/math&gt;" appear on lines of a [[formal proof|proof]], "&lt;math&gt;P \to R&lt;/math&gt;" can be placed on a subsequent line.

Hypothetical syllogism is closely related and similar to [[disjunctive syllogism]], in that it is also type of syllogism, and also the name of a rule of inference.



== Formal notation ==
The ''hypothetical syllogism'' inference rule may be written in [[sequent]] notation, which amounts to a specialization of the cut rule:

: &lt;math&gt;\frac{P \vdash Q\quad Q \vdash R}{P \vdash R}&lt;/math&gt;

where &lt;math&gt;\vdash&lt;/math&gt; is a [[metalogic]]al symbol and &lt;math&gt;A \vdash B&lt;/math&gt; meaning that &lt;math&gt;B&lt;/math&gt; is a [[logical consequence|syntactic consequence]] of &lt;math&gt;A&lt;/math&gt; in some [[formal system|logical system]];

and expressed as a truth-functional [[tautology (logic)|tautology]] or [[theorem]] of [[propositional calculus|propositional logic]]:

:&lt;math&gt;((P \to Q) \land (Q \to R)) \to (P \to R)&lt;/math&gt;

where &lt;math&gt;P&lt;/math&gt;, &lt;math&gt;Q&lt;/math&gt;, and &lt;math&gt;R&lt;/math&gt; are propositions expressed in some [[formal system]].

==See also==
*[[Modus ponens]]
*[[Modus tollens]]
*[[Affirming the consequent]]
*[[Denying the antecedent]]
*[[Transitive relation]]

==References==
{{Reflist}}

== External links ==
*[http://www.philosophy-index.com/logic/forms/hypothetical-syllogism.php Philosophy Index: Hypothetical Syllogism]

{{DEFAULTSORT:Hypothetical Syllogism}}
[[Category:Rules of inference]]
[[Category:Theorems in propositional logic]]
[[Category:Classical logic]]
[[Category:Syllogism]]</text>
      <sha1>jxkgmyq3v7pt4xcx278q2d0bz7ilek3</sha1>
    </revision>
  </page>
  <page>
    <title>Intermediate Math League of Eastern Massachusetts</title>
    <ns>0</ns>
    <id>18305757</id>
    <revision>
      <id>825350909</id>
      <parentid>813329771</parentid>
      <timestamp>2018-02-12T22:17:24Z</timestamp>
      <contributor>
        <username>Nick Moyes</username>
        <id>11378440</id>
      </contributor>
      <minor/>
      <comment>/* Schools */clean up, [[WP:AWB/T|typo(s) fixed]]: offically → officially using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13512">The '''Intermediate Math League of Eastern Massachusetts''' (or '''IMLEM''') is a math league for middle schools across Eastern Massachusetts. A brief history of IMLEM is given in its By-Laws:&lt;ref name="BYLAW"&gt;{{cite web | url=http://www.imlem.org/mathleague/By-Laws-2003-09.htm | title=IMLEM By-Laws, March 2003}}&lt;/ref&gt;

{{Quotation | The first contest of the Intermediate Math League was held in March, 1965. This meeting was viewed as an experiment to determine the advisability of this type of academic competition for the age levels of students in grades 7 through 9. Enthusiasm and commitment to the contest idea spread quickly. Throughout the years, the League has grown in membership from ten to forty three schools. Students have the honor of representing their school in an academic competition, while interacting academically and socially with students from other communities. Advisors have the opportunity to discuss and compare their programs in mathematics. In 1986, the academic competition was redesigned to accommodate the regional change from junior high schools to middle schools. Teams then consisted of students in grades 7 and 8. Sixth grade students were allowed to participate as seventh graders.}}

==Schools==
As of 2017, 86 different schools attend the competition. Each school is allowed to send more than 1 team and each team can consist of at most 10 people. Alternates, people who are not officially part of team, can be taken too. There are a total of 15 different geographic clusters of schools and there is even a cluster of schools from Pennsylvania. The schools are then separated into different divisions with the schools in each division be approximately the same level. Schools can then make their way up through divisions to try to get into the top division, which is the Lexington Division. In total there are 13 divisions. 
 
Schools may send more than one team, however no student can compete on more than one team in a year. Also, a school may send alternates to gain the experience of a meet.&lt;ref name="BYLAW" /&gt;

==Meets==
IMLEM has five meets every year, one in October, November, January, February, and March. For the first three meets, no calculators or external aids of any sort are allowed for any round. However, for the last two meets, calculators without programming or graphing capabilities, and without a QWERTY keyboard, are allowed for all rounds.&lt;ref name="BYLAW" /&gt;

Meets are held at distinct locations for each of the ten geographic clusters. Schools within the clusters generally hold at most one of the meets. Each of the meets are generally held on the same day by all clusters; however for scheduling conflicts, schools may host meets on other days.

There are five individual categories, and they are: Mystery, Geometry, Number Theory, Arithmetic, and Algebra. Each individual round contains three questions, varying in content but focusing on topics published by the test writer. Questions in individual rounds are worth 2 points each. (Before this was the case, questions would be worth the question number, i.e., one point for question one, two points for question two and three points for question three.) Students are given 10 minutes to complete the round, along with a 1-minute warning.&lt;ref name="BYLAW" /&gt;

The sixth category is a 15-minute team round that consists of six or nine questions (The amount of questions is unknown before the round starts). The entire team collaborates to solve each of the questions. The questions are usually based on topics from the five individual rounds with some extra knowledge required to solve other questions.

Students take three individual categories, and no more than six students on a team may take a single category. In a round, the maximum score for an individual is 6, and the maximum score for a team is 36. It follows that the maximum individual score is 18, and the maximum team score is 216.&lt;ref name="BYLAW" /&gt;

18s are not uncommon, and each meet sees many individuals who get 18s. Coming into the 2004-05 school year, a score above 200 had been achieved only five times: twice by Diamond Middle School, twice by Clarke Middle School, and the long-standing record of 205 set by Marblehead in 1983. (This was when individual rounds were weighted 1-2-3 instead of 2-2-2.) The first meet of 2004-05 saw two teams score 200: Clarke scored 200, and Diamond shattered the record by scoring 212. Diamond scored perfect 36s in each of the rounds except for round 3 (number theory), in which 2 students each got a question wrong.

The 2007-08 season saw Clarke Middle School shatter many records, recent and not alike. Firstly, Clarke scored 1006 for the year, which is an ''average'' of 201.2 per meet.&lt;ref&gt;{{cite web | url=http://sch.ci.lexington.ma.us/~jfrost/mathteam | title=Jonas Clarke Math Team}}&lt;/ref&gt; This broke the previous record set by Diamond back in the 2004-05 year. Secondly, they outscored second place Diamond by 122 points, also breaking a Diamond record, this one set in 2005-06. This is in part an effect of the test-writers' push to make problems easier so as to encourage more participation.&lt;ref&gt;{{cite web | url=http://www.imlem.org/minutes/index.htm | title=IMLEM Minutes}}&lt;/ref&gt;

Each of the meets follows the following basic format. Each of the teams arrive usually by 3:15, and after snacks, the five individual rounds are held in succession. After that, a fifteen-minute team round is conducted. For all rounds, alternates and regulars are split. Finally, awards are distributed and teams should depart around 5:30.

==Topics==
{| class="wikitable" style=width:80em
! colspan=6 | &lt;span style="font-size:125%;"&gt;IMLEM Topics (As of 2007-08)&lt;ref&gt;{{cite web | url=http://www.imlem.org/topics-2007-08.htm | title=Categories or Topics for Meets, 2007-2008}}&lt;/ref&gt;&lt;/span&gt;
|-
! rowspan=2 | Meet
! Cat 1
! Cat 2
! Cat 3
! Cat 4
! Cat 5
|-
! Mystery
! Geometry
! Number Theory
! Arithmetic
! Algebra
|-
| 1
| ?
| Angle measures in plane figures including supplements and complements
| Divisibility rules, factors, primes, composites
| Order of operations; mean, median, mode; rounding; statistics
| Simplifying and evaluating expressions; solving equations with 1 unknown including identities
|-
| 2
| ?
| Area and perimeter of polygons
| GCF, LCM, prime factorization
| Fractions, terminating and repeating decimals, percents
| Word problems with 1 unknown; working with formulas; reasoning in number sentences
|-
| 3
| ?
| Properties of polygons; Pythagorean Theorem
| Bases, scientific notation
| Integral powers (positive, negative, and zero), roots up to the sixth
| Absolute value; inequalities in one variable including interpreting line graphs
|-
| 4
| ?
| Properties of circles
| Modular arithmetic, series and sequences
| Percent applications:
* find percent of a number,
* find what percent a number is of another,
* find a number where the percent of that number is known,
* find percent of change,
* compound interest
| Word problems (linear, including direct proportions or systems)
|-
| 5
| ?
| Solid geometry (volume and surface area)
| Set theory, Venn diagrams
| Combinatorics and Probability
| Solving quadratics with rational solutions, including word problems
|}

==Divisions==
{| class="wikitable" style=width:70em
! colspan=5 | &lt;span style="font-size:125%;"&gt;IMLEM Divisions (As of 2013-14)&lt;ref&gt;{{cite web | url=http://imlem.org/Divisions.htm | title=IMLEM Scoring Divisions 2013-2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://imlem.org/Geographic_Clusters.htm | title=IMLEM Geographic Clusters 2013-2014}}&lt;/ref&gt;&lt;/span&gt;
|-
! Division !! School !! Team !! Cluster !! Championships (INCOMPLETE)†
|-
! rowspan="2" |[[Lexington, Massachusetts|Lexington]]
|[[Lexington Public Schools (Massachusetts)#Jonas Clarke Middle School|Lexington - Clarke]]
!
|8
!
|-
|[[Lexington Public Schools (Massachusetts)#William Diamond Middle School|Lexington - Diamond]]
!
|8
!
|-
! rowspan="5" |[[Euclid]]ean
| [[Weston, Massachusetts#Education|Meadowbrook - Weston]] ||  || 3 || 
|-
| colspan="2" | [[Winchester, Massachusetts#Public schools|Winchester McCall]] || 8 || 
|-
| colspan="2" |[[Newton, Massachusetts#Primary and secondary education|Newton - Brown]]
|7
!
|-
| colspan="2" |[[Carlisle, Massachusetts|Carlisle Middle School]]
|6
!
|-
| colspan="2" |[[Westford, Massachusetts#Education|Westford Stony Brook]]
|6
!
|-
! rowspan="5" |[[Pythagoras|Pythagorean]]
| [[Westborough, Massachusetts#Public schools|Westborough - Gibbons]] || 1 and 2 || 9 || 
|-
|[[Belmont Public Schools|Belmont - Chenery]]
!
|8
!
|-
|[[Sharon, Massachusetts#Education|Sharon]]
|1 and 2
|10
!
|-
|[[Weston, Massachusetts#Education|Weston]]
!
|3
!
|-
|[[Advanced Math and Science Academy Charter School|AMSA Charter]]
!
|9
!
|-
! rowspan="5" style="background-color:#CCFFFF" | [[Blaise Pascal|Pascal]]
| colspan=2 | [[Andover, Massachusetts#Public schools|Andover - Doherty]] ||  || 
|-
|[[Andover, Massachusetts#Public schools|Andover - West]]
!
|6
!
|-
|[[Newton, Massachusetts#Primary and secondary education|Newton - Bigelow]]
!
!
!
|-
|[[Worcester Academy]]
!
|9
!
|-
|[[Newton, Massachusetts#Primary and secondary education|Newton - Oak Hill]]
!
|7
!
|-
! rowspan="6" |[[Fibonacci]]
|[[Sudbury Public Schools|Sudbury Curtis MS]]
|1 and 2
|11
!
|-
|[[Wayland Middle School|Wayland]]
!
|3
!
|-
|[[Reading, Massachusetts#Education|Reading - Parker]]
!
|4
!
|-
|[[Lincoln, Massachusetts|Lincoln]]
!
|8
!
|-
|[[Bb&amp;n|BB&amp;N]]
!
|8
!
|-
|[[Jewish Community Day School]]
!
|7
!
|-
! rowspan="6" |[[Da Vinci]]
|[[Northborough, Massachusetts#Education|Northborough - Melican]]
!
|9
!
|-
|[[Topsfield, Massachusetts#Education|Masconomet Regional]]
!
|12
!
|-
|[[Natick, Massachusetts#Public schools|Natick - Wilson]]
|1 and 2
|3
!
|-
|[[Coolidge Middle School (Reading, Massachusetts)|Reading - Coolidge 1]]
!
|4
!
|-
|[[Education in Framingham, Massachusetts#Middle schools|Framingham - Walsh]]
!
|11
!
|-
|[[Melrose Public Schools#Secondary schools|Melrose - Veterans Memorial Middle School]]
!
|4
!
|-
! rowspan="5" |[[Carl Friedrich Gauss|Gauss]]
|[[Newton, Massachusetts#Primary and secondary education|Newton - Day]]
!
!
!
|-
|[[Concord, Massachusetts#Education|Concord]]
!
|11
!
|-
|[[Andover, Massachusetts#Public schools|Andover - Wood Hill]]
!
|6
!
|-
|[[Franklin, Massachusetts#Education|Franklin - Horace Mann]]
|1 and 2
|10
!
|-
|[[Hampstead, New Hampshire#Education|Hampstead]]
!
|1
!
|-
! rowspan="5" |[[Vaag Mosca|Mosca]]
|[[Amesbury Middle School|Amesbury]]
!
|1
!
|-
|[[Shore Country Day School|Shore Country Day]]&lt;ref&gt;{{cite web | url=http://www.shoreschool.org/home/home.asp | title=Shore Country Day: private Pre-Kindergarten, (Readiness) Kindergarten, Elementary and Middle School in Beverly Massachusetts }}&lt;/ref&gt;
!
|2
!
|-
|[[Marblehead, Massachusetts#Education|Marblehead - Veterans]]
!
|5
!
|-
|[[Heath School|Brookline - Heath]]
!
|7
!
|-
|[[King Philip Regional Middle School|Norfolk - King Philip MS]]
!
|10
!
|-
! rowspan="5" style="background-color:silver" | [[Hypatia]]
| colspan="2" | [[Beverly, Massachusetts|Beverly - Briscoe]] || 2 || 
|-
|[[Tewksbury, Massachusetts|Tewksbury - Wynn]]
!
|6
!
|-
|[[Swampscott, Massachusetts|Swampscott]]
!
|5
!
|-
|[[Triton Regional Middle School|Triton Regional MS]]&lt;ref&gt;{{cite web | url=http://www.trsd.net/middle/default.htm | title=Triton Regional Middle School}}&lt;/ref&gt;
!
|1
!
|-
|[[Ipswich, Massachusetts#Middle and high schools|Ipswich]]
!
|12
!
|-
! rowspan="6" |[[Martin Gardner|Gardner]]
|[[Essex, Massachusetts#Primary|Manchester Essex]]
|1 and 2
|12
!
|-
|[[Mansfield, Massachusetts#Education|Mansfield - Qualters]]
|1 and 2
|10
!
|-
|[[Lynnfield, Massachusetts|Lynnfield]]
!
|4
!
|-
|[[Hamilton, Massachusetts#Education|Hamilton Wenham - Miles River Regional]]
!
|12
!
|-
|[[Newburyport, Massachusetts#Education|Newburyport Nock]]
|1 and 2
|1
!
|-
|[[Natick, Massachusetts|Natick Kennedy]]
!
|3
!
|-
! rowspan="5" |[[Noether]]
|[[Beverly, Massachusetts|Beverly - Waring School]] 
!
|2
!
|-
|[[Stoneham, Massachusetts|Stoneham]]
!
|4
!
|-
|[[Pentucket Regional School District|West Newbury - Pentucket Regional]]
!
|1
!
|-
|[[Danvers, Massachusetts#Private schools|St. Mary Danvers]]
|1 and 2
|2
!
|-
|[[Norwood, Massachusetts#Education|Norwood Coakley]]
!
|10
!
|-
! rowspan="5" |[[Ramanujan]]
|[[Coolidge Middle School (Reading, Massachusetts)|Reading Coolidge 2]]
|
|4
!
|-
|[[Rockport, Massachusetts|Rockport]]
!
|12
!
|-
|[[Austin Preparatory School|Austin Prep]]
!
|4
!
|-
|[[Lynn, Massachusetts|Lynn Pickering]]
|1 and 2
|5
!
|-
|[[Peabody, Massachusetts|St. John Peabody]]
!
|2
!
|-
! rowspan="5" |[[Liu Hui]]
|[[Education in Framingham, Massachusetts#Middle schools|Framingham - Fuller]]
!
|11
!
|-
|[[Lynn, Massachusetts|Lynn Breed]]
|1 and 2
|5
!
|-
|[[Lynn, Massachusetts#Education|Lynn Marshall]]
!
|5
!
|-
|[[Abby Kelley Foster Charter Public School|Abby Kelley Foster Charter]]
!
!
!
|-
|[[Framingham Public School District#Middle schools|Framingham Cameron]]
!
!
!
|-
! rowspan="3" |undecided
|[[Saugus, Massachusetts|Saugus]]
!
|5
!
|-
|[[Cambridge, Massachusetts|Cambridge Friends School]]
!
|7
!
|-
|[[Westford, Massachusetts#Education|Westford - Blanchard]]
!
|6
!
|}

† Year indicated is that of the ''end'' of that school year (i.e., 2007 represents the 2006-07 school year).

==Notes and references==
{{reflist}}

==External links==
*[http://www.imlem.org/ IMLEM Math League]
*[http://www.imlem.org/mathleague/By-Laws-2003-09.htm IMLEM By-Laws, March 2003]

[[Category:Mathematics competitions]]</text>
      <sha1>42vu8g4b4z1cwrlj9gjhwq6o2ygo8bl</sha1>
    </revision>
  </page>
  <page>
    <title>Jean Dieudonné</title>
    <ns>0</ns>
    <id>1070084</id>
    <revision>
      <id>866705649</id>
      <parentid>866366121</parentid>
      <timestamp>2018-11-01T00:51:46Z</timestamp>
      <contributor>
        <username>GeorgeMHall</username>
        <id>33119759</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11979">{{Infobox scientist
|birth_name              = Jean Alexandre Eugène Dieudonné
|image             = JeanDieudonné.jpg
|alt=Jean Alexandre Eugène Dieudonné
|caption           = Jean Alexandre Eugène Dieudonné
|birth_date        = {{birth date|1906|7|1|df=y}}
|birth_place       = [[Lille]], France
|death_date        = {{death date and age|1992|11|29|1906|7|1|df=y}}
|death_place       = [[Paris]], France
|nationality       = French
|field             = [[Mathematics]]
|work_institutions = [[Institut des Hautes Études Scientifiques]]&lt;br&gt;[[University of Nice]]&lt;br&gt;[[University of São Paulo]]&lt;br&gt;[[University of Nancy]]&lt;br&gt;[[University of Michigan]]&lt;br&gt;[[Northwestern University]]
|alma_mater        = [[École Normale Supérieure]]
|doctoral_advisor  = [[Paul Montel]]
|doctoral_students = [[Alexander Grothendieck]]&lt;br&gt;[[Paulo Ribenboim]]
|known_for         = [[Cartan–Dieudonné theorem]]&lt;br&gt;[[Paracompact space]]s
|prizes            = [[Leroy P. Steele Prize]] &lt;small&gt;(1971)&lt;/small&gt;
}}

'''Jean Alexandre Eugène Dieudonné''' ({{IPA-fr|djødɔne|lang}}; 1 July 1906 – 29 November 1992) was a French mathematician, notable for research in [[abstract algebra]], [[algebraic geometry]], and [[functional analysis]], for close involvement with the [[Nicolas Bourbaki]] [[pseudonym]]ous group and the ''[[Éléments de géométrie algébrique]]'' project of [[Alexander Grothendieck]], and as a historian of mathematics, particularly in the fields of functional analysis and [[algebraic topology]]. His work on the [[classical group]]s (the book ''La Géométrie des groupes classiques'' was published in 1955), and on [[formal group]]s, introducing what now are called [[Dieudonné module]]s, had a major effect on those fields.

He was born and brought up in [[Lille]], with a formative stay in [[England]] where he was introduced to [[algebra]]. In 1924 he was admitted to the [[École Normale Supérieure]], where [[André Weil]] was a classmate.&lt;ref&gt;[[André Weil|Weil, André]] (1992). The apprenticeship of a mathematician. Springer.&lt;/ref&gt; He began working, conventionally enough, in [[complex analysis]]. In 1934 he was one of the group of ''normaliens'' convened by Weil, which would become '[[Nicolas Bourbaki|Bourbaki]]'.

== Education and teaching ==
He served in the [[French Army]] during [[World War II]], and then taught in [[Clermont-Ferrand]] until the liberation of France. After holding professorships at the [[University of São Paulo]] (1946–47), the [[University of Nancy]] (1948–1952) and the [[University of Michigan]] (1952–53), he joined the Department of Mathematics at [[Northwestern University]] in 1953, before returning to France as a founding member of the [[Institut des Hautes Études Scientifiques]]. He moved to the [[University of Nice Sophia Antipolis|University of Nice]] to found the Department of Mathematics in 1964, and retired in 1970. He was elected as a member of the [[Académie des Sciences]] in 1968.

== Career ==
Dieudonné drafted much of the Bourbaki series of texts, the many volumes of the [[Éléments de géométrie algébrique|EGA]] [[algebraic geometry]] series, and nine volumes of his own [[Éléments d'Analyse]]. The first volume of the ''Traité'' is a French version of the book ''Foundations of Modern Analysis'' (1960), which had become a graduate textbook on functional analysis.

He also wrote individual monographs on ''Infinitesimal Calculus'', ''Linear Algebra and Elementary Geometry'', [[invariant theory]], [[commutative algebra]], [[algebraic geometry]], and formal groups.

With [[Laurent Schwartz]] he supervised the early research of [[Alexander Grothendieck]]. Later from 1959 to 1964 he was at the [[Institut des Hautes Études Scientifiques]] alongside Grothendieck, and collaborating on the expository work needed to support the project of refounding [[algebraic geometry]] on the new basis of [[scheme (mathematics)|schemes]].

== Selected works ==
*{{cite book|title=Sur les groupes classiques|year=1948|publisher=Hermann|location=Paris}}&lt;ref&gt;{{cite journal|author=Kolchin, Ellis R.|authorlink=Ellis Kolchin|title=Review: ''Sur les groupes classiques'', by Jean Dieudonné|journal=Bull. Amer. Math. Soc.|year=1949|volume=55|issue=3, Part 1|pages=317–320|url=http://www.ams.org/journals/bull/1949-55-03/S0002-9904-1949-09196-6/S0002-9904-1949-09196-6.pdf|doi=10.1090/s0002-9904-1949-09196-6}}&lt;/ref&gt;
*{{Citation | last1=Dieudonné | first1=Jean | author1-link=| title=La géométrie des groupes classiques | url=https://books.google.com/books?id=AfYZAQAAIAAJ | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Ergebnisse der Mathematik und ihrer Grenzgebiete (N.F.), Heft 5 | isbn=978-0-387-05391-2 | year=1955 | mr=0072144}}&lt;ref&gt;{{cite journal|author=Reiner, Irving|authorlink=Irving Reiner|title=Review: ''La géométrie des groupes classiques'', by Jean Dieudonné|journal=Bull. Amer. Math. Soc.|year=1956|volume=62|issue=4|pages=417–420|url=http://www.ams.org/journals/bull/1956-62-04/S0002-9904-1956-10056-6/S0002-9904-1956-10056-6.pdf|doi=10.1090/s0002-9904-1956-10056-6}}&lt;/ref&gt;
* 9 volumes of ''Éléments d'analyse'' (1960-1982), éd. Gauthier-Villars&lt;ref&gt;{{cite journal|author=Marsden, Jerrold E.|authorlink=Jerrold E. Marsden|title=Review: ''Treatise on analysis'', by Jean Dieudonné, trans. by I. G. MacDonald|journal=Bull. Amer. Math. Soc. (N.S.)|year=1980|volume=3|issue=1|pages=719–724|url=http://www.ams.org/journals/bull/1980-03-01/S0273-0979-1980-14804-1/S0273-0979-1980-14804-1.pdf|doi=10.1090/S0273-0979-1980-14804-1}}&lt;/ref&gt;
**{{cite book|title=Foundations of Modern Analysis|url=https://archive.org/stream/FoundationsOfModernAnalysis_578/Dieudonne-FoundationsOfModernAnalysis#page/n0/mode/1up|year=1960|publisher=Academic Press}}&lt;ref&gt;{{cite journal|author=Nachbin, Leopoldo|authorlink=Leopoldo Nachbin|title=Review: ''Foundations of Modern Analysis'', by Jean Dieudonné|journal=Bull. Amer. Math. Soc.|year=1960|volume=67|issue=3|pages=246–250|url=http://www.ams.org/journals/bull/1961-67-03/S0002-9904-1961-10566-1/S0002-9904-1961-10566-1.pdf|doi=10.1090/s0002-9904-1961-10566-1}}&lt;/ref&gt;
*{{cite book|title=Algèbre linéaire et géométrie élémentaire|publisher=Hermann|year=1964}}; Eng. trans: {{cite book|title=Linear algebra and geometry|year=1969}}
*{{cite journal|title=The work of Nicolas Bourbaki|journal=Amer. Math. Monthly|year=1970|volume=77|pages=134–145|doi=10.2307/2317325|url=http://www.maa.org/programs/maa-awards/writing-awards/the-work-of-nicholas-bourbaki}}
*{{Citation | last1=Dieudonné | first1=Jean A. | last2=Carrell | first2=James B. | title=Invariant theory, old and new | publisher=[[Academic Press]] | location=Boston, MA | isbn=978-0-12-215540-6 | doi=10.1016/0001-8708(70)90015-0 | year=1971 | mr=0279102 | journal=Advances in Mathematics | volume=4 | pages=1–80}} (a reprint of {{Citation | last1=Dieudonné | first1=Jean A. | last2=Carrell | first2=James B. | title=Invariant theory, old and new | doi=10.1016/0001-8708(70)90015-0 | year=1970 | journal=Advances in Mathematics | issn=0001-8708 | volume=4 | pages=1–80 | mr=0255525}})
*{{cite book|title=Historical development of algebraic geometry|journal=American Mathematical Monthly|volume=79|issue=8|pages=827–866|date=Oct 1972|url=https://www.maa.org/sites/default/files/pdf/upload_library/22/Ford/Dieudonne.pdf|doi=10.2307/2317664}}
*{{cite book|title=Introduction to the theory of formal groups|publisher=Dekker|year=1973}}
*{{cite book|title=Cours de géométrie algébrique I|publisher=P.U.F.|year=1974}};&lt;ref name="HartshorneRev"&gt;{{cite journal|author=Hartshorne, Robin|authorlink=Robin Hartshorne|title=Review: ''Cours de géométrie algébrique'', vols. I and II by Jean Dieudonné; ''Basic Algebraic Geometry'', by I R. Shararevich, trans. by K. A. Hirsch|journal=Bull. Amer. Math. Soc.|year=1976|volume=82|issue=3|pages=455–459|url=http://www.ams.org/journals/bull/1976-82-03/S0002-9904-1976-14042-6/S0002-9904-1976-14042-6.pdf|doi=10.1090/S0002-9904-1976-14042-6}}&lt;/ref&gt; Eng. trans: {{cite book|title=History of Algebraic Geometry|publisher=Wadsworth Inc. |year=1985}}
*{{cite book|title=Cours de géométrie algébrique II|publisher=P.U.F.|year=1974}}&lt;ref name="HartshorneRev"/&gt;
*{{Citation | last1=Dieudonné | first1=Jean Alexandre | title=A panorama of pure mathematics | url=https://books.google.com/books/about/A_panorama_of_pure_mathematics_as_seen_b.html?id=1V4-kdN6aGkC | publisher=Academic Press Inc. [Harcourt Brace Jovanovich Publishers] | location=London | series=Pure and Applied Mathematics | isbn=978-0-12-215560-4 | year=1982 | volume=97 | mr=0478177}}&lt;ref&gt;{{cite journal|author=Halmos, Paul  R.|authorlink=Paul Halmos|title=Review: ''Panorama des mathématiques pure'', by Jean Dieudonné|journal=Bull. Amer. Math. Soc. (N.S.)|year=1979|volume=1|issue=4|pages=678–681|url=http://www.ams.org/journals/bull/1979-01-04/S0273-0979-1979-14661-5/S0273-0979-1979-14661-5.pdf|doi=10.1090/s0273-0979-1979-14661-5}}&lt;/ref&gt;
*{{Citation | last1=Dieudonné | first1=Jean | author1-link= | title=Choix d'œuvres mathématiques. Tome I | publisher=Hermann | location=Paris | language=French | isbn=978-2-7056-5922-6 | year=1981 | mr=611149}}
*{{Citation | last1=Dieudonné | first1=Jean | author1-link= | title=Choix d'œuvres mathématiques. Tome II | publisher=Hermann | location=Paris | language=French | isbn=978-2-7056-5923-3 | year=1981 | mr=611150}}
*{{cite book|title=History of functional analysis|publisher=North-Holland|year=1981|url=https://books.google.com/books/about/History_of_Functional_Analysis.html?id=mg7r4acKgq0C}}&lt;ref&gt;{{cite journal|author=Doran, Robert S.|authorlink=Robert S. Doran|title=Review: ''History of functional analysis'', by Jean Dieudonné|journal=Bull. Amer. Math. Soc. (N.S.)|year=1982|volume=7|issue=2|pages=403–409|url=http://www.ams.org/journals/bull/1982-07-02/S0273-0979-1982-15049-2/S0273-0979-1982-15049-2.pdf|doi=10.1090/s0273-0979-1982-15049-2}}&lt;/ref&gt;
*{{cite book|title=Pour l'honneur de l'esprit humain: les mathématiques aujourd'hui|publisher=Hachette|year=1987}}
*{{cite book|title=A History of Algebraic and Differential Topology 1900-1960|publisher=Birkhäuser Boston|year=1988|url=https://books.google.com/books/about/A_History_of_Algebraic_and_Differential.html?id=RUV5Dz90rDkC}}
*{{cite book|title=Mathematics - the music of reason|publisher=Springer|year=1992|url=https://books.google.com/books?id=lQosnIw05dYC}}

== References ==
{{reflist}}
*{{citation|first=Pierre|last=Dugac|title=Jean Dieudonné: Mathématicien complet (Plus de lumiere)|language = French|isbn= 978-2-87647-156-6|publisher=Editions Jacques Gabay|year=1995|url=http://www.gabay-editeur.com/epages/300555.sf/fr_FR/?ObjectPath=/Shops/300555/Products/156}}
* {{MacTutor Biography|id=Dieudonne}}
* {{MathGenealogy|id=34219}}

== See also ==
* [[Dieudonné determinant]]
* [[Dieudonné plank]]
* [[Dieudonné's theorem]]

== External links ==
{{wikiquote}}

* A talk on the history of Algebraic Geometry given by Jean Dieudonné at the Department of Mathematics of the University of Wisconsin-Milwaukee in 1972 has been recently restored and is available [https://web.archive.org/web/20131113165913/https://pantherfile.uwm.edu/stevesch/Math%20Lectures/Jean%20Dieudonne%20EIAJ.mov here]
* Dieudonné appears in the [[Horizon (BBC TV series)|Horizon]] BBC documentary [https://www.imdb.com/title/tt2389597/ A Mathematical Mystery Tour]

{{Authority control}}

{{DEFAULTSORT:Dieudonne, Jean}}
[[Category:1906 births]]
[[Category:1992 deaths]]
[[Category:20th-century French mathematicians]]
[[Category:Algebraic geometers]]
[[Category:Historians of mathematics]]
[[Category:École Normale Supérieure alumni]]
[[Category:Nicolas Bourbaki]]
[[Category:University of São Paulo faculty]]
[[Category:Members of the French Academy of Sciences]]
[[Category:People from Lille]]
[[Category:Nancy-Université faculty]]
[[Category:University of Michigan faculty]]
[[Category:University of Nice faculty]]</text>
      <sha1>iosv7fclcycd7s2725k55cufg9jkmg3</sha1>
    </revision>
  </page>
  <page>
    <title>Linear optical quantum computing</title>
    <ns>0</ns>
    <id>41312173</id>
    <revision>
      <id>866822224</id>
      <parentid>863459681</parentid>
      <timestamp>2018-11-01T19:43:52Z</timestamp>
      <contributor>
        <username>Gehenna1510</username>
        <id>34982813</id>
      </contributor>
      <minor/>
      <comment>CS1 error fixed</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="28706">'''Linear Optical Quantum Computing''' or '''Linear Optics Quantum Computation''' ('''LOQC''') is a paradigm of [[quantum computer|quantum computation]], allowing (under certain conditions, described below) [[universal quantum computer|universal quantum computation]]. LOQC uses [[photon]]s as information carriers, mainly uses [[linear optics|linear optical]] elements (including [[beam splitter]]s, [[phase shift module|phase shifters]], and [[mirror]]s) to process [[quantum information]], and uses photon detectors and [[quantum memory|quantum memories]] to detect and store quantum information.&lt;ref name="Adami1999"&gt;{{Cite book | title=Quantum computation with linear optics | author1=Adami, C. | author2=Cerf, N. J. | journal=Quantum Computing and Quantum Communications | year=1999 | pages=391–401 | publisher=Springer | doi=10.1007/3-540-49208-9_36 | series=Lecture Notes in Computer Science | isbn=978-3-540-65514-5 | volume=1509| arxiv=quant-ph/9806048 }}&lt;/ref&gt;&lt;ref name="Knill2001"&gt;{{cite journal | title=A scheme for efficient quantum computation with linear optics | author1=Knill, E. | journal=Nature | year=2001 | volume=409 | pages=46–52 | author2=Laflamme, R. | author3=Milburn, G. J. | doi=10.1038/35051009 | pmid=11343107 | issue=6816|bibcode = 2001Natur.409...46K }}&lt;/ref&gt;&lt;ref name="Kok2007"&gt;{{cite journal | title=Linear optical quantum computing with photonic qubits | author=Kok, P. | journal=Rev. Mod. Phys. | year=2007 | volume=79 | issue=1 | pages=135–174 | author2=Munro, W. J. | author3=Nemoto, K. | author4=Ralph, T. C. | author5=Dowling, J. P. | author6=Milburn, G. J. | doi=10.1103/RevModPhys.79.135|arxiv = quant-ph/0512071 |bibcode = 2007RvMP...79..135K }}&lt;/ref&gt;

== Overview ==

Although there are many other implementations for [[quantum information processing]] (QIP) and quantum computation, [[quantum optics|optical quantum systems]] are prominent candidates, since they link quantum computation and [[quantum communication]] in the same framework. In optical systems for quantum information processing, the unit of light in a given mode—or [[photon]]—is used to represent a [[qubit]]. [[Quantum superposition|Superpositions]] of quantum states can be easily represented, [[quantum encryption|encrypted]], transmitted and detected using photons. Besides, linear optical elements of optical systems may be the simplest building blocks to realize quantum operations and [[quantum gate]]s. Each linear optical element equivalently applies a [[unitary transformation]] on a finite number of qubits. The system of finite linear optical elements constructs a network of linear optics, which can realize any [[quantum circuit]] diagram or [[quantum network]] based on the [[quantum circuit]] model. Quantum computing with continuous variables is also possible under the linear optics scheme.&lt;ref name="Lloyd2003"&gt;{{cite journal | title=Quantum computation over continuous variables | author=Lloyd, S. | journal=Quantum Information with Continuous Variables | year=2003 | pages=9–17 | author2=Braunstein, S. L. | bibcode=1999PhRvL..82.1784L | volume=82 | doi=10.1103/PhysRevLett.82.1784 | issue=8|arxiv = quant-ph/9810082 }}&lt;/ref&gt;

The universality of 1- and 2-bit [[quantum gate|gates]] to implement arbitrary quantum computation has been proven.&lt;ref&gt;{{Cite journal
| doi = 10.1103/PhysRevA.51.1015
| volume = 51
| issue = 2
| pages = 1015–1022
| last = DiVincenzo
| first = David P.
| title = Two-bit gates are universal for quantum computation
| journal = Physical Review A
| date = 1995-02-01
|arxiv = cond-mat/9407022 |bibcode = 1995PhRvA..51.1015D }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal
| doi = 10.1098/rspa.1995.0065
| issn = 1471-2946
| volume = 449
| issue = 1937
| pages = 669–677
| last = Deutsch
| first = David
| first2 = Adriano
| last2 = Barenco
| first3 = Artur
| last3 = Ekert
| title = Universality in Quantum Computation
| journal = Proceedings of the Royal Society of London A: Mathematical and Physical Sciences
| accessdate = 2014-01-25
| date = 1995-06-08
| url = http://rspa.royalsocietypublishing.org/content/449/1937/669
|arxiv = quant-ph/9505018 |bibcode = 1995RSPSA.449..669D | citeseerx = 10.1.1.54.2646
}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal
| doi = 10.1098/rspa.1995.0066
| issn = 1471-2946
| volume = 449
| issue = 1937
| pages = 679–683
| last = Barenco
| first = Adriano
| title = A Universal Two-Bit Gate for Quantum Computation
| journal = Proceedings of the Royal Society of London A: Mathematical and Physical Sciences
| accessdate = 2014-01-25
| date = 1995-06-08
| url = http://rspa.royalsocietypublishing.org/content/449/1937/679
|arxiv = quant-ph/9505016 |bibcode = 1995RSPSA.449..679B }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal
| doi = 10.1103/PhysRevLett.75.346
| pmid = 10059671
| volume = 75
| issue = 2
| pages = 346–349
| last = Lloyd
| first = Seth
| title = Almost Any Quantum Logic Gate is Universal
| journal = Physical Review Letters
| date = 1995-07-10
|bibcode = 1995PhRvL..75..346L }}&lt;/ref&gt; Up to &lt;math&gt;N\times N&lt;/math&gt; unitary matrix operations (&lt;math&gt;U(N)&lt;/math&gt;) can be realized by only using mirrors, beam splitters and phase shifters&lt;ref&gt;{{Cite journal
| doi = 10.1103/PhysRevLett.73.58
| volume = 73
| issue = 1
| pages = 58–61
| last = Reck
| first = Michael
| first2 = Anton
| last2 = Zeilinger
| first3 = Herbert J.
| last3 = Bernstein
| first4 = Philip
| last4 = Bertani
| title = Experimental realization of any discrete unitary operator
| journal = Physical Review Letters
| date = 1994-07-04
|bibcode = 1994PhRvL..73...58R
| pmid=10056719}}&lt;/ref&gt; (this is also a starting point of [[boson sampling]] and of [[Computational complexity theory|computational complexity]] analysis for LOQC). It points out that each &lt;math&gt;U(N)&lt;/math&gt; operator with &lt;math&gt;N&lt;/math&gt; inputs and &lt;math&gt;N&lt;/math&gt; outputs can be constructed  via &lt;math&gt;\mathcal{O}(N^2)&lt;/math&gt; linear optical elements. Based on the reason of universality and complexity, LOQC usually only uses mirrors, beam splitters, phase shifters and their combinations such as [[Mach-Zehnder interferometer]]s with phase shifts to implement arbitrary [[operator (physics)|quantum operators]]. If using a non-deterministic scheme, this fact also implies that LOQC could be resource-inefficient in terms of the number of optical elements and time steps needed to implement a certain quantum gate or circuit, which is a major drawback of LOQC.

Operations via linear optical elements (beam splitters, mirrors and phase shifters, in this case) preserve the photon statistics of input light. For example, a [[coherent light|coherent]] (classical) light input produces a coherent light output; a superposition of quantum states input yields a [[nonclassical light|quantum light state]] output.&lt;ref name="Kok2007" /&gt; Due to this reason, people usually use single photon source case to analyze the effect of linear optical elements and operators. Multi-photon cases can be implied through some statistical transformations.

An intrinsic problem in using photons as information carriers is that photons hardly interact with each other. This potentially causes a scalability problem for LOQC, since nonlinear operations are hard to implement, which can increase the complexity of operators and hence can increase the resources required to realize a given computational function. One way to solve this problem is to bring nonlinear devices into the quantum network. For instance, the [[Kerr effect]] can be applied into LOQC to make a single-photon [[controlled NOT gate|controlled-NOT]] and other operations.&lt;ref&gt;{{Cite journal
| doi = 10.1103/PhysRevLett.62.2124
| pmid = 10039862
| volume = 62
| issue = 18
| pages = 2124–2127
| last = Milburn
| first = G. J.
| title = Quantum optical Fredkin gate
| journal = Physical Review Letters
| date = 1989-05-01
|bibcode = 1989PhRvL..62.2124M }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal
| doi = 10.1080/09500340408230417
| issn = 0950-0340
| volume = 51
| issue = 8
| pages = 1211–1222
| last = Hutchinson
| first = G. D.
| first2 = G. J.
| last2 = Milburn
| title = Nonlinear quantum optical computing via measurement
| journal = Journal of Modern Optics
| year = 2004
|arxiv = quant-ph/0409198 |bibcode = 2004JMOp...51.1211H }}&lt;/ref&gt;

=== KLM protocol ===
{{main article|KLM protocol}}
It was believed that adding nonlinearity to the linear optical network was sufficient to realize efficient quantum computation.&lt;ref&gt;{{Cite journal
| doi = 10.1016/0375-9601(92)90201-V
| issn = 0375-9601
| volume = 167
| issue = 3
| pages = 255–260
| last = Lloyd
| first = Seth
| title = Any nonlinear gate, with linear gates, suffices for computation
| journal = Physics Letters A
| accessdate = 2014-01-25
| date = 1992-07-20
| url = http://www.sciencedirect.com/science/article/pii/037596019290201V
|bibcode = 1992PhLA..167..255L }}&lt;/ref&gt; However, to implement nonlinear optical effects is a difficult task. In 2000, Knill, Laflamme and Milburn proved that it is possible to create universal quantum computers solely with linear optical tools.&lt;ref name="Knill2001" /&gt; Their work has become known as the "KLM scheme" or "[[KLM protocol]]", which uses linear optical elements, single photon sources and photon detectors as resources to construct a quantum computation scheme involving only [[ancilla (quantum computing)|ancilla]] resources, [[quantum teleportation]]s and [[quantum error correction|error corrections]]. It uses another way of efficient quantum computation with linear optical systems, and promotes nonlinear operations solely with linear optical elements.&lt;ref name="Kok2007" /&gt;

At its root, the KLM scheme induces an effective interaction between photons by making projective measurements with [[photodetector]]s, which falls into the category of non-deterministic quantum computation. It is based on a non-linear sign shift between two qubits that uses two ancilla photons and post-selection.&lt;ref&gt;{{Cite journal
| doi = 10.1137/S0097539795293639
| issn = 0097-5397
| volume = 26
| issue = 5
| pages = 1524–1540
| last = Adleman
| first = Leonard M.
| first2 = Jonathan
| last2 = DeMarrais
| first3 = Ming-Deh A.
| last3 = Huang
| title = Quantum Computability
| journal = SIAM Journal on Computing
| date = 1997
}}&lt;/ref&gt; It is also based on the demonstrations that the probability of success of the quantum gates can be made close to one by using entangled states prepared non-deterministically and [[quantum teleportation]] with single-qubit operations&lt;ref&gt;{{Cite journal
| doi = 10.1103/PhysRevLett.70.1895
| volume = 70
| issue = 13
| pages = 1895–1899
| last = Bennett
| first = Charles H.
| first2 = Gilles
| last2 = Brassard
| first3 = Claude
| last3 = Crépeau
| first4 = Richard
| last4 = Jozsa
| first5 = Asher
| last5 = Peres
| first6 = William K.
| last6 = Wootters
| title = Teleporting an unknown quantum state via dual classical and Einstein-Podolsky-Rosen channels
| journal = Physical Review Letters
| date = 1993-03-29
|bibcode = 1993PhRvL..70.1895B
| pmid=10053414}}&lt;/ref&gt;&lt;ref name="Gottesman1999"&gt;{{Cite journal
| doi = 10.1038/46503
| issn = 0028-0836
| volume = 402
| issue = 6760
| pages = 390–393
| last = Gottesman
| first = Daniel
| first2 = Isaac L.
| last2 = Chuang
| title = Demonstrating the viability of universal quantum computation using teleportation and single-qubit operations
| journal = Nature
| accessdate = 2014-01-26
| date = 1999-11-25
| url = http://www.nature.com/nature/journal/v402/n6760/abs/402390a0.html
|arxiv = quant-ph/9908010 |bibcode = 1999Natur.402..390G }}&lt;/ref&gt; Otherwise, without a high enough success rate of a single quantum gate unit, it may require an exponential amount of computing resources. Meanwhile, the KLM scheme is based on the fact that proper quantum coding can reduce the resources for obtaining accurately encoded qubits efficiently with respect to the accuracy achieved, and can make LOQC fault-tolerant for photon loss, detector inefficiency and phase [[decoherence]]. As a result, LOQC can be robustly implemented through the KLM scheme with a low enough resource requirement to suggest practical scalability, making it as promising a technology for QIP as other known implementations.

=== Boson sampling ===
{{main article|boson sampling}}
The more limited [[boson sampling]] model was suggested and analyzed by Aaronson and Arkhipov in 2013.&lt;ref name="Aaronson13"&gt;{{cite journal|last1=Aaronson|first1=Scott|last2=Arkhipov|first2=Alex|title=The computational complexity of linear optics|journal=Theory of Computing|date=2013|volume=9|pages=143–252|doi=10.4086/toc.2013.v009a004|url=http://www.theoryofcomputing.org/articles/v009a004/}}&lt;/ref&gt; It is not believed to be universal,&lt;ref name="Aaronson13"/&gt; but can still solve problems that are believed to be beyond the ability of classical computers, such as the [[boson sampling#The task of boson sampling|boson sampling problem]].

== Elements of LOQC ==

[[DiVincenzo's criteria]] for quantum computation and QIP&lt;ref name="DiVincenzo1998"&gt;{{cite journal | title=Quantum information is physical | author=DiVincenzo, D. | journal=Superlattices and Microstructures | year=1998 | volume=23 | pages=419–432 | author2=Loss, D. | doi=10.1006/spmi.1997.0520 | issue=3–4|arxiv = cond-mat/9710259 |bibcode = 1998SuMi...23..419D }}&lt;/ref&gt;&lt;ref name="DiVincenzo2000"&gt;{{cite journal | title=The Physical Implementation of Quantum Computation | author=Divincenzo, D. P. | journal=Fortschritte der Physik | year=2000 | volume=48 | pages=771–783 | bibcode=2000ForPh..48..771D | doi=10.1002/1521-3978(200009)48:9/11&lt;771::AID-PROP771&gt;3.0.CO;2-E | issue=9–11|arxiv = quant-ph/0002077 }}&lt;/ref&gt; give that a universal system for QIP should satisfy at least the following requirements:
# a scalable physical system with well characterized qubits,
# the ability to initialize the state of the qubits to a simple fiducial state, such as &lt;math&gt;|000\cdots\rangle&lt;/math&gt;,
# long relevant decoherence times, much longer than the gate operation time,
# a "universal" set of quantum gates (this requirement cannot be satisfied by a non-universal system),
# a qubit-specific measurement capability;&lt;br /&gt;if the system is also aiming for quantum communication, it should also satisfy at least the following two requirements:
# the ability to interconvert stationary and [[flying qubit]]s, and
# the ability to faithfully transmit flying qubits between specified location.

As a result of using photons and linear optical circuits, in general LOQC systems can easily satisfy conditions 3, 6 and 7.&lt;ref name="Kok2007" /&gt; The following sections mainly focus on the implementations of quantum information preparation, readout, manipulation, scalability and error corrections, in order to discuss the advantages and disadvantages of LOQC as a candidate for QIP

=== Qubits and modes ===

A [[qubit]] is one of the fundamental QIP units. A [[qubit#Qubit states|qubit state]] which can be represented by
&lt;math&gt;\alpha |0\rangle + \beta|1\rangle&lt;/math&gt; is a [[quantum superposition|superposition state]] which, if [[Quantum measurement|measured]] in the [[orthonormal basis]] &lt;math&gt;\{|0\rangle, |1\rangle\}&lt;/math&gt;, has probability &lt;math&gt;|\alpha|^2&lt;/math&gt; of being in the &lt;math&gt;|0\rangle&lt;/math&gt; state and probability &lt;math&gt;|\beta|^2&lt;/math&gt; of being in the &lt;math&gt;|1\rangle&lt;/math&gt; state, where &lt;math&gt;|\alpha|^2+|\beta|^2=1&lt;/math&gt; is the normalization condition. An optical mode is a distinguishable optical communication channel, which is usually labeled by subscripts of a quantum state. There are many ways to define distinguishable optical communication channels. For example, a set of modes could be different [[Photon polarization|polarization]] of light which can be picked out with linear optical elements, various [[frequency|frequencies]], or a combination of the two cases above.

In the KLM protocol, each of the photons is usually in one of two modes, and the modes are different between the photons (the possibility that a mode is occupied by more than one photon is zero). This is not the case only during implementations of [[Quantum gate#Controlled gates|controlled quantum gates]] such as CNOT. When the state of the system is as described, the photons can be distinguished, since they are in different modes, and therefore a qubit state can be represented using a single photon in two modes, vertical (V) and horizontal (H): for example, &lt;math&gt;|0\rangle \equiv |0,1\rangle _{VH}&lt;/math&gt; and &lt;math&gt;|1\rangle \equiv |1,0\rangle _{VH}&lt;/math&gt;. It is common to refer to the states defined via occupation of modes as [[Fock state]]s.

In boson sampling, photons are not distinguished, and therefore cannot directly represent the qubit state. Instead, we represent the &lt;!-- not a typo --&gt;[[qudit]] state of the entire quantum system by using the Fock states of &lt;math&gt;M&lt;/math&gt; modes which are occupied by &lt;math&gt;N&lt;/math&gt; indistinguishable single photons (this is a &lt;math&gt;\tbinom {M+N-1} {M} &lt;/math&gt;-level quantum system).

=== State preparation ===

To prepare a desired multi-photon quantum state for LOQC, a single-photon state is first required. Therefore, [[Non-linear optics|non-linear optical elements]], such as [[single photon sources|single-photon generators]] and some optical modules, will be employed. For example, [[Spontaneous parametric down-conversion|optical parametric down-conversion]] can be used to conditionally generate the &lt;math&gt;|1\rangle \equiv |1,0\rangle _{VH}&lt;/math&gt; state in the vertical polarization channel at time &lt;math&gt;t&lt;/math&gt; (subscripts are ignored for this single qubit case). By using a conditional single-photon source, the output state is guaranteed, although this may require several attempts (depending on the success rate). A joint multi-qubit state can be prepared in a similar way. In general, an arbitrary quantum state can be generated for QIP with a proper set of photon sources.

=== Implementations of elementary quantum gates ===

To achieve universal quantum computing, LOQC should be capable of realizing a complete set of [[quantum gate#Universal quantum gates|universal gates]]. This can be achieved in the KLM protocol but not in the boson sampling model.

Ignoring error correction and other issues, the basic principle in implementations of elementary quantum gates using only mirrors, beam splitters and phase shifters is that by using these [[linear optics|linear optical]] elements, one can construct any arbitrary 1-qubit unitary operation; in other words, those linear optical elements support a complete set of operators on any single qubit.

The unitary matrix associated with a beam splitter &lt;math&gt;\mathbf{B}_{\theta,\phi}&lt;/math&gt; is:

:&lt;math&gt; U(\mathbf{B}_{\theta,\phi})
=\begin{bmatrix}
\cos \theta &amp; -e^{i\phi}\sin \theta \\
e^{-i\phi} \sin \theta &amp; \cos \theta \end{bmatrix}&lt;/math&gt;,

where &lt;math&gt;\theta&lt;/math&gt; and &lt;math&gt;\phi&lt;/math&gt; are determined by the [[Reflection coefficient|reflection amplitude]] &lt;math&gt;r&lt;/math&gt; and the [[transmission coefficient|transmission amplitude]] &lt;math&gt;t&lt;/math&gt; (relationship will be given later for a simpler case). For a symmetric beam splitter, which has a phase shift &lt;math&gt;\phi=\frac{\pi}{2}&lt;/math&gt; under the unitary transformation condition &lt;math&gt;|t|^2+|r|^2=1&lt;/math&gt; and &lt;math&gt;t^*r+tr^*=0&lt;/math&gt;, one can show that

:&lt;math&gt; U(\mathbf{B}_{\theta,\phi=\frac{\pi}{2}})
=\begin{bmatrix} t &amp; r\\
r &amp; t\end{bmatrix}
=\begin{bmatrix}
\cos \theta &amp; -i\sin \theta \\
-i \sin \theta &amp; \cos \theta \end{bmatrix}=\cos \theta \hat{I}-i \sin \theta \hat{\sigma}_x=e^{-i\theta\hat{\sigma}_x}&lt;/math&gt;,

which is a rotation of the single qubit state about the &lt;math&gt;x&lt;/math&gt;-axis by &lt;math&gt;2\theta=2\cos^{-1}(|t|)&lt;/math&gt; in the [[Bloch sphere]].

A mirror is a special case where the reflecting rate is 1, so that the corresponding unitary operator is a [[rotation matrix]] given by

:&lt;math&gt;R(\theta) = 
\begin{bmatrix}
\cos \theta &amp; -\sin \theta \\
\sin \theta &amp; \cos \theta \\
\end{bmatrix}
&lt;/math&gt;.

For most cases of mirrors used in QIP, the [[angle of incidence (optics)|incident angle]] &lt;math&gt;\theta=45^\circ&lt;/math&gt;.

Similarly, a phase shifter operator &lt;math&gt;\mathbf{P}_\phi&lt;/math&gt; associates with a unitary operator described by &lt;math&gt;U(\mathbf{P}_\phi)=e^{i\phi}&lt;/math&gt;, or, if written in a 2-mode format

:&lt;math&gt; U(\mathbf{P}_{\phi})= \begin{bmatrix}
 e^{i\phi} &amp; 0 \\
0 &amp; 1  \end{bmatrix}=\begin{bmatrix} e^{i\phi/2} &amp; 0\\
0 &amp; e^{-i\phi/2}\end{bmatrix} \text{(global phase ignored)}=e^{i\frac{\phi}{2} \hat{\sigma}_z}&lt;/math&gt;,

which is equivalent to a rotation of &lt;math&gt;-\phi&lt;/math&gt; about the &lt;math&gt;z&lt;/math&gt;-axis.

Since any two [[Special unitary group|&lt;math&gt;SU(2)&lt;/math&gt; rotations]] along orthogonal rotating axes can generate arbitrary rotations in the Bloch sphere, one can use a set of symmetric beam splitters and mirrors to realize an arbitrary &lt;math&gt;SU(2)&lt;/math&gt; operators for QIP. The figures below are examples of implementing a [[Quantum gate#Hadamard gate|Hadamard gate]] and a [[Quantum gate#Pauli-X gate (= NOT gate)|Pauli-X-gate]] (NOT gate) by using beam splitters (illustrated as rectangles connecting two sets of crossing lines with parameters &lt;math&gt;\theta&lt;/math&gt; and &lt;math&gt;\phi&lt;/math&gt;) and mirrors (illustrated as rectangles connecting two sets of crossing lines with parameter &lt;math&gt;R(\theta)&lt;/math&gt;).

{| class="wikitable"
| [[File:Hadamar Linar optics Circuit.jpg|thumb|none|Implementation of a Hadamard gate with a beam splitter and a mirror. [[Quantum circuit]] is on the top part.]]
| [[File:X circuit in LOQC.gif|thumb|none|Implementation of a Pauli-X gate (NOT gate) with a beam splitter. [[Quantum circuit]] is on the top part.]]
|}

In the above figures, a qubit is encoded using two mode channels (horizontal lines): &lt;math&gt;\left\vert0\right\rangle&lt;/math&gt; represents a [[photon]] in the top mode, and &lt;math&gt;\left\vert1\right\rangle&lt;/math&gt; represents a photon in the bottom mode.

== Integrated photonic circuits for LOQC ==

In reality, assembling a whole bunch (possibly on the order of &lt;math&gt;10^4&lt;/math&gt;&lt;ref name="Hayes2004"&gt;{{Cite journal
| doi = 10.1088/1464-4266/6/12/008
| issn = 1464-4266
| volume = 6
| issue = 12
| pages = 533–541
| last = Hayes
| first = A. J. F.
| first2 = A.
| last2 = Gilchrist
| first3 = C. R.
| last3 = Myers
| first4 = T. C.
| last4 = Ralph
| title = Utilizing encoding in scalable linear optics quantum computing
| journal = Journal of Optics B: Quantum and Semiclassical Optics
| accessdate = 2014-01-26
| date = 2004-12-01
| url = http://iopscience.iop.org/1464-4266/6/12/008
|arxiv = quant-ph/0408098 |bibcode = 2004JOptB...6..533H }}&lt;/ref&gt;) of beam splitters and phase shifters in an optical experimental table is challenging and unrealistic. To make LOQC functional, useful and compact, one solution is to miniaturize all linear optical elements, photon sources and photon detectors, and to integrate them onto a chip. If using a [[semiconductor]] platform, single photon sources and photon detectors can be easily integrated. To separate modes, there have been integrated [[arrayed waveguide grating]] (AWG) which are commonly used as optical (de)multiplexers in [[wavelength division multiplexing|wavelength division multiplexed]] (WDM). In principle, beam splitters and other linear optical elements can also be miniaturized or replaced by equivalent [[nanophotonics]] elements. Some progress in these endeavors can be found in the literature, for example, Refs.&lt;ref&gt;{{cite journal | last1 = Gevaux | first1 = D | year = 2008 | title = Optical quantum circuits: To the quantum level | url = | journal = Nature Photonics | volume = 2 | issue = 6| pages = 337 | doi=10.1038/nphoton.2008.92|bibcode = 2008NaPho...2..337G }}&lt;/ref&gt;&lt;ref&gt;{{cite journal | last1 = Politi | first1 = A. | last2 = Cryan | first2 = M. J. | last3 = Rarity | first3 = J. G. | last4 = Yu | first4 = S. | last5 = O'Brien | first5 = J. L. | year = 2008 | title = Silica-on-silicon waveguide quantum circuits | url = | journal = Science | volume = 320 | issue = 5876| pages = 646–649 | doi=10.1126/science.1155441 | pmid=18369104|arxiv = 0802.0136 |bibcode = 2008Sci...320..646P }}&lt;/ref&gt;&lt;ref&gt;{{cite journal | last1 = Thompson | first1 = M. G. | last2 = Politi | first2 = A. | last3 = Matthews | first3 = J. C. | last4 = O'Brien | first4 = J. L. | year = 2011 | title = Integrated waveguide circuits for optical quantum computing | url = | journal = IET Circuits, Devices &amp; Systems | volume = 5 | issue = 2| pages = 94–102 | doi=10.1049/iet-cds.2010.0108}}&lt;/ref&gt; In 2013, the first integrated photonic circuit for quantum information processing has been demonstrated using photonic crystal waveguide to realize the interaction between guided field and atoms.&lt;ref&gt;{{cite journal|arxiv=1312.3446|last1=Goban|first1=A.|last2=Hung|first2=C. -L.|last3=Yu|first3=S. -P.|last4=Hood|first4=J. D.|last5=Muniz|first5=J. A.|last6=Lee|first6=J. H.|last7=Martin|first7=M. J.|last8=McClung|first8=A. C.|last9=Choi|first9=K. S.|title=Atom-Light Interactions in Photonic Crystals|year=2013|last10=Chang|first10=D. E.|last11=Painter|first11=O.|last12=Kimble|first12=H. J.|doi=10.1038/ncomms4808|pmid=24806520|volume=5|pages=3808|journal=Nature Communications|bibcode=2014NatCo...5E3808G}}&lt;/ref&gt;
&lt;!--complexity analysis to be added--&gt;
&lt;!--More content is welcome to be added on entanglement generation/purification/distillation and realizing practical functions using present technologies.--&gt;

== Implementations comparison ==

=== Comparison of the KLM protocol and the boson sampling model ===

The advantage of the KLM protocol over the boson sampling model is that while the KLM protocol is a universal model, boson sampling is not believed to be universal. On the other hand, it seems that the scalability issues in boson sampling are more manageable than those in the KLM protocol.

In boson sampling only a single measurement is allowed, a measurement of all the modes at the end of the computation. The only scalability problem in this model arises from the requirement that all the photons arrive at the photon detectors within a short-enough time interval and with close-enough frequencies.&lt;ref name="Aaronson13" /&gt;

In the KLM protocol, there are non-deterministic quantum gates, which are essential for the model to be universal. These rely on gate teleportation, where multiple probabilistic gates are prepared offline and additional measurements are performed mid-circuit. Those two factors are the cause for additional scalability problems in the KLM protocol.

In the KLM protocol the desired initial state is one in which each of the photons is in one of two modes, and the possibility that a mode is occupied by more than one photon is zero. In boson sampling, however, the desired initial state is specific, requiring that the first &lt;math&gt;N&lt;/math&gt; modes are each occupied by a single photon&lt;ref name="Aaronson13" /&gt; (&lt;math&gt;N&lt;/math&gt; is the number of photons and &lt;math&gt;M \ge N&lt;/math&gt; is the number of modes) and all the other states are empty.
&lt;!--If there is a brief summary about advantages and disadvantages compared to other implementation methods, that would be great! --&gt;

=== Earlier models ===

Another, earlier model which relies on the representation of several qubits by a single photon is based on the work of C. Adami and N. J. Cerf.&lt;ref name="Adami1999" /&gt; By using both the location and the polarization of photons, a single photon in this model can represent several qubits; however, as a result, [[Quantum gate#Controlled gates|CNOT-gate]] can only be implemented between the two qubits represented by the same photon.

The figures below are examples of making an equivalent [[quantum gate#Hadamard gate|Hadamard-gate]] and [[quantum gate#Controlled gates|CNOT-gate]] using beam splitters (illustrated as rectangles connecting two sets of crossing lines with parameters &lt;math&gt;\theta&lt;/math&gt; and &lt;math&gt;\phi&lt;/math&gt;) and phase shifters (illustrated as rectangles on a line with parameter &lt;math&gt;\phi&lt;/math&gt;).

{| class="wikitable"
| [[File:Linear optics H gate.svg|thumb|none|Implementation of Hadamard-gate on a "location" qubit with a beam splitter and phase shifters. [[Quantum circuit]] is on the top part.]]
| [[File:Linear optics CNOT gate.svg|thumb|none|Implementation of Controlled-NOT-gate with a beam splitter. [[Quantum circuit]] is on the top part.]]
|}

In the optical realization of the CNOT gate, the polarization and location are the control and target qubit, respectively.

== References ==
{{Reflist|40em}}

== External links==
*{{cite web|url=http://www.kurzweilai.net/optical-chip-allows-for-reprogramming-quantum-computer-in-seconds|title=Optical chip allows for reprogramming quantum computer in seconds|date=August 14, 2015|publisher=kurzweilai.net}}

{{Quantum computing}}

[[Category:Quantum information science]]
[[Category:Quantum optics]]
[[Category:Quantum gates]]</text>
      <sha1>ix7zvun31k742pb88pisg4vj9x3mwdy</sha1>
    </revision>
  </page>
  <page>
    <title>List of things named after Hermann Grassmann</title>
    <ns>0</ns>
    <id>49064417</id>
    <revision>
      <id>699406476</id>
      <parentid>699406431</parentid>
      <timestamp>2016-01-12T03:28:59Z</timestamp>
      <contributor>
        <username>Ema--or</username>
        <id>16005968</id>
      </contributor>
      <minor/>
      <comment>/* =Grassmannian */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="460">The following is a list of  things named in the memory of the German scholar and polymath [[Hermann Grassmann]]:


*[[Grassmann's law (optics)]]
*[[Grassmann algebra]], or exterior algebra.
*[[Grassmann number]]
*[[Grassmann integral]] 
==Grassmannian==
*[[Grassmannian]]
**[[Lagrangian Grassmannian]]

==Other==
*[[Grassmann's law (phonology)]]

==See also==
*[[Grassmannian (disambiguation)]]
[[Category:Lists of things named after mathematicians|Grassmann]]</text>
      <sha1>a2ons7ofxypnbt1t8h3wtifgikz45ii</sha1>
    </revision>
  </page>
  <page>
    <title>List of things named after Johannes Kepler</title>
    <ns>0</ns>
    <id>38040638</id>
    <revision>
      <id>853931039</id>
      <parentid>839452568</parentid>
      <timestamp>2018-08-07T21:40:46Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <comment>/* Instruments and spacecraft */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1580">This is a list of things [[eponymy|named after]] German mathematician and astronomer  [[Johannes Kepler]] (1571 – 1630).

==Geometry==

*[[Kepler conjecture]]
*[[Kepler triangle]]
*[[Kepler–Bouwkamp constant]]
*[[Kepler–Poinsot polyhedron]]

==Celestial mechanics==

*[[Kepler's laws of planetary motion]]
*[[Kepler's equation]]
*[[Keplerian elements]]
*[[Kepler problem]]
*[[Kepler problem in general relativity]]

==Astronomy==

===Instruments and spacecraft===

*[[Kepler (spacecraft)]]
*[[Kepler Launch Site]]
*[[Kepler photometer]]
*[[Keplerian telescope]]
*[[Kepler refractor]]
*[[Johannes Kepler ATV]]

===Astronomical objects===

;[[Moon]]s and [[Asteroid]]s

*[[Kepler (lunar crater)]]
*[[Kepler (Martian crater)]]
*[[Kepler Dorsum]]
*[[1134 Kepler]]
*[[Kepler orbit]]

;[[Star]]s

*[[Kepler Object of Interest]]
*[[Kepler-11]]
*[[Kepler-22]]
*[[Kepler-22b]]
*[[Kepler's Supernova]]

===Computing===

*[[Kepler Follow-up Program]]
*[[Kepler Input Catalog]]

==Software==

*[[Kepler scientific workflow system]]
*[[Kepler (software)]]

==Geography and institutions==

*[[Kepler Mire]]
*[[Kepler Museum]]
*[[Kepler Track]]
*[[Kepler College]]
*[[Johannes Kepler University Linz]]
*Kepplerstraße, a street in [[Frankfurt am Main]]

==Other==

*[[Kepler Kessel]]
*[[Kepler (opera)]]
*[[Kepler Challenge]]
*[[Kepler (microarchitecture)]]

{{DEFAULTSORT:List of things named after Johannes Kepler}}
[[Category:Lists of things named after astronomers|Kepler, Johannes]]
[[Category:Lists of things named after mathematicians|Kepler, Johannes]]
[[Category:Johannes Kepler]]</text>
      <sha1>40uq4sp28bzu3niww8np8l6nc0b4kgv</sha1>
    </revision>
  </page>
  <page>
    <title>Locally finite space</title>
    <ns>0</ns>
    <id>17267863</id>
    <revision>
      <id>742315266</id>
      <parentid>572144183</parentid>
      <timestamp>2016-10-03T00:25:11Z</timestamp>
      <contributor>
        <username>AxelBoldt</username>
        <id>2</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="754">In the [[mathematics|mathematical]] field of [[topology]], a '''locally finite space''' is a [[topological space]] in which every point has a [[finite set|finite]] [[neighbourhood (mathematics)|neighborhood]].

A locally finite space is [[Alexandrov topology|Alexandrov]].

A [[T1 space|T&lt;sub&gt;1&lt;/sub&gt; space]] is locally finite if and only if it is [[discrete space|discrete]].

==References==
*{{Citation |first=Fumie |last=Nakaoka |first2=Nobuyuki |last2=Oda |year=2001 |title=Some applications of minimal open sets |journal=International Journal of Mathematics and Mathematical Sciences |volume=29 |issue=8 |pages=471–476 |doi=10.1155/S0161171201006482}}

[[Category:General topology]]
[[Category:Properties of topological spaces]]

{{topology-stub}}</text>
      <sha1>j3nhc28i4b6bh0hlwlyzoytsdmsz2d9</sha1>
    </revision>
  </page>
  <page>
    <title>Mahler's compactness theorem</title>
    <ns>0</ns>
    <id>636427</id>
    <revision>
      <id>822753403</id>
      <parentid>532237138</parentid>
      <timestamp>2018-01-28T08:42:10Z</timestamp>
      <contributor>
        <username>Turgidson</username>
        <id>1747755</id>
      </contributor>
      <minor/>
      <comment>link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2553">In [[mathematics]], '''Mahler's compactness theorem''', proved by  {{harvs|txt|authorlink=Kurt Mahler|first=Kurt|last=Mahler|year=1946}}, is a foundational result on [[lattice (group)|lattices]] in [[Euclidean space]], characterising sets of lattices that are 'bounded' in a certain definite sense. Looked at another way, it explains the ways in which a lattice could [[degeneracy (mathematics)|degenerate]] (''go off to infinity'') in a [[sequence]] of lattices. In intuitive terms it says that this is possible in just two ways: becoming ''coarse-grained'' with a [[fundamental domain]] that has ever larger volume; or containing shorter and shorter vectors. It is also called his '''selection theorem''', following an older convention used in naming compactness theorems, because they were formulated in terms of [[sequential compactness]] (the possibility of selecting a convergent subsequence).

Let ''X'' be the space 

:&lt;math&gt;\mathrm{GL}_n(\mathbb{R})/\mathrm{GL}_n(\mathbb{Z})&lt;/math&gt;

that parametrises lattices in &lt;math&gt;\mathbb{R}^n&lt;/math&gt;, with its [[quotient topology]]. There is a [[well-defined]] function Δ on ''X'', which is the [[absolute value]] of the [[determinant]] of a matrix&amp;nbsp;– this is constant on the [[coset]]s, since an [[invertible]] integer matrix has [[determinant]] 1 or −1.

'''Mahler's compactness theorem''' states that a subset ''Y'' of ''X'' is [[relatively compact]] [[if and only if]] Δ is [[bounded set|bounded]] on ''Y'', and there is a neighbourhood&amp;nbsp;''N'' of {0} in &lt;math&gt;\mathbb{R}^n&lt;/math&gt; such that for all ''Λ'' in ''Y'', the only lattice point of Λ in ''N'' is 0 itself.

The assertion of Mahler's theorem is equivalent to the compactness of the space of unit-covolume lattices in &lt;math&gt;\mathbb{R}^n&lt;/math&gt; whose [[systolic geometry|systole]] is larger or equal than any fixed &lt;math&gt;\epsilon&gt;0&lt;/math&gt;.

Mahler's compactness theorem was generalized to [[semisimple Lie group]]s by [[David Mumford]]; see [[Mumford's compactness theorem]].

==References==
*William Andrew Coppel (2006), ''Number theory'', p.&amp;nbsp;418.
{{reflist}}
*{{Citation | last1=Mahler | first1=K. | title=On lattice points in &lt;var&gt;n&lt;/var&gt;-dimensional star bodies. I. Existence theorems | jstor=97965 | mr=0017753 | year=1946 | journal=Proceedings of the Royal Society. London. Series A. Mathematical, Physical and Engineering Sciences | issn=0962-8444 | volume=187 | pages=151–187}}

[[Category:Geometry of numbers]]
[[Category:Discrete groups]]
[[Category:Compactness theorems]]
[[Category:Theorems in number theory]]</text>
      <sha1>adcop51qh5f19zlupjbspalfs98u396</sha1>
    </revision>
  </page>
  <page>
    <title>Marden's theorem</title>
    <ns>0</ns>
    <id>17185722</id>
    <revision>
      <id>793471287</id>
      <parentid>736240523</parentid>
      <timestamp>2017-08-01T23:23:47Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>[[Category:Triangle geometry]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6389">[[File:Marden theorem.svg|thumb|A triangle and its Steiner inellipse. The zeroes of {{math|''p''(''z'')}} are the black dots, and the zeroes of {{math|''p'''(''z'')}} are the red dots). The center green dot is the zero of&amp;nbsp;{{math|''p''"(''z'')}}. Marden's theorem states that the red dots are the foci of the ellipse.]]
In [[mathematics]], '''Marden's theorem''', named after Morris Marden but proven much earlier by Jörg Siebeck, gives a geometric relationship between the zeroes of a third-degree [[polynomial]] with [[complex number|complex]] coefficients and the zeroes of its [[derivative]].

==Statement of the theorem==
A cubic polynomial has three zeroes in the complex number plane, which in general form a triangle, and the [[Gauss–Lucas theorem]] states that the roots of its derivative lie within this triangle. Marden's theorem states their location within this triangle more precisely:

:Suppose the zeroes {{math|''z''&lt;sub&gt;1&lt;/sub&gt;}}, {{math|''z''&lt;sub&gt;2&lt;/sub&gt;}}, and {{math|''z''&lt;sub&gt;3&lt;/sub&gt;}} of a third-degree polynomial {{math|''p''(''z'')}} are non-collinear.  There is a unique ellipse inscribed in the [[triangle]] with vertices {{math|''z''&lt;sub&gt;1&lt;/sub&gt;}}, {{math|''z''&lt;sub&gt;2&lt;/sub&gt;}}, {{math|''z''&lt;sub&gt;3&lt;/sub&gt;}} and [[tangent]] to the sides at their [[midpoint]]s: the [[Steiner inellipse]].  The [[focus (geometry)|foci]] of that ellipse are the zeroes of the derivative {{math|''p'''(''z'')}}.

==Additional relations between root locations and the Steiner inellipse==
By the [[Gauss–Lucas theorem]], the root of the double derivative {{math|''p''"(''z'')}} must be the average of the two foci, which is the center point of the ellipse and the [[centroid]] of the triangle.
In the special case that the triangle is equilateral (as happens, for instance, for the polynomial {{math|1=''p''(''z'') = ''z''&lt;sup&gt;3&lt;/sup&gt; &amp;minus; 1}}) the inscribed ellipse degenerates to a circle, and the derivative of&amp;nbsp;{{math|''p''}} has a [[double root]] at the center of the circle. Conversely, if the derivative has a double root, then the triangle must be equilateral {{harv|Kalman|2008a}}.

==Generalizations==
A more general version of the theorem, due to {{harvtxt|Linfield|1920}}, applies to polynomials {{math|1=''p''(''z'') = (''z'' &amp;minus; ''a'')&lt;sup&gt;''i''&lt;/sup&gt; (''z'' &amp;minus; ''b'')&lt;sup&gt;''j''&lt;/sup&gt; (''z'' &amp;minus; ''c'')&lt;sup&gt;''k''&lt;/sup&gt;}} whose degree {{math|''i'' + ''j'' + ''k''}} may be higher than three, but that have only three roots {{math|''a''}}, {{math|''b''}}, and {{math|''c''}}. For such polynomials, the roots of the derivative may be found at the multiple roots of the given polynomial (the roots whose exponent is greater than one) and at the foci of an ellipse whose points of tangency to the triangle divide its sides in the ratios {{math|''i'' : ''j''}}, {{math|''j'' : ''k''}}, and {{math|''k'' : ''i''}}.

Another generalization ({{harvtxt|Parish|2006}}) is to ''n''-gons: some ''n''-gons have an interior ellipse that is tangent to each side at the side's midpoint. Marden's theorem still applies: the foci of this midpoint-tangent inellipse are zeroes of the derivative of the polynomial whose zeroes are the vertices of the ''n''-gon.

==History==
Jörg Siebeck discovered this theorem 81 years before Marden wrote about it.  However, Dan Kalman titled his American Mathematical Monthly paper "Marden's theorem" because, as he writes, "I call this Marden’s Theorem because I first read it in M. Marden’s wonderful book".

{{harvs|last=Marden|year=1945|year2=1966|txt}} attributes what is now known as Marden's theorem to  {{harvtxt|Siebeck|1864}} and cites nine papers that included a version of the theorem. Dan Kalman won the 2009 [[Lester R. Ford]] Award of the [[Mathematical Association of America]] for his 2008 paper in the [[American Mathematical Monthly]] describing the theorem.

A short and elementary proof of Marden’s theorem is explained in the solution of an exercise in Fritz Carlson’s book “Geometri” (in Swedish, 1943).&lt;ref&gt;{{Cite web|url=http://www.su.se/polopoly_fs/1.229312.1426783194!/menu/standard/file/marden.pdf|title=Carlson’s proof of Marden’s theorem.|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt;

== See also ==
*[[Bôcher's theorem]] for rational functions

== References ==
{{Reflist}}
* {{Citation | last1=Kalman | first1=Dan | title=An Elementary Proof of Marden's Theorem | year=2008a | journal=[[American Mathematical Monthly|The American Mathematical Monthly]] | issn=0002-9890 | volume=115 | pages=330–338|url=http://mathdl.maa.org/mathDL/22/?pa=content&amp;sa=viewDocument&amp;nodeId=3338&amp;pf=1}}
* {{Citation | last1=Kalman | first1=Dan | title=The Most Marvelous Theorem in Mathematics  | url=http://mathdl.maa.org/mathDL/4/?pa=content&amp;sa=viewDocument&amp;nodeId=1663 | year=2008b | journal=[http://mathdl.maa.org/mathDL/4/ Journal of Online Mathematics and its Applications]}}
* {{Citation | last=Linfield | first=B. Z. | title=On the relation of the roots and poles of a rational function to the roots of its derivative | journal=Bulletin of the American Mathematical Society | volume=27 | year=1920 | pages=17–21 | doi=10.1090/S0002-9904-1920-03350-1 }}.
* {{Citation | last1=Marden | first1=Morris | title=A note on the zeroes of the sections of a partial fraction | url=http://www.ams.org/bull/1945-51-12/S0002-9904-1945-08470-5/home.html | year=1945 | journal=[[Bulletin of the American Mathematical Society]] | volume=51 | issue=12 | pages=935–940 | doi=10.1090/S0002-9904-1945-08470-5}}
* {{Citation | last1=Marden | first1=Morris | title=Geometry of Polynomials | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys | volume=3 | year=1966}}
* {{Citation | last1=Parish | first1=James L. | title=On the derivative of a vertex polynomial | url=http://forumgeom.fau.edu/FG2006volume6/FG200633.pdf | year=2006 | journal=Forum Geometricorum | volume=6 | pages=285–288: Proposition 5}}
* {{Citation | last1=Siebeck | first1=Jörg | title=Über eine neue analytische Behandlungweise der Brennpunkte | year=1864 | journal=[[Journal für die reine und angewandte Mathematik]] | issn=0075-4102 | volume=64 | pages=175–182|url=http://gdz.sub.uni-goettingen.de/dms/load/img/?PPN=GDZPPN002152495}}

[[Category:Triangle geometry]]
[[Category:Polynomials]]
[[Category:Conic sections]]
[[Category:Theorems in complex geometry]]</text>
      <sha1>ra08y2jxd6uhuoitfy6f816tbcr1qb2</sha1>
    </revision>
  </page>
  <page>
    <title>Mihai Pătrașcu</title>
    <ns>0</ns>
    <id>35613685</id>
    <revision>
      <id>860710601</id>
      <parentid>855923207</parentid>
      <timestamp>2018-09-22T14:54:56Z</timestamp>
      <contributor>
        <username>Hmains</username>
        <id>508734</id>
      </contributor>
      <minor/>
      <comment>standard quote handling in WP;standard Apostrophe/quotation marks in WP; MOS general fixes</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6470">{{Infobox scientist 
|name = Mihai Pătraşcu
|image = mihai_patrascu2.jpg
|caption = 
|birth_date = {{birth date|1982|07|17|df=y}}
|birth_place = [[Craiova]], Romania
|death_date = {{death date and age|2012|06|05|1982|07|17|df=y}}
|death_place = [[New York City]], U.S.
|field = [[Computer Science]] 
|work_institution = [[AT&amp;T Labs]]
|alma_mater = [[Massachusetts Institute of Technology]]
|doctoral_advisor = [[Erik Demaine]]
|thesis_title = Lower bound techniques for data structures
|thesis_year = 2008
|thesis_url = http://dspace.mit.edu/handle/1721.1/45866
}}

'''Mihai Pătraşcu''' (17 July 1982 – 5 June 2012) was a [[Romanian-American]] [[computer scientist]] at [[AT&amp;T Labs]] in [[Florham Park, New Jersey]], [[United States|USA]].&lt;ref&gt;[http://www.research.att.com/people/Patrascu_Mihai/index.html?fbid=RYoy-83A8_p Staff profile]{{dead link|date=November 2017 |bot=InternetArchiveBot |fix-attempted=yes }}, [[AT&amp;T Labs]], accessed 2012-04-25.&lt;/ref&gt;

Pătraşcu attended [[Carol I National College]] in [[Craiova]].&lt;ref&gt;{{ro}} {{web cite|url=https://pressone.ro/geniul/|title=Geniul|website=PressOne|access-date=2018-03-19}}&lt;/ref&gt;
As a high school student, he won multiple medals at the [[International Olympiad in Informatics]].&lt;ref&gt;{{ro}} {{citation|title=Craiova: Mihai Pătraşcu, informaticianul care a cucerit America| language=Romanian|url=http://www.adevarul.ro/Patrascu-America-Mihai-informaticianul-cucerit_0_66593490.html|date=June 23, 2009|first=Cristina|last=Ghenea|journal=[[Adevărul]]}}.&lt;/ref&gt; He completed his [[undergraduate]] and [[postgraduate education|graduate]] studies in [[Computer Science]] at [[Massachusetts Institute of Technology]], completing his thesis under the supervision of [[Erik Demaine]] in 2008.&lt;ref&gt;{{mathgenealogy|id=132987}}&lt;/ref&gt;

Pătraşcu’s work was concerned with fundamental questions about basic [[data structure]]s.
Pătraşcu received the [[Machtey Award]] for the best student paper at the [[Symposium on Foundations of Computer Science]]  in 2008, and the [[Presburger Award]] from the [[European Association for Theoretical Computer Science]] in 2012, for breaking "many old barriers on fundamental data structure problems, not only revitalizing but also revolutionizing a field that was almost silent for over a decade."&lt;ref&gt;{{citation|url=http://eatcs.org/index.php/component/content/article/1-news/1243-presburger-award-2012|title=Presburger Award 2012|publisher=[[European Association for Theoretical Computer Science]]|accessdate=2012-04-25}}.&lt;/ref&gt;

Pătraşcu died in 2012 after suffering from [[brain cancer]] for a year and a half.&lt;ref&gt;{{cite web|first=Michael|last=Mitzenmacher|authorlink=Michael Mitzenmacher|date=June 6, 2012|title=Sad Passing: Mihai Pătraşcu|url=http://mybiasedcoin.blogspot.com/2012/06/sad-passing-mihai-patrascu.html}}&lt;/ref&gt;&lt;ref&gt;{{cite web|first1=Lance|last1=Fortnow|author1-link=Lance Fortnow|first2=Mohammad Taghi|last2=Hajiaghayi|date=June 7, 2012|title=Mihai Pătraşcu (1982-2012)|url=http://blog.computationalcomplexity.org/2012/06/mihai-patrascu-1982-2012.html}}&lt;/ref&gt;

==Selected publications==
* {{cite journal
| first1 = Timothy M.| last1 = Chan | author1-link = Timothy M. Chan | first2 = Mihai 
| last2 = Pătraşcu
  | first3 =Liam |last3=Roditty | title =Dynamic connectivity: connecting to networks and geometry | journal = [[SIAM Journal on Computing]] | volume = 40 | issue = 2 | pages = 333–349  | year =2011 | doi = 10.1137/090751670 | url = http://people.csail.mit.edu/mip/papers/subconn/paper.pdf
| arxiv =0808.1128}} Preliminary version published in FOCS 2008, {{doi|10.1109/FOCS.2008.29}}.
* {{cite journal
| first =  Mihai | last =Pătraşcu | title =Unifying the landscape of cell-probe lower bounds | journal = [[SIAM Journal on Computing]] | volume = 40 | issue = 3 |pages= 827–847 | year = 2011 | doi = 10.1137/09075336X | url = http://people.csail.mit.edu/mip/papers/structures/paper.pdf
}}
* {{cite journal
| first1 =  Timothy | last1 =Chan | author1-link = Timothy M. Chan | first2 = Mihai | last2 = Pătraşcu | title =Transdichotomous results in computational geometry, I: Point location in sublogarithmic time
  | journal = [[SIAM Journal on Computing]] | volume = 39 | issue = 2| pages =703–729  | year =2010 | doi = 10.1137/07068669X | url = http://people.csail.mit.edu/mip/papers/planar/paper.pdf
}}
* {{cite journal
| first1 =  Mihai | last1 =Pătraşcu  | first2 =Mikkel | last2 = Thorup |  author2-link = Mikkel Thorup| title =Higher lower bounds for near-neighbor and further rich problems | journal = [[SIAM Journal on Computing]] | volume = 39 | issue = 2 | pages = 730–741  | year =2010 | doi = 10.1137/070684859 | url = http://people.csail.mit.edu/mip/papers/higher/higher.pdf
}} Preliminary version published in FOCS 2006, {{doi|10.1109/FOCS.2006.35}}.
* {{cite journal
| first1 =Erik | last1 =Demaine| author1-link=Erik Demaine| first2 = Dion | last2 =Harmon | first3 =John | last3=Iacono | first4 =Mihai | last4=Pătraşcu | title =Dynamic optimality—almost  | journal = [[SIAM Journal on Computing]] | volume = 37 | issue = 1 | pages = 240–251  | year =2007 | doi = 10.1137/S0097539705447347 | url = http://erikdemaine.org/papers/Tango_SICOMP/paper.pdf
}} Preliminary version published in FOCS 2004, {{doi|10.1109/FOCS.2004.23}}. See [[Tango tree]].
* {{cite journal
 | first1 =Mihai | last1 =Pătraşcu | first2 =Erik | last2 =Demaine | title =Logarithmic lower bounds in the cell-probe model | journal = [[SIAM Journal on Computing]] | volume = 35 | issue = 4 | pages = 932–963  | year =2006 | doi = 10.1137/S0097539705447256 | url = http://erikdemaine.org/papers/DynamicConnectivity_SICOMP/paper.pdf
| arxiv =cs/0502041 }}

==References==
{{reflist}}

==External links==
* [http://infoweekly.blogspot.com/ Pătrașcu’s blog WebDiarios de Motocicleta]
* [http://mipmemorial.blogspot.ro/ Mihai Pătrașcu Memorial]
* {{Google scholar id|name=Mihai Patrascu}}

{{Presburger winners}}

{{Authority control}}

{{DEFAULTSORT:Patrascu, Mihai}}
[[Category:1982 births]]
[[Category:2012 deaths]]
[[Category:Romanian computer scientists]]
[[Category:Theoretical computer scientists]]
[[Category:Researchers in geometric algorithms]]
[[Category:Massachusetts Institute of Technology alumni]]
[[Category:American people of Romanian descent]]
[[Category:Deaths from brain tumor]]
[[Category:AT&amp;T people]]
[[Category:People from Craiova]]
[[Category:Carol I National College alumni]]


{{Romania-scientist-stub}}</text>
      <sha1>o3n9j17ot0z1z00ye9il6ch2ym7zria</sha1>
    </revision>
  </page>
  <page>
    <title>Minkowski problem</title>
    <ns>0</ns>
    <id>27966551</id>
    <revision>
      <id>799003860</id>
      <parentid>799001761</parentid>
      <timestamp>2017-09-05T02:27:04Z</timestamp>
      <contributor>
        <username>Convex math</username>
        <id>31724689</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6077">In [[differential geometry]], the '''Minkowski problem''', named after [[Hermann Minkowski]], asks for the construction of a strictly convex [[compact space|compact]] [[surface (topology)|surface]] ''S'' whose [[Gaussian curvature]] is specified. More precisely, the input to the problem is a strictly positive real function ''ƒ'' defined on a sphere, and the surface that is to be constructed should have [[Gaussian curvature]] ''ƒ''(''n''(''x'')) at the point ''x'', where ''n''(''x'') denotes the normal to ''S'' at&amp;nbsp;''x''.  [[Eugenio Calabi]] stated: "From the geometric view point it [the Minkowski problem] is the [[Rosetta Stone]], from which several related problems can be solved."&lt;ref&gt;{{citation|first=Eugenio|last=Calabi|authorlink=Eugenio Calabi|title=Review of ''The Minkowski multidimensional problem'', by Aleksey Vasil'yevich Pogorelov|journal=[[Bulletin of the American Mathematical Society]]|volume=1|year=1979|pages=636–639|doi=10.1090/S0273-0979-1979-14645-7}}.&lt;/ref&gt;

In full generality, the '''Minkowski problem''' asks for necessary and sufficient conditions on a non-negative Borel measure on the unit sphere ''S''&lt;sup&gt;n-1&lt;/sup&gt; to be the surface area measure of a [[convex body]] in &lt;math&gt;\mathbb{R}^n&lt;/math&gt;. Here the surface area measure ''S&lt;sub&gt;K&lt;/sub&gt;'' of a convex body ''K'' is the pushforward of the ''(n-1)''-dimensional Hausdorff measure restricted to the boundary of ''K'' via the [[Gauss map]]. The Minkowski problem was solved by [[Hermann Minkowski]], [[Aleksandr Danilovich Aleksandrov]], [[Werner Fenchel]] and [[Børge Jessen]]&lt;ref&gt;{{Citation |first=Rolf |last=Schneider |title=Convex Bodies: the Brunn-Minkowski Theory |publisher=Cambridge University Press |location=Cambridge |year=1993 |isbn= }}&lt;/ref&gt;: a Borel measure ''μ'' on the unit sphere is the surface area measure of a convex body if and only if ''μ'' has centroid at the origin and is not concentrated on a great subsphere. The convex body is then uniquely determined by ''μ'' up to translations.

The problem of [[radiolocation]] is easily reduced to the Minkowski problem in [[Euclidean space|Euclidean 3-space]]: restoration of convex shape over the given Gauss surface curvature.  The inverse problem of the short-wave diffraction is reduced to the Minkowski problem. The Minkowski problem is the basis of the mathematical theory of [[diffraction]] as well as for the physical theory of diffraction. In the 1960s  [[Petr Ufimtsev]] (P.&amp;nbsp;Ya.&amp;nbsp;Ufimtsev) began developing a high-frequency asymptotic theory for predicting the scattering of [[electromagnetic wave]]s from two-dimensional and three-dimensional objects.  Now this theory is well known as the physical theory of diffraction (PTD).  This theory played the main role in the design of American [[stealth-aircraft]] F-117 and B-2.

In 1953 Louis Nirenberg published the solutions of two long standing open problems, the Weyl problem and the Minkowski problem in Euclidean 3-space. L. Nirenberg's solution of the Minkowski problem was a milestone in global geometry. He has been selected to be the first recipient of the Chern Medal (in 2010) for his role in the formulation of the modern theory of non-liner elliptic partial differential equations, particularly for solving the Weyl problem and the Minkowski problems in Euclidean 3-space.

[[Aleksei Pogorelov|A. V. Pogorelov]] received [[Shevchenko National Prize|Ukraine State Prize]] (1973) for resolving the multidimensional Minkowski problem in Euclidean spaces. Pogorelov resolved the Weyl problem in [[Riemannian space]] in 1969.

[[Shing-Tung Yau]]'s joint work with [[Shiu-Yuen Cheng]] gives a complete proof of the higher-dimensional Minkowski problem in Euclidean spaces. Shing-Tung Yau received the [[Fields Medal]] at the [[International Congress of Mathematicians]] in Warsaw in 1982 for his work in [[global differential geometry]] and [[elliptic partial differential equation]]s, particularly for solving such difficult problems as the [[Calabi conjecture]] of 1954, and a problem of [[Hermann Minkowski]] in Euclidean spaces concerning the [[Dirichlet problem]] for the real [[Monge–Ampère equation]].

==Notes==
{{reflist}}

==References==
*{{cite journal |last=Minkowski |first=H. |year=1903 |title=Volumen und Oberfläche |journal=[[Mathematische Annalen]] |volume=57 |issue=4 |pages=447–495 |doi=10.1007/BF01445180 }}
*{{cite journal |last=Cheng |first=Shiu Yuen |last2=Yau |first2=Shing Tung |year=1976 |title=On the regularity of the solution of the n-dimensional Minkowski problem |journal=[[Communications on Pure and Applied Mathematics|Comm. Pure Appl. Math.]] |volume=29 |issue=5 |pages=495–516 |doi=10.1002/cpa.3160290504 }}
*{{cite journal |last=Nirenberg |first=L. |year=1953 |title=The Weyl and Minkowski problems in differential geometry in the large |journal=Comm. Pure Appl. Math. |volume=6 |issue=3 |pages=337–394 |doi=10.1002/cpa.3160060303 }}
* Pogorelov, A. V. (1979) ''The Minkowsky multidimensional problem''.  Washington: Scripta, 97 p.
* Thompson, Donald O.  Dale E. Chimenti. Review of Progress in Quantitative Nondestructive Evaluation.
* ''Equations of Mathematical Diffraction Theory'', Mezhlum A. Sumbatyan, Rostov State University, Russia; Antonio Scalia, University of Catania, Italy.
*{{cite book |first=P. Ya. |last=Ufimtsev |year=2007 |title=Fundamentals of the Physical Theory of Diffraction |publisher=Wiley &amp; Sons |location=Hoboken, New Jersey |isbn=0-470-09771-X }}
*{{cite book |first=P. Ya. |last=Ufimtsev |year=1962 |title=Method of Edge Waves in the Physical Theory of Diffraction |publisher=Soviet Radio |location=Moscow }}
*{{cite book |first=А. И. |last=Бодренко |year=2013 |title=Проблема Минковского в римановом пространстве. Деформации поверхностей |publisher=LAP LAMBERT Academic Publishing |location=Saarbrücken, Germany |isbn=978-3-659-18088-0 }}

[[Category:Theorems in geometry]]
[[Category:Differential geometry]]
[[Category:Partial differential equations]]
[[Category:Hermann Minkowski]]</text>
      <sha1>mc4l5rd2v5ulbhu3z5tt12gihkz71ng</sha1>
    </revision>
  </page>
  <page>
    <title>Modus ponens</title>
    <ns>0</ns>
    <id>18900</id>
    <revision>
      <id>869176494</id>
      <parentid>865080547</parentid>
      <timestamp>2018-11-16T22:12:01Z</timestamp>
      <contributor>
        <ip>82.31.226.78</ip>
      </contributor>
      <comment>"P implies Q and P _are both_ asserted to be true, [...]" is not grammatically nor logically correct. P *is* asserted to be true[, therefore Q must be true], is both grammatically and logically correct. This is a difficult enough subject as it is! — Arguably there should also be an Oxford comma after the first Q, but not everyone is British.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11988">{{Use dmy dates|date=May 2018}}
{{Italic title}}
{{Transformation rules}}

In [[propositional calculus|propositional logic]], '''''modus ponens''''' ({{IPAc-en|ˈ|m|oʊ|d|ə|s|_|ˈ|p|oʊ|n|ɛ|n|z}}; '''MP'''; also '''''modus ponendo ponens''''' ([[Latin]] for "mode that affirms by affirming")&lt;ref&gt;{{cite book |last=Stone |first=Jon R. |year=1996 |title=Latin for the Illiterati: Exorcizing the Ghosts of a Dead Language |location=London |publisher=Routledge |isbn=0-415-91775-1 |page=60 |url=https://books.google.com/books?id=p6_KRzXsnKIC&amp;pg=PA60 }}&lt;/ref&gt; or '''implication elimination''') is a [[rule of inference]].&lt;ref&gt;Enderton 2001:110&lt;/ref&gt; It can be summarized as "''P [[material conditional|implies]] Q'' and ''P'' is asserted to be true, therefore ''Q'' must be true."

''Modus ponens'' is closely related to another valid form of argument, ''[[modus tollens]]''. Both have apparently similar but invalid forms such as [[affirming the consequent]],  [[denying the antecedent]], and [[evidence of absence]]. [[Constructive dilemma]] is the [[Logical disjunction|disjunctive]] version of ''modus ponens''. [[Hypothetical syllogism]] is closely related to ''modus ponens'' and sometimes thought of as "double ''modus ponens''."

The history of ''modus ponens'' goes back to [[Classical antiquity|antiquity]].&lt;ref&gt;[[Susanne Bobzien]] (2002). "The Development of Modus Ponens in Antiquity", ''Phronesis'' 47, No. 4, 2002.&lt;/ref&gt; The first to explicitly describe the argument form ''modus ponens'' was [[Theophrastus]].&lt;ref&gt;[http://plato.stanford.edu/entries/logic-ancient/#StoSyl "Ancient Logic: Forerunners of ''Modus Ponens'' and ''Modus Tollens''"]. ''[[Stanford Encyclopedia of Philosophy]].&lt;/ref&gt;

== Formal notation ==

The ''modus ponens'' rule may be written in [[sequent]] notation as
:&lt;math&gt;P \to Q,\; P\;\; \vdash\;\; Q&lt;/math&gt;

where ''P'', ''Q'' and ''P'' → ''Q'' are statements (or propositions) in a formal language and [[⊢]] is a [[metalogic]]al symbol meaning that ''Q'' is a [[Logical consequence|syntactic consequence]] of ''P'' and ''P'' → ''Q'' in some [[formal system|logical system]].

== Explanation ==

The argument form has two premises (hypothesis). The first premise is the "if–then" or [[Material conditional|conditional]] claim, namely that ''P'' implies ''Q''. The second premise is that ''P'', the [[Antecedent (logic)|antecedent]] of the conditional claim, is true. From these two premises it can be logically concluded that ''Q'', the [[consequent]] of the conditional claim, must be true as well. In [[artificial intelligence]], ''modus ponens'' is often called [[forward chaining]].

An example of an argument that fits the form ''modus ponens'':

:If today is Tuesday, then John will go to work.
:Today is Tuesday.
:Therefore, John will go to work.

This argument is valid, but this has no bearing on whether any of the statements in the argument are [[Truth|true]]; for ''modus ponens'' to be a sound argument, the premises must be true for any true instances of the conclusion. An [[argument]] can be valid but nonetheless [[Soundness|unsound]] if one or more premises are false; if an argument is valid ''and'' all the premises are true, then the argument is sound. For example, John might be going to work on Wednesday. In this case, the reasoning for John's going to work (because it is Wednesday) is unsound. The argument is not only sound on Tuesdays (when John goes to work), but valid on every day of the week. A [[propositional calculus|propositional]] argument using ''modus ponens'' is said to be [[Deductive reasoning|deductive]].

In single-conclusion [[sequent calculus|sequent calculi]], ''modus ponens'' is the Cut rule. The [[cut-elimination theorem]] for a calculus says that every proof involving Cut can be transformed (generally, by a constructive method) into a proof without Cut, and hence that Cut is [[admissible rule|admissible]].

The [[Curry–Howard correspondence]] between proofs and programs relates ''modus ponens'' to [[function application]]:  if ''f'' is a function of type ''P'' → ''Q'' and ''x'' is of type ''P'', then ''f x'' is of type ''Q''.

==Justification via truth table==
The validity of ''modus ponens'' in classical two-valued logic can be clearly demonstrated by use of a [[truth table]].
{| align="center" border="1" cellpadding="8" cellspacing="0" style="font-weight:bold; text-align:center;" class="wikitable"
|-
! ''p''
! ''q''
! ''p'' → ''q''
|- 
| T || T || T
|- style="color:red"
| T || F || F
|- style="color:pink"
| F || T || T
|- style="color:blue"
| F || F || T
|}

In instances of ''modus ponens'' we assume as premises that ''p'' → ''q'' is true and ''p'' is true. Only one line of the truth table—the first—satisfies these two conditions (''p''  and ''p'' → ''q''). On this line, ''q'' is also true. Therefore, whenever ''p'' → ''q'' is true and ''p'' is true, ''q'' must also be true.

== Status ==

While ''modus ponens'' is one of the most commonly used [[argument form]]s in logic it must not be mistaken for a logical law; rather, it is one of the accepted mechanisms for the construction of deductive proofs that includes the "rule of definition" and the "rule of substitution".&lt;ref&gt;Alfred Tarski 1946:47. Also Enderton 2001:110ff.&lt;/ref&gt; ''Modus ponens'' allows one to eliminate a [[material conditional|conditional statement]] from a [[formal proof|logical proof or argument]] (the antecedents) and thereby not carry these antecedents forward in an ever-lengthening string of symbols; for this reason modus ponens is sometimes called the '''rule of detachment'''&lt;ref&gt;Tarski 1946:47&lt;/ref&gt; or the '''law of detachment'''.&lt;ref&gt;{{cite web|url=https://www.encyclopediaofmath.org/index.php/Modus_ponens|title=Modus ponens - Encyclopedia of Mathematics|author=|date=|website=www.encyclopediaofmath.org|accessdate=5 April 2018}}&lt;/ref&gt; Enderton, for example, observes that "modus ponens can produce shorter formulas from longer ones",&lt;ref&gt;Enderton 2001:111&lt;/ref&gt; and Russell observes that "the process of the inference cannot be reduced to symbols. Its sole record is the occurrence of ⊦q [the consequent] . . . an inference is the dropping of a true premise; it is the dissolution of an implication".&lt;ref name="auto"&gt;Whitehead and Russell 1927:9&lt;/ref&gt;

A justification for the "trust in inference is the belief that if the two former assertions [the antecedents] are not in error, the final assertion [the consequent] is not in error".&lt;ref name="auto"/&gt; In other words: if one [[statement (logic)|statement]] or [[proposition]] [[material conditional|implies]] a second one, and the first statement or proposition is true, then the second one is also true. If ''P'' implies ''Q'' and  ''P'' is true, then ''Q'' is true.&lt;ref&gt;{{cite book | last=Jago | first=Mark | title=Formal Logic | publisher=[http://www.humanities-ebooks.co.uk/ Humanities-Ebooks LLP] |year= 2007 |isbn=978-1-84760-041-7 }}&lt;/ref&gt;

==Correspondence to other mathematical frameworks==
===Probability calculus===
''Modus ponens'' represents an instance of the [[Law of total probability]] which for a binary variable is expressed as:

&lt;math&gt;\Pr(Q)=\Pr(Q\mid P)\Pr(P)+\Pr(Q\mid \lnot P)\Pr(\lnot P)\,&lt;/math&gt;,

where e.g. &lt;math&gt;\Pr(Q)&lt;/math&gt; denotes the probability of &lt;math&gt;Q&lt;/math&gt; and the [[conditional probability]] &lt;math&gt;\Pr(Q\mid P)&lt;/math&gt; generalizes the logical implication &lt;math&gt;P \to Q&lt;/math&gt;. Assume that &lt;math&gt;\Pr(Q) = 1&lt;/math&gt; is equivalent to &lt;math&gt;Q&lt;/math&gt; being TRUE, and that &lt;math&gt;\Pr(Q) = 0&lt;/math&gt; is equivalent to &lt;math&gt;Q&lt;/math&gt; being FALSE.  It is then easy to see that &lt;math&gt;\Pr(Q) = 1&lt;/math&gt; when &lt;math&gt;\Pr(Q\mid P) = 1&lt;/math&gt; and &lt;math&gt;\Pr(P) = 1&lt;/math&gt;. Hence, the [[law of total probability]] represents a generalization of ''modus ponens'' &lt;ref&gt;Audun Jøsang 2016:2&lt;/ref&gt;.

===Subjective logic===
''Modus ponens'' represents an instance of the binomial deduction operator in [[subjective logic]] expressed as:

&lt;math&gt;\omega^{A}_{Q\|P}= (\omega^{A}_{Q|P},\omega^{A}_{Q|\lnot P})\circledcirc \omega^{A}_{P}\,&lt;/math&gt;,

where &lt;math&gt;\omega^{A}_{P}&lt;/math&gt; denotes the subjective opinion about &lt;math&gt;P&lt;/math&gt; as expressed by source &lt;math&gt;A&lt;/math&gt;, and the conditional opinion &lt;math&gt;\omega^{A}_{Q|P}&lt;/math&gt; generalizes the logical implication &lt;math&gt;P \to Q&lt;/math&gt;. The deduced marginal opinion about &lt;math&gt;Q&lt;/math&gt; is denoted by &lt;math&gt;\omega^{A}_{Q\|P}&lt;/math&gt;. The case where &lt;math&gt;\omega^{A}_{P}&lt;/math&gt; is an absolute TRUE opinion about &lt;math&gt;P&lt;/math&gt; is equivalent to source &lt;math&gt;A&lt;/math&gt; saying that &lt;math&gt;P&lt;/math&gt; is TRUE, and the case where &lt;math&gt;\omega^{A}_{P}&lt;/math&gt; is an absolute FALSE opinion about &lt;math&gt;P&lt;/math&gt; is equivalent to source &lt;math&gt;A&lt;/math&gt; saying that &lt;math&gt;P&lt;/math&gt; is FALSE.  The deduction operator &lt;math&gt;\circledcirc&lt;/math&gt; of [[subjective logic]] produces an absolute TRUE deduced opinion &lt;math&gt;\omega^{A}_{Q\|P}&lt;/math&gt; when the conditional opinion &lt;math&gt;\omega^{A}_{Q|P}&lt;/math&gt; is absolute TRUE and the antecedent opinion &lt;math&gt;\omega^{A}_{P}&lt;/math&gt; is absolute TRUE. Hence, subjective logic deduction represents a generalization of both ''modus ponens'' and the [[Law of total probability]] &lt;ref&gt;Audun Jøsang 2016:92&lt;/ref&gt;.

==Alleged cases of failure==

The philosopher and logician Vann McGee has argued that ''modus ponens'' can fail to be valid when the consequent is itself a conditional sentence.&lt;ref&gt;Vann McGee (1985). "A Counterexample to Modus Ponens", ''The Journal of Philosophy'' 82, 462–471.&lt;/ref&gt; Here is an example:

:Either [[Shakespeare]] or [[Thomas Hobbes|Hobbes]] wrote ''[[Hamlet]]''.
:If either Shakespeare or Hobbes wrote ''Hamlet'', then if Shakespeare didn't do it, Hobbes did.
:Therefore, if Shakespeare didn't write ''Hamlet'', Hobbes did it.

The first premise seems reasonable enough, because Shakespeare is generally credited with writing ''Hamlet''. The second premise seems reasonable, as well, because with the range of ''Hamlet'' 's possible authors limited to just Shakespeare and Hobbes, eliminating one leaves only the other. But the conclusion is dubious, because if Shakespeare is ruled out as ''Hamlet'''s author, there are many more plausible alternatives than Hobbes.

The general form of McGee-type counterexamples to ''modus ponens'' is simply &lt;math&gt;P, P \rightarrow (Q \rightarrow R)&lt;/math&gt;, therefore &lt;math&gt;Q \rightarrow R&lt;/math&gt;; it is not essential that &lt;math&gt;P&lt;/math&gt; have the form of a disjunction, as in the example given. That these kinds of cases constitute failures of ''modus ponens'' remains a minority view among logicians, but there is no consensus on how the cases should be disposed of.

==Possible fallacies==
The fallacy of [[affirming the consequent]] is a common misinterpretation of the modus ponens.{{citation needed|date=November 2016}}

==See also==
* [[Condensed detachment]]
* [[Latin phrases]]
* ''[[Modus tollens]]''
* ''[[Modus vivendi]]''
* [[Stoic logic]]
* "[[What the Tortoise Said to Achilles]]"

== References ==
{{Reflist}}

== Sources ==
*Herbert B. Enderton, 2001, ''A Mathematical Introduction to Logic Second Edition'', Harcourt Academic Press, Burlington MA, {{ISBN|978-0-12-238452-3}}.
* Audun Jøsang, 2016, ''Subjective Logic; A formalism for Reasoning Under Uncertainty'' Springer, Cham, {{ISBN|978-3-319-42337-1}}
*[[Alfred North Whitehead]] and [[Bertrand Russell]] 1927 ''Principia Mathematica to *56 (Second Edition)'' paperback edition 1962, Cambridge at the University Press, London UK. No ISBN, no LCCCN.
*Alfred Tarski 1946 ''Introduction to Logic and to the Methodology of the Deductive Sciences'' 2nd Edition, reprinted by Dover Publications, Mineola NY. {{ISBN|0-486-28462-X}} (pbk).

== External links ==
*{{springer|title=Modus ponens|id=p/m064570}}
*{{PhilPapers|search|modus_ponens}}
* ''[http://mathworld.wolfram.com/ModusPonens.html Modus ponens]'' at Wolfram MathWorld

{{DEFAULTSORT:Modus Ponens}}
[[Category:Rules of inference]]
[[Category:Latin logical phrases]]
[[Category:Theorems in propositional logic]]
[[Category:Classical logic]]</text>
      <sha1>2yqeoe0le1fddu2anuus1m61392s3jw</sha1>
    </revision>
  </page>
  <page>
    <title>Mrs. Miniver's problem</title>
    <ns>0</ns>
    <id>62047</id>
    <revision>
      <id>869320116</id>
      <parentid>869181851</parentid>
      <timestamp>2018-11-17T21:39:20Z</timestamp>
      <contributor>
        <username>Loraof</username>
        <id>22399950</id>
      </contributor>
      <comment>/* top */ illustration</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3513">[[File:Venn0110.svg|thumb|Mrs. Miniver’s problem is to adjust the positions of the circles so that the inner white area equals the sum of the red areas.]]


'''Mrs. Miniver's problem''' is a [[geometry]] problem about [[circle]]s. Given a circle ''A'', find a circle ''B'' such that the [[area]] of the [[Lens (geometry)|lens]] formed by [[intersection (Euclidean geometry)|intersecting]] their two interiors is equal to the area of the [[symmetric difference]] of ''A'' and ''B'' (the sum of the areas contained in one but not both circles).&lt;ref&gt;[https://books.google.com/books?id=xdpQAQAAQBAJ&amp;pg=PA64&amp;lpg=PA64&amp;dq=mrs+miniver's+problem&amp;source=bl&amp;ots=Huzgr72gQy&amp;sig=BNR6rXflBRVGSKQXHEIt0ikjk54&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjOg7fF5a7UAhUG6RQKHciTBxsQ6AEIYjAN#v=onepage&amp;q=mrs%20miniver's%20problem&amp;f=false Graham, Louis, A. ''Ingenious Mathematical Problems and Methods'', pp.64–66. Courier Corporation, April 10, 2013] {{isbn|0486282937}}&lt;/ref&gt;&lt;ref&gt;[http://www.rachaelmatthews.co.uk/blog/2010/04/mrs-minivers-problem-and-other-stories.html Matthews, Rachael. "Mrs. Miniver's Problem and Other Stories by Ingrid Mu" (April 16, 2010)]&lt;/ref&gt;

==Origin==
The problem derives from "A Country House Visit", one of [[Jan Struther]]'s newspaper articles featuring her character [[Mrs. Miniver (character)|Mrs. Miniver]]. According to the story:

&lt;blockquote&gt;She saw every relationship as a pair of intersecting circles. It would seem at first glance that the more they overlapped the better the relationship; but this is not so. Beyond a certain point the law of diminishing returns sets in, and there are not enough private resources left on either side to enrich the life that is shared. Probably perfection is reached when the area of the two outer crescents, added together, is exactly equal to that of the leaf-shaped piece in the middle. On paper there must be some neat mathematical formula for arriving at this; in life, none.&lt;/blockquote&gt;

Alan Wachtel writes of the problem:

&lt;blockquote&gt;It seems that certain mathematicians took this literary challenge literally, and Fadiman follows it with an excerpt from "Ingenious Mathematical Problems and Methods," by L. A. Graham, who had evidently posed the problem in a mathematics journal. Graham gives a solution by William W. Johnson of Cleveland for the general case of unequal circles. The analysis isn't difficult, but the resulting transcendental equation is messy and can't be solved exactly. When the circles are of equal size, the equation is much simpler, but it still can be solved only approximately.&lt;/blockquote&gt;

==Solution==
In the case of two circles of equal size, the ratio of the distance between their centers and their radius is often quoted as approximately 0.807946. However, that actually describes the case when the three areas each are of equal size. The solution for the problem as stated in the story ("when the area of the two outer crescents, '''''added together''''', is exactly equal to that of the leaf-shaped piece in the middle") is approximately 0.529864.

==Sources==
* [[Jan Struther]] [http://digital.library.upenn.edu/women/struther/miniver/miniver.html#12 "A Country House Visit"] from [[University of Pennsylvania]].
* [[Clifton Fadiman]] editor (1962) [[The Mathematical Magpie]], pages 298 to 300, [[Simon &amp; Schuster]].

==References==
{{reflist}}

==External links==
*[http://www.reenigne.org/blog/romantic/ Romantic mathematical puzzles]

[[Category:Arithmetic problems of plane geometry]]
[[Category:Mathematical problems]]</text>
      <sha1>kgq3rfxf1wbmpcbcsgfov7pzusc3c1s</sha1>
    </revision>
  </page>
  <page>
    <title>Paul Epstein</title>
    <ns>0</ns>
    <id>5494042</id>
    <revision>
      <id>866392151</id>
      <parentid>865419388</parentid>
      <timestamp>2018-10-30T01:47:10Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Removing category Mathematicians who died in the Holocaust per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2018 October 12]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1730">{{other people}}
'''Paul Epstein''' (July 24, 1871 &amp;ndash; August 11, 1939) was a [[Germany|German]] [[mathematician]]. He was known for his contributions to [[number theory]], in particular the [[Epstein zeta function]].

Epstein was born and brought up in [[Frankfurt]], where his father was a professor. He received his PhD in 1895 from the [[University of Strasbourg]]. From 1895 to 1918 he was a [[Privatdozent]] at the University in Strasbourg, which at that time was part of the [[German Empire]]. At the end of [[World War I]] the city of [[Strasbourg]] reverted to [[France]], and Epstein, being German, had to return to Frankfurt.

Epstein was appointed to a non-tenured post at the university and he lectured in Frankfurt from 1919. Later he was appointed professor at Frankfurt. However, after the [[Nazi]]s came to power in Germany he lost his university position. Because of his age he was unable to find a new position abroad, and finally committed suicide by [[barbital]] overdose at [[Dornbusch (Frankfurt am Main)|Dornbusch]], fearing [[Gestapo]] torture because he was a Jew.

== External links ==
* {{MacTutor Biography|id=Epstein}}
* {{MathGenealogy|id=53268}}

{{Authority control}}

{{DEFAULTSORT:Epstein, Paul}}
[[Category:1871 births]]
[[Category:1939 deaths]]
[[Category:20th-century German mathematicians]]
[[Category:Number theorists]]
[[Category:Mathematicians who committed suicide]]
[[Category:German Jewish people who died in the Holocaust]]
[[Category:Drug-related suicides in Germany]]
[[Category:Scientists from Frankfurt]]
[[Category:University of Strasbourg alumni]]
[[Category:University of Strasbourg faculty]]
[[Category:Goethe University Frankfurt faculty]]

{{Germany-mathematician-stub}}</text>
      <sha1>eywb8ty9x06a6xf8tmuwbmcm6prgy58</sha1>
    </revision>
  </page>
  <page>
    <title>Pettis integral</title>
    <ns>0</ns>
    <id>20983125</id>
    <revision>
      <id>866746317</id>
      <parentid>862710186</parentid>
      <timestamp>2018-11-01T08:44:18Z</timestamp>
      <contributor>
        <username>Latex-yow</username>
        <id>27692366</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7396">In [[mathematics]], the '''Pettis integral''' or '''Gelfand&amp;ndash;Pettis integral''', named after [[Israel Gelfand|Israel M. Gelfand]] and [[Billy James Pettis]], extends the definition of the [[Lebesgue integral]] to vector-valued functions on a [[measure space]], by exploiting [[dual space|duality]]. The integral was introduced by Gelfand for the case when the measure space is an interval with [[Lebesgue measure]]. The integral is also called the '''weak integral''' in contrast to the [[Bochner integral]], which is the strong integral.

==Definition==

Let &lt;math&gt;f: X\to V,&lt;/math&gt; where &lt;math&gt;(X,\Sigma,\mu)&lt;/math&gt; is a measure space and &lt;math&gt;V&lt;/math&gt; is a [[topological vector space]]. Suppose that &lt;math&gt;V&lt;/math&gt; admits a dual space &lt;math&gt;V^*&lt;/math&gt; that separates points, e.g. &lt;math&gt;V&lt;/math&gt; is a Banach space or (more generally) a is locally-convex Hausdorff vector space. We write evaluation of a functional as duality pairing: &lt;math&gt;\langle \varphi, x \rangle = \varphi[x]&lt;/math&gt;.

We say that &lt;math&gt;f&lt;/math&gt; is Pettis integrable if &lt;math&gt;\varphi\circ f \in L^1(X,\Sigma,\mu)&lt;/math&gt; for all &lt;math&gt;\varphi\in V^\ast&lt;/math&gt; and there exists a vector &lt;math&gt;e \in V&lt;/math&gt; so that:

:&lt;math&gt;\forall \varphi \in V^*: \qquad \langle \varphi, e\rangle = \int_X \langle \varphi, f(x) \rangle \, d\mu(x).&lt;/math&gt;

In this case, we call &lt;math&gt;e&lt;/math&gt; the Pettis integral of &lt;math&gt;f&lt;/math&gt;. Common notations for the Pettis integral &lt;math&gt;e&lt;/math&gt; include 

:&lt;math&gt;\int_X f d\mu, \qquad \int_X f(x) \, d\mu(x), \quad \text{and} \quad \mu[f].&lt;/math&gt;

==Properties==
* An immediate consequence of the definition is that Pettis integrals are compatible with continuous, linear operators: If &lt;math&gt;\Phi:V_1\to V_2&lt;/math&gt; is and linear and continuous and &lt;math&gt;f:X\to V_1&lt;/math&gt; is Pettis integrable, then &lt;math&gt;\Phi\circ f&lt;/math&gt; is Pettis integrable as well and: 
::&lt;math&gt;\int_X \Phi(f(x))\,d\mu(x) = \Phi \left (\int_X f(x)\,d\mu(x) \right ).&lt;/math&gt;

* The standard estimate 
::&lt;math&gt; \left |\int_X f(x)\,d\mu(x) \right |\leqslant \int_X |f(x)|\,d\mu(x)&lt;/math&gt; 
:for real- and complex-valued functions generalises to Pettis integrals in the following sense: For all continuous seminorms &lt;math&gt;p:V\to\mathbb{R}&lt;/math&gt; and all Pettis integrable &lt;math&gt;f:X\to V,&lt;/math&gt; 
::&lt;math&gt;p \left (\int_X f(x)\,d\mu(x) \right ) \leqslant  \underline{\int_X} p(f(x)) \,d\mu(x)&lt;/math&gt; 
:holds. The right hand side is the lower Lebesgue integral of a &lt;math&gt;[0,\infty]&lt;/math&gt;-valued function, i.e. 
::&lt;math&gt;\underline{\int_X} g \,d\mu := \sup \left \{ \left. \int_X h\,d\mu \right | h:X\to[0,\infty] \text{ is measurable and } 0\leqslant h\leqslant g \right \}.&lt;/math&gt;
:Taking a lower Lebesgue integral is necessary because the integrand &lt;math&gt;p\circ f&lt;/math&gt; may not be measurable. This follows from the [[Hahn-Banach theorem]] because for every vector &lt;math&gt;v\in V&lt;/math&gt; there must be a continuous functional &lt;math&gt;\varphi\in V^\ast&lt;/math&gt; such that &lt;math&gt;\lambda(v)=p(v)&lt;/math&gt; and &lt;math&gt;\forall w\in V: |\lambda(w)|\leq p(w)&lt;/math&gt;. Applying this to &lt;math&gt;v:=\int_X f\,d\mu&lt;/math&gt; it gives the result.

===Mean value theorem===
An important property is that the Pettis integral with respect to a finite measure is contained in the closure of the convex hull of the values scaled by the measure of the integration domain:

:&lt;math&gt;\mu(A)&lt;\infty \implies \int_A f\,d\mu \in \mu(A) \cdot \overline{co(f(A))}&lt;/math&gt;

This is a consequence of the [[Hahn-Banach theorem]] and generalises the [[Mean_value_theorem#Mean_value_theorems_for_definite_integrals|mean value theorem for integrals of real-valued functions]]: If &lt;math&gt;V=\R,&lt;/math&gt; then closed convex sets are simply intervals and for &lt;math&gt;f:X\to[a,b],&lt;/math&gt; the inequalities 

:&lt;math&gt;\mu(A) a \leqslant \int_A f\,d\mu \leqslant \mu(A)b&lt;/math&gt; 

hold.

===Existence===
* If &lt;math&gt;V = \R^n&lt;/math&gt; is finite-dimensional then &lt;math&gt;f&lt;/math&gt; is Pettis integrable if and only if each of &lt;math&gt;f&lt;/math&gt;'s coordinates is Lebesgue integrable.

* If &lt;math&gt;f&lt;/math&gt; is Pettis integrable and &lt;math&gt;A\in\Sigma&lt;/math&gt; is a measurable subset of &lt;math&gt;X&lt;/math&gt;, then &lt;math&gt;f_{|A}: A\to V&lt;/math&gt; and &lt;math&gt;f\cdot 1_A: X\to V&lt;/math&gt; are also Pettis integrable and 
::&lt;math&gt;\int_A f_{|A} \,d\mu = \int_X f\cdot 1_A \,d\mu.&lt;/math&gt;

* If &lt;math&gt;X&lt;/math&gt; is a topological space, &lt;math&gt;\Sigma=\mathfrak{B}_X&lt;/math&gt; its [[Borel set|Borel-&lt;math&gt;\sigma&lt;/math&gt;-algebra]], &lt;math&gt;\mu&lt;/math&gt; a [[Borel measure]] that assigns finite values to compact subsets, &lt;math&gt;V&lt;/math&gt; is [[quasi-complete space|quasi-complete]] (i.e. if every ''bounded'' [[Cauchy net]] converges) and if &lt;math&gt;f&lt;/math&gt; is continuous with compact support, then &lt;math&gt;f&lt;/math&gt; is Pettis integrable.

* More generally: If &lt;math&gt;f&lt;/math&gt; is weakly measurable and there exists a compact, convex &lt;math&gt;C\subseteq V&lt;/math&gt; and a null set &lt;math&gt;N\subseteq X&lt;/math&gt; such that &lt;math&gt;f(X\setminus N)\subseteq C&lt;/math&gt;, then &lt;math&gt;f&lt;/math&gt; is Pettis-integrable.

==Law of large numbers for Pettis-integrable random variables==

Let &lt;math&gt;(\Omega, \mathcal F, \operatorname P)&lt;/math&gt; be a probability space, and let &lt;math&gt;V&lt;/math&gt; be a topological vector space with a dual space that separates points. Let &lt;math&gt;v_n \colon \Omega \to V&lt;/math&gt; be a sequence of Pettis-integrable random variables, and write &lt;math&gt;\operatorname E[v_n]&lt;/math&gt; for the Pettis integral of &lt;math&gt;v_n&lt;/math&gt; (over &lt;math&gt;X&lt;/math&gt;). Note that &lt;math&gt;\operatorname E[v_n]&lt;/math&gt; is a (non-random) vector in &lt;math&gt;V&lt;/math&gt;, and is not a scalar value.

Let 

:&lt;math&gt;\bar v_N := \frac{1}{N} \sum_{n=1}^N v_n&lt;/math&gt; 

denote the sample average. By linearity, &lt;math&gt;\bar v_N&lt;/math&gt; is Pettis integrable, and 

:&lt;math&gt;\operatorname E[\bar v_N] = \frac{1}{N} \sum_{n=1}^N \operatorname E[v_n] \in V.&lt;/math&gt;

Suppose that the partial sums 

:&lt;math&gt;\frac{1}{N} \sum_{n=1}^N \operatorname E[\bar v_n]&lt;/math&gt; 

converge absolutely in the topology of &lt;math&gt;V&lt;/math&gt;, in the sense that all rearrangements of the sum converge to a single vector &lt;math&gt;\lambda \in V&lt;/math&gt;. The weak law of large numbers implies that &lt;math&gt;\langle \varphi, \operatorname E[\bar v_N] - \lambda \rangle \to 0&lt;/math&gt; for every functional &lt;math&gt;\varphi \in V^*&lt;/math&gt;. Consequently, &lt;math&gt;\operatorname E[\bar v_N] \to \lambda&lt;/math&gt; in the [[weak topology]] on &lt;math&gt;X&lt;/math&gt;.

Without further assumptions, it is possible that &lt;math&gt;\operatorname E[\bar v_N]&lt;/math&gt; does not converge to &lt;math&gt;\lambda&lt;/math&gt;.{{Citation needed|date=October 2013}} To get strong convergence, more assumptions are necessary.{{Citation needed|date=October 2013}}

==See also==
* [[Vector measure]]
* [[Weakly measurable function]]

==References==
* James K. Brooks, ''Representations of weak and strong integrals in Banach spaces'', [[Proceedings of the National Academy of Sciences of the United States of America]] 63,  1969, 266–270. [http://www.pnas.org/content/63/2/266.abstract Fulltext] {{MR|0274697}}
* [[Israel Gelfand|Israel M. Gel'fand]], ''Sur un lemme de la théorie des espaces linéaires'', Commun. Inst. Sci. Math. et Mecan., Univ. Kharkoff et Soc. Math. Kharkoff, IV. Ser. 13, 1936, 35–40 {{Zbl|0014.16202}}
* [[Michel Talagrand]], ''Pettis Integral and Measure Theory'', Memoirs of the AMS no. 307 (1984) {{MR|0756174}}
* {{springer|title=Pettis integral|id=p/p072490|first=V. I.|last=Sobolev}}

{{integral}}
{{Functional Analysis}}

[[Category:Functional analysis]]
[[Category:Integrals]]</text>
      <sha1>2htux51vb5zabzmnatphet9fx6e2py4</sha1>
    </revision>
  </page>
  <page>
    <title>Pompeiu's theorem</title>
    <ns>0</ns>
    <id>8635114</id>
    <revision>
      <id>854914900</id>
      <parentid>694681156</parentid>
      <timestamp>2018-08-14T17:22:23Z</timestamp>
      <contributor>
        <username>Ira Leviton</username>
        <id>25046916</id>
      </contributor>
      <minor/>
      <comment>Repalced the phrase "it is obvious that"  - see [[Wikipedia:Manual_of_Style/Words_to_watch#Editorializing]].  It is obvious only to Wikipedia readers with a background in geometry.  Most of the rest won't even understand this article.  Also replaced the 'we'.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1498">[[File:Pompeiu theorem1.svg|thumb|320px|right]]
[[File:Pompeiu theorem2.svg|thumb|320px|right]]

'''Pompeiu's theorem''' is a result of [[plane geometry]], discovered by the Romanian mathematician [[Dimitrie Pompeiu]]. The theorem is simple, but not classical. It states the following:

:''Given an [[equilateral triangle]] ABC in the plane, and a point P in the plane of the triangle ABC, the lengths PA, PB, and PC form the sides of a (maybe, degenerate) triangle.''

The proof is quick.  Consider a rotation of 60° about the point ''C''.  Assume ''A'' maps to ''B'', and ''P'' maps to ''P''&amp;nbsp;&lt;nowiki&gt;'&lt;/nowiki&gt;.  Then &lt;math&gt;\scriptstyle PC\ =\ P'C&lt;/math&gt;, and &lt;math&gt;\scriptstyle\angle PCP'\ =\ 60^{\circ}&lt;/math&gt;.  Hence triangle ''PCP''&amp;nbsp;&lt;nowiki&gt;'&lt;/nowiki&gt; is equilateral and &lt;math&gt;\scriptstyle PP'\ =\ PC&lt;/math&gt;.  Then &lt;math&gt;\scriptstyle PA\ =\ P'B&lt;/math&gt;.  Thus, triangle ''PBP''&amp;nbsp;&lt;nowiki&gt;'&lt;/nowiki&gt; has sides equal to ''PA'', ''PB'', and ''PC'' and the [[proof by construction]] is complete.

Further investigations reveal that if ''P'' is not in the interior of the triangle, but rather on the [[circumcircle]], then ''PA'', ''PB'', ''PC'' form a degenerate triangle, with the largest being equal to the sum of the others.

==External links==
*[http://mathworld.wolfram.com/PompeiusTheorem.html MathWorld's page on Pompeiu's Theorem]

[[Category:Elementary geometry]]
[[Category:Triangle geometry]]
[[Category:Articles containing proofs]]
[[Category:Theorems in plane geometry]]</text>
      <sha1>s2y3m8363q2x0kg7v5sjbvi92k39dh6</sha1>
    </revision>
  </page>
  <page>
    <title>Quasi-arithmetic mean</title>
    <ns>0</ns>
    <id>566869</id>
    <revision>
      <id>871694420</id>
      <parentid>863010129</parentid>
      <timestamp>2018-12-02T21:14:47Z</timestamp>
      <contributor>
        <username>Nbarth</username>
        <id>570614</id>
      </contributor>
      <comment>/* Examples */ Explain LSE a bit better</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6952">In [[mathematics]] and [[statistics]], the '''quasi-arithmetic mean''' or '''generalised ''f''-mean''' is one generalisation of the more familiar [[mean]]s such as the [[arithmetic mean]] and the [[geometric mean]], using a function &lt;math&gt;f&lt;/math&gt;. It is also called '''Kolmogorov mean''' after Russian mathematician [[Andrey Kolmogorov]]. It is a broader generalization than the regular [[generalized mean]].

==Definition==

If ''f'' is a function which maps an interval &lt;math&gt;I&lt;/math&gt; of the real line to the [[real number]]s, and is both [[continuous function|continuous]] and [[injective function|injective]], the '''''f''-mean of &lt;math&gt;n&lt;/math&gt; numbers'''
:&lt;math&gt;x_1, \dots, x_n \in I&lt;/math&gt;
is defined as
:&lt;math&gt;M_f(x_1, \dots, x_n) = f^{-1}\left( \frac{f(x_1)+ \cdots + f(x_n)}n \right).&lt;/math&gt;

We require ''f'' to be injective in order for the [[inverse function]] &lt;math&gt;f^{-1}&lt;/math&gt; to exist. Since &lt;math&gt;f&lt;/math&gt; is defined over an interval, &lt;math&gt;\frac{f(x_1)+ \cdots + f(x_n)}n&lt;/math&gt; lies within the domain of &lt;math&gt;f^{-1}&lt;/math&gt;.

Since ''f'' is injective and continuous, it follows that ''f'' is a strictly [[monotonic function]], and therefore that the ''f''-mean is neither larger than the largest number of the tuple &lt;math&gt;x&lt;/math&gt; nor smaller than the smallest number in &lt;math&gt;x&lt;/math&gt;.

== Examples ==

* If &lt;math&gt;I&lt;/math&gt; = ℝ, the [[real line]],  and &lt;math&gt;f(x) = x&lt;/math&gt;, (or indeed any linear function &lt;math&gt;x\mapsto a\cdot x + b&lt;/math&gt;, &lt;math&gt;a&lt;/math&gt; not equal to 0) then the ''f''-mean corresponds to the [[arithmetic mean]].
* If &lt;math&gt;I&lt;/math&gt; = ℝ&lt;sup&gt;+&lt;/sup&gt;, the [[positive real numbers]] and &lt;math&gt;f(x) = \log(x)&lt;/math&gt;, then the ''f''-mean corresponds to the [[geometric mean]]. According to the ''f''-mean properties, the result does not depend on the base of the [[logarithm]] as long as it is positive and not 1.
* If &lt;math&gt;I&lt;/math&gt; = ℝ&lt;sup&gt;+&lt;/sup&gt; and &lt;math&gt;f(x) = \frac{1}{x}&lt;/math&gt;, then the ''f''-mean corresponds to the [[harmonic mean]].
* If &lt;math&gt;I&lt;/math&gt; = ℝ&lt;sup&gt;+&lt;/sup&gt; and &lt;math&gt;f(x) = x^p&lt;/math&gt;, then the ''f''-mean corresponds to the [[power mean]] with exponent &lt;math&gt;p&lt;/math&gt;.
* If &lt;math&gt;I&lt;/math&gt; = ℝ and &lt;math&gt;f(x) = \exp(x)&lt;/math&gt;, then the ''f''-mean is the mean in the [[log semiring]], which is a constant shifted version of the [[LogSumExp]] (LSE) function (which is the logarithmic sum), &lt;math&gt;M_f(x_1, \dots, x_n) = \mathrm{LSE}(x_1, \dots, x_n)-\log(n)&lt;/math&gt;. The &lt;math&gt;-\log(n)&lt;/math&gt; corresponds to dividing by {{mvar|''n''}}, since logarithmic division is linear subtraction. The LogSumExp function is a [[smooth maximum]]: a smooth approximation to the maximum function.

== Properties ==

* [[Partition of a set|Partitioning]]: The computation of the mean can be split into computations of equal sized sub-blocks.
:: &lt;math&gt;
M_f(x_1,\dots,x_{n\cdot k}) =
  M_f(M_f(x_1,\dots,x_{k}),
      M_f(x_{k+1},\dots,x_{2\cdot k}),
      \dots,
      M_f(x_{(n-1)\cdot k + 1},\dots,x_{n\cdot k}))
&lt;/math&gt;
* Subsets of elements can be averaged a priori, without altering the mean, given that the multiplicity of elements is maintained.
:With &lt;math&gt;m=M_f(x_1,\dots,x_k)&lt;/math&gt; it holds
::&lt;math&gt;M_f(x_1,\dots,x_k,x_{k+1},\dots,x_n) = M_f(\underbrace{m,\dots,m}_{k \text{ times}},x_{k+1},\dots,x_n)&lt;/math&gt;
* The quasi-arithmetic mean is invariant with respect to offsets and scaling of &lt;math&gt;f&lt;/math&gt;:
::&lt;math&gt;\forall a\ \forall b\ne0 ((\forall t\ g(t)=a+b\cdot f(t)) \Rightarrow \forall x\ M_f (x) = M_g (x)&lt;/math&gt;.
* If &lt;math&gt;f&lt;/math&gt; is [[Monotonic function|monotonic]], then &lt;math&gt;M_f&lt;/math&gt; is monotonic.
* Any quasi-arithmetic mean &lt;math&gt;M&lt;/math&gt; of two variables has the ''mediality property'' &lt;math&gt;M(M(x,y),M(z,w))=M(M(x,z),M(y,w))&lt;/math&gt; and the ''self-distributivity'' property &lt;math&gt;M(x,M(y,z))=M(M(x,y),M(x,z))&lt;/math&gt;. Moreover, any of those properties is essentially sufficient to characterize quasi-arithmetic means; see Aczél&amp;ndash;Dhombres, Chapter 17.
* Any quasi-arithmetic mean &lt;math&gt;M&lt;/math&gt; of two variables has the ''balancing property'' &lt;math&gt;M\big(M(x, M(x, y)), M(y, M(x, y))\big)=M(x, y)&lt;/math&gt;. An interesting problem is whether this condition (together with fixed-point, symmetry, monotonicity and continuity properties) implies that the mean is quasi-arithmetic. [[Georg Aumann]] showed in the 1930s that the answer is no in general,&lt;ref&gt;{{cite journal|last=Aumann|first=Georg|title=Vollkommene Funktionalmittel und gewisse Kegelschnitteigenschaften|journal=[[Journal für die reine und angewandte Mathematik]]|year=1937|volume=176|pages=49–55|doi=10.1515/crll.1937.176.49}}&lt;/ref&gt; but that if one additionally assumes &lt;math&gt;M&lt;/math&gt; to be an [[analytic function]] then the answer is positive.&lt;ref&gt;{{cite journal|last=Aumann|first=Georg|title=Grundlegung der Theorie der analytischen Analytische Mittelwerte|journal=Sitzungsberichte der Bayerischen Akademie der Wissenschaften|year=1934|pages=45–81}}&lt;/ref&gt;
* Under regularity conditions, a [[central limit theorem]] can be derived for the generalised f-mean, thus implying that for a large sample &lt;math&gt;\sqrt{n}\{M_f(X_1, \dots, X_n) - f^{-1}(E_f(X_1, \dots, X_n))\}&lt;/math&gt; is approximately normal.&lt;ref&gt;{{cite journal|last=de Carvalho|first=Miguel|title=Mean, what do you Mean?|journal=[[The American Statistician]]|year=2016|volume=70|pages=764‒776|doi=10.1080/00031305.2016.1148632}}&lt;/ref&gt;
== Homogeneity ==

[[Mean]]s are usually [[Homogeneous function|homogeneous]], but for most functions &lt;math&gt;f&lt;/math&gt;, the ''f''-mean is not.
Indeed, the only homogeneous quasi-arithmetic means are the [[power mean]]s and the [[geometric mean]]; see Hardy&amp;ndash;Littlewood&amp;ndash;Pólya, page 68.

The homogeneity property can be achieved by normalizing the input values by some (homogeneous) mean &lt;math&gt;C&lt;/math&gt;.
:&lt;math&gt;M_{f,C} x = C x \cdot f^{-1}\left( \frac{f\left(\frac{x_1}{C x}\right) + \cdots + f\left(\frac{x_n}{C x}\right)}{n} \right)&lt;/math&gt;
However this modification may violate [[Monotonic function|monotonicity]] and the partitioning property of the mean.

== References ==
{{reflist}}
* Aczél, J.; Dhombres, J. G. (1989) Functional equations in several variables. With applications to mathematics, information theory and to the natural and social sciences. Encyclopedia of Mathematics and its Applications, 31. Cambridge Univ. Press, Cambridge, 1989.
* Andrey Kolmogorov (1930) “On the Notion of Mean”, in “Mathematics and Mechanics” (Kluwer 1991) — pp.&amp;nbsp;144&amp;ndash;146.
* Andrey Kolmogorov (1930) Sur la notion de la moyenne. Atti Accad. Naz. Lincei 12, pp.&amp;nbsp;388&amp;ndash;391.
* John Bibby (1974) “Axiomatisations of the average and a further generalisation of monotonic sequences,” Glasgow Mathematical Journal, vol. 15, pp.&amp;nbsp;63–65.
* Hardy, G. H.; Littlewood, J. E.; Pólya, G. (1952) Inequalities. 2nd ed. Cambridge Univ. Press, Cambridge, 1952.

== See also ==
* [[Generalized mean]]
* [[Jensen's inequality]]

{{DEFAULTSORT:Quasi-Arithmetic Mean}}
[[Category:Means]]</text>
      <sha1>ebhqte6125893apz8bvkh7kjir2zvrj</sha1>
    </revision>
  </page>
  <page>
    <title>Quasiperfect number</title>
    <ns>0</ns>
    <id>321913</id>
    <revision>
      <id>864134450</id>
      <parentid>859303568</parentid>
      <timestamp>2018-10-15T09:30:04Z</timestamp>
      <contributor>
        <username>Jeppesn</username>
        <id>451783</id>
      </contributor>
      <comment>/* Related */ [[Betrothed numbers]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3504">In [[mathematics]], a '''quasiperfect number''' is a [[natural number]] ''n'' for which the sum of all its [[divisor]]s (the [[divisor function]] ''σ''(''n'')) is equal to 2''n'' + 1. Equivalently, ''n'' is the sum of its non-trivial divisors (that is, its divisors excluding 1 and ''n''). No quasiperfect numbers have been found so far.

The quasiperfect numbers are the [[abundant number]]s of minimal abundance 1.

== Theorems ==
If a quasiperfect number exists, it must be an odd [[square number]] greater than 10&lt;sup&gt;35&lt;/sup&gt; and have at least seven distinct [[prime factor]]s.&lt;ref&gt;{{cite journal|last1=Hagis|first1=Peter |last2=Cohen|first2=Graeme L.|title=Some results concerning quasiperfect numbers|journal=J. Austral. Math. Soc. Ser. A|volume=33|year=1982|pages=275–286|doi=10.1017/S1446788700018401|issue=2|mr=0668448}}&lt;/ref&gt;

== Related ==
Numbers do exist where the sum of all the [[divisor]]s ''σ''(''n'') is equal to 2''n'' + 2:  20, 104, 464, 650, 1952, 130304, 522752 ... {{OEIS|A088831}}. 
Many of these numbers are of the form 2&lt;sup&gt;''n''−1&lt;/sup&gt;(2&lt;sup&gt;''n''&lt;/sup&gt; − 3) where 2&lt;sup&gt;''n''&lt;/sup&gt; − 3 is prime (instead of  2&lt;sup&gt;''n''&lt;/sup&gt; − 1 with [[perfect number]]s). In addition, [[Almost perfect number|numbers exist]] where the sum of all the divisors ''σ''(''n'') is equal to 2n - 1, such as the powers of 2.

[[Betrothed numbers]] relate to quasiperfect numbers like [[amicable numbers]] relate to perfect numbers.

==Notes==
&lt;references/&gt;

== References ==
* {{cite journal|first1=E. |last1=Brown
|first2=H. |last2=Abbott
|first3=C. |last3=Aull
|first4=D. |last4=Suryanarayana
|title=Quasiperfect numbers
|journal=Acta Arith.
|year=1973
|volume=22
|pages=439–447
|mr=0316368
|url=http://matwbn.icm.edu.pl/ksiazki/aa/aa22/aa2245.pdf |doi=10.4064/aa-22-4-439-447
}}
* {{cite journal 
| last=Kishore 
| first=Masao 
| title=Odd integers ''N'' with five distinct prime factors for which 2−10&lt;sup&gt;−12&lt;/sup&gt; &lt; σ(''N'')/''N'' &lt; 2+10&lt;sup&gt;−12&lt;/sup&gt; 
| journal=[[Mathematics of Computation]] 
| volume=32 
| pages=303–309 
| year=1978 
| issn=0025-5718 
| zbl=0376.10005 
| mr=0485658
| url=http://www.ams.org/journals/mcom/1978-32-141/S0025-5718-1978-0485658-X/S0025-5718-1978-0485658-X.pdf 
| doi=10.2307/2006281
}}
* {{cite journal|first1=Graeme L.
|last1=Cohen|title= On odd perfect numbers (ii), multiperfect numbers and quasiperfect numbers 
|year=1980
|journal=J. Austral. Math. Soc., Ser. A 
|volume=29 
|pages=369–384 
|doi=10.1017/S1446788700021376 
| mr=0569525 
| zbl=0425.10005 
| issn=0263-6115
 }}
* {{cite book 
| author=James J. Tattersall 
| title=Elementary number theory in nine chapters 
| publisher=[[Cambridge University Press]] 
| isbn=0-521-58531-7 
| year=1999 
| pages=147 
| zbl=0958.11001 }}
* {{cite book
 | last = Guy
 | first = Richard
 | authorlink = Richard K. Guy
 | year = 2004
 | title = Unsolved Problems in Number Theory, third edition
 |page=74
 | publisher = [[Springer-Verlag]]
 | isbn=0-387-20860-7
}}
* {{cite book
 | editor1-last=Sándor 
| editor1-first=József 
| editor2-last=Mitrinović 
| editor2-first=Dragoslav S. 
| editor3-last=Crstici 
|editor3-first=Borislav 
| title=Handbook of number theory I 
| location=Dordrecht 
| publisher=[[Springer-Verlag]] 
| year=2006 
| isbn=1-4020-4215-9 
| zbl=1151.11300 
| pages=109–110
}}

{{Divisor classes}}
{{Classes of natural numbers}}

[[Category:Divisor function]]
[[Category:Integer sequences]]
[[Category:Unsolved problems in mathematics]]


{{numtheory-stub}}</text>
      <sha1>91kuzoq3p7dw0co0ohc202pp5cy2ugi</sha1>
    </revision>
  </page>
  <page>
    <title>Regularization (mathematics)</title>
    <ns>0</ns>
    <id>2009061</id>
    <revision>
      <id>870227242</id>
      <parentid>868882739</parentid>
      <timestamp>2018-11-23T09:50:54Z</timestamp>
      <contributor>
        <ip>94.65.216.254</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="23639">[[File:Regularization.svg|frame|The green and blue functions both incur zero loss on the given data points. A learned model can be induced to prefer the green function, which may generalize better to more points drawn from the underlying unknown distribution, by adjusting &lt;math&gt;\lambda&lt;/math&gt;, the weight of the regularization term.]]
In [[mathematics]], [[statistics]], and [[computer science]], particularly in the fields of [[machine learning]] and [[inverse problem]]s, '''regularization''' is a process of introducing additional information in order to solve an [[ill-posed problem]] or to prevent [[overfitting]].&lt;ref&gt;{{cite journal |doi=10.1007/978-3-642-20192-9 |title=Statistics for High-Dimensional Data|series=Springer Series in Statistics |year=2011 |last1=Bühlmann |first1=Peter |last2=Van De Geer |first2=Sara |isbn=978-3-642-20191-2 |page=9 |quote=If p &gt; n, the ordinary least squares estimator is not unique and will heavily overfit the data. Thus, a form of complexity regularization will be necessary.}}&lt;/ref&gt;

== Introduction ==
In general, regularization is a technique that applies to objective functions in '''ill-posed''' optimization problems.

== Use of regularization in classification ==

One particular use of regularization is in the field of classification. Empirical learning of classifiers (learning from a finite data set) is always an underdetermined problem, because in general we are trying to infer a function of any &lt;math&gt;x&lt;/math&gt; given only some examples &lt;math&gt;x_1, x_2, ... x_n&lt;/math&gt;.

A '''regularization term''' (or '''regularizer''') &lt;math&gt;R(f)&lt;/math&gt; is added to a [[Loss functions for classification|loss function]]:
: &lt;math&gt;\min_f \sum_{i=1}^{n} V(f(x_i), y_i) + \lambda R(f)&lt;/math&gt;

where &lt;math&gt;V&lt;/math&gt; is an underlying loss function that describes the cost of predicting &lt;math&gt;f(x)&lt;/math&gt; when the label is &lt;math&gt;y&lt;/math&gt;, such as the [[Loss functions for classification#Square_loss|square loss]] or [[hinge loss]]; and &lt;math&gt;\lambda&lt;/math&gt; is a parameter which controls the importance of the regularization term. &lt;math&gt;R(f)&lt;/math&gt; is typically chosen to impose a penalty on the complexity of &lt;math&gt;f&lt;/math&gt;. Concrete notions of complexity used include restrictions for [[smooth function|smoothness]] and bounds on the [[normed vector space|vector space norm]].&lt;ref name=":0" /&gt;{{page needed|date=May 2017}}

A theoretical justification for regularization is that it attempts to impose [[Occam's razor]] on the solution (as depicted in the figure above, where the green function, the simpler one, may be preferred). From a [[Bayesian inference|Bayesian]] point of view, many regularization techniques correspond to imposing certain [[prior probability|prior]] distributions on model parameters.

Regularization can be used to learn simpler models, induce models to be sparse, introduce group structure{{clarify|reason=What is structure in the learning process?|date=October 2018}} into the learning problem, and more.

The same idea arose in many fields of [[science]]. For example, the [[least-squares method]] can be viewed as a very simple form of regularization {{Citation needed|reason=It is not clear at all how the basic (non-regularized) least-squares method can be viewed as a very simple form of regularization|date=December 2016}}. A simple form of regularization applied to [[integral equation]]s, generally termed [[Tikhonov regularization]] after [[Andrey Nikolayevich Tikhonov]], is essentially a trade-off between fitting the data and reducing a norm of the solution. More recently, non-linear regularization methods, including [[total variation regularization]], have become popular.

=== Generalization ===
{{Main|Generalization error}}

Regularization can be motivated as a technique to improve the generalizability of a learned model.

The goal of this learning problem is to find a function that fits or predicts the outcome (label) that minimizes the expected error over all possible inputs and labels. The expected error of a function &lt;math&gt;f_n&lt;/math&gt; is:

:&lt;math&gt; I[f_n] = \int_{X \times Y} V(f_n(x),y) \rho(x,y) \, dx \, dy &lt;/math&gt;

Typically in learning problems, only a subset of input data and labels are available, measured with some noise. Therefore, the expected error is unmeasurable, and the best surrogate available is the empirical error over the &lt;math&gt; N &lt;/math&gt; available samples:

:&lt;math&gt; I_S[f_n] = \frac{1}{n} \sum_{i=1}^N V(f_n(\hat x_i), \hat y_i) &lt;/math&gt;

Without bounds on the complexity of the function space (formally, the [[reproducing kernel Hilbert space]]) available, a model will be learned that incurs zero loss on the surrogate empirical error. If measurements (e.g. of &lt;math&gt;x_i&lt;/math&gt;) were made with noise, this model may suffer from [[overfitting]] and display poor expected error. Regularization introduces a penalty for exploring certain regions of the function space used to build the model, which can improve generalization.

== Tikhonov regularization ==
{{Main|Tikhonov regularization}}When learning a linear function &lt;math&gt;f&lt;/math&gt;, characterized by an unknown [[Vector space|vector]] &lt;math&gt;w&lt;/math&gt; such that &lt;math&gt;f(x) = w \cdot x&lt;/math&gt;, the &lt;math&gt;L_2&lt;/math&gt;-norm loss {{clarify inline|text=corresponds to|date=May 2017}} Tikhonov regularization. This is one of the most common forms of regularization, is also known as [[ridge regression]], and is expressed as:

:&lt;math&gt;\min_w \sum_{i=1}^{n} V(\hat x_i \cdot w, \hat y_i) + \lambda \|w\|_{2}^{2}&lt;/math&gt;

In the case of a general function, we take the norm of the function in its [[reproducing kernel Hilbert space]]:

:&lt;math&gt;\min_f \sum_{i=1}^{n} V(f(\hat x_i), \hat y_i) + \lambda \|f\|_{\mathcal{H}}^{2}&lt;/math&gt;

As the &lt;math&gt;L_2&lt;/math&gt; norm is [[Differentiable function#Differentiability in higher dimensions|differentiable]], learning problems using Tikhonov regularization can be solved by [[gradient descent]].

=== Tikhonov-regularized least squares ===
The learning problem with the [[least squares]] loss function and Tikhonov regularization can be solved analytically. Written in matrix form, the optimal &lt;math&gt;w&lt;/math&gt; will be the one for which the gradient of the loss function with respect to &lt;math&gt;w&lt;/math&gt; is 0.

:&lt;math&gt;\min_w \frac{1}{n} (\hat X w - Y)^T(\hat X w - Y)+ \lambda \|w\|_{2}^{2}&lt;/math&gt;

:&lt;math&gt;\nabla_w = \frac{2}{n} \hat X^T (\hat X w -  Y) + 2 \lambda w&lt;/math&gt; &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; &lt;math&gt;\leftarrow&lt;/math&gt;This is the [[first-order condition]] for this optimization problem

:&lt;math&gt;0 = \hat X^T (\hat X w -  Y) + n \lambda w&lt;/math&gt;

:&lt;math&gt;w = (\hat X^T \hat X + \lambda n I)^{-1} (\hat X^T  Y)&lt;/math&gt;

By construction of the optimization problem, other values of &lt;math&gt;w&lt;/math&gt; would give larger values for the loss function. This could be verified by examining the second derivative &lt;math&gt;\nabla_{ww}&lt;/math&gt;.

During training, this algorithm takes &lt;math&gt;O(d^3 + nd^2)&lt;/math&gt; [[Time complexity|time]]. The terms correspond to the matrix inversion and calculating &lt;math&gt;X^T X&lt;/math&gt;, respectively. Testing takes &lt;math&gt;O(nd)&lt;/math&gt; time.

== Early stopping ==
{{Main|Early stopping}}
Early stopping can be viewed as regularization in time. Intuitively, a training procedure like gradient descent will tend to learn more and more complex functions as the number of iterations increases. By regularizing on time, the complexity of the model can be controlled, improving generalization.

In practice, early stopping is implemented by training on a training set and measuring accuracy on a statistically independent validation set. The model is trained until performance on the validation set no longer improves. The model is then tested on a testing set.

=== Theoretical motivation in least squares ===
Consider the finite approximation of [[Neumann series]] for an invertible matrix {{mvar|A}} where &lt;math&gt;\| I-A \| &lt; 1&lt;/math&gt;:

:&lt;math&gt;\sum_{i=0}^{T-1}(I-A)^i \approx A^{-1}&lt;/math&gt;

This can be used to approximate the analytical solution of unregularized least squares, if {{mvar|&amp;gamma;}} is introduced to ensure the norm is less than one.

:&lt;math&gt;w_T = \frac{\gamma}{n} \sum_{i=0}^{T-1} ( I - \frac{\gamma}{n} \hat X^T \hat X )^i \hat X^T \hat Y&lt;/math&gt;

The exact solution to the unregularized least squares learning problem will minimize the empirical error, but may fail to generalize and minimize the expected error. By limiting {{mvar|T}}, the only free parameter in the algorithm above, the problem is regularized on time which may improve its generalization.

The algorithm above is equivalent to restricting the number of gradient descent iterations for the empirical risk

:&lt;math&gt;I_s[w] = \frac{1}{2n} \| \hat X w - \hat Y \|^{2}_{\mathbb{R}^n}&lt;/math&gt;

with the gradient descent update:

:&lt;math&gt;\begin{align}
w_0 &amp;= 0
\\
w_{t+1} &amp;= (I - \frac{\gamma}{n} \hat X^T \hat X)w_t + \frac{\gamma}{n}\hat X^T \hat Y
\end{align}&lt;/math&gt;

The base case is trivial. The inductive case is proved as follows:

:&lt;math&gt;\begin{align}
w_{T} &amp;= (I - \frac{\gamma}{n} \hat X^T \hat X)\frac{\gamma}{n} \sum_{i=0}^{T-2}(I - \frac{\gamma}{n} \hat X^T \hat X )^i \hat X^T \hat Y  + \frac{\gamma}{n}\hat X^T \hat Y
\\
&amp;= \frac{\gamma}{n} \sum_{i=1}^{T-1}(I - \frac{\gamma}{n} \hat X^T \hat X )^i \hat X^T \hat Y  + \frac{\gamma}{n}\hat X^T \hat Y
\\
&amp;= \frac{\gamma}{n} \sum_{i=0}^{T-1}(I - \frac{\gamma}{n} \hat X^T \hat X )^i \hat X^T \hat Y
\end{align}&lt;/math&gt;

== Regularizers for sparsity ==
Assume that a dictionary &lt;math&gt;\phi_j&lt;/math&gt; with dimension &lt;math&gt;p&lt;/math&gt; is given such that a function in the function space can be expressed as:

:&lt;math&gt;f(x) = \sum_{j=1}^{p} \phi_j(x) w_j&lt;/math&gt;

[[File:Sparsityl1.png|thumb|A comparison between the L1 ball and the L2 ball in two dimensions gives an intuition on how L1 regularization achieves sparsity.]]

Enforcing a sparsity constraint on &lt;math&gt;w&lt;/math&gt; can lead to simpler and more interpretable models. This is useful in many real-life applications such as [[computational biology]]. An example is developing a simple predictive test for a disease in order to minimize the cost of performing medical tests while maximizing predictive power.

A sensible sparsity constraint is the [[Norm (mathematics)|&lt;math&gt;L_0&lt;/math&gt; norm]] &lt;math&gt;\|w\|_0&lt;/math&gt;, defined as the number of non-zero elements in &lt;math&gt;w&lt;/math&gt;. Solving a &lt;math&gt;L_0&lt;/math&gt; regularized learning problem, however, has been demonstrated to be [[NP-hardness|NP-hard]].&lt;ref&gt;{{Cite journal|last=Natarajan|first=B.|date=1995-04-01|title=Sparse Approximate Solutions to Linear Systems|url=http://epubs.siam.org/doi/abs/10.1137/S0097539792240406|journal=SIAM Journal on Computing|volume=24|issue=2|pages=227–234|doi=10.1137/S0097539792240406|issn=0097-5397}}&lt;/ref&gt;

The [[Norm (mathematics)|&lt;math&gt;L_1&lt;/math&gt;]] norm can be used to approximate the optimal [[Norm (mathematics)|&lt;math&gt;L_0&lt;/math&gt;]] norm via convex relaxation. It can be shown that the [[Norm (mathematics)|&lt;math&gt;L_1&lt;/math&gt;]] norm induces sparsity. In the case of least squares, this problem is known as [[Lasso (statistics)|LASSO]] in statistics and [[basis pursuit]] in signal processing.

:&lt;math&gt;\min_{w \in \mathbb{R}^p} \frac{1}{n} \|\hat X w - \hat Y \|^2 + \lambda \|w\|_{1}&lt;/math&gt;

[[File:Sparsityen.png|thumb|Elastic net regularization]]

[[Norm (mathematics)|&lt;math&gt;L_1&lt;/math&gt;]] regularization can occasionally produce non-unique solutions. A simple example is provided in the figure when the space of possible solutions lies on a 45 degree line. This can be problematic for certain applications, and is overcome by combining [[Norm (mathematics)|&lt;math&gt;L_1&lt;/math&gt;]]   with [[Norm (mathematics)|&lt;math&gt;L_2&lt;/math&gt;]] regularization in [[elastic net regularization]], which takes the following form:
:&lt;math&gt;\min_{w \in \mathbb{R}^p} \frac{1}{n} \|\hat X w - \hat Y \|^2 + \lambda (\alpha \|w\|_{1} + (1 - \alpha)\|w\|_{2}^{2}), \alpha \in [0, 1]&lt;/math&gt;

Elastic net regularization tends to have a grouping effect, where correlated input features are assigned equal weights.

Elastic net regularization is commonly used in practice and is implemented in many machine learning libraries.

=== Proximal methods ===
{{Main|Proximal gradient method}}While the [[Norm (mathematics)|&lt;math&gt;L_1&lt;/math&gt;]] norm does not result in an NP-hard problem, the [[Norm (mathematics)|&lt;math&gt;L_1&lt;/math&gt;]] norm is convex but is not strictly diffentiable due to the kink at x = 0. [[Subgradient method]]s which rely on the [[subderivative]] can be used to solve [[Norm (mathematics)|&lt;math&gt;L_1&lt;/math&gt;]] regularized learning problems. However, faster convergence can be achieved through proximal methods.

For a problem &lt;math&gt;\min_{w \in H} F(w) + R(w)&lt;/math&gt; such that &lt;math&gt;F&lt;/math&gt; is convex, continuous, differentiable, with Lipschitz continuous gradient (such as the least squares loss function), and &lt;math&gt;R&lt;/math&gt; is convex, continuous, and proper, then the proximal method to solve the problem is as follows. First define the [[proximal operator]]

:&lt;math&gt;\operatorname{prox}_R(v) = \operatorname{argmin}\limits_{w \in \mathbb{R}^D} \{ R(w) + \frac{1}{2}\|w-v\|^2\}, &lt;/math&gt;

and then iterate

:&lt;math&gt;w_{k+1} = \operatorname{prox}\limits_{\gamma, R}(w_k - \gamma \nabla F(w_k))&lt;/math&gt;

The proximal method iteratively performs gradient descent and then projects the result back into the space permitted by &lt;math&gt;R&lt;/math&gt;.

When &lt;math&gt;R&lt;/math&gt; is the [[Norm (mathematics)|&lt;math&gt;L_1&lt;/math&gt;]] regularizer, the proximal operator is equivalent to the soft-thresholding operator,

:&lt;math&gt;S_\lambda(v)f(n) = \begin{cases} v_i - \lambda, &amp; \text{if }v_i &gt; \lambda \\ 0, &amp; \text{if } v_i \in [-\lambda, \lambda] \\ v_i + \lambda, &amp; \text{if }v_i &lt; - \lambda \end{cases}&lt;/math&gt;

This allows for efficient computation.

=== Group sparsity without overlaps ===
Groups of features can be regularized by a sparsity constraint, which can be useful for expressing certain prior knowledge into an optimization problem.

In the case of a linear model with non-overlapping known groups, a regularizer can be defined:

:&lt;math&gt;R(w) = \sum_{g=1}^G \|w_g\|_g,&lt;/math&gt; where &lt;math&gt;\|w_g\|_g = \sqrt{\sum_{j=1}^{|G_g|}(w_g^j)^2}&lt;/math&gt;

This can be viewed as inducing a regularizer over the &lt;math&gt;L_2&lt;/math&gt; norm over members of each group followed by an &lt;math&gt;L_1&lt;/math&gt; norm over groups.

This can be solved by the proximal method, where the proximal operator is a block-wise soft-thresholding function:

: &lt;math&gt;( \operatorname{prox}\limits_{\lambda, R, g}(w_g))^j = \begin{cases} (w_g^j - \lambda \frac{w_g^j}{\|w_g\|_g}), &amp; \text{if } \|w_g\|_g &gt; \lambda \\ 0 &amp; \text{if } \|w_g\|_g \in [-\lambda, \lambda] \\ (w_g^j + \lambda \frac{w_g^j}{\|w_g\|_g}), &amp; \text{if } \|w_g\|_g &lt; - \lambda \end{cases}&lt;/math&gt;

=== Group sparsity with overlaps ===
The algorithm described for group sparsity without overlaps can be applied to the case where groups do overlap, in certain situations. This will likely result in some groups with all zero elements, and other groups with some non-zero and some zero elements.

If it is desired to preserve the group structure, a new regularizer can be defined:

:&lt;math&gt;R(w) = \inf \left\{ \sum_{g=1}^G \|w_g\|_g : w = \sum_{g=1}^G \bar w_g \right\}&lt;/math&gt;

For each &lt;math&gt;w_g&lt;/math&gt;, &lt;math&gt;\bar w_g&lt;/math&gt; is defined as the vector such that the restriction of &lt;math&gt;\bar w_g&lt;/math&gt; to the group &lt;math&gt;g&lt;/math&gt; equals &lt;math&gt;w_g&lt;/math&gt; and all other entries of &lt;math&gt;\bar w_g&lt;/math&gt; are zero. The regularizer finds the optimal disintegration of &lt;math&gt;w&lt;/math&gt; into parts. It can be viewed as duplicating all elements that exist in multiple groups. Learning problems with this regularizer can also be solved with the proximal method with a complication. The proximal operator cannot be computed in closed form, but can be effectively solved iteratively, inducing an inner iteration within the proximal method iteration.

== Regularizers for semi-supervised learning ==
{{Main|Semi-supervised learning}}

When labels are more expensive to gather than input examples, semi-supervised learning can be useful. Regularizers have been designed to guide learning algorithms to learn models that respect the structure of unsupervised training samples. If a symmetric weight matrix &lt;math&gt;W&lt;/math&gt; is given, a regularizer can be defined:

:&lt;math&gt;R(f) = \sum_{i,j} w_{ij}(f(x_i) - f(x_j))^2&lt;/math&gt;

If &lt;math&gt;W_{ij}&lt;/math&gt; encodes the result of some distance metric for points &lt;math&gt;x_i&lt;/math&gt; and &lt;math&gt;x_j&lt;/math&gt;, it is desirable that &lt;math&gt;f(x_i) \approx f(x_j)&lt;/math&gt;. This regularizer captures this intuition, and is equivalent to:

:&lt;math&gt;R(f) = \bar f^T L \bar f&lt;/math&gt; where &lt;math&gt;L = D- W&lt;/math&gt; is the [[Laplacian matrix]] of the graph induced by &lt;math&gt;W&lt;/math&gt;.

The optimization problem &lt;math&gt;\min_{f \in \mathbb{R}^m} R(f), m = u + l&lt;/math&gt; can be solved analytically if the constraint &lt;math&gt;f(x_i) = y_i&lt;/math&gt; is applied for all supervised samples. The labeled part of the vector &lt;math&gt;f&lt;/math&gt; is therefore obvious. The unlabeled part of &lt;math&gt;f&lt;/math&gt; is solved for by:

:&lt;math&gt;\min_{f_u \in \mathbb{R}^u} f^T L f = \min_{f_u \in \mathbb{R}^u} \{ f^T_u L_{uu} f_u + f^T_l L_{lu} f_u + f^T_u L_{ul} f_l \}&lt;/math&gt;

:&lt;math&gt;\nabla_{f_u} = 2L_{uu}f_u + 2L_{ul}Y&lt;/math&gt;

:&lt;math&gt;f_u = L_{uu}^\dagger (L_{ul} Y)&lt;/math&gt;

Note that the pseudo-inverse can be taken because &lt;math&gt;L_{ul}&lt;/math&gt; has the same range as &lt;math&gt;L_{uu}&lt;/math&gt;.

== Regularizers for multitask learning ==
{{Main|Multi-task learning}}In the case of multitask learning, &lt;math&gt;T&lt;/math&gt; problems are considered simultaneously, each related in some way. The goal is to learn &lt;math&gt;T&lt;/math&gt; functions, ideally borrowing strength from the relatedness of tasks, that have predictive power. This is equivalent to learning the matrix &lt;math&gt;W: T \times D&lt;/math&gt; .

=== Sparse regularizer on columns ===
:&lt;math&gt;R(w) = \sum_{i=1}^D \|W\|_{2,1}&lt;/math&gt;

This regularizer defines an L2 norm on each column and an L1 norm over all columns. It can be solved by proximal methods.

=== Nuclear norm regularization ===
:&lt;math&gt;R(w) = \|\sigma(W)\|_1&lt;/math&gt; where &lt;math&gt;\sigma(W)&lt;/math&gt; is the eigenvalues in the [[singular value decomposition]] of &lt;math&gt;W&lt;/math&gt;.

=== Mean-constrained regularization ===
:&lt;math&gt;R(f_1 \cdots f_T) = \sum_{t=1}^T \|f_t - \frac{1}{T} \sum_{s=1}^T f_s \|_{H_k}^2&lt;/math&gt;

This regularizer constrains the functions learned for each task to be similar to the overall average of the functions across all tasks. This is useful for expressing prior information that each task is expected to share similarities with each other task. An example is predicting blood iron levels measured at different times of the day, where each task represents a different person.

=== Clustered mean-constrained regularization ===
:&lt;math&gt;R(f_1 \cdots f_T) = \sum_{r=1}^C \sum_{t \in I(r)} \|f_t - \frac{1}{I(r)} \sum_{s \in I(r)} f_s\|_{H_k}^2&lt;/math&gt; where &lt;math&gt;I(r)&lt;/math&gt; is a cluster of tasks.

This regularizer is similar to the mean-constrained regularizer, but instead enforces similarity between tasks within the same cluster. This can capture more complex prior information. This technique has been used to predict [[Netflix]] recommendations. A cluster would correspond to a group of people who share similar preferences in movies.

=== Graph-based similarity ===
More general than above, similarity between tasks can be defined by a function. The regularizer encourages the model to learn similar functions for similar tasks.

:&lt;math&gt;R(f_1 \cdots f_T) = \sum_{t,s=1, t \neq s}^T \| f_t - f_s \|^2 M_{ts} &lt;/math&gt; for a given symmetric similarity matrix &lt;math&gt;M&lt;/math&gt;.

== Other uses of regularization in statistics and machine learning ==
[[Bayesian model comparison|Bayesian learning]] methods make use of a [[prior probability]] that (usually) gives lower probability to more complex models. Well-known model selection techniques include the [[Akaike information criterion]] (AIC), [[minimum description length]] (MDL), and the [[Bayesian information criterion]] (BIC). Alternative methods of controlling overfitting not involving regularization include [[cross-validation (statistics)|cross-validation]].

Examples of applications of different methods of regularization to the [[linear model]] are:
{|class="wikitable sortable"
!Model!!Fit measure!!Entropy measure&lt;ref name=":0"&gt;{{cite book|last1=Bishop|first1=Christopher M.|title=Pattern recognition and machine learning|date=2007|publisher=Springer|location=New York|isbn=978-0387310732|edition=Corr. printing.}}&lt;/ref&gt;&lt;ref&gt;{{cite book|last1=Duda|first1=Richard O.|title=Pattern classification + computer manual : hardcover set|date=2004|publisher=Wiley|location=New York [u.a.]|isbn=978-0471703501|edition=2.}}&lt;/ref&gt;
|-
|[[Akaike information criterion|AIC]]/[[Bayesian information criterion|BIC]]||&lt;math&gt;\|Y-X\beta\|_2&lt;/math&gt;||&lt;math&gt;\|\beta\|_0&lt;/math&gt;
|-
|[[Ridge regression]]&lt;ref name="ridge"&gt;{{Cite journal
 |title=Ridge regression: Biased estimation for nonorthogonal problems
 |author=Arthur E. Hoerl
 |author2=Robert W. Kennard
 |journal=Technometrics
 |volume=12
 |issue=1
 |year=1970
 |pages=55–67
 |doi=10.2307/1267351
}}&lt;/ref&gt; || &lt;math&gt;\|Y-X\beta\|_2&lt;/math&gt; || &lt;math&gt;\|\beta\|_2&lt;/math&gt;
|-
|[[Lasso (statistics)|Lasso]]&lt;ref&gt;{{Cite journal
 | last = Tibshirani
 | first = Robert
 | title = Regression Shrinkage and Selection via the Lasso
 | journal = [[Journal of the Royal Statistical Society, Series B]]
 | year = 1996
 | volume = 58
 | issue = 1
 | pages = 267&amp;ndash;288
 | url = http://www-stat.stanford.edu/~tibs/ftp/lasso.ps
 | format = [[PostScript]]
 | accessdate = 2009-03-19
 | mr = 1379242
}}&lt;/ref&gt;
|&lt;math&gt;\|Y-X\beta\|_2&lt;/math&gt;||&lt;math&gt;\|\beta\|_1&lt;/math&gt;
|-
|[[Basis pursuit denoising]] || &lt;math&gt;\|Y-X\beta\|_2&lt;/math&gt; || &lt;math&gt;\lambda\|\beta\|_1&lt;/math&gt;
|-
|Rudin–Osher–Fatemi model (TV) || &lt;math&gt;\|Y-X\beta\|_2&lt;/math&gt; || &lt;math&gt;\lambda\|\nabla\beta\|_1&lt;/math&gt;
|-
| [[Potts model]] || &lt;math&gt;\|Y-X\beta\|_2&lt;/math&gt; || &lt;math&gt;\lambda\|\nabla\beta\|_0&lt;/math&gt;
|-
|RLAD&lt;ref&gt;{{Cite conference
 | author = Li Wang, Michael D. Gordon &amp; Ji Zhu
 | title = Regularized Least Absolute Deviations Regression and an Efficient Algorithm for Parameter Tuning
 | booktitle = Sixth International Conference on Data Mining
 | date= 2006
 | pages = 690&amp;ndash;700
 | doi = 10.1109/ICDM.2006.134
}}&lt;/ref&gt;
| &lt;math&gt;\|Y-X\beta\|_1&lt;/math&gt; || &lt;math&gt;\|\beta\|_1&lt;/math&gt;
|-
|Dantzig Selector&lt;ref&gt;{{Cite journal
 | last = Candes
 | first = Emmanuel | authorlink = Emmanuel Candès
 |author2=Tao, Terence |authorlink2=Terence Tao 
 | title = The Dantzig selector: Statistical estimation when ''p'' is much larger than ''n''
 | journal = Annals of Statistics
 | year = 2007
 | volume = 35
 | issue = 6
 | pages = 2313&amp;ndash;2351
 | doi = 10.1214/009053606000001523
 | mr = 2382644
 | arxiv = math/0506081
}}&lt;/ref&gt;
|&lt;math&gt;\|X^\top (Y-X\beta)\|_\infty&lt;/math&gt;||&lt;math&gt;\|\beta\|_1&lt;/math&gt;
|-
|SLOPE&lt;ref&gt;
{{Cite arxiv
 | author = Małgorzata Bogdan, Ewout van den Berg, Weijie Su &amp; Emmanuel J. Candes
 | title = Statistical estimation and testing via the ordered L1 norm
 | year = 2013
 | eprint = 1310.1969
| class = stat.ME
 }}&lt;/ref&gt;
|&lt;math&gt;\|Y-X\beta\|_2&lt;/math&gt; || &lt;math&gt;\sum_{i=1}^p \lambda_i|\beta|_{(i)}&lt;/math&gt;
|}

== See also ==
* [[Bayesian interpretation of regularization]]
* [[Bias–variance tradeoff]]
* [[Matrix regularization]]
* [[Regularization by spectral filtering]]
* [[Regularized least squares]]

==Notes==
{{Reflist}}

==References==
* A. Neumaier, Solving ill-conditioned and singular linear systems: A tutorial on regularization, SIAM Review 40 (1998), 636–666. Available in [http://www.mat.univie.ac.at/~neum/ms/regtutorial.pdf pdf] from [http://www.mat.univie.ac.at/~neum/ author's website].

[[Category:Mathematical analysis]]
[[Category:Inverse problems]]</text>
      <sha1>8ushw3xw16d2yi81th3xzaxgsd9b5tw</sha1>
    </revision>
  </page>
  <page>
    <title>Relativity (M. C. Escher)</title>
    <ns>0</ns>
    <id>977922</id>
    <revision>
      <id>831389392</id>
      <parentid>831388992</parentid>
      <timestamp>2018-03-20T10:04:18Z</timestamp>
      <contributor>
        <username>Daiyusha</username>
        <id>26599585</id>
      </contributor>
      <minor/>
      <comment>Reverted 1 edit by [[Special:Contributions/42.62.222.186|42.62.222.186]] ([[User talk:42.62.222.186|talk]]) to last revision by ClueBot NG. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2527">{{Unreferenced|date=November 2015}}
{{Artwork
| image_file=Escher's_Relativity.jpg
| title=Relativity
| artist=M. C. Escher
| year=1953
| type=[[Lithography|lithograph]]
| height_metric = 27.7
| width_metric = 29.2
| metric_unit = cm
}}
'''''Relativity'''''  is a [[Lithography|lithograph]] print by the [[Netherlands|Dutch]] artist [[M. C. Escher]], first printed in December 1953. 

It depicts a world in which the normal laws of [[gravity]] do not apply. The architectural structure seems to be the centre of an idyllic community, with most of its inhabitants casually going about their ordinary business, such as dining. There are windows and doorways leading to park-like outdoor settings. All of the figures are dressed in identical attire and have featureless bulb-shaped heads. Identical characters such as these can be found in many other Escher works. 

In the world of ''Relativity'', there are three sources of gravity, each being [[orthogonal]] to the two others. Each inhabitant lives in one of the [[gravity well]]s, where normal physical laws apply. There are sixteen characters, spread between each gravity source, six in one and five each in the other two. The apparent confusion of the lithograph print comes from the fact that the three gravity sources are depicted in the same space.

The structure has seven stairways, and each stairway can be used by people who belong to two different gravity sources. This creates interesting phenomena, such as in the top stairway, where two inhabitants use the same stairway in the same direction and on the same side, but each using a different face of each step; thus, one descends the stairway as the other climbs it, even while moving in the same direction nearly side-by-side. In the other stairways, inhabitants are depicted as climbing the stairways upside-down, but based on their own gravity source, they are climbing normally.

Each of the three parks belongs to one of the gravity wells.  All but one of the doors seem to lead to basements below the parks. Though physically possible, such basements are certainly unusual and add to the surreal effect of the picture.

This is one of Escher’s most popular works and has been used in a [[M. C. Escher in popular culture#Pop culture references to Relativity|variety of ways]].

== References ==
{{Reflist}}


{{M. C. Escher|state=expanded}}
{{Mathematical art|state=expanded}}

[[Category:1953 prints]]
[[Category:Fictional dimensions]]
[[Category:Mathematical artworks]]
[[Category:Works by M. C. Escher]]</text>
      <sha1>79ody83plne49ddkycdacn1my8009by</sha1>
    </revision>
  </page>
  <page>
    <title>Robert Frucht</title>
    <ns>0</ns>
    <id>27069553</id>
    <revision>
      <id>857357767</id>
      <parentid>857343659</parentid>
      <timestamp>2018-08-31T05:13:11Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>Removing from [[Category:German mathematicians]] using [[c:Help:Cat-a-lot|Cat-a-lot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5522">'''Robert Wertheimer Frucht''' (later known as '''Roberto Frucht''') (9 August 1906 – 26 June 1997)&lt;ref name="nams"&gt;{{citation|title=Mathematics People|journal=Notices of the AMS|date=October 1997|volume=44|issue=9|url=http://www.ams.org/notices/199709/people.pdf|pages=1111–1113}}.&lt;/ref&gt;&lt;ref name="gg"&gt;[http://sansanos.us/pictures/Profesores/Dr.RobertoFrucht.html Biography of Frucht] (in Spanish), Walter Gaete and Raúl González, retrieved 2010-04-22.&lt;/ref&gt; was a [[Germans|German]]-[[Chile]]an [[mathematician]]; his research specialty was [[graph theory]] and the [[graph automorphism|symmetries of graphs]].

[[File:Frucht planar Lombardi.svg|thumb|150px|The [[Frucht graph]].]]
In 1908, Frucht's family moved from [[Brno|Brünn]], [[Austria-Hungary]] (now in the [[Czech Republic]]), where he was born, to [[Berlin]].&lt;ref name="gg"/&gt; Frucht entered the [[University of Berlin]] in 1924 with an interest in [[differential geometry]], but switched to [[group theory]] under the influence of his [[doctoral advisor]], [[Issai Schur]]; he received his Ph.D. in 1931.&lt;ref name="how"&gt;{{citation|first=R.|last=Frucht|title=How I became interested in graphs and groups|journal=Journal of Graph Theory|volume=6|issue=2|year=1982|pages=101–104|doi=10.1002/jgt.3190060203}}.&lt;/ref&gt;&lt;ref&gt;{{mathgenealogy|name=Robert Frucht|id=17971}}.&lt;/ref&gt; Unable to find academic employment in Germany due to his [[Jews|Jewish]] descent, he became an [[actuary]] in [[Trieste]], but left Italy in 1938 because of the racial laws that came into effect at that time.&lt;ref name="how"/&gt;&lt;ref name="rss"/&gt; He moved to [[Argentina]], where relatives of his wife lived, and attempted to move from there to the United States, but his employment outside academia prevented him from obtaining the necessary visa.&lt;ref name="gg"/&gt;&lt;ref name="rss"&gt;{{citation|title=Mathematicians fleeing from Nazi Germany: individual fates and global impact|first=Reinhard|last=Siegmund-Schultze|publisher=Princeton University Press|year=2009|isbn=978-0-691-14041-4|pages=9, 132, 305}}.&lt;/ref&gt; At the same time [[Robert Breusch]], another German mathematician who had been working in Chile for three years but was leaving for the U.S., invited Frucht to fill his position at [[Federico Santa María Technical University]] in [[Valparaiso]], [[Chile]], where Frucht found an academic home beginning in 1939.&lt;ref name="nams"/&gt;&lt;ref name="gg"/&gt;&lt;ref&gt;{{citation|contribution=Robert Breusch|title=Memorial Minutes|publisher=[[Amherst College]]|url=https://www.amherst.edu/academiclife/dean_faculty/facmeetings/memorialminutes#Breusch|accessdate=2010-04-24|first1=David|last1=Armacost|first2=James|last2=Denton|first3=Robert|last3=Romer|first4=Dudley|last4= Towne}}.&lt;/ref&gt; At Santa María, Frucht became dean of the faculty of mathematics and physics from 1948 to 1968, and retired to become an emeritus professor in 1970.&lt;ref name="gg"/&gt;

Frucht is known for [[Frucht's theorem]], the result that every group can be realized as the group of [[graph automorphism|symmetries]] of an [[undirected graph]],&lt;ref name="f38"&gt;{{Citation | last1=Frucht | first1=R. | title=Herstellung von Graphen mit vorgegebener abstrakter Gruppe. | url=http://www.numdam.org/item?id=CM_1939__6__239_0 | language=German | zbl=0020.07804 | year=1939 | journal=Compositio Mathematica | issn=0010-437X | volume=6 | pages=239–250}}.&lt;/ref&gt;&lt;ref&gt;{{Citation | last1=Frucht | first1=R. | title=Graphs of degree three with a given abstract group | url=http://cms.math.ca/cjm/v1/p365 | doi = 10.4153/CJM-1949-033-6 | mr=0032987 | year=1949 | journal=[[Canadian Journal of Mathematics]] | issn=0008-414X | volume=1 | pages=365–378}}.&lt;/ref&gt; and for the [[Frucht graph]], one of the two smallest [[cubic graph]]s without any nontrivial symmetries. [[LCF notation]], a method for describing cubic [[Hamiltonian graph]]s, was named for the initials of [[Joshua Lederberg]], [[H. S. M. Coxeter]], and Frucht, its key developers.&lt;ref&gt;{{citation|last=Frucht|first=R.|title=A canonical representation of trivalent Hamiltonian graphs|journal=Journal of Graph Theory|volume=1|pages=45–60|year=1976|issue=1|doi=10.1002/jgt.3190010111}}.&lt;/ref&gt;

Frucht was elected to the Chilean Academy of Sciences as a corresponding member in 1979.&lt;ref name="gg"/&gt;
A special issue of the ''[[Journal of Graph Theory]]'' was published in Frucht's honor in 1982,&lt;ref name="gg"/&gt;&lt;ref&gt;{{citation|first=F.|last=Harary|authorlink=Frank Harary|title=Homage to Roberto Frucht|journal=Journal of Graph Theory|volume=6|issue=2|year=1982|pages=99–100|doi=10.1002/jgt.3190060202}}.&lt;/ref&gt; and another special issue of the journal ''Scientia, Series A'' (the journal of the mathematics department of Federico Santa María Technical University) was published in honor of his 80th birthday in 1986.&lt;ref name="gg"/&gt;&lt;ref&gt;{{citation|first=Carlos|last=González de la Fuente|title=Roberto W. Frucht: The mathematician, the teacher, the man|journal=Scientia Ser. A Math. Sci. Univ. Técnica Federico Santa María (Valparaíso)|volume=1|year=1988|pages=iii–v}}.&lt;/ref&gt;

==References==
{{reflist}}

{{authority control}}

{{DEFAULTSORT:Frucht, Robert}}
[[Category:1906 births]]
[[Category:1997 deaths]]
[[Category:People from Brno]]
[[Category:People from the Margraviate of Moravia]]
[[Category:Czech Jews]]
[[Category:Chilean people of Czech-Jewish descent]]
[[Category:Chilean mathematicians]]
[[Category:Graph theorists]]
[[Category:20th-century German mathematicians]]
[[Category:20th-century German people]]
[[Category:20th-century Chilean people]]</text>
      <sha1>7e30fgcwtvyhc1bk6s5da9p4xsdzzlw</sha1>
    </revision>
  </page>
  <page>
    <title>Static timing analysis</title>
    <ns>0</ns>
    <id>2546747</id>
    <revision>
      <id>868747620</id>
      <parentid>861093062</parentid>
      <timestamp>2018-11-14T05:16:09Z</timestamp>
      <contributor>
        <ip>103.252.26.12</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10086">'''Static timing analysis''' (STA) is a simulation method of computing the expected timing of a digital circuit without requiring a simulation of the full circuit.

High-performance [[integrated circuit]]s have traditionally been characterized by the [[clock frequency]] at which they operate. Measuring the ability of a circuit to operate at the specified speed requires an ability to measure, during the design process, its delay at numerous steps. Moreover, [[delay calculation]] must be incorporated into the inner loop of timing optimizers at various phases of design, such as [[logic synthesis]], layout ([[Placement (EDA)|placement]] and [[Routing (EDA)|routing]]), and in in-place optimizations performed late in the design cycle. While such timing measurements can theoretically be performed using a rigorous [[SPICE|circuit simulation]], such an approach is liable to be too slow to be practical. Static timing analysis plays a vital role in facilitating the fast and reasonably accurate measurement of circuit timing. The speedup comes from the use of simplified timing models and by mostly ignoring logical interactions in circuits. This has become a mainstay of design over the last few decades.

One of the earliest descriptions of a static timing approach was based on the [[Program Evaluation and Review Technique]] (PERT), in 1966.&lt;ref&gt;{{cite journal  |title=PERT as an aid to logic design
 |author1=Kirkpatrick, TI  |author2=Clark, NR
  |lastauthoramp=yes | journal=IBM Journal of Research and Development
 | volume=10
  |issue=2
  |pages=135–141
  |year=1966
  |publisher=IBM Corp.
  |url=http://dl.acm.org/citation.cfm?id=1662478
  | doi=10.1147/rd.102.0135}}&lt;/ref&gt;  More modern versions and algorithms appeared in the early 1980s.&lt;ref&gt;{{cite conference 
 |title=Verification of timing constraints on large digital systems
  |author=McWilliams, T.M.
  |booktitle=Design Automation, 1980. 17th Conference on
  |pages=139–147
  |year=1980
  |publisher=IEEE
 |url=https://e-reports-ext.llnl.gov/pdf/185870.pdf}}&lt;/ref&gt;&lt;ref&gt;{{cite news 
 |title=An integrated LSI design aids system
 |author1=G. Martin |author2=J. Berrie |author3=T. Little |author4=D. Mackay |author5=J. McVean |author6=D. Tomsett |author7=L. Weston |doi=10.1016/S0026-2692(81)80259-5
 |journal=Microelectronics Journal
 |volume=12
 |issue= 4
 |year=1981
 |url=http://www.sciencedirect.com/science/article/pii/S0026269281802595}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |title=Timing analysis of computer hardware
  |author=Hitchcock, R. and Smith, G.L. and Cheng, D.D.
  |journal=IBM Journal of Research and Development
  |volume=26
  |number=1
  |pages=100–105
  |year=1982
  |publisher=IBM
  |citeseerx = 10.1.1.83.2093
 |doi=10.1147/rd.261.0100}}&lt;/ref&gt;

== Purpose ==
In a [[Synchronous circuit|synchronous digital system]], data is supposed to move in [[lockstep (computing)|lockstep]], advancing one stage on each tick of the [[clock signal]]. This is enforced by synchronizing elements such as [[Flip-flop (electronics)|flip-flops]] or [[Latch (electronic)|latches]], which copy their input to their output when instructed to do so by the clock. Only two kinds of timing errors are possible in such a system:
*A '''setup time violation''', when a signal arrives too late, and misses the time when it should advance;
*A '''hold time violation''', when an input signal changes too soon after the clock's active transition.

The time when a signal arrives can vary due to many reasons. The input data may vary, the circuit may perform different operations, the temperature and voltage may change, and there are manufacturing differences in the exact construction of each part. The main goal of static timing analysis is to verify that despite these possible variations, all signals will arrive neither too early nor too late, and hence proper circuit operation can be assured.

Since STA is capable of verifying every path, it can detect other problems like [[glitches]], slow paths and [[clock skew]].

== Definitions ==
* The '''critical path''' is defined as the path between an input and an output with the maximum delay. Once the circuit timing has been computed by one of the techniques listed below, the critical path can easily be found by using a '''traceback method'''.
* The '''arrival time''' of a signal is the time elapsed for a signal to arrive at a certain point. The reference, or time 0.0, is often taken as the [[arrival time]] of a clock signal. To calculate the arrival time, [[delay calculation]] of all the components in the path will be required. Arrival times, and indeed almost all times in timing analysis, are normally kept as a pair of values - the earliest possible time at which a signal can change, and the latest.
* Another useful concept is '''required time'''. This is the latest time at which a signal can arrive without making the clock cycle longer than desired.  The computation of the required time proceeds as follows: at each primary output, the required times for rise/fall are set according to the specifications provided to the circuit. Next, a backward topological traversal is carried out, processing each gate when the required times at all of its fanouts are known.
* The '''slack''' associated with each connection is the difference between the required time and the arrival time. A ''positive slack'' '''s''' at some node implies that the arrival time at that node may be increased by '''s''', without affecting the overall delay of the circuit. Conversely, ''negative slack'' implies that a path is too slow, and the path must be sped up (or the reference signal delayed) if the whole circuit is to work at the desired speed.

==Corners and STA==
Quite often, designers will want to qualify their design across many conditions. Behavior of an electronic circuit is often dependent on various factors in its environment like temperature or local voltage variations. In such a case either STA needs to be performed for more than one such set of conditions, or STA must be prepared to work with a range of possible delays for each component, as opposed to a single value. 

With proper techniques, the patterns of condition variations are characterized and their extremes are recorded. Each extreme condition can be termed as a corner. Let us say, each extreme in cell characteristics as ‘PVT corner’ and net characteristics as ‘extraction corner’. Then each combination pattern of PVT extraction corners is referred to as a ‘timing corner’ as it represents a point where timing will be extreme. If the design works at each extreme condition, then under the assumption of [[monotonic]] behavior, the design is also qualified for all intermediate points.

The use of corners in static timing analysis has several limitations. It may be overly optimistic, since it assumes perfect tracking: if one gate is fast, all gates are assumed fast, or if the voltage is low for one gate, it is also low for all others. Corners may also be overly pessimistic, for the worst case corner may seldom occur. In an IC, for example, it may not be rare to have one metal layer at the thin or thick end of its allowed range, but it would be very rare for all 10 layers to be at the same limit, since they are manufactured independently. Statistical STA, which replaces delays with distributions, and tracking with correlation, offers a more sophisticated approach to the same problem.

==The most prominent techniques for STA ==
In static timing analysis, the word ''static'' alludes to the fact that this timing analysis is carried out in an input-independent manner, and purports to find the worst-case delay of the circuit over all possible input combinations. The computational efficiency (linear in the number of edges in the graph) of such an approach has resulted in its widespread use, even though it has some limitations. A method that is commonly referred to as [[Program Evaluation and Review Technique|PERT]] is popularly used in STA. However, PERT is a misnomer, and the so-called PERT method discussed in most of the literature on timing analysis refers to the [[critical path method]] (CPM) that is widely used in project management. While the CPM-based methods are the dominant ones in use today, other methods for traversing circuit graphs, such as [[depth-first search]], have been used by various timing analyzers.

== Interface Timing Analysis ==
Many of the common problems in chip designing are related to interface timing between different components of the design. These can arise because of many factors including incomplete simulation models, lack of test cases to properly verify interface timing, requirements for synchronization, incorrect interface specifications, and lack of designer understanding of a component supplied as a 'black box'. There are specialized CAD tools designed explicitly to analyze interface timing, just as there are specific CAD tools to verify that an implementation of an interface conforms to the functional specification (using techniques such as [[model checking]]).

==Statistical static timing analysis (SSTA) ==
[[Statistical static timing analysis]] (SSTA) is a procedure that is becoming increasingly necessary to handle the complexities of process and environmental variations in integrated circuits.

==See also==
* [[Dynamic timing verification]]
* [[Electronic design automation]]
* [[Integrated circuit design]]
* [[Logic analyzer]]—for verification of STA
* [[Logic simulation]]
* [[Simulation]]
* [[Timing margin]]
* [[Worst-case execution time]]
* [[Signoff (electronic design automation)]]

== Notes ==
{{reflist}}

== References ==
*''Electronic Design Automation For Integrated Circuits Handbook'', by Lavagno, Martin, and Scheffer, {{ISBN|0-8493-3096-3}} A survey of the field. This article was derived from Volume II, Chapter 8, 'Static Timing Analysis' by Sachin Sapatnekar, with permission.
*''Static Timing Analysis for Nanometer Designs'', by R. Chadha and J. Bhasker, {{ISBN|978-0-387-93819-6}}, Springer, 2009.

[[Category:Timing in electronic circuits]]
[[Category:Formal methods]]</text>
      <sha1>h3hvpn4gdq31flx1026iitap1dx8lfl</sha1>
    </revision>
  </page>
  <page>
    <title>Τ-additivity</title>
    <ns>0</ns>
    <id>34283581</id>
    <revision>
      <id>805165032</id>
      <parentid>791882854</parentid>
      <timestamp>2017-10-13T14:13:52Z</timestamp>
      <contributor>
        <username>Geodud</username>
        <id>22292892</id>
      </contributor>
      <minor/>
      <comment>Added "see also" with 3 entries.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="971">{{lowercase}}
{{orphan|date=January 2012}}
In [[mathematics]], in the field of [[measure theory]], '''τ-additivity''' is a certain property of [[Measure (mathematics)|measure]]s on [[topological space]]s.

A measure ''µ'' on a space ''X'', defined on a [[sigma-algebra]] Σ is said to be τ-additive, if for any [[Directed set|upward-directed]] family &lt;math&gt;\mathcal{G}\subseteq \Sigma&lt;/math&gt; of nonempty open sets, such that its [[Union (set theory)|union]] is in Σ, the measure of the union is the [[supremum]] of measures of elements of &lt;math&gt;\mathcal G&lt;/math&gt;, i.e.:
:&lt;math&gt;\mu\left(\bigcup \mathcal{G}\right)=\sup_{G\in\mathcal{G}}\mu(G)&lt;/math&gt;

== See also ==
* [[Sigma additivity]]
* [[Valuation (measure theory)]]
* [[Net (mathematics)]]

==References==
* {{citation|first=D.H.|last=Fremlin|title=Measure Theory, Volume 4|publisher=Torres Fremlin|year=2003|isbn=0-9538129-4-4}}.

{{DEFAULTSORT:T-additivity}}
[[Category:Measure theory]]


{{mathanalysis-stub}}</text>
      <sha1>h12ugk44btj7k91ez03r5b3n5zp5he9</sha1>
    </revision>
  </page>
</mediawiki>
