<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Bilateral key exchange</title>
    <ns>0</ns>
    <id>6096872</id>
    <revision>
      <id>718614491</id>
      <parentid>718606496</parentid>
      <timestamp>2016-05-04T15:20:10Z</timestamp>
      <contributor>
        <username>Gab4gab</username>
        <id>21417351</id>
      </contributor>
      <comment>add POV - emphasis is all on SWIFT using BKE, almost nothing about BKE itself</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1300">{{Unreferenced|date=May 2016}}
{{POV|date=May 2016}}
'''Bilateral Key Exchange''' ('''BKE''') was an encryption scheme utilized by [[SWIFT|The Society for Worldwide Interbank Financial Telecommunications]] ('''SWIFT''').

The scheme was retired on January 1, 2009 and has now been replaced by the [[Relationship Management Application (RMA)]]. All key management is now based on the SWIFT PKI that was implemented in SWIFT phase 2.

A Bilateral Key allowed secure communication across the [[SWIFT|SWIFT Network]]. The text of a [[SWIFT:Message Types]] and the authentication key were used to generate a [[Message Authentication Code]] or MAC. The MAC ensured the origin of a message and the authenticity of the message contents. This was normally accomplished by the exchange of various [[FIN Message|SWIFT Messages]] used specifically for establishing a communicating key pair.

BKE keys were generated either manually inside the SWIFT software, or automatically with the use of an SCR or Secure Card Reader.

Since 1994, the keys used in the card reader and the authentication keys themselves were 1024bit [[RSA (algorithm)|RSA]].

{{DEFAULTSORT:Bilateral Key Exchange}}
[[Category:Cryptographic protocols]]
[[Category:Society for Worldwide Interbank Financial Telecommunication]]


{{crypto-stub}}</text>
      <sha1>pms8okyxw5d5pynuz20p2cwuff3empv</sha1>
    </revision>
  </page>
  <page>
    <title>Bisection</title>
    <ns>0</ns>
    <id>152547</id>
    <revision>
      <id>863282558</id>
      <parentid>854569603</parentid>
      <timestamp>2018-10-09T20:21:01Z</timestamp>
      <contributor>
        <username>Loraof</username>
        <id>22399950</id>
      </contributor>
      <comment>/* Perpendicular bisectors */ link into reference</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17926">{{distinguish|Dissection}}
{{about||the bisection theorem in measure theory|Ham sandwich theorem|the root-finding method|Bisection method|other uses|Bisect (disambiguation)}}
[[Image:Bisectors.svg|right|thumb|Line DE bisects line AB at D, line EF is a perpendicular bisector of segment AD at C, and line EF is the interior bisector of right angle AED]]
In [[geometry]], '''bisection''' is the division of something into two equal or [[congruence (geometry)|congruent]] parts, usually by a [[line (mathematics)|line]], which is then called a ''bisector''. The most often considered types of bisectors are the ''segment bisector'' (a line that passes through the midpoint of a given segment) and the ''angle bisector'' (a line that passes through the apex of an angle, that divides it into two equal angles).

In [[three-dimensional space]], bisection is usually done by a plane, also called the ''bisector'' or ''bisecting plane''.

==Line segment bisector==
[[File:Perpendicular Bisector.gif|thumb|300px|Perpendicular Bisector]]

A [[line segment]] bisector passes through the [[midpoint]] of the segment.
Particularly important is the [[perpendicular]] bisector of a segment, which, according to its name, meets the segment at [[right angle]]s.  The perpendicular bisector of a segment also has the property that each of its points is [[equidistant]] from the segment's endpoints. Therefore, [[Voronoi diagram]] boundaries consist of segments of such lines or planes.

In classical geometry, the bisection is a simple [[compass and straightedge construction]], whose possibility depends on the ability to draw [[circle]]s of equal radii and different centers.  The segment is bisected by drawing intersecting circles of equal radius, whose centers are the endpoints of the segment and such that each circle goes through one endpoint.  The line determined by the points of intersection of the two circles is the perpendicular bisector of the segment, since it crosses the segment at its center. This construction is in fact used when constructing a line perpendicular to a given line at a given point: drawing an arbitrary circle whose center is that point, it intersects the line in two more points, and the perpendicular to be constructed is the one bisecting the segment defined by these two points.

[[Brahmagupta's theorem]] states that if a [[cyclic quadrilateral]] is [[Orthodiagonal quadrilateral|orthodiagonal]] (that is, has [[perpendicular]] [[diagonals]]), then the perpendicular to a side from the point of intersection of the diagonals always bisects the opposite side.

Algebraically, the perpendicular bisector of a line segment with endpoints &lt;math&gt;P_1(x_1,y_1)&lt;/math&gt; and &lt;math&gt;P_2(x_2,y_2)&lt;/math&gt; is given by the equation

:&lt;math&gt;(x-x_1)^2+(y-y_1)^2=(x-x_2)^2+(y-y_2)^2,&lt;/math&gt;

which states that the squared distance of a point on the bisector to one endpoint equals the squared distance from that point to the other endpoint.

==Angle bisector==
[[File:Bisection construction.gif|right|thumb|Bisection of an angle using a compass and straightedge]]

An [[angle]] bisector divides the angle into two angles with [[equality (mathematics)|equal]] measures.  An angle only has one bisector.  Each point of an angle bisector is equidistant from the sides of the angle.

The ''interior'' or ''internal bisector'' of an angle is the line, [[half-line]], or line segment that divides an angle of less than 180° into two equal angles.  The ''exterior'' or ''external bisector'' is the line that divides the [[supplementary angle]] (of 180° minus the original angle), formed by one side forming the original angle and the extension of the other side, into two equal angles.&lt;ref&gt;[http://mathworld.wolfram.com/ExteriorAngleBisector.html Weisstein, Eric W. "Exterior Angle Bisector." From MathWorld--A Wolfram Web Resource.]&lt;/ref&gt; 

To bisect an angle with [[straightedge and compass]], one draws a circle whose center is the vertex.  The circle meets the angle at two points: one on each leg.  Using each of these points as a center, draw two circles of the same size.  The intersection of the circles (two points) determines a line that is the angle bisector.

The proof of the correctness of this construction is fairly intuitive, relying on the symmetry of the problem. The [[trisecting the angle|trisection of an angle]] (dividing it into three equal parts) cannot be achieved with the compass and ruler alone (this was first proved by [[Pierre Wantzel]]).

The internal and external bisectors of an angle are [[perpendicular]]. If the angle is formed by the two lines given algebraically as  &lt;math&gt;l_1x+m_1y+n_1=0&lt;/math&gt; and &lt;math&gt;l_2x+m_2y+n_2=0,&lt;/math&gt; then the internal and external bisectors are given by the two equations&lt;ref&gt;Spain, Barry. ''Analytical Conics'', Dover Publications, 2007 (orig. 1957).&lt;/ref&gt;{{rp|p.15}}

:&lt;math&gt;\frac{l_1x+m_1y+n_1}{\sqrt{l_1^2+m_1^2}} = \pm \frac{l_2x+m_2y+n_2}{\sqrt{l_2^2+m_2^2}}.&lt;/math&gt;

===Triangle===
[[File:Incircle.svg|right]]

====Concurrencies and collinearities====

The interior angle bisectors of a [[triangle]] are [[concurrent lines|concurrent]] in a point called the [[incenter]] of the triangle, as seen in the diagram at right.

The bisectors of two [[exterior angle]]s and the bisector of the other [[interior angle]] are concurrent.&lt;ref name=Johnson/&gt;{{rp|p.149}}

Three intersection points, each of an external angle bisector with the opposite [[extended side]], are [[collinearity|collinear]] (fall on the same line as each other).&lt;ref name=Johnson/&gt;{{rp|p. 149}}

Three intersection points, two of them between an interior angle bisector and the opposite side, and the third between the other exterior angle bisector and the opposite side extended, are collinear.&lt;ref name=Johnson/&gt;{{rp|p. 149}}

==== Angle bisector theorem ====
{{main|Angle bisector theorem}}
[[Image:Triangle ABC with bisector AD.svg|thumb|240px|right|In this diagram, BD:DC = AB:AC.]]
The angle bisector theorem is concerned with the relative [[length]]s of the two segments that a [[triangle]]'s side is divided into by a line that bisects the opposite angle. It equates their relative lengths to the relative lengths of the other two sides of the triangle. 

====Lengths====

If the side lengths of a triangle are &lt;math&gt;a,b,c&lt;/math&gt;, the semiperimeter &lt;math&gt;s=(a+b+c)/2,&lt;/math&gt; and A is the angle opposite side &lt;math&gt;a&lt;/math&gt;, then the length of the internal bisector of angle A is&lt;ref name=Johnson&gt;Johnson, Roger A., ''Advanced Euclidean Geometry'', Dover Publ., 2007 (orig. 1929).&lt;/ref&gt;{{rp|p. 70}}

:&lt;math&gt; \frac{2 \sqrt{bcs(s-a)}}{b+c},&lt;/math&gt;

or in trigonometric terms,&lt;ref&gt;Oxman, Victor. "On the existence of triangles with given lengths of one side and two adjacent angle bisectors", ''Forum Geometricorum'' 4, 2004, 215–218. http://forumgeom.fau.edu/FG2004volume4/FG200425.pdf &lt;/ref&gt;

:&lt;math&gt;\frac{2bc}{b+c}\cos \frac{A}{2}.  &lt;/math&gt;

If the internal bisector of angle A in triangle ABC has length &lt;math&gt;t_a&lt;/math&gt; and if this bisector divides the side opposite A into segments of lengths ''m'' and ''n'', then&lt;ref name=Johnson/&gt;{{rp|p.70}}

:&lt;math&gt;t_a^2+mn = bc&lt;/math&gt;

where ''b'' and ''c'' are the side lengths opposite vertices B and C; and the side opposite A is divided in the proportion ''b'':''c''.

If the internal bisectors of angles A, B, and C have lengths &lt;math&gt;t_a, t_b,&lt;/math&gt; and &lt;math&gt;t_c&lt;/math&gt;, then&lt;ref&gt;Simons, Stuart.  ''Mathematical Gazette'' 93, March 2009, 115-116.&lt;/ref&gt;

:&lt;math&gt;\frac{(b+c)^2}{bc}t_a^2+ \frac{(c+a)^2}{ca}t_b^2+\frac{(a+b)^2}{ab}t_c^2 = (a+b+c)^2.&lt;/math&gt;

No two non-congruent triangles share the same set of three internal angle bisector lengths.&lt;ref&gt;Mironescu, P., and Panaitopol, L.,  "The existence of a triangle with prescribed angle bisector lengths", ''[[American Mathematical Monthly]]'' 101 (1994): 58–60.&lt;/ref&gt;&lt;ref&gt;[http://forumgeom.fau.edu/FG2008volume8/FG200828.pdf Oxman, Victor, "A purely geometric proof of the uniqueness of a triangle with prescribed angle bisectors", ''Forum Geometricorum'' 8 (2008): 197–200.]&lt;/ref&gt;

====Integer triangles====

There exist [[Integer triangle#Integer triangles with a rational angle bisector|integer triangles with a rational angle bisector]].

===Quadrilateral===

The internal angle bisectors of a [[Convex polygon|convex]] [[quadrilateral]] either form a [[cyclic quadrilateral]] (that is, the four intersection points of adjacent angle bisectors are [[concyclic points|concyclic]]),&lt;ref&gt;

Weisstein, Eric W. "Quadrilateral." From MathWorld--A Wolfram Web Resource. http://mathworld.wolfram.com/Quadrilateral.html&lt;/ref&gt;  or they are [[Concurrent lines|concurrent]]. In the latter case the quadrilateral is a [[tangential quadrilateral]]. 

====Rhombus====

Each diagonal of a [[rhombus]] bisects opposite angles.

====Ex-tangential quadrilateral====

The excenter of an [[ex-tangential quadrilateral]] lies at the intersection of six angle bisectors. These are the internal angle bisectors at two opposite vertex angles, the external angle bisectors (supplementary angle bisectors) at the other two vertex angles, and the external angle bisectors at the angles formed where the [[Extended side|extensions of opposite sides]] intersect.

===Parabola===
{{Main|Parabola#Tangent bisection property}}

The [[tangent]] to a [[parabola]] at any point bisects the angle between the line joining the point to the focus and the line from the point and [[perpendicular]] to the directrix.

==Bisectors of the sides of a polygon==

===Triangle===

====Medians====

Each of the three [[Median (geometry)|medians]] of a triangle is a line segment going through one [[Vertex (geometry)#Of a polytope|vertex]] and the midpoint of the opposite side, so it bisects that side (though not in general perpendicularly). The three medians intersect each other at the [[Centroid#Of triangle and tetrahedron|centroid]] of the triangle, which is its [[center of mass]] if it has uniform density; thus any line through a triangle's centroid and one of its vertices bisects the opposite side. The centroid is twice as close to the midpoint of any one side as it is to the opposite vertex.

====Perpendicular bisectors====

The interior [[perpendicular]] bisector of a side of a triangle is the segment, falling entirely on and inside the triangle, of the line that perpendicularly bisects that side. The three perpendicular bisectors of a triangle's three sides intersect at the [[circumcenter]] (the center of the circle through the three vertices). Thus any line through a triangle's circumcenter and perpendicular to a side bisects that side.

In an [[acute triangle]] the circumcenter divides the interior perpendicular bisectors of the two shortest sides in equal proportions. In an [[obtuse triangle]] the two shortest sides' perpendicular bisectors (extended beyond their opposite triangle sides to the circumcenter) are divided by their respective intersecting triangle sides in equal proportions.&lt;ref name=Mitchell&gt;Mitchell, Douglas W. (2013), "Perpendicular Bisectors of Triangle Sides", ''Forum Geometricorum'' 13, 53-59. http://forumgeom.fau.edu/FG2013volume13/FG201307.pdf&lt;/ref&gt;{{rp|Corollaries 5 and 6}}

For any triangle the interior perpendicular bisectors are given by &lt;math&gt;p_a=\tfrac{2aT}{a^2+b^2-c^2},&lt;/math&gt; &lt;math&gt;p_b=\tfrac{2bT}{a^2+b^2-c^2},&lt;/math&gt; and &lt;math&gt;p_c=\tfrac{2cT}{a^2-b^2+c^2},&lt;/math&gt; where the sides are &lt;math&gt;a \ge b \ge c&lt;/math&gt; and the area is &lt;math&gt;T.&lt;/math&gt;&lt;ref name=Mitchell/&gt;{{rp|Thm 2}}

===Quadrilateral===

The two [[Quadrilateral#Bimedians|bimedians]] of a [[Convex polygon|convex]] [[quadrilateral]] are the line segments that connect the midpoints of opposite sides, hence each bisecting two sides. The two bimedians and the line segment joining the midpoints of the diagonals are concurrent at a point called the "vertex centroid"  and are all bisected by this point.&lt;ref name=Altshiller-Court&gt;Altshiller-Court, Nathan, ''College Geometry'', Dover Publ., 2007.&lt;/ref&gt;{{rp|p.125}}

The four "maltitudes" of a convex quadrilateral are the perpendiculars to a side through the midpoint of the opposite side, hence bisecting the latter side. If the quadrilateral is [[Cyclic quadrilateral|cyclic]] (inscribed in a circle), these maltitudes are [[Concurrent lines|concurrent]] at (all meet at) a common point called the "anticenter".

[[Brahmagupta's theorem]] states that if a cyclic quadrilateral is [[Orthodiagonal quadrilateral|orthodiagonal]] (that is, has [[perpendicular]] [[diagonals]]), then the perpendicular to a side from the point of intersection of the diagonals always bisects the opposite side.

The [[perpendicular bisector construction of a quadrilateral|perpendicular bisector construction]] forms a quadrilateral from the perpendicular bisectors of the sides of another quadrilateral.

==Area bisectors and perimeter bisectors==

===Triangle===

There are an infinitude of lines that bisect the [[area]] of a [[triangle]]. Three of them are the [[Median (geometry)|medians]] of the triangle (which connect the sides' midpoints with the opposite vertices), and these are [[Concurrent lines|concurrent]] at the triangle's [[centroid]]; indeed, they are the only area bisectors that go through the centroid. Three other area bisectors are parallel to the triangle's sides; each of these intersects the other two sides so as to divide them into segments with the proportions &lt;math&gt;\sqrt{2}+1:1&lt;/math&gt;.&lt;ref name=Dunn&gt;Dunn, J. A., and Pretty, J. E., "Halving a triangle," ''[[Mathematical Gazette]]'' 56, May 1972, 105-108.&lt;/ref&gt; These six lines are concurrent three at a time: in addition to the three medians being concurrent, any one median is concurrent with two of the side-parallel area bisectors.

The [[Envelope (mathematics)|envelope]] of the infinitude of area bisectors is a [[Deltoid curve|deltoid]] (broadly defined as a figure with three vertices connected by curves that are concave to the exterior of the deltoid, making the interior points a non-convex set).&lt;ref name=Dunn/&gt; The vertices of the deltoid are at the midpoints of the medians; all points inside the deltoid are on three different area bisectors, while all points outside it are on just one. [http://www.se16.info/js/halfarea.htm]
The sides of the deltoid are arcs of [[hyperbola]]s that are [[asymptotic]] to the extended sides of the triangle.&lt;ref name=Dunn/&gt; The ratio of the area of the envelope of area bisectors to the area of the triangle is invariant for all triangles, and equals &lt;math&gt;\tfrac{3}{4} \log_e(2) - \tfrac{1}{2},&lt;/math&gt; i.e. 0.019860... or less than 2%.

A [[Cleaver (geometry)|cleaver]] of a triangle is a line segment that bisects the [[perimeter]] of the triangle and has one endpoint at the midpoint of one of the three sides. The three cleavers [[Concurrent lines|concur]] at (all pass through) the [[Spieker center|center of the Spieker circle]], which is the [[incircle]] of the [[medial triangle]]. The cleavers are parallel to the angle bisectors.

A [[Splitter (geometry)|splitter]] of a triangle is a line segment having one endpoint at one of the three vertices of the triangle and bisecting the perimeter. The three splitters concur at the [[Nagel point]] of the triangle.

Any line through a triangle that splits both the triangle's area and its perimeter in half goes through the triangle's incenter (the center of its [[incircle]]). There are either one, two, or three of these for any given triangle. A line through the incenter bisects one of the area or perimeter if and only if it also bisects the other.&lt;ref&gt;Kodokostas, Dimitrios, "Triangle Equalizers," ''[[Mathematics Magazine]]'' 83, April 2010, pp. 141-146.&lt;/ref&gt;

===Parallelogram===

Any line through the midpoint of a [[parallelogram]] bisects the area&lt;ref&gt;Dunn, J. A., and J. E. Pretty, "Halving a triangle", ''Mathematical Gazette'' 56, May 1972, p. 105.&lt;/ref&gt; and the perimeter. 

===Circle and ellipse===

All area bisectors and perimeter bisectors of a circle or other ellipse go through the [[Center (geometry)|center]], and any [[Chord (geometry)|chord]]s through the center bisect the area and perimeter. In the case of a circle they are the [[diameter]]s of the circle.

==Bisectors of diagonals==

===Parallelogram===

The [[diagonal]]s of a parallelogram bisect each other.

===Quadrilateral===

If a line segment connecting the diagonals of a quadrilateral bisects both diagonals, then this line segment (the [[Newton line|Newton Line]]) is itself bisected by the [[Quadrilateral#Remarkable points and lines in a convex quadrilateral|vertex centroid.]]

==Volume bisectors==

A plane that divides two opposite edges of a tetrahedron in a given ratio also divides the volume of the tetrahedron in the same ratio. Thus any plane containing a bimedian (connector of opposite edges' midpoints) of a tetrahedron bisects the volume of the tetrahedron&lt;ref&gt;Weisstein, Eric W. "Tetrahedron." From MathWorld--A Wolfram Web Resource. http://mathworld.wolfram.com/Tetrahedron.html&lt;/ref&gt;&lt;ref&gt;Altshiller-Court, N. "The tetrahedron." Ch. 4 in ''Modern Pure Solid Geometry'': Chelsea, 1979.&lt;/ref&gt;{{rp|pp.89–90}}

==References==
&lt;references/&gt;

==External links==
* [http://www.cut-the-knot.org/triangle/ABisector.shtml The Angle Bisector] at [[cut-the-knot]]
* [http://www.mathopenref.com/bisectorangle.html Angle Bisector definition.  Math Open Reference] With interactive applet
* [http://www.mathopenref.com/bisectorline.html Line Bisector definition.  Math Open Reference] With interactive applet
* [http://www.mathopenref.com/bisectorperpendicular.html Perpendicular Line Bisector.] With interactive applet
* [http://www.mathopenref.com/constbisectangle.html Animated instructions for bisecting an angle] and [http://www.mathopenref.com/constbisectline.html bisecting a line] Using a compass and straightedge
* {{MathWorld|title=Line Bisector|urlname=LineBisector}}

{{PlanetMath attribution|id=3623|title=Angle bisector}}

[[Category:Elementary geometry]]</text>
      <sha1>26upnwhjed9xewqf8yv3vgdo5tbfvxl</sha1>
    </revision>
  </page>
  <page>
    <title>Chebyshev's inequality</title>
    <ns>0</ns>
    <id>156533</id>
    <revision>
      <id>871649456</id>
      <parentid>871649403</parentid>
      <timestamp>2018-12-02T15:17:16Z</timestamp>
      <contributor>
        <username>Serols</username>
        <id>9929111</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/2405:204:430E:5753:0:0:88:88A0|2405:204:430E:5753:0:0:88:88A0]] ([[User talk:2405:204:430E:5753:0:0:88:88A0|talk]]) ([[WP:HG|HG]]) (3.4.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="70013">{{For|the similarly named inequality involving series|Chebyshev's sum inequality}}

In [[probability theory]], '''Chebyshev's inequality''' (also called the '''Bienaymé-Chebyshev inequality''') guarantees that, for a wide class of [[probability distributions]], no more than a certain fraction of values can be more than a certain distance from the [[expected value|mean]]. Specifically, no more than 1/''k''&lt;sup&gt;2&lt;/sup&gt; of the distribution's values can be more than ''k'' [[standard deviations]] away from the mean (or equivalently, at least 1−1/''k''&lt;sup&gt;2&lt;/sup&gt; of the distribution's values are within ''k'' standard deviations of the mean). The rule is often called Chebyshev's theorem, about the range of standard deviations around the mean, in statistics. The inequality has great utility because it can be applied to any probability distribution in which the mean and variance are defined. For example, it can be used to prove the [[weak law of large numbers]].

In practical usage, in contrast to the [[68–95–99.7 rule]], which applies to [[normal distribution]]s, Chebyshev's inequality is weaker, stating that a minimum of just 75% of values must lie within two standard deviations of the mean and 89% within three standard deviations.&lt;ref name=Kvanli&gt;{{cite book |last1=Kvanli |first1=Alan H. |last2=Pavur |first2=Robert J. |last3=Keeling |first3= Kellie B. |title=Concise Managerial Statistics |url=https://books.google.com/books?id=h6CQ1J0gwNgC&amp;pg=PT95 |year=2006 |publisher=[[cEngage Learning]] |isbn=9780324223880 |pages=81–82}}&lt;/ref&gt;&lt;ref name=Chernick&gt;{{cite book |last=Chernick |first=Michael R. |title=The Essentials of Biostatistics for Physicians, Nurses, and Clinicians |url=https://books.google.com/books?id=JP4azqd8ONEC&amp;pg=PA50 |year=2011 |publisher=[[John Wiley &amp; Sons]] |isbn=9780470641859 |pages=49–50}}&lt;/ref&gt;

The term ''Chebyshev's inequality'' also refer to [[Markov's inequality]], especially in the context of analysis. They are closely related, and some authors refer to [[Markov's inequality]] as "Chebyshev's First Inequality," and the similar one referred to on this page as "Chebyshev's Second Inequality."

==History==
The theorem is named after Russian mathematician [[Pafnuty Chebyshev]], although it was first formulated by his friend and colleague [[Irénée-Jules Bienaymé]].&lt;ref&gt;{{cite book
 |title=The Art of Computer Programming: Fundamental Algorithms, Volume 1
 |edition=3rd
 |last1=Knuth |first1=Donald |authorlink1=Donald Knuth
 |year=1997
 |publisher=Addison–Wesley
 |location=Reading, Massachusetts
 |isbn=978-0-201-89683-1
 |url=http://www-cs-faculty.stanford.edu/~uno/taocp.html
 |accessdate=1 October 2012
 |ref=KnuthTAOCP1
}}
&lt;/ref&gt;{{rp|98}} The theorem was first stated without proof by Bienaymé in 1853&lt;ref name="Bienaymé1853"&gt;{{cite journal | last1 = Bienaymé | first1 = I.-J. | year = 1853 | title = Considérations àl'appui de la découverte de Laplace | url = | journal = Comptes Rendus de l'Académie des Sciences | volume = 37 | issue = | pages = 309–324 }}&lt;/ref&gt; and later proved by Chebyshev in 1867.&lt;ref name=Chebyshev1867&gt;{{cite journal|last=Tchebichef|first=P.|title=Des valeurs moyennes|journal=Journal de Mathématiques Pures et Appliquées|year=1867|volume=12|series=2|pages=177–184}}&lt;/ref&gt; His student [[Andrey Markov]] provided another proof in his 1884 Ph.D. thesis.&lt;ref name=Markov1884&gt;Markov A. (1884) On certain applications of algebraic continued fractions, Ph.D. thesis, St. Petersburg&lt;/ref&gt;

==Statement==
Chebyshev's inequality is usually stated for [[random variable]]s, but can be generalized to a statement about [[measure theory|measure spaces]].

===Probabilistic statement===
Let ''X'' (integrable) be a [[random variable]] with finite [[expected value]] ''μ'' and finite non-zero [[variance]] ''σ''&lt;sup&gt;2&lt;/sup&gt;. Then for any [[real number]] {{nowrap|''k'' &gt; 0}},
: &lt;math&gt;
    \Pr(|X-\mu|\geq k\sigma) \leq \frac{1}{k^2}.
  &lt;/math&gt;
Only the case &lt;math&gt;k &gt; 1&lt;/math&gt; is useful. When &lt;math&gt;k \leq 1&lt;/math&gt; the right hand side &lt;math&gt;
 \frac{1}{k^2} \geq 1
  &lt;/math&gt;
and the inequality is trivial as all probabilities are ≤ 1.

As an example, using &lt;math&gt;k = \sqrt{2}&lt;/math&gt; shows that the probability that values lie outside the interval &lt;math&gt;(\mu - \sqrt{2}\sigma, \mu + \sqrt{2}\sigma)&lt;/math&gt; does not exceed &lt;math&gt;\frac{1}{2}&lt;/math&gt;.

Because it can be applied to completely arbitrary distributions provided they have a known finite mean and variance, the inequality generally gives a poor bound compared to what might be deduced if more aspects are known about the distribution involved.

{|class="wikitable" style="background-color:#FFFFFF; text-align:center"
|-
! k
! Min. % within ''k'' standard{{ns}}&lt;br&gt;deviations of mean
! Max. % beyond ''k'' standard&lt;br&gt;deviations from mean
|-
|  1
|| 0%
|| 100%
|-
|  {{sqrt|2}}
|| 50%
|| 50%
|-
|  1.5
|| 55.56%
|| 44.44%
|-
|  2
|| 75%
|| 25%
|-
|  3
|| 88.8889%
|| 11.1111%
|-
|  4
|| 93.75%
|| 6.25%
|-
|  5
|| 96%
|| 4%
|-
|  6
|| 97.2222%
|| 2.7778%
|-
|  7
|| 97.9592%
|| 2.0408%
|-
|  8
|| 98.4375%
|| 1.5625%
|-
|  9
|| 98.7654%
|| 1.2346%
|-
|  10
|| 99%
|| 1%
|}

===Measure-theoretic statement===
Let (''X'',&amp;nbsp;Σ,&amp;nbsp;μ) be a [[measure space]], and let ''f'' be an [[extended real number line|extended real]]-valued [[measurable function]] defined on ''X''. Then for any real number ''t'' &gt; 0 and ''0 &lt; p &lt; ∞'',&lt;ref&gt;{{cite book|last1=Grafakos|first1=Lukas|title=Classical and Modern Fourier Analysis|date=2004|publisher=Pearson Education Inc.|page=5}}&lt;/ref&gt;

:&lt;math&gt;\mu(\{x\in X\,:\,\,|f(x)|\geq t\}) \leq {1\over t^p} \int_{|f| \geq t} |f|^p \, d\mu.&lt;/math&gt;

More generally, if ''g'' is an extended real-valued measurable function, nonnegative and nondecreasing on the range of ''f'', then{{citation needed|date=May 2012}}

:&lt;math&gt;\mu(\{x\in X\,:\,\,f(x)\geq t\}) \leq {1\over g(t)} \int_X g\circ f\, d\mu.&lt;/math&gt;

The previous statement then follows by defining &lt;math&gt;g(x)&lt;/math&gt; as &lt;math&gt;|x|^p&lt;/math&gt; if &lt;math&gt;x\ge t&lt;/math&gt; and &lt;math&gt;0&lt;/math&gt; otherwise.

==Example==
Suppose we randomly select a journal article from a source with an average of 1000 words per article, with a standard deviation of 200 words. We can then infer that the probability that it has between 600 and 1400 words (i.e. within ''k''&amp;nbsp;=&amp;nbsp;2 standard deviations of the mean) must be at least 75%, because there is no more than {{nowrap|{{frac|1|''k''{{su|p=2}}}} {{=}} {{frac2|1|4}}}} chance to be outside that range, by Chebyshev's inequality. But if we additionally know that the distribution is normal, we can say there is a 75% chance the word count is between 770 and 1230 (which is an even tighter bound).

==Sharpness of bounds==
As shown in the example above, the theorem typically provides rather loose bounds. However, these bounds cannot in general (remaining true for arbitrary distributions) be improved upon. The bounds are sharp for the following example: for any ''k''&amp;nbsp;≥&amp;nbsp;1,
: &lt;math&gt;
    X = \begin{cases}
        -1, &amp; \text{with probability }\frac{1}{2k^2} \\
         0, &amp; \text{with probability }1 - \frac{1}{k^2} \\
         1, &amp; \text{with probability }\frac{1}{2k^2}
        \end{cases}
  &lt;/math&gt;

For this distribution, the mean ''μ'' = 0 and the standard deviation ''σ'' = {{frac2|1|''k'' }}, so
: &lt;math&gt;
    \Pr(|X-\mu| \ge k\sigma) = \Pr(|X| \ge 1) = \frac{1}{k^2}.
  &lt;/math&gt;
Chebyshev's inequality is an equality for precisely those distributions that are a linear transformation of this example.

==Proof (of the two-sided version)==

===Probabilistic proof===
[[Markov's inequality]] states that for any real-valued random variable ''Y'' and any positive number ''a'', we have Pr(|''Y''|&amp;nbsp;&gt;&amp;nbsp;''a'') ≤ E(|''Y''|)/''a''. One way to prove Chebyshev's inequality is to apply Markov's inequality to the random variable {{math|''Y'' {{=}} (''X'' − ''μ'')&lt;sup&gt;2&lt;/sup&gt;}} with ''a'' = (''kσ'')&lt;sup&gt;2&lt;/sup&gt;.

It can also be proved directly. For any event ''A'', let ''I''&lt;sub&gt;''A''&lt;/sub&gt; be the indicator random variable of ''A'', i.e. ''I''&lt;sub&gt;''A''&lt;/sub&gt; equals 1 if ''A'' occurs and 0 otherwise. Then

:&lt;math&gt;\begin{align}
\Pr(|X-\mu| \geq k\sigma) &amp;= \operatorname{E} \left (I_{|X-\mu| \geq k\sigma} \right ) \\
&amp;= \operatorname{E} \left (I_{\left (\frac{X-\mu}{k\sigma} \right )^2 \geq 1} \right ) \\[6pt]
&amp;\leq \operatorname{E}\left(\left({X-\mu \over k\sigma} \right)^2 \right) \\[6pt]
&amp;= {1 \over k^2} {\operatorname{E}((X-\mu)^2) \over \sigma^2} \\
&amp;= {1 \over k^2}.
\end{align}&lt;/math&gt;

The Inequality of direct proof shows why the bounds are quite loose in typical cases: 1) If &lt;math&gt;0\leq(\frac{X-\mu}{k\sigma})^2 &lt; 1&lt;/math&gt;, instead of taking the indicating value 0 as given by the left side of the inequality, a positive value of &lt;math&gt;(\frac{X-\mu}{k\sigma})^2&lt;/math&gt; is counted. 2) If &lt;math&gt;(\frac{X-\mu}{k\sigma})^2 \geq 1&lt;/math&gt;, instead of taking the indicting value 1 as given by the left side of the inequality, a value &lt;math&gt;(\frac{X-\mu}{k\sigma})^2&lt;/math&gt; greater or equal to 1 is counted. In some cases it exceeds 1 by a very wide margin.

===Measure-theoretic proof===
Fix &lt;math&gt;t&lt;/math&gt; and let &lt;math&gt;A_t&lt;/math&gt; be defined as &lt;math&gt;A_t = \{x\in X\mid f(x)\ge t\}&lt;/math&gt;, and let &lt;math&gt;1_{A_t}&lt;/math&gt; be the [[indicator function]] of the set&amp;nbsp;&lt;math&gt;A_t&lt;/math&gt;. Then, it is easy to check that, for any &lt;math&gt;x&lt;/math&gt;,

:&lt;math&gt;0\leq g(t) 1_{A_t}\leq g(f(x))\,1_{A_t},&lt;/math&gt;

since ''g'' is nondecreasing on the range of ''f'', and therefore,

:&lt;math&gt;\begin{align}g(t)\mu(A_t)&amp;=\int_X g(t)1_{A_t}\,d\mu\\ &amp;\leq\int_{A_t} g\circ f\,d\mu\\ &amp;\leq\int_X g\circ f\,d\mu.\end{align}&lt;/math&gt;

The desired inequality follows from dividing the above inequality by&amp;nbsp;''g''(''t'').

==Extensions==
Several extensions of Chebyshev's inequality have been developed.

===Asymmetric two-sided case===
An asymmetric two-sided version of this inequality is also known.&lt;ref name=Steliga2010&gt;{{cite journal |last1=Steliga |first1=Katarzyna |last2=Szynal |first2=Dominik |title=On Markov-Type Inequalities |journal=International Journal of Pure and Applied Mathematics |year=2010 |volume=58 |issue=2 |pages=137–152 |url=http://ijpam.eu/contents/2010-58-2/2/2.pdf |accessdate=10 October 2012 |issn=1311-8080}}&lt;/ref&gt;

When the distribution is known to be symmetric for any &lt;math&gt; k_1 + k_2 = 0 &lt;/math&gt;
: &lt;math&gt; \Pr( k_1 &lt; X &lt; k_2) \ge 1 - \frac{ 4 \sigma^2 }{ ( k_2 - k_1 )^2 }&lt;/math&gt;
where ''σ''&lt;sup&gt;2&lt;/sup&gt; is the [[variance]].

Similarly when the distribution is asymmetric or is unknown and &lt;math&gt; k_1 + k_2 = 2 \mu &lt;/math&gt;

: &lt;math&gt; \Pr( k_1 &lt; X &lt; k_2 ) \ge \frac{ 4 [ ( \mu - k_1 )( k_2 - \mu ) - \sigma^2 ] }{ ( k_2 - k_1 )^2 },&lt;/math&gt;

where {{math|''σ''&lt;sup&gt;2&lt;/sup&gt;}} is the variance and {{mvar|μ}} is the [[mean]].

===Bivariate case===
A version for the bivariate case is known.&lt;ref name=Ferentinos1982&gt;{{cite journal | last1 = Ferentinos | first1 = K | year = 1982 | title = On Tchebycheﬀ type inequalities | url = | journal = Trabajos Estadıst Investigacion Oper | volume = 33 | issue = | pages = 125–132 }}&lt;/ref&gt;

Let {{math|''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;}} be two random variables with means {{math|''μ''&lt;sub&gt;1&lt;/sub&gt;, ''μ''&lt;sub&gt;2&lt;/sub&gt;}} and finite variances {{math|''σ''&lt;sub&gt;1&lt;/sub&gt;, ''σ''&lt;sub&gt;2&lt;/sub&gt;}} respectively. Then

:&lt;math&gt; \Pr( k_{ 11 } \le X_1 \le k_{ 12 }, k_{ 21 } \le X_2 \le k_{ 22 }) \ge 1 - \sum T_i&lt;/math&gt;

where for ''i'' = 1, 2,

:&lt;math&gt; T_i = \frac{ 4 \sigma_i^2 + [ 2 \mu_i - ( k_{ i1 } + k_{ i2 } ) ]^2 } { ( k_{ i2 } - k_{ i1 } )^2 }.&lt;/math&gt;

===Two correlated variables===
Berge derived an inequality for two correlated variables {{math|''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;}}.&lt;ref name=Berge1938&gt;{{cite journal | last1 = Berge | first1 = P. O. | year = 1938 | title = A note on a form of Tchebycheff's theorem for two variables | url = | journal = Biometrika | volume = 29 | issue = 3/4| pages = 405–406 | doi=10.2307/2332015| jstor = 2332015 }}&lt;/ref&gt; Let {{mvar|ρ}} be the correlation coefficient between ''X''&lt;sub&gt;1&lt;/sub&gt; and ''X''&lt;sub&gt;2&lt;/sub&gt; and let ''σ''&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; be the variance of {{mvar|X&lt;sub&gt;i&lt;/sub&gt;}}. Then

: &lt;math&gt; \Pr\left( \bigcap_{ i = 1}^2 \left[ \frac{ | X_i - \mu_i | } { \sigma_i }  &lt; k \right] \right) \ge 1 - \frac{ 1 + \sqrt{ 1 - \rho^2 } } { k^2 }.&lt;/math&gt;

Lal later obtained an alternative bound&lt;ref name=Lal1955&gt;Lal D. N. (1955) A note on a form of Tchebycheﬀ's inequality for two or more variables. [[Sankhya (journal)|Sankhya]] 15(3):317–320&lt;/ref&gt;

: &lt;math&gt; \Pr\left( \bigcap_{i=1}^2 \left[ \frac{|X_i - \mu_i|}{\sigma_i} \le k_i \right] \right) \ge 1 - \frac{k_1^2 + k_2^2 + \sqrt{ ( k_1^2 + k_2^2 )^2 - 4 k_1^2 k_2^2 \rho}}{2( k_1 k_2)^2} &lt;/math&gt;

Isii derived a further generalisation.&lt;ref name=Isii1959&gt;Isii K. (1959) On a method for generalizations of Tchebycheff's inequality. Ann Inst Stat Math 10: 65–88&lt;/ref&gt; Let

: &lt;math&gt; Z = \Pr\left(\left (-k_1 &lt; X_1 &lt; k_2 \right) \cap \left(-k_1 &lt; X_2 &lt; k_2 \right) \right), \qquad 0 &lt; k_1 \leq k_2.&lt;/math&gt;

and define:

: &lt;math&gt; \lambda = \frac{ k_1( 1 + \rho ) + \sqrt{ ( 1 - \rho^2 )( k_1^2 + \rho ) } }{ 2k_1 - 1 + \rho } &lt;/math&gt;

There are now three cases.

*'''Case A:''' If &lt;math&gt; 2k_1^2 &gt; 1 - \rho &lt;/math&gt; and &lt;math&gt; k_2 - k_1 \ge 2 \lambda &lt;/math&gt; then
::&lt;math&gt; Z \le \frac{ 2 \lambda^2 } { 2 \lambda^2 + 1 + \rho }. &lt;/math&gt;

*'''Case B:''' If the conditions in case A are not met but {{math|''k''&lt;sub&gt;1&lt;/sub&gt;''k''&lt;sub&gt;2&lt;/sub&gt; ≥ 1}} and
::&lt;math&gt; 2 ( k_1 k_2 - 1 )^2 \ge 2( 1 - \rho^2 ) + ( 1 - \rho )( k_2 - k_1 )^2 &lt;/math&gt;
:then
::&lt;math&gt; Z \le \frac{ ( k_2 - k_1 )^2 + 4 + \sqrt{ 16 ( 1 - \rho^2 ) + 8 ( 1 - \rho )( k_2 - k_1 ) } }{ ( k_1 +k_2 )^2 }.&lt;/math&gt;

*'''Case C:''' If none of the conditions in cases A or B are satisfied then there is no universal bound other than 1.

===Multivariate case===
The general case is known as the Birnbaum–Raymond–Zuckerman inequality after the authors who proved it for two dimensions.&lt;ref name=Birnbaum1947&gt;{{cite journal |last1=Birnbaum |first1=Z. W. |last2=Raymond |first2=J. |last3=Zuckerman |first3=H. S. |title=A Generalization of Tshebyshev's Inequality to Two Dimensions |journal=The Annals of Mathematical Statistics |issn=0003-4851 |year=1947 |volume=18 |issue=1 |pages=70–79 |doi=10.1214/aoms/1177730493 |mr=19849 |zbl=0032.03402 |url=http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.aoms/1177730493 |accessdate=7 October 2012}}&lt;/ref&gt;

:&lt;math&gt;\Pr\left(\sum_{i=1}^n \frac{(X_i - \mu_i)^2}{\sigma_i^2 t_i^2} \ge k^2 \right) \le \frac{1}{k^2} \sum_{i=1}^n \frac{1}{t_i^2} &lt;/math&gt;

where {{mvar|X&lt;sub&gt;i&lt;/sub&gt;}} is the {{mvar|i}}-th random variable, {{mvar|μ&lt;sub&gt;i&lt;/sub&gt;}} is the {{mvar|i}}-th mean and ''σ''&lt;sub&gt;i&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; is the {{mvar|i}}-th variance.

If the variables are independent this inequality can be sharpened.&lt;ref name=Kotz2000&gt;{{cite book |last1=Kotz |first1=Samuel  |authorlink1=Samuel Kotz |last2=Balakrishnan |first2=N. |last3= Johnson |first3=Norman L. |authorlink3=Norman Lloyd Johnson |title=Continuous Multivariate Distributions, Volume 1, Models and Applications |year=2000 |publisher=Houghton Mifflin |location=Boston [u.a.] |isbn=978-0-471-18387-7 |url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471183873.html |edition=2nd |accessdate=7 October 2012}}&lt;/ref&gt;

:&lt;math&gt;\Pr\left (\bigcap_{i = 1}^n \frac{|X_i - \mu_i|}{\sigma_i} \le k_i \right ) \ge \prod \left (1 - \frac{1}{k_i^2} \right)&lt;/math&gt;

Olkin and Pratt derived an inequality for {{mvar|n}} correlated variables.&lt;ref name=Olkin1958&gt;{{cite journal|last1=Olkin|first1=Ingram |authorlink1=Ingram Olkin | last2=Pratt |first2=John W. |authorlink2=John W. Pratt |title=A Multivariate Tchebycheff Inequality| journal=The Annals of Mathematical Statistics|year=1958|volume=29|issue=1|pages=226–234|doi=10.1214/aoms/1177706720|url=http://projecteuclid.org/euclid.aoms/1177706720|zbl=0085.35204 |mr=93865 |accessdate=2 October 2012}}&lt;/ref&gt;

: &lt;math&gt; \Pr\left(\bigcap_{i = 1 }^n \frac{|X_i - \mu_i|}{\sigma_i} &lt; k_i \right) \ge 1 - \frac{1}{n^2} \left(\sqrt{U} + \sqrt{n-1} \sqrt{n \sum_i \frac 1 { k_i^2} - u} \right)^2 &lt;/math&gt;

where the sum is taken over the ''n'' variables and

: &lt;math&gt; u = \sum_{i=1}^n \frac{1}{ k_i^2} + 2\sum_{i=1}^n \sum_{j&lt;i} \frac{\rho_{ij}}{k_i k_j} &lt;/math&gt;

where {{mvar|ρ&lt;sub&gt;ij&lt;/sub&gt;}} is the correlation between {{mvar|X&lt;sub&gt;i&lt;/sub&gt;}} and {{mvar|X&lt;sub&gt;j&lt;/sub&gt;}}.

Olkin and Pratt's inequality was subsequently generalised by Godwin.&lt;ref name=Godwin1964&gt;Godwin H. J. (1964) Inequalities on distribution functions. New York, Hafner Pub. Co.&lt;/ref&gt;

===Vector version===
Ferentinos&lt;ref name="Ferentinos1982"/&gt; has shown that for a [[Multivariate random variable|vector]] {{math|''X'' {{=}} (''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ...)}} with mean {{math|''μ'' {{=}} (''μ''&lt;sub&gt;1&lt;/sub&gt;, ''μ''&lt;sub&gt;2&lt;/sub&gt;, ...)}}, standard deviation ''σ'' = (''σ''&lt;sub&gt;1&lt;/sub&gt;, ''σ''&lt;sub&gt;2&lt;/sub&gt;, ...) and the Euclidean norm {{math|{{!!}} ⋅ {{!!}}}} that

: &lt;math&gt; \Pr(\| X - \mu \| \ge k \| \sigma \|) \le \frac{ 1 } { k^2 }. &lt;/math&gt;

A second related inequality has also been derived by Chen.&lt;ref name=Chen2007&gt;{{cite arXiv |author=Xinjia Chen |eprint=0707.0805v2 |title=A New Generalization of Chebyshev Inequality for Random Vectors |year=2007 |version=|class=math.ST }}&lt;/ref&gt; Let {{mvar|n}} be the [[dimension]] of the stochastic vector {{mvar|X}} and let {{math|E(''X'')}} be the mean of {{mvar|X}}. Let {{mvar|S}} be the [[covariance matrix]] and {{math|''k'' &gt; 0}}. Then

: &lt;math&gt; \Pr \left( ( X - \operatorname{E}(X) )^T S^{-1} (X - \operatorname{E}(X)) &lt; k  \right) \ge 1 - \frac{n}{k} &lt;/math&gt;

where ''Y''&lt;sup&gt;T&lt;/sup&gt; is the [[transpose]] of {{mvar|Y}}.  A simple proof was obtained in Navarro&lt;ref name=Navarro2013&gt;{{cite journal |author=Jorge Navarro |volume=45 |issue=12 |pages=3458–3463 |title=A very simple proof of the multivariate Chebyshev's inequality |journal=Communications in Statistics – Theory and Methods |year=2016 |doi=10.1080/03610926.2013.873135}}&lt;/ref&gt; as follows:

: &lt;math&gt; Z=( X - \operatorname{E}(X) )^T S^{-1} (X - \operatorname{E}(X)) 
=( X - \operatorname{E}(X) )^T S^{-1/2}S^{-1/2} (X - \operatorname{E}(X))=Y^TY\geq 0 &lt;/math&gt;

where

:&lt;math&gt; Y=(Y_1,...,Y_n)^T=S^{-1/2}(X-\operatorname{E}(X)) &lt;/math&gt;

and &lt;math&gt;S^{-1/2}&lt;/math&gt; is a symmetric invertible matrix such that: &lt;math&gt;S^{-1/2}S^{-1/2}=S^{-1}&lt;/math&gt;. Hence &lt;math&gt; E(Y)=(0,...,0)^T &lt;/math&gt; and &lt;math&gt; Cov(Y)=I_n &lt;/math&gt; where &lt;math&gt; I_n &lt;/math&gt; represents the identity matrix of dimension n. Then &lt;math&gt; E(Y_i^2)=Var(Y_i)=1&lt;/math&gt; and

: &lt;math&gt; E(Z)=E(Y^TY)=\sum_{i=1}^n E(Y_i^2)= n&lt;/math&gt;

Finally, by applying [[Markov's inequality]] to Z we get

: &lt;math&gt;  \Pr\left( Z\geq k\right )=
\Pr \left( ( X - \operatorname{E}(X) )^T S^{-1} (X - \operatorname{E}(X)) \geq k  \right) 
\le \frac{E(Z)}{k}=\frac{n}{k} &lt;/math&gt;

and so the desired inequality holds.

The inequality can be written in terms of the [[Mahalanobis distance]] as

: &lt;math&gt; \Pr \left( d^2_S(X,\operatorname{E}(X)) &lt; k  \right) \ge 1 - \frac{n}{k} &lt;/math&gt;

where the Mahalanobis distance based on S is defined by

: &lt;math&gt; d_S(x,y) =\sqrt{ (x -y)^T S^{-1} (x -y) } &lt;/math&gt;

Navarro&lt;ref name=Navarro2014&gt;{{cite journal |author=Jorge Navarro |volume=91 |pages=1–5 |title=Can the bounds in the multivariate Chebyshev inequality be attained?   |journal=Statistics and Probability Letters  |year=2014 |doi=10.1016/j.spl.2014.03.028}}&lt;/ref&gt; proved that these bounds are sharp, that is, they are the best possible bounds for that regions when we just know the mean and the covariance matrix of X.

Stellato et al.&lt;ref name=":0"&gt;{{Cite journal|last=Stellato|first=Bartolomeo|last2=Parys|first2=Bart P. G. Van|last3=Goulart|first3=Paul J.|date=2016-05-31|title=Multivariate Chebyshev Inequality with Estimated Mean and Variance|journal=The American Statistician|volume=0|issue=ja|pages=1–13|doi=10.1080/00031305.2016.1186559|issn=0003-1305|arxiv=1509.08398}}&lt;/ref&gt; showed that this multivariate version of the Chebyshev inequality can be easily derived analytically as a special case of Vandenberghe et al.&lt;ref&gt;{{Cite journal|last=Vandenberghe|first=L.|last2=Boyd|first2=S.|last3=Comanor|first3=K.|date=2007-01-01|title=Generalized Chebyshev Bounds via Semidefinite Programming|journal=SIAM Review|volume=49|issue=1|pages=52–64|doi=10.1137/S0036144504440543|issn=0036-1445|bibcode=2007SIAMR..49...52V|citeseerx=10.1.1.126.9105}}&lt;/ref&gt; where the bound is computed by solving a [[Semidefinite programming|semidefinite program (SDP).]]

===Infinite dimensions===
There is a straightforward extension of the vector version of Chebyshev's inequality to infinite dimensional settings. Let {{mvar|X}} be a random variable which takes values in a [[Fréchet space]] &lt;math&gt;\mathcal X&lt;/math&gt; (equipped with seminorms {{math|{{!!}} ⋅ {{!!}}&lt;sub&gt;''α''&lt;/sub&gt;}}). This includes most common settings of vector-valued random variables, e.g., when &lt;math&gt;\mathcal X&lt;/math&gt; is a [[Banach space]] (equipped with a single norm), a [[Hilbert space]], or the finite-dimensional setting as described above.

Suppose that {{mvar|X}} is of "[[strong order two]]", meaning that

: &lt;math&gt; \operatorname{E}\left(\| X\|_\alpha^2 \right) &lt; \infty &lt;/math&gt;

for every seminorm {{math|{{!!}} ⋅ {{!!}}&lt;sub&gt;''α''&lt;/sub&gt;}}. This is a generalization of the requirement that {{mvar|X}} have finite variance, and is necessary for this strong form of Chebyshev's inequality in infinite dimensions. The terminology "strong order two" is due to [[Vakhania]].&lt;ref&gt;Vakhania, Nikolai Nikolaevich. Probability distributions on linear spaces. New York: North Holland, 1981.&lt;/ref&gt;

Let &lt;math&gt;\mu \in \mathcal X&lt;/math&gt; be the [[Pettis integral]] of {{mvar|X}} (i.e., the vector generalization of the mean), and let

:&lt;math&gt;\sigma_a := \sqrt{\operatorname{E}\|X - \mu\|_\alpha^2}&lt;/math&gt;

be the standard deviation with respect to the seminorm {{math|{{!!}} ⋅ {{!!}}&lt;sub&gt;''α''&lt;/sub&gt;}}. In this setting we can state the following:

:'''General version of Chebyshev's inequality.''' &lt;math&gt;\forall k &gt; 0: \quad \Pr\left( \|X - \mu\|_\alpha \ge k \sigma_\alpha \right) \le \frac{1}{ k^2 }.&lt;/math&gt;

'''Proof.''' The proof is straightforward, and essentially the same as the finitary version. If {{math|''σ&lt;sub&gt;α&lt;/sub&gt;'' {{=}} 0}}, then {{mvar|X}} is constant (and equal to {{mvar|μ}}) almost surely, so the inequality is trivial.

If

:&lt;math&gt;\|X - \mu\|_\alpha \ge k \sigma_\alpha^2&lt;/math&gt;

then {{math|{{!!}}''X'' − ''μ''{{!!}}&lt;sub&gt;''α''&lt;/sub&gt; &gt; 0}}, so we may safely divide by {{math|{{!!}}''X'' − ''μ''{{!!}}&lt;sub&gt;''α''&lt;/sub&gt;}}. The crucial trick in Chebyshev's inequality is to recognize that &lt;math&gt;1 = \tfrac{\|X - \mu\|_\alpha^2}{\|X - \mu\|_\alpha^2}&lt;/math&gt;.

The following calculations complete the proof:

:&lt;math&gt;\begin{align}
\Pr\left( \|X - \mu\|_\alpha \ge k \sigma_\alpha \right) &amp;= \int_\Omega \mathbf{1}_{\|X - \mu\|_\alpha \ge k \sigma_\alpha} \, \mathrm d\Pr \\
&amp; = \int_\Omega \left ( \frac{\|X - \mu\|_\alpha^2}{\|X - \mu\|_\alpha^2} \right ) \cdot \mathbf{1}_{\|X - \mu\|_\alpha \ge k \sigma_\alpha} \, \mathrm d\Pr \\
&amp;\le \int_\Omega \left (\frac{\|X - \mu\|_\alpha^2}{(k\sigma_\alpha)^2} \right ) \cdot \mathbf{1}_{\|X - \mu\|_\alpha \ge k \sigma_\alpha} \, \mathrm d\Pr \\
&amp;\le \frac{1}{k^2 \sigma_\alpha^2} \int_\Omega \|X - \mu\|_\alpha^2 \, \mathrm d\Pr &amp;&amp; \mathbf{1}_{\|X - \mu\|_\alpha \ge k \sigma_\alpha} \le 1\\
&amp;= \frac{1}{k^2 \sigma_\alpha^2} \left (\operatorname{E}\|X - \mu\|_\alpha^2 \right )\\
&amp;= \frac{1}{k^2 \sigma_\alpha^2} \left (\sigma_\alpha^2 \right )\\
&amp;= \frac{1}{k^2}
\end{align}&lt;/math&gt;

===Higher moments===
An extension to higher moments is also possible:

:&lt;math&gt; \Pr\left(| X - \operatorname{E}(X) | \ge k \operatorname{E}(|X - \operatorname{E}(X) |^n )^{ \frac{1}{n} }\right) \le \frac{1 } {k^n}, \qquad k &gt;0, n \geq 2.&lt;/math&gt;

===Exponential version===
A related inequality sometimes known as the exponential Chebyshev's inequality&lt;ref name=RassoulAgha2010&gt;[http://www.math.utah.edu/~firas/Papers/rassoul-seppalainen-ldp.pdf  Section 2.1] {{webarchive |url=https://web.archive.org/web/20150430075226/http://www.math.utah.edu/~firas/Papers/rassoul-seppalainen-ldp.pdf |date=April 30, 2015 }}&lt;/ref&gt; is the inequality

:&lt;math&gt; \Pr(X \ge \varepsilon) \le e^{ -t \varepsilon }\operatorname{E}\left (e^{ t X } \right), \qquad t &gt; 0.&lt;/math&gt;

Let {{math|''K''(''t'')}} be the [[cumulant generating function]],

: &lt;math&gt; K( t ) = \log \left(\operatorname{E}\left( e^{ t x } \right) \right). &lt;/math&gt;

Taking the [[Legendre–Fenchel transformation]]{{clarify|reason=articles should be reasonably self contained, more explanation needed|date=May 2012}} of {{math|''K''(''t'')}} and using the exponential Chebyshev's inequality we have

: &lt;math&gt;-\log( \Pr (X \ge \varepsilon )) \ge \sup_t( t \varepsilon - K( t ) ). &lt;/math&gt;

This inequality may be used to obtain exponential inequalities for unbounded variables.&lt;ref name=Baranoski2001&gt;{{cite journal |last1=Baranoski |first1=Gladimir V. G. |last2=Rokne |first2=Jon G. |last3=Xu |first3=Guangwu |title=Applying the exponential Chebyshev inequality to the nondeterministic computation of form factors |journal=Journal of Quantitative Spectroscopy and Radiative Transfer |date=15 May 2001 |volume=69 |issue=4 |pages=199–200 |doi=10.1016/S0022-4073(00)00095-9 |url=http://www.sciencedirect.com/science/article/pii/S0022407300000959 |accessdate=2 October 2012 |bibcode=2001JQSRT..69..447B}} (the references for this article are corrected by {{cite journal|last1=Baranoski |first1=Gladimir V. G. |last2=Rokne |first2=Jon G. |author3=Guangwu Xu |title=Corrigendum to: 'Applying the exponential Chebyshev inequality to the nondeterministic computation of form factors' |journal=Journal of Quantitative Spectroscopy and Radiative Transfer |date=15 January 2002 |volume=72 |issue=2 |pages=199–200 |doi=10.1016/S0022-4073(01)00171-6 |url=http://www.sciencedirect.com/science/article/pii/S0022407301001716 |accessdate=2 October 2012 |bibcode=2002JQSRT..72..199B}})&lt;/ref&gt;

===Inequalities for bounded variables===
If P(''x'') has finite support based on the interval {{math|[''a'', ''b'']}}, let {{math|''M'' {{=}} max({{!}}''a''{{!}}, {{!}}''b''{{!}})}} where |''x''| is the [[absolute value]] of {{mvar|x}}. If the mean of P(''x'') is zero then for all {{math|''k'' &gt; 0}}&lt;ref name=Dufour2003&gt;Dufour (2003) [http://www2.cirano.qc.ca/~dufourj/Web_Site/ResE/Dufour_1999_C_TS_Moments.pdf Properties of moments of random variables]&lt;/ref&gt;

: &lt;math&gt;\frac{\operatorname{E}(|X|^r ) - k^r }{M^r} \le \Pr( | X |  \ge k ) \le \frac{\operatorname{E}(| X |^r ) }{ k^r }.&lt;/math&gt;

The second of these inequalities with {{math|''r'' {{=}} 2}} is the Chebyshev bound. The first provides a lower bound for the value of P(''x'').

Sharp bounds for a bounded variate have been proposed by Niemitalo, but without a proof though&lt;ref name=Niemitalo2012&gt;Niemitalo O. (2012) [http://yehar.com/blog/?p=1225 One-sided Chebyshev-type inequalities for bounded probability distributions.]&lt;/ref&gt;

Let {{math|0 ≤ ''X'' ≤ ''M''}} where {{math|''M'' &gt; 0}}. Then

*'''Case 1:'''
:: &lt;math&gt;\Pr(X&lt;k) = 0 \qquad \text{if} \qquad \operatorname{E}(X) &gt; k \quad \text{and} \quad \operatorname{E}(X^2) &lt; k\operatorname{E}(X) + M\operatorname{E}(X) - kM &lt;/math&gt;

*'''Case 2:'''
::&lt;math&gt;\Pr(X&lt;k) \ge  1 - \frac{k\operatorname{E}(X) + M\operatorname{E}(X) - \operatorname{E}(X^2)}{kM} \qquad \text{if} \qquad \begin{cases} \operatorname{E}(X)&gt; k \quad \text{and} \quad E( X^2 )\ge k\operatorname{E}(X) + M\operatorname{E}(X) - kM \\ \qquad \qquad \qquad \text{or} \\ \operatorname{E}(X) \le k \quad \text{and} \quad \operatorname{E}(X^2)\ge k\operatorname{E}(X) \end{cases}&lt;/math&gt;

*'''Case 3:'''
::&lt;math&gt;\Pr(X&lt;k) \ge \frac{\operatorname{E}(X)^2 - 2 k\operatorname{E}(X) + k^2 }{\operatorname{E}(X^2)- 2 k\operatorname{E}(X) + k^2 } \qquad \text{if} \qquad \operatorname{E}(X) \le k \quad \text{and} \quad \operatorname{E}(X^2) &lt; k\operatorname{E}(X)&lt;/math&gt;

==Finite samples==

=== Univariate case ===
Saw ''et al'' extended Chebyshev's inequality to cases where the population mean and variance are not known and may not exist, but the sample mean and sample standard deviation from ''N'' samples are to be employed to bound the expected value of a new drawing from the same distribution.&lt;ref name=":1"&gt;{{cite journal
  |title = Chebyshev Inequality with Estimated Mean and Variance
  |last1 = Saw
  |first1 = John G.
  |last2 = Yang
  |first2 = Mark C. K.
  |last3 = Mo
  |first3 = Tse Chin
  |journal = [[The American Statistician]]
  |issn = 0003-1305
  |volume = 38
  |issue = 2
  |year = 1984
  |pages = 130–2
  |doi = 10.2307/2683249
  |jstor = 2683249
  
  }}&lt;/ref&gt;

: &lt;math&gt; P( | X - m | \ge ks) \le \frac{ g_{N + 1}\left( \frac{N k^2}{N - 1 + k^2} \right) }{N + 1} \left( \frac N {N + 1} \right)^{1/2} &lt;/math&gt;

where ''X'' is a random variable which we have sampled ''N'' times, ''m'' is the sample mean, ''k'' is a constant and ''s'' is the sample standard deviation. ''g''(''x'') is defined as follows:

Let ''x'' ≥ 1, ''Q'' = ''N'' + 1, and ''R'' be the greatest integer less than ''Q''/''x''. Let

: &lt;math&gt; a^ 2 = \frac{ Q( Q - R ) } { 1 + R( Q - R ) }. &lt;/math&gt;

Now

: &lt;math&gt;
g_Q(x) = \begin{cases}
R &amp; \text{if }R \text{ is even,} \\
R &amp; \text{if }R \text{ is odd and }x &lt; a^2, \\
R - 1 &amp; \text{if } R \text{ is odd and } x \ge a^2.
\end{cases}
&lt;/math&gt;

This inequality holds even when the population moments do not exist, and when the sample is only [[Exchangeable random variables|weakly exchangeably]] distributed; this criterion is met for randomised sampling. A table of values for the Saw–Yang–Mo inequality for finite sample sizes (''N'' &lt; 100) has been determined by Konijn.&lt;ref name=Konijn1987&gt;{{cite journal |last=Konijn |first=Hendrik S. |title=Distribution-Free and Other Prediction Intervals |journal=[[The American Statistician]] |date=February 1987 |volume=41 |issue=1 |pages=11–15 |jstor=2684311 |doi=10.2307/2684311  }}&lt;/ref&gt; The table allows the calculation of various confidence intervals for the mean, based on multiples, C, of the standard error of the mean as calculated from the sample. For example, Konijn shows that for ''N''&amp;nbsp;=&amp;nbsp;59, the 95 percent confidence interval for the mean ''m'' is {{nowrap|(''m'' − ''Cs'', ''m'' + ''Cs'')}} where {{nowrap|1=''C'' = 4.447 × 1.006 = 4.47}} (this is 2.28 times larger than the value found on the assumption of normality showing the loss on precision resulting from ignorance of the precise nature of the distribution).

Kabán gives a somewhat less complex version of this inequality.&lt;ref name="Kabán2011"&gt;{{cite journal
  |last = Kabán
  |first = Ata
  |title = Non-parametric detection of meaningless distances in high dimensional data
  |journal = [[Statistics and Computing]]
  |volume = 22
  |issue = 2
  |pages = 375–85
  |year = 2012
  |doi = 10.1007/s11222-011-9229-0
}}&lt;/ref&gt;

: &lt;math&gt;P( | X - m | \ge ks ) \le \frac 1 {[N( N + 1 )]^{1/2} }\left[  \left( \frac{N - 1}{k^2} + 1 \right) \right]&lt;/math&gt;

If the standard deviation is a multiple of the mean then a further inequality can be derived,&lt;ref name="Kabán2011" /&gt;

: &lt;math&gt;P( | X - m | \ge ks ) \le \frac{N - 1} N \frac 1 {k^2} \frac{s^2}{m^2} + \frac 1 N.&lt;/math&gt;

A table of values for the Saw–Yang–Mo inequality for finite sample sizes (''N'' &lt; 100) has been determined by Konijn.&lt;ref name="Konijn1987"/&gt;

For fixed ''N'' and large ''m'' the Saw–Yang–Mo inequality is approximately&lt;ref name=Beasley2004&gt;{{cite journal |last1=Beasley |first1=T. Mark |last2=Page |first2=Grier P. |last3=Brand |first3=Jaap P. L. |last4=Gadbury |first4=Gary L. |last5=Mountz |first5=John D. |last6=Allison |first6=David B. |authorlink6=David B. Allison |title=Chebyshev's inequality for nonparametric testing with small ''N'' and α in microarray research |journal=Journal of the Royal Statistical Society |issn=1467-9876 |date=January 2004 |volume=53 |series=C (Applied Statistics) |issue=1 |pages=95–108 |doi=10.1111/j.1467-9876.2004.00428.x }}&lt;/ref&gt;

: &lt;math&gt; P( | X - m | \ge ks ) \le \frac 1 {N + 1}. &lt;/math&gt;

Beasley ''et al'' have suggested a modification of this inequality&lt;ref name=Beasley2004 /&gt;

: &lt;math&gt; P( | X - m | \ge ks ) \le \frac 1 {k^2( N + 1 )}. &lt;/math&gt;

In empirical testing this modification is conservative but appears to have low statistical power. Its theoretical basis currently remains unexplored.

====Dependence on sample size====
The bounds these inequalities give on a finite sample are less tight than those the Chebyshev inequality gives for a distribution. To illustrate this let the sample size ''N'' = 100 and let ''k'' = 3. Chebyshev's inequality states that at most approximately 11.11% of the distribution will lie at least three standard deviations away from the mean. Kabán's version of the inequality for a finite sample states that at most approximately 12.05% of the sample lies outside these limits. The dependence of the confidence intervals on sample size is further illustrated below.

For ''N'' = 10, the 95% confidence interval is approximately ±13.5789 standard deviations.

For ''N'' = 100 the 95% confidence interval is approximately ±4.9595 standard deviations; the 99% confidence interval is approximately ±140.0 standard deviations.

For ''N'' = 500 the 95% confidence interval is approximately ±4.5574 standard deviations; the 99% confidence interval is approximately ±11.1620 standard deviations.

For ''N'' = 1000 the 95% and 99% confidence intervals are approximately ±4.5141 and approximately ±10.5330 standard deviations respectively.

The Chebyshev inequality for the distribution gives 95% and 99% confidence intervals of approximately ±4.472 standard deviations and ±10 standard deviations respectively.

====Samuelson's inequality====
Although Chebyshev's inequality is the best possible bound for an arbitrary distribution, this is not necessarily true for finite samples. [[Samuelson's inequality]] states that all values of a sample will lie within  {{radic|''N''&amp;nbsp;−&amp;nbsp;1}} standard deviations of the mean. Chebyshev's bound improves as the sample size increases.

When ''N'' = 10, Samuelson's inequality states that all members of the sample lie within 3 standard deviations of the mean: in contrast Chebyshev's states that 99.5% of the sample lies within 13.5789 standard deviations of the mean.

When ''N'' = 100, Samuelson's inequality states that all members of the sample lie within approximately 9.9499 standard deviations of the mean: Chebyshev's states that 99% of the sample lies within 10 standard deviations of the mean.

When ''N'' = 500, Samuelson's inequality states that all members of the sample lie within approximately 22.3383 standard deviations of the mean: Chebyshev's states that 99% of the sample lies within 10 standard deviations of the mean.

=== Multivariate case ===
Stellato et al.&lt;ref name=":0" /&gt; simplified the notation and extended the empirical Chebyshev inequality from Saw et al.&lt;ref name=":1" /&gt; to the multivariate case. Let &lt;math display="inline"&gt;\xi \in \mathbb{R}^{n_\xi}&lt;/math&gt; be a random variable and let &lt;math display="inline"&gt;N \in \mathbb{Z}_{\geq n_\xi}&lt;/math&gt;. We draw &lt;math display="inline"&gt;N+1&lt;/math&gt; iid samples of &lt;math display="inline"&gt;\xi&lt;/math&gt; denoted as &lt;math display="inline"&gt;\xi^{(1)},\dots,\xi^{(N)},\xi^{(N+1)} \in \mathbb{R}^{n_\xi}&lt;/math&gt;. Based on the first &lt;math display="inline"&gt;N&lt;/math&gt; samples, we define the empirical mean as &lt;math display="inline"&gt;\mu_N = \frac 1 N \sum_{i=1}^N \xi^{(i)}&lt;/math&gt; and the unbiased empirical covariance as &lt;math display="inline"&gt;\Sigma_N = \frac 1 N \sum_{i=1}^N (\xi^{(i)} - \mu_{N})(\xi^{(i)} - \mu_N)^\top&lt;/math&gt;. If &lt;math&gt;\Sigma_N&lt;/math&gt; is nonsingular, then for all &lt;math&gt;\lambda \in \mathbb{R}_{\geq 0} &lt;/math&gt; then

: &lt;math&gt;
\begin{align}
&amp; P^{N+1} \left((\xi^{(N+1)} - \mu_N)^\top \Sigma_N^{-1}(\xi^{(N+1)} - \mu_N) \geq \lambda^2\right) \\[8pt]
\leq {} &amp; \min\left\{1, \frac 1 {N+1} \left\lfloor \frac{n_\xi(N+1)(N^2 - 1 + N\lambda^2)}{N^2\lambda^2}\right\rfloor\right\}.
\end{align}
&lt;/math&gt;

==== Remarks ====
In the univariate case, i.e. &lt;math display="inline"&gt;n_\xi = 1&lt;/math&gt;, this inequality corresponds to the one from Saw et al.&lt;ref name=":1" /&gt;  Moreover, the right-hand side can be simplified by upper bounding the floor function by its argument

: &lt;math&gt;P^{N+1}\left((\xi^{(N+1)} - \mu_N)^\top \Sigma_N^{-1}(\xi^{(N+1)} - \mu_N) \geq \lambda^2\right) \leq \min\left\{1,  \frac{n_\xi(N^2 - 1 + N\lambda^2)}{N^2\lambda^2}\right\}.&lt;/math&gt;

As &lt;math display="inline"&gt;N \to \infty&lt;/math&gt;, the right-hand side tends to &lt;math display="inline"&gt;\min \left\{1, \frac{n_\xi}{\lambda^2}\right\}&lt;/math&gt; which corresponds to the [[Chebyshev's inequality#Vector version|multivariate Chebyshev inequality]] over ellipsoids shaped according to &lt;math display="inline"&gt;\Sigma&lt;/math&gt; and centered in &lt;math display="inline"&gt;\mu&lt;/math&gt;.

==Sharpened bounds==
Chebyshev's inequality is important because of its applicability to any distribution. As a result of its generality it may not (and usually does not) provide as sharp a bound as alternative methods that can be used if the distribution of the random variable is known. To improve the sharpness of the bounds provided by Chebyshev's inequality a number of methods have been developed; for a review see eg.&lt;ref&gt;[http://nvlpubs.nist.gov/nistpubs/jres/65B/jresv65Bn3p211_A1b.pdf Savage, I. Richard. "Probability inequalities of the Tchebycheff type." Journal of Research of the National Bureau of Standards-B. Mathematics and Mathematical Physics B 65 (1961): 211-222]&lt;/ref&gt;

===Standardised variables===
Sharpened bounds can be derived by first standardising the random variable.&lt;ref name=Ion2001&gt;{{cite book|last=Ion|first=Roxana Alice|title=Nonparametric Statistical Process Control|year=2001|publisher=Universiteit van Amsterdam|isbn=978-9057760761|url=http://dare.uva.nl/document/60326|accessdate=1 October 2012|chapter=Chapter 4: Sharp Chebyshev-type inequalities}}&lt;/ref&gt;

Let ''X'' be a random variable with finite variance ''Var''(''x''). Let ''Z'' be the standardised form defined as

:&lt;math&gt; Z = \frac {X - \operatorname{E}(X) } {  \operatorname{Var}(X)^{ 1/2 } }.&lt;/math&gt;

[[Cantelli's inequality|Cantelli's lemma]] is then

:&lt;math&gt; P(Z \ge k) \le \frac{ 1 } { 1 + k^2 }.&lt;/math&gt;

This inequality is sharp and is attained by ''k'' and −1/''k'' with probability 1/(1&amp;nbsp;+&amp;nbsp;''k''&lt;sup&gt;2&lt;/sup&gt;) and ''k''&lt;sup&gt;2&lt;/sup&gt;/(1&amp;nbsp;+&amp;nbsp;''k''&lt;sup&gt;2&lt;/sup&gt;) respectively.

If ''k'' &gt; 1 and the distribution of ''X'' is symmetric then we have

:&lt;math&gt; P(Z \ge k) \le \frac { 1 } { 2 k^2 } .&lt;/math&gt;

Equality holds if and only if ''Z'' = −''k'', 0 or ''k'' with probabilities {{nowrap|1= 1 / 2 ''k''&lt;sup&gt;2&lt;/sup&gt;}}, {{nowrap|1 − 1 / ''k''&lt;sup&gt;2&lt;/sup&gt;}} and {{nowrap|1 / 2 ''k''&lt;sup&gt;2&lt;/sup&gt;}} respectively.&lt;ref name=Ion2001/&gt;
An extension to a two-sided inequality is also possible.

Let ''u'', ''v'' &gt; 0. Then we have&lt;ref name=Ion2001/&gt;
:&lt;math&gt; P(Z \le -u  \text{ or }  Z \ge v) \le \frac{ 4 + (u - v)^2 } { (u + v)^2  } .&lt;/math&gt;

===Semivariances===
An alternative method of obtaining sharper bounds is through the use of [[semivariance]]s (partial variances). The upper (''σ''&lt;sub&gt;+&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;) and lower (''σ''&lt;sub&gt;−&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;) semivariances are defined as

: &lt;math&gt; \sigma_+^2 = \frac { \sum_{x&gt;m} (x - m)^2 } { n - 1 } ,&lt;/math&gt;

: &lt;math&gt; \sigma_-^2 = \frac { \sum_{x&lt;m} (m - x)^2 } { n - 1 }, &lt;/math&gt;

where ''m'' is the arithmetic mean of the sample and ''n'' is the number of elements in the sample.

The variance of the sample is the sum of the two semivariances:

: &lt;math&gt; \sigma^2 = \sigma_+^2 + \sigma_-^2. &lt;/math&gt;

In terms of the lower semivariance Chebyshev's inequality can be written&lt;ref name=Berck1982&gt;{{cite journal|authorlink1=Peter Berck |last1=Berck |first1=Peter |last2=Hihn |first2=Jairus M. |title=Using the Semivariance to Estimate Safety-First Rules |journal=American Journal of Agricultural Economics |date=May 1982 |volume=64 |issue=2 |pages=298–300 |doi=10.2307/1241139 |url=http://ajae.oxfordjournals.org/content/64/2/298.full.pdf+html |accessdate=8 October 2012 |issn=0002-9092|jstor=1241139 }}&lt;/ref&gt;

: &lt;math&gt; \Pr(x \le m - a \sigma_-) \le \frac { 1 } { a^2 }.&lt;/math&gt;

Putting

: &lt;math&gt; a = \frac{ k \sigma } { \sigma_- }. &lt;/math&gt;

Chebyshev's inequality can now be written

: &lt;math&gt; \Pr(x \le m - k \sigma) \le \frac { 1 } { k^2 } \frac { \sigma_-^2 } { \sigma^2 }.&lt;/math&gt;

A similar result can also be derived for the upper semivariance.

If we put

: &lt;math&gt; \sigma_u^2 = \max(\sigma_-^2, \sigma_+^2) , &lt;/math&gt;

Chebyshev's inequality can be written

: &lt;math&gt; \Pr(| x \le m - k \sigma |) \le \frac { 1 } { k^2 } \frac { \sigma_u^2 } { \sigma^2 } .&lt;/math&gt;

Because ''σ''&lt;sub&gt;u&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; ≤ ''σ''&lt;sup&gt;2&lt;/sup&gt;, use of the semivariance sharpens the original inequality.

If the distribution is known to be symmetric, then

: &lt;math&gt; \sigma_+^2 = \sigma_-^2  = \frac{ 1 } { 2 } \sigma^2 &lt;/math&gt;

and

: &lt;math&gt; \Pr(x \le m - k \sigma) \le \frac { 1 } { 2 k^2 } .&lt;/math&gt;

This result agrees with that derived using standardised variables.

;Note: The inequality with the lower semivariance has been found to be of use in estimating downside risk in finance and agriculture.&lt;ref name="Berck1982"/&gt;&lt;ref name=Nantell1979&gt;{{cite journal |last1=Nantell |first1=Timothy J. |last2=Price |first2=Barbara |title=An Analytical Comparison of Variance and Semivariance Capital Market Theories |journal=[[The Journal of Financial and Quantitative Analysis]] |date=June 1979 |volume=14 |issue=2 |pages=221–42 |doi=10.2307/2330500 |jstor=2330500  }}&lt;/ref&gt;&lt;ref name=Neave2008&gt;{{cite journal
  |title = Distinguishing upside potential from downside risk
  |last1 = Neave
  |first1 = Edwin H.
  |last2 = Ross
  |first2 = Michael N.
  |last3 = Yang
  |first3 = Jun
  |journal = [[Management Research News]]
  |issn = 0140-9174
  |year = 2009
  |volume = 32
  |issue = 1
  |pages = 26–36
  |doi = 10.1108/01409170910922005
  }}&lt;/ref&gt;

===Selberg's inequality===
Selberg derived an inequality for ''P''(''x'') when ''a'' ≤ ''x'' ≤ ''b''.&lt;ref name=Selberg1940&gt;{{cite journal |last=Selberg |first=Henrik L. |title=Zwei Ungleichungen zur Ergänzung des Tchebycheffschen Lemmas |trans-title=Two Inequalities Supplementing the Tchebycheff Lemma |journal=Skandinavisk Aktuarietidskrift (Scandinavian Actuarial Journal) |year=1940 |volume=1940 |issue=3–4 |pages=121–125 |doi=10.1080/03461238.1940.10404804 |language=German |issn=0346-1238 |oclc=610399869}}&lt;/ref&gt; To simplify the notation let

: &lt;math&gt; Y = \alpha X + \beta &lt;/math&gt;

where

: &lt;math&gt; \alpha  = \frac{ 2 k }{ b - a }&lt;/math&gt;

and

: &lt;math&gt; \beta  = \frac{ - ( b + a ) k }{ b - a }. &lt;/math&gt;

The result of this linear transformation is to make ''P''(''a'' ≤ ''X'' ≤ ''b'') equal to ''P''(|''Y''| ≤ ''k'').

The mean (''μ''&lt;sub&gt;''X''&lt;/sub&gt;) and variance (''σ''&lt;sub&gt;''X''&lt;/sub&gt;) of ''X'' are related to the mean (''μ''&lt;sub&gt;''Y''&lt;/sub&gt;) and variance (''σ''&lt;sub&gt;''Y''&lt;/sub&gt;) of ''Y'':

: &lt;math&gt; \mu_Y = \alpha \mu_X + \beta &lt;/math&gt;

: &lt;math&gt; \sigma_Y^2 = \alpha^2 \sigma_X^2. &lt;/math&gt;

With this notation Selberg's inequality states that

: &lt;math&gt; P( | Y | &lt; k ) \ge \frac{ ( k - \mu_Y )^ 2 }{ ( k - \mu_Y )^2 + \sigma_Y^2 } \quad\text{ if }\quad \sigma_Y^2 \le \mu_Y ( k - \mu_Y ) &lt;/math&gt;

: &lt;math&gt;  P( | Y | &lt; k ) \ge 1 - \frac{ \sigma_Y^2 + \mu_Y^2 }{ k^2 } \quad\text{ if }\quad \mu_Y ( k - \mu_Y ) \le \sigma_Y^2 \le k^2 - \mu_Y^2 &lt;/math&gt;

: &lt;math&gt;  P( | Y | &lt; k ) \ge 0 \quad\text{ if }\quad k^2 - \mu_Y^2 \le \sigma_Y^2.&lt;/math&gt;

These are known to be the best possible bounds.&lt;ref name=Conlon00&gt;{{cite journal |last1=Conlon |first1=J. |last2=Dulá |first2=J. H. |title=A geometric derivation and interpretation of Tchebyscheff's Inequality |url=http://www.people.vcu.edu/~jdula/WORKINGPAPERS/tcheby.pdf |accessdate=2 October 2012}}&lt;/ref&gt;

===Cantelli's inequality===
[[Cantelli's inequality]]&lt;ref name=Cantelli1910&gt;Cantelli F. (1910) Intorno ad un teorema fondamentale della teoria del rischio. Bolletino dell Associazione degli Attuari Italiani&lt;/ref&gt; due to [[Francesco Paolo Cantelli]] states that for a real random variable (''X'') with mean (''μ'') and variance (''σ''&lt;sup&gt;2&lt;/sup&gt;)

: &lt;math&gt; P(X - \mu \ge a) \le \frac{\sigma^2}{ \sigma^2 + a^2 } &lt;/math&gt;

where ''a'' ≥ 0.

This inequality can be used to prove a one tailed variant of Chebyshev's inequality with ''k'' &gt; 0&lt;ref name=Grimmett00&gt;Grimmett and Stirzaker, problem 7.11.9. Several proofs of this result can be found in [http://www.mcdowella.demon.co.uk/Chebyshev.html Chebyshev's Inequalities] by A. G. McDowell.&lt;/ref&gt;

:&lt;math&gt; \Pr(X - \mu \geq k \sigma) \leq \frac{ 1 }{ 1 + k^2 }. &lt;/math&gt;

The bound on the one tailed variant is known to be sharp. To see this consider the random variable ''X'' that takes the values

: &lt;math&gt; X = 1 &lt;/math&gt; with probability &lt;math&gt; \frac{ \sigma^2 } { 1 + \sigma^2 }&lt;/math&gt;
: &lt;math&gt; X = - \sigma^2 &lt;/math&gt; with probability &lt;math&gt; \frac{ 1 } { 1 + \sigma^2 }.&lt;/math&gt;

Then E(''X'') = 0 and E(''X''&lt;sup&gt;2&lt;/sup&gt;) = ''σ''&lt;sup&gt;2&lt;/sup&gt; and ''P''(''X'' &lt; 1) = 1 / (1 + ''σ''&lt;sup&gt;2&lt;/sup&gt;).

; An application – distance between the mean and the median &lt;!-- This section is linked from [[median]]. --&gt;

The one-sided variant can be used to prove the proposition that for [[probability distribution]]s having an [[expected value]] and a [[median]], the mean and the median can never differ from each other by more than one [[standard deviation]].  To express this in symbols let ''μ'', ''ν'', and ''σ'' be respectively the mean, the median, and the standard deviation. Then

:&lt;math&gt; \left | \mu - \nu \right | \leq \sigma. &lt;/math&gt;

There is no need to assume that the variance is finite because this inequality is trivially true if the variance is infinite.

The proof is as follows. Setting ''k''&amp;nbsp;=&amp;nbsp;1 in the statement for the one-sided inequality gives:

:&lt;math&gt;\Pr(X - \mu \geq \sigma) \leq \frac{ 1 }{ 2 } \implies \Pr(X \geq \mu + \sigma) \leq \frac{ 1 }{ 2 }. &lt;/math&gt;

Changing the sign of ''X'' and of ''μ'', we get

:&lt;math&gt;\Pr(X \leq \mu - \sigma) \leq \frac{ 1 }{ 2 }. &lt;/math&gt;

As the median is by definition any real number&amp;nbsp;''m'' that satisfies the inequalities

:&lt;math&gt;\operatorname{P}(X\leq m) \geq \frac{1}{2}\text{ and }\operatorname{P}(X\geq m) \geq \frac{1}{2}&lt;/math&gt;

this implies that the median lies within one standard deviation of the mean. A proof using Jensen's inequality also [[Median#An inequality relating means and medians|exists]].

===Bhattacharyya's inequality===
Bhattacharyya&lt;ref name=Bhattacharyya1987&gt;{{cite journal |last=Bhattacharyya |first=B. B. |title=One-sided chebyshev inequality when the first four moments are known |journal=Communications in Statistics – Theory and Methods |year=1987 |volume=16 |issue=9 |pages=2789–91 |doi=10.1080/03610928708829540 |issn=0361-0926}}&lt;/ref&gt; extended Cantelli's inequality using the third and fourth moments of the distribution.

Let ''μ'' = 0 and ''σ''&lt;sup&gt;2&lt;/sup&gt; be the variance. Let γ = ''E''(''X''&lt;sup&gt;3&lt;/sup&gt;) / ''σ''&lt;sup&gt;3&lt;/sup&gt; and κ = ''E''(''X''&lt;sup&gt;4&lt;/sup&gt;) / ''σ''&lt;sup&gt;4&lt;/sup&gt;.

If ''k''&lt;sup&gt;2&lt;/sup&gt; − ''k''γ − 1 &gt; 0 then

:&lt;math&gt; P(X &gt; k\sigma) \le \frac{ \kappa - \gamma^2 - 1 }{ (\kappa - \gamma^2 - 1) (1 + k^2) + (k^2 - k\gamma - 1) }.&lt;/math&gt;

The necessity of ''k''&lt;sup&gt;2&lt;/sup&gt; − ''k''γ − 1 &gt; 0 requires that ''k'' be reasonably large.

===Mitzenmacher and Upfal's inequality===
[[Michael Mitzenmacher|Mitzenmacher]] and [[Eli Upfal|Upfal]]&lt;ref name=Mitzenmacher2005&gt;{{cite book |last1=Mitzenmacher |first1=Michael |authorlink1=Michael Mitzenmacher |last2=Upfal |first2=Eli |authorlink2=Eli Upfal |title=Probability and Computing: Randomized Algorithms and Probabilistic Analysis |date=January 2005 |publisher=Cambridge Univ. Press |location=Cambridge [u.a.] |isbn=9780521835404 |url=http://www.cambridge.org/us/knowledge/isbn/item1171566/?site_locale=en_US |edition=Repr. |accessdate=6 October 2012}}&lt;/ref&gt; note that

: &lt;math&gt;  ( X - \operatorname{E}[ X ] )^{ 2k } &gt; 0 &lt;/math&gt;

for any integer ''k'' &gt; 0 and that

: &lt;math&gt;  \operatorname{E}[ ( X -  \operatorname{E}( X ) )^{ 2k } ] &lt;/math&gt;

is the ''2k''&lt;sup&gt;th&lt;/sup&gt; central moment. They then show that for ''t'' &gt; 0

: &lt;math&gt; \Pr\left( | X - \operatorname{E}[ X ] | &gt; t \operatorname{E}[ ( X - \operatorname{E}[ X ] )^{ 2k } ]^{ 1 / 2k } \right) \le \frac{ 1 }{ t^{ 2k } } . &lt;/math&gt;

For ''k'' = 1 we obtain Chebyshev's inequality. For ''t'' ≥ 1, ''k'' &gt; 2 and assuming that the ''k''&lt;sup&gt;th&lt;/sup&gt; moment exists, this bound is tighter than Chebyshev's inequality.

==Related inequalities==
Several other related inequalities are also known.

===Zelen's inequality===
Zelen has shown that&lt;ref name=Zelen1954&gt;Zelen M. (1954) Bounds on a distribution function that are functions of moments to order four. J Res Nat Bur Stand 53:377–381&lt;/ref&gt;

:&lt;math&gt;\Pr( X - \mu  \ge k \sigma ) \le \left[1 + k^2 + \frac{\left( k^2 - k \theta_3 - 1 \right)^2}{\theta_4 - \theta_3^2 - 1} \right]^{-1} &lt;/math&gt;

with

:&lt;math&gt;k \ge \frac{\theta_3 + \sqrt{\theta_3^2 + 4}}{ 2 }, \qquad \theta_m = \frac{ M_m }{ \sigma } &lt;/math&gt;

where {{mvar|M&lt;sub&gt;m&lt;/sub&gt;}} is the {{mvar|m}}-th moment and {{mvar|σ}} is the standard deviation.

===He, Zhang and Zhang's inequality===
For any collection of {{mvar|n}} non-negative independent random variables {{mvar|X&lt;sub&gt;i&lt;/sub&gt;}} with expectation 1 &lt;ref name=He2010&gt;{{cite journal | last1 = He | first1 = S. | last2 = Zhang | first2 = J. | last3 = Zhang | first3 = S. | year = 2010 | title = Bounding probability of small deviation: A fourth moment approach | url = | journal = Mathematics of Operations Research | volume = 35 | issue = 1| pages = 208–232 | doi = 10.1287/moor.1090.0438 }}&lt;/ref&gt;

: &lt;math&gt; \Pr\left ( \frac{\sum_{i=1}^n X_i }{n} - 1 \ge \frac{1}{n} \right) \le \frac{ 7 }{ 8 }. &lt;/math&gt;

===Hoeffding's lemma===
Let {{mvar|X}} be a random variable with {{math|''a'' ≤ ''X'' ≤ ''b''}} and {{math|E[''X''] {{=}} 0}}, then for any {{math|''s'' &gt; 0}}, we have

: &lt;math&gt; E \left[ e^{ sX } \right ] \le e^{ \frac{1}{8}s^2 ( b - a )^2 }.&lt;/math&gt;

===Van Zuijlen's bound===
Let {{mvar|X&lt;sub&gt;i&lt;/sub&gt;}} be a set of independent [[Rademacher distribution|Rademacher random variables]]: {{math|Pr(''X&lt;sub&gt;i&lt;/sub&gt;'' {{=}} 1) {{=}} Pr(''X&lt;sub&gt;i&lt;/sub&gt;'' {{=}} −1) {{=}} 0.5}}. Then&lt;ref name=vanZuijlen2011&gt;Martien C. A. van Zuijlen (2011) [https://arxiv.org/abs/1112.4988 On a conjecture concerning the sum of independent Rademacher random variables]&lt;/ref&gt;

: &lt;math&gt; \Pr \left( \left | \frac{\sum_{i=1}^n X_i} \sqrt n \right| \le 1  \right ) \ge 0.5. &lt;/math&gt;

The bound is sharp and better than that which can be derived from the normal distribution (approximately {{math|Pr &gt; 0.31}}).

==Unimodal distributions==
A distribution function ''F'' is unimodal at ''ν'' if its cumulative distribution function is [[convex function|convex]] on (−∞, ''ν'') and [[concave function|concave]] on (''ν'',∞)&lt;ref name=Feller1966&gt;{{cite book |last=Feller |first=William |authorlink=William Feller |title=An Introduction to Probability Theory and Its Applications, Volume 2 |year=1966 |publisher=Wiley |url=https://books.google.com/?id=LhrvAAAAMAAJ&amp;dq=%22An+introduction+to+probability+theory+and+its+applications%22+%22volume+2%22+feller |edition=2 |accessdate=6 October 2012 |page=155}}&lt;/ref&gt; An empirical distribution can be tested for unimodality with the [[dip test]].&lt;ref name=Hartigan1985&gt;Hartigan J. A., Hartigan P. M. (1985) [http://projecteuclid.org/euclid.aos/1176346577 "The dip test of unimodality"]. ''Annals of Statistics'' 13(1):70–84  {{doi|10.1214/aos/1176346577}} {{MR|773153}}&lt;/ref&gt;

In 1823 [[Gauss]] showed that for a [[unimodal distribution]] with a mode of zero&lt;ref name=Gauss1823&gt;Gauss C. F. Theoria Combinationis Observationum Erroribus Minimis Obnoxiae. Pars Prior. Pars Posterior. Supplementum. Theory of the Combination of Observations Least Subject to Errors. Part One. Part Two. Supplement. 1995. Translated by G. W. Stewart. Classics in Applied Mathematics Series, Society for Industrial and Applied Mathematics, Philadelphia&lt;/ref&gt;

: &lt;math&gt; P( | X | \ge k ) \le  \frac{ 4 \operatorname{ E }( X^2 ) } { 9k^2 } \quad\text{if} \quad k^2 \ge \frac{ 4 } { 3 } \operatorname{E} (X^2) ,&lt;/math&gt;

: &lt;math&gt; P( | X | \ge k ) \le  1 - \frac{ k } { \sqrt{3} \operatorname{ E }( X^2 ) } \quad \text{if} \quad k^2 \le \frac{ 4 } { 3 } \operatorname{ E }( X^2 ). &lt;/math&gt;

If the mode is not zero and the mean (''μ'') and standard deviation (''σ'') are both finite,  then denoting the median as ''ν'' and the root mean square deviation from the mode by ''ω'', we have{{citation needed|date=July 2012}}

: &lt;math&gt; \sigma \le \omega \le 2 \sigma &lt;/math&gt;

and

: &lt;math&gt; | \nu - \mu | \le \sqrt{ \frac{ 3 }{ 4 } } \omega. &lt;/math&gt;

Winkler in 1866 extended Gauss' inequality to ''r''&lt;sup&gt;th&lt;/sup&gt; moments &lt;ref name=Winkler1886&gt;Winkler A. (1886) Math-Natur theorie Kl. Akad. Wiss Wien Zweite Abt 53, 6–41&lt;/ref&gt; where ''r'' &gt; 0 and the distribution is unimodal with a mode of zero:

: &lt;math&gt; P( | X | \ge k ) \le  \left( \frac{ r } { r + 1 } \right)^r \frac{ \operatorname{ E }( | X | )^r } { k^r } \quad \text{if} \quad k^r \ge \frac{ r^r } { ( r + 1 )^{ r + 1 } } \operatorname{ E }( | X |^r ), &lt;/math&gt;

: &lt;math&gt; P( | X | \ge k) \le \left( 1 - \left[ \frac{ k^r }{ ( r + 1 ) \operatorname{ E }( | X | )^r } \right]^{ 1 / r } \right) \quad \text{if} \quad k^r \le \frac{ r^r } { ( r + 1 )^{ r + 1 } } \operatorname{ E }( | X |^r ). &lt;/math&gt;

Gauss' bound has been subsequently sharpened and extended to apply to departures from the mean rather than the mode due to the Vysochanskiï–Petunin inequality.

The [[Vysochanskiï–Petunin inequality]] has been extended by Dharmadhikari and Joag-Dev&lt;ref name=Dharmadhikari1985&gt;{{cite journal | last1 = Dharmadhikari | first1 = S. W. | last2 = Joag-Dev | first2 = K. | year = 1985 | title = The Gauss–Tchebyshev inequality for unimodal distributions | url = | journal = Teor Veroyatnost I Primenen | volume = 30 | issue = 4| pages = 817–820 }}&lt;/ref&gt;

: &lt;math&gt; P( | X | &gt; k ) \le \max\left( \left[ \frac{ r }{( r + 1 ) k } \right]^r E| X^r |, \frac{ s }{( s - 1 ) k^r } E| X^r | - \frac{ 1 }{ s - 1 } \right) &lt;/math&gt;

where ''s'' is a constant satisfying both ''s'' &gt; ''r'' + 1 and ''s''(''s''&amp;nbsp;−&amp;nbsp;''r''&amp;nbsp;−&amp;nbsp;1) =&amp;nbsp;''r''&lt;sup&gt;''r''&lt;/sup&gt; and&amp;nbsp;''r''&amp;nbsp;&gt;&amp;nbsp;0.

It can be shown that these inequalities are the best possible and that further sharpening of the bounds requires that additional restrictions be placed on the distributions.

===Unimodal symmetrical distributions===
The bounds on this inequality can also be sharpened if the distribution is both [[unimodal]] and [[Symmetric probability distribution|symmetrical]].&lt;ref name=Clarkson2009&gt;{{cite journal
  |title = ROC and the bounds on tail probabilities via theorems of Dubins and F. Riesz
  |last1 = Clarkson
  |first1 = Eric
  |last2 = Denny
  |first2 = J. L.
  |last3 = Shepp
  |first3 = Larry
  |journal = [[The Annals of Applied Probability]]
  |volume = 19
  |issue = 1
  |pages = 467–76
  |year = 2009
  |pmid =  20191100
  |pmc = 2828638
  |doi = 10.1214/08-AAP536
|arxiv = 0903.0518
  }}&lt;/ref&gt; An empirical distribution can be tested for symmetry with a number of tests including McWilliam's R*.&lt;ref&gt;{{cite journal
  |title = A Distribution-Free Test for Symmetry Based on a Runs Statistic
  |last = McWilliams
  |first = Thomas P.
  |journal = [[Journal of the American Statistical Association]]
  |issn = 0162-1459
  |volume = 85
  |issue = 412
  |year = 1990
  |pages = 1130–3
  |doi = 10.2307/2289611
  |jstor = 2289611
  
  }}&lt;/ref&gt; It is known that the variance of a unimodal symmetrical distribution with finite support [''a'',&amp;nbsp;''b''] is less than or equal to ( ''b''&amp;nbsp;−&amp;nbsp;''a'' )&lt;sup&gt;2&lt;/sup&gt; / 12.&lt;ref name=Seaman1987&gt;{{cite journal |last1=Seaman |first1=John W., Jr. |last3=Odell |first3=Patrick L. |last2=Young |first2=Dean M. |title=Improving small sample variance estimators for bounded random variables |journal=Industrial Mathematics |issn=0019-8528 |year=1987 |volume=37 |zbl=0637.62024 | pages=65–75}}&lt;/ref&gt;

Let the distribution be supported on the finite [[interval (mathematics)|interval]] [ −''N'',&amp;nbsp;''N'' ] and the variance be finite. Let the [[mode (statistics)|mode]] of the distribution be zero and rescale the variance to&amp;nbsp;1. Let ''k''&amp;nbsp;&gt;&amp;nbsp;0 and assume ''k''&amp;nbsp;&lt;&amp;nbsp;2''N''/3. Then&lt;ref name="Clarkson2009"/&gt;

: &lt;math&gt; P( X \ge k ) \le \frac{ 1 }{ 2 } - \frac{ k }{ 2 \sqrt{ 3 } } \quad \text{if} \quad 0 \le k \le \frac{ 2 }{ \sqrt{ 3 } },&lt;/math&gt;

: &lt;math&gt; P( X \ge k ) \le \frac{ 2 }{ 9k^2 } \quad \text{if} \quad \frac{ 2 }{ \sqrt{ 3 } } \le k \le \frac{ 2N }{ 3 }. &lt;/math&gt;

If 0 &lt; ''k'' ≤ 2 / {{radic|3}} the bounds are reached with the density&lt;ref name="Clarkson2009"/&gt;

: &lt;math&gt; f( x ) = \frac{ 1 }{ 2 \sqrt{ 3 } } \quad \text{if} \quad | x | &lt; \sqrt{ 3 } &lt;/math&gt;

: &lt;math&gt; f( x ) = 0 \quad \text{if} \quad  | x | \ge \sqrt{ 3 }. &lt;/math&gt;

If 2 / {{radic|3}} &lt; ''k'' ≤ 2''N'' / 3 the bounds are attained by the distribution

:&lt;math&gt; ( 1 - \beta_k ) \delta_0 ( x ) + \beta_k f_k( x ), &lt;/math&gt;

where ''β''&lt;sub&gt;k&lt;/sub&gt; = 4 / 3''k''&lt;sup&gt;2&lt;/sup&gt;, ''δ''&lt;sub&gt;0&lt;/sub&gt; is the [[Dirac delta function]] and where

: &lt;math&gt; f_k( x ) = \frac{ 1 }{ 3k } \quad \text{if} \quad | x | &lt; \frac{ 3k }{ 2 }, &lt;/math&gt;

: &lt;math&gt; f_k( x ) = 0 \quad \text{if} \quad | x | \ge \frac{ 3k }{ 2 }. &lt;/math&gt;

The existence of these densities shows that the bounds are optimal. Since ''N'' is arbitrary these bounds apply to any value of ''N''.

The Camp–Meidell's inequality is a related inequality.&lt;ref name=Bickel1992&gt;{{cite journal |last1=Bickel |first1=Peter J. |authorlink1=Peter J. Bickel |last2=Krieger |first2=Abba M. |title=Extensions of Chebyshev's Inequality with Applications |journal=Probability and Mathematical Statistics |year=1992 |volume=13 |issue=2 |pages=293–310 |url=http://www.math.uni.wroc.pl/~pms/publicationsArticle.php?nr=13.2&amp;nrA=11&amp;ppB=%20293&amp;ppE=%20310 |accessdate=6 October 2012 |issn=0208-4147}}&lt;/ref&gt; For an absolutely continuous unimodal and symmetrical distribution

: &lt;math&gt; P( | X - \mu | \ge k \sigma ) \le 1 - \frac{ k }{ \sqrt{ 3 } } \quad \text{if} \quad k \le \frac{ 2 }{ \sqrt { 3 } },&lt;/math&gt;

: &lt;math&gt; P( | X - \mu | \ge k \sigma ) \le \frac{ 4 }{ 9k^2 } \quad \text{if} \quad k &gt; \frac{ 2 }{ \sqrt { 3 } }.&lt;/math&gt;

The second of these inequalities is the same as the Vysochanskiï–Petunin inequality.

DasGupta has shown that if the distribution is known to be normal&lt;ref name=DasGupta2000&gt;{{cite journal | last1 = DasGupta | first1 = A | year = 2000 | title = Best constants in Chebychev inequalities with various applications | url = | journal = Metrika | volume = 5 | issue = 1| pages = 185–200 }}&lt;/ref&gt;

: &lt;math&gt; P( | X - \mu | \ge k \sigma ) \le \frac{ 1 }{ 3 k^2 } .&lt;/math&gt;

===Notes===
;Effects of symmetry and unimodality

Symmetry of the distribution decreases the inequality's bounds by a factor of 2 while unimodality sharpens the bounds by a factor of 4/9.

Because the mean and the mode in a unimodal distribution differ by at most {{radic|3}} standard deviations&lt;ref name=unimodal&gt;{{cite web|url=http://www.se16.info/~se16/hgb/cheb2.htm#3unimodalinequalities |title=More thoughts on a one tailed version of Chebyshev's inequality – by Henry Bottomley |publisher=se16.info |accessdate=2012-06-12}}&lt;/ref&gt; at most 5% of a symmetrical unimodal distribution lies outside (2{{radic|10}}&amp;nbsp;+&amp;nbsp;3{{radic|3}})/3 standard deviations of the mean (approximately 3.840 standard deviations). This is sharper than the bounds provided by the Chebyshev inequality (approximately 4.472 standard deviations).

These bounds on the mean are less sharp than those that can be derived from symmetry of the distribution alone which shows that at most 5% of the distribution lies outside approximately 3.162 standard deviations of the mean. The Vysochanskiï–Petunin inequality further sharpens this bound by showing that for such a distribution that at most 5% of the distribution lies outside 4{{radic|5}}/3 (approximately 2.981) standard deviations of the mean.

; Symmetrical unimodal distributions

For any symmetrical unimodal distribution

*  at most approximately 5.784% of the distribution lies outside 1.96 standard deviations of the mode
*  at most 5% of the distribution lies outside 2{{radic|10}}/3 (approximately 2.11) standard deviations of the mode

;Normal distributions

DasGupta's inequality states that for a normal distribution at least 95% lies within approximately 2.582 standard deviations of the mean. This is less sharp than the true figure (approximately 1.96 standard deviations of the mean).

==Bounds for specific distributions==
*DasGupta has determined a set of best possible bounds for a [[normal distribution]] for this inequality.&lt;ref name=DasGupta2000 /&gt;
*Steliga and Szynal have extended these bounds to the [[Pareto distribution]].&lt;ref name=Steliga2010 /&gt;
*Grechuk et.al. developed a general method for deriving the best possible bounds in Chebyshev's inequality for any family of distributions, and any [[deviation risk measure]] in place of standard deviation. In particular, they derived Chebyshev inequality for distributions with [[Logarithmically concave function|log-concave]] densities.&lt;ref name="cheb"&gt;Grechuk, B., Molyboha, A., Zabarankin, M. (2010).
[https://www.researchgate.net/publication/231939730_Chebyshev_inequalities_with_law-invariant_deviation_measures Chebyshev Inequalities with Law Invariant Deviation Measures], Probability in the Engineering and Informational Sciences, 24(1), 145-170.&lt;/ref&gt;

==Zero means==
When the mean (''μ'') is zero Chebyshev's inequality takes a simple form. Let ''σ''&lt;sup&gt;2&lt;/sup&gt; be the variance. Then

: &lt;math&gt; P(| X | \ge 1) \le \sigma^2 .&lt;/math&gt;

With the same conditions Cantelli's inequality takes the form

: &lt;math&gt; P(X \ge 1) \le \frac{ \sigma^2 }{ 1 + \sigma^2 } .&lt;/math&gt;

===Unit variance===
If in addition ''E''( ''X''&lt;sup&gt;2&lt;/sup&gt; ) = 1 and ''E''( ''X''&lt;sup&gt;4&lt;/sup&gt; ) = ''ψ'' then for any 0 ≤ ''ε'' ≤ 1&lt;ref name=Godwin1964a&gt;Godwin H. J. (1964) Inequalities on distribution functions. (Chapter 3) New York, Hafner Pub. Co.&lt;/ref&gt;

: &lt;math&gt; P( | X | &gt; \epsilon ) \ge \frac{ ( 1 - \epsilon^2 )^2 }{ \psi - 1 + ( 1 - \epsilon^2 )^2 } \ge \frac{( 1 - \epsilon^2 )^2 }{ \psi }.&lt;/math&gt;

The first inequality is sharp.
This is known as the [[Paley–Zygmund inequality]].

It is also known that for a random variable obeying the above conditions that&lt;ref name=Lesley2003&gt;Lesley F. D., Rotar V. I. (2003) Some remarks on lower bounds of Chebyshev's type for half-lines. J Inequalities Pure Appl Math 4(5) Art 96&lt;/ref&gt;

: &lt;math&gt; P( X \ge \epsilon ) \ge \frac{ C_0 }{ \psi } - \frac{ C_1 }{ \sqrt{ \psi } } \epsilon + \frac{ C_2 }{ \psi \sqrt{ \psi } } \epsilon &lt;/math&gt;

where

: &lt;math&gt; C_0 = 2 \sqrt{ 3 } - 3 \quad ( \approxeq 0.464 ), &lt;/math&gt;

: &lt;math&gt; C_1 = 1.397 ,&lt;/math&gt;

: &lt;math&gt; C_2 = 0.0231 .&lt;/math&gt;

It is also known that&lt;ref name="Lesley2003"/&gt;

: &lt;math&gt; P( X &gt; 0 ) \ge \frac{ C_0 }{ \psi }. &lt;/math&gt;

The value of C&lt;sub&gt;0&lt;/sub&gt; is optimal and the bounds are sharp if

: &lt;math&gt; \psi \ge \frac{ 3 }{ \sqrt{ 3 } + 1 } \quad ( \approxeq 1.098 ) .&lt;/math&gt;

If

: &lt;math&gt; \psi \le \frac{ 3 }{ \sqrt{ 3 } + 1 } &lt;/math&gt;

then the sharp bound is

: &lt;math&gt; P( X &gt; 0 ) \ge \frac{ 2 }{ 3 + \psi + \sqrt{ ( 1 + \psi )^2 - 4 } }. &lt;/math&gt;

==Integral Chebyshev inequality==

There is a second (less well known) inequality also named after Chebyshev&lt;ref name=Fink1984&gt;{{cite journal |last1=Fink |first1=A. M. |last2=Jodeit |first2=Max, Jr. |title=On Chebyshev's Other Inequality |journal=Institute of Mathematical Statistics Lecture Notes – Monograph Series |isbn=978-0-940600-04-1 |mr=789242 |editor1-first=Y. L. |editor1-last=Tong |editor2-last=Gupta |editor2-first=Shanti S. |year=1984 |volume=5 |series=Institute of Mathematical Statistics Lecture Notes - Monograph Series |pages=115–120 |doi=10.1214/lnms/1215465637 |url=http://projecteuclid.org/euclid.lnms/1215465617 |accessdate=7 October 2012}}&lt;/ref&gt;

If ''f'', ''g'' : [''a'', ''b''] → '''R''' are two [[monotonic]] [[function (mathematics)|function]]s of the same monotonicity, then

: &lt;math&gt; \frac{ 1 }{ b - a } \int_a^b \! f(x) g(x) \,dx \ge  \left[ \frac{ 1 }{ b - a } \int_a^b \! f(x) \,dx \right] \left[ \frac{ 1 }{ b - a } \int_a^b \! g(x) \,dx \right] .&lt;/math&gt;

If ''f'' and ''g'' are of opposite monotonicity, then the above inequality works in the reverse way.

This inequality is related to [[Jensen's inequality]],&lt;ref name=Niculescu2001&gt;{{cite journal |last=Niculescu |first=Constantin P. |title=An extension of Chebyshev's inequality and its connection with Jensen's inequality |journal=Journal of Inequalities and Applications |year=2001 |volume=6 |issue=4 |pages=451–462 |doi=10.1155/S1025583401000273 |url=http://emis.matem.unam.mx/journals/HOA/JIA/Volume6_4/462.html |accessdate=6 October 2012 |issn=1025-5834}}&lt;/ref&gt; [[Kantorovich's inequality]],&lt;ref name=Niculescu2001a&gt;{{cite journal |last1=Niculescu |first1=Constantin P. |last2=Pečarić |first2=Josip |authorlink2=Josip Pečarić |title=The Equivalence of Chebyshev's Inequality to the Hermite–Hadamard Inequality |journal=Mathematical Reports |year=2010 |volume=12 |issue=62 |pages=145–156 |url=http://www.csm.ro/reviste/Mathematical_Reports/Pdfs/2010/2/Niculescu.pdf |accessdate=6 October 2012 |issn=1582-3067}}&lt;/ref&gt; the [[Hermite–Hadamard inequality]]&lt;ref name="Niculescu2001a"/&gt; and [[Walter's conjecture]].&lt;ref name=Malamud2001&gt;{{cite journal |last=Malamud |first=S. M. |title=Some complements to the Jensen and Chebyshev inequalities and a problem of W. Walter |journal=Proceedings of the American Mathematical Society |date=15 February 2001 |volume=129 |issue=9 |pages=2671–2678 |doi=10.1090/S0002-9939-01-05849-X |mr=1838791 |url=http://www.ams.org/journals/proc/2001-129-09/S0002-9939-01-05849-X/ |accessdate=7 October 2012 |issn=0002-9939}}&lt;/ref&gt;

===Other inequalities===

There are also a number of other inequalities associated with Chebyshev:

*[[Chebyshev's sum inequality]]
*[[Chebyshev–Markov–Stieltjes inequalities]]

==Haldane's transformation==
One use of Chebyshev's inequality in applications is to create confidence intervals for variates with an unknown distribution. [[J. B. S. Haldane|Haldane]] noted,&lt;ref name=Haldane1952&gt;{{cite journal | last1 = Haldane | first1 = J. B.|authorlink=J. B. S. Haldane | year = 1952 | title = Simple tests for bimodality and bitangentiality | url = | journal = [[Annals of Eugenics]] | volume = 16 | issue = 4| pages = 359–364 | doi = 10.1111/j.1469-1809.1951.tb02488.x }}&lt;/ref&gt; using an equation derived by [[Maurice Kendall|Kendall]],&lt;ref name=Kendall1943&gt;Kendall M. G. (1943) The Advanced Theory of Statistics, 1. London&lt;/ref&gt; that if a variate (''x'') has a zero mean, unit variance and both finite [[skewness]] (''γ'') and [[kurtosis]] (''κ'') then the variate can be converted to a normally distributed [[standard score]] (''z''):

: &lt;math&gt; z = x - \frac{ \gamma }{ 6 } (x^2 - 1) + \frac{ x }{ 72 } [ 2 \gamma^2 (4 x^2 - 7) - 3 \kappa (x^2 - 3) ] + \cdots &lt;/math&gt;

This transformation may be useful as an alternative to Chebyshev's inequality or as an adjunct to it for deriving confidence intervals for variates with unknown distributions.

While this transformation may be useful for moderately skewed and/or kurtotic distributions, it performs poorly when the distribution is markedly skewed and/or kurtotic.

==Notes==

The [[Environmental Protection Agency]] has suggested best practices for the use of Chebyshev's inequality for estimating confidence intervals.&lt;ref&gt;{{cite report
 | title      = Calculating Upper Confidence Limits for Exposure Point Concentrations at hazardous Waste Sites
 | publisher  = Office of Emergency and Remedial Response of the U.S. Environmental Protection Agency
 |date=December 2002
 | url        = http://nepis.epa.gov/Exe/ZyNET.exe/P100CYCE.TXT?ZyActionD=ZyDocument&amp;Client=EPA&amp;Index=2000+Thru+2005&amp;Docs=&amp;Query=&amp;Time=&amp;EndTime=&amp;SearchMethod=1&amp;TocRestrict=n&amp;Toc=&amp;TocEntry=&amp;QField=&amp;QFieldYear=&amp;QFieldMonth=&amp;QFieldDay=&amp;IntQFieldOp=0&amp;ExtQFieldOp=0&amp;XmlQuery=&amp;File=D%3A%5Czyfiles%5CIndex%20Data%5C00thru05%5CTxt%5C00000029%5CP100CYCE.txt&amp;User=ANONYMOUS&amp;Password=anonymous&amp;SortMethod=h%7C-&amp;MaximumDocuments=1&amp;FuzzyDegree=0&amp;ImageQuality=r75g8/r75g8/x150y150g16/i425&amp;Display=p%7Cf&amp;DefSeekPage=x&amp;SearchBack=ZyActionL&amp;Back=ZyActionS&amp;BackDesc=Results%20page&amp;MaximumPages=1&amp;ZyEntry=1&amp;SeekPage=x&amp;ZyPURL#
 | accessdate = 5 August 2016}}&lt;/ref&gt; This caution appears to be justified as its use in this context may be seriously misleading.&lt;ref&gt;{{cite web
  |title = Statistical Tests: The Chebyshev UCL Proposal
  |website = Quantitative Decisions
  |date = 25 March 2001
  |accessdate = 26 November 2015
  |url = http://www.quantdec.com/envstats/notes/class_12/ucl.htm
}}&lt;/ref&gt;

==See also==
*[[Multidimensional Chebyshev's inequality]]
*[[Concentration inequality]] – a summary of tail-bounds on random variables.
*[[Cornish–Fisher expansion]]
*[[Eaton's inequality]]
*[[Kolmogorov's inequality]]
*[[Law of large numbers/Proof|Proof of the weak law of large numbers]] using Chebyshev's inequality
*[[Le Cam's theorem]]
*[[Paley–Zygmund inequality]]
*[[Vysochanskiï–Petunin inequality]] — a stronger result applicable to [[unimodal probability distributions]]

==References==
{{reflist|30em}}

==Further reading==
* A. Papoulis (1991), ''Probability, Random Variables, and Stochastic Processes'', 3rd ed. McGraw–Hill. {{isbn|0-07-100870-5}}. pp.&amp;nbsp;113–114.
* [[Geoffrey Grimmett|G. Grimmett]] and D. Stirzaker (2001), ''Probability and Random Processes'', 3rd ed. Oxford. {{isbn|0-19-857222-0}}. Section 7.3.

==External links==
{{commons category}}
* {{springer|title=Chebyshev inequality in probability theory|id=p/c021890}}
* [http://mws.cs.ru.nl/mwiki/random_2.html#T7 Formal proof] in the [[Mizar system]].

[[Category:Articles containing proofs]]
[[Category:Probabilistic inequalities]]
[[Category:Statistical inequalities]]</text>
      <sha1>njiu4a33uw3ww8aptata0a511oq6c0u</sha1>
    </revision>
  </page>
  <page>
    <title>Cop number</title>
    <ns>0</ns>
    <id>56005174</id>
    <revision>
      <id>846517581</id>
      <parentid>845054897</parentid>
      <timestamp>2018-06-19T07:37:58Z</timestamp>
      <contributor>
        <username>Bibcode Bot</username>
        <id>14394459</id>
      </contributor>
      <minor/>
      <comment>Adding 0 [[arXiv|arxiv eprint(s)]], 1 [[bibcode|bibcode(s)]] and 0 [[digital object identifier|doi(s)]]. Did it miss something? Report bugs, errors, and suggestions at [[User talk:Bibcode Bot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6966">In [[graph theory]], a branch of mathematics, the '''cop number''' or '''copnumber''' of an [[undirected graph]] is the number of cops to win a certain [[pursuit-evasion]] game on the graph.

==Rules==
In this game, one player controls the position of a given number of cops and the other player controls the position of a robber. The cops are trying to catch the robber by moving to the same position, while the robber is trying to remain uncaught. Thus, the players perform the following actions, taking turns with each other:{{r|af}}
*On the first turn of the game, the player controlling the cops places each cop on a vertex of the graph (allowing more than one cop to be placed on the same vertex).
*Next, the player controlling the robber places the robber on a vertex of the graph.
*On each subsequent turn, the player controlling the cops chooses a (possibly empty) subset of the cops, and moves each of these cops to adjacent vertices. The remaining cops (if any) stay put.
*On the robber's turn, he may either move to an adjacent vertex or stay put.
The game ends with a win for the cops whenever the robber occupies the same vertex as a cop. If this never happens, the robber wins.

The cop number of a graph &lt;math&gt;G&lt;/math&gt; is the minimum number &lt;math&gt;k&lt;/math&gt; such that &lt;math&gt;k&lt;/math&gt; cops can win the game on &lt;math&gt;G&lt;/math&gt;.{{r|af}}

==Example==
On a [[tree (graph theory)|tree]], the cop number is one. The cop can start anywhere, and at each step move to the unique neighbor that is closer to the robber. Each of the cop's steps reduces the size of the subtree that the robber is confined to, so the game eventually ends.

[[File:Rangersrundown.jpg|thumb|By gradually moving closer together, two cops can eventually catch a robber on any cycle (here, a baseball diamond)]]
On a [[cycle graph]] of length more than three, the cop number is two. If there is only one cop, the robber can move to a position two steps away from the cop, and always maintain the same distance after each move of the robber. In this way, the robber can evade capture forever. However, if there are two cops, one can stay at one vertex and cause the robber and the other cop to play in the remaining path. If the other cop follows the tree strategy, the robber will eventually lose.

==General results==
Every graph whose [[girth (graph theory)|girth]] is greater than four has cop number at least equal to its [[degree (graph theory)|minimum degree]].{{r|m}} It follows that there exist graphs of arbitrarily high cop number.

{{unsolved|mathematics|What is the largest possible cop number of an &lt;math&gt;n&lt;/math&gt;-vertex graph?}}
Henri Meyniel (also known for [[Meyniel graph]]s) conjectured in 1985 that every &lt;math&gt;n&lt;/math&gt;-vertex graph has cop number &lt;math&gt;O(\sqrt n)&lt;/math&gt;. The [[Levi graph]]s of [[finite projective plane]]s have girth six and minimum degree &lt;math&gt;\Omega(\sqrt n)&lt;/math&gt;, so if true this bound would be the best possible.{{r|bb}}
It is known that all graphs have sublinear cop number,{{r|f}}
but the problems of obtaining a tight bound, and of proving or disproving '''Meyniel's conjecture''', remain unsolved.{{r|bb}}

Computing the cop number of a given graph is [[EXPTIME-complete]],{{r|k}} and hard for [[parameterized complexity]].{{r|fgk}}

==Special classes of graphs==
The [[cop-win graph]]s are the graphs with cop number equal to one.{{r|af}}

Every [[planar graph]] has cop number at most three.{{r|af}}
More generally, every graph has cop number at most proportional to its [[Graph embedding|genus]].{{r|s}} However, the best known lower bound for
the cop number in terms of the genus is approximately the square root of the genus, which is
far from the upper bound when the genus is large.{{r|m}}

The [[treewidth]] of a graph can also be obtained as the result of a pursuit-evasion game, but one in which the robber can move along arbitrary-length paths instead of a single edge in each turn. This extra freedom means that the cop number is generally smaller than the treewidth. More specifically,
on graphs of [[treewidth]] &lt;math&gt;w&lt;/math&gt;, the cop number is at most &lt;math&gt;\lfloor w/2\rfloor+1&lt;/math&gt;.{{r|c}}

==References==
{{reflist|30em|refs=

&lt;ref name=af&gt;{{citation
 | last1 = Aigner | first1 = M. | author1-link = Martin Aigner
 | last2 = Fromme | first2 = M.
 | issue = 1
 | journal = [[Discrete Applied Mathematics]]
 | mr = 739593
 | pages = 1–11
 | title = A game of cops and robbers
 | doi = 10.1016/0166-218X(84)90073-8
 | volume = 8
 | year = 1984}}&lt;/ref&gt;

&lt;ref name=bb&gt;{{citation
 | last1 = Baird | first1 = William
 | last2 = Bonato | first2 = Anthony
 | arxiv = 1308.3385
 | doi = 10.4310/JOC.2012.v3.n2.a6
 | issue = 2
 | journal = Journal of Combinatorics
 | mr = 2980752
 | pages = 225–238
 | title = Meyniel's conjecture on the cop number: a survey
 | volume = 3
 | year = 2012}}&lt;/ref&gt;

&lt;ref name=c&gt;{{citation
 | last = Clarke | first = Nancy Elaine Blanche
 | location = Canada
 | mr = 2704200
 | publisher = Dalhousie University
 | series = Ph.D. thesis
 | title = Constrained cops and robber
 | url = https://search.proquest.com/docview/305503876
 | year = 2002}}&lt;/ref&gt;

&lt;ref name=f&gt;{{citation
 | last = Frankl | first = Péter | authorlink = Péter Frankl
 | issue = 3
 | journal = [[Discrete Applied Mathematics]]
 | mr = 890640
 | pages = 301–305
 | title = Cops and robbers in graphs with large girth and Cayley graphs
 | doi = 10.1016/0166-218X(87)90033-3
 | volume = 17
 | year = 1987}}&lt;/ref&gt;

&lt;ref name=fgk&gt;{{citation
 | last1 = Fomin | first1 = Fedor V. | author1-link = Fedor Fomin
 | last2 = Golovach | first2 = Petr A.
 | last3 = Kratochvíl | first3 = Jan | author3-link = Jan Kratochvíl
 | contribution = On tractability of cops and robbers game
 | doi = 10.1007/978-0-387-09680-3_12
 | mr = 2757374
 | pages = 171–185
 | publisher = Springer | location = New York
 | series = IFIP Int. Fed. Inf. Process.
 | title = Fifth IFIP International Conference on Theoretical Computer Science—TCS 2008
 | volume = 273
 | year = 2008}}&lt;/ref&gt;

&lt;ref name=k&gt;{{citation
 | last = Kinnersley | first = William B.
 | arxiv = 1309.5405
 | doi = 10.1016/j.jctb.2014.11.002
 | journal = Journal of Combinatorial Theory
 | mr = 3315605
 | pages = 201–220
 | series = Series B
 | title = Cops and robbers is EXPTIME-complete
 | volume = 111
 | year = 2015}}&lt;/ref&gt;

&lt;ref name=m&gt;{{citation
 | last = Mohar | first = Bojan | author-link = Bojan Mohar
 | arxiv = 1710.11281
 | title = Notes on Cops and Robber game on graphs
 | year = 2017| bibcode = 2017arXiv171011281M}}&lt;/ref&gt;

&lt;ref name=s&gt;{{citation
 | last = Schroeder | first = Bernd S. W.
 | contribution = The copnumber of a graph is bounded by &lt;math&gt;\lfloor\tfrac{3}{2} \operatorname{genus}(G)\rfloor+3&lt;/math&gt;
 | mr = 1827672
 | pages = 243–263
 | publisher = Birkhäuser | location = Boston
 | series = Trends Math.
 | title = Categorical perspectives (Kent, OH, 1998)
 | year = 2001}}&lt;/ref&gt;
}}

[[Category:Graph invariants]]
[[Category:Pursuit-evasion]]</text>
      <sha1>7s0qcswmgsdc6ln06yitjmhm52wfd3a</sha1>
    </revision>
  </page>
  <page>
    <title>Critical value</title>
    <ns>0</ns>
    <id>1086571</id>
    <revision>
      <id>791317318</id>
      <parentid>789996993</parentid>
      <timestamp>2017-07-19T13:21:31Z</timestamp>
      <contributor>
        <ip>80.149.135.170</ip>
      </contributor>
      <comment>In "statistical test", the usage of the critical value is explained and illustrated extensively.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1298">Critical value  may  refer to:

==Topology==
In [[differential topology]], a '''critical value''' of a [[differentiable function]] {{nowrap|1=ƒ : ''M'' → ''N''}} between [[differentiable manifold]]s is the [[Image (mathematics)|image]] (value of) ƒ(''x'') in ''N'' of a [[critical point (mathematics)|critical point]] ''x'' in ''M''.&lt;ref&gt;do Carmo, Manfredo Perdigão,  ''Differential geometry of curves and surfaces'', [[Prentice Hall]], 1976.&lt;/ref&gt;

== Statistics ==

In [[statistical hypothesis testing]], the critical values of a [[statistical test]] are the boundaries of the acceptance region of the test&lt;ref&gt;{{Cite web|url=https://www.statlect.com/glossary/critical-value|title=Critical value|website=www.statlect.com|access-date=2017-07-10}}&lt;/ref&gt;. The acceptance region is the set of values of the test statistic for which the null hypothesis is not rejected. Depending on the shape of the acceptance region, there can be one or more than one critical value. 

==Complex dynamics ==
In [[complex dynamics]], a [[Complex_quadratic_polynomial#Critical_value|critical value]] is the image of a [[Complex_quadratic_polynomial#Critical_point|critical point]].

==References==
{{reflist}}

[[Category:Multivariable calculus]]
[[Category:Differential topology]]
[[Category:Critical phenomena]]</text>
      <sha1>tekuzm6xkyzkirb6sc7clswzlnbkz78</sha1>
    </revision>
  </page>
  <page>
    <title>De Sitter space</title>
    <ns>0</ns>
    <id>301666</id>
    <revision>
      <id>864452288</id>
      <parentid>863608220</parentid>
      <timestamp>2018-10-17T08:59:33Z</timestamp>
      <contributor>
        <username>Exile oi</username>
        <id>9423239</id>
      </contributor>
      <comment>/* References */ add doi</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10151">{{multiple issues|
{{expert needed|1=physics|date=May 2017}}
{{technical|date=May 2017}}
}}
In [[mathematics]] and [[physics]], a '''de Sitter space''' is the analog in [[Minkowski space]], or spacetime, of a sphere in ordinary [[Euclidean space]]. The ''n''-dimensional de Sitter space, denoted dS&lt;sub&gt;''n''&lt;/sub&gt;, is the [[Lorentzian manifold]] analog of an [[n-sphere|''n''-sphere]] (with its canonical [[Riemannian metric]]); it is maximally [[symmetric]], has constant positive [[scalar curvature|curvature]], and is [[simply connected]] for ''n'' at least 3. De Sitter space and [[anti-de Sitter space]] are named after [[Willem de Sitter]] (1872–1934), professor of astronomy at [[Leiden University]] and director of the [[Leiden Observatory]]. Willem de Sitter and [[Albert Einstein]] worked closely together in [[Leiden]] in the 1920s on the spacetime structure of our universe.

In the language of [[general relativity]], de Sitter space is the maximally symmetric [[vacuum solution]] of [[Einstein's field equations]] with a positive [[cosmological constant]] &lt;math&gt;\Lambda&lt;/math&gt; (corresponding to a positive vacuum energy density and negative pressure). When {{nowrap|1=''n'' = 4}} (3 space dimensions plus time), it is a cosmological model for the physical universe; see [[de Sitter universe]].

De Sitter space&lt;ref&gt;{{Citation|last=de Sitter|first=W.|year=1917|title=On the relativity of inertia: Remarks concerning Einstein's latest hypothesis|journal=Proc. Kon. Ned. Acad. Wet.|volume=19|pages=1217–1225}}&lt;/ref&gt;&lt;ref&gt;{{Citation|last=de Sitter|first=W.|year=1917|title=On the curvature of space|journal=Proc. Kon. Ned. Acad. Wet.|volume=20|pages=229–243}}&lt;/ref&gt; was also discovered, independently, and about the same time, by [[Tullio Levi-Civita]].&lt;ref&gt;{{Citation|first=Tullio|last=Levi-Civita|authorlink=Tullio Levi-Civita|title=Realtà fisica di alcuni spazî normali del Bianchi|journal=Rendiconti, Reale Accademia Dei Lincei|volume=26|year=1917|pages=519–31}}
&lt;/ref&gt;

More recently it has been considered as the setting for [[special relativity]] rather than using [[Minkowski space]], since a [[group contraction]] reduces the [[isometry]] group of de Sitter space to the [[Poincaré group]], allowing a unification of the [[spacetime]] [[Translation (geometry)|translation subgroup]] and [[Lorentz transformation|Lorentz transformation subgroup]] of the Poincaré group into a [[simple group]] rather than a [[semi-simple group]]. This alternative formulation of special relativity is called [[de Sitter relativity]].

==Definition==
De Sitter space can be defined as a [[submanifold]] of a generalized [[Minkowski space]] of one higher [[dimension]]. Take Minkowski space '''R'''&lt;sup&gt;1,''n''&lt;/sup&gt; with the standard [[metric tensor|metric]]:
:&lt;math&gt;ds^2 = -dx_0^2 + \sum_{i=1}^n dx_i^2.&lt;/math&gt;
De Sitter space is the submanifold described by the [[hyperboloid]] of one sheet
:&lt;math&gt;-x_0^2 + \sum_{i=1}^n x_i^2 = \alpha^2&lt;/math&gt;
where &lt;math&gt;\alpha&lt;/math&gt; is some nonzero constant with dimensions of length. The [[metric tensor|metric]] on de Sitter space is the metric induced from the ambient Minkowski metric. The induced metric is [[nondegenerate]] and has Lorentzian signature. (Note that if one replaces &lt;math&gt;\alpha^2&lt;/math&gt; with &lt;math&gt;-\alpha^2&lt;/math&gt; in the above definition, one obtains a [[hyperboloid]] of two sheets. The induced metric in this case is [[Definite quadratic form|positive-definite]], and each sheet is a copy of [[hyperbolic space|hyperbolic ''n''-space]]. For a detailed proof, see [[Minkowski space#Geometry|geometry of Minkowski space]].)

De Sitter space can also be defined as the [[Homogeneous space|quotient]] {{nowrap|O(1, ''n'') / O(1, ''n'' − 1)}} of two [[indefinite orthogonal group]]s, which shows that it is a non-Riemannian [[symmetric space]].

[[Topology|Topologically]], de Sitter space is {{nowrap|'''R''' × ''S''&lt;sup&gt;''n''−1&lt;/sup&gt;}} (so that if {{nowrap|''n'' ≥ 3}} then de Sitter space is [[simply connected]]).

==Properties==
The [[isometry group]] of de Sitter space is the [[Lorentz group]] {{nowrap|O(1, ''n'')}}. The metric therefore then has {{nowrap|''n''(''n'' + 1)/2}} independent [[Killing vector field]]s and is maximally symmetric. Every maximally symmetric space has constant curvature. The [[Riemann curvature tensor]] of de Sitter is given by
:&lt;math&gt;R_{\rho\sigma\mu\nu} = {1\over \alpha^2}(g_{\rho\mu}g_{\sigma\nu} - g_{\rho\nu}g_{\sigma\mu}) .&lt;/math&gt;

De Sitter space is an [[Einstein manifold]] since the [[Ricci tensor]] is proportional to the metric:
:&lt;math&gt;R_{\mu\nu} = \frac{n-1}{\alpha^2}g_{\mu\nu}&lt;/math&gt;
This means de Sitter space is a vacuum solution of Einstein's equation with cosmological constant given by
:&lt;math&gt;\Lambda = \frac{(n-1)(n-2)}{2\alpha^2}.&lt;/math&gt;
The [[scalar curvature]] of de Sitter space is given by
:&lt;math&gt;R = \frac{n(n-1)}{\alpha^2} = \frac{2n}{n-2}\Lambda.&lt;/math&gt;
For the case {{nowrap|1=''n'' = 4}}, we have {{nowrap|1=Λ = 3/''α''&lt;sup&gt;2&lt;/sup&gt;}} and {{nowrap|1=''R'' = 4Λ = 12/''α''&lt;sup&gt;2&lt;/sup&gt;}}.

==Static coordinates==
We can introduce [[static spacetime|static coordinates]] &lt;math&gt;(t, r, \ldots)&lt;/math&gt; for de Sitter as follows:
:&lt;math&gt;x_0 = \sqrt{\alpha^2-r^2}\sinh(t/\alpha)&lt;/math&gt;
:&lt;math&gt;x_1 = \sqrt{\alpha^2-r^2}\cosh(t/\alpha)&lt;/math&gt;
:&lt;math&gt;x_i = r z_i \qquad\qquad\qquad\qquad\qquad 2\le i\le n.&lt;/math&gt;
where &lt;math&gt;z_i&lt;/math&gt; gives the standard embedding the {{nowrap|(''n'' − 2)}}-sphere in '''R'''&lt;sup&gt;''n''&amp;minus;1&lt;/sup&gt;. In these coordinates the de Sitter metric takes the form:

:&lt;math&gt;ds^2 = -\left(1-\frac{r^2}{\alpha^2}\right)dt^2 + \left(1-\frac{r^2}{\alpha^2}\right)^{-1}dr^2 + r^2 d\Omega_{n-2}^2.&lt;/math&gt;

Note that there is a [[cosmological horizon]] at &lt;math&gt;r = \alpha&lt;/math&gt;.

==Flat slicing==
Let
:&lt;math&gt;x_0 = \alpha \sinh(t/\alpha) + r^2 e^{t/\alpha}/2\alpha,&lt;/math&gt;
:&lt;math&gt;x_1 = \alpha \cosh(t/\alpha) - r^2 e^{t/\alpha}/2\alpha,&lt;/math&gt;
:&lt;math&gt;x_i = e^{t/\alpha}y_i, \qquad 2 \leq i \leq n&lt;/math&gt;
where &lt;math&gt;r^2=\sum_i y_i^2&lt;/math&gt;.  Then in the &lt;math&gt;(t,y_i)&lt;/math&gt; coordinates metric reads:
:&lt;math&gt;ds^{2} = -dt^{2} + e^{2t/\alpha} dy^{2}&lt;/math&gt;
where &lt;math&gt;dy^2=\sum_i dy_i^2&lt;/math&gt; is the flat metric on &lt;math&gt;y_i&lt;/math&gt;'s.

Setting &lt;math&gt;\zeta = \zeta_{\infty} - \alpha e^{-t/\alpha}&lt;/math&gt;, we obtain the conformally flat metric:
:&lt;math&gt;ds^{2} = \frac{\alpha^2}{(\zeta_{\infty} - \zeta)^2}(dy^{2}-d\zeta^2)&lt;/math&gt;

==Open slicing==
Let
:&lt;math&gt;x_0 = \alpha \sinh(t/\alpha) \cosh\xi,&lt;/math&gt;
:&lt;math&gt;x_1 = \alpha \cosh(t/\alpha),&lt;/math&gt;
:&lt;math&gt;x_i = \alpha z_i \sinh(t/\alpha) \sinh\xi, \qquad 2 \leq i \leq n&lt;/math&gt;
where &lt;math&gt;\sum_i z_i^2 = 1&lt;/math&gt; forming a &lt;math&gt;S^{n-2}&lt;/math&gt; with the standard metric &lt;math&gt;\sum_i dz_i^2 = d\Omega_{n-2}^2&lt;/math&gt;.  Then the metric of the de Sitter space reads
:&lt;math&gt;ds^2 = -dt^2 + \alpha^2 \sinh^2(t/\alpha) dH_{n-1}^2,&lt;/math&gt;
where
:&lt;math&gt;dH_{n-1}^2 = d\xi^2 + \sinh^2(\xi) d\Omega_{n-2}^2&lt;/math&gt;
is the standard hyperbolic metric.

==Closed slicing==
Let
:&lt;math&gt;x_0 = \alpha \sinh(t/\alpha),&lt;/math&gt;
:&lt;math&gt;x_i = \alpha \cosh(t/\alpha) z_i, \qquad 1 \leq i \leq n&lt;/math&gt;
where &lt;math&gt;z_i&lt;/math&gt;s describe a &lt;math&gt;S^{n-1}&lt;/math&gt;.  Then the metric reads:
:&lt;math&gt;ds^2 = -dt^2 + \alpha^2 \cosh^2(t/\alpha) d\Omega_{n-1}^2.&lt;/math&gt;

Changing the time variable to the conformal time via &lt;math&gt;\tan(\eta/2)=\tanh(t/2\alpha)&lt;/math&gt; we obtain a metric conformally equivalent to Einstein static universe:
:&lt;math&gt;ds^2 = \frac{\alpha^2}{\cos^2\eta}(-d\eta^2 + d\Omega_{n-1}^2).&lt;/math&gt;
This serves to find the [[Penrose diagram]] of de Sitter space.{{clarify|date=November 2012}}

==dS slicing==
Let
:&lt;math&gt;x_0 = \alpha \sin(\chi/\alpha) \sinh(t/\alpha) \cosh\xi,&lt;/math&gt;
:&lt;math&gt;x_1 = \alpha \cos(\chi/\alpha),&lt;/math&gt;
:&lt;math&gt;x_2 = \alpha \sin(\chi/\alpha) \cosh(t/\alpha),&lt;/math&gt;
:&lt;math&gt;x_i = \alpha z_i \sin(\chi/\alpha) \sinh(t/\alpha) \sinh\xi, \qquad 3 \leq i \leq n&lt;/math&gt;
where &lt;math&gt;z_i&lt;/math&gt;s describe a &lt;math&gt;S^{n-3}&lt;/math&gt;.  Then the metric reads:
:&lt;math&gt;ds^2 = d\chi^2 + \sin^2(\chi/\alpha) ds_{dS,\alpha,n-1}^2,&lt;/math&gt;
where
:&lt;math&gt;ds_{dS,\alpha,n-1}^2 = -dt^2 + \alpha^2 \sinh^2(t/\alpha) dH_{n-2}^2&lt;/math&gt;
is the metric of an &lt;math&gt;n-1&lt;/math&gt; dimensional de Sitter space with radius of curvature &lt;math&gt;\alpha&lt;/math&gt; in open slicing coordinates.  The hyperbolic metric is given by:
:&lt;math&gt;dH_{n-2}^2 = d\xi^2 + \sinh^2\xi d\Omega_{n-3}^2.&lt;/math&gt;

This is the analytic continuation of the open slicing coordinates under &lt;math&gt;(t,\xi,\theta,\phi_1,\phi_2,\cdots,\phi_{n-3}) \to (i\chi,\xi,it,\theta,\phi_1,\cdots,\phi_{n-4})&lt;/math&gt; and also switching &lt;math&gt;x_0&lt;/math&gt; and &lt;math&gt;x_2&lt;/math&gt; because they change their timelike/spacelike nature.

==See also==
* [[Anti-de Sitter space]]
* [[de Sitter universe]]
* [[AdS/CFT correspondence]]
* [[de Sitter–Schwarzschild metric]]

==References==
*{{springer|id=d/d110040|title=De Sitter space|author=Qingming Cheng}}
*{{Citation|last=Nomizu|first=Katsumi|authorlink=Katsumi Nomizu|title=The Lorentz–Poincaré metric on the upper half-space and its extension|journal=[[Hokkaido Mathematical Journal]]|volume=11|year=1982|pages=253–261|issue=3|doi=10.14492/hokmj/1381757803}}
*{{Citation|last=Coxeter|first=H. S. M.|authorlink=Harold Scott MacDonald Coxeter|title=A geometrical background for de Sitter's world|journal=[[American Mathematical Monthly]]|volume=50|year=1943|pages=217–228|doi=10.2307/2303924|issue=4|publisher=Mathematical Association of America|jstor=2303924}}
*{{Citation|last1=Susskind|first1=L.|last2=Lindesay|first2=J.|title=An Introduction to Black Holes, Information and the String Theory Revolution:The Holographic Universe|year=2005|page=119(11.5.25)}}

==External links==
* [http://www.quantumfieldtheory.info/dS_and_AdS_spaces.pdf  Simplified Guide to de Sitter and anti-de Sitter Spaces] A pedagogic introduction to de Sitter and anti-de Sitter spaces. The main article is simplified, with almost no math. The appendix is technical and intended for readers with physics or math backgrounds.

==Notes==
{{reflist}}

{{DEFAULTSORT:De Sitter Space}}
[[Category:Exact solutions in general relativity]]
[[Category:Differential geometry]]
[[Category:Minkowski spacetime]]</text>
      <sha1>o7y2glalyxa76ufutt5c536ggdkci0k</sha1>
    </revision>
  </page>
  <page>
    <title>Devex algorithm</title>
    <ns>0</ns>
    <id>40129720</id>
    <revision>
      <id>827743254</id>
      <parentid>825542327</parentid>
      <timestamp>2018-02-26T13:45:52Z</timestamp>
      <contributor>
        <username>Certes</username>
        <id>5984052</id>
      </contributor>
      <comment>+wikilink</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="701">{{refimprove|date=August 2013}}
In applied mathematics, the '''devex algorithm''' is a pivot rule for the [[simplex method]] developed by Harris.&lt;ref&gt;Harris, Paula MJ. "[https://link.springer.com/article/10.1007/BF01580108 Pivot selection methods of the Devex LP code]." Mathematical programming 5.1 (1973): 1–28.&lt;/ref&gt; It identifies the steepest-edge approximately in its search for the optimal solution.&lt;ref&gt;Forrest, John J., and [[Donald Goldfarb]]. "[https://link.springer.com/article/10.1007/BF01581089 Steepest-edge simplex algorithms for linear programming]." Mathematical programming 57.1–3 (1992): 341–374.&lt;/ref&gt;

==References==
{{reflist}}

[[Category:Algorithms]]


{{algorithm-stub}}</text>
      <sha1>j13rx9s7x4chqvwqnzwikdr52600cm8</sha1>
    </revision>
  </page>
  <page>
    <title>Distinguished limit</title>
    <ns>0</ns>
    <id>43559240</id>
    <revision>
      <id>788408519</id>
      <parentid>788408499</parentid>
      <timestamp>2017-07-01T06:43:18Z</timestamp>
      <contributor>
        <username>EngiZe</username>
        <id>26299357</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="394">{{context|date=August 2014}}
In [[mathematics]], a '''distinguished limit''' is an appropriately chosen scale factor used in the [[method of matched asymptotic expansions]].

==External links==
* [http://www.scholarpedia.org/article/Singular_perturbation_theory Singular perturbation theory], [[Scholarpedia]]

[[Category:Differential equations]]
[[Category:Asymptotic analysis]]

{{math-stub}}</text>
      <sha1>g1tv1sj7up8bww6zxczb5uq7q4vwo3u</sha1>
    </revision>
  </page>
  <page>
    <title>Divergent series</title>
    <ns>0</ns>
    <id>876428</id>
    <revision>
      <id>868817174</id>
      <parentid>862875186</parentid>
      <timestamp>2018-11-14T16:49:56Z</timestamp>
      <contributor>
        <username>Gamren</username>
        <id>25596954</id>
      </contributor>
      <comment>/* Properties of summation methods */ Doesn't seem to relate to anything.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="28964">{{About||the book series|Divergent trilogy|the film series|The Divergent Series}}

:''For an elementary calculus-based introduction, see [[v:Divergent series|Divergent series on Wikiversity]].''
{{quotebox|thumb|width=30%

|quote=Les séries divergentes sont en général&lt;br&gt;quelque chose de bien fatal et c’est une honte qu’on ose y fonder aucune démonstration.
("Divergent series are in general something fatal, and it is a disgrace to base any proof on them." Often translated as "Divergent series are an invention of the devil …")
|source=[[N. H. Abel]], letter to Holmboe, January 1826, reprinted in volume 2 of his collected papers.}}

In [[mathematics]], a '''divergent series''' is an [[infinite series]] that is not [[Convergent series|convergent]], meaning that the infinite [[sequence]] of the [[partial sum]]s of the series does not have a finite [[limit of a sequence|limit]].

If a series converges, the individual terms of the series must approach zero. Thus any series in which the individual terms do not approach zero diverges. However, convergence is a stronger condition: not all series whose terms approach zero converge. A counterexample is the [[harmonic series (mathematics)|harmonic series]]

:&lt;math&gt;1 + \frac{1}{2} + \frac{1}{3} + \frac{1}{4} + \frac{1}{5} + \cdots =\sum_{n=1}^\infty\frac{1}{n}.&lt;/math&gt;

The divergence of the harmonic series [[Harmonic series (mathematics)#Divergence|was proven]] by the medieval mathematician [[Nicole Oresme]].

In specialized mathematical contexts, values can be objectively assigned to certain series whose sequence of partial sums diverges, this is to make meaning of the divergence of the series. A '''summability method''' or '''summation method''' is a [[partial function]] from the set of series to values. For example, [[Cesàro summation]] assigns [[Grandi's series|Grandi's divergent series]]

:&lt;math&gt;1 - 1 + 1 - 1 + \cdots&lt;/math&gt;

the value {{sfrac|2}}. Cesàro summation is an '''averaging''' method, in that it relies on the [[arithmetic mean]] of the sequence of partial sums. Other methods involve [[analytic continuation]]s of related series. In [[physics]], there are a wide variety of summability methods; these are discussed in greater detail in the article on [[regularization (physics)|regularization]].

==History==

{{quotebox|right|thumb|width=33%
|quote=... before Cauchy mathematicians asked not 'How shall we define 1&amp;nbsp;−&amp;nbsp;1&amp;nbsp;+&amp;nbsp;1...?' but 
'What is 1&amp;nbsp;−&amp;nbsp;1&amp;nbsp;+&amp;nbsp;1...?' and that this habit of mind led them into unnecessary perplexities and controversies which were often really verbal.
|source=G. H. Hardy, Divergent series, page 6}}

Before the 19th century, divergent series were widely used by [[Leonhard Euler]] and others, but often led to confusing and contradictory results. A major problem was Euler's idea that any divergent series should have a natural sum, without first defining what is meant by the sum of a divergent series. [[Augustin-Louis Cauchy]] eventually gave a rigorous definition of the sum of a (convergent) series, and for some time after this, divergent series were mostly excluded from mathematics. They reappeared in 1886 with [[Henri Poincaré]]'s work on asymptotic series. In 1890, [[Ernesto Cesàro]] realized that one could give a rigorous definition of the sum of some divergent series, and defined [[Cesàro summation]]. (This was not the first use of Cesàro summation, which was used implicitly by [[Ferdinand Georg Frobenius]] in 1880; Cesàro's key contribution was not the discovery of this method, but his idea that one should give an explicit definition of the sum of a divergent series.) In the years after Cesàro's paper, several other mathematicians gave other definitions of the sum of a divergent series, although these are not always compatible: different definitions can give different answers for the sum of the same divergent series; so, when talking about the sum of a divergent series, it is necessary to specify which summation method one is using.

==Examples==

* [[Grandi's series|1 - 1 + 1 - 1 + · · ·]] "&lt;math&gt;=&lt;/math&gt;" &lt;math&gt;\frac{1}{2}&lt;/math&gt;
* [[1 − 2 + 3 − 4 + · · ·]] "&lt;math&gt;=&lt;/math&gt;" &lt;math&gt;\frac{1}{4}&lt;/math&gt;
* [[1 − 1 + 2 − 6 + 24 − 120 + ...]] "&lt;math&gt;=&lt;/math&gt;" &lt;math&gt;\int_0^\infty \frac{e^{-x}}{1+x} \, dx \approx 0.596\,347\ldots&lt;/math&gt;
* [[1 − 2 + 4 − 8 + ⋯]] "&lt;math&gt;=&lt;/math&gt;" &lt;math&gt;\frac{1}{3}&lt;/math&gt;
* [[1 + 2 + 4 + 8 + ⋯]] "&lt;math&gt;=&lt;/math&gt;" &lt;math&gt;-1&lt;/math&gt;
* [[1 + 1 + 1 + 1 + · · ·]] "&lt;math&gt;=&lt;/math&gt;" &lt;math&gt;-\frac{1}{2}&lt;/math&gt;
* [[1 + 2 + 3 + 4 + · · ·]] "&lt;math&gt;=&lt;/math&gt;" &lt;math&gt;-\frac{1}{12}&lt;/math&gt;

==Theorems on methods for summing divergent series==
A summability method ''M'' is '''[[#Properties of summation methods|regular]]''' if it agrees with the actual limit on all [[convergent series]]. Such a result is called an '''[[Abelian theorem]]''' for ''M'', from the prototypical [[Abel's theorem]]. More interesting, and in general more subtle, are partial converse results, called '''[[tauberian theorems]]''', from a prototype proved by [[Alfred Tauber]]. Here ''partial converse'' means that if ''M'' sums the series ''Σ'', and some side-condition holds, then ''Σ'' was convergent in the first place; without any side-condition such a result would say that ''M'' only summed convergent series (making it useless as a summation method for divergent series).

The function giving the sum of a convergent series is '''[[#Properties of summation methods|linear]]''', and it follows from the [[Hahn–Banach theorem]] that it may be extended to a summation method summing any series with bounded partial sums. This is called the [[Banach limit]]. This fact is not very useful in practice, since there are many such extensions, [[inconsistent]] with each other, and also since proving such operators exist requires invoking the [[axiom of choice]] or its equivalents, such as [[Zorn's lemma]]. They are therefore nonconstructive.

The subject of divergent series, as a domain of [[mathematical analysis]], is primarily concerned with explicit and natural techniques such as [[Abel summation]], [[Cesàro summation]] and [[Borel summation]], and their relationships. The advent of [[Wiener's tauberian theorem]] marked an epoch in the subject, introducing unexpected connections to [[Banach algebra]] methods in [[Fourier analysis]].

Summation of divergent series is also related to [[extrapolation]] methods and [[sequence transformation]]s as numerical techniques. Examples of such techniques are [[Padé approximant]]s, [[Levin-type sequence transformation]]s, and order-dependent mappings related to [[renormalization]] techniques for large-order [[perturbation theory]] in [[quantum mechanics]].

==Properties of summation methods==
Summation methods usually concentrate on the sequence of partial sums of the series. While this sequence does not converge, we may often find that when we take an [[average]] of larger and larger numbers of initial terms of the sequence, the average converges, and we can use this average instead of a limit to evaluate the sum of the series. A '''summation method''' can be seen as a function from a set of sequences of partial sums to values. If '''A''' is any summation method assigning values to a set of sequences, we may mechanically translate this to a '''series-summation method''' '''A'''&lt;sup&gt;''Σ''&lt;/sup&gt; that assigns the same values to the corresponding series. There are certain properties it is desirable for these methods to possess if they are to arrive at values corresponding to limits and sums, respectively.

# '''Regularity'''. A summation method is ''regular'' if, whenever the sequence ''s'' converges to ''x'', {{nowrap|1='''A'''(''s'') = ''x''.}} Equivalently, the corresponding series-summation method evaluates {{nowrap|1='''A'''&lt;sup&gt;''Σ''&lt;/sup&gt;(''a'') = ''x''.}}
# '''Linearity'''. '''A''' is ''linear'' if it is a linear functional on the sequences where it is defined, so that {{nowrap|1 = '''A'''(''k'' ''r'' + ''s'') = ''k'' '''A'''(''r'') + '''A'''(''s'')}}  for sequences ''r'', ''s'' and a real or complex scalar ''k''. Since the terms {{nowrap|1 = ''a''&lt;sub&gt;''n''+1&lt;/sub&gt; = ''s''&lt;sub&gt;''n''+1&lt;/sub&gt; − ''s''&lt;sub&gt;''n''&lt;/sub&gt;}} of the series ''a'' are linear functionals on the sequence ''s'' and vice versa, this is equivalent to '''A'''&lt;sup&gt;''Σ''&lt;/sup&gt; being a linear functional on the terms of the series.
# '''Stability''' (also called translativity). If ''s'' is a sequence starting from ''s''&lt;sub&gt;0&lt;/sub&gt; and ''s''′ is the sequence obtained by omitting the first value and subtracting it from the rest, so that {{nowrap|1=''s''′&lt;sub&gt;''n''&lt;/sub&gt; = ''s''&lt;sub&gt;''n''+1&lt;/sub&gt; − ''s''&lt;sub&gt;0&lt;/sub&gt;}}, then '''A'''(''s'') is defined if and only if '''A'''(''s''′) is defined, and {{nowrap|1='''A'''(''s'') = ''s''&lt;sub&gt;0&lt;/sub&gt; + '''A'''(''s''′).}} Equivalently, whenever {{nowrap|1=''a''′&lt;sub&gt;''n''&lt;/sub&gt; = ''a''&lt;sub&gt;''n''+1&lt;/sub&gt;}} for all ''n'', then {{nowrap|1='''A'''&lt;sup&gt;''Σ''&lt;/sup&gt;(''a'') = ''a''&lt;sub&gt;0&lt;/sub&gt; + '''A'''&lt;sup&gt;''Σ''&lt;/sup&gt;(''a''′).}}&lt;ref&gt;{{cite web|title=Summation methods|website=Michon's Numericana|url=http://www.numericana.com/answer/sums.htm}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Translativity|website=The Encyclopedia of Mathematics|publisher=Springer|url=http://www.encyclopediaofmath.org/index.php/Translativity_of_a_summation_method}}&lt;/ref&gt; Another way of stating this is that the [[shift rule]] must be valid for the series that are summable by this method.

The third condition is less important, and some significant methods, such as [[Borel summation]], do not possess it.&lt;ref&gt;{{citation
 | last = Muraev | first = E. B.
 | issue = 6
 | journal = Akademiya Nauk SSSR
 | mr = 515185
 | pages = 1332–1340, 1438
 | title = Borel summation of ''n''-multiple series, and entire functions associated with them
 | volume = 19
 | year = 1978}}. Muraev observes that Borel summation is translative in one of the two directions: augmenting a series by a zero placed at its start does not change the summability or value of the series. However, he states "the converse is false".&lt;/ref&gt;

One can also give a weaker alternative to the last condition.  
{{ordered list|start=3A|'''Finite re-indexability'''.  If ''a'' and ''a''′ are two series such that there exists a [[bijection]] &lt;math&gt; f: \mathbb{N} \rightarrow \mathbb{N} &lt;/math&gt; such that {{nowrap|1=''a''&lt;sub&gt;''i''&lt;/sub&gt; = ''a''′&lt;sub&gt;''f''(''i'')&lt;/sub&gt;}} for all ''i'', and if there exists some &lt;math&gt; N \in \mathbb{N} &lt;/math&gt; such that {{nowrap|1=''a''&lt;sub&gt;''i''&lt;/sub&gt; = ''a''′&lt;sub&gt;''i''&lt;/sub&gt;}} for all ''i''&amp;nbsp;&gt;&amp;nbsp;''N'', then {{nowrap|1='''A'''&lt;sup&gt;''Σ''&lt;/sup&gt;(''a'') = '''A'''&lt;sup&gt;''Σ''&lt;/sup&gt;(''a''′).}}  (In other words, ''a''′ is the same series as ''a'', with only finitely many terms re-indexed.)  Note that this is a weaker condition than '''Stability''', because any summation method that exhibits '''Stability''' also exhibits '''Finite re-indexability''', but the converse is not true.}}

A desirable property for two distinct summation methods '''A''' and '''B''' to share is ''consistency'': '''A''' and '''B''' are [[consistent]] if for every sequence ''s'' to which both assign a value, {{nowrap|1='''A'''(''s'') = '''B'''(''s'').}} If two methods are consistent, and one sums more series than the other, the one summing more series is ''stronger''.

There are powerful numerical summation methods that are neither regular nor linear, for instance nonlinear [[sequence transformation]]s like [[Levin-type sequence transformation]]s and [[Padé approximant]]s, as well as the order-dependent mappings of perturbative series based on [[renormalization]] techniques.

Taking regularity, linearity and stability as axioms, it is possible to sum many divergent series by elementary algebraic manipulations. This partly explains why many different summation methods give the same answer for certain series.

For instance, whenever {{nowrap|1=''r'' ≠ 1,}} the [[Divergent geometric series|geometric series]]
:&lt;math&gt;\begin{align}
G(r,c) &amp; = \sum_{k=0}^\infty cr^k &amp; &amp; \\
 &amp; = c + \sum_{k=0}^\infty cr^{k+1} &amp; &amp; \text{ (stability) } \\
 &amp; = c + r \sum_{k=0}^\infty cr^k &amp; &amp; \text{ (linearity) } \\
 &amp; = c + r \, G(r,c), &amp; &amp; \text{ hence } \\
G(r,c) &amp; = \frac{c}{1-r} ,\text{ unless it is infinite} &amp; &amp; \\
\end{align}&lt;/math&gt;
can be evaluated regardless of convergence. More rigorously, any summation method that possesses these properties and which assigns a finite value to the geometric series must assign this value. However, when ''r'' is a real number larger than 1, the partial sums increase without bound, and averaging methods assign a limit of infinity.

==Classical summation methods==

The two classical summation methods for series, ordinary convergence and absolute convergence, define the sum as a limit of certain partial sums. These are included only for completeness; strictly speaking they are not true summation methods for divergent series since, by definition, a series is divergent only if these methods do not work. Most but not all summation methods for divergent series extend these methods to a larger class of sequences.

===Absolute convergence===

Absolute convergence defines the sum of a sequence (or set) of numbers to be the limit of the net of all partial sums {{nowrap|''a''&lt;sub&gt;''k''&lt;sub&gt;1&lt;/sub&gt;&lt;/sub&gt; + ... + ''a''&lt;sub&gt;''k''&lt;sub&gt;''n''&lt;/sub&gt;&lt;/sub&gt;}}, if it exists. It does not depend on the order of the elements of the sequence, and a classical theorem says that a sequence is absolutely convergent if and only if the sequence of absolute values is convergent in the standard sense.

===Sum of a series===

Cauchy's classical definition of the sum of a series {{nowrap|''a''&lt;sub&gt;0&lt;/sub&gt; + ''a''&lt;sub&gt;1&lt;/sub&gt; + ...}} defines the sum to be the limit of the sequence of partial sums {{nowrap|''a''&lt;sub&gt;0&lt;/sub&gt; + ... + ''a''&lt;sub&gt;''n''&lt;/sub&gt;}}. This is the default definition of convergence of a sequence.

==Nørlund means==
Suppose ''p&lt;sub&gt;n&lt;/sub&gt;'' is a sequence of positive terms, starting from ''p''&lt;sub&gt;0&lt;/sub&gt;. Suppose also that
:&lt;math&gt;\frac{p_n}{p_0+p_1 + \cdots + p_n} \rightarrow 0.&lt;/math&gt;
If now we transform a sequence s by using ''p'' to give weighted means, setting
:&lt;math&gt;t_m = \frac{p_m s_0 + p_{m-1}s_1 + \cdots + p_0 s_m}{p_0+p_1+\cdots+p_m}&lt;/math&gt;
then the limit of ''t&lt;sub&gt;n&lt;/sub&gt;'' as ''n'' goes to infinity is an average called the '''[[Niels Erik Nørlund|Nørlund]] mean''' '''N'''&lt;sub&gt;''p''&lt;/sub&gt;(''s'').

The Nørlund mean is regular, linear, and stable. Moreover, any two Nørlund means are consistent.

===Cesàro summation===

The most significant of the Nørlund means are the Cesàro sums. Here, if we define the sequence ''p&lt;sup&gt;k&lt;/sup&gt;'' by
:&lt;math&gt;p_n^k = {n+k-1 \choose k-1}&lt;/math&gt;
then the Cesàro sum ''C''&lt;sub&gt;''k''&lt;/sub&gt; is defined by {{nowrap|1=''C''&lt;sub&gt;''k''&lt;/sub&gt;(''s'') = '''N'''&lt;sub&gt;&lt;big&gt;(&lt;/big&gt;''p&lt;sup&gt;k&lt;/sup&gt;''&lt;big&gt;)&lt;/big&gt;&lt;/sub&gt;(''s'').}} Cesàro sums are Nørlund means if {{nowrap|1=''k'' ≥ 0}}, and hence are regular, linear, stable, and consistent. ''C''&lt;sub&gt;0&lt;/sub&gt; is ordinary summation, and ''C''&lt;sub&gt;1&lt;/sub&gt; is ordinary [[Cesàro summation]]. Cesàro sums have the property that if {{nowrap|1=''h'' &gt; ''k'',}} then ''C''&lt;sub&gt;''h''&lt;/sub&gt; is stronger than ''C''&lt;sub&gt;''k''&lt;/sub&gt;.

==Abelian means==
Suppose {{nowrap|1=''λ'' = {''λ''&lt;sub&gt;0&lt;/sub&gt;, ''λ''&lt;sub&gt;1&lt;/sub&gt;, ''λ''&lt;sub&gt;2&lt;/sub&gt;,...}}} is a strictly increasing sequence tending towards infinity, and that {{nowrap|1=''λ''&lt;sub&gt;0&lt;/sub&gt; ≥ 0}}. Suppose
:&lt;math&gt;f(x) = \sum_{n=0}^\infty a_n \exp(-\lambda_n x)&lt;/math&gt;
converges for all real numbers ''x''&amp;nbsp;&gt;&amp;nbsp;0. Then the '''Abelian mean''' ''A''&lt;sub&gt;''λ''&lt;/sub&gt; is defined as
:&lt;math&gt;A_\lambda(s) = \lim_{x \rightarrow 0^{+}} f(x).&lt;/math&gt;

More generally, if the series for ''f'' only converges for large ''x'' but can be analytically continued to all positive real ''x'', then one can still define the sum of the divergent series by the limit above.

A series of this type is known as a generalized [[Dirichlet series]]; in applications to physics, this is known as the method of [[heat-kernel regularization]].

Abelian means are regular and linear, but not stable and not always consistent between different choices of ''λ''. However, some special cases are very important summation methods.

===Abel summation===
{{see also|Abel's theorem}}
If {{nowrap|1 = ''λ''&lt;sub&gt;''n''&lt;/sub&gt; = ''n''}}, then we obtain the method of '''Abel summation'''. Here

:&lt;math&gt;f(x) = \sum_{n=0}^\infty a_n e^{-nx} = \sum_{n=0}^\infty a_n z^n,&lt;/math&gt;

where ''z''&amp;nbsp;=&amp;nbsp;exp(−''x''). Then the limit of ''f''(''x'') as ''x'' approaches 0 through [[positive reals]] is the limit of the power series for ''f''(''z'') as ''z'' approaches 1 from below through positive reals, and the Abel sum ''A''(''s'') is defined as

:&lt;math&gt;A(s) = \lim_{z \rightarrow 1^{-}} \sum_{n=0}^\infty a_n z^n.&lt;/math&gt;

Abel summation is interesting in part because it is consistent with but more powerful than [[Cesàro summation]]: {{nowrap|1 = ''A''(''s'') = ''C''&lt;sub&gt;''k''&lt;/sub&gt;(''s'')}} whenever the latter is defined. The Abel sum is therefore regular, linear, stable, and consistent with Cesàro summation.

===Lindelöf summation===
If {{nowrap|1 = ''λ''&lt;sub&gt;''n''&lt;/sub&gt; = ''n'' log(''n'')}}, then (indexing from one) we have

:&lt;math&gt;f(x) = a_1 + a_2 2^{-2x} + a_3 3^{-3x} + \cdots .&lt;/math&gt;

Then ''L''(''s''), the '''Lindelöf sum''' {{harv|Volkov|2001}}, is the limit of ''f''(''x'') as ''x'' goes to positive zero. The Lindelöf sum is a powerful method when applied to power series among other applications, summing power series in the [[Mittag-Leffler star]].

If ''g''(''z'') is analytic in a disk around zero, and hence has a [[Maclaurin series]] ''G''(''z'') with a positive radius of convergence, then {{nowrap|1 = ''L''(''G''(''z'')) = ''g''(''z'')}} in the Mittag-Leffler star.  Moreover, convergence to ''g''(''z'') is uniform on compact subsets of the star.

==Analytic continuation==

Several summation methods involve taking the value of an analytic continuation of a function.

===Analytic continuation of power series===

If Σ''a''&lt;sub&gt;''n''&lt;/sub&gt;''x''&lt;sup&gt;''n''&lt;/sup&gt; converges for small complex ''x'' and can be analytically continued along some path from ''x''&amp;nbsp;=&amp;nbsp;0 to the point ''x''&amp;nbsp;=&amp;nbsp;1, then the sum of the series can be defined to be the value at ''x''&amp;nbsp;=&amp;nbsp;1. This value may depend on the choice of path.

===Euler summation===

{{main article|Euler summation}}
Euler summation is essentially an explicit form of analytic continuation.  If a power series converges for small complex ''z'' and can be analytically continued to the open disk with diameter from {{sfrac|−1|''q''&amp;nbsp;+&amp;nbsp;1}} to 1 and is continuous at 1, then its value at is called the Euler or (E,''q'') sum of the series ''a''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;.... Euler used it before analytic continuation was defined in general, and gave explicit formulas for the power series of the analytic continuation.

The operation of Euler summation can be repeated several times, and this is essentially equivalent to taking an analytic continuation of a power series to the point&amp;nbsp;''z''&amp;nbsp;=&amp;nbsp;1.

===Analytic continuation of Dirichlet series===

This method defines the sum of a series to be the value of the analytic continuation of the Dirichlet series

:&lt;math&gt;f(s) = \frac{a_1}{1^s} + \frac{a_2}{2^s} + \frac{a_3}{3^s}+ \cdots &lt;/math&gt;
at ''s''&amp;nbsp;=&amp;nbsp;0, if this exists and is unique. This method is sometimes confused with zeta function regularization.

===Zeta function regularization===

If the series

:&lt;math&gt;f(s) = \frac{1}{a_1^s} + \frac{1}{a_2^s} + \frac{1}{a_3^s}+ \cdots &lt;/math&gt;

(for positive values of the ''a''&lt;sub&gt;''n''&lt;/sub&gt;) converges for large real ''s'' and can be [[Analytic continuation|analytically continued]] along the real line to ''s''&amp;nbsp;=&amp;nbsp;−1, then its value at ''s''&amp;nbsp;=&amp;nbsp;−1 is called the [[Zeta function regularization|zeta regularized]] sum of the series ''a''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;''a''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;... Zeta function regularization is nonlinear. In applications, the numbers ''a''&lt;sub&gt;''i''&lt;/sub&gt; are sometimes the eigenvalues of a self-adjoint operator ''A'' with compact resolvent, and ''f''(''s'') is then the trace of ''A''&lt;sup&gt;−''s''&lt;/sup&gt;. For example, if ''A'' has eigenvalues 1, 2, 3, ...  then ''f''(''s'') is the [[Riemann zeta function]], ''ζ''(''s''), whose value at ''s''&amp;nbsp;=&amp;nbsp;−1 is −{{sfrac|1|12}}, assigning a value to the divergent series {{nowrap|[[1 + 2 + 3 + 4 + ...]]}}. Other values of ''s'' can also be used to assign values for the divergent sums {{nowrap|1=''ζ''(0) = 1 + 1 + 1 + ... = −{{sfrac|1|2}}}}, {{nowrap|1=''ζ''(−2) = 1 + 4 + 9 + ... = 0}} and in general
:&lt;math&gt;\zeta(-s)=\sum_{n=1}^\infty n^s=1^s + 2^s + 3^s + \cdots = -\frac{B_{s+1}}{s+1}\, ,&lt;/math&gt;
where ''B&lt;sub&gt;k&lt;/sub&gt;'' is a [[Bernoulli number]].&lt;ref&gt;{{cite web|url=http://terrytao.wordpress.com/2010/04/10/the-euler-maclaurin-formula-bernoulli-numbers-the-zeta-function-and-real-variable-analytic-continuation/|title=The Euler-Maclaurin formula, Bernoulli numbers, the zeta function, and real-variable analytic continuation|first=Terence|last=Tao|date=10 April 2010}}&lt;/ref&gt;

==Integral function means==
If ''J''(''x'')&amp;nbsp;=&amp;nbsp;Σ''p''&lt;sub&gt;''n''&lt;/sub&gt;''x''&lt;sup&gt;''n''&lt;/sup&gt; is an integral function, then the ''J'' sum of the series ''a''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;... is defined to be
:&lt;math&gt;\lim_{x\rightarrow\infty}\frac{\sum_np_n(a_0+\cdots+a_n)x^n}{\sum_np_nx^n},&lt;/math&gt;
if this limit exists.

There is a variation of this method where the series for ''J'' has a finite radius of convergence ''r'' and diverges at ''x''&amp;nbsp;=&amp;nbsp;''r''. In this case one defines the sum as above, except taking the limit as ''x'' tends to ''r'' rather than infinity.

===Borel summation===
In the special case when ''J''(''x'')&amp;nbsp;=&amp;nbsp;''e''&lt;sup&gt;''x''&lt;/sup&gt; this gives one (weak) form of [[Borel summation]].

===Valiron's method===
Valiron's method is a generalization of Borel summation to certain more general integral functions ''J''. Valiron showed that under certain conditions it is equivalent to defining the sum of a series as 
:&lt;math&gt; \lim_{n\rightarrow +\infty}\sqrt{\frac{H(n)}{2\pi}}\sum_{h\in Z} e^{-\frac12 h^2H(n)}(a_0+\cdots+a_h)&lt;/math&gt;
where ''H'' is the second derivative of ''G'' and ''c''(''n'')&amp;nbsp;=&amp;nbsp;''e''&lt;sup&gt;−''G''(''n'')&lt;/sup&gt;, and ''a''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;...&amp;nbsp;+&amp;nbsp;''a''&lt;sub&gt;h&lt;/sub&gt; is to be interpreted as 0 when h &lt;0.

==Moment methods==

Suppose that ''dμ'' is a measure on the  real line such that all the moments

: &lt;math&gt;\mu_n=\int x^n \, d\mu&lt;/math&gt;

are finite. If ''a''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;''a''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;... is a series such that

:&lt;math&gt;a(x)=\frac{a_0x^0}{\mu_0}+\frac{a_1x^1}{\mu_1}+\cdots&lt;/math&gt;

converges for all ''x'' in the support of ''μ'', then the (''dμ'') sum of the series is defined to be the value of the integral

: &lt;math&gt;\int a(x) \, d\mu&lt;/math&gt;

if it is defined. (Note that if the numbers ''μ''&lt;sub&gt;''n''&lt;/sub&gt; increase too rapidly then they do not uniquely determine the measure ''μ''.)

===Borel summation===
For example, if ''dμ''&amp;nbsp;=&amp;nbsp;''e''&lt;sup&gt;−''x''&lt;/sup&gt;&amp;nbsp;''dx'' for positive ''x'' and 0 for negative ''x'' then ''μ''&lt;sub&gt;''n''&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''n''!, and this gives one version of [[Borel summation]], where the value of a sum is given by 
:&lt;math&gt;\int_0^\infty e^{-t}\sum\frac{a_nt^n}{n!} \, dt.&lt;/math&gt;

There is a generalization of this depending on a variable ''α'', called the (B′,''α'') sum, where the sum of a series ''a''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;... is defined to be

: &lt;math&gt;\int_0^\infty e^{-t}\sum\frac{a_nt^{n\alpha}}{\Gamma(n\alpha+1)} \, dt&lt;/math&gt;

if this integral exists. A further generalization is to replace the sum under the integral by its analytic continuation from small&amp;nbsp;''t''.

==Miscellaneous methods==

===Hausdorff transformations===

{{harvtxt|Hardy|1949|loc=chapter 11}}.

===Hölder summation===

{{main article|Hölder summation}}

===Hutton's method===

In 1812 Hutton introduced a method of summing divergent series by starting with the sequence of partial sums, and repeated applying the operation of replacing a sequence&amp;nbsp;''s''&lt;sub&gt;0&lt;/sub&gt;,&amp;nbsp;''s''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;... by the sequence of averages {{sfrac|''s''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;''s''&lt;sub&gt;1&lt;/sub&gt;|2}}, {{sfrac|''s''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;''s''&lt;sub&gt;2&lt;/sub&gt;|2}},&amp;nbsp;..., and then taking the limit {{harv|Hardy|1949|loc=p. 21}}.

===Ingham summability===

The series ''a''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;... is called Ingham summable to ''s'' if

: &lt;math&gt;\lim_{x\rightarrow \infty} \sum_{1\le n\le x} a_n\frac{n}{x}\left[\frac{x}{n}\right] = s.&lt;/math&gt;

[[Albert Ingham]] showed that if ''δ'' is any positive number then (C,−''δ'') (Cesàro) summability implies Ingham summability, and Ingham summability implies (C,''δ'') summability {{harvtxt|Hardy|1949|loc=Appendix II}}.

===Lambert summability===

The series ''a''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;... is called [[Lambert summable]] to ''s'' if
:&lt;math&gt;\lim_{y\rightarrow 0^+} \sum_{n\ge 1} a_n\frac{nye^{-ny}}{1-e^{-ny}} = s. &lt;/math&gt;

If a series is (C,''k'') (Cesàro) summable for any ''k'' then it is Lambert summable to the same value, and if a series is Lambert summable then it is Abel summable to the same value {{harvtxt|Hardy|1949|loc=Appendix II}}.

===Le Roy summation===

The series ''a''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;... is called Le Roy summable to ''s'' if

: &lt;math&gt;\lim_{\zeta\rightarrow 1^-} \sum_{n} \frac{\Gamma(1+\zeta n)}{\Gamma(1+ n)} a_n = s. &lt;/math&gt;
{{harvtxt|Hardy|1949|loc=4.11}}

===Mittag-Leffler summation===
The series ''a''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;... is called Mittag-Leffler (M) summable to ''s'' if
:&lt;math&gt;\lim_{\delta\rightarrow 0} \sum_{n} \frac{a_n}{\Gamma(1+\delta n)} = s.&lt;/math&gt;

{{harvtxt|Hardy|1949|loc=4.11}}

===Ramanujan summation===

{{main article|Ramanujan summation}}
Ramanujan summation is a method of assigning a value to divergent series used by Ramanujan and based on the [[Euler–Maclaurin summation formula]]. The Ramanujan sum of a series ''f''(0) + ''f''(1) + ... depends not only on the values of ''f'' at integers, but also on values of the function ''f'' at non-integral points, so it is not really a summation method in the sense of this article.

===Riemann summability===

The series ''a''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;... is called (R,''k'') (or Riemann) summable to ''s'' if

: &lt;math&gt;\lim_{h\rightarrow 0} \sum_{n} a_n\left(\frac{\sin nh}{nh}\right)^k = s.&lt;/math&gt;
{{harvtxt|Hardy|1949|loc=4.17}}
The series ''a''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;... is called R&lt;sub&gt;2&lt;/sub&gt; summable to ''s'' if

: &lt;math&gt;\lim_{h\rightarrow 0} \frac{2}{\pi}\sum_n \frac{\sin^2 nh}{n^2h}(a_1+\cdots + a_n) = s.&lt;/math&gt;

===Riesz means===
{{main article|Riesz mean}}
If ''λ''&lt;sub&gt;''n''&lt;/sub&gt; form an increasing sequence of real numbers and

: &lt;math&gt;A_\lambda(x)=a_0+\cdots+a_n \text{ for } \lambda_n&lt;x\le \lambda_{n+1}&lt;/math&gt;

then the Riesz (R,''λ'',''κ'') sum of the series ''a''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;... is defined to be

: &lt;math&gt;\lim_{\omega\rightarrow\infty} \frac{\kappa}{\omega^\kappa} \int_0^\omega A_\lambda(x)(\omega-x)^{\kappa-1} \, dx.&lt;/math&gt;

===Vallée-Poussin summability===
The series ''a''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;... is called VP (or Vallée-Poussin) summable to ''s'' if

: &lt;math&gt;\lim_{m\rightarrow \infty} a_0+a_1\frac{m}{m+1}+a_2\frac{m(m-1)}{(m+1)(m+2)}+\cdots = s. &lt;/math&gt;
{{harvtxt|Hardy|1949|loc=4.17}}.

==See also==
* [[Silverman–Toeplitz theorem]]

==Notes==
{{reflist}}

==References==
* {{citation|title=Large-Order Perturbation Theory and Summation Methods in Quantum Mechanics|first1=G.A.|last1=Arteca|first2=F.M.|last2=Fernández|first3=E.A.|last3=Castro|publisher=Springer-Verlag|publication-place=Berlin|year=1990}}.
* {{citation|title=Padé Approximants|first1=G. A.|last1=Baker, Jr.|first2=P. |last2=Graves-Morris|publisher=Cambridge University Press|year=1996}}.
* {{citation|title=Extrapolation Methods. Theory and Practice|first1=C.|last1=Brezinski|first2=M. Redivo|last2=Zaglia|publisher=North-Holland|year=1991}}.
* {{citation|title=Divergent Series|first=G. H.|last=Hardy|authorlink=G. H. Hardy|publication-place=Oxford|publisher=Clarendon Press|year=1949|url=https://archive.org/details/divergentseries033523mbp}}.
* {{citation|title=Large-Order Behaviour of Perturbation Theory|first1=J.-C.|last1=LeGuillou|first2=J.|last2=Zinn-Justin|publisher=North-Holland|publication-place=Amsterdam|year=1990}}.
* {{springer|id=l/l058990|title=Lindelöf summation method|first=I.I.|last=Volkov|year=2001}}.
* {{springer|id=a/a010170|title=Abel summation method|first=A.A.|last=Zakharov|year=2001}}.
* {{springer|title=Riesz summation method|id=p/r082300}}

{{Series (mathematics)}}

[[Category:Divergent series]]
[[Category:Mathematical series]]
[[Category:Summability methods]]
[[Category:Asymptotic analysis]]
[[Category:Summability theory]]</text>
      <sha1>sxzs0x8lecdnfzwwmlrxn8zigkr2q83</sha1>
    </revision>
  </page>
  <page>
    <title>Erdős–Nicolas number</title>
    <ns>0</ns>
    <id>42260413</id>
    <revision>
      <id>832968930</id>
      <parentid>820339821</parentid>
      <timestamp>2018-03-28T23:24:02Z</timestamp>
      <contributor>
        <username>DePiep</username>
        <id>199625</id>
      </contributor>
      <minor/>
      <comment>(via [[WP:JWB]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1940">{{Infobox integer sequence
| named_after           = [[Paul Erdős]], [[Jean-Louis Nicolas]]
| publication_year      = 1975
| author                = [[Paul Erdős|Erdős, P.]], [[Jean-Louis Nicolas|Nicolas, J. L.]]
| parentsequence        = {{nowrap|[[Abundant number]]s}}
| first_terms           = [[24 (number)|24]], [[2016 (number)|2016]], [[8190 (number)|8190]]
| largest_known_term    = [[1371332329173024768 (number)|1371332329173024768]]
| OEIS                  = A194472
| OEIS_name             = Erdős-Nicolas numbers 
}}

In [[number theory]], an '''Erdős–Nicolas number''' is a number that is not [[perfect number|perfect]], but that equals one of the [[partial sum]]s of its [[divisor]]s.
That is, a number {{mvar|n}} is Erdős–Nicolas number when there exists another number {{mvar|m}} such that

: &lt;math&gt;\sum_{d\mid n,\ d\leq m}d=n.&lt;/math&gt; &lt;ref&gt;
{{cite book
|last      = De Koninck
|first     = Jean-Marie
|year      = 2009
|title     = Those Fascinating Numbers
|publisher = 
|page      = 141
|isbn      = 978-0-8218-4807-4
|url       = http://www.ams.org/bookpages/mbk-64
}}&lt;/ref&gt;

The first ten Erdős–Nicolas numbers are
:[[24 (number)|24]], 2016, 8190, 42336, 45864, 392448, 714240, 1571328, 61900800 and 91963648. ({{oeis|A194472}})
They are named after [[Paul Erdős]] and [[Jean-Louis Nicolas]], who wrote about them in 1975.&lt;ref&gt;{{citation
|first1  = P. |last1 = Erdős | author1-link=Paul Erdős
|first2  = J.L. |last2 = Nicolas | author2-link=Jean-Louis Nicolas
|title   = Répartition des nombres superabondants
|journal = Bull. Soc. Math. France
|issue   = 103
|year    = 1975
|pages   = 65–90
|url     = http://archive.numdam.org/article/BSMF_1975__103__65_0.pdf
|zbl=0306.10025
}}&lt;/ref&gt;

==See also==
*[[Descartes number]], another type of almost-perfect number

== References ==
{{Reflist}}

{{DEFAULTSORT:Erdos-Nicolas Number}}
[[Category:Integer sequences]]

{{Divisor classes}}
{{numtheory-stub}}</text>
      <sha1>n9kb18iqoxlkenfahfufsvdawvc3e54</sha1>
    </revision>
  </page>
  <page>
    <title>Feedforward neural network</title>
    <ns>0</ns>
    <id>1706332</id>
    <revision>
      <id>856029366</id>
      <parentid>855768521</parentid>
      <timestamp>2018-08-22T11:40:47Z</timestamp>
      <contributor>
        <username>Jtsaur</username>
        <id>4319338</id>
      </contributor>
      <comment>/* Single-layer perceptron */Fixed subject</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9747">{{refimprove|date=September 2011}}
[[Image:Feed forward neural net.gif|right|thumb|400px|In a feed forward network information always moves one direction; it never goes backwards.]]

A '''feedforward neural network''' is an [[artificial neural network]] wherein connections between the nodes do ''not'' form a cycle.&lt;ref name=Zell1994p73&gt;{{cite book |last=Zell |first=Andreas |year=1994 |title=Simulation Neuronaler Netze |trans-title=Simulation of Neural Networks |language=German |edition=1st |publisher=Addison-Wesley |page=73 |isbn=3-89319-554-8}}&lt;/ref&gt; As such, it is different from [[recurrent neural networks]].

The feedforward neural network was the first and simplest type of artificial neural network devised&lt;ref&gt;{{Cite journal|date=2015-01-01|title=Deep learning in neural networks: An overview|url=https://www.sciencedirect.com/science/article/pii/S0893608014002135|journal=Neural Networks|language=en|volume=61|pages=85–117|doi=10.1016/j.neunet.2014.09.003|issn=0893-6080|arxiv=1404.7828}}&lt;/ref&gt;. In this network, the information moves in only one direction, forward, from the input nodes, through the hidden nodes (if any) and to the output nodes. There are no cycles or loops in the network.&lt;ref name=Zell1994p73 /&gt;

==Single-layer perceptron==
{{main|Perceptron}}

The simplest kind of neural network is a ''single-layer perceptron'' network, which consists of a single layer of output nodes; the inputs are fed directly to the outputs via a series of weights. The sum of the products of the weights and the inputs is calculated in each node, and if the value is above some threshold (typically 0) the neuron fires and takes the activated value (typically 1); otherwise it takes the deactivated value (typically -1). Neurons with this kind of [[activation function]] are also called ''[[artificial neurons]]'' or ''linear threshold units''. In the literature the term ''[[perceptron]]'' often refers to networks consisting of just one of these units. A similar neuron was described by [[Warren McCulloch]] and [[Walter Pitts]] in the 1940s.

A perceptron can be created using any values for the activated and deactivated states as long as the threshold value lies between the two.

Perceptrons can be trained by a simple learning algorithm that is usually called the ''[[delta rule]]''. It calculates the errors between calculated output and sample output data, and uses this to create an adjustment to the weights, thus implementing a form of [[gradient descent]].

Single-layer perceptrons are only capable of learning [[linearly separable]] patterns; in 1969 in a famous [[monograph]] entitled ''[[Perceptrons (book)|Perceptrons]]'', [[Marvin Minsky]] and [[Seymour Papert]] showed that it was impossible for a single-layer perceptron network to learn an [[Exclusive or|XOR function]] (nonetheless, it was known that multi-layer perceptrons are capable of producing any possible boolean function). 

Although a single threshold unit is quite limited in its computational power, it has been shown that networks of parallel threshold units can [[Universal approximation theorem|approximate any continuous function]] from a compact interval of the real numbers into the interval [-1,1]. This result can be found in Peter Auer, [[Harald Burgsteiner]] and [[Wolfgang Maass]] "A learning rule for very simple universal approximators consisting of a single layer of perceptrons".&lt;ref name=Auer2008&gt;{{cite journal
 | first = Peter | last = Auer |author2=Harald Burgsteiner |author3=Wolfgang Maass
 | url = http://www.igi.tugraz.at/harry/psfiles/biopdelta-07.pdf 
 | title = A learning rule for very simple universal approximators consisting of a single layer of perceptrons
 | journal = Neural Networks
 | volume = 21 | issue = 5 | pages = 786–795 | year = 2008
 | doi = 10.1016/j.neunet.2007.12.036
 | pmid = 18249524}}&lt;/ref&gt;

A single-layer neural network can compute a continuous output instead of a [[step function]]. A common choice is the so-called [[logistic function]]:

: &lt;math&gt;f(x) = \frac{1}{1+e^{-x}}&lt;/math&gt;

With this choice, the single-layer network is identical to the [[logistic regression]] model, widely used in [[statistical model]]ing. The [[logistic function]] is also known as the [[sigmoid function]]. It has a continuous derivative, which allows it to be used in [[backpropagation]]. This function is also preferred because its derivative is easily calculated:

: &lt;math&gt;f'(x) = f(x)(1-f(x))&lt;/math&gt;.

(The fact that f satisfies the differential equation above can easily be shown by applying the [[Chain Rule]].)

==Multi-layer perceptron==
{{main|Multilayer perceptron}}
[[Image:XOR perceptron net.png|thumb|right|250px|A two-layer neural network capable of calculating XOR. The numbers within the neurons represent each neuron's explicit threshold (which can be factored out so that all neurons have the same threshold, usually 1). The numbers that annotate arrows represent the weight of the inputs. This net assumes that if the threshold is not reached, zero (not -1) is output. Note that the bottom layer of inputs is not always considered a real neural network layer]]

This class of networks consists of multiple layers of computational units, usually interconnected in a feed-forward way. Each neuron in one layer has directed connections to the neurons of the subsequent layer. In many applications the units of these networks apply a ''[[sigmoid function]]'' as an activation function.

The ''[[universal approximation theorem]]'' for neural networks states that every continuous function that maps intervals of real numbers to some output interval of real numbers can be approximated arbitrarily closely by a multi-layer perceptron with just one hidden layer. This result holds for a wide range of activation functions, e.g. for the sigmoidal functions.

Multi-layer networks use a variety of learning techniques, the most popular being ''[[back-propagation]]''. Here, the output values are compared with the correct answer to compute the value of some predefined error-function. By various techniques, the error is then fed back through the network. Using this information, the algorithm adjusts the weights of each connection in order to reduce the value of the error function by some small amount. After repeating this process for a sufficiently large number of training cycles, the network will usually converge to some state where the error of the calculations is small. In this case, one would say that the network has ''learned'' a certain target function. To adjust weights properly, one applies a general method for non-linear [[Optimization (mathematics)|optimization]] that is called [[gradient descent]]. For this, the network calculates the derivative of the error function with respect to the network weights, and changes the weights such that the error decreases (thus going downhill on the surface of the error function). For this reason, back-propagation can only be applied on networks with differentiable activation functions.

In general, the problem of teaching a network to perform well, even on samples that were not used as training samples, is a quite subtle issue that requires additional techniques. This is especially important for cases where only very limited numbers of training samples are available.&lt;ref name=Balabin_2007&gt;{{cite journal |journal=[[Chemometrics and intelligent laboratory systems|Chemometr Intell Lab]] |volume = 88 |issue = 2 |pages = 183–188 |doi=10.1016/j.chemolab.2007.04.006 |title=Comparison of linear and nonlinear calibration models based on near infrared (NIR) spectroscopy data for gasoline properties prediction |year=2007 |author1=Roman M. Balabin |author2=Ravilya Z. Safieva |author3=Ekaterina I. Lomakina }}&lt;/ref&gt; The danger is that the network [[overfitting|overfits]] the training data and fails to capture the true statistical process generating the data.  [[Computational learning theory]] is concerned with training classifiers on a limited amount of data.  In the context of neural networks a simple [[heuristic]], called [[early stopping]], often ensures that the network will generalize well to examples not in the training set.

Other typical problems of the back-propagation algorithm are the speed of convergence and the possibility of ending up in a [[local minimum]] of the error function. Today there are practical methods&lt;!-- examples? --&gt; that make back-propagation in multi-layer perceptrons the tool of choice for many [[machine learning]] tasks.

One also can use a series of independent neural networks moderated by some intermediary, a similar behavior that happens in brain. These neurons can perform separably and handle a large task, and the results can be finally combined. &lt;ref&gt;{{cite journal|last1=Tahmasebi|first1=Pejman|last2=Hezarkhani|first2=Ardeshir|title=Application of a Modular Feedforward Neural Network for Grade Estimation|journal=Natural Resources Research|date=21 January 2011|volume=20|issue=1|pages=25–32|doi=10.1007/s11053-011-9135-3|url=https://www.researchgate.net/publication/225535280_Application_of_a_Modular_Feedforward_Neural_Network_for_Grade_Estimation}}&lt;/ref&gt;

== See also ==
* [[Hopfield network]]
* [[Convolutional neural network]]
* [[Feed forward (control)|Feed-forward]]
* [[Backpropagation]]
* [[Rprop]]

==References==
{{reflist}}

==External links==
* [http://www.emilstefanov.net/Projects/NeuralNetworks.aspx Feedforward neural networks tutorial]
* [https://web.archive.org/web/20090923121811/http://wiki.syncleus.com/index.php/DANN%3ABackprop_Feedforward_Neural_Network Feedforward Neural Network: Example]
* [http://media.wiley.com/product_data/excerpt/19/04713491/0471349119.pdf Feedforward Neural Networks: An Introduction]

[[Category:Artificial neural networks]]</text>
      <sha1>rqtydbwnnn007bo1gpzf4s3bk8c1lgs</sha1>
    </revision>
  </page>
  <page>
    <title>Geometric mean</title>
    <ns>0</ns>
    <id>13046</id>
    <revision>
      <id>870051405</id>
      <parentid>870051287</parentid>
      <timestamp>2018-11-22T02:10:13Z</timestamp>
      <contributor>
        <username>Hawkmoth-accent</username>
        <id>30898013</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="26294">{{More citations needed|article|date=May 2010}}
[[File:01-Mittlere Proportionale.gif|thumb|400px|Construction geometric mean/ mean proportional,&lt;ref&gt;Matt Friehauf, Mikaela Hertel, Juan Liu, and Stacey Luong {{cite web|url=https://sites.math.washington.edu/~julia/teaching/445_Spring2013/ConstructionsI.pdf#page=6&amp;zoom=80,-502,802 |title=On Compass and Straightedge Constructions: Means|publisher=UNIVERSITY of WASHINGTON, DEPARTMENT OF MATHEMATICS|year=2013 |accessdate=14 June 2018}}&lt;/ref&gt;&lt;ref&gt;Euclid, Book VI, Proposition 13&lt;/ref&gt; in an example in which the line segment &lt;math&gt;l_2\;(\overline{BC})&lt;/math&gt; is given as a perpendicular to &lt;math&gt;\overline{AB}&lt;/math&gt;.]]

In mathematics, the '''geometric mean''' is a [[mean]] or [[average]], which indicates the central tendency or typical value of a set of numbers by using the product of their values (as opposed to the [[arithmetic mean]] which uses their sum). The geometric mean is defined as the [[Nth root|{{math|''n''}}th root]] of the [[product (mathematics)|product]] of {{mvar|n}} numbers, i.e., for a set of numbers {{math|''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ..., ''x&lt;sub&gt;n&lt;/sub&gt;''}}, the geometric mean is defined as

:&lt;math&gt;\left(\prod_{i=1}^n x_i\right)^\frac{1}{n} = \sqrt[n]{x_1 x_2 \cdots x_n}&lt;/math&gt;

For instance, the geometric mean of two numbers, say 2 and 8, is just the [[square root]] of their product, that is, &lt;math&gt;\sqrt{2 \cdot 8} = 4&lt;/math&gt;. As another example, the geometric mean of the three numbers 4, 1, and 1/32 is the [[cube root]] of their product (1/8), which is 1/2, that is, &lt;math&gt;\sqrt[3]{4 \cdot 1 \cdot 1/32} = 1/2&lt;/math&gt;.

A geometric mean is often used when comparing different items—finding a single "figure of merit" for these items—when each item has multiple properties that have different numeric ranges.&lt;ref&gt;{{cite web|title=TPC-D – Frequently Asked Questions (FAQ)|url=http://www.tpc.org/tpcd/faq.asp#anchor1140017|publisher=Transaction Processing Performance Council|accessdate=9 January 2012|deadurl=no|archiveurl=https://web.archive.org/web/20111104224323/http://www.tpc.org/tpcd/faq.asp#anchor1140017|archivedate=4 November 2011|df=}}&lt;/ref&gt; For example, the geometric mean can give a meaningful "average" to compare two companies which are each rated at 0 to 5 for their environmental sustainability, and are rated at 0 to 100 for their financial viability. If an arithmetic mean were used instead of a geometric mean, the financial viability is given more weight because its numeric range is larger—so a small percentage change in the financial rating (e.g. going from 80 to 90) makes a much larger difference in the arithmetic mean than a large percentage change in environmental sustainability (e.g. going from 2 to 5). The use of a geometric mean "normalizes" the ranges being averaged, so that no range dominates the weighting, and a given percentage change in any of the properties has the same effect on the geometric mean. So, a 20% change in environmental sustainability from 4 to 4.8 has the same effect on the geometric mean as a 20% change in financial viability from 60 to 72.

The geometric mean can be understood in terms of [[geometry]]. The geometric mean of two numbers, &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt;, is the length of one side of a [[square (geometry)|square]] whose area is equal to the area of a [[rectangle]] with sides of lengths &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt;. Similarly, the geometric mean of three numbers, &lt;math&gt;a&lt;/math&gt;, &lt;math&gt;b&lt;/math&gt;, and &lt;math&gt;c&lt;/math&gt;, is the length of one edge of a [[cube]] whose volume is the same as that of a [[cuboid]] with sides whose lengths are equal to the three given numbers.

The geometric mean applies only to positive numbers.&lt;ref&gt;The geometric mean only applies to numbers of the same sign in order to avoid taking the root of a negative product, which would result in [[imaginary number]]s, and also to satisfy certain properties about means, which is explained later in the article. The definition is unambiguous if one allows 0 (which yields a geometric mean of 0), but may be excluded, as one frequently wishes to take the logarithm of geometric means (to convert between multiplication and addition), and one cannot take the logarithm of 0.&lt;/ref&gt; It is also often used for a set of numbers whose values are meant to be multiplied together or are exponential in nature, such as data on the growth of the [[World population|human population]] or interest rates of a financial investment.

The geometric mean is also one of the three classical [[Pythagorean means]], together with the aforementioned arithmetic mean and the [[harmonic mean]]. For all positive data sets containing at least one pair of unequal values, the harmonic mean is always the least of the three means, while the arithmetic mean is always the greatest of the three and the geometric mean is always in between (see [[Inequality of arithmetic and geometric means]].)

==Calculation==
The geometric mean of a data set &lt;math display="inline"&gt;\left\{a_1, a_2,\, \ldots,\, a_n\right\}&lt;/math&gt; is given by:
:&lt;math&gt;\left(\prod_{i=1}^n a_i \right)^\frac{1}{n} = \sqrt[n]{a_1 a_2 \cdots a_n}.&lt;/math&gt;

The above figure uses [[capital pi notation]] to show a series of multiplications.  Each side of the equal sign shows that a set of values is multiplied in succession (the number of values is represented by "n") to give a total [[product (mathematics)|product]] of the set, and then the ''n''th root of the total product is taken to give the geometric mean of the original set.  For example, in a set of four numbers &lt;math display="inline"&gt;\{1, 2, 3, 4\}&lt;/math&gt;, the product of &lt;math display="inline"&gt;1 \times 2 \times 3 \times 4&lt;/math&gt; is &lt;math display="inline"&gt;24&lt;/math&gt;, and the geometric mean is the fourth root of 24, or ~ 2.213.  The exponent &lt;math display="inline"&gt;\frac{1}{n}&lt;/math&gt; on the left side is equivalent to the taking ''n''th root.  For example, &lt;math display="inline"&gt;24^\frac{1}{4} = \sqrt[4]{24}&lt;/math&gt;.

The geometric mean of a data set [[inequality of arithmetic and geometric means|is less than]] the data set's [[arithmetic mean]] unless all members of the data set are equal, in which case the geometric and arithmetic means are equal. This allows the definition of the [[arithmetic-geometric mean]], an intersection of the two which always lies in between.

The geometric mean is also the '''arithmetic-harmonic mean''' in the sense that if two [[sequence]]s (&lt;math display="inline"&gt;a_n&lt;/math&gt;) and (&lt;math display="inline"&gt;h_n&lt;/math&gt;) are defined:
:&lt;math&gt;a_{n+1} = \frac{a_n + h_n}{2}, \quad a_0 = x&lt;/math&gt;

and
:&lt;math&gt;h_{n+1} = \frac{2}{\frac{1}{a_n} + \frac{1}{h_n}}, \quad h_0 = y&lt;/math&gt;

where &lt;math display="inline"&gt;h_{n+1}&lt;/math&gt; is the [[harmonic mean]] of the previous values of the two sequences, then &lt;math display="inline"&gt;a_n&lt;/math&gt; and &lt;math display="inline"&gt;h_n&lt;/math&gt; will converge to the geometric mean of &lt;math display="inline"&gt;x&lt;/math&gt; and &lt;math display="inline"&gt;y&lt;/math&gt;.

This can be seen easily from the fact that the sequences do converge to a common limit (which can be shown by [[Bolzano–Weierstrass theorem]]) and the fact that geometric mean is preserved:
:&lt;math&gt;\sqrt{a_i h_i} =
  \sqrt{\frac{a_i + h_i}{\frac{a_i + h_i}{h_i a_i}}} =
  \sqrt{\frac{a_i + h_i}{\frac{1}{a_i} + \frac{1}{h_i}}} =
  \sqrt{a_{i+1} h_{i+1}}
&lt;/math&gt;

Replacing the arithmetic and harmonic mean by a pair of [[generalized mean]]s of opposite, finite exponents yields the same result.

==={{anchor|Log-average}}Relationship with logarithms ===&lt;!--"Log-average" redirects here--&gt;
The geometric mean can also be expressed as the exponential of the arithmetic mean of logarithms.&lt;ref&gt;{{cite book |title=Statistics: An Introduction using R |first=Michael J. |last=Crawley |publisher=John Wiley &amp; Sons Ltd. |year=2005 |isbn=9780470022986 }}&lt;/ref&gt; By using [[logarithmic identities]] to transform the formula, the multiplications can be expressed as a sum and the power as a multiplication:
: When &lt;math&gt;a_1, a_2, \dots, a_n &gt; 0&lt;/math&gt;
:: &lt;math&gt;\left( \prod_{i=1}^n a_i \right)^\frac{1}{n} = \exp\left[\frac{1}{n} \sum_{i=1}^n \ln a_i\right]&lt;/math&gt; 
: if &lt;math&gt;\exists a_j &lt; 0&lt;/math&gt; then
:: &lt;math&gt;\left( \prod_{i=1}^n a_i \right)^\frac{1}{n} = \left(-1\right)^m \exp\left[\frac{1}{n}\sum_{i=1}^n \ln \left|a_i\right| \right]&lt;/math&gt;  
: Where m is the number of negative numbers.

This is sometimes called the '''log-average''' (not to be confused with the [[logarithmic average]]).  It is simply computing the [[arithmetic mean]] of the logarithm-transformed values of &lt;math&gt;a_i&lt;/math&gt; (i.e., the arithmetic mean on the log scale) and then using the exponentiation to return the computation to the original scale, i.e., it is the [[generalised f-mean]] with &lt;math&gt;f(x) = \log x&lt;/math&gt;. For example, the geometric mean of 2 and 8 can be calculated as the following, where &lt;math&gt;b&lt;/math&gt; is any base of a [[logarithm]] (commonly 2, [[e (mathematical constant)|&lt;math&gt;e&lt;/math&gt;]] or 10):
:&lt;math&gt;b^{\frac{1}{2}\left[\log_b (2) + \log_b (8)\right]} = 4&lt;/math&gt;

Related to the above, it can be seen that for a given sample of points &lt;math&gt;a_1, \ldots, a_n&lt;/math&gt;, the geometric mean is the minimizer of &lt;math&gt;f(a) = \sum_{i=1}^n (\log(a_i) - \log(a))^2&lt;/math&gt;, whereas the arithmetic mean is the minimizer of &lt;math&gt;f(a) = \sum_{i=1}^n (a_i - a)^2&lt;/math&gt;. Thus, the geometric mean provides a summary of the samples whose exponent best matches the exponents of the samples (in the least squares sense).

The log form of the geometric mean is generally the preferred alternative for implementation in computer languages because calculating the product of many numbers can lead to an [[arithmetic overflow]] or [[arithmetic underflow]]. This is less likely to occur with the sum of the logarithms for each number.

===Relationship with arithmetic mean and mean-preserving spread===

If a set of non-identical numbers is subjected to a [[mean-preserving spread]] — that is, two or more elements of the set are "spread apart" from each other while leaving the arithmetic mean unchanged — then the geometric mean always decreases.&lt;ref&gt;{{cite journal |last=Mitchell |first=Douglas W. |title=More on spreads and non-arithmetic means |journal=[[The Mathematical Gazette]] |volume=88 |year=2004 |pages=142–144 }}&lt;/ref&gt;

===Computation in constant time===
In cases where the geometric mean is being used to determine the average growth rate of some quantity, and the initial and final values &lt;math&gt;a_0&lt;/math&gt; and &lt;math&gt;a_n&lt;/math&gt; of that quantity are known, the product of the measured growth rate at every step need not be taken.{{citation needed|reason=What is the proof of this?|date=July 2016}}  Instead, the geometric mean is simply
:&lt;math&gt;\left(\frac{a_n}{a_0}\right)^\frac{1}{n},&lt;/math&gt;

where &lt;math&gt;n&lt;/math&gt; is the number of steps from the initial to final state.

If the values are &lt;math&gt;a_0,\, \ldots,\, a_n&lt;/math&gt;, then the growth rate between measurement &lt;math&gt;a_k&lt;/math&gt; and &lt;math&gt;a_{k+1}&lt;/math&gt; is &lt;math&gt;a_{k+1}/a_k&lt;/math&gt;.  The geometric mean of these growth rates is just
:&lt;math&gt;\left( \frac{a_1}{a_0} \frac{a_2}{a_1} \cdots \frac{a_n}{a_{n-1}} \right)^\frac{1}{n} = \left(\frac{a_n}{a_0}\right)^\frac{1}{n}&lt;/math&gt;

==Properties==
The fundamental property of the geometric mean, which can be proven to be false for any other mean, is

: &lt;math&gt;\operatorname{GM}\left(\frac{X_i}{Y_i}\right) = \frac{\operatorname{GM}(X_i)}{\operatorname{GM}(Y_i)}&lt;/math&gt;

This makes the geometric mean the only correct mean when averaging ''normalized'' results; that is, results that are presented as ratios to reference values.&lt;ref&gt;{{cite journal |first=Philip J. |last=Fleming |first2=John J. |last2=Wallace |title=How not to lie with statistics: the correct way to summarize benchmark results |journal=Communications of the ACM |volume=29 |issue=3 |pages=218–221 |year=1986 |doi=10.1145/5666.5673 }}&lt;/ref&gt;  This is the case when presenting computer performance with respect to a reference computer, or when computing a single average index from several heterogeneous sources (for example, life expectancy, education years, and infant mortality).  In this scenario, using the arithmetic or harmonic mean would change the ranking of the results depending on what is used as a reference.  For example, take the following comparison of execution time of computer programs:

{| class="wikitable"
|-
! &amp;nbsp; !! Computer A !! Computer B !! Computer C
|-
| '''Program 1''' || 1 || 10 || 20
|-
| '''Program 2''' || 1000 || 100 || 20
|-
| '''Arithmetic mean''' || 500.5 || 55 || '''20'''
|-
| '''Geometric mean''' || 31.622 . . . || 31.622 . . . || '''20'''
|-
| '''Harmonic mean''' || '''1.998 . . .''' || 18.182 . . . || 20
|}

The arithmetic and geometric means "agree" that computer C is the fastest.  However, by presenting appropriately normalized values ''and'' using the arithmetic mean, we can show either of the other two computers to be the fastest.  Normalizing by A's result gives A as the fastest computer according to the arithmetic mean:

{| class="wikitable"
|-
! &amp;nbsp; !! Computer A !! Computer B !! Computer C
|-
| '''Program 1''' || 1 || 10 || 20
|-
| '''Program 2''' || 1 || 0.1 || 0.02
|-
| '''Arithmetic mean''' || '''1''' || 5.05 || 10.01
|-
| '''Geometric mean''' || 1 || 1 || '''0.632 . . .'''
|-
| '''Harmonic mean''' || 1 || 0.198 . . . || '''0.039 . . .'''
|}

while normalizing by B's result gives B as the fastest computer according to the arithmetic mean but A as the fastest according to the harmonic mean:

{| class="wikitable"
|-
! &amp;nbsp; !! Computer A !! Computer B !! Computer C
|-
| '''Program 1''' || 0.1 || 1 || 2
|-
| '''Program 2''' || 10 || 1 || 0.2
|-
| '''Arithmetic mean''' || 5.05 || '''1''' || 1.1
|-
| '''Geometric mean''' || 1 || 1 || '''0.632'''
|-
| '''Harmonic mean''' || '''0.198 . . .''' || 1 || 0.363 . . .
|}

and normalizing by C's result gives C as the fastest computer according to the arithmetic mean but A as the fastest according to the harmonic mean:

{| class="wikitable"
|-
! &amp;nbsp; !! Computer A !! Computer B !! Computer C
|-
| '''Program 1''' || 0.05 || 0.5 || 1
|-
| '''Program 2''' || 50 || 5 || 1
|-
| '''Arithmetic mean''' || 25.025 || 2.75 || '''1'''
|-
| '''Geometric mean''' || 1.581 . . . || 1.581 . . . || '''1'''
|-
| '''Harmonic mean''' || '''0.099 . . .''' || 0.909 . . . || 1
|}

In all cases, the ranking given by the geometric mean stays the same as the one obtained with unnormalized values.

However, this reasoning has been questioned.&lt;ref&gt;{{cite journal |first=James E. |last=Smith |title=Characterizing computer performance with a single number |journal=Communications of the ACM |volume=31 |issue=10 |pages=1202–1206 |year=1988 |doi=10.1145/63039.63043}}&lt;/ref&gt;
Giving consistent results is not always equal to giving the correct results. In general, it is more rigorous to assign weights to each of the programs, calculate the average weighted execution time (using the arithmetic mean), and then normalize that result to one of the computers. The three tables above just give a different weight to each of the programs, explaining the inconsistent results of the arithmetic and harmonic means (the first table gives equal weight to both programs, the second gives a weight of 1/1000 to the second program, and the third gives a weight of 1/100 to the second program and 1/10 to the first one). The use of the geometric mean for aggregating performance numbers should be avoided if possible, because multiplying execution times has no physical meaning, in contrast to adding times as in the arithmetic mean. Metrics that are inversely proportional to time (speedup, [[Instructions per cycle|IPC]]) should be averaged using the harmonic mean.

==Applications==

===Proportional growth===
{{Further|Compound annual growth rate}}
The geometric mean is more appropriate than the [[arithmetic mean]] for describing proportional growth, both [[exponential growth]] (constant proportional growth) and varying growth; in business the geometric mean of growth rates is known as the [[compound annual growth rate]] (CAGR). The geometric mean of growth over periods yields the equivalent constant growth rate that would yield the same final amount.

Suppose an orange tree yields 100 oranges one year and then 180, 210 and 300 the following years, so the growth is 80%, 16.6666% and 42.8571% for each year respectively. Using the [[arithmetic mean]] calculates a (linear) average growth of 46.5079% (80% + 16.6666% + 42.8571%, that sum then divided by 3). However, if we start with 100 oranges and let it grow 46.5079% each year, the result is 314 oranges, not 300, so the linear average ''over''-states the year-on-year growth.

Instead, we can use the geometric mean. Growing with 80% corresponds to multiplying with 1.80, so we take the geometric mean of 1.80, 1.166666 and 1.428571, i.e. &lt;math&gt;\sqrt[3]{1.80 \times 1.166666 \times 1.428571} \approx 1.442249&lt;/math&gt;; thus the "average" growth per year is 44.2249%. If we start with 100 oranges and let the number grow with 44.2249% each year, the result is 300 oranges.

===Applications in the social sciences===

Although the geometric mean has been relatively rare in computing social statistics, starting from 2010 the United Nations Human Development Index did switch to this mode of calculation, on the grounds that it better reflected the non-substitutable nature of the statistics being compiled and compared:
: The geometric mean decreases the level of substitutability between dimensions [being compared] and at the same time ensures that a 1 percent decline in say life expectancy at birth has the same impact on the HDI as a 1 percent decline in education or income. Thus, as a basis for comparisons of achievements, this method is also more respectful of the intrinsic differences across the dimensions than a simple average.&lt;ref&gt;{{cite web|url=http://hdr.undp.org/en/statistics/faq/|title=Frequently Asked Questions - Human Development Reports|author=|date=|website=hdr.undp.org|deadurl=no|archiveurl=https://web.archive.org/web/20110302103418/http://hdr.undp.org/en/statistics/faq/|archivedate=2011-03-02|df=}}&lt;/ref&gt;

Not all values used to compute the [[Human Development Index|HDI (Human Development Index)]] are normalized; some of them instead have the form &lt;math&gt;\left(X - X_\text{min}\right) / \left(X_\text{norm} - X_\text{min}\right)&lt;/math&gt;. This makes the choice of the geometric mean less obvious than one would expect from the "Properties" section above.

===Aspect ratios===
[[File:Dr. Kerns Powers, SMPTE derivation of 16-9 aspect ratio.svg|thumb|right|Equal area comparison of the aspect ratios used by Kerns Powers to derive the [[SMPTE]] [[16:9]] standard.&lt;ref name="Cinemasource" /&gt; {{colorbox|red}}{{nbsp}}TV 4:3/1.33 in red, {{colorbox|orange}}{{nbsp}}1.66 in orange, {{colorbox|blue}}{{nbsp}}'''16:9/1.7{{overline|7}} in blue''', {{colorbox|#aaaa00}}{{nbsp}}1.85 in yellow, {{colorbox|mauve}}{{nbsp}}[[Panavision]]/2.2 in mauve and {{colorbox|purple}}{{nbsp}}[[CinemaScope]]/2.35 in purple.]]
The geometric mean has been used in choosing a compromise [[aspect ratio (image)|aspect ratio]] in film and video: given two aspect ratios, the geometric mean of them provides a compromise between them, distorting or cropping both in some sense equally. Concretely, two equal area rectangles (with the same center and parallel sides) of different aspect ratios intersect in a rectangle whose aspect ratio is the geometric mean, and their hull (smallest rectangle which contains both of them) likewise has aspect ratio their geometric mean.

In [[aspect ratio (image)#Why 16:9 was chosen by the SMPTE|the choice of 16:9]] aspect ratio by the [[SMPTE]], balancing 2.35 and 4:3, the geometric mean is &lt;math display="inline"&gt;\sqrt{2.35 \times \frac{4}{3}} \approx 1.7701&lt;/math&gt;, and thus &lt;math display="inline"&gt;16:9 = 1.77\overline{7}&lt;/math&gt;... was chosen. This was discovered empirically by Kerns Powers, who cut out rectangles with equal areas and shaped them to match each of the popular aspect ratios.  When overlapped with their center points aligned, he found that all of those aspect ratio rectangles fit within an outer rectangle with an aspect ratio of 1.77:1 and all of them also covered a smaller common inner rectangle with the same aspect ratio 1.77:1.&lt;ref name="Cinemasource"&gt;{{cite journal |url=http://www.cinemasource.com/articles/aspect_ratios.pdf#page=8 |title=TECHNICAL BULLETIN: Understanding Aspect Ratios |publisher=The CinemaSource Press |year=2001 |accessdate=2009-10-24 |deadurl=no |archiveurl=https://web.archive.org/web/20090909132530/http://www.cinemasource.com/articles/aspect_ratios.pdf#page=8 |archivedate=2009-09-09 |df= }}&lt;/ref&gt; The value found by Powers is exactly the geometric mean of the extreme aspect ratios, [[4:3]]{{nbsp}}(1.33:1) and [[CinemaScope]]{{nbsp}}(2.35:1), which is coincidentally close to &lt;math display="inline"&gt;16:9&lt;/math&gt; (&lt;math display="inline"&gt;1.77\overline{7}:1&lt;/math&gt;). The intermediate ratios have no effect on the result, only the two extreme ratios.

Applying the same geometric mean technique to 16:9 and 4:3 approximately yields the [[14:9]] (&lt;math display="inline"&gt;1.55\overline{5}&lt;/math&gt;...) aspect ratio, which is likewise used as a compromise between these ratios.&lt;ref&gt;{{cite patent | title = Method of showing 16:9 pictures on 4:3 displays | country = US | number = 5956091 | gdate = September 21, 1999 }}&lt;/ref&gt; In this case 14:9 is exactly the ''[[arithmetic mean]]'' of &lt;math display="inline"&gt;16:9&lt;/math&gt; and &lt;math display="inline"&gt;4:3 = 12:9&lt;/math&gt;, since 14 is the average of 16 and 12, while the precise ''geometric mean'' is &lt;math display="inline"&gt;\sqrt{\frac{16}{9}\times\frac{4}{3}} \approx 1.5396 \approx 13.8:9,&lt;/math&gt; but the two different ''means'', arithmetic and geometric, are approximately equal because both numbers are sufficiently close to each other (a difference of less than 2%).

===Anti-reflective coatings===
In optical coatings, where reflection needs to be minimised between two media of refractive indices ''n''&lt;sub&gt;0&lt;/sub&gt; and ''n''&lt;sub&gt;2&lt;/sub&gt;, the optimum refractive index ''n''&lt;sub&gt;1&lt;/sub&gt; of the [[anti-reflective coating]] is given by the geometric mean: &lt;math&gt;n_1 = \sqrt{n_0 n_2}&lt;/math&gt;.

===Spectral flatness===
In [[signal processing]], [[spectral flatness]], a measure of how flat or spiky a spectrum is, is defined as the ratio of the geometric mean of the power spectrum to its arithmetic mean.

===Geometry===
{{right_angle_altitude.svg}}
In the case of a [[right triangle]], its altitude is the length of a line extending perpendicularly from the hypotenuse to its 90° vertex. Imagining that this line splits the hypotenuse into two segments, the geometric mean of these segment lengths is the length of the altitude.

In an [[ellipse]], the [[semi-minor axis]] is the geometric mean of the maximum and minimum distances of the ellipse from a [[Focus (mathematics)|focus]]; it is also the geometric mean of the [[semi-major axis]] and the [[conic section#Conic parameters|semi-latus rectum]]. The [[semi-major axis]] of an ellipse is the geometric mean of the distance from the center to either focus and the distance from the center to either [[Directrix (conic section)|directrix]].

Distance to the [[horizon]] of a [[sphere]] is the geometric mean of the distance to the closest point of the sphere and the distance to the farthest point of the sphere.

Both in the approximation of [[squaring the circle#Modern approximative constructions|squaring the circle according to S.A. Ramanujan (1914)]] and in the construction of the [[Heptadecagon#Construction|Heptadecagon]] according to ''"sent by T. P. Stowell, credited to Leybourn's Math. Repository, 1818"'', the geometric mean is employed.

===Financial===
The geometric mean has from time to time been used to calculate financial indices (the averaging is over the components of the index). For example, in the past the [[FT 30]] index used a geometric mean.&lt;ref name="Rowley 1987"&gt;{{cite book |title=The Financial System Today |first=Eric E. |last=Rowley |publisher=Manchester University Press |year=1987 |isbn=0719014875 }}&lt;/ref&gt; It is also used in the recently introduced "[[RPIJ]]" measure of inflation in the United Kingdom and elsewhere in the European Union.

This has the effect of understating movements in the index compared to using the arithmetic mean.&lt;ref name="Rowley 1987"/&gt;

=== Image Processing ===
The [[geometric mean filter]] is used as a noise filter in image processing.

==See also==
{{Portal|Statistics}}
{{div col|colwidth=22em}}
*[[Arithmetic mean]]
*[[Arithmetic-geometric mean]]
*[[Average]]
*[[Generalized mean]]
*[[Geometric mean theorem]]
*[[Geometric standard deviation]]
*[[Harmonic mean]]
*[[Heronian mean]]
*[[Hyperbolic coordinates]]
*[[Log-normal distribution]]
*[[Muirhead's inequality]]
*[[Multiplicative calculus]]
*[[Product (mathematics)|Product]]
*[[Pythagorean means]]
*[[Quadratic mean]]
*[[Quadrature (mathematics)]]
*[[Rate of return]]
*[[Weighted geometric mean]]
{{div col end}}

==Notes and references==
{{Reflist}}

==External links==
*[http://www.sengpielaudio.com/calculator-geommean.htm Calculation of the geometric mean of two numbers in comparison to the arithmetic solution]
*[http://www.cut-the-knot.org/Generalization/means.shtml Arithmetic and geometric means]
*[http://www.math.toronto.edu/mathnet/questionCorner/geomean.html When to use the geometric mean]
*[http://www.buzzardsbay.org/geomean.htm Practical solutions for calculating geometric mean with different kinds of data]
*[http://mathworld.wolfram.com/GeometricMean.html Geometric Mean on MathWorld]
*[http://www.cut-the-knot.org/pythagoras/GeometricMean.shtml Geometric Meaning of the Geometric Mean]
*[http://www.graftacs.com/geomean.php3 Geometric Mean Calculator for larger data sets]
*[https://www.census.gov/population/apportionment/about/how.html Computing [[Congressional apportionment]] using Geometric Mean ]
*[http://www.statisticshowto.com/geometric-mean-2/ Geometric Mean Definition and Examples]
*[https://medium.com/@JLMC/understanding-three-simple-statistics-for-data-visualizations-2619dbb3677a Why you should summarize your data with the geometric mean] – [[Medium (website)|Medium]] article

{{Statistics|descriptive}}

{{DEFAULTSORT:Geometric Mean}}
[[Category:Means]]</text>
      <sha1>aiaeaijy0rgec9bl446f9yqsgohg69g</sha1>
    </revision>
  </page>
  <page>
    <title>Graphon</title>
    <ns>0</ns>
    <id>26987628</id>
    <revision>
      <id>868801880</id>
      <parentid>868801010</parentid>
      <timestamp>2018-11-14T14:52:06Z</timestamp>
      <contributor>
        <ip>134.59.11.233</ip>
      </contributor>
      <comment>/* Graphon estimation */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10271">[[File:Exchangeable random graph from graphon.png|thumb|300px|A realization of an exchangeable random graph defined by a '''graphon'''. The graphon is shown as a magenta heatmap (lower right). A random graph of size &lt;math&gt;n&lt;/math&gt; is generated by independently assigning to each vertex &lt;math&gt;k \in \{1,\dotsc,n\}&lt;/math&gt; a latent random variable
    &lt;math&gt;U_{k} \sim \mathrm{U}(0,1)&lt;/math&gt; (values along vertical axis) and
    including each edge &lt;math&gt;(k,l)&lt;/math&gt; independently with probability &lt;math&gt;f(U_{k},U_{l})&lt;/math&gt;.
    For example, edge &lt;math&gt;(3,5)&lt;/math&gt; (green, dotted) is present with probability
    &lt;math&gt;f(0.72,0.9)&lt;/math&gt;; the green boxes in the right square represent the
    values of &lt;math&gt;(u_{3},u_{5})&lt;/math&gt; and &lt;math&gt;(u_{5},u_{3})&lt;/math&gt;. The upper left
    panel shows the graph realization as an adjacency matrix.]]

In [[graph theory]] and [[statistics]], a '''graphon''' is a symmetric measurable function &lt;math&gt;f:[0,1]^2\to[0,1]&lt;/math&gt;, that is important in the study of [[dense graph]]s. Graphons arise as the fundamental objects in two areas: as the defining objects of [[Exchangeable random variables|exchangeable]] random graph models and as a natural notion of limit for sequences of dense graphs. Graphons are tied to dense graphs by the following pair of observations: the random graph models defined by graphons give rise to dense graphs [[almost surely]], and, by the [[Szemerédi regularity lemma|regularity lemma]], graphons capture the structure of arbitrary large dense graphs.

Graphons are sometimes referred to as “continuous graphs”, but this is bad practice because there are many distinct objects that this label might be applied to. In particular, there are generalizations of graphons to the sparse graph regime that could just as well be called “continuous graphs.”

== Definition ==

A graphon is a symmetric measurable function &lt;math&gt;f:[0,1]^{2}\to[0,1]&lt;/math&gt;. Usually a graphon is understood as defining an exchangeable random graph model according to the following scheme:

# Each vertex &lt;math&gt;j&lt;/math&gt; of the graph is assigned an independent random value &lt;math&gt;u_{j}\sim U[0,1]&lt;/math&gt;
# Edge &lt;math&gt;(i,j)&lt;/math&gt; is independently included in the graph with probability &lt;math&gt;f(u_{i},u_{j})&lt;/math&gt;.

A random graph model is an exchangeable random graph model if and only if it can be defined in terms of a (possibly random) graphon in this way.

It is an immediate consequence of this definition and the law of large numbers that, if &lt;math&gt;f\neq0&lt;/math&gt;, exchangeable random graph models are dense almost surely.&lt;ref name=Orbanz:Roy:2015 /&gt;

== Examples ==

The simplest example of a graphon is &lt;math&gt;f(x,y)\equiv p&lt;/math&gt; for some constant &lt;math&gt;p\in[0,1]&lt;/math&gt;. In this case the associated exchangeable random graph model is the [[Erdős–Rényi]] model that includes each edge independently with probability &lt;math&gt;p&lt;/math&gt;.

The Erdős–Rényi model can be generalized by:

# Divide the unit square into &lt;math&gt;k\times k&lt;/math&gt; blocks
# Let &lt;math&gt;f&lt;/math&gt; equal &lt;math&gt;p_{lm}&lt;/math&gt; on the &lt;math&gt;l,m&lt;/math&gt;th block.

Effectively, this gives rise to the random graph model that has &lt;math&gt;k&lt;/math&gt; distinct Erdős–Rényi graphs with parameters &lt;math&gt;p_{ll}&lt;/math&gt; respectively and bigraphs between them where each possible edge between blocks &lt;math&gt;l,l&lt;/math&gt; and &lt;math&gt;m,m&lt;/math&gt; is included independently with probability &lt;math&gt;p_{lm}&lt;/math&gt;. This is simply the &lt;math&gt;k&lt;/math&gt; community [[stochastic block model]].

Many other popular random graph models can be understood as exchangeable random graph models defined by some graphon, a detailed survey is included in.&lt;ref name=Orbanz:Roy:2015 /&gt;

== Jointly exchangeable adjacency matrices ==

A random graph of size &lt;math&gt;n&lt;/math&gt; can be represented as a random &lt;math&gt;n\times n&lt;/math&gt; adjacency matrix. In order to impose consistency (in the sense of [[projective (probability)|projectivity]]) between random graphs of different sizes it is natural to study the sequence of adjacency matrices arising as the upper-left &lt;math&gt;n\times n&lt;/math&gt; sub-matrices of some infinite array of random variables; this allows us to generate &lt;math&gt;G_{n}&lt;/math&gt; by adding a node to &lt;math&gt;G_{n-1}&lt;/math&gt; and sampling the edges &lt;math&gt;(j,n)&lt;/math&gt; for &lt;math&gt;j&lt;n&lt;/math&gt;. With this perspective, random graphs are defined as random infinite symmetric arrays &lt;math&gt;(X_{ij})&lt;/math&gt;.

Following the fundamental importance of [[Exchangeable random variables|exchangeable sequences]] in classical probability, it is natural to look for an analogous notion in the random graph setting. One such notion is given by jointly exchangeable matrices; i.e. random matrices satisfying

: &lt;math&gt; (X_{ij}) \  \overset{d}{=} \, (X_{\sigma(i)\sigma(j)})&lt;/math&gt;

for all permutations &lt;math&gt;\sigma&lt;/math&gt; of the natural numbers, where &lt;math&gt;\overset{d}{=}&lt;/math&gt; means [[Random variable#Equality in distribution|equal in distribution]]. Intuitively, this condition means that the distribution of the random graph is unchanged by a relabeling of its vertices: that is, the labels of the vertices carry no information.

There is a representation theorem for jointly exchangeable random adjacency matrices, analogous to [[de Finetti’s representation theorem]] for exchangeable sequences. This is a special case of the [[Aldous–Hoover theorem]] for jointly exchangeable arrays and, in this setting, asserts that the random matrix &lt;math&gt;(X_{ij})&lt;/math&gt; is generated by:

# Sample &lt;math&gt;u_{j}\sim U[0,1]&lt;/math&gt; independently
# &lt;math&gt;X_{ij}=X_{ji}=1&lt;/math&gt; independently at random with probability &lt;math&gt;f(u_i,u_j),&lt;/math&gt;

where &lt;math&gt;f:[0,1]^2\to[0,1]&lt;/math&gt; is a (possibly random) graphon. That is, a random graph model has a jointly exchangeable adjacency matrix if and only if it is a jointly exchangeable random graph model defined in terms of some graphon.

== Limits of sequences of dense graphs ==

Consider a sequence of graphs &lt;math&gt;(G_n)&lt;/math&gt; where the number of vertices of &lt;math&gt;G_{n}&lt;/math&gt; goes to infinity. It is possible to define several notions of convergence of such sequences, each of which may give rise to a distinct limit object. For example, if the sequence &lt;math&gt;(G_n)&lt;/math&gt; was a realization of Erdős–Rényi graphs with parameter &lt;math&gt;p&lt;/math&gt; the natural notion of limit would be the edge density of the graphs, which converges to &lt;math&gt;p&lt;/math&gt;. In this case it would be natural to say that the limit of the sequence is the constant graphon &lt;math&gt;f(x,y)\equiv p&lt;/math&gt;. It turns out that for sequences of dense graphs a number of apparently distinct notions of convergence are equivalent and under all of them the natural limit object is a graphon.&lt;ref name=Lovasz:2013 /&gt;

== Graphon estimation ==
Due to identifiability issues, it is impossible to estimate either the graphon function &lt;math&gt;f&lt;/math&gt; or the node latent positions &lt;math&gt;u_i,&lt;/math&gt; and there are two main directions of graphon estimation.  One direction aims at estimating &lt;math&gt;f&lt;/math&gt;up to an equivalent class,&lt;ref&gt;{{Cite arxiv|last=Wolfe|first=Patrick J.|last2=Olhede|first2=Sofia C.|date=2013-09-23|title=Nonparametric graphon estimation|eprint=1309.5936|class=math.ST}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Choi|first=David|last2=Wolfe|first2=Patrick J.|date=2014-2|title=Co-clustering separately exchangeable network data|arxiv=1212.4093|journal=The Annals of Statistics|volume=42|issue=1|pages=29–63|doi=10.1214/13-AOS1173|issn=0090-5364}}&lt;/ref&gt; or estimate the probability matrix induced by &lt;math&gt;f&lt;/math&gt;.&lt;ref&gt;{{Cite journal|last=Gao|first=Chao|last2=Lu|first2=Yu|last3=Zhou|first3=Harrison H.|date=2015-12|title=Rate-optimal graphon estimation|url=https://projecteuclid.org/euclid.aos/1444222087|journal=The Annals of Statistics|language=EN|volume=43|issue=6|pages=2624–2652|doi=10.1214/15-AOS1354|issn=0090-5364|arxiv=1410.5837}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Yuan|first=Zhang,|last2=Elizaveta|first2=Levina,|last3=Ji|first3=Zhu,|date=2017|title=Estimating network edge probabilities by neighbourhood smoothing|url=https://academic.oup.com/biomet/article-abstract/104/4/771/4158787?redirectedFrom=fulltext|journal=Biometrika|language=en|volume=104|issue=4|pages=|doi=10.1093/biomet/asx042|issn=0006-3444|via=}}&lt;/ref&gt;

== Related notions ==

Graphons are naturally associated with dense simple graphs. There are straightforward extensions to dense directed weighted graphs. There are also recent extensions to the sparse graph regime, from both the perspective of random graph models &lt;ref name=Veitch:Roy:2015 /&gt; and graph limit theory.&lt;ref name=Borgs:Chayes:Cohn:Zhao:2014:sgc1 /&gt;&lt;ref name=Borgs:Chayes:Cohn:Zhao:2014:sgc2 /&gt;

== References ==

&lt;references&gt;
&lt;ref name=Veitch:Roy:2015&gt;{{Cite arxiv| last1 = Veitch| first1 = V.| last2 = Roy| first2 = D. M.| title = The Class of Random Graphs Arising from Exchangeable Random Measures |eprint=1512.03099| class = math.ST| year = 2015}}&lt;/ref&gt;
&lt;ref name=Borgs:Chayes:Cohn:Zhao:2014:sgc1&gt;{{Cite arxiv| last1 = Borgs| first1 = C.| last2 = Chayes| first2 = J. T.| last3 = Cohn| first3 = H.| last4 = Zhao| first4 = Y.| title = An ''L''&lt;sup&gt;''p''&lt;/sup&gt; theory of sparse graph convergence I: limits, sparse random graph models, and power law distributions |eprint=1401.2906 | class = math.CO| year = 2014}}&lt;/ref&gt;
&lt;ref name=Borgs:Chayes:Cohn:Zhao:2014:sgc2&gt;{{Cite journal| last1 = Borgs| first1 = C.| last2 = Chayes| first2 = J. T.| last3 = Cohn| first3 = H.| last4 = Zhao| first4 = Y.| title = An ''L''&lt;sup&gt;''p''&lt;/sup&gt; theory of sparse graph convergence II: LD convergence, quotients, and right convergence | journal = The Annals of Probability| volume = 46| issue = 2018| pages = 337|arxiv=1408.0744| year = 2014| doi = 10.1214/17-AOP1187}}&lt;/ref&gt;
&lt;ref name=Lovasz:2013&gt;{{Cite book| publisher = American Mathematical Society| last = Lovász| first = L.| title = Large Networks and Graph Limits}}&lt;/ref&gt;
&lt;ref name=Orbanz:Roy:2015&gt;{{Cite journal| volume = 37| issue = 2| pages = 437–461| last1 = Orbanz| first1 = P.| last2 = Roy| first2 = D.M.| title = Bayesian Models of Graphs, Arrays and Other Exchangeable Random Structures| journal = IEEE Transactions on Pattern Analysis and Machine Intelligence| doi=10.1109/tpami.2014.2334607| arxiv = 1312.7857| year = 2015}}&lt;/ref&gt;
&lt;/references&gt;

[[Category:Graph theory]]
[[Category:Probability theory]]</text>
      <sha1>qakytid2ph8szxi4vqckgzwsw11go11</sha1>
    </revision>
  </page>
  <page>
    <title>HH-suite</title>
    <ns>0</ns>
    <id>35881441</id>
    <revision>
      <id>855223851</id>
      <parentid>855221296</parentid>
      <timestamp>2018-08-16T19:44:45Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor/>
      <comment>Dating maintenance tags: {{COI}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10666">{{Multiple issues|
{{COI|date=August 2018}}
{{primary sources|date=July 2012}}
{{technical|date=July 2012}}
}}

{{Infobox Software
| name                   = HH-suite
| developer              = Johannes Söding, Michael Remmert, Andreas Biegert, Andreas Hauser, Markus Meier, Martin Steinegger
| latest_release_version = 2.0.16
| latest_release_date    = {{release date|2013|02|18|df=yes}}
| programming language   = [[C++]]
| language               = [[English language|English]]
| genre                  = [[Bioinformatics]] tool
| license                = [[GNU General Public License|GPL v3]]
| website                = https://github.com/soedinglab/hh-suite
}}

The '''HH-suite''' is an open-source software package for sensitive [[protein]] sequence searching. It contains programs that can search for similar protein sequences in protein sequence databases. Sequence searches are a standard tool in modern biology with which the function of unknown proteins can be inferred from the functions of proteins with similar sequences. 

==Sequence searches in biology==

Proteins are central players in all of life's processes. To understand how life in cells is organised, we have to understand what each of the proteins involved in these molecular processes does. This is particularly important in order to understand the origin of diseases. But for a large fraction of the approximately 20 000 human proteins the structures and functions remain unknown. Many proteins have been investigated in model organisms such as many bacteria, baker's yeast, fruit flies, zebra fish or mice, for which experiments can be often done more easily than with human cells. To predict the function, structure, or other properties of a protein for which only its sequence of amino acids is known, the protein sequence is compared to the sequences of other proteins in public databases. If a protein with sufficiently similar sequence is found, the two proteins are likely to be evolutionarily related ([[Homology (biology)#Sequence homology|"homologous"]]). In that case, they are likely to share similar structures and functions. Therefore, if a protein with a sufficiently similar sequence and with known functions and/or structure can be found by the sequence search, the unknown protein's functions, structure, and domain composition can be predicted.  Such predictions greatly facilitate the determination of the function or structure by targeted validation experiments.

==Description==

The HH-suite  [[HHpred / HHsearch|HHsearch]]  contains HHsearch
&lt;ref name="pmid15531603"&gt;{{ cite journal | author = Söding J | title = Protein homology detection by HMM-HMM comparison | journal = Bioinformatics | year =  2005 | volume = 21 | issue = 7 | pages = 951–960 | pmid = 15531603 | doi = 10.1093/bioinformatics/bti125}}&lt;/ref&gt; 
and HHblits 
&lt;ref name="pmid22198341"&gt;{{ cite journal |vauthors=Remmert M, Biegert A, Hauser A, Söding J | title = HHblits: Lightning-fast iterative protein sequence searching by HMM-HMM alignment. | journal = Nat. Methods | year =  2011 | volume = 9 | issue = 2 | pages = 173–175 | pmid = 22198341 | doi = 10.1038/NMETH.1818}}&lt;/ref&gt; 
among other programs and utilities. HHsearch is among the most popular methods for the detection of remotely related sequences and for protein structure prediction, having been cited over 2000 times in Google Scholar.&lt;ref&gt;[https://scholar.google.de/scholar?q=hhsearch+soeding Number of citations to HHsearch on Google Scholar]&lt;/ref&gt; The HHsearch and HHblits programs owe their power to the fact that both the query and the database sequences are represented by [[multiple sequence alignment]]s (MSAs). In these MSAs, the query or database sequence is written in a table together with homologous (related) sequences in such a way that each column contains homologous amino acid residues, that is, residues that have descended from the same residue in the ancestral sequence. The frequencies of amino acids in the columns of such an MSA can be interpreted as probabilities to observe an amino acid in a further homologous sequence at that position. To facilitate automatic scoring of potential sequences for their relatedness to the sequences in the MSA, the MSAs are succinctly described by profile [[hidden Markov model]]s (HMMs). These are extensions of [[position-specific scoring matrix|''position-specific scoring matrices'']] (PSSMs). The core algorithms for '''H'''MM-'''H'''MM alignment give HH-suite its name.

[[HHpred / HHsearch|HHsearch]] takes as input a [[multiple sequence alignment]] or a profile [[hidden Markov Model]] (HMM) and searches a database of profile HMMs for homologous (related) proteins. 
[[HHpred / HHsearch|HHsearch]] is often used for [[homology modeling]], that is, to build a model of the structure of a query protein for which only the sequence is known: For that purpose, a database of proteins with known structures such as the [[Protein Data Bank|protein data bank]] is searched for "template" proteins similar to the query protein. If such a template protein is found, the structure of the protein of interest can be predicted based on a pairwise [[multiple sequence alignment|sequence alignment]] of the query with the template protein sequence. In the [[CASP| CASP9]] protein structure prediction competition in 2010, a fully automated version of HHpred based on HHsearch and HHblits was ranked best out of 81 servers in template-based structure prediction [http://predictioncenter.org/casp9/groups_analysis.cgi?type=server&amp;tbm=on&amp;tbmfm=on&amp;submit=Filter CASP9 TBM/FM].

[[File:HHblits-Schematic.png|thumb|Iterative sequence search scheme of HHblits]]
HHblits was added to the HH-suite in 2011. It can build high-quality [[multiple sequence alignment]]s (MSAs) starting from a single query sequence or MSA. From the query, a profile HMM can be calculated. By using MSAs instead of single sequences, the sensitivity of sequence searches and the quality of the resulting sequence alignments can be improved dramatically. MSAs are also the starting point for a multitude of downstream computational methods, such as methods to predict the secondary and tertiary structure of proteins, to predict their molecular functions or cellular pathways, to predict the positions in their sequence or structure that contribute to enzymatic activity or ligand-binding, to predict evolutionarily conserved residues, disease-causing versus neutral mutations, the proteins' cellular localization and many more. This explains the importance to produce MSAs of the highest quality.

HHblits works similarly to [[PSI-BLAST]], the most popular iterative sequence search method. HHblits generates a profile HMM from the query sequence and iteratively searches through a large database of profile HMMs, such as HH-suite's uniprot20 database. The uniprot20 database contains all public, high-quality protein sequences that are collected in the [[UniProt]] database. These sequences are clustered and aligned into multiple sequence alignments, from which the profile HMMs in uniprot20 are generated. Significantly similar sequences from the previous search are added to the query profile HMM for the next search iteration. Compared to [[PSI-BLAST]] and [[HMMER]], HHblits is faster, up to twice as sensitive and produces more accurate alignments.&lt;ref name="pmid22198341"/&gt; HHblits uses the same HMM-HMM alignment algorithms as HHsearch, but it employs a fast prefilter that reduces the number of database HMMs for which to perform the slow HMM-HMM comparison from tens of millions to a few thousands.

The HH-suite comes with a number of useful databases of profile HMMs that can be searched using HHblits and HHsearch, among them a clustered version of the [[UniProt| UniProt database]], HMMs for the [[Protein Data Bank|protein data bank]] of protein structures, for the [[Pfam| Pfam database]] of protein family alignments, the [[Structural Classification of Proteins database|SCOP database]] of structural protein domains, and many more.

The HH-suite runs on most Linux and Unix distributions, including RedHat, Debian, Ubuntu, and Mac OS X. A [[Debian]] package is available.&lt;ref&gt;[http://packages.debian.org/unstable/science/hhsuite Debian hhsuite package]&lt;/ref&gt;

== Overview of programs in HH-suite ==

In addition to HHsearch and HHblits, the HH-suite contains programs and perl scripts for format conversion, filtering of MSAs, generation of profile HMMs, the addition of secondary structure predictions to MSAs, the extraction of alignments from program output, and the generation of customized databases.

{| border="0" cellpadding="0"
|-
| hhblits || (Iteratively) search an HHblits database with a query sequence or MSA 
|-
| hhsearch || Search an HHsearch database of HMMs with a query MSA or HMM
|-
| hhmake || Build an HMM from an input MSA
|-
| hhfilter || Filter an MSA by maximum sequence identity, coverage, and other criteria
|-
| hhalign || Calculate pairwise alignments, dot plots etc. for two HMMs/MSAs
|-
| reformat.pl || Reformat one or many MSAs
|-
|addss.pl || Add [[Psipred]] predicted secondary structure to an MSA or HHM file
|-
| hhmakemodel.pl   || Generate MSAs or coarse 3D models from HHsearch or HHblits results
|-
| hhblitsdb.pl || Build HHblits database with prefiltering, packed MSA/HMM, and index files
|-
| multithread.pl || Run a command for many files in parallel using multiple threads
|-
| splitfasta.pl || Split a multiple-sequence FASTA file into multiple single-sequence files
|-
| renumberpdb.pl || Generate PDB file with indices renumbered to match input sequence indices
|-
|}

== References ==
{{reflist}}

== External links ==
*[http://www.mpibpc.mpg.de/soeding Soeding Lab] at Max-Planck Institute in Göttingen - HH-suite developers
*[https://github.com/soedinglab/hh-suite HH-suite source code] download from github
*[http://wwwuser.gwdg.de/~compbiol/data/hhsuite/ Precompiled HH-suite binaries and databases] download from developers
*[http://toolkit.tuebingen.mpg.de/hhpred HHpred] &amp;mdash; free server at Max-Planck Institute in Tuebingen
*[http://toolkit.tuebingen.mpg.de/hhblits HHblits] &amp;mdash; free server at Max-Planck Institute in Tuebingen
*[http://predictioncenter.org/ CASP website]
*[http://predictioncenter.org/casp9/groups_analysis.cgi?type=server&amp;tbm=on&amp;tbmfm=on&amp;submit=Filter CASP9 template-based modeling results]
*[https://packages.debian.org/de/sid/science/hhsuite HH-suite debian package]
*[http://packages.ubuntu.com/raring/science/hhsuite HH-suite ubuntu package]
*[https://aur.archlinux.org/packages/hhsuite HH-suite arch linux user repository]

[[Category:Bioinformatics software]]
[[Category:Computational science]]</text>
      <sha1>rlt8mwdfehmc2rv1o6uskxpzwxz8qsp</sha1>
    </revision>
  </page>
  <page>
    <title>Heisenberg group</title>
    <ns>0</ns>
    <id>448518</id>
    <revision>
      <id>868520852</id>
      <parentid>856976546</parentid>
      <timestamp>2018-11-12T18:53:48Z</timestamp>
      <contributor>
        <username>Simplekto Murphison</username>
        <id>28016825</id>
      </contributor>
      <minor/>
      <comment>/* Heisenberg group of a locally compact abelian group */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="31526">In [[mathematics]],  the '''Heisenberg group''' &lt;math&gt;H&lt;/math&gt;, named after [[Werner Heisenberg]], is the [[group (mathematics)|group]] of 3&amp;times;3 [[triangular matrix|upper triangular matrices]] of the form

::&lt;math&gt;\begin{pmatrix}
 1 &amp; a &amp; c\\
 0 &amp; 1 &amp; b\\
 0 &amp; 0 &amp; 1\\
\end{pmatrix}&lt;/math&gt;

under the operation of [[matrix multiplication]]. Elements ''a, b'' and ''c'' can be taken from any [[commutative ring]] with identity, often taken to be the ring of [[real number]]s (resulting in the "continuous Heisenberg group") or the ring of [[integer]]s (resulting in the "discrete Heisenberg group").

The continuous Heisenberg group arises in the description of one-dimensional [[quantum mechanical]] systems, especially in the context of the [[Stone–von Neumann theorem]]. More generally, one can consider Heisenberg groups associated to ''n''-dimensional systems, and most generally, to any [[symplectic vector space]].

==The three-dimensional case==
In the three-dimensional case, the product of two Heisenberg matrices is given by:
:&lt;math&gt;\begin{pmatrix}
 1 &amp; a &amp; c\\
 0 &amp; 1 &amp; b\\
 0 &amp; 0 &amp; 1\\
\end{pmatrix}
\begin{pmatrix}
 1 &amp; a' &amp; c'\\
 0 &amp; 1 &amp; b'\\
 0 &amp; 0 &amp; 1\\
\end{pmatrix}=
\begin{pmatrix}
 1 &amp; a+a' &amp; c+c'+ab'\\
 0 &amp; 1 &amp; b+b'\\
 0 &amp; 0 &amp; 1\\
\end{pmatrix}\, .&lt;/math&gt;
Since the multiplication is not commutative, the group is [[Non-abelian group|non-abelian]].

The neutral element of the Heisenberg group is the [[identity matrix]], and inverses are given by

:&lt;math&gt;\begin{pmatrix}
 1 &amp; a &amp; c\\
 0 &amp; 1 &amp; b\\
 0 &amp; 0 &amp; 1\\
\end{pmatrix}^{-1}=
\begin{pmatrix}
 1 &amp; -a &amp; ab-c\\
 0 &amp; 1 &amp; -b\\
 0 &amp; 0 &amp; 1\\
\end{pmatrix}\,  .&lt;/math&gt;

The group is a subgroup of the 2-dimensional affine group Aff(2):  &lt;math&gt;\begin{pmatrix}
 1 &amp; a &amp; c\\
 0 &amp; 1 &amp; b\\
 0 &amp; 0 &amp; 1\\
\end{pmatrix}&lt;/math&gt; acting on &lt;math&gt;(\vec{x},1)&lt;/math&gt; corresponds to the affine transform &lt;math&gt;\begin{pmatrix}
 1 &amp; a\\
 0 &amp; 1 
\end{pmatrix}{\vec  x}+\begin{pmatrix}
c\\
 b
\end{pmatrix}&lt;/math&gt;.

There are several prominent examples of the three-dimensional case.

===Continuous Heisenberg group===
If {{math|''a, b, c''}}, are [[real number]]s (in the ring '''R''') then one has the '''continuous Heisenberg group''' H&lt;sub&gt;3&lt;/sub&gt;('''R''').

It is a [[nilpotent group|nilpotent]] real [[Lie group]] of dimension 3.

In addition to the representation as real 3x3 matrices, the continuous Heisenberg group also has several different [[group representation|representations]] in terms of [[function space]]s. By [[Stone–von Neumann theorem]], there is, up to isomorphism, a unique irreducible unitary representation of H in which its [[center of a group|centre]] acts by a given nontrivial [[Character theory|character]]. This representation has several important realizations, or models. In the ''Schrödinger model'', the Heisenberg group acts on the space of [[square integrable]] functions. In the [[theta representation]], it acts on the space of [[holomorphic function]]s on the [[upper half-plane]]; it is so named for its connection with the [[theta function]]s.

===Discrete Heisenberg group===
[[Image:HeisenbergCayleyGraph.png|thumb|350px|right|A portion of the [[Cayley graph]] of the discrete Heisenberg group, with generators ''x,y,z'' as in the text. (The coloring is only for visual aid.)]]

If {{math|''a, b, c''}},  are integers (in the ring '''Z''') then one has the '''discrete Heisenberg group''' H&lt;sub&gt;3&lt;/sub&gt;('''Z'''). It is a [[nonabelian group|non-abelian]] [[nilpotent group]]. It has two generators,
:&lt;math&gt;x=\begin{pmatrix}
 1 &amp; 1 &amp; 0\\
 0 &amp; 1 &amp; 0\\
 0 &amp; 0 &amp; 1\\
\end{pmatrix},\ \ y=\begin{pmatrix}
 1 &amp; 0 &amp; 0\\
 0 &amp; 1 &amp; 1\\
 0 &amp; 0 &amp; 1\\
\end{pmatrix}&lt;/math&gt;

and relations

:&lt;math&gt; z^{}_{}=xyx^{-1}y^{-1},\  xz=zx,\  yz=zy &lt;/math&gt;,

where

:&lt;math&gt;z=\begin{pmatrix}
 1 &amp; 0 &amp; 1\\
 0 &amp; 1 &amp; 0\\
 0 &amp; 0 &amp; 1\\
\end{pmatrix}&lt;/math&gt;
is the generator of the [[center of a group|center]] of H&lt;sub&gt;3&lt;/sub&gt;. (Note that the inverses of ''x'', ''y'', and ''z'' replace the 1 above the diagonal with −1.)

By [[Bass's theorem]], it has a polynomial [[Growth rate (group theory)|growth rate]] of order 4.

One can generate any element through
::&lt;math&gt;\begin{pmatrix}
 1 &amp; a &amp; c\\
 0 &amp; 1 &amp; b\\
 0 &amp; 0 &amp; 1\\
\end{pmatrix}=y^bz^cx^a\, .&lt;/math&gt;

=== Heisenberg group modulo an odd prime ''p'' ===
If one takes ''a, b, c'' in [[modular arithmetic|'''Z'''/''p'' '''Z''']] for an odd [[prime number|prime]] ''p'', then one has the '''Heisenberg group modulo''' '''''p'''''. It is a group of [[order (group theory)|order]] ''p''&lt;sup&gt;3&lt;/sup&gt; with generators ''x,y'' and relations:

:&lt;math&gt; z^{}_{}=xyx^{-1}y^{-1},\   x^p=y^p=z^p=1,\  xz=zx,\  yz=zy. &lt;/math&gt;

Analogues of Heisenberg groups over ''finite'' fields of odd prime order ''p'' are called [[extra special group]]s, or more properly, extra special groups of [[exponent (group theory)|exponent]] ''p''.  More generally, if the [[derived subgroup]] of a group ''G'' is contained in the center ''Z'' of  ''G'', then the map from ''G/Z'' × ''G/Z'' → ''Z'' is a skew-symmetric bilinear operator on abelian groups.  

However, requiring that ''G/Z'' to be a finite [[vector space]] requires the [[Frattini subgroup]] of ''G'' to be contained in the center, and requiring that ''Z'' be a one-dimensional vector space over '''Z'''/''p'' '''Z''' requires that ''Z'' have order ''p'', so if ''G'' is not abelian, then ''G'' is extra special.  If ''G'' is extra special but does not have exponent ''p'', then the general construction below applied to the symplectic vector space ''G/Z'' does not yield a group isomorphic to ''G''.

===Heisenberg group modulo 2===
The Heisenberg group modulo 2 is of order 8 and is isomorphic to the [[dihedral group]] D&lt;sub&gt;4&lt;/sub&gt; (the symmetries of a square). Observe that if

:&lt;math&gt;x=\begin{pmatrix}
 1 &amp; 1 &amp; 0\\
 0 &amp; 1 &amp; 0\\
 0 &amp; 0 &amp; 1\\
\end{pmatrix},\ \ y=\begin{pmatrix}
 1 &amp; 0 &amp; 0\\
 0 &amp; 1 &amp; 1\\
 0 &amp; 0 &amp; 1\\
\end{pmatrix}&lt;/math&gt;.

Then

:&lt;math&gt;xy=\begin{pmatrix}
 1 &amp; 1 &amp; 1\\
 0 &amp; 1 &amp; 1\\
 0 &amp; 0 &amp; 1\\
\end{pmatrix}, &lt;/math&gt;

and

:&lt;math&gt;yx=\begin{pmatrix}
 1 &amp; 1 &amp; 0\\
 0 &amp; 1 &amp; 1\\
 0 &amp; 0 &amp; 1\\
\end{pmatrix}. &lt;/math&gt;

The elements ''x'' and ''y'' correspond to reflections (with 45° between them), whereas ''xy'' and ''yx'' correspond to rotations by 90°. The other reflections are ''xyx'' and ''yxy'', and rotation by 180° is ''xyxy'' (=''yxyx'').

==Lie algebra==
The Lie algebra &lt;math&gt;\mathfrak h&lt;/math&gt; of the Heisenberg group &lt;math&gt;H&lt;/math&gt; (over the real numbers) is the space of &lt;math&gt;3\times 3&lt;/math&gt; matrices of the form&lt;ref&gt;{{harvnb|Hall|2015}} Proposition 3.26&lt;/ref&gt;
:&lt;math&gt;\begin{pmatrix}
 0 &amp; a &amp; b\\
 0 &amp; 0 &amp; c\\
 0 &amp; 0 &amp; 0\\
\end{pmatrix}&lt;/math&gt;,
with &lt;math&gt;a,b,c\in\mathbb R&lt;/math&gt;. The following three elements form a basis for &lt;math&gt;\mathfrak h&lt;/math&gt;:
:&lt;math&gt;X=\begin{pmatrix}
 0 &amp; 1 &amp; 0\\
 0 &amp; 0 &amp; 0\\
 0 &amp; 0 &amp; 0\\
\end{pmatrix};\quad
Y=\begin{pmatrix}
 0 &amp; 0 &amp; 0\\
 0 &amp; 0 &amp; 1\\
 0 &amp; 0 &amp; 0\\
\end{pmatrix};\quad
Z=\begin{pmatrix}
 0 &amp; 0 &amp; 1\\
 0 &amp; 0 &amp; 0\\
 0 &amp; 0 &amp; 0\\
\end{pmatrix}
&lt;/math&gt;.
The basis elements satisfy the commutation relations:
:&lt;math&gt;[X,Y]=Z;\quad [X,Z]=0;\quad [Y,Z]=0&lt;/math&gt;.
The name "Heisenberg group" is motivated by the preceding relations, which have the same form as the [[canonical commutation relation]]s in quantum mechanics:
:&lt;math&gt;[\hat x,\hat p]=i\hbar I;\quad [\hat x,i\hbar I]=0;\quad [\hat p,i\hbar I]=0&lt;/math&gt;,
where &lt;math&gt;\hat x&lt;/math&gt; is the position operator, &lt;math&gt;\hat p&lt;/math&gt; is the momentum operator, and &lt;math&gt;\hbar&lt;/math&gt; is Planck's constant.

The Heisenberg group &lt;math&gt;H&lt;/math&gt; has the special property that the exponential map is a one-to-one and onto map from the Lie algebra &lt;math&gt;\mathfrak h&lt;/math&gt; to the group &lt;math&gt;H&lt;/math&gt;.&lt;ref&gt;{{harvnb|Hall|2015}} Chapter 2, Exercise 9&lt;/ref&gt;

==Higher dimensions==
More general Heisenberg groups &lt;math&gt;H_{2n+1}&lt;/math&gt; may be defined for higher dimensions in Euclidean space, and more generally on [[symplectic vector space]]s.  The simplest general case is the real Heisenberg group of dimension &lt;math&gt;2n+1&lt;/math&gt;, for any integer &lt;math&gt;n\geq 1&lt;/math&gt;. As a group of matrices,  &lt;math&gt;H_{2n+1}&lt;/math&gt; (or &lt;math&gt;H_{2n+1}(\mathbb R)&lt;/math&gt; to indicate this is the Heisenberg group over the field &lt;math&gt;\mathbb R&lt;/math&gt; of real numbers) is defined as the group &lt;math&gt;(n+2)\times (n+2)&lt;/math&gt; matrices with entries in &lt;math&gt;\mathbb R&lt;/math&gt; and having the form:

:&lt;math&gt; \begin{bmatrix} 1 &amp; \mathbf a &amp; c \\ 0 &amp; I_n &amp; \mathbf b \\ 0 &amp; 0 &amp; 1 \end{bmatrix} &lt;/math&gt;

where
: '''a''' is a [[row vector]] of length ''n'',
: '''b''' is a [[column vector]] of length ''n'',
: ''I''&lt;sub&gt;''n''&lt;/sub&gt; is the [[identity matrix]] of size ''n''.

===Group Structure===
This is indeed a group, as is shown by the multiplication:

:&lt;math&gt; \begin{bmatrix} 1 &amp; \mathbf a &amp; c \\ 0 &amp; I_n &amp; \mathbf b \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix}1 &amp; \mathbf a' &amp; c' \\ 0 &amp; I_n &amp; \mathbf b' \\ 0 &amp; 0 &amp; 1 \end{bmatrix} =  \begin{bmatrix} 1 &amp; \mathbf a+ \mathbf a' &amp; c+c' +\mathbf a \cdot \mathbf b' \\ 0 &amp; I_n &amp; \mathbf b+\mathbf b' \\ 0 &amp; 0 &amp; 1 \end{bmatrix}  &lt;/math&gt;

and

:&lt;math&gt; \begin{bmatrix} 1 &amp; \mathbf a &amp; c \\ 0 &amp; I_n &amp; \mathbf b \\ 0 &amp; 0 &amp; 1 \end{bmatrix} \cdot \begin{bmatrix}1 &amp; -\mathbf a &amp; -c +\mathbf a \cdot \mathbf b\\ 0 &amp; I_n &amp; -\mathbf b \\ 0 &amp; 0 &amp; 1 \end{bmatrix} =  \begin{bmatrix} 1 &amp; 0 &amp; 0 \\ 0 &amp; I_n &amp; 0 \\ 0 &amp; 0 &amp; 1 \end{bmatrix}.  &lt;/math&gt;

===Lie algebra===
The Heisenberg group is a [[simply-connected]] Lie group whose [[Lie algebra]] consists of matrices

:&lt;math&gt; \begin{bmatrix} 0 &amp; \mathbf a &amp; c \\ 0 &amp; 0_n &amp; \mathbf b \\ 0 &amp; 0 &amp; 0 \end{bmatrix}, &lt;/math&gt;

where

: '''a''' is a row vector of length ''n'', 
: '''b''' is a column vector of length ''n'',
: 0&lt;sub&gt;''n''&lt;/sub&gt; is the [[zero matrix]] of size ''n''.

By letting e&lt;sub&gt;1&lt;/sub&gt;, ..., e&lt;sub&gt;''n''&lt;/sub&gt; be the canonical basis of '''R'''&lt;sup&gt;''n''&lt;/sup&gt;, and setting

:&lt;math&gt; p_i = \begin{bmatrix} 0 &amp; \operatorname{e}_i^{\mathrm{T}} &amp; 0 \\ 0 &amp; 0_n &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix}, &lt;/math&gt;
:&lt;math&gt; q_j = \begin{bmatrix} 0 &amp; 0 &amp; 0 \\ 0 &amp; 0_n &amp; \operatorname{e}_j \\ 0 &amp; 0 &amp; 0 \end{bmatrix}, &lt;/math&gt;
:&lt;math&gt; z = \begin{bmatrix} 0 &amp; 0  &amp; 1\\ 0 &amp; 0_n &amp; 0 \\ 0 &amp; 0 &amp; 0 \end{bmatrix}, &lt;/math&gt;

the associated [[Lie algebra]] can be characterized by the [[canonical commutation relations]],
{{NumBlk|:|&lt;math&gt; \begin{bmatrix} p_i, q_j \end{bmatrix} = \delta_{ij}z,   \qquad 
\begin{bmatrix}  p_i, z \end{bmatrix} = 0,  \qquad \begin{bmatrix}  q_j, z \end{bmatrix} = 0~,&lt;/math&gt;|1}}

where ''p''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;...,&amp;nbsp;''p''&lt;sub&gt;''n''&lt;/sub&gt;, ''q''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;...,&amp;nbsp;''q''&lt;sub&gt;''n''&lt;/sub&gt;, ''z'' are the algebra generators.

In particular, ''z'' is a ''central'' element of the Heisenberg Lie algebra. Note that the Lie algebra of the Heisenberg group is nilpotent.

===Exponential Map===
The [[exponential map (Lie theory)|exponential map]] is given by the following expression

:&lt;math&gt; \exp  \begin{bmatrix} 0 &amp; \mathbf a &amp; c \\ 0 &amp; 0_n &amp; \mathbf b \\ 0 &amp; 0 &amp; 0 \end{bmatrix} = \sum_{k=0}^\infty \frac{1}{k!}\begin{bmatrix} 0 &amp; \mathbf a &amp; c \\ 0 &amp; 0_n &amp; \mathbf b \\ 0 &amp; 0 &amp; 0 \end{bmatrix}^k = \begin{bmatrix} 1 &amp; \mathbf a &amp; c + {1\over 2}\mathbf a \cdot \mathbf b\\ 0 &amp; I_n &amp; \mathbf b \\ 0 &amp; 0 &amp; 1 \end{bmatrix}. &lt;/math&gt;
The exponential map of a nilpotent Lie algebra is a [[diffeomorphism]] between the Lie algebra and the unique associated [[connected space|connected]], [[simply-connected]] Lie group.

This discussion (aside from statements referring to dimension and Lie group) further applies if we replace '''R''' by any commutative ring ''A''. The corresponding group is denoted ''H''&lt;sub&gt;''n''&lt;/sub&gt;(''A'' ).

Under the additional assumption that the prime 2 is invertible in the ring ''A'', the exponential map is also defined, since it reduces to a finite sum and has the form above (i.e. ''A'' could be a ring '''Z'''/''p'' '''Z''' with an odd prime ''p'' or any [[field (mathematics)|field]] of [[characteristic (algebra)|characteristic]] 0).

== Representation theory ==
{{main article|Stone–von Neumann theorem}}

The unitary [[Lie group representation|representation theory]] of the Heisenberg group is fairly simple – later generalized by [[Mackey theory]] – and was the motivation for its introduction in quantum physics, as discussed below.

For each nonzero real number &lt;math&gt;\hbar&lt;/math&gt;, we can define an irreducible unitary representation &lt;math&gt;\Pi_\hbar&lt;/math&gt; of &lt;math&gt;H_{2n+1}&lt;/math&gt; acting on the Hilbert space &lt;math&gt;L^2(\mathbb R^n)&lt;/math&gt; by the formula:&lt;ref&gt;{{harvnb|Hall|2013}} Proposition 14.7&lt;/ref&gt;

:&lt;math&gt; \left[\Pi_\hbar\begin{pmatrix} 1 &amp; \mathbf a &amp; c \\ 0 &amp; I_n &amp; \mathbf b \\ 0 &amp; 0 &amp; 1 \end{pmatrix}\psi\right](x)=e^{i\hbar c}e^{ib\cdot x}\psi(x+\hbar a) &lt;/math&gt;

This representation is known as the [[Oscillator_representation#Commutation_relations_of_Heisenberg_and_Weyl|Schrödinger representation]]. The motivation for the this representation is the action of the exponentiated [[position operator|position]] and [[momentum operator]]s in quantum mechanics. The parameter &lt;math&gt;a&lt;/math&gt; describes translations in position space, the parameter &lt;math&gt;b&lt;/math&gt; describes translations in momentum space, and the parameter &lt;math&gt;c&lt;/math&gt; gives an overall phase factor. The phase factor is needed to obtain a group of operators, since translations in position space and translations in momentum space do not commute.

The key result is the [[Stone–von Neumann theorem]], which states that every (strongly continuous) irreducible unitary representation of the Heisenberg group in which the center acts nontrivially is equivalent to &lt;math&gt;\Pi_\hbar&lt;/math&gt; for some &lt;math&gt;\hbar&lt;/math&gt;.&lt;ref&gt;{{harvnb|Hall|2013}} Theorem 14.8&lt;/ref&gt; Alternatively, that they are all equivalent to the [[Weyl algebra]] (or [[CCR algebra]]) on a symplectic space of dimension 2''n''.

Since the Heisenberg group is a one-dimensional central extension of &lt;math&gt;\mathbb R^{2n}&lt;/math&gt;, its irreducible unitary representations can be viewed as irreducible unitary [[projective representation]]s of &lt;math&gt;\mathbb R^{2n}&lt;/math&gt;. Conceptually, the representation given above constitutes the quantum mechanical counterpart to the group of translational symmetries on the classical phase space, &lt;math&gt;\mathbb R^{2n}&lt;/math&gt;. The fact that the quantum version is only a ''projective'' representation of &lt;math&gt;R^{2n}&lt;/math&gt; is suggested already at the classical level. The Hamiltonian generators of translations in phase space are the position and momentum functions. The span of these functions do not form a Lie algebra under the [[Poisson bracket]] however, because &lt;math&gt;\{x_i,p_j\}=\delta_{i,j}.&lt;/math&gt; Rather, the span of the position and momentum functions ''and the constants'' forms a Lie algebra under the Poisson bracket. This Lie algebra is a one-dimensional central extension of the commutative Lie algebra &lt;math&gt;\mathbb R^{2n}&lt;/math&gt;, isomorphic to the Lie algebra of the Heisenberg group.

==On symplectic vector spaces==
The general abstraction of a Heisenberg group is constructed from any [[symplectic vector space]].&lt;ref&gt;Hans Tilgner, "[http://www.numdam.org/numdam-bin/fitem?id=AIHPA_1970__13_2_103_0 A class of solvable Lie groups and their relation to the canonical formalism] {{webarchive|url=https://web.archive.org/web/20110605030417/http://www.numdam.org/numdam-bin/fitem?id=AIHPA_1970__13_2_103_0 |date=2011-06-05 }}", ''Annales de l'institut Henri Poincaré (A) Physique théorique'', '''13 ''' no. 2 (1970), pp. 103-127.&lt;/ref&gt;  For example, let (''V'',ω) be a finite-dimensional real symplectic vector space (so ω is a [[nondegenerate form|nondegenerate]] [[skew symmetric]] [[bilinear form]] on ''V'').  The Heisenberg group H(''V'') on (''V'',ω) (or simply ''V'' for brevity) is the set ''V''×'''R''' endowed with the group law

:&lt;math&gt;(v,t)\cdot(v',t') =\left (v+v',t+t'+\tfrac{1}{2}\omega(v,v')\right).&lt;/math&gt;

The Heisenberg group is a [[Group extension#Central extension|central extension]] of the additive group ''V''.  Thus there is an [[exact sequence]]

:&lt;math&gt;0\to\mathbf{R}\to H(V)\to V\to 0.&lt;/math&gt;

Any symplectic vector space admits a [[Symplectic vector space#Standard symplectic space|Darboux basis]] {'''e'''&lt;sub&gt;''j''&lt;/sub&gt;,'''f'''&lt;sup&gt;''k''&lt;/sup&gt;}&lt;sub&gt;1 ≤ ''j'',''k'' ≤ ''n''&lt;/sub&gt; satisfying ω('''e'''&lt;sub&gt;''j''&lt;/sub&gt;,'''f'''&lt;sup&gt;''k''&lt;/sup&gt;) = δ&lt;sub&gt;''j''&lt;/sub&gt;&lt;sup&gt;''k''&lt;/sup&gt; and where 2''n'' is the dimension of ''V'' (the dimension of ''V'' is necessarily even).  In terms of this basis, every vector decomposes as

:&lt;math&gt;v=q^a\mathbf{e}_a+p_a\mathbf{f}^a.&lt;/math&gt;

The ''q''&lt;sup&gt;''a''&lt;/sup&gt; and ''p''&lt;sub&gt;''a''&lt;/sub&gt; are [[canonically conjugate coordinates]].

If {'''e'''&lt;sub&gt;''j''&lt;/sub&gt;, '''f'''&lt;sup&gt;''k''&lt;/sup&gt;}&lt;sub&gt;1 ≤ ''j'',''k'' ≤ ''n''&lt;/sub&gt; is a Darboux basis for ''V'', then let {''E''} be a basis for '''R''', and {'''e'''&lt;sub&gt;''j''&lt;/sub&gt;, '''f'''&lt;sup&gt;''k''&lt;/sup&gt;, ''E''}&lt;sub&gt;1 ≤ ''j'',''k'' ≤ ''n''&lt;/sub&gt; is the corresponding basis for ''V''×'''R'''.  A vector in H(''V'') is then given by

:&lt;math&gt;v=q^a\mathbf{e}_a+p_a\mathbf{f}^a+tE&lt;/math&gt;

and the group law becomes

:&lt;math&gt;(p,q,t)\cdot(p',q',t') =\left (p+p',q+q',t+t'+\frac{1}{2}(p q'-p' q)\right).&lt;/math&gt;

Because the underlying manifold of the Heisenberg group is a linear space, vectors in the Lie algebra can be canonically identified with vectors in the group.  The Lie algebra of the Heisenberg group is given by the commutation relation

:&lt;math&gt;\begin{bmatrix} (v_1,t_1),(v_2,t_2) \end{bmatrix} =\omega(v_1,v_2)&lt;/math&gt;

or written in terms of the Darboux basis

:&lt;math&gt;[\mathbf{e}_a,\mathbf{f}^b]=\delta_a^b&lt;/math&gt;

and all other commutators vanish.

It is also possible to define the group law in a different way but which yields a group isomorphic to the group we have just defined. To avoid confusion, we will use ''u'' instead of ''t'', so a vector is given by

:&lt;math&gt;v=q^a\mathbf{e}_a+p_a\mathbf{f}^a+uE&lt;/math&gt;

and the group law is

:&lt;math&gt;(p,q,u)\cdot(p',q',u')=(p+p',q+q',u+u'+p q').&lt;/math&gt;

An element of the group 
:&lt;math&gt;v=q^a\mathbf{e}_a+p_a\mathbf{f}^a+uE&lt;/math&gt; 
can then be expressed as a matrix
:&lt;math&gt;
\begin{bmatrix}
1 &amp; p   &amp; u\\
0 &amp; I_n &amp; q\\
0 &amp; 0   &amp; 1
\end{bmatrix}&lt;/math&gt; ,
which gives a faithful [[matrix representation]] of H(''V''). The ''u'' in this formulation is related to ''t'' in our previous formulation by &lt;math&gt;u=t+\tfrac{1}{2}pq&lt;/math&gt;, so that the ''t'' value for the product comes to

:&lt;math&gt;u+u'+p q'-\tfrac{1}{2}(p+p')(q+q')&lt;/math&gt;
:&lt;math&gt;=t+\tfrac{1}{2}p q+t'+\tfrac{1}{2}p' q'+p q'-\tfrac{1}{2}(p+p')(q+q')&lt;/math&gt;
:&lt;math&gt;=t+t'+\tfrac{1}{2}(p q'-p' q)&lt;/math&gt; ,
as before.

The isomorphism to the group using upper triangular matrices relies on the decomposition of ''V'' into a Darboux basis, which amounts to a choice of isomorphism ''V'' ≅ ''U'' ⊕ ''U''*. Although the new group law yields a group isomorphic to the one given higher up, the group with this law is sometimes referred to as the '''polarized Heisenberg group''' as a reminder that this group law relies on a choice of basis (a choice of a Lagrangian subspace of ''V'' is a [[polarization (abelian variety)|polarization]]).

To any Lie algebra, there is a unique [[connected space|connected]], [[simply connected]] Lie group ''G''.  All other connected Lie groups with the same Lie algebra as ''G'' are of the form ''G''/''N'' where ''N'' is a central discrete group in ''G''.  In this case, the center of H(''V'') is '''R''' and the only discrete subgroups are isomorphic to ''Z''.  Thus H(''V'')/'''Z''' is another Lie group which shares this Lie algebra.  Of note about this Lie group is that it admits no faithful finite-dimensional representations; it is not isomorphic to any matrix group.  It does however have a well-known family of infinite-dimensional unitary representations.

==The connection with the Weyl algebra==
{{main article|Weyl algebra}}
The Lie algebra &lt;math&gt;\mathfrak{h}_n&lt;/math&gt; of the Heisenberg group was described above, (1), as a Lie algebra of matrices. The [[Poincaré–Birkhoff–Witt theorem]] applies to determine the [[universal enveloping algebra]] &lt;math&gt;U(\mathfrak{h}_n)&lt;/math&gt;. Among other properties, the universal enveloping algebra is an [[associative algebra]] into which &lt;math&gt;\mathfrak{h}_n&lt;/math&gt; injectively imbeds.

By the Poincaré–Birkhoff–Witt theorem, it is thus the [[free module|free vector space]] generated by the monomials
:&lt;math&gt; z^j p_1^{k_1} p_2^{k_2} \cdots p_n^{k_n} q_1^{\ell_1} q_2^{\ell_2} \cdots q_n^{\ell_n} ~,&lt;/math&gt;
where the exponents are all non-negative.

Consequently, &lt;math&gt;U(\mathfrak{h}_n)&lt;/math&gt; consists of real polynomials 
:&lt;math&gt; \sum_{j, \vec{k}, \vec{\ell}} c_{j \vec{k} \vec{\ell}} \,\, z^j  p_1^{k_1} p_2^{k_2} \cdots p_n^{k_n} q_1^{\ell_1} q_2^{\ell_2} \cdots q_n^{\ell_n}   ~,&lt;/math&gt;
with the commutation relations
:&lt;math&gt; p_k p_\ell = p_\ell p_k, \quad q_k q_\ell = q_\ell q_k, \quad p_k q_\ell - q_\ell p_k = \delta_{k \ell} z, \quad z p_k - p_k z =0, \quad z q_k - q_k z =0~.&lt;/math&gt;

The algebra &lt;math&gt;U(\mathfrak{h}_n)&lt;/math&gt; is closely related to the algebra of differential operators on ℝ&lt;sup&gt;''n''&lt;/sup&gt; with polynomial coefficients, since any such operator has a unique representation in the form
:&lt;math&gt;P=\sum_{\vec{k}, \vec{\ell}} c_{\vec{k} \vec{\ell}} \,\, \partial_{x_1}^{k_1} \partial_{x_2}^{k_2} \cdots \partial_{x_n}^{k_n} x_1^{\ell_1} x_2^{\ell_2} \cdots x_n^{\ell_n}  ~.&lt;/math&gt;

This algebra is called the [[Weyl algebra]]. It follows from [[abstract nonsense]] that the [[Weyl algebra]] ''W&lt;sub&gt;n&lt;/sub&gt;'' is a quotient of &lt;math&gt;U(\mathfrak{h}_n)&lt;/math&gt;.  However, this is also easy to see directly from the above representations; [[viz.]] by the mapping
:&lt;math&gt; z^j p_1^{k_1} p_2^{k_2} \cdots p_n^{k_n} q_1^{\ell_1} q_2^{\ell_2} \cdots q_n^{\ell_n} \, \mapsto \, \partial_{x_1}^{k_1} \partial_{x_2}^{k_2} \cdots \partial_{x_n}^{k_n} x_1^{\ell_1} x_2^{\ell_2} \cdots x_n^{\ell_n}~.&lt;/math&gt;

== Applications ==

=== Weyl's parameterization of quantum mechanics ===
{{main article|Wigner–Weyl transform}}
The application that led [[Hermann Weyl]] to an explicit realization of the Heisenberg group was the question of why the [[Schrödinger picture]] and [[Heisenberg picture]] are physically equivalent. Abstractly, the reason is the [[Stone–von Neumann theorem]]: there is a unique [[unitary representation]] with given action of the central Lie algebra element ''z'',  up to a unitary equivalence: the nontrivial elements of the algebra are all equivalent to the usual position and momentum operators.

Thus, the Schrödinger picture and Heisenberg picture are equivalent – they are just different ways of realizing this essentially unique representation.

=== Theta representation ===
{{main article|theta representation}}
The same uniqueness result was used by [[David Mumford]] for discrete Heisenberg groups, in his theory of [[equations defining abelian varieties]]. This is a large generalization of the approach used in [[Jacobi's elliptic functions]], which is the case of the modulo 2 Heisenberg group, of order 8. The simplest case is the [[theta representation]] of the Heisenberg group, of which the discrete case gives the [[theta function]].

=== Fourier analysis ===
The Heisenberg group also occurs in [[Fourier analysis]], where it is used in some formulations of the [[Stone–von Neumann theorem]]. In this case, the Heisenberg group can be understood to act on the space of [[square integrable]] functions; the result is a representation of the Heisenberg groups sometimes called the Weyl representation.

==As a sub-Riemannian manifold==
The three-dimensional Heisenberg group ''H''&lt;sub&gt;3&lt;/sub&gt;('''R''') on the reals can also be understood to be a smooth [[manifold]], and specifically, a simple example of a [[sub-Riemannian manifold]].&lt;ref&gt;Richard Montgomery, ''A Tour of Subriemannian Geometries, Their Geodesics and Applications (Mathematical Surveys and Monographs, Volume 91)'', (2002) American Mathematical Society, {{ISBN|0-8218-1391-9}}.&lt;/ref&gt; Given a point ''p''=(''x'',''y'',''z'') in '''R'''&lt;sup&gt;3&lt;/sup&gt;, define a differential [[1-form]] Θ at this point as

:&lt;math&gt;\Theta_p=dz -\frac{1}{2}\left(xdy - ydx\right).&lt;/math&gt;

This [[one-form]] belongs to the [[cotangent bundle]] of '''R'''&lt;sup&gt;3&lt;/sup&gt;; that is,

:&lt;math&gt;\Theta_p:T_p\mathbf{R}^3\to\mathbf{R}&lt;/math&gt;

is a map on the [[tangent bundle]].  Let

:&lt;math&gt;H_p = \{ v\in T_p\mathbf{R}^3 \mid \Theta_p(v) = 0 \}.&lt;/math&gt;

It can be seen that ''H'' is a [[subbundle]] of the tangent bundle T'''R'''&lt;sup&gt;3&lt;/sup&gt;.  A [[cometric]] on ''H'' is given by projecting vectors to the two-dimensional space spanned by vectors in the ''x'' and ''y'' direction.  That is, given vectors &lt;math&gt;v=(v_1,v_2,v_3)&lt;/math&gt; and &lt;math&gt;w=(w_1,w_2,w_3)&lt;/math&gt; in T'''R'''&lt;sup&gt;3&lt;/sup&gt;, the inner product is given by

:&lt;math&gt;\langle v,w\rangle = v_1w_1+v_2w_2.&lt;/math&gt;

The resulting structure turns ''H'' into the manifold of the Heisenberg group. An orthonormal frame on the manifold is given by the Lie [[vector field]]s

:&lt;math&gt;X=\frac{\partial}{\partial x} - \frac{1}{2} y\frac{\partial}{\partial z},&lt;/math&gt;
:&lt;math&gt;Y=\frac{\partial}{\partial y} + \frac{1}{2} x\frac{\partial}{\partial z},&lt;/math&gt;
:&lt;math&gt;Z=\frac{\partial}{\partial z},&lt;/math&gt;

which obey the relations [''X'',''Y'']=''Z'' and [''X'',''Z'']=[''Y'',''Z'']=0. Being Lie vector fields, these form a left-invariant basis for the group action.  The [[geodesic]]s on the manifold are spirals, projecting down to circles in two dimensions. That is, if

:&lt;math&gt;\gamma(t)=(x(t),y(t),z(t))&lt;/math&gt;

is a geodesic curve, then the curve &lt;math&gt;c(t)=(x(t),y(t))&lt;/math&gt; is an arc of a circle, and

:&lt;math&gt;z(t)=\frac{1}{2}\int_c xdy-ydx&lt;/math&gt;

with the integral limited to the two-dimensional plane. That is, the height of the curve is proportional to the area of the circle subtended by the [[circular arc]], which follows by [[Stokes' theorem]].

==Heisenberg group of a locally compact abelian group==
It is more generally possible to define the Heisenberg group of a [[locally compact abelian group]] ''K'', equipped with a [[Haar measure]].&lt;ref&gt;{{citation|author=[[David Mumford]]|title=Tata lectures on theta III|journal=Progress in Mathematics|volume=97|publisher=Birkhauser|year=1991}}&lt;/ref&gt;  Such a group has a [[Pontrjagin dual]] &lt;math&gt;\hat{K}&lt;/math&gt;, consisting of all continuous &lt;math&gt;U(1)&lt;/math&gt;-valued characters on ''K'', which is also a locally compact abelian group if endowed with the [[Compact-open topology|compact-open topology]].  The Heisenberg group associated with the locally compact abelian group ''K'' is the subgroup of the unitary group of &lt;math&gt;L^2(K)&lt;/math&gt; generated by translations from ''K'' and multiplications by elements of &lt;math&gt;\hat{K}&lt;/math&gt;.

In more detail, the [[Hilbert space]] &lt;math&gt;L^2(K)&lt;/math&gt; consists of square-integrable complex-valued functions &lt;math&gt;f&lt;/math&gt; on ''K''.  The translations in ''K'' form a [[unitary representation]] of ''K'' as operators on &lt;math&gt;L^2(K)&lt;/math&gt;:
:&lt;math&gt;(T_xf)(y) = f(x+y)&lt;/math&gt;
for &lt;math&gt;x,y\in K&lt;/math&gt;.  So too do the multiplications by characters:
:&lt;math&gt;(M_\chi f)(y) = \chi(y)f(y)&lt;/math&gt;
for &lt;math&gt;\chi\in\hat{K}&lt;/math&gt;.  These operators do not commute, and instead satisfy
:&lt;math&gt;(T_xM_\chi T^{-1}_x M_\chi^{-1}f)(y) = \overline{\chi(x)}f(y)&lt;/math&gt;
multiplication by a fixed unit modulus complex number.

So the Heisenberg group &lt;math&gt;H(K)&lt;/math&gt; associated with ''K'' is a type of [[Central extension (mathematics)|central extension]] of &lt;math&gt;K\times\hat{K}&lt;/math&gt;, via an exact sequence of groups:
:&lt;math&gt;1\to U(1) \to H(K) \to K\times\hat{K}\to 0.&lt;/math&gt;
More general Heisenberg groups are described by 2-cocyles in the [[group cohomology|cohomology group]] &lt;math&gt;H^2(K,U(1))&lt;/math&gt;.  The existence of a duality between &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;\hat{K}&lt;/math&gt; gives rise to a canonical cocycle, but there are generally others.

The Heisenberg group acts irreducibly on &lt;math&gt;L^2(K)&lt;/math&gt;. Indeed, the continuous characters separate points&lt;ref&gt;{{citation|title = The structure of compact groups: a primer for students, a handbook for the expert
|   author=  Karl Heinrich Hofmann, Sidney A. Morris
|publisher = Walter de Gruyter,
|   isbn =      9783110190069
|   year =      2006
|   series =    De Gruyter studies in mathematics 25,
|   edition =   2nd rev. ed,
}}&lt;/ref&gt; so any unitary operator of &lt;math&gt;L^2(K)&lt;/math&gt; that commutes with them is an &lt;math&gt;L^\infty&lt;/math&gt; [[Fourier multiplier|multiplier]].  But commuting with translations implies that the multiplier is constant.&lt;ref&gt;This argument appears in a slightly different setting in {{citation|author=Roger Howe|title=On the role of the Heisenberg group in harmonic analysis|journal=[[Bulletin of the American Mathematical Society]]|volume=3|number=2|year=1980 | doi = 10.1090/S0273-0979-1980-14825-9 | mr = 578375 |pages=821–844}}&lt;/ref&gt;

A version of the [[Stone–von Neumann theorem]], proved by [[George Mackey]], holds for the Heisenberg group &lt;math&gt;H(K)&lt;/math&gt;.&lt;ref&gt;{{citation|author=George Mackey|title=On a theorem of Stone and von Neumann|journal=Duke Mathematical Journal|volume=16|number=2|year=1949|pages=313–326|doi=10.1215/s0012-7094-49-01631-2}}&lt;/ref&gt;&lt;ref&gt;{{citation|author=A Prasad|arxiv=0912.0574|title=An easy proof of the Stone–von Neumann–Mackey theorem|year=2009|doi=10.1016/j.exmath.2010.06.001}}&lt;/ref&gt;  The [[Fourier transform]] is the unique intertwiner between the representations of &lt;math&gt;L^2(K)&lt;/math&gt; and &lt;math&gt;L^2(\hat{K})&lt;/math&gt;.  See the discussion at [[Stone–von Neumann theorem#Relation to the Fourier transform]] for details.

==See also==
* [[Canonical commutation relations]]
* [[Wigner–Weyl transform]]
* [[Stone–von Neumann theorem]]
* [[Projective representation]]

==Notes==
&lt;references/&gt;

==References==
*{{cite book |first=Ernst |last=Binz |first2=Sonja |last2=Pods |title=Geometry of Heisenberg Groups |publisher=[[American Mathematical Society]] |year=2008 |isbn=978-0-8218-4495-3 }}
*{{citation|first=Brian C.|last=Hall|title=Quantum Theory for Mathematicians|series=Graduate Texts in Mathematics|volume=267 |publisher=Springer|year=2013| isbn=978-1461471158}}
*{{cite book | last = Hall | first = Brian C. | title = Lie Groups, Lie Algebras, and Representations: An Elementary Introduction|edition=second|volume=222 | year = 2015 | publisher = Springer |series=Graduate Texts in Mathematics | isbn = 978-3319134666}}
*{{cite journal |first=Roger |last=Howe |authorlink=Roger Evans Howe |title=On the role of the Heisenberg group in harmonic analysis |journal=[[Bulletin of the American Mathematical Society]] |volume=3 |issue=2 |pages=821–843 |year=1980 |url=http://www.ams.org/journals/bull/1980-03-02/S0273-0979-1980-14825-9/ |mr=578375 |doi=10.1090/s0273-0979-1980-14825-9}}
*{{cite book |first=Alexandre A. |last=Kirillov | authorlink=Alexandre Kirillov|chapter=Ch. 2: "Representations and Orbits of the Heisenberg Group |title=Lectures on the Orbit Method |publisher=American Mathematical Society |year=2004 |isbn=0-8218-3530-0 }}
*{{cite book |first=George |last=Mackey |authorlink=George Mackey| title=The theory of Unitary Group Representations |series=Chicago Lectures in Mathematics |publisher=[[University of Chicago Press]] |year=1976 |isbn=978-0226500522 }}

== External links ==

* Groupprops, The Group Properties Wiki [http://groupprops.subwiki.org/w/index.php?title=Unitriangular_matrix_group:UT(3,p)  Unitriangular matrix group UT(3,p)]

{{DEFAULTSORT:Heisenberg Group}}
[[Category:Group theory]]
[[Category:Lie groups]]
[[Category:Mathematical quantization]]
[[Category:Mathematical physics]]
[[Category:Werner Heisenberg]]</text>
      <sha1>gxegb57yrpvqpd1b5oh6e1v4jsxojxg</sha1>
    </revision>
  </page>
  <page>
    <title>Higgs field (classical)</title>
    <ns>0</ns>
    <id>38973439</id>
    <revision>
      <id>785591471</id>
      <parentid>671483921</parentid>
      <timestamp>2017-06-14T10:57:56Z</timestamp>
      <contributor>
        <username>Magic links bot</username>
        <id>30707369</id>
      </contributor>
      <minor/>
      <comment>Replace [[Help:Magic links|magic links]] with templates per [[Special:Permalink/772743896#Future of magic links|local RfC]] and [[:mw:Requests for comment/Future of magic links|MediaWiki RfC]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3349">{{See also|Higgs boson}}
[[Spontaneous symmetry breaking]], a vacuum [[Higgs field]], and its associated [[fundamental particle]] the Higgs boson are quantum phenomena. A vacuum '''Higgs field''' is responsible for spontaneous symmetry breaking the [[gauge theory|gauge symmetries]] of fundamental interactions and provides the [[Higgs mechanism]] of generating mass of elementary particles.

At the same time, [[gauge theory|classical gauge theory]] admits comprehensive geometric formulation where [[gauge theory|gauge fields]] are represented by [[connection (principal bundle)|connections]] on [[principal bundle]]s. In this framework, spontaneous symmetry breaking is characterized as a [[reduction of the structure group]] &lt;math&gt;G&lt;/math&gt; of a principal bundle &lt;math&gt;P\to X&lt;/math&gt; to its closed subgroup &lt;math&gt;H&lt;/math&gt;. By the well-known theorem, such a reduction takes place if and only if there exists a global section &lt;math&gt;h&lt;/math&gt; of the quotient bundle &lt;math&gt;P/G\to X&lt;/math&gt;. This section is treated as a '''classical Higgs field'''.

A key point is that there exists a composite bundle &lt;math&gt;P\to P/G\to X&lt;/math&gt; where &lt;math&gt;P\to P/G&lt;/math&gt; is a principal bundle with the structure group &lt;math&gt;H&lt;/math&gt;. Then matter fields, possessing an exact symmetry group &lt;math&gt;H&lt;/math&gt;, in the presence of classical Higgs fields are described by sections of some [[Connection (composite bundle)|composite bundle]] &lt;math&gt;E\to P/G\to X&lt;/math&gt;, where &lt;math&gt;E\to P/G&lt;/math&gt; is some [[associated bundle]] to &lt;math&gt;P\to P/G&lt;/math&gt;. Herewith, a [[Lagrangian system|Lagrangian]] of these matter fields is gauge invariant only if it factorizes through the vertical covariant differential of some connection on a principal bundle &lt;math&gt;P\to P/G&lt;/math&gt;, but not &lt;math&gt;P\to X&lt;/math&gt;.

An example of a classical Higgs field is a classical [[gravitational field]] identified with a [[pseudo-Riemannian manifold|pseudo-Riemannian metric]] on a [[world manifold]] &lt;math&gt; X&lt;/math&gt;. In the framework of [[gauge gravitation theory]], it is described as a global section of the quotient bundle &lt;math&gt;FX/O(1,3)\to X&lt;/math&gt; where &lt;math&gt;FX&lt;/math&gt; is a principal bundle of the tangent frames to &lt;math&gt;X&lt;/math&gt; with the structure group &lt;math&gt;GL(4,\mathbb R)&lt;/math&gt;.

== See also ==

*[[Gauge gravitation theory]]
*[[Reduction of the structure group]]
*[[Spontaneous symmetry breaking]]
== References ==

* [[Dmitri Ivanenko|D. Ivanenko]] and [[Sardanashvily|G. Sardanashvily]], The gauge treatment of gravity, Phys. Rep. '''94''' (1983) 1.
*A. Trautman, ‘’Differential Geometry for Physicists’’’ (Bibliopolis, Naples, 1984).
*L. Nikolova and V. Rizov, V. (1984). Geometrical approach to the reduction of gauge theories with spontaneous broken symmetries, Rep. Math. Phys. '''20''' (1984) 287.
* M. Keyl,  About the geometric structure of symmetry breaking, J. Math. Phys. '''32''' (1991) 1065.
* G. Giachetta, L. Mangiarotti and [[Gennadi Sardanashvily|G. Sardanashvily]], "Advanced  Classical Field Theory", World Scientific, 2009, {{ISBN|978-981-283-895-7}}.

==External links==
* [[Gennadi Sardanashvily|G. Sardanashvily]], Geometry of classical Higgs fields, Int. J. Geom. Methods Mod. Phys. '''3''' (2006) 139; [http://xxx.lanl.gov/abs/hep-th/0510168 arXiv: hep-th/0510168].


[[Category:Theoretical physics]]
[[Category:Gauge theories]]
[[Category:Symmetry]]</text>
      <sha1>7xpprolk8l2urcgz2in4b469vprncfb</sha1>
    </revision>
  </page>
  <page>
    <title>Hilbert's twenty-first problem</title>
    <ns>0</ns>
    <id>1965023</id>
    <revision>
      <id>854179996</id>
      <parentid>841492474</parentid>
      <timestamp>2018-08-09T14:03:43Z</timestamp>
      <contributor>
        <username>JCW-CleanerBot</username>
        <id>31737083</id>
      </contributor>
      <minor/>
      <comment>/* References */[[User:JCW-CleanerBot#Logic|task]], replaced: Comptes Rendus de l'Académie des Sciences. Série I. Mathématique → Comptes Rendus de l'Académie des Sciences, Série I using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8491">{{for|Riemann–Hilbert factorization problems on the complex plane|Riemann–Hilbert}}

The '''twenty-first problem''' of the 23 [[Hilbert problems]], from the celebrated list put forth in 1900 by [[David Hilbert]], concerns the existence of a certain class of linear differential equations with specified [[Mathematical singularity|singular points]] and [[monodromic group]].

==Statement==
The original problem was stated as follows (English translation from 1902):

:'''''Proof of the existence of linear differential equations having a prescribed monodromic group'''''

:''In the theory of [[linear differential equation]]s with one independent variable z, I wish to indicate an important problem one which very likely [[Riemann]] himself may have had in mind. This problem is as follows: To show that there always exists a [[linear differential equation of the Fuchsian class]], with given [[Mathematical singularity|singular points]] and [[monodromic group]]. The problem requires the production of n functions of the variable z, regular throughout the complex z-plane except at the given singular points; at these points the functions may become infinite of only finite order, and when z describes circuits about these points the functions shall undergo the prescribed [[local monodromy|linear substitution]]s. The existence of such differential equations has been shown to be probable by [[counting the constants]], but the rigorous proof has been obtained up to this time only in the particular case where the fundamental equations of the given substitutions have roots all of absolute magnitude unity. {{harvs|txt|authorlink=Ludwig Schlesinger|first=L. |last=Schlesinger|year=1895}} has given this proof, based upon [[Henri Poincaré|Poincaré]]'s theory of the [[Fuchsian zeta-function]]s. The theory of linear differential equations would evidently have a more finished appearance if the problem here sketched could be disposed of by some perfectly general method.'' [http://aleph0.clarku.edu/~djoyce/hilbert/problems.html]

==Definitions==
In fact it is more appropriate to speak not about differential equations but about linear systems of differential equations: in order to realise any monodromy by a differential equation one has to admit, in general, the presence of additional apparent singularities, i.e. singularities with trivial local monodromy. In more modern language, the (systems of) differential equations in question are those defined in the [[complex plane]], less a few points, and with a [[regular singularity]] at those. A more strict version of the problem requires these singularities to be [[Fuchsian]], i.e. poles of first order (logarithmic poles). A [[monodromy group]] is prescribed, by means of a finite-dimensional [[complex representation]] of the [[fundamental group]] of the complement in the [[Riemann sphere]] of those points, plus the [[point at infinity]], up to equivalence. The fundamental group is actually a [[free group]], on 'circuits' going once round each missing point, starting and ending at a given [[base point]]. The question is whether the mapping from these ''Fuchsian'' equations to classes of representations is [[surjective]].

==History==
This problem is more commonly called the '''[[Riemann–Hilbert problem]]'''. There is now a modern ([[D-module]] and [[derived category]]) version, the ''''[[Riemann–Hilbert correspondence]]'''' in all dimensions. The history of proofs involving a single complex variable is complicated. [[Josip Plemelj]] published a solution in 1908. This work was for a long time accepted as a definitive solution; there was work of [[G. D. Birkhoff]] in 1913 also, but the whole area, including work of [[Ludwig Schlesinger]] on [[isomonodromic deformations]] that would much later be revived in connection with [[soliton theory]], went out of fashion. {{harvtxt|Plemelj|1964}} wrote a  monograph  summing up his work. A few years later the Soviet mathematician [[Yuliy S. Il'yashenko]] and others started raising doubts about Plemelj's work. In fact, Plemelj correctly proves that any monodromy group can be realised by a regular linear system which is Fuchsian at all but one of the singular points. Plemelj's claim that the system can be made Fuchsian at the last point as well is wrong. (Il'yashenko has shown that if one of the monodromy operators is diagonalizable, then Plemelj's claim is true.)
 
Indeed {{harvs|txt|authorlink=Andrei Bolibrukh|first=Andrey A. |last=Bolibrukh|year=1990}} found a counterexample to Plemelj's statement.  
This is commonly viewed as providing a counterexample to the precise question Hilbert had in mind;
Bolibrukh showed that for a given pole configuration certain monodromy groups can be realised by regular, but not by Fuchsian systems. (In 1990 he published the thorough study of the case of regular systems of size 3 exhibiting all situations when such counterexamples exists. In 1978 Dekkers had shown that for systems of size 2 Plemelj's claim is true. {{harvs|txt|authorlink=Andrei Bolibrukh|first=Andrey A. |last=Bolibrukh|year=1992}} and independently {{harvs|txt|authorlink=Vladimir Petrov Kostov|first=Vladimir |last=Kostov|year=1992}} showed that for any size, an irreducible monodromy group can be realised by a Fuchsian system. The codimension of the variety of monodromy groups of regular systems of size &lt;math&gt;n&lt;/math&gt; with &lt;math&gt;p+1&lt;/math&gt; poles which cannot be realised by Fuchsian systems equals &lt;math&gt;2(n-1)p&lt;/math&gt; ({{harvs|txt|authorlink=Vladimir Petrov Kostov|first=Vladimir |last=Kostov|year=1992}}).) Parallel to this the Grothendieck school of algebraic geometry had become interested in questions of 'integrable connections on algebraic varieties', generalising the theory of linear differential equations on [[Riemann surface]]s. [[Pierre Deligne]] proved a precise  Riemann–Hilbert correspondence in this general context (a major point being to say what 'Fuchsian' means). With work by [[Helmut Röhrl]], the case in one complex dimension was again covered.

==References==
*{{Citation | last1=Anosov | first1=D. V. | last2=Bolibruch | first2=A. A. | authorlink2=Andrei Bolibrukh | title=The Riemann-Hilbert problem | publisher=Friedr. Vieweg &amp; Sohn | location=Braunschweig | series=Aspects of Mathematics, E22 | isbn=978-3-528-06496-9 | mr=1276272 | year=1994 | doi=10.1007/978-3-322-92909-9}}
*{{Citation | last1=Bolibrukh | first1=A. A. | title=The Riemann-Hilbert problem | doi=10.1070/RM1990v045n02ABEH002350 | mr=1069347 | year=1990 | journal=Akademiya Nauk SSSR i Moskovskoe Matematicheskoe Obshchestvo. Uspekhi Matematicheskikh Nauk | issn=0042-1316 | volume=45 | issue=2 | pages=3–47|language=Russian}}
*{{Citation | last1=Plemelj | first1=Josip | editor1-last=Radok. | editor1-first=J. R. M. | title=Problems in the sense of Riemann and Klein | url=https://books.google.com/books?id=f0urAAAAIAAJ | publisher=Interscience Publishers John Wiley &amp; Sons Inc.|location= New York-London-Sydney | series= Interscience Tracts in Pure and Applied Mathematics | mr=0174815 | year=1964 | volume=16}}
*{{Citation | last1=Bolibrukh | first1=A.A. | title=Sufficient conditions for the positive solvability of the Riemann-Hilbert problem | journal=Matematicheskie Zametki |pages=9–19, 156 (translation in ''Math. Notes'' '''51''' (1–2) (1992) pp. 110–117) | language=Russian | year=1992|mr=1165460 | doi=10.1007/BF02102113}}
*{{Citation | last1=Kostov | first1=Vladimir Petrov | title=Fuchsian linear systems on &lt;math&gt;CP^1&lt;/math&gt; and the Riemann-Hilbert problem | journal=Comptes Rendus de l'Académie des Sciences, Série I | year=1992 | volume=315 | issue=2 | pages=143–148 | mr=1197226}}
*{{Citation | last1=Schlesinger | first1=L. | title=Handbuch der Theorie der linearen Differentialgleichungen vol. 2, part 2, No. 366 | year=1895}}
*{{Citation | last1=Katz | first1=N.M. | authorlink=Nick Katz | title=An Overview of Deligne's work on Hilbert's Twenty-First Problem | journal=Proceedings of Symposia in Pure Mathematics | volume=28 | year=1976}}

==External links==
* [https://web.archive.org/web/20050305104624/http://www.gang.umass.edu/~kilian/mathesis/mathesis.html On the Riemann–Hilbert Problem] ([https://web.archive.org/web/20050305104624/http://www.gang.umass.edu/~kilian/mathesis/mathesis.html archive.org copy] [http://euclid.ucc.ie/pages/staff/mk/mathesis.pdf])

{{Hilbert's problems}}

[[Category:Hilbert's problems|#21]]
[[Category:Ordinary differential equations]]
[[Category:Algebraic curves]]</text>
      <sha1>64ubv065btvthcnsnf5k61wyeetrlsf</sha1>
    </revision>
  </page>
  <page>
    <title>Individual participant data</title>
    <ns>0</ns>
    <id>51262716</id>
    <revision>
      <id>821051293</id>
      <parentid>807349146</parentid>
      <timestamp>2018-01-18T03:43:22Z</timestamp>
      <contributor>
        <username>Volunteer1234</username>
        <id>30845620</id>
      </contributor>
      <comment>redundant</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4677">'''Individual participant data''' (also known as '''individual patient data''', often abbreviated '''IPD''') is raw data from individual participants, and is often used in the context of [[meta-analysis]].

The [[International Committee of Medical Journal Editors]] (ICMJE) has stated that sharing of deidentified individual participant data is an ethical obligation.&lt;ref name="TaichmanBackus2016"&gt;{{cite journal|last1=Taichman|first1=Darren B.|last2=Backus|first2=Joyce|last3=Baethge|first3=Christopher|last4=Bauchner|first4=Howard|last5=de Leeuw|first5=Peter W.|last6=Drazen|first6=Jeffrey M.|last7=Fletcher|first7=John|last8=Frizelle|first8=Frank A.|last9=Groves|first9=Trish|year=2016|title=Sharing Clinical Trial Data: A Proposal From the International Committee of Medical Journal Editors|journal=Annals of Internal Medicine|volume=164|issue=7|pages=505|doi=10.7326/M15-2928|issn=0003-4819|last10=Haileamlak|first10=Abraham|last11=James|first11=Astrid|last12=Laine|first12=Christine|last13=Peiperl|first13=Larry|last14=Pinborg|first14=Anja|last15=Sahni|first15=Peush|last16=Wu|first16=Sinan}}&lt;/ref&gt;

== IPD meta-analysis ==
In an IPD meta-analysis, patient-level data from multiple studies or settings are combined to address a certain research question. IPD meta-analyses tend to be common for large-scale and international projects, and they are less limited than AD meta-analyses in terms of the availability and quality of data they can use.&lt;ref&gt;{{cite journal|last1=Tierney|first1=Jayne F.|last2=Pignon|first2=Jean-Pierre|last3=Gueffyier|first3=Francois|last4=Clarke|first4=Mike|last5=Askie|first5=Lisa|last6=Vale|first6=Claire L.|last7=Burdett|first7=Sarah|date=November 2015|title=How individual participant data meta-analyses have influenced trial design, conduct, and analysis|journal=Journal of Clinical Epidemiology|volume=68|issue=11|pages=1325–1335|doi=10.1016/j.jclinepi.2015.05.024}}&lt;/ref&gt; Due to the high level of precision and consistency this approach allows for (which in turn makes it easier for researchers to minimize [[heterogeneity]]), it is considered the gold standard of evidence synthesis.&lt;ref&gt;{{cite journal|last1=Thomas|first1=Doneal|last2=Radji|first2=Sanyath|last3=Benedetti|first3=Andrea|date=19 June 2014|title=Systematic review of methods for individual patient data meta- analysis with binary outcomes|journal=BMC Medical Research Methodology|volume=14|issue=1|doi=10.1186/1471-2288-14-79}}&lt;/ref&gt;  

Common aims for a IPD meta-analysis are
* to evaluate the safety or efficacy of medical interventions &lt;ref&gt;{{Cite journal|last=Tierney|first=Jayne F.|last2=Vale|first2=Claire|last3=Riley|first3=Richard|last4=Smith|first4=Catrin Tudur|last5=Stewart|first5=Lesley|last6=Clarke|first6=Mike|last7=Rovers|first7=Maroeska|date=2015-07-21|title=Individual Participant Data (IPD) Meta-analyses of Randomised Controlled Trials: Guidance on Their Use|url=http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001855|journal=PLOS Medicine|volume=12|issue=7|pages=e1001855|doi=10.1371/journal.pmed.1001855|issn=1549-1676}}&lt;/ref&gt;
* to identify modifiers of treatment effect
* to evaluate the accuracy of diagnostic tests
* to evaluate the association of prognostic markers &lt;ref name=":0"&gt;{{Cite journal|last=Debray|first=Thomas P. A.|last2=Riley|first2=Richard D.|last3=Rovers|first3=Maroeska M.|last4=Reitsma|first4=Johannes B.|last5=Moons|first5=Karel G. M.|last6=Group|first6=Cochrane IPD Meta-analysis Methods|date=2015-10-13|title=Individual Participant Data (IPD) Meta-analyses of Diagnostic and Prognostic Modeling Studies: Guidance on Their Use|url=http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.1001886|journal=PLOS Medicine|volume=12|issue=10|pages=e1001886|doi=10.1371/journal.pmed.1001886|issn=1549-1676}}&lt;/ref&gt;
* to develop multivariable prediction models (rules) &lt;ref name=":0" /&gt;
* to evaluate the predictive performance of prognostic models &lt;ref name=":0" /&gt;
Over the past few decades, meta-analyses conducted with IPD (also known as IPD meta-analyses) have become increasingly popular.&lt;ref&gt;{{cite journal|last1=Riley|first1=R. D.|last2=Lambert|first2=P. C.|last3=Abo-Zaid|first3=G.|date=5 February 2010|title=Meta-analysis of individual participant data: rationale, conduct, and reporting|journal=BMJ|volume=340|issue=feb05 1|pages=c221–c221|doi=10.1136/bmj.c221}}&lt;/ref&gt; 

==References==
{{Reflist}}

==External links==
*[http://methods.cochrane.org/news/individual-participant-data-meta-analysis-everything-you-always-wanted-know Individual participant data meta-analysis information] at the [[Cochrane (organisation)|Cochrane]] website

[[Category:Meta-analysis]]


{{Statistics-stub}}</text>
      <sha1>n1m42bvyjt8ioq6t2z2mgwe4t5hbk6h</sha1>
    </revision>
  </page>
  <page>
    <title>Invariant factorization of LPDOs</title>
    <ns>0</ns>
    <id>17143362</id>
    <revision>
      <id>822424201</id>
      <parentid>819429490</parentid>
      <timestamp>2018-01-26T08:44:25Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v481)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15616">{{context|date=January 2018}}
The '''factorization of a linear partial differential operator''' (LPDO) is an important issue in the theory of integrability, due to the Laplace-Darboux transformations,&lt;ref&gt;Weiss (1986)&lt;/ref&gt; which allow construction of integrable LPDEs.  [[Laplace]] solved the factorization problem for a '''bivariate hyperbolic operator of the second order''' (see [[Hyperbolic partial differential equation]]), constructing two Laplace invariants. Each [[Laplace invariant]] is an explicit polynomial condition of factorization; coefficients of this polynomial are explicit functions of the coefficients of the initial LPDO.  The polynomial conditions of factorization are called '''invariants''' because they have the same form for equivalent (i.e. self-adjoint) operators.

'''Beals-Kartashova-factorization''' (also called BK-factorization) is a constructive procedure to factorize '''a bivariate operator of the arbitrary order and arbitrary form'''. Correspondingly, the factorization conditions in this case also have polynomial form, are invariants and '''coincide with Laplace invariants''' for bivariate hyperbolic operators of the second order. The factorization procedure is purely algebraic, the number of possible factorizations depending on the number of simple roots of the [[Characteristic polynomial]] (also called symbol) of the initial LPDO and reduced LPDOs appearing at each factorization step. Below the factorization procedure is described for a bivariate operator of arbitrary form, of order 2 and 3. Explicit factorization formulas for an operator of the order &lt;math&gt; n &lt;/math&gt; can be found in&lt;ref&gt;R. Beals, E. Kartashova. Constructively factoring linear partial differential operators in two variables. [http://www.springerlink.com/content/yx664142514k0217/ Theor. Math. Phys. '''145'''(2), pp. 1510-1523 (2005)] &lt;/ref&gt; General invariants are defined in&lt;ref&gt; 
E. Kartashova. A Hierarchy of Generalized Invariants for Linear Partial Differential Operators. [http://www.springerlink.com/content/lp81238030114354/ Theor. Math. Phys. '''147'''(3), pp. 839-846 (2006)] &lt;/ref&gt; and invariant formulation of the Beals-Kartashova factorization is given in&lt;ref&gt;  E. Kartashova, O. Rudenko.  Invariant Form of BK-factorization and its Applications. Proc. GIFT-2006, pp.225-241, Eds.: J. Calmet, R. W. Tucker,  Karlsruhe University Press  (2006); [https://arxiv.org/abs/math-ph/0607040/ arXiv]&lt;/ref&gt;

==Beals-Kartashova Factorization==
===Operator of order 2===

Consider an operator
:&lt;math&gt; 
\mathcal{A}_2 = a_{20}\partial_x^2 + a_{11}\partial_x\partial_y + a_{02}\partial_y^2+a_{10}\partial_x+a_{01}\partial_y+a_{00}.
&lt;/math&gt;

with smooth coefficients and look for a factorization
:&lt;math&gt; 
\mathcal{A}_2=(p_1\partial_x+p_2\partial_y+p_3)(p_4\partial_x+p_5\partial_y+p_6).
&lt;/math&gt;

Let us write down the equations on &lt;math&gt; p_i&lt;/math&gt; explicitly, keeping in
mind the rule of '''left''' composition, i.e. that 
:&lt;math&gt;  \partial_x (\alpha
\partial_y) = \partial_x (\alpha) \partial_y +
\alpha \partial_{xy}.&lt;/math&gt; 

Then in all cases

:&lt;math&gt; a_{20} =  p_1p_4, &lt;/math&gt;

:&lt;math&gt;  a_{11} =  p_2p_4+p_1p_5, &lt;/math&gt;

:&lt;math&gt;  a_{02} =  p_2p_5, &lt;/math&gt;

:&lt;math&gt;  a_{10} =  \mathcal{L}(p_4) + p_3p_4+p_1p_6, &lt;/math&gt;

:&lt;math&gt;  a_{01} =  \mathcal{L}(p_5) + p_3p_5+p_2p_6, &lt;/math&gt;

:&lt;math&gt;  a_{00} =  \mathcal{L}(p_6) + p_3p_6, &lt;/math&gt;

where the notation &lt;math&gt; \mathcal{L} = p_1 \partial_x + p_2 \partial_y &lt;/math&gt; is used.

Without loss of generality, &lt;math&gt; 
a_{20}\ne 0,
&lt;/math&gt; i.e. &lt;math&gt;  p_1\ne 0, &lt;/math&gt;  and  it can be taken as 1, &lt;math&gt; 
p_1 = 1. &lt;/math&gt;  Now solution of the system of 6 equations on the variables 
:&lt;math&gt; p_2, &lt;/math&gt; &lt;math&gt;...  &lt;/math&gt; &lt;math&gt; p_6 &lt;/math&gt;
can be found in '''three steps'''. 

'''At the first step''', the roots of a '''''quadratic polynomial''''' have to be found. 

'''At the second step''', a linear system of '''''two algebraic equations''''' has to be solved.

'''At the third step''', '''''one algebraic condition'''''   has to be checked.

'''Step 1.'''
Variables  
:&lt;math&gt; p_2,&lt;/math&gt;   &lt;math&gt;  p_4, &lt;/math&gt;  &lt;math&gt; p_5 
&lt;/math&gt; 
can be found from the first three equations, 

:&lt;math&gt; a_{20} =  p_1p_4, &lt;/math&gt;

:&lt;math&gt;  a_{11} =  p_2p_4+p_1p_5, &lt;/math&gt;

:&lt;math&gt;  a_{02} =  p_2p_5. &lt;/math&gt;

The (possible) solutions are then the functions of the roots of a quadratic polynomial:

:&lt;math&gt; 
\mathcal{P}_2(-p_2) =  a_{20}(- p_2)^2 +a_{11}(- p_2) +a_{02} = 0
&lt;/math&gt;

Let  &lt;math&gt; \omega &lt;/math&gt; be a root of the polynomial &lt;math&gt; 
\mathcal{P}_2,
&lt;/math&gt;
then

:&lt;math&gt;  p_1=1, &lt;/math&gt;
:&lt;math&gt;  p_2=-\omega, &lt;/math&gt;
:&lt;math&gt;  p_4=a_{20},&lt;/math&gt;
:&lt;math&gt;  p_5=a_{20} \omega +a_{11},&lt;/math&gt;

'''Step 2.'''
Substitution of the results obtained at the first step, into the next two equations

:&lt;math&gt;  a_{10} =  \mathcal{L}(p_4) + p_3p_4+p_1p_6, &lt;/math&gt;

:&lt;math&gt;  a_{01} =  \mathcal{L}(p_5) + p_3p_5+p_2p_6, &lt;/math&gt;

yields linear system of two algebraic equations:

:&lt;math&gt;  a_{10} =  \mathcal{L} a_{20} +p_3 a_{20} +p_6, &lt;/math&gt;
:&lt;math&gt;  a_{01} =  \mathcal{L}(a_{11}+a_{20} \omega)+p_3( a_{11} + a_{20}\omega)-
  \omega p_6., &lt;/math&gt;

'''In particularly''', if the root &lt;math&gt;\omega&lt;/math&gt; is simple,
i.e.

:&lt;math&gt; \mathcal{P}_2'(\omega)=2a_{20}\omega+a_{11}\ne 0,&lt;/math&gt; then these
equations have the unique solution:

:&lt;math&gt;   p_3 =  \frac{\omega a_{10}+a_{01} -\omega\mathcal{L}a_{20}- \mathcal{L}(a_{20} \omega+a_{11})}
{2a_{20}\omega+a_{11}},&lt;/math&gt;
:&lt;math&gt;   p_6 =\frac{ (a_{20}\omega+a_{11})(a_{10}-\mathcal{L}a_{20})-a_{20}(a_{01}
 -\mathcal{L}(a_{20}\omega+a_{11}))}{2a_{20}\omega+a_{11}}.&lt;/math&gt; 

At this step, for each 
root of the polynomial &lt;math&gt; \mathcal{P}_2 &lt;/math&gt; a corresponding set of  coefficients  &lt;math&gt; p_j &lt;/math&gt; is computed. 

'''Step 3.'''
Check factorization condition (which is the last of the initial 6 equations)

:&lt;math&gt; a_{00} =  \mathcal{L}(p_6)+p_3p_6, &lt;/math&gt;
written in the known variables &lt;math&gt; p_j &lt;/math&gt; and &lt;math&gt; \omega &lt;/math&gt;):

:&lt;math&gt; 
a_{00} = \mathcal{L} \left\{
 \frac{\omega a_{10}+a_{01} - \mathcal{L}(2a_{20} \omega+a_{11})}
{2a_{20}\omega+a_{11}}\right\}+ \frac{\omega a_{10}+a_{01} -
\mathcal{L}(2a_{20} \omega+a_{11})}
{2a_{20}\omega+a_{11}}\times
\frac{ a_{20}(a_{01}-\mathcal{L}(a_{20}\omega+a_{11}))+
(a_{20}\omega+a_{11})(a_{10}-\mathcal{L}a_{20})}{2a_{20}\omega+a_{11}}
&lt;/math&gt;

If

:&lt;math&gt; 
l_2= a_{00} - \mathcal{L} \left\{
 \frac{\omega a_{10}+a_{01} - \mathcal{L}(2a_{20} \omega+a_{11})}
{2a_{20}\omega+a_{11}}\right\}+ \frac{\omega a_{10}+a_{01} -
\mathcal{L}(2a_{20} \omega+a_{11})}
{2a_{20}\omega+a_{11}}\times
\frac{ a_{20}(a_{01}-\mathcal{L}(a_{20}\omega+a_{11}))+
(a_{20}\omega+a_{11})(a_{10}-\mathcal{L}a_{20})}{2a_{20}\omega+a_{11}} =0,
&lt;/math&gt;

the operator &lt;math&gt; \mathcal{A}_2&lt;/math&gt; is factorizable and explicit form for the factorization coefficients &lt;math&gt; p_j&lt;/math&gt;  is given above.

===Operator of order 3===
Consider an operator

:&lt;math&gt;
\mathcal{A}_3=\sum_{j+k\le3}a_{jk}\partial_x^j\partial_y^k =a_{30}\partial_x^3 +
a_{21}\partial_x^2 \partial_y + a_{12}\partial_x \partial_y^2 +a_{03}\partial_y^3 +
a_{20}\partial_x^2+a_{11}\partial_x\partial_y+a_{02}\partial_y^2+a_{10}\partial_x+a_{01}\partial_y+a_{00}.
&lt;/math&gt;

with smooth coefficients and look for a factorization

:&lt;math&gt;
\mathcal{A}_3=(p_1\partial_x+p_2\partial_y+p_3)(p_4 \partial_x^2 +p_5 \partial_x\partial_y  + p_6 \partial_y^2 + p_7
\partial_x + p_8 \partial_y + p_9).
&lt;/math&gt;

Similar to the case of the operator &lt;math&gt; \mathcal{A}_2, &lt;/math&gt;  the conditions of factorization are described by the following system:
:&lt;math&gt;  a_{30} =  p_1p_4,&lt;/math&gt;

:&lt;math&gt;  a_{21} =  p_2p_4+p_1p_5,&lt;/math&gt;

:&lt;math&gt;  a_{12} =  p_2p_5+p_1p_6,&lt;/math&gt;

:&lt;math&gt;  a_{03} =  p_2p_6,&lt;/math&gt;

:&lt;math&gt;  a_{20} =  \mathcal{L}(p_4)+p_3p_4+p_1p_7,&lt;/math&gt;

:&lt;math&gt;  a_{11} =  \mathcal{L}(p_5)+p_3p_5+p_2p_7+p_1p_8,&lt;/math&gt;

:&lt;math&gt;  a_{02} =  \mathcal{L}(p_6)+p_3p_6+p_2p_8,&lt;/math&gt;

:&lt;math&gt;  a_{10} =  \mathcal{L}(p_7)+p_3p_7+p_1p_9,&lt;/math&gt;

:&lt;math&gt;  a_{01} =  \mathcal{L}(p_8)+p_3p_8+p_2p_9,&lt;/math&gt;

:&lt;math&gt;  a_{00} =  \mathcal{L}(p_9)+p_3p_9,
&lt;/math&gt;
with &lt;math&gt;\mathcal{L} = p_1 \partial_x + p_2 \partial_y,&lt;/math&gt; and again &lt;math&gt; 
a_{30}\ne 0,
&lt;/math&gt; i.e. &lt;math&gt;  p_1=1, &lt;/math&gt;  and three-step procedure yields:

'''At the first step''', the roots of a '''''cubic polynomial''''' 

:&lt;math&gt;  \mathcal{P}_3(-p_2):=  a_{30}(-p_2)^3 +a_{21}(- p_2)^2 +
a_{12}(-p_2)+a_{03}=0. 
&lt;/math&gt;

have to be found. Again &lt;math&gt; \omega  &lt;/math&gt; denotes a root and first four coefficients are 

:&lt;math&gt; p_1=1,  &lt;/math&gt;
:&lt;math&gt;p_2=-\omega, &lt;/math&gt;
:&lt;math&gt;p_4=a_{30}, &lt;/math&gt;
:&lt;math&gt;p_5=a_{30} \omega+a_{21}, &lt;/math&gt;
:&lt;math&gt;p_6=a_{30}\omega^2+a_{21}\omega+a_{12}.
&lt;/math&gt;

'''At the second step''', a linear system of '''''three algebraic equations''''' has to be solved:

:&lt;math&gt;   a_{20}-\mathcal{L} a_{30} = p_3 a_{30} +p_7,&lt;/math&gt;
:&lt;math&gt;   a_{11}-\mathcal{L}(a_{30} \omega + a_{21}) = p_3(a_{30}\omega+a_{21})- \omega p_7+p_8,&lt;/math&gt;
:&lt;math&gt;   a_{02}-\mathcal{L}(a_{30}\omega^2+a_{21}\omega+a_{12})= p_3 (a_{30}\omega^2+a_{21}\omega+a_{12})-\omega p_8.&lt;/math&gt;

'''At the third step''', '''''two algebraic conditions'''''   have to be checked.

===Operator of order &lt;math&gt;n&lt;/math&gt;===
==Invariant Formulation==

'''Definition''' The operators &lt;math&gt; \mathcal{A} &lt;/math&gt;, &lt;math&gt; \tilde{\mathcal{A}}  &lt;/math&gt; are called
equivalent if there is a gauge transformation that takes one to the
other:
:&lt;math&gt;
\tilde{\mathcal{A}} g= e^{-\varphi}\mathcal{A} (e^{\varphi}g).
&lt;/math&gt;
BK-factorization is then pure algebraic procedure which allows to
construct explicitly a factorization of an arbitrary order LPDO &lt;math&gt; \tilde{\mathcal{A}}  &lt;/math&gt;
in the form
:&lt;math&gt;
\mathcal{A}=\sum_{j+k\le n}a_{jk}\partial_x^j\partial_y^k=\mathcal{L}\circ
\sum_{j+k\le (n-1)}p_{jk}\partial_x^j\partial_y^k
&lt;/math&gt;
with first-order operator &lt;math&gt; \mathcal{L}=\partial_x-\omega\partial_y+p&lt;/math&gt; where &lt;math&gt; \omega&lt;/math&gt; is '''an arbitrary  simple root'''  of the characteristic polynomial 
:&lt;math&gt;
\mathcal{P}(t)=\sum^n_{k=0}a_{n-k,k}t^{n-k}, \quad
\mathcal{P}(\omega)=0.&lt;/math&gt;
Factorization is possible then for each simple root &lt;math&gt;\tilde{\omega}&lt;/math&gt;   '''iff'''

for &lt;math&gt;n=2 \ \ \rightarrow l_2=0,&lt;/math&gt;

for &lt;math&gt;n=3 \ \ \rightarrow l_3=0, l_{31}=0,&lt;/math&gt;

for &lt;math&gt;n=4 \ \ \rightarrow l_4=0,  l_{41}=0, l_{42}=0,&lt;/math&gt;

and so on. All functions &lt;math&gt;l_2,  l_3,  l_{31},  l_4, 
l_{41}, \ \ l_{42}, ...&lt;/math&gt; are known functions, for instance, 

:&lt;math&gt;  l_2= a_{00} - \mathcal{L}(p_6)+p_3p_6, &lt;/math&gt;

:&lt;math&gt;  l_3= a_{00} - \mathcal{L}(p_9)+p_3p_9, &lt;/math&gt;

:&lt;math&gt;  l_{31} = a_{01} -  \mathcal{L}(p_8)+p_3p_8+p_2p_9,&lt;/math&gt;

and so on.

'''Theorem'''  All functions 
:&lt;math&gt;l_2= a_{00} - \mathcal{L}(p_6)+p_3p_6,  
l_3= a_{00} - \mathcal{L}(p_9)+p_3p_9, 
l_{31}, ....&lt;/math&gt; 
are '''invariants''' under gauge transformations.

'''Definition''' Invariants &lt;math&gt;l_2= a_{00} - \mathcal{L}(p_6)+p_3p_6,  
l_3= a_{00} - \mathcal{L}(p_9)+p_3p_9, 
l_{31}, .... .&lt;/math&gt; are
called '''generalized invariants''' of a bivariate operator of arbitrary
order.

In particular case of the bivariate hyperbolic operator  its generalized
invariants '''coincide with Laplace invariants''' (see [[Laplace invariant]]).

'''Corollary''' If an operator &lt;math&gt; \tilde{\mathcal{A}}  &lt;/math&gt; is factorizable, then all
operators  equivalent to it, are also factorizable.

Equivalent operators are easy to compute:
:&lt;math&gt; e^{-\varphi} \partial_x e^{\varphi}= \partial_x+\varphi_x, \quad e^{-\varphi}\partial_y e^{\varphi}=
\partial_y+\varphi_y,&lt;/math&gt;

:&lt;math&gt; e^{-\varphi} \partial_x \partial_y e^{\varphi}= e^{-\varphi} \partial_x e^{\varphi}
e^{-\varphi} \partial_y e^{\varphi}=(\partial_x+\varphi_x) \circ (\partial_y+\varphi_y)&lt;/math&gt;
and so on. Some example are given below:

:&lt;math&gt; A_1=\partial_x \partial_y + x\partial_x + 1= \partial_x(\partial_y+x), \quad
l_2(A_1)=1-1-0=0;&lt;/math&gt;

:&lt;math&gt;A_2=\partial_x \partial_y + x\partial_x + \partial_y +x + 1, \quad
A_2=e^{-x}A_1e^{x};\quad l_2(A_2)=(x+1)-1-x=0;&lt;/math&gt;

:&lt;math&gt;A_3=\partial_x \partial_y + 2x\partial_x + (y+1)\partial_y +2(xy +x+1), \quad
A_3=e^{-xy}A_2e^{xy}; \quad l_2(A_3)=2(x+1+xy)-2-2x(y+1)=0;&lt;/math&gt;

:&lt;math&gt;A_4=\partial_x \partial_y +x\partial_x + (\cos x +1) \partial_y + x \cos x +x +1, \quad
A_4=e^{-\sin x}A_2e^{\sin x}; \quad l_2(A_4)=0.&lt;/math&gt;

==Transpose==

Factorization of an operator is the first step on the way of solving corresponding equation. But for solution we need '''right''' factors and BK-factorization constructs  '''left''' factors which are easy to construct. On the other hand, the existence of a certain right factor of a LPDO is equivalent to the existence of a corresponding left factor of the '''transpose''' of that operator.

'''Definition'''
The transpose &lt;math&gt; \mathcal{A}^t&lt;/math&gt; of an operator
&lt;math&gt;
\mathcal{A}=\sum a_{\alpha}\partial^{\alpha},\qquad \partial^{\alpha}=\partial_1^{\alpha_1}\cdots\partial_n^{\alpha_n}.
&lt;/math&gt;
is defined as
&lt;math&gt;
\mathcal{A}^t u = \sum (-1)^{|\alpha|}\partial^\alpha(a_\alpha u).
&lt;/math&gt;
and the identity
&lt;math&gt;
\partial^\gamma(uv)=\sum \binom\gamma\alpha \partial^\alpha u,\partial^{\gamma-\alpha}v
&lt;/math&gt;
implies that
&lt;math&gt;
\mathcal{A}^t=\sum (-1)^{|\alpha+\beta|}\binom{\alpha+\beta}\alpha (\partial^\beta a_{\alpha+\beta})\partial^\alpha.
&lt;/math&gt;

Now the coefficients are

&lt;math&gt; \mathcal{A}^t=\sum \tilde{a}_{\alpha} \partial^{\alpha},&lt;/math&gt;
&lt;math&gt; \tilde{a}_{\alpha}=\sum (-1)^{|\alpha+\beta|}
\binom{\alpha+\beta}{\alpha}\partial^\beta(a_{\alpha+\beta}).
&lt;/math&gt;

with a standard convention for binomial coefficients in several
variables (see [[Binomial coefficient]]), e.g. in two variables
:&lt;math&gt;
\binom\alpha\beta=\binom{(\alpha_1,\alpha_2)}{(\beta_1,\beta_2)}=\binom{\alpha_1}{\beta_1}\,\binom{\alpha_2}{\beta_2}.
&lt;/math&gt;
In particular, for the operator &lt;math&gt; \mathcal{A}_2 &lt;/math&gt; the coefficients are
&lt;math&gt; \tilde{a}_{jk}=a_{jk},\quad j+k=2; \tilde{a}_{10}=-a_{10}+2\partial_x a_{20}+\partial_y
a_{11}, \tilde{a}_{01}=-a_{01}+\partial_x a_{11}+2\partial_y a_{02},&lt;/math&gt;
:&lt;math&gt;
\tilde{a}_{00}=a_{00}-\partial_x a_{10}-\partial_y a_{01}+\partial_x^2 a_{20}+\partial_x \partial_x
a_{11}+\partial_y^2 a_{02}.
&lt;/math&gt;
For instance,  the operator 
:&lt;math&gt; \partial_{xx}-\partial_{yy}+y\partial_x+x\partial_y+\frac{1}{4}(y^2-x^2)-1 &lt;/math&gt;
is factorizable as
:&lt;math&gt; \big[\partial_x+\partial_y+\tfrac12(y-x)\big]\,\big[...\big]&lt;/math&gt;
and its transpose &lt;math&gt; \mathcal{A}_1^t &lt;/math&gt;  is factorizable then as
&lt;math&gt; \big[...\big]\,\big[\partial_x-\partial_y+\tfrac12(y+x)\big].&lt;/math&gt;

==See also==
* [[Partial derivative]]
* [[Invariant (mathematics)]]
* [[Invariant theory]]
* [[Characteristic polynomial]]

== Notes ==
&lt;references/&gt;

== References ==
* J. Weiss. Bäcklund transformation and the Painlevé property. [http://www2.appmath.com:8080/site/few/few.html] J. Math. Phys. '''27''', 1293-1305 (1986).
* R. Beals, E. Kartashova. Constructively factoring linear partial differential operators in two variables. [http://www.springerlink.com/content/yx664142514k0217/ Theor. Math. Phys. '''145'''(2), pp. 1510-1523 (2005)]  
*   E. Kartashova. A Hierarchy of Generalized Invariants for Linear Partial Differential Operators. [http://www.springerlink.com/content/lp81238030114354/ Theor. Math. Phys. '''147'''(3), pp. 839-846 (2006)] 
*  E. Kartashova, O. Rudenko.  Invariant Form of BK-factorization and its Applications. Proc. GIFT-2006, pp.225-241, Eds.: J. Calmet, R. W. Tucker,  Karlsruhe University Press  (2006); [https://arxiv.org/abs/math-ph/0607040/ arXiv]

[[Category:Multivariable calculus]]
[[Category:Differential operators]]
[[Category:Partial differential equations]]</text>
      <sha1>cttl7vbkb6xf0o15xp6l18fuhc415y4</sha1>
    </revision>
  </page>
  <page>
    <title>Inversive distance</title>
    <ns>0</ns>
    <id>18849148</id>
    <revision>
      <id>842557897</id>
      <parentid>841539979</parentid>
      <timestamp>2018-05-23T06:22:49Z</timestamp>
      <contributor>
        <username>OAbot</username>
        <id>28481209</id>
      </contributor>
      <minor/>
      <comment>[[Wikipedia:OABOT|Open access bot]]: add arxiv identifier to citation with #oabot.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7928">In [[inversive geometry]], the '''inversive distance''' is a way of measuring the "[[distance]]" between two [[circle]]s, regardless of whether the circles cross each other, are tangent to each other, or are disjoint from each other.&lt;ref name="bh"/&gt;

==Properties==
The inversive distance remains unchanged if the circles are [[inversive geometry|inverted]], or transformed by a [[Möbius transformation]].&lt;ref name="bh"/&gt;&lt;ref name="ampa"/&gt;&lt;ref name="bq"/&gt; One pair of circles can be transformed to another pair by a Möbius transformation if and only if both pairs have the same inversive distance.&lt;ref name="bh"/&gt;

An analogue of the [[Beckman–Quarles theorem]] holds true for the inversive distance: if a [[bijection]] of the set of circles in the inversive plane preserves the inversive distance between pairs of circles at some chosen fixed distance &lt;math&gt;\delta&lt;/math&gt;, then it must be a Möbius transformation that preserves all inversive distances.&lt;ref name="bq"&gt;{{citation
 | last = Lester | first = J. A.
 | doi = 10.4153/CMB-1991-079-6
 | issue = 4
 | journal = Canadian Mathematical Bulletin
 | mr = 1136651
 | pages = 492–498
 | title = A Beckman-Quarles type theorem for Coxeter's inversive distance
 | volume = 34
 | year = 1991}}.&lt;/ref&gt;

==Distance formula==
For two circles in the [[Euclidean plane]] with radii &lt;math&gt;r&lt;/math&gt; and &lt;math&gt;R&lt;/math&gt;, and distance &lt;math&gt;d&lt;/math&gt; between their centers, the inversive distance can be defined
by the formula&lt;ref name="bh"&gt;{{citation
 | last1 = Bowers | first1 = Philip L.
 | last2 = Hurdal | first2 = Monica K.
 | editor1-last = Hege | editor1-first = Hans-Christian
 | editor2-last = Polthier | editor2-first = Konrad
 | contribution = Planar conformal mappings of piecewise flat surfaces
 | doi = 10.1007/978-3-662-05105-4_1
 | mr = 2046999
 | pages = 3–34
 | publisher = Springer
 | series = Mathematics and Visualization
 | title = Visualization and Mathematics III
 | year = 2003}}.&lt;/ref&gt;
:&lt;math&gt;I=\frac{d^2-r^2-R^2}{2rR}.&lt;/math&gt;
This formula gives:
*a value greater than 1 for two disjoint circles,
*a value of 1 for two circles that are tangent to each other and both  outside each other,
*a value between &amp;minus;1 and 1 for two circles that intersect,
**a value of 0 for two circles that intersect each other at [[right angle]]s , 
*a value of &amp;minus;1 for two circles that are tangent to each other, one inside of the other,
*and a value less than &amp;minus;1 when one circle contains the other.

(Some authors define the absolute inversive distance as the absolute value of the inversive distance.)

Some authors modify this formula by taking the [[inverse hyperbolic cosine]] of the value given above, rather than the value itself.&lt;ref name="ampa"/&gt;&lt;ref&gt;{{citation|title = Geometry Revisited| author1-link = Harold Scott MacDonald Coxeter | last1=Coxeter | first1=H.S.M. | author2-link=S. L. Greitzer | last2=Greitzer | first2=S.L. | year = 1967| publisher = [[Mathematical Association of America]]| location = [[Washington, D.C.]] | series=[[New Mathematical Library]] | volume=19 | isbn = 978-0-88385-619-2| zbl=0166.16402 | pages=123–124 }}&lt;/ref&gt; That is, rather than using the number &lt;math&gt;I&lt;/math&gt; as the inversive distance, the distance is instead defined as the number &lt;math&gt;\delta&lt;/math&gt; obeying the equation
:&lt;math&gt;\delta=\operatorname{arcosh}( I).&lt;/math&gt;
Although transforming the inversive distance in this way makes the distance formula more complicated, and prevents its application to crossing pairs of circles, it has the advantage that (like the usual distance for points on a line) the distance becomes additive for circles in a [[pencil of circles]]. That is, if three circles belong to a common pencil, then (using &lt;math&gt;\delta&lt;/math&gt; in place of &lt;math&gt;I&lt;/math&gt; as the inversive distance) one of their three pairwise distances will be the sum of the other two.&lt;ref name="ampa"&gt;{{citation
 | last = Coxeter | first = H. S. M. | authorlink = Harold Scott MacDonald Coxeter
 | doi = 10.1007/BF02413734
 | journal = Annali di Matematica Pura ed Applicata
 | mr = 0203568
 | pages = 73–83
 | title = Inversive distance
 | volume = 71
 | year = 1966}}.&lt;/ref&gt;

==In other geometries==
It is also possible to define the inversive distance for circles on a [[sphere]], or for circles in the [[hyperbolic plane]].&lt;ref name="bh"/&gt;

==Applications==
===Steiner chains===
A [[Steiner chain]] for two disjoint circles is a finite cyclic sequence of additional circles, each of which is tangent to the two given circles and to its two neighbors in the chain.
Steiner's porism states that if two circles have a Steiner chain, they have infinitely many such chains.
The chain is allowed to wrap more than once around the two circles, and can be characterized by a rational number &lt;math&gt;p&lt;/math&gt; whose numerator is the number of circles in the chain and whose denominator is the number of times it wraps around. All chains for the same two circles have the same value of &lt;math&gt;p&lt;/math&gt;. If the inversive distance between the two circles (after taking the inverse hyperbolic cosine) is &lt;math&gt;\delta&lt;/math&gt;, then &lt;math&gt;p&lt;/math&gt; can be found by the formula
:&lt;math&gt;p=\frac{\pi}{\sin^{-1}\tanh(\delta/2)}.&lt;/math&gt;
Conversely, every two disjoint circles for which this formula gives a [[rational number]] will support a Steiner chain. More generally, an arbitrary pair of disjoint circles can be approximated arbitrarily closely by pairs of circles that support Steiner chains whose &lt;math&gt;p&lt;/math&gt; values are [[Diophantine approximation|rational approximations]] to the value of this formula for the given two circles.&lt;ref name="ampa"/&gt;

===Circle packings===
The inversive distance has been used to define the concept of an inversive-distance [[circle packing]]: a collection of circles such that a specified subset of pairs of circles (corresponding to the edges of a [[planar graph]] ) have a given inversive distance with respect to each other. This concept generalizes the circle packings described by the [[circle packing theorem]], in which specified pairs of circles are tangent to each other.&lt;ref name="bh"/&gt;&lt;ref&gt;{{citation
 | last1 = Bowers | first1 = Philip L.
 | last2 = Stephenson | first2 = Kenneth
 | contribution = 8.2 Inversive distance packings
 | doi = 10.1090/memo/0805
 | mr = 2053391
 | pages = 78–82
 | series = Memoirs of the American Mathematical Society
 | title = Uniformizing dessins and Belyĭ maps via circle packing
 | volume = 805
 | year = 2004}}.&lt;/ref&gt; Although less is known about the existence of inversive distance circle packings than for tangent circle packings, it is known that, when they exist, they can be uniquely specified (up to Möbius transformations) by a given [[maximal planar graph]] and set of Euclidean or hyperbolic inversive distances. This [[Rigidity (mathematics)|rigidity property]] can be generalized broadly, to Euclidean or hyperbolic metrics on triangulated [[manifold]]s with [[angular defect]]s at their vertices.&lt;ref&gt;{{citation
 | last = Luo | first = Feng
 | doi = 10.2140/gt.2011.15.2299
 | issue = 4
 | journal = Geometry &amp; Topology
 | mr = 2862158
 | pages = 2299–2319
 | title = Rigidity of polyhedral surfaces, III
 | volume = 15
 | year = 2011| arxiv = 1010.3284
 }}.&lt;/ref&gt; However, for manifolds with spherical geometry, these packings are no longer unique.&lt;ref&gt;{{citation
 | last1 = Ma | first1 = Jiming
 | last2 = Schlenker | first2 = Jean-Marc
 | doi = 10.1007/s00454-012-9399-3
 | issue = 3
 | journal = Discrete Comput. Geom.
 | mr = 2891251
 | pages = 610–617
 | title = Non-rigidity of spherical inversive distance circle packings
 | volume = 47
 | year = 2012| arxiv = 1105.1469
 }}.&lt;/ref&gt; In turn, inversive-distance circle packings have been used to construct approximations to [[conformal mapping]]s.&lt;ref name="bh"/&gt;

== References ==
{{reflist}}

==External links==
*{{mathworld|title=Inversive Distance|urlname=InversiveDistance}}

[[Category:Inversive geometry]]</text>
      <sha1>1b05b64d87vzzgocqs63veisb1n9iip</sha1>
    </revision>
  </page>
  <page>
    <title>Ira Gessel</title>
    <ns>0</ns>
    <id>53794712</id>
    <revision>
      <id>857360768</id>
      <parentid>842799846</parentid>
      <timestamp>2018-08-31T05:32:07Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>/* External links */add authority control, test</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3405">'''Ira Martin Gessel''' (born 9 April 1951&lt;ref&gt;[http://people.brandeis.edu/~gessel/homepage/cv.pdf Ira Gessel's CV]&lt;/ref&gt; in [[Philadelphia]], [[Pennsylvania]]) is an American mathematician, known for his work in [[combinatorics]].  He is a long-time faculty at [[Brandeis University]] and resides in [[Arlington, Massachusetts]].

== Education and career ==
Gessel studied at [[Harvard University]] graduating ''[[magna cum laude]]'' in 1973. There, he became a [[Putnam Fellow]] in 1972, alongside [[Arthur Rubin]] and [[David Vogan]].&lt;ref&gt;[http://www.maa.org/programs/maa-awards/putnam-competition-individual-and-team-winners Putnam Competition Individual and Team Winners], ''[[Mathematical Association of America|MAA]]'' website.&lt;/ref&gt;

He received his Ph.D. at [[MIT]] in was the first student of [[Richard P. Stanley]].  He was then a postdoctoral fellow at the [[Thomas J. Watson Research Center|IBM Watson Research Center]] and MIT. He then joined Brandeis University faculty in 1984.  He was promoted to Professor of Mathematics and Computer Science in 1990, became a chair in 1996–98, and [[Professor Emeritus]] in 2015.

Gessel is a prolific contributor to [[enumerative combinatorics|enumerative]] and [[algebraic combinatorics]].  He is credited with the invention of [[quasisymmetric function]]s in 1984&lt;ref&gt;K. Luoto, S. Mykytiuk, S. van Willigenburg, ''[https://www.springer.com/us/book/9781461472995 An Introduction to Quasisymmetric Schur Functions Hopf Algebras, Quasisymmetric Functions, and Young Composition Tableaux]'', Springer, New York, 2013, p. vii.&lt;/ref&gt; and foundational work on the [[Lagrange inversion theorem]].  As of 2017, Gessel was an advisor of 27 Ph.D. students.

Gessel was elected a Fellow of the [[American Mathematical Society]] in the inaugural class of 2012. Since 2015, he is an Associate Editor of the ''[[Digital Library of Mathematical Functions]]''.&lt;ref&gt;[http://dlmf.nist.gov/about/bio/IGessel Profile of Ira Gessel], ''[[Digital Library of Mathematical Functions|DLMF]]''.&lt;/ref&gt;

==Political activism==
In 1970, while a senior in High School, Ira Gessel and his brother Michael Gessel started a [[grass-roots]] political organization to [[Committee to End Pay Toilets in America|end pay toilets in America]].&lt;ref&gt;A. Gordon,
 [https://psmag.com/why-don-t-we-have-pay-toilets-in-america-26efede62d6b Why Don’t We Have Pay Toilets in America?], ''Pacific Standard'', Sep 17, 2014.&lt;/ref&gt; The movement was largely successful and was disbanded in 1976.

== See also ==
* [[Lindström–Gessel–Viennot lemma]]
* [[Dyson conjecture]]
* [[Stirling permutation]]
* [[Dixon's identity]]
* [[Catalan number#Generalizations|Super-Catalan numbers]]

==References==
{{Reflist}}

==External links==
* [http://people.brandeis.edu/~gessel Ira Gessel's homepage]
* [https://www.linkedin.com/in/ira-gessel-6049b39/ Ira Gessel's LinkedIn page]

{{authority control}}

{{DEFAULTSORT:Gessel, Ira}}
[[Category:1951 births]]
[[Category:Living people]]
[[Category:20th-century American mathematicians]]
[[Category:21st-century American mathematicians]]
[[Category:Combinatorialists]]
[[Category:Fellows of the American Mathematical Society]]
[[Category:American Jews]]
[[Category:Jewish scientists]]
[[Category:People from Philadelphia]]
[[Category:Brandeis University faculty]]
[[Category:Harvard University alumni]]
[[Category:Massachusetts Institute of Technology alumni]]</text>
      <sha1>av3aelh51e3eh5d4d71mwtdo9oluxkd</sha1>
    </revision>
  </page>
  <page>
    <title>Kerry Mitchell</title>
    <ns>0</ns>
    <id>47286788</id>
    <revision>
      <id>787309704</id>
      <parentid>777035573</parentid>
      <timestamp>2017-06-24T17:31:35Z</timestamp>
      <contributor>
        <username>PrimeBOT</username>
        <id>29463730</id>
      </contributor>
      <minor/>
      <comment>Replace [[Help:Magic_links|magic links]] with templates per [[Special:PermaLink/772743896#Future_of_magic_links|local RfC]] - [[User:PrimeBOT/13|BRFA]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6299">'''Kerry Mitchell''' (born 1961) is an American artist known for his [[algorithmic art|algorithmic]] and [[fractal art]], which has been exhibited at  the [[Nature in Art|Nature in Art Museum]],&lt;ref name=NIA/&gt; [[The Bridges Organization|The Bridges Conference]],&lt;ref name=Bridges/&gt; and the [[Los Angeles Center for Digital Art]],&lt;ref name=LACDA/&gt; and for his "Fractal Art Manifesto".&lt;ref name=Manifesto/&gt;

==Life==
[[File:Mandelbrot Contours.jpg|thumb|Mitchell was inspired by a 1985 article on the [[Mandelbrot set]]. Shown here is a detail, with contours (just outside the boundary of the set).]]

Mitchell was born in [[Iowa]], United States, in 1961. His parents were LeRoy and Shirley Mitchell. His father was an art teacher and mother was a stay-at-home mother until Mitchell started seventh grade. Mitchell was a Presidential Scholar in 1979 and went on to pursue engineering at and graduated from [[Purdue University]] in [[aerospace engineering]], did a master's degree at [[Stanford University]], and then a PhD work at Purdue. He worked at [[NASA]] doing aerospace research. He then worked as a scientist at [[Arizona Science Center]]. He served as a mathematics and science professor at the [[University of Advancing Technology]] in [[Tempe, Arizona]]. As of 2015, he works as a manager at [[Maricopa County Community College District]] in  [[Tempe, Arizona]].&lt;ref name=FrantzCrannell2011&gt;{{cite book |last1=Frantz |first1=Marc |last2=Crannell |first2=Annalisa |title=Viewpoints: Mathematical Perspective and Fractal Geometry in Art |url=https://books.google.com/books?id=H85WUaD9f5cC&amp;pg=PA193 |year=2011 |publisher=Princeton University Press |isbn=1-4008-3905-X |pages=193–196}}&lt;/ref&gt;

Alongside his technical career, Mitchell works on [[algorithmic art]]. He ascribes his artistic awakening to a 1985 article in ''[[Scientific American]]'' on the [[Mandelbrot set]], explaining:&lt;ref name=FrantzCrannell2011/&gt;

{{quote|Like many others, I was amazed at the beauty that arose from [[iteration|iterating]] such a simple formula. Unlike most, I had the means and inclination to investigate the process further, which fed both sides of me.&lt;ref name=FrantzCrannell2011/&gt;}}

In 1999, Mitchell published his ''Fractal Art Manifesto''.&lt;ref name=Manifesto&gt;{{cite web|author=Mitchell, Kerry |title=The Fractal Art Manifesto |url=https://www.fractalus.com/info/manifesto.htm |publisher=Fractalus.com |accessdate=27 December 2015 |date=1999}}&lt;/ref&gt;&lt;!--[http://www.kerrymitchellart.com/gallery37/mobius-julia4.html ''Mobius Julia 4'']--&gt; The artist Janet Parke notes that in the manifesto, Mitchell suggests that fractal art cannot be made by a computer alone, and that not everyone who has a computer can necessarily make good fractal art. Instead, she explains, Mitchell is arguing that the artist's creative process is needed to inject elements such as the considered selection of colours and gradients, the merging of multiple layers, and decisions on composition such as by zooming in to a fractal.&lt;ref&gt;{{cite web|last1=Parke|first1=Janet|title=Fractal Art: A Comparison of Styles|url=http://www.infinite-art.com/index.about.comparison.php|website=Infinite Art|accessdate=27 December 2015|date=2003}}&lt;/ref&gt;

Mitchell also prepared tutorials on how to create fractal art with tools including [[Ultra Fractal]].&lt;ref&gt;{{cite web |title=Resources |url=http://www.ultrafractal.com/resources.html |publisher=UltraFractal.com |accessdate=27 December 2015}}&lt;/ref&gt; In 2011 he served on the panel of the "Fractal Art Contest".&lt;ref&gt;{{cite web |title=Contest Rules |publisher=Fractal Art Contests.com |accessdate=27 December 2015 |url=http://www.fractalartcontests.com/2011/rules.php |archiveurl=https://web.archive.org/web/20150222074629/http://www.fractalartcontests.com/2011/rules.php |archivedate=22 February 1015}}&lt;/ref&gt;

==Exhibitions, collections==
* [[Nature in Art|Nature in Art Museum]], Gloucestershire, 2007&lt;ref name=NIA&gt;[http://aartika-fractal-art.deviantart.com/journal/Art-of-Infinity-Fractal-Art-Exhibition-2007-515058102 Art of Infinity Fractal Art Exhibition 2007]&lt;/ref&gt;
* [[The Bridges Organization|The Bridges Conference]], 2015&lt;ref name=Bridges&gt;[http://gallery.bridgesmathart.org/exhibitions/2015-bridges-conference/lkmitch 2015 Bridges Conference]&lt;/ref&gt;
* Los Angeles Center for Digital Art (LACDA), 2015-2016&lt;ref name=LACDA&gt;[http://www.lacda.com/ Technarte: fusion of art, science and technology. Electron Salon International Group Exhibit]. Archived at https://web.archive.org/web/20151227160217/http://www.lacda.com/&lt;/ref&gt;

==Works==

===Books===
* ''Selected Works'' (self-published with Lulu.com), 2009. {{ISBN|978-0-557-08398-5}}

===Papers===
* Fractal Art Manifesto, 1999
* Introduction to [[Ultra Fractal]] version 2, 2001
* Using [[Ultra Fractal]] as a Drawing Tool, 2001
* [http://www.kerrymitchellart.com/articles/Rendering_Space-Filling_Curves.pdf Techniques for Artistically Rendering Space-Filling Curves]
* A Statistical Investigation of the Area of the Mandelbrot Set, 2001
* Rendering Fractal Images using Photographs, 2001
* Modeling Vortical Flows
* Fractal Tessellations and the Pythagorean Theorem
* Sequences and Patterns Arising from Mancala on an Infinite Board
* Toward a Chaotic World View [http://www.kerrymitchellart.com/articles/ChaoticWorldView.pdf]
* Transcendental Signature Sequences
* Fun with Chaotic Orbits in the Mandelbrot Set [http://archive.bridgesmathart.org/2012/bridges2012-389.pdf]
* Spirolateral Images from Integer Sequences [http://kerrymitchellart.com/articles/Spirolateral-Type_Images_from_Integer_Sequences.pdf]
* Fun with Whirls [http://www.kerrymitchellart.com/articles/Fun-with-Whirls.pdf]

==References==
{{reflist}}

==External links==
* [http://www.kerrymitchellart.com/ Kerry Mitchell website]
* [http://www.ams.org/mathimagery/thumbnails.php?album=44  Mathematical Imagery] at the [[American Mathematical Society]]
* [http://fineartamerica.com/profiles/kerry-mitchell.html Fine Art America: Kerry Mitchell]

{{Authority control}}

{{DEFAULTSORT:Mitchell, Kerry}}
[[Category:1961 births]]
[[Category:Living people]]
[[Category:Artists from Iowa]]
[[Category:Purdue University alumni]]
[[Category:Stanford University alumni]]
[[Category:American digital artists]]
[[Category:Mathematical artists]]
[[Category:Algorithmic art]]</text>
      <sha1>p9pboy08zwviqbnmfd901chavm1cb99</sha1>
    </revision>
  </page>
  <page>
    <title>Level set</title>
    <ns>0</ns>
    <id>559622</id>
    <revision>
      <id>869472209</id>
      <parentid>869453460</parentid>
      <timestamp>2018-11-18T20:56:52Z</timestamp>
      <contributor>
        <username>Colonies Chris</username>
        <id>577301</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8348">{{for|the computational technique|Level-set method}}
{{for|level surfaces of force fields|Equipotential surface}}
{{multiple image
|align=right
|width=140
|image1=Level sets linear function 2d.svg
|caption1=Points at constant slices of {{nowrap|''x''&lt;sub&gt;2&lt;/sub&gt; {{=}} ''f''(''x''&lt;sub&gt;1&lt;/sub&gt;)}}.
|image2=Level sets linear function 3d.svg
|caption2=Lines at constant slices of {{nowrap|''x''&lt;sub&gt;3&lt;/sub&gt; {{=}} ''f''(''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;)}}.
|image3=Level sets linear function 4d.svg
|caption3=Planes at constant slices of {{nowrap|''x''&lt;sub&gt;4&lt;/sub&gt; {{=}} ''f''(''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ''x''&lt;sub&gt;3&lt;/sub&gt;)}}.
|footer={{nowrap|(''n'' − 1)}}-dimensional level sets for functions of the form {{nowrap|''f''(''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ..., ''x''&lt;sub&gt;''n''&lt;/sub&gt;) {{=}} ''a''&lt;sub&gt;1&lt;/sub&gt;''x''&lt;sub&gt;1&lt;/sub&gt; + ''a''&lt;sub&gt;2&lt;/sub&gt;''x''&lt;sub&gt;2&lt;/sub&gt; + ... + ''a''&lt;sub&gt;''n''&lt;/sub&gt;''x''&lt;sub&gt;''n''&lt;/sub&gt;}} where ''a''&lt;sub&gt;1&lt;/sub&gt;, ''a''&lt;sub&gt;2&lt;/sub&gt;, ..., ''a''&lt;sub&gt;''n''&lt;/sub&gt; are constants, in {{nowrap|(''n'' + 1)}}-dimensional Euclidean space, for ''n'' = 1, 2, 3.}}

{{multiple image
|width=140
|align=right
|image1=Level sets non-linear function 2d.svg
|caption1=Points at constant slices of {{nowrap|''x''&lt;sub&gt;2&lt;/sub&gt; {{=}} ''f''(''x''&lt;sub&gt;1&lt;/sub&gt;)}}.
|image2=Level sets non-linear function 3d.svg
|caption2=Contour curves at constant slices of {{nowrap|''x''&lt;sub&gt;3&lt;/sub&gt; {{=}} ''f''(''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;)}}.
|image3=Level sets non-linear function 4d.svg
|caption3=Curved surfaces at constant slices of {{nowrap|''x''&lt;sub&gt;4&lt;/sub&gt; {{=}} ''f''(''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ''x''&lt;sub&gt;3&lt;/sub&gt;)}}.
|footer={{nowrap|(''n'' − 1)}}-dimensional level sets of non-linear functions ''f''(''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ..., ''x''&lt;sub&gt;''n''&lt;/sub&gt;) in {{nowrap|(''n'' + 1)}}-dimensional Euclidean space, for ''n'' = 1, 2, 3.}}

In [[mathematics]], a '''level set''' of a [[real number|real]]-valued [[function of several real variables|function ''f'' of ''n'' real variables]] is a set of the form

: &lt;math&gt; L_c(f) = \left\{ (x_1, \cdots, x_n) \, \mid \, f(x_1, \cdots, x_n) = c \right\}~, &lt;/math&gt;

that is, a set where the function takes on a given constant value ''c''.

When the number of variables is two, a level set is generically a curve, called  a level curve, [[contour line]], or isoline.  So a level curve is the set of all real-valued solutions of an equation in two variables ''x''&lt;sub&gt;1&lt;/sub&gt; and ''x''&lt;sub&gt;2&lt;/sub&gt;.   When ''n''&amp;nbsp;=&amp;nbsp;3, a level set is called a level surface (see also [[isosurface]]), and for  higher values of ''n'' the level set is a level hypersurface.  So a ''level surface'' is the set of all real-valued roots of an equation in three variables ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt; and ''x''&lt;sub&gt;3&lt;/sub&gt;, and a level [[hypersurface]] is the set of all real-valued roots of an equation in ''n'' (''n'' &gt; 3) variables.

A level set is a special case of a [[fiber (mathematics)|fiber]].

==Alternative names==

[[Image:trefoil knot level curves.png|thumb|Intersections of a [[co-ordinate]] function's level surfaces with a [[trefoil knot]].  Red curves are closest to the viewer, while yellow curves are farthest.]]

Level sets show up in many applications, often under different names. 

For example, an [[implicit curve]] is a level curve, which is considered independently of its neighbor curves, emphasizing that such a curve is defined by an [[implicit equation]]. Analogously, a level surface is sometimes called an implicit surface or an [[isosurface]].

The name isocontour is also used, which means a contour of equal height. In various application areas, isocontours have received specific names, which indicate often the nature of the values of the considered function, such as [[isobar (meteorology)|isobar]], [[isotherm (contour line)|isotherm]], [[isogon]], [[isochrone map|isochrone]], [[isoquant]] and [[indifference curve]].

== Examples ==

Consider the 2-dimensional Euclidean distance: &lt;math display="block"&gt;d(x, y) = \sqrt{x^2 + y^2}&lt;/math&gt; A level set &lt;math&gt;L_r(d)&lt;/math&gt; of this function consists of those points that lie at a distance of &lt;math&gt;r&lt;/math&gt; from the origin, otherwise known as a [[circle]]. For example, &lt;math&gt;(3, 4) \in L_5(d)&lt;/math&gt;, because &lt;math&gt;d(3, 4) = 5&lt;/math&gt;. Geometrically, this means that the point &lt;math&gt;(3, 4)&lt;/math&gt; lies on the circle of radius 5 centered at the origin. More generally, a [[sphere]] in a [[metric space]] &lt;math&gt;(M, m)&lt;/math&gt; with radius &lt;math&gt;r&lt;/math&gt; centered at &lt;math&gt;x \in M&lt;/math&gt; can be defined as the level set &lt;math&gt;L_r(y \mapsto m(x, y))&lt;/math&gt;.

A second example is the plot of [[Himmelblau's function]] shown in the figure to the right. Each curve shown is a level curve of the function, and  they are spaced logarithmically: if a curve represents &lt;math&gt;L_x&lt;/math&gt;, the curve directly "within" represents &lt;math&gt;L_{x/10}&lt;/math&gt;, and the curve directly "outside" represents &lt;math&gt;L_{10x}&lt;/math&gt;.
[[File:Himmelblau contour.svg|thumb|right|Log-spaced level curve plot of [[Himmelblau's function]] &lt;ref&gt;{{cite journal|last=Simionescu|first=P.A.|title=Some Advancements to Visualizing Constrained Functions and Inequalities of Two Variables|journal=Transactions of the ASME - Journal of Computing and Information Science in Engineering|volume=11|issue=1|year=2011|url=http://computingengineering.asmedigitalcollection.asme.org/article.aspx?articleid=1402305|doi=10.1115/1.3570770|}}&lt;/ref&gt;]]

==Level sets versus the gradient==
[[Image:level grad.svg|right|thumb|Consider a function ''f'' whose graph looks like a hill. The blue curves are the level sets; the red curves follow the direction of the gradient. The cautious hiker follows the blue paths; the bold hiker follows the red paths.]]

:'''[[Theorem]]:''' If the function {{mvar|f}}  is [[differentiable function|differentiable]], the [[gradient]] of  {{mvar|f}}  at a point is either zero, or perpendicular to the level set of  {{mvar|f}} at that point. 

To understand what this means, imagine that two hikers are at the same location on a mountain. One of them is bold, and he decides to go in the direction where the slope is steepest. The other one is more cautious; he does not want to either climb or descend, choosing a path which will keep him at the same height. In our analogy, the above theorem says that the two hikers will depart in directions perpendicular to each other.

A consequence of this theorem (and its proof) is that if {{mvar|f}} is differentiable, a level set is a [[hypersurface]] and a [[manifold]] outside the [[critical point (mathematics)|critical points]] of {{mvar|f}}. At a critical point, a level set may be reduced to a point (for example at a [[local extremum]] of {{mvar|f}} ) or may have a 
[[singular point of an algebraic variety|singularity]] such as a [[intersection theory|self-intersection point]] or a [[cusp (singularity)|cusp]].

==Sublevel and superlevel sets==
A set of the form

: &lt;math&gt; L_c^-(f) = \left\{ (x_1, \cdots, x_n) \, \mid \, f(x_1, \cdots, x_n) \leq c \right\} &lt;/math&gt;

is called a sublevel set of ''f'' (or, alternatively, a lower level set or trench of ''f''). 
: &lt;math&gt; L_c^+(f) = \left\{ (x_1, \cdots, x_n) \, \mid \, f(x_1, \cdots, x_n) \geq c \right\} &lt;/math&gt;

is called a superlevel set of ''f''.&lt;ref&gt;{{eom|first=M.I.|last=Voitsekhovskii|id=L/l058220}}&lt;/ref&gt;&lt;ref&gt;{{MathWorld|title=Level Set|urlname=LevelSet}}&lt;/ref&gt; Sublevel sets are important in [[mathematical optimization|minimization theory]]. The [[totally bounded set|boundness]] of some [[empty set|non-empty]] sublevel set and the lower-semicontinuity of the function implies that a function attains its minimum, by [[Extreme_value_theorem#Extension_to_semi-continuous_functions|Weierstrass's theorem]]. The [[convex set|convexity]] of all the sublevel sets characterizes [[quasiconvex function]]s.&lt;ref&gt;{{cite article|last=Kiwiel|first=Krzysztof C.|title=Convergence and efficiency of subgradient methods for quasiconvex minimization|journal=Mathematical Programming  (Series A)|publisher=Springer|location=Berlin, Heidelberg|issn=0025-5610|pages=1–25|volume=90|issue=1|doi=10.1007/PL00011414|year=2001|mr=1819784}}&lt;/ref&gt;

== See also ==
* [[Epigraph (mathematics)|Epigraph]]
* [[Level-set method]]
* [[Level set (data structures)]]

==References==
{{Reflist}}

[[Category:Multivariable calculus]]</text>
      <sha1>nq8ielez6cmqxaq4kjvzyyfmughj2s4</sha1>
    </revision>
  </page>
  <page>
    <title>Lexis ratio</title>
    <ns>0</ns>
    <id>8410162</id>
    <revision>
      <id>790730874</id>
      <parentid>733377234</parentid>
      <timestamp>2017-07-15T18:43:57Z</timestamp>
      <contributor>
        <username>Deacon Vorbis</username>
        <id>29330520</id>
      </contributor>
      <minor/>
      <comment>/* top */LaTeX spacing clean up, replaced: \,&lt;/math&gt; → &lt;/math&gt; using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1462">{{Unreferenced|date=December 2006}}
The '''Lexis ratio''' is used in [[statistics]] as a measure which seeks to evaluate differences between the statistical properties of random mechanisms where the outcome is two-valued &amp;mdash; for example "success" or "failure", "win" or "lose". The idea is that the probability of success might vary between different sets of trials in different situations. 

The measure compares the between-set variance of the sample proportions (evaluated for each set) with what the variance should be if there were no difference between in the true proportions of success across the different sets. Thus the measure is used to evaluate how data compares to a fixed-probability-of-success [[Bernoulli distribution]]. The term "Lexis ratio" is sometimes referred to as ''L'' or ''Q'', where

:&lt;math&gt;L^2 =Q^2 = \frac{s^2}{\sigma_0^2}.&lt;/math&gt;

Where &lt;math&gt;s^2 &lt;/math&gt; is the (weighted) [[sample variance]] derived from the observed proportions of success in sets in "Lexis trials" and &lt;math&gt;\sigma_0^2&lt;/math&gt; is the variance computed from the expected Bernoulli distribution on the basis of the overall average proportion of success.  Trials where ''L'' falls significantly above or below 1 are known as ''supernormal'' and ''subnormal,'' respectively.

==See also==
*[[Overdispersion#Binomial]]

{{DEFAULTSORT:Lexis Ratio}}
[[Category:Summary statistics]]
[[Category:Statistical ratios]]
[[Category:Statistical tests]]

{{Statistics-stub}}</text>
      <sha1>2rvdi007lf4b8amm0mhcdumkr8d22jp</sha1>
    </revision>
  </page>
  <page>
    <title>List of Intelligent Systems for Molecular Biology keynote speakers</title>
    <ns>0</ns>
    <id>40922136</id>
    <revision>
      <id>817042974</id>
      <parentid>789687131</parentid>
      <timestamp>2017-12-25T18:11:30Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.6.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="18749">The following is a '''list of [[Intelligent Systems for Molecular Biology]] (ISMB) keynote speakers'''.

ISMB is an [[academic conference]] on the subjects of [[bioinformatics]] and [[computational biology]] organised by the [[International Society for Computational Biology]] (ISCB). The conference has been held annually since 1993 and [[keynote|keynote talks]] have been presented since 1994. Keynotes are chosen to reflect outstanding research in bioinformatics. The recipients of the ISCB [[Overton Prize]] and [[ISCB Senior Scientist Awards|ISCB Accomplishment by a Senior Scientist Award]] are invited to give keynote talks as part of the programme.

Keynote speakers include eight [[Nobel Prize|Nobel laureates]]: [[Richard J. Roberts]] (1994, 2006), [[John Sulston]] (1995), [[Manfred Eigen]] (1999), [[Gerald Edelman]] (2000), [[Sydney Brenner]] (2003), [[Kurt Wüthrich]] (2006), [[Robert Huber]] (2006) and [[Michael Levitt]] (2015).&lt;ref name="about-ismb"&gt;{{cite web|title=About ISMB|url=http://www.iscb.org/about-ismb|work=www.iscb.org|publisher=[[International Society for Computational Biology|ISCB]]|accessdate=29 October 2013}}&lt;/ref&gt;&lt;ref name=Neshich2006&gt;{{cite journal|last=Neshich|first=Goran|author2=Bourne, Philip E. |author3=Brunak, Søren |title=ISMB 2006|journal=PLoS Computational Biology|date=1 January 2006|volume=2|issue=2|pages=e11|doi=10.1371/journal.pcbi.0020011}}&lt;/ref&gt;&lt;ref name=ismb2015&gt;{{cite web|title=Keynotes|url=http://www.iscb.org/ismbeccb2015-program/ismbeccb2015-keynotes|work=ISMB/ECCB 2015|publisher=ISCB|accessdate=18 May 2015}}&lt;/ref&gt; 

==List of Speakers==
{{Expand list|date=October 2013}}
{| class="wikitable"
! Conference !! Keynote Speakers !! Title !! Notes
|-
| rowspan="3" | ISMB 1994&lt;ref name=ismb1994&gt;{{cite web|title=ISMB-94|url=http://www.iscb.org/cms_addon/conferences/ismb1994/|publisher=International Society for Computational Biology|accessdate=27 October 2013}}&lt;/ref&gt; 
| Bruce Buchanan
|
|
|-
| [[Lawrence Hunter]]
|
| Plenary speaker
|-
| [[Richard J. Roberts]]
|
| Plenary speaker
|-
| rowspan="3" | ISMB 1995&lt;ref name=keynotes-list&gt;{{cite web|title=ISMB|url=http://www.iscb.org/high-quality-meetings/121|publisher=International Society for Computational Biology|accessdate=27 October 2013}}&lt;/ref&gt; 
| [[Douglas Brutlag]]
|
|
|-
| [[John Sulston]]
|
|
|-
| [[Janet Thornton]]
|
|
|-
| rowspan="4" | ISMB 1996&lt;ref name=ismb1996&gt;{{cite web|title=ISMB-96 Home Page |url=http://www.ibc.wustl.edu/ismb96/#KeynoteSpeakers |accessdate=27 October 2013 |deadurl=yes |archiveurl=https://web.archive.org/web/19970606232955/http://www.ibc.wustl.edu/ismb96/#KeynoteSpeakers |archivedate=June 6, 1997 }}&lt;/ref&gt; 
| [[Bob Waterston|Robert Waterston]]
|
|
|-
| [[David Haussler]]
|
|
|-
| [[Russell Doolittle]]
|
|
|-
| [[Chris Sander (scientist)|Chris Sander]]
|
|
|-
| rowspan="3" | ISMB 1997&lt;ref name=keynotes-list /&gt; 
| [[Richard H. Lathrop]]
|
|
|-
| [[Marcie McClure]]
|
|
|-
| [[Hans Westerhoff]]
|
|
|-
| rowspan="3" |ISMB 1998&lt;ref name=ismb1998&gt;{{cite web|title=Intelligent Systems on Molecular Biology|url=http://www-lbit.iro.umontreal.ca/ISMB98/anglais/invites_en.html|accessdate=27 October 2013|deadurl=yes|archiveurl=https://web.archive.org/web/20120508203531/http://www-lbit.iro.umontreal.ca/ISMB98/anglais/invites_en.html|archivedate=8 May 2012|df=}}&lt;/ref&gt; 
| [[Robert Cedergren]]
|
|
|-
| [[Michael Waterman]]
|
|
|-
| [[Shoshana Wodak]]
|
|
|-
| rowspan="8" | ISMB 1999&lt;ref name=ismb1999&gt;{{cite web|title=ISMB 99 - Keynotes Abstracts|url=http://bioinf.mpi-inf.mpg.de/conferences/ismb99/WWW/key_abs.html|accessdate=27 October 2013}}&lt;/ref&gt; 
| [[Manfred Eigen]]
| The Origin of Biological Information
|
|-
| [[Amos Bairoch]]
| [[Swiss-Prot]] in the 21st century!
|
|-
| [[Richard M. Karp]]
| Combinatorial Problems in [[Gene expression]] Analysis Using [[DNA microarrays]]
|
|-
| [[Anthony R. Kerlavage]]
| [[Computational genomics]]: Biological Discovery in Complete Genomes
|
|-
| [[Eugene Koonin]]
| [[Comparative genomics]]: Is it changing the paradigm of [[evolutionary biology]]?
|
|-
| David Balaban
| Genes, Chips, and Genomes
|
|-
| [[Matthias Mann]]
| Gene Function via the [[Mass spectrometry|Mass Spectrometric]] Analysis of Multi-Protein Complexes
|-
| [[Michael Sternberg]]
| Exploiting Protein Structure in the Post-genome Era
|
|-
| rowspan="7" | ISMB 2000&lt;ref name=keynotes-list /&gt; 
| [[Gerald Edelman]]
|
|
|-
| [[Leroy Hood]]
|
|
|-
| [[Minoru Kanehisa]]
|
|
|-
| [[J. Andrew McCammon]]
|
|
|-
| [[Eugene Myers]]
|
|
|-
| [[Harold Scheraga]]
|
|
|-
| [[David Searls]]
|
|
|-
| rowspan="7" | ISMB 2001&lt;ref name=keynotes-list /&gt; 
| [[Christopher Burge]]
|
|
|-
| [[Chris Dobson]]
|
|
|-
| [[Sean Eddy]]
|
|
|-
| [[David Eisenberg]]
|
|
|-
| [[Bernardo Huberman]]
|
|
|-
| Chris Sander
|
|
|-
| [[Gunnar von Heijne]]
|
| 
|-
| rowspan="8" | ISMB 2002&lt;ref&gt;{{cite web|title=ISMB 2002 Agenda|url=http://www.iscb.org/cms_addon/conferences/ismb2002/agenda.htm|accessdate=27 October 2013}}&lt;/ref&gt; 
| [[Stephen Altschul]]
| Assessing the accuracy of database search methods, and improving the performance of [[PSI-BLAST]]
|
|-
| [[Michael Ashburner]]
|
| 
|-
| [[Ford Doolittle]]
|
|
|-
| [[Terry Gaasterland]]
|
|
|-
| [[Barry Honig]]
|
|
|-
| [[David Baker (biochemist)|David Baker]]
|
| 2002 ISCB Overton Prize winner
|-
| [[John Reinitz]]
|
|
|-
| [[Isidore Rigoutsos]]
|
|
|-
| rowspan="8" | ISMB 2003&lt;ref name=ismb2003&gt;{{cite web|title=ISMB 2003 - Brisbane Australia|url=http://www.iscb.org/cms_addon/conferences/ismb2003/detailed_agenda.shtml|accessdate=27 October 2013}}&lt;/ref&gt; 
| [[Sydney Brenner]]
| The Evolution of Genes and Genomes
|
|-
| David Haussler
| Identifying functional elements in the human genome by tracing the evolutionary history of the bases: a key challenge for comparative genomics
|
|-
| [[Yoshihide Hayashizaki]]
| Dynamic Eukaryotic Transcriptome
|
|-
| [[Jim Kent]]
| Patching and Painting the Human Genome
| 2003 ISCB Overton Prize winner
|-
| [[John Mattick]]
| Programming of the autopoietic development of complex organisms: the hidden layer of noncoding RNA
|
|-
| [[David Sankoff]]
| The Parameters of Genome Rearrangement
| 2003 ISCB Senior Scientist Award winner
|-
| [[Ron Shamir]]
| Reconstructing Genetic Networks
|
|-
| Michael Waterman
| Dynamic Programming Algorithms for Haplotype Block Partitioning
|
|-
| rowspan="8" | ISMB/ECCB 2004&lt;ref name=ismbeccb2004&gt;{{cite web|title=ISMB/ECCB 2004|url=http://www.iscb.org/cms_addon/conferences/ismbeccb2004/detailed.html|publisher=[[International Society for Computational Biology|ISCB]]|accessdate=29 October 2013}}&lt;/ref&gt; 
| Leroy Hood
| Systems Biology: Strategies for Deciphering Life
|
|-
| [[Denis Noble]]
| Computational systems biology of the heart
|
|-
| [[Eric D. Green]]
| Decoding the Human Genome by Multi-Species Sequence Comparisons
|
|-
| [[Svante Pääbo]]
| Evolution of the primate transcriptome
|
|-
| [[Matthias Mann]]
| Organellar and time resolved proteomics
|
|-
| [[Anna Tramontano]]
| Progress, assessment and perspectives in protein structure prediction
|
|-
| [[Uri Alon]]
| Simplicity in complex biological networks
| 2004 ISCB Overton Prize winner
|-
| [[David J. Lipman]]
| Message and meaning in sequence comparison: is systems biology possible?
| 2004 ISCB Senior Scientist Award winner
|-
| rowspan="8" | ISMB 2005&lt;ref name=ismb2005&gt;{{cite web|title=ISMB 2005: Michigan , June 25-29|url=http://www.iscb.org/cms_addon/conferences/ismb2005/detailed1.html|publisher=[[International Society for Computational Biology|ISCB]]|accessdate=29 October 2013}}&lt;/ref&gt; 
| [[Howard Cash]]
| Biology of Life and Death: Disaster, DNA and the Information Science of Human Identification
|
|-
| Gunnar von Heijne
| Membrane Proteins ''in vivo'' and ''in silico'' - Getting the Best of Two Worlds
|
|-
| [[Jill Mesirov]]
| Gene Expression Analysis: A Knowledge-based Approach
|
|-
| [[Pavel A. Pevzner]]
| Transforming Men into Mice: Fragile versus Random Breakage Models of Chromosome Evolution
|
|-
| [[Peter Hunter (bioengineer)|Peter Hunter]]
| Computational Physiology and the IUPS Physiome Project
|
|-
| [[Satoru Miyano]]
| Computational Challenges for Gene Networks
|
|-
| [[Ewan Birney]]
| Genomes to Systems Biology
| 2005 ISCB Overton Prize winner
|-
| Janet Thornton
| From Proteins to Life - Old and New Challenges
| 2005 ISCB Senior Scientist Award winner
|-
| rowspan="8" | ISMB 2006&lt;ref name=ismb2006&gt;{{cite web|title=ISMB2006 Agenda|url=http://www.iscb.org/cms_addon/conferences/ismb2006/archive/ismb2006.cbi.cnptia.embrapa.br/pdfs/ISMB2006_Agenda.pdf|accessdate=29 October 2013}}&lt;/ref&gt; 
| [[Robert Huber]]
| Molecular machines for protein degradation
|
|-
| [[Tom Blundell]]
| Structural biology, informatics and the discovery of new medicines
|
|-
| [[Kurt Wüthrich]]
| Computational Aspects of NMR Studies with Proteins in Solution
|
|-
| [[Mathieu Blanchette (computational biologist)|Mathieu Blanchette]]
| What mammalian genomes tell us about our ancestors, and vice versa
| 2006 ISCB Overton Prize winner
|-
| [[Elena Conti]] 
| Molecular mechanisms in RNA degradation
| 
|-
| [[Charles DeLisi]]
| New Approaches to Biomarker Discovery
| 
|-
| Richard J. Roberts
| The need of Bioinformatics for experimental biologists
|
|-
| Michael Waterman
| Whole Genome Optical Mapping
| 2006 ISCB Senior Scientist Award winner
|-
| rowspan="10" | ISMB/ECCB 2007&lt;ref name=ismbeccb2007&gt;{{cite web|title=ISMB/ECCB 2007 Keynotes|url=http://www.iscb.org/cms_addon/conferences/ismbeccb2007/keynotes.html|accessdate=29 October 2013}}&lt;/ref&gt; 
| [[Eran Segal]]
| Quantitative Models for Chromatin and Transcription Regulation
| 2007 ISCB Overton Prize winner
|-
| [[Temple F. Smith]]
| Computational Biology: What is next?
| 2007 ISCB Senior Scientist Award winner
|-
| [[Søren Brunak]]
| Understanding interactomes by data integration 
| 
|-
| [[Stephen K. Burley]] 
| Fragment-based discovery of BCR-ABL inhibitors for treatment of chronic myelogenous leukemia 
| 
|-
| [[Michael Eisen]]
| Understanding and exploiting the evolution of the sequences that control gene expression
|
|-
| [[Anne-Claude Gavin]]
| Interaction Networks Probed by Mass Spectrometry
|
|-
| [[John Mattick]]
| The majority of the genome of complex organisms is devoted to an RNA regulatory system that directs differentiation and development
|
|-
| [[Erin K. O'Shea]]
| Dissecting Transcriptional Network Structure and Function
|
|-
| [[Renée Schroeder]]
| Genomic SELEX for the identification of novel non-coding RNAs independent of their expression level 
|
|-
| [[Terry Speed]]
| Genome-wide genotyping: the great classification challenge
|
|-
| rowspan="8" | ISMB 2008&lt;ref name=ismb2008&gt;{{cite web|title=Keynotes|url=http://www.iscb.org/cms_addon/conferences/ismb2008/keynotes.php|work=ISMB 2008|publisher=[[International Society for Computational Biology|ISCB]]|accessdate=27 October 2013}}&lt;/ref&gt; 
| [[Aviv Regev]]
| Modular biology: the function and evolution of molecular networks
| 2008 ISCB Overton Prize winner
|-
| [[David Haussler]]
| 100 Million Years of Evolutionary History of the Human Genome
| 2008 ISCB Senior Scientist Award winner
|-
| [[Claire M. Fraser|Claire M. Fraser-Liggett]]
| Microbial Communities in Health and Disease 
|
|-
| [[David Jaffe (mathematical biologist)|David Jaffe]]
| Tiny bits and pieces: new sequencing technologies and what they can do for you
|
|-
| [[Eugene Myers]]
|  Imaging Bioinformatics
|
|-
| [[Morag Park]]
| Profiling the Breast Tumor Microenvironment
|
|-
| [[Bernhard Palsson]]
| Systems Biology: an era of reconstruction and interrogation
|
|-
| [[Hanah Margalit]]
| Intriguing roles for small non-coding RNAs in the cellular regulatory networks
|
|-
| rowspan="8" | ISMB/ECCB 2009&lt;ref name=ismbeccb2009&gt;{{cite web|title=ISMB/ECCB 2009 Keynotes|url=http://www.iscb.org/cms_addon/conferences/ismbeccb2009/keynotes.php|work=ISMB/ECCB 2009|publisher=[[International Society for Computational Biology|ISCB]]|accessdate=15 September 2013}}&lt;/ref&gt;  
| [[Trey Ideker]]
| New Challenges and Opportunities in Network Biology
| 2009 ISCB Overton Prize winner
|-
| [[Webb Miller]]
| Bioinformatics Methods to Study Species Extinctions
| 2009 ISCB Senior Scientist Award winner
|-
| [[Pierre-Henri Gouyon]]
| Information and Biology
|
|-
| [[Daphne Koller]]
| Individual Genetic Variation: From Networks to Mechanisms
|
|-
| [[Thomas Lengauer]]
| Chasing the AIDS Virus
|
|-
| [[Eugenia Del Pino|Eugenia María del Pino Veintimilla]]
| The comparative analysis reveals independence of developmental processes during early development in frogs
|
|-
| [[Tomaso Poggio]]
| Computational Neuroscience: Models of the Visual System
|
|-
| [[Mathias Uhlén]]
| A global view on protein expression based on the Human Protein Atlas 
|
|-
| rowspan="7" | ISMB 2010&lt;ref name=ismb2010&gt;{{cite web|title=ISMB2010 - Keynotes|url=http://www.iscb.org/archive/conferences/iscb/ismb2010-program/ismb2010-keynotes.html|work=ISMB 2010|publisher=[[International Society for Computational Biology|ISCB]]|accessdate=14 September 2013}}&lt;/ref&gt;
| [[Steven E. Brenner]] 
| Ultraconserved nonsense: gene regulation by splicing &amp; RNA surveillance
| 2010 ISCB Overton Prize winner
|-
| [[Susan Lindquist]]
| Protein Folding and Environmental Stress REDRAW the Relationship between Genotype and Phenotype
|
|-
| [[Svante Pääbo]]
| Analyses of Pleistocene Genomes
|
|-
| [[Chris Sander (scientist)|Chris Sander]] 
| Systems Biology of Cancer Cells
| 2010 ISCB Senior Scientist Award winner
|-
| [[David Altshuler]]
| Genomic Variation and the Inherited Basis of Common Disease
|
|-
| [[George M. Church]]
| BI/O: Reading and Writing Genomes
|
|-
| [[Robert Weinberg]] 
| Cancer Stem Cells and the Evolution of Malignancy
| Special Public Lecture 
|-
| rowspan="6" | ISMB/ECCB 2011&lt;ref name=ismbeccb2011&gt;{{cite web|title=Keynotes|url=http://www.iscb.org/archive/conferences/iscb/ismbeccb2011-program/keynotes.html|work=ISMB/ECCB 2011|publisher=[[International Society for Computational Biology|ISCB]]|accessdate=26 August 2013}}&lt;/ref&gt;
| [[Bonnie Berger]]
| Computational biology in the 21st century: making sense out of massive data
|
|-
| [[Olga Troyanskaya]]
| Integrating computation and experiments for a molecular-level understanding of human disease
| 2011 ISCB Overton Prize winner
|-
| [[Janet Thornton]] 
| The Evolution of Enzyme Mechanisms and Functional Diversity
| [[European Conference on Computational Biology|ECCB]] 10th Anniversary Keynote
|-
| [[Alfonso Valencia]] 
| Challenges for Bioinformatics in Personalized Cancer Medicine
| 2011 ISCB Fellow
|-
| [[Luis Serrano (scientist)|Luis Serrano]]
| ''M. pneumoniae'' (Towards a full quantitative understanding of a free-living system)
|
|-
| [[Michael Ashburner]] 
| From sequences to ontologies - adventures in informatics
| 2011 ISCB Senior Scientist Award winner
|-
| rowspan="6" | ISMB 2012&lt;ref name=ismb2012&gt;{{cite web|title=Keynotes|url=http://www.iscb.org/ismb2012-program/ismb2012-keynotes|work=ISMB 2012|publisher=[[International Society for Computational Biology|ISCB]]|accessdate=26 August 2013}}&lt;/ref&gt;
| Richard H. Lathrop &amp; [[Lawrence Hunter]]
| Seeing forward by looking back
| ISMB 20th Anniversary Keynote
|-
| [[Ziv Bar-Joseph]]
| Data integration for understanding dynamic biological systems
| 2012 ISCB Overton Prize winner
|-
| [[Barbara Wold]]
| Analysis of transcriptome structure and chromatin landscapes
|
|-
| [[Richard M. Durbin]]
| Progress, challenges and opportunities in population genome sequencing
| 2012 ISCB Fellow
|-
| [[Andrej Šali]]
| Integrative Structural Biology
|
|-
| Gunnar von Heijne
| The other Third: Coming to grips with membrane proteins
| 2012 ISCB Senior Scientist Award winner
|-
| rowspan="6" | ISMB/ECCB 2013&lt;ref name=ismbeccb2013&gt;{{cite web|title=All Keynotes|url=http://www.iscb.org/ismbeccb2013-program/ismbeccb2013-keynotes/ismbeccb2013-all-keynotes|work=ISMB/ECCB 2013|publisher=[[International Society for Computational Biology|ISCB]]|accessdate=26 August 2013}}&lt;/ref&gt; 
| [[Gil Ast]]
| How Chromatin organization and epigenetics talk with alternative splicing 
|
|-
| [[Gonçalo Abecasis]] 
| Insights from Sequencing Thousands of Human Genomes
| 2013 ISCB Overton Prize winner
|-
| [[Lior Pachter]]
| Sequencing based functional genomics (analysis)
|
|-
| [[Gary Stormo]]
| Searching for Signals in Sequences
| 2013 ISCB Fellow
|-
| [[Carole Goble]]
| Results may vary: what is reproducible? why do open science and who gets the credit?
|
|-
| [[David Eisenberg]]
| Protein Interactions in Health and Disease
| 2013 ISCB Senior Scientist Award winner 
|-
| rowspan="6" | ISMB 2014&lt;ref name=ismb2014&gt;{{cite web|title=Keynotes|url=http://www.iscb.org/ismb2014-program/ismb2014-keynotes|work=ISMB2014|publisher=ISCB|accessdate=9 February 2014}}&lt;/ref&gt; 
| [[Isaac Kohane]]
| Biomedical Quants of the World Unite! We only have our disease burden to lose
|
|-
| [[Eugene Myers]] 
| DNA Assembly: Past, Present, and Future
| 2014 ISCB Senior Scientist Award winner 
|-
| [[Michal Linial]]
| Good Things Come in Small Packages – Replicators and Innovators
|
|-
| [[Dana Pe'er]]
| A multidimensional single cell approach to understand cellular behavior
| 2014 ISCB Overton Prize winner
|-
| [[Robert S. Langer]]
| Biomaterials and biotechnology: From the discovery of the first angiogenesis inhibitors to the development of controlled drug delivery systems and the foundation of tissue engineering
|
|-
| [[Russ Altman]]
| Informatics for understanding drug response at all scales
| 2014 ISCB Fellow
|-
| rowspan="6" | ISMB 2015&lt;ref name=ismb2015 /&gt;
| Michael Levitt
| Birth &amp; Future of Multiscale Modeling of Macromolecules
|
|-
| [[Curtis Huttenhower]] 
| Understanding microbial community function and the human microbiome in health and disease
| 2015 ISCB Overton Prize winner
|-
| Eileen Furlong
| Genome regulation during embryonic development
|
|-
| Kenneth H Wolfe
| TBA
| 
|-
| [[Cyrus Chothia]]
| TBA
| 2015 ISCB Senior Scientist Award winner 
|-
| Amos Bairoch
| TBA
| 2015 ISCB Fellow
|-
| rowspan="6" | ISMB 2016&lt;ref name="ismb2016-keynotes"&gt;{{cite web|title=Keynotes|url=https://www.iscb.org/ismb2016program/keynotes|accessdate=11 April 2016}}&lt;/ref&gt;
| [[Ruth Nussinov]]
| Ras signaling: a challenge to the biological sciences
| 2016 ISCB Fellow
|-
| [[Debora Marks]]
| 
| 2016 ISCB Overton Prize winner
|-
| Sandrine Dudoit
| Identification of Novel Cell Types in the Brain Using Single-Cell Transcriptome Sequencing
|
|-
| [[Sarah Teichmann]]
| Understanding Cellular Heterogeneity
| 
|-
| Serafim Batzoglou
| 
| 2016 ISCB Innovator Award winner
|-
| [[Søren Brunak]]
| 
| 2016 ISCB Senior Scientist Award winner 
|}

==References==
{{reflist|2}}

[[Category:Bioinformaticians]]
[[Category:Biology conferences]]
[[Category:Computational science]]</text>
      <sha1>ob3hf96rlq9th4dr8zw6o89ni73kkjm</sha1>
    </revision>
  </page>
  <page>
    <title>Log-spectral distance</title>
    <ns>0</ns>
    <id>15602155</id>
    <revision>
      <id>849517170</id>
      <parentid>815356001</parentid>
      <timestamp>2018-07-09T15:02:28Z</timestamp>
      <contributor>
        <ip>183.83.80.47</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1376">The '''log-spectral distance (LSD)''', also referred to as '''log-spectral distortion''', is a distance measure (expressed in dB) between two spectra&lt;ref&gt;
{{cite book
 | title = Fundamentals of speech recognition
 |author1=Rabiner, Lawrence R |author2=Juang, Biing-Hwang | publisher = PTR Prentice Hall
 | year = 1993
 | url = http://www.citeulike.org/group/10577/article/308923
 }}&lt;/ref&gt;. The log-spectral distance between spectra &lt;math&gt;P\left(\omega\right)&lt;/math&gt; and &lt;math&gt;\hat{P}\left(\omega\right)&lt;/math&gt; is defined as:

: &lt;math&gt;D_{LS}=\sqrt{\frac{1}{2\pi} \int_{-\pi}^\pi \left[ 10\log_{10} \frac{P(\omega)}{\hat{P}(\omega)} \right]^2 \,d\omega },&lt;/math&gt;
where &lt;math&gt;P\left(\omega\right)&lt;/math&gt; and &lt;math&gt;\hat{P}\left(\omega\right)&lt;/math&gt; are power spectra.
Unlike the [[Itakura–Saito distance]], the log-spectral distance is symmetric.

In speech coding log spectral distortion for a given frame is defined as the root mean square difference between the original LPC log power spectrum and the quantize or interpolated LPC log power spectrum. Usually the  average of spectral distortion over a large number of frames is calculated and that is used as the measure of performance of quantization or interpolation.

== See also ==

* [[Itakura–Saito distance]]

== References ==
&lt;references /&gt;

[[Category:Signal processing]]

{{compu-stub}}
{{Signal-processing-stub}}</text>
      <sha1>n5ee0l0t1dlthaqzpo57d5n09en4oxo</sha1>
    </revision>
  </page>
  <page>
    <title>Loop integral</title>
    <ns>0</ns>
    <id>2703676</id>
    <revision>
      <id>800396919</id>
      <parentid>798824686</parentid>
      <timestamp>2017-09-13T08:16:43Z</timestamp>
      <contributor>
        <username>Rathfelder</username>
        <id>398607</id>
      </contributor>
      <comment>removed [[Category:Mathematical mechanics]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="473">{{unreferenced|date=November 2008}}
In [[quantum field theory]] and [[statistical mechanics]], '''loop integrals''' are the integrals which appear when evaluating the [[Feynman diagram]]s with one or more loops by integrating over the internal momenta.

There are two standard techniques for evaluating loop integrals:

*[[Feynman parametrization]]
*[[Schwinger parametrization]]

[[Category:Quantum field theory]]
[[Category:Statistical mechanics]]


{{applied-math-stub}}</text>
      <sha1>9qvvvbanl7agb8zhg57p06gxtti6uyb</sha1>
    </revision>
  </page>
  <page>
    <title>MATLAB</title>
    <ns>0</ns>
    <id>20412</id>
    <revision>
      <id>871476511</id>
      <parentid>871434722</parentid>
      <timestamp>2018-12-01T09:38:03Z</timestamp>
      <contributor>
        <ip>2A01:CB04:25E:FD00:C941:F65E:2586:CA12</ip>
      </contributor>
      <comment>I have added an external link about MATLAB &amp; Simulink tutorials for beginners.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="51478">{{Use mdy dates|date=September 2018}}
{{for|the region in Bangladesh|Matlab (Bangladesh)}}
{{distinguish|MATHLAB}}
{{Infobox software
| name = MATLAB
| logo = [[File:Matlab Logo.png|100px]]
| logo caption = L-shaped membrane logo&lt;ref&gt;{{cite web|title=The L-Shaped Membrane|url=http://www.mathworks.com/company/newsletters/articles/the-l-shaped-membrane.html|publisher=MathWorks|accessdate=February 7, 2014|year=2003}}&lt;/ref&gt;
| screenshot = [[File:MATLAB R2013a Win8 screenshot.png|320px]]
| caption = MATLAB R2013a running on [[Windows 8]]
| developer = [[MathWorks]]
| released = {{Start date and age|1984}}
| latest release version = R2018b
| latest release date = {{Start date and age|2018|09|12}}
| latest preview version = 
| latest preview date = 
| status = Active
| programming language = [[C (programming language)|C]], [[C++]], [[Java (programming language)|Java]]
| operating system = [[Microsoft Windows|Windows]], [[macOS]], and [[Linux]]&lt;ref&gt;{{cite web|url=http://www.mathworks.com/products/availability/index.html#ML|title=System Requirements and Platform Availability|publisher=MathWorks|accessdate=August 14, 2013}}&lt;/ref&gt;
| platform = [[IA-32]], [[x86-64]]
| genre = [[List of numerical analysis software|Numerical computing]]
| license = [[Proprietary software|Proprietary]] [[commercial software]]
| website = {{URL|https://www.mathworks.com/products/matlab.html|mathworks.com}}
}}
{{Infobox programming language
| name                   = MATLAB
| paradigm               = [[Multi-paradigm programming language|multi-paradigm]]: [[Functional programming|functional]], [[Imperative programming|imperative]], [[Procedural programming|procedural]], [[Object-oriented programming|object-oriented]], [[Array programming|array]]
| family                 =
| year                   = late 1970s
| designer               = [[Cleve Moler]]
| developer              = [[MathWorks]]
| latest release version = 9.4 (R2018a)
| latest release date    = {{Start date and age|2018|03|14}}
| latest preview version =
| latest preview date    =
| typing                 = [[dynamic typing|dynamic]], [[weak typing|weak]]
| turing-complete        = Yes
| scope                  =
| implementations        =
| dialects               =
| influenced by          = {{startflatlist}}
*[[APL (programming language)|APL]]
*[[EISPACK]]
*[[LINPACK]]
*[[PL/0]]
*[[Speakeasy (computational environment)|Speakeasy]]&lt;ref&gt;{{cite web |url=http://archive.computerhistory.org/resources/access/text/2013/12/102746804-05-01-acc.pdf |title=An interview with CLEVE MOLER Conducted by Thomas Haigh On 8 and 9 March, 2004 Santa Barbara, California |publisher=Computer History Museum |quote=So APL, Speakeasy, LINPACK, EISPACK, and PL0 were the predecessors to MATLAB. |accessdate=December 6, 2016}}&lt;/ref&gt;
{{endflatlist}}
| influenced             = {{startflatlist}}
*[[Julia (programming language)|Julia]]&lt;ref name="Julia"&gt;{{cite web |url=http://julialang.org/blog/2012/02/why-we-created-julia |first1=Jeff |last1=Bezanson |first2=Stefan |last2=Karpinski |first3=Viral |last3=Shah |first4=Alan |last4=Edelman |title=Why We Created Julia  |publisher=Julia Language |date=February 14, 2012 |accessdate=December 1, 2016}}&lt;/ref&gt;
*[[GNU Octave|Octave]]&lt;ref name="Octave"&gt;{{cite web |url=http://jbrwww.che.wisc.edu/tech-reports/twmcc-2001-03.pdf |first=John W. |last=Eaton |title=Octave: Past, Present, and Future |work=Texas-Wisconsin Modeling and Control Consortium |date=May 21, 2001 |accessdate=December 1, 2016}}&lt;/ref&gt; 
*[[Scilab]]&lt;ref name="Scilab"&gt;{{cite web |url=https://www.scilab.org/scilab/history |title=History |publisher=Scilab |accessdate=December 1, 2016}}&lt;/ref&gt;
{{endflatlist}}
| operating system       =
| license                =
| file ext               = .m
| website                = {{URL|https://www.mathworks.com/products/matlab.html|mathworks.com}}
| wikibooks              = MATLAB Programming
}}

'''MATLAB''' (''matrix laboratory'') is a [[multi-paradigm programming language|multi-paradigm]] [[numerical analysis|numerical computing]] environment and [[proprietary programming language]] developed by [[MathWorks]]. MATLAB allows [[matrix (mathematics)|matrix]] manipulations, plotting of [[function (mathematics)|functions]] and data, implementation of [[algorithm]]s, creation of [[user interface]]s, and interfacing with programs written in other languages, including [[C (programming language)|C]], [[C++]], [[C Sharp (programming language)|C#]], [[Java (programming language)|Java]], [[Fortran]] and [[Python (programming language)|Python]].

Although MATLAB is intended primarily for numerical computing, an optional toolbox uses the [[MuPAD]] [[computer algebra system|symbolic engine]], allowing access to [[symbolic computing]] abilities. An additional package, [[Simulink]], adds graphical multi-domain simulation and [[model-based design]] for [[dynamical system|dynamic]] and [[embedded system]]s.

As of 2018, MATLAB has more than 3 million users worldwide.&lt;ref name="mathworksCompanyOverview"&gt;{{cite web |url=https://www.mathworks.com/content/dam/mathworks/tag-team/Objects/c/company-fact-sheet-8282v18.pdf |title=Company Overview |author = The MathWorks |date=April 2018}}&lt;/ref&gt; MATLAB users come from various backgrounds of [[engineering]], [[science]], and [[economics]].

== History ==
[[Cleve Moler]], the chairman of the [[computer science]] department at the [[University of New Mexico]], started developing MATLAB in the late 1970s.&lt;ref name="origins"&gt;{{cite web | url = http://www.mathworks.com/company/newsletters/articles/the-origins-of-matlab.html | title = The Origins of MATLAB| author = Cleve Moler | accessdate = April 15, 2007 |date=December 2004 }}&lt;/ref&gt; He designed it to give his students access to [[LINPACK]] and [[EISPACK]] without them having to learn [[Fortran]]. It soon spread to other universities and found a strong audience within the [[applied mathematics]] community. [[John N. Little|Jack Little]], an engineer, was exposed to it during a visit Moler made to [[Stanford University]] in 1983. Recognizing its commercial potential, he joined with Moler and Steve Bangert. They rewrote MATLAB in [[C (programming language)|C]] and founded [[MathWorks]] in 1984 to continue its development. These rewritten libraries were known as JACKPAC.&lt;ref&gt;{{cite web|url=http://www.altiusdirectory.com/Computers/matlab-programming-language.php|title=MATLAB Programming Language|publisher=Altius Directory|accessdate=December 17, 2010}}&lt;/ref&gt; In 2000, MATLAB was rewritten to use a newer set of libraries for matrix manipulation, [[LAPACK]].&lt;ref&gt;{{cite web|title=MATLAB Incorporates LAPACK|url=http://www.mathworks.com/company/newsletters/articles/matlab-incorporates-lapack.html|work=Cleve's Corner|publisher=MathWorks|accessdate=December 20, 2008|first=Cleve |last=Moler|date=January 2000}}&lt;/ref&gt;

MATLAB was first adopted by researchers and practitioners in [[control engineering]], Little's specialty, but quickly spread to many other domains. It is now also used in education, in particular the teaching of [[linear algebra]], [[numerical analysis]], and is popular amongst scientists involved in [[image processing]].&lt;ref name="origins" /&gt;

== Syntax ==
The MATLAB application is built around the MATLAB scripting language. Common usage of the MATLAB application involves using the Command Window as an interactive mathematical [[command line interface|shell]] or executing text files containing MATLAB code.&lt;ref&gt;{{cite web|url=http://www.mathworks.com/help/matlab/index.html|title=MATLAB Documentation |publisher=MathWorks|accessdate=August 14, 2013}}&lt;/ref&gt;

=== Variables ===
Variables are defined using the assignment operator, &lt;code&gt;=&lt;/code&gt;. MATLAB is a [[Strong and weak typing|weakly typed]] programming language because types are implicitly converted.&lt;ref&gt;{{cite web|title=Comparing MATLAB with Other OO Languages|url=http://www.mathworks.com/help/matlab/matlab_oop/matlab-vs-other-oo-languages.html|work=MATLAB|publisher=MathWorks|accessdate=August 14, 2013}}&lt;/ref&gt;  It is an inferred typed language because variables can be assigned without declaring their type, except if they are to be treated as symbolic objects,&lt;ref&gt;{{cite web|title=Create Symbolic Variables and Expressions|url=http://www.mathworks.com/help/symbolic/creating-symbolic-variables-and-expressions.html|work=Symbolic Math Toolbox|publisher=MathWorks|accessdate=August 14, 2013}}&lt;/ref&gt; and that their type can change. Values can come from [[constant (computer science)|constant]]s, from computation involving values of other variables, or from the output of a function. For example:
&lt;source lang="matlabsession"&gt;
&gt;&gt; x = 17
x =
 17

&gt;&gt; x = 'hat'
x =
hat

&gt;&gt; x = [3*4, pi/2]
x =
   12.0000    1.5708

&gt;&gt; y = 3*sin(x)
y =
   -1.6097    3.0000
&lt;/source&gt;

=== Vectors and matrices ===
A simple array is defined using the colon syntax: ''initial''&lt;code&gt;:&lt;/code&gt;''increment''&lt;code&gt;:&lt;/code&gt;''terminator''. For instance:
&lt;source lang="matlab"&gt;
&gt;&gt; array = 1:2:9
array=
 1 3 5 7 9
&lt;/source&gt;
defines a variable named &lt;code&gt;array&lt;/code&gt; (or assigns a new value to an existing variable with the name &lt;code&gt;array&lt;/code&gt;) which is an array consisting of the values 1, 3, 5, 7, and 9. That is, the array starts at 1 (the ''initial'' value), increments with each step from the previous value by 2 (the ''increment'' value), and stops once it reaches (or to avoid exceeding) 9 (the ''terminator'' value).
&lt;source lang="matlab"&gt;
&gt;&gt; array = 1:3:9
array =
 1 4 7
&lt;/source&gt;
the ''increment'' value can actually be left out of this syntax (along with one of the colons), to use a default value of 1.
&lt;source lang="matlab"&gt;
&gt;&gt; ari = 1:5
ari =
 1 2 3 4 5
&lt;/source&gt;
assigns to the variable named &lt;code&gt;ari&lt;/code&gt; an array with the values 1, 2, 3, 4, and 5, since the default value of 1 is used as the incrementer.

[[One-based indexing|Indexing]] is one-based,&lt;ref&gt;{{cite web|title=Matrix Indexing|url=http://www.mathworks.com/help/matlab/math/matrix-indexing.html|publisher=MathWorks|accessdate=August 14, 2013}}&lt;/ref&gt; which is the usual convention for [[matrix (mathematics)|matrices]] in mathematics, although not for some programming languages such as C, C++, and Java.

Matrices can be defined by separating the elements of a row with blank space or comma and using a semicolon to terminate each row. The list of elements should be surrounded by square brackets: []. Parentheses: () are used to access elements and subarrays (they are also used to denote a function argument list).

&lt;source lang="matlab"&gt;
&gt;&gt; A = [16 3 2 13; 5 10 11 8; 9 6 7 12; 4 15 14 1]
A =
 16  3  2 13
  5 10 11  8
  9  6  7 12
  4 15 14  1

&gt;&gt; A(2,3)
ans =
 11
&lt;/source&gt;

Sets of indices can be specified by expressions such as "2:4", which evaluates to [2, 3, 4].  For example, a submatrix taken from rows 2 through 4 and columns 3 through 4 can be written as:
&lt;source lang="matlab"&gt;
&gt;&gt; A(2:4,3:4)
ans =
 11 8
 7 12
 14 1
&lt;/source&gt;
A square [[identity matrix]] of size ''n'' can be generated using the function ''eye'', and matrices of any size with zeros or ones can be generated with the functions ''zeros'' and ''ones'', respectively.
&lt;source lang="matlab"&gt;
&gt;&gt; eye(3,3)
ans =
 1 0 0
 0 1 0
 0 0 1

&gt;&gt; zeros(2,3)
ans =
 0 0 0
 0 0 0

&gt;&gt; ones(2,3)
ans =
 1 1 1
 1 1 1
&lt;/source&gt;

[[Transpose|Transposing]] a vector or a matrix is done either by the function ''transpose'' or by adding prime after a dot to the matrix. Without the dot MATLAB will perform [[conjugate transpose]].
&lt;source lang="matlab"&gt;
&gt;&gt; A = [1 ; 2],  B = A.', C = transpose(A)
A =
     1
     2
B =
     1     2
C =
     1     2

&gt;&gt; D = [0 3 ; 1 5], D.'
D =
     0     3
     1     5
ans =
     0     1
     3     5
&lt;/source&gt;

Most MATLAB functions can accept matrices and will apply themselves to each element. For example, &lt;code&gt;mod(2*J,n)&lt;/code&gt; will multiply every element in "J" by 2, and then reduce each element modulo "n". MATLAB does include standard "for" and "while" loops, but (as in other similar applications such as [[R (programming language)|R]]), using the [[Array programming|vectorized]] notation often produces code that is faster to execute. This code, excerpted from the function ''magic.m'', creates a [[magic square]] ''M'' for odd values of ''n'' (MATLAB function &lt;code&gt;meshgrid&lt;/code&gt; is used here to generate square matrices I and J containing 1:n).

&lt;source lang="matlab"&gt;
[J,I] = meshgrid(1:n);
A = mod(I + J - (n + 3) / 2, n);
B = mod(I + 2 * J - 2, n);
M = n * A + B + 1;
&lt;/source&gt;

=== Structures ===
MATLAB has structure data types.&lt;ref&gt;{{cite web|title=Structures|url=http://www.mathworks.com/help/matlab/structures.html|publisher=MathWorks|accessdate=August 14, 2013}}&lt;/ref&gt; Since all variables in MATLAB are arrays, a more adequate name is "structure array", where each element of the array has the same field names. In addition, MATLAB supports dynamic field names&lt;ref&gt;{{cite web|title=Generate Field Names from Variables|url=http://www.mathworks.com/help/matlab/matlab_prog/generate-field-names-from-variables.html|publisher=MathWorks|accessdate=August 14, 2013}}&lt;/ref&gt; (field look-ups by name, field manipulations, etc.). Unfortunately, MATLAB JIT does not support MATLAB structures, therefore just a simple bundling of various variables into a structure will come at a cost.&lt;ref&gt;[http://blogs.mathworks.com/loren/2012/03/26/considering-performance-in-object-oriented-matlab-code/ Considering Performance in Object-Oriented MATLAB Code], Loren Shure, MATLAB Central, March 26, 2012: "function calls on structs, cells, and function handles will not benefit from JIT optimization of the function call and can be many times slower than function calls on purely numeric arguments"&lt;/ref&gt;

=== Functions ===
When creating a MATLAB function, the name of the file should match the name of the first function in the file. Valid function names begin with an alphabetic character, and can contain letters, numbers, or underscores.  Functions are often case sensitive.

=== Function handles ===
MATLAB supports elements of [[lambda calculus]] by introducing function handles,&lt;ref&gt;{{cite web|title=Function Handles|url=http://www.mathworks.com/help/matlab/function-handles.html|publisher=MathWorks|accessdate=August 14, 2013}}&lt;/ref&gt; or function references, which are implemented either in .m files or anonymous&lt;ref&gt;{{cite web|title=Anonymous Functions|url=http://www.mathworks.com/help/matlab/matlab_prog/anonymous-functions.html|publisher=MathWorks|accessdate=August 14, 2013}}&lt;/ref&gt;/nested functions.&lt;ref&gt;{{cite web|title=Nested Functions|url=http://www.mathworks.com/help/matlab/matlab_prog/nested-functions.html|publisher=MathWorks.}}&lt;/ref&gt;

=== Classes and object-oriented programming ===
MATLAB supports [[object-oriented programming]] including classes, inheritance, virtual dispatch, packages, pass-by-value semantics, and pass-by-reference semantics.&lt;ref&gt;{{cite web|url=http://www.mathworks.com/help/matlab/object-oriented-programming.html |title=Object-Oriented Programming|publisher=MathWorks|accessdate=August 14, 2013}}&lt;/ref&gt; However, the syntax and calling conventions are significantly different from other languages. MATLAB has value classes and reference classes, depending on whether the class has ''handle'' as a super-class (for reference classes) or not (for value classes).&lt;ref&gt;{{cite web|title=Comparing Handle and Value Classes|url=http://www.mathworks.com/help/matlab/matlab_oop/comparing-handle-and-value-classes.html|publisher=MathWorks}}&lt;/ref&gt;

Method call behavior is different between value and reference classes. For example, a call to a method
&lt;source lang="matlab"&gt;
object.method();
&lt;/source&gt;
can alter any member of ''object'' only if ''object'' is an instance of a reference class.

An example of a simple class is provided below.

&lt;source lang="matlab"&gt;
classdef hello
    methods
        function greet(this)
            disp('Hello!')
        end
    end
end
&lt;/source&gt;

When put into a file named &lt;tt&gt;hello.m&lt;/tt&gt;, this can be executed with the following commands:
&lt;source lang="matlabsession"&gt;
&gt;&gt; x = hello;
&gt;&gt; x.greet();
Hello!
&lt;/source&gt;

== Graphics and graphical user interface programming ==
MATLAB supports developing applications with [[graphical user interface]] (GUI) features. MATLAB includes GUIDE&lt;ref&gt;{{cite web|title=Create a Simple GUIDE GUI|url=http://www.mathworks.com/help/matlab/creating_guis/about-the-simple-guide-gui-example.html|publisher=MathWorks|accessdate=August 14, 2014}}&lt;/ref&gt; (GUI development environment) for graphically designing GUIs.&lt;ref&gt;{{cite web| url=http://www.mathworks.com/discovery/matlab-gui.html | title=MATLAB GUI | publisher=MathWorks | date=April 30, 2011 | accessdate=August 14, 2013}}&lt;/ref&gt; It also has tightly integrated graph-plotting features. For example, the function ''plot'' can be used to produce a graph from two vectors ''x'' and ''y''. The code:
&lt;source lang="matlab"&gt;
x = 0:pi/100:2*pi;
y = sin(x);
plot(x,y)
&lt;/source&gt;
produces the following figure of the [[sine wave|sine function]]:

[[File:Matlab plot sin.svg|350px]]

A MATLAB program can produce three-dimensional graphics using the functions ''surf'', ''plot3'' or ''mesh''.
{|
|-
| valign="top" |&lt;source lang="matlab"&gt;[X,Y] = meshgrid(-10:0.25:10,-10:0.25:10);
f = sinc(sqrt((X/pi).^2+(Y/pi).^2));
mesh(X,Y,f);
axis([-10 10 -10 10 -0.3 1])
xlabel('{\bfx}')
ylabel('{\bfy}')
zlabel('{\bfsinc} ({\bfR})')
hidden off
&lt;/source&gt;
| &amp;nbsp;&amp;nbsp;&amp;nbsp;
| valign="top" |&lt;source lang="matlab"&gt;
[X,Y] = meshgrid(-10:0.25:10,-10:0.25:10);
f = sinc(sqrt((X/pi).^2+(Y/pi).^2));
surf(X,Y,f);
axis([-10 10 -10 10 -0.3 1])
xlabel('{\bfx}')
ylabel('{\bfy}')
zlabel('{\bfsinc} ({\bfR})')
&lt;/source&gt;
|-
| This code produces a '''[[wire frame model|wireframe]]''' 3D plot of the two-dimensional unnormalized [[sinc function]]:
| &amp;nbsp;&amp;nbsp;&amp;nbsp;
| This code produces a '''surface''' 3D plot of the two-dimensional unnormalized [[sinc function]]:
|-
| style="text-align:center;"|[[File:MATLAB mesh sinc3D.svg]]
| &amp;nbsp;&amp;nbsp;&amp;nbsp;
| style="text-align:center;"|[[File:MATLAB surf sinc3D.svg]]
|}

In MATLAB, graphical user interfaces can be programmed with the GUI design environment (GUIDE) tool.&lt;ref&gt;{{cite book | title=MATLAB: Advanced GUI Development | publisher=Dog Ear Publishing | last=Smith |first=S. T. | year=2006 | isbn=978-1-59858-181-2}}&lt;/ref&gt;

== Interfacing with other languages ==
MATLAB can call functions and subroutines written in the programming languages [[C (programming language)|C]] or [[Fortran]].&lt;ref&gt;{{cite web|title=Application Programming Interfaces to MATLAB|url=http://www.mathworks.com/help/matlab/programming-interfaces-for-c-c-fortran-com.html|publisher=MathWorks|accessdate=August 14, 2013}}&lt;/ref&gt; A wrapper function is created allowing MATLAB data types to be passed and returned. [[MEX file]]s (MATLAB executables) are the dynamically loadable object files created by compiling such functions.&lt;ref&gt;{{cite web|title=Create MEX-Files|url=http://www.mathworks.com/help/matlab/create-mex-files.html|publisher=MathWorks|accessdate=August 14, 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Connecting C and Matlab | last=Spielman | first=Dan | publisher=Yale University, Computer Science Department | date=February 10, 2004 | url=http://www.cs.yale.edu/homes/spielman/ECC/cMatlab.html | accessdate=May 20, 2008}}&lt;/ref&gt; Since 2014 increasing two-way interfacing with [[Python (programming language)|Python]] was being added.&lt;ref&gt;{{cite web|title=MATLAB Engine for Python|url=http://www.mathworks.com/help/matlab/matlab-engine-for-python.html|publisher=MathWorks|accessdate=June 13, 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Call Python Libraries|url=http://www.mathworks.com/help/matlab/call-python-libraries.html|publisher=MathWorks|accessdate=June 13, 2015}}&lt;/ref&gt;

Libraries written in [[Perl]], [[Java (programming language)|Java]], [[ActiveX]] or [[.NET Framework|.NET]] can be directly called from MATLAB,&lt;ref&gt;{{cite web|title=External Programming Language Interfaces|url=http://www.mathworks.com/help/matlab/external-interfaces.html|publisher=MathWorks|accessdate=August 14, 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Call Perl script using appropriate operating system executable|url=http://www.mathworks.com/help/matlab/ref/perl.html|publisher=MathWorks|accessdate=November 7, 2013}}&lt;/ref&gt; and many MATLAB libraries (for example [[XML]] or [[SQL]] support) are implemented as wrappers around Java or ActiveX libraries. Calling MATLAB from Java is more complicated, but can be done with a MATLAB toolbox&lt;ref&gt;{{cite web|url=http://www.mathworks.com/products/javabuilder/ |title=MATLAB Builder JA |publisher=MathWorks |accessdate=June 7, 2010}}&lt;/ref&gt; which is sold separately by [[MathWorks]], or using an undocumented mechanism called JMI (Java-to-MATLAB  Interface),&lt;ref&gt;{{cite web|url=http://undocumentedmatlab.com/blog/jmi-java-to-matlab-interface/|first=Yair |last=Altman |title=Java-to-Matlab Interface |publisher=Undocumented Matlab |date=April 14, 2010 |accessdate=June 7, 2010}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=matlabcontrol JMI|url=https://code.google.com/p/matlabcontrol/wiki/JMI|first=Joshua |last=Kaplan}}&lt;/ref&gt; (which should not be confused with the unrelated [[Java Metadata Interface]] that is also called JMI). Official MATLAB API for Java was added in 2016.&lt;ref name="MATLAB Engine API for Java"&gt;{{cite web|title=MATLAB Engine API for Java|url=http://www.mathworks.com/help/matlab/matlab-engine-api-for-java.html|publisher=MathWorks|accessdate=September 15, 2016}}&lt;/ref&gt;

As alternatives to the [[MuPAD]] based Symbolic Math Toolbox available from MathWorks, MATLAB can be connected to [[Maple (software)|Maple]] or [[Mathematica]].&lt;ref&gt;{{cite web|title=MaMa: Calling MATLAB from Mathematica with MathLink|url=http://library.wolfram.com/infocenter/MathSource/618/|publisher=Wolfram Library Archive|first=Roger |last=Germundsson |work=[[Wolfram Research]] |date=September 30, 1998}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=MATLink:   Communicate with MATLAB from Mathematica|url=http://matlink.org/|accessdate=August 14, 2013|author1=rsmenon |author2=szhorvat|year=2013}}&lt;/ref&gt;

Libraries also exist to import and export [[MathML]].&lt;ref&gt;{{cite web |first=Michael |last=Weitzel |url=http://www.mathworks.com/matlabcentral/fileexchange/7709-mathml-importexport |title=MathML import/export |publisher=MathWorks - File Exchange |date=September 1, 2006 |accessdate=August 14, 2013}}&lt;/ref&gt;

== License ==
MATLAB is a [[Proprietary software|proprietary]] product of MathWorks, so users are subject to [[vendor lock-in]].&lt;ref name="eetimes2004"&gt;{{cite news |url=http://www.eetimes.com/document.asp?doc_id=1151422 |title=Matlab edges closer to electronic design automation world |work=EE Times |first=Richard |last=Goering |date=October 4, 2004}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=The Wrong Choice: Locked in by license restrictions|url=http://searchenterpriselinux.techtarget.com/news/902076/The-Wrong-Choice-Locked-in-by-license-restrictions|publisher=SearchOpenSource.com|accessdate=August 14, 2013|first=Jan |last=Stafford|date=May 21, 2003}}&lt;/ref&gt;  Although MATLAB Builder products can deploy MATLAB functions as library files which can be used with [[.NET Framework|.NET]]&lt;ref&gt;{{cite web|title=MATLAB Builder NE|url=http://www.mathworks.com/products/netbuilder/|publisher=MathWorks|accessdate=August 14, 2013}}&lt;/ref&gt; or [[Java (software platform)|Java]]&lt;ref&gt;{{cite web|title=MATLAB Builder JA|url=http://www.mathworks.com/products/javabuilder/|publisher=MathWorks|accessdate=August 14, 2013}}&lt;/ref&gt; application building environment, future development will still be tied to the MATLAB language.

Each toolbox is purchased separately. If an evaluation license is requested, the MathWorks sales department requires detailed information about the project for which MATLAB is to be evaluated. If granted (which it often is), the evaluation license is valid for two to four weeks. A student version of MATLAB is available as is a home-use license for MATLAB, Simulink, and a subset of Mathwork's Toolboxes at substantially reduced prices.

It has been reported that [[European Union]] (EU) competition regulators are investigating whether MathWorks refused to sell licenses to a competitor.&lt;ref&gt;{{cite web|title=MathWorks Software Licenses Probed by EU Antitrust Regulators|url=https://www.bloomberg.com/news/2012-03-01/mathworks-software-licenses-probed-by-eu-antitrust-regulators.html|publisher=Bloomberg news|date=March 1, 2012}}&lt;/ref&gt;  The regulators dropped the investigation after the complainant withdrew its accusation and no evidence of wrongdoing was found.&lt;ref&gt;{{cite web|title=EU regulators scrap antitrust case against MathWorks|url=https://www.reuters.com/article/2014/09/02/us-eu-mathworks-antitrust-idUSKBN0GX14V20140902|publisher=Reuters|date=September 2, 2014}}&lt;/ref&gt;

== Alternatives ==
{{See also|list of numerical analysis software|comparison of numerical analysis software}}

MATLAB has a number of competitors.&lt;ref&gt;{{cite web|title=Comparison of mathematical programs for data analysis|url=http://www.scientificweb.de/ncrunch/|first=Stefan |last=Steinhaus|date=February 24, 2008}}&lt;/ref&gt; Commercial competitors include [[Mathematica]], [[TK Solver]], [[Maple software|Maple]], and [[IDL (programming language)|IDL]]. There are also [[free software|free]] [[open source software|open source]] alternatives to MATLAB, in particular [[GNU Octave]], [[Scilab]], [[FreeMat]], and [[SageMath]], which are intended to be mostly compatible with the MATLAB language; the [[Julia (programming language)|Julia]] programming language also initially used MATLAB-like syntax. Among other languages that treat arrays as basic entities (array programming languages) are [[APL (programming language)|APL]], [[Fortran]] 90 and higher, [[S-Lang]], as well as the statistical languages [[R (programming language)|R]] and [[S (programming language)|S]]. There are also libraries to add similar functionality to existing languages, such as [[IT++]] for [[C++]], [[Perl Data Language]] for [[Perl]], [[ILNumerics.Net|ILNumerics]] for [[.NET Framework|.NET]], [[NumPy]]/[[SciPy]]/[[matplotlib]] for [[Python (programming language)|Python]], SciLua/[[Torch (machine learning)|Torch]] for [[Lua (programming language)|Lua]], SciRuby for [[Ruby (programming language)|Ruby]], and Numeric.js for [[JavaScript]].

[[GNU Octave]] is unique from other alternatives because it treats incompatibility with MATLAB as a bug (see [[GNU Octave#Matlab|MATLAB Compatibility of GNU Octave]]), therefore, making [[GNU Octave]] a superset of the MATLAB language.

== Release history ==
{| class="wikitable"
|-
! Version&lt;ref name="growth"&gt;{{cite web|title=The Growth of MATLAB and The MathWorks over Two Decades|url=http://www.mathworks.com/company/newsletters/articles/the-growth-of-matlab-and-the-mathworks-over-two-decades.html|work=News &amp; Notes Newsletter|publisher=MathWorks|accessdate=August 14, 2013|first=Cleve |last=Moler|date=January 2006}}&lt;/ref&gt; !! Release name !! Number !! Bundled [[Java virtual machine|JVM]] !! Year !! Release date !! Notes
|-
| MATLAB 1.0
|
|
|
| 1984
|
|
|-
| MATLAB  2
|
|
|
| 1986
|
|
|-
| MATLAB  3
|
|
|
| 1987
|
|
|-
| MATLAB  3.5
|
|
|
| 1990
|
| Ran on [[DOS]] but needed at least a [[Intel 80386|386]] processor; version 3.5m needed [[80387|math coprocessor]]
|-
| MATLAB  4
|
|
|
| 1992
|
| Ran on [[Windows 3.1x]] and Macintosh
|-
| MATLAB  4.2c
|
|
|
| 1994
|
| Ran on Windows 3.1x, needed a [[math coprocessor]]
|-
| MATLAB 5.0
| Volume 8
|
|
| 1996
| December 1996
| Unified releases across all platforms
|-
| MATLAB 5.1
| Volume 9
|
|
| rowspan=2| 1997
| May 1997
|
|-
| MATLAB 5.1.1
| R9.1
|
|
|
|
|-
| MATLAB 5.2
| R10
|
|
| rowspan=2| 1998
| March 1998
| Last version working on classic Macs
|-
| MATLAB 5.2.1
| R10.1
|
|
|
|
|-
| MATLAB 5.3
| R11
|
|
| rowspan=2| 1999
| January 1999
|
|-
| MATLAB 5.3.1
| R11.1
|
|
| November 1999
|
|-
| MATLAB 6.0
| R12
| rowspan=2|12
| 1.1.8
| 2000
| November 2000
| First release with bundled Java virtual machine (JVM)
|-
| MATLAB 6.1
| R12.1
| 1.3.0
| 2001
| June 2001
|
|-
| MATLAB 6.5
| R13
| rowspan=3|13
| 1.3.1
| 2002
| July 2002
|
|-
| MATLAB 6.5.1
| R13SP1
|
| rowspan=2| 2003
|
|
|-
| MATLAB 6.5.2
| R13SP2
|
|
| Last release for IBM/AIX, Alpha/TRU64, and SGI/IRIX&lt;ref&gt;{{cite web|title=MATLAB System Requirements - Release 13|url=http://www.mathworks.com/support/sysreq/release13/unix.html|publisher=MathWorks|accessdate=October 6, 2015}}&lt;/ref&gt;
|-
| MATLAB 7
| R14
| rowspan=4| 14
| 1.4.2
| rowspan=2| 2004
| June 2004
| Introduced anonymous and nested functions&lt;ref&gt;{{cite web|title=Dynamic Function Creation with Anonymous and Nested Functions|url=http://www.mathworks.com/company/newsletters/articles/dynamic-function-creation-with-anonymous-and-nested-functions.html|publisher=MathWorks|accessdate=January 15, 2016}}&lt;/ref&gt;&lt;br /&gt;
Re-introduced for Mac (under Mac OS X)
|-
| MATLAB 7.0.1
| R14SP1
|
| October 2004
|
|-
| MATLAB 7.0.4
| R14SP2
| 1.5.0
| rowspan=2| 2005
| March 7, 2005
| Support for memory-mapped files&lt;ref&gt;{{cite web|title=Memory Mapping|url=http://www.mathworks.com/help/matlab/memory-mapping.html|publisher=MathWorks|accessdate=January 22, 2014}}&lt;/ref&gt; 
|-
| MATLAB 7.1
| R14SP3
| 1.5.0
| September 1, 2005
|
|-
| MATLAB 7.2
| R2006a
| 15
| 1.5.0
| rowspan=2| 2006
| March 1, 2006
|
|-
| MATLAB 7.3
| R2006b
| 16
| 1.5.0
| September 1, 2006
| [[Hierarchical Data Format|HDF5]]-based MAT-file support
|-
| MATLAB 7.4
| R2007a
| 17
| 1.5.0_07
| rowspan=2| 2007
| March 1, 2007
| New &lt;code&gt;bsxfun&lt;/code&gt; function to apply element-by-element binary operation with singleton expansion enabled&lt;ref&gt;{{cite web|title=MATLAB bsxfun|url=http://www.mathworks.com/help/matlab/ref/bsxfun.html|publisher=MathWorks|accessdate=January 22, 2014}}&lt;/ref&gt; 
|-
| MATLAB 7.5
| R2007b
| 18
| 1.6.0
| September 1, 2007
| Last release for Windows 2000 and [[PowerPC]] Mac; License Server support for Windows Vista;&lt;ref name="matsol"&gt;{{cite web|title=Do MATLAB versions prior to R2007a run under Windows Vista?|url=http://www.mathworks.com/support/solutions/en/data/1-43EHE5/|publisher=MathWorks|accessdate=February 8, 2011|date=September 3, 2010}}&lt;/ref&gt; new internal format for P-code
|-
| MATLAB 7.6
| R2008a
| 19
| 1.6.0
| rowspan=2| 2008
| March 1, 2008
| Major enhancements to object-oriented programming abilities with a new class definition syntax,&lt;ref&gt;{{cite web |url=http://www.mathworks.com/help/matlab/matlab_oop/compatibility-with-previous-versions-.html |title=OOP Compatibility with Previous Versions |publisher=MathWorks |accessdate=March 11, 2013}}&lt;/ref&gt; and ability to manage namespaces with packages&lt;ref&gt;{{cite web|title=Packages Create Namespaces|url=http://www.mathworks.com/help/matlab/matlab_oop/scoping-classes-with-packages.html|publisher=MathWorks|accessdate=January 22, 2014}}&lt;/ref&gt;
|-
| MATLAB 7.7
| R2008b
| 20
| 1.6.0_04
| October 9, 2008
| New Map data structure:&lt;ref&gt;{{cite web|title=Map Containers|url=http://www.mathworks.com/help/matlab/map-containers.html|publisher=MathWorks|accessdate=January 22, 2014}}&lt;/ref&gt; upgrades to random number generators&lt;ref&gt;{{cite web|title=Creating and Controlling a Random Number Stream|url=http://www.mathworks.com/help/matlab/math/creating-and-controlling-a-random-number-stream.html|publisher=MathWorks|accessdate=January 22, 2014}}&lt;/ref&gt; 
|-
| MATLAB 7.8
| R2009a
| 21
| 1.6.0_04
| rowspan=2| 2009
| March 6, 2009
| First release for Microsoft 32-bit &amp; 64-bit Windows 7, new external interface to .NET Framework&lt;ref&gt;{{cite web|title=New MATLAB External Interfacing Features in R2009a|url=http://www.mathworks.com/support/2013b/matlab/8.2/demos/New-MATLAB-External-Interfacing-Features-in-R2009a.html|publisher=MathWorks|accessdate=January 22, 2014}}&lt;/ref&gt; 
|-
| MATLAB 7.9
| R2009b
| rowspan=2| 22
| 1.6.0_12
| September 4, 2009
| First release for [[Mac OS X Snow Leopard#64-bit architecture|Intel 64-bit Mac]], and last for [[Solaris (operating system)|Solaris]] [[SPARC]]; new use for the tilde operator (&lt;code&gt;~&lt;/code&gt;) to ignore arguments in function calls&lt;ref&gt;{{cite web|title=Ignore Function Outputs|url=http://www.mathworks.com/help/matlab/matlab_prog/ignore-function-outputs.html|publisher=MathWorks|accessdate=January 22, 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Ignore Function Inputs|url=http://www.mathworks.com/help/matlab/matlab_prog/ignore-function-inputs.html|publisher=MathWorks|accessdate=January 22, 2014}}&lt;/ref&gt;
|-
| MATLAB 7.9.1
| R2009bSP1
| 1.6.0_12
| rowspan=3| 2010
| April 1, 2010
| bug fixes.
|-
| MATLAB 7.10
| R2010a
| 23
| 1.6.0_12
| March 5, 2010
| Last release for [[Apple–Intel architecture|Intel 32-bit Mac]]
|-
| MATLAB 7.11
| R2010b
| rowspan=3|24
| 1.6.0_17
| September 3, 2010
| Add support for enumerations&lt;ref&gt;{{cite web|title=Working with Enumerations|url=http://www.mathworks.com/help/matlab/matlab_oop/enumerations.html|publisher=MathWorks|accessdate=January 22, 2014}}&lt;/ref&gt; 
|-
| MATLAB 7.11.1
| R2010bSP1
| 1.6.0_17
| rowspan=4| 2011
| March 17, 2011
| bug fixes and updates
|-
| MATLAB 7.11.2
| R2010bSP2
| 1.6.0_17
| April 5, 2012&lt;ref&gt;{{cite web|title=What's New in Release 2010b|url=http://www.mathworks.com/products/new_products/release2010b.html|publisher=MathWorks|accessdate=January 22, 2014}}&lt;/ref&gt;
| bug fixes
|-
| MATLAB 7.12
| R2011a
| 25
| 1.6.0_17
| April 8, 2011
| New &lt;code&gt;rng&lt;/code&gt; function to control random number generation&lt;ref&gt;{{cite web|title=New RNG Function for Controlling Random Number Generation in Release 2011a|url=http://www.mathworks.com/support/2013b/matlab/8.2/demos/new-rng-function-in-r2011a.html|publisher=MathWorks|accessdate=January 22, 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=MATLAB rng|url=http://www.mathworks.com/help/matlab/ref/rng.html|publisher=MathWorks|accessdate=January 22, 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Replace Discouraged Syntaxes of rand and randn|url=http://www.mathworks.com/help/matlab/math/updating-your-random-number-generator-syntax.html|publisher=MathWorks|accessdate=January 22, 2014}}&lt;/ref&gt;
|-
| MATLAB 7.13
| R2011b
| 26
| 1.6.0_17
| September 1, 2011
| Access-change parts of variables directly in MAT-files, without loading into memory;&lt;ref&gt;{{cite web|title=MATLAB matfile|url=http://www.mathworks.com/help/matlab/ref/matfile.html|publisher=MathWorks|accessdate=January 22, 2014}}&lt;/ref&gt; increased maximum local workers with Parallel Computing Toolbox from 8 to 12&lt;ref&gt;{{cite web|title=MATLAB max workers|url=http://www.mathworks.com/matlabcentral/answers/25987|accessdate=January 22, 2014}}&lt;/ref&gt;
|-
| MATLAB 7.14
| R2012a
| 27
| 1.6.0_17
| rowspan=2| 2012
| March 1, 2012
| Last version with 32-bit Linux support.&lt;ref&gt;{{cite web |url=https://www.mathworks.com/matlabcentral/answers/222489-is-matlab-supported-on-32-bit-linux|title=Is MATLAB supported on 32-bit Linux? |author=MathWorks Support Team |date=June 4, 2015 |quote=Versions of MATLAB prior to R2012a are fully supported on 32-bit Linux. After R2012a, MATLAB is no longer supported on 32-bit Linux.}}&lt;/ref&gt;
|-
| MATLAB 8
| R2012b
| 28
| 1.6.0_17
| September 11, 2012
| First release with [[Ribbon (computing)|Toolstrip]] interface;&lt;ref&gt;{{cite web |url=http://blogs.mathworks.com/loren/2012/09/12/the-matlab-r2012b-desktop-part-1-introduction-to-the-toolstrip/ |title=The MATLAB R2012b Desktop – Part 1: Introduction to the Toolstrip |first=Loren |last=Shure |date=September 2012}}&lt;/ref&gt; MATLAB Apps.&lt;ref&gt;{{cite web |url=http://www.mathworks.com/discovery/matlab-apps.html |title=MATLAB Apps |publisher=MathWorks |accessdate=August 14, 2013}}&lt;/ref&gt; redesigned documentation system
|-
| MATLAB 8.1
| R2013a
| 29
| 1.6.0_17
| rowspan=2| 2013
| March 7, 2013
| New [[unit testing]] framework&lt;ref&gt;{{cite web |url=http://www.mathworks.com/help/matlab/matlab-unit-test-framework.html |title=MATLAB Unit Testing Framework |publisher=MathWorks |accessdate=August 14, 2013}}&lt;/ref&gt;
|-
| MATLAB 8.2
| R2013b
| 30
| 1.7.0_11
| September 6, 2013&lt;ref&gt;{{cite web | url = http://www.mathworks.com/company/newsroom/mathworks-announces-release-2013b-of-the-matlab-and-simulink-product-families.html | title = MathWorks Announces Release 2013b of the MATLAB and Simulink Product Families | publisher=MathWorks |date=September 2013 }}&lt;/ref&gt;
| Built in Java Runtime Environment (JRE) updated to version 7;&lt;ref&gt;{{cite web|title=R2013b Release Notes|url=https://www.mathworks.com/help/matlab/release-notes.html?rntext=&amp;startrelease=R2013b&amp;endrelease=R2013b&amp;category=desktop|publisher=MathWorks|accessdate=September 17, 2018}}&lt;/ref&gt; New table data type&lt;ref&gt;{{cite web|title=MATLAB Tables|url=http://www.mathworks.com/help/matlab/tables.html|publisher=MathWorks|accessdate=September 14, 2013}}&lt;/ref&gt; 
|-
| MATLAB 8.3
| R2014a
| 31
| 1.7.0_11
| rowspan=2| 2014
| March 7, 2014&lt;ref&gt;{{cite web|title=MathWorks Announces Release 2014a of the MATLAB and Simulink Product Families|url=http://www.mathworks.com/company/newsroom/mathworks-announces-release-2014a-of-the-matlab-and-simulink-product-families.html|publisher=MathWorks|accessdate=March 11, 2014}}&lt;/ref&gt;
| Simplified compiler setup for building MEX-files; USB Webcams support in core MATLAB; number of local workers no longer limited to 12 with Parallel Computing Toolbox
|-
| MATLAB 8.4
| R2014b
| 32
| 1.7.0_11
| October 3, 2014
| New class-based graphics engine (a.k.a. HG2);&lt;ref&gt;{{cite web|title=Graphics Changes in R2014b|url=http://www.mathworks.com/help/matlab/graphics-changes-in-r2014b.html|publisher=MathWorks|accessdate=October 3, 2014}}&lt;/ref&gt; tabbing function in GUI;&lt;ref&gt;{{cite web|title=uitab: Create tabbed panel|url=http://www.mathworks.com/help/matlab/ref/uitab.html|publisher=MathWorks|accessdate=October 3, 2014}}&lt;/ref&gt; improved user toolbox packaging and help files;&lt;ref&gt;{{cite web|title=Create and Share Toolboxes|url=http://www.mathworks.com/help/matlab/matlab_prog/create-and-share-custom-matlab-toolboxes.html|publisher=MathWorks|accessdate=October 3, 2014}}&lt;/ref&gt; new objects for time-date manipulations;&lt;ref&gt;{{cite web|title=Dates and Time|url=http://www.mathworks.com/help/matlab/date-and-time-operations.html|publisher=MathWorks|accessdate=October 3, 2014}}&lt;/ref&gt; [[Git (software)|Git]]-[[Apache Subversion|Subversion]] integration in IDE;&lt;ref&gt;{{cite web|title=Source Control Integration|url=http://www.mathworks.com/help/matlab/source-control.html|publisher=MathWorks|accessdate=October 3, 2014}}&lt;/ref&gt; [[big data]] abilities with [[MapReduce]] (scalable to [[Apache Hadoop|Hadoop]]);&lt;ref&gt;{{cite web|title=MATLAB MapReduce and Hadoop|url=http://www.mathworks.com/discovery/matlab-mapreduce-hadoop.html|publisher=MathWorks|accessdate=October 3, 2014}}&lt;/ref&gt; new &lt;code&gt;py&lt;/code&gt; package for using [[Python (programming language)|Python]] from inside MATLAB,&lt;ref&gt;{{cite web|title=Call Python Libraries|url=http://www.mathworks.com/help/matlab/call-python-libraries.html|publisher=MathWorks|accessdate=October 3, 2014}}&lt;/ref&gt; new engine interface to call MATLAB from Python;&lt;ref&gt;{{cite web|title=MATLAB Engine for Python|url=http://www.mathworks.com/help/matlab/matlab-engine-for-python.html|publisher=MathWorks|accessdate=October 3, 2014}}&lt;/ref&gt; several new and improved functions: &lt;code&gt;webread&lt;/code&gt; (RESTful web services with JSON/XML support), &lt;code&gt;tcpclient&lt;/code&gt; (socket-based connections), &lt;code&gt;histcounts&lt;/code&gt;, &lt;code&gt;histogram&lt;/code&gt;, &lt;code&gt;animatedline&lt;/code&gt;, and others
|-
| MATLAB 8.5
| R2015a
| rowspan=2 | 33
| 1.7.0_60
| rowspan=3 | 2015
| March 5, 2015
| Last release supporting Windows XP and Windows Vista
|-
| MATLAB 8.5
| R2015aSP1
| 1.7.0_60
| October 14, 2015
| 
|-
| MATLAB 8.6
| R2015b
| 34
| 1.7.0_60
| September 3, 2015
| New MATLAB execution engine (a.k.a. LXE);&lt;ref&gt;{{cite web|title=MATLAB Execution Engine|url=http://www.mathworks.com/products/matlab/matlab-execution-engine/|publisher=MathWorks|accessdate=September 15, 2016}}&lt;/ref&gt; &lt;code&gt;graph&lt;/code&gt; and &lt;code&gt;digraph&lt;/code&gt; classes to work with graphs and networks;&lt;ref&gt;{{cite web|title=Graph and Network Algorithms|url=http://www.mathworks.com/help/matlab/graph-and-network-algorithms.html|publisher=MathWorks|accessdate=September 15, 2016}}&lt;/ref&gt; MinGW-w64 as supported compiler on Windows;&lt;ref&gt;{{cite web|title=Install MinGW-w64 Compiler|url=http://www.mathworks.com/help/matlab/matlab_external/install-mingw-support-package.html|publisher=MathWorks|accessdate=September 15, 2016}}&lt;/ref&gt; Last version with 32-bit support
|-
| MATLAB 9.0
| R2016a
| 35
| 1.7.0_60
| rowspan=2 | 2016
| March 3, 2016
| Live Scripts: interactive documents that combine text, code, and output (in the style of [[Literate programming]]);&lt;ref&gt;{{cite web|title=What Is a Live Script?|url=http://www.mathworks.com/help/matlab/matlab_prog/what-is-a-live-script.html|publisher=MathWorks|accessdate=September 15, 2016}}&lt;/ref&gt;  App Designer: a new development environment for building apps (with new kind of UI figures, axes, and components);&lt;ref&gt;{{cite web|title=MATLAB App Designer|url=http://www.mathworks.com/products/matlab/app-designer/|publisher=MathWorks|accessdate=September 15, 2016}}&lt;/ref&gt; pause execution of running programs using a Pause Button
|-
| MATLAB 9.1
| R2016b
| 36
| 1.7.0_60
| September 15, 2016
| define local functions in scripts;&lt;ref&gt;{{cite web|title=Add Functions to Scripts|url=http://www.mathworks.com/help/matlab/matlab_prog/local-functions-in-scripts.html|publisher=MathWorks|accessdate=September 15, 2016}}&lt;/ref&gt; automatic expansion of dimensions (previously provided via explicit call to &lt;code&gt;bsxfun&lt;/code&gt;); &lt;code&gt;tall&lt;/code&gt; arrays for [[Big data]];&lt;ref&gt;{{cite web|title=Tall Arrays|url=http://www.mathworks.com/help/matlab/tall-arrays.html|publisher=MathWorks|accessdate=September 15, 2016}}&lt;/ref&gt; new &lt;code&gt;string&lt;/code&gt; type;&lt;ref&gt;{{cite web|title=Create String Arrays|url=http://www.mathworks.com/help/matlab/matlab_prog/create-string-arrays.html|publisher=MathWorks|accessdate=September 15, 2016}}&lt;/ref&gt; new functions to encode/decode [[JSON]];&lt;ref&gt;{{Cite web|url=http://mathworks.com/help/matlab/json-format.html|title=JSON Format - MATLAB &amp; Simulink|website=mathworks.com|access-date=August 20, 2017}}&lt;/ref&gt; official MATLAB Engine API for Java&lt;ref name="MATLAB Engine API for Java"/&gt;
|-
| MATLAB 9.2
| R2017a
| 37
| 1.7.0_60
| rowspan=2 | 2017
| March 9, 2017
| MATLAB Online: cloud-based MATLAB desktop accessed in a web browser;&lt;ref&gt;{{cite web|title=MATLAB Online|url=https://www.mathworks.com/products/matlab-online.html|publisher=MathWorks|accessdate=April 10, 2017}}&lt;/ref&gt;  double-quoted strings; new &lt;code&gt;memoize&lt;/code&gt; function for [[Memoization]]; expanded object properties validation;&lt;ref&gt;{{cite web|title=Validate Property Values|url=https://www.mathworks.com/help/matlab/matlab_oop/validate-property-values.html|publisher=MathWorks|accessdate=April 10, 2017}}&lt;/ref&gt;  [[Mock object|mocking]] framework for unit testing;&lt;ref&gt;{{cite web|title=Mocking Framework|url=https://www.mathworks.com/help/matlab/mocking-framework.html|publisher=MathWorks|accessdate=April 10, 2017}}&lt;/ref&gt;  MEX targets 64-bit by default;  new &lt;code&gt;heatmap&lt;/code&gt; function for creating [[Heat map|heatmap charts]]&lt;ref&gt;{{cite web|title=Create Heatmap from Tabular Data|url=https://www.mathworks.com/help/matlab/creating_plots/create-heatmap-from-tabular-data.html|publisher=MathWorks|accessdate=April 10, 2017}}&lt;/ref&gt;
|-
| MATLAB 9.3
| R2017b
| 38
| 1.8.0_121
| September 21, 2017
| 
|-
| MATLAB 9.4
| R2018a
| 39
| 1.8.0_144
| rowspan=2 | 2018
| March 15, 2018&lt;ref&gt;{{cite web|title=MathWorks Announces Release 2018a of the MATLAB and Simulink Product Families|url=https://www.mathworks.com/company/newsroom/mathworks-announces-release-2018a-of-the-matlab-and-simulink-product-families.html|publisher=MathWorks|accessdate=April 5, 2018}}&lt;/ref&gt;
|
|-
| MATLAB 9.5
| R2018b
| 40
| 1.8.0_152
| September 12, 2018
|
|}

The number (or release number) is the version reported by Concurrent License Manager program [[FlexNet Publisher|FLEXlm]].

For a complete list of changes of both MATLAB and official toolboxes, consult the MATLAB release notes.&lt;ref&gt;{{cite web|title=MATLAB Release Notes|url=http://www.mathworks.com/help/relnotes/index.html|publisher=MathWorks|accessdate=January 25, 2014}}&lt;/ref&gt;

== File extensions ==

=== MATLAB ===
; .m : MATLAB code (function, script, or class)
; .mat : MATLAB data (binary file for storing variables)
; .mex* (.mexw32, .mexw64, .mexglx, .mexa64, .mexmaci64, ...) : MATLAB executable MEX-files&lt;ref&gt;{{cite web |title = Introducing MEX-Files |url = http://www.mathworks.com/help/matlab/matlab_external/introducing-mex-files.html |publisher = MathWorks |accessdate = August 14, 2013 }}&lt;/ref&gt; (platform specific, e.g. ".mexmac" for the [[Macintosh|Mac]], ".mexglx" for [[Linux]], etc.&lt;ref&gt;{{cite web |title = Binary MEX-File Extensions |url = http://www.mathworks.com/help/matlab/matlab_external/using-mex-files-to-call-c-c-and-fortran-programs.html#bra56dy-1 |publisher = MathWorks |accessdate = August 14, 2013 }}&lt;/ref&gt;)
; .p : MATLAB content-obscured .m file (P-code&lt;ref&gt;{{cite web |title = Protect Your Source Code |url = http://www.mathworks.com/help/matlab/matlab_prog/protect-your-source-code.html |publisher = MathWorks |accessdate = August 14, 2013 }}&lt;/ref&gt;)
; .mlx : MATLAB live script&lt;ref&gt;{{cite web|title=What Is a Live Script?|url=http://www.mathworks.com/help/matlab/matlab_prog/what-is-a-live-script.html|publisher=MathWorks|accessdate=August 21, 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Live Script File Format (.mlx)|url=http://www.mathworks.com/help/matlab/matlab_prog/live-script-file-format.html|publisher=MathWorks|accessdate=August 21, 2016}}&lt;/ref&gt;
; .fig : MATLAB figures (created with GUIDE)
; .mlapp : MATLAB apps (created with App Designer&lt;ref&gt;{{cite web|title=MATLAB App Designer|url=http://www.mathworks.com/products/matlab/app-designer/|publisher=MathWorks|accessdate=August 21, 2016}}&lt;/ref&gt;)
; .mlappinstall : MATLAB packaged App Installer&lt;ref&gt;{{cite web |title = MATLAB App Installer File |url = http://www.mathworks.com/help/matlab/creating_guis/what-is-an-app.html |publisher = MathWorks |accessdate = August 14, 2013 }}&lt;/ref&gt;
; .mlpkginstall: support package installer (add-on for third-party hardware)&lt;ref&gt;{{cite web |title = Support Package Installation |url = http://www.mathworks.com/help/matlab/matlab_external/support-package-installation.html |publisher = MathWorks |accessdate = October 3, 2014 }}&lt;/ref&gt;
; .mltx, .mltbx: packaged custom toolbox&lt;ref&gt;{{cite web |title = Manage Toolboxes |url = http://www.mathworks.com/help/matlab/matlab_prog/manage-toolboxes.html |publisher = MathWorks |accessdate = October 3, 2014 }}&lt;/ref&gt;&lt;ref&gt;{{cite web |title = Toolbox Distribution |url = http://www.mathworks.com/help/matlab/creating-help.html |publisher = MathWorks |accessdate = August 6, 2016 }}&lt;/ref&gt;&lt;ref&gt;{{cite web |title = What are MATLAB toolboxes? |url = https://www.lynda.com/MATLAB-tutorials/What-MATLAB-toolboxes/124067/138192-4.html |publisher = [[Lynda.com]] |accessdate = August 6, 2016 }}&lt;/ref&gt;
; .prj: project file used by various solutions (packaged app/toolbox projects, MATLAB Compiler/Coder projects, Simulink projects)
; .rpt: report setup file created by MATLAB Report Generator&lt;ref&gt;{{cite web |title = MATLAB Report Generator |url = http://www.mathworks.com/products/ML_reportgenerator/ |publisher = MathWorks |accessdate = October 3, 2014 }}&lt;/ref&gt;

=== Simulink ===
; .mdl : Simulink Model
; .mdlp : Simulink Protected Model
; .slx : Simulink Model (SLX format)
; .slxp : Simulink Protected Model (SLX format)

=== Simscape ===
; .ssc : Simscape&lt;ref&gt;{{cite web|title=Simscape|url=http://www.mathworks.com/products/simscape/|publisher=MathWorks|accessdate=August 14, 2013}}&lt;/ref&gt; Model

=== MuPAD ===
; .mn : MuPAD [[Notebook interface|Notebook]]
; .mu : MuPAD Code
; .xvc, .xvz : MuPAD Graphics

=== Third-party ===
; .jkt : GPU Cache file generated by Jacket for MATLAB (AccelerEyes)
; .mum : MATLAB CAPE-OPEN Unit Operation Model File (AmsterCHEM)

== Easter eggs ==
Several [[Easter egg (media)|easter eggs]] exist in MATLAB.&lt;ref&gt;{{cite web|url=http://www.mathworks.com/matlabcentral/answers/2001-what-matlab-easter-eggs-do-you-know |title=What MATLAB Easter eggs do you know? |publisher=MathWorks - MATLAB Answers |date=February 25, 2011|accessdate=August 14, 2013}}&lt;/ref&gt; These include hidden pictures,&lt;ref&gt;{{cite web|title=The Story Behind the MATLAB Default Image|url=http://blogs.mathworks.com/steve/2006/10/17/the-story-behind-the-matlab-default-image/|accessdate=August 14, 2013|first=Steve |last=Eddins|date=October 17, 2006}}&lt;/ref&gt; and jokes. For example, typing in "spy" used to generate a picture of the spies from [[Spy vs Spy]], but now displays an image of a dog. Typing in "why" randomly outputs a philosophical answer. Other commands include "penny", "toilet", "image", and "life".  Not every Easter egg appears in every version of MATLAB.

{{clear}}

== See also ==
* [[Comparison of numerical analysis software]]
* [[List of numerical analysis software]]

== Notes ==
{{Reflist|30em}}

== References ==
{{Refbegin}}
* {{cite book |last=Gilat |first=Amos |authorlink= |title=MATLAB: An Introduction with Applications 2nd Edition |year=2004 |publisher=John Wiley &amp; Sons |location= |isbn= 978-0-471-69420-5 }}
* {{cite book |last=Quarteroni |first=Alfio |authorlink= |first2=Fausto |last2=Saleri |title=Scientific Computing with MATLAB and Octave |year=2006 |publisher=Springer |location= |isbn= 978-3-540-32612-0 }}
* {{cite book |last=Ferreira |first=A.J.M. |authorlink= |title=MATLAB Codes for Finite Element Analysis |year=2009 |publisher=Springer |location= |isbn= 978-1-4020-9199-5 }}
* {{cite book |last=Lynch |first=Stephen |authorlink= |title=Dynamical Systems with Applications using MATLAB |year=2004 |publisher=Birkhäuser |location= |isbn=978-0-8176-4321-8 }}
{{Refend}}

== External links ==
{{Wikibooks|MATLAB Programming}}
{{Commons category|MATLAB}}
{{Wikiversity|MATLAB essential}}
* {{Official website|https://www.mathworks.com/products/matlab.html}}
*{{Dmoz|Science/Math/Software/MATLAB|MATLAB}}
*[http://realtechnologytools.com/category/articles/ MATLAB Tutorial]



&lt;!-- Please do not add any links to NI and/or LabVIEW. Instead, argue your case on the talk page --&gt;

{{Numerical analysis software}}
{{Computer algebra systems}}
{{Statistical software}}
{{Image Processing Software}}
{{Linear algebra}}

{{DEFAULTSORT:Matlab}}
[[Category:Array programming languages]]
[[Category:Articles with example MATLAB/Octave code]]
[[Category:C software]]
[[Category:Computer algebra system software for Linux]]
[[Category:Computer algebra system software for MacOS]]
[[Category:Computer algebra system software for Windows]]
[[Category:Computer algebra systems]]
[[Category:Computer vision software]]
[[Category:Cross-platform software]]
[[Category:Data mining and machine learning software]]
[[Category:Data visualization software]]
[[Category:Data-centric programming languages]]
[[Category:Dynamically typed programming languages]]
[[Category:Econometrics software]]
[[Category:High-level programming languages]]
[[Category:IRIX software]]
[[Category:Linear algebra]]
[[Category:Mathematical optimization software]]
[[Category:Numerical analysis software for Linux]]
[[Category:Numerical analysis software for MacOS]]
[[Category:Numerical analysis software for Windows]]
[[Category:Numerical linear algebra]]
[[Category:Numerical programming languages]]
[[Category:Numerical software]]
[[Category:Parallel computing]]
[[Category:Plotting software]]
[[Category:Proprietary commercial software for Linux]]
[[Category:Proprietary cross-platform software]]
[[Category:Regression and curve fitting software]]
[[Category:Software modeling language]]
[[Category:Statistical programming languages]]
[[Category:Time series software]]</text>
      <sha1>cmoj4vej75d65as0h2oteb6y0sanbri</sha1>
    </revision>
  </page>
  <page>
    <title>Magic formula investing</title>
    <ns>0</ns>
    <id>4607950</id>
    <revision>
      <id>870503557</id>
      <parentid>809619141</parentid>
      <timestamp>2018-11-25T07:01:07Z</timestamp>
      <contributor>
        <ip>155.93.141.85</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2017">'''Magic formula investing''' is an investment technique outlined by [[Joel Greenblatt]] that uses the principles of [[value investing]]. 

==Methodology==
Greenblatt suggests purchasing 30 "good companies": cheap stocks with a high [[earnings yield]] and a high [[return on capital]]. He touts the success of his magic formula in his book 'The Little Book that Beats the Market' ({{ISBN|0-471-73306-7}}), claiming that it does in fact beat the [[S&amp;P 500]] 96% of the time, and has averaged a 17-year annual return of 30.8%&lt;ref&gt;Zen, Brian and Hamai, Garrett. "[http://www.gurufocus.com/news.php?id=802 Joel Greenblatt Speaking at NYSSA]". December 28, 2005.&lt;/ref&gt;

===Formula===
#Establish a minimum [[market capitalization]] (usually greater than $50 million).
#Exclude [[Public utility|utility]] and [[financial]] stocks. 
#Exclude foreign companies ([[American Depositary Receipt]]s).
#Determine company's [[earnings yield]] = [[earnings_before_interest_and_taxes|EBIT]] / [[enterprise value]].
#Determine company's [[return on capital]] = [[earnings_before_interest_and_taxes|EBIT]] / (net [[fixed assets]] + [[working capital]]).
#Rank all companies above chosen [[market capitalization]] by highest earnings yield and highest return on capital (ranked as [[percentage]]s).
#Invest in 20–30 highest ranked companies, accumulating 2–3 positions per month over a 12-month period.
#Re-balance [[Portfolio (finance)|portfolio]] once per year, selling losers one week before the year-mark and winners one week after the year mark.
#Continue over a long-term (5–10+ year) period.

== See also ==
* [[Piotroski F-Score]]

==References==
&lt;references/&gt;

==External links==
*[http://www.magicformulainvesting.com Official website]


[[Category:Finance theories]]
[[Category:Financial markets]]
[[Category:Market trends]]
[[Category:Mathematical finance]]
[[Category:Personal finance]]
[[Category:Financial risk management]]
[[Category:Securities (finance)]]
[[Category:Stock market]]
[[Category:Valuation (finance)]]</text>
      <sha1>k94tg9prgyns1pmldawit97w48p4psp</sha1>
    </revision>
  </page>
  <page>
    <title>Mahler's theorem</title>
    <ns>0</ns>
    <id>408111</id>
    <revision>
      <id>862709811</id>
      <parentid>852551549</parentid>
      <timestamp>2018-10-06T05:23:57Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Removing category Eponymous scientific concepts per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2018 September 22]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2047">{{distinguish|Mahler's compactness theorem}}
In mathematics, '''Mahler's theorem''', introduced by {{harvs|txt|authorlink=Kurt Mahler|first=Kurt|last=Mahler|year=1958}}, expresses continuous [[p-adic number|''p''-adic]] functions in terms of polynomials. Over any [[field (mathematics)|field]], one has the following result:

Let &lt;math&gt;(\Delta f)(x)=f(x+1)-f(x)&lt;/math&gt; be the forward [[difference operator]].  Then for [[polynomial function]]s ''f'' we have the [[Newton series]]

:&lt;math&gt;f(x)=\sum_{k=0}^\infty (\Delta^k f)(0){x \choose k},&lt;/math&gt;

where

:&lt;math&gt;{x \choose k}=\frac{x(x-1)(x-2)\cdots(x-k+1)}{k!}&lt;/math&gt;

is the ''k''th binomial coefficient polynomial.

Over the field of [[real numbers]], the assumption that the function ''f'' is a polynomial can be weakened, but it cannot be weakened all the way down to mere [[continuous function|continuity]]. Mahler's theorem states that if ''f'' is a continuous [[p-adic number|p-adic]]-valued function on the ''p''-adic integers then the same identity holds. The relationship between the operator Δ and this [[polynomial sequence]] is much like that between differentiation and the sequence whose ''k''th term is ''x''&lt;sup&gt;''k''&lt;/sup&gt;.

It is remarkable that as weak an assumption as continuity is enough; by contrast, Newton series on the field of [[complex numbers]] are far more tightly constrained, and require [[Carlson's theorem]] to hold. It is a fact of algebra that if ''f'' is a polynomial function with coefficients in any [[field (mathematics)|field]] of [[characteristic (algebra)|characteristic]] 0, the same identity holds where the sum has finitely many terms.

==References==
*{{Citation | last1=Mahler | first1=K. | title=An interpolation series for continuous functions of a p-adic variable | url=http://resolver.sub.uni-goettingen.de/purl?GDZPPN002177846 | mr=0095821 | year=1958 | journal=[[Journal für die reine und angewandte Mathematik]] | issn=0075-4102 | volume=199 | pages=23–34}}

[[Category:Factorial and binomial topics]]
[[Category:Theorems in analysis]]</text>
      <sha1>souoehfm5iohwv6tnceceflhb47fa2e</sha1>
    </revision>
  </page>
  <page>
    <title>McCarthy 91 function</title>
    <ns>0</ns>
    <id>222665</id>
    <revision>
      <id>846675759</id>
      <parentid>829865646</parentid>
      <timestamp>2018-06-20T06:59:21Z</timestamp>
      <contributor>
        <username>Bibcode Bot</username>
        <id>14394459</id>
      </contributor>
      <minor/>
      <comment>Adding 0 [[arXiv|arxiv eprint(s)]], 1 [[bibcode|bibcode(s)]] and 0 [[digital object identifier|doi(s)]]. Did it miss something? Report bugs, errors, and suggestions at [[User talk:Bibcode Bot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7832">{{no footnotes|date=October 2015}}
The '''McCarthy 91 function''' is a [[Recursion (computer science)|recursive function]], defined by the [[computer scientist]] [[John McCarthy (computer scientist)|John McCarthy]] as a test case for [[formal verification]] within [[computer science]].

The McCarthy 91 function is defined as

:&lt;math&gt;M(n)=\begin{cases}
 n - 10, &amp; \mbox{if }n &gt; 100\mbox{ } \\
 M(M(n+11)), &amp; \mbox{if }n \le 100\mbox{ }
\end{cases}&lt;/math&gt;

The results of evaluating the function are given by ''M''(''n'')&amp;nbsp;=&amp;nbsp;91 for all integer arguments ''n''&amp;nbsp;≤&amp;nbsp;100, and ''M''(''n'')&amp;nbsp;=&amp;nbsp;''n''&amp;nbsp;&amp;minus;&amp;nbsp;10 for ''n'' &amp;gt; 100. Indeed, the result of M(101) is also 91 (101 - 10 = 91). All results of M(n) after n = 101 are continually increasing by 1, e.g. M(102) = 92, M(103) = 93.

==History==
The 91 function was introduced in papers published by [[Zohar Manna]], [[Amir Pnueli]] and [[John McCarthy (computer scientist)|John McCarthy]] in 1970.  These papers represented early developments towards the application of [[formal methods]] to [[formal verification|program verification]].  The 91 function was chosen for being nested-recursive (contrasted with [[single recursion]], such as defining &lt;math&gt;f(n)&lt;/math&gt; by means of &lt;math&gt;f(n-1)&lt;/math&gt;).  The example was popularized by Manna's book, ''Mathematical Theory of Computation'' (1974). As the field of Formal Methods advanced, this example appeared repeatedly in the research literature.
In particular, it is viewed as a "challenge problem" for automated program verification.

It is easier to reason about [[tail recursion|tail-recursive]] control flow, this is an equivalent ([[extensionality|extensionally equal]]) definition:
:&lt;math&gt;Mt(n)= Mtaux(n,1)&lt;/math&gt;
:&lt;math&gt;Mtaux(n, c)=\begin{cases}
 n, &amp; \mbox{if }c = 0\\
 Mtaux(n-10, c-1), &amp; \mbox{if }n &gt; 100\mbox{ and } c \ne 0 \\
 Mtaux(n+11, c+1), &amp; \mbox{if }n \le 100\mbox{ and } c \ne 0
\end{cases}
&lt;/math&gt;
As one of the examples used to demonstrate such reasoning, Manna's book includes a tail-recursive algorithm equivalent to the nested-recursive 91 function. Many of the papers that report an "automated verification" (or [[termination proof]]) of the 91 function only handle the tail-recursive version.

This is an equivalent [[mutual recursion|mutually]] tail-recursive definition:
:&lt;math&gt;Mmt(n)= Mmtaux(n,0)&lt;/math&gt;
:&lt;math&gt;Mmtaux(n,c)=\begin{cases}
 Mmtaux2(n-10,c), &amp; \mbox{if }n &gt; 100\mbox{ } \\
 Mmtaux(n+11,c+1), &amp; \mbox{if }n \le 100\mbox{ }
\end{cases}&lt;/math&gt;
:&lt;math&gt;Mmtaux2(n,c)=\begin{cases}
 n, &amp; \mbox{if }c = 0\mbox{ } \\
 Mmtaux(n,c-1), &amp; \mbox{if }c \ne 0\mbox{ }
\end{cases}&lt;/math&gt;
A formal derivation of the mutually tail-recursive version from the nested-recursive one was given in a 1980 article by [[Mitchell Wand]], based on the use of [[continuation]]s.

==Examples==
Example A:

 M(99) = M(M(110)) since 99 ≤ 100
       = M(100)    since 110 &gt; 100
       = M(M(111)) since 100 ≤ 100
       = M(101)    since 111 &gt; 100
       = 91        since 101 &gt; 100

Example B:

 M(87) = M(M(98))
       = M(M(M(109)))
       = M(M(99))
       = M(M(M(110)))
       = M(M(100))
       = M(M(M(111)))
       = M(M(101))
       = M(91)
       = M(M(102))
       = M(92)
       = M(M(103))
       = M(93)
    .... Pattern continues increasing till M(99), M(100) and M(101), exactly as we saw on the example A)
       = M(101)    since 111 &gt; 100
       = 91        since 101 &gt; 100

==Code==
Here is an implementation of the nested-recursive algorithm in [[Lisp programming language|Lisp]]:

&lt;source lang="lisp"&gt;
 (defun mc91 (n)
   (cond ((&lt;= n 100) (mc91 (mc91 (+ n 11))))
         (t (- n 10))))
&lt;/source&gt;

Here is an implementation of the nested-recursive algorithm in [[Haskell (programming language)|Haskell]]:

&lt;source lang="haskell"&gt;
  mc91 n 
    | n &gt; 100   = n - 10
    | otherwise = mc91 $ mc91 $ n + 11
&lt;/source&gt;

Here is an implementation of the nested-recursive algorithm in [[OCaml (programming language)|OCaml]]:

&lt;source lang="ocaml"&gt;
  let rec mc91 n =
    if n &gt; 100 then n - 10
    else mc91 (mc91 (n + 11))
&lt;/source&gt;

Here is an implementation of the tail-recursive algorithm in [[OCaml (programming language)|OCaml]]:

&lt;source lang="ocaml"&gt;
  let mc91 n =
    let rec aux n c =
      if c = 0 then n
      else if n &gt; 100 then aux (n - 10) (c - 1)
      else aux (n + 11) (c + 1)
    in
    aux n 1
&lt;/source&gt;

Here is an implementation of the nested-recursive algorithm in [[Python (programming language)|Python]]:

&lt;source lang="python"&gt;
 def mc91(n):
    if n &gt; 100:
        return n - 10
    else:
        return mc91(mc91(n + 11))
&lt;/source&gt;

Here is an implementation of the nested-recursive algorithm in [[C (programming language)|C]]:

&lt;source lang="c"&gt;
 int mc91(int n)
 {
     if( n &gt; 100 ) {
         return n - 10;
     } else {
         return mc91(mc91(n+11));
     }
 }
&lt;/source&gt;

Here is an implementation of the tail-recursive algorithm in [[C (programming language)|C]]:

&lt;source lang="c"&gt;
 int mc91(int n)
 {
     mc91taux(n, 1);
 }

 int mc91taux(int n, int c)
 {
     if( c != 0 ) {
         if( n &gt; 100 ) {
             return mc91taux(n - 10, c - 1);
         } else {
             return mc91taux(n + 11, c + 1);
         }
     } else {
         return n;
     }
 }
&lt;/source&gt;

==Proof==
Here is a proof that the function is equivalent to non-recursive:
:&lt;math&gt;M(n)=\begin{cases}
 n - 10, &amp; \mbox{if }n &gt; 100\mbox{ } \\
 91, &amp; \mbox{if }n \le 100\mbox{ }
\end{cases}&lt;/math&gt;

For 90 ≤ ''n'' &amp;lt; 101,

 M(n) = M(M(n + 11))
      = M(n + 11 - 10), where n + 11 &gt;= 101 since n &gt;= 90
      = M(n + 1)

So ''M''(''n'') = 91 for 90 ≤ ''n'' &amp;lt; 101.

We can use this as a base case for [[Inductive proof|induction]] on blocks of 11 numbers, like so:

Assume that ''M''(''n'') = 91 for ''a'' ≤ ''n'' &amp;lt; ''a'' + 11.

Then, for any ''n'' such that ''a'' - 11 ≤ ''n'' &amp;lt; ''a'',

 M(n) = M(M(n + 11))
      = M(91), by hypothesis, since a ≤ n + 11 &lt; a + 11
      = 91, by the base case.

Now by induction ''M''(''n'') = 91 for any ''n'' in such a block. There are no holes between the blocks, so ''M''(''n'') = 91 for ''n'' &amp;lt; 101. We can also add ''n'' = 101 as a special case.

== Knuth's generalization ==

[[Donald Knuth]] generalized the 91 function to include additional parameters. [[John Cowles]] developed a formal proof that Knuth's generalized function was total, using the [[ACL2]] theorem prover.

== References ==
* {{cite journal | author=Zohar Manna and Amir Pnueli | title=Formalization of Properties of Functional Programs  | journal=Journal of the ACM |date=July 1970 | volume=17 | issue=3  | pages=555–569 | doi=10.1145/321592.321606}}
* {{cite journal | author=Zohar Manna and John McCarthy | title=Properties of programs and partial function logic  | journal=Machine Intelligence | year=1970 | volume=5 }}
* Zohar Manna. ''Mathematical Theory of Computation.'' McGraw-Hill Book Company, New-York, 1974. Reprinted in 2003 by Dover Publications.
* {{cite journal | author=Mitchell Wand | title=Continuation-Based Program Transformation Strategies | journal=Journal of the ACM |date=January 1980 | volume=27 | issue=1  | pages=164–180 | doi=10.1145/322169.322183}}
* {{cite journal | author = Donald E. Knuth | title = Textbook Examples of Recursion | year = 1991 | journal = Artificial intelligence and mathematical theory of computation | arxiv = cs/9301113| bibcode = 1993cs........1113K }}
* {{cite book | author = John Cowles | chapter = Knuth's generalization of McCarthy's 91 function
| title = Computer-Aided reasoning: ACL2 case studies | publisher = Kluwer Academic Publishers
| year = 2000 | pages = 283–299 | url = http://www.cs.utexas.edu/users/moore/acl2/workshop-1999/Cowles-abstract.html}}

{{John McCarthy navbox}}

[[Category:Formal methods]]
[[Category:Recurrence relations]]</text>
      <sha1>qnw8s6jmx5oliez9t5hg84y16rzklrm</sha1>
    </revision>
  </page>
  <page>
    <title>Necklace (combinatorics)</title>
    <ns>0</ns>
    <id>6226587</id>
    <revision>
      <id>868414383</id>
      <parentid>868413769</parentid>
      <timestamp>2018-11-12T01:35:07Z</timestamp>
      <contributor>
        <username>Magyar25</username>
        <id>21802707</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6812">[[File:Partition necklaces by integer partition.svg|thumb|400px|Possible patterns of bracelets of length ''n''&lt;br&gt;corresponding to the ''k''-th [[Partition (number theory)|integer partition]]&lt;br&gt;([[Partition of a set|set partitions]] [[up to]] rotation and reflection)]]
[[File:Bracelets33.svg|thumb|300px|The 3 '''bracelets''' with 3 red and 3 green beads. The one in the middle is [[Chirality (mathematics)|chiral]], so there are 4 '''necklaces'''.&lt;br&gt;&lt;small&gt;Compare box(6,9) in the triangle.&lt;/small&gt;]]
[[File:Bracelets222.svg|thumb|300px|The 11 '''bracelets''' with 2 red, 2 yellow and 2 green beads. The leftmost one and the four rightmost ones are chiral, so there are 16 '''necklaces'''.&lt;br&gt;&lt;small&gt;Compare box(6,7) in the triangle.&lt;/small&gt;]]
[[File:Tantrix tiles ryg.svg|thumb|300px|16 [[Tantrix]] tiles, corresponding to the 16 '''necklaces''' with 2 red, 2 yellow and 2 green beads.]]
In [[combinatorics]], a ''k''-ary '''necklace''' of length ''n'' is an [[equivalence class]] of ''n''-character [[string (computer science)#Formal theory|string]]s over an [[alphabet (computer science)|alphabet]] of size ''k'', taking all [[circular shift|rotations]] as equivalent. It represents a structure with ''n'' circularly connected beads which have ''k'' available colors. 

A ''k''-ary '''bracelet''', also referred to as a '''turnover''' (or '''free''') '''necklace''', is a necklace such that strings may also be equivalent under reflection. That is, given two strings, if each is the reverse of the other then they belong to the same equivalence class.  For this reason, a necklace might also be called a '''fixed necklace''' to distinguish it from a turnover necklace.

Formally, one may represent a necklace as an [[orbit (group theory)|orbit]] of the [[cyclic group]] [[group action|acting]] on ''n''-character strings, and a bracelet as an orbit of the [[dihedral group]]. One can count these orbits, and thus necklaces and bracelets, using [[Pólya enumeration theorem|Pólya's enumeration theorem]].

== Equivalence classes ==
=== Number of necklaces ===
{{main | Necklace polynomial }}
There are

:&lt;math&gt;N_k(n)=\frac{1}{n}\sum_{d\mid n}\varphi(d)k^\frac{n}{d}&lt;/math&gt;

different ''k''-ary necklaces of length ''n'', where ''&lt;math&gt; \varphi &lt;/math&gt;'' is [[Euler's totient function]].&lt;ref&gt;{{MathWorld|id=Necklace|title=Necklace}}&lt;/ref&gt; This follows directly from [[Pólya enumeration theorem|Pólya's enumeration theorem]] applied to the action of the cyclic group &lt;math&gt;C_n&lt;/math&gt; acting on the set of all functions &lt;math&gt;f : \{1,\ldots,n\} \to\{1,\ldots,k\}&lt;/math&gt;.

=== Number of bracelets ===
There are
:&lt;math&gt;B_k(n) = \begin{cases}
\tfrac12 N_k(n) + \tfrac14 (k+1)k^\frac{n}{2} &amp; \text{if }n\text{ is even} \\[10px]
\tfrac12 N_k(n) + \tfrac12 k^\frac{n+1}{2} &amp; \text{if }n\text{ is odd}
\end{cases}&lt;/math&gt;
different ''k''-ary bracelets of length ''n'', where ''N''&lt;sub&gt;''k''&lt;/sub&gt;(''n'') is the number of ''k''-ary necklaces of length&amp;nbsp;''n''.  This follows from Pólya's method applied to the action of the [[dihedral group]] &lt;math&gt;D_n&lt;/math&gt;.

== Examples ==
=== Necklace example ===
If there are ''n'' beads, all distinct, on a necklace joined at the ends, then the number of distinct orderings on the necklace, after allowing for rotations, is {{sfrac|''n''!|''n''}}, for ''n''&amp;nbsp;&gt;&amp;nbsp;0.  This may also be expressed as (''n''&amp;nbsp;−&amp;nbsp;1)!. This number is less than the general case, which lacks the requirement that each bead must be distinct.

An intuitive justification for this can be given. If there is a line of ''n'' distinct objects ("beads"), the number of combinations would be ''n''!. If the ends are joined together, the number of combinations are divided by ''n'', as it is possible to rotate the string of ''n'' beads into ''n'' positions.

=== Bracelet example ===

If there are ''n'' beads, all distinct, on a bracelet joined at the ends, then the number of distinct orderings on the bracelet, after allowing for rotations and reflection, is {{sfrac|''n''!|2''n''}}, for ''n''&amp;nbsp;&gt;&amp;nbsp;2. Note that this number is less than the general case of ''B&lt;sub&gt;n&lt;/sub&gt;''(''n''), which lacks the requirement that each bead must be distinct.

To explain this, one may begin with the count for a necklace. This number can be further divided by&amp;nbsp;2, because it is also possible to flip the bracelet over.

== Aperiodic necklaces ==
An '''aperiodic necklace''' of length ''n'' is a rotation [[equivalence class]] having size ''n'', i.e., no two distinct rotations of a necklace from such class are equal. 

According to [[Moreau's necklace-counting function]], there are
:&lt;math&gt;M_k(n)=\frac{1}{n}\sum_{d\mid n}\mu(d)k^\frac{n}{d}&lt;/math&gt;

different ''k''-ary aperiodic necklaces of length ''n'', where ''μ'' is the [[Möbius function]]. The two necklace-counting functions are related by: &lt;math&gt;N_k(n)\ =\ \sum\nolimits_{d|n} M_k(d),&lt;/math&gt; where the sum is over all divisors of ''n'', which is equivlent by [[Möbius inversion formula|Möbius inversion]] to &lt;math&gt;M_k(n)\ =\ \sum\nolimits_{d|n} N_k(d)\,\mu(\tfrac{n}{d}).&lt;/math&gt; 

Each aperiodic necklace contains a single [[Lyndon word]] so that Lyndon words form [[Equivalence Class Representative|representatives]] of aperiodic necklaces.

&lt;!-- hide section pending fixing of issues - see Talk page section "Broken formula(s)" …
== Products of necklaces ==
The limit of the product of the numbers of fixed necklaces of length ''n'' composed of ''k'' types of beads:

:&lt;math&gt;\lim_{n \to \infty} \prod_{k=1}^n N_k(n)= \frac {k^n} {n!} 1(1+X)\left(1+X+X^2\right)\cdots\left(1+X+X^2+\cdots+X^{n-1}\right)&lt;/math&gt;{{dubious|date=February 2018}}

where the coefficient of ''X&lt;sup&gt;k&lt;/sup&gt;'' in the expansion of the product

:&lt;math&gt;\prod_{m=1}^n\sum_{i=0}^{m-1}X^i=1(1+X)\left(1+X+X^2\right)\cdots\left(1+X+X^2+\cdots+X^{n-1}\right)&lt;/math&gt;

presents the number of [[permutations]] of ''n'' with ''k'' [[Inversion (discrete mathematics)|inversions]], expressed by a Mahonian number: {{OEIS link|A008302}} (See Gaichenkov link)

… hide section pending fixing of issues - see Talk page section "Broken formula(s)"  --&gt;

== See also ==

* [[Lyndon word]]
* [[Inversion (discrete mathematics)]]
* [[Necklace problem]]
* [[Necklace splitting problem]]
* [[Permutation]]
* [[Proofs of Fermat's little theorem#Proof by counting necklaces]]
* [[Forte number]], a representation of binary bracelets of length 12 used in [[atonal music]].

== References ==
{{reflist}}

== External links ==

* {{MathWorld|Necklace|Necklace}}
* [https://web.archive.org/web/20061002130346/http://www.theory.csc.uvic.ca/~cos/inf/neck/NecklaceInfo.html Info on necklaces, Lyndon words, De Bruijn sequences]

[[Category:Combinatorics on words]]
[[Category:Enumerative combinatorics]]
[[Category:Articles with images not understandable by color blind users]]</text>
      <sha1>byjtyye1jf7x5c20dmqz7m9hi0ktole</sha1>
    </revision>
  </page>
  <page>
    <title>Order-4 pentagonal tiling</title>
    <ns>0</ns>
    <id>9421111</id>
    <revision>
      <id>786594091</id>
      <parentid>777257738</parentid>
      <timestamp>2017-06-20T11:56:26Z</timestamp>
      <contributor>
        <username>CBM</username>
        <id>1108292</id>
      </contributor>
      <minor/>
      <comment>Manually reviewed edit to replace magic words per [[Special:PermanentLink/772743896#Future_of_magic_links|local rfc]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3874">{{Uniform hyperbolic tiles db|Reg hyperbolic tiling stat table|U54_0}}
In [[geometry]], the '''order-4 pentagonal tiling''' is a [[List_of_regular_polytopes#Hyperbolic_tilings|regular]] tiling of the [[Hyperbolic geometry|hyperbolic plane]]. It has [[Schläfli symbol]] of {5,4}. It can also be called a '''pentapentagonal tiling''' in a bicolored quasiregular form.

== Symmetry ==
This tiling represents a hyperbolic [[kaleidoscope]] of 5 mirrors meeting as edges of a regular pentagon. This symmetry by [[orbifold notation]] is called *22222 with 5 order-2 mirror intersections. In [[Coxeter notation]] can be represented as [5&lt;sup&gt;*&lt;/sup&gt;,4], removing two of three mirrors (passing through the pentagon center) in the [5,4] symmetry.

The kaleidoscopic domains can be seen as bicolored pentagons, representing mirror images of the fundamental domain. This coloring represents the uniform tiling t&lt;sub&gt;1&lt;/sub&gt;{5,5} and as a [[quasiregular tiling]] is called a ''pentapentagonal tiling''.
:[[File:Uniform tiling 552-t1.png|240px]]

== Related polyhedra and tiling ==

{{Order 5-4 tiling table}}
{{Order_5-5_tiling_table}}

This tiling is topologically related as a part of sequence of regular polyhedra and tilings with [[pentagon]]al faces, starting with the [[dodecahedron]], with [[Schläfli symbol]] {5,n}, and [[Coxeter diagram]] {{CDD|node_1|5|node|n|node}}, progressing to infinity.
{| class="wikitable collapsible collapsed"
!colspan=5| {5,n} tilings
|- align=center
|[[Image:Uniform polyhedron-53-t0.png|100px]]&lt;BR&gt;[[Dodecahedron|{5,3}]]&lt;BR&gt;{{CDD|node_1|5|node|3|node}}
|[[Image:Uniform tiling 54-t0.png|100px]]&lt;BR&gt;{5,4}&lt;BR&gt;{{CDD|node_1|5|node|4|node}}
|[[Image:Uniform tiling 55-t0.png|100px]]&lt;BR&gt;{5,5}&lt;BR&gt;{{CDD|node_1|5|node|5|node}}
|[[Image:Uniform tiling 56-t0.png|100px]]&lt;BR&gt;{5,6}&lt;BR&gt;{{CDD|node_1|5|node|6|node}}
|[[Image:Uniform tiling 57-t0.png|100px]]&lt;BR&gt;{5,7}&lt;BR&gt;{{CDD|node_1|5|node|7|node}}
|}

This tiling is also topologically related as a part of sequence of regular polyhedra and tilings with four faces per vertex, starting with the [[octahedron]], with [[Schläfli symbol]] {n,4}, and Coxeter diagram {{CDD|node_1|n|node|4|node}}, with n progressing to infinity.
{{Order-4_regular_tilings}}

This tiling is topologically related as a part of sequence of regular polyhedra and tilings with vertex figure (4&lt;sup&gt;n&lt;/sup&gt;).
{{Regular square tiling table}}

{{Quasiregular5 table}}

==References==
* [[John Horton Conway|John H. Conway]], Heidi Burgiel, Chaim Goodman-Strass, ''The Symmetries of Things'' 2008, {{isbn|978-1-56881-220-5}} (Chapter 19, The Hyperbolic Archimedean Tessellations)
* {{citation|first=H. S. M.|last=Coxeter|authorlink= H. S. M. Coxeter|series=The Beauty of Geometry: Twelve Essays|year=1999|publisher=Dover Publications|lccn=99035678|isbn=0-486-40919-8|title= Chapter 10: Regular honeycombs in hyperbolic space|
url=http://www.mathunion.org/ICM/ICM1954.3/Main/icm1954.3.0155.0169.ocr.pdf}}, invited lecture, ICM, Amsterdam, 1954.

==See also==
{{Commonscat|Order-4 pentagonal tiling}}
*[[Square tiling]]
*[[Tilings of regular polygons]]
*[[List of uniform planar tilings]]
*[[List of regular polytopes]]

== External links ==
*{{MathWorld | urlname= HyperbolicTiling | title = Hyperbolic tiling}}
*{{MathWorld | urlname=PoincareHyperbolicDisk | title = Poincaré hyperbolic disk }}
* [http://bork.hampshire.edu/~bernie/hyper/ Hyperbolic and Spherical Tiling Gallery]
* [http://geometrygames.org/KaleidoTile/index.html KaleidoTile 3: Educational software to create spherical, planar and hyperbolic tilings]
* [http://www.plunk.org/~hatch/HyperbolicTesselations Hyperbolic Planar Tessellations, Don Hatch]

{{Tessellation}}

[[Category:Hyperbolic tilings]]
[[Category:Isogonal tilings]]
[[Category:Isohedral tilings]]
[[Category:Order-4 tilings]]
[[Category:Pentagonal tilings]]
[[Category:Regular tilings]]

{{geometry-stub}}</text>
      <sha1>gwsmqy759lmlrrgpwcd280a3zu1qhhx</sha1>
    </revision>
  </page>
  <page>
    <title>Partially ordered set</title>
    <ns>0</ns>
    <id>23572</id>
    <revision>
      <id>863546799</id>
      <parentid>860585391</parentid>
      <timestamp>2018-10-11T13:34:04Z</timestamp>
      <contributor>
        <ip>14.162.154.169</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="28383">[[Image:Hasse diagram of powerset of 3.svg|right|thumb|250px|The [[Hasse diagram]] of the [[power set|set of all subsets]] of a three-element set {x, y, z}, ordered by inclusion. Distinct sets on the same horizontal level are incomparable with each other. Some other pairs, such as {x} and {y,z}, are also incomparable.]]
In [[mathematics]], especially [[order theory]], a '''partially ordered set''' (also '''poset''') formalizes and generalizes the intuitive concept of an ordering, sequencing, or arrangement of the elements of a [[Set (mathematics)|set]]. A poset consists of a set together with a [[binary relation]] indicating that, for certain pairs of elements in the set, one of the elements precedes the other in the ordering. The word "partial" in the names "partial order" or "partially ordered set" is used as an indication that not every pair of elements needs to be comparable. That is, there may be pairs of elements for which neither element precedes the other in the poset. Partial orders thus generalize [[total order]]s, in which every pair is comparable.  

To be a partial order, a binary relation must be [[Reflexive relation|reflexive]] (each element is comparable to itself), [[Antisymmetric relation|antisymmetric]] (no two different elements precede each other), and [[Transitive relation|transitive]] (the start of a chain of precedence relations must precede the end of the chain).

One familiar example of a partially ordered set is a collection of people ordered by [[genealogy|genealogical]] descendancy. Some pairs of people bear the descendant-ancestor relationship, but other pairs of people are incomparable, with neither being a descendent of the other. 

A poset can be visualized through its [[Hasse diagram]], which depicts the ordering relation.&lt;ref&gt;{{cite book |last1=Merrifield |first1=Richard E. |last2=Simmons |first2=Howard E. |authorlink2=Howard Ensign Simmons, Jr. |title=Topological Methods in Chemistry |year=1989 |publisher=John Wiley &amp; Sons |location=New York |isbn=0-471-83817-9 |url=http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471838179.html |accessdate=27 July 2012 |pages=28 |quote=A partially ordered set is conveniently represented by a ''Hasse diagram''...}}&lt;/ref&gt;

== Formal definition ==

{{stack|{{Binary relations}}}}

A (non-strict) '''partial order'''&lt;ref&gt;{{cite book|chapter=Partially Ordered Sets|title=Mathematical Tools for Data Mining: Set Theory, Partial Orders, Combinatorics|publisher=Springer|year=2008|isbn=9781848002012|url=https://books.google.com/books?id=6i-F3ZNcub4C&amp;pg=PA127|author1=Simovici, Dan A.  |author2=Djeraba, Chabane |lastauthoramp=yes }}&lt;/ref&gt; is a [[binary relation]] ≤ over a [[Set (mathematics)|set]] ''P'' satisfying particular axioms which are discussed below. When ''a'' ≤ ''b'', we say that ''a'' is '''related to''' ''b''. (This does not imply that ''b'' is also related to ''a'', because the relation need not be [[symmetric relation|symmetric]].)  

The axioms for a non-strict partial order state that the relation ≤ is [[reflexive relation|reflexive]], [[antisymmetric relation|antisymmetric]], and [[transitive relation|transitive]]. That is, for all ''a'', ''b'', and ''c'' in ''P'', it must satisfy:

# ''a'' ≤ ''a'' ([[Reflexive relation|reflexivity]]: every element is related to itself).
# if ''a'' ≤ ''b'' and ''b'' ≤ ''a'', then ''a'' = ''b'' ([[Antisymmetric relation|antisymmetry]]: two distinct elements cannot be related in both directions).
# if ''a'' ≤ ''b'' and ''b'' ≤ ''c'', then ''a'' ≤ ''c'' ([[Transitive relation|transitivity]]: if a first element is related to a second element, and, in turn, that element is related to a third element, then the first element is related to the third element).

In other words, a partial order is an antisymmetric [[preorder]].

A set with a partial order is called a '''partially ordered set''' (also called a '''poset'''). The term ''ordered set'' is sometimes also used, as long as it is clear from the context that no other kind of order is meant. In particular, [[Total order|totally ordered sets]] can also be referred to as "ordered sets", especially in areas where these structures are more common than posets.

For ''a, b'', elements of a partially ordered set ''P'', if ''a'' ≤ ''b'' or ''b'' ≤ ''a'', then ''a'' and ''b'' are '''[[Comparability|comparable]]'''. Otherwise they are '''incomparable'''. In the figure on top-right, e.g. {x} and {x,y,z} are comparable, while {x} and {y} are not. A partial order under which every pair of elements is comparable is called a '''[[totally ordered set|total order]]''' or '''linear order'''; a totally ordered set is also called a '''chain''' (e.g., the natural numbers with their standard order). A subset of a poset in which no two distinct elements are comparable is called an '''[[antichain]]''' (e.g. the set of [[singleton (mathematics)|singleton]]s {{x}, {y}, {z}} in the top-right figure). An element ''a'' is said to be '''[[Covering relation|covered]]''' by another element ''b'', written ''a''&lt;:''b'', if ''a'' is strictly less than ''b'' and no third element ''c'' fits between them; formally: if both ''a''≤''b'' and ''a''≠''b'' are true, and ''a''≤''c''≤''b'' is false for each ''c'' with ''a''≠''c''≠''b''. A more concise definition will be given [[#Strict and non-strict partial orders|below]] using the strict order corresponding to "≤". For example, {x} is covered by {x,z} in the top-right figure, but not by {x,y,z}.

== Examples ==

Standard examples of posets arising in mathematics include:

* The [[real number]]s ordered by the standard ''less-than-or-equal'' relation ≤ (a totally ordered set as well).
* The set of [[subset]]s of a given set (its [[power set]]) ordered by [[subset|inclusion]] (see the figure on top-right). Similarly, the set of [[sequence]]s ordered by [[subsequence]], and the set of [[string (computer science)|string]]s ordered by [[substring]].
* The set of [[natural number]]s equipped with the relation of [[divisor#Divisibility of numbers|divisibility]].
* The vertex set of a [[directed acyclic graph]] ordered by [[reachability]].
* The set of [[Linear subspace|subspaces]] of a [[vector space]] ordered by inclusion.
* For a partially ordered set ''P'', the [[sequence space]] containing all [[sequence]]s of elements from ''P'', where sequence ''a'' precedes sequence ''b'' if every item in ''a'' precedes the corresponding item in ''b''. Formally, {{math|(''a''&lt;sub&gt;''n''&lt;/sub&gt;)&lt;sub&gt;''n''∈ℕ&lt;/sub&gt;&amp;nbsp;≤&amp;nbsp;(''b''&lt;sub&gt;''n''&lt;/sub&gt;)&lt;sub&gt;''n''∈ℕ&lt;/sub&gt;}} if and only if {{math|''a''&lt;sub&gt;''n''&lt;/sub&gt;&amp;nbsp;≤&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;}} for all ''n'' in ℕ, i.e. a [[componentwise order]].
* For a set ''X'' and a partially ordered set ''P'', the [[function space]] containing all functions from ''X'' to ''P'', where ''f'' ≤ ''g'' if and only if ''f''(''x'') ≤ ''g''(''x'') for all ''x'' in ''X''.
* A [[Fence (mathematics)|fence]], a partially ordered set defined by an alternating sequence of order relations ''a'' &amp;lt; ''b'' &amp;gt; ''c'' &amp;lt; ''d'' ...
* The set of events in [[special relativity]], where for two events X and Y, X ≤ Y if and only if Y is in the future [[light cone]] of X. An event Y can only be causally affected by X if X ≤ Y.

== Extrema ==
{| style="float:right"
|-
| [[File:Infinite lattice of divisors.svg|thumb|x150px|Nonnegative integers, ordered by divisibility]]
|}
{| style="float:right"
|-
| [[File:Hasse diagram of powerset of 3 no greatest or least.svg|thumb|x150px|The figure above with the greatest and least elements removed. In this reduced poset, the top row of elements are all ''maximal'' elements, and the bottom row are all ''minimal'' elements, but there is no ''greatest'' and no ''least'' element. The set {x, y} is an ''upper bound'' for the collection of elements {{x}, {y}}.]]
|}
There are several notions of "greatest" and "least" element in a poset ''P'', notably:
* [[Greatest element]] and least element: An element ''g'' in ''P'' is a greatest element if for every element ''a'' in ''P'', ''a''&amp;nbsp;≤&amp;nbsp;''g''. An element ''m'' in ''P'' is a least element if for every element ''a'' in ''P'', ''a''&amp;nbsp;≥&amp;nbsp;''m''. A poset can only have one greatest or least element.
* [[Maximal element]]s and minimal elements: An element ''g'' in P is a maximal element if there is no element ''a'' in ''P'' such that ''a''&amp;nbsp;&gt;&amp;nbsp;''g''. Similarly, an element ''m'' in ''P'' is a minimal element if there is no element ''a'' in P such that ''a''&amp;nbsp;&lt;&amp;nbsp;''m''. If a poset has a greatest element, it must be the unique maximal element, but otherwise there can be more than one maximal element, and similarly for least elements and minimal elements.
* [[Upper and lower bounds]]: For a subset ''A'' of ''P'', an element ''x'' in ''P'' is an upper bound of ''A'' if ''a''&amp;nbsp;≤&amp;nbsp;''x'', for each element ''a'' in ''A''. In particular, ''x'' need not be in ''A'' to be an upper bound of ''A''. Similarly, an element ''x'' in ''P'' is a lower bound of ''A'' if ''a''&amp;nbsp;≥&amp;nbsp;''x'', for each element ''a'' in ''A''. A greatest element of ''P'' is an upper bound of ''P'' itself, and a least element is a lower bound of ''P''.

For example, consider the [[positive integer]]s, ordered by divisibility: 1 is a least element, as it divides all other elements; on the other hand this poset does not have a greatest element (although if one would include 0 in the poset, which is a multiple of any integer, that would be a greatest element; see figure). This partially ordered set does not even have any maximal elements, since any ''g'' divides for instance 2''g'', which is distinct from it, so ''g'' is not maximal. If the number 1 is excluded, while keeping divisibility as ordering on the elements greater than 1, then the resulting poset does not have a least element, but any [[prime number]] is a minimal element for it. In this poset, 60 is an upper bound (though not a least upper bound) of the subset {2,3,5,10}, which does not have any lower bound (since 1 is not in the poset); on the other hand 2 is a lower bound of the subset of powers of 2, which does not have any upper bound.

==Orders on the Cartesian product of partially ordered sets==
{| style="float:right"
|-
|[[File:Strict product order on pairs of natural numbers.svg|thumb|x150px|Reflexive closure of strict direct product order on ℕ×ℕ. Elements [[#Formal definition|covered]] by (3,3) and covering (3,3) are highlighted in green and red, respectively.]]
|}
{| style="float:right"
|-
|[[File:N-Quadrat, gedreht.svg|thumb|x150px|Product order on ℕ×ℕ]]
|}
{| style="float:right"
|-
|[[File:Lexicographic order on pairs of natural numbers.svg|thumb|x150px|Lexicographic order on ℕ×ℕ]]
|}
In order of increasing strength, i.e., decreasing sets of pairs, three of the possible partial orders on the [[Cartesian product]] of two partially ordered sets are (see figures):
*the [[lexicographical order]]: &amp;nbsp; (''a'',''b'') ≤ (''c'',''d'') if ''a'' &lt; ''c'' or (''a'' = ''c'' and ''b'' ≤ ''d'');
*the [[product order]]:  &amp;nbsp; (''a'',''b'') ≤ (''c'',''d'') if ''a'' ≤ ''c'' and ''b'' ≤ ''d'';
*the [[reflexive closure]] of the [[Direct product#Direct product of binary relations|direct product]] of the corresponding strict orders:  &amp;nbsp; (''a'',''b'') ≤ (''c'',''d'') if (''a'' &lt; ''c'' and ''b'' &lt; ''d'') or (''a'' = ''c'' and ''b'' = ''d'').

All three can similarly be defined for the Cartesian product of more than two sets.

Applied to [[ordered vector space]]s over the same [[Field (mathematics)|field]], the result is in each case also an ordered vector space.

See also [[Total order#Orders on the Cartesian product of totally ordered sets|orders on the Cartesian product of totally ordered sets]].

== Sums of partially ordered sets ==
{{anchor|sum}}
[[File:Series-parallel partial order.svg|thumb|upright=1.35|[[Hasse diagram]] of a [[series-parallel partial order]], formed as the ordinal sum of three smaller partial orders.]]
Another way to combine two posets is the '''ordinal sum'''&lt;ref&gt;{{citation
 | last1 = Neggers | first1 = J.
 | last2 = Kim | first2 = Hee Sik
 | contribution = 4.2 Product Order and Lexicographic Order
 | isbn = 9789810235895
 | pages = 62–63
 | publisher = World Scientific
 | title = Basic Posets
 | year = 1998}}&lt;/ref&gt; (or '''linear sum'''&lt;ref&gt;{{cite book |last=Davey |first=B. A. |last2=Priestley |first2=H. A. |title=Introduction to Lattices and Order |edition=Second |location=New York |publisher=Cambridge University Press |year=2002 |isbn=0-521-78451-4 |pages=17–18 |url=https://books.google.com/books?id=vVVTxeuiyvQC&amp;pg=PA17 |via=[[Google Books]] }}&lt;/ref&gt;), ''Z'' = ''X'' ⊕ ''Y'', defined on the union of the underlying sets ''X'' and ''Y'' by the order ''a'' ≤&lt;sub&gt;''Z''&lt;/sub&gt; ''b'' if and only if:
* ''a'', ''b'' ∈ ''X'' with ''a'' ≤&lt;sub&gt;''X''&lt;/sub&gt; ''b'', or
* ''a'', ''b'' ∈ ''Y'' with ''a'' ≤&lt;sub&gt;''Y''&lt;/sub&gt; ''b'', or
* ''a'' ∈ ''X'' and ''b'' ∈ ''Y''.

If two posets are [[well-ordered]], then so is their ordinal sum.&lt;ref&gt;{{cite book|author=P. R. Halmos|title=Naive Set Theory|year=1974|publisher=Springer |isbn=978-1-4757-1645-0|page=82}}&lt;/ref&gt;
The ordinal sum operation is one of two operations used to form [[series-parallel partial order]]s, and in this context is called series composition. The other operation used to form these orders, the disjoint union of two partially ordered sets (with no order relation between elements of one set and elements of the other set) is called in this context parallel composition.

== Strict and non-strict partial orders ==

In some contexts, the partial order defined above is called a '''non-strict''' (or '''reflexive''', or '''weak''') '''partial order'''. In these contexts, a '''strict''' (or '''irreflexive''') '''partial order''' "&lt;" is a binary relation that is [[irreflexive relation|irreflexive]], [[transitive relation|transitive]] and [[asymmetric relation|asymmetric]], i.e. which satisfies for all ''a'', ''b'', and ''c'' in ''P'':

*not ''a &lt; a''  (irreflexivity),
*if ''a &lt; b'' and ''b &lt; c'' then ''a &lt; c''  (transitivity), and
*if ''a &lt; b'' then not ''b &lt; a''  (asymmetry; implied by irreflexivity and transitivity&lt;ref&gt;{{cite book|last1=Flaška|first1=V.|last2=Ježek|first2=J.|last3=Kepka|first3=T.|last4=Kortelainen|first4=J.|title=Transitive Closures of Binary Relations I|year=2007|publisher=School of Mathematics - Physics Charles University|location=Prague|page=1|url=http://dml.cz/dmlcz/142762}} Lemma 1.1 (iv). Note that this source refers to asymmetric relations as "strictly antisymmetric".&lt;/ref&gt;).

Strict and non-strict partial orders are closely related. A non-strict partial order may be converted to a strict partial order by removing all relationships of the form ''a'' &amp;le; ''a''. Conversely, a strict partial order may be converted to a non-strict partial order by adjoining all relationships of that form. Thus, if "≤" is a non-strict partial order, then the corresponding strict partial order "&lt;" is the [[irreflexive kernel]] given by:

:''a'' &lt; ''b'' if ''a'' ≤ ''b'' and ''a'' ≠ ''b''
Conversely, if "&lt;" is a strict partial order, then the corresponding non-strict partial order "≤" is the [[Binary relation#Operations on binary relations|reflexive closure]] given by:

: ''a'' ≤ ''b'' if ''a'' &lt; ''b'' or ''a'' = ''b''.
This is the reason for using the notation "≤".

Using the strict order "&lt;", the relation "''a'' is covered by ''b''" can be equivalently rephrased as "''a''&lt;''b'', but not ''a''&lt;''c''&lt;''b'' for any ''c''".
Strict partial orders are useful because they correspond more directly to [[directed acyclic graph]]s (dags): every strict partial order is a dag, and the [[transitive closure]] of a dag is both a strict partial order and also a dag itself.

==Inverse and order dual==

The inverse (or converse) of a partial order relation ≤ is the [[converse relation|converse]] of ≤. Typically denoted ≥, it is the relation that satisfies ''x''&amp;nbsp;≥&amp;nbsp;''y'' if and only if ''y''&amp;nbsp;≤&amp;nbsp;''x''.  The inverse of a partial order relation is reflexive, transitive, and antisymmetric, and hence itself a partial order relation.  The [[Duality (order theory)|order dual]] of a partially ordered set is the same set with the partial order relation replaced by its inverse.  The irreflexive relation &amp;gt; is to ≥ as &amp;lt; is to ≤.

Any one of the four relations ≤, &amp;lt;, ≥, and &amp;gt; on a given set uniquely determines the other three.

In general two elements ''x'' and ''y'' of a partial order may stand in any of four mutually exclusive relationships to each other: either ''x''&amp;nbsp;&lt;&amp;nbsp;''y'', or ''x''&amp;nbsp;=&amp;nbsp;''y'', or ''x''&amp;nbsp;&gt;&amp;nbsp;''y'', or ''x'' and ''y'' are ''incomparable'' (none of the other three).  A [[total order|totally ordered]] set is one that rules out this fourth possibility: all pairs of elements are comparable and we then say that [[Trichotomy (mathematics)|trichotomy]] holds.  The [[natural number]]s, the [[integer]]s, the [[Rational number|rationals]], and the [[Real number|real]]s are all totally ordered by their algebraic (signed) magnitude whereas the [[complex number]]s are not.  This is not to say that the complex numbers cannot be totally ordered; we could for example order them lexicographically via ''x''+'''i'''''y''&amp;nbsp;&lt;&amp;nbsp;''u''+'''i'''''v'' if and only if ''x''&amp;nbsp;&lt;&amp;nbsp;''u'' or (''x''&amp;nbsp;=&amp;nbsp;''u'' and ''y''&amp;nbsp;&lt;&amp;nbsp;''v''), but this is not ordering by magnitude in any reasonable sense as it makes 1 greater than 100'''i'''.  Ordering them by absolute magnitude yields a preorder in which all pairs are comparable, but this is not a partial order since 1 and '''i''' have the same absolute magnitude but are not equal, violating antisymmetry.

==Mappings between partially ordered sets==

{| style="float:right"
|-
|[[File:Birkhoff120.svg|thumb|x150px|Order isomorphism between the divisors of 120 (partially ordered by divisibility) and the divisor-closed subsets of {2,3,4,5,8} (partially ordered by set inclusion)]]
|}
{| style="float:right"
|-
|[[File:Monotonic but nonhomomorphic map between lattices.gif|thumb|x150px|Order-preserving, but not order-reflecting (since ''f''(''u'')≤''f''(''v''), but not ''u''≤''v'') map.]]
|}
Given two partially ordered sets (''S'',≤) and (''T'',≤), a function ''f'': ''S'' → ''T'' is called '''[[order-preserving]]''', or '''[[Monotonic function#Monotonicity in order theory|monotone]]''', or '''isotone''', if for all ''x'' and ''y'' in ''S'', ''x''≤''y'' implies ''f''(''x'') ≤ ''f''(''y'').
If (''U'',≤) is also a partially ordered set, and both ''f'': ''S'' → ''T'' and ''g'': ''T'' → ''U'' are order-preserving, their [[function composition|composition]] (''g''∘''f''): ''S'' → ''U'' is order-preserving, too.
A function ''f'': ''S'' → ''T'' is called '''order-reflecting''' if for all ''x'' and ''y'' in ''S'', ''f''(''x'') ≤ ''f''(''y'') implies ''x''≤''y''.
If ''f'' is both order-preserving and order-reflecting, then it is called an '''[[order-embedding]]''' of (''S'',≤) into (''T'',≤).
In the latter case, ''f'' is necessarily [[injective]], since ''f''(''x'') = ''f''(''y'') implies ''x'' ≤ ''y'' and ''y'' ≤ ''x''. If an order-embedding between two posets ''S'' and ''T'' exists, one says that ''S'' can be '''embedded''' into ''T''. If an order-embedding ''f'': ''S'' → ''T'' is [[bijective]], it is called an '''[[order isomorphism]]''', and the partial orders (''S'',≤) and (''T'',≤) are said to be '''isomorphic'''. Isomorphic orders have structurally similar [[Hasse diagram]]s (cf. right picture). It can be shown that if order-preserving maps ''f'': ''S'' → ''T'' and ''g'': ''T'' → ''S'' exist such that ''g''∘''f'' and ''f''∘''g'' yields the [[identity function]] on ''S'' and ''T'', respectively, then ''S'' and ''T'' are order-isomorphic.
&lt;ref name="dp02"&gt;{{Cite book
 | last1 = Davey | first1 = B. A.
 | last2 = Priestley | first2 = H. A.
 | contribution = Maps between ordered sets
 | edition = 2nd
 | isbn = 0-521-78451-4
 | location = New York
 | mr = 1902334
 | pages = 23–24
 | publisher = Cambridge University Press
 | title = Introduction to Lattices and Order
 | chapterurl = https://books.google.com/books?id=vVVTxeuiyvQC&amp;pg=PA23
 | year = 2002
 | postscript = &lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}.&lt;/ref&gt;

For example, a mapping ''f'': ℕ → ℙ(ℕ) from the set of natural numbers (ordered by divisibility) to the [[power set]] of natural numbers (ordered by set inclusion) can be defined by taking each number to the set of its [[prime divisor]]s. It is order-preserving: if ''x'' divides ''y'', then each prime divisor of ''x'' is also a prime divisor of ''y''. However, it is neither injective (since it maps both 12 and 6 to {2,3}) nor order-reflecting (since besides 12 doesn't divide 6). Taking instead each number to the set of its [[prime power]] divisors defines a map ''g'': ℕ → ℙ(ℕ) that is order-preserving, order-reflecting, and hence an order-embedding. It is not an order-isomorphism (since it e.g. doesn't map any number to the set {4}), but it can be made one by [[Injective function#Injections may be made invertible|restricting its codomain]] to ''g''(ℕ). The right picture shows a subset of ℕ and its isomorphic image under ''g''. The construction of such an order-isomorphism into a power set can be generalized to a wide class of partial orders, called [[distributive lattice]]s, see "[[Birkhoff's representation theorem]]".

==Number of partial orders==
Sequence [{{fullurl:OEIS:A001035}} A001035] in [[On-Line Encyclopedia of Integer Sequences|OEIS]] gives the number of partial orders on a set of ''n'' labeled elements:

{{Number of relations}}

The number of strict partial orders is the same as that of partial orders.

If the count is made only [[up to]] isomorphism, the sequence 1, 1, 2, 5, 16, 63, 318, … {{OEIS|A000112}} is obtained.

== Linear extension ==
A partial order ≤&lt;sup&gt;*&lt;/sup&gt; on a set ''X'' is an '''extension''' of another partial order ≤ on ''X'' provided that for all elements ''x'' and ''y'' of ''X'', whenever ''x'' ≤ ''y'', it is also the case that ''x''&amp;nbsp;≤&lt;sup&gt;*&lt;/sup&gt;&amp;nbsp;''y''. A [[linear extension]] is an extension that is also a linear (i.e., total) order.  Every partial order can be extended to a total order ([[order-extension principle]]).&lt;ref&gt;{{cite book |last=Jech |first=Thomas |authorlink=Thomas Jech |title=The Axiom of Choice |year=2008 |origyear=1973 |publisher=[[Dover Publications]] |isbn=0-486-46624-8}}&lt;/ref&gt;

In [[computer science]], algorithms for finding linear extensions of partial orders (represented as the [[reachability]] orders of [[directed acyclic graph]]s) are called [[topological sorting]].

== In category theory ==
Every poset (and every [[preorder]]) may be considered as a [[category (mathematics)|category]] in which every hom-set has at most one element. More explicitly, let hom(''x'', ''y'') = {(''x'', ''y'')} if ''x'' ≤ ''y'' (and otherwise the empty set) and (''y'', ''z'')∘(''x'', ''y'') = (''x'', ''z''). Such categories are sometimes called ''posetal''.

Posets are [[Equivalence of categories|equivalent]] to one another if and only if they are [[Isomorphism of categories|isomorphic]]. In a poset, the smallest element, if it exists, is an [[initial object]], and the largest element, if it exists, is a [[terminal object]]. Also, every preordered set is equivalent to a poset. Finally, every subcategory of a poset is [[isomorphism-closed]].

==Partial orders in topological spaces==
{{Main|Partially ordered space}}
If ''P'' is a partially ordered set that has also been given the structure of a [[topological space]], then it is customary to assume that {{math|{(''a'', ''b'') : ''a'' &amp;le; ''b''} }} is a [[closed (mathematics)|closed]] subset of the topological [[product space]] &lt;math&gt;P\times P&lt;/math&gt;. Under this assumption partial order relations are well behaved at [[Limit of a sequence|limits]] in the sense that if &lt;math&gt;a_i\to a&lt;/math&gt;, &lt;math&gt;b_i\to b&lt;/math&gt; and &lt;span class="texhtml"&gt;''a''&lt;sub&gt;''i''&lt;/sub&gt;&amp;nbsp;≤&amp;nbsp;''b''&lt;sub&gt;''i''&lt;/sub&gt;&lt;/span&gt; for all ''i'', then &lt;span class="texhtml"&gt;''a''&amp;nbsp;≤&amp;nbsp;''b''&lt;/span&gt;.&lt;ref name="ward-1954"&gt;{{Cite journal|first=L. E. Jr|last=Ward|title=Partially Ordered Topological Spaces|journal=Proceedings of the American Mathematical Society|volume=5 |year=1954|pages= 144–161|issue= 1|doi=10.1090/S0002-9939-1954-0063016-5|postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}}}}&lt;/ref&gt;

==Interval==
For ''a'' ≤ ''b'', the [[interval (mathematics)|closed interval]] {{closed-closed|''a'',''b''}} is the set of elements ''x'' satisfying ''a'' ≤ ''x'' ≤ ''b'' (i.e. ''a'' ≤ ''x'' and ''x'' ≤ ''b''). It contains at least the elements ''a'' and ''b''.

Using the corresponding strict relation "&lt;", the [[open interval]] {{open-open|''a'',''b''}} is the set of elements ''x'' satisfying ''a'' &lt; ''x'' &lt; ''b'' (i.e. ''a'' &lt; ''x'' and ''x'' &lt; ''b''). An open interval may be empty even if ''a'' &lt; ''b''.  For example, the open interval {{open-open|1,2}} on the integers is empty since there are no integers ''i'' such that 1 &lt; ''i'' &lt; 2.

Sometimes the definitions are extended to allow ''a'' &gt; ''b'', in which case the interval is empty.

The ''half-open intervals'' {{closed-open|''a'',''b''}} and {{open-closed|''a'',''b''}} are defined similarly.

A poset is [[Locally finite poset|locally finite]] if every interval is finite. For example, the [[integers]] are locally finite under their natural ordering. The lexicographical order on the cartesian product ℕ×ℕ is not locally finite, since e.g. (1,2)≤(1,3)≤(1,4)≤(1,5)≤...≤(2,1).
Using the interval notation, the property "''a'' is covered by ''b''" can be rephrased equivalently as [''a'',''b''] = {''a'',''b''}.

This concept of an interval in a partial order should not be confused with the particular class of partial orders known as the [[interval order]]s.

== See also ==
{{div col|colwidth=22em}}
*[[antimatroid]], a formalization of orderings on a set that allows more general families of orderings than posets
*[[causal set]]
*[[comparability graph]]
*[[complete partial order]]
*[[directed set]]
*[[graded poset]]
*[[incidence algebra]]
*[[lattice (order)|lattice]]
*[[locally finite poset]]
*[[incidence algebra|Möbius function on posets]]
*[[ordered group]]
*[[poset topology]], a kind of topological space that can be defined from any poset
*[[Scott continuity]] – continuity of a function between two partial orders.
*[[semilattice]]
*[[semiorder]]
*[[stochastic dominance]]
*[[strict weak ordering]] – strict partial order "&lt;" in which the relation {{nowrap|"neither ''a'' &lt; ''b''}} {{nowrap|nor ''b'' &lt; ''a''"}} is transitive.
*[[Zorn's lemma]]
{{div col end}}

==Notes==
&lt;references/&gt;

==References==
* {{Cite journal|first=Jayant V. |last=Deshpande|title= On Continuity of a Partial Order|journal= Proceedings of the American Mathematical Society|volume= 19|year= 1968|pages= 383–386|issue= 2|doi=10.1090/S0002-9939-1968-0236071-7|postscript=.}}
* {{cite book|first=Gunther|last=Schmidt|authorlink=Gunther Schmidt|year= 2010|title=Relational Mathematics|series=Encyclopedia of Mathematics and its Applications|volume= 132|publisher=Cambridge University Press|isbn=978-0-521-76268-7}}
* {{cite book|first=Bernd S. W. |last=Schröder|title= Ordered Sets:  An Introduction |publisher=Birkhäuser, Boston|year=2003}}
* {{cite book|first=Richard P.|last=Stanley|authorlink=Richard P. Stanley|title=Enumerative Combinatorics 1|series=Cambridge Studies in Advanced Mathematics|volume=49|publisher=Cambridge University Press|isbn=0-521-66351-2}}

==External links==
{{Commons|Hasse diagram}}

* {{OEIS el|1=A001035|2= Number of posets with ''n'' labeled elements|formalname=Number of partially ordered sets ("posets") with n labeled elements (or labeled acyclic transitive digraphs)}}
* {{OEIS el|1=A000112|2=Number of partially ordered sets ("posets") with n unlabeled elements.}}

[[Category:Order theory]]
[[Category:Binary relations]]

[[de:Ordnungsrelation#Halbordnung]]</text>
      <sha1>iam55u2n9i6bqgflq9g5ampf0ojjxvh</sha1>
    </revision>
  </page>
  <page>
    <title>Password strength</title>
    <ns>0</ns>
    <id>4459886</id>
    <revision>
      <id>870914628</id>
      <parentid>869829196</parentid>
      <timestamp>2018-11-27T19:38:08Z</timestamp>
      <contributor>
        <username>DesertPipeline</username>
        <id>21386896</id>
      </contributor>
      <comment>/* Examples of weak passwords */ person's &gt; a person's</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="53804">[[File:KeePass random password.png|thumb|400px|Options menu of a [[Keepass|password generation program]]. Enabling more character subsets raises the strength of generated passwords a small amount, increasing the length raises the strength a large amount.]]
'''Password strength'''  is a measure of the effectiveness of a [[password]] against guessing or [[brute-force attack]]s. In its usual form, it estimates how many trials an attacker who does not have direct access to the password would need, on average, to guess it correctly. The strength of a password is a function of length, complexity, and unpredictability.&lt;ref name=CERT&gt;{{cite web | url = http://www.us-cert.gov/cas/tips/ST04-002.html | title = Cyber Security Tip ST04-002 | work = Choosing and Protecting Passwords | publisher = US CERT | accessdate = June 20, 2009 | deadurl = no | archiveurl = https://web.archive.org/web/20090707141138/http://www.us-cert.gov/cas/tips/ST04-002.html | archivedate = July 7, 2009 | df =  }}&lt;/ref&gt;

Using strong passwords lowers overall [[risk]] of a security breach, but strong passwords do not replace the need for other effective [[security controls]]. The effectiveness of a password of a given strength is strongly determined by the design and implementation of the [[Authentication#Factors and identity|factors]] (knowledge, ownership, inherence). The first factor is the main focus in this article.

The rate at which an attacker can submit guessed passwords to the system is a key factor in determining system security. Some systems impose a time-out of several seconds after a small number (e.g. three) of failed password entry attempts. In the absence of other vulnerabilities, such systems can be effectively secured with relatively simple passwords. However the system must store information about the user passwords in some form and if that information is stolen, say by breaching system security, the user passwords can be at risk.

==Password creation==

Passwords are created either automatically (using randomizing equipment) or by a human; the latter case is more common. While the strength of randomly chosen passwords against a [[brute-force attack]] can be calculated with precision, determining the strength of human-generated passwords is challenging.

Typically, humans are asked to choose a password, sometimes guided by suggestions or restricted by a set of rules, when creating a new account for a computer system or Internet Web site. Only rough estimates of strength are possible, since humans tend to follow patterns in such tasks, and those patterns can usually assist an attacker.&lt;ref name=NIST /&gt; In addition, lists of commonly chosen passwords are widely available for use by password guessing programs. Such lists include the numerous online dictionaries for various human languages, breached databases of plaintext and hashed passwords from various online business and social accounts, along with other common passwords. All items in such lists are considered weak, as are passwords that are simple modifications of them. For some decades, investigations of passwords on multi-user computer systems have shown that 40% or more{{Citation needed|date=March 2016}} are readily guessed using only computer programs, and more can be found when information about a particular user is taken into account during the attack.

==Password guess validation==
Systems that use passwords for authentication must have some way to check any password entered to gain access. If the valid passwords are simply stored in a system file or database, an attacker who gains sufficient access to the system will obtain all user passwords, giving the attacker access to all accounts on the attacked system, and possibly other systems where users employ the same or similar passwords. One way to reduce this risk is to store only a [[cryptographic hash]] of each password instead of the password itself. Standard cryptographic hashes, such as the [[Secure Hash Algorithm (disambiguation)|Secure Hash Algorithm]] (SHA) series, are very hard to reverse, so an attacker who gets hold of the hash value cannot directly recover the password. However, knowledge of the hash value lets the attacker quickly test guesses offline. [[Password cracking]] programs are widely available that will test a large number of trial passwords against a purloined cryptographic hash.

Improvements in computing technology keep increasing the rate at which guessed passwords can be tested. For example, in 2010, the [[Georgia Tech Research Institute]] developed a method of using [[GPGPU]] to crack passwords much faster.&lt;ref name="gtri"/&gt; [[Elcomsoft]] invented the usage of common graphic cards for quicker password recovery in August 2007 and soon filed a corresponding patent in the US.&lt;ref name="belenko"&gt;{{ cite patent | country=US | number=7929707 | status=patent | title=Use of graphics processors as parallel math co-processors for password recovery | assign1=Elcomsoft Co. Ltd. | inventor= Andrey V. Belenko | gdate=2011-04-19 }}&lt;/ref&gt; As of 2011, commercial products are available that claim the ability to test up to 112,000 passwords per second on a standard desktop computer using a high-end graphics processor.&lt;ref name=elcomsoft&gt;[http://www.elcomsoft.com/eprb.html#gpu Elcomsoft.com] {{webarchive|url=https://web.archive.org/web/20061017173506/http://www.elcomsoft.com/eprb.html |date=2006-10-17 }}, [[ElcomSoft]] Password Recovery Speed table, [[NTLM]] passwords, [[Nvidia Tesla]] S1070 GPU, accessed 2011-02-01&lt;/ref&gt; Such a device will crack a 6 letter single-case password in one day. Note that the work can be distributed over many computers for an additional speedup proportional to the number of available computers with comparable GPUs. Special [[key stretching]] hashes are available that take a relatively long time to compute, reducing the rate at which guessing can take place. Although it is considered best practice to use key stretching, many common systems do not.

Another situation where quick guessing is possible is when the password is used to form a [[cryptographic key]]. In such cases, an attacker can quickly check to see if a guessed password successfully decodes encrypted data. For example, one commercial product claims to test 103,000 [[Wi-Fi Protected Access|WPA]] PSK passwords per second.&lt;ref&gt;[http://www.elcomsoft.com/ewsa.html Elcomsoft Wireless Security Auditor, HD5970 GPU] {{webarchive|url=https://web.archive.org/web/20110219131825/http://www.elcomsoft.com/ewsa.html |date=2011-02-19 }} accessed 2011-02-11&lt;/ref&gt;

If a password system only stores the hash of the password, an attacker can pre-compute hash values for common passwords variants and for all passwords shorter than a certain length, allowing very rapid recovery of the password once its hash is obtained. Very long lists of pre-computed password hashes can be efficiently stored using [[rainbow tables]]. This method of attack can be foiled by storing a random value, called a [[cryptographic salt]], along with the hash. The salt is combined with the password when computing the hash, so an attacker precomputing a rainbow table would have to store for each password its hash with every possible salt value. This becomes infeasible if the salt has a big enough range, say a 32-bit number. Unfortunately, many authentication systems in common use do not employ salts and rainbow tables are available on the Internet for several such systems.

===Entropy as a measure of password strength===
It is usual in the computer industry to specify password strength in terms of [[information entropy]], measured in bits, a concept from [[information theory]]. Instead of the number of guesses needed to find the password with certainty, the [[Binary logarithm|base-2 logarithm]] of that number is given, which is the number of "entropy bits" in a password. A password with, say, 42 bits of strength calculated in this way would be as strong as a string of 42 bits chosen randomly, say by a [[fair coin]] toss. Put another way, a password with 42 bits of strength would require 2&lt;sup&gt;42&lt;/sup&gt; (4,398,046,511,104) attempts to exhaust all possibilities during a [[brute force search]]. Thus, adding one bit of entropy to a password doubles the number of guesses required, which makes an attacker's task twice as difficult. On average, an attacker will have to try half of the possible passwords before finding the correct one.&lt;ref name=NIST /&gt;

=== Random passwords ===
{{Main|Random password generator}}

Random passwords consist of a string of symbols of specified length taken from some set of symbols using a random selection process in which each symbol is equally likely to be selected. The symbols can be individual characters from a character set (e.g., the [[ASCII]] character set), syllables designed to form pronounceable passwords, or even words from a word list (thus forming a [[passphrase]]).

The strength of random passwords depends on the actual entropy of the underlying number generator; however, these are often not truly random, but pseudo random. Many publicly available password generators use random number generators found in programming libraries that offer limited entropy. However most modern operating systems offer cryptographically strong random number generators that are suitable for password generation. It is also possible to use ordinary [[dice]] to generate random passwords. ''See [[Random password generator#Stronger methods|stronger methods]].'' Random password programs often have the ability to ensure that the resulting password complies with a local [[password policy]]; for instance, by always producing a mix of letters, numbers and special characters.

For passwords generated by a process that randomly selects a string of symbols of length, ''L'', from a set of ''N'' possible symbols, the number of possible passwords can be found by raising the number of symbols to the power ''L'', i.e. ''N''&lt;sup&gt;''L''&lt;/sup&gt;. Increasing either ''L'' or ''N'' will strengthen the generated password. The strength of a random password as measured by the [[information entropy]] is just the [[binary logarithm|base-2 logarithm]] or log&lt;sub&gt;2&lt;/sub&gt; of the number of possible passwords, assuming each symbol in the password is produced independently. Thus a random password's information entropy, ''H'', is given by the formula

:&lt;math&gt;H = \log_2 N^L = L\log_2 N = L {\log N \over \log 2}&lt;/math&gt;

where ''N'' is the number of possible symbols and ''L'' is the number of symbols in the password. ''H'' is measured in [[bit]]s.&lt;ref name=NIST/&gt;&lt;ref&gt;Schneier, B: ''Applied Cryptography'', 2e, page 233 ff. John Wiley and Sons.&lt;/ref&gt; In the last expression, ''log'' can be to any [[Base (exponentiation)|base]].

:{| class="wikitable" style="text-align: right;"
|+ Entropy per symbol for different symbol sets
! Symbol set || Symbol count ''N'' || Entropy per symbol ''H''
|-
| align=left|[[Arabic numerals]] (0–9) (e.g. [[Personal identification number|PIN]]) || 10 || {{Rnd|3.3219280948874|3}} bits
|-
| align=left|[[hexadecimal]] numerals (0–9, A–F) (e.g. [[Wired Equivalent Privacy|WEP]] keys) || 16 || 4.000 bits
|-
| align=left|[[Case sensitivity|Case insensitive]] [[Latin alphabet]] (a–z or A–Z) || 26 || {{Rnd|4.7004397181411|3}} bits
|-
| align=left|Case insensitive [[alphanumeric]] (a–z or A–Z, 0–9) || 36 || {{Rnd|5.1699250014423|3}} bits
|-
| align=left|[[Case sensitivity|Case sensitive]] Latin alphabet (a–z, A–Z) || 52 || {{Rnd|5.7004397181411|3}} bits
|-
| align=left|Case sensitive alphanumeric (a–z, A–Z, 0–9) || 62 || {{Rnd|5.9541963103869|3}} bits
|-
| align=left|All [[Printable characters|ASCII printable characters]] except space || 94 || {{Rnd|6.55458885|3}} bits
|-
| align=left|All [[Printable characters|ASCII printable characters]] || 95 || {{Rnd|6.5698556083309|3}} bits
|-
| align=left|All [[Extended ASCII|extended ASCII printable characters]] || 218 || {{Rnd|7.7681843247769|3}} bits
|-
| align=left|[[Binary number|Binary]] (0–255 or 8 [[bit]]s or 1 [[byte]]) || 256 || {{Rnd|8|3}} bits
|-
| align=left|[[Diceware]] word list || 7776 || {{Rnd|12.924812503606|3}} bits per word
|}

A [[binary number|binary]] [[byte]] is usually expressed using two hexadecimal characters.

To find the length, ''L,'' needed to achieve a desired strength ''H,'' with a password drawn randomly from a set of ''N'' symbols, one computes

:&lt;math&gt;L = {H \over \log_2 N}&lt;/math&gt;, rounded up to the next largest [[natural number|whole number]].

The following table uses this formula to show the required lengths of truly randomly generated passwords to achieve desired password entropies for common symbol sets:

{| class="wikitable"
|+ Lengths ''L'' of truly randomly generated passwords required to achieve a desired password entropy ''H'' for symbol sets containing ''N'' symbols.
|-
! Desired password entropy ''H'' !! [[Arabic numerals]] !! [[Hexadecimal]] !! [[Case sensitivity|Case insensitive]] [[Latin alphabet]] !! Case insensitive [[alphanumeric]] !! [[Case sensitivity|Case sensitive]] Latin alphabet !! Case sensitive alphanumeric !! All [[Printable characters|ASCII printable characters]] !! All [[Extended ASCII|extended ASCII printable characters]] !! [[Diceware]] word list
|-
| 8 bits (1 byte) || 3 || 2 || 2 || 2 || 2 || 2 || 2 || 2 || 1 word
|-
| 32 bits (4 bytes) || 10 || 8 || 7 || 7 || 6 || 6 || 5 || 5 || 3 words
|-
| 40 bits (5 bytes) || 13 || 10 || 9 || 8 || 8 || 7 || 7 || 6 || 4 words
|-
| 64 bits (8 bytes) || 20 || 16 || 14 || 13 || 12 || 11 || 10 || 9 || 5 words
|-
| 80 bits (10 bytes) || 25 || 20 || 18 || 16 || 15 || 14 || 13 || 11 || 7 words
|-
| 96 bits (12 bytes) || 29 || 24 || 21 || 19 || 17 || 17 || 15 || 13 || 8 words
|-
| 128 bits (16 bytes) || 39 || 32 || 28 || 25 || 23 || 22 || 20 || 17 || 10 words
|-
| 160 bits (20 bytes) || 49 || 40 || 35 || 31 || 29 || 27 || 25 || 21 || 13 words
|-
| 192 bits (24 bytes) || 58 || 48 || 41 || 38 || 34 || 33 || 30 || 25 || 15 words
|-
| 224 bits (28 bytes) || 68 || 56 || 48 || 44 || 40 || 38 || 35 || 29 || 18 words
|-
| 256 bits (32 bytes) || 78 || 64 || 55 || 50 || 45 || 43 || 39 || 33 || 20 words
|}

=== Human-generated passwords ===
People are notoriously poor at achieving sufficient entropy to produce satisfactory passwords. According to one study involving half a million users, the average password entropy was estimated at 40.54 bits.&lt;ref&gt;{{cite journal|last1=Florencio|first1=Dinei|last2=Herley|first2=Cormac|title=A Large-Scale Study of Web Password Habits|journal=Proceeds of the International World Wide Web Conference Committee|date=May 8, 2007|url=http://research.microsoft.com/pubs/74164/www2007.pdf|ref=ACM 978-1-59593-654-7/07/0005.|deadurl=no|archiveurl=https://web.archive.org/web/20150327031521/http://research.microsoft.com/pubs/74164/www2007.pdf|archivedate=March 27, 2015|df=}}&lt;/ref&gt;  Some stage magicians exploit this inability for amusement, in a minor way, by divining supposed random choices (of numbers, say) made by audience members.

Thus, in one analysis of over 3 million eight-character passwords, the letter "e" was used over 1.5 million times, while the letter "f" was used only 250,000 times. A [[uniform distribution (discrete)|uniform distribution]] would have had each character being used about 900,000 times. The most common number used is "1", whereas the most common letters are a, e, o, and r.&lt;ref name=perfect /&gt;

Users rarely make full use of larger character sets in forming passwords. For example, hacking results obtained from a MySpace phishing scheme in 2006 revealed 34,000 passwords, of which only 8.3% used mixed case, numbers, and symbols.&lt;ref name=myspace-passwords&gt;{{cite news | url = http://archive.wired.com/politics/security/commentary/securitymatters/2006/12/72300?currentPage=all | title = MySpace Passwords aren't so Dumb | author = Bruce Schneier | publisher = Wired Magazine | accessdate = April 11, 2008 | date = December 14, 2006 | deadurl = no | archiveurl = https://web.archive.org/web/20140521031354/http://archive.wired.com/politics/security/commentary/securitymatters/2006/12/72300?currentPage=all | archivedate = May 21, 2014 | df =  }}&lt;/ref&gt;

The full strength associated with using the entire ASCII character set (numerals, mixed case letters and special characters) is only achieved if each possible password is equally likely.  This seems to suggest that all passwords must contain characters from each of several character classes, perhaps upper and lower case letters, numbers, and non-alphanumeric characters.  In fact, such a requirement is a pattern in password choice and can be expected to reduce an attacker's "work factor" (in Claude Shannon's terms).  This is a reduction in password "strength".  A better requirement would be to require a password NOT to contain any word in an online dictionary, or list of names, or any license plate pattern from any state (in the US) or country (as in the EU). If patterned choices are required, humans are likely to use them in predictable ways, such a capitalizing a letter, adding one or two numbers, and a special character. This predictability means that the increase in password strength is minor when compared to random passwords.

==== NIST Special Publication 800-63-2 ====
[[NIST]] Special Publication 800-63 of June 2004 (revision 2) suggested the following scheme to roughly estimate the entropy of human-generated passwords:&lt;ref name=NIST&gt;{{cite web | url = http://csrc.nist.gov/publications/nistpubs/800-63/SP800-63v6_3_3.pdf | title = SP 800-63 – Electronic Authentication Guideline | format = PDF | publisher = NIST | accessdate = April 20, 2014 | deadurl = yes | archiveurl = https://web.archive.org/web/20040712152833/http://csrc.nist.gov/publications/nistpubs/800-63/SP800-63v6_3_3.pdf | archivedate = July 12, 2004 | df =  }}&lt;/ref&gt;
* The entropy of the first character is four bits;
* The entropy of the next seven characters are two bits per character;
* The ninth through the twentieth character has 1.5 bits of entropy per character;
* Characters 21 and above have one bit of entropy per character.
* A "bonus" of six bits is added if both upper case letters and non-alphabetic characters are used.
* A "bonus" of six bits is added for passwords of length 1 through 19 characters following an extensive dictionary check to ensure the password is not contained within a large dictionary. Passwords of 20 characters or more do not receive this bonus because it is assumed they are pass-phrases consisting of multiple dictionary words.

Using this scheme, an eight-character human-selected password without upper case letters and non-alphabetic characters is estimated to have 18 bits of entropy. The NIST publication concedes that at the time of development, little information was available on the real world selection of passwords.

Later research into human-selected password entropy using newly available real world data has demonstrated that the NIST scheme does not provide a valid metric for entropy estimation of human-selected passwords.&lt;ref name=WeirEtAl&gt;{{cite web | url = http://reusablesec.blogspot.com/2010/10/new-paper-on-password-security-metrics.html | title = Testing Metrics for Password Creation Policies by Attacking Large Sets of Revealed Passwords | format = PDF | author1 = Matt Weir | author2 = Susdhir Aggarwal | author3 = Michael Collins | author4 = Henry Stern | accessdate = March 21, 2012 | deadurl = no | archiveurl = https://web.archive.org/web/20120706124704/http://reusablesec.blogspot.com/2010/10/new-paper-on-password-security-metrics.html | archivedate = July 6, 2012 | df =  }}&lt;/ref&gt; The June 2017 revision of SP 800-63 (Revision 3) drops this approach.&lt;ref&gt;{{cite web | url = https://pages.nist.gov/800-63-3 | title = SP 800-63-3 – Digital Identity Guidelines | format = PDF | publisher = NIST | date = June 2017 | accessdate = August 6, 2017 | deadurl = no | archiveurl = https://web.archive.org/web/20170806142240/https://pages.nist.gov/800-63-3/ | archivedate = August 6, 2017 | df =  }}&lt;/ref&gt;

===Usability and implementation considerations===
Because national keyboard implementations vary, not all 94 ASCII printable characters can be used everywhere.&lt;!-- The situation of assorted 7-bit ASCII character sets is long and sordid, quite indeterminate, and basically a dog's breakfast. The advent of Unicode / UCS has at least 'settled' what the lowest 128 characters are, though this is not exactly what most of the prior character sets had. --&gt; This can present a problem to an international traveler who wished to log into remote system using a keyboard on a local computer. ''See'' [[List of Latin-script keyboard layouts|''keyboard layout'']]. Many hand held devices, such as [[tablet computer]]s and [[smart phone]]s, require complex shift sequences to enter special characters.

Authentication programs vary in which characters they allow in passwords. Some do not recognize case differences (e.g., the upper-case "E" is considered equivalent to the lower-case "e"), others prohibit some of the other symbols. In the past few decades, systems have permitted more characters in passwords, but limitations still exist. Systems also vary in the maximum length of passwords allowed.

As a practical matter, passwords must be both reasonable and functional for the end user as well as strong enough for the intended purpose. Passwords that are too difficult to remember may be forgotten and so are more likely to be written on paper, which some consider a security risk.&lt;ref name=Gartner&gt;{{cite web | url = http://www.indevis.de/dokumente/gartner_passwords_breakpoint.pdf | title = Passwords are Near the Breaking Point | format = PDF | publisher = Gartner | author = A. Allan | accessdate = April 10, 2008 | deadurl = yes | archiveurl = https://web.archive.org/web/20060427032938/http://www.indevis.de/dokumente/gartner_passwords_breakpoint.pdf | archivedate = April 27, 2006 | df =  }}&lt;/ref&gt; In contrast, others argue that forcing users to remember passwords without assistance can only accommodate weak passwords, and thus poses a greater security risk. According to [[Bruce Schneier]], most people are good at securing their wallets or purses, which is a "great place" to store a written password.&lt;ref name=Schneier-writedown&gt;{{cite web | url = http://www.schneier.com/blog/archives/2005/06/write_down_your.html | title = Schneier on Security | work = Write Down Your Password | author = Bruce Schneier | accessdate = April 10, 2008 | deadurl = no | archiveurl = https://web.archive.org/web/20080413032636/http://www.schneier.com/blog/archives/2005/06/write_down_your.html | archivedate = April 13, 2008 | df =  }}&lt;/ref&gt;

== Required bits of entropy ==
The minimum number of bits of entropy needed for a password depends on the [[threat model]] for the given application.  If [[key stretching]] is not used, passwords with more entropy are needed.  RFC 4086, "Randomness Requirements for Security", presents some example threat models and how to calculate the entropy desired for each one.&lt;ref&gt;{{cite IETF | title = Randomness Requirements for Security | rfc = 4086}}&lt;/ref&gt;  Their answers vary between 29 bits of entropy needed if only online attacks are expected, and up to 96 bits of entropy needed for important cryptographic keys used in applications like encryption where the password or key needs to be secure for a long period of time and stretching isn't applicable.  A 2010 [[Georgia Tech Research Institute]] study based on unstretched keys recommended a 12-character random password, but as a minimum length requirement.&lt;ref name="gtri"&gt;{{cite web|url=http://www.gtri.gatech.edu/casestudy/Teraflop-Troubles-Power-Graphics-Processing-Units-GPUs-Password-Security-System|title=Teraflop Troubles: The Power of Graphics Processing Units May Threaten the World's Password Security System|publisher=[[Georgia Tech Research Institute]]|accessdate=2010-11-07|deadurl=no|archiveurl=https://web.archive.org/web/20101230063449/http://www.gtri.gatech.edu/casestudy/Teraflop-Troubles-Power-Graphics-Processing-Units-GPUs-Password-Security-System|archivedate=2010-12-30|df=}}&lt;/ref&gt;&lt;ref name="msnbc"&gt;{{cite news|url=http://www.msnbc.msn.com/id/38771772/|title=Want to deter hackers? Make your password longer|publisher=[[MSNBC]]|date=2010-08-19|accessdate=2010-11-07|deadurl=no|archiveurl=https://web.archive.org/web/20101029055718/http://www.msnbc.msn.com/id/38771772|archivedate=2010-10-29|df=}}&lt;/ref&gt;

The upper end is related to the stringent requirements of choosing keys used in encryption.  In 1999, [[EFF DES cracker|an Electronic Frontier Foundation project]] broke 56-bit [[Data Encryption Standard|DES]] encryption in less than a day using specially designed hardware.&lt;ref name=EFF-deep-crack&gt;{{cite web | url = http://w2.eff.org/Privacy/Crypto/Crypto_misc/DESCracker/HTML/19980716_eff_descracker_pressrel.html | title = EFF DES Cracker machine brings honesty to crypto debate | publisher = EFF | accessdate = March 27, 2008 | deadurl = yes | archiveurl = https://web.archive.org/web/20100101001853/http://w2.eff.org/Privacy/Crypto/Crypto_misc/DESCracker/HTML/19980716_eff_descracker_pressrel.html | archivedate = January 1, 2010 | df =  }}&lt;/ref&gt; In 2002, ''[[distributed.net]]'' cracked a 64-bit key in 4 years, 9 months, and 23 days.&lt;ref name=distributed&gt;{{cite web | url = http://stats.distributed.net/projects.php?project_id=5 | title = 64-bit key project status | publisher = Distributed.net | accessdate = March 27, 2008 }}&lt;/ref&gt; As of October 12, 2011, ''distributed.net'' estimates that cracking a 72-bit key using current hardware will take about 45,579 days or 124.8 years.&lt;ref name=distributed-72&gt;{{cite web | url = http://stats.distributed.net/projects.php?project_id=8 | title = 72-bit key project status | publisher = Distributed.net | accessdate = October 12, 2011 }}&lt;/ref&gt; Due to currently understood limitations from fundamental physics, there is no expectation that any [[digital computer]] (or combination) will be capable of breaking 256-bit encryption via a brute-force attack.&lt;ref name=schneier-cyptogram&gt;{{cite web | url = http://www.schneier.com/crypto-gram-9902.html | title = Snakeoil: Warning Sign #5: Ridiculous key lengths | author = Bruce Schneier | accessdate = March 27, 2008 | deadurl = no | archiveurl = https://web.archive.org/web/20080418225248/http://www.schneier.com/crypto-gram-9902.html | archivedate = April 18, 2008 | df =  }}&lt;/ref&gt; Whether or not [[quantum computers]] will be able to do so in practice is still unknown, though theoretical analysis suggests such possibilities.&lt;ref&gt;{{cite web |url=https://stackoverflow.com/questions/2768807/quantum-computing-and-encryption-breaking |title=Quantum Computing and Encryption Breaking |publisher=Stack Overflow |date=2011-05-27 |accessdate=2013-03-17 |deadurl=no |archiveurl=https://web.archive.org/web/20130521043721/http://stackoverflow.com/questions/2768807/quantum-computing-and-encryption-breaking |archivedate=2013-05-21 |df= }}&lt;/ref&gt;

== Guidelines for strong passwords ==

=== Common guidelines ===

Guidelines for choosing good passwords are typically designed to make passwords harder to discover by intelligent guessing. Common guidelines advocated by proponents of software system security include:&lt;ref&gt;Microsoft Corporation, [http://www.microsoft.com/protect/yourself/password/create.mspx Strong passwords: How to create and use them] {{webarchive|url=https://web.archive.org/web/20080101132156/http://www.microsoft.com/protect/yourself/password/create.mspx |date=2008-01-01 }}&lt;/ref&gt;&lt;ref name="schneier07"&gt;Bruce Schneier, [http://www.schneier.com/blog/archives/2007/01/choosing_secure.html Choosing Secure Passwords] {{webarchive|url=https://web.archive.org/web/20080223002450/http://www.schneier.com/blog/archives/2007/01/choosing_secure.html |date=2008-02-23 }}&lt;/ref&gt;&lt;ref&gt;Google, Inc., [https://www.google.com/accounts/PasswordHelp How safe is your password?] {{webarchive|url=https://web.archive.org/web/20080222225549/https://www.google.com/accounts/PasswordHelp |date=2008-02-22 }}&lt;/ref&gt;&lt;ref name="UMD01"&gt;University of Maryland, [http://www.cs.umd.edu/faq/Passwords.shtml Choosing a Good Password] {{webarchive|url=https://web.archive.org/web/20140614022254/http://www.cs.umd.edu/faq/Passwords.shtml |date=2014-06-14 }}&lt;/ref&gt;&lt;ref name="Bidwell000"&gt;{{cite book
| first     = Teri
| last      = Bidwell
| title     = Hack Proofing Your Identity in the Information Age
| publisher = Syngress Publishing
| year      = 2002
| isbn      = 1-931836-51-5
}}&lt;/ref&gt;
* Use a minimum password length of 8 or more characters if permitted.
* Include lowercase and uppercase alphabetic characters, numbers and symbols if permitted.
* Generate passwords randomly where feasible.
* Avoid using the same password twice (e.g., across multiple user accounts and/or software systems).
* Avoid character repetition, keyboard patterns, dictionary words, letter or number sequences, usernames, relative or pet names, romantic links (current or past) and biographical information (e.g., ID numbers, ancestors' names or dates).
* Avoid using information that is or might become publicly associated with the user or the account.
* Avoid using information that the user's colleagues and/or acquaintances might know to be associated with the user.
* Do not use passwords which consist wholly of any simple combination of the aforementioned weak components.

Some guidelines advise against writing passwords down, while others, noting the large numbers of password protected systems users must access, encourage writing down passwords as long as the written password lists are kept in a safe place, not attached to a monitor or in an unlocked desk drawer.&lt;ref name="schneier.com"&gt;{{cite web|url=http://www.schneier.com/blog/archives/2005/06/write_down_your.html|title=Write Down Your Password - Schneier on Security|website=www.schneier.com|deadurl=no|archiveurl=https://web.archive.org/web/20080413032636/http://www.schneier.com/blog/archives/2005/06/write_down_your.html|archivedate=2008-04-13|df=}}&lt;/ref&gt;

The possible character set for a password can be constrained by different web sites or by the range of keyboards on which the password must be entered.&lt;ref&gt;E.g., for a keyboard with only 17 nonalphanumeric characters, see one for a BlackBerry phone in [http://www.hardwaresecrets.com/fullimage.php?image=18705 an enlarged image]  {{webarchive|url=https://web.archive.org/web/20110406121058/http://www.hardwaresecrets.com/fullimage.php?image=18705 |date=2011-04-06 }} in support of [http://www.hardwaresecrets.com/article/795/2 Sandy Berger, ''BlackBerry Tour 9630 (Verizon) Cell Phone Review'', in Hardware Secrets (August 31, 2009)] {{webarchive|url=https://web.archive.org/web/20110406121111/http://www.hardwaresecrets.com/article/795/2 |date=April 6, 2011 }}, both as accessed January 19, 2010. That some websites don’t allow nonalphanumerics is indicated by [http://forums.theregister.co.uk/post/527230 Kanhef, ''Idiots, For Different Reasons'' (June 30, 2009) (topic post)] {{webarchive|url=https://web.archive.org/web/20110406121058/http://forums.theregister.co.uk/post/527230 |date=April 6, 2011 }}, as accessed January 20, 2010.&lt;/ref&gt;

=== Examples of weak passwords ===
{{See also|Password cracking|List of the most common passwords}}

As with any security measure, passwords vary in effectiveness (i.e., strength); some are weaker than others. For example, the difference in weakness between a dictionary word and a word with obfuscation (i.e., letters in the password are substituted by, say, numbers — a common approach) may cost a password cracking device a few more seconds; this adds little strength. The examples below illustrate various ways weak passwords might be constructed, all of which are based on simple patterns which result in extremely low entropy, allowing them to be tested automatically at high speeds.:&lt;ref name=perfect&gt;{{cite book
  | last = Burnett
  | first = Mark
  | editor = Kleiman, Dave
  | title = Perfect Passwords
  | publisher = Syngress Publishing
  | year = 2006
  | location = Rockland, Massachusetts
  | isbn = 1-59749-041-5
  | page = 181
  | editor-link = Dave Kleiman }}&lt;/ref&gt;
* [[Default password]]s (as supplied by the system vendor and meant to be changed at installation time): ''password'', ''default'', ''admin'', ''guest'', etc. Lists of default passwords are widely available on the internet.
* Dictionary words: ''chameleon'', ''RedSox'', ''sandbags'', ''bunnyhop!'', ''IntenseCrabtree'', etc., including words in non-English dictionaries.
* Words with numbers appended: ''password1'', ''deer2000'', ''john1234'', etc., can be easily tested automatically with little lost time.
* Words with simple obfuscation: ''p@ssw0rd'', ''l33th4x0r'', ''g0ldf1sh'', etc., can be tested automatically with little additional effort. For example, a domain administrator password compromised in the [[DigiNotar]] attack was reportedly ''Pr0d@dm1n.''&lt;ref&gt;{{cite web |url=http://thehackernews.com/2011/09/comodohacker-responsible-for-diginotar.html |title=ComodoHacker responsible for DigiNotar Attack – Hacking News |publisher=Thehackernews.com |date=2011-09-06 |accessdate=2013-03-17 |deadurl=no |archiveurl=https://web.archive.org/web/20130517204022/http://thehackernews.com/2011/09/comodohacker-responsible-for-diginotar.html |archivedate=2013-05-17 |df= }}&lt;/ref&gt;
* Doubled words: ''crabcrab'', ''stopstop'', ''treetree'', ''passpass'', etc.
* Common sequences from a keyboard row: ''qwerty'', ''[[Spaceballs#Plot|12345]]'', ''asdfgh'', ''fred'', etc.
* Numeric sequences based on well known numbers such as 911 &lt;sup&gt;([[9-1-1]], [[September 11 attacks|9/11]])&lt;/sup&gt;, 314159... &lt;sup&gt;([[pi]])&lt;/sup&gt;, 27182... &lt;sup&gt;([[E (mathematical constant)|e]])&lt;/sup&gt;, 112 &lt;sup&gt;([[112 (emergency telephone number)|1-1-2]])&lt;/sup&gt;, etc.
* Identifiers: ''jsmith123'', ''1/1/1970'', ''555–1234'', one's username, etc.
* Anything personally related to an individual: license plate number, Social Security number, current or past telephone numbers, student ID, current address, previous addresses, birthday, sports team, relative's or pet's names/nicknames/birthdays/initials, etc., can easily be tested automatically after a simple investigation of a person's details.

There are many other ways a password can be weak,&lt;ref&gt;Bidwell, p. 87&lt;/ref&gt; corresponding to the strengths of various attack schemes; the core principle is that a password should have high entropy (usually taken to be equivalent to randomness) and ''not'' be readily derivable by any "clever" pattern, nor should passwords be mixed with information identifying the user. On-line services often provide a restore password function that a hacker can figure out and by doing so bypass a password. Choosing hard-to-guess restore password questions can further secure the password.&lt;ref&gt;{{cite web |url=http://www.lockdown.co.uk/?pg=password_guide |title=Guidelines for choosing a good password |publisher=Lockdown.co.uk |date=2009-07-10 |accessdate=2013-03-17 |deadurl=no |archiveurl=https://web.archive.org/web/20130326163635/http://www.lockdown.co.uk/?pg=password_guide |archivedate=2013-03-26 |df= }}&lt;/ref&gt;

=== Rethinking password change guidelines ===
{{outdated section|reason="standards of today" and processor speeds have both change significantly since 2012|date=September 2017}} 
In December, 2012, [[William Cheswick]] wrote an article published in ACM magazine that included the mathematical possibilities of how easy or difficult it would be to break passwords that are constructed using the commonly recommended, and sometimes followed, standards of today. In his article, William showed that a standard eight character alpha-numeric password could withstand a brute force attack of ten million attempts per second, and remain unbroken for 252 days. Ten million attempts each second is the acceptable rate of attempts using a multi-core system that most users would have access to. A much greater degree of attempts, at the rate of 7 billion per second, could also be achieved when using modern GPUs. At this rate, the same 8 character alpha-numeric password could be broken in approximately 30 seconds. Increasing the password complexity to a 13 character alpha-numeric password increases the time needed to crack it to more than 900,000 years at 7 billion attempts per second. This is, of course, assuming the password does not use a common word that a dictionary attack could break much sooner. Using a password of this strength reduces the obligation to change it as often as many organizations require, including the U.S. Government, as it could not be reasonably broken in such a short period of time.&lt;ref&gt;Cheswick, William. ''Rethinking Passwords''. Association of Computing Machinery, 2012&lt;/ref&gt;

== Password policy ==
{{Main|Password policy}}

A password policy is a guide to choosing satisfactory passwords. It is intended to:
* assist users in choosing strong passwords
* ensure the passwords are suited to the target population
* provide recommendations for users with regard to the handling of their passwords
* impose a requirement to change any password which has been lost or compromised, and perhaps that no password be used longer than a limited time
* (in some cases) prescribe the pattern of characters which passwords must contain
* use a [[Blacklist (computing)#Usernames and passwords|password blacklist]] to block the usage of weak or easily guessed passwords.

For example, password expiration is often covered by password policies. Password expiration serves two purposes:&lt;ref name=LOPSA&gt;{{cite web | url = http://lopsa.org/node/295 | title = In Defense of Password Expiration | publisher = League of Professional Systems Administrators | accessdate = April 14, 2008 | deadurl = yes | archiveurl = https://web.archive.org/web/20081012063918/http://lopsa.org/node/295 | archivedate = October 12, 2008 | df =  }}&lt;/ref&gt;
* If the time to crack a password is estimated to be 100 days, password expiration times fewer than 100 days may help ensure insufficient time for an attacker.
* If a password has been compromised, requiring it to be changed regularly should limit the access time for the attacker.
However, password expiration has its drawbacks:&lt;ref name=WEB&gt;
{{cite web
 |url         = https://www.cesg.gov.uk/articles/problems-forcing-regular-password-expiry
 |title       = The problems with forcing regular password expiry
 |date        = 15 April 2016
 |work        = IA Matters
 |publisher   = CESG: the Information Security Arm of GCHQ
 |accessdate  = 5 Aug 2016
 |deadurl     = yes
 |archiveurl  = https://web.archive.org/web/20160817223701/https://www.cesg.gov.uk/articles/problems-forcing-regular-password-expiry
 |archivedate = 17 August 2016
 |df          = 
}}
&lt;/ref&gt;&lt;ref name=CERIAS&gt;{{cite web | url = http://www.cerias.purdue.edu/weblogs/spaf/general/post-30/ | title = Security Myths and Passwords | publisher = The Center for Education and Research in Information Assurance and Security | author = Eugene Spafford | accessdate = April 14, 2008 | deadurl = no | archiveurl = https://web.archive.org/web/20080411123000/http://www.cerias.purdue.edu/weblogs/spaf/general/post-30/ | archivedate = April 11, 2008 | df =  }}&lt;/ref&gt;
* Asking users to change passwords frequently encourages simple, weak passwords.
* If one has a truly strong password, there is little point in changing it. Changing passwords which are already strong introduces risk that the new password may be less strong.
* A compromised password is likely to be used immediately by an attacker to install a [[backdoor (computing)|backdoor]], often via [[privilege escalation]]. Once this is accomplished, password changes won't prevent future attacker access.
* Moving from never changing one's password to changing the password on every authenticate attempt (pass ''or'' fail attempts) only doubles the number of attempts the attacker must make on average before guessing the password in a brute force attack. One gains ''much'' more security by just increasing the password length by one character than changing the password on every use.

=== Creating and handling passwords ===

The hardest passwords to crack, for a given length and character set, are random character strings; if long enough they resist brute force attacks (because there are many characters) and guessing attacks (due to high entropy). However, such passwords are typically the hardest to remember. The imposition of a requirement for such passwords in a password policy may encourage users to write them down, store them in PDAs or cellphones, or share them with others as a safeguard against memory failure. While some people consider each of these user resorts to increase security risks, others suggest the absurdity of expecting users to remember distinct complex passwords for each of the dozens of accounts they access. For example, in 2005, security expert [[Bruce Schneier]] recommended writing down one's password:

{{Quote|quote=''Simply, people can no longer remember passwords good enough to reliably defend against dictionary attacks, and are much more secure if they choose a password too complicated to remember and then write it down. We're all good at securing small pieces of paper. I recommend that people write their passwords down on a small piece of paper, and keep it with their other valuable small pieces of paper: in their wallet''.&lt;ref name="schneier.com"/&gt;}}

The following measures may increase acceptance of strong password requirements, if carefully used:
* a training program. Also, updated training for those who fail to follow the password policy (lost passwords, inadequate passwords, etc.).
* rewarding strong password users by reducing the rate, or eliminating altogether, the need for password changes (password expiration). The strength of user-chosen passwords can be estimated by automatic programs which inspect and evaluate proposed passwords, when setting or changing a password.
* displaying to each user the last login date and time in the hope that the user may notice unauthorized access, suggesting a compromised password.
* allowing users to reset their passwords via an automatic system, which reduces help desk call volume. However, some systems are themselves insecure; for instance, easily guessed or researched answers to password reset questions bypass the advantages of a strong password system.
* using randomly generated passwords that do not allow users to choose their own passwords, or at least offering randomly generated passwords as an option.

=== Memory techniques ===

Password policies sometimes suggest memory techniques to assist remembering passwords:
* mnemonic passwords: Some users develop [[mnemonic]] phrases and use them to generate more or less random passwords which are nevertheless relatively easy for the user to remember. For instance, the first letter of each word in a memorable phrase. Research estimates the password strength of such passwords to be about 3.7 bits per character, compared to the 6.6 bits for random passwords from ASCII printable characters.&lt;ref&gt;{{cite conference |url=https://www.internetsociety.org/sites/default/files/ndss2017_03A-4_Kiesel_paper.pdf |title=A Large-scale Analysis of the Mnemonic Password Advice |author=Johannes Kiesel |authorlink= |author2=Benno Stein |author3=Stefan Lucks |year=2017 |publisher=Internet Society |booktitle=Proceedings of the 24th Annual Network and Distributed System Security Symposium (NDSS 17) |deadurl=no |archiveurl=https://web.archive.org/web/20170330174637/https://www.internetsociety.org/sites/default/files/ndss2017_03A-4_Kiesel_paper.pdf |archivedate=2017-03-30 |df= }}&lt;/ref&gt; Silly ones are possibly more memorable.&lt;ref&gt;[http://uc.iupui.edu/uploadedFiles/Learning_Center_Site/Mnemonic%20Devices.pdf ''Mnemonic Devices'' (Indianapolis, Ind.: Bepko Learning Ctr., University College)], as accessed January 19, 2010 {{webarchive |url=https://web.archive.org/web/20100610000727/http://uc.iupui.edu/uploadedFiles/Learning_Center_Site/Mnemonic%20Devices.pdf |date=June 10, 2010 }}&lt;/ref&gt; Another way to make random-appearing passwords more memorable is to use random words (see [[diceware]]) or syllables instead of randomly chosen letters.
* after-the-fact mnemonics: After the password has been established, invent a mnemonic that fits.&lt;ref&gt;[http://changingminds.org/techniques/memory/remembering_passwords.htm Remembering Passwords (ChangingMinds.org)] {{webarchive|url=http://archive.wikiwix.com/cache/20100121181700/http://changingminds.org/techniques/memory/remembering_passwords.htm |date=2010-01-21 }}, as accessed January 19, 2010&lt;/ref&gt; It does not have to be reasonable or sensible, only memorable. This allows passwords to be random.
* visual representations of passwords: a password is memorized based on a sequence of keys pressed, not the values of the keys themselves, e.g. a sequence !qAsdE#2 represents a [[rhomboid]] on a US keyboard. The method to produce such passwords is called PsychoPass,&lt;ref name=":10"&gt;{{cite journal | last1 = Cipresso | first1 = P | last2 = Gaggioli | first2 = A | last3 = Serino | first3 = S | last4 = Cipresso | first4 = S | last5 = Riva | first5 = G | year = 2012 | title = How to Create Memorizable and Strong Passwords | journal = J Med Internet Res | volume = 14 | issue = 1| page = e10 | doi = 10.2196/jmir.1906 | pmid = 22233980 | pmc=3846346}}&lt;/ref&gt; moreover, such spatially patterned passwords can be improved.&lt;ref&gt;{{cite journal | pmc = 3742392 | pmid=23942458 | doi=10.2196/jmir.2366 | volume=15 | title=Security analysis and improvements to the PsychoPass method. | year=2013 | journal=J Med Internet Res | page=e161 | last1 = Brumen | first1 = B | last2 = Heričko | first2 = M | last3 = Rozman | first3 = I | last4 = Hölbl | first4 = M}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://blogs.dropbox.com/tech/2012/04/zxcvbn-realistic-password-strength-estimation/|title=zxcvbn: realistic password strength estimation|website=Dropbox Tech Blog|deadurl=no|archiveurl=https://web.archive.org/web/20150405131234/https://blogs.dropbox.com/tech/2012/04/zxcvbn-realistic-password-strength-estimation/|archivedate=2015-04-05|df=}}&lt;/ref&gt;
* password patterns: Any pattern in a password makes guessing (automated or not) easier and reduces an attacker's work factor.
** For example, passwords of the following case-insensitive form: consonant, vowel, consonant, consonant, vowel, consonant, number, number (for example ''pinray45'') are called Environ passwords.&lt;ref&gt;{{cite book
| first     = Ross
| last      = Anderson
| year      = 2001
| title     = Security engineering: A guide to building dependable distributed systems
| publisher = John Wiley &amp; Sons, Inc.
| isbn      = 0470068523
}}&lt;/ref&gt; The pattern of alternating vowel and consonant characters was intended to make passwords more likely to be pronounceable and thus more memorable. Unfortunately, such patterns severely reduce the password's [[information entropy]], making [[brute force attack|brute force]] password attacks considerably more efficient. In the UK in October 2005, employees of [[Departments of the United Kingdom Government|the British government]] were advised to use passwords in this form.{{Citation needed|date=January 2012}}

=== Protecting passwords ===
{{how-to|date=March 2013}}
Computer users are generally advised to "never write down a password anywhere, no matter what" and "never use the same password for more than one account."&lt;ref&gt;{{Cite news|url=https://www.telegraph.co.uk/finance/personalfinance/bank-accounts/12149022/Use-the-same-password-for-everything-Youre-fuelling-a-surge-in-current-account-fraud.html|title=Use the same password for everything? You're fuelling a surge in current account fraud|last=Morley|first=Katie|date=2016-02-10|work=Telegraph.co.uk|access-date=2017-05-22|language=en|deadurl=no|archiveurl=https://web.archive.org/web/20170513044756/http://www.telegraph.co.uk/finance/personalfinance/bank-accounts/12149022/Use-the-same-password-for-everything-Youre-fuelling-a-surge-in-current-account-fraud.html|archivedate=2017-05-13|df=}}&lt;/ref&gt; However, an ordinary computer user may have dozens of password-protected accounts. Users with multiple accounts needing passwords often give up and use the same password for every account. When varied password complexity requirements prevent use of the same (memorable) scheme for producing high-strength passwords, oversimplified passwords will often be created to satisfy irritating and conflicting password requirements.
A [[Microsoft]] expert was quoted as saying at a 2005 security conference: "I claim that password policy should say you should write down your password. I have 68 different passwords. If I am not allowed to write any of them down, guess what I am going to do? I am going to use the same password on every one of them."&lt;ref&gt;[http://www.cnet.com/news/microsoft-security-guru-jot-down-your-passwords/ Microsoft security guru: Jot down your passwords] {{webarchive|url=https://web.archive.org/web/20160205211730/http://www.cnet.com/news/microsoft-security-guru-jot-down-your-passwords/ |date=2016-02-05 }}, ''c\net'' Retrieved on 2016-02-02&lt;/ref&gt;

If passwords are written down, they should never be kept in obvious places such as address books, [[Rolodex]] files, under drawers or keyboards, or behind pictures. Perhaps the worst, but all too common, location is a [[Post-It]] note on the computer monitor. Better locations are a [[safe deposit box]] or a locked file approved for information of sensitivity comparable to that protected by the password. Most locks on office file cabinets are far from adequate. Software is available for popular hand-held computers that can store passwords for numerous accounts in encrypted form. Another approach is to [[Encryption|encrypt]] by hand on paper and remember the encryption method and key.&lt;ref&gt;Simple methods (e.g., [[Rot13|ROT13]] and [[Cipher#Historical ciphers|some other old ciphers]]) may suffice; for more sophisticated hand-methods see [http://www.schneier.com/solitaire.html Bruce Schneier, The Solitaire Encryption Algorithm (May 26, 1999) (ver. 1.2)] {{webarchive|url=https://web.archive.org/web/20151113061059/https://www.schneier.com/solitaire.html |date=November 13, 2015 }}, as accessed January 19, 2010, and [http://www.ibm.com/developerworks/power/library/pa-bigiron5/ Sam Siewert, ''Big Iron Lessons, Part 5: Introduction to Cryptography, From Egypt Through Enigma'' (IBM, July 26, 2005)] {{webarchive|url=https://web.archive.org/web/20100803202847/http://www.ibm.com/developerworks/power/library/pa-bigiron5/ |date=August 3, 2010 }}, as accessed January 19, 2010.&lt;/ref&gt; And another approach is to use a single password or slightly varying passwords for low-security accounts and select distinctly separate strong passwords for a smaller number of high-value applications such as for [[online banking]].

Another effective approach for remembering multiple passwords is to memorize a single "master" password and use software to generate a new password for each application, based on the master password and the application's name. This approach is used by Stanford's PwdHash,&lt;ref&gt;{{cite conference |url=http://crypto.stanford.edu/PwdHash/pwdhash.pdf |title=Stronger Password Authentication Using Browser Extensions |author=Blake Ross |authorlink= |author2=Collin Jackson |author3=Nicholas Miyake |author4=Dan Boneh |author5=John C. Mitchell |year=2005 |publisher=USENIX |booktitle=Proceedings of the 14th Usenix Security Symposium |pages=17–32 |deadurl=no |archiveurl=http://archive.wikiwix.com/cache/20120429031741/http://crypto.stanford.edu/PwdHash/pwdhash.pdf |archivedate=2012-04-29 |df= }}&lt;/ref&gt; Princeton's Password Multiplier,&lt;ref&gt;{{cite conference |url=http://www.cs.utexas.edu/~bwaters/publications/papers/www2005.pdf |title=A Convenient Method for Securely Managing Passwords |author=[[J. Alex Halderman]] |authorlink= |author2=Brent Waters |author3=Edward W. Felten |year=2005 |publisher=ACM |pages=1–9 |deadurl=no |archiveurl=https://web.archive.org/web/20160115062049/http://www.cs.utexas.edu/~bwaters/publications/papers/www2005.pdf |archivedate=2016-01-15 |df= }}&lt;/ref&gt; and other stateless password managers. In this approach, protecting the master password is essential, as all passwords are compromised if the master password is revealed, and lost if the master password is forgotten or misplaced.

==Password managers==
{{Main|Password manager}}
A reasonable compromise for using large numbers of passwords is to record them in a password manager program, which include stand-alone applications, web browser extensions, or a manager built into the operating system. A password manager allows the user to use hundreds of different passwords, and only have to remember a single password, the one which opens the encrypted password database. Needless to say, this single password should be strong and well-protected (not recorded anywhere). Most password managers can automatically create strong passwords using a cryptographically secure [[random password generator]], as well as calculating the entropy of the generated password. A good password manager will provide resistance against attacks such as [[key logging]], clipboard logging and various other memory spying techniques.

==See also==

{{Portal|Computer security}}
* [[Keystroke logging]]
* [[Passphrase]]
* [[Phishing]]
* [[Vulnerability (computing)]]

== References ==
{{Reflist|30em}}

== External links ==
* [http://tools.ietf.org/html/rfc4086 RFC 4086: Randomness Requirements for Security]
* [https://web.archive.org/web/20160416035311/http://www.architectingsecurity.com/2010/09/11/password-patterns/ Password Patterns:The next generation dictionary attacks]

{{DEFAULTSORT:Password Strength}}
[[Category:Cryptography]]
[[Category:Password authentication]]</text>
      <sha1>0b8s0a1e8qoayils14di9h6uhes6w5v</sha1>
    </revision>
  </page>
  <page>
    <title>Poincaré inequality</title>
    <ns>0</ns>
    <id>4722074</id>
    <revision>
      <id>863744855</id>
      <parentid>850899017</parentid>
      <timestamp>2018-10-12T18:50:06Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <comment>This instance of "rad" is one place where it's really conspicuous that \text{} does not yield the same results as \operatorname{}.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7711">In [[mathematics]], the '''Poincaré inequality''' is a result in the theory of [[Sobolev space]]s, named after the [[France|French]] [[mathematician]] [[Henri Poincaré]]. The inequality allows one to obtain bounds on a function using bounds on its derivatives and the geometry of its domain of definition. Such bounds are of great importance in the modern, [[Direct method in calculus of variations|direct methods of the calculus of variations]]. A very closely related result is the [[Friedrichs' inequality]].

==Statement of the inequality==

===The classical Poincaré inequality===

Let ''p'', so that 1&amp;nbsp;≤&amp;nbsp;''p''&amp;nbsp;&lt;&amp;nbsp;∞ and Ω a subset with at least one bound. Then there exists a constant ''C'', depending only on Ω and ''p'', so that, for every function ''u'' of the [[Sobolev space]] ''W''&lt;sub&gt;0&lt;/sub&gt;&lt;sup&gt;1,''p''&lt;/sup&gt;(Ω) of zero-trace functions,

:&lt;math&gt;\| u \|_{L^{p} (\Omega)} \leq C \| \nabla u \|_{L^{p} (\Omega)}.&lt;/math&gt;

===Poincaré–Wirtinger inequality===

Assume that 1&amp;nbsp;≤&amp;nbsp;''p''&amp;nbsp;≤&amp;nbsp;∞ and that Ω is a [[bounded set|bounded]] [[connected set|connected]] [[open set|open subset]] of the ''n''-[[dimension]]al [[Euclidean space]] '''R'''&lt;sup&gt;''n''&lt;/sup&gt; with a [[Lipschitz boundary]] (i.e., Ω is a [[Lipschitz domain|Lipschitz]] [[Domain (mathematical analysis)|domain]]). Then there exists a constant ''C'', depending only on Ω and ''p'', such that for every function ''u'' in the Sobolev space ''W''&lt;sup&gt;1,''p''&lt;/sup&gt;(Ω),

:&lt;math&gt;\| u - u_{\Omega} \|_{L^{p} (\Omega)} \leq C \| \nabla u \|_{L^{p} (\Omega)},&lt;/math&gt;

where

:&lt;math&gt;u_{\Omega} = \frac{1}{|\Omega|} \int_{\Omega} u(y) \, \mathrm{d} y&lt;/math&gt;

is the average value of ''u'' over Ω, with |Ω| standing for the [[Lebesgue measure]] of the domain Ω. When Ω is a ball, the above inequality is
called a (p,p)-Poincaré inequality; for more general domains Ω, the above is more familiarly known as a Sobolev inequality.

===Generalizations===

In the context of metric measure spaces (for example, sub-Riemannian manifolds), such spaces support a (q,p)-Poincare inequality for some &lt;math&gt;1\le q,p&lt;\infty&lt;/math&gt; if there are constants C and &lt;math&gt;\lambda\ge 1&lt;/math&gt; so that for each ball B in the space,

:&lt;math&gt;\mu(B)^{-\frac{1}{q}} \left \|u-u_B \right \|_{L^q(B)}\le C \operatorname{rad}(B) \mu(B)^{-\frac{1}{p}} \| \nabla u\|_{L^p(\lambda B)}.&lt;/math&gt;

In the context of metric measure spaces, &lt;math&gt;|\nabla u|&lt;/math&gt; is the minimal p-weak upper gradient of u in the sense of 
Heinonen and Koskela [J. Heinonen and P. Koskela, Quasiconformal maps in metric spaces with controlled geometry, Acta Math. 181 (1998), 1–61]

There exist other generalizations of the Poincaré inequality to other Sobolev spaces.  For example, the following (taken from {{harvtxt|Garroni|Müller|2005}}) is a Poincaré inequality for the Sobolev space ''H''&lt;sup&gt;1/2&lt;/sup&gt;('''T'''&lt;sup&gt;2&lt;/sup&gt;), i.e. the space of functions ''u'' in the [[Lp space|''L''&lt;sup&gt;2&lt;/sup&gt; space]] of the unit [[torus]] '''T'''&lt;sup&gt;2&lt;/sup&gt; with [[Fourier transform]] ''û'' satisfying

:&lt;math&gt;[u]_{H^{1/2} (\mathbf{T}^{2})}^2 = \sum_{k \in \mathbf{Z}^2} | k | \left | \hat{u} (k) \right |^2 &lt; + \infty:&lt;/math&gt;

there exists a constant ''C'' such that, for every ''u''&amp;nbsp;∈&amp;nbsp;''H''&lt;sup&gt;1/2&lt;/sup&gt;('''T'''&lt;sup&gt;2&lt;/sup&gt;) with ''u'' identically zero on an open set ''E''&amp;nbsp;⊆&amp;nbsp;'''T'''&lt;sup&gt;2&lt;/sup&gt;,

:&lt;math&gt;\int_{\mathbf{T}^2} | u(x) |^2 \, \mathrm{d} x \leq C \left( 1 + \frac1{\operatorname{cap} (E \times \{ 0 \})} \right) [ u ]_{H^{1/2} (\mathbf{T}^2)}^2,&lt;/math&gt;

where cap(''E''&amp;nbsp;&amp;times;&amp;nbsp;{0}) denotes the [[harmonic capacity]] of ''E''&amp;nbsp;&amp;times;&amp;nbsp;{0} when thought of as a subset of '''R'''&lt;sup&gt;3&lt;/sup&gt;.

==The Poincaré constant==

The optimal constant ''C'' in the Poincaré inequality is sometimes known as the '''Poincaré constant''' for the domain Ω. Determining the Poincaré constant is, in general, a very hard task that depends upon the value of ''p'' and the geometry of the domain Ω. Certain special cases are tractable, however. For example, if Ω is a [[bounded set|bounded]], [[convex set|convex]], Lipschitz domain with [[diameter]] ''d'', then the Poincaré constant is at most ''d''/2 for ''p''&amp;nbsp;=&amp;nbsp;1, &lt;math&gt;d/\pi&lt;/math&gt; for ''p''&amp;nbsp;=&amp;nbsp;2 ({{harvnb|Acosta|Durán|2004}}; {{harvnb|Payne|Weinberger|1960}}), and this is the best possible estimate on the Poincaré constant in terms of the diameter alone. For smooth functions, this can be understood as an application of the [[isoperimetric inequality]] to the function's [[level sets]]. [http://maze5.net/?page_id=790] In one dimension, this is [[Wirtinger's inequality for functions]].

However, in some special cases the constant ''C'' can be determined concretely. For example, for ''p''&amp;nbsp;=&amp;nbsp;2, it is well known that over the domain of unit isosceles right triangle, ''C''&amp;nbsp;=&amp;nbsp;1/π (&amp;nbsp;&lt;&amp;nbsp;''d''/π where &lt;math&gt;d=\sqrt{2}&lt;/math&gt;). (See, for instance,{{harvtxt|Kikuchi|Liu|2007}}.)

Furthermore, for a smooth, bounded domain &lt;math&gt;\Omega&lt;/math&gt;, since the [[Rayleigh quotient]] for the [[Laplace operator]] in the space &lt;math&gt;W^{1,2}_0(\Omega)&lt;/math&gt; is minimized by the eigenfunction corresponding to the minimal eigenvalue λ&lt;sub&gt;1&lt;/sub&gt; of the (negative) Laplacian, it is a simple consequence that, for any &lt;math&gt;u\in W^{1,2}_0(\Omega)&lt;/math&gt;,

:&lt;math&gt; \|u\|_{L^2}^2\leq \lambda_1^{-1} \left \|\nabla u\right \|_{L^2}^2&lt;/math&gt;

and furthermore, that the constant λ&lt;sub&gt;1&lt;/sub&gt; is optimal.

==See also==
*[[Friedrichs' inequality]]
*[[Korn's inequality]]

==References==

* {{citation
| last1 = Acosta|first1=Gabriel|last2=Durán|first2=Ricardo G.
| title = An optimal Poincaré inequality in ''L''&lt;sup&gt;1&lt;/sup&gt; for convex domains
| journal = Proc. Amer. Math. Soc.
| volume = 132
| year = 2004
| issue = 1
| pages = 195&amp;ndash;202 (electronic)
| doi = 10.1090/S0002-9939-03-07004-7
}}
* {{citation
| last = Evans|first=Lawrence C. 
| title = Partial differential equations 
| location = Providence, RI 
| publisher = American Mathematical Society 
| year = 1998
| isbn = 0-8218-0772-2
}}
* {{citation
| first1 = Fumio
| last1 = Kikuchi 
| first2= Xuefeng|last2=Liu
| title = Estimation of interpolation error constants for the P0 and P1 triangular finite elements
| journal = Comput. Methods. Appl. Mech. Engrg.
| volume = 196
| year = 2007
| pages = 3750&amp;ndash;3758
| doi = 10.1016/j.cma.2006.10.029
| issue = 37–40
}} {{MathSciNet|id=2340000}}
* {{citation
| last1 = Garroni
| first1 = Adriana
| last2 = Müller |first2 = Stefan
| title = &amp;Gamma;-limit of a phase-field model of dislocations
| journal = SIAM J. Math. Anal.
| volume = 36
| year = 2005
| issue = 6
| pages = 1943&amp;ndash;1964 (electronic)
| doi = 10.1137/S003614100343768X
}} {{MathSciNet|id=2178227}}
* Leoni, Giovanni (2009), ''[http://bookstore.ams.org/gsm-105 A First Course in Sobolev Spaces]'', Graduate Studies in Mathematics, American Mathematical Society, pp. xvi+607 {{isbn|978-0-8218-4768-8}}, {{MR|2527916}}, {{Zbl|1180.46001}}, [http://www.maa.org/press/maa-reviews/a-first-course-in-sobolev-spaces MAA]
*{{Citation | last1=Payne | first1=L. E. | last2=Weinberger | first2=H. F. | title=An optimal Poincaré inequality for convex domains | year=1960 | journal=Archive for Rational Mechanics and Analysis | issn=0003-9527 | pages=286–292}}
*{{Citation | last1=Heinonen | first1=J. | last2=Koskela | first2=P. | title=Quasiconformal maps in metric spaces with controlled geometry | journal=Acta Mathematica |   doi=  10.1007/BF02392747 |issn=  1871-2509 | year=1998 | pages=1–61}}

{{DEFAULTSORT:Poincare inequality}}
[[Category:Theorems in analysis]]
[[Category:Inequalities]]
[[Category:Sobolev spaces]]</text>
      <sha1>4ip53avuthrilcjj8b2gdyjppl20har</sha1>
    </revision>
  </page>
  <page>
    <title>Pre-intuitionism</title>
    <ns>0</ns>
    <id>1196185</id>
    <revision>
      <id>860512735</id>
      <parentid>857497917</parentid>
      <timestamp>2018-09-21T04:46:35Z</timestamp>
      <contributor>
        <username>Jerodlycett</username>
        <id>24269494</id>
      </contributor>
      <minor/>
      <comment>[[WP:WCW]] project (Reference list missing)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7146">In the [[philosophy of mathematics|mathematical philosophy]], the '''pre-intuitionists''' were a small but influential group who informally shared similar philosophies on the nature of mathematics. The term itself was used by [[L. E. J. Brouwer]], who in his 1951 lectures at [[University of Cambridge|Cambridge]] described the differences between [[intuitionism]] and its predecessors:&lt;ref name=CW&gt;Luitzen Egbertus Jan Brouwer (edited by [[Arend Heyting]], ''Collected Works'', North-Holland, 1975, p. 509.&lt;/ref&gt;

&lt;blockquote&gt;Of a totally different orientation &lt;nowiki&gt;[&lt;/nowiki&gt;from the "Old Formalist School" of [[Richard Dedekind|Dedekind]], [[Georg Cantor|Cantor]], [[Giuseppe Peano|Peano]],  [[Ernst Zermelo|Zermelo]], and [[Louis Couturat|Couturat]], etc.&lt;nowiki&gt;]&lt;/nowiki&gt; was the Pre-Intuitionist School, mainly led by [[Henri Poincaré|Poincaré]], [[Émile Borel|Borel]] and [[Henri Lebesgue|Lebesgue]]. These thinkers seem to have maintained a modified observational standpoint for the '''introduction of natural numbers''', for '''the principle of complete induction''' &lt;nowiki&gt;[&lt;/nowiki&gt;...&lt;nowiki&gt;]&lt;/nowiki&gt; For these, even for such theorems as were deduced by means of classical logic, they postulated an existence and exactness independent of language and logic and regarded its non-contradictority as certain, even without logical proof. For the continuum, however, they seem not to have sought an origin strictly extraneous to language and logic.&lt;/blockquote&gt;

==The introduction of natural numbers==

The pre-intuitionists, as defined by [[Luitzen Egbertus Jan Brouwer]], differed from the [[Formalism (philosophy of mathematics)|formalist]] standpoint in several ways,&lt;ref name=CW/&gt; particularly in regard to the introduction of natural numbers, or how the natural numbers are defined/denoted. For [[Henri Poincaré|Poincaré]], the definition of a mathematical entity is the construction of the entity itself and not an expression of an underlying essence or existence.

This is to say that no mathematical object exists without human construction of it, both in mind and language.

==The principle of complete induction==
This sense of definition allowed [[Henri Poincaré|Poincaré]] to argue with [[Bertrand Russell]] over [[Giuseppe Peano| Giuseppe Peano's]] [[Peano axioms|axiomatic theory of natural numbers]].

[[Giuseppe Peano|Peano's]] fifth [[axiom]] states: 
*Allow that; zero has a property ''P''; 
*And; if every natural number less than a number ''x'' has the property ''P'' then ''x'' also has the property ''P''. 
*Therefore; every natural number has the property ''P''.

This is the principle of [[complete induction]], which establishes the property of [[mathematical induction|induction]] as necessary to the system. Since [[Giuseppe Peano|Peano's]] axiom is as [[infinity|infinite]] as the [[natural number]]s, it is difficult to prove that the property of ''P'' does belong to any ''x'' and also ''x''&amp;nbsp;+&amp;nbsp;1. What one can do is say that, if after some number ''n'' of trials that show a property ''P'' conserved in ''x'' and ''x''&amp;nbsp;+&amp;nbsp;1, then we may infer that it will still hold to be true after ''n''&amp;nbsp;+&amp;nbsp;1 trials. But this is itself induction. And hence the argument is a [[begging the question|vicious circle]].

From this [[Henri Poincaré|Poincaré]] argues that if we fail to establish the consistency of [[Giuseppe Peano|Peano's]] axioms for natural numbers without falling into circularity, then the principle of [[complete induction]] is not provable by [[logic|general logic]].

Thus arithmetic and mathematics in general is not [[analytic proposition|analytic]] but [[synthetic proposition|synthetic]]. [[Logicism]] thus rebuked and [[intuitionism|Intuition]] is held up. What [[Henri Poincaré|Poincaré]] and the Pre-Intuitionists shared was the perception of a difference between logic and mathematics that is not a matter of [[language]] alone, but of [[knowledge]] itself.

==Arguments over the excluded middle==
It was for this assertion, among others, that [[Henri Poincaré|Poincaré]] was considered to be similar to the intuitionists. For [[Luitzen Egbertus Jan Brouwer|Brouwer]] though, the Pre-Intuitionists failed to go as far as necessary in divesting mathematics from metaphysics, for they still used ''principium tertii exclusi'' (the "[[law of excluded middle]]").

The principle of the excluded middle does lead to some strange situations. For instance, statements about the future such as "There will be a naval battle tomorrow" do not seem to be either true or false, ''yet''. So there is some question whether statements must be either true or false in some [[temporal logic|situations]]. To an intuitionist this seems to rank the law of excluded middle as just as un[[rigour|rigorous]] as [[Giuseppe Peano|Peano's]] vicious circle.

Yet to the Pre-Intuitionists this is mixing apples and oranges. For them mathematics was one thing (a muddled invention of the human mind, ''i.e.'', synthetic), and logic was another (analytic).

==Other pre-intuitionists==
The above examples only include the works of [[Henri Poincaré|Poincaré]], and yet [[Luitzen Egbertus Jan Brouwer|Brouwer]] named other mathematicians as Pre-Intuitionists too; [[Émile Borel|Borel]] and [[Henri Lebesgue|Lebesgue]]. Other mathematicians such as [[Hermann Weyl]] (who eventually became disenchanted with intuitionism, feeling that it places excessive strictures on mathematical progress) and [[Leopold Kronecker]] also played a role—though they are not cited by [[Luitzen Egbertus Jan Brouwer|Brouwer]] in his definitive speech.

In fact [[Leopold Kronecker|Kronecker]] might be the most famous of the Pre-Intuitionists for his singular and oft quoted phrase, "God made the natural numbers; all else is the work of man."

[[Leopold Kronecker|Kronecker]]  goes in almost the opposite direction from [[Henri Poincaré|Poincaré]], believing in the natural numbers but not the law of the excluded middle. He was the first mathematician to express doubt on [[nonconstructive proof|non-constructive]] [[existence theorem|existence proofs]] that state that something must exist because it can be shown that it is "impossible" for it not to.

== See also ==
* [[Conventionalism]]

== References ==
{{Reflist}}
*[https://web.archive.org/web/20051029080803/http://www.journalofcriticalrealism.org/archive/ALETHIAv3n2_straathof8.pdf Logical Meanderings] – a brief article by Jan Sraathof on [[Luitzen Egbertus Jan Brouwer|Brouwer]]'s various attacks on arguments of the Pre-Intuitionists about the Principle of the Excluded Third.
*[https://web.archive.org/web/20050111050713/http://www.acmsonline.org/Detlefsen87.pdf Proof And Intuition] – an article on the many varieties of knowledge as they relate to the Intuitionist and Logicist.
*[http://www.marxists.org/reference/subject/philosophy/works/ne/brouwer.htm Brouwer's Cambridge Lectures on Intuitionism] – wherein [[Luitzen Egbertus Jan Brouwer|Brouwer]] talks about the Pre-Intuitionist School and addresses what he sees as its many shortcomings.

[[Category:Theories of deduction]]
[[Category:History of mathematics]]</text>
      <sha1>hprcw6crt605j6gdrwhwtf9rfqz9agr</sha1>
    </revision>
  </page>
  <page>
    <title>Purification of quantum state</title>
    <ns>0</ns>
    <id>5583245</id>
    <revision>
      <id>868618242</id>
      <parentid>844132680</parentid>
      <timestamp>2018-11-13T11:02:56Z</timestamp>
      <contributor>
        <ip>68.170.77.93</ip>
      </contributor>
      <comment>Formatting</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4176">In [[quantum mechanics]], especially [[quantum information]], '''purification''' refers to the fact that every [[Mixed state (physics)|mixed state]] acting on [[Finite dimensional hilbert spaces|finite-dimensional Hilbert spaces]] can be viewed as the [[partial trace|reduced state]] of some pure state.

In purely linear algebraic terms, it can be viewed as a statement about [[positive-semidefinite matrix|positive-semidefinite matrices]].

== Statement ==

Let ρ be a [[density matrix]] acting on a [[Hilbert space]] &lt;math&gt;H_A&lt;/math&gt; of finite dimension ''n''. Then it is possible to construct a second Hilbert space &lt;math&gt;H_B&lt;/math&gt; and a pure state &lt;math&gt;| \psi \rangle \in H_A \otimes H_B&lt;/math&gt; such that ρ is the partial trace of &lt;math&gt;| \psi \rangle \langle \psi |&lt;/math&gt; with respect to &lt;math&gt;H_B&lt;/math&gt;.  While the initial Hilbert space &lt;math&gt;H_A&lt;/math&gt; might correspond to physically meaningful quantities, the second Hilbert space &lt;math&gt;H_B&lt;/math&gt; needn't have any physical interpretation whatsoever.  However, in physics the process of state purification is assumed to be physical, and so the second Hilbert space &lt;math&gt;H_B&lt;/math&gt; should also correspond to a physical space, such as the environment.  The exact form of &lt;math&gt;H_B&lt;/math&gt; in such cases will depend on the problem. Here is a [[proof of principle]], showing that at very least &lt;math&gt;H_B&lt;/math&gt; has to have dimensions greater than or equal to   &lt;math&gt;H_A&lt;/math&gt;.

With these statements in mind, if,
:&lt;math&gt;\operatorname{tr_B} \left( | \psi \rangle \langle \psi | \right )= \rho,&lt;/math&gt;

we say that &lt;math&gt;| \psi \rangle&lt;/math&gt; purifies &lt;math&gt;\rho&lt;/math&gt;.

=== Proof ===

A density matrix is by definition positive semidefinite. So ρ can be [[Diagonalizable matrix|diagonalized]] and written as &lt;math&gt;\rho = \sum_{i =1} ^n p_i | i \rangle \langle i |&lt;/math&gt; for some [[Basis (linear algebra)|basis]] &lt;math&gt;\{ | i \rangle \}&lt;/math&gt;. Let &lt;math&gt;H_B&lt;/math&gt; be another copy of the  ''n''-dimensional Hilbert space with an [[orthonormal basis]] &lt;math&gt;\{ | i' \rangle \}&lt;/math&gt;. Define &lt;math&gt;| \psi \rangle \in H_A \otimes H_B&lt;/math&gt; by

:&lt;math&gt;| \psi \rangle = \sum_{i} \sqrt{p_i} |i \rangle \otimes | i' \rangle.&lt;/math&gt;

Direct calculation gives

:&lt;math&gt;
\begin{align}
\operatorname{tr_B} \left( | \psi \rangle \langle \psi | \right ) &amp;= 
\operatorname{tr_B} \left[\left( \sum_{i} \sqrt{p_i} |i \rangle \otimes | i' \rangle \right ) \left( \sum_{j} \sqrt{p_j} \langle j | \otimes \langle j'| \right) \right] \\
&amp;=\operatorname{tr_B} \left( \sum_{i, j} \sqrt{p_ip_j} |i \rangle \langle j | \otimes | i' \rangle \langle j'| \right ) \\
&amp;= \sum_{i,j} \delta_{ij} \sqrt{p_i p_j}| i \rangle \langle j | \\
&amp;= \rho.
\end{align} &lt;/math&gt;

This proves the claim.

==== Note ====

* The purification is not unique, but if during the construction of &lt;math&gt; | \psi \rangle &lt;/math&gt; in the proof above &lt;math&gt;H_B&lt;/math&gt; is generated by only the &lt;math&gt;\{ | i' \rangle \}&lt;/math&gt; for which &lt;math&gt; p_i &lt;/math&gt; is non-zero, any other purification &lt;math&gt; | \varphi \rangle &lt;/math&gt; on &lt;math&gt; H_A \otimes H_C &lt;/math&gt; induces an [[isometry]] &lt;math&gt; V:  H_B  \to  H_C &lt;/math&gt;  such that &lt;math&gt; | \varphi \rangle = (I \otimes V ) | \psi \rangle &lt;/math&gt;.
* The vectorial pure state &lt;math&gt;| \psi \rangle&lt;/math&gt; is in the form specified by the [[Schmidt decomposition]].
* Since [[square root]] [[Matrix decomposition|decompositions]] of a positive semidefinite matrix are not unique, neither are purifications.
* In linear algebraic terms, a square matrix is positive semidefinite [[if and only if]] it can be purified in the above sense. The ''if'' part of the implication follows immediately from the fact that the [[partial trace]] of a positive map remains a [[Choi's theorem on completely positive maps|positive map]].

== An application: Stinespring's theorem ==
{{Expand section|date=June 2008}}
By combining [[Choi's theorem on completely positive maps]] and purification of a mixed state, we can recover the [[Stinespring factorization theorem|Stinespring dilation theorem]] for the finite-dimensional case.

{{DEFAULTSORT:Purification Of Quantum State}}
[[Category:Linear algebra]]
[[Category:Quantum information science]]</text>
      <sha1>85oj7xnqz9nc1t11c677lrjxg81tjzd</sha1>
    </revision>
  </page>
  <page>
    <title>Quadratic growth</title>
    <ns>0</ns>
    <id>9228246</id>
    <revision>
      <id>830959271</id>
      <parentid>775286441</parentid>
      <timestamp>2018-03-17T22:56:23Z</timestamp>
      <contributor>
        <username>Rich Farmbrough</username>
        <id>82835</id>
      </contributor>
      <minor/>
      <comment>/* Examples */Spell out American postal abbreviations (Florida) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4357">{{For|other uses of the word "quadratic" in mathematics|Quadratic (disambiguation)}}
In [[mathematics]], a function or sequence is said to exhibit '''quadratic growth''' when its values are [[proportionality (mathematics)|proportional]] to the [[squaring|square]] of the function argument or sequence position. "Quadratic growth" often means more generally "quadratic growth in the limit", as the argument or sequence position goes to infinity – in [[big Theta notation]], ''f''(''x'')&amp;nbsp;=&amp;nbsp;Θ(''x''&lt;sup&gt;2&lt;/sup&gt;).&lt;ref&gt;{{citation|title=The Nature of Computation|first1=Cristopher|last1=Moore|authorlink=Cristopher Moore|first2=Stephan|last2=Mertens|publisher=Oxford University Press|year=2011|isbn=9780191620805|page=22|url=https://books.google.com/books?id=jnGKbpMV8xoC&amp;pg=PA22}}.&lt;/ref&gt; This can be defined both continuously (for a real-valued function of a real variable) or discretely (for a sequence of real numbers, i.e., real-valued function of an integer or natural number variable).

== Examples ==
Examples of quadratic growth include:

*Any [[quadratic polynomial]].
*Certain [[integer sequence]]s such as the [[triangular number]]s. The ''n''th triangular number has value ''n''(''n''+1)/2, approximately ''n''&lt;sup&gt;2&lt;/sup&gt;/2.
For a real function of a real variable, quadratic growth is equivalent to the second derivative being constant (i.e., the third derivative being zero), and thus functions with quadratic growth are exactly the quadratic polynomials, as these are the [[Kernel (linear operator)|kernel]] of the third derivative operator ''D''&lt;sup&gt;3&lt;/sup&gt;. Similarly, for a sequence (a real function of an integer or natural number variable), quadratic growth is equivalent to the second [[finite difference]] being constant (the third finite difference being zero),&lt;ref&gt;{{citation|title=Elementary Mathematical Models: Order Aplenty and a Glimpse of Chaos|first=Dan|last=Kalman|publisher=Cambridge University Press|year=1997|isbn=9780883857076|page=81|url=https://books.google.com/books?id=jhiZSkDtgvYC&amp;pg=PA81}}.&lt;/ref&gt; and thus a sequence with quadratic growth is also a quadratic polynomial. Indeed, an integer-valued sequence with quadratic growth is a polynomial in the zeroth, first, and second [[binomial coefficient]] with integer values. The coefficients can be determined by taking the [[Taylor polynomial]] (if continuous) or [[Difference operator#Newton.27s series|Newton polynomial]] (if discrete).

Algorithmic examples include:
*The amount of time taken in the worst case by certain [[algorithm]]s, such as [[insertion sort]], as a function of the input length.&lt;ref&gt;{{citation
 | last = Estivill-Castro | first = Vladimir
 | editor-last = Atallah | editor-first = Mikhail J. | editor-link = Mikhail Atallah
 | contribution = Sorting and order statistics
 | location = Boca Raton, Florida
 | mr = 1797171
 | pages = 3&amp;#45;1–3&amp;#45;25
 | publisher = CRC
 | title = Algorithms and Theory of Computation Handbook
 | year = 1999}}.&lt;/ref&gt;
*The numbers of live cells in space-filling [[cellular automaton]] patterns such as the [[Breeder (cellular automaton)|breeder]], as a function of the number of time steps for which the pattern is simulated.&lt;ref&gt;{{citation
 | last1 = Griffeath | first1 = David
 | last2 = Hickerson | first2 = Dean
 | contribution = A two-dimensional cellular automaton crystal with irrational density
 | location = New York
 | mr = 2079729
 | pages = 79–91
 | publisher = Oxford Univ. Press
 | series = St. Fe Inst. Stud. Sci. Complex.
 | title = New constructions in cellular automata
 | year = 2003}}. See in particular [https://books.google.com/books?id=_nTUZffjvb4C&amp;pg=PA81 p.&amp;nbsp;81]: "A breeder is any pattern which grows quadratically by creating a steady stream of copies of a second object, each of which creates a stream of a third."&lt;/ref&gt;
*[[Metcalfe's law]] stating that the value of a communications network grows quadratically as a function of its number of users&lt;ref&gt;{{citation|title=Bandwagon Effects in High-technology Industries|first=Jeffrey H.|last=Rohlfs|publisher=MIT Press|year=2003|isbn=9780262681384|contribution=3.3 Metcalfe's law|pages=29–30|url=https://books.google.com/books?id=rmFag8P4CF8C&amp;pg=PA29}}.&lt;/ref&gt;

==See also==
* [[Exponential growth]]

==References==
{{reflist}}

{{DEFAULTSORT:Quadratic Growth}}
[[Category:Asymptotic analysis]]


{{Mathanalysis-stub}}</text>
      <sha1>cneprg1513mstzy116t8g33ef4bjx70</sha1>
    </revision>
  </page>
  <page>
    <title>Reflected binary code</title>
    <ns>0</ns>
    <id>8757436</id>
    <redirect title="Gray code" />
    <revision>
      <id>703347003</id>
      <parentid>462068498</parentid>
      <timestamp>2016-02-04T23:37:49Z</timestamp>
      <contributor>
        <username>Matthiaspaul</username>
        <id>13467261</id>
      </contributor>
      <comment>+cat</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="112">#REDIRECT [[Gray code]] {{R from alternative name}}

[[Category:Data transmission]]
[[Category:Numeral systems]]</text>
      <sha1>oxx3hmymoukhplp3nhs3eiio08rxw21</sha1>
    </revision>
  </page>
  <page>
    <title>Rigorous Approach to Industrial Software Engineering</title>
    <ns>0</ns>
    <id>2576598</id>
    <revision>
      <id>840663638</id>
      <parentid>835097769</parentid>
      <timestamp>2018-05-11T09:50:49Z</timestamp>
      <contributor>
        <username>Rod57</username>
        <id>2542622</id>
      </contributor>
      <minor/>
      <comment>Rod57 moved page [[RAISE]] to [[Rigorous Approach to Industrial Software Engineering]]: Expansion of shared acronym </comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1083">{{About|software development tools|other uses|Raise (disambiguation)}}
'''RAISE''' (''Rigorous Approach to Industrial Software Engineering'') was developed as part of the European [[European Strategic Program on Research in Information Technology|ESPRIT]] II LaCoS project in the 1990s, led by [[Dines Bjørner]]. It consists of a set of tools designed for a [[specification language]] (RSL) for [[software development]]. It is especially espoused by [[UNU-IIST]] in [[Macau]], who run training courses on site and around the world, especially in [[developing countries]].

==See also==
*[[Formal methods]]
*[[Formal specification]]

==External links==
*[https://web.archive.org/web/20050829213910/http://www.iist.unu.edu/raise/ RAISE Virtual Library entry]
*[http://spd-web.terma.com/Projects/RAISE/ RAISE &amp;ndash; Rigorous Approach to Industrial Software Engineering]
*[http://www2.imm.dtu.dk/~db/raise/ RAISE information] from [[Dines Bjørner]]

[[Category:Formal specification languages]]
[[Category:Formal methods tools]]
[[Category:Software testing tools]]

{{compu-lang-stub}}</text>
      <sha1>9j5dpnstbfxrp3ccoy340yzl0bbs6fc</sha1>
    </revision>
  </page>
  <page>
    <title>Sheldon Axler</title>
    <ns>0</ns>
    <id>30456092</id>
    <revision>
      <id>845764097</id>
      <parentid>845764029</parentid>
      <timestamp>2018-06-13T23:51:48Z</timestamp>
      <contributor>
        <username>Turgidson</username>
        <id>1747755</id>
      </contributor>
      <comment>added [[Category:Mathematicians from Philadelphia]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4058">'''Sheldon Jay Axler''' (born 6 November 1949, [[Philadelphia]]) is an [[United States|American]] mathematician,  professor of mathematics and the Dean of the College of Science and Engineering at [[San Francisco State University]]. He has made contributions to mathematics education, publishing several mathematics textbooks.

He went to Palmetto High School at Miami, Florida (1967). He obtained his AB in mathematics with highest honors at [[Princeton University]] (1971) and his [[Ph.D.]] in mathematics, under professor [[Donald Sarason]], from the [[University of California, Berkeley]] (1975, Dissertation: "Subalgebras of &lt;math&gt;L^{\infty}&lt;/math&gt;"). As a postdoc he was a [[C. L. E. Moore instructor]] at the [[Massachusetts Institute of Technology]].

He taught for many years and became a Full Professor at [[Michigan State University]]. In 1997 Axler moved to [[San Francisco State University]] where he became the Chair of the Mathematics Department.

Axler received the [[Lester R. Ford Award]] for expository writing in 1996 from the [[Mathematical Association of America]].&lt;ref&gt;{{cite journal|author=Axler, Sheldon|title=Down with determinants!|journal=Amer. Math. Monthly|volume=102|year=1995|pages=139–154|url=http://www.maa.org/programs/maa-awards/writing-awards/down-with-determinants|doi=10.2307/2975348}}&lt;/ref&gt; In 2012 he became a fellow of the [[American Mathematical Society]].&lt;ref&gt;[http://www.ams.org/profession/fellows-list List of Fellows of the American Mathematical Society], retrieved 2012-11-03.&lt;/ref&gt;

He was an Associate Editor of the ''[[American Mathematical Monthly]]'' and the Editor-in-Chief of the [[Mathematical Intelligencer]].

Axler's book ''Linear Algebra Done Right'' eschews the use of [[determinant]]s, in favor of other methods.

==Books==
* ''Linear Algebra Done Right'', third edition, [[Undergraduate Texts in Mathematics]], Springer, 2015 (twelfth printing, 2009).
* (with John E. McCarthy, and Donald Sarason) editors. ''Holomorphic Spaces'', [[Cambridge University Press]] 1998.
* (with Paul Bourdon, and Wade Ramey) [https://books.google.com/books/about/Harmonic_Function_Theory.html?id=wATLzBfup-wC ''Harmonic Function Theory'', second edition], [[Graduate Texts in Mathematics]], Springer, 2001.
* ''Harmonic Function Theory software'', a [[Mathematica]] package for symbolic manipulation of [[harmonic function]]s, version 7.00, released 1 January 2009 (previous versions released in 1992, 1993, 1994, 1996, 1999, 2000, 2001, 2002, 2003, and 2008).
* ''Precalculus: A Prelude to Calculus'', Wiley, 2009 (third printing, 2010).
* (with [[Peter Rosenthal]] and Donald Sarason) editors. [https://books.google.com/books/about/A_Glimpse_at_Hilbert_Space_Operators.html?id=8rcv8Od38McC ''A Glimpse at Hilbert Space Operators''], [[Birkhäuser]], 2010.
* ''College Algebra'', [[John Wiley &amp; Sons]] 2011.
* ''Algebra &amp; Trigonometry'', John Wiley &amp; Sons, January 2011.

==References==
{{reflist}}

==External links==
* [http://www.axler.net/ Axler's Home Page]
* {{MathGenealogy |id=15605}}
* [http://www.sfsu.edu/~science/newsletters/spring2002.pdf College of Science &amp; Engineering Newsletter] from San Francisco State University.
* [http://www.ccst.us/ccstinfo/fellows/bios/axler.php Senior Fellow Sheldon Axler] from [[California Council on Science and Technology]]. 
* [https://zbmath.org/authors/?q=ai:axler.sheldon Author profile] in the database [[Zentralblatt MATH|zbMATH]]

{{Use dmy dates|date=February 2011}}

{{Authority control}}

{{DEFAULTSORT:Axler, Sheldon}}
[[Category:20th-century American mathematicians]]
[[Category:21st-century American mathematicians]]
[[Category:1949 births]]
[[Category:Living people]]
[[Category:San Francisco State University faculty]]
[[Category:Fellows of the American Mathematical Society]]
[[Category:Princeton University alumni]]
[[Category:Place of birth missing (living people)]]
[[Category:University of California, Berkeley faculty]]
[[Category:People from Miami]]
[[Category:Michigan State University faculty]]
[[Category:Mathematicians from Philadelphia]]</text>
      <sha1>cdyro04m188o9u8c27nljfoaorszrum</sha1>
    </revision>
  </page>
  <page>
    <title>Six circles theorem</title>
    <ns>0</ns>
    <id>19334943</id>
    <revision>
      <id>723900408</id>
      <parentid>723890861</parentid>
      <timestamp>2016-06-05T23:35:30Z</timestamp>
      <contributor>
        <username>Tom.Reding</username>
        <id>9784415</id>
      </contributor>
      <minor/>
      <comment>/* top */Fix [[:Category:CS1 errors: Vancouver style|Vancouver-style CS1 error(s)]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1525">[[File:Six circles theorem.svg|thumb|right|400px|Some examples of theorem configuration changing the radius of the first circle. In the last configuration the circles are pairwise coincident.]]

In [[geometry]], the '''six circles theorem''' relates to a chain of six [[circle]]s together with a [[triangle]], such that each circle is [[tangent circles|tangent]] to two sides of the triangle and also to the preceding circle in the chain.  The chain closes, in the sense that the sixth circle is always tangent to the first circle.&lt;ref&gt;{{cite book |vauthors=((Evelyn CJA)), Money-Coutts GB, Tyrrell JA | year = 1974 | title = The Seven Circles Theorem and Other New Theorems | publisher = Stacey International | location = London | isbn = 978-0-9503304-0-2|pages= 49–58}}&lt;/ref&gt;

The name may also refer to [[Miquel's six circles theorem]], the result that if five circles have four triple points of intersection then the remaining four points of intersection lie on a sixth circle.

== References ==
{{reflist}} 
* {{cite book | author = Wells D | year = 1991 | title = The Penguin Dictionary of Curious and Interesting Geometry | publisher = Penguin Books | location = New York | isbn = 0-14-011813-6 | pages = 231}}

==External links==
* {{MathWorld|title=Six Circles Theorem|urlname=SixCirclesTheorem}}
* [https://www.math.psu.edu/tabachni/prints/Circles.pdf The Six Circle Theorem revisited by D. Ivanov and S. Tabachnikov]

[[Category:Circles]]
[[Category:Theorems in geometry]]
[[Category:Theorems in plane geometry]]</text>
      <sha1>nc62i6j77l5vkiky3enkrpdwsdaqqfk</sha1>
    </revision>
  </page>
  <page>
    <title>Social complexity</title>
    <ns>0</ns>
    <id>984629</id>
    <revision>
      <id>832713702</id>
      <parentid>821898352</parentid>
      <timestamp>2018-03-27T16:10:05Z</timestamp>
      <contributor>
        <username>Bcastellani5</username>
        <id>33401056</id>
      </contributor>
      <comment>/* Further reading */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="22052">In [[sociology]], '''social complexity''' is a [[conceptual framework]] used in the [[analysis]] of society. Contemporary definitions of [[complexity]] in the [[science]]s are found in relation to [[systems theory]], in which a [[phenomenon]] under study has many parts and many possible arrangements of the relationships between those parts. At the same time, what is complex and what is simple is relative and may change with time.&lt;ref&gt;Waldrop, M. Mitchell (1992.) ''Complexity: The Emerging Science at the Edge of Order and Chaos.'' New York, NY: Simon &amp; Schuster.&lt;/ref&gt;

Current usage of the term "complexity" in the field of sociology typically refers specifically to theories of society as a [[complex adaptive system]]. However, social complexity and its [[emergence|emergent]] properties are central recurring themes throughout the historical development of [[Social philosophy|social thought]] and the study of [[social change]].&lt;ref name="CCS-MMT"&gt;Eve, Raymond, Sara Horsfall and Mary E. Lee (eds.) (1997). ''Chaos, Complexity and Sociology: Myths, Models, and Theories.'' Thousand Oaks, CA: Sage Publications.&lt;/ref&gt; The [[History of sociology|early founders]] of [[sociological theory]], such as [[Ferdinand Tönnies]], [[Émile Durkheim]], [[Max Weber]], [[Vilfredo Pareto]], and [[Georg Simmel]], all examined the exponential growth and increasing interrelatedness of social encounters and [[Social exchange theory|exchanges]]. This emphasis on [[interconnectivity]] in social relationships and the emergence of new properties within society is found in [[Social theory|theoretical thinking]] in multiple [[Subfields of sociology|areas of sociology]].&lt;ref name="AGid-79"&gt;Giddens, Anthony (1979). ''Central problems in Social Theory: Action, Structure and Contradiction in Social Analysis.'' London: Macmillan.&lt;/ref&gt; As a theoretical tool, social complexity theory serves as a basis for the connection of [[Microsociology|micro-]] and [[Macrosociology|macro]]-level social phenomena, providing a [[wikt:meso-|meso-]]level or [[Middle range theory (sociology)|middle-range]] theoretical platform for [[hypothesis]] formation.&lt;ref&gt;Freese, Lee (1980). "Formal Theorizing." ''Annual Review of Sociology'', 6: 187–212 (August 1980).&lt;/ref&gt;&lt;ref&gt;Cohen, B. P. (1989). ''Developing sociological knowledge: theory and method'' (2nd ed.). Chicago: Nelson–Hall.&lt;/ref&gt; [[Research methods#Research methods|Methodologically]], the concept of social complexity is theory-neutral, meaning that it accommodates both local ([[Microsociology|micro]]) and global ([[Macrosociology|macro]]) phenomena in sociological research.&lt;ref name="CCS-MMT" /&gt;

==Theoretical background==
[[File:Penrose tiling.gif|thumb|Illustration of complexity ([[Penrose tiling]] [[fractal]])]]
The American sociologist [[Talcott Parsons]] carried on the work of the early founders mentioned above in his early (1937) work on [[Action theory (sociology)|action theory]].&lt;ref name="Parsons-a"&gt;Parsons, Talcott ([1937] 1949). ''The Structure of Social Action: A Study in Social Theory with Special Reference to a Group of European Writers''. New York, NY: The Free Press.&lt;/ref&gt; By 1951, Parsons places these earlier ideas firmly into the realm of formal [[systems theory]] in ''The Social System''.&lt;ref name="Parsons-b"&gt;Parsons, Talcott (1951). ''The Social System''. New York, NY: The Free Press&lt;/ref&gt; For the next several decades, this synergy between general [[systems thinking]] and the further development of [[social system]] theories is carried forward by Parson's student, [[Robert K. Merton]], and a long line of others, in discussions of theories of the [[Middle range theory (sociology)|middle-range]] and [[Structure and agency|social structure and agency]]. During part of this same period, from the late 1970s through the early 1990s, discussion ensues in any number of other research areas about the properties of systems in which strong correlation of sub-parts leads to observed behaviors variously described as [[autopoiesis|autopoetic]], [[self-organization|self-organizing]], [[Dynamical system|dynamical]], [[turbulent]], and [[Chaotic system|chaotic]]. All of these are forms of system behavior arising from mathematical [[Complexity science|complexity]]. By the early 1990s, the work of social theorists such as [[Niklas Luhmann]]&lt;ref&gt;Luhmann, Niklas (1990.) ''Essays on Self-Reference'', New York: Columbia University Press.&lt;/ref&gt; began reflecting these themes of complex behavior.

One of the earliest usages of the term "complexity", in the [[Social science|social]] and [[behavioral sciences]], to refer specifically to a [[complex system]] is found in the study of [[Complexity theory and organizations|modern organizations]] and [[management studies]].&lt;ref&gt;Kiel, L. Douglas (1994). ''Managing Chaos and Complexity in Government: A New Paradigm for Managing Change, Innovation and Organizational Renewal.'' Jossey-Bass: San Francisco.&lt;/ref&gt; However, particularly in management studies, the term often has been used in a [[metaphor]]ical rather than in a [[Qualitative property|qualitative]] or [[quantitative property|quantiative]] theoretical manner.&lt;ref name="CCS-MMT" /&gt; By the mid-1990s, the "complexity turn"&lt;ref name=Urry&gt;Urry, John (2005). "The Complexity Turn." ''Theory, Culture and Society'', 22(5): 1–14.&lt;/ref&gt; in social sciences begins as some of the same tools generally used in [[complexity science]] are incorporated into the social sciences. By 1998, the international, electronic periodical, ''[[Journal of Artificial Societies and Social Simulation]]'', had been created. In the last several years, many publications have presented overviews of complexity theory within the field of sociology. Within this body of work, connections also are drawn to yet other theoretical traditions, including [[constructivist epistemology]] and the philosophical positions of [[Phenomenology (philosophy)|phenomenology]], [[postmodernism]] and [[critical realism (philosophy of the social sciences)|critical realism]].

==Methodologies==
Methodologically, social complexity is theory-neutral, meaning that it accommodates both local and global approaches to sociological research.&lt;ref name="CCS-MMT"/&gt; The very idea of social complexity arises out of the [[Historical comparative research|historical-comparative]] methods of early sociologists; obviously, this method is important in developing, defining, and refining the theoretical construct of social complexity. As complex social systems have many parts and there are many possible relationships between those parts, appropriate methodologies are typically determined to some degree by the research level of analysis [[Differentiation (sociology)|differentiated]]&lt;ref&gt;Luhmann, Niklas (1982). ''The Differentiation of Society.'' New York, NY: Columbia University Press.&lt;/ref&gt; by the researcher according to the level of description or explanation demanded by the research hypotheses.

At the most localized level of analysis, [[ethnographic]], [[Participant observation|participant-]] or non-participant observation, [[content analysis]] and other [[qualitative research]] methods may be appropriate. More recently, highly sophisticated [[quantitative research]] methodologies are being developed and used in sociology at both local and global [[level of analysis|levels of analysis]]. Such methods include (but are not limited to) [[bifurcation diagram]]s, [[Social network analysis|network analysis]], [[Nonlinear system|non-linear]] modeling, and [[Computational sociology|computational]] models including [[Cellular automaton|cellular automata]] programming, [[sociocybernetics]] and other methods of [[social simulation]].

===Complex social network analysis===
{{Main|Dynamic network analysis}}
Complex [[social network]] analysis is used to study the dynamics of large, complex social networks. [[Dynamic network analysis]] brings together traditional [[social network analysis]], [[link analysis]] and [[multi-agent system]]s within [[network science]] and [[network theory]].&lt;ref&gt;Carley, Kathleen M. (2003), "Dynamic Network Analysis." ''Dynamic Social Network Modeling and Analysis: Workshop Summary and Papers'', Ronald Breiger, Kathleen Carley, and Philippa Pattison (eds.), National Research Council (Committee on Human Factors): Washington, D.C.: 133–145.&lt;/ref&gt; Through the use of key concepts and methods in [[social network analysis]], [[agent-based modeling]], theoretical [[physics]], and modern [[mathematics]] (particularly [[graph theory]] and [[fractal geometry]]), this method of inquiry brought insights into the dynamics and structure of social systems. New computational methods of localized social network analysis are coming out of the work of [[Duncan Watts]], [[Albert-László Barabási]], [[Nicholas A. Christakis]], [[Kathleen Carley]] and others.

New methods of global network analysis are emerging from the work of [[John Urry (sociologist)|John Urry]] and the sociological study of globalization, linked to the work of [[Manuel Castells]] and the later work of [[Immanuel Wallerstein]]. Since the late 1990s, Wallerstein increasingly makes use of complexity theory, particularly the work of [[Ilya Prigogine]].&lt;ref&gt;Barabási, Albert-László (2003). ''Linked: The New Science of Networks.'' Cambridge, MA: Perseus Publishing.&lt;/ref&gt;&lt;ref&gt;Freeman, Linton C. (2004). ''The Development of Social Network Analysis: A Study in the Sociology of Science.'' Vancouver Canada: Empirical Press.&lt;/ref&gt;&lt;ref&gt;Watts, Duncan J. (2004). "The New Science of Networks." ''Annual Review of Sociology'', 30: 243–270.&lt;/ref&gt; Dynamic social network analysis is linked to a variety of methodological traditions, above and beyond [[systems thinking]], including [[graph theory]], traditional [[social network]] analysis in sociology, and [[mathematical sociology]]. It also links to [[chaos theory|mathematical chaos]] and [[complex dynamics]] through the work of [[Duncan Watts]] and [[Steven Strogatz]], as well as fractal geometry through [[Albert-László Barabási]] and his work on [[scale-free networks]].

===Computational sociology===
{{Main|Computational sociology}}
The development of [[computational sociology]] involves such scholars as [[Nigel Gilbert]], [[Klaus G. Troitzsch]], [[Joshua M. Epstein]], and others. The foci of methods in this field include [[social simulation]] and [[data-mining]], both of which are sub-areas of computational sociology. Social simulation uses computers to create an artificial laboratory for the study of complex social systems; [[Data mining|data-mining]] uses machine intelligence to search for non-trivial patterns of relations in large, complex, real-world databases. The emerging methods of [[socionics]] are a variant of computational sociology.&lt;ref&gt;Gilbert, Nigel and Klaus G. Troitzsch (2005). ''Simulation for Social Scientists'', 2nd Edition. New York, NY: Open University Press.&lt;/ref&gt;&lt;ref name=epstein07&gt;Epstein, Joshua M. (2007). ''Generative Social Science: Studies in Agent-Based Computational Modeling''. Princeton, NJ: Princeton University Press.&lt;/ref&gt;

Computational sociology is influenced by a number of micro-sociological areas as well as the macro-level traditions of systems science and systems thinking. The micro-level influences of [[symbolic interactionism|symbolic interaction]], [[exchange theory|exchange]], and [[rational choice theory|rational choice]], along with the micro-level focus of computational political scientists, such as [[Robert Axelrod]], helped to develop computational sociology's [[:wikt:bottom-up|bottom-up]], [[agent-based]] approach to modeling complex systems. This is what [[Joshua M. Epstein]] calls [[generative science]].&lt;ref name=epstein07 /&gt; Other important areas of influence include [[statistics]], [[mathematical modeling]] and computer [[simulation]].

===Sociocybernetics===
{{Main|Sociocybernetics}}
[[Sociocybernetics]] integrates sociology with [[second-order cybernetics]] and the work of [[Niklas Luhmann]], along with the latest advances in [[complexity science]]. In terms of scholarly work, the focus of sociocybernetics has been primarily conceptual and only slightly methodological or empirical.&lt;ref&gt;[[Geyer, Felix]] and [[Johannes van der Zouwen]] (1992). "Sociocybernetics." ''Handbook of Cybernetics'', C.V. Negoita (ed.): 95–124. New York: Marcel Dekker.&lt;/ref&gt; Sociocybernetics is directly tied to [[Systems thinking|systems thought]] inside and outside of sociology, specifically in the area of second-order cybernetics.

==Areas of application==
As a [[Middle range theory (sociology)|middle-range]] theoretical platform, social complexity can be applied to any research in which [[social interaction]] or the outcomes of such interactions can be observed, but particularly where they can be [[Measurement|measured]] and expressed as [[Continuous function (set theory)|continuous]] or [[Discrete mathematics|discrete]] data points. One common criticism often cited regarding the usefulness of complexity science in sociology is the difficulty of obtaining adequate data.&lt;ref&gt;Stewart, Peter (2001). "Complexity Theories, Social Theory, and the Question of Social Complexity." ''Philosophy of the Social Sciences'', 31(3): 323–360.&lt;/ref&gt; Nonetheless, application of the concept of social complexity and the analysis of such complexity has begun and continues to be an ongoing field of inquiry in sociology. From [[childhood]] friendships and [[teen pregnancy]]&lt;ref name="CCS-MMT" /&gt; to [[criminology]]&lt;ref&gt;Lee, Ju-Sung. (2001). "Evolving Drug Networks." [http://www.casos.cs.cmu.edu/ Carnegie Mellon Center for Computational Analysis of Social and Organizational Systems (CASOS)] Conference Presentation (unpublished).&lt;/ref&gt; and [[counter-terrorism]],&lt;ref&gt;Carley, Kathleen (2003). "Destabilizing Terrorist Networks." ''Proceedings of the 8th International Command and Control Research and Technology Symposium''. Conference held at the National Defense War College: Washington D.C., Evidence Based Research, Track 3. [http://www.dodccrp.org/events/2003/8th_ICCRTS/pdf/021.pdf (Electronic Publication).] {{webarchive|url=https://web.archive.org/web/20041218014917/http://www.dodccrp.org/events/2003/8th_ICCRTS/pdf/021.pdf |date=2004-12-18 }}&lt;/ref&gt; theories of social complexity are being applied in almost all [[Subfields of sociology|areas of sociological research]].

In the area of [[Communications theory|communications research]] and [[informetrics]], the concept of self-organizing systems appears in mid-1990s research related to scientific communications.&lt;ref&gt;[[Loet Leydesdorff|Leydesdorff, Loet]] (1995). ''The Challenge of Scientometrics: The development, measurement, and self-organization of scientific communications''. Leiden: DSWO Press, Leiden University.&lt;/ref&gt; [[Scientometrics]] and [[bibliometrics]] are areas of research in which discrete data are available, as are several other areas of social communications research such as [[sociolinguistics]].&lt;ref name="CCS-MMT" /&gt; Social complexity is also a concept used in [[semiotics]].&lt;ref&gt;Dimitrov, Vladimir and Robert Woog (1997). "Studying Social Complexity: From Soft to Virtual Systems Methodology." [http://www.complex-systems.com/pdf/11-6-5.pdf Complex Systems, 11:(6)].&lt;/ref&gt;

In the first decade of the 21st century, the diversity of areas of application has grown&lt;ref&gt;Saberi, Mohammad Karim, Alireza Isfandyari-Moghaddam and Sedigheh Mohamadesmaeil (2011). "Web Citations Analysis of the JASSS: the First Ten Years." [http://jasss.soc.surrey.ac.uk/14/4/22.html ''Journal of Artificial Societies and Social Simulation'', 14:(4), 22].&lt;/ref&gt; as more sophisticated methods have developed. Social complexity theory is applied in studies of social [[cooperation]] and [[public goods]];&lt;ref&gt;Nowak, Martin and Roger Highfield (2011). ''Super Cooperators: Altruism, Evolution, and Why We Need Each Other to Succeed''. New York, NY: Free Press.&lt;/ref&gt; [[Altruism (ethics)|altruism]];&lt;ref&gt;Hang, Ye, Fei Tan, Mei Ding, Yongmin Jia and Yefeng Chen (2011). "Sympathy and Punishment: Evolution of Cooperation in Public Goods Game." [http://jasss.soc.surrey.ac.uk/14/4/20.html ''Journal of Artificial Societies and Social Simulation'', 14(4): 20].&lt;/ref&gt; [[voting behavior]];&lt;ref&gt; Braha, D., &amp; de Aguiar, M. A. (2016). [https://arxiv.org/abs/1610.04406 Voting Contagion]. arXiv preprint arXiv:1610.04406.&lt;/ref&gt;&lt;ref&gt;Braha, D., &amp; de Aguiar, M. A. (2017). [http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0177970 Voting contagion: Modeling and analysis of a century of U.S. presidential elections]. PLoS ONE 12(5): e0177970.  https://doi.org/10.1371/journal.pone.0177970&lt;/ref&gt; [[education]];&lt;ref&gt;Mason, Mark (2008). ''Complexity Theory and the Philosophy of Education''. Hoboken, NJ: Wiley-Blackwell (Educational Philosophy and Theory Special Issues).&lt;/ref&gt; global [[civil unrest]];&lt;ref&gt;Braha, Dan. (2012). [http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0048596 "Global Civil Unrest: Contagion, Self-Organization, and Prediction."] PLoS ONE 7(10): e48596. doi:10.1371/journal.pone .0048596.&lt;/ref&gt; [[collective action]] and [[social movement]]s;&lt;ref&gt;Lohmann Susanne (1994). "Dynamics of Informational Cascades: The Monday Demonstrations in Leipzig, East Germany, 1989–1991," ''World Politics'', 47: 42–101.&lt;/ref&gt;&lt;ref&gt;Chesters, Graeme and Ian Welsh (2006). ''Complexity and Social Movements: Protest at the Edge of Chaos." London: Routledge (International Library of Sociology).&lt;/ref&gt; [[social inequality]];&lt;ref&gt;Castellani, Brian et al. (2011). "Addressing the U.S. Financial/Housing Crisis: Pareto, Schelling and Social Mobility."[http://cch.ashtabula.kent.edu/publications/Addressing%20the%20U.S.%20Financial%20&amp;%20Housing%20Crisis.pdf Working Paper].&lt;/ref&gt; workforce and [[unemployment]];&lt;ref&gt;Hedström, Peter and Yvonne Åberg (2011). "Social interaction and youth unemployment." ''Analytical Sociology and Social Mechanisms'', Pierre Demeulenaere (ed.). Cambridge: Cambridge University Press.&lt;/ref&gt;&lt;ref&gt;Yilmaz, Levent (2011). "Toward Multi-Level, Multi-Theoretical Model Portfolios for Scientific Enterprise Workforce Dynamics." [http://jasss.soc.surrey.ac.uk/14/4/2.html ''Journal of Artificial Societies and Social Simulation'', 14(4): 2.]&lt;/ref&gt; [[economic geography]] and [[economic sociology]];&lt;ref&gt; Dan Braha, Blake Stacey and Yaneer Bar-Yam. (2011).  [http://necsi.edu/affiliates/braha/Journal_Version_SON_Braha.pdf "Corporate Competition: A Self-Organizing Network."] Social Networks, 33(3): 219-230.&lt;/ref&gt; [[policy analysis]];&lt;ref&gt;Jervis, Robert (1998). ''System Effects: Complexity in Political and Social Life''. Princeton, NJ: Princeton University Press.&lt;/ref&gt;&lt;ref&gt;Elliott, Euel and L. Douglas Kiel (eds.) (2000). ''Nonlinear Dynamics, Complexity and Public Policy''. Hauppauge NY: Nova Science Publishers.&lt;/ref&gt; [[health care systems]];&lt;ref&gt;Brian Castellani, Rajeev Rajaram, J. Galen Buckwalter, Michael Ball and Frederic Hafferty (2012). [https://www.springer.com/public+health/book/978-3-319-09733-6 "Place and Health as Complex Systems: A Case Study and Empirical Test"]. ''SpringerBriefs in Public Health.''&lt;/ref&gt; and [[innovation]] and [[social change]],&lt;ref&gt;Leydesdorff, Loet (2006). ''The Knowledge-Based Economy Modeled, Measured, Simulated''. Boca Raton, FL: Universal-Publishers .&lt;/ref&gt;&lt;ref&gt;Lane, D.; Pumain, D.; Leeuw, S.E. van der; West, G. (eds.) (2009). ''Complexity Perspectives in Innovation and Social Change''. New York, NY: Springer (Methodos Series, Vol. 7).&lt;/ref&gt; to name a few. A current international scientific research project, the [[Seshat (project)|Seshat: Global History Databank]], was explicitly designed to analyze changes in social complexity from the [[Neolithic Revolution]] until the [[Industrial Revolution]].

==See also==
{{col-begin}}
{{col-2}}
=== Social science ===
* [[Complex society]]
* [[Complexity economics]]
* [[Complexity theory and organizations]]
* [[Differentiation (sociology)]]
* [[Econophysics]]
* [[Engaged theory]]
* ''[[Network Analysis and Ethnographic Problems]]''
* [[Personal information management]]
{{col-2}}
{{Portal|Sociology|Systems science}}
=== General ===
* [[Aggregate data]]
* [[Artificial neural network]]
* [[Cognitive complexity]]
* [[Computational complexity theory]]
* [[Dual-phase evolution]]
* [[Evolutionary programming]]
* [[Game theory]]
* [[Generic-case complexity]]
* [[Multi-agent system]]
* [[Systemography]]
{{col-end}}

==References==
{{Reflist|35em}}

== Further reading ==
* Byrne, David (1998). ''Complexity Theory and the Social Sciences.'' London: Routledge.
* Byrne, D., &amp; Callaghan, G. (2013). Complexity theory and the social sciences: The state of the art. Routledge.
* Castellani, Brian and Frederic William Hafferty (2009). ''Sociology and Complexity Science: A New Area of Inquiry'' (Series: Understanding Complex Systems XV). Berlin, Heidelberg: Springer-Verlag.
* Eve, Raymond, Sara Horsfall and Mary E. Lee (1997). ''Chaos, Complexity and Sociology: Myths, Models, and Theories.'' Thousand Oaks, CA: Sage Publications.
* Jenks, Chris and John Smith (2006). ''Qualitative Complexity: Ecology, Cognitive Processes and the Re-Emergence of Structures in Post-Humanist Social Theory.'' New York, NY: Routledge.
* Kiel, L. Douglas (ed.) (2008). [http://www.eolss.net/ebooks/Sample%20Chapters/C15/E1-29.pdf ''Knowledge Management, Organizational Intelligence, Learning and Complexity.'' UNESCO (EOLSS): Paris, France.]
* Kiel, L. Douglas and Euel Elliott (eds.) (1997). ''Chaos Theory in the Social Sciences: Foundations and Applications.'' The University of Michigan Press: Ann Arbor, MI.
* [[Leydesdorff, Loet]] (2001). ''A Sociological Theory of Communication: The Self-Organization of the Knowledge-Based Society''. Parkland, FL: Universal Publishers.
* [[John Urry (sociologist)|Urry, John]] (2005). "The Complexity Turn." ''Theory, Culture and Society'', 22(5): 1–14.

[[Category:Complex systems theory]]
[[Category:Self-organization]]
[[Category:Nonlinear systems]]
[[Category:Sociological theories]]
[[Category:Sociological terminology]]</text>
      <sha1>kjswc6unnl76coznf49498388l580uj</sha1>
    </revision>
  </page>
  <page>
    <title>Statutory reserve</title>
    <ns>0</ns>
    <id>9594871</id>
    <revision>
      <id>788455553</id>
      <parentid>773689958</parentid>
      <timestamp>2017-07-01T13:56:41Z</timestamp>
      <contributor>
        <username>Magic links bot</username>
        <id>30707369</id>
      </contributor>
      <minor/>
      <comment>Replace [[Help:Magic links|magic links]] with templates per [[Special:Permalink/772743896#Future of magic links|local RfC]] and [[:mw:Requests for comment/Future of magic links|MediaWiki RfC]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3827">In the business of [[insurance]], '''statutory reserves''' are those assets an insurance company is legally required to maintain on its [[balance sheet]] with respect to the unmatured obligations (i.e., expected future claims) of the company. Statutory reserves are a type of [[actuarial reserve]].

==Purpose==
Statutory reserves are intended to ensure that insurance companies are able to meet future obligations created by insurance policies. These reserves must be reported in statements filed with insurance regulatory bodies. They are calculated with a certain level of conservatism in order to protect policyholders and beneficiaries.&lt;ref name=Klugman295&gt;{{cite book |editor-first=Stuart A. |editor-last=Klugman |first1=Jeffrey A. |last1=Beckley |first2=Patricia L. |last2=Scahill |first3=Matthew C. |last3=Varitek |first4=Toby A. |last4=White |title=Understanding Actuarial Practice |publisher=[[Society of Actuaries]] |year=2012 |page=295}}&lt;/ref&gt;

==Methods==
There are two types of methods for calculation of statutory reserves. Reserve methodology may be fully prescribed by law, which is often called formula-based reserving. This is in contrast to principles-based reserves, where [[actuaries]] are given latitude to use professional judgment in determining methodology and assumptions for reserve calculation.&lt;ref name="Klugman295"/&gt; In the United States, where formula-based reserves are used, the [[National Association of Insurance Commissioners]] plans to implement principles-based reserves in 2017.&lt;ref&gt;{{cite journal |title=Principle-Based Reserving (PBR) Implementation Plan |date=23 March 2015 |url=http://www.naic.org/documents/committees_ex_pbr_implementation_tf_150323_pbr_implementation_plan.pdf |publisher=[[National Association of Insurance Commissioners]]}}&lt;/ref&gt;

==Life insurance in the United States==
In the U.S. [[life insurance]] industry, statutory reserves are most commonly computed using the [[Commissioner's Reserve Valuation Method]], or CRVM, the method prescribed by law for computing minimum required reserves.

The size of a CRVM reserve, as with most life reserves, is affected by the age and sex of the insured person, how long the policy for which it is computed has been in force, the plan of insurance offered by the policy, the rate of interest used in the calculation, and the [[mortality table]] with which the [[actuarial present value]]s are computed.

The Commissioner's Reserve Valuation Method was itself established by the Standard Valuation Law (SVL), which was created by the [[National Association of Insurance Commissioners|NAIC]] and adopted by the several states shortly after World War II. The first mortality table prescribed by the SVL was the 1941 CSO (Commissioner's Standard Ordinary) table,&lt;ref&gt;The 1941 CSO table was prescribed for policies of ''ordinary'' life insurance. For policies of ''industrial'' life insurance the SVL prescribed use of the 1941 CSI (Commissioner's Standard Industrial) table.&lt;/ref&gt; at a maximum interest rate of 3½%. Subsequent amendments to the Standard Valuation Law have permitted the use of more modern mortality tables and higher rates of interest. The effect of these changes has in general been to reduce the amount of the reserves which life insurance companies are legally required to hold.

==See also==
*[[Actuarial reserves]]
*[[Elizur Wright]]
*[[Statutory accounting principles]]

==Notes==
&lt;references/&gt;

==References==
*N. L. Bowers, H. U. Gerber, J. C. Hickman, D. A. Jones, &amp; C. J. Nesbitt, ''Actuarial Mathematics'', Society of Actuaries (1986). {{ISBN|0-938959-10-7}}

==External links==
*[http://www.naic.org/ NAIC Official Website]
*[http://www.naic.org/committees_e_app_sapwg.htm Statutory Accounting Principles Working Group]

[[Category:Actuarial science]]
[[Category:Accounting in the United States]]</text>
      <sha1>k06siioenvautllwvmrkoobhyeddjte</sha1>
    </revision>
  </page>
  <page>
    <title>Suslin's problem</title>
    <ns>0</ns>
    <id>228668</id>
    <revision>
      <id>843884137</id>
      <parentid>841493879</parentid>
      <timestamp>2018-06-01T03:37:36Z</timestamp>
      <contributor>
        <ip>24.65.134.91</ip>
      </contributor>
      <comment>Correcting grammar of translation from French</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5841">In [[mathematics]], '''Suslin's problem''' is a question about [[totally ordered set]]s posed by {{harvs|txt|authorlink=Mikhail Yakovlevich Suslin|first=Mikhail Yakovlevich |last=Suslin|year=1920}} and published posthumously.
It has been shown to be [[independence (mathematical logic)|independent]] of the standard [[axiomatic system]] of [[set theory]] known as [[ZFC]]: {{harvtxt|Solovay|Tennenbaum|1971}} showed that the statement can neither be proven nor disproven from those axioms, assuming ZF is consistent.

(Suslin is also sometimes written with the French transliteration as Souslin, from the Cyrillic Суслин.)
{{quotebox|right|width=50%
|quote=Un ensemble ordonné (linéairement) sans sauts ni lacunes et tel que tout ensemble de ses intervalles (contenant plus qu'un élément) n'empiétant pas les uns sur les autres est au plus dénumerable, est-il nécessairement un continue linéaire (ordinaire)?&lt;br&gt;Is a (linearly) ordered set without jumps or gaps and such that every set of its intervals (containing more than one element) not overlapping each other is at most denumerable, necessarily an (ordinary) linear continuum?
|source=The original statement of Suslin's problem from {{harv|Suslin|1920}}
}}

==Formulation==
Given a [[non-empty]] [[totally ordered set]] ''R'' with the following four properties:
# ''R'' does not have a [[greatest element|least nor a greatest element]];
# the order on ''R'' is [[dense order|dense]] (between any two elements there is another);
# the order on ''R'' is [[completeness (order theory)|complete]], in the sense that every non-empty bounded subset has a [[supremum]] and an [[infimum]];
# every collection of mutually [[disjoint sets|disjoint]] non-empty open [[interval (mathematics)|interval]]s in ''R'' is [[countable]] (this is the [[countable chain condition]] for the [[order topology]] of ''R'').
Is ''R'' necessarily [[order isomorphism|order-isomorphic]] to the [[real line]] '''R'''?

If the requirement for the countable chain condition is replaced with the requirement that ''R'' contains a countable dense subset (i.e., ''R'' is a [[separable space]]) then the answer is indeed yes: any such set ''R'' is necessarily order-isomorphic to '''R''' (proved by [[Georg Cantor|Cantor]]).

The condition for a topological space that every collection of non-empty disjoint open sets is at most countable is called the '''Suslin property'''.

==Implications==
Any totally ordered set that is ''not'' isomorphic to '''R''' but satisfies (1)&amp;nbsp;–&amp;nbsp;(4) is known as a '''Suslin line'''. The '''Suslin hypothesis''' says that there are no Suslin lines: that every countable-chain-condition dense complete linear order without endpoints is isomorphic to the real line. An equivalent statement is that every [[tree (set theory)|tree]] of height &amp;omega;&lt;sub&gt;1&lt;/sub&gt; either has a branch of length &amp;omega;&lt;sub&gt;1&lt;/sub&gt; or an [[antichain]] of cardinality &lt;math&gt;\aleph_1&lt;/math&gt;.

The '''generalized Suslin hypothesis''' says that for every infinite [[regular cardinal]] &amp;kappa; every tree of height &amp;kappa; either has a branch of length &amp;kappa; or an antichain of cardinality &amp;kappa;. The existence of Suslin lines is equivalent to the existence of [[Suslin tree]]s and to [[Suslin algebra]]s.

The Suslin hypothesis is independent of ZFC.
{{harvtxt|Jech|1967}} and {{harvtxt|Tennenbaum|1968}} independently used [[Forcing (mathematics)|forcing methods]] to construct models of ZFC in which Suslin lines exist. [[Ronald Jensen|Jensen]] later proved that Suslin lines exist if the [[diamond principle]], a consequence of the [[Axiom of constructibility]] V=L, is assumed. (Jensen's result was a surprise as it had previously been conjectured that V=L implies that no Suslin lines exist, on the grounds that V=L implies there are "few" sets.) On the other hand, {{harvtxt|Solovay|Tennenbaum|1971}} used forcing to construct a model of ZFC in which there are no Suslin lines; more precisely they showed that [[Martin's axiom]] plus the negation of the Continuum Hypothesis implies the Suslin Hypothesis.

The Suslin hypothesis is also independent of both the [[generalized continuum hypothesis]] (proved by [[Ronald Jensen]]) and of the negation of the [[continuum hypothesis]].    It is not known whether the Generalized Suslin Hypothesis is consistent with the Generalized Continuum Hypothesis; however, since the combination implies the negation of the [[square principle]] at a singular strong [[limit cardinal]]—in fact, at all [[singular cardinal]]s and all regular [[successor cardinal]]s—it implies that the [[axiom of determinacy]] holds in L(R) and is believed to imply the existence of an [[inner model]] with a [[superstrong cardinal]].

==See also==
* [[List of statements independent of ZFC]]
* [[AD+]]

==References==
*K. Devlin and H. Johnsbråten, The Souslin Problem, Lecture Notes in Mathematics (405) Springer 1974
*{{citation|mr=0215729
|last=Jech|first= Tomáš
|title=Non-provability of Souslin's hypothesis
|journal=Comment. Math. Univ. Carolinae |volume=8 |year=1967 |pages=291–305}}
*{{citation
|title=Problème 3
|last= Souslin
|first=M.
|journal=Fundamenta Mathematicae
|volume=1
|year=1920
|page=223
|url=http://matwbn.icm.edu.pl/ksiazki/fm/fm1/fm1125.pdf
}}
*{{citation|last1=Solovay|first1=R.M.|last2=S.|first2=Tennenbaum|title=Iterated Cohen Extensions and Souslin's Problem|journal=Annals of Mathematics|date=1971|volume=94|issue=2|pages=201–245|doi=10.2307/1970860|jstor=1970860}}
*{{citation|mr=0224456
|last=Tennenbaum|first= S.
|title=Souslin's problem.
|journal=Proc. Natl. Acad. Sci. U.S.A.|volume= 59 |year=1968|pages= 60–63|doi=10.1073/pnas.59.1.60|pmc=286001|pmid=16591594}}
*{{springer|id=S/s091460
|first=V.N. |last=Grishin|title=Suslin hypothesis}}

{{Set theory}}

[[Category:Independence results]]
[[Category:Order theory]]</text>
      <sha1>1ek8f0q2q74r0401nvi4cjeaam6hp4n</sha1>
    </revision>
  </page>
  <page>
    <title>Szemerédi's theorem</title>
    <ns>0</ns>
    <id>591703</id>
    <revision>
      <id>858870074</id>
      <parentid>858405806</parentid>
      <timestamp>2018-09-10T04:58:39Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <minor/>
      <comment>[[Julia Wolf]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="18840">In [[arithmetic combinatorics]], '''Szemerédi's theorem''' is a result concerning [[arithmetic progression]]s in subsets of the integers. In 1936, [[Paul Erdős|Erdős]] and [[Pál Turán|Turán]] conjectured&lt;ref name="erdos turan"&gt;{{cite journal|authorlink1=Paul Erdős|first1=Paul|last1=Erdős|authorlink2=Pál Turán|first2=Paul|last2=Turán|title=On some sequences of integers|journal=[[Journal of the London Mathematical Society]]|volume=11|issue=4|year=1936|pages=261–264|url=http://www.renyi.hu/~p_erdos/1936-05.pdf|doi=10.1112/jlms/s1-11.4.261|mr=1574918}}&lt;/ref&gt; that every set of integers ''A'' with positive [[natural density]] contains a ''k''-term [[arithmetic progression]] for every ''k''. [[Endre Szemerédi]] proved the conjecture in 1975.

==Statement==
A subset ''A'' of the [[natural numbers]] is said to have positive upper density if

:&lt;math&gt;\limsup_{n \to \infty}\frac{|A\cap \{1, 2, 3, \dotsc, n\}|}{n} &gt; 0&lt;/math&gt;.

Szemerédi's theorem asserts that a subset of the natural numbers with positive upper density contains infinitely many arithmetic progressions of length ''k'' for all positive integers ''k''.

An often-used equivalent finitary version of the theorem states that for every positive integer ''k'' and real number &lt;math&gt;\delta \in (0, 1]&lt;/math&gt;, there exists a positive integer

:&lt;math&gt;N = N(k,\delta)&lt;/math&gt;

such that every subset of  {1, 2, ..., ''N''} of size at least δ''N'' contains an arithmetic progression of length ''k''.

Another formulation uses the function ''r''&lt;sub&gt;''k''&lt;/sub&gt;(''N''), the size of the largest subset of {1, 2, ..., ''N''} without an arithmetic progression of length ''k''. Szemerédi's theorem is equivalent to the asymptotic bound

:&lt;math&gt;r_k(N) = o(N)&lt;/math&gt;.

That is, ''r''&lt;sub&gt;''k''&lt;/sub&gt;(''N'') grows less than linearly with ''N''.

==History==
[[Van der Waerden's theorem]], a precursor of Szemerédi's theorem, was proven in 1927.

The cases ''k''&amp;nbsp;=&amp;nbsp;1 and ''k''&amp;nbsp;=&amp;nbsp;2  of Szemerédi's theorem are trivial.  The case ''k'' = 3 was established in 1953 by [[Klaus Roth]]&lt;ref&gt;{{cite journal|authorlink=Klaus Friedrich Roth|first=Klaus Friedrich|last=Roth|title=On certain sets of integers|journal=[[Journal of the London Mathematical Society]]|volume=28|pages=104–109|year=1953|zbl=0050.04002|mr=0051853|doi=10.1112/jlms/s1-28.1.104|issue=1}}&lt;/ref&gt; via an adaptation of the [[Hardy–Littlewood circle method]]. [[Endre Szemerédi]]&lt;ref&gt;{{cite journal|authorlink=Endre Szemerédi|first=Endre|last=Szemerédi|title=On sets of integers containing no four elements in arithmetic progression|journal=Acta Math. Acad. Sci. Hung.|volume=20|pages=89–104|year=1969|zbl=0175.04301|mr=0245555|doi=10.1007/BF01894569}}&lt;/ref&gt; proved the case ''k'' = 4 through combinatorics. Using an approach similar to the one he used for the case ''k'' = 3, Roth&lt;ref&gt;{{cite journal|authorlink=Klaus Friedrich Roth|first=Klaus Friedrich|last=Roth|title=Irregularities of sequences relative to arithmetic progressions, IV|journal=Periodica Math. Hungar.|volume=2|pages=301–326|year=1972|mr=0369311|doi=10.1007/BF02018670}}&lt;/ref&gt; gave a second proof for this in 1972.

The general case was settled in 1975, also by Szemerédi,&lt;ref&gt;{{cite journal|authorlink=Endre Szemerédi|first=Endre|last=Szemerédi|title=On sets of integers containing no ''k'' elements in arithmetic progression|journal=[[Acta Arithmetica]]|volume=27|pages=199–245|year=1975|url=http://matwbn.icm.edu.pl/ksiazki/aa/aa27/aa27132.pdf|zbl=0303.10056|mr=0369312|doi=10.4064/aa-27-1-199-245}}&lt;/ref&gt; who developed an ingenious and complicated extension of his previous combinatorial argument for ''k'' = 4 (called "a masterpiece of combinatorial reasoning" by [[Paul Erdos|Erdős]]&lt;ref&gt;{{cite encyclopedia|last=Erdős|first=Paul|chapter=Some of My Favorite Problems and Results|pages=51–70|year=2013|encyclopedia=The Mathematics of Paul Erdős I|doi=10.1007/978-1-4614-7258-2_3|mr=1425174|editor1-last=Graham|editor1-first=Ronald L.|editor2-last=Nešetřil|editor2-first=Jaroslav|editor3-last=Butler|editor3-first=Steve|edition=Second|isbn=978-1-4614-7257-5|publisher=Springer|location=New York}}&lt;/ref&gt;).  Several other proofs are now known, the most important being those by [[Hillel Furstenberg]]&lt;ref&gt;{{cite journal|authorlink=Hillel Furstenberg|first=Hillel|last=Furstenberg|title=Ergodic behavior of diagonal measures and a theorem of Szemerédi on arithmetic progressions|journal=J. D'Analyse Math.|volume=31|pages=204–256|year=1977|mr=0498471|doi=10.1007/BF02813304}}.&lt;/ref&gt;&lt;ref&gt;{{cite journal | last1=Furstenberg | first1=Hillel | last2=Katznelson | first2=Yitzhak | last3=Ornstein | first3=Donald Samuel | title=The ergodic theoretical proof of Szemerédi’s theorem | journal=[[Bull. Amer. Math. Soc.]] | year=1982 | volume=7 | issue=3 | pages=527–552 | mr=0670131 | doi=10.1090/S0273-0979-1982-15052-2}}&lt;/ref&gt; in 1977, using [[ergodic theory]], and by [[Timothy Gowers]]&lt;ref name="gowers"&gt;{{cite journal|authorlink=Timothy Gowers|first=Timothy|last=Gowers|title=A new proof of Szemerédi's theorem|journal=[[Geom. Funct. Anal.]]|volume=11|issue=3|pages=465–588|url=http://www.dpmms.cam.ac.uk/~wtg10/sz898.dvi|year=2001|mr=1844079|doi=10.1007/s00039-001-0332-9}}&lt;/ref&gt; in 2001, using both [[Fourier analysis]] and [[combinatorics]]. [[Terence Tao]] has called the various proofs of Szemerédi's theorem a "[[Rosetta stone]]" for connecting disparate fields of mathematics.&lt;ref&gt;{{cite encyclopedia | last=Tao | first=Terence | chapter=The dichotomy between structure and randomness, arithmetic progressions, and the primes | arxiv=math/0512114 | encyclopedia=International Congress of Mathematicians | volume=1 | publisher=[[European Mathematical Society]] | location=Zürich | year=2007 | pages=581–608 | mr=2334204 | doi=10.4171/022-1/22 | editor1-first=Marta | editor1-last=Sanz-Solé | editor2-first=Javier | editor2-last=Soria | editor3-first=Juan Luis | editor3-last=Varona | editor4-first=Joan | editor4-last=Verdera }}&lt;/ref&gt;

==Quantitative bounds==
It is an open problem to determine the exact growth rate of ''r''&lt;sub&gt;''k''&lt;/sub&gt;(''N''). The best known general bounds are

:&lt;math&gt;CN\exp\left(-n2^{(n - 1)/2}\sqrt[n]{\log N} + \frac{1}{2n}\log \log N\right) \leq r_k(N) \leq \frac{N}{(\log \log N)^{2^{-2^{k + 9}}}},&lt;/math&gt;

where &lt;math&gt;n = \lceil \log k\rceil&lt;/math&gt;. The lower bound is due to O'Bryant&lt;ref name="obryant"&gt;{{cite journal | last=O'Bryant | first=Kevin | title=Sets of integers that do not contain long arithmetic progressions | journal=Electronic Journal of Combinatorics | year=2011 | volume=18 | issue=1 | url=http://www.combinatorics.org/ojs/index.php/eljc/article/download/v18i1p59/pdf | mr=2788676}}&lt;/ref&gt; building on the work of [[Felix A. Behrend|Behrend]],&lt;ref&gt;{{cite journal|authorlink=Felix A. Behrend|first=Felix A.|last=Behrend|title=On the sets of integers which contain no three terms in arithmetic progression|journal=[[Proceedings of the National Academy of Sciences]]|volume=23|issue=12|pages=331–332|year=1946|zbl=0060.10302|doi=10.1073/pnas.32.12.331|mr=0018694|pmc=1078539|pmid=16588588|bibcode=1946PNAS...32..331B}}&lt;/ref&gt; [[Robert Alexander Rankin|Rankin]],&lt;ref&gt;{{cite journal|authorlink=Robert A. Rankin|first=Robert A.|last=Rankin|title=Sets of integers containing not more than a given number of terms in arithmetical progression|journal=Proc. Roy. Soc. Edinburgh Sect. A|volume=65|pages=332–344|year=1962|zbl=0104.03705|mr=0142526}}&lt;/ref&gt; and Elkin.&lt;ref&gt;{{cite journal | last=Elkin | first=Michael | title=An improved construction of progression-free sets | journal=Israel Journal of Mathematics | year=2011 | volume=184 | pages=93–128 | doi=10.1007/s11856-011-0061-1 | mr=2823971 | issue=1| arxiv=0801.4310 }}&lt;/ref&gt;&lt;ref&gt;{{cite encyclopedia | last1=Green | first1=Ben | last2=Wolf | first2=Julia|author2-link= Julia Wolf | chapter=A note on Elkin's improvement of Behrend's construction | publisher=Springer | location=New York | mr=2744752 | year=2010 | pages=141–144 | encyclopedia=Additive number theory. Festschrift in honor of the sixtieth birthday of Melvyn B. Nathanson | editor1-last=Chudnovsky | editor1-first=David | editor2-last=Chudnovsky | editor2-first=Gregory | isbn=978-0-387-37029-3 | doi=10.1007/978-0-387-68361-4_9| arxiv=0810.0732 }}&lt;/ref&gt; The upper bound is due to Gowers.&lt;ref name="gowers"/&gt;

For small ''k'', there are tighter bounds than the general case. When ''k'' = 3, [[Jean Bourgain|Bourgain]],&lt;ref&gt;{{cite journal|authorlink=Jean Bourgain|first=Jean|last=Bourgain|title=On triples in arithmetic progression|journal=[[Geom. Funct. Anal.]]|volume=9|issue=5|year=1999|pages=968–984|mr=1726234|doi=10.1007/s000390050105}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|authorlink=Jean Bourgain|first=Jean|last=Bourgain|title=Roth's theorem on progressions revisited|volume=104|year=2008|pages=155–192|journal=J. Anal. Math.|mr=2403433|doi=10.1007/s11854-008-0020-x|issue=1}}&lt;/ref&gt; Heath-Brown,&lt;ref&gt;{{cite journal|authorlink=D. R. Heath-Brown|first=Roger|last=Heath-Brown|title=Integer sets containing no arithmetic progressions|volume=35|journal=[[Journal of the London Mathematical Society]]|issue=3|year=1987|mr=889362|pages=385–394|doi=10.1112/jlms/s2-35.3.385}}&lt;/ref&gt; Szemerédi,&lt;ref&gt;{{cite journal|first=Endre|last=Szemerédi|title=Integer sets containing no arithmetic progressions|journal=Acta Math. Hungar.|year=1990|volume=56|issue=1–2|doi=10.1007/BF01903717|mr=1100788|pages=155–158}}&lt;/ref&gt; and [[Tom Sanders (mathematician)|Sanders]]&lt;ref&gt;{{cite journal|authorlink=Tom Sanders (mathematician)|first=Tom|last=Sanders|title=On Roth's theorem on progressions|journal=[[Annals of Mathematics]]|volume=174|issue=1|year=2011|pages=619–636|mr=2811612|doi=10.4007/annals.2011.174.1.20|arxiv=1011.0104}}&lt;/ref&gt; provided increasingly smaller upper bounds. The current best bounds are

:&lt;math&gt;N2^{-\sqrt{8\log N}} \leq r_3(N) \leq C\frac{(\log \log N)^4}{\log N}N&lt;/math&gt;

due to O'Bryant&lt;ref name="obryant"/&gt; and Bloom&lt;ref&gt;{{cite journal | last=Bloom | first=Thomas F. | title=A quantitative improvement for Roth's theorem on arithmetic progressions | arxiv=1405.5800 | year=2016 | issue=3 | volume=93 | pages=643–663 | journal=Journal of the London Mathematical Society |series=Second Series | mr=3509957 | doi=10.1112/jlms/jdw010}}&lt;/ref&gt; respectively.

For ''k'' = 4, Green and Tao&lt;ref&gt;{{cite encyclopedia|last1=Green|first1=Ben|last2=Tao|first2=Terence|chapter=New bounds for Szemeredi's theorem. II. A new bound for ''r''&lt;sub&gt;4&lt;/sub&gt;(''N'')|arxiv=math/0610604|year=2009|pages=180–204|publisher=[[Cambridge University Press]]|location=Cambridge|mr=2508645|encyclopedia=Analytic number theory. Essays in honour of Klaus Roth on the occasion of his 80th birthday|isbn=978-0-521-51538-2|editor1-last=Chen|editor1-first=William W. L.|editor2-last=Gowers|editor2-first=Timothy|editor2-link=Timothy Gowers|editor3-last=Halberstam|editor3-first=Heini|editor3-link=Heini Halberstam|editor4-last=Schmidt|editor4-link=Wolfgang M. Schmidt|editor4-first=Wolfgang|editor5-last=Vaughan|editor5-first=Robert Charles|editor5-link=Bob Vaughan|zbl=1158.11007|bibcode=2006math.....10604G}}&lt;/ref&gt;&lt;ref&gt;{{cite journal| title=New bounds for Szemerédi's theorem, III: A polylogarithmic bound for r&lt;sub&gt;4&lt;/sub&gt;(N) | year=2017 | last1=Green | first1=Ben | last2=Tao|first2=Terence|arxiv=1705.01703|mr=3731312|journal=Mathematika|volume=63|issue=3|pages=944–1040|doi=10.1112/S0025579317000316}}&lt;/ref&gt; proved that

:&lt;math&gt;r_4(N) \leq C\frac{N}{(\log N)^c}&lt;/math&gt;

for some ''c'' &gt; 0.

==Extensions and generalizations==

A multidimensional generalization of Szemerédi's theorem was first proven by [[Hillel Furstenberg]] and [[Yitzhak Katznelson]] using ergodic theory.&lt;ref&gt;{{cite journal | last1=Furstenberg | first1=Hillel | author1link=Hillel Furstenberg | last2=Katznelson | first2=Yitzhak | author2link=Yitzhak Katznelson | title=An ergodic Szemerédi theorem for commuting transformations | journal=Journal d'Analyse Mathématique | doi=10.1007/BF02790016 | mr=531279 | volume=38 | year=1978 | pages=275–291 | issue=1}}&lt;/ref&gt; [[Timothy Gowers]],&lt;ref&gt;{{cite journal | last=Gowers | first=Timothy | authorlink=Timothy Gowers | title=Hypergraph regularity and the multidimensional Szemerédi theorem | journal=[[Annals of Mathematics]] | volume=166 | year=2007 | issue=3 | pages=897–946 | doi=10.4007/annals.2007.166.897 | mr=2373376| arxiv=0710.3032 }}&lt;/ref&gt;  Vojtěch Rödl and Jozef Skokan&lt;ref&gt;{{cite journal | last1=Rödl | first1=Vojtěch | author1link=Vojtěch Rödl | last2=Skokan | first2=Jozef | title=Regularity lemma for k-uniform hypergraphs | journal= Random Structures Algorithms | volume=25 | year=2004 | issue=1 | pages=1–42 | mr=2069663 | doi=10.1002/rsa.20017}}&lt;/ref&gt;&lt;ref&gt;{{cite journal | last1=Rödl | first1=Vojtěch | author1link=Vojtěch Rödl | last2=Skokan | first2=Jozef | title=Applications of the regularity lemma for uniform hypergraphs | journal=Random Structures Algorithms | volume=28 | year=2006 | issue=2 | pages=180–194 | mr=2198496 | doi=10.1002/rsa.20108}}&lt;/ref&gt; with Brendan Nagle, Rödl, and [[Mathias Schacht]],&lt;ref&gt;{{cite journal | last1=Nagle | first1=Brendan | last2=Rödl | first2=Vojtěch | author2link=Vojtěch Rödl | last3=Schacht | first3=Mathias|author3-link=Mathias Schacht | title=The counting lemma for regular k-uniform hypergraphs | journal=Random Structures Algorithms | volume=28 | year=2006 | issue=2 | pages=113–179 | doi=10.1002/rsa.20117 | mr=2198495}}&lt;/ref&gt; and [[Terence Tao]]&lt;ref&gt;{{cite journal | last=Tao | first=Terence | title=A variant of the hypergraph removal lemma | journal=Journal of Combinatorial Theory, Series A | volume=113 | year=2006 | issue=7 | pages=1257–1280 | doi=10.1016/j.jcta.2005.11.006 | mr=2259060}}&lt;/ref&gt; provided combinatorial proofs.

Alexander Leibman and [[Vitaly Bergelson]]&lt;ref&gt;{{cite journal | first1=Vitaly | last1=Bergelson | author1link=Vitaly Bergelson | first2=Alexander | last2=Leibman | title=Polynomial extensions of van der Waerden's and Szemerédi's theorems | journal=[[Journal of the American Mathematical Society]] | volume=9 | issue=3 | year=1996 | pages=725–753 | mr=1325795 | doi=10.1090/S0894-0347-96-00194-4}}&lt;/ref&gt; generalized Szemerédi's to polynomial progressions: If &lt;math&gt;A \subset \mathbb{N}&lt;/math&gt; is a set with positive upper density and &lt;math&gt;p_1(n),p_2(n),\dotsc,p_k(n)&lt;/math&gt; are [[integer-valued polynomial]]s such that &lt;math&gt;p_i(0) = 0&lt;/math&gt;, then there are infinitely many &lt;math&gt;u, n \in \mathbb{Z}&lt;/math&gt; such that &lt;math&gt;u + p_i(n) \in A&lt;/math&gt; for all &lt;math&gt;1 \leq i \leq k&lt;/math&gt;. Leibman and Bergelson's result also holds in a multidimensional setting.

The finitary version of Szemerédi's theorem can be generalized to finite [[additive group]]s including vector spaces over [[finite field]]s.&lt;ref&gt;{{cite journal | last1=Furstenberg | first1=Hillel | last2=Katznelson | author1link=Hillel Furstenberg | first2=Yitzhak | author2link=Yitzhak Katznelson | title=A density version of the Hales–Jewett theorem | journal=Journal d'Analyse Mathématique | volume=57 | year=1991 | pages=64–119 | mr=1191743 | issue=1 | doi=10.1007/BF03041066}}&lt;/ref&gt; The finite field analog can be used as a model for understanding the theorem in the natural numbers.&lt;ref&gt;{{cite journal | first=Julia | last=Wolf | title=Finite field models in arithmetic combinatorics—ten years on | year=2015 | volume=32 | pages=233–274 | journal=Finite Fields and Their Applications | doi=10.1016/j.ffa.2014.11.003 | mr=3293412}}&lt;/ref&gt; The problem of obtaining bounds in the k=3 case of Szemerédi's theorem in vector spaces over &lt;math&gt;\mathbb{F}_3^n&lt;/math&gt; is known as the [[cap set]] problem.

The [[Green–Tao theorem]] asserts the prime numbers contain arbitrary long arithmetic progressions. It is not implied by Szemerédi's theorem because the primes have density 0 in the natural numbers. As part of their proof, [[Ben Green (mathematician)|Ben Green]] and Tao introduced a "relative" Szemerédi theorem which applies to subsets of the integers (even those with 0 density) satisfying certain pseudorandomness conditions. A more general relative Szemerédi theorem has since been given by [[David Conlon]], [[Jacob Fox]], and Yufei Zhao.&lt;ref&gt;{{cite journal | first1=David | last1=Conlon | author1link=David Conlon | first2=Jacob | last2=Fox | author2link=Jacob Fox | first3=Yufei | last3=Zhao | title=A relative Szemerédi theorem | arxiv=1305.5440 | year=2015 | mr=3361771 | volume=25 | issue=3 | pages=733–762 | doi=10.1007/s00039-015-0324-9 | journal=[[Geometric and Functional Analysis]]}}&lt;/ref&gt;&lt;ref&gt;{{cite journal | first=Yufei | last=Zhao | title=An arithmetic transference proof of a relative Szemerédi theorem | journal=[[Mathematical Proceedings of the Cambridge Philosophical Society]] | volume=156 | year=2014 | pages=255–261 | issue=2 | mr=3177868 | doi=10.1017/S0305004113000662| arxiv=1307.4959 | bibcode=2014MPCPS.156..255Z }}&lt;/ref&gt;

The [[Erdős conjecture on arithmetic progressions]] would imply both Szemerédi's theorem and the Green–Tao theorem.

==See also==
* [[Problems involving arithmetic progressions]]
* [[Ergodic Ramsey theory]]
* [[Arithmetic combinatorics]]
* [[Szemerédi regularity lemma]]

==Notes==
{{reflist|colwidth=30em}}

==Further reading==
* {{cite encyclopedia | first=Terence | last=Tao | authorlink=Terence Tao | chapter=The ergodic and combinatorial approaches to Szemerédi's theorem | pages=145–193 | editor1-first=Andrew | editor1-last=Granville | editor2-first=Melvyn B. | editor2-last=Nathanson| editor3-first=József | editor3-last=Solymosi | encyclopedia=Additive Combinatorics | series=CRM Proceedings &amp; Lecture Notes | volume=43 | publisher=[[American Mathematical Society]] | year=2007 | isbn=978-0-8218-4351-2 | zbl=1159.11005 | mr=2359471 | arxiv=math/0604456 | location=Providence, RI| bibcode=2006math......4456T }}

==External links==
* [http://planetmath.org/encyclopedia/SzemeredisTheorem.html PlanetMath source for initial version of this page]
* [http://www.math.ucla.edu/~tao/whatsnew.html Announcement by Ben Green and Terence Tao] – the preprint is available at [http://front.math.ucdavis.edu/math.NT/0404188 math.NT/0404188]
* [http://in-theory.blogspot.com/2006/06/szemeredis-theorem.html Discussion of Szemerédi's theorem (part 1 of 5)]
* Ben Green and Terence Tao: [http://www.scholarpedia.org/article/Szemeredi%27s_Theorem Szemerédi's theorem] on [[Scholarpedia]]
* {{MathWorld|SzemeredisTheorem|SzemeredisTheorem}}
* {{cite web|last=Grime|first=James|title=6,000,000: Endre Szemerédi wins the Abel Prize|url=http://www.numberphile.com/videos/abel_prize.html|work=Numberphile|year=2012|publisher=[[Brady Haran]]|author2=Hodge, David }}

{{DEFAULTSORT:Szemeredi's theorem}}
[[Category:Additive combinatorics]]
[[Category:Ramsey theory]]
[[Category:Theorems in combinatorics]]
[[Category:Theorems in number theory]]</text>
      <sha1>1tv0srb8q6jmyyg1k5exij64mi4vhky</sha1>
    </revision>
  </page>
  <page>
    <title>Triple system</title>
    <ns>0</ns>
    <id>19287276</id>
    <revision>
      <id>849546482</id>
      <parentid>838051612</parentid>
      <timestamp>2018-07-09T18:57:48Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v2.0beta)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9838">{{for|an account of that concept in '''combinatorics''' |Steiner triple system |block design}}

In [[algebra]], a '''triple system''' (or '''ternar''') is a [[vector space]] ''V'' over a field '''F''' together with a [[multilinear map|'''F'''-trilinear map]]
:&lt;math&gt; (\cdot,\cdot,\cdot) \colon V\times V \times V\to V.&lt;/math&gt;
The most important examples are '''Lie triple systems''' and '''Jordan triple systems'''. They were introduced by [[Nathan Jacobson]] in 1949 to study subspaces of [[associative algebra]]s closed under triple commutators [[''u'', ''v''], ''w''] and triple [[Commutator|anticommutator]]s {''u'', {''v'', ''w''}}. In particular, any [[Lie algebra]] defines a Lie triple system and any [[Jordan algebra]] defines a Jordan triple system. They are important in the theories of [[symmetric space]]s, particularly [[Hermitian symmetric space]]s and their generalizations ([[symmetric R-space]]s and their noncompact duals).

==Lie triple systems==

A triple system is said to be a Lie triple system if the trilinear form, denoted [.,.,.], satisfies the following identities:
:&lt;math&gt; [u,v,w] = -[v,u,w] &lt;/math&gt;
:&lt;math&gt; [u,v,w] + [w,u,v] + [v,w,u] = 0&lt;/math&gt;
:&lt;math&gt; [u,v,[w,x,y]] = [[u,v,w],x,y] + [w,[u,v,x],y] + [w,x,[u,v,y]].&lt;/math&gt;
The first two identities abstract the [[skew symmetry]] and [[Jacobi identity]] for the triple commutator, while the third identity means that the linear map L&lt;sub&gt;''u'',''v''&lt;/sub&gt;:''V''→''V'', defined by L&lt;sub&gt;''u'',''v''&lt;/sub&gt;(''w'') = [''u'', ''v'', ''w''], is a [[derivation (algebra)|derivation]] of the triple product. The identity also shows that the space '''k''' = span {L&lt;sub&gt;''u'',''v''&lt;/sub&gt;: ''u'', ''v'' ∈ ''V''} is closed under commutator bracket, hence a Lie algebra.

Writing '''m''' in place of ''V'', it follows that
:&lt;math&gt;\mathfrak g := k \oplus\mathfrak m&lt;/math&gt;
can be made into a &lt;math&gt;\mathbb{Z}_2&lt;/math&gt;-graded Lie algebra, the ''standard embedding'' of '''m''', with bracket
:&lt;math&gt;[(L,u),(M,v)] = ([L,M]+L_{u,v}, L(v) - M(u)).&lt;/math&gt;
The decomposition of '''g''' is clearly a [[symmetric space|symmetric decomposition]] for this Lie bracket, and hence if ''G'' is a connected Lie group with Lie algebra '''g''' and ''K'' is a subgroup with Lie algebra '''k''', then ''G''/''K'' is a [[symmetric space]].

Conversely, given a Lie algebra '''g''' with such a symmetric decomposition (i.e., it is the Lie algebra of a symmetric space), the triple bracket [[''u'', ''v''], ''w''] makes '''m''' into a Lie triple system.

==Jordan triple systems==

A triple system is said to be a Jordan triple system if the trilinear form, denoted {.,.,.}, satisfies the following identities:
:&lt;math&gt; \{u,v,w\} = \{u,w,v\} &lt;/math&gt;
:&lt;math&gt; \{u,v,\{w,x,y\}\} = \{w,x,\{u,v,y\}\} + \{w, \{u,v,x\},y\} -\{\{v,u,w\},x,y\}. &lt;/math&gt;
The first identity abstracts the symmetry of the triple anticommutator, while the second identity means that if L&lt;sub&gt;''u'',''v''&lt;/sub&gt;:''V''→''V'' is defined by  L&lt;sub&gt;''u'',''v''&lt;/sub&gt;(''y'') = {''u'', ''v'', ''y''} then
:&lt;math&gt; [L_{u,v},L_{w,x}]:= L_{u,v}\circ L_{w,x} - L_{w,x} \circ L_{u,v} = L_{w,\{u,v,x\}}-L_{\{v,u,w\},x} &lt;/math&gt;
so that the space of linear maps span {L&lt;sub&gt;''u'',''v''&lt;/sub&gt;:''u'',''v'' ∈ ''V''} is closed under commutator bracket, and hence is a Lie algebra '''g'''&lt;sub&gt;0&lt;/sub&gt;.

Any Jordan triple system is a Lie triple system with respect to the product
:&lt;math&gt; [u,v,w] = \{u,v,w\} - \{v,u,w\}. &lt;/math&gt;

A Jordan triple system is said to be '''positive definite''' (resp. '''nondegenerate''') if the bilinear form on ''V'' defined by the trace of L&lt;sub&gt;''u'',''v''&lt;/sub&gt; is positive definite (resp. nondegenerate). In either case, there is an identification of ''V'' with its dual space, and a corresponding involution on '''g'''&lt;sub&gt;0&lt;/sub&gt;. They induce an involution of
:&lt;math&gt;V\oplus\mathfrak g_0\oplus V^*&lt;/math&gt;
which in the positive definite case is a Cartan involution. The corresponding [[symmetric space]] is a [[symmetric R-space]]. It has a noncompact dual given by replacing the Cartan involution by its composite with the involution equal to +1 on '''g'''&lt;sub&gt;0&lt;/sub&gt; and &amp;minus;1 on ''V'' and ''V''&lt;sup&gt;*&lt;/sup&gt;. A special case of this construction arises when '''g'''&lt;sub&gt;0&lt;/sub&gt; preserves a complex structure on ''V''. In this case we obtain dual [[Hermitian symmetric space]]s of compact and noncompact type (the latter being [[bounded symmetric domain]]s).

==Jordan pair==

A Jordan pair is a generalization of a Jordan triple system involving two vector spaces ''V''&lt;sub&gt;+&lt;/sub&gt; and ''V''&lt;sub&gt;&amp;minus;&lt;/sub&gt;. The trilinear form is then replaced by a pair of trilinear forms
:&lt;math&gt; \{\cdot,\cdot,\cdot\}_+\colon V_-\times S^2V_+ \to V_+&lt;/math&gt;
:&lt;math&gt; \{\cdot,\cdot,\cdot\}_-\colon V_+\times S^2V_- \to V_-&lt;/math&gt;
which are often viewed as quadratic maps ''V''&lt;sub&gt;+&lt;/sub&gt; → Hom(''V''&lt;sub&gt;&amp;minus;&lt;/sub&gt;, ''V''&lt;sub&gt;+&lt;/sub&gt;) and ''V''&lt;sub&gt;&amp;minus;&lt;/sub&gt; → Hom(''V''&lt;sub&gt;+&lt;/sub&gt;, ''V''&lt;sub&gt;&amp;minus;&lt;/sub&gt;). The other Jordan axiom (apart from symmetry) is likewise replaced by two axioms, one being
:&lt;math&gt; \{u,v,\{w,x,y\}_+\}_+ = \{w,x,\{u,v,y\}_+\}_+ + \{w, \{u,v,x\}_+,y\}_+ - \{\{v,u,w\}_-,x,y\}_+ &lt;/math&gt;
and the other being the analogue with + and &amp;minus; subscripts exchanged.

As in the case of Jordan triple systems, one can define, for ''u'' in ''V''&lt;sub&gt;&amp;minus;&lt;/sub&gt; and ''v'' in ''V''&lt;sub&gt;+&lt;/sub&gt;, a linear map
:&lt;math&gt; L^+_{u,v}:V_+\to V_+ \quad\text{by} \quad L^+_{u,v}(y) = \{u,v,y\}_+&lt;/math&gt;
and similarly L&lt;sup&gt;&amp;minus;&lt;/sup&gt;. The Jordan axioms (apart from symmetry) may then be written
:&lt;math&gt; [L^{\pm}_{u,v},L^{\pm}_{w,x}] = L^{\pm}_{w,\{u,v,x\}_\pm}-L^{\pm}_{\{v,u,w\}_{\mp},x} &lt;/math&gt;
which imply that the images of L&lt;sup&gt;+&lt;/sup&gt; and L&lt;sup&gt;&amp;minus;&lt;/sup&gt; are closed under commutator brackets in End(''V''&lt;sub&gt;+&lt;/sub&gt;) and End(''V''&lt;sub&gt;&amp;minus;&lt;/sub&gt;). Together they determine a linear map
:&lt;math&gt; V_+\otimes V_- \to \mathfrak{gl}(V_+)\oplus \mathfrak{gl}(V_-)&lt;/math&gt;
whose image is a Lie subalgebra &lt;math&gt;\mathfrak{g}_0&lt;/math&gt;, and the Jordan identities become Jacobi identities for a graded Lie bracket on
:&lt;math&gt; V_+\oplus \mathfrak g_0\oplus V_-,&lt;/math&gt;
so that conversely, if
:&lt;math&gt; \mathfrak g = \mathfrak g_{+1} \oplus \mathfrak g_0\oplus \mathfrak g_{-1}&lt;/math&gt;
is a graded Lie algebra, then the pair &lt;math&gt;(\mathfrak g_{+1}, \mathfrak g_{-1})&lt;/math&gt; is a Jordan pair, with brackets
:&lt;math&gt; \{X_{\mp},Y_{\pm},Z_{\pm}\}_{\pm} := [[X_{\mp},Y_{\pm}],Z_{\pm}].&lt;/math&gt;

Jordan triple systems are Jordan pairs with ''V''&lt;sub&gt;+&lt;/sub&gt; = ''V''&lt;sub&gt;&amp;minus;&lt;/sub&gt; and equal trilinear forms. Another important case occurs when ''V''&lt;sub&gt;+&lt;/sub&gt; and ''V''&lt;sub&gt;&amp;minus;&lt;/sub&gt; are dual to one another, with dual trilinear forms determined by an element of
:&lt;math&gt; \mathrm{End}(S^2V_+) \cong S^2V_+^* \otimes S^2V_-^*\cong \mathrm{End}(S^2V_-).&lt;/math&gt;
These arise in particular when &lt;math&gt; \mathfrak g &lt;/math&gt; above is semisimple, when the Killing form provides a duality between &lt;math&gt;\mathfrak g_{+1}&lt;/math&gt; and &lt;math&gt; \mathfrak g_{-1}&lt;/math&gt;.

==See also==
*[[Associator]]
*[[Quadratic Jordan algebra]]

==References==

* {{citation|first=Wolfgang|last= Bertram|year=2000|title=The geometry of Jordan and Lie structures|series= Lecture Notes in Mathematics|volume=1754|publisher=Springer-Verlag|isbn= 3-540-41426-6}}
* {{citation|first=Sigurdur|last= Helgason|year=2001|title=Differential geometry, Lie groups, and symmetric spaces|publisher= American Mathematical Society}} (1st edition: Academic Press, New York, 1978).
* {{citation|first=Nathan|last= Jacobson|year=1949|jstor=2372102|title= Lie and Jordan triple systems|journal=American Journal of Mathematics|volume= 71|pages=149–170|doi=10.2307/2372102}}
* {{springer|id=Lie_triple_system|title=Lie triple system|first=Noriaki|last= Kamiya}}.
* {{springer|id=Jordan_triple_system|title=Jordan triple system|first=Noriaki|last= Kamiya}}.
* {{citation|first=M.|last= Koecher|year=1969|title= An elementary approach to bounded symmetric domains|series= Lecture Notes|publisher=Rice University}}
* {{citation|first=Ottmar|last= Loos|year=1969|title=Symmetric spaces. Volume 1: General Theory|publisher=  W. A. Benjamin}}
* {{citation|first=Ottmar|last= Loos|year=1969|title=Symmetric spaces. Volume 2: Compact Spaces and Classification|publisher=  W. A. Benjamin}}
* {{citation|first=Ottmar |last=Loos|year=1971|url=http://www.ams.org/bull/1971-77-04/S0002-9904-1971-12753-2/home.html|title= Jordan triple systems, ''R''-spaces, and bounded symmetric domains|journal=  Bulletin of the American Mathematical Society|volume= 77| pages=558–561|doi=10.1090/s0002-9904-1971-12753-2}}
* {{citation|first=Ottmar|last= Loos|year=1975|title=Jordan pairs|series= Lecture Notes in Mathematics|volume=460|publisher= Springer-Verlag}}
*{{citation|last=Loos|first=Ottmar|title=Bounded symmetric domains and Jordan pairs|series=Mathematical lectures|publisher=University of California, Irvine|year=1977|url=http://molle.fernuni-hagen.de/~loos/jordan/archive/irvine/irvine.pdf|deadurl=yes|archiveurl=https://web.archive.org/web/20160303234008/http://molle.fernuni-hagen.de/~loos/jordan/archive/irvine/irvine.pdf|archivedate=2016-03-03|df=}}
*{{citation|last=Meyberg|first= K.|title=Lectures on algebras and triple systems|publisher=[[University of Virginia]]|year= 1972|url=http://www.math.uci.edu/~brusso/Meyberg(Reduced2).pdf}}
* {{citation | last=Rosenfeld | first=Boris | title=Geometry of Lie groups | page=92 | zbl=0867.53002 | series=Mathematics and its Applications | location=Dordrecht | volume=393 | publisher=Kluwer Academic Publishers | year=1997 | isbn=0792343905 }}
* {{citation|last=Tevelev|first= E. |year=2002|url=http://www.emis.de/journals/JLT/vol.12_no.2/9.html|title=Moore-Penrose inverse, parabolic subgroups, and Jordan pairs|journal= Journal of Lie theory|volume=12|pages=461–481}}

[[Category:Representation theory]]</text>
      <sha1>mi3x9uuwb16xl2nfse77st0vrj6a4kl</sha1>
    </revision>
  </page>
  <page>
    <title>Von Neumann neighborhood</title>
    <ns>0</ns>
    <id>8485219</id>
    <revision>
      <id>868646934</id>
      <parentid>858400298</parentid>
      <timestamp>2018-11-13T15:26:38Z</timestamp>
      <contributor>
        <username>Gordon Guojun Zhang</username>
        <id>34873936</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3775">&lt;!--Do not add the lowercase template to this article.  "Von" in "von Neumann" is properly capitalized when it begins a sentence or an article title.--&gt;
[[File:Von Neumann neighborhood.svg|right|thumb|150px|Manhattan distance r = 1]]
[[File:Manhattan_distance_r=2.svg|right|thumb|150px|Manhattan distance r = 2]]

In [[cellular automata]], the '''von Neumann neighborhood''' (or '''4-neighborhood''') is classically defined on a two-dimensional [[square lattice]] and is composed of a central cell and its four adjacent cells.&lt;ref&gt;{{citation|title=Cellular Automata Machines: A New Environment for Modeling|first1=Tommaso|last1=Toffoli|author1-link=Tommaso Toffoli|first2=Norman|last2=Margolus|author2-link=Norman Margolus|year=1987|publisher=MIT Press|page=60}}.&lt;/ref&gt;  The neighborhood is named after [[John von Neumann]], who used it to define the [[von Neumann cellular automaton]] and the [[von Neumann universal constructor]] within it.&lt;ref&gt;{{citation|title=Historical Encyclopedia of Natural and Mathematical Sciences, Volume 1|first=Ari|last=Ben-Menahem|publisher=Springer|year=2009|isbn=9783540688310|page=4632|url=https://books.google.com/books?id=9tUrarQYhKMC&amp;pg=PA4632}}.&lt;/ref&gt;  It is one of the two most commonly used neighborhood types for two-dimensional cellular automata, the other one being the [[Moore neighborhood]].

This neighbourhood can be used to define the notion of [[4-connected neighborhood|4-connected]] [[pixel]]s in [[computer graphics]].&lt;ref&gt;{{citation|title=Handbook of Computer Vision Algorithms in Image Algebra|first1=Joseph N.|last1=Wilson|first2=Gerhard X.|last2=Ritter|edition=2nd|publisher=CRC Press|year=2000|isbn=9781420042382|page=177|url=https://books.google.com/books?id=YBlSUIybptwC&amp;pg=PA177}}.&lt;/ref&gt;

The von Neumann neighbourhood of a cell is the cell itself and the cells at a [[Manhattan distance]] of 1.

The concept can be extended to higher dimensions, for example forming a 6-cell [[octahedron|octahedral]] neighborhood for a cubic cellular automaton in three dimensions.&lt;ref name="bb05"/&gt;

== Von Neumann neighborhood of range ''r'' ==

An extension of the simple von Neumann neighborhood described above is to take the set of points at a [[Manhattan distance]] of ''r''&amp;nbsp;&gt;&amp;nbsp;1. This results in a diamond-shaped region (shown for ''r''&amp;nbsp;=&amp;nbsp;2 in the illustration). These are called von Neumann neighborhoods of range or extent ''r''. The number of cells in a 2-dimensional von Neumann neighborhood of range ''r'' can be expressed as &lt;math&gt;1+2r(r+1)&lt;/math&gt;. The number of cells in a ''d''-dimensional von Neumann neighborhood of range ''r'' is the [[Delannoy number]] ''D''(''d'',''r'').&lt;ref name="bb05"&gt;{{citation
 | last1 = Breukelaar | first1 = R.
 | last2 = Bäck | first2 = Th.
 | contribution = Using a Genetic Algorithm to Evolve Behavior in Multi Dimensional Cellular Automata: Emergence of Behavior
 | doi = 10.1145/1068009.1068024
 | isbn = 1-59593-010-8
 | location = New York, NY, USA
 | pages = 107–114
 | publisher = ACM
 | title = Proceedings of the 7th Annual Conference on Genetic and Evolutionary Computation (GECCO '05)
 | year = 2005}}.&lt;/ref&gt; The number of cells on a surface of a ''d''-dimensional von Neumann neighborhood of range ''r'' is the Zaitsev number {{OEIS|id=A266213}}.

== See also ==
* [[Moore neighborhood]]
* [[Neighbourhood (graph theory)]]
* [[Taxicab geometry]]
* [[Lattice graph]]
* [[Pixel connectivity]]
* [[Chain code]]

== References ==
{{reflist}}

==External links==
* {{mathworld|urlname=vonNeumannNeighborhood|title=von Neumann Neighborhood}}
* Tyler, Tim, ''[http://cell-auto.com/neighbourhood/vn/ The von Neumann neighborhood]'' at [http://cell-auto.com/ cell-auto.com]

[[Category:Cellular automata]]

{{Conway's Game of Life}}

{{comp-sci-theory-stub}}</text>
      <sha1>1dyw0v40ude4d2m6n1ud2t8ck7t280c</sha1>
    </revision>
  </page>
  <page>
    <title>Well equidistributed long-period linear</title>
    <ns>0</ns>
    <id>25476856</id>
    <revision>
      <id>849903838</id>
      <parentid>839108562</parentid>
      <timestamp>2018-07-12T06:01:54Z</timestamp>
      <contributor>
        <username>Eyreland</username>
        <id>180850</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3443">{{multiple issues|
{{One source|date=November 2017}}
{{EL|date=November 2017}}
}}
The '''Well Equidistributed Long-period Linear (WELL)''' is a family of [[pseudorandom number generator]]s developed in 2006 by [[François Panneton]], [[Pierre L'Ecuyer]], and {{nihongo|{{Interlanguage link multi|Makoto Matsumoto|ja|3=松本眞}}|松本 眞}}.&lt;ref&gt;{{Cite journal| ref = harv| doi = 10.1145/1132973.1132974| title = Improved long-period generators based on linear recurrences modulo 2| journal = ACM Transactions on Mathematical Software| volume = 32| number = 1| pages = 1&amp;ndash;16| date = March 2006| last1 = Panneton | first1 = François O. | last2 = l'Ecuyer | first2 = Pierre | last3 = Matsumoto | first3 = Pierre | url = http://www.iro.umontreal.ca/~lecuyer/myftp/papers/wellrng.pdf}}&lt;/ref&gt; It is a form of [[linear feedback shift register]] optimized for software implementation on a 32-bit machine.


== Operational Design ==
The structure is similar to the [[Mersenne twister]], a large state made up of previous output words (32 bits each), from which a new output word is generated using linear [[Recurrence relation|recurrences]] modulo 2 over a finite [[GF(2)|binary field]] &lt;math&gt;F_2&lt;/math&gt;.  However, a more complex recurrence produces a denser generator polynomial, producing better statistical properties.

Each step of the generator reads five words of state: the oldest 32 bits (which may straddle a word boundary if the state size is not a multiple of 32), the newest 32 bits, and three other words in between. 

Then a series of eight single-word transformations (mostly of the form &lt;code&gt;x := x ⊕ (x &gt;&gt; k)&lt;/code&gt;) and six exclusive-or operations combine those into two words, which become the newest two words of state, one of which will be the output.

== Variants ==

Specific parameters are provided for the following generators:
* WELL512a
* WELL521a, WELL521b
* WELL607a, WELL607b
* WELL800a, WELL800b
* WELL1024a, WELL1024b
* WELL19937a, WELL19937b, WELL19937c
* WELL21701a
* WELL23209a, WELL23209b
* WELL44497a, WELL44497b.  

Numbers give the state size in bits; letter suffixes denote variants of the same size.

== Implementations ==

* [http://www.iro.umontreal.ca/~panneton/WELLRNG.html Implementations of WELL512a, WELL1024a, WELL19937a, WELL19937c, WELL44497a, WELL44497b in C] (Free for non-commercial use)
* [https://github.com/non/spire/tree/v0.11.0/core/shared/src/main/scala/spire/random/rng/ Implementations of same algorithms in Scala]
* [http://bitbucket.org/sergiu/random/src/tip/well.hpp Implementations in  C++]
* [http://www.iro.umontreal.ca/~simardr/ssj/doc/html/umontreal/iro/lecuyer/rng/RandomStreamBase.html Implementations of WELL512, WELL1024, WELL607 in Java]
* [http://bbcbasic.co.uk/wiki/doku.php?id=high_20quality_20random_20number_20generation Implementations of WELL512, WELL1024 in BBC BASIC]
* [http://www.ritsumei.ac.jp/~harase/megenerators.html Modified "maximally equidistributed" implementations of WELL19937, WELL44497 in C] (Free for non-commercial use)
* [http://lomont.org/Math/Papers/2008/Lomont_PRNG_2008.pdf  Implementation of WELL512 in C] (Public Domain)

==References==
{{Reflist}}

== External links ==
* [http://www.iro.umontreal.ca/~panneton/  The academic paper, and related articles by François Panneton]
* [http://www.iro.umontreal.ca/~lecuyer/papers.html Pierre L'Ecuyer's publications]

[[Category:Pseudorandom number generators]]


{{comp-sci-theory-stub}}</text>
      <sha1>30rrhj1ujhucgmfn2filaqctj0wgn78</sha1>
    </revision>
  </page>
</mediawiki>
