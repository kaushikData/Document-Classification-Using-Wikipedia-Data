<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Acyclic coloring</title>
    <ns>0</ns>
    <id>690769</id>
    <revision>
      <id>742445146</id>
      <parentid>709337421</parentid>
      <timestamp>2016-10-03T19:15:26Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6521">[[Image:Acyclic coloring.svg|300px|right|thumb|The acyclic chromatic number of [[McGee graph]] is 3.]]

In [[graph theory]], an '''acyclic coloring''' is a [[Graph coloring|(proper) vertex coloring]] in which every [[Graph coloring|2-chromatic]] subgraph is [[Glossary of graph theory|acyclic]].
The '''acyclic chromatic number''' A(''G'') of a graph ''G'' is the least number of colors needed in any acyclic coloring of ''G''.

Acyclic coloring is often associated with graphs embedded on non-plane surfaces.

== Upper Bounds ==
A(''G'') ≤ 2 if and only if ''G'' is acyclic.

Bounds on A(''G'') in terms of the [[Glossary of graph theory|maximum degree]] Δ(''G'') of ''G'' include the following:
* A(''G'') ≤ 4 if Δ(''G'') = 3. {{harv|Grünbaum|1973}}
* A(''G'') ≤ 5 if Δ(''G'') = 4. {{harv|Burstein|1979}}
* A(''G'') ≤ 7 if Δ(''G'') = 5. {{harv|Kostochka|Stocker|2011}}
* A(''G'') ≤ 12 if Δ(''G'') = 6. {{harv|Yadav|Satish|2009}}

A milestone in the study of acyclic coloring is the following affirmative answer to a conjecture of 
Grünbaum:

'''Theorem.''' {{harv|Borodin|1979}} 
:A(''G'') ≤ 5 if ''G'' is planar graph.

{{harvtxt|Grünbaum|1973}} introduced acyclic coloring and acyclic chromatic number, and conjectured the result in the above theorem.
Borodin's proof involved several years of painstaking inspection of 450 reducible configurations.
One consequence of this theorem is that every planar graph can be decomposed into an [[Glossary of graph theory|independent set]] and two [[induced subgraph|induced]] [[Glossary of graph theory|forests]]. {{harvs|last=Stein|year=1970|year2=1971}}

== Algorithms and Complexity ==
It is [[NP-complete]] to determine whether A(''G'') ≤ 3. {{harv|Kostochka|1978}}

{{harvtxt|Coleman|Cai|1986}} showed that the decision variant of the problem is NP-complete even when ''G'' is a bipartite graph.

{{harvtxt|Gebremedhin|Tarafdar|Pothen|Walther|2008}} demonstrated that every proper vertex coloring of a [[chordal graph]] is also an acyclic coloring.
Since chordal graphs can be optimally colored in ''O(n+m)'' time, the same is also true for acyclic coloring on that class of graphs.

A linear-time algorithm to acyclically color a graph of maximum degree ≤ 3 using 4 colors or fewer was given by {{harvtxt|Skulrattanakulchai|2004}}.

== See also ==
* [[Star coloring]]

== References ==
*{{citation
 | last = Borodin | first = O. V.
 | title = On acyclic colorings of planar graphs
 | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]
 | year = 1979 | volume = 25 | pages = 211–236
 | doi = 10.1016/0012-365X(79)90077-3}}.
* {{Citation | last1 = Burstein | first1 = M. I. | year = 1979 | title = Every 4-valent graph has an acyclic 5-coloring (in Russian) | url = | journal = Soobšč. Akad. Nauk Gruzin. SSR | volume = 93 | issue = | pages = 21–24 | postscript = . }}
* {{Citation | doi = 10.1007/BF02764716 | last1 = Grünbaum | first1 = B. | authorlink = Branko Grünbaum | year = 1973 | title = Acyclic colorings of planar graphs | url = | journal = Israel J. Math. | volume = 14 | issue = | pages = 390–408 | postscript = . }}
*{{citation
 | last1 = Coleman | first1 = Thomas F.
 | last2 = Cai | first2 = Jin-Yi
 | title = The Cyclic Coloring Problem and Estimation of Sparse Hessian Matrices
 | journal = SIAM. J. on Algebraic and Discrete Methods
 | volume = 7 | issue = 2 | pages = 221–235 | year = 1986
 | doi = 10.1137/0607026}}.
*{{citation
 | last1 = Fertin | first1 = Guillaume
 | last2 = Raspaud | first2 = André
 | title = Acyclic coloring of graphs of maximum degree five: Nine colors are enough
 | journal = Information Processing Letters
 | volume = 105 | issue = 2 | year = 2008 | pages = 65–72
 | doi = 10.1016/j.ipl.2007.08.022}}.
*{{citation
 | last1 = Gebremedhin | first1 = Assefaw H.
 | last2 = Tarafdar | first2 = Arijit
 | last3 = Pothen | first3 = Alex
 | last4 = Walther | first4 = Andrea
 | title = Efficient Computation of Sparse Hessians Using Coloring and Automatic Differentiation
 | journal = Informs Journal on Computing
 | year = 2008
 | doi = 10.1287/ijoc.1080.0286
 | volume = 21
 | pages = 209}}.
*{{citation|last1=Jensen|first1=Tommy R.|last2=Toft|first2=Bjarne|year=1995|title=Graph Coloring Problems|location=New York|publisher=Wiley-Interscience|isbn=0-471-02865-7}}.
*{{citation|last=Kostochka|first=A. V.|year=1978|title=Upper bounds of chromatic functions of graphs|language=Russian|series=Doctoral thesis|location=Novosibirsk}}.
*{{citation
 | last1 = Kostochka | first1 = Alexandr V.
 | last2 = Stocker | first2 = Christopher
 | issue = 1
 | journal = Ars Mathematica Contemporanea
 | mr = 2785823
 | pages = 153–164
 | title = Graphs with maximum degree 5 are acyclically 7-colorable
 | url = http://amc-journal.eu/index.php/amc/article/view/198
 | volume = 4
 | year = 2011}}.
*{{citation
 | last = Skulrattanakulchai | first1 = San
 | title = Acyclic colorings of subcubic graphs
 | journal = Information Processing Letters
 | volume = 92 | issue = 4 | year = 2004 | pages = 161–167
 | doi = 10.1016/j.ipl.2004.08.002}}.
* {{Citation | doi = 10.1090/S0002-9904-1970-12559-9 | last1 = Stein | first1 = S. K. | year = 1970 | title = B-sets and coloring problems | url = | journal = Bull. Amer. Math. Soc. | volume = 76 | issue = | pages = 805–806 | postscript = . }}
* {{Citation | last1 = Stein | first1 = S. K. | year = 1971 | title = B-sets and planar maps | url = | journal = Pacific J. Math. | volume = 37 | issue = 1| pages = 217–224 | postscript = . | doi=10.2140/pjm.1971.37.217}}
*{{citation
 | last1 = Yadav| first1 = Kishore
 | last2 = Satish | first2 = Venkaiah
 | last3 = Yadav
 | first3 = Kishore
 | last4 = Kothapalli
 | first4 = Kishore
 | title = Acyclic coloring of graphs of maximum degree six: Twelve colors are enough
 | journal = Electronic Notes in Discrete Mathematics
 | volume = 35 | issue =   | year = 2009 | pages = 177–182
 | doi =10.1016/j.endm.2009.11.030 }}.

==External links==
* [http://www.math.uiuc.edu/~west/regs/starcol.html Star colorings and acyclic colorings (1973)], present at the [http://www.math.uiuc.edu/~west/regs/ Research Experiences for Graduate Students (REGS)] at the University of Illinois, 2008.
* [https://web.archive.org/web/20070628204549/http://www.math.tu-berlin.de/EuroComb05/Talks/Contributed/15-Fertin.pdf Acyclic Coloring of Graphs of Maximum Degree ∆], talk slides presented by G. Fertin and A. Raspaud at EUROCOMB 05, Berlin, 2005.

{{DEFAULTSORT:Acyclic Coloring}}
[[Category:Graph coloring]]</text>
      <sha1>pp4ajrgf9ghc5sl60o9mfklx61bcrue</sha1>
    </revision>
  </page>
  <page>
    <title>Arthur Hobbs (mathematician)</title>
    <ns>0</ns>
    <id>42691004</id>
    <revision>
      <id>857346372</id>
      <parentid>825875031</parentid>
      <timestamp>2018-08-31T03:41:22Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>/* External links */add authority control, test</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5660">'''Arthur Hobbs''' (born 1940) is an American [[mathematician]] specializing in [[graph theory]]. He spent his teaching career at [[Texas A&amp;M University]].

==Early and personal life==
Arthur Hobbs was born on June 19, 1940, in Washington, D.C. He is the eldest child of his family, having two younger brothers. His father was an engineer and later became an attorney. The family moved in 1941 to Pennsylvania, and again after World War II to [[South Bend, Indiana]], where Arthur Hobbs grew up. He married his wife Barbara in 1964; they have two daughters and five grandchildren.

==Education and early career==

After graduating in 1958 from [[John Adams High School (South Bend, Indiana)|John Adams High School]], Hobbs studied mathematics at the [[University of Michigan]], graduating in 1962. He then served in the US Army in Washington, D.C., for approximately two years, and then from 1965 to 1968 worked for the [[National Bureau of Standards]].

He received his Ph.D. from the [[University of Waterloo]] in Ontario, Canada, in 1971. His research focused on [[Hamiltonian path|Hamiltonian]] cycles, particularly concentrating in squares and higher powers of graphs, and his thesis adviser was the graph theorist [[William Thomas Tutte]].

==Academic career==
After receiving his Ph.D., Hobbs began teaching as a mathematics professor at [[Texas A&amp;M University]] in 1971, where he worked until his retirement in 2008. He was the faculty senator for twelve years, and also taught various mathematics courses including, but not limited to [[calculus]], [[combinatorics]], [[discrete mathematics]], [[graph theory]], and [[number theory]]. Hobbs and his colleague taught a course in the intersection of graph theory and number theory, he explains:

{{quote|text=We taught enough of the elements of our specialties that students could read a research paper including elements of both subjects. Then students were asked to select a paper from a list we provided, read it, and report on it to the class. An important aspect of the course was gaining a feeling for the discovery process involved in research. We asked about each idea presented, "Are there questions that are not addressed here? Can these ideas be extended in ways the authors did not discuss?" There was a test on each of number theory and graph theory just after the lectures on that topic, and the grade was based on the results of those tests and on the presentations made. One consequence of this course was a published research paper.&lt;ref name=TAMU&gt;{{cite web|url=http://www.math.tamu.edu/~arthur.hobbs/ |title=Arthur Hobbs, Professor |publisher=Texas A&amp;M University |accessdate=May 14, 2004}}&lt;/ref&gt;}}

==Research==
Hobbs' research before entering graduate school was on thickness of graphs. Later, in graduate school and for ten years following, he concentrated on Hamiltonian cycles, particularly in squares and higher powers of graphs. He then spent a couple of years working on the Gyarfas and Lehel conjecture that any family of trees T1; T2; : : : Tn, with 1; 2; : : : ; ''n'' vertices respectively, can be packed in an edge-disjoint manner into the complete graph on ''n'' vertices. This conjecture is still open. Hobbs has also worked with packings of graphs with trees and coverings by trees, which he worked on with several co-authors, including [[Paul A. Catlin]], Jerrold W. Grossman, Lavanya Kannan, and Hong-Jian Lai.

They defined the fractional [[arboricity]] of a graph as

: &lt;math&gt; \gamma(G) = \max_{H \subseteq G} \left({{|E(H)}|\over{|V(H)| - \omega(H)}}\right),&lt;/math&gt;

where ''&amp;omega;''(''H'' is the number of components of H and the maximum is taken over all subgraphs H for which the denominator is not zero. They also defined the [[strength of a graph]] as

: &lt;math&gt;\eta(G) = \min_{S \subseteq E(G)} \left( {|S|\over{\omega(G-S)-\omega(G)}}\right),&lt;/math&gt;

where the maximum is taken over all subsets ''S'' of ''E''(''G'') for which the denominator is not zero. Additionally, they characterized uniformly dense graphs, and have found several classes of uniformly dense graphs and several ways of constructing such graphs.

Hobbs has also done research in [[matroid]] theory.

==Publications==
Dr. Hobbs has 40 publications in graph theory, and in 1989 co-authored the book ''Elementary Linear Algebra.'' He has also written an essay on how to read research papers. A few publications are listed below:

* Hobbs, Arthur M.; Kannan, Lavanya; Lai, Hong-Jian; Lai, Hongyuan; Weng, Guoqing Balanced and 1-balanced graph constructions. Discrete Appl. Math. 158 (2010), no. 14, 1511–1523.
* Fleischner, Herbert; Hobbs, Arthur M.; Tapfuma Muzheve, Michael Hamiltonicity in vertex envelopes of plane cubic graphs. Discrete Math. 309 (2009), no. 14, 4793–4809.
* Kannan, Lavanya; Hobbs, Arthur; Lai, Hong-Jian; Lai, Hongyuan Transforming a graph into a 1-balanced graph. Discrete Appl. Math. 157 (2009), no. 2, 300–308&lt;ref&gt;{{cite web|url=http://www.ams.org/mathscinet/search/publications.html?pg1=INDI&amp;s1=86630|title=search: Arthur Hobbs |publisher=MathSciNet}} (subscription required)&lt;/ref&gt;
* A. M. Hobbs, H.-J. Lai, H. Lai, and G. Weng, Constructing Uniformly Dense Graphs, preprint, October 1, 1994&lt;ref name=TAMU/&gt;

== References ==
&lt;references /&gt;

==External links==
*[http://www.math.tamu.edu/~arthur.hobbs Arthur Hobbs], Texas A&amp;M University

{{authority control}}

{{DEFAULTSORT:Hobbs, Arthur}}
[[Category:1940 births]]
[[Category:Living people]]
[[Category:People from Washington, D.C.]]
[[Category:University of Michigan alumni]]
[[Category:20th-century American mathematicians]]
[[Category:21st-century American mathematicians]]
[[Category:Graph theorists]]</text>
      <sha1>69zyvqrsj3z35slzr1e3enf1paxipxk</sha1>
    </revision>
  </page>
  <page>
    <title>Bangladesh Mathematical Olympiad</title>
    <ns>0</ns>
    <id>5377011</id>
    <revision>
      <id>852155755</id>
      <parentid>852155590</parentid>
      <timestamp>2018-07-27T01:04:26Z</timestamp>
      <contributor>
        <username>DhanshiriM</username>
        <id>34259145</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5373">{{Refimprove|date=February 2016}}
[[Image:bdmo logo.png|right|Logo of BdMO]]
The '''Bangladesh Mathematical Olympiad''' ('''BdMO''') is an annual mathematical competition arranged for school and college students to nourish their interest and capabilities for [[mathematics]]. It has been regularly organized by the Bangladesh Math Olympiad Committee (BdMOC) since 2001. The first Math Olympiad was held in [[Shahjalal University of Science and Technology]]. Mohammad Kaykobad, Muhammad Zafar Iqbal and Munir Hasan were instrumental in establishing the Mathematics Olympiad in Bangladesh.

With the dedicated endeavor of the members of the committee, the daily newspaper ''[[Prothom Alo]]'' and the [[Dutch Bangla Bank|Dutch Bangla Bank Limitedf]], BdMOC has already achieved its primary goal – to send a team to the [[International Mathematical Olympiad]] (IMO). Bangladeshi students have participated in the [[International Mathematical Olympiad]] since 2005.

Besides arranging Divisional and National Math Olympiads, BdMOC always extends its cooperation to all interested groups and individuals who want to arrange a Mathematics Olympiad. The Bangladesh Math Olympiad and the selection of the Bangladeshi national team for the IMO is bounded by rules set by the BdMOC, called BdMO Regulations. The Bangladesh Mathematical Olympiad is open for school and college students from the country. The competitions usually take place around December–January–February. In IMO 2014, the Bangladesh team achieved one silver, one bronze and four honorable mentions, placing the country at 53 among 101 participating countries.&lt;ref&gt;{{cite web| url=http://www.dhakatribune.com/education/2014/jul/13/bangladesh-wins-silver-and-bronze-55th-mathematics-olympiad |title=Bangladesh wins silver and bronze at 55th Mathematics Olympiad |accessdate=August 16, 2015}}&lt;/ref&gt; In IMO 2015, the Bangladesh team achieved one silver, four bronze and one honorable mention, finishing in 33rd place.&lt;ref&gt;{{cite web| url=https://www.imo-official.org/country_individual_r.aspx?code=BGD |title=Persistence of Contestants BGD |accessdate=August 16, 2015}}&lt;/ref&gt; In IMO 2018 Bangladesh won a gold medal for the first time. 

==Format==
The students are divided into four academic categories:
* Primary: Class 3-5
* Junior: Class 6-8
* Secondary: Class 9-10
* Higher Secondary: Class 11–12

===Regional Olympiad===
The country is divided in 24 regions for the Regional Olympiad. In each division except Dhaka, nearly 60 students among 1000 participants are selected for the National Olympiad.{{Citation needed|date=February 2016}} In Dhaka, the number of participants is more than 3000 and 100–150 are selected for the National Olympiad.{{Citation needed|date=February 2016}} In all of the problems in the Regional Olympiad, only the final answers are necessary.
===National Olympiad===
In the National Olympiad, the top 71 participants are given prizes. The time given for solving the problems depends on the category: 2 hours for the Primary category, 3 hours for the Junior category, and 4 hours for the Secondary and Higher Secondary category.

== National Math Camp ==
A group for the National Math Camp is selected from the winners of the National Olympiad.

== Medal winners in International Mathematically Olympiad ==
For every year since 2005, Bangladeshi students have participated in the [[International Mathematical Olympiad]].
Samin Riasat and Nazia Chowdhury won bronze medals for Bangladesh in 2009. Dhananjoy Biswas won the first silver medal for Bangladesh in 2012.

The following is the full list of medal winners from Bangladesh:
{| border="1" style="border-collapse:collapse; margin: 1em auto 1em auto; height:200pt; text-align:center;"
! scope="col" width="70pt" | Year
! scope="col" width="300pt" | Name of the Recipient
! scope="col" width="80pt" | Medal
|-
|rowspan="2" | 2009 || Samin Riasat || Bronze
|-
|  Nazia Chowdhury || Bronze
|-
| 2010 || Tarik Adnan || Bronze
|-
| 2011 ||Dhananjoy Biswas|| Bronze
|-
|rowspan="3" | 2012 || Dhananjoy Biswas || Silver
|-
| Sourav Das || Bronze
|-
| Nur Muhammad Shafiullah || Bronze
|-
|rowspan="3" | 2013 || Nur Muhammad Shafiullah || Bronze
|-
| Adib Hasan || Bronze
|-
| Sourav Das || Bronze
|-
|rowspan="2" | 2014 || Nur Muhammad Shafiullah || Silver
|-
| Adib Hasan || Bronze
|-
|rowspan="5" | 2015 || Md Sanzeed Anwar || Silver
|-
| Adib Hasan || Bronze
|-
| Asif E Elahi || Bronze
|-
| Md Sabbir Rahman || Bronze
|-
| Sazid Akhter Turzo || Bronze
|-
|rowspan="6" | 2016 || Asif E Elahi || Silver
|-
| Ahmed Zawad Chowdhury || Bronze
|-
| Md Sabbir Rahman || Bronze
|-
| Sazid Akhter Turzo || Bronze
|-
| S M Nayeemul Islam Swad || Honourable Mention 
|- 
| Md Sanzeed Anwar || Honourable Mention 
|-
|rowspan="6" | 2017 || Asif E Elahi || Silver
|-
| Ahmed Zawad Chowdhury || Silver
|-
| Rahul Saha || Bronze
|-
| Tamzid Morshed Rubab || Bronze
|-
|  S M Nayeemul Islam Swad || Honourable Mention 
|- 
| Md Sabbir Rahman || Honourable Mention 
|-
|rowspan="1" | 2018 || Ahmed Zawad Chowdhury || Gold
|}

== References ==
{{Reflist}}

== External links ==
{{Commons category|Bangladesh Mathematical Olympiad}}
* {{Official website|matholympiad.org.bd}}

[[Category:Education in Bangladesh]]
[[Category:2001 establishments in Bangladesh]]
[[Category:Mathematics competitions]]
[[Category:Science and technology in Bangladesh]]</text>
      <sha1>l4fjmi5x174lxhifkcddcblgggoea7x</sha1>
    </revision>
  </page>
  <page>
    <title>Bel–Robinson tensor</title>
    <ns>0</ns>
    <id>34248077</id>
    <revision>
      <id>823453085</id>
      <parentid>735685055</parentid>
      <timestamp>2018-02-01T10:05:28Z</timestamp>
      <contributor>
        <username>Bibcode Bot</username>
        <id>14394459</id>
      </contributor>
      <minor/>
      <comment>Adding 0 [[arXiv|arxiv eprint(s)]], 1 [[bibcode|bibcode(s)]] and 0 [[digital object identifier|doi(s)]]. Did it miss something? Report bugs, errors, and suggestions at [[User talk:Bibcode Bot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2001">In [[general relativity]] and [[differential geometry]], the '''Bel–Robinson tensor''' is a tensor defined in the [[abstract index notation]] by:
:&lt;math&gt;T_{abcd}=C_{aecf}C_{b} {}^{e} {}_{d} {}^{f} + \frac{1}{4}\epsilon_{ae}{}^{hi} \epsilon_{b}{}^{ej}{}_{k} C_{hicf} C_{j}{}^{k}{}_{d}{}^{f}&lt;/math&gt;
Alternatively,
:&lt;math&gt;T_{abcd}=C_{aecf}C_{b} {}^{e} {}_{d} {}^{f} - \frac{3}{2} g_{a[b} C_{jk]cf} C^{jk}{}_{d}{}^{f}&lt;/math&gt;
where &lt;math&gt;C_{abcd}&lt;/math&gt; is the [[Weyl tensor]].  It was introduced by Lluís Bel in 1959.&lt;ref&gt;{{citation|first1=L.|last1=Bel|title=Introduction d'un tenseur du quatrième ordre|journal=Comptes rendus hebdomadaires des séances de l'Académie des sciences|volume=248|page=1297|year=1959|url=http://gallica.bnf.fr/ark:/12148/bpt6k32002/f1321.image.langEN}}&lt;/ref&gt;&lt;ref&gt;{{citation|first1=J. M. M.|last1=Senovilla|title=Editor's Note: Radiation States and the Problem of Energy in General Relativity by Louis Bel|journal=General Relativity and Gravitation|volume=32|page=2043|year=2000|doi=10.1023/A:1001906821162|bibcode=2000GReGr..32.2043S}}&lt;/ref&gt; The Bel–[[Ivor Robinson (physicist)|Robinson]] tensor is constructed from the Weyl tensor in a manner analogous to the way the [[electromagnetic stress–energy tensor]] is built from the [[electromagnetic tensor]]. Like the electromagnetic stress–energy tensor, the Bel–Robinson tensor is totally symmetric and traceless:
:&lt;math&gt;T_{abcd}=T_{(abcd)}&lt;/math&gt;
:&lt;math&gt;T^{a}{}_{acd} = 0&lt;/math&gt;
In general relativity, there is no unique definition of the local energy of the gravitational field. The Bel–Robinson tensor is a possible definition for local energy, since it can be shown that whenever the [[Ricci tensor]] vanishes (i.e. in vacuum), the Bel–Robinson tensor is divergence-free:
:&lt;math&gt;\nabla^{a} T_{abcd} = 0&lt;/math&gt;

==References==
&lt;references/&gt;

{{DEFAULTSORT:Bel-Robinson tensor}}
[[Category:Tensors in general relativity]]
[[Category:Differential geometry]]


{{relativity-stub}}
{{differential-geometry-stub}}</text>
      <sha1>32fa309jg1tbduf2s3uwjl69770vfdi</sha1>
    </revision>
  </page>
  <page>
    <title>Blood pressure</title>
    <ns>0</ns>
    <id>56558</id>
    <revision>
      <id>866548283</id>
      <parentid>866548261</parentid>
      <timestamp>2018-10-31T00:52:29Z</timestamp>
      <contributor>
        <username>Red Phoenix</username>
        <id>5170388</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/38.86.48.42|38.86.48.42]] ([[User talk:38.86.48.42|talk]]) ([[WP:HG|HG]]) (3.4.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="48964">{{Other uses|Blood pressure (disambiguation)}}
{{Infobox diagnostic
| name            = Blood pressure
| image           = Blutdruck.jpg
| alt             =
| caption         = A [[sphygmomanometer]], a device used for measuring arterial pressure
| DiseasesDB      =
| ICD10           =
| ICD9            =
| ICDO            =
| MedlinePlus     = 007490
| eMedicine       =
| MeshID          = D001795
| LOINC           = {{LOINC|35094-2}}
| HCPCSlevel2     =
| OPS301          =
| reference_range =
}}

'''Blood pressure''' ('''BP''') is the [[pressure]] of circulating [[blood]] on the walls of [[blood vessel]]s. Used without further specification, "blood pressure" usually refers to the pressure in large [[Artery|arteries]] of the [[systemic circulation]].  Blood pressure is usually expressed in terms of the [[systole|systolic pressure]] (maximum during one heart beat) over [[diastole|diastolic pressure]] (minimum in between two heart beats) and is measured in millimeters of mercury ([[Millimeter of mercury|mmHg]]), above the surrounding atmospheric pressure.

Blood pressure is one of the [[vital signs]], along with [[respiratory rate]], [[heart rate]], [[oxygen saturation]], and [[body temperature]].  Normal resting blood pressure in an [[adult]] is approximately {{convert|120|mmHg}} systolic, and {{convert|80|mmHg}} diastolic, abbreviated "120/80&amp;nbsp;mmHg".

Traditionally, blood pressure was measured non-invasively using a [[mercury (element)|mercury-tube]] [[sphygmomanometer]], or an aneroid gauge, which is still generally considered to be the gold standard of accuracy for auscultatory readings.&lt;ref name=":0"&gt;{{Cite journal|last=Ogedegbe|first=Gbenga|last2=Pickering|first2=Thomas|date=2010-11-01|title=Principles and techniques of blood pressure measurement|journal=Cardiology Clinics|volume=28|issue=4|pages=571–86|doi=10.1016/j.ccl.2010.07.006|issn=1558-2264|pmc=3639494|pmid=20937442}}&lt;/ref&gt; More recently other semi-automated methods have become common, largely due to concerns about potential mercury toxicity,&lt;ref&gt;{{Cite journal|last=O'brien|first=Eoin|date=2001-01-01|title=Blood pressure measurement is changing!|url=http://heart.bmj.com/content/85/1/3|journal=Heart|language=en|volume=85|issue=1|pages=3–5|doi=10.1136/heart.85.1.3|issn=1468-201X|pmc=1729570|pmid=11119446}}&lt;/ref&gt; although cost and ease of use have also influenced this trend.&lt;ref name=":0" /&gt; Early automated alternatives to mercury-tube sphygmomanometers were often seriously inaccurate, but validated devices allow for an average difference between two standardized reading methods of 5&amp;nbsp;mm Hg or less and a standard deviation of less than 8&amp;nbsp;mm Hg.&lt;ref name=":0" /&gt;

Blood pressure is influenced by [[cardiac output]], [[total peripheral resistance]] and [[arterial stiffness]] and varies depending on situation, emotional state, activity, and relative health/disease states.  In the short term, blood pressure is [[Homeostasis#The arterial blood pressure homeostat|regulated]] by [[baroreceptor]]s which act via the brain to influence [[nervous system|nervous]] and [[endocrine system|endocrine]] systems.

Blood pressure that is low is called [[hypotension]], and pressure that is consistently high is [[hypertension]].  Both have many causes and may be of sudden onset or of long duration.  Long-term hypertension is a risk factor for many diseases, including [[heart disease]], [[stroke]] and [[kidney failure]].  Long-term hypertension is more common than long-term hypotension, which often goes undetected because of infrequent monitoring and the absence of symptoms.
{{TOC limit|3}}

== Classification ==

=== Systemic arterial pressure ===
{| class="wikitable" style = "float: right; margin-left:15px; text-align:center"
|+Classification of blood pressure (adults)&lt;ref name="Heartorg"/&gt;&lt;ref name='Mayo2009causes'&gt;{{cite web |url=http://www.mayoclinic.com/health/low-blood-pressure/DS00590/DSECTION=causes |title=Low blood pressure (hypotension) – Causes    |accessdate=2010-10-19 |author=Mayo Clinic staff |date=2009-05-23 |website=MayoClinic.com |publisher=Mayo Foundation for Medical Education and Research}}&lt;/ref&gt;&lt;ref name=AU2016&gt;{{cite web|title=Guideline for the diagnosis and management of hypertension in adults|url=https://www.heartfoundation.org.au/images/uploads/publications/PRO-167_Hypertension-guideline-2016_WEB.pdf|publisher=Heart Foundation|accessdate=12 January 2017|page=12|date=2016}}&lt;/ref&gt;

|-
! style="width:200px;"| ''' Category'''
! style="width:150px;"| '''[[systolic]], [[mmHg]]'''
! style="width:150px;"| '''[[diastolic]], mmHg'''
|-
| &lt;center&gt;[[Hypotension]]&lt;/center&gt;
| &lt;center&gt;&lt; 90&lt;/center&gt;
| &lt;center&gt;&lt; 60&lt;/center&gt;
|-
| &lt;center&gt;'''Normal'''&lt;/center&gt;
| &lt;center&gt;'''90–119'''&lt;ref name=AHA2017/&gt;&lt;/center&gt;&lt;br&gt;&lt;center&gt;'''90–129'''&lt;ref name=AU2016/&gt;&lt;/center&gt;
| &lt;center&gt;'''60–79'''&lt;ref name=AHA2017/&gt;&lt;/center&gt;&lt;br&gt;&lt;center&gt;'''60–84'''&lt;ref name=AU2016/&gt;&lt;/center&gt;
|-
| &lt;center&gt;[[Prehypertension]] (high normal)&lt;/center&gt;
| &lt;center&gt;120–129&lt;ref name=AHA2017/&gt;&lt;/center&gt;&lt;br&gt;&lt;center&gt;130–139&lt;ref name=AU2016/&gt;&lt;/center&gt;
| &lt;center&gt;60–79&lt;ref name=AHA2017/&gt;&lt;/center&gt;&lt;br&gt;&lt;center&gt;85–89&lt;ref name=AU2016/&gt;&lt;/center&gt;
|-
| &lt;center&gt;Stage 1 [[hypertension]]&lt;/center&gt;
| &lt;center&gt;140–159&lt;/center&gt;&lt;br&gt;&lt;center&gt;130–139&lt;ref name=AHA2017&gt;{{cite journal|last1=Whelton|first1=Paul K.|last2=Carey|first2=Robert M.|last3=Aronow|first3=Wilbert S.|last4=Casey|first4=Donald E.|last5=Collins|first5=Karen J.|last6=Dennison Himmelfarb|first6=Cheryl|last7=DePalma|first7=Sondra M.|last8=Gidding|first8=Samuel|last9=Jamerson|first9=Kenneth A.|last10=Jones|first10=Daniel W.|last11=MacLaughlin|first11=Eric J.|last12=Muntner|first12=Paul|last13=Ovbiagele|first13=Bruce|last14=Smith|first14=Sidney C.|last15=Spencer|first15=Crystal C.|last16=Stafford|first16=Randall S.|last17=Taler|first17=Sandra J.|last18=Thomas|first18=Randal J.|last19=Williams|first19=Kim A.|last20=Williamson|first20=Jeff D.|last21=Wright|first21=Jackson T.|title=2017 ACC/AHA/AAPA/ABC/ACPM/AGS/APhA/ASH/ASPC/NMA/PCNA Guideline for the Prevention, Detection, Evaluation, and Management of High Blood Pressure in Adults|journal=Hypertension|volume=71|issue=6|date=13 November 2017|pages=e13–e115|doi=10.1161/HYP.0000000000000065|pmid=29133356}}&lt;/ref&gt;&lt;/center&gt;
| &lt;center&gt; 90–99&lt;/center&gt;&lt;br&gt;&lt;center&gt;80–89&lt;ref name=AHA2017/&gt;&lt;/center&gt;
|-
| &lt;center&gt;Stage 2 hypertension&lt;/center&gt;
| &lt;center&gt;160–179&lt;/center&gt;&lt;br&gt;&lt;center&gt;&gt;140&lt;ref name=AHA2017/&gt;&lt;/center&gt;
| &lt;center&gt;100–109&lt;/center&gt;&lt;br&gt;&lt;center&gt;&gt;90&lt;ref name=AHA2017/&gt;&lt;/center&gt;
|-
| &lt;center&gt;[[Hypertensive emergency|Hypertensive urgency]]&lt;/center&gt;
| &lt;center&gt;≥ 180&lt;/center&gt;
| &lt;center&gt; ≥ 110&lt;/center&gt;
|-
| &lt;center&gt;[[Isolated systolic hypertension]]&lt;/center&gt;
| &lt;center&gt;≥ 160&lt;ref name=AHA2017/&gt;&lt;/center&gt;
| &lt;center&gt; &lt; 90&lt;/center&gt;
|}
The risk of cardiovascular disease increases progressively above 115/75&amp;nbsp;mmHg.&lt;ref&gt;{{cite journal|date=February 2006|title=Dietary approaches to prevent and treat hypertension: a scientific statement from the American Heart Association|journal=Hypertension|volume=47|issue=2|pages=296–308|doi=10.1161/01.HYP.0000202568.01167.B6|pmid=16434724|vauthors=Appel LJ, Brands MW, Daniels SR, Karanja N, Elmer PJ, Sacks FM|citeseerx=10.1.1.617.6244}}&lt;/ref&gt; In practice blood pressure is considered too low only if noticeable [[Hypotension#Signs and symptoms|symptoms]] are present.&lt;ref name="Mayo2009causes" /&gt;

Observational studies demonstrate that people who maintain arterial pressures at the low end of these pressure ranges have much better long term cardiovascular health. There is an ongoing medical debate over what is the optimal level of blood pressure to target when using drugs to lower blood pressure with hypertension, particularly in older people.&lt;ref&gt;{{Cite journal|last=Yusuf|first=Salim|last2=Lonn|first2=Eva|date=2016-11-01|title=The SPRINT and the HOPE-3 Trial in the Context of Other Blood Pressure-Lowering Trials|journal=JAMA Cardiology|volume=1|issue=8|pages=857–58|doi=10.1001/jamacardio.2016.2169|issn=2380-6591|pmid=27602555}}&lt;/ref&gt;

The table shows the classification of blood pressure adopted by the American Heart Association for adults who are 18 years and older.&lt;ref name="Heartorg"&gt;{{cite web |url=http://www.heart.org/HEARTORG/Conditions/HighBloodPressure/AboutHighBloodPressure/Understanding-Blood-Pressure-Readings_UCM_301764_Article.jsp |title=Understanding blood pressure readings
 |date=11 January 2011 |publisher=[[American Heart Association]] |accessdate=30 March 2011}}&lt;/ref&gt; It assumes the values are a result of averaging resting blood pressure readings measured at two or more visits to the doctor.&lt;ref name="Chobanian2003"&gt;{{cite journal |vauthors=Chobanian AV, Bakris GL, Black HR, Cushman WC, Green LA, Izzo JL, Jones DW, Materson BJ, Oparil S, Wright JT, Roccella EJ | title = Seventh report of the Joint National Committee on Prevention, Detection, Evaluation, and Treatment of High Blood Pressure | journal = Hypertension | volume = 42 | issue = 6 | pages = 1206–52 | date = December 2003 | pmid = 14656957 | doi = 10.1161/01.HYP.0000107251.49515.c2 | url = http://www.nhlbi.nih.gov/guidelines/hypertension/ }}&lt;/ref&gt;&lt;ref name="NHLBI2008"&gt;{{cite web |url=http://www.nhlbi.nih.gov/health/dci/Diseases/hyp/hyp_whatis.html |title=Diseases and conditions index – hypotension |accessdate=2008-09-16 |date=September 2008 |publisher=National Heart Lung and Blood Institute }}&lt;/ref&gt;

In November 2017 the American Heart Association announced revised definitions for blood pressure categories that increased the number of people considered to have high blood pressure.&lt;ref name="Heartorg2"&gt;{{cite web |url= https://news.heart.org/nearly-half-u-s-adults-now-classified-high-blood-pressure-new-definitions/ |title= Nearly half of US adults could now be classified with high blood pressure, under new definitions |date= 13 November 2017 |publisher=[[American Heart Association]] |accessdate=14 November 2017}}&lt;/ref&gt;

In the UK, clinic blood pressures are usually categorized into three groups; low (90/60 or lower), normal (between 90/60 and 139/89), and high (140/90 or higher).&lt;ref&gt;[http://www.nhs.uk/chq/pages/what-is-blood-pressure.aspx?categoryid=201&amp;subcategoryid=201 NHS choices: What is blood pressure?] Retrieved 2012-03-27&lt;/ref&gt;&lt;ref&gt;[http://www.nhs.uk/conditions/Blood-pressure-(high)/Pages/Introduction.aspx NHS choices: High blood pressure (hypertension)] Retrieved 2012-03-27&lt;/ref&gt;

Blood pressure fluctuates from minute to minute and normally shows a circadian rhythm over a 24-hour period, with highest readings in the early morning and evenings and lowest readings at night.&lt;ref&gt;{{Cite journal|last=van Berge-Landry|first=Helene M.|last2=Bovbjerg|first2=Dana H.|last3=James|first3=Gary D.|date=2008-10|title=Relationship between waking–sleep blood pressure and catecholamine changes in African–American and European–American women|url=https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2655229/table/T2/|journal=Blood Pressure Monitoring|volume=13|issue=5|pages=257–262|doi=10.1097/MBP.0b013e3283078f45|issn=1359-5237|pmc=2655229|pmid=18799950|quote=Table2: Comparison of ambulatory blood pressures and urinary norepinephrine and epinephrine excretion measured at work, home, and during sleep between European–American (n = 110) and African–American (n = 51) women|via=}}&lt;/ref&gt;&lt;ref&gt;{{cite journal | vauthors = van Berge-Landry HM, Bovbjerg DH, James GD | title = Relationship between waking-sleep blood pressure and catecholamine changes in African-American and European-American women | journal = Blood Press Monit | volume = 13 | issue = 5 | pages = 257–62 | date = October 2008 | pmid = 18799950 | pmc = 2655229 | doi = 10.1097/MBP.0b013e3283078f45 | id = NIHMS90092 }}&lt;/ref&gt; Loss of the normal fall in blood pressure at night is associated with a greater future risk of cardiovascular disease and there is evidence that night-time blood pressure is a stronger predictor of cardiovascular events than day-time blood pressure.&lt;ref name="HansenLi2010"&gt;{{cite journal|last1=Hansen|first1=T. W.|last2=Li|first2=Y.|last3=Boggia|first3=J.|last4=Thijs|first4=L.|last5=Richart|first5=T.|last6=Staessen|first6=J. A.|title=Predictive Role of the Nighttime Blood Pressure|journal=Hypertension|volume=57|issue=1|year=2010|pages=3–10|issn=0194-911X|doi=10.1161/HYPERTENSIONAHA.109.133900|pmid=21079049}}&lt;/ref&gt; Also, an individual's blood pressure varies with exercise, emotional reactions, sleep, digestion and time of day ([[circadian rhythm]]).

Various other factors, such as age and [[sex]], also influence a person's blood pressure. In children, the normal ranges are lower than for adults and depend on height.&lt;ref&gt;{{cite journal| title=Blood pressure tables for children and adolescents| authors=National Heart, Lung and Blood Institute| url=http://www.nhlbi.nih.gov/guidelines/hypertension/child_tbl.htm}} (Note that the median blood pressure is given by the 50th percentile and hypertension is defined by the [[Percentile|95th percentile]] for a given age, height, and sex.)&lt;/ref&gt; Reference blood pressure values have been developed for children in different countries, based on the distribution of blood pressure in children of these countries.&lt;ref&gt;{{cite journal | author = Chiolero A | title = The quest for blood pressure reference values in children | journal = Journal of Hypertension | volume = 32 | issue = 3 | pages = 477–79 | date = Mar 2014 | pmid = 24477093 | doi=10.1097/HJH.0000000000000109}}&lt;/ref&gt; As adults age, systolic pressure tends to rise and diastolic pressure tends to fall.&lt;ref name='Pickering2005p145a'&gt;{{harv|Pickering|Hall|Appel|Falkner|2005|p=145}} See ''Isolated Systolic Hypertension''.&lt;/ref&gt;&lt;!-- Other coauthors omitted since 4 author limit. See Template:Harv#Usage . --&gt; Consequently, in the elderly, systolic blood pressure often exceeds the normal adult range,&lt;ref name='Pickering2005p144'&gt;"...more than half of all Americans aged 65 or older have hypertension." {{harv|Pickering|Hall|Appel|Falkner|2005|p=144}}&lt;/ref&gt;&lt;!-- Other coauthors omitted since 4 author limit. See Template:Harv#Usage . --&gt; this is  thought to be due to increased [[Arterial stiffness|stiffness of the arteries]].&lt;ref&gt;{{Cite journal|last=Franklin|first=Stanley S.|date=2008-05-01|title=Beyond blood pressure: Arterial stiffness as a new biomarker of cardiovascular disease|journal=Journal of the American Society of Hypertension: JASH|volume=2|issue=3|pages=140–51|doi=10.1016/j.jash.2007.09.002|issn=1933-1711|pmid=20409896}}&lt;/ref&gt;

Differences between left and right arm blood pressure measurements tend to be small. However, occasionally there is a consistent difference greater than 10&amp;nbsp;mmHg which may need further investigation, e.g. for [[Cardiovascular disease|obstructive arterial disease]].&lt;ref&gt;{{cite journal |vauthors=Eguchi K, Yacoub M, Jhalani J, Gerin W, Schwartz JE, Pickering TG | title = Consistency of blood pressure differences between the left and right arms | journal = Arch Intern Med | volume = 167 | issue = 4 | pages = 388–93 | date = February 2007 | pmid = 17325301 | doi = 10.1001/archinte.167.4.388 | url = http://archinte.ama-assn.org/cgi/content/full/167/4/388 }}&lt;/ref&gt;&lt;ref&gt;{{cite journal |vauthors=Agarwal R, Bunaye Z, Bekele DM | title = Prognostic significance of between-arm blood pressure differences | journal = Hypertension | volume = 51 | issue = 3 | pages = 657–62 | date = March 2008 | pmid = 18212263 | doi = 10.1161/HYPERTENSIONAHA.107.104943 | url =  }}&lt;/ref&gt;

{|class="wikitable"
|+ [[Reference range]]s for blood pressure in children&lt;ref name=ucla&gt;[http://hr.uclahealth.org/workfiles/AgeSpecificSLM-Peds.pdf Pediatric Age Specific], p. 6. Revised 6/10. By Theresa Kirkpatrick and Kateri Tobias. UCLA Health System&lt;/ref&gt;
|-
! Stage !! Approximate age !! Systolic !! Diastolic
|-
! Infants
| 1 to 12 months || 75–100 || 50–70
|-
! Toddlers and preschoolers
| 1 to 5 years || 80–110 || 50–80
|-
! School age
| 6 to 12 years || 85–120 || 50–80
|-
! Adolescents
| 13 to 18 years || 95–140 || 60–90
|}

===Mean arterial pressure===
The [[mean arterial pressure]] (MAP) is the average over a [[cardiac cycle]] and is determined by the [[cardiac output]] (CO, [[systemic vascular resistance]] (SVR), and [[central venous pressure]] (CVP):&lt;ref name=KlabundeMAP2007&gt;{{cite web |url=http://www.cvphysiology.com/Blood%20Pressure/BP006.htm |title=Cardiovascular Physiology Concepts – Mean Arterial Pressure |accessdate=2008-09-29 |last=Klabunde |first=RE |year=2007 |deadurl=yes |archiveurl=https://www.webcitation.org/5kGLMdqnn?url=http://www.cvphysiology.com/Blood%20Pressure/BP006.htm |archivedate=2009-10-04 |df= }}&lt;/ref&gt;

:::::::::::&lt;math&gt;\! \text{MAP} = (\text{CO} \cdot \text{SVR}) + \text{CVP} &lt;/math&gt;

In practice the contribution of CVP (which is small) is generally ignored and so

:::::::::::&lt;math&gt;\! \text{MAP} = \text{CO} \cdot \text{SVR} &lt;/math&gt;

MAP can be estimated from measurements of the systolic pressure &lt;math&gt; \! P_{\text{sys}}&lt;/math&gt;&amp;nbsp; and the diastolic pressure &lt;math&gt; \! P_{\text{dias}}&lt;/math&gt;&amp;nbsp;&lt;ref name="KlabundeMAP2007" /&gt;
:::::::::::&lt;math&gt;\! \text{MAP} \approxeq P_{\text{dias}} + \frac{1}{3} (P_{\text{sys}} - P_{\text{dias}})&lt;/math&gt;

===Pulse pressure===
{{main|Pulse pressure}}
[[File:Arterial-blood-pressure-curve.svg|thumb|Curve of the arterial pressure during one cardiac cycle. The closing of the aortic valve causes the notch in the curve.]]
The [[pulse pressure]] is the difference between the measured systolic and diastolic pressures,&lt;ref name = KlabundePulse2007 /&gt;

:::::::::::&lt;math&gt;\! P_{\text{pulse}} = P_{\text{sys}} - P_{\text{dias}}.&lt;/math&gt;

The up and down fluctuation of the [[arterial]] pressure results from the pulsatile nature of the [[cardiac output]], i.e. the heartbeat. Pulse pressure is determined by the interaction of the [[stroke volume]] of the heart, the compliance (ability to expand) of the arterial system—largely attributable to the [[aorta]] and large elastic arteries—and the [[Drag (physics)|resistance]] to flow in the [[arterial tree]]. By expanding under pressure, the aorta absorbs some of the force of the blood surge from the heart during a heartbeat. In this way, the pulse pressure is reduced from what it would be if the aorta were not compliant.&lt;ref name=KlabundePulse2007&gt;{{cite web |url=http://www.cvphysiology.com/Blood%20Pressure/BP003.htm |title=Cardiovascular Physiology Concepts – Pulse Pressure |accessdate=2008-10-02 |last=Klabunde |first=RE |year=2007 |deadurl=yes |archiveurl=https://www.webcitation.org/5kGLuC47S?url=http://www.cvphysiology.com/Blood%20Pressure/BP003.htm |archivedate=2009-10-04 |df= }}&lt;/ref&gt; The loss of arterial compliance that occurs with aging explains the elevated pulse pressures found in elderly patients.

=== Systemic venous pressure ===
{{Non-systemic blood pressures}}
Blood pressure generally refers to the arterial pressure in the [[systemic circulation]]. However, measurement of pressures in the venous system and the [[Pulmonary circulation|pulmonary vessels]] plays an important role in [[intensive care medicine]] but requires invasive measurement of pressure using a [[catheter]].

Venous pressure is the vascular pressure in a [[vein]] or in the [[atrium (anatomy)|atria of the heart]]. It is much less than arterial pressure, with common values of 5&amp;nbsp;mmHg in the [[right atrium]] and 8&amp;nbsp;mmHg in the left atrium.

Variants of venous pressure include:
* [[Central venous pressure]], which is a good approximation of right atrial pressure,&lt;ref name="urlCentral Venous Catheter Physiology"&gt;{{cite web |url=http://www.healthsystem.virginia.edu/internet/anesthesiology-elective/cardiac/cvcphys.cfm |title=Central Venous Catheter Physiology |accessdate=2009-02-27 |deadurl=yes |archiveurl=https://web.archive.org/web/20080821165806/http://www.healthsystem.virginia.edu/internet/anesthesiology-elective/cardiac/cvcphys.cfm |archivedate=2008-08-21 |df= }}&lt;/ref&gt; which is a major determinant of right ventricular end diastolic volume. (However, there can be exceptions in some cases.)&lt;ref name="pmid12533747"&gt;{{cite journal |vauthors=Tkachenko BI, Evlakhov VI, Poyasov IZ | title = Independence of changes in right atrial pressure and central venous pressure | journal = Bull. Exp. Biol. Med. | volume = 134 | issue = 4 | pages = 318–20 | year = 2002 | pmid = 12533747 | doi = 10.1023/A:1021931508946 }}&lt;/ref&gt;
* The [[jugular venous pressure]] (JVP) is the indirectly observed pressure over the venous system. It can be useful in the differentiation of different forms of [[heart disease|heart]] and [[lung disease]].
* The [[portal venous pressure]] is the blood pressure in the [[portal vein]]. It is normally 5–10&amp;nbsp;mmHg&lt;ref&gt;{{cite web|url=http://www.emedicine.com/med/byname/esophageal-varices.htm |title=Esophageal Varices : Article Excerpt by: Samy A Azer |publisher=eMedicine |accessdate=2011-08-22}}&lt;/ref&gt;

=== Pulmonary pressure ===
{{Main|Pulmonary artery pressure}}
Normally, the pressure in the [[pulmonary artery]] is about 15&amp;nbsp;mmHg at rest.&lt;ref&gt;[http://www.nhlbi.nih.gov/health/dci/Diseases/pah/pah_what.html What Is Pulmonary Hypertension?] From Diseases and Conditions Index (DCI). National Heart, Lung, and Blood Institute. Last updated September 2008. Retrieved on 6 April 2009.&lt;/ref&gt;

Increased blood pressure in the capillaries of the lung causes [[pulmonary hypertension]], leading to interstitial [[edema]] if the pressure increases to above 20&amp;nbsp;mmHg, and to [[pulmonary edema]] at pressures above 25&amp;nbsp;mmHg.&lt;ref&gt;[https://books.google.com/books?id=IYFAsxAUA_MC&amp;printsec=frontcover#PPR3,M1 Chapter 41, p. 210 in: ''Cardiology secrets'']
By Olivia Vynn Adair
Edition: 2, illustrated
Published by Elsevier Health Sciences, 2001
{{ISBN|1-56053-420-6|978-1-56053-420-4}}&lt;/ref&gt;

== Disorders ==
Disorders of blood pressure control include [[Hypertension|high blood pressure]], [[Hypotension|low blood pressure]], and blood pressure that shows excessive or maladaptive fluctuation.

===High===
{{Main|Hypertension}}

[[File:Main complications of persistent high blood pressure.svg|thumb|right|300px|Overview of main complications of persistent high blood pressure]]

[[Arterial hypertension]] can be an indicator of other problems and may have long-term adverse effects. Sometimes it can be an acute problem, for example [[hypertensive emergency]].

Levels of arterial pressure put mechanical stress on the arterial walls. Higher pressures increase heart workload and progression of unhealthy tissue growth ([[atheroma]]) that develops within the walls of arteries. The higher the pressure, the more stress that is present and the more [[atheroma]] tend to progress and the [[Myocardium|heart muscle]] tends to thicken, enlarge and become weaker over time.

Persistent [[hypertension]] is one of the risk factors for [[stroke]]s, [[myocardial infarction|heart attacks]], [[heart failure]] and arterial aneurysms, and is the leading cause of [[chronic kidney failure]]. Even moderate elevation of arterial pressure leads to shortened [[life expectancy]]. At severely high pressures, mean arterial pressures 50% or more above average, a person can expect to live no more than a few years unless appropriately treated.&lt;ref&gt;Textbook of Medical Physiology, 7th Ed., Guyton &amp; Hall, Elsevier-Saunders, {{ISBN|0-7216-0240-1}}, p. 220.&lt;/ref&gt;

In the past, most attention was paid to [[diastolic]] pressure; but nowadays it is recognized that both high [[Systole (medicine)|systolic]] pressure and high [[pulse pressure]] (the numerical difference between systolic and diastolic pressures) are also risk factors. In some cases, it appears that a decrease in excessive diastolic pressure can actually increase risk, due probably to the increased difference between systolic and diastolic pressures (see the article on [[pulse pressure]]).  If systolic blood pressure is elevated (&gt;140&amp;nbsp;mmHg) with a normal diastolic blood pressure (&lt;90&amp;nbsp;mmHg), it is called "isolated systolic hypertension" and may present a health concern.&lt;ref name="urlIsolated systolic hypertension: A health concern? – MayoClinic.com"&gt;{{cite web |url=https://www.mayoclinic.org/diseases-conditions/high-blood-pressure/expert-answers/hypertension/faq-20058527 |title=Isolated systolic hypertension: A health concern? – MayoClinic.com |format= |accessdate=2018-01-25}}&lt;/ref&gt;&lt;ref name="urlThe Cleveland Clinic Center for Continuing Education"&gt;{{cite web|url=http://www.clevelandclinicmeded.com/medicalpubs/diseasemanagement/nephrology/isosystolic/isosystolic.htm |title=Clinical Management of Isolated Systolic Hypertension |accessdate=2011-12-07 |deadurl=yes |archiveurl=https://web.archive.org/web/20110929232613/http://www.clevelandclinicmeded.com/medicalpubs/diseasemanagement/nephrology/isosystolic/isosystolic.htm |archivedate=September 29, 2011 }}&lt;/ref&gt;

For those with [[heart valve]] regurgitation, a change in its severity may be associated with a change in diastolic pressure.  In a study of people with heart valve regurgitation that compared measurements 2 weeks apart for each person, there was an increased severity of [[aortic insufficiency|aortic]] and [[mitral regurgitation]] when diastolic blood pressure increased, whereas when diastolic blood pressure decreased, there was a decreased severity.&lt;ref name='Gottdiener2002'&gt;{{cite journal |vauthors=Gottdiener JS, Panza JA, St John Sutton M, Bannon P, Kushner H, Weissman NJ | title = Testing the test: The reliability of echocardiography in the sequential assessment of valvular regurgitation | journal = American Heart Journal | volume = 144 | issue = 1 | pages = 115–21 | date = July 2002 | pmid = 12094197 | doi = 10.1067/mhj.2002.123139 | url = http://www.medscape.com/viewarticle/439534 | accessdate = 2010-06-30 }}&lt;/ref&gt;

===Low===
{{Main|Hypotension}}
Blood pressure that is too low is known as [[hypotension]]. This is a medical concern if it causes signs or symptoms, such as dizziness, fainting, or in extreme cases, [[shock (circulatory)|shock]].&lt;ref name='NHLBI2008' /&gt;

When arterial pressure and blood [[rate of fluid flow|flow]] decrease beyond a certain point, the [[perfusion]] of the brain becomes critically decreased (i.e., the blood supply is not sufficient), causing lightheadedness, dizziness, weakness or fainting.&lt;ref&gt;{{vcite2 journal |vauthors=Franco Folino A |title=Cerebral autoregulation and syncope |journal=Prog Cardiovasc Dis |volume=50 |issue=1 |pages=49–80 |year=2007 |pmid=17631437 |doi=10.1016/j.pcad.2007.01.001 |url=}}&lt;/ref&gt;

Sometimes the arterial pressure drops significantly when a patient stands up from sitting. This is known as [[orthostatic hypotension]] (postural hypotension); gravity reduces the rate of blood return from the body veins below the heart back to the heart, thus reducing stroke volume and cardiac output.{{citation needed|date=August 2015}}

When people are healthy, the veins below their heart quickly constrict and the heart rate increases to minimize and compensate for the gravity effect. This is carried out involuntarily by the autonomic nervous system. The system usually requires a few seconds to fully adjust and if the compensations are too slow or inadequate, the individual will suffer reduced blood flow to the brain, dizziness and potential blackout. Increases in G-loading, such as routinely experienced by aerobatic or combat pilots '[[G-force|pulling Gs]]', greatly increases this effect. Repositioning the body horizontally largely eliminates the problem.{{citation needed|date=August 2015}}

Other causes of low arterial pressure include:{{citation needed|date=August 2015}}
* [[Sepsis]]
* [[Hemorrhage]] – blood loss
* [[Toxins]] including toxic doses of blood pressure medicine
* [[Hormone|Hormonal]] abnormalities, such as [[Addison's disease]]
* [[Eating disorder]]s, particularly [[anorexia nervosa]] and [[bulimia]]

[[shock (circulatory)|Shock]] is a complex condition which leads to critically decreased [[perfusion]]. The usual mechanisms are loss of blood volume, pooling of blood within the veins reducing adequate return to the heart and/or low effective heart pumping. Low arterial pressure, especially low pulse pressure, is a sign of shock and contributes to and reflects decreased perfusion.

If there is a significant difference in the pressure from one arm to the other, that may indicate a narrowing (for example, due to [[aortic coarctation]], [[aortic dissection]], [[thrombosis]] or [[embolism]]) of an [[artery]].{{citation needed|date=August 2015}}

===Fluctuating blood pressure===
Normal fluctuation in blood pressure is adaptive and necessary.  Fluctuations in pressure that are significantly greater than the norm are associated with greater [[white matter hyperintensity]], a finding consistent with reduced local cerebral blood flow&lt;ref&gt;{{cite journal |vauthors=Thomas AJ, Perry R, Barber R, Kalaria RN, O'Brien JT | title = Pathologies and Pathological Mechanisms for White Matter Hyperintensities in Depression | journal = Annals of the New York Academy of Sciences | volume = 977 | pages = 333–39 | year = 2002 | pmid = 12480770 | pmc =  | doi = 10.1111/j.1749-6632.2002.tb04835.x }}&lt;/ref&gt; and a heightened risk of [[cerebrovascular]] disease.&lt;ref name="ncbi.nlm.nih.gov"&gt;{{cite journal |vauthors=Brickman AM, Reitz C, Luchsinger JA, Manly JJ, Schupf N, Muraskin J, DeCarli C, Brown TR, Mayeux R | title = Long-term Blood Pressure Fluctuation and Cerebrovascular Disease in an Elderly Cohort | journal = Archives of Neurology | volume = 67 | issue = 5 | pages = 564–69 | year = 2010 | pmid = 20457955 | pmc = 2917204 | doi = 10.1001/archneurol.2010.70 }}&lt;/ref&gt;  Within both high and low blood pressure groups, a greater degree of fluctuation was found to correlate with an increase in cerebrovascular disease compared to those with less variability, suggesting the consideration of the clinical management of blood pressure fluctuations, even among [[normotensive]] older adults.&lt;ref name="ncbi.nlm.nih.gov"/&gt;  Older individuals and those who had received blood pressure medications were more likely to exhibit larger fluctuations in pressure.&lt;ref name="ncbi.nlm.nih.gov"/&gt;

== Physiology ==
[[File:Systolevs Diastole.png|thumb|Systole on the left and diastole on the right]]

During each heartbeat, blood pressure varies between a maximum (systolic) and a minimum (diastolic) pressure.&lt;ref&gt;{{cite web | url = http://healthlifeandstuff.com/2010/06/normal-blood-pressure-range-adults/ | title = Normal Blood Pressure Range Adults | publisher = Health and Life }}&lt;/ref&gt; The blood pressure in the circulation is principally due to the pumping action of the heart.&lt;ref name = 'Caro'&gt;{{cite book |author=Caro, Colin G. |title=The Mechanics of The Circulation|publisher=Oxford University Press |location=Oxford [Oxfordshire] |year=1978 |isbn=978-0-19-263323-1}}&lt;/ref&gt; Differences in mean blood pressure are responsible for blood flow from one location to another in the circulation. The rate of mean blood flow depends on both blood pressure and the resistance to flow presented by the blood vessels. Mean blood pressure decreases as the [[Circulatory system|circulating blood]] moves away from the [[heart]] through arteries and [[capillaries]] due to [[Viscosity|viscous]] losses of energy. Mean blood pressure drops over the whole circulation, although most of the fall occurs along the small arteries and [[arterioles]].&lt;ref name='Klabunde2005p93-4'&gt;{{cite book | last = Klabunde | first = Richard | title = Cardiovascular Physiology Concepts | publisher = Lippincott Williams &amp; Wilkins | year = 2005 | pages = 93–94 | isbn = 978-0-7817-5030-1 }}&lt;/ref&gt; Gravity affects blood pressure via [[Fluid statics|hydrostatic]] forces (e.g., during standing), and valves in veins, [[breathing]], and pumping from contraction of skeletal muscles also influence blood pressure in veins.&lt;ref name = 'Caro'/&gt;

=== Hemodynamics ===
{{main|Hemodynamics}}

Most influences on blood pressure can be understood in terms of their effect on cardiac output and resistance (the determinants of mean arterial pressure).&lt;ref&gt;[http://www.americanheart.org/presenter.jhtml?identifier=4650 ] {{webarchive |url=https://web.archive.org/web/20110415135713/http://www.americanheart.org/presenter.jhtml?identifier=4650 |date=April 15, 2011 }}&lt;/ref&gt;

Some factors are:
* [[Blood volume]]: the greater the blood volume, the higher the cardiac output. There is some relationship between dietary salt intake and increased blood volume, potentially resulting in higher arterial pressure, though this varies with the individual and is highly dependent on autonomic nervous system response and the [[renin–angiotensin system]].&lt;ref&gt;{{cite journal | author = Freis ED | title = Salt, volume and the prevention of hypertension | journal = Circulation | volume = 53 | issue = 4 | pages = 589–95 | year = 1976 | pmid = 767020 | doi = 10.1161/01.CIR.53.4.589 | url = http://circ.ahajournals.org/content/53/4/589.short | accessdate = 4 April 2012 }}&lt;/ref&gt;&lt;ref&gt;{{cite journal |vauthors=Caplea A, Seachrist D, Dunphy G, Ely D | title = Sodium-induced rise in blood pressure is suppressed by androgen receptor blockade | journal = AJP: Heart | volume = 280 | issue = 4 | pages = H1793–801 | date = April 2001 | pmid = 11247793 | url = http://ajpheart.physiology.org/content/280/4/H1793.abstract | series = 4 | accessdate = 4 April 2012 | doi = 10.1152/ajpheart.2001.280.4.H1793 }}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Houston|first=Mark|title=Sodium and Hypertension: A Review|journal=Archives of Internal Medicine|date=January 1986|volume=146|series=1|pages=179–85|url=http://archinte.ama-assn.org/cgi/content/abstract/146/1/179|accessdate=4 April 2012|issue=1|doi=10.1001/archinte.1986.00360130217028|pmid=3510595}}&lt;/ref&gt;
* Cardiac output: the pumping action of the heart is ultimately responsible for blood pressure. Increases or decreases in cardiac output can result in increases or decreases respectively in blood pressure.&lt;ref&gt;{{Cite journal|last=Guyton|first=A. C.|date=1981-12-01|title=The relationship of cardiac output and arterial pressure control|journal=Circulation|volume=64|issue=6|pages=1079–88|issn=0009-7322|pmid=6794930|doi=10.1161/01.cir.64.6.1079}}&lt;/ref&gt;
* Systemic vascular resistance: the higher the resistance to blood flow, the higher the arterial pressure upstream needs to be to maintain flow. In simple terms, resistance is related to vessel radius by the [[Hagen–Poiseuille equation|Hagen-Poiseuille's equation]]  (resistance∝1/radius&lt;sup&gt;4&lt;/sup&gt;, so the smaller the radius, the very much higher the resistance). Other physical factors that affect resistance include: vessel length (the longer the vessel, the higher the resistance), blood viscosity (the higher the viscosity, the higher the resistance)&lt;ref&gt;{{Cite journal|last=Lee|first=A. J.|date=1997-12-01|title=The role of rheological and haemostatic factors in hypertension|journal=Journal of Human Hypertension|volume=11|issue=12|pages=767–76|issn=0950-9240|pmid=9468002|doi=10.1038/sj.jhh.1000556}}&lt;/ref&gt; and the presence of an arterial [[stenosis]] (a narrow stenosis increases resistance to flow, however this increase in resistance rarely if ever increases systemic blood pressure, it decreases downstream flow).&lt;ref&gt;{{Cite journal|last=Coffman|first=J. D.|date=1988-12-01|title=Pathophysiology of obstructive arterial disease|journal=Herz|volume=13|issue=6|pages=343–50|issn=0340-9937|pmid=3061915}}&lt;/ref&gt; Substances called [[vasoconstrictor]]s can reduce the calibre of blood vessels, thereby increasing blood pressure. [[Vasodilator]]s (such as [[nitroglycerin]]) increase the calibre of blood vessels, thereby decreasing arterial pressure.

In practice, each individual's autonomic nervous system and other systems regulating blood pressure respond to and regulate all these factors so that, although the above issues are important, they rarely act in isolation and the actual arterial pressure response of a given individual can vary widely in the short and long term.

=== Regulation ===
{{main|Homeostasis}}
The [[endogenous]] regulation of arterial pressure is not completely understood, but the following mechanisms of regulating arterial pressure have been well-characterized:
* [[Baroreceptor reflex]]: [[Baroreceptor]]s in the [[high pressure receptor zones]] detect changes in arterial pressure. These baroreceptors send signals ultimately to the [[medulla oblongata|medulla of the brain stem]], specifically to the [[rostral ventrolateral medulla]] (RVLM). The medulla, by way of the [[autonomic nervous system]], adjusts the mean arterial pressure by altering both the force and speed of the heart's contractions, as well as the systemic vascular resistance. The most important arterial baroreceptors are located in the left and right [[carotid sinus]]es and in the [[aortic arch]].&lt;ref name='KlabundeArtBar2007'&gt;{{cite web|url=http://www.cvphysiology.com/Blood%20Pressure/BP012.htm |title=Cardiovascular Physiology Concepts – Arterial Baroreceptors |accessdate=2008-09-09 |last=Klabunde |first=RE |year=2007 }} [http://www.cvphysiology.com/Blood%20Pressure/BP012.htm Archived version 2009-10-03]&lt;/ref&gt;
* [[Renin–angiotensin system]] (RAS): This system is generally known for its long-term adjustment of arterial pressure. This system allows the [[kidney]] to compensate for loss in [[blood volume]] or drops in arterial pressure by activating an endogenous [[vasoconstrictor]] known as [[angiotensin II]].
* [[Aldosterone]] release: This [[steroid hormone]] is released from the [[adrenal cortex]] in response to angiotensin II or high serum [[potassium]] levels. Aldosterone stimulates [[sodium]] retention and potassium excretion by the kidneys. Since sodium is the main ion that determines the amount of fluid in the blood vessels by [[osmosis]], aldosterone will increase fluid retention, and indirectly, arterial pressure.
* [[Baroreceptor]]s in [[low pressure receptor zones]] (mainly in the [[venae cavae]] and the [[pulmonary veins]], and in the [[atrium (heart)|atria]]) result in feedback by regulating the secretion of [[antidiuretic hormone]] (ADH/Vasopressin), [[renin]] and [[aldosterone]]. The resultant increase in [[blood volume]] results in an increased [[cardiac output]] by the [[Frank–Starling law of the heart]], in turn increasing arterial blood pressure.

These different mechanisms are not necessarily independent of each other, as indicated by the link between the RAS and aldosterone release. When blood pressure falls many physiological cascades commence in order to return the blood pressure to a more appropriate level.

# The blood pressure fall is detected by a decrease in blood flow and thus a decrease in [[Glomerular filtration rate]] (GFR).
# Decrease in GFR is sensed as a decrease in Na&lt;sup&gt;+&lt;/sup&gt; levels by the [[macula densa]].
# The macula densa causes an increase in Na&lt;sup&gt;+&lt;/sup&gt; reabsorption, which causes water to follow in via [[osmosis]] and leads to an ultimate increase in [[blood plasma|plasma]] volume. Further, the macula densa releases adenosine which causes constriction of the afferent arterioles.
# At the same time,  the [[juxtaglomerular cells]] sense the decrease in blood pressure and release [[renin]].
# Renin converts [[angiotensinogen]] (inactive form) to [[angiotensin I]] (active form).
# Angiotensin I flows in the bloodstream until it reaches the capillaries of the lungs where [[angiotensin converting enzyme]] (ACE) acts on it to convert it into [[angiotensin II]].
# Angiotensin II is a vasoconstrictor which will increase bloodflow to the heart and subsequently the preload, ultimately increasing the [[cardiac output]].
# Angiotensin II also causes an increase in the release of [[aldosterone]] from the [[adrenal gland]]s.
# Aldosterone further increases the Na&lt;sup&gt;+&lt;/sup&gt; and H&lt;sub&gt;2&lt;/sub&gt;O reabsorption in the [[distal convoluted tubule]] of the [[nephron]].

Currently, the RAS is targeted pharmacologically by [[ACE inhibitor]]s and [[angiotensin II receptor antagonist]]s, also known as angiotensin receptor blockers (ARBs). The aldosterone system is directly targeted by [[spironolactone]], an [[aldosterone antagonist]]. The fluid retention may be targeted by [[diuretic]]s; the antihypertensive effect of diuretics is due to its effect on blood volume. Generally, the baroreceptor reflex is not targeted in [[hypertension]] because if blocked, individuals may suffer from [[orthostatic hypotension]] and [[fainting]].

== Measurement ==
[[File:Blood Pressure - Take Another Person.png|thumb|Taking blood pressure with a sphygmomanometer]]
{{Main|Blood pressure measurement}}
Arterial pressure is most commonly measured via a [[sphygmomanometer]], which uses the height of a column of mercury, or an aneroid gauge, to reflect the blood pressure by auscultation.&lt;ref name='Booth1977'&gt;{{cite journal | author = Booth J | title = A short history of blood pressure measurement | journal = Proceedings of the Royal Society of Medicine | volume = 70 | issue = 11 | pages = 793–99 | year = 1977 | pmid = 341169 | pmc = 1543468 }}&lt;/ref&gt; The most common automated blood pressure measurement technique is based on the so-called "oscillometric" method.&lt;ref name=For2015&gt;{{Cite journal|last=Forouzanfar|first=M.|last2=Dajani|first2=H. R.|last3=Groza|first3=V. Z.|last4=Bolic|first4=M.|last5=Rajan|first5=S.|last6=Batkin|first6=I.|date=2015-01-01|title=Oscillometric Blood Pressure Estimation: Past, Present, and Future|url=http://ieeexplore.ieee.org/document/7109154/|journal=IEEE Reviews in Biomedical Engineering|volume=8|pages=44–63|doi=10.1109/RBME.2015.2434215|pmid=25993705|issn=1937-3333}}&lt;/ref&gt; Blood pressure values are generally reported in [[millimetre of mercury|millimetres of mercury]] (mmHg), though aneroid and electronic devices do not contain [[mercury (element)|mercury]].

For each heartbeat, blood pressure varies between systolic and diastolic pressures. Systolic pressure is peak pressure in the arteries, which occurs near the end of the [[cardiac cycle]] when the [[Ventricle (heart)|ventricles]] are contracting. Diastolic pressure is minimum pressure in the arteries, which occurs near the beginning of the cardiac cycle when the ventricles are filled with blood. An example of normal measured values for a resting, healthy adult human is 120&amp;nbsp;mmHg [[Systole (medicine)|systolic]] and 80&amp;nbsp;mmHg [[diastolic]] (written as 120/80&amp;nbsp;mmHg, and spoken as "one-twenty over eighty").

Systolic and diastolic arterial blood pressures are not static but undergo natural variations from one heartbeat to another and throughout the day (in a [[circadian]] rhythm). They also change in response to [[stress (medicine)|stress]], nutritional factors, [[medication|drugs]], disease, exercise, and [[orthostatic hypotension|momentarily from standing up]]. Sometimes the variations are large. [[Hypertension]] refers to arterial pressure being abnormally high, as opposed to [[hypotension]], when it is abnormally low. Along with [[body temperature]], [[respiratory rate]], and [[pulse rate]], blood pressure is one of the four main vital signs routinely monitored by medical professionals and healthcare providers.&lt;ref name='OHSU'&gt;{{cite web|url=https://www.urmc.rochester.edu/Encyclopedia/Content.aspx?ContentTypeID=85&amp;ContentID=P00866 |title=Vital Signs (Body Temperature, Pulse Rate, Respiration Rate, Blood Pressure) |accessdate=2014-04-16 |website=OHSU Health Information |publisher= Oregon Health &amp; Science University }}&lt;/ref&gt;

Measuring pressure [[Invasive blood pressure|invasively]], by penetrating the arterial wall to take the measurement, is much less common and usually restricted to a hospital setting.

==Fetal blood pressure==
{{Further|Fetal circulation#Blood pressure}}
In [[pregnancy]], it is the fetal heart and not the mother's heart that builds up the fetal blood pressure to drive blood through the fetal circulation. The blood pressure in the fetal aorta is approximately 30&amp;nbsp;mmHg at 20 weeks of gestation, and increases to approximately 45&amp;nbsp;mmHg at 40 weeks of gestation.&lt;ref name=Struijk&gt;{{cite journal | vauthors = Struijk PC, Mathews VJ, Loupas T, Stewart PA, Clark EB, Steegers EA, Wladimiroff JW | title = Blood pressure estimation in the human fetal descending aorta | journal = Ultrasound Obstet Gynecol | volume = 32 | issue = 5 | pages = 673–81 | date = October 2008 | pmid = 18816497 | doi = 10.1002/uog.6137  }}&lt;/ref&gt;

The average blood pressure for full-term infants:&lt;ref name=SharonSmithMurray&gt;Sharon, S. M. &amp; Emily, S. M. (2006). ''Foundations of Maternal-Newborn Nursing.'' (4th ed p. 476). Philadelphia:Elsevier.&lt;/ref&gt;
* Systolic 65–95&amp;nbsp;mmHg
* Diastolic 30–60&amp;nbsp;mmHg

==Blood pressure in other animals==
Blood pressure in non-human mammals is similar to human blood pressure. In contrast, heart rate differs markedly, largely depending on the size of the animal (larger animals have slower heart rates).&lt;ref&gt;{{Cite book|url=https://www.worldcat.org/oclc/907295832|title=The design of mammals : a scaling approach|last=William,|first=Prothero, John|isbn=9781107110472|location=Cambridge|oclc=907295832|date=2015-10-22}}&lt;/ref&gt;  As in humans, blood pressure in animals differs by age, sex, time of day and circumstances:&lt;ref name=":1"&gt;{{Cite book|url=https://www.worldcat.org/oclc/432709394|title=Animal models in cardiovascular research|last=Gross|first=David R.|publisher=Springer|year=2009|isbn=9780387959627|edition= 3rd|location=Dordrecht|pages=5|oclc=432709394}}&lt;/ref&gt;&lt;ref name=":2"&gt;{{Cite journal|last=Brown|first=S.|last2=Atkins|first2=C.|last3=Bagley|first3=R.|last4=Carr|first4=A.|last5=Cowgill|first5=L.|last6=Davidson|first6=M.|last7=Egner|first7=B.|last8=Elliott|first8=J.|last9=Henik|first9=R.|date=2007|title=Guidelines for the identification, evaluation, and management of systemic hypertension in dogs and cats|journal=Journal of Veterinary Internal Medicine|volume=21|issue=3|pages=542–558|issn=0891-6640|pmid=17552466}}&lt;/ref&gt;  measurements made in laboratories or anesthesia may not be repesentative of values under free-living conditions. Rats, mice, dogs and rabbits have been used extensively to study the causes of high blood pressure.&lt;ref&gt;{{Cite journal|last=Lerman|first=Lilach O.|last2=Chade|first2=Alejandro R.|last3=Sica|first3=Vincenzo|last4=Napoli|first4=Claudio|date=2005|title=Animal models of hypertension: An overview|url=http://linkinghub.elsevier.com/retrieve/pii/S0022214305001605|journal=Journal of Laboratory and Clinical Medicine|volume=146|issue=3|pages=160–173|doi=10.1016/j.lab.2005.05.005|pmid=16131455|issn=0022-2143|via=}}&lt;/ref&gt;
{| class="wikitable"
|+Blood pressure and heart rate of various mammals (modifed from &lt;ref name=":1" /&gt;)
!Species
!Systolic blood pressure, 
mm Hg 
!Diastolic blood pressure, 
mm Hg
!Heart rate, 
beats per minute
|-
|Calves
|140
|70
|75–146
|-
|Cats
|155
|68
|100–259
|-
|Dogs
|183
|51
|62–170
|-
|Goats
|140
|90
|80–120
|-
|Guinea-pigs
|140
|90
|240–300
|-
|Mice
|120
|75
|580–680
|-
|Pigs
|169
|55
|74–116
|-
|Rabbits
|118
|67
|205–306
|-
|Rats
|153
|51
|305–500
|-
|Rhesus monkeys
|160
|125
|180–210
|-
|Sheep
|140
|80
|63–210
|}

===Hypertension in cats and dogs===
Hypertension in cats and dogs is diagnosed if the blood pressure is greater than 150&amp;nbsp;mm Hg (systolic) and/or 95&amp;nbsp;mm Hg (diastolic).&lt;ref name=":2" /&gt;

==References==
{{reflist}}

==Further reading==
* {{cite journal | vauthors = Pickering TG, Hall JE, Appel LJ, Falkner BE, Graves J, Hill MN, Jones DW, Kurtz T, Sheps SG, Roccella EJ | title = Recommendations for blood pressure measurement in humans and experimental animals: Part 1: blood pressure measurement in humans: a statement for professionals from the Subcommittee of Professional and Public Education of the American Heart Association Council on High Blood Pressure Research | journal = Hypertension | volume = 45 | issue = 5 | pages = 142–61 | year = 2005 | pmid = 15611362 | doi = 10.1161/01.HYP.0000150859.47929.8e | url = http://hyper.ahajournals.org/cgi/content/full/45/1/142 | ref = harv | format =  | display-authors = 3 | accessdate = 2009-10-01 | others = Subcommittee of Professional Public Education of the American Heart Association Council on High Blood Pressure Research }}

==External links==
{{commons category|Blood pressure}}
* [http://www.bloodpressureuk.org.uk Blood Pressure Association (UK)]
* [http://www.heart.org/HEARTORG/Conditions/HighBloodPressure/AboutHighBloodPressure/About-High-Blood-Pressure_UCM_002050_Article.jsp About High Blood Pressure], [[American Heart Association]]
* [http://pie.med.utoronto.ca/CA/CA_content/CA_cardiacPhys_intro.html Control of Blood Pressure], [[Toronto General Hospital]]
* [http://www.vaughns-1-pagers.com/medicine/blood-pressure.htm Blood Pressure Chart], Vaughn's Summaries

{{Cardiovascular physiology}}

{{Authority control}}

{{DEFAULTSORT:Blood Pressure}}
[[Category:Blood pressure| ]]
[[Category:Mathematics in medicine]]
[[Category:Articles containing video clips]]</text>
      <sha1>mxq2g1cpsmh7ocsnk7p06z9w3z6thdw</sha1>
    </revision>
  </page>
  <page>
    <title>Cofunction</title>
    <ns>0</ns>
    <id>3986044</id>
    <revision>
      <id>795473064</id>
      <parentid>795472778</parentid>
      <timestamp>2017-08-14T13:09:34Z</timestamp>
      <contributor>
        <username>Matthiaspaul</username>
        <id>13467261</id>
      </contributor>
      <comment>/* References */ improved refs</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6832">{{about|trigonometric functions|the computer program components|Coroutine}}
[[File:Sine cosine one period.svg|thumb|[[Sine]] and [[cosine]] are each other's cofunctions.]]
In [[mathematics]], a [[function (mathematics)|function]] ''f'' is '''cofunction''' of a function ''g'' if ''f''(''A'') = ''g''(''B'') whenever ''A'' and ''B'' are [[complementary angles]].&lt;ref name="Hall_1909"/&gt; This definition typically applies to [[trigonometric functions]].&lt;ref name="Aufmann_Nation_2014"/&gt;&lt;ref name="Bales_2012"/&gt; The prefix "co-" can be found already in [[Edmund Gunter]]'s ''Canon triangulorum'' (1620).&lt;ref name="Gunter_1620"/&gt;&lt;ref name="Roegel_2010"/&gt;

{{anchor|Identities}}For example, [[sine]] (Latin: ''sinus'') and [[cosine]] (Latin: ''cosinus'',&lt;ref name="Gunter_1620"/&gt;&lt;ref name="Roegel_2010"/&gt; ''sinus complementi''&lt;ref name="Gunter_1620"/&gt;&lt;ref name="Roegel_2010"/&gt;) are cofunctions of each other (hence the "co" in "cosine"):

{| class="wikitable"
|-
| {{nowrap|&lt;math&gt;\sin\left(\frac{\pi}{2} - A\right) = \cos(A)&lt;/math&gt;&lt;ref name="Hall_1909"/&gt;&lt;ref name="Bales_2012"/&gt;}}
| {{nowrap|&lt;math&gt;\cos\left(\frac{\pi}{2} - A\right) = \sin(A)&lt;/math&gt;&lt;ref name="Hall_1909"/&gt;&lt;ref name="Bales_2012"/&gt;}}
|-
|}

The same is true of [[secant (trigonometry)|secant]] (Latin: ''secans'') and [[cosecant]] (Latin: ''cosecans'', ''secans complementi'') as well as of [[tangent (trigonometry)|tangent]] (Latin: ''tangens'') and [[cotangent]] (Latin: ''cotangens'',&lt;ref name="Gunter_1620"/&gt;&lt;ref name="Roegel_2010"/&gt; ''tangens complementi''&lt;ref name="Gunter_1620"/&gt;&lt;ref name="Roegel_2010"/&gt;):

{| class="wikitable"
|-
| {{nowrap|&lt;math&gt;\sec\left(\frac{\pi}{2} - A\right) = \csc(A)&lt;/math&gt;&lt;ref name="Hall_1909"/&gt;&lt;ref name="Bales_2012"/&gt;}}
| {{nowrap|&lt;math&gt;\csc\left(\frac{\pi}{2} - A\right) = \sec(A)&lt;/math&gt;&lt;ref name="Hall_1909"/&gt;&lt;ref name="Bales_2012"/&gt;}}
|-
| {{nowrap|&lt;math&gt;\tan\left(\frac{\pi}{2} - A\right) = \cot(A)&lt;/math&gt;&lt;ref name="Hall_1909"/&gt;&lt;ref name="Bales_2012"/&gt;}}
| {{nowrap|&lt;math&gt;\cot\left(\frac{\pi}{2} - A\right) = \tan(A)&lt;/math&gt;&lt;ref name="Hall_1909"/&gt;&lt;ref name="Bales_2012"/&gt;}}
|-
|}

These equations are also known as the '''cofunction identities'''.&lt;ref name="Aufmann_Nation_2014"/&gt;&lt;ref name="Bales_2012"/&gt;

This also holds true for the [[versine]] (versed sine, ver) and [[coversine]] (coversed sine, cvs), the [[vercosine]] (versed cosine, vcs) and [[covercosine]] (coversed cosine, cvc), the [[haversine]] (half-versed sine, hav) and [[hacoversine]] (half-coversed sine, hcv), the [[havercosine]] (half-versed cosine, hvc) and [[hacovercosine]] (half-coversed cosine, hcc), as well as the [[exsecant]] (external secant, exs) and [[excosecant]] (external cosecant, exc):

{| class="wikitable"
|-
| {{nowrap|&lt;math&gt;\operatorname{ver}\left(\frac{\pi}{2} - A\right) = \operatorname{cvs}(A)&lt;/math&gt;&lt;ref name="Weisstein_covers"/&gt;}}
| {{nowrap|&lt;math&gt;\operatorname{cvs}\left(\frac{\pi}{2} - A\right) = \operatorname{ver}(A)&lt;/math&gt;}}
|-
| {{nowrap|&lt;math&gt;\operatorname{vcs}\left(\frac{\pi}{2} - A\right) = \operatorname{cvc}(A)&lt;/math&gt;&lt;ref name="Weisstein_covercos"/&gt;}}
| {{nowrap|&lt;math&gt;\operatorname{cvc}\left(\frac{\pi}{2} - A\right) = \operatorname{vcs}(A)&lt;/math&gt;}}
|-
| {{nowrap|&lt;math&gt;\operatorname{hav}\left(\frac{\pi}{2} - A\right) = \operatorname{hcv}(A)&lt;/math&gt;}}
| {{nowrap|&lt;math&gt;\operatorname{hcv}\left(\frac{\pi}{2} - A\right) = \operatorname{hav}(A)&lt;/math&gt;}}
|-
| {{nowrap|&lt;math&gt;\operatorname{hvc}\left(\frac{\pi}{2} - A\right) = \operatorname{hcc}(A)&lt;/math&gt;}}
| {{nowrap|&lt;math&gt;\operatorname{hcc}\left(\frac{\pi}{2} - A\right) = \operatorname{hvc}(A)&lt;/math&gt;}}
|-
| {{nowrap|&lt;math&gt;\operatorname{exs}\left(\frac{\pi}{2} - A\right) = \operatorname{exc}(A)&lt;/math&gt;}}
| {{nowrap|&lt;math&gt;\operatorname{exc}\left(\frac{\pi}{2} - A\right) = \operatorname{exs}(A)&lt;/math&gt;}}
|-
|}

==See also==
*[[Hyperbolic cosine]]
*[[Hyperbolic cosecant]]
*[[Hyperbolic cotangent]]
*[[Lemniscatic cosine]]
*[[Jacobi elliptic cosine]]
*[[Cologarithm]]
*[[Covariance]]
*[[List of trigonometric identities]]

==References==
{{reflist|refs=
&lt;ref name="Aufmann_Nation_2014"&gt;{{cite book |title=Algebra and Trigonometry |author-first1=Richard |author-last1=Aufmann |author-first2=Richard |author-last2=Nation |edition=8 |publisher=[[Cengage Learning]] |year=2014 |isbn=978-128596583-3 |page=528 |url=https://books.google.com/books?id=JEDAAgAAQBAJ&amp;pg=PA528 |access-date=2017-07-28}}&lt;/ref&gt;
&lt;ref name="Gunter_1620"&gt;{{cite book |author-first=Edmund |author-last=Gunter |author-link=Edmund Gunter |title=Canon triangulorum |date=1620}}&lt;/ref&gt;
&lt;ref name="Roegel_2010"&gt;{{cite web |title=A reconstruction of Gunter's Canon triangulorum (1620) |editor-first=Denis |editor-last=Roegel |type=Research report |publisher=HAL |date=2010-12-06 |id=inria-00543938 |url=https://hal.inria.fr/inria-00543938/document |access-date=2017-07-28 |dead-url=no |archive-url=https://web.archive.org/web/20170728192238/https://hal.inria.fr/inria-00543938/document |archive-date=2017-07-28}}&lt;/ref&gt;
&lt;ref name="Bales_2012"&gt;{{cite web |title=5.1 The Elementary Identities |work=Precalculus |author-first=John W. |author-last=Bales |date=2012 |orig-year=2001 |url=http://jwbales.home.mindspring.com/precal/part5/part5.1.html |access-date=2017-07-30 |dead-url=no |archive-url=https://web.archive.org/web/20170730201433/http://jwbales.home.mindspring.com/precal/part5/part5.1.html |archive-date=2017-07-30}}&lt;/ref&gt;
&lt;ref name="Hall_1909"&gt;{{cite book |title=Trigonometry |volume=Part I: Plane Trigonometry |author-first1=Arthur Graham |author-last1=Hall |author-first2=Fred Goodrich |author-last2=Frink |date=January 1909 |location=Ann Arbor, Michigan, USA |chapter=Chapter II. The Acute Angle [10] Functions of complementary angles |publisher=[[Henry Holt and Company]] / Norwood Press / J. S. Cushing Co. - Berwick &amp; Smith Co., Norwood, Massachusetts, USA |publication-place=New York, USA |pages=11-12 |url=https://archive.org/stream/planetrigonometr00hallrich#page/n26/mode/1up |access-date=2017-08-12 |dead-url=no}}&lt;/ref&gt;
&lt;ref name="Weisstein_covers"&gt;{{cite web |author-first=Eric Wolfgang |author-last=Weisstein |author-link=Eric Wolfgang Weisstein |title=Coversine |work=[[MathWorld]] |publisher=[[Wolfram Research, Inc.]] |url=http://mathworld.wolfram.com/Coversine.html |access-date=2015-11-06 |dead-url=no |archive-url=https://web.archive.org/web/20051127184403/http://mathworld.wolfram.com/Coversine.html |archive-date=2005-11-27}}&lt;/ref&gt;
&lt;ref name="Weisstein_covercos"&gt;{{cite web |author-first=Eric Wolfgang |author-last=Weisstein |author-link=Eric Wolfgang Weisstein |title=Covercosine |work=[[MathWorld]] |publisher=[[Wolfram Research, Inc.]] |url=http://mathworld.wolfram.com/Covercosine.html |access-date=2015-11-06 |dead-url=no |archive-url=https://web.archive.org/web/20140328110051/http://mathworld.wolfram.com/Covercosine.html |archive-date=2014-03-28}}&lt;/ref&gt;
}}

[[Category:Trigonometry]]</text>
      <sha1>i21j6uw3tvivsuq94xm5gdhlup5q1ru</sha1>
    </revision>
  </page>
  <page>
    <title>Computation in the limit</title>
    <ns>0</ns>
    <id>3086963</id>
    <revision>
      <id>769632523</id>
      <parentid>755286937</parentid>
      <timestamp>2017-03-10T18:59:25Z</timestamp>
      <contributor>
        <ip>24.85.232.10</ip>
      </contributor>
      <comment>/* Formal definition */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6025">In [[computability theory]], a function is called '''limit computable''' if it is the limit of a uniformly computable sequence of functions.  The terms '''computable in the limit''', '''limit recursive''' and '''recursively approximable''' are also used.  One can think of limit computable functions as those admitting an eventually correct computable guessing procedure at their true value.  A set is limit computable just when its [[Indicator function|characteristic function]] is limit computable.

If the sequence is uniformly computable relative to ''D'', then the function is '''limit computable in ''D'''''.

== Formal definition ==

A [[total function]] &lt;math&gt;r(x)&lt;/math&gt; is limit computable if there is a [[total computable function]] &lt;math&gt;\hat{r}(x,s)&lt;/math&gt; such that
: &lt;math&gt;\displaystyle r(x) = \lim_{s\to\infty} \hat{r}(x,s)&lt;/math&gt;

The total function &lt;math&gt;r(x)&lt;/math&gt; is limit computable in ''D'' if there is a total function &lt;math&gt;\hat{r}(x,s)&lt;/math&gt; [[Computable#Relative Computability|computable in ''D'']] also satisfying
: &lt;math&gt;\displaystyle r(x) = \lim_{s\to\infty} \hat{r}(x,s)&lt;/math&gt;

A set of [[natural number]]s is defined to be computable in the limit if and only if its [[Indicator function|characteristic function]] is computable in the limit.  In contrast, the set is [[computable set|computable]] if and only if it is computable in the limit by a function &lt;math&gt;\phi(t,i)&lt;/math&gt; and there is a second computable function that takes input ''i'' and returns a value of ''t'' large enough that the &lt;math&gt;\phi(t,i)&lt;/math&gt; has stabilized.

== Limit lemma ==

The '''limit lemma''' states that a set of natural numbers is limit computable if and only if the set is computable from &lt;math&gt;0'&lt;/math&gt; (the [[Turing jump]] of the empty set).  The relativized limit lemma states that a set is limit computable in &lt;math&gt;D&lt;/math&gt; if and only if it is computable from &lt;math&gt;D'&lt;/math&gt;.
Moreover, the limit lemma (and its relativization) hold uniformly.  Thus one can go from an index for the function &lt;math&gt;\hat{r}(x,s)&lt;/math&gt; to an index for &lt;math&gt;\hat{r}(x)&lt;/math&gt; relative to &lt;math&gt;0'&lt;/math&gt;.  One can also go from an index for &lt;math&gt;\hat{r}(x)&lt;/math&gt; relative to &lt;math&gt;0'&lt;/math&gt; to an index for some &lt;math&gt;\hat{r}(x,s)&lt;/math&gt; that has limit &lt;math&gt;\hat{r}(x)&lt;/math&gt;.

=== Proof ===
As &lt;math&gt;0'&lt;/math&gt; is a [computably enumerable] set, it must be computable in the limit itself as the computable function can be defined
: &lt;math&gt;\displaystyle \hat{r}(x,s)=\begin{cases}
1 &amp; \text{if by stage } s, x \text{ has been enumerated into } 0'\\
0 &amp; \text{if not}
\end{cases}&lt;/math&gt;
whose limit &lt;math&gt;r(x)&lt;/math&gt; as &lt;math&gt;s&lt;/math&gt; goes to infinity is the characteristic function of &lt;math&gt;0'&lt;/math&gt;.

It therefore suffices to show that if limit computability is preserved by [[Turing reduction]], as this will show that all sets computable from &lt;math&gt;0'&lt;/math&gt; are limit computable.  Fix sets &lt;math&gt;X,Y&lt;/math&gt; which are identified with their characteristic functions and a computable function &lt;math&gt;X_s&lt;/math&gt; with limit &lt;math&gt;X&lt;/math&gt;.  Suppose that &lt;math&gt;Y(z)=\phi^{X}(z)&lt;/math&gt; for some Turing reduction &lt;math&gt;\phi&lt;/math&gt; and define a computable function &lt;math&gt;Y_s&lt;/math&gt; as follows

: &lt;math&gt;\displaystyle Y_s(z)=\begin{cases}
                                      \phi^{X_s}(z)  &amp; \text{if } \phi^{X_s} \text{ converges in at most } s \text{ steps.}\\
                                       0 &amp; \text{otherwise }
                                    \end{cases} &lt;/math&gt;
Now suppose that the computation &lt;math&gt;\phi^{X}(z)&lt;/math&gt; converges in &lt;math&gt;s&lt;/math&gt; steps and only looks at the first &lt;math&gt;s&lt;/math&gt; bits of &lt;math&gt;X&lt;/math&gt;.  Now pick &lt;math&gt;s'&gt;s&lt;/math&gt; such that for all &lt;math&gt;z &lt; s+1&lt;/math&gt; &lt;math&gt;X_{s'}(z)=X(z)&lt;/math&gt;.  If &lt;math&gt;t &gt; s'&lt;/math&gt; then the computation &lt;math&gt;\phi^{X_t}(z)&lt;/math&gt; converges in at most &lt;math&gt;s' &lt; t&lt;/math&gt; steps to &lt;math&gt;\phi^{X}(z)&lt;/math&gt;.  Hence &lt;math&gt;Y_s(z)&lt;/math&gt; has a limit of &lt;math&gt;\phi^{X}(z)=Y(z)&lt;/math&gt;, so &lt;math&gt;Y&lt;/math&gt; is limit computable.

As the &lt;math&gt;\Delta^0_2&lt;/math&gt; sets are just the sets computable from &lt;math&gt;0'&lt;/math&gt; by [[Post's theorem]], the limit lemma also entails that the limit computable sets are the &lt;math&gt;\Delta^0_2&lt;/math&gt; sets.

== Limit computable real numbers ==

A [[real number]] ''x'' is '''computable in the limit''' if there is a computable sequence &lt;math&gt;r_i&lt;/math&gt; of [[rational numbers]] (or, which is equivalent, [[computable numbers|computable real numbers]]) which converges to ''x''.   In contrast, a real number is [[computable]] if and only if there is a sequence of rational numbers which converges to it and which has a computable [[modulus of convergence]].

When a real number is viewed as a sequence of bits, the following equivalent definition holds.  An infinite sequence &lt;math&gt;\omega&lt;/math&gt; of binary digits is computable in the limit if and only if there is a total computable function &lt;math&gt;\phi(t,i)&lt;/math&gt; taking values in the set &lt;math&gt;\{0,1\}&lt;/math&gt; such that for each ''i'' the limit &lt;math&gt;\lim_{t \rightarrow \infty} \phi(t,i)&lt;/math&gt; exists and equals &lt;math&gt;\omega(i)&lt;/math&gt;.  Thus for each ''i'', as ''t'' increases the value of &lt;math&gt;\phi(t,i)&lt;/math&gt; eventually becomes constant and equals &lt;math&gt;\omega(i)&lt;/math&gt;.  As with the case of computable real numbers, it is not  possible to effectively move between the two representations of limit computable reals.

== Examples ==

* The real whose binary expansion encodes the [[halting problem]] is computable in the limit but not computable.
* The real whose binary expansion encodes the truth set of [[first-order arithmetic]] is not computable in the limit.

== See also ==
* [[Specker sequence]]

== References ==

# J. Schmidhuber, "Hierarchies of generalized Kolmogorov complexities and nonenumerable universal measures computable in the limit", ''International Journal of Foundations of Computer Science'', 2002.
# R. Soare. ''Recursively Enumerable Sets and Degrees''. Springer-Verlag 1987.

[[Category:Computability theory]]
[[Category:Theory of computation]]</text>
      <sha1>9whbr6eudd5x6duz1bkcepwdld6uk4i</sha1>
    </revision>
  </page>
  <page>
    <title>Counting board</title>
    <ns>0</ns>
    <id>17910805</id>
    <revision>
      <id>795354182</id>
      <parentid>752480045</parentid>
      <timestamp>2017-08-13T18:21:33Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.5beta)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1595">[[Image:Rechentisch.png|thumb|Rechentisch/Counting board (engraving probably from [[Strasbourg]])]]

The '''counting board''' is the precursor of the [[abacus]], and the earliest known form of a counting device (excluding fingers and other very simple methods). Counting boards were made of stone or wood, and the counting was done on the board with beads, or pebbles etc. Not many boards survive because of the perishable materials used in their construction. 

The oldest known counting board, the [[Salamis Tablet]] (''c.'' 300 BC) was discovered on the Greek island of [[Salamis Island|Salamis]] in 1899.&lt;ref&gt;{{cite web |url=http://users.ju.edu/ssundbe/salamis.html |title=Archived copy |accessdate=2008-04-01 |deadurl=yes |archiveurl=https://web.archive.org/web/20080103213141/http://users.ju.edu/ssundbe/salamis.html |archivedate=2008-01-03 |df= }}&lt;/ref&gt;  It is thought to have been used by the Babylonians in about 300 BC and is more of a gaming board than a calculating device. It is marble, about 150 x 75 x 4.5&amp;nbsp;cm, and is in the [[Greek National museum]] in [[Athens]].  It has carved Greek letters and parallel grooves. 

The German mathematicican [[Adam Ries]] described the use of counting boards in ''Rechenbuch auf Linien und Ziphren in allerlei Handthierung / geschäfften und Kaufmanschafft''. In the novel ''[[Wolf Hall]]'', [[Hilary Mantel]] refers to [[Thomas Cromwell]] using a counting board in 16th-century England.

==References==
{{reflist}}

==See also==
* [[Abacus]]
* [[Calculator]]

{{DEFAULTSORT:Counting Board}}
[[Category:Mathematical tools]]


{{math-stub}}</text>
      <sha1>fe2w4zvp6bbibqxjppl91hmmus7qdv9</sha1>
    </revision>
  </page>
  <page>
    <title>Difference set</title>
    <ns>0</ns>
    <id>2992310</id>
    <revision>
      <id>864442164</id>
      <parentid>864442126</parentid>
      <timestamp>2018-10-17T06:55:45Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>nvm, wrong diff set</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14351">:''For the set of elements in one set but not another, see [[relative complement]]. For the set of differences of pairs of elements, see [[Minkowski difference]].''
In [[combinatorics]], a &lt;math&gt;(v,k,\lambda)&lt;/math&gt; '''difference set''' is a [[subset]] &lt;math&gt;D&lt;/math&gt; of [[cardinality|size]] &lt;math&gt;k&lt;/math&gt; of a [[group (mathematics)|group]] &lt;math&gt;G&lt;/math&gt; of [[order of a group|order]] &lt;math&gt;v&lt;/math&gt; such that every nonidentity element of &lt;math&gt;G&lt;/math&gt; can be expressed as a product &lt;math&gt;d_1d_2^{-1}&lt;/math&gt; of elements of &lt;math&gt;D&lt;/math&gt; in exactly &lt;math&gt;\lambda&lt;/math&gt; ways. A difference set &lt;math&gt;D&lt;/math&gt; is said to be ''cyclic'', ''abelian'', ''non-abelian'', etc., if the group &lt;math&gt;G&lt;/math&gt; has the corresponding property. A difference set with &lt;math&gt;\lambda = 1&lt;/math&gt; is sometimes called ''planar'' or ''simple''.&lt;ref&gt;{{harvnb|van Lint|Wilson|1992|loc=p. 331}}&lt;/ref&gt; If &lt;math&gt;G&lt;/math&gt; is an [[abelian group]] written in additive notation, the defining condition is that every nonzero element of &lt;math&gt;G&lt;/math&gt; can be written as a ''difference'' of elements of &lt;math&gt;D&lt;/math&gt; in exactly &lt;math&gt;\lambda&lt;/math&gt; ways. The term "difference set" arises in this way.

==Basic facts==
* A simple counting argument shows that there are exactly &lt;math&gt;k^2-k&lt;/math&gt; pairs of elements from &lt;math&gt;D&lt;/math&gt; that will yield nonidentity elements, so every difference set must satisfy the equation &lt;math&gt;k^2-k=(v-1)\lambda&lt;/math&gt;.
* If &lt;math&gt;D&lt;/math&gt; is a difference set, and &lt;math&gt;g\in G&lt;/math&gt;, then &lt;math&gt;gD=\{gd:d\in D\}&lt;/math&gt; is also a difference set, and is called a '''translate''' of &lt;math&gt;D&lt;/math&gt; (&lt;math&gt;D + g&lt;/math&gt; in additive notation).
* The complement of a &lt;math&gt;(v,k,\lambda)&lt;/math&gt;-difference set is a &lt;math&gt;(v,v-k,v-2k+\lambda)&lt;/math&gt;-difference set.&lt;ref&gt;{{harvnb|Wallis|1988|loc=p. 61 - Theorem 4.5}}&lt;/ref&gt;
* The set of all translates of a difference set &lt;math&gt;D&lt;/math&gt; forms a [[Block design#Symmetric BIBDs|symmetric block design]], called the ''development'' of &lt;math&gt;D&lt;/math&gt; and denoted by &lt;math&gt;dev(D)&lt;/math&gt;. In such a design there are &lt;math&gt;v&lt;/math&gt; ''elements'' (usually called points) and &lt;math&gt;v&lt;/math&gt; ''blocks'' (subsets). Each block of the design consists of &lt;math&gt;k&lt;/math&gt; points, each point is contained in &lt;math&gt;k&lt;/math&gt; blocks. Any two blocks have exactly &lt;math&gt;\lambda&lt;/math&gt; elements in common and any two points are simultaneously contained in exactly &lt;math&gt;\lambda&lt;/math&gt; blocks. The group &lt;math&gt;G&lt;/math&gt;  acts as an automorphism group of the design. It is sharply transitive on both points and blocks.&lt;ref&gt;{{harvnb|van Lint|Wilson|1992|loc=p. 331 - Theorem 27.2}}. The theorem only states point transitivity, but block transitivity follows from this by the second corollary on p. 330.&lt;/ref&gt; 
** In particular, if &lt;math&gt;\lambda=1&lt;/math&gt;, then the difference set gives rise to a [[projective plane]]. An example of a (7,3,1) difference set in the group &lt;math&gt;\mathbb{Z}/7\mathbb{Z}&lt;/math&gt; is the subset &lt;math&gt;\{1,2,4\}&lt;/math&gt;. The translates of this difference set form the [[Fano plane]].
* Since every difference set gives a [[Block design#Symmetric BIBDs|symmetric design]], the parameter set must satisfy the [[Bruck–Ryser–Chowla theorem]].&lt;ref&gt;{{harvnb|Colbourn|Dinitz|2007|loc=p. 420  (18.7 Remark 2)}}&lt;/ref&gt;
* Not every [[Block design#Symmetric BIBDs|symmetric design]] gives a difference set.&lt;ref&gt;{{harvnb|Colbourn|Dinitz|2007|loc=p. 420 (18.7 Remark 1)}}&lt;/ref&gt;

==Equivalent and isomorphic difference sets==
Two difference sets &lt;math&gt;D_1&lt;/math&gt; in group &lt;math&gt;G_1&lt;/math&gt; and &lt;math&gt;D_2&lt;/math&gt; in group &lt;math&gt;G_2&lt;/math&gt; are '''equivalent''' if there is a [[group isomorphism]] &lt;math&gt;\psi&lt;/math&gt; between &lt;math&gt;G_1&lt;/math&gt; and &lt;math&gt;G_2&lt;/math&gt; such that &lt;math&gt;D_1^{\psi} = \{d^{\psi}\colon d \in D_1 \} = g D_2&lt;/math&gt; for some &lt;math&gt;g \in G_2&lt;/math&gt;. The two difference sets are '''isomorphic''' if the designs &lt;math&gt;dev(D_1)&lt;/math&gt; and &lt;math&gt;dev(D_2)&lt;/math&gt; are isomorphic as block designs.

Equivalent difference sets are isomorphic, but there exist examples of isomorphic difference sets which are not equivalent.  In the cyclic difference set case, all known isomorphic difference sets are equivalent.&lt;ref&gt;{{harvnb|Colbourn|Diniz|2007|loc=p. 420 (Remark 18.9)}}&lt;/ref&gt;

==Multipliers==
A '''multiplier''' of a difference set &lt;math&gt;D&lt;/math&gt; in group &lt;math&gt;G&lt;/math&gt; is a [[group automorphism]] &lt;math&gt;\phi&lt;/math&gt; of &lt;math&gt;G&lt;/math&gt; such that &lt;math&gt;D^{\phi} = gD&lt;/math&gt; for some &lt;math&gt;g \in G&lt;/math&gt;. If &lt;math&gt;G&lt;/math&gt; is abelian and &lt;math&gt;\phi&lt;/math&gt; is the automorphism that maps &lt;math&gt;h \mapsto h^t&lt;/math&gt;, then &lt;math&gt;t&lt;/math&gt; is called a ''numerical'' or ''Hall'' '''multiplier'''.&lt;ref&gt;{{harvnb|van Lint|Wilson|1992|loc=p. 345}}&lt;/ref&gt;

It has been conjectured that if ''p'' is a prime dividing &lt;math&gt;k-\lambda&lt;/math&gt; and not dividing ''v'', then the group automorphism defined by &lt;math&gt;g\mapsto g^p&lt;/math&gt; fixes some translate of ''D'' (this is equivalent to being a multiplier). It is known to be true for &lt;math&gt;p&gt;\lambda&lt;/math&gt; when &lt;math&gt;G&lt;/math&gt; is an abelian group, and this is known as the First Multiplier Theorem. A more general known result, the Second Multiplier Theorem, says that if &lt;math&gt;D&lt;/math&gt; is a &lt;math&gt;(v,k,\lambda)&lt;/math&gt;-difference set in an abelian group &lt;math&gt;G&lt;/math&gt; of exponent &lt;math&gt;v^*&lt;/math&gt; (the [[least common multiple]] of the orders of every element), let &lt;math&gt;t&lt;/math&gt; be an integer coprime to &lt;math&gt;v&lt;/math&gt;. If there exists a divisor &lt;math&gt;m&gt;\lambda&lt;/math&gt; of &lt;math&gt;k-\lambda&lt;/math&gt; such that for every prime ''p'' dividing ''m'', there exists an integer ''i'' with &lt;math&gt;t\equiv p^i\ \pmod{v^*}&lt;/math&gt;, then ''t'' is a numerical divisor.&lt;ref&gt;{{harvnb|van Lint|Wilson|1992|loc=p. 349 (Theorem 28.7)}}&lt;/ref&gt;

For example, 2 is a multiplier of the (7,3,1)-difference set mentioned above.

It has been mentioned that a numerical multiplier of a difference set &lt;math&gt;D&lt;/math&gt; in an abelian group &lt;math&gt;G&lt;/math&gt; fixes a translate of &lt;math&gt;D&lt;/math&gt;, but it can also be shown that there is a translate of &lt;math&gt;D&lt;/math&gt; which is fixed by all numerical multipliers of &lt;math&gt;D&lt;/math&gt;.&lt;ref&gt;{{harvnb|Beth|Jungnickel|Lenz|1986|loc=p. 280 (Theorem 4.6)}}&lt;/ref&gt;

==Parameters==
The known difference sets or their complements have one of the following parameter sets:&lt;ref&gt;{{harvnb|Colburn|Dinitz|2007|loc=pp. 422-425}}&lt;/ref&gt;

*&lt;math&gt;((q^{n+2}-1)/(q-1), (q^{n+1}-1)/(q-1), (q^n-1)/(q-1))&lt;/math&gt;-difference set for some prime power &lt;math&gt;q&lt;/math&gt; and some positive integer &lt;math&gt;n&lt;/math&gt;. These are known as the ''classical parameters'' and there are many constructions of difference sets having these parameters.
*&lt;math&gt;(4n-1,2n-1,n-1)&lt;/math&gt;-difference set for some positive integer &lt;math&gt;n&lt;/math&gt;. Difference sets with ''v'' = 4n - 1 are called ''Paley-type difference sets''.
*&lt;math&gt;(4n^2,2n^2-n,n^2-n)&lt;/math&gt;-difference set for some positive integer &lt;math&gt;n&lt;/math&gt;. A difference set with these parameters is a ''Hadamard difference set''.
*&lt;math&gt;(q^{n+1}(1+(q^{n+1}-1)/(q-1)),q^n(q^{n+1}-1)/(q-1),q^n(q^n-1)/(q-1))&lt;/math&gt;-difference set for some prime power &lt;math&gt;q&lt;/math&gt; and some positive integer &lt;math&gt;n&lt;/math&gt;. Known as the ''McFarland parameters''.
*&lt;math&gt;(3^{n+1}(3^{n+1}-1)/2,3^n(3^{n+1}+1)/2,3^n(3^n+1)/2)&lt;/math&gt;-difference set for some positive integer &lt;math&gt;n&lt;/math&gt;. Known as the ''Spence parameters''.
*&lt;math&gt;(4q^{2n}(q^{2n}-1)/(q-1),q^{2n-1}(1+2(q^{2n}-1)/(q+1)),q^{2n-1}(q^{2n-1}+1)(q-1)/(q+1))&lt;/math&gt;-difference set for some prime power &lt;math&gt;q&lt;/math&gt; and some positive integer &lt;math&gt;n&lt;/math&gt;. Difference sets with these parameters are called ''Davis-Jedwab-Chen difference sets''.

==Known difference sets==
In many constructions of difference sets the groups that are used are related to the additive and multiplicative groups of finite fields. The notation used to denote these fields differs according to discipline. In this section, &lt;math&gt;{\rm GF}(q)&lt;/math&gt; is the [[Galois field]] of order &lt;math&gt;q&lt;/math&gt;, where &lt;math&gt;q&lt;/math&gt; is a prime or prime power. The group under addition is denoted by &lt;math&gt;G = ({\rm GF}(q), +)&lt;/math&gt;, while &lt;math&gt;{\rm GF}(q)^*&lt;/math&gt; is the multiplicative group of non-zero elements.

* Paley &lt;math&gt;(4n-1, 2n-1, n-1)&lt;/math&gt;-difference set:
::Let &lt;math&gt;q = 4n -1&lt;/math&gt; be a prime power. In the group &lt;math&gt;G = ({\rm GF}(q), +)&lt;/math&gt;, let &lt;math&gt;D&lt;/math&gt; be the set of all non-zero squares.

* Singer &lt;math&gt;((q^{n+2}-1)/(q-1), (q^{n+1}-1)/(q-1), (q^n-1)/(q-1))&lt;/math&gt;-difference set: 
::Let &lt;math&gt;G={\rm GF}(q^{n+2})^*/{\rm GF}(q)^*&lt;/math&gt;. Then the set &lt;math&gt;D=\{x\in G~|~{\rm Tr}_{q^{n+2}/q}(x)=0\}&lt;/math&gt; is a &lt;math&gt;((q^{n+2}-1)/(q-1), (q^{n+1}-1)/(q-1), (q^n-1)/(q-1))&lt;/math&gt;-difference set, where &lt;math&gt;{\rm Tr}_{q^{n+2}/q}:{\rm GF}(q^{n+2})\rightarrow{\rm GF}(q)&lt;/math&gt; is the [[trace function]] &lt;math&gt;{\rm Tr}_{q^{n+2}/q}(x)=x+x^q+\cdots+x^{q^{n+1}}&lt;/math&gt;.

* Twin prime power &lt;math&gt; \left ( q^2 + 2q, \frac{q^2 + 2q -1}{2}, \frac{q^2+2q-3}{4} \right )&lt;/math&gt;-difference set when &lt;math&gt;q&lt;/math&gt; and &lt;math&gt;q+2&lt;/math&gt; are both prime powers:
::In the group &lt;math&gt;G = ({\rm GF}(q), +) \oplus ({\rm GF}(q+2), +)&lt;/math&gt;, let &lt;math&gt;D = \{(x,y) \colon y = 0 \text{ or } x \text{ and } y \text{ are non-zero and both are squares or both are non-squares} \}.&lt;/math&gt;&lt;ref&gt;{{harvnb|Colbourn|Dinitz|2007|loc=p. 425 (Construction 18.49)}}&lt;/ref&gt;

==History==
The systematic use of cyclic difference sets and methods for the construction of symmetric block designs dates back to [[R. C. Bose]] and a seminal paper of his in 1939.&lt;ref&gt;{{citation|first=R.C.|last=Bose|title=On the construction of balanced incomplete block designs|journal=Annals of Eugenics|volume=9|year=1939|pages=353&amp;ndash;399|doi=10.1111/j.1469-1809.1939.tb02219.x|jfm=65.1110.04|zbl=0023.00102}}&lt;/ref&gt; However, various examples appeared earlier than this, such as the "Paley Difference Sets" which date back to 1933.&lt;ref&gt;{{harvnb|Wallis|1988|loc=p. 69}}&lt;/ref&gt; The generalization of the cyclic difference set concept to more general groups is due to [[Richard Bruck|R.H. Bruck]]&lt;ref&gt;{{citation|first=R.H.|last=Bruck| authorlink=Richard Bruck | title=Difference sets in a finite group|journal=Transactions of the American Mathematical Society|volume=78|year=1955|pages=464&amp;ndash;481|doi=10.2307/1993074 | zbl=0065.13302 }}&lt;/ref&gt; in 1955.&lt;ref&gt;{{harvnb|van Lint|Wilson|1992|loc=p. 340}}&lt;/ref&gt; Multipliers were introduced by [[Marshall Hall (mathematician)|Marshall Hall Jr.]]&lt;ref&gt;{{citation|last=Hall Jr.|first=Marshall|authorlink=Marshall Hall (mathematician) | title=Cyclic projective planes|journal=Duke Journal of Mathematics|volume=14|year=1947|pages=1079&amp;ndash;1090|doi=10.1215/s0012-7094-47-01482-8 | zbl=0029.22502}}&lt;/ref&gt; in 1947.&lt;ref&gt;{{harvnb|Beth|Jungnickel|Lenz|1986|loc=p. 275}}&lt;/ref&gt;

==Application==

It is found by Xia, Zhou and [[Georgios B. Giannakis|Giannakis]] that difference sets can be used to construct a complex vector codebook that achieves the difficult [[Welch bounds|Welch bound]] on maximum cross correlation amplitude. The so-constructed codebook also forms the so-called [[Grassmannian]] manifold.

==Generalisations==
A &lt;math&gt;(v,k,\lambda,s)&lt;/math&gt; '''difference family''' is a set of subsets &lt;math&gt;B=\{B_1,...B_s\}&lt;/math&gt; of a [[group (mathematics)|group]] &lt;math&gt;G&lt;/math&gt; such that the [[order of a group|order]] of &lt;math&gt;G&lt;/math&gt; is &lt;math&gt;v&lt;/math&gt;, the [[cardinality|size]] of &lt;math&gt;B_i&lt;/math&gt; is &lt;math&gt;k&lt;/math&gt; for all &lt;math&gt;i&lt;/math&gt;, and every nonidentity element of &lt;math&gt;G&lt;/math&gt; can be expressed as a product &lt;math&gt;d_1d_2^{-1}&lt;/math&gt; of elements of &lt;math&gt;B_i&lt;/math&gt; for some &lt;math&gt;i&lt;/math&gt; (i.e. both &lt;math&gt;d_1,d_2&lt;/math&gt; come from the same &lt;math&gt;B_i&lt;/math&gt;) in exactly &lt;math&gt;\lambda&lt;/math&gt; ways.

A difference set is a difference family with &lt;math&gt;s=1&lt;/math&gt;. The parameter equation above generalises to &lt;math&gt;s(k^2-k)=(v-1)\lambda&lt;/math&gt;.&lt;ref&gt;{{harvnb|Beth|Jungnickel|Lenz|1986|loc=p. 310 (2.8.a)}}&lt;/ref&gt; 
The development &lt;math&gt;dev (B) = \{B_i+g: i=1,...,s, g \in G\}&lt;/math&gt; of a difference family is a [[Block design|2-design]].
Every 2-design with a regular automorphism group is &lt;math&gt;dev (B)&lt;/math&gt; for some difference family &lt;math&gt;B&lt;/math&gt;.

==See also==
*[[Combinatorial design]]

==Notes==
{{Reflist}}

==References==
* {{citation|first1=Thomas|last1=Beth|first2=Dieter|last2=Jungnickel|first3=Hanfried|last3=Lenz|title=Design Theory|year=1986|publisher=Cambridge University Press|place=Cambridge|isbn=0-521-33334-2 | zbl=0602.05001 }}
* {{citation|last1=Colbourn|first1=Charles J.|last2=Dinitz|first2=Jeffrey H.|title=Handbook of Combinatorial Designs|year=2007|publisher=Chapman &amp; Hall/ CRC|location=Boca Raton|isbn=1-58488-506-8|edition=2nd | zbl=1101.05001 | series=Discrete Mathematics and its Applications }}
* {{citation|first1=J.H. |last1=van Lint|first2=R.M. |last2=Wilson|title=A Course in Combinatorics|publisher=[[Cambridge University Press]]  | place=Cambridge|year=1992|isbn=0-521-42260-4 | zbl=0769.05001}} 
* {{cite book | first=W.D. | last=Wallis | title=Combinatorial Designs | publisher=Marcel Dekker |year=1988 |isbn=0-8247-7942-8 | zbl=0637.05004 }}

==Further reading==
*{{cite book|last1 = Moore | first1 = EH | last2 = Pollastek | first2 = HSK | title =  Difference Sets: Connecting Algebra, Combinatorics, and Geometry |  year = 2013 | isbn = 978-0-8218-9176-6 | publisher = AMS | url = http://bookstore.ams.org/stml-67}}
* {{cite book | last=Storer | first=Thomas | title=Cyclotomy and difference sets | location=Chicago | publisher=Markham Publishing Company | year=1967 | zbl=0157.03301 }}
* {{cite journal | last1=Xia | first1=Pengfei | last2=Zhou | first2=Shengli | last3=Giannakis | first3=Georgios B. | title=Achieving the Welch Bound with Difference Sets |journal=IEEE Transactions on Information Theory |volume=51 |issue=5 |pages=1900–1907 |year=2005 |doi=10.1109/TIT.2005.846411 |url=http://www.engr.uconn.edu/~shengli/papers/conf05/05icassp.pdf | issn=0018-9448 | zbl=1237.94007}}.  
:{{ cite journal | last1=Xia | first1=Pengfei | last2=Zhou | first2=Shengli | last3=Giannakis | first3=Georgios B. | title=Correction to ``Achieving the Welch bound with difference sets | journal=IEEE Trans. Inf. Theory | volume=52 | issue=7 | page=3359 | year=2006 | zbl=1237.94008 | doi=10.1109/tit.2006.876214}}

* {{cite book | first=Daniel | last=Zwillinger |title=CRC Standard Mathematical Tables and Formulae |publisher=CRC Press |year=2003 |isbn=1-58488-291-3 |page=246}}

[[Category:Combinatorics]]</text>
      <sha1>fnm0uegwf9349o837iax22u2khhcf2p</sha1>
    </revision>
  </page>
  <page>
    <title>Differential poset</title>
    <ns>0</ns>
    <id>31881212</id>
    <revision>
      <id>842505695</id>
      <parentid>841542352</parentid>
      <timestamp>2018-05-22T22:05:10Z</timestamp>
      <contributor>
        <username>OAbot</username>
        <id>28481209</id>
      </contributor>
      <minor/>
      <comment>[[Wikipedia:OABOT|Open access bot]]: add arxiv identifier to citation with #oabot.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12640">In [[mathematics]], a '''differential poset''' is a [[partially ordered set]] (or ''poset'' for short) satisfying certain local properties.  (The formal definition is given below.)  This family of posets was introduced by {{harvtxt|Stanley|1988}} as a generalization of [[Young's lattice]] (the poset of [[integer partition]]s ordered by inclusion), many of whose [[combinatorics|combinatorial]] properties are shared by all differential posets.  In addition to Young's lattice, the other most significant example of a differential poset is the [[Young–Fibonacci lattice]].

==Definitions==

A poset ''P'' is said to be a differential poset, and in particular to be ''r''-differential (where ''r'' is a positive integer), if it satisfies the following conditions:
* ''P'' is [[graded poset|graded]] and [[locally finite poset|locally finite]] with a unique minimal element;
* for every two distinct elements ''x'', ''y'' of ''P'', the number of elements [[covering relation|covering]] both ''x'' and ''y'' is the same as the number of elements covered by both ''x'' and&amp;nbsp;''y''; and
* for every element ''x'' of ''P'', the number of elements covering ''x'' is exactly ''r'' more than the number of elements covered by&amp;nbsp;''x''.

These basic properties may be restated in various ways.  For example, Stanley shows that the number of elements covering two distinct elements ''x'' and ''y'' of a differential poset is always either 0 or 1, so the second defining property could be altered accordingly.

The defining properties may also be restated in the following [[linear algebra]]ic setting: taking the elements of the poset ''P'' to be formal [[basis (linear algebra)|basis]] vectors of an (infinite dimensional) [[vector space]], let ''D'' and ''U'' be the [[linear operator|operators]] defined so that ''D''&amp;nbsp;''x'' is equal to the sum of the elements covered by ''x'', and ''U''&amp;nbsp;''x'' is equal to the sum of the elements covering&amp;nbsp;''x''.  (The operators ''D'' and ''U'' are called the ''down'' and ''up operator'', for obvious reasons.)  Then the second and third conditions may be replaced by the statement that ''DU''&amp;nbsp;–&amp;nbsp;''UD''&amp;nbsp;=&amp;nbsp;''rI'' (where ''I'' is the identity).

This latter reformulation makes a differential poset into a combinatorial realization of a [[Weyl algebra]], and in particular explains the name ''differential'': the operators "''d''/''dx''" and "multiplication by ''x''" on the vector space of polynomials obey the same commutation relation as ''U'' and ''D''/''r''.

==Examples==
[[File:Young-Fibonacci.svg|thumb|300px|The Young–Fibonacci graph, the [[Hasse diagram]] of the Young–Fibonacci lattice.]]
The canonical examples of differential posets are Young's lattice, the poset of [[integer partition]]s ordered by inclusion, and the Young–Fibonacci lattice.  Stanley's initial paper established that Young's lattice is the only 1-differential [[distributive lattice]],  while {{harvtxt|Byrnes|2012}} showed that these are the only 1-differential [[lattice (order)|lattice]]s.

There is a canonical construction (called "reflection") of a differential poset given a finite poset that obeys all of the defining axioms below its top rank.  (The Young–Fibonacci lattice is the poset that arises by applying this construction beginning with a single point.)  This can be used to show that there are infinitely many differential posets.  {{harvtxt|Stanley|1988}} includes a remark that "[David] Wagner described a very general method for constructing differential posets which make it unlikely that [they can be classified]."  This is made precise in {{harvtxt|Lewis|2007}}, where it is shown that there are uncountably many 1-differential posets.  On the other hand, explicit examples of differential posets are rare; {{harvtxt|Lewis|2007}} gives a convoluted description of a differential poset other than the Young and Young–Fibonacci lattices.

The Young-Fibonacci lattice has a natural ''r''-differential analogue for every positive integer&amp;nbsp;''r''.  These posets are lattices, and can be constructed by a variation of the reflection construction.  In addition, the product of an ''r''-differential and ''s''-differential poset is always an (''r''&amp;nbsp;+&amp;nbsp;''s'')-differential poset.  This construction also preserves the lattice property.  It is not known for any ''r'' &gt; 1 whether there are any ''r''-differential lattices other than those that arise by taking products of the Young–Fibonacci lattices and Young's lattice.

{{unsolved|mathematics|Are there any differential lattices that are not products of Young's lattice and the Young–Fibonacci lattices?}}

==Rank growth==
In addition to the question of whether there are other differential lattices, there are several long-standing open problems relating to the rank growth of differential posets.  It was conjectured in {{harvtxt|Stanley|1988}} that if ''P'' is a differential poset with {{math|''r''&lt;sub&gt;''n''&lt;/sub&gt;}} vertices at rank ''n'', then

: &lt;math&gt; p(n) \le r_n \le F_n, &lt;/math&gt;

where ''p''(''n'') is the number of integer partitions of ''n'' and {{math|''F''&lt;sub&gt;''n''&lt;/sub&gt;}} is the ''n''th [[Fibonacci number]].  In other words, the conjecture states that at every rank, every differential poset has a number of vertices lying between the numbers for Young's lattice and the Young-Fibonacci lattice.  The upper bound was proved in {{harvtxt|Byrnes|2012}}.  The lower bound remains open.  {{harvtxt|Stanley|Zanello|2012}} proved an [[asymptotic analysis|asymptotic]] version of the lower bound, showing that
: &lt;math&gt; r_n \gg n^a \exp(2\sqrt{n}) &lt;/math&gt;
for every differential poset and some constant ''a''.  By comparison, the partition function has asymptotics
: &lt;math&gt; p(n) \sim \frac{1}{4n\sqrt{3}} \exp\left({\pi \sqrt {\frac{2n}{3}}}\right).&lt;/math&gt;

All known bounds on rank sizes of differential posets are quickly growing functions.  In the original paper of Stanley, it was shown (using [[eigenvalue]]s of the operator ''DU'') that the rank sizes are weakly increasing.  However, it took 25 years before {{harvtxt|Miller|2013}} showed that the rank sizes of an ''r''-differential poset strictly increase (except trivially between ranks 0 and 1 when ''r'' = 1).

==Properties==
[[File:Young's lattice.svg|thumb|300px|A [[Hasse diagram]] of Young's lattice]]
Every differential poset ''P'' shares a large number of combinatorial properties.  A few of these include:
* The number of paths of length 2''n'' in the Hasse diagram of ''P'' beginning and ending at the minimal element is {{math|(2''n'' − 1)!!}} (here exclamation marks denote the [[double factorial]]).  In an ''r''-differential poset, the number of such paths is {{math|(2''n'' − 1)!! ''r''&lt;sup&gt;''n''&lt;/sup&gt;}}.&lt;ref&gt;Richard Stanley, ''Enumerative Combinatorics, Volume 1'' (second edition).  Cambridge University Press, 2011.  [http://www-math.mit.edu/~rstan/ec/ec1.pdf], version of 15 July 2011.  Theorem 3.21.7, page 384.&lt;/ref&gt;
* The number of paths of length 2''n'' in the Hasse diagram of ''P'' beginning with the minimal element such that the first ''n'' steps are covering relations from a smaller to a larger element of ''P'' while the last ''n'' steps are covering relations from a larger to a smaller element of ''P'' is ''n''!.  In an ''r''-differential poset, the number is {{math|''n''! ''r''&lt;sup&gt;''n''&lt;/sup&gt;}}.&lt;ref&gt;Richard Stanley, ''Enumerative Combinatorics, Volume 1'' (second edition).  Cambridge University Press, 2011.  [http://www-math.mit.edu/~rstan/ec/ec1.pdf], version of 15 July 2011.  Theorem 3.21.8, page 385.&lt;/ref&gt;
* The number of upward paths of length ''n'' in the Hasse diagram of ''P'' beginning with the minimal element is equal to the number of [[involution (mathematics)|involutions]] in the [[symmetric group]] on ''n'' letters.  In an ''r''-differential poset, the sequence of these numbers has [[exponential generating function]] {{math|''e''&lt;sup&gt;''rx'' + ''x''&lt;sup&gt;2&lt;/sup&gt;/2&lt;/sup&gt;}}.&lt;ref&gt;Richard Stanley, ''Enumerative Combinatorics, Volume 1'' (second edition).  Cambridge University Press, 2011.  [http://www-math.mit.edu/~rstan/ec/ec1.pdf], version of 15 July 2011.  Theorem 3.21.10, page 386.&lt;/ref&gt;

==Generalizations==
In a differential poset, the same set of edges is used to compute the up and down operators ''U'' and ''D''.  If one permits different sets of up edges and down edges (sharing the same vertex sets, and satisfying the same relation), the resulting concept is the ''dual graded graph'', initially defined by {{harvtxt|Fomin|1994}}.  One recovers differential posets as the case that the two sets of edges coincide.

Much of the interest in differential posets is inspired by their connections to [[representation theory]].  The elements of Young's lattice are integer partitions, which encode the representations of the [[symmetric group]]s, and are connected to the [[ring of symmetric functions]]; {{harvtxt|Okada|1994}} defined [[algebra over a field|algebras]] whose representation is encoded instead by the Young–Fibonacci lattice, and allow for analogous constructions such as a Fibonacci version of symmetric functions.  It is not known whether similar algebras exist for every differential poset.{{citation needed|date=May 2017}}  In another direction, {{harvtxt|Lam|Shimozono|2009}} defined dual graded graphs corresponding to any [[Kac–Moody algebra]].

Other variations are possible; {{harvtxt|Stanley|1990}} defined versions in which the number ''r'' in the definition varies from rank to rank, while {{harvtxt|Lam|2008}} defined a signed analogue of differential posets in which cover relations may be assigned a "weight" of −1.

==References==
{{Reflist}}
* {{citation
 | last = Byrnes | first = Patrick
 | title = Structural Aspects of Differential Posets
 | year = 2012
 | isbn=9781267855169}} ([[University of Minnesota|UMN Ph.D. Thesis]])
* {{citation
 | last = Fomin | first = Sergey | author-link = Sergey Fomin
 | title = Duality of graded graphs
 | journal = Journal of Algebraic Combinatorics
 | volume = 3
 | year = 1994
 | issue = 4
 | pages = 357–404
 | doi = 10.1023/A:1022412010826}}
* {{citation
 | last = Lam | first = Thomas
 | title = Signed differential posets and sign-imbalance
 | journal = Journal of Combinatorial Theory, Series A
 | volume = 115
 | issue = 3
 | year = 2008
 | pages = 466–484
 | doi=10.1016/j.jcta.2007.07.003
 }}
* {{citation
 | last1 = Lam | first1 = Thomas F. | last2 = Shimozono | first2 = Mark
 | title = Dual graded graphs for Kac-Moody algebras
 | journal = Algebra &amp; Number Theory
 | volume = 1
 | year = 2007
 | issue = 4
 | pages = 451–488
 | doi = 10.2140/ant.2007.1.451
 | arxiv = math/0702090}}
* {{citation
 | last = Lewis | first = Joel Brewster
 | title = On Differential Posets
 | year = 2007
 | url = https://blogs.gwu.edu/jblewis/files/2018/04/JBLHarvardSeniorThesis-20drube.pdf}} ([[Harvard College]] undergraduate thesis)
* {{citation
 | last = Miller | first = Alexander
 | year = 2013
 | title = Differential posets have strict rank growth: a conjecture of Stanley
 | journal = Order
 | volume = 30
 | issue = 2
 | pages = 657–662
 | doi = 10.1007/s11083-012-9268-y| arxiv = 1202.3006
 }} [https://arxiv.org/abs/1202.3006 arXiv:1202.3006 &amp;#91;math.CO&amp;#93;]
*{{citation
 | last = Okada | first = Soichi 
 | title = Algebras associated to the Young-Fibonacci lattice
 | journal = Transactions of the American Mathematical Society
 | volume = 346
 | issue = 2
 | pages = 549–568
 | year = 1994
 | publisher = American Mathematical Society
 | doi=10.2307/2154860}}
*{{citation
 | last = Stanley | first = Richard P. | author-link = Richard P. Stanley
 | title = Differential posets
 | journal = Journal of the American Mathematical Society
 | volume = 1
 | issue = 4
 | pages = 919–961
 | year = 1988
 | doi = 10.2307/1990995
 | jstor = 1990995
 | publisher = American Mathematical Society}}
* {{citation
 | last = Stanley | first = Richard P. | author-link = Richard P. Stanley
 | title = Variations on differential posets
 |booktitle = Invariant theory and tableaux (Minneapolis, MN), 1988
 |   series = IMA Vol. Math. Appl.
 |   volume = 19
 |    pages = 145–165
 |publisher = Springer
 |     year = 1990
 }}
* {{citation
 | last1 = Stanley | first1 = Richard P. | author1-link = Richard P. Stanley
 | last2 = Zanello | first2 = Fabrizio 
 | title = On the Rank Function of a Differential Poset
 | journal = Electronic Journal of Combinatorics
 | volume = 19
 | issue = 2
 | year = 2012
 | pages = P13
 | url = http://www.combinatorics.org/ojs/index.php/eljc/article/view/v19i2p13}}

[[Category:Representation theory]]
[[Category:Algebraic combinatorics]]</text>
      <sha1>pl12t2ar18d3lu5k3t0l2z1vtygd1uu</sha1>
    </revision>
  </page>
  <page>
    <title>Directional derivative</title>
    <ns>0</ns>
    <id>487859</id>
    <revision>
      <id>861873547</id>
      <parentid>857160102</parentid>
      <timestamp>2018-09-30T17:27:15Z</timestamp>
      <contributor>
        <username>Deansg</username>
        <id>5607454</id>
      </contributor>
      <comment>Add proof for an elementary identity regarding the directional derivative</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="26515">{{refimprove section|date=October 2012|talk=Verifiability of definition}}
{{Calculus |Vector}}

In [[mathematics]], the '''directional derivative''' of a multivariate [[differentiable function]] along a given [[vector (mathematics)|vector]] '''v''' at a given point '''x''' intuitively represents the instantaneous rate of change of the function, moving through '''x''' with a velocity specified by '''v'''.  It therefore generalizes the notion of a [[partial derivative]], in which the rate of change is taken along one of the [[Curvilinear coordinates|curvilinear]] [[coordinate curves]], all other coordinates being constant.

The directional derivative is a special case of the [[Gâteaux derivative]].

== Notation ==
Let ''&amp;tau;'' be a curve whose tangent vector at some chosen point is '''v'''.  The directional derivative of a function ''f'' with respect to '''v''' may be denoted by any of the following:
*&lt;math&gt;\nabla_{\mathbf{v}}{f}(\mathbf{x}),&lt;/math&gt;
*&lt;math&gt;f'_\mathbf{v}(\mathbf{x}),&lt;/math&gt;
*&lt;math&gt;D_\mathbf{v}f(\mathbf{x}),&lt;/math&gt;
*&lt;math&gt;Df(\mathbf{x})(\mathbf{v}),&lt;/math&gt;
*&lt;math&gt;\partial_\mathbf{v}f(\mathbf{x}),&lt;/math&gt;
*&lt;math&gt;\frac{\partial{f(\mathbf{x})}}{\partial{\tau}},&lt;/math&gt;
*&lt;math&gt;\mathbf{v}\cdot{\nabla f(\mathbf{x})},&lt;/math&gt;
*&lt;math&gt;\mathbf{v}\cdot \frac{\partial f(\mathbf{x})}{\partial\mathbf{x}}.&lt;/math&gt;

== Definition ==
[[File:Directional derivative contour plot.svg|thumb|275px|A [[contour plot]] of &lt;math&gt;f(x, y)=x^2 + y^2&lt;/math&gt;, showing the gradient vector in black, and the unit vector &lt;math&gt;\mathbf{u}&lt;/math&gt; scaled by the directional derivative in the direction of &lt;math&gt;\mathbf{u}&lt;/math&gt; in orange. The gradient vector is longer because the gradient points in the direction of greatest rate of increase of a function.]]

The ''directional derivative'' of a [[scalar function]] 
:&lt;math&gt;f(\mathbf{x}) = f(x_1, x_2, \ldots, x_n)&lt;/math&gt;
along a vector
:&lt;math&gt;\mathbf{v} = (v_1, \ldots, v_n)&lt;/math&gt;
is the [[function (mathematics)|function]] &lt;math&gt;\nabla_{\mathbf{v}}{f}&lt;/math&gt; defined by the [[limit (mathematics)|limit]]&lt;ref&gt;{{cite book |author1=R. Wrede |author2=M.R. Spiegel | title=Advanced Calculus|edition=3rd| publisher=Schaum's Outline Series| year=2010 | isbn=978-0-07-162366-7}}&lt;/ref&gt;
:&lt;math&gt;\nabla_{\mathbf{v}}{f}(\mathbf{x}) = \lim_{h \rightarrow 0}{\frac{f(\mathbf{x} + h\mathbf{v}) - f(\mathbf{x})}{h}}.&lt;/math&gt;

This definition is valid in a broad range of contexts, for example where the [[Euclidean norm|norm]] of a vector (and hence a unit vector) is undefined.&lt;ref&gt;The applicability extends to functions over spaces without a [[metric (mathematics)|metric]] and to [[differentiable manifold]]s, such as in [[general relativity]].&lt;/ref&gt;

If the function ''f'' is [[Differentiable function#Differentiability in higher dimensions|differentiable]] at '''x''', then the directional derivative exists along any vector '''v''', and one has

:&lt;math&gt;\nabla_{\mathbf{v}}{f}(\mathbf{x}) = \nabla f(\mathbf{x}) \cdot \mathbf{v}&lt;/math&gt;

where the &lt;math&gt;\nabla&lt;/math&gt; on the right denotes the gradient and &lt;math&gt;\cdot&lt;/math&gt; is the [[dot product]].&lt;ref&gt;If the dot product is undefined, the [[gradient]] is also undefined; however, for differentiable ''f'', the directional derivative is still defined, and a similar relation exists with the exterior derivative.&lt;/ref&gt; This follows from defining a path &lt;math&gt;h(t)=x+tv&lt;/math&gt; and using the definition of the derivative as a limit which can be calculated along this path to get:

:&lt;math&gt;
\begin{align}
0=\lim_{t\rightarrow 0}\frac {f(x+tv)-f(x)-t*D_f(x)(v)} t =\lim_{t\rightarrow 0}\frac {f(x+tv)-f(x)} t - D_f(x)(v)=\nabla_v f(x)-D_f(x)(v) \\
 \rightarrow \nabla f(\mathbf{x}) \cdot \mathbf{v}=D_f(x)(v)=\nabla_{\mathbf{v}}{f}(\mathbf{x})
\end{align}
&lt;/math&gt;

Intuitively, the directional derivative of ''f'' at a point '''x''' represents the [[derivative|rate of change]] of ''f'', in the direction of '''v''' with respect to time, when moving past '''x'''.

=== Using only direction of vector ===
[[image:Geometrical interpretation of a directional derivative.svg|thumb|The angle ''α'' between the tangent ''A'' and the horizontal will be maximum if the cutting plane contains the direction of the gradient ''A''.]]
In a [[Euclidean space]], some authors&lt;ref&gt;Thomas, George B. Jr.; and Finney, Ross L. (1979) ''Calculus and Analytic Geometry'', Addison-Wesley Publ. Co., fifth edition, p. 593.&lt;/ref&gt; define the directional derivative to be with respect to an arbitrary nonzero vector '''v''' after [[Normalized vector|normalization]], thus being independent of its magnitude and depending only on its direction.&lt;ref&gt;This typically assumes a [[Euclidean space]] – for example, a function of several variables typically has no definition of the magnitude of a vector, and hence of a unit vector.&lt;/ref&gt;

This definition gives the rate of increase of ''f'' per unit of distance moved in the direction given by '''v'''. In this case, one has
:&lt;math&gt;\nabla_{\mathbf{v}}{f}(\mathbf{x}) = \lim_{h \rightarrow 0}{\frac{f(\mathbf{x} + h\mathbf{v}) - f(\mathbf{x})}{h|\mathbf{v}|}},&lt;/math&gt;
or in case ''f'' is differentiable at '''x''',
:&lt;math&gt;\nabla_{\mathbf{v}}{f}(\mathbf{x}) = \nabla f(\mathbf{x}) \cdot \frac{\mathbf{v}}{|\mathbf{v}|} .&lt;/math&gt;

=== Restriction to a unit vector ===

In the context of a function on a [[Euclidean space]], some texts restrict the vector '''v''' to being a [[unit vector]].  With this restriction, both the above definitions are equivalent.&lt;ref&gt;{{Cite book|url=https://www.worldcat.org/oclc/828768012|title=Calculus : Single and multivariable.|last=Hughes-Hallet|first=Deborah|last2=McCallum|first2=William G.|last3=Gleason|first3=Andrew M.|date=2012-01-01|publisher=John wiley|year=|isbn=9780470888612|location=|pages=780|oclc=828768012}}&lt;/ref&gt;

== Properties ==
Many of the familiar properties of the ordinary [[derivative]] hold for the directional derivative.  These include, for any functions ''f'' and ''g'' defined in a [[neighborhood (mathematics)|neighborhood]] of, and [[total derivative|differentiable]] at, '''p''': 
{{ordered list
|1=  '''[[Sum rule in differentiation|sum rule]]''':
:&lt;math&gt;\nabla_{\mathbf{v}} (f + g) = \nabla_{\mathbf{v}} f + \nabla_{\mathbf{v}} g.&lt;/math&gt;
|2=  '''[[Constant factor rule in differentiation|constant factor rule]]''': For any constant ''c'', 
:&lt;math&gt;\nabla_{\mathbf{v}} (cf) = c\nabla_{\mathbf{v}} f.&lt;/math&gt;
|3=  '''[[product rule]]''' (or '''Leibniz's rule'''):
:&lt;math&gt;\nabla_{\mathbf{v}} (fg) = g\nabla_{\mathbf{v}} f + f\nabla_{\mathbf{v}} g.&lt;/math&gt;
|4=  '''[[chain rule]]''': If ''g'' is differentiable at '''p''' and ''h'' is differentiable at ''g''('''p'''), then
:&lt;math&gt;\nabla_{\mathbf{v}}(h\circ g)(\mathbf{p}) = h'(g(\mathbf{p})) \nabla_{\mathbf{v}} g (\mathbf{p}).&lt;/math&gt;
}}

== In differential geometry ==
{{see also|Tangent space#Tangent vectors as directional derivatives}}

Let {{math|''M''}} be a [[differentiable manifold]] and {{math|'''p'''}} a point of {{math|''M''}}.  Suppose that {{math|''f''}} is a function defined in a neighborhood of {{math|'''p'''}}, and [[total derivative|differentiable]] at {{math|'''p'''}}.  If {{math|'''v'''}} is a [[tangent vector]] to {{math|''M''}} at {{math|'''p'''}}, then the '''directional derivative''' of {{math|''f''}} along {{math|'''v'''}}, denoted variously as {{math|''df''('''v''')}} (see [[Exterior derivative]]), &lt;math&gt;\nabla_{\mathbf{v}} f(\mathbf{p})&lt;/math&gt; (see [[Covariant derivative]]), &lt;math&gt;L_{\mathbf{v}} f(\mathbf{p})&lt;/math&gt; (see [[Lie derivative]]), or &lt;math&gt;{\mathbf{v}}_{\mathbf{p}}(f)&lt;/math&gt; (see {{section link|Tangent space|Definition via derivations}}), can be defined as follows.  Let {{math|''γ'' : [−1, 1] → ''M''}} be a differentiable curve with {{math|1=''γ''(0) = '''p'''}} and {{math|1=''γ''′(0) = '''v'''}}.  Then the directional derivative is defined by
:&lt;math&gt;\nabla_{\mathbf{v}} f(\mathbf{p}) = \left.\frac{d}{d\tau} f\circ\gamma(\tau)\right|_{\tau=0}.&lt;/math&gt;
This definition can be proven independent of the choice of {{math|''γ''}}, provided {{math|''γ''}} is selected in the prescribed manner so that {{math|1=''γ''′(0) = '''v'''}}.

===The Lie derivative===
The [[Lie derivative]] of a vector field &lt;math&gt;\scriptstyle W^\mu(x)&lt;/math&gt; along a vector field &lt;math&gt;\scriptstyle V^\mu(x)&lt;/math&gt; is given by the difference of two directional derivatives (with vanishing torsion):
:&lt;math&gt;\mathcal{L}_V W^\mu=(V\cdot\nabla) W^\mu-(W\cdot\nabla) V^\mu.&lt;/math&gt;
In particular, for a scalar field &lt;math&gt;\scriptstyle \phi(x)&lt;/math&gt;, the Lie derivative reduces to the standard directional derivative:
:&lt;math&gt;\mathcal{L}_V \phi=(V\cdot\nabla) \phi.&lt;/math&gt;

===The Riemann tensor===
Directional derivatives are often used in introductory derivations of the [[Riemann curvature tensor]]. Consider a curved rectangle with an infinitesimal vector ''δ'' along one edge and ''δ''′ along the other. We translate a covector ''S'' along ''δ'' then ''δ''′ and then subtract the translation along ''δ''′ and then ''δ''. Instead of building the directional derivative using partial derivatives, we use the [[covariant derivative]]. The translation operator for ''δ'' is thus
:&lt;math&gt;1+\sum_\nu \delta^\nu D_\nu=1+\delta\cdot D,&lt;/math&gt;
and for ''δ''′,
:&lt;math&gt;1+\sum_\mu \delta'^\mu D_\mu=1+\delta'\cdot D.&lt;/math&gt;
The difference between the two paths is then
:&lt;math&gt;(1+\delta'\cdot D)(1+\delta\cdot D)S^\rho-(1+\delta\cdot D)(1+\delta'\cdot D)S^\rho=\sum_{\mu,\nu}\delta'^\mu \delta^\nu[D_\mu,D_\nu]S_\rho.&lt;/math&gt;
It can be argued&lt;ref&gt;{{cite book|last1=Zee|first1=A.|title=Einstein gravity in a nutshell|date=2013|publisher=Princeton University Press|location=Princeton|isbn=9780691145587|page=341}}&lt;/ref&gt; that the noncommutativity of the covariant derivatives measures the curvature of the manifold:
:&lt;math&gt;[D_\mu,D_\nu]S_\rho=\pm \sum_\sigma R^\sigma{}_{\rho\mu\nu}S_\sigma,&lt;/math&gt;
where ''R'' is the Riemann curvature tensor and the sign depends on the [[sign convention]] of the author.

== In group theory ==

===Translations===
In the [[Poincaré algebra]], we can define an infinitesimal translation operator '''P''' as
:&lt;math&gt;\mathbf{P}=i\nabla.&lt;/math&gt;
(the i ensures that '''P''' is a [[self-adjoint operator]]) For a finite displacement '''λ''', the [[Unitary operator|unitary]] [[Hilbert space]] [[Group representation|representation]] for translations is&lt;ref&gt;{{cite book|last1=Weinberg|first1=Steven|title=The quantum theory of fields|date=1999|publisher=Cambridge Univ. Press|location=Cambridge [u.a.]|isbn=9780521550017|edition=Reprinted (with corr.).}}&lt;/ref&gt; 
:&lt;math&gt;U(\mathbf{\lambda})=\exp\left(-i\mathbf{\lambda}\cdot\mathbf{P}\right).&lt;/math&gt;
By using the above definition of the infinitesimal translation operator, we see that the finite translation operator is an exponentiated directional derivative:
:&lt;math&gt;U(\mathbf{\lambda})=\exp\left(\mathbf{\lambda}\cdot\nabla\right).&lt;/math&gt;
This is a translation operator in the sense that it acts on multivariable functions f('''x''') as
:&lt;math&gt;U(\mathbf{\lambda}) f(\mathbf{x})=\exp\left(\mathbf{\lambda}\cdot\nabla\right) f(\mathbf{x})=f(\mathbf{x}+\mathbf{\lambda}).&lt;/math&gt;

{| class="toccolours collapsible collapsed" width="80%" style="text-align:left"
!Proof of the last equation
|-
|
In standard single-variable calculus, the derivative of a smooth function f(x) is defined by (for small ε)
:&lt;math&gt;\frac{df}{dx}=\frac{f(x+\epsilon)-f(x)}{\epsilon}.&lt;/math&gt;
This can be rearranged to find f(x+ε):
:&lt;math&gt;f(x+\epsilon)=f(x)+\epsilon \,\frac{df}{dx}=\left(1+\epsilon\,\frac{d}{dx}\right)f(x).&lt;/math&gt;
It follows that &lt;math&gt;[1+\epsilon\,(d/dx)] &lt;/math&gt; is a translation operator. This is instantly generalized&lt;ref&gt;{{cite book|last1=Zee|first1=A.|title=Einstein gravity in a nutshell|date=2013|publisher=Princeton University Press|location=Princeton|isbn=9780691145587}}&lt;/ref&gt; to multivariable functions f('''x''')
:&lt;math&gt;f(\mathbf{x}+\mathbf{\epsilon})=\left(1+\mathbf{\epsilon}\cdot\nabla\right)f(\mathbf{x}).&lt;/math&gt;
Here &lt;math&gt; \mathbf{\epsilon}\cdot\nabla&lt;/math&gt; is the directional derivative along the infinitesimal displacement '''ε'''. We have found the infinitesimal version of the translation operator:
:&lt;math&gt;U(\mathbf{\epsilon})=1+\mathbf{\epsilon}\cdot\nabla.&lt;/math&gt;
It is evident that the group multiplication law&lt;ref&gt;{{cite book|last1=Mexico|first1=Kevin Cahill, University of New|title=Physical mathematics|date=2013|publisher=Cambridge University Press|location=Cambridge|isbn=978-1107005211|edition=Repr.}}&lt;/ref&gt; U(g)U(f)=U(gf) takes the form
:&lt;math&gt;U(\mathbf{a})U(\mathbf{b})=U(\mathbf{a+b}).&lt;/math&gt;
So suppose that we take the finite displacement '''λ''' and divide it into N parts (N→∞ is implied everywhere), so that '''λ'''/N='''ε'''. In other words,
:&lt;math&gt;\mathbf{\lambda}=N\mathbf{\epsilon}.&lt;/math&gt;
Then by applying U('''ε''') N times, we can construct U('''λ'''):
:&lt;math&gt;[U(\mathbf{\epsilon})]^N=U(N\mathbf{\epsilon})=U(\mathbf{\lambda}).&lt;/math&gt;
We can now plug in our above expression for U('''ε'''):
:&lt;math&gt;[U(\mathbf{\epsilon})]^N=\left[1+\mathbf{\epsilon}\cdot\nabla\right]^N=\left[1+\frac{\mathbf{\lambda}\cdot\nabla}{N}\right]^N.&lt;/math&gt;
Using the identity&lt;ref&gt;{{cite book|last1=Edwards|first1=Ron Larson, Robert, Bruce H.|title=Calculus of a single variable|date=2010|publisher=Brooks/Cole|location=Belmont|isbn=9780547209982|edition=9th}}&lt;/ref&gt;
:&lt;math&gt;\exp(x)=\left[1+\frac{x}{N}\right]^N,&lt;/math&gt;
we have
:&lt;math&gt;U(\mathbf{\lambda})=\exp\left(\mathbf{\lambda}\cdot\nabla\right).&lt;/math&gt;
And since U('''ε''')f('''x''')=f('''x'''+'''ε''') we have
:&lt;math&gt;[U(\mathbf{\epsilon})]^Nf(\mathbf{x})=f(\mathbf{x}+N\mathbf{\epsilon})=f(\mathbf{x}+\mathbf{\lambda})=U(\mathbf{\lambda})f(\mathbf{x})=\exp\left(\mathbf{\lambda}\cdot\nabla\right)f(\mathbf{x}),&lt;/math&gt;
Q.E.D.

As a technical note, this procedure is only possible because the translation group forms an [[abelian group|Abelian]] [[subgroup]] ([[Cartan subalgebra]]) in the Poincaré algebra. In particular, the group multiplication law U('''a''')U('''b''')=U('''a'''+'''b''') should not be taken for granted. We also note that Poincaré is a connected Lie group. It is a group of transformations  T(ξ) that are described by a continuous set of real parameters &lt;math&gt;\scriptstyle \xi^a&lt;/math&gt;. The group multiplication law takes the form
:&lt;math&gt;T(\bar{\xi})T(\xi)=T(f(\bar{\xi},\xi)).&lt;/math&gt;
Taking &lt;math&gt;\scriptstyle \xi^a&lt;/math&gt;=0 as the coordinates of the identity, we must have
:&lt;math&gt;f^a(\xi,0)=f^a(0,\xi)=\xi^a.&lt;/math&gt;
The actual operators on the Hilbert space are represented by unitary operators U(T(ξ)). In the above notation we suppressed the T; we now write U('''λ''') as U('''P'''('''λ''')). For a small neighborhood around the identity, the power series representation 
:&lt;math&gt;U(T(\xi))=1+i\sum_a\xi^a t_a+\frac{1}{2}\sum_{b,c}\xi^b\xi^c t_{bc}+\cdots&lt;/math&gt;
is quite good. Suppose that U(T(ξ)) form a non-projective representation, i.e. that
:&lt;math&gt;U(T(\bar{\xi}))U(T(\xi))=U(T(f(\bar{\xi},\xi))).&lt;/math&gt;
The expansion of f to second power is
:&lt;math&gt;f^a(\bar{\xi},\xi)=\xi^a+\bar{\xi}^a+\sum_{b,c}f^{abc}\bar{\xi}^b\xi^c.&lt;/math&gt;
After expanding the representation multiplication equation and equating coefficients, we have the nontrivial condition
:&lt;math&gt;t_{bc}=-t_b t_c-i\sum_a f^{abc}t_a.&lt;/math&gt;
Since &lt;math&gt;\scriptstyle t_{ab}&lt;/math&gt; is by definition symmetric in its indices, we have the standard [[Lie algebra]] commutator:
:&lt;math&gt;[t_b, t_c]=i\sum_a(-f^{abc}+f^{acb})t_a=i\sum_a C^{abc}t_a,&lt;/math&gt;
with C the [[structure constant]]. The generators for translations are partial derivative operators, which commute:
:&lt;math&gt;\left[\frac{\partial}{\partial x^b},\frac{\partial }{\partial x^c}\right]=0.&lt;/math&gt;
This implies that the structure constants vanish and thus the quadratic coefficients in the f expansion vanish as well. This means that f is simply additive:
:&lt;math&gt;f^a_\text{abelian}(\bar{\xi},\xi)=\xi^a+\bar{\xi}^a,&lt;/math&gt;
and thus for abelian groups,
:&lt;math&gt;U(T(\bar{\xi}))U(T(\xi))=U(T(\bar{\xi}+\xi)).&lt;/math&gt;
Q.E.D.
|}

===Rotations===
The [[rotation operator (quantum mechanics)|rotation operator]] also contains a directional derivative. The rotation operator for an angle '''θ''', i.e. by an amount θ=|'''θ'''| about an axis parallel to &lt;math&gt;\scriptstyle \hat{\theta}&lt;/math&gt;='''θ'''/θ is
:&lt;math&gt;U(R(\mathbf{\theta}))=\exp(-i\mathbf{\theta}\cdot\mathbf{L}).&lt;/math&gt;
Here '''L''' is the vector operator that generates [[SO(3)]]:
:&lt;math&gt;\mathbf{L}=\begin{pmatrix}
 0&amp; 0 &amp; 0\\ 
 0&amp; 0 &amp; 1\\ 
 0&amp; -1 &amp; 0
\end{pmatrix}\mathbf{i}+\begin{pmatrix}
0 &amp;0  &amp; -1\\ 
 0&amp; 0 &amp;0 \\ 
1 &amp; 0 &amp; 0
\end{pmatrix}\mathbf{j}+\begin{pmatrix}
 0&amp;1  &amp;0 \\ 
 -1&amp;0  &amp;0 \\ 
0 &amp; 0 &amp; 0
\end{pmatrix}\mathbf{k}.&lt;/math&gt;
It may be shown geometrically that an infinitesimal right-handed rotation changes the position vector '''x''' by
:&lt;math&gt;\mathbf{x}\rightarrow \mathbf{x}-\delta\mathbf{\theta}\times\mathbf{x}.&lt;/math&gt;
So we would expect under infinitesimal rotation:
:&lt;math&gt;U(R(\delta\mathbf{\theta}))f(\mathbf{x})=f(\mathbf{x}-\delta\mathbf{\theta}\times\mathbf{x})=f(\mathbf{x})-(\delta\mathbf{\theta}\times\mathbf{x})\cdot\nabla f.&lt;/math&gt;
It follows that 
:&lt;math&gt;U(R(\delta\mathbf{\theta}))=1-(\delta\mathbf{\theta}\times\mathbf{x})\cdot\nabla.&lt;/math&gt;
Following the same exponentiation procedure as above, we arrive at the rotation operator in the position basis, which is an exponentiated directional derivative:&lt;ref&gt;{{cite book|last1=Shankar|first1=R.|title=Principles of quantum mechanics|date=1994|publisher=Kluwer Academic / Plenum|location=New York|isbn=9780306447907|page=318|edition=2nd}}&lt;/ref&gt;
:&lt;math&gt;U(R(\mathbf{\theta}))=\exp(-(\mathbf{\theta}\times\mathbf{x})\cdot\nabla).&lt;/math&gt;

== Normal derivative ==

A '''normal derivative''' is a directional derivative taken in the direction normal (that is, [[orthogonal]]) to some surface in space, or more generally along a [[normal vector]] field orthogonal to some [[hypersurface]]. See for example [[Neumann boundary condition]].  If the normal direction is denoted by &lt;math&gt;\mathbf{n}&lt;/math&gt;, then the directional derivative of a function ''f'' is sometimes denoted as &lt;math&gt;\frac{ \partial f}{\partial n}&lt;/math&gt;.  In other notations,
:&lt;math&gt;\frac{ \partial f}{\partial \mathbf{n}} = \nabla f(\mathbf{x}) \cdot \mathbf{n} = \nabla_{\mathbf{n}}{f}(\mathbf{x}) = \frac{\partial f}{\partial \mathbf{x}}\cdot\mathbf{n} = Df(\mathbf{x})[\mathbf{n}].&lt;/math&gt;

== In the continuum mechanics of solids ==

Several important results in continuum mechanics require the derivatives of vectors with respect to vectors and of [[tensors]] with respect to vectors and tensors.&lt;ref name=Marsden00&gt;J. E. Marsden and T. J. R. Hughes, 2000, ''Mathematical Foundations of Elasticity'', Dover.&lt;/ref&gt;  The '''directional directive''' provides a systematic way of finding these derivatives.

The definitions of directional derivatives for various situations are given below.  It is assumed that the functions are sufficiently smooth that derivatives can be taken.

===Derivatives of scalar-valued functions of vectors===
Let &lt;math&gt;f(\mathbf{v})&lt;/math&gt; be a real-valued function of the vector &lt;math&gt;\mathbf{v}&lt;/math&gt;.  Then the derivative of &lt;math&gt;f(\mathbf{v})&lt;/math&gt; with respect to &lt;math&gt;\mathbf{v}&lt;/math&gt; (or at &lt;math&gt;\mathbf{v}&lt;/math&gt;) in the direction &lt;math&gt;\mathbf{u}&lt;/math&gt; is defined as
:&lt;math&gt;
  \frac{\partial f}{\partial \mathbf{v}}\cdot\mathbf{u} = Df(\mathbf{v})[\mathbf{u}] 
     = \left[\frac{d }{d \alpha}~f(\mathbf{v} + \alpha~\mathbf{u})\right]_{\alpha = 0}
&lt;/math&gt;
for all vectors &lt;math&gt;\mathbf{u}&lt;/math&gt;.

''Properties:''
{{ordered list
|1= If &lt;math&gt;f(\mathbf{v}) = f_1(\mathbf{v}) + f_2(\mathbf{v})&lt;/math&gt; then &lt;math&gt;
   \frac{\partial f}{\partial \mathbf{v}}\cdot\mathbf{u} =  \left(\frac{\partial f_1}{\partial \mathbf{v}} + \frac{\partial f_2}{\partial \mathbf{v}}\right)\cdot\mathbf{u}.
 &lt;/math&gt;

|2= If &lt;math&gt;f(\mathbf{v}) = f_1(\mathbf{v})~ f_2(\mathbf{v})&lt;/math&gt; then &lt;math&gt;
   \frac{\partial f}{\partial \mathbf{v}}\cdot\mathbf{u} =  \left(\frac{\partial f_1}{\partial \mathbf{v}}\cdot\mathbf{u}\right)~f_2(\mathbf{v}) + f_1(\mathbf{v})~\left(\frac{\partial f_2}{\partial \mathbf{v}}\cdot\mathbf{u} \right).
 &lt;/math&gt;

|3= If &lt;math&gt;f(\mathbf{v}) = f_1(f_2(\mathbf{v}))&lt;/math&gt; then &lt;math&gt;
   \frac{\partial f}{\partial \mathbf{v}}\cdot\mathbf{u} =  \frac{\partial f_1}{\partial f_2}~\frac{\partial f_2}{\partial \mathbf{v}}\cdot\mathbf{u}.
 &lt;/math&gt;
}}

===Derivatives of vector-valued functions of vectors===
Let &lt;math&gt;\mathbf{f}(\mathbf{v})&lt;/math&gt; be a vector-valued function of the vector &lt;math&gt;\mathbf{v}&lt;/math&gt;.  Then the derivative of &lt;math&gt;\mathbf{f}(\mathbf{v})&lt;/math&gt; with respect to &lt;math&gt;\mathbf{v}&lt;/math&gt; (or at &lt;math&gt;\mathbf{v}&lt;/math&gt;) in the direction &lt;math&gt;\mathbf{u}&lt;/math&gt; is the '''second-order tensor''' defined as
:&lt;math&gt;
  \frac{\partial \mathbf{f}}{\partial \mathbf{v}}\cdot\mathbf{u} = D\mathbf{f}(\mathbf{v})[\mathbf{u}] 
     = \left[\frac{d }{d \alpha}~\mathbf{f}(\mathbf{v} + \alpha~\mathbf{u})\right]_{\alpha = 0}
&lt;/math&gt;
for all vectors &lt;math&gt;\mathbf{u}&lt;/math&gt;.

''Properties:''
{{ordered list
|1= If &lt;math&gt;\mathbf{f}(\mathbf{v}) = \mathbf{f}_1(\mathbf{v}) + \mathbf{f}_2(\mathbf{v})&lt;/math&gt; then &lt;math&gt;
   \frac{\partial \mathbf{f}}{\partial \mathbf{v}}\cdot\mathbf{u} =  \left(\frac{\partial \mathbf{f}_1}{\partial \mathbf{v}} + \frac{\partial \mathbf{f}_2}{\partial \mathbf{v}}\right)\cdot\mathbf{u} .
 &lt;/math&gt;

|2= If &lt;math&gt;\mathbf{f}(\mathbf{v}) = \mathbf{f}_1(\mathbf{v})\times\mathbf{f}_2(\mathbf{v})&lt;/math&gt; then &lt;math&gt;
   \frac{\partial \mathbf{f}}{\partial \mathbf{v}}\cdot\mathbf{u} =  \left(\frac{\partial \mathbf{f}_1}{\partial \mathbf{v}}\cdot\mathbf{u}\right)\times\mathbf{f}_2(\mathbf{v}) + \mathbf{f}_1(\mathbf{v})\times\left(\frac{\partial \mathbf{f}_2}{\partial \mathbf{v}}\cdot\mathbf{u} \right).
 &lt;/math&gt;

|3= If &lt;math&gt;\mathbf{f}(\mathbf{v}) = \mathbf{f}_1(\mathbf{f}_2(\mathbf{v}))&lt;/math&gt; then &lt;math&gt;
   \frac{\partial \mathbf{f}}{\partial \mathbf{v}}\cdot\mathbf{u} =  \frac{\partial \mathbf{f}_1}{\partial \mathbf{f}_2}\cdot\left(\frac{\partial \mathbf{f}_2}{\partial \mathbf{v}}\cdot\mathbf{u} \right).
 &lt;/math&gt;
}}

===Derivatives of scalar-valued functions of second-order tensors===
Let &lt;math&gt;f(\mathbf{S})&lt;/math&gt; be a real-valued function of the second order tensor &lt;math&gt;\mathbf{S}&lt;/math&gt;.  Then the derivative of &lt;math&gt;f(\mathbf{S})&lt;/math&gt; with respect to &lt;math&gt;\mathbf{S}&lt;/math&gt; (or at &lt;math&gt;\mathbf{S}&lt;/math&gt;) in the direction
&lt;math&gt;\mathbf{T}&lt;/math&gt; is the ''' second order tensor''' defined as
:&lt;math&gt;
  \frac{\partial f}{\partial \mathbf{S}}:\mathbf{T} = Df(\mathbf{S})[\mathbf{T}] 
     = \left[\frac{d }{d \alpha}~f(\mathbf{S} + \alpha\mathbf{T})\right]_{\alpha = 0}
&lt;/math&gt;
for all second order tensors &lt;math&gt;\mathbf{T}&lt;/math&gt;.

''Properties:''
{{ordered list
|1=  If &lt;math&gt;f(\mathbf{S}) = f_1(\mathbf{S}) + f_2(\mathbf{S})&lt;/math&gt; then &lt;math&gt; \frac{\partial f}{\partial \mathbf{S}}:\mathbf{T} =  \left(\frac{\partial f_1}{\partial \mathbf{S}} + \frac{\partial f_2}{\partial \mathbf{S}}\right):\mathbf{T}.&lt;/math&gt;

|2= If &lt;math&gt;f(\mathbf{S}) = f_1(\mathbf{S})~ f_2(\mathbf{S})&lt;/math&gt; then &lt;math&gt; \frac{\partial f}{\partial \mathbf{S}}:\mathbf{T} =  \left(\frac{\partial f_1}{\partial \mathbf{S}}:\mathbf{T}\right)~f_2(\mathbf{S}) + f_1(\mathbf{S})~\left(\frac{\partial f_2}{\partial \mathbf{S}}:\mathbf{T} \right).&lt;/math&gt;

|3= If &lt;math&gt;f(\mathbf{S}) = f_1(f_2(\mathbf{S}))&lt;/math&gt; then &lt;math&gt; \frac{\partial f}{\partial \mathbf{S}}:\mathbf{T} =  \frac{\partial f_1}{\partial f_2}~\left(\frac{\partial f_2}{\partial \mathbf{S}}:\mathbf{T} \right).&lt;/math&gt;
}}

===Derivatives of tensor-valued functions of second-order tensors===
Let &lt;math&gt;\mathbf{F}(\mathbf{S})&lt;/math&gt; be a second order tensor-valued function of the second order tensor &lt;math&gt;\mathbf{S}&lt;/math&gt;.  Then the derivative of &lt;math&gt;\mathbf{F}(\mathbf{S})&lt;/math&gt; with respect to &lt;math&gt;\mathbf{S}&lt;/math&gt; 
(or at &lt;math&gt;\mathbf{S}&lt;/math&gt;) in the direction &lt;math&gt;\mathbf{T}&lt;/math&gt; is the ''' fourth order tensor''' defined as
:&lt;math&gt;
  \frac{\partial \mathbf{F}}{\partial \mathbf{S}}:\mathbf{T} = D\mathbf{F}(\mathbf{S})[\mathbf{T}] 
     = \left[\frac{d }{d \alpha}~\mathbf{F}(\mathbf{S} + \alpha\mathbf{T})\right]_{\alpha = 0}
&lt;/math&gt;
for all second order tensors &lt;math&gt;\mathbf{T}&lt;/math&gt;.

''Properties:''
{{ordered list
|1= If &lt;math&gt;\mathbf{F}(\mathbf{S}) = \mathbf{F}_1(\mathbf{S}) + \mathbf{F}_2(\mathbf{S})&lt;/math&gt; then &lt;math&gt; \frac{\partial \mathbf{F}}{\partial \mathbf{S}}:\mathbf{T} =  \left(\frac{\partial \mathbf{F}_1}{\partial \mathbf{S}} + \frac{\partial \mathbf{F}_2}{\partial \mathbf{S}}\right):\mathbf{T}.&lt;/math&gt;

|2= If &lt;math&gt;\mathbf{F}(\mathbf{S}) = \mathbf{F}_1(\mathbf{S})\cdot\mathbf{F}_2(\mathbf{S})&lt;/math&gt; then &lt;math&gt; \frac{\partial \mathbf{F}}{\partial \mathbf{S}}:\mathbf{T} =  \left(\frac{\partial \mathbf{F}_1}{\partial \mathbf{S}}:\mathbf{T}\right)\cdot\mathbf{F}_2(\mathbf{S}) + \mathbf{F}_1(\mathbf{S})\cdot\left(\frac{\partial \mathbf{F}_2}{\partial \mathbf{S}}:\mathbf{T} \right).&lt;/math&gt;

|3= If &lt;math&gt;\mathbf{F}(\mathbf{S}) = \mathbf{F}_1(\mathbf{F}_2(\mathbf{S}))&lt;/math&gt; then &lt;math&gt; \frac{\partial \mathbf{F}}{\partial \mathbf{S}}:\mathbf{T} =  \frac{\partial \mathbf{F}_1}{\partial \mathbf{F}_2}:\left(\frac{\partial \mathbf{F}_2}{\partial \mathbf{S}}:\mathbf{T} \right).&lt;/math&gt;

|4= If &lt;math&gt;f(\mathbf{S}) = f_1(\mathbf{F}_2(\mathbf{S}))&lt;/math&gt; then &lt;math&gt; \frac{\partial f}{\partial \mathbf{S}}:\mathbf{T} =  \frac{\partial f_1}{\partial \mathbf{F}_2}:\left(\frac{\partial \mathbf{F}_2}{\partial \mathbf{S}}:\mathbf{T} \right).&lt;/math&gt;
}}

== See also ==
* [[Fréchet derivative]]
* [[Gâteaux derivative]]
* [[Derivative (generalizations)]]
* [[Lie derivative]]
* [[Differential form]]
* [[Structure tensor]]
* [[Tensor derivative (continuum mechanics)]]
* [[Del in cylindrical and spherical coordinates]]

== Notes ==
{{reflist|2}}

== References ==
*{{cite book | first=F. B. | last=Hildebrand | title=Advanced Calculus for Applications| publisher=Prentice Hall | year=1976 | isbn=0-13-011189-9 }}
*{{cite book |author1=K.F. Riley |author2=M.P. Hobson |author3=S.J. Bence | title=Mathematical methods for physics and engineering| publisher=Cambridge University Press| year=2010 | isbn=978-0-521-86153-3}}

== External links ==
*[http://mathworld.wolfram.com/DirectionalDerivative.html Directional derivatives] at [[MathWorld]].
*[http://planetmath.org/directionalderivative Directional derivative] at [[PlanetMath]].

[[Category:Differential calculus]]
[[Category:Differential geometry]]
[[Category:Generalizations of the derivative]]
[[Category:Multivariable calculus]]</text>
      <sha1>284h7w34d0wf60cvbw0tn93kjeqr7y9</sha1>
    </revision>
  </page>
  <page>
    <title>Distributive category</title>
    <ns>0</ns>
    <id>6053993</id>
    <revision>
      <id>851490594</id>
      <parentid>804541333</parentid>
      <timestamp>2018-07-22T17:29:15Z</timestamp>
      <contributor>
        <username>LilHelpa</username>
        <id>8024439</id>
      </contributor>
      <minor/>
      <comment>typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3249">{{cleanup|reason=there's more than one proposed notion under this name, see last ref in further reading|date=July 2014}}
In [[mathematics]], a [[category (category theory)|category]] is '''distributive''' if it has finite [[product (category theory)|product]]s and finite [[coproduct (category theory)|coproduct]]s such that for every choice of objects &lt;math&gt;A,B,C&lt;/math&gt;, the canonical map

: &lt;math&gt;[\mathit{id}_A \times\iota_1, \mathit{id}_A \times\iota_2] : A\times B + A\times C\to A\times(B+C)&lt;/math&gt;

is an [[isomorphism]], and for all objects &lt;math&gt;A&lt;/math&gt;, the canonical map &lt;math&gt;0 \to A\times 0&lt;/math&gt; is an isomorphism (where 0 denotes the [[initial object]]). Equivalently. if for every object &lt;math&gt;A&lt;/math&gt; the [[endofunctor]] &lt;math&gt;A\times -&lt;/math&gt; defined by &lt;math&gt;B\mapsto A\times B&lt;/math&gt; preserves coproducts up to isomorphisms &lt;math&gt;f&lt;/math&gt;.&lt;ref&gt;{{cite book|last=Taylor|first=Paul|title=Practical Foundations of Mathematics|publisher=Cambridge University Press|year=1999|page=275}}&lt;/ref&gt; It follows that &lt;math&gt;f&lt;/math&gt; and aforementioned canonical maps are equal for each choice of objects. 

In particular, if the functor &lt;math&gt;A\times -&lt;/math&gt; has a right [[adjoint functors|adjoint]] (i.e., if the category is [[cartesian closed category|cartesian closed]]), it necessarily preserves all colimits, and thus any cartesian closed category with finite coproducts (i.e., any [[bicartesian closed category]]) is distributive.

== Example ==

The [[category of sets]] is distributive. Let {{var|A}}, {{var|B}}, and {{var|C}} be sets. Then
:&lt;math&gt;\begin{align}
  A\times (B\amalg C) 
  &amp;= \{(a,d)|a\in A\text{ and }d\in B\amalg C\} \\
  &amp;\cong \{(a,d)|a\in A\text{ and }d\in B\}\amalg \{(a,d)|a\in A\text{ and }d\in C\} \\
  &amp;= (A\times B)\amalg (A\times C)
\end{align}&lt;/math&gt;
where &lt;math&gt;\amalg&lt;/math&gt; denotes the coproduct in '''Set''', namely the [[disjoint union]], and &lt;math&gt;\cong&lt;/math&gt; denotes a [[bijection]].  In the case where {{var|A}}, {{var|B}}, and {{var|C}} are [[finite sets]], this result reflects the [[distributive property]]: the above sets each have cardinality &lt;math&gt;|A|\cdot (|B|+|C|)=|A|\cdot|B|+|A|\cdot |C|&lt;/math&gt;.

The category '''[[category of groups|Grp]]''' is not distributive, even though it has both products and coproducts.

An even simpler category that has both products and coproducts but is not distributive is the category of [[pointed set]]s.&lt;ref name="LawvereSchanuel2009"&gt;{{cite book|author1=F. W. Lawvere|author2=Stephen Hoel Schanuel|title=Conceptual Mathematics: A First Introduction to Categories|year=2009|publisher=Cambridge University Press|isbn=978-0-521-89485-2|edition=2nd|pages=296–298}}&lt;/ref&gt;

==References==
{{reflist}}

==Further reading==
* {{cite journal | doi = 10.1017/S0960129500000232 | title=Introduction to distributive categories | journal=Mathematical Structures in Computer Science | date=1993 | volume=3 | issue=3 | pages=277 | first=J. R. B. | last=Cockett}}
* {{cite journal | doi = 10.1016/0022-4049(93)90035-R | title=Introduction to extensive and distributive categories | journal=Journal of Pure and Applied Algebra | date=1993 | volume=84 | issue=2 | pages=145–158 | first=Aurelio | last=Carboni}}

[[Category:Category theory]]

{{categorytheory-stub}}</text>
      <sha1>2aufufcvsfwd98mmqwwnv52269l3dwd</sha1>
    </revision>
  </page>
  <page>
    <title>Dual of BCH is an independent source</title>
    <ns>0</ns>
    <id>22762908</id>
    <revision>
      <id>692920239</id>
      <parentid>532048827</parentid>
      <timestamp>2015-11-29T06:36:45Z</timestamp>
      <contributor>
        <username>Johnny Zoo</username>
        <id>1077120</id>
      </contributor>
      <minor/>
      <comment>Fix link to MIT course</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3109">A certain family of [[BCH code]]s have a particularly useful property, which is that
treated as [[linear operator]]s, their [[dual basis|dual operators]] turns their input into an &lt;math&gt;\ell&lt;/math&gt;-wise [[pairwise independence|independent source]].  That is, the set of vectors from the input [[vector space]] are mapped to an &lt;math&gt;\ell&lt;/math&gt;-wise independent source.  The proof of this fact below as the following Lemma and Corollary is useful in derandomizing the algorithm for a &lt;math&gt;1-2^{-\ell}&lt;/math&gt;-approximation to [[MAXEkSAT]].

==Lemma==

Let &lt;math&gt;C\subseteq F_2^n&lt;/math&gt; be a linear code such that &lt;math&gt;C^\perp &lt;/math&gt; has distance greater than &lt;math&gt; \ell +1&lt;/math&gt;.  Then &lt;math&gt;C&lt;/math&gt; is an &lt;math&gt;\ell&lt;/math&gt;-wise independent source.

==Proof of Lemma==

It is sufficient to show that given any &lt;math&gt;k \times l&lt;/math&gt; matrix ''M'', where ''k'' is greater than or equal to ''l'', such that the rank of ''M'' is ''l'', for all &lt;math&gt;x\in F_2^k&lt;/math&gt;, &lt;math&gt;xM&lt;/math&gt; takes every value in &lt;math&gt;F_2^l&lt;/math&gt; the same number of times.

Since ''M'' has rank ''l'', we can write ''M'' as two matrices of the same size, &lt;math&gt;M_1&lt;/math&gt; and &lt;math&gt;M_2&lt;/math&gt;, where &lt;math&gt;M_1&lt;/math&gt; has rank equal to ''l''.  This means that &lt;math&gt;xM&lt;/math&gt; can be rewritten as &lt;math&gt;x_1M_1 + x_2M_2&lt;/math&gt; for some &lt;math&gt;x_1&lt;/math&gt; and &lt;math&gt;x_2&lt;/math&gt;.

If we consider ''M'' written with respect to a basis where the first ''l'' rows are the identity matrix, then &lt;math&gt;x_1&lt;/math&gt; has zeros wherever &lt;math&gt;M_2&lt;/math&gt; has nonzero rows, and &lt;math&gt;x_2&lt;/math&gt; has zeros wherever &lt;math&gt;M_1&lt;/math&gt; has nonzero rows.

Now any value ''y'', where &lt;math&gt;y=xM&lt;/math&gt;, can be written as &lt;math&gt;x_1M_1+x_2M_2&lt;/math&gt; for some vectors &lt;math&gt;x_1, x_2&lt;/math&gt;.

We can rewrite this as:

&lt;math&gt;x_1M_1 = y - x_2M_2&lt;/math&gt;

Fixing the value of the last &lt;math&gt;k-l&lt;/math&gt; coordinates of
&lt;math&gt;x_2\in F_2^k&lt;/math&gt; (note that there are exactly &lt;math&gt;2^{k-l}&lt;/math&gt;
such choices), we can rewrite this equation again as:

&lt;math&gt;x_1M_1 = b&lt;/math&gt; for some ''b''.

Since &lt;math&gt;M_1&lt;/math&gt; has rank equal to ''l'', there
is exactly one solution &lt;math&gt;x_1&lt;/math&gt;, so the total number of solutions is exactly &lt;math&gt;2^{k-l}&lt;/math&gt;, proving the lemma.

==Corollary==

Recall that BCH&lt;sub&gt;2,''m'',''d''&lt;/sub&gt; is an &lt;math&gt; [n=2^m, n-1 -\lceil {d-2}/2\rceil m, d]_2&lt;/math&gt; linear code.

Let &lt;math&gt;C^\perp&lt;/math&gt; be BCH&lt;sub&gt;2,log&amp;nbsp;''n'',''ℓ''+1&lt;/sub&gt;.  Then &lt;math&gt;C&lt;/math&gt; is an &lt;math&gt;\ell&lt;/math&gt;-wise independent source of size &lt;math&gt;O(n^{\lfloor \ell/2 \rfloor})&lt;/math&gt;.

==Proof of Corollary==

The dimension ''d'' of ''C'' is just &lt;math&gt;\lceil{(\ell +1 -2)/{2}}\rceil \log n +1 &lt;/math&gt;.  So &lt;math&gt;d = \lceil {(\ell -1)}/2\rceil \log n +1 = \lfloor \ell/2 \rfloor \log n +1&lt;/math&gt;.

So the cardinality of &lt;math&gt;C&lt;/math&gt; considered as a set is just
&lt;math&gt; 2^{d}=O(n^{\lfloor \ell/2 \rfloor})&lt;/math&gt;, proving the Corollary.

== References ==
[http://www.cse.buffalo.edu/~atri/courses/coding-theory/ Coding Theory notes at University at Buffalo]

[http://people.csail.mit.edu/madhu/ST13/ Coding Theory notes at MIT]

[[Category:Article proofs]]</text>
      <sha1>6zsovt6chmq0dvsqs48278uv51xatg6</sha1>
    </revision>
  </page>
  <page>
    <title>Elias gamma coding</title>
    <ns>0</ns>
    <id>51761</id>
    <revision>
      <id>854707155</id>
      <parentid>849499566</parentid>
      <timestamp>2018-08-13T06:51:47Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <minor/>
      <comment>&amp;lfloor &amp;rfloor -&gt; ⌊⌋ etc. for readability</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5827">{{redirect|Gamma encoding|the signal processing operation|gamma correction}}

'''Elias γ code''' or '''Elias gamma code''' is a [[universal code (data compression)|universal code]] encoding positive integers developed by [[Peter Elias]].&lt;ref name="Elias"/&gt;{{rp|197, 199}} It is used most commonly when coding integers whose upper-bound cannot be determined beforehand.

==Encoding==

To code a [[number]] ''x''&amp;nbsp;≥ 1:
# Let ''N''&amp;nbsp;= ⌊log&lt;sub&gt;2&lt;/sub&gt; ''x''⌋ be the highest power of 2 it contains, so 2&lt;sup&gt;''N''&lt;/sup&gt; ≤ ''x'' &amp;lt; 2&lt;sup&gt;''N''+1&lt;/sup&gt;.
# Write out ''N'' zero bits, then
# Append the [[binary numeral system|binary]] form of ''x'', an ''N''+1-bit binary number.

An equivalent way to express the same process:
# Encode ''N'' in [[Unary numeral system|unary]]; that is, as ''N'' zeroes followed by a one.
# Append the remaining ''N'' binary digits of ''x'' to this representation of ''N''.

To represent a number &lt;math&gt;x&lt;/math&gt;, Elias gamma (γ) uses &lt;math&gt;2 \lfloor \log_2(x) \rfloor + 1&lt;/math&gt; bits.&lt;ref name="Elias"/&gt;{{rp|199}}

The code begins (the [[implied probability]] distribution for the code is added for clarity):

{| class=wikitable
! Number !! Binary !! &amp;gamma; encoding !! Implied probability
|-
| 1&amp;nbsp;=&amp;nbsp;2&lt;sup&gt;0&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;0 || &lt;code&gt;1&lt;/code&gt; || &lt;code&gt;1&lt;/code&gt; || 1/2
|-
|colspan=4|
|-
| 2 = 2&lt;sup&gt;1&lt;/sup&gt; + ''0'' || &lt;code&gt;1&amp;thinsp;0&lt;/code&gt; || &lt;code&gt;0&amp;thinsp;1&amp;thinsp;0&lt;/code&gt; || 1/8
|-
| 3 = 2&lt;sup&gt;1&lt;/sup&gt; + ''1'' || &lt;code&gt;1&amp;thinsp;1&lt;/code&gt; || &lt;code&gt;0&amp;thinsp;1&amp;thinsp;1&lt;/code&gt; || 1/8
|-
|colspan=4|
|-
| 4 = 2&lt;sup&gt;2&lt;/sup&gt; + ''0'' || &lt;code&gt;1&amp;thinsp;00&lt;/code&gt; || &lt;code&gt;00&amp;thinsp;1&amp;thinsp;00&lt;/code&gt; || 1/32
|-
| 5 = 2&lt;sup&gt;2&lt;/sup&gt; + ''1'' || &lt;code&gt;1&amp;thinsp;01&lt;/code&gt; || &lt;code&gt;00&amp;thinsp;1&amp;thinsp;01&lt;/code&gt; || 1/32
|-
| 6 = 2&lt;sup&gt;2&lt;/sup&gt; + ''2'' || &lt;code&gt;1&amp;thinsp;10&lt;/code&gt; || &lt;code&gt;00&amp;thinsp;1&amp;thinsp;10&lt;/code&gt; || 1/32
|-
| 7 = 2&lt;sup&gt;2&lt;/sup&gt; + ''3'' || &lt;code&gt;1&amp;thinsp;11&lt;/code&gt; || &lt;code&gt;00&amp;thinsp;1&amp;thinsp;11&lt;/code&gt; || 1/32
|-
|colspan=4|
|-
| 8 = 2&lt;sup&gt;3&lt;/sup&gt; + ''0'' || &lt;code&gt;1&amp;thinsp;000&lt;/code&gt; || &lt;code&gt;000&amp;thinsp;1&amp;thinsp;000&lt;/code&gt; || 1/128
|-
| 9 = 2&lt;sup&gt;3&lt;/sup&gt; + ''1'' || &lt;code&gt;1&amp;thinsp;001&lt;/code&gt; || &lt;code&gt;000&amp;thinsp;1&amp;thinsp;001&lt;/code&gt; || 1/128
|-
| 10 = 2&lt;sup&gt;3&lt;/sup&gt; + ''2'' || &lt;code&gt;1&amp;thinsp;010&lt;/code&gt; || &lt;code&gt;000&amp;thinsp;1&amp;thinsp;010&lt;/code&gt; || 1/128
|-
| 11 = 2&lt;sup&gt;3&lt;/sup&gt; + ''3'' || &lt;code&gt;1&amp;thinsp;011&lt;/code&gt; || &lt;code&gt;000&amp;thinsp;1&amp;thinsp;011&lt;/code&gt; || 1/128
|-
| 12 = 2&lt;sup&gt;3&lt;/sup&gt; + ''4'' || &lt;code&gt;1&amp;thinsp;100&lt;/code&gt; || &lt;code&gt;000&amp;thinsp;1&amp;thinsp;100&lt;/code&gt; || 1/128
|-
| 13 = 2&lt;sup&gt;3&lt;/sup&gt; + ''5'' || &lt;code&gt;1&amp;thinsp;101&lt;/code&gt; || &lt;code&gt;000&amp;thinsp;1&amp;thinsp;101&lt;/code&gt; || 1/128
|-
| 14 = 2&lt;sup&gt;3&lt;/sup&gt; + ''6'' || &lt;code&gt;1&amp;thinsp;110&lt;/code&gt; || &lt;code&gt;000&amp;thinsp;1&amp;thinsp;110&lt;/code&gt; || 1/128
|-
| 15 = 2&lt;sup&gt;3&lt;/sup&gt; + ''7'' || &lt;code&gt;1&amp;thinsp;111&lt;/code&gt; || &lt;code&gt;000&amp;thinsp;1&amp;thinsp;111&lt;/code&gt; || 1/128
|-
|colspan=4|
|-
| 16 = 2&lt;sup&gt;4&lt;/sup&gt; + ''0'' || &lt;code&gt;1&amp;thinsp;0000&lt;/code&gt; || &lt;code&gt;0000&amp;thinsp;1&amp;thinsp;0000&lt;/code&gt; || 1/512
|-
| 17&amp;nbsp;=&amp;nbsp;2&lt;sup&gt;4&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''1'' || {{nowrap|&lt;code&gt;1&amp;thinsp;0001&lt;/code&gt;}} || {{nowrap|&lt;code&gt;0000&amp;thinsp;1&amp;thinsp;0001&lt;/code&gt;}} || 1/512
|}

==Decoding==

To decode an Elias gamma-coded integer:
#Read and count 0s from the stream until you reach the first 1. Call this count of zeroes ''N''.
#Considering the one that was reached to be the first digit of the integer, with a value of 2&lt;sup&gt;''N''&lt;/sup&gt;, read the remaining ''N'' digits of the integer.

==Uses==
Gamma coding is used in applications where the largest encoded value is not known ahead of time, or to [[Data compression|compress]] data in which small values are much more frequent than large values.

Gamma coding is a building block in the [[Elias delta code]].

== Generalizations ==&lt;!-- This section is linked from [[Elias delta coding]] --&gt;
Gamma coding does not code zero or negative integers.
One way of handling zero is to add 1 before coding and then subtract 1 after decoding.
Another way is to prefix each nonzero code with a 1 and then code zero as a single 0.

One way to code all integers is to set up a [[bijection]], mapping integers (0, −1, 1, −2, 2, −3, 3, ...) to (1, 2, 3, 4, 5, 6, 7, ...) before coding.  In software, this is most easily done by mapping non-negative inputs to odd outputs, and negative inputs to even outputs, so the least-significant bit becomes an inverted [[sign bit]]:&lt;br/&gt;
&lt;math&gt;\begin{cases}
x \mapsto 2x+1 &amp; \mathrm{when~} x \geq 0 \\
x \mapsto -2x  &amp; \mathrm{when~} x &lt; 0 \\
\end{cases}&lt;/math&gt;

[[Exponential-Golomb coding]] generalizes the gamma code to integers with a "flatter" power-law distribution, just as [[Golomb coding]] generalizes the unary code.
It involves dividing the number by a positive divisor, commonly a power of 2, writing the gamma code for one more than the quotient, and writing out the remainder in an ordinary binary code.

==See also==
* [[Elias delta coding|Elias delta (δ) coding]]
* [[Elias omega coding|Elias omega (ω) coding]]
* [[Posit (number format)]]

==References==
{{Reflist|refs=
&lt;ref name="Elias"&gt;{{cite journal |author-first=Peter |author-last=Elias |author-link=Peter Elias |title=Universal codeword sets and representations of the integers |journal=[[IEEE Transactions on Information Theory]] |volume=21 |issue=2 |pages=194–203 |date=March 1975 |url=http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=1055349 |doi=10.1109/tit.1975.1055349}}&lt;/ref&gt;
}}

==Further reading==
* {{cite book |author-first=Khalid |author-last=Sayood |title=Lossless Compression Handbook |publisher=[[Elsevier]] |date=2003 |chapter=Levenstein and Elias Gamma Codes |isbn=978-0-12-620861-0}}

{{Compression Methods}}

{{DEFAULTSORT:Elias Gamma Coding}}
[[Category:Numeral systems]]
[[Category:Lossless compression algorithms]]
[[Category:Articles with example C code]]</text>
      <sha1>5l3pqjvqsykcor7nvm1x6t28ffvz05b</sha1>
    </revision>
  </page>
  <page>
    <title>Enigform</title>
    <ns>0</ns>
    <id>11643809</id>
    <revision>
      <id>834175701</id>
      <parentid>746578268</parentid>
      <timestamp>2018-04-04T09:50:22Z</timestamp>
      <contributor>
        <username>Störm</username>
        <id>25154634</id>
      </contributor>
      <minor/>
      <comment>Moving from [[Category:Mozilla add-ons]] to [[Category:Firefox add-ons]] using [[c:Help:Cat-a-lot|Cat-a-lot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3188">{{multiple issues|
{{advert|date=August 2012}}
{{external links|date=August 2012}}
{{primary sources|date=August 2012}}
{{refimprove|date=August 2012}}
{{Notability|date=June 2010}}
}}

'''Enigform''' is a [[Firefox|Mozilla Firefox]] extension authored by Arturo 'Buanzo' Busleiman which uses [[GNU Privacy Guard|GnuPG]] to implement [[Pretty Good Privacy#OpenPGP|OpenPGP]]-signed HTTP requests. OpenPGP encryption began to be implemented in 2007. Some {{who|date=August 2012}} people believe it to be an alternative for the [[Transport Layer Security|Secure Sockets Layer]] method for encrypting [[Hypertext Transfer Protocol]] (or ''HTTP'') connections.{{citation needed|date=August 2012}} However, the author never had such an intention in his mind. Guaranteeing the identity of a requester and the integrity of the request is Enigform's primary goal, through the use of [[digital signature]]s. In this instance, requests are [[form (web)|form]] submissions to web servers. [[Apache HTTP Server]] support via the [[mod_openpgp]] module currently supports request verification.

The project got its initial funding from [[OWASP]] in 2007. A secure instant messaging system based on Enigform and HTTPS has been announced during the OWASP Ibero-American Web-Application Security Conference was announced in 2010.

Enigform was granted the Trusted status on the Mozilla Add-ons website in 2008.&lt;ref&gt;{{cite news|url=http://blogs.buanzo.com.ar/2008/09/enigform-granted-trusted-status-in-addonsmozillaorg.html|title=Enigform granted Trusted Status in addons.mozilla.org|publisher=''Buanzo''|author=Buanzo|date=2008-09-02}}{{unreliable source?|date=August 2012}}&lt;/ref&gt;

On April 23, 2009, Enigform was declared a Finalist in the Security category of [[Les Trophées du Libre]], and was awarded the second prize.&lt;ref&gt;{{cite news|url=http://lists.owasp.org/pipermail/owasp-leaders/2009-June/001690.html|title=Enigform awarded second prize}}&lt;/ref&gt;

In April 2012, Enigform is now available for testing in the Firefox 12+ series.

Currently, the author has not received the prize money from the Les Trophees du Libre parent company, Cetril, and it seems that the company has vanished.{{citation needed|date=August 2012}}

==References==
{{reflist}}

==External links==
*[http://enigform.mozdev.org/ Enigform's Website on Mozdev.org]
*[https://addons.mozilla.org/en-US/firefox/addon/enigform/ Enigform Extension for Firefox]
*[http://www.freesoftwaremagazine.com/columns/interview_with_arturo_busleiman Interview with Enigform's developer, Arturo "Buanzo" Busleiman]
*[http://wiki.buanzo.org/index.php?n=Main.Wp-enigform-authentication Enigform: The Definitive Guide]
*[http://wordpress.org/extend/plugins/wp-enigform-authentication/ Wordpress Enigform Authentication Plugin]
*[http://wiki.buanzo.org/index.php?n=Main.Enigform-on-windows-installation Guide for Installing GnuPG and Enigform on Windows]
*[https://www.owasp.org/index.php/Category:OWASP_OpenPGP_Extensions_for_HTTP_-_Enigform_and_mod_openpgp Enigform at OWASP Projects]

[[Category:Cryptographic software]]
[[Category:Firefox add-ons]]
[[Category:OpenPGP]]
[[Category:Access control software]]


{{crypto-stub}}
{{web-software-stub}}</text>
      <sha1>rnoumfk6xz3cvqdbxew0vztv7x6ulwk</sha1>
    </revision>
  </page>
  <page>
    <title>Entanglement-assisted stabilizer formalism</title>
    <ns>0</ns>
    <id>25568464</id>
    <revision>
      <id>841949618</id>
      <parentid>822440925</parentid>
      <timestamp>2018-05-19T05:09:45Z</timestamp>
      <contributor>
        <username>Melissa11111th</username>
        <id>33781892</id>
      </contributor>
      <comment>/* Operation */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19979">In the theory of [[quantum communication]], the '''entanglement-assisted stabilizer formalism''' is a method for protecting quantum information with the help of entanglement shared between a sender and receiver before they transmit quantum data over a quantum communication channel. It extends the standard [[stabilizer formalism]]
by including [[quantum entanglement|shared entanglement]] (Brun ''et al.'' 2006).
The advantage of entanglement-assisted stabilizer codes is that the sender can
exploit the error-correcting properties of an arbitrary set of [[Pauli operator]]s.
The sender's [[Pauli operator]]s do not necessarily have to form an
[[Abelian group|Abelian]] [[subgroup]] of the [[Pauli group]] &lt;math&gt;\Pi^{n}&lt;/math&gt; over &lt;math&gt;n&lt;/math&gt; [[qubit]]s.
The sender can make clever use of her shared
[[quantum entanglement|ebit]]s so that the global stabilizer is Abelian and thus forms a valid
[[quantum error-correcting code]].

== Definition ==

We review the construction of an entanglement-assisted code (Brun ''et al.'' 2006). Suppose that
there is a [[Non-abelian group|nonabelian]] [[subgroup]] &lt;math&gt;\mathcal{S}\subset\Pi^{n}&lt;/math&gt; of size &lt;math&gt;n-k=2c+s&lt;/math&gt;.
Application of the fundamental theorem of [[symplectic geometry]] (Lemma 1 in the first external reference)
states that there exists a minimal set of independent generators
&lt;math&gt;\left\{
\bar{Z}_{1},\ldots,\bar{Z}_{s+c},\bar{X}_{s+1},\ldots,\bar{X}_{s+c}\right\}  &lt;/math&gt;
for &lt;math&gt;\mathcal{S}&lt;/math&gt; with the following [[Commutativity|commutation]] relations:
:&lt;math&gt;
\left[  \bar{Z}_{i},\bar{Z}_{j}\right]    = 0\ \ \ \ \ \forall
i,j,&lt;/math&gt;
:&lt;math&gt;
\left[  \bar{X}_{i},\bar{X}_{j}\right]   = 0\ \ \ \ \ \forall
i,j,&lt;/math&gt;
:&lt;math&gt;
\left[  \bar{X}_{i},\bar{Z}_{j}\right]    = 0\ \ \ \ \ \forall i\neq
j,&lt;/math&gt;
:&lt;math&gt;\left\{  \bar{X}_{i},\bar{Z}_{i}\right\}     = 0\ \ \ \ \ \forall i.
&lt;/math&gt;
The decomposition of &lt;math&gt;\mathcal{S}&lt;/math&gt; into the above minimal generating set
determines that the code requires &lt;math&gt;s&lt;/math&gt; ancilla qubits and &lt;math&gt;c&lt;/math&gt; [[Bell state|ebit]]s. The code
requires an [[Bell state|ebit]] for every [[anticommuting]] pair in the minimal generating set.
The simple reason for this requirement is that an [[Bell state|ebit]] is a simultaneous
&lt;math&gt;+1&lt;/math&gt;-[[eigenstate]] of the [[Pauli operator]]s &lt;math&gt;\left\{  XX,ZZ\right\}  &lt;/math&gt;. The second [[qubit]]
in the [[Bell state|ebit]] transforms the [[anticommuting]] pair &lt;math&gt;\left\{  X,Z\right\}  &lt;/math&gt; into a
[[commuting]] pair &lt;math&gt;\left\{  XX,ZZ\right\}  &lt;/math&gt;. The above decomposition also
minimizes the number of [[Bell state|ebit]]s required for the code---it is an optimal decomposition.

We can partition the [[nonabelian group]] &lt;math&gt;\mathcal{S}&lt;/math&gt; into two [[subgroup]]s: the
isotropic subgroup &lt;math&gt;\mathcal{S}_{I}&lt;/math&gt; and the entanglement subgroup
&lt;math&gt;\mathcal{S}_{E}&lt;/math&gt;. The isotropic subgroup &lt;math&gt;\mathcal{S}_{I}&lt;/math&gt; is a commuting
subgroup of &lt;math&gt;\mathcal{S}&lt;/math&gt; and thus corresponds to ancilla
qubits:
:&lt;math&gt;\mathcal{S}_{I}=\left\{  \bar{Z}_{1},\ldots,\bar{Z}_{s}\right\}  &lt;/math&gt;.

The elements of the entanglement subgroup &lt;math&gt;\mathcal{S}_{E}&lt;/math&gt; come in
anticommuting pairs and thus correspond to [[Bell state|ebit]]s:
:&lt;math&gt;\mathcal{S}_{E}=\left\{
\bar{Z}_{s+1},\ldots,\bar{Z}_{s+c},\bar{X}_{s+1},\ldots,\bar{X}_{s+c}\right\}
&lt;/math&gt;.

== Entanglement-assisted stabilizer code error correction conditions ==

The two subgroups &lt;math&gt;\mathcal{S}_{I}&lt;/math&gt; and &lt;math&gt;\mathcal{S}_{E}&lt;/math&gt; play a role in the
error-correcting conditions for the entanglement-assisted stabilizer
formalism. An entanglement-assisted code corrects errors in a set
&lt;math&gt;\mathcal{E}\subset\Pi^{n}&lt;/math&gt; if for all &lt;math&gt;E_{1},E_{2}\in\mathcal{E}&lt;/math&gt;,
:&lt;math&gt;E_{1}^{\dagger}E_{2}\in\mathcal{S}_{I}\cup\left(  \Pi^{n}-\mathcal{Z}\left(
\left\langle \mathcal{S}_{I},\mathcal{S}_{E}\right\rangle \right)  \right)  .&lt;/math&gt;

== Operation ==

The operation of an entanglement-assisted code is as follows. The sender
performs an encoding unitary on her unprotected qubits, ancilla qubits, and
her half of the [[Bell state|ebit]]s. The unencoded state is a simultaneous +1-[[eigenstate]] of
the following [[Pauli operator]]s:
:&lt;math&gt;
\left\{
Z_{1},\ldots,Z_{s}, Z_{s+1}|Z_{1},\ldots,Z_{s+c}|Z_{c},
X_{s+1}|X_{1},\ldots,X_{s+c}|X_{c}
\right\}  .
&lt;/math&gt;

The [[Pauli operator]]s to the right of the vertical bars indicate the receiver's half
of the shared [[Bell state|ebit]]s. The encoding unitary transforms the unencoded [[Pauli operator]]s
to the following encoded [[Pauli operator]]s:
:&lt;math&gt;
\left\{
\bar{Z}_{1},\ldots,\bar{Z}_{s}, 
\bar{Z}_{s+1}|Z_{1},\ldots,\bar{Z}_{s+c}|Z_{c},
\bar{X}_{s+1}|X_{1},\ldots,\bar{X}_{s+c}|X_{c}
\right\}  .
&lt;/math&gt;
The sender transmits all of her [[qubit]]s over the noisy [[quantum channel]]. The
receiver then possesses the transmitted qubits and his half of the [[Bell state|ebit]]s. He
measures the above encoded operators to diagnose the error. The last step is
to correct the error.

== Rate of an entanglement-assisted code ==

We can interpret the rate of an entanglement-assisted code
in three different ways (Wilde and Brun 2007b).
Suppose that an entanglement-assisted quantum code encodes &lt;math&gt;k&lt;/math&gt; information
qubits into &lt;math&gt;n&lt;/math&gt; physical qubits with the help of &lt;math&gt;c&lt;/math&gt; ebits.

* The ''entanglement-assisted'' rate assumes that entanglement shared between sender and receiver is free. Bennett et al. make this assumption when deriving the [[entanglement assisted capacity]] of a quantum channel for sending quantum information. The entanglement-assisted rate is &lt;math&gt;k/n&lt;/math&gt; for a code with the above parameters.
* The ''trade-off'' rate assumes that entanglement is not free and a rate pair determines performance. The first number in the pair is the number of noiseless qubits generated per channel use, and the second number in the pair is the number of ebits consumed per channel use. The rate pair is &lt;math&gt;\left(  k/n,c/n\right)&lt;/math&gt; for a code with the above parameters. Quantum information theorists have computed asymptotic trade-off curves that bound the rate region in which achievable rate pairs lie. The construction for an entanglement-assisted quantum block code minimizes the number &lt;math&gt;c&lt;/math&gt; of ebits given a fixed number &lt;math&gt;k&lt;/math&gt; and &lt;math&gt;n&lt;/math&gt; of respective information qubits and physical qubits.
* The ''catalytic rate'' assumes that bits of entanglement are built up at the expense of transmitted qubits. A noiseless quantum channel or the encoded use of noisy quantum channel are two different ways to build up entanglement between a sender and receiver. The catalytic rate of an &lt;math&gt;\left[  n,k;c\right]&lt;/math&gt; code is &lt;math&gt;\left(  k-c\right)  /n&lt;/math&gt;.

Which interpretation is most reasonable depends on the context in which we use
the code. In any case, the parameters &lt;math&gt;n&lt;/math&gt;, &lt;math&gt;k&lt;/math&gt;, and &lt;math&gt;c&lt;/math&gt; ultimately govern
performance, regardless of which definition of the rate we use to interpret
that performance.

== Example of an entanglement-assisted code ==

We present an example of an entanglement-assisted code
that corrects an arbitrary single-qubit error (Brun ''et al.'' 2006). Suppose
the sender wants to use the quantum error-correcting properties of the
following nonabelian subgroup of &lt;math&gt;\Pi^{4}&lt;/math&gt;:
:&lt;math&gt;
\begin{array}
[c]{cccc}
Z &amp; X &amp; Z &amp; I\\
Z &amp; Z &amp; I &amp; Z\\
X &amp; Y &amp; X &amp; I\\
X &amp; X &amp; I &amp; X
\end{array}
&lt;/math&gt;
The first two generators anticommute. We obtain a modified third generator by
multiplying the third generator by the second. We then multiply the last
generator by the first, second, and modified third generators. The
error-correcting properties of the generators are invariant under these
operations. The modified generators are as follows:
:&lt;math&gt;
\begin{array}
[c]{cccccc}
g_{1} &amp; = &amp; Z &amp; X &amp; Z &amp; I\\
g_{2} &amp; = &amp; Z &amp; Z &amp; I &amp; Z\\
g_{3} &amp; = &amp; Y &amp; X &amp; X &amp; Z\\
g_{4} &amp; = &amp; Z &amp; Y &amp; Y &amp; X
\end{array}
&lt;/math&gt;
The above set of generators have the commutation relations given by the
fundamental theorem of symplectic geometry:
:&lt;math&gt;
  \left\{  g_{1},g_{2}\right\}  =\left[  g_{1},g_{3}\right]  =\left[
g_{1},g_{4}\right]   =\left[  g_{2},g_{3}\right]  =\left[  g_{2},g_{4}\right]  =\left[
g_{3},g_{4}\right]  =0.
&lt;/math&gt;
The above set of generators is unitarily equivalent to the following canonical
generators:
:&lt;math&gt;
\begin{array}
[c]{cccc}
X &amp; I &amp; I &amp; I\\
Z &amp; I &amp; I &amp; I\\
I &amp; Z &amp; I &amp; I\\
I &amp; I &amp; Z &amp; I
\end{array}
&lt;/math&gt;
We can add one ebit to resolve the anticommutativity of the first two
generators and obtain the canonical stabilizer:
:&lt;math&gt;
\begin{array}
[c]{c}
X\\
Z\\
I\\
I
\end{array}
\left\vert
\begin{array}
[c]{cccc}
X &amp; I &amp; I &amp; I\\
Z &amp; I &amp; I &amp; I\\
I &amp; Z &amp; I &amp; I\\
I &amp; I &amp; Z &amp; I
\end{array}
\right.
&lt;/math&gt;
The receiver Bob possesses the qubit on the left and the sender Alice
possesses the four qubits on the right. The following state is an eigenstate
of the above stabilizer
:&lt;math&gt;
\left\vert \Phi^{+}\right\rangle ^{BA}\left\vert 00\right\rangle
^{A}\left\vert \psi\right\rangle ^{A}.
&lt;/math&gt;
where &lt;math&gt;\left\vert \psi\right\rangle ^{A}&lt;/math&gt; is a qubit that the sender wants to
encode. The encoding unitary then rotates the canonical stabilizer to the following set of globally commuting
generators:
:&lt;math&gt;
\begin{array}
[c]{c}
X\\
Z\\
I\\
I
\end{array}
\left\vert
\begin{array}
[c]{cccc}
Z &amp; X &amp; Z &amp; I\\
Z &amp; Z &amp; I &amp; Z\\
Y &amp; X &amp; X &amp; Z\\
Z &amp; Y &amp; Y &amp; X
\end{array}
\right.
&lt;/math&gt;
The receiver measures the above generators upon receipt of all qubits to
detect and correct errors.

== Encoding algorithm ==

We continue with the previous example. We
detail an algorithm for determining an encoding circuit and the optimal number
of ebits for the entanglement-assisted code---this algorithm first appeared in the appendix of (Wilde and Brun 2007a) and later in the appendix of (Shaw ''et al.'' 2008). The operators in
the above example have the following representation as a binary
matrix (See the [[stabilizer code]] article):
:&lt;math&gt;
H=\left[  \left.
\begin{array}
[c]{cccc}
1 &amp; 0 &amp; 1 &amp; 0\\
1 &amp; 1 &amp; 0 &amp; 1\\
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0
\end{array}
\right\vert
\begin{array}
[c]{cccc}
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 1 &amp; 1 &amp; 0\\
1 &amp; 1 &amp; 0 &amp; 1
\end{array}
\right]  .
&lt;/math&gt;
Call the matrix to the left of the vertical bar the "&lt;math&gt;Z&lt;/math&gt;
matrix" and the matrix to the right of the vertical bar the
"&lt;math&gt;X&lt;/math&gt; matrix."

The algorithm consists of row and column operations on the above matrix. Row
operations do not affect the error-correcting properties of the code but are
crucial for arriving at the optimal decomposition from the fundamental theorem
of symplectic geometry. The operations available for manipulating columns of
the above matrix are Clifford operations. Clifford
operations preserve the Pauli group &lt;math&gt;\Pi^{n}&lt;/math&gt; under conjugation. The
CNOT gate, the Hadamard gate, and the Phase gate generate the Clifford group.
A CNOT gate from qubit &lt;math&gt;i&lt;/math&gt; to qubit &lt;math&gt;j&lt;/math&gt; adds column &lt;math&gt;i&lt;/math&gt; to column &lt;math&gt;j&lt;/math&gt; in the
&lt;math&gt;X&lt;/math&gt; matrix and adds column &lt;math&gt;j&lt;/math&gt; to column &lt;math&gt;i&lt;/math&gt; in the &lt;math&gt;Z&lt;/math&gt; matrix. A Hadamard
gate on qubit &lt;math&gt;i&lt;/math&gt; swaps column &lt;math&gt;i&lt;/math&gt; in the &lt;math&gt;Z&lt;/math&gt; matrix with column &lt;math&gt;i&lt;/math&gt; in the
&lt;math&gt;X&lt;/math&gt; matrix and vice versa. A phase gate on qubit &lt;math&gt;i&lt;/math&gt; adds column &lt;math&gt;i&lt;/math&gt; in the
&lt;math&gt;X&lt;/math&gt; matrix to column &lt;math&gt;i&lt;/math&gt; in the &lt;math&gt;Z&lt;/math&gt; matrix. Three CNOT gates implement a
qubit swap operation. The effect of a swap on qubits
&lt;math&gt;i&lt;/math&gt; and &lt;math&gt;j&lt;/math&gt; is to swap columns &lt;math&gt;i&lt;/math&gt; and &lt;math&gt;j&lt;/math&gt; in both the &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Z&lt;/math&gt; matrix.

The algorithm begins by computing the symplectic product between the first row
and all other rows. We emphasize that the symplectic product here is the
standard symplectic product. Leave the matrix as it is if the first row is not
symplectically orthogonal to the second row or if the first row is
symplectically orthogonal to all other rows. Otherwise, swap the second row
with the first available row that is not symplectically orthogonal to the
first row. In our example, the first row is not symplectically orthogonal to
the second so we leave all rows as they are.

Arrange the first row so that the top left entry in the &lt;math&gt;X&lt;/math&gt; matrix is one. A
CNOT, swap, Hadamard, or combinations of these operations can achieve this
result. We can have this result in our example by swapping qubits one and two.
The matrix becomes
:&lt;math&gt;
\left[  \left.
\begin{array}
[c]{cccc}
0 &amp; 1 &amp; 1 &amp; 0\\
1 &amp; 1 &amp; 0 &amp; 1\\
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0
\end{array}
\right\vert
\begin{array}
[c]{cccc}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 1 &amp; 1 &amp; 0\\
1 &amp; 1 &amp; 0 &amp; 1
\end{array}
\right]  .
&lt;/math&gt;
Perform CNOTs to clear the entries in the &lt;math&gt;X&lt;/math&gt; matrix in the top row to the
right of the leftmost entry. These entries are already zero in this example so
we need not do anything. Proceed to the clear the entries in the first row of
the &lt;math&gt;Z&lt;/math&gt; matrix. Perform a phase gate to clear the leftmost entry in the first
row of the &lt;math&gt;Z&lt;/math&gt; matrix if it is equal to one. It is equal to zero in this case
so we need not do anything. We then use Hadamards and CNOTs to clear the other
entries in the first row of the &lt;math&gt;Z&lt;/math&gt; matrix.

We perform the above operations for our example. Perform a Hadamard on qubits
two and three. The matrix becomes
:&lt;math&gt;
\left[  \left.
\begin{array}
[c]{cccc}
0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 1\\
1 &amp; 1 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0
\end{array}
\right\vert
\begin{array}
[c]{cccc}
1 &amp; 1 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 1
\end{array}
\right]  .
&lt;/math&gt;
Perform a CNOT from qubit one to qubit two and from qubit one to qubit three.
The matrix becomes
:&lt;math&gt;
\left[  \left.
\begin{array}
[c]{cccc}
0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 1\\
1 &amp; 1 &amp; 1 &amp; 0\\
1 &amp; 1 &amp; 0 &amp; 0
\end{array}
\right\vert
\begin{array}
[c]{cccc}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
1 &amp; 1 &amp; 1 &amp; 0\\
1 &amp; 1 &amp; 1 &amp; 1
\end{array}
\right]  .
&lt;/math&gt;
The first row is complete. We now proceed to clear the entries in the second
row. Perform a Hadamard on qubits one and four. The matrix becomes
:&lt;math&gt;
\left[  \left.
\begin{array}
[c]{cccc}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 1 &amp; 1 &amp; 0\\
1 &amp; 1 &amp; 0 &amp; 1
\end{array}
\right\vert
\begin{array}
[c]{cccc}
0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 1 &amp; 0 &amp; 1\\
1 &amp; 1 &amp; 1 &amp; 0\\
1 &amp; 1 &amp; 1 &amp; 0
\end{array}
\right]  .
&lt;/math&gt;
Perform a CNOT from qubit one to qubit two and from qubit one to qubit four.
The matrix becomes
:&lt;math&gt;
\left[  \left.
\begin{array}
[c]{cccc}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 1 &amp; 0\\
1 &amp; 1 &amp; 0 &amp; 1
\end{array}
\right\vert
\begin{array}
[c]{cccc}
0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 1 &amp; 1\\
1 &amp; 0 &amp; 1 &amp; 1
\end{array}
\right]  .
&lt;/math&gt;
The first two rows are now complete. They need one ebit to compensate for
their anticommutativity or their nonorthogonality with respect to the
symplectic product.

Now we perform a "Gram-Schmidt
orthogonalization" with respect to the symplectic product.
Add row one to any other row that has one as the leftmost entry in its &lt;math&gt;Z&lt;/math&gt;
matrix. Add row two to any other row that has one as the leftmost entry in its
&lt;math&gt;X&lt;/math&gt; matrix. For our example, we add row one to row four and we add row two to
rows three and four. The matrix becomes
:&lt;math&gt;
\left[  \left.
\begin{array}
[c]{cccc}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 1
\end{array}
\right\vert
\begin{array}
[c]{cccc}
0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 1\\
0 &amp; 0 &amp; 1 &amp; 1
\end{array}
\right]  .
&lt;/math&gt;
The first two rows are now symplectically orthogonal to all other rows per the
fundamental theorem of symplectic geometry.
We proceed with the same algorithm on the next two rows. The next two rows are
symplectically orthogonal to each other so we can deal with them individually.
Perform a Hadamard on qubit two. The matrix becomes
:&lt;math&gt;
\left[  \left.
\begin{array}
[c]{cccc}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1
\end{array}
\right\vert
\begin{array}
[c]{cccc}
0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 1 &amp; 1\\
0 &amp; 1 &amp; 1 &amp; 1
\end{array}
\right]  .
&lt;/math&gt;
Perform a CNOT from qubit two to qubit three and from qubit two to qubit
four. The matrix becomes
:&lt;math&gt;
\left[  \left.
\begin{array}
[c]{cccc}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 1 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 1
\end{array}
\right\vert
\begin{array}
[c]{cccc}
0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0
\end{array}
\right]  .
&lt;/math&gt;
Perform a phase gate on qubit two:
:&lt;math&gt;
\left[  \left.
\begin{array}
[c]{cccc}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1
\end{array}
\right\vert
\begin{array}
[c]{cccc}
0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0
\end{array}
\right]  .
&lt;/math&gt;
Perform a Hadamard on qubit three followed by a CNOT from qubit two to qubit
three:
:&lt;math&gt;
\left[  \left.
\begin{array}
[c]{cccc}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1
\end{array}
\right\vert
\begin{array}
[c]{cccc}
0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 1 &amp; 0
\end{array}
\right]  .
&lt;/math&gt;
Add row three to row four and perform a Hadamard on qubit two:
:&lt;math&gt;
\left[  \left.
\begin{array}
[c]{cccc}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 1
\end{array}
\right\vert
\begin{array}
[c]{cccc}
0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0
\end{array}
\right]  .
&lt;/math&gt;
Perform a Hadamard on qubit four followed by a CNOT from qubit three to qubit
four. End by performing a Hadamard on qubit three:
:&lt;math&gt;
\left[  \left.
\begin{array}
[c]{cccc}
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 1 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 1 &amp; 0
\end{array}
\right\vert
\begin{array}
[c]{cccc}
0 &amp; 0 &amp; 0 &amp; 0\\
1 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0\\
0 &amp; 0 &amp; 0 &amp; 0
\end{array}
\right]  .
&lt;/math&gt;
The above matrix now corresponds to the canonical Pauli operators. Adding one half of an ebit to the receiver's side
gives the canonical stabilizer whose
simultaneous +1-eigenstate is the above state.
The above operations in reverse order
take the canonical stabilizer to the encoded
stabilizer.

== References ==

* [[Todd Brun|Todd A. Brun]], Igor Devetak, and Min-Hsiu Hsieh. ''Correcting Quantum Errors with Entanglement.'' Science 314, 436 (2006). Available at https://arxiv.org/abs/quant-ph/0610092
* Min-Hsiu Hsieh. ''Entanglement-assisted Coding Theory.'' Ph.D. Dissertation, University of Southern California, August 2008. Available at https://arxiv.org/abs/0807.2080
* Mark M. Wilde. ''Quantum Coding with Entanglement.'' Ph.D. Dissertation, University of Southern California, August 2008. Available at https://arxiv.org/abs/0806.4214
* Min-Hsiu Hsieh, Igor Devetak, Todd A. Brun. ''General entanglement-assisted quantum error-correcting codes.'' 	Phys. Rev. A 76, 062313 (2007). Available at https://arxiv.org/abs/0708.2142
* Isaac Kremsky, Min-Hsiu Hsieh, Todd A. Brun. ''Classical Enhancement of Quantum Error-Correcting Codes.'' Phys. Rev. A 78, 012341 (2008). Available at https://arxiv.org/abs/0802.2414
* Mark M. Wilde and Todd A. Brun. ''Optimal Entanglement Formulas for Entanglement-Assisted Quantum Coding.'' Phys. Rev. A 77, 064302 (2008). Available at https://arxiv.org/abs/0804.1404
* Mark M. Wilde and Todd A. Brun. (2007a) ''Convolutional Entanglement Distillation.'' Available at https://arxiv.org/abs/0708.3699
* Mark M. Wilde and Todd A. Brun. (2007b) ''Entanglement-assisted quantum convolutional coding.'' Available at https://arxiv.org/abs/0712.2223
* Mark M. Wilde and Todd A. Brun. ''Quantum Convolutional Coding with Shared Entanglement: General Structure.'' Available at https://arxiv.org/abs/0807.3803
* Bilal Shaw, Mark M. Wilde, Ognyan Oreshkov, Isaac Kremsky, and [[Daniel Lidar|Daniel A. Lidar]]. ''Encoding One Logical Qubit Into Six Physical Qubits.'' Physical Review A 78, 012337 (2008). Available at https://arxiv.org/abs/0803.1495

{{Quantum computing}}

[[Category:Linear algebra]]</text>
      <sha1>4g1a29y1dwutyup3bbgbt4g5iillnvn</sha1>
    </revision>
  </page>
  <page>
    <title>Francis Allotey</title>
    <ns>0</ns>
    <id>33473957</id>
    <revision>
      <id>869904462</id>
      <parentid>869327536</parentid>
      <timestamp>2018-11-21T03:47:21Z</timestamp>
      <contributor>
        <username>Kandymotownie</username>
        <id>9044494</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14138">{{EngvarB|date=February 2018}}
{{Use dmy dates|date=February 2018}}
{{Infobox scientist
| name = Francis Kofi Ampenyin Allotey
| birth_date = {{Birth date|1932|08|09|df=y}}
| birth_place = [[Saltpond]], [[Gold Coast (British colony)|Gold Coast]]
| death_date = {{Death date and age|2017|11|02|1932|08|09|df=yes}}
| death_place = [[Accra]], [[Ghana]]
| resting place = [[Saltpond]], [[Ghana]]
| image = Francis_Allotey.png 
| other_names = 
| nationality = {{unbulleted list|[[British subject]] (1932 - 1957)|[[Ghanaian]] (1957 - 2017)}}
| fields = [[Mathematical physics]]
| workplaces = [[Kwame Nkrumah University of Science and Technology]]
| known_for = Allotey Formalism
| education = {{unbulleted list|[[Ghana National College]]|[[Imperial College London]] ([[Diploma of Imperial College|DIC]])|[[Princeton University]] ([[MSc]], [[PhD]])}}
| awards = UK Prince Philip Golden Award (1973)
| years_active = 
| organization = 
| agent = 
}}
'''Francis Kofi Ampenyin Allotey''' (9 August 1932&lt;ref&gt;[https://books.google.com/books?id=wgiTQUcE21UC&amp;pg=PA128&amp;dq=Francis+Kofi+Allotey+1932&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiwxsjMtqLXAhXnJsAKHSn3AnAQ6AEIJDAA#v=onepage&amp;q=Francis%20Kofi%20Allotey%201932&amp;f=false] {{webarchive|url=https://web.archive.org/web/20171103175415/https://books.google.com/books?id=wgiTQUcE21UC&amp;pg=PA128&amp;dq=Francis+Kofi+Allotey+1932&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiwxsjMtqLXAhXnJsAKHSn3AnAQ6AEIJDAA|date=3 November 2017}}&lt;/ref&gt;&lt;ref name=":3"&gt;{{cite web|url=http://publications.uew.edu.gh/2015/hof/prof-f-k-ampenyin-allotey|title=Archived copy|accessdate=4 November 2017|deadurl=no|archiveurl=https://web.archive.org/web/20171103175417/http://publications.uew.edu.gh/2015/hof/prof-f-k-ampenyin-allotey|archivedate=3 November 2017}}&lt;/ref&gt; – 2 November 2017&lt;ref name=":0"&gt;{{Cite web|url=https://www.graphic.com.gh/news/general-news/renowned-ghanaian-mathematician-prof-allotey-dies-at-85.html|title=Renowned Ghanaian mathematician Prof Francis Allotey dies at 85|last=graphic.com.gh|website=Graphic Online|access-date=4 November 2017}}&lt;/ref&gt;) was a [[Ghana]]ian [[Mathematical physics|mathematical physicist]].&lt;ref&gt;{{Cite web|url=https://www.graphic.com.gh/news/general-news/carry-on-torch-lit-by-prof-francis-allotey-prez-urges-ghanaians.html|title=Carry on torch lit by Prof. Francis Allotey - Prez urges Ghanaians|last=Graphic|first=Daily|website=www.graphic.com.gh|language=en-gb|access-date=2018-02-24|deadurl=no|archiveurl=https://web.archive.org/web/20180224165442/https://www.graphic.com.gh/news/general-news/carry-on-torch-lit-by-prof-francis-allotey-prez-urges-ghanaians.html|archivedate=24 February 2018|df=dmy-all}}&lt;/ref&gt;&lt;ref name=":8"&gt;{{Cite web|url=https://www.graphic.com.gh/features/features/the-great-mathematician-and-scientist-in-memoriam-to-a-native-of-saltpond-a-son-of-ghana-and-a-citizen-of-the-world.html|title=Professor Allotey: The great mathematician and scientist|last=Graphic|first=Daily|website=www.graphic.com.gh|language=en-gb|access-date=2018-02-24|deadurl=no|archiveurl=https://web.archive.org/web/20180223172924/https://www.graphic.com.gh/features/features/the-great-mathematician-and-scientist-in-memoriam-to-a-native-of-saltpond-a-son-of-ghana-and-a-citizen-of-the-world.html|archivedate=23 February 2018|df=dmy-all}}&lt;/ref&gt; 

==Early life and education==
Allotey was born on 9 August 1932 in the fishing town of [[Saltpond]] in the [[Central Region (Ghana)|Central Region]] of [[Ghana]] to Joseph Kofi Allotey, a general commodities [[merchant]] of the Royal Sempe Mankrado We, Accra and Alice Esi Nyena Allotey, a [[dressmaker]] from the Royal Dehyena family of Enyan Owomase and Ekumfi Edumafa, in the [[Central Region (Ghana)|Central Region]] of Ghana.&lt;ref name=":4"&gt;{{cite web|url=http://www.ghanaweb.com/GhanaHomePage/people/pop-up.php?ID=136|title=Professor Francis Kofi Ampenyi Allotey |work=ghanaweb.com|deadurl=no|archiveurl=https://web.archive.org/web/20070415123500/http://www.ghanaweb.com/GhanaHomePage/people/pop-up.php?ID=136|archivedate=15 April 2007}}&lt;/ref&gt;&lt;ref name=":5" /&gt;&lt;ref name=":6"&gt;{{Cite web|url=https://www.ghanaweb.com/GhanaHomePage/NewsArchive/In-memoriam-of-the-great-mathematician-and-scientist-Prof-Allotey-628389|title=In memoriam of the great mathematician and scientist; Prof. Allotey|website=www.ghanaweb.com|language=en|access-date=2018-02-24|deadurl=no|archiveurl=https://web.archive.org/web/20180222130200/https://www.ghanaweb.com/GhanaHomePage/NewsArchive/In-memoriam-of-the-great-mathematician-and-scientist-Prof-Allotey-628389|archivedate=22 February 2018|df=dmy-all}}&lt;/ref&gt; His father owned a [[bookstore]].&lt;ref name=":0" /&gt;&lt;ref name=":6" /&gt; During his childhood,  Allotey spent his free time in his father's [[bookstore]] reading the biographies of famous scientists which piqued his interest in science. &lt;ref name=":3" /&gt;&lt;ref name=":6" /&gt; He was raised a [[Catholic Church|Roman Catholic]].&lt;ref name=":6" /&gt; He had his primary education at the St. John the Baptist Catholic School, Saltpond and was among the pioneer batch of [[Ghana National College]] when the school was founded in July 1948 by [[Kwame Nkrumah]].&lt;ref name=":1" /&gt; After secondary school, he attended the University Tutorial College in Ghana and the [[London South Bank University|London Borough Polytechnic]].&lt;ref name=":7"&gt;{{Cite web|url=http://wwwf.imperial.ac.uk/~ruzh/papers/Allotey.pdf|title=Francis Allotey Biography|last=Imperial College London|first=|date=|website=|access-date=|deadurl=no|archiveurl=https://web.archive.org/web/20180224165438/http://wwwf.imperial.ac.uk/~ruzh/papers/Allotey.pdf|archivedate=24 February 2018|df=dmy-all}}&lt;/ref&gt; He held master's and doctorate degrees from [[Princeton University]] and the [[Diploma of Imperial College]], obtained in 1960.&lt;ref name=":5"&gt;{{cite web|date=27 May 1997|publisher=buffalo.edu|website=Physicists of the African Diaspora|title=Francis Kofi Ampenyin Allotey|access-date=22 March 2015|url=http://www.math.buffalo.edu/mad/physics/allotey_franciska.html|deadurl=no|archiveurl=https://web.archive.org/web/20150406015635/http://www.math.buffalo.edu/mad/physics/allotey_franciska.html|archivedate=6 April 2015}}&lt;/ref&gt;&lt;ref name=iop&gt;{{cite web|publisher=IOP [[Institute of Physics]]|title=Honorary fellows: Professor Francis Kofi Ampenyin Allotey - African Institute of Mathematical Sciences, Ghana|access-date=22 March 2015|url=http://www.iop.org/about/awards/hon_fellowship/hon_fellows/page_56407.html}}&lt;/ref&gt; During his time at Princeton, he was mentored by many physicists such as [[Robert H. Dicke|Robert Dicke]], [[Val Logsdon Fitch|Val Fitch]], [[J. Robert Oppenheimer|Robert Oppenheimer]], [[Paul Dirac|Paul A.M. Dirac]] and [[Chen-Ning Yang|C.N. Yang]].&lt;ref name=":1" /&gt;

==Career==

He was known for the "''Allotey Formalism''" which arose from his work on [[soft X-ray]] [[spectroscopy]]. He was the 1973 recipient of the ''UK Prince Philip Golden Award'' for his work in this area.&lt;ref name=":7" /&gt; A founding fellow of the [[African Academy of Sciences]],&lt;ref&gt;Gerdes, Paulus (2007)
[https://books.google.com/books?id=wgiTQUcE21UC&amp;pg=PA370 ''African Doctorates in Mathematics''] {{webarchive|url=https://web.archive.org/web/20171103175415/https://books.google.com/books?id=wgiTQUcE21UC&amp;pg=PA370|date=3 November 2017}}, p. 370. Commission on the History of Mathematics in Africa&lt;/ref&gt; in 1974 he became the first Ghanaian full professor of mathematics and head of the Department of Mathematics and later Dean of the Faculty of Science at the [[Kwame Nkrumah University of Science and Technology]].&lt;ref&gt;''[[New African]]'' (January 2007).  [http://goliath.ecnext.com/coms2/gi_0199-6161953/The-Allotey-formalism-the-man.html "The Allotey formalism: the man who formulated the technique used to determine matter in outer space, Professor Francis K.A. Allotey, receives the 2006 Black S/heroes Award"] {{webarchive|url=https://web.archive.org/web/20090801070920/http://goliath.ecnext.com/coms2/gi_0199-6161953/The-Allotey-formalism-the-man.html|date=1 August 2009}}&lt;/ref&gt;&lt;ref name=":1"&gt;{{Cite news|url=https://citifmonline.com/2018/02/23/state-funeral-held-for-prof-allotey-photos/|title=State funeral held for Prof. Allotey [Photos] - Ghana News|date=2018-02-23|work=Ghana News|access-date=2018-02-23|language=en-US|deadurl=no|archiveurl=https://web.archive.org/web/20180223194341/https://citifmonline.com/2018/02/23/state-funeral-held-for-prof-allotey-photos/|archivedate=2018-02-23|df=}}&lt;/ref&gt; He was also the founding director of the [[Kwame Nkrumah University of Science and Technology|KNUST]] Computer Centre before he assumed his position as the Pro-Vice-Chancellor of the university. &lt;ref name=":1" /&gt; 

Allotey was the President of the [[Ghana Academy of Arts and Sciences]] and a member of a number of international scientific organizations including the [[International Centre for Theoretical Physics|Abdus Salam International Centre for Theoretical Physics]] Scientific Council since 1996.&lt;ref name=":2"&gt;{{Cite news|url=https://citifmonline.com/2018/02/23/state-funeral-held-for-prof-allotey-photos/|title=State funeral held for Prof. Allotey [Photos] - Ghana News|date=2018-02-23|work=Ghana News|access-date=2018-02-23|language=en-US|deadurl=no|archiveurl=https://web.archive.org/web/20180223194341/https://citifmonline.com/2018/02/23/state-funeral-held-for-prof-allotey-photos/|archivedate=2018-02-23|df=}}&lt;/ref&gt; He was also the President of the Ghana Institute of Physics and the founding President of the African Physical Society.&lt;ref name=":1" /&gt; He was instrumental in getting Ghana to join the [[International Union of Pure and Applied Physics]], making it one of the first few African countries to join the Union. He collaborated with the [[International Union of Pure and Applied Physics|IUPAP]] and [[International Centre for Theoretical Physics|ICTP]] to encourage [[physics education]] in developing countries through workshops and conferences in order to create awareness on the continent.&lt;ref name=":1" /&gt;&lt;ref name=":2" /&gt;

Allotey was the Chairman of Board of Trustees of the [[Accra Institute of Technology]], the President of the [[African Institute for Mathematical Sciences]], Ghana. He was an honorary fellow of the [[Institute of Physics]].&lt;ref name=iop/&gt;  He was an honorary Fellow of the Nigerian Mathematical Society among others.&lt;ref name=":1" /&gt; He consulted for many international institutions such as the [[UNESCO]], [[International Atomic Energy Agency|IAEA]] and [[United Nations Industrial Development Organization|UNIDO]].&lt;ref name=":1" /&gt; He was also the Vice president, 7th General Assembly of [[Intergovernmental Bureau for Informatics|Intergovernmental Bureau of Informatics]] (IBI).&lt;ref name=":7" /&gt; He was also instrumental in the advancement of [[computer education]] in Africa and worked closely with organisations such as the [[IBM|IBM International]] and the International Federation for Information Processing.&lt;ref name=":6" /&gt;&lt;ref name=":7" /&gt; In 2004, he was the only African among the 100 most eminent physicists and mathematicians in the world to be cited in a book titled, "''One hundred reasons to be a scientist''."&lt;ref name=":2" /&gt;

The Professor Francis Allotey Graduate School was established in 2009 at the [[Accra Institute of Technology]].&lt;ref&gt;Ani-Awukubea, Regina (21 January 2009) [http://www.thestatesmanonline.com/pages/news_detail.php?section=1&amp;newsid=8056 "The AIT commences Open University programme"] {{webarchive|url=https://web.archive.org/web/20111109193250/http://thestatesmanonline.com/pages/news_detail.php?section=1&amp;newsid=8056|date=9 November 2011}}. ''The Statesman'' (Ghana); accessed 4 November 2017.&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://www.ghanabusinessnews.com/2017/11/03/prof-francis-allotey-dies|title=Archived copy|archiveurl=https://web.archive.org/web/20171103175415/https://www.ghanabusinessnews.com/2017/11/03/prof-francis-allotey-dies|archivedate=4 November 2017|deadurl=no|accessdate=4 November 2017}}&lt;/ref&gt; The institute provides master's degrees in [[Business administration|Business Administration]] and [[Software engineering|Software Engineering]] and doctoral programmes in Information Technology and Philosophy.&lt;ref name=":1" /&gt; The [[Government of Ghana]] awarded him the Millennium Excellence Award in 2005, and dedicated a [[postage stamp]] in his honour. In 2009 he received the [[Order of the Volta]] and was posthumously awarded the ''Osagyefo Kwame Nkrumah African Genius Award'' in 2017.&lt;ref name=":1" /&gt; He helped establish the African Institute of Mathematical Sciences in Ghana in 2012.&lt;ref name=":0" /&gt;&lt;ref name=":1" /&gt;

== Personal life ==
Allotey first married Edoris Enid Chandler from [[Barbados]] who he met while they were both studying in [[London]].&lt;ref name=":8" /&gt; They had two children, Francis Kojo Enu Allotey and Joseph Kobina Nyansa Allotey. Chandler died in November 1981. He then remarried to Ruby Asie Mirekuwa Akuamoah.&lt;ref name=":8" /&gt; Together they raised her two children, Cilinnie and Kay. Akuamoah died in October, 2011. Overall, Allotey had four children and 20 grandchildren.&lt;ref name=":8" /&gt;

== Death and state funeral ==
Francis Allotey died of [[Death by natural causes|natural causes]] on 2 November 2017.&lt;ref name=":0" /&gt; The Ghanaian government accorded him a [[state funeral]] in recognition of his contributions to the advancement of science and technology in Ghana.&lt;ref name=":1" /&gt;  His body was interred in his hometown, [[Saltpond]].&lt;ref name=":1" /&gt;&lt;ref name=":2" /&gt; 

==References==
{{Reflist|2}}

==External links==
* [http://www.aims.edu.gh/ AIMS Ghana]
* [http://www.math.buffalo.edu/mad/physics/allotey_franciska.html Allotey profile]

{{DEFAULTSORT:Allotey, Francis}}
[[Category:1932 births]]
[[Category:2017 deaths]]
[[Category:Academics of the Kwame Nkrumah University of Science and Technology]]
[[Category:African mathematicians]]
[[Category: Akan people]]
[[Category:Alumni of Imperial College London]]
[[Category:Fante people]]
[[Category:Ghanaian academics]]
[[Category:Ghanaian physicists]]
[[Category:Ghanaian Roman Catholics]]
[[Category:Ghanaian scientists]]
[[Category:Honorary Fellows of the Institute of Physics]]
[[Category:Princeton University alumni]]</text>
      <sha1>4qx1suw7bvvk22rscgbulksn03rolfe</sha1>
    </revision>
  </page>
  <page>
    <title>Gröbner fan</title>
    <ns>0</ns>
    <id>53625492</id>
    <revision>
      <id>798370240</id>
      <parentid>798369263</parentid>
      <timestamp>2017-09-01T14:27:05Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>/* top */Journal cites, Added 1 doi to a journal cite using [[Project:AWB|AWB]] (12158)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2596">In [[symbolic computation|computer algebra]], the '''Gröbner fan''' of an [[Ideal (ring theory)|ideal]] in the [[Polynomial ring|ring of polynomials]] is a concept in the theory of [[Gröbner basis|Gröbner bases]]. It is defined to be a [[fan (order)|fan]] consisting of [[cone]]s that correspond to different [[monomial order]]s on that ideal. The concept has been introduced by [[Teo Mora|Mora]] and [[Lorenzo Robbiano|Robbiano]] in 1988.&lt;ref&gt;{{Cite journal|url=http://www.sciencedirect.com/science/article/pii/S0747717188800427|title=The Gröbner fan of an ideal |language=en|access-date=2017-03-29|doi=10.1016/S0747-7171(88)80042-7|volume=6|journal=Journal of Symbolic Computation|pages=183–208 | last1 = Mora | first1 = Teo}}&lt;/ref&gt; 
The result is a weaker version of the result presented in the same issue of the journal by Bayer and Morrison.&lt;ref&gt;{{Cite journal|url=http://www.sciencedirect.com/science/article/pii/S0747717188800439|title=Standard bases and geometric invariant theory I. Initial ideals and state polytopes|language=en|access-date=2017-03-29|doi=10.1016/S0747-7171(88)80043-9|volume=6|journal=Journal of Symbolic Computation|pages=209–217 | last1 = Bayer | first1 = David}}&lt;/ref&gt; Gröbner fan is a base for the nowadays active field of [[tropical geometry]].
One implementation of the Gröbner fan is called Gfan,&lt;ref&gt;{{Cite web|url=http://home.math.au.dk/jensen/software/gfan/gfan.html|title=Gfan|website=home.math.au.dk|access-date=2017-04-03}}&lt;/ref&gt; based on an article of Fukuda, et. al.&lt;ref&gt;{{Cite journal|last=KOMEI FUKUDA, ANDERS N. JENSEN, AND REKHA R. THOMAS|first=|year=2007|title=Computing Gröbner Fan|url=http://www.ams.org/journals/mcom/2007-76-260/S0025-5718-07-01986-2/S0025-5718-07-01986-2.pdf|journal=Mathematics of Computation|volume=76|pages=2189–2212|via=|doi=10.1090/s0025-5718-07-01986-2}}&lt;/ref&gt; which is included in some computer algebra systems such as [[SINGULAR computer algebra system|Singular]]&lt;ref&gt;{{Cite web|url=https://www.singular.uni-kl.de/Manual/4-0-3/sing_2406.htm|title=Singular Manual: groebnerFan|website=www.singular.uni-kl.de|access-date=2017-03-29}}&lt;/ref&gt; and [[Macaulay 2|Macaulay2]].&lt;ref&gt;{{Cite web|url=http://www.math.uiuc.edu/Macaulay2/doc/Macaulay2-1.9/share/doc/Macaulay2/gfanInterface/html/_groebner__Fan.html|title=groebnerFan – the fan of all groebner bases of an ideal|website=www.math.uiuc.edu|access-date=2017-03-29}}&lt;/ref&gt;

== See also ==
* [[Gröbner basis]]
* [[Tropical geometry]]

== References ==
{{Reflist}}

[[Category:Computer algebra]]
[[Category:Algebraic geometry]]
[[Category:Commutative algebra]]</text>
      <sha1>8crhiebcfkmybxya08djcan1bccp1hd</sha1>
    </revision>
  </page>
  <page>
    <title>Hadamard manifold</title>
    <ns>0</ns>
    <id>12471936</id>
    <revision>
      <id>815025667</id>
      <parentid>787016796</parentid>
      <timestamp>2017-12-12T08:38:58Z</timestamp>
      <contributor>
        <username>Zaslav</username>
        <id>88809</id>
      </contributor>
      <minor/>
      <comment>Ce.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1126">In [[mathematics]], a '''Hadamard manifold''', named after [[Jacques Hadamard]] — sometimes called a '''Cartan–Hadamard manifold''', after [[Élie Cartan]] — is a [[Riemannian manifold]] (''M'',&amp;nbsp;''g'') that is [[complete metric space|complete]] and [[simply connected space|simply connected]] and has everywhere non-positive [[sectional curvature]].&lt;ref name=Li2102&gt;{{cite book|last=Li|first=Peter|title=Geometric Analysis|year=2012|publisher=Cambridge University Press|isbn=9781107020641|pages=381}}&lt;/ref&gt;&lt;ref name=Lang1893&gt;{{cite book|last=Lang|first=Serge|title=Fundamentals of Differential Geometry, Volume 160|year=1989|publisher=Springer|isbn=9780387985930|pages=252–253}}&lt;/ref&gt;

==Examples==

* The [[real line]] '''R''' with its usual metric is a Hadamard manifold with constant sectional curvature equal to 0.
* Standard ''n''-dimensional [[hyperbolic space]] '''H'''&lt;sup&gt;''n''&lt;/sup&gt; is a Hadamard manifold with constant sectional curvature equal to −1.

==See also==
* [[Cartan–Hadamard theorem]]
* [[Hadamard space]]

==References==
{{reflist}}

[[Category:Riemannian manifolds]]

{{topology-stub}}</text>
      <sha1>b4dli5o2fbfq3qlgg8o8ha2kch2se0m</sha1>
    </revision>
  </page>
  <page>
    <title>Hamming distance</title>
    <ns>0</ns>
    <id>41227</id>
    <revision>
      <id>867395110</id>
      <parentid>862709290</parentid>
      <timestamp>2018-11-05T13:04:49Z</timestamp>
      <contributor>
        <username>Emergingtech1</username>
        <id>34983860</id>
      </contributor>
      <comment>Including example of use of Hamming distance in reducing interconnect energy</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12947">{{more inline|date=May 2015}}

{{multiple image
| width = 140
| image1 = Hamming distance 3 bit binary.svg
| alt1 = 3-bit binary cube
| caption1 = 3-bit binary [[cube]] for finding Hamming distance
| image2 = Hamming distance 3 bit binary example.svg
| alt2 = 3-bit binary cube Hamming distance examples
| caption2 = Two example distances: {{font color|red|100→011}} has distance 3; {{font color|blue|010→111}} has distance 2
| footer = The minimum distance between any two vertices is the Hamming distance between the two binary strings.
}}

{{multiple image
| width = 280
| image1 = Hamming distance 4 bit binary.svg
| alt1 = 4-bit binary tesseract
| caption1 = 4-bit binary [[tesseract]] for finding Hamming distance.
| image2 =Hamming distance 4 bit binary example.svg
| alt2 = 4-bit binary tesseract Hamming distance examples
| caption2 = Two example distances: {{font color|red|0100→1001}} has distance 3; {{font color|blue|0110→1110}} has distance 1
}}

In [[information theory]], the '''Hamming distance''' between two [[String (computer science)|string]]s of equal length is the number of positions at which the corresponding [[symbol]]s are different. In other words, it measures the minimum number of ''substitutions'' required to change one string into the other, or the minimum number of ''errors'' that could have transformed one string into the other. In a more general context, the Hamming distance is one of several [[string metric]]s for measuring the [[edit distance]] between two sequences. It is named after the American mathematician [[Richard Hamming]] (1915-1998).

A major application is in [[coding theory]], more specifically to [[block code]]s, in which the equal-length strings are [[Vector space|vectors]] over a [[finite field]].

==Examples==
The Hamming distance between:
* "'''&lt;/span&gt;ka&lt;span style="color:#0082ff"&gt;rol&lt;/span&gt;in&lt;/span&gt;'''" and "'''&lt;/span&gt;ka&lt;span style="color:red;"&gt;thr&lt;/span&gt;in&lt;/span&gt;'''" is 3.
* "'''&lt;/span&gt;k&lt;span style="color:#0082ff"&gt;a&lt;/span&gt;r&lt;span style="color:#0082ff"&gt;ol&lt;/span&gt;in&lt;/span&gt;'''" and "'''&lt;/span&gt;k&lt;span style="color:red;"&gt;e&lt;/span&gt;r&lt;span style="color:red;"&gt;st&lt;/span&gt;in&lt;/span&gt;'''" is 3.
* '''10&lt;span style="color:#0082ff"&gt;1&lt;/span&gt;1&lt;span style="color:#0082ff"&gt;1&lt;/span&gt;01''' and '''10&lt;span style="color:red;"&gt;0&lt;/span&gt;1&lt;span style="color:red;"&gt;0&lt;/span&gt;01''' is 2.
* '''2&lt;span style="color:#0082ff"&gt;17&lt;/span&gt;3&lt;span style="color:#0082ff"&gt;8&lt;/span&gt;96''' and '''2&lt;span style="color:red;"&gt;23&lt;/span&gt;3&lt;span style="color:red;"&gt;7&lt;/span&gt;96''' is 3.

==Properties==
For a fixed length ''n'', the Hamming distance is a [[Metric (mathematics)|metric]] on the set of the [[word (mathematics)|word]]s of length n (also known as a [[Hamming space]]), as it fulfills the conditions of non-negativity, [[identity of indiscernibles]] and symmetry, and it can be shown by [[complete induction]] that it satisfies the [[triangle inequality]] as well.&lt;ref name="Robinson2003"/&gt; The Hamming distance between two words ''a'' and ''b'' can also be seen as the [[Hamming weight]] of ''a'' &amp;minus; ''b'' for an appropriate choice of the &amp;minus; operator, much as the difference between two integers can be seen as a distance from zero on the number line.

For binary strings ''a'' and ''b'' the Hamming distance is equal to the number of ones ([[Hamming weight|population count]]) in ''a'' [[Exclusive or|XOR]] ''b''.&lt;ref name="Warren_2013"/&gt; The metric space of length-''n'' binary strings, with the Hamming distance, is known as the ''Hamming cube''; it is equivalent as a metric space to the set of distances between vertices in a [[hypercube graph]]. One can also view a binary string of length ''n'' as a vector in &lt;math&gt;\mathbb{R}^{n}&lt;/math&gt; by treating each symbol in the string as a real coordinate; with this embedding, the strings form the vertices of an ''n''-dimensional [[hypercube]], and the Hamming distance of the strings is equivalent to the [[Manhattan distance]] between the vertices.

== Error detection and error correction ==
The '''minimum Hamming distance''' is used to define some essential notions in [[coding theory]], such as [[Error detection and correction|error detecting and error correcting codes]]. In particular, a [[code (coding theory)|code]] ''C'' is said to be ''k'' error detecting if, and only if, the minimum Hamming distance between any two of its codewords is at least ''k''+1.&lt;ref name="Robinson2003"&gt;{{cite book |author-first=Derek J. S. |author-last=Robinson |title=An Introduction to Abstract Algebra |date=2003 |publisher=[[Walter de Gruyter]] |isbn=978-3-11-019816-4 |pages=255–257}}&lt;/ref&gt;

A code ''C'' is said to be ''k-errors correcting'' if, for every word ''w'' in the underlying Hamming space ''H'', there exists at most one codeword ''c'' (from ''C'') such that the Hamming distance between ''w'' and ''c'' is at most ''k''. In other words, a code is ''k''-errors correcting if, and only if, the minimum Hamming distance between any two of its codewords is at least 2''k''+1. This is more easily understood geometrically as any [[Ball (mathematics)#Balls in general metric spaces|closed balls]] of radius ''k'' centered on distinct codewords being disjoint.&lt;ref name="Robinson2003"/&gt; These balls are also called ''[[Hamming sphere]]s'' in this context.&lt;ref name="cc17"/&gt;

Thus a code with minimum Hamming distance ''d'' between its codewords can detect at most ''d''-1 errors and can correct ⌊(''d''-1)/2⌋ errors.&lt;ref name="Robinson2003"/&gt; The latter number is also called the ''[[Sphere packing#Other spaces|packing radius]]'' or the ''error-correcting capability'' of the code.&lt;ref name="cc17"&gt;{{citation |title=Covering Codes |volume=54 |series=North-Holland Mathematical Library |author-first1=G. |author-last1=Cohen |author-first2=I. |author-last2=Honkala |author-first3=S. |author-last3=Litsyn |author-first4=A. |author-last4=Lobstein |publisher=[[Elsevier]] |date=1997 |isbn=9780080530079 |pages=16–17}}&lt;/ref&gt;

==History and applications==

The Hamming distance is named after [[Richard Hamming]], who introduced the concept in his fundamental paper on [[Hamming code]]s ''Error detecting and error correcting codes'' in 1950.&lt;ref&gt;{{Cite journal|last=Hamming|first=R. W.|date=April 1950|title=Error detecting and error correcting codes|url=http://ieeexplore.ieee.org/document/6772729/|journal=The Bell System Technical Journal|volume=29|issue=2|pages=147–160|doi=10.1002/j.1538-7305.1950.tb00463.x|issn=0005-8580}}&lt;/ref&gt; Hamming weight analysis of bits is used in several disciplines including [[information theory]], [[coding theory]], and [[cryptography]].

It is used in [[telecommunication]] to count the number of flipped bits in a fixed-length binary word as an estimate of error, and therefore is sometimes called the '''signal distance'''.&lt;ref name="Ayala2012"&gt;{{cite book |author-first=Jose |author-last=Ayala |title=Integrated Circuit and System Design |date=2012 |publisher=[[Springer Science+Business Media|Springer]] |isbn=978-3-642-36156-2 |page=62}}&lt;/ref&gt; For ''q''-ary strings over an [[alphabet]] of size ''q''&amp;nbsp;≥&amp;nbsp;2 the Hamming distance is applied in case of the [[Binary symmetric channel|q-ary symmetric channel]], while the [[Lee distance]] is used for [[phase-shift keying]] or more generally channels susceptible to [[synchronization error]]s because the Lee distance accounts for errors of ±1.&lt;ref name="Roth2006"&gt;{{cite book |author-first=Ron |author-last=Roth |title=Introduction to Coding Theory |date=2006 |publisher=[[Cambridge University Press]] |isbn=978-0-521-84504-5 |page=298}}&lt;/ref&gt; If &lt;math&gt;q = 2&lt;/math&gt; or &lt;math&gt;q = 3&lt;/math&gt; both distances coincide because any pair of elements from &lt;math display="inline"&gt;\mathbb{Z}/2\mathbb{Z}&lt;/math&gt; or &lt;math display="inline"&gt;\mathbb{Z}/3\mathbb{Z}&lt;/math&gt; differ by 1, but the distances are different for larger &lt;math&gt;q&lt;/math&gt;.

The Hamming distance is also used in [[systematics]] as a measure of genetic distance.&lt;ref&gt;{{Cite journal|last=Pilcher|first=Christopher D.|last2=Wong|first2=Joseph K.|last3=Pillai|first3=Satish K.|date=2008-03-18|title=Inferring HIV Transmission Dynamics from Phylogenetic Sequence Relationships|url=http://journals.plos.org/plosmedicine/article?id=10.1371/journal.pmed.0050069|journal=PLOS Medicine|language=en|volume=5|issue=3|pages=e69|doi=10.1371/journal.pmed.0050069|issn=1549-1676}}&lt;/ref&gt;

However, for comparing strings of different lengths, or strings where not just substitutions but also insertions or deletions have to be expected, a more sophisticated metric like the [[Levenshtein distance]] is more appropriate.

In processor interconnects, the dynamic energy consumption depends on the number of transitions. With level-signaling scheme, the number of transitions depends on Hamming distance between consecutively transmitted buses.&lt;ref&gt;"[https://www.academia.edu/37704595/A_Survey_of_Encoding_Techniques_for_Reducing_Data-Movement_Energy A Survey of Encoding Techniques for Reducing Data-Movement Energy]", JSA, 2018&lt;/ref&gt; Hence, by reducing this Hamming distance, the data-movement energy can be reduced.  

== Algorithm example ==
The function &lt;code&gt;hamming_distance()&lt;/code&gt;, implemented in [[Python (programming language)|Python 2.3+]], computes the Hamming distance between
two strings (or other [[Iterator|iterable]] objects) of equal length by creating a sequence of Boolean values indicating mismatches and matches between corresponding positions in the two inputs and then summing the sequence with False and True values being interpreted as zero and one.
{{Clear}}

&lt;syntaxhighlight lang="python"&gt;
def hamming_distance(s1, s2):
    """Return the Hamming distance between equal-length sequences"""
    if len(s1) != len(s2):
        raise ValueError("Undefined for sequences of unequal length")
    return sum(el1 != el2 for el1, el2 in zip(s1, s2))
&lt;/syntaxhighlight&gt;
where the [https://docs.python.org/2/library/functions.html#zip zip()] function merges two equal-length collections in pairs.
 
The following [[C (programming language)|C]] function will compute the Hamming distance of two integers (considered as binary values, that is, as sequences of bits). The running time of this procedure is proportional to the Hamming distance rather than to the number of bits in the inputs. It computes the [[bitwise operation|bitwise]] [[exclusive or]] of the two inputs, and then finds the [[Hamming weight]] of the result (the number of nonzero bits) using an algorithm of {{harvtxt|Wegner|1960}} that repeatedly finds and clears the lowest-order nonzero bit.  Some compilers support the [[Hamming weight#Language support|__builtin_popcount]] function which can calculate this using specialized processor hardware where available.

&lt;syntaxhighlight lang="c"&gt;
int hamming_distance(unsigned x, unsigned y)
{
    int dist = 0;
    unsigned  val = x ^ y;

    // Count the number of bits set
    while (val != 0)
    {
        // A bit is set, so increment the count and clear the bit
        dist++;
        val &amp;= val - 1;
    }

    // Return the number of differing bits
    return dist;
}
&lt;/syntaxhighlight&gt;

Or, a much faster hardware alternative (for compilers that support builtins) is to use popcount like so.

&lt;syntaxhighlight lang="c"&gt;
int hamming_distance(unsigned x, unsigned y)
{
    return __builtin_popcount(x ^ y);
}
//if your compiler supports 64-bit integers
int hamming_distance(unsigned long long x, unsigned long long y)
{
    return __builtin_popcountll(x ^ y);
}
&lt;/syntaxhighlight&gt;

==See also==
{{Portal|Mathematics}}
* [[Closest string]]
* [[Damerau–Levenshtein distance]]
* [[Euclidean distance]]
* [[Mahalanobis distance]]
* [[Jaccard index]]
* [[Sørensen similarity index]]
* [[Word ladder]]
* [[Gray code]]
* [[Levenshtein distance]]
* [[Sparse distributed memory]]

==References==
{{Reflist|refs=
&lt;ref name="Warren_2013"&gt;{{cite book |title=Hacker's Delight |title-link=Hacker's Delight |author-first=Henry S. |author-last=Warren, Jr. |date=2013 |orig-year=2002 |edition=2 |publisher=[[Addison Wesley]] - [[Pearson Education, Inc.]] |isbn=978-0-321-84268-8 |id=0-321-84268-5 |pages=81-96}}&lt;/ref&gt;
}}

==Further reading==
*{{FS1037C}}
*{{cite journal
 |author-last=Wegner |author-first=Peter |author-link=Peter Wegner
 |doi=10.1145/367236.367286
 |issue=5
 |journal=[[Communications of the ACM]]
 |page=322
 |title=A technique for counting ones in a binary computer
 |volume=3
 |date=1960}}
* {{cite book |author-link=David J. C. MacKay |author-last=MacKay |author-first=David J. C. |url=http://www.inference.phy.cam.ac.uk/mackay/itila/book.html |title=Information Theory, Inference, and Learning Algorithms |location=Cambridge |publisher=[[Cambridge University Press]] |date=2003 |isbn=0-521-64298-1}}

{{Strings}}

[[Category:String similarity measures]]
[[Category:Coding theory]]
[[Category:Articles with example Python code]]
[[Category:Articles with example C++ code]]
[[Category:Metric geometry]]
[[Category:Cubes]]
[[Category:Similarity and distance measures]]</text>
      <sha1>ogom46k6t1nlduthkobnbmmt5y6559i</sha1>
    </revision>
  </page>
  <page>
    <title>IBM hexadecimal floating point</title>
    <ns>0</ns>
    <id>537969</id>
    <revision>
      <id>850695916</id>
      <parentid>849921939</parentid>
      <timestamp>2018-07-17T14:10:30Z</timestamp>
      <contributor>
        <username>Matthiaspaul</username>
        <id>13467261</id>
      </contributor>
      <comment>improved ref</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="18557">[[IBM]] [[System/360]] computers, and subsequent machines based on that architecture (mainframes), support a '''[[hexadecimal]] [[floating point|floating-point]]''' format ('''HFP''').&lt;ref&gt;[http://www.bitsavers.org/pdf/ibm/360/princOps/A22-6821-6_360PrincOpsJan67.pdf ''IBM System/360 Principles of Operation''], IBM Publication A22-6821-6, Seventh Edition (January 13, 1967), pp.41-50&lt;/ref&gt;&lt;ref&gt;[http://www.bitsavers.org/pdf/ibm/370/princOps/GA22-7000-4_370_Principles_Of_Operation_Sep75.pdf ''IBM System/370 Principles of Operation''], IBM Publication GA22-7000-4, Fifth Edition (September 1, 1975), pp.157-170&lt;/ref&gt;&lt;ref&gt;[http://publibz.boulder.ibm.com/epubs/pdf/dz9zr001.pdf ''z/Architecture Principles of Operation''], IBM Publication SA22-7832-01, Second Edition (October, 2001), chapter 9 ff.&lt;/ref&gt;

In comparison to [[IEEE 754]] floating-point, the IBM floating-point format has a longer [[significand]], and a shorter [[exponent]]. All IBM floating-point formats have 7 bits of exponent with a [[exponent bias|bias]] of 64. The [[normalized mantissa|normalized]] range of representable numbers is from 16&lt;sup&gt;−65&lt;/sup&gt; to 16&lt;sup&gt;63&lt;/sup&gt; (approx. 5.39761 × 10&lt;sup&gt;−79&lt;/sup&gt; to 7.237005 × 10&lt;sup&gt;75&lt;/sup&gt;).

The number is represented as the following formula: (−1)&lt;sup&gt;sign&lt;/sup&gt; × 0.&lt;small&gt;significand&lt;/small&gt; × 16&lt;sup&gt;exponent−64&lt;/sup&gt;.

== Single-precision 32-bit ==
A [[single precision|single-precision]] binary floating-point number (called "short" by IBM) is stored in a 32-bit word:

:{|
|- style="text-align:center"
|style="width:20px"|1
|style="width:20px"|
|style="width:50px"|7
|style="width:20px"|
|style="width:20px"|
|style="width:210px"|24
|style="width:20px"|
|style="text-align:left"|''(width in bits)''
|- style="text-align:center"
|colspan="1" style="text-align:center;background-color:#FC9"|S
|colspan="3" style="text-align:center;background-color:#99F"|Exp
|colspan="3" style="text-align:center;background-color:#9F9"|Fraction
|colspan="1" style="text-align:center;background-color:#FFF"|&amp;nbsp;
|- style="text-align:center"
|31
|30
|...
|24
|23
|...
|0
|align="left"|''(bit index)''*
|-
|colspan="8"| ''* IBM documentation numbers the bits from left to right, so that the most significant bit is designated as bit number 0.''
|}

In this format the initial bit is not suppressed, and the
radix point is set to the left of the mantissa in increments of 4 bits.

Since the base is 16, the exponent in this form is about twice as large as the equivalent in IEEE 754, in order to have similar exponent range in binary, 9 exponent bits would be required.

=== Example ===
Consider encoding the value −118.625 as an IBM single-precision floating-point value.

The value is negative, so the sign bit is 1.

The value 118.625&lt;sub&gt;10&lt;/sub&gt; in binary is 1110110.101&lt;sub&gt;2&lt;/sub&gt;. This value is normalized by moving the radix point left four bits (one hexadecimal digit) at a time until the leftmost digit is zero, yielding 0.01110110101&lt;sub&gt;2&lt;/sub&gt;. The remaining rightmost digits are padded with zeros, yielding a 24-bit fraction of .0111&amp;nbsp;0110&amp;nbsp;1010&amp;nbsp;0000&amp;nbsp;0000&amp;nbsp;0000&lt;sub&gt;2&lt;/sub&gt;.

The normalized value moved the radix point two digits to the left, yielding a multiplier and exponent of 16&lt;sup&gt;+2&lt;/sup&gt;. A bias of +64 is added to the exponent (+2), yielding +66, which is 100&amp;nbsp;0010&lt;sub&gt;2&lt;/sub&gt;.

Combining the sign, exponent plus bias, and normalized fraction produces this encoding:

:{|
|- style="text-align:center"
|style="width:20px;text-align:center;background-color:#FC9"|S
|style="width:90px;text-align:center;background-color:#99F"|Exp
|style="width:250px;text-align:center;background-color:#9F9"|Fraction
|style="text-align:center;background-color:#FFF"|&amp;nbsp;
|- style="text-align:center"
|style="text-align:center;background-color:#FEC"|&lt;tt&gt;1&lt;/tt&gt;
|style="text-align:center;background-color:#CCF"|&lt;tt&gt;100 0010&lt;/tt&gt;
|style="text-align:center;background-color:#CFC"|&lt;tt&gt;0111 0110 1010 0000 0000 0000&lt;/tt&gt;
|style="text-align:center;background-color:#FFF"|&amp;nbsp;
|}

In other words, the number represented is −0.76A000&lt;sub&gt;16&lt;/sub&gt; × 16&lt;sup&gt;66 − 64&lt;/sup&gt; = −0.4633789&amp;hellip; × 16&lt;sup&gt;+2&lt;/sup&gt; = −118.625

=== Largest representable number ===
:{|
|- style="text-align:center"
|style="width:20px;text-align:center;background-color:#FC9"|S
|style="width:90px;text-align:center;background-color:#99F"|Exp
|style="width:250px;text-align:center;background-color:#9F9"|Fraction
|style="text-align:center;background-color:#FFF"|&amp;nbsp;
|- style="text-align:center"
|style="text-align:center;background-color:#FEC"|&lt;tt&gt;0&lt;/tt&gt;
|style="text-align:center;background-color:#CCF"|&lt;tt&gt;111 1111&lt;/tt&gt;
|style="text-align:center;background-color:#CFC"|&lt;tt&gt;1111 1111 1111 1111 1111 1111&lt;/tt&gt;
|style="text-align:center;background-color:#FFF"|&amp;nbsp;
|}

The number represented is +0.FFFFFF&lt;sub&gt;16&lt;/sub&gt; × 16&lt;sup&gt;127 − 64&lt;/sup&gt; = (1 − 16&lt;sup&gt;−6&lt;/sup&gt;) × 16&lt;sup&gt;63&lt;/sup&gt; ≈ +7.2370051 × 10&lt;sup&gt;75&lt;/sup&gt;

=== Smallest positive normalized number ===
:{|
|- style="text-align:center"
|style="width:20px;text-align:center;background-color:#FC9"|S
|style="width:90px;text-align:center;background-color:#99F"|Exp
|style="width:250px;text-align:center;background-color:#9F9"|Fraction
|style="text-align:center;background-color:#FFF"|&amp;nbsp;
|- style="text-align:center"
|style="text-align:center;background-color:#FEC"|&lt;tt&gt;0&lt;/tt&gt;
|style="text-align:center;background-color:#CCF"|&lt;tt&gt;000 0000&lt;/tt&gt;
|style="text-align:center;background-color:#CFC"|&lt;tt&gt;0001 0000 0000 0000 0000 0000&lt;/tt&gt;
|style="text-align:center;background-color:#FFF"|&amp;nbsp;
|}

The number represented is +0.1&lt;sub&gt;16&lt;/sub&gt; × 16&lt;sup&gt;0 − 64&lt;/sup&gt; = 16&lt;sup&gt;−1&lt;/sup&gt; × 16&lt;sup&gt;−64&lt;/sup&gt; ≈ +5.397605 × 10&lt;sup&gt;−79&lt;/sup&gt;

=== Zero ===
:{|
|- style="text-align:center"
|style="width:20px;text-align:center;background-color:#FC9"|S
|style="width:90px;text-align:center;background-color:#99F"|Exp
|style="width:250px;text-align:center;background-color:#9F9"|Fraction
|style="text-align:center;background-color:#FFF"|&amp;nbsp;
|- style="text-align:center"
|style="text-align:center;background-color:#FEC"|&lt;tt&gt;0&lt;/tt&gt;
|style="text-align:center;background-color:#CCF"|&lt;tt&gt;000 0000&lt;/tt&gt;
|style="text-align:center;background-color:#CFC"|&lt;tt&gt;0000 0000 0000 0000 0000 0000&lt;/tt&gt;
|style="text-align:center;background-color:#FFF"|&amp;nbsp;
|}

Zero (0.0) is represented in normalized form as all zero bits, which is arithmetically the value +0.0&lt;sub&gt;16&lt;/sub&gt; × 16&lt;sup&gt;0 − 64&lt;/sup&gt; = +0 × 16&lt;sup&gt;−64&lt;/sup&gt; ≈ +0.000000 × 10&lt;sup&gt;−79&lt;/sup&gt; = 0. Given a significand of all-bits zero, any combination of positive or negative sign bit and a non-zero biased exponent will yield a value arithmetically equal to zero. However, the normalized form generated for zero by CPU hardware is all-bits zero. This is true for all three floating-point precision formats.

=== Precision issues ===
Since the base is 16, there can be up to three leading zero bits in the binary significand. That means when the number is converted into binary, there can be as few as 21 bits of precision. Because of the "wobbling precision" effect, this can cause some calculations to be very inaccurate.

A good example of the inaccuracy is representation of decimal value 0.1. It has no exact binary or hexadecimal representation. In hexadecimal format, it is represented as 0.19999999...&lt;sub&gt;16&lt;/sub&gt; or 0.0001 1001 1001 1001 1001 1001 1001...&lt;sub&gt;2&lt;/sub&gt;, that is:

:{|
|- style="text-align:center"
|style="width:20px;text-align:center;background-color:#FC9"|S
|style="width:90px;text-align:center;background-color:#99F"|Exp
|style="width:250px;text-align:center;background-color:#9F9"|Fraction
|style="text-align:center;background-color:#FFF"|&amp;nbsp;
|- style="text-align:center"
|style="text-align:center;background-color:#FEC"|&lt;tt&gt;0&lt;/tt&gt;
|style="text-align:center;background-color:#CCF"|&lt;tt&gt;100 0000&lt;/tt&gt;
|style="text-align:center;background-color:#CFC"|&lt;tt&gt;0001 1001 1001 1001 1001 1010&lt;/tt&gt;
|style="text-align:center;background-color:#FFF"|&amp;nbsp;
|}

This has only 21 bits, whereas the binary version has 24 bits of precision.

Six hexadecimal digits of precision is roughly equivalent to six decimal digits (i.e. (6 − 1) log&lt;sub&gt;10&lt;/sub&gt;(16) ≈ 6.02). A conversion of single precision hexadecimal float to decimal string would require at least 9 significant digits (i.e. 6 log&lt;sub&gt;10&lt;/sub&gt;(16) + 1 ≈ 8.22) in order to convert back to the same hexadecimal float value.

== Double-precision 64-bit ==
The [[Double precision|double-precision]] floating-point format (called "long" by IBM) is the same as the "short" format except that the mantissa (fraction) field is wider and the double-precision number is stored in a double word (8 bytes):

:{|
|- style="text-align:center"
|style="width:20px"|1
|style="width:20px"|
|style="width:50px"|7
|style="width:20px"|
|style="width:20px"|
|style="width:320px"|56
|style="width:20px"|
|style="text-align:left"|''(width in bits)''
|- style="text-align:center"
|colspan="1" style="text-align:center;background-color:#FC9"|S
|colspan="3" style="text-align:center;background-color:#99F"|Exp
|colspan="3" style="text-align:center;background-color:#9F9"|Fraction
|colspan="1" style="text-align:center;background-color:#FFF"|&amp;nbsp;
|- style="text-align:center"
|63
|62
|...
|56
|55
|...
|0
|align="left"|''(bit index)''*
|-
|colspan="8"| ''* IBM documentation numbers the bits from left to right, so that the most significant bit is designated as bit number 0.''
|}

The exponent for this format covers only about a quarter of the range as the corresponding IEEE binary format.

14 hexadecimal digits of precision is roughly equivalent to 17 decimal digits. A conversion of double precision hexadecimal float to decimal string would require at least 18 significant digits in order to convert back to the same hexadecimal float value.

== Extended-precision 128-bit ==
Called extended-precision by IBM, a  [[Quadruple precision floating-point format|quadruple-precision]]  floating-point format was added to the System/370 series and was available on some S/360 models (S/360-85, -195, and others by special request or simulated by OS software). The extended-precision mantissa (fraction) field is wider, and the extended-precision number is stored as two double words (16 bytes):

:{|
|-
|colspan="8" |'''High-order part'''
|- style="text-align:center"
|style="width:20px"|1
|style="width:20px"|
|style="width:50px"|7
|style="width:20px"|
|style="width:20px"|
|style="width:320px"|56
|style="width:20px"|
|style="text-align:left"|''(width in bits)''
|- style="text-align:center"
|colspan="1" style="text-align:center;background-color:#FC9"|S
|colspan="3" style="text-align:center;background-color:#99F"|Exp
|colspan="3" style="text-align:center;background-color:#9F9"|Fraction (high-order 14 digits)
|colspan="1" style="text-align:center;background-color:#FFF"|&amp;nbsp;
|- style="text-align:center"
|127
|126
|...
|120
|119
|...
|64
|align="left"|''(bit index)''*
|-
|colspan="8" |'''Low-order part'''
|- style="text-align:center"
|colspan="4"|8
|
|56
|
|style="text-align:left"|''(width in bits)''
|- style="text-align:center"
|colspan="4" style="text-align:center;background-color:#CCC"|Unused
|colspan="3" style="text-align:center;background-color:#9F9"|Fraction (low-order 14 digits)
|colspan="1" style="text-align:center;background-color:#FFF"|&amp;nbsp;
|- style="text-align:center"
|63
|colspan="2"|...
|56
|55
|...
|0
|align="left"|''(bit index)''*
|-
|colspan="8"| ''* IBM documentation numbers the bits from left to right, so that the most significant bit is designated as bit number 0.''
|}

28 hexadecimal digits of precision is roughly equivalent to 32 decimal digits. A conversion of extended precision hexadecimal float to decimal string would require at least 35 significant digits in order to convert back to the same hexadecimal float value.

== Arithmetic operations ==
Most arithmetic operations truncate like simple pocket calculators. Therefore, 1 − 16&lt;sup&gt;−7&lt;/sup&gt; = 1. In this case, the result is rounded away from zero.&lt;ref&gt;[ftp://public.dhe.ibm.com/software/websphere/awdtools/hlasm/sh93fpov.pdf ESA/390 Enhanced Floating Point Support: An Overview]&lt;/ref&gt;

== IEEE 754 on IBM mainframes ==
Starting with the [[S/390]] G5 in 1998,&lt;ref&gt;{{Cite journal| last1 = Schwarz | first1 = E. M.| last2 = Krygowski | first2 = C. A.| doi = 10.1147/rd.435.0707| title = The S/390 G5 floating-point unit| journal = [[IBM Journal of Research and Development]] | volume = 43| issue = 5.6| pages = 707–721| date=September 1999 }}&lt;/ref&gt; IBM mainframes have also included IEEE binary floating-point units which conform to the [[IEEE 754|IEEE 754 Standard for Floating-Point Arithmetic]]. IEEE decimal floating-point was added to [[IBM System z9]] GA2&lt;ref&gt;{{Cite journal| last1 = Duale | first1 = A. Y.| last2 = Decker | first2 = M. H.| last3 = Zipperer | first3 = H. -G.| last4 = Aharoni | first4 = M.| last5 = Bohizic | first5 = T. J.| title = Decimal floating-point in z9: An implementation and testing perspective| doi = 10.1147/rd.511.0217| journal = [[IBM Journal of Research and Development]] | volume = 51| issue = 1.2| pages = 217–227| date=January 2007 }}&lt;/ref&gt; in 2007 using [[millicode]]&lt;ref&gt;{{Cite journal| last1 = Heller | first1 = L. C.| last2 = Farrell | first2 = M. S.| doi = 10.1147/rd.483.0425| title = Millicode in an IBM zSeries processor| journal = [[IBM Journal of Research and Development]] | volume = 48| issue = 3.4| pages = 425–434| date=May 2004 }}&lt;/ref&gt; and in 2008 to the [[IBM System z10]] in hardware.&lt;ref&gt;{{Cite journal| last1 = Schwarz | first1 = E. M.| last2 = Kapernick | first2 = J. S.| last3 = Cowlishaw | first3 = M. F.| title = Decimal floating-point support on the IBM System z10 processor| doi = 10.1147/JRD.2009.5388585| journal = [[IBM Journal of Research and Development]] | volume = 53| issue = 1| pages = 4:1–4:10| date=January 2009 }}&lt;/ref&gt;

Modern IBM mainframes support three floating-point radices with 3 hexadecimal (HFP) formats, 3 binary (BFP) formats, and 3 decimal (DFP) formats. There are two floating-point units per core; one supporting HFP and BFP, and one supporting DFP; there is one register file, FPRs, which holds all 3 formats.

== Special uses ==
The IBM floating-point format is used in:
* SAS 5 Transport files (.XPT) as required by the [[Food and Drug Administration]] (FDA) for New Drug Application (NDA) study submissions&lt;ref name=SAS&gt;{{cite web |url=http://support.sas.com/techsup/technote/ts140.pdf |title=The Record Layout of a Data Set in SAS Transport (XPORT) Format |accessdate=September 18, 2014}},&lt;/ref&gt;
* [[GRIB]] (GRIdded Binary) data files to exchange the output of weather prediction models (IEEE [[single-precision floating-point format]] in current version),
* [[GDSII|GDS II]] (Graphic Database System II) format files ([[Open Artwork System Interchange Standard|OASIS]] is the replacement), and
* [[SEG Y]] (Society of Exploration Geophysicists Y) format files (single-precision floating-point was added to the format in 2002).&lt;ref&gt;http://www.seg.org/documents/10161/77915/seg_y_rev1.pdf&lt;/ref&gt;

As IBM is the only remaining provider of hardware (and only in their mainframes) using their non-standard floating-point format, no popular file format requires it; Except the FDA requires the SAS file format and "All floating-point numbers in the file are stored using the IBM mainframe representation. [..] Most platforms use the IEEE representation for floating-point numbers. [..] To assist you in reading and/or writing transport files, we are providing routines to convert from IEEE representation (either big endian or little endian) to transport representation and back again."&lt;ref name=SAS/&gt; Code for IBM's format is also available under [[GNU Lesser General Public License|LGPLv2.1]].&lt;ref&gt;https://cran.r-project.org/web/packages/SASxport/SASxport.pdf&lt;/ref&gt;

== Systems that use the IBM floating-point format ==
* [[IBM System/360]]
* [[RCA Spectra 70]]
* [[English Electric System 4]]
* [[GEC 4000 series]] minicomputers
* [[Interdata]] 16-bit and 32-bit computers
* [[Texas Instruments]] [[TI-990|990]]/12

== See also ==
* [[IEEE 754|IEEE 754 Standard for Floating-Point Arithmetic]]
* [[Microsoft Binary Format]]

== References ==
{{Reflist}}

== Further reading ==
* {{Cite journal| last1 = Sweeney | first1 = D. W.| doi = 10.1147/sj.41.0031| title = An analysis of floating-point addition| journal = [[IBM Systems Journal]] | volume = 4| issue = 1| pages = 31–42| year = 1965}}
* {{cite journal |last1=Tomayko | first1=J. |date=Summer 1995 |title=System 360 Floating-Point Problems |journal=[[IEEE Annals of the History of Computing]] |volume=17 |issue=2 |pages=62–63 |ISSN=1058-6180 |doi=10.1109/MAHC.1995.10006 |url=http://doi.ieeecomputersociety.org/10.1109/MAHC.1995.10006}}
* Harding, L. J. (1966), "Idiosyncrasies of System/360 Floating-Point", ''[http://discover.lib.umn.edu/cgi/f/findaid/findaid-idx?c=umfa;cc=umfa;rgn=main;view=text;didno=cbi00021 Proceedings of SHARE 27, Aug. 8–12 1966]'', Presented at  SHARE XXVII, Toronto, Canada
* Harding, L. J. (1966), "Modifications of System/360 Floating Point", ''[http://discover.lib.umn.edu/cgi/f/findaid/findaid-idx?c=umfa;cc=umfa;rgn=main;view=text;didno=cbi00021 SHARE Secretary Distribution]'', pp.&amp;nbsp;11–27, SSD 157, C4470
* {{Cite journal |author-last1=Anderson |author-first1=Stanley F. |author-last2=Earle |author-first2=John G. |author-last3=Goldschmidt |author-first3=Robert Elliott |author-last4=Powers |author-first4=Don M. |title=The IBM System/360 Model 91: Floating-Point Execution Unit |doi=10.1147/rd.111.0034 |journal=[[IBM Journal of Research and Development]] |volume=11 |issue=1 |pages=34–53 |date=January 1967}}
* {{Cite journal| last1 = Padegs | first1 = A.| doi = 10.1147/sj.71.0022| title = Structural aspects of the System/360 Model 85, III: Extensions to floating-point architecture| journal = [[IBM Systems Journal]] | volume = 7| issue = 1| pages = 22–29| year = 1968}}
* {{Cite journal| last1 = Schwarz | first1 = E. M.| last2 = Sigal | first2 = L.| last3 = McPherson | first3 = T. J.| doi = 10.1147/rd.414.0475| title = CMOS floating-point unit for the S/390 Parallel Enterprise Server G4| journal = [[IBM Journal of Research and Development]] | volume = 41| issue = 4.5| pages = 475–488| date=July 1997 }}

{{DEFAULTSORT:IBM Floating Point Architecture}}
[[Category:Computer arithmetic]]
[[Category:IBM System/360 mainframe line|Floating Point Architecture]]
[[Category:Floating point]]
[[Category:Floating point types]]</text>
      <sha1>pxvu2e7cpou6klqzb12r9t5bob5l2s7</sha1>
    </revision>
  </page>
  <page>
    <title>Ian G. Macdonald</title>
    <ns>0</ns>
    <id>3100378</id>
    <revision>
      <id>825489829</id>
      <parentid>825489641</parentid>
      <timestamp>2018-02-13T18:03:58Z</timestamp>
      <contributor>
        <ip>86.170.37.87</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5260">{{BLP sources|date=February 2013}}
{{Use dmy dates|date=November 2012}}
{{Use British English|date=November 2012}}
[[File:Ian Macdonald.jpg|thumb|Ian G. Macdonald at [[Mathematical Research Institute of Oberwolfach|Oberwolfach]] in 1977]]
'''Ian Grant Macdonald''' {{post-nominals|country=GBR|FRS}} (born 11 October 1928 in London, England) is a British [[mathematician]] known for his contributions to [[symmetric function]]s, [[special function]]s, [[Lie algebra]] theory and other aspects of [[algebra]], [[algebraic combinatorics]], and [[combinatorics]].

He was educated at [[Winchester College]] and [[Trinity College, Cambridge]], graduating in 1952. He then spent five years as a [[civil servant]]. He was offered a position at [[Manchester University]] in 1957 by [[Max Newman]], on the basis of work he had done while outside academia. In 1960 he moved to the [[University of Exeter]], and in 1963 became a Fellow of [[Magdalen College, Oxford]]. He became [[Fielden Chair of Pure Mathematics|Fielden Professor]] at Manchester in 1972, and professor at [[Queen Mary, University of London|Queen Mary College, University of London]], in 1976.

He worked on [[symmetric products of algebraic curves]], [[Jordan algebra]]s and the [[representation theory]] of groups over [[local field]]s. In 1972 he proved the [[Macdonald identities]], after a pattern known to [[Freeman Dyson]]. His 1979 book ''Symmetric Functions and Hall Polynomials'' has become a classic. Symmetric functions are an old theory, part of the [[theory of equations]], to which both [[K-theory]] and [[representation theory]] lead. His was the first text to integrate much classical theory, such as [[Hall polynomial]]s, [[Schur polynomial|Schur functions]], the [[Littlewood–Richardson rule]], with the [[abstract algebra]] approach. It was both an expository work and, in part, a research monograph, and had a major impact in the field. The [[Macdonald polynomial]]s are now named after him. The [[Macdonald conjecture (disambiguation)|Macdonald conjectures]] from 1982 also proved most influential.

Macdonald was elected a [[Fellow of the Royal Society]] in 1979. In 1991 he received the [[Pólya Prize (LMS)|Pólya Prize]] of the [[London Mathematical Society]].&lt;ref&gt;{{cite web| url = http://royalsociety.org/uploadedFiles/Royal_Society_Content/about-us/fellowship/Fellows_and_Foreign_Members.pdf| title= Fellows of the Royal Society|publisher= Royal Society|accessdate = 28 July 2013}}&lt;/ref&gt; He was awarded the 2009 [[Steele Prize]] for Mathematical Exposition. In 2012 he became a fellow of the [[American Mathematical Society]].&lt;ref&gt;[http://www.ams.org/profession/fellows-list List of Fellows of the American Mathematical Society], retrieved 2013-02-02.&lt;/ref&gt;

==Selected publications==
*Macdonald, I. G. ''Affine Hecke Algebras and Orthogonal Polynomials.'' Cambridge Tracts in Mathematics, 157. Cambridge University Press, Cambridge, 2003. x+175 pp.&amp;nbsp;{{ISBN|0-521-82472-9}} {{MathSciNet|id=1976581}}
*Macdonald, I. G. ''Symmetric Functions and Hall Polynomials.'' Second edition. Oxford Mathematical Monographs. Oxford Science Publications. The Clarendon Press, Oxford University Press, New York, 1995. x+475 pp.&amp;nbsp;{{ISBN|0-19-853489-2}} {{MathSciNet|id=1354144}} {{cite book|title=1st edition|year=1979}}&lt;ref&gt;{{cite journal|author=Stanley, Richard P.|authorlink=Richard P. Stanley|title=Review: ''Symmetric functions and Hall polynomials'' by I. G. Macdonald|journal=Bull. Amer. Math. Soc. (N.S.)|year=1981|volume=4|issue=2|pages=254–265|url=http://www.ams.org/journals/bull/1981-04-02/S0273-0979-1981-14902-8/S0273-0979-1981-14902-8.pdf|doi=10.1090/s0273-0979-1981-14902-8}}&lt;/ref&gt;
*Macdonald, I. G. ''Symmetric Functions and Orthogonal Polynomials.'' Dean Jacqueline B. Lewis Memorial Lectures presented at Rutgers University, New Brunswick, New Jersey. University Lecture Series, 12. American Mathematical Society, Providence, Rhode Island, 1998. xvi+53 pp.&amp;nbsp;{{ISBN|0-8218-0770-6}} {{MathSciNet|id=1488699}}
*[[Michael Atiyah|Atiyah, M. F.]]; Macdonald, I. G. ''Introduction to Commutative Algebra.'' Addison-Wesley Publishing Co., Reading, Mass.-London-Don Mills, Ont., 1969. ix+128 pp.&amp;nbsp;{{ISBN|0-201-40751-5}} {{MathSciNet|id=0242802}}

==References==
{{reflist}}

==External links==
*{{MathGenealogy |id=87545}}
*[https://web.archive.org/web/20051030042929/http://www.maths.leeds.ac.uk/~vadim/Morris3.pdf Biographical notice]

{{s-start}}
{{succession box | before=[[Frank Adams]] | title=[[Fielden Chair of Pure Mathematics]] | after= Norman Blackburn | years=}}
{{s-end}}

{{Authority control}}

{{DEFAULTSORT:Macdonald, Ian Grant}}
[[Category:1928 births]]
[[Category:Living people]]
[[Category:People educated at Winchester College]]
[[Category:Alumni of Trinity College, Cambridge]]
[[Category:English mathematicians]]
[[Category:Symmetric functions]]
[[Category:Academics of Queen Mary University of London]]
[[Category:Academics of the University of Manchester]]
[[Category:Fellows of Magdalen College, Oxford]]
[[Category:Fellows of the American Mathematical Society]]
[[Category:Fellows of the Royal Society]]
[[Category:21st-century British mathematicians]]
[[Category:20th-century British mathematicians]]
[[Category:People from London]]</text>
      <sha1>os44gf2jpsjcpj9808ylepr6vfq0c8c</sha1>
    </revision>
  </page>
  <page>
    <title>Jon T. Pitts</title>
    <ns>0</ns>
    <id>46721841</id>
    <revision>
      <id>857361786</id>
      <parentid>822506822</parentid>
      <timestamp>2018-08-31T05:40:14Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>/* External links */add authority control, test</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2214">'''Jon T. Pitts''' (born 1948) is an [[United States|American]] [[mathematician]] working on [[geometric analysis]] and [[variational calculus]]. He is a professor at [[Texas A&amp;M University]].

Pitts obtained his Ph.D. from [[Princeton University]] in 1974 under the supervision of [[Frederick Almgren|Frederick Almgren, Jr.]], with the thesis ''Every Compact Three-Dimensional Manifold Contains Two-Dimensional Minimal Submanifolds''.&lt;ref&gt;{{MathGenealogy|id=17342}}&lt;/ref&gt;

He received a [[Sloan Fellowship]] in 1981.&lt;ref&gt;{{cite web|url=http://www.sloan.org/sloan-research-fellowships/past-fellows/?tx_sloangrants_sloanfellows%5Bpage%5D=65&amp;tx_sloangrants_sloanfellows%5BlastPage%5D=188&amp;tx_sloangrants_sloanfellows%5Bsortby%5D=2&amp;tx_sloangrants_sloanfellows%5Border%5D=2&amp;tx_sloangrants_sloanfellows%5Bsearch%5D=&amp;tx_sloangrants_sloanfellows%5Bcontroller%5D=Fellows&amp;cHash=767d61f4d048e9cc78ee91b7c787ed4a |title=Past Fellows |publisher=Sloan.org |date=2012-07-18 |accessdate=2015-05-16}}&lt;/ref&gt;

The [[Almgren–Pitts min-max theory]] is named after his teacher and him.&lt;ref&gt;{{cite arXiv|eprint=1312.0792|title=A Note on the Geometry of Positively-Curved Riemannian Manifolds |author=Yashar Memarian |class=math.MG |year=2013 }}&lt;/ref&gt;

==Selected publications==
*"Existence and regularity of minimal surfaces on Riemannian manifolds"
*"Applications of minimax to minimal surfaces and the topology of 3-manifolds"
*"Existence of minimal surfaces of bounded topological type in three-manifolds"

==References==
{{Reflist}}

==External links==
*[http://www.math.tamu.edu/~jon.pitts/ Home page of Jon T. Pitts at the Texas A&amp;M University]
*[[Tobias Colding]] &amp; [[Camillo De Lellis]]: "[https://arxiv.org/pdf/math/0303305v2.pdf The min-max construction of minimal surfaces]"


{{authority control}}

{{DEFAULTSORT:Pitts, Jon T.}}
[[Category:1948 births]]
[[Category:Living people]]
[[Category:People from Austin, Texas]]
[[Category:Princeton University alumni]]
[[Category:Geometers]]
[[Category:Variational analysts]]
[[Category:Texas A&amp;M University faculty]]
[[Category:Sloan Research Fellows]]
[[Category:20th-century American mathematicians]]
[[Category:21st-century American mathematicians]]


{{US-mathematician-stub}}</text>
      <sha1>qwwzsmll3suitesbxfxx3dfj52cb2fn</sha1>
    </revision>
  </page>
  <page>
    <title>Kunstweg</title>
    <ns>0</ns>
    <id>49589765</id>
    <revision>
      <id>735640747</id>
      <parentid>713982685</parentid>
      <timestamp>2016-08-22T03:17:14Z</timestamp>
      <contributor>
        <username>Andy M. Wang</username>
        <id>516856</id>
      </contributor>
      <minor/>
      <comment>direct link ahead of move using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1878">{{Use mdy dates|date=February 2016}}
Bürgi's '''Kunstweg''' is a set of [[algorithm]]s invented by [[Jost Bürgi]] at the end of the 16th century.&lt;ref name="staudacher"&gt;Staudacher, S., 2014.  Jost Bürgi, Kepler und der Kaiser.  Verlag NZZ, Zürich.&lt;/ref&gt; They can be used for the calculation of [[sine]]s to an arbitrary precision. Bürgi used these algorithms to calculate a [[Canon Sinuum (Bürgi)|Canon Sinuum]], a table of sines in steps of 2 [[arc seconds]]. It is thought that this table had 8 sexagesimal places. Some authors have speculated that this table only covered the range from 0 to 45 degrees, but nothing seems to support this claim. Such tables were extremely important for [[navigation]] at sea. [[Johannes Kepler]] called the Canon Sinuum the most precise known table of sines (reference?). Bürgi explained his algorithms in his work [[Fundamentum Astronomiae]] which he presented to Emperor Rudolf II. in 1592.

The principles of iterative sine table calculation through the Kunstweg are as follows: cells in a column sum up the values of the two previous cells in the same [[column]]. The final cell's value is divided by two, and the next iteration starts. Finally, the values of the last column get normalized. Rather accurate [[approximations]] of sines are obtained after few iterations.

As recently as 2015, Folkerts et al. showed that this simple process converges indeed towards the true sines.&lt;ref name="folkerts"&gt;{{cite journal|bibcode=2015arXiv151003180F|arxiv=1510.03180|title=Jost Bürgi's Method for Calculating Sines|volume=1510|pages=17|author1=Folkerts|first1=Menso|last2=Launert|first2=Dieter|last3=Thom|first3=Andreas|year=2015}}&lt;/ref&gt; According to Folkerts, this was the first step towards [[difference calculus]].

== References ==
{{Reflist}}

{{DEFAULTSORT:Kunstweg (Jost Burgi)}}
[[Category:Algorithms]]
[[Category:Trigonometry]]</text>
      <sha1>sn5didiwlpkqv9zw314x3fpc56morbe</sha1>
    </revision>
  </page>
  <page>
    <title>Lajos Pósa (mathematician)</title>
    <ns>0</ns>
    <id>22874054</id>
    <revision>
      <id>861380357</id>
      <parentid>825044026</parentid>
      <timestamp>2018-09-27T00:42:21Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>/* References */add authority control, test</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9720">{{eastern  name  order|Pósa Lajos}}
'''Lajos Pósa''' (born December 9, 1947 in [[Budapest]]) is a Hungarian [[mathematician]] working in the topic of [[combinatorics]], and one of the most prominent mathematics educators of [[Hungary]], best known of his mathematics camps for gifted students. Winner of the [[Széchenyi Prize]].
[[Paul Erdős]]'s favorite "child", he discovered theorems at the age of 16. Since 2002 he works at the Rényi Institute of the [[Hungarian Academy of Sciences]]; earlier he was at the [[Eötvös Loránd University]], at the Departments of Mathematical Analysis, Computer Science.

== Biography ==
He was born in [[Budapest]], Hungary on December 9, 1947. His father was a [[chemist]], his mother a mathematics teacher. He was a child prodigy. While still in elementary school, the educator [[Rózsa Péter]], friend of his mother introduced him to Paul Erdős, who invited him
for lunch in a restaurant, and bombarded him with mathematical questions. Pósa finished the problems sooner than his soup, which impressed Erdős, who himself had been a child prodigy, and who supported young talents with much care and competence. That is how Pósa’s first paper was born, co-authored with Erdős (hence his [[Erdős number]] is 1).

He went to the first special mathematics class of the country at [[Fazekas Mihály Gimnázium (Budapest)|Fazekas Mihály Secondary School]] from 1962 to 1966, where his classmates included [[Miklós Laczkovich]], [[László Lovász]], [[József Pelikán]], [[Zsolt Baranyai]], István Berkes, [[Katalin Vesztergombi]], Péter Major. He won the first prize on the [[International Mathematical Olympiad]] in 1966 ([[Bulgaria]]) and second prize in 1965 ([[Germany]]).&lt;ref&gt;[http://www.imo-official.org/participant_r.aspx?id=9937]&lt;/ref&gt;

He started his Mathematics studies at ELTE University in 1966, and graduated in 1971. From 1971 to 1982 he worked at the Department of Mathematical Analysis at ELTE University, and he obtained a doctorate in 1983 with his dissertation about Hamiltonian circuits of random graphs. From 1984 to 2002 he worked at the Department of Computer Science at ELTE University, and since 2002 he has been a member of the [[Rényi Mathematical Institute]].

Despite his significant results in mathematical research, he stopped research and devoted himself fully to Mathematics Education. Erdős, who preferred him among all his protégés, expressed his regret that Pósa had stopped research with the typical Erdős style phrase “Pósa is dead.”

==Mathematics education==

He started teaching mathematics very early. He tutored his secondary school classmates, and during his first year at university he started teaching extracurricular courses at his former secondary school. His students at that time included: [[László Babai]], [[György Elekes]], [[Péter Komjáth]], [[Imre Z. Ruzsa]].

At the beginning of the 1970s he got involved with the school reform movement called complex teaching of mathematics led by Tamás Varga. Pósa worked on the reform of secondary mathematics teaching, while he taught at Radnóti Miklós Secondary School from 1976 to 1980. From 1982 to 1989 he was a member of the Research Group on Mathematics Education led by János Surányi. From 1982 to 1991 he had two experimental
classes at Eötvös József Secondary School. His teaching materials written at that time were tested in several classes, and based on these he and colleagues have written a textbook series for the four years of secondary school.

Nevertheless, Pósa is best known for finding and teaching gifted students. Since 1988 he has been organizing his own week-end maths camps. There are several groups of 20-35 students, and each group has two or three camps a year. In a camp, students mostly work in groups of
2-4, but there are also plenary sessions where they discuss solutions, and sum up important thoughts. Students work on problems carefully built on each other. There are several topics running parallelly, and one topic spans several camps. The emphasis is on thinking, proving, and the connection between seemingly distant ideas. The camps are also an important scene of teacher training, as prospective teachers observe and help in the camps.

== Mathematical research ==
* He gave sufficient conditions for the existence of [[Hamiltonian path|Hamiltonian circuit]].
* He proved that a [[random graph]] on ''n'' vertices with ''cn''&amp;nbsp;log&amp;nbsp;''n'' edges almost surely contains a Hamiltonian circuit,&lt;ref&gt;{{cite journal|last1=Pósa|first1=Lajos|title=Hamiltonian circuits in random graphs|journal=Discrete Mathematics|date=1976|volume=14|issue=4|pages=359–364|doi=10.1016/0012-365X(76)90068-6}}&lt;/ref&gt; thus affirming a conjecture of Erdős and [[Alfréd Rényi|Rényi]] (also proved by A. D. Korshunov). The result  was later improved by [[János Komlós (mathematician)|Komlós]] and [[Endre Szemerédi|Szemerédi]].&lt;ref&gt;{{cite journal|last1=Komlós|first1=János|last2=Szemerédi|first2=Endre|title=Limit distribution for the existence of hamiltonian cycles in a random graph|journal=Discrete Mathematics|date=1983|volume=43|issue=1|pages=55–63|doi=10.1016/0012-365X(83)90021-3}}&lt;/ref&gt;
* With [[Paul Erdős|Erdős]] he proved the [[Erdős–Pósa theorem]].

== Prizes, awards==
* Prize for Children Support („Gyermekekért” díj), 1989
* Beke Manó Prize I. degree by Bolyai János Mathematical Society, 1994
* Officer’s Cross Order of Merit of the Republic of Hungary, 1998
* Charles Simonyi Scholarship, 2000
* MOL prize for Gifted Education (“Tehetséggondozásért” díj), 2008
* [[Széchenyi Prize]], 2011

==Publications==

* Pósa Lajos: A prímszámok egy tulajdonságáról, Matematikai Lapok, 11 (1960), 124-128.
* P. Erdős, L. Pósa: On the maximal number of disjoint circuits of a graph, Publ. Math. Debrecen, 9 (1962), 3-13.
* L. Pósa: A theorem concerning Hamilton lines, MTA Mat. Kut. Int. Közl. 7 (1962), 225-226.
* L. Pósa: On the circuits of finite graphs, MTA Mat. Kut. Int. Közl. 8 (1964), 355-361.
* P. Erdős, L. Pósa: On independent circuits contained in a graph, Can. J. Math. 17 (1965), 347-352.
* P. Erdős, A. W. Goodman, L. Pósa: The representation of a graph by set intersection, Can. J. Math. 18 (1966), 106-112.
* P. Erdős, A. Hajnal, L. Pósa: Strong embeddings of graphs into colored graphs,Infinite and finite sets (Colloq. Keszthely 1973; dedicated to P. Erdős on his 60th birthday), Vol. I. Colloq. Math. Soc. J. Bolyai, Vol. 10, North Holland, Amsterdam,1975, 585-595.
* L. Pósa: Hamiltonian circuits in random graphs, Discrete Mathematics, 14 (1976), 359-364.
* Pósa Lajos: Véletlen gráfok Hamilton körei, egyetemi doktori értekezés, Budapest, 1982.
* Pósa Lajos: Beszélgetés az új felvételi rendszer tervéről, Köznevelés, XXXV (1979) (20), 7-8.
* Pósa Lajos: Variációk egy témára, Matematika-tanárképzés – matematikatanár-továbbképzés, 3 (1995), 41-59.

==Textbooks==
* Pósa Lajos: Analízis I. Differenciálszámítás, 11. o., Műszaki Könyvkiadó Kft., 2000
* Pósa Lajos: Analízis I. Differenciálszámítás. Tanári útmutató, 11. o., Műszaki Könyvkiadó Kft., 2000
* Pósa Lajos: Analízis II. - Integrálszámítás, Műszaki Könyvkiadó Kft.
* Pósa Lajos: Analízis II. Integrálszámítás (tanári útmutató), Műszaki Könyvkiadó Kft.
* Pósa Lajos: Összefoglalás. Algebra. Megoldások, Műszaki Könyvkiadó Kft., 1999
* Pósa Lajos: Összefoglalás. Algebra, függvények, Műszaki Könyvkiadó Kft., 1999
* Pósa Lajos: Sorozatok 11.-12. o. Műszaki Könyvkiadó Kft., 1998
* Pósa Lajos: Sorozatok. Tanári útmutató 11.-12. o., Műszaki Könyvkiadó Kft., 1998
* Pósa Lajos: Vegyes feladatok 1. 7.-9. o., Műszaki Könyvkiadó Kft., 1998
* Pósa Lajos: Vegyes feladatok 1. Tanári útmutató 9. o., Műszaki Könyvkiadó Kft., 1998
* Pósa Lajos: Hatványozás kiterjesztése és logaritmus-feladatsorok, Műszaki Könyvkiadó Kft.
* Pósa Lajos: Hatványozás kiterjesztése és logaritmus-megoldások, végeredmények, Műszaki Könyvkiadó Kft.
* Pósa Lajos: Vegyes feladatok tanári útmutató 1. osztály (2. kiadás, Műszaki Könyvkiadó, Budapest, 1999.)
* Pósa Lajos: Vegyes feladatok 2. - Tanári útmutató (Műszaki Könyvkiadó, Budapest, 2000.)
* Pósa Lajos: Vektorok és koordinátageometria - Feladatsorok (Műszaki Könyvkiadó, Budapest)
* Pósa Lajos: Vektorok és koordinátageometria - Megoldások, végeredmények (Műszaki Könyvkiadó, Budapest)
* Pósa Lajos: Vektorok és koordináta geometria - Tanári útmutató (Műszaki Könyvkiadó, Budapest, 2000.)
* Pósa Lajos: Összefoglalás - Matematika, elmélet, feladatok (Műszaki Könyvkiadó, Budapest)
* Pósa Lajos: Összefoglalás - Matematika, megoldások (Műszaki Könyvkiadó, Budapest)
* Pósa Lajos - Halmos Mária - Gábos Adél: Kombinatorika (Műszaki Könyvkiadó, Budapest, 1998.)
* Gábos Adél - Halmos Mária - Pósa Lajos: Kombinatorika - Tanári útmutató (Műszaki Könyvkiadó, Budapest, 2000.)

== References==
* [[Ross Honsberger]]: [http://www.math.uwaterloo.ca/navigation/ideas/articles/honsberger/index.shtml "The story of Louis Pósa"], in:''Mathematical Gems'', [[The Mathematical Association of America]], 1973.
* An [http://www.nol.hu/belfold/lap-20090411-20090411-20 article] on Pósa, in the Hungarian daily ''[[Népszabadság]]''
&lt;references/&gt;

{{authority control}}

{{DEFAULTSORT:Posa, Lajos}}
[[Category:Hungarian mathematicians]]
[[Category:Combinatorialists]]
[[Category:1947 births]]
[[Category:Living people]]
[[Category:Officer's Crosses of the Order of Merit of the Republic of Hungary (civil)]]
[[Category:International Mathematical Olympiad participants]]</text>
      <sha1>s4f3vy199epimaoaedm0cjz0ukh3g2m</sha1>
    </revision>
  </page>
  <page>
    <title>Leray cover</title>
    <ns>0</ns>
    <id>2854390</id>
    <revision>
      <id>862709700</id>
      <parentid>859450686</parentid>
      <timestamp>2018-10-06T05:23:01Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Removing category Eponymous scientific concepts per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2018 September 22]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2758">In [[mathematics]], a '''Leray cover(ing)''' is a [[Cover (topology)|cover]] of a [[topological space]] which allows for easy calculation of its [[cohomology]].  Such covers are named after [[Jean Leray]]. 

[[Sheaf cohomology]] measures the extent to which a locally exact sequence on a fixed topological space, for instance the [[De Rham cohomology|de Rham sequence]], fails to be globally exact.  Its definition, using [[derived functor]]s, is reasonably natural, if technical.  Moreover, important properties, such as the existence of a [[long exact sequence]] in cohomology corresponding to any [[short exact sequence]] of [[Sheaf (mathematics)|sheaves]], follow directly from the definition. However, it is virtually impossible to calculate from the definition.  On the other hand, [[Čech cohomology]] with respect to an [[open cover]] is well-suited to calculation, but of limited usefulness because it depends on the open cover chosen, not only on the sheaves and the space.  By taking a direct limit of Čech cohomology over arbitrarily fine covers, we obtain a Čech cohomology theory that does not depend on the open cover chosen.  In reasonable circumstances (for instance, if the topological space is [[paracompact]]), the derived-functor cohomology agrees with this Čech cohomology obtained by direct limits.  However, like the derived functor cohomology, this cover-independent Čech cohomology is virtually impossible to calculate from the definition. The Leray condition on an open cover ensures that the cover in question is already "fine enough." The derived functor cohomology agrees with the Čech cohomology with respect to any Leray cover.

Let &lt;math&gt;\mathfrak{U} = \{U_i\}&lt;/math&gt; be an open cover of the topological space &lt;math&gt;X&lt;/math&gt;, and &lt;math&gt;\mathcal{F}&lt;/math&gt; a sheaf on X. We say that &lt;math&gt;\mathfrak{U}&lt;/math&gt; is a Leray cover with respect to &lt;math&gt;\mathcal{F}&lt;/math&gt; if, for every nonempty finite set &lt;math&gt;\{i_1, \ldots, i_n\}&lt;/math&gt; of indices, and for all &lt;math&gt;k &gt; 0&lt;/math&gt;, we have that &lt;math&gt;H^k(U_{i_1} \cap \cdots \cap U_{i_n}, \mathcal{F}) = 0&lt;/math&gt;, in the derived functor cohomology.&lt;ref&gt;Taylor, Joseph L. Several complex variables with connections to algebraic geometry and Lie groups. [[Graduate Studies in Mathematics]] v. 46.  American Mathematical Society, Providence, RI. 2002.&lt;/ref&gt; For example, if &lt;math&gt;X&lt;/math&gt; is a separated scheme, and &lt;math&gt;\mathcal{F}&lt;/math&gt; is quasicoherent, then any cover of &lt;math&gt;X&lt;/math&gt; by open affine subschemes is a Leray cover.&lt;ref&gt;[[Ian G. Macdonald|Macdonald, Ian G.]] Algebraic geometry. Introduction to schemes. W. A. Benjamin, Inc., New York-Amsterdam 1968 vii+113 pp.&lt;/ref&gt;

== References ==
&lt;references&gt;
&lt;/references&gt;

[[Category:Sheaf theory]]

{{topology-stub}}</text>
      <sha1>r1m8hol09gclzosl7blhicujfwczplu</sha1>
    </revision>
  </page>
  <page>
    <title>List of PPAD-complete problems</title>
    <ns>0</ns>
    <id>23256783</id>
    <revision>
      <id>841568762</id>
      <parentid>821827204</parentid>
      <timestamp>2018-05-16T16:58:12Z</timestamp>
      <contributor>
        <username>Nemo bis</username>
        <id>2584239</id>
      </contributor>
      <comment>Added free to read link in citations with [[WP:OABOT|OAbot]] #oabot</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1950">This is a list of [[PPAD (complexity)|PPAD]]-complete problems.

== Fixed Point Theorems ==

* [[Sperner's lemma]]
* [[Brouwer fixed point theorem]]
* [[Kakutani fixed point theorem]]

== Game theory ==
* [[Nash equilibrium]]
* [[Core (game theory)|Core of Balanced Games]]

== Equilibria in Game Theory and Economics ==

* [[Fisher market| Fisher market equilibria]]
* [[Arrow–Debreu model|Arrow-Debreu Equilibria]]
* [[Approximate Competitive Equilibrium from Equal Incomes]]
* Finding clearing payments in financial networks

== Graph theory ==
* [[Fractional Stable Paths Problems]]
* [[Fractional hypergraph matching]] (see also the NP-complete [[Hypergraph matching]])
* [[Fractional Strong Kernel]] 

== Miscellaneous ==
* [[Herbert Scarf#6. Scarf’s Combinatorial Lemma|Scarf's Lemma]]
* [[Fractional Bounded Budget Connection Games]]

== References ==

* {{cite journal
| doi = 10.1016/S0022-0000(05)80063-7
| last = Papadimitriou
| first = Christos
| year = 1994
| title = On the Complexity of the Parity Argument and Other Inefficient Proofs of Existence.
| journal = Journal of Computer and System Sciences
| volume = 48
| issue = 3
| pages = 498–532
}} Paper available online at [http://www.cs.berkeley.edu/~christos/papers/On%20the%20Complexity.pdf Papadimitriou's Homepage].

* {{cite journal
| doi = 10.1137/070699652
| author = C. Daskalakis, P.W. Goldberg and [[Christos Papadimitriou|C.H. Papadimitriou]]
| year = 2009
| title = The Complexity of Computing a Nash Equilibrium
| journal = [[SICOMP|SIAM Journal on Computing]]
| volume = 39
| issue = 3
| pages = 195–259
| citeseerx = 10.1.1.68.6111
}} 

*{{cite book
|    author = Xi Chen and Xiaotie Deng
|   title = Settling the complexity of two-player Nash equilibrium
|    booktitle = In Proc. 47th FOCS
|    year = 2006
|    pages = 261–272
}}


{{DEFAULTSORT:List Of Ppad-Complete Problems}}
[[Category:Mathematics-related lists]]
[[Category:Computational problems]]</text>
      <sha1>po5d3p83qzlr4zoth8uzopv5p12kzhd</sha1>
    </revision>
  </page>
  <page>
    <title>MacVector</title>
    <ns>0</ns>
    <id>20935181</id>
    <revision>
      <id>810658146</id>
      <parentid>808515982</parentid>
      <timestamp>2017-11-16T16:59:33Z</timestamp>
      <contributor>
        <username>Kku</username>
        <id>5846</id>
      </contributor>
      <minor/>
      <comment>link [[multiple sequence alignment]] using [[:en:User:Edward/Find link|Find link]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3820">{{Infobox Software
| name = MacVector
| logo = 
| screenshot = 
| caption = 
| developer = MacVector, Inc.
| latest release version = 16.0
| latest release date = [[3 November 2017]]
| latest preview version = 
| latest preview date = 
| operating system = macOS
| platform = [[Xcode]]
| genre = [[Bioinformatics]]
| license = commercial 
| website = [http://www.macvector.com macvector.com]
}}

'''MacVector''' is a commercial sequence analysis application for Apple Macintosh computers running [[Mac OS X]]. It is intended to be used by [[Molecular biology|molecular biologists]] to help analyze, design, research and document their experiments in the laboratory.

== Features ==
MacVector is a collection of sequence analysis algorithms linked to various sequence editors, including a single sequence editor, a [[multiple sequence alignment]] editor and a contig editor.  MacVector tries to use a minimum of windows and steps to access all the functionality. Functions include:

* [[Sequence alignment]] ([[ClustalW]], [[MUSCLE (alignment software)|Muscle]] and [[T-COFFEE|T-Coffee]]) and editing.
* Subsequence search and [[open reading frames]] (ORFs) analysis.
* [[Phylogenetic tree]] construction [[UPGMA]], [[Neighbour joining]] with bootstrapping and consensus trees
* Online Database searching - Search public databases at the NCBI such as [[Genbank]], [[PubMed]], and  [[UniProt]].
* Perform online [[BLAST]] searches.
* Protein analysis.
* Contig assembly and [[chromatogram]] editing
* Aligning [[cDNA]] against genomic templates
* Creating [[Dot plot (bioinformatics)|dot plots]] of DNA to DNA, Protein to Protein and DNA to protein.
* Restriction analysis - find and view restriction cut sites. Uses digested fragments to clone genes into vectors. Stores a history of digested fragments allowing multi fragment ligations.
* [[PCR]] Primer design - easy primer design and testing. Also uses [[primer3]]
* [[Agarose_gel_electrophoresis|Agarose Gel]] simulation.

MacVector has a contig assembly plugin called [[MacVector Assembler|Assembler]] that uses [[Phred quality score|phred]], [[phrap]], [[List of sequence alignment software#Short-Read Sequence Alignment|Bowtie]], SPAdes, [[Velvet assembler|Velvet]] and [[cross match]].

As of version 13.0.1 MacVector uses [[Sparkle (software)|Sparkle]] for updating between releases.

== History ==

MacVector was originally developed by IBI in 1994.&lt;ref&gt;"MacVector: an integrated sequence analysis program for the Macintosh". Olson SA. ''Methods Mol Biol''. (1994) 25,195-201.&lt;/ref&gt; It was acquired by Kodak, and subsequently Oxford Molecular in 1996.&lt;ref&gt;{{cite journal|url=https://www.sciencemag.org/site/products/bt-dna.xhtml |title=DNA Sequencing Software - Special Ad Section|date= 7 June 1996|journal=Science}}&lt;/ref&gt;&lt;ref&gt;http://findarticles.com/p/articles/mi_hb197/is_/ai_n5566980#{{Dead link|date=July 2012}}&lt;/ref&gt; Oxford Molecular was merged into Accelrys in 2001.&lt;ref&gt;{{cite web|url=http://accelrys.com/company/ |title=About Accelrys, including company leadership, recognition, and news |publisher=Accelrys.com |date=2009-05-04 |accessdate=2012-07-26}}&lt;/ref&gt; It was acquired by MacVector, Inc on 1 January 2007.&lt;ref&gt;{{cite web|url=http://accelrys.com/products/macvector/ |archiveurl=https://web.archive.org/web/20080607134226/http://accelrys.com/products/additional-products.html |archivedate=2008-06-07 |title=Accelrys - Additional Products |publisher=accelrys.com |accessdate=2012-07-24}}&lt;/ref&gt;

==References==
{{reflist}}

== External links ==
*[http://www.macvector.com MacVector homepage]
*[http://www.macvector.com/phpbb MacVector Forums]

=== Other Software ===
*[http://www.genomecompiler.com/ Genome Compiler] 
*[[Gene Designer]]
*[[Geneious]]
*[[Vector NTI]]
*[[UGENE]]

[[Category:Bioinformatics software]]
[[Category:Computational science]]</text>
      <sha1>nurgvgyl1b3r7s0zgsrtjnoz7loh7ag</sha1>
    </revision>
  </page>
  <page>
    <title>Mandelbox</title>
    <ns>0</ns>
    <id>27857702</id>
    <revision>
      <id>836635887</id>
      <parentid>823026613</parentid>
      <timestamp>2018-04-15T23:41:20Z</timestamp>
      <contributor>
        <ip>140.253.50.108</ip>
      </contributor>
      <comment>Whether it is multifractal or not is not proven as far as I know, and if it was, it wouldn't be a result of being applicable in any number of dimensions. So I removed this line.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3104">[[Image:Mandelboxpwr2.png|alt=A three-dimensional Mandelbox fractal of scale 2.|thumb|right|A 'scale 2' Mandelbox]]
[[Image:Mandelboxpwr3.png|alt=A three-dimensional Mandelbox fractal of scale 3.|thumb|right|A 'scale 3' Mandelbox]]
In mathematics, the '''mandelbox''' is a [[fractal]] with a boxlike shape found by Tom Lowe in 2010. It is defined in a similar way to the famous [[Mandelbrot set]] as the values of a parameter such that the origin does not escape to infinity under iteration of certain geometrical transformations. The mandelbox is defined as a map of continuous [[Julia set]]s, but, unlike the Mandelbrot set, can be defined in any number of dimensions.&lt;ref&gt;{{cite web|last1=Lowe |first1=Tom |title=What Is A Mandelbox? |url=https://sites.google.com/site/mandelbox/what-is-a-mandelbox |accessdate=15 November 2016 |archiveurl=https://web.archive.org/web/20161008202249/https://sites.google.com/site/mandelbox/what-is-a-mandelbox |archivedate=8 October 2016 |deadurl=yes |df= }}&lt;/ref&gt;. It is typically drawn in three dimensions for illustrative purposes.

== Generation ==
The iteration applies to vector ''z'' as follows:

 '''function''' iterate(''z''):
     '''for each''' component '''in''' ''z'':
         '''if''' component &gt; 1:
             component := 2 - component
         '''else if''' component &lt; -1:
             component := -2 - component
 
     '''if''' magnitude of ''z'' &lt; 0.5:
         ''z'' := ''z'' * 4
     '''else if''' magnitude of ''z'' &lt; 1:
         ''z'' := ''z'' / (magnitude of ''z'')^2
    
     ''z'' := ''scale'' * ''z'' + ''c''

Here, ''c'' is the constant being tested, and ''scale'' is a real number.

== Properties ==
A notable property of the mandelbox, particularly for scale -1.5, is that it contains approximations of many well known fractals within it.&lt;ref&gt;[http://sites.google.com/site/mandelbox/negative-mandelbox negative-mandelbox]&lt;/ref&gt;&lt;ref&gt;[http://sites.google.com/site/mandelbox/more-negatives more-negatives]&lt;/ref&gt;&lt;ref&gt;[https://web.archive.org/web/20110213231307/http://www.miqel.com/fractals_math_patterns/mandelbox_3d_fractal.html mandelbox_3d_fractal]&lt;/ref&gt;

For 1&lt;|scale|&lt;2 the mandelbox contains a solid core. Consequently its fractal dimension is 3, or n when generalised to n dimensions.&lt;ref name="properties"&gt;{{cite web|last1=Chen|first1=Rudi|title=The Mandelbox Set|url=http://digitalfreepen.com/mandelbox370}}&lt;/ref&gt;

For scale &lt; -1 the mandelbox sides have length 4 and for 1 &lt; scale &lt;= 4{{math|{{radical|n}}}}+1 they have length 4(scale+1)/(scale-1)&lt;ref name = "properties"/&gt;

==See also ==

*[[Mandelbulb]]

==Notes==
{{Reflist}}

==References==
*{{citation|first=Jos |last=Leys|title= Mandelbox. Images des Mathématiques|publisher= CNRS|year= 2010|url= http://images.math.cnrs.fr/Mandelbox.html}}

==External links==
*[https://sites.google.com/site/mandelbox/ Gallery and description]
*[http://images.math.cnrs.fr/Mandelbox.html Images of some Mandelbox cubes]
*[https://www.youtube.com/watch?v=7Pf6jZWguCc Video : zoom in the Mandelbox cube]

{{Fractal software}}
{{Fractals}}
{{Mathematics and art}}

[[Category:Fractals]]</text>
      <sha1>j68sa2pll15p6j9dem15grkus53camh</sha1>
    </revision>
  </page>
  <page>
    <title>Mathematical Tables Project</title>
    <ns>0</ns>
    <id>488896</id>
    <revision>
      <id>829983093</id>
      <parentid>769307843</parentid>
      <timestamp>2018-03-12T00:33:48Z</timestamp>
      <contributor>
        <username>Omnipaedista</username>
        <id>8524693</id>
      </contributor>
      <comment>resectioning per WP:LEAD</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3515">The '''Mathematical Tables Project'''&lt;ref&gt;{{cite journal|last=Grier|first=David Alan|title=The Math Tables Project of the Work Projects Administration: The Reluctant Start of the Computing Era|journal=IEEE Ann. Hist. Comput.|year=1998|volume=20|issue=3|pages=33–50|doi=10.1109/85.707573|issn=1058-6180}}&lt;/ref&gt;&lt;ref&gt;{{cite book|last=Grier|first=David Alan|title=When Computers Were Human|year=2005|publisher=Princeton University Press|isbn=978-0-691-09157-0}}&lt;/ref&gt; was one of the largest and most sophisticated computing organizations that operated prior to the invention of the digital electronic computer. Begun in the [[United States]] in 1938 as a project of the [[Works Progress Administration]] (WPA), it employed 450 unemployed clerks to [[mathematical table|tabulate higher mathematical functions]], such as [[exponential function]]s, [[logarithm]]s, and [[trigonometric functions]]. These tables were eventually published in a 28 volume set by [[Columbia University Press]].

==History==
The group was led by a group of mathematicians and physicists, most of whom had been unable to find professional work during the [[Great Depression]]. The mathematical leader was [[Gertrude Blanch]], who had just finished her doctorate in mathematics at [[Cornell University]]. She had been unable to find a university position and was working at a photographic company before joining the project.

The administrative director was Arnold Lowan, who had a degree in physics from [[Columbia University]] and had spent a year at the [[Institute for Advanced Study]] in [[Princeton University]] before returning to New York without a job. Perhaps the most accomplished mathematician to be associated with the group was [[Cornelius Lanczos]], who had once served as an assistant to [[Albert Einstein]]. He spent a year with the project and organized seminars on computation and [[applied mathematics]] at the project's office in [[Lower Manhattan]].

In addition to computing tables of mathematical functions, the project did large computations for sciences, including the physicist [[Hans Bethe]] and did calculations for a variety of war projects including tables for the [[LORAN]] navigation system, tables for microwave radar, bombing tables, and shock wave propagation tables.

The Mathematical Tables Project survived the termination of the WPA in 1943 and continued to operate in New York until 1948. At that point, roughly 25 members of the group moved to Washington, DC to become the Computation Laboratory of the National Bureau of Standards, now the [[National Institute of Standards and Technology]]. Blanch moved to Los Angeles to lead the computing office of the Institute for Numerical Analysis at UCLA and Arnold Lowan joined the faculty of [[Yeshiva University]] in New York. The greatest legacy of the project is the [[Abramowitz and Stegun|''Handbook of Mathematical Functions'']],&lt;ref&gt;{{AS ref}}&lt;/ref&gt; which was published 16 years after the group disbanded. Edited by two veterans of the project, [[Milton Abramowitz]] and [[Irene Stegun]], it became a widely circulated mathematical and scientific reference.

==References==
&lt;references /&gt;

==External links==
*[http://archives.lib.umn.edu/repositories/3/resources/278#7a2350b90d4e30f1ae30739a1f279dfb Gertrude Blanch Papers]
*[http://www.philsoc.org/2001Spring/2132transcript.html The Human Computer and the Birth of the Information Age]

[[Category:History of mathematics]]
[[Category:Mathematical tables]]
[[Category:Works Progress Administration]]</text>
      <sha1>l7lvyrdi6qqu03tuzkl7fa6gvf44o9f</sha1>
    </revision>
  </page>
  <page>
    <title>Median</title>
    <ns>0</ns>
    <id>18837</id>
    <revision>
      <id>871079083</id>
      <parentid>871079069</parentid>
      <timestamp>2018-11-28T20:07:14Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor/>
      <comment>Reverting possible vandalism by [[Special:Contribs/ExplosiveUpset|ExplosiveUpset]] to version by Josve05a. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (3549893) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="58673">{{About|the statistical concept}}
[[File:Finding the median.png|thumb|Finding the median in sets of data with an odd and even number of values]]
The '''median''' is the value separating the higher half from the lower half of a data [[Sample (statistics)|sample]] (a [[statistical population|population]] or a [[probability distribution]]). For a data set, it may be thought of as the "middle" value. For example, in the data set {1, 3, 3, 6, 7, 8, 9}, the median is 6, the fourth largest, and also the fourth smallest, number in the sample. For a [[continuous probability distribution]], the median is the value such that a number is equally likely to fall above or below it.

The median is a commonly used measure of the properties of a data set in [[statistics]] and [[probability theory]]. The basic advantage of the median in describing data compared to the [[Arithmetic mean|mean]] (often simply described as the "average") is that it is not [[Skewness|skewed]] so much by extremely large or small values, and so it may give a better idea of a "typical" value. For example, in understanding statistics like household income or assets which vary greatly, a mean may be skewed by a small number of extremely high or low values. Median income, for example, may be a better way to suggest what a "typical" income is.

Because of this, the median is of central importance&lt;!-- pun not entirely unintended --&gt; in [[robust statistics]], as it is the most [[resistant statistic]], having a [[breakdown point]] of 50%: so long as no more than half the data are contaminated, the median will not give an arbitrarily large or small result.

==Finite set of numbers==
The median of a finite list of numbers can be found by arranging all the numbers from smallest to greatest.

If there is an odd number of numbers, the middle one is picked. For example, consider the list of numbers

: 1, 3, 3, 6, 7, 8, 9

This list contains seven numbers. The median is the fourth of them, which is 6.

If there is an even number of observations, then there is no single middle value; the median is then usually defined to be the [[arithmetic mean|mean]] of the two middle values.&lt;ref name="StatisticalMedian"&gt;{{MathWorld |urlname=StatisticalMedian |title=Statistical Median }}&lt;/ref&gt;&lt;ref&gt;Simon, Laura J.; [http://www.stat.psu.edu/old_resources/ClassNotes/ljs_07/sld008.htm "Descriptive statistics"] {{webarchive|url=https://web.archive.org/web/20100730032416/http://www.stat.psu.edu/old_resources/ClassNotes/ljs_07/sld008.htm |date=2010-07-30 }}, ''Statistical Education Resource Kit'', Pennsylvania State Department of Statistics&lt;/ref&gt; For example, in the data set

: 1, 2, 3, 4, 5, 6, 8, 9

the median is the mean of the middle two numbers: this is &lt;math&gt;(4 + 5)/2&lt;/math&gt;, which is &lt;math&gt;4.5&lt;/math&gt;. (In more technical terms, this interprets the median as the fully [[trimmed estimator|trimmed]] [[mid-range]]).

The formula used to find the index of the middle number of a data set of ''n'' numerically ordered numbers is &lt;math&gt;(n + 1)/2&lt;/math&gt;. This either gives the middle number (for an odd number of values) or the halfway point between the two middle values. For example, with 14 values, the formula will give an index of 7.5, and the median will be taken by averaging the seventh (the floor of this index) and eighth (the ceiling of this index) values. So the median can be represented by the following formula:

: &lt;math&gt;\mathrm{median}(a) = \frac{a_{\lceil\#x \div 2\rceil} + a_{\lceil\#x \div 2 + 1\rceil}}{2}&lt;/math&gt;

{| class="wikitable"
|+ Comparison of common [[average]]s of values { 1, 2, 2, 3, 4, 7, 9 }
! Type
! Description
! Example
! Result
|-
| align="center" | [[Arithmetic mean]]
| Sum of values of a data set divided by number of values: &lt;math&gt;\scriptstyle\bar{x} = \frac{1}{n}\sum_{i=1}^n x_i&lt;/math&gt;
| align="center" | (1+2+2+3+4+7+9) / 7
| align="center" | '''4'''
|-
| align="center" | Median
| Middle value separating the greater and lesser halves of a data set
| align="center" | 1, 2, 2, '''3''', 4, 7, 9
| align="center" | '''3'''
|-
| align="center" | [[Mode (statistics)|Mode]]
| Most frequent value in a data set
| align="center" | 1, '''2''', '''2''', 3, 4, 7, 9
| align="center" | '''2'''
|}

One can find the median using the [[stem-and-leaf display|Stem-and-Leaf Plot]].

There is no widely accepted standard notation for the median, but some authors represent the median of a variable ''x'' either as ''x͂'' or as ''μ''&lt;sub&gt;1/2&lt;/sub&gt;&lt;ref name="StatisticalMedian" /&gt; sometimes also ''M''.&lt;ref name="Sheskin2003"&gt;{{cite book|author=David J. Sheskin|title=Handbook of Parametric and Nonparametric Statistical Procedures: Third Edition|url=https://books.google.com/books?id=bmwhcJqq01cC&amp;pg=PA7|accessdate=25 February 2013|date=27 August 2003|publisher=CRC Press|isbn=978-1-4200-3626-8|pages=7–}}&lt;/ref&gt;&lt;ref name="Bissell1994"&gt;{{cite book|author=Derek Bissell|title=Statistical Methods for Spc and Tqm|url=https://books.google.com/books?id=cTwwtyBX7PAC&amp;pg=PA26|accessdate=25 February 2013|year=1994|publisher=CRC Press|isbn=978-0-412-39440-9|pages=26–}}&lt;/ref&gt; In any of these cases, the use of these or other symbols for the median needs to be explicitly defined when they are introduced.

The median is used primarily for [[skewness|skewed]] distributions, which it summarizes differently from the [[arithmetic mean]]. Consider the [[multiset]] { 1, 2, 2, 2, 3, 14 }. The median is 2 in this case, (as is the [[mode (statistics)|mode]]), and it might be seen as a better indication of [[central tendency]] (less susceptible to the exceptionally large value in data) than the [[arithmetic mean]] of 4.

The median is a popular [[summary statistic]] used in [[descriptive statistics]], since it is simple to understand and easy to calculate, while also giving a measure that is more robust in the presence of [[outlier]] values than is the [[mean]]. The widely cited empirical relationship between the relative locations of the mean and the median for skewed distributions is, however, not generally true.&lt;ref&gt;{{cite web|url=http://www.amstat.org/publications/jse/v13n2/vonhippel.html|title=Journal of Statistics Education, v13n2: Paul T. von Hippel|work=amstat.org}}&lt;/ref&gt; There are, however, various relationships for the ''absolute'' difference between them; see below.

With an even number of observations (as shown above) no value need be exactly at the value of the median. Nonetheless, the value of the median is uniquely determined with the usual definition. A related concept, in which the outcome is forced to correspond to a member of the sample, is the [[medoid]].

In a population, at most half have values strictly less than the median and at most half have values strictly greater than it. If each group contains less than half the population, then some of the population is exactly equal to the median. For example, if ''a''&amp;nbsp;&lt;&amp;nbsp;''b''&amp;nbsp;&lt;&amp;nbsp;''c'', then the median of the list {''a'',&amp;nbsp;''b'',&amp;nbsp;''c''} is ''b'', and, if ''a''&amp;nbsp;&lt;&amp;nbsp;''b''&amp;nbsp;&lt;&amp;nbsp;''c''&amp;nbsp;&lt;&amp;nbsp;''d'', then the median of the list {''a'',&amp;nbsp;''b'',&amp;nbsp;''c'',&amp;nbsp;''d''} is the mean of ''b'' and ''c''; i.e., it is (''b''&amp;nbsp;+&amp;nbsp;''c'')/2. Indeed, as it is based on the middle data in a group, it is not necessary to even know the value of extreme results in order to calculate a median. For example, in a psychology test investigating the time needed to solve a problem, if a small number of people failed to solve the problem at all in the given time a median can still be calculated.&lt;ref name="Robson"&gt;{{cite book|last1=Robson|first1=Colin|title=Experiment, Design and Statistics in Psychology|date=1994|publisher=Penguin|isbn=0-14-017648-9|pages=42–45}}&lt;/ref&gt;

The median can be used as a measure of [[location parameter|location]] when a distribution is [[skewness|skewed]], when end-values are not known, or when one requires reduced importance to be attached to [[outlier]]s, e.g., because they may be measurement errors.

A median is only defined on [[Weak ordering|ordered]] one-dimensional data, and is independent of any [[distance metric]]. A [[geometric median]], on the other hand, is defined in any number of dimensions.

The median is one of a number of ways of summarising the typical values associated with members of a statistical population; thus, it is a possible [[location parameter]]. The median is the 2nd [[quartile]], 5th [[decile]], and 50th [[percentile]]. Since the median is the same as the ''second quartile'', its calculation is illustrated in the article on [[quartile]]s. A median can be worked out for ranked but not numerical classes (e.g. working out a median grade when students are graded from A to F), although the result might be halfway between grades if there is an even number of cases.

When the median is used as a location parameter in descriptive statistics, there are several choices for a measure of variability: the [[Range (statistics)|range]], the [[interquartile range]], the [[mean absolute deviation]], and the [[median absolute deviation]].

For practical purposes, different measures of location and dispersion are often compared on the basis of how well the corresponding population values can be estimated from a sample of data. The median, estimated using the sample median, has good properties in this regard. While it is not usually optimal if a given population distribution is assumed, its properties are always reasonably good. For example, a comparison of the [[Efficiency (statistics)|efficiency]] of candidate estimators shows that the sample mean is more statistically efficient than the sample median when data are uncontaminated by data from heavy-tailed distributions or from mixtures of distributions, but less efficient otherwise, and that the efficiency of the sample median is higher than that for a wide range of distributions. More specifically, the median has a 64% efficiency compared to the minimum-variance mean (for large normal samples), which is to say the variance of the median will be ~50% greater than the variance of the mean—see [[Efficiency (statistics)#Asymptotic efficiency|asymptotic efficiency]] and references therein.

==Probability distributions==
[[File:visualisation mode median mean.svg|thumb|100px|Geometric visualisation of the mode, median and mean of an arbitrary probability density function.&lt;ref&gt;{{cite web|title=AP Statistics Review - Density Curves and the Normal Distributions|url=http://apstatsreview.tumblr.com/post/50058615236/density-curves-and-the-normal-distributions?action=purge|accessdate=16 March 2015}}&lt;/ref&gt;]]
For any [[probability distribution]] on the [[real number|real]] line '''R''' with [[cumulative distribution function]]&amp;nbsp;''F'', regardless of whether it is any kind of continuous probability distribution, in particular an [[absolute continuity|absolutely continuous distribution]] (which has a [[probability density function]]), or a discrete probability distribution, a median is by definition any real number&amp;nbsp;''m'' that satisfies the inequalities

:&lt;math&gt;\operatorname{P}(X\leq m) \geq \frac{1}{2}\text{ and }\operatorname{P}(X\geq m) \geq \frac{1}{2}\,\!&lt;/math&gt;

or, equivalently, the inequalities

:&lt;math&gt;\int_{(-\infty,m]} dF(x) \geq \frac{1}{2}\text{ and }\int_{[m,\infty)} dF(x) \geq \frac{1}{2}\,\!&lt;/math&gt;

in which a [[Lebesgue–Stieltjes integral]] is used. For an absolutely continuous probability distribution with [[probability density function]] ''ƒ'', the median satisfies

:&lt;math&gt;\operatorname{P}(X\leq m) = \operatorname{P}(X\geq m)=\int_{-\infty}^m f(x)\, dx=\frac{1}{2}.\,\!&lt;/math&gt;

Any [[probability distribution]] on '''R''' has at least one median, but in specific cases there may be more than one median. Specifically, if a probability density is zero on an interval [''a'',&amp;nbsp;''b''], and the [[cumulative distribution function]] at ''a'' is 1/2, any value between ''a'' and ''b'' will also be a median.

===Medians of particular distributions===
The medians of certain types of distributions can be easily calculated from their parameters; furthermore, they exist even for some distributions lacking a well-defined mean, such as the [[Cauchy distribution]]:
* The median of a symmetric [[unimodal distribution]] coincides with the mode.
* The median of a [[symmetric distribution]] which possesses a mean ''μ'' also takes the value ''μ''.
** The median of a [[normal distribution]] with mean ''μ'' and variance ''σ''&lt;sup&gt;2&lt;/sup&gt; is&amp;nbsp;μ. In fact, for a normal distribution, mean = median = mode.
** The median of a [[uniform distribution (continuous)|uniform distribution]] in the interval [''a'',&amp;nbsp;''b''] is (''a''&amp;nbsp;+&amp;nbsp;''b'')&amp;nbsp;/&amp;nbsp;2, which is also the mean.
* The median of a [[Cauchy distribution]] with location parameter ''x''&lt;sub&gt;0&lt;/sub&gt; and scale parameter ''y'' is&amp;nbsp;''x''&lt;sub&gt;0&lt;/sub&gt;, the location parameter.
* The median of a [[Power law|power law distribution]] ''x''&lt;sup&gt;−''a''&lt;/sup&gt;, with exponent ''a''&amp;nbsp;&gt;&amp;nbsp;1 is 2&lt;sup&gt;1/(''a''&amp;nbsp;−&amp;nbsp;1)&lt;/sup&gt;''x''&lt;sub&gt;min&lt;/sub&gt;, where ''x''&lt;sub&gt;min&lt;/sub&gt; is the minimum value for which the power law holds&lt;ref&gt;[https://arxiv.org/pdf/cond-mat/0412004.pdf Newman, Mark EJ. "Power laws, Pareto distributions and Zipf's law." Contemporary physics 46.5 (2005): 323–351.]&lt;/ref&gt;
* The median of an [[exponential distribution]] with [[rate parameter]] ''λ'' is the natural logarithm of 2 divided by the rate parameter: ''λ''&lt;sup&gt;−1&lt;/sup&gt;ln&amp;nbsp;2.
* The median of a [[Weibull distribution]] with shape parameter ''k'' and scale parameter ''λ'' is&amp;nbsp;''λ''(ln&amp;nbsp;2)&lt;sup&gt;1/''k''&lt;/sup&gt;.

==Populations==

===Optimality property===
The ''mean absolute error'' of a real variable ''c'' with respect to the [[random variable]]&amp;nbsp;''X'' is
:&lt;math&gt;E(\left|X-c\right|)\,&lt;/math&gt;
Provided that the probability distribution of ''X'' is such that the above expectation exists, then ''m'' is a median of  ''X'' if and only if ''m'' is a minimizer of the mean absolute error with respect to ''X''.&lt;ref&gt;{{cite book |last=Stroock |first=Daniel |title=Probability Theory |year=2011 |publisher=Cambridge University Press |isbn=978-0-521-13250-3 |pages=43 }}&lt;/ref&gt; In particular, ''m'' is a sample median if and only if ''m'' minimizes the arithmetic mean of the absolute deviations.

More generally, a median is defined as a minimum of
:&lt;math&gt;E(|X-c| - |X| ),&lt;/math&gt;
as discussed below in the section on [[multivariate median]]s (specifically, the [[spatial median]]).

This optimization-based definition of the median is useful in statistical data-analysis, for example, in [[k-medians clustering|''k''-medians clustering]]&lt;!-- The "k" in that article title needs to be in LOWER CASE because it's mathematical notation. --&gt;.

===Unimodal distributions===
[[File:Comparison mean median mode.svg|thumb|300px|Comparison of [[mean]], median and [[mode (statistics)|mode]] of two [[log-normal distribution]]s with different [[skewness]].]]

It can be shown for a unimodal distribution that the median &lt;math&gt;\tilde{X}&lt;/math&gt; and the mean &lt;math&gt;\bar{X}&lt;/math&gt; lie within (3/5)&lt;sup&gt;1/2&lt;/sup&gt; ≈ 0.7746 standard deviations of each other.&lt;ref name="unimodal"&gt;{{Cite journal|title=The Mean, Median, and Mode of Unimodal Distributions:A Characterization|journal=Theory of Probability &amp; its Applications|volume=41|issue=2|pages=210–223|doi=10.1137/S0040585X97975447|year = 1997|last1 = Basu|first1 = S.|last2=Dasgupta|first2=A.}}&lt;/ref&gt; In symbols,

: &lt;math&gt;\frac{\left|\tilde{X} - \bar{X}\right|}{\sigma} \le \left(\frac{3}{5}\right)^\frac{1}{2}&lt;/math&gt;

where |·| is the absolute value.

A similar relation holds between the median and the mode: they lie within 3&lt;sup&gt;1/2&lt;/sup&gt; ≈ 1.732 standard deviations of each other:

: &lt;math&gt;\frac{|\tilde{X} - \mathrm{mode}|}{\sigma} \le 3^\frac{1}{2}.&lt;/math&gt;

===Inequality relating means and medians===
If the distribution has finite variance, then the distance between the median and the mean is bounded by one [[standard deviation]].

This bound was proved by Mallows,&lt;ref&gt;{{cite journal |last=Mallows |first=Colin |title=Another comment on O'Cinneide |journal=The American Statistician |date=August 1991 |volume=45 |issue=3 |pages=257 | doi = 10.1080/00031305.1991.10475815}}&lt;/ref&gt; who used [[Jensen's inequality]] twice, as follows. We have

: &lt;math&gt;\begin{align}
  |\mu - m| = |\operatorname{E}(X - m)| &amp; \leq \operatorname{E}(|X - m|) \\
                                        &amp; \leq \operatorname{E}(|X - \mu|) \\
                                        &amp; \leq \sqrt{\operatorname{E}\left((X - \mu)^2\right)} = \sigma.
\end{align}&lt;/math&gt;

The first and third inequalities come from Jensen's inequality applied to the absolute-value function and the square function, which are each convex.  The second inequality comes from the fact that a median minimizes the [[absolute deviation]] function

:&lt;math&gt;a \mapsto \operatorname{E}(|X-a|).\,&lt;/math&gt;

This proof also follows directly from [[Cantelli's inequality]].&lt;ref&gt;[http://www.montefiore.ulg.ac.be/~kvansteen/MATH0008-2/ac20112012/Class3/Chapter2_ac1112_vfinalPartII.pdf K.Van Steen ''Notes on probability and statistics'' ]&lt;/ref&gt;
The result can be generalized to obtain a multivariate version of the inequality,&lt;ref name=PicheRandomVectorsSequences&gt;{{cite book|last=Piché|first=Robert|title=Random Vectors and Random Sequences|year=2012|publisher=Lambert Academic Publishing|isbn=978-3659211966}}&lt;/ref&gt; as follows:
:  &lt;math&gt;\begin{align}
  \|\mu - m\| = \| \operatorname{E}(X - m)\|
    &amp; \leq  \operatorname{E}\|X - m\|  \\
    &amp; \leq  \operatorname{E}(\| X - \mu\| ) \\
    &amp; \leq \sqrt{ \operatorname{E}\left(\|X - \mu\|^2\right) }
      = \sqrt{ \operatorname{trace}\left(\operatorname{var}(X)\right) }
\end{align}&lt;/math&gt;

where ''m'' is a [[spatial median]], that is, a minimizer of the function
&lt;math&gt;a \mapsto \operatorname{E}(\|X-a\|).\,&lt;/math&gt; The spatial median is unique when the data-set's dimension is two or more.&lt;ref name="Kemperman"&gt;{{cite journal |first=Johannes H. B. |last=Kemperman |title=The median of a finite measure on a Banach space: Statistical data analysis based on the L1-norm and related methods |journal=Papers from the First International Conference held at Neuchâtel, August 31–September 4, 1987 |editor-first=Yadolah |editor-last=Dodge |publisher=North-Holland Publishing Co. |location=Amsterdam |pages=217–230 |mr=949228 |ref=harv |year=1987 }}&lt;/ref&gt;&lt;ref name="MilasevicDucharme"&gt;{{cite journal |first=Philip |last=Milasevic |first2=Gilles R. |last2=Ducharme |title=Uniqueness of the spatial median |journal=[[Annals of Statistics]] |volume=15 |year=1987 |number=3 |pages=1332–1333 |mr=902264 |ref=harv |doi=10.1214/aos/1176350511}}&lt;/ref&gt; An alternative proof uses the one-sided Chebyshev inequality; it appears in [[An inequality on location and scale parameters#An application: distance between the mean and the median|an inequality on location and scale parameters]].

==Jensen's inequality for medians==

Jensen's inequality states that for any random variable ''x'' with a finite expectation ''E''(''x'') and for any convex function ''f''

: &lt;math&gt; f[ E(x) ] \le E[ f(x) ] &lt;/math&gt;

It has been shown&lt;ref name=Merkle2005&gt;{{cite journal |last=Merkle |first=M. |year=2005 |title=Jensen’s inequality for medians |journal=Statistics &amp; Probability Letters |volume=71 |issue=3 |pages=277–281 |doi=10.1016/j.spl.2004.11.010 }}&lt;/ref&gt; that if ''x'' is a real variable with a unique median ''m'' and ''f'' is a C function then

: &lt;math&gt; f(m) \le \operatorname{Median}[ f(x)] &lt;/math&gt;

A C function is a real valued function, defined on the set of real numbers ''R'', with the property that for any real ''t''

: &lt;math&gt; f^{-1}\left( \,(-\infty, t]\, \right) = \{ x \in R \mid f(x) \le t \} &lt;/math&gt;

is a closed [[interval (mathematics)|interval]], a [[singleton (mathematics)|singleton]] or an [[empty set]].

==Medians for samples==

===The sample median===

===={{anchor|Ninther}} {{anchor|Remedian}} Efficient computation of the sample median====
Even though [[sorting algorithm|comparison-sorting]] ''n'' items requires {{math|[[Big O notation|Ω]](''n'' log ''n'')}} operations, [[selection algorithm]]s can compute the [[order statistic|{{mvar|k}}'th-smallest of {{mvar|n}} items]] with only {{math|Θ(''n'')}} operations. This includes the median, which is the {{math|{{sfrac|''n''|2}}}}'th order statistic (or for an even number of samples, the [[arithmetic mean]] of the two middle order statistics).

Selection algorithms still have the downside of requiring {{math|Ω(''n'')}} memory, that is, they need to have the full sample (or a linear-sized portion of it) in memory. Because this, as well as the linear time requirement, can be prohibitive, several estimation procedures for the median have been developed. A simple one is the median of three rule, which estimates the median as the median of a three-element subsample; this is commonly used as a subroutine in the [[quicksort]] sorting algorithm, which uses an estimate of its input's median. A more [[robust estimator]] is [[John Tukey|Tukey]]'s ''ninther'', which is the median of three rule applied with limited recursion:&lt;ref&gt;{{cite journal |first1=Jon L. |last1=Bentley |first2=M. Douglas |last2=McIlroy |title=Engineering a sort function |journal=Software—Practice and Experience |volume=23 |issue=11 |pages=1249–1265 |year=1993 |url=http://citeseer.ist.psu.edu/viewdoc/summary?doi=10.1.1.14.8162 |doi=10.1002/spe.4380231105}}&lt;/ref&gt; if {{mvar|A}} is the sample laid out as an [[array (data structure)|array]], and

:{{math|med3(''A'') {{=}} median(''A''[1], ''A''[{{sfrac|''n''|2}}], ''A''[''n''])}},

then

:{{math|ninther(''A'') {{=}} med3(med3(''A''[1 ... {{sfrac|1|3}}''n'']), med3(''A''[{{sfrac|1|3}}''n'' ... {{sfrac|2|3}}''n'']), med3(''A''[{{sfrac|2|3}}''n'' ... ''n'']))}}

The ''remedian'' is an estimator for the median that requires linear time but sub-linear memory, operating in a single pass over the sample.&lt;ref&gt;{{cite journal |last1=Rousseeuw |first1=Peter J. |last2=Bassett |first2=Gilbert W. Jr.|title=The remedian: a robust averaging method for large data sets |journal=J. Amer. Stat. Soc. |volume=85 |issue=409 |year=1990 |url=http://wis.kuleuven.be/stat/robust/papers/publications-1990/rousseeuwbassett-remedian-jasa-1990.pdf |pages=97–104 |doi=10.1080/01621459.1990.10475311}}&lt;/ref&gt;

====Easy explanation of the sample median====
In individual series (if number of observation is very low) first one must arrange all the observations in order. Then count(''n'') is the total number of observation in given data.

If '''''n'' is odd''' then Median (''M'') = value of ((''n''&amp;nbsp;+&amp;nbsp;1)/2)th item term.

If '''''n'' is even''' then Median (''M'') = value of [(''n''/2)th item term + (''n''/2 + 1)th item term]/2

;For an odd number of values

As an example, we will calculate the sample median for the following set of observations: 1, 5, 2, 8, 7.

Start by sorting the values: 1, 2, 5, 7, 8.

In this case, the median is 5 since it is the middle observation in the ordered list.

The median is the ((''n''&amp;nbsp;+&amp;nbsp;1)/2)th item, where ''n'' is the number of values. For example, for the list {1,&amp;nbsp;2,&amp;nbsp;5,&amp;nbsp;7,&amp;nbsp;8}, we have ''n''&amp;nbsp;=&amp;nbsp;5, so the median is the ((5&amp;nbsp;+&amp;nbsp;1)/2)th item.
: median = (6/2)th item
: median = 3rd item
: median = 5

;For an even number of values

As an example, we will calculate the sample median for the following set of observations: 1, 6, 2, 8, 7, 2.

Start by sorting the values: 1, 2, 2, 6, 7, 8.

In this case, the arithmetic mean of the two middlemost terms is (2 + 6)/2 = 4. Therefore, the median is 4 since it is the arithmetic mean of the middle observations in the ordered list.

====Sampling distribution====

The distributions of both the sample mean and the sample median were determined by [[Pierre-Simon Laplace|Laplace]].&lt;ref name=Stigler1973&gt;{{cite journal
 | last = Stigler
 | first = Stephen
 | authorlink = Stephen Stigler
 |date= December 1973
 | title = Studies in the History of Probability and Statistics. XXXII: Laplace, Fisher and the Discovery of the Concept of Sufficiency
 | journal = Biometrika
 | volume = 60
 | issue = 3
 | pages = 439–445
 | doi = 10.1093/biomet/60.3.439
 | mr = 0326872 | jstor = 2334992
 }}&lt;/ref&gt; The distribution of the sample median from a population with a density function &lt;math&gt;f(x)&lt;/math&gt; is asymptotically normal with mean &lt;math&gt;m&lt;/math&gt; and variance&lt;ref name="Rider1960"&gt;{{cite journal |last=Rider |first=Paul R. |year=1960 |title=Variance of the median of small samples from several special populations |journal=[[Journal of the American Statistical Association|J. Amer. Statist. Assoc.]] |volume=55 |issue=289 |pages=148–150 |doi=10.1080/01621459.1960.10482056 }}&lt;/ref&gt;

: &lt;math&gt; \frac{ 1 }{ 4n f( m )^2 }&lt;/math&gt;

where &lt;math&gt;m&lt;/math&gt; is the median of &lt;math&gt;f(x)&lt;/math&gt; and &lt;math&gt;n&lt;/math&gt; is the sample size.

These results have also been extended.&lt;ref name="Stuart1994"&gt;{{cite book |title=Kendall's Advanced Theory of Statistics |first=Alan |last=Stuart |first2=Keith |last2=Ord |location=London |publisher=Arnold |year=1994 |isbn=0340614307 }}&lt;/ref&gt; It is now known for the &lt;math&gt;p&lt;/math&gt;-th quantile that the distribution of the sample &lt;math&gt;p&lt;/math&gt;-th quantile is asymptotically normal around the &lt;math&gt;p&lt;/math&gt;-th quantile with variance equal to
: &lt;math&gt; \frac{p( 1 - p )}{nf(x_p)^2}&lt;/math&gt;
where &lt;math&gt;f(x_{p})&lt;/math&gt; is the value of the distribution density at the &lt;math&gt;p&lt;/math&gt;-th quantile.

In the case of a discrete variable, the sampling distribution of the median for small-samples can be investigated as follows.  We take the sample size to be an odd number &lt;math&gt; N = 2n + 1 &lt;/math&gt;.  If a given value &lt;math&gt; v &lt;/math&gt; is to be the median of the sample then two conditions must be satisfied.  The first is that at most &lt;math&gt; n &lt;/math&gt; observations can have a value of &lt;math&gt; v - 1 &lt;/math&gt; or less.  The second is that at most &lt;math&gt; n &lt;/math&gt; observations can have a value of &lt;math&gt; v + 1 &lt;/math&gt; or more.  Let &lt;math&gt; i &lt;/math&gt; be the number of observations which have a value of &lt;math&gt; v - 1 &lt;/math&gt; or less and let &lt;math&gt; k &lt;/math&gt; be the number of observations which have a value of &lt;math&gt; v + 1 &lt;/math&gt; or more.  Then &lt;math&gt; i &lt;/math&gt; and &lt;math&gt; k &lt;/math&gt; both have a minimum value of 0 and a maximum of &lt;math&gt; n &lt;/math&gt;.  If an observation has a value below &lt;math&gt; v &lt;/math&gt;, it is not relevant how far below &lt;math&gt; v &lt;/math&gt; it is and conversely, if an observation has a value above &lt;math&gt; v &lt;/math&gt;, it is not relevant how far above &lt;math&gt; v &lt;/math&gt; it is.  We can therefore represent the observations as following a trinomial distribution with probabilities  &lt;math&gt; F(v - 1) &lt;/math&gt;, &lt;math&gt; f(v) &lt;/math&gt; and  &lt;math&gt; 1 - F(v) &lt;/math&gt;.  The probability that the median &lt;math&gt; m &lt;/math&gt; will have a value &lt;math&gt; v &lt;/math&gt; is then given by

: &lt;math&gt; \Pr(m = v) = \sum_{i=0}^n \sum_{k=0}^n \frac{N!}{i!(N-i-k)!k!} [F(v-1)]^i [f(v)]^{N-i-k} [1 - F(v)]^k. &lt;/math&gt;

Summing this over all values of &lt;math&gt; v &lt;/math&gt; defines a proper distribution and gives a unit sum.  In practice, the function &lt;math&gt; f(v) &lt;/math&gt; will often not be known but it can be estimated from an observed frequency distribution.  An example is given in the following table where the actual distribution is not known but a sample of 3,800 observations allows a sufficiently accurate assessment of &lt;math&gt; f(v) &lt;/math&gt;.

{| class="wikitable" style="margin-left:auto;margin-right:auto;text-align:center;"
! v !! 0 !! 0.5 !! 1 !! 1.5 !! 2 !! 2.5 !! 3 !! 3.5 !! 4 !! 4.5 !! 5
|-
! f(v)
| 0.000 || 0.008 || 0.010 || 0.013 || 0.083 || 0.108 || 0.328 || 0.220 ||  0.202 || 0.023 || 0.005
|-
! F(v)
| 0.000 || 0.008 || 0.018 || 0.031 || 0.114 || 0.222 || 0.550 || 0.770 ||  0.972 || 0.995 || 1.000
|-
|}

Using these data it is possible to investigate the effect of sample size on the standard errors of the mean and median.  The observed mean is 3.16, the observed raw median is 3 and the observed interpolated median is 3.174.  The following table gives some comparison statistics.  The standard error of the median is given both from the above expression for &lt;math&gt; pr(m = v) &lt;/math&gt; and from the asymptotic approximation given earlier.

{| class="wikitable" style="margin-left:auto;margin-right:auto;text-align:center;"
! {{Diagonal split header|Statistic|Sample size}} !! 3 !! 9 !! 15 !! 21
|-
! Expected value of median
| 3.198 || 3.191 || 3.174 || 3.161
|-
! Standard error of median (above formula)
| 0.482 || 0.305 || 0.257 || 0.239
|-
! Standard error of median (asymptotic approximation)
| 0.879 || 0.508 || 0.393 || 0.332
|-
! Standard error of mean
| 0.421 || 0.243 || 0.188 || 0.159
|}
The expected value of the median falls slightly as sample size increases while, as would be expected, the standard errors of both the median and the mean are proportionate to the inverse square root of the sample size.  The asymptotic approximation errs on the side of caution by overestimating the standard error.

In the case of a continuous variable, the following argument can be used.  If a given value &lt;math&gt; v &lt;/math&gt; is to be the median, then one observation must take the value &lt;math&gt; v &lt;/math&gt;.  The elemental probability of this is &lt;math&gt; f(v)\,dv &lt;/math&gt;.  Then, of the remaining &lt;math&gt; 2n &lt;/math&gt; observations, exactly &lt;math&gt; n &lt;/math&gt; of them must be above &lt;math&gt; v &lt;/math&gt; and the remaining &lt;math&gt; n &lt;/math&gt; below.  The probability of this is the &lt;math&gt; n &lt;/math&gt;th term of a binomial distribution with parameters &lt;math&gt; F(v) &lt;/math&gt; and &lt;math&gt; 2n &lt;/math&gt;.  Finally we multiply by &lt;math&gt; 2n+1 &lt;/math&gt; since any of the observations in the sample can be the median observation.  Hence the elemental probability of the median at the point &lt;math&gt; v &lt;/math&gt;  is given by

: &lt;math&gt; f(v) \frac{(2n)!}{n!n!} [F(v)]^n [1 - F(v)]^n (2n + 1) \, dv. &lt;/math&gt;

Now we introduce the beta function.  For integer arguments &lt;math&gt; \alpha &lt;/math&gt; and &lt;math&gt; \beta &lt;/math&gt;, this can be expressed as &lt;math&gt; \Beta(\alpha,\beta) = (\alpha - 1)! (\beta - 1)! / (\alpha + \beta - 1)! &lt;/math&gt;.  Also, we note that &lt;math&gt; f(v) = dF(v)/dv &lt;/math&gt;.  Using these relationships and setting both &lt;math&gt; \alpha &lt;/math&gt; and &lt;math&gt; \beta &lt;/math&gt; equal to  &lt;math&gt; (n+1) &lt;/math&gt; allows the last expression to be written as

: &lt;math&gt; \frac{[F(v)]^n[1 - F(v)]^n}{\Beta(n+1,n+1)} \, dF(v) &lt;/math&gt;

Hence the density function of the median is a symmetric beta distribution over the unit interval which supports &lt;math&gt; F(v) &lt;/math&gt;.  Its mean, as we would expect, is 0.5 and its variance is  &lt;math&gt; 1/(4(N+2)) &lt;/math&gt;. The corresponding variance of the sample median is

: &lt;math&gt;\frac{ 1 }{ 4 (N + 2) f( m )^2 }.&lt;/math&gt;

However this finding can only be used if the density function &lt;math&gt; f(v) &lt;/math&gt; is known or can be assumed.  As this will not always be the case, the median variance has to be estimated sometimes from the sample data.

;Estimation of variance from sample data

The value of &lt;math&gt;(2 f(x))^{-2}&lt;/math&gt;—the asymptotic value of &lt;math&gt;n^{-\frac{1}{2}} (\nu - m)&lt;/math&gt; where &lt;math&gt;\nu&lt;/math&gt; is the population median—has been studied by several authors. The standard "delete one" [[Resampling (statistics)#Jackknife|jackknife]] method produces [[consistent estimator|inconsistent]] results.&lt;ref name=Efron1982&gt;{{cite book |last=Efron |first=B. |year=1982 |title=The Jackknife, the Bootstrap and other Resampling Plans |publisher=SIAM |location=Philadelphia |isbn=0898711797 }}&lt;/ref&gt; An alternative—the "delete k" method—where &lt;math&gt;k&lt;/math&gt; grows with the sample size has been shown to be asymptotically consistent.&lt;ref name=Shao1989&gt;{{cite journal |last=Shao |first=J. |last2=Wu |first2=C. F. |year=1989 |title=A General Theory for Jackknife Variance Estimation |journal=[[Annals of Statistics|Ann. Stat.]] |volume=17 |issue=3 |pages=1176–1197 |jstor=2241717 |doi=10.1214/aos/1176347263}}&lt;/ref&gt; This method may be computationally expensive for large data sets. A bootstrap estimate is known to be consistent,&lt;ref name=Efron1979&gt;{{cite journal |last=Efron |first=B. |year=1979 |title=Bootstrap Methods: Another Look at the Jackknife |journal=[[Annals of Statistics|Ann. Stat.]] |volume=7 |issue=1 |pages=1–26 |jstor=2958830 |doi=10.1214/aos/1176344552}}&lt;/ref&gt; but converges very slowly ([[computational complexity theory|order]] of &lt;math&gt;n^{-\frac{1}{4}}&lt;/math&gt;).&lt;ref name=Hall1988&gt;{{cite journal |last=Hall |first=P. |last2=Martin |first2=M. A. |year=1988 |title=Exact Convergence Rate of Bootstrap Quantile Variance Estimator |journal=Probab Theory Related Fields |volume=80 |issue=2 |pages=261–268 |doi=10.1007/BF00356105 }}&lt;/ref&gt; Other methods have been proposed but their behavior may differ between large and small samples.&lt;ref name=Jimenez-Gamero2004&gt;{{cite journal |last=Jiménez-Gamero |first=M. D. |last2=Munoz-García |first2=J. |first3=R. |last3=Pino-Mejías |year=2004 |title=Reduced bootstrap for the median |journal=Statistica Sinica |volume=14 |issue=4 |pages=1179–1198 |doi= |url=http://www3.stat.sinica.edu.tw/statistica/password.asp?vol=14&amp;num=4&amp;art=11 }}&lt;/ref&gt;

;Efficiency

The [[Efficiency (statistics)|efficiency]] of the sample median, measured as the ratio of the variance of the mean to the variance of the median, depends on the sample size and on the underlying population distribution. For a sample of size &lt;math&gt;N = 2n + 1&lt;/math&gt; from the [[normal distribution]], the efficiency for large N is

: &lt;math&gt; \frac{2}{\pi} \frac{N+2}{N} &lt;/math&gt;

The efficiency tends to &lt;math&gt; \frac{2}{\pi} &lt;/math&gt; as &lt;math&gt;N&lt;/math&gt; tends to infinity.

===Other estimators===
For univariate distributions that are ''symmetric'' about one median, the [[Hodges–Lehmann estimator]] is a [[robust statistics|robust]] and highly [[Efficiency (statistics)|efficient estimator]] of the population median.&lt;ref name="HM"&gt;{{cite book |last1=Hettmansperger |first1=Thomas P. |last2=McKean |first2=Joseph W. |title=Robust nonparametric statistical methods |series=Kendall's Library of Statistics |volume=5 |publisher=Edward Arnold |location=London |year=1998 |isbn=0-340-54937-8 |mr=1604954 |ref=harv }}&lt;/ref&gt;

If data are represented by a [[statistical model]] specifying a particular family of [[probability distribution]]s, then estimates of the median can be obtained by fitting that family of probability distributions to the data and calculating the theoretical median of the fitted distribution.{{Citation needed|date=May 2012}} [[Pareto interpolation]] is an application of this when the population is assumed to have a [[Pareto distribution]].

===Coefficient of dispersion===

The coefficient of dispersion (CD) is defined as the ratio of the average absolute deviation from the median to the median of the data.&lt;ref name=Bonett2006&gt;{{cite journal | last1 = Bonett | first1 = DG | last2 = Seier | first2 = E | year = 2006 | title = Confidence interval for a coefficient of dispersion in non-normal distributions | url = | journal = Biometrical Journal | volume = 48 | issue = 1| pages = 144–148 | doi=10.1002/bimj.200410148}}&lt;/ref&gt; It is a statistical measure used by the states of [[Iowa]], [[New York (state)|New York]] and [[South Dakota]] in estimating dues taxes.&lt;ref&gt;{{cite web |url=http://www.iowa.gov/tax/locgov/Statistical_Calculation_Definitions.pdf|title=Statistical Calculation Definitions for Mass Appraisal |work=Iowa.gov |quote=Median Ratio: The ratio located midway between the highest ratio and the lowest ratio when individual ratios for a class of realty are ranked in ascending or descending order. The median ratio is most frequently used to determine the level of assessment for a given class of real estate.|archive-url=https://web.archive.org/web/20101111214903/http://iowa.gov/tax/locgov/Statistical_Calculation_Definitions.pdf|archive-date=11 November 2010}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.tax.ny.gov/research/property/reports/cod/2010mvs/reporttext.htm|title=Assessment equity in New York: Results from the 2010 market value survey|archive-url=https://web.archive.org/web/20121106015231/http://www.tax.ny.gov/research/property/reports/cod/2010mvs/reporttext.htm|archive-date=6 November 2012}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.state.sd.us/drr2/publications/assess1199.pdf|title=Summary of the Assessment Process|work=state.sd.us|publisher=South Dakota Department of Revenue - Property/Special Taxes Division|archive-url=https://web.archive.org/web/20090510034115/http://www.state.sd.us/drr2/publications/assess1199.pdf|archive-date=10 May 2009}}&lt;/ref&gt; In symbols

: &lt;math&gt; CD = \frac{ 1 }{ n } \frac{ \sum| m - x | }{ m } &lt;/math&gt;

where ''n'' is the sample size, ''m'' is the sample median and ''x'' is a variate. The sum is taken over the whole sample.

Confidence intervals for a two-sample test in which the sample sizes are large have been derived by Bonett and Seier&lt;ref name="Bonett2006"/&gt; This test assumes that both samples have the same median but differ in the dispersion around it. The confidence interval (CI) is bounded inferiorly by

: &lt;math&gt; \exp \left[ \log \left( \frac{t_a} {t_b} \right) - z_\alpha \left( \operatorname{var} \left[ \log \left( \frac{t_a} {t_b} \right) \right] \right)^\frac{1}{2} \right] &lt;/math&gt;

where ''t''&lt;sub&gt;j&lt;/sub&gt; is the mean absolute deviation of the ''j''&lt;sup&gt;th&lt;/sup&gt; sample, var() is the variance and ''z&lt;sub&gt;α&lt;/sub&gt;'' is the value from the normal distribution for the chosen value of ''α'': for ''α'' = 0.05, ''z&lt;sub&gt;α&lt;/sub&gt;'' = 1.96. The following formulae are used in the derivation of these confidence intervals

: &lt;math&gt; \operatorname{var} [ \log ( t_a ) ] = \frac{1}{n} \left[ \frac{ s_a^2 } { t_a^2 } + \left( \frac{x_a - \bar{x}} {t_a} \right)^2 - 1 \right] &lt;/math&gt;

: &lt;math&gt; \operatorname{var} \left[ \log\left( \frac{t_a}{t_b} \right) \right] = \operatorname{var}[ \log( t_a ) ] + \operatorname{var}[ \log( t_b )] - 2r (\operatorname{var}[ \log(t_a) ] \operatorname{var}[ \log( t_b ) ] )^\frac{1}{2} &lt;/math&gt;

where ''r'' is the Pearson correlation coefficient between the squared deviation scores

: &lt;math&gt; d_{ia} = | x_{ia} - \bar{x}_a | &lt;/math&gt; and &lt;math&gt; d_{ib} = |x_{ib} - \bar{x}_b | &lt;/math&gt;

''a'' and ''b'' here are constants equal to 1 and 2, ''x'' is a variate and ''s'' is the standard deviation of the sample.

==Multivariate median==
Previously, this article discussed the univariate median, when the sample or population had one-dimension. When the dimension is two or higher, there are multiple concepts that extend the definition of the univariate median; each such multivariate median agrees with the univariate median when the dimension is exactly one.&lt;ref name="HM" /&gt;&lt;ref&gt;Small, Christopher G. "A survey of multidimensional medians." International Statistical Review/Revue Internationale de Statistique (1990): 263–277. {{doi|10.2307/1403809}} {{JSTOR|1403809}}&lt;/ref&gt;&lt;ref&gt;Niinimaa, A., and H. Oja. "Multivariate median." Encyclopedia of statistical sciences (1999).&lt;/ref&gt;&lt;ref&gt;Mosler, Karl. Multivariate Dispersion, Central Regions, and Depth: The Lift Zonoid Approach. Vol. 165. Springer Science &amp; Business Media, 2012.&lt;/ref&gt;

=== Medoid ===
Let  be a set of  points in a space with a [[Metric (mathematics)|distance function]] &lt;math&gt;d&lt;/math&gt;. [[Medoid]] is defined as

&lt;math&gt;{\displaystyle x_{\text{medoid}}={\text{argmin}}_{y\in \{x_{1},x_{2},\cdots ,x_{n}\}}\sum _{i=1}^{n}d(y,x_{i}).} &lt;/math&gt;

The medoid is often used in clustering using the [[K-medoids|k-medoid]] algorithm.

===Marginal median===
The marginal median is defined for vectors defined with respect to a fixed set of coordinates. A marginal median is defined to be the vector whose components are univariate medians. The marginal median is easy to compute, and its properties were studied by Puri and Sen.&lt;ref name="HM" /&gt;&lt;ref&gt;Puri, Madan L.; Sen, Pranab K.; ''Nonparametric Methods in Multivariate Analysis'', John Wiley &amp; Sons, New York, NY, 197l. (Reprinted by Krieger Publishing)&lt;/ref&gt;

===Spatial median===
{{anchor|Spatial_median}}{{anchor|Spatial}}{{anchor|Spatial median}}

For ''N'' vectors in a [[normed space|normed]] [[vector space]], a '''spatial median''' &lt;!-- ''[[spatial median]]'' redirects here --&gt; minimizes the average distance

:&lt;math&gt;a \mapsto \frac 1 N \sum_n (\|x_n-a\|), \,&lt;/math&gt;

where ''x''&lt;sub&gt;''n''&lt;/sub&gt; and ''a'' are vectors. The spatial median is unique when the data-set's dimension is two or more and the norm is the [[Euclidean norm]] (or another [[strictly convex function|strictly convex]] norm).&lt;ref name="Kemperman" /&gt;&lt;ref name="MilasevicDucharme" /&gt;&lt;ref name="HM" /&gt; The spatial median is also called the '''L1 median''', even when the norm is Euclidean. Other names are used especially for finite sets of points: '''[[geometric median]]''', Fermat point (in mechanics), or Weber or Fermat-Weber point (in geographical [[location theory]]).&lt;ref&gt;
{{cite journal
 | last = Wesolowsky | first = G.
 | title = The Weber problem: History and perspective
 | journal = Location Science
 | volume = 1
 | pages = 5–23
 | year = 1993
 | ref = harv}}
&lt;/ref&gt; In the special case where the norm is an [[L1-norm]], then the spatial median and the marginal median are the same.

More generally, a spatial median is defined as a minimizer of

:&lt;math&gt;a \mapsto \frac 1 N \sum_n{(\|x_n-a\| - \|x_n\|)}; &lt;/math&gt;&lt;ref
name="HM" /&gt;&lt;ref name="Oja 2010 xiv+232"&gt;{{cite book |last=Oja |first=Hannu |title=Multivariate nonparametric methods with&amp;nbsp;''R'': An approach based on spatial signs and ranks |series=Lecture Notes in Statistics |volume=199 |publisher=Springer |location=New York, NY |year=2010 |pages=xiv+232 |isbn=978-1-4419-0467-6 |doi=10.1007/978-1-4419-0468-3 |mr=2598854 |ref=harv }}&lt;/ref&gt;

this general definition is convenient for defining a spatial median of a [[population (statistics)|population]] in a [[dimension (linear algebra)|finite-dimensional]] normed space, for example, for distributions without a finite mean.&lt;ref name="Kemperman" /&gt;&lt;ref name="HM" /&gt; Spatial medians are defined for random vectors with values in a [[Banach space]].&lt;ref name="Kemperman" /&gt;

The spatial median is a [[Robust statistics|robust]] and highly [[Efficiency (statistics)|efficient]] estimator of a [[central tendency]] of a population.&lt;ref name="HM" /&gt;&lt;ref name="Oja 2010 xiv+232"/&gt;&lt;ref&gt;{{cite journal |last=Vardi |first=Yehuda |last2=Zhang |first2=Cun-Hui |title=The multivariate l1-median and associated data depth |journal=[[Proceedings of the National Academy of Sciences of the United States of America]] |volume=97 |year=2000 |issue=4 |pages=1423–1426 |doi=10.1073/pnas.97.4.1423 |pmc=26449 |pmid=10677477}}&lt;/ref&gt;&lt;ref&gt;
{{cite journal
 | last1 = Lopuhaä | first1 = Hendrick P.
 | last2 = Rousseeuw | first2 = Peter J. | author2-link = Peter Rousseeuw
 | title = Breakdown points of affine equivariant estimators of multivariate location and covariance matrices
 | year = 1991
 | journal = [[Annals of Statistics]]
 | volume = 19
 | pages = 229–248
 | issue = 1
 | doi = 10.1214/aos/1176347978
 | ref = harv
 | jstor=2241852| url = http://repository.tudelft.nl/islandora/object/uuid%3A8e67fb99-7cb7-4b11-8e6a-02039c7ed1bb/datastream/OBJ/view}}
&lt;/ref&gt;

===Other multivariate medians===

An alternative generalization of the spatial median in higher dimensions that does not relate to a particular metric is the [[Centerpoint (geometry)|centerpoint]].

==Other median-related concepts==

===Interpolated median===
When dealing with a discrete variable, it is sometimes useful to regard the observed values as being midpoints of underlying continuous intervals. An example of this is a Likert scale, on which opinions or preferences are expressed on a scale with a set number of possible responses. If the scale consists of the positive integers, an observation of 3 might be regarded as representing the interval from 2.50 to 3.50. It is possible to estimate the median of the underlying variable. If, say, 22% of the observations are of value 2 or below and 55.0% are of 3 or below (so 33% have the value 3), then the median &lt;math&gt; m &lt;/math&gt; is 3 since the median is the smallest value of &lt;math&gt; x &lt;/math&gt; for which &lt;math&gt; F(x) &lt;/math&gt; is greater than a half. But the interpolated median is somewhere between 2.50 and 3.50.  First we add half of the interval width &lt;math&gt; w &lt;/math&gt; to the median to get the upper bound of the median interval. Then we subtract that proportion of the interval width which equals the proportion of the 33% which lies above the 50% mark.  In other words, we split up the interval width pro rata to the numbers of observations.  In this case, the 33% is split into 28% below the median and 5% above it so we subtract 5/33 of the interval width from the upper bound of 3.50 to give an interpolated median of 3.35. More formally, if the values &lt;math&gt; f(x) &lt;/math&gt; are known, the interpolated median can be calculated from

: &lt;math&gt; m_\text{int} = m + w\left[\frac{1}{2} - \frac{F( m ) - \frac{1}{2} }{f( m )}\right]. &lt;/math&gt;

Alternatively, if in an observed sample there are &lt;math&gt; k &lt;/math&gt; scores above the median category, &lt;math&gt; j &lt;/math&gt; scores in it and &lt;math&gt; i &lt;/math&gt; scores below it then the interpolated median is given by

: &lt;math&gt; m_\text{int} = m + \frac{w}{2} \left[\frac{k - i} j\right]. &lt;/math&gt;

===Pseudo-median===
For univariate distributions that are ''symmetric'' about one median, the [[Hodges–Lehmann estimator]] is a robust and highly efficient estimator of the population median; for non-symmetric distributions, the Hodges–Lehmann estimator is a robust and highly efficient estimator of the population ''pseudo-median'', which is the median of a symmetrized distribution and which is close to the population median.&lt;ref&gt;{{Cite journal|last=Pratt|first=William K.|last2=Cooper|first2=Ted J.|last3=Kabir|first3=Ihtisham|date=1985-07-11|title=Pseudomedian Filter|journal=Architectures and Algorithms for Digital Image Processing II|volume=0534|pages=34|doi=10.1117/12.946562}}&lt;/ref&gt; The Hodges–Lehmann estimator has been generalized to multivariate distributions.&lt;ref name="Oja 2010 xiv+232"/&gt;

===Variants of regression===
The [[Theil–Sen estimator]] is a method for [[robust statistics|robust]] [[linear regression]] based on finding medians of [[slope]]s.&lt;ref&gt;{{citation
 | last = Wilcox | first = Rand R.
 | contribution = Theil–Sen estimator
 | isbn = 978-0-387-95157-7
 | pages = 207–210
 | publisher = Springer-Verlag
 | title = Fundamentals of Modern Statistical Methods: Substantially Improving Power and Accuracy
 | url = https://books.google.com/books?id=YSFb4QX2UIoC&amp;pg=PA207
 | year = 2001}}.&lt;/ref&gt;

===Median filter===
{{Main|Median filter}}
In the context of [[image processing]] of [[monochrome]] [[raster image]]s there is a type of noise, known as the [[salt and pepper noise]], when each pixel independently becomes black (with some small probability) or white (with some small probability), and is unchanged otherwise (with the probability close to 1). An image constructed of median values of neighborhoods (like 3×3 square) can effectively [[noise reduction|reduce noise]] in this case.{{Citation needed |date=May 2012 }}

===Cluster analysis===
{{main|k-medians clustering}}
In [[cluster analysis]], the [[k-medians clustering]] algorithm provides a way of defining clusters, in which the criterion of maximising the distance between cluster-means that is used in [[k-means clustering]], is replaced by maximising the distance between cluster-medians.

===Median–median line===

This is a method of robust regression. The idea dates back to [[Abraham Wald|Wald]] in 1940 who suggested dividing a set of bivariate data into two halves depending on the value of the independent parameter &lt;math&gt;x&lt;/math&gt;: a left half with values less than the median and a right half with values greater than the median.&lt;ref name=Wald1940&gt;{{cite journal |last=Wald |first=A. |year=1940 |title=The Fitting of Straight Lines if Both Variables are Subject to Error |journal=[[Annals of Mathematical Statistics]] |volume=11 |issue=3 |pages=282–300 |jstor=2235677 |doi=10.1214/aoms/1177731868 }}&lt;/ref&gt; He suggested taking the means of the dependent &lt;math&gt;y&lt;/math&gt; and independent &lt;math&gt;x&lt;/math&gt; variables of the left and the right halves and estimating the slope of the line joining these two points. The line could then be adjusted to fit the majority of the points in the data set.

Nair and Shrivastava in 1942 suggested a similar idea but instead advocated dividing the sample into three equal parts before calculating the means of the subsamples.&lt;ref name=Nair1942&gt;{{cite journal |title=On a Simple Method of Curve Fitting |first=K. R. |last=Nair |first2=M. P. |last2=Shrivastava |journal=Sankhyā: The Indian Journal of Statistics |volume=6 |issue=2 |year=1942 |pages=121–132 |jstor=25047749 }}&lt;/ref&gt; Brown and Mood in 1951 proposed the idea of using the medians of two subsamples rather the means.&lt;ref name=Brown1951&gt;{{cite book |last=Brown |first=G. W. |last2=Mood |first2=A. M. |year=1951 |chapter=On Median Tests for Linear Hypotheses |title=Proc Second Berkeley Symposium on Mathematical Statistics and Probability |location=Berkeley, CA |publisher=University of California Press |pages=159–166 |isbn= |zbl=0045.08606 }}&lt;/ref&gt; Tukey combined these ideas and recommended dividing the sample into three equal size subsamples and estimating the line based on the medians of the subsamples.&lt;ref name=Tukey1971&gt;{{cite book |last=Tukey |first=J. W. |year=1977 |title=Exploratory Data Analysis |location=Reading, MA |publisher=Addison-Wesley |isbn=0201076160 }}&lt;/ref&gt;

==Median-unbiased estimators==
{{main|Bias of an estimator#Median-unbiased estimators}}
Any [[Bias of an estimator|''mean''-unbiased estimator]] minimizes the [[risk]] ([[expected loss]]) with respect to the squared-error [[loss function]], as observed by [[Gauss]]. A [[Bias of an estimator#Median unbiased estimators, and bias with respect to other loss functions|''median''-unbiased estimator]] minimizes the risk with respect to the [[Absolute deviation|absolute-deviation]] loss function, as observed by [[Laplace]]. Other [[loss functions]] are used in [[statistical theory]], particularly in [[robust statistics]].

The theory of median-unbiased estimators was revived by [https://web.archive.org/web/20110310043642/http://www.universityofcalifornia.edu/senate/inmemoriam/georgewbrown.htm George W. Brown] in 1947:&lt;ref name="Brown" /&gt;

{{quote|An estimate of a one-dimensional parameter θ will be said to be median-unbiased if, for fixed θ, the median of the distribution of the estimate is at the value θ; i.e., the estimate underestimates just as often as it overestimates. This requirement seems for most purposes to accomplish as much as the mean-unbiased requirement and has the additional property that it is invariant under one-to-one transformation.|page 584}}

Further properties of median-unbiased estimators have been reported.&lt;ref name="Lehmann" /&gt;&lt;ref name="Birnbaum" /&gt;&lt;ref name="vdW" /&gt;&lt;ref name="Pf" /&gt;  Median-unbiased estimators are invariant under [[Injective function|one-to-one transformations]].

There are methods of construction median-unbiased estimators that are optimal (in a sense analogous to minimum-variance property considered for mean-unbiased estimators). Such constructions exist for probability distributions having [[monotone likelihood ratio|monotone likelihood-functions]].&lt;ref&gt;Pfanzagl, Johann. "On optimal median unbiased estimators in the presence of nuisance parameters." The Annals of Statistics (1979): 187–193.&lt;/ref&gt;&lt;ref&gt;Brown, L. D.; Cohen, Arthur; Strawderman, W. E. A Complete Class Theorem for Strict Monotone Likelihood Ratio With Applications. Ann. Statist. 4 (1976), no. 4, 712–722. {{doi|10.1214/aos/1176343543}}. http://projecteuclid.org/euclid.aos/1176343543.&lt;/ref&gt; One such procedure is an analogue of the [[Rao–Blackwell theorem|Rao–Blackwell procedure]] for mean-unbiased estimators: The procedure holds for a smaller class of probability distributions than does the Rao—Blackwell procedure but for a larger class of [[loss function]]s.&lt;ref&gt;Page 713: Brown, L. D.; Cohen, Arthur; Strawderman, W. E. A Complete Class Theorem for Strict Monotone Likelihood Ratio With Applications. Ann. Statist. 4 (1976), no. 4, 712–722. {{doi|10.1214/aos/1176343543}}. http://projecteuclid.org/euclid.aos/1176343543.&lt;/ref&gt;

==History==

The idea of the median appeared in the 13th century in the [[Talmud]] &lt;ref&gt;[http://danadler.com/blog/2014/12/31/talmud-and-modern-economics/ Talmud and Modern Economics]&lt;/ref&gt;&lt;ref&gt;[http://www.wisdom.weizmann.ac.il/math/AABeyond12/presentations/Aumann.pdf Modern Economic Theory in the Talmud] by [[Yisrael Aumann]]&lt;/ref&gt; (further {{citation needed|date=October 2015}} for possible older mentions)

The idea of the median also appeared later in [[Edward Wright (mathematician)|Edward Wright]]'s book on navigation (''Certaine Errors in Navigation'') in 1599 in a section concerning the determination of location with a [[compass]]. Wright felt that this value was the most likely to be the correct value in a series of observations.

In 1757, [[Roger Joseph Boscovich]] developed a regression method based on the [[L1 norm]] and therefore implicitly on the median.&lt;ref name=Stigler1986&gt;{{cite book |last=Stigler |first=S. M. |year=1986 |title=The History of Statistics: The Measurement of Uncertainty Before 1900 |publisher=Harvard University Press |isbn=0674403401 }}&lt;/ref&gt;

In 1774, [[Pierre-Simon Laplace|Laplace]] suggested the median be used as the standard estimator of the value of a posterior pdf. The specific criterion was to minimize the expected magnitude of the error; &lt;math&gt;|\alpha - \alpha^{*}|&lt;/math&gt; where &lt;math&gt;\alpha^{*}&lt;/math&gt; is the estimate and &lt;math&gt;\alpha&lt;/math&gt; is the true value. Laplaces's criterion was generally rejected for 150 years in favor of the [[least squares]] method of [[Carl Friedrich Gauss|Gauss]] and [[Adrien-Marie Legendre|Legendre]] which minimizes &lt;math&gt;(\alpha - \alpha^{*})^{2}&lt;/math&gt; to obtain the mean.&lt;ref&gt;{{cite book|last1=Jaynes|first1=E.T.|title=Probability theory : the logic of science|date=2007|publisher=Cambridge Univ. Press|location=Cambridge [u.a.]|isbn=978-0-521-59271-0|page=172|edition=5. print.}}&lt;/ref&gt; The distribution of both the sample mean and the sample median were determined by Laplace in the early 1800s.&lt;ref name=Stigler1973 /&gt;&lt;ref name=Laplace1818&gt;Laplace PS de (1818) ''Deuxième supplément à la Théorie Analytique des Probabilités'', Paris, Courcier&lt;/ref&gt;

[[Antoine Augustin Cournot]] in 1843 was the first{{citation needed|date=July 2012}} to use the term ''median'' (''valeur médiane'') for the value that divides a probability distribution into two equal halves. [[Gustav Theodor Fechner]] used the median (''Centralwerth'') in sociological and psychological phenomena.&lt;ref name=Keynes1921&gt;Keynes, J.M. (1921) ''[[A Treatise on Probability]]''. Pt II Ch XVII §5 (p 201) (2006 reprint, Cosimo Classics, {{isbn|9781596055308}} : multiple other reprints)&lt;/ref&gt; It had earlier been used only in astronomy and related fields. [[Gustav Theodor Fechner|Gustav Fechner]] popularized the median into the formal analysis of data, although it had been used previously by Laplace.&lt;ref name=Keynes1921/&gt;

[[Francis Galton]] used the English term ''median'' in 1881,&lt;ref name=Galton1881&gt;Galton F (1881) "Report of the Anthropometric Committee" pp 245–260. [https://www.biodiversitylibrary.org/item/94448 ''Report of the 51st Meeting of the British Association for the Advancement of Science'']&lt;/ref&gt; having earlier used the terms ''middle-most value'' in 1869, and the ''medium'' in 1880.&lt;ref&gt;[https://www.encyclopediaofmath.org/index.php/Galton,_Francis ''encyclopediaofmath.org'']&lt;/ref&gt;&lt;ref&gt;[http://www.personal.psu.edu/users/e/c/ecb5/Courses/M475W/WeeklyReadings/Week%2013/DevelopmentOfModernStatistics.pdf  ''personal.psu.edu'']&lt;/ref&gt;

==See also==
{{Portal|Statistics}}
* [[Medoid]]s which are a generalisation of the median in higher dimensions
* [[Central tendency]]
** [[Mean]]
** [[Mode (statistics)|Mode]]
* [[Absolute deviation]]
* [[Bias of an estimator]]
* [[Concentration of measure]] for [[Lipschitz functions]]
* [[Median (geometry)]]
* [[Median graph]]
* [[Median search]]
* [[Median slope]]
* [[Median voter theory]]
* [[Weighted median]]

==References==
{{Reflist|30em|refs=
&lt;ref name="Brown"&gt;{{cite journal |last=Brown |first=George W. |year=1947 |title=On Small-Sample Estimation |journal=[[Annals of Mathematical Statistics]] |volume=18 |issue=4 |pages=582–585 |jstor=2236236 |doi=10.1214/aoms/1177730349}}&lt;/ref&gt;
&lt;ref name="Lehmann"&gt;{{cite journal |authorlink=Erich Leo Lehmann |last=Lehmann |first=Erich L. |year=1951 |title=A General Concept of Unbiasedness |journal=[[Annals of Mathematical Statistics]] |volume=22 |issue=4 |pages=587–592 |jstor=2236928 |doi=10.1214/aoms/1177729549}}&lt;/ref&gt;
&lt;ref name="Birnbaum"&gt;{{cite journal |authorlink=Allan Birnbaum |last=Birnbaum |first=Allan |year=1961 |title=A Unified Theory of Estimation, I |journal=[[Annals of Mathematical Statistics]] |volume=32 |issue=1 |pages=112–135 |jstor=2237612 |doi=10.1214/aoms/1177705145}}&lt;/ref&gt;
&lt;ref name="vdW"&gt;{{cite journal |last=van der Vaart |first=H. Robert |year=1961 |title=Some Extensions of the Idea of Bias |journal=[[Annals of Mathematical Statistics]] |volume=32 |issue=2 |pages=436–447 |jstor=2237754 |doi=10.1214/aoms/1177705051 |mr=125674 }}&lt;/ref&gt;
&lt;ref name="Pf"&gt;{{cite book|title=Parametric Statistical Theory | last1=Pfanzagl | first1=Johann |last2=with the assistance of R. Hamböker |year=1994 |publisher=Walter de Gruyter |isbn=3-11-013863-8 |mr=1291393 }}&lt;/ref&gt;
}}

==External links==
* {{springer|title=Median (in statistics)|id=p/m063310}}
* [http://www.accessecon.com/pubs/EB/2004/Volume3/EB-04C10011A.pdf Median as a weighted arithmetic mean of all Sample Observations]
* [http://www.poorcity.richcity.org/cgi-bin/inequality.cgi On-line calculator]
* [http://www.statcan.gc.ca/edu/power-pouvoir/ch11/median-mediane/5214872-eng.htm Calculating the median]
* [http://mathschallenge.net/index.php?section=problems&amp;show=true&amp;titleid=average_problem A problem involving the mean, the median, and the mode.]
* {{MathWorld | urlname= StatisticalMedian | title= Statistical Median}}
* [http://www.poorcity.richcity.org/oei/#GiniHooverTheil Python script] for Median computations and [[income inequality metrics]]
* [https://arxiv.org/abs/0806.3301 Fast Computation of the Median by Successive Binning]
* [http://www.celiagreen.com/charlesmccreery/statistics/meanmedianmode.pdf 'Mean, median, mode and skewness'], A tutorial devised for first-year psychology students at Oxford University, based on a worked example.

{{PlanetMath attribution|id=5900|title=Median of a distribution}}
{{Statistics|descriptive}}

[[Category:Means]]
[[Category:Robust statistics]]</text>
      <sha1>eb0x2tyw33mtfq2x3ul3gpq5lv162sa</sha1>
    </revision>
  </page>
  <page>
    <title>Minkowski plane</title>
    <ns>0</ns>
    <id>4500167</id>
    <revision>
      <id>796018237</id>
      <parentid>786601558</parentid>
      <timestamp>2017-08-17T23:30:14Z</timestamp>
      <contributor>
        <ip>24.237.158.15</ip>
      </contributor>
      <comment>/* The axioms of a Minkowski plane */ Fixed grammar</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13019">{{about-distinguish|the Benz plane|Minkowski space}}

In mathematics, a '''Minkowski plane''' (named after [[Hermann Minkowski]]) is one of the [[Benz plane]]s: [[M&amp;ouml;bius plane]], [[Laguerre plane]] and Minkowski plane.

==Classical real Minkowski plane==
[[Image:Minkowski-2d3d-model.svg|300px|thumb|classical Minkowski plane: 2d/3d-model]]
Applying the [[pseudo-euclidean]] distance &lt;math&gt;d(P_1,P_2)=(x'_1-x'_2)^2-(y'_1-y'_2)^2&lt;/math&gt; on two points &lt;math&gt;P_i=(x'_i,y'_i)&lt;/math&gt; (instead of the euclidean distance) we get the geometry of ''hyperbolas'', because a pseudo-euclidean circle &lt;math&gt;\{P\in \R^2 \ | \ d(P,M)=r\}&lt;/math&gt; is a [[hyperbola]] with midpoint &lt;math&gt;M&lt;/math&gt;. 

By a transformation of coordinates &lt;math&gt;x_i=x'_i+y'_i&lt;/math&gt;, &lt;math&gt;y_i=x'_i-y'_i&lt;/math&gt;, the pseudo-euclidean distance can be rewritten as &lt;math&gt;d(P_1,P_2)=(x_1-x_2)(y_1-y_2)&lt;/math&gt;. The hyperbolas then have [[asymptotes]] parallel to the non-primed coordinate axes.

The following completion (see Möbius and Laguerre planes) ''homogenizes'' the geometry of hyperbolas:

: &lt;math&gt;\mathcal P:=(\R\cup \{\infty\})^2=
\R^2 \cup (\{\infty\} \times\R) \cup (\R\times\{\infty\}) \ 
     \cup \{(\infty,\infty)\} \ ,
 \ \infty \notin \R&lt;/math&gt;, the set of '''points''',
: &lt;math&gt;\mathcal Z:=\{\{(x,y)\in \R^2 \ | \ y=ax+b\}\cup\{(\infty,\infty)\} \ | 
                           \ a,b \in \R, a\ne 0\}&lt;/math&gt;
::: &lt;math&gt;\cup \{\{(x,y)\in \R^2\ | y=\frac{a}{x-b}+c,x\ne b\}
\cup \{(b,\infty),(\infty,c)\} \ | \ a,b,c \in \R, a\ne 0\},&lt;/math&gt; the set of '''cycles'''.

The [[incidence structure]] &lt;math&gt;({\mathcal P},{\mathcal Z},\in)&lt;/math&gt; is called the '''classical real Minkowski plane'''.

The set of points consists of &lt;math&gt;\R^2&lt;/math&gt;, two copies of &lt;math&gt;\R&lt;/math&gt; and the point &lt;math&gt;(\infty,\infty)&lt;/math&gt;.

Any line &lt;math&gt;y=ax+b ,a\ne0&lt;/math&gt; is completed by point &lt;math&gt;(\infty,\infty)&lt;/math&gt;, any hyperbola
&lt;math&gt; y=\frac{a}{x-b}+c,a\ne0 &lt;/math&gt; by the two points &lt;math&gt;(b,\infty),(\infty,c)&lt;/math&gt; (see figure).

Two points &lt;math&gt;(x_1,y_1)\ne(x_2,y_2)&lt;/math&gt; can not be connected by a cycle if and only if
&lt;math&gt;x_1=x_2&lt;/math&gt; or &lt;math&gt;y_1=y_2&lt;/math&gt;. 

We define:
Two points &lt;math&gt;P_1,P_2&lt;/math&gt; are '''(+)-parallel''' (&lt;math&gt;P_1\parallel_+ P_2&lt;/math&gt;) if &lt;math&gt;x_1=x_2&lt;/math&gt; and '''(−)-parallel''' (&lt;math&gt;P_1\parallel_- P_2&lt;/math&gt;) if &lt;math&gt;y_1=y_2&lt;/math&gt;. &lt;br /&gt;
Both these relations are [[equivalence relations]] on the set of points.

Two points &lt;math&gt;P_1,P_2&lt;/math&gt; are called '''parallel''' (&lt;math&gt;P_1\parallel P_2&lt;/math&gt;) if
&lt;math&gt;P_1\parallel_+ P_2&lt;/math&gt; or &lt;math&gt;P_1\parallel_- P_2&lt;/math&gt;.

From the definition above we find:

'''Lemma:'''
:*For any pair of non parallel points &lt;math&gt;A,B&lt;/math&gt; there is exactly one point &lt;math&gt;C&lt;/math&gt; with &lt;math&gt;A\parallel_+ C \parallel_- B&lt;/math&gt;.
:*For any point &lt;math&gt;P&lt;/math&gt; and any cycle &lt;math&gt;z&lt;/math&gt; there are exactly two points &lt;math&gt;A,B \in z&lt;/math&gt; with &lt;math&gt;A\parallel_+ P \parallel_- B&lt;/math&gt;.
:*For any three points &lt;math&gt;A&lt;/math&gt;, &lt;math&gt;B&lt;/math&gt;, &lt;math&gt;C&lt;/math&gt;, pairwise non parallel, there is exactly one cycle &lt;math&gt;z&lt;/math&gt; that contains &lt;math&gt;A,B,C&lt;/math&gt;.
:*For any cycle &lt;math&gt;z&lt;/math&gt;, any point &lt;math&gt;P\in z&lt;/math&gt; and any point &lt;math&gt;Q, P \not\parallel Q&lt;/math&gt; and &lt;math&gt;Q\notin z&lt;/math&gt; there exists exactly one cycle &lt;math&gt;z'&lt;/math&gt; such that &lt;math&gt;z\cap z'=\{P\}&lt;/math&gt;, i.e. &lt;math&gt;z&lt;/math&gt; '''touches''' &lt;math&gt;z'&lt;/math&gt; at point P.

Like the classical Möbius and Laguerre planes Minkowski planes can be described as the geometry of plane sections of a suitable quadric. But in this case the quadric lives in '''projective''' 3-space: The classical real Minkowski plane is isomorphic to the geometry of plane sections of a [[hyperboloid of one sheet]] (not degenerated quadric of index 2).

==The axioms of a Minkowski plane==

Let &lt;math&gt; \left( {\mathcal P} , {\mathcal Z} ; \parallel_+ , \parallel_- , \in \right) &lt;/math&gt; be an incidence structure with the set &lt;math&gt;\mathcal P&lt;/math&gt; of points, the set &lt;math&gt;\mathcal Z&lt;/math&gt; of cycles and two equivalence relations &lt;math&gt;\parallel_+&lt;/math&gt; ((+)-parallel) and &lt;math&gt;\parallel_-&lt;/math&gt; ((−)-parallel) on set &lt;math&gt;\mathcal P&lt;/math&gt;.  For &lt;math&gt;P\in \mathcal P&lt;/math&gt; we define:
&lt;math&gt; \overline{P}_+ := \left\{ \left. Q \in \mathcal P \ \right| \ Q\parallel_+ P \right\} &lt;/math&gt; and
&lt;math&gt; \overline{P}_- := \left\{ \left. Q \in \mathcal P \ \right| \ Q\parallel_- P \right\} &lt;/math&gt;.
An equivalence class &lt;math&gt;\overline{P}_+&lt;/math&gt; or &lt;math&gt;\overline{P}_-&lt;/math&gt; is called '''(+)-generator'''
and '''(−)-generator''', respectively. (For the space model of the classical Minkowski plane a generator is a line on the hyperboloid.)&lt;br /&gt;
Two points &lt;math&gt; A , B &lt;/math&gt; are called '''parallel''' (&lt;math&gt; A \parallel B &lt;/math&gt;) if &lt;math&gt; A \parallel_+ B &lt;/math&gt; or &lt;math&gt; A \parallel_- B&lt;/math&gt;.

An incidence structure &lt;math&gt; {\mathfrak M} := ( {\mathcal P} , {\mathcal Z} ; \parallel_+ , \parallel_- , \in ) &lt;/math&gt; is called '''Minkowski plane''' if the following axioms hold:&lt;br /&gt;
[[File:Minkowski-axioms-c1-c2.svg|thumb|Minkowski-axioms-c1-c2]]
[[File:Minkowski-axioms-c3-c4.svg|thumb|Minkowski-axioms-c3-c4]]
* '''C1''': For any pair of non parallel points &lt;math&gt;A,B&lt;/math&gt; there is exactly one point &lt;math&gt;C&lt;/math&gt; with &lt;math&gt;A\parallel_+ C \parallel_- B &lt;/math&gt;.
* '''C2''': For any point &lt;math&gt;P&lt;/math&gt; and any cycle &lt;math&gt;z&lt;/math&gt; there are exactly two points &lt;math&gt; A , B \in z&lt;/math&gt; with &lt;math&gt;A\parallel_+ P \parallel_- B &lt;/math&gt;.
* '''C3''': For any three points &lt;math&gt;A,B,C&lt;/math&gt;, pairwise non parallel, there is exactly one cycle &lt;math&gt;z&lt;/math&gt; which contains &lt;math&gt; A , B , C &lt;/math&gt;.
* '''C4''': For any cycle &lt;math&gt;z&lt;/math&gt;, any point &lt;math&gt;P\in z&lt;/math&gt; and any point &lt;math&gt; Q, P \not\parallel Q &lt;/math&gt; and &lt;math&gt;Q\notin z&lt;/math&gt; there exists exactly one cycle &lt;math&gt;z'&lt;/math&gt; such that &lt;math&gt; z \cap z' = \{ P \} &lt;/math&gt;, i.e. &lt;math&gt;z&lt;/math&gt; '''touches''' &lt;math&gt;z'&lt;/math&gt; at point &lt;math&gt; P &lt;/math&gt;.
* '''C5''': Any cycle contains at least 3 points. There is at least one cycle &lt;math&gt;z&lt;/math&gt; and a point &lt;math&gt;P&lt;/math&gt; not in &lt;math&gt;z&lt;/math&gt;.

For investigations the following statements on parallel classes (equivalent to C1, C2 respectively) are advantageous.
:'''C1′''': For any two points &lt;math&gt; A , B &lt;/math&gt; we have &lt;math&gt; \left| \overline{A}_+ \cap\overline{B}_- \right| = 1 &lt;/math&gt;.
:'''C2′''': For any point &lt;math&gt; P &lt;/math&gt; and any cycle &lt;math&gt; z &lt;/math&gt; we have: &lt;math&gt; \left| \overline{P}_+ \cap z \right | = 1 = \left| \overline{P}_- \cap z \right| &lt;/math&gt;.

First consequences of the axioms are

'''Lemma:''' For a Minkowski plane &lt;math&gt;{\mathfrak M}&lt;/math&gt; the following is true
:a) Any point is contained in at least one cycle.
:b) Any generator contains at least 3 points.
:c) Two points can be connected by a cycle if and only if they are non parallel.

Analogously to Möbius and Laguerre planes we get the connection to the linear
geometry via the residues.

For a Minkowski plane &lt;math&gt;{\mathfrak M}=({\mathcal P},{\mathcal Z};\parallel_+,\parallel_-,\in)&lt;/math&gt; and &lt;math&gt;P \in \mathcal P&lt;/math&gt; we define the local structure
: &lt;math&gt;\mathfrak A_P:= (\mathcal P\setminus\overline{P},\{z\setminus\{\overline{P}\} \ | \ P\in z\in\mathcal Z\}
\cup \{E\setminus \overline{P} \ | \ E\in {\mathcal E}\setminus\{\overline{P}_+,\overline{P}_-\}\}, \in)&lt;/math&gt;
and call it the '''residue at point P'''.

For the classical Minkowski plane &lt;math&gt;\mathfrak A_{(\infty,\infty)}&lt;/math&gt; is the real affine plane &lt;math&gt;\R^2&lt;/math&gt;.

An immediate consequence of axioms C1 to C4 and C1′, C2′ are the following two theorems.

'''Theorem''': For a Minkowski plane &lt;math&gt; {\mathfrak M} = ( {\mathcal P} , {\mathcal Z} ; \parallel_+ , \parallel , \in ) &lt;/math&gt; any residue is an affine plane.

'''Theorem''':
Let be &lt;math&gt;{\mathfrak M}=({\mathcal P},{\mathcal Z};\parallel_+,\parallel_-,\in)&lt;/math&gt; an incidence structure with two equivalence relations &lt;math&gt;\parallel_+&lt;/math&gt; and &lt;math&gt;\parallel_-&lt;/math&gt; on the set &lt;math&gt;\mathcal P&lt;/math&gt; of points (see above).
:&lt;math&gt;{\mathfrak M}&lt;/math&gt; is a Minkowski plane if and only if for any point &lt;math&gt;P&lt;/math&gt; the residue &lt;math&gt;\mathfrak A_P&lt;/math&gt; is an affine plane.

===Minimal model===
[[File:Minkowski-minimal-model.svg|300px|thumb|Minkowski plane: minimal model]]

The '''minimal model''' of a Minkowski plane can be established over the set
&lt;math&gt;\overline{K}:=\{0,1,\infty\}&lt;/math&gt; of three elements:

&lt;math&gt;\mathcal {P}:= {\overline{K}}^2 \qquad &lt;/math&gt;

&lt;math&gt;\mathcal Z:= \{ \{ (a_1,b_1),(a_2,b_2),(a_3,b_3) \} | &lt;/math&gt; 

&lt;math&gt; | \{a_1,a_2,a_3\} = \{b_1,b_2,b_3\}=\overline{K} \} =  &lt;/math&gt;

&lt;math&gt;\{ \{ (0,0),(1,1),(\infty,\infty) \} ,&lt;/math&gt; 
&lt;math&gt; \{ (0,0),(1,\infty),(\infty,1) \}, &lt;/math&gt; 
&lt;math&gt; \{ (0,1),(1,0),(\infty,\infty) \}, &lt;/math&gt; 
&lt;math&gt; \{ (0,1),(1,\infty),(\infty,0) \}, &lt;/math&gt; 
&lt;math&gt; \{ (0,\infty),(1,1),(\infty,0) \}, &lt;/math&gt; 
&lt;math&gt; \{ (0,\infty),(1,0),(\infty,1) \}\} &lt;/math&gt; 

Parallel points:

&lt;math&gt; (x_1,y_1) \parallel_+ (x_2,y_2) &lt;/math&gt; if and only if &lt;math&gt; x_1 = x_2 &lt;/math&gt; 

&lt;math&gt;(x_1,y_1)\parallel_- (x_2,y_2)&lt;/math&gt; if and only if &lt;math&gt; y_1 = y_2 &lt;/math&gt;.

Hence: &lt;math&gt; \left| \mathcal P \right| = 9 &lt;/math&gt;
and  &lt;math&gt; \left| \mathcal Z \right|  = 6 &lt;/math&gt;.

===Finite Minkowski-planes===
For finite Minkowski-planes we get from C1′, C2′:

'''Lemma''':
Let be &lt;math&gt;{\mathfrak M}=({\mathcal P},{\mathcal Z};\parallel_+,\parallel_-,\in)&lt;/math&gt; a finite Minkowski plane, i.e. &lt;math&gt; \left| \mathcal P \right| &lt; \infty &lt;/math&gt;. For any pair of cycles &lt;math&gt; z_1 , z_2 &lt;/math&gt; and any pair of generators &lt;math&gt; e_1 , e_2 &lt;/math&gt; we have:
&lt;math&gt; \left| z_1 \right| = \left| z_2 \right| = \left| e_1 \right| = \left| e_2 \right| &lt;/math&gt;.

This gives rise of the '''definition''':&lt;br /&gt;
For a finite Minkowski plane &lt;math&gt;{\mathfrak M}&lt;/math&gt; and a cycle &lt;math&gt;z&lt;/math&gt; of &lt;math&gt;{\mathfrak M}&lt;/math&gt; we call the integer &lt;math&gt; n = \left| z \right| - 1 &lt;/math&gt; the '''order''' of &lt;math&gt;{\mathfrak M}&lt;/math&gt;.

Simple combinatorial considerations yield

'''Lemma''':
For a finite Minkowski plane &lt;math&gt; {\mathfrak M} = ({\mathcal P} , {\mathcal Z} ; \parallel_+ , \parallel_- , \in ) &lt;/math&gt; the following is true:
: a) Any residue (affine plane) has order &lt;math&gt;n&lt;/math&gt;.
: b) &lt;math&gt; \left| \mathcal P \right| = ( n + 1 ) ^2 &lt;/math&gt;,  
: c) &lt;math&gt; \left| \mathcal Z \right| = ( n + 1 ) n ( n - 1 ) &lt;/math&gt;.

==Miquelian Minkowski planes==

We get the most important examples of Minkowski planes by generalizing the classical real model: Just replace &lt;math&gt;\R&lt;/math&gt; by an arbitrary [[Field (mathematics)|field]] &lt;math&gt;K&lt;/math&gt; then we get ''in any case'' a Minkowski plane &lt;math&gt;{\mathfrak M}(K)=({\mathcal P},{\mathcal Z};\parallel_+,\parallel_-,\in)&lt;/math&gt;.

Analogously to Möbius and Laguerre planes the Theorem of Miquel is a characteristic property of a Minkowski plane &lt;math&gt;\mathfrak M (K)&lt;/math&gt;.

[[File:Theorem-of-miquel.svg|300px|thumb|Theorem of Miquel]]
'''Theorem (Miquel):''' For the Minkowski plane &lt;math&gt;\mathfrak M (K)&lt;/math&gt; the following is true:
: If for any 8 pairwise not parallel points &lt;math&gt;P_1,...,P_8 &lt;/math&gt; which can be assigned to the vertices of a cube such that the points in 5 faces correspond to concyclical quadruples than the sixth quadruple of points is concyclical, too.

(For a better overview in the figure there are circles drawn instead of hyperbolas.)

'''Theorem (Chen):''' Only a Minkowski plane &lt;math&gt;\mathfrak M (K)&lt;/math&gt; satisfies the theorem of Miquel.

Because of the last theorem &lt;math&gt;\mathfrak M(K) &lt;/math&gt; is called a '''miquelian Minkowski plane'''.

'''Remark:''' The '''minimal model''' of a Minkowski plane is miquelian.
: It is isomorphic to the Minkowski plane &lt;math&gt;\mathfrak M(K) &lt;/math&gt; with &lt;math&gt; K = \operatorname{GF}(2)&lt;/math&gt; (field &lt;math&gt;\{0,1\}&lt;/math&gt;).

An astonishing result is

'''Theorem (Heise):'''  Any Minkowski plane of ''even'' order is miquelian.

'''Remark:''' A suitable [[stereographic projection]] shows: &lt;math&gt;\mathfrak M(K) &lt;/math&gt; is isomorphic
to the geometry of the plane sections on a hyperboloid of one sheet ([[quadric]] of index 2) in projective 3-space over field &lt;math&gt; K &lt;/math&gt;.

'''Remark:''' There are a lot of Minkowski planes that are '''not miquelian''' (s. weblink below). But there are no "ovoidal Minkowski" planes, in difference to Möbius and Laguerre planes. Because any [[quadratic set]] of index 2 in projective 3-space is a quadric (see [[quadratic set]]).

==See also==
* [[Conformal geometry]]

==References==
&lt;references/&gt;
* W. Benz, ''Vorlesungen über Geomerie der Algebren'', [[Springer Science+Business Media|Springer]] (1973)
* F. Buekenhout (ed.), ''Handbook of [[Incidence (geometry)|Incidence Geometry]]'', [[Elsevier]] (1995) {{isbn|0-444-88355-X}}

==External links==
* [http://eom.springer.de/b/b110290.htm Benz plane at SpringerLink]
*[http://www.mathematik.tu-darmstadt.de/~ehartmann/circlegeom.pdf Lecture Note '''''Planar Circle Geometries''''', an Introduction to Moebius-, Laguerre- and Minkowski Planes]

[[Category:Geometry]]</text>
      <sha1>s5fmtzcky3yetp2eqa2ysf3c0jokg86</sha1>
    </revision>
  </page>
  <page>
    <title>Mnemonics in trigonometry</title>
    <ns>0</ns>
    <id>32147219</id>
    <revision>
      <id>862395305</id>
      <parentid>862395295</parentid>
      <timestamp>2018-10-04T03:13:36Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor/>
      <comment>Reverting possible vandalism by [[Special:Contribs/Udan khatola|Udan khatola]] to version by Maczkopeti. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (3496476) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6933">In [[trigonometry]], it is common to use [[mnemonic]]s to help remember [[trigonometric identities]] and the relationships between the various [[trigonometric functions]]. 

== SOH-CAH-TOA ==
The ''sine'', ''cosine'', and ''tangent'' ratios in a right triangle can be remembered by representing them as strings of letters, for instance SOH-CAH-TOA in English:

:'''S'''ine = '''O'''pposite ÷ '''H'''ypotenuse
:'''C'''osine = '''A'''djacent ÷ '''H'''ypotenuse
:'''T'''angent = '''O'''pposite ÷ '''A'''djacent

One way to remember the letters is to sound them out phonetically (i.e. {{IPAc-en|ˌ|s|oʊ|k|ə|ˈ|t|oʊ|.|ə}} {{respell|SOH|kə|TOH|ə}}).&lt;ref&gt;{{MathWorld|title=SOHCAHTOA|urlname=SOHCAHTOA}}&lt;/ref&gt; 

Another method is to expand the letters into a sentence, such as "'''S'''ome '''O'''ld '''H'''ags '''C'''an't '''A'''lways '''H'''ide '''T'''heir '''O'''ld '''A'''ge", "'''S'''ome '''O'''ld '''H'''ippy '''C'''aught '''A'''nother '''H'''ippy '''T'''rippin' '''O'''n '''A'''cid", or sentences that might be more appropriate for younger people is "'''S'''ome '''O'''ld '''H'''orse '''C'''ame '''A'H'''opping '''T'''hrough '''O'''ur '''A'''lley" and "'''S'''itting '''O'''n '''H'''ard '''C'''oncrete '''A'''lways '''H'''urts '''T'''hose '''O'''uter '''A'''reas".&lt;ref&gt;{{cite book |title=Memory: A Very Short Introduction|first=Jonathan K.|last=Foster|publisher=Oxford|year=2008|isbn=0-19-280675-0|page=128}}&lt;/ref&gt;

Communities in Chinese circles may choose to remember it as TOA-CAH-SOH, which also means 'big-footed woman' ({{lang-zh|c=大腳嫂|poj=tōa-kha-só}}) in [[Hokkien]].

An alternate way to remember the letters for Sin, Cos, and Tan is to memorize the nonsense syllables Oh, Ah, Oh-Ah (i.e. {{IPAc-en|oʊ|_|ə|_|ˈ|oʊ|.|ə}}) for O/H, A/H, O/A. Or, to remember all six functions, Sin, Cos, Tan, Cot, Sec, and Csc, memorize the syllables O/H, A/H, Oh/Ah, Ah/Oh, H/A, H/O (i.e. {{IPAc-en|oʊ|_|ə|_|ˈ|oʊ|.|ə|_|ə|ˈ|oʊ|_|h|ə|_|ˈ|h|oʊ}}). 

==All Students Take Calculus==
[[File:trigonometric_function_quadrant_sign.svg|thumb|Signs of trigonometric functions in each quadrant.]]
'''All S'''tudents '''T'''ake '''C'''alculus is a [[mnemonic]] for the sign of each [[trigonometric functions]] in each [[Cartesian coordinate system|quadrant]] of the plane. The letters ASTC signify which of the trigonometric functions are positive, starting in the top right 1st quadrant and moving [[counterclockwise]] through quadrants 2 to 4.
* Quadrant I (angles from 0 to 90 degrees, or 0 to π/2 radians): '''All''' trigonometric functions are positive in this quadrant.
* Quadrant II (angles from 90 to 180 degrees, or π/2 to π radians): '''S'''ine and cosecant functions are positive in this quadrant.
* Quadrant III (angles from 180 to 270 degrees, or π to 3π/2 radians): '''T'''angent and cotangent functions are positive in this quadrant.
* Quadrant IV (angles from 270 to 360 degrees, or 3π/2 to 2π radians): '''C'''osine and secant functions are positive in this quadrant.

Other mnemonics include:
*'''All S'''tations '''T'''o '''C'''entral&lt;ref name=mathfun&gt;{{cite web |url=http://www.mathsisfun.com/algebra/trig-four-quadrants.html |title=Sine, Cosine and Tangent in Four Quadrants |accessdate=2015-01-18 |archiveurl=https://web.archive.org/web/20150118121241/http://www.mathsisfun.com/algebra/trig-four-quadrants.html |archivedate=2015-01-18 }}&lt;/ref&gt;
*'''All S'''illy '''T'''om '''C'''ats&lt;ref name=mathfun/&gt;
*'''A'''dd '''S'''ugar '''T'''o '''C'''offee&lt;ref name=mathfun/&gt;
*'''All''' '''S'''cience '''T'''eachers (are) '''C'''razy&lt;ref&gt;Heng, Cheng and Talbert, [https://books.google.com/books?id=ZZoxLiJBwOUC&amp;amp;pg=PA228 "Additional Mathematics"], page 228&lt;/ref&gt;

Other easy to remember mnemonics are the '''ACTS''' and '''CAST''' laws.  These have the disadvantages of not going sequentially from quadrants 1 to 4 and not reinforcing the numbering convention of the quadrants. 
* '''CAST''' still goes counterclockwise but starts in quadrant 4 going through quadrants 4, 1, 2, then 3.
* '''ACTS''' still starts in quadrant 1 but goes clockwise going through quadrants 1, 4, 3, then 2.

==Hexagon chart==
[[File:Trigonometric identity mnemonic.png|thumb|Trigonometric identities mnemonic]] 

Another mnemonic permits all of the basic identities to be read off quickly. Although the word part of the mnemonic used to build the chart does not hold in English{{clarify|date=February 2018}}, the chart itself is fairly easy to reconstruct with a little thought. Functions without "co" appear on the left, co-functions on the right, a 1 goes in the middle, triangles point down, and the entire drawing looks like a [[Fallout shelter|fallout shelter]] [[trefoil]].&lt;ref&gt;{{cite web|url=https://www.mathsisfun.com/algebra/trig-magic-hexagon.html|title=Magic Hexagon for Trig Identities|website=Math is Fun}}&lt;/ref&gt;

Starting at any corner of the hexagon:
* The starting corner equals one over the opposite corner.
* Going either clockwise or counter-clockwise, the starting corner equals the next corner divided by the corner after that.
* The starting corner equals the product of its two nearest neighbors.
* The sum of the squares of each item at the top of a triangle equals the square of the item at the bottom. These are the [[Pythagorean trigonometric identity|trigonometric Pythagorean identities]]:
::&lt;math&gt;\sin^2 A + \cos^2 A = 1 \ &lt;/math&gt;
::&lt;math&gt;1 + \cot^2 A = \csc^2 A \ &lt;/math&gt;
::&lt;math&gt;\tan^2 A + 1 = \sec^2 A \ &lt;/math&gt;

Aside from the last bullet, the specific values for each identity are summarized in this table:
{| class="wikitable"
|-
! Starting function !! ... equals one over the opposite !! ... equals the first over the second, going clockwise  !! ... equals the first over the second, going counter-clockwise  !! ... equals the product of two nearest neighbors
|-
| &lt;math&gt;\tan A&lt;/math&gt; || &lt;math&gt; = {1 / \cot A}&lt;/math&gt; || &lt;math&gt; = {\sin A / \cos A} &lt;/math&gt; || &lt;math&gt;= {\sec A / \csc A} &lt;/math&gt; || &lt;math&gt;= \sin A \cdot \sec A&lt;/math&gt;
|-
| &lt;math&gt;\sin A&lt;/math&gt; || &lt;math&gt; = {1 / \csc A}&lt;/math&gt; || &lt;math&gt; = {\cos A / \cot A} &lt;/math&gt; || &lt;math&gt;= {\tan A / \sec A} &lt;/math&gt; || &lt;math&gt;= \cos A \cdot \tan A&lt;/math&gt;
|-
| &lt;math&gt;\cos A&lt;/math&gt; || &lt;math&gt; = {1 / \sec A}&lt;/math&gt; || &lt;math&gt; = {\cot A / \csc A} &lt;/math&gt; || &lt;math&gt;= {\sin A / \tan A} &lt;/math&gt; || &lt;math&gt;= \sin A \cdot \cot A&lt;/math&gt;
|-
| &lt;math&gt;\cot A&lt;/math&gt; || &lt;math&gt; = {1 / \tan A}&lt;/math&gt; || &lt;math&gt; = {\csc A / \sec A} &lt;/math&gt; || &lt;math&gt;= {\cos A / \sin A} &lt;/math&gt; || &lt;math&gt;= \cos A \cdot \csc A&lt;/math&gt;
|-
| &lt;math&gt;\csc A&lt;/math&gt; || &lt;math&gt; = {1 / \sin A}&lt;/math&gt; || &lt;math&gt; = {\sec A / \tan A} &lt;/math&gt; || &lt;math&gt;= {\cot A / \cos A} &lt;/math&gt; || &lt;math&gt;= \cot A \cdot \sec A&lt;/math&gt;
|-
| &lt;math&gt;\sec A&lt;/math&gt; || &lt;math&gt; = {1 / \cos A}&lt;/math&gt; || &lt;math&gt; = {\tan A / \sin A} &lt;/math&gt; || &lt;math&gt;= {\csc A / \cot A} &lt;/math&gt; || &lt;math&gt;= \csc A \cdot \tan A&lt;/math&gt;
|}

==References==
{{reflist}}

{{Math mnemonics}}

[[Category:Trigonometry]]
[[Category:Science mnemonics]]</text>
      <sha1>4pl73x4jiaqth05qe63tcy9vh2q239z</sha1>
    </revision>
  </page>
  <page>
    <title>Partial differential algebraic equation</title>
    <ns>0</ns>
    <id>36713242</id>
    <revision>
      <id>594148477</id>
      <parentid>594109694</parentid>
      <timestamp>2014-02-06T03:46:05Z</timestamp>
      <contributor>
        <username>Discospinster</username>
        <id>82432</id>
      </contributor>
      <minor/>
      <comment>[[Help:Reverting|Reverted]] edits by [[Special:Contributions/Ultroloth|Ultroloth]] ([[User talk:Ultroloth|talk]]) to last version by BG19bot</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2984">In [[mathematics]] a '''partial differential algebraic equation (PDAE)''' set is an incomplete system of [[partial differential equation]]s that is closed with a set of [[algebraic equation]]s.

== Definition ==
A general PDAE is defined as:

: &lt;math&gt;0 = \mathbf F \left( \mathbf x, \mathbf y, \frac{\partial y_i}{\partial x_j}, \frac{\partial^2 y_i}{\partial x_j \partial x_k}, \ldots, \mathbf z \right),&lt;/math&gt;

where:

* '''F''' is a set of arbitrary functions;
* '''x''' is a set of independent variables;
* '''y''' is a set of dependent variables for which partial derivatives are defined; and
* '''z''' is a set of dependent variables for which no partial derivatives are defined.

The relationship between a PDAE and a [[partial differential equation]] (PDE) is analogous to the relationship between an [[ordinary differential equation]] (ODE) and a [[differential algebraic equation]] (DAE).

PDAEs of this general form are challenging to solve.  Simplified forms are studied in more detail in the literature.&lt;ref&gt;Wagner, Y.  2000.  "A further index concept for linear PDAEs of hyperbolic type," Mathematics and Computers in Simulation, v. 53, pp. 287–291.&lt;/ref&gt;&lt;ref&gt;W. S. Martinson, P. I. Barton. (2002) "Index and characteristic analysis of linear PDAE systems," Siam Journal on Scientific Computing, v. 24, n. 3, pp. 905–923.&lt;/ref&gt;&lt;ref&gt;Lucht, W.; Strehmel, K..  1998.  "Discretization based indices for semilinear partial differential algebraic equations," Applied Numerical Mathematics, v. 28, pp. 371–386.&lt;/ref&gt;  Even as recently as 2000, the term "PDAE" has been handled as unfamiliar by those in related fields.&lt;ref&gt;Simeon, B.; Arnold, M..  2000.  "Coupling DAEs and PDEs for simulating the interaction of pantograph and catenary," Mathematical and Computer Modelling of Dynamical Systems, v. 6, pp. 129–144.&lt;/ref&gt;

== Solution methods ==
[[Semi-discretization]] is a common method for solving PDAEs whose independent variables are those of [[time]] and [[space]], and has been used for decades.&lt;ref&gt;Jacob, J.; Le Lann, J; Pinguad, H.; Capdeville, B.. 1996. "A generalized approach for dynamic modelling 
and simulation of biofilters: application to waste-water denitrification," Chemical Engineering Journal, v. 
65, pp. 133–143.&lt;/ref&gt;&lt;ref&gt;de Dieuvleveult, C.; Erhel, J.; Kern, M..  2009.  "A global strategy for solving reactive transport equations," Journal of Computational Physics, v. 228, pp. 6395–6410.&lt;/ref&gt;  This method involves removing the spatial variables using a [[discretization]] method, such as the [[finite volume method]], and incorporating the resulting linear equations as part of the algebraic relations.  This reduces the system to a [[differential algebraic equation|DAE]], for which conventional solution methods can be employed.

== References ==
{{Reflist}}

[[Category:Partial differential equations]]
[[Category:Differential equations]]
[[Category:Multivariable calculus]]
[[Category:Numerical analysis]]


{{applied-math-stub}}</text>
      <sha1>icjhazhi4zoutypzvh0o7ykkxfcck9k</sha1>
    </revision>
  </page>
  <page>
    <title>Philippe Di Francesco</title>
    <ns>0</ns>
    <id>54037964</id>
    <revision>
      <id>866527296</id>
      <parentid>866527269</parentid>
      <timestamp>2018-10-30T21:52:43Z</timestamp>
      <contributor>
        <ip>76.10.0.245</ip>
      </contributor>
      <comment>Undid revision 865596729 by [[Special:Contributions/72.36.113.209|72.36.113.209]] ([[User talk:72.36.113.209|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="877">'''Philippe Di Francesco''' is a French-American mathematician, focusing in mathematical physics, physical combinatorics and integrable systems, currently the Morris and Gertrude Fine Distinguished Professor of Mathematics at [[University of Illinois]].&lt;ref&gt;{{Cite web |url=http://www.math.illinois.edu/~philippe/ |title=Philippe Di Francesco |publisher=illinois.edu |accessdate=May 13, 2017}}&lt;/ref&gt;&lt;ref&gt;{{Cite web |url=http://physics.illinois.edu/people/directory/profile/philippe |title=Philippe Di Francesco |publisher=illinois.edu |accessdate=May 13, 2017}}&lt;/ref&gt;

==References==
{{Reflist}}

{{Authority control}}

{{DEFAULTSORT:Di Francesco, Philippe}}
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]
[[Category:University of Illinois faculty]]
[[Category:American mathematicians]]
[[Category:French mathematicians]]


{{mathematician-stub}}</text>
      <sha1>no2m6isbpgwypx4gy24g651fmcv3z3u</sha1>
    </revision>
  </page>
  <page>
    <title>Pinhole camera model</title>
    <ns>0</ns>
    <id>12518245</id>
    <revision>
      <id>871297383</id>
      <parentid>871128499</parentid>
      <timestamp>2018-11-30T03:31:37Z</timestamp>
      <contributor>
        <username>Srleffler</username>
        <id>252195</id>
      </contributor>
      <comment>/* The geometry and mathematics of the pinhole camera */ "2D" is redundant here.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10128">{{broader|Epipolar geometry}}
{{More footnotes|date=February 2008}}
[[Image:Pinhole-camera.svg|thumb|A diagram of a [[pinhole camera]].]]
The '''pinhole camera model''' describes the mathematical relationship between the [[coordinate]]s of a point in [[three-dimensional space]] and its [[3D projection|projection]] onto the image plane of an ''ideal'' [[pinhole camera]], where the camera aperture is described as a point and no lenses are used to focus light.  The model does not include, for example, geometric distortions or blurring of unfocused objects caused by lenses and finite sized apertures.  It also does not take into account that most practical cameras have only discrete image coordinates.  This means that the pinhole camera model can only be used as a first order approximation of the mapping from a [[3D scene]] to a [[two-dimensional_space|2D]] [[image]].  Its validity depends on the quality of the camera and, in general, decreases from the center of the image to the edges as lens distortion effects increase.

Some of the effects that the pinhole camera model does not take into account can be compensated, for example by applying suitable coordinate transformations on the image coordinates; other effects are sufficiently small to be neglected if a high quality camera is used.  This means that the pinhole camera model often can be used as a reasonable description of how a camera depicts a 3D scene, for example in [[computer vision]] and [[computer graphics]].

== The geometry and mathematics of the pinhole camera ==

[[Image:Pinhole.svg|thumb|400px|right|The geometry of a pinhole camera]]
'' NOTE: The x&lt;sub&gt;1&lt;/sub&gt;x&lt;sub&gt;2&lt;/sub&gt;x&lt;sub&gt;3&lt;/sub&gt; coordinate system in the figure is left-handed, that is the direction of the OZ axis is in reverse to the system the reader may be used to.''

The [[geometry]] related to the mapping of a pinhole camera is illustrated in the figure. The figure contains the following basic objects:

* A 3D orthogonal coordinate system with its origin at '''O'''.  This is also where the [[Aperture|''camera aperture'']] is located.  The three axes of the coordinate system are referred to as X1, X2, X3.  Axis X3 is pointing in the viewing direction of the camera and is referred to as the ''[[optical axis]]'', ''principal axis'', or ''principal ray''.  The plane which is spanned by axes X1 and X2 is the front side of the camera, or ''principal plane''.
* An image plane, where the 3D world is projected through the aperture of the camera.  The image plane is parallel to axes X1 and X2 and is located at distance &lt;math&gt;f&lt;/math&gt; from the origin '''O''' in the negative direction of the X3 axis, where ''f'' is the [[focal length]] of the pinhole camera.  A practical implementation of a pinhole camera implies that the image plane is located such that it intersects the X3 axis at coordinate ''-f'' where ''f &gt; 0''.
* A point '''R''' at the intersection of the optical axis and the image plane.  This point is referred to as the ''principal point''{{cn|date=August 2018}} or ''image center''.
* A point '''P''' somewhere in the world at coordinate &lt;math&gt; (x_1, x_2, x_3) &lt;/math&gt; relative to the axes X1,X2,X3.
* The ''projection line'' of point '''P''' into the camera.  This is the green line which passes through point '''P''' and the point '''O'''.
* The projection of point '''P''' onto the image plane, denoted '''Q'''.  This point is given by the intersection of the projection line (green) and the image plane.  In any practical situation we can assume that &lt;math&gt;x_3&lt;/math&gt; &gt; 0 which means that the intersection point is well defined.
* There is also a 2D coordinate system in the image plane, with origin at '''R''' and with axes Y1 and Y2 which are parallel to X1 and X2, respectively.  The coordinates of point '''Q''' relative to this coordinate system is &lt;math&gt; (y_1, y_2) &lt;/math&gt;.

The ''pinhole'' aperture of the camera, through which all projection lines must pass, is assumed to be infinitely small, a point.  In the literature this point in 3D space is referred to as the ''optical (or lens or camera) center''.&lt;ref&gt;{{cite web|author=Andrea Fusiello |url=http://homepages.inf.ed.ac.uk/rbf/CVonline/LOCAL_COPIES/FUSIELLO4/tutorial.html#x1-30003 |title=Elements of Geometric Computer Vision |publisher=Homepages.inf.ed.ac.uk |date=2005-12-27 |accessdate=2013-12-18}}&lt;/ref&gt;

Next we want to understand how the coordinates &lt;math&gt; (y_1, y_2) &lt;/math&gt; of point '''Q''' depend on the coordinates &lt;math&gt; (x_1, x_2, x_3) &lt;/math&gt; of point '''P'''.  This can be done with the help of the following figure which shows the same scene as the previous figure but now from above, looking down in the negative direction of the X2 axis.

[[Image:Pinhole2.svg|thumb|400px|right|The geometry of a pinhole camera as seen from the X2 axis]]

In this figure we see two [[similar triangles]], both having parts of the projection line (green) as their [[hypotenuse]]s.  The [[Cathetus|catheti]] of the left triangle are &lt;math&gt; -y_1 &lt;/math&gt; and ''f'' and the catheti of the right triangle are &lt;math&gt; x_1 &lt;/math&gt; and &lt;math&gt; x_3 &lt;/math&gt;.  Since the two triangles are similar it follows that

: &lt;math&gt; \frac{-y_1}{f} = \frac{x_1}{x_3} &lt;/math&gt; or &lt;math&gt; y_1 = -\frac{f \, x_1}{x_3} &lt;/math&gt;

A similar investigation, looking in the negative direction of the X1 axis gives

: &lt;math&gt; \frac{-y_2}{f} = \frac{x_2}{x_3} &lt;/math&gt; or &lt;math&gt; y_2 = -\frac{f \, x_2}{x_3} &lt;/math&gt;

This can be summarized as

: &lt;math&gt; \begin{pmatrix} y_1 \\ y_2 \end{pmatrix} = -\frac{f}{x_3} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} &lt;/math&gt;

which is an expression that describes the relation between the 3D coordinates &lt;math&gt; (x_1,x_2,x_3) &lt;/math&gt; of point '''P''' and its image coordinates &lt;math&gt; (y_1,y_2) &lt;/math&gt; given by point '''Q''' in the image plane.

=== Rotated image and the virtual image plane ===

The mapping from 3D to 2D coordinates described by a pinhole camera is a [[perspective projection]] followed by a 180° rotation in the image plane.  This corresponds to how a real pinhole camera operates; the resulting image is rotated 180° and the relative size of projected objects depends on their distance to the focal point and the overall size of the image depends on the distance ''f'' between the image plane and the focal point.  In order to produce an unrotated image, which is what we expect from a camera, there are two possibilities:

* Rotate the coordinate system in the image plane 180° (in either direction).  This is the way any practical implementation of a pinhole camera would solve the problem; for a photographic camera we rotate the image before looking at it, and for a digital camera we read out the pixels in such an order that it becomes rotated.
* Place the image plane so that it intersects the X3 axis at ''f'' instead of at ''-f'' and rework the previous calculations.  This would generate a ''virtual (or front) image plane'' which cannot be implemented in practice, but provides a theoretical camera which may be simpler to analyse than the real one.

In both cases, the resulting mapping from 3D coordinates to 2D image coordinates is given by the expression above, but without the negation, thus

: &lt;math&gt; \begin{pmatrix} y_1 \\ y_2 \end{pmatrix} = \frac{f}{x_3} \begin{pmatrix} x_1 \\ x_2 \end{pmatrix} &lt;/math&gt;

== Homogeneous coordinates ==

{{main|Camera matrix}}

The mapping from 3D coordinates of points in space to 2D image coordinates can also be represented in [[homogeneous coordinates]].  Let &lt;math&gt; \mathbf{x} &lt;/math&gt; be a representation of a 3D point in [[homogeneous coordinates]] (a 4-dimensional vector), and let &lt;math&gt; \mathbf{y} &lt;/math&gt; be a representation of the image of this point in the pinhole camera (a 3-dimensional vector).  Then the following relation holds

: &lt;math&gt; \mathbf{y} \sim \mathbf{C} \, \mathbf{x} &lt;/math&gt;

where &lt;math&gt; \mathbf{C} &lt;/math&gt; is the &lt;math&gt; 3 \times 4 &lt;/math&gt; [[camera matrix]] and the &lt;math&gt;\, \sim &lt;/math&gt; means equality between elements of [[projective space]]s.  This implies that the left and right hand sides are equal up to a non-zero scalar multiplication.  A consequence of this relation is that also &lt;math&gt; \mathbf{C} &lt;/math&gt; can be seen as an element of a [[projective space]]; two camera matrices are equivalent if they are equal up to a scalar multiplication.  This description of the pinhole camera mapping, as a linear transformation &lt;math&gt; \mathbf{C} &lt;/math&gt; instead of as a fraction of two linear expressions, makes it possible to simplify many derivations of relations between 3D and 2D coordinates.{{Citation needed|date=September 2007}}

== See also ==

* [[Entrance pupil]], the equivalent location of the pinhole in relation to object space in a real camera.
* [[Exit pupil]], the equivalent location of the pinhole in relation to the image plane in a real camera.
* [[Pinhole camera]], the practical implementation of the mathematical model described in this article.

== References ==
{{Refimprove|date=January 2008}}
{{reflist}}

==Bibliography==
*{{cite book| author=David A. Forsyth and Jean Ponce| title=Computer Vision, A Modern Approach| publisher=Prentice Hall| year=2003| isbn=0-12-379777-2}}
*{{cite book| author=Richard Hartley and Andrew Zisserman| title=Multiple View Geometry in computer vision| publisher=Cambridge University Press| year=2003| isbn=0-521-54051-8| url = https://books.google.com/books?id=si3R3Pfa98QC&amp;pg=PA153&amp;dq=pinhole+intitle:%22Multiple+View+Geometry+in+computer+vision%22 }}
*{{cite book| author=Bernd Jähne| title=Practical Handbook on Image Processing for Scientific Applications| publisher=CRC Press| year=1997| isbn=0-8493-8906-2}}
*{{cite book| author=[[Linda Shapiro|Linda G. Shapiro]] and George C. Stockman| title=Computer Vision| publisher=Prentice Hall| year=2001| isbn=0-13-030796-3}}
*{{cite book| author=Gang Xu and Zhengyou Zhang| title=Epipolar geometry in Stereo, Motion and Object Recognition| publisher=Kluwer Academic Publishers| year=1996| isbn=0-7923-4199-6| url = https://books.google.com/books?id=DnFaUidM-B0C&amp;pg=PA7&amp;dq=pinhole+intitle:%22Epipolar+geometry%22}}

[[Category:Geometry in computer vision]]
[[Category:Cameras]]</text>
      <sha1>mui4n4g5tm2kxzvji60xfv9jsoh461f</sha1>
    </revision>
  </page>
  <page>
    <title>Point at infinity</title>
    <ns>0</ns>
    <id>403139</id>
    <revision>
      <id>869969385</id>
      <parentid>869967779</parentid>
      <timestamp>2018-11-21T15:15:15Z</timestamp>
      <contributor>
        <ip>141.101.203.69</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4509">{{Refimprove|date=July 2017}}[[Image:Real projective line.svg|right|thumb|150px|The real line with the point at infinity; it is called the [[real projective line]].]]
In [[geometry]], a '''point at infinity''' or '''ideal point''' is an idealized limiting point at the "end" of each line.

In the case of an [[affine plane]] (including the [[Euclidean plane]]), there is one ideal point for each [[Pencil (mathematics)|pencil]] of parallel lines of the plane. Adjoining these points produces a [[projective plane]], in which no point can be distinguished, if we "forget" which points were added. This holds for a geometry over any [[Field (mathematics)|field]], and more generally over any [[division ring]].&lt;ref&gt;{{cite web|last1=Weisstein|first1=Eric W.|title=Point at Infinity|url=http://mathworld.wolfram.com/PointatInfinity.html|website=mathworld.wolfram.com|publisher=Wolfram Research|accessdate=28 December 2016|language=en}}&lt;/ref&gt;

In the real case, a point at infinity completes a line into a topologically closed curve.  In higher dimensions, all the points at infinity form a projective subspace of one dimension less than that of the whole projective space to which they belong.  A point at infinity can also be added to the [[complex line]] (which may be thought of as the complex plane), thereby turning it into a closed surface known as the complex projective line, '''C'''P&lt;sup&gt;1&lt;/sup&gt;, also called the [[Riemann sphere]] (when complex numbers are mapped to each point).

In the case of a [[hyperbolic space]], each line has two distinct [[ideal point]]s.  Here, the set of ideal points takes the form of a [[Quadric (projective geometry)|quadric]].

== Affine geometry ==
In an [[affine space|affine]] or [[Euclidean space]] of higher dimension, the '''points at infinity''' are the points which are added to the space to get the [[projective space|projective completion]]. The set of the points at infinity is called, depending on the dimension of the space, the [[line at infinity]], the [[plane at infinity]] or the [[hyperplane at infinity]], in all cases a projective space of one less dimension.

As a projective space over a field is a [[smooth algebraic variety]], the same is true for the set of points at infinity. Similarly, if the ground field is the real or the complex field, the set of points at infinity is a [[manifold]].

=== Perspective ===
{{main|Perspective (graphical)}}
In artistic drawing and technical perspective, the projection on the picture plane of the point at infinity of a class of parallel lines is called their [[vanishing point]].

== Hyperbolic geometry ==
{{main|Ideal point}}
In [[hyperbolic geometry]], '''points at infinity''' are typically named [[ideal point]]s.  Unlike [[Euclidean geometry|Euclidean]] and [[Elliptic geometry|elliptic]] geometries, each line has two points at infinity: given a line ''l'' and a point ''P'' not on ''l'', the right- and left-[[limiting parallel]]s [[Convergence (mathematics)|converge]] [[asymptotically]] to different points at infinity.

All points at infinity together form the [[Cayley absolute]] or boundary of a [[hyperbolic plane]].

== Other generalisations ==
{{main|Compactification (mathematics)}}
This construction can be generalized to [[topological space]]s. Different compactifications may exist for a given space, but arbitrary topological space admits [[Alexandroff extension]], also called the ''one-point [[compactification (mathematics)|compactification]]'' when the original space is not itself [[compact space|compact]].  Projective line (over arbitrary field) is the Alexandroff extension&lt;!-- such way is correct.  finite fields do not have "compactifications" but Alexandroff extensions. --&gt; of the corresponding field. Thus the circle is the one-point compactification of the [[real line]], and the sphere is the one-point compactification of the plane. [[Projective space]]s '''P'''&lt;sup&gt;{{mvar|n}}&lt;/sup&gt; for {{mvar|n}}&amp;nbsp;&gt;&amp;nbsp;1 are not ''one-point'' compactifications of corresponding affine spaces for the reason mentioned above under {{section link||Affine geometry}}, and completions of hyperbolic spaces with ideal points are also not one-point compactifications.

== See also == 
*[[Division by zero]]
*{{multi-section link|Midpoint|Generalizations}}
*{{multi-section link|Asymptote|Algebraic curves}}

== References ==
{{reflist}}

[[Category:Projective geometry]]
[[Category:Hyperbolic geometry]]
[[Category:Infinity]]

[[it:Glossario di geometria descrittiva#Punto improprio]]</text>
      <sha1>ct58rpuwlwqouohdtbs9sugcyohua1l</sha1>
    </revision>
  </page>
  <page>
    <title>Primecoin</title>
    <ns>0</ns>
    <id>40102361</id>
    <revision>
      <id>870518996</id>
      <parentid>870494839</parentid>
      <timestamp>2018-11-25T10:00:09Z</timestamp>
      <contributor>
        <username>David Gerard</username>
        <id>36389</id>
      </contributor>
      <comment>rm primary sources, bitcoin blogs</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6604">{{infobox currency
| image_1 = Primecoin Logo.png
| image_title_1 = Primecoin logo
| iso_code = 
| image_width_1 = 200
| issuing_authority = None, the primecoin [[peer-to-peer]] network regulates and distributes through consensus in [[Network protocol|protocol]].{{citation needed|date=November 2018}}
| date_of_introduction = 7 July 2013&lt;ref name=clark2013/&gt;
| date_of_introduction_source = 
| using_countries = International
| inflation_rate = Limited release, production rate before this limit re-evaluated with the production of every block (at a rate of approximately 1 block per minute) based on the difficulty with which primecoins are produced.{{citation needed|date=November 2018}}
| inflation_source_date = 
| inflation_method = 
| symbol = Ψ
| nickname = XPM
| subunit_ratio_1 = 0.001
| iso_number = 
| subunit_name_1 = mXPM (millicoin)
| subunit_ratio_2 = 0.000001
| subunit_name_2 = μXPM (microcoin)
| subunit_ratio_3 = 0.00000001
| subunit_name_3 = Smallest unit
| plural = Primecoin, primecoins
| plural_subunit_1 = 
| plural_subunit_2 = 
| frequently_used_coins = 
| printer = 
| mint = 
| Social Media/Community = 
}}
'''Primecoin''' ([[currency sign|sign]]: '''Ψ'''; code: '''XPM''') is a [[cryptocurrency]] that implements a [[proof-of-work system]] that searches for chains of [[prime number]]s.

Launched on July 7, 2013 by anonymous hacker and [[peercoin]] founder Sunny King, Primecoin was the first cryptocurrency to have a proof-of-work system with a practical use. Earlier cryptocurrencies, such as [[Bitcoin]], were [[Cryptocurrency#Mining|mined]] using algorithms that solved arbitrary mathematical problems, the results of which had no value or use outside of mining the cryptocurrency itself. Primecoin's algorithm, however, computed chains of prime numbers ([[Cunningham chain|Cunningham]] and [[Bi-twin chain|bi-twin chains]]), the results of which were published on its [[blockchain]]'s public [[ledger]], available for use by scientists, mathematicians, and anyone else. Use of a proof-of-work system to calculate chains of prime numbers was an innovation that produced useful results while also meeting the criteria for a proof-of-work system: it involved a calculation that was difficult to perform but easy to verify, and the difficulty was adjustable.&lt;ref name="clark2013" /&gt;&lt;ref name=":0"&gt;{{Cite book|url=https://books.google.com/books?id=erMQBQAAQBAJ&amp;pg=PA175#v=onepage&amp;q&amp;f=false|title=Understanding Bitcoin: Cryptography, Engineering and Economics|last=Franco|first=Pedro|date=2018-10-21|publisher=[[John Wiley &amp; Sons]]|isbn=9781119019145|location=Chichester, West Sussex, UK|pages=175–76|language=en|archive-url=https://web.archive.org/web/20181117190848/https://books.google.com/books?id=erMQBQAAQBAJ&amp;pg=PA175#v=onepage&amp;q&amp;f=false|archive-date=2018-11-17|dead-url=no}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=https://www.thefreelibrary.com/Research+issues+regarding+the+Bitcoin+and+Alternative+Coins+digital...-a0484156869|title=Research issues regarding the Bitcoin and Alternative Coins digital currencies.|last=Pirjan|first=Alexandru|last2=Petrosanu|first2=Dana-Mihaela|year=2015|website=www.thefreelibrary.com|publisher=[[Romanian-American University]] via [[Gale (publisher)]]|archive-url=https://web.archive.org/web/20181117191126/https://www.thefreelibrary.com/Research+issues+regarding+the+Bitcoin+and+Alternative+Coins+digital...-a0484156869|archive-date=2018-11-17|dead-url=no|access-date=2018-11-17|last3=Huth|first3=Mihnea|last4=Negoita|first4=Mihaela}}&lt;/ref&gt;&lt;ref&gt;{{Cite news|url=https://www.scientificamerican.com/article/bitcoin-vies-with-new-cryptocurrencies-as-coin-of-the-cyber-realm/|title=Bitcoin Vies with New Cryptocurrencies as Coin of the Cyber Realm|last=Peck|first=Morgen E.|date=2014-04-29|work=[[Scientific American]]|access-date=2018-11-17|language=en}}&lt;/ref&gt;&lt;ref name=":1"&gt;{{Cite news|url=http://www.theguardian.com/technology/2013/nov/28/bitcoin-alternatives-future-currency-investments|title=Nine Bitcoin alternatives for future currency investments|last=Gibbs|first=Samuel|date=2013-11-28|newspaper=[[The Guardian]]|language=en|archive-url=|archive-date=|dead-url=|access-date=2018-11-17}}&lt;/ref&gt;

Shortly after its launch, some trade journals reported that the rush of over 18,000 new users seeking to mine Primecoin overwhelmed providers of dedicated servers.&lt;ref name="clark2013"&gt;{{cite news|url=https://www.theregister.co.uk/2013/07/16/digitalocean_primecoin_cloud_problems/|title=Virtual currency speculators shut down cloud|last=Clark|first=Jack|date=2013-07-16|work=[[The Register]]|accessdate=2018-11-17}}&lt;/ref&gt;&lt;ref&gt;{{cite news|last=Miller|first=Rich|date=2013-12-17|title=Currency Miners Cause Spot Shortages of Dedicated Servers|work=Data Center Knowledge|url=http://www.datacenterknowledge.com/archives/2013/12/17/currency-miners-cause-spot-shortages-dedicated-servers/|accessdate=2013-12-18}}&lt;/ref&gt;

Unlike Bitcoin, Primecoin targets a block generation period of one minute rather than every ten minutes, changes difficulty every block rather than every 2016 blocks, and has a block reward that is a function of the difficulty (blockreward = 999/difficulty) rather than fixed. Primecoin transactions are confirmed approximately 8–10 times as fast as Bitcoin transactions.&lt;ref name=":0" /&gt;&lt;ref name=":1" /&gt;

==References==
{{reflist}}

== Further reading ==
&lt;UL&gt;
&lt;li&gt;{{cite book|last1=Orrell|first1=David|authorlink1=David Orrell|last2=Chlupatý|first2=Roman|date=2016|title=The Evolution of Money|url=https://books.google.com/books?id=CXu0CwAAQBAJ&amp;pg=PA208|location=New York|publisher=[[Columbia University Press]]|pages=200–201|isbn=978-0-231-17372-8|accessdate=2018-11-13}}&lt;/li&gt;
&lt;li&gt;{{cite book |last=Antonopoulos |first=Andreas M. |date=2014 |title=Mastering Bitcoin: Unlocking Digital Cryptocurrencies |url=https://books.google.com/books?id=k3qrBQAAQBAJ |location=Sebastopol, California |publisher=[[O'Reilly Media]] |isbn=978-1-4919-2198-2 |accessdate=2018-11-13 }}&lt;/li&gt;
&lt;li&gt;{{cite journal|last1=Fanning|first1=Kurt|last2=Centers|first2=David P.|date=2016-06-13|title=Blockchain and Its Coming Impact on Financial Services|journal=Journal of Corporate Accounting &amp; Finance|volume=27|issue=5|pages=54–55|doi=10.1002/jcaf.22179}}&lt;/li&gt;&lt;/ul&gt;

==External links==
* {{official website|http://primecoin.io}}

{{Cryptocurrency-stub}}

{{Cryptocurrencies|state=expanded}}
{{Portal bar|Anarchism|Cryptography|Economics|Free software|Internet|Number theory|Numismatics}}

[[Category:Alternative currencies]]
[[Category:Cryptocurrencies]]
[[Category:E-commerce]]
[[Category:Number theory]]
[[Category:Currency introduced in 2013]]</text>
      <sha1>qgq8x7saqgmzm4swe1wz5p7jpah83aa</sha1>
    </revision>
  </page>
  <page>
    <title>Radius of convergence</title>
    <ns>0</ns>
    <id>61476</id>
    <revision>
      <id>863036985</id>
      <parentid>861788520</parentid>
      <timestamp>2018-10-08T09:15:17Z</timestamp>
      <contributor>
        <username>WikicyclopediaUser</username>
        <id>34318618</id>
      </contributor>
      <comment>/* Definition */ include reference</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15838">In [[mathematics]], the '''radius of convergence''' of a [[power series]] is the radius of the largest [[disk (mathematics)|disk]] in which the [[power series|series]] [[Convergent series|converge]]s.  It is either a non-negative real number or &lt;math&gt;\infty&lt;/math&gt;.  When it is positive, the power series [[absolute convergence|converges absolutely]] and [[compact convergence|uniformly on compact sets]] inside the open disk of radius equal to the radius of convergence, and it is the [[Taylor series]] of the [[analytic function]] to which it converges.

==Definition==
For a power series ''ƒ'' defined as:

:&lt;math&gt;f(z) =  \sum_{n=0}^\infty c_n (z-a)^n, &lt;/math&gt;

where

:''a'' is a  [[complex number|complex]] constant, the center of the [[disk (mathematics)|disk]] of convergence,
:''c''&lt;sub&gt;''n''&lt;/sub&gt; is the ''n''&lt;sup&gt;th&lt;/sup&gt; complex coefficient, and
:''z'' is a complex variable.

The radius of convergence ''r'' is a nonnegative real number or &lt;math&gt;\infty&lt;/math&gt; such that the series converges if

:&lt;math&gt;|z-a| &lt; r\,&lt;/math&gt;

and diverges if

:&lt;math&gt;|z-a| &gt; r.\,&lt;/math&gt;

Some may prefer an alternative definition, as existence is obvious:

: &lt;math&gt;r=\sup \left\{ |z-a|\ \left|\ \sum_{n=0}^\infty c_n(z-a)^n\ \text{ converges } \right.\right\} &lt;/math&gt;

On the boundary, that is, where |''z''&amp;nbsp;&amp;minus;&amp;nbsp;''a''| = ''r'', the behavior of the power series may be complicated, and the series may converge for some values of ''z'' and diverge for others. The radius of convergence is infinite if the series converges for all [[complex number]]s ''z''.&lt;ref&gt;{{Cite book|url=https://books.google.com.ph/books?redir_esc=y&amp;id=nw9eFnCSDNoC&amp;q=radius+of+convergence#v=snippet&amp;q=radius%20of%20convergence&amp;f=false|title=Mathematical Analysis-II|publisher=Krishna Prakashan Media|language=en}}&lt;/ref&gt;

==Finding the radius of convergence==

Two cases arise.  The first case is theoretical: when you know all the coefficients &lt;math&gt;c_n&lt;/math&gt; then you take certain limits and find the precise radius of convergence.  The second case is practical: when you construct a power series solution of a difficult problem you typically will only know a finite number of terms in a power series, anywhere from a couple of terms to a hundred terms.  In this second case, extrapolating a plot estimates the radius of convergence.

===Theoretical radius===

The radius of convergence can be found by applying the [[root test]] to the terms of the series. The root test uses the number

:&lt;math&gt;C = \limsup_{n\rightarrow\infty}\sqrt[n]{|c_n(z-a)^n|} = \limsup_{n\rightarrow\infty}\sqrt[n]{|c_n|}|z-a|&lt;/math&gt;

"lim&amp;nbsp;sup" denotes the [[limit superior]].  The root test states that the series converges if ''C''&amp;nbsp;&lt;&amp;nbsp;1 and diverges if&amp;nbsp;''C''&amp;nbsp;&gt;&amp;nbsp;1.  It follows that the power series converges if the distance from ''z'' to the center ''a'' is less than

:&lt;math&gt;r = \frac{1}{\limsup_{n\rightarrow\infty}\sqrt[n]{|c_n|}}&lt;/math&gt;

and diverges if the distance exceeds that number; this statement is the [[Cauchy–Hadamard theorem]].  Note that ''r''&amp;nbsp;=&amp;nbsp;1/0 is interpreted as an infinite radius, meaning that ''ƒ'' is an [[entire function]].

The limit involved in the [[ratio test]] is usually easier to compute, and when that limit exists, it shows that the radius of convergence is finite.

&lt;!-- NOTE: The ratio test as usually stated involves c_{n+1}/c_n, but THIS statement correctly uses c_{n+1}/c_n.  --&gt;
:&lt;math&gt;r = \lim_{n\rightarrow\infty} \left| \frac{c_{n}}{c_{n+1}} \right|.&lt;/math&gt;
&lt;!-- NOTE: The ratio test as usually stated involves c_{n+1}/c_n, but THIS statement correctly uses c_n/c_{n+1}. --&gt;

This is shown as follows.  The ratio test says the series converges if

: &lt;math&gt; \lim_{n\to\infty} \frac{|c_{n+1}(z-a)^{n+1}|}{|c_n(z-a)^n|} &lt; 1. &lt;/math&gt;

That is equivalent to

: &lt;math&gt; |z - a| &lt; \frac{1}{\lim_{n\to\infty} \frac{|c_{n+1}|}{|c_n|}} = \lim_{n\to\infty} \left|\frac{c_n}{c_{n+1}}\right|. &lt;/math&gt;

==={{anchor|Domb–Sykes plot|Domb–Sykes plot}} Practical estimation of radius in the case of real coefficients === &lt;!-- [[Domb–Sykes plot]] redirects here (and so in [[MOS:BOLD|boldface]] --&gt;
[[File:Domb Sykes plot Hinch.svg|thumb|right|400px|Domb–Sykes plot of the function &lt;math&gt;f(\varepsilon)=\frac{\varepsilon(1+\varepsilon^3)}{\sqrt{1+2\varepsilon}}.&lt;/math&gt;&lt;ref&gt;See Figure 8.1 in: {{citation| first=E.J. |last=Hinch |year=1991 |title=Perturbation Methods |series=Cambridge Texts in Applied Mathematics |volume=6 |publisher=Cambridge University Press |isbn=0-521-37897-4 |page=146}}&lt;/ref&gt; On the left (a) is a straightforward plot of the ratio of the power-series coefficients &lt;math&gt;c_{n-1} / c_n&lt;/math&gt; as a function of index &lt;math&gt;n&lt;/math&gt;; on the right, (b) is the Domb–Sykes plot of &lt;math&gt;c_n/c_{n-1}&lt;/math&gt; as a function of &lt;math&gt;1/n&lt;/math&gt;. The solid green line is the [[straight line|straight-line]] [[asymptote]] in the Domb–Sykes plot, which intercepts the vertical axis at −2 and has a slope +1. Thus there is a singularity at &lt;math&gt;\varepsilon=-1/2&lt;/math&gt; and so the radius of convergence is &lt;math&gt;r=1/2.&lt;/math&gt;]]
Usually, in scientific applications, only a finite number of coefficients &lt;math&gt;c_n&lt;/math&gt; are known.  Typically,{{vague|date=April 2014}} as &lt;math&gt;n&lt;/math&gt; increases, these coefficients settle into a regular behavior determined by the nearest radius-limiting singularity.  In this case, two main techniques have been developed, based on the fact that the coefficients of a Taylor series are roughly exponential with ratio &lt;math&gt;1/r&lt;/math&gt; where r is the radius of convergence.

* The basic case is when the coefficients ultimately share a common sign or alternate in sign. As pointed out earlier in the article, in many cases the limit &lt;math&gt;\lim_{n\to \infty}{{c_n}/{c_{n-1}}}&lt;/math&gt; exists, and in this case &lt;math&gt;{1}/{r} = \lim_{n \to \infty}{{c_n}/{c_{n-1}}}.&lt;/math&gt;  Negative &lt;math&gt;r&lt;/math&gt; means the convergence-limiting singularity is on the negative axis. Estimate this limit, by plotting the  &lt;math&gt;c_n/c_{n-1}&lt;/math&gt; versus &lt;math&gt;1/n&lt;/math&gt;, and graphically extrapolate to &lt;math&gt;1/n=0&lt;/math&gt; (effectively &lt;math&gt;n=\infty&lt;/math&gt;) via a linear fit.  The intercept with &lt;math&gt;1/n=0&lt;/math&gt; estimates the reciprocal of the radius of convergence, &lt;math&gt;1/r&lt;/math&gt;.  This plot  is called a '''Domb–Sykes plot'''.
* The more complicated case is when the signs of the coefficients have a more complex pattern. Mercer and Roberts proposed the following procedure.&lt;ref&gt;{{citation |first1=G.N. |last1=Mercer |first2=A.J. |last2=Roberts |title=A centre manifold description of contaminant dispersion in channels with varying flow properties |journal=SIAM J. Appl. Math. |volume=50 |pages=1547–1565 |year=1990 |doi=10.1137/0150091 |issue=6}}&lt;/ref&gt;  Define the associated sequence

:: &lt;math&gt;b_n^2=\frac{c_{n+1}c_{n-1}-c_n^2}{c_nc_{n-2}-c_{n-1}^2} \quad n=3,4,5,\ldots.&lt;/math&gt;

: Plot the finitely many known &lt;math&gt;b_n&lt;/math&gt; versus &lt;math&gt;1/n&lt;/math&gt;, and graphically extrapolate to &lt;math&gt;1/n=0&lt;/math&gt; via a linear fit.
The intercept with &lt;math&gt;1/n=0&lt;/math&gt; estimates the reciprocal of the radius of convergence, &lt;math&gt;1/r&lt;/math&gt;.

: This procedure also estimates two other characteristics of the convergence limiting singularity.  Suppose the nearest singularity is of degree &lt;math&gt;p&lt;/math&gt; and has angle &lt;math&gt;\pm\theta&lt;/math&gt; to the real axis.  Then the slope of the linear fit given above is &lt;math&gt;-(p+1)/r&lt;/math&gt;.  Further, plot &lt;math&gt;\frac12\left(\frac{c_{n-1}b_n}{c_n} + \frac{c_{n+1}}{c_nb_n}\right)&lt;/math&gt; versus &lt;math&gt;1/n^2&lt;/math&gt;, then a linear fit extrapolated to &lt;math&gt;1/n^2=0&lt;/math&gt; has intercept at &lt;math&gt;\cos\theta&lt;/math&gt;.

== Radius of convergence in complex analysis ==
A power series with a positive radius of convergence can be made into a [[holomorphic function]] by taking its argument to be a complex variable. The radius of convergence can be characterized by the following theorem:

: The radius of convergence of a power series ''ƒ'' centered on a point ''a'' is equal to the distance from ''a'' to the nearest point where ''ƒ'' cannot be defined in a way that makes it holomorphic.

The set of all points whose distance to ''a'' is strictly less than the radius of convergence is called the ''disk of convergence''.

[[File:TaylorComplexConv.png|thumb|300px|A graph of the functions explained in the text: Approximations in blue, circle of convergence in white]]

''The nearest point'' means the nearest point in the [[complex plane]], not necessarily on the real line, even if the center and all coefficients are real. For example, the function

: &lt;math&gt;f(z)=\frac 1 {1+z^2}&lt;/math&gt;

has no singularities on the real line, since &lt;math&gt;1+z^2&lt;/math&gt; has no real roots. Its Taylor series about 0 is given by

:&lt;math&gt;\sum_{n=0}^\infty (-1)^n z^{2n}.&lt;/math&gt;

The root test shows that its radius of convergence is 1. In accordance with this, the function ''&amp;fnof;''(''z'') has singularities at&amp;nbsp;±''i'', which are at a distance 1 from&amp;nbsp;0.

For a proof of this theorem, see [[analyticity of holomorphic functions]].

===A simple example===
The arctangent function of [[trigonometry]] can be expanded in a power series familiar to calculus students:

:&lt;math&gt;\arctan(z)=z-\frac{z^3} 3 + \frac{z^5} 5 -\frac{z^7} 7 +\cdots .&lt;/math&gt;

It is easy to apply the root test in this case to find that the radius of convergence is 1.

===A more complicated example===

Consider this power series:

:&lt;math&gt;\frac z {e^z-1}=\sum_{n=0}^\infty \frac{B_n}{n!} z^n &lt;/math&gt;

where the rational numbers ''B''&lt;sub&gt;''n''&lt;/sub&gt; are the [[Bernoulli numbers]].  It may be cumbersome to try to apply the ratio test to find the radius of convergence of this series.  But the theorem of complex analysis stated above  quickly solves the problem.  At ''z'' = 0, there is in effect no singularity since [[removable singularity|the singularity is removable]].  The only non-removable singularities are therefore located at the ''other'' points where the denominator is zero.  We solve

:&lt;math&gt;e^z-1=0\,&lt;/math&gt;

by recalling that if ''z'' = ''x''&amp;nbsp;+&amp;nbsp;''iy'' and ''e''&lt;sup&gt;&amp;nbsp;''iy''&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;cos(''y'')&amp;nbsp;+&amp;nbsp;''i''&amp;nbsp;sin(''y'') then

:&lt;math&gt;e^z = e^x e^{iy} = e^x(\cos(y)+i\sin(y)),\,&lt;/math&gt;

and then take ''x'' and ''y'' to be real.  Since ''y'' is real, the absolute value of cos(''y'')&amp;nbsp;+&amp;nbsp;''i''&amp;nbsp;sin(''y'') is necessarily 1.  Therefore, the absolute value of ''e''&lt;sup&gt;&amp;nbsp;''z''&lt;/sup&gt; can be 1 only if ''e''&lt;sup&gt;&amp;nbsp;''x''&lt;/sup&gt; is 1; since ''x'' is real, that happens only if ''x'' = 0.  Therefore ''z'' is pure imaginary and cos(''y'')&amp;nbsp;+&amp;nbsp;''i''&amp;nbsp;sin(''y'') = 1.  Since ''y'' is real, that happens only if cos(''y'') = 1 and sin(''y'') = 0, so that ''y'' is an integer multiple of&amp;nbsp;2{{pi}}.  Consequently the singular points of this function occur at

: ''z'' = a nonzero integer multiple of&amp;nbsp;2{{pi}}''i''.

The singularities nearest 0, which is the center of the power series expansion, are at ±2{{pi}}''i''.  The distance from the center to either of those points is 2{{pi}}, so the radius of convergence is&amp;nbsp;2{{pi}}.

== Convergence on the boundary ==
If the power series is expanded around the point ''a'' and the radius of convergence is {{math|''r''}}, then the set of all points {{math|''z''}} such that {{math|{{mabs|''z'' − ''a''}} {{=}} ''r''}} is a [[circle]] called the ''boundary'' of the disk of convergence. A power series may diverge at every point on the boundary, or diverge on some points and converge at other points, or converge at all the points on the boundary. Furthermore, even if the series converges everywhere on the boundary (even uniformly), it does not necessarily converge absolutely.

Example 1: The power series for the function {{math|''ƒ''(''z'') {{=}} 1/(1 − ''z'')}}, expanded around {{math|''z'' {{=}} 0}}, which is simply
:&lt;math&gt; \sum_{n=0}^\infty z^n,&lt;/math&gt; 
has radius of convergence 1 and diverges at every point on the boundary.

Example 2: The power series for {{math|''g''(''z'') {{=}} −ln(1 − ''z'')}}, expanded around {{math|''z'' {{=}} 0}}, which is
:&lt;math&gt; \sum_{n=1}^\infty \frac{1}{n} z^n,&lt;/math&gt; 
has radius of convergence 1, and diverges for {{math|''z'' {{=}} 1}}  but converges for all other points on the boundary. The function {{math|''ƒ''(''z'')}} of Example 1 is the [[derivative]] of {{math|''g''(''z'')}}.

Example 3: The power series
:&lt;math&gt; \sum_{n=1}^\infty \frac 1 {n^2} z^n &lt;/math&gt; 
has radius of convergence 1 and converges everywhere on the boundary absolutely. If {{math|''h''}} is the function represented by this series on the unit disk, then the derivative  of ''h''(''z'') is equal to ''g''(''z'')/''z'' with ''g'' of Example 2. It turns out that {{math|''h''(''z'')}} is the [[dilogarithm]] function.

Example 4: The power series
:&lt;math&gt;\sum_{i=1}^\infty a_i z^i \text{ where } a_i = \frac{(-1)^{n-1}}{2^nn}\text{ for } n = \lfloor\log_2(i)\rfloor+1\text{, the unique integer with }2^{n-1}\le i &lt; 2^n,&lt;/math&gt;
has radius of convergence 1 and converges [[uniform convergence|uniformly]] on the entire boundary {|''z''|&amp;nbsp;=&amp;nbsp;1}, but does not [[Absolute convergence|converge absolutely]] on the boundary.&lt;ref&gt;{{citation|last=Sierpiński|first=Wacław|author-link=Wacław Sierpiński|year=1918|title=O szeregu potęgowym który jest zbieżny na całem swem kole zbieżności jednostajnie ale nie bezwzględnie|periodical=Prace matematyka-fizyka|volume=29|pages=263–266}}&lt;/ref&gt;

==Rate of convergence==

If we expand the function

:&lt;math&gt;f(x)=\sin x = \sum^{\infin}_{n=0} \frac{(-1)^n}{(2n+1)!} x^{2n+1} =  x - \frac{x^3}{3!} + \frac{x^5}{5!} - \cdots\text{ for all } x&lt;/math&gt;

around the point ''x'' = 0, we find out that the radius of convergence of this series is &lt;math&gt;\scriptstyle\infty&lt;/math&gt; meaning that this series converges for all complex numbers.  However, in applications, one is often interested in the precision of a [[numerical analysis|numerical answer]].  Both the number of terms and the value at which the series is to be evaluated affect the accuracy of the answer.  For example, if we want to calculate {{math|1=''f''(0.1) = sin(0.1)}} accurate up to five decimal places, we only need the first two terms of the series.  However, if we want the same precision for {{math|1=''x'' = 1}} we must evaluate and sum the first five terms of the series.  For {{math|''f''(10)}}, one requires the first 18 terms of the series, and for {{math|''f''(100)}} we need to evaluate the first 141 terms.

So the fastest convergence of a power series expansion is at the center, and as one moves away from the center of convergence, the [[rate of convergence]] slows down until you reach the boundary (if it exists) and cross over, in which case the [[Series (mathematics)|series]] will diverge.

==Abscissa of convergence of a Dirichlet series==

An analogous concept is the '''abscissa of convergence of a [[Dirichlet series]]

:&lt;math&gt;\sum_{n=1}^\infty {a_n \over n^s}.&lt;/math&gt;

Such a series converges if the real part of ''s'' is greater than a particular number depending on the coefficients ''a''&lt;sub&gt;''n''&lt;/sub&gt;: the [[abscissa]] of convergence.

==Notes==
{{reflist}}

==References==
* {{Citation | last1=Brown | first1=James | last2=Churchill | first2=Ruel | title=Complex variables and applications | publisher=[[McGraw-Hill]] | location=New York | isbn=978-0-07-010905-6 | year=1989}}
* {{Citation | last1=Stein | first1=Elias | authorlink=Elias M. Stein |last2=Shakarchi | first2=Rami | title=Complex Analysis | publisher=[[Princeton University Press]] | location=Princeton, New Jersey | isbn=0-691-11385-8 | year=2003}}

==External links==
*[http://www.lassp.cornell.edu/sethna/Cracks/What_Is_Radius_of_Convergence.html What is radius of convergence?]

[[Category:Analytic functions]]
[[Category:Convergence (mathematics)]]
[[Category:Mathematical physics]]
[[Category:Theoretical physics]]</text>
      <sha1>12xy23d7kbqt8ny5u0xw4ki1dz588jq</sha1>
    </revision>
  </page>
  <page>
    <title>Recursive data type</title>
    <ns>0</ns>
    <id>2227485</id>
    <revision>
      <id>812740950</id>
      <parentid>812740762</parentid>
      <timestamp>2017-11-29T16:12:58Z</timestamp>
      <contributor>
        <username>Bradypusedinae</username>
        <id>28214459</id>
      </contributor>
      <minor/>
      <comment>More direct description.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7111">In computer [[programming language]]s, a '''recursive data type''' (also known as a '''recursively-defined''', '''inductively-defined''' or '''inductive data type''') is a [[data type]] for values that may contain other values of the same type. Data of recursive types are usually viewed as [[directed graph]]s.

An important application of recursion in computer science is in defining dynamic data structures such as Lists and Trees.  Recursive data structures can dynamically grow to an arbitrarily large size in response to runtime requirements; in contrast, a static array's size requirements must be set at compile time.

Sometimes the term "inductive data type" is used for [[algebraic data type]]s which are not necessarily recursive.

==Example==
{{also|Recursion (computer science)#Recursive data structures (structural recursion)}}
An example is the [[List (computing)|list]] type, in [[Haskell (programming language)|Haskell]]:

&lt;source lang="haskell"&gt;
data List a = Nil | Cons a (List a)
&lt;/source&gt;

This indicates that a list of a's is either an empty list or a '''cons cell''' containing an 'a' (the "head" of the list) and another list (the "tail").

Another example is a similar singly linked type in Java:

&lt;source lang="java"&gt;
class List&lt;E&gt; {
    E value;
    List&lt;E&gt; next;
}
&lt;/source&gt;

This indicates that non-empty list of type E contains a data member of type E, and a reference to another List object for the rest of the list (or a null reference to indicate that this is the end of the list).

===Mutually recursive data types===
{{see|Mutual recursion#Data types}}
Data types can also be defined by [[mutual recursion]]. The most important basic example of this is a [[tree (data structure)|tree]], which can be defined mutually recursively in terms of a forest (a list of trees). Symbolically:
 f: &lt;nowiki&gt;[t[1], ..., t[k]]&lt;/nowiki&gt;
 t: v f
A forest ''f'' consists of a list of trees, while a tree ''t'' consists of a pair of a value ''v'' and a forest ''f'' (its children). This definition is elegant and easy to work with abstractly (such as when proving theorems about properties of trees), as it expresses a tree in simple terms: a list of one type, and a pair of two types.

This mutually recursive definition can be converted to a singly recursive definition by inlining the definition of a forest:
 t: v &lt;nowiki&gt;[t[1], ..., t[k]]&lt;/nowiki&gt;
A tree ''t'' consists of a pair of a value ''v'' and a list of trees (its children). This definition is more compact, but somewhat messier: a tree consists of a pair of one type and a list another, which require disentangling to prove results about.

In [[Standard ML]], the tree and forest data types can be mutually recursively defined as follows, allowing empty trees:{{sfn|Harper|2000|loc="[http://www.cs.cmu.edu/~rwh/introsml/core/datatypes.htm Data Types]"}}
&lt;source lang=sml&gt;
datatype 'a tree = Empty | Node of 'a * 'a forest
and      'a forest = Nil | Cons of 'a tree * 'a forest
&lt;/source&gt;

==Theory==
In [[type theory]], a recursive type has the general form μα.T where the [[type variable]] α may appear in the type T and stands for the entire type itself.

For example, the natural numbers (see [[Peano arithmetic]]) may be defined by the Haskell datatype:

&lt;source lang="haskell"&gt;
data Nat = Zero | Succ Nat
&lt;/source&gt;

In type theory, we would say: &lt;math&gt;nat = \mu \alpha. 1 + \alpha&lt;/math&gt; where the two arms of the [[sum type]] represent the Zero and Succ data constructors. Zero takes no arguments (thus represented by the [[unit type]]) and Succ takes another Nat (thus another element of &lt;math&gt;\mu \alpha. 1 + \alpha&lt;/math&gt;).

There are two forms of recursive types: the so-called isorecursive types, and equirecursive types.  The two forms differ in how terms of a recursive type are introduced and eliminated.

===Isorecursive types===
With isorecursive types, the recursive type &lt;math&gt;\mu \alpha . T&lt;/math&gt; and its expansion (or ''unrolling'') &lt;math&gt;T[\mu \alpha.T / \alpha]&lt;/math&gt; (Where the notation&lt;math&gt;X[Y/Z]&lt;/math&gt; indicates that all instances of Z are replaced with Y in X) are distinct (and disjoint) types with special term constructs, usually called ''roll'' and ''unroll'', that form an [[isomorphism]] between them.  To be precise: &lt;math&gt;roll : T[\mu\alpha.T/\alpha] \to \mu\alpha.T&lt;/math&gt; and &lt;math&gt;unroll : \mu\alpha.T \to T[\mu\alpha.T/\alpha]&lt;/math&gt;, and these two are [[inverse function]]s.

===Equirecursive types===
Under equirecursive rules, a recursive type &lt;math&gt;\mu \alpha . T&lt;/math&gt; and its unrolling &lt;math&gt;T[\mu\alpha.T/\alpha]&lt;/math&gt; are ''equal'' -- that is, those two type expressions are understood to denote the same type.  In fact, most theories of equirecursive types go further and essentially stipulate that any two type expressions with the same "infinite expansion" are equivalent.  As a result of these rules, equirecursive types contribute significantly more complexity to a type system than isorecursive types do.  Algorithmic problems such as type checking and [[type inference]] are more difficult for equirecursive types as well.  Since direct comparison does not make sense on an equirecursive type, they can be converted into a canonical form in O(n log n) time, which can easily be compared.&lt;ref name="canonicalize"&gt;
{{cite web|url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.4.2276|title=Numbering Matters: First-Order Canonical Forms for Second-Order Recursive Types}}
&lt;/ref&gt;

Equirecursive types capture the form of self-referential (or mutually referential) type definitions seen in procedural and [[object-oriented]] programming languages, and also arise in type-theoretic semantics of objects and [[Class (computer science)|class]]es.
In functional programming languages, isorecursive types (in the guise of datatypes) are far more common.

====In type synonyms====
Recursion is not allowed in [[type synonym]]s in [[Miranda programming language|Miranda]], [[OCaml]] (unless -rectypes flag is used or it's a record or variant), and Haskell; so for example the following Haskell types are illegal:

&lt;source lang="haskell"&gt;
type Bad = (Int, Bad)
type Evil = Bool -&gt; Evil
&lt;/source&gt;

Instead, you must wrap it inside an algebraic data type (even if it only has one constructor):

&lt;source lang="haskell"&gt;
data Good = Pair Int Good
data Fine = Fun (Bool-&gt;Fine)
&lt;/source&gt;

This is because type synonyms, like [[typedef]]s in C, are replaced with their definition at compile time. (Type synonyms are not "real" types; they are just "aliases" for convenience of the programmer.) But if you try to do this with a recursive type, it will loop infinitely because no matter how many times you substitute it, it still refers to itself, e.g. "Bad" will grow indefinitely: (Int, (Int, (Int, ... .

Another way to see it is that a level of indirection (the algebraic data type) is required to allow the isorecursive type system to figure out when to ''roll'' and ''unroll''.

==See also==
* [[Recursive definition]]
* [[Algebraic data type]]
* [[Node (computer science)]]

==Notes==
{{Reflist}}

{{Data types}}

{{FOLDOC}}

[[Category:Data types]]
[[Category:Type theory]]</text>
      <sha1>3umm0j9r5qfd6lkfmz8wotylkg8cpgj</sha1>
    </revision>
  </page>
  <page>
    <title>Reflection symmetry</title>
    <ns>0</ns>
    <id>1390873</id>
    <revision>
      <id>871791144</id>
      <parentid>871791107</parentid>
      <timestamp>2018-12-03T13:18:45Z</timestamp>
      <contributor>
        <username>Acroterion</username>
        <id>1839637</id>
      </contributor>
      <comment>Reverted to revision 870539431 by [[Special:Contributions/Arjayay|Arjayay]] ([[User talk:Arjayay|talk]]): Rv. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8405">{{refimprove|date=October 2015}}
[[Image:Symmetry.png|thumb|250px|right|Figures with the axes of [[symmetry]] drawn in. The figure with no axes is [[asymmetry|asymmetric]].]]

'''Reflection symmetry''', '''line symmetry''', '''mirror symmetry''', '''mirror-image symmetry''', is [[symmetry]] with respect to [[Reflection (mathematics)|reflection]]. That is, a figure which does not change upon undergoing a reflection has reflectional symmetry.

In [[2D geometric model|2D]] there is a line/axis of symmetry, in [[Three-dimensional space|3D]] a [[plane (mathematics)|plane]] of symmetry. An object or figure which is indistinguishable from its transformed image is called [[mirror image|mirror symmetric]].

==Symmetric function== 
[[File:Empirical Rule.PNG|thumb|A [[normal distribution]] bell curve is an example symmetric function]]

In formal terms, a [[mathematical object]] is symmetric with respect to a given [[mathematical operation|operation]] such as reflection, [[Rotational symmetry|rotation]] or [[Translational symmetry|translation]], if, when applied to the object, this operation preserves some property of the object.&lt;ref name=Stewart32&gt;{{cite book | title=What Shape is a Snowflake? Magical Numbers in Nature | publisher=Weidenfeld &amp; Nicolson | author=Stewart, Ian | year=2001 | page=32}}&lt;/ref&gt; The set of operations that preserve a given property of the object form a [[group (algebra)|group]]. Two objects are symmetric to each other with respect to a given group of operations if one is obtained from the other by some of the operations (and vice versa).

The symmetric function of a two-dimensional figure is a line such that, for each [[perpendicular]] constructed, if the perpendicular intersects the figure at a distance 'd' from the axis along the perpendicular, then there exists another intersection of the shape and the perpendicular, at the same distance 'd' from the axis, in the opposite direction along the perpendicular.

Another way to think about the symmetric function is that if the shape were to be folded in half over the axis, the two halves would be identical: the two halves are each other's [[mirror image]]s.&lt;ref name=Stewart32/&gt;

Thus a square has four axes of symmetry, because there are four different ways to fold it and have the edges all match. A circle has infinitely many axes of symmetry.
{{-}}

==Symmetric geometrical shapes==
{| class=wikitable align=right
|+ 2D shapes w/reflective symmetry
|[[File:Isosceles_trapezoid.svg|100px]]
|[[File:GeometricKite.svg|100px]]
|-
!colspan=2|[[isosceles trapezoid]] and [[kite (geometry)|kite]]
|-
|[[File:Hexagon p2 symmetry.png|100px]]
|[[File:Hexagon d3 symmetry.png|100px]]
|-
!colspan=2|[[Hexagon]]s
|-
|[[File:Octagon p2 symmetry.png|100px]]
|[[File:Octagon d2 symmetry.png|100px]]
|-
!colspan=2|[[octagon]]s
|}

[[Triangle]]s with reflection symmetry are [[isosceles]]. [[Quadrilateral]]s with reflection symmetry are [[kite (geometry)|kite]]s, (concave) deltoids, [[rhombus]]es,&lt;ref&gt;{{cite book |last1=Gullberg |first1=Jan |authorlink=Jan Gullberg |title=Mathematics: From the Birth of Numbers |date=1997 |publisher=W. W. Norton |isbn=0-393-04002-X|pages=394–395}}&lt;/ref&gt; and [[isosceles trapezoid]]s. All even-sided polygons have two simple reflective forms, one with lines of reflections through vertices, and one through edges.

For an arbitrary shape, the [[axiality (geometry)|axiality]] of the shape measures how close it is to being bilaterally symmetric. It equals 1 for shapes with reflection symmetry, and between 2/3 and 1 for any convex shape.

==Mathematical equivalents==

For each line or plane of reflection, the [[symmetry group]] is isomorphic with ''C&lt;sub&gt;s&lt;/sub&gt;'' (see [[point groups in three dimensions]]), one of the three types of order two ([[Involution (mathematics)|involution]]s), hence algebraically  ''C&lt;sub&gt;2&lt;/sub&gt;''. The [[fundamental domain]] is a half-plane or half-space.

In certain contexts there is rotational as well as reflection symmetry. Then mirror-image symmetry is equivalent to inversion symmetry; in such contexts in modern physics the term [[Parity (physics)|parity]] or P-symmetry is used for both.

==Advanced types of reflection symmetry==
For more general types of [[reflection (mathematics)|reflection]] there are correspondingly more general types of reflection symmetry. For example:

* with respect to a non-isometric [[affine involution]] (an [[oblique reflection]] in a line, plane, etc.)
* with respect to [[inversive geometry|circle inversion]].

==In nature==
[[File:Maja crispata (Maia verrucosa) - Museo Civico di Storia Naturale Giacomo Doria - Genoa, Italy - DSC03222 Cropped.JPG|thumb|Many animals, such as this [[spider crab]] ''[[Maja crispata]]'', are bilaterally symmetric.]]
{{main|Bilateral symmetry}}

[[Bilateria|Animals that are bilaterally symmetric]] have reflection symmetry in the sagittal plane, which divides the body vertically into left and right halves, with one of each sense organ and limb pair on either side. Most animals are bilaterally symmetric, likely because this supports forward movement and streamlining.&lt;ref&gt;{{cite web |last=Valentine |first=James W. |title=Bilateria |url=http://www.accessscience.com/abstract.aspx?id=802620&amp;referURL=http%3a%2f%2fwww.accessscience.com%2fcontent.aspx%3fid%3d802620 |publisher=AccessScience |accessdate=29 May 2013}}&lt;/ref&gt;&lt;ref name=NHM&gt;{{cite web | url=http://www.nhm.ac.uk/nature-online/evolution/what-is-the-evidence/morphology/bilateralism/ | title=Bilateral symmetry | publisher=Natural History Museum | accessdate=14 June 2014}}&lt;/ref&gt;&lt;ref name=Finnerty&gt;{{cite journal | url=http://faculty.weber.edu/rmeyers/PDFs/Finnerty%20-%20symmetry%20evol.pdf | title=Did internal transport, rather than directed locomotion, favor the evolution of bilateral symmetry in animals? | author=Finnerty, John R. | journal=BioEssays | year=2005 | volume=27 | pages=1174–1180 | doi=10.1002/bies.20299 | pmid=16237677}}&lt;/ref&gt;&lt;ref name=Berkeley&gt;{{cite web | url=http://evolution.berkeley.edu/evolibrary/article/arthropods_04 | title=Bilateral (left/right) symmetry | publisher=Berkeley | accessdate=14 June 2014}}&lt;/ref&gt;
{{-}}

==In architecture==

[[File:Santa Maria Novella.jpg|thumb|Mirror symmetry is often used in [[architecture]], as in the facade of [[Santa Maria Novella]], [[Florence]], 1470.]]

{{main|Mathematics and architecture}}

Mirror symmetry is often used in [[architecture]], as in the facade of [[Santa Maria Novella]], [[Venice]].&lt;ref name="Tavernor1998"&gt;{{cite book |last=Tavernor|first=Robert |title=On Alberti and the Art of Building |url=https://books.google.com/books?id=hOs2zXz7M7wC&amp;pg=PA103 |year=1998 |publisher=Yale University Press |isbn=978-0-300-07615-8 |pages=102–106 |quote=More accurate surveys indicate that the facade lacks a precise symmetry, but there can be little doubt that Alberti intended the composition of number and geometry to be regarded as perfect. The facade fits within a square of 60 Florentine braccia}}&lt;/ref&gt; It is also found in the design of ancient structures such as [[Stonehenge]].&lt;ref name="Johnson, Anthony 2008"&gt;Johnson, Anthony (2008). ''Solving Stonehenge: The New Key to an Ancient Enigma''. Thames &amp; Hudson.&lt;/ref&gt; Symmetry was a core element in some styles of architecture, such as [[Palladianism]].&lt;ref&gt;{{cite web |last1=Waters |first1=Suzanne |title=Palladianism |url=https://www.architecture.com/Explore/ArchitecturalStyles/Palladianism.aspx |publisher=Royal Institution of British Architects |accessdate=29 October 2015}}&lt;/ref&gt;

==See also ==

* [[Patterns in nature]]
* [[Point reflection]] symmetry

== References ==
{{reflist}}

==Bibliography==

===General===

* {{cite book | title=What Shape is a Snowflake? Magical Numbers in Nature | publisher=Weidenfeld &amp; Nicolson | author=Stewart, Ian | year=2001}}is potty

===Advanced===

* {{cite book |title=Symmetry |last=Weyl |first=Hermann |authorlink=Hermann Weyl |coauthors= |year=1982 |origyear=1952 |publisher=Princeton University Press |location=Princeton |isbn=0-691-02374-3 |pages= |url= |ref=Weyl 1982}}

==External links==
{{commons category|Reflection symmetry}}
*[http://republika.pl/fraktal/mapping.html Mapping with symmetry - source in Delphi]
*[http://www.mathsisfun.com/geometry/symmetry-reflection.html Reflection Symmetry Examples] from [[Math Is Fun]]

[[Category:Elementary geometry]]
[[Category:Euclidean symmetries]]

[[es:Eje de simetría]]</text>
      <sha1>hjmgntnkicbwd7fgc8s7wizpx0u5izk</sha1>
    </revision>
  </page>
  <page>
    <title>Riemann–Lebesgue lemma</title>
    <ns>0</ns>
    <id>1461209</id>
    <revision>
      <id>870013628</id>
      <parentid>870013549</parentid>
      <timestamp>2018-11-21T21:08:15Z</timestamp>
      <contributor>
        <ip>81.243.243.97</ip>
      </contributor>
      <comment>The proof on research gate is wrong.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6022">[[Image:Highly oscillatory function.png|right|frame|The Riemann–Lebesgue lemma states that the integral of a function like the above is small. The integral will approach zero as the number of oscillations increases.]]

In [[mathematics]], the '''Riemann–Lebesgue lemma''', named after [[Bernhard Riemann]] and [[Henri Lebesgue]], is of importance in [[harmonic analysis]] and [[asymptotic analysis]].

The lemma says that the [[Fourier transform]] or [[Laplace transform]] of an [[Lp space|''L''&lt;sup&gt;1&lt;/sup&gt; function]] vanishes at infinity.

== Statement ==
If ''ƒ'' is  [[Lp space|''L''&lt;sup&gt;1&lt;/sup&gt; integrable]] on '''R'''&lt;sup&gt;''d''&lt;/sup&gt;, that is to say, if the Lebesgue integral of |''ƒ''| is finite, then the [[Fourier transform]] of ''ƒ'' satisfies

:&lt;math&gt; \hat{f}(z):=\int_{\mathbb{R}^d} f(x) \exp(-iz \cdot x)\,dx \rightarrow 0\text{ as } |z|\rightarrow \infty.&lt;/math&gt;

=== Proof ===
First suppose that &lt;math&gt;f(x)=\chi_{(a,b)}(x)&lt;/math&gt;, the [[characteristic function]] of an [[Interval (mathematics)|open interval]].

Then:&lt;blockquote&gt;&lt;math&gt;\int f(x)e^{i\xi x}dx=\int_a^be^{i\xi x}dx=\frac{e^{i\xi b}-e^{i\xi a}}{i\xi} \rightarrow 0&lt;/math&gt; as &lt;math&gt;|\xi| \rightarrow \infty&lt;/math&gt;&lt;/blockquote&gt;By additivity of limits, the same holds for an arbitrary [[simple function]].

That is, for any function &lt;math&gt;f&lt;/math&gt; of the form:&lt;blockquote&gt;&lt;math&gt;f=\sum_{i=1}^Nc_i\chi_{(a_i,b_i)},~~c_i\in\R,~~a_i\leq b_i\in\R&lt;/math&gt;&lt;/blockquote&gt;We have that:&lt;blockquote&gt;&lt;math&gt;\lim_{|\xi|\rightarrow\infty}\int f(x)e^{i\xi x}dx=0&lt;/math&gt;&lt;/blockquote&gt;Finally, let &lt;math&gt;f\in L^1&lt;/math&gt;be arbitrary.

Let &lt;math&gt;\epsilon \in \R&gt;0&lt;/math&gt; be fixed.

Since the simple functions are dense in &lt;math&gt;L^1&lt;/math&gt;, there exists a [[simple function]] &lt;math&gt;g&lt;/math&gt; such that:&lt;blockquote&gt;&lt;math&gt;\int\left\vert f(x)-g(x)\right\vert dx &lt;\epsilon&lt;/math&gt;&lt;/blockquote&gt;By our previous argument and the definition of a limit of a complex function, there exists &lt;math&gt;N\in\N&lt;/math&gt; such that for all &lt;math&gt;|\xi|&gt;N&lt;/math&gt;:&lt;blockquote&gt;&lt;math&gt;\left\vert \int g(x)e^{i\xi x}dx \right\vert&lt;\epsilon&lt;/math&gt;&lt;/blockquote&gt;By Integral of Integrable Function is Additive:&lt;blockquote&gt;&lt;math&gt;\int f(x)e^{i\xi x}dx=\int(f(x)-g(x))e^{i\xi x}dx+\int g(x)e^{i\xi x}dx&lt;/math&gt;&lt;/blockquote&gt;By the [[triangle inequality]] for complex numbers, the [triangle inequality] for integrals, multiplicativity of the absolute value, and [[Euler's formula|Euler's Formula]]:&lt;blockquote&gt;&lt;math&gt;\left\vert \int f(x)e^{i\xi x}dx \right\vert\leq\int \left\vert f(x)-g(x) \right\vert dx+\left\vert \int g(x)e^{i\xi x}dx \right\vert&lt;/math&gt;&lt;/blockquote&gt;For all &lt;math&gt;|\xi|&gt;N&lt;/math&gt;, the right side is bounded by &lt;math&gt;2\epsilon&lt;/math&gt; by our previous arguments.

Since &lt;math&gt;\epsilon&lt;/math&gt; was arbitrary, this establishes:&lt;blockquote&gt;&lt;math&gt;\lim_{|\xi|\rightarrow \infty}\int f(x)e^{i\xi x}dx=0&lt;/math&gt;&lt;/blockquote&gt;for all &lt;math&gt;f\in L^1&lt;/math&gt;.

==Other versions==
The Riemann&amp;ndash;Lebesgue lemma holds in a variety of other situations.

* If ''ƒ'' is ''L''&lt;sup&gt;1&lt;/sup&gt; integrable and supported on (0,&amp;nbsp;∞), then the Riemann&amp;ndash;Lebesgue lemma also holds for the Laplace transform of&amp;nbsp;''ƒ''.  That is,

::&lt;math&gt;\int_0^\infty f(t) e^{-tz}\,dt \to 0&lt;/math&gt;

:as |''z''|&amp;nbsp;&amp;rarr;&amp;nbsp;&amp;infin; within the half-plane Re(''z'')&amp;nbsp;≥&amp;nbsp;0.

* A version holds for [[Fourier series]] as well: if ''ƒ'' is an integrable function on an interval, then the [[Fourier coefficient]]s of ''ƒ'' tend to 0 as ''n''&amp;nbsp;→&amp;nbsp;±∞,
::&lt;math&gt;\hat{f}_n \ \to \ 0 .&lt;/math&gt;
:This follows by extending ''ƒ'' by zero outside the interval, and then applying the version of the lemma on the entire real line.

*A similar statement is trivial for {{math|''L''&lt;sup&gt;2&lt;/sup&gt;}} functions.  To see this, note that the Fourier transform takes {{math|''L''&lt;sup&gt;2&lt;/sup&gt;}} to {{math|''L''&lt;sup&gt;2&lt;/sup&gt;}} and such functions have {{math|''l''&lt;sup&gt;2&lt;/sup&gt;}} Fourier series.

* However, the lemma does ''not'' hold for arbitrary distributions. For example, the Dirac delta function distribution formally has a finite integral over the real line, but its Fourier transform is a constant (the exact value depends on the form of the transform used) and does not vanish at infinity.

==Applications==
The Riemann–Lebesgue lemma can be used to prove the validity of asymptotic approximations for integrals. Rigorous treatments of the [[method of steepest descent]] and the [[method of stationary phase]], amongst others, are based on the Riemann–Lebesgue lemma.

==Proof==

We'll focus on the one-dimensional case, the proof in higher dimensions is similar. Suppose first that ''ƒ'' is a [[compact set|compactly]] supported [[smooth function]]. Then [[integration by parts]] yields

:&lt;math&gt; \left| \int f(x) e^{-izx} \, dx\right|=\left|\int \frac{1}{iz} f'(x)e^{-izx} \, dx\right| \leq \frac{1}{|z|}\int|f'(x)| \, dx  \rightarrow 0 \text{ as } z\rightarrow\pm\infty. &lt;/math&gt;

If ''ƒ'' is an arbitrary integrable function, it may be approximated in the ''L''&lt;sup&gt;1&lt;/sup&gt; norm by a compactly supported smooth function ''g''. Pick such a ''g'' so that ||''ƒ''&amp;nbsp;&amp;minus;&amp;nbsp;''g''||&lt;sub&gt;''L''&lt;sup&gt;1&lt;/sup&gt;&lt;/sub&gt;&amp;nbsp;&lt;&amp;nbsp;''ε''. Then

:&lt;math&gt; \limsup_{z\rightarrow\pm\infty} |\hat{f}(z)| \leq  \limsup_{z\to\pm\infty}  \left|\int (f(x)-g(x))e^{-ixz} \, dx\right| + \limsup_{z\rightarrow\pm\infty}  \left|\int g(x)e^{-ixz} \, dx\right| \leq \varepsilon+0=\varepsilon,&lt;/math&gt;
and since this holds for any ''ε''&amp;nbsp;&gt;&amp;nbsp;0, the theorem follows.

==References==
*{{cite book | author =[[Salomon Bochner|Bochner S.]], [[K. S. Chandrasekharan|Chandrasekharan K.]] | title=Fourier Transforms | publisher= Princeton University Press | year=1949}}
* {{mathworld|urlname=Riemann-LebesgueLemma|title=Riemann–Lebesgue Lemma}}
*  https://proofwiki.org/wiki/Euler%27s_Formula

{{DEFAULTSORT:Riemann-Lebesgue lemma}}
[[Category:Asymptotic analysis]]
[[Category:Harmonic analysis]]
[[Category:Lemmas]]
[[Category:Theorems in analysis]]
[[Category:Theorems in harmonic analysis]]
[[Category:Bernhard Riemann]]</text>
      <sha1>qi815f1g3wmso56ei8bx5rxm3mmkimd</sha1>
    </revision>
  </page>
  <page>
    <title>Ruppeiner geometry</title>
    <ns>0</ns>
    <id>9258361</id>
    <revision>
      <id>822461862</id>
      <parentid>711613010</parentid>
      <timestamp>2018-01-26T14:27:06Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v481)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5079">'''Ruppeiner geometry''' is thermodynamic geometry (a type of [[information geometry]]) using the language of [[Riemannian geometry]] to study [[thermodynamics]]. George Ruppeiner proposed it in 1979. He claimed that [[thermodynamic system]]s can be represented by Riemannian geometry, and that statistical properties can be derived from the model.

This geometrical model is based on the inclusion of the theory of fluctuations into the [[axioms]] of [[equilibrium thermodynamics]], namely, there exist equilibrium states which can be represented by points on two-dimensional surface (manifold) and the distance between these equilibrium states is related to the fluctuation between them. This concept is associated to probabilities, i.e. the less probable a fluctuation between states, the further apart they are. This can be recognized if one considers the [[metric tensor]] g&lt;sub&gt;ij&lt;/sub&gt; in the distance formula (line element) between the two equilibrium states

:&lt;math&gt; ds^2 = g^R_{ij} dx^i dx^j, \, &lt;/math&gt;

where the matrix of coefficients ''g''&lt;sub&gt;''ij''&lt;/sub&gt; is the symmetric metric tensor which is called a [[Ruppeiner metric]], defined as a negative Hessian of the [[entropy]] function

:&lt;math&gt; g^R_{ij} = -\partial_i \partial_j S(U, N^a) &lt;/math&gt;

where U is the [[internal energy]] (mass) of the system and N&lt;sup&gt;a&lt;/sup&gt; refers to the extensive parameters of the system. Mathematically, the Ruppeiner geometry is one particular type of [[information geometry]] and it is similar to the [[Fisher-Rao]] metric used in mathematical statistics.

The Ruppeiner metric can be understood as the thermodynamic limit (large systems limit) of the more general [[Fisher information metric]].&lt;ref&gt;Gavin E. Crooks, "Measuring thermodynamic length" (2007), [https://arxiv.org/abs/0706.0559 ArXiv 0706.0559]&lt;/ref&gt; For small systems (systems where fluctuations are large), the Ruppeiner metric may not exist, as second derivatives of the entropy are not guaranteed to be non-negative.

The Ruppeiner metric is conformally related to the [[Weinhold metric]] via

:&lt;math&gt; ds^2_R = \frac{1}{T} ds^2_W \, &lt;/math&gt;

where T is the temperature of the system under consideration. Proof of the conformal relation can be easily done when one writes down the [[first law of thermodynamics]] (dU=TdS+...) in differential form with a few manipulations. The Weinhold geometry is also considered as a thermodynamic geometry. It is defined as a Hessian of the internal energy with respect to entropy and other extensive parameters.

:&lt;math&gt; g^W_{ij} = \partial_i \partial_j U(S, N^a) &lt;/math&gt;

It has long been observed that the Ruppeiner metric is flat for systems with noninteracting underlying statistical mechanics such as the ideal gas. Curvature singularities signal critical behaviors. In addition, it has been applied to a number of statistical systems including Van de Waals gas. Recently the anyon gas has been studied using this approach.

==Application to black hole systems==

In the last five years or so, this geometry has been applied to [[black hole thermodynamics]], with some physically relevant results. The most physically significant case is for the [[Kerr black hole]] in higher dimensions, where the curvature singularity signals thermodynamic instability, as found earlier by conventional methods.

The entropy of a black hole is given by the well-known [[Bekenstein-Hawking formula]]

:&lt;math&gt; S =\frac{k_B c^3 A}{4G \hbar} &lt;/math&gt;

where &lt;math&gt; k_B &lt;/math&gt; is [[Boltzmann's constant]], &lt;math&gt; c &lt;/math&gt; the [[speed of light]], &lt;math&gt; G &lt;/math&gt; [[Newton's constant]] and &lt;math&gt; A &lt;/math&gt; is the area of the [[event horizon]] of the black hole. Calculating the Ruppeiner geometry of the black hole's entropy is, in principle, straightforward, but it is important that the entropy should be written in terms of extensive parameters, 
:&lt;math&gt; S= S(M, N^a) &lt;/math&gt;
where &lt;math&gt; M &lt;/math&gt; is [[ADM mass]] of the black hole and &lt;math&gt; N^a &lt;/math&gt; are the conserved charges and &lt;math&gt; a&lt;/math&gt; runs from 1 to n. The signature of the metric reflects the sign of the hole's [[specific heat]]. For a [[Reissner-Nordström]] black hole, the Ruppeiner metric has a Lorentzian signature which corresponds to the negative [[heat capacity]] it possess, while for the [[BTZ black hole]], we have a [[Euclidean space|Euclidean]] signature. This calculation cannot be done for the Schwarzschild black hole, because its entropy is 
:&lt;math&gt; S = S(M)&lt;/math&gt; 
which renders the metric degenerate.

==References==
{{reflist}}

* {{citation | first=George | last=Ruppeiner | year=1995 | title=Riemannian geometry in thermodynamic fluctuation theory | journal=Reviews of Modern Physics | volume=67 | pages=605–659 | doi=10.1103/RevModPhys.67.605 | issue=3 | bibcode=1995RvMP...67..605R}}.
* John E. Åman, Ingemar Bengtsson, Narit Pidokrajt, John Ward, [http://www.physto.se/~narit/NaritMG11proceeding.pdf "Thermodynamic geometries of black holes"] (2006)

[[Category:Riemannian geometry]]
[[Category:Thermodynamics]]
[[Category:New College of Florida faculty]]
[[Category:Mathematical physics]]</text>
      <sha1>sl1gozwm8b5stbcjdvpiufszqtntmnn</sha1>
    </revision>
  </page>
  <page>
    <title>Simon problems</title>
    <ns>0</ns>
    <id>57577573</id>
    <revision>
      <id>858846549</id>
      <parentid>858846494</parentid>
      <timestamp>2018-09-10T00:53:19Z</timestamp>
      <contributor>
        <username>Vermont</username>
        <id>28103413</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/150.101.228.160|150.101.228.160]] ([[User talk:150.101.228.160|talk]]) ([[WP:HG|HG]]) (3.4.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3612">{{distinguish|text=[[Simon's problem]] in computational complexity}}
In mathematics, the '''Simon problems''' (or '''Simon's problems''') are a series of fifteen questions posed in the year 2000 by [[Barry Simon]], an American mathematical physicist.&lt;ref&gt;{{cite book|last=Simon |first=Barry |chapter=Schrödinger Operators in the Twenty-First Century | title=Mathematical Physics 2000 |publisher=[[Imperial College London]] |isbn=978-1-86094-230-3 |doi=10.1142/9781848160224_0014 |year=2000}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|title=Dynamics and Spectral Theory of Quasi-Periodic Schrödinger-type Operators |first1=C. A. |last1=Marx |first2=S. |last2=Jitomirskaya |journal=Ergodic Theory and Dynamical Systems |volume=37 |pages=2353&amp;mdash;2393 |year=2017 |arxiv=1503.05740 |doi=10.1017/etds.2016.16}}&lt;/ref&gt; Inspired by other collections of mathematical problems and open conjectures, such as the [[Hilbert problems|famous list]] by [[David Hilbert]], the Simon problems concern [[Operator (physics)#Operators in quantum mechanics|quantum operators]].&lt;ref&gt;{{cite web|url=http://www.math.pku.edu.cn/teachers/gansb/conference09/Damanik-Minicourse-2009-08-09.pdf |title=Dynamics of SL(2,R)-Cocycles and Applications to Spectral Theory; Lecture 1: Barry Simon’s 21st Century Problems |first=David |last=Damanik |access-date=2018-07-07 |website=Beijing International Center for Mathematical Research, [[Peking University]]}}&lt;/ref&gt; In 2014, [[Artur Avila]] won a [[Fields Medal]] for work including the solution of three Simon problems.&lt;ref&gt;{{cite web|url=http://www2.cnrs.fr/en/2435.htm?debut=8&amp;theme1=12 |title=Fields Medal awarded to Artur Avila |website=[[Centre national de la recherche scientifique]] |date=2014-08-13 |access-date=2018-07-07}}&lt;/ref&gt;&lt;ref name="guardian"&gt;{{cite web|url=https://www.theguardian.com/science/alexs-adventures-in-numberland/2014/aug/13/fields-medals-2014-maths-avila-bhargava-hairer-mirzakhani |title=Fields Medals 2014: the maths of Avila, Bhargava, Hairer and Mirzakhani explained |website=[[The Guardian]] |last=Bellos |first=Alex |date=2014-08-13 |access-date=2018-07-07}}&lt;/ref&gt; Among these was the problem of proving that the set of energy levels of one particular abstract quantum system was in fact the [[Cantor set]], a challenge known as the "Ten Martini Problem" after the reward that [[Mark Kac]] offered for solving it.&lt;ref name="guardian"/&gt;&lt;ref&gt;{{cite web|last=Tao |first=Terry |title=Avila, Bhargava, Hairer, Mirzakhani |url=https://terrytao.wordpress.com/2014/08/12/avila-bhargava-hairer-mirzakhani/ |website=What's New |date=2014-08-12 |access-date=2018-07-07 |author-link=Terence Tao}}&lt;/ref&gt; Eight of the problems pertain to anomalous [[Spectrum (functional analysis)|spectral]] behavior of Schrödinger operators, and five concern operators that incorporate the [[Coulomb potential]].

The 2000 list was a refinement of a similar set of problems that Simon had posed in 1984.&lt;ref&gt;{{cite book|last=Simon |first=Barry |chapter=Fifteen problems in mathematical physics |title=Perspectives in Mathematics: Anniversary of Oberwolfach 1984 |year=1984 |publisher=[[Birkhäuser]] |pages=423&amp;ndash;454}}&lt;/ref&gt;&lt;ref&gt;{{cite arxiv|last=Coley |first=Alan A. |title=Open problems in mathematical physics |eprint=1710.02105 }}&lt;/ref&gt;

==See also==
*[[Almost Mathieu operator]]
*[[Lieb–Thirring inequality]]

==External links==
*{{cite web|url=http://mathworld.wolfram.com/SimonsProblems.html |title=Simon's Problems |website=[[MathWorld]] |access-date=2018-06-13}}

==References==
{{reflist}}

{{math-stub}}

[[Category:unsolved problems in mathematics]]
[[Category:Mathematical physics]]</text>
      <sha1>76bk8n2mtcghjpe2ijgdsrqtv6q0inn</sha1>
    </revision>
  </page>
  <page>
    <title>Simulations and games in economics education</title>
    <ns>0</ns>
    <id>19133673</id>
    <revision>
      <id>867993193</id>
      <parentid>761486783</parentid>
      <timestamp>2018-11-09T08:31:29Z</timestamp>
      <contributor>
        <username>Tim Rogmans</username>
        <id>35095571</id>
      </contributor>
      <comment>Added example of Macroeconomics simulation</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11171">{{Economics sidebar}}
A [[simulation game]] is "a [[game]] that contains a mixture of [[game of skill|skill]], [[game of chance|chance]], and [[game of strategy|strategy]] to simulate an aspect of reality, such as a [[stock exchange]]". Similarly, Finnish author [[Virpi Ruohomäki]] states that "a simulation game combines the features of a game (competition, cooperation, rules, participants, roles) with those of a simulation (incorporation of critical features of reality). A game is a simulation game if its rules refer to an empirical model of reality."&lt;ref name="Riis"&gt;{{cite book | author= Ruohomaki, V. | title= Viewpoints on Learning and Education with Simulation Games in Simulation Games and Learning in Production Management edited by Jens O. Riis | publisher= Springer | year=1995| id=ISBN O-412-72100-7 | pages = 14–28}}&lt;/ref&gt; A properly built simulation game used to teach or learn [[economics]] would closely follow the assumptions and rules of the [[Model (economics)|theoretical model]]s within this discipline.
__TOC__

==In economics education==

[[Economics education]] studies recommend the adoption of more active and collaborative learning methodologies (Greenlaw, 1999).&lt;ref name="Greenlaw"&gt;{{cite journal | author = Greenlaw, S.A.| year = 1999| title = Using groupware to enhance teaching and learning in undergraduate economics | journal = Journal of Economic Education | volume = 30 (Winter) | pages =33–42 | doi = 10.2307/1183031 | issue = 1 | jstor = 1183031}}&lt;/ref&gt; Simkins (1999) stated "… ''teaching practices, which rely heavily on the lecture format, are not doing enough to develop students' cognitive learning skills, attract good students to economics, and motivate them to continue coursework in the discipline.''" (p. 278).&lt;ref name="Simkins"&gt;{{cite journal | author = Simkins, S.P.| year = 1999| title = Promoting active-student learning using the World Wide Web in economics courses| journal = Journal of Economic Education | volume = 30 (Summer) | pages =278–91 | doi = 10.2307/1183067 | issue = 3 | jstor = 1183067}}&lt;/ref&gt; This is consistent with the results of a survey published in the American Economic Review by Allgood (2004) that shows that students "rarely take economics as a free elective – especially beyond  principles." (p.&amp;nbsp;5).&lt;ref name="Allgood"&gt;{{cite journal | author = Allgood, S.Bosshardt, W., Van der Klaauw, W., and Watts, M. | year = 2004| title = What Students Remember and Say about College Economics Years Later| journal = American Economic Review | volume = 94 | pages =259–65 | doi = 10.1257/0002828041301731 | issue=2}}&lt;/ref&gt;  More is needed to be done in the classroom to excite students about economics education.

Simulations supplement the standard lecture.  Both computerized and non-computer based simulation and games show significant levels of growth in education (see Lean, Moizer, Towler, and Abbey, 2006;&lt;ref name="Lean"&gt;{{cite journal | author = Lean, J., Moizer, M., Towler, C. A.| year = 2006| journal = Active Learning in Higher Education| title = Simulation and games | volume = 7 | pages =227–42 | issue=3 |url=https://hal.archives-ouvertes.fr/hal-00571951/document}}&lt;/ref&gt; Dobbins, Boehlje, Erickson and Taylor, 1995;&lt;ref name="Dobbins"&gt;{{cite journal | author = Dobbins, C. L., Boehlje, M., Erickson, S., and Taylor, R. | year = 1995| title = Using Games to Teach Farm and Agribusiness Management| journal = Review of Agricultural Economics| volume = 17 | pages =247–55 | doi = 10.2307/1349570 | issue = 3 | jstor = 1349570}}&lt;/ref&gt; Gentry, 1990;.&lt;ref name="Gentry"&gt;{{cite book | author= Gentry, J. | title= Guide to Business Gaming and Experiential Learning | publisher= ABSEL and Nichols/GP Publishing | year=1990| id=ISBN O-89397-369-6}}&lt;/ref&gt;

==Example in monopolistic competition==

Through a simulation game, students may participate directly in a market by managing a simulated firm and making decisions on price and production to maximize profits.  An excellent review of the use of a successful market simulation is given by Motahar (1994) in the Journal of Economics Education.&lt;ref name="Motahar"&gt;{{cite journal | author = Motahar, E. | year = 1994| title = Teaching Modeling and Simulation in Economics: A Pleasant Surprise | journal = Journal of Economics Education| volume = 25 | pages =335–342 | doi = 10.2307/1182981 | issue = 4 | jstor = 1182981}}&lt;/ref&gt;

A monopolistic competition simulation game can be used as an example in the standard economics classroom or for [[experimental economics]]. Economic experiments using monopolistic competition simulations can create real-world incentives that may be used in the teaching and learning of economics to help students better understand why markets and other exchange systems work the way they do.  An explanation of experimental economics is given by Roth (1995).
&lt;ref name="Roth"&gt;{{cite book | author= Roth, A.E. | title= Introduction to experimental economics in The Handbook of Experimental Economics by Kagel, J.H. and Roth, A.E.| publisher= Princeton University Press | year=1995|  pages = 3–109}}&lt;/ref&gt;

'''Assumptions of monopolistic competition'''

A simulation game in [[monopolistic competition]] needs to incorporate the standard theoretical assumptions of this market structure, including: 
* Many buyers and sellers
* Easy entry and exit
* Some degree of product differentiation
* Zero economic profits in the long-run

In a simulation of monopolistic competition, each firm must be small in size, and should not be able to influence the direction of the overall market.  Yet each firm has some control over price owing to product differentiation.  To be consistent with economic theory, the simulation model should allow entry of new firms to occur as long as profits are greater than normal, and economic profits exist. The entry of new firms will decrease the market price, and eventually cause economic profits to return to zero (see Baye, 2009).&lt;ref name="Baye"&gt;{{cite book | author=Baye, Michael | title= Managerial Economics and Business Strategy | publisher= McGraw-Hill/Irwin | year=2009| pages = 294–304 | isbn=978-0-07-337568-7}}&lt;/ref&gt;

'''Controllable decisions in monopolistic competition'''

To simulate monopolistic competition, the controllable firm decisions of the participants (students) must include, at a minimum, those specified in the standard theoretical model, including (see Baye, 2009):&lt;ref name="Baye"/&gt; 
* Firm price
* Advertising
* Firm production
* Plant Size

'''Simulation game experience'''

From an educational point of view, students will have an "opportunity" to learn by their own observations and experience through participation in a simulation game (see Schmidt, 2003).&lt;ref name="Schmidt"&gt;{{cite journal | doi = 10.1080/00220480309595209 | author = Schmidt, Stephen J. | year = 2003| title = Active and cooperative learning using Web-based simulations | journal = Journal of Economics Education| volume = 34 | pages =151–167 | issue=2}}&lt;/ref&gt; Consistent with the theoretical model of monopolistic competition (see Baye, 2009),&lt;ref name="Baye"/&gt; student participants would observe and experience that their pricing decisions are controlled by the market. They would "experience" that in the simulation they would have to lower their firm's price to be competitive as new firms entered the market.  In the long-run, they would see the impact of changing plant size.  They would observe that the successful firms would take advantage of economies of scale, but would also be careful not to incur [[diseconomies of scale]] in the long-run.  Students would experience that economic profits cannot be maintained in the long-run. They would see, first hand, that their accounting profits will inevitably decline and move closer to normal profits.  This experience provides students an opportunity to learn (as a supplement to the lecture and readings) the economic messages of monopolistic competition.

== Example in Macroeconomics ==
In 2018, Harvard Business Publishing published "Macroeconomics Simulation: Econland". This 30-minute simulation brings economic policymaking to life by allowing students to make monetary and fiscal policy decisions and consider their impact on the economy of a fictional country. Students manage the economy through a 7-year business cycle in an effort to maximize the approval rating from their population. Exploring the trade-offs of economic policy decision-making and the effects of the global economic environment on a country, students consolidate their understanding of core macroeconomic concepts, including GDP, unemployment, inflation, and budget deficit. At a deeper level, students develop critical thinking skills and learn about economic modeling and system dynamics. The simulation won a Silver Medal at the International Serious Play Awards.

==See also==
*[[:Category:Economic simulation board games]]
*[[Business game]]
*[[Business simulation]]
*[[Business simulation game]]
*[[Game (simulation)]]
*[[Government simulation game]]
*[[Miniconomy]]
*[[Training simulation]] 

==Notes==
{{reflist|2}}

==References==

* Allgood, S., Bosshardt, W., Van der Klaauw, W., and Watts, M. (2004). What Students Remember and Say about College Economics Years Later.  American Economic Review, 94(2), 259-65.
* Dobbins, C. L., Boehlje, M., Erickson, S., and Taylor, R. (1995). Using Games to Teach Farm and Agribusiness Management, Review of Agricultural Economics, 17(3), 247-255.
* Fritzche, D., and Cotter, R. (1990). Guidelines for Administering Business Games, in Guide to Business Gaming and Experiential Learning, edited by Gentry, J., ABSEL, Nichols/GP Publishing, East Brunswick, 74-89.
* Gentry, J., ed., (1990), Guide to Business Gaming and Experiential Learning, ABSEL, Nichols/GP Publishing, East Brunswick.
* Greenlaw, S.A. (1999). Using groupware to enhance teaching and learning in undergraduate economics, Journal of Economic Education, 30(winter), 33-42.
* Lean, J., Moizer, M., Towler, C. A. (2006). Active Learning in Higher Education, Journal of Simulation and games, 7(3), 227-242.
* McHaney, R., White, D., Heilman, G. E. (2002). Simulation Project Success and Failure: Survey Findings, Simulation &amp; Gaming, 33(1), 49-66.
* Mills, B.J. and Cottell, P.G. (1998), Cooperative learning for higher education faculty, Phoenix, Ariz.: Oryx Press
* Simkins, S.P. (1999), Promoting active-student learning using the World Wide Web in economics courses, Journal of Economic Education, 30(Summer), 278-91.

==External links==
* [http://rfe.org/showCat.php?cat_id=95 American Economic Association (Resources for Economists) list of tutorials and exercises]
* [http://economicsnetwork.ac.uk/themes/games.htm Classroom Experiments &amp; Games: a guide by the Economics Network (UK)]
* {{cite web|url=http://absel.org/Packages/packages.html|title=Association of Business Simulations and Experiential Learning (ABSEL) list of games|archiveurl=https://web.archive.org/web/20100504171320/http://www.absel.org/Packages/packages.html|archivedate=19 March 2015}}

[[Category:Game theory]]
[[Category:Pedagogy]]
[[Category:Business education]]
[[Category:Economics education]]
[[Category:Simulation]]</text>
      <sha1>qmeu5jjszki2udzd3m8wuso4m2g2px5</sha1>
    </revision>
  </page>
  <page>
    <title>Steric number</title>
    <ns>0</ns>
    <id>13633935</id>
    <revision>
      <id>817479244</id>
      <parentid>794884388</parentid>
      <timestamp>2017-12-28T17:32:31Z</timestamp>
      <contributor>
        <username>Alex Nico</username>
        <id>13704595</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1260">[[Image:Sulfur-tetrafluoride-2D-dimensions.png|thumb|Sulfur tetrafluoride has a steric number of 5.]]
The '''steric number''' of a molecule is the number of atoms bonded to the central atom of a molecule (σ sigma bond) plus the number of [[lone pairs]] on the central atom.  It is often used in [[VSEPR theory]] (valence shell electron-pair repulsion) in order to determine the particular shape, or [[molecular geometry]], that will be formed.

==Steric number in VSEPR==
Calculating the steric number of a molecule's central atom is a vital step in predicting its geometry by VSEPR theory. On the molecule [[Sulfur tetrafluoride|SF&lt;sub&gt;4&lt;/sub&gt;]], for example, the central sulfur atom has four ligands about it, calculated by considering sulfur's [[coordination number]]. In addition to the four ligands, sulfur also has one remaining lone pair. Thus, the steric number is 5. The central atom's steric number together with the number of lone pairs allows anyone to predict the geometry of that central atom, using the [[VSEPR Theory#AXE method|table of molecular geometries]] for the VSEPR theory.

==See also==
*[[AXE method]]
*[[Stereochemistry]]

== References ==
{{Reflist}}
[[Category:Stereochemistry]]
[[Category:Molecular geometry]]

{{chemistry-stub}}</text>
      <sha1>d17zgktp7hnp44n4vd8qtpmye2vnszo</sha1>
    </revision>
  </page>
  <page>
    <title>Stirling numbers of the second kind</title>
    <ns>0</ns>
    <id>2229292</id>
    <revision>
      <id>868943090</id>
      <parentid>868942941</parentid>
      <timestamp>2018-11-15T11:37:53Z</timestamp>
      <contributor>
        <username>Cedar101</username>
        <id>374440</id>
      </contributor>
      <minor/>
      <comment>/* Simple identities */ {{math}}, {{sup}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="20859">[[File:Set partitions 4; Hasse; circles.svg|thumb|right|300px|The [[Bell number|15]] partitions of a 4-element set&lt;br&gt;ordered in a [[Hasse diagram]]&lt;br&gt;&lt;br&gt;There are ''S''(4,1),...,''S''(4,4) = 1,7,6,1 partitions&lt;br&gt;containing 1,2,3,4 sets.]]
In [[mathematics]], particularly in [[combinatorics]], a '''Stirling number of the second kind''' (or Stirling partition number) is the number of ways to [[Partition of a set|partition a set]] of ''n'' objects into ''k'' non-empty subsets and is denoted by &lt;math&gt;S(n,k)&lt;/math&gt; or &lt;math&gt;\textstyle \lbrace{n\atop k}\rbrace&lt;/math&gt;.&lt;ref&gt;Ronald L. Graham, Donald E. Knuth, Oren Patashnik (1988) ''[[Concrete Mathematics]]'', Addison–Wesley, Reading MA. {{isbn|0-201-14236-8}}, p.&amp;nbsp;244.&lt;/ref&gt; Stirling numbers of the second kind occur in the field of  [[mathematics]] called [[combinatorics]] and the study of [[partition (number theory)|partitions]].

Stirling numbers of the second kind are one of two kinds of [[Stirling number]]s, the other kind being called [[Stirling numbers of the first kind]] (or Stirling cycle numbers). Mutually inverse (finite or infinite) [[Triangular matrix|triangular matrices]] can be formed from the Stirling numbers of each kind according to the parameters ''n'', ''k''.

==Definition==
The Stirling numbers of the second kind, written &lt;math&gt;S(n,k)&lt;/math&gt; or &lt;math&gt;\lbrace\textstyle{n\atop k}\rbrace&lt;/math&gt; or with other notations, count the number of ways to [[partition of a set|partition]] a [[Set (mathematics)|set]] of &lt;math&gt;n&lt;/math&gt; labelled objects into &lt;math&gt;k&lt;/math&gt; nonempty unlabelled subsets. Equivalently, they count the number of different [[equivalence relation]]s with precisely &lt;math&gt;k&lt;/math&gt; equivalence classes that can be defined on an &lt;math&gt;n&lt;/math&gt; element set. In fact, there is a [[bijection]] between the set of partitions and the set of equivalence relations on a given set. Obviously, 
:&lt;math&gt;\left\{ {n \atop n} \right\} = 1&lt;/math&gt; and for &lt;math&gt;n \geq 1, \left\{ {n \atop 1}\right\} = 1&lt;/math&gt; 
as the only way to partition an ''n''-element set into ''n'' parts is to put each element of the set into its own part, and the only way to partition a nonempty set into one part is to put all of the elements in the same part.
They can be calculated using the following explicit formula:&lt;ref&gt;{{Cite web|url=http://mathworld.wolfram.com/StirlingNumberoftheSecondKind.html|title=Stirling Number of the Second Kind|last=|first=|date=|website=|publisher=|access-date=}}&lt;/ref&gt;
:&lt;math&gt;\left\{ {n \atop k}\right\} = \frac{1}{k!}\sum_{j=0}^{k} (-1)^{k-j} \binom{k}{j} j^n.&lt;/math&gt;

The Stirling numbers of the second kind may also be characterized as the numbers that arise when one expresses powers of an indeterminate ''x'' in terms of the [[falling factorial]]s&lt;ref&gt;Confusingly, the notation that combinatorialists use for ''falling'' factorials coincides with the notation used in [[special function]]s for ''rising'' factorials; see [[Pochhammer symbol]].&lt;/ref&gt;

:&lt;math&gt;(x)_n=x(x-1)(x-2)\cdots(x-n+1) .&lt;/math&gt;

(In particular, (''x'')&lt;sub&gt;0&lt;/sub&gt; = 1 because it is an [[empty product]].)  In particular, one has

:&lt;math&gt;\sum_{k=0}^n \left\{ {n \atop k} \right\}(x)_k=x^n.&lt;/math&gt;

==Notation==
Various notations have been used for Stirling numbers of the second kind.  The brace notation &lt;math&gt;\textstyle \lbrace{n\atop k}\rbrace&lt;/math&gt;  was used by Imanuel Marx and Antonio Salmeri in 1962 for variants of these numbers.&lt;ref&gt;Transformation of Series by a Variant of Stirling's Numbers, Imanuel Marx, ''The American Mathematical Monthly'' '''69''', #6 (June–July 1962), pp. 530–532, {{JSTOR|2311194}}.&lt;/ref&gt;&lt;ref&gt;Antonio Salmeri, Introduzione alla teoria dei coefficienti fattoriali, ''Giornale di Matematiche di Battaglini '''90''' (1962), pp. 44–54.&lt;/ref&gt; This led [[Donald Ervin Knuth|Knuth]] to use it, as shown here, in the first volume of ''[[The Art of Computer Programming]]'' (1968).&lt;ref name=tnn&gt;{{citation |author-link=Donald Knuth|first=D.E.|last=Knuth| journal = Amer. Math. Monthly | title=Two notes on notation| year=1992 | volume=99 | pages=403-422 | arxiv=math/9205211 | doi=10.2307/2325085 | jstor= 2325085}}&lt;/ref&gt;&lt;ref&gt;Donald E. Knuth, ''Fundamental Algorithms'', Reading, Mass.: Addison–Wesley, 1968.&lt;/ref&gt;  However, according to the third edition of ''The Art of Computer Programming'', this notation was also used earlier by [[Jovan Karamata]] in 1935.&lt;ref&gt;p. 66, Donald E. Knuth, ''Fundamental Algorithms'', 3rd ed., Reading, Mass.: Addison–Wesley, 1997.&lt;/ref&gt;&lt;ref&gt;Jovan Karamata, Théorèmes sur la sommabilité exponentielle et d'autres sommabilités s'y rattachant, ''Mathematica'' (Cluj) '''9''' (1935), pp, 164–178.&lt;/ref&gt;  The notation ''S''(''n'', ''k'') was used by [[Richard P. Stanley|Richard Stanley]] in his book ''[[Enumerative Combinatorics]]''.&lt;ref name=tnn /&gt;

==Relation to Bell numbers==
{{main|Bell number}}

Since the Stirling number &lt;math&gt;\left\{ {n \atop k} \right\}&lt;/math&gt; counts set partitions of an ''n''-element set into ''k'' parts, the sum 

:&lt;math&gt;B_n=\sum_{k=0}^n \left\{ {n \atop k} \right\}&lt;/math&gt;

over all values of ''k'' is the total number of partitions of a set with ''n'' members.  This number is known as the ''n''th [[Bell numbers|Bell number]]. 

Analogously, the [[ordered Bell number]]s can be computed from the Stirling numbers of the second kind via
:&lt;math&gt;a_n = \sum_{k=0}^n k! \left\{ {n \atop k}\right\}.&lt;/math&gt;&lt;ref&gt;{{citation
 | last = Sprugnoli | first = Renzo
 | doi = 10.1016/0012-365X(92)00570-H
 | issue = 1-3
 | journal = Discrete Mathematics
 | mr = 1297386
 | pages = 267–290
 | title = Riordan arrays and combinatorial sums
 | volume = 132
 | year = 1994}}&lt;/ref&gt;

==Table of values==
Below is a [[triangular array]] of values for the Stirling numbers of the second kind {{OEIS|A008277}}:
{| cellspacing="0" cellpadding="5" style="text-align:right;" class="wikitable"
|-
| {{diagonal split header|'''n'''|''k''}}
! {{rh|align=right}} | 0
! {{rh|align=right}} | 1
! {{rh|align=right}} | 2
! {{rh|align=right}} | 3
! {{rh|align=right}} | 4
! {{rh|align=right}} | 5
! {{rh|align=right}} | 6
! {{rh|align=right}} | 7
! {{rh|align=right}} | 8
! {{rh|align=right}} | 9
! {{rh|align=right}} | 10
|-
! {{rh|align=right}} | 0
| 1
|-
! {{rh|align=right}} | 1
| 0
| 1
|-
! {{rh|align=right}} | 2
| 0
| 1
| 1
|-
! {{rh|align=right}} | 3
| 0
| 1
| 3
| 1
|-
! {{rh|align=right}} | 4
| 0
| 1
| 7
| 6
| 1
|-
! {{rh|align=right}} | 5
| 0
| 1
| 15
| 25
| 10
| 1
|-
! {{rh|align=right}} | 6
| 0
| 1
| 31
| 90
| 65
| 15
| 1
|-
! {{rh|align=right}} | 7
| 0
| 1
| 63
| 301
| 350
| 140
| 21
| 1
|-
! {{rh|align=right}} | 8
| 0
| 1
| 127
| 966
| 1701
| 1050
| 266
| 28
| 1
|-
! {{rh|align=right}} | 9
| 0
| 1
| 255
| 3025
| 7770
| 6951
| 2646
| 462
| 36
| 1
|-
! {{rh|align=right}} | 10
| 0
| 1
| 511
| 9330
| 34105
| 42525
| 22827
| 5880
| 750
| 45
| 1
|-
 |}

As with the [[binomial coefficients]], this table could be extended to&amp;nbsp;{{math|''k'' &gt; ''n''}}, but those entries would all be&amp;nbsp;0.

== Properties ==

===Recurrence relation===
Stirling numbers of the second kind obey the recurrence relation

:&lt;math&gt;\left\{{n+1\atop k}\right\} = k \left\{{ n \atop k }\right\} + \left\{{n\atop k-1}\right\}
&lt;/math&gt;
for ''k'' &gt; 0 with initial conditions
:&lt;math&gt;\left\{{ 0 \atop 0 }\right\} = 1
\quad \mbox{ and } \quad
\left\{{ n \atop 0 }\right\} = \left\{{ 0 \atop n }\right\} = 0&lt;/math&gt;
for ''n'' &gt; 0.

For instance, the number 25 in column ''k''=3 and row ''n''=5 is given by 25=7+(3×6), where 7 is the number above and to the left of 25, 6 is the number above 25 and 3 is the column containing the 6.

To understand this recurrence, observe that a partition of the {{tmath|n+1}} objects into ''k'' nonempty subsets either contains the {{tmath|(n+1)}}-th object as a singleton  or it does not. The number of ways that the singleton is one of the subsets is given by

:&lt;math&gt;\left\{{ n \atop k-1 }\right\}&lt;/math&gt;

since we must partition the remaining {{mvar|n}} objects into the available {{tmath|k-1}} subsets. In the other case the {{tmath|(n+1)}}-th object belongs to a subset containing other objects. The number of ways is given by

:&lt;math&gt;k \left\{{ n \atop k }\right\}&lt;/math&gt;

since we partition all objects other than the {{tmath|(n+1)}}-th into ''k'' subsets, and then we are left with ''k'' choices for inserting object {{tmath|n+1}}. Summing these two values gives the desired result.

Some more recurrences are as follows:

:&lt;math&gt;\left\{{n+1\atop k+1}\right\} = \sum_{j=k}^n {n \choose j} \left\{{ j \atop k }\right\}, &lt;/math&gt;

:&lt;math&gt;\left\{{n+1\atop k+1}\right\} = \sum_{j=k}^n (k+1)^{n-j} \left\{{j \atop k}\right\} , &lt;/math&gt;

:&lt;math&gt;\left\{{n+k+1 \atop k}\right\} = \sum_{j=0}^k j \left\{{ n+j \atop j }\right\}. &lt;/math&gt;

===Lower and upper bounds===
If &lt;math&gt;n \geq 2&lt;/math&gt; and &lt;math&gt;1 \leq k \leq n-1&lt;/math&gt;, then

:&lt;math&gt;L(n,k) \leq \left\{{n \atop k}\right\} \leq U(n,k)&lt;/math&gt;

where 
:&lt;math&gt;L(n,k) = \frac{1}{2}(k^2+k+2)k^{n-k-1}-1&lt;/math&gt; 
and 
:&lt;math&gt;U(n,k) = \frac{1}{2}{n \choose k} k^{n-k}.&lt;/math&gt; &lt;ref name="ac.els-cdn.com"&gt;[http://ac.els-cdn.com/S0021980069800451/1-s2.0-S0021980069800451-main.pdf?_tid=4e52b648-711a-11e2-a90d-00000aab0f26&amp;acdnat=1360237038_c9d4f38f83a536ed624e29e3554016e2 B.C. Rennie, A.J. Dobson. "On Stirling Numbers of the Second Kind"]&lt;/ref&gt;

===Maximum===
For fixed &lt;math&gt;n&lt;/math&gt;, &lt;math&gt;\left\{{n \atop k}\right\}&lt;/math&gt; has a single maximum, which is attained for at most two consecutive values of ''k''. That is, there is an integer &lt;math&gt;K_n&lt;/math&gt; such that

:&lt;math&gt;\left\{{n \atop 1}\right\} &lt; \left\{{n \atop 2}\right\} &lt; \cdots &lt; \left\{{n \atop K_n}\right\},&lt;/math&gt;
:&lt;math&gt;\left\{{n \atop K_n}\right\} \geq \left\{{n \atop K_n+1}\right\} &gt; \cdots &gt; \left\{{n \atop n}\right\}.&lt;/math&gt;

When &lt;math&gt;n&lt;/math&gt; is large

:&lt;math&gt;K_n \sim \frac{n}{\log n},&lt;/math&gt;

and the maximum value of the Stirling number of second kind is
:&lt;math&gt;\log \left\{{n \atop K_n}\right\} = n\log n - n \log\log n - n + O(n \log\log n / \log n).&lt;/math&gt; &lt;ref name="ac.els-cdn.com"/&gt;

===Parity===
[[Image:Stirling numbers of the second kind - Parity.svg|256px|right|thumb|Parity of Stirling numbers of the second kind.]]

The [[Parity (mathematics)|parity]] of a Stirling number of the second kind is equal to the parity of a related [[binomial coefficient]]:
:&lt;math&gt;\left\{ {n\atop k}\right\}\equiv \binom{z}{w}\ \pmod{2},&lt;/math&gt; where &lt;math&gt;z = n - \left\lceil\displaystyle\frac{k + 1}{2}\right\rceil,\ w = \left\lfloor\displaystyle\frac{k - 1}{2}\right\rfloor.&lt;/math&gt;
This relation is specified by mapping ''n'' and ''k'' coordinates onto the [[Sierpinski triangle|Sierpiński triangle]].

More directly, let two sets contain positions of 1's in binary representations of results of respective expressions:

:&lt;math&gt;
\begin{align}
\mathbb{A}:\ \sum_{i\in\mathbb{A}} 2^i &amp;= n-k,\\
\mathbb{B}:\ \sum_{j\in\mathbb{B}} 2^j &amp;= \left\lfloor\dfrac{k - 1}{2}\right\rfloor.\\
\end{align}
&lt;/math&gt;

One can mimic a [[bitwise AND]] operation by intersecting these two sets:

:&lt;math&gt;
\begin{Bmatrix}n\\k\end{Bmatrix}\,\bmod\,2 =
\begin{cases}
 0, &amp; \mathbb{A}\cap\mathbb{B}\ne\empty;\\
 1, &amp; \mathbb{A}\cap\mathbb{B}=\empty;
\end{cases}
&lt;/math&gt;

to obtain the parity of a Stirling number of the second kind in [[Big O notation|''O''(1)]] time. In [[pseudocode]]:

:&lt;math&gt;
\begin{Bmatrix}n\\k\end{Bmatrix}\,\bmod\,2 := \left[\left( \left(n-k\right)\ \And\ \left( \left(k-1\right)\,\mathrm{div}\,2 \right)\right) = 0\right];
&lt;/math&gt;
where &lt;math&gt;
\left[b\right]
&lt;/math&gt; is the [[Iverson bracket]].

===Simple identities===
Some simple identities include

:&lt;math&gt;\left\{ {n \atop n-1}\right\} = \binom{n}{2}. &lt;/math&gt;

This is because dividing ''n'' elements into {{math|''n''&amp;nbsp;&amp;minus;&amp;nbsp;1}} sets necessarily means dividing it into one set of size 2 and {{math|''n''&amp;nbsp;&amp;minus;&amp;nbsp;2}} sets of size 1.  Therefore we need only pick those two elements;

and

:&lt;math&gt;\left\{ {n \atop 2}\right\} = 2^{n-1}-1.&lt;/math&gt;

To see this, first note that there are 2{{sup|''n''}} ''ordered'' pairs of complementary subsets ''A'' and ''B''.  In one case, ''A'' is empty, and in another ''B'' is empty, so {{math|2{{sup|''n''}}&amp;nbsp;&amp;minus;&amp;nbsp;2}} ordered pairs of subsets remain.  Finally, since we want ''unordered'' pairs rather than ''ordered'' pairs we divide this last number by 2, giving the result above.

Another explicit expansion of the recurrence-relation gives identities in the spirit of the above example.

:&lt;math&gt;
\begin{align}
\left\{ {n \atop 2} \right\} &amp; = \frac{ \frac11 (2^{n-1}-1^{n-1}) }{0!} \\[8pt]
\left\{ {n \atop 3} \right\} &amp; = \frac{ \frac11 (3^{n-1}-2^{n-1})- \frac12 (3^{n-1}-1^{n-1}) }{1!} \\[8pt]
\left\{ {n \atop 4} \right\} &amp; = \frac{ \frac11 (4^{n-1}-3^{n-1})- \frac22 (4^{n-1}-2^{n-1}) +  \frac13 (4^{n-1}-1^{n-1})}{2!} \\[8pt]
\left\{ {n \atop 5} \right\} &amp; = \frac{ \frac11 (5^{n-1}-4^{n-1})- \frac32 (5^{n-1}-3^{n-1}) + \frac33 (5^{n-1}-2^{n-1}) -  \frac14 (5^{n-1}-1^{n-1}) }{3!} \\[8pt]
&amp; {}\ \  \vdots
\end{align}
&lt;/math&gt;

===Explicit formula===
The Stirling numbers of the second kind are given by the explicit formula:

:&lt;math&gt;\left\{ {n \atop k} \right\}
=\sum_{j=1}^k (-1)^{k-j} \frac{j^{n-1}}{(j-1)!(k-j)!}
=\frac{1}{k!}\sum_{j=0}^{k}(-1)^{k-j}{k \choose j} j^n
.&lt;/math&gt;

This formula is a special case of the ''k''th [[forward difference]] of the [[monomial]] &lt;math&gt;x^n&lt;/math&gt; evaluated at ''x'' = 0:

:&lt;math&gt; \Delta^k x^n = \sum_{j=0}^{k}(-1)^{k-j}{k \choose j} (x+j)^n.&lt;/math&gt;

Because the [[Bernoulli polynomial]]s may be written in terms of these forward differences, one immediately obtains a relation in the [[Bernoulli number]]s:

:&lt;math&gt;B_m(0)=\sum_{k=0}^m \frac {(-1)^k k!}{k+1} \left\{ {m \atop k} \right\}. &lt;/math&gt;

===Generating functions===
For a fixed integer ''n'', the [[ordinary generating function]] for the Stirling numbers of the second kind &lt;math&gt;\left\{ {n\atop 0} \right\}, \left\{ {n\atop 1} \right\}, \ldots&lt;/math&gt; is given by
:&lt;math&gt; \sum_{k=0}^n \left\{ {n\atop k} \right\} x^k = T_n(x),&lt;/math&gt;
where &lt;math&gt;T_n(x)&lt;/math&gt; are [[Touchard polynomials]].
If one sums the Stirling numbers against the falling factorial instead, one can show the following identities, among others:
:&lt;math&gt; \sum_{k=0}^n \left\{ {n\atop k} \right\} (x)_k = x^n&lt;/math&gt;
and
:&lt;math&gt; \sum_{k=1}^n \left\{ {n+1 \atop k} \right\} (x-1)_{k-1} = x^n.&lt;/math&gt;

For a fixed integer ''k'', the Stirling numbers of the second kind &lt;math&gt;\left\{ {0\atop k} \right\}, \left\{ {1\atop k} \right\}, \ldots&lt;/math&gt; have 
rational ordinary generating function
:&lt;math&gt;\sum_{n=k}^\infty \left\{ {n\atop k} \right\} x^{n - k} = \prod_{r=1}^k \frac{1}{1-rx} = \frac{1}{(k+1)! x^{k + 1} \binom{\frac{1}{x}}{k+1}}&lt;/math&gt;
and have [[exponential generating function]] given by
:&lt;math&gt; \sum_{n=k}^\infty \left\{ {n \atop k}\right\} \frac{x^n}{n!} = \frac{(e^x-1)^k}{k!}.&lt;/math&gt;

A mixed bivariate generating function for the Stirling numbers of the second kind is
:&lt;math&gt; \sum_{0 \leq k \leq n} \left\{ {n \atop k} \right\} \frac{x^n}{n!} y^k = e^{y(e^x-1)}.&lt;/math&gt;

===Asymptotic approximation===
For fixed value of &lt;math&gt;k,&lt;/math&gt; the asymptotic value of the Stirling numbers of the second kind as &lt;math&gt; n\rightarrow \infty &lt;/math&gt; is given by
:&lt;math&gt; \left\{{n \atop k}\right\} \sim \frac{k^n}{k!}.&lt;/math&gt;

On the other side, if &lt;math&gt; k = o(\sqrt{n}) &lt;/math&gt; (where ''o'' denotes the [[little o notation]]) then
:&lt;math&gt; \left\{{n \atop n-k}\right\} \sim \frac{(n-k)^{2k}}{2^k k!} \left( 1 + \frac{1}{3} \frac{2 k^2 + k}{n-k} + \frac{1}{18} \frac{4k^4-k^2-3k}{(n-k)^2} + \cdots \right).&lt;/math&gt;&lt;ref&gt;L. C. Hsu, Note on an Asymptotic Expansion of the nth Difference of Zero, AMS Vol.19 NO.2 1948, pp. 273--277&lt;/ref&gt;

Uniformly valid approximation also exist: for all {{mvar|k}} such that {{math|1 &lt; ''k'' &lt; ''n''}}, one has

:&lt;math&gt; \left\{{n \atop k}\right\} \sim \frac{\sqrt{n-k}}{\sqrt{n (1-G)}\ G^k\ (v-G)^{n-k}} \left(\frac{n-k}{e}\right)^{n-k} \left({n \atop k}\right),&lt;/math&gt;

where &lt;math&gt; G = - W_0(-v e^{-v}), v=n/k &lt;/math&gt;, &lt;math&gt;\ W_0(z)&lt;/math&gt; is main branch of [[Lambert W function]].&lt;ref&gt;W. E. Bleick and Peter C. C. Wang, Asymptotics of Stirling Numbers of the Second Kind, Proceedings of the AMS Vol.42 No.2, 1974.&lt;/ref&gt;&lt;ref&gt;N. M. Temme, Asymptotic Estimates of Stirling Numbers, STUDIES IN APPLIED MATHEMATICS 89:233-243 (1993), Elsevier Science Publishing.&lt;/ref&gt; Relative error is bounded by about &lt;math&gt; 0.066/n &lt;/math&gt;.

==Applications==

===Moments of the Poisson distribution===
If ''X'' is a [[random variable]] with a [[Poisson distribution]] with [[expected value]] λ, then its ''n''th [[moment (mathematics)|moment]] is

:&lt;math&gt;E(X^n)=\sum_{k=1}^n \left\{ {n \atop k} \right\}\lambda^k.&lt;/math&gt;

In particular, the ''n''th moment of the Poisson distribution with expected value 1 is precisely the number of [[partition of a set|partitions of a set]] of size ''n'', i.e., it is the ''n''th [[Bell number]] (this fact is [[Dobiński's formula]]).

===Moments of fixed points of random permutations===
Let the random variable ''X'' be the number of fixed points of a [[discrete uniform distribution|uniformly distributed]] [[random permutation]] of a finite set of size ''m''.  Then the ''n''th moment of ''X'' is

:&lt;math&gt;E(X^n) = \sum_{k=1}^m \left\{ {n \atop k} \right\}.&lt;/math&gt;

'''Note:''' The upper bound of summation is ''m'', not ''n''.

In other words, the ''n''th moment of this [[probability distribution]] is the number of partitions of a set of size ''n'' into no more than ''m'' parts.
This is proved in the article on [[Random permutation statistics#Moments of fixed points|random permutation statistics]], although the notation is a bit different.

===Rhyming schemes===
The Stirling numbers of the second kind can represent the total number of [[rhyme scheme]]s for a poem of ''n'' lines. &lt;math&gt;S(n,k)&lt;/math&gt; gives the number of possible rhyming schemes for ''n'' lines using ''k'' unique rhyming syllables. As an example, for a poem of 3 lines, there is 1 rhyme scheme using just one rhyme (aaa), 3 rhyme schemes using two rhymes (aab, aba, abb), and 1 rhyme scheme using three rhymes (abc).

==Variants==

===Associated Stirling numbers of the second kind===
An ''r''-associated Stirling number of the second kind is the number of ways to partition a set of ''n'' objects into ''k'' subsets, with each subset containing at least ''r'' elements.&lt;ref&gt;L. Comtet, ''Advanced Combinatorics'', Reidel, 1974, p. 222.&lt;/ref&gt; It is denoted by &lt;math&gt;S_r(n,k)&lt;/math&gt; and obeys the recurrence relation

:&lt;math&gt;S_r(n+1, k)=k\ S_r(n, k)+\binom{n}{r-1}S_r(n-r+1, k-1)&lt;/math&gt;

The 2-associated numbers {{OEIS|A008299}} appear elsewhere as "Ward numbers" and as the magnitudes of the coefficients of [[Mahler polynomial]]s.

===Reduced Stirling numbers of the second kind===
Denote the ''n'' objects to partition by the integers 1, 2, ..., ''n''.  Define the reduced Stirling numbers of the second kind, denoted &lt;math&gt;S^d(n, k)&lt;/math&gt;, to be the number of ways to partition the integers 1, 2, ..., ''n'' into ''k'' nonempty subsets such that all elements in each subset have pairwise distance at least ''d''.  That is, for any integers ''i'' and ''j'' in a given subset, it is required that &lt;math&gt;|i-j| \geq d&lt;/math&gt;. It has been shown that these numbers satisfy

:&lt;math&gt;S^d(n, k) = S(n-d+1, k-d+1), n \geq k \geq d&lt;/math&gt;

(hence the name "reduced").&lt;ref&gt;A. Mohr and T.D. Porter, ''[http://www.austinmohr.com/work/files/stirling.pdf Applications of Chromatic Polynomials Involving Stirling Numbers]'', Journal of Combinatorial Mathematics and Combinatorial Computing '''70''' (2009), 57–64.&lt;/ref&gt; Observe (both by definition and by the reduction formula), that &lt;math&gt;S^1(n, k) = S(n, k)&lt;/math&gt;, the familiar Stirling numbers of the second kind.

==See also==
* [[Bell number]] &amp;ndash; the number of partitions of a set with ''n'' members
* [[Stirling numbers of the first kind]]
* [[Stirling polynomials]]
* [[Twelvefold way]]
* [[File:Wikiversity-logo-en.svg|20px]] [[v:Partition related number triangles|Partition related number triangles]]

==References==

{{reflist|colwidth=30em}}

* {{cite journal| author=Khristo N. Boyadzhiev |title=Close encounters with the Stirling numbers of the second kind |year=2012 |journal=Mathematics Magazine |volume=85 |number=4 |pages=252–266}}
* {{planetmath reference |id=2805|title=Stirling numbers of the second kind, S(n,k)}}.
* {{MathWorld |urlname=StirlingNumberoftheSecondKind |title=Stirling Number of the Second Kind}}
* [http://austinmohr.com/home/?page_id=431 Calculator for Stirling Numbers of the Second Kind]
* [http://dlmf.nist.gov/26.8#vii Set Partitions: Stirling Numbers]
* {{cite book |author=Jack van der Elsen |title=Black and white transformations |publisher=Maastricht |year=2005 |isbn=90-423-0263-1}}

[[Category:Permutations]]
[[Category:Factorial and binomial topics]]
[[Category:Triangles of numbers]]

[[pl:Liczby Stirlinga#Liczby Stirlinga II rodzaju]]</text>
      <sha1>e88f6cqxodun6yl6jqo3r2s1t849x1k</sha1>
    </revision>
  </page>
  <page>
    <title>Ulrica Wilson</title>
    <ns>0</ns>
    <id>58688880</id>
    <revision>
      <id>867992002</id>
      <parentid>866639953</parentid>
      <timestamp>2018-11-09T08:20:04Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>move ethnicity out of lead as insufficiently central to her notability to be so prominent in the article</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3980">'''Ulrica Wilson''' is a mathematician specializing in the theory of [[noncommutative ring]]s and in the [[combinatorics]] of [[Matrix (mathematics)|matrices]].{{r|morehouse}} She is an associate professor at [[Morehouse College]], associate director of diversity and outreach at the [[Institute for Computational and Experimental Research in Mathematics]] (ICERM),{{r|mgb|morehouse}} and vice president of the [[National Association of Mathematicians]].{{r|vpnam}}

==Education and career==
Wilson is [[African-American]],{{r|mgb}} and originally from Massachusetts, but grew up in [[Birmingham, Alabama]].{{r|mgb}}
She is a 1992 graduate of [[Spelman College]],{{r|spelman}}
and completed her Ph.D. at [[Emory University]] in 2004. Her dissertation, ''Cyclicity of Division Algebras over an Arithmetically Nice Field'', was supervised by Eric Brussel.{{r|mgp}}

After two stints as a postdoctoral researcher,{{r|mgb}} she joined the Morehouse College faculty in 2007, and became associate director at ICERM in 2013.{{r|morehouse}}

==Recognition==
Wilson was the Morehouse College Vulcan Teaching Excellence Award winner for 2016–2017.{{r|vulcan}}
In 2018, she won the [[Presidential Award for Excellence in Science, Mathematics, and Engineering Mentoring]].{{r|paesmem}}
She was included in the 2019 class of fellows of the [[Association for Women in Mathematics]] "for her many years of supporting the professional development of women in their pursuit of graduate degrees in mathematics, most visibly through mentoring, teaching and program administration within the EDGE Program, and as associate director of diversity and outreach at ICERM".{{r|fawm}}

==References==
{{reflist|refs=

&lt;ref name=fawm&gt;{{citation|url=https://sites.google.com/site/awmmath/awm-fellows|title=2019 Class of AWM Fellows|publisher=[[Association for Women in Mathematics]]|accessdate=2018-10-07}}&lt;/ref&gt;

&lt;ref name=mgb&gt;{{citation|url=http://mathematicallygiftedandblack.com/honorees/ulrica-wilson/|title=Ulrica Wilson|work=Mathematically Gifted and Black: Black History Month 2017 Honoree|publisher=The Network of Minorities in Mathematical Sciences|accessdate=2018-10-07}}&lt;/ref&gt;

&lt;ref name=mgp&gt;{{mathgenealogy|id=32744}}&lt;/ref&gt;

&lt;ref name=morehouse&gt;{{citation|url=http://www.morehouse.edu/academics/math/uwilson.html|title=Ulrica Wilson: Associate Professor of Mathematics|publisher=[[Morehouse College]]|accessdate=2018-10-07}}&lt;/ref&gt;

&lt;ref name=paesmem&gt;{{citation|url=http://www.morehouse.edu/newscenter/morehouseprofessorswinwhitehousenationalsciencefoundationawards.html|title=Morehouse Professors Win White House/National Science Foundation Awards|date=July 2, 2018|publisher=[[Morehouse College]]|accessdate=2018-10-07}}&lt;/ref&gt;

&lt;ref name=spelman&gt;{{citation|title=A Century of Mathematical Excellence at Spelman College|first=Colm|last=Mulcahy|authorlink=Colm Mulcahy|year=2017|series=Spelman College Faculty Publications|volume=13|url=http://digitalcommons.auctr.edu/scpubs/13}}&lt;/ref&gt;

&lt;ref name=vpnam&gt;{{citation|url=https://www.nam-math.org/governance.html|title=Standing committees|publisher=[[National Association of Mathematicians]]|accessdate=2018-10-07}}&lt;/ref&gt;

&lt;ref name=vulcan&gt;{{citation|url=http://www.morehouse.edu/newscenter/mathematicsprofessorulricawilsonnamedmorehousecolleges2016-2017vulcanteachingexcellenceawardwinner.html|title=Mathematics Professor Ulrica Wilson Named Morehouse College’s 2016–2017 Vulcan Teaching Excellence Award Winner|date=May 26, 2017|publisher=[[Morehouse College]]|accessdate=2018-10-07}}&lt;/ref&gt;

}}

{{Authority control}}
{{DEFAULTSORT:Wilson, Ulrica}}
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]
[[Category:21st-century American mathematicians]]
[[Category:American women mathematicians]]
[[Category:African-American mathematicians]]
[[Category:Spelman College alumni]]
[[Category:Emory University alumni]]
[[Category:Morehouse College faculty]]
[[Category:Fellows of the Association for Women in Mathematics]]</text>
      <sha1>m1de003bwwsfa843mpzrjpg6hzi9lfv</sha1>
    </revision>
  </page>
  <page>
    <title>United States Army Training and Doctrine Command Analysis Center</title>
    <ns>0</ns>
    <id>2310296</id>
    <revision>
      <id>821129249</id>
      <parentid>613572892</parentid>
      <timestamp>2018-01-18T15:50:52Z</timestamp>
      <contributor>
        <ip>141.107.138.181</ip>
      </contributor>
      <comment>/* TRAC organization */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6469">{{Use dmy dates|date=February 2013}}
[[Image:TRADOC Analysis Center.png|right|200px]]
The '''United States Army Training and Doctrine Command Analysis Center (TRAC)''' is an analysis agency of the [[United States Army]]. TRAC conducts research on potential military operations worldwide to inform decisions about the most challenging issues facing the Army and the [[United States Department of Defense|Department of Defense]] (DoD). TRAC relies upon the intellectual capital of a highly skilled workforce of military and civilian personnel to execute its mission. 

TRAC conducts [[operations research]] (OR) on a wide range of military topics, some contemporary but most often set 5 to 15 years in the future. How should Army units be organized? What new systems should be procured? How should soldiers and commanders be trained? What are the costs and benefits of competing options? What are the potential risks and rewards of a planned military course of action? TRAC directly supports the mission of the Army's Training and Doctrine Command ([[United States Army Training and Doctrine Command|TRADOC]]), to develop future concepts and requirements while also serving the decision needs of many military clients.

== TRAC mission statement ==
To produce relevant and credible operations analysis to inform decisions. TRAC serves many clients and has many stakeholders, but has only one shareholder: the American Soldier.

== TRAC organization ==
TRAC is led by a civilian SES director, subordinate to the Commanding General of the US Army Training and Doctrine Command, and comprises four centers:&lt;ref&gt;TRADOC Regulation 10-5-7, Organization and Functions U.S. Army TRADOC Analysis Center, 29 December 2005&lt;/ref&gt;
* TRAC-Fort Leavenworth (TRAC-FLVN), led by a civilian SES director, is co-located with TRAC headquarters at [[Fort Leavenworth, Kansas]] (i.e. US Army Combined Arms Center (CAC)) and has traditionally conducted analysis at the operational level (i.e. Corps and Division).
* TRAC-White Sands Missile Range (TRAC-WSMR), led by a civilian SES director, is located at [[White Sands Missile Range]] in [[New Mexico]] and has traditionally conducted analysis at the tactical level (i.e. Brigade and below).
* TRAC-Lee, led by a civilian GS-15, is located at [[Fort Lee, VA]] and has traditionally conducted analysis in the area of Logistics.
* TRAC-Monterey, led by a Lieutenant Colonel (LTC), is co-located with the [[Naval Postgraduate School]] (NPS) in [[Monterey, CA]] and has traditionally utilized the resources of NPS to conduct research into new models and methodologies.

Each center director is subordinate to the TRAC director.

TRAC headquarters has two components:
* The director's staff element comprising the deputy director, an O-6 colonel, and administrative assistants
* The Program and Resources Directorate (PRD), in coordination with center directors, oversees the day-to-day operations of all TRAC elements
*

== The discipline of OR ==
The discipline of [[operations research]] is built upon the collaboration of interdisciplinary team members who have mutually supporting knowledge, skills and experiences pertinent to the study problem. The TRAC building blocks of military operations analysis are future scenarios; leading edge models and simulations; realistic data about systems, forces, and behavior; and skilled operations analysts. Leading a core team of analysts, a TRAC Study Director may receive support from other TRADOC and Army agencies, and from other government agencies and industry as well. TRAC adheres to the proven principles of scientific inquiry and applies the problem solving model to perform its analysis.

=== The TRAC program ===
The TRAC program of operations research and analysis is forward-looking and addresses a wide range of military topics. The analysis is conducted within a joint framework of combined arms operations across a full spectrum of missions and environments. TRAC leads TRADOC's major studies of new warfighting operations and organization (O&amp;O) concepts and requirements. TRAC leads the Army's analysis of [[Advanced Warfighting Experiments]] (AWEs), and the Army's [[Analysis of Alternatives]] (AoA). The analysis topics span doctrine, training, leader development, organization, materiel, and soldier support.

Scenarios are used by the U.S. Army for education, training and force development. Director, TRAC is the TRADOC executive agent for development of scenarios for use in studies and analysis.&lt;ref&gt;TRADOC regulation TR 71-4. United States Army Training And Doctrine Command Standard Scenarios For Capability Developments, 23 September 2008.&lt;/ref&gt; TRAC develops scenarios of potential military operations set in the future for use in modeling and analysis. TRAC relies upon input and assistance from many Army and DoD agencies, other Services and the Combatant Commanders to develop and apply a family of scenarios depicting joint operations of corps and divisions, and brigades and battalions. The family of scenarios undergoes continual review and change in anticipation of emerging threats and new operational environments around the world based on intelligence estimates.

=== Tools ===
Military operations are highly complex processes and typically must be modeled in order to be analyzed. The analytic tools may take the form of:
* table-top map games,
* [[human-in-the-loop]] (HITL),
* (HITL) simulations and simulators,
* closed-form Models &amp; Simulations (M&amp;S), and
* controlled field experiments.
TRAC develops and maintains a class of warfighting M&amp;S referred to as force-on-force, ranging from individual objects (e.g., soldier, weapon, terrain feature) to aggregated objects (e.g., battalions) at corps level. TRAC M&amp;S represent the Army's de facto standards for force-on-force M&amp;S and are widely used by military, industry and allies. TRAC is a significant contributor to advanced M&amp;S research and improved modeling methodologies in the military.

== References ==
{{Reflist}}

==External links==
{{commons category|United States Army Training and Doctrine Command Analysis Center}}
* [http://www.trac.army.mil U.S. Army TRADOC Analysis Center (TRAC)]
* [https://www.youtube.com/watch?v=hASCwXtJ3lM U.S. Army TRADOC Analysis Center (TRAC) Video]
* [http://www.tradoc.army.mil/ U.S Army Training and Doctrine Command (TRADOC)]

{{TRADOC}}

[[Category:United States Army Training and Doctrine Command]]
[[Category:Operations research]]
[[Category:Fort Leavenworth]]</text>
      <sha1>bu41wklqnrtt5fzql93rdp1tbn0iq0q</sha1>
    </revision>
  </page>
  <page>
    <title>Étale homotopy type</title>
    <ns>0</ns>
    <id>39583234</id>
    <revision>
      <id>823055568</id>
      <parentid>656645640</parentid>
      <timestamp>2018-01-30T00:41:29Z</timestamp>
      <contributor>
        <username>BTotaro</username>
        <id>13888337</id>
      </contributor>
      <minor/>
      <comment>Links added</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1866">In mathematics, especially in [[algebraic geometry]], the '''étale homotopy type''' is an analogue of the [[homotopy type]] of [[topological space]]s for [[algebraic variety|algebraic varieties]].

Roughly speaking, for a variety or scheme ''X'', the idea is to consider étale coverings &lt;math&gt;U \rightarrow X&lt;/math&gt; and to replace each [[connected component (topology)|connected component]] of ''U'' and the higher "intersections", i.e., [[fiber product of schemes|fiber product]]s, &lt;math&gt;U_n := U \times_X U \times_X \dots \times_X U&lt;/math&gt; (''n''+1 copies of ''U'', &lt;math&gt;n \geq 0&lt;/math&gt;) by a single point. This gives a [[simplicial set]] which captures some information related to ''X'' and the étale topology of it.

Slightly more precisely, it is in general necessary to work with [[etale morphism|étale]] [[hypercover]]s &lt;math&gt;(U_n)_{n \geq 0}&lt;/math&gt; instead of the above simplicial scheme determined by a usual étale cover. Taking finer and finer hypercoverings (which is technically accomplished by working with the [[pro-object]] in simplicial sets determined by taking all hypercoverings), the resulting object is the étale homotopy type of ''X''. Similarly to classical topology, it is able to recover much of the usual data related to the étale topology, in particular the [[étale fundamental group]] of the scheme and the [[étale cohomology]] of locally constant étale [[sheaf (mathematics)|sheaves]].

== References ==

* {{cite book|last=Artin|first=Michael|title=Etale homotopy|year=1969|publisher=Springer|author2=Mazur, Barry}}
*  {{cite book|last=Friedlander|first=Eric|title=Étale homotopy of simplicial schemes|year=1982|publisher=Annals of Mathematics Studies, PUP}}

== External links ==
*http://ncatlab.org/nlab/show/étale+homotopy

{{DEFAULTSORT:Etale homotopy type}}
[[Category:Homotopy theory]]
[[Category:Algebraic geometry]]</text>
      <sha1>3jhkc6izr4k3q8hmvilses4fug4aj1a</sha1>
    </revision>
  </page>
</mediawiki>
