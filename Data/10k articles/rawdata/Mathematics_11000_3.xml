<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>234 (number)</title>
    <ns>0</ns>
    <id>5560456</id>
    <revision>
      <id>832584310</id>
      <parentid>721023804</parentid>
      <timestamp>2018-03-26T21:16:52Z</timestamp>
      <contributor>
        <username>DePiep</username>
        <id>199625</id>
      </contributor>
      <minor/>
      <comment>save text from invisible. replace SloanesRef: use {{Cite OEIS}} (via [[WP:JWB]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="617">{{Infobox number
| number = 234
| divisor = 2, 3, 6, 9, 13, 18, 26, 39, 78, 117
}}
'''234''' ('''two hundred [and] thirty-four''') is the [[integer]] following [[233 (number)|233]] and preceding [[235 (number)|235]].

234 is a [[practical number]].&lt;ref&gt;{{Cite OEIS|A005153|name=Practical numbers}}&lt;/ref&gt;
There are 234 ways of grouping six children into rings of at least two children with one child at the center of each ring.&lt;ref&gt;{{Cite OEIS|A066165|name=Variant of Stanley's children's game}}&lt;/ref&gt;

==References==
{{reflist|30em}}
{{Integers|2}}

{{DEFAULTSORT:234 (Number)}}
[[Category:Integers]]

{{Number-stub}}</text>
      <sha1>tp58an5ug62x4kr1482pc1yo1ll9944</sha1>
    </revision>
  </page>
  <page>
    <title>Affix grammar</title>
    <ns>0</ns>
    <id>16110989</id>
    <revision>
      <id>821560669</id>
      <parentid>756510113</parentid>
      <timestamp>2018-01-21T06:41:56Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <comment>adding a reference</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5181">{{refimprove|date=February 2009}}
An '''affix grammar''' is a kind of [[formal grammar]]; it is used to describe the [[Syntax (programming languages)|syntax]] of languages, mainly [[computer language]]s, using an approach based on how natural language is typically described.&lt;ref&gt;Koster, Cornelis HA. "[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.53.5264&amp;rep=rep1&amp;type=pdf Affix grammars for natural languages]." Attribute Grammars, Applications and Systems. Springer, Berlin, Heidelberg, 1991.&lt;/ref&gt;

The grammatical rules of an affix grammar are those of a [[context-free grammar]], except that certain parts in the nonterminals (the [[affix]]es) are used as arguments.  If the same affix occurs multiple times in a rule, its value must [[agreement (linguistics)|agree]], i.e. it must be the same everywhere.  In some types of affix grammar, more complex relationships between affix values are possible.

== Example ==

We can describe an extremely simple fragment of English in the following manner:

: ''Sentence'' → ''Subject'' ''Predicate''
: ''Subject'' → ''Noun''
: ''Predicate'' → ''Verb'' ''Object''
: ''Object'' → ''Noun''
: 
: ''Noun'' → John
: ''Noun'' → Mary
: ''Noun'' → children
: ''Noun'' → parents
: 
: ''Verb'' → like
: ''Verb'' → likes
: ''Verb'' → help
: ''Verb'' → helps

This [[context-free grammar]] describes simple sentences such as

: John likes children
: Mary helps John
: children help parents
: parents like John

With more nouns and verbs, and more rules to introduce other parts of speech, a large range of English sentences can be described; so this is a promising approach for describing the syntax of English.

However, the given grammar also describes sentences such as

: John like children
: children helps parents

These sentences are wrong: in English, subject and verb have a [[grammatical number]], which must agree.

An affix grammar can express this directly:
: ''Sentence'' &amp;rarr; ''Subject'' + ''number  Predicate''+''number''
: ''Subject'' + ''number'' &amp;rarr; ''Noun'' + ''number''
: ''Predicate'' + ''number'' &amp;rarr; ''Verb'' + ''number  Object''
: ''Object'' &amp;rarr; ''Noun'' + ''number''

: ''Noun'' + ''singular'' &amp;rarr; John
: ''Noun'' + ''singular'' &amp;rarr; Mary
: ''Noun'' + ''plural'' &amp;rarr; children
: ''Noun'' + ''plural'' &amp;rarr; parents
: 
: ''Verb'' + ''singular'' &amp;rarr; likes
: ''Verb'' + ''plural'' &amp;rarr; like
: ''Verb'' + ''singular'' &amp;rarr; helps
: ''Verb'' + ''plural'' &amp;rarr; help

This grammar only describes correct English sentences, although it could be argued that
: John likes John
is still incorrect and should instead read
: John likes himself

This, too, can be incorporated using affixes, if the means of describing the relationships between different affix values are powerful enough. As remarked above, these means depend on the type of affix grammar chosen.

== Types ==

In the simplest type of affix grammar, affixes can only take values from a finite domain, and affix values can only be related through agreement, as in the example.
Applied in this way, affixes increase compactness of grammars, but do not add expressive power.

Another approach is to allow affixes to take arbitrary strings as values and allow   concatenations of affixes to be used in rules.  The ranges of allowable values for affixes can be described with context-free grammar rules.  This produces the formalism of [[two-level grammar]]s, also known as ''[[Van Wijngaarden grammar]]s'' or ''2VW'' grammars.  These have been successfully used to describe complicated languages, in particular, the syntax of the [[Algol 68]] [[programming language]].  However, it turns out that, even though affix values can only be manipulated with string concatenation, this formalism is [[Turing complete]]; hence, even the most basic questions about the language described by an arbitrary 2VW grammar are [[Undecidable language|undecidable]] in general.

[[Extended Affix Grammar]]s, developed in the 1980s, are a more restricted version of the same idea.  They were mainly applied to describe the grammar of natural language, e.g. English.

Another possibility is to allow the values of affixes to be computed by code written in some programming language.  Two basic approaches have been used:
* In [[attribute grammar]]s, the affixes (called attributes) can take values from arbitrary domains (e.g. integer or real numbers, complex data structures) and arbitrary functions can be specified, written in a language of choice, to describe how affix values in rules are derived from each other.
* In CDL (the [[Compiler Description Language]]) and its successor [[CDL2]], developed in the 1970s, fragments of source code (usually in [[assembly language]]) can be used in rules instead of normal right-hand sides, allowing primitives for input scanning and affix value computations to be expressed directly.  Designed as a basis for practical [[compiler]] construction, this approach was used to write compilers, and other software, e.g. a [[text editor]].

== References ==

{{reflist}}

[[Category:Formal languages]]
[[Category:Compiler construction]]
[[Category:Syntax]]
[[Category:Grammar frameworks]]</text>
      <sha1>48prp54kbmzrgjsfkl5bvobbhl2qxu6</sha1>
    </revision>
  </page>
  <page>
    <title>Akra–Bazzi method</title>
    <ns>0</ns>
    <id>230982</id>
    <revision>
      <id>866677820</id>
      <parentid>835625716</parentid>
      <timestamp>2018-10-31T20:53:10Z</timestamp>
      <contributor>
        <ip>138.51.156.57</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5123">{{no footnotes|date=February 2013}}
In [[computer science]], the '''Akra–Bazzi method''', or '''Akra–Bazzi theorem''', is used to analyze the asymptotic behavior of the mathematical [[recurrence relation|recurrences]] that appear in the analysis of [[divide and conquer algorithm|divide and conquer]] [[algorithm]]s where the sub-problems have substantially different  sizes. It is a generalization of the [[master theorem (analysis of algorithms)|master theorem for divide-and-conquer recurrences]], which assumes that the sub-problems have equal size. It is named after mathematicians [[Mohamad Akra]] and [[Louay Bazzi]]&lt;ref name=":0" /&gt;.

== Formulation ==
The Akra–Bazzi method applies to recurrence formulas of the form&lt;ref name=":0"&gt;{{Cite journal|last=Akra|first=Mohamad|last2=Bazzi|first2=Louay|date=May 1998|year=1998|title=On the solution of linear recurrence equations|url=https://doi.org/10.1023/A:1018373005182|journal=Computational Optimization and Applications|volume=10|issue=2|pages=195-210|via=Springer Link}}&lt;/ref&gt;

:&lt;math&gt;T(x)=g(x) + \sum_{i=1}^k a_i T(b_i x + h_i(x))\qquad \text{for }x \geq x_0.&lt;/math&gt;

The conditions for usage are:

* sufficient base cases are provided
* &lt;math&gt;a_i&lt;/math&gt; and &lt;math&gt;b_i&lt;/math&gt; are constants for all &lt;math&gt;i&lt;/math&gt;
* &lt;math&gt;a_i &gt; 0&lt;/math&gt; for all &lt;math&gt;i&lt;/math&gt;
* &lt;math&gt;0 &lt; b_i &lt; 1&lt;/math&gt; for all &lt;math&gt;i&lt;/math&gt;
* &lt;math&gt;\left|g(x)\right| \in O(x^c)&lt;/math&gt;, where ''c'' is a constant and ''O'' notates [[Big O notation]]
* &lt;math&gt;\left| h_i(x) \right| \in O\left(\frac{x}{(\log x)^2}\right)&lt;/math&gt; for all &lt;math&gt;i&lt;/math&gt;
* &lt;math&gt;x_0&lt;/math&gt; is a constant

The asymptotic behavior of &lt;math&gt;T(x)&lt;/math&gt; is found by determining the value of &lt;math&gt;p&lt;/math&gt; for which &lt;math&gt;\sum_{i=1}^k a_i b_i^p = 1&lt;/math&gt; and plugging that value into the equation&lt;ref&gt;{{Cite web|url=https://people.mpi-inf.mpg.de/~mehlhorn/DatAlg2008/NewMasterTheorem.pdf|title=Proof and application on few examples|last=|first=|date=|website=|archive-url=|archive-date=|dead-url=|access-date=}}&lt;/ref&gt;

:&lt;math&gt;T(x) \in \Theta \left( x^p\left( 1+\int_1^x \frac{g(u)}{u^{p+1}}du \right)\right)&lt;/math&gt;

(see [[Big O notation|Θ]]). Intuitively, &lt;math&gt;h_i(x)&lt;/math&gt; represents a small perturbation in the index of &lt;math&gt;T&lt;/math&gt;.  By noting that &lt;math&gt;\lfloor b_i x \rfloor = b_i x + (\lfloor b_i x \rfloor - b_i x)&lt;/math&gt; and that the absolute value of &lt;math&gt;\lfloor b_i x \rfloor - b_i x&lt;/math&gt; is always between 0 and 1, &lt;math&gt;h_i(x)&lt;/math&gt; can be used to ignore the [[floor function]] in the index.  Similarly, one can also ignore the [[ceiling function]].  For example, &lt;math&gt;T(n) = n + T \left(\frac{1}{2} n \right)&lt;/math&gt; and &lt;math&gt;T(n) = n + T \left(\left\lfloor \frac{1}{2} n \right\rfloor \right)&lt;/math&gt; will, as per the Akra–Bazzi theorem, have the same asymptotic behavior.

== Example ==
Suppose &lt;math&gt;T(n)&lt;/math&gt; is defined as 1 for integers &lt;math&gt;0 \leq n \leq 3&lt;/math&gt; and &lt;math&gt;n^2 + \frac{7}{4} T \left( \left\lfloor \frac{1}{2} n \right\rfloor \right) + T \left( \left\lceil \frac{3}{4} n \right\rceil \right)&lt;/math&gt; for integers &lt;math&gt;n &gt; 3&lt;/math&gt;.  In applying the Akra–Bazzi method, the first step is to find the value of &lt;math&gt;p&lt;/math&gt; for which &lt;math&gt;\frac{7}{4} \left(\frac{1}{2}\right)^p + \left(\frac{3}{4} \right)^p = 1&lt;/math&gt;.  In this example, &lt;math&gt;p=2&lt;/math&gt;.  Then, using the formula, the asymptotic behavior can be determined as follows&lt;ref&gt;{{Cite book|title=Introduction to Algorithms|last=Cormen|first=Thomas|last2=Leiserson|first2=Charles|last3=Rivest|first3=Ronald|last4=Stein|first4=Clifford|publisher=MIT Press|year=2009|isbn=978-0262033848|location=|pages=}}&lt;/ref&gt;:

:&lt;math&gt;
\begin{align}
T(x) &amp; \in \Theta \left( x^p\left( 1+\int_1^x \frac{g(u)}{u^{p+1}}\,du \right)\right) \\
&amp; = \Theta \left( x^2 \left( 1+\int_1^x \frac{u^2}{u^3}\,du \right)\right) \\
&amp; = \Theta(x^2(1 + \ln x)) \\
&amp; = \Theta(x^2+ \log x).
\end{align}
&lt;/math&gt;

== Significance ==
The Akra–Bazzi method is more useful than most other techniques for determining asymptotic behavior because it covers such a wide variety of cases.  Its primary application is the approximation of the [[Run time (program lifecycle phase)|runtime]] of many divide-and-conquer algorithms.  For example, in the [[merge sort]], the number of comparisons required in the worst case, which is roughly proportional to its runtime, is given recursively as &lt;math&gt;T(1) = 0&lt;/math&gt; and 

:&lt;math&gt;T(n) = T\left(\left\lfloor \frac{1}{2} n \right\rfloor \right) + T\left(\left\lceil \frac{1}{2} n \right\rceil \right) + n - 1&lt;/math&gt; 

for integers &lt;math&gt;n &gt; 0&lt;/math&gt;, and can thus be computed using the Akra–Bazzi method to be &lt;math&gt;\Theta(n \log n)&lt;/math&gt;.

== References ==
&lt;references /&gt;
==See also==
* [[Master theorem (analysis of algorithms)]]
* [[Asymptotic complexity]] 

{{DEFAULTSORT:Akra-Bazzi Method}}
[[Category:Asymptotic analysis]]
[[Category:Theorems in discrete mathematics]]
[[Category:Recurrence relations]]
[[Category:Bazzi family]]

== External Links ==
[https://www.blogcyberini.com/2017/07/metodo-de-akra-bazzi.html O Método de Akra-Bazzi na Resolução de Equações de Recorrência] {{pt icon}}</text>
      <sha1>hpq6rs8gd6egvkzcd1jv6nmlwrgivi6</sha1>
    </revision>
  </page>
  <page>
    <title>Aparna Higgins</title>
    <ns>0</ns>
    <id>56615271</id>
    <revision>
      <id>862228917</id>
      <parentid>862228832</parentid>
      <timestamp>2018-10-03T00:36:32Z</timestamp>
      <contributor>
        <username>Oshwah</username>
        <id>3174456</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/Dka572958|Dka572958]] ([[User talk:Dka572958|talk]]) ([[WP:HG|HG]]) (3.4.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3945">'''Aparna W. Higgins''' is a mathematician known for her encouragement of undergraduate mathematicians to participate in mathematical research.{{r|section}} Higgins originally specialized in [[universal algebra]], but her more recent research concerns [[graph theory]], including [[graph pebbling]] and [[line graph]]s.{{r|pcumc}} She is a professor of mathematics at the [[University of Dayton]].{{r|dayton}}

==Education and career==
Higgins is originally from [[Mumbai]], India, and did her undergraduate studies at the [[University of Mumbai]], graduating in 1978.{{r|pcumc}} She completed her Ph.D. in 1983 at the [[University of Notre Dame]]; her dissertation, ''Heterogeneous Algebras Associated with Non-Indexed Algebras, a Representation Theorem on Weak Automorphisms of Universal Algebras'', was supervised by Abraham Goetz.{{r|mgp}}

In 2009 she became director of [[Project NExT]], after the previous director, [[T. Christine Stevens]], stepped down; this project is an initiative of the [[Mathematical Association of America]] to provide career guidance to new doctorates in mathematics.{{r|focus}}

Higgins is married to Bill Higgins, a mathematics professor at [[Wittenberg University]], and the two regularly take their [[sabbatical]]s together in California.{{r|clu}}

==Recognition==
Higgins won a Distinguished Teaching Award from the [[Mathematical Association of America]] in 1995, for her contributions to undergraduate research.{{r|section}} In 2005 she was one of three winners of the [[Deborah and Franklin Haimo Awards for Distinguished College or University Teaching of Mathematics|Deborah and Franklin Haimo Award for Distinguished College or University Teaching of Mathematics]] of the Mathematical Association of America.{{r|atlanta}}

==References==
{{reflist|refs=

&lt;ref name=atlanta&gt;{{citation|url=http://www.ams.org/notices/200505/comm-maa.pdf|title=MAA Prizes Presented in Atlanta|journal=Notices of the American Mathematical Society|pages=543–544|date=May 2005|volume=52|issue=5}}&lt;/ref&gt;

&lt;ref name=clu&gt;{{citation|url=https://www.cluecho.com/2016/02/married-math-professors-join-clu-community-one-year/|title=Married math professors join the CLU community for one year|date=February 24, 2016|magazine=The Echo|publisher=California Lutheran University|accessdate=2018-02-17}}&lt;/ref&gt;

&lt;ref name=dayton&gt;{{citation|url=https://udayton.edu/directory/artssciences/mathematics/higgins_aparna.php|title=Aparna Higgins|work=College of Arts and Sciences Directory|publisher=[[University of Dayton]]|accessdate=2018-02-17}}&lt;/ref&gt;

&lt;ref name=focus&gt;{{citation|url=https://www.maa.org/sites/default/files/pdf/pubs/febmar09pg15.pdf|title=Aparna Higgins to Become Director of Project NExT|journal=MAA Focus|publisher=[[Mathematical Association of America]]|date=February–March 2009|page=15}}&lt;/ref&gt;

&lt;ref name=mgp&gt;{{mathgenealogy|id=51184}}&lt;/ref&gt;

&lt;ref name=pcumc&gt;{{citation|url=http://acrans.lmu.build/PCUMC2009/invited_speakers.html|title=Morning speaker|publisher=Pacific Coast Undergraduate Mathematics Conference|date=March 14, 2009|accessdate=2018-02-17}}&lt;/ref&gt;

&lt;ref name=section&gt;{{citation|url=http://sections.maa.org/ohio/Award/aparna.html|title=Aparna Higgins|publisher=Ohio Section of the Mathematical Association of America|accessdate=2018-02-17}}&lt;/ref&gt;

}}

{{Authority control}}
{{DEFAULTSORT:Higgins, Aparna}}
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]
[[Category:Indian women mathematicians]]
[[Category:20th-century American mathematicians]]
[[Category:21st-century American mathematicians]]
[[Category:American women mathematicians]]
[[Category:Graph theorists]]
[[Category:University of Mumbai alumni]]
[[Category:University of Notre Dame alumni]]
[[Category:University of Dayton faculty]]
[[Category:Women scientists from Maharashtra]]
[[Category:Indian combinatorialists]]
[[Category:20th-century Indian mathematicians]]
[[Category:21st-century Indian mathematicians]]</text>
      <sha1>keslagqmjl0p4jxxfkl108yp0xkrosg</sha1>
    </revision>
  </page>
  <page>
    <title>Apodicticity</title>
    <ns>0</ns>
    <id>2887375</id>
    <revision>
      <id>803819538</id>
      <parentid>801049603</parentid>
      <timestamp>2017-10-04T20:59:13Z</timestamp>
      <contributor>
        <username>DavidBrooks-AWB</username>
        <id>20962961</id>
      </contributor>
      <minor/>
      <comment>Specific EB1911 cite using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2635">{{The Works of Aristotle}}
"'''Apodictic'''" or "'''apodeictic'''" ({{lang-grc|ἀποδεικτικός}}, "capable of demonstration") is an [[adjective|adjectival]] expression from [[syllogism|Aristotelean logic]] that refers to [[proposition]]s that are demonstrably, necessarily or [[self-evident]]ly the case.&lt;ref&gt;[http://dictionary.reference.com/browse/apodictic Dictionary definitions of apodictic], from dictionary.com, including material from the [[Random House Unabridged Dictionary]], Random House, Inc (2006), [[The American Heritage Dictionary of the English Language]], Fourth Edition, 2006 by Houghton Mifflin Company, and [[WordNet]] 3.0, [[Princeton University]] 2006.&lt;/ref&gt; '''Apodicticity''' or [[wikt:apodixis|apodixis]] is the corresponding [[abstract noun]], referring to [[logical truth|logical certainty]].

Apodictic propositions contrast with [[assertoric]] propositions, which merely assert that something is (or is not) the case, and with problematic propositions, which assert only the possibility of something being true. Apodictic judgments are clearly provable or logically certain. For instance, "Two plus two equals four" is apodictic.  "Chicago is larger than Omaha" is assertoric.  "A corporation could be wealthier than a country" is problematic.  In [[Aristotelianism|Aristotelian]] logic, "apodictic" is opposed to "[[dialectic]]," as [[scientific proof]] is opposed to philosophical [[reasoning]]. [[Immanuel Kant|Kant]] contrasted "apodictic" with "problematic" and "assertoric" in the ''[[Critique of Pure Reason]]'', on page A70/B95.&lt;ref name="EB1911"&gt;{{Cite EB1911|wstitle=Apodictic|volume=2|page=183}}&lt;/ref&gt;

==Apodictic a priorism==
[[Hans Reichenbach]], one of the founders of [[logical positivism]], offered a modified version of [[Immanuel Kant]]'s [[a priorism]] by distinguishing between ''apodictic a priorism'' and ''constitutive a priorism''.&lt;ref name="Mormann"&gt;{{cite book|last1=Mormann|first1=Thomas|title=Toward a Theory of the Pragmatic A Priori: From Carnap to Lewis and Beyond|work=Vienna Circle Institute Yearbook 16|date=2012|publisher=Springer Science+Business Media|location=Dordrecht|editor-last1=Creath|editor-first1=Richard|isbn=978-94-007-3928-4|page=116|edition=2012.|doi=10.1007/978-94-007-3929-1_7|url=https://www.amazon.com/Rudolf-Logical-Empiricism-Institute-Yearbook/dp/9400739281|accessdate=7 March 2016}}&lt;/ref&gt;

==Notes==
&lt;references/&gt;

==References==
* [[Antony Flew]]. ''A Dictionary of Philosophy - Revised Second Edition'' St. Martin's Press, NY, 1979

==External links==
* {{Wiktionary-inline|apodictic}}

[[Category:Modal logic]]
[[Category:Term logic]]</text>
      <sha1>ak4i4umie6kbt2nfsm11likbjff5lof</sha1>
    </revision>
  </page>
  <page>
    <title>Arithmeum</title>
    <ns>0</ns>
    <id>19410107</id>
    <revision>
      <id>817284097</id>
      <parentid>797930405</parentid>
      <timestamp>2017-12-27T11:00:05Z</timestamp>
      <contributor>
        <username>JzG</username>
        <id>760284</id>
      </contributor>
      <minor/>
      <comment>/* Exhibitions */Tag self-published source using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2189">[[Image:Arithmeum von Norden.jpg|thumb|Façade of the museum]]
The '''Arithmeum''' is a mathematics [[museum]] owned by the Forschungsinstitut für Diskrete Mathematik (Research Institute for Discrete Mathematics) at the [[University of Bonn]].

It was founded by the Director of the Institute, [[Bernhard Korte]], who contributed his private collection of [[calculating machine]]s.

The building's steel-glass facade - located at Lennéstrasse 2 - is meant to represent the "transparency of science".

== Exhibitions ==
[[Image:Arithmeum Calculating Machines.jpg|thumb|upright=0.8|Calculators in the Arithmeum]]
The permanent exhibit "Calculating in Olden and Modern Times" ({{lang-de|Rechnen Einst und Heute}}) shows the progression of mechanical calculating machines through 1,200 pieces.

It holds the very large,(4000 pieces), IJzebrand Schuitema (1929-2013) 400 year collection of [[slide rules]].&lt;ref&gt;{{cite web|url=http://www.arithmeum.uni-bonn.de/en/shop/article/72 |title=Archived copy |accessdate=2016-01-09 |deadurl=yes |archiveurl=https://web.archive.org/web/20150108044739/http://www.arithmeum.uni-bonn.de/en/shop/article/72 |archivedate=2015-01-08 |df= }}&lt;/ref&gt;&lt;ref&gt;http://www.rekeninstrumenten.nl/MIR/MIR%20Articles/MIR49%20p%209-13%20Schuitema%20Collection.pdf (English)&lt;/ref&gt;&lt;ref&gt;https://www.amazon.de/dp/3894798335&lt;/ref&gt;&lt;ref&gt;lulu.com/shop/ijzebrand-schuitema-and-otto-van-poelje/the-schuitema-collection-hardcover-version/ebook/product-17414042.html Free&lt;/ref&gt;{{Self-published inline|certain=yes|date=December 2017}}

There are also exhibits on very-large-scale integrated ([[VLSI]]) logic chips, historical arithmetic books dating back to [[Johannes Gutenberg]]'s times, and the relationship between art and science.

==References==
{{Reflist}}

== External links ==
{{Commons category|Arithmeum}}
* {{Official website|http://www.arithmeum.de/}}
*[http://www.tuxamoon.de/magazine/kultur/kunst/2004/10/27/arithmeum-bonn/ The Arithmeum's History]

{{coord|50|43|50|N|7|6|18|E|display=title|type:landmark_scale:1_region:DE-NW}}

[[Category:Museums in Bonn]]
[[Category:History of mathematics]]
[[Category:Science museums in Germany]]


{{NorthRhineWestphalia-struct-stub}}</text>
      <sha1>sq3c1j7k8wj4x20xn7l51rdoja5hc7d</sha1>
    </revision>
  </page>
  <page>
    <title>Asynchronous system</title>
    <ns>0</ns>
    <id>3801097</id>
    <revision>
      <id>864221607</id>
      <parentid>790025884</parentid>
      <timestamp>2018-10-15T21:50:08Z</timestamp>
      <contributor>
        <ip>128.59.20.245</ip>
      </contributor>
      <comment>Added reference to a more recent and modern 2-part overview article on the state-of-the-field for asynchronous systems.  The article is published in a top technical magazine, IEEE Design and Test.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9655">{{for|additional information|Asynchronous circuit}}

The primary focus of this article is asynchronous control in digital electronic systems.&lt;ref&gt;{{Cite book|title = Asynchronous Control for Networked Systems|url = https://books.google.com/books?id=1EqGCgAAQBAJ|publisher = Springer|date = 2015-10-04|isbn = 9783319212999|language = en|first = María Guinaldo|last = Losada|first2 = Francisco Rodríguez|last2 = Rubio|first3 = Sebastián|last3 = Dormido}}&lt;/ref&gt;&lt;ref&gt;{{Cite book|title = Principles of Asynchronous Circuit Design: A Systems Perspective|url = https://books.google.com/books?id=fefgBwAAQBAJ|publisher = Springer Science &amp; Business Media|date = 2013-04-17|isbn = 9781475733853|language = en|first = Jens|last = Sparsø|first2 = Steve|last2 = Furber}}&lt;/ref&gt;   In a [[synchronous system]], operations ([[Opcode|instructions]], [[Arithmetic logic unit|calculations]], [[Sequential logic|logic]], etc.)  are coordinated by one, or more, centralized [[clock signal]]s.  An asynchronous digital system, in contrast, has no global clock.  Asynchronous systems do not depend on strict arrival times of signals or messages for reliable operation.   Coordination is achieved via [[Event-driven architecture|events]] such as: [[Network packet|packet]] arrival, changes (transitions) of signals,  handshake protocols, and other methods.

==Modularity==

Asynchronous systems – much like [[object-oriented]] software – are typically constructed out of [[Module (programming)|modular]] 'hardware objects', each with well-defined communication [[Interface (computer science)|interface]]s. These [[modularity (programming)|modules]] may operate at variable speeds, whether due to data-dependent processing, [[dynamic voltage scaling]], or [[process variation]]. The modules can then be combined together to form a correct working system, without reference to a global [[clock signal]]. Typically, [[Low-power electronics|low power]] is obtained since components are activated only on demand. Furthermore, several asynchronous styles have been shown  to accommodate clocked interfaces, and thereby support mixed-timing design. Hence, asynchronous systems match well the need for correct-by-construction [[methodology|methodologies]] in assembling large-scale heterogeneous and scalable systems.

==Design styles==

There is a large spectrum of asynchronous design styles, with tradeoffs between robustness and performance (and other parameters such as power). The choice of design style depends on the application target: reliability/ease-of-design vs. speed.  The most robust designs use  '[[Delay insensitive circuit|delay-insensitive circuits]]', whose operation is correct regardless
of [[Delay calculation|gate and wire delays]]; however, only limited useful systems can be  designed with this style.  Slightly less robust, but much more useful, are  [[quasi-delay-insensitive circuit]]s (also known as speed-independent circuits), such as [[delay-insensitive minterm synthesis]], which operate correctly regardless of [[Delay calculation|gate delays]]; however, wires at each [[fanout]] point must be tuned for roughly equal delays.  Less robust but faster circuits, requiring simple localized one-sided [[timing constraints]],  include [[Controller (computing)|controllers]] using fundamental-mode operation (i.e. with setup/hold  requirements on when new inputs can be received), and bundled datapaths using matched delays (see below).  At the extreme, high-performance "timed circuits" have been proposed, which use tight two-side timing constraints, where the [[clock signal|clock]] can still be avoided but careful physical delay tuning is required, such as for some high-speed  [[pipeline (computing)|pipeline]] applications.

==Asynchronous communication==

[[Asynchronous communication]] is typically performed on [[Channel (communications)|channels]]. Communication is used both to [[synchronize]] operations of the concurrent system as well as to pass data.  A simple channel typically consists  of two wires:  a request and an acknowledge.  In a '4-phase [[handshaking]]  [[protocol (computing)|protocol]]' (or return-to-zero), the request is asserted by the sender component, and the receiver responds by asserting the acknowledge; then both signals are de-asserted in turn.  In a '2-phase [[handshaking]] [[protocol (computing)|protocol]]' (or transition-signalling), the requester simply toggles the value on the request wire (once), and the receiver responds by toggling the value on the acknowledge wire.  Channels can also be extended to communicate data.

==Asynchronous datapaths==

Asynchronous [[datapath]]s are typically encoded using several schemes. Robust schemes use two wires or 'rails' for each bit, called 'dual-rail encoding'.  In this case, first rail is asserted to transmit a 0 value, or the second rail is asserted to transmit a 1 value.  The asserted rail is then reset to zero before the next data value is transmitted, thereby indicating 'no data' or a 'spacer' state.  A less robust, but widely used and practical scheme, is called 'single-[[Rail (electronics)|rail]] bundled data'. Here, a single-rail (i.e. synchronous-style) function block can be used, with an accompanying [[worst-case]] matched delay.  After valid data inputs arrive, a request signal is [[assertion (software development)|asserted]] as the input to the matched delay.  When the matched delay produces a 'done' output, the block guaranteed to have completed computation.  While this scheme has timing constraints, they are simple, localized (unlike in [[synchronous system]]s), and one-sided, hence are usually easy to validate.

==Literature==

The literature in this field exists in a variety of conference and journal proceedings.  The leading symposium is the IEEE Async Symposium (International Symposium on Asynchronous Circuits and Systems), founded in 1994.   A variety of asynchronous papers have also been published since the mid-1980s in such conferences as IEEE/ACM [[Design Automation Conference]],  IEEE [http://www.iccd-conference.com International Conference on Computer Design], IEEE/ACM [[International Conference on Computer-Aided Design]], [http://www.isscc.org/isscc/ International Solid-State Circuits  Conference], and Advanced Research in VLSI, as well as in leading journals  such as IEEE Transactions on VLSI Systems, [[IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems]], and Transactions on Distributed Computing.

==See also==
*[[Plesiochronous system]]
*[[Mesochronous network]]
*[[Isochronous timing]]
*[[Integrated circuit design]]
*[[Electronic design automation]]
*[[Design flow (EDA)]]
* [[perfect clock gating]]

==References==
{{reflist}}

* S.M. Nowick and M. Singh, [http://www.cs.columbia.edu/~nowick/nowick-singh-async-IEEE-DT-15-overview-article-pt1.pdf "Asynchronous Design -- Part 1:  Overview and Recent Advances"], IEEE Design and Test, vol. 32:3, pp. 5-18 (May/June 2015).
*S.M. Nowick and M. Singh, [http://www.cs.columbia.edu/~nowick/nowick-singh-async-IEEE-DT-15-overview-article-pt2.pdf "Asynchronous Design -- Part 2:  Systems and Methodologies"], IEEE Design and Test, vol. 32:3, pp. 19-28 (May/June 2015)
**These two articles provide a broad and modern snapshot of the state-of-the-art of asynchronous design. They include a short history of asynchronous design, as well as a technical introduction to handshaking protocols and data encoding, hazard-free logic, and controller design.  They also cover recent industrial successes in mainstream technologies (IBM, Intel, Philips Semiconductors, etc.), as well as recent application to emerging areas (neuromorphic computers, flexible electronics, quantum cellular automata, continuous-time DSPs, ultra-low voltage design, extreme environments).Highlights several application areas in depth, with a wide range of cited publications: GALS systems, networks-on-chip, computer architecture, testing and design-for-testability, and CAD tool development.
*Claire Tristram, "It's Time for Clockless Chips", cover story, MIT's Technology Review Magazine, vol. 104:8, pp.&amp;nbsp;36–41, October 2001.
* C.H. van Berkel, M.B. Josephs, and S.M. Nowick, {{doi-inline|10.1109/5.740016|Applications of Asynchronous Circuits}}, Proceedings of the IEEE, Vol. 87, No. 2, pp.&amp;nbsp;223–233, February 1999.  (''This entire issue is devoted to asynchronous circuits, with many other relevant articles''.)
* L. Lavagno and S.M. Nowick, "Asynchronous Control Circuits", chapter 10 in {{cite book | title=Logic Synthesis and Verification | author = eds. Soha Hassoun and Tsutomu Sasao
|publisher = Kluwer Academic Publishers | year= 2002 | isbn=0-7923-7606-4}},  pp.&amp;nbsp;255–284,(''Includes pointers to recent asynchronous chips, as well as coverage of CAD techniques for asynchronous control circuits''.)

----
Adapted from [http://www1.cs.columbia.edu/~nowick/ Steve Nowick]'s column in the ACM [http://www.sigda.org SIGDA] [https://web.archive.org/web/20070208034716/http://www.sigda.org/newsletter/index.html e-newsletter] by [http://www.eecs.umich.edu/~imarkov/ Igor Markov] &lt;br&gt;
Original text is available at https://web.archive.org/web/20060624073502/http://www.sigda.org/newsletter/2006/eNews_060115.html
----

==External links==
* [http://www.scitechinfo.com/node/638 ARM ARM996HS clockless processor ]
* [https://web.archive.org/web/20060620114944/http://www.na2.es/research.htm Navarre AsyncArt. N-Protocol: Asynchronous Design Methodology for FPGAs]

[[Category:Electrical circuits]]
[[Category:Synchronization]]
[[Category:Electronic design automation]]
[[Category:Network architecture]]
[[Category:Formal methods]]</text>
      <sha1>ajbtnf3n400p7p9ch0foxz3fidgg90v</sha1>
    </revision>
  </page>
  <page>
    <title>Banach function algebra</title>
    <ns>0</ns>
    <id>4827724</id>
    <revision>
      <id>793623896</id>
      <parentid>787002333</parentid>
      <timestamp>2017-08-02T23:10:15Z</timestamp>
      <contributor>
        <username>Rgdboer</username>
        <id>92899</id>
      </contributor>
      <comment>/* References */ cite Browder, Allan</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1967">In [[functional analysis]] a '''Banach function algebra''' on a [[compact space|compact]] [[Hausdorff space]] ''X'' is [[unital algebra|unital]] [[subalgebra]], ''A'' of the [[commutative]] [[C*-algebra]]  ''C(X)'' of all [[continuous function|continuous]], [[complex number|complex]] valued functions from ''X'', together with a norm on ''A'' which makes it a [[Banach algebra]].

A function algebra is said to vanish at a point p if f(p) = 0 for all &lt;math&gt; (f\in A) &lt;/math&gt;. A function algebra [[separating set|separates points]] if for each distinct pair of points &lt;math&gt; (p,q \in X) &lt;/math&gt;, there is a function &lt;math&gt; (f\in A) &lt;/math&gt; such that &lt;math&gt; f(p) \neq f(q) &lt;/math&gt;.

For every &lt;math&gt;x\in X&lt;/math&gt; define &lt;math&gt;\varepsilon_x(f)=f(x)\ (f\in A)&lt;/math&gt;. Then &lt;math&gt;\varepsilon_x&lt;/math&gt;
is a non-zero homomorphism (character) on &lt;math&gt;A&lt;/math&gt;.

'''Theorem:''' A Banach function algebra is [[semisimple algebra|semisimple]] (that is its [[Jacobson radical]] is equal to zero) and  each commutative [[unital ring|unital]], semisimple Banach algebra is [[isomorphic]] (via the [[Gelfand transform]]) to a Banach function algebra on its [[character space]] (the space of algebra homomorphisms from ''A'' into the complex numbers given the [[relative topology|relative]] [[weak* topology]]).

If the norm on &lt;math&gt;A&lt;/math&gt; is the uniform norm (or sup-norm) on &lt;math&gt;X&lt;/math&gt;, then &lt;math&gt;A&lt;/math&gt; is called
a '''uniform algebra'''. Uniform algebras are an important special case of Banach function algebras.
==References==
* [[Andrew Browder]] (1969) ''Introduction to Function Algebras'', [[W. A. Benjamin]]
* H.G. Dales (2000) ''Banach Algebras and Automatic Continuity'', [[London Mathematical Society]] Monographs 24, [[Clarendon Press]] {{ISBN|0-19-850013-0}}
* [[Graham Allan]] &amp; H. Garth Dales (2011) ''Introduction to Banach Spaces and Algebras'', [[Oxford University Press]] {{ISBN|978-0-19-920654-4}}

{{Mathanalysis-stub}}
[[Category:Banach algebras]]</text>
      <sha1>i1ghl8led1iunfsvu69uvfxmydisecm</sha1>
    </revision>
  </page>
  <page>
    <title>Biot–Tolstoy–Medwin diffraction model</title>
    <ns>0</ns>
    <id>30485319</id>
    <revision>
      <id>741983122</id>
      <parentid>631133264</parentid>
      <timestamp>2016-09-30T22:33:09Z</timestamp>
      <contributor>
        <ip>213.233.84.49</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2338">In [[applied mathematics]], the '''Biot–Tolstoy–Medwin (BTM) diffraction model''' describes [[edge diffraction]].  Unlike the [[uniform theory of diffraction]] (UTD), BTM does not make the [[high frequency]] assumption (in which edge lengths and distances from source and receiver are much larger than the wavelength). BTM sees use in acoustic simulations.&lt;ref&gt;Calamia 2007, p. 182.&lt;/ref&gt;

== Impulse response ==

The [[impulse response]] according to BTM is given as follows:&lt;ref&gt;Calamia 2007, p. 183.&lt;/ref&gt;

The general expression for [[sound pressure]] is given by the [[convolution]] integral

: &lt;math&gt;
p(t) = \int_0^\infty h(\tau) q (t - \tau) \, d \tau
&lt;/math&gt;

where &lt;math&gt;q(t)&lt;/math&gt; represents the source signal, and &lt;math&gt;h(t)&lt;/math&gt; represents the impulse response at the receiver position. The BTM gives the latter in terms of

* the source position in cylindrical coordinates &lt;math&gt;( r_S, \theta_S, z_S )&lt;/math&gt; where the &lt;math&gt;z&lt;/math&gt;-axis is considered to lie on the edge and &lt;math&gt;\theta&lt;/math&gt; is measured from one of the faces of the wedge.
* the receiver position &lt;math&gt;( r_R, \theta_R, z_R )&lt;/math&gt;
* the (outer) wedge angle &lt;math&gt;\theta_W&lt;/math&gt; and from this the wedge index &lt;math&gt;\nu = \pi / \theta_W&lt;/math&gt;
* the speed of sound &lt;math&gt;c&lt;/math&gt;

as an integral over edge positions &lt;math&gt;z&lt;/math&gt;

: &lt;math&gt;
h(\tau) = -\frac{\nu}{4\pi} \sum_{\phi_i = \pi \pm \phi_S \pm \phi_R} \int_{z_1}^{z_2} \delta\left(\tau - \frac{m+l}{c}\right) \frac{\beta_i}{ml} \, dz
&lt;/math&gt;

where the summation is over the four possible choices of the two signs, &lt;math&gt;m&lt;/math&gt; and &lt;math&gt;l&lt;/math&gt; are the distances from the point &lt;math&gt;z&lt;/math&gt; to the source and receiver respectively, and &lt;math&gt;\delta&lt;/math&gt; is the [[Dirac delta function]].

: &lt;math&gt;
\beta_i = \frac{\sin (\nu \phi_i)}{\cosh(\nu \eta) - \cos(\nu \phi_i)}
&lt;/math&gt;

where

: &lt;math&gt;
\eta = \cosh^{-1} \frac{ml + (z - z_S)(z - z_R)}{r_S r_R}
&lt;/math&gt;

== See also ==
* [[Uniform theory of diffraction]]

== Notes ==
{{reflist}}

== References ==
* Calamia, Paul T. and Svensson, U. Peter, "Fast time-domain edge-diffraction calculations for interactive acoustic simulations," EURASIP Journal on Advances in Signal Processing, Volume 2007, Article ID 63560.

{{DEFAULTSORT:Biot-Tolstoy-Medwin diffraction model}}
[[Category:Signal processing]]


{{signal-processing-stub}}</text>
      <sha1>b3ct95vmvcrsk2yyg5u8y9dy0jw982f</sha1>
    </revision>
  </page>
  <page>
    <title>Branching theorem</title>
    <ns>0</ns>
    <id>6598775</id>
    <revision>
      <id>624080696</id>
      <parentid>493942483</parentid>
      <timestamp>2014-09-03T22:58:05Z</timestamp>
      <contributor>
        <username>Mark viking</username>
        <id>17698045</id>
      </contributor>
      <comment>/* Statement of the theorem */ Added wl</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1487">In [[mathematics]], the '''branching theorem''' is a [[theorem]] about [[Riemann surface]]s. Intuitively, it states that every non-constant [[holomorphic function]] is [[locally]] a [[polynomial]].

==Statement of the theorem==

Let &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; be Riemann surfaces, and let &lt;math&gt;f : X \to Y&lt;/math&gt; be a non-constant holomorphic map. Fix a point &lt;math&gt;a \in X&lt;/math&gt; and set &lt;math&gt;b := f(a) \in Y&lt;/math&gt;. Then there exist &lt;math&gt;k \in \N&lt;/math&gt; and [[Chart_(topology)|chart]]s &lt;math&gt;\psi_{1} : U_{1} \to V_{1}&lt;/math&gt; on &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;\psi_{2} : U_{2} \to V_{2}&lt;/math&gt; on &lt;math&gt;Y&lt;/math&gt; such that
* &lt;math&gt;\psi_{1} (a) = \psi_{2} (b) = 0&lt;/math&gt;; and
* &lt;math&gt;\psi_{2} \circ f \circ \psi_{1}^{-1} : V_{1} \to V_{2}&lt;/math&gt; is &lt;math&gt;z \mapsto z^{k}.&lt;/math&gt;

This theorem gives rise to several definitions:
* We call &lt;math&gt;k&lt;/math&gt; the ''[[Multiplicity (mathematics)|multiplicity
]]'' of &lt;math&gt;f&lt;/math&gt; at &lt;math&gt;a&lt;/math&gt;. Some authors denote this &lt;math&gt;\nu (f, a)&lt;/math&gt;.
* If &lt;math&gt;k &gt; 1&lt;/math&gt;, the point &lt;math&gt;a&lt;/math&gt; is called a ''[[branch point]]'' of &lt;math&gt;f&lt;/math&gt;.
* If &lt;math&gt;f&lt;/math&gt; has no branch points, it is called ''unbranched''. See also [[unramified morphism]].

== References ==
*{{citation|first=Lars|last=Ahlfors|authorlink=Lars Ahlfors|title=Complex analysis|publisher=McGraw Hill|year=1953|publication-date=1979|edition=3rd|isbn=0-07-000657-1}}.

[[Category:Theorems in complex analysis]]
[[Category:Riemann surfaces]]
{{mathanalysis-stub}}</text>
      <sha1>3gjk8qxhvsjyp94hsblvx77a7mzuqy3</sha1>
    </revision>
  </page>
  <page>
    <title>Choice function</title>
    <ns>0</ns>
    <id>1432664</id>
    <revision>
      <id>799325465</id>
      <parentid>799325271</parentid>
      <timestamp>2017-09-07T00:48:19Z</timestamp>
      <contributor>
        <username>RJGray</username>
        <id>8268674</id>
      </contributor>
      <comment>/* Refinement of the notion of choice function */ Added link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4963">{{for|the combinatorial choice function C(n, k)|Combination|Binomial coefficient}}

A '''choice function''' ('''selector''', '''selection''')  is a [[mathematical function]] ''f''  that is defined on some collection ''X'' of nonempty [[Set (mathematics)|sets]] and assigns to each set ''S'' in that collection some element ''f''(''S'') of ''S''.  In other words, ''f'' is a choice function for ''X'' if and only if it belongs to the [[direct product]] of ''X''.

== An example ==
Let ''X''&amp;nbsp;=&amp;nbsp;{&amp;nbsp;{1,4,7},&amp;nbsp;{9},&amp;nbsp;{2,7}&amp;nbsp;}. Then the function that assigns 7 to the set {1,4,7}, 9 to {9}, and 2 to {2,7} is a choice function on ''X''.

== History and importance ==
[[Ernst Zermelo]] (1904) introduced choice functions as well as the [[axiom of choice]] (AC) and proved the [[well-ordering theorem]],&lt;ref name="Zermelo, 1904"&gt;{{cite journal| first=Ernst| last=Zermelo| year=1904| title=Beweis, dass jede Menge wohlgeordnet werden kann| journal=Mathematische Annalen| volume=59| issue=4| pages=514–16| doi=10.1007/BF01445300| url=http://gdz.sub.uni-goettingen.de/no_cache/en/dms/load/img/?IDDOC=28526}}&lt;/ref&gt; which states that every set can be [[well ordering|well-ordered]]. AC states that every set of nonempty sets has a choice function. A weaker form of AC, the [[axiom of countable choice]] (AC&lt;sub&gt;ω&lt;/sub&gt;) states that every [[countable set]] of nonempty sets has a choice function. However, in the absence of either AC or AC&lt;sub&gt;ω&lt;/sub&gt;, some sets can still be shown to have a choice function.

*If &lt;math&gt;X&lt;/math&gt; is a [[finite set|finite]] set of nonempty sets, then one can construct a choice function for &lt;math&gt;X&lt;/math&gt; by picking one element from each member of &lt;math&gt;X.&lt;/math&gt; This requires only finitely many choices, so neither AC or AC&lt;sub&gt;ω&lt;/sub&gt; is needed.
*If every member of &lt;math&gt;X&lt;/math&gt; is a nonempty set, and the [[union (set theory)|union]] &lt;math&gt;\bigcup X&lt;/math&gt; is well-ordered, then one may choose the least element of each member of &lt;math&gt;X&lt;/math&gt;. In this case, it was possible to simultaneously well-order every member of &lt;math&gt;X&lt;/math&gt; by making just one choice of a well-order of the union, so neither AC nor AC&lt;sub&gt;ω&lt;/sub&gt; was needed. (This example shows that the well-ordering theorem implies AC. The [[Converse (logic)|converse]] is also true, but less trivial.)

== Refinement of the notion of choice function ==
A function &lt;math&gt;f: A \rightarrow B&lt;/math&gt; is said to be a selection of a [[multivalued function|multivalued map]] φ:''A'' &amp;rarr; ''B''  (that is, a function &lt;math&gt;\varphi:A\rightarrow\mathcal{P}(B)&lt;/math&gt; from ''A'' to the [[power set]] &lt;math&gt;\mathcal{P}(B)&lt;/math&gt;), if
:&lt;math&gt;\forall a \in A \, ( f(a) \in \varphi(a) ) \,.&lt;/math&gt;

The existence of more regular choice functions, namely continuous or measurable selections is important in the theory of [[differential inclusion]]s, [[optimal control]], and [[mathematical economics]].&lt;ref&gt;{{cite book
    | last = Border
    | first = Kim C.
    | title = Fixed Point Theorems with Applications to Economics and Game Theory
    | year = 1989
    | publisher = Cambridge University Press
    | isbn = 0-521-26564-9
  }}&lt;/ref&gt;

===Bourbaki tau function===
[[Nicolas Bourbaki]] used [[epsilon calculus]] for their foundations that had a &lt;math&gt; \tau &lt;/math&gt; symbol that could be interpreted as choosing an object (if one existed) that satisfies a given proposition. So if &lt;math&gt; P(x) &lt;/math&gt; is a predicate, then &lt;math&gt;\tau_{x}(P)&lt;/math&gt; is the object that satisfies &lt;math&gt;P&lt;/math&gt; (if one exists, otherwise it returns an arbitrary object). Hence we may obtain quantifiers from the choice function, for example &lt;math&gt; P( \tau_{x}(P))&lt;/math&gt; was equivalent to &lt;math&gt; (\exists x)(P(x))&lt;/math&gt;.&lt;ref&gt;{{cite book|last=Bourbaki|first=Nicolas|title=Elements of Mathematics: Theory of Sets|isbn=0-201-00634-0}}&lt;/ref&gt;

However, Bourbaki's choice operator is stronger than usual: it's a ''global'' choice operator. That is, it implies the [[axiom of global choice]].&lt;ref&gt;John Harrison, "The Bourbaki View" [http://www.rbjones.com/rbjpub/logic/jrh0105.htm eprint].&lt;/ref&gt; Hilbert realized this when introducing epsilon calculus.&lt;ref&gt;"Here, moreover, we come upon a very remarkable circumstance, namely, that all of these transfinite axioms are derivable from a single axiom, one that also contains the core of one of the most attacked axioms in the literature of mathematics, namely, the axiom of choice: &lt;math&gt;A(a)\to A(\varepsilon(A))&lt;/math&gt;, where &lt;math&gt;\varepsilon&lt;/math&gt; is the transfinite logical choice function." Hilbert (1925), “On the Infinite”, excerpted in Jean van Heijenoort, ''From Frege to Gödel'', p. 382. From [http://ncatlab.org/nlab/show/choice+operator nCatLab].&lt;/ref&gt;

==See also==
* [[Axiom of countable choice]]
* [[Hausdorff paradox]]
* [[Hemicontinuity]]

==Notes==
{{Reflist}}

==References==
{{PlanetMath attribution|id=6419|title=Choice function}}

[[Category:Basic concepts in set theory]]
[[Category:Axiom of choice]]</text>
      <sha1>9dp1i370dber8jfbv2ekbtgz3ruqhtv</sha1>
    </revision>
  </page>
  <page>
    <title>Computational complexity of mathematical operations</title>
    <ns>0</ns>
    <id>6497220</id>
    <revision>
      <id>866352124</id>
      <parentid>862348700</parentid>
      <timestamp>2018-10-29T20:41:19Z</timestamp>
      <contributor>
        <ip>128.135.98.216</ip>
      </contributor>
      <comment>/* Special functions */ Section does include some — albeit perhaps insufficiently many (one) — citation; see first sentence</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15869">{{refimprove|date=April 2015}}
{{comparison computational complexity.svg}}
The following tables list the [[computational complexity]] of various [[algorithm]]s for common [[mathematical operation]]s.

Here, complexity refers to the [[time complexity]] of performing computations on a [[multitape Turing machine]].&lt;ref name="Schonhage"&gt;A. Schönhage, A.F.W. Grotefeld, E. Vetter: ''Fast Algorithms—A Multitape Turing Machine Implementation'', BI Wissenschafts-Verlag, Mannheim, 1994&lt;/ref&gt; See [[big O notation]] for an explanation of the notation used.

Note: Due to the variety of multiplication algorithms, ''M''(''n'') below stands in for the complexity of the chosen multiplication algorithm.

==Arithmetic functions==
{{clear}}
{| class="wikitable"
!Operation
!Input
!Output
!Algorithm
!Complexity
|-
|[[Addition]]
|Two ''n''-digit numbers ''N'', ''N''
|One ''n''+1-digit number
|Schoolbook addition with carry
|Θ(''n''), Θ(log(''N''))
|-
|[[Subtraction]]
|Two ''n''-digit numbers ''N'', ''N''
|One ''n''+1-digit number 
|Schoolbook subtraction with borrow
|Θ(''n''), Θ(log(''N'')) 
|-
|rowspan=7|[[Multiplication]]
|rowspan=7|Two ''n''-digit numbers&lt;br/&gt;
|rowspan=7|One 2''n''-digit number
|[[Multiplication algorithm#Long multiplication|Schoolbook long multiplication]]
|''O''(''n''&lt;sup&gt;2&lt;/sup&gt;)
|-
|[[Karatsuba algorithm]]
|''O''(''n''&lt;sup&gt;1.585&lt;/sup&gt;)
|-
|3-way [[Toom–Cook multiplication]]
|''O''(''n''&lt;sup&gt;1.465&lt;/sup&gt;)
|-
|''k''-way Toom–Cook multiplication
|''O''(''n''&lt;sup&gt;log (2''k'' − 1)/log ''k''&lt;/sup&gt;)
|-
|Mixed-level Toom–Cook (Knuth 4.3.3-T)&lt;ref&gt;D. Knuth. ''[[The Art of Computer Programming]]'', Volume 2. Third Edition, Addison-Wesley 1997.&lt;/ref&gt;
|''O''(''n'' 2&lt;sup&gt;{{sqrt|2 log ''n''}}&lt;/sup&gt; log ''n'')
|-
|[[Schönhage–Strassen algorithm]]
|''O''(''n'' log ''n'' log log ''n'')
|-
|[[Fürer's algorithm]]&lt;ref&gt;Martin Fürer. ''[http://www.cse.psu.edu/~furer/Papers/mult.pdf Faster Integer Multiplication] {{webarchive|url=https://web.archive.org/web/20130425232048/http://www.cse.psu.edu/~furer/Papers/mult.pdf |date=2013-04-25 }}''. Proceedings of the 39th Annual [[ACM Symposium on Theory of Computing]], San Diego, California, USA, June 11–13, 2007, pp. 55–67.&lt;/ref&gt;
|''O''(''n'' log ''n'' 2&lt;sup&gt;2 [[Iterated logarithm|log*]] ''n''&lt;/sup&gt;)
|-
|rowspan=2|[[Division (mathematics)|Division]]
|rowspan=2|Two ''n''-digit numbers
| rowspan="2" |One ''n''-digit number
|[[Long division|Schoolbook long division]]
|''O''(''n''&lt;sup&gt;2&lt;/sup&gt;)
|-
|[[Newton–Raphson division]]
|''O''(''M''(''n''))
|-
|[[Square root]]
|One ''n''-digit number
|One ''n''-digit number
|[[Newton's method]]
|''O''(''M''(''n''))
|-
|rowspan=3|[[Modular exponentiation]]
|rowspan=3|Two ''n''-digit numbers and a ''k''-bit exponent
|rowspan=3|One ''n''-digit number
|Repeated multiplication and reduction
|''O''(''M''(''n'') 2&lt;sup&gt;''k''&lt;/sup&gt;)
|-
|[[Exponentiation by squaring]]
|''O''(''M''(''n'') ''k'')
|-
|Exponentiation with [[Montgomery reduction]]
|''O''(''M''(''n'') ''k'')
|}

==Algebraic functions==

{| class="wikitable"
!Operation
!Input
!Output
!Algorithm
!Complexity
|-
|rowspan=2|[[Polynomial]] evaluation
|rowspan=2| One polynomial of degree ''n'' with fixed-size polynomial coefficients
|rowspan=2| One fixed-size number
|Direct evaluation
|Θ(''n'')
|-
|[[Horner's method]]
|Θ(''n'')
|-
|rowspan=2|Polynomial gcd (over '''Z'''[''x''] or '''F'''[''x''])
|rowspan=2| Two polynomials of degree ''n'' with fixed-size polynomial coefficients
|rowspan=2| One polynomial of degree at most ''n''
|[[Euclidean algorithm]]
|''O''(''n''&lt;sup&gt;2&lt;/sup&gt;)
|-
|Fast Euclidean algorithm &lt;ref&gt;http://planetmath.org/fasteuclideanalgorithm&lt;/ref&gt;
|''O''(''M''(''n'') log&amp;nbsp;''n'')
|}

==Special functions==
Many of the methods in this section are given in Borwein &amp; Borwein.&lt;ref&gt;J. Borwein &amp; P. Borwein. ''Pi and the AGM: A Study in Analytic Number Theory and Computational Complexity''. John Wiley 1987.&lt;/ref&gt;

===Elementary functions===
The [[elementary function]]s are constructed by composing arithmetic operations, the [[exponential function]] (exp), the [[natural logarithm]] (log), [[trigonometric function]]s (sin, cos), and their inverses. The complexity of an elementary function is equivalent to that of its inverse, since all elementary functions are [[analytic function|analytic]] and hence invertible by means of Newton's method. In particular, if either exp or log in the complex domain can be computed with some complexity, then that complexity is attainable for all other elementary functions.

Below, the size ''n'' refers to the number of digits of precision at which the function is to be evaluated.

{| class="wikitable"
!Algorithm
!Applicability
!Complexity
|-
|[[Taylor series]]; repeated argument reduction (e.g. exp(2''x'') = [exp(''x'')]&lt;sup&gt;2&lt;/sup&gt;) and direct summation
|exp, log, sin, cos, arctan
|''O''(''M''(''n'') ''n''&lt;sup&gt;1/2&lt;/sup&gt;)
|-
|Taylor series; [[Fast Fourier transform|FFT]]-based acceleration
|exp, log, sin, cos, arctan
|''O''(''M''(''n'') ''n''&lt;sup&gt;1/3&lt;/sup&gt; (log ''n'')&lt;sup&gt;2&lt;/sup&gt;)
|-
|Taylor series; [[binary splitting]] + [[bit burst]] method&lt;ref&gt;David and Gregory Chudnovsky. Approximations and complex multiplication according to Ramanujan. ''Ramanujan revisited'', Academic Press, 1988, pp 375–472.&lt;/ref&gt;
|exp, log, sin, cos, arctan
|''O''(''M''(''n'') (log ''n'')&lt;sup&gt;2&lt;/sup&gt;)
|-
|[[Arithmetic–geometric mean]] iteration&lt;ref&gt;Richard P. Brent, [https://arxiv.org/abs/1004.3412 ''Multiple-precision zero-finding methods and the complexity of elementary function evaluation''], in: Analytic Computational Complexity (J. F. Traub, ed.), Academic Press, New York, 1975, 151–176.&lt;/ref&gt;
|exp, log, sin, cos, arctan
|''O''(''M''(''n'') log ''n'')
|}

It is not known whether O(''M''(''n'') log ''n'') is the optimal complexity for elementary functions. The best known lower bound is the trivial bound Ω(''M''(''n'')).

===Non-elementary functions===
{| class="wikitable"
!Function
!Input
!Algorithm
!Complexity
|-
|rowspan=3|[[Gamma function]]
|''n''-digit number
|Series approximation of the [[incomplete gamma function]]
|''O''(''M''(''n'') ''n''&lt;sup&gt;1/2&lt;/sup&gt; (log ''n'')&lt;sup&gt;2&lt;/sup&gt;)
|-
|Fixed rational number
|Hypergeometric series
|''O''(''M''(''n'') (log ''n'')&lt;sup&gt;2&lt;/sup&gt;)
|-
|''m''/24, ''m'' an integer
|[[Arithmetic-geometric mean]] iteration
|''O''(''M''(''n'') log ''n'')
|-
|rowspan=2|[[Hypergeometric function]] ''&lt;sub&gt;p&lt;/sub&gt;F&lt;sub&gt;q&lt;/sub&gt;''
|''n''-digit number
|(As described in Borwein &amp; Borwein)
|''O''(''M''(''n'') ''n''&lt;sup&gt;1/2&lt;/sup&gt; (log ''n'')&lt;sup&gt;2&lt;/sup&gt;)
|-
|Fixed rational number
|Hypergeometric series
|''O''(''M''(''n'') (log ''n'')&lt;sup&gt;2&lt;/sup&gt;)
|}

===Mathematical constants===
This table gives the complexity of computing approximations to the given constants to ''n'' correct digits.
{| class="wikitable"
!Constant
!Algorithm
!Complexity
|-
|[[Golden ratio]], ''φ''
|[[Newton's method]]
|''O''(''M''(''n''))
|-
|[[Square root of 2]], {{sqrt|2}}
|Newton's method
|''O''(''M''(''n''))
|-
|rowspan=2|[[e (mathematical constant)|Euler's number]], ''e''
|[[Binary splitting]] of the Taylor series for the exponential function
|''O''(''M''(''n'') log&amp;nbsp;''n'')
|-
|Newton inversion of the natural logarithm
|''O''(''M''(''n'') log&amp;nbsp;''n'')
|-
|rowspan=2|[[Pi]], ''π''
|Binary splitting of the arctan series in [[Machin's formula]]
|''O''(''M''(''n'') (log&amp;nbsp;''n'')&lt;sup&gt;2&lt;/sup&gt;)
|-
|[[Salamin–Brent algorithm]]
|''O''(''M''(''n'') log&amp;nbsp;''n'')
|-
|[[Euler–Mascheroni constant|Euler's constant]], ''γ''
|Sweeney's method (approximation in terms of the [[exponential integral]])
|''O''(''M''(''n'') (log&amp;nbsp;''n'')&lt;sup&gt;2&lt;/sup&gt;)
|}

==Number theory==
Algorithms for [[number theory|number theoretical]] calculations are studied in [[computational number theory]].

{| class="wikitable"
!Operation
!Input
!Output
!Algorithm
!Complexity
|-
|rowspan=5|[[Greatest common divisor]]
|rowspan=5|Two ''n''-digit numbers
|rowspan=5|One number with at most ''n'' digits
|[[Euclidean algorithm]]
|''O''(''n''&lt;sup&gt;2&lt;/sup&gt;)
|-
|[[Binary GCD algorithm]]
|''O''(''n''&lt;sup&gt;2&lt;/sup&gt;)
|-
|Left/right ''k''-ary binary GCD algorithm&lt;ref&gt;{{cite journal | author = J. Sorenson. | title = Two Fast GCD Algorithms | journal = Journal of Algorithms | volume = 16 | issue = 1| pages = 110–144 | year = 1994 | doi=10.1006/jagm.1994.1006}}&lt;/ref&gt;
|O(''n''&lt;sup&gt;2&lt;/sup&gt;/ log ''n'')
|-
|[[Stehlé–Zimmermann algorithm]]&lt;ref&gt;R. Crandall &amp; C. Pomerance. ''Prime Numbers – A Computational Perspective''. Second Edition, Springer 2005.&lt;/ref&gt;
|''O''(''M''(''n'') log ''n'')
|-
|[[Schönhage controlled Euclidean descent algorithm]]&lt;ref&gt;{{cite journal | author = Möller N | title =  On Schönhage's algorithm and subquadratic integer gcd computation | journal = Mathematics of Computation | volume = 77 | issue = 261 | pages = 589–607 | doi = 10.1090/S0025-5718-07-02017-0 | year = 2008 | url=http://www.lysator.liu.se/~nisse/archive/sgcd.pdf| bibcode = 2008MaCom..77..589M }}&lt;/ref&gt;
|O(''M''(''n'') log ''n'')
|-
|rowspan=3|[[Jacobi symbol]]
|rowspan=3|Two ''n''-digit numbers
|rowspan=3|0, −1, or 1
|-
|Schönhage controlled Euclidean descent algorithm&lt;ref&gt;{{cite web | author = Bernstein D J | title = Faster Algorithms to Find Non-squares Modulo Worst-case Integers | url = http://cr.yp.to/papers/nonsquare.ps}}&lt;/ref&gt;
|''O''(''M''(''n'') log ''n'')
|-
| Stehlé–Zimmermann algorithm&lt;ref&gt;{{cite arXiv|author1=Richard P. Brent|author2=Paul Zimmermann|title=An O(M(n) log n) algorithm for the Jacobi symbol|year=2010|eprint=1004.2091 }}&lt;/ref&gt;
|''O''(''M''(''n'') log ''n'')
|-
|rowspan=3|[[Factorial]]
|rowspan=3|A fixed-size number ''m''
|rowspan=3|One ''O''(''m'' log ''m'')-digit number
|Bottom-up multiplication
|''O''(M(''m''&lt;sup&gt;2&lt;/sup&gt;) log ''m'')
|-
|Binary splitting
|''O''(''M''(''m'' log ''m'') log ''m'')
|-
|Exponentiation of the prime factors of ''m''
|''O''(''M''(''m'' log ''m'') log log ''m''),&lt;ref&gt;{{cite journal | last1 = Borwein | first1 = P. | year = 1985 | title = On the complexity of calculating factorials | url = | journal = Journal of Algorithms | volume = 6 | issue = | pages = 376–380 | doi=10.1016/0196-6774(85)90006-9}}&lt;/ref&gt;&lt;br&gt;O(''M''(''m'' log ''m''))&lt;ref name="Schonhage" /&gt;
|}

==Matrix algebra==
The following complexity figures assume that arithmetic with individual elements has complexity ''O''(1), as is the case with fixed-precision [[floating-point arithmetic]] or operations on a [[finite field]].

{| class="wikitable"
!Operation
!Input
!Output
!Algorithm
!Complexity
|-
|rowspan=4|[[Matrix multiplication algorithm|Matrix multiplication]]
|rowspan=4|Two ''n''×''n'' matrices
|rowspan=4|One ''n''×''n'' matrix
|[[Matrix multiplication algorithm#Iterative algorithm|Schoolbook matrix multiplication]]
|''O''(''n''&lt;sup&gt;3&lt;/sup&gt;)
|-
|[[Strassen algorithm]]
|''O''(''n''&lt;sup&gt;2.807&lt;/sup&gt;)
|-
|[[Coppersmith–Winograd algorithm]]
|''O''(''n''&lt;sup&gt;2.376&lt;/sup&gt;)
|-
|Optimized CW-like algorithms&lt;ref&gt;{{Citation | last1=Davie | first1=A.M. | last2=Stothers | first2=A.J. | title=Improved bound for complexity of matrix multiplication|journal=Proceedings of the Royal Society of Edinburgh|volume=143A|pages=351–370|year=2013|doi=10.1017/S0308210511001648}}&lt;/ref&gt;&lt;ref&gt;{{Citation | last1=Vassilevska Williams | first1=Virginia|authorlink= Virginia Vassilevska Williams | title=Breaking the Coppersmith-Winograd barrier | url=http://theory.stanford.edu/~virgi/matrixmult-f.pdf | year=2011}}&lt;/ref&gt;&lt;ref&gt;{{Citation | last1=Le Gall | first1=François | contribution=Powers of tensors and fast matrix multiplication | year = 2014 | arxiv=1401.7714 | title = Proceedings of the 39th International Symposium on Symbolic and Algebraic Computation ([[ISSAC]] 2014)| bibcode=2014arXiv1401.7714L }}&lt;/ref&gt;
|''O''(''n''&lt;sup&gt;2.373&lt;/sup&gt;)
|-
|Matrix multiplication
|One ''n''×''m'' matrix &amp;

one ''m''×''p'' matrix
|One ''n''×''p'' matrix
|Schoolbook matrix multiplication
|''O''(''nmp'')
|-
|rowspan=4|[[Matrix inversion]]{{ref|blockinversion|{{big|*}}}}
|rowspan=4|One ''n''×''n'' matrix
|rowspan=4|One ''n''×''n'' matrix
|[[Gauss–Jordan elimination]]
|''O''(''n''&lt;sup&gt;3&lt;/sup&gt;)
|-
|Strassen algorithm
|''O''(''n''&lt;sup&gt;2.807&lt;/sup&gt;)
|-
|Coppersmith–Winograd algorithm
|''O''(''n''&lt;sup&gt;2.376&lt;/sup&gt;)
|-
|Optimized CW-like algorithms
|''O''(''n''&lt;sup&gt;2.373&lt;/sup&gt;)
|-
|rowspan=2|[[Singular value decomposition]]
|rowspan=2|One ''m''×''n'' matrix
|One ''m''×''m'' matrix, &lt;br/&gt; one ''m''×''n'' matrix, &amp; &lt;br/&gt; one ''n''×''n'' matrix
|
|O(''mn''&lt;sup&gt;2&lt;/sup&gt;) &lt;br/&gt; (''m'' ≥ ''n'')
|-
|One ''m''×''r'' matrix, &lt;br/&gt; one ''r''×''r'' matrix, &amp; &lt;br/&gt; one ''n''×''r'' matrix
|
|
|-
|rowspan=5|[[Determinant]]
|rowspan=5|One ''n''×''n'' matrix
|rowspan=5|One number
|[[Laplace expansion]]
|''O''(''n''!)
|-
|Division-free algorithm&lt;ref&gt;http://page.mi.fu-berlin.de/rote/Papers/pdf/Division-free+algorithms.pdf&lt;/ref&gt;
|''O''(''n''&lt;sup&gt;4&lt;/sup&gt;)
|-
|[[LU decomposition]]
|''O''(''n''&lt;sup&gt;3&lt;/sup&gt;)
|-
|[[Bareiss algorithm]]
|''O''(''n''&lt;sup&gt;3&lt;/sup&gt;)
|-
|Fast matrix multiplication&lt;ref&gt;{{citation|at=Theorem 6.6, p.&amp;nbsp;241|title=The Design and Analysis of Computer Algorithms|first1=Alfred V.|last1=Aho|author1-link=Alfred Aho|first2=John E.|last2=Hopcroft|author2-link=John Hopcroft|first3=Jeffrey D.|last3=Ullman|author3-link=Jeffrey Ullman|publisher=Addison-Wesley|year=1974}}&lt;/ref&gt;
|''O''(''n''&lt;sup&gt;2.373&lt;/sup&gt;)
|-

|Back substitution
|[[Triangular matrix]]
|''n'' solutions
|Back substitution&lt;ref&gt;J. B. Fraleigh and R. A. Beauregard, "Linear Algebra," Addison-Wesley Publishing Company, 1987, p 95.&lt;/ref&gt;
|''O''(''n&lt;sup&gt;2&lt;/sup&gt;'')
|-
|}

In 2005, [[Henry Cohn]], [[Robert Kleinberg]], [[Balázs Szegedy]], and [[Chris Umans]] showed that either of two different conjectures would imply that the exponent of matrix multiplication is 2.&lt;ref&gt;Henry Cohn, Robert Kleinberg, Balazs Szegedy, and Chris Umans. Group-theoretic Algorithms for Matrix Multiplication. {{arxiv|math.GR/0511460}}. ''Proceedings of the 46th Annual Symposium on Foundations of Computer Science'', 23–25 October 2005, Pittsburgh, PA, IEEE Computer Society, pp. 379–388.&lt;/ref&gt;

{{note|blockinversion|{{big|*}}}}Because of the possibility of [[Invertible matrix#Blockwise inversion|blockwise inverting a matrix]], where an inversion of an {{math|{{var|n}}×{{var|n}}}} matrix requires inversion of two half-sized matrices and six multiplications between two half-sized matrices, and since matrix multiplication has a lower bound of {{math|[[Big O notation#Family of Bachmann–Landau notations|Ω]]({{var|n}}{{sup|2}} log {{var|n}})}} operations,&lt;ref&gt;[[Ran Raz]]. On the complexity of matrix product. In Proceedings of the thirty-fourth annual ACM symposium on Theory of computing. ACM Press, 2002. {{doi|10.1145/509907.509932}}.&lt;/ref&gt; it can be shown that a [[divide and conquer algorithm]] that uses blockwise inversion to invert a matrix runs with the same time complexity as the matrix multiplication algorithm that is used internally.&lt;ref&gt;T.H. Cormen, C.E. Leiserson, R.L. Rivest, C. Stein, ''Introduction to Algorithms'', 3rd ed., MIT Press, Cambridge, MA, 2009, §28.2.&lt;/ref&gt;

==References==
{{reflist|40em}}

==Further reading==
* {{cite book |last1=Brent |first1=Richard P. |authorlink1=Richard P. Brent |last2=Zimmermann |first2=Paul |authorlink2=Paul Zimmermann (mathematician) |title=Modern Computer Arithmetic |year=2010 |publisher=Cambridge University Press |isbn=9780521194693}}
* {{cite book |last1=Knuth |first1=Donald Ervin |authorlink=Donald Knuth |title=The Art of Computer Programming. Volume 2: Seminumerical Algorithms|year=1997| edition=3rd |publisher=Addison-Wesley |isbn=0201896842}}

{{DEFAULTSORT:Computational Complexity Of Mathematical Operations}}
[[Category:Computer arithmetic algorithms]]
[[Category:Computational complexity theory]]
[[Category:Mathematics-related lists]]
[[Category:Number theoretic algorithms]]
[[Category:Unsolved problems in computer science]]</text>
      <sha1>iqoafxn0q94kv093vmsh543nh26b2k1</sha1>
    </revision>
  </page>
  <page>
    <title>Consensus–expectations gap</title>
    <ns>0</ns>
    <id>30879232</id>
    <revision>
      <id>764770930</id>
      <parentid>739532821</parentid>
      <timestamp>2017-02-10T21:00:54Z</timestamp>
      <contributor>
        <username>Loraof</username>
        <id>22399950</id>
      </contributor>
      <comment>/* top */ grammar</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1026">A '''consensus–expectations gap''' is a gap between what a group of decision-makers are expected to agree on, and what they are actually able to agree on. The expression was first used by  [[Asle Toje]] in the book ''The European Union as a small power : after the post-Cold War''.&lt;ref&gt;Toje, A. (2010). The European Union as a small power: After the post-Cold War. New York: Palgrave Macmillan.&lt;/ref&gt; The term owes to [[Christopher J. Hill|Christopher Hill]]'s [[capability–expectations gap]] between what the European Communities had been talked up to do and what the collective was actually able to deliver. Hill saw the capability–expectations gap as having three primary components, namely, the ability to agree, resource availability and the instruments at its disposal. The 'consensus–expectations gap' focuses on one of Hill's variables: the ability to agree.

==References==
{{Reflist}}

{{DEFAULTSORT:Consensus-expectations gap}}
[[Category:Decision-making]]
[[Category:European Union]]


{{probability-stub}}</text>
      <sha1>gvw1hv8u0r2116ev5mcm5x99gu9z1oh</sha1>
    </revision>
  </page>
  <page>
    <title>Convex bipartite graph</title>
    <ns>0</ns>
    <id>23674225</id>
    <revision>
      <id>742022150</id>
      <parentid>740786239</parentid>
      <timestamp>2016-10-01T05:18:52Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>[[Category:Bipartite graphs]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2819">In the [[mathematical]] field of [[graph theory]], a '''convex bipartite graph''' is a [[bipartite graph]] with specific properties.
A bipartite graph, (''U''&amp;nbsp;∪&amp;nbsp;''V'',&amp;nbsp;''E''), is said to be convex over the vertex set ''U'' if ''U'' can be [[enumeration|enumerated]] such that for all ''v''&amp;nbsp;∈&amp;nbsp;''V'' the vertices adjacent to ''v'' are consecutive.

Convexity over ''V'' is defined analogously. A bipartite graph (''U''&amp;nbsp;∪&amp;nbsp;''V'',&amp;nbsp;''E'') that is convex over both ''U'' and ''V'' is said to be '''biconvex''' or '''doubly convex'''.

==Formal definition==
Let ''G''&amp;nbsp;=&amp;nbsp;(''U''&amp;nbsp;∪&amp;nbsp;''V'',&amp;nbsp;''E'') be a bipartite graph, i.e., the vertex set is ''U''&amp;nbsp;∪&amp;nbsp;''V'' where ''U''&amp;nbsp;∩&amp;nbsp;''V''&amp;nbsp;=&amp;nbsp;∅.
Let ''N''&lt;sub&gt;''G''&lt;/sub&gt;(''v'') denote the neighborhood of a vertex ''v''&amp;nbsp;∈&amp;nbsp;''V''. 
The graph ''G'' is '''convex''' over ''U'' if and only if there exists a [[bijective]] mapping, ''f'':&amp;nbsp;''U''&amp;nbsp;→&amp;nbsp;{1, …, |''U''|}, such that for all ''v''&amp;nbsp;∈&amp;nbsp;''V'',
for any two vertices ''x'',''y''&amp;nbsp;∈&amp;nbsp;''N''&lt;sub&gt;''G''&lt;/sub&gt;(''v'')&amp;nbsp;⊆&amp;nbsp;''U'' there does not exist a ''z''&amp;nbsp;∉&amp;nbsp;''N''&lt;sub&gt;''G''&lt;/sub&gt;(''v'') such that ''f''(''x'')&amp;nbsp;&lt;&amp;nbsp;''f''(''z'')&amp;nbsp;&lt;&amp;nbsp;''f''(''y'').

==See also==
*[[Convex plane graph]]

==References==
*{{cite journal|author=W. Lipski Jr.|author2=[[Franco P. Preparata]] |date=August 1981|title=Efficient algorithms for finding maximum matchings in convex bipartite graphs and related problems|journal=Acta Informatica|volume=15|issue=4|pages=329–346|doi=10.1007/BF00264533|url=http://www.springerlink.com/content/u18656lrg6424n3u/|accessdate=2009-07-20}}
*{{cite journal|author=Ten-hwang Lai|author2=Shu-shang Wei|date=April 1997|title=Bipartite permutation graphs with application to the minimum buffer size problem|journal=Discrete Applied Mathematics|volume=74|issue=1|pages=33–55|doi=10.1016/S0166-218X(96)00014-5|url=http://citeseer.ist.psu.edu/old/lai94bipartite.html|accessdate=2009-07-20}}
*{{cite book|title=Efficient graph representations|author=Jeremy P. Spinrad|year=2003|publisher=[[American Mathematical Society|AMS]] Bookstore|isbn= 978-0-8218-2815-1 |page=128|url=https://books.google.com/books?id=RrtXSKMAmWgC&amp;pg=PA128&amp;lpg=PA128&amp;dq=%22a+bipartite+graph+is+a+convex+graph%22|accessdate=2009-07-20}}
*{{cite book|title=Graph classes: a survey|author=[[Andreas Brandstädt]]|author2=Van Bang Le |author3=Jeremy P. Spinrad |year=1999|publisher=[[Society for Industrial and Applied Mathematics|SIAM]]|isbn=978-0-89871-432-6 |page=94|url=https://books.google.com/books?id=es9ZbB6qHRYC&amp;pg=PA94&amp;lpg=PA94&amp;dq=%22convex+if+there+is+an+ordering%22|accessdate=2009-07-20}}

[[Category:Graph families]]
[[Category:Bipartite graphs]]

{{combin-stub}}</text>
      <sha1>gyy9ld6hih4yig2wtoi2j5i0ge9zc0a</sha1>
    </revision>
  </page>
  <page>
    <title>Degasperis–Procesi equation</title>
    <ns>0</ns>
    <id>15054768</id>
    <revision>
      <id>841541827</id>
      <parentid>826304944</parentid>
      <timestamp>2018-05-16T13:36:45Z</timestamp>
      <contributor>
        <username>OAbot</username>
        <id>28481209</id>
      </contributor>
      <minor/>
      <comment>[[Wikipedia:OABOT|Open access bot]]: add arxiv identifier to citation with #oabot.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="16485">In [[mathematical physics]], the '''Degasperis–Procesi equation'''

: &lt;math&gt;\displaystyle u_t - u_{xxt} + 2\kappa u_x + 4u u_x = 3 u_x u_{xx} + u u_{xxx}&lt;/math&gt;

is one of only two [[Exactly solvable model|exactly solvable]] equations in the following family of third-[[Order (differential equation)|order]], non-linear, [[dispersive PDE]]s:

:&lt;math&gt;\displaystyle u_t - u_{xxt} + 2\kappa u_x + (b+1)u u_x = b u_x u_{xx} + u u_{xxx},&lt;/math&gt;

where &lt;math&gt;\kappa&lt;/math&gt; and ''b'' are real parameters (''b''=3 for the Degasperis–Procesi equation). It was discovered by Degasperis and Procesi in a search for [[Integrable system|integrable equation]]s similar in form to the [[Camassa–Holm equation]], which is the other integrable equation in this family (corresponding to ''b''=2); that those two equations are the only integrable cases has been verified using a variety of different integrability tests.&lt;ref&gt;Degasperis &amp; Procesi 1999; Degasperis, Holm &amp; Hone 2002; Mikhailov &amp; Novikov 2002; Hone &amp; Wang 2003; Ivanov 2005&lt;/ref&gt; Although discovered solely because of its mathematical properties, the Degasperis–Procesi equation (with &lt;math&gt;\kappa &gt; 0&lt;/math&gt;) has later been found to play a similar role in [[water wave]] theory as the Camassa–Holm equation.&lt;ref&gt;Johnson 2003; Dullin, Gottwald &amp; Holm 2004; Constantin &amp; Lannes 2007; Ivanov 2007&lt;/ref&gt;

== Soliton solutions ==
{{main|Peakon}}

Among the solutions of the Degasperis–Procesi equation (in the special case &lt;math&gt;\kappa=0&lt;/math&gt;) are the so-called [[peakon|multipeakon]] solutions, which are functions of the form

:&lt;math&gt;\displaystyle u(x,t)=\sum_{i=1}^n m_i(t) e^{-|x-x_i(t)|}&lt;/math&gt;

where the functions &lt;math&gt;m_i&lt;/math&gt; and &lt;math&gt;x_i&lt;/math&gt; satisfy&lt;ref&gt;Degasperis, Holm &amp; Hone 2002&lt;/ref&gt;

:&lt;math&gt;\dot{x}_i = \sum_{j=1}^n m_j e^{-|x_i-x_j|},\qquad \dot{m}_i = 2 m_i \sum_{j=1}^n m_j\, \sgn{(x_i-x_j)} e^{-|x_i-x_j|}.&lt;/math&gt;

These [[Ordinary differential equation|ODEs]] can be solved explicitly in terms of elementary functions, using [[Integrable system#Solitons and inverse spectral methods|inverse spectral methods]].&lt;ref&gt;Lundmark &amp; Szmigielski 2003, 2005&lt;/ref&gt;

When &lt;math&gt;\kappa &gt; 0&lt;/math&gt; the [[soliton]] solutions of the Degasperis–Procesi equation are smooth; they converge to peakons in the limit as &lt;math&gt;\kappa&lt;/math&gt; tends to zero.&lt;ref&gt;Matsuno 2005a, 2005b&lt;/ref&gt;

== Discontinuous solutions ==

The Degasperis–Procesi equation (with &lt;math&gt;\kappa=0&lt;/math&gt;) is formally equivalent to the (nonlocal) [[Hyperbolic partial differential equation#Hyperbolic system and conservation laws|hyperbolic conservation law]]

:&lt;math&gt;
\partial_t u + \partial_x \left[\frac{u^2}{2} + \frac{G}{2} * \frac{3 u^2}{2} \right] = 0,
&lt;/math&gt;

where &lt;math&gt;G(x) = \exp(-|x|)&lt;/math&gt;, and where the star denotes [[convolution]] with respect to ''x''.
In this formulation, it admits [[weak solution]]s with a very low degree of regularity, even discontinuous ones ([[shock wave]]s).&lt;ref&gt;Coclite &amp; Karlsen 2006, 2007; Lundmark 2007; Escher, Liu &amp; Yin 2007&lt;/ref&gt; In contrast, the corresponding formulation of the Camassa–Holm equation contains a convolution involving both &lt;math&gt;u^2&lt;/math&gt; and &lt;math&gt;u_x^2&lt;/math&gt;, which only makes sense if ''u'' lies in the [[Sobolev space]] &lt;math&gt;H^1 = W^{1,2}&lt;/math&gt; with respect to ''x''. By the [[Sobolev embedding theorem]], this means in particular that the weak solutions of the Camassa–Holm equation must be continuous with respect to ''x''.

== Notes ==
{{reflist}}

== References ==
{{refbegin}}
*{{Citation
 |last=Coclite 
 |first=Giuseppe Maria 
 |author-link= 
 |last2=Karlsen 
 |first2=Kenneth Hvistendahl 
 |year=2006 
 |title=On the well-posedness of the Degasperis–Procesi equation 
 |periodical=J. Funct. Anal. 
 |volume=233 
 |issue=1 
 |pages=60–91 
 |url=http://www.math.uio.no/~kennethk/articles/art113_journal.pdf 
 |doi=10.1016/j.jfa.2005.07.008 
}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
*{{Citation
 |last=Coclite 
 |first=Giuseppe Maria 
 |last2=Karlsen 
 |first2=Kenneth Hvistendahl 
 |year=2007 
 |title=On the uniqueness of discontinuous solutions to the Degasperis–Procesi equation 
 |periodical=J. Differential Equations 
 |volume=234 
 |issue=1 
 |pages=142–160 
 |url=http://www.math.uio.no/~kennethk/articles/art122_journal.pdf 
 |doi=10.1016/j.jde.2006.11.008 
 |bibcode=2007JDE...234..142C 
}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
*{{Citation
 | last = Constantin
 | first = Adrian
 | author-link =
 | last2 = Lannes
 | first2 = David
 | year = 2007
 | title = The hydrodynamical relevance of the Camassa–Holm and Degasperis–Procesi equations
 | arxiv = 0709.0905
|bibcode = 2009ArRMA.192..165C |doi = 10.1007/s00205-008-0128-2
 | volume=192
 | journal=Archive for Rational Mechanics and Analysis
 | pages=165–186}}
*{{Citation
 | last = Degasperis
 | first = Antonio
 | author-link =
 | last2 = Holm
 | first2 = Darryl D.
 | last3 = Hone
 | first3 = Andrew N. W.
 | year = 2002
 | title = A new integrable equation with peakon solutions
 | periodical = Theoret. and Math. Phys.
 | volume = 133
 | issue = 2
 | pages = 1463–1474
 | arxiv = nlin.SI/0205023
 | doi = 10.1023/A:1021186408422
}}
*{{Citation
 | last = Degasperis
 | first = Antonio
 | last2 = Procesi
 | first2 = Michela
 | year = 1999
 | contribution = Asymptotic integrability
 | contribution-url = http://web.tiscalinet.it/SPT2001/SPT98papers/degproc98.ps
 | editor-last = Degasperis
 | editor-first = Antonio
 | editor2-last = Gaeta
 | editor2-first = Giuseppe
 | title = Symmetry and Perturbation Theory (Rome, 1998)
 | publication-place = River Edge, NJ
 | publisher = World Scientific
 | pages = 23–37
}}
*{{Citation
 | last = Dullin
 | first = Holger R.
 | last2 = Gottwald
 | first2 = Georg A.
 | last3 = Holm
 | first3 = Darryl D.
 | year = 2004
 | title = On asymptotically equivalent shallow water wave equations
 | periodical = Physica D
 | volume = 190
 | issue =
 | pages = 1–14
 | arxiv =nlin.PS/0307011
 | doi = 10.1016/j.physd.2003.11.004
|bibcode = 2004PhyD..190....1D }}
*{{Citation
 | last = Escher
 | first = Joachim
 | last2 = Liu
 | first2 = Yue
 | last3 = Yin
 | first3 = Zhaoyang
 | year = 2007
 | title = Shock waves and blow-up phenomena for the periodic Degasperis–Procesi equation
 | periodical = Indiana Univ. Math. J.
 | volume = 56
 | issue = 1
 | pages = 87–117
 | url = http://www.iumj.indiana.edu/IUMJ/ftdload.php?year=2007&amp;volume=56&amp;artid=3040&amp;ext=pdf
 | doi=10.1512/iumj.2007.56.3040
}}
*{{Citation
 | last = Hone 
 | first = Andrew N. W.
 | year = 2003
 | last2 = Wang
 | first2 = Jing Ping
 | title = Prolongation algebras and Hamiltonian operators for peakon equations
 | periodical = Inverse Problems
 | volume = 19
 | issue = 1
 | pages = 129–145
 | url = 
 | doi = 10.1088/0266-5611/19/1/307
|bibcode = 2003InvPr..19..129H }}
*{{Citation
 | last = Ivanov 
 | first = Rossen
 | year = 2005
 | title = On the integrability of a class of nonlinear dispersive wave equations
 | periodical = J. Nonlin. Math. Phys.
 | volume = 12
 | issue = 4
 | pages = 462–468
 | url = 
 | doi = 10.2991/jnmp.2005.12.4.2
|bibcode = 2005JNMP...12..462R | arxiv = nlin/0606046}}
*{{Citation
 | last = Ivanov 
 | first = Rossen
 | year = 2007
 | title = Water waves and integrability
 | periodical = Phil. Trans. R. Soc. A
 | volume = 365
 | issue = 1858
 | pages = 2267–2280
 | url = 
 | doi = 10.1098/rsta.2007.2007
|bibcode = 2007RSPTA.365.2267I |arxiv = 0707.1839 }}
*{{Citation
 | last = Johnson
 | first = Robin S.
 | year = 2003 
 | title = The classical problem of water waves: a reservoir of integrable and nearly-integrable equations
 | periodical = J. Nonlin. Math. Phys.
 | volume = 10
 | issue = Supplement 1
 | pages = 72–92
 | doi = 10.2991/jnmp.2003.10.s1.6
|bibcode = 2003JNMP...10S..72J }}
*{{Citation
 |last=Lundmark 
 |first=Hans 
 |year=2007 
 |title=Formation and dynamics of shock waves in the Degasperis–Procesi equation 
 |periodical=J. Nonlinear Sci. 
 |volume=17 
 |issue=3 
 |pages=169–198 
 |url=https://dissem.in/p/28857173/formation-and-dynamics-of-shock-waves-in-the-degasperis-procesi-equation
 |doi=10.1007/s00332-006-0803-3 
 |bibcode=2007JNS....17..169L 
}}
*{{Citation
 | last = Lundmark
 | first = Hans
 | last2 = Szmigielski
 | first2 = Jacek
 | year = 2003
 | title = Multi-peakon solutions of the Degasperis–Procesi equation
 | periodical = Inverse Problems
 | volume = 19
 | issue = 6
 | pages = 1241–1245
 | arxiv = nlin.SI/0503033
 | doi = 10.1088/0266-5611/19/6/001
|bibcode = 2003InvPr..19.1241L }}
*{{Citation
 | last = Lundmark
 | first = Hans
 | last2 = Szmigielski
 | first2 = Jacek
 | year = 2005
 | title = Degasperis–Procesi peakons and the discrete cubic string
 | periodical = Internat. Math. Res. Papers
 | volume = 2005
 | issue = 2
 | pages = 53–116
 | arxiv = nlin.SI/0503036
 | doi = 10.1155/IMRP.2005.53
}}
*{{Citation
 | last = Matsuno
 | first = Yoshimasa
 | year = 2005a
 | title = Multisoliton solutions of the Degasperis–Procesi equation and their peakon limit
 | periodical = Inverse Problems
 | volume = 21
 | issue = 5
 | pages = 1553–1570
 | doi = 10.1088/0266-5611/21/5/004
|arxiv = nlin/0511029 |bibcode = 2005InvPr..21.1553M }}
*{{Citation
 | last = Matsuno
 | first = Yoshimasa
 | year = 2005b
 | title = The ''N''-soliton solution of the Degasperis–Procesi equation
 | periodical = Inverse Problems
 | volume = 21
 | issue = 6
 | pages = 2085–2101
 | arxiv = nlin.SI/0511029
 | doi = 10.1088/0266-5611/21/6/018
|bibcode = 2005InvPr..21.2085M }}
*{{Citation
 | last = Mikhailov 
 | first = Alexander V.
 | last2 = Novikov
 | first2 = Vladimir S.
 | year = 2002
 | title = Perturbative symmetry approach
 | periodical = J. Phys. A: Math. Gen.
 | volume = 35
 | issue = 22
 | pages = 4775–4790
 | arxiv = nlin.SI/0203055v1
 | doi = 10.1088/0305-4470/35/22/309
|bibcode = 2002JPhA...35.4775M }}
*{{citation 
| last=Liao 
| first=S.J. | authorlink = Liao Shijun
| title= Do peaked solitary water waves indeed exist? 
| journal=Communications in Nonlinear Science and Numerical Simulation
|year=2013 
| doi=10.1016/j.cnsns.2013.09.042
|arxiv = 1204.3354 |bibcode = 2014CNSNS..19.1792L
 | volume=19
 | pages=1792–1821}}
{{refend}}

== Further reading ==
{{hidden begin
|toggle = left
|bodystyle = font-size: 100%
|title = 
}}
{{refbegin}}
*{{Citation
 |last=Coclite 
 |first=Giuseppe Maria 
 |author-link= 
 |last2=Karlsen 
 |first2=Kenneth Hvistendahl 
 |last3=Risebro 
 |first3=Nils Henrik 
 |year=2008 
 |title=Numerical schemes for computing discontinuous solutions of the Degasperis–Procesi equation 
 |periodical=IMA J. Numer. Anal. 
 |volume=28 
 |issue=1 
 |pages=80–105 
 |url=http://www.math.uio.no/~kennethk/articles/art125.pdf 
 |issn= 
 |doi=10.1093/imanum/drm003 
 |oclc= 
 |accessdate= 
}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
*{{Citation
 | last = Escher 
 | first = Joachim
 | year = 2007
 | title = Wave breaking and shock waves for a periodic shallow water equation
 | periodical = Phil. Trans. R. Soc. A
 | volume = 365
 | issue = 1858
 | pages = 2281–2289
 | url = 
 | doi = 10.1098/rsta.2007.2008
|bibcode = 2007RSPTA.365.2281E }}
*{{Citation
 | last = Escher
 | first = Joachim
 | last2 = Liu
 | first2 = Yue
 | last3 = Yin
 | first3 = Zhaoyang
 | year = 2006
 | title = Global weak solutions and blow-up structure for the Degasperis–Procesi equation
 | periodical = J. Funct. Anal.
 | volume = 241
 | issue = 2
 | pages = 457–485 
 | doi = 10.1016/j.jfa.2006.03.022 
}}
*{{Citation
 | last = Escher 
 | first = Joachim
 | last2 = Yin 
 | first2 = Zhaoyang
 | year = 2007
 | title = On the initial boundary value problems for the Degasperis–Procesi equation
 | periodical = Phys. Lett. A
 | volume = 368
 | issue = 1–2
 | pages = 69–76
 | url = 
 | doi = 10.1016/j.physleta.2007.03.073
|bibcode = 2007PhLA..368...69E }}
*{{Citation
 | last = Guha
 | first = Parta
 | year = 2007
 | title = Euler–Poincaré formalism of (two component) Degasperis–Procesi and Holm–Staley type systems
 | periodical = J. Nonlin. Math. Phys.
 | volume = 14
 | issue = 3
 | pages = 390–421
 | doi = 10.2991/jnmp.2007.14.3.8
|bibcode = 2007JNMP...14..390G }}
*{{Citation
 | last = Henry
 | first = David
 | year = 2005
 | title = Infinite propagation speed for the Degasperis–Procesi equation
 | periodical = J. Math. Anal. Appl.
 | volume = 311
 | issue = 2
 | pages = 755–759
 | doi = 10.1016/j.jmaa.2005.03.001
|bibcode = 2005JMAA..311..755H }}
*{{Citation
 | last = Hoel
 | first = Håkon A.
 | author-link =
 | year = 2007
 | title = A numerical scheme using multi-shockpeakons to compute solutions of the Degasperis–Procesi equation
 | periodical = Electron. J. Differential Equations
 | volume = 2007
 | issue = 100
 | pages = 1–22
 | url = http://ejde.math.txstate.edu/Volumes/2007/100/hoel.pdf
 | doi = 
}}
*{{Citation
 | last = Lenells
 | first = Jonatan
 | year = 2005
 | title = Traveling wave solutions of the Degasperis–Procesi equation
 | periodical = J. Math. Anal. Appl.
 | volume = 306
 | issue = 1
 | pages = 72–82
 | url =
 | doi = 10.1016/j.jmaa.2004.11.038
|bibcode = 2005JMAA..306...72L }}
*{{Citation
 | last = Lin
 | first = Zhiwu
 | last2 = Liu
 | first2 = Yue
 | year = 2008
 | title = Stability of peakons for the Degasperis–Procesi equation
 | periodical = Comm. Pure Appl. Math.
 | volume = 62
 | issue = 1
 | pages = 125–146
 | arxiv = 0712.2007
 | doi = 10.1002/cpa.20239
}}
*{{Citation
 |last=Liu 
 |first=Yue 
 |last2=Yin 
 |first2=Zhaoyang 
 |year=2006 
 |title=Global existence and blow-up phenomena for the Degasperis–Procesi equation 
 |periodical=Comm. Math. Phys. 
 |volume=267 
 |issue=3 
 |pages=801–820 
 |url=http://www.mittag-leffler.se/preprints/0506f/info.php?id=22 
 |doi=10.1007/s00220-006-0082-5 
 |bibcode=2006CMaPh.267..801L 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20061011122559/http://www.mittag-leffler.se/preprints/0506f/info.php?id=22 
 |archivedate=2006-10-11 
 |df= 
}}
*{{Citation
 | last = Liu
 | first = Yue
 | last2 = Yin
 | first2 = Zhaoyang
 | year = 2007
 | title = On the blow-up phenomena for the Degasperis–Procesi equation
 | periodical = Internat. Math. Res. Notices
 | volume = 2007
 | issue = 
 | pages = 
 | doi = 10.1093/imrn/rnm117
}}
*{{Citation
 | last = Mustafa
 | first = Octavian G.
 | year = 2005
 | title = A note on the Degasperis–Procesi equation
 | periodical = J. Nonlin. Math. Phys.
 | volume = 12
 | issue = 1
 | pages = 10–14
 | doi = 10.2991/jnmp.2005.12.1.2
|bibcode = 2005JNMP...12...10M }}
*{{Citation
 |last=Vakhnenko 
 |first=Vyacheslav O. 
 |last2=Parkes 
 |first2=E. John 
 |year=2004 
 |title=Periodic and solitary-wave solutions of the Degasperis–Procesi equation 
 |periodical=Chaos, Solitons and Fractals 
 |volume=20 
 |issue=5 
 |pages=1059–1073 
 |url=http://www.maths.strath.ac.uk/~caas35/v&amp;pCSF04.pdf 
 |doi=10.1016/j.chaos.2003.09.043 
 |bibcode=2004CSF....20.1059V 
}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
*{{Citation
 | last = Yin 
 | first = Zhaoyang
 | year = 2003a
 | title = Global existence for a new periodic integrable equation
 | periodical = J. Math. Anal. Appl.
 | volume = 283
 | issue = 1
 | pages = 129–139
 | url = 
 | doi = 10.1016/S0022-247X(03)00250-6
}}
*{{Citation
 |last=Yin 
 |first=Zhaoyang 
 |year=2003b 
 |title=On the Cauchy problem for an integrable equation with peakon solutions 
 |periodical=Illinois J. Math. 
 |volume=47 
 |issue=3 
 |pages=649–666. 
 |url=http://www.math.uiuc.edu/~hildebr/ijm/fall03/final/yin.html 
 |doi= 
}}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}
*{{Citation
 | last = Yin 
 | first = Zhaoyang
 | year = 2004a
 | title = Global solutions to a new integrable equation with peakons
 | periodical = Indiana Univ. Math. J.
 | volume = 53
 | issue = 4
 | pages = 1189–1209
 | url = 
 | doi = 10.1512/iumj.2004.53.2479
}}
*{{Citation
 | last = Yin 
 | first = Zhaoyang
 | year = 2004b
 | title = Global weak solutions for a new periodic integrable equation with peakon solutions
 | periodical = J. Funct. Anal.
 | volume = 212
 | issue = 1
 | pages = 182–194
 | url = 
 | doi = 10.1016/j.jfa.2003.07.010
}}
{{refend}}
{{hidden end}}

{{DEFAULTSORT:Degasperis-Procesi equation}}
[[Category:Mathematical physics]]
[[Category:Solitons]]
[[Category:Partial differential equations]]
[[Category:Equations of fluid dynamics]]</text>
      <sha1>jyf4t8re31lx53cceq9u186nmjc7a54</sha1>
    </revision>
  </page>
  <page>
    <title>Dominant functor</title>
    <ns>0</ns>
    <id>35243831</id>
    <revision>
      <id>687838193</id>
      <parentid>687838190</parentid>
      <timestamp>2015-10-28T01:37:44Z</timestamp>
      <contributor>
        <username>ClueBot NG</username>
        <id>13286072</id>
      </contributor>
      <minor/>
      <comment>Reverting possible vandalism by [[Special:Contribs/Cxitlynscott|Cxitlynscott]] to version by David Eppstein. [[WP:CBFP|Report False Positive?]] Thanks, [[WP:CBNG|ClueBot NG]]. (2409132) (Bot)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="654">{{orphan|date=April 2012}}

In [[category theory]], an abstract branch of mathematics, a '''dominant functor''' is a [[functor]] ''F''&amp;nbsp;:&amp;nbsp;''C''&amp;nbsp;→&amp;nbsp;''D'' in which every object of ''D'' is a [[Deformation retract|retract]] of an object of the form ''F''(''x'') for some object ''X'' of&amp;nbsp;''C''.&lt;ref&gt;{{citation|journal=Applied Categorical Structures|date=March 2014|title=On normal tensor functors and coset decompositions for fusion categories|first1=A.|last1=Bruguières|first2=Sebastian|last2=Burciu|doi=10.1007/s10485-014-9371-x|arxiv=1210.3922}}.&lt;/ref&gt;

==References==
{{reflist}}

[[Category:Functors]]


{{categorytheory-stub}}</text>
      <sha1>391favolfkh283usewdbfbmg7lopek4</sha1>
    </revision>
  </page>
  <page>
    <title>Ehrenfeucht–Fraïssé game</title>
    <ns>0</ns>
    <id>1040475</id>
    <revision>
      <id>804670684</id>
      <parentid>787006555</parentid>
      <timestamp>2017-10-10T12:44:30Z</timestamp>
      <contributor>
        <username>Neils51</username>
        <id>16416757</id>
      </contributor>
      <minor/>
      <comment>/* Main idea */ gr</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8344">In the mathematical discipline of [[model theory]], the '''Ehrenfeucht–Fraïssé game''' (also called back-and-forth games)
is a technique for determining whether two [[structure (mathematical logic)|structure]]s 
are [[elementarily equivalent]]. The main application of Ehrenfeucht–Fraïssé games is in proving the inexpressibility of certain properties in first-order logic. Indeed, Ehrenfeucht–Fraïssé games provide a complete methodology for proving inexpressibility results for first-order logic. In this role, these games are of particular importance in [[finite model theory]] and its applications in computer science (specifically [[Computer Aided Verification]] and [[database theory]]), since Ehrenfeucht–Fraïssé games are one of the few techniques from model theory that remain valid in the context of finite models. Other widely used techniques for proving inexpressibility results, such as the [[compactness theorem]], do not work in finite models.

Ehrenfeucht–Fraïssé like games can also be defined for other logics, such as [[fixpoint logic]]s&lt;ref&gt;{{cite book | title=Computer Science Logic: 6th Workshop, CSL'92, San Miniato, Italy, September 28 - October 2, 1992. Selected Papers | volume=702 | series=Lecture Notes in Computer Science | editor1-first=Egon | editor1-last=Börger | publisher=[[Springer-Verlag]] | year=1993 | zbl=0808.03024 | isbn=3-540-56992-8 | pages=100–114 | first=Uwe | last=Bosse | chapter=An Ehrenfeucht–Fraïssé game for fixpoint logic and stratified fixpoint logic }}&lt;/ref&gt; and [[pebble game]]s for finite variable logics; extensions are powerful enough to characterise definability in [[existential second-order logic]].

== Main idea ==
The main idea behind the game is that we have two structures, and two players (defined below). One of the players wants to show that the two structures are different, whereas the other player wants to show that they are [[elementarily equivalent]] (satisfy the same first-order sentences). The game is played in turns and rounds; A round proceeds as follows: First the first player (Spoiler) chooses any element from one of the structures, and the other player chooses an element from the other structure. The other player's task is to always pick an element that is "similar" to the one that Spoiler chose. The second player (Duplicator) wins if there exists an isomorphism between the elements chosen in the two different structures.

The game lasts for a fixed number of steps (&lt;math&gt;\gamma&lt;/math&gt;) (an ordinal, but usually a finite number or &lt;math&gt;\omega&lt;/math&gt;).

== Definition ==
Suppose that we are given two structures &lt;math&gt;\mathfrak{A}&lt;/math&gt; 
and &lt;math&gt;\mathfrak{B}&lt;/math&gt;, each with no [[function (mathematics)|function]] symbols and the same set of [[Relation (mathematics)|relation]] symbols, 
and a fixed [[natural number]] ''n''.  We can then define the Ehrenfeucht–Fraïssé 
game &lt;math&gt;G_n(\mathfrak{A},\mathfrak{B})&lt;/math&gt; to be a game between two players, Spoiler and Duplicator, 
played as follows:
# The first player, Spoiler, picks either a member &lt;math&gt;a_1&lt;/math&gt; of &lt;math&gt;\mathfrak{A}&lt;/math&gt; or a member &lt;math&gt;b_1&lt;/math&gt; of &lt;math&gt;\mathfrak{B}&lt;/math&gt;.
# If Spoiler picked a member of &lt;math&gt;\mathfrak{A}&lt;/math&gt;, Duplicator picks a member &lt;math&gt;b_1&lt;/math&gt; of &lt;math&gt;\mathfrak{B}&lt;/math&gt;; otherwise, Duplicator picks a member &lt;math&gt;a_1&lt;/math&gt; of &lt;math&gt;\mathfrak{A}&lt;/math&gt;.
# Spoiler picks either a member &lt;math&gt;a_2&lt;/math&gt; of &lt;math&gt;\mathfrak{A}&lt;/math&gt; or a member &lt;math&gt;b_2&lt;/math&gt; of &lt;math&gt;\mathfrak{B}&lt;/math&gt;.
# Duplicator picks an element &lt;math&gt;a_2&lt;/math&gt; or &lt;math&gt;b_2&lt;/math&gt; in the model from which Spoiler did not pick.
# Spoiler and Duplicator continue to pick members of &lt;math&gt;\mathfrak{A}&lt;/math&gt; and &lt;math&gt;\mathfrak{B}&lt;/math&gt; for &lt;math&gt;n-2&lt;/math&gt; more steps.
# At the conclusion of the game, we have chosen distinct elements &lt;math&gt;a_1, \dots, a_n&lt;/math&gt; of &lt;math&gt;\mathfrak{A}&lt;/math&gt; and &lt;math&gt;b_1, \dots, b_n&lt;/math&gt; of &lt;math&gt;\mathfrak{B}&lt;/math&gt;.  We therefore have two structures on the set &lt;math&gt;\{1, \dots,n\}&lt;/math&gt;, one induced from &lt;math&gt;\mathfrak{A}&lt;/math&gt; via the map sending &lt;math&gt;i&lt;/math&gt; to &lt;math&gt;a_i&lt;/math&gt;, and the other induced from  &lt;math&gt;\mathfrak{B}&lt;/math&gt; via the map sending &lt;math&gt;i&lt;/math&gt; to &lt;math&gt;b_i&lt;/math&gt;.  Duplicator wins if these structures are the same; Spoiler wins if they are not.

For each ''n'' we define a relation &lt;math&gt;\mathfrak{A} \stackrel{n}{\sim} \mathfrak{B}&lt;/math&gt; if Duplicator wins the ''n''-move game &lt;math&gt;G_n(\mathfrak{A},\mathfrak{B})&lt;/math&gt;.  These are all equivalence relations on the class of structures with the given relation symbols.  The intersection of all these relations is again an equivalence relation &lt;math&gt;\mathfrak{A} \sim \mathfrak{B}&lt;/math&gt;.

It is easy to prove that if Duplicator wins this game for all ''n'', that is, &lt;math&gt;\mathfrak{A} \sim \mathfrak{B}&lt;/math&gt;, then &lt;math&gt;\mathfrak{A}&lt;/math&gt; and &lt;math&gt;\mathfrak{B}&lt;/math&gt; are elementarily equivalent.  If the set of relation symbols being considered is finite, the converse is also true.

== History ==

The [[back-and-forth method]] used in the Ehrenfeucht–Fraïssé game to verify elementary equivalence was given by [[Roland Fraïssé]] 
in his thesis;&lt;ref&gt;''Sur une nouvelle classification des systèmes de relations'', Roland Fraïssé, ''Comptes Rendus'' '''230''' (1950), 1022&amp;ndash;1024.&lt;/ref&gt;&lt;ref&gt;''Sur quelques classifications des systèmes de relations'', Roland Fraïssé, thesis, Paris, 1953;
published in ''Publications Scientifiques de l'Université d'Alger'', series A '''1''' (1954), 35&amp;ndash;182.&lt;/ref&gt;
it was formulated as a game by [[Andrzej Ehrenfeucht]].&lt;ref&gt;An application of games to the completeness problem for formalized theories, A. Ehrenfeucht, ''Fundamenta Mathematicae'' '''49''' (1961), 129&amp;ndash;141.&lt;/ref&gt; The names Spoiler and Duplicator are due to [[Joel Spencer]].&lt;ref&gt;[http://plato.stanford.edu/entries/logic-games/ Stanford Encyclopedia of Philosophy, entry on Logic and Games.]&lt;/ref&gt; Other usual names are Eloise [sic] and Abelard (and often denoted by &lt;math&gt;\exists&lt;/math&gt; and &lt;math&gt;\forall&lt;/math&gt;) after [[Heloise and Abelard]], a naming scheme introduced by [[Wilfrid Hodges]] in his book ''Model Theory'', or alternatively Eve and Adam.

== Further reading ==

Chapter 1 of [[Bruno Poizat|Poizat]]'s model theory text&lt;ref&gt;''A Course in Model Theory'', Bruno Poizat, tr. Moses Klein, New York: 
Springer-Verlag, 2000.&lt;/ref&gt; contains an introduction to the Ehrenfeucht–Fraïssé game, and so do Chapters 6, 7, and 13 of Rosenstein's book on [[Linear ordering|linear orders]].&lt;ref&gt;''Linear Orderings'', Joseph G. Rosenstein, New York: Academic Press, 1982.&lt;/ref&gt;  A simple example of the Ehrenfeucht–Fraïssé game is given in one of Ivars Peterson's MathTrek columns .&lt;ref&gt;[https://www.sciencenews.org/article/subtle-logic-winning-game Example of the Ehrenfeucht-Fraïssé game.]&lt;/ref&gt;

Phokion Kolaitis' slides&lt;ref&gt;[http://www.cs.ucsc.edu/~kolaitis/talks/essllif.ps Course on combinatorial games in finite model theory by Phokion Kolaitis (.ps file)]&lt;/ref&gt; and [[Neil Immerman]]'s book chapter&lt;ref name="Immerman1999"&gt;{{cite book|author=Neil Immerman|title=Descriptive complexity|url=https://books.google.com/books?id=kWSZ0OWnupkC&amp;pg=PA91|year=1999|publisher=Springer|isbn=978-0-387-98600-5}}, chapter 6&lt;/ref&gt; on Ehrenfeucht–Fraïssé games discuss applications in computer science, the methodology theorem for proving inexpressibility results, and several simple inexpressibility proofs using this methodology.

== References ==
&lt;references /&gt;
* {{cite book | last1=Grädel | first1=Erich | last2=Kolaitis | first2=Phokion G. | last3=Libkin | first3=Leonid | last4=Maarten | first4=Marx | last5=Spencer | first5=Joel | author5-link=Joel Spencer | last6=Vardi | first6=Moshe Y. | author6-link=Moshe Y. Vardi | last7=Venema | first7=Yde | last8=Weinstein | first8=Scott | title=Finite model theory and its applications | zbl=1133.03001 | series=Texts in Theoretical Computer Science. An EATCS Series | location=Berlin | publisher=[[Springer-Verlag]] | isbn=978-3-540-00428-8 | year=2007 }}

==External links==
* [http://www.math.cornell.edu/~mec/Summer2009/Raluca/index.html Six Lectures Ehrenfeucht-Fraïssé games] at MATH EXPLORERS' CLUB, Cornell Department of Mathematics.

{{DEFAULTSORT:Ehrenfeucht-Fraisse Game}}
[[Category:Model theory]]</text>
      <sha1>8hqk7q11lb2eo7qju30bz6tc3ywdx3s</sha1>
    </revision>
  </page>
  <page>
    <title>Graph labeling</title>
    <ns>0</ns>
    <id>1609861</id>
    <revision>
      <id>797936539</id>
      <parentid>785246716</parentid>
      <timestamp>2017-08-30T01:45:42Z</timestamp>
      <contributor>
        <username>Izno</username>
        <id>2927383</id>
      </contributor>
      <comment>clean per [[Special:LintErrors/tidy-whitespace-bug]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7975">In the [[mathematical]] discipline of [[graph theory]], a '''graph labeling''' is the assignment of labels, traditionally represented by [[integers]], to the [[edge (graph theory)|edges]] or [[vertex (graph theory)|vertices]], or both, of a [[Graph (discrete mathematics)|graph]].&lt;ref name=mathw&gt;{{mathworld|LabeledGraph|Labeled graph}}&lt;/ref&gt;

Formally, given a graph {{nowrap|1=''G'' = (''V'', ''E'')}}, a '''vertex labeling''' is a function of ''V'' to a set of ''labels''. A graph with such a function defined is called a '''vertex-labeled graph'''. Likewise, an '''edge labeling''' is a function of ''E'' to a set of labels. In this case, the graph is called an '''edge-labeled graph'''.

When the edge labels are members of an ordered set (e.g., the [[real number]]s), it may be called a '''weighted graph'''.

When used without qualification, the term '''labeled graph''' generally refers to a vertex-labeled graph with all labels distinct. Such a graph may equivalently be labeled by the consecutive integers {{nowrap|{1, …, {{abs|''V''}}}}}, where {{abs|''V''}} is the number of vertices in the graph.&lt;ref name=mathw/&gt; For many applications, the edges or vertices are given labels that are meaningful in the associated domain. For example, the edges may be assigned [[Weighted graph|weights]] representing the "cost" of traversing between the incident vertices.&lt;ref&gt;"Different Aspects of Coding Theory", by Robert Calderbank (1995) {{ISBN|0-8218-0379-4}}, [https://books.google.com/books?id=TcOzdq3nDp4C&amp;pg=PA57&amp;dq=%22labeled+graph%22&amp;lr=#PPA53,M1 p. 53]"&lt;/ref&gt;

In the above definition a graph is understood to be a finite undirected simple graph. However, the notion of labeling may be applied to all extensions and generalizations of graphs. For example, in [[automata theory]] and [[formal language]] theory it is convenient to consider labeled [[multigraph]]s, i.e., a pair of vertices may be connected by several labeled edges.&lt;ref&gt;"[[Developments in Language Theory]]", Proc. 9th. Internat.Conf., 2005, {{ISBN|3-540-26546-5}}, [https://books.google.com/books?id=QPgojKbuuUEC&amp;pg=PA314&amp;dq=%22labeled+graph%22#PPA313,M1 p. 313] &lt;/ref&gt;

==History==
Most graph labelings trace their origins to labelings presented by Alex Rosa in his 1967 paper.&lt;ref name="Gallian"&gt;{{cite journal|author=Gallian, J.|title=A Dynamic Survey of Graph Labelings, 1996-2005|publisher=The Electronic Journal of Combinatorics}}&lt;/ref&gt; Rosa identified three types of labelings, which he called α-, β-, and ρ-labelings.&lt;ref name="Rosa"&gt;{{cite journal|author=Rosa, A.|title=On certain valuations of vertices in a graph}}&lt;/ref&gt; β-labelings were later renamed ''graceful'' by [[Solomon Golomb|S. W. Golomb]] and the name has been popular since.

==Special cases==
===Graceful labeling===
{{main|Graceful labeling}}
[[File:Graceful labeling.svg|300px|thumb|A graceful labeling. Vertex labels are in black, edge labels in red.]]

A graph is known as graceful when its vertices are labeled from 0 to {{abs|''E''}}, the size of the graph, and this labeling induces an edge labeling from 1 to {{abs|''E''}}. For any edge ''e'', the label of ''e'' is the positive difference between the two vertices incident with ''e''. In other words, if ''e'' is incident with vertices labeled ''i'' and ''j'', ''e'' will be labeled {{abs|''i'' − ''j''}}. Thus, a graph {{nowrap|1=''G'' = (''V'', ''E'')}} is graceful if and only if there exists an injection that induces a bijection from ''E'' to the positive integers up to {{abs|''E''}}.

In his original paper, Rosa proved that all [[eulerian graph]]s with size [[Equivalence relation|equivalent]] to 1 or 2 ([[Modular arithmetic|mod]] 4) are not graceful. Whether or not certain families of graphs are graceful is an area of graph theory under extensive study. Arguably, the largest unproven conjecture in graph labeling is the Ringel–Kotzig conjecture, which hypothesizes that all trees are graceful. This has been proven for all [[Path graph|paths]], [[Caterpillar tree|caterpillars]], and many other infinite families of trees. Kotzig himself has called the effort to prove the conjecture a "disease."&lt;ref&gt;{{cite journal | zbl=1163.05007 | last=Vietri | first=Andrea | title=Sailing towards, and then against, the graceful tree conjecture: some promiscuous results | journal=Bull. Inst. Comb. Appl. | volume=53 | pages=31–46 | year=2008 | issn=1183-1278 }}&lt;/ref&gt;

===Edge-graceful labeling===
{{main|Edge-graceful labeling}}
An ''edge-graceful labeling'' on a simple graph (no loops or multiple edges) on ''p'' vertices and ''q'' edges is a labelling of the edges by distinct integers in {{nowrap|{1, …, ''q''}}} such that the labeling on the vertices induced by labeling a vertex with the sum of the incident edges taken modulo ''p'' assigns all values from 0 to {{nowrap|''p'' − 1}} to the vertices. A graph ''G'' is said to be ''edge-graceful'' if it admits an edge-graceful labeling.

Edge-graceful labelings were first introduced by S. Lo in 1985.&lt;ref&gt;{{cite journal | zbl=0597.05054 | last=Lo | first=Sheng-Ping | title=On edge-graceful labelings of graphs | conference=Proc. Conf., Sundance/Utah 1985 | journal=Congressus Numerantium | volume=50 | year=1985 | pages=231–241}}&lt;/ref&gt;

A necessary condition for a graph to be edge-graceful is ''Lo's condition'':
: &lt;math&gt;q(q + 1) = p/(p - 1)2 \mod p.&lt;/math&gt;

===Harmonious labeling===
A ''harmonious labeling'' on a graph ''G'' is an injection from the vertices of ''G'' to the [[Group (mathematics)|group]] of integers [[Modular arithmetic|modulo]] ''k'', where ''k'' is the number of edges of ''G'', that induces a bijection between the edges of ''G'' and the numbers modulo ''k'' by taking the edge label for an edge (''x'', ''y'') to be the sum of the labels of the two vertices ''x'', ''y'' (mod ''k''). A ''harmonious graph'' is one that has a harmonious labeling. Odd cycles are harmonious, as is the [[Petersen graph]]. It is conjectured that trees are all harmonious if one vertex label is allowed to be reused.&lt;ref&gt;Guy (2004) pp.190–191&lt;/ref&gt;
The seven-page [[Book (graph theory)|book graph]] {{math|''K''&lt;sub&gt;1,7&lt;/sub&gt; &amp;times; ''K''&lt;sub&gt;2&lt;/sub&gt;}} provides an example of a graph that is not harmonious.&lt;ref&gt;{{citation
 | last = Gallian | first = Joseph A. | author-link = Joseph Gallian
 | journal = [[Electronic Journal of Combinatorics]]
 | mr = 1668059
 | page = Dynamic Survey 6
 | title = A dynamic survey of graph labeling
 | url = http://www.combinatorics.org/ojs/index.php/eljc/article/view/DS6
 | volume = 5
 | year = 1998}}.&lt;/ref&gt;

===Graph coloring===
{{main|Graph coloring}}
A ''graph coloring'' is a subclass of graph labelings. A ''vertex coloring'' assigns different labels to adjacent vertices; an ''edge colouring'' assigns different labels to adjacent edges.

===Lucky labeling===
A ''lucky labeling'' of a graph ''G'' is an assignment of positive integers to the vertices of ''G'' such that if ''S''(''v'') denotes the sum of the labels on the neighbours of ''v'', then ''S'' is a vertex coloring of ''G''. The ''lucky number'' of ''G'' is the least ''k'' such that ''G'' has a lucky labeling with the integers {{nowrap|{1, …, ''k''}}}.&lt;ref&gt;{{cite journal | zbl=1197.05125 | last1=Czerwiński | first1=Sebastian | last2=Grytczuk | first2=Jarosław | last3=Ẓelazny | first3=Wiktor | title=Lucky labelings of graphs | journal=Inf. Process. Lett. | volume=109 | number=18 | pages=1078–1081 | year=2009 }}&lt;/ref&gt;

==References==
{{reflist}}
* {{cite conference | zbl=0193.53204 | last=Rosa | first=A. | title=On certain valuations of the vertices of a graph | conference=Theory of Graphs, Int. Symp. Rome July 1966 | pages=349–355 | year=1967 | publisher=Gordon and Breach }}
* {{cite book | last=Guy | first=Richard K. | authorlink=Richard K. Guy | title=Unsolved problems in number theory | publisher=[[Springer-Verlag]] |edition=3rd | year=2004 |isbn=0-387-20860-7 | at=C13 | zbl=1058.11001 }}

[[Category:Extensions and generalizations of graphs]]</text>
      <sha1>0orfbz1zo6frevx2gysp3plfscji6fd</sha1>
    </revision>
  </page>
  <page>
    <title>Idealizer</title>
    <ns>0</ns>
    <id>24575820</id>
    <revision>
      <id>713088767</id>
      <parentid>597642032</parentid>
      <timestamp>2016-04-01T21:44:14Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor/>
      <comment>/* References */Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3404">In [[abstract algebra]], the '''idealizer''' of a subsemigroup ''T'' of a [[semigroup]] ''S'' is the largest subsemigroup of ''S'' in which ''T'' is an [[Semigroup#Subsemigroups and ideals|ideal]].{{sfn|Mikhalev|2002|loc=p.30}} Such an idealizer is given by

:&lt;math&gt;\mathbb{I}_S(T)=\{s\in S \mid sT\subseteq T \text{ and } Ts\subseteq T\}&lt;/math&gt;

In [[ring theory]], if ''A'' is an additive subgroup of a [[ring (mathematics)|ring]] ''R'', then &lt;math&gt;\mathbb{I}_R(A)&lt;/math&gt; (defined in the multiplicative semigroup of ''R'') is the largest subring of ''R'' in which ''A'' is a two-sided ideal.{{sfn|Goodearl|1976|loc=p.121}}{{sfn|Levy|Robson|2011|loc=p.7}}

In [[Lie algebra]], if ''L'' is a [[Lie ring]] (or [[Lie algebra]]) with Lie product [''x'',''y''], and ''S'' is an additive subgroup of ''L'', then the set

:&lt;math&gt;\{r\in L\mid [r,S]\subseteq S\}&lt;/math&gt;

is classically called the '''[[normalizer]]''' of ''S'', however it is apparent that this set is actually the Lie ring equivalent of the idealizer. It is not necessary to mention that [''S'',''r'']⊆''S'', because [[anticommutativity]] of the Lie product causes [''s'',''r'']&amp;nbsp;=&amp;nbsp;−[''r'',''s'']∈''S''. The Lie "normalizer" of ''S'' is the largest subring of ''S'' in which ''S'' is a Lie ideal.

==Comments==

Often, when right or left ideals are the additive subgroups of ''R'' of interest, the idealizer is defined more simply by taking advantage of the fact that multiplication by ring elements is already absorbed on one side. Explicitly,
:&lt;math&gt;\mathbb{I}_R(T)=\{r\in R \mid rT\subseteq T \}&lt;/math&gt;
if ''T'' is a right ideal, or
:&lt;math&gt;\mathbb{I}_R(L)=\{r\in R \mid Lr\subseteq L \}&lt;/math&gt;
if ''L'' is a left ideal.

In [[commutative algebra]], the idealizer is related to a more general construction. Given a commutative ring ''R'', and given two subsets ''A'' and ''B'' of an ''R'' module ''M'', the '''conductor''' or '''transporter''' is given by 
:&lt;math&gt;(A:B):=\{r\in R \mid Br\subseteq A\}&lt;/math&gt;.
In terms of this conductor notation, an additive subgroup ''B'' of ''R'' has idealizer 
:&lt;math&gt;\mathbb{I}_R(B)=(B:B)&lt;/math&gt;.

When ''A'' and ''B'' are ideals of ''R'', the conductor is part of the structure of the [[residuated lattice]] of ideals of ''R''.

;Examples
The [[multiplier algebra]] ''M''(''A'') of a ''C''&lt;sup&gt;*&lt;/sup&gt;-algebra ''A'' is isomorphic to the idealizer of ''&amp;pi;''(''A'') where ''&amp;pi;'' is any faithful nondegenerate representation of ''A'' on a [[Hilbert space]]&amp;nbsp;''H''.

==Notes==
{{Reflist}}

==References==
*{{citation   |last=Goodearl|first=K. R.   |title=Ring theory: Nonsingular rings and modules   |series=Pure and Applied Mathematics, No. 33   |publisher=Marcel Dekker Inc.   |place=New York   |year=1976   |pages=viii+206   |mr=0429962}}
*{{citation |last1=Levy|first1=Lawrence S. |last2=Robson |first2=J. Chris|title=Hereditary Noetherian prime rings and idealizers |series=Mathematical Surveys and Monographs |volume=174 |publisher=American Mathematical Society |place=Providence, RI |year=2011 |pages=iv+228 |isbn=978-0-8218-5350-4|mr=2790801}}
*{{citation |title=The concise handbook of algebra |editor1=Mikhalev, Alexander V.  |editor2=Pilz, Günter F. |publisher=Kluwer Academic Publishers |place=Dordrecht |year=2002 |pages=xvi+618 |isbn=0-7923-7072-4 |mr=1966155}}

[[Category:Abstract algebra]]
[[Category:Group theory]]
[[Category:Ring theory]]


{{Abstract-algebra-stub}}</text>
      <sha1>m6zuqd6ckzptynd2ftq2fcjj3b2qpgn</sha1>
    </revision>
  </page>
  <page>
    <title>Identity element</title>
    <ns>0</ns>
    <id>14962</id>
    <revision>
      <id>840321304</id>
      <parentid>840320942</parentid>
      <timestamp>2018-05-09T03:47:01Z</timestamp>
      <contributor>
        <username>Imaginatorium</username>
        <id>158856</id>
      </contributor>
      <comment>/* top */ more rewording</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6039">{{single source|date=July 2013}}
In [[mathematics]], an '''identity element''' or '''neutral element''' is a special type of element of a [[Set (mathematics)|set]] with respect to a [[binary operation]] on that set, which leaves other elements unchanged when combined with them. This concept is used in [[algebraic structure]]s such as [[group (mathematics)|group]]s and [[ring (mathematics)|ring]]s. The term ''identity element'' is often shortened to ''identity'' (as will be done in this article) when there is no possibility of confusion, but the identity implicitly depends on the binary operation it is associated with.

Let {{math|(''S'', ∗)}} be a set&amp;nbsp;{{mvar|S}} with a binary operation&amp;nbsp;∗ on it. Then an element&amp;nbsp;{{mvar|e}} of&amp;nbsp;{{mvar|S}} is called a '''[[left and right (algebra)|left]] identity''' if {{math|1=''e'' ∗ ''a'' = ''a''}} for all&amp;nbsp;{{mvar|a}} in&amp;nbsp;{{mvar|S}}, and a '''[[left and right (algebra)|right]] identity''' if {{math|1=''a'' ∗ ''e'' = ''a''}} for all&amp;nbsp;{{mvar|a}} in&amp;nbsp;{{mvar|S}}. If {{mvar|e}} is both a left identity and a right identity, then it is called a '''two-sided identity''', or simply an '''identity'''.

An identity with respect to addition is called an [[additive identity]] (often denoted as&amp;nbsp;0) and an identity with respect to multiplication is called a [[multiplicative identity]] (often denoted as&amp;nbsp;1). These need not be ordinary addition and multiplication, but rather arbitrary operations. The distinction is used most often for sets that support both binary operations, such as [[ring (mathematics)|ring]]s and [[field (mathematics)|field]]s. The multiplicative identity is often called '''unity''' in the latter context (a ring with unity). This should not be confused with a [[unit (ring theory)|unit]] in ring theory, which is any element having a [[multiplicative inverse]]. Unity itself is necessarily a unit.

==Examples==
{| class="wikitable"
! Set !! Operation !! Identity
|-
| [[Real number]]s || + ([[addition]]) || [[0 (number)|0]]
|-
| Real numbers || · ([[multiplication]]) || [[1 (number)|1]]
|-
| [[Positive integer]]s || [[Least common multiple]] || 1
|-
| [[Non-negative integer]]s || [[Greatest common divisor]] || 0 (under most definitions of GCD)
|-
&lt;!-- ||' ''R'''&lt;sup&gt;''n''&lt;/sup&gt; || · (multiplication) || [[1 (number)|1]] --&gt;
| {{mvar|m}}-by-{{mvar|n}} [[matrix (mathematics)|matrices]] || + (addition)
| [[Zero matrix]]
|-
| {{mvar|n}}-by-{{mvar|n}} square matrices || [[Matrix multiplication]]
| ''I''&lt;sub&gt;''n''&lt;/sub&gt; ([[identity matrix]])
|-
| {{mvar|m}}-by-{{mvar|n}} matrices || ○ ([[Hadamard product (matrices)|Hadamard product]])
| {{math|''J''&lt;sub&gt;''m'', ''n''&lt;/sub&gt;}} ([[matrix of ones]])
|-
| All [[function (mathematics)|functions]] from a set,&amp;nbsp;{{mvar|M}}, to itself || ∘ ([[function composition]]) || [[Identity function]]
|-
| All [[distribution (mathematics)|distributions]] on a [[group (mathematics)|group]],&amp;nbsp;{{mvar|G}}&lt;!-- a crap refurbished --&gt; || ∗ ([[convolution]]) || {{math|''δ''}} ([[Dirac delta]])
|-
| [[Extended real number]]s || [[Minimum]]/infimum || +∞
|-
| Extended real numbers || [[Maximum]]/supremum || −∞
|-
| Subsets of a [[Set (mathematics)|set]]&amp;nbsp;{{mvar|M}} || ∩ ([[set intersection|intersection]]) || {{mvar|M}}
|-
| Sets || ∪ ([[set union|union]]) || ∅ ([[empty set]])
|-
| [[string (computer science)|Strings]], [[tuple|lists]] || [[Concatenation]] || [[Empty string]], empty list
|-
| A [[Boolean algebra (structure)|Boolean algebra]] || ∧ ([[logical and]]) || ⊤ (truth)
|-
| A Boolean algebra || ∨ ([[logical or]]) || ⊥ (falsity)
|-
| A Boolean algebra || ⊕ ([[exclusive or]]) || ⊥ (falsity)
|-
| [[knot (mathematics)|Knots]] || [[Knot sum]] || [[Unknot]]
|-
| [[Compact surfaces]] || # ([[connected sum]]) || [[sphere|''S''&lt;sup&gt;2&lt;/sup&gt;]]
|-
| Two elements, {{math|{''e'', ''f''} }}
| ∗ defined by&lt;br&gt; {{math|1=''e'' ∗ ''e'' = ''f'' ∗ ''e'' = ''e''}} and &lt;br&gt; {{math|1=''f'' ∗ ''f'' = ''e'' ∗ ''f'' = ''f''}}
| Both {{mvar|e}} and {{mvar|f}} are left identities,&lt;br&gt; but there is no right identity&lt;br&gt; and no two-sided identity
|}

==Properties==
As the last example (a [[semigroup]]) shows, it is possible for {{math|(''S'', ∗)}} to have several left identities. In fact, every element can be a left identity. Similarly, there can be several right identities. But if there is both a right identity and a left identity, then they are equal and there is just a single two-sided identity. To see this, note that if {{mvar|l}} is a left identity and {{mvar|r}} is a right identity then {{math|1=''l'' = ''l'' ∗ ''r'' = ''r''}}. In particular, there can never be more than one two-sided identity.  If there were two, {{mvar|e}} and {{mvar|f}}, then {{math|''e'' ∗ ''f''}} would have to be equal to both {{mvar|e}} and {{mvar|f}}.

It is also quite possible for {{math|(''S'', ∗)}} to have ''no'' identity element. A common example of this is the [[cross product]] of [[Euclidean vector|vectors]]; in this case, the absence of an identity element is related to the fact that the [[Direction (geometry)|direction]] of any nonzero cross product is always [[orthogonal]] to any element multiplied&amp;nbsp;– so that it is not possible to obtain a non-zero vector in the same direction as the original. Another example would be the additive [[semigroup]] of [[Positive number|positive]] [[natural number]]s.

==See also==
* [[Absorbing element]]
* [[Additive inverse]]
* [[Inverse element]]
* [[Monoid]]
* [[Pseudo-ring #Properties weaker than having an identity|Pseudo-ring]]
* [[Quasigroup]]
* [[Unital (disambiguation)]]

==References==
* M. Kilp, U. Knauer, A.V. Mikhalev, ''Monoids, Acts and Categories with Applications to Wreath Products and Graphs'', De Gruyter Expositions in Mathematics vol.&amp;nbsp;29, Walter de Gruyter, 2000, {{ISBN|3-11-015248-7}}, p.&amp;nbsp;14–15

[[Category:Abstract algebra]]
[[Category:Algebra]]
[[Category:Binary operations|*Identity element]]
[[Category:1 (number)]]

__NOTOC__</text>
      <sha1>50qg8crxecdyicagy7417mi39sdmpm6</sha1>
    </revision>
  </page>
  <page>
    <title>Iterated logarithm</title>
    <ns>0</ns>
    <id>1038048</id>
    <revision>
      <id>849578249</id>
      <parentid>823162174</parentid>
      <timestamp>2018-07-09T23:40:06Z</timestamp>
      <contributor>
        <username>Matthiaspaul</username>
        <id>13467261</id>
      </contributor>
      <comment>+link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4653">{{For|the theorem in probability theory|Law of the iterated logarithm}}
In [[computer science]], the '''iterated logarithm''' of ''n'', written {{log-star}}&amp;nbsp;''n'' (usually read "'''log star'''"), is the number of times the [[logarithm]] function must be  [[iteration|iteratively]] applied before the result is less than or equal to 1. The simplest formal definition is the result of this [[recurrence relation]]:

:&lt;math&gt;
  \log^* n :=
  \begin{cases}
    0                  &amp; \mbox{if } n \le 1; \\
    1 + \log^*(\log n) &amp; \mbox{if } n &gt; 1
   \end{cases}
 &lt;/math&gt;

On the [[positive real numbers]], the continuous [[super-logarithm]] (inverse [[tetration]]) is essentially equivalent:
:&lt;math&gt;\log^* n = \lceil \mathrm {slog}_e(n) \rceil&lt;/math&gt;
but on the negative real numbers, log-star is 0, whereas &lt;math&gt;\lceil \text{slog}_e(-x)\rceil = -1&lt;/math&gt; for positive ''x'', so the two functions differ for negative arguments.

[[Image:Iterated logarithm.png|right|300px|thumb|'''Figure 1.''' Demonstrating log* 4 = 2]]
In computer science, {{lg-star}} is often used to indicate the binary iterated logarithm, which iterates the [[binary logarithm]] instead. The iterated logarithm accepts any positive [[real number]] and yields an [[integer]]. Graphically, it can be understood as the number of "zig-zags" needed in Figure 1 to reach the interval [0, 1] on the ''x''-axis.

Mathematically, the iterated logarithm is well-defined not only for base 2 and base ''e'', but for any base greater than &lt;math&gt;1&lt;/math&gt;.

==Analysis of algorithms==
The iterated logarithm is useful in [[analysis of algorithms]] and [[computational complexity theory|computational complexity]], appearing in the time and space complexity bounds of some algorithms such as:

* Finding the [[Delaunay triangulation]] of a set of points knowing the [[Euclidean minimum spanning tree]]: randomized [[Big-O notation|O]](''n''&amp;nbsp;{{log-star}}&amp;nbsp;''n'') time&lt;ref&gt;Olivier Devillers, "Randomization yields simple O(n log* n) algorithms for difficult ω(n) problems.". ''International Journal of Computational Geometry &amp; Applications'' '''2''':01 (1992), pp. 97–111.&lt;/ref&gt;
* [[Fürer's algorithm]] for integer multiplication: O(''n''&amp;nbsp;log&amp;nbsp;''n''&amp;nbsp;2&lt;sup&gt;''O''({{lg-star}}&amp;nbsp;''n'')&lt;/sup&gt;)
* Finding an approximate maximum (element at least as large as the median): {{lg-star}}&amp;nbsp;''n'' − 4 to {{lg-star}}&amp;nbsp;''n'' + 2 parallel operations&lt;ref&gt;Noga Alon and Yossi Azar, "Finding an Approximate Maximum". ''SIAM Journal on Computing'' '''18''':2 (1989), pp. 258–267.&lt;/ref&gt;
* Richard Cole and [[Uzi Vishkin]]'s [[Graph coloring#Parallel and distributed algorithms|distributed algorithm for 3-coloring an ''n''-cycle]]: ''O''({{log-star}}&amp;nbsp;''n'') synchronous communication rounds.&lt;ref&gt;Richard Cole and Uzi Vishkin: "Deterministic coin tossing with applications to optimal parallel list ranking", Information and Control 70:1(1986), pp. 32–53.&lt;/ref&gt;&lt;ref&gt;{{Introduction to Algorithms|1}} Section 30.5.&lt;/ref&gt;
* Performing weighted quick-union with path compression &lt;ref&gt;https://www.cs.princeton.edu/~rs/AlgsDS07/01UnionFind.pdf&lt;/ref&gt;

The iterated logarithm grows at an extremely slow rate, much slower than the logarithm itself. For all values of ''n'' relevant to counting the running times of algorithms implemented in practice (i.e., ''n''&amp;nbsp;≤&amp;nbsp;2&lt;sup&gt;65536&lt;/sup&gt;, which is far more than the estimated number of atoms in the known universe), the iterated logarithm with base 2 has a value no more than 5.

{|class=wikitable
! ''x'' !! {{lg-star}}&amp;nbsp;''x''
|-
| (−∞, 1] || 0
|-
| (1, 2] || 1
|-
| (2, 4] || 2
|-
| (4, 16] || 3
|-
| (16, 65536] || 4
|-
| (65536, 2&lt;sup&gt;65536&lt;/sup&gt;] || 5
|}

Higher bases give smaller iterated logarithms. Indeed, the only function commonly used in complexity theory that grows more slowly is the [[Ackermann function#Inverse|inverse Ackermann function]].

==Other applications==
The iterated logarithm is closely related to the [[generalized logarithm function]] used in [[symmetric level-index arithmetic]]. It is also proportional to the additive [[persistence of a number]], the number of times someone must replace the number by the sum of its digits before reaching its [[digital root]].

Santhanam&lt;ref&gt;[http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.22.2392 On Separators, Segregators and Time versus Space]&lt;/ref&gt; shows that [[DTIME]] and [[NTIME]] are distinct up to &lt;math&gt;n\sqrt{\log^*n}.&lt;/math&gt;

==Notes==

{{reflist}}

==References==

*{{Introduction to Algorithms|2|chapter=3.2: Standard notations and common functions|pages=55–56}}

[[Category:Asymptotic analysis]]
[[Category:Logarithms]]</text>
      <sha1>9l69i8rn6tx9kouuqbg4g7buvqypmyu</sha1>
    </revision>
  </page>
  <page>
    <title>Johann Friedrich Hennert</title>
    <ns>0</ns>
    <id>14829789</id>
    <revision>
      <id>809771484</id>
      <parentid>806650054</parentid>
      <timestamp>2017-11-11T08:39:55Z</timestamp>
      <contributor>
        <username>Omnipaedista</username>
        <id>8524693</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2341">{{Infobox scientist
| name              = Johann Friedrich Hennert
| image             = J F Hennert 01.jpg 
| image_size        = 
| caption           = 
| birth_date        = {{birth date|1733|10|19|df=y}}
| birth_place       = [[Berlin]], [[Kingdom of Prussia|Prussia]]
| death_date        = {{death date and age|1813|03|30|1733|10|19|df=y}}
| death_place       = [[Utrecht]]
| nationality       = [[Holy Roman Empire|German]]
| fields            = [[Mathematics]]&lt;br&gt;[[Physics]]
| workplaces        = [[University of Utrecht]]
| alma_mater        = 
| doctoral_advisor  = [[Leonhard Euler]]&lt;br&gt;[[Joseph-Nicolas Delisle]]
| doctoral_students =
| known_for         = 
| awards            = 
}}
'''Johann Friedrich Hennert''' (19 October 1733 – 30 March 1813) was [[Germany|German]]-born and lectured in [[mathematics]] and [[physics]] at the [[University of Utrecht]]. He was a significant student of [[Leonhard Euler]]. He was known for his inclination towards the [[United Kingdom|British]] school of [[philosophy]].

==Work==
Hennert held the chair of mathematics at the [[University of Utrecht]] until 1805.

Hennert was an important figure in the history of [[Netherlands|Dutch]] mathematics. He wrote a number of textbooks on [[differential calculus]].
[[File:Acta Eruditorum - IV geometria, 1761 – BEIC 13448843.jpg|thumb|Illustration of ''Problemata de centro aequilibrii potentiarum obliquarum vecti adplicatarum. ...'' from ''[[Acta Eruditorum]]'', 1761]]
[[Jan van Swinden]] was one of his most important students.

==References==
*{{Cite book |first=Klaas |last=van Berkel  | authorlink1 = Klaas van Berkel |first2=Albert |last2=van Helden |lastauthoramp=yes |first3=L. C. |last3=Palm |title=A History of Science in the Netherlands: Survey, Themes and Reference |location=Leiden |publisher=Brill |year=1998 |isbn=90-04-10006-7 }}
*Helmers Dini M., [http://www.inghist.nl/Onderzoek/Projecten/DVN/lemmata/data/Timmerman Timmerman, Petronella Johanna de], in: Digitaal Vrouwenlexicon van Nederland.

==External links==
*{{MathGenealogy |id=112812 }}

{{Authority control}}

{{DEFAULTSORT:Hennert, Johann Friedrich}}
[[Category:18th-century German mathematicians]]
[[Category:19th-century German mathematicians]]
[[Category:Number theorists]]
[[Category:1733 births]]
[[Category:1813 deaths]]
[[Category:Mathematical analysts]]</text>
      <sha1>65oz6y3bvik49wy58sgok36w40j4opd</sha1>
    </revision>
  </page>
  <page>
    <title>John M. Ball</title>
    <ns>0</ns>
    <id>3578468</id>
    <revision>
      <id>859461420</id>
      <parentid>830310117</parentid>
      <timestamp>2018-09-14T05:53:42Z</timestamp>
      <contributor>
        <username>Smasongarrison</username>
        <id>16185737</id>
      </contributor>
      <comment>/* top */copy edit with [[Wikipedia:AutoWikiBrowser/General_fixes|General fixes]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5442">{{Use dmy dates|date=March 2018}}
{{Use British English|date=March 2018}}
{{Infobox scientist
|name              = John M. Ball
|image             = Sir John Ball.jpg
|caption           = Sir John Macleod Ball
|birth_date        = {{birth date and age|1948|05|19|df=y}}&lt;ref name="cv"&gt;{{cite web|url=http://people.maths.ox.ac.uk/~ball/Files/JMB%20CV%2029.01.09.pdf|title=CV|publisher=John M. Ball|accessdate=2009-07-30}}&lt;/ref&gt;
|birth_place       = [[Farnham]], [[Surrey]]
|death_date        = 
|death_place       = 
|residence         = [[Oxford]]
|nationality       = [[United Kingdom|British]]
|fields            = 
|workplaces        = [[Heriot-Watt University]]&lt;br /&gt;[[University of Oxford]]
|alma_mater        = [[University of Cambridge]]&lt;br /&gt;[[University of Sussex]]
|doctoral_advisor  = [[David Eric Edmunds]]&lt;ref&gt;{{MathGenealogy|id=27014}}&lt;/ref&gt;
|awards            = [[Sir Edmund Whittaker Memorial Prize|Whittaker Prize]] (1981)&lt;br/&gt;[[Junior Whitehead Prize]] {{small|(1982)}}&lt;br/&gt;[[David Crighton Medal]]&lt;br/&gt;[[Sylvester Medal]] &lt;small&gt;(2009)&lt;br/&gt;[[King Faisal International Prize]] (2018)}}

'''Sir John Macleod Ball''' {{postnominals|country=GBR|FRS|FRSE}} (born 1948) is [[Sedleian Professor of Natural Philosophy]] at the [[University of Oxford]]. He was the President of the [[International Mathematical Union]] from 2003–06 and a Fellow of [[The Queen's College, Oxford|Queen's College, Oxford]]. He was educated at the [[University of Cambridge]] and [[Sussex University]], and prior to taking up his Oxford post was a professor of mathematics at [[Heriot-Watt University]] in [[Edinburgh]].

Ball's research interests include [[Elasticity (physics)|elasticity]], the [[calculus of variations]], and infinite-dimensional [[dynamical system]]s.  He was [[Knight Bachelor|knighted]] in the [[New Year Honours 2006|New Year Honours list]] for 2006 "for services to Science".&lt;ref&gt;{{cite web|url=http://news.bbc.co.uk/1/shared/bsp/hi/pdfs/30_12_05_hons.pdf|title=Central Chancery of the Orders of Knighthood|publisher=BBC|accessdate=2009-07-30}}&lt;/ref&gt; He is a member of the [[Norwegian Academy of Science and Letters]]&lt;ref&gt;{{cite web|url=http://www.dnva.no/c26849/artikkel/vis.html?tid=40116 |title=Gruppe 1: Matematiske fag |publisher=[[Norwegian Academy of Science and Letters]] |language=Norwegian |accessdate=7 October 2010 |deadurl=yes |archiveurl=https://web.archive.org/web/20131110152102/http://www.dnva.no/c26849/artikkel/vis.html?tid=40116 |archivedate=10 November 2013 |df= }}&lt;/ref&gt; and a fellow of the [[American Mathematical Society]].&lt;ref&gt;[http://www.ams.org/profession/fellows-list List of Fellows of the American Mathematical Society], retrieved 2012-11-03.&lt;/ref&gt;

He was a member of the first Abel Prize Committee in 2002&lt;ref&gt;http://www.abelprize.no/c53678/artikkel/vis.html?tid=56487&lt;/ref&gt; and for the [[Fields Medal]] Committee in 1998. From 1996 - 1998 he was president of the [[London Mathematical Society]], and from 2003 - 2006 he was president of the [[International Mathematical Union]], IMU. In October 2011 he was elected on the Executive Board of [[ICSU]] for a three-year period starting January 2012. Ball is listed as an [[ISI highly cited researcher]].&lt;ref&gt;{{cite web|title=List of ISI highly cited researchers|url=http://highlycited.com}}&lt;/ref&gt;

Along with [[Stuart S. Antman]] he won the [[Theodore von Kármán Prize]] in 1999.&lt;ref name="biosketch"&gt;[http://www.math.umd.edu/~ssa/cvshort.pdf Biographical sketch], retrieved 2014-12-20.&lt;/ref&gt; In 2018 he received the [[King Faisal International Prize]] in Mathematics.&lt;ref&gt;[http://kfip.org/en King Faisal International Prize 2018]&lt;/ref&gt;

Ball received an Honorary Doctorate from [[Heriot-Watt University]] in 1998.&lt;ref&gt;{{Cite web|url=http://www.ma.hw.ac.uk/maths/annualrep/AnnualReport1998.html|title=Annual Report 1998|website=www.ma.hw.ac.uk|access-date=2016-04-04}}&lt;/ref&gt;

He was elected a Fellow of [[Royal Society of Edinburgh|The Royal Society of Edinburgh]] in 1980.&lt;ref&gt;{{Cite news|url=https://www.rse.org.uk/fellow/john-ball/|title=Professor Sir John Macleod Ball FRS FRSE - The Royal Society of Edinburgh|work=The Royal Society of Edinburgh|access-date=2017-12-20|language=en-GB}}&lt;/ref&gt;

==Personal life==
Ball is married to Sedhar Chozam (Lady Sedhar Ball), a [[Tibet]]an-born [[actress]]. He has three children.&lt;ref name="cv" /&gt;

==References==
{{Reflist|2}}

==External links==
*[http://www.maths.ox.ac.uk/~ball/ Ball's home page]
{{Sedleian Professors of Natural Philosophy}}
{{John von Neumann Lecturers}}
{{FRS 1989}}
{{Authority control}}

{{DEFAULTSORT:Ball, John M.}}
[[Category:20th-century British mathematicians]]
[[Category:21st-century British mathematicians]]
[[Category:Living people]]
[[Category:Knights Bachelor]]
[[Category:Fellows of the American Mathematical Society]]
[[Category:Fellows of the Royal Society]]
[[Category:Fellows of the Royal Society of Edinburgh]]
[[Category:Fellows of St John's College, Cambridge]]
[[Category:Fellows of The Queen's College, Oxford]]
[[Category:David Crighton medalists]]
[[Category:Alumni of the University of Sussex]]
[[Category:Alumni of St John's College, Cambridge]]
[[Category:Academics of Heriot-Watt University]]
[[Category:ISI highly cited researchers]]
[[Category:1948 births]]
[[Category:Members of the Norwegian Academy of Science and Letters]]
[[Category:Whitehead Prize winners]]
[[Category:Sedleian Professors of Natural Philosophy]]


{{UK-mathematician-stub}}</text>
      <sha1>rzbe8izv8mek7n55opw3l417qf14kbq</sha1>
    </revision>
  </page>
  <page>
    <title>Klaus Wagner</title>
    <ns>0</ns>
    <id>16988587</id>
    <revision>
      <id>814878064</id>
      <parentid>714124932</parentid>
      <timestamp>2017-12-11T13:13:55Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 0 sources and tagging 1 as dead. #IABot (v1.6.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5102">{{for|the German equestrian|Klaus Wagner (equestrian)}}
[[File:Wagner and Harary.jpg|thumb|240px|Klaus Wagner (right) and [[Frank Harary]] at Oberwolfach, 1972]]
'''Klaus Wagner''' (March 31, 1910 – February 6, 2000) was a [[Germans|German]] [[mathematician]]. He studied [[topology]] at the [[University of Cologne]] under the supervision of [[Karl Dörge]],&lt;sup&gt;([[:de:Karl Dörge|de]])&lt;/sup&gt; who had been a student of [[Issai Schur]]. Wagner received his Ph.D. in 1937, and taught at Cologne for many years himself.&lt;ref&gt;{{mathgenealogy|name=Klaus Wagner|id=19958}}&lt;/ref&gt; In 1970, he moved to the [[University of Duisburg]], where he remained until his retirement in 1978.

Wagner was honored in 1990 by a [[festschrift]] on graph theory,&lt;ref&gt;{{citation
 | editor-last = Bodendieck | editor-first = Rainer
 | isbn = 978-3-411-14301-6
 | location = Mannheim
 | publisher = Bibliographisches Institut, Wissenschaftsverlag
 | title = Contemporary Methods in Graph Theory: In honour of Prof. Dr. Klaus Wagner
 | year = 1990}}.&lt;/ref&gt; and in June 2000, following Wagner's death, the University of Cologne hosted a Festkolloquium in his memory.&lt;ref&gt;[http://elib.zib.de/pub/opt-net/documents/v00w23n2 Conference announcement.]{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;

==Graph minors==
[[File:Wagner graph ham.svg|thumb|160px|The [[Wagner graph]], an eight-vertex [[Möbius ladder]] arising in Wagner's characterization of ''K''&lt;sub&gt;5&lt;/sub&gt;-free graphs.]]
Wagner is known for his contributions to [[graph theory]] and particularly the theory of [[graph minor]]s, graphs that can be formed from a larger graph by contracting and removing edges.

[[Wagner's theorem]] characterizes the [[planar graph]]s as exactly those graphs that do not have as a minor either a [[complete graph]] ''K''&lt;sub&gt;5&lt;/sub&gt; on five vertices or a [[complete bipartite graph]] ''K''&lt;sub&gt;3,3&lt;/sub&gt; with three vertices on each side of its bipartition. That is, these two graphs are the only minor-minimal non-planar graphs. It is closely related to, but should be distinguished from, [[Kuratowski's theorem]], which states that the planar graphs are exactly those graphs that do not contain as a subgraph a [[Homeomorphism (graph theory)|subdivision]] of ''K''&lt;sub&gt;5&lt;/sub&gt; or ''K''&lt;sub&gt;3,3&lt;/sub&gt;.

Another result of his, also known as Wagner's theorem, is that a [[k-vertex-connected graph|four-connected graph]] is planar if and only if it has no ''K''&lt;sub&gt;5&lt;/sub&gt; minor. This implies a characterization of the graphs with no ''K''&lt;sub&gt;5&lt;/sub&gt; minor as being constructed from planar graphs and [[Wagner graph]] (an eight-vertex [[Möbius ladder]]) by [[clique-sum]]s, operations that glue together subgraphs at [[clique]]s of up to three vertices and then possibly remove edges from those cliques. This characterization was used by Wagner to show that the case ''k'' = 5 of the [[Hadwiger conjecture (graph theory)|Hadwiger conjecture]] on the chromatic number of ''K&lt;sub&gt;k&lt;/sub&gt;''-minor-free graphs is equivalent to the [[four color theorem]]. Analogous characterizations of other families of graphs in terms of the summands of their clique-sum decompositions have since become standard in graph minor theory.

Wagner conjectured in the 1930s (although this conjecture was not published until later)&lt;ref&gt;{{citation|last=Casselman|first=Bill|title=Variations on Graph Minor|publisher=American Mathematical Society|url=http://www.ams.org/featurecolumn/archive/gminor.html}}.&lt;/ref&gt; that in any infinite set of graphs, one graph is [[Graph isomorphism|isomorphic]] to a minor of another. The truth of this conjecture implies that any family of graphs closed under the operation of taking minors (as planar graphs are) can automatically be characterized by [[Forbidden graph characterization|finitely many forbidden minors]] analogously to Wagner's theorem characterizing the planar graphs. [[Neil Robertson (mathematician)|Neil Robertson]] and [[Paul Seymour (mathematician)|Paul Seymour]] finally published a proof of Wagner's conjecture in 2004 and it is now known as the [[Robertson–Seymour theorem]].&lt;ref&gt;{{citation|last1=Robertson|first1=Neil|authorlink1=Neil Robertson (mathematician)|last2=Seymour|first2=Paul|authorlink2=Paul Seymour (mathematician)|title=Graph Minors XX: Wagner's Conjecture|journal=Journal of Combinatorial Theory, Series B|volume=92|year=2004|pages=325–357|doi=10.1016/j.jctb.2004.08.001|issue=2}}.&lt;/ref&gt;

==Selected publications==
*{{citation
  | author = Wagner, K. &lt;!-- | authorlink = Klaus Wagner (mathematician) --&gt;
  | title = Über eine Eigenschaft der ebenen Komplexe
  | journal = Mathematische Annalen
  | volume = 114
  | year = 1937
  | pages = 570–590
  | url = http://gdz.sub.uni-goettingen.de/index.php?id=11&amp;PPN=PPN235181684_0114&amp;DMDID=DMDLOG_0040&amp;L=1
  | doi = 10.1007/BF01594196}}.

==References==
{{reflist}}

{{Authority control}}

{{DEFAULTSORT:Wagner, Klaus}}
[[Category:1910 births]]
[[Category:2000 deaths]]
[[Category:German mathematicians]]
[[Category:Topologists]]
[[Category:Graph theorists]]
[[Category:20th-century mathematicians]]</text>
      <sha1>713usy0mpfpfhkbiafu8fyz0zdq77l6</sha1>
    </revision>
  </page>
  <page>
    <title>Linear form</title>
    <ns>0</ns>
    <id>214137</id>
    <revision>
      <id>866354858</id>
      <parentid>866341657</parentid>
      <timestamp>2018-10-29T21:00:11Z</timestamp>
      <contributor>
        <username>D.Lazard</username>
        <id>12336988</id>
      </contributor>
      <comment>Reverted 1 edit by [[Special:Contributions/2600:1702:1010:E520:0:0:0:43|2600:1702:1010:E520:0:0:0:43]]: Simply wrong. Apparently a confusion between the morphisms in category theory and homomorphisms of vector spaces. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13919">In [[linear algebra]], a '''linear functional''' or '''linear form''' (also called a '''[[one-form]]''' or '''covector''') is a [[linear map]] from a [[vector space]] to its field of [[scalar (mathematics)|scalar]]s. In [[Euclidean space|'''ℝ'''&lt;sup&gt;''n''&lt;/sup&gt;]], if [[euclidean vector|vectors]] are represented as [[column vector]]s, then linear functionals are represented as [[row vector]]s, and their action on vectors is given by the [[dot product]], or the [[matrix product]] with the [[row vector]] on the left and the [[column vector]] on the right.&amp;nbsp; In general, if ''V'' is a [[vector space]] over a [[field (mathematics)|field]] ''k'', then a linear functional ''f'' is a function from ''V'' to ''k'' that is linear:

:&lt;math&gt;f(\vec{v}+\vec{w}) = f(\vec{v})+f(\vec{w})&lt;/math&gt; for all &lt;math&gt;\vec{v}, \vec{w}\in V&lt;/math&gt;
:&lt;math&gt;f(a\vec{v}) = af(\vec{v})&lt;/math&gt; for all &lt;math&gt;\vec{v}\in V, a\in k.&lt;/math&gt;

The set of all linear functionals from ''V'' to ''k'', Hom&lt;sub&gt;''k''&lt;/sub&gt;(''V'',''k''), forms a vector space over ''k'' with the addition of the operations of addition and scalar multiplication (defined [[pointwise]]).&amp;nbsp; This space is called the [[dual space]] of ''V'', or sometimes the '''algebraic dual space''', to distinguish it from the [[continuous dual space]].&amp;nbsp; It is often written ''V''&lt;sup&gt;∗&lt;/sup&gt;, ''V′'', or ''V''&lt;sup&gt;&lt;small&gt;ᐯ&lt;/small&gt;&lt;/sup&gt; when the field ''k'' is understood.

== Continuous linear functionals ==
{{see also|Continuous linear operator}}

If ''V'' is a [[topological vector space]], the space of [[continuous function|continuous]] linear functionals &amp;mdash;  the ''[[continuous dual space|continuous dual]]'' &amp;mdash; is often simply called the dual space.&amp;nbsp; If ''V'' is a [[Banach space]], then so is its (continuous) dual.&amp;nbsp; To distinguish the ordinary dual space from the continuous dual space, the former is sometimes called the ''algebraic dual space''.&amp;nbsp; In finite dimensions, every linear functional is continuous, so the continuous dual is the same as the algebraic dual, but in infinite dimensions the continuous dual is a proper subspace of the algebraic dual.

== Examples and applications ==

=== Linear functionals in R&lt;sup&gt;''n''&lt;/sup&gt; ===
Suppose that vectors in the real coordinate space '''R'''&lt;sup&gt;''n''&lt;/sup&gt; are represented as column vectors

:&lt;math&gt;\vec{x} = \begin{bmatrix}x_1\\ \vdots\\ x_n\end{bmatrix}.&lt;/math&gt;

For each row vector [''a''&lt;sub&gt;1&lt;/sub&gt; … ''a''&lt;sub&gt;''n''&lt;/sub&gt;] there is a linear functional ''f'' defined by
:&lt;math&gt;f(\vec{x}) = a_1x_1 + \cdots + a_n x_n,&lt;/math&gt;
and each linear functional can be expressed in this form.

This can be interpreted as either the matrix product or the dot product of the row vector [''a''&lt;sub&gt;1&lt;/sub&gt; ... ''a''&lt;sub&gt;''n''&lt;/sub&gt;] and the column vector &lt;math&gt;\vec{x}&lt;/math&gt;:
:&lt;math&gt;f(\vec{x}) = [a_1 \dots a_n] \begin{bmatrix}x_1\\ \vdots\\ x_n\end{bmatrix}.&lt;/math&gt;

=== Integration ===
Linear functionals first appeared in [[functional analysis]], the study of [[function space|vector spaces of functions]].&amp;nbsp; A typical example of a linear functional is [[integral|integration]]: the linear transformation defined by the [[Riemann integral]]
:&lt;math&gt;I(f) = \int_a^b f(x)\, dx&lt;/math&gt;

is a linear functional from the vector space C[''a'',&amp;nbsp;''b''] of continuous functions on the interval [''a'',&amp;nbsp;''b''] to the real numbers. The linearity of {{math|''I''}} follows from the standard facts about the integral:
:&lt;math&gt;\begin{align}
     I(f + g) &amp;= \int_a^b[f(x) + g(x)]\, dx = \int_a^b f(x)\, dx + \int_a^b g(x)\, dx = I(f) + I(g) \\
  I(\alpha f) &amp;= \int_a^b \alpha f(x)\, dx = \alpha\int_a^b f(x)\, dx = \alpha I(f).
\end{align}&lt;/math&gt;

=== Evaluation ===
Let ''P&lt;sub&gt;n&lt;/sub&gt;'' denote the vector space of real-valued polynomial functions of degree ≤''n'' defined on an interval [''a'',&amp;nbsp;''b''].&amp;nbsp; If ''c''&amp;nbsp;∈&amp;nbsp;[''a'',&amp;nbsp;''b''], then let {{nowrap|ev&lt;sub&gt;''c''&lt;/sub&gt; : ''P&lt;sub&gt;n&lt;/sub&gt;'' → '''R'''}} be the '''evaluation functional'''
:&lt;math&gt;\operatorname{ev}_c f = f(c).&lt;/math&gt;
The mapping ''f''&amp;nbsp;→&amp;nbsp;''f''(''c'') is linear since
:&lt;math&gt;\begin{align}
     (f + g)(c) &amp;= f(c) + g(c) \\
  (\alpha f)(c) &amp;= \alpha f(c).
\end{align}&lt;/math&gt;

If ''x''&lt;sub&gt;0&lt;/sub&gt;, ..., ''x&lt;sub&gt;n&lt;/sub&gt;'' are {{nowrap|''n'' + 1}} distinct points in {{nowrap|[''a'', ''b'']}}, then the evaluation functionals {{nowrap|1=ev''&lt;sub&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/sub&gt;'', ''i'' = 0, 1, ..., ''n''}} form a [[basis of a vector space|basis]] of the dual space of ''P&lt;sub&gt;n&lt;/sub&gt;''.&amp;nbsp; ({{harvtxt|Lax|1996}} proves this last fact using [[Lagrange interpolation]].)

=== Application to quadrature ===
The integration functional {{math|''I''}} defined above defines a linear functional on the [[linear subspace|subspace]] {{math|''P&lt;sub&gt;n&lt;/sub&gt;''}} of polynomials of degree {{math|≤ ''n''}}. If {{math|''x''&lt;sub&gt;0&lt;/sub&gt;, ..., ''x''&lt;sub&gt;''n''&lt;/sub&gt;}} are {{math|''n'' + 1}} distinct points in {{math|[''a'', ''b'']}}, then there are coefficients {{math|''a''&lt;sub&gt;0&lt;/sub&gt;, ..., ''a''&lt;sub&gt;''n''&lt;/sub&gt;}} for which

:&lt;math&gt;I(f) = a_0 f(x_0) + a_1 f(x_1) + \dots + a_n f(x_n)&lt;/math&gt;

for all {{math|''f'' ∈ ''P''&lt;sub&gt;''n''&lt;/sub&gt;}}. This forms the foundation of the theory of [[numerical quadrature]].

This follows from the fact that the linear functionals {{math|ev''&lt;sub&gt;x&lt;sub&gt;i&lt;/sub&gt;&lt;/sub&gt;'' : ''f'' → ''f''(''x''&lt;sub&gt;''i''&lt;/sub&gt;)}} defined above form a [[basis of a vector space|basis]] of the dual space of {{math|''P''&lt;sub&gt;''n''&lt;/sub&gt;}}.&lt;ref&gt;{{harvnb|Lax|1996}}&lt;/ref&gt;

=== Linear functionals in quantum mechanics ===
Linear functionals are particularly important in [[quantum mechanics]].&amp;nbsp; Quantum mechanical systems are represented by [[Hilbert space]]s, which are [[antilinear|anti]]&amp;ndash;[[linear isomorphism|isomorphic]] to their own dual spaces.&amp;nbsp; A state of a quantum mechanical system can be  identified with a linear functional.&amp;nbsp; For more information see [[bra–ket notation]].

=== Distributions ===
In the theory of [[generalized function]]s, certain kinds of generalized functions called [[distribution (mathematics)|distributions]] can be realized as linear functionals on spaces of [[test function]]s.

== Properties ==
* Any linear functional ''L'' is either trivial (equal to 0 everywhere) or [[surjective]] onto the scalar field.&amp;nbsp; Indeed, this follows since just as the image of a vector [[linear subspace|subspace]] under a linear transformation is a subspace, so is the image of ''V'' under ''L''.&amp;nbsp; however, the only subspaces (i.e., ''k''-subspaces) of ''k'' are {0} and ''k'' itself.
* A linear functional is continuous if and only if its [[Kernel (linear operator)|kernel]] is closed. &lt;ref&gt;{{harvnb|Rudin|1991|loc=Theorem 1.18}}&lt;/ref&gt;
* Linear functionals with the same kernel are proportional.
* The absolute value of any linear functional is a [[seminorm]] on its vector space.{{clarify|reason=This property appears use as premise a real or complex vector space, but this article is introduced using more general scalars; the actual premise should thus be explicitly stated.|date=August 2017}}

==Visualizing linear functionals==
[[File:Gradient 1-form.svg|thumb|200px|Geometric interpretation of a 1-form '''α''' as a stack of [[hyperplane]]s of constant value, each corresponding to those vectors that '''α''' maps to a given scalar value shown next to it along with the "sense" of increase. The {{color box|purple}} zero plane is through the origin.]]

In finite dimensions, a linear functional can be visualized in terms of its [[level set]]s.&amp;nbsp; In three dimensions, the level sets of a linear functional are a family of mutually parallel planes; in higher dimensions, they are parallel [[hyperplane]]s.&amp;nbsp; This method of visualizing linear functionals is sometimes introduced in [[general relativity]] texts, such as [[Gravitation (book)|''Gravitation'']] by {{harvtxt|Misner|Thorne|Wheeler|1973}}.
{{Clear}}

== Dual vectors and bilinear forms ==

[[File:1-form linear functional.svg|thumb|400px|Linear functionals (1-forms) '''α''', '''β''' and their sum '''σ''' and vectors '''u''', '''v''', '''w''', in [[three-dimensional space|3d]] [[Euclidean space]]. The number of (1-form) [[hyperplane]]s intersected by a vector equals the [[inner product]].&lt;ref&gt;{{cite book|title=Gravitation|author1=J.A. Wheeler |author2=C. Misner |author3=K.S. Thorne |publisher=W.H. Freeman &amp; Co|year=1973|page=57|isbn=0-7167-0344-0}}&lt;/ref&gt;]]

Every non-degenerate [[bilinear form]] on a finite-dimensional vector space ''V'' induces an [[isomorphism]] {{nowrap|''V'' → ''V''{{i sup|∗}} : ''v'' ↦ ''v''{{sup|∗}}}} such that 
: &lt;math&gt; v^*(w) := \langle v,  w\rangle \quad \forall w \in V ,&lt;/math&gt;

where the bilinear form on ''V'' is denoted {{nowrap|{{langle}} , {{rangle}}}} (for instance, in [[Euclidean space]] {{nowrap|1={{langle}}''v'', ''w''{{rangle}} = ''v'' ⋅ ''w''}} is the [[dot product]] of ''v'' and ''w'').

The inverse isomorphism is {{nowrap|''V''{{i sup|∗}} → ''V'' : ''v''{{sup|∗}} ↦ ''v''}}, where ''v'' is the unique element of ''V'' such that

: &lt;math&gt; \langle v, w\rangle = v^*(w) \quad \forall w \in V .&lt;/math&gt;

The above defined vector {{nowrap|''v''{{sup|∗}} ∈ ''V''{{i sup|∗}}}} is said to be the '''dual vector''' of {{nowrap|''v'' ∈ ''V''}}.

In an infinite dimensional [[Hilbert space]], analogous results hold by the [[Riesz representation theorem]].&amp;nbsp; There is a mapping {{nowrap|''V'' → ''V''{{i sup|∗}}}} into the ''continuous dual space'' ''V''{{i sup|∗}}.&amp;nbsp; However, this mapping is [[antilinear]] rather than linear.
{{Clear}}

==Bases in finite dimensions==

===Basis of the dual space in finite dimensions===
Let the vector space ''V'' have a basis &lt;math&gt;\vec{e}_1, \vec{e}_2,\dots,\vec{e}_n&lt;/math&gt;, not necessarily [[orthogonal]].&amp;nbsp; Then the [[dual space]] ''V*'' has a basis &lt;math&gt;\tilde{\omega}^1,\tilde{\omega}^2,\dots,\tilde{\omega}^n&lt;/math&gt; called the [[dual basis]] defined by the special property that

:&lt;math&gt; \tilde{\omega}^i (\vec e_j) = \left\{\begin{matrix} 1 &amp;\mathrm{if}\ i=j\\ 0 &amp;\mathrm{if}\ i\not=j.\end{matrix}\right. &lt;/math&gt;

Or, more succinctly,

:&lt;math&gt; \tilde{\omega}^i (\vec e_j) = \delta^i_j &lt;/math&gt;

where δ is the [[Kronecker delta]].&amp;nbsp; Here the superscripts of the basis functionals are not exponents but are instead [[covariance and contravariance of vectors|contravariant]] indices.

A linear functional &lt;math&gt;\tilde{u}&lt;/math&gt; belonging to the dual space &lt;math&gt;\tilde{V}&lt;/math&gt; can be expressed as a [[linear combination]] of basis functionals, with coefficients ("components") ''u&lt;sub&gt;i&lt;/sub&gt;'', 
:&lt;math&gt;\tilde{u} = \sum_{i=1}^n u_i \, \tilde{\omega}^i. &lt;/math&gt;

Then, applying the functional &lt;math&gt;\tilde{u}&lt;/math&gt; to a basis vector ''e&lt;sub&gt;j&lt;/sub&gt;'' yields
:&lt;math&gt;\tilde{u}(\vec e_j) = \sum_{i=1}^n \left(u_i \, \tilde{\omega}^i\right) \vec e_j = \sum_i u_i \left[\tilde{\omega}^i \left(\vec e_j\right)\right] &lt;/math&gt;

due to linearity of scalar multiples of functionals and pointwise linearity of sums of functionals.&amp;nbsp; Then
:&lt;math&gt;\begin{align}
  \tilde{u}({\vec e}_j) &amp;= \sum_i u_i \left[\tilde{\omega}^i \left({\vec e}_j\right)\right] = \sum_i u_i {\delta^i}_j \\
                        &amp;= u_j.
\end{align}&lt;/math&gt;

So each component of a linear functional can be extracted by applying the functional to the corresponding basis vector.

=== The dual basis and inner product ===
When the space ''V'' carries an [[inner product]], then it is possible to write explicitly a formula for the dual basis of a given basis.&amp;nbsp; Let ''V'' have (not necessarily orthogonal) basis &lt;math&gt;\vec{e}_1,\dots, \vec{e}_n&lt;/math&gt;.&amp;nbsp; In three dimensions ({{nowrap|1=''n'' = 3}}), the dual basis can be written explicitly
:&lt;math&gt; \tilde{\omega}^i(\vec{v}) = {1 \over 2} \, \left\langle  { \sum_{j=1}^3\sum_{k=1}^3\varepsilon^{ijk} \, (\vec e_j \times \vec e_k) \over \vec  e_1 \cdot \vec e_2 \times \vec e_3} , \vec{v} \right\rangle ,&lt;/math&gt;
for ''i'' = 1, 2, 3, where ''ε'' is the [[Levi-Civita symbol]] and &lt;math&gt;\langle,\rangle&lt;/math&gt; the inner product (or [[dot product]]) on ''V''.

In higher dimensions, this generalizes as follows
:&lt;math&gt; \tilde{\omega}^i(\vec{v}) = \left\langle \frac{\underset{{}^{1\le i_2&lt;i_3&lt;\dots&lt;i_n\le n}}{\sum}\varepsilon^{ii_2\dots i_n}(\star \vec{e}_{i_2}\wedge\dots\wedge\vec{e}_{i_n})}{\star(\vec{e}_1\wedge\dots\wedge\vec{e}_n)}, \vec{v} \right\rangle ,&lt;/math&gt;
where &lt;math&gt;\star&lt;/math&gt; is the [[Hodge star operator]].

==See also==
*[[Discontinuous linear map]]
*[[Positive linear functional]]
*[[Multilinear form]]

==Notes==
{{reflist}}

==References==
*{{citation|first1=Richard|last1=Bishop|author1-link=Richard L. Bishop|first2=Samuel|last2=Goldberg|year=1980|title=Tensor Analysis on Manifolds|publisher=Dover Publications|chapter=Chapter 4|isbn=0-486-64039-6}}
* {{citation|first=Paul|last=Halmos|authorlink=Paul Halmos|title=Finite dimensional vector spaces|year=1974|publisher=Springer|isbn=0-387-90093-4}}
* {{citation|authorlink=Peter Lax|first=Peter|last=Lax|title=Linear algebra|year=1996|publisher=Wiley-Interscience|isbn=978-0-471-11111-5}}
* {{citation|first1=Charles W. | last1=Misner | author1-link = Charles W. Misner | first2=Kip. S. |last2=Thorne | author2-link = Kip Thorne | first3=John A. | last3=Wheeler | author3-link = John Archibald Wheeler |title=Gravitation | publisher= W. H. Freeman | year=1973 | isbn=0-7167-0344-0 }}
*{{citation | last1=Rudin | first1=Walter | author1-link=Walter Rudin | title=Functional Analysis | publisher=McGraw-Hill Science/Engineering/Math | isbn=978-0-07-054236-5 | year=1991}}
* {{citation|first=Bernard|last=Schutz | author1-link = Bernard F. Schutz |year=1985|title=A first course in general relativity|publisher=Cambridge University Press|location=Cambridge, UK|chapter=Chapter 3|isbn=0-521-27703-5}}

{{Functional Analysis}}

[[Category:Functional analysis]]
[[Category:Linear algebra]]
[[Category:Linear operators]]</text>
      <sha1>tg9u220xp0df94m5qhcrw83n8gim74e</sha1>
    </revision>
  </page>
  <page>
    <title>Liquidity at risk</title>
    <ns>0</ns>
    <id>49076898</id>
    <revision>
      <id>699463548</id>
      <parentid>699459262</parentid>
      <timestamp>2016-01-12T13:56:02Z</timestamp>
      <contributor>
        <username>Hou710</username>
        <id>21700557</id>
      </contributor>
      <comment>Another reference, scientific one actually (includes many references somebody should have a look at)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1935">{{Distinguish|Liquidity risk}}
The '''Liquidity-at-Risk''' (short: '''LaR''') is a quantity to measure [[financial risk]]s and is the maximum net liquidity drain relative to the expected liquidity position which should not be exceeded at a given confidence level (e.g. 95%). The LaR is analog to the [[Value at risk|Value-at-Risk (VaR)]] where a [[quantile]] of the [[Earnings before interest and taxes|EBIT]]-distribution is considered, however it does take stochastic [[cash flow]]s into account.&lt;ref name=XtremValueTheoryCom&gt;{{cite web|title=Liquidity-at-Risk (LaR) of a German private bank|url=http://www.extreme-value-theory.com/case-studies/liquidity-at-risk-lar/|website=extreme-value-theory.com|publisher=RC Banken-Consulting GmbH &amp; Co. KG|accessdate=12 January 2016|location=Buxtehude, Germany|language=English|date=2011}}&lt;/ref&gt;&lt;ref name=BachelorWork&gt;{{cite book|last1=Conzen|first1=Sander|title=Liquidity at Risk (LaR) und LiquidityValue at Risk (LVaR): Zwei neue Ansätze für das Liquiditätsmanagement|date=6 September 2009|publisher=Diplomica|location=Hamburg, Germany|isbn=978-3-8366-3500-4|edition=Frankfurt School of Finance &amp; Management|url=https://books.google.de/books?id=-VRwAQAAQBAJ|accessdate=12 January 2016|language=German}}&lt;/ref&gt;

== Critics ==
Statistical measures for [[financial risk]] are not intuitive. Increasing the [[confidence level]] (e.g. from 99.0% to 99.9%) does not capture very [[rare events]] with possibly high impact. The only way around is to use [[extreme value theory]] for modelling the distribution tails. In other words: Statistical liquidity risk modelling approaches do not provide certainty in terms of a reliable lower limit for future liquidity.

== See also ==
* [[Margin at risk]]
* [[Value at risk]]
* [[Profit at risk]]

== References ==
{{reflist}}

{{Financial risk}}

[[Category:Mathematical finance]]
[[Category:Financial risk]]
[[Category:Monte Carlo methods in finance]]</text>
      <sha1>6ihyu7p5b3qy863hldjklnmvohwg9f4</sha1>
    </revision>
  </page>
  <page>
    <title>Low and high hierarchies</title>
    <ns>0</ns>
    <id>20250230</id>
    <revision>
      <id>742506972</id>
      <parentid>711406353</parentid>
      <timestamp>2016-10-04T03:32:08Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor/>
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1357">In the [[computational complexity theory]], the '''low hierarchy''' and '''high hierarchy''' of complexity levels were introduced in 1983 by [[Uwe Schöning]] to describe the internal structure of the [[complexity class NP]].
&lt;ref&gt;{{cite journal
| doi = 10.1016/0022-0000(83)90027-2
| author = [[Uwe Schöning]]
| title     = A Low and a High Hierarchy within NP
|  journal   = J. Comput. Syst. Sci.
|  volume    = 27
|  issue    = 1
|  year      = 1983
|  pages     = 14–28
}}&lt;/ref&gt; The low hierarchy starts from [[complexity class P]] and grows "upwards", while the high hierarchy starts from class NP and grows "downwards". &lt;ref&gt;"Complexity Theory and Cryptology", by Jörg Rothe [https://books.google.com/books?doi=1mMz89RTyaYC&amp;pg=PA232&amp;dq=low++high+hierarchy+within+NP+complexity&amp;lr= p. 232]&lt;/ref&gt;

Later these hierarchies were extended to sets outside NP.

The framework of high/low hierarchies makes sense only under the assumption that  [[P is not NP]]. On the other hand, if the low hierarchy consists of at least two levels, then P is not NP.&lt;ref&gt;Lane A. Hemaspaandra,  "Lowness: a yardstick for NP-P", ''ACM SIGACT News'', 1993, vol. 24, no.2, pp. 11-14. {{doi|10.1145/156063.156064}}&lt;/ref&gt;

It is not known whether these hierarchies cover all NP.

==References==
{{reflist}}

[[Category:Structural complexity theory]]

{{comp-sci-theory-stub}}</text>
      <sha1>jiex9d4erhq7m1ng315bxdbx10vdaiu</sha1>
    </revision>
  </page>
  <page>
    <title>Margolus–Levitin theorem</title>
    <ns>0</ns>
    <id>3390080</id>
    <revision>
      <id>856246692</id>
      <parentid>844457994</parentid>
      <timestamp>2018-08-23T21:48:48Z</timestamp>
      <contributor>
        <username>Omnipaedista</username>
        <id>8524693</id>
      </contributor>
      <comment>dubious/unsourced</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2224">The '''Margolus–Levitin theorem''', named for [[Norman Margolus]] and [[Lev B. Levitin]], gives a fundamental limit on [[quantum computation]] (strictly speaking on all forms on computation). The processing rate cannot be higher than 6 &amp;times; 10&lt;sup&gt;33&lt;/sup&gt; operations per second per [[joule]] of energy. Or stating the bound for one bit:

:A quantum system of energy ''E'' needs at least a time of &lt;math&gt;\frac{h}{4 E}&lt;/math&gt; to go from one state to an orthogonal state, where ''h'' is [[Planck's constant]] (6.626 &amp;times; 10&lt;sup&gt;−34&lt;/sup&gt; [[joule|J]]·[[second|s]]) and ''E'' is average energy.

== See also ==
* [[Bekenstein bound]]
* [[Bremermann's limit]]
* [[Landauer's principle]]
* [[Kolmogorov complexity]]
* [[Koomey's law]]
* [[Limits to computation]]
* [[Moore's law]]

== References ==
* {{cite journal | authors = Norman Margolus, Lev B. Levitin | arxiv = quant-ph/9710043 | title = The maximum speed of dynamical evolution | journal = [[Physica D]] | volume = 120 | year = 1998 | pages = 188–195 | doi = 10.1016/S0167-2789(98)00054-2|bibcode = 1998PhyD..120..188M }}
*{{citation| first1= Sebastian| last1= Deffner | first2= Steve | last2= Campbell | title= Quantum speed limits | year=2017 |journal= [[Journal of Physics A]] |volume=50 | page= 453001 | doi= 10.1088/1751-8121/aa86c6| arxiv= 1705.08023 | bibcode= 2017JPhA...50S3001D }}
*{{citation| last=Jordan|first=Stephen P.| title=Fast quantum computation at arbitrarily low energy|journal = [[Physical Review A]] | volume = 95 | pages=032305 | year=2017 |arxiv = 1701.01175|url=https://arxiv.org/pdf/1701.01175.pdf|bibcode=2017PhRvA..95c2305J|doi=10.1103/PhysRevA.95.032305}}
* [[Seth Lloyd|Lloyd, Seth]]; Ng, Y. Jack, "[https://www.scientificamerican.com/article/black-hole-computers-2007-04/ Black Hole Computers]", ''[[Scientific American]]'' (April 2007), p.&amp;nbsp;53–61
*{{cite journal|first=Nikolai A.|last=Sinitsyn|title=Is there a quantum limit on speed of computation?|year=2017||arxiv = 1701.05550|url=https://arxiv.org/pdf/1701.05550.pdf|bibcode=2018PhLA..382..477S|doi=10.1016/j.physleta.2017.12.042}}

{{DEFAULTSORT:Margolus-Levitin theorem}}
[[Category:Quantum information science]]
[[Category:Limits of computation]]

{{phys-stub}}</text>
      <sha1>pvxh0mvc9qv8uyhxxding1uwwxxi59v</sha1>
    </revision>
  </page>
  <page>
    <title>Mathematical sciences</title>
    <ns>0</ns>
    <id>25750436</id>
    <revision>
      <id>859371793</id>
      <parentid>855230706</parentid>
      <timestamp>2018-09-13T16:35:30Z</timestamp>
      <contributor>
        <username>Omnipaedista</username>
        <id>8524693</id>
      </contributor>
      <comment>add citation</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2416">The '''mathematical sciences''' are a group of areas of study that includes, in addition to [[mathematics]], those [[academic disciplines]] that are primarily [[mathematical]] in nature but may not be universally considered subfields of [[mathematics]] proper.

[[Statistics]], for e.g., is mathematical in its methods but grew out of [[Observation#Observation in science|scientific observations]]&lt;ref&gt;{{cite book |title=The History of Statistics: The Measurement of Uncertainty Before 1900 |first=Stephen M. |last=Stigler |publisher=Harvard University Press |year=1986 |isbn=0-67440341-X |pages=[https://books.google.com/books?id=M7yvkERHIIMC&amp;lpg=PA225&amp;ots=Glm4Zj_E6p&amp;pg=PA225#v=onepage 225–226]}}&lt;/ref&gt; which merged with [[inverse probability]] and grew through applications in the [[social sciences]], some areas of [[physics]] and [[biometrics]] to become its own separate, though closely allied field. [[Computer science]], [[computational science]], [[population genetics]], [[operations research]], [[cryptology]], [[econometrics]], [[theoretical physics]], [[chemical reaction network theory]] and [[actuarial science]] are other fields that may be considered part of mathematical sciences.

Some institutions offer degrees in mathematical sciences (e.g. the [[United States Military Academy]], [[Stanford University]], and [[University of Khartoum]]) or applied mathematical sciences (e.g. the [[University of Rhode Island]]).

== See also ==
* [[Relationship between mathematics and physics]]

==References==
{{Reflist}}

== External links ==
*[https://www.nsf.gov/div/index.jsp?div=DMS Division of Mathematical Sciences] at the [[National Science Foundation]], including a list of disciplinary areas supported
*[http://fms.uofk.edu Faculty of Mathematical Sciences] at [[University of Khartoum]], offers academic degrees in [[Mathematics]], [[Computer science|Computer Sciences]] and [[Statistics]]
*[https://web.archive.org/web/20091006192311/http://www.msri.org/about/mission/index_html Mission statement] of the [[Mathematical Sciences Research Institute]]
*[http://www.newton.ac.uk/about/history History] of the [[Isaac Newton Institute for Mathematical Sciences]]
*[https://www.aaas.org/fy16budget/mathematical-sciences-fy-2016-budget Mathematical Sciences in the U.S. FY 2016 Budget]; a report from the [[American Association for the Advancement of Science|AAAS]]

[[Category:Applied mathematics]]</text>
      <sha1>dh8qrqb97uiunxckodoa10zrjxa2bjz</sha1>
    </revision>
  </page>
  <page>
    <title>Mode of a linear field</title>
    <ns>0</ns>
    <id>32364992</id>
    <revision>
      <id>646598991</id>
      <parentid>641424328</parentid>
      <timestamp>2015-02-11T03:51:47Z</timestamp>
      <contributor>
        <username>Waldir</username>
        <id>182472</id>
      </contributor>
      <comment>add some links, but they don't help much with such a hard to read text. Hopefully someone knowledgeable on the subject will improve this article soon.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1380">{{multiple issues|
{{expert-subject|date=September 2014}}
{{unreferenced|date=July 2011}}
{{confusing|date=July 2011}}
}}

In [[physics]], a [[vector field]] is linear if it is a solution of a [[System of linear equations|set of linear equations]]&amp;nbsp;''E''. For instance, in physics, the [[electromagnetic field]] in vacuum, defined in the usual [[Spacetime|(3&amp;nbsp;+&amp;nbsp;1)-dimensional space]] ''S'', obeys [[Maxwell's equations]]. A linear combination of electromagnetic fields, with constant, [[real number|real]] coefficients, is a new field which obeys Maxwell's equations.

The solutions of the linear equations are represented in a real [[vector space]]&amp;nbsp;''M''. A radius of ''M'', which represents proportional solutions, is called a "mode".{{clarify|reason=Please clarify what radius in this context means, as well as what a proportional solution is|date=August 2011}}

A [[Norm (mathematics)|norm]] may be defined. For instance, in electromagnetism, it is usually the [[energy]] of the solution ''assuming that there is no other field in S''. From the norm are defined the [[orthogonality]] and the [[Dot product|scalar product]] of solutions. The orthogonality of solutions extends to the corresponding modes.

[[Category:Linear algebra]]
[[Category:Differential equations]]
[[Category:Functional analysis]]
[[Category:Electromagnetism]]
[[Category:Field theory]]</text>
      <sha1>t7pukzu7yfi7ibhhu4hbyjpuire2ky0</sha1>
    </revision>
  </page>
  <page>
    <title>Parabolic induction</title>
    <ns>0</ns>
    <id>11118768</id>
    <revision>
      <id>862205552</id>
      <parentid>786530205</parentid>
      <timestamp>2018-10-02T20:57:20Z</timestamp>
      <contributor>
        <username>Jakob.scholbach</username>
        <id>1935000</id>
      </contributor>
      <minor/>
      <comment>/* References */ polish 1</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3027">In [[mathematics]], '''parabolic induction ''' is a method of constructing [[group representation|representation]]s of a [[reductive group]] from representations of its [[parabolic subgroup]]s. 

If ''G'' is a reductive algebraic group and &lt;math&gt;P=MAN&lt;/math&gt; is the [[Langlands decomposition]] of a parabolic subgroup ''P'', then parabolic induction consists of taking a representation of &lt;math&gt;MA&lt;/math&gt;, extending it to ''P'' by letting ''N'' act trivially, and [[induced representation|inducing]] the result from ''P'' to ''G''.

There are some generalizations of parabolic induction using [[cohomology]], such as [[cohomological parabolic induction]] and [[Deligne–Lusztig theory]].

==Philosophy of cusp forms==

The ''philosophy of [[cusp form]]s'' was a slogan of [[Harish-Chandra]], expressing his idea of a kind of reverse engineering of [[automorphic form]] theory, from the point of view of [[representation theory]].&lt;ref&gt;[[Daniel Bump]], ''Automorphic Forms and Representations'' (1998), p. 421.&lt;/ref&gt; The [[discrete group]] Γ fundamental to the classical theory disappears, superficially. What remains is the basic idea that representations in general are to be constructed by parabolic induction of [[cuspidal representation]]s.&lt;ref&gt;See Daniel Bump, ''Lie Groups'' (2004), p. 397.&lt;/ref&gt; A similar philosophy was enunciated by [[Israel Gelfand]],&lt;ref&gt;{{Citation |first=I. M. |last=Gelfand |title=Automorphic functions and the theory of representations |work=Proceedings, International Congress of Mathematicians |location=Stockholm |year=1962 |pages=74–85 }}.&lt;/ref&gt; and the philosophy is a precursor of the [[Langlands program]]. A consequence for thinking about representation theory is that [[cuspidal representation]]s are the fundamental class of objects, from which other representations may be constructed by procedures of induction.

According to [[Nolan Wallach]]&lt;ref&gt;[http://www.math.ucsd.edu/~nwallach/luminy-port2.pdf PDF], p.80.&lt;/ref&gt;

&lt;blockquote&gt;Put in the simplest terms the "philosophy of cusp forms" says that for each Γ-conjugacy classes of Q-rational parabolic subgroups one should construct automorphic functions (from objects from spaces of lower dimensions) whose constant terms are zero for other conjugacy classes and the constant terms for [an]&lt;!-- source says "and" --&gt; element of the given class give all constant terms for this parabolic subgroup. This is almost possible and leads to a description of all automorphic forms in terms of these constructs and cusp forms. The construction that does this is the [[Eisenstein series]].&lt;/blockquote&gt;

==Notes==
{{Reflist}}

==References==
*A. W. Knapp, ''Representation Theory of Semisimple Groups: An Overview Based on Examples'', Princeton Landmarks in Mathematics, Princeton University Press, 2001. {{isbn|0-691-09089-0}}.
*{{Citation|first=Daniel|last=Bump|title=Lie Groups|series=Graduate Texts in Mathematics|volume=225|publisher=Springer-Verlag|location=New York|year=2004|isbn=0-387-21154-3}}

[[Category:Representation theory]]</text>
      <sha1>exiw9x6yqwt6ldmyi8odfrk5t4o7mns</sha1>
    </revision>
  </page>
  <page>
    <title>Parsimonious reduction</title>
    <ns>0</ns>
    <id>55213021</id>
    <revision>
      <id>870362892</id>
      <parentid>870362844</parentid>
      <timestamp>2018-11-24T08:05:39Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>link [[♯P-complete]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4367">In [[computational complexity theory]] and [[game complexity]], a '''parsimonious reduction''' is a transformation from one problem to another (a [[Reduction (complexity)|reduction]]) that preserves the number of solutions.

More formally, 
parsimonious reductions are defined for problems in [[Non-deterministic Turing machine|nondeterministic]] complexity classes such as [[NP (complexity)|NP]],
which are defined by a ''verifier'' algorithm whose input is a pair of strings (an instance of the problem and a [[Certificate (complexity)|candidate solution]]) and whose output is true when the candidate solution is a valid solution of the instance.
For example, in the [[Boolean satisfiability problem]], the instance might be a [[Boolean expression]] and the candidate solution might be a [[truth assignment]] to its variables; a valid truth assignment is one that makes the expression evaluate to true.
A parsimonious reduction from one problem ''X'' of this type to another problem ''Y'' is an algorithmic transformation from instances of ''X'' to instances of ''Y''
such that the number of solutions of an instance of ''X'' equals the number of solutions of the transformed instance of ''Y''.&lt;ref name=goldreich&gt;{{citation|title=Computational Complexity: A Conceptual Perspective|first=Oded|last=Goldreich|authorlink=Oded Goldreich|publisher=Cambridge University Press|year=2008|isbn=9781139472746|pages=203–204|url=https://books.google.com/books?id=EuguvA-w5OEC&amp;pg=PA203}}&lt;/ref&gt;

Just as [[many-one reduction]]s are important for proving [[NP-completeness]],
parsimonious reductions are important for proving completeness for [[Counting problem (complexity)|counting complexity classes]] such as [[♯P]].&lt;ref name=goldreich/&gt; Because parsimonious reductions preserve the property of having a unique solution, they are also used in [[game complexity]], to show the hardness of puzzles such as [[sudoku]] where the uniqueness of the solution is an important part of the definition of the puzzle.&lt;ref&gt;{{citation|title=Complexity and Completeness of Finding Another Solution and Its Application to Puzzles|first1=Takayuki|last1=Yato|first2=Takahiro|last2=Seta|journal=IEICE Transactions on Fundamentals of Electronics, Communications and Computer Sciences|volume=E86-A|issue=5|year=2003|pages=1052–1060|url=https://search.ieice.org/bin/summary.php?id=e86-a_5_1052}}&lt;/ref&gt;

Specific types of parsimonious reductions may be defined by the computational complexity or other properties of the transformation algorithm.
For instance, a ''polynomial-time parsimonious reduction'' is one in which the transformation algorithm takes [[polynomial time]]. These are the types of reduction used to prove [[♯P-complete]]ness.&lt;ref name=goldreich/&gt; In [[parameterized complexity]], ''fpt parsimonious reductions'' are used; these are parsimonious reductions whose transformation is a fixed-parameter tractable algorithm and that map bounded parameter values to bounded parameter values by a computable function.&lt;ref&gt;{{citation|title=Parameterized Complexity Theory|series=EATCS Texts in Theoretical Computer Science|first1=J.|last1=Flum|first2=M.|last2=Grohe|publisher=Springer|year=2006|isbn=9783540299530|page=363|url=https://books.google.com/books?id=VfJz6hvFAjoC&amp;pg=PA363}}&lt;/ref&gt;

Polynomial-time parsimonious reductions are a special case of a more general class of reductions for counting problems, the [[polynomial-time counting reduction]]s.&lt;ref&gt;{{citation
 | last1 = Gomes | first1 = Carla P. | author1-link = Carla Gomes
 | last2 = Sabharwal | first2 = Ashish
 | last3 = Selman | first3 = Bart | author3-link = Bart Selman
 | editor1-last = Biere | editor1-first = Armin
 | editor2-last = Heule | editor2-first = Marijn
 | editor3-last = van Maaren | editor3-first = Hans
 | editor4-last = Walsh | editor4-first = Toby
 | contribution = Chapter 20. Model Counting
 | isbn = 9781586039295
 | pages = 633–654
 | publisher = IOS Press
 | series = Frontiers in Artificial Intelligence and Applications
 | title = Handbook of Satisfiability
 | url = http://www.cs.cornell.edu/~sabhar/chapters/ModelCounting-SAT-Handbook-prelim.pdf
 | volume = 185
 | year = 2009}}. See in particular [https://books.google.com/books?id=YVSM3sxhBhcC&amp;pg=PA634 pp. 634–635].&lt;/ref&gt;

==References==
{{reflist}}

[[Category:Reduction (complexity)]]
[[Category:Combinatorial game theory]]</text>
      <sha1>d65i4z6g8lvfpp97yccq1mmdgj6k9ro</sha1>
    </revision>
  </page>
  <page>
    <title>Pick's theorem</title>
    <ns>0</ns>
    <id>328252</id>
    <revision>
      <id>870388723</id>
      <parentid>869113290</parentid>
      <timestamp>2018-11-24T13:21:10Z</timestamp>
      <contributor>
        <username>Saung Tadashi</username>
        <id>16809467</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6406">{{for|the theorem in complex analysis|Schwarz lemma#Schwarz–Pick theorem}}
[[File:Pick-theorem.png|right|thumb|{{color|red|{{math|''i'' {{=}} 7}}}}, {{color|green|{{math|''b'' {{=}} 8}}}}, {{math|''A'' {{=}} {{color|red|''i''}} + {{sfrac|{{color|green|''b''}}|2}} − 1 {{=}} 10}}]]
[[Image:coprime-lattice.svg|thumb|right|300px| The triangle with vertices at the lower left, lower right, and upper right points has {{math|''i'' {{=}} 12}} and {{math|''b'' {{=}} 14}}, giving by Pick's theorem {{math|''A'' {{=}} ''i'' + {{sfrac|''b''|2}} − 1 {{=}} 18}}; this is confirmed by the triangle area formula {{nowrap|{{sfrac|1|2}} × base × height}} = {{nowrap|{{sfrac|1|2}} × 9 × 4}} = 18.]]

Given a [[simple polygon]] constructed on a grid of equal-distanced points (i.e., points with [[integer]] coordinates) such that all the polygon's [[vertex (geometry)|vertices]] are grid points, '''Pick's theorem''' provides a simple [[formula]] for calculating the [[area]] {{mvar|A}} of this polygon in terms of the number {{mvar|i}} of ''lattice points in the interior'' located in the polygon and the number {{mvar|b}} of ''lattice points on the boundary'' placed on the polygon's perimeter:&lt;ref&gt;{{cite journal |last=Trainin |first=J. |title=An elementary proof of Pick's theorem |journal=[[Mathematical Gazette]] |volume=91 |issue=522 |date=November 2007 |pages=536–540}}&lt;/ref&gt;

:&lt;math&gt;A = i + \frac{b}{2} - 1.&lt;/math&gt;

In the example shown, we have {{math|''i'' {{=}} 7}} interior points  and {{math|''b'' {{=}} 8}} boundary points, so the area is {{mvar|A}}&amp;nbsp;=&amp;nbsp;7&amp;nbsp;+&amp;nbsp;{{sfrac|8|2}}&amp;nbsp;−&amp;nbsp;1 =&amp;nbsp;7&amp;nbsp;+&amp;nbsp;4&amp;nbsp;−&amp;nbsp;1 =&amp;nbsp;10 square units.

Note that the theorem as stated above is only valid for ''simple'' polygons, i.e., ones that consist of a single piece and do not contain holes. For a polygon that has {{mvar|h}} holes, with a boundary in the form of {{math|''h'' + 1}} [[simple closed curve]]s, the slightly more complicated formula {{math|''i'' + {{sfrac|''b''|2}} + ''h'' − 1}} gives the area.

The result was first described by [[Georg Alexander Pick]] in 1899.&lt;ref&gt;{{cite journal |last=Pick |first=Georg |title=Geometrisches zur Zahlenlehre |journal=Sitzungsberichte des deutschen naturwissenschaftlich-medicinischen Vereines für Böhmen "Lotos" in Prag |series=(Neue Folge) |year=1899 |volume=19 |pages=311–319 |url=https://www.biodiversitylibrary.org/item/50207#page/327 |jfm=33.0216.01 }} [http://citebank.org/node/47270 CiteBank:47270]&lt;/ref&gt; The [[Reeve tetrahedron]] shows that there is no analogue of Pick's theorem in three dimensions that expresses the volume of a polytope by counting its interior and boundary points. However, there is a generalization in higher dimensions via [[Ehrhart polynomial]]s. The formula also generalizes to surfaces of [[polyhedron|polyhedra]].

==Proof==
Consider a polygon {{mvar|P}} and a triangle {{mvar|T}}, with one edge in common with {{mvar|P}}. Assume Pick's theorem is true for both {{mvar|P}} and {{mvar|T}} separately; we want to show that it is also true for the polygon {{mvar|PT}} obtained by adding {{mvar|T}} to {{mvar|P}}. Since {{mvar|P}} and {{mvar|T}} share an edge, all the boundary points along the edge in common are merged to interior points, except for the two endpoints of the edge, which are merged to boundary points. So, calling the number of boundary points in common {{mvar|c}}, we have&lt;ref&gt;{{cite book|last1=Beck|first1=Matthias|last2=Robins|first2=Sinai|date=2007|title=Computing the Continuous Discretely, Integer-point enumeration in polyhedra|series=[[Undergraduate Texts in Mathematics]]|location=New York|publisher=Springer-Verlag|ISBN=978-0-387-29139-0|MR=2271992|at=ch. 2}}&lt;/ref&gt;

:&lt;math&gt;i_{PT} = i_P + i_T + (c - 2)&lt;/math&gt;

and

:&lt;math&gt;b_{PT} = b_P + b_T - 2(c - 2) - 2.&lt;/math&gt;

From the above follows

:&lt;math&gt;i_P + i_T = i_{PT} - (c - 2)&lt;/math&gt;

and

:&lt;math&gt;b_P + b_T = b_{PT} + 2(c - 2) + 2.&lt;/math&gt;

Since we are assuming the theorem for {{mvar|P}} and for {{mvar|T}} separately,

: &lt;math&gt;\begin{align}
A_{PT} &amp;= A_P + A_T\\
&amp;= \left(i_P + \frac{b_P}{2} - 1\right) + \left(i_T + \frac{b_T}{2} - 1\right)\\
&amp;= i_P + i_T + \frac{b_P + b_T}{2} - 2\\
&amp;= i_{PT} - (c - 2) + \frac{b_{PT} + 2(c - 2) + 2}{2} - 2\\
&amp;= i_{PT} + \frac{b_{PT}}{2} - 1.
\end{align}&lt;/math&gt;

Therefore, if the theorem is true for polygons constructed from {{mvar|n}} triangles, the theorem is also true for polygons constructed from {{math|''n'' + 1}} triangles. For general [[polytope]]s, it is well known that they can always be [[triangulation (geometry)|triangulated]]. That this is true in dimension 2 is an easy fact. To finish the proof by [[mathematical induction]], it remains to show that the theorem is true for triangles.  The verification for this case can be done in these short steps:

* observe that the formula holds for any unit square (with vertices having integer coordinates);
* deduce from this that the formula is correct for any [[rectangle]] with sides [[parallel (geometry)|parallel]] to the axes;
* deduce it, now, for right-angled triangles obtained by cutting such rectangles along a [[diagonal]];
* now any triangle can be turned into a rectangle by attaching such right triangles; since the formula is correct for the right triangles and for the rectangle, it also follows for the original triangle.

The last step uses the fact that if the theorem is true for the polygon {{mvar|PT}} and for the triangle {{mvar|T}}, then it's also true for {{mvar|P}}; this can be seen by a calculation very much similar to the one shown above.

==See also==
*[[Integer points in convex polyhedra]]
*[[Steinhaus longimeter]]

==References==
{{reflist}}

==External links==
{{commons category}}
* [http://www.cut-the-knot.org/ctk/Pick.shtml Pick's Theorem (Java)] at [[cut-the-knot]]
* [http://www.mcs.drexel.edu/~crorres/Archimedes/Stomachion/Pick.html Pick's Theorem]
* [http://www.geometer.org/mathcircles/pick.pdf Pick's Theorem proof] by Tom Davis
* [http://demonstrations.wolfram.com/PicksTheorem/ Pick's Theorem] by [[Ed Pegg, Jr.]], the [[Wolfram Demonstrations Project]].
* {{MathWorld |title=Pick's Theorem  |urlname=PicksTheorem}}

[[Category:Digital geometry]]
[[Category:Lattice points]]
[[Category:Euclidean plane geometry]]
[[Category:Theorems in geometry]]
[[Category:Area]]
[[Category:Polygons]]
[[Category:Articles containing proofs]]
[[Category:Analytic geometry]]</text>
      <sha1>fm7d91474t4kgtfyglz2wkjbfdlv3gk</sha1>
    </revision>
  </page>
  <page>
    <title>Radial set</title>
    <ns>0</ns>
    <id>33297737</id>
    <revision>
      <id>787013807</id>
      <parentid>753094736</parentid>
      <timestamp>2017-06-22T22:20:52Z</timestamp>
      <contributor>
        <username>CBM</username>
        <id>1108292</id>
      </contributor>
      <minor/>
      <comment>Copyediting in citations - nonbreaking spaces and en dashes (manually reviewed)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2330">In [[mathematics]], given a [[linear space]] &lt;math&gt;X&lt;/math&gt;, a set &lt;math&gt;A \subseteq X&lt;/math&gt; is '''radial''' at the point &lt;math&gt;x_0 \in A&lt;/math&gt; if for every &lt;math&gt;x \in X&lt;/math&gt; there exists a &lt;math&gt;t_x &gt; 0&lt;/math&gt; such that for every &lt;math&gt;t \in [0,t_x]&lt;/math&gt;, &lt;math&gt;x_0 + tx \in A&lt;/math&gt;.&lt;ref name="coherent"&gt;{{cite journal|title=Coherent Risk Measures, Valuation Bounds, and (&lt;math&gt;\mu,\rho&lt;/math&gt;)-Portfolio Optimization|first1=Stefan|last1=Jaschke|first2=Uwe|last2=Küchler|year=2000}}&lt;/ref&gt; Geometrically, this means &lt;math&gt;A&lt;/math&gt; is radial at &lt;math&gt;x_0&lt;/math&gt; if for every &lt;math&gt;x \in X&lt;/math&gt; a line segment emanating from &lt;math&gt;x_0&lt;/math&gt; in the direction of &lt;math&gt;x&lt;/math&gt; lies in &lt;math&gt;A&lt;/math&gt;, where the length of the line segment is required to be non-zero but can depend on &lt;math&gt;x&lt;/math&gt;.

The set of all points at which &lt;math&gt;A \subseteq X&lt;/math&gt; is radial is equal to the [[algebraic interior]].&lt;ref name="coherent" /&gt;&lt;ref&gt;{{cite book|author=Nikolaĭ Kapitonovich Nikolʹskiĭ|title=Functional analysis I: linear functional analysis|year=1992|publisher=Springer|isbn=978-3-540-50584-6}}&lt;/ref&gt;  The points at which a set is radial are often referred to as internal points.&lt;ref name="aliprantis+border"&gt;{{cite book|last=Aliprantis|first=C.D.|last2=Border|first2=K.C.|title=Infinite Dimensional Analysis: A Hitchhiker's Guide|edition=3|publisher=Springer|year=2007|isbn=978-3-540-32696-0|doi=10.1007/3-540-29587-9|pages=199–200}}&lt;/ref&gt;&lt;ref name="cook"&gt;{{cite web|url=http://www.johndcook.com/SeparationOfConvexSets.pdf | accessdate=November 14, 2012 |format=pdf |title=Separation of Convex Sets in Linear Topological Spaces |author=John Cook |date=May 21, 1988}}&lt;/ref&gt;

A set &lt;math&gt;A \subseteq X&lt;/math&gt; is [[absorbing set|absorbing]] if and only if it is radial at 0.&lt;ref name="coherent" /&gt; Some authors use the term ''radial'' as a synonym for ''absorbing'', i. e. they call a set radial if it is radial at 0.&lt;ref name="schaefer"&gt;{{cite book | last = Schaefer | first = Helmuth H. &lt;!-- | authorlink = Helmuth Schaefer --&gt; | year = 1971 | title = Topological vector spaces | series=[[Graduate Texts in Mathematics|GTM]] | volume=3 | publisher = Springer-Verlag | location = New York | isbn = 0-387-98726-6}}&lt;/ref&gt;

== References ==
{{Reflist}}

{{Functional Analysis}}

[[Category:Topology]]

{{topology-stub}}</text>
      <sha1>lxk4q0itmmfy27wcycp7ml04suz2hg8</sha1>
    </revision>
  </page>
  <page>
    <title>Rayleigh's method of dimensional analysis</title>
    <ns>0</ns>
    <id>21118619</id>
    <revision>
      <id>769212517</id>
      <parentid>703038150</parentid>
      <timestamp>2017-03-08T05:25:16Z</timestamp>
      <contributor>
        <username>GeoffreyT2000</username>
        <id>21491290</id>
      </contributor>
      <comment>This article is unreferenced.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2108">{{unreferenced|date=March 2017}}
'''Rayleigh's method of dimensional analysis''' is a conceptual tool used in [[physics]], [[chemistry]], and [[engineering]]. This form of [[dimensional analysis]] expresses a [[functional relationship]] of some [[variable (mathematics)|variables]] in the form of an [[exponential equation]]. It was named after [[John Strutt, 3rd Baron Rayleigh|Lord Rayleigh]].

The method involves the following steps:

# Gather all the [[independent variable]]s that are likely to influence the [[dependent variable]].
# If ''R'' is a variable that depends upon independent variables ''R''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''R''&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;''R''&lt;sub&gt;3&lt;/sub&gt;,&amp;nbsp;...,&amp;nbsp;''R''&lt;sub&gt;''n''&lt;/sub&gt;, then the [[functional equation]] can be written as ''R'' = ''F''(''R''&lt;sub&gt;1&lt;/sub&gt;, ''R''&lt;sub&gt;2&lt;/sub&gt;, ''R''&lt;sub&gt;3&lt;/sub&gt;, ..., ''R''&lt;sub&gt;''n''&lt;/sub&gt;).
# Write the above equation in the form ''R'' = ''C'' ''R''&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;a&lt;/sup&gt; ''R''&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;b&lt;/sup&gt; ''R''&lt;sub&gt;3&lt;/sub&gt;&lt;sup&gt;c&lt;/sup&gt; ... ''R''&lt;sub&gt;n&lt;/sub&gt;&lt;sup&gt;m&lt;/sup&gt;, where ''C'' is a [[dimensionless constant]] and ''a'', ''b'', ''c'', ..., ''m'' are arbitrary exponents.
# Express each of the quantities in the equation in some [[Base unit (measurement)|base unit]]s in which the solution is required.
# By using [[Dimensional analysis#Commensurability|dimensional homogeneity]], obtain a [[set (mathematics)|set]] of [[simultaneous equations]] involving the exponents ''a'', ''b'', ''c'', ..., ''m''.
# [[Equation solving|Solve]] these equations to obtain the value of exponents ''a'', ''b'', ''c'', ..., ''m''.
# [[Simultaneous equations#Substitution method|Substitute]] the values of exponents in the main equation, and form the [[non-dimensional]] [[parameter]]s by [[Combining like terms|grouping]] the variables with like exponents.

'''Drawback''' – It doesn't provide any information regarding number of dimensionless groups to be obtained as a result of dimension analysis

==See also==
* [[Physical quantity]]
* [[Buckingham pi theorem]]

==References==
{{reflist}}

[[Category:Dimensional analysis]]


{{applied-math-stub}}</text>
      <sha1>9wxixgt98n6o8g0faiymk5604m5tuwn</sha1>
    </revision>
  </page>
  <page>
    <title>Receiver (information theory)</title>
    <ns>0</ns>
    <id>4412890</id>
    <revision>
      <id>540685267</id>
      <parentid>524418003</parentid>
      <timestamp>2013-02-26T16:58:14Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 8 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q1339255]] ([[User talk:Addbot|Report Errors]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="525">The '''receiver''' in [[information theory]] is the receiving end of a [[communication channel]]. It receives [[Code|decode]]d [[message]]s/[[information]] from the sender, who first [[Code|encode]]d them. Sometimes the receiver is modeled so as to include the decoder. Real-world receivers like [[Receiver (radio)|radio receivers]] or [[telephone]]s can not be expected to receive as much information as predicted by the [[noisy channel coding theorem]].

{{Unreferenced|date=December 2006}}

[[Category:Information theory]]</text>
      <sha1>h33cebyqabcr3zzch34w3nv4nkc20hw</sha1>
    </revision>
  </page>
  <page>
    <title>Road coloring theorem</title>
    <ns>0</ns>
    <id>19271448</id>
    <revision>
      <id>852316197</id>
      <parentid>832838855</parentid>
      <timestamp>2018-07-28T03:04:12Z</timestamp>
      <contributor>
        <username>Rohan Rawat NN</username>
        <id>33434017</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5824">In [[graph theory]] the '''road coloring [[theorem]]''', known until recently as the '''road coloring [[conjecture]]''', deals with [[Synchronization|synchronized]] instructions. The issue involves whether by using such instructions, one can reach or locate an object or destination from any other point within a [[wikt:network|network]] (which might be a representation of city streets or a [[maze]]).&lt;ref&gt;{{cite news | last =Seigel-Itzkovich | first =Judy | title =Russian immigrant solves math puzzle | pages = | publisher =The Jerusalem Post | date = 2008-02-08 | 
url =http://www.jpost.com/Home/Article.aspx?id=91431 | accessdate = 2010-09-13}}&lt;/ref&gt; In the real world, this phenomenon would be as if you called a friend to ask for directions to his house, and he gave you a set of directions that worked no matter where you started from. This theorem also has implications in [[symbolic dynamics]].

The theorem was first conjectured by {{harvs|first1=Roy|last1=Adler|author1-link=Roy Adler|first2=Benjamin|last2=Weiss|author2-link=Benjamin Weiss|year=1970|txt}}. It was proved by {{harvs|first=Avraham|last=Trahtman|authorlink=Avraham Trahtman|year=2009|txt}}.

==Example and intuition==
[[Image:Road coloring conjecture.svg|thumb|right|A directed graph with a synchronizing coloring]]
The image to the right shows a [[directed graph]] on eight [[Vertex (graph theory)|vertices]] in which each vertex has [[Degree (graph theory)#Directed graphs|out-degree]]&amp;nbsp;2. (Each vertex in this case also has in-degree&amp;nbsp;2, but that is not necessary for a synchronizing coloring to exist.) The edges of this graph have been colored red and blue to create a synchronizing coloring.

For example, consider the vertex marked in yellow. No matter where in the graph you start, if you traverse all nine edges in the walk "blue-red-red—blue-red-red—blue-red-red", you will end up at the yellow vertex. Similarly, if you traverse all nine edges in the walk "blue-blue-red—blue-blue-red—blue-blue-red", you will always end up at the vertex marked in green, no matter where you started.

The road coloring theorem states that for a certain category of directed graphs, it is always possible to create such a coloring.

===Mathematical description===
Let ''G'' be a finite, [[strongly connected component|strongly connected]], [[directed graph]]  where all the vertices have the same [[out-degree]] ''k''. Let ''A'' be the alphabet containing the letters 1, ..., ''k''. A ''synchronizing coloring'' (also known as a ''collapsible coloring'') in ''G'' is a labeling of the edges in ''G'' with letters from ''A'' such that (1) each vertex has exactly one outgoing edge with a given label and (2) for every vertex ''v'' in the graph, there exists a word ''w'' over ''A'' such that all paths in ''G'' corresponding to ''w'' terminate at ''v''.

The terminology ''synchronizing coloring'' is due to the relation between this notion and that of a [[synchronizing word]] in [[finite automata]] theory.

For such a coloring to exist at all, it is [[Necessary and sufficient conditions|necessary]] that ''G'' be [[aperiodic graph|aperiodic]].&lt;ref&gt;{{harvtxt|Hegde|Jain|2005}}.&lt;/ref&gt; The road coloring theorem states that aperiodicity is also ''[[Necessary and sufficient conditions|sufficient]]'' for such a coloring to exist. Therefore, the road coloring problem can be stated briefly as:

:''Every finite strongly connected directed aperiodic graph of uniform out-degree has a synchronizing coloring.''

==Previous partial results==
Previous partial or special-case results include the following:

*If ''G'' is a finite strongly connected [[Periodic function|aperiodic]] directed graph with no [[multiple edges]], and ''G'' contains a [[simple cycle]] of [[prime number|prime]] length which is a proper subset of ''G'', then ''G'' has a synchronizing coloring. (O'Brien 1981)
*If ''G'' is a finite strongly connected aperiodic directed graph (multiple edges allowed) and every vertex has the same in-degree and out-degree ''k'', then ''G'' has a synchronizing coloring. (Kari 2003)

==See also==
* [[Four color theorem]]
* [[Graph coloring]]

== Notes ==
{{reflist}}

==References==
*{{citation
 | last1 = Adler | first1 = R.L.
 | last2 = Weiss | first2 = B. | author2-link = Benjamin Weiss
 | series = Memoires of the American Mathematical Society
 | title = Similarity of automorphisms of the torus
 | volume = 98
 | year = 1970}}.
*{{citation
 | last1 = Hegde | first1 = Rajneesh
 | last2 = Jain | first2 = Kamal
 | contribution = A min-max theorem about the road coloring conjecture
 | pages = 279–284
 | series = Discrete Mathematics &amp; Theoretical Computer Science
 | title = Proc. EuroComb 2005
 | url = http://www.emis.de/journals/DMTCS/pdfpapers/dmAE0155.pdf
 | year = 2005}}.
*{{citation
 | doi = 10.1016/S0304-3975(02)00405-X
 | last = Kari | first = Jarkko | author-link = Jarkko Kari
 | journal = [[Theoretical Computer Science (journal)|Theoretical Computer Science]]
 | pages = 223–232
 | title = Synchronizing finite automata on Eulerian digraphs
 | issue = 1-3
 | volume = 295
 | year = 2003}}.
*{{citation
 | last = O'Brien | first = G. L.
 | doi = 10.1007/BF02762860
 | issue = 1–2
 | journal = Israel Journal of Mathematics
 | pages = 145–154
 | title = The road-colouring problem
 | volume = 39
 | year = 1981}}.
*{{citation
 | title = The road coloring problem
 | last = Trahtman | first = Avraham N. | authorlink = Avraham Trahtman
 | year = 2009
 | doi = 10.1007/s11856-009-0062-5
 | arxiv = 0709.0099 
 | issue = 1
 | journal = Israel Journal of Mathematics
 | pages = 51–60
 | volume = 172}}.

[[Category:Combinatorics]]
[[Category:Automata (computation)]]
[[Category:Mathematics and culture]]
[[Category:Graph coloring]]
[[Category:Topological graph theory]]
[[Category:Theorems in graph theory]]</text>
      <sha1>hse6vidvl8sx9h1ikyio0ardt7nk7rg</sha1>
    </revision>
  </page>
  <page>
    <title>Sieve of Eratosthenes</title>
    <ns>0</ns>
    <id>73415</id>
    <revision>
      <id>871394626</id>
      <parentid>871394072</parentid>
      <timestamp>2018-11-30T19:26:12Z</timestamp>
      <contributor>
        <username>WillNess</username>
        <id>10841018</id>
      </contributor>
      <comment>/* Overview */ this (what the previous edit had reverted) keeps getting proposed; try to incorporate it, see if it is better.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="34853">{{For|the sculpture|The Sieve of Eratosthenes (sculpture)}}
[[File:Sieve of Eratosthenes animation.gif|right|frame|Sieve of Eratosthenes: algorithm steps for primes below 121 (including optimization of starting from prime's square).]]

In [[mathematics]], the '''[[Sieve theory|sieve]] of Eratosthenes''' is a simple, ancient [[algorithm]] for finding all [[prime number]]s up to any given limit.

It does so by iteratively marking as [[composite number|composite]] (i.e., not prime) the multiples of each prime, starting with the first prime number, {{math|2}}. The multiples of a given prime are generated as a sequence of numbers starting from that prime, with [[arithmetic progression|constant difference between them]] that is equal to that prime.&lt;ref name="horsley"&gt;Horsley, Rev. Samuel, F. R. S., "''{{lang|el|Κόσκινον Ερατοσθένους}}'' or, The Sieve of Eratosthenes. Being an account of his method of finding all the Prime Numbers," [https://www.jstor.org/stable/106053 ''Philosophical Transactions'' (1683–1775), Vol. 62. (1772), pp. 327–347].&lt;/ref&gt; This is the sieve's key distinction from using [[trial division]] to sequentially test each candidate number for divisibility by each prime.&lt;ref name="ONeill" /&gt;

The earliest known reference to the sieve ({{lang-grc|κόσκινον Ἐρατοσθένους}}, ''kóskinon Eratosthénous'') is in [[Nicomachus|Nicomachus of Gerasa]]'s ''[[Introduction to Arithmetic]]'',&lt;ref name=nicomachus&gt;{{citation|editor-first=Richard|editor-last=Hoche|editor-link=Richard Hoche|title=Nicomachi Geraseni Pythagorei Introductionis arithmeticae libri II|year=1866|location= Leipzig|publisher= B.G. Teubner|page=31|url=https://archive.org/stream/nicomachigerasen00nicouoft#page/30/mode/2up}}&lt;/ref&gt; which describes it and attributes it to [[Eratosthenes|Eratosthenes of Cyrene]], a [[Greek mathematics|Greek mathematician]].

One of a number of [[Generating primes#Prime sieves|prime number sieve]]s, it is one of the most efficient ways to find all of the smaller primes. It may be used to find primes in [[arithmetic progression]]s.&lt;ref&gt;J. C. Morehead, "Extension of the Sieve of Eratosthenes to arithmetical progressions and applications", [https://www.jstor.org/stable/1967477 Annals of Mathematics, Second Series '''10''':2 (1909), pp. 88–104].&lt;/ref&gt;

==Overview==
{{quote box|fontsize = 105%|''Sift the Two's and Sift the Three's,''&lt;br /&gt;''The Sieve of Eratosthenes.''&lt;br /&gt;''When the multiples sublime,''&lt;br /&gt;''The numbers that remain are Prime.''|quoted=1|salign=center|source=Anonymous&lt;ref&gt;Clocksin, William F., Christopher S. Mellish, Programming in Prolog, 1984, p.&amp;nbsp;170. {{isbn|3-540-11046-1}}.&lt;/ref&gt;}}

A [[prime number]] is a [[natural number]] that has exactly two distinct natural number [[divisor]]s: [[1 (number)|1]] and itself.

To find all the prime numbers less than or equal to a given integer {{mvar|n}} by Eratosthenes' method:

# Create a list of consecutive integers from 2 through {{mvar|n}}: {{math|(2, 3, 4, ..., ''n'')}}.
# Initially, let {{mvar|p}} equal 2, the smallest prime number.
# Enumerate the multiples of {{mvar|p}} by counting in increments of {{mvar|p}} from {{math|2''p''}} to {{mvar|n}}, and mark them in the list (these will be {{math|2''p'', 3''p'', 4''p'', ...}}; the {{mvar|p}} itself should not be marked). 
# Find the first number greater than {{mvar|p}} in the list that is not marked. If there was no such number, stop. Otherwise, let {{mvar|p}} now equal this new number (which is the next prime), and repeat from step 3.
# When the algorithm terminates, the numbers remaining not marked in the list are all the primes below {{mvar|n}}.

The main idea here is that every value given to {{mvar|p}} will be prime, because if it were composite it would be marked as a multiple of some other, smaller prime. Note that some of the numbers may be marked more than once (e.g., 15 will be marked both for 3 and 5).

As a refinement, it is sufficient to mark the numbers in step 3 starting from {{math|''p''&lt;sup&gt;2&lt;/sup&gt;}}, as all the smaller multiples of {{mvar|p}} will have already been marked at that point. This means that the algorithm is allowed to terminate in step 4 when {{math|''p''&lt;sup&gt;2&lt;/sup&gt;}} is greater than {{mvar|n}}.&lt;ref name="horsley" /&gt; &lt;!-- This does not appear in the algorithm as described by Nicomachus.&lt;ref&gt;Nicomachus, ibid., p.&amp;nbsp;31, where, e.g., 93 is marked as a multiple of 31.&lt;/ref&gt; ---- The above is based on wrong reading of the table in Nicomachus p.&amp;nbsp;31: main factors actually appear in upper row in cells, and their corresponding coefficients in the bottom row; 5 first appears in bottom row for 15 but in top row for 25, and 7 for 49. For 93, 31 is marked in bottom row, below 3. (He also marks 81 as 9*9 and 3*27, so wasn't working with only prime factors, but that's besides the point here.) What the text is saying in Latin or Greek, I have no idea. --&gt;

Another refinement is to initially list odd numbers only, {{math|(3, 5, ..., ''n'')}}, and count in increments of {{math|2''p''}} from {{math|''p''&lt;sup&gt;2&lt;/sup&gt;}} in step 3, thus marking only odd multiples of {{mvar|p}}. This actually appears in the original algorithm.&lt;ref name="horsley" /&gt; &lt;!-- &lt;ref&gt;Nicomachus, ibid., p.&amp;nbsp;31, where only odd numbers appear in the table.&lt;/ref&gt; Again, the table is not by Nicomachus; it is in the margins, and in Latin. --&gt; This can be generalized with [[wheel factorization]], forming the initial list only from numbers [[coprime]] with the first few primes and not just from odds (i.e., numbers coprime with 2), and counting in the correspondingly adjusted increments so that only such multiples of {{mvar|p}} are generated that are coprime with those small primes, in the first place.&lt;ref name="Runciman"&gt;{{Cite journal | doi = 10.1017/S0956796897002670| title = Functional Pearl: Lazy wheel sieves and spirals of primes| journal = J. Functional Programming| volume = 7| issue = 2| pages = 219–225| year = 1997| last1 = Runciman | first1 = Colin| url = http://eprints.whiterose.ac.uk/3784/1/runcimanc1.pdf}}&lt;/ref&gt;

===Example===
To find all the prime numbers less than or equal to 30, proceed as follows.

First, generate a list of integers from 2 to 30:

 &amp;nbsp;2  3  4  5  6  7  8  9  10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30

The first number in the list is 2; cross out every 2nd number in the list after 2 by counting up from 2 in increments of 2 (these will be all the multiples of 2 in the list):

 &amp;nbsp;2  3 {{gray|&lt;s&gt; 4 &lt;/s&gt;}} 5 {{gray|&lt;s&gt; 6 &lt;/s&gt;}} 7 {{gray|&lt;s&gt; 8 &lt;/s&gt;}} 9  {{gray|&lt;s&gt;10&lt;/s&gt;}} 11 {{gray|&lt;s&gt;12&lt;/s&gt;}} 13 {{gray|&lt;s&gt;14&lt;/s&gt;}} 15 {{gray|&lt;s&gt;16&lt;/s&gt;}} 17 {{gray|&lt;s&gt;18&lt;/s&gt;}} 19 {{gray|&lt;s&gt;20&lt;/s&gt;}} 21 {{gray|&lt;s&gt;22&lt;/s&gt;}} 23 {{gray|&lt;s&gt;24&lt;/s&gt;}} 25 {{gray|&lt;s&gt;26&lt;/s&gt;}} 27 {{gray|&lt;s&gt;28&lt;/s&gt;}} 29 {{gray|&lt;s&gt;30&lt;/s&gt;}}

The next number in the list after 2 is 3; cross out every 3rd number in the list after 3 by counting up from 3 in increments of 3 (these will be all the multiples of 3 in the list):

 &amp;nbsp;2  3 {{gray|&lt;s&gt; 4 &lt;/s&gt;}} 5 {{gray|&lt;s&gt; 6 &lt;/s&gt;}} 7 {{gray|&lt;s&gt; 8 &lt;/s&gt;}}{{gray|&lt;s&gt; 9 &lt;/s&gt;}}{{gray|&lt;s&gt; 10&lt;/s&gt;}} 11 {{gray|&lt;s&gt;12&lt;/s&gt;}} 13 {{gray|&lt;s&gt;14 &lt;/s&gt;}}{{gray|&lt;s&gt;15 &lt;/s&gt;}}{{gray|&lt;s&gt;16&lt;/s&gt;}} 17 {{gray|&lt;s&gt;18&lt;/s&gt;}} 19 {{gray|&lt;s&gt;20 &lt;/s&gt;}}{{gray|&lt;s&gt;21 &lt;/s&gt;}}{{gray|&lt;s&gt;22&lt;/s&gt;}} 23 {{gray|&lt;s&gt;24&lt;/s&gt;}} 25 {{gray|&lt;s&gt;26 &lt;/s&gt;}}{{gray|&lt;s&gt;27 &lt;/s&gt;}}{{gray|&lt;s&gt;28&lt;/s&gt;}} 29 {{gray|&lt;s&gt;30&lt;/s&gt;}}

The next number not yet crossed out in the list after 3 is 5; cross out every 5th number in the list after 5 by counting up from 5 in increments of 5 (i.e. all the multiples of 5):

 &amp;nbsp;2  3 {{gray|&lt;s&gt; 4 &lt;/s&gt;}} 5 {{gray|&lt;s&gt; 6 &lt;/s&gt;}} 7 {{gray|&lt;s&gt; 8 &lt;/s&gt;}}{{gray|&lt;s&gt; 9 &lt;/s&gt;}}{{gray|&lt;s&gt; 10&lt;/s&gt;}} 11 {{gray|&lt;s&gt;12&lt;/s&gt;}} 13 {{gray|&lt;s&gt;14 &lt;/s&gt;}}{{gray|&lt;s&gt;15 &lt;/s&gt;}}{{gray|&lt;s&gt;16&lt;/s&gt;}} 17 {{gray|&lt;s&gt;18&lt;/s&gt;}} 19 {{gray|&lt;s&gt;20 &lt;/s&gt;}}{{gray|&lt;s&gt;21 &lt;/s&gt;}}{{gray|&lt;s&gt;22&lt;/s&gt;}} 23 {{gray|&lt;s&gt;24 &lt;/s&gt;}}{{gray|&lt;s&gt;25 &lt;/s&gt;}}{{gray|&lt;s&gt;26 &lt;/s&gt;}}{{gray|&lt;s&gt;27 &lt;/s&gt;}}{{gray|&lt;s&gt;28&lt;/s&gt;}} 29 {{gray|&lt;s&gt;30&lt;/s&gt;}}

The next number not yet crossed out in the list after 5 is 7; the next step would be to cross out every 7th number in the list after 7, but they are all already crossed out at this point, as these numbers (14, 21, 28) are also multiples of smaller primes because 7 × 7 is greater than 30. The numbers not crossed out at this point in the list are all the prime numbers below 30:

 &amp;nbsp;2  3     5     7           11    13          17    19          23                29

==Algorithm and variants==
===Pseudocode===
The sieve of Eratosthenes can be expressed in [[pseudocode]], as follows:&lt;ref name="sedgewick"&gt;{{cite book
|last1=Sedgewick |first1=Robert |title=Algorithms in C++
|publisher=Addison-Wesley |year=1992 |isbn=0-201-51059-6 
}}, p.&amp;nbsp;16.&lt;/ref&gt;&lt;ref name="intro"&gt;[http://research.cs.wisc.edu/techreports/1990/TR909.pdf Jonathan Sorenson, An Introduction to Prime Number Sieves], Computer Sciences Technical Report #909, Department of Computer Sciences University of Wisconsin-Madison, January 2, 1990 (the use of optimization of starting from squares, and thus using only the numbers whose square is below the upper limit, is shown).&lt;/ref&gt;

 &amp;nbsp;'''Input''': an integer ''n'' &gt; 1.
 &amp;nbsp;&lt;!-- these &amp;nbsp;s prevent bots messing it up --&gt;
 &amp;nbsp;'''Let''' ''A'' be an '''array of [[Boolean data type|Boolean]]''' values, indexed by '''integer'''s 2 to ''n'',
 &amp;nbsp;initially all '''set''' to '''true'''.
 &amp;nbsp;
 &amp;nbsp;'''for''' ''i'' = 2, 3, 4, ..., not exceeding {{math|''{{sqrt|n}}''}}:
 &amp;nbsp;  '''if''' ''A''[''i''] '''is''' '''true''':
 &amp;nbsp;    '''for''' ''j'' = ''i''&lt;sup&gt;2&lt;/sup&gt;, ''i''&lt;sup&gt;2&lt;/sup&gt;+''i'', ''i''&lt;sup&gt;2&lt;/sup&gt;+2''i'', ''i''&lt;sup&gt;2&lt;/sup&gt;+3''i'', ..., not exceeding ''n'':
 &amp;nbsp;      ''A''[''j''] := '''false'''.
 &amp;nbsp;
 &amp;nbsp;'''Output''': all ''i'' such that ''A''[''i''] '''is''' '''true'''.

This algorithm produces all primes not greater than {{mvar|n}}. It includes a common optimization, which is to start enumerating the multiples of each prime {{mvar|i}} from {{math|''i''&lt;sup&gt;2&lt;/sup&gt;}}. The [[time complexity]] of this algorithm is {{math|''O''(''n'' log log ''n'')}},{{r|intro}} provided the array update is an {{math|''O''(1)}} operation, as is usually the case.

===Segmented sieve===
As Sorenson notes, the problem with the sieve of Eratosthenes is not the number of operations it performs but rather its memory requirements.{{r|intro}} For large {{mvar|n}}, the range of primes may not fit in memory; worse, even for moderate {{mvar|n}}, its [[CPU cache|cache]] use is highly suboptimal. The algorithm walks through the entire array {{mvar|A}}, exhibiting almost no [[locality of reference]].

A solution to these problems is offered by ''segmented'' sieves, where only portions of the range are sieved at a time.&lt;ref&gt;Crandall &amp; Pomerance, ''Prime Numbers: A Computational Perspective'', second edition, Springer: 2005, pp. 121–24.&lt;/ref&gt; These have been known since the 1970s, and work as follows:{{r|intro}}&lt;ref&gt;{{Cite journal | last1 = Bays | first1 = Carter | last2 = Hudson | first2 = Richard H. | year = 1977 | title = The segmented sieve of Eratosthenes and primes in arithmetic progressions to 10&lt;sup&gt;12&lt;/sup&gt; | journal = BIT | volume = 17 | issue = 2 | pages = 121–127 | publisher = | jstor = | doi = 10.1007/BF01932283 | url = | format = | accessdate = }}&lt;/ref&gt;

# Divide the range 2 through {{mvar|n}} into segments of some size {{math|Δ ≤ {{sqrt|''n''}}}}.
# Find the primes in the first (i.e. the lowest) segment, using the regular sieve.
# For each of the following segments, in increasing order, with {{mvar|m}} being the segment's topmost value, find the primes in it as follows:
## Set up a Boolean array of size {{math|Δ}}, and
## Eliminate from it the multiples of each prime {{math|''p'' ≤ {{sqrt|''m''}}}} found so far, by calculating the lowest multiple of {{math|''p''}} between {{math|{{mvar|m}} - Δ}} and {{mvar|m}}, and enumerating its multiples in steps of {{math|''p''}} as usual, marking the corresponding positions in the array as non-prime.

If {{math|Δ}} is chosen to be {{math|{{sqrt|''n''}}}}, the space complexity of the algorithm is {{math|''O''({{sqrt|''n''}})}}, while the time complexity is the same as that of the regular sieve.{{r|intro}}

For ranges with upper limit {{math|''n''}} so large that the sieving primes below {{math|{{sqrt|''n''}}}} as required by the page segmented sieve of Eratosthenes cannot fit in memory, a slower but much more space-efficient sieve like the [[sieve of Sorenson]] can be used instead.&lt;ref&gt;J. Sorenson, [http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.94.1737 The pseudosquares prime sieve], Proceedings of the 7th International Symposium on Algorithmic Number Theory. (ANTS-VII, 2006).&lt;/ref&gt;

===Incremental sieve===
An incremental formulation of the sieve&lt;ref name="ONeill"&gt;O'Neill, Melissa E., [http://www.cs.hmc.edu/~oneill/papers/Sieve-JFP.pdf "The Genuine Sieve of Eratosthenes"], Journal of Functional Programming, Published online by Cambridge University Press 9 October 2008 {{doi|10.1017/S0956796808007004}}, pp. 10, 11 (contains two incremental sieves in Haskell: a priority-queue–based one by O'Neill and a list–based, by Richard Bird).&lt;/ref&gt; generates primes indefinitely (i.e., without an upper bound) by interleaving the generation of primes with the generation of their multiples (so that primes can be found in gaps between the multiples), where the multiples of each prime {{mvar|p}} are generated directly by counting up from the square of the prime in increments of {{mvar|p}} (or {{math|2''p''}} for odd primes). The generation must be initiated only when the prime's square is reached, to avoid adverse effects on efficiency. It can be expressed symbolically under the [[Dataflow programming|dataflow]] paradigm as

   ''primes'' = [''2'', ''3'', ...] \ [[''p''², ''p''²+''p'', ...] for ''p'' in ''primes''],

using [[list comprehension]] notation with &lt;code&gt;\&lt;/code&gt; denoting [[Complement (set theory)#Relative complement|set subtraction]] of [[arithmetic progressions]] of numbers.

Primes can also be produced by iteratively sieving out the composites through [[Trial division|divisibility testing]] by sequential primes, one prime at a time. It is not the sieve of Eratosthenes but is often confused with it, even though the sieve of Eratosthenes directly generates the composites instead of testing for them. Trial division has worse theoretical [[Analysis of algorithms|complexity]] than that of the sieve of Eratosthenes in generating ranges of primes.&lt;ref name="ONeill"/&gt;

When testing each prime, the ''optimal'' trial division algorithm uses all prime numbers not exceeding its square root, whereas the sieve of Eratosthenes produces each composite from its prime factors only, and gets the primes "for free", between the composites. The widely known 1975 [[functional programming|functional]] sieve code by [[David Turner (computer scientist)|David Turner]]&lt;ref&gt;Turner, David A. SASL language manual. Tech. rept. CS/75/1. Department of Computational Science, University of St. Andrews 1975. (&lt;source lang="haskell" inline&gt;primes = sieve [2..]; sieve (p:nos) = p:sieve (remove (multsof p) nos); remove m = filter (not . m); multsof p n = rem n p==0&lt;/source&gt;). But see also [http://dl.acm.org/citation.cfm?id=811543&amp;dl=ACM&amp;coll=DL&amp;CFID=663592028&amp;CFTOKEN=36641676 Peter Henderson, Morris, James Jr., A Lazy Evaluator, 1976], where we [http://www.seas.gwu.edu/~rhyspj/cs3221/lab8/henderson.pdf find the following], attributed to P. Quarendon: &lt;source lang="python" inline&gt;primeswrt[x;l] = if car[l] mod x=0 then primeswrt[x;cdr[l]] else cons[car[l];primeswrt[x;cdr[l]]] ; primes[l] = cons[car[l];primes[primeswrt[car[l];cdr[l]]]] ; primes[integers[2]]&lt;/source&gt;; the priority is unclear.&lt;/ref&gt; is often presented as an example of the sieve of Eratosthenes&lt;ref name="Runciman"/&gt; but is actually a sub-optimal trial division sieve.&lt;ref name="ONeill"/&gt;

==Computational analysis==
{{refimprove section|date=June 2015}}
The work performed by this algorithm is almost entirely the operations to cull the composite number representations which for the basic non-optimized version is the sum of the range divided by each of the primes up to that range or
:&lt;math&gt;n\sum_{p\le n}\frac1p,&lt;/math&gt;
where {{mvar|n}} is the sieving range in this and all further analysis.

By rearranging [[Mertens' theorems|Mertens' second theorem]], this is equal to {{math|''n'' ( log log ''n'' + ''M'' )}} as {{mvar|n}} approaches infinity, where M is the [[Meissel–Mertens constant]] of about {{val|0.2614972128476427837554268386086958590516}}...

The optimization of starting at the square of each prime and only culling for primes less than the square root changes the "{{math|''n''}}" in the above expression to {{math|{{sqrt|''n''}}}} (or {{math|''n''&lt;sup&gt;{{sfrac|1|2}}&lt;/sup&gt;}}) and not culling until the square means that the sum of the base primes each minus two is subtracted from the operations. As the sum of the first {{math|''x''}} primes is {{math|{{sfrac|1|2}}''x''&lt;sup&gt;2&lt;/sup&gt; log ''x''}}&lt;ref&gt;E. Bach and J. Shallit, §2.7 in Algorithmic Number Theory, Vol. 1: Efficient Algorithms, MIT Press, Cambridge, MA, 1996.&lt;/ref&gt; and the [[prime number theorem]] says that {{math|''x''}} is approximately {{math|{{sfrac|''x''|log ''x''}}}}, then the sum of primes to {{math|''n''}} is {{math|{{sfrac|''n''&lt;sup&gt;2&lt;/sup&gt;|2 log ''n''}}}}, and therefore the sum of base primes to {{math|{{sqrt|''n''}}}} is {{math|{{sfrac|1|log ''n''}}}} expressed as a factor of {{math|''n''}}. The extra offset of two per base prime is {{math|2''π''({{sqrt|''n''}})}}, where {{mvar|π}} is the [[prime-counting function]] in this case, or {{math|{{sfrac|4{{sqrt|''n''}}|log ''n''}}}}; expressing this as a factor of {{math|''n''}} as are the other terms, this is {{math|{{sfrac|4|{{sqrt|''n''}} log ''n''}}}}. Combining all of this, the expression for the number of optimized operations without wheel factorization is
:&lt;math&gt;\log\log n-\frac1{\log n}\left(1-\frac{4}{\sqrt{n}}\right)+M-\log 2.&lt;/math&gt;

For the wheel factorization cases, there is a further offset of the operations not done of
:&lt;math&gt;\sum_{p\le x}\frac1p&lt;/math&gt;
where {{math|''x''}} is the highest wheel prime and a constant factor of the whole expression is applied which is the fraction of remaining prime candidates as compared to the repeating wheel circumference. The wheel circumference is
:&lt;math&gt;\prod_{p\le x}p&lt;/math&gt;
and it can easily be determined that this wheel factor is
:&lt;math&gt;\prod_{p\le x}\frac{p-1}{p}&lt;/math&gt;
as {{math|{{sfrac|''p'' − 1|''p''}}}} is the fraction of remaining candidates for the highest wheel prime, {{math|''x''}}, and each succeeding smaller prime leaves its corresponding fraction of the previous combined fraction.

Combining all of the above analysis, the total number of operations for a sieving range up to {{math|''n''}} including wheel factorization for primes up to {{math|''x''}} is approximately

:&lt;math&gt;\prod_{p\le x}\frac{p-1}{p}\left(\log\log n-\frac1{\log n}\left(1-\frac{4}{\sqrt{n}}\right)+M-\log 2-\sum_{p\le x}\frac1p\right)&lt;/math&gt;.

To show that the above expression is a good approximation to the number of composite number cull operations performed by the algorithm, following is a table showing the actually measured number of operations for a practical implementation of the sieve of Eratosthenes as compared to the number of operations predicted from the above expression with both expressed as a fraction of the range (rounded to four decimal places) for different sieve ranges and wheel factorizations (Note that the last column is a maximum practical wheel as to the size of the wheel gaps Look Up Table - almost 10 million values):

:{| class="wikitable" style="text-align: center"
! {{mvar|n}}
! colspan=2| no wheel
! colspan=2| odds
! colspan=2| 2/3/5 wheel
! colspan=2| 2/3/5/7 wheel
! colspan=2| 2/3/5/7/11/13/17/19 wheel
|-
! 10&lt;sup&gt;3&lt;/sup&gt;
| 1.4090 || 1.3745
| 0.4510 || 0.4372
| 0.1000 || 0.0909
| 0.0580 || 0.0453
| 0.0060 || —
|-
! 10&lt;sup&gt;4&lt;/sup&gt;
| 1.6962 || 1.6844
| 0.5972 || 0.5922
| 0.1764 || 0.1736
| 0.1176 || 0.1161
| 0.0473 || 0.0391
|-
! 10&lt;sup&gt;5&lt;/sup&gt;
| 1.9299 || 1.9261
| 0.7148 || 0.7130
| 0.2388 || 0.2381
| 0.1719 || 0.1714
| 0.0799 || 0.0805
|-
! 10&lt;sup&gt;6&lt;/sup&gt;
| 2.1218 || 2.1220
| 0.8109 || 0.8110
| 0.2902 || 0.2903
| 0.2161 || 0.2162
| 0.1134 || 0.1140
|-
! 10&lt;sup&gt;7&lt;/sup&gt;
| 2.2850 || 2.2863
| 0.8925 || 0.8932
| 0.3337 || 0.3341
| 0.2534 || 0.2538
| 0.1419 || 0.1421
|-
! 10&lt;sup&gt;8&lt;/sup&gt;
| 2.4257 || 2.4276
| 0.9628 || 0.9638
| 0.3713 || 0.3718
| 0.2856 || 0.2860
| 0.1660 || 0.1662
|}

The above table shows that the above expression is a very good approximation to the total number of culling operations for sieve ranges of about a hundred thousand (10&lt;sup&gt;5&lt;/sup&gt;) and above.

==Algorithmic complexity==
The sieve of Eratosthenes is a popular way to benchmark computer performance.&lt;ref name="peng1985fall"&gt;{{cite news | url=https://archive.org/stream/byte-magazine-1985-11/1985_11_BYTE_10-11_Inside_the_IBM_PCs#page/n245/mode/2up | title=One Million Primes Through the Sieve | work=BYTE | date=Fall 1985 | accessdate=19 March 2016 | author=Peng, T. A. | pages=243–244}}&lt;/ref&gt; As can be seen from the above by removing all constant offsets and constant factors and ignoring terms that tend to zero as n approaches infinity, the [[time complexity]] of calculating all primes below {{mvar|n}} in the [[random access machine]] model is {{math|''O''(''n'' log log ''n'')}} operations, a direct consequence of the fact that the [[prime harmonic series]] asymptotically approaches {{math|log log ''n''}}. It has an exponential time complexity with regard to input size, though, which makes it a [[Pseudo-polynomial time|pseudo-polynomial]] algorithm. The basic algorithm requires {{math|''O''(''n'')}} of memory.

The [[bit complexity]] of the algorithm is {{math|''O''&lt;big&gt;(&lt;/big&gt;''n'' (log ''n'') (log log ''n'')&lt;big&gt;)&lt;/big&gt;}} bit operations with a memory requirement of {{math|''O''(''n'')}}.&lt;ref&gt;Pritchard, Paul, "Linear prime-number sieves: a family tree," ''Sci. Comput. Programming'' '''9''':1 (1987), pp. 17–35.&lt;/ref&gt;

The normally implemented page segmented version has the same operational complexity of {{math|''O''(''n'' log log ''n'')}} as the non-segmented version but reduces the space requirements to the very minimal size of the segment page plus the memory required to store the base primes less than the square root of the range used to cull composites from successive page segments of size {{math|''O''&lt;big&gt;&lt;big&gt;(&lt;/big&gt;&lt;/big&gt;{{sfrac|{{sqrt|''n''}}|log ''n''}}&lt;big&gt;&lt;big&gt;)&lt;/big&gt;&lt;/big&gt;}}.

A special rarely if ever implemented segmented version of the sieve of Eratosthenes, with basic optimizations, uses {{math|''O''(''n'')}} operations and {{math|''O''&lt;big&gt;&lt;big&gt;(&lt;/big&gt;&lt;/big&gt;{{sqrt|''n''}}{{sfrac|log log ''n''|log ''n''}}&lt;big&gt;&lt;big&gt;)&lt;/big&gt;&lt;/big&gt;}} bits of memory.&lt;ref name="Pritchard1"&gt;Paul Pritchard, A sublinear additive sieve for finding prime numbers, Communications of the ACM 24 (1981), 18–23. {{MR|600730}}&lt;/ref&gt;&lt;ref name="Pritchard2"&gt;Paul Pritchard, Explaining the wheel sieve, Acta Informatica 17 (1982), 477–485. {{MR|685983}}&lt;/ref&gt;&lt;ref name="Pritchard3"&gt;Paul Pritchard, Fast compact prime number sieves (among others), Journal of Algorithms 4
(1983), 332–344. {{MR|729229}}&lt;/ref&gt;

To show that the above approximation in complexity is not very accurate even for about as large as practical a range, the following is a table of the estimated number of operations as a fraction of the range rounded to four places, the calculated ratio for a factor of ten change in range based on this estimate, and the factor based on the {{math|log log ''n''}} estimate for various ranges and wheel factorizations (the combo column uses a frequently practically used pre-cull by the maximum wheel factorization but only the 2/3/5/7 wheel for the wheel factor as the full factorization is difficult to implement efficiently for page segmentation):

:{| class="wikitable" 
! {{mvar|n}}
! colspan=3 | no wheel
! colspan=3 | odds
! colspan=3 | 2/3/5 wheel
! colspan=3 | 2/3/5/7 wheel
! colspan=3 | combo wheel
! colspan=3 | 2/3/5/7/11/13/17/19 wheel
|-
! 10&lt;sup&gt;6&lt;/sup&gt;
| 2.122 || 1.102 || 1.075
| 0.811 || 1.137 || 1.075
| 0.2903 || 1.22 || 1.075
| 0.2162 || 1.261 || 1.075
| 0.1524 || 1.416 || 1.075
| 0.114 || 1.416 || 1.075
|-
! 10&lt;sup&gt;7&lt;/sup&gt;
| 2.2863 || 1.077 || 1.059
| 0.8932 || 1.101 || 1.059
| 0.3341 || 1.151 || 1.059
| 0.2537 || 1.174 || 1.059
| 0.1899 || 1.246 || 1.059
| 0.1421 || 1.246 || 1.059
|-
! 10&lt;sup&gt;8&lt;/sup&gt;
| 2.4276 || 1.062 || 1.048
| 0.9638 || 1.079 || 1.048
| 0.3718 || 1.113 || 1.048
| 0.286 || 1.127 || 1.048
| 0.2222 || 1.17 || 1.048
| 0.1662 || 1.17 || 1.048
|-
! 10&lt;sup&gt;9&lt;/sup&gt;
| 2.5514 || 1.051 || 1.04
| 1.0257 || 1.064 || 1.04
| 0.4048 || 1.089 || 1.04
| 0.3143 || 1.099 || 1.04
| 0.2505 || 1.127 || 1.04
| 0.1874 || 1.127 || 1.04
|-
! 10&lt;sup&gt;10&lt;/sup&gt;
| 2.6615 || 1.043 || 1.035
| 1.0808 || 1.054 || 1.035
| 0.4342 || 1.073 || 1.035
| 0.3395 || 1.08 || 1.035
| 0.2757 || 1.101 || 1.035
| 0.2063 || 1.101 || 1.035
|-
! 10&lt;sup&gt;11&lt;/sup&gt;
| 2.7608 || 1.037 || 1.03
| 1.1304 || 1.046 || 1.03
| 0.4607 || 1.061 || 1.03
| 0.3622 || 1.067 || 1.03
| 0.2984 || 1.082 || 1.03
| 0.2232 || 1.082 || 1.03
|-
! 10&lt;sup&gt;12&lt;/sup&gt;
| 2.8511 || 1.033 || 1.027
| 1.1755 || 1.04 || 1.027
| 0.4847 || 1.052 || 1.027
| 0.3828 || 1.057 || 1.027
| 0.319 || 1.069 || 1.027
| 0.2387 || 1.069 || 1.027
|-
! 10&lt;sup&gt;13&lt;/sup&gt;
| 2.9339 || 1.029 || 1.024
| 1.217 || 1.035 || 1.024
| 0.5068 || 1.046 || 1.024
| 0.4018 || 1.049 || 1.024
| 0.3379 || 1.059 || 1.024
| 0.2528 || 1.059 || 1.024
|-
! 10&lt;sup&gt;14&lt;/sup&gt;
| 3.0104 || 1.026 || 1.022
| 1.2552 || 1.031 || 1.022
| 0.5272 || 1.04 || 1.022
| 0.4193 || 1.044 || 1.022
| 0.3554 || 1.052 || 1.022
| 0.2659 || 1.052 || 1.022
|-
! 10&lt;sup&gt;15&lt;/sup&gt;
| 3.0815 || 1.024 || 1.02
| 1.2907 || 1.028 || 1.02
| 0.5462 || 1.036 || 1.02
| 0.4355 || 1.039 || 1.02
| 0.3717 || 1.046 || 1.02
| 0.2781 || 1.046 || 1.02
|-
! 10&lt;sup&gt;16&lt;/sup&gt;
| 3.1478 || 1.022 || 1.018
| 1.3239 || 1.026 || 1.018
| 0.5639 || 1.032 || 1.018
| 0.4507 || 1.035 || 1.018
| 0.3868 || 1.041 || 1.018
| 0.2894 || 1.041 || 1.018
|}

The above shows that the {{math|log log ''n''}} estimate is not very accurate even for maximum practical ranges of about 10&lt;sup&gt;16&lt;/sup&gt;. One can see why it does not match by looking at the computational analysis above and seeing that within these practical sieving range limits, there are very significant constant offset terms such that the very slowly growing {{math|log log ''n''}} term does not get large enough so as to make these terms insignificant until the sieving range approaches infinity – well beyond any practical sieving range. Within these practical ranges, these significant constant offsets mean that the performance of the Sieve of Eratosthenes is much better than one would expect just using the asymptotic time complexity estimates by a significant amount, but that also means that the slope of the performance with increasing range is steeper than predicted as the benefit of the constant offsets becomes slightly less significant.

One should also note that in using the calculated operation ratios to the sieve range, it must be less than about 0.2587 in order to be faster than the often compared [[sieve of Atkin]] if the operations take approximately the same time each in CPU clock cycles, which is a reasonable assumption for the one huge bit array algorithm. Using that assumption, the sieve of Atkin is only faster than the maximally wheel factorized sieve of Eratosthenes for ranges of over 10&lt;sup&gt;13&lt;/sup&gt; at which point the huge sieve buffer array would need about a quarter of a terabyte (about 250 gigabytes) of RAM memory even if bit packing were used. An analysis of the page segmented versions will show that the assumption that the time per operation stays the same between the two algorithms does not hold for page segmentation and that the sieve of Atkin operations get slower much faster than the sieve of Eratosthenes with increasing range. Thus for practical purposes, the maximally wheel factorized Sieve of Eratosthenes is faster than the Sieve of Atkin although the Sieve of Atkin is faster for lesser amounts of wheel factorization.

Using [[big O notation]] is also not the correct way to compare practical performance of even variations of the Sieve of Eratosthenes as it ignores constant factors and offsets that may be very significant for practical ranges: The sieve of Eratosthenes variation known as the Pritchard wheel sieve&lt;ref name="Pritchard1" /&gt;&lt;ref name="Pritchard2" /&gt;&lt;ref name="Pritchard3" /&gt; has an {{math|''O''(''n'')}} performance, but its basic implementation requires either a "one large array" algorithm which limits its usable range to the amount of available memory else it needs to be page segmented to reduce memory use. When implemented with page segmentation in order to save memory, the basic algorithm still requires about {{math|''O''&lt;big&gt;&lt;big&gt;(&lt;/big&gt;&lt;/big&gt;{{sfrac|''n''|log ''n''}}&lt;big&gt;&lt;big&gt;)&lt;/big&gt;&lt;/big&gt;}} bits of memory (much more than the requirement of the basic page segmented sieve of Eratosthenes using {{math|''O''&lt;big&gt;&lt;big&gt;(&lt;/big&gt;&lt;/big&gt;{{sfrac|{{sqrt|''n''}}|log ''n''}}&lt;big&gt;&lt;big&gt;)&lt;/big&gt;&lt;/big&gt;}} bits of memory). Pritchard's work reduced the memory requirement to the limit as described above the table, but the cost is a fairly large constant factor of about three in execution time to about three quarters the sieve range due to the complex computations required to do so. As can be seen from the above table for the basic sieve of Eratosthenes, even though the resulting wheel sieve has {{math|''O''(''n'')}} performance and an acceptable memory requirement, it will never be faster than a reasonably Wheel Factorized basic sieve of Eratosthenes for any practical sieving range by a factor of about two. Other than that it is quite complex to implement, it is rarely practically implemented because it still uses more memory than the basic Sieve of Eratosthenes implementations described here as well as being slower for practical ranges. It is thus more of an intellectual curiosity than something practical.

==Euler's Sieve==
Euler's [[Proof of the Euler product formula for the Riemann zeta function#Proof of the Euler product formula|proof of the zeta product formula]] contains a version of the sieve of Eratosthenes in which each composite number is eliminated exactly once.&lt;ref name="intro" /&gt; The same sieve was rediscovered and observed to take [[linear time]] by {{harvtxt|Gries|Misra|1978}}.&lt;ref&gt;{{citation
 | last1 = Gries | first1 = David | author1-link = David Gries
 | last2 = Misra | first2 = Jayadev
 | date = December 1978
 | doi = 10.1145/359657.359660
 | issue = 12
 | journal = [[Communications of the ACM]]
 | pages = 999–1003
 | title = A linear sieve algorithm for finding prime numbers
 | volume = 21}}.&lt;/ref&gt; It, too, starts with a [[list (computing)|list]] of numbers from 2 to {{mvar|n}} in order. On each step the first element is identified as the next prime and the results of multiplying this prime with each element of the list are marked in the list for subsequent deletion. The initial element and the marked elements are then removed from the working sequence, and the process is repeated:
&lt;small&gt;  &lt;!-- these &amp;nbsp;s are put here hoping to prevent bots messing it up --&gt;
 &amp;nbsp;[2] (3) 5  7  &lt;u&gt;9&lt;/u&gt;  11  13 &lt;u&gt;15&lt;/u&gt; 17 19 &lt;u&gt;21&lt;/u&gt; 23 25 &lt;u&gt;27&lt;/u&gt; 29 31 &lt;u&gt;33&lt;/u&gt; 35 37 &lt;u&gt;39&lt;/u&gt; 41 43 &lt;u&gt;45&lt;/u&gt; 47 49 &lt;u&gt;51&lt;/u&gt; 53 55 &lt;u&gt;57&lt;/u&gt; 59 61 &lt;u&gt;63&lt;/u&gt; 65 67 &lt;u&gt;69&lt;/u&gt; 71 73 &lt;u&gt;75&lt;/u&gt; 77 79  ...
 &amp;nbsp;[3]    (5) 7     11  13    17 19    23 &lt;u&gt;25&lt;/u&gt;    29 31    &lt;u&gt;35&lt;/u&gt; 37    41 43    47 49    53 &lt;u&gt;55&lt;/u&gt;    59 61    &lt;u&gt;65&lt;/u&gt; 67    71 73    77 79  ...
 &amp;nbsp;[4]       (7)    11  13    17 19    23       29 31       37    41 43    47 &lt;u&gt;49&lt;/u&gt;    53       59 61       67    71 73    &lt;u&gt;77&lt;/u&gt; 79  ...
 &amp;nbsp;[5]             (11) 13    17 19    23       29 31       37    41 43    47       53       59 61       67    71 73       79  ...
 &amp;nbsp;[...]&lt;/small&gt;

Here the example is shown starting from odds, after the first step of the algorithm. Thus, on the {{mvar|k}}th step all the remaining multiples of the {{mvar|k}}th prime are removed from the list, which will thereafter contain only numbers coprime with the first {{mvar|k}} primes (cf. [[wheel factorization]]), so that the list will start with the next prime, and all the numbers in it below the square of its first element will be prime too.

Thus, when generating a bounded sequence of primes, when the next identified prime exceeds the square root of the upper limit, all the remaining numbers in the list are prime.&lt;ref name="intro" /&gt; In the example given above that is achieved on identifying 11 as next prime, giving a list of all primes less than or equal to 80.

Note that numbers that will be discarded by a step are still used while marking the multiples in that step, e.g., for the multiples of 3 it is {{nowrap|1=3 × 3 = 9}}, {{nowrap|1=3 × 5 = 15}}, {{nowrap|1=3 × 7 = 21}}, {{nowrap|1=3 × '''''9''''' = 27}}, ..., {{nowrap|1=3 × '''''15''''' = 45}}, ..., so care must be taken dealing with this.&lt;ref name="intro" /&gt;

==See also==
* [[Sieve of Sundaram]]
* [[Sieve of Atkin]]
* [[Sieve theory]]

==References==
{{Reflist|2}}

==External links==
* [http://primesieve.org/ primesieve - Very fast highly optimized C/C++ segmented Sieve of Eratosthenes]
* [https://www.encyclopediaofmath.org/index.php/Eratosthenes,_sieve_of ''Eratosthenes, sieve of'' at Encyclopaedia of Mathematics]
* [http://www.hbmeyer.de/eratosiv.htm Interactive JavaScript Page]
* [http://demonstrations.wolfram.com/SieveOfEratosthenes/ Sieve of Eratosthenes] by George Beck, [[Wolfram Demonstrations Project]].
* [https://wiki.haskell.org/Prime_numbers#Sieve_of_Eratosthenes Sieve of Eratosthenes in Haskell]
* [http://www.algolist.net/Algorithms/Number_theoretic_algorithms/Sieve_of_Eratosthenes Sieve of Eratosthenes algorithm illustrated and explained. Java and C++ implementations.]
* [http://zsmith.co/primes.html A related sieve written in x86 assembly language]
* [https://sites.google.com/site/bbuhrow/home/cuda-sieve-of-eratosthenes Fast optimized highly parallel  CUDA segmented Sieve of Eratosthenes in C]
* [http://paratechnical.blogspot.com/2011/01/c-implementation-of-parallel-sieve-of.html A parallel implementation in C#]
* [http://c2.com/cgi/wiki?SieveOfEratosthenesInManyProgrammingLanguages SieveOfEratosthenesInManyProgrammingLanguages c2 wiki page]
* [http://wwwhomes.uni-bielefeld.de/achim/prime_sieve.html The Art of Prime Sieving] Sieve of Eratosthenes in C from 1998 with nice features and algorithmic tricks explained.

&lt;!--spacing--&gt;
{{number theoretic algorithms}}

{{DEFAULTSORT:Sieve Of Eratosthenes}}
[[Category:Primality tests]]
[[Category:Articles with example pseudocode]]
[[Category:Sieve theory| ]]
[[Category:Algorithms]]</text>
      <sha1>bnaskgparwp91y7fce7umc4hyhenums</sha1>
    </revision>
  </page>
  <page>
    <title>Signed number representations</title>
    <ns>0</ns>
    <id>586694</id>
    <revision>
      <id>871242611</id>
      <parentid>871222971</parentid>
      <timestamp>2018-11-29T20:32:56Z</timestamp>
      <contributor>
        <username>Mindmatrix</username>
        <id>160367</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contribs/95.89.48.88|95.89.48.88]] ([[User talk:95.89.48.88|talk]]) to last version by Bumm13</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="21587">{{Refimprove|date=April 2013}}

In [[computing]], '''signed number representations''' are required to encode [[negative number]]s in binary number systems.

In [[mathematics]], negative numbers in any base are represented by prefixing them with a minus ("&amp;minus;") sign. However, in [[computer hardware]], numbers are represented only as sequences of [[bit]]s, without extra symbols. The four best-known methods of extending the [[binary numeral system]] to represent [[signed number]]s are: [[#Sign-and-magnitude method|sign-and-magnitude]], [[#Ones' complement|ones' complement]], [[#Two's complement|two's complement]], and [[#Excess-K|offset binary]]. Some of the alternative methods use implicit instead of explicit signs, such as negative binary, using the [[#Base −2|base −2]]. Corresponding methods can be devised for [[positional notation|other bases]], whether positive, negative, fractional, or other elaborations on such themes.

There is no definitive criterion by which any of the representations is universally superior. The representation used in most current computing devices is two's complement, although the [[UNIVAC 1100/2200 series|Unisys ClearPath Dorado series]] mainframes use ones' complement.

==History==
The early days of digital computing were marked by a lot of competing ideas about both hardware technology and mathematics technology (numbering systems). One of the great debates was the format of negative numbers, with some of the era's most expert people having very strong and different opinions.{{Citation needed|reason=Please reference these debates|date=May 2012}} One camp supported [[two's complement]], the system that is dominant today.  Another camp supported ones' complement, where any positive value is made into its negative equivalent by inverting all of the bits in a word.  A third group supported "sign &amp;amp; magnitude" (sign-magnitude), where a value is changed from positive to negative simply by toggling the word's sign (high-order) bit.

There were arguments for and against each of the systems.&lt;!--{{Citation needed|reason=Cite these arguments|date=May 2012}}--the rest of the article gives reasons --&gt;  Sign &amp;amp; magnitude allowed for easier tracing of memory dumps (a common process 40 years ago) as small numeric values use fewer 1 bits.&lt;!-- this is obvious enough --&gt; Internally, these systems did ones' complement math so numbers would have to be converted to ones' complement values when they were transmitted from a register to the math unit and then converted back to sign-magnitude when the result was transmitted back to the register&lt;!--{{Citation needed|reason=Cite one of these systems|date=May 2012}}-- this is the most hardware efficient way to do it. It is likely how they did it--&gt;.  The electronics required more gates than the other systems{{snd}}a key concern when the cost and packaging of discrete transistors was critical.  IBM was one of the early supporters of sign-magnitude, with their [[IBM 704|704]], [[IBM 709|709]] and [[IBM 7090|709x]] series computers being perhaps the best known systems to use it.

Ones' complement allowed for somewhat simpler hardware designs as there was no need to convert values when passed to and from the math unit.  But it also shared an undesirable characteristic with sign-magnitude{{snd}}the ability to represent [[negative zero]] (−0). Negative zero behaves exactly like positive zero; when used as an operand in any calculation, the result will be the same whether an operand is positive or negative zero.  The disadvantage, however, is that the existence of two forms of the same value necessitates two rather than a single comparison when checking for equality with zero.  Ones' complement subtraction can also result in an [[end-around borrow]] (described below).  It can be argued that this makes the addition/subtraction logic more complicated or that it makes it simpler as a subtraction requires simply inverting the bits of the second operand as it is passed to the adder.  The [[PDP-1]], [[CDC 160 series]], [[CDC 3000]] series, [[CDC 6000 series]], [[UNIVAC 1100]] series, and the [[LINC]] computer use ones' complement representation.

Two's complement is the easiest to implement in hardware, which may be the ultimate reason for its widespread popularity.{{Citation needed|reason=Has it ever been proven to be the easiest to implement|date=May 2012}} Processors on the early mainframes often consisted of thousands of transistors{{snd}}eliminating a significant number of transistors was a significant cost savings.  Mainframes such as the [[IBM System/360]], the [[GE-600 series]],&lt;ref&gt;{{cite book|url=http://ed-thelen.org/comp-hist/GE-635.html#Binary%20Fixed-Point%20Numbers|title=GE-625 / 635 Programming Reference Manual|publisher=[[General Electric]]|date=January 1966|accessdate=August 15, 2013}}&lt;/ref&gt; and the [[PDP-6]] and [[PDP-10]] use two's complement, as did minicomputers such as the [[PDP-5]] and [[PDP-8]] and the [[PDP-11]] and [[VAX]].  The architects of the early integrated circuit-based CPUs ([[Intel 8080]], etc.) chose to use two's complement math.  As IC technology advanced, virtually all adopted two's complement technology. [[x86]],&lt;ref&gt;{{cite book|url=http://download.intel.com/products/processor/manual/325462.pdf|title=Intel 64 and IA-32 Architectures Software Developer’s Manual|at=Section 4.2.1|publisher=[[Intel]]|accessdate=August 6, 2013}}&lt;/ref&gt; [[m68k]], [[Power Architecture]],&lt;ref&gt;{{cite book|url=https://www.power.org/documentation/power-isa-version-2-07/|title=Power ISA Version 2.07|at=Section 1.4|publisher=[[Power.org]]|accessdate=August 6, 2013}},&lt;/ref&gt; [[MIPS architecture|MIPS]], [[SPARC]], [[ARM architecture|ARM]], [[Itanium]], [[PA-RISC]], and [[DEC Alpha]] processors are all two's complement.

=={{anchor|Sign-and-magnitude method}}Signed magnitude representation==
This representation is also called "sign–magnitude" or "sign and magnitude" representation.  In this approach, a number's sign is represented with a '''[[sign bit]]''': setting that [[bit]] (often the [[most significant bit]]) to 0 for a positive number or positive zero, and setting it to 1 for a negative number or negative zero.  The remaining bits in the number indicate the magnitude (or [[absolute value]]). Hence, in a [[byte]] with only seven bits (apart from the sign bit), the magnitude can range from 0000000 (0) to 1111111 (127). Thus numbers ranging from &amp;minus;127&lt;sub&gt;10&lt;/sub&gt; to +127&lt;sub&gt;10&lt;/sub&gt; can be represented once the sign bit (the eighth bit) is added.  For example, &amp;minus;43&lt;sub&gt;10&lt;/sub&gt; encoded in an eight-bit byte is &lt;strong&gt;1&lt;/strong&gt;0101011 while 43&lt;sub&gt;10&lt;/sub&gt; is &lt;strong&gt;0&lt;/strong&gt;0101011. A consequence of using signed magnitude representation is that there are two ways to represent zero, 00000000 (0) and 10000000 ([[−0]]).

This approach is directly comparable to the common way of showing a sign (placing a "+" or "&amp;minus;" next to the number's magnitude).  Some early binary computers (e.g., [[IBM 7090]]) use this representation, perhaps because of its natural relation to common usage. Signed magnitude is the most common way of representing the [[significand]] in [[floating point]] values.

==Ones' complement==
{|class="wikitable" style="float:right; width: 20em; margin-left: 1em; text-align:center"
|+ Eight-bit ones' complement
|-
! Binary value
! Ones' complement interpretation
! Unsigned interpretation
|-
| 00000000 ||   +0 ||   0
|-
| 00000001 ||    1 ||   1
|-
|    ⋮    ||   ⋮  ||  ⋮ 
|-
| 01111101 ||  125 || 125
|-
| 01111110 ||  126 || 126
|-
| 01111111 ||  127 || 127
|-
| 10000000 || −127 || 128
|-
| 10000001 || −126 || 129
|-
| 10000010 || −125 || 130
|-
|    ⋮    ||   ⋮  ||  ⋮ 
|-
| 11111101 ||   −2 || 253
|-
| 11111110 ||   −1 || 254
|-
| 11111111 ||   −0 || 255
|}

{{Main|Ones' complement}}

Alternatively, a system known as ones' complement can be used to represent negative numbers. The ones' complement form of a negative binary number is the [[bitwise NOT]] applied to it, i.e. the "complement" of its positive counterpart. Like sign-and-magnitude representation, ones' complement has two representations of 0: 00000000 (+0) and 11111111 ([[−0]]).

As an example, the ones' complement form of 00101011 (43&lt;sub&gt;10&lt;/sub&gt;) becomes 11010100 (&amp;minus;43&lt;sub&gt;10&lt;/sub&gt;). The range of [[signedness|signed]] numbers using ones' complement is represented by {{nobr|−(2&lt;sup&gt;''N''−1&lt;/sup&gt; − 1)}} to {{nobr|(2&lt;sup&gt;''N''−1&lt;/sup&gt; − 1)}} and ±0. A conventional eight-bit byte is −127&lt;sub&gt;10&lt;/sub&gt; to +127&lt;sub&gt;10&lt;/sub&gt; with zero being either 00000000 (+0) or 11111111 (−0).

To add two numbers represented in this system, one does a conventional binary addition, but it is then necessary to do an ''end-around carry'': that is, add any resulting [[carry flag|carry]] back into the resulting sum. To see why this is necessary, consider the following example showing the case of the addition of &amp;minus;1 (11111110) to +2 (00000010):

&lt;pre style="width:35em;"&gt;
          binary    decimal
        11111110     –1
     +  00000010     +2
     ───────────     ──
      1 00000000      0   ← Not the correct answer
               1     +1   ← Add carry
     ───────────     ──
        00000001      1   ← Correct answer
&lt;/pre&gt;

In the previous example, the first binary addition gives 00000000, which is incorrect. The correct result (00000001) only appears when the carry is added back in. 

A remark on terminology: The system is referred to as "ones' complement" because the [[Negation#Programming|negation]] of a positive value &lt;var&gt;x&lt;/var&gt; (represented as the [[bitwise NOT]] of &lt;var&gt;x&lt;/var&gt;) can also be formed by subtracting &lt;var&gt;x&lt;/var&gt; from the ones' complement representation of zero that is a long sequence of ones (−0). Two's complement arithmetic, on the other hand, forms the negation of &lt;var&gt;x&lt;/var&gt; by subtracting &lt;var&gt;x&lt;/var&gt; from a single large power of two that is [[Congruence relation|congruent]] to +0.&lt;ref&gt;Donald Knuth: ''[[The Art of Computer Programming]]'', Volume 2: Seminumerical Algorithms, chapter 4.1&lt;/ref&gt; Therefore, ones' complement and two's complement representations of the same negative value will differ by one.

Note that the ones' complement representation of a negative number can be obtained from the sign-magnitude representation merely by bitwise complementing the magnitude.

==Two's complement==
{|class="wikitable" style="float:right; width: 20em; margin-left: 1em; text-align:center"
|+ Eight-bit two's complement
|-
! Binary value
! Two's complement interpretation
! Unsigned interpretation
|-
| 00000000 ||    0 ||   0
|-
| 00000001 ||    1 ||   1
|-
|    ⋮     ||   ⋮  ||  ⋮ 
|-
| 01111110 ||  126 || 126
|-
| 01111111 ||  127 || 127
|-
| 10000000 || −128 || 128
|-
| 10000001 || −127 || 129
|-
| 10000010 || −126 || 130
|-
|    ⋮     ||   ⋮  ||  ⋮ 
|-
| 11111110 ||   −2 || 254
|-
| 11111111 ||   −1 || 255
|}

{{Main|Two's complement}}

The problems of multiple representations of 0 and the need for the [[end-around carry]] are circumvented by a system called '''two's complement'''. In two's complement, negative numbers are represented by the bit pattern which is one greater (in an unsigned sense) than the ones' complement of the positive value.

In two's-complement, there is only one zero, represented as 00000000. Negating a number (whether negative or positive) is done by inverting all the bits and then adding one to that result.&lt;ref&gt;{{cite web|title=Two's Complement|url=http://www.cs.cornell.edu/~tomf/notes/cps104/twoscomp.html|publisher=[[Cornell University]]|date=April 2000|author=Thomas Finley|accessdate=15 September 2015}}&lt;/ref&gt; This actually reflects the [[ring (mathematics)|ring]] structure on all integers [[modular arithmetic|modulo]] [[power of two|2&lt;sup&gt;''N''&lt;/sup&gt;]]: &lt;math&gt;\mathbb{Z}/2^N\mathbb{Z}&lt;/math&gt;. Addition of a pair of two's-complement integers is the same as addition of a pair of [[signedness|unsigned numbers]] (except for detection of [[integer overflow|overflow]], if that is done); the same is true for subtraction and even for ''N'' lowest significant bits of a product (value of multiplication). For instance, a two's-complement addition of 127 and −128 gives the same binary bit pattern as an unsigned addition of 127 and 128, as can be seen from the 8-bit two's complement table.

An easier method to get the negation of a number in two's complement is as follows:

{|class="wikitable"
!
! Example 1
! Example 2
|-
| 1. Starting from the right, find the first "1"
|align="center"| 0010100'''1'''
|align="center"| 00101'''1'''00
|-
| 2. Invert all of the bits to the left of that "1"
|align="center"| '''1101011'''1
|align="center"| '''11010'''100
|}

Method two:

# Invert all the bits through the number
# Add one

Example: for +2, which is 00000010 in binary (the ~ character is the [[C (programming language)|C]] [[bitwise NOT]] operator, so ~X means "invert all the bits in X"):

# ~00000010 → 11111101
# 11111101 + 1 → 11111110 (−2 in two's complement)

{{clear}}

=={{anchor|Excess-128|Excess-K}}Offset binary==
{|class="wikitable" style="float:right; width: 20em; margin-left: 1em; text-align:center"
|+ Eight-bit excess-128
|-
!Binary value
!Excess-128 interpretation
!Unsigned interpretation
|-
| 00000000 || −128 ||  0
|-
| 00000001 || −127 ||  1
|-
|    ⋮    ||   ⋮  ||  ⋮ 
|-
| 01111111 ||  −1  || 127
|-
| 10000000 ||   0  || 128
|-
| 10000001 ||   1  || 129
|-
|    ⋮    ||   ⋮  ||  ⋮ 
|-
| 11111111 || +127 || 255
|-
|}

{{Main|Offset binary}}

[[Offset binary]], also called [[Excess-K|excess-&lt;var&gt;K&lt;/var&gt;]] or [[biased representation (arithmetics)|biased representation]], uses a pre-specified number &lt;var&gt;K&lt;/var&gt; as a biasing value. A value is represented by the unsigned number which is &lt;var&gt;K&lt;/var&gt; greater than the intended value. Thus 0 is represented by &lt;var&gt;K&lt;/var&gt;, and &amp;minus;&lt;var&gt;K&lt;/var&gt; is represented by the all-zeros bit pattern. This can be seen as a slight modification and generalization of the aforementioned two's-complement, which is virtually the {{nobr|excess-(2&lt;sup&gt;&lt;var&gt;N&lt;/var&gt;−1&lt;/sup&gt;)}} representation with [[negated]] [[most significant bit]].

Biased representations are now primarily used for the exponent of [[floating-point]] numbers. The [[IEEE floating-point standard]] defines the exponent field of a [[single-precision]] (32-bit) number as an 8-bit [[excess-127]] field. The [[double-precision]] (64-bit) exponent field is an 11-bit [[excess-1023]] field; see [[exponent bias]]. It also had use for binary-coded decimal numbers as [[excess-3]].

==Base −2==
{{See also|Negative base}}
In conventional binary number systems, the base, or [[radix]], is 2; thus the rightmost bit represents 2&lt;sup&gt;0&lt;/sup&gt;, the next bit represents 2&lt;sup&gt;1&lt;/sup&gt;, the next bit 2&lt;sup&gt;2&lt;/sup&gt;, and so on. However, a binary number system with base &amp;minus;2 is also possible.
The rightmost bit represents {{nowrap|(&amp;minus;2)&lt;sup&gt;0&lt;/sup&gt; {{=}} +1}}, the next bit represents {{nowrap|(&amp;minus;2)&lt;sup&gt;1&lt;/sup&gt; {{=}} &amp;minus;2}}, the next bit {{nowrap|(&amp;minus;2)&lt;sup&gt;2&lt;/sup&gt; {{=}} +4}} and so on, with alternating sign. The numbers that can be represented with four bits are shown in the comparison table below.

The range of numbers that can be represented is asymmetric. If the word has an even number of bits, the magnitude of the largest negative number that can be represented is twice as large as the largest positive number that can be represented, and vice versa if the word has an odd number of bits.

==Comparison table==
The following table shows the positive and negative integers that can be represented using four bits.

{| class="wikitable" style="text-align: center"
|+ Four-bit integer representations
|-
! style="width: 7em" | Decimal
! style="width: 7em" | Unsigned
! style="width: 7em" | Sign and magnitude
! style="width: 7em" | Ones' complement
! style="width: 7em" | Two's complement
! style="width: 7em" | Excess-8 (biased)
! style="width: 7em" | Base &amp;minus;2
|-
| align=right|+16&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|-
| align=right|+15&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| 1111
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|-
| align=right|+14&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| 1110
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|-
| align=right|+13&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| 1101
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|-
| align=right|+12&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| 1100
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|-
| align=right|+11&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| 1011
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|-
| align=right|+10&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| 1010
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|-
| align=right|+9&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| 1001
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|-
| align=right|+8&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| 1000
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|-
| align=right|+7&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| 0111
| 0111
| 0111
| 0111
| 1111
| {{n/a}}
|-
| align=right|+6&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| 0110
| 0110
| 0110
| 0110
| 1110
| {{n/a}}
|-
| align=right|+5&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| 0101
| 0101
| 0101
| 0101
| 1101
| 0101
|-
| align=right|+4&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| 0100
| 0100
| 0100
| 0100
| 1100
| 0100
|-
| align=right|+3&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| 0011
| 0011
| 0011
| 0011
| 1011
| 0111
|-
| align=right|+2&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| 0010
| 0010
| 0010
| 0010
| 1010
| 0110
|-
| align=right|+1&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| 0001
| 0001
| 0001
| 0001
| 1001
| 0001
|-
| align=right|+0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| rowspan=2 valign=center |0000
| 0000
| 0000
| rowspan=2 valign=center | 0000
| rowspan=2 valign=center | 1000
| rowspan=2 valign=center | 0000
|-
| align=right|−0&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| 1000
| 1111
|-
| align=right|−1&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| {{n/a}}
| 1001
| 1110
| 1111
| 0111
| 0011
|-
| align=right|−2&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| {{n/a}}
| 1010
| 1101
| 1110
| 0110
| 0010
|-
| align=right|−3&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| {{n/a}}
| 1011
| 1100
| 1101
| 0101
| 1101
|-
| align=right|−4&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| {{n/a}}
| 1100
| 1011
| 1100
| 0100
| 1100
|-
| align=right|−5&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| {{n/a}}
| 1101
| 1010
| 1011
| 0011
| 1111
|-
| align=right|−6&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| {{n/a}}
| 1110
| 1001
| 1010
| 0010
| 1110
|-
| align=right|−7&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| {{n/a}}
| 1111
| 1000
| 1001
| 0001
| 1001
|-
| align=right|−8&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| {{n/a}}
| {{n/a}}
| {{n/a}}
| 1000
| 0000
| 1000
|-
| align=right|−9&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
| 1011
|-
| align=right|−10&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
| 1010
|-
| align=right|−11&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
| {{n/a}}
|}


Same table, as viewed from "given these binary bits, what is the number as interpreted by the representation system":

{| class="wikitable" style="text-align: center"
|-
! Binary !! Unsigned !! Sign and magnitude !! Ones' complement !! Two's complement !! Excess-8 !! Base &amp;minus;2
|-
| 0000 || 0 || 0 || 0 || 0 || &amp;minus;8 || 0
|-
| 0001 || 1 || 1 || 1 || 1 || &amp;minus;7 || 1
|-
| 0010 || 2 || 2 || 2 || 2 || &amp;minus;6 || &amp;minus;2
|-
| 0011 || 3 || 3 || 3 || 3 || &amp;minus;5 || &amp;minus;1
|-
| 0100 || 4 || 4 || 4 || 4 || &amp;minus;4 || 4
|-
| 0101 || 5 || 5 || 5 || 5 || &amp;minus;3 || 5
|-
| 0110 || 6 || 6 || 6 || 6 || &amp;minus;2 || 2
|-
| 0111 || 7 || 7 || 7 || 7 || &amp;minus;1 || 3
|-
| 1000 || 8 || &amp;minus;0 || &amp;minus;7 || &amp;minus;8 || 0 || &amp;minus;8
|-
| 1001 || 9 || &amp;minus;1 || &amp;minus;6 || &amp;minus;7 || 1 || &amp;minus;7
|-
| 1010 || 10 || &amp;minus;2 || &amp;minus;5 || &amp;minus;6 || 2 || &amp;minus;10
|-
| 1011 || 11 || &amp;minus;3 || &amp;minus;4 || &amp;minus;5 || 3 || &amp;minus;9
|-
| 1100 || 12 || &amp;minus;4 || &amp;minus;3 || &amp;minus;4 || 4 || &amp;minus;4
|-
| 1101 || 13 || &amp;minus;5 || &amp;minus;2 || &amp;minus;3 || 5 || &amp;minus;3
|-
| 1110 || 14 || &amp;minus;6 || &amp;minus;1 || &amp;minus;2 || 6 || &amp;minus;6
|-
| 1111 || 15 || &amp;minus;7 || &amp;minus;0 || &amp;minus;1 || 7 || &amp;minus;5
|}

==Other systems==
Google's [[Protocol Buffers]] "zig-zag encoding" is a system similar to sign-and-magnitude, but uses the [[least significant bit]] to represent the sign and has a single representation of zero. This allows a [[variable-length quantity]] encoding intended for nonnegative (unsigned) integers to be used efficiently for signed integers.&lt;ref&gt;[http://developers.google.com/protocol-buffers/docs/encoding#types Protocol Buffers: Signed Integers]&lt;/ref&gt;

Another approach is to give each [[numerical digit|digit]] a sign, yielding the [[signed-digit representation]]. For instance, in 1726, [[John Colson]] advocated reducing expressions to "small numbers", numerals 1, 2, 3, 4, and 5. In 1840, [[Augustin Cauchy]] also expressed preference for such modified decimal numbers to reduce errors in computation.

==See also==
{{Portal|Computer science}}

* [[Balanced ternary]]
* [[Binary-coded decimal]]
* [[Computer numbering formats]]
* [[Method of complements]]
* [[Signedness]]

==References==
{{reflist}}
* Ivan Flores, ''The Logic of Computer Arithmetic'', Prentice-Hall (1963)
* Israel Koren, ''Computer Arithmetic Algorithms'', A.K. Peters (2002), {{ISBN|1-56881-160-8}}

{{DEFAULTSORT:Signed Number Representations}}
[[Category:Computer arithmetic]]

[[ca:Representació de nombres amb signe]]
[[cs:Dvojková soustava#Zobrazení záporných čísel]]
[[fr:Système binaire#Représentation des entiers négatifs]]
[[vi:Biểu diễn số âm]]</text>
      <sha1>a7s0575wtpvs21octjxnp6sgkvivo7j</sha1>
    </revision>
  </page>
  <page>
    <title>Simulation preorder</title>
    <ns>0</ns>
    <id>396513</id>
    <revision>
      <id>830517781</id>
      <parentid>803964326</parentid>
      <timestamp>2018-03-15T10:10:44Z</timestamp>
      <contributor>
        <ip>134.96.74.163</ip>
      </contributor>
      <comment>/* Similarity of separate transition systems */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3182">In [[theoretical computer science]] a '''simulation preorder''' is a [[Relation (mathematics)|relation]] between [[state transition system]]s associating systems which behave in the same way in the sense that one system ''simulates'' the other.

Intuitively, a system simulates another system if it can match all of its moves.

The basic definition relates states within one transition system, but this is easily adapted to relate two separate transition systems by building a system consisting of the [[disjoint union]] of the corresponding components.

==Formal definition==
Given a [[state transition system|labelled state transition system]] (S, Λ, →), a ''simulation'' relation is a [[binary relation]] R over S (i.e. R ⊆ S &amp;times; S) such that for every pair of elements (p,q) ∈ R, for all α ∈ Λ, and for all p' ∈ S, 

:&lt;math&gt; p  \overset{\alpha}{\rightarrow}   p' &lt;/math&gt;

implies that there is a q' ∈ S such that

:&lt;math&gt; q   \overset{\alpha}{\rightarrow}  q' &lt;/math&gt;

and (p',q') ∈ R.

Equivalently, in terms of [[Composition of relations|relational composition]]:
:&lt;math&gt;R^{-1}\,; \overset{\alpha}{\rightarrow}\quad {\subseteq}\quad \overset{\alpha}{\rightarrow}\,; R^{-1}&lt;/math&gt;

Given two states p and q in S, q ''simulates'' p, written p ≤ q if there is a simulation R such that (p, q) ∈ R. The relation ≤ is a [[preorder]], and is usually called the ''simulation preorder''. It is the largest simulation relation over a given transition system.

Two states ''p'' and ''q'' are said to be ''similar'', written p ≤≥ q, if ''p'' simulates ''q'' and ''q'' simulates ''p''. Similarity is an [[equivalence relation]], but it is coarser than [[bisimilarity]].

==Similarity of separate transition systems==
When comparing two different transition systems (S', Λ', →') and (S", Λ", →"), the basic notions of simulation and similarity can be used by forming the disjoint composition of the two machines, (S, Λ, →) with S = S' ∐ S", Λ = Λ' ∪ Λ" and → = →' ∪ →", where ∐ is the [[disjoint union]] operator between sets.

==See also==
* [[State transition system]]s
* [[Bisimulation]]
* [[Coinduction]]
* [[Operational semantics]]

== References ==
# {{Cite conference
 | first = David
 | last = Park
 | year = 1981
 | title = Concurrency and Automata on Infinite Sequences
 | booktitle = Proceedings of the 5th GI-Conference, Karlsruhe
 | series = [[Lecture Notes in Computer Science]]
 | editor = Deussen, Peter
 | pages = 167–183
 | volume = 104
 | publisher = [[Springer-Verlag]]
 | isbn = 978-3-540-10576-3
 | doi = 10.1007/BFb0017309
}}
# {{Cite book
 | last = Milner
 | first = Robin
 | title = Communication and Concurrency
 | year = 1989
 | publisher = [[Prentice Hall]]
 | isbn = 0-13-114984-9
}}
# {{Cite conference
 | last = van Glabbeek
 | first = R. J.
 | title = The Linear Time – Branching Time Spectrum I: The Semantics of Concrete, Sequential Processes
 | booktitle = Handbook of Process Algebra
 | url = http://boole.stanford.edu/pub/spectrum1.pdf.gz
 | chapter = 1
 | pages = 3–99
 | year = 2001
 | publisher = Elsevier
}}

{{DEFAULTSORT:Simulation Preorder}}
[[Category:Theoretical computer science]]</text>
      <sha1>5mtulhvcskjwrv21e2jbkegw8f4x2le</sha1>
    </revision>
  </page>
  <page>
    <title>Sphericity</title>
    <ns>0</ns>
    <id>2628049</id>
    <revision>
      <id>850439131</id>
      <parentid>850407272</parentid>
      <timestamp>2018-07-15T21:46:13Z</timestamp>
      <contributor>
        <username>Mwtoews</username>
        <id>711150</id>
      </contributor>
      <minor/>
      <comment>cleanup self-link [[Sphericity scale]]; Image: -&gt; File:</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6803">{{Use dmy dates|date=June 2018}}

{{for|sphericity in statistics|Mauchly's sphericity test}}
[[File:Rounding &amp; sphericity EN.svg|thumb|300px|Schematic representation of difference in grain shape. Two parameters are shown: sphericity (vertical) and [[roundness (object)|rounding]] (horizontal).]]
'''Sphericity''' is the measure of how closely the shape of an object approaches that of a mathematically perfect [[sphere]]. For example, the sphericity of the [[ball (bearing)|balls]] inside a [[ball bearing]] determines the [[quality (business)|quality]] of the bearing, such as the load it can bear or the speed at which it can turn without failing. Sphericity is a specific example of a [[compactness measure of a shape]].  Defined by Wadell in 1935,&lt;ref&gt;{{cite journal |first=Hakon |last=Wadell |title=Volume, Shape and Roundness of Quartz Particles |journal=The Journal of Geology |volume=43 |year=1935 |pages=250–280 |doi=10.1086/624298 |issue=3 }}&lt;/ref&gt; the sphericity, &lt;math&gt;\Psi &lt;/math&gt;, of a particle is: the ratio of the [[surface area]] of a [[sphere]] (with the same [[volume]] as the given particle) to the surface area of the particle:

:&lt;math&gt;\Psi = \frac{\pi^{\frac{1}{3}}(6V_p)^{\frac{2}{3}}}{A_p}
&lt;/math&gt;

where &lt;math&gt;V_p&lt;/math&gt; is volume of the particle and &lt;math&gt;A_p&lt;/math&gt; is the surface area of the particle. The sphericity of a sphere is [[1 (number)|unity]] by definition and, by the [[isoperimetric inequality]], any particle which is not a sphere will have sphericity less than 1.

Sphericity applies in [[three-dimensional space|three dimensions]]; its analogue in [[two-dimensional space|two dimensions]], such as the [[cross section (geometry)|cross sectional]] circles along a [[cylinder|cylindrical]] object such as a [[shaft (mechanical engineering)|shaft]], is called [[roundness (object)|roundness]].

== Ellipsoidal objects ==
{{See_also|Earth radius}}
The sphericity, &lt;math&gt;\Psi &lt;/math&gt;, of an [[oblate spheroid]] (similar to the shape of the planet [[Earth]]) is:

:&lt;math&gt;\Psi = 
\frac{\pi^{\frac{1}{3}}(6V_p)^{\frac{2}{3}}}{A_p} = 
\frac{2\sqrt[3]{ab^2}}{a+\frac{b^2}{\sqrt{a^2-b^2}}\ln{\left(\frac{a+\sqrt{a^2-b^2}}b\right)}},
&lt;/math&gt; &lt;!-- can we get the value for the Earth? --&gt;

where ''a'' and ''b'' are the [[Semi-major axis|semi-major]] and [[semi-minor axis|semi-minor]] axes respectively.

== Derivation ==

Hakon Wadell defined sphericity as the surface area of a 
sphere of the same volume as the particle divided by the actual surface area of the particle. 

First we need to write surface area of the sphere, &lt;math&gt;A_s&lt;/math&gt; in terms of the volume of the particle, &lt;math&gt;V_p&lt;/math&gt;

:&lt;math&gt;A_{s}^3 = \left(4 \pi r^2\right)^3 = 4^3 \pi^3 r^6 = 4 \pi \left(4^2 \pi^2 r^6\right) = 4 \pi \cdot 3^2 \left(\frac{4^2 \pi^2}{3^2} r^6\right) = 36 \pi \left(\frac{4 \pi}{3} r^3\right)^2 = 36\,\pi V_{p}^2
&lt;/math&gt;

therefore

:&lt;math&gt;A_{s} = \left(36\,\pi V_{p}^2\right)^{\frac{1}{3}} = 36^{\frac{1}{3}} \pi^{\frac{1}{3}} V_{p}^{\frac{2}{3}} = 6^{\frac{2}{3}} \pi^{\frac{1}{3}} V_{p}^{\frac{2}{3}} = \pi^{\frac{1}{3}} \left(6V_{p}\right)^{\frac{2}{3}}
&lt;/math&gt;

hence we define &lt;math&gt;\Psi&lt;/math&gt; as:

:&lt;math&gt;
\Psi = \frac{A_s}{A_p} = \frac{ \pi^{\frac{1}{3}} \left(6V_{p}\right)^{\frac{2}{3}} }{A_{p}}
&lt;/math&gt;

== Sphericity of common objects ==
{| border="1" cellpadding="7" style="margin:0 auto; text-align:center; border-collapse: collapse;"
|-
!Name 
!Picture
!Volume
!Surface Area
!Sphericity
|-
| colspan=5 align=left|'''[[Platonic Solids]]'''
|-
|[[tetrahedron]]
| [[File:tetrahedron.jpg|50px|Tetrahedron]]
| &lt;math&gt;\frac{\sqrt{2}}{12}\,s^3&lt;/math&gt; || &lt;math&gt;\sqrt{3}\,s^2&lt;/math&gt; ||  &lt;math&gt;\left(\frac{\pi}{6\sqrt{3}}\right)^{\frac{1}{3}} \approx 0.671&lt;/math&gt;
|-
|[[cube]] (hexahedron)
|[[File:hexahedron.jpg|50px|Hexahedron (cube)]]
| &lt;math&gt;\,s^3&lt;/math&gt; || &lt;math&gt;6\,s^2&lt;/math&gt; || 
&lt;math&gt;\left(
\frac{\pi}{6}
\right)^{\frac{1}{3}} \approx 0.806&lt;/math&gt;
|-
|[[octahedron]]
|[[File:octahedron.svg|50px|Octahedron]]
| &lt;math&gt; \frac{1}{3} \sqrt{2}\, s^3&lt;/math&gt; || &lt;math&gt; 2 \sqrt{3}\, s^2&lt;/math&gt; || 
&lt;math&gt;\left(
\frac{\pi}{3\sqrt{3}}
\right)^{\frac{1}{3}} \approx 0.846 &lt;/math&gt;
|-
|[[dodecahedron]]
| [[File:POV-Ray-Dodecahedron.svg|50px|Dodecahedron]]
| &lt;math&gt; \frac{1}{4} \left(15 + 7\sqrt{5}\right)\, s^3&lt;/math&gt; || &lt;math&gt; 3 \sqrt{25 + 10\sqrt{5}}\, s^2&lt;/math&gt; || 
&lt;math&gt;\left(
\frac{\left(15 + 7\sqrt{5}\right)^2 \pi}{12\left(25+10\sqrt{5}\right)^{\frac{3}{2}}}
\right)^{\frac{1}{3}} \approx 0.910&lt;/math&gt;
|-
|[[icosahedron]]
| [[File:icosahedron.jpg|50px|Icosahedron]]
| &lt;math&gt;\frac{5}{12}\left(3+\sqrt{5}\right)\, s^3&lt;/math&gt; || &lt;math&gt;5\sqrt{3}\,s^2&lt;/math&gt; || &lt;math&gt;\left(
\frac{ \left(3 + \sqrt{5} \right)^2 \pi}{60\sqrt{3}}
\right)^{\frac{1}{3}} \approx 0.939&lt;/math&gt;
|-
| colspan=5 align=left|'''Round Shapes'''
|-
|ideal [[cone (geometry)|cone]]&lt;BR&gt;&lt;math&gt;(h=2\sqrt{2}r)&lt;/math&gt;
||[[File:Blender-mesh-cone.png|70px]] || &lt;math&gt;\frac{1}{3} \pi\, r^2 h &lt;/math&gt;&lt;BR&gt;
&lt;math&gt;= \frac{2\sqrt{2}}{3} \pi\, r^3&lt;/math&gt; 
|| &lt;math&gt;\pi\, r (r + \sqrt{r^2 + h^2}) &lt;/math&gt;&lt;BR&gt;
&lt;math&gt;= 4 \pi\, r^2 &lt;/math&gt;
|| &lt;math&gt;\left(
\frac{1}{2}
\right)^{\frac{1}{3}} \approx 0.794&lt;/math&gt;
|-
|hemisphere&lt;BR&gt;(half sphere)
|[[File:Sphere symmetry group cs.png|70px]]
| &lt;math&gt;\frac{2}{3} \pi\, r^3&lt;/math&gt; || &lt;math&gt;3 \pi\, r^2&lt;/math&gt; || 
&lt;math&gt;\left(
\frac{16}{27}
\right)^{\frac{1}{3}} \approx 0.840&lt;/math&gt;
|-
|ideal [[cylinder (geometry)|cylinder]]&lt;BR&gt;&lt;math&gt;(h=2\,r)&lt;/math&gt;
|[[File:Circular cylinder rh.svg|70px]]
| &lt;math&gt;\pi r^2 h = 2 \pi\,r^3&lt;/math&gt; || &lt;math&gt;2 \pi r ( r + h ) = 6 \pi\,r^2&lt;/math&gt; || 
&lt;math&gt;\left(
\frac{2}{3}
\right)^{\frac{1}{3}} \approx 0.874&lt;/math&gt;
|-
|ideal [[torus]]&lt;BR&gt;&lt;math&gt;(R=r)&lt;/math&gt;
|[[File:Torus.png|70px]]
| &lt;math&gt;2 \pi^2 R r^2 = 2 \pi^2 \,r^3&lt;/math&gt; || &lt;math&gt;4 \pi^2 R r = 4 \pi^2\,r^2&lt;/math&gt; || 
&lt;math&gt;\left(
\frac{9}{4 \pi}
\right)^{\frac{1}{3}} \approx 0.894&lt;/math&gt;
|-
|[[sphere]]
|[[File:Sphere wireframe 10deg 6r.svg|70px]]
| &lt;math&gt;\frac{4}{3} \pi r^3&lt;/math&gt; || &lt;math&gt;4 \pi\,r^2&lt;/math&gt; || 
&lt;math&gt;
1\,&lt;/math&gt;
|-
| colspan=5 align=left|'''Other Shapes'''
|-
|[[disdyakis triacontahedron]]
| [[File:Disdyakistriacontahedron.jpg|50px|Disdyakis triacontahedron]]
| &lt;math&gt;\frac{180}{11}\sqrt{179-24\sqrt{5}}&lt;/math&gt; || &lt;math&gt;\frac{180}{11}\left(5 + 4\sqrt{5}\right)&lt;/math&gt; ||  &lt;math&gt;\frac{\left(\left(5 + 4\sqrt{5}\right)^{2}\frac{11\pi}{5}\right)^{\frac{1}{3}}}{\sqrt{179 - 24\sqrt{5}}} \approx 0.986&lt;/math&gt; 
|-
|}
== See also ==
*[[Equivalent spherical diameter]]
*[[Flattening]]
*[[Index of sphericity]]
*[[Isoperimetric ratio]]
*[[Rounding (sediment)]]
*[[Willmore energy]]

==References==
{{Reflist}}

== External links ==
{{Wiktionary|sphericity}}
*[http://people.uncw.edu/dockal/gly312/grains/grains.htm Grain Morphology: Roundness, Surface Features, and Sphericity of Grains]

[[Category:Geometric measurement]]
[[Category:Spheres]]
[[Category:Metrology]]</text>
      <sha1>7pna7412azbgcaxuh1v6m22vs5aqnfj</sha1>
    </revision>
  </page>
  <page>
    <title>Stoer–Wagner algorithm</title>
    <ns>0</ns>
    <id>48751362</id>
    <revision>
      <id>845823745</id>
      <parentid>825989032</parentid>
      <timestamp>2018-06-14T11:40:23Z</timestamp>
      <contributor>
        <username>Safarnejad</username>
        <id>26996172</id>
      </contributor>
      <comment>I have read the original paper to edit the page. The new sentence is more clear.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15080">[[File:Stoer wagner-example-min-cut.gif|thumb|A min-cut of a weighted graph having min-cut weight 4&lt;ref&gt;{{Cite web|title = Boost Graph Library: Stoer–Wagner Min-Cut - 1.46.1|url = http://www.boost.org/doc/libs/1_46_1/libs/graph/doc/stoer_wagner_min_cut.html|website = www.boost.org|accessdate = 2015-12-07}}&lt;/ref&gt;]]
In [[graph theory]], the '''Stoer–Wagner algorithm''' is a [[recursive algorithm]] to solve the [[minimum cut]] problem in [[Undirected graph|undirected]] weighted graphs with non-negative weights. It was proposed by Mechthild Stoer and Frank Wagner in 1995. The essential idea of this algorithm is to shrink the graph by merging the most intensive vertices, until the graph only contains two combined vertex sets.&lt;ref name=":0" /&gt; At each phase, the algorithm finds the minimum &lt;math&gt;s&lt;/math&gt;-&lt;math&gt;t&lt;/math&gt; cut for two vertices &lt;math&gt;s&lt;/math&gt; and &lt;math&gt;t&lt;/math&gt; chosen as its will. Then the algorithm shrinks the edge between &lt;math&gt;s&lt;/math&gt; and &lt;math&gt;t&lt;/math&gt; to search for non &lt;math&gt;s&lt;/math&gt;-&lt;math&gt;t&lt;/math&gt; cuts. The minimum cut found in all phases will be the minimum weighted cut of the graph.

A '''[[cut (graph theory)|cut]]''' is a partition of the vertices of a graph into two disjoint subsets. A [[minimum cut]] is a cut for which the size or weight of the cut is not larger than the size of any other cut. For an unweighted graph, the minimum cut would simply be the cut with the least edges. For a weighted graph, the sum of all edges' weight on the cut determines whether it is a minimum cut. In practice, the minimum cut problem is always discussed with the [[maximum flow problem]], to explore the maximum capacity of a [[Network topology|network]], since the minimum cut is a bottleneck in a graph or network.

== Stoer–Wagner minimum cut algorithm ==
Let &lt;math&gt;G=(V,E,w)&lt;/math&gt; be a weighted undirected graph. Let &lt;math&gt;(S,T)&lt;/math&gt; be a global min-cut of &lt;math&gt;G&lt;/math&gt;. Suppose that &lt;math&gt;s,t\in V&lt;/math&gt;. If exactly one of &lt;math&gt;s&lt;/math&gt; or &lt;math&gt;t&lt;/math&gt; is in &lt;math&gt;S&lt;/math&gt;, then &lt;math&gt;(S,T)&lt;/math&gt; is also a &lt;math&gt;s&lt;/math&gt;-&lt;math&gt;t&lt;/math&gt; min-cut of &lt;math&gt;G&lt;/math&gt;.&lt;ref name=":1"&gt;{{Cite web|url = http://www.cse.iitd.ernet.in/~ssen/csl356/root.pdf|title = Lecture notes for Analysis of Algorithms": Global minimum cuts|date = |accessdate = |website = |publisher = |last = |first = }}&lt;/ref&gt;

This algorithm starts by finding a s-t min-cut &lt;math&gt;(S,T)&lt;/math&gt; of &lt;math&gt;G&lt;/math&gt;, for the two vertices &lt;math&gt;\left\{s,t\right\}\in V&lt;/math&gt;. For the pair of &lt;math&gt;\left\{s,t\right\}&lt;/math&gt;, it has two different situations: &lt;math&gt;(S,T)&lt;/math&gt; is a global min-cut of &lt;math&gt;G&lt;/math&gt;; or they belong to the same side of the global min-cut of &lt;math&gt;G&lt;/math&gt;. Therefore, the global min-cut can be found by checking the graph &lt;math&gt;G/\left\{s,t\right\}&lt;/math&gt;, which is the graph after the merging of vertices &lt;math&gt;s&lt;/math&gt; and &lt;math&gt;t&lt;/math&gt;. During the merging, if &lt;math&gt;s&lt;/math&gt; and &lt;math&gt;t&lt;/math&gt; are connected by an edge then this edge disappears. If s and t both have edges to some vertex v, then the weight of the edge from the new vertex st to v is &lt;math&gt;w(s,v)+w(t,v)&lt;/math&gt;.&lt;ref name=":1" /&gt; The algorithm is described as:&lt;ref name=":0"&gt;{{Cite web|url = http://www.diss.fu-berlin.de/docs/servlets/MCRFileNodeServlet/FUDOCS_derivate_000000000270/1994_12.pdf|title = A Simple Min-Cut Algorithm|date = |accessdate = |website = |publisher = |last = |first = }}&lt;/ref&gt;
* '''MinimumCutPhase'''&lt;math&gt;(G,w,a)&lt;/math&gt;
    &lt;math&gt;A\gets\left\{a\right\}&lt;/math&gt;
    '''while''' &lt;math&gt;\ A\ne V&lt;/math&gt;
        add to &lt;math&gt;A&lt;/math&gt; the most tightly connected vertex
    store the cut-of-the-phase and shrink &lt;math&gt;G&lt;/math&gt; by merging the two vertices added last

* '''MinimumCut'''&lt;math&gt;(G,w,a)&lt;/math&gt;
    '''while''' &lt;math&gt;|V|&gt;1&lt;/math&gt;
        '''MinimumCutPhase'''&lt;math&gt;(G,w,a)&lt;/math&gt;
        '''if''' the cut-of-the-phase is lighter than the current minimum cut
            '''then''' store the cut-of-the-phase as the current minimum cut
The algorithm works in phases. In the MinimumCutPhase, the subset &lt;math&gt;A&lt;/math&gt; of the graphs vertices grows starting with an arbitrary single vertex until &lt;math&gt;A&lt;/math&gt; is equal to &lt;math&gt;V&lt;/math&gt;. In each step, the vertex which is outside of &lt;math&gt;A&lt;/math&gt;, but most tightly connected with &lt;math&gt;A&lt;/math&gt; is added to the set &lt;math&gt;A&lt;/math&gt;. This procedure can be formally shown as:&lt;ref name=":0" /&gt; add vertex &lt;math&gt;z\notin A&lt;/math&gt; such that &lt;math&gt;w(A,z)=\max\{w(A,y\mid y\notin A)\}&lt;/math&gt;, where &lt;math&gt;w(A,y)&lt;/math&gt; is the sum of the weights of all the edges between &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt;. So, in a single phase, a pair of vertices &lt;math&gt;s&lt;/math&gt; and &lt;math&gt;t&lt;/math&gt; , and a min &lt;math&gt;s\text{-}t&lt;/math&gt; cut &lt;math&gt;C&lt;/math&gt; is determined.&lt;ref name=":2"&gt;{{Cite web|url = http://e-maxx.ru/bookz/files/mehlhorn_mincut_stoer_wagner.pdf|title = The minimum cut algorithm of Stoer and Wagner|date = |accessdate = |website = |publisher = |last = |first = }}&lt;/ref&gt; After one phase of the MinimumCutPhase, the two vertices are merged as a new vertex, and edges from the two vertices to a remaining vertex are replaced by an edge weighted by the sum of the weights of the previous two edges. Edges joining the merged nodes are removed. If there is a minimum cut of &lt;math&gt;G&lt;/math&gt; separating &lt;math&gt;s&lt;/math&gt; and &lt;math&gt;t&lt;/math&gt;, the &lt;math&gt;C&lt;/math&gt; is a minimum cut of &lt;math&gt;G&lt;/math&gt;. If not, then the minimum cut of &lt;math&gt;G&lt;/math&gt; must have &lt;math&gt;s&lt;/math&gt; and &lt;math&gt;t&lt;/math&gt; on a same side. Therefore, the algorithm would merge them as one node. In addition, the MinimumCut would record and update the global minimum cut after each MinimumCutPhase. After &lt;math&gt;n-1&lt;/math&gt; phases, the [[minimum cut]] can be determined.&lt;ref name=":2" /&gt;

=== Example ===
The graph in step 1 shows the original graph &lt;math&gt;G&lt;/math&gt; and randomly selects node 2 as the starting node for this algorithm. In the MinimumCutPhase, set &lt;math&gt;A&lt;/math&gt; only has node 2, the heaviest edge is edge (2,3), so node 3 is added into set &lt;math&gt;A&lt;/math&gt;. Next, set &lt;math&gt;A&lt;/math&gt; contains node 2 and node 3, the heaviest edge is (3,4), thus node 4 is added to set &lt;math&gt;A&lt;/math&gt;. By following this procedure, the last two nodes are node 5 and node 1, which are &lt;math&gt;s&lt;/math&gt; and &lt;math&gt;t&lt;/math&gt; in this phase. By merging them, the new graph is as shown in step 2. In this phase, the weight of cut is 5, which is the summation of edges (1,2) and (1,5). Right now, the first loop of MinimumCut is completed.

In step 2, starting from node 2, the heaviest edge is (2,15), thus node 15 is put in set &lt;math&gt;A&lt;/math&gt;. The next heaviest edges is (2,3) or (15,6), we choose (15,6) thus node 6 is added to the set. Then we compare edge (2,3) and (6,7) and choose node 3 to put in set &lt;math&gt;A&lt;/math&gt;. The last two nodes are node 7 and node 8. Therefore, merge edge (7,8). The minimum cut is 5, so remain the minimum as 5.

The following steps repeat the same operations on the merged graph, until there is only one edge in the graph, as shown in step 7. The global minimum cut has edge (2,3) and edge (6,7), which is detected in step 5.

== Proof of correctness ==
To prove the correctness of this algorithm, we need to prove that MinimumCutPhase is in fact a minimum &lt;math&gt;s\text{-}t&lt;/math&gt; cut of the graph, where s and t are the two vertices last added in the phase. Therefore, a lemma is shown below:&lt;blockquote&gt;'''Lemma 1''': MinimumCutPhase returns a minimum &lt;math&gt;s\text{-}t&lt;/math&gt;-cut of &lt;math&gt;G&lt;/math&gt;.&lt;/blockquote&gt;We prove this by induction on the set of active vertices. Let &lt;math&gt;C=(X,\overline{X})&lt;/math&gt; be an arbitrary &lt;math&gt;s\text{-}t&lt;/math&gt; cut, and &lt;math&gt;CP&lt;/math&gt; be the cut of the phase. We must show that &lt;math&gt;W(C)\ge W(CP)&lt;/math&gt;. Observe that a single run of MinimumCutPhase gives us a permutation of all the vertices in the graph (where &lt;math&gt;a&lt;/math&gt; is the first and &lt;math&gt;s&lt;/math&gt; and &lt;math&gt;t&lt;/math&gt; are the two vertices added last in the phase). So, we say that the vertex &lt;math&gt;v&lt;/math&gt; is active if &lt;math&gt;v\in X&lt;/math&gt;, the vertex before &lt;math&gt;v&lt;/math&gt; in the ordering of vertices produced by MinimumCutPhase is in &lt;math&gt;\overline{X}&lt;/math&gt; or vice versa, which is to say, they are on opposite sides of the cut. We define &lt;math&gt;A_v&lt;/math&gt; as the set of vertices added to &lt;math&gt;A&lt;/math&gt; before &lt;math&gt;v&lt;/math&gt; and &lt;math&gt;C_v&lt;/math&gt; to be the cut of the set &lt;math&gt;A_v \cup \{v\}&lt;/math&gt;induced by &lt;math&gt;C&lt;/math&gt;. For all the active vertex &lt;math&gt;v&lt;/math&gt;:&lt;blockquote&gt;&lt;math&gt;w(A_v,v)\le w(C_v)&lt;/math&gt;&lt;/blockquote&gt;Let &lt;math&gt;v_0&lt;/math&gt; be the first active vertex. By the definition of these two quantities, the &lt;math&gt;w(A_{v_0},v_0)&lt;/math&gt; and &lt;math&gt;w(C_{v_0})&lt;/math&gt; are equivalent. &lt;math&gt;A_{v_0}&lt;/math&gt; is simply all vertices added to &lt;math&gt;A&lt;/math&gt; before &lt;math&gt;v_0&lt;/math&gt;, and the edges between these vertices and &lt;math&gt;v_0&lt;/math&gt; are the edges that cross the cut &lt;math&gt;C&lt;/math&gt;. Therefore, as shown above, for the active vertex &lt;math&gt;u&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; (&lt;math&gt;v&lt;/math&gt; is added to &lt;math&gt;A&lt;/math&gt; before &lt;math&gt;u&lt;/math&gt;):&lt;blockquote&gt;&lt;math&gt;w(A_u,u)=w(A_v,u)+w(A_u-A_v,u)&lt;/math&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;math&gt;w(A_u,u)\le w(C_v)+w(A_u-A_v,u)&lt;/math&gt; by induction, &lt;math&gt;w(A_v,u)\le w(A_v,v)\le w(C_v)&lt;/math&gt;&lt;/blockquote&gt;&lt;blockquote&gt;&lt;math&gt;w(A_{u},u)\le w(C_{u})&lt;/math&gt; since &lt;math&gt;w(A_u-A_v,u)&lt;/math&gt; contributes to &lt;math&gt;w(C_{u})&lt;/math&gt; but not to &lt;math&gt;w(C_{v})&lt;/math&gt; (and other edges are of non-negative weights)&lt;/blockquote&gt;Thus, since &lt;math&gt;t&lt;/math&gt; is always an active vertex since the last cut of the phase separates &lt;math&gt;s&lt;/math&gt; from &lt;math&gt;t&lt;/math&gt; by definition, for any active vertex &lt;math&gt;t&lt;/math&gt;:&lt;blockquote&gt;&lt;math&gt;w(A_t,t)\le w(C_t)=w(C)&lt;/math&gt;&lt;/blockquote&gt;Therefore, the cut of the phase is at most as heavy as &lt;math&gt;C&lt;/math&gt;.

== Time complexity ==
The [[running time]] of the algorithm '''MinimumCut''' is equal to the added running time of the &lt;math&gt;|V|-1&lt;/math&gt; runs of '''MinimumCutPhase''', which is called on graphs with decreasing number of vertices and edges.

For the '''MinimumCutPhase''', a single run of it needs at most &lt;math&gt;O(|E|+|V|\log|V|)&lt;/math&gt; time.

Therefore, the overall running time should be the product of two phase complexity, which is &lt;math&gt;O(|V||E|+|V|^2\log|V|)&lt;/math&gt;[2].&lt;ref name=":0" /&gt;

For the further improvement, the key is to make it easy to select the next vertex to be added to the set &lt;math&gt;A&lt;/math&gt;, the most tightly connected vertex. During execution of a phase, all vertices that are not in &lt;math&gt;A&lt;/math&gt; reside in a priority queue based on a key field. The key of a vertex &lt;math&gt;V&lt;/math&gt; is the sum of the weights of the edges connecting it to the current &lt;math&gt;A&lt;/math&gt;, that is, &lt;math&gt;w(A,v)&lt;/math&gt;. Whenever a vertex &lt;math&gt;v&lt;/math&gt; is added to &lt;math&gt;A&lt;/math&gt; we have to perform an update of the queue. &lt;math&gt;v&lt;/math&gt; has to be deleted from the queue, and the key of every vertex &lt;math&gt;w&lt;/math&gt; not in &lt;math&gt;A&lt;/math&gt;, connected to &lt;math&gt;v&lt;/math&gt; has to be increased by the weight of the edge &lt;math&gt;vw&lt;/math&gt;, if it exists. As this is done exactly once for every edge, overall we have to perform &lt;math&gt;|V|&lt;/math&gt; ExtractMax and &lt;math&gt;|E|&lt;/math&gt; IncreaseKey operations. By using the [[Fibonacci heap]] we can perform an ExtractMax operation in &lt;math&gt;O(\log|V|)&lt;/math&gt; amortized time and an IncreaseKey operation in &lt;math&gt;O(1)&lt;/math&gt; amortized time. Thus, the time we need for this key step that dominates the rest of the phase, is &lt;math&gt;O(|E|+|V|\log|V|)&lt;/math&gt;.&lt;ref name=":0" /&gt;

== Example code&lt;ref&gt;{{Cite web|title = Stanford University ACM Team Notebook (2014–15)|url = https://web.stanford.edu/~liszt90/acm/notebook.html|website = web.stanford.edu|accessdate = 2015-12-07}}&lt;/ref&gt; ==
&lt;syntaxhighlight lang="c++"&gt;
// Adjacency matrix implementation of Stoer–Wagner min cut algorithm.
//
// Running time:
//     O(|V|^3)
//
// INPUT: 
//     - graph, constructed using AddEdge()
//
// OUTPUT:
//     - (min cut value, nodes in half of min cut)

#include &lt;cmath&gt;
#include &lt;vector&gt;
#include &lt;iostream&gt;

using namespace std;

typedef vector&lt;int&gt; VI;
typedef vector&lt;VI&gt; VVI;

const int INF = 1000000000;

pair&lt;int, VI&gt; GetMinCut(VVI &amp;weights) 
{
    int N = weights.size();
    VI used(N), cut, best_cut;
    int best_weight = -1;
  
    for (int phase = N-1; phase &gt;= 0; phase--) 
    {
        VI w = weights[0];
        VI added = used;
        int prev, last = 0;
        for (int i = 0; i &lt; phase; i++) 
        {
            prev = last;
            last = -1;
            for (int j = 1; j &lt; N; j++)
            {
                if (!added[j] &amp;&amp; (last == -1 || w[j] &gt; w[last])) last = j;
            }
            if (i == phase-1) 
            {
                for (int j = 0; j &lt; N; j++) weights[prev][j] += weights[last][j];
                for (int j = 0; j &lt; N; j++) weights[j][prev] = weights[prev][j];
                used[last] = true;
                cut.push_back(last);  // this part gives wrong answer. 
                                      // EX) n=4, 1st step: prev=1, last=2 / 2nd step: prev=3, last=4
                                      // if 2nd step gives mincut, the cut is {1,2,3},{4} but this code gives wrong answer - {1,3},{2,4}
                if (best_weight == -1 || w[last] &lt; best_weight) 
                {
                    best_cut = cut;
                    best_weight = w[last];
                }
            }
            else 
            {
                for (int j = 0; j &lt; N; j++)
                {
                    w[j] += weights[last][j];
                    added[last] = true;
                }
            }
        }
    }
    return make_pair(best_weight, best_cut);
}
&lt;/syntaxhighlight&gt;

{{citation needed|reason=I don't think the code below works.  It doesn't even appear to be valid C|date=April 2016}}
&lt;syntaxhighlight lang="c"&gt;
const int maxn = 550;  
const int inf = 1000000000;  
int n, r;  
int edge[maxn][maxn], dist[maxn];  
bool vis[maxn], bin[maxn];  
void init()  
{  
    memset(edge, 0, sizeof(edge));  
    memset(bin, false, sizeof(bin));  
}  
int contract( int &amp;s, int &amp;t )          // Find s,t  
{  
    memset(dist, 0, sizeof(dist));  
    memset(vis, false, sizeof(vis));  
    int i, j, k, mincut, maxc;  
    for(i = 1; i &lt;= n; i++)  
    {  
        k = -1; maxc = -1;  
        for(j = 1; j &lt;= n; j++)if(!bin[j] &amp;&amp; !vis[j] &amp;&amp; dist[j] &gt; maxc)  
        {  
            k = j;  maxc = dist[j];  
        }  
        if(k == -1)return mincut;  
        s = t;  t = k;  
        mincut = maxc;  
        vis[k] = true;  
        for(j = 1; j &lt;= n; j++)if(!bin[j] &amp;&amp; !vis[j])  
            dist[j] += edge[k][j];  
    }  
    return mincut;  
}
  
int Stoer_Wagner()  
{  
    int mincut, i, j, s, t, ans;  
    for(mincut = inf, i = 1; i &lt; n; i++)  
    {  
        ans = contract( s, t );  
        bin[t] = true;  
        if(mincut &gt; ans)mincut = ans;  
        if(mincut == 0)return 0;  
        for(j = 1; j &lt;= n; j++)if(!bin[j])  
            edge[s][j] = (edge[j][s] += edge[j][t]);  
    }  
    return mincut;  
}
&lt;/syntaxhighlight&gt;

== References ==
&lt;references /&gt;

{{DEFAULTSORT:Stoer-Wagner algorithm}}
[[Category:Graph algorithms]]
[[Category:Graph connectivity]]</text>
      <sha1>cl6d2bgmsk1v3a3q6zi31in26bmpvpx</sha1>
    </revision>
  </page>
  <page>
    <title>Stolarsky mean</title>
    <ns>0</ns>
    <id>7423338</id>
    <revision>
      <id>805478605</id>
      <parentid>791138244</parentid>
      <timestamp>2017-10-15T17:40:23Z</timestamp>
      <contributor>
        <ip>76.65.87.72</ip>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2357">In [[mathematics]], the '''Stolarsky mean''' of two positive [[real number]]s ''x'',&amp;nbsp;''y'' is defined as: 

:&lt;math&gt;
\begin{align}
S_p(x,y)
&amp; = \lim_{(\xi,\eta)\to(x,y)}
\left({\frac{\xi^p-\eta^p}{p (\xi-\eta)}}\right)^{1/(p-1)} \\[10pt]
&amp; = \begin{cases}
x &amp; \text{if }x=y \\
\left({\frac{x^p-y^p}{p (x-y)}}\right)^{1/(p-1)} &amp; \text{else}
\end{cases}
\end{align}
&lt;/math&gt;

It is derived from the [[mean value theorem]], which states that a [[secant line]], cutting the graph of a [[Differentiable function|differentiable]] function &lt;math&gt;f&lt;/math&gt; at &lt;math&gt;( x, f(x) )&lt;/math&gt; and &lt;math&gt;( y, f(y) )&lt;/math&gt;, has the same [[slope]] as a line [[tangent]] to the graph at some point &lt;math&gt;\xi&lt;/math&gt; in the [[Interval (mathematics)|interval]] &lt;math&gt;[x,y]&lt;/math&gt;.
:&lt;math&gt; \exists \xi\in[x,y]\ f'(\xi) = \frac{f(x)-f(y)}{x-y} &lt;/math&gt;

The Stolarsky mean is obtained by
:&lt;math&gt; \xi = f'^{-1}\left(\frac{f(x)-f(y)}{x-y}\right) &lt;/math&gt;
when choosing &lt;math&gt;f(x) = x^p&lt;/math&gt;.

== Special cases ==

*&lt;math&gt;\lim_{p\to -\infty} S_p(x,y)&lt;/math&gt; is the [[minimum]].
*&lt;math&gt;S_{-1}(x,y)&lt;/math&gt; is the [[geometric mean]].
*&lt;math&gt;\lim_{p\to 0} S_p(x,y)&lt;/math&gt; is the [[logarithmic mean]]. It can be obtained from the mean value theorem by choosing &lt;math&gt;f(x) = \ln x&lt;/math&gt;.
*&lt;math&gt;S_{\frac{1}{2}}(x,y)&lt;/math&gt; is the [[power mean]] with exponent &lt;math&gt;\frac{1}{2}&lt;/math&gt;.
*&lt;math&gt;\lim_{p\to 1} S_p(x,y)&lt;/math&gt; is the [[identric mean]]. It can be obtained from the mean value theorem by choosing &lt;math&gt;f(x) = x\cdot \ln x&lt;/math&gt;.
*&lt;math&gt;S_2(x,y)&lt;/math&gt; is the [[arithmetic mean]].
*&lt;math&gt;S_3(x,y) = QM(x,y,GM(x,y))&lt;/math&gt; is a connection to the [[quadratic mean]] and the [[geometric mean]].
*&lt;math&gt;\lim_{p\to\infty} S_p(x,y)&lt;/math&gt; is the [[maximum]].

== Generalizations ==

One can generalize the mean to ''n''&amp;nbsp;+&amp;nbsp;1 variables by considering the [[mean value theorem for divided differences]] for the ''n''th [[derivative]].
One obtains
:&lt;math&gt;S_p(x_0,\dots,x_n) = {f^{(n)}}^{-1}(n!\cdot f[x_0,\dots,x_n])&lt;/math&gt; for &lt;math&gt;f(x)=x^p&lt;/math&gt;.

== See also ==
*[[Mean]]

== References ==
{{reflist}}
* {{cite journal | zbl=0302.26003 | last=Stolarsky | first=Kenneth B. | title=Generalizations of the logarithmic mean | journal=[[Mathematics Magazine]] | volume=48 | pages=87–92 | year=1975 | issn=0025-570X | jstor=2689825 | doi=10.2307/2689825}}

[[Category:Means]]</text>
      <sha1>elv7xet64i3nlpc0nq1xwzk8h0337xr</sha1>
    </revision>
  </page>
  <page>
    <title>Studies in Applied Mathematics</title>
    <ns>0</ns>
    <id>23402810</id>
    <revision>
      <id>854449262</id>
      <parentid>832600382</parentid>
      <timestamp>2018-08-11T12:46:31Z</timestamp>
      <contributor>
        <ip>193.84.204.98</ip>
      </contributor>
      <comment>link fixed</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1774">{{Infobox Journal
|title	      = Studies in Applied Mathematics
|cover	      = Studies in Applied Mathematics.jpg
|abbreviation =	Stud. Appl. Math.
|discipline   =	[[Applied Mathematics]]
|language     =	English
|publisher    =	[[Wiley&amp;ndash;Blackwell]] 
|country      = U.S.
|history      =	1969 to present
|frequency    =	8 issues per year
|impact        = 1.525
|impact-year   = 2009
|website      =	https://onlinelibrary.wiley.com/journal/14679590
|ISSN	      = 0022-2526
|eISSN     = 1467-9590
}}

The journal '''''Studies in Applied Mathematics''''' is published by [[Wiley&amp;ndash;Blackwell]] on behalf of the [[Massachusetts Institute of Technology]].

It features scholarly articles on mathematical applications in allied fields, notably computer science, mechanics, astrophysics, geophysics, and high-energy physics. 
Its pedigree came from the ''[[MIT]] Journal of Mathematics and Physics'' which was founded by the [[MIT Mathematics Department]] in 1920. The Journal changed to its present name in 1969.

The journal was edited from 1969 by [[David Benney]] of the Department of Mathematics, Massachusetts Institute of Technology.&lt;ref&gt;[http://math.mit.edu/people/profile.php?pid=19 ''David J. Benney'', math.mit.edu]&lt;/ref&gt;

According to ISI [[Journal Citation Reports]], in 2009 it ranked 34th among the 202 journals in the Applied Mathematics category.

==Notes==
{{reflist}}

==External links==
*[http://www.wiley.com/bw/journal.asp?ref=0022-2526 Journal Home Page]
*[http://www-math.mit.edu/people/profiles/benney-david.html MIT Faculty Page of Dr. David Benney]

{{DEFAULTSORT:Studies In Applied Mathematics}}
[[Category:Mathematics journals]]
[[Category:Wiley-Blackwell academic journals]]
[[Category:English-language journals]]


{{mathematics-journal-stub}}</text>
      <sha1>aye9nvdc50t2m53w4a335sd61m6imni</sha1>
    </revision>
  </page>
  <page>
    <title>Subhamiltonian graph</title>
    <ns>0</ns>
    <id>43063523</id>
    <revision>
      <id>846779288</id>
      <parentid>683711259</parentid>
      <timestamp>2018-06-20T20:50:11Z</timestamp>
      <contributor>
        <username>Bibcode Bot</username>
        <id>14394459</id>
      </contributor>
      <minor/>
      <comment>Adding 0 [[arXiv|arxiv eprint(s)]], 1 [[bibcode|bibcode(s)]] and 0 [[digital object identifier|doi(s)]]. Did it miss something? Report bugs, errors, and suggestions at [[User talk:Bibcode Bot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5590">In [[graph theory]] and [[graph drawing]], a '''subhamiltonian graph''' is a [[Glossary of graph theory#Subgraphs|subgraph]] of a [[planar graph|planar]] [[Hamiltonian graph]].&lt;ref&gt;{{citation
 | last = Heath | first = Lenwood S.
 | doi = 10.1137/0608018
 | issue = 2
 | journal = SIAM Journal on Algebraic and Discrete Methods
 | mr = 881181
 | pages = 198–218
 | title = Embedding outerplanar graphs in small books
 | volume = 8
 | year = 1987}}.&lt;/ref&gt;&lt;ref name="hamaug"&gt;{{citation
 | last1 = Di Giacomo | first1 = Emilio
 | last2 = Liotta | first2 = Giuseppe
 | contribution = The Hamiltonian augmentation problem and its applications to graph drawing
 | doi = 10.1007/978-3-642-11440-3_4
 | location = Berlin
 | mr = 2749626
 | pages = 35–46
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = WALCOM: Algorithms and Computation, 4th International Workshop, WALCOM 2010, Dhaka, Bangladesh, February 10-12, 2010, Proceedings
 | volume = 5942
 | year = 2010}}.&lt;/ref&gt;

==Definition==
A graph ''G'' is subhamiltonian if ''G'' is a subgraph of another graph aug(''G'') on the same vertex set, such that aug(''G'') is planar and contains a [[Hamiltonian cycle]]. For this to be true, ''G'' itself must be planar, and additionally it must be possible to add edges to ''G'', preserving planarity, 
in order to create a cycle in the augmented graph that passes through each vertex exactly once. The graph aug(''G'') is called a '''Hamiltonian augmentation''' of ''G''.&lt;ref name="hamaug"/&gt;

It would be equivalent to define ''G'' to be subhamiltonian if ''G'' is a subgraph of a Hamiltonian planar graph, without requiring this larger graph to have the same vertex set. That is, for this alternative definition, it should be possible to add both vertices and edges to ''G'' to create a Hamiltonian cycle. However, if a graph can be made Hamiltonian by the addition of vertices and edges it can also be made Hamiltonian by the addition of edges alone, so this extra freedom does not change the definition.&lt;ref&gt;For instance in a 2003 technical report "[http://faculty.georgetown.edu/kainen/pbip3.pdf Book embeddings of graphs and a theorem of Whitney]", [[Paul Chester Kainen|Paul Kainen]] defines subhamiltonian graphs to be subgraphs of planar Hamiltonian graphs, without restriction on the vertex set of the augmentation, but writes that "in the definition of subhamiltonian graph, one can require that the extension only involve the inclusion of new edges."&lt;/ref&gt;
 
In a subhamiltonian graph, a '''subhamiltonian cycle''' is a cyclic sequence of vertices such that adding an edge between each consecutive pair of vertices in the sequence preserves the planarity of the graph. A graph is subhamiltonian if and only if it has a subhamiltonian cycle.&lt;ref name="bgr14"/&gt;

==History and applications==
The class of subhamiltonian graphs (but not this name for them) was introduced by {{harvtxt|Bernhart|Kainen|1979}}, who proved that these are exactly the graphs with two-page [[book embedding]]s.&lt;ref&gt;{{citation|first1=Frank R.|last1=Bernhart|first2=Paul C.|last2=Kainen|author2-link=Paul Chester Kainen|title=The book thickness of a graph|journal=[[Journal of Combinatorial Theory]]|series=Series B|volume=27|issue=3|pages=320–331|year=1979|doi=10.1016/0095-8956(79)90021-2}}.&lt;/ref&gt; Subhamiltonian graphs and Hamiltonian augmentations have also been applied in [[graph drawing]] to problems involving embedding graphs onto [[universal point set]]s, [[simultaneous embedding]] of multiple graphs, and [[layered graph drawing]].&lt;ref name="hamaug"/&gt;

==Related graph classes==
Some classes of planar graphs are necessarily Hamiltonian, and therefore also subhamiltonian; these include the [[k-vertex-connected graph|4-connected planar graphs]]&lt;ref name="nc"&gt;{{citation
 | last1 = Nishizeki | first1 = Takao | author1-link = Takao Nishizeki
 | last2 = Chiba | first2 = Norishige
 | isbn = 9780486466712
 | contribution = Chapter 10. Hamiltonian Cycles
 | pages = 171–184
 | publisher = Courier Dover Publications
 | series = Dover Books on Mathematics
 | title = Planar Graphs: Theory and Algorithms
 | year = 2008}}.&lt;/ref&gt; and the [[Halin graph]]s.&lt;ref&gt;{{citation|title=Halin graphs and the travelling salesman problem|first1=G.|last1=Cornuéjols|author1-link= Gérard Cornuéjols |first2=D.|last2=Naddef|first3=W. R.|last3=Pulleyblank|author3-link=William R. Pulleyblank|journal=Mathematical Programming|volume=26|issue=3|year=1983|pages=287–294|doi=10.1007/BF02591867}}.&lt;/ref&gt;

Every planar graph with [[degree (graph theory)|maximum degree]] at most four is subhamiltonian,&lt;ref name="bgr14"&gt;{{citation
 | last1 = Bekos | first1 = Michael A.
 | last2 = Gronemann | first2 = Martin
 | last3 = Raftopoulou | first3 = Chrysanthi N.
 | arxiv = 1401.0684
 | contribution = Two-page book embeddings of 4-planar graphs
 | title = STACS
 | year = 2014| bibcode = 2014arXiv1401.0684B}}.&lt;/ref&gt; as is every planar graph with no separating triangles.&lt;ref&gt;{{citation
 | last1 = Kainen | first1 = Paul C.
 | last2 = Overbay | first2 = Shannon
 | doi = 10.1016/j.aml.2006.08.019
 | issue = 7
 | journal = Applied Mathematics Letters
 | mr = 2314718
 | pages = 835–837
 | title = Extension of a theorem of Whitney
 | volume = 20
 | year = 2007}}.&lt;/ref&gt;
If the edges of an arbitrary planar graph are [[subdivision (graph theory)|subdivided]] into paths of length two, the resulting subdivided graph is always subhamiltonian.&lt;ref name="hamaug"/&gt;

==References==
{{reflist|30em}}

[[Category:Graph families]]
[[Category:Planar graphs]]
[[Category:Hamiltonian paths and cycles]]</text>
      <sha1>6hjtaf722x02ixnq42dzztd5ju0aotq</sha1>
    </revision>
  </page>
  <page>
    <title>Summability kernel</title>
    <ns>0</ns>
    <id>42625595</id>
    <revision>
      <id>720692681</id>
      <parentid>646559395</parentid>
      <timestamp>2016-05-17T11:10:08Z</timestamp>
      <contributor>
        <ip>85.121.32.1</ip>
      </contributor>
      <comment>/* Definition */ logical conseq</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3191">In mathematics, a '''summability kernel''' is a family or sequence of periodic integrable functions satisfying a certain set of properties, listed below. Certain kernels, such as the [[Fejér kernel]], are particularly useful in [[Fourier analysis]]. Summability kernels are related to [[approximation of the identity]]; definitions of an approximation of identity vary,&lt;ref&gt;http://math.stackexchange.com/questions/114028/approximation-of-the-identity-and-hardy-littlewood-maximal-function&lt;/ref&gt; but sometimes the definition of an approximation of the identity is taken to be the same as for a summability kernel.

==Definition==
Let &lt;math&gt;\mathbb{T}:=\mathbb{R}/\mathbb{Z}&lt;/math&gt;. A '''summability kernel''' is a sequence &lt;math&gt;(k_n)&lt;/math&gt; in &lt;math&gt;L^1(\mathbb{T})&lt;/math&gt; that satisfies
# &lt;math&gt;\int_\mathbb{T}k_n(t)\,dt=1&lt;/math&gt; 
# &lt;math&gt;\int_\mathbb{T}|k_n(t)|\,dt\le M&lt;/math&gt; (uniformly bounded) 
# &lt;math&gt;\int_{\delta\le|t|\le\frac{1}{2}}|k_n(t)|\,dt\to0&lt;/math&gt; as &lt;math&gt;n\to\infty&lt;/math&gt;, for every &lt;math&gt;\delta&gt;0&lt;/math&gt;.

Note that if &lt;math&gt;k_n\ge0&lt;/math&gt; for all &lt;math&gt;n&lt;/math&gt;, i.e. &lt;math&gt;(k_n)&lt;/math&gt; is a '''positive summability kernel''', then the second requirement [[logical consequence|follows automatically]] from the first.

If instead we take the convention &lt;math&gt;\mathbb{T}=\mathbb{R}/2\pi\mathbb{Z}&lt;/math&gt;, the first equation becomes &lt;math&gt;\frac{1}{2\pi}\int_\mathbb{T}k_n(t)\,dt=1&lt;/math&gt;, and the upper limit of integration on the third equation should be extended to &lt;math&gt;\pi&lt;/math&gt;.

We can also consider &lt;math&gt;\mathbb{R}&lt;/math&gt; rather than &lt;math&gt;\mathbb{T}&lt;/math&gt;; then we integrate (1) and (2) over &lt;math&gt;\mathbb{R}&lt;/math&gt;, and (3) over &lt;math&gt;|t|&gt;\delta&lt;/math&gt;.

==Examples==
* The [[Fejér kernel]]
* The [[Poisson kernel]] (continuous index)
* The [[Dirichlet kernel]] is ''not'' a summability kernel, since it fails the second requirement.

==Convolutions==
Let &lt;math&gt;(k_n)&lt;/math&gt; be a summability kernel, and &lt;math&gt;*&lt;/math&gt; denote the [[convolution]] operation.
* If &lt;math&gt;(k_n),f\in\mathcal{C}(\mathbb{T})&lt;/math&gt; (continuous functions on &lt;math&gt;\mathbb{T}&lt;/math&gt;), then &lt;math&gt;k_n*f\to f&lt;/math&gt; in &lt;math&gt;\mathcal{C}(\mathbb{T})&lt;/math&gt;, i.e. uniformly, as &lt;math&gt;n\to\infty&lt;/math&gt;.
* If &lt;math&gt;(k_n),f\in L^1(\mathbb{T})&lt;/math&gt;, then &lt;math&gt;k_n*f\to f&lt;/math&gt; in &lt;math&gt;L^1(\mathbb{T})&lt;/math&gt;, as &lt;math&gt;n\to\infty&lt;/math&gt;.
* If &lt;math&gt;(k_n)&lt;/math&gt; is radially decreasing symmetric and &lt;math&gt;f\in L^1(\mathbb{T})&lt;/math&gt;, then &lt;math&gt;k_n*f\to f&lt;/math&gt; [[Pointwise convergence#Almost everywhere convergence|pointwise a.e.]], as &lt;math&gt;n\to\infty&lt;/math&gt;. This uses the [[Hardy–Littlewood maximal function]]. If &lt;math&gt;(k_n)&lt;/math&gt; is not radially decreasing symmetric, but the decreasing symmetrization &lt;math&gt;\widetilde{k}_n(x):=\sup_{|y|\ge|x|}k_n(y)&lt;/math&gt; satisfies &lt;math&gt;\sup_{n\in\mathbb{N}}\|\widetilde{k_n}\|_1&lt;\infty&lt;/math&gt;, then a.e. convergence still holds, using a similar argument.

==References==
{{reflist}}
*{{citation
|first=Yitzhak
|last=Katznelson
|authorlink=Yitzhak Katznelson
|title=An introduction to Harmonic Analysis
|year=2004
|publisher=Cambridge University Press
|isbn=0-521-54359-2}}

[[Category:Mathematical analysis]]
[[Category:Fourier series]]</text>
      <sha1>hs2yrvhnukfqgjvnnxecu8ppeoz4267</sha1>
    </revision>
  </page>
  <page>
    <title>Surrogate model</title>
    <ns>0</ns>
    <id>6500531</id>
    <revision>
      <id>866000601</id>
      <parentid>855390704</parentid>
      <timestamp>2018-10-27T16:22:13Z</timestamp>
      <contributor>
        <ip>145.94.151.206</ip>
      </contributor>
      <comment>/* Types of surrogate models */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9715">A '''surrogate model''' is an engineering method used when an outcome of interest cannot be easily directly measured, so a model of the outcome is used instead. Most engineering design problems require experiments and/or simulations to evaluate design objective and constraint functions as function of design variables.  For example, in order to find the optimal airfoil shape for an aircraft wing, an engineer simulates the air flow around the wing for different shape variables (length, curvature, material, ..).  For many real world problems, however, a single simulation can take many minutes, hours, or even days to complete.  As a result, routine tasks such as design optimization, design space exploration, sensitivity analysis and ''what-if'' analysis become impossible since they require thousands or even millions of simulation evaluations.

One way of alleviating this burden is by constructing approximation models, known as '''surrogate models''', [[Response surface methodology|response surface models]], ''metamodels'' or ''emulators'', that mimic the behavior of the simulation model as closely as possible while being computationally cheap(er) to evaluate. Surrogate models are constructed using a data-driven, bottom-up approach. The exact, inner working of the simulation code is not assumed to be known (or even understood), solely the input-output behavior is important. A model is constructed based on modeling the response of the simulator to a limited number of intelligently chosen data points. This approach is also known as behavioral modeling or black-box modeling, though the terminology is not always consistent.  When only a single design variable is involved, the process is known as [[curve fitting]].

Though using surrogate models in lieu of experiments and simulations in engineering design is more common, surrogate modelling may be used in many other areas of science where there are expensive experiments and/or function evaluations.

==Goals==

The scientific challenge of surrogate modeling is the generation of a surrogate that is as accurate as possible, using as few simulation evaluations as possible. The process comprises three major steps which may be interleaved iteratively:

* Sample selection (also known as sequential design, optimal experimental design (OED) or active learning)
* Construction of the surrogate model and optimizing the model parameters (bias–variance trade-off)
* Appraisal of the accuracy of the surrogate.

The accuracy of the surrogate depends on the number and location of samples (expensive experiments or simulations) in the design space. Various [[design of experiments]] (DOE) techniques cater to different sources of errors, in particular, errors due to noise in the data or errors due to an improper surrogate model.

==Types of surrogate models==

The most popular surrogate models are polynomial [[response surface]]s, [[kriging]], [[Gradient-Enhanced Kriging (GEK)]], [[radial basis function]], [[support vector machine]]s, [[space mapping]],&lt;ref name="space mapping"&gt;[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1262727 J.W. Bandler, Q. Cheng, S.A. Dakroury, A.S. Mohamed, M.H. Bakr, K. Madsen and J. Søndergaard, "Space mapping: the state of the art," IEEE Trans. Microwave Theory Tech., vol. 52, no. 1, pp. 337-361, Jan. 2004.]&lt;/ref&gt; and [[artificial neural networks]]. For some problems, the nature of true function is not known a priori so it is not clear which surrogate model will be most accurate. In addition, there is no consensus on how to obtain the most reliable estimates of the accuracy of a given surrogate.
Many other problems have known physics properties. In these cases, physics-based surrogates such as [[space-mapping]] based models are the most efficient.&lt;ref name="space mapping" /&gt;

A recent survey of surrogate-assisted evolutionary optimization techniques can be found in.&lt;ref&gt;Jin Y (2011). Surrogate-assisted evolutionary computation: Recent advances and future challenges. Swarm and Evolutionary Computation, 1(2):61–70.&lt;/ref&gt;

Spanning two decades of development and engineering applications, Rayas-Sanchez reviews aggressive [[space mapping]] exploiting surrogate models.&lt;ref&gt;J.E. Rayas-Sanchez,[http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=7423860&amp;action=search&amp;sortType=&amp;rowsPerPage=&amp;searchField=Search_All&amp;matchBoolean=true&amp;queryText=(%22Document%20Title%22:simplicity%20in%20asm) "Power in simplicity with ASM: tracing the aggressive space mapping algorithm over two decades of development and engineering applications"], IEEE Microwave Magazine, vol. 17, no. 4, pp. 64-76, April 2016.&lt;/ref&gt; Recently, Razavi et al. has published a state-of-the-art review of surrogate models used in water resources management field. &lt;ref&gt; Razavi, S., B. A.Tolson, and D. H.Burn (2012), Review of surrogate modeling in water resources, Water Resour. Res., 48, W07401, doi: 10.1029/2011WR011527. &lt;/ref&gt;

==Invariance properties==
Recently proposed comparison-based surrogate models (e.g. ranking [[support vector machine]]) for [[evolutionary algorithms]], such as [[CMA-ES]], allow to preserve some invariance properties of surrogate-assisted optimizers:
&lt;ref&gt;{{cite conference
| first = I.
| last = Loshchilov |author2=M. Schoenauer |author3=M. Sebag
| title = Comparison-Based Optimizers Need Comparison-Based Surrogates
| booktitle = Parallel Problem Solving from Nature (PPSN XI)
| pages = 364–1373
| publisher = Springer
| date = 2010
| location = 
| url = https://hal.inria.fr/file/index/docid/493921/filename/ACM-ES.pdf
| accessdate = 
| id = 
}}&lt;/ref&gt;
*1. Invariance with respect to monotonous transformations of the function (scaling)
*2. Invariance with respect to [[orthogonal transform]]ations of the search space (rotation).

==Applications==

An important distinction can be made between two different applications of surrogate models: design optimization and design space approximation (also known as emulation).

In surrogate model based optimization an initial surrogate is constructed using some of the available budget of expensive experiments and/or simulations. The remaining experiments/simulations are run for designs which the surrogate model predicts may have promising performance. The process usually takes the form of the following search/update procedure.

*1. Initial sample selection (the experiments and/or simulations to be run)
*2. Construct surrogate model
*3. Search surrogate model (the model can be searched extensively, e.g. using a [[genetic algorithm]], as it is cheap to evaluate)
*4. Run and update experiment/simulation at new location(s) found by search and add to sample
*5. Iterate steps 2 to 4 until out of time or design 'good enough'

Depending on the type of surrogate used and the complexity of the problem, the process may converge on a local or global optimum, or perhaps none at all.&lt;ref&gt;Jones, D.R (2001), "A taxonomy of global optimization methods based on
response surfaces," Journal of Global Optimization, 21:345–383.&lt;/ref&gt;

In design space approximation, one is not interested in finding the optimal parameter vector but rather in the global behavior of the system. Here the surrogate is tuned to mimic the underlying model as closely as needed over the complete design space. Such surrogates are a useful, cheap way to gain insight into the global behavior of the system. Optimization can still occur as a post processing step, although with no update procedure (see above) the optimum found cannot be validated.

==See also==
*[[Linear approximation]]
*[[Response surface methodology]]
*[[Kriging]]
*[[Radial basis function|Radial Basis Functions]]
*[[Gradient-Enhanced Kriging (GEK)]]
*[[OptiY]]
*[[Space mapping]]
*[[Surrogate endpoint]]
*[[Surrogate data]]
*[[Fitness approximation]]
*[[Computer experiment]]
*[[Conceptual model|Model]]

==References==
&lt;references/&gt;

==Reading==
* Queipo, N.V., Haftka, R.T., [[Wei Shyy|Shyy, W.]], Goel, T., Vaidyanathan, R., Tucker, P.K. (2005), “Surrogate-based analysis and optimization,” Progress in Aerospace Sciences, 41, 1–28.
* D. Gorissen, I. Couckuyt, P. Demeester, T. Dhaene, K. Crombecq, (2010), “[http://jmlr.csail.mit.edu/papers/volume11/gorissen10a/gorissen10a.pdf A Surrogate Modeling and Adaptive Sampling Toolbox for Computer Based Design]," Journal of Machine Learning Research, Vol. 11, pp.&amp;nbsp;2051−2055, July 2010.
* T-Q. Pham, A. Kamusella, H. Neubert, “[http://www.ep.liu.se/ecp/063/074/ecp11063074.pdf Auto-Extraction of Modelica Code from Finite Element Analysis or Measurement Data]," 8th International Modelica Conference, 20–22 March 2011 in Dresden.
* Forrester, Alexander, Andras Sobester, and Andy Keane, ''Engineering design via surrogate modelling: a practical guide'', John Wiley &amp; Sons, 2008.
* Bouhlel, M. A. and Bartoli, N. and Otsmane, A. and Morlier, J. (2016) "Improving kriging surrogates of high-dimensional design models by Partial Least Squares dimension reduction", Structural and Multidisciplinary Optimization 53 (5), 935-952
* Bouhlel, M. A. and Bartoli, N. and Otsmane, A. and Morlier, J. (2016) "An improved approach for estimating the hyperparameters of the kriging model for high-dimensional problems through the partial least squares method", Mathematical Problems in Engineering

==External links==
* [http://www.wiley.com//legacy/wileychi/forrester/terms.html Matlab code for surrogate modelling]
* [http://sumowiki.intec.ugent.be Matlab '''SU'''rrogate '''MO'''deling Toolbox – Matlab SUMO Toolbox]
* [https://github.com/SMTorg/SMT Surrogate Modeling Toolbox -- Python]

[[Category:Design of experiments]]
[[Category:Numerical analysis]]
[[Category:Scientific modeling]]
[[Category:Mathematical modeling]]</text>
      <sha1>tut907341o9polswiytg699ew5983es</sha1>
    </revision>
  </page>
  <page>
    <title>Tertiary ideal</title>
    <ns>0</ns>
    <id>25162434</id>
    <revision>
      <id>807634325</id>
      <parentid>790777863</parentid>
      <timestamp>2017-10-29T05:27:57Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v473)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2125">In [[mathematics]], a '''tertiary ideal''' is an (two-sided) [[Ideal (ring theory)|ideal]] in a (perhaps noncommutative) [[Ring (mathematics)|ring]] that cannot be expressed as a nontrivial intersection of a right [[fractional ideal]] with another ideal. Tertiary ideals generalize [[primary ideal]]s to the case of [[noncommutative ring]]s. Although [[primary decomposition]]s do not exist in general for ideals in noncommutative rings, tertiary decompositions do, at least if the ring is [[Noetherian ring|Noetherian]].

Every primary ideal is tertiary. Tertiary ideals and primary ideals coincide for commutative rings. To any (two-sided) ideal, a tertiary ideal can be associated called the tertiary radical, defined as

:&lt;math&gt;t(I) = \{r \in R \mbox{ }|\mbox{ } \forall s \notin I, \mbox{ }\exists x \in (s)\mbox{ } x \notin I \text{ and } (x)(r) \subset I \}. &lt;/math&gt;

Then ''t''(''I'') always contains ''I''.

If ''R'' is a (not necessarily commutative) Noetherian ring and ''I'' a right ideal in ''R'', then ''I'' has a unique irredundant decomposition into tertiary ideals

:&lt;math&gt;I = T_1 \cap \dots \cap T_n&lt;/math&gt;.

== See also ==
* [[Primary ideal]]
* [[Lasker–Noether theorem]]

== References ==
* {{Citation | last=Riley | first=J.A. | title=Axiomatic primary and tertiary decomposition theory | journal=Trans. Amer. Math. Soc. | year=1962 | volume=105 | pages=177–201|doi=10.1090/s0002-9947-1962-0141683-4 }}
* [http://www.encyclopediaofmath.org/index.php/Tertiary_ideal Tertiary ideal], Encyclopedia of Mathematics, Springer Online Reference Works.
* {{Citation | last=Behrens | first=Ernst-August | title=Ring Theory | publisher=Verlag Academic Press | year=1972 | url=https://books.google.ch/books?id=ZKGq4IQHhHUC&amp;lpg=PP1&amp;pg=PP1#v=onepage&amp;q=&amp;f=false}}
* {{Citation | last=Kurata | first=Yoshiki | title=On an additive ideal theory in a non-associative ring | journal=Mathematische Zeitschrift | volume=88 | issue=2 | year=1965 | doi=10.1007/BF01112095 | pages=129–135 | url=http://www.springerlink.com/content/h772w68514700345/}}

{{algebra-stub}}

{{DEFAULTSORT:Tertiary ideals}}
[[Category:Algebra]]</text>
      <sha1>1ey1jun2wh2ayuqsr6w8xc6hl1w93h5</sha1>
    </revision>
  </page>
  <page>
    <title>Trinomial</title>
    <ns>0</ns>
    <id>2647057</id>
    <revision>
      <id>806061708</id>
      <parentid>804610165</parentid>
      <timestamp>2017-10-19T12:34:09Z</timestamp>
      <contributor>
        <username>Onel5969</username>
        <id>10951369</id>
      </contributor>
      <minor/>
      <comment>Disambiguating links to [[Multinomial]] (intentional link to DAB) using [[User:Qwertyytrewqqwerty/DisamAssist|DisamAssist]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1854">{{About|mathematics|the use in taxonomy|Trinomial name|the use identifying archaeological sites in the United States|Smithsonian trinomial}}
In [[elementary algebra]], a '''trinomial''' is a [[polynomial]] consisting of three terms or [[monomial]]s.&lt;ref&gt;{{cite web |url=https://www.mathsisfun.com/definitions/trinomial.html | title=Definition of Trinomial | work=Math Is Fun | accessdate=16 April 2016}}&lt;/ref&gt;

==Trinomial expressions==
# &lt;math&gt;3x + 5y + 8z&lt;/math&gt; with &lt;math&gt;x, y, z&lt;/math&gt; variables
# &lt;math&gt;3t + 9s^2 + 3y^3&lt;/math&gt; with &lt;math&gt;t, s, y&lt;/math&gt; variables
# &lt;math&gt;3ts + 9t + 5s&lt;/math&gt; with &lt;math&gt;t, s&lt;/math&gt; variables 
# &lt;math&gt;A x^a y^b z^c + B t + C s&lt;/math&gt; with &lt;math&gt;x, y, z, t, s&lt;/math&gt;  variables, &lt;math&gt;a, b, c&lt;/math&gt; nonnegative integers and &lt;math&gt;A, B, C&lt;/math&gt; any constants.  
# &lt;math&gt;Px^a + Qx^b + Rx^c&lt;/math&gt; where &lt;math&gt;x&lt;/math&gt; is variable and constants &lt;math&gt;a, b, c&lt;/math&gt; are nonnegative integers and &lt;math&gt;P, Q, R&lt;/math&gt; any constants.

==Trinomial equation==
A trinomial equation is a polynomial equation involving three terms. An example is the equation &lt;math&gt;x = q + x^m&lt;/math&gt; studied by [[Johann Heinrich Lambert]] in the 18th century.&lt;ref&gt;{{cite journal |first=R. M. |last=Corless |first2=G. H. |last2=Gonnet |first3=D. E. G. |last3=Hare |first4=D. J. |last4=Jerey |first5=D. E. |last5=Knuth |year=1996 |url=http://www.cs.uwaterloo.ca/research/tr/1993/03/W.pdf |title=On the Lambert ''W'' Function |journal=Advances in Computational Mathematics |volume=5 |issue=1 |pages=329–359 |doi=10.1007/BF02124750 }}&lt;/ref&gt;

==See also==
*[[Trinomial expansion]]
*[[Monomial]]
*[[Binomial (polynomial)|Binomial]]
*[[Multinomial (disambiguation)|Multinomial]]
*[[Simple expression]]
*[[Compound expression]]

==References==
{{reflist}}

{{polynomials}}
{{algebra-stub}}

[[Category:Elementary algebra]]
[[Category:Polynomials]]</text>
      <sha1>mtmui6lk1fvd3ct249i4oy7527gl0iv</sha1>
    </revision>
  </page>
  <page>
    <title>Worley noise</title>
    <ns>0</ns>
    <id>25815583</id>
    <revision>
      <id>779268561</id>
      <parentid>757293179</parentid>
      <timestamp>2017-05-07T23:59:01Z</timestamp>
      <contributor>
        <username>Shiftchange</username>
        <id>75349</id>
      </contributor>
      <comment>/* External links */ add {{Fractals}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2596">[[File:Worley.jpg|thumb|200px|right|Example picture generated with Worley noise's basic [[algorithm]]. Tweaking of seed points and colors would be necessary to make this look like stone.]]

'''Worley noise''' is a noise function introduced by [[Steven Worley]] in 1996. In [[computer graphics]] it is used to create [[procedural texture]]s,&lt;ref name="CozziRiccio2012"&gt;{{cite book|author1=Patrick Cozzi|author2=Christophe Riccio|title=OpenGL Insights|url=https://books.google.com/books?id=CCVenzOGjpcC&amp;pg=PA113|year=2012|publisher=CRC Press|isbn=978-1-4398-9376-0|pages=113–115}}&lt;/ref&gt; that is textures that are created automatically in arbitrary precision and don't have to be drawn by hand. Worley noise comes close to simulating textures of stone, water, or [[cell noise]].

==Basic algorithm==
The basic idea is to take random points in space (2- or 3-dimensional) and then for every point in space take the distance to the ''n''th-closest point (e.g. the second-closest point) as some kind of color information.
More precisely:
* Randomly distribute feature points in space
* Noise ''F''&lt;sub&gt;''n''&lt;/sub&gt;(''x'') is distance to ''n''th-closest point to&amp;nbsp;''x''

Typical implementations, in three dimensions, divide the space into cubes.  A fixed number of positions are generated for each cube.  In the case of three dimensions, nine cubes' points need to be generated, to be sure to find the closest.

==See also==
{{Portal|Computer science}}
* [[Fractal]]
* [[Voronoi diagram]]

==References==
{{reflist}}

==Further reading==
* {{cite book|last=Worley|first=Steven|title=A cellular texture basis function|publisher=acm.org|year=1996|series=Proceedings of the 23rd annual conference on computer graphics and interactive techniques |pages=291–294|isbn=0-89791-746-4|url=http://www.rhythmiccanvas.com/research/papers/worley.pdf}}
* {{cite book|author1=David S. Ebert|author2=F. Kenton Musgrave|author3=Darwyn Peachey |author4=Ken Perlin |author5=Steve Worley|title=Texturing and Modeling: A Procedural Approach|url=https://books.google.com/books?id=bDlSJd8GfMcC&amp;pg=PA135|year=2002|publisher=Morgan Kaufmann|isbn=978-1-55860-848-1|pages=135–155}}

==External links==
* [http://aftbit.com/cell-noise-2/ A Good Tutorial on Worley Noise]
* [http://www.carljohanrosen.com/share/CellNoiseAndProcessing.pdf Detailed description on how to implement cell noise]

{{Fractals}}
{{Coherent noise |state=expanded}}
{{Noise}}

[[Category:Noise (graphics)]]
[[Category:Special effects]]
[[Category:Fractals]]
[[Category:Computer graphic techniques]]

{{compu-graphics-stub}}
{{Computer-science-stub}}</text>
      <sha1>drddtcg4k941b9i82xrgfbpf2x9bdq4</sha1>
    </revision>
  </page>
</mediawiki>
