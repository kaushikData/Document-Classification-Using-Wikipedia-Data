<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Antimatroid</title>
    <ns>0</ns>
    <id>667063</id>
    <revision>
      <id>845112280</id>
      <parentid>828790174</parentid>
      <timestamp>2018-06-09T13:27:37Z</timestamp>
      <contributor>
        <username>Bibcode Bot</username>
        <id>14394459</id>
      </contributor>
      <minor/>
      <comment>Adding 0 [[arXiv|arxiv eprint(s)]], 1 [[bibcode|bibcode(s)]] and 0 [[digital object identifier|doi(s)]]. Did it miss something? Report bugs, errors, and suggestions at [[User talk:Bibcode Bot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="29126">[[Image:Antimatroid.svg|thumb|360px|Three views of an antimatroid: an inclusion ordering on its family of feasible sets, a formal language, and the corresponding path poset.]]
In [[mathematics]], an '''antimatroid''' is a [[formal system]] that describes processes in which a [[set (mathematics)|set]] is built up by including elements one at a time, and in which an element, once available for inclusion, remains available until it is included. Antimatroids are commonly [[Cryptomorphism|axiomatized in two equivalent ways]], either as a [[set system]] modeling the possible states of such a process, or as a [[formal language]] modeling the different sequences in which elements may be included.
[[Robert P. Dilworth|Dilworth]] (1940) was the first to study antimatroids, using yet another axiomatization based on [[lattice (order)|lattice theory]], and they have been frequently rediscovered in other contexts;&lt;ref&gt;Two early references are {{harvtxt|Edelman|1980}} and {{harvtxt|Jamison|1980}}; Jamison was the first to use the term "antimatroid". {{harvtxt|Monjardet|1985}} surveys the history of rediscovery of antimatroids.&lt;/ref&gt; see Korte et al. (1991) for a comprehensive survey of antimatroid theory with many additional references.

The axioms defining antimatroids as set systems are very similar to those of [[matroid]]s, but whereas matroids are defined by an ''[[Matroid#Independent sets, bases, and circuits|exchange axiom]]'' (e.g., the ''basis exchange'', or ''independent set exchange'' axioms), antimatroids are defined instead by an ''[[#Convex geometries|anti-exchange axiom]]'', from which their name derives.
Antimatroids can be viewed as a special case of [[greedoid]]s and of [[semimodular lattice]]s, and as a generalization of [[partial order]]s and of [[distributive lattice]]s. 
Antimatroids are equivalent, by [[complement (set theory)|complementation]], to '''[[#Convex geometries|convex geometries]]''', a combinatorial abstraction of [[convex set]]s in [[geometry]].

Antimatroids have been applied to model precedence constraints in [[Job shop scheduling|scheduling problems]], potential event sequences in simulations, task planning in [[artificial intelligence]], and the states of knowledge of human learners.

== Definitions ==
An antimatroid can be defined as a finite family ''F'' of sets, called ''feasible sets'', with the following two properties:
* The [[Union (set theory)|union]] of any two feasible sets is also feasible. That is, ''F'' is [[Closure (mathematics)|closed]] under unions.
* If ''S'' is a nonempty feasible set, then there exists some ''x'' in ''S'' such that ''S'' \ {''x''} (the set formed by removing ''x'' from ''S'') is also feasible. That is, ''F'' is an [[accessible set system]].

Antimatroids also have an equivalent definition as a [[formal language]], that is, as a set of [[String (computer science)|strings]] defined from a finite alphabet of [[symbol]]s. A language ''L'' defining an antimatroid must satisfy the following properties:
* Every symbol of the alphabet occurs in at least one word of ''L''.
* Each word of ''L'' contains at most one copy of any symbol.
* Every [[Prefix (computer science)|prefix]] of a string in ''L'' is also in ''L''.
* If ''s'' and ''t'' are strings in ''L'', and ''s'' contains at least one symbol that is not in ''t'', then there is a symbol ''x'' in ''s'' such that ''tx'' is another string in ''L''.

If ''L'' is an antimatroid defined as a formal language, then the sets of symbols in strings of ''L'' form an accessible union-closed set system. In the other direction, if ''F'' is an accessible union-closed set system, and ''L'' is the language of strings ''s'' with the property that the set of symbols in each prefix of ''s'' is feasible, then ''L'' defines an antimatroid. Thus, these two definitions lead to mathematically equivalent classes of objects.&lt;ref&gt;Korte et al., Theorem 1.4.&lt;/ref&gt;

==Examples==
[[Image:Convex shelling.svg|thumb|300px|A shelling sequence of a planar point set. The line segments show edges of the [[convex hull]]s after some of the points have been removed.]]

*A ''chain antimatroid'' has as its formal language the prefixes of a single word, and as its feasible sets the sets of symbols in these prefixes. For instance the chain antimatroid defined by the word "abcd" has as its formal language the strings {ε, "a", "ab", "abc", "abcd"} and as its feasible sets the sets Ø, {a}, {a,b}, {a,b,c}, and {a,b,c,d}.&lt;ref name="Gordon (1997)"/&gt;
*A ''poset antimatroid'' has as its feasible sets the [[lower set]]s of a finite [[partially ordered set]]. By [[Birkhoff's representation theorem]] for distributive lattices, the feasible sets in a poset antimatroid (ordered by set inclusion) form a distributive lattice, and any distributive lattice can be formed in this way. Thus, antimatroids can be seen as generalizations of distributive lattices. A chain antimatroid is the special case of a poset antimatroid for a [[total order]].&lt;ref name="Gordon (1997)"/&gt;
*A ''shelling sequence'' of a finite set ''U'' of points in the [[Euclidean plane]] or a higher-dimensional [[Euclidean space]] is an ordering on the points such that, for each point ''p'', there is a [[Line (geometry)|line]] (in the Euclidean plane, or a [[hyperplane]] in a Euclidean space) that separates ''p'' from all later points in the sequence. Equivalently, ''p'' must be a vertex of the [[convex hull]] of it and all later points. The partial shelling sequences of a point set form an antimatroid, called a ''shelling antimatroid''. The feasible sets of the shelling antimatroid are the [[Intersection (set theory)|intersection]]s of ''U'' with the [[complement (set theory)|complement]] of a convex set.&lt;ref name="Gordon (1997)"/&gt; Every antimatroid is isomorphic to a shelling antimatroid of points in a sufficiently high-dimensional space.{{sfnp|Kashiwabara|Nakamura|Okamoto|2005}}
*A ''[[perfect elimination ordering]]'' of a [[chordal graph]] is an ordering of its vertices such that, for each vertex ''v'', the neighbors of ''v'' that occur later than ''v'' in the ordering form a [[clique (graph theory)|clique]]. The prefixes of perfect elimination orderings of a chordal graph form an antimatroid.&lt;ref name="Gordon (1997)"&gt;Gordon (1997) describes several results related to antimatroids of this type, but these antimatroids were mentioned earlier e.g. by Korte et al. Chandran et al. (2003) use the connection to antimatroids as part of an algorithm for efficiently listing all perfect elimination orderings of a given chordal graph.&lt;/ref&gt; Antimatroids also describe some other kinds of vertex removal orderings in graphs, such as the dismantling orders of [[cop-win graph]]s.

==Paths and basic words==
In the set theoretic axiomatization of an antimatroid there are certain special sets called ''paths'' that determine the whole antimatroid, in the sense that the sets of the antimatroid are exactly the unions of paths. If ''S'' is any feasible set of the antimatroid, an element ''x'' that can be removed from ''S'' to form another feasible set is called an ''endpoint'' of ''S'', and a feasible set that has only one endpoint is called a ''path'' of the antimatroid. The family of paths can be partially ordered by set inclusion, forming the ''path poset'' of the antimatroid.

For every feasible set ''S'' in the antimatroid, and every element ''x'' of ''S'', one may find a path subset of ''S'' for which ''x'' is an endpoint: to do so, remove one at a time elements other than ''x'' until no such removal leaves a feasible subset. Therefore, each feasible set in an antimatroid is the union of its path subsets. If ''S'' is not a path, each subset in this union is a [[proper subset]] of ''S''. But, if ''S'' is itself a path with endpoint ''x'', each proper subset of ''S'' that belongs to the antimatroid excludes ''x''. Therefore, the paths of an antimatroid are exactly the sets that do not equal the unions of their proper subsets in the antimatroid. Equivalently, a given family of sets ''P'' forms the set of paths of an antimatroid if and only if, for each ''S'' in ''P'', the union of subsets of ''S'' in ''P'' has one fewer element than ''S'' itself. If so, ''F'' itself is the family of unions of subsets of ''P''.

In the formal language formalization of an antimatroid we may also identify a subset of words that determine the whole language, the ''basic words''.
The longest strings in ''L'' are called ''basic words''; each basic word forms a permutation of the whole alphabet. For instance, the basic words of a poset antimatroid are the [[linear extension]]s of the given partial order. If ''B'' is the set of basic words, ''L'' can be defined from ''B'' as the set of prefixes of words in ''B''. It is often convenient to define antimatroids from basic words in this way, but it is not straightforward to write an axiomatic definition of antimatroids in terms of their basic words.

==Convex geometries==
{{See also|Convex set|Convex geometry|Closure operator}}
If ''F'' is the set system defining an antimatroid, with ''U'' equal to the union of the sets in ''F'', then the family of sets
:&lt;math&gt;G = \{U\setminus S\mid S\in F\}&lt;/math&gt;
[[Complement (set theory)|complementary]] to the sets in ''F'' is sometimes called a '''convex geometry''' and the sets in ''G'' are called '''convex sets'''. For instance, in a shelling antimatroid, the convex sets are intersections of ''U'' with convex subsets of the Euclidean space into which ''U'' is embedded.

Complementarily to the properties of set systems that define antimatroids, the set system defining a convex geometry should be closed under intersections, and for any set ''S'' in ''G'' that is not equal to ''U'' there must be an element ''x'' not in ''S'' that can be added to ''S'' to form another set in ''G''.

A convex geometry can also be defined in terms of a [[closure operator]] τ that maps any subset of ''U'' to its minimal closed superset. To be a closure operator, τ should have the following properties:
* τ(∅) = ∅: the closure of the [[empty set]] is empty.
* Any set ''S'' is a subset of τ(''S'').
* If ''S'' is a subset of ''T'', then τ(''S'') must be a subset of τ(''T'').
* For any set ''S'', τ(''S'') = τ(τ(''S'')).
The family of closed sets resulting from a closure operation of this type is necessarily closed under intersections. The closure operators that define convex geometries also satisfy an additional '''anti-exchange axiom''':
*If neither ''y'' nor ''z'' belong to τ(''S''), but ''z'' belongs to τ(''S'' ∪ {''y''}), then ''y'' does not belong to τ(''S'' ∪ {''z''}).
A closure operation satisfying this axiom is called an '''anti-exchange closure'''. If ''S'' is a closed set in an anti-exchange closure, then the anti-exchange axiom determines a partial order on the elements not belonging to ''S'', where ''x'' ≤ ''y'' in the partial order when ''x'' belongs to τ(''S'' ∪ {''y''}). If ''x'' is a minimal element of this partial order, then ''S'' ∪ {''x''} is closed. That is, the family of closed sets of an anti-exchange closure has the property that for any set other than the universal set there is an element ''x'' that can be added to it to produce another closed set. This property is complementary to the accessibility property of antimatroids, and the fact that intersections of closed sets are closed is complementary to the property that unions of feasible sets in an antimatroid are feasible. Therefore, the complements of the closed sets of any anti-exchange closure form an antimatroid.&lt;ref&gt;Korte et al., Theorem 1.1.&lt;/ref&gt;

The [[undirected graph]]s in which the convex sets (subsets of vertices that contain all [[shortest path]]s between vertices in the subset) form a convex geometry are exactly the [[Ptolemaic graph]]s.{{sfnp|Farber|Jamison|1986}}

==Join-distributive lattices==
Any two sets in an antimatroid have a unique [[least upper bound]] (their union) and a unique [[greatest lower bound]] (the union of the sets in the antimatroid that are contained in both of them). Therefore, the sets of an antimatroid, [[partial order|partially ordered]] by set inclusion, form a [[Lattice (order)|lattice]]. Various important features of an antimatroid can be interpreted in lattice-theoretic terms; for instance the paths of an antimatroid are the [[Lattice (order)#Important lattice-theoretic notions|join-irreducible]] elements of the corresponding lattice, and the basic words of the antimatroid correspond to [[maximal chain]]s in the lattice. The lattices that arise from antimatroids in this way generalize the finite [[distributive lattice]]s, and can be characterized in several different ways.

*The description originally considered by {{harvtxt|Dilworth|1940}} concerns [[Lattice (order)#Important lattice-theoretic notions|meet-irreducible]] elements of the lattice. For each element ''x'' of an antimatroid, there exists a unique maximal feasible set ''S&lt;sub&gt;x&lt;/sub&gt;'' that does not contain ''x'' (''S&lt;sub&gt;x&lt;/sub&gt;'' is the union of all feasible sets not containing ''x''). ''S&lt;sub&gt;x&lt;/sub&gt;'' is meet-irreducible, meaning that it is not the meet of any two larger lattice elements: any larger feasible set, and any intersection of larger feasible sets, contains ''x'' and so does not equal ''S&lt;sub&gt;x&lt;/sub&gt;''. Any element of any lattice can be decomposed as a meet of meet-irreducible sets, often in multiple ways, but in the lattice corresponding to an antimatroid each element ''T'' has a unique minimal family of meet-irreducible sets ''S&lt;sub&gt;x&lt;/sub&gt;'' whose meet is ''T''; this family consists of the sets ''S&lt;sub&gt;x&lt;/sub&gt;'' such that ''T''&amp;nbsp;∪&amp;nbsp;{''x''} belongs to the antimatroid. That is, the lattice has ''unique meet-irreducible decompositions''.
*A second characterization concerns the ''intervals'' in the lattice, the sublattices defined by a pair of lattice elements ''x''&amp;nbsp;≤&amp;nbsp;''y'' and consisting of all lattice elements ''z'' with ''x''&amp;nbsp;≤&amp;nbsp;''z''&amp;nbsp;≤&amp;nbsp;''y''. An interval is [[Atom (order theory)|atomistic]] if every element in it is the join of atoms (the minimal elements above the bottom element ''x''), and it is [[Boolean algebra (structure)|Boolean]] if it is isomorphic to the lattice of [[power set|all subsets]] of a finite set. For an antimatroid, every interval that is atomistic is also boolean.
*Thirdly, the lattices arising from antimatroids are [[semimodular lattice]]s, lattices that satisfy the [[Semimodular lattice|''upper semimodular law'']] that for any two elements ''x'' and ''y'', if ''y'' covers ''x''&amp;nbsp;∧&amp;nbsp;''y'' then ''x''&amp;nbsp;∨&amp;nbsp;''y'' covers ''x''. Translating this condition into the sets of an antimatroid, if a set ''Y'' has only one element not belonging to ''X'' then that one element may be added to ''X'' to form another set in the antimatroid. Additionally, the lattice of an antimatroid has the ''meet-semidistributive property'': for all lattice elements ''x'', ''y'', and ''z'', if ''x''&amp;nbsp;∧&amp;nbsp;''y'' and ''x''&amp;nbsp;∧&amp;nbsp;''z'' are both equal then they also equal ''x''&amp;nbsp;∧&amp;nbsp;(''y''&amp;nbsp;∨&amp;nbsp;''z''). A semimodular and meet-semidistributive lattice is called a ''join-distributive lattice''.

These three characterizations are equivalent: any lattice with unique meet-irreducible decompositions has boolean atomistic intervals and is join-distributive, any lattice with boolean atomistic intervals has unique meet-irreducible decompositions and is join-distributive, and any join-distributive lattice has unique meet-irreducible decompositions and boolean atomistic intervals.&lt;ref&gt;{{harvtxt|Adaricheva|Gorbunov|Tumanov|2003}}, Theorems 1.7 and 1.9; {{harvtxt|Armstrong|2007}}, Theorem 2.7.&lt;/ref&gt; Thus, we may refer to a lattice with any of these three properties as join-distributive. Any antimatroid gives rise to a finite join-distributive lattice, and any finite join-distributive lattice comes from an antimatroid in this way.&lt;ref&gt;{{harvtxt|Edelman|1980}}, Theorem 3.3; {{harvtxt|Armstrong|2007}}, Theorem 2.8.&lt;/ref&gt; Another equivalent characterization of finite join-distributive lattices is that they are [[graded poset|graded]] (any two maximal chains have the same length), and the length of a maximal chain equals the number of meet-irreducible elements of the lattice.&lt;ref&gt;{{harvtxt|Monjardet|1985}} credits a dual form of this characterization to several papers from the 1960s by S. P. Avann.&lt;/ref&gt; The antimatroid representing a finite join-distributive lattice can be recovered from the lattice: the elements of the antimatroid can be taken to be the meet-irreducible elements of the lattice, and the feasible set corresponding to any element ''x'' of the lattice consists of the set of meet-irreducible elements ''y'' such that ''y'' is not greater than or equal to ''x'' in the lattice.

This representation of any finite join-distributive lattice as an accessible family of sets closed under unions (that is, as an antimatroid) may be viewed as an analogue of [[Birkhoff's representation theorem]] under which any finite distributive lattice has a representation as a family of sets closed under unions and intersections.

==Supersolvable antimatroids==
Motivated by a problem of defining partial orders on the elements of a [[Coxeter group]], {{harvtxt|Armstrong|2007}} studied antimatroids which are also supersolvable lattices. A supersolvable antimatroid is defined by a [[Total order|totally ordered]] collection of elements, and a [[family of sets]] of these elements. The family must include the empty set. Additionally, it must have the property that if two sets ''A'' and ''B'' belong to the family, the [[set-theoretic difference]] ''B''&amp;nbsp;\&amp;nbsp;''A'' is nonempty, and ''x'' is the smallest element of ''B''&amp;nbsp;\&amp;nbsp;''A'', then ''A''&amp;nbsp;∪&amp;nbsp;{''x''} also belongs to the family. As Armstrong observes, any family of sets of this type forms an antimatroid. Armstrong also provides a lattice-theoretic characterization of the antimatroids that this construction can form.

==Join operation and convex dimension==
If ''A'' and ''B'' are two antimatroids, both described as a family of sets, and if the maximal sets in ''A'' and ''B'' are equal, we can form another antimatroid, the ''join'' of ''A'' and ''B'', as follows:

:&lt;math&gt;A\vee B = \{ S\cup T \mid S\in A \wedge T\in B \}.&lt;/math&gt;

This is a different operation than the join considered in the lattice-theoretic characterizations of antimatroids: it combines two antimatroids to form another antimatroid, rather than combining two sets in an antimatroid to form another set.
The family of all antimatroids that have a given maximal set forms a [[semilattice]] with this join operation.

Joins are closely related to a closure operation that maps formal languages to antimatroids, where the closure of a language ''L'' is the intersection of all antimatroids containing ''L'' as a sublanguage. This closure has as its feasible sets the unions of prefixes of strings in ''L''. In terms of this closure operation, the join is the closure of the union of the languages of ''A'' and ''B''.

Every antimatroid can be represented as a join of a family of chain antimatroids, or equivalently as the closure of a set of basic words; the ''convex dimension'' of an antimatroid ''A'' is the minimum number of chain antimatroids (or equivalently the minimum number of basic words) in such a representation. If ''F'' is a family of chain antimatroids whose basic words all belong to ''A'', then ''F'' generates ''A'' if and only if the feasible sets of ''F'' include all paths of ''A''. The paths of ''A'' belonging to a single chain antimatroid must form a [[chain (order theory)|chain]] in the path poset of ''A'', so the convex dimension of an antimatroid equals the minimum number of chains needed to cover the path poset, which by [[Dilworth's theorem]] equals the width of the path poset.&lt;ref&gt;{{harvtxt|Edelman|Saks|1988}}; Korte et al., Theorem 6.9.&lt;/ref&gt;

If one has a representation of an antimatroid as the closure of a set of ''d'' basic words, then this representation can be used to map the feasible sets of the antimatroid into ''d''-dimensional Euclidean space: assign one coordinate per basic word ''w'', and make the coordinate value of a feasible set ''S'' be the length of the longest prefix of ''w'' that is a subset of ''S''. With this embedding, ''S'' is a subset of ''T'' if and only if the coordinates for ''S'' are all less than or equal to the corresponding coordinates of ''T''. Therefore, the [[order dimension]] of the inclusion ordering of the feasible sets is at most equal to the convex dimension of the antimatroid.&lt;ref&gt;Korte et al., Corollary 6.10.&lt;/ref&gt; However, in general these two dimensions may be very different: there exist antimatroids with order dimension three but with arbitrarily large convex dimension.

==Enumeration==
The number of possible antimatroids on a set of elements grows rapidly with the number of elements in the set. For sets of one, two, three, etc. elements, the number of distinct antimatroids is
:1, 3, 22, 485, 59386, 133059751, ... {{OEIS|id=A119770}}.

==Applications==
Both the precedence and release time constraints in the standard [[notation for theoretic scheduling problems]] may be modeled by antimatroids. {{harvtxt|Boyd|Faigle|1990}} use antimatroids to generalize a [[greedy algorithm]] of [[Eugene Lawler]] for optimally solving single-processor scheduling problems with precedence constraints in which the goal is to minimize the maximum penalty incurred by the late scheduling of a task.

{{harvtxt|Glasserman|Yao|1994}} use antimatroids to model the ordering of events in [[discrete event simulation]] systems.

{{harvtxt|Parmar|2003}} uses antimatroids to model progress towards a goal in [[artificial intelligence]] [[Automated planning and scheduling|planning]] problems.

In [[Optimality Theory]], grammars are logically equivalent to antimatroids ({{harvtxt|Merchant|Riggle|2016}}).

In [[mathematical psychology]], antimatroids have been used to describe [[knowledge space|feasible states of knowledge]] of a human learner. Each element of the antimatroid represents a concept that is to be understood by the learner, or a class of problems that he or she might be able to solve correctly, and the sets of elements that form the antimatroid represent possible sets of concepts that could be understood by a single person. The axioms defining an antimatroid may be phrased informally as stating that learning one concept can never prevent the learner from learning another concept, and that any feasible state of knowledge can be reached by learning a single concept at a time. The task of a knowledge assessment system is to infer the set of concepts known by a given learner by analyzing his or her responses to a small and well-chosen set of problems. In this context antimatroids have also been called "learning spaces" and "well-graded knowledge spaces".&lt;ref&gt;{{harvtxt|Doignon|Falmagne|1999}}.&lt;/ref&gt;

==Notes==
{{reflist|2}}

==References==
{{refbegin|2}}
*{{citation
 | last1 = Adaricheva | first1 = K. V.
 | last2 = Gorbunov | first2 = V. A.
 | last3 = Tumanov | first3 = V. I.
 | doi = 10.1016/S0001-8708(02)00011-7
 | issue = 1
 | journal = Advances in Mathematics
 | pages = 1–49
 | title = Join-semidistributive lattices and convex geometries
 | volume = 173
 | year = 2003}}.
*{{citation
 | last = Armstrong | first = Drew
 | title = The sorting order on a Coxeter group
 | year = 2007
 | arxiv = 0712.1047| bibcode = 2007arXiv0712.1047A}}.
*{{citation
 | last1 = Birkhoff | first1 = Garrett | author1-link = Garrett Birkhoff
 | last2 = Bennett | first2 = M. K.
 | doi = 10.1007/BF00333128
 | issue = 3
 | journal = [[Order (journal)|Order]]
 | pages = 223–242
 | title = The convexity lattice of a poset
 | volume = 2
 | year = 1985}}.
* {{Citation|last1=Björner|first1=Anders|last2=Ziegler|first2=Günter M.|authorlink2=Günter M. Ziegler|authorlink1=Anders Björner|chapter=Introduction to greedoids|series=Encyclopedia of Mathematics and its Applications|volume=40|editor-last=White|editor-first=Neil|publisher=Cambridge University Press|location=Cambridge|year=1992|isbn=0-521-38165-7|pages=284–357|doi=10.1017/CBO9780511662041.009|ref=harv|mr=1165537|title=Matroid Applications}}
*{{citation
 | last1 = Boyd | first1 = E. Andrew
 | last2 = Faigle | first2 = Ulrich
 | doi = 10.1016/0166-218X(90)90002-T
 | issue = 3
 | journal = Discrete Applied Mathematics
 | pages = 197–205
 | title = An algorithmic characterization of antimatroids
 | volume = 28
 | year = 1990}}.
*{{citation
 |last1=Chandran 
 |first1=L. S. 
 |last2=Ibarra 
 |first2=L. 
 |last3=Ruskey 
 |first3=F. 
 |last4=Sawada 
 |first4=J. 
 |doi=10.1016/S0304-3975(03)00221-4 
 |journal=Theoretical Computer Science 
 |pages=303–317 
 |title=Generating and characterizing the perfect elimination orderings of a chordal graph 
 |url=http://skeeter.socs.uoguelph.ca/~sawada/papers/chordal.pdf
 |volume=307 
 |year=2003 
 |issue=2 
}}
*{{citation
 | last = Dilworth | first = Robert P. | author-link = Robert P. Dilworth
 | doi = 10.2307/1968857
 | journal = [[Annals of Mathematics]]
 | pages = 771–777
 | issue = 4
 | title = Lattices with unique irreducible decompositions
 | volume = 41
 | year = 1940
 | jstor = 1968857}}.
*{{citation
 | last1 = Doignon
 | first1 = Jean-Paul
 | authorlink1 = Jean-Paul Doignon
 | last2 = Falmagne
 | first2 = Jean-Claude
 | authorlink2 = Jean-Claude Falmagne
 | title = Knowledge Spaces
 | year = 1999
 | publisher = Springer-Verlag
 | isbn = 3-540-64501-2}}.
*{{citation
 | last = Edelman | first = Paul H.
 | doi = 10.1007/BF02482912
 | issue = 1
 | journal = Algebra Universalis
 | pages = 290–299
 | title = Meet-distributive lattices and the anti-exchange closure
 | volume = 10
 | year = 1980}}.
*{{citation
 | last1 = Edelman | first1 = Paul H.
 | last2 = Saks | first2 = Michael E.
 | doi = 10.1007/BF00143895
 | issue = 1
 | journal = [[Order (journal)|Order]]
 | pages = 23–32
 | title = Combinatorial representation and convex dimension of convex geometries
 | volume = 5
 | year = 1988}}.
*{{citation
 | last1 = Farber | first1 = Martin
 | last2 = Jamison | first2 = Robert E.
 | doi = 10.1137/0607049
 | issue = 3
 | journal = Society for Industrial and Applied Mathematics
 | mr = 844046
 | pages = 433–444
 | title = Convexity in graphs and hypergraphs
 | volume = 7
 | year = 1986}}.
*{{citation
 | last1 = Glasserman | first1 = Paul
 | last2 = Yao | first2 = David D.
 | isbn = 978-0-471-58041-6
 | publisher = Wiley Interscience
 | series = Wiley Series in Probability and Statistics
 | title = Monotone Structure in Discrete Event Systems
 | year = 1994}}.
*{{citation
 | last = Gordon | first = Gary
 | issue = 1
 | journal = [[Electronic Journal of Combinatorics]]
 | page = Research Paper 13
 | title = A β invariant for greedoids and antimatroids
 | url = http://www.combinatorics.org/Volume_4/Abstracts/v4i1r13.html
 | volume = 4
 | year = 1997
 | mr = 1445628}}.
*{{citation
 | last = Jamison | first = Robert
 | contribution = Copoints in antimatroids
 | series = Congressus Numerantium
 | pages = 535–544
 | title = Proceedings of the Eleventh Southeastern Conference on Combinatorics, Graph Theory and Computing (Florida Atlantic Univ., Boca Raton, Fla., 1980), Vol. II
 | volume = 29
 | year = 1980
 | mr = 608454}}.
*{{citation
 | last1 = Kashiwabara | first1 = Kenji
 | last2 = Nakamura | first2 = Masataka
 | last3 = Okamoto | first3 = Yoshio
 | doi = 10.1016/j.comgeo.2004.05.001
 | issue = 2
 | journal = Computational Geometry
 | mr = 2107032
 | pages = 129–144
 | title = The affine representation theorem for abstract convex geometries
 | volume = 30
 | year = 2005}}.
*{{citation
 | last1 = Korte | first1 = Bernhard| author1-link = Bernhard Korte
 | last2 = Lovász | first2 = László | author2-link = László Lovász
 | last3 = Schrader | first3 = Rainer
 | isbn = 3-540-18190-3
 | pages = 19–43
 | publisher = Springer-Verlag
 | title = Greedoids
 | year = 1991}}.

*{{citation
 | last1 = Merchant | first1 = Nazarre
 | last2 = Riggle | first2 = Jason
 | pages = 241
 | volume = 34
 | title = OT grammars, beyond partial orders: ERC sets and antimatroids
 | url = http://roa.rutgers.edu/article/view/1226
 | doi = 10.1007/s11049-015-9297-5
 | journal = Nat Lang Linguist Theory
 | year = 2016}}.

*{{citation
 | last = Monjardet | first = Bernard
 | doi = 10.1007/BF00582748
 | issue = 4
 | journal = [[Order (journal)|Order]]
 | pages = 415–417
 | title = A use for frequently rediscovering a concept
 | volume = 1
 | year = 1985}}.
*{{citation
 | last = Parmar | first = Aarati
 | contribution = Some Mathematical Structures Underlying Efficient Planning
 | title = AAAI Spring Symposium on Logical Formalization of Commonsense Reasoning
 | url = http://www-formal.stanford.edu/aarati/papers/SS603AParmar.pdf
 | year = 2003}}.
{{refend}}

[[Category:Algebraic combinatorics]]
[[Category:Lattice theory]]
[[Category:Convex geometry]]
[[Category:Formal languages]]
[[Category:Set families]]
[[Category:Matroid theory]]
[[Category:Discrete mathematics]]</text>
      <sha1>texbb1d4c6li54hnojo81nonvbd4pti</sha1>
    </revision>
  </page>
  <page>
    <title>Balanced boolean function</title>
    <ns>0</ns>
    <id>24635433</id>
    <revision>
      <id>749850455</id>
      <parentid>545743743</parentid>
      <timestamp>2016-11-16T14:05:29Z</timestamp>
      <contributor>
        <username>Quondum</username>
        <id>12331483</id>
      </contributor>
      <comment>/* top */ fmt</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1206">In [[mathematics]] and [[computer science]], a '''balanced boolean function''' is a [[boolean function]] whose output yields as many '''0'''s as '''1'''s over its [[Domain of a function|input set]]. This means that for a uniformly random input string of bits, the probability of getting a '''1''' is 1/2.

An example of a balanced boolean function is the function that assigns a '''1''' to every [[even number]] and '''0''' to all odd numbers (likewise the other way around). The same applies for functions assigning '''1''' to all positive numbers and '''0''' otherwise.

A Boolean function of ''n'' bits is balanced if it takes the value 1 with probability 1⁄2.

== Usage ==
Balanced boolean functions are primarily used in [[cryptography]].  If a function is not balanced, it will have a [[statistical bias]], making it subject to [[cryptanalysis]] such as the [[correlation attack]].

== See also ==
* [[Bent function]]

== References ==
* [http://portal.acm.org/citation.cfm?id=1060627 Balanced boolean functions that can be evaluated so that every input bit is unlikely to be read], Annual ACM Symposium on Theory of Computing

[[Category:Boolean algebra]]

{{comp-sci-theory-stub}}
{{crypto-stub}}</text>
      <sha1>krs9ezqthjz9ukw62vjny8pw60kqwlb</sha1>
    </revision>
  </page>
  <page>
    <title>Bessel's inequality</title>
    <ns>0</ns>
    <id>1446277</id>
    <revision>
      <id>823573371</id>
      <parentid>797082144</parentid>
      <timestamp>2018-02-02T01:59:51Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>[[Karen Saxe]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2932">In [[mathematics]], especially [[functional analysis]], '''Bessel's inequality''' is a statement about the coefficients of an element &lt;math&gt;x&lt;/math&gt;  in a [[Hilbert space]] with respect to an [[orthonormal]] [[sequence]].

Let &lt;math&gt;H&lt;/math&gt; be a Hilbert space, and suppose that &lt;math&gt;e_1, e_2, ...&lt;/math&gt; is an orthonormal sequence in &lt;math&gt;H&lt;/math&gt;. Then, for any &lt;math&gt;x&lt;/math&gt; in &lt;math&gt;H&lt;/math&gt; one has
:&lt;math&gt;\sum_{k=1}^{\infty}\left\vert\left\langle x,e_k\right\rangle \right\vert^2 \le \left\Vert x\right\Vert^2,&lt;/math&gt;

where 〈•,•〉 denotes the [[inner product space|inner product]] in the Hilbert space &lt;math&gt;H&lt;/math&gt;.&lt;ref&gt;{{Cite book|url=https://books.google.com/books?id=QALoZC64ea0C|title=Beginning Functional Analysis|last=Saxe|first=Karen|authorlink= Karen Saxe |date=2001-12-07|publisher=Springer Science &amp; Business Media|isbn=9780387952246|pages=82|language=en}}&lt;/ref&gt;&lt;ref&gt;{{Cite book|url=https://books.google.com/books?id=XF8W9W-eyrgC|title=Mathematical Analysis II|last=Zorich|first=Vladimir A.|last2=Cooke|first2=R.|date=2004-01-22|publisher=Springer Science &amp; Business Media|isbn=9783540406334|pages=508–509|language=en}}&lt;/ref&gt;&lt;ref&gt;{{Cite book|url=https://books.google.com/books?id=LBZEBAAAQBAJ|title=Foundations of Signal Processing|last=Vetterli|first=Martin|last2=Kovačević|first2=Jelena|last3=Goyal|first3=Vivek K.|date=2014-09-04|publisher=Cambridge University Press|isbn=9781139916578|pages=83|language=en}}&lt;/ref&gt; If we define the infinite sum
:&lt;math&gt;x' = \sum_{k=1}^{\infty}\left\langle x,e_k\right\rangle e_k, &lt;/math&gt;
consisting of "infinite sum" of [[vector resolute]] &lt;math&gt;x&lt;/math&gt; in direction &lt;math&gt;e_k&lt;/math&gt;, Bessel's [[inequality (mathematics)|inequality]] tells us that this [[series (mathematics)|series]] [[Limit of a sequence|converges]]. One can think of it that there exists &lt;math&gt;x' \in H&lt;/math&gt; that can be described in terms of potential basis &lt;math&gt;e_1, e_2, \dots&lt;/math&gt;.

For a complete orthonormal sequence (that is, for an orthonormal sequence that is a [[Orthonormal basis|basis]]), we have [[Parseval's identity]], which replaces the inequality with an equality (and consequently &lt;math&gt;x'&lt;/math&gt; with &lt;math&gt;x&lt;/math&gt;).

Bessel's inequality follows from the identity
:&lt;math&gt;0 \le \left\| x - \sum_{k=1}^n \langle x, e_k \rangle e_k\right\|^2 = \|x\|^2 - 2 \sum_{k=1}^n |\langle x, e_k \rangle |^2 + \sum_{k=1}^n | \langle x, e_k \rangle |^2 = \|x\|^2 - \sum_{k=1}^n | \langle x, e_k \rangle |^2,&lt;/math&gt;
which holds for any natural ''n''.

==See also==
* [[Cauchy–Schwarz inequality]]
* [[Parseval's theorem]]

== Notes ==
{{reflist}}

==External links==
* {{springer|title=Bessel inequality|id=p/b015850}}
* [http://mathworld.wolfram.com/BesselsInequality.html Bessel's Inequality] the article on Bessel's Inequality on MathWorld.

{{PlanetMath attribution|title=Bessel inequality|id=3089}}

{{Functional Analysis}}

[[Category:Hilbert space]]
[[Category:Inequalities]]</text>
      <sha1>akoxql445dj97fza925ucrjgf8kjdss</sha1>
    </revision>
  </page>
  <page>
    <title>Calculus of moving surfaces</title>
    <ns>0</ns>
    <id>28357925</id>
    <revision>
      <id>810448018</id>
      <parentid>794505378</parentid>
      <timestamp>2017-11-15T08:50:45Z</timestamp>
      <contributor>
        <ip>24.246.20.26</ip>
      </contributor>
      <comment>Reformulated and Adjusted Equations to the reformed definition of the Invariant Time Tensor.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8889">[[File:Dannebrog.jpg|thumb|200px|The surface of a flag in the wind is an example of a deforming manifold.]]

The '''calculus of moving surfaces''' ('''CMS''') &lt;ref&gt;Grinfeld, P. (2010). "Hamiltonian Dynamic Equations for Fluid Films". Studies in Applied Mathematics. {{doi|10.1111/j.1467-9590.2010.00485.x}}. {{ISSN|0022-2526}}.&lt;/ref&gt; is an extension of the classical [[Tensor|tensor calculus]] to deforming [[manifold]]s. Central to the CMS is the Tensorial Time Derivative &lt;math&gt;\dot{\nabla}&lt;/math&gt; whose original definition &lt;ref&gt;J. Hadamard, Lecons Sur La Propagation Des Ondes et Les Equations De
l’Hydrodynamique. Paris: Hermann, 1903.&lt;/ref&gt; was put forth by [[Jacques Hadamard]]. It plays the role analogous to that of the [[covariant derivative]] &lt;math&gt;\nabla _{\alpha }&lt;/math&gt; on [[Differentiable manifold|differential manifolds]]. in that it produces a [[tensor]] when applied to a tensor.

[[File:Hadamard2.jpg|thumb|right|130px|Jacques Salomon Hadamard, French Mathematician, 1865–1963 CE]]

Suppose that &lt;math&gt;\Sigma_t&lt;/math&gt; is the evolution of the [[surface (differential geometry)|surface]] &lt;math&gt;\Sigma&lt;/math&gt; indexed by a time-like parameter &lt;math&gt;t&lt;/math&gt;. The definitions of the surface [[velocity]] &lt;math&gt;C&lt;/math&gt; and the [[operator (mathematics)|operator]] &lt;math&gt;\dot{\nabla}&lt;/math&gt; are the [[geometric]] foundations of the CMS. The velocity C is the [[rate (mathematics)|rate]] of deformation of the surface &lt;math&gt;\Sigma&lt;/math&gt; in the instantaneous [[Surface normal|normal]] direction. The value of &lt;math&gt;C&lt;/math&gt; at a point &lt;math&gt;P&lt;/math&gt; is defined as the [[limit of a function|limit]]

: &lt;math&gt;C=\lim_{h\to 0} \frac{\text{Distance}(P,P^*)}{h}&lt;/math&gt;

where &lt;math&gt;P^{*}&lt;/math&gt; is the point on &lt;math&gt;\Sigma_{t+h}&lt;/math&gt; that lies on the straight line perpendicular to &lt;math&gt;\Sigma_{t}&lt;/math&gt; at point P. This definition is illustrated in the first geometric figure below. The velocity &lt;math&gt;C&lt;/math&gt; is a signed quantity: it is positive when &lt;math&gt;\overline{PP^{*}}&lt;/math&gt; points in the direction of the chosen normal, and negative otherwise. The relationship between &lt;math&gt;\Sigma_{t}&lt;/math&gt; and &lt;math&gt;C&lt;/math&gt; is analogous to the relationship between location and velocity in elementary calculus:&amp;nbsp;knowing either quantity allows one to construct the other by [[Derivative|differentiation]] or [[Initial value problem|integration]].

[[File:C3x3t.png|thumb|350px|Geometric construction of the surface velocity C]]
[[File:DFdt3x3t.png|thumb|350px|Geometric construction of the &lt;math&gt;\delta/\delta t&lt;/math&gt;-derivative of an invariant field F]]

The Tensorial Time Derivative &lt;math&gt;\dot{\nabla}&lt;/math&gt; for a scalar field F defined on &lt;math&gt;\Sigma_{t}&lt;/math&gt; is the [[derivative|rate of change]] in &lt;math&gt;F&lt;/math&gt; in the instantaneously normal direction:

: &lt;math&gt;\frac{\delta F}{\delta t}=\lim_{h\to 0} \frac{F(P^*)-F(P)}{h}&lt;/math&gt;

This definition is also illustrated in second geometric figure.

The above definitions are ''[[geometry|geometric]]''. In analytical settings, direct application of these definitions may not be possible. The CMS gives ''analytical'' definitions of C and &lt;math&gt;\dot{\nabla}&lt;/math&gt; in terms of elementary operations from [[calculus]] and [[differential geometry]].

==Analytical definitions==

For [[mathematical analysis|analytical]] definitions of &lt;math&gt;C&lt;/math&gt; and &lt;math&gt;\dot{\nabla}&lt;/math&gt;, consider the evolution of &lt;math&gt;S&lt;/math&gt; given by

: &lt;math&gt;Z^i = Z^i \left( t ,S \right) &lt;/math&gt;

where &lt;math&gt;Z^{i}&lt;/math&gt; are general [[curvilinear coordinates|curvilinear space coordinates]] and &lt;math&gt;S^{\alpha }&lt;/math&gt; are the surface coordinates. By convention, tensor indices of function arguments are dropped. Thus the above equations contains &lt;math&gt;S&lt;/math&gt; rather than &lt;math&gt;S^\alpha&lt;/math&gt;.The velocity object &lt;math&gt;\textbf{V}=V^i\textbf{Z}_i&lt;/math&gt; is defined as the [[partial derivative]]

: &lt;math&gt;V^i =\frac{\partial Z^i \left( t ,S \right)}{\partial t }&lt;/math&gt;

The velocity &lt;math&gt;C&lt;/math&gt; can be computed most directly by the formula

: &lt;math&gt;C=V^i N_i &lt;/math&gt;

where &lt;math&gt;N_i&lt;/math&gt; are the covariant components of the normal vector &lt;math&gt;\vec{N}&lt;/math&gt;.

Also, defining the shift tensor representation of the Surface's Tangent Space &lt;math&gt;Z^{\alpha}_{i}=\textbf{S}^\alpha\cdot\textbf{Z}_i&lt;/math&gt; and the Tangent Velocity as &lt;math&gt;V^\alpha=Z^\alpha_iV^i&lt;/math&gt; , then the definition of the &lt;math&gt;\dot{\nabla}&lt;/math&gt; derivative for an [[Tensor field|invariant]] ''F'' reads

: &lt;math&gt;\dot{\nabla}F=\frac{\partial F\left( t ,S \right)}{\partial t }-V^\alpha\nabla _{\alpha }F&lt;/math&gt;

where &lt;math&gt;\nabla_\alpha &lt;/math&gt; is the covariant derivative on S.

For ''tensors'', an appropriate generalization is needed. The proper definition for a representative tensor &lt;math&gt;T^{i\alpha }_{j\beta }&lt;/math&gt; reads

: &lt;math&gt;\dot{\nabla}T^{i\alpha }_{j\beta }=\frac{\partial T^{i\alpha }_{j\beta }}{\partial t}-V^{\eta }\nabla _{\eta }T^{i\alpha }_{j\beta }+V^{m}\Gamma ^{i}_{mk}T^{k\alpha }_{j\beta }-V^{m}\Gamma ^{k}_{mj}T^{i\alpha }_{k\beta }+\dot{\Gamma}^\alpha_\eta T^{i\eta }_{j\beta }-\dot{\Gamma}^\eta_\beta T^{i\alpha }_{j\eta }&lt;/math&gt;

where &lt;math&gt;\Gamma^k_{mj}&lt;/math&gt; are [[Christoffel symbols]] and &lt;math&gt;\dot{\Gamma}^\alpha_\beta=\nabla_\beta V^\alpha-C B^\alpha_\beta&lt;/math&gt; is the surface's appropriate temporal symbols (&lt;math&gt;B^\alpha_\beta&lt;/math&gt; is a matrix representation of the surface's curvature shape operator)

==Properties of the ''δ''/''δt''-derivative==

The &lt;math&gt;\dot{\nabla}&lt;/math&gt;-derivative commutes with contraction, satisfies the [[product rule]] for any collection of indices

: &lt;math&gt;\dot{\nabla}(S^i_\alpha T^\beta_j
 )=T^\beta_j\dot{\nabla}S^i_\alpha + S^i_\alpha\dot{\nabla}T^\beta_j&lt;/math&gt;

and obeys a [[chain rule]] for surface [[Function (mathematics)#Restrictions and extensions|restrictions]] of spatial tensors:

: &lt;math&gt;\dot{\nabla}F^j_k(Z,t) =\frac{\partial F^j_k}{\partial t}+CN^i \nabla _i F^j_k&lt;/math&gt;

Chain rule shows that the &lt;math&gt;\delta /\delta t&lt;/math&gt;-derivative of spatial "metrics"
vanishes

: &lt;math&gt;\dot{\nabla}\delta^i_j=0,\dot{\nabla}Z_{ij}=0,\dot{\nabla}Z^{ij}=0,\dot{\nabla}\varepsilon _{ijk}=0,\dot{\nabla}\varepsilon^{ijk}=0&lt;/math&gt;

where &lt;math&gt;Z_{ij}&lt;/math&gt; and &lt;math&gt;Z^{ij}&lt;/math&gt; are covariant and contravariant [[metric tensor]]s, &lt;math&gt;\delta ^{i}_{j}&lt;/math&gt; is the [[Kronecker delta]] symbol,  and &lt;math&gt;\varepsilon _{ijk}&lt;/math&gt; and &lt;math&gt;\varepsilon ^{ijk}&lt;/math&gt; are the [[Levi-Civita symbol]]s. The [[Levi-Civita symbol|main article]] on Levi-Civita symbols describes them for [[Cartesian coordinate systems]]. The preceding rule is valid in general coordinates, where the definition of the Levi-Civita symbols must include the square root of the [[determinant]] of the covariant metric tensor &lt;math&gt;Z_{ij}&lt;/math&gt;.

==Differentiation table for the δ/δt-derivative==

The &lt;math&gt;\dot{\nabla}&lt;/math&gt; derivative of the key surface objects leads to highly concise and attractive formulas. When applied to the [[covariance and contravariance of vectors|covariant]] surface [[metric tensor]] &lt;math&gt;S_{\alpha \beta }&lt;/math&gt; and the [[Covariance and contravariance of vectors|contravariant]] metric tensor &lt;math&gt;S^{\alpha \beta }&lt;/math&gt;, the following identities result

: &lt;math&gt;\begin{align}
\dot{\nabla}S_{\alpha \beta } &amp;=0 \\[8pt]
\dot{\nabla}S^{\alpha \beta } &amp;=0
\end{align}&lt;/math&gt;

where &lt;math&gt;B_{\alpha \beta }&lt;/math&gt; and &lt;math&gt;B^{\alpha \beta }&lt;/math&gt; are the doubly covariant and doubly contravariant [[Sectional curvature|curvature tensors]]. These curvature tensors, as well as for the mixed curvature tensor &lt;math&gt;B^\alpha_\beta&lt;/math&gt;, satisfy

: &lt;math&gt;\begin{align}
\dot{\nabla}B_{\alpha \beta }&amp; = \nabla_\alpha \nabla_\beta C + CB_{\alpha \gamma }B^\gamma_\beta \\[8pt]
\dot{\nabla}B^\alpha_\beta&amp; = \nabla_\beta\nabla^\alpha C + CB^\alpha_\gamma B^\gamma_\beta \\[8pt]
\dot{\nabla}B^{\alpha \beta }&amp; = \nabla ^\alpha \nabla^\beta C + CB^{\gamma \alpha}B^\beta_\gamma
\end{align}&lt;/math&gt;

The shift tensor &lt;math&gt;Z^i_\alpha&lt;/math&gt; and the normal&lt;math&gt;N^i&lt;/math&gt; satisfy

: &lt;math&gt;\begin{align}
\dot{\nabla}Z^i_\alpha &amp; = N^i\nabla _\alpha C \\[8pt]
\dot{\nabla}N^i &amp; = -Z^i_\alpha \nabla^\alpha C
\end{align}&lt;/math&gt;

Finally, the surface [[Levi-Civita symbol]]s &lt;math&gt;\varepsilon _{\alpha \beta }&lt;/math&gt; and &lt;math&gt;\varepsilon ^{\alpha \beta }&lt;/math&gt; satisfy

: &lt;math&gt;\begin{align}
\frac{\delta \varepsilon _{\alpha \beta }}{\delta t} &amp; = -\varepsilon _{\alpha \beta }CB^{\gamma }_{\gamma } \\[8pt]
\frac{\delta \varepsilon ^{\alpha \beta }}{\delta t} &amp; = \varepsilon ^{\alpha \beta }CB^\gamma_\gamma
\end{align}&lt;/math&gt;

== Time differentiation of integrals ==
The CMS provides rules for [[time evolution of integrals|time differentiation of volume and surface integrals]].

==References==
{{Reflist}}

[[Category:Tensors]]
[[Category:Differential geometry]]
[[Category:Riemannian geometry]]
[[Category:Curvature (mathematics)]]
[[Category:Calculus]]</text>
      <sha1>h38sw2y152qxltr1ggnzucf4xroxt4e</sha1>
    </revision>
  </page>
  <page>
    <title>Charles Dupin</title>
    <ns>0</ns>
    <id>2252922</id>
    <revision>
      <id>867982730</id>
      <parentid>849420269</parentid>
      <timestamp>2018-11-09T06:48:01Z</timestamp>
      <contributor>
        <username>Abcbalbuena</username>
        <id>2046264</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7118">{{Infobox scientist
| name        = Pierre Charles François Dupin
| image       = Charles Dupin.jpeg
| image_size  = 
| caption     = 
| birth_date  = {{birth date|df=y|1784|10|6}}
| birth_place = Varzy, France
| death_date  = {{death date and age|df=y|1873|1|18|1784|10|6}}
| death_place = Paris, France
| residence   = France
| citizenship = 
| nationality = 
| fields      = [[Mathematics]]&lt;br&gt;[[Engineering]]&lt;br&gt;[[Economics]]
| workplaces  = 
| patrons     = 
| education   = 
| alma_mater  = 
| thesis_title =        &lt;!--(or  | thesis1_title =  and  | thesis2_title = )--&gt;
| thesis_year =         &lt;!--(or  | thesis1_year =   and  | thesis2_year =  )--&gt;
| doctoral_advisor =    &lt;!--(or  | doctoral_advisors = )--&gt;
| academic_advisors = 
| doctoral_students = 
| notable_students = 
| known_for   = 
| influences  = 
| influenced  = 
| awards      = 
| author_abbrev_bot = 
| author_abbrev_zoo = 
| spouse      =         &lt;!--(or | spouses = )--&gt;
| partner     =         &lt;!--(or | partners = )--&gt;
| children    = 
| signature   =         &lt;!--(filename only)--&gt;
| signature_alt = 
| footnotes   = 
}}
Baron '''Pierre Charles François Dupin''' (6 October 1784, [[Varzy]], [[Nièvre]] – 18 January 1873, [[Paris]], [[France]]) was a French [[Catholic]] [[mathematician]], engineer, economist&lt;ref&gt;[http://www.newadvent.org/cathen/05205a.htm Entry in Catholic Encyclopedia]&lt;/ref&gt; and politician, particularly known for work in the field of mathematics, where the [[Dupin cyclide]] and [[Dupin indicatrix]] are named after him; and for his work in the field of statistical and [[thematic map]]ping,&lt;ref name="GP 2007"&gt;[[Palsky, Gilles]]. "[http://belgeo.revues.org/11893?lang=en Connections and Exchanges in European Thematic Cartography. The case of XIXth century choropleth maps]." ''Formatting Europe. Mapping a continent.'' 2007&lt;/ref&gt; In 1826 he created the earliest known [[choropleth map]].&lt;ref name = "MF08"&gt;[[Michael Friendly]] (2008). [http://www.math.yorku.ca/SCS/Gallery/milestone/milestone.pdf "Milestones in the history of thematic cartography, statistical graphics, and data visualization"].&lt;/ref&gt;

== Life and work ==
He was born in [[Varzy]] in France, the son of Charles Andre Dupin, a lawyer, and Catherine Agnes Dupin.&lt;ref&gt;{{cite book|title=Biographical Index of Former Fellows of the Royal Society of Edinburgh 1783–2002|date=July 2006|publisher=The Royal Society of Edinburgh|isbn=0 902 198 84 X|url=https://www.royalsoced.org.uk/cms/files/fellows/biographical_index/fells_indexp1.pdf}}&lt;/ref&gt;

He studied [[geometry]] with [[Gaspard Monge|Monge]] at the [[École Polytechnique]] and then became a naval engineer ([[ENSTA]]). From 1807 he was responsible for the restoration of the damaged port and arsenal at [[Corfu]]. In 1813 he founded the [[Toulon]] Maritime Museum.

In 1819 he was appointed professor at the [[Conservatoire des Arts et Métiers]]; he kept this post until 1854. In 1822, he was elected a foreign member of the [[Royal Swedish Academy of Sciences]].

In 1808, he participated in the Greek science revival by teaching mathematics and mechanics lessons in [[Corfu]]. One of his students was [[Giovanni Carandino]], who would go on to be the founder of the Greek Mathematics School in the 1820s.

In 1818, Dupin was elected to the body of the [[French Academy of Sciences]], one of the [[Institut de France]]'s five Academies. &lt;ref name=":0"&gt;{{Cite web|url=http://www-history.mcs.st-andrews.ac.uk/Biographies/Dupin.html|title=Dupin biography|website=www-history.mcs.st-andrews.ac.uk|access-date=2017-12-18}}&lt;/ref&gt;
[[File:Carte figurative de l'instruction populaire de la France.jpg|thumb|1826 [[choropleth map]] of France.]]
In 1826 he published a [[thematic map]] showing the distribution of [[illiteracy]] in France, using shadings (from black to white), the first known instance of what is called a [[choropleth map]] today.&lt;ref name = "MF08"/&gt; Duplin had been inspired by the work of the German statisticians [[Georg Hassel]] and [[August Friedrich Wilhelm Crome]].&lt;ref name="GP 2007"/&gt;
Dupin was named rapporteur for the central jury of the [[Exposition des produits de l'industrie française en 1834]].
For each branch of industry he noted the quantities and value of French exports and imports, with comparative figures for 1823, 1827 and 1834.&lt;ref&gt;{{citation|page=156 |last=Colmont|first=Achille de|title=Histoire des Expositions des produits de l'Industrie Française|url=https://books.google.com/books?id=270eAQAAMAAJ&amp;pg=PA156 |accessdate=2017-10-11|year=1855|publisher=Guillaumin|language=fr}}&lt;/ref&gt;

In addition, he had a political career and was appointed to the Senate in 1852.&lt;ref name=":0" /&gt; His mathematical work was in [[Descriptive geometry|descriptive]] and [[differential geometry]]. He was the discoverer of [[conjugate tangents]] to a point on a surface and of the [[Dupin indicatrix]].&lt;ref&gt;{{Cite book|url=https://books.google.tn/books/about/Pierre_Charles_Francois_Dupin.html?id=Ym3bjwEACAAJ&amp;redir_esc=y|title=Pierre Charles Francois Dupin|language=en}}&lt;/ref&gt;

== Selected publications ==
[[File:Dupin - Essai historique sur les services et les travaux scientifiques de Gaspard Monge, 1819 - 765494.tif|thumb|''Essai historique sur les services et les travaux scientifiques de [[Gaspard Monge]]'', 1819]]
* Dupin, François Pierre Charles. ''Développements de géométrie.'' (1813).
* Dupin, François Pierre Charles. ''Discours et leçons sur l'industrie, le commerce, la marine, et sur les sciences appliquées aux arts.'' 1825.
* Dupin, François Pierre Charles. ''[https://books.google.com/books?id=qk9NAAAAcAAJ&amp;dq Canal maritime de Suez. Imprimerie de Mallet-Bachelier],'' 1858.

==References==
{{reflist|2}}

== External links ==
* {{commons category-inline}}
* {{Wikiquote-inline}}
* [http://www-groups.dcs.st-and.ac.uk/~history/Mathematicians/Dupin.html Entry in MacTutor History of Mathematics]

{{Authority control}}

{{Cabinet of Hugues-Bernard Maret}}

{{DEFAULTSORT:Dupin, Charles}}
[[Category:1784 births]]
[[Category:1873 deaths]]
[[Category:People from Varzy]]
[[Category:French Roman Catholics]]
[[Category:Politicians from Bourgogne-Franche-Comté]]
[[Category:Orléanists]]
[[Category:Party of Order politicians]]
[[Category:Bonapartists]]
[[Category:Members of the Chamber of Deputies of the Bourbon Restoration]]
[[Category:Members of the 1st Chamber of Deputies of the July Monarchy]]
[[Category:Members of the 2nd Chamber of Deputies of the July Monarchy]]
[[Category:Members of the 3rd Chamber of Deputies of the July Monarchy]]
[[Category:Members of the 1848 Constituent Assembly]]
[[Category:Members of the National Legislative Assembly of the French Second Republic]]
[[Category:French Senators of the Second Empire]]
[[Category:18th-century French mathematicians]]
[[Category:19th-century French mathematicians]]
[[Category:Differential geometers]]
[[Category:Information visualization experts]]
[[Category:Marine engineers]]
[[Category:École Polytechnique alumni]]
[[Category:Members of the French Academy of Sciences]]
[[Category:Members of the Royal Swedish Academy of Sciences]]
[[Category:Mathematician politicians]]</text>
      <sha1>40thswgx89mp8n51odx6afydi9v71f5</sha1>
    </revision>
  </page>
  <page>
    <title>Colinear map</title>
    <ns>0</ns>
    <id>11598940</id>
    <revision>
      <id>861142153</id>
      <parentid>861140343</parentid>
      <timestamp>2018-09-25T11:49:13Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor/>
      <comment>Dating maintenance tags: {{Mergeto}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="923">{{mergeto|coalgebra|date=September 2018}}
{{Multiple issues|

{{technical|date=June 2012}}
{{refimprove|date=June 2007}}
{{notability|date=June 2010}}
}}

In [[coalgebra]] theory, the notion of colinear map is dual to the notion for [[linear map]] of [[vector space]], or more generally, for morphism between [[R-module]]. Specifically, let R be a [[Ring (mathematics)|ring]], M,N,C be R-modules, and

&lt;math&gt; \rho_M: M\rightarrow M\otimes C, \rho_N: N\rightarrow N\otimes C &lt;/math&gt;

be right C-[[comodule]]s. Then an R-linear map &lt;math&gt; f:M\rightarrow N&lt;/math&gt; is called a '''(right) comodule morphism''', or '''(right) C-colinear''', if

&lt;math&gt; \rho_N \circ f = (f \otimes 1) \circ \rho_M &lt;/math&gt;

==References==
*Khaled AL-Takhman, ''Equivalences of Comodule Categories for Coalgebras over Rings'', J. Pure Appl. Algebra,.V. 173, Issue: 3, September 7, 2002, pp.&amp;nbsp;245–271

[[Category:Coalgebras]]


{{algebra-stub}}</text>
      <sha1>crozmyv3mmze2qtdgphpd2cg4z1g6u6</sha1>
    </revision>
  </page>
  <page>
    <title>Condensation point</title>
    <ns>0</ns>
    <id>19472545</id>
    <revision>
      <id>870698408</id>
      <parentid>786347033</parentid>
      <timestamp>2018-11-26T13:50:24Z</timestamp>
      <contributor>
        <ip>134.59.11.233</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1137">{{No footnotes|date=April 2014}}
In mathematics, a '''condensation point''' ''p'' of a subset ''S'' of a [[topological space]], is any point ''p'', such that every [[open neighborhood]] of ''p'' contains [[uncountably many]] points of ''S''. Thus, "condensation point" is synonymous with "&lt;math&gt;\aleph_1&lt;/math&gt;-[[accumulation point]]".&lt;ref&gt;https://www.encyclopediaofmath.org/index.php/Condensation_point_of_a_set&lt;/ref&gt;

==Examples==
*If ''S'' = (0,1) is the open unit interval, a subset of the [[real numbers]], then 0 is a condensation point of ''S''.
*If ''S'' is an uncountable subset of a set ''X'' endowed with the [[indiscrete topology]], then any point ''p'' of ''X'' is a condensation point of ''X'' as the only open neighborhood of ''p'' is ''X'' itself.

==References==
&lt;references/&gt;
* [[Walter Rudin]], ''Principles of Mathematical Analysis'', 3rd Edition, Chapter 2, exercise 27
* [[John C. Oxtoby]], ''Measure and Category'', 2nd Edition (1980), 
* [[Lynn Steen]] and [[J. Arthur Seebach, Jr.]], ''Counterexamples in Topology'', 2nd Edition, pg. 4

[[Category:Mathematical objects]]
[[Category:Topology]]


{{topology-stub}}</text>
      <sha1>n50uf84hkotca65wf0nyw7annf8wt8m</sha1>
    </revision>
  </page>
  <page>
    <title>Conjunctive grammar</title>
    <ns>0</ns>
    <id>3237776</id>
    <revision>
      <id>848063926</id>
      <parentid>848061144</parentid>
      <timestamp>2018-06-29T15:26:14Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor/>
      <comment>Dating maintenance tags: {{Clarify span}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7815">'''Conjunctive grammars''' are a class of formal grammars
studied in [[formal language]] theory.
They extend the basic type of grammars,
the [[context-free grammars]],
with a [[Logical_conjunction|conjunction]] operation.
Besides explicit conjunction,
conjunctive grammars allow implicit [[Logical_disjunction|disjunction]]
represented by multiple rules for a single nonterminal symbol,
which is the only logical connective expressible in context-free grammars.
Conjunction can be used, in particular,
to specify intersection of languages.
A further extension of conjunctive grammars
known as [[Boolean grammar]]s
additionally allows explicit [[negation]].

The rules of a conjunctive grammar are of the form

:&lt;math&gt;A \to \alpha_1 \And \ldots \And \alpha_m&lt;/math&gt;

where &lt;math&gt;A&lt;/math&gt; is a nonterminal and
&lt;math&gt;\alpha_1&lt;/math&gt;, ..., &lt;math&gt;\alpha_m&lt;/math&gt;
are strings formed of symbols in &lt;math&gt;\Sigma&lt;/math&gt; and &lt;math&gt;N&lt;/math&gt; (finite sets of terminal and nonterminal symbols respectively).
Informally, such a rule asserts that 
every string &lt;math&gt;w&lt;/math&gt; over &lt;math&gt;\Sigma&lt;/math&gt;
that satisfies each of the syntactical conditions represented
by &lt;math&gt;\alpha_1&lt;/math&gt;, ..., &lt;math&gt;\alpha_m&lt;/math&gt;
therefore satisfies the condition defined by &lt;math&gt;A&lt;/math&gt;.

== Formal definition ==
A conjunctive grammar &lt;math&gt;G&lt;/math&gt; is defined by the 4-[[tuple]] &lt;math&gt;G = (V, \Sigma, R, S)&lt;/math&gt; where
# {{mvar|V}} is a finite set; each element &lt;math&gt; v\in V&lt;/math&gt; is called ''a nonterminal symbol'' or a ''variable''. Each variable represents a different type of phrase or clause in the sentence. Variables are also sometimes called syntactic categories.
# {{math|&amp;Sigma;}} is a finite set of ''terminal''s, disjoint from {{mvar|V}}, which make up the actual content of the sentence. The set of terminals is the alphabet of the language defined by the grammar {{mvar|G}}.
# {{mvar|R}} is a finite set of productions. The members of {{mvar|R}} are called the ''rule''s or ''production''s of the grammar.
# {{mvar|S}} is the start variable (or start symbol), used to represent the whole sentence (or program). It must be an element of {{mvar|V}}.

It is common to list all right-hand sides for the same left-hand side on the same line, using | (the [[pipe symbol]]) to separate them. Rules &lt;math&gt;A\rightarrow\alpha_1\&amp;\ldots\&amp;\alpha_m&lt;/math&gt; and &lt;math&gt;A\rightarrow\beta_1\&amp;\ldots\&amp;\beta_m&lt;/math&gt; can hence be written as &lt;math&gt;A\rightarrow\alpha_1\&amp;\ldots\&amp;\alpha_m\ |\ \beta_1\&amp;\ldots\&amp;\beta_m&lt;/math&gt;.

Two equivalent formal definitions
of the language specified by a conjunctive grammar exist.
One definition is based upon representing the grammar
as a system of [[language equation]]s with union, intersection and concatenation
and considering its least solution.
The other definition generalizes
[[Noam Chomsky|Chomsky's]] generative definition of the context-free grammars
using rewriting of terms over conjunction and concatenation.

===Definition by derivation===
For any strings &lt;math&gt;u, v\in (V\cup\Sigma)^{*}&lt;/math&gt;, we say {{mvar|u}} directly yields {{mvar|v}}, written as &lt;math&gt;u\Rightarrow v\,&lt;/math&gt;, if 
* either there is a rule &lt;math&gt;A \rightarrow \alpha_1 \&amp; \ldots \&amp; \alpha_m \in R&lt;/math&gt; such that &lt;math&gt;u\,=u_{1} A u_{2}&lt;/math&gt; and &lt;math&gt;v\,=u_{1} (\alpha_1 \&amp; \ldots \&amp; \alpha_m) u_{2}&lt;/math&gt;, 
* or there exists a string &lt;math&gt;w \in (V\cup\Sigma)^{*}&lt;/math&gt; such that &lt;math&gt;u\,=u_{1} (w \&amp; \ldots \&amp; w) u_{2}&lt;/math&gt; and &lt;math&gt;v\,=u_{1} w u_{2}&lt;/math&gt;.

For any string &lt;math&gt;w \in (V\cup\Sigma)^{*}, &lt;/math&gt; we say {{mvar|G}} '''generates''' {{mvar|w}}, written as &lt;math&gt;S\stackrel{*}{\Rightarrow} w&lt;/math&gt; if &lt;math&gt;\exists k\geq 1\, \exists \, u_{1}, \cdots, u_{k}\in (V\cup\Sigma\cup\{ \text{“(“}, \text{“}\&amp;\text{“}, \text{“)“} \})^{*}&lt;/math&gt; such that &lt;math&gt;S = \, u_{1} \Rightarrow u_{2} \Rightarrow \cdots \Rightarrow u_{k} \, = w&lt;/math&gt;.

The language of a grammar &lt;math&gt;G = (V, \Sigma, R, S)&lt;/math&gt; is the set of {{clarify span|all strings|reason=Should be 'all strings from Sigma*' ?|date=June 2018}} it generates.

=== Example ===
The grammar &lt;math&gt;G = (\{S\}, \{a, b\}, P, S)&lt;/math&gt;, with productions
 
:&lt;math&gt;S\rightarrow AB \&amp; DC&lt;/math&gt;,
:&lt;math&gt;A\rightarrow aA\ |\ \epsilon&lt;/math&gt;,
:&lt;math&gt;B\rightarrow bBc\ |\ \epsilon&lt;/math&gt;,
:&lt;math&gt;C\rightarrow cC\ |\ \epsilon&lt;/math&gt;,
:&lt;math&gt;D\rightarrow aDb\ |\ \epsilon&lt;/math&gt;,

is conjunctive. A typical derivation is :&lt;math&gt;S \Rightarrow (AB \&amp; DC) \Rightarrow (aAB \&amp; DC) \Rightarrow (aB \&amp; DC) \Rightarrow (abBc \&amp; DC) \Rightarrow (abc \&amp; DC) \Rightarrow (abc \&amp; aDbC) \Rightarrow (abc \&amp; abC) \Rightarrow (abc \&amp; abcC) \Rightarrow (abc \&amp; abc) \Rightarrow abc&lt;/math&gt; This makes it clear that  &lt;math&gt;L(G) = \{a^nb^nc^n:n \ge 0\}&lt;/math&gt;. The language is not context-free, proved by the [[Pumping lemma for context-free languages|pumping lemma]].

==Parsing algorithms==
Though the expressive power of conjunctive grammars
is greater than those of context-free grammars,
conjunctive grammars retain some of the latter.
Most importantly, there are generalizations of the main context-free parsing algorithms,
including the linear-time [[Recursive descent parser|recursive descent]],
the cubic-time [[GLR parser|generalized LR]],
the cubic-time [[CYK algorithm|Cocke-Kasami-Younger]],
as well as [[Leslie Valiant|Valiant's]] algorithm running as fast as matrix multiplication.

==Theoretical properties==
A number of theoretical properties of conjunctive grammars have been researched,
including the expressive power of grammars over a one-letter alphabet{{cn|date=June 2018}}
and {{clarify span|numerous [[undecidable problem]]s|reason=Both the main decidable and the main undecidable properties should be stated, as well as the practical relevance of both.|date=June 2018}}.
This work provided a basis
for the study [[language equation]]s of a more general form.

==Synchronized alternating pushdown automata==
Aizikowitz and Kaminski&lt;ref name="AizikowitzKaminski2011"&gt;{{cite journal|last1=Aizikowitz|first1=Tamar|last2=Kaminski|first2=Michael|title=LR(0) Conjunctive Grammars and Deterministic Synchronized Alternating Pushdown Automata|volume=6651|year=2011|pages=345–358|issn=0302-9743|doi=10.1007/978-3-642-20712-9_27}}&lt;/ref&gt; introduced a new class of [[pushdown automata]] (PDA) called [[pushdown automata#alternating pushdown automata|synchronized alternating pushdown automata]] (SAPDA). They proved it to be equivalent to conjunctive grammars in the same way as nondeterministic PDAs are equivalent to context-free grammars.

== References ==
{{reflist}}
* [[Alexander Okhotin]], ''Conjunctive grammars.'' [[Journal of Automata, Languages and Combinatorics]], 6:4 (2001), 519-535. [http://users.utu.fi/aleokh/papers/conjunctive.pdf (pdf)] 
* Alexander Okhotin, [https://dx.doi.org/10.1016/j.cosrev.2013.06.001 ''Conjunctive and Boolean grammars: The true general case of the context-free grammars.''] [[Computer Science Review]], 9 (2013), 27-59.
* Artur Jeż. ''Conjunctive grammars can generate non-regular unary languages.'' International Journal of Foundations of Computer Science 19(3): 597-615 (2008) [http://www.ii.uni.wroc.pl/cms/files/TR012007.pdf Technical report version (pdf)]{{dead link|date=August 2017 |bot=InternetArchiveBot |fix-attempted=yes }}

==External links==
*Artur Jeż. [http://www.ii.uni.wroc.pl/~aje/presentation/PresentationDLT.pdf Conjunctive grammars can generate non-regular unary languages]. Slides of talk held at the [[International Conference on Developments in Language Theory]] 2007.  
*[http://users.utu.fi/aleokh/conjunctive/ Alexander Okhotin's page on conjunctive grammars].
*Alexander Okhotin. [https://web.archive.org/web/20070929110312/http://www.tucs.fi/publications/insight.php?id=tOkhotin06a Nine open problems for conjunctive and Boolean grammars].
[[Category:Formal languages]]</text>
      <sha1>pclgjodpji3ncuc2pomcrxfjve0t3it</sha1>
    </revision>
  </page>
  <page>
    <title>Contact graph</title>
    <ns>0</ns>
    <id>51513238</id>
    <revision>
      <id>791879869</id>
      <parentid>766378254</parentid>
      <timestamp>2017-07-23T01:31:58Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>stub sort</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4570">In the [[mathematics|mathematical]] area of [[graph theory]], a '''contact graph''' or '''tangency graph''' is a [[Graph (discrete mathematics)|graph]] whose vertices are represented by geometric objects (e.g. [[curve]]s, [[line segment]]s, or [[polygon]]s), and whose edges correspond to two objects [[Tangent|touching]] (but not crossing) according to some specified notion.&lt;ref name="ELCG"&gt;{{cite arXiv |first1=Steven|last1=Chaplick|first2=Stephen |last2=G. Kobourov|first3=Torsten|last3=Ueckerdt|eprint=1303.1279 |title=Equilateral L-Contact Graphs |class= |date=2013-06-19 }} [https://www.cs.arizona.edu/~kobourov/elc.pdf online PDF]&lt;/ref&gt; It is similar to the notion of an [[intersection graph]] but differs from it in restricting the ways that the underlying objects are allowed to intersect each other.

The [[circle packing theorem]]&lt;ref&gt;{{citation
 | last = Koebe | first = Paul | author-link = Paul Koebe
 | journal = Ber. Sächs. Akad. Wiss. Leipzig, Math.-Phys. Kl.
 | pages = 141–164
 | title = Kontaktprobleme der Konformen Abbildung
 | volume = 88
 | year = 1936}}&lt;/ref&gt; states that every [[planar graph]] can be represented as a contact graph of circles. The contact graphs of [[unit circle]]s are called [[penny graph]]s.&lt;ref&gt;{{citation
 | last1 = Pisanski | first1 = Tomaž | author1-link = Tomaž Pisanski
 | last2 = Randić | first2 = Milan
 | editor-last = Gorini | editor-first = Catherine A.
 | contribution = Bridges between geometry and graph theory
 | contribution-url = http://preprinti.imfm.si/PDF/00595.pdf
 | mr = 1782654
 | pages = 174–194
 | publisher = Cambridge University Press
 | series = MAA Notes
 | title = Geometry at Work
 | volume = 53
 | year = 2000}}. See especially [https://books.google.com/books?id=Eb6uSLa2k6IC&amp;pg=PA176 p.&amp;nbsp;176].&lt;/ref&gt; Representations as contact graphs of [[triangle]]s,&lt;ref&gt;{{citation
 | last1 = de Fraysseix | first1 = Hubert
 | last2 = Ossona de Mendez | first2 = Patrice | author2-link = Patrice Ossona de Mendez
 | last3 = Rosenstiehl | first3 = Pierre | author3-link = Pierre Rosenstiehl
 | doi = 10.1017/S0963548300001139
 | issue = 2
 | journal = Combinatorics, Probability and Computing
 | mr = 1288442
 | pages = 233–246
 | title = On triangle contact graphs
 | volume = 3
 | year = 1994}}&lt;/ref&gt; [[rectangle]]s,&lt;ref&gt;{{citation
 | last1 = Buchsbaum | first1 = Adam L.
 | last2 = Gansner | first2 = Emden R.
 | last3 = Procopiuc | first3 = Cecilia M.
 | last4 = Venkatasubramanian | first4 = Suresh
 | arxiv = cs/0611107
 | doi = 10.1145/1328911.1328919
 | issue = 1
 | journal = ACM Transactions on Algorithms
 | mr = 2398588
 | page = Art. 8, 28
 | title = Rectangular layouts and contact graphs
 | volume = 4
 | year = 2008}}&lt;/ref&gt; [[square]]s,&lt;ref&gt;{{citation
 | last1 = Klawitter | first1 = Jonathan
 | last2 = Nöllenburg | first2 = Martin
 | last3 = Ueckerdt | first3 = Torsten
 | arxiv = 1509.00835
 | contribution = Combinatorial properties of triangle-free rectangle arrangements and the squarability problem
 | doi = 10.1007/978-3-319-27261-0_20
 | pages = 231–244
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = Graph Drawing and Network Visualization: 23rd International Symposium, GD 2015, Los Angeles, CA, USA, September 24-26, 2015, Revised Selected Papers
 | volume = 9411
 | year = 2015}}&lt;/ref&gt; [[line segment]]s,&lt;ref&gt;{{citation
 | last = Hliněný | first = Petr
 | doi = 10.1016/S0012-365X(00)00263-6
 | issue = 1-3
 | journal = Discrete Mathematics
 | mr = 1829839
 | pages = 95–106
 | title = Contact graphs of line segments are NP-complete
 | url = http://www.fi.muni.cz/~hlineny/papers/cont-seg.pdf
 | volume = 235
 | year = 2001}}&lt;/ref&gt; or [[Arc (geometry)|circular arcs]]&lt;ref&gt;{{citation
 | last1 = Alam | first1 = Md. Jawaherul
 | last2 = Eppstein | first2 = David | author2-link = David Eppstein
 | last3 = Kaufmann | first3 = Michael
 | last4 = Kobourov | first4 = Stephen G.
 | last5 = Pupyrev | first5 = Sergey
 | last6 = Schulz | first6 = André
 | last7 = Ueckerdt | first7 = Torsten
 | arxiv = 1501.00318
 | contribution = Contact graphs of circular arcs
 | doi = 10.1007/978-3-319-21840-3_1
 | pages = 1–13
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = Algorithms and Data Structures: 14th International Symposium, WADS 2015, Victoria, BC, Canada, August 5-7, 2015, Proceedings
 | volume = 9214
 | year = 2015}}.&lt;/ref&gt; have also been studied.

==References==
{{reflist}}

{{combin-stub}}

[[Category:Geometric graph theory]]
[[Category:Graph families]]
[[Category:Planar graphs]]</text>
      <sha1>qbcfhmg8he0x9b450vxp1upwy9iwzoe</sha1>
    </revision>
  </page>
  <page>
    <title>Covariant transformation</title>
    <ns>0</ns>
    <id>443235</id>
    <revision>
      <id>871714574</id>
      <parentid>849765059</parentid>
      <timestamp>2018-12-03T00:00:31Z</timestamp>
      <contributor>
        <username>RJFJR</username>
        <id>141808</id>
      </contributor>
      <comment>Added {{[[:Template:unreferenced|unreferenced]]}} tag (within {{[[Template:multiple issues|multiple issues]]}}) to article ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15092">{{Multiple issues|
{{refimprove|date=September 2012}}
{{cleanup|date=April 2012}}
{{unreferenced|date=December 2018}}
}}
{{see also|Covariance and contravariance of vectors}}
In [[physics]], a '''covariant transformation''' is a rule that specifies how certain entities, such as [[Vector (geometric)|vector]]s or [[tensor]]s, change under a [[change of basis]]. The transformation that describes the new [[Basis (linear algebra)|basis vectors]] as a linear combination of the old basis vectors is ''defined'' as a '''covariant transformation'''. Conventionally, indices identifying the basis vectors are placed as '''lower indices''' and so are all entities that transform in the same way. The inverse of a covariant transformation is a '''[[Covariance and contravariance of vectors|contravariant]] transformation'''. Whenever a vector should be ''invariant'' under a change of basis, that is to say it should represent the same geometrical or physical object having the same magnitude and direction as before, its ''components'' must transform according to the contravariant rule. Conventionally, indices identifying the components of a vector are placed as '''upper indices''' and so are all indices of entities that transform in the same way. The sum over pairwise matching indices of a product with the same lower and upper indices are [[Invariant (physics)|invariant]] under a transformation.

A vector itself is a geometrical quantity, in principle, independent (invariant) of the chosen basis. A vector '''v''' is given, say, in components ''v''&lt;sup&gt;''i''&lt;/sup&gt; on a chosen basis '''e'''&lt;sub&gt;''i''&lt;/sub&gt;. On another basis, say '''e'''′&lt;sub&gt;''j''&lt;/sub&gt;, the same vector '''v''' has different components ''v''′&lt;sup&gt;''j''&lt;/sup&gt; and
:&lt;math&gt; \mathbf{v} = \sum_i v^i {\mathbf e}_i  = \sum_j {v'\,}^j \mathbf{e}'_j.&lt;/math&gt;
As a vector, '''v''' should be invariant to the chosen coordinate system and independent of any chosen basis, i.e. its "real world" direction and magnitude should appear the same regardless of the basis vectors. If we perform a change of basis by transforming the vectors '''e'''&lt;sub&gt;''i''&lt;/sub&gt; into the basis vectors '''e'''&lt;sub&gt;''j''&lt;/sub&gt;, we must also ensure that the components ''v''&lt;sup&gt;''i''&lt;/sup&gt; transform into the new components ''v''&lt;sup&gt;''j''&lt;/sup&gt; to compensate.

The needed transformation of '''v''' is called the '''contravariant transformation''' rule.

&lt;div style="float:left; border:1px solid #aaa; padding:3px; margin-right:1em; text-align:left"&gt;
&lt;gallery widths=200px heights=200px&gt; 
  Image:Transformation2polar_basis_vectors.svg|A vector '''v''', and local tangent basis vectors {{nowrap|{'''e'''&lt;sub&gt;x&lt;/sub&gt;, '''e'''&lt;sub&gt;y&lt;/sub&gt;} }} and {{nowrap|{'''e'''&lt;sub&gt;r&lt;/sub&gt;, '''e'''&lt;sub&gt;φ&lt;/sub&gt;} }}.
  &lt;!-- Image:Transformation2polar.svg|Components (''v''&lt;sup&gt;x&lt;/sup&gt;,''v''&lt;sup&gt;y&lt;/sup&gt;) and (''v''&lt;sup&gt;r&lt;/sup&gt;,''v''&lt;sup&gt;φ&lt;/sup&gt;) of '''v'''. --&gt;
  Image:Transformation2polar contravariant vector.svg|Coordinate representations of '''v'''.
&lt;/gallery&gt;
&lt;/div&gt;
In the shown example, a vector &lt;math&gt;\mathbf{v} = \sum_{i \in \{x,y\} } v^i {\mathbf e}_i  = \sum_{j \in \{r,\phi\}} {v'\,}^j \mathbf{e}'_j.&lt;/math&gt; is described by two different coordinate systems: a rectangular coordinate system (the black grid), and a radial coordinate system (the red grid).  Basis vectors have been chosen for both coordinate systems: '''e'''&lt;sub&gt;x&lt;/sub&gt; and '''e'''&lt;sub&gt;y&lt;/sub&gt; for the rectangular coordinate system, and '''e'''&lt;sub&gt;r&lt;/sub&gt; and '''e'''&lt;sub&gt;φ&lt;/sub&gt; for the radial coordinate system. The radial basis vectors '''e'''&lt;sub&gt;r&lt;/sub&gt; and '''e'''&lt;sub&gt;φ&lt;/sub&gt; appear rotated anticlockwise with respect to the rectangular basis vectors '''e'''&lt;sub&gt;x&lt;/sub&gt; and '''e'''&lt;sub&gt;y&lt;/sub&gt;. The '''covariant transformation,''' performed to the basis vectors, is thus an anticlockwise rotation, rotating from the first basis vectors to the second basis vectors.

The coordinates of '''v''' must be transformed into the new coordinate system, but the vector '''v''' itself, as a mathematical object, remains independent of the basis chosen, appearing to point in the same direction and with the same magnitude, invariant to the change of coordinates. The contravariant transformation ensures this, by compensating for the rotation between the different bases. If we view '''v''' from the context of the radial coordinate system, it appears to be rotated more clockwise from the basis vectors '''e'''&lt;sub&gt;r&lt;/sub&gt; and '''e'''&lt;sub&gt;φ&lt;/sub&gt;. compared to how it appeared relative to the rectangular basis vectors '''e'''&lt;sub&gt;x&lt;/sub&gt; and '''e'''&lt;sub&gt;y&lt;/sub&gt;. Thus, the needed contravariant transformation to '''v''' in this example is a clockwise rotation.{{clear}}

==Examples of covariant transformation==
===The derivative of a function transforms covariantly===
The explicit form of a covariant transformation is best introduced with the transformation properties of the derivative of a function.  Consider a scalar function ''f'' (like the temperature at a location in a space) defined on a set of points ''p'', identifiable in a given coordinate system &lt;math&gt;x^i,\; i=0,1,\dots&lt;/math&gt; (such a collection is called a [[manifold]]).  If we adopt a new coordinates system &lt;math&gt;{x'}^j, j=0,1,\dots&lt;/math&gt; then for each ''i'', the original coordinate &lt;math&gt;{x}^i&lt;/math&gt; can be expressed as a function of the new coordinates, so &lt;math&gt;x^i \left({x'}^j\right), j=0,1,\dots&lt;/math&gt; One can express the derivative of ''f'' in new coordinates in terms of the old coordinates, using the [[chain rule]] of the derivative, as
:&lt;math&gt;
  \frac{\partial f}{\partial {x}^i} =
    \frac{\partial f}{\partial {x'}^j} \;
    \frac{\partial {x'}^j}{\partial {x}^i}
&lt;/math&gt;

This is the explicit form of the '''covariant transformation''' rule. The notation of a normal derivative with respect to the coordinates sometimes uses a comma, as follows
:&lt;math&gt;f_{,i} \ \stackrel{\mathrm{def}}{=}\  \frac{\partial f}{\partial x^i}&lt;/math&gt;
where the index ''i'' is placed as a lower index, because of the covariant transformation.

===Basis vectors transform covariantly===
A vector can be expressed in terms of basis vectors. For a certain coordinate system, we can choose the vectors tangent to the coordinate grid. This basis is called the coordinate basis.

To illustrate the transformation properties, consider again the set of points ''p'', identifiable in a given coordinate system &lt;math&gt;x^i&lt;/math&gt;  where &lt;math&gt;i = 0, 1, \dots&lt;/math&gt; ([[manifold]]). A scalar function ''f'', that assigns a real number to every point ''p'' in this space, is a function of the coordinates &lt;math&gt;f\;\left(x^0, x^1, \dots\right)&lt;/math&gt;. A curve is a one-parameter collection of points ''c'', say with curve parameter λ, ''c''(λ). A tangent vector '''v''' to the curve is the derivative &lt;math&gt;dc/d\lambda&lt;/math&gt; along the curve with the derivative taken at the point ''p'' under consideration. Note that we can see the '''[[tangent vector]] v''' as an '''operator''' (the '''[[directional derivative]]''') which can be applied to a function
:&lt;math&gt;\mathbf{v}[f] \ \stackrel{\mathrm{def}}{=}\  \frac{df}{d\lambda} = \frac{d\;\;}{d\lambda} f(c(\lambda))&lt;/math&gt;

The parallel between the tangent vector and the operator can also be worked out in coordinates
:&lt;math&gt;\mathbf{v}[f] = \frac{dx^i}{d\lambda} \frac{\partial f}{\partial x^i}&lt;/math&gt;

or in terms of operators &lt;math&gt;\partial/\partial x^i&lt;/math&gt; 
:&lt;math&gt;\mathbf{v} = \frac{dx^i}{d\lambda} \frac{\partial \;\;}{\partial x^i} = \frac{dx^i}{d\lambda} \mathbf{e}_i&lt;/math&gt;
where we have written &lt;math&gt;\mathbf{e}_i = \partial/\partial x^i&lt;/math&gt;, the tangent vectors to the curves which are simply the coordinate grid itself.

If we adopt a new coordinates system &lt;math&gt;{x'}^i, \;i=0,1,\dots&lt;/math&gt; then for each ''i'', the old coordinate &lt;math&gt;{x^i}&lt;/math&gt; can be expressed as function of the new system, so &lt;math&gt;x^i\left({x'}^j\right), j=0,1,\dots&lt;/math&gt;
Let &lt;math&gt;\mathbf{e}'_i = {\partial}/{\partial {x'}^i}&lt;/math&gt; be the basis, tangent vectors in this new coordinates system. We can express &lt;math&gt;\mathbf{e}_i&lt;/math&gt; in the new system by applying the [[chain rule]] on ''x''. As a function of coordinates we find the following transformation
:&lt;math&gt;
  \mathbf{e}'_i = \frac{\partial}{\partial {x'}^i} =
                  \frac{\partial x^j}{\partial {x'}^i}
                    \frac{\partial}{\partial x^j} =
                  \frac{\partial x^j}{\partial {x'}^i} 
                    \mathbf{e}_j
&lt;/math&gt;
which indeed is the same as the covariant transformation for the derivative of a function.

==Contravariant transformation==

The ''components'' of a (tangent) vector transform in a different way, called contravariant transformation.  Consider a tangent vector '''v''' and call its components &lt;math&gt;v^i&lt;/math&gt; on a basis &lt;math&gt;\mathbf{e}_i&lt;/math&gt;.  On another basis &lt;math&gt;\mathbf{e}'_i&lt;/math&gt; we call the components &lt;math&gt;{v'}^i &lt;/math&gt;, so
:&lt;math&gt;\mathbf{v} =  v^i \mathbf{e}_i  = {v'}^i  \mathbf{e}'_i&lt;/math&gt;
in which 
:&lt;math&gt;
     v^i = \frac{dx^i}{d\lambda} \;\mbox{ and }\;
  {v'}^i = \frac{d{x'}^i}{d\lambda}    
&lt;/math&gt;

If we express the new components in terms of the old ones, then
:&lt;math&gt; 
  {v'}^i = \frac{d{x'}^i}{d\lambda\;\;} =
             \frac{\partial {x'}^i}{\partial x^j}
           \frac{dx^j}{d\lambda} =
             \frac{\partial {x'}^i}{\partial x^j} {v}^j
&lt;/math&gt;
This is the explicit form of a transformation called the '''contravariant transformation''' and we note that it is different and just the inverse of the covariant rule. In order to distinguish them from the covariant (tangent) vectors, the index is placed on top.

===Differential forms transform contravariantly===

An example of a contravariant transformation is given by a [[differential form]] ''df''.  For ''f'' as a function of coordinates &lt;math&gt;x^i&lt;/math&gt;, ''df'' can be expressed in terms of &lt;math&gt; dx^i&lt;/math&gt;.  The differentials ''dx''  transform according to the contravariant rule since
:&lt;math&gt;d{x'}^i = \frac{\partial {x'}^i}{\partial {x}^j} {dx}^j&lt;/math&gt;

==Dual properties==

Entities that transform covariantly (like basis vectors) and the ones that transform contravariantly (like components of a vector and differential forms) are "almost the same" and yet they are different. They have "dual" properties.
What is behind this, is mathematically known as the [[dual space]] that always goes together with a given linear [[vector space]].

Take any vector space T. A function ''f'' on T is called linear if, for any vectors '''v''', '''w''' and scalar α:
:&lt;math&gt;\begin{align}
  f(\mathbf{v} + \mathbf{w}) &amp;= f(\mathbf{v}) + f(\mathbf{w}) \\
        f(\alpha \mathbf{v}) &amp;= \alpha f(\mathbf{v})
\end{align}&lt;/math&gt;

A simple example is the function which assigns a vector the value of one of its components (called a ''projection function''). It has a vector as argument and assigns a real number, the value of a component.

All such ''scalar-valued'' linear functions together form a vector space, called the '''dual space''' of T. The sum ''f+g'' is again a linear function for linear ''f''  and ''g'', and the same holds for scalar multiplication α''f''.

Given a basis &lt;math&gt;\mathbf{e}_i&lt;/math&gt; for T, we can define a basis, called the '''dual basis''' for the dual space in a natural way by taking the set of linear functions mentioned above: the projection functions.  Each projection function (indexed by ω) produces the number 1 when applied to one of the basis vectors &lt;math&gt;\mathbf{e}_i&lt;/math&gt;.  For example, &lt;math&gt;\omega^0&lt;/math&gt; gives a 1 on &lt;math&gt;\mathbf{e}_0&lt;/math&gt; and zero elsewhere.  Applying this linear function &lt;math&gt;{\omega}^0&lt;/math&gt; to a vector &lt;math&gt;\mathbf{v} =v^i \mathbf{e}_i&lt;/math&gt;, gives (using its linearity)
:&lt;math&gt;
  \omega^0(\mathbf{v}) = \omega^0(v^i \mathbf{e}_i) = 
    v^i \omega^0(\mathbf{e}_i) = v^0
&lt;/math&gt;
so just the value of the first coordinate. For this reason it is called the '''projection function'''.

There are as many dual basis vectors &lt;math&gt;\omega^i&lt;/math&gt; as there are basis vectors &lt;math&gt;\mathbf{e}_i&lt;/math&gt;, so the dual space has the same dimension as the linear space itself. It is "almost the same space", except that the elements of the dual space (called '''dual vectors''') transform covariantly and the elements of the tangent vector space transform contravariantly.

Sometimes an extra notation is introduced where the real value of a linear function σ on a tangent vector '''u''' is given as
:&lt;math&gt;\sigma [\mathbf{u}] :=  \langle \sigma, \mathbf{u}\rangle&lt;/math&gt;
where &lt;math&gt;\langle\sigma, \mathbf{u}\rangle&lt;/math&gt; is a real number. This notation emphasizes the bilinear character of the form. It is linear in σ since that is a linear function and it is linear in '''u''' since that is an element of a vector space.

==Co- and contravariant tensor components==
===Without coordinates===
A [[tensor]] of [[type of a tensor|type (''r'', ''s'')]] may be defined as a real-valued multilinear function of ''r'' dual vectors and ''s'' vectors. Since vectors and dual vectors may be defined without dependence on a coordinate system, a tensor defined in this way is independent of the choice of a coordinate system.

The notation of a tensor is
:&lt;math&gt;\begin{align}
            &amp;T\left(\sigma, \ldots ,\rho, \mathbf{u}, \ldots, \mathbf{v}\right) \\
  \equiv {} &amp;{T^{\sigma \ldots \rho}}_{\mathbf{u} \ldots \mathbf{v}}
\end{align}&lt;/math&gt;
for dual vectors (differential forms) ''ρ'', ''σ'' and tangent vectors &lt;math&gt;\mathbf{u}, \mathbf{v}&lt;/math&gt;. In the second notation the distinction between vectors and  differential forms is more obvious.

===With coordinates===

Because a tensor depends linearly on its arguments, it is completely determined if one knows the values on a basis &lt;math&gt;\omega^i \ldots \omega^j&lt;/math&gt; and &lt;math&gt;\mathbf{e}_k \ldots \mathbf{e}_l&lt;/math&gt;
:&lt;math&gt;
  T(\omega^i,\ldots,\omega^j, \mathbf{e}_k \ldots \mathbf{e}_l) =
    {T^{i\ldots j}}_{k\ldots l}
&lt;/math&gt;
The numbers &lt;math&gt;{T^{i\ldots j}}_{k\ldots l}&lt;/math&gt; are called the '''components of the tensor on the chosen basis'''.

If we choose another basis (which are a linear combination of the original basis), we can use the linear properties of the tensor and we will find that the tensor components in the upper indices transform as dual vectors (so contravariant), whereas the lower indices will transform as the basis of tangent vectors and are thus covariant. For a tensor of rank 2, we can verify that
:&lt;math&gt; 
     {A'}_{i j} = \frac{\partial x^l}{\partial {x'}^i}
                       \frac{\partial x^m}{\partial {x'}^j} A_{l m}
&lt;/math&gt; '''covariant tensor'''
:&lt;math&gt;  
     {A'\,}^{i j} = \frac{\partial {x'}^i}{\partial x^l}
                 \frac{\partial {x'}^j}{\partial x^m} A^{l m}
&lt;/math&gt; '''contravariant tensor'''

For a mixed co- and contravariant tensor of rank 2
:&lt;math&gt;
     {A'\,}^i{}_j= \frac {\partial {x'}^i} {\partial x^l}
                 \frac {\partial x^m}      {\partial {x'}^j} A^l{}_m
&lt;/math&gt; '''mixed co- and contravariant tensor'''

==See also==
* [[General covariance]]
* [[Lorentz covariance]]

{{tensors}}

[[Category:Tensors]]
[[Category:Differential geometry]]</text>
      <sha1>d5pgvz88khojhy45pin9261vultyns8</sha1>
    </revision>
  </page>
  <page>
    <title>Cycle (graph theory)</title>
    <ns>0</ns>
    <id>168609</id>
    <revision>
      <id>868885719</id>
      <parentid>848520654</parentid>
      <timestamp>2018-11-15T02:19:34Z</timestamp>
      <contributor>
        <ip>43.231.70.252</ip>
      </contributor>
      <comment>Added a needed space between last sentence and next.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11936">[[File:Graph cycle.svg|thumb| A graph with edges colored to illustrate path H-A-B (green), closed path or walk with a repeated vertex B-D-E-F-D-C-B (blue) and a cycle with no repeated edge or vertex H-D-G-H (red)]]
In [[graph theory]], a '''cycle''' is a path of edges and vertices wherein a [[Vertex (graph theory)|vertex]] is reachable from itself.
There are several different types of cycles, principally a '''closed walk''' and a '''simple cycle'''; also, e.g., an element of the cycle space of the graph.

==Definitions==

A '''closed walk''' consists of a sequence of [[vertex (graph theory)|vertices]] starting and ending at the same vertex, with each two consecutive vertices in the sequence adjacent to each other in the graph. In a directed graph, each edge must be traversed by the walk consistently with its direction: the edge must be oriented from the earlier of two consecutive vertices to the later of the two vertices in the sequence. The choice of starting vertex is not important: traversing the same cyclic sequence of edges from different starting vertices produces the same closed walk.

A '''simple cycle''' may be defined either as a closed walk with no repetitions of vertices and edges allowed, other than the repetition of the starting and ending vertex, or as the set of edges in such a walk. The two definitions are equivalent in directed graphs, where simple cycles are also called '''directed cycles''': the cyclic sequence of vertices and edges in a walk is completely determined by the set of edges that it uses. In undirected graphs the set of edges of a cycle can be traversed by a walk in either of two directions, giving two possible directed cycles for every undirected cycle. (For closed walks more generally, in directed or undirected graphs, the multiset of edges does not unambiguously determine the vertex ordering.) A '''circuit''' can be a closed walk allowing repetitions of vertices but not edges; however, it can also be a simple cycle, so explicit definition is recommended when it is used.&lt;ref name=schaum /&gt;

In order to maintain a consistent terminology, for the rest of this article, "cycle" means a simple cycle, except where otherwise stated.

==Chordless cycles==
[[File:Graph with Chordless and Chorded Cycles.svg|thumb|right|In this graph the green cycle (A-B-C-D-E-F-A) is chordless whereas the red cycle (G-H-I-J-K-L-G) is not. This is because the edge K-I is a chord.]]

A [[chordless cycle]] in a graph, also called a hole or an induced cycle, is a cycle such that no two vertices of the cycle are connected by an edge that does not itself belong to the cycle. An antihole is the [[complement graph|complement]] of a graph hole. Chordless cycles may be used to characterize [[perfect graph]]s: by the [[strong perfect graph theorem]], a graph is perfect if and only if none of its holes or antiholes have an odd number of vertices that is greater than three. A [[chordal graph]], a special type of perfect graph, has no holes of any size greater than three.

The [[Girth (graph theory)|girth]] of a graph is the length of its shortest cycle; this cycle is necessarily chordless. [[Cage (graph theory)|Cages]] are defined as the smallest regular graphs with given combinations of degree and girth.

A [[peripheral cycle]] is a cycle in a graph with the property that every two edges not on the cycle can be connected by a path whose interior vertices avoid the cycle. In a graph that is not formed by adding one edge to a cycle, a peripheral cycle must be an induced cycle.

==Cycle space==
The term ''cycle'' may also refer to an element of the [[cycle space]] of a graph. There are many cycle spaces, one for each coefficient field or ring.  The most common is the ''binary cycle space'' (usually called simply the ''cycle space''), which consists of the edge sets that have even degree at every vertex; it forms a [[vector space]] over the two-element [[finite field|field]].  By [[Veblen's theorem]], every element of the cycle space may be formed as an edge-disjoint union of simple cycles. A [[cycle basis]] of the graph is a set of simple cycles that forms a [[basis (linear algebra)|basis]] of the cycle space.&lt;ref name="gy"&gt;{{citation|title=Graph Theory and Its Applications|edition=2nd|first1=Jonathan L.|last1=Gross|first2=Jay|last2=Yellen|publisher=CRC Press|year=2005|isbn=9781584885054|chapter=4.6 Graphs and Vector Spaces|pages=197–207|url=https://books.google.com/books?id=-7Q_POGh-2cC&amp;pg=PA197}}.&lt;/ref&gt;

Using ideas from [[algebraic topology]], the binary cycle space generalizes to vector spaces or [[module (mathematics)|modules]] over other [[ring (mathematics)|rings]] such as the integers, rational or real numbers, etc.&lt;ref name="diestel"&gt;{{citation|title=Graph Theory|volume=173|series=Graduate Texts in Mathematics|first=Reinhard|last=Diestel|publisher=Springer|year=2012|chapter=1.9 Some linear algebra|pages=23–28|url=https://books.google.com/books?id=eZi8AAAAQBAJ&amp;pg=PA23}}.&lt;/ref&gt;

==Cycle detection==
The existence of a cycle in directed and undirected graphs can be determined by whether [[depth-first search]] (DFS) finds an edge that points to an ancestor of the current vertex (it contains a [[Depth-first search#Output of a depth-first search|back edge]]).&lt;ref&gt;{{cite book|last=Tucker|first=Alan|authorlink=Alan Tucker|title=Applied Combinatorics |year=2006|publisher=John Wiley &amp; sons|location=Hoboken|isbn=978-0-471-73507-6|edition=5th|page=49|chapter=Chapter 2: Covering Circuits and Graph Colorings}}&lt;/ref&gt; In an undirected graph, finding any already visited vertex will indicate a back edge.
All the back edges which DFS skips over are part of cycles.&lt;ref name="sedgewick"&gt;{{citation
| first=Robert | last=Sedgewick | author-link=Robert Sedgewick (computer scientist)
| title=Algorithms | chapter=Graph algorithms
| year=1983
| publisher=Addison–Wesley | isbn=0-201-06672-6
}}&lt;/ref&gt; In the case of undirected graphs, only ''O''(''n'') time is required to find a cycle in an ''n''-vertex graph, since at most ''n''&amp;nbsp;−&amp;nbsp;1 edges can be tree edges.

Many [[topological sorting]] algorithms will detect cycles too, since those are obstacles for topological order to exist.  Also, if a directed graph has been divided into [[strongly connected component]]s, cycles only exist within the components and not between them, since cycles are strongly connected.&lt;ref name="sedgewick" /&gt;

For directed graphs, [[Rocha–Thatte cycle detection algorithm|Rocha–Thatte Algorithm]]&lt;ref name="rcor2015"&gt;{{cite journal|title=Distributed cycle detection in large-scale sparse graphs|first1=Rodrigo Caetano|last1=Rocha|first2=Bhalchandra|last2=Thatte|year=2015|doi=10.13140/RG.2.1.1233.8640|publisher=Simpósio Brasileiro de Pesquisa Operacional (SBPO)}}&lt;/ref&gt; is a distributed cycle detection algorithm. Distributed cycle detection algorithms are useful for processing large-scale graphs using a distributed graph processing system on a [[computer cluster]] (or supercomputer).

Applications of cycle detection include the use of [[wait-for graph]]s to detect [[deadlock]]s in concurrent systems.&lt;ref&gt;{{cite book
  | last = Silberschatz
  | first = Abraham
  |author2=Peter Galvin |author3=Greg Gagne
   | title = Operating System Concepts
  | publisher = John Wiley &amp; Sons, INC.
  | year = 2003
  | pages = 260
  | isbn = 0-471-25060-0}}&lt;/ref&gt;

==Covering graphs by cycles==
In his 1736 paper on the [[Seven Bridges of Königsberg]], widely considered to be the birth of graph theory, [[Leonhard Euler]] proved that, for a finite undirected graph to have a closed walk that visits each edge exactly once, it is necessary and sufficient that it be connected except for isolated vertices (that is, all edges are contained in one component) and have even degree at each vertex. The corresponding characterization for the existence of a closed walk visiting each edge exactly once in a directed graph is that the graph be [[strongly connected]] and have equal numbers of incoming and outgoing edges at each vertex. In either case, the resulting walk is known as an [[Euler cycle]] or Euler tour. If a finite undirected graph has even degree at each of its vertices, regardless of whether it is connected, then it is possible to find a set of simple cycles that together cover each edge exactly once: this is [[Veblen's theorem]].&lt;ref&gt;{{Citation | last1=Veblen | first1=Oswald | author1-link=Oswald Veblen | title=An Application of Modular Equations in Analysis Situs | jstor=1967604 | series=Second Series | year=1912 | journal=[[Annals of Mathematics]] | volume=14 | issue=1 | pages=86–94 | doi = 10.2307/1967604 }}.&lt;/ref&gt; When a connected graph does not meet the conditions of Euler's theorem, a closed walk of minimum length covering each edge at least once can nevertheless be found in [[polynomial time]] by solving the [[route inspection problem]].

The problem of finding a single simple cycle that covers each vertex exactly once, rather than covering the edges, is much harder. Such a cycle is known as a [[Hamiltonian cycle]], and determining whether it exists is [[NP-complete]].&lt;ref&gt;{{citation
 | author = [[Richard M. Karp]]
 | chapter = Reducibility Among Combinatorial Problems
 | chapter-url = http://cgi.di.uoa.gr/~sgk/teaching/grad/handouts/karp.pdf
 | title = Complexity of Computer Computations
 | editor = R. E. Miller and J. W. Thatcher
 | publisher = New York: Plenum
 | pages = 85–103
 | year = 1972}}.&lt;/ref&gt; Much research has been published concerning classes of graphs that can be guaranteed to contain Hamiltonian cycles; one example is [[Ore's theorem]] that a Hamiltonian cycle can always be found in a graph for which every non-adjacent pair of vertices have degrees summing to at least the total number of vertices in the graph.&lt;ref&gt;{{citation|first=Ø.|last=Ore|authorlink=Øystein Ore|title=Note on Hamilton circuits|journal=[[American Mathematical Monthly]]|volume=67|year=1960|page=55|issue=1|jstor=2308928|doi=10.2307/2308928}}.&lt;/ref&gt;

The [[cycle double cover conjecture]] states that, for every [[bridgeless graph]], there exists a [[multiset]] of simple cycles that covers each edge of the graph exactly twice. Proving that this is true (or finding a counterexample) remains an open problem.&lt;ref&gt;{{citation
 | last = Jaeger | first = F.
 | contribution = A survey of the cycle double cover conjecture
 | doi = 10.1016/S0304-0208(08)72993-1
 | pages = 1–12
 | series = North-Holland Mathematics Studies
 | title = Annals of Discrete Mathematics 27 – Cycles in Graphs
 | volume = 27
 | year = 1985}}..&lt;/ref&gt;

==Graph classes defined by cycles==
Several important classes of graphs can be defined by or characterized by their cycles. These include:
* [[Bipartite graph]], a graph without odd cycles (cycles with an odd number of vertices).
* [[Cactus graph]], a graph in which every nontrivial biconnected component is a cycle
* [[Cycle graph]], a graph that consists of a single cycle.
* [[Chordal graph]], a graph in which every induced cycle is a triangle
* [[Directed acyclic graph]], a directed graph with no cycles
* [[Line perfect graph]], a graph in which every odd cycle is a triangle
* [[Perfect graph]], a graph with no induced cycles or their complements of odd length greater than three
* [[Pseudoforest]], a graph in which each connected component has at most one cycle
* [[Strangulated graph]], a graph in which every peripheral cycle is a triangle
* [[Strongly connected graph]], a directed graph in which every edge is part of a cycle
* [[Triangle-free graph]], a graph without three-vertex cycles

==See also==
*[[Cycle space]]
*[[Cycle basis]]
*[[Cycle detection]] in a sequence of iterated function values

==References==
{{Reflist|refs=
&lt;ref name=schaum&gt;{{cite book|last=Balakrishnan|first=V.K.|title=Schaum's outline of theory and problems of graph theory|year=2005|publisher=McGraw–Hill|isbn=978-0070054899|edition=[Nachdr.].}}&lt;/ref&gt; 
}}

[[Category:Graph theory objects]]</text>
      <sha1>3xxawboxhx46m1cdxe0izbcmgr1gvhn</sha1>
    </revision>
  </page>
  <page>
    <title>Domain coloring</title>
    <ns>0</ns>
    <id>5280990</id>
    <revision>
      <id>860621597</id>
      <parentid>860620507</parentid>
      <timestamp>2018-09-21T23:08:34Z</timestamp>
      <contributor>
        <username>Mark viking</username>
        <id>17698045</id>
      </contributor>
      <comment>Adding local [[Wikipedia:Short description|short description]]: "Technique for visualizing complex functions" ([[User:Galobtter/Shortdesc helper|Shortdesc helper]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8486">{{short description|Technique for visualizing complex functions}}
[[File:Domain coloring x2-1 x-2-i x-2-i d x2+2+2i.xcf|270px|right|thumb|Domain coloring plot of the function
{{math|''f''(''x'') {{=}} {{sfrac|(''x''&lt;sup&gt;2&lt;/sup&gt; − 1)(''x'' − 2 − ''i'')&lt;sup&gt;2&lt;/sup&gt;|''x''&lt;sup&gt;2&lt;/sup&gt; + 2 + 2''i''}}}}, using the structured color function described below.]]

In mathematics, '''domain coloring''' or a '''color wheel graph''' is a technique for visualizing [[complex function]]s, which assigns a [[color]] to each point of the [[complex plane]].

== Motivation: four dimensions ==
A [[graph of a function|graph]] of a [[real function]] can be drawn in two dimensions, such as x and y. By contrast, a graph of a complex function (more precisely, a [[complex-valued function]] of one [[complex variable]] {{math|''f'' : ℂ → ℂ}}) requires the visualization of four dimensions. One way to achieve that is with a [[Riemann surface]], another by domain coloring.

== Method ==
[[File:Complex_coloring.jpg|thumbnail|thumb|200px|HL plot of {{math|''z''}}, as per the simple color function example described in the text]]
[[File:Complex x hoch 3.jpg|thumb|200px|Graph of complex function {{math|''z''&lt;sup&gt;3&lt;/sup&gt;&amp;nbsp;−&amp;nbsp;1}} using the same color function, showing the three zeros, as well as the negative real numbers as cyan rays starting at the zeros. ]]

For easy visibility, complex values are represented with colors. This assignment is called a "color function". There are many different color functions used. A common practice is to represent the [[Argument (complex analysis)|complex argument]] (also known as "phase" or "angle") with a [[hue]] following the [[color wheel]], and the [[magnitude (mathematics)|magnitude]] by other means, such as [[brightness]] or [[Colorfulness#Saturation|saturation]]. 

=== Simple color function ===
The following example colors the [[Origin (mathematics)|origin]] in black, {{math|1}} in [[red]], {{math|−1}} in [[cyan]], and a point at infinity in white:
::&lt;math&gt;\begin{cases}
H &amp; = \arg z, \\
L &amp; = \left(1-a^\left| z \right|\right) \times 100\%, \\
S &amp; = 100\%.
\end{cases}&lt;/math&gt;
where &lt;math&gt; 0&lt;a&lt;1 &lt;/math&gt;.

More precisely, it uses the [[HLS color space|HLS]] (hue, lightness, saturation) color model. Saturation is always set at the maximum of 100%. Vivid colors of the rainbow are rotating in a continuous way on the complex unit circle, so the sixth [[Root of unity|roots of unity]] (starting with 1) are: red, yellow, green, cyan, blue, and magenta. Magnitude is coded by intensity via a [[Monotonic function|strictly monotonic]] [[Continuous function|continuous]] function. 

Since the HSL color space is not perceptually uniform, one can see streaks of perceived brightness at yellow, cyan, and magenta (even though their absolute values are the same as red, green, and blue) and a halo around {{math|1=''L'' = {{sfrac|1|2}}}}. Use of the [[Lab color space]] corrects this, making the images more accurate, but also makes them more drab/pastel.
{{clear}}

=== A structured color function ===
:[[File:Domain coloring.png|450px|right]]
The magnitude of a real number can range from {{math|0}} to {{math|∞}}, a much wider range than that of the argument. Therefore, in a strictly monotonic continuous function that stretches the whole range compromises the resolution of smaller changes in magnitude. This can be achieved with a discontinuous color function, such as the following, which shows a repeating pattern for the magnitude. 

In addition, this color function shows white rays for arguments {{math|0}}, {{math|π/6}}, {{math|π/3}}, {{math|π/2}}, {{math|2π/3}}, {{math|5π/6}}, {{math|π}}, {{math|7π/6}}, {{math|4π/3}}, {{math|3π/2}}, {{math|5π/3}}, {{math|11π/6}} and a gray grid for equal real and imaginary values. A similar color function has been used for the graph on top of the article.

== History ==
The method was probably first used in publication in the late 1980s by [[Larry Crone]] and [[Hans Lundmark]].&lt;ref&gt;{{cite book
 | author = Elias Wegert
 | title = Visual Complex Functions: An Introduction with Phase Portraits
 | publisher = Springer Basel
 | date = 2012
 | page = 29
 | isbn = 9783034801799
 | url = https://www.springer.com/us/book/9783034801799
 | accessdate = 6 January 2016}}
&lt;/ref&gt;

The term "domain coloring" was coined by Frank Farris, possibly around 1998.&lt;ref&gt;Frank A. Farris, [http://www.maa.org/pubs/amm_complements/complex.html Visualizing complex-valued functions in the plane]&lt;/ref&gt;&lt;ref name="Ludmark1"&gt;{{Cite web |url=http://www.mai.liu.se/~halun/complex/domain_coloring-unicode.html |title=Visualizing complex analytic functions using domain coloring |accessdate=2006-05-25 |year=2004 |author=Hans Lundmark |deadurl=yes |archiveurl=https://web.archive.org/web/20060502154939/http://www.mai.liu.se/~halun/complex/domain_coloring-unicode.html |archivedate=2006-05-02 |df= }} Ludmark refers to Farris' coining the term "domain coloring" in this 2004 article.&lt;/ref&gt; There were many earlier uses of color to visualize complex functions, typically mapping [[complex argument|argument]] ([[phase (waves)|phase]]) to hue.&lt;ref&gt;
{{cite journal
 | journal = Pixel: the magazine of scientific visualization
 | title = A Color Gallery of Complex Functions
 | author = David A. Rabenhorst
 | volume = 1
 | issue = 4
 | publisher = Pixel Communications
 | pages = 42 et seq
 | issn = 
 | date = 1990
 | url = https://books.google.com/books?id=DRUSAQAAMAAJ&amp;q=%22phase%22+%22hue%22+complex-functions&amp;dq=%22phase%22+%22hue%22+complex-functions&amp;hl=en&amp;sa=X&amp;ei=9iCFT4SRL8WdiAKb5KT1BA&amp;ved=0CDMQ6AEwAA
 }}&lt;/ref&gt; The technique of using continuous color to map points from domain to codomain or image plane was used in 1999 by George Abdo and Paul Godfrey&lt;ref name="Abdo1"&gt;{{Cite web|url=http://my.fit.edu/~gabdo/|title=Plotting functions of a complex variable: Table of Conformal Mappings Using Continuous Coloring|accessdate=2008-05-17|year=1999|author=George Abdo &amp; Paul Godfrey}}&lt;/ref&gt; and colored grids were used in graphics by [[Douglas N. Arnold|Doug Arnold]] that he dates to 1997.&lt;ref name="Arnold1"&gt;{{Cite web|url=http://www.ima.umn.edu/~arnold/complex.html|title=Graphics for complex analysis|accessdate=2008-05-17|year=2008|author=Douglas N. Arnold|authorlink=Douglas N. Arnold}}&lt;/ref&gt;

== Limitations ==
People who suffer [[color blindness]] may have trouble interpreting such graphs.

==References==
&lt;references /&gt;

==External links==
* [https://web.archive.org/web/20120511021419/http://w.american.edu/cas/mathstat/lcrone/ComplexPlot.html Color Graphs of Complex Functions]
* [http://www.maa.org/pubs/amm_complements/complex.html Visualizing complex-valued functions in the plane.]
*[http://www.cs.berkeley.edu/~flab/complex/gallery.html Gallery of Complex Functions]
*[http://alessandrorosa.altervista.org/pages/downloads.php Complex Mapper] by Alessandro Rosa
*[http://www.jedsoft.org/fun/complex/ John Davis software] − [[S-Lang (programming language)|S-Lang]] script for Domain Coloring
*[https://web.archive.org/web/20110725232043/http://devrand.org/show_item.html?item=72&amp;page=Project Open source C and Python domain coloring software]
*[http://www.hansfbaier.de/wordpress/computers-and-mathematics/ Enhanced 3D Domain coloring]
*[http://www.codeproject.com/Articles/492355/Domain-Coloring-Method-on-GPU Domain Coloring Method on GPU]
*[http://complexanalysis.sourceforge.net Java domain coloring software (In development)]
*[[MATLAB]] routines [http://www.mathworks.com/matlabcentral/fileexchange/29028]
* [https://archive.is/20121210065156/http://www.analysis.uni-hannover.de/~gruber/Computing/Conformal/ Python script for GIMP by Michael J. Gruber]
*[[Matplotlib]] and [[MayaVi]] implementation of domain coloring by E. Petrisor [http://nbviewer.ipython.org/github/empet/Math/blob/master/DomainColoring.ipynb]
* [[MATLAB]] [http://www.mathworks.com/matlabcentral/fileexchange/44375 routines with user interface and various color schemes]
* [[MATLAB]] [http://www.mathworks.com/matlabcentral/fileexchange/45464 routines for 3D-visualization of complex functions]
* [https://web.archive.org/web/20070101081320/http://wmi.math.u-szeged.hu/~kovzol/rtzme/doku.php?id=color_wheel_method Color wheel method]
* [http://rtzme.sf.net Real-Time Zooming Math Engine]
* [https://sourceforge.net/projects/fractalzoomer/ Fractal Zoomer : Software that utilizes domain coloring]

{{DEFAULTSORT:Domain Coloring}}
[[Category:Complex analysis]]
[[Category:Numerical function drawing]]</text>
      <sha1>mtd0w2o5jg8ptpoce982x6p9g7e7g35</sha1>
    </revision>
  </page>
  <page>
    <title>Donald A. Gillies</title>
    <ns>0</ns>
    <id>24874263</id>
    <revision>
      <id>836158869</id>
      <parentid>825427469</parentid>
      <timestamp>2018-04-13T00:49:15Z</timestamp>
      <contributor>
        <username>Tom.Reding</username>
        <id>9784415</id>
      </contributor>
      <minor/>
      <comment>Birth/death year categories, [[WP:GenFixes]] on, using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4708">{{Other people|Donald Gillies|Donald B. Gillies}}
{{BLP sources|date=January 2013}}
{{Infobox philosopher
| region = [[Western philosophy]]
| era    = [[Contemporary philosophy]]
| name        = Donald A. Gillies
| image       = 
| caption     =
| birth_date  = 1944
| birth_place = 
| school_tradition = [[Analytic philosophy|Analytic]]
| alma_mater     = [[Cambridge University]]&lt;br&gt;[[London School of Economics]]
| main_interests = [[Philosophy of mathematics]]&lt;br&gt;[[Philosophy of artificial intelligence]]
| influences = [[Karl Popper]], [[Imre Lakatos]]
| influenced = [[David Corfield]]&lt;ref&gt;[https://thalesandfriends.org/2007/07/22/mclarty-corfield/ Corfield interviewed by McLarty - Thales + Friends]&lt;/ref&gt;
| notable_ideas = Non-[[Bayesian epistemology|Bayesian]] [[Bayesian confirmation theory|confirmation theory]]
}}
'''Donald A. Gillies''' ({{IPAc-en|ˈ|g|ɪ|l|iː|z}}; born 1944) is a British philosopher and historian of [[science]] and [[mathematics]]. He is an Emeritus Professor in the Department of Science and Technology Studies at [[University College, London]].

==Career==

After undergraduate studies in mathematics and philosophy at [[Cambridge University|Cambridge]], Gillies became a graduate student of [[Karl Popper]] and [[Imre Lakatos]] at the [[London School of Economics]], where he completed a PhD on the foundations of probability.&lt;ref name="g"&gt;Wenceslao J. González, [https://books.google.com/books?id=94aZqJWfSowC&amp;pg=PR5&amp;dq=%22Donald+A.+Gillies%22&amp;as_brr=3&amp;ei=sqRdS5TJHJK-MqiC3egB&amp;cd=8#v=onepage&amp;q=%22Donald%20A.%20Gillies%22&amp;f=false Contemporary Perspectives in Philosophy and Methodology of Science.]  Netbiblo, 2006, {{ISBN|0-9729892-3-4}}; pp. v-vi&lt;/ref&gt;

Gilles is a past President&lt;ref name="g"/&gt; and a current Vice-President&lt;ref&gt;[http://www.thebsps.org/society/bsps/committee.html BSPS Officers and Committees 2009-2010], British Society for the Philosophy of Science. Accessed January 25, 2010&lt;/ref&gt; of [[British Society for the Philosophy of Science]]. From 1982 to 1985 he was an editor of the ''British Journal for the Philosophy of Science''.&lt;ref name="g"/&gt;

Gillies is probably best known for his work on [[Bayesian confirmation theory]], his attempt to simplify and extend Popper’s theory of [[corroboration]].  He proposes a novel "principle of explanatory surplus", likening a successful theoretician to a successful entrepreneur.  The entrepreneur generates a surplus (of income) over and above his initial investment (of funds) to meet the necessary expenses of the enterprise.  Similarly, the theoretician generates a surplus (of explanations) over and above his initial investment (of assumptions) to make the necessary explanations of known facts.  The size of this surplus is held to be a measure of the confirmation of the theory — but only in qualitative, rather than quantitative, terms.

Gillies has researched the philosophy of science, most particularly the foundations of probability; the philosophy of logic and mathematics; and the interactions of [[artificial intelligence]] with some aspects of philosophy, including probability, logic, causality and scientific method.

==Books and articles (selection)==
*Gillies, Donald (2011) "An objective theory of probability".  London: Routledge.
*Gillies, Donald (2011). "Frege, Dedekind, and Peano on the foundations of arithmetic".  London: Routledge.
*Gillies, Donald (2000) ''Philosophical Theories of Probability''.  London: Routledge.
*Gillies, Donald (1996) "Artificial intelligence and scientific method". Oxford: Oxford University Press.
*Gillies, Donald ed. (1992) ''[[Revolutions in Mathematics]]''. Oxford Science Publications. The Clarendon Press, Oxford University Press, New York.
*Gillies, Donald (1989). Non-Bayesian Confirmation Theory and the Principle of Explanatory Surplus. The Philosophy of Science Association, PSA 1988, Volume 2, pp.&amp;nbsp;373–380.
*Gillies, Donald and Chihara, Charles S. (1988). [http://fitelson.org/probability/cgpm.pdf An Interchange on the Popper-Miller Argument]. Philosophical Studies, Volume 54, pp.&amp;nbsp;1–8.

==References==
{{reflist}}

==External links==
*[http://www.ucl.ac.uk/sts/gillies/ Professor Donald Gillies, personal webpage], [[University College, London]]

{{Authority control}}

{{DEFAULTSORT:Gillies, Donald A.}}
[[Category:Philosophers of mathematics]]
[[Category:Philosophers of science]]
[[Category:Historians of mathematics]]
[[Category:20th-century English mathematicians]]
[[Category:21st-century English mathematicians]]
[[Category:Academics of University College London]]
[[Category:Living people]]
[[Category:20th-century British philosophers]]
[[Category:21st-century British philosophers]]
[[Category:1944 births]]</text>
      <sha1>g2p59eeys3cbjspr3zf01xydjgsleis</sha1>
    </revision>
  </page>
  <page>
    <title>Encyclopedic Dictionary of Mathematics</title>
    <ns>0</ns>
    <id>29902421</id>
    <revision>
      <id>756429582</id>
      <parentid>720590912</parentid>
      <timestamp>2016-12-24T05:01:10Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2987">{{italic title}}
The '''''Encyclopedic Dictionary of Mathematics''''' is a translation of the Japanese {{nihongo|''Iwanami Sūgaku Jiten''|岩波数学辞典}}.

==Editions==
*{{Citation | editor1-last=Iyanaga | editor1-first= Shōkichi| title=Iwanami Sūgaku Jiten | publisher=Iwanami Shoten | location=Tokyo | edition=1st | mr=0062668 | year=1954|language=Japanese}}
*{{Citation | editor1-last=Iyanaga | editor1-first=Shōkichi | title=Iwanami mathematical dictionary | publisher=Iwanami Shoten | location=Tokyo | language=Japanese | series=Edited by the Mathematical Society of Japan. Revised and enlarged | mr=0130788 | year=1960}}
*{{Citation | editor1-last=Iyanaga | editor1-first=Shōkichi | title=Iwanami Sūgaku Jiten | publisher=Iwanami Shoten Publishers, Tokyo | edition=2nd | mr=0241228 | year=1968|language=Japanese}}
*{{Citation | editor1-last=Iyanaga | editor1-first=Shōkichi | title=Encyclopedic dictionary of mathematics, Volumes I, II | publisher=[[MIT Press]] | edition=1 |series=hardback, translated from the 2nd Japanese edition | isbn=978-0-262-09016-2 | mr=490100 | year=1977 | pages=1–1750}}
*{{Citation | editor1-last=Iyanaga | editor1-first=Shōkichi | editor2-last=Kawada | editor2-first=Yukiyosi | title=Encyclopedic dictionary of mathematics, Volumes I, II| origyear=1977 | publisher=[[MIT Press]] | edition=1st | series=Translated from the 2nd Japanese edition, paperback version of the 1977 edition | isbn=978-0-262-59010-5 | mr=591028 | year=1980}}
*{{Citation | title=Sūgaku Jiten | publisher=Iwanami Shoten | location=Tokyo | language=Japanese | edition=3rd | isbn=978-4-00-080016-7 | mr=924841 | year=1985}}
*{{Citation|editor1-last=Itō |editor1-first=Kiyosi |title=Encyclopedic dictionary of mathematics. Vol. I--IV |publisher=[[MIT Press]] |edition=2nd |series=Hardback, Translated from the Japanese 3rd edition |isbn=978-0-262-09026-1 |mr=901762 |year=1987 |pages=2148 |url=http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=7771 |deadurl=yes |archiveurl=https://web.archive.org/web/20110629143131/http://mitpress.mit.edu/catalog/item/default.asp?ttype=2&amp;tid=7771 |archivedate=2011-06-29 |df= }}
*{{Citation | title=Iwanami Sūgaku Jiten | publisher=Iwanami Shoten | location=Tokyo | language=Japanese | edition=Fourth | isbn=978-4-00-080309-0 | mr=2383190 | year=2007}}

==References==

*{{Citation | last1=Dieudonne | first1=J. | title=Reviews: Encyclopedic Dictionary of Mathematics | doi=10.2307/2321544 | mr=1538996 | year=1979 | journal=[[American Mathematical Monthly|The American Mathematical Monthly]] | issn=0002-9890 | volume=86 | issue=3 | pages=232–233}}
*{{Citation | last1=Halmos | first1=Paul | title=About books: review of Encyclopedic Dictionary of Mathematics | doi=10.1007/BF03022868 | year=1981 | journal=[[The Mathematical Intelligencer]] | issn=0343-6993 | volume=3 | issue=3 | pages=138–140}}

[[Category:Mathematics books]]
[[Category:Encyclopedias of mathematics|Dictionary]]
[[Category:MIT Press books]]</text>
      <sha1>gt7tkwpsbmxisk39bqg9kp577g5ymvb</sha1>
    </revision>
  </page>
  <page>
    <title>Engelbert–Schmidt zero–one law</title>
    <ns>0</ns>
    <id>56591592</id>
    <revision>
      <id>849588895</id>
      <parentid>826299788</parentid>
      <timestamp>2018-07-10T01:18:49Z</timestamp>
      <contributor>
        <username>Josve05a</username>
        <id>12023796</id>
      </contributor>
      <minor/>
      <comment>/* References */fixing [[WP:CHECKWIKI]] error #37 (no DEFAULTSORT for article with special character) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2415">The '''Engelbert–Schmidt zero–one law''' is a theorem that gives a mathematical criterion for an event associated with a continuous, non-decreasing additive functional of Brownian motion to have probability either 0 or 1, without the possibility of an intermediate value. This zero-one law is used in the study of questions of finiteness and asymptotic behavior for [[stochastic differential equation]]s.&lt;ref&gt;{{cite book|author=Karatzas, Ioannis|author2=Shreve, Steven|title=Brownian motion and stochastic calculus|publisher=Springer|year=2012|pages=215|url=https://books.google.com/books?id=ATNy_Zg3PSsC&amp;pg=PA215}}&lt;/ref&gt; (A [[Wiener process]] is a mathematical formalization of Brownian motion used in the statement of the theorem.) This 0-1 law, published in 1981, is named after Hans-Jürgen Engelbert&lt;ref&gt;{{MathGenealogy|id=29366|title=Hans-Jürgen Engelbert}}&lt;/ref&gt; and the probabilist Wolfgang Schmidt&lt;ref&gt;{{MathGenealogy|id=90300|title=Wolfgang Schmidt}}&lt;/ref&gt; (not to be confused with the number theorist [[Wolfgang M. Schmidt]]).

==Engelbert–Schmidt 0–1 law==
Let &lt;math&gt;\mathcal{F}&lt;/math&gt; be a [[σ-algebra]] and let &lt;math&gt;F = (\mathcal{F}_t)_{t \ge 0}&lt;/math&gt; be an increasing family of sub-''σ''-algebras of &lt;math&gt;\mathcal{F}&lt;/math&gt;. Let &lt;math&gt;(W, F)&lt;/math&gt; be a [[Wiener process]] on the [[probability space]] &lt;math&gt;(\Omega, \mathcal{F}, P)&lt;/math&gt;.
Suppose that &lt;math&gt;f&lt;/math&gt; is a [[Borel measurable]] function of the real line into [0,∞].
Then the following three assertions are equivalent:

(i) &lt;math&gt; P \left( \int_0^t f (W_s)\,ds &lt; \infty, \text{ for all } t \ge 0 \right) &gt; 0 &lt;/math&gt;

(ii) &lt;math&gt; P \left( \int_0^t f (W_s)\,ds &lt; \infty \text{ for all } t \ge 0 \right) = 1 &lt;/math&gt;

(iii) &lt;math&gt; \int_K f (y)\,dy &lt; \infty \, &lt;/math&gt;

for all compact subsets &lt;math&gt;K&lt;/math&gt; of the real line.&lt;ref&gt;{{cite book|author=Engelbert, H. J.|author2=Schmidt, W.|chapter=On the behavior of certain functionals of the Wiener process and applications to stochastic differential equations|editor=Arató, M.|editor2=Vermes, D.|editor3=Balakrishnan, A. V.|title=Stochastic Differential Systems|series=Lectures Notes in Control and Information Sciences, vol. 36|year=1981|publisher=Springer|location=Berlin; Heidelberg|pages=47–55|doi=10.1007/BFb0006406}}&lt;/ref&gt;

==See also==
*[[zero-one law]]

==References==
{{reflist}}

{{DEFAULTSORT:Engelbert-Schmidt zero-one law}}
[[Category:Probability theorems]]</text>
      <sha1>9kiqurvwf7jhe2knm6l47l7yjgetw8k</sha1>
    </revision>
  </page>
  <page>
    <title>Entanglement (graph measure)</title>
    <ns>0</ns>
    <id>27646966</id>
    <revision>
      <id>846699804</id>
      <parentid>627311616</parentid>
      <timestamp>2018-06-20T10:57:53Z</timestamp>
      <contributor>
        <ip>130.232.105.232</ip>
      </contributor>
      <comment>fixed a typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2563">In [[graph theory]], '''entanglement''' of a [[directed graph]] is a number measuring how strongly
the cycles of the graph are intertwined. It is defined in terms of a [[mathematical game]] in which
''n'' cops try to capture a robber, who escapes along the edges of the graph. Similar to other
graph measures, such as [[cycle rank]], some algorithmic problems, e.g. [[parity game]], can be
efficiently solved on graphs of bounded entanglement.

== Definition ==

The '''entanglement game'''&lt;ref name=BG&gt;D. Berwanger and E. Grädel,
[http://www.logic.rwth-aachen.de/pub/dwb/BG_lpar04.pdf Entanglement &amp;ndash; A Measure for the Complexity of Directed Graphs with Applications to Logic and Games],
Proceedings of [[Logic for Programming, Artificial Intelligence and Reasoning|LPAR]]'04, vol. 3452 of LNCS, pp. 209&amp;ndash;223 (2004)&lt;/ref&gt; is played by ''n'' cops against a robber on
a directed graph ''G''. Initially, all cops are outside the graph and the robber selects an arbitrary starting vertex
''v'' of ''G''. Further on, the players move in turn. In each move the cops either stay where they are, or place one
of them on the vertex currently occupied by the robber. The robber must move from her current vertex, along an edge,
to a successor that is not occupied by a cop. The robber must move if no cop is following him. If there is
no free successor to which the robber can move, she is caught, and the cops win. The robber wins if she
cannot be caught, i.e. if the play can be made to last forever.

A graph ''G'' has entanglement ''n'' if ''n'' cops win in the entanglement game on ''G'' but ''n''&amp;nbsp;&amp;minus;&amp;nbsp;1 cops lose the game.

== Properties and applications ==

Graphs of entanglement zero and one can be characterized as follows:&lt;ref name=BG/&gt;
* entanglement of ''G'' is 0 if and only if ''G'' is  [[Directed acyclic graph|acyclic]]
* entanglement of ''G'' is 1 if and only if ''G'' is not acyclic, and in every [[strongly connected component]] of ''G'' there is a node whose removal makes the component acyclic.

Entanglement has also been a key notion in proving that the variable hierarchy
of the modal [[mu calculus]] is strict.&lt;ref&gt;D. Berwanger, E. Grädel and G. Lenzi,
[http://www.logic.rwth-aachen.de/pub/graedel/BeGrLe-tocs07.pdf The variable hierarchy of the mu-calculus is strict],
Theory of Computing Systems, vol. 40, pp. 437&amp;ndash;466 (2007)&lt;/ref&gt;

== References ==
{{Reflist}}

==External links==

You can play the entanglement game online on [http://tplay.org/index.html?game=Entanglement tPlay].

[[Category:Graph invariants]]</text>
      <sha1>ja14ebdgaim1nb2cz990amr6purcq3b</sha1>
    </revision>
  </page>
  <page>
    <title>Evolution of a random network</title>
    <ns>0</ns>
    <id>46900621</id>
    <revision>
      <id>841535275</id>
      <parentid>806745212</parentid>
      <timestamp>2018-05-16T12:40:56Z</timestamp>
      <contributor>
        <username>OAbot</username>
        <id>28481209</id>
      </contributor>
      <minor/>
      <comment>[[Wikipedia:OABOT|Open access bot]]: add arxiv identifier to citation with #oabot.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14370">{{orphan|date=October 2017}}

'''Evolution of a random network''' is a dynamical process, usually leading to emergence of giant component accompanied with striking consequences on the network topology. To quantify this process, there is a need of inspection on how the size of the largest connected cluster within the network, &lt;math&gt;{N_{G}}&lt;/math&gt;, varies with &lt;math&gt;{\left \langle k \right \rangle}&lt;/math&gt;.&lt;ref name=":1"&gt;Albert-László Barabási. Network Science: Chapter 3&lt;/ref&gt; Networks change their topology as they evolve, undergoing phase transitions. Phase transitions are generally known from physics, where it occurs as matter changes state according to its thermal energy level, or when ferromagnetic properties emerge in some materials as they are cooling down. Such phase transitions take place in matter because it is a network of particles, and as such, rules of network phase transition directly apply to it. Phase transitions in networks happen as links are added to a network, meaning that having N nodes, in each time increment, a link is placed between a randomly chosen pair of them. The transformation from a set of disconnected nodes to a fully connected network is called the evolution of a network.

If we begin with a network having N totally disconnected nodes (number of links is zero), and start adding links between randomly selected pairs of nodes, the evolution of the network begins. For some time we will just create pairs of nodes. After a while some of these pairs will connect, forming little trees. As we continue adding more links to the network, there comes a point when a giant component emerges in the network as some of these isolated trees connect to each other. This is called the critical point. In our natural example, this point corresponds to temperatures where materials change their state. Further adding nodes to the system, the giant component becomes even larger, as more and more nodes get a link to an other node which is already part of the giant component. The other special moment in this transition is when the network becomes fully connected, that is, when all nodes belong to the one giant component, which is effectively the network itself at that point.&lt;ref name=":1" /&gt;

== Conditions for emergence of a giant component ==

'''Condition for the emergence''' of the giant component was predicted by Erdős and Renyi in their paper:&lt;ref&gt;Erdős P., Rényi A. On random graphs //Publicationes Mathematicae Debrecen. – 1959. – Т. 6. – С. 290-297&lt;/ref&gt;

&lt;math&gt;{\left \langle k \right \rangle=1}&lt;/math&gt;, where &lt;math&gt;{\left \langle k \right \rangle}&lt;/math&gt; is the average degree of a random network.
 
Thus, one link is sufficient for its emergence of the giant component.
If expressing the condition in terms of &lt;math&gt;{p}&lt;/math&gt;, one obtain:
&lt;br /&gt;  
&lt;math&gt;{P_c=\frac{1}{N-1}\approx \frac{1}{N}}&lt;/math&gt; '''(1)'''
&lt;br /&gt;
Whew &lt;math&gt;{N}&lt;/math&gt; is the number of nodes, &lt;math&gt;{P_c}&lt;/math&gt; is the probability of clustering.                                                                                                                        
Therefore, the larger a network, the smaller &lt;math&gt;{p}&lt;/math&gt; is sufficient for the giant component.

== Regimes of evolution of a random network ==
Three topological regimes with its unique characteristics can be distinguished in network science: subcritical, supercritical and connected regimes.

=== Subcritical Regime ===
The subcritical phase is characterised by small isolated clusters, as the number of links is
much less than the number of nodes. A giant component can be designated any time to be the largest isolated small component, but the difference in cluster sizes is effectively negligible in this phase.

&lt;br /&gt;
&lt;math&gt;{0&lt;\left \langle k \right \rangle&lt; 1}&lt;/math&gt;, &lt;math&gt;{\left(p&lt;\frac{1}{n}\right)}&lt;/math&gt;

For &lt;math&gt;{\left \langle k \right \rangle = 0}&lt;/math&gt; the network consists of &lt;math&gt;{N}&lt;/math&gt; isolated nodes. Increasing &lt;math&gt;{\left \langle k \right \rangle}&lt;/math&gt; means adding &lt;math&gt;{N\left \langle k \right \rangle = pN(N-1)/2}&lt;/math&gt; links to the network. Yet, given that &lt;math&gt;{\left \langle k \right \rangle &lt; 1}&lt;/math&gt;, there is only a small number of links in this regime, hence mainly tiny clusters could be observed. 
At any moment the largest cluster can be designated to be the giant component. Yet in this regime the relative size of the largest cluster,&lt;math&gt;{\frac{N _{G}}{N}}&lt;/math&gt;, remains zero. The reason is that for &lt;math&gt;{\left \langle k \right \rangle &lt; 1}&lt;/math&gt; the largest cluster is a tree with size &lt;math&gt;{N_{G}\sim lnN}&lt;/math&gt;, hence its size increases much slower than the size of the network.
Therefore, &lt;math&gt;{N_{G}/N = ln N/N\rightarrow 0 }&lt;/math&gt; in the &lt;math&gt;{N\rightarrow \infty}&lt;/math&gt; limit.
In summary, in the subcritical regime the network consists of numerous tiny components, whose size follows the exponential distribution. Hence, these components have comparable sizes, lacking a clear winner that we could designate as a giant.&lt;ref name=":1"/&gt;

=== Critical Point ===

As we keep connecting nodes, the pairs joining together will form small trees, and if we keep
connecting nodes, a distinguishable giant component emerges at critical point &lt;k&gt; = 1.

This means that at the moment that each component has on average 1 link, a giant component emerges. This point corresponds to probability p = 1/ (N-1), as the probability of having a link between two nodes is the
ratio of the one case when that one link connect the two randomly chosen nodes, divided by all the other possibilities how that one connection can connect one of the nodes to an other node, which is N-1, as a node can connect to all other nodes but itself (excluding the possibility of a self loop in this model).

This also has the implication, that the larger a network is, the smaller p
it needs to have a giant component emerging.

&lt;br /&gt;
&lt;math&gt;{0&lt;\left \langle k \right \rangle= 1}&lt;/math&gt;, &lt;math&gt;{\left(p=\frac{1}{n}\right)}&lt;/math&gt;.

The critical point separates the regime where there is not yet a giant component ( &lt;math&gt;{0&lt;\left \langle k \right \rangle&lt; 1}&lt;/math&gt; ) from the regime where there is one ( &lt;math&gt;{\left \langle k \right \rangle&gt; 1}&lt;/math&gt; ). At this point, the relative size of the largest component is still zero. Indeed, the size of the largest component is &lt;math&gt;{N_{G}\sim N^{\tfrac{2}{3}}}&lt;/math&gt;. Consequently, &lt;math&gt;{N_{G}}&lt;/math&gt; grows much slower than the network's size, so its relative size decreases as &lt;math&gt;{\frac{N_{G}}{N}\sim N^{-\tfrac{1}{3}}}&lt;/math&gt; in the &lt;math&gt;{N\rightarrow \infty}&lt;/math&gt; limit.
Note, however, that in absolute terms there is a significant jump in the size of the largest component at &lt;math&gt;{\left \langle k \right \rangle=1}&lt;/math&gt;.
For example, for a random network with &lt;math&gt;{N = 7\ast109}&lt;/math&gt; nodes, comparable to the globe's social network, for &lt;math&gt;{\left \langle k \right \rangle=1}&lt;/math&gt; the largest cluster is of the order of &lt;math&gt;{N_{G}\simeq lnN = ln(7\ast109)\simeq 22.7}&lt;/math&gt;. In contrast at &lt;math&gt;{\left \langle k \right \rangle= 1}&lt;/math&gt; we expect &lt;math&gt;{N_{G}\simeq N^{\tfrac{2}{3}} = (7\ast109){\tfrac{2}{3}}\sim 3\ast106}&lt;/math&gt;, a jump of about five orders of magnitude. Yet, both in the subcritical regime and at the critical point the largest component contains only a vanishing fraction of the total number of nodes in the network.
In summary, at the critical point most nodes are located in numerous small components, whose size distribution follows. The power law form indicates that components of rather different sizes coexist. These numerous small components are mainly trees, while the giant component may contain loops. Note that many properties of the network at the critical point resemble the properties of a physical system undergoing a phase.&lt;ref name=":1"/&gt;

=== Supercritical Regime ===

Once the giant component had emerged upon passing the critical point, as we add more connections, the network will consist of a growing giant component, and less and less smaller isolated clusters and nodes.
Most real networks belong to ths regime. The size of the giant component is described as follows Ng = (p – pc) N.

&lt;br /&gt;
&lt;math&gt;{0&lt;\left \langle k \right \rangle&gt;1}&lt;/math&gt;, &lt;math&gt;{\left(p&gt;\frac{1}{n}\right)}&lt;/math&gt;.

This regime has the most relevance to real systems, as for the first time we have a giant component that looks like a network. In the vicinity of the critical point the size of the giant component varies as:
&lt;br /&gt;
&lt;math&gt;{N_{G}/N \sim \left \langle k \right \rangle-1}&lt;/math&gt; 
&lt;br /&gt;
or 
&lt;br /&gt;
&lt;math&gt;{N_{G} \sim (p - p_{c})N}&lt;/math&gt; '''(2)'''
&lt;br /&gt;                                                                                                                                                                   
where pc is given by (1).  In other words, the giant component contains a finite fraction of the nodes. The further we move from the critical point, a larger fraction of nodes will belong to it. Note that (2) is valid only in the vicinity of &lt;math&gt;{0&lt;\left \langle k \right \rangle=1}&lt;/math&gt;. For large &lt;math&gt;{0&lt;\left \langle k \right \rangle}&lt;/math&gt; the dependence between &lt;math&gt;{N_{G}}&lt;/math&gt; and &lt;math&gt;{0&lt;\left \langle k \right \rangle}&lt;/math&gt; is nonlinear.
In summary in the supercritical regime numerous isolated components coexist with the giant component, their size distribution following exponential distribution. These small components are trees, while the giant component contains Loops and cycles. The supercritical regime lasts until all nodes are absorbed by the giant.&lt;ref name=":1"/&gt;

=== Connected Regime ===
As connections are being added to a network there comes a point when &lt;k&gt; = lnN, and the giant
component absorbs all nodes making the network fully connected, having a complete graph.

&lt;br /&gt;
&lt;math&gt;{0&lt;\left \langle k \right \rangle &gt;lnN}&lt;/math&gt;, &lt;math&gt;{\left ( p&gt;\frac{lnN}{N} \right)}&lt;/math&gt;.

For sufficiently large p the giant component absorbs all nodes and components, hence &lt;math&gt;{N_{G} \simeq N}&lt;/math&gt;. In the absence of isolated nodes the network becomes connected. The average degree at which this happens depends on &lt;math&gt;{N}&lt;/math&gt; as &lt;math&gt;{\left (\frac{lnN}{N} \right)\rightarrow 0}&lt;/math&gt;. Note that when we enter the connected regime the network is still relatively sparse, as &lt;math&gt;{\left (\frac{lnN}{N} \right)\rightarrow 0}&lt;/math&gt; for large N. The network turns into a complete graph only at &lt;math&gt;{\left \langle k \right \rangle =  N - 1}&lt;/math&gt;.
In summary, the random network model predicts that the emergence of a network is not a smooth, gradual process: The isolated nodes and tiny components observed for small &lt;k&gt; collapse into a giant component through a phase.&lt;ref name=":1"/&gt;

== Examples for occurrences in nature ==

=== Water-ice transition ===
[[Phase transition]]s take place in matter, as it can be considered as a network of particles. When water is [[Freezing|frozen]], upon reaching 0 degree (the critical point) the crystalline structure of ice emerges according to the phase transitions of random networks: As cooling continues, each water molecule binds strongly to four others, forming the ice lattice, which is the emerging network.

=== Magnetic phase transition ===
Similarly, magnetic phase transition in [[Ferromagnetism|ferromagnetic]] materials also follow the pattern of network evolution: Above a certain temperature, [[Spin (physics)|spins]] of individual atoms can point in two different directions. However, upon cooling the material down, upon reaching a certain critical temperature, spins start to point in the same direction, creating the [[magnetic field]]. The emergence of magnetic properties in the structure of the material resemble to the evolution of a random network.&lt;ref name=":1"/&gt;

== Applications ==

=== Physics and chemistry ===
As we could see in the above examples, network theory applies to the structure of materials, therefore it is also applied in research related to materials and their properties in physics and chemistry.

Particularly important areas are [[polymer]]s,&lt;ref&gt;Samulionis, V., Svirskas, Š., Banys, J., Sánchez-Ferrer, A., Gimeno, N.,  &amp; Ros, M. B. (2015). Phase Transitions in Smectic Bent-Core Main-Chain Polymer Networks Detected by Dielectric and Ultrasonic Techniques. ''Ferroelectrics'', ''479''(1), 76-81. doi:10.1080/00150193.2015.1012011&lt;/ref&gt; [[gel]]s,&lt;ref&gt;Habicht, A., Schmolke, W., Lange, F., Saalwachter, K., &amp; Seiffert, S. (n.d). The Non-effect of Polymer-Network Inhomogeneities in Microgel 
Volume Phase Transitions: Support for the Mean-Field Perspective. ''Macromolecular Chemistry And Physics'', ''215''(11), 1116-1133.&lt;/ref&gt; and other material development such as [[cellulose]] with tunable properties.&lt;ref&gt;Liu, C., Zhong, G., Huang, H., &amp; Li, Z. (n.d). Phase assembly-induced transition of three dimensional nanofibril- to sheet-networks in porous cellulose with tunable properties. ''Cellulose'', ''21''(1), 383-394.&lt;/ref&gt;

=== Biology and medicine ===
Phase transitions are used in research related to the functioning of proteins or emergence of diabetes on the cell-level.&lt;ref&gt;Stamper, I., Jackson, E., &amp; Wang, X. (n.d). Phase transitions in 
pancreatic islet cellular networks and implications for type-1 diabetes.
 ''Physical Review E'', ''89''(1),&lt;/ref&gt; Neuroscience also extensively makes use of the model of the evolution of networks as phase-transition occur in neuron-networks.&lt;ref&gt;{{Cite journal|url = |title = Critical phenomena and noise-induced phase transitions in neuronal networks|last = Lee, KE Lopes, MA Mendes, JFF Goltsev, AV|first = |journal = Physical Review E |date=2014 |volume=89 |issue= |pages= 012701|doi = 10.1103/PhysRevE.89.012701|pmid = |access-date = |bibcode = 2014PhRvE..89a2701L|arxiv= 1310.4232}}&lt;/ref&gt;

=== Network science, statistics and machine learning ===
Phase transition of a network is naturally a building block of more advanced models within its own discipline too. It comes back in research examining clustering and percolation in networks,&lt;ref&gt;Colomer-de-Simon, P., &amp; Boguna, M. (2014). Double percolation phase transition in clustered complex networks.&lt;/ref&gt; or prediction of node properties.&lt;ref&gt;Zhang, P., Moore, C., &amp; Zdeborová, L. (2014). Phase transitions in semisupervised clustering of sparse networks.&lt;/ref&gt;

==References==
{{reflist}}

[[Category:Network theory]]
[[Category:Network topology]]</text>
      <sha1>hbbus91elu0n67aqw32d1c2zph73qn7</sha1>
    </revision>
  </page>
  <page>
    <title>Factorial prime</title>
    <ns>0</ns>
    <id>409293</id>
    <revision>
      <id>863392296</id>
      <parentid>823577174</parentid>
      <timestamp>2018-10-10T13:55:06Z</timestamp>
      <contributor>
        <username>Xayahrainie43</username>
        <id>34489455</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2277">{{Infobox integer sequence
| terms_number       = 49
| con_number         = Infinite
| parentsequence     = n!±1
| first_terms        = 2, 3, 5, 7, 23, 719, 5039, 39916801, 479001599, 87178291199
| largest_known_term = 208003!−1
| OEIS               = A088054
}}
A '''factorial prime''' is a [[prime number]] that is one less or one more than a [[factorial]] (all factorials &gt; 1 are even). 

The first 10 factorial primes (for n = 1, 2, 3, 4, 6, 7, 11, 12, 14) are {{OEIS|id=A088054}}:
:[[2 (number)|2]] (0!&amp;nbsp;+&amp;nbsp;1 or 1!&amp;nbsp;+&amp;nbsp;1), [[3 (number)|3]] (2!&amp;nbsp;+&amp;nbsp;1), [[5 (number)|5]] (3!&amp;nbsp;&amp;minus;&amp;nbsp;1), [[7 (number)|7]] (3!&amp;nbsp;+&amp;nbsp;1), [[23 (number)|23]] (4!&amp;nbsp;&amp;minus;&amp;nbsp;1), 719 (6!&amp;nbsp;&amp;minus;&amp;nbsp;1), 5039 (7!&amp;nbsp;&amp;minus;&amp;nbsp;1), 39916801 (11!&amp;nbsp;+&amp;nbsp;1), 479001599 (12!&amp;nbsp;&amp;minus;&amp;nbsp;1), 87178291199 (14!&amp;nbsp;&amp;minus;&amp;nbsp;1), ...

''n''! &amp;minus; 1 is prime for {{OEIS|id=A002982}}:
:''n'' = 3, 4, 6, 7, 12, 14, 30, 32, 33, 38, 94, 166, 324, 379, 469, 546, 974, 1963, 3507, 3610, 6917, 21480, 34790, 94550, 103040, 147855, 208003, ... (resulting in 27 factorial primes)

''n''! + 1 is prime for {{OEIS|id=A002981}}:
:''n'' = 0, 1, 2, 3, 11, 27, 37, 41, 73, 77, 116, 154, 320, 340, 399, 427, 872, 1477, 6380, 26951, 110059, 150209, ... (resulting in 21 factorial primes - the prime 2 is repeated)

No other factorial primes are known {{as of|2016|12|lc=on}}.

Absence of primes to both sides of a factorial ''n''! implies a run of at least 2''n''+1 consecutive [[composite number]]s, since ''n''!&amp;nbsp;±&amp;nbsp;''k'' is [[divisible]] by ''k'' for 2&amp;nbsp;≤&amp;nbsp;''k''&amp;nbsp;≤&amp;nbsp;''n''.  However, the necessary length of this run is asymptotically smaller than the average composite run for integers of similar size (see [[prime gap]]).

==See also==
* [[Primorial prime]]

==External links==
* {{MathWorld|urlname=FactorialPrime|title=Factorial Prime}}
* [http://primes.utm.edu/top20/page.php?id=30 The Top Twenty: Factorial primes] from the [[Prime Pages]]
* [http://www.primegrid.com/forum_thread.php?id=3008&amp;nowrap=true#30721 Factorial Prime Search] from [[PrimeGrid]]

{{Prime number classes|state=collapsed}}

[[Category:Integer sequences]]
[[Category:Classes of prime numbers]]
[[Category:Factorial and binomial topics]]</text>
      <sha1>gohvne013joukh17poub05ftvtvesnz</sha1>
    </revision>
  </page>
  <page>
    <title>Geometric lattice</title>
    <ns>0</ns>
    <id>17243191</id>
    <revision>
      <id>854897721</id>
      <parentid>817784722</parentid>
      <timestamp>2018-08-14T15:10:04Z</timestamp>
      <contributor>
        <username>Entropeter</username>
        <id>8779582</id>
      </contributor>
      <comment>/* Cryptomorphism */ identity is changed to inequality</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7489">In the mathematics of [[matroid]]s and [[lattice (order)|lattices]], a '''geometric lattice''' is a [[finite set|finite]] [[Atom (order theory)|atomistic]] [[semimodular lattice]], and a '''matroid lattice''' is an atomistic semimodular lattice without the assumptions of finiteness. Geometric lattices and matroid lattices, respectively, form the lattices of flats of finite and infinite matroids, and every geometric or matroid lattice comes from a matroid in this way.

==Definition==
A '''[[lattice (order)|lattice]]''' is a [[partially ordered set|poset]] in which any two elements &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt; have both a [[supremum]], denoted by &lt;math&gt;x\vee y&lt;/math&gt;, and an [[infimum]], denoted by &lt;math&gt;x\wedge y&lt;/math&gt;.
: The following definitions apply to posets in general, not just lattices, except where otherwise stated.
* For a [[minimal element]] &lt;math&gt;x&lt;/math&gt;, there is no element &lt;math&gt;y&lt;/math&gt; such that &lt;math&gt;y &lt; x&lt;/math&gt;.
* An element &lt;math&gt;x&lt;/math&gt; '''[[covering relation|covers]]''' another element &lt;math&gt;y&lt;/math&gt; (written as &lt;math&gt;x :&gt; y&lt;/math&gt; or &lt;math&gt; y &lt;: x&lt;/math&gt;) if &lt;math&gt;x &gt; y&lt;/math&gt; and there is no element &lt;math&gt;z&lt;/math&gt; distinct from both &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt; so that &lt;math&gt;x &gt; z &gt; y&lt;/math&gt;.
* A cover of a minimal element is called an '''[[Atom (order theory)|atom]]'''.
* A lattice is '''[[atomistic (order theory)|atomistic]]''' if every element is the supremum of some set of atoms.
* A poset is '''[[Graded poset|graded]]''' when it can be given a rank function &lt;math&gt;r(x)&lt;/math&gt; mapping its elements to integers, such that &lt;math&gt;r(x)&gt;r(y)&lt;/math&gt; whenever &lt;math&gt;x&gt;y&lt;/math&gt;, and in particular &lt;math&gt;r(x)=r(y)+1&lt;/math&gt; whenever &lt;math&gt;x :&gt; y&lt;/math&gt;.
: When a graded poset has a bottom element, one may assume, without loss of generality, that its rank is zero.  In this case, the atoms are the elements with rank one.
* A graded lattice is '''[[semimodular lattice|semimodular]]''' if, for every &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt;, its rank function obeys the identity&lt;ref&gt;{{harvtxt|Birkhoff|1995}}, Theorem 15, p.&amp;nbsp;40. More precisely, Birkhoff's definition reads "We shall call P (upper) semimodular when it satisfies: If ''a''≠''b'' both cover ''c'', then there exists a ''d''∈''P'' which covers both ''a'' and ''b''" (p.39). Theorem 15 states: "A graded lattice of finite length is semimodular if and only if ''r''(''x'')+''r''(''y'')≥''r''(''x''∧''y'')+''r''(''x''∨''y'')".&lt;/ref&gt;
:: &lt;math&gt;r(x)+r(y)\ge r(x\wedge y)+r(x\vee y). \, &lt;/math&gt;
* A '''matroid lattice''' is a lattice that is both atomistic and semimodular.&lt;ref&gt;{{citation
 | last1 = Maeda | first1 = F.
 | last2 = Maeda | first2 = S.
 | location = New York
 | mr = 0282889
 | publisher = Springer-Verlag
 | series = Die Grundlehren der mathematischen Wissenschaften, Band 173
 | title = Theory of Symmetric Lattices
 | year = 1970}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last = Welsh | first = D. J. A.
 | isbn = 9780486474397
 | page = 388
 | publisher = Courier Dover Publications
 | title = Matroid Theory
 | year = 2010}}.&lt;/ref&gt;  A '''geometric lattice''' is a ''finite'' matroid lattice.&lt;ref name="w10-51"&gt;{{harvtxt|Welsh|2010}}, p.&amp;nbsp;51.&lt;/ref&gt;
: Some authors consider only finite matroid lattices, and use the terms "geometric lattice" and "matroid lattice" interchangeably for both.&lt;ref&gt;{{citation|title=Lattice Theory|volume=25|series=Colloquium Publications|publisher=American Mathematical Society|first=Garrett|last=Birkhoff|authorlink=Garrett Birkhoff|edition=3rd|year=1995|isbn=9780821810255|page=80|url=https://books.google.com/books?id=0Y8d-MdtVwkC&amp;pg=PA80}}.&lt;/ref&gt;

==Cryptomorphism==
The geometric lattices are [[cryptomorphism|cryptomorphic]] to (finite, simple) matroids, and the matroid lattices are cryptomorphic to simple matroids without the assumption of finiteness.

Like geometric lattices, matroids are endowed with [[matroid rank|rank functions]], but these functions map sets of elements to numbers rather than taking individual elements as arguments. The rank function of a matroid must be monotonic (adding an element to a set can never decrease its rank) and they must be [[submodular function]]s, meaning that they obey an inequality similar to the one for semimodular lattices:

:&lt;math&gt;r(X)+r(Y)\ge r(X\cap Y)+r(X\cup Y). \,&lt;/math&gt;

The [[maximal element|maximal]] sets of a given rank are called flats. The intersection of two flats is again a flat, defining a greatest lower bound operation on pairs of flats; one can also define a least upper bound of a pair of flats to be the (unique) maximal superset of their union that has the same rank as their union. In this way, the flats of a matroid form a matroid lattice, or (if the matroid is finite) a geometric lattice.&lt;ref name="w10-51"/&gt;

Conversely, if &lt;math&gt;L&lt;/math&gt; is a matroid lattice, one may define a rank function on sets of its atoms, by defining the rank of a set of atoms to be the lattice rank of the greatest lower bound of the set. This rank function is necessarily monotonic and submodular, so it defines a matroid. This matroid is necessarily simple, meaning that every two-element set has rank two.&lt;ref name="w10-51"/&gt;

These two constructions, of a simple matroid from a lattice and of a lattice from a matroid, are inverse to each other: starting from a geometric lattice or a simple matroid, and performing both constructions one after the other, gives a lattice or matroid that is isomorphic to the original one.&lt;ref name="w10-51"/&gt;

==Duality==
There are two different natural notions of duality for a geometric lattice &lt;math&gt;L&lt;/math&gt;: the dual matroid, which has as its basis sets the [[complement (set theory)|complements]] of the bases of the matroid corresponding to &lt;math&gt;L&lt;/math&gt;, and the [[order dual|dual lattice]], the lattice that has the same elements as &lt;math&gt;L&lt;/math&gt; in the reverse order. They are not the same, and indeed the dual lattice is generally not itself a geometric lattice: the property of being atomistic is not preserved by order-reversal. {{harvtxt|Cheung|1974}} defines the [[adjoint]] of a geometric lattice &lt;math&gt;L&lt;/math&gt; (or of the matroid defined from it) to be a minimal geometric lattice into which the dual lattice of &lt;math&gt;L&lt;/math&gt; is [[order embedding|order-embedded]]. Some matroids do not have adjoints; an example is the [[Vámos matroid]].&lt;ref&gt;{{citation
 | last = Cheung | first = Alan L. C.
 | doi = 10.4153/CMB-1974-066-5
 | issue = 3
 | journal = [[Canadian Mathematical Bulletin]]
 | mr = 0373976
 | pages = 363–365; correction, ibid. 17 (1974), no. 4, 623
 | title = Adjoints of a geometry
 | volume = 17
 | year = 1974}}.&lt;/ref&gt;

==Additional properties==
Every interval of a geometric lattice (the subset of the lattice between given lower and upper bound elements) is itself geometric; taking an interval of a geometric lattice corresponds to forming a [[matroid minor|minor]] of the associated matroid. Geometric lattices are [[complemented lattice|complemented]], and because of the interval property they are also relatively complemented.&lt;ref&gt;{{harvtxt|Welsh|2010}}, pp. 55, 65–67.&lt;/ref&gt;

Every finite lattice is a sublattice of a geometric lattice.&lt;ref&gt;{{harvtxt|Welsh|2010}}, p. 58; Welsh credits this result to [[Robert P. Dilworth]], who proved it in 1941–1942, but does not give a specific citation for its original proof.&lt;/ref&gt;

==References==
{{reflist}}

==External links==
* {{planetmath reference|id=7972|title=Geometric lattice}}

[[Category:Lattice theory]]
[[Category:Matroid theory]]</text>
      <sha1>5yvjb8zxisl35mo702xrh2z5qzxdn3v</sha1>
    </revision>
  </page>
  <page>
    <title>Glossary of probability and statistics</title>
    <ns>0</ns>
    <id>2142850</id>
    <revision>
      <id>869098577</id>
      <parentid>869062918</parentid>
      <timestamp>2018-11-16T11:44:38Z</timestamp>
      <contributor>
        <username>Limit-theorem</username>
        <id>17495744</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contribs/Shanon Glenn Williams|Shanon Glenn Williams]] ([[User talk:Shanon Glenn Williams|talk]]) to last version by 41.113.155.160</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="27843">''Most of the terms listed in Wikipedia glossaries are already defined and explained within Wikipedia itself.  However, glossaries like this one are useful for looking up, comparing and reviewing large numbers of terms together.  You can help enhance this page by adding new terms or writing definitions for existing ones.''


The following is a [[glossary]] of terms used in the '''[[mathematics|mathematical]]''' sciences '''[[statistics]]''' and '''[[probability]]'''.

{{compact ToC|side=yes|center=yes|nobreak=yes|seealso=yes|refs=yes|}}






{{StatsTopicTOC}}
{{ProbabilityTopicsTOC}}
{{Science}}


== A ==

{{term|term=algebra of random variables|content=[[algebra of random variables]]}}

{{term|term=alternative hypothesis|content=[[alternative hypothesis]]}}

{{term|term=analysis of variance|content=[[analysis of variance]]}}

{{term|term=atomic event|content=[[Elementary event|atomic event]]}}
{{defn|1=Another name for elementary event}}

== B ==
{{term|term=bar chart|content=[[bar chart]]}}

{{term|term=Bayes' theorem|content=[[Bayes' theorem]]}}

{{term|term=Bayes estimator|content=[[Bayes estimator]]}}

{{term|term=Bayesian inference|content=[[Bayesian inference]]}}

{{term|term=bias|content=[[Bias (statistics)|bias]]}}
{{defn|no=1|A feature of a sample that is not representative of the population}}
{{defn|no=2|The difference between the expected value of an estimator and the true value}}

{{term|term=binary data|content=[[binary data]]}}
{{defn|1=Data that can take only two values, usually represented by 0 and 1}}

{{term|term=binomial distribution|content=[[binomial distribution]]}}

{{term|term=bivariate analysis|content=[[bivariate analysis]]}}

{{term|term=blocking|content=[[blocking (statistics)|blocking]]}}

{{term|term=Box-Jenkins method|content=[[Box-Jenkins method]]}}

{{term|term=box plot|content=[[box plot]]}}

== C ==

{{term|term= causal study |content= [[causality#Statistics and economics|causal study]]}}
{{defn|defn= A statistical study in which the objective is to measure the effect of some variable on the outcome of a different variable. For example, how will my headache feel if I take aspirin, versus if I do not take aspirin? Causal studies may be either experimental or observational.&lt;ref name=Reiter&gt;{{cite journal|last1=Reiter|first1=Jerome|title=Using Statistics to Determine Causal Relationships|journal=American Mathematical Monthly|date=January 24, 2000|url=https://www.jstor.org/discover/10.2307/2589374?uid=2&amp;uid=4&amp;sid=21104583336681|doi=10.2307/2589374 }}&lt;/ref&gt;}}

{{term|term=central limit theorem|content=[[central limit theorem]]}}

{{term|term=central moment|content=[[central moment]]}}

{{term|term=characteristic function (probability theory)|content=[[characteristic function (probability theory)|characteristic function]]}}

{{term|term=chi-squared distribution|content=[[chi-squared distribution]]}}

{{term|term=chi-squared test|content=[[chi-squared test]]}}

{{term|term=cluster analysis|content=[[cluster analysis]]}}

{{term|term=cluster sampling|content=[[cluster sampling]]}}

{{term|term=complementary event|content=[[complementary event]]}}

{{term|term=completely randomized design|content=[[completely randomized design]]}}

{{term|term=computational statistics|content=[[computational statistics]]}}

{{term|term= concomitants}}
{{defn|defn= In a statistical study, concomitants are any variables whose values are unaffected by treatments, such as a unit’s age, gender, and cholesterol level before starting a diet (treatment).&lt;ref name=Reiter/&gt;}}

{{term|term=conditional distribution|content=[[conditional distribution]]}}
{{defn|1=Given two jointly distributed random variables ''X'' and ''Y'', the conditional probability distribution of ''Y'' given ''X'' (written "''Y'' &lt;nowiki&gt;|&lt;/nowiki&gt; ''X''") is the probability distribution of ''Y'' when ''X'' is known to be a particular value}}

{{term|term=conditional probability|content=[[conditional probability]]}}
{{defn|1=The probability of some event A, assuming event B. Conditional probability is written P(''A''&lt;nowiki&gt;|&lt;/nowiki&gt;''B''), and is read "the probability of ''A'', given ''B''"}}

{{term|term=conditional probability distribution|content=[[conditional probability distribution]]}}

{{term|term=confidence interval|content=[[confidence interval]]}}
{{defn|1=In inferential statistics, a CI is a range of plausible values for some parameter, such as the population mean.&lt;ref name="Kalinowski"&gt;Pav Kalinowski. Understanding Confidence Intervals (CIs) and Effect Size Estimation. Association for Psychological Science Observer April 10, 2010. http://www.psychologicalscience.org/index.php/publications/observer/2010/april-10/understanding-confidence-intervals-cis-and-effect-size-estimation.html&lt;/ref&gt; For example, based on a study of sleep habits among 100 people, a researcher may estimate that the overall population sleeps somewhere between 5 and 9 hours per night. This is different from the sample mean, which can be measured directly.}}

{{term|term=confidence level}}
{{defn|1=Also known as a confidence coefficient, the confidence level indicates the probability that the confidence interval (range) captures the true population mean. For example, a confidence interval with a 95 percent confidence level has a 95 percent chance of capturing the population mean. Technically, this means that, if the experiment were repeated many times, 95 percent of the CIs would contain the true population mean.&lt;ref name="Kalinowski"/&gt;}}

{{term|term=confounding|content=[[confounding]]}}

{{term|term=conjugate prior|content=[[conjugate prior]]}}

{{term|term=continuous variaable|content=[[continuous variable]]}}

{{term|term=convenience sampling|content=[[convenience sampling]]}}

{{term|term=correlation|content=[[correlation]]}}
{{defn|1=Also called correlation coefficient, a numeric measure of the strength of linear relationship between two random variables (one can use it to quantify, for example, how shoe size and height are correlated in the population). An example is the [[Pearson product-moment correlation coefficient]], which is found by dividing the covariance of the two variables by the product of their standard deviations. Independent variables have a correlation of 0}}

{{term|term=count data|content=[[count data]]}}
{{defn|1=Data arising from [[counting]] that can take only non-negative integer values}}

{{term|term=covariance|content=[[covariance]]}}
{{defn|1=Given two random variables ''X'' and ''Y'', with expected values &lt;math&gt;E(X)=\mu&lt;/math&gt; and &lt;math&gt;E(Y)=\nu&lt;/math&gt;, covariance is defined as the expected value of random variable &lt;math&gt;(X - \mu) (Y - \nu)&lt;/math&gt;, and is written &lt;math&gt;\operatorname{cov}(X, Y)&lt;/math&gt;. It is used for measuring correlation}}

== D ==
{{term|term=data|content=[[data]]}}

{{term|term=data analysis|content=[[data analysis]]}}

{{term|term=data set|content=[[data set]]}}
{{defn|1=A sample and the associated ''data points''}}

{{term|term=data point|content=[[data point]]}}
{{defn|1=A typed measurement&amp;nbsp;— it can be a [[Boolean data type|Boolean]] value, a real number, a vector (in which case it's also called a data vector), etc}}

{{term|term=decision theory|content=[[decision theory]]}}

{{term|term=degrees of freedom|content=[[Degrees of freedom (statistics)|degrees of freedom]]}}

{{term|term=density estimation|content=[[density estimation]]}}

{{term|term=dependence|content=[[correlation and dependence|dependence]]}}

{{term|term=ddependent variable|content=[[dependent variable]]}}

{{term|term=descriptive statistics|content=[[descriptive statistics]]}}

{{term|term=design of experiments|content=[[design of experiments]]}}

{{term|term=deviation (statistics)|content=[[deviation (statistics)|deviation]]}}

{{term|term=discrete variable|content=[[discrete variable]]}}

{{term|term=dot plot|content=[[dot plot (statistics)|dot plot]]}}

{{term|term=double counting|content=[[double counting (fallacy)|double counting]]}}

== E ==

{{term|term=elementary event|content=[[elementary event]]}}
{{defn|1=An event with only one element. For example, when pulling a card out of a deck, "getting the jack of spades" is an elementary event, while "getting a king or an ace" is not}}

{{term|term=estimation theory|content=[[estimation theory]]}}

{{term|term=estimator|content=[[estimator]]}}
{{defn|1=A function of the known data that is used to estimate an unknown parameter; an estimate is the result from the actual application of the function to a particular set of data. The mean can be used as an estimator}}

{{term|term=expected value|content=[[expected value]]}}
{{defn|1=The sum of the probability of each possible outcome of the experiment multiplied by its payoff ("value"). Thus, it represents the average amount one "expects" to win per bet if bets with identical odds are repeated many times. For example, the expected value of a six-sided die roll is 3.5. The concept is similar to the mean. The expected value of random variable ''X'' is typically written E(X) for the operator and &lt;math&gt;\mu&lt;/math&gt; ([[Mu (letter)|mu]]) for the parameter}}

{{term|term=experiment|content=[[Experiment (probability theory)|experiment]]}}
{{defn|1=Any procedure that can be infinitely repeated and has a well-defined set of outcomes}}

{{term|term=exponential family|content=[[exponential family]]}}

{{term|term=event|content=[[Event (probability theory)|event]]}}
{{defn|1=A subset of the sample space (a possible experiment's outcome), to which a probability can be assigned. For example, on rolling a die, "getting a five or a six" is an event (with a probability of one third if the die is fair)}}

== F ==
{{term|term=factor analysis|content=[[factor analysis]]}}

{{term|term=factorial experiment|content=[[factorial experiment]]}}

{{term|term=frequency (statistics)|frequency|content=[[frequency (statistics)|frequency]]}}

{{term|term=frequency distribution|content=[[frequency distribution]]}}

{{term|term=frequency domain|content=[[frequency domain]]}}

{{term|term=frequentist inference|content=[[frequentist inference]]}}



== G ==

{{term|term=general linear model|content=[[general linear model]]}}

{{term|term=generalized linear model|content=[[generalized linear model]]}}

{{term|term=grouped data|content=[[grouped data]]}}

== H ==

{{term|term=histogram|content=[[histogram]]}}

== I ==

{{term|term=independent variable|content=[[independent variable]]}}

{{term|term=interquartile range|content=[[interquartile range]]}}

== J ==

{{term|term=joint distribution|content=[[joint distribution]]}}
{{defn|1=Given two random variables ''X'' and ''Y'', the joint distribution of ''X'' and ''Y'' is the probability distribution of X and Y together}}

{{term|term=joint probability|content=[[joint probability]]}}
{{defn|1=The probability of two events occurring together. The joint probability of  ''A'' and ''B'' is written &lt;math&gt;P(A \cap B)&lt;/math&gt; or &lt;math&gt;P(A, \ B).&lt;/math&gt;}}

== K ==

{{term|term=Kalman filter|content=[[Kalman filter]]}}

{{term|term=kernel (statistics)|content=[[kernel (statistics)|kernel]]}}

{{term|term=kernel density estimation|content=[[kernel density estimation]]}}

{{term|term=kurtosis|content=[[kurtosis]]}}
{{defn|1=A measure of the infrequent extreme observations (outliers) of the probability distribution of a real-valued random variable. Higher kurtosis means more of the variance is due to infrequent extreme deviations, as opposed to frequent modestly sized deviations}}

== L ==

{{term|term=L-moment|content=[[L-moment]]}}

{{term|term=law of large numbers|content=[[law of large numbers]]}}

{{term|term=likelihood function|content=[[likelihood function]]}}
{{defn|1=A conditional probability function considered a function of its second argument with its first argument held fixed. For example, imagine pulling a numbered ball with the number k from a bag of n balls, numbered 1 to n. Then you could describe a likelihood function for the random variable N as the probability of getting k given that there are n balls : the likelihood will be 1/n for n greater or equal to k, and 0 for n smaller than k. Unlike a probability distribution function, this likelihood function will not sum up to 1 on the sample space}}

{{term|term=likelihood-ratio test|content=[[likelihood-ratio test]]}}

== M ==

{{term|term=M-estimator|content=[[M-estimator]]}}

{{term|term=marginal distribution|content=[[marginal distribution]]}}
{{defn|1=Given two jointly distributed random variables ''X'' and ''Y'', the marginal distribution of ''X'' is simply the probability distribution of ''X'' ignoring information about ''Y''}}

{{term|term=marginal probability|content=[[marginal probability]]}}
{{defn|1=The probability of an event, ignoring any information about other events. The marginal probability of ''A'' is written ''P''(''A''). Contrast with conditional probability}}

{{term|term=Markov chain Monte Carlo|content=[[Markov chain Monte Carlo]]}}

{{term|term=mathematical statistics|content=[[mathematical statistics]]}}

{{term|term=maximum likelihood estimation|content=[[maximum likelihood estimation]]}}

{{term|term=mean|content=[[mean]]}}
{{defn|no=1|The expected value of a random variable}}
{{defn|no=2|The arithmetic mean is the average of a set of numbers, or the sum of the values divided by the number of values}}

{{term|term=median|content=[[median]]}}

{{term|term=median absolute deviation|content=[[median absolute deviation]]}}

{{term|term=mode|content=[[mode (statistics)|mode]]}}

{{term|term=moving average|content=[[moving average]]}}

{{term|term=multimodal distribution|content=[[multimodal distribution]]}}

{{term|term=multivariate analysis|content=[[multivariate analysis]]}}

{{term|term=multivariate kernel density estimation|content=[[multivariate kernel density estimation]]}}

{{term|term=multivariate random variable|content=[[multivariate random variable]]}}
{{defn|1=A vector whose components are random variables on the same probability space}}

{{term|term=mutual exclusivity|content=[[mutual exclusivity]]}}

{{term|term=mutual independence|content=[[mutual independence]]}}
{{defn|1=A collection of events is mutually independent if for any subset of the collection, the joint probability of all events occurring is equal to the product of the joint probabilities of the individual events. Think of the result of a series of coin-flips. This is a stronger condition than pairwise independence}}

== N ==

{{term|term=nonparametric regression|content=[[nonparametric regression]]}}

{{term|term=nonparametric statistics|content=[[nonparametric statistics]]}}

{{term|term=non-sampling error|content=[[non-sampling error]]}}

{{term|term=normal distribution|content=[[normal distribution]]}}

{{term|term=normal probability plot|content=[[normal probability plot]]}}

{{term|term=null hypothesis|content=[[null hypothesis]]}}
{{defn|1=The statement being tested in a test of statistical significance Usually the null hypothesis is a statement of 'no effect' or 'no difference'."&lt;ref name=moore&gt;{{cite book|last1=Moore|first1=David|last2=McCabe|first2=George|title=Introduction to the Practice of Statistics|publisher=W.H. Freeman and Co|location=New York|year=2003|page=438|edition=4|isbn=9780716796572}}&lt;/ref&gt; For example, if one wanted to test whether light has an effect on sleep, the null hypothesis would be that there is no effect. It is often symbolized as H&lt;sub&gt;0&lt;/sub&gt;.}}

== O ==

{{term|term=opinion poll|content=[[opinion poll]]}}

{{term|term=optimal decision|content=[[optimal decision]]}}

{{term|term=optimal design|content=[[optimal design]]}}

{{term|term=outlier|content=[[outlier]]}}

== P ==

{{term|term=p-value|content=[[p-value]]}}

{{term|term=pairwise independence|content=[[pairwise independence]]}}
{{defn|1=A pairwise independent collection of random variables is a set of random variables any two of which are independent}}

{{term|term=parameter|content=[[Statistical parameter|parameter]]}}
{{defn|1=Can be a population parameter, a distribution parameter, an unobserved parameter (with different shades of meaning). In statistics, this is often a quantity to be estimated}}

{{term|term=particle filter|content=[[particle filter]]}}

{{term|term=percentile|content=[[percentile]]}}

{{term|term=pie chart|content=[[pie chart]]}}

{{term|term=point estimation|content=[[point estimation]]}}

{{term|term=power (statistics)|content=[[power (statistics)|power]]}}

{{term|term=prior probability|content=[[prior probability]]}}
{{defn|1=In [[Bayesian inference]], this represents prior beliefs or other information that is available before new data or observations are taken into account}}

{{term|term=population parameter|content=[[population parameter]]}}
{{defn|1=See parameter}}

{{term|term=posterior probability|content=[[posterior probability]]}}
{{defn|1=The result of a [[Bayesian analysis]] that encapsulates the combination of prior beliefs or information with observed data}}

{{term|term=principal component analysis|content=[[principal component analysis]]}}

{{term|term=probability|content=[[probability]]}}

{{term|term=probability density|content=[[Probability density function|probability density]]}}
{{defn|1=Describes the probability in a continuous probability distribution. For example, you can't say that the probability of a man being six feet tall is 20%, but you can say he has 20% of chances of being between five and six feet tall. Probability density is given by a probability density function. Contrast with probability mass}}

{{term|term=probability density function|content=[[probability density function]]}}
{{defn|1=Gives the probability distribution for a continuous random variable}}

{{term|term=probability distribution|content=[[probability distribution]]}}
{{defn|1=A function that gives the probability of all elements in a given space: see [[List of probability distributions]]}}

{{term|term=probability measure|content=[[probability measure]]}}
{{defn|1=The probability of events in a probability space}}

{{term|term=probability plot|content=[[probability plot]]}}

{{term|term=probability space|content=[[probability space]]}}
{{defn|1=A sample space over which a probability measure has been defined}}

== Q ==

{{term|term=quantile|content=[[quantile]]}}

{{term|term=quartile|content=[[quartile]]}}

{{term|term=quota sampling|content=[[quota sampling]]}}

== R ==

{{term|term=random variable|content=[[random variable]]}}
{{defn|1=A measurable function on a probability space, often real-valued. The distribution function of a random variable gives the probability of different results. We can also derive the mean and variance of a random variable}}
{{see also|Discrete random variable|Continuous random variable}}

{{term|term=randomized block design|content=randomized block design}}

{{term|term=range|content=[[Range (statistics)|range]]}}
{{defn|1=The length of the smallest interval which contains all the data}}

{{term|term=recursive Bayesian estimation|content=[[recursive Bayesian estimation]]}}

{{term|term=regression analysis|content=[[regression analysis]]}}

{{term|term=repeated measures design|content=[[repeated measures design]]}}

{{term|term= responses}}
{{defn|defn= In a statistical study, any variables whose values may have been affected by the treatments, such as cholesterol levels after following a particular diet for six months.&lt;ref name=Reiter/&gt;}}

{{term|term=restricted randomization|content=[[restricted randomization]]}}

{{term|term=robust statistics|content=[[robust statistics]]}}

{{term|term=round-off error|content=[[round-off error]]}}

== S ==

{{term|term=sample|content=[[Statistical sample|sample]]}}
{{defn|1=That part of a population which is actually observed}}

{{term|term= sample mean}}
{{defn|defn=The arithmetic mean of a sample of values drawn from the population. It is denoted by &lt;math&gt;\overline{x}&lt;/math&gt;. An example is the average test score of a subset of 10 students from a class. Sample mean is used as an estimator of the population mean, which in this example would be the average test score of all of the students in the class.}}

{{term|term=sample space|content=[[sample space]]}}
{{defn|1=The set of possible outcomes of an experiment. For example, the sample space for rolling a six-sided die will be &lt;nowiki&gt;{1, 2, 3, 4, 5, 6}&lt;/nowiki&gt;}}

{{term|term=sampling|content=[[Sampling (statistics)|sampling]]}}
{{defn|1=A process of selecting observations to obtain knowledge about a population. There are many methods to choose on which sample to do the observations}}

{{term|term=sampling bias|content=[[sampling bias]]}}

{{term|term=sampling distribution|content=[[sampling distribution]]}}
{{defn|1=The probability distribution, under repeated sampling of the population, of a given statistic}}

{{term|term=sampling error|content=[[sampling error]]}}

{{term|term=scatter plot|content=[[scatter plot]]}}

{{term|term=significance level|content=significance level}}

{{term|term=simple random sample|content=[[simple random sample]]}}

{{term|term=Simpson's paradox|content=[[Simpson's paradox]]}}

{{term|term=skewness|content=[[skewness]]}}
{{defn|1=A measure of the asymmetry of the probability distribution of a real-valued random variable. Roughly speaking, a distribution has positive skew (right-skewed) if the higher tail is longer and negative skew (left-skewed) if the lower tail is longer (confusing the two is a common error)}}

{{term|term=spaghetti plot|content=[[spaghetti plot]]}}

{{term|term=spectrum bias|content=[[spectrum bias]]}}

{{term|term=standard deviation|content=[[standard deviation]]}}
{{defn|1=The most commonly used measure of statistical dispersion. It is the [[square root]] of the variance, and is generally written &lt;math&gt;\sigma&lt;/math&gt; ([[Sigma (letter)|sigma]])}}

{{term|term=standard error|content=[[standard error]]}}

{{term|term=standard score|content=[[standard score]]}}

{{term|term=statistic|content=[[statistic]]}}
{{defn|1=The result of applying a statistical algorithm to a data set. It can also be described as an observable random variable}}

{{term|term=statistical dispersion|content=[[statistical dispersion]]}}

{{term|term=statistical graphics|content=[[statistical graphics]]}}

{{term|term=statistical hypothesis testing|content=[[statistical hypothesis testing]]}}

{{term|term=statistical independence|content=[[statistical independence]]}}
{{defn|1=Two events are independent if the outcome of one does not affect that of the other (for example, getting a 1 on one die roll does not affect the probability of getting a 1 on a second roll). Similarly, when we assert that two random variables are independent, we intuitively mean that knowing something about the value of one of them does not yield any information about the value of the other}}

{{term|term=statistical inference|content=[[statistical inference]]}}
{{defn|1=Inference about a population from a random sample drawn from it or, more generally, about a random process from its observed behavior during a finite period of time}}

{{term|term=statistical interference|content=[[statistical interference]]}}

{{term|term=statistical model|content=[[statistical model]]}}

{{term|term=statistical population|content=[[statistical population]]}}
{{defn|1=A set of entities about which statistical inferences are to be drawn, often based on random sampling. One can also talk about a population of measurements or values}}

{{term|term=statistical dispersion|content=[[statistical dispersion]]}}
{{defn|1=Statistical variability is a measure of how diverse some data is. It can be expressed by the variance or the standard deviation}}

{{term|term=statistical parameter|content=[[statistical parameter]]}}
{{defn|1=A parameter that indexes a family of probability distributions}}

{{term|term=statistical significance|content=[[statistical significance]]}}

{{term|term=statistics|content=[[statistics]]}}

{{term|term=sstem-and-leaf display|content=[[stem-and-leaf display]]}}

{{term|term=stratified sampling|content=[[stratified sampling]]}}

{{term|term=survey methodology|content=[[survey methodology]]}}

{{term|term=survival function|content=[[survival function]]}}

{{term|term=survivorship bias|content=[[survivorship bias]]}}

{{term|term=symmetric probability distribution|content=[[symmetric probability distribution]]}}

{{term|term=systematic sampling|content=[[systematic sampling]]}}

== T ==

{{term|term=test statistic|content=[[test statistic]]}}

{{term|term=time domain|content=[[time domain]]}}

{{term|term=time series|content=[[time series]]}}

{{term|term=time series analysis|content=[[time series#time series analysis|time series analysis]]}}

{{term|term=time series forecasting|content=[[time series#time series analysis|time series forecasting]]}}

{{term|term= treatments}}
{{defn|defn= Variables in a statistical study that are conceptually manipulable. For example, in a health study, following a certain diet is a treatment whereas age is not.&lt;ref name=Reiter/&gt;}}

{{term|term=trial|content=[[Experiment (probability theory)|trial]]}}
{{defn|1=Can refer to each individual repetition when talking about an experiment composed of any fixed number of them. As an example, one can think of an experiment being any number from one to ''n'' coin tosses, say 17. In this case, one toss can be called a trial to avoid confusion, since the whole experiment is composed of 17 ones. }}

{{term|term=trimmed estimator|content=[[trimmed estimator]]}}

{{term|term=type I and type II errors|content=[[type I and type II errors]]}}

== U ==

{{term|term=[[unimodal probability distribution]]|content=[[unimodal probability distribution]]}}

{{term|term= units}}
{{defn|defn= In a statistical study, the objects to which treatments are assigned. For example, in a study examining the effects of smoking cigarettes, the units would be people.&lt;ref name=Reiter/&gt;}}

== V ==

{{term|term=variance|content=[[variance]]}}
{{defn|1=A measure of its statistical dispersion of a random variable, indicating how far from the expected value its values typically are. The variance of random variable ''X'' is typically designated as &lt;math&gt;\operatorname{var}(X)&lt;/math&gt;, &lt;math&gt;\sigma_X^2&lt;/math&gt;, or simply &lt;math&gt;\sigma^2&lt;/math&gt;}}

== W ==

{{term|term=weighted arithmetic mean|content=[[weighted arithmetic mean]]}}

{{term|term=weighted median|content=[[weighted median]]}}

== X ==

{{term|term=XOR|content=[[XOR|XOR, exclusive disjunction]]}}

== Y ==

{{term|term=Yates's correction for continuity|content=[[Yates's correction for continuity]]}}

== Z ==
{{term|term=z-test|content=[[z-test]]}}


{{glossend}}

== See also ==
* [[Notation in probability and statistics]]
* [[Probability axioms]]
* [[Glossary of experimental design]]
* [[List of statistical topics]]
* [[List of probability topics]]
* [[Glossary of areas of mathematics]]
* [[Glossary of calculus]]

==References ==
{{Reflist|30em}}

== External links ==
* {{citation | title = NIST/SEMATECH e-Handbook of Statistical Methods | chapter = A Glossary of DOE Terminology | url = http://www.itl.nist.gov/div898/handbook/pri/section7/pri7.htm | publisher= [[NIST]] | accessdate = 28 February 2009}}
* {{ citation | url = http://www.statistics.com/resources/glossary/ | title = Statistical glossary | publisher = "statistics.com" | accessdate = 28 February 2009}}
* [http://www.economics.soton.ac.uk/staff/aldrich/Probability%20Earliest%20Uses.htm Probability and Statistics on the Earliest Uses Pages (Univ. of Southampton)]



{{Statistics}}
{{Glossaries of science and engineering}}


[[Category:Probability and statistics| Glossary]]
[[Category:Statistics-related lists]]
[[Category:Glossaries of science|Probability and statistics]]
[[Category:Glossaries of mathematics|Probability and statistics]]
[[Category:Wikipedia glossaries|Probability and statistics]]</text>
      <sha1>heuos8e711ejmnbu42ybh1xspqic96f</sha1>
    </revision>
  </page>
  <page>
    <title>Hilbert's theorem (differential geometry)</title>
    <ns>0</ns>
    <id>11720315</id>
    <revision>
      <id>862709397</id>
      <parentid>860504987</parentid>
      <timestamp>2018-10-06T05:20:46Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Removing category Eponymous scientific concepts per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2018 September 22]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9337">In [[differential geometry]], '''Hilbert's theorem''' (1901) states that there exists no complete [[Smooth surface|regular surface]] &lt;math&gt;S&lt;/math&gt; of constant negative [[gaussian curvature]] &lt;math&gt;K&lt;/math&gt; [[immersion (mathematics)|immersed]] in &lt;math&gt;\mathbb{R}^{3}&lt;/math&gt;. This theorem answers the question for the negative case of which surfaces in &lt;math&gt;\mathbb{R}^{3}&lt;/math&gt; can be obtained by isometrically immersing [[complete manifold]]s with [[constant curvature]].

==History==

*Hilbert's theorem was first treated by [[David Hilbert]] in, "Über Flächen von konstanter Krümmung" ([[Trans. Amer. Math. Soc.]] 2 (1901), 87-99). 
*A different proof was given shortly after by E. Holmgren, "Sur les surfaces à courbure constante négative," (1902).
*A far leading generalization was obtained by [[Nikolai Efimov]] in 1975.&lt;ref&gt;Ефимов, Н. В. Непогружаемость   полуплоскости   Лобачевского. Вестн. МГУ. Сер. мат.,  мех.  — 1975. — No  2.  — С. 83—86.&lt;/ref&gt;

==Proof==
The [[proof (mathematics)|proof]] of Hilbert's theorem is elaborate and requires several [[lemma (mathematics)|lemma]]s. The idea is to show the nonexistence of an isometric [[immersion (mathematics)|immersion]] 
:&lt;math&gt;\varphi = \psi \circ \exp_p: S' \longrightarrow \mathbb{R}^{3}&lt;/math&gt;

of a plane &lt;math&gt;S'&lt;/math&gt; to the real space &lt;math&gt;\mathbb{R}^{3}&lt;/math&gt;. This proof is basically the same as in Hilbert's paper, although based in the books of [[Manfredo do Carmo|Do Carmo]] and [[Michael Spivak|Spivak]].

''Observations'': In order to have a more manageable treatment, but [[without loss of generality]], the [[curvature]] may be considered equal to minus one, &lt;math&gt;K=-1&lt;/math&gt;. There is no loss of generality, since it is being dealt with constant curvatures, and similarities of &lt;math&gt;\mathbb{R}^{3}&lt;/math&gt; multiply &lt;math&gt;K&lt;/math&gt; by a constant. The [[exponential map (Riemannian geometry)|exponential map]] &lt;math&gt;\exp_p: T_p(S) \longrightarrow S&lt;/math&gt; is a [[local diffeomorphism]] (in fact a covering map, by Cartan-Hadamard theorem), therefore, it induces an [[inner product]] in the [[tangent space]] of &lt;math&gt;S&lt;/math&gt; at &lt;math&gt;p&lt;/math&gt;: &lt;math&gt;T_p(S)&lt;/math&gt;. Furthermore, &lt;math&gt;S'&lt;/math&gt; denotes the geometric surface &lt;math&gt;T_p(S)&lt;/math&gt; with this inner product. If &lt;math&gt;\psi:S \longrightarrow \mathbb{R}^{3}&lt;/math&gt; is an isometric immersion, the same holds for 
:&lt;math&gt;\varphi = \psi \circ \exp_o:S' \longrightarrow \mathbb{R}^{3}&lt;/math&gt;.

The first lemma is independent from the other ones, and will be used at the end as the counter statement to reject the results from the other lemmas.

'''Lemma 1''': The area of &lt;math&gt;S'&lt;/math&gt; is infinite. &lt;br /&gt;
''Proof's Sketch:'' &lt;br /&gt;
The idea of the proof is to create a [[global isometry]] between &lt;math&gt;H&lt;/math&gt; and &lt;math&gt;S'&lt;/math&gt;. Then, since &lt;math&gt;H&lt;/math&gt; has an infinite area, &lt;math&gt;S'&lt;/math&gt; will have it too. &lt;br /&gt;
The fact that the [[Hyperbolic manifold|hyperbolic plane]] &lt;math&gt;H&lt;/math&gt; has an infinite area comes by computing the [[surface integral]] with the corresponding [[coefficient]]s of the [[First fundamental form]]. To obtain these ones, the hyperbolic plane can be defined as the plane with the following inner product around a point &lt;math&gt;q\in \mathbb{R}^{2}&lt;/math&gt; with coordinates &lt;math&gt;(u,v)&lt;/math&gt;&lt;br /&gt;
:&lt;math&gt;E = \left\langle \frac{\partial}{\partial u}, \frac{\partial}{\partial u} \right\rangle = 1 \qquad F = \left\langle \frac{\partial}{\partial u}, \frac{\partial}{\partial v} \right\rangle = \left\langle \frac{\partial}{\partial v}, \frac{\partial}{\partial u} \right\rangle = 0 \qquad G = \left\langle \frac{\partial}{\partial v}, \frac{\partial}{\partial v} \right\rangle = e^{u} &lt;/math&gt;

Since the hyperbolic plane is unbounded, the limits of the integral are [[Infinity|infinite]], and the area can be calculated through
:&lt;math&gt;\int_{-\infty}^{\infty} \int_{-\infty}^{\infty} e^{u} du dv = \infty&lt;/math&gt;

Next it is needed to create a map, which will show that the global information from the hyperbolic plane can be transfer to the surface &lt;math&gt;S'&lt;/math&gt;, i.e. a global isometry. &lt;math&gt;\varphi: H \rightarrow S'&lt;/math&gt; will be the map, whose domain is the hyperbolic plane and image the [[2-dimensional manifold]] &lt;math&gt;S'&lt;/math&gt;, which carries the inner product from the surface &lt;math&gt;S&lt;/math&gt; with negative curvature. &lt;math&gt;\varphi&lt;/math&gt; will be defined via the exponential map, its inverse, and a linear isometry between their tangent spaces, 
:&lt;math&gt;\psi:T_p(H) \rightarrow T_{p'}(S')&lt;/math&gt;.

That is 
:&lt;math&gt;\varphi = \exp_{p'} \circ \psi \circ \exp_p^{-1}&lt;/math&gt;,

where &lt;math&gt;p\in H, p' \in S'&lt;/math&gt;. That is to say, the starting point &lt;math&gt;p\in H&lt;/math&gt; goes to the tangent plane from &lt;math&gt;H&lt;/math&gt; through the inverse of the exponential map. Then travels from one tangent plane to the other through the isometry &lt;math&gt;\psi&lt;/math&gt;, and then down to the surface &lt;math&gt;S'&lt;/math&gt; with another exponential map.

The following step involves the use of [[polar coordinates]], &lt;math&gt;(\rho, \theta)&lt;/math&gt; and &lt;math&gt;(\rho', \theta')&lt;/math&gt;, around &lt;math&gt;p&lt;/math&gt; and &lt;math&gt;p'&lt;/math&gt; respectively. The requirement will be that the axis are mapped to each other, that is &lt;math&gt;\theta=0&lt;/math&gt; goes to &lt;math&gt;\theta'=0&lt;/math&gt;. Then &lt;math&gt;\varphi&lt;/math&gt; preserves the first fundamental form. &lt;br /&gt;
In a geodesic polar system, the [[Gaussian curvature]] &lt;math&gt;K&lt;/math&gt; can be expressed as 
:&lt;math&gt;K = - \frac{(\sqrt{G})_{\rho \rho}}{\sqrt{G}}&lt;/math&gt;.

In addition K is constant and fulfills the following differential equation 
:&lt;math&gt;(\sqrt{G})_{\rho \rho} + K\cdot \sqrt{G} = 0&lt;/math&gt;

Since &lt;math&gt;H&lt;/math&gt; and &lt;math&gt;S'&lt;/math&gt; have the same constant Gaussian curvature, then they are locally isometric ([[Minding's Theorem]]). That means that &lt;math&gt;\varphi&lt;/math&gt; is a local isometry between &lt;math&gt;H&lt;/math&gt; and &lt;math&gt;S'&lt;/math&gt;. Furthermore, from the Hadamard's theorem it follows that &lt;math&gt;\varphi&lt;/math&gt; is also a covering map. &lt;br /&gt;
Since &lt;math&gt;S'&lt;/math&gt; is simply connected, &lt;math&gt;\varphi&lt;/math&gt; is a homeomorphism, and hence, a (global) isometry. Therefore, &lt;math&gt;H&lt;/math&gt; and &lt;math&gt;S'&lt;/math&gt; are globally isometric, and because &lt;math&gt;H&lt;/math&gt; has an infinite area, then &lt;math&gt;S'=T_p(S)&lt;/math&gt; has an infinite area, as well. &lt;math&gt;\square&lt;/math&gt;

'''Lemma 2''': For each &lt;math&gt;p\in S'&lt;/math&gt; exists a parametrization &lt;math&gt;x:U \subset \mathbb{R}^{2} \longrightarrow S', \qquad p \in x(U)&lt;/math&gt;, such that the [[coordinate curves]] of &lt;math&gt;x&lt;/math&gt; are asymptotic curves of &lt;math&gt; x(U) = V'&lt;/math&gt; and form a Tchebyshef net.

'''Lemma 3''': Let &lt;math&gt;V' \subset S'&lt;/math&gt; be a coordinate [[neighborhood]] of &lt;math&gt;S'&lt;/math&gt; such that the coordinate curves are asymptotic curves in &lt;math&gt;V'&lt;/math&gt;. Then the area A of any quadrilateral formed by the coordinate curves is smaller than &lt;math&gt;2\pi&lt;/math&gt;.

The next goal is to show that &lt;math&gt;x&lt;/math&gt; is a parametrization of &lt;math&gt;S'&lt;/math&gt;.

'''Lemma 4''': For a fixed &lt;math&gt;t&lt;/math&gt;, the curve &lt;math&gt;x(s,t), -\infty &lt; s &lt; +\infty &lt;/math&gt;, is an asymptotic curve with &lt;math&gt;s&lt;/math&gt; as arc length.

The following 2 lemmas together with lemma 8 will demonstrate the existence of a [[parametrization]] &lt;math&gt;x:\mathbb{R}^{2} \longrightarrow S'&lt;/math&gt;

'''Lemma 5''': &lt;math&gt;x&lt;/math&gt; is a local diffeomorphism.

'''Lemma 6''': &lt;math&gt;x&lt;/math&gt; is [[surjective]].

'''Lemma 7''': On &lt;math&gt;S'&lt;/math&gt; there are two differentiable linearly independent vector fields which are tangent to the [[asymptotic curve]]s of &lt;math&gt;S'&lt;/math&gt;.

'''Lemma 8''': &lt;math&gt;x&lt;/math&gt; is [[injective]].

''Proof of Hilbert's Theorem:'' &lt;br /&gt;
First, it will be assumed that an isometric immersion from a [[complete surface]] with negative curvature&lt;math&gt;S&lt;/math&gt; exists: &lt;math&gt;\psi:S \longrightarrow \mathbb{R}^{3}&lt;/math&gt;

As stated in the observations, the tangent plane &lt;math&gt;T_p(S)&lt;/math&gt; is endowed with the metric induced by the exponential map &lt;math&gt;\exp_p: T_p(S) \longrightarrow S&lt;/math&gt; . Moreover, &lt;math&gt;\varphi = \psi \circ \exp_p:S' \longrightarrow \mathbb{R}^{3}&lt;/math&gt; is an isometric immersion and Lemmas 5,6, and 8 show the existence of a parametrization &lt;math&gt;x:\mathbb{R}^{2} \longrightarrow S'&lt;/math&gt; of the whole &lt;math&gt;S'&lt;/math&gt;, such that the coordinate curves of &lt;math&gt;x&lt;/math&gt; are the asymptotic curves of &lt;math&gt;S'&lt;/math&gt;. This result was provided by Lemma 4. Therefore, &lt;math&gt;S'&lt;/math&gt; can be covered by a union of "coordinate" quadrilaterals &lt;math&gt;Q_{n}&lt;/math&gt; with &lt;math&gt; Q_{n} \subset Q_{n+1}&lt;/math&gt;. By Lemma 3, the area of each quadrilateral is smaller than &lt;math&gt;2 \pi &lt;/math&gt;. On the other hand, by Lemma 1, the area of &lt;math&gt;S'&lt;/math&gt; is infinite, therefore has no bounds. This is a contradiction and the proof is concluded. &lt;math&gt;\square&lt;/math&gt;

==See also ==
* [[Nash embedding theorem]], states that every Riemannian manifold can be isometrically embedded into some Euclidean space.

==References==
{{Reflist}}
* {{aut|Manfredo do Carmo|Do Carmo, Manfredo}}, ''Differential Geometry of Curves and Surfaces'', Prentice Hall, 1976.
* {{aut|[[Michael Spivak|Spivak, Michael]]}}, ''A Comprehensive Introduction to Differential Geometry'', Publish or Perish, 1999.

{{DEFAULTSORT:Hilberts theorem}}
[[Category:Hyperbolic geometry]]
[[Category:Theorems in differential geometry]]
[[Category:Articles containing proofs]]</text>
      <sha1>rohwue9huohxx1l0yoktjbqoja061o7</sha1>
    </revision>
  </page>
  <page>
    <title>Horizontal translation</title>
    <ns>0</ns>
    <id>42633078</id>
    <revision>
      <id>825649811</id>
      <parentid>825649365</parentid>
      <timestamp>2018-02-14T16:12:39Z</timestamp>
      <contributor>
        <username>Thinking of England</username>
        <id>9901790</id>
      </contributor>
      <comment>Undid 10:46, 28 January 2018 (UTC) test edit (added blank lines) by [[Special:Contributions/80.194.11.70|80.194.11.70]] ([[User talk:80.194.11.70|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2584">{{Multiple issues|{{more footnotes|date=June 2014}}{{technical|date=June 2014}}}}

In [[function graphing]], a '''horizontal [[translation (geometry)|translation]]''' is a [[transformation (function)|transformation]] which results in a graph that is equivalent to shifting the base graph left or right in the direction of the ''x''-axis. A graph is translated ''k'' units horizontally by moving each point on the graph ''k'' units horizontally.

For the [[base function]] ''f''(''x'') and a [[constant (mathematics)|constant]] ''k'', the function given by ''g''(''x'')&amp;nbsp;=&amp;nbsp;''f''(''x''&amp;nbsp;&amp;minus;&amp;nbsp;''k''), can be sketched ''f''(''x'') shifted ''k'' units horizontally.

If function transformation was talked about in terms of geometric transformations it may be clearer why functions translate horizontally the way they do. When addressing translations on the [[Cartesian plane]] it is natural to introduce translations in this type of notation:

&lt;math&gt;(x,y)\rightarrow(x+a,y+b)&lt;/math&gt;

or

&lt;math&gt;T(x,y) = (x+a,y+b)&lt;/math&gt;

where &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; are horizontal and vertical changes respectively..

==Example==

Taking the [[parabola]] ''y''&amp;nbsp;=&amp;nbsp;''x''&lt;sup&gt;2&lt;/sup&gt; ,  a horizontal translation 5 units to the right would be represented by ''T''((''x'',''y'')) = (''x'' + 5, ''y''). Now we must connect this transformation notation to an algebraic notation.  Consider the point (''a''.''b'') on the original parabola that moves to point (''c'',''d'') on the translated parabola. According to our translation, ''c'' = ''a'' + 5  and ''d'' = ''b''. The point on the original parabola was ''b'' = ''a''&lt;sup&gt;2&lt;/sup&gt;. Our new point can be described by relating ''d'' and ''c'' in the same equation. ''b'' = ''d'' and ''a'' = ''c'' &amp;nbsp;&amp;minus;&amp;nbsp;5.
So ''d'' = ''b'' = ''a''&lt;sup&gt;2&lt;/sup&gt; = (''c'' &amp;minus; 5)&lt;sup&gt;2&lt;/sup&gt; Since this is true for all the points on our new parabola the new equation is ''y'' = (''x'' &amp;minus; 5)&lt;sup&gt;2&lt;/sup&gt;.

==See also==
* [[Vertical translation]]

==References==
{{Reflist}}
*Zazkis, R., Liljedahl, P., &amp; Gadowsky, K. Conceptions of function translation: obstacles, intuitions, and rerouting. Journal of Mathematical Behavior, 22, 437-450. Retrieved April 29, 2014, from www.elsevier.com/locate/jmathb
*[http://www.biology.arizona.edu/biomath/tutorials/transformations/horizontaltranslations.html Transformations of Graphs: Horizontal Translations]. (2006, January 1). BioMath: Transformation of Graphs. Retrieved April 29, 2014

[[Category:Functions and mappings]]
[[Category:Transformation (function)]]


{{maths-stub}}</text>
      <sha1>ekmi1phsij4z1ngatyfqvv1lk8l7vre</sha1>
    </revision>
  </page>
  <page>
    <title>Hypersequent</title>
    <ns>0</ns>
    <id>54262912</id>
    <revision>
      <id>858499084</id>
      <parentid>858499059</parentid>
      <timestamp>2018-09-07T16:29:07Z</timestamp>
      <contributor>
        <username>Citation bot</username>
        <id>7903804</id>
      </contributor>
      <minor/>
      <comment>Alter: isbn, template type, journal. Add: issue. Removed parameters. You can [[WP:UCB|use this bot]] yourself. [[WP:DBUG|Report bugs here]]. | [[User:Headbomb|Headbomb]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12809">In [[mathematical logic]], the '''hypersequent''' framework is an extension of the proof-theoretical framework of [[Sequent calculus|sequent calculi]] used in [[structural proof theory]] to provide [[Analytic proof|analytic calculi]] for logics which are not captured in the sequent framework. A hypersequent is usually taken to be a finite [[multiset]] of ordinary [[sequent]]s, written

: &lt;math&gt;\Gamma_1 \Rightarrow \Delta_1 \mid \cdots \mid \Gamma_n \Rightarrow \Delta_n&lt;/math&gt;  
The sequents making up a hypersequent are called components. The added expressivity of the hypersequent framework is provided by rules manipulating different components, such as the communication rule for [[intermediate logic]] LC (below left) or the modal splitting rule for [[modal logic]] [[S5 (modal logic)|S5]] (below right):&lt;ref name=":0"&gt;{{Cite book|last=Avron|first=Arnon|date=1996|title=The method of hypersequents in the proof theory of propositional non-classical logics|url=|journal=Logic: From Foundations to Applications|volume=|pages=1–32|isbn=978-0-19-853862-2|via=}}&lt;/ref&gt;

: &lt;math&gt;\frac{\Gamma_1 \Rightarrow \Delta_1 \mid \dots \mid \Gamma_n \Rightarrow \Delta_n \mid \Sigma \Rightarrow A \qquad \Omega_1 \Rightarrow \Theta_1 \mid \dots \mid \Omega_m \Rightarrow \Theta_m \mid \Pi \Rightarrow B}{\Gamma_1 \Rightarrow \Delta_1 \mid \dots \mid \Gamma_n \Rightarrow \Delta_n \mid \Omega_1 \Rightarrow \Theta_1 \mid \dots \mid \Omega_m \Rightarrow \Theta_m \mid \Sigma \Rightarrow B \mid \Pi \Rightarrow A}&lt;/math&gt;   &lt;math&gt;\frac{\Gamma_1 \Rightarrow \Delta_1 \mid \dots \mid \Gamma_n \Rightarrow \Delta_n \mid \Box \Sigma, \Theta \Rightarrow \Box \Pi, \Omega}{\Gamma_1 \Rightarrow \Delta_1 \mid \dots \mid \Gamma_n \Rightarrow \Delta_n \mid \Box \Sigma \Rightarrow \Box \Pi \mid \Theta \Rightarrow \Omega}&lt;/math&gt;

Hypersequent calculi have been used to treat [[modal logic]]s, [[intermediate logics]], and [[substructural logics]]. Hypersequents usually have a formula interpretation, i.e., are interpreted by a formula in the object language, nearly always as some kind of disjunction. The precise formula interpretation depends on the considered logic.

== Formal definitions and propositional rules ==
Formally, a hypersequent is usually taken to be a finite [[multiset]] of ordinary [[sequent]]s, written

: &lt;math&gt;\Gamma_1 \Rightarrow \Delta_1 \mid \dots \mid \Gamma_n \Rightarrow \Delta_n&lt;/math&gt;

The sequents making up a hypersequent consist of tuples of multisets of formulae, and are called the components of the hypersequent. Variants defining hypersequents and sequents in terms of sets or lists instead of multisets are also considered, and depending on the considered logic the sequents can be classical or intuitionistic. The rules for the propositional connectives usually are adaptions of the corresponding standard sequent rules with an additional side hypersequent, also called hypersequent context. E.g., a common set of rules for the [[functionally complete]] set of connectives &lt;math&gt;\{\bot,\to\}&lt;/math&gt; for [[classical propositional logic]] is given by the following four rules:

: &lt;math&gt;\frac{}{\mathcal{G} \mid \Gamma, p \Rightarrow p,\Delta} &lt;/math&gt;

: &lt;math&gt;\frac{}{\mathcal{G} \mid \Gamma, \bot \Rightarrow \Delta}&lt;/math&gt;

: &lt;math&gt;\frac{\mathcal{G} \mid \Gamma, B \Rightarrow \Delta
\qquad
\mathcal{G} \mid \Gamma \Rightarrow A, \Delta
}
{\mathcal{G}\mid \Gamma, A \to B \Rightarrow \Delta
}&lt;/math&gt;

&lt;math&gt;\frac{\mathcal{G} \mid \Gamma,A \Rightarrow B, \Delta
}
{\mathcal{G} \mid \Gamma \Rightarrow A \to B, \Delta
}&lt;/math&gt;

Due to the additional structure in the hypersequent setting the [[structural rule]]s are considered in their internal and external variants. The internal weakening and internal contraction rules are the adaptions of the corresponding sequent rules with an added hypersequent context:

: &lt;math&gt;\frac{\mathcal{G} \mid \Gamma \Rightarrow \Delta
}
{\mathcal{G}\mid \Gamma, \Sigma \Rightarrow \Delta, \Pi
}&lt;/math&gt;

&lt;math&gt;\frac{\mathcal{G} \mid \Gamma, A, A \Rightarrow \Delta
}
{\mathcal{G} \mid \Gamma, A \Rightarrow \Delta
}&lt;/math&gt;

&lt;math&gt;\frac{\mathcal{G} \mid \Gamma \Rightarrow A, A, \Delta
}
{\mathcal{G} \mid \Gamma \Rightarrow A, \Delta
}&lt;/math&gt;

The external weakening and external contraction rules are the corresponding rules on the level of hypersequent components instead of formulae:

&lt;math&gt;\frac{\mathcal{G}}{\mathcal{G} \mid \Gamma \Rightarrow\Delta}&lt;/math&gt;

&lt;math&gt;\frac{\mathcal{G} \mid \Gamma \Rightarrow\Delta \mid \Gamma\Rightarrow\Delta
}
{\mathcal{G} \mid \Gamma\Rightarrow\Delta
}&lt;/math&gt;

Soundness of these rules is closely connected to the formula interpretation of the hypersequent structure, nearly always as some form of [[disjunction]]. The precise formula interpretation depends on the considered logic, see below for some examples.

== Main examples ==

=== Modal logics ===
Hypersequents have been used to obtain analytic calculi for [[modal logic]]s, for which analytic [[Sequent calculus|sequent calculi]] proved elusive.  In the context of modal logics the standard formula interpretation of a hypersequent

: &lt;math&gt;\Gamma_1 \Rightarrow \Delta_1 \mid \dots \mid \Gamma_n \Rightarrow \Delta_n&lt;/math&gt;

is the formula

: &lt;math&gt;\Box (\bigwedge \Gamma_1 \to \bigvee \Delta_1) \lor \dots \lor \Box( \bigwedge\Gamma_n \to \bigvee\Delta_n)&lt;/math&gt;

Here if &lt;math&gt;\Gamma&lt;/math&gt; is the multiset &lt;math&gt;A_1, \dots, A_n&lt;/math&gt; we write &lt;math&gt;\Box \Gamma&lt;/math&gt; for the result of prefixing every formula in &lt;math&gt;\Gamma&lt;/math&gt; with &lt;math&gt;\Box&lt;/math&gt;, i.e., the multiset &lt;math&gt;\Box A_1, \dots, \Box A_n&lt;/math&gt;. Note that the single components are interpreted using the standard formula interpretation for sequents, and the hypersequent bar &lt;math&gt;\mid&lt;/math&gt; is interpreted as a disjunction of boxes.The prime example of a modal logic for which hypersequents provide an analytic calculus is the logic [[S5 (modal logic)|S5]]. In a standard hypersequent calculus for this logic&lt;ref name=":0" /&gt; the formula interpretation is as above, and the propositional and structural rules are the ones from the previous section. Additionally, the calculus contains the modal rules

: &lt;math&gt;\frac{\mathcal{G} \mid \Box \Gamma \Rightarrow A
}
{\mathcal{G} \mid \Box \Gamma \Rightarrow \Box A
}&lt;/math&gt;

&lt;math&gt;\frac{\mathcal{G} \mid \Gamma, A \Rightarrow \Delta
}
{\mathcal{G} \mid \Gamma, \Box A \Rightarrow \Delta
}&lt;/math&gt;

&lt;math&gt;\frac{\mathcal{G} \mid \Box \Gamma, \Sigma \Rightarrow \Box \Delta, \Pi
}
{\mathcal{G} \mid \Box \Gamma \Rightarrow \Box\Delta \mid \Sigma \Rightarrow \Pi
}&lt;/math&gt;

[[Admissible set|Admissibility]] of a suitably formulated version of the [[cut rule]] can be shown by a syntactic argument on the structure of derivations or by showing [[completeness (logic)|completeness]] of the calculus without the cut rule directly using the semantics of S5. In line with the importance of modal logic S5, a number of alternative calculi have been formulated.&lt;ref name=":4"&gt;{{Cite journal|last=Mints|first=Grigori|date=1971|title=On some calculi of modal logic|url=|journal=Proc. Steklov Inst. Of Mathematics|volume=98|pages=97–122|via=}}&lt;/ref&gt;&lt;ref name=":5"&gt;{{Cite journal|last=Pottinger|first=Garrell|date=1983|title=Uniform, cut-free formulations of T, S4 and S5 (abstract)|url=|journal=J. Symb. Log.|volume=48|issue=3|pages=900|via=}}&lt;/ref&gt;&lt;ref name=":0" /&gt;&lt;ref&gt;{{Cite journal|last=Poggiolesi|first=Francesca|date=2008|title=A cut-free simple sequent calculus for modal logic S5|url=|journal=Rev. Symb. Log.|volume=1|pages=3–15|via=}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Restall|first=Greg|date=2007|title=Proofnets for S5: Sequents and circuits for modal logic|url=|journal=Logic Colloquium 2005|series=Lecture Notes in Logic|volume=28|pages=151–172|via=}}&lt;/ref&gt;&lt;ref name=":1"&gt;{{Cite journal|last=Kurokawa|first=Hidenori|date=2014|title=Hypersequent Calculi for Modal Logics Extending S4|url=|journal=New Frontiers in Artificial Intelligence|series=Lecture Notes in Computer Science|volume=8417|pages=51–68|via=}}&lt;/ref&gt;&lt;ref name=":2"&gt;{{Cite book |doi = 10.1109/LICS.2013.47|chapter = From Frame Properties to Hypersequent Rules in Modal Logics|title = 2013 28th Annual ACM/IEEE Symposium on Logic in Computer Science|pages = 408–417|year = 2013|last1 = Lahav|first1 = Ori|isbn = 978-1-4799-0413-6}}&lt;/ref&gt; Hypersequent calculi have also been proposed for many other modal logics.&lt;ref name=":1" /&gt;&lt;ref name=":2" /&gt;&lt;ref&gt;{{Cite journal|last=Indrzejczak|first=Andrzej|date=2015|title=Eliminability of cut in hypersequent calculi for some modal logics of linear frames|url=|journal=Information Processing Letters|volume=115|issue=2|pages=75–81|doi=10.1016/j.ipl.2014.07.002|via=}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Lellmann|first=Björn|date=2016|title=Hypersequent rules with restricted contexts for propositional modal logics|url=|journal=Theor. Comput. Sci.|volume=656|pages=76–105|via=}}&lt;/ref&gt;

=== Intermediate logics ===
Hypersequent calculi based on intuitionistic or [[single-succedent sequents]] have been used successfully to capture a large class of [[intermediate logics]], i.e., extensions of [[Intuitionistic logic|intuitionistic propositional logic]]. Since the hypersequents in this setting are based on single-succedent sequents, they have the following form:

: &lt;math&gt;\Gamma_1 \Rightarrow A_1 \mid \dots \mid \Gamma_n \Rightarrow A_n&lt;/math&gt;

The standard formula interpretation for such an hypersequent is

: &lt;math&gt;(\bigwedge\Gamma_1 \to A_1) \lor \dots \lor (\bigwedge\Gamma_n \to A_n)&lt;/math&gt;

Most hypersequent calculi for intermediate logics include the single-succedent versions of the propositional rules given above, a selection of the structural rules. The characteristics of a particular intermediate logic are mostly captured using a number of additional [[structural rule]]s. E.g., the standard calculus for intermediate logic [[LC (modal logic)|LC]], sometimes also called Gödel–Dummett logic, contains additionally the so-called communication rule:&lt;ref name=":0" /&gt;

: &lt;math&gt;\frac{\Gamma_1 \Rightarrow \Delta_1 \mid \dots \mid \Gamma_n \Rightarrow \Delta_n \mid \Sigma \Rightarrow A \qquad \Omega_1 \Rightarrow \Theta_1 \mid \dots \mid \Omega_m \Rightarrow \Theta_m \mid \Pi \Rightarrow B}{\Gamma_1 \Rightarrow \Delta_1 \mid \dots \mid \Gamma_n \Rightarrow \Delta_n \mid \Omega_1 \Rightarrow \Theta_1 \mid \dots \mid \Omega_m \Rightarrow \Theta_m \mid \Sigma \Rightarrow B \mid \Pi \Rightarrow A}&lt;/math&gt;

Hypersequent calculi for many other intermediate logics have been introduced,&lt;ref name=":0" /&gt;&lt;ref&gt;{{Cite journal|last=Ciabattoni|first=Agata|last2=Ferrari|first2=Mauro|date=2001|title=Hypersequent calculi for some intermediate logics with bounded Kripke models|url=|journal=J. Log. Comput.|volume=11|pages=283–294|via=}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Ciabattoni|first=Agata|last2=Maffezioli|first2=Paolo|last3=Spendier|first3=Lara|date=2013|editor-last=Galmiche|editor-first=Didier|editor2-last=Larchey-Wendling|editor2-first=Dominique|title=Hypersequent and Labelled Calculi for Intermediate Logics|url=|journal=Tableaux 2013|volume=|pages=81–96|via=}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Baaz|first=Matthias|last2=Ciabattoni|first2=Agata|last3=Fermüller|first3=Christian G.|date=2003|title=Hypersequent Calculi for Gödel Logics – A Survey|url=https://academic.oup.com/logcom/article-abstract/13/6/835/971703/Hypersequent-Calculi-for-Godel-Logics-a-Survey?redirectedFrom=fulltext|journal=J. Log. Comput.|volume=13|pages=835–861|via=}}&lt;/ref&gt; and there are very general results about [[cut elimination]] in such calculi.&lt;ref name=":3"&gt;{{Cite book |doi = 10.1109/LICS.2008.39|chapter = From Axioms to Analytic Rules in Nonclassical Logics|title = 2008 23rd Annual IEEE Symposium on Logic in Computer Science|pages = 229–240|year = 2008|last1 = Ciabattoni|first1 = Agata|last2 = Galatos|first2 = Nikolaos|last3 = Terui|first3 = Kazushige|isbn = 978-0-7695-3183-0|citeseerx = 10.1.1.405.8176}}&lt;/ref&gt;

=== Substructural logics ===
As for intermediate logics, hypersequents have been used to obtain analytic calculi for many [[substructural logics]] and [[fuzzy logics]].&lt;ref name=":0" /&gt;&lt;ref name=":3" /&gt;&lt;ref&gt;{{Cite book|title=Proof theory for fuzzy logics|last=Metcalfe|first=George|last2=Olivetti|first2=Nicola|last3=Gabbay|first3=Dov|publisher=Springer, Berlin|year=2008|isbn=|location=|pages=}}&lt;/ref&gt;

== History ==
The hypersequent structure seem to have appeared first in&lt;ref name=":4" /&gt; under the name of cortege to obtain a calculus for modal logic [[S5 (modal logic)|S5]]. It seems to have been developed independently in,&lt;ref name=":5" /&gt; also for treating modal logics, and in the influential,&lt;ref name=":0" /&gt; where calculi for modal, intermediate and substructural logics are considered, and the term hypersequent is introduced.

== References ==
&lt;references /&gt;

[[Category:Proof theory]]</text>
      <sha1>eyekhm3vh5f73js647oj7pe82h2ppkv</sha1>
    </revision>
  </page>
  <page>
    <title>Hypotenuse</title>
    <ns>0</ns>
    <id>251366</id>
    <revision>
      <id>868678456</id>
      <parentid>862655288</parentid>
      <timestamp>2018-11-13T19:01:23Z</timestamp>
      <contributor>
        <ip>2602:306:CDD1:2D00:CDF:BC4F:9B15:4EB5</ip>
      </contributor>
      <comment>Minor</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6668">{{One source|date=November 2010}}
[[File:Hypotenuse.svg|thumb|150px|Aright|A right-angled triangle and its hypotenuse.]]
In [[geometry]], a '''hypotenuse'''  is the longest side of a [[Right triangle|right-angled triangle]], the side opposite of the [[right angle]]. The [[length]] of the hypotenuse of a [[right triangle]] can be found using the [[Pythagorean theorem]], which states that the [[Square (algebra)|square]] of the length of the hypotenuse equals the sum of the squares of the lengths of the other two sides.
For example, if one of the other sides has a length of 3 (when squared, 9) and the other has a length of 4 (when squared, 16), then their squares add up to 25. The length of the hypotenuse is the [[square root]] of 25, that is, 5.

==Etymology==
{{wiktionary|ὑποτείνουσα}}
The word ''hypotenuse'' is derived from [[Ancient Greek|Greek]] {{lang|grc|ἡ τὴν ὀρθὴν γωνίαν &lt;u&gt;ὑποτείνουσα&lt;/u&gt;}} (sc. {{lang|grc|γραμμή}} or {{lang|grc|πλευρά}}), meaning "[side] &lt;u&gt;subtending&lt;/u&gt; the right angle" ([[Apollodorus of Athens|Apollodorus]]),&lt;ref&gt;{{LSJ|u(potei/nw}}, {{LSJ|u(po/}}, {{LSJ|tei/nw}}, {{LSJ|pleura/|ref}}&lt;/ref&gt; 
{{lang|grc|ὑποτείνουσα}} ''hupoteinousa'' being the feminine present active participle of the verb {{lang|grc|ὑποτείνω}} ''hupo-teinō'' "to stretch below, to subtend", from {{lang|grc|τείνω}} ''teinō'' "to stretch, extend". The nominalised participle, {{lang|grc|ἡ ὑποτείνουσα}},  was used for the hypotenuse of a triangle in the 4th century BC (attested in  [[Plato]], ''[[Timaeus (dialogue)|Timaeus]]'' 54d).
The Greek term was [[Romanization of Greek|loaned]] into  [[Late Latin]],&lt;!--"Late Latin" is from etymonline, without specification; we have evidence for the term being used in New Latin, but so far we have no reference to actual Late Latin--&gt; 
as ''hypotēnūsa''. 
Adoption as a learned Latinism used in modern languages dates to the 16th century.&lt;ref&gt;{{OEtymD|hypotenuse}}.
E.g. Z. Lochner, ''Tractätlein, darinnen etliche schöne Exempel, auss der Geometria, etc.'', Nuremberg (1583), [https://books.google.ch/books?id=deZmAAAAcAAJ&amp;pg=PA64  p. 64]: "''Qudarir den lengern ''Cathetum'', als 24. und sein ''basis'' als 7. wirdt 576. unnd 49. addir die 2. Quadrat/ wirdt 625. unnd ist das Quadrat der ''Hypotenusa'' [...]"&lt;/ref&gt; The spelling in ''-e'', as ''hypotenuse'', is French in origin ([[Estienne de La Roche]] 1520).&lt;ref&gt;Estienne de La Roche, ''l'Arismetique'' (1520), fol. 221r (cited after [http://www.cnrtl.fr/etymologie/hypotenuse TLFi]).&lt;/ref&gt;

&lt;!--WP:DUE
A [[folk etymology]] mentioned in the 1940s incorrectly claims that ''tenuse'' means "side" and ''hypotenuse'' means a support like a prop or [[buttress]].&lt;ref&gt;{{cite book |title=Romping Through Mathematics |last=Anderson |first=Raymond |coauthors= |year=1947 |publisher=Faber |location= |isbn= |pages=52}}&lt;/ref&gt;

[[Merriam-Webster's Collegiate Dictionary]]{{year needed|date=October 2018}} offers the alternative unetymological spelling ''hypothenuse'', but this is very rarely seen.
--&gt;

==Calculating the hypotenuse==
[[File:Triangle Sides.svg|200px|frame|right|A right-angled triangle and its hypotenuse, ''h'', along with [[Cathetus|catheti]] (legs) ''c&lt;sub&gt;1&lt;/sub&gt;'' and ''c&lt;sub&gt;2&lt;/sub&gt;''.]]

The length of the hypotenuse is calculated using the [[square root]] function implied by the [[Pythagorean theorem]]. Using the common notation that the length of the two legs of the triangle (the sides perpendicular to each other) are ''a'' and ''b'' and that of the hypotenuse is ''c'', we have

:&lt;math&gt;c = \sqrt { a^2 + b^2 } .&lt;/math&gt;

The Pythagorean theorem, and hence this length, can also be derived from the [[law of cosines]] by observing that the angle opposite the hypotenuse is 90&amp;deg; and noting that its cosine is 0:

:&lt;math&gt;c^2 = a^2 + b^2 - 2ab\cos90^\circ = a^2 + b^2 \therefore c = \sqrt{a^2 + b^2}.&lt;/math&gt;

Many computer languages support the ISO C standard function hypot(''x'',''y''), which returns the value above. The function is designed not to fail where the straightforward calculation might overflow or underflow and can be slightly more accurate.

Some scientific calculators provide a function to convert from [[rectangular coordinates]] to [[polar coordinates]]. This gives both the length of the hypotenuse and the [[angle]] the hypotenuse makes with the base line (''c&lt;sub&gt;1&lt;/sub&gt;'' above) at the same time when given ''x'' and ''y''. The angle returned is normally given by [[atan2]](''y'',''x'').

== Properties ==
[[File:Triângulo retângulo.svg|thumb|225px|right|In the figure, '''a''' is the hypotenuse and '''b''' and '''c''' are the catheti. The orthographic projection of '''b''' is '''m''', and of '''c''' is '''n'''.]]

[[Orthographic projection]]s:

* The length of the hypotenuse equals the sum  of the lengths  of the orthographic projections of both catheti. And
* The square of the length of a cathetus equals the [[Product (mathematics)|product]] of the lengths of its orthographic projection on the hypotenuse times the length of this.

::'''b² = a · m'''
::'''c² = a · n'''

* Also, the length of a cathetus '''b''' is the proportional mean between the lengths of its projection '''m''' and the hypotenuse '''a'''.

::'''a/b = b/m'''
::'''a/c = c/n'''

== Trigonometric ratios ==

By means of [[Trigonometry|trigonometric ratios]], one can obtain the value of two acute angles, &lt;math&gt;\alpha\,&lt;/math&gt; and &lt;math&gt; \beta\,&lt;/math&gt;, of the right triangle.

Given the length of the hypotenuse &lt;math&gt; c\,&lt;/math&gt; and of a cathetus &lt;math&gt; b\,&lt;/math&gt;, the ratio is:

[[File:Euklidova veta.svg|330px|right]]

:::&lt;math&gt; \frac{b}{c} = \sin (\beta)\,&lt;/math&gt;

The trigonometric inverse function is:

:::&lt;math&gt; \beta\ = \arcsin\left(\frac {b}{c} \right)\,&lt;/math&gt;
in which &lt;math&gt;\beta\,&lt;/math&gt; is the angle opposite the cathetus &lt;math&gt; b\,&lt;/math&gt;.

The adjacent angle of the catheti &lt;math&gt; b\,&lt;/math&gt; is &lt;math&gt;\alpha\,&lt;/math&gt; = 90° – &lt;math&gt;\beta\,&lt;/math&gt;

One may also obtain the value of the angle &lt;math&gt;\beta\,&lt;/math&gt; by the equation:

:::&lt;math&gt; \beta\ = \arccos\left(\frac {a}{c} \right)\,&lt;/math&gt;

in which &lt;math&gt; a\,&lt;/math&gt; is the other cathetus.

==See also==
*[[Cathetus]]
*[[Triangle]]
*[[Space diagonal]]
*[[Nonhypotenuse number]]
*[[Taxicab geometry]]
*[[Trigonometry]]
*[[Special right triangles]]
*[[Pythagoras]]

== Notes ==
{{Reflist}}

== References ==
* [http://www.encyclopediaofmath.org/index.php/Hypotenuse ''Hypotenuse'' at Encyclopaedia of Mathematics]
* {{mathworld|urlname=Hypotenuse|title=Hypotenuse}}

[[Category:Elementary geometry]]
[[Category:Triangle geometry]]
[[Category:Trigonometry]]</text>
      <sha1>bngc71azor3y03i22vxtvgzre5qdu18</sha1>
    </revision>
  </page>
  <page>
    <title>IBM 4758</title>
    <ns>0</ns>
    <id>1222310</id>
    <revision>
      <id>760786521</id>
      <parentid>678714660</parentid>
      <timestamp>2017-01-19T01:50:14Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor/>
      <comment>/* External links */HTTP&amp;rarr;HTTPS for [[Yahoo!]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1701">[[Image:IBM4758 outside1.JPG|thumb|128px|IBM 4758]]
The '''IBM 4758''' PCI Cryptographic Coprocessor is a [[secure cryptoprocessor]] implemented on a high-security, tamper resistant, programmable [[Peripheral Component Interconnect|PCI]] board. Specialized cryptographic electronics, [[microprocessor]], [[Random Access Memory|memory]], and [[random number generator]] housed within a tamper-responding environment provide a highly secure subsystem in which [[data processing]] and [[cryptography]] can be performed. 

IBM supplies two cryptographic-system implementations, and toolkits for custom application development:
* The [[PKCS11|PKCS#11]], version 2.01 implementation creates a high-security solution for application programs developed for this industry-standard API. 
* The IBM [[Common Cryptographic Architecture]] implementation provides many functions of special interest in the finance industry and a base on which custom processing and cryptographic functions can be added. 

As of June 2005, the 4758 was discontinued and was replaced by an improved, faster model called the [[IBM 4764]].

==External links== 
*[http://www.ibm.com/security/cryptocards/ IBM CryptoCards]
*[http://www.cl.cam.ac.uk/~rnc1/descrack/ How to hack one that's running an old CCA version]
*[http://www.cl.cam.ac.uk/users/rja14/talks/birmingham.ppt Satan’s Computer Revisited: API Security explains flaws]
*[http://www.puff65537.com/Home/4758-tear-down Puff65537's IBM 4758 teardown]
*[https://www.flickr.com/photos/flyingspores/6330246119/in/set-72157628094589706 Teardown of an IBM 4758 that is FIPS-140 level 4 Certified]

{{crypto-stub}}

[[Category:Cryptographic hardware]]
[[Category:IBM computers|4758]]</text>
      <sha1>43svgxq54h8zo5vzaz5i8gkpgzzt0lm</sha1>
    </revision>
  </page>
  <page>
    <title>Krichevsky–Trofimov estimator</title>
    <ns>0</ns>
    <id>26292477</id>
    <revision>
      <id>848052695</id>
      <parentid>843245437</parentid>
      <timestamp>2018-06-29T14:00:30Z</timestamp>
      <contributor>
        <username>JCW-CleanerBot</username>
        <id>31737083</id>
      </contributor>
      <minor/>
      <comment>/* top */[[User:JCW-CleanerBot#Logic|task]], replaced: IEEE Trans. Information Theory → IEEE Trans. Inf. Theory using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1071">In [[information theory]], given an unknown [[Stationary process|stationary]] source &amp;pi; with alphabet ''A'' and a sample ''w'' from &amp;pi;, the '''Krichevsky–Trofimov (KT) estimator''' produces an estimate p&lt;sub&gt;i&lt;/sub&gt;(''w'') of the probability of each symbol ''i''&amp;nbsp;&amp;isin;&amp;nbsp;''A''. This estimator is optimal in the sense that it minimizes the worst-case [[Regret (decision theory)|regret]] asymptotically.

For a binary alphabet and a string ''w'' with ''m'' zeroes and ''n'' ones, the KT estimator p&lt;sub&gt;i&lt;/sub&gt;(''w'') is defined as:&lt;ref&gt;Krichevsky, R. E. and Trofimov V. K. (1981), "The Performance of Universal Encoding", IEEE Trans. Inf. Theory, Vol. IT-27, No. 2, pp. 199–207.&lt;/ref&gt;

: &lt;math&gt;
\begin{align}
 p_0(w) &amp;= \dfrac{m + 1/2}{m + n + 1}, \\
 p_1(w) &amp;= \dfrac{n + 1/2}{m + n + 1}. \\
\end{align}
&lt;/math&gt;

==See also==

* [[Rule of succession]]
* [[Dirichlet-multinomial distribution]]

==References==
&lt;references/&gt;

{{DEFAULTSORT:Krichevsky-Trofimov estimator}}
[[Category:Information theory]]
[[Category:Data compression]]


{{probability-stub}}</text>
      <sha1>dnxd6tynyfj1mqakz8189ie2f8irvj5</sha1>
    </revision>
  </page>
  <page>
    <title>Lagrange multipliers on Banach spaces</title>
    <ns>0</ns>
    <id>10399346</id>
    <revision>
      <id>844963392</id>
      <parentid>825225862</parentid>
      <timestamp>2018-06-08T11:39:06Z</timestamp>
      <contributor>
        <ip>2A00:8A60:C000:1:5A67:1240:1808:4FA7</ip>
      </contributor>
      <comment>Fixed broken PlanetMath link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4001">In the field of [[calculus of variations]] in [[mathematics]], the method of '''Lagrange multipliers on Banach spaces''' can be used to solve certain infinite-dimensional [[constraint (mathematics)|constrained]] [[optimization (mathematics)|optimization problems]]. The method is a generalization of the classical method of [[Lagrange multipliers]] as used to find [[extremum|extrema]] of a [[function (mathematics)|function]] of finitely many variables.

==The Lagrange multiplier theorem for Banach spaces==
Let ''X'' and ''Y'' be [[real number|real]] [[Banach space]]s. Let ''U'' be an [[open set|open subset]] of ''X'' and let ''f'' : ''U'' → '''R''' be a continuously [[differentiable function]]. Let ''g'' : ''U'' → ''Y'' be another continuously differentiable function, the ''constraint'': the objective is to find the extremal points (maxima or minima) of ''f'' subject to the constraint that ''g'' is zero.

Suppose that ''u''&lt;sub&gt;0&lt;/sub&gt; is a ''constrained extremum'' of ''f'', i.e. an extremum of ''f'' on

:&lt;math&gt;g^{-1} (0) = \{ x \in U \mid g(x) = 0 \in Y \} \subseteq U.&lt;/math&gt;

Suppose also that the [[Fréchet derivative]] D''g''(''u''&lt;sub&gt;0&lt;/sub&gt;) : ''X'' → ''Y'' of ''g'' at ''u''&lt;sub&gt;0&lt;/sub&gt; is a [[surjective]] [[linear map]]. Then there exists a '''Lagrange multiplier''' ''λ'' : ''Y'' → '''R''' in ''Y''&lt;sup&gt;∗&lt;/sup&gt;, the [[dual space]] to ''Y'', such that

:&lt;math&gt;\mathrm{D} f (u_{0}) = \lambda \circ \mathrm{D} g (u_{0}). \quad \mbox{(L)}&lt;/math&gt;

Since D''f''(''u''&lt;sub&gt;0&lt;/sub&gt;) is an element of the dual space ''X''&lt;sup&gt;∗&lt;/sup&gt;, equation (L) can also be written as

:&lt;math&gt;\mathrm{D} f (u_{0}) = \left( \mathrm{D} g (u_{0}) \right)^{*} (\lambda),&lt;/math&gt;

where (D''g''(''u''&lt;sub&gt;0&lt;/sub&gt;))&lt;sup&gt;∗&lt;/sup&gt;(''λ'') is the [[pullback]] of ''λ'' by D''g''(''u''&lt;sub&gt;0&lt;/sub&gt;), i.e. the action of the [[adjoint]] map (D''g''(''u''&lt;sub&gt;0&lt;/sub&gt;))&lt;sup&gt;∗&lt;/sup&gt; on ''λ'', as defined by

:&lt;math&gt;\left( \mathrm{D} g (u_{0}) \right)^{*} (\lambda) = \lambda \circ \mathrm{D} g (u_{0}).&lt;/math&gt;

==Connection to the finite-dimensional case==
In the case that ''X'' and ''Y'' are both finite-dimensional (i.e. [[linear isomorphism|linearly isomorphic]] to '''R'''&lt;sup&gt;''m''&lt;/sup&gt; and '''R'''&lt;sup&gt;''n''&lt;/sup&gt; for some [[natural numbers]] ''m'' and ''n'') then writing out equation (L) in [[matrix (mathematics)|matrix]] form shows that ''λ'' is the usual Lagrange multiplier vector; in the case ''n'' = 1, ''λ'' is the usual Lagrange multiplier, a real number.

==Application==
In many optimization problems, one seeks to minimize a functional defined on an infinite-dimensional space such as a Banach space.

Consider, for example, the [[Sobolev space]] ''X'' = ''H''&lt;sub&gt;0&lt;/sub&gt;&lt;sup&gt;1&lt;/sup&gt;([&amp;minus;1, +1]; '''R''') and the functional ''f'' : ''X'' → '''R''' given by

:&lt;math&gt;f(u) = \int_{-1}^{+1} u'(x)^{2} \, \mathrm{d} x.&lt;/math&gt;

Without any constraint, the minimum value of ''f'' would be 0, attained by ''u''&lt;sub&gt;0&lt;/sub&gt;(''x'') = 0 for all ''x'' between &amp;minus;1 and +1. One could also consider the constrained optimization problem, to minimize ''f'' among all those ''u'' ∈ ''X'' such that the mean value of ''u'' is +1. In terms of the above theorem, the constraint ''g'' would be given by

:&lt;math&gt;g(u) = \frac{1}{2} \int_{-1}^{+1} u(x) \, \mathrm{d} x - 1.&lt;/math&gt;

However this problem can be solved as in the finite dimensional case since the Lagrange multiplier &lt;math&gt; \lambda &lt;/math&gt; is only a scalar.

==See also==
* [[Pontryagin's minimum principle]], Hamiltonian method in calculus of variations

==References==
* {{cite book | title=Applied functional analysis: Variational Methods and Optimization | last=Zeidler | first=Eberhard | publisher=Springer-Verlag | year=1995 | isbn=978-1-4612-9529-7 | series=Applied Mathematical Sciences 109 | location=New York, NY }} (See Section 4.14, pp.270&amp;ndash;271.)

{{PlanetMath attribution| urlname = LagrangeMultipliersOnBanachSpaces |title=Lagrange multipliers on Banach spaces}}

[[Category:Calculus of variations]]</text>
      <sha1>k2y1b3mew919y75jp4lnifiyeriams5</sha1>
    </revision>
  </page>
  <page>
    <title>Leggett inequality</title>
    <ns>0</ns>
    <id>27478537</id>
    <revision>
      <id>808365009</id>
      <parentid>765841889</parentid>
      <timestamp>2017-11-02T12:45:38Z</timestamp>
      <contributor>
        <ip>82.137.10.247</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2655">The '''Leggett inequalities''',&lt;ref name="Leggett"&gt;Nonlocal Hidden-Variable Theories and Quantum Mechanics: An Incompatibility Theorem. A. J. Leggett, ''Found. of Phys.'', '''33''', 1469 (2003).&lt;/ref&gt; named for [[Anthony James Leggett]], who derived them, are a related pair of mathematical expressions concerning the correlations of properties of [[quantum entanglement|entangled]] particles.  (As published by Leggett, the inequalities were exemplified in terms of relative angles of elliptical and linear [[polarization (waves)|polarizations]].)  They are fulfilled by a large class of physical theories based on particular [[Quantum nonlocality|non-local]] and [[Philosophical realism|realistic]] assumptions, that may be considered to be plausible or intuitive according to common physical [[reasoning]].

The Leggett inequalities are violated by [[quantum mechanics|quantum mechanical theory]].  The results of experimental tests in 2007 and 2010 have shown agreement with quantum mechanics rather than the Leggett inequalities.&lt;ref name="Gröblacher"&gt;An experimental test of non-local realism. Gröblacher, et al., ''Nature'', '''446''', 871 (2007).&lt;/ref&gt;&lt;ref name="Romero"&gt;Violation of Leggett inequalities in orbital angular momentum subspaces. Romero, et al., ''New J. Phys.'', '''12''', 123007 (2010).&lt;/ref&gt; Given that experimental tests of [[Bell's inequalities]] have ruled out [[local realism]] in quantum mechanics, the violation of Leggett's inequalities is considered to have falsified [[Philosophical realism|realism]] in quantum mechanics.&lt;ref&gt;http://physicsworld.com/cws/article/news/2007/apr/20/quantum-physics-says-goodbye-to-reality&lt;/ref&gt; In quantum mechanics "realism" means "notion that physical systems possess complete sets of definite values for various parameters prior to, and independent of, measurement".&lt;ref&gt;http://journals.aps.org/prl/abstract/10.1103/PhysRevLett.117.050402&lt;/ref&gt;

==See also==
* [[Leggett-Garg inequality]]

==References==
{{reflist}}

==External links==
* [http://seedmagazine.com/content/article/the_reality_tests/ "The Reality Tests", Joshua Roebke, SEED, June 2008.]
* [http://vcq.quantum.at/publications/all-publications/details/572.html "A quantum renaissance", Markus Aspelmeyer and Anton Zeilinger, Physics World, July 2008.]
* [http://physicsworld.com/cws/article/news/44580 "Quantum theory survives latest challenge", Kate McAlpine, Physics World, December 2010.]

{{DEFAULTSORT:Leggett Inequality}}
[[Category:Concepts in physics]]
[[Category:Quantum information science]]
[[Category:Quantum measurement]]
[[Category:Physics theorems]]
[[Category:Quantum mechanics]]
[[Category:Inequalities]]</text>
      <sha1>g9c0v8ls0u69d4x9pp36z6jojgz4cja</sha1>
    </revision>
  </page>
  <page>
    <title>Martingale pricing</title>
    <ns>0</ns>
    <id>5257795</id>
    <revision>
      <id>855920988</id>
      <parentid>855920892</parentid>
      <timestamp>2018-08-21T18:31:23Z</timestamp>
      <contributor>
        <username>Geoff918</username>
        <id>5035838</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v2.0beta8)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3846">'''Martingale pricing''' is a pricing approach based on the notions of [[Martingale (probability theory)|martingale]] and [[risk-neutral measure|risk neutrality]]. The martingale pricing approach is a cornerstone of modern quantitative finance and can be applied to a variety of [[derivative (finance)|derivatives]] contracts, e.g. [[option (finance)|options]], [[Futures contract|futures]], [[interest rate derivative]]s, [[credit derivatives]], etc.

In contrast to the [[Partial differential equation|PDE]] approach to pricing, martingale pricing formulae are in the form of expectations which can be efficiently solved numerically using a [[Monte Carlo method|Monte Carlo]] approach. As such, Martingale pricing is preferred when valuing high-dimensional contracts such as a basket of options. On the other hand, valuing [[American option|American-style contracts]] is troublesome and requires discretizing the problem (making it like a [[Bermudan option]]) and only in 2001 [[Francis Longstaff|F. A. Longstaff]] and [[Eduardo Schwartz|E. S. Schwartz]] developed a practical Monte Carlo method for pricing American options.&lt;ref&gt;{{cite journal|last1=Longstaff|first1=F.A.|first2=E.S.|last2=Schwartz|url=http://repositories.cdlib.org/anderson/fin/1-01/|accessdate=October 8, 2011|title=Valuing American options by simulation: a simple least squares approach|journal=Review of Financial Studies|volume=14|year=2001|pages=113–148|doi=10.1093/rfs/14.1.113|archive-url=https://web.archive.org/web/20091016055122/http://repositories.cdlib.org/anderson/fin/1-01/|archive-date=2009-10-16|dead-url=no|df=}}&lt;/ref&gt;

==Measure theory representation==

Suppose the state of the market can be represented by the [[filtered probability space]],&lt;math&gt;(\Omega,(\mathcal{F}_{t})_{t\in[0,T]},\tilde{\mathbb{P}})&lt;/math&gt;. Let &lt;math&gt;\{S(t)\}_{t\in[0,T]} &lt;/math&gt; be a stochastic price process on this space. One may price a derivative security, &lt;math&gt;V(t,S(t))&lt;/math&gt; under the philosophy of no arbitrage as,

&lt;center&gt;&lt;math&gt;D(t)V(t,S(t))=\tilde{\mathbb{E}}[D(T)V(T,S(T))|\mathcal{F}_t], \qquad dD(t)=-r(t)D(t) \ dt&lt;/math&gt;&lt;/center&gt;

where &lt;math&gt;\tilde{\mathbb{P}}&lt;/math&gt; is the [[risk-neutral measure]].

&lt;math&gt;(r(t))_{t\in [0,T]}&lt;/math&gt; is an &lt;math&gt;\mathcal{F}_t&lt;/math&gt;-measurable (risk-free, possibly stochastic) interest rate process.

This is accomplished through [[almost surely|almost sure]] replication of the derivative's time &lt;math&gt;T&lt;/math&gt; payoff using only underlying securities, and the risk-free money market (MMA). These underlyings have prices that are observable and known.
Specifically, one constructs a portfolio process &lt;math&gt;\{X(t)\}_{t\in[0,T]}&lt;/math&gt; in continuous time, where he holds &lt;math&gt;\Delta(t)&lt;/math&gt; shares of the underlying stock at each time &lt;math&gt;t&lt;/math&gt;, and &lt;math&gt;X(t)-\Delta(t)S(t)&lt;/math&gt; cash earning the risk-free rate &lt;math&gt;r(t)&lt;/math&gt;. The portfolio obeys the stochastic differential equation

&lt;math&gt;dX(t)=\Delta(t) \ dS(t) + r(t)(X(t)-\Delta(t)S(t)) \ dt&lt;/math&gt;

One will then attempt to apply [[Girsanov theorem]] by first computing &lt;math&gt;\frac{d\tilde{\mathbb{P}}}{d\mathbb{P}}&lt;/math&gt;; that is, the [[Radon–Nikodym derivative]] with respect to the observed market probability distribution. This ensures that the discounted replicating portfolio process is a Martingale under risk neutral conditions.

If such a process &lt;math&gt;\Delta(t)&lt;/math&gt; can be well-defined and constructed, then choosing &lt;math&gt;V(0,S(0))=X(0)&lt;/math&gt; will result in &lt;math&gt;\tilde{\mathbb{P}}[X(T)=V(T)] = 1&lt;/math&gt;, which immediately implies that this happens &lt;math&gt;\mathbb{P}&lt;/math&gt;-[[almost surely]] as well, since the two measures are equivalent.

==See also==
* [[Martingale (probability theory)]]

==References==
{{Reflist}}

{{DEFAULTSORT:Martingale Pricing}}
[[Category:Finance theories]]
[[Category:Mathematical finance]]
[[Category:Pricing]]</text>
      <sha1>jqnqqchzbozk5zixdnu0qzybneqzm4x</sha1>
    </revision>
  </page>
  <page>
    <title>Measure-preserving dynamical system</title>
    <ns>0</ns>
    <id>398931</id>
    <revision>
      <id>858489283</id>
      <parentid>827243085</parentid>
      <timestamp>2018-09-07T15:06:37Z</timestamp>
      <contributor>
        <username>Krauss</username>
        <id>1222358</id>
      </contributor>
      <minor/>
      <comment>add link and cite application</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9037">In [[mathematics]], a '''measure-preserving dynamical system''' is an object of study in the abstract formulation of [[dynamical systems]], and [[ergodic theory]] in particular.

Despite the name, there are important "static applications", as in the [[Map_projection#Equal-area|Equal-area map projections]]. 

==Definition==
A measure-preserving dynamical system is defined as a [[probability space]] and a measure-preserving transformation on it. In more detail, it is a system

:&lt;math&gt;(X, \mathcal{B}, \mu, T)&lt;/math&gt;

with the following structure:

*&lt;math&gt;X&lt;/math&gt; is a set,
*&lt;math&gt;\mathcal B&lt;/math&gt; is a [[sigma-algebra|&amp;sigma;-algebra]] over &lt;math&gt;X&lt;/math&gt;,
*&lt;math&gt;\mu:\mathcal{B}\rightarrow[0,1]&lt;/math&gt; is a [[probability measure]], so that μ(''X'') = 1, and μ(∅) = 0,
*&lt;math&gt; T:X \rightarrow X&lt;/math&gt; is a [[measurable function|measurable]] transformation which preserves the measure &lt;math&gt;\mu&lt;/math&gt;, i.e., &lt;math&gt;\forall A\in \mathcal{B}\;\; \mu(T^{-1}(A))=\mu(A) &lt;/math&gt;.

This definition can be generalized to the case in which ''T'' is not a single transformation that is iterated to give the dynamics of the system, but instead is a [[monoid]] (or even a [[group (mathematics)|group]]) of transformations ''T&lt;sub&gt;s&lt;/sub&gt;'' : ''X'' → ''X'' parametrized by ''s'' ∈ '''Z''' (or '''R''', or '''N''' ∪ {0}, or [0, +∞)), where each transformation ''T&lt;sub&gt;s&lt;/sub&gt;'' satisfies the same requirements as ''T'' above. In particular, the transformations obey the rules:
* &lt;math&gt;T_0 = id_X :X \rightarrow X&lt;/math&gt;, the [[identity function]] on ''X'';
* &lt;math&gt;T_{s} \circ T_{t} = T_{t + s}&lt;/math&gt;, whenever all the terms are [[well-defined]];
* &lt;math&gt;T_{s}^{-1} = T_{-s}&lt;/math&gt;, whenever all the terms are well-defined.

The earlier, simpler case fits into this framework by defining''T&lt;sub&gt;s&lt;/sub&gt;'' = ''T&lt;sup&gt;s&lt;/sup&gt;'' for ''s'' ∈ '''N'''.

The existence of invariant measures for certain maps and Markov processes is established by the [[Krylov–Bogolyubov theorem]].

==Examples==
[[Image:Exampleergodicmap.svg|thumb|Example of a ([[Lebesgue measure]]) preserving map: ''T'' : [0,1) → [0,1), &lt;math&gt;x \mapsto 2x \mod 1.&lt;/math&gt;]] Examples include:

* μ could be the normalized angle measure dθ/2π on the [[unit circle]], and ''T'' a rotation. See [[equidistribution theorem]];
* the [[Bernoulli scheme]];
* the [[interval exchange transformation]];
* with the definition of an appropriate measure, a [[subshift of finite type]];
* the [[base flow (random dynamical systems)|base flow]] of a [[random dynamical system]];
* the flow of a Hamiltonian vector field on the tangent bundle of a closed connected smooth manifold is measure-preserving (using the measure induced on the Borel sets by the [[Volume_ form#Symplectic manifolds|symplectic volume form]]) by [[Liouville's theorem (Hamiltonian)]].

==Homomorphisms==
The concept of a [[homomorphism]] and an [[isomorphism]] may be defined.

Consider two dynamical systems &lt;math&gt;(X, \mathcal{A}, \mu, T)&lt;/math&gt; and &lt;math&gt;(Y, \mathcal{B}, \nu, S)&lt;/math&gt;. Then a mapping

:&lt;math&gt;\varphi:X \to Y&lt;/math&gt;

is a '''homomorphism of dynamical systems''' if it satisfies the following three properties:

# The map &lt;math&gt;\varphi\ &lt;/math&gt; is [[measurable function|measurable]].
# For each &lt;math&gt;B \in \mathcal{B}&lt;/math&gt;, one has &lt;math&gt;\mu (\varphi^{-1}B) = \nu(B)&lt;/math&gt;.
# For [[almost everywhere|μ-almost all]] &lt;math&gt;x \in X&lt;/math&gt;, one has &lt;math&gt;\varphi(Tx) = S(\varphi x)&lt;/math&gt;.

The system &lt;math&gt;(Y, \mathcal{B}, \nu, S)&lt;/math&gt; is then called a '''factor''' of &lt;math&gt;(X, \mathcal{A}, \mu, T)&lt;/math&gt;.

The map &lt;math&gt;\varphi\;&lt;/math&gt; is an '''isomorphism of dynamical systems''' if, in addition, there exists another mapping

:&lt;math&gt;\psi:Y \to X&lt;/math&gt;

that is also a homomorphism, which satisfies

# for μ-almost all &lt;math&gt;x \in X&lt;/math&gt;, one has &lt;math&gt;x = \psi(\varphi x)&lt;/math&gt;;
# for ν-almost all &lt;math&gt;y \in Y&lt;/math&gt;, one has &lt;math&gt;y = \varphi(\psi y)&lt;/math&gt;.

Hence, one may form a [[category (mathematics)|category]] of dynamical systems and their homomorphisms.

==Generic points==
A point ''x'' ∈ ''X'' is called a '''generic point''' if the [[orbit (dynamics)|orbit]] of the point is [[ergodic theorem|distributed uniformly]] according to the measure.

==Symbolic names and generators==
Consider a dynamical system &lt;math&gt;(X, \mathcal{B}, T, \mu)&lt;/math&gt;, and let ''Q'' = {''Q''&lt;sub&gt;1&lt;/sub&gt;, ..., ''Q&lt;sub&gt;k&lt;/sub&gt;''} be a [[partition of a set|partition]] of ''X'' into ''k'' measurable pair-wise disjoint pieces.  Given a point ''x'' ∈ ''X'', clearly ''x'' belongs to only one of the ''Q&lt;sub&gt;i&lt;/sub&gt;''.  Similarly, the iterated point ''T&lt;sup&gt;n&lt;/sup&gt;x'' can belong to only one of the parts as well. The '''symbolic name''' of ''x'', with regards to the partition ''Q'', is the sequence of integers {''a''&lt;sub&gt;''n''&lt;/sub&gt;} such that

:&lt;math&gt;T^nx \in Q_{a_n}.&lt;/math&gt;

The set of symbolic names with respect to a partition is called the [[symbolic dynamics]] of the dynamical system.  A partition ''Q'' is called a '''generator''' or '''generating partition''' if μ-almost every point ''x'' has a unique symbolic name.

==Operations on partitions==
Given a partition Q = {''Q''&lt;sub&gt;1&lt;/sub&gt;, ..., ''Q''&lt;sub&gt;''k''&lt;/sub&gt;} and a dynamical system &lt;math&gt;(X, \mathcal{B}, T, \mu)&lt;/math&gt;, we define ''T''-pullback of ''Q'' as

:&lt;math&gt; T^{-1}Q = \{T^{-1}Q_1,\ldots,T^{-1}Q_k\}.&lt;/math&gt;

Further, given two [[partition of a set|partitions]] ''Q'' = {''Q''&lt;sub&gt;1&lt;/sub&gt;, ..., ''Q&lt;sub&gt;k&lt;/sub&gt;''} and ''R'' = {''R''&lt;sub&gt;1&lt;/sub&gt;, ..., ''R''&lt;sub&gt;''m''&lt;/sub&gt;}, we define their [[join (sigma algebra)|refinement]] as

:&lt;math&gt; Q \vee R = \{Q_i \cap R_j \mid i=1,\ldots,k,\ j=1,\ldots,m,\ \mu(Q_i \cap R_j) &gt; 0 \}.&lt;/math&gt;

With these two constructs we may define ''refinement of an iterated pullback''

:&lt;math&gt;
\begin{align}
\bigvee_{n=0}^N T^{-n}Q &amp; =  \{Q_{i_0} \cap T^{-1}Q_{i_1} \cap \cdots \cap T^{-N}Q_{i_N} \\
&amp; {} \qquad \mbox { where }i_\ell = 1,\ldots,k ,\ \ell=0,\ldots,N,\ \\
&amp; {} \qquad \qquad \mu \left (Q_{i_0} \cap T^{-1}Q_{i_1} \cap \cdots \cap T^{-N}Q_{i_N} \right )&gt;0 \} \\
\end{align}
&lt;/math&gt;

which plays crucial role in the construction of the measure-theoretic entropy of a dynamical system.

==Measure-theoretic entropy==
The [[information entropy|entropy]] of a partition ''Q'' is defined as&lt;ref&gt;Ya.G. Sinai, (1959) "On the Notion of Entropy of a Dynamical System", ''Doklady of Russian Academy of Sciences'' '''124''', pp. 768–771.&lt;/ref&gt;&lt;ref&gt;Ya. G. Sinai, (2007) "[http://web.math.princeton.edu/facultypapers/Sinai/MetricEntropy2.pdf Metric Entropy of Dynamical System]"&lt;/ref&gt;

:&lt;math&gt;H(Q)=-\sum_{m=1}^k \mu (Q_m) \log \mu(Q_m).&lt;/math&gt;

The measure-theoretic entropy of a dynamical system &lt;math&gt;(X, \mathcal{B}, T, \mu)&lt;/math&gt; with respect to a partition ''Q'' = {''Q''&lt;sub&gt;1&lt;/sub&gt;, ..., ''Q''&lt;sub&gt;''k''&lt;/sub&gt;} is then defined as

:&lt;math&gt;h_\mu(T,Q) = \lim_{N \rightarrow \infty} \frac{1}{N} H\left(\bigvee_{n=0}^N T^{-n}Q\right).&lt;/math&gt;

Finally, the '''Kolmogorov–Sinai''' or '''metric''' or '''measure-theoretic entropy''' of a dynamical system &lt;math&gt;(X, \mathcal{B},T,\mu)&lt;/math&gt; is defined as

:&lt;math&gt;h_\mu(T) = \sup_Q h_\mu(T,Q).&lt;/math&gt;

where the [[supremum]] is taken over all finite measurable partitions. A theorem of [[Yakov Sinai]] in 1959 shows that the supremum is actually obtained on partitions that are generators.  Thus, for example, the entropy of the [[Bernoulli process]] is log&amp;nbsp;2, since [[almost every]] [[real number]] has a unique [[binary expansion]]. That is, one may partition the [[unit interval]] into the intervals &lt;nowiki&gt;[&lt;/nowiki&gt;0,&amp;nbsp;1/2&lt;nowiki&gt;)&lt;/nowiki&gt; and [1/2,&amp;nbsp;1]. Every real number ''x'' is either less than 1/2 or not; and likewise so is the fractional part of 2&lt;sup&gt;''n''&lt;/sup&gt;''x''.

If the space ''X'' is compact and endowed with a topology, or is a metric space, then the [[topological entropy]] may also be defined.

==See also==
*[[Krylov–Bogolyubov theorem]] on the existence of invariant measures
*[[Poincaré recurrence theorem]]

==References==
{{reflist}}

* Michael S. Keane, "Ergodic theory and subshifts of finite type", (1991), appearing as Chapter 2 in ''Ergodic Theory, Symbolic Dynamics and Hyperbolic Spaces'', Tim Bedford, Michael Keane and Caroline Series, Eds. Oxford University Press, Oxford (1991). {{isbn|0-19-853390-X}} ''(Provides expository introduction, with exercises, and extensive references.)''
* [[Lai-Sang Young]], "Entropy in Dynamical Systems" ([http://www.math.nyu.edu/~lsy/papers/entropy.pdf pdf]; [http://www.math.nyu.edu/~lsy/papers/entropy.ps ps]), appearing as Chapter 16 in ''Entropy'', Andreas Greven, Gerhard Keller, and Gerald Warnecke, eds. Princeton University Press, Princeton, NJ (2003).  {{isbn|0-691-11338-6}}

==Examples==
* T. Schürmann and I. Hoffmann, ''The entropy of strange billiards inside n-simplexes.'' J. Phys. A28, page 5033ff, 1995. [https://arxiv.org/abs/nlin/0208048 PDF-Dokument]

[[Category:Dynamical systems]]
[[Category:Entropy and information]]
[[Category:Measure theory]]
[[Category:Entropy]]
[[Category:Information theory]]</text>
      <sha1>g4d5u06xzwykkk601bkyckiz2m956mg</sha1>
    </revision>
  </page>
  <page>
    <title>Meertens number</title>
    <ns>0</ns>
    <id>27666612</id>
    <revision>
      <id>832708118</id>
      <parentid>792647094</parentid>
      <timestamp>2018-03-27T15:28:02Z</timestamp>
      <contributor>
        <username>DePiep</username>
        <id>199625</id>
      </contributor>
      <minor/>
      <comment>OEIS in external links: use dedicated {{OEIS el}} template (via [[WP:JWB]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1379">In [[mathematical logic]], a '''Meertens number''' is an integer that is its own [[Gödel numbering|Gödel number]].

The Gödel encoding of a [[decimal number]] with ''n'' digits is the product of the first ''n'' primes raised to the values of their corresponding digits in the sequence. 

The only Meertens number that has been found is 81312000 = 2&lt;sup&gt;8&lt;/sup&gt;3&lt;sup&gt;1&lt;/sup&gt;5&lt;sup&gt;3&lt;/sup&gt;7&lt;sup&gt;1&lt;/sup&gt;11&lt;sup&gt;2&lt;/sup&gt;13&lt;sup&gt;0&lt;/sup&gt;17&lt;sup&gt;0&lt;/sup&gt;19&lt;sup&gt;0&lt;/sup&gt;. 

It was "given" to [[Lambert Meertens]] by [[Richard S. Bird]] as a present during the celebration of his 25 years at the [[Centrum Wiskunde &amp; Informatica|CWI]], [[Amsterdam]]. &lt;ref&gt;{{cite journal |author=[[Richard S. Bird]] |year=1998 |title=Meertens number |journal=[[Journal of Functional Programming]] |volume=8 |issue=1 |pages=83–88 |doi= 	 10.1017/S0956796897002931}}&lt;/ref&gt;

If other Meertens numbers exist, they must be greater than 1&amp;nbsp;×&amp;nbsp;10&lt;sup&gt;25&lt;/sup&gt;.&lt;ref&gt;[https://twitter.com/NicolasPalau/status/566596212418433024 Nicolas PALAU : No new Meertens number &lt; 1&amp;nbsp;×&amp;nbsp;10&lt;sup&gt;25&lt;/sup&gt;]&lt;/ref&gt;
== References ==
&lt;references/&gt;

== External links ==
* {{OEIS el|1=A189398|2=a(n) = 2^d(1) * 3^d(2) * ... * prime(k)^d(k)|formalname=a(n) = 2^d(1) * 3^d(2) * ... * prime(k)^d(k), where d(1)d(2)...d(k) is the decimal representation of n}}

{{Classes of natural numbers}}

[[Category:Integer sequences]]</text>
      <sha1>dz5gb29082grqxz3akjvh7k1uk54swg</sha1>
    </revision>
  </page>
  <page>
    <title>Modular neural network</title>
    <ns>0</ns>
    <id>17319790</id>
    <revision>
      <id>843197396</id>
      <parentid>841545109</parentid>
      <timestamp>2018-05-27T15:17:53Z</timestamp>
      <contributor>
        <username>CitationCleanerBot</username>
        <id>15270283</id>
      </contributor>
      <minor/>
      <comment>/* References */arxivify URL / redundant url</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5714">{{Underlinked|date=December 2012}}

A '''modular neural network''' is an [[artificial neural network]] characterized by a series of independent neural networks moderated by some intermediary. Each independent neural network serves as a module and operates on separate inputs to accomplish some subtask of the task the network hopes to perform.{{sfn|Azam|2000}} The intermediary takes the outputs of each module and processes them to produce the output of the network as a whole. The intermediary only accepts the modules' outputs—it does not respond to, nor otherwise signal, the modules. As well, the modules do not interact with each other.

==Biological basis==
As [[artificial neural network]] research progresses, it is appropriate that artificial neural networks continue to draw on their biological inspiration and emulate the segmentation and modularization found in the brain. The brain, for example, divides the complex task of visual perception into many subtasks.{{sfn|Happel|Murre|1994}} Within a part of the [[brain]], called the [[thalamus]], lies the [[lateral geniculate nucleus]] (LGN), which is divided into layers that separately process color and contrast: both major components of [[Visual perception|vision]].{{sfn|Hubel|Livingstone|1990}} After the LGN processes each component in parallel, it passes the result to another region to compile the results.

Some tasks that the brain handles, like vision, employ a hierarchy of sub-networks. However, it is not clear whether some intermediary ties these separate processes together. Rather, as the tasks grow more abstract, the modules communicate with each other, unlike the modular neural network model.

== Design ==
Unlike a single large network that can be assigned to arbitrary tasks, each module in a modular network must be assigned  a specific task and connected to other modules in specific ways by a designer. In the vision example, the brain evolved (rather than learned) to create the LGN. In some cases, the designer may choose to follow biological models. In other cases, other models may be superior. The quality of the result will be a function of the quality of the design.

==Complexity==
Modular neural networks reduce a single large, unwieldy neural network to smaller, potentially more manageable components.{{sfn|Azam|2000}} Some tasks are intractably large for a single neural network. The benefits of modular neural networks include:

===Efficiency===
The possible [[neuron]] (node) connections increase exponentially as nodes are added to a network. Computation time depends on the number of nodes and their connections, any increase has drastic consequences for processing time. Assigning specific subtasks to individual modules reduce the number of necessary connections.

===Training===
A large [[neural network]] attempting to model multiple parameters can suffer from interference as new data can alter existing connections or just serve to confuse. Each module can be trained independently and more precisely master its simpler task. This means the training [[algorithm]] and the training data can be implemented more quickly.

===Robustness===
Regardless of whether a large neural network is biological or artificial, it remains largely susceptible to interference at and failure in any one of its nodes. By compartmentalizing subtasks, failure and interference are much more readily diagnosed and their effects on other sub-networks are eliminated as each one is independent of the other.

== Notes ==
{{reflist|30em}}

== References ==

*{{cite web|last=Azam |first=Farooq |title=Biologically Inspired Modular Neural Networks.  PhD Dissertation |publisher=Virginia Tech |year=2000 |url=http://scholar.lib.vt.edu/theses/available/etd-06092000-12150028/unrestricted/etd.pdf |ref=harv}}
*{{cite journal | last1 = Happel | first1 = Bart | last2 = Murre | first2 = Jacob | year = 1994 | title = The Design and Evolution of Modular Neural Network Architectures | url = http://citeseer.comp.nus.edu.sg/cache/papers/cs/3480/ftp:zSzzSzftp.mrc-apu.cam.ac.ukzSzpubzSznnzSzmurrezSznnga1.pdf/the-design-and-evolution.pdf | format = PDF | journal = Neural Networks | volume = 7 | issue = | pages = 985–1004 | doi=10.1016/s0893-6080(05)80155-8|ref=harv}}
*{{cite journal | last1 = Hubel | first1 = DH | last2 = Livingstone | first2 = MS | year = 1990 | title = Color and contrast sensitivity in the lateral geniculate body and primary visual cortex of the macaque monkey | url = http://www.jneurosci.org/cgi/content/abstract/10/7/2223 | journal = Journal of Neuroscience | volume = 10 | issue = | pages = 2223–2237|ref=harv }}
* {{cite journal | last1 = Tahmasebi | first1 = P. | last2 = Hezarkhani | first2 = A. | year = 2011 | title = Application of a Modular Feedforward for Grade Estimation | url = | journal = Natural Resources Research | volume = 20 | issue = 1| pages = 25–32 | doi = 10.1007/s11053-011-9135-3 }}
* {{Cite journal|last=Clune|first=Jeff|last2=Mouret|first2=Jean-Baptiste|last3=Lipson|first3=Hod|date=2013-01-30|title=The evolutionary origins of modularity|journal=Proceedings of the Royal Society B: Biological Sciences|volume=280|issue=1755|pages=20122863–20122863|doi=10.1098/rspb.2012.2863|issn=0962-8452|arxiv=1207.2743}}
* {{cite journal|last1 = Tahmasebi|first1 = Pejman|last2 = Hezarkhani|first2 = Ardeshir|date=|year = 2012|title = A fast and independent architecture of artificial neural network for permeability prediction|url =|journal = Journal of Petroleum Science and Engineering|volume = 86|issue =|pages = 118–126|doi=10.1016/j.petrol.2012.03.019|via=}}

[[Category:Computational neuroscience]]
[[Category:Artificial neural networks]]
[[Category:Modularity|Neural network]]</text>
      <sha1>30z8gi22ap72cjiok1lcz7k6hsgr5bz</sha1>
    </revision>
  </page>
  <page>
    <title>Mutual knowledge (logic)</title>
    <ns>0</ns>
    <id>33788375</id>
    <revision>
      <id>730931078</id>
      <parentid>713130721</parentid>
      <timestamp>2016-07-21T19:50:30Z</timestamp>
      <contributor>
        <username>Nøkkenbuer</username>
        <id>24344970</id>
      </contributor>
      <minor/>
      <comment>/* top */ punctuation</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1978">'''Mutual knowledge''' is a fundamental concept about information in [[game theory]], (epistemic) [[logic]], and [[epistemology]].  An [[Event (probability theory)|event]] is mutual knowledge if all agents know that the event occurred.&lt;ref name=Osborne&gt;Osborne, Martin J., and [[Ariel Rubinstein]]. ''A Course in Game Theory''. Cambridge, MA: MIT, 1994. Print.&lt;/ref&gt;{{rp|73}} However, mutual knowledge by itself implies nothing about what agents know about other agents' knowledge: i.e. it is possible that an event is mutual knowledge but that each agent is unaware that the other agents know it has occurred.&lt;ref&gt;Peter Vanderschraaf, Giacomo Sillari (2007). [http://plato.stanford.edu/entries/common-knowledge/  Common Knowledge]. ''Stanford Encyclopedia of Philosophy''.  Accessed 18 November 2011.&lt;/ref&gt; [[Common knowledge (logic)|Common knowledge]] is a related but stronger notion; any event that is common knowledge is also mutual knowledge.

The philosopher [[Stephen Schiffer]], in his book ''Meaning'', developed a notion he called "mutual knowledge" which functions quite similarly to [[David Lewis (philosopher)|David K. Lewis]]'s "common knowledge".&lt;ref&gt;Stephen Schiffer, ''Meaning'', 2nd edition, Oxford University Press, 1988.  The first edition was published by OUP in 1972.  Also, David Lewis, ''Convention'', Cambridge, MA:  Harvard University Press, 1969.  For a discussion of both Lewis's and Schiffer's notions, see Russell Dale, ''[http://www.russelldale.com/dissertation/1996.RussellDale.TheTheoryOfMeaning.pdf The Theory of Meaning]'' (1996).&lt;/ref&gt;

==See also==
* [[Elephant in the room]]
* [[The Emperor's New Clothes]]

==External links==
* {{cite web |url = https://www.youtube.com/watch?v=5S1d3cNge24&amp;t=46m02s |author = [[Steven Pinker]] |title = The Stuff of Thought: Language as a window into human nature |publisher = [[Royal Society of Arts|RSA]] }}

==References==
{{reflist}}

[[Category:Game theory]]
[[Category:Knowledge]]
[[Category:Logic]]</text>
      <sha1>dskgxelg5ntkr65kzofl271755hwze3</sha1>
    </revision>
  </page>
  <page>
    <title>Narcissistic number</title>
    <ns>0</ns>
    <id>3758667</id>
    <revision>
      <id>870663104</id>
      <parentid>869291232</parentid>
      <timestamp>2018-11-26T07:29:36Z</timestamp>
      <contributor>
        <username>Orientls</username>
        <id>20986398</id>
      </contributor>
      <comment>Undid revision 867061264 by [[Special:Contributions/Marian olarescu ruiz90|Marian olarescu ruiz90]] ([[User talk:Marian olarescu ruiz90|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12056">In [[Recreational mathematics|recreational]] [[number theory]], a '''narcissistic number'''&lt;ref name="mw"&gt;{{MathWorld |title=Narcissistic Number |urlname=NarcissisticNumber}}&lt;/ref&gt;&lt;ref name="moore"&gt;[http://www.cs.umd.edu/Honors/reports/NarcissisticNums/NarcissisticNums.html ''Perfect and Plus Perfect Digital Invariants''] {{webarchive|url=https://web.archive.org/web/20071010035540/http://www.cs.umd.edu/Honors/reports/NarcissisticNums/NarcissisticNums.html |date=2007-10-10 }} by Scott Moore&lt;/ref&gt; (also known as a '''pluperfect digital invariant''' ('''PPDI'''),&lt;ref&gt;[https://web.archive.org/web/20091027123639/http://www.geocities.com/~harveyh/narciss.htm  PPDI (Armstrong) Numbers] by Harvey Heinz&lt;/ref&gt; an '''Armstrong number'''&lt;ref&gt;[http://www.deimel.org/rec_math/dik1.htm Armstrong Numbers] by Dik T. Winter&lt;/ref&gt; (after Michael F. Armstrong)&lt;ref&gt;[http://blog.deimel.org/2010/05/mystery-solved.html Lionel Deimel’s Web Log]&lt;/ref&gt; or a '''plus perfect number''')&lt;ref&gt;{{OEIS|id=A005188}}&lt;/ref&gt; is a number that is the sum of its own digits each raised to the power of the number of digits. This definition depends on the base ''b'' of the number system used, e.g., ''b''&amp;nbsp;=&amp;nbsp;10 for the [[decimal|decimal system]] or ''b''&amp;nbsp;=&amp;nbsp;2 for the [[binary numeral system|binary system]].

==Definition==
The definition of a narcissistic number relies on the decimal representation ''n''&amp;nbsp;=&amp;nbsp;''d''&lt;sub&gt;''k''&lt;/sub&gt;''d''&lt;sub&gt;''k''-1&lt;/sub&gt;...''d''&lt;sub&gt;1&lt;/sub&gt; of a [[natural number]]&amp;nbsp;''n'', i.e.,
:''n''&amp;nbsp;=&amp;nbsp;''d''&lt;sub&gt;''k''&lt;/sub&gt;·10&lt;sup&gt;''k''-1&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''d''&lt;sub&gt;''k''-1&lt;/sub&gt;·10&lt;sup&gt;''k''-2&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;...&amp;nbsp;+&amp;nbsp;''d''&lt;sub&gt;2&lt;/sub&gt;·10&amp;nbsp;+&amp;nbsp;''d''&lt;sub&gt;1&lt;/sub&gt;,
with ''k'' digits ''d''&lt;sub&gt;''i''&lt;/sub&gt; satisfying 0&amp;nbsp;≤&amp;nbsp;''d''&lt;sub&gt;''i''&lt;/sub&gt;&amp;nbsp;≤&amp;nbsp;9. Such a number ''n'' is called narcissistic if it satisfies the condition
:''n''&amp;nbsp;=&amp;nbsp;''d''&lt;sub&gt;''k''&lt;/sub&gt;&lt;sup&gt;''k''&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''d''&lt;sub&gt;''k''-1&lt;/sub&gt;&lt;sup&gt;''k''&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;...&amp;nbsp;+&amp;nbsp;''d''&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;''k''&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''d''&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;''k''&lt;/sup&gt;.
For example, the 3-digit decimal number 153 is a narcissistic number because 153&amp;nbsp;=&amp;nbsp;1&lt;sup&gt;3&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;5&lt;sup&gt;3&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;3&lt;sup&gt;3&lt;/sup&gt;.

Narcissistic numbers can also be defined with respect to [[numeral system]]s with a base ''b'' other than ''b''&amp;nbsp;=&amp;nbsp;10. The base-''b'' representation of a natural number ''n'' is defined by
:''n''&amp;nbsp;=&amp;nbsp;''d''&lt;sub&gt;''k''&lt;/sub&gt;''b''&lt;sup&gt;''k''-1&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''d''&lt;sub&gt;''k''-1&lt;/sub&gt;''b''&lt;sup&gt;''k''-2&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;...&amp;nbsp;+&amp;nbsp;''d''&lt;sub&gt;2&lt;/sub&gt;''b''&amp;nbsp;+&amp;nbsp;''d''&lt;sub&gt;1&lt;/sub&gt;,
where the base-''b'' digits ''d''&lt;sub&gt;''i''&lt;/sub&gt; satisfy the condition 0&amp;nbsp;≤&amp;nbsp;''d''&lt;sub&gt;i&lt;/sub&gt;&amp;nbsp;≤&amp;nbsp;''b''-1.
For example, the (decimal) number 17 is a narcissistic number with respect to the numeral system with base ''b''&amp;nbsp;=&amp;nbsp;3. Its three base-3 digits are 122, because 17&amp;nbsp;=&amp;nbsp;1·3&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;2·3&amp;nbsp;+&amp;nbsp;2&amp;nbsp;, and it satisfies the equation 17&amp;nbsp;=&amp;nbsp;1&lt;sup&gt;3&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;2&lt;sup&gt;3&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;2&lt;sup&gt;3&lt;/sup&gt;.

If the constraint that the power must equal the number of digits is dropped, so that for some ''m'' possibly different from ''k'' it happens that
:''n''&amp;nbsp;=&amp;nbsp;''d''&lt;sub&gt;''k''&lt;/sub&gt;&lt;sup&gt;''m''&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''d''&lt;sub&gt;''k''-1&lt;/sub&gt;&lt;sup&gt;''m''&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;...&amp;nbsp;+&amp;nbsp;''d''&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;''m''&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''d''&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;''m''&lt;/sup&gt;,
then ''n'' is called a '''perfect digital invariant''' or '''PDI'''.&lt;ref name="moore"/&gt;&lt;ref&gt;[https://web.archive.org/web/20091027123639/http://www.geocities.com/~harveyh/narciss.htm  PDIs] by Harvey Heinz&lt;/ref&gt; For example, the decimal number 4150 has four decimal digits and is the sum of the ''fifth'' powers of its decimal digits
:4150&amp;nbsp;=&amp;nbsp;4&lt;sup&gt;5&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;1&lt;sup&gt;5&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;5&lt;sup&gt;5&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;0&lt;sup&gt;5&lt;/sup&gt;,
so it is a perfect digital invariant but ''not'' a narcissistic number.

In "[[A Mathematician's Apology]]", [[G. H. Hardy]] wrote:

:''There are just four numbers, after unity, which are the sums of the cubes of their digits:''

::&lt;math&gt;153=1^3+5^3+3^3&lt;/math&gt;
::&lt;math&gt;370=3^3+7^3+0^3&lt;/math&gt;
::&lt;math&gt; 371=3^3+7^3+1^3&lt;/math&gt;
::&lt;math&gt;407=4^3+0^3+7^3&lt;/math&gt;.
:''These are odd facts, very suitable for puzzle columns and likely to amuse amateurs, but there is nothing in them which appeals to the mathematician.''

==Narcissistic numbers in various bases==
The sequence of [[decimal|base 10]] narcissistic numbers starts:
[[0 (number)|0]], [[1 (number)|1]], [[2 (number)|2]], [[3 (number)|3]], [[4 (number)|4]], [[5 (number)|5]], [[6 (number)|6]], [[7 (number)|7]], [[8 (number)|8]], [[9 (number)|9]], [[153 (number)|153]], [[370 (number)|370]], [[371 (number)|371]], [[407 (number)|407]], [[1634 (number)|1634]], [[8208 (number)|8208]], [[9474 (number)|9474]], ... {{OEIS|id=A005188}}

The sequence of [[octal|base 8]] narcissistic numbers starts:
[[0 (number)|0]], [[1 (number)|1]], [[2 (number)|2]], [[3 (number)|3]], [[4 (number)|4]], [[5 (number)|5]], [[6 (number)|6]], [[7 (number)|7]], [[20 (number)|24]], [[52 (number)|64]], [[92 (number)|134]], [[133 (number)|205]], [[307 (number)|463]], [[432 (number)|660]], [[433 (number)|661]], ... (sequence {{OEIS link|id=A010354}} and {{OEIS link|id=A010351}} in [[OEIS]])

The sequence of [[duodecimal|base 12]] narcissistic numbers starts:
[[0 (number)|0]], [[1 (number)|1]], [[2 (number)|2]], [[3 (number)|3]], [[4 (number)|4]], [[5 (number)|5]], [[6 (number)|6]], [[7 (number)|7]], [[8 (number)|8]], [[9 (number)|9]], [[10 (number)|ᘔ]], [[11 (number)|Ɛ]], [[29 (number)|25]], [[125 (number)|ᘔ5]], [[811 (number)|577]], [[944 (number)|668]], [[1539 (number)|ᘔ83]], ... {{OEIS|id=A161949}}

The sequence of [[hexadecimal|base 16]] narcissistic numbers starts:
[[0 (number)|0]], [[1 (number)|1]], [[2 (number)|2]], [[3 (number)|3]], [[4 (number)|4]], [[5 (number)|5]], [[6 (number)|6]], [[7 (number)|7]], [[8 (number)|8]], [[9 (number)|9]], [[10 (number)|A]], [[11 (number)|B]], [[12 (number)|C]], [[13 (number)|D]], [[14 (number)|E]], [[15 (number)|F]], [[342 (number)|156]], [[371 (number)|173]], [[520 (number)|208]], [[584 (number)|248]], [[645 (number)|285]], [[1189 (number)|4A5]], [[1456 (number)|5B0]], [[1457 (number)|5B1]], [[1547 (number)|60B]], [[1611 (number)|64B]], ... {{OEIS|id=A161953}}

The sequence of [[ternary numeral system|base 3]] narcissistic numbers starts:
[[0 (number)|0]], [[1 (number)|1]], [[2 (number)|2]], [[5 (number)|12]], [[8 (number)|22]], [[17 (number)|122]]

The sequence of [[quaternary numeral system|base 4]] narcissistic numbers starts:
[[0 (number)|0]], [[1 (number)|1]], [[2 (number)|2]], [[3 (number)|3]], [[28 (number)|130]], [[29 (number)|131]], [[35 (number)|203]], [[43 (number)|223]], [[55 (number)|313]], [[62 (number)|332]], [[83 (number)|1103]], [[243 (number)|3303]] (sequence {{OEIS link|id=A010344}} and {{OEIS link|id=A010343}} in [[OEIS]])

In [[Binary number|base 2]], the only narcissistic numbers are [[0 (number)|0]] and [[1 (number)|1]].

The number of narcissistic numbers in a given base is finite, since the maximum possible sum of the ''k''th powers of a ''k'' digit number in base ''b'' is
:&lt;math&gt;k(b-1)^k\, ,&lt;/math&gt;

and if ''k'' is large enough then

:&lt;math&gt;k(b-1)^k&lt;b^{k-1}\, ,&lt;/math&gt;

in which case no base ''b'' narcissistic number can have ''k'' or more digits. Setting ''b'' equal to 10 shows that the largest narcissistic number in base 10 must be less than 10&lt;sup&gt;60&lt;/sup&gt;.&lt;ref name="mw" /&gt;

There are only 88 narcissistic numbers in base 10, of which the largest is

:115,132,219,018,763,992,565,095,597,973,971,522,401

with 39 digits.&lt;ref name="mw" /&gt;

Clearly, in all bases, all one-digit numbers are narcissistic numbers.

A base ''b'' has at least one two-digit narcissistic number [[if and only if]] ''b''&lt;sup&gt;2&lt;/sup&gt; + 1 is not prime, and the number of two-digit narcissistic numbers in base ''b'' equals &lt;math&gt;\tau(b^2+1)-2&lt;/math&gt;, where &lt;math&gt;\tau(n)&lt;/math&gt; is the number of positive divisors of ''n''.

Every base ''b'' ≥ 3 that is not a multiple of nine has at least one three-digit narcissistic number. The bases that do not are
:2, 72, 90, 108, 153, 270, 423, 450, 531, 558, 630, 648, 738, 1044, 1098, 1125, 1224, 1242, 1287, 1440, 1503, 1566, 1611, 1620, 1800, 1935, ... {{OEIS|id=A248970}}

Unlike narcissistic numbers, no upper bound can be determined for the size of PDIs in a given base, and it is not currently known whether or not the number of PDIs for an arbitrary base is finite or infinite.&lt;ref name="moore"/&gt;

==Related concepts==
The term "narcissistic number" is sometimes used in a wider sense to mean a number that is equal to any mathematical manipulation of its own digits. With this wider definition narcisstic numbers include:
* [[Constant base numbers]] : &lt;math&gt;n=m^{d_k} + m^{d_{k-1}} + \dots + m^{d_2} + m^{d_1}&lt;/math&gt;  for some ''m''.
* [[Perfect digit-to-digit invariants]] or [[Münchhausen number]]s {{OEIS|id=A046253}} : :&lt;math&gt;\textstyle n = \sum_{i=0}^k d_i^{d_i}\, ,\text{ e.g. } 3435 = 3^3 + 4^4 + 3^3 + 5^5\, .&lt;/math&gt;
* [[Ascending power numbers]] {{OEIS|id=A032799}} : &lt;math&gt;n = d_k^1 + d_{k-1}^2 + \dots + d_2^{k-1} + d_1^k\, ,\text{ e.g. } 135 = 1^1 + 3^2 + 5^3 \, .&lt;/math&gt;
* [[Friedman number]]s {{OEIS|id=A036057}}.
* [[Radical narcissistic number]]s {{OEIS|id= A119710}} &lt;ref&gt;Rose, Colin (2005), Radical Narcissistic Numbers, Journal of Recreational Mathematics, 33(4), pages 250-254.&lt;/ref&gt; &lt;math&gt;{ e.g. } 729 = (7+2)^{\sqrt{9}}, \text{  }4096=\sqrt{\sqrt{\sqrt{\sqrt{4}+0}}}^{96}&lt;/math&gt; 
* [[Sum-product number]]s {{OEIS|id=A038369}} : &lt;math&gt;n=\left(\sum_{i=1}^{k}{d_i}\right) \left(\prod_{i=1}^{k}{d_i}\right) \, ,\text{ e.g. } 144 = (1+4+4) \times (1 \times4 \times 4) \, .&lt;/math&gt;
* [[Dudeney number]]s {{OEIS|id=A061209}} :&lt;math&gt;n=\left(\sum_{i=1}^{k}{d_i}\right)^3\, ,\text{ e.g. } 512 = (5+1+2)^3 \, .&lt;/math&gt;
* [[Factorion]]s {{OEIS|id=A014080}} :&lt;math&gt;n=\sum_{i=1}^{k}{d_i}!\, ,\text{ e.g. } 145 = 1! + 4! + 5! \, .&lt;/math&gt;

where ''d''&lt;sub&gt;''i''&lt;/sub&gt; are the digits of ''n'' in some base.

* [[Fresnillense number]]s {{OEIS|id=A240511}} :&lt;math&gt;\, ,\text{ e.g. } 336 = (3^1 + 3^1 + 6^1) + (3^2 + 3^2 + 6^2) + (3^3 + 3^3 + 6^3)\, .&lt;/math&gt;
* Numbers n that are equal to a product of powers of digits where the exponents from left to right decrease with 1 and the exponent for the units digit is 1 (There are no other terms up to 10^200; this list is probably complete). {{OEIS|id=A282782}} :&lt;math&gt;\, ,\text{ e.g. } 0= (0^1), 1=(1^1)..., 1715=(1^4)*(7^3)*(1^2)*(5^1)\, .&lt;/math&gt;
* Numbers n such that n = (sum of digits of n)*((sum of digits of n) + 1) (the sequence is finite and full). {{OEIS|id=A282693}} :&lt;math&gt;\, ,\text{ e.g. } 0= 0*(0+1), 12=(1+2)*(1+2+1), 42=(4+2)*(4+2+1), 90=(9+0)*(9+0+1), 156=(1+5+6)*(1+5+6+1)\, .&lt;/math&gt;

==References==
{{reflist}}
{{refbegin}}
* [[Joseph Madachy|Joseph S. Madachy]], ''Mathematics on Vacation'', Thomas Nelson &amp; Sons Ltd. 1966, pages 163-175.
* Rose, Colin (2005), ''Radical narcissistic numbers'', Journal of Recreational Mathematics,  33(4), 2004-2005, pages 250-254.
* [https://web.archive.org/web/20040606020332/http://mathews-archive.com/digit-related-numbers/pdi.html ''Perfect Digital Invariants''] by Walter Schneider
{{refend}}

==External links==
* [http://www.deimel.org/rec_math/DI_0.htm Digital Invariants]
* [http://everything2.net/index.pl?node_id=1407017&amp;displaytype=printable&amp;lastnode_id=1407017 Armstrong Numbers]
* [https://web.archive.org/web/20100109234250/http://ftp.cwi.nl/dik/Armstrong Armstrong Numbers in base 2 to 16]
* [http://www.cs.mtu.edu/~shene/COURSES/cs201/NOTES/chap04/arms.html Armstrong numbers between 1-999 calculator]
* {{cite web|last=Symonds|first=Ria|title=153 ♥ Narcissistic Number|url=http://www.numberphile.com/videos/153_narcissistic.html|work=Numberphile|publisher=[[Brady Haran]]}}

{{Classes of natural numbers}}

[[Category:Base-dependent integer sequences]]
[[Category:Recreational mathematics]]</text>
      <sha1>lr0t27xpjq9poqwde2ukrj71phg6583</sha1>
    </revision>
  </page>
  <page>
    <title>Nimber</title>
    <ns>0</ns>
    <id>164501</id>
    <revision>
      <id>833025382</id>
      <parentid>833025259</parentid>
      <timestamp>2018-03-29T08:01:04Z</timestamp>
      <contributor>
        <username>Tea2min</username>
        <id>36029</id>
      </contributor>
      <comment>/* top */ Link to [[minimum excludant]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11499">{{Distinguish|Number}}

In [[mathematics]], the '''nimbers''', also called '''Grundy numbers''', are introduced in [[combinatorial game theory]], where they are defined as the values of heaps in the game [[Nim]].  The nimbers are the [[ordinal number]]s endowed with '''nimber addition''' and '''nimber multiplication''', which are distinct from [[ordinal addition]] and [[ordinal multiplication]].

Because of the [[Sprague–Grundy theorem]] which states that every [[impartial game]] is equivalent to a Nim heap of a certain size, nimbers arise in a much larger class of impartial games. They may also occur in [[partizan game]]s like [[Domineering]].

Nimbers have the characteristic that their Left and Right options are identical, following a certain schema, and that they are their own negatives, such that a positive ordinal may be added to another positive ordinal using '''nimber addition''' to find an ordinal of a lower value.&lt;ref&gt;{{Cite book|url=https://www.worldcat.org/oclc/933627646|title=Advances in computer games : 14th International Conference, ACG 2015, Leiden, the Netherlands, July 1-3, 2015, Revised selected papers|others=Herik, Jaap van den,, Plaat, Aske,, Kosters, Walter,|isbn=3319279920|location=Cham|oclc=933627646}}&lt;/ref&gt;  The [[minimum excludant]] operation is applied to sets of nimbers.

== Uses ==

=== Nim ===
{{main article|Nim}}
Nim is a game in which two players take turns removing objects from distinct heaps. As moves depend only on the position and not on which of the two players is currently moving, and where the payoffs are symmetric, Nim is an impartial game. On each turn, a player must remove at least one object, and may remove any number of objects provided they all come from the same heap. The goal of the game is to avoid being the player who must remove the last object. Using nimber addition, each heap can be summed together to give a nim value for the heap. Furthermore, as all the heaps together can be summed using nim addition, one can calculate the nimber of the game as a whole. The winning strategy of this game is to force the cumulative nimber of the game to 0 for the opponents turn.&lt;ref&gt;{{Cite book|url=https://www.worldcat.org/oclc/743298766|title=Introduction to the design &amp; analysis of algorithms|last=Anany.|first=Levitin,|date=2012|publisher=Pearson|isbn=9780132316811|edition=3rd|location=Boston|oclc=743298766}}&lt;/ref&gt;

=== Cram ===
Cram is a game often played on a rectangular board in which players take turns placing dominoes either horizontally or vertically until no more dominoes can be placed. The first player that cannot make a move loses. As the possible moves for both players are the same, it is an impartial game and can have a nimber value. If each row and column is considered a heap, then the value of the game is the sum of all rows and columns using nimber addition. For example, any 2xn board will have a nimber of 0 for all even n and a nimber of 1 for all odd n.

=== Northcott's Game ===
A game where pegs for each player are placed along a column with a finite number of spaces. Each turn each player must move the piece up or down the column, but may not move past the other player's piece. Several columns are stacked together to add complexity. The player that can no longer make any moves loses. Unlike many other nimber related games, the number of spaces between the two tokens on each row are the sizes of the Nim heaps. If your opponent increases the number of spaces between two tokens, just decrease it on your next move. Else, play the game of Nim and make the Nim-sum of the number of spaces between the tokens on each row be 0.&lt;ref&gt;{{Cite web|url=http://web.mit.edu/sp.268/www/nim.pdf|title=Theory of Impartial Games|last=|first=|date=Feb 3, 2009|website=|archive-url=|archive-date=|dead-url=|access-date=}}&lt;/ref&gt;

=== Hackenbush ===
{{main article|Hackenbush}}
Hackenbush is a game invented by mathematician [[John Horton Conway]]. It may be played on any configuration of colored [[line segment]]s connected to one another by their endpoints and to a "ground" line. players take turns removing line segments. An impartial game version, thereby a game able to be analyzed using nimbers can be found by removing distinction from the lines, allowing either player to cut any branch. Any segments reliant on the newly removed segment in order to connect to the ground line are removed as well. In this way, each connection to the ground can be considered a nim heap with a nimber value. Additionally, all the separate connections to the ground line can also be summed for a nimber of the game state.

== Addition ==
Nimber addition (also known as '''nim-addition''') can be used to calculate the size of a single nim heap equivalent to a collection of nim heaps.  It is defined recursively by
:{{math|1=''α'' ⊕ ''β'' = mex({{mset|''α''′ ⊕ ''β'' : ''α''' &lt; ''α''}} ∪ {{mset|''α'' ⊕ ''β''′ : ''β''′ &lt; ''β''}})}},
where the [[minimum excludant]] {{math|mex(''S'')}} of a set {{math|''S''}} of ordinals is defined to be the smallest ordinal that is ''not'' an element of {{math|''S''}}.

For finite ordinals, the '''nim-sum''' is easily evaluated on a computer by taking the [[Bitwise operation|bitwise]] [[exclusive or]] (XOR, denoted by {{math|⊕}}) of the corresponding numbers. For example, the nim-sum of 7 and 14 can be found by writing 7 as 111 and 14 as 1110; the ones place adds to 1; the twos place adds to 2, which we replace with 0; the fours place adds to 2, which we replace with 0; the eights place adds to 1. So the nim-sum is written in binary as 1001, or in decimal as 9.

This property of addition follows from the fact that both mex and XOR yield a winning strategy for Nim and there can be only one such strategy; or it can be shown directly by induction: Let {{math|''α''}} and {{math|''β''}} be two finite ordinals, and assume that the nim-sum of all pairs with one of them reduced is already defined. The only number whose XOR with {{math|''α''}} is {{math|''α'' ⊕ ''β''}} is {{math|''β''}}, and vice versa; thus {{math|''α'' ⊕ ''β''}} is excluded. On the other hand, for any ordinal {{math|''γ'' &lt; ''α'' ⊕ ''β''}}, XORing {{math|1=''ξ'' ≔ ''α'' ⊕ ''β'' ⊕ ''γ''}} with all of {{math|''α''}}, {{math|''β''}} and {{math|''γ''}} must lead to a reduction for one of them (since the leading 1 in {{math|''ξ''}} must be present in at least one of the three); since {{math|1=''ξ'' ⊕ ''γ'' = ''α'' ⊕ ''β'' &gt; ''γ''}}, we must have {{math|1=''α'' &gt; ''ξ'' ⊕ ''α'' = ''β'' ⊕ ''γ''}} or {{math|1=''β'' &gt; ''ξ'' ⊕ ''β'' = ''α'' ⊕ ''γ''}}; thus {{math|''γ''}} is included as {{math|(''β'' ⊕ ''γ'') ⊕ ''β''}} or as {{math|α ⊕ (α ⊕ γ)}}, and hence {{math|''α'' ⊕ ''β''}} is the minimum excluded ordinal.

== Multiplication ==
Nimber multiplication ('''nim-multiplication''') is defined recursively by

:{{math|1=''α'' ''β'' = mex({{mset|''α''′ ''β'' ⊕ ''α'' ''β''′ ⊕ ''α''' ''β''′ : ''α''′ &lt; ''α'', ''β''′ &lt; ''β''}})}}.

Except for the fact that nimbers form a [[class (set theory)|proper class]] and not a set, the class of nimbers determines an [[algebraically closed field]] of [[characteristic (algebra)|characteristic]] 2. The nimber additive identity is the ordinal 0, and the nimber multiplicative identity is the ordinal 1. In keeping with the characteristic being 2, the nimber additive inverse of the ordinal {{math|''α''}} is {{math|''α''}} itself. The nimber multiplicative inverse of the nonzero ordinal {{math|''α''}} is given by {{math|1=1/''α'' = mex(''S'')}}, where {{math|''S''}} is the smallest set of ordinals (nimbers) such that

# 0 is an element of {{math|''S''}};
# if {{math|0 &lt; ''α''′ &lt; ''α''}} and {{math|''β''′}} is an element of {{math|''S''}}, then {{math|[1 + (α′ − α) β′] / α′}} is also an element of {{math|''S''}}.

For all natural numbers {{math|''n''}}, the set of nimbers less than {{math|2&lt;sup&gt;2&lt;sup&gt;''n''&lt;/sup&gt;&lt;/sup&gt;}} form the [[Galois field]] {{math|GF(2&lt;sup&gt;2&lt;sup&gt;''n''&lt;/sup&gt;&lt;/sup&gt;)}} of order&amp;nbsp;{{math|2&lt;sup&gt;2&lt;sup&gt;''n''&lt;/sup&gt;&lt;/sup&gt;}}.

In particular, this implies that the set of finite nimbers is isomorphic to the [[direct limit]] as {{math|''n'' → ∞}} of the fields {{math|GF(2&lt;sup&gt;2&lt;sup&gt;''n''&lt;/sup&gt;&lt;/sup&gt;)}}. This subfield is not algebraically closed, since no other field {{math|GF(2&lt;sup&gt;''k''&lt;/sup&gt;)}} (so with {{math|''k''}} not a power of 2) is contained in any of those fields, and therefore not in their direct limit; for instance the polynomial {{math|''x''&lt;sup&gt;3&lt;/sup&gt; + ''x'' + 1}}, which has a root in {{math|GF(2&lt;sup&gt;3&lt;/sup&gt;)}}, does not have a root in  the set of finite nimbers.

Just as in the case of nimber addition, there is a means of computing the nimber product of finite ordinals. This is determined by the rules that

# The nimber product of a Fermat 2-power (numbers of the form {{math|2&lt;sup&gt;2&lt;sup&gt;''n''&lt;/sup&gt;&lt;/sup&gt;}}) with a smaller number is equal to their ordinary product;
# The nimber square of a Fermat 2-power {{math|''x''}} is equal to {{math|3''x''/2}} as evaluated under the ordinary multiplication of natural numbers.

The smallest algebraically closed field of nimbers is the set of nimbers less than the ordinal {{math|''ω''&lt;sup&gt;''ω''&lt;sup&gt;''ω''&lt;/sup&gt;&lt;/sup&gt;}}, where {{math|''ω''}} is the smallest infinite ordinal. It follows that as a nimber, {{math|''ω''&lt;sup&gt;''ω''&lt;sup&gt;''ω''&lt;/sup&gt;&lt;/sup&gt;}} is [[transcendental number|transcendental]] over the field.&lt;ref&gt;Conway 1976, p.&amp;nbsp;61.&lt;/ref&gt;

== Addition and multiplication tables ==
The following tables exhibit addition and multiplication among the first 16 nimbers. &lt;br&gt;
This subset is closed under both operations, since 16 is of the form&amp;nbsp;{{math|2&lt;sup&gt;2&lt;sup&gt;''n''&lt;/sup&gt;&lt;/sup&gt;}}.
&lt;small&gt;(If you prefer simple text tables, they are {{Oldid|page=Nimber|oldid=383699838|label=here}}.)&lt;/small&gt;

[[File:Z2^4; Cayley table; binary.svg|thumb|center|500px|Nimber addition {{OEIS|A003987}}&lt;br&gt;This is also the [[Cayley table]] of [[List of small groups#List of small abelian groups|Z&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;4&lt;/sup&gt;]] - or the table of [[Bitwise operation|bitwise]] [[w:Exclusive or|XOR]] operations.&lt;br&gt;The small matrices show the single digits of the binary numbers.]]

[[File:Nimbers 0...15 multiplication.svg|thumb|center|500px|Nimber multiplication {{OEIS|A051775}}&lt;br&gt;The nonzero elements form the [[w:Cayley table|Cayley table]] of [[w:List of small groups#List of small abelian groups|Z&lt;sub&gt;15&lt;/sub&gt;]].&lt;br&gt;The small matrices are permuted binary [[Walsh matrix|Walsh matrices]].]]

[[File:Nimber products of powers of two.svg|thumb|center|900px|Nimber multiplication of [[Power of two|powers of two]] {{OEIS|A223541}}&lt;br&gt;Calculating the nim-products of powers of two is a decisive point in the recursive algorithm of nimber-multiplication.]]

==Notes==
{{reflist}}

==References==
{{refbegin}}
* {{cite book
|first1=John Horton
|last1=Conway
|authorlink1=John Horton Conway
|title=[[On Numbers and Games]]
|publisher=[[Academic Press]] Inc. (London) Ltd.
|year=1976
}}
* {{cite web
|first1=H. W.
|last1=Lenstra
|authorlink1=Hendrik Willem Lenstra, Jr.
|title=Nim multiplication
|hdl=1887/2125
|year=1978
}}
* {{cite arXiv
|first1=Dierk
|last1=Schleicher
|first2=Michael
|last2=Stoll
|eprint=math.DO/0410026
|title= An Introduction to Conway's Games and Numbers}} which discusses games, [[surreal number]]s, and nimbers.
{{refend}}

[[Category:Combinatorial game theory]]
[[Category:Finite fields]]</text>
      <sha1>bvog13ylb33terc63n7jmygbsumwwk9</sha1>
    </revision>
  </page>
  <page>
    <title>Non-logical symbol</title>
    <ns>0</ns>
    <id>7791994</id>
    <revision>
      <id>852679861</id>
      <parentid>852679559</parentid>
      <timestamp>2018-07-30T16:56:50Z</timestamp>
      <contributor>
        <ip>212.100.146.194</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5661">In [[logic]], the [[formal language]]s used to create expressions consist of [[symbol (formal)|symbol]]s, which can be broadly divided into [[logical constants|constants]] and [[Variable (mathematics)|variables]]. The constants of a language can further be divided into [[logical constant|logical symbols]] and '''non-logical symbols''' (sometimes also called '''logical''' and '''non-logical constants''').

The non-logical symbols of a language of [[first-order logic]] consist of [[Predicate (mathematical logic)|predicate]]s and individual constants. These include symbols that, in an interpretation, may stand for individual constants, [[Variable (mathematics)|variables]], [[Function (mathematics)|functions]], or [[Predicate (logic)|predicates]]. A language of first-order logic is a formal language over the alphabet consisting of its non-logical symbols and its [[logical constants|logical symbols]].  The latter include [[logical connective]]s, [[Quantifier (logic)|quantifier]]s, and variables that stand for [[statement (logic)|statements]].

A non-logical symbol only has meaning or semantic content when one is assigned to it by means of an [[Interpretation (logic)|interpretation]].  Consequently, a [[Sentence (mathematical logic)|sentence]] containing a non-logical symbol lacks meaning except under an interpretation, so a sentence is said to be ''true or false under an interpretation''.   Main article: [[first order logic]]; especially, ''Syntax of first-order logic''.

The [[logical constants]], by contrast, have the same meaning in all interpretations.  They include the symbols for truth-functional connectives (such as "and", "or", "not", "implies", and [[logical equivalence]]) and the symbols for the quantifiers "for all" and "there exists".

The [[Equality (mathematics)|equality]] symbol is sometimes treated as a non-logical symbol and sometimes treated as a symbol of logic. If it is treated as a logical symbol, then any interpretation will be required to interpret the equality sign using true equality; if interpreted as a non-logical symbol, it may be interpreted by an arbitrary [[equivalence relation]].

==Signatures==
{{main|signature (logic)}}
A ''signature'' is a set of non-logical constants together with additional information identifying each symbol as either a constant symbol, or a function symbol of a specific [[arity]] ''n'' (a natural number), or a relation symbol of a specific arity. The additional information controls how the non-logical symbols can be used to form terms and formulas. For instance if ''f'' is a binary function symbol and ''c'' is a constant symbol, then ''f''(''x'',&amp;nbsp;''c'') is a term, but ''c''(''x'',&amp;nbsp;''f'') is not a term. Relation symbols cannot be used in terms, but they can be used to combine one or more (depending on the arity) terms into an atomic formula.

For example a signature could consist of a binary function symbol +, a constant symbol 0, and a binary relation symbol &lt;.

==Models==
{{main|structure (mathematical logic)}}
''Structures'' over a signature, also known as ''models'', provide [[Formal semantics (logic)|formal semantics]] to a signature and the [[first-order logic|first-order]] language over it.

A structure over a signature consists of a set ''D'', known as the [[domain of discourse]], together with interpretations of the non-logical symbols: Every constant symbol is interpreted by an element of ''D'', and the interpretation of an ''n''-ary function symbol is an ''n''-ary function on ''D'', i.e. a function ''D&lt;sup&gt;n&lt;/sup&gt;''&amp;nbsp;→&amp;nbsp;''D'' from the ''n''-fold [[cartesian product]] of the domain to the domain itself. Every ''n''-ary relation symbol is interpreted by an ''n''-ary relation on the domain, i.e. by a subset of ''D&lt;sup&gt;n&lt;/sup&gt;''.

An example of a structure over the signature mentioned above is the ordered group of [[integer]]s. Its domain is the set &lt;math&gt;{\mathbb Z}&lt;/math&gt;&amp;nbsp;=&amp;nbsp;{…,&amp;nbsp;–2,&amp;nbsp;–1,&amp;nbsp;0,&amp;nbsp;1,&amp;nbsp;2,&amp;nbsp;…} of integers. The binary function symbol + is interpreted by addition, the constant symbol 0 by the additive identity, and the binary relation symbol &lt; by the relation less than.

==Informal semantics==
Outside a mathematical context, it is often more appropriate to work with more informal interpretations.

== Descriptive signs ==
[[Rudolf Carnap]] introduced a terminology distinguishing between logical and non-logical symbols (which he called ''descriptive signs'') of  a [[formal system]] under a certain type of [[Interpretation (logic)|interpretation]], defined by what they describe in the world.

A descriptive sign is defined as any symbol of a formal language which designates things or processes in the world, or properties or relations of things. This is in contrast to ''logical signs'' which do not designate any thing in the world of objects. The use of logical signs is determined by the logical rules of the language, whereas meaning is arbitrarily attached to descriptive signs when they are applied to a given domain of individuals.&lt;ref&gt;Carnap, Rudolf, ''Introduction to Symbolic Logic and its Applications.''&lt;/ref&gt;

== See also ==
* [[Logical constant]]

== References ==
{{reflist}}
;Notes
* {{Citation | last1=Hinman | first1=P. | title=Fundamentals of Mathematical Logic | publisher=[[A K Peters]] | isbn=978-1-56881-262-5 | year=2005}}

== External links ==
* [http://plato.stanford.edu/entries/logic-classical/#4 Semantics] section in [http://plato.stanford.edu/entries/logic-classical/ Classical Logic] (an entry of [http://plato.stanford.edu Stanford Encyclopedia of Philosophy])

[[Category:Logic symbols]]
[[Category:Formal languages]]</text>
      <sha1>envj9x9fc3c3u3w00ucmyjbla1ge84b</sha1>
    </revision>
  </page>
  <page>
    <title>Option type</title>
    <ns>0</ns>
    <id>22774540</id>
    <revision>
      <id>868858926</id>
      <parentid>868353831</parentid>
      <timestamp>2018-11-14T22:22:45Z</timestamp>
      <contributor>
        <username>Texvc2LaTeXBot</username>
        <id>33995001</id>
      </contributor>
      <minor/>
      <comment>Replacing deprecated latex syntax [[mw:Extension:Math/Roadmap]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14000">{{for|families of option contracts in finance|Option style}}

In [[programming language]]s (more so [[functional programming]] languages) and [[type theory]], an '''option type''' or '''maybe type''' is a [[parametric polymorphism|polymorphic type]] that represents encapsulation of an optional value; e.g., it is used as the return type of functions which may or may not return a meaningful value when they are applied.  It consists of a constructor which either is empty (named ''None'' or ''Nothing''), or which encapsulates the original data type A (written ''Just'' A or ''Some'' A). Outside of functional programming, these are termed [[nullable type]]s.

==Names and definitions==
In different programming languages, the option type has various names and definitions.

* In [[Agda (programming language)|Agda]], it is named {{code|2=agda|Maybe}} with variants {{code|2=agda|nothing}} and {{code|2=agda|just a}}.
* In [[C++17]] it is defined as the template class {{code|2=c++|std::optional&lt;T&gt;}}.
* In [[C Sharp (programming language)|C#]], it is defined as {{code|2=CSharp|Nullable&lt;T&gt;}} but is generally written as {{code|2=CSharp|T?}}.
* In [[Coq]], it is defined as {{code|2=coq|1=Inductive option (A:Type) : Type := {{!}} Some : A -&gt; option A {{!}} None : option A. }}.
* In [[Haskell (programming language)|Haskell]], it is named ''Maybe'', and defined as {{code|2=haskell|1=data Maybe a = Nothing {{!}} Just a}}.
* In [[Idris (programming language)|Idris]], it is defined as {{code|2=idris|1=data Maybe a = Nothing {{!}} Just a}}.
* In [[Java (programming language)|Java]], since version 8, it is defined as parameterized final class {{code|2=java|Optional&lt;T&gt;}}.
* In [[Julia (programming language)|Julia]], it is named {{code|2=julia|Nullable{T} }}.
* In [[Kotlin (programming language)|Kotlin]], it is defined as {{code|2=kotlin|T?}}.&lt;ref&gt;{{cite web |url=https://kotlinlang.org/docs/reference/null-safety.html |title=Null Safety - Kotlin Programming Language |accessdate=2016-08-01}}&lt;/ref&gt;
* In [[OCaml]], it is defined as {{code|2=ocaml|1=type 'a option = None {{!}} Some of 'a}}.
* In [[Perl 6]], this is the default, but you can add a {{code|2=perl6|:D}} "smiley" to opt into a non option type.
* In [[Rust (programming language)|Rust]], it is defined as {{code|2=rust|enum Option&lt;T&gt; { None, Some(T) } }}.
* In [[Scala (programming language)|Scala]], it is defined as parameterized abstract class {{code|2=scala|1='.. Option[A] = if (x == null) None else Some(x)..}}.
* In [[Standard ML]], it is defined as {{code|2=sml|1=datatype 'a option = NONE {{!}} SOME of 'a}}.
* In [[Swift (programming language)|Swift]], it is defined as {{code|2=swift|enum Optional&lt;T&gt; { case none, some(T) } }} but is generally written as {{code|2=swift|T?}}.

In [[type theory]], it may be written as: &lt;math&gt;A^{?} = A + 1&lt;/math&gt;.

In languages having [[tagged union]]s, as in most [[functional programming]] languages, option types can be expressed as the tagged union of a [[unit type]] plus the encapsulated type.

In the [[Curry-Howard correspondence]], option types are related to the [[absorption law|annihilation law]] for ∨: x∨1=1.

An option type can also be seen as a [[collection (computing)|collection]] containing either one or zero elements.

== The option monad ==
{{Further information|Monad (functional programming)#The Maybe monad}}
The option type is a [[monad (functional programming)|monad]] under these functions:
:&lt;math&gt;\text{return}\colon A \to A^{?} = a \mapsto \text{Just} \, a&lt;/math&gt;
:&lt;math&gt;\text{bind}\colon A^{?} \to (A \to B^{?}) \to B^{?} = a \mapsto f \mapsto \begin{cases} \text{Nothing} &amp; \text{if} \ a = \text{Nothing}\\ f \, a' &amp; \text{if} \ a = \text{Just} \, a' \end{cases}&lt;/math&gt;
We may also describe the option monad in terms of functions ''return'', ''fmap'' and ''join'', where the latter two are given by:
:&lt;math&gt;\text{fmap} \colon (A \to B) \to A^{?} \to B^{?} = f \mapsto a \mapsto \begin{cases} \text{Nothing} &amp; \text{if} \ a = \text{Nothing}\\ \text{Just} \, f \, a' &amp; \text{if} \ a = \text{Just} \, a' \end{cases}&lt;/math&gt;
:&lt;math&gt;\text{join} \colon {A^{?}}^{?} \to A^{?} = a \mapsto \begin{cases} \text{Nothing} &amp; \text{if} \ a = \text{Nothing}\\ \text{Nothing} &amp; \text{if} \ a = \text{Just} \, \text{Nothing}\\ \text{Just} \, a' &amp; \text{if} \ a = \text{Just} \, \text{Just} \, a' \end{cases}&lt;/math&gt;

The option monad is an additive monad: it has ''Nothing'' as a zero constructor and the following function as a monadic sum:

:&lt;math&gt;\text{mplus} \colon A^{?} \to A^{?} \to A^{?} = a_1 \mapsto a_2 \mapsto \begin{cases} \text{Nothing} &amp; \text{if} \ a_1 = \text{Nothing} \land a_2 = \text{Nothing}\\ \text{Just} \, a'_2 &amp; \text{if} \ a_1 = \text{Nothing} \land a_2 = \text{Just} \, a'_2 \\ \text{Just} \, a'_1 &amp; \text{if} \ a_1 = \text{Just} \, a'_1 \end{cases}&lt;/math&gt;

The resulting structure is an [[Idempotence|idempotent]] [[monoid]].

== Examples ==

=== Scala ===
[[Scala (programming language)|Scala]] implements Option as a parameterized type, so a variable can be an Option, accessed as follows:&lt;ref name="OderskySpoon2008"&gt;{{cite book|author1=Martin Odersky|author2=Lex Spoon|author3=Bill Venners|title=Programming in Scala|url=https://books.google.com/books?id=MFjNhTjeQKkC&amp;pg=PA283|accessdate=6 September 2011|year=2008|publisher=Artima Inc|isbn=978-0-9815316-0-1|pages=282–284}}&lt;/ref&gt;
&lt;source lang="scala"&gt;
// Defining variables that are Options of type Int
val res1: Option[Int] = Some(42)
val res2: Option[Int] = None

// sample 1 :  This function uses pattern matching to deconstruct Options
def compute(opt: Option[Int]) = opt match {
  case None =&gt; "No value"
  case Some(x) =&gt; "The value is: " + x
}

// sample 2 :  This function uses monad method
def compute(opt: Option[Int]) = opt.fold("No Value")(v =&gt; "The value is:" + v )

println(compute(res1))  // The value is: 42
println(compute(res2))  // No value
&lt;/source&gt;

Two main ways to use an Option value exist. The first, not the best, is the [[pattern matching]], as in the first example. The second, the best practice, is the monad method, as in the second example. In this way, a program is safe, as it can generate no exception or error (e.g., by trying to obtain the value of an &lt;code&gt;Option&lt;/code&gt; variable that is equal to &lt;code&gt;None&lt;/code&gt;). Thus, it essentially works as a type-safe alternative to the null value.

=== OCaml ===
[[OCaml]] implements Option as a parameterized variant type.  Options are constructed and deconstructed as follows:
&lt;source lang="ocaml"&gt;
(* Constructing options *)
let none = None
let some = Some 42

(* Deconstructing options *)
let compute opt = match opt with
  | None -&gt; "No value"
  | Some x -&gt; "The value is: " ^ string_of_int x

print_endline (compute none) (* "No value" *)
print_endline (compute some) (* "The value is: 42" *)
&lt;/source&gt;

=== F# ===
&lt;source lang="ocaml"&gt;

(* This function uses pattern matching to deconstruct Options *)
let compute = function
  | None   -&gt; "No value"
  | Some x -&gt; sprintf "The value is: %d" x

printfn "%s" (compute &lt;| Some 42)(* The value is: 42 *)
printfn "%s" (compute None)      (* No value         *)
&lt;/source&gt;

=== Haskell ===
&lt;source lang="haskell"&gt;
-- This function uses pattern matching to deconstruct Maybes
compute :: Maybe Int -&gt; String
compute Nothing  = "No value"
compute (Just x) = "The value is: " ++ show x

main :: IO ()
main = do
    print $ compute (Just 42) -- The value is: 42
    print $ compute Nothing -- No value
&lt;/source&gt;

=== Swift ===
&lt;source lang="swift"&gt;
func compute(_ x: Int?) -&gt; String {
  // This function uses optional binding to deconstruct optionals
  if let y = x {
    // y is now the non-optional `Int` content of `x`, if it has any
    return "The value is: \(y)"
  } else {
    return "No value"
  }
}

print(compute(42)) // The value is: 42
print(compute(nil)) // No value
&lt;/source&gt;

&lt;source lang="swift"&gt;
func compute(_ x: Int?) -&gt; String {
  // This function uses `map` to transform the optional if has a value,
  // or pass along the nil if it doesn't. If `map` results in nil,
  // the nil coalescing operator `??` sets a fall-back value of "No value"
  return x.map { unwrappedX in "The value is: \(unwrappedX)" } ?? "No value"
}

print(compute(42)) // The value is: 42
print(compute(nil)) // No value
&lt;/source&gt;
&lt;source lang="swift"&gt;
func compute(_ x: Int?) -&gt; String {
  // This function uses pattern matching to deconstruct optionals
  switch x {
  case .none: 
    return "No value"
  case .some(let y): 
    return "The value is: \(y)"
  }
}

print(compute(42)) // The value is: 42
print(compute(nil)) // No value
&lt;/source&gt;

&lt;source lang="swift"&gt;
func compute(_ x: Int?) -&gt; String {
  // This function asserts that x has a value, forcefully unwraps x
  // and CRASHES if nil is encountered!
  return "The value is: \(x!)"
}

print(compute(42)) // The value is: 42
print(compute(nil)) // Crash!
&lt;/source&gt;

=== Rust ===
Rust allows using either pattern matching or optional binding to deconstruct the Option type:

&lt;source lang="rust"&gt;
fn main() {
    // This function uses pattern matching to deconstruct optionals
    fn compute(x: Option&lt;i32&gt;) -&gt; String {
        match x {
            Some(a) =&gt; format!("The value is: {}", a),
            None    =&gt; format!("No value")
        }
    }

    println!("{}", compute(Some(42))); // The value is: 42
    println!("{}", compute(None)); // No value
}
&lt;/source&gt;

&lt;source lang="rust"&gt;
fn main() {
    // This function uses optional binding to deconstruct optionals
    fn compute(x: Option&lt;i32&gt;) -&gt; String {
        if let Some(a) = x {
            format!("The value is: {}", a)
        } else {
            format!("No value")
        }
    }

    println!("{}", compute(Some(42))); // The value is: 42
    println!("{}", compute(None)); // No value
}
&lt;/source&gt;

=== Julia ===
Julia requires explicit deconstruction to access a nullable value:

&lt;source lang="julia"&gt;
function compute(x::Nullable{Int})
    if !isnull(x)
        println("The value is: $(get(x))")
    else
        println("No value")
    end
end
&lt;/source&gt;

&lt;source lang="jlcon"&gt;
julia&gt; compute(Nullable(42))
The value is: 42
julia&gt; compute(Nullable{Int}())
No value
&lt;/source&gt;

=== Perl 6 ===
There are as many null values as there are types, that is because every type is its own null.
So all types are also their own option type.

Basically when use type in a declaration, it can be a value of that type or a null of that type.

To designate that it must have a defined value assigned to it (not be in the null state), use the {{code|2=perl6|:D}} "smiley" designation.

It is also possible to designate that it must always be a null using the {{code|2=perl6|:U}} "smiley".

&lt;hr&gt;

* '''Default Option Type Declaration:'''

&lt;source lang="perl6"&gt;
# no type declaration
my $a;
say defined $a; # False
# has a typed null value
say $a; # (Any)
# Note that Any is the default base class

# with a type declaration
my Int   $b;
my Int:_ $b;    # same as above
say defined $b; # False
say $b;         # (Int)
$b = 42;
say defined $b; # True
&lt;/source&gt;

* '''Defined Declaration:'''

&lt;source lang="perl6"&gt;
# my Int:D $c;      # Error, you have to initialize it with a defined value
my Int:D $c = 0;
say defined $c;     # True
say $c.VAR.default; # (Int:D)
# $c = Nil;         # Error, the default is `Int:D` which isn't a defined value

# If you use the defined type "smiley" you may want to use `is default`
# to be able to accept a Nil. (Nil is not the same as a null in other languages)
my Int:D $d is default(10) = 0;
say $d;          # 0
$d = Nil;
say $d;          # 10

# no need to assign a value if it's the same as the default
my Int:D $e is default(10);
say $e;          # 10
&lt;/source&gt;

* '''Typed Null Declaration:'''

&lt;source lang="perl6"&gt;
my Numeric:U $n;
$n = 1.WHAT;
say $n;          # (Int)
$n = (1/2).WHAT;
say $n;          # (Rat)
# $n = 1;        # Error, only accepts undefined values
# $n = Str;      # Error, only accepts types that do the Numeric role
&lt;/source&gt;

* '''Signatures:'''

Type "smileys" are used more often for method and subroutine signatures than they are for variable declarations.

&lt;source lang="perl6"&gt;
proto sub is-it-defined ( Any:_ $ ) {*} # `Any:_` is the same as `Any`, only used here to line up the signatures

multi sub is-it-defined ( Any:U $ ) { 'undefined' } # a null value that is of type `Any` or a subtype
multi sub is-it-defined ( Any:D $ ) {   'defined' } # a non-null value of type `Any` or a subtype
&lt;/source&gt;

* '''Additional Syntax Relief:'''

There are special variations of {{code|2=perl6|if}} and {{code|2=perl6|unless}} called {{code|2=perl6|with}} and {{code|2=perl6|without}} that check for definedness rather than truthiness.

These set {{code|2=perl6|$_}} by default, unlike their boolean cousins.

&lt;source lang="perl6"&gt;
sub say-is-it-defined ( $value ) {
    # notice that these set `$_` to the argument by default, but `if` does not
    with    $value { say "$_.perl() is defined" }
    without $value { say "$_.perl() is not defined" }
}

say-is-it-defined 0;     # 0 is defined
say-is-it-defined '';    # "" is defined

say-is-it-defined Any;   # Any is not defined
say-is-it-defined my $;  # Any is not defined

my $a;
sub something-or-other { … }

# postfix variation of `with` also sets `$_`
$a = $_ with something-or-other;
# `$a` will change to the result, but only if the result was defined
&lt;/source&gt;

There are also variations of {{code|2=perl6|{{!}}{{!}}}}, {{code|2=perl6|or}} and {{code|2=perl6|and}} that test for definedness.

&lt;source lang="perl6"&gt;
say   0 // 42; # 0
say Nil // 42; # 42

# notice that these set `$_` to the left value
Nil orelse  say "$_.perl() is undefined"; # Nil is undefined
0   andthen say "$_.perl() is defined";   # 0 is defined
&lt;/source&gt;

== See also ==
* [[Monad (functional programming)]]
* [[Tagged union]]
* [[Nullable type]]
* [[Null Object pattern]]
* [[Sentinel value]]
* [[exception handling]]

== References ==
{{Reflist}}

{{Data types}}

[[Category:Functional programming]]
[[Category:Data types]]
[[Category:Type theory]]</text>
      <sha1>nek937yryvdwu0wmzmglxlkfidba6ko</sha1>
    </revision>
  </page>
  <page>
    <title>Oracle Unified Method</title>
    <ns>0</ns>
    <id>11735522</id>
    <revision>
      <id>747165162</id>
      <parentid>733037685</parentid>
      <timestamp>2016-10-31T20:23:34Z</timestamp>
      <contributor>
        <username>Ominfo123</username>
        <id>12950670</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2937">The '''Oracle Unified Method''' ('''OUM'''), first released by [[Oracle Corporation]] in 2006, is a [[standards-based method]] with roots in the [[Unified Process]] (UP). OUM is business-process and [[use case | use-case]] driven and includes support for the [[Unified Modeling Language]] (UML), though the use of UML is not required. OUM combines these standards with aspects of Oracle's legacy methods and Oracle implementation best-practices.

OUM is applicable to any size or type of information technology project. While OUM is a plan-based method – that includes extensive overview material, task and artifact descriptions, and associated templates – the method is intended to be tailored to support the appropriate level of ceremony required for each project. Guidance is provided for identifying the minimum subset of tasks, tailoring the project approach, executing iterative and incremental project planning, and applying [[Agile technique]]s. Supplemental guidance provides specific support for [[Oracle product]]s, tools, and technologies.

OUM v6.4.0 provides support for:
* Application Implementation
* Cloud Application Services Implementation
* Software Upgrade projects

as well as the complete range of technology projects including:
* [[Business intelligence]] (BI)
* Enterprise Security
* WebCenter
* [[Service-oriented architecture]] (SOA)
* Application Integration Architecture (AIA)
* Business Process Management (BPM)
* [[Enterprise integration]]
* Custom Software

Detailed techniques and tool guidance are provided, including a supplemental guide related to [[Oracle Tutor]] and UPK.

OUM is available for use by Oracle employees; for Oracle PartnerNetwork Diamond, Platinum, and Gold Partners; and for customers who participate in the OUM Customer Program.

Legacy method retirement dates:
* Oracle Custom Development Method (CDM), February 2010
* Oracle CDM Fast Track, February 2010
* Oracle Application Implementation Methodology (AIM), January 2011
* Oracle AIM for Business Flows, January 2011
* Oracle's Siebel Results Roadmap, January 2011
* Oracle Data Warehouse Method (DWM) Fast Track, May 2011
* Oracle EasiPath Migration Method (EMM), December 2011
* Oracle's [[PeopleSoft]] Compass Methodology, June 2013

== References ==
* [http://www.oracle.com/us/products/consulting/resource-library/oracle-unified-method1-069271.pdf Oracle Unified Method: Brief]
* [http://www.oracle.com/us/products/consulting/resource-library/oracle-unified-method-069204.pdf Oracle Unified Method: Oracle's Full Lifecycle Method for Deploying Oracle-based Business Solutions White Paper]
* [http://www.oracle.com/us/products/consulting/resource-library/unified-method-customer-program-069768.pdf Oracle Unified Method: Customer Program Data Sheet]
{{Reflist}}

== Further reading ==
* [https://blogs.oracle.com/oum/ OUM official blog]

{{Software engineering}}

[[Category:Formal methods]]
[[Category:Oracle software]]</text>
      <sha1>0j1ni3d0dgti4c9ynheb3nfkoh6qhzs</sha1>
    </revision>
  </page>
  <page>
    <title>Philip Rabinowitz (mathematician)</title>
    <ns>0</ns>
    <id>15046429</id>
    <revision>
      <id>854840267</id>
      <parentid>694740850</parentid>
      <timestamp>2018-08-14T04:29:01Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>[[Nira Dyn]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2850">{{Infobox scientist
| name              = Philip Rabinowitz
| image             = &lt;!--(as myimage.jpg, no 'File:')--&gt;
| image_size        = 
| alt               = 
| caption           = 
| birth_date        = {{Birth date|1926|8|14}}
| birth_place       = [[Philadelphia]], [[Pennsylvania]], [[United States]]
| death_date        = {{death date and age|2006|7|21|1926|8|14}}
| death_place       = [[Jerusalem]], [[Israel]]
| resting_place             = 
| resting_place_coordinates = &lt;!-- {{Coord|LAT|LONG|type:landmark|display=inline,title}} --&gt;
| residence         = 
| citizenship       = 
| nationality       = 
| fields            = [[applied mathematics]],&lt;br/&gt; [[numerical analysis]]
| workplaces        = [[National Bureau of Standards]],&lt;br/&gt; [[Weizmann Institute of Science]]
| alma_mater        = [[University of Pennsylvania]]
| thesis_title      = Normal Coverings and Uniform Spaces
| thesis_url        = 
| thesis_year       = 1951
| doctoral_advisor  = [[Walter Gottschalk]]
| academic_advisors = 
| doctoral_students = [[Nira Dyn]]
| notable_students  = 
| known_for         = 
| author_abbrev_bot = 
| author_abbrev_zoo = 
| influences        = 
| influenced        = 
| awards            = 
| signature         = &lt;!--(filename only)--&gt;
| signature_alt     = 
| website           = 
| footnotes         = 
| spouse            = 
}}
'''Philip Rabinowitz''' (August 14, 1926 &amp;ndash; July 21, 2006) was an [[United States|American]] and [[Israel]]i [[Applied mathematics|applied mathematician]].&lt;ref&gt;{{citation|title=Remembering Philip Rabinowitz|first1=Philip J.|last1=Davis|author1-link=Philip J. Davis|first2=Aviezri S.|last2=Fraenkel|author2-link=Aviezri Fraenkel|journal=Notices of the American Mathematical Society|volume=54|issue=11|url=http://www.ams.org/notices/200711/tx071101502p.pdf|pages=1502–1506|date=December 2007}}.&lt;/ref&gt; He was best known for his work in [[numerical analysis]], including his books ''A First Course in Numerical Analysis'' with Anthony Ralston and ''Methods of Numerical Integration'' with [[Philip J. Davis]]. He was the author of numerous articles on numerical computation.

He earned his [[Ph.D.]] in 1951 under [[Walter Gottschalk]] at the [[University of Pennsylvania]].&lt;ref&gt;{{mathgenealogy|id=19353}}&lt;/ref&gt; He worked for the [[United States|American]] [[National Bureau of Standards]] and taught at the [[Weizmann Institute of Science]] in [[Israel]].

==References==
{{reflist}}

==External links==
*[http://www.wisdom.weizmann.ac.il/mathusers/NoMoreUsers/rabino/ Personal web page] at the Weizmann Institute of Science

{{Authority control}}

{{DEFAULTSORT:Rabinowitz, Philip}}
[[Category:20th-century American mathematicians]]
[[Category:21st-century American mathematicians]]
[[Category:1926 births]]
[[Category:2006 deaths]]
[[Category:Weizmann Institute faculty]]


{{mathapplied-stub}}</text>
      <sha1>4aheaqguo6zzn69kyj3xftmspinncjt</sha1>
    </revision>
  </page>
  <page>
    <title>Quantum radar</title>
    <ns>0</ns>
    <id>10113122</id>
    <revision>
      <id>864762636</id>
      <parentid>859574450</parentid>
      <timestamp>2018-10-19T09:00:02Z</timestamp>
      <contributor>
        <username>BilCat</username>
        <id>2043601</id>
      </contributor>
      <comment>/* References */ Removed deprecated Reflist parameter</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8313">'''Quantum radar''' is an emerging [[Remote sensing|remote-sensing]] technology based on input quantum correlations (in particular, [[quantum entanglement]]) and output quantum detections. If it is successfully developed, it will allow the radar system to pick out its own signal even when swamped by background noise. This allows it to detect [[stealth aircraft]], filter out deliberate [[Radar jamming and deception|jamming]] attempts, and operate in areas of high background noise, e.g., due to ground [[Clutter (radar)|clutter]]. The first feasible design of a quantum radar was proposed in 2015 by an international team&lt;ref name=qradar&gt;{{Cite journal |last=Barzanjeh|first=Shabir |last2=Guha |first2=Saikat |last3=Weedbrook |first3=Christian|last4=Vitali|first4=David|last5=Shapiro|first5=Jeffrey H.|last6=Pirandola|first6=Stefano|date=2015-02-27|title=Microwave Quantum Illumination|url=https://link.aps.org/doi/10.1103/PhysRevLett.114.080503|journal=Physical Review Letters|volume=114|issue=8|pages=080503|doi=10.1103/PhysRevLett.114.080503|arxiv=1503.00189|bibcode=2015PhRvL.114h0503B}}&lt;/ref&gt;&lt;ref name=focus&gt;{{cite journal |journal=Physical Review Letters |date=27 February 2015 |volume=8 |page=18 |title=Focus: Quantum Mechanics Could Improve Radar |first= Philip |last=Ball |url=https://physics.aps.org/articles/v8/18}}&lt;/ref&gt; and is based on the protocol of Gaussian [[quantum illumination]].&lt;ref name=GaussQI&gt;{{Cite journal|last=Tan|first=Si-Hui|date=2008|title=Quantum Illumination with Gaussian States|url=https://link.aps.org/doi/10.1103/PhysRevLett.101.253601|journal=Physical Review Letters|volume=101|issue=25|doi=10.1103/PhysRevLett.101.253601|arxiv=0810.0534|bibcode=2008PhRvL.101y3601T}}&lt;/ref&gt;

==Concept==
The basic concept is to create a stream of entangled visible-frequency photons and split it in half. One half, the "signal beam", goes through a conversion to [[microwave]] frequencies in a way that preserves the original quantum state. The microwave signal is then sent and received as in a normal [[radar]] system. When the reflected signal is received it is converted back into photons and compared with the other half of the original entangled beam, the "idler beam".&lt;ref name=qradar/&gt;&lt;ref name=focus/&gt;

Although most of the original entanglement will be lost due to [[quantum decoherence]] as the microwaves travel to the target objects and back, enough quantum correlations will still remain between the reflected-signal and the idler beams. Using a suitable quantum detection scheme, the system can pick out just those photons that were originally sent by the radar, completely filtering out any other sources. If the system can be made to work in the field, it represents an enormous advance in detection capability.&lt;ref name=focus/&gt;

One way to defeat conventional radar systems is to broadcast signals on the same frequencies used by the radar, making it impossible for the receiver to distinguish between their own broadcasts and the spoofing signal (or "jamming"). However, such systems cannot know, even in theory, what the original quantum state of the radar's internal signal was. Lacking such information, their broadcasts will not match the original signal and will be filtered out in the correlator. Environmental sources, like ground [[Clutter (radar)|clutter]] and [[aurora]], will similarly be filtered out.&lt;ref name=focus/&gt;

There is considerable discussion of the use of quantum radar as an anti-stealth technology&lt;ref name=Seffers&gt;{{cite news|url=http://www.afcea.org/content/?q%3DArticle-quantum-radar-could-render-stealth-aircraft-obsolete |title=Quantum Radar Could Render Stealth Aircraft Obsolete |accessdate=2015-07-03 |work=SIGNAL (AFCEA magazine) |first=George |last=Seffers |date=2015-07-01 |deadurl=yes |archiveurl=https://web.archive.org/web/20150704090325/http://www.afcea.org/content/?q=Article-quantum-radar-could-render-stealth-aircraft-obsolete |archivedate=2015-07-04 |df= }}&lt;/ref&gt;. [[Stealth aircraft]] are designed to reflect signals away from the radar, typically by using rounded surfaces and avoiding anything that might form a partial [[corner reflector]]. This so reduces the amount of signal returned to the radar's receiver that the target is (ideally) lost in the [[Johnson–Nyquist noise|thermal background noise]]. Although stealth technologies will still be just as effective at reflecting the original signal away from the receiver of a quantum radar, it is the system's ability to separate out the remaining tiny signal, even when swamped by other sources, that allows it to pick out the return even from highly stealthy designs.&lt;ref name=focus/&gt;&lt;ref name=Seffers/&gt;

==History==
The most convincing model was proposed by an international team of researchers&lt;ref name=qradar/&gt;&lt;ref name=focus/&gt;. The team designed a model of quantum radar for remote sensing of a low-reflectivity target that is embedded within a bright [[microwave]] background, with detection performance well beyond the capability of a classical microwave radar. By using a suitable wavelength "electro-optomechanical converter", this scheme generates excellent [[quantum entanglement]] between a microwave signal beam, sent to probe the target region, and an optical idler beam, retained for detection. The microwave return collected from the target region is subsequently converted into an optical beam and then measured jointly with the idler beam. Such a technique extends the powerful protocol of [[quantum illumination]]&lt;ref name=GaussQI&gt;&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Lloyd|first=Seth|date=2008-09-12|title=Enhanced Sensitivity of Photodetection via Quantum Illumination|url=http://science.sciencemag.org/content/321/5895/1463|journal=Science|language=en|volume=321|issue=5895|pages=1463–1465|doi=10.1126/science.1160627|issn=0036-8075|pmid=18787162|bibcode=2008Sci...321.1463L}}&lt;/ref&gt; to its more natural spectral domain, namely microwave wavelengths. A prototype quantum radar could be realized with current technology, and is suited to applications from standoff sensing of [[Stealth technology|stealth]] objects&lt;ref name=Seffers/&gt; to environmental scanning of electrical circuits. Thanks to its quantum-enhanced sensitivity, this device could lead to low-flux non-invasive techniques for protein spectroscopy and biomedical imaging.&lt;ref&gt;{{Cite web|url=http://www.york.ac.uk/news-and-events/news/2015/research/quantum-radar/|title=New research signals big future for quantum radar - News and events, The University of York|website=www.york.ac.uk|language=en|access-date=2018-04-30}}&lt;/ref&gt; Alternative methods were also considered by defense contractor [[Lockheed Martin]]&lt;ref&gt;{{ cite news | url = https://www.theguardian.com/science/story/0,,2027298,00.html | title = US defence contractor looks for quantum leap in radar research | accessdate = 2007-03-17 | work=The Guardian | location=London | first=David | last=Adam | date=2007-03-06}}&lt;/ref&gt;&lt;ref name="EP1750145"&gt;{{cite patent |country=EP |number=1750145 |status=grant |title=Radar systems and methods using entangled quantum particles |pubdate= |gdate=2013-03-13 |fdate= |pridate=2005-08-04 |inventor=Edward H. Allen |assign1=Lockheed Martin Corp |url=https://patents.google.com/patent/EP1750145B1/ }}&lt;/ref&gt; whose aim was to create a radar system providing a better resolution and higher detail than classical radar could provide.&lt;ref&gt;Marco Lanzagorta, ''[https://www.amazon.com/Quantum-Radar-Synthesis-Lectures-Computing/dp/1608458261 Quantum Radar]'', Morgan &amp; Claypool (2011).&lt;/ref&gt; According to [[China|Chinese]] state media, the first quantum radar was developed and tested by China in real-world environment in August 2016.&lt;ref&gt;{{Cite news|url=https://www.rt.com/news/358664-china-quantum-radar-test/|title=China says it has stealth-defeating quantum radar|work=RT International|access-date=2018-04-30|language=en-US}}&lt;/ref&gt;. More recently, the generation of large numbers of entangled photons for radar detection has been studied by the [[University of Waterloo]].&lt;ref&gt;{{cite news |url=https://www.bbc.com/news/technology-43877682 |title= Canada developing quantum radar to detect stealth aircraft |first=Mary-Ann |last=Russon |date= 24 April 2018 |website=BBC}}&lt;/ref&gt;

== References ==
{{reflist}}

[[Category:Quantum optics]]
[[Category:Quantum information science]]
[[Category:Radar]]</text>
      <sha1>6gw792cxfckjad0bwgz8oeknyhpzwwq</sha1>
    </revision>
  </page>
  <page>
    <title>Robert Ammann</title>
    <ns>0</ns>
    <id>1505379</id>
    <revision>
      <id>857402793</id>
      <parentid>828921723</parentid>
      <timestamp>2018-08-31T13:22:59Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>/* References */add authority control, test</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4482">'''Robert Ammann''' (October 1, 1946 – May, 1994) was an [[List of amateur mathematicians|amateur mathematician]] who made several significant and groundbreaking contributions to the theory of [[quasicrystal]]s and [[aperiodic tiling]]s.

[[Image:AmmannBeenker.jpg|300px|thumb|right|[[Ammann–Beenker tiling]]]]
Ammann attended [[Brandeis University]], but generally did not go to classes, and left after three years. He worked as a [[programmer]] for [[Honeywell]]. After ten years, his position was eliminated as part of a routine cutback, and Ammann ended up working as a mail sorter for a [[post office]].

In 1975, Ammann read an announcement by [[Martin Gardner]] of new work by [[Roger Penrose]]. Penrose had discovered two simple sets of aperiodic tiles, each consisting of just two [[quadrilaterals]]. Since Penrose was taking out a [[patent]], he wasn't ready to publish them, and Gardner's description was rather vague. Ammann wrote a letter to Gardner, describing his own work, which duplicated one of Penrose's sets, plus a foursome of "[[Golden rhombus|golden rhombohedra]]" that formed aperiodic tilings in space.

More letters followed, and Ammann became a correspondent with many of the professional researchers. He discovered several new aperiodic tilings, each among the simplest known examples of aperiodic sets of tiles. He also showed how to generate tilings using lines in the plane as guides for lines marked on the tiles, now called "[[Ammann-Beenker tiling|Ammann bars]]".

The discovery of [[quasicrystal]]s in 1982 changed the status of aperiodic tilings and Ammann's work from mere [[recreational mathematics]] to respectable academic research.

After more than ten years of coaxing, he agreed to meet various professionals in person, and eventually even went to two conferences and delivered a lecture at each. Afterwards, Ammann dropped out of sight, and died of a heart attack a few years later.  News of his death did not reach the research community for a few more years.

Five sets of tiles discovered by Ammann were described in ''Tilings and Patterns''&lt;ref&gt;[[Branko Grünbaum|B. Grünbaum]] and G.C. Shephard, ''Tilings and Patterns'', Freemann, NY 1986&lt;/ref&gt; and later, in collaboration with the authors of the book, he published a paper&lt;ref&gt;R.Ammann, B. Grünbaum and G.C. Shephard, Aperiodic Tiles, ''[[Discrete Comput Geom]]'' 8 (1992),1–25&lt;/ref&gt; proving the aperiodicity for four of them. Ammann's discoveries came to notice only after Penrose had published his own discovery and gained priority. In 1981 [[Nicolaas Govert de Bruijn|de Bruijn]] exposed the cut and project method  and in 1984 came the sensational news about [[Dan Shechtman|Shechtman]] quasicrystals which promoted the [[Penrose tiling]]  to fame. But in 1982 Beenker published a similar mathematical explanation for the octagonal case which became known as the [[Ammann–Beenker tiling]].&lt;ref&gt;Beenker FPM, "Algebraic theory of non periodic tilings of the plane by two simple building blocks: a square and a rhombus", ''TH Report'' 82-WSK-04 (1982), [[Technische Hogeschool, Eindhoven]]&lt;/ref&gt; In 1987 Wang, Chen and Kuo announced the discovery of a quasicrystal with octagonal symmetry.&lt;ref&gt;{{cite journal |last=Wang |first=N. |last2=Chen |first2=H. |last3=Kuo |first3=K. |title=none |journal=[[Phys. Rev. Lett.]] |volume=59 |year=1987 |issue= |pages=1010 | doi = 10.1103/physrevlett.59.1010 }}&lt;/ref&gt; The decagonal covering of the Penrose tiling was proposed in 1996 and  two years later F. Gahler proposed an octagonal variant for the Ammann–Beenker tiling&lt;ref&gt;S. Ben Abraham and F. Gahler, ''[[Phys. Rev. B]]'' 60(1999)860&lt;/ref&gt; Ammann's name  became that of the perennial second. It is acknowledged however that Robert Ammann  first proposed the  construction of rhombic prisms which is the three-dimensional model of Shechtman's quasicrystals.

==Notes==
{{Reflist}}

==References==
*{{citation|last=Senechal|first=Marjorie|authorlink=Marjorie Senechal|title=The Mysterious Mr. Ammann|journal=[[The Mathematical Intelligencer]]|volume=26|issue=4|year=2004|pages=10–21|doi=10.1007/BF02985414|mr=2104463}}.
* Amman tilings and references at the [http://tilings.math.uni-bielefeld.de/tilings/mld_class/ammann  Tilings encyclopedia]

{{authority control}}

{{DEFAULTSORT:Ammann, Robert}}
[[Category:Amateur mathematicians]]
[[Category:20th-century American mathematicians]]
[[Category:1946 births]]
[[Category:1994 deaths]]
[[Category:Brandeis University alumni]]</text>
      <sha1>00mhfp330d30thbtjze5bfvpb9jkx09</sha1>
    </revision>
  </page>
  <page>
    <title>Scott Draves</title>
    <ns>0</ns>
    <id>1443423</id>
    <revision>
      <id>844112270</id>
      <parentid>837482074</parentid>
      <timestamp>2018-06-02T18:03:40Z</timestamp>
      <contributor>
        <username>Robertgombos</username>
        <id>17240037</id>
      </contributor>
      <minor/>
      <comment>+ [[Wikipedia:Authority control|Authority control]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4438">{{Infobox artist
| name          = Scott Draves
| image         = Scott_Draves_2014.png  
| imagesize     = 250px
| caption       = 
| birth_name    = 
| birth_date    = {{Birth year and age|1968}}
| birth_place   = 
| death_date    =
| death_place   = 
| nationality   = [[United States|American]]
| field         = [[Software art]]
| training      = 
| movement      = 
| works         = Flame, Fuse, Bomb, Electric Sheep, Dreams in High Fidelity
| patrons       =
| influenced by =
| influenced    =
| awards        = Prix Ars Electronica, Vida 2.0, Vida 4.0, ZKM App Art Award
}}
[[Image:Electricsheep-0-1000.jpg|thumb|200px|An image from the Electric Sheep.]]

'''Scott Draves''' is the inventor of [[Fractal Flames]]&lt;ref&gt;{{cite web|last=Birch|first=K.|url=http://www.cogito.org/Interviews/InterviewsDetail.aspx?ContentID=16808|archive-url=https://web.archive.org/web/20070827160027/http://www.cogito.org/Interviews/InterviewsDetail.aspx?ContentID=16808|dead-url=yes|archive-date=2007-08-27|title=Cogito Interview: Damien Jones, Fractal Artist|date=2007-08-20}}&lt;/ref&gt; and the leader of the [[distributed computing]] project [[Electric Sheep]].&lt;ref&gt;{{cite news|last=Johnson|first=S.|title=Sheep in Shining Armor|publisher=Discover Magazine|date=August 2004}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.newyorker.com/archive/2004/06/07/040607ta_talk_wilkinson |first=Alec |last=Wilkinson |title=Incomprehensible|publisher=New Yorker Magazine |date=2004-06-07}}&lt;/ref&gt; He also invented patch-based [[texture synthesis]] and published the first implementation of this class of algorithms.  He is also a [[video artist]]&lt;ref&gt;{{cite web| last=Bamberger|first=A.|url=http://www.artbusiness.com/1open/011807.html|title=San Francisco Art Galleries - Openings|date=2007-01-18|accessdate=2008-03-11}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.baxterchangpatri.com/artwork.html|title=Gallery representing Draves' video art|accessdate=2008-03-11 |archiveurl = https://web.archive.org/web/20080606060122/http://www.baxterchangpatri.com/artwork.html &lt;!-- Bot retrieved archive --&gt; |archivedate = 2008-06-06}}&lt;/ref&gt; and accomplished [[VJ (video performance artist)|VJ]].&lt;ref&gt;{{cite web|url=http://www.keyboardmag.com/article/vj-its-not/apr-05/7446|title=VJ: It's not a disease|publisher=Keyboard Magazine|date=April 2005|deadurl=yes|archiveurl=https://web.archive.org/web/20080412020022/http://www.keyboardmag.com/article/vj-its-not/apr-05/7446|archivedate=2008-04-12|df=}}&lt;/ref&gt;

In summer 2010, Draves' work was exhibited at Google's New York City office, including his video piece "Generation 243" which was generated by the collaborative influences of 350,000 people and computers worldwide.&lt;ref&gt;[http://www.fest21.com/en/blog/avivapress/digital_art_google_data_poetics_presents_scott_draves_generation_243 Digital Art @Google Data Poetics Presents Scott Draves’ Generation 243.]&lt;/ref&gt; [[Stephen Hawking]]'s 2010 book [[The Grand Design (book)|The Grand Design]] used an image generated by Draves' "flame" algorithm on its cover. Known as "Spot,"&lt;ref&gt;{{cite web|last=Windley|first=P.|url=http://itc.conversationsnetwork.org/shows/detail1071.html|title=Art of Networks|date=2006-05-07}}&lt;/ref&gt; Draves currently resides in [[New York City]].

In July 2012 Draves won the [[ZKM]] App Art Award Special Prize for Cloud Art for the mobile Android version of Electric Sheep.&lt;ref&gt;{{cite web|url=http://container.zkm.de/presse/PM_AAA2_Gewinner_E.pdf|title=ZKM Press Release}}&lt;/ref&gt;

== Background ==
Draves earned a Bachelor's in mathematics at [[Brown University]], where he was a student of [[Andries van Dam|Andy van Dam]] before continuing on to earn a PhD in computer science at [[Carnegie Mellon University]].&lt;ref&gt;{{cite web|url=http://www.cs.cmu.edu/~fox/staged-bib.html|title=Bibliography of Draves' CMU research papers|accessdate=2008-03-11}}&lt;/ref&gt;  At CMU he studied under [[Andy Witkin]], [[Dana Scott]], and [[Peter Lee (computer scientist)|Peter Lee]].

==References==
{{reflist}}

==External links==
*{{Official website|www.scottdraves.com}}

{{Mathematical art}}

{{authority control}}

{{DEFAULTSORT:Draves, Scott}}
[[Category:Computer programmers]]
[[Category:American digital artists]]
[[Category:Psychedelic artists]]
[[Category:American video artists]]
[[Category:Brown University alumni]]
[[Category:Carnegie Mellon University alumni]]
[[Category:Living people]]
[[Category:1968 births]]
[[Category:Mathematical artists]]


{{US-compu-bio-stub}}</text>
      <sha1>9f5ilvpii6rsj7qr418szgjpwmm3xnk</sha1>
    </revision>
  </page>
  <page>
    <title>Simplicial approximation theorem</title>
    <ns>0</ns>
    <id>583637</id>
    <revision>
      <id>795047925</id>
      <parentid>795047845</parentid>
      <timestamp>2017-08-11T17:28:33Z</timestamp>
      <contributor>
        <ip>151.230.70.150</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2967">In [[mathematics]], the '''simplicial approximation theorem''' is a foundational result for [[algebraic topology]], guaranteeing that [[continuous mapping]]s can be (by a slight deformation) approximated by ones that are [[piecewise]] of the simplest kind. It applies to mappings between spaces that are built up from [[simplex|simplices]]&amp;mdash;that is, finite [[simplicial complex]]es. The general continuous mapping between such spaces can be represented approximately by the type of mapping that is (''affine''-) linear on each simplex into another simplex, at the cost (i) of sufficient [[barycentric subdivision]] of the simplices of the domain, and (ii) replacement of the actual mapping by a [[homotopic]] one.

This theorem was first proved by [[L.E.J. Brouwer]], by use of the [[Lebesgue covering theorem]] (a result based on [[compactness]]). It served to put the [[homology theory]] of the time&amp;mdash;the first decade of the twentieth century&amp;mdash;on a rigorous basis, since it showed that the topological effect (on [[homology group]]s) of continuous mappings could in a given case be expressed in a [[finitary]] way. This must be seen against the background of a realisation at the time that continuity was in general compatible with the [[pathological (mathematics)|pathological]], in some other areas. This initiated, one could say, the era of [[combinatorial topology]].

There is a further '''simplicial approximation theorem for homotopies''', stating that a [[homotopy]] between continuous mappings can likewise be approximated by a combinatorial version.

==Formal statement of the theorem==

Let &lt;math&gt; K &lt;/math&gt; and &lt;math&gt; L &lt;/math&gt; be two [[simplicial complex]]es. A [[simplicial map|simplicial mapping]] &lt;math&gt; f : K \to L &lt;/math&gt; is called a simplicial approximation of a continuous function &lt;math&gt; F : |K| \to |L| &lt;/math&gt; if for every point &lt;math&gt; x \in |K| &lt;/math&gt;, &lt;math&gt; |f|(x) &lt;/math&gt; belongs to the minimal closed simplex of &lt;math&gt; L &lt;/math&gt; containing the point &lt;math&gt; F(x) &lt;/math&gt;. If &lt;math&gt; f &lt;/math&gt; is a simplicial approximation to a continuous map &lt;math&gt; F &lt;/math&gt;, then the geometric realization of &lt;math&gt; f &lt;/math&gt;, &lt;math&gt; |f| &lt;/math&gt; is necessarily homotopic to &lt;math&gt; F &lt;/math&gt;.

The simplicial approximation theorem states that given any continuous map &lt;math&gt; F : |K| \to |L| &lt;/math&gt; there exists a natural number &lt;math&gt; n_0 &lt;/math&gt; such that for all &lt;math&gt; n \ge n_0 &lt;/math&gt; there exists a simplicial approximation &lt;math&gt; f : \mathrm{Bd}^n K \to L &lt;/math&gt; to &lt;math&gt; F &lt;/math&gt; (where &lt;math&gt; \mathrm{Bd}\; K &lt;/math&gt; denotes the [[barycentric subdivision]] of &lt;math&gt; K &lt;/math&gt;, and &lt;math&gt; \mathrm{Bd}^n K &lt;/math&gt; denotes the result of applying barycentric subdivision &lt;math&gt; n &lt;/math&gt; times.)

==References==
*{{Springer|id=Simplicial_complex|title=Simplicial complex}}

{{DEFAULTSORT:Simplicial Approximation Theorem}}
[[Category:Continuous mappings]]
[[Category:Simplicial sets]]
[[Category:Theorems in algebraic topology]]</text>
      <sha1>atueetyt4jpnnz0d558tl8ugb147xba</sha1>
    </revision>
  </page>
  <page>
    <title>Spitzer's formula</title>
    <ns>0</ns>
    <id>37986691</id>
    <revision>
      <id>846772595</id>
      <parentid>845118955</parentid>
      <timestamp>2018-06-20T20:06:42Z</timestamp>
      <contributor>
        <username>Bibcode Bot</username>
        <id>14394459</id>
      </contributor>
      <minor/>
      <comment>Adding 0 [[arXiv|arxiv eprint(s)]], 1 [[bibcode|bibcode(s)]] and 0 [[digital object identifier|doi(s)]]. Did it miss something? Report bugs, errors, and suggestions at [[User talk:Bibcode Bot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2585">In [[probability theory]], '''Spitzer's formula''' or '''Spitzer's identity''' gives the joint distribution of partial sums and maximal partial sums of a collection of random variables. The result was first published by [[Frank Spitzer]] in 1956.&lt;ref name="spitzer"&gt;{{Cite journal | last1 = Spitzer | first1 = F. | authorlink = Frank Spitzer| title = A combinatorial lemma and its application to probability theory | doi = 10.1090/S0002-9947-1956-0079851-X | journal = Transactions of the American Mathematical Society | volume = 82 | issue = 2 | pages = 323–339 | year = 1956 | pmid =  | pmc = }}&lt;/ref&gt; The formula is regarded as "a stepping stone in the theory of sums of independent random variables".&lt;ref&gt;{{Cite journal | last1 = Ebrahimi-Fard | first1 = K. | last2 = Guo | first2 = L. | last3 = Kreimer | first3 = D. | doi = 10.1088/0305-4470/37/45/020 | title = Spitzer's identity and the algebraic Birkhoff decomposition in pQFT | journal = Journal of Physics A: Mathematical and General | volume = 37 | issue = 45 | pages = 11037 | year = 2004 | pmid =  | pmc = | arxiv = hep-th/0407082 | bibcode = 2004JPhA...3711037E }}&lt;/ref&gt;

==Statement of theorem==

Let ''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;, ... be [[independent and identically distributed random variables]] and define the partial sums ''S''&lt;sub&gt;''n''&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''X''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;''X''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;...&amp;nbsp;+&amp;nbsp;''X''&lt;sub&gt;''n''&lt;/sub&gt;. Define ''R''&lt;sub&gt;''n''&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;max(0,''S''&lt;sub&gt;1&lt;/sub&gt;,''S''&lt;sub&gt;2&lt;/sub&gt;,...,''S''&lt;sub&gt;''n''&lt;/sub&gt;). Then&lt;ref name="wendel" /&gt;

::&lt;math&gt;\sum_{n=0}^\infty \phi_n(\alpha,\beta)t^n = \exp \left[ \sum_{n=1}^\infty \frac{t^n}{n} \left( u_n (\alpha) + v_n(\beta) -1 \right) \right]&lt;/math&gt;

where

::&lt;math&gt;\begin{align}
\phi_n(\alpha,\beta) &amp;= \operatorname E(\exp\left[ i(\alpha R_n + \beta(R_n-S_n)\right])\\
u_n(\alpha) &amp;= \operatorname E(\exp \left[i\alpha S_n^+\right]) \\
v_n(\beta) &amp;= \operatorname E(\exp \left[i \beta S_n^-\right])
\end{align}&lt;/math&gt;

and ''S''&lt;sup&gt;±&lt;/sup&gt; denotes (|''S''|&amp;nbsp;±&amp;nbsp;''S'')/2.

===Proof===

Two proofs are known, due to Spitzer&lt;ref name="spitzer" /&gt; and Wendel.&lt;ref name="wendel"&gt;{{Cite journal | last1 = Wendel | first1 = James G. | title = Spitzer's formula: A short proof | doi = 10.1090/S0002-9939-1958-0103531-2 | journal = Proceedings of the American Mathematical Society | volume = 9 | issue = 6 | pages = 905–908 | year = 1958 | mr = 0103531 | pmid =  | pmc = }}&lt;/ref&gt;

==References==

{{reflist}}

[[Category:Stochastic processes]]
[[Category:Probability theorems]]


{{probability-stub}}</text>
      <sha1>3uh046259vspr87r61i3cv2vo5joirw</sha1>
    </revision>
  </page>
  <page>
    <title>Starlight Information Visualization System</title>
    <ns>0</ns>
    <id>24226726</id>
    <revision>
      <id>844205436</id>
      <parentid>837195003</parentid>
      <timestamp>2018-06-03T10:32:59Z</timestamp>
      <contributor>
        <username>Arthur MILCHIOR</username>
        <id>9557656</id>
      </contributor>
      <minor/>
      <comment>Moving a category to a more precise subcategory</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5094">{{Notability|Products|date=April 2018}}
'''Starlight''' is a software product originally developed at [[Pacific Northwest National Laboratory]] and now by Future Point Systems. It is an advanced [[Visual analytics|visual analysis]] environment. In addition to using [[information visualization]] to show the importance of individual pieces of data by showing how they relate to one another, it also contains a small suite of tools useful for collaboration and data sharing, as well as data conversion, processing, augmentation and loading.

The software, originally developed for the [[United States Intelligence Community|intelligence community]], allows users to load data from XML files, databases, RSS feeds, web services, HTML files, Microsoft Word, PowerPoint, Excel, CSV, Adobe PDF, TXT files, etc. and analyze it with a variety of visualizations and tools. The system integrates structured, unstructured, geospatial, and multimedia data, offering comparisons of information at multiple levels of abstraction, simultaneously and in near real-time. In addition Starlight allows users to build their own named entity-extractors using a combination of algorithms, targeted normalization lists and regular expressions in the Starlight Data Engineer (SDE).

As an example, Starlight might be used to look for correlations in a database containing records about chemical spills. An analyst could begin by grouping records according to the cause of the spill to reveal general trends. Sorting the data a second time, they could apply different colors based on related details such as the company responsible, age of equipment or geographic location. Maps and photographs could be integrated into the display, making it even easier to recognize connections among multiple variables.

Starlight has been deployed to both the Iraq and Afghanistan wars and used on a number of large-scale projects.

PNNL began developing Starlight in the mid-90's, with funding from the Land Information Warfare Agency,&lt;ref&gt;[http://www.eurekalert.org/features/doe/2002-03/dnnl-ias061402.php Information analysis—by Starlight]&lt;/ref&gt; a part of the Army Intelligence and Security Command and continued developed at the laboratory with funding from the NSA and the CIA. Starlight integrates visual representations of reports, radio transcripts, radar signals, maps and other information. The software system was recently honored with an R&amp;D 100 Award&lt;ref&gt;[http://www.rdmag.com/ShowPR.aspx?PUBCODE=014&amp;ACCT=1400000100&amp;ISSUE=0309&amp;RELTYPE=PR&amp;ORIGRELTYPE=FE&amp;PRODCODE=00000000&amp;PRODLETT=AE&amp;CommonCount=0 R&amp;D 100 Awards]&lt;/ref&gt; for technical innovation.

In 2006 Future Point Systems, a Silicon Valley startup, acquired rights to jointly develop and distribute the Starlight product in cooperation with the Pacific Northwest National Laboratory.

The software is now also used outside of the military/intelligence communities in a number of commercial environments.

==References==
{{reflist|2}}

==Further reading==
* [http://platts.com/Electric%20Power/Newsletters%20&amp;%20Reports/Inside%20Energy/ Inside Energy With Federal Lands.] (March 10, 2003) ''PNNL offers info-system license.'' Volume 20; Issue 45; Page 2
*Research &amp; Development. (September 1, 2003) ''[http://www.rdmag.com/ShowPR.aspx?PUBCODE=014&amp;ACCT=1400000100&amp;ISSUE=0309&amp;RELTYPE=PR&amp;ORIGRELTYPE=FE&amp;PRODCODE=00000000&amp;PRODLETT=AE&amp;CommonCount=0 Software Offers 3-D Data Management.]'' Section: Special; Volume 45; Issue 9; Page 58.
*R&amp;D Management. (September 1, 2003) ''[http://www.battelle.org/news/03/8-21-03R&amp;D%20100%20Awards.stm 11 innovative, award winning technologies.]'' Volume 45; Issue 9; Page 18.
*Kritzstein, Brian. (December 10, 2003) Military Geospatial Technology. ''[http://www.military-geospatial-technology.com/article.cfm?DocID=339 Starlight, the leading edge of an emerging class of information systems that couples advanced information modeling and management techniques within a visual interface, links data and information top points on a map.]'' Volume: 1 Issue: 1
*[[United States Army Corps of Engineers|Commercial terrain visualization software product information.]] (2003) ''[http://www.tec.army.mil/TD/tvd/survey/Starlight.html Pacific Northwest National Laboratory (PNNL) ; Starlight.]''
*Reid, Hal. (March 8, 2005) Directions Magazine. ''[http://www.directionsmag.com/article.php?article_id=775 Starlight Overview and Interview with Battelle's Brian Kritzstein.]
*St. John, Jeff. (February 16, 2006) [[Tri-City Herald]] ''PNNL earns 4 technology awards.''
*Ascribe Newswire. (February 16, 2006) ''[http://www.google.com/search?num=100&amp;hl=en&amp;safe=off&amp;q=Pacific+Northwest+National+Laboratory+Recognized+for+Commercializing+Technology&amp;btnG=Search Pacific Northwest National Laboratory Recognized for Commercializing Technology.]''

==External links==
* [http://starlight.pnl.gov Starlight PNNL website]
* [http://www.futurepointsystems.com Starlight Official Website]

[[Category:Computational science]]
[[Category:Computer graphics]]
[[Category:Infographics]]
[[Category:Scientific modeling]]
[[Category:Data visualization software]]</text>
      <sha1>aa6r5vjq1t5bznark9kd0ijqt1ht2e7</sha1>
    </revision>
  </page>
  <page>
    <title>Steve Butler (mathematician)</title>
    <ns>0</ns>
    <id>58673886</id>
    <revision>
      <id>864791133</id>
      <parentid>862790683</parentid>
      <timestamp>2018-10-19T13:49:11Z</timestamp>
      <contributor>
        <username>Vycl1994</username>
        <id>19014806</id>
      </contributor>
      <comment>/* Education and career */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2798">'''Steven Kay Butler''' (born May 16, 1977) is an American mathematician specializing in [[graph theory]] and [[combinatorics]]. He is an associate professor of mathematics and Barbara J. Janson Professor in Mathematics at [[Iowa State University]].

==Education and career==
Butler earned his master's degree at [[Brigham Young University]] in 2003. His master's thesis was titled ''Bounding the Number of Graphs Containing Very Long Induced Paths''.&lt;ref&gt;{{cite journal |last1=Butler |first1=Steven Kay |title=Bounding the Number of Graphs Containing Very Long Induced Paths |date=February 7, 2003 |url=https://scholarsarchive.byu.edu/etd/31/ |accessdate=October 6, 2018 |publisher=Brigham Young University}}&lt;/ref&gt; He completed a doctorate at the [[University of California, San Diego]] in 2008, authoring the dissertation ''Eigenvalues and Structures of Graphs'', advised by [[Fan Chung]].&lt;ref&gt;{{cite journal |last1=Butler |first1=Steven Kay |title=Eigenvalues and Structures of Graphs |url=https://orion.math.iastate.edu/butler/PDF/dissertation.pdf |publisher=University of California, San Diego |date=2008 |accessdate=October 6, 2018}}&lt;/ref&gt; Upon completing his postdoctoral studies at the [[University of California, Los Angeles]], Butler joined the [[Iowa State University]] faculty in 2011, and was named the Barbara J. Janson Professor in Mathematics in 2017.&lt;ref&gt;{{cite news |title=Steve Butler receives Barbara J. Janson Professorship in Mathematics |url=https://math.iastate.edu/2017/09/19/steve-butler-receives-barbara-j-janson-professorship-in-mathematics/ |accessdate=October 6, 2018 |publisher=Iowa State University |date=September 19, 2017}}&lt;/ref&gt; Butler's [[Erdős number]] is one.&lt;ref&gt;{{cite news |last1=Roberts |first1=Siobhan |title=New Erdős Paper Solves Egyptian Fraction Problem |url=https://www.simonsfoundation.org/2015/12/10/new-erdos-paper-solves-egyptian-fraction-problem/ |accessdate=October 19, 2018 |publisher=Simons Foundation |date=December 10, 2015}}&lt;/ref&gt; In 2015, Butler published the paper ''Egyptian Fractions With Each Denominator Having Three Distinct Prime Divisors'' with coauthors [[Paul Erdős]] and [[Ronald Graham]].&lt;ref&gt;{{cite journal |title=Egyptian Fractions With Each Denominator Having Three Distinct Prime Divisors |date=2015 |url=http://www.math.ucsd.edu/~ronspubs/pre_tres_egyptian.pdf |first1=Steve |last1=Butler |first2=Paul |last2=Erdős |first3=Ron| last3=Graham}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
*{{Google Scholar id|43G6knEAAAAJ}}

{{authority control}}
{{DEFAULTSORT:Butler, Steve}}
[[Category:1977 births]]
[[Category:Living people]]
[[Category:Graph theorists]]
[[Category:21st-century American mathematicians]]
[[Category:Brigham Young University alumni]]
[[Category:University of California, San Diego alumni]]</text>
      <sha1>aiksdck2cv8fnld8gggi27b81qt3oiy</sha1>
    </revision>
  </page>
  <page>
    <title>Timeline of programming languages</title>
    <ns>0</ns>
    <id>23696</id>
    <revision>
      <id>870930167</id>
      <parentid>870929437</parentid>
      <timestamp>2018-11-27T21:33:05Z</timestamp>
      <contributor>
        <username>Zupanto</username>
        <id>28902360</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="45144">{{Multiple issues|
{{refimprove|date=December 2010}}
{{primary sources|date=December 2010}}
}}
{{Use dmy dates|date=July 2012}}
{{Programming language lists}}
This is a record of historically important [[programming language]]s, by decade.

==Pre-1950==
{| class="wikitable sortable"
|-
! Year
! Name
! Chief developer, company
! Predecessor(s)
|-
|1804
|[[Jacquard Loom]]
|[[Joseph Marie Jacquard]]
| {{n/a|none (unique language)}}
|-
|1943–45
|[[Plankalkül]] (concept)
|[[Konrad Zuse]]
|{{n/a|none (unique language)}}
|-
| 1943–46
| [[ENIAC|ENIAC coding system]]
| [[John von Neumann]], [[John Mauchly]], [[J. Presper Eckert]], [[Herman Goldstine]] after [[Alan Turing]]
| {{n/a|none (unique language)}}
|-
| 1946
| [[ENIAC Short Code]]
| Richard Clippinger, [[John von Neumann]] after [[Alan Turing]]
| ENIAC coding system
|-
| 1946
| [[Von Neumann and Goldstine graphing system]] (Notation)
| [[John von Neumann]] and [[Herman Goldstine]]
| ENIAC coding system
|-
| 1947
| [[ARC Assembly]]
| [[Kathleen Booth]]&lt;ref&gt;{{cite web|last=Booth|first=Kathleen|title=machine language for Automatic Relay Computer|work=Birkbeck College Computation Laboratory,|publisher=University of London}}&lt;/ref&gt;&lt;ref&gt;Campbell-Kelly, Martin "The Development of Computer Programming in Britain (1945 to 1955)", The Birkbeck College Machines, in (1982) Annals of the History of Computing 4(2) April 1982 IEEE&lt;/ref&gt;
| ENIAC coding system
|-
| 1948
| [[CPC Coding scheme]]
| [[Howard H. Aiken]]
| Analytical Engine order code
|-
| 1948
| [[Curry notation system]]
| [[Haskell Curry]]
| ENIAC coding system
|-
| 1948
| [[Plankalkül]] (concept published)
| [[Konrad Zuse]]
| {{n/a|none (unique language)}}
|-
| 1949
| [[Short Code (computer language)|Short Code]]
| [[John Mauchly]] and William F. Schmitt
| ENIAC Short Code
|- class="sortbottom"
! Year
! Name
! Chief developer, company
! Predecessor(s)
|}

==1950s==
{| class="wikitable sortable"
|-
! Year
! Name
! Chief developer, company
! Predecessor(s)
|-
| 1950
| [[Short Code (computer language)|Short Code]]
| William F. Schmidt, Albert B. Tonik,&lt;ref&gt;[http://purl.umn.edu/104288 UNIVAC conference], [[Charles Babbage Institute]], University of Minnesota.  171-page transcript of oral history with computer pioneers, including Albert B. Tonik, involved with the [[Univac]] computer, held on 17–18 May 1990.&lt;/ref&gt; J. R. Logan
| Brief Code
|-
| 1950
| [[Birkbeck Assembler]]
| [[Kathleen Booth]]
| ARC
|-
| 1951
| [[Superplan]]
| [[Heinz Rutishauser]]
| Plankalkül
|-
| 1951
| [[ALGAE]]
| Edward A. Voorhees and Karl Balke
| {{n/a|none (unique language)}}
|-
| 1951
| Intermediate Programming Language
| [[Arthur Burks]]
| Short Code
|-
| 1951
| [[Regional Assembly Language]]
| [[Maurice Wilkes]]
| EDSAC
|-
| 1951
| [[Boehm unnamed coding system]]
| [[Corrado Böhm]]
| CPC Coding scheme
|-
| 1951
| [[Klammerausdrücke]]
| [[Konrad Zuse]]
| Plankalkül
|-
| 1951
| [[OMNIBAC Symbolic Assembler]]
| [[Charles Katz]]
| Short Code
|-
| 1951
| [[Stanislaus (programming language)|Stanislaus]] (Notation)
| [[Friedrich L. Bauer|Fritz Bauer]]
| {{n/a|none (unique language)}}
|-
| 1951
| [[Whirlwind assembler]]
| Charles Adams and Jack Gilmore at [[Massachusetts Institute of Technology|MIT]] [[Whirlwind (computer)|Project Whirlwind]]
| EDSAC
|-
| 1951
| [[Rochester assembler]]
| [[Nathaniel Rochester (computer scientist)|Nat Rochester]]
| EDSAC
|-
| 1951
| [[Sort Merge Generator]]
| [[Betty Holberton]]
| {{n/a|none (unique language)}}
|-
| 1952
| [[A-0 System|A-0]]
| [[Grace Hopper]]
| Short Code
|-
| 1952
| [[Autocode|Glennie Autocode]]
| [[Alick Glennie]] after [[Alan Turing]]
| CPC Coding scheme
|-
| 1952
| [[Editing Generator]]
| Milly Koss
| SORT/MERGE
|-
| 1952
| [[COMPOOL]]
| [[RAND/SDC]]
| {{n/a|none (unique language)}}
|-
| 1953
| [[Speedcoding]]
| [[John Backus|John W. Backus]]
| {{n/a|none (unique language)}}
|-
| 1953
| [[READ/PRINT]]
| Don Harroff, James Fishman, George Ryckman
| {{n/a|none (unique language)}}
|-
| 1954
| [[Laning and Zierler system]]
| Laning, Zierler, Adams at [[Massachusetts Institute of Technology|MIT]] [[Whirlwind (computer)|Project Whirlwind]]
| {{n/a|none (unique language)}}
|-
| 1954
| [[Autocode|Mark I Autocode]]
| [[Tony Brooker]]
| Glennie Autocode
|-
| 1954–55
| [[Fortran]] (concept)
| Team led by [[John Backus|John W. Backus]] at [[IBM]]
| Speedcoding
|-
| 1954
| [[ARITH-MATIC]]
| Team led by [[Grace Hopper]] at UNIVAC
| A-0
|-
| 1954
| [[MATH-MATIC]]
| Team led by Charles Katz
| A-0
|-
| 1954
| [[MATRIX MATH]]
| H G Kahrimanian
| {{n/a|none (unique language)}}
|-
| 1954
| [[Information Processing Language|IPL I]] (concept)
| [[Allen Newell]], [[Cliff Shaw]], [[Herbert A. Simon]]
| {{n/a|none (unique language)}}
|-
| 1955
| [[FLOW-MATIC]]
| Team led by [[Grace Hopper]] at UNIVAC
| A-0
|-
| 1955
| BACAIC
| M. Grems and R. Porter
|
|-
| 1955
| [[PACT I]]
| [[SHARE (computing)|SHARE]]
| FORTRAN, A-2
|-
| 1955
| [[Freiburger Code]]&lt;ref&gt;{{cite web|url=http://pl.attitu.de/zuse/technik/freiburger.html|title=Der Freiburger Code auf der Zuse|language=German|accessdate=26 October 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.horst-zuse.homepage.t-online.de/seite51.html|title=Z22|author=H. Zuse|accessdate=26 October 2014}}&lt;/ref&gt;
| [[University of Freiburg]]
| {{n/a}}
|-
| 1955–56
| [[Sequentielle Formelübersetzung]]
| [[Friedrich L. Bauer|Fritz Bauer]] and Karl Samelson
| Boehm
|-
| 1955–56
| [[Internal Translator|IT]]
| Team led by [[Alan Perlis]]
| Laning and Zierler
|-
| 1955
| PRINT
| IBM
|
|-
| 1958
| [[Information Processing Language|IPL II]] (implementation)
| [[Allen Newell]], [[Cliff Shaw]], [[Herbert A. Simon]]
| IPL I
|-
| 1956–58
| [[Lisp (programming language)|LISP]] (concept)
| [[John McCarthy (computer scientist)|John McCarthy]]
| IPL
|-
| 1957
| [[COMTRAN]]
| [[Bob Bemer]]
| FLOW-MATIC
|-
| 1957
| [[GEORGE (programming language)|GEORGE]]
| [[Charles Leonard Hamblin]]
| {{n/a|none (unique language)}}
|-
| 1957
| [[FORTRAN I|Fortran I]] (implementation)
| [[John Backus|John W. Backus]] at [[IBM]]
| FORTRAN
|-
| 1957–58
| UNICODE
| Remington Rand UNIVAC
| MATH-MATIC
|-
| 1957
| [[COMIT]] (concept)
|
| {{n/a|none (unique language)}}
|-
| 1958
| [[Fortran#FORTRAN II|Fortran II]]
| Team led by [[John Backus|John W. Backus]] at [[IBM]]
| FORTRAN I
|-
| 1958
| [[ALGOL 58]] (IAL)
| ACM/GAMM
| FORTRAN, IT, Sequentielle Formelübersetzung
|-
| 1958
| [[Information Processing Language|IPL V]]
| [[Allen Newell]], [[Cliff Shaw]], [[Herbert A. Simon]]
| IPL II
|-
| 1959
| [[APT (programming language)|APT]]
| [[Douglas T. Ross]]
|-
| 1959
| [[FACT computer language|FACT]]
| [[Fletcher R. Jones]], [[Roy Nutt]], Robert L. Patrick
| {{n/a|none (unique language)}}
|-
| 1959
| [[COBOL]] (concept)
| The [[CODASYL]] Committee
| FLOW-MATIC, COMTRAN, FACT
|-
| 1959
| [[JOVIAL]]
| [[Jules Schwartz]] at [[System Development Corporation|SDC]]
| [[ALGOL 58]]
|-
| 1959
| [[Lisp (programming language)|LISP]] (implementation)
| [[John McCarthy (computer scientist)|John McCarthy]]
| IPL
|-
| 1959
| [[MAD (programming language)|MAD – Michigan Algorithm Decoder]]
| [[Bruce Arden]], [[Bernard Galler]], and [[Robert M. Graham]]
| [[ALGOL 58]]
|-
| 1959
| [[TRAC (programming language)|TRAC]] (concept)
| [[Calvin Mooers]]
|
|- class="sortbottom"
! Year
! Name
! Chief developer, company
! Predecessor(s)
|}

==1960s==
{| class="wikitable sortable"
|-
! Year
! Name
! Chief developer, company
! Predecessor(s)
|-
| 1960
| [[ALGOL 60]]
|
| ALGOL 58
|-
| 1960
| [[COBOL|COBOL 61]] (implementation)
| The [[CODASYL]] Committee
| FLOW-MATIC, COMTRAN
|-
| 1961
| [[COMIT]] (implementation)
|
| {{n/a|none (unique language)}}
|-
| 1962
| [[Fortran#FORTRAN IV|FORTRAN IV]]
| [[IBM]]
| FORTRAN II
|-
| 1962
| [[APL (programming language)|APL]] (concept)
| [[Kenneth E. Iverson]]
| {{n/a|none (unique language)}}
|-
| 1962
| [[Simula]] (concept)
|
| ALGOL 60
|-
| 1962
| [[SNOBOL]]
| [[Ralph Griswold]], ''et al.''
| FORTRAN II, COMIT
|-
| 1963
| [[Combined Programming Language|CPL]]
| Barron, [[Christopher Strachey]], ''et al.''
| ALGOL 60
|-
| 1963
| [[SNOBOL]]3
| [[Ralph Griswold|Griswold]], ''et al.''
| SNOBOL
|-
| 1963
| [[ALGOL 68]] (concept)
| [[Adriaan van Wijngaarden]], ''et al.''
| ALGOL 60
|-
| 1963
| [[JOSS|JOSS I]]
| Cliff Shaw, [[RAND Corporation|RAND]]
| ALGOL 58
|-
| 1964
| [[MIMIC]]
| H. E. Petersen, ''et al.''
| MIDAS
|-
| 1964
| [[COWSEL]]
| [[Rod Burstall]], [[Robin Popplestone]]
| CPL, LISP
|-
| 1964
| [[PL/I]] (concept)
| [[IBM]]
| ALGOL 60, COBOL, FORTRAN
|-
| 1964
| [[BASIC]]
| [[John George Kemeny]] and [[Thomas Eugene Kurtz]] at [[Dartmouth College]]
| FORTRAN II, JOSS
|-
| 1964
| [[IBM RPG]]
| [[IBM]]
| FARGO
|-
| 1964
| [[MARK IV (software)|Mark-IV]]
| [[Sterling Software|Informatics]]
|
|-
| 1964
| [[Speakeasy (computational environment)|Speakeasy-2]]
| Stanley Cohen at [[Argonne National Laboratory]]
| Speakeasy
|-
| 1964
| [[TRAC (programming language)|TRAC]] (implementation)
| [[Calvin Mooers]]
|
|-
| 1964
| [[P′′]]
| [[Corrado Böhm]]
| {{n/a|none (unique language)}}
|-
| 1964?
| [[IITRAN]]
|
|-
| 1965
| [[RPG II]]
| [[IBM]]
| [[FARGO]], [[IBM RPG|RPG]]
|-
| 1965
| [[MAD (programming language)|MAD/I]] (concept)
| [[University of Michigan]]
| [[MAD (programming language)|MAD]], [[ALGOL 60]], [[PL/I]]
|-
| 1965
| [[TELCOMP]]
| [[BBN Technologies|BBN]]
| JOSS
|-
| 1965
| [[Atlas Autocode]]
| [[Tony Brooker]], Derrick Morris at [[Manchester University]]
| [[Algol 60]], [[Autocode]]
|-
| 1966
| [[JOSS|JOSS II]]
| Chuck Baker, [[RAND]]
| JOSS I
|-
| 1966
| [[ALGOL W]]
| [[Niklaus Wirth]], [[C. A. R. Hoare]] &lt;!-- "A Contribution to the Development of Algol" in [ACM] (1966) [ACM] CACM 9(06) June 1966 --&gt;
| ALGOL 60
|-
| 1966
| [[FORTRAN 66]]
|
| FORTRAN IV
|-
| 1966
| [[ISWIM]] (Concept)
| [[Peter J. Landin]]
| LISP
|-
| 1966
| [[Coral 66|CORAL66]]
|
| ALGOL 60
|-
| 1966
| [[APL (programming language)|APL]] (implementation)&lt;ref&gt;{{cite web|last=Smillie|first=Keith|url=http://amturing.acm.org/award_winners/iverson_9147499.cfm |title=Kenneth E. Iverson - A.M. Turing Award Winner |publisher=ACM}}&lt;/ref&gt;
| [[Kenneth E. Iverson]]
| {{n/a|none (unique language)}}
|-
| 1967
| [[BCPL]]
| [[Martin Richards (computer scientist)|Martin Richards]]
| CPL
|-
| 1967
| [[MUMPS]]
| [[Massachusetts General Hospital]]
| FORTRAN, TELCOMP
|-
| 1967
| [[Simula|Simula 67]] (implementation)
| [[Ole-Johan Dahl]], Bjørn Myhrhaug, [[Kristen Nygaard]] at [[Norwegian Computing Center|Norsk Regnesentral]]
| ALGOL 60
|-
| 1967
| [[Interlisp|InterLisp]]
| D.G. Bobrow and D.L. Murphy
| Lisp
|-
| 1967
| [[EXAPT]]
| Herwart Opitz, Wilhelm Simon, Günter Spur, and Gottfried Stute at [[RWTH Aachen University]] and [[TU Berlin]]
| [[APT (programming language)|APT]]
|-
| 1967
| [[SNOBOL]]4
| [[Ralph Griswold]], ''et al.''
| SNOBOL3
|-
| 1967
| [[XPL]]
| [[William M. McKeeman]], ''et al.'' at [[University of California]] [[Santa Cruz, California]]&lt;br/&gt;[[Jim Horning|J. J. Horning]], ''et al.'' at [[Stanford University]]
| PL/I
|-
| 1967
| [[Space Programming Language]] (SPL)
| [[System Development Corporation]]
| JOVIAL
|-
| 1968
| [[ALGOL 68]] ([[UNESCO]]/[[International Federation for Information Processing|IFIP]] standard)
| [[Adriaan van Wijngaarden]], [[Barry J. Mailloux]], [[John E. L. Peck]] and [[Cornelis H. A. Koster]], ''et al.'' &lt;!-- The final version, MR 101, was adopted by the Working Group on 20 December 1968 in Munich, and was subsequently approved for publication by the General Assembly of I.F.I.P. --&gt;
| ALGOL 60
|-
| 1968
| [[COWSEL|POP-1]]
| [[Rod Burstall]], [[Robin Popplestone]]
| COWSEL
|-
| 1968
| [[DIBOL|DIBOL-8]]
| [[Digital Equipment Corporation|DEC]]
| DIBOL
|-
| 1968
| [[Forth (programming language)|Forth]] (concept)
| [[Charles H. Moore|Moore]]
|
|-
| 1968
| [[Logo (programming language)|LOGO]]
| [[Wally Feurzeig]], [[Seymour Papert]], [[Cynthia Solomon]]
| LISP
|-
| 1968
| [[MAPPER]]
| [[Unisys]]
| CRT RPS
|-
| 1968
| [[REFAL]] (implementation)
| [[Valentin Turchin]]
| {{n/a|none (unique language)}}
|-
| 1968
| [[TTM (programming language)|TTM]] (implementation)
| Steven Caine and E. Kent Gordon, [[California Institute of Technology]]
| GAP, GPM
|-
| 1968
| [[PILOT]]
| [[John Amsden Starkweather]], [[University of California, San Francisco]]
| Computest
|-
| 1969
| [[PL/I]] (implementation)
| [[IBM]]
| ALGOL 60, COBOL, FORTRAN
|-
| 1969
| [[B (programming language)|B]]
| [[Ken Thompson]], with contributions from [[Dennis Ritchie]]
| BCPL
|-
| 1969
| [[Polymorphic Programming Language]] (PPL)
| Thomas A. Standish at [[Harvard University]]
|
|-
| 1969
| [[SETL]]
| [[Jack Schwartz]] at [[Courant Institute of Mathematical Sciences]]
|
|-
| 1969
| [[TUTOR (programming language)|TUTOR]]
| Paul Tenczar &amp; [[University of Illinois at Urbana–Champaign]]
|
|-
| 1969
| [[Edinburgh IMP]]
| [[Edinburgh University]]
| [[Algol 60]], [[Autocode]], [[Atlas Autocode]]
|- class="sortbottom"
! Year
! Name
! Chief developer, company
! Predecessor(s)
|}

==1970s==
{| class="wikitable sortable"
|-
! Year
! Name
! Chief developer, company
! Predecessor(s)
|-
| 1970?
| [[Forth (programming language)|Forth]] (implementation)
| [[Charles H. Moore]]
|
|-
| 1970
| [[POP-2]]
| [[Robin Popplestone]]
| POP-1
|-
| 1970
| [[Pascal (programming language)|Pascal]]
| [[Niklaus Wirth]], Kathleen Jensen
| ALGOL 60, ALGOL W
|-
| 1970
| [[BLISS]]
| Wulf, Russell, Habermann at [[Carnegie Mellon University]]
| ALGOL
|-
| 1971
| [[KRL (programming language)|KRL]]
| [[Daniel G. Bobrow]] at [[Xerox PARC]], [[Terry Winograd]] at [[Stanford University]]
| KM, FRL (MIT)
|-
| 1971
| [[Sue (programming language)|Sue]]
| [[Ric Holt]] ''et al.'' at [[University of Toronto]]
| Pascal, XPL
|-
| 1971
| [[Compiler Description Language]] (CDL)
| [[Cornelis H.A. Koster]] at [[University of Nijmegen]]
|
|-
| 1972
| [[Smalltalk-72]]
| [[Alan Kay]], [[Adele Goldberg (computer scientist)|Adele Goldberg]], [[Dan Ingalls]], [[PARC (company)|Xerox PARC]]
| Simula 67
|-
| 1972
| [[PL/M]]
| [[Gary Kildall]] at [[Digital Research]]
| PL/I, ALGOL, XPL
|-
| 1972
| [[C (programming language)|C]]
| [[Dennis Ritchie]]
| B, BCPL, ALGOL 68
|-
| 1972
| [[INTERCAL]]
| [[Don Woods (programmer)|Don Woods]] and James M. Lyon
| {{n/a|none (unique language)}}
|-
| 1972
| [[Prolog]]
| [[Alain Colmerauer]]
| 2-level W-Grammar
|-
| 1972
| Structured Query language ([[SQL]])
| [[IBM]]
| ALPHA, Quel (Ingres)
|-
| 1973
| [[COMAL]]
| Børge Christensen, Benedict Løfstedt
| Pascal, BASIC
|-
| 1973
| [[ML (programming language)|ML]]
| [[Robin Milner]]
|
|-
| 1973
| [[LIS (programming language)|LIS]]
| [[Jean Ichbiah]] ''et al.'' at [[Groupe Bull|CII Honeywell Bull]]
| Pascal, Sue
|-
| 1973
| [[Speakeasy (computational environment)|Speakeasy-3]]
| Stanley Cohen, Steven Pieper at [[Argonne National Laboratory]]
| Speakeasy-2
|-
| 1974
| [[CLU (programming language)|CLU]]
| [[Barbara Liskov]]
| ALGOL 60, Lisp, Simula
|-
| 1974
| [[GRASS (programming language)|GRASS]]
| [[Thomas A. DeFanti]]
| BASIC
|-
| 1974
| [[MAI Basic Four|BASIC FOUR]]
| MAI BASIC Four Inc.
| Business BASIC
|-
| 1974
| [[PROSE modeling language]]
| [[CDC 6600]] Cybernet Services
| SLANG, FORTRAN
|-
| 1975
| [[ABC (programming language)|ABC]]
| Leo Geurts and [[Lambert Meertens]]
| [[SETL]]
|-
| 1975
| [[Irvine Dataflow]] (concept)
| Arvind?, University of California, Irvine
| 
|-
| 1975
| [[PROSE modeling language]] Time-Sharing Version
| [[CDC 6400]] Cybernet KRONOS Services
| SLANG, FORTRAN
|-
| 1975
| [[Scheme (programming language)|Scheme]]
| [[Gerald Jay Sussman]], [[Guy L. Steele, Jr.]]
| LISP
|-
| 1975
| [[Altair BASIC]]
| [[Bill Gates]], [[Paul Allen]]
| BASIC
|-
| 1975
| [[CS-4 (programming language)|CS-4]]
| James S. Miller, Benjamin M. Brosgol ''et al.'' at [[Intermetrics]]
| ALGOL 68, BLISS, ECL, HAL
|-
| 1975
| [[Modula]]
| [[Niklaus Wirth]]
| Pascal
|-
| 1976
| [[Plus (programming language)|Plus]]
| Allan Ballard, Paul Whaley at the [[University of British Columbia]]
| Pascal, Sue
|-
| 1976
| [[Smalltalk-76]]
| [[PARC (company)|Xerox PARC]]
| Smalltalk-72
|-
| 1976
| [[Mesa (programming language)|Mesa]]
| [[PARC (company)|Xerox PARC]]
| ALGOL
|-
| 1976
| [[SAM76]]
| Claude A.R. Kagan
| LISP, TRAC
|-
| 1976
| [[Ratfor]]
| [[Brian Kernighan]]
| C, FORTRAN
|-
| 1976
| [[S (programming language)|S]]
| [[John Chambers (programmer)|John Chambers]] at [[Bell Labs]]
| APL, PPL, Scheme
|-
| 1976
| [[SAS language|SAS]]
| [[SAS Institute]]
| 
|-
| 1976
| [[Integer BASIC]]
| [[Steve Wozniak]]
| BASIC
|-
| 1977
| [[FP (programming language)|FP]]
| [[John Backus]]
| {{n/a|none (unique language)}}
|-
| 1977
| [[Bourne shell|Bourne Shell]] (''sh'')
| [[Stephen R. Bourne]]
| {{n/a|none (unique language)}}
|-
| 1977
| [[Commodore BASIC]]
| [[Jack Tramiel]]
| BASIC
|-
| 1977
| [[IDL (programming language)|IDL]]
| David Stern of Research Systems Inc
| Fortran
|-
| 1977
| [[MUMPS|Standard MUMPS]]
|
| MUMPS
|-
| 1977
| [[Icon (programming language)|Icon]] (concept)
| [[Ralph Griswold]]
| SNOBOL
|-
| 1977
| Red
| Benjamin M. Brosgol ''et al.'' at [[Intermetrics]] for [[United States Department of Defense|US Dept of Defense]]
| ALGOL 68, CS-4
|-
| 1977
| [[Blue (programming language)|Blue]]
| [[John B. Goodenough]] ''et al.'' at [[SofTech]] for [[United States Department of Defense|US Dept of Defense]]
| ALGOL 68
|-
| 1977
| [[Yellow (programming language)|Yellow]]
| Jay Spitzen ''et al.'' at [[SRI International]] for [[United States Department of Defense|US Dept of Defense]]
| ALGOL 68
|-
|1977
|[[Euclid (programming language)|Euclid]]
|[[Butler Lampson]] at [[PARC (company)|Xerox Parc]], [[Ric Holt]] and [[James Cordy]] at [[University of Toronto]]
|
|-
| 1977
| [[Applesoft BASIC]]
| [[Marc McDonald]] and [[Ric Weiland]]
| BASIC
|-
| 1978
| [[Freddy II|RAPT]]
| [[Pat Ambler]] and [[Robin Popplestone]]
| [[APT (programming language)|APT]]
|-
| 1978
| [[C shell]]
| [[Bill Joy]]
| [[C (programming language)|C]]
|-
| 1978
| [[RPG III]]
| [[IBM]]
| [[FARGO]], [[IBM RPG|RPG]], [[RPG II]]
|-
| 1978
| [[HAL/S]]
| designed by Intermetrics for NASA
| [[XPL]]
|-
| 1978
| [[Applesoft BASIC|Applesoft II BASIC]]
| [[Marc McDonald]] and [[Ric Weiland]]
| Applesoft BASIC
|-
| 1975
| [[Irvine Dataflow]] (implementation)
| Arvind and Gostelow, University of California, Irvine
| 
|-
| 1978?
| [[MATLAB]]
| [[Cleve Moler]] at the [[University of New Mexico]]
| [[Fortran]]
|-
| 1978?
| [[SMALL]]
| Nevil Brownlee at the [[University of Auckland]]
| Algol60
|-
| 1978
| [[VisiCalc]]
| [[Dan Bricklin]], [[Bob Frankston]] marketed by [[VisiCorp]]
| {{n/a|none (unique language)}}
|-
| 1979
| [[Modula-2]]
| [[Niklaus Wirth]]
| Modula, Mesa
|-
| 1979
| [[REXX]]
| [[Mike Cowlishaw]] at [[IBM]]
| PL/I, BASIC, EXEC 2
|-
| 1979
| [[AWK]]
| [[Alfred Aho]], [[Peter J. Weinberger]], [[Brian Kernighan]]
| C, SNOBOL
|-
| 1979
| [[Icon (programming language)|Icon]] (implementation)
| [[Ralph Griswold]]
| SNOBOL
|-
| 1979
| [[DBase|Vulcan dBase-II]]
| [[Wayne Ratliff]]
| {{n/a|none (unique language)}}
|- class="sortbottom"
! Year
! Name
! Chief developer, company
! Predecessor(s)
|}

==1980s==
{| class="wikitable sortable"
|-
! Year
! Name
! Chief developer, company
! Predecessor(s)
|-
| 1980
| [[Ada (programming language)|Ada 80]] (MIL-STD-1815)
| [[Jean Ichbiah]] at [[Groupe Bull|CII Honeywell Bull]]
| Green
|-
| 1980
| [[C++|C with classes]]
| [[Bjarne Stroustrup]]&lt;ref&gt;http://isocpp.org/tour&lt;/ref&gt;
| C, Simula 67
|-
| 1980
| [[Applesoft BASIC|Applesoft III]]
| [[Apple Computer]]
| Applesoft II BASIC
|-
| 1980
| [[Applesoft BASIC|Apple III Microsoft BASIC]]
| Microsoft
| Microsoft BASIC
|-
| 1980–81
| [[CBASIC]]
| [[Gordon Eubanks]]
| BASIC, Compiler Systems, Digital Research
|-
| 1981
| [[BBC BASIC]]
| [[Acorn Computers]], [[Sophie Wilson]]
| BASIC
|-
| 1981
| [[IBM BASICA]]
| [[Microsoft]]
| BASIC
|-
| 1982?
| [[Speakeasy (computational environment)|Speakeasy-IV]]
| Stanley Cohen, ''et al.'' at [[Speakeasy Computing Corporation]]
| Speakeasy-3
|-
| 1982?
| [[Draco (programming language)|Draco]]
| Chris Gray
| [[Pascal (programming language)|Pascal]], [[C (programming language)|C]], [[ALGOL 68]]
|-
| 1982
| [[PostScript]]
| [[John Warnock|Warnock]]
| [[InterPress]]
|-
|1982
|[[Turing (programming language)|Turing]]
|[[Ric Holt]] and [[James Cordy]], at [[University of Toronto]]
|[[Euclid (programming language)|Euclid]]
|-
| 1983
| [[GW-BASIC]]
| [[Microsoft]]
| [[IBM BASICA]]
|-
| 1983
| [[Turbo Pascal]]
| [[Anders Hejlsberg|Hejlsberg]] at [[Borland]]
| [[Pascal (programming language)|Pascal]]
|-
| 1983
| [[Ada (programming language)|Ada 83]] (ANSI/MIL-STD-1815A)
| [[Jean Ichbiah]] at [[Alsys]]
| Ada 80, Green
|-
| 1983
| [[Objective-C]]
| [[Brad Cox]]
| Smalltalk, C
|-
| 1983
| [[C++]]
| [[Bjarne Stroustrup]]
| C with Classes
|-
| 1983
| [[True BASIC]]
| [[John George Kemeny]], [[Thomas Eugene Kurtz]] at [[Dartmouth College]]
| BASIC
|-
| 1983
| [[Occam (programming language)|occam]]
| [[David May (computer scientist)|David May]]
| EPL
|-
| 1983?
| [[ABAP]]
| [[SAP AG]]
| [[COBOL]]
|-
| 1983
| [[Korn shell|Korn Shell]] (''ksh'')
| [[David Korn (computer scientist)|David Korn]]
| sh
|-
|1983
|[[Clascal]]
| [[Apple Inc.|Apple Computer Inc.]]
| [[Pascal (programming language)|Pascal]]
|-
|| 1984
| [[Clipper (programming language)|CLIPPER]]
| [[Nantucket, Massachusetts|Nantucket]]
| dBase
|-
| 1984
| [[Common Lisp]]
| [[Guy L. Steele, Jr.]] and many others
| LISP
|-
| 1984?
| [[MAD (programming language)|GOM]] – Good Old Mad
| Don Boettner, [[University of Michigan]]
| [[MAD (programming language)|MAD]]
|-
| 1984
| [[RPL (programming language)|RPL]]
| [[Hewlett-Packard]]
| [[Forth (programming language)|Forth]], [[Lisp (programming language)|Lisp]]
|-
| 1984
| [[Standard ML]]
|
| ML
|-
| 1984
| [[Core War|Redcode]]
| [[Alexander Dewdney]] and [[D. G. Jones|D.G. Jones]]
|
|-
| 1984
| [[Open Programming Language|OPL]]
| [[PSION]]
| [[BASIC]]
|-
| 1985
| [[Paradox (database)|PARADOX]]
| [[Borland]]
| dBase
|-
| 1985
| [[QuickBASIC]]
| [[Microsoft]]
| [[BASIC]]
|-
|1986
| [[Clarion (programming language)|Clarion]]
| Bruce Barrington
|
|-
| 1986
| [[CorVision]]
| Cortex
| INFORM
|-
| 1986
| [[Eiffel (programming language)|Eiffel]]
| [[Bertrand Meyer]]
| Simula 67, Ada
|-
| 1986
| [[GFA BASIC]]
| [[Frank Ostrowski]]
| [[BASIC]]
|-
| 1986
| [[IBM Informix-4GL|Informix-4GL]]
| [[IBM Informix|Informix]]
|
|-
| 1986
| [[LabVIEW]]
| [[National Instruments]]
|
|-
| 1986
| [[Miranda (programming language)|Miranda]]
| [[David Turner (computer scientist)|David Turner]] at [[University of Kent]]
|
|-
| 1986
| [[Object Pascal]]
| [[Apple Inc.|Apple Computer Inc.]]
| Pascal
|-
| 1986
| [[PROMAL]]
|
| C
|-
| 1986
| [[Erlang (programming language)|Erlang]]
| Joe Armstrong and others in [[Ericsson]]
| Prolog
|-
| 1987
| [[Ada (programming language)|Ada ISO 8652:1987]]
| [[ANSI/MIL-STD-1815A unchanged]]
| Ada 83
|-
| 1987
| [[Self (programming language)|Self]] (concept)
| [[Sun Microsystems]] Inc.
| Smalltalk
|-
| 1987
| [[Occam (programming language)|occam 2]]
| [[David May (computer scientist)|David May]] and [[INMOS]]
| [[Occam (programming language)|occam]]
|-
| 1987
| [[HyperTalk]]
| [[Apple Inc.|Apple Computer Inc.]]
| {{n/a|none (unique language)}}
|-
| 1987
| [[Perl]]
| [[Larry Wall]]
| C, sed, awk, sh
|-
| 1987
| [[Oberon (programming language)|Oberon]]
| [[Niklaus Wirth]]
| Modula-2
|-
| 1987
| [[Mathematica]] ([[Wolfram Language]]&lt;!-- was named that years later, is the language of the larger system Mathematica--&gt;)
| [[Wolfram Research]]
| {{n/a|none (unique language)}}
|-
| 1987
| [[Turbo Basic]]
| Robert 'Bob' Zale
| BASIC/Z
|-
| 1987
| [[Clean (programming language)|Clean]]
| Software Technology Research Group of [[Radboud University Nijmegen]]
| {{n/a|none (unique language)}}
|-
| 1988
| [[RPG/400]]
| [[IBM]]
| [[FARGO]], [[IBM RPG|RPG]], [[RPG II]], [[RPG III]]
|-
| 1988
| [[GNU Octave|Octave]]
|
| [[MATLAB]]
|-
| 1988
| [[Tcl]]
| [[John Ousterhout]]
| Awk, Lisp
|-
| 1988
| [[STOS BASIC]]
| [[François Lionet]] and [[Constantin Sotiropoulos]]
| [[BASIC]]
|-
| 1988
| [[Actor_(programming_language)|Actor]]
| [[Charles Duff, the Whitewater Group]]
| [[Forth, Smalltalk]]
|-
| 1988
| [[Object REXX]]
| Simon C. Nash
| REXX, Smalltalk
|-
| 1988
| [[SPARK (programming language)|SPARK]]
| Bernard A. Carré
| Ada
|-
| 1988
| [[A+ (programming language)|A+]]
| [[Arthur Whitney (computer scientist)|Arthur Whitney]]
| APL
|-
| 1988
| [[Hamilton C shell]]
| Nicole Hamilton
| [[C shell]]
|-
| 1989
| [[Turbo Pascal#Object-oriented programming|Turbo Pascal OOP]]
| [[Anders Hejlsberg]] at [[Borland]]
| Turbo Pascal, Object Pascal
|-
| 1989
| [[Modula-3]]
| Cardeli, et al. [[Digital Equipment Corporation|DEC]] and [[Olivetti]]
| Modula-2
|-
| 1989
| [[PowerBASIC]]
| Robert 'Bob' Zale
| Turbo Basic
|-
| 1989
| [[VisSim]]
| Peter Darnell, [[Visual Solutions]]
|
|-
| 1989
| [[LPC (programming language)|LPC]]
| [[Lars Pensjö]]
|
|-
| 1989
| [[Bash (Unix shell)|Bash]]
| [[Brian Fox (computer programmer)|Brian Fox]]
| [[Bourne shell]], [[C shell]], [[Korn shell]]
|-
|1989
|[[Magik (programming language)]]
|Arthur Chance, of [[Smallworld]] Systems Ltd
|[[Smalltalk]]
|- class="sortbottom"
| 1989
| [[Python (programming language)]]
| [[Guido Van Rossum]]
| [[SETL]]
|-
! Year
! Name
! Chief developer, company
! Predecessor(s)
|}

==1990s==
{| class="wikitable sortable"
|-
! Year
! Name
! Chief developer, company
! Predecessor(s)
|-
| 1990
| [[AMOS (programming language)|AMOS BASIC]]
| [[François Lionet]] and [[Constantin Sotiropoulos]]
| [[STOS BASIC]]
|-
| 1990
| [[AMPL]]
| [[Robert Fourer]], David Gay and [[Brian Kernighan]] at [[Bell Laboratories]]
| 
|-
| 1990
| [[Object Oberon]]
| H Mössenböck, J Templ, R Griesemer
| Oberon
|-
| 1990
| [[J (programming language)|J]]
| [[Kenneth E. Iverson]], [[Roger Hui]] at Iverson Software
| APL, FP
|-
| 1990
| [[Haskell (programming language)|Haskell]]
|
| Miranda
|-
| 1990
| [[EuLisp]]
|
| [[Common Lisp]], Scheme
|-
| 1990
| [[Z shell|Z Shell]] (''zsh'')
| Paul Falstad at [[Princeton University]]
| ksh
|-
| 1991
| [[GNU E]]
| David J. DeWitt, Michael J. Carey
| [[C++]]
|-
| 1991
| [[Oberon-2 (programming language)|Oberon-2]]
| Hanspeter Mössenböck, [[Niklaus Wirth|Wirth]]
| Object Oberon
|-
| 1991
| [[Python (programming language)|Python]]
| [[Guido van Rossum]]
| [[ABC (programming language)|ABC]], [[ALGOL 68]], [[Icon (programming language)|Icon]], [[Modula-3]]
|-
| 1991
| [[Oz (programming language)|Oz]]
| Gert Smolka and his students
| Prolog
|-
| 1991
| [[Pure (programming language)|Q]]
| Albert Gräf
|
|-
| 1991
| [[Visual Basic]]
| [[Alan Cooper]], sold to [[Microsoft]]
| QuickBASIC
|-
| 1992
| [[Turbo Pascal|Borland Pascal]]
|
| Turbo Pascal OOP
|-
| 1992
| [[Dylan (programming language)|Dylan]]
| many people at [[Apple Inc.|Apple Computer Inc.]]
| [[Common Lisp]], Scheme
|-
| 1992
| [[S-Lang]]
| John E. Davis
| [[PostScript]]
|-
| 1993?
| [[Self (programming language)|Self]] (implementation)
| [[Sun Microsystems]] Inc.
| Smalltalk
|-
| 1993
| [[Amiga E]]
| [[Wouter van Oortmerssen]]
| DEX, C, Modula-2
|-
| 1993
| [[Brainfuck]]
| Urban Müller
| P′′
|-
| 1993
| [[Transcript (programming language)|LiveCode Transcript]]
|
| HyperTalk
|-
| 1993
| [[AppleScript]]
| [[Apple Inc.|Apple Computer Inc.]]
| HyperTalk
|-
| 1993
| [[K (programming language)|K]]
| [[Arthur Whitney (computer scientist)|Arthur Whitney]]
| APL, Lisp
|-
| 1993
| [[Lua (programming language)|Lua]]
| [[Roberto Ierusalimschy]] ''et al.'' at [[Tecgraf, PUC-Rio]]
| Scheme, SNOBOL, Modula, CLU, C++
|-
| 1993
| [[R (programming language)|R]]
| [[Robert Gentleman (statistician)|Robert Gentleman]] and [[Ross Ihaka]]
| S
|-
| 1993
| [[ZPL (programming language)|ZPL]]
| Chamberlain ''et al.'' at [[University of Washington]]
| C
|-
| 1993
| [[NewtonScript]]
| Walter Smith
| Self, Dylan
|-
| 1993
| [[Euphoria (programming language)| Euphoria]]
| Robert Craig
| SNOBOL, AWK, ABC, Icon, Python
|-
| 1994
| [[Claire (programming language)|Claire]]
| Yves Caseau
| Smalltalk, SETL, OPS5, Lisp, ML, C, LORE, LAURE
|-
| 1994
| [[Common Lisp|ANSI Common Lisp]]
|
| [[Common Lisp]]
|-
| 1994
| [[RAPID]]
| [[ABB Group]]
| ARLA
|-
| 1994
| [[Pike (programming language)|Pike]]
| Fredrik Hübinette et al. at [[Linköping University]]
| [[LPC (programming language)|LPC]], C, µLPC
|-
| 1994
| [[Forth (programming language)|ANS Forth]]
| [[Elizabeth Rather]], et al.
| Forth
|-
| 1995
| [[Ada (programming language)|Ada 95]]
| S. Tucker Taft, et al. at Intermetrics, Inc.
| Ada 83
|-
| 1995
| [[CodeGear Delphi|Borland Delphi]]
| [[Anders Hejlsberg]] at [[Borland]]
| Borland Pascal
|-
| 1995
| [[ColdFusion|ColdFusion (CFML)]]
| [[Allaire Corporation|Allaire]]
|
|-
| 1995
| [[Java (programming language)|Java]]
| [[James Gosling]] at [[Sun Microsystems]]
| C, Simula 67, C++, Smalltalk, Ada 83, Objective-C, Mesa
|-
| 1995
| [[JavaScript|LiveScript]]
| [[Brendan Eich]] at [[Netscape]]
| Self, C, Scheme
|-
| 1995
| [[Mercury (programming language)|Mercury]]
| [[Zoltan Somogyi]] at [[University of Melbourne]]
| Prolog, Hope, Haskell
|-
| 1995
| [[PHP]]
| [[Rasmus Lerdorf]]
| Perl
|-
| 1995
| [[Ruby (programming language)|Ruby]]
| [[Yukihiro Matsumoto]]
| Smalltalk, Perl
|-
| 1995
| [[JavaScript]]
| [[Brendan Eich]] at [[Netscape]]
| LiveScript
|-
|1995
|[[Racket (programming language)|Racket]]
|[[Matthew Flatt]] at [[Rice University]]
|[[Scheme (programming language)|Scheme]], [[Lisp (programming language)|Lisp]]
|-
| 1996
| [[Curl (programming language)|Curl]]
| David Kranz, Steve Ward, Chris Terman at [[Massachusetts Institute of Technology|MIT]]
| Lisp, C++, Tcl/Tk, TeX, HTML
|-
| 1996
| [[Lasso (programming language)|Lasso]]
| [[Blue World Communications Inc.]]
|
|-
| 1996
| [[Perl Data Language]] (PDL)
| [[Karl Glazebrook]], [[Jarle Brinchmann]], [[Tuomas Lukka]], and [[Christian Soeller]]
| APL, Perl
|-
| 1996
| [[VBScript]]
| Microsoft
| Visual Basic
|-
| 1996
| [[OCaml]]
| [[INRIA]]
| Caml Light, Standard ML
|-
| 1996
| [[NetRexx]]
| [[Mike Cowlishaw]]
| REXX
|-
| 1997
| [[Component Pascal]]
| Oberon microsystems, Inc
| Oberon-2
|-
| 1997
| [[E (programming language)|E]]
| [[Mark S. Miller]]
| Joule, Original-E
|-
| 1997
| [[Pico (programming language)|Pico]]
| Free University of [[Brussels]]
| Scheme
|-
| 1997
| [[Squeak]] Smalltalk
| [[Alan Kay]], ''et al.'' at [[Apple Inc.|Apple Computer Inc.]]
| Smalltalk-80, Self
|-
| 1997
| [[ECMAScript]]
| [[Ecma International|ECMA]] TC39-TG1
| JavaScript
|-
| 1997
| [[F-Script (programming language)|F-Script]]
| Philippe Mougin
| Smalltalk, APL, Objective-C
|-
| 1997
| [[ISLISP]]
| ISO Standard ISLISP
| [[Common Lisp]]
|-
| 1997
| [[Tea (programming language)|Tea]]
| Jorge Nunes
| [[Java (programming language)|Java]], [[Scheme (programming language)|Scheme]], [[Tcl]]
|-
| 1997
| [[REBOL]]
| [[Carl Sassenrath]], Rebol Technologies
| [[Self (programming language)|Self]], [[Forth (programming language)|Forth]], [[Lisp (programming language)|Lisp]], [[Logo (programming language)|Logo]]
|-
| 1998
| [[Logtalk]]
| Paulo Moura (then at [[University of Coimbra]])
| Prolog
|-
| 1998
| [[ActionScript]]
| [[Gary Grossman]]
| ECMAScript
|-
| 1998
| [[C++|Standard C++]]
| ANSI/ISO Standard C++
| C++, Standard C, C
|-
| 1998
| [[M2001]]
| Ronald E. Prather, [[Trinity University (Texas)]]
| {{n/a|none (unique language)}}
|-
| 1998
| [[PIKT|Pikt]]
| Robert Osterlund (then at [[University of Chicago]])
| AWK, Perl, Unix shell
|-
| 1998
| [[PureBasic]]
| Frederic Laboureur, Fantaisie Software
|
|-
| 1998
| [[UnrealScript]]
| [[Tim Sweeney (game developer)|Tim Sweeney]] at [[Epic Games]]
| C++, Java
|-
| 1998
| [[XSL Transformations|XSLT]] (+ [[XPath]])
| [[World Wide Web Consortium|W3C]], [[James Clark (XML expert)|James Clark]]
| [[Document Style Semantics and Specification Language|DSSSL]]
|-
| 1999
| [[Game Maker Language]] (GML)
| [[Mark Overmars]]
| [[GameMaker: Studio|Game Maker]]
|-
| 1999
| [[Harbour (software)|Harbour]]
| Antonio Linares
| [[dBase]]
|- class="sortbottom"
! Year
! Name
! Chief developer, company
! Predecessor(s)
|}

==2000s==
{| class="wikitable sortable"
|-
! Year
! Name
! Chief developer, company
! Predecessor(s)
|-
| 2000
| [[Join Java]]
| G Stewart von Itzstein
| Java
|-
| 2000
|[[DarkBASIC|DarkBasic]]
|[[The Game Creators]]
| 
|-
| 2000
| [[C Sharp (programming language)|C#]]
| [[Anders Hejlsberg]], [[Microsoft]] ([[Ecma International|ECMA]])
| C, C++, Java, Delphi, Modula-2
|-
| 2001
| [[Joy (programming language)|Joy]]
| Manfred von Thun
| FP, Forth
|-
| 2001
| [[AspectJ]]
| [[Gregor Kiczales]], [[PARC (company)|Xerox PARC]]
| Java, Common Lisp
|-
| 2001
| [[D (programming language)|D]]
| [[Walter Bright]], Digital Mars
| C, C++, C#, Java
|-
| 2001
| [[Processing (programming language)|Processing]]
| [[Casey Reas]] and [[Benjamin Fry]]
| [[Java (programming language)|Java]], [[C (programming language)|C]], [[C++]]&lt;ref&gt;https://www.arduino.cc/en/Reference/HomePage&lt;/ref&gt;
|-
| 2001
| [[Visual Basic .NET]]
| [[Microsoft]]
| Visual Basic
|-
| 2001
| [[RPG IV (RPGLE, ILE RPG, RPG Free)]]
| [[IBM]]
| [[FARGO]], [[IBM RPG|RPG]], [[RPG II]], [[RPG III]], [[RPG/400]]
|-
| 2001
| [[GDScript]] (GDS)
| [[OKAM Studio]]
| [[Godot (game engine)|Godot]]
|-
| 2002
| [[Io (programming language)|Io]]
| Steve Dekorte
| Self, NewtonScript, Lua
|-
| 2002
| [[Gosu (programming language)|Gosu]]
| Guidewire Software
| GScript
|-
|2002
|PlayBasic
|Kevin Picone
|
|-
| 2002
| [[Scratch (programming language)|Scratch]]
| [[Mitchel Resnick]], John Maloney, Natalie Rusk, Evelyn Eastmond, Tammy Stern, Amon Millner, Jay Silver, and Brian Silverman
| [[Logo (programming language)|Logo]], [[Smalltalk]], [[Squeak]], [[Squeak#E-Toys|E-Toys]], [[HyperCard]], AgentSheets, StarLogo, Tweak, [[BYOB (programming language)|BYOB]]
|-
| 2003
| [[Nemerle]]
| University of [[Wrocław]]
| C#, [[Standard ML|ML]], MetaHaskell
|-
| 2003
| [[Factor (programming language)|Factor]]
| [[Slava Pestov]]
| Joy, Forth, Lisp
|-
| 2003
| [[Falcon (programming language)|Falcon]]
| Giancarlo Niccolai
| C++, Perl, Lua, Smalltalk, PHP, Lisp, Python, Ruby
|-
| 2003
| [[Scala (programming language)|Scala]]
| [[Martin Odersky]]
| Smalltalk, Java, Haskell, Standard ML, OCaml
|-
| 2003
| [[C++03]]
| C++ ISO/IEC 14882:2003
| [[C++]], Standard C, [[C (programming language)|C]]
|-
| 2003
| [[Squirrel (programming language)|Squirrel]]
| Alberto Demichelis
| Lua
|-
| 2003
| [[Boo (programming language)|Boo]]
| Rodrigo B. de Oliveira
| Python, C#
|-
| 2004
| [[Subtext (programming language)|Subtext]]
| Jonathan Edwards
| {{n/a|none (unique language)}}
|-
| 2004
| [[Alma-0]]
| Krzysztof Apt, [[Centrum Wiskunde &amp; Informatica]]
| {{n/a|none (unique language)}}
|-
| 2004
| [[FreeBASIC]]
| Andre Victor
| QBasic
|-
| 2004
| [[Groovy (programming language)|Groovy]]
| [[James Strachan (programmer)|James Strachan]]
| [[Java (programming language)|Java]]
|-
| 2004
| [[Little b (programming language)|Little b]]
| Aneil Mallavarapu, [[Harvard Medical School]], Department of Systems Biology
| Lisp
|-
| 2005
| [[Fantom (programming language)|Fantom]]
| Brian Frank, Andy Frank
| C#, Scala, Ruby, Erlang
|-
| 2005
| [[F Sharp (programming language)|F#]]
| [[Don Syme]], [[Microsoft Research]]
| [[OCaml]], [[C Sharp (programming language)|C#]], [[Haskell (programming language)|Haskell]]
|-
| 2005
| [[Haxe]]
| Nicolas Cannasse
| [[ActionScript]], [[OCaml]], [[Java (programming language)|Java]]
|-
| 2005
| [[Oxygene (programming language)|Oxygene]]
| [[RemObjects Software]]
| [[Object Pascal]], [[C_Sharp_(programming_language)|C#]]
|-
| 2005
| [[Seed7]]
| Thomas Mertes
| {{n/a|none (unique language)}}
|-
| 2006
| [[Links (programming language)|Links]]
| [[Philip Wadler]], [[University of Edinburgh]]
| [[Haskell (programming language)|Haskell]]
|-
| 2006
| [[Cobra (programming language)|Cobra]]
| ChuckEsterbrook
| Python, C#, Eiffel, Objective-C
|-
| 2006
| [[Windows PowerShell]]
| [[Microsoft]]
| C#, ksh, Perl, [[AS/400 Control Language|CL]], [[DIGITAL Command Language|DCL]], SQL
|-
| 2006
| [[OptimJ]]
| [[Ateji]]
| [[Java (programming language)|Java]]
|-
| 2006
| [[Fortress (programming language)|Fortress]]
| [[Guy L. Steele, Jr.|Guy Steele]]
| [[Scala (programming language)|Scala]], [[Standard ML|ML]], [[Haskell (programming language)|Haskell]]
|-
| 2006
| [[Vala (programming language)|Vala]]
| [[GNOME]]
| C#
|-
| 2007
| [[Ada (programming language)|Ada 2005]]
| Ada Rapporteur Group
| Ada 95
|-
| 2007
| [[Agda (programming language)|Agda]]
| Ulf Norell
| [[Coq]], [[Epigram (programming language)|Epigram]], [[Haskell (programming language)|Haskell]]
|-
|2007
|[[QB64]]
|Galleon, QB64Team
|QBasic
|-
| 2007
| [[Clojure]]
| [[Rich Hickey]]
| [[Lisp (programming language)|Lisp]], [[ML (programming language)|ML]], [[Haskell (programming language)|Haskell]], [[Erlang (programming language)|Erlang]]
|-
| 2007
| [[LOLCODE]]
| Adam Lindsay
| {{n/a|none (unique language)}}
|-
| 2007
| [[Oberon_(programming_language)#Oberon-07|Oberon-07]]
| [[Niklaus Wirth|Wirth]]
| Oberon
|-
| 2007
| [[Swift (parallel scripting language)]]
| [[University of Chicago]], [[Argonne National Laboratory]]
|
|-
| 2008
| [[Nim (programming language)|Nim]]
| Andreas Rumpf
| [[Python (programming language)|Python]], [[Lisp (programming language)|Lisp]], [[Object Pascal]]
|-
| 2008
| [[Genie (programming language)|Genie]]
| Jamie McCracken
| [[Python (programming language)|Python]], [[Boo (programming language)|Boo]], [[D (programming language)|D]], [[Object Pascal]]
|-
| 2008
| [[Pure (programming language)|Pure]]
| Albert Gräf
| Q
|-
| 2009
| [[Chapel (programming language)|Chapel]]
| Brad Chamberlain, [[Cray]] Inc.
| [[High Performance Fortran|HPF]], [[ZPL (programming language)|ZPL]]
|-
| 2009
| [[Go (programming language)|Go]]
| [[Google]]
| [[C (programming language)|C]], [[Oberon (programming language)|Oberon]], [[Limbo (programming language)|Limbo]], [[Smalltalk]]
|-
| 2009
| [[CoffeeScript]]
| [[Jeremy Ashkenas]]
| [[JavaScript]], [[Ruby (programming language)|Ruby]], [[Python (programming language)|Python]], [[Haskell (programming language)|Haskell]]
|-
| 2009
| [[Idris (programming language)|Idris]]
| Edwin Brady
| [[Haskell (programming language)|Haskell]], [[Agda (programming language)|Agda]], [[Coq]]
|-
| 2009
| [[ParaSail (programming language)|Parasail]]
| S. Tucker Taft, [[AdaCore]]
| [[Modula]], [[Ada (programming language)|Ada]], [[Pascal (programming language)|Pascal]], [[ML (programming language)|ML]]
|-
| 2009
| [[Whiley (programming language)|Whiley]]
| David J. Pearce
| [[Java (programming language)|Java]], [[C (programming language)|C]], [[Python (programming language)|Python]]
|-
| 2009
| [[Dafny|Dafny]]
| K. Rustan M. Leino
| [[Java (programming language)|Java]], [[Spec Sharp|Spec#]]
|- class="sortbottom"
! Year
! Name
! Chief developer, company
! Predecessor(s)
|}

==2010s==
{| class="wikitable sortable"
|-
! Year
! Name
! Chief developer, company
! Predecessor(s)
|-
| 2010
| [[Rust (programming language)|Rust]]
| Graydon Hoare, [[Mozilla]]
| [[Alef (programming language)|Alef]], [[C++]], [[Camlp4]], [[Erlang (programming language)|Erlang]], [[Hermes (programming language)|Hermes]], [[Limbo (programming language)|Limbo]], Napier, [[Napier88]], [[Newsqueak]], NIL, [[Sather]], [[Standard ML]]
|-
| 2011
| [[Ceylon Project|Ceylon]]
| Gavin King, [[Red Hat]]
| [[Java (programming language)|Java]]
|-
| 2011
| [[Dart (programming language)|Dart]]
| [[Google]]
| [[Java (programming language)|Java]], [[JavaScript]], [[CoffeeScript]], [[Go (programming language)|Go]]
|-
| 2011
| [[C++11]]
| C++ ISO/IEC 14882:2011
| [[C++]], Standard C, [[C (programming language)|C]]
|-
| 2011
| [[Kotlin (programming language)|Kotlin]]
| [[JetBrains]]
| [[Java (programming language)|Java]], [[Scala (programming language)|Scala]], [[Groovy (programming language)|Groovy]], [[C Sharp (programming language)|C#]], [[Gosu (programming language)|Gosu]]
|-
|2011
| [[Red (programming language)|Red]]
| Nenad Rakocevic
| [[Rebol]], [[Scala (programming language)|Scala]], [[Lua (programming language)|Lua]]
|-
|2011
|[[Monkey X|MonkeyX]]
|Mark Sibly
|
|-
|2011
| [[Opa (programming language)|Opa]]
| MLstate
| [[OCaml]], [[Erlang (programming language)|Erlang]], [[JavaScript]]
|-
| 2012
| [[Elixir (programming language)|Elixir]]
| José Valim
| [[Erlang (programming language)|Erlang]], [[Ruby (programming language)|Ruby]], [[Clojure]]
|-
| 2012
| [[Elm (programming language)|Elm]]
| Evan Czaplicki
| [[Haskell (programming language)|Haskell]], [[Standard ML]], [[OCaml]], [[F Sharp (programming language)|F#]]
|-
| 2012
| [[TypeScript]]
| [[Anders Hejlsberg]], [[Microsoft]]
| [[JavaScript]], [[CoffeeScript]]
|-
| 2012
| [[Julia (programming language)|Julia]]
| [[Jeff Bezanson]], [[Stefan Karpinski]], Viral Shah, [[Alan Edelman]],&lt;!-- Leave out Alan? He is left out as "developer", but included as "designer" in Julia's infobox as Jeff's PhD advisor(?):--&gt; [[Massachusetts Institute of Technology|MIT]]
| [[MATLAB]], [[Lisp (programming language)|Lisp]],&lt;!--only the parser is implemented by Femtolisp, a [[Scheme (programming language)|Scheme]] implementation (see: "unsupported" "julia --lisp" that invokes it) the only influence from Lisp/Scheme are Lisp-like macros, redundant to list Scheme. Also Dylan was cut from the list despite its "multiple dispatch"--&gt; [[C (programming language)|C]], [[Fortran]], [[Mathematica]]&lt;ref&gt;{{cite web
  | title      = Why We Created Julia
  | date       = February 2012
  | website    = Julia website
  | url        = http://julialang.org/blog/2012/02/why-we-created-julia
  | accessdate = 7 February 2013
  }}&lt;/ref&gt; (strictly its [[Wolfram Language]]), [[Python (programming language)|Python]], [[Perl]], [[R (programming language)|R]], [[Ruby (programming language)|Ruby]], [[Lua (programming language)|Lua]]&lt;ref&gt;{{cite web|title=Introduction|url=http://julia.readthedocs.org/en/latest/manual/introduction/|website=The Julia Manual|deadurl=yes|archiveurl=https://web.archive.org/web/20160408134008/http://julia.readthedocs.org/en/latest/manual/introduction/|archivedate=8 April 2016|df=dmy-all}}&lt;/ref&gt;
|-
| 2012
| [[P (programming language)|P]]
| Vivek Gupta, [[Ethan Jackson]], [[Shaz Qadeer]], [[Sriram Rajamani]], [[Microsoft]]
| 
|-
| 2012
| [[Ada (programming language)|Ada 2012]]
| ARA and Ada Europe (ISO/IEC 8652:2012)
| Ada 2005, ISO/IEC 8652:1995/Amd 1:2007
|-
|2013
|[[P4 (programming language)|P4]]
|P4 Language Consortium ([https://P4.org P4.org])
|
|-
|2013
|[[Cuneiform (programming language)|Cuneiform]]
|Jörgen Brandt
|[[Swift (parallel scripting language)]]
|-
| 2014
| [[Crystal (programming language)|Crystal]]
| Ary Borenszweig, Manas Technology Solutions
| [[Ruby (programming language)|Ruby]], [[C (programming language)|C]], [[Rust (programming language)|Rust]], [[Go (programming language)|Go]], [[C Sharp (programming language)|C#]], [[Python (programming language)|Python]]
|-
| 2014
| [[Hack (programming language)|Hack]]
| [[Facebook]]
| [[PHP]]
|-
| 2014
| [[Swift (programming language)|Swift]]
| [[Apple Inc.]]
| [[Objective-C]], [[Rust (programming language)|Rust]], [[Haskell (programming language)|Haskell]], [[Ruby (programming language)|Ruby]], [[Python (programming language)|Python]], [[C Sharp (programming language)|C#]], [[CLU (programming language)|CLU]]
|-
| 2014
| [[C++14]]
| C++ ISO/IEC 14882:2014
| [[C++]], Standard C, [[C (programming language)|C]]
|-
| 2015
| [[Perl 6]]
| [https://github.com/rakudo/rakudo/blob/master/CREDITS The Rakudo Team]
| [[Perl]], [[Haskell (programming language)|Haskell]], [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]]
|-
| 2016
| [[Ring (programming language)|Ring]]
| Mahmoud Fayed
| [[Lua (programming language)|Lua]], [[Python (programming language)|Python]], [[Ruby (programming language)|Ruby]], [[C (programming language)|C]], [[C Sharp (programming language)|C#]], [[BASIC]], [[QML]], [[xBase]], Supernova
|-
| 2017
| [[C++17]]
| C++ ISO/IEC 14882:2017
| [[C++]], Standard C, [[C (programming language)|C]]
|-
| 2017
| [[Ballerina (programming language)|Ballerina]]
| [[WSO2]], Open Source&lt;ref&gt;https://github.com/ballerina-platform/ballerina-lang&lt;/ref&gt;
| 
|-
| 2018
| [[Fortran#Fortran_2018|Fortran 2018]]
| ISO/IEC JTC1/SC22/WG5 N2150:2018
| [[Fortran#Fortran_2008|Fortran 2008]]
|- class="sortbottom"
! Year
! Name
! Chief developer, company
! Predecessor(s)
|}

==See also==
* [[Programming language]]
* [[Timeline of computing]]
* [[History of computing hardware]]
* [[History of programming languages]]

==References==
{{Reflist}}

==External links==
* [https://web.archive.org/web/20110220044217/http://hopl.murdoch.edu.au/ Online encyclopedia for the history of programming languages]
* [http://merd.sourceforge.net/pixel/language-study/diagram.html Diagram &amp; history of programming languages]
* [http://www.levenez.com/lang/ Eric Levenez's timeline diagram of computer languages history]

{{Programming languages}}

{{DEFAULTSORT:Timeline Of Programming Languages}}
[[Category:Computing timelines|Programming]]
[[Category:Lists of programming languages]]
[[Category:History of computer science]]</text>
      <sha1>2sxmfdy5436aia02qofor1v8soz1aw8</sha1>
    </revision>
  </page>
  <page>
    <title>Wiener–Khinchin theorem</title>
    <ns>0</ns>
    <id>3350111</id>
    <revision>
      <id>865930916</id>
      <parentid>865930837</parentid>
      <timestamp>2018-10-27T03:34:04Z</timestamp>
      <contributor>
        <ip>123.205.189.156</ip>
      </contributor>
      <comment>/* The case of a continuous-time process */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12263">In applied mathematics, the '''Wiener–Khinchin theorem''', also known as the '''Wiener–Khintchine theorem''' and sometimes as the '''Wiener–Khinchin–Einstein theorem''' or the '''Khinchin–Kolmogorov theorem''', states that the [[autocorrelation]] function of a [[wide-sense-stationary random process]] has a spectral decomposition given by the [[power spectrum]] of that process.&lt;ref name="C. Chatfield 1989 94–95"&gt;{{cite book | title = The Analysis of Time Series—An Introduction | author = C. Chatfield | edition = fourth | publisher = Chapman and Hall, London | year = 1989 | isbn=0-412-31820-2 | pages = 94–95}}&lt;/ref&gt;&lt;ref&gt;{{cite book | title = Time Series | author = Norbert Wiener | publisher = M.I.T. Press, Cambridge, Massachusetts | year = 1964 | page = 42}}&lt;/ref&gt;&lt;ref&gt;Hannan, E.J., "Stationary Time Series", in: John Eatwell, Murray Milgate, and Peter Newman, editors, ''The New Palgrave: A Dictionary of Economics.  Time Series and Statistics'', Macmillan, London, 1990, p. 271.&lt;/ref&gt;&lt;ref&gt;{{cite book | title = Echo Signal Processing | author = Dennis Ward Ricker | publisher = Springer | year = 2003 | isbn = 1-4020-7395-X | url = https://books.google.com/books?id=NF2Tmty9nugC&amp;pg=PA23 }}&lt;/ref&gt;&lt;ref&gt;{{cite book | title = Digital and Analog Communications Systems | author = Leon W. Couch II | edition = sixth | publisher = Prentice Hall, New Jersey | year = 2001 | isbn=0-13-522583-3| pages = 406–409}}&lt;/ref&gt;&lt;ref&gt;{{cite book | title = Wireless Technologies: Circuits, Systems, and Devices | author =  Krzysztof Iniewski | publisher = CRC Press | year = 2007 | isbn = 0-8493-7996-2 | url = https://books.google.com/books?id=JJXrpazX9FkC&amp;pg=PA390 }}&lt;/ref&gt;&lt;ref&gt;{{cite book | title = Statistical Optics | author = Joseph W. Goodman | publisher = Wiley-Interscience | year = 1985 | isbn=0-471-01502-4}}&lt;/ref&gt;

==History==
[[Norbert Wiener]] proved this [[theorem]] for the case of a deterministic function in 1930;&lt;ref&gt;{{cite journal|last=Wiener|first=Norbert|title=Generalized Harmonic Analysis|journal=Acta Mathematica|year=1930|volume=55|pages=117–258|doi=10.1007/bf02546511}}&lt;/ref&gt; [[Aleksandr Khinchin]] later formulated an analogous result for stationary stochastic processes and published that probabilistic analogue in 1934.&lt;ref name="Champeney"&gt;{{cite book|author=D.C. Champeney|title=A Handbook of Fourier Theorems|year=1987|publisher=Cambridge University Press|chapter=Power spectra and Wiener's theorems|page=102 |quote=Wiener's basic theory of 'generalised harmonic analysis' is in no way probabilistic, and the theorems apply to single well defined functions rather than to ensembles of functions [...] A further development of these ideas occurs in the work of A. I. Khintchine (1894–1959) on stationary random processes (or stochastic processes) [...] in contexts in which it is not important to distinguish the two approaches the theory is often referred to as the Wiener—Khintchine theory. }}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Khintchine|first=Alexander|title=Korrelationstheorie der stationären stochastischen Prozesse |journal=[[Mathematische Annalen]]|year=1934 |volume=109 |issue=1 |pages=604–615 |doi=10.1007/BF01449156 }}&lt;/ref&gt;   [[Albert Einstein]] explained, without proofs, the idea in a brief two-page memo in 1914.&lt;ref&gt;{{cite book|title=The Legacy of Norbert Wiener: A Centennial Symposium (Proceedings of Symposia in Pure Mathematics)|page=95|first1=David|last1=Jerison|first2=Isadore Manuel|last2=Singer|first3=Daniel W.|last3=Stroock|publisher=American Mathematical Society|year=1997|isbn=0-8218-0415-4}}&lt;/ref&gt;

==The case of a continuous-time process==
For continuous time, the Wiener–Khinchin theorem says that if &lt;math&gt; x &lt;/math&gt; is a wide-sense stationary process such that its [[autocorrelation function]] (sometimes called [[autocovariance]]) defined in terms of statistical [[expected value]],
&lt;math&gt;r_{xx}(\tau) = \operatorname{E}\big[x(t)^*x(t - \tau)\big]&lt;/math&gt; (the asterisk denotes complex conjugate, and of course it can be omitted if the random process is real-valued), exists and is finite at every lag &lt;math&gt; \tau &lt;/math&gt;, then there exists a monotone function &lt;math&gt; F(f) &lt;/math&gt; in the frequency domain &lt;math&gt; -\infty &lt; f &lt; \infty &lt;/math&gt; such that

:&lt;math&gt; r_{xx} (\tau) = \int_{-\infty}^\infty e^{2\pi i\tau f}  dF(f), &lt;/math&gt;

where the integral is a [[Riemann–Stieltjes integral]].&lt;ref name="C. Chatfield 1989 94–95"/&gt;&lt;ref&gt;{{cite book |last=Hannan |first=E. J. |chapter=Stationary Time Series |editor-first=John |editor-last=Eatwell |editor2-first=Murray |editor2-last=Milgate |editor3-first=Peter |editor3-last=Newman |title=The New Palgrave: A Dictionary of Economics. Time Series and Statistics |publisher=Macmillan |location=London |year=1990 |page=271 |chapterurl=https://books.google.com/books?id=CUevCwAAQBAJ&amp;pg=PA271 }}&lt;/ref&gt; This is a kind of spectral decomposition of the auto-correlation function.  ''F'' is called the power spectral distribution function and is a statistical distribution function.  It is sometimes called the integrated spectrum.

Note that the Fourier transform of &lt;math&gt;x(t)&lt;/math&gt; does not exist in general, because stationary random functions are not generally either [[square-integrable function|square-integrable]] or absolutely integrable.  Nor is &lt;math&gt; r_{xx} &lt;/math&gt; assumed to be absolutely integrable, so it need not have a Fourier transform either.

But if &lt;math&gt; F(f) &lt;/math&gt; is absolutely continuous, for example, if the process is purely indeterministic, then &lt;math&gt; F &lt;/math&gt; is differentiable [[almost everywhere]].  In this case, one can define &lt;math&gt; S(f)&lt;/math&gt;, the power [[spectral density]] of &lt;math&gt;x(t)&lt;/math&gt;, by taking the averaged derivative of &lt;math&gt; F &lt;/math&gt;.  Because the left and right derivatives of &lt;math&gt; F &lt;/math&gt; exist everywhere, we can put &lt;math&gt; S(f) = \frac12 \left(\lim_{\epsilon \downarrow 0} \frac1\epsilon \big(F(f + \epsilon) - F(f)\big) + \lim_{\epsilon \uparrow 0} \frac1\epsilon \big(F(f + \epsilon) - F(f)\big)\right)&lt;/math&gt; everywhere,&lt;ref&gt;{{cite book | title = The Analysis of Time Series—An Introduction |first = C. |last=Chatfield | edition = Fourth | publisher = Chapman and Hall |location=London | year = 1989 | isbn=0-412-31820-2 | page = 96 |url=https://books.google.com/books?id=qKzyAbdaDFAC }}&lt;/ref&gt; (obtaining that ''F'' is the integral of its averaged derivative&lt;ref&gt;{{cite book | title = A Handbook of Fourier Theorems | first = D. C. |last=Champeney | publisher = Cambridge Univ. Press | year = 1987 | pages = 20–22 |url=https://books.google.com/books?id=IkQoC1ava5sC&amp;pg=PA20 }}&lt;/ref&gt;), and the theorem simplifies to

:&lt;math&gt; r_{xx} (\tau) = \int_{-\infty}^\infty S(f) e^{2\pi i\tau f} \,df. &lt;/math&gt;

If now one assumes that ''r'' and ''S'' satisfy the necessary conditions for Fourier inversion to be valid, the Wiener–Khinchin theorem takes the simple form of saying that ''r'' and ''S'' are a Fourier-transform pair, and

:&lt;math&gt; S(f) = \int_{-\infty}^\infty r_{xx} (\tau)  e^{-2\pi if\tau} \,d\tau. &lt;/math&gt;

==The case of a discrete-time process==
For the discrete-time case, the power spectral density of the function with discrete values &lt;math&gt;x[n]&lt;/math&gt; is

:&lt;math&gt; S(f)=\sum_{k=-\infty}^\infty r_{xx}[k]e^{-i(2\pi f) k}, &lt;/math&gt;

where

:&lt;math&gt;r_{xx}[k] = \operatorname{E}\big[ x[n] ^*x[n-k] \big] &lt;/math&gt;

is the discrete autocorrelation function of &lt;math&gt;x[n]&lt;/math&gt;, provided this is absolutely integrable.  Being a sampled and discrete-time sequence, the spectral density is periodic in the frequency domain.  This is due to the problem of [[aliasing]]: the contribution of any frequency 
higher than the [[Nyquist frequency]] seems to be equal to that of its alias between 0 and 1.  For this reason, the domain of the function &lt;math&gt; S &lt;/math&gt; is usually restricted to lie between 0 and 1.

==Application==
The theorem is useful for analyzing [[LTI system theory|linear time-invariant systems]] (LTI systems) when the inputs and outputs are not square-integrable, so their Fourier transforms do not exist.  A corollary is that the Fourier transform of the autocorrelation function of the output of an LTI system is equal to the product of the Fourier transform of the autocorrelation function of the input of the system times the squared magnitude of the Fourier transform of the system impulse response.&lt;ref&gt;
{{cite book
 | title = Random signals and noise: a mathematical introduction
 | author = Shlomo Engelberg
 | publisher = CRC Press
 | year = 2007
 | isbn = 978-0-8493-7554-5
 | page = 130
 | url = https://books.google.com/books?id=Zl51JGnoww4C&amp;pg=PA130
 }}&lt;/ref&gt;
This works even when the Fourier transforms of the input and output signals do not exist because these signals are not square-integrable, so the system inputs and outputs cannot be directly related by the Fourier transform of the impulse response.

Since the Fourier transform of the autocorrelation function of a signal is the power spectrum of the signal, this corollary is equivalent to saying that the power spectrum of the output is equal to the power spectrum of the input times the energy [[transfer function]].

This corollary is used in the parametric method for power spectrum estimation.

==Discrepancies in terminology==

In many textbooks and in much of the technical literature it is tacitly assumed that Fourier inversion of the [[autocorrelation]] function and the power spectral density is valid, and the Wiener–Khinchin theorem is stated, very simply, as if it said that the Fourier transform of the autocorrelation function was equal to the power [[spectral density]], ignoring all questions of convergence&lt;ref&gt;{{cite book | title = The Analysis of Time Series—An Introduction | author = C. Chatfield | edition = fourth | publisher = Chapman and Hall, London | year = 1989 | isbn=0-412-31820-2 | page = 98}}&lt;/ref&gt; (Einstein is an example).
But the theorem (as stated here) was applied by [[Norbert Wiener]] and [[Aleksandr Khinchin]] to the sample functions (signals) of [[wide-sense-stationary random process]]es, signals whose Fourier transforms do not exist.
The whole point of Wiener's contribution was to make sense of the spectral decomposition of the autocorrelation function of a sample function of a wide-sense-stationary random process even when the integrals for the Fourier transform and Fourier inversion do not make sense.

Further complicating the issue is that the discrete Fourier transform always exists for digital, finite-length sequences, meaning that the theorem can be blindly applied to calculate auto-correlations of numerical sequences. As mentioned earlier, the relation of this discrete sampled data to a mathematical model is often misleading, and related errors can show up as a divergence when the sequence length is modified.

Some authors refer to &lt;math&gt;R&lt;/math&gt; as the autocovariance function.  They then proceed to normalise it, by dividing by &lt;math&gt;R(0)&lt;/math&gt;, to obtain what they refer to as the autocorrelation function.

==References==
{{Reflist|30em}}

==Further reading==
*{{cite book |last=Brockwell |first=Peter A. |last2=Davis |first2=Richard J. |title=Introduction to Time Series and Forecasting |edition=Second |publisher=Springer-Verlag |location=New York |year=2002 |isbn=038721657X }}
*{{cite book |last=Chatfield |first=C. |title=The Analysis of Time Series—An Introduction |edition=Fourth |publisher=Chapman and Hall |location=London |year=1989 |isbn=0412318202 }}
*{{cite book |last=Fuller |first=Wayne |title=Introduction to Statistical Time Series |series=Wiley Series in Probability and Statistics |edition=Second |publisher=Wiley |location=New York |year=1996 |isbn=0471552399 }}
*{{cite paper |last=Wiener |first=Norbert |title=Extrapolation, Interpolation, and Smoothing of Stationary Time Series |publisher=Technology Press and Johns Hopkins Univ. Press |location=Cambridge, Massachusetts |year=1949 }} (a classified document written for the Dept. of War in 1943).
*{{cite book |last=Yaglom |first=A. M. |title=An Introduction to the Theory of Stationary Random Functions |publisher=Prentice–Hall |location=Englewood Cliffs, New Jersey |year=1962 }}

{{DEFAULTSORT:Wiener-Khinchin theorem}}
[[Category:Theorems in Fourier analysis]]
[[Category:Signal processing]]
[[Category:Probability theorems]]</text>
      <sha1>9oex2rd5en9nr1ytcwypdoi26dsn21m</sha1>
    </revision>
  </page>
  <page>
    <title>William Shaw (mathematician)</title>
    <ns>0</ns>
    <id>20134027</id>
    <revision>
      <id>861388833</id>
      <parentid>715464044</parentid>
      <timestamp>2018-09-27T02:07:42Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>add authority control, test</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2302">{{other people|William Shaw}}
'''William Shaw'''  (born 14 May 1958) is a [[United Kingdom|British]] [[mathematician]], currently visiting professor of the mathematics and computation of risk at [[University College London]].&lt;ref&gt;http://www.ucl.ac.uk/maths/staff/academic&lt;/ref&gt;&lt;ref&gt;http://iris.ucl.ac.uk/iris/browse/profile?upi=WTSHA30&lt;/ref&gt; He is a consultant on [[Derivative (finance)|financial derivatives]], an author of a primary book on using [[Mathematica]] to model financial derivatives, co-[[Editor-in-Chief]] of the journal ''Applied Mathematical Finance'', and a member of the Mathematics and Computer Science Departments at University College London.

Willian Shaw studied at [[King's College, Cambridge]], where he studied mathematics; he was [[Wrangler (University of Cambridge)|Wrangler]] and earned a B.A. in 1980. In 1981 he won the [[Mayhew Prize]]&lt;ref&gt;[http://www.maths.leeds.ac.uk/~morrison/mayhew.html Mayhew Prize winners list]&lt;/ref&gt; for his performance on the [[Cambridge Mathematical Tripos]]. In 1984 he received a Ph.D. in mathematical physics from [[Wolfson College, Oxford]]. From 1984 to 1987 he was a research fellow at Cambridge and [[C.L.E. Moore instructor|C.L.E. Moore Instructor]] at the [[Massachusetts Institute of Technology]]. From 1991 to 2002 he was a lecturer in mathematics at [[Balliol College, Oxford]], and in 2002 he moved to [[St Catherine's College, Oxford]], where he was University lecturer in financial mathematics. In 2006 he moved to a Professorship at [[King's College London]] and in 2011 to UCL.

==Books==
* Applied Mathematica: Getting Started, Getting it Done by W.T. Shaw and J. Tigg. Addison-Wesley, 1993.
* Modelling Financial Derivatives with Mathematica by W.T. Shaw, Cambridge University Press, 1998.
* Complex Analysis with Mathematica by W.T. Shaw, Cambridge University Press, 2006.

==References==
{{reflist}}

==External links==
* [http://www.ucl.ac.uk/Mathematics/staff/WShaw.html William Shaw's web-page]

{{authority control}}

{{DEFAULTSORT:Shaw, William}}
[[Category:1958 births]]
[[Category:Living people]]
[[Category:Alumni of King's College, Cambridge]]
[[Category:Academics of King's College London]]
[[Category:Academics of University College London]]
[[Category:English mathematicians]]
[[Category:Mathematical finance]]</text>
      <sha1>g7l5ar3uc3xnxotx9inzwrmeytq0ynk</sha1>
    </revision>
  </page>
  <page>
    <title>Éléments de géométrie algébrique</title>
    <ns>0</ns>
    <id>471387</id>
    <revision>
      <id>782831114</id>
      <parentid>762756890</parentid>
      <timestamp>2017-05-29T13:16:12Z</timestamp>
      <contributor>
        <username>David Schwein</username>
        <id>10360153</id>
      </contributor>
      <minor/>
      <comment>/* External links */ Fixed dead link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10280">{{italic title}}
{{Infobox book
| name          = Éléments de géométrie algébrique
| image         = Éléments de géométrie algébrique title page.jpg
| image_size = 200px
| author        = [[Alexander Grothendieck]] and [[Jean Dieudonné]]
| language      = French
| subject       = [[Algebraic geometry]]
| publisher     = [[Institut des Hautes Études Scientifiques]]
| pub_date      = 1960&amp;ndash;1967
}}

The '''''Éléments de géométrie algébrique''''' ("Elements of [[algebraic geometry|Algebraic Geometry]]") by [[Alexander Grothendieck]] (assisted by [[Jean Dieudonné]]), or '''''EGA''''' for short, is a rigorous treatise, in French, on [[algebraic geometry]] that was published (in eight parts or [[fascicle (book)|fascicles]]) from 1960 through 1967 by the [[Institut des Hautes Études Scientifiques]].  In it, Grothendieck established systematic foundations of algebraic geometry, building upon the concept of [[Scheme (mathematics)|schemes]], which he defined. The work is now considered the foundation stone and basic reference of modern algebraic geometry.

==Editions==
Initially thirteen chapters were planned, but only the first four (making a total of approximately 1500 pages) were published. Much of the material which would have been found in the following chapters can be found, in a less polished form, in the [[Séminaire de géométrie algébrique]] (known as SGA). Indeed, as explained by Grothendieck in the preface of the published version of SGA, by 1970 it had become clear  that incorporating all of the planned material in EGA would require significant changes in the earlier chapters already published, and that therefore the prospects of completing EGA in the near term were limited. An obvious example is provided by [[derived category|derived categories]], which became an indispensable tool in the later SGA volumes, was not yet used in EGA III as the theory was not yet developed at the time. Considerable effort was therefore spent to bring the published SGA volumes to a high degree of completeness and rigour. Before work on the treatise was abandoned, there were plans in 1966-67 to expand the group of authors to include Grothendieck's students [[Pierre Deligne]] and [[Michel Raynaud]], as evidenced by published correspondence between Grothendieck and [[David Mumford]].&lt;ref&gt;
{{cite book | first=David | last=Mumford |  title=Selected papers, Volume II. On algebraic geometry, including correspondence with Grothendieck. Edited by Ching-Li Chai, Amnon Neeman and Takahiro Shiota. | publisher=Springer | year=2010 | pages= 720, 722 | ISBN=978-0-387-72491-1 }}
&lt;/ref&gt; Grothendieck's letter of 4 November 1966 to Mumford also indicates that the second-edition revised structure was in place by that time, with Chapter VIII already intended to cover the Picard scheme. In that letter he estimated that at the pace of writing up to that point, the following four chapters (V to VIII) would have taken eight years to complete, indicating an intended length comparable to the first four chapters, which had been in preparation for about eight years at the time.

Grothendieck nevertheless wrote a revised version of EGA I which was published by [[Springer Science+Business Media|Springer-Verlag]].  It updates the terminology, replacing "prescheme" by "scheme" and "scheme" by "separated scheme", and heavily emphasizes the use of [[representable functor]]s. The new preface of the second edition also includes a slightly revised plan of the complete treatise, now divided into twelve chapters.

Grothendieck's EGA 5 which deals with Bertini type theorems is to some extent available
from the Grothendieck Circle website. Monografie Matematyczne in Poland has accepted this
volume for publication but the editing process is quite slow at this time 2010.
[[James Milne (mathematician)|James Milne]] has preserved some of the original Grothendieck notes and a translation of them
into English. They may be available from his websites  connected with the University of Michigan in Ann Arbor.

==Chapters==
The following table lays out the original and revised plan of the treatise and indicates where (in SGA or elsewhere) the topics intended for the later, unpublished chapters were treated by Grothendieck and his collaborators.

{| class="wikitable"
|-
! #
! width= "27%"|  First edition 
! width= "27%"|  Second edition
!  Comments
|-
| I
| '''Le langage des schémas'''
| '''Le langage des schémas'''
| Second edition brings in certain schemes representing functors such as [[Grassmannian]]s, presumably from intended Chapter V of the first edition. In addition, the contents of Section 1 of Chapter IV of first edition was moved to Chapter I in the second edition.
|-
| II
| '''Étude globale élémentaire de quelques classes de morphismes'''
| Étude globale élémentaire de quelques classes de morphismes
| First edition complete, second edition did not appear.
|-
| III
| '''Étude cohomologique des faisceaux cohérents'''
| Cohomologie des Faisceaux algébriques cohérents. Applications.
| First edition complete except for last four sections, intended for publication after Chapter IV: elementary projective duality, local cohomology and its relation to projective cohomology, and Picard groups (all but projective duality treated in SGA 2).
|-
| IV
| '''Étude locale des schémas et des morphismes de schémas''' 
| Étude locale des schémas et des morphismes de schémas
| First edition essentially complete; some changes made in last sections; the section on hyperplane sections made into the new Chapter V of second edition (draft exists)
|-
| V
| Procédés élémentaires de construction de schémas
| Complements sur les morphismes projectifs
| Did not appear. Some elementary constructions of schemes apparently intended for first edition appear in Chapter I of second edition. The existing draft of Chapter V corresponds to the second edition plan. It includes also expanded treatment of some material from SGA 7.
|-
| VI
| Technique de descente.&lt;br/&gt; Méthode générale de construction des schémas
| Techniques de construction de schémas
| Did not appear. [[Descent theory]] and related construction techniques summarised by Grothendieck in [[Fondements de la Géometrie Algébrique|FGA]]. By 1968 the plan had evolved to treat [[algebraic space]]s and [[algebraic stack]]s.
|-
| VII
| Schémas de groupes, espaces fibrés principaux
| Schémas en groupes, espaces fibrés principaux
| Did not appear. Treated in detail in SGA 3.
|-
| VIII
| Étude différentielle des espaces fibrés
| Le schéma de Picard
| Did not appear. Material apparently intended for first edition can be found in SGA 3, construction and results on Picard scheme are summarised in [[Fondements de la Géometrie Algébrique|FGA]].
|-
| IX
| Le groupe fondamental
| Le groupe fondamental
| Did not appear. Treated in detail in SGA 1.
|-
| X
| Résidus et dualité
| Résidus et dualité
| Did not appear. Treated in detail in Hartshorne's edition of Grothendieck's notes "Residues and duality"
|-
| XI
| Théorie d'intersection, classes de Chern, théorème de Riemann-Roch 
| Théorie d'intersection, classes de Chern, théorème de Riemann-Roch 
| Did not appear. Treated in detail in SGA 6.
|-
| XII
| Schémas abéliens et schémas de Picard 
| Cohomologie étale des schémas
| Did not appear.&lt;br/&gt;Étale cohomology treated in detail in SGA 4, SGA 5.
|-
| XIII
| Cohomologie de Weil
| none
| Intended to cover étale cohomology in the first edition.
|}

In addition to the actual chapters, an extensive "Chapter 0" on various preliminaries was divided between the volumes in which the treatise appeared. Topics treated range from [[category theory]], [[sheaf (mathematics)|sheaf theory]] and [[general topology]] to [[commutative algebra]] and [[homological algebra]]. The longest part of Chapter 0, attached to Chapter IV, is more than 200 pages.

Grothendieck never gave permission for the 2nd edition of EGA I to be republished, so copies are rare but found in many libraries. The work on EGA was finally disrupted by Grothendieck's departure first from [[IHÉS]] in 1970 and soon afterwards from the mathematical establishment altogether. Grothendieck's incomplete notes on EGA V can be found at [http://www.grothendieckcircle.org/].

In historical terms, the development of the ''EGA'' approach set the seal on the application of [[sheaf theory]] to algebraic geometry, set in motion by [[Jean-Pierre Serre|Serre]]'s basic paper ''[[Faisceaux algébriques cohérents|FAC]]''. It also contained the first complete exposition of the algebraic approach to differential calculus, via principal parts. The foundational unification it proposed (see for example [[unifying theories in mathematics]]) has stood the test of time.

''EGA'' has been scanned by [[NUMDAM]] and is available at [http://www.numdam.org] under "Publications mathématiques de l'IHÉS", volumes 4, 8, 11, 17, 20, 24, 28 and 32.

== Bibliographic information ==
*{{EGA|book=1-2}}
*{{EGA|book=1| pages = 5–228}}&lt;ref&gt;{{cite journal|author=Lang, S.|authorlink=Serge Lang|title=Review: ''Éléments de géométrie algébrique'', par A. Grothendieck, rédigés avec la collaboration de J. Dieudonné|journal=Bull. Amer. Math. Soc.|year=1961|volume=67|issue=3|pages=239–246|url=http://www.ams.org/journals/bull/1961-67-03/S0002-9904-1961-10564-8/S0002-9904-1961-10564-8.pdf|doi=10.1090/S0002-9904-1961-10564-8}}&lt;/ref&gt;
*{{EGA|book=2| pages = 5–222}}
*{{EGA|book=3-1| pages = 5–167}}
*{{EGA|book=3-2| pages = 5–91}}
*{{EGA|book=4-1| pages = 5–259}}
*{{EGA|book=4-2| pages = 5–231}}
*{{EGA|book=4-3| pages = 5–255}}
*{{EGA|book=4-4| pages = 5–361}}

==See also==
* ''[[Fondements de la Géometrie Algébrique]]''
* ''[[Séminaire de Géométrie Algébrique du Bois Marie]]''

==References==
{{reflist}}

==External links==
*Scanned copies and partial English translations: [http://www.math.jussieu.fr/~leila/grothendieckcircle/mathtexts.php Mathematical Texts]
*Detailed table of contents: [http://perso.telecom-paristech.fr/~madore/ega-toc.pdf]

{{DEFAULTSORT:Elements De Geometrie Algebrique}}
[[Category:Scheme theory]]
[[Category:1960 books]]
[[Category:Mathematics books]]
[[Category:Unfinished books]]
[[Category:Mathematics literature]]</text>
      <sha1>ar57jr1halmml7cbqqsqbjrd3bebe6f</sha1>
    </revision>
  </page>
</mediawiki>
