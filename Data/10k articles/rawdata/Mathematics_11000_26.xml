<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>213 (number)</title>
    <ns>0</ns>
    <id>834364</id>
    <revision>
      <id>832584091</id>
      <parentid>768947288</parentid>
      <timestamp>2018-03-26T21:15:34Z</timestamp>
      <contributor>
        <username>DePiep</username>
        <id>199625</id>
      </contributor>
      <minor/>
      <comment>save text from invisible. replace SloanesRef: use {{Cite OEIS}} (via [[WP:JWB]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1128">{{Infobox number
| number = 213
| divisor = 1, 3, 71, 213
}}

'''213''' ('''two hundred [and] thirteen''') is the number following [[212 (number)|212]] and preceding [[214 (number)|214]].

It is the smallest of a triple consecutive numbers that are products of two distinct [[prime number]]s: 213&amp;nbsp;=&amp;nbsp;3&amp;nbsp;&amp;times;&amp;nbsp;71, 214&amp;nbsp;=&amp;nbsp;2&amp;nbsp;&amp;times;&amp;nbsp;107, and 215&amp;nbsp;=&amp;nbsp;5&amp;nbsp;&amp;times;&amp;nbsp;43.&lt;ref&gt;{{Cite OEIS|A039833|name=Smallest of three consecutive squarefree numbers n, n+1, n+2 of the form p*q where p and q are primes}}.&lt;/ref&gt; Its square is a sum of distinct factorials: 213&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;45369&amp;nbsp;=&amp;nbsp;1!&amp;nbsp;+&amp;nbsp;2!&amp;nbsp;+&amp;nbsp;3!&amp;nbsp;+&amp;nbsp;7!&amp;nbsp;+&amp;nbsp;8!.&lt;ref&gt;{{Cite OEIS|A014597|name=Numbers n such that n^2 is a sum of distinct factorials}}.&lt;/ref&gt;

==See also==
* [[213 (group)|213]], a hip hop music group
* [[Area code 213]]
* [[213 Lilaea]] a [[main belt]] asteroid.
* [[+213]] is the code for international direct-dial phone calls to [[Algeria]].

==References==
{{reflist}}

{{Integers|2}}
[[Category:Integers]]
{{Num-stub}}
[[ca:Nombre 210#Nombres del 211 al 219]]</text>
      <sha1>5xsk0abd7e03pguy8607u1d7n8hlub4</sha1>
    </revision>
  </page>
  <page>
    <title>66 (number)</title>
    <ns>0</ns>
    <id>403326</id>
    <revision>
      <id>865495580</id>
      <parentid>865466570</parentid>
      <timestamp>2018-10-24T08:52:15Z</timestamp>
      <contributor>
        <username>Arthur Rubin</username>
        <id>374195</id>
      </contributor>
      <comment>Reverted 1 edit by [[Special:Contributions/73.94.20.139|73.94.20.139]] ([[User talk:73.94.20.139|talk]]): Clearly contrived and unimportant. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5635">{{example farm|date=March 2010}}
{{Infobox number
| number = 66
| divisor = 1, 2, 3, 6, 11, 22, 33, 66
}}
'''66''' ('''sixty-six''') is the [[natural number]] following [[65 (number)|65]] and preceding [[67 (number)|67]].

Usages of this number include:

==In mathematics==
'''66''' is:

*a [[sphenic number]].&lt;ref&gt;{{Cite web|url=https://oeis.org/A007304|title=Sloane's A007304 : Sphenic numbers|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-29}}&lt;/ref&gt;
*a [[triangular number]].&lt;ref&gt;{{Cite web|url=https://oeis.org/A000217|title=Sloane's A000217 : Triangular numbers|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-29}}&lt;/ref&gt;
*a [[hexagonal number]].&lt;ref&gt;{{Cite web|url=https://oeis.org/A000384|title=Sloane's A000384 : Hexagonal numbers|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-29}}&lt;/ref&gt;
*a [[semi-meandric number]].&lt;ref&gt;{{Cite web|url=https://oeis.org/A000682|title=Sloane's A000682 : Semimeanders|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-29}}&lt;/ref&gt;
*a [[semiperfect number]], being a multiple of a [[perfect number]].
*an [[Erdős–Woods number]], since it is possible to find sequences of 66 consecutive integers such that each inner member shares a factor with either the first or the last member.&lt;ref&gt;{{Cite web|url=https://oeis.org/A059756|title=Sloane's A059756 : Erdős-Woods numbers|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-29}}&lt;/ref&gt;
*palindromic and a [[repdigit]] in bases 10 (66&lt;sub&gt;10&lt;/sub&gt;), 21 (33&lt;sub&gt;21&lt;/sub&gt;) and 32 (22&lt;sub&gt;32&lt;/sub&gt;)

==In science==

===Astronomy===
*[[Messier object]] [[Spiral Galaxy M66]], a [[visual magnitude|magnitude]] 10.0 [[galaxy]] in the [[constellation]] [[Leo (constellation)|Leo]].
*The [[New General Catalogue]] object&lt;ref&gt;{{cite web|url=http://www.ngcic.org/|title=The Ecig Database Files|work=ngcic.org}}&lt;/ref&gt;  [[NGC 66]], a [[peculiar galaxy|peculiar]] [[barred spiral galaxy]] in the constellation [[Cetus]].

===Physics===
*The [[atomic number]] of [[dysprosium]], a lanthanide.

==In computing==
66 (more specifically 66.667) megahertz (MHz) is a common divisor for the [[front side bus]] (FSB) speed, overall [[central processing unit]] (CPU) speed, and base bus speed. On a Core 2 CPU, and a Core 2 [[motherboard]], the FSB is 1066&amp;nbsp;MHz (~16 × 66&amp;nbsp;MHz), the memory speed is usually 666.67&amp;nbsp;MHz (~10 × 66&amp;nbsp;MHz), and the processor speed ranges from 1.86 gigahertz (GHz) (~66&amp;nbsp;MHz × 28) to 2.93&amp;nbsp;GHz (~66&amp;nbsp;MHz × 44), in 266&amp;nbsp;MHz (~66&amp;nbsp;MHz × 4) increments.

==In motor vehicle transportation==
[[Image:Route66 sign.jpg|thumb|[[U.S. Route 66|Route 66]] sign]]

*The designation of the historic [[U.S. Route 66]], dubbed the "Mother Road" by novelist [[John Steinbeck]], and [[List of highways numbered 66|other roads]].
*[[Phillips 66]], a brand of [[gasoline]] and [[filling station|service station]] in the United States.

==In religion==
*The total number of chapters in the [[Bible]] book of [[Isaiah]].
*The number of verses in Chapter 3 of the book of [[Book of Lamentations|Lamentations]] in the [[Old Testament]].
*The total number of books in the Protestant edition of the [[Bible]] ([[Old Testament]] and [[New Testament]]) combined.{{Citation needed|reason=|date=January 2018}}
*In ''[[Abjad numerals]]'', [[The Name Of Allah (الله)]] numeric value is '''66'''.

==In sports==
The number of the laps of the [[Spanish Grand Prix]].

==In entertainment==

===Cinema===
*''[[Sixty Six (film)|Sixty Six]]'' is a 2006 British movie about a [[bar mitzvah]] in London on the day of the [[1966 FIFA World Cup|1966 World Cup final]].
*In the ''[[Star Wars]]'' movie series, [[Order 66]] is a prepared order to the [[clone troopers]] to kill the [[Jedi]] commanding them.

===Television===
*[[Route 66 (TV series)|''Route 66'']] was a popular US television series on [[CBS]] from 1960 to 1964.

===Video games===
*In the video game ''[[Fullmetal Alchemist]]'', elusive villain Barry the Chopper is imprisoned in cell number 66, which later becomes his alias when battling the brothers at Laboratory Five.

==In other fields==
*The [[calling codes|international direct dialing]] (IDD) code for [[Thailand]].
*The number of the French department [[Pyrénées-Orientales]].
*The name of a [[Germany|German]] [[card game]], translated from ''sechsundsechzig'' (see [[Sixty-six (game)]]).
*In [[Telecommunications]] a [[66 block]] is used to organize telephone lines.
*Sergio Mendes and [[Brazil '66]] was a 1960s group.
*66 [[WFAN|WNBC radio]] was a popular New York radio station, which  became [[WFAN]] on 1 July 1987.
*[[Lil B]] has a song entitled "OMG 66".
* ''[[Le 66]]'' is an [[opérette]] in one act of 1856 with music by [[Jacques Offenbach]] and a [[French language|French]] libretto by Pittaud de Forges and Laurencin.
*In the [[horror genre]] and [[western esotericism]] there are 66 legions of demons/spirits that Baal controls.&lt;ref&gt;{{cite web|url=http://horrornews.net/50654/demon-profile-baal/|title=Demon Profile: Baal|work=HNN - Horrornews.net - Official News Site}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.meta-religion.com/Esoterism/Satanism/goetia_demons.htm|title=Goetia Demons|work=meta-religion.com}}&lt;/ref&gt;

{{Integers|zero}}

==References==
{{reflist}}

==External links==
*{{Commons category-inline|66 (number)}}

[[Category:Integers]]</text>
      <sha1>apf3qw25yywy2uymjx4n8u37n4kecxw</sha1>
    </revision>
  </page>
  <page>
    <title>Age of the captain</title>
    <ns>0</ns>
    <id>30904877</id>
    <revision>
      <id>853529433</id>
      <parentid>851296893</parentid>
      <timestamp>2018-08-05T11:35:01Z</timestamp>
      <contributor>
        <username>Antiquary</username>
        <id>5831124</id>
      </contributor>
      <comment>+wikilink to [[Flaubert's letters]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3210">The '''age of the captain''' is a [[mathematical]] [[Word problem (mathematics education)|word problem]] which cannot be answered even though there seems to be plenty of information supplied. It was given for the first time by [[Gustave Flaubert]] in [[Flaubert's letters|a letter]] to his sister Caroline in 1841:&lt;ref name="Flaubert"&gt;Flaubert, Gustave; [http://gallica.bnf.fr/ark:/12148/bpt6k6533968v/f97.image.r=Flaubert%20correspondance Lettre à Caroline, 16 mai 1841], ''Correspondance'' , première série (1830–1850), G. Charpentier et C&lt;sup&gt;ie&lt;/sup&gt;, Éditeurs, Paris, 1887&lt;/ref&gt;
&lt;blockquote&gt;
Puisque tu fais de la géométrie et de la trigonométrie, je vais te donner un problème : Un navire est en mer, il est parti de Boston chargé de coton, il jauge 200 tonneaux. Il fait voile vers le Havre, le grand mât est cassé, il y a un mousse sur le gaillard d’avant, les passagers sont au nombre de douze, le vent souffle N.-E.-E., l’horloge marque 3 heures un quart d’après-midi, on est au mois de mai…. On demande l’âge du capitaine?&lt;ref name="Flaubert" /&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
Since you are now studying geometry and trigonometry, I will give you a problem. A ship sails the ocean. It left Boston with a cargo of wool. It grosses 200 tons. It is bound for Le Havre. The mainmast is broken, the cabin boy is on deck, there are 12 passengers aboard, the wind is blowing East-North-East, the clock points to a quarter past three in the afternoon. It is the month of May. How old is the captain?&lt;ref name="Maths Quotes"&gt;{{cite web|url=http://math.furman.edu/~mwoodard/ascquotf.html |title=Mathematical Quotations – F |publisher=Math.furman.edu |date= |accessdate=2018-05-24}}&lt;/ref&gt;
&lt;/blockquote&gt;

More recently, a simpler version has been used to study how students react to word problems:
&lt;blockquote&gt;
A captain owns 26 sheep and 10 goats. How old is the captain?&lt;ref name="Jstor"&gt;Verschaffel, L.; Greer, B.; de Corte, E.; [https://www.jstor.org/pss/3483286 ''Making Sense of Word Problems''], Educational Studies in Mathematics, Vol. 42, No. 2 (2000), pp. 211–213]&lt;/ref&gt;
&lt;/blockquote&gt;

Many children in elementary school, from different parts of the world, attempt to  "solve" this nonsensical problem by giving the answer 36, obtained by adding the numbers 26 and 10.&lt;ref name=decisions&gt;Presh Talwalkar[https://mindyourdecisions.com/blog/2018/02/08/the-real-answer-to-the-viral-chinese-math-problem-how-old-is-the-captain-stumping-the-internet/ “The REAL Answer To The Viral Chinese Math Problem ‘How Old Is The Captain?’ Stumping The Internet”,] ''Mind Your Decisions'' (MindYourDecisions.com) February 8, 2018. ''(Retrieved 2018-07-21.)''&lt;/ref&gt; It has been suggested that this indicates schooling and education fail to teach children critical thinking, and that a question may be unsolvable.&lt;ref name=decisions/&gt; But, others have countered that in education students are taught that all questions have a solution and that giving any answer is better than leaving it blank, hence the attempt to "solve" it.&lt;ref name=decisions/&gt;

==References==
{{Portal|Mathematics}}
{{reflist}}

{{DEFAULTSORT:Age Of The Captain}}
[[Category:Recreational mathematics]]
{{math-stub}}</text>
      <sha1>o51skf7c79j3xvzuj41l4d8ywammpw8</sha1>
    </revision>
  </page>
  <page>
    <title>Biconditional elimination</title>
    <ns>0</ns>
    <id>4287</id>
    <revision>
      <id>857234448</id>
      <parentid>637933553</parentid>
      <timestamp>2018-08-30T12:14:45Z</timestamp>
      <contributor>
        <username>RussBot</username>
        <id>279219</id>
      </contributor>
      <minor/>
      <comment>Robot: fix [[WP:DPL|links]] to [[WP:D|disambiguation]] page [[Validity]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2254">{{Transformation rules}}

'''Biconditional elimination''' is the name of two [[Validity (logic)|valid]] [[rule of inference|rules of inference]] of [[propositional calculus|propositional logic]]. It allows for one to [[inference|infer]] a [[Material conditional|conditional]] from a [[Logical biconditional|biconditional]]. If &lt;math&gt;(P \leftrightarrow Q)&lt;/math&gt; is true, then one may infer that &lt;math&gt;(P \to Q)&lt;/math&gt; is true, and also that &lt;math&gt;(Q \to P)&lt;/math&gt; is true.&lt;ref name=Cohen2007&gt;{{cite web|last=Cohen|first=S. Marc|title=Chapter 8: The Logic of Conditionals|url=http://faculty.washington.edu/smcohen/120/Chapter8.pdf|publisher=University of Washington|accessdate=8 October 2013}}&lt;/ref&gt; For example, if it's true that I'm breathing [[if and only if]] I'm alive, then it's true that if I'm breathing, I'm alive; likewise, it's true that if I'm alive, I'm breathing. The rules can be stated formally as:

:&lt;math&gt;\frac{(P \leftrightarrow Q)}{\therefore (P \to Q)}&lt;/math&gt;
and
:&lt;math&gt;\frac{(P \leftrightarrow Q)}{\therefore (Q \to P)}&lt;/math&gt;

where the rule is that wherever an instance of "&lt;math&gt;(P \leftrightarrow Q)&lt;/math&gt;" appears on a line of a proof, either "&lt;math&gt;(P \to Q)&lt;/math&gt;" or "&lt;math&gt;(Q \to P)&lt;/math&gt;" can be placed on a subsequent line;

== Formal notation ==
The ''biconditional elimination'' rule may be written in [[sequent]] notation:
:&lt;math&gt;(P \leftrightarrow Q) \vdash (P \to Q)&lt;/math&gt;
and
:&lt;math&gt;(P \leftrightarrow Q) \vdash (Q \to P)&lt;/math&gt;

where &lt;math&gt;\vdash&lt;/math&gt; is a [[metalogic]]al symbol meaning that &lt;math&gt;(P \to Q)&lt;/math&gt;, in the first case, and &lt;math&gt;(Q \to P)&lt;/math&gt; in the other are [[logical consequence|syntactic consequences]] of &lt;math&gt;(P \leftrightarrow Q)&lt;/math&gt; in some [[formal system|logical system]];

or as the statement of a truth-functional [[Tautology (logic)|tautology]] or [[theorem]] of propositional logic:

:&lt;math&gt;(P \leftrightarrow Q) \to (P \to Q)&lt;/math&gt;
:&lt;math&gt;(P \leftrightarrow Q) \to (Q \to P)&lt;/math&gt;

where &lt;math&gt;P&lt;/math&gt;, and &lt;math&gt;Q&lt;/math&gt; are propositions expressed in some [[formal system]].

==See also==
* [[Logical biconditional]]

==References==
{{Reflist}}

{{DEFAULTSORT:Biconditional Elimination}}
[[Category:Rules of inference]]
[[Category:Theorems in propositional logic]]</text>
      <sha1>muujydfk204o76hupyfjjl7jyg2gztm</sha1>
    </revision>
  </page>
  <page>
    <title>Bondage number</title>
    <ns>0</ns>
    <id>43402512</id>
    <revision>
      <id>865865605</id>
      <parentid>850529675</parentid>
      <timestamp>2018-10-26T17:53:53Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1122">In mathematics, the '''bondage number''' of a nonempty [[Graph (discrete mathematics)|graph]] is the [[cardinality]] of the smallest set ''E'' of edges such that the [[domination number]] of the graph with the edges ''E'' removed is strictly greater than the domination number of the original graph.&lt;ref&gt;{{cite journal | doi = 10.1016/0012-365X(90)90348-L | title=The bondage number of a graph | journal=Discrete Mathematics | date=1990 | volume=86 | issue=1-3 | pages=47–57 | first=John Frederick | last=Fink}}&lt;/ref&gt;&lt;ref&gt;{{cite journal | doi = 10.1016/0012-365X(94)90111-2 | title=Bounds on the bondage number of a graph | journal=Discrete Mathematics | date=1994 | volume=128 | issue=1-3 | pages=173–177 | first=Bert L. | last=Hartnell}}&lt;/ref&gt;
The concept was introduced by Fink et. al.&lt;ref&gt;{{Cite journal | doi = 10.1155/2013/595210| title = On Bondage Numbers of Graphs: A Survey with Some Comments| journal = International Journal of Combinatorics| volume = 2013| issue = 1
| pages = 1| year = 2013| last1 = Xu | first1 = J. M. }}&lt;/ref&gt;
== References ==
{{reflist}}

{{combin-stub}}

[[Category:Graph invariants]]</text>
      <sha1>d3bn41sppxzyex2mrhoig59c6a55yqx</sha1>
    </revision>
  </page>
  <page>
    <title>Bondi–Metzner–Sachs group</title>
    <ns>0</ns>
    <id>52671280</id>
    <revision>
      <id>849585489</id>
      <parentid>835199707</parentid>
      <timestamp>2018-07-10T00:50:20Z</timestamp>
      <contributor>
        <username>Josve05a</username>
        <id>12023796</id>
      </contributor>
      <minor/>
      <comment>/* References */fixing [[WP:CHECKWIKI]] error #37 (no DEFAULTSORT for article with special character) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="865">{{Use dmy dates|date=October 2017}}
{{context|date=December 2016}}
The ordinary '''Bondi–Metzner–Sachs (BMS) group''' B is the common asymptotic [[symmetry group]] of all radiating, [[Asymptotically flat spacetime|asymptotically flat]], Lorentzian [[spacetime]]s. This means that B is currently the best candidate for the universal symmetry group of [[General Relativity]].&lt;ref&gt;{{cite web|url=http://iopscience.iop.org/article/10.1088/1742-6596/33/1/028/meta |title=Generalizations of the BMS group and results}}&lt;/ref&gt; It was originally proposed in 1962 by H. Bondi, M. G. van der Burg, and A. W. Metzner.&lt;ref&gt;{{cite web|url=https://arxiv.org/pdf/1403.5803.pdf |title=Notes on the BMS group in three dimensions: I. Induced representations}}&lt;/ref&gt;

==References==
{{reflist}}

{{DEFAULTSORT:Bondi-Metzner-Sachs group}}
[[Category:Symmetry]]


{{relativity-stub}}</text>
      <sha1>o658f12nuit3xgy6k496087eygmmhx5</sha1>
    </revision>
  </page>
  <page>
    <title>Cauchy–Euler operator</title>
    <ns>0</ns>
    <id>40900167</id>
    <revision>
      <id>871702270</id>
      <parentid>871702249</parentid>
      <timestamp>2018-12-02T22:13:11Z</timestamp>
      <contributor>
        <username>Dreamy Jazz</username>
        <id>27131311</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/Gjhchkjv|Gjhchkjv]] ([[User talk:Gjhchkjv|talk]]): unexplained content removal ([[WP:HG|HG]]) (3.4.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="794">In mathematics a '''Cauchy–Euler operator''' is a [[differential operator]] of the form &lt;math&gt;p(x)\cdot{d \over dx}&lt;/math&gt;&lt;ref&gt;{{cite book|last=Ross|first=Clay C|title=Differential Equations: An Introduction with Mathematica&amp;reg;|year=2004|publisher=Springer|isbn=9781441919410}}&lt;/ref&gt; for a polynomial&amp;nbsp;''p''.  It is named after [[Augustin-Louis Cauchy]] and [[Leonhard Euler]].  The simplest example is that in which ''p''(''x'')&amp;nbsp;=&amp;nbsp;''x'', which has eigenvalues ''n''&amp;nbsp;=&amp;nbsp;0,&amp;nbsp;1,&amp;nbsp;2,&amp;nbsp;3,&amp;nbsp;... and corresponding eigenfunctions ''x''&lt;sup&gt;''n''&lt;/sup&gt;.

== See also ==
* [[Cauchy–Euler equation]]
* [[Sturm–Liouville theory]]

== References ==
{{reflist}}

{{DEFAULTSORT:Cauchy-Euler operator}}
[[Category:Differential operators]]


{{mathanalysis-stub}}</text>
      <sha1>hj1iwhd3800b9fopszu78z7yxudu9h2</sha1>
    </revision>
  </page>
  <page>
    <title>Centrifugal governor</title>
    <ns>0</ns>
    <id>84130</id>
    <revision>
      <id>862076881</id>
      <parentid>859258876</parentid>
      <timestamp>2018-10-02T00:59:15Z</timestamp>
      <contributor>
        <ip>115.188.252.31</ip>
      </contributor>
      <comment>/* Natural selection */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9244">[[Image:centrifugal governor.png|right|thumb|Drawing of a centrifugal "flyball" governor]]
A '''centrifugal governor''' is a specific type of [[governor (device)|governor]] with a feedback system that controls the [[speed]] of an [[engine]] by regulating the amount of [[fuel]] (or [[working fluid]]) admitted, so as to maintain a near-constant speed, irrespective of the load or fuel-supply conditions. It uses the principle of [[proportional control]].

Centrifugal governors were invented by [[Christiaan Huygens]] and used to regulate the distance and pressure between [[millstone]]s in [[windmill]]s in the 17th century.&lt;ref&gt;{{citation|last=Hills|first=Richard L|title=Power From the Wind|publisher=Cambridge University Press|year=1996}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=https://books.google.com/books?id=iwbWCgAAQBAJ&amp;pg=PA36&amp;dq=%22Centrifugal+Governor%22+Huygens&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjq8cC3uaHPAhWCmBoKHR2OCqwQ6AEIPjAH#v=onepage&amp;q=%22Centrifugal+Governor%22+Huygens&amp;f=false|title=Adaptive Control Processes: A Guided Tour|first=Richard E.|last=Bellman|date=8 December 2015|publisher=Princeton University Press|accessdate=13 April 2018|via=Google Books}}&lt;/ref&gt; In 1788, [[James Watt]] adapted one to control his [[steam engine]] where it regulates the admission of steam into the [[cylinder (engine)|cylinder]](s)&lt;ref&gt;[http://mi.eng.cam.ac.uk/IALego/steam.html University of Cambridge: Steam engines and control theory]&lt;/ref&gt;, a development that proved so important he is sometimes called the inventor. Centrifugal governors' widest use was on steam engines during the [[Steam power during the Industrial Revolution|Steam Age]] in the 19th century. They are also found on [[Stationary engine|stationary]] [[internal combustion engine]]s and variously fueled [[turbine]]s, and in some modern [[striking clock]]s.

A simple governor does not maintain an exact speed but a speed range, since under increasing load the governor opens the throttle as the speed (RPM) decreases.

==Operation==
[[File:Centrifugal governor and balanced steam valve (New Catechism of the Steam Engine, 1904).jpg|thumb| Cut-away drawing of steam engine speed governor. The valve starts fully open at zero speed, but as the balls rotate and rise, the central [[valve stem]] is forced downward and closes the valve. The drive shaft whose speed is being sensed is top right]]
[[File:Ashton Frost engine governor.jpg|thumb|Porter governor on a Corliss steam engine]]
The devices shown are on steam engines. Power is supplied to the governor from the engine's output shaft by a belt or chain connected to the lower belt wheel. The governor is connected to a [[throttle]] valve that regulates the flow of [[working fluid]] (steam) supplying the [[Prime mover (locomotive)|prime mover]].  As the speed of the prime mover increases, the central spindle of the governor rotates at a faster rate and the kinetic energy of the balls increases.  This allows the two [[mass]]es on lever arms to move outwards and upwards against gravity. If the motion goes far enough, this motion causes the lever arms to pull down on a [[thrust bearing]], which moves a beam linkage, which reduces the [[aperture]] of a throttle valve. The rate of working-fluid entering the cylinder is thus reduced and the speed of the prime mover is controlled, preventing over-speeding.

Mechanical stops may be used to limit the range of throttle motion, as seen near the masses in the image at right.

=== Non-gravitational regulation ===
A limitation of the two-arm, two-ball governor is its reliance on gravity, and that the governor must stay upright relative to the surface of the Earth for gravity to retract the balls when the governor slows down.

Governors can be built that do not use gravitational force, by using a single straight arm with weights on both ends, a center pivot attached to a spinning axle, and a spring that tries to force the weights towards the center of the spinning axle. The two weights on opposite ends of the pivot arm counterbalance any gravitational effects, but both weights use centrifugal force to work against the spring and attempt to rotate the pivot arm towards a perpendicular axis relative to the spinning axle.

Spring-retracted non-gravitational governors are commonly used in [[Single-phase electric power|single-phase]] [[alternating current]] (AC) [[induction motor]]s to turn off the starting [[field coil]] when the motor's rotational speed is high enough.

They are also commonly used in [[snowmobile]] and [[all-terrain vehicle]] (ATV) [[continuously variable transmission]]s (CVT), both to engage/disengage vehicle motion and to vary the transmission's pulley diameter ratio in relation to the engine [[revolutions per minute]].

==History==
[[File:Boulton and Watt centrifugal governor-MJ.jpg|thumb|right|Boulton &amp; Watt engine of 1788]]
James Watt designed his first governor in 1788 following a suggestion from his business partner [[Matthew Boulton]]. It was a [[conical pendulum]] governor and one of the final series of innovations Watt had employed for steam engines. James Watt never claimed the centrifugal governor to be an invention of his own. A giant statue of Watt's governor stands at [[Smethwick]] in the [[England|English]] [[West Midlands (county)|West Midlands]]. It is known as the flyball governor.

Centrifugal governors are also used in many modern [[Repeater (horology)|repeating watches]] to limit the speed of the [[Striking clock#Parts of mechanism|striking train]], so the repeater doesn't run too quickly.

Another kind of centrifugal governor consists of a pair of masses on a spindle inside a cylinder, the masses or the cylinder being coated with pads, somewhat like a [[drum brake]]. This is used in a spring-loaded [[record player]] and a spring-loaded [[telephone]] dial to limit the speed.

==Dynamic systems==
The centrifugal governor is often used in the cognitive sciences as an example of a [[dynamic system]], in which the representation of information cannot be clearly separated from the operations being applied to the representation. And, because the governor is a [[servomechanism]], its analysis in a dynamic system is not trivial.  In 1868, [[James Clerk Maxwell]] wrote a famous paper "On governors"&lt;ref&gt;{{cite journal|last=Maxwell|first=James Clerk|title=On Governors|journal=Proceedings of the Royal Society of London|volume= 16|year= 1868 |pages= 270–283 | doi = 10.1098/rspl.1867.0055 | jstor=112510}}&lt;/ref&gt; that is widely considered a classic in feedback [[control theory]]. Maxwell distinguishes moderators (a centrifugal [[brake]]) and governors which control [[motive power]] input. He considers devices by [[James Watt]], Professor [[James Thomson (engineer)|James Thomson]], [[Fleeming Jenkin]], [[William Thomson, 1st Baron Kelvin|William Thomson]], [[Léon Foucault]] and [[Carl Wilhelm Siemens]] (a liquid governor).

=== Natural selection ===
In his famous 1858 paper to the [[Linnean Society of London|Linnean Society]], which led [[Charles Darwin|Darwin]] to publish [[On the Origin of Species]], [[Alfred Russel Wallace]] used governors as a metaphor for the [[Evolution|evolutionary principle]]:

&lt;blockquote&gt;The action of this principle is exactly like that of the '''centrifugal governor''' of the steam engine, which checks and corrects any irregularities almost before they become evident; and in like manner no unbalanced deficiency in the animal kingdom can ever reach any conspicuous magnitude, because it would make itself felt at the very first step, by rendering existence difficult and extinction almost sure soon to follow.&lt;ref&gt;{{cite web|last=Wallace|first=Alfred Russel|title=On the Tendency of Varieties to Depart Indefinitely From the Original Type|url=http://www.wku.edu/~smithch/wallace/S043.htm|accessdate=2009-04-18}}&lt;/ref&gt;&lt;/blockquote&gt;

Bateson revisited the topic in his 1979 book ''Mind and Nature: A Necessary Unity'', and other scholars have continued to explore the connection between natural selection and [[systems theory]].&lt;ref name="Unfinished Business"&gt;{{cite web|last=Smith|first=Charles H.|title=Wallace's Unfinished Business|url=http://www.wku.edu/~smithch/essays/UNFIN.htm|publisher=Complexity (publisher Wiley Periodicals, Inc.) Volume 10, No 2, 2004|accessdate=2007-05-11}}&lt;/ref&gt;

== Culture ==
A centrifugal governor is part of the city seal of [[Manchester, New Hampshire]] in the U.S. and is also used on the city flag. A 2017 effort to change the design was rejected by voters. &lt;ref&gt;[http://www.concordmonitor.com/centrigual-governor-manchester-city-flag-13566970 "Voters can keep a cool Industrial Revolution invention on Manchester’s flag" Concord Monitor Nov. 2017.]&lt;/ref&gt;

==See also==
{{Commons category|Centrifugal governors|&lt;br /&gt;Centrifugal governors}}
* [[Cataract (beam engine)]]
* [[Hit and miss engine]]

==References==
{{Reflist}}

{{Steam engine configurations}}
{{Christiaan Huygens}}

{{DEFAULTSORT:Centrifugal Governor}}
[[Category:Steam engine governors]]
[[Category:Mechanisms (engineering)]]
[[Category:Control devices]]
[[Category:Cybernetics]]
[[Category:Mechanical power control]]
[[Category:Rotating machines]]
[[Category:Scottish inventions]]
[[Category:British inventions]]
[[Category:Inventions by Christiaan Huygens]]</text>
      <sha1>ci8f5zfmgzpoyr124sosjg6w5d1alfh</sha1>
    </revision>
  </page>
  <page>
    <title>Chuvash numerals</title>
    <ns>0</ns>
    <id>2310842</id>
    <revision>
      <id>855907252</id>
      <parentid>855244734</parentid>
      <timestamp>2018-08-21T16:42:34Z</timestamp>
      <contributor>
        <username>Highpeaks35</username>
        <id>32647982</id>
      </contributor>
      <comment>POV</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1645">{{Refimprove|date=August 2007}}
[[Image:Ch zh.jpg|frame|right|Chuvash [[numeral system|numeral]]s.]]
{{Numeral systems}}
'''Chuvash numerals''' is an ancient [[numeral system]] from the [[Old Turkic alphabet|Old Turkic script]] the [[Chuvash people]] used. (Modern Chuvash use [[Hindu-Arabic numerals]].)

Those numerals originate from ''finger numeration''. They look like [[Roman numerals]], but larger numerals stay at the right side. It was possible to carve those numerals on wood. In some cases numerals were preserved until the beginning of the 20th century.&lt;ref&gt;{{Cite web|url=http://festival.1september.ru/articles/649788/|title=Интегрированный урок на тему "Чувашские числовые знаки". 5-й класс|last=сентября»|first=Федотова Людмила Аркадьевна, Флегентова Апполинария Алексеевна, Издательский дом «Первое|website=festival.1september.ru|access-date=2016-06-17}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=http://nikiforova.21416s15.edusite.ru/DswMedia/17drevnyayachuvashskayapis-mennost-.pps|title=Chuvashskaya pismennost|last=Nikiforova|first=Ludmila|date=|website=|publisher=|access-date=}}&lt;/ref&gt;

{| class="wikitable"
! Numeral !! Chuvash numeral
|-
| 1 || I
|-
| 5 || /
|-
| 10 || X
|-
| 50 || (upside down) 𐠂
|-
| 100 || 𐠀
|-
| 500 || 𐠁
|-
| 1000 || ✳
|}

== Examples ==

{| class="wikitable"
! Hindu-Arabic !! Chuvash
|-
| 2 || II
|-
| 4 || IIII
|-
| 6 || I/
|-
| 19 || IIII/X 
|-
| 32 || IIXXX 
|-
| 47 || II/XXXX
|}

==References==
{{Reflist}}

[[Category:Numerals]]
[[Category:Numeral systems]]</text>
      <sha1>9uqu3mtgka5ay6b7xm7mtcq2fs9y11k</sha1>
    </revision>
  </page>
  <page>
    <title>Compression theorem</title>
    <ns>0</ns>
    <id>2817175</id>
    <revision>
      <id>761706923</id>
      <parentid>760457343</parentid>
      <timestamp>2017-01-24T10:23:24Z</timestamp>
      <contributor>
        <username>Denisarona</username>
        <id>4770293</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contribs/24.85.232.10|24.85.232.10]] ([[User talk:24.85.232.10|talk]]) to last version by Bender the Bot</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1734">In [[computational complexity theory]] the '''compression theorem''' is an important theorem about the complexity of [[computable function]]s. 

The theorem states that there exists no largest [[complexity class]], with computable boundary, which contains all computable functions.

==Compression theorem==
Given a [[Gödel numbering]] &lt;math&gt;\varphi&lt;/math&gt; of the computable functions and a [[Blum complexity measure]] &lt;math&gt;\Phi&lt;/math&gt; where a complexity class for a boundary function &lt;math&gt;f&lt;/math&gt; is defined as
:&lt;math&gt;\mathrm{C}(f):= \{\varphi_i \in \mathbf{R}^{(1)} | (\forall^\infty x) \, \Phi_i (x) \leq f(x) \}.&lt;/math&gt;

Then there exists a [[total computable function]] &lt;math&gt;f&lt;/math&gt; so that for all &lt;math&gt;i&lt;/math&gt;
:&lt;math&gt;\mathrm{Dom}(\varphi_i) = \mathrm{Dom}(\varphi_{f(i)})&lt;/math&gt;
and
:&lt;math&gt;\mathrm{C}(\varphi_i) \subsetneq \mathrm{C}(\varphi_{f(i)}).&lt;/math&gt;

==References==
*{{citation|title=Computation and Automata|volume=25|series=Encyclopedia of Mathematics and Its Applications|first=Arto|last=Salomaa|authorlink=Arto Salomaa|publisher=Cambridge University Press|year=1985|isbn=9780521302456|contribution=Theorem 6.9|pages=149–150|url=https://books.google.com/books?id=IblDi626fBAC&amp;pg=PA149}}.
*{{citation|title=Computational Complexity: A Quantitative Perspective|volume=196|series=North-Holland Mathematics Studies|first=Marius|last=Zimand|publisher=Elsevier|year=2004|isbn=9780444828415|contribution=Theorem 2.4.3 (Compression theorem)|page=42|url=https://books.google.com/books?id=j-nhMYoZhgYC&amp;pg=PA42}}.

{{DEFAULTSORT:Compression Theorem}}
[[Category:Computational complexity theory]]
[[Category:Structural complexity theory]]
[[Category:Theorems in the foundations of mathematics]]
{{Comp-sci-theory-stub}}</text>
      <sha1>88vxb31jsq5drpn6rxk34ffape1xybe</sha1>
    </revision>
  </page>
  <page>
    <title>Cryptography newsgroups</title>
    <ns>0</ns>
    <id>725557</id>
    <revision>
      <id>809379197</id>
      <parentid>809336717</parentid>
      <timestamp>2017-11-08T19:01:13Z</timestamp>
      <contributor>
        <username>Intgr</username>
        <id>246230</id>
      </contributor>
      <comment>Add 'unreferenced' tag</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2167">{{Unreferenced|date=November 2017}}

There are several [[newsgroup]]s relevant for discussions about [[cryptography]] and related issues.

* '''[news://sci.crypt sci.crypt]''' &amp;mdash; an unmoderated forum for discussions on technical aspects of [[cryptography]]. 
* '''[news://sci.crypt.research sci.crypt.research]''' &amp;mdash; a similar, [[Usenet_newsgroup#Moderated_newsgroups|moderated]] group, focusing on research into cryptography. It was founded based on a charter by [[Peter Gutmann (computer scientist)|Peter Gutmann]].
* '''[news://sci.crypt.random-numbers sci.crypt.random-numbers]''' &amp;mdash; discuss [[Cryptographically secure pseudorandom number generator|generation of cryptographically secure random number]]s.
* '''[news://talk.politics.crypto talk.politics.crypto]''' &amp;mdash; discussions of the relationship between cryptography and government. The original charter was by D.J. Silverton.
* '''[news://alt.security.pgp alt.security.pgp]''' &amp;mdash; discussion of [[Pretty Good Privacy]] (PGP) and related software.

==sci.crypt==
In 1995, [[Bruce Schneier]] commented, "It is read by an estimated 100,000 people worldwide. Most of the posts are nonsense, bickering, or both; some are political, and most of the rest are requests for information or basic questions. Occasionally nuggets of new and useful information are posted to this newsgroup." (''[[Practical_Cryptography#Less_mathematical|Applied Cryptography]]'', 2nd ed, pages 608-609).

Leaked descriptions of secret algorithms have been posted to the Internet via sci.crypt, for example [[RC2]], [[RC4 (cipher)|RC4]]  and [[Khufu and Khafre]]. Others have been hoaxes: [[Iraqi block cipher]] and [[S-1 block cipher|S-1]], the latter an alleged description of the (then-secret) [[Skipjack (cipher)|Skipjack]] cipher. The group is also the origin of the term, "[[Rubber-hose cryptanalysis]]".

==External links==
* [http://www.alt-security-keydist.info/newsgroups Newsgroups for cryptography]
* [http://www.faqs.org/faqs/cryptography-faq/ sci.crypt Frequently Asked Questions]
{{Usenetnav}}
[[Category:Cryptography journals]]
[[Category:Sci.* hierarchy]]



{{crypto-stub}}
{{compu-journal-stub}}</text>
      <sha1>hri4ej5scxprfsvahcfvjb8uu2obvaw</sha1>
    </revision>
  </page>
  <page>
    <title>De Finetti's theorem</title>
    <ns>0</ns>
    <id>180835</id>
    <revision>
      <id>869577122</id>
      <parentid>868622659</parentid>
      <timestamp>2018-11-19T13:58:51Z</timestamp>
      <contributor>
        <username>Nejimban</username>
        <id>11290629</id>
      </contributor>
      <minor/>
      <comment>/* Statement of the theorem */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11616">{{lowercase}}
In [[probability theory]], '''de Finetti's theorem''' states that [[exchangeable random variables|exchangeable]] observations are [[conditionally independent]] relative to some [[latent variable]]. An  [[epistemic probability]] [[probability distribution|distribution]] could then be assigned to this variable. It is named in honor of [[Bruno de Finetti]].

For the special case of an exchangeable sequence of [[Bernoulli distribution|Bernoulli]] random variables it states that such a sequence is a "mixture" of sequences of [[independent and identically distributed]] (i.i.d.) Bernoulli random variables. 

While the variables of the exchangeable sequence are not ''themselves'' independent, only exchangeable, there is an ''underlying'' family of i.i.d. random variables. That is, there are underlying, generally unobservable, quantities that are i.i.d. – exchangeable sequences are mixtures of i.i.d. sequences.

== Background ==
A Bayesian statistician often seeks the conditional probability distribution of a random quantity given the data. The concept of [[exchangeability]] was introduced by de Finetti.  De Finetti's theorem explains a mathematical relationship between independence and exchangeability.&lt;ref&gt;See the Oxford lecture notes of Steffen Lauritzen  http://www.stats.ox.ac.uk/~steffen/teaching/grad/definetti.pdf&lt;/ref&gt;

An infinite sequence

:&lt;math&gt;X_1, X_2, X_3, \dots &lt;/math&gt;

of random variables is said to be exchangeable if for any finite [[cardinal number]] ''n'' and any two finite sequences ''i''&lt;sub&gt;1&lt;/sub&gt;, ..., ''i''&lt;sub&gt;''n''&lt;/sub&gt; and ''j''&lt;sub&gt;1&lt;/sub&gt;, ..., ''j''&lt;sub&gt;''n''&lt;/sub&gt; (with each of the ''i''s distinct, and each of the ''j''s distinct), the two sequences

:&lt;math&gt;X_{i_1},\dots,X_{i_n} \text{ and } X_{j_1},\dots,X_{j_n} &lt;/math&gt;

both have the same joint probability distribution.

If an identically distributed sequence is [[statistical independence|independent]], then the sequence is exchangeable; however, the converse is false—there exist exchangeable random variables that are not statistically independent, for example the [[Polya urn model]].

== Statement of the theorem ==

A [[random variable]] ''X'' has a [[Bernoulli distribution]] if Pr(''X''&amp;nbsp;=&amp;nbsp;1)&amp;nbsp;=&amp;nbsp;''p'' and Pr(''X''&amp;nbsp;=&amp;nbsp;0)&amp;nbsp;=&amp;nbsp;1&amp;nbsp;&amp;minus;&amp;nbsp;''p'' for some ''p''&amp;nbsp;∈&amp;nbsp;(0,&amp;nbsp;1).

De Finetti's theorem states that the probability distribution of any infinite exchangeable sequence of Bernoulli random variables is a "mixture" of the probability distributions of independent and identically distributed sequences of Bernoulli random variables.  "Mixture", in this sense, means a weighted average, but this need not mean a finite or countably infinite (i.e., discrete) weighted average: it can be an integral rather than a sum.

More precisely, suppose ''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;, ''X''&lt;sub&gt;3&lt;/sub&gt;, ... is an infinite exchangeable sequence of Bernoulli-distributed random variables.  Then there is some probability distribution ''m'' on the interval [0,&amp;nbsp;1] and some random variable ''Y'' such that
* The probability distribution of ''Y'' is ''m'', and
* The [[conditional probability distribution]] of the whole sequence ''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;, ''X''&lt;sub&gt;3&lt;/sub&gt;, ... given the value of ''Y'' is described by saying that
** ''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;, ''X''&lt;sub&gt;3&lt;/sub&gt;, ... are [[conditional independence|conditionally independent]] given ''Y'', and
** For any ''i'' ∈ {1, 2, 3, ...}, the conditional probability that ''X''&lt;sub&gt;''i''&lt;/sub&gt; = 1, given the value of ''Y'', is ''Y''.

=== Another way of stating the theorem ===

Suppose &lt;math&gt;X_1,X_2,X_3,\ldots&lt;/math&gt; is an infinite exchangeable sequence of Bernoulli random variables.  Then &lt;math&gt;X_1,X_2,X_3,\ldots&lt;/math&gt; are conditionally independent and identically distributed given the [[exchangeable sigma-algebra]] (i.e., the sigma-algebra of events measurable with respect to &lt;math&gt;X_1,X_2,\ldots&lt;/math&gt; and invariant under finite permutations of the indices).

== Example ==

Here is a concrete example. We construct a sequence

:&lt;math&gt;X_1, X_2, X_3, \dots &lt;/math&gt;

of random variables, by "mixing" two i.i.d. sequences as follows.

We assume ''p'' = 2/3 with probability 1/2 and ''p'' = 9/10 with probability 1/2. Given the event ''p'' = 2/3, the conditional distribution of the sequence is that the ''X''&lt;sub&gt;i&lt;/sub&gt; are independent and identically distributed and ''X''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;1 with probability 2/3 and ''X''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;0 with probability 1&amp;nbsp;&amp;minus;&amp;nbsp;2/3.  Given the event  ''p''&amp;nbsp;=&amp;nbsp;9/10, the conditional distribution of the sequence is that the ''X''&lt;sub&gt;i&lt;/sub&gt; are independent and identically distributed and ''X''&lt;sub&gt;1&lt;/sub&gt; = 1 with probability 9/10 and ''X''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;0 with probability 1&amp;nbsp;&amp;minus;&amp;nbsp;9/10.  

The independence asserted here is ''conditional'' independence, i.e. the Bernoulli random variables in the sequence are conditionally independent given the event that ''p''&amp;nbsp;=&amp;nbsp;2/3, and are conditionally independent given the event that ''p''&amp;nbsp;=&amp;nbsp;9/10. But they are not unconditionally independent; they are positively [[correlation|correlated]]. 

In view of the [[law of large numbers|strong law of large numbers]], we can say that

:&lt;math&gt;\lim_{n\rightarrow\infty} \frac{X_1+\cdots+X_n}{n} = \begin{cases}
2/3 &amp; \text{with probability }1/2, \\
9/10 &amp; \text{with probability }1/2.
\end{cases} &lt;/math&gt;

Rather than concentrating probability 1/2 at each of two points between 0 and 1, the "mixing distribution" can be any [[probability distribution]] supported on the interval from 0 to 1; which one it is depends on the joint distribution of the infinite sequence of Bernoulli random variables.

The definition of exchangeability, and the statement of the theorem, also makes sense for finite length sequences

:&lt;math&gt;X_1,\dots, X_n,&lt;/math&gt;

but the theorem is not generally true in that case.  It is true if the sequence can be extended to an exchangeable sequence that is infinitely long.  The simplest example of an exchangeable sequence of Bernoulli random variables that cannot be so extended is the one in which ''X''&lt;sub&gt;1&lt;/sub&gt; = 1&amp;nbsp;&amp;minus;&amp;nbsp;''X''&lt;sub&gt;2&lt;/sub&gt; and ''X''&lt;sub&gt;1&lt;/sub&gt; is either 0 or 1, each with probability 1/2.  This sequence is exchangeable, but cannot be extended to an exchangeable sequence of length 3, let alone an infinitely long one.

== Extensions ==
Versions of de&amp;nbsp;Finetti's theorem for  ''finite'' exchangeable sequences,&lt;ref&gt;{{cite journal
 |first=P. |last=Diaconis |authorlink=Persi Diaconis
 |first2=D. |last2=Freedman |authorlink2=David A. Freedman (statistician)
 |title=Finite exchangeable sequences
 |journal=Annals of Probability
 |volume=8 |issue=4 |year=1980 |pages=745&amp;ndash;764
 |doi=10.1214/aop/1176994663 |mr=577313 | zbl = 0434.60034
}}&lt;/ref&gt; 
&lt;ref&gt;{{cite journal
 |first=G.&amp;nbsp;J. |last=Szekely |authorlink=Gabor J Szekely
 |first2= J.&amp;nbsp;G.  |last2=Kerns  |title=De&amp;nbsp;Finetti’s theorem for abstract finite exchangeable sequences |journal=Journal of Theoretical Probability |volume=19 |issue=3 |year=2006 |pages= 589–608 |doi=10.1007/s10959-006-0028-z}}
&lt;/ref&gt; and for ''Markov&amp;nbsp;exchangeable'' sequences&lt;ref&gt;{{cite journal
 |first=P. |last=Diaconis |authorlink=Persi Diaconis
 |first2=D. |last2=Freedman |authorlink2=David A. Freedman (statistician)
 |title=De&amp;nbsp;Finetti's theorem for Markov chains
 |journal=Annals of Probability
 |volume=8 |issue=1 |year=1980 |pages=115&amp;ndash;130
 |doi=10.1214/aop/1176994828 |mr=556418| zbl=0426.60064
}}&lt;/ref&gt; have been proved by Diaconis and Freedman and by Kerns and Szekely. 
Two notions of partial exchangeability of arrays, known as ''separate'' and ''joint exchangeability'' lead to extensions of de&amp;nbsp;Finetti's theorem for arrays by Aldous and Hoover.&lt;ref&gt;Persi Diaconis and [[Svante Janson]] (2008) [http://www.stat.berkeley.edu/~aldous/Research/persi-svante.pdf "Graph Limits and Exchangeable Random Graphs"],''Rendiconti di&amp;nbsp;Matematica'', Ser. VII 28(1), 33–61.&lt;/ref&gt;

The computable de&amp;nbsp;Finetti theorem shows that if an exchangeable sequence of real random variables is given by a computer program, then a program which samples from the mixing measure can be automatically recovered.&lt;ref&gt;
Cameron Freer and Daniel Roy (2009) [http://www.springerlink.com/index/g62r5424q2k2l304.pdf "Computable exchangeable sequences have computable de&amp;nbsp;Finetti measures"],  ''Proceedings of the 5th Conference on Computability in Europe: Mathematical Theory and Computational Practice'', Lecture Notes in Computer Science, Vol. 5635, pp. 218&amp;ndash;231.&lt;/ref&gt;

In the setting of [[free probability]], there is a noncommutative extension of de Finetti's theorem which characterizes noncommutative sequences invariant under quantum permutations.&lt;ref&gt;
{{cite journal |first=Claus |last=Koestler |first2=Roland |last2=Speicher |year=2009 |url=http://www.springerlink.com/index/a0457504u214177k.pdf |title=A noncommutative de&amp;nbsp;Finetti theorem: Invariance under quantum permutations is equivalent to freeness with amalgamation |journal=Commun. Math. Phys. |volume=291 |issue= 2|pages=473&amp;ndash;490 |doi= 10.1007/s00220-009-0802-8|bibcode=2009CMaPh.291..473K |arxiv=0807.0677 }}
&lt;/ref&gt;

Extensions of de Finetti's theorem to quantum states have been found to be useful in [[quantum information]],&lt;ref&gt;{{Cite journal|last=Caves|first=Carlton M.|last2=Fuchs|first2=Christopher A.|last3=Schack|first3=Ruediger|date=2002-08-20|year=|title=Unknown quantum states: The quantum de Finetti representation|url=http://aip.scitation.org/doi/10.1063/1.1494475|journal=Journal of Mathematical Physics|volume=43|issue=9|pages=4537–4559|arxiv=quant-ph/0104088|doi=10.1063/1.1494475|issn=0022-2488|via=|bibcode=2002JMP....43.4537C}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://math.ucr.edu/home/baez/week251.html|title=This Week's Finds in Mathematical Physics (Week 251)|year=2007|author=[[John C. Baez|J. Baez]]|accessdate=29 April 2012}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Brandao|first=Fernando G.S.L.|last2=Harrow|first2=Aram W.|date=2013-01-01|title=Quantum De Finetti Theorems Under Local Measurements with Applications|url=http://doi.acm.org/10.1145/2488608.2488718|journal=Proceedings of the Forty-fifth Annual ACM Symposium on Theory of Computing|series=STOC '13|location=New York, NY, USA|publisher=ACM|volume=|pages=861–870|arxiv=1210.6367|doi=10.1145/2488608.2488718|isbn=9781450320290}}&lt;/ref&gt; in topics like [[quantum key distribution]]&lt;ref&gt;{{cite arxiv|last=Renner|first=Renato|date=2005-12-30|title=Security of Quantum Key Distribution|eprint=quant-ph/0512258}}&lt;/ref&gt; and [[quantum entanglement|entanglement]] detection.&lt;ref&gt;{{Cite journal|last=Doherty|first=Andrew C.|last2=Parrilo|first2=Pablo A.|last3=Spedalieri|first3=Federico M.|date=2005-01-01|year=|title=Detecting multipartite entanglement|url=http://link.aps.org/doi/10.1103/PhysRevA.71.032333|journal=Physical Review A|volume=71|issue=3|pages=032333|arxiv=quant-ph/0407143|doi=10.1103/PhysRevA.71.032333|via=|bibcode=2005PhRvA..71c2333D}}&lt;/ref&gt;

==See also==
* [[Choquet theory]]
* [[Hewitt–Savage zero–one law]]
* [[Krein–Milman theorem]]

== References ==

&lt;references/&gt;

==External links==
*{{SpringerEOM|id=De_Finetti_theorem|first=L.|last= Accardi|title=De Finetti theorem}}
*[http://stats.stackexchange.com/questions/34465/what-is-so-cool-about-de-finettis-representation-theorem What is so cool about De Finetti's representation theorem?]

[[Category:Probability theorems]]
[[Category:Bayesian statistics]]
[[Category:Integral representations]]</text>
      <sha1>e9e0amrapvhuhjx3pakmjib7jx9uq4r</sha1>
    </revision>
  </page>
  <page>
    <title>Field with one element</title>
    <ns>0</ns>
    <id>13798040</id>
    <revision>
      <id>870170298</id>
      <parentid>866705077</parentid>
      <timestamp>2018-11-22T22:24:56Z</timestamp>
      <contributor>
        <ip>178.220.112.229</ip>
      </contributor>
      <comment>/* History */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="21114">{{Use dmy dates|date=July 2013}}
In [[mathematics]], the '''field with one element''' is a suggestive name for an object that should behave similarly to a [[finite field]] with a single element, if such a field could exist. This object is denoted '''F'''&lt;sub&gt;1&lt;/sub&gt;, or, in a French–English pun, '''F'''&lt;sub&gt;un&lt;/sub&gt;.&lt;ref&gt;"[[wikt:un#French|un]]" is French for "one", and [[wikt:fun|fun]] is a playful English word. For examples of this notation, see, e.g. {{harvtxt|Le Bruyn|2009}}, or the links by Le Bruyn, Connes, and Consani.&lt;/ref&gt; The name "field with one element" and the notation '''F'''&lt;sub&gt;1&lt;/sub&gt; are only suggestive, as there is no field with one element in classical [[abstract algebra]]. Instead, '''F'''&lt;sub&gt;1&lt;/sub&gt; refers to the idea that there should be a way to replace sets and operations, the traditional building blocks for abstract algebra, with other, more flexible objects. While there is still no field with a single element in these theories, there is a field-like object whose [[characteristic (algebra)|characteristic]] is one.

'''F'''&lt;sub&gt;1&lt;/sub&gt; cannot be a field because all fields must contain two distinct elements, the [[additive identity]] zero and the [[multiplicative identity]] one. Even if this restriction is dropped, a ring with one element must be the [[zero ring]], which does not behave like a finite field. Instead, most proposed theories of '''F'''&lt;sub&gt;1&lt;/sub&gt; replace abstract algebra entirely. Mathematical objects such as [[vector space]]s and [[polynomial ring]]s can be carried over into these new theories by mimicking their abstract properties. This allows the development of [[commutative algebra]] and [[algebraic geometry]] on new foundations. One of the defining features of theories of '''F'''&lt;sub&gt;1&lt;/sub&gt; is that these new foundations allow more objects than classical abstract algebra, one of which behaves like a field of characteristic one.

The possibility of studying the mathematics of '''F'''&lt;sub&gt;1&lt;/sub&gt; was originally suggested in 1956 by [[Jacques Tits]], published in {{harvnb|Tits|1957}}, on the basis of an analogy between symmetries in [[projective geometry]] and the combinatorics of [[simplicial complex]]es. '''F'''&lt;sub&gt;1&lt;/sub&gt; has been connected to [[noncommutative geometry]] and to a possible proof of the [[Riemann hypothesis]]. Many theories of '''F'''&lt;sub&gt;1&lt;/sub&gt; have been proposed, but it is not clear which, if any, of them give '''F'''&lt;sub&gt;1&lt;/sub&gt; all the desired properties.

==History==
In 1957, Jacques Tits introduced the theory of [[building (mathematics)|buildings]], which relate [[algebraic group]]s to [[abstract simplicial complex]]es. One of the assumptions is a non-triviality condition: If the building is an ''n''-dimensional abstract simplicial complex, and if {{nowrap|''k'' &lt; ''n''}}, then every ''k''-simplex of the building must be contained in at least three ''n''-simplices. This is analogous to the condition in classical [[projective geometry]] that a line must contain at least three points. However, there are [[Degeneracy (mathematics)|degenerate]] geometries which satisfy all the conditions to be a projective geometry except that the lines admit only two points. The analogous objects in the theory of buildings are called apartments. Apartments play such a constituent role in the theory of buildings that Tits conjectured the existence of a theory of projective geometry in which the degenerate geometries would have equal standing with the classical ones. This geometry would take place, he said, over a ''field of characteristic one''.&lt;ref&gt;{{harvtxt|Tits|1957}}.&lt;/ref&gt; Using this analogy it was possible to describe some of the elementary properties of '''F'''&lt;sub&gt;1&lt;/sub&gt;, but it was not possible to construct it.

A separate inspiration for '''F'''&lt;sub&gt;1&lt;/sub&gt; came from [[algebraic number theory]]. Weil's proof of the [[Riemann hypothesis for curves over finite fields]] started with a curve ''C'' over a finite field ''k'', took its product {{nowrap|''C'' ×&lt;sub&gt;''k''&lt;/sub&gt; ''C''}}, and then examined its diagonal. If the integers were a curve over a field, the same proof would prove the [[Riemann hypothesis]]. The integers '''Z''' are [[Krull dimension|one-dimensional]], which suggests that they may be a curve, but they are not an algebra over any field. One of the conjectured properties of '''F'''&lt;sub&gt;1&lt;/sub&gt; is that '''Z''' should be an '''F'''&lt;sub&gt;1&lt;/sub&gt;-algebra. This would make it possible to construct the product {{nowrap|'''Z''' ×&lt;sub&gt;'''F'''&lt;sub&gt;1&lt;/sub&gt;&lt;/sub&gt; '''Z'''}}, and it is hoped that the Riemann hypothesis for '''Z''' can be proved in the same way as the Riemann hypothesis for a curve over a finite field.

Another angle comes from [[Arakelov geometry]], where [[Diophantine equations]] are studied using tools from [[complex geometry]]. The theory involves complicated comparisons between finite fields and the complex numbers. Here the existence of '''F'''&lt;sub&gt;1&lt;/sub&gt; is useful for technical reasons.

By 1991, Alexander Smirnov had taken some steps towards algebraic geometry over '''F'''&lt;sub&gt;1&lt;/sub&gt;.&lt;ref&gt;{{harvtxt|Smirnov|1992}}&lt;/ref&gt; He introduced extensions of '''F'''&lt;sub&gt;1&lt;/sub&gt; and used them to handle the projective line '''P'''&lt;sup&gt;1&lt;/sup&gt; over '''F'''&lt;sub&gt;1&lt;/sub&gt;. [[Algebraic number]]s were treated as maps to this '''P'''&lt;sup&gt;1&lt;/sup&gt;, and conjectural approximations to [[Riemann–Hurwitz formula|the Riemann–Hurwitz formula]] for these maps were suggested. These approximations imply very profound assertions like [[abc conjecture|the abc conjecture]]. The extensions of '''F'''&lt;sub&gt;1&lt;/sub&gt; later on were denoted&lt;ref&gt;{{harvtxt|Kapranov|Smirnov|1995}}&lt;/ref&gt; as '''F'''&lt;sub&gt;''q''&lt;/sub&gt; with ''q'' = 1&lt;sup&gt;''n''&lt;/sup&gt;.

In 1993, [[Yuri Manin]] gave a series of lectures on [[Riemann zeta function|zeta functions]] where he proposed developing a theory of algebraic geometry over '''F'''&lt;sub&gt;1&lt;/sub&gt;.&lt;ref&gt;{{harvtxt|Manin|1995}}.&lt;/ref&gt; He suggested that zeta functions of varieties over '''F'''&lt;sub&gt;1&lt;/sub&gt; would have very simple descriptions, and he proposed a relation between the [[algebraic K-theory|K-theory]] of '''F'''&lt;sub&gt;1&lt;/sub&gt; and the [[homotopy groups of spheres]]. This inspired several people to attempt to construct '''F'''&lt;sub&gt;1&lt;/sub&gt;. In 2000, Zhu proposed that '''F'''&lt;sub&gt;1&lt;/sub&gt; was the same as '''F'''&lt;sub&gt;2&lt;/sub&gt; except that the sum of one and one was one, not zero.&lt;ref&gt;{{harvtxt|Lescot|2009}}.&lt;/ref&gt; Deitmar suggested that '''F'''&lt;sub&gt;1&lt;/sub&gt; should be found by forgetting the additive structure of a ring and focusing on the multiplication.&lt;ref&gt;{{harvtxt|Deitmar|2005}}.&lt;/ref&gt; Toën and Vaquié built on Hakim's theory of relative schemes and defined '''F'''&lt;sub&gt;1&lt;/sub&gt; using [[symmetric monoidal category|symmetric monoidal categories]].&lt;ref&gt;{{harvtxt|Toën|Vaquié|2005}}.&lt;/ref&gt; [[Nikolai Durov]] constructed '''F'''&lt;sub&gt;1&lt;/sub&gt; as a commutative algebraic [[monad (category theory)|monad]].&lt;ref&gt;{{harvtxt|Durov|2008}}.&lt;/ref&gt;  Soulé constructed it using algebras over the complex numbers and functors from categories of certain rings.&lt;ref name="Soule1999"&gt;{{harvtxt|Soulé|1999}}&lt;/ref&gt; Borger used [[descent (category theory)|descent]] to construct it from the finite fields and the integers.&lt;ref&gt;{{harvtxt|Borger|2009}}.&lt;/ref&gt;

Recently, [[Alain Connes]], [[Caterina Consani]] and [[Matilde Marcolli]] have connected '''F'''&lt;sub&gt;1&lt;/sub&gt; with [[noncommutative geometry]].&lt;ref&gt;{{harvtxt|Connes|Consani|Marcolli|2009}}.&lt;/ref&gt;
It has also been suggested to have connections to the [[unique games conjecture]] in [[computational complexity theory]].&lt;ref&gt;{{citation|url=https://gilkalai.wordpress.com/2018/01/10/subhash-khot-dor-minzer-and-muli-safra-proved-the-2-to-2-games-conjecture/|title=Subhash Khot, Dor Minzer and Muli Safra proved the 2-to-2 Games Conjecture|work=Combinatorics and more|first=Gil|last=Kalai|authorlink=Gil Kalai|date=2018-01-10}}&lt;/ref&gt;

Lorscheid, along with others, has recently achieved Tit's original aim of describing Chevalley groups over '''F'''&lt;sub&gt;1&lt;/sub&gt; by introducing objects called blueprints, which are a simultaneous generalisation of both semirings and monoids. These are used to define so-called "blue schemes", one of which is Spec '''F'''&lt;sub&gt;1&lt;/sub&gt;&lt;ref&gt;{{Citation|last=Lorscheid|first=Oliver|title=A blueprinted view on $\mathbb F_1$-geometry|url=http://dx.doi.org/10.4171/157-1/4|work=Absolute Arithmetic and $\mathbb F_1$-Geometry|pages=161–219|publisher=European Mathematical Society Publishing House|isbn=9783037191576|access-date=2018-10-31}}&lt;/ref&gt;.

==Properties==
'''F'''&lt;sub&gt;1&lt;/sub&gt; is expected to have the following properties.
* [[Finite set]]s are both [[affine space]]s and [[projective space]]s over '''F'''&lt;sub&gt;1&lt;/sub&gt;.
* [[Pointed set]]s are [[vector space]]s over '''F'''&lt;sub&gt;1&lt;/sub&gt;.&lt;ref&gt;[http://sbseminar.wordpress.com/2007/08/14/the-field-with-one-element Noah Snyder, The field with one element, Secret Blogging Seminar, 14 August 2007.]&lt;/ref&gt;
* The finite fields '''F'''&lt;sub&gt;''q''&lt;/sub&gt; are [[quantum group|quantum deformations]] of '''F'''&lt;sub&gt;1&lt;/sub&gt;, where ''q'' is the deformation.
* [[Weyl group]]s are simple algebraic groups over '''F'''&lt;sub&gt;1&lt;/sub&gt;:
*: Given a [[Dynkin diagram]] for a semisimple algebraic group, its [[Weyl group]] is&lt;ref&gt;[http://math.ucr.edu/home/baez/week187.html This Week's Finds in Mathematical Physics, Week 187]&lt;/ref&gt; the semisimple algebraic group over '''F'''&lt;sub&gt;1&lt;/sub&gt;.
* The [[affine scheme]] Spec '''Z''' is a curve over '''F'''&lt;sub&gt;1&lt;/sub&gt;.
* Groups are [[Hopf algebra]]s over '''F'''&lt;sub&gt;1&lt;/sub&gt;. More generally, anything defined purely in terms of diagrams of algebraic objects should have an '''F'''&lt;sub&gt;1&lt;/sub&gt;-analog in the category of sets.
* [[Group action]]s on sets are projective representations of ''G'' over '''F'''&lt;sub&gt;1&lt;/sub&gt;, and in this way, ''G'' is the [[group Hopf algebra]] '''F'''&lt;sub&gt;1&lt;/sub&gt;[''G''].
* [[Toric variety|Toric varieties]] determine '''F'''&lt;sub&gt;1&lt;/sub&gt;-varieties. In some descriptions of '''F'''&lt;sub&gt;1&lt;/sub&gt;-geometry the converse is also true, in the sense that the extension of scalars of  '''F'''&lt;sub&gt;1&lt;/sub&gt;-varieties to '''Z''' are toric&lt;ref&gt;{{harvtxt|Deitmar|2006}}.&lt;/ref&gt; Whilst other approaches to '''F'''&lt;sub&gt;1&lt;/sub&gt;-geometry admit wider classes of examples, toric varieties appear to lie at the very heart of the theory.
* The zeta function of '''P'''&lt;sup&gt;''N''&lt;/sup&gt;('''F'''&lt;sub&gt;1&lt;/sub&gt;) should be {{nowrap|1=ζ(''s'') = ''s''(''s'' − 1)⋯(''s'' − ''N'')}}.&lt;ref name="Soule1999"/&gt;
* The ''m''-th ''K''-group of '''F'''&lt;sub&gt;1&lt;/sub&gt; should be the ''m''-th [[stable homotopy group]] of the [[sphere spectrum]].&lt;ref name="Soule1999"/&gt;

==Computations==
Various structures on a [[Set (mathematics)|set]] are analogous to structures on a projective space, and can be computed in the same way:

===Sets are projective spaces===
The number of elements of '''P'''('''F'''{{su|b=''q''|p=''n''}}) = '''P'''&lt;sup&gt;''n''−1&lt;/sup&gt;('''F'''&lt;sub&gt;''q''&lt;/sub&gt;), the {{nowrap|(''n'' − 1)}}-dimensional [[projective space]] over the [[finite field]] '''F'''&lt;sub&gt;''q''&lt;/sub&gt;, is the '''[[q-bracket|''q''-integer]]'''&lt;ref&gt;[http://math.ucr.edu/home/baez/week183.html This Week's Finds in Mathematical Physics, Week 183, ''q''-arithmetic]&lt;/ref&gt;
:&lt;math&gt;[n]_q := \frac{q^n-1}{q-1}=1+q+q^2+\dots+q^{n-1}.&lt;/math&gt;
Taking {{nowrap|1=''q'' = 1}} yields {{nowrap|1=[''n'']&lt;sub&gt;''q''&lt;/sub&gt; = ''n''}}.

The expansion of the ''q''-integer into a sum of powers of ''q'' corresponds to the [[Schubert cell]] decomposition of projective space.

===Permutations are [[Flag (linear algebra)|flags]]===
There are ''n''! permutations of a set with ''n'' elements, and [''n'']&lt;sub&gt;''q''&lt;/sub&gt;! maximal flags in '''F'''{{su|b=''q''|p=''n''}}, where
:&lt;math&gt;[n]_q! := [1]_q [2]_q \dots [n]_q&lt;/math&gt;
is the [[Q-Pochhammer symbol#Relationship to other q-functions|''q''-factorial]]. Indeed, a permutation of a set can be considered a [[Filtration (mathematics)#Sets|filtered set]], as a flag is a filtered vector space: for instance, the ordering {{nowrap|(0, 1, 2)}} of the set {0,1,2} corresponds to the filtration {0} ⊂ {0,1} ⊂ {0,1,2}.

===Subsets are subspaces===
The [[binomial coefficient]] 
:&lt;math&gt;\frac{n!}{m!(n-m)!}&lt;/math&gt; 
gives the number of ''m''-element subsets of an ''n''-element set, and the [[Q-factorial#Relationship to the q-bracket and the q-binomial|''q''-binomial coefficient]] 
:&lt;math&gt;\frac{[n]_q!}{[m]_q![n-m]_q!}&lt;/math&gt; 
gives the number of ''m''-dimensional subspaces of an ''n''-dimensional vector space over '''F'''&lt;sub&gt;''q''&lt;/sub&gt;.

The expansion of the ''q''-binomial coefficient into a sum of powers of ''q'' corresponds to the [[Schubert cell]] decomposition of the [[Grassmannian]].

==Field extensions==
One may define [[field extension]]s of the field with one element as the group of [[roots of unity]], or more finely (with a geometric structure) as the [[group scheme of roots of unity]]. This is non-naturally isomorphic to the [[cyclic group]] of order ''n'', the isomorphism depending on choice of a [[primitive root of unity]]:&lt;ref&gt;Mikhail Kapranov, linked at The F_un folklore&lt;/ref&gt;
:&lt;math&gt;\mathbf{F}_{1^n} = \mu_n.&lt;/math&gt;
Thus a vector space of dimension ''d'' over '''F'''&lt;sub&gt;1&lt;sup&gt;''n''&lt;/sup&gt;&lt;/sub&gt; is a finite set of order ''dn'' on which the roots of unity act freely, together with a base point.

From this point of view the [[finite field]] '''F'''&lt;sub&gt;''q''&lt;/sub&gt; is an algebra over '''F'''&lt;sub&gt;1&lt;sup&gt;''n''&lt;/sup&gt;&lt;/sub&gt;, of dimension {{nowrap|1=''d'' = (''q'' − 1)/''n''}} for any ''n'' that is a factor of {{nowrap|''q'' − 1}} (for example {{nowrap|1=''n'' = ''q'' − 1}} or {{nowrap|1=''n'' = 1}}). This corresponds to the fact that the group of units of a finite field '''F'''&lt;sub&gt;''q''&lt;/sub&gt; (which are the {{nowrap|''q'' − 1}} non-zero elements) is a cyclic group of order {{nowrap|''q'' − 1}}, on which any cyclic group of order dividing {{nowrap|''q'' − 1}} acts freely (by raising to a power), and the zero element of the field is the base point.

Similarly, the [[real number]]s '''R''' are an algebra over '''F'''&lt;sub&gt;1&lt;sup&gt;''2''&lt;/sup&gt;&lt;/sub&gt;, of infinite dimension, as the real numbers contain ±1, but no other roots of unity, and the complex numbers '''C''' are an algebra over '''F'''&lt;sub&gt;1&lt;sup&gt;''n''&lt;/sup&gt;&lt;/sub&gt; for all ''n'', again of infinite dimension, as the complex numbers have all roots of unity.

From this point of view, any phenomenon that only depends on a field having roots of unity can be seen as coming from '''F'''&lt;sub&gt;1&lt;/sub&gt; – for example, the [[discrete Fourier transform]] (complex-valued) and the related [[number-theoretic transform]] ('''Z'''/''n'''''Z'''-valued).

==See also==
* [[Arithmetic derivative]]
* [[Semigroup with one element]]

==Notes==
{{Reflist|2}}

==Bibliography==
* {{ Citation | last1 = Borger | first1 = James | title = Λ-rings and the field with one element | year = 2009 | arxiv=0906.3146 }}
* {{citation | editor1-last=Consani | editor1-first=Caterina | editor2-last=Connes | editor2-first=Alain | editor2-link=Alain Connes | title=Noncommutative geometry, arithmetic, and related topics. Proceedings of the 21st meeting of the Japan-U.S. Mathematics Institute (JAMI) held at Johns Hopkins University, Baltimore, MD, USA, March 23–26, 2009 | location=Baltimore, MD | publisher=Johns Hopkins University Press | isbn=1-4214-0352-8 | year=2011 | zbl=1245.00040 }}
* {{ Citation | last1 = Connes | first1 = Alain | author-link = Alain Connes | last2 = Consani | first2 = Caterina | last3 = Marcolli | first3 = Matilde | author3-link=Matilde Marcolli | title = Fun with &lt;math&gt;\mathbb{F}_1&lt;/math&gt; | journal = Journal of Number Theory | volume = 129 | issue = 6 | year = 2009 | arxiv = 0806.2401 | pages=1532–1561 |zbl=1228.11143 | mr=2521492 | doi = 10.1016/j.jnt.2008.08.007 }}
* {{ Citation | last1 = Deitmar | first1 = Anton | chapter = Schemes over '''F'''&lt;sub&gt;1&lt;/sub&gt; | title = Number Fields and Function Fields: Two Parallel Worlds | editor-last = van der Geer | editor-first = G. | editor2-last = Moonen | editor2-first = B. | editor3-last = Schoof | editor3-first = R. | series = Progress in Mathematics | volume = 239 | year = 2005 }}
* {{ Citation | last1 = Deitmar | first1 = Anton | title = '''F'''&lt;sub&gt;1&lt;/sub&gt;-schemes and toric varieties | year = 2006 | arxiv = math/0608179 }}
* {{ cite arxiv | last1 = Durov | first1 = Nikolai | authorlink=Nikolai Durov | title = New Approach to Arakelov Geometry | year = 2008 | eprint = 0704.2030 |mode=cs2| class = math.AG }}
* {{ Citation | last1 = Kapranov | first1 = Mikhail | last2 = Smirnov | first2 = Alexander | title = Cohomology determinants and reciprocity laws: number field case | year = 1995 | url = http://www.neverendingbooks.org/DATA/KapranovSmirnov.pdf }}
* {{ cite arxiv | last = Le Bruyn | first = Lieven | title = (non)commutative f-un geometry  | year = 2009 | eprint = 0909.2522 |mode=cs2 | class = math.RA }}
* {{Citation | last1 = Lescot | first1 = Paul | title = Algebre absolue | year = 2009 | url = http://www.univ-rouen.fr/LMRS/Persopage/Lescot/algabsodef.pdf | access-date = 21 November 2009 | archive-url = https://web.archive.org/web/20110727024554/http://www.univ-rouen.fr/LMRS/Persopage/Lescot/algabsodef.pdf | archive-date = 27 July 2011 | dead-url = yes | df = dmy-all }}
* {{ Citation | last1 = López Peña | first1 = Javier | last2 = Lorscheid | first2 = Oliver | title = Mapping '''F'''&lt;sub&gt;1&lt;/sub&gt;-land: An overview of geometries over the field with one element | journal = Noncommutative Geometry, Arithmetic, and related topics | pages = 241–265 | year = 2011 | arxiv = 0909.0069 }}
* {{ cite arxiv | title = Algebraic groups over the field with one element | first = Oliver | last = Lorscheid | year = 2009 | eprint = 0907.3824 |mode=cs2 | class = math.AG }}
*Oliver Lorscheid, Oliver (2016), "A blueprinted view on '''F'''&lt;sub&gt;1&lt;/sub&gt;-geometry", In Absolute arithmetic and F 1 -  geometry (edited by Koen Thas). European Mathematical Society Publishing House
* {{ Citation | last1 = Manin | first1 = Yuri | author-link = Yuri Manin | title = Lectures on zeta functions and motives (according to Deninger and Kurokawa) | journal = Astérisque | volume = 228 | issue = 4 | year = 1995 | pages = 121–163 }}
* {{ Citation | last1 = Scholze | first1 = Peter | author-link = Peter Scholze | title = p-adic geometry | arxiv = 1712.03708 | year = 2017 | pages = 13}}
* {{ Citation | last1 = Smirnov | first1 = Alexander |  title = Hurwitz inequalities for number fields | journal = Algebra I Analiz | volume = 4 | issue = 2 | year = 1992 | pages = 186–209 }}
* {{ Citation | last1 = Soulé | first1 = Christophe | author1-link = Christophe Soule | title = On the field with one element (exposé à l'Arbeitstagung, Bonn, June 1999) | publisher = Preprint IHES | year = 1999 | url = http://www.ihes.fr/%7Esoule/f1-soule.pdf }}
* {{ Citation | last1 = Soulé | first1 = Christophe | author1-link = Christophe Soule | title = Les variétés sur le corps à un élément | year = 2003 | arxiv = math/0304444 | language = French }}
* {{ Citation | last1 = Tits | first1 = Jacques | author1-link = Jacques Tits|chapter = Sur les analogues algébriques des groupes semi-simples complexes | title = Colloque d'algèbre supérieure, tenu à Bruxelles du 19 au 22 décembre 1956, Centre Belge de Recherches Mathématiques Établissements Ceuterick, Louvain | publisher = Librairie Gauthier-Villars | place = Paris | year = 1957 | pages = 261–289 }}
* {{ Citation | last1 = Toën | first1 = Bertrand | last2 = Vaquié | first2 = Michel | author1-link = Bertrand Toën | title = Au dessous de Spec '''Z''' | arxiv = math/0509684 | year = 2005 }}

==External links==
* [[John Baez]]'s This Week's Finds in Mathematical Physics: [http://math.ucr.edu/home/baez/week259.html Week 259]
* [http://golem.ph.utexas.edu/category/2007/04/the_field_with_one_element.html The Field With One Element] at the ''n''-category cafe
* [http://sbseminar.wordpress.com/2007/08/14/the-field-with-one-element/ The Field With One Element] at Secret Blogging Seminar
* [http://www.neverendingbooks.org/looking-for-f_un Looking for F&lt;sub&gt;un&lt;/sub&gt;] and [http://www.neverendingbooks.org/the-f_un-folklore The F&lt;sub&gt;un&lt;/sub&gt; folklore], Lieven le Bruyn.
* [http://front.math.ucdavis.edu/0909.0069 Mapping F_1-land:An overview of geometries over the field with one element], Javier López Peña, Oliver Lorscheid
* [http://cage.ugent.be/~kthas/Fun  F&lt;sub&gt;un&lt;/sub&gt; Mathematics], Lieven le Bruyn, [[Thas, Koen|Koen Thas]].
* [http://www.ihes.fr/IHES/Scientifique/soule/ Conference at IHES on algebraic geometry over '''F'''&lt;sub&gt;1&lt;/sub&gt;]{{dead link|date=September 2017 |bot=InternetArchiveBot |fix-attempted=yes }}
* Vanderbilt conference on [http://www.math.vanderbilt.edu/~ncgoa/workshop2008.html Noncommutative Geometry and Geometry over the Field with One Element] ([http://www.math.vanderbilt.edu/~ncgoa/schedule_workshop08.pdf Schedule])
* [http://noncommutativegeometry.blogspot.com/2008/05/ncg-and-fun.html NCG and F_un], by [[Alain Connes]] and K. Consani: summary of talks and slides

{{DEFAULTSORT:Field With One Element}}
[[Category:Algebraic geometry]]
[[Category:Noncommutative geometry]]
[[Category:Finite fields]]</text>
      <sha1>il2m5xj7p6euoliiumd2c0436eta47p</sha1>
    </revision>
  </page>
  <page>
    <title>Graduate Studies in Mathematics</title>
    <ns>0</ns>
    <id>8871519</id>
    <revision>
      <id>870814467</id>
      <parentid>867699562</parentid>
      <timestamp>2018-11-27T04:41:07Z</timestamp>
      <contributor>
        <username>Mattbeck</username>
        <id>622123</id>
      </contributor>
      <minor/>
      <comment>Added coauthor to #195</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="24447">'''Graduate Studies in Mathematics''' ('''GSM''') is a series of [[Graduate School|graduate-level]] [[textbooks]] in [[mathematics]] published by the [[American Mathematical Society]] (AMS). These books elaborate on several theories from notable personas, such as [[Martin Schechter (mathematician)|Martin Schechter]] and [[Terence Tao]], in the mathematical industry. The books in this series are published only in hardcover.

==List of books==

*1	''The General Topology of Dynamical Systems'', Ethan Akin (1993, {{ISBN|978-0-8218-4932-3}})&lt;ref&gt;The hardback is {{ISBN|978-0-8218-3800-6}}.&lt;/ref&gt;
*2	''Combinatorial Rigidity'', Jack Graver, [[Brigitte Servatius]], Herman Servatius (1993, {{ISBN|978-0-8218-3801-3}})
*3	''An Introduction to Gröbner Bases'', William W. Adams, Philippe Loustaunau (1994, {{ISBN|978-0-8218-3804-4}})
*4	''The Integrals of Lebesgue, Denjoy, Perron, and Henstock'', Russell A. Gordon (1994, {{ISBN|978-0-8218-3805-1}})
*5	''Algebraic Curves and Riemann Surfaces'', Rick Miranda (1995, {{ISBN|978-0-8218-0268-7}})
*6	''Lectures on Quantum Groups'', [[Jens Carsten Jantzen]] (1996, {{ISBN|978-0-8218-0478-0}})
*7	''Algebraic Number Fields'', Gerald J. Janusz (1996, 2nd ed., {{ISBN|978-0-8218-0429-2}})
*8	''Discovering Modern Set Theory. I: The Basics'', Winfried Just, Martin Weese (1996, {{ISBN|978-0-8218-0266-3}})
*9	''An Invitation to Arithmetic Geometry'', Dino Lorenzini (1996, {{ISBN|978-0-8218-0267-0}})
*10	''Representations of Finite and Compact Groups'', [[Barry Simon]] (1996, {{ISBN|978-0-8218-0453-7}})
*11	''Enveloping Algebras'', [[Jacques Dixmier]] (1996, {{ISBN|978-0-8218-0560-2}})
*12	''Lectures on Elliptic and Parabolic Equations in Hölder Spaces'', N. V. Krylov (1996, {{ISBN|978-0-8218-0569-5}})
*13	''The Ergodic Theory of Discrete Sample Paths'', Paul C. Shields (1996, {{ISBN|978-0-8218-0477-3}})
*14	''Analysis'', [[Elliott H. Lieb]], [[Michael Loss]] (2001, 2nd ed., {{ISBN|978-0-8218-2783-3}})
*15	''Fundamentals of the Theory of Operator Algebras. Volume I: Elementary Theory'', [[Richard V. Kadison]], [[John R. Ringrose]] (1997, {{ISBN|978-0-8218-0819-1}})&lt;ref&gt;This book has a companion volume: '''FTOAN/3.S''' ''Fundamentals of the Theory of Operator Algebras. Volume III'', [[Richard V. Kadison]], [[John R. Ringrose]] (1991, {{ISBN|978-0-8218-9469-9}}).&lt;/ref&gt;
*16	''Fundamentals of the Theory of Operator Algebras. Volume II: Advanced Theory'', [[Richard V. Kadison]], [[John R. Ringrose]] (1997, {{ISBN|978-0-8218-0820-7}})&lt;ref&gt;This book has a companion volume: '''FTOAN/4.S''' ''Fundamentals of the Theory of Operator Algebras. Volume IV'', [[Richard V. Kadison]], [[John R. Ringrose]] (1991, {{ISBN|978-0-8218-9468-2}}).&lt;/ref&gt;
*17	''Topics in Classical Automorphic Forms'', [[Henryk Iwaniec]] (1997, {{ISBN|978-0-8218-0777-4}})
*18	''Discovering Modern Set Theory. II: Set-Theoretic Tools for Every Mathematician'', Winfried Just, Martin Weese (1997, {{ISBN|978-0-8218-0528-2}})
*19	''Partial Differential Equations'', [[Lawrence C. Evans]] (2010, 2nd ed., {{ISBN|978-0-8218-4974-3}})
*20	''4-Manifolds and Kirby Calculus'', Robert E. Gompf, András I. Stipsicz (1999, {{ISBN|978-0-8218-0994-5}})
*21	''A Course in Operator Theory'', [[John B. Conway]] (2000, {{ISBN|978-0-8218-2065-0}})
*22	''Growth of Algebras and Gelfand-Kirillov Dimension'', Günter R. Krause, Thomas H. Lenagan (2000, Revised ed., {{ISBN|978-0-8218-0859-7}})
*23	''Foliations I'', Alberto Candel, Lawrence Conlon (2000, {{ISBN|978-0-8218-0809-2}})
*24	''Number Theory: Algebraic Numbers and Functions'', Helmut Koch (2000, {{ISBN|978-0-8218-2054-4}})
*25	''Dirac Operators in Riemannian Geometry'', Thomas Friedrich (2000, {{ISBN|978-0-8218-2055-1}})
*26	''An Introduction to Symplectic Geometry'', Rolf Berndt (2001, {{ISBN|978-0-8218-2056-8}})
*27	''A Course in Differential Geometry'', [[Thierry Aubin]] (2001, {{ISBN|978-0-8218-2709-3}})
*28	''Notes on Seiberg-Witten Theory'', Liviu I. Nicolaescu (2000, {{ISBN|978-0-8218-2145-9}})
*29	''Fourier Analysis'', Javier Duoandikoetxea (2001, {{ISBN|978-0-8218-2172-5}})
*30	''Noncommutative Noetherian Rings'', J. C. McConnell, J. C. Robson (1987, {{ISBN|978-0-8218-2169-5}})
*31	''Option Pricing and Portfolio Optimization: Modern Methods of Financial Mathematics'', Ralf Korn, Elke Korn (2001, {{ISBN|978-0-8218-2123-7}})
*32	''A Modern Theory of Integration'', [[Robert G. Bartle]] (2001, {{ISBN|978-0-8218-0845-0}})&lt;ref&gt;This book has a companion volume: '''GSM/32.M''' ''Solutions Manual to A Modern Theory of Integration'', Robert G. Bartle (2001, {{ISBN|978-0-8218-2821-2}}).&lt;/ref&gt;
*33	''A Course in Metric Geometry'', Dmitri Burago, [[Yuri Burago]], Sergei Ivanov (2001, {{ISBN|978-0-8218-2129-9}})
*34	''Differential Geometry, Lie Groups, and Symmetric Spaces'', [[Sigurdur Helgason (mathematician)|Sigurdur Helgason]] (2001, {{ISBN|978-0-8218-2848-9}})
*35	''Lecture Notes in Algebraic Topology'', James F. Davis, Paul Kirk (2001, {{ISBN|978-0-8218-2160-2}})
*36	''Principles of Functional Analysis'', [[Martin Schechter (mathematician)|Martin Schechter]] (2002, 2nd ed., {{ISBN|978-0-8218-2895-3}})
*37	''Theta Constants, Riemann Surfaces and the Modular Group: An Introduction with Applications to Uniformization Theorems, Partition Identities and Combinatorial Number Theory'', Hershel M. Farkas, [[Irwin Kra]] (2001, {{ISBN|978-0-8218-1392-8}})
*38	''Stochastic Analysis on Manifolds'', Elton P. Hsu (2002, {{ISBN|978-0-8218-0802-3}})
*39	''Classical Groups and Geometric Algebra'', Larry C. Grove (2002, {{ISBN|978-0-8218-2019-3}})
*40	''Function Theory of One Complex Variable'', [[Robert Everist Greene|Robert E. Greene]], [[Steven G. Krantz]] (2006, 3rd ed., {{ISBN|978-0-8218-3962-1}})
*41	''Introduction to the Theory of Differential Inclusions'', Georgi V. Smirnov (2002, {{ISBN|978-0-8218-2977-6}})
*42	''Introduction to Quantum Groups and Crystal Bases'', Jin Hong, Seok-Jin Kang (2002, {{ISBN|978-0-8218-2874-8}})
*43	''Introduction to the Theory of Random Processes'', N. V. Krylov (2002, {{ISBN|978-0-8218-2985-1}})
*44	''Pick Interpolation and Hilbert Function Spaces'', Jim Agler, John E. McCarthy (2002, {{ISBN|978-0-8218-2898-4}})
*45	''An Introduction to Measure and Integration'', Inder K. Rana (2002, 2nd ed., {{ISBN|978-0-8218-2974-5}})
*46	''Several Complex Variables with Connections to Algebraic Geometry and Lie Groups'', [[Joseph L. Taylor]] (2002, {{ISBN|978-0-8218-3178-6}})
*47	''Classical and Quantum Computation'', A. Yu. Kitaev, A. H. Shen, M. N. Vyalyi (2002, {{ISBN|978-0-8218-3229-5}})&lt;ref&gt;The hardback is {{ISBN|978-0-8218-2161-9}}.&lt;/ref&gt;
*48	''Introduction to the h-Principle'', Y. Eliashberg, N. Mishachev (2002, {{ISBN|978-0-8218-3227-1}})
*49	''Secondary Cohomology Operations'', John R. Harper (2002, {{ISBN|978-0-8218-3270-7}})
*50	''An Invitation to Operator Theory'', Y. A. Abramovich, [[C. D. Aliprantis]] (2002, {{ISBN|978-0-8218-2146-6}})&lt;ref name="GSMSET"&gt;Two volume set is '''GSMSET''' (2002, {{ISBN|978-0-8218-3333-9}}).&lt;/ref&gt;
*51	''Problems in Operator Theory'', Y. A. Abramovich, [[C. D. Aliprantis]] (2002, {{ISBN|978-0-8218-2147-3}})&lt;ref name="GSMSET" /&gt;
*52	''Global Analysis: Differential Forms in Analysis, Geometry and Physics'', [[Ilka Agricola]], Thomas Friedrich (2002, {{ISBN|978-0-8218-2951-6}})
*53	''Spectral Methods of Automorphic Forms'', [[Henryk Iwaniec]] (2002, 2nd ed., {{ISBN|978-0-8218-3160-1}})
*54	''A Course in Convexity'', [[Alexander Barvinok]] (2002, {{ISBN|978-0-8218-2968-4}})
*55	''A Scrapbook of Complex Curve Theory'', C. Herbert Clemens (2003, 2nd ed., {{ISBN|978-0-8218-3307-0}})
*56	''A Course in Algebra'', [[E. B. Vinberg]] (2003, {{ISBN|978-0-8218-3318-6}})
*57	''Concise Numerical Mathematics'', Robert Plato (2003, {{ISBN|978-0-8218-2953-0}})
*58	''Topics in Optimal Transportation'', [[Cédric Villani]] (2003, {{ISBN|978-0-8218-3312-4}})
*59	''Representation Theory of Finite Groups: Algebra and Arithmetic'', Steven H. Weintraub (2003, {{ISBN|978-0-8218-3222-6}})
*60	''Foliations II'', Alberto Candel, Lawrence Conlon (2003, {{ISBN|978-0-8218-0881-8}})
*61	''Cartan for Beginners: Differential Geometry via Moving Frames and Exterior Differential Systems'', Thomas A. Ivey, J. M. Landsberg (2003, {{ISBN|978-0-8218-3375-9}})
*62	''A Companion to Analysis: A Second First and First Second Course in Analysis'', T. W. Körner (2004, {{ISBN|978-0-8218-3447-3}})
*63	''Resolution of Singularities'', Steven Dale Cutkosky (2004, {{ISBN|978-0-8218-3555-5}})
*64	''Lectures on the Orbit Method'', [[A. A. Kirillov]] (2004, {{ISBN|978-0-8218-3530-2}})
*65	''Global Calculus'', [[S. Ramanan]] (2005, {{ISBN|978-0-8218-3702-3}})
*66	''Functional Analysis: An Introduction'', Yuli Eidelman, [[Vitali Milman]], Antonis Tsolomitis (2004, {{ISBN|978-0-8218-3646-0}})
*67	''Introduction to Quadratic Forms over Fields'', [[T.Y. Lam]] (2005, {{ISBN|978-0-8218-1095-8}})
*68	''A Geometric Approach to Free Boundary Problems'', [[Luis Caffarelli]], Sandro Salsa (2005, {{ISBN|978-0-8218-3784-9}})
*69	''Curves and Surfaces'', Sebastián Montiel, Antonio Ros (2009, 2nd ed., {{ISBN|978-0-8218-4763-3}})
*70	''Probability Theory in Finance: A Mathematical Guide to the Black-Scholes Formula'', Seán Dineen (2013, 2nd ed., {{ISBN|978-0-8218-9490-3}})
*71	''Modern Geometric Structures and Fields'', [[S. P. Novikov]], I. A. Taimanov (2006, {{ISBN|978-0-8218-3929-4}})
*72	''Introduction to the Mathematics of Finance'', [[Ruth J. Williams]] (2006, {{ISBN|978-0-8218-3903-4}})
*73	''Graduate Algebra: Commutative View'', Louis Halle Rowen (2006, {{ISBN|978-0-8218-0570-1}})
*74	''Elements of Combinatorial and Differential Topology'', V. V. Prasolov (2006, {{ISBN|978-0-8218-3809-9}})
*75	''Applied Asymptotic Analysis'', Peter D. Miller (2006, {{ISBN|978-0-8218-4078-8}})
*76	''Measure Theory and Integration'', [[Michael E. Taylor]] (2006, {{ISBN|978-0-8218-4180-8}})
*77	''Hamilton's Ricci Flow'', Bennett Chow, Peng Lu, Lei Ni (2006, {{ISBN|978-0-8218-4231-7}})
*78	''Linear Algebra in Action'', [[Harry Dym]] (2013, 2nd ed., {{ISBN|978-1-4704-0908-1}})
*79	'' Modular Forms, a Computational Approach'', [[William A. Stein]] (2007, {{ISBN|978-0-8218-3960-7}})
*80	''Probability'', Davar Khoshnevisan (2007, {{ISBN|978-0-8218-4215-7}})
*81	''Elements of Homology Theory'', V. V. Prasolov (2007, {{ISBN|978-0-8218-3812-9}})&lt;ref&gt;This book is a natural continuation of volume 74.&lt;/ref&gt;
*82	''Pseudo-differential Operators and the Nash-Moser Theorem'', Serge Alinhac, Patrick Gérard (2007, {{ISBN|978-0-8218-3454-1}})
*83	''Functions of Several Complex Variables and Their Singularities'', Wolfgang Ebeling (2007, {{ISBN|978-0-8218-3319-3}})
*84	''Cones and Duality'', [[Charalambos D. Aliprantis]], Rabee Tourky (2007, {{ISBN|978-0-8218-4146-4}})
*85	''Recurrence and Topology'', John M. Alongi, [[Gail S. Nelson]] (2007, {{ISBN|978-0-8218-4234-8}})
*86	''Lectures on Analytic Differential Equations'', Yulij Ilyashenko, Sergei Yakovenko (2008, {{ISBN|978-0-8218-3667-5}})
*87	''Twenty-Four Hours of Local Cohomology'', Srikanth B. Iyengar, Graham J. Leuschke, Anton Leykin, Claudia Miller, Ezra Miller, Anurag K. Singh, Uli Walther (2007, {{ISBN|978-0-8218-4126-6}})
*88	''C∗-Algebras and Finite-Dimensional Approximations'', Nathanial P. Brown, [[Narutaka Ozawa]] (2008, {{ISBN|978-0-8218-4381-9}})
*89	''A Course on the Web Graph'', Anthony Bonato (2008, {{ISBN|978-0-8218-4467-0}})
*90	''Basic Quadratic Forms'', Larry J. Gerstein (2008, {{ISBN|978-0-8218-4465-6}})
*91	''Graduate Algebra: Noncommutative View'', Louis Halle Rowen (2008, {{ISBN|978-0-8218-4153-2}})&lt;ref&gt;This book is a continuation of volume 73 in which chapter 1~12 are.&lt;/ref&gt;
*92	''Finite Group Theory'', [[I. Martin Isaacs]] (2008, {{ISBN|978-0-8218-4344-4}})
*93	''Topics in Differential Geometry'', Peter W. Michor (2008, {{ISBN|978-0-8218-2003-2}})
*94	''Representations of Semisimple Lie Algebras in the BGG Category O'', [[James E. Humphreys]] (2008, {{ISBN|978-0-8218-4678-0}})
*95	''Quantum Mechanics for Mathematicians'', Leon A. Takhtajan (2008, {{ISBN|978-0-8218-4630-8}})
*96	''Lectures on Elliptic and Parabolic Equations in Sobolev Spaces'', [[Nikolai V. Krylov|N. V. Krylov]] (2008, {{ISBN|978-0-8218-4684-1}})
*97	''Complex Made Simple'', David C. Ullrich (2008, {{ISBN|978-0-8218-4479-3}})
*98	''Discrete Differential Geometry: Integrable Structure'', Alexander I. Bobenko, Yuri B. Suris (2008, {{ISBN|978-0-8218-4700-8}})
*99	''Mathematical Methods in Quantum Mechanics: With Applications to Schrödinger Operators'', [[Gerald Teschl]] (2009, {{ISBN|978-0-8218-4660-5}})&lt;ref&gt;The second edition of this titile is volume 157.&lt;/ref&gt;
*100	''Algebra: A Graduate Course'', [[I. Martin Isaacs]] (1994, {{ISBN|978-0-8218-4799-2}})
*101	''A Course in Approximation Theory'', [[Ward Cheney]], Will Light (2000, {{ISBN|978-0-8218-4798-5}})
*102	''Introduction to Fourier Analysis and Wavelets'', Mark A. Pinsky (2002, {{ISBN|978-0-8218-4797-8}})
*103	''Configurations of Points and Lines'', [[Branko Grünbaum]] (2009, {{ISBN|978-0-8218-4308-6}})
*104	''Algebra: Chapter 0'', Paolo Aluffi (2009, {{ISBN|978-0-8218-4781-7}})
*105	''A First Course in Sobolev Spaces'', Giovanni Leoni (2009, {{ISBN|978-0-8218-4768-8}})&lt;ref&gt;The second edition of this titile is volume 181.&lt;/ref&gt;
*106	''Embeddings in Manifolds'', Robert J. Daverman, Gerard A. Venema (2009, {{ISBN|978-0-8218-3697-2}})
*107	''Manifolds and Differential Geometry'', Jeffrey M. Lee (2009, {{ISBN|978-0-8218-4815-9}})
*108	''Mapping Degree Theory'', Enrique Outerelo, Jesús M. Ruiz (2009, {{ISBN|978-0-8218-4915-6}})
*109	''Training Manual on Transport and Fluids'', John C. Neu (2010, {{ISBN|978-0-8218-4083-2}})
*110	''Differential Algebraic Topology: From Stratifolds to Exotic Spheres'', [[Matthias Kreck]] (2010, {{ISBN|978-0-8218-4898-2}})
*111	''Ricci Flow and the Sphere Theorem'', [[Simon Brendle]] (2010, {{ISBN|978-0-8218-4938-5}})
*112	''Optimal Control of Partial Differential Equations: Theory, Methods and Applications'', Fredi Troltzsch (2010, {{ISBN|978-0-8218-4904-0}})
*113	''Continuous Time Markov Processes: An Introduction'', [[Thomas M. Liggett]] (2010, {{ISBN|978-0-8218-4949-1}})
*114	''Advanced Modern Algebra'', Joseph J. Rotman (2010, 2nd ed., {{ISBN|978-0-8218-4741-1}})&lt;ref&gt;The third edition of this title is volume 165 and 180.&lt;/ref&gt;
*115	''An Introductory Course on Mathematical Game Theory'', Julio González-Díaz, Ignacio García-Jurado, M. Gloria Fiestras-Janeiro (2010, {{ISBN|978-0-8218-5151-7}})
*116	''Linear Functional Analysis'', Joan Cerdà (2010, {{ISBN|978-0-8218-5115-9}})
*117	''An Epsilon of Room, I: Real Analysis: pages from year three of a mathematical blog'', [[Terence Tao]] (2010, {{ISBN|978-0-8218-5278-1}})
*118	''Dynamical Systems and Population Persistence'', Hal L. Smith, Horst R. Thieme (2011, {{ISBN|978-0-8218-4945-3}})
*119	''Mathematical Statistics: Asymptotic Minimax Theory'', Alexander Korostelev, Olga Korosteleva (2011, {{ISBN|978-0-8218-5283-5}})
*120	''A Basic Course in Partial Differential Equations'', Qing Han (2011, {{ISBN|978-0-8218-5255-2}})
*121	''A Course in Minimal Surfaces'', [[Tobias Holck Colding]], William P. Minicozzi II (2011, {{ISBN|978-0-8218-5323-8}})
*122	''Algebraic Groups and Differential Galois Theory'', Teresa Crespo, Zbigniew Hajto (2011, {{ISBN|978-0-8218-5318-4}})
*123	''Lectures on Linear Partial Differential Equations'', [[Gregory Eskin]] (2011, {{ISBN|978-0-8218-5284-2}})
*124	''Toric Varieties'', [[David A. Cox]], John B. Little, Henry K. Schenck (2011, {{ISBN|978-0-8218-4819-7}})
*125	''Riemann Surfaces by Way of Complex Analytic Geometry'', Dror Varolin (2011, {{ISBN|978-0-8218-5369-6}})
*126	''An Introduction to Measure Theory'', [[Terence Tao]] (2011, {{ISBN|978-0-8218-6919-2}})
*127	''Modern Classical Homotopy Theory'', Jeffrey Strom (2011, {{ISBN|978-0-8218-5286-6}})
*128	''Tensors: Geometry and Applications'', J. M. Landsberg (2012, {{ISBN|978-0-8218-6907-9}})
*129	''Classical Methods in Ordinary Differential Equations: With Applications to Boundary Value Problems'', Stuart P. Hastings, J. Bryce McLeod (2012, {{ISBN|978-0-8218-4694-0}})
*130	''Gröbner Bases in Commutative Algebra'', Viviana Ene, Jürgen Herzog (2011, {{ISBN|978-0-8218-7287-1}})
*131	''Lie Superalgebras and Enveloping Algebras'', Ian M. Musson (2012, {{ISBN|978-0-8218-6867-6}})
*132	''Topics in Random Matrix Theory'', [[Terence Tao]] (2012, {{ISBN|978-0-8218-7430-1}})
*133	''Hyperbolic Partial Differential Equations and Geometric Optics'', [[Jeffrey Rauch]] (2012, {{ISBN|978-0-8218-7291-8}})
*134	''Analytic Number Theory: Exploring the Anatomy of Integers'', [[Jean-Marie De Koninck]], [[Florian Luca]] (2012, {{ISBN|978-0-8218-7577-3}})
*135	''Linear and Quasi-linear Evolution Equations in Hilbert Spaces'', Pascal Cherrier, Albert Milani (2012, {{ISBN|978-0-8218-7576-6}})
*136	''Regularity of Free Boundaries in Obstacle-Type Problems'', Arshak Petrosyan, Henrik Shahgholian, [[Nina Uraltseva]] (2012, {{ISBN|978-0-8218-8794-3}})
*137	''Ordinary Differential Equations: Qualitative Theory'', Luis Barreira, Claudia Valls (2012, {{ISBN|978-0-8218-8749-3}})
*138	''Semiclassical Analysis'', [[Maciej Zworski]] (2012, {{ISBN|978-0-8218-8320-4}})
*139	''Knowing the Odds: An Introduction to Probability'', John B. Walsh (2012, {{ISBN|978-0-8218-8532-1}})
*140	''Ordinary Differential Equations and Dynamical Systems'', [[Gerald Teschl]] (2012, {{ISBN|978-0-8218-8328-0}})
*141	''A Course in Abstract Analysis'', [[John B. Conway]] (2012, {{ISBN|978-0-8218-9083-7}})
*142	''Higher Order Fourier Analysis'', [[Terence Tao]] (2012, {{ISBN|978-0-8218-8986-2}})
*143	''Lecture Notes on Functional Analysis: With Applications to Linear Partial Differential Equations'', [[Alberto Bressan]] (2013, {{ISBN|978-0-8218-8771-4}})
*144	''Dualities and Representations of Lie Superalgebras'', Shun-Jen Cheng, Weiqiang Wang (2012, {{ISBN|978-0-8218-9118-6}})
*145	''The K-book: An Introduction to Algebraic K-theory'', Charles A. Weibel (2013, {{ISBN|978-0-8218-9132-2}})
*146	''Combinatorial Game Theory'', Aaron N. Siegel (2013, {{ISBN|978-0-8218-5190-6}})
*147	''Matrix Theory'', Xingzhi Zhan (2013, {{ISBN|978-0-8218-9491-0}})
*148	''Introduction to Smooth Ergodic Theory'', Luis Barreira, Yakov Pesin (2013, {{ISBN|978-0-8218-9853-6}})
*149	''Mathematics of Probability'', [[Daniel W. Stroock]] (2013, {{ISBN|978-1-4704-0907-4}})
*150	''The Joys of Haar Measure'', Joe Diestel, [[Angela Spalsbury]] (2013, {{ISBN|978-1-4704-0935-7}})
*151    ''Introduction to 3-Manifolds'', Jennifer Schultens (2014, {{ISBN|978-1-4704-1020-9}})
*152    ''An Introduction to Extremal Kähler Metrics'', Gábor Székelyhidi (2014, {{ISBN|978-1-4704-1047-6}})
*153    ''Hilbert's Fifth Problem and Related Topics'', [[Terence Tao]] (2014, {{ISBN|978-1-4704-1564-8}})
*154    ''A Course in Complex Analysis and Riemann Surfaces'', Wilhelm Schlag (2014, {{ISBN|978-0-8218-9847-5}})
*155    ''An Introduction to the Representation Theory of Groups'', Emmanuel Kowalski (2014, {{ISBN|978-1-4704-0966-1}})
*156    ''Functional Analysis: An Elementary Introduction'', Markus Haase (2014, {{ISBN|978-0-8218-9171-1}})
*157    ''Mathematical Methods in Quantum Mechanics: With Applications to Schrödinger Operators'', Gerald Teschl (2014, 2nd ed., {{ISBN|978-1-4704-1704-8}})
*158    ''Dynamical Systems and Linear Algebra'', Fritz Colonius, Wolfgang Kliemann (2014, {{ISBN|978-0-8218-8319-8}})
*159    ''The Role of Nonassociative Algebra in Projective Geometry'', John R. Faulkner (2014, {{ISBN|978-1-4704-1849-6}})
*160    ''A Course in Analytic Number Theory'', Marius Overholt (2014, {{ISBN|978-1-4704-1706-2}})
*161    ''Introduction to Tropical Geometry'', [[Diane Maclagan]], [[Bernd Sturmfels]] (2015, {{ISBN|978-0-8218-5198-2}})
*162    ''A Course on Large Deviations with an Introduction to Gibbs Measures'', Firas Rassoul-Agha, Timo Seppäläinen (2015, {{ISBN|978-0-8218-7578-0}})
*163    ''Introduction to Analytic and Probabilistic Number Theory'', Gérald Tenenbaum (2015, 3rd ed., {{ISBN|978-0-8218-9854-3}})
*164    ''Expansion in Finite Simple Groups of Lie Type'', [[Terence Tao]] (2015, {{ISBN|978-1-4704-2196-0}})
*165    ''Advanced Modern Algebra, Part 1'', Joseph J. Rotman (2015, 3rd ed., {{ISBN|978-1-4704-1554-9}})&lt;ref name="GSMSET-165-180"&gt;Two volume set is '''GSM/165/180''' (2017, {{ISBN|978-1-4704-4174-6}}).&lt;/ref&gt;
*166    ''Problems in Real and Functional Analysis'', Alberto Torchinsky (2015, {{ISBN|978-1-4704-2057-4}})
*167    ''Singular Perturbation in the Physical Sciences'', John C. Neu (2015, {{ISBN|978-1-4704-2555-5}})
*168    ''Random Operators: Disorder Effects on Quantum Spectra and Dynamics'', [[Michael Aizenman]], [[Simone Warzel]] (2015, {{ISBN|978-1-4704-1913-4}})
*169    ''Partial Differential Equations: An Accessible Route through Theory and Applications'', András Vasy (2015, {{ISBN|978-1-4704-1881-6}})
*170    ''Colored Operads'', Donald Yau (2016, {{ISBN|978-1-4704-2723-8}})
*171    ''Nonlinear Elliptic Equations of the Second Order'', Qing Han (2016, {{ISBN|978-1-4704-2607-1}})
*172    ''Combinatorics and Random Matrix Theory'', Jinho Baik, [[Percy Deift]], Toufic Suidan (2016, {{ISBN|978-0-8218-4841-8}})
*173    ''Differentiable Dynamical Systems: An Introduction to Structural Stability and Hyperbolicity'', Lan Wen (2016, {{ISBN|978-1-4704-2799-3}})
*174    ''Quiver Representations and Quiver Varieties'', [[Alexander Kirillov, Jr.|Alexander Kirillov Jr.]] (2016, {{ISBN|978-1-4704-2307-0}})
*175    ''Cartan for Beginners: Differential Geometry via Moving Frames and Exterior Differential Systems'', Thomas A. Ivey, Joseph M. Landsberg (2016, 2nd ed., {{ISBN|978-1-4704-0986-9}})
*176    ''Ordered Groups and Topology'', Adam Clay, Dale Rolfsen (2016, {{ISBN|978-1-4704-3106-8}})
*177    ''Differential Galois Theory through Riemann-Hilbert Correspondence: An Elementary Introduction'', Jacques Sauloy (2016, {{ISBN|978-1-4704-3095-5}})
*178    ''From Frenet to Cartan: The Method of Moving Frames'', Jeanne N. Clelland (2017, {{ISBN|978-1-4704-2952-2}})
*179    ''Modular Forms: A Classical Approach'', Henri Cohen, Fredrik Strömberg (2017, {{ISBN|978-0-8218-4947-7}})
*180    ''Advanced Modern Algebra, Part 2'', Joseph J. Rotman (2017, 3rd ed., {{ISBN|978-1-4704-2311-7}})&lt;ref name="GSMSET-165-180"/&gt;
*181    ''A First Course in Sobolev Spaces'', Giovanni Leoni (2017, 2nd ed., {{ISBN|978-1-4704-2921-8}})
*182    ''Nonlinear PDEs: A Dynamical Systems Approach'', Guido Schneider, Hannes Uecker (2017, {{ISBN|978-1-4704-3613-1}})
*183    ''Separable Algebras'', Timothy J. Ford (2017, {{ISBN|978-1-4704-3770-1}})
*184    ''An Introduction to Quiver Representations'', Harm Derksen, Jerzy Weyman (2017, {{ISBN|978-1-4704-2556-2}})
*185    ''Braid Foliations in Low-Dimensional Topology'', Douglas J. LaFountain, William W. Menasco (2017, {{ISBN|978-1-4704-3660-5}})
*186    ''Rational Points on Varieties'', Bjorn Poonen (2017, {{ISBN|978-1-4704-3773-2}})
*187    ''Introduction to Global Analysis: Minimal Surfaces in Riemannian Manifolds'', John Douglas Moore (2017, {{ISBN|978-1-4704-2950-8}})
*188    ''Introduction to Algebraic Geometry'', Steven Dale Cutkosky (2018, {{ISBN|978-1-4704-3518-9}})
*189    ''Characters of Solvable Groups'', I. Martin Isaacs (2018, {{ISBN|978-1-4704-3485-4}})
*190    '' Lectures  on Finite Fields'', Xiang-dong Hou (2018, {{ISBN|978-1-4704-4289-7}})
*191    '' Functional Analysis'', Theo Bühler, Dietmar A. Salamon (2018, {{ISBN|978-1-4704-4190-6}})
*192    '' Lectures on Navier-Stokes Equations'', Tai-Peng Tsai (2018, {{ISBN|978-1-4704-3096-2}})
*193    '' A Tour of Representation Theory'', Martin Lorenz (2018, {{ISBN|978-1-4704-3680-3}})
*194    '' Algebraic Statistics'', Seth Sullivant (2018, {{ISBN|978-1-4704-3517-2}})
*195    '' Combinatorial Reciprocity Theorems:An Invitation to Enumerative Geometric Combinatorics'', Matthias Beck, Raman Sanyal (2018, {{ISBN|978-1-4704-2200-4}})
*196    '' Convection-Diffusion Problems:An Introduction to Their Analysis and Numerical Solution'', Martin Stynes, David Stynes (2018, {{ISBN|978-1-4704-4868-4}})
*197    '' A Course on Partial Differential Equations'', Walter Craig (2018, {{ISBN|978-1-4704-4292-7}})

==See also==
*[[Graduate Texts in Mathematics]]

==Notes==
{{reflist}}

==External links==
* [http://bookstore.ams.org/GSM Graduate Studies in Mathematics on AMS Bookstore]

{{Authority control}}

[[Category:Series of mathematics books]]
[[Category:Mathematics-related lists]]</text>
      <sha1>exaa05v21k6rnv8hqugd3a24a4ys8pc</sha1>
    </revision>
  </page>
  <page>
    <title>Graph paper</title>
    <ns>0</ns>
    <id>293606</id>
    <revision>
      <id>871794558</id>
      <parentid>871794101</parentid>
      <timestamp>2018-12-03T13:51:15Z</timestamp>
      <contributor>
        <ip>180.191.114.253</ip>
      </contributor>
      <comment>ist all about bluecraft55</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7800">Graphing Paper[[File:Graph-paper-10sqsm-5sqin-4sqin.jpg|thumb|350px|Three styles of [[loose leaf]] graph paper: 10 squares per centimeter ("millimeter paper"), 5 squares per inch ("Engineering paper"), 4 squares per inch ("Quad paper")]]
[[File:Minecraft Cathedrale (7602271558).jpg|thumb|mcpe]]
'''Graph paper''', '''coordinate paper''', '''grid paper''', or '''squared paper''' is writing [[paper]] that is printed with fine lines making up a [[regular grid]]. The lines are often used as guides for for  [[mathematical function]]s or [[experimental data]] and drawing [[two-dimensional graph]]s. It is commonly found in mathematics and engineering education settings and in [[laboratory notebook]]s. Graph paper is available either as [[loose leaf]] paper or bound in [[notebook]]s.

== History ==
{{Minecraft}}
The first commercially published "coordinate paper" is usually attributed to Dr. Buxton of England, who patented paper, printed with a rectangular coordinate grid, in 1794.&lt;ref&gt;{{cite paper |url=http://web.calstatela.edu/curvebank/index/milestone.pdf |title=Milestones in the history of thematic cartography, statistical graphics, and data visualization |publisher=York University |first1=Michael |last1=Friendly |first2=Daniel J. |last2=Denis |page=13 |date=5 July 2006}}&lt;/ref&gt; A century later, E. H. Moore, a distinguished mathematician at the University of Chicago, advocated usage of paper with "squared lines" by students of high schools and universities.&lt;ref&gt;{{cite web |url=https://micromath.wordpress.com/2008/08/07/graphed-paper/ |title=Graphed Paper |work=Mathematics under the Microscope |first=Alexandre |last=Borovik |date=7 August 2008 |accessdate=25 March 2017}}&lt;/ref&gt; The 1906 edition of ''Algebra for Beginners'' by H. S. Hall and S. R. Knight included a strong statement that "the squared paper should be of good quality and accurately ruled to inches and tenths of an inch. Experience shows that anything on a smaller scale (such as 'millimeter' paper) is practically worthless in the hands of beginners."&lt;ref name="pballew-graph"&gt;{{cite book |chapterurl=http://www.pballew.net/arithme7.html#graphppr |chapter=Some Notes on graph paper |title=Math Words, and Some Other Words, of Interest |website=PBallew.net |first=Pat |last=Ballew |accessdate=25 March 2017}}&lt;/ref&gt;

The term "graph paper" did not catch on quickly in American usage. ''A School Arithmetic'' (1919) by H. S. Hall and F. H. Stevens had a chapter on graphing with "squared paper". ''Analytic Geometry'' (1937) by W. A. Wilson and J. A. Tracey used the phrase "coordinate paper". The term "squared paper" remained in British usage for longer; for example it was used in ''Public School Arithmetic'' (1961) by W. M. Baker and A. A. Bourne published in London.&lt;ref name="pballew-graph"/&gt; minecraft.apk

== For Minecraft ==
* '''Quad paper''', sometimes referred to as '''quadrille paper''' from French quadrillé, 'small square',&lt;ref&gt;{{cite encyclopedia |url=https://en.oxforddictionaries.com/definition/quadrille#quadrille_Noun_300 |title=quadrille |encyclopedia=Oxford Living Dictionaries |publisher=Oxford University Press |year=2017}}&lt;/ref&gt; is a common form of graph paper with a sparse grid printed in light blue or gray and right to the edge of the paper. In the U.S. and Canada, it often has two, four or five squares to the inch for work not needing too much detail. Metric paper with similarly sparse grid typically has one or two squares per centimeter.
*[[File:Minecraft Lego head (6790028921).jpg|thumb|by DestroyerCraft55]]'''Engineering paper''', or an '''Engineer's Pad''',&lt;ref name="mit-pset"&gt;{{cite web |url=http://web.mit.edu/me-ugoffice/communication/pset-format.pdf |title=The Preparation of Engineering Problem Sets |publisher=Massachusetts Institute of Technology |agency=Technical Communications in Mechanical Engineering |accessdate=25 March 2017}}&lt;/ref&gt; is traditionally printed on light green or tan translucent paper. It may have four, five or ten squares per inch. The grid lines are printed on the back side of each page and show through faintly to the front side.  Each page has an unprinted margin.  When [[Photocopier|photocopied]] or [[Image scanner|scanned]], the grid lines typically do not show up in the resulting copy, which often gives the work a neat, uncluttered appearance.  In the U.S. and Canada, some engineering professors require student [[homework]] to be completed on engineering paper.&lt;ref name="mit-pset"/&gt;&lt;ref&gt;{{cite web |url=http://eng.auburn.edu/cheweb/student/Homework_Format.pdf |title=Required Homework Format |agency=Department of Chemical Engineering |publisher=Auburn University |accessdate=25 March 2017}}&lt;/ref&gt;
* '''Millimeter paper''' has ten squares per centimeter and is used for [[technical drawing]]s.
* '''Hexagonal paper''' shows regular hexagons instead of squares. These can be used to map geometric [[Tiling by regular polygons|tiled or tesselated]] designs among other uses.
* '''Isometric graph paper''' or '''3D graph paper''' is a triangular graph paper which uses a series of three guidelines forming a 60° grid of small triangles. The triangles are arranged in groups of six to make hexagons. The name suggests the use for [[isometric projection|isometric views]] or pseudo-three-dimensional views. Among other functions, they can be used in the design of [[trianglepoint]] [[embroidery]]. It can be used to draw angles accurately.
* '''Logarithmic paper''' has rectangles drawn in varying widths corresponding to [[logarithmic scale]]s for [[semi-log plot]]s or [[log-log plot]]s.
* '''Normal [[probability]] paper''' is another graph paper with rectangles of variable widths. It is designed so that "the graph of the normal distribution function is represented on it by a straight line", i.e. it can be used for a [[normal probability plot]].&lt;ref name="eom-prob"&gt;{{cite web |url=http://www.encyclopediaofmath.org/index.php?title=Probability_graph_paper&amp;oldid=11790 |title=Probability graph paper |work=Encyclopedia of Mathematics |first=A. V. |last=Prokhorov |year=2011 |accessdate=18 January 2014}}&lt;/ref&gt;
* '''Polar coordinate paper''' has concentric circles divided into small arcs or 'pie wedges' to allow plotting in [[polar coordinate system|polar coordinates]].

In general, graphs showing grids are sometimes called '''Cartesian''' graphs because the square can be used to map measurements onto a [[cartesian coordinate system|Cartesian (x vs. y)]] coordinate system. It is also available without lines but with dots at the positions where the lines would intersect.

== Examples ==
&lt;gallery mode="packed" heights="180px"&gt;
File:graph-paper.svg|Regular graphing paper
File:Log paper.svg|Logarithmic graphing paper
File:Isometric graph paper, US letter size SVG.svg|Isometric graphing paper
File:Engineering-pad-simulation.gif|Engineering paper
File:Russian school graph paper.jpg|Squared [[exercise book]] used in [[Russia]]n schools (12 and 18 sheets)
File:Graph-ruled_composition_book,_4_squares_per_inch,_80_pages.jpg|Graph [[exercise book|composition book]] used in the [[United States]] (80 sheets)
File:Two styles of graph paper (5843580902).jpg|Two styles of [[loose leaf]] graph paper
&lt;/gallery&gt;

== See also ==
* [[Notebook]]
* [[Ruled paper]]
* [[Exercise book]]
* [[Examination book]]
* [[Laboratory notebook]]

== References ==
{{reflist}}

== External links ==
{{commons category|Graph paper}}
* [https://web.archive.org/web/20120401233424/http://www.peregraph.com/ Graph paper downloads] at Peregraph.com
* [http://www.print-graph-paper.com/ Graph paper downloads] at Print-graph-paper.com

{{Paper}}

{{Use dmy dates|date=March 2017}}

{{DEFAULTSORT:Graph Paper}}
[[Category:Printing and writing paper]]
[[Category:Engineering equipment]]
[[Category:Technical drawing]]
[[Category:Mathematical tools]]</text>
      <sha1>5uy2wc64zmzbp2l4dd1pfb5fujg5sgq</sha1>
    </revision>
  </page>
  <page>
    <title>Greninger chart</title>
    <ns>0</ns>
    <id>32089853</id>
    <revision>
      <id>587555293</id>
      <parentid>558413345</parentid>
      <timestamp>2013-12-24T20:29:02Z</timestamp>
      <contributor>
        <username>FrescoBot</username>
        <id>9021902</id>
      </contributor>
      <minor/>
      <comment>Bot: [[Wikipedia:Manual of Style/Layout#Standard appendices and footers|standard sections headers]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2123">[[File:GreningerChart.png|thumb|GreningerChart]]
In [[crystallography]], a '''Greninger chart'''&lt;ref&gt;It is named after Alden Buchanan Greninger (17 September 1907, [[Glendale, Oregon]]{{spaced ndash}}20 April 1998).&lt;/ref&gt; {{IPAc-en|ˈ|ɡ|r|ɛ|n|ɪ|ŋ|ər}} is a [[chart]] that allows angular relations between zones and planes in a [[crystal]] to be directly read from an [[x-ray diffraction]] photograph.

The Greninger chart is a simple trigonometric tool to determine ''g'' and ''d'' for a fixed sample-to-film distance. (If one uses a 2-d detector the problem of determining ''g'' and ''d''&lt;!--WHAT are "g" and "d"?--&gt; could be solved mathematically using the equations which generate the Greninger chart) A new chart must be generated for different sample to detector distances. (2''s'' is 2''q'' for the diffraction peak and tan&amp;nbsp;''m'' is ''x''/''y''&lt;!--"s", "q", "m", "x", and "y" are not defined here!! --&gt; for the Cartesian coordinates of the diffraction peak.) The Greninger chart gives directly the two angles needed to plot poles on the [[Wulff net]]&lt;!--WHAT is a Wulff net?  Whoever wrote this seems to assume the reader knows.--&gt;.  It is critical to keep track of the relative arrangement of the sample to the film, if photographic film is used then this is achieved by cutting the corner of the film. For [[Instant film|Polaroid]] film one must make a note of the arrangement of the face of the film in the camera.

==See also==
* [[Bernal chart]]

== References ==
{{Reflist}}

==Sources==
* Greninger A. B. (1935). ''Zeitschrift fur Kristallographie'' '''91''': 424.

== External links ==
* http://www.answers.com/topic/greninger-chart
* http://www.eng.uc.edu/~gbeaucag/Classes/XRD/Labs/Lab3Laue.html
* http://www-xray.fzu.cz/xraygroup/www/grchart.html
&lt;!--biographical sources:
http://histsoc.stanford.edu/pdfmem/ShepardOC.pdf
http://gadget1945.websitetoolbox.com/post?id=132141
http://freepages.genealogy.rootsweb.ancestry.com/~herbarkin/Wright%20html/d0069/g0000063.html
--&gt;

[[Category:Articles created via the Article Wizard]]
[[Category:Trigonometry]]
[[Category:X-rays]]
[[Category:Diffraction]]</text>
      <sha1>6go0fdrr7i1vavk8hbemfrvzapz8rh4</sha1>
    </revision>
  </page>
  <page>
    <title>HAIFA construction</title>
    <ns>0</ns>
    <id>54580441</id>
    <revision>
      <id>830980764</id>
      <parentid>813625778</parentid>
      <timestamp>2018-03-18T01:39:29Z</timestamp>
      <contributor>
        <username>Widefox</username>
        <id>1588193</id>
      </contributor>
      <comment>/* top */ MOS bold</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1393">{{Redirect|HAIFA|3=Haifa (disambiguation)}}{{Refimprove
| date = July 2017
}}
The '''HAIFA construction''' (''hash iterative framework'') is a cryptographic structure used in the design of [[Cryptographic hash function|hash functions]]. It is one of the modern alternatives to the [[Merkle–Damgård construction]],&lt;ref&gt;{{Cite conference|last=Biham|first=Eli|last2=Dunkelman|first2=Orr|date=24 August 2006|title=A Framework for Iterative Hash Functions - HAIFA|url=https://eprint.iacr.org/2007/278|conference=Second NIST Cryptographic Hash Workshop|via=Cryptology ePrint Archive: Report 2007/278}}&lt;/ref&gt; avoiding its weaknesses like [[length extension attack]]s. The construction was designed by [[Eli Biham]] and Orr Dunkelman in 2007.

Three of the 14 second round candidates in the [[NIST hash function competition]] were based on HAIFA constructions ([[BLAKE (hash function)|BLAKE]], SHAvite-3, ECHO). Other hash functions based on it are LAKE, Sarmal, [[SWIFFT|SWIFFTX]] and HNF-256. The construction of [[Skein (hash function)|Skein]] ([[Unique Block Iteration]]) is similar to HAIFA.&lt;ref&gt;Jean-Philippe Aumasson, Willi Meier, Raphael Phan, Luca Henzen: ''The Hash Function BLAKE'', p. 35&lt;/ref&gt; Another alternative construction is the [[sponge construction]].

== References ==
&lt;references /&gt;

{{crypto-stub}}
[[Category:Cryptographic hash functions]]
[[Category:Theory of cryptography]]</text>
      <sha1>1r4aipdpmbx2xgxbxwqousnmtv85vi5</sha1>
    </revision>
  </page>
  <page>
    <title>Heat kernel signature</title>
    <ns>0</ns>
    <id>34060358</id>
    <revision>
      <id>843695991</id>
      <parentid>823577437</parentid>
      <timestamp>2018-05-30T20:55:52Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>Contemporary Mathematics is not a journal</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15235">A '''heat kernel signature (HKS)''' is a feature descriptor for use in deformable [[Shape analysis (digital geometry)|shape analysis]] and belongs to the group of [[spectral shape analysis]] methods. For each point in the shape, HKS defines its [[feature vector]] representing the point's local and global geometric properties. Applications include segmentation, classification, structure discovery, shape matching and shape retrieval.

HKS was introduced in 2009 by Jian Sun, Maks Ovsjanikov and [[Leonidas Guibas]].&lt;ref name="sun2009concise"&gt;{{cite conference
 | author = Sun, J. and Ovsjanikov, M. and Guibas, L.
 | year = 2009
 | title = A Concise and Provably Informative Multi-Scale Signature-Based on Heat Diffusion
 | booktitle = Computer Graphics Forum
 | volume = 28
 | number = 5
 | pages = 1383–1392
}}&lt;/ref&gt; It is based on [[heat kernel]], which is a fundamental solution to the [[heat equation]]. HKS is one of the many recently introduced shape descriptors which are based on the [[Laplace–Beltrami operator]] associated with the shape.&lt;ref name="bronstein2011spectral"&gt;{{cite journal
 | author = Alexander M. Bronstein
 | year = 2011
 | title = Spectral descriptors for deformable shapes
 | arxiv = 1110.5015| bibcode = 2011arXiv1110.5015B
 }}&lt;/ref&gt;

== Overview ==
[[Shape analysis (digital geometry)|Shape analysis]] is the field of automatic digital analysis of shapes, e.g., 3D objects. For many shape analysis tasks (such as shape matching/retrieval), [[feature vector]]s for certain key points are used instead of using the complete [[3D model]] of the shape. An important requirement of such feature descriptors is for them to be invariant under certain transformations. For [[rigid transformation]]s, commonly used feature descriptors  include [[shape context]], spin images, integral volume descriptors and multiscale local features, among others.&lt;ref name="bronstein2011spectral" /&gt; HKS allows [[isometric transformation]]s which generalizes rigid transformations.

HKS is based on the concept of [[heat diffusion]] over a surface. Given an initial heat distribution &lt;math&gt; u_0(x)&lt;/math&gt; over the surface, the heat kernel &lt;math&gt; h_t(x,y) &lt;/math&gt; relates the amount of heat transferred from &lt;math&gt;x&lt;/math&gt; to &lt;math&gt;y&lt;/math&gt; after time &lt;math&gt;t&lt;/math&gt;. The heat kernel is invariant under isometric transformations and stable under small perturbations to the isometry.&lt;ref name="sun2009concise" /&gt; In addition, the heat kernel fully characterizes shapes up to an isometry and represents increasingly global properties of the shape with increasing time.&lt;ref name="grigor2006heat"&gt;{{cite conference
 | last = Grigor'yan | first = Alexander
 | contribution = Heat kernels on weighted manifolds and applications
 | doi = 10.1090/conm/398/07486
 | mr = 2218016
 | pages = 93–191
 | publisher = American Mathematical Society | location = Providence, RI
 | series = Contemporary Mathematics
 | title = The ubiquitous heat kernel
 | volume = 398
 | year = 2006}}&lt;/ref&gt; Since &lt;math&gt;h_t(x,y)&lt;/math&gt; is defined for a pair of points over a temporal domain, using heat kernels directly as features would lead to a high complexity. HKS instead restricts itself to just the temporal domain by considering only &lt;math&gt;h_t(x,x)&lt;/math&gt;. HKS inherits most of the properties of heat kernels under certain conditions.&lt;ref name="sun2009concise" /&gt;

== Technical details ==
The [[heat diffusion]] equation over a [[Compact space|compact]] [[Riemannian manifold]] &lt;math&gt;M&lt;/math&gt; (possibly with a boundary) is given by,
:&lt;math&gt;
\left(\Delta - \frac{\partial}{\partial t} \right) u(x,t) = 0
&lt;/math&gt;
where &lt;math&gt;\Delta&lt;/math&gt; is the [[Laplace–Beltrami operator]] and &lt;math&gt;u(x,t)&lt;/math&gt; is the heat distribution at a point &lt;math&gt;x&lt;/math&gt; at time &lt;math&gt;t&lt;/math&gt;. The solution to this equation can be expressed as,&lt;ref name="sun2009concise" /&gt;
:&lt;math&gt;
u(x,t) = \int h_t(x,y)u_0(y)dy.
&lt;/math&gt;
The eigen decomposition of the heat kernel is expressed as,
:&lt;math&gt;
h_t(x,y) = \sum_{i=0}^{\infty} \exp(-\lambda_i t) \phi_i(x) \phi_i(y)
&lt;/math&gt;
where &lt;math&gt;\lambda_i&lt;/math&gt; and &lt;math&gt;\phi_i&lt;/math&gt; are the &lt;math&gt;i^{th}&lt;/math&gt; eigenvalue and eigenfunction of &lt;math&gt;\Delta&lt;/math&gt;. The heat kernel fully characterizes a surface up to an isometry: For any [[surjective map]] &lt;math&gt;T:M\rightarrow N&lt;/math&gt; between two Riemannian manifolds &lt;math&gt;M&lt;/math&gt; and &lt;math&gt;N&lt;/math&gt;, if &lt;math&gt;h_t(x,y) = h_t(T(x),T(y))&lt;/math&gt; then &lt;math&gt;T&lt;/math&gt; is an isometry, and vice versa.&lt;ref name="sun2009concise" /&gt; For a concise feature descriptor, HKS restricts the heat kernel only to the temporal domain,
:&lt;math&gt;
h_t(x,x) = \sum_{i=0}^{\infty} \exp(-\lambda_i t) \phi_i^2(x).
&lt;/math&gt;
HKS, similar to the heat kernel, characterizes surfaces under the condition that the eigenvalues of &lt;math&gt;\Delta&lt;/math&gt; for &lt;math&gt;M&lt;/math&gt; and &lt;math&gt;N&lt;/math&gt; are non-repeating. The terms &lt;math&gt;\exp(-\lambda_i t)&lt;/math&gt; can be intuited as a bank of low-pass filters, with &lt;math&gt;\lambda_i&lt;/math&gt; determining the cutoff frequencies.&lt;ref name="bronstein2011spectral"/&gt;

=== Practical considerations ===
Since &lt;math&gt;h_t(x,x)&lt;/math&gt; is, in general, a non-parametric continuous function, HKS is in practice represented as a discrete sequence of &lt;math&gt;\{h_{t_1}(x,x),\ldots,h_{t_n}(x,x)\}&lt;/math&gt; values sampled at times &lt;math&gt;t_1,\ldots,t_n&lt;/math&gt;.

In most applications, the underlying manifold for an object is not known. The HKS can be computed if a [[Polygon mesh|mesh]] representation of the manifold is available, by using a discrete approximation to &lt;math&gt;\Delta&lt;/math&gt; and using the discrete analogue of the heat equation. In the discrete case, the Laplace–Beltrami operator is a sparse matrix and can be written as,&lt;ref name="sun2009concise" /&gt;
:&lt;math&gt;
L = A^{-1}W
&lt;/math&gt;
where &lt;math&gt;A&lt;/math&gt; is a positive diagonal matrix with entries &lt;math&gt;A(i,i)&lt;/math&gt; corresponding to the area of the triangles in the mesh sharing the vertex &lt;math&gt;i&lt;/math&gt;, and &lt;math&gt;W&lt;/math&gt; is a symmetric semi-definite weighting matrix. &lt;math&gt;L&lt;/math&gt; can be decomposed into &lt;math&gt;L = \Phi \Lambda \Phi^T A&lt;/math&gt;, where &lt;math&gt;\Lambda&lt;/math&gt; is a diagonal matrix of the eigenvalues of &lt;math&gt;L&lt;/math&gt; arranged in the ascending order, and &lt;math&gt;\Phi&lt;/math&gt; is the matrix with the corresponding orthonormal eigenvectors. The discrete heat kernel is the matrix given by,
:&lt;math&gt;
K_t = \Phi \exp(-t\Lambda) \Phi^T.
&lt;/math&gt;
The elements &lt;math&gt;k_t(i,j)&lt;/math&gt; represents the heat diffusion between vertices &lt;math&gt;i&lt;/math&gt; and &lt;math&gt;j&lt;/math&gt; after time &lt;math&gt;t&lt;/math&gt;. The HKS is then given by the diagonal entries of this matrix, sampled at discrete time intervals. Similar to the continuous case, the discrete HKS is robust to noise.&lt;ref name="sun2009concise" /&gt;

== Limitations ==

=== Non-repeating eigenvalues ===
The main property that characterizes surfaces using HKS up to an isometry holds only when the eigenvalues of the surfaces are non-repeating. There are certain surfaces (especially those with symmetry) where this condition is violated. A sphere is a simple example of such a surface.

=== Time parameter selection ===
The time parameter in the HKS is closely related to the scale of global information. However, there is no direct way to choose the time discretization. The existing method chooses time samples logarithmically which is a heuristic with no guarantees&lt;ref name="aubry2011wks"&gt;{{cite conference
 | author = Aubry, M. and Schlickewei, U. and Cremers, D.
 | year = 2011
 | title = The Wave Kernel Signature—A Quantum Mechanical Approach to Shape Analysis
 | booktitle = IEEE International Conference on Computer Vision (ICCV) - Workshop on Dynamic Shape Capture and Analysis (4DMOD)
}}&lt;/ref&gt;

=== Time complexity ===
The discrete heat kernel requires eigendecomposition of a matrix of size &lt;math&gt;n \times n&lt;/math&gt;, where &lt;math&gt;n&lt;/math&gt; is the number of vertices in the mesh representation of the manifold. Computing the eigendecomposition is an expensive operation, especially as &lt;math&gt;n&lt;/math&gt; increases.
Note, however, that because of the inverse exponential dependence on the eigenvalue, typically only a small (less than 100) eigenvectors are sufficient to obtain a good approximation of the HKS.

=== Non-isometric transformations ===
The performance guarantees for HKS only hold for truly isometric transformations. However, deformations for real shapes are often not isometric. A simple example of such transformation is closing of the fist by a person, where the geodesic distances between two fingers changes.

== Relation with other methods&lt;ref name="bronstein2011spectral" /&gt; ==

=== Curvature ===
The (continuous) HKS at a point &lt;math&gt;x&lt;/math&gt;, &lt;math&gt;h_t(x,x)&lt;/math&gt; on the Riemannian manifold is related to the [[scalar curvature]] &lt;math&gt;s(x)&lt;/math&gt; by,
:&lt;math&gt;
h_t(x,x) = \frac{1}{4\pi t} + \frac{s(x)}{12\pi} + O(t).
&lt;/math&gt;
Hence, HKS can as be interpreted as the curvature of &lt;math&gt;x&lt;/math&gt; at scale &lt;math&gt;t&lt;/math&gt;.

=== Wave kernel signature (WKS) ===
The WKS&lt;ref name="aubry2011wks" /&gt; follows a similar idea to the HKS, replacing the heat equation with the [[Schrödinger equation|Schrödinger wave equation]],
:&lt;math&gt;
\left( i\Delta + \frac{\partial}{\partial t} \right) \psi(x,t) = 0
&lt;/math&gt;
where &lt;math&gt;\psi(x,t)&lt;/math&gt; is the complex wave function. The average probability of measuring the particle at a point &lt;math&gt;x&lt;/math&gt; is given by,
:&lt;math&gt;
p(x) = \sum_{i=0}^{\infty} f^2(\lambda_i) \phi_i^2(x)
&lt;/math&gt;
where &lt;math&gt;f&lt;/math&gt; is the initial energy distribution. By fixing a family of these energy distributions &lt;math&gt;f_i(x)&lt;/math&gt;, the WKS can be obtained as a discrete sequence &lt;math&gt;\{ p_{f_1}(x),\ldots,p_{f_n}(x)\}&lt;/math&gt;. Unlike HKS, the WKS can be intuited as a set of band-pass filters leading to better feature localization. However, the WKS does not represent large-scale features well (as they are ''filtered'' out) yielding poor performance at shape matching applications.

=== Global point signature (GPS) ===
Similar to the HKS, the GPS&lt;ref name="rustamov2007gps"&gt;{{cite conference
 | author = Rustamov, R.M.
 | year = 2007
 | title = Laplace–Beltrami eigenfunctions for deformation invariant shape representation
 | booktitle = Proceedings of the fifth Eurographics symposium on Geometry processing
 | pages = 225–233
 | publisher = Eurographics Association 
}}&lt;/ref&gt; is based on the Laplace-Beltrami operator. GPS at a point &lt;math&gt;x&lt;/math&gt; is a vector of scaled eigenfunctions of the Laplace–Beltrami operator computed at &lt;math&gt;x&lt;/math&gt;. The GPS is a global feature whereas the scale of the HKS can be varied by varying the time parameter for heat diffusion. Hence, the HKS can be used in partial shape matching applications whereas the GPS cannot.

=== Spectral graph wavelet signature (SGWS) ===
SGWS&lt;ref name="Li13SGWS"&gt;{{cite journal
 |author1=C. Li |author2=A. Ben Hamza | year = 2013
 | title = A multiresolution descriptor for deformable 3D shape retrieval
 | journal = The Visual Computer
 | pages = 513–524
}}&lt;/ref&gt; provides a general form for [[Spectral shape analysis|spectral descriptors]], where one can obtain HKS by specifying the filter function. SGWS is a multiresolution local descriptor that is not only isometric invariant, but also compact, easy to compute and combines the advantages of both band-pass and low-pass filters.

== Extensions ==

=== Scale invariance ===
Even though the HKS represents the shape at multiple scales, it is not inherently scale invariant. For example, the HKS for a shape and its scaled version are not the same without pre-normalization. A simple way to ensure scale invariance is by pre-scaling each shape to have the same surface area (e.g. 1). Using the notation above, this means:

&lt;math&gt;
\begin{align}
 s &amp;= \sum_j A_j \\
 A &amp;= A / s \\
 \lambda_i &amp;= s \lambda_i \text{ for each } i \\
 \phi_i &amp;= \sqrt{s} \phi_i \text{ for each } i \\
\end{align}
&lt;/math&gt;

Alternatively, scale-invariant version of the HKS can also be constructed by generating a [[Scale space representation]].&lt;ref name="bronstein2010si"&gt;{{cite conference
 |author1=Bronstein, M.M.  |author2=Kokkinos, I.
 | year = 2010
 | title = Scale-invariant heat kernel signatures for non-rigid shape recognition
 | booktitle = Computer Vision and Pattern Recognition (CVPR), 2010
 | pages = 1704–1711
 | publisher = IEEE
}}&lt;/ref&gt; In the scale-space, the HKS of a scaled shape corresponds to a translation up to a multiplicative factor. The Fourier transform of this HKS changes the [[Time translation|time-translation]] into the complex plane, and the dependency on translation can be eliminated by considering the modulus of the transform.
{{YouTube|id=kDfxrrh1h38|title=Demo of Scale-invariant HKS}}. 
An alternative scale invariant HKS can be established by working out its construction through a scale invariant metric, as defined in &lt;ref name="raviv2013scale"&gt;{{cite journal
 | author = Y. Aflalo,  R. Kimmel and R. Raviv
 | year = 2013
 | title = Scale invariant geometry for non-rigid shapes
 | journal = SIAM Journal of Imaging Science (SIAG on Imaging Science 2016 Best Paper Prize)
 | pages = 6(3):1579-1597
 | publisher = SIAM
}}&lt;/ref&gt;.

=== Volumetric HKS ===
The HKS is defined for a boundary surface of a 3D shape, represented as a 2D Riemannian manifold. Instead of considering only the boundary, the entire volume of the 3D shape can be considered to define the volumetric version of the HKS.&lt;ref name="raviv2010volumetric"&gt;{{cite conference
 | author = Raviv, D. and Bronstein, M.M. and Bronstein, A.M. and Kimmel, R.
 | year = 2010
 | title = Volumetric heat kernel signatures
 | booktitle = Proceedings of the ACM workshop on 3D object retrieval
 | pages = 30–44
 | publisher = ACM
}}&lt;/ref&gt; The Volumetric HKS is defined analogous to the normal HKS by considering the heat equation over the entire volume (as a 3-submanifold) and defining a [[Neumann boundary conditions|Neumann boundary condition]] over the 2-manifold boundary of the shape. Volumetric HKS characterizes transformations up to a volume isometry, which represent the transformation for real 3D objects more faithfully than boundary isometry.&lt;ref name="raviv2010volumetric" /&gt;

=== Shape Search ===
The scale-invariant HKS features can be used in the [[bag of words model in computer vision|bag-of-features]] model for shape retrieval applications.&lt;ref name="bronsteing2011google"&gt;{{cite journal
 | author = Bronstein, A.M. and Bronstein, M.M. and Guibas, L.J. and Ovsjanikov, M.
 | year = 2011
 | title = Shape google: Geometric words and expressions for invariant shape retrieval
 | journal = ACM Transactions on Graphics
 | volume = 30
 | number = 1
 | publisher = ACM
}}&lt;/ref&gt; The features are used to construct geometric words by taking into account their spatial relations, from which shapes can be constructed (analogous to using features as words and shapes as sentences). Shapes themselves are represented using compact binary codes to form an indexed collection. Given a query shape, similar shapes in the index with possibly isometric transformations can be retrieved by using the Hamming distance of the code as the nearness-measure.

== References ==
&lt;references /&gt;

[[Category:Image processing]]
[[Category:Heat transfer]]
[[Category:Digital geometry]]
[[Category:Topology]]
[[Category:Differential geometry]]</text>
      <sha1>rk5hg7yyrmmsc277pkvmb1axv1xqyy2</sha1>
    </revision>
  </page>
  <page>
    <title>IEEE 754-1985</title>
    <ns>0</ns>
    <id>15189</id>
    <revision>
      <id>869649222</id>
      <parentid>865826859</parentid>
      <timestamp>2018-11-19T22:29:57Z</timestamp>
      <contributor>
        <username>Kvng</username>
        <id>910180</id>
      </contributor>
      <minor/>
      <comment>/* See also */ rm already linked</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="24162">'''IEEE 754-1985''' was an industry [[technical standard|standard]] for representing [[floating-point]] numbers in [[computers]], officially adopted in 1985 and superseded in 2008 by [[IEEE 754-2008]]. During its 23 years, it was the most widely used format for floating-point computation. It was implemented in software, in the form of floating-point [[library (computing)|libraries]], and in hardware, in the [[instruction (computer science)|instructions]] of many [[CPU]]s and [[floating-point unit|FPUs]]. The first [[integrated circuit]] to implement the draft of what was to become IEEE&amp;nbsp;754-1985 was the [[Intel 8087]].

IEEE 754-1985 represents numbers in [[binary numeral system|binary]], providing definitions for four levels of precision, of which the two most commonly used are:

{| class="wikitable"
|-
!Level
!Width
!Range at full precision
!Precision{{efn|Precision: The number of decimal digits precision is calculated via number_of_mantissa_bits * Log&lt;sub&gt;10&lt;/sub&gt;(2). Thus ~7.2 and ~15.9 for single and double precision respectively.}}
|-
|Single precision
|32 bits
|±1.18{{e|-38}} to ±3.4{{e|38}}
|Approximately 7 decimal digits
|-
|Double precision
|64 bits
|±2.23{{e|-308}} to ±1.80{{e|308}}
|Approximately 16 decimal digits
|}

The standard also defines representations for positive and negative [[infinity]], a "[[negative zero]]", five exceptions to handle invalid results like [[division by zero]], special values called [[NaN]]s for representing those exceptions, [[denormal numbers]] to represent numbers smaller than shown above, and four [[rounding]] modes.

== Representation of numbers ==

[[Image:IEEE 754 Single Floating Point Format.svg|right|frame|The number 0.15625 represented as a single-precision IEEE 754-1985 floating-point number. See text for explanation.]]
[[Image:IEEE 754 Double Floating Point Format.svg|right|frame|The three fields in a 64bit IEEE 754 float]]

Floating-point numbers in IEEE&amp;nbsp;754 format consist of three fields: a [[sign bit]], a [[exponent bias|biased exponent]], and a fraction. The following example illustrates the meaning of each.

The decimal number 0.15625&lt;sub&gt;10&lt;/sub&gt; represented in binary is 0.00101&lt;sub&gt;2&lt;/sub&gt; (that is, 1/8 + 1/32). (Subscripts indicate the number [[radix|base]].) Analogous to [[scientific notation]], where numbers are written to have a single non-zero digit to the left of the decimal point, we rewrite this number so it has a single 1 bit to the left of the "binary point". We simply multiply by the appropriate power of 2 to compensate for shifting the bits left by three positions:

: &lt;math&gt;0.00101_2 = 1.01_2 \times 2^{-3}&lt;/math&gt;

Now we can read off the fraction and the exponent: the fraction is .01&lt;sub&gt;2&lt;/sub&gt; and the exponent is −3.

As illustrated in the pictures, the three fields in the IEEE&amp;nbsp;754 representation of this number are:

: ''sign'' = 0, because the number is positive. (1 indicates negative.)
: ''biased exponent'' = −3 + the "bias". In '''single precision''', the bias is '''127''', so in this example the biased exponent is 124; in '''double precision''', the bias is '''1023''', so the biased exponent in this example is 1020.
: ''fraction'' = .01000…&lt;sub&gt;2&lt;/sub&gt;.

IEEE&amp;nbsp;754 adds a [[offset binary|bias]] to the exponent so that numbers can in many cases be compared conveniently by the same hardware that compares signed [[2's-complement]] integers. Using a biased exponent, the lesser of two positive floating-point numbers will come out "less than" the greater following the same ordering as for [[sign and magnitude]] integers. If two floating-point numbers have different signs, the sign-and-magnitude comparison also works with biased exponents. However, if both biased-exponent floating-point numbers are negative, then the ordering must be reversed. If the exponent were represented as, say, a 2's-complement number, comparison to see which of two numbers is greater would not be as convenient.

The leading 1 bit is omitted since all numbers except zero start with a leading 1; the leading 1 is implicit and doesn't actually need to be stored which gives an extra bit of precision for "free."

=== Zero ===

The number zero is represented specially:

: ''sign'' = 0 for [[signed zero|positive zero]], 1 for [[signed zero|negative zero]].
: ''biased exponent'' = 0.
: ''fraction'' = 0.

=== Denormalized numbers ===

The number representations described above are called ''normalized,'' meaning that the implicit leading binary digit is a 1. To reduce the loss of precision when an [[Arithmetic underflow|underflow]] occurs, IEEE&amp;nbsp;754 includes the ability to represent fractions smaller than are possible in the normalized representation, by making the implicit leading digit a 0. Such numbers are called [[denormal numbers|denormal]]. They don't include as many [[significant digits]] as a normalized number, but they enable a gradual loss of precision when the result of an [[Floating-point arithmetic#Floating-point arithmetic operations|arithmetic operation]] is not exactly zero but is too close to zero to be represented by a normalized number.

A denormal number is represented with a biased exponent of all 0 bits, which represents an exponent of −126 in single precision (not −127), or −1022 in double precision (not −1023).&lt;ref&gt;{{cite book|last=Hennessy|title=Computer Organization and Design|publisher=Morgan Kaufmann|page=270}}&lt;/ref&gt; In contrast, the smallest biased exponent representing a normal number is 1 (see [[#Examples|examples]] below).

== Representation of non-numbers ==

The biased-exponent field is filled with all 1 bits to indicate either infinity or an invalid result of a computation.

=== Positive and negative infinity ===

[[Extended real line|Positive and negative infinity]] are represented thus:

: ''sign'' = 0 for positive infinity, 1 for negative infinity.
: ''biased exponent'' = all 1 bits.
: ''fraction'' = all 0 bits.

=== NaN ===

Some operations of [[Floating-point arithmetic#Floating-point arithmetic operations|floating-point arithmetic]] are invalid, such as taking the square root of a negative number. The act of reaching an invalid result is called a floating-point ''exception.'' An exceptional result is represented by a special code called a NaN, for "[[Not a Number]]". All NaNs in IEEE&amp;nbsp;754-1985 have this format:

: ''sign'' = either 0 or 1.
: ''biased exponent'' = all 1 bits.
: ''fraction'' = anything except all 0 bits (since all 0 bits represents infinity).

== Range and precision ==
[[File:IEEE 754 relative precision.svg|thumb|Relative precision of single (binary32) and double precision (binary64) numbers, compared with decimal representations using a fixed number of [[significant digits]]. Relative precision is defined here as ulp(''x'')/''x'', where ulp(''x'') is the [[unit in the last place]] in the representation of ''x'', i.e. the gap between ''x'' and the next representable number.]]
Precision is defined as the minimum difference between two successive mantissa representations; thus it is a function only in the mantissa; while the gap is defined as the difference between two successive numbers.&lt;ref&gt;{{citation |title=Computer Arithmetic |author1=Hossam A. H. Fahmy |author2=Shlomo Waser |author3=Michael J. Flynn |url=http://arith.stanford.edu/~hfahmy/webpages/arith_class/arith.pdf |accessdate=2011-01-02 |deadurl=yes |archiveurl=https://web.archive.org/web/20101008203307/http://arith.stanford.edu/~hfahmy/webpages/arith_class/arith.pdf |archivedate=2010-10-08}}&lt;/ref&gt;

=== Single precision ===
[[Single-precision]] numbers occupy 32 bits. In single precision:
* The positive and negative numbers closest to zero (represented by the denormalized value with all 0s in the exponent field and the binary value 1 in the fraction field) are
*: ±2&lt;sup&gt;&amp;minus;149&lt;/sup&gt; ≈ ±1.40130{{e|−45}}
* The positive and negative normalized numbers closest to zero (represented with the binary value 1 in the exponent field and 0 in the fraction field) are
*: ±2&lt;sup&gt;&amp;minus;126&lt;/sup&gt; ≈ ±1.17549{{e|−38}}
* The finite positive and finite negative numbers furthest from zero (represented by the value with 254 in the exponent field and all 1s in the fraction field) are
*: ±(1−2&lt;sup&gt;−24&lt;/sup&gt;) &amp;times; 2&lt;sup&gt;128&lt;/sup&gt;&lt;ref name="Kahan"&gt;{{Cite journal
  | author = William Kahan |author-link=William Kahan
  | title = Lecture Notes on the Status of IEEE 754
  | version =  October 1, 1997 3:36 am
  | publisher = Elect. Eng. &amp; Computer Science University of California
  | url = http://www.cs.berkeley.edu/~wkahan/ieee754status/IEEE754.PDF
  | format = PDF
  | accessdate = 2007-04-12 }}&lt;/ref&gt; ≈ ±3.40282{{e|38}}

Some example range and gap values for given exponents in single precision:

{| class="wikitable" style="text-align:right;"
|- style="text-align:center;"
! Actual Exponent (unbiased)
! Exp (biased)
! Minimum
! Maximum
! Gap
|-
| 0
| 127
| 1
| ≈ 1.999999880791
| ≈ 1.19209e-7
|-
| 1
| 128
| 2
| ≈ 3.999999761581
| ≈ 2.38419e-7
|-
| 2
| 129
| 4
| ≈ 7.999999523163
| ≈ 4.76837e-7
|-
| 10
| 137
| 1024
| ≈ 2047.999877930
| ≈ 1.22070e-4
|-
| 11
| 138
| 2048
| ≈ 4095.999755859
| ≈ 2.44141e-4
|-
| 23
| 150
| 8388608
| 16777215
| 1
|-
| 24
| 151
| 16777216
| 33554430
| 2
|-
| 127
| 254
| ≈ 1.70141e38
| ≈ 3.40282e38
| ≈ 2.02824e31
|}

As an example, 16,777,217 can not be encoded as a 32-bit float as it will be rounded to 16,777,216. This shows why floating point arithmetic is unsuitable for accounting software.  However, all integers within the representable range that are a power of 2 can be stored in a 32-bit float without rounding.

=== Double precision ===

[[Double-precision]] numbers occupy 64 bits. In double precision:
* The positive and negative numbers closest to zero (represented by the denormalized value with all 0s in the Exp field and the binary value 1 in the Fraction field) are
*: ±2&lt;sup&gt;&amp;minus;1074&lt;/sup&gt; ≈ ±4.94066{{e|−324}}
* The positive and negative normalized numbers closest to zero (represented with the binary value 1 in the Exp field and 0 in the fraction field) are
*: ±2&lt;sup&gt;&amp;minus;1022&lt;/sup&gt; ≈ ±2.22507{{e|−308}}
* The finite positive and finite negative numbers furthest from zero (represented by the value with 2046 in the Exp field and all 1s in the fraction field) are
*: ±(1−2&lt;sup&gt;−53&lt;/sup&gt;) &amp;times; 2&lt;sup&gt;1024&lt;/sup&gt;&lt;ref name="Kahan" /&gt; ≈ ±1.79769{{e|308}}

Some example range and gap values for given exponents in double precision:

{| class="wikitable" style="text-align:right;"
|- style="text-align:center;"
! Actual Exponent (unbiased)
! Exp (biased)
! Minimum
! Maximum
! Gap
|-
| 0
| 1023
| 1
| ≈ 1.999999999999999777955
| ≈ 2.22045e-16
|-
| 1
| 1024
| 2
| ≈ 3.999999999999999555911
| ≈ 4.44089e-16
|-
| 2
| 1025
| 4
| ≈ 7.999999999999999111822
| ≈ 8.88178e-16
|-
| 10
| 1033
| 1024
| ≈ 2047.999999999999772626
| ≈ 2.27374e-13
|-
| 11
| 1034
| 2048
| ≈ 4095.999999999999545253
| ≈ 4.54747e-13
|-
| 52
| 1075
| 4503599627370496	
| 9007199254740991
| 1
|-
| 53
| 1076
| 9007199254740992	
| 18014398509481982
| 2
|-
| 1023
| 2046
| ≈ 8.98847e307
| ≈ 1.79769e308
| ≈ 1.99584e292
|}

=== Extended formats ===

The standard also recommends extended format(s) to be used to perform internal computations at a higher precision than that required for the final result, to minimise round-off errors: the standard only specifies minimum precision and exponent requirements for such formats. The [[x87]] [[extended precision|80-bit extended format]] is the most commonly implemented extended format that meets these requirements.

== Examples ==

Here are some examples of single-precision IEEE 754 representations:

{| class="wikitable"
|-
! Type
! Sign
! Actual Exponent
! Exp (biased)
! Exponent field
! Significand (fraction field)
! Value
|-
| Zero
| style="text-align:center;"| 0
| −127
| style="text-align:right;"| 0
| 0000 0000
| 000 0000 0000 0000 0000 0000
| 0.0
|-
| [[Negative zero]]
| style="text-align:center;"| 1
| −127
| style="text-align:right;"| 0
| 0000 0000
| 000 0000 0000 0000 0000 0000
| &amp;minus;0.0
|-
| One
| style="text-align:center;"| 0
| 0
| style="text-align:right;"| 127
| 0111 1111
| 000 0000 0000 0000 0000 0000
| 1.0
|-
| Minus One
| style="text-align:center;"| 1
| 0
| style="text-align:right;"| 127
| 0111 1111
| 000 0000 0000 0000 0000 0000
| &amp;minus;1.0
|-
| Smallest [[Denormal number|denormalized number]]
| style="text-align:center;"| *
| −126
| style="text-align:right;"| 0
| 0000 0000
| 000 0000 0000 0000 0000 0001
| ±2&lt;sup&gt;&amp;minus;23&lt;/sup&gt; &amp;times; 2&lt;sup&gt;&amp;minus;126&lt;/sup&gt; = ±2&lt;sup&gt;&amp;minus;149&lt;/sup&gt; ≈ ±1.4{{e|-45}}
|-
| "Middle" denormalized number
| style="text-align:center;"| *
| −126
| style="text-align:right;"| 0
| 0000 0000
| 100 0000 0000 0000 0000 0000
| ±2&lt;sup&gt;&amp;minus;1&lt;/sup&gt; &amp;times; 2&lt;sup&gt;&amp;minus;126&lt;/sup&gt; = ±2&lt;sup&gt;&amp;minus;127&lt;/sup&gt; ≈ ±5.88{{e|-39}}
|-
| Largest denormalized number
| style="text-align:center;"| *
| −126
| style="text-align:right;"| 0
| 0000 0000
| 111 1111 1111 1111 1111 1111
| ±(1−2&lt;sup&gt;−23&lt;/sup&gt;) &amp;times; 2&lt;sup&gt;−126&lt;/sup&gt; ≈ ±1.18{{e|-38}}
|-
| Smallest normalized number
| style="text-align:center;"| *
| −126
| style="text-align:right;"| 1
| 0000 0001
| 000 0000 0000 0000 0000 0000
| ±2&lt;sup&gt;−126&lt;/sup&gt; ≈ ±1.18{{e|-38}}
|-
| Largest normalized number
| style="text-align:center;"| *
| 127
| style="text-align:right;"| 254
| 1111 1110
| 111 1111 1111 1111 1111 1111
| ±(2&amp;minus;2&lt;sup&gt;−23&lt;/sup&gt;) &amp;times; 2&lt;sup&gt;127&lt;/sup&gt; ≈ ±3.4{{e|38}}
|-
| Positive infinity
| style="text-align:center;"| 0
| 128
| style="text-align:right;"| 255
| 1111 1111
| 000 0000 0000 0000 0000 0000
| +∞
|-
| Negative infinity
| style="text-align:center;"| 1
| 128
| style="text-align:right;"| 255
| 1111 1111
| 000 0000 0000 0000 0000 0000
| −∞
|-
| [[Not a number]]
| style="text-align:center;"| *
| 128
| style="text-align:right;"| 255
| 1111 1111
| non zero
| NaN
|-
| colspan="7" | *  Sign bit can be either 0 or 1&amp;nbsp;.
|}

==Comparing floating-point numbers==
Every possible bit combination is either a NaN or a number with a unique value in the [[affinely extended real number system]] with its associated order, except for the two bit combinations negative zero and positive zero, which sometimes require special attention (see below).  The [[#Representation_of_numbers|binary representation]] has the special property that, excluding NaNs, any two numbers can be compared as [[sign and magnitude]] integers ([[endianness]] issues apply).  When comparing as [[2's-complement]] integers:  If the sign bits differ, the negative number precedes the positive number, so 2's complement gives the correct result (except that negative zero and positive zero should be considered equal).  If both values are positive, the 2's complement comparison again gives the correct result.  Otherwise (two negative numbers), the correct FP ordering is the opposite of the 2's complement ordering.

Rounding errors inherent in floating point calculations often make comparison of results for exact equality not useful.  Choosing an acceptable range is a complex topic.

Although negative zero and positive zero are generally considered equal for comparison purposes, some [[programming language]] [[relational operator]]s and similar constructs might or do treat them as distinct. According to the [[Java (programming language)|Java]] Language Specification,&lt;ref&gt;{{cite web|url=http://java.sun.com/docs/books/jls/|title=Java Language and Virtual Machine Specifications|website=Java Documentation}}&lt;/ref&gt; comparison and equality operators treat them as equal, but &lt;code&gt;Math.min()&lt;/code&gt; and &lt;code&gt;Math.max()&lt;/code&gt; distinguish them (officially starting with Java version 1.1 but actually with 1.1.1), as do the comparison methods &lt;code&gt;equals()&lt;/code&gt;, &lt;code&gt;compareTo()&lt;/code&gt; and even &lt;code&gt;compare()&lt;/code&gt; of classes &lt;code&gt;Float&lt;/code&gt; and &lt;code&gt;Double&lt;/code&gt;.

==Rounding floating-point numbers==
The IEEE standard has four different rounding modes; the first is the default; the others are called ''[[directed rounding]]s''.

* '''Round to Nearest''' &amp;ndash; rounds to the nearest value; if the number falls midway it is rounded to the nearest value with an even (zero) least significant bit, which means it is rounded up 50% of the time (in [[IEEE 754-2008]] this mode is called ''roundTiesToEven'' to distinguish it from another round-to-nearest mode)
* '''Round toward 0''' &amp;ndash; directed rounding towards zero
* '''Round toward +∞''' &amp;ndash; directed rounding towards positive infinity
* '''Round toward −∞''' &amp;ndash; directed rounding towards negative infinity.

==Extending the real numbers==
The IEEE standard employs (and extends) the [[affinely extended real number system]], with separate positive and negative infinities. During drafting, there was a proposal for the standard to incorporate the [[projectively extended real number system]], with a single unsigned infinity, by providing programmers with a mode selection option. In the interest of reducing the complexity of the final standard, the projective mode was dropped, however. The [[Intel 8087]] and [[Intel 80287]] [[floating point]] [[co-processor]]s both support this projective mode.&lt;ref&gt;{{cite journal|journal=ACM Transactions on Programming Languages and Systems|volume=18|issue=2|date=March 1996|format=PDF|url=http://www.jhauser.us/publications/1996_Hauser_FloatingPointExceptions.html|author=John R. Hauser|title=Handling Floating-Point Exceptions in Numeric Programs|doi=10.1145/227699.227701|pages=139–174}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|title=IEEE Task P754: A proposed standard for binary floating-point arithmetic|date=March 1981|journal=IEEE Computer|volume=14|issue=3|pages=51–62|author=David Stevenson}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|author= William Kahan and John Palmer|year=1979|title=On a proposed floating-point standard|journal=SIGNUM Newsletter|volume=14|issue=Special|pages=13–21|doi= 10.1145/1057520.1057522}}&lt;/ref&gt;

==Functions and predicates==

===Standard operations===
The following functions must be provided:
*[[arithmetic operations|Add, subtract, multiply, divide]]
*[[Square root]]
*Floating point remainder. This is not like a normal [[modulo operation]], it can be negative for two positive numbers. It returns the exact value of {{math|x–(round(x/y)·y)}}.
*[[rounding to integer|Round to nearest integer]]. For undirected rounding when halfway between two integers the even integer is chosen.
*Comparison operations. Besides the more obvious results, IEEE&amp;nbsp;754 defines that −∞&amp;nbsp;=&amp;nbsp;−∞, +∞&amp;nbsp;=&amp;nbsp;+∞ and &lt;var&gt;x&lt;/var&gt;&amp;nbsp;≠&amp;nbsp;&lt;code&gt;NaN&lt;/code&gt; for any &lt;var&gt;x&lt;/var&gt; (including &lt;code&gt;NaN&lt;/code&gt;).

===Recommended functions and predicates===
* &lt;code&gt;copysign(x,y)&lt;/code&gt; returns x with the sign of y, so &lt;code&gt;abs(x)&lt;/code&gt; equals &lt;code&gt;copysign(x,1.0)&lt;/code&gt;. This is one of the few operations which operates on a NaN in a way resembling arithmetic. The function &lt;code&gt;copysign&lt;/code&gt; is new in the C99 standard.
* &amp;minus;x returns x with the sign reversed. This is different from 0&amp;minus;x in some cases, notably when x is 0. So &amp;minus;(0) is &amp;minus;0, but the sign of 0&amp;minus;0 depends on the rounding mode.
* &lt;code&gt;scalb(y, N)&lt;/code&gt;
* &lt;code&gt;logb(x)&lt;/code&gt;
* &lt;code&gt;finite(x)&lt;/code&gt; a [[Predicate (mathematics)|predicate]] for "x is a finite value", equivalent to &amp;minus;Inf &lt; x &lt; Inf
* &lt;code&gt;isnan(x)&lt;/code&gt; a predicate for "x is a NaN", equivalent to "x ≠ x"
* &lt;code&gt;x &lt;&gt; y&lt;/code&gt; which turns out to have different exception behavior than NOT(x = y).
* &lt;code&gt;unordered(x, y)&lt;/code&gt; is true when "x is unordered with y", i.e., either x or y is a NaN.
* &lt;code&gt;class(x)&lt;/code&gt;
* &lt;code&gt;nextafter(x,y)&lt;/code&gt; returns the next representable value from x in the direction towards y

=={{anchor|P754}}History==
In 1976 [[Intel]] began planning to produce a floating point coprocessor. John Palmer, the manager of the effort, persuaded them that they should try to develop a standard for all their floating point operations. [[William Kahan]] was hired as a consultant; he had helped improve the accuracy of [[Hewlett-Packard]]'s calculators.  Kahan initially recommended that the floating point base be decimal&lt;ref&gt;W. Kahan 2003, pers. comm. to [[Mike Cowlishaw]] and others after an IEEE 754 meeting{{rs|date=October 2016}}&lt;/ref&gt; but the hardware design of the coprocessor was too far along to make that change.

The work within Intel worried other vendors, who set up a standardization effort to ensure a 'level playing field'. Kahan attended the second IEEE 754 standards working group meeting, held in November 1977. Here, he received permission from Intel to put forward a draft proposal based on the standard arithmetic part of their design for a coprocessor. The arguments over [[gradual underflow]] lasted until 1981 when an expert hired by [[Digital Equipment Corporation|DEC]] to assess it sided against the dissenters.

Even before it was approved, the draft standard had been implemented by a number of manufacturers.&lt;ref&gt;{{cite web|url=http://www.eecs.berkeley.edu/~wkahan/ieee754status/754story.html|title=An Interview with the Old Man of Floating-Point| author=Charles Severance |author-link=Charles Severance |date=20 February 1998}}&lt;/ref&gt;&lt;ref&gt;{{cite web|publisher=Connexions |url=http://cnx.org/content/m32770/latest/ |title=History of IEEE Floating-Point Format|author=Charles Severance |author-link=Charles Severance}}&lt;/ref&gt; The [[Intel 8087]], which was announced in 1980, was the first chip to implement the draft standard.

==See also==
* [[Intel 8087]]
* [[minifloat]] for simple examples of properties of IEEE 754 floating point numbers
* [[Fixed-point arithmetic]]

==Notes==
{{notelist}}

==References==
{{reflist}}

== Further reading ==
*{{cite journal
 |     author = Charles Severance |author-link=Charles Severance
 |      title = IEEE 754: An Interview with William Kahan
 |    journal = [[IEEE Computer]]
 |date=March 1998
 |     volume = 31
 |      issue = 3
 |      pages = 114–115
 |        doi = 10.1109/MC.1998.660194
 |        url = http://www.freecollab.com/dr-chuck/papers/columns/r3114.pdf
 | accessdate = 2008-04-28
 |     format = PDF
 |      quote =  
 }}
* {{cite journal
 |     author = David Goldberg
 |      title = What Every Computer Scientist Should Know About Floating-Point Arithmetic
 |    journal = [[ACM Computing Surveys]]
 |date=March 1991
 |     volume = 23
 |      issue = 1
 |      pages = 5–48
 |        doi = 10.1145/103162.103163
 |        url = http://www.validlab.com/goldberg/paper.pdf
 | accessdate = 2008-04-28
 |     format = PDF
 |      quote =  
 }}
* {{cite journal
 |     author = Chris Hecker
 |      title = Let's Get To The (Floating) Point
 |    journal = Game Developer Magazine
 |date=February 1996
 |      pages = 19–24
 |       issn = 1073-922X
 |        url = http://www.d6.com/users/checker/pdfs/gdmfp.pdf
 |format=PDF| accessdate = 
 |      quote =  
 }}
* {{cite journal
 |     author = David Monniaux
 |      title = The pitfalls of verifying floating-point computations
 |    journal = [[ACM Transactions on Programming Languages and Systems]]
 |date=May 2008
 |      pages = article #12
 |     volume = 30
 |      issue = 3
 |        doi = 10.1145/1353445.1353446
 |       issn = 0164-0925
 |        url = http://hal.archives-ouvertes.fr/hal-00128124/en/|arxiv = cs/0701192
 }}: A compendium of non-intuitive behaviours of floating-point on popular architectures, with implications for program verification and testing.

==External links==
* [http://www.cygnus-software.com/papers/comparingfloats/Obsolete%20comparing%20floating%20point%20numbers.htm Comparing floats]
* [https://web.archive.org/web/20070314154031/http://www.coprocessor.info/ Coprocessor.info: x87 FPU pictures, development and manufacturer information]
* [http://speleotrove.com/decimal/854mins.html IEEE 854-1987] &amp;mdash; History and minutes
* [http://www.binaryconvert.com/convert_float.html IEEE754 (Single and Double precision) Online Converter]

{{IEEE standards}}

{{DEFAULTSORT:Ieee 754-1985}}
[[Category:Computer arithmetic]]
[[Category:IEEE standards]]
[[Category:Floating point]]</text>
      <sha1>n6f9wkvkcdgail3vvx1qhj0c1h04e28</sha1>
    </revision>
  </page>
  <page>
    <title>Information seeking behavior</title>
    <ns>0</ns>
    <id>27454991</id>
    <revision>
      <id>863886664</id>
      <parentid>859187505</parentid>
      <timestamp>2018-10-13T18:33:32Z</timestamp>
      <contributor>
        <username>Citation bot</username>
        <id>7903804</id>
      </contributor>
      <minor/>
      <comment>Alter: journal, year. Add: hdl. Removed parameters. You can [[WP:UCB|use this bot]] yourself. [[WP:DBUG|Report bugs here]]. | [[User:Headbomb|Headbomb]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="22349">{{Refimprove|date=May 2010}}

'''Information seeking behavior''' refers to the way people [[Information seeking|search for]] and utilize information.&lt;ref&gt;Fairer–Wessels, 1990, page 361.&lt;/ref&gt; The term was coined by [[Thomas D. Wilson]] in his 1981 paper, on the grounds that the then current 'information needs' was unhelpful as a basis for a research agenda, since 'need' could not be directly observed, while how people behaved in seeking information could be observed and investigated.&lt;ref&gt;{{cite journal | last1 = Wilson | first1 = T.D. | year =1981 | title = On user studies and information needs | url = | journal = [[Journal of Documentation]] | volume = 37 | issue = 1| pages = 3–15 | doi=10.1108/eb026702}}&lt;/ref&gt; However, there is increasing work in the information searching field that is relating behaviors to underlying needs.&lt;ref&gt;BJ Jansen, D Booth, B Smith (2009) [https://faculty.ist.psu.edu/jjansen/academic/pubs/jansen_using_the_taxonomy_of_cognitive_learning_to_model_online_searching.pdf Using the taxonomy of cognitive learning to model online searching]. [[Information Processing and Management|Information Processing &amp; Management]]. 45 (6), 643-663.&lt;/ref&gt;

In 2000, [[Thomas D. Wilson|Wilson]] described information behavior as the totality of human behavior in relation to sources and channels of information, including both active and passive information-seeking, and information use.&lt;ref&gt;{{cite journal|last=Wilson|first=T.D.|title=Human Information Behaviour|journal=Informing Science|year=2000|volume=3|issue=2|pages=49–55}}&lt;/ref&gt; He described information seeking behavior as purposive seeking of information as a consequence of a need to satisfy some goal.  Information seeking behavior is the micro-level of behavior employed by the searcher in interacting with information systems of all kinds, be it between the seeker and the system, or the pure method of creating and following up on a search.

A variety of theories of information behavior – e.g. [[Zipf]]'s principle of least effort, [[Brenda Dervin]]'s sensemaking, [[Elfreda Chatman]]'s life in the round – seek to understand the processes that surround information seeking.&lt;ref name="Case, DO 2012"&gt;{{Cite book|title=Looking for information: a survey of research on information seeking, needs and behavior|author=Case, DO|publisher= Emerald |year=2012|isbn= 9781780526546}}&lt;/ref&gt; The analysis of the most cited publications on information behavior during the first years of this century shows its theoretical nature.&lt;ref&gt;{{cite journal | last1 = Gonzalez-Teruel | first1 = A | last2 = González-Alcaide | first2 = G | last3 = Barrios | first3 = M | last4 = Abad-García | first4 = MF. | year = 2015 | title = Mapping recent information behavior research: an analysis of co-authorship and co-citation networks | url = | journal = Scientometrics | volume = 103 | issue = 2| pages = 687–705 | doi=10.1007/s11192-015-1548-z| hdl = 2445/100263 }}&lt;/ref&gt; Together with some works that have a constructivist focus, using references to Dewey, Kelly, Bruner and Vygotsky,&lt;ref&gt;{{Cite book|title= Seeking Meaning: A process approach to library and information services |author= Kuhlthau, CC |publisher= Ablex |year=1993|isbn=}}&lt;/ref&gt; others mention sociological concepts, such as Bourdieu's habitus.&lt;ref&gt;{{cite journal | last1 = Savolainen | first1 = R | year = 1995 | title = Everyday life information seeking approaching information seeking in the context of "way of life"  | url = | journal = Library &amp; Information Science Research | volume = 17 | issue = 13| pages = 259–294}}&lt;/ref&gt; Several adopt a constructionist-discursive focus,&lt;ref&gt;{{cite journal | last1 = McKenzie | first1 = PJ | year = 2003 | title = A model of information practices in accounts of everyday-life information seeking  | url = | journal = Journal of Documentation | volume = 59 | issue = 1 | pages = 19–40 | doi=10.1108/00220410310457993}}&lt;/ref&gt; whereas some, such as Chatman,&lt;ref&gt;{{cite journal | last1 = Chatman | first1 = EA | year = 1996 | title = The impoverished life-world of outsiders  | url = | journal = Journal of the American Society for Information Science | volume = 47 | issue = 3| pages = 193–206 | doi=10.1002/(sici)1097-4571(199603)47:3&lt;193::aid-asi3&gt;3.3.co;2-m}}&lt;/ref&gt;&lt;ref&gt;{{cite journal | last1 = Chatman | first1 = EA | year = 1999 | title = A theory of life in the round | url = | journal = Journal of the American Society for Information Science | volume = 50 | issue = 3| pages = 207–217 | doi=10.1002/(sici)1097-4571(1999)50:3&lt;207::aid-asi3&gt;3.3.co;2-#}}&lt;/ref&gt; who can in general be described as using an ethnographic perspective, stand out for the quantity and diversity of references to social research.&lt;ref name="Case, DO 2012"/&gt; The term 'information behaviour' was also coined by Wilson and occasioned some controversy on its introduction,&lt;ref&gt;JESSE, discussion list http://listserv.utk.edu/cgi-bin/wa?A2=ind9912&amp;L=JESSE&amp;D=0&amp;P=3346&lt;/ref&gt; but now seems to have been adopted, not only by researchers in information science but also in other disciplines.

The digital world is changing human information behavior and process. Focused almost exclusively on information seeking and using, information receiving, a central modality of the process is generally overlooked.  As information seeking continues to migrate to the Internet, and artificial intelligence continues to advance the analysis of user behavior on the Internet across a range of user interactions, information receiving moves to the heart of the process, as systems  "learn" what users like, want and need, as well as their search habits.&lt;ref&gt;(1998 October) Giannini, Tula. ASIS Annual Conference, Pittsburgh, PA. "Information Receiving, a Primary Mode of the Information Process.&lt;/ref&gt;

==Models==

===Information search process (ISP)===

[[Information search process|ISP]] was proposed and developed by [[Carol Kuhlthau]].

An holistic framework based initially on research into high school students, but extended over time to include a diverse range of people, including those in the workplace. It examined the role of emotions, specifically uncertainty, in the information seeking process, concluding that many searches are abandoned due to an overwhelmingly high level of uncertainty.&lt;ref name="Kuhlthau"&gt;{{cite web|last=Kuhlthau|first=Carol|title=Carol Kuhlthau|url=http://wp.comminfo.rutgers.edu/ckuhlthau/|accessdate=2017-12-05}}&lt;/ref&gt;

ISP is a 6-stage process, with each stage each encompassing 4 aspects;
* Cognitive (thoughts) – what is to be accomplished
* Affective (feelings) – what the searcher was feeling
* Actions (physical) – what the searcher did
* Strategies (physical) – what the searcher was trying to achieve&lt;ref name="Kuhlthau"/&gt;

{| class="wikitable"
|-
! Stage !! Task !! Thoughts !! Feelings !! Actions !! Strategies
|-
|1|| Task initiation || Contemplating assignment, comprehending task, relating prior experience and knowledge, considering possible topics || Apprehension of work ahead, uncertainty || Talking with others, browsing library || Brainstorming, discussing, contemplating possibilities, tolerating uncertainty
|-
|2|| Topic selection || Weighing topics against criteria such as personal interest, project requirements, information available, time available; predicting outcome of possible choices, choosing topic with potential for success||Confusion, sometimes anxiety, brief elation (after selection), anticipation of task || Consulting informal mediators, using reference collections, preliminary searches || Discussing possible topics, predicting outcomes of choices, gaining general overview of topic
|-
|3|| Pre-focus exploration || Becoming informed about general topic, seeking focus in general information found, identifying possible foci, inability to express precise information needed|| Confusion, doubt, sometimes threat, uncertainty || Locating relevant information, reading to become informed, taking notes, making bibliographic citations || Reading to learn about topic, tolerating inconsistency and incompatibility of information encountered, intentionally seeking possible focus, listing descriptors
|-
|4|| Focus formation || Predicting outcome of possible foci, using stage 2 task criteria, identifying ideas in information to form focus, sometimes characterised by a sudden moment of insight|| Optimism, confidence of ability to complete task || Reading notes for themes||Making a survey of notes, listing possible foci, choosing a focus while rejecting others OR combining several themes to form one focus
|-
|5|| Information collection || Seeking information to support focus, defining and extending focus through information, gathering pertinent information, organising information in notes|| Realisation of extensive work to be done, confidence in ability to complete task, increased interest ||Using library to collect pertinent information, requesting specific sources, taking detailed notes with bibliographic citations ||Using descriptors to search out pertinent information, making comprehensive search of various types of materials i.e. reference, periodicals, non-fiction and biography, using indexes, requesting assistance of librarian
|-
|6|| Search closure || Identify need for any additional information, considering time limit, diminishing relevance, increasing redundancy, exhausting resources||Sense of relief, sometimes satisfaction, sometimes disappointment ||Re-checking information for information initially overlooked, confirming information and bibliographic citations ||Returning to library to make summary search, keeping books until completion of writing to re-check information
|}&lt;ref&gt;{{cite web|url=http://www2.southeastern.edu/Academics/Faculty/nadams/lsa618/infostagesKuhlthau.html|title=Kuhlthau's Model of the Stages of the Information Process|last=|first=|date=|website=|archive-url=|archive-date=|dead-url=|accessdate=2011-03-30}}&lt;/ref&gt;

===David Ellis===

Investigated the behavior of researchers in the physical and social sciences&lt;ref&gt;{{cite journal|last=Ellis|first=David|author2=Cox, Deborah, Hall, Katherine|title=A COMPARISON OF THE INFORMATION SEEKING PATTERNS OF RESEARCHERS IN THE PHYSICAL AND SOCIAL SCIENCES|journal=Journal of Documentation|year=1993|volume=49|issue=4|pages=356–369|doi=10.1108/eb026919}}&lt;/ref&gt;  and engineers and research scientists&lt;ref&gt;{{cite journal|last=David|first=Ellis|author2=Haugan, Merete|title=Modelling the information seeking patterns of engineers and research scientists in an industrial environment|journal=Journal of Documentation|year=1997|volume=53|issue=4|pages=384–403|doi=10.1108/eum0000000007204}}&lt;/ref&gt;  through semi-structured interviews using a [[grounded theory]] approach, with a focus on describing the activities rather than a process.

These initial investigations produced six key activities within the information seeking process:

*Starting (activities that form the information search)
*Chaining (backwards or forwards – following references in initial information sources)
*Browsing (semi-directed search)
*Differentiating (filtering and selecting sources based on judgement of quality and relevance)
*Monitoring ([[wikt:up-to-date|keeping track of developments in an area]])
*Extracting (systematic extraction of material of interest from sources)

Later studies by Ellis (focusing on academic researchers in other disciplines) resulted in the addition of two more activities;

* Verifying (checking accuracy)
* Ending (a final search, checking all material covered)

===Episodic model===

The episodic model was developed by [[Nicholas J. Belkin]].

The episodic model is based largely on intuition and insight and concentrates on interactions with information. There are 4 dimensions which characterize search behavior. These dimensions can be combined in 16 different ways.

* Method of interaction (scanning/searching)
* Goal of interaction (learning/selecting)
* Mode of retrieval (recognition/specification)
* Resource considered (information/meta-information)

===Anomalous state of knowledge (ASK)===

ASK was also developed by Nicholas J. Belkin.

An anomalous state of knowledge is one in which the searcher recognises a gap in the state of knowledge. This, his further hypothesis, is influential in studying why people start to search.&lt;ref&gt;{{cite journal | last1 = Belkin | first1 = Nicholas J | year = | title = Anomalous states of knowledge as a basis for information retrieval | url = | journal = Canadian Journal of Information and Library Science | volume = 5 | issue = | pages = 133–143 }}&lt;/ref&gt;

===Wilson's theory of information behavior===
{{main article|Wilson's model of information behavior|Thomas D. Wilson}}
&lt;!-- Deleted image removed: [[File:P269fig3.jpg|thumb|alt=A simple diagram|A basic diagram illustrating Wilson's theory]] --&gt;
Thomas Wilson proposed that information behavior covers all aspects of human information behavior, whether active or passive.

Information ''Seeking'' behavior is the act of actively seeking information in order to answer a specific query.

Information ''Searching'' behavior is the behavior which stems from the searcher interacting with the system in question.  This system could be a technological one, such as the searcher interacting with a search engine, or a manual one, such as the searcher selecting which book is most pertinent to their query.

Information ''Use'' behavior pertains to the searcher adopting the knowledge they sought.

===Information foraging===
{{main article|Information foraging}}
Developed by [[Stuart Card]], [[Ed H. Chi]] and [[Peter Pirolli]].

This model is derived from anthropological theories and is comparable to foraging for food. Information seekers use clues (or information scents) such as links, summaries and images to estimate how close they are to target information. A scent must be obvious as users often browse aimlessly or look for specific information. Information foraging is descriptive of why and not how people search in particular ways.&lt;ref&gt;{{cite journal | last1 = Chi | first1 = E.H | last2 = Pirolli | first2 = P | last3 = Chen | first3 = K | last4 = Pitkow | first4 = J. | year = | title = Using information scent to model user information needs and actions and the web | url = | journal = In Proceedings of the SIGCHI Conference on Human Factors in Computing Systems | volume = 2001 | issue = | pages = 490–497 }}&lt;/ref&gt;

===Life in the round===

Developed by [[Elfreda Chatman]].

She defines life in the round as a world of tolerated approximation. It acknowledges reality at its most routine, predictable enough that unless an initial problem should arise, there is no point in seeking information.&lt;ref name="Chatman"&gt;{{cite journal | last1 = Chatman | first1 = Elfreda | year = 1999| title = A theory of life in the round | url = | journal = The Journal of the American Society for Information Science | volume = 50 | issue = 3| pages = 207–217 | doi=10.1002/(sici)1097-4571(1999)50:3&lt;207::aid-asi3&gt;3.0.co;2-8}}&lt;/ref&gt;

Chatman examined this principle within a small world: a world which imposes on its participants similar concerns and awareness of who is important; which ideas are relevant and whom to trust. Participants in this world are considered insiders.&lt;ref name="Chatman"/&gt;

Chatman focused her study on women at a maximum security prison. She learned that over time, prisoner's private views were assimilated to a communal acceptance of life in the round: a small world perceived in accordance with agreed upon standards and communal perspective. Members who live in the round will not cross the boundaries of their world to seek information unless it is critical; there is a collective expectation that information is relevant; or life lived in the round no longer functions. The world outside prison has secondary importance to inmates who are absent from this reality which is changing with time.&lt;ref name="Chatman"/&gt;

===Sensemaking===
{{main article|Sensemaking}}
[[Brenda Dervin]] developed the concept of sensemaking.  Sensemaking considers how we (attempt to) make sense of uncertain situations.&lt;ref&gt;{{cite journal | last1 = Klein | first1 = G. | last2 = Moon | first2 = B. | last3 = Hoffman | first3 = R.F. | year = 2006| title = Making sense of sensemaking I: alternative perspectives | url = | journal = IEEE Intelligent Systems | volume = 21 | issue = 4| pages = 70–73 | doi=10.1109/mis.2006.75}}&lt;/ref&gt;  Her description of Sensemaking consisted of the definition of how we interpret information to use for our own information related decisions.

Brenda Dervin described sensemaking as a method through which people make sense of their worlds in their own language.

===Principle of least effort===
This principle explains that information seekers prioritise the most convenient path to acceptable information.&lt;ref&gt;Case Donald O. "Principle of least effort,"''Theories Of Information Behavior'', Karen Fisher ed. p50.&lt;/ref&gt;
{{main article|Principle of least effort}}

===Navigators and explorers===
This compares the internet search methods of experienced information seekers (navigators) and inexperienced information seekers (explorers). Navigators revisit domains; follow sequential searches and have few deviations or regressions within their search patterns and interactions. Explorers visit many domains; submit many questions and their search trails branch frequently.&lt;ref&gt;White, Ryen W and Drucker, Steven M "Investigating behavioural variability in web search," ''16th International World Wide Web Conference (WWW 2007)''&lt;/ref&gt;

=== Information sources: Other people and/or information repositories ===
Robinson's (2010)&lt;ref&gt;{{cite journal | last1 = Robinson | first1 = M. A. | year = 2010 | title = An empirical analysis of engineers' information behaviors | url = | journal = Journal of the American Society for Information Science and Technology | volume = 61 | issue = 4| pages = 640–658 | doi = 10.1002/asi.21290 }}&lt;/ref&gt; research suggests that when seeking information at work, people rely on both other people and information repositories (e.g., documents and databases), and spend similar amounts of time consulting each (7.8% and 6.4% of work time, respectively; 14.2% in total). However, of theoretical interest, the distribution of time among the constituent information seeking stages differs depending on the source. When consulting other people, people spend less time locating the information source and information within that source, similar time understanding the information, and more time problem solving and decision making, than when consulting information repositories. Furthermore, the research found that people spend substantially more time receiving information passively (i.e., information that they have not requested) than actively (i.e., information that they have requested), and this pattern is also reflected when they provide others with information.

==Similarities between models==
A review of the literature on information seeking behavior shows that information seeking has generally been accepted as dynamic and non-linear (Foster, 2005; Kuhlthau 2006). People experience the [[information search process]] as an interplay of thoughts, feelings and actions ([[Carol Kuhlthau|Kuhlthau, 2006]]).

Information seeking has been found to be linked to a variety of interpersonal communication behaviors beyond question-asking, to include strategies such as candidate answers.{{Citation needed|date=May 2010}}

A search for information may be linked to [[decision making]]. The decision involved may vary from a trivial personal matter to a decision which affects billions or may have cumulative economic or political effects as individual buying or voting decisions may.&lt;ref&gt;{{Cite book|title=Looking for information: a survey of research on information seeking, needs and behavior|author=Case, Donald O.|publisher=Academic Press|year=2002|isbn=978-0-12-369430-0|page=10}}&lt;/ref&gt;

==Types==

Nicolaisen&lt;ref&gt;{{cite journal|last=Nicolaisen|first=J.|year=2009|title=Compromised need and the label effect: An examination of claims and evidence|url=|journal=Journal of the American Society for Information Science and Technology|volume=60|issue=10|pages=2004–2009|doi=10.1002/asi.21129|via=}}&lt;/ref&gt; described four distinct types of information seeking behavior: visceral, conscious, formalized and compromised.  The visceral need is expressed as the actual information need before it has been expressed.  The conscious need is the need once it has been recognized by the seeker.  The formalized need is the statement of the need and the compromised need is the query when related to the information system.

[http://www.jisc.ac.uk/ JISC]'s study of the Google Generation&lt;ref&gt;{{cite web|author=JISC|title=The Google Generation|url=http://www.jisc.ac.uk/whatwedo/programmes/resourcediscovery/googlegen.aspx}}&lt;/ref&gt; detailed six different characteristics of online information seeking behavior;

* horizontal information seekers
* navigation
* viewers
* squirreling behavior
* diverse information seekers
* checking information seekers.

Horizontal information seeking is the method sometimes referred to as "[[Skimming (reading)|skimming]]".  An information seeker who skims views a couple of pages, then subsequently follows other links without necessarily returning to the initial sites.  Navigators, as might be expected, spend their time finding their way around.  Wilson found that users of e-book or e-journal sites were most likely spend, on average, a mere four to eight minutes viewing said sites.  Squirreling behavior relates to users who download lots of documents but might not necessarily end up reading them.  Checking information seekers assess the host in order to ascertain trustworthiness.  The bracket of users named diverse information seekers are users whose behavior differs from the above sectors.

==Notes==
{{Reflist}}

==External links and further reading==
&lt;!--MODEL REFERENCES--&gt;
*Donald O. Case, ''Looking for information: a survey of research on information seeking, needs and behavior'', Academic Press (2002) 370 pages {{ISBN|012150381-X}}
**[http://informationr.net/ir/reviews/revs095.html Review ''Information Research'']
**2nd edition (December 26, 2006), hardcover, 440 pages, {{ISBN|0123694302}} {{ISBN|978-0123694300}}


[[Category:Behavior]]
[[Category:Information theory]]</text>
      <sha1>5mbkzn6kxava4vgxle9mcrtiswffj3s</sha1>
    </revision>
  </page>
  <page>
    <title>Insurance score</title>
    <ns>0</ns>
    <id>2605397</id>
    <revision>
      <id>833014541</id>
      <parentid>738068532</parentid>
      <timestamp>2018-03-29T06:06:21Z</timestamp>
      <contributor>
        <username>Fayenatic london</username>
        <id>1639942</id>
      </contributor>
      <comment>removed [[Category:Insurance terms]]; added [[Category:Actuarial science]] using [[WP:HC|HotCat]] - [[Wikipedia:Categories_for_discussion/Log/2018_February_11]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12892">An '''[[insurance]] score''' - also called an '''insurance credit score''' - is a numerical point system based on select credit report characteristics. There is no direct relationship to financial [[Credit score|credit scores]] used in lending decisions, as insurance scores are not intended to measure creditworthiness, but rather to predict [[Risk management|risk]]. Insurance companies use insurance scores for underwriting decisions, and to partially determine charges for [[Insurance premium|premium]]s. Insurance scores are applied in personal product lines, namely homeowners and private passenger automobile insurance, and typically not elsewhere.

== Background ==
Insurance scoring models are built from selections of credit report factors, combined with insurance claim and profitability data, to produce numerical formulae or algorithms. A scoring model may be unique to an insurance company and to each line of business (e.g. homeowners or automobile), in terms of the factors selected for consideration and the weighting of the point assignments.  As insurance credit scores are not intended to measure [[Credit risk|creditworthiness]], they commonly focus on financial habits and choices (i.e., age of oldest account, number of inquiries in 24 months, ratio of total balance to total limits, number of open retail [[Credit card|credit cards]], number of revolving accounts with balances greater than 75% of limits, etc.) Therefore it is possible for a consumer with a high financial credit score, and excellent payment history, to receive a poor insurance score. Insurers consider [[Credit history|credit report]] information in their underwriting and pricing decisions as a predictor of profitability and [[risk of loss]]. 

Various studies have found a strong relationship between credit-based insurance scores and profitability or risk of loss. The scores are generally most predictive when little or no other information exists, such as in the case of clean driving records, or claims-free policies; in instances where past claims, points, or other similar information exist on record, the personal histories will typically be more predictive than the scores. Insurers consider credit report information, along with other factors, such as driving experience, previous claims and vehicle age, to develop a picture of a consumer's risk profile and to establish premium rates. The correlation, between credit-based insurance scores and overall insurance profitability and loss, has not been disputed.

== Support and opposition ==
The use of credit information in insurance pricing and underwriting is heavily disputed. Proponents of insurance credit scoring include insurance companies, the [[American Academy of Actuaries]] (AAA),&lt;ref name="AAA"&gt;[http://www.actuary.org/pdf/casualty/credit_dec02.pdf The Use of Credit History for Personal Lines of Insurance], American Academy of Actuaries&lt;/ref&gt; the [[Insurance Information Institute]] (III),&lt;ref&gt;[http://www.iii.org/iu/credit-scoring/  Insurance Information Institute], Topic: Credit Scoring&lt;/ref&gt; and credit bureaus such as [[Fair Isaac]]&lt;ref&gt;[http://www.insurancescore.com/ Fair Isaac] (For Consumers)&lt;/ref&gt;&lt;ref&gt;[http://www.fico.com/en/Products/Scoring/Pages/FICO-Insurance-Risk-Scores.asp  Fair Isaac, FICO Insurance Risk Scores] (For Insurers)&lt;/ref&gt; and [[TransUnion]].&lt;ref&gt;[http://www.transunion.com/docs/business/industrySolutions/insuranceRiskScores.pdf TransUnion]&lt;/ref&gt; Active opponents include many state insurance departments and regulators, and consumer protection organizations such as the Center for Economic Justice,&lt;ref&gt;[http://www.cej-online.org/creditscoringmainpage.htm Center for Economic Justice], Topic: Insurance Credit Scoring&lt;/ref&gt; the [[Consumer Federation of America]],&lt;ref&gt;[http://www.insurancejournal.com/news/southeast/2010/02/05/107143.htm Robert Hunter, Director of Insurance, Consumer Federation of America], ''Insurance Journal'' (February 2010)&lt;/ref&gt; the [[National Consumer Law Center]]&lt;ref&gt;[http://www.cej-online.org/NCLC_CEJ_Insurance_Scoring_Racial_Divide_0706.pdf Credit Scoring and Insurance: Costing Consumers Billions and Perpetuating the Economic Racial Divide], The National Consumer Law Center (June 2007)&lt;/ref&gt; and Texas Watch.&lt;ref&gt;[http://www.texaswatch.org/issues/insurance-scoring/ Ban Texas Insurance Scoring], Texas Watch&lt;/ref&gt; As a result of successful lobbying by the insurance industry, credit scoring is legal in nearly all states. The state of [[Hawaii]] has banned all use of credit information in personal automobile underwriting and rating, and other states have established restrictions.&lt;ref&gt;{{cite web |url=http://www.namic.org/reports/credithistory/credithistory.asp |archiveurl=https://web.archive.org/web/20110719124507/http://www.namic.org/reports/credithistory/credithistory.asp |archivedate=2011-07-19 |title=State Laws Governing Insurance Scoring Practices National Association of Mutual Insurance Companies |accessdate=2012-10-20 }}&lt;/ref&gt; A number of states have also made unsuccessful attempts to ban or restrict the practice. The [[National Association of Insurance Commissioners]] has acknowledged that a correlation does exist between insurance scores and losses, but asserts that the benefit of credit reports to ''consumers'' has not yet been established.&lt;ref&gt;[http://www.naic.org/topics/topic_credit_scoring.htm National Association of Insurance Commissioners], Topic: Credit Scoring&lt;/ref&gt;

== Public information ==
Insurance credit-scoring models are considered proprietary, and a [[trade secret]], in most cases. The designers wish to protect their models from view for a number of reasons: they may provide competitive advantage in the insurance marketplace, or they anticipate consumers might attempt to alter results, by changing the information they provide, if the computations were common knowledge. Thus there is little public information available about the details of insurance credit-scoring models. 
 
* One actuarial study has been published, ''The Impact of Personal Credit History on Loss Performance in Personal Lines'', by James Monaghan, ACAS MAAA.&lt;ref name="Monaghan"&gt;[http://www.casact.org/pubs/forum/00wforum/00wf079.pdf The Impact of Personal Credit History on Loss Performance in Personal Lines], by James Monaghan ACAS MAAA&lt;/ref&gt;
* [[Allstate]] has published a private passenger automobile credit scoring model, the ISM7 (NI) Scorecard (where "NI" indicates no inquiries are considered).&lt;ref&gt;[http://infoportal.ncdoi.net/getfile.jsp?sfp=/PC/PC095000/PC095470A815823.PDF ISM7 (NI) Scorecard], Allstate Property &amp; Casualty Company&lt;/ref&gt;

== Key reports and studies ==
''Credit-Based Insurance Scores: Impacts on Consumers of Automobile Insurance, A Report to Congress by the Federal Trade Commission''.&lt;ref name="FTC Study"&gt;[http://ftc.gov/opa/2007/07/facta.shtm Credit-Based Insurance Scores: Impacts on Consumers of Automobile Insurance], [[Federal Trade Commission]] (July 2007)&lt;/ref&gt;  This study found that insurance credit scores are effective predictors of risk. It also showed that African-Americans and Hispanics are substantially overrepresented in the lowest credit scores, and substantially underrepresented in the highest, while Caucasians and Asians are more evenly spread across the scores. The credit scores were also found to predict risk within each of the ethnic groups, leading the Federal Trade Commission (FTC) to conclude that the scoring models are not solely proxies for [[redlining]]. The FTC stated that little data was available to evaluate benefit of insurance scores to consumers. The report was disputed by representatives of the Consumer Federation of America, the National Fair Housing Alliance, the National Consumer Law Center, and the Center for Economic Justice, for relying on data provided by the insurance industry, which was not open to examination.&lt;ref&gt;[http://www.consumeraffairs.com/news04/2007/07/insurance_credit.html Consumers Dispute FTC Report on Insurance Credit Scoring], www.consumeraffairs.com (July 2007)&lt;/ref&gt;   

''The Impact of Personal Credit History on Loss Performance in Personal Lines'', by James Monaghan ACAS MAAA.&lt;ref name="Monaghan"/&gt; This actuarial study matched 170,000 policy records with credit report information to show the correlation between historical loss ratios and various credit report elements.   
   
''The Use of Credit History for Personal Lines of Insurance: Report to the National Association of Insurance Commissioners'', American Academy of Actuaries Risk Classification Subcommittee of the Property/Casualty Products, Pricing and Market Committee.&lt;ref name="AAA"/&gt;

''Insurers' Use of Credit Scoring for Homeowners Insurance in Ohio: A Report to the Ohio Civil Rights Commission'', from Birny Birnbaum, Center for Economic Justice.&lt;ref&gt;[http://www.cej-online.org/report_to_ohio_civil_rights_commission.pdf Insurers' Use of Credit Scoring for Homeowners Insurance in Ohio], Birny Birnbaum, Center for Economic Justice (January 2003)&lt;/ref&gt; Birny Birnbaum, Consulting Economist, argues that insurance credit scoring is inherently unfair to consumers and violates basic risk classification principles.

''Insurance Credit Scoring: An Unfair Practice'', Center for Economic Justice.&lt;ref&gt;[http://www.cej-online.org/cej%20report%20ins%20cr%20scoring%200501.pdf Insurance Credit Scoring: An Unfair Practice], Center for Economic Justice (January 2005)&lt;/ref&gt; This report argues that insurance scoring: is inherently unfair; has a disproportionate impact on consumers in poor and minority communities; penalizes consumers for rational behavior and sound financial management practices; penalizes consumers for lenders’ business decisions unrelated to payment history; is an arbitrary practice; and undermines the basic insurance mechanism and public policy goals for insurance. 

''The Use of Credit Scoring in Automobile and Homeowners Insurance, A Report to the Governor, the Legislature and the People of Michigan'', by Frank M. Fitzgerald, Commissioner, Office of Financial and Insurance Services.&lt;ref&gt;[http://www.michigan.gov/documents/cis_ofis_credit_scoring_report_52885_7.pdf  The Use of Credit Scoring in Automobile and Homeowners Insurance], Frank M. Fitzgerald, Commissioner, Office of Financial and Insurance Services, Michigan (December 2002)&lt;/ref&gt; 
This report reviewed the viewpoints of the industry, agents, consumers, and other interested parties. In conclusion, insurance credit scoring was found to be within the scope of Michigan law.  

''Use of Credit Information by Insurers in Texas, Report to the 79th Legislature'', Texas Department of Insurance.&lt;ref&gt;[http://www.tdi.state.tx.us/reports/documents/creditrpt04.pdf Use of Credit Information by Insurers in Texas], Texas Department of Insurance (December 2004)&lt;/ref&gt; This study found a consistent pattern of differences in credit scores among different racial/ethnic groups. Whites and Asians were found to have better scores than Blacks and Hispanics. Differences in income levels were not as pronounced as for racial/ethnic groups, but average credit scores at upper income levels were better than those at lower and moderate income levels. The study found a strong relationship between credit scores and claims experience on an aggregate basis. In 2002, the Texas Department of Insurance received a peak of 600 complaints related to credit scoring, which declined and leveled to 300 per year.    
 
''Insurance Credit Scoring in Alaska'', State of Alaska, Department of Community and Economic Development, Division of Insurance.&lt;ref&gt;[http://www.naic.org/documents/topics_credit_scoring_ak.pdf Insurance Credit Scoring in Alaska], State of Alaska, Division of Insurance (February 2003)&lt;/ref&gt; The study suggested unequal effects on consumers of varying income and ethnic backgrounds. Specifically, the higher income neighborhoods and those with a higher proportion of Caucasians were the least impacted by credit scoring. Although data available for the study was limited, the state of Alaska determined that some restrictions on credit scoring would be appropriate to protect the public.

== References ==
&lt;references/&gt;

==Sources==
* [http://www2.iii.org/individuals/credit/ Insurance Information Institute Credit Insurance Fact Sheets]

==External links==
* [http://www.insurancescored.com/ The Truth Behind Insurance Scoring], www.InsuranceScored.com
* [http://www.msnbc.msn.com/id/35103647/ Insurance firms blasted for credit score rules], by Herb Weisbaum, Con$umerMan on MSNBC (January 2010)
* [http://www.insurancejournal.com/news/west/2010/07/16/111627.htm Washington Commissioner Criticizes Insurers' Credit Score Use], ''Insurance Journal'' (July 2010)
* [http://www.consumersunion.org/pdf/CR-Aug2006.pdf Caution! The Secret Score Behind Your Auto Insurance], Consumer Reports (August 2006)

[[Category:Metrics]]
[[Category:Actuarial science|Insurance score]]</text>
      <sha1>cmox2le6e0b5ejc7d9v4f7kr7nklmvz</sha1>
    </revision>
  </page>
  <page>
    <title>Interval (graph theory)</title>
    <ns>0</ns>
    <id>39034538</id>
    <revision>
      <id>810307496</id>
      <parentid>785420982</parentid>
      <timestamp>2017-11-14T13:29:40Z</timestamp>
      <contributor>
        <ip>2001:6B0:1:1041:8F86:34BA:1ADF:39AC</ip>
      </contributor>
      <comment>Correct date for Interval reference.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2906">{{about|a concept in graph-theoretic data flow analysis|graphs defined from intersections of intervals|Interval graph|the set of elements in a DAG that are reachable from one vertex and can reach another vertex|Partially ordered set#Interval}}
In [[graph theory]], an '''interval''' ''I''(''h'') in a [[directed graph]] is a maximal, single entry subgraph in which ''h'' is the only entry to ''I''(''h'') and all closed [[Path (graph theory)|paths]] in ''I''(''h'') contain ''h''. Intervals were described in 1970 by [[F. E. Allen]] and [[John Cocke|J. Cocke]].&lt;ref&gt;{{cite journal|author=F.E. Allen|title=Control flow analysis|journal=Comm. ACM|date=July 1970|volume=5|issue=7|pages=1-19|doi=10.1145/800028.808479}}&lt;/ref&gt; Interval graphs are integral to some algorithms used in [[compilers]], specifically data flow analyses.

The following algorithm finds all the intervals in a graph consisting of vertices ''N'' and the entry vertex ''n''&lt;sub&gt;0&lt;/sub&gt;, and with the functions &lt;code&gt;pred(n)&lt;/code&gt; and &lt;code&gt;succ(n)&lt;/code&gt; which return the list of predecessors and successors of a given node ''n'', respectively.

    H = { n0 }                               // Initialize work list
    while H is not empty
        remove next h from H	
        create the interval I(h)
        I(h) += { h }
        while ∃n ∈ { succ(I(h)) — I(h) } such that pred(n) ⊆ I(h)
            I(h) += { n }
        while ∃n ∈ N such that n ∉ I(h) and    // find next headers
              ∃m ∈ pred(n) such that m ∈ I(h)
            H += n  
The algorithm effectively [[Partition of a set|partitions]] the graph into its intervals. 

Each interval can in turn be replaced with a single node, while all edges between nodes in different intervals in the original graph become edges between their corresponding nodes in the new graph. This new graph is called an '''interval derived graph'''. The process of creating derived graphs can be repeated until the resulting graph can't be reduced further. If the final graph consists of a single node, then the original graph is said to be '''reducible'''.

==References==
{{Reflist}}
* {{cite journal | doi = 10.1145/800152.804919 | title=Flow graph reducibility | journal=Proceedings of the fourth annual ACM symposium on Theory of computing - STOC '72 | date=1972 | first=Matthew S. | last=Hecht}} cites the notion to two papers: F. E. ALLEN, Control flow analysis, SIGPLAN Notices, 5 (1970), pp. 1-19. and to J. Cocke, Global common subexpression elimination, SIGPLAN Notices, 5 (1970), pp. 20-24.
* {{cite journal | doi = 10.1145/321832.321835 | title=Characterizations of Reducible Flow Graphs | journal=Journal of the ACM | date=1974 | volume=21 | issue=3 | pages=367–375 | first=M. S. | last=Hecht}} additional characterizations of reducible flow graphs, including via [[natural loop]]s

[[Category:Graph theory objects]]
[[Category:Control-flow analysis]]</text>
      <sha1>t9r5ls9mvao4kurs1lo249yejjzvus9</sha1>
    </revision>
  </page>
  <page>
    <title>Isaac Watts</title>
    <ns>0</ns>
    <id>239143</id>
    <revision>
      <id>869623516</id>
      <parentid>869610844</parentid>
      <timestamp>2018-11-19T19:45:57Z</timestamp>
      <contributor>
        <username>GrindtXX</username>
        <id>12418561</id>
      </contributor>
      <minor/>
      <comment>Watts' title</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="27252">{{Use dmy dates|date=March 2012}}
{{Use British English|date=March 2012}}

{{Infobox person
 | name          = Isaac Watts
 | image         = Isaac Watts from NPG.jpg
 | image_size    =
 | alt           =
 | caption       = Portrait by an unknown artist 
 | birth_name    = 
 | birth_date    = {{Birth date|df=yes|1674|07|17}}
 | birth_place   = [[Southampton]], [[Kingdom of England]]
 | death_date    = {{Death date and age|df=yes|1748|11|25|1674|07|17}}
 | death_place   = [[Stoke Newington]], [[Kingdom of Great Britain]]
 | spouse        =
 | occupation    = [[Hymn]]writer, [[theologian]]
 | known_for     = "[[When I Survey the Wondrous Cross]]", "[[Joy to the World]]", "[[Our God, Our Help in Ages Past]]"
}}
[[File:Statue of Isaac Watts, Abney Park Cemetery.jpg|thumb|Statue of Isaac Watts, Abney Park Cemetery]]
'''Isaac Watts''' (17 July 1674 – 25 November 1748) was an English Christian minister (Congregational), [[hymn]] writer, [[theologian]], and [[logician]]. He was a prolific and popular hymn writer and is credited with some 750 hymns. He is recognised as the "Godfather of English [[Hymnody]]"; many of his hymns remain in use today and have been translated into numerous languages.

==Life==
{{refimprove section|date=January 2014}}
Watts was born in [[Southampton]], England in 1674 and was brought up in the home of a committed religious [[nonconformist]]; his father, also Isaac Watts, had been incarcerated twice for his views. Watts had a classical education at [[King Edward VI School, Southampton|King Edward VI School]], learning [[Latin]], [[Greek language|Greek]], and [[Hebrew]].

Watts displayed a propensity for rhyme from an early age. He was once asked why he had his eyes open during prayers, to which he responded:
{{bq|text=&lt;poem&gt;A little mouse for want of stairs
ran up a rope to say its prayers.&lt;/poem&gt;}}

He received [[corporal punishment]] for this, to which he cried:
{{bq|text=&lt;poem&gt;O father, father, pity take
And I will no more verses make.&lt;ref&gt;{{Citation | first = Norman | last = Mable | title = Popular Hymns and their Writers | page = 179}}.&lt;/ref&gt;&lt;ref&gt;{{Citation | first = F. W. | last = Boreham | title = A Late Lark Singing | year = 1945 | page = 29}}.&lt;/ref&gt;&lt;/poem&gt;}}

Watts could not attend [[Oxford University|Oxford]] or [[University of Cambridge|Cambridge]] because he was a noncomformist and these universities were restricted to Anglicans—as were government positions at the time. He went to the [[Dissenting academies|Dissenting Academy]] at [[Stoke Newington]] in 1690. Much of the remainder of his life centred on that village, which is now part of [[Inner London]].

Following his education, Watts was called as pastor of a large independent chapel in London, Mark Lane Congregational Chapel, where he helped train preachers, despite his poor health. He held religious opinions that were more nondenominational or [[ecumenical]] than was common for a nonconformist Congregationalist. He had a greater interest in promoting education and scholarship than preaching for any particular sect.

Watts took work as a private tutor and lived with the nonconformist Hartopp family at [[Abney Park|Fleetwood House]] on [[Stoke Newington Church Street|Church Street]] in [[Stoke Newington]]. Through them, he became acquainted with their immediate neighbours Sir [[Thomas Abney]] and [[Lady Mary Abney|Lady Mary]]. He eventually lived for a total of 36 years in the Abney household, most of the time at Abney House, their second residence. (Lady Mary had inherited the manor of [[Stoke Newington]] in 1701 from her late brother Thomas Gunston.)

On the death of Sir Thomas Abney in 1722, his widow Lady Mary and her unmarried daughter Elizabeth moved all her household to Abney House from Hertfordshire, and she invited Watts to continue with them. He particularly enjoyed the grounds at [[Abney Park]], which Lady Mary planted with two elm walks leading down to an island [[heronry]] in the [[Hackney Brook]], and he often sought inspiration there for the many books and hymns that he wrote.

Watts lived at Abney Hall in Stoke Newington until his death in 1748; he was buried in [[Bunhill Fields]]. He left an extensive legacy of hymns, treatises, educational works, and essays. His work was influential amongst nonconformist independents and religious revivalists of the 18th century, such as [[Philip Doddridge]], who dedicated his best-known work to Watts.

==Watts and hymnody==
Sacred music scholars Stephen Marini, Denny Prutow and Michael LeFebvre describe the ways in which Watts contributed to English hymnody and the previous tradition of the Church. Watts led the change in practice by including new poetry for "original songs of Christian experience" to be used in worship, according to Marini.{{Sfn | Marini | 2003 | p = 76}}  The older tradition was based on the poetry of the Bible: the [[Psalm]]s.  According to LeFebvre, Psalms had been sung by God's people from the time of King David, who with a large staff over many years assembled the complete book of Psalms in a form appropriate for singing (by the Levites, during Temple sacrifices at the time).  The practice of singing Psalms in worship was continued by Biblical command in the New Testament Church from its beginnings in Acts through the time of Watts, as documented by Prutow.  The teachings of 16th-century [[Protestant Reformation|Reformation]] leaders such as [[John Calvin]], who translated the Psalms in the vernacular for congregational singing, followed this historic worship practice.{{Sfn | Marini | 2003 | p = 71}}  Watts was not the first Protestant to promote the singing of hymns; however, his prolific hymnwriting helped usher in a new era of English worship as many other poets followed in his path.&lt;ref&gt;{{Harvnb | Marini | 2003 | p = 76}} lists hymn writers who followed in the tradition established by Watts, including  [[Charles Wesley]], [[Edward Perronet]], Ann Steele, [[Samuel Stennet]], [[Augustus Toplady]], [[John Newton]], [[William Cowper]], [[Reginald Heber]], [[Samuel Davies (Presbyterian educator)|Samuel Davies]], [[Timothy Dwight IV|Timothy Dwight]], [[John Leland (Baptist)|John Leland]], and [[Peter Cartwright (revivalist)|Peter Cartwright]].&lt;/ref&gt;

Watts also introduced a new way of rendering the Psalms in verse for church services, proposing that they be adapted for hymns with a specifically Christian perspective.  As Watts put it in the title of his 1719 metrical Psalter, the Psalms should be "imitated in the language of the New Testament."{{Sfn | Marini | 2003 | p = 76}} Besides writing hymns, Isaac Watts was also a [[theologian]] and [[logician]], writing books and essays on these subjects.

&lt;!---The following paragraph is hidden because it is largely incomprehensible. It needs to be MUCH better explicated before unhiding; otherwise, it can be deleted altogether: 
Marini discerns two particular trends in Watts' verses, which he calls "emotional subjectivity" and "doctrinal objectivity".  He suggests that "Watts' voice broke down the distance between poet and singer and invested the text with personal spirituality,"  As an example of this, he cites "[[When I Survey the Wondrous Cross]]".  By "doctrinal objectivity," Marini means that Watts verse achieved an "axiomatic quality" that "presented Christian doctrinal content with the explicit confidence that befits affirmations of faith."  As examples, Marini cites the hymns "[[Joy to the World]]" as well as "From All That Dwell Below the Skies":{{Sfn | Marini | 2003 | p = 76}}
From all that dwell below the skies
Let the Creator's praise arise;
Let the Redeemer's name be sung
Through every land, by every tongue.---&gt;

===Logic===
[[File:Isaac Watts.jpg|thumb|upright|Isaac Watts]]
Watts wrote a textbook on [[logic]] which was particularly popular; its full title was, ''Logic, or The Right Use of Reason in the Enquiry After Truth With a Variety of Rules to Guard Against Error in the Affairs of Religion and Human Life, as well as in the Sciences''. This was first published in 1724, and it was printed in twenty editions.

Watts wrote this work for beginners of logic, and arranged the book methodically. He divided the content of his elementary treatment of logic into four parts: [[perception]], [[judgement]], [[reasoning]], and [[wikt:method|method]], which he treated in this order. Each of these parts is divided into chapters, and some of these chapters are divided into sections. The content of the chapters and sections is subdivided by the following devices: divisions, distributions, notes, observations, directions, rules, illustrations, and remarks. Every contentum of the book comes under one or more of these headings, and this methodical arrangement serves to make the exposition clear.

In Watts' ''Logic,'' there are notable departures from other works of the time, and some notable innovations. The influence of British [[empiricism]] may be seen, especially that of contemporary [[philosopher]] and empiricist [[John Locke]].  ''Logic'' includes several references to Locke and his ''[[Essay Concerning Human Understanding]]'',{{Sfn | Watts | 1825 | p = 14}} in which he espoused his empiricist views. Watts was careful to distinguish between [[judgment|judgements]] and [[proposition]]s, unlike some other logic authors. According to Watts, judgement is "to compare... ideas together, and to join them by [[Proposition|affirmation]], or disjoin then by [[negation]], according as we find them to agree or disagree".{{Sfn | Watts | 1825 | p = 115}}  He continues, "when mere ideas are joined in the mind without words, it is rather called a judgement; but when clothed with words it is called a proposition".{{Sfn | Watts | 1825 | p = 117}}  Watts' ''Logic'' follows the [[Scholasticism|scholastic]] tradition and divides propositions into universal affirmative, universal negative, particular affirmative, and particular negative.

In the third part, Watts discusses [[reasoning]] and [[argumentation]], with particular emphasis on the theory of [[syllogism]]. This was considered a centrally important part of [[classical logic]]. According to Watts, and in keeping with logicians of his day, Watts defined logic as an art (see [[liberal arts]]), as opposed to a [[science]]. Throughout ''Logic,'' Watts revealed his high conception of logic by stressing the [[practical]] side of logic, rather than the [[speculative philosophy|speculative]] side. According to Watts, as a practical art, logic can be really useful in any inquiry, whether it is an inquiry in the arts, or inquiry in the sciences, or inquiry of an [[ethics|ethical]] kind. Watts' emphasis on logic as a practical art distinguishes his book from others.

By stressing a practical and non-formal part of logic, Watts gave rules and directions for any kind of inquiry, including the inquiries of science and the inquiries of [[philosophy]]. These rules of inquiry were given in addition to the formal content of classical logic common to textbooks on logic from that time. Watts' conception of logic as being divided into its practical part and its speculative part marks a departure from the conception of logic of most other authors. His conception of logic is more akin to that of the later, nineteenth-century logician, [[Charles Sanders Peirce|C. S. Peirce]].{{citation needed|date=January 2014}}

Isaac Watts' ''Logic'' became the standard text on logic at [[Oxford University|Oxford]], [[Cambridge University|Cambridge]], [[Harvard University|Harvard]] and [[Yale University|Yale]], being used at Oxford for well over 100 years. C. S. Peirce, the great nineteenth-century logician, wrote favourably of Watts' ''Logic''. When preparing his own textbook, titled ''A Critick of Arguments: How to Reason'' (also known as the ''Grand Logic''), Peirce wrote,  'I shall suppose the reader to be acquainted with what is contained in Dr Watts' ''Logick'', a book... far superior to the treatises now used in colleges, being the production of a man distinguished for good sense.'&lt;ref&gt;Peirce, C. S. (1933) ''The Collected Papers of Charles Sanders Peirce, vol. II'', Paul Weiss and Charles Hartshorne, eds. Cambridge MASS, Harvard University Press{{page needed|date=January 2014}}&lt;/ref&gt;

Watts followed the ''Logic'' in 1741 by a supplement, ''The Improvement of the Mind.'' This also went through numerous editions and later inspired [[Michael Faraday]]. It was also widely used as a moral textbook in schools.

==Legacy, honours and memorials==
[[File:Isaac Watts DD tomb in Bunhill Fields.jpg|thumb|left|Isaac Watts' tomb in [[Bunhill Fields]]]]
[[File:Abneysharelist.jpeg|thumb|upright|London's only public statue to Isaac Watts is in [[Abney Park Cemetery|Abney Park]], [[Stoke Newington]].]]
[[File:Watts Park.jpg|thumb|Statue of Isaac Watts in Watts Park, Southampton (city of his birth)]]

On his death, Isaac Watts' papers were given to [[Yale University]] in the [[Colony of Connecticut]], which nonconformists (Puritans/Congregationalists) had established. [[King Edward VI School, Southampton]], which he attended, named one of its [[House system|houses]] "Watts" in his honour.

The [[Church of England]] and [[Lutheran Church]] remember Watts (and his ministerial service) annually in the [[Calendar of Saints (Lutheran)|Calendar of Saints]] on 25 November, and the [[Episcopal Church (USA)|Episcopal Church]] on the following day.

The earliest surviving built memorial to Isaac Watts is at [[Westminster Abbey]]; this was completed shortly after his death. His much-visited chest tomb at [[Bunhill Fields]] dates from 1808, replacing the original that had been paid for and erected by [[Lady Mary Abney]] and the Hartopp family.&lt;ref&gt;{{NHLE | num=1396517 | desc=Monument to Isaac Watts, East Enclosure | accessdate=24 February 2014}}&lt;/ref&gt; A stone bust of Watts is installed at the nonconformist [[Dr Williams's Library]], in central London. The earliest public statue, erected in 1845, stands at [[Abney Park Cemetery|Abney Park]], where Watts had lived for more than 30 years at the manor house, where he also died. The park was later devoted to uses as a cemetery and public arboretum. A later, rather similar statue was funded by public subscription and erected in a new Victorian public park named for Watts in Southampton, the city of his birth. In the mid-nineteenth century, the Congregational Dr Watts Memorial Hall was built in Southampton and named for him. After [[World War II]], it was lost to redevelopment. The Isaac Watts Memorial United Reformed Church was built on the site and named for him. One of the earliest built memorials may also now be lost: a bust to Watts that was commissioned on his death for the London chapel with which he was associated. The chapel was demolished in the late 18th century; remaining parts of the memorial were rescued at the last minute by a wealthy landowner for installation in his chapel near [[Liverpool]]. It is unclear whether the bust survives. The stone statue in front of the [[Abney Park Chapel]] at Dr Watts' Walk, Abney Park Cemetery, was erected in 1845 by public subscription. It was designed by the leading British [[Sculpture|sculptor]], [[Edward Hodges Baily]] [[Royal Academy|RA]] [[Fellow of the Royal Society|FRS]]. A scheme for a commemorative statue on this spot had first been promoted in the late 1830s by [[George Collison]], who in 1840 published an engraving as the frontispiece of his book about cemetery design in Europe and America; and at [[Abney Park Cemetery]] in particular. This first cenotaph proposal was never commissioned, and Baily's later design was adopted in 1845. In 1974, the City of Southampton (Watts' home city) commemorated the 300 year anniversary of his birth by commissioning the biography ''Isaac Watts Remembered'', written by David G. Fountain, who like Watts, was also a nonconformist minister from Southampton.

==Cultural or contemporary influences==
[[File:Watts-BusyBee.png|thumb|left|"[[wikisource:Against Idleness and Mischief|Against Idleness and Mischief]]"]]

One of Watts' best-known poems was an exhortation "[[wikisource:Against Idleness and Mischief|Against Idleness and Mischief]]" in ''[[Divine Songs for Children]].'' This was [[parody|parodied]] by [[Lewis Carroll]] in the poem "[[How Doth the Little Crocodile]]", included in his book ''[[Alice's Adventures in Wonderland]].'' His parody is better known than Watts' original poem. The poem was also featured in the segment on the cartoon programme "Rocky and His Friends" called "Bullwinkle's Corner", in which Bullwinkle Moose recites poetry. In this case, the poem was titled "The Bee", with no author credit.

In his novel ''[[David Copperfield (novel)|David Copperfield]]'' (1850), [[Charles Dickens]] has school master Dr. Strong quote from Watts' "Against Idleness and Mischief".

The 1884 [[comic opera]] ''[[Princess Ida]]'' includes a punning reference to Watts in Act I.  At Princess Ida's women's university, no males are allowed. Her father King Gama says that "She'll scarcely suffer Dr. Watts' 'hymns'".

A poem often referred to as "False Greatness" by [[Joseph Merrick]] ("The Elephant Man"), which was used in writing or "signature block" by Merrick, starting "Tis true, my form is something odd
but blaming me, is blaming God..." is often (incorrectly) quoted or cited as a work by Isaac Watts. In fact only the last few sentences were penned by Watts ("False Greatness", book II-Horae lyricae 1743) starting "Mylo, forbear to call him bless'd That only boasts a large estate..."&lt;ref&gt;Watts, Isaac, ''[https://books.google.com/books?id=1bMyAQAAMAAJ&amp;pg=PA193&amp;lpg= The Poems of Isaac Watts, Volumes 44–46]'', Press of C. Whittingham, 1822, p. 193.&lt;/ref&gt;
{{clear left}}

==Works==

===Books===
* ''[https://archive.org/details/horael00watt Horae Lyricae: Poems, Chiefly of the Lyric Kind]'' (2nd ed. 1709)
* ''[https://archive.org/details/staaaimi00watt Psalms of David: Imitated in the Language of the New Testament, and Apply'd to the Christian State and Worship]''(1719)
* ''[https://archive.org/details/logickorrightuse00wattuoft Logick: or, the Right use of Reason in the Enquiry After Truth, with a Variety of Rules to Guard Against Error in the Affairs of Religion and Human Life, as Well as in the Sciences]'' (1726)
* ''[https://archive.org/details/cu31924029064751 The Strength and Weakness of Human Reason: or, the Important Question about the Sufficiency of Reason to Conduct Mankind to Religion and Future Happiness, Argued Between an Inquiring Deist and a Christian Divine: and the Debate Compromis'd and Determin'd to the Satisfaction of Both]'' (1731)
* ''[https://archive.org/details/faithpracticerep00watt Faith and Practice: Represented in Fifty-Four Sermons on the Principal Heads of the Christian Religion: Preached at Berry-street, 1733]'' (1739)
* ''[https://archive.org/details/suppimproveofmin00wattuoft The Improvement of the Mind: or, a Supplement to the Art of Logick: Containing a Variety of Remarks and Rules for the Attainment and Communication of Useful Knowledge, in Religion, in the Sciences, and in Common Life] (1741) 
**[https://archive.org/details/improvementofmin01wattuoft Vol 1] [https://archive.org/details/improvementofmin02wattuoft Vol 2] at [[Internet Archive|The Internet Archive]] (1768, 1773, [https://archive.org/details/remaimprovement00wattuoft 1787 edition])
* ''The Knowledge of the Heavens and the Earth Made Easy ...'', first edition, 1726; [https://books.google.com/books/about/The_knowledge_of_the_heavens_and_the_ear.html?id=0NYPAAAAYAAJ&amp;redir_esc=y 1760 edition] at [[Google Books]]
* ''[https://archive.org/details/doctrinepassion00wattgoog The Doctrine of the Passions – Explain'd and Improv'd]'', [fifth edition] (1795)
* ''A Short View of the Whole Scripture History: With a Continuation of the Jewish Affairs From the Old Testament Till the Time of Christ; and an Account of the Chief Prophesies that Relate to Him''[https://archive.org/details/ashortviewofwhol00wattrich]
* Watts is thought to be the author of the tract: ''An Essay on the Freedom of Will in God and Creatures'' [https://archive.org/details/essayonfreedomof00wattuoft (copy on The Internet Archive)].
* ''[[Divine Songs Attempted in Easy Language for the Use of Children]]'' (1715)

===Hymns===
Watts' hymns include:

*''[[Joy to the World]]'' (based on [[Psalm 98]], tune by an unknown composer using fragments from [[Handel]], first published in England in 1833, popularized by American [[Lowell Mason]])
*[http://www.hymntime.com/tch/htm/m/a/r/marching.htm ''Come ye that Love the Lord''] (often sung with the chorus [and titled] "We’re marching to Zion")
*[http://www.hymntime.com/tch/htm/c/o/m/e/comehshd.htm ''Come Holy Spirit, Heavenly Dove'']
*[http://www.hymntime.com/tch/htm/j/s/r/jsreign.htm ''Jesus Shall Reign Where’er the Sun''] (based on [[Psalm 72]])
*''[[Our God, Our Help in Ages Past]]'' (based on [[Psalm 90]])
*''[[When I Survey the Wondrous Cross]]''
*[http://www.hymntime.com/tch/htm/a/l/a/alasand.htm ''Alas! and Did My Saviour Bleed'']
*[http://www.hymntime.com/tch/htm/h/s/w/hsweetaw.htm ''How Sweet and Aweful Is the Place'']
*[http://www.hymntime.com/tch/htm/t/h/i/thisitdl.htm ''This Is the Day the Lord Hath Made'']
*[http://hymntime.com/tch/htm/t/i/s/tisbythy.htm '''Tis by Thy Strength the Mountains Stand'']
*[http://hymntime.com/tch/htm/t/i/s/tisbythy.htm '''When I Can See My Title Clear'']
*[http://www.hymntime.com/tch/htm/i/s/i/isingthe.htm ''I Sing the Mighty Power of God''] (originally entitled ''Praise for Creation and Providence'' from ''[[Divine Songs Attempted in Easy Language for the Use of Children]]'')
*[http://www.hymntime.com/tch/htm/m/s/h/mshpwill.htm ''My Shepherd Will Supply My Need''] (based on [[Psalm 23]])
*[http://cyberhymnal.org/htm/b/l/blessomy.htm ''Bless, O My Soul! the Living God''] (based on [[Psalm 103]])

Many of Watts' hymns are included in the Anglican [[Hymns Ancient and Modern]], the Oxford [[Book of Common Praise]], the [[Christadelphian]] hymnal, the Episcopal Church's ''Hymnal 1982'', ''Evangelical Lutheran Worship'', the [[Baptist Hymnal]], the Presbyterian [[Trinity Hymnal]], and the Methodist [[Hymns and Psalms]]. Many of his texts are also used in the American hymnal, ''The [[Sacred Harp]],'' using what is known as the [[shape note]] notation used for teaching non-musicians. Several of his hymns are used in the hymnals of the [[Church of Christ, Scientist]] and [[The Church of Jesus Christ of Latter-day Saints]].

==See also==&lt;!-- Please respect alphabetical order --&gt;
{{Portal|Saints}}
*[[Africa (William Billings)]]
*[[Congregational church]]
*[[English Dissenter]]
*[[Independent (religion)]]
*[[Puritan]]

==Notes==
{{Reflist}}

==References==
* {{cite book |editor-first=J. A. |editor-last=Jones |title=Bunhill Memorials: sacred reminiscences of three hundred ministers and other persons of note, who are buried in Bunhill Fields, of every denomination |place=London |publisher=James Paul |year=1849 |pages=298–304 }}
* {{cite book |last=Marini |first=Stephen A. |year=2003 | title = Sacred Song in America: Religion, Music, and Public Culture |place=Urbana |publisher=University of Illinois Press|ref=harv }}
* {{cite book |title=Trust in Freedom: The Story of [[Newington Green Unitarian Church]] 1708–1958 |first=Michael |last=Thorncroft |publisher=Privately printed for church trustees |year=1958 |chapter=The Fertile Soil; The Church is Built; The Early Years (1714–1758); The Age of Richard Price; New Causes for Old; The Ideal of Service; The Lights Go Out; The Present Day}}.
* {{cite book |last=Watts |first=Isaac |year=1825 |edition=reprint |title=Logic or the Right Use of Reason in the Inquiry After Truth; with a Variety of Rules to Guard Against Error in the Affairs of Religion and Human Life, as well as in the sciences |publisher=Kessinger |place=US|ref=harv}}.

==External links==
{{wikiquote}}
{{wikisource author}}
{{Commons category}}
* [http://www.eighteenthcenturypoetry.org/authors/pers00292.shtml Isaac Watts] at the [http://www.eighteenthcenturypoetry.org/ Eighteenth-Century Poetry Archive (ECPA)]
* {{Gutenberg author |id=Watts,+Isaac | name=Isaac Watts}}
* {{Internet Archive author |sname=Isaac Watts}}
* {{Librivox author |id=194}}
*{{prdl|78}}
*[https://web.archive.org/web/20080407155135/http://204.239.18.204/index.php?title=A_Solemn_Address_to_the_Deity ''A Solemn Address to the Great and Ever Blessed God''] (1802), originally published as ''A Faithful Inquiry after the Ancient and Original Doctrine of the Trinity'' (1745)
*{{webarchive |url=https://web.archive.org/web/20091027063116/http://geocities.com/Athens/Forum/7632/ |date=27 October 2009 |title=The Isaac Watts Fan Club }} background info and midi files
*[http://www.hymntime.com/tch/bio/w/a/t/watts_i.htm Hymns by Isaac Watts]
*[http://www.hymntime.com/tch/htm/o/g/o/ogohiap.htm O God our help in ages past] (school hymn of King Edward VI School, Southampton, Watts' [[alma mater]])
*[http://mcbs.springroad.org.uk/book/9780903556576 Isaac Watts Remembered] (David G. Fountain, 1974) Biography commissioned by Southampton City Council (his home city) to commemorate 300 years of his birth ({{ISBN|978-0903556576}})
*[https://web.archive.org/web/20080411102550/http://www.monergism.com/directory/link_category/Puritans/Misc-Puritans/Isaac-Watts/ Monergism.com Isaac Watts] Links to works of Isaac Watts
*[https://www.google.com/books?id=5EwWAAAAYAAJ&amp;printsec=frontcover&amp;dq=Isaac+Watts+Logic&amp;lr= ''Logic'' by Isaac Watts] 
*[https://books.google.com/books/about/The_Improvement_of_the_Mind.html?id=LMwAAAAAcAAJ&amp;redir_esc=y ''Improvement of the Mind'' by Isaac Watts] 
*[https://books.google.com/books?id=xsUOAAAAIAAJ&amp;pg=PA327&amp;dq=isaac+watts++Harmony+of+all+the+Religions&amp;hl=en&amp;ei=CytXTei8NInEgAel76WfDQ&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=1&amp;ved=0CC0Q6AEwAA#v=onepage&amp;q=pentecost&amp;f=false ''The Harmony of all the Religions which God ever Prescribed to Men and all his Dispensations towards them'' by Isaac Watts] 
*[https://books.google.com/books?id=xxwtAAAAYAAJ&amp;pg=PR1#v=onepage&amp;q&amp;f=false ''The Ruin and Recovery of Mankind'' by Isaac Watts]

{{Authority control}}

{{DEFAULTSORT:Watts, Isaac}}
[[Category:1674 births]]
[[Category:People from Southampton]]
[[Category:1748 deaths]]
[[Category:English nonconformist hymnwriters]]
[[Category:English evangelicals]]
[[Category:People celebrated in the Lutheran liturgical calendar]]
[[Category:English Congregationalist ministers]]
[[Category:17th-century Congregationalist ministers]]
[[Category:People educated at King Edward VI School, Southampton]]
[[Category:Anglican saints]]
[[Category:English logicians]]
[[Category:English male poets]]
[[Category:English Calvinist and Reformed theologians]]
[[Category:History of logic]]
[[Category:18th-century Congregationalist ministers]]
[[Category:17th-century English clergy]]
[[Category:18th-century English writers]]
[[Category:18th-century male writers]]
[[Category:Burials at Bunhill Fields]]
[[Category:English philosophers]]
[[Category:18th-century Calvinist and Reformed theologians]]
[[Category:Calvinist and Reformed hymnwriters]]
[[Category:Congregationalist hymnwriters]]
[[Category:English male non-fiction writers]]
[[Category:18th-century English poets]]</text>
      <sha1>sriw9lkeygna5rxeayuz87jyi9nmifi</sha1>
    </revision>
  </page>
  <page>
    <title>Joaquim Gomes de Souza</title>
    <ns>0</ns>
    <id>47375779</id>
    <revision>
      <id>861377772</id>
      <parentid>728184635</parentid>
      <timestamp>2018-09-27T00:17:54Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>/* References */add authority control, test</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4928">'''Joaquim Gomes de Souza "Souzinha"''' (15 February 1829, in [[Itapecuru Mirim]] – 1 June 1864, in [[London]]) was a [[Brazil]]ian [[mathematician]] who worked on [[numerical analysis]] and [[differential equations]].&lt;ref name="AntologiaBio"&gt;Humberto de Campos, "Souzinha, O Matemático", contained in book: {{cite book|author=Malba Tahan|title=Antologia da matemática: histórias, fantasias, biografias, numéricas, problemas, curiosidades, recreaçoes numéricas, problemas célebres, erros famosos, etc|url=https://books.google.com/books?id=huLuAAAAMAAJ|year=1964|publisher=Edição Saraiva|pages=185–190|language=Portuguese}}&lt;/ref&gt; He was a pioneer on the study of mathematics in Brazil, and was described by [[José Leite Lopes]] as "the first great mathematician from Brazil".&lt;ref&gt;Free translation of the original in Portuguese: "primeiro vulto matemático do Brasil" in: José Leite Lopes, "Joaquim Gomes de Souza", ''Ciência e Sociedade'', Rio de Janeiro, 1989&lt;/ref&gt;

In 1844, Gomes de Souza enrolled at the Faculdade de Medicina do Rio de Janeiro (now a part of the [[Federal University of Rio de Janeiro]]) to study [[medicine]]. He had a deep love for the natural sciences, which led him to also be interested in mathematics, and so he started to learn mathematics as a self-taught in parallel with his studies of medicine.&lt;ref name="AntologiaBio" /&gt;

In 1848, he obtained his doctorate in mathematics from the Escola Real Militar, with the thesis ''Dissertação Sobre o Modo de Indagar novos Astros sem o Auxílio das Observações Directas''&lt;ref&gt;D'AMBROSIO, Ubiratan. Joaquim Gomes de Souza, o. “Souzinha” (1829-1864). In: MARTINS, R. A.; MARTINS,. L. A. C. P.; SILVA, C. C.; FERREIRA, J. H. M. (eds.) ''Filosofia e história da ciência no Cone Sul''.&lt;/ref&gt; (Dissertation about the means of investigating new celestial objects without the aid of direct observations).

He later went to the Sorbonne, in France, where he continued his mathematical studies. He was a personal friend of [[Augustin-Louis Cauchy|Cauchy]], of whose classes he attended (in one of them, Souza spotted a mathematical mistake by Cauchy, he then asked his license and corrected it on the blackboard). In 1856, he obtained a doctorate in medicine from Paris Faculty of Medicine. In the same year, he presented his mathematical works at the ''[[Académie des sciences]]''.&lt;ref name="AntologiaBio" /&gt;

Souza held a paid public post in Brazil, and after much time in Europe, he was noticed he should return immediately to Brazil because he had been elected a member of the parliament. Souza had already married Rosa Edith in England and then had to return to Brazil without her.&lt;ref name="AntologiaBio" /&gt;

In his book ''[https://archive.org/details/mlangesdecalcul00henrgoog Mélanges de calcul intégral]'' (1882), Souza aimed to obtain a general method to solve [[Partial differential equation|PDE]]s, according to [[Manfredo do Carmo]]: "[in his book] He [Souza] employed methods not entirely rigorous and it is not clear exactly how much of his work would remain if submitted to a careful scrutiny; as far as I know, it was never put to such a test."&lt;ref&gt;{{cite book|title=Manfredo P. do Carmo – Selected Papers|url=https://books.google.com/books?id=a-BxmZJ3hjQC&amp;pg=PA368|date=2 April 2012|publisher=Springer Science &amp; Business Media|isbn=978-3-642-25588-5|page=368}}&lt;/ref&gt;

He died at the age of 35, in London. The cause of death was a disease of the lung.&lt;ref name="AntologiaBio" /&gt; C. S. Fernandez and C. M. Souza described his endeavorer in Europe: "He was audacious and fought with insistence for his scientific recognition in Europe. His effort was fruitless, though."&lt;ref&gt;Carlos Sanchez Fernandez and Cícero Monteiro de Souza, "[http://www.uefs.br/nef/sanchez3.pdf Joaquim Gomes de Souza e as controvérsias sobre o uso das séries divergentes no século XIX]", Ideação, Feira de Santana, n.3, pp. 131-157, jan./jun. 1999&lt;/ref&gt;

==Writings==
* ''Resoluções das Equações Numéricas'' (1850, in Portuguese)
* ''Recuel de Memoires d’Analise Mathematiques'' (1857, in French)
* ''Anthologie universelle'' (1859, in French)
* ''Mélanges de calcul intégral'' (1882, posthumous, in French)

==Further reading==
* Irine Coelho de Araujo, ''[http://www.sapientia.pucsp.br/tde_arquivos/13/TDE-2012-08-21T06:14:00Z-12819/Publico/Irene%20Coelho%20de%20Araujo.pdf Joaquim Gomes de Souza (1829-1864): A construção de uma imagem de Souzinha]'', São Paulo, 2012
* Carlos Ociran Silva Nascimento, ''[http://www.bibliotecadigital.unicamp.br/document/?code=000437140 Alguns aspectos da obra matematica de Joaquim Gomes de Souza]'', Campinas, 2008

==References==
{{reflist}}

{{authority control}}

{{DEFAULTSORT:Gomes de Souza, Joaquim}}
[[Category:1829 births]]
[[Category:1864 deaths]]
[[Category:Brazilian mathematicians]]
[[Category:Brazilian politicians]]
[[Category:Mathematical analysts]]
[[Category:PDE theorists]]</text>
      <sha1>oqbtuyruu2g8rd67mcv8aw95i2cp6la</sha1>
    </revision>
  </page>
  <page>
    <title>Joint Statistical Meetings</title>
    <ns>0</ns>
    <id>15571662</id>
    <revision>
      <id>862292768</id>
      <parentid>857855574</parentid>
      <timestamp>2018-10-03T12:41:50Z</timestamp>
      <contributor>
        <username>Andland</username>
        <id>314800</id>
      </contributor>
      <comment>/* Past Meetings */ added 2018 attendance</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5516">The '''Joint Statistical Meetings''' ('''JSM''') is a [[professional conference]]/[[academic conference]] for [[statistician]]s held annually every year since 1840 (usually in August). Billed as "the largest gathering of statisticians held in North America", JSM has attracted over 5000 participants in recent years. The following statistical societies are designated as official JSM partners:
* the [[American Statistical Association]] (ASA)
* the [[Institute of Mathematical Statistics]] (IMS)
* two regions of the [[International Biometric Society]] (IBS)
** the Eastern North American Region (ENAR)
** the Western North American Region (WNAR)
* the [[Statistical Society of Canada]] (SSC)
* [[International Society for Bayesian Analysis]] (ISBA)
* [[International Chinese Statistical Association]] (ICSA)
* [[International Indian Statistical Association]] (IISA)
* [[Korean International Statistical Society]] (KISS)
The founding members of JSM were the ASA, IMS, IBS, and SSC.&lt;ref name=JSM2013&gt;{{cite web|title=2013 Joint Statistical Meetings|url=https://www.amstat.org/meetings/jsm/2013/index.cfm|publisher=American Statistical Association|accessdate=15 February 2013}}&lt;/ref&gt;

In addition to committee meetings, JSM activities include
* a [[career placement]] service
* [[continuing education]] courses
* [[oral presentation]]s
* [[panel session]]s
* [[plenary session]]s
* [[poster session]]s

==Future Meetings==
{| class="wikitable"
|-
! Dates
! Location
! Venue
|- 	
| July 27 - August 1, 2019
| [[Denver, Colorado]]
|
&lt;!-- | [[tba]]---&gt;
|- 	
| August 1–6, 2020
| [[Philadelphia, Pennsylvania]]
|
&lt;!-- | [[tba]]---&gt;
|- 	
| August 7–12, 2021
| [[Seattle, Washington]]
| [[Washington State Convention &amp; Trade Center]]
|-
| August 6–11, 2022
| [[Washington, D.C.]]
|
|-
| August 5–10, 2023
| [[Toronto, Ontario, Canada]]
|
|-
| August 3–8, 2024
| [[Portland, Oregon]]
|
&lt;!-- | [[tba]]---&gt;
|}

==Past Meetings==
Since 1978 when attendance figures were first reported.&lt;ref&gt;{{cite web |url=http://www.amstat.org/newsroom/pdfs/PressKit_JSMHistory.pdf
|format=PDF|title=History of ASA Joint Statistical Meetings}} }&lt;/ref&gt;
{| class="wikitable"
|-
! Dates
! Location
! Attendance
|-
| August 14–17, 1978
| [[San Diego, California]]
| 2,650
|-
| August 13–16, 1979
| [[Washington, D.C.]]
| 3,300
|-
| August 11–14, 1980
| [[Houston, Texas]]
| 2,000
|-
| August 10–13, 1981
| [[Detroit]], [[Michigan]]
| 2,100
|-
| August 16–19, 1982
| [[Cincinnati, Ohio]]
| 2,400
|-
| August 15–18, 1983
| [[Toronto|Toronto, Ontario]]
| 3,100
|-
| August 13–16, 1984
| [[Philadelphia, Pennsylvania]]
| 3,200
|-
| August 5–8, 1985
| [[Las Vegas, Nevada]]
| 2,700
|-
| August 18–21, 1986
| [[Chicago]], [[Illinois]]
| 3,100
|-
| August 17–20, 1987
| [[San Francisco, California]]
| 3,700
|-
| August 22–25, 1988
| [[New Orleans, Louisiana]]
| 2,600
|-
| August 6–10, 1989
| [[Washington, D.C.]]
| 4,200
|-
| August 6–9, 1990
| [[Anaheim, California]]
| 2,800
|-
| August 18–22, 1991
| [[Atlanta, Georgia]]
| 3,700
|-
| August 9–13, 1992
| [[Boston, Massachusetts]]
| 4,400
|-
| August 8–12, 1993
| [[San Francisco, California]]
| 4,800
|-
| August 13–18, 1994
| [[Toronto|Toronto, Ontario]]
| 4,500
|-
| August 13–17, 1995
| [[Orlando, Florida]]
| 3,800
|-
| August 4–8, 1996
| [[Chicago]], [[Illinois]]
| 4,750
|-
| August 10–14, 1997
| [[Anaheim, California]]
| 4,200
|-
| August 9–13, 1998
| [[Dallas, Texas]]
| 3,850
|-
| August 8–12, 1999
| [[Baltimore, Maryland]]
| 5,000
|-
| August 13–17, 2000
| [[Indianapolis, Indiana]]
| 3,635
|-
| August 5–9, 2001
| [[Atlanta, Georgia]]
| 4,038
|-
| August 11–15, 2002
| [[New York City, New York]]
| 5,444
|-
| August 3–7, 2003
| [[San Francisco, California]]
| 5,542
|-
| August 8–12, 2004
| [[Toronto|Toronto, Ontario]]
| 5,138
|-
| August 7–11, 2005
| [[Minneapolis, Minnesota]]
| 5,157
|-
| August 6–10, 2006
| [[Seattle, Washington]]
| 6,034
|-
| July 29–August 2, 2007
| [[Salt Lake City, Utah]]
| 5,186
|-
| August 3–7, 2008
| [[Denver, Colorado]]
| 5,592
|-
| August 1–6, 2009
| [[Washington, D.C.]]
| 6,804
|-
| July 31–August 5, 2010
| [[Vancouver|Vancouver, British Columbia]]
| 5,600 &lt;ref&gt;[http://www.amstat.org/about/pressreleases/JSM2010MA_StatisticiansDescendonVancouverforJSM.pdf Press Release JSM Vancouver 2010] {{Dead link|date=February 2014}}&lt;/ref&gt;
|-
| July 30–August 4, 2011
| [[Miami, Florida]]
|5,300
|-
| July 28–August 2, 2012
| [[San Diego, California]]
|6,200
|-
| August 3–8, 2013
| [[Montreal|Montreal, Quebec]]
|
|-
| August 2–7, 2014
| [[Boston, Massachusetts]]
| 
|-
| August 8–13, 2015
| [[Seattle, Washington]]
|6,850 
|-
| July 30–August 4, 2016
| [[Chicago]], [[Illinois]]
|7,200 
|- 	
| July 29 - August 3, 2017
| [[Baltimore, Maryland]]
| 
|- 	
| July 28 - August 2, 2018
| [[Vancouver, British Columbia, Canada]]
| 6,346 &lt;ref&gt;[http://magazine.amstat.org/blog/2018/10/01/jsm-2018-a-tremendous-success/ JSM 2018: A Tremendous Success!]&lt;/ref&gt;
|- 
|}

==References==
{{Reflist}}&lt;!--added above External links/Sources by script-assisted edit--&gt;

==External links==
*[http://www.amstat.org/meetings/jsm/ Joint Statistical Meetings]
*[http://www.google.com/search?hl=en&amp;client=firefox-a&amp;hs=BpX&amp;rls=com.ubuntu%3Aen-US%3Aofficial&amp;q=define%3A+joint+statistical+meetings&amp;btnG=Search  Joint Statistical Meetings Defined on Google]
*[http://www.acronymfinder.com/JSM.html Other Abbreviations for JSM]

[[Category:Statistical societies]]
[[Category:Mathematics conferences]]</text>
      <sha1>7fhslinzhccwtv1o5dlqsd56lhgfth1</sha1>
    </revision>
  </page>
  <page>
    <title>KY-68</title>
    <ns>0</ns>
    <id>3884735</id>
    <revision>
      <id>713090014</id>
      <parentid>713089524</parentid>
      <timestamp>2016-04-01T21:52:33Z</timestamp>
      <contributor>
        <username>Forbes72</username>
        <id>6174461</id>
      </contributor>
      <minor/>
      <comment>removed [[Category:Cryptographic hardware]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1893">[[Image:KY-68.nsa.jpg|thumb|KY-68 tactical secure telephone]]
[[File:US Military KY-68 phone.jpg|thumb|TSEC/KY68 Basic Unit|200px]]
'''TSEC/KY-68 DSVT, '''commonly known as '''Digital Subscriber Voice Terminal''', is a US military [[Rugged computer|ruggedized]], [[full-duplex|full-]] or [[half-duplex]] tactical telephone system with a built-in [[encryption]]/[[decryption]] module for [[secure voice|secure]] traffic. 

It transmits voice and data at 16 or 32 [[kbit/s]], converting voice to a digital signal. The KY-68 can operate via civilian and military [[Electronic Switching System|switches]] in either encrypted or un-encrypted mode, or [[point-to-point (telecommunications)|point-to-point]] (encrypted mode only).

Although used primarily for secure communications, the KY-68 can also transmit to a Digital Non-secure Voice Terminal (DNVT). A local switch warns the KY-68 user with a tone signal when initiating communication with a non-secure terminal.
 
The KY-68 is [[key (cryptography)|keyed]] using an [[fill device|Electronic Transfer Device]], typically either a [[KYK-13]] or [[AN/CYZ-10]].

An almost identical office version (KY-78) features the same electronics as the KY-68, but has an exterior casing composed of lighter materials.

The KY-68 and KY-78 are approved for use with [[Classified information#Government classification|SECRET]]-classified information, and despite the KY-78 being [[compromise]]d in the early 1990s, both versions remain in use.{{Citation needed|date=December 2007}}

==See also==
* [[KY-57]]
* [[KY-58]]

== External links ==
* [http://www.jproc.ca/crypto/ky68.html Jerry Proc's page on the KY-68]
* [http://www.globalsecurity.org/military/library/policy/army/fm/11-43/c3u.htm The Signal Leader's Guide] &amp;mdash; Field Manual 11-43

{{Cryptography navbox | machines}}

[[Category:National Security Agency encryption devices]]


{{crypto-stub}}</text>
      <sha1>c5havo2oj6ekllaisv1olul6gmw1vm2</sha1>
    </revision>
  </page>
  <page>
    <title>Limits of computation</title>
    <ns>0</ns>
    <id>2855255</id>
    <revision>
      <id>847362218</id>
      <parentid>846661022</parentid>
      <timestamp>2018-06-24T20:03:15Z</timestamp>
      <contributor>
        <username>Bibcode Bot</username>
        <id>14394459</id>
      </contributor>
      <minor/>
      <comment>Adding 0 [[arXiv|arxiv eprint(s)]], 0 [[bibcode|bibcode(s)]] and 1 [[digital object identifier|doi(s)]]. Did it miss something? Report bugs, errors, and suggestions at [[User talk:Bibcode Bot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8981">==Physical limits==
There are several physical and practical limits to the amount of [[computation]] or [[data storage device|data storage]] that can be performed with a given amount of mass, volume, or energy:

* The [[Bekenstein bound]] limits the amount of information that can be stored within a spherical volume to the entropy of a black hole with the same surface area.
* Thermodynamics limit the data storage of a system based on its energy, number of particles and particle modes. In practice it is a stronger bound than Bekenstein bound.&lt;ref&gt;{{cite journal|last=Sandberg|first=Anders|title=The Physics of Information Processing Superobjects: Daily Life Among the Jupiter Brains|date=22 December 1999|url=http://transhumanist.com/volume5/Brains2.pdf}}&lt;/ref&gt;
* [[Landauer's principle]] defines a lower theoretical limit for energy consumption: {{math|''[[Boltzmann constant|k]]T'' ln 2}} [[joule]]s consumed per irreversible state change, where ''k'' is the [[Boltzmann constant]] and ''T'' is the operating temperature of the computer.&lt;ref&gt;{{cite journal|title=The physics of forgetting: Landauer's erasure principle and information theory|journal=Contemporary Physics|volume=42|issue=1|doi=10.1080/00107510010018916|pages=25–60|issn=0010-7514|eissn=1366-5812|last=Vitelli|first=M.B.|last2=Plenio|first2=V.|date=2001|url=http://www3.imperial.ac.uk/pls/portallive/docs/1/55905.PDF|arxiv=quant-ph/0103108}}&lt;/ref&gt;  [[Reversible computing]] is not subject to this lower bound.  ''T'' cannot, even in theory, be made lower than 3 [[kelvins]], the approximate temperature of the [[cosmic microwave background radiation]], without spending more energy on cooling than is saved in computation.
* [[Bremermann's limit]] is the maximum computational speed of a self-contained system in the material universe, and is based on mass-energy versus quantum uncertainty constraints.
* The [[Margolus–Levitin theorem]] sets a bound on the maximum computational speed per unit of energy: 6 &amp;times; 10&lt;sup&gt;33&lt;/sup&gt; operations per second per [[joule]]. This bound, however, can be avoided if there is access to quantum memory. Computational algorithms can then be designed that require arbitrarily small amount of energy/time per one elementary computation step.&lt;ref&gt;{{cite journal|last=Jordan|first=Stephen P.|title=Fast quantum computation at arbitrarily low energy|journal = Phys. Rev. A | volume = 95 | pages=032305 | year=2017 |arxiv = 1701.01175|url=https://arxiv.org/pdf/1701.01175.pdf | doi=10.1103/physreva.95.032305|bibcode=2017PhRvA..95c2305J}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Sinitsyn|first=Nikolai A.|title=Is there a quantum limit on speed of computation?|year=2017|arxiv = 1701.05550|url=https://arxiv.org/pdf/1701.05550.pdf|bibcode=2018PhLA..382..477S|doi=10.1016/j.physleta.2017.12.042}}&lt;/ref&gt;

==Building devices that approach physical limits==
Several methods have been proposed for producing computing devices or data storage devices that approach physical and practical limits:
* A cold [[degenerate star]] could conceivably be used as a giant data storage device, by carefully perturbing it to various excited states, in the same manner as an atom or [[quantum well]] used for these purposes. Such a star would have to be artificially constructed, as no natural degenerate stars will cool to this temperature for an extremely long time. It is also possible that [[nucleon]]s on the surface of [[neutron star]]s could form complex "molecules",&lt;ref&gt;{{cite encyclopedia |year= |title=Life on neutron stars |encyclopedia=The Internet Encyclopedia of Science |publisher= |location= |url=http://www.daviddarling.info/encyclopedia/N/neutronstarlife.html }}&lt;/ref&gt; which some have suggested might be used for computing purposes,&lt;ref&gt;{{cite web|url=http://www.cs.usu.edu/~degaris/essays/femtotech.html |title=Femtotech? (Sub)Nuclear Scale Engineering and Computation |accessdate=2006-10-30 |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20041025030505/http://www.cs.usu.edu/~degaris/essays/femtotech.html |archivedate=October 25, 2004 |df= }}&lt;/ref&gt; creating a type of [[computronium]] based on [[femtotechnology]], which would be faster and denser than computronium based on [[nanotechnology]].
* It may be possible to use a [[black hole]] as a data storage or computing device, if a practical mechanism for extraction of contained information can be found. Such extraction may in principle be possible ([[Stephen Hawking]]'s proposed resolution to the [[black hole information paradox]]). This would achieve storage density exactly equal to the [[Bekenstein bound]]. [[Seth Lloyd]] calculated&lt;ref&gt;https://arxiv.org/pdf/quant-ph/9908043.pdf&lt;/ref&gt; the computational abilities of an "ultimate laptop" formed by compressing a kilogram of matter into a black hole of radius 1.485 &amp;times; 10&lt;sup&gt;−27&lt;/sup&gt; meters, concluding that it would only last about 10&lt;sup&gt;−19&lt;/sup&gt; seconds before [[Hawking radiation#Black hole evaporation|evaporating]] due to [[Hawking radiation]], but that during this brief time it could compute at a rate of about 5 &amp;times; 10&lt;sup&gt;50&lt;/sup&gt; operations per second, ultimately performing about 10&lt;sup&gt;32&lt;/sup&gt; operations on 10&lt;sup&gt;16&lt;/sup&gt; bits (~1 [[petabyte|PB]]). Lloyd notes that "Interestingly, although this hypothetical computation is performed at ultra-high densities and speeds, the total number of bits available to be processed is not far from the number available to current computers operating in more familiar surroundings."&lt;ref&gt;{{cite journal|last=Lloyd |first=Seth |authorlink= |coauthors= |year=2000 |month= |title=Ultimate physical limits to computation |journal=[[Nature (journal)|Nature]] |volume=406 |issue=6799 |pages=1047–1054 |doi=10.1038/35023282 |url=http://puhep1.princeton.edu/~mcdonald/examples/QM/lloyd_nature_406_1047_00.pdf |accessdate= |quote= |pmid=10984064 |arxiv=quant-ph/9908043 |deadurl=yes |archiveurl=https://web.archive.org/web/20080807173904/http://puhep1.princeton.edu/~mcdonald/examples/QM/lloyd_nature_406_1047_00.pdf |archivedate=2008-08-07 |df= }}&lt;/ref&gt;
* In ''[[The Singularity is Near]]'', Ray Kurzweil cites the calculations of Seth Lloyd that a universal-scale computer is capable of 10&lt;sup&gt;90&lt;/sup&gt; operations per second. The mass of the universe can be estimated at 3 × 10&lt;sup&gt;52&lt;/sup&gt; kilograms. If all matter in the universe was turned into a black hole it would have a lifetime of 2.8 × 10&lt;sup&gt;139&lt;/sup&gt; seconds before evaporating due to Hawking radiation. During that lifetime such a universal-scale black hole computer would perform 2.8 × 10&lt;sup&gt;229&lt;/sup&gt; operations.&lt;ref&gt;{{Cite book|title=The Singularity is Near|last=Kurzweil|first=Ray|publisher=New York: Viking|year=2005|isbn=|location=|pages=911}}&lt;/ref&gt;
* It is expected that a black hole computer cannot find exact solutions to all [[NP-completeness|NP-Complete]] problems in polynomial time since they scale as &lt;math&gt;O({2^N})&lt;/math&gt; or &lt;math&gt;O({N!})&lt;/math&gt;. Searching for an exact solution to a [[Travelling salesman problem|Traveling Salesman Problem]] with an instance of 3000 locations would take both the standard [[Grover's algorithm]] with &lt;math&gt;O(\sqrt{N})&lt;/math&gt; steps and [[De Broglie–Bohm theory|Bohmian]] [[Grover's algorithm]] with &lt;math&gt;O(\sqrt[3]{N})&lt;/math&gt; steps more than 2.8 × 10&lt;sup&gt;229&lt;/sup&gt; operations.

==Abstract limits in computer science==
In the field of theoretical [[computer science]] the [[Computability Theory|computability]] and [[Computational complexity theory|complexity]] of computational problems are often sought-after.  Computability theory describes the degree to which problems are computable; whereas complexity theory describes the asymptotic degree of resource consumption.  Computational problems are therefore confined into [[complexity classes]].  The [[arithmetical hierarchy]] and [[polynomial hierarchy]] classify the degree to which problems are respectively computable and computable in polynomial time.  For instance, the level &lt;math&gt;\Sigma^0_0=\Pi^0_0=\Delta^0_0&lt;/math&gt; of the arithmetical hierarchy classifies computable, partial functions.  Moreover, this hierarchy is strict such that at any other class in the arithmetic hierarchy classifies strictly [[Uncomputable function|uncomputable]] functions.

==Loose and tight limits==
Many limits derived in terms of physical constants and abstract models of computation in Computer Science are loose.&lt;ref&gt;{{cite journal |last=Markov |first=Igor |authorlink= |coauthors= |year=2014 |month= |title=Limits on Fundamental Limits to Computation|journal=[[Nature (journal)|Nature]] |volume=512 |issue= |pages=147–154|doi=10.1038/nature13570 |url= |accessdate= |quote= |arxiv=1408.3821 |bibcode=2014Natur.512..147M}}&lt;/ref&gt; Very few known limits directly obstruct leading-edge technologies, but many engineering obstacles currently cannot be explained by closed-form limits.

==See also==
*[[Transcomputational problem]]

==References==
{{Reflist}}

[[Category:Theory of computation]]
[[Category:Limits of computation|*]]</text>
      <sha1>rqheqfknt34zm3nyjmh24so6my5y2pb</sha1>
    </revision>
  </page>
  <page>
    <title>Linear seismic inversion</title>
    <ns>0</ns>
    <id>39291986</id>
    <revision>
      <id>868590642</id>
      <parentid>858516684</parentid>
      <timestamp>2018-11-13T04:49:19Z</timestamp>
      <contributor>
        <username>RockMagnetist</username>
        <id>12961884</id>
      </contributor>
      <comment>shortened short description</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="25117">{{short description|Interpretation of seismic data using linear model}}
[[Inverse problem|Inverse modeling]] is a mathematical technique where the objective is to determine the physical properties of the subsurface of an earth region that has produced a given [[seismogram]]. Cooke and Schneider (1983)&lt;ref name="Cook1983"&gt;{{cite journal|last=Cooke|first=D. A.|author2=Schneider W. A. |title=Generalized linear inversion of reflection seismic data|journal=Geophysics|date=June 1983|volume=48|issue=6|pages=665–676|doi=10.1190/1.1441497}}&lt;/ref&gt; defined it as calculation of the earth’s structure and physical [[parameters]] from some set of observed [[seismic]] data. The underlying assumption in this method is that the collected seismic data are from an earth structure that matches the cross-section computed from the inversion [[algorithm]].&lt;ref name="Pica1990"&gt;{{cite journal|last=Pica|first=A.|author2=Diet J. P. |author3=Tarantola A. |title=Nonlinear inversion of seismic reflection data in a laterally invariant medium|journal=Geophysics|date=March 1990|volume=55|issue=3|pages=284–292|doi=10.1190/1.1442836}}&lt;/ref&gt; Some common earth properties that are inverted for include acoustic velocity, [[Formation (stratigraphy)|formation]] and fluid [[Density|densities]], [[acoustic impedance]], [[Poisson's ratio]], formation compressibility, shear rigidity, [[porosity]], and fluid saturation.

The method has long been useful for geophysicists and can be categorized into two broad types:&lt;ref name="Earthworks"&gt;{{cite journal|last=Francis|first=A.M.|title=Understanding Stochastic and Seismic Inversion|journal=First Break|date=November 2006|volume=24|issue=11|doi=10.3997/1365-2397.2006026}}&lt;/ref&gt; [[Deterministic algorithm|Deterministic]] and [[stochastic]] inversion. Deterministic inversion methods are based on comparison of the output from an earth model with the observed field data and continuously updating the earth model parameters to minimize a function, which is usually some form of difference between model output and field observation. As such, this method of inversion to which linear inversion falls under is posed as an minimization problem and the accepted earth model is the set of model parameters that minimizes the [[objective function]] in producing a numerical seismogram which best compares with collected field seismic data.

On the other hand, stochastic inversion methods are used to generate constrained models as used in [[reservoir]] flow simulation, using geostatistical tools like [[kriging]]. As opposed to deterministic inversion methods which produce a single set of model parameters, stochastic methods generate a suite of alternate earth model parameters which all obey the model constraint. However, the two methods are related as the results of deterministic models is the average of all the possible non-unique solutions of stochastic methods.&lt;ref name="Earthworks"/&gt; Since seismic linear inversion is a deterministic inversion method, the stochastic method will not be discussed beyond this point.

[[File:Linear Seismic Inversion Flow Chart.jpg|thumb|upright=1.5|Figure 1: Linear Seismic Inversion Flow Chart]]

== Linear inversion ==

The [[Deterministic algorithm|deterministic]] nature of [[Inverse problem|linear inversion]] requires a [[Function (mathematics)|functional]] relationship which models, in terms of the earth model [[parameters]], the seismic variable to be inverted. This functional relationship is some mathematical model derived from the fundamental laws of physics and is more often called a forward model. The aim of the technique is to minimize a function which is dependent on the difference between the convolution of the forward model with a source [[wavelet]] and the field collected [[seismic trace]]. As in the field of optimization, this function to be minimized is called the [[objective function]] and in convectional inverse modeling, is simply the difference between the convolved forward model and the seismic trace. As earlier mentioned, different types of variables can be inverted for but for clarity, these variables will be referred to as the [[Wave impedance|impedance]] series of the earth model. In the following subsections we will describe in more detail, in the context of linear inversion as a minimization problem, the different components that are necessary to invert seismic data.

=== Forward model ===

The centerpiece of seismic linear inversion is the forward model which models the generation of the experimental data collected.&lt;ref name="Cook1983"/&gt; According to Wiggins (1972),&lt;ref name="wiggns1972"&gt;{{cite journal|last=Wiggins|first=Ralph|title=The general linear inverse problem: Implication of surface waves and free oscillations for Earth structure|journal=Reviews of Geophysics|date=February 1972|volume=10|issue=1|pages=251–285|doi=10.1029/RG010i001p00251}}&lt;/ref&gt; it provides a functional (computational) relationship between the model parameters and calculated values for the observed traces. Depending on the seismic data collected, this model may vary from the classical [[wave equation]]s for predicting [[particle displacement]] or fluid pressure for sound wave propagation through rock or fluids, to some variants of these classical equations. For example, the forward model in Tarantola (1984)&lt;ref&gt;{{cite journal|last=Tarantola|first=A.|title=Linearized and inversion of seismic reflection-data|journal=Geophysical Prospecting|year=1984|volume=32|issue=6|pages=908–1015|doi=10.1111/j.1365-2478.1984.tb00751.x}}&lt;/ref&gt; is the wave equation for pressure variation in a liquid media during seismic wave propagation while by assuming constant velocity layers with plane interfaces, Kanasewich and Chiu (1985)&lt;ref name="kan"&gt;{{cite journal|last=Kanasewich|first=E. R.|author2=Chiu S. K. L |title=Least squares inversion of spatial seismic refraction data|journal=Bulletin of the Seismological Society of America|date=June 1985|volume=75|issue=3|pages=865–880|url=http://bssa.geoscienceworld.org/content/75/3/865.abstract}}&lt;/ref&gt; used the brachistotrone model of John Bernoulli for travel time of a [[Ray (geometry)|ray]] along a path. In Cooke and Schneider (1983),&lt;ref name="Cook1983"/&gt; the model is a synthetic trace generation algorithm expressed as in Eqn. 3, where R(t) is generated in the Z-domain by recursive formula. In whatever form the forward model appears, it is important that it not only predicts the collected field data, but also models how the data is generated. Thus, the forward model by Cooke and Schneider (1983)&lt;ref name="Cook1983"/&gt; can only be used to invert CMP data since the model invariably assumes no spreading loss by mimicking the response of a laterally [[homogeneous]] earth to a plane-wave source

&lt;ol start="1"&gt;
&lt;li&gt;&lt;math&gt;t=\sum_{i=1}^n\frac{\big[(x_i-x_{i-1})^2+(y_i-y_{i-1})^2+(z_i-z_{i-1})^2\big]^{\frac{1}{2}}}{v_i}&lt;/math&gt; &lt;/li&gt;

where t is ray travel time, x, y, z are depth coordinates and vi is the constant velocity between interfaces i − 1 and i.

&lt;li&gt;&lt;math&gt;\left[\frac{1}{K(\vec{r})}\frac{\partial^2}{\partial t^2}-\nabla\cdot\big(\frac{1}{\rho(\vec{r}\big)}\nabla)\right] U(\vec{r},t)= s(\vec{r},t)
&lt;/math&gt;&lt;/li&gt;

where &lt;math&gt;K(\vec{r})&lt;/math&gt; represent bulk modulus, &lt;math&gt;\rho(\vec{r})&lt;/math&gt; density, &lt;math&gt;s(\vec{r},t)&lt;/math&gt; the source of acoustic waves, and &lt;math&gt;U(\vec{r},t)&lt;/math&gt;  the pressure variation.

&lt;li&gt;&lt;math&gt;s(t)=w(t)*R(t)&lt;/math&gt;&lt;/li&gt;
&lt;/ol&gt;

where ''s''(''t'') = synthetic trace, ''w''(''t'') = source wavelet, and ''R''(''t'') = reflectivity function.

===Objective function===

An important numerical process in inverse modeling is to minimize the objective function, which is a function defined in terms of the difference between the collected field seismic data and the numerically computed seismic data. Classical objective functions include the sum of squared deviations between experimental and numerical data, as in the [[least squares]] methods, the sum of the [[Magnitude (mathematics)|magnitude]] of the difference between field and numerical data, or some variant of these definitions. Irrespective of the definition used, numerical solution of the inverse problem is obtained as earth model that minimize the objective function.

In addition to the objective function, other constraints like known model parameters and known layer interfaces in some regions of the earth are also incorporated in the inverse modeling procedure. These constraints, according to Francis 2006,&lt;ref name="Earthworks"/&gt; help to reduce non-uniqueness of the inversion solution by providing a priori information that is not contained in the inverted data while Cooke and Schneider (1983)&lt;ref name="Cook1983"/&gt; reports their useful in controlling noise and when working in a geophysically well-known area.

===Mathematical analysis of generalized linear inversion procedure===

The objective of mathematical analysis of inverse modeling is to cast the generalized linear inverse problem into a simple [[Matrix (mathematics)|matrix]] algebra by considering all the components described in previous sections. viz; forward model, objective function etc. Generally, the numerically generated seismic data are non-linear functions of the earth model parameters. To remove the non-linearity and create a platform for application of [[linear algebra]] concepts, the forward model is [[Linearization|linearized]] by expansion using a [[Taylor series]] as carried out below. For more details see Wiggins (1972),&lt;ref name="wiggns1972"/&gt; Cooke and Schneider (1983).&lt;ref name="Cook1983"/&gt;

Consider a set of &lt;math&gt;m&lt;/math&gt; seismic field observations &lt;math&gt;F_j&lt;/math&gt;, for &lt;math&gt;j = 1, \ldots, m&lt;/math&gt; and a set of &lt;math&gt;n&lt;/math&gt; earth model parameters &lt;math&gt;p_i&lt;/math&gt; to be inverted for, for &lt;math&gt;i=1,\ldots, n&lt;/math&gt;. The field observations can be represented in either &lt;math&gt;\vec{F}\,(\vec{p})&lt;/math&gt; or &lt;math&gt;F_j\,(p_i)&lt;/math&gt;, where &lt;math&gt;\vec{p}&lt;/math&gt; and &lt;math&gt;\vec{F}\,(\vec{p})&lt;/math&gt; are vectorial representations of model parameters and the field observations as a function of earth parameters. Similarly, for &lt;math&gt;q_i&lt;/math&gt; representing guesses of model parameters, &lt;math&gt;\vec{F}\,(\vec{q})&lt;/math&gt; is the vector of numerical computed seismic data using the forward model of Sec. 1.3. Taylor's series expansion of &lt;math&gt;\vec{F}\,(\vec{p})&lt;/math&gt; about &lt;math&gt;\vec{q}&lt;/math&gt; is given below.

&lt;ol start="4"&gt;
&lt;li&gt;&lt;math&gt;\vec{F}\,(\vec{p}) = \vec{F}\,(\vec{q})+(\vec{p}-\vec{q})\frac{\partial \vec{F}\,(\vec{q})}{\partial \vec{p}}+(\vec{p}-\vec{q})^2\frac{\partial^2 \vec{F}\,(\vec{q})}{\partial \vec{p}^2} +O(\vec{p}-\vec{q})^3&lt;/math&gt;&lt;/li&gt;

On linearization by dropping the non-linear terms (terms with (p⃗ − ⃗q) of order 2 and above), the equation becomes

&lt;li&gt;&lt;math&gt;\vec{F}\,(\vec{p}) - \vec{F}\,(\vec{q})=(\vec{p}-\vec{q})\frac{\partial \vec{F}\,(\vec{q})}{\partial \vec{p}}&lt;/math&gt;&lt;/li&gt;

Considering that &lt;math&gt;\vec{F}&lt;/math&gt; has &lt;math&gt;m&lt;/math&gt; components and &lt;math&gt;\vec{p}&lt;/math&gt; and &lt;math&gt;\vec{q}&lt;/math&gt; have &lt;math&gt;n&lt;/math&gt; components, the discrete form of Eqn. 5 results in a system of &lt;math&gt;m&lt;/math&gt; [[linear equation]]s in &lt;math&gt;n&lt;/math&gt; variables whose matrix form is shown below.

&lt;li&gt;&lt;math&gt;\Delta \vec{F} = \mathbf{A}\,\Delta\vec{p}&lt;/math&gt;&lt;/li&gt;

&lt;li&gt;&lt;math&gt;\Delta\vec{F} = \begin{bmatrix}F_1(\vec{p})-F_1(\vec{q})\\\vdots\\F_m(\vec{p})-F_m(\vec{q})\end{bmatrix}&lt;/math&gt;&lt;/li&gt;

&lt;li&gt;&lt;math&gt;\Delta\vec{p} = \vec{p}-\vec{q} = 
\begin{bmatrix}
p_1-q_1\\
\vdots	\\
p_n-q_n
\end{bmatrix}&lt;/math&gt;&lt;/li&gt;

&lt;li&gt;&lt;math&gt;\mathbf{A} = 
\begin{bmatrix}
\frac{\partial F_1(\vec{q})}{\partial p_1} &amp; \frac{\partial F_1(\vec{q})}{\partial p_2} &amp; \cdots &amp; \frac{\partial F_1(\vec{q})}{\partial p_n} \\
\frac{\partial F_2(\vec{q})}{\partial p_1} &amp; \cdots &amp;  \frac{\partial F_2(\vec{q})}{\partial p_{n-1}} &amp; \frac{\partial F_2(\vec{q})}{\partial p_n} \\
\vdots	&amp;  \frac{\partial F_j(\vec{q})}{\partial p_i} &amp; \vdots &amp; \vdots \\
\frac{\partial F_m(\vec{q})}{\partial p_1} &amp; \frac{\partial F_m(\vec{q})}{\partial p_2} &amp; \cdots &amp; \frac{\partial F_m(\vec{q})}{\partial p_n} \\
\end{bmatrix}&lt;/math&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;math&gt;\Delta \vec{F}&lt;/math&gt; is called the difference [[vector (mathematics and physics)|vector]] in Cooke and Schneider (1983).&lt;ref name="Cook1983"/&gt; It has a size of &lt;math&gt;m\times 1&lt;/math&gt; and its components are the difference between the observed trace and the numerically computed seismic data. &lt;math&gt;\Delta\vec{p}&lt;/math&gt; is the corrector vector of size &lt;math&gt;n\times 1&lt;/math&gt;, while &lt;math&gt;\mathbf{A}&lt;/math&gt; is called the sensitivity matrix. It has a size of &lt;math&gt;m\times n&lt;/math&gt; and its comments are such that each column is the [[partial derivative]] of a component of the forward function with respect to one of the unknown earth model parameters. Similarly, each row is the partial derivative of a component of the numerically computed seismic trace with respect to all unknown model parameters.

===Solution algorithm===

&lt;math&gt;\vec{F}\,(\vec{q})&lt;/math&gt; is computed from the forward model, while &lt;math&gt;\vec{F}\,(\vec{p})&lt;/math&gt; is the experimental data. Thus,&lt;math&gt;\Delta \vec{F}&lt;/math&gt; is a known quality. On the other hand, &lt;math&gt;\Delta\vec{p}&lt;/math&gt; is unknown and is obtained by solution of Eqn. 10. This equation is theoretically solvable only when &lt;math&gt;\mathbf{A}&lt;/math&gt; is invertible, that is, if it is a square matrix so that the number of observations &lt;math&gt;m&lt;/math&gt; is equal to the number &lt;math&gt;n&lt;/math&gt; of unknown earth parameters. If this is the case, the unknown corrector vector &lt;math&gt;\Delta\vec{p}&lt;/math&gt;, is solved for as shown below, using any of the classical direct or iterative solvers for solution of a set of linear equations.

&lt;ol start="10"&gt;
&lt;li&gt;&lt;math&gt;\Delta \vec{p} = \mathbf{A}^{-1}\,\Delta\vec{F}&lt;/math&gt;&lt;/li&gt;
&lt;/ol&gt;

In most [[seismic inversion]] applications, there are more observations than the number of earth parameters to be inverted for, i.e. &lt;math&gt;m&gt;n&lt;/math&gt;, leading to a system of equations that is mathematically over-determined. As a result, Eqn. 10 is not theoretically solvable and an exact solution is not obtainable.&lt;ref name="kan"/&gt; An estimate of the corrector vector is obtained using the least squares procedure to find the corrector vector &lt;math&gt;\Delta \vec{p}&lt;/math&gt; that minimizes &lt;math&gt;\vec{e}\,^T \vec{e}&lt;/math&gt;, which is the sum of the squares of the error, &lt;math&gt;\vec{e}&lt;/math&gt;.&lt;ref name="kan"/&gt;

The error&lt;math&gt;\vec{e}&lt;/math&gt; is given by

 &lt;ol start="11"&gt;
&lt;li&gt;&lt;math&gt;\vec{e} = \Delta\vec{F}-\mathbf{A}\,\Delta \vec{p}&lt;/math&gt;&lt;/li&gt;
&lt;/ol&gt;

In the least squares procedure, the corrector vector that minimizes &lt;math&gt;\vec{e}\,^T\vec{e}&lt;/math&gt; is obtained as below.

&lt;ol start="12"&gt;
&lt;li&gt;&lt;math&gt;\begin{align}
\mathbf{A}\,\Delta \vec{p} &amp;=\Delta\vec{F}\\
\mathbf{A}^T\mathbf{A}\,\Delta \vec{p}  &amp;= \mathbf{A}^T\Delta\vec{F}
\end{align}&lt;/math&gt;&lt;/li&gt;
&lt;/ol&gt;

Thus,

&lt;ol start="13"&gt;
&lt;li&gt;&lt;math&gt;
\Delta \vec{p}  = (\mathbf{A}^T\mathbf{A})^{-1}\,\mathbf{A}^T\Delta\vec{F}&lt;/math&gt;&lt;/li&gt;
&lt;/ol&gt;

From the above discussions, the objective function is defined as either the &lt;math&gt;L_1&lt;/math&gt; or &lt;math&gt;L_2&lt;/math&gt; norm of &lt;math&gt;\Delta \vec{p}&lt;/math&gt; given by
&lt;math&gt;\sum_{j=0}^n|\Delta p_j|&lt;/math&gt; or &lt;math&gt;\sum_{j=0}^n|\Delta p_j|^2&lt;/math&gt; or of &lt;math&gt;\Delta \vec{F}&lt;/math&gt; given by &lt;math&gt;\sum_{i=0}^m|\Delta F_i|&lt;/math&gt; or &lt;math&gt;\sum_{i=0}^m|\Delta F_i|^2&lt;/math&gt;.

The generalized procedure for inverting any experimental seismic data for &lt;math&gt;m = n&lt;/math&gt; or &lt;math&gt;m &gt;n&lt;/math&gt;, using the mathematical theory for inverse modeling, as described above, is shown in Fig. 1 and described as follows.

An initial guess of the model impedance is provided to initiate the inversion process. The forward model uses this initial guess to compute a synthetic seismic data which is subtracted from the observed seismic data to calculate the difference vector.

# An initial guess of the model impedance &lt;math&gt;\vec{q}&lt;/math&gt; is provided to initiate the inversion process.
# A synthetic seismic data &lt;math&gt;\vec{F}(\vec{q})&lt;/math&gt; is computed by the forward model, using the model impedance above.
# The difference vector &lt;math&gt;\vec{F}(\vec{p})-\vec{F}(\vec{q})&lt;/math&gt; is computed as the difference between experimental and synthetic seismic data.
# The sensitivity matrix &lt;math&gt;\mathbf{A}&lt;/math&gt; is computed at this value of the impedance profile.
# Using &lt;math&gt;\mathbf{A}&lt;/math&gt; and the difference vector from 3 above, the corrector vector &lt;math&gt;\Delta \vec{p}&lt;/math&gt; is calculated. A new impedance profile is obtained as &lt;ol start="14"&gt;&lt;li&gt;&lt;math&gt;\vec{p}=\vec{q}+\Delta \vec{p}&lt;/math&gt;&lt;/li&gt;&lt;/ol&gt;
# The &lt;math&gt;L_1&lt;/math&gt; or &lt;math&gt;L_2&lt;/math&gt; norm of the computed corrector vector is compared with a provided tolerance value. If the computed norm is less than the tolerance, the numerical procedure is concluded and the inverted impedance profile for the earth region is given by &lt;math&gt;\vec{p}&lt;/math&gt; from Eqn. 14. On the other hand, if the norm is greater than the tolerance, iterations through steps 2-6 are repeated but with an updated impedance profile as computed from Eqn. 14. Fig. 2&lt;ref&gt;{{cite journal|last=Cooke|first=D|author2=Cant J. |title=Model-based seismic inversion: Comparing deterministic and probabilistic approaches|journal=CSEG Recorder|date=April 2010}}&lt;/ref&gt; shows a typical example of impedance profile updating during successive iteration process. According to Cooke and Schneider (1983),&lt;ref name="Cook1983"/&gt; use of the corrected guess from Eqn. 14 as the new initial guess during iteration reduces the error.

==Parameterization of the earth model space==

Irrespective of the variable to be inverted for, the earth’s impedance is a continuous function of depth (or time in seismic data) and for numerical linear inversion technique to be applicable for this continuous physical model, the continuous properties have to be discretized and/or sampled at discrete intervals along the depth of the earth model. Thus, the total depth over which model properties are to be determined is a necessary starting point for the discretization. Commonly, as shown in Fig. 3, this properties are sampled at close discrete intervals over this depth to ensure high resolution of impedance variation along the earth’s depth. The impedance values inverted from the [[algorithm]] represents the average value in the discrete interval.

Considering that inverse modeling problem is only theoretically solvable when the number of discrete intervals for sampling the properties is equal to the number of observation in the trace to be inverted, a high-resolution sampling will lead to a large matrix which will be very expensive to invert. Furthermore, the matrix may be singular for dependent equations, the inversion can be unstable in the presence of noise and the system may be under-constrained if parameters other than the primary variables inverted for, are desired. In relation to parameters desired, other than impedance, Cooke and Schneider (1983)&lt;ref name="Cook1983"/&gt; gives them to include source wavelet and scale factor.

Finally, by treating constraints as known impedance values in some layers or discrete intervals, the number of unknown impedance values to be solved for are reduced, leading to greater accuracy in the results of the inversion algorithm.

[[File:Amplitude Log.jpg|thumb|upright=1.5|Figure 8:Amplitude Log]]

[[File:Impedance Logs Inverted From Amplitude.jpg|thumb|upright=1.5|Figure 9a:Impedance Logs Inverted From Amplitude]]

[[File:Impedance Well Log.jpg|thumb|upright=1.5|Figure 9b: Impedance Well Log]]

==Inversion examples==

===Temperature inversion from Marescot (2010)&lt;ref name="m"/&gt;===

We start with an example to invert for earth parameter values from temperature depth distribution in a given earth region. Although this example does not directly relate to [[seismic inversion]] since no traveling acoustic waves are involved, it nonetheless introduces practical application of the inversion technique in a manner easy to comprehend, before moving on to seismic applications. In this example, the temperature of the earth is measured at discrete locations in a well bore by placing temperature sensors in the target depths. By assuming a forward model of linear distribution of temperature with depth, two parameters are inverted for from the temperature depth measurements.

The forward model is given by
&lt;ol start="15"&gt;&lt;li&gt;
&lt;math&gt;\vec{F}(\vec{q}) = \vec{T} = a+bz&lt;/math&gt;&lt;/li&gt;&lt;/ol&gt;

where &lt;math&gt;\vec{q} = [a,b]&lt;/math&gt;. Thus, the dimension of &lt;math&gt;\vec{q}&lt;/math&gt; is 2 i.e. the number of parameters inverted for is 2.

The objective of this inversion algorithm is to find &lt;math&gt;\vec{p}&lt;/math&gt;, which is the value of &lt;math&gt;[a,b]&lt;/math&gt; that minimizes the difference between the observed temperature distribution and those obtained using the forward model of Eqn. 15. Considering the dimension of the forward model or the number of temperature observations to be &lt;math&gt;n&lt;/math&gt;, the components of the forward model is written as

&lt;ol start="16"&gt;&lt;li&gt;&lt;math&gt;\begin{align}
T_1&amp;=a+bz_1		\\
T_2&amp;=a+bz_2	\\
\vdots	\\
T_{n-1}&amp;=a+bz_{n-1}		\\
T_n&amp;=a+bz_n	\\
\end{align}&lt;/math&gt;&lt;/li&gt;

so that &lt;math&gt;\vec{F}(\vec{q}) = T&lt;/math&gt;

&lt;li&gt;&lt;math&gt;\mathbf{A} = 
\begin{bmatrix}
1 &amp; z_1  \\
1 &amp; z_2  \\
\vdots &amp; \vdots	\\
1 &amp; z_{n-1}  \\
1 &amp; z_n  \\
\end{bmatrix}&lt;/math&gt;&lt;/li&gt;&lt;/ol&gt;

We present results from Marescot (2010)&lt;ref name="m"&gt;{{cite web|last=Marescot|first=Laurent|title=Introduction to Inversion in Geophysics|url=http://www.tomoquest.com|accessdate=3 May 2013}}&lt;/ref&gt; for the case of &lt;math&gt;n = 2&lt;/math&gt; for which the observed temperature values at depths were &lt;math&gt;T_1 = 19 ^{\circ}C&lt;/math&gt; at &lt;math&gt;z=2m&lt;/math&gt; and &lt;math&gt;T_2=22^{\circ}C&lt;/math&gt; at &lt;math&gt;z=8m&lt;/math&gt;. These experimental data were inverted to obtain earth parameter values of &lt;math&gt;a = 0.5&lt;/math&gt; and &lt;math&gt;b=18^{\circ}C&lt;/math&gt;. For a more general case with large number of temperature observations, Fig. 4 shows the final linear forward model obtained from using the inverted values of &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt;. The figure shows a good match between experimental and numerical data.

===Wave travel time inversion from Marescot (2010)&lt;ref name="m"/&gt;===

This examples inverts for earth layer [[velocity]] from recorded seismic wave travel times. Fig. 5 shows the initial velocity guesses and the travel times recorded from the field, while Fig. 6a shows the inverted [[heterogeneous]] velocity model, which is the solution of the inversion algorithm obtained after 30 [[iteration]]s. As seen in Fig. 6b, there is good comparison between the final travel times obtained from the forward model using the inverted velocity and the field record travel times. Using these solutions, the ray path was reconstructed and is shown to be highly tortuous through the earth model as shown in Fig. 7.

===Seismic trace inversion from Cooke and Schneider (1983)===

This example, taken from Cooke and Schneider (1983),&lt;ref name="Cook1983"/&gt; shows inversion of a CMP seismic trace for earth model impedance (product of density and velocity) profile. The seismic trace inverted is shown in Fig. 8 while Fig. 9a shows the inverted impedance profile with the input initial impedance used for the inversion algorithm. Also recorded alongside the seismic trace is an impedance log of the earth region as shown in Fig. 9b. The figures show good comparison between the recorded impedance log and the numerical inverted impedance from the seismic trace.

==References==
{{Reflist}}

==Further reading==

*Backus, G. 1970. “Inference from inadequate and inaccurate data.” Proceedings of the National Academy of Sciences of the United States of America 65, no. 1.
*Backus, G., and F. Gilbert. 1968. “The Resolving Power of Gross Earth Data.” Geophysical Journal of the Royal Astronomical Society 16 (2): 169–205.
*Backus, G. E., and J. F. Gilbert. 1967. “Numerical applications of a formalism for geophysical inverse problems.” Geophysical Journal of the Royal Astronomical Society. 13 (1-3): 247.
*Bamberger, A., G. Chavent, C. Hemon, and P. Lailly. 1982. “Inversion of normal incidence seisomograms.” Geophysics 47 (5): 757–770.
*Clayton, R. W., and R. H. Stolt. 1981. “A Born-WKBJ inversion method for acoustic reflection data.” Geophysics 46 (11): 1559–1567.
*Franklin, J. N. 1970. “Well-posed stochastic extensions of ill-posed linear problems.” Journal of Mathematical Analysis and Applications 31 (3): 682.
*Parker, R. L. 1977. “Understanding inverse theory.” Annual Review of Earth and planetary sciences 5:35–64.
*Rawlinson, N. 2000. “Inversion of Seismic Dat for Layered Crustal Structure.” Ph.D. diss., Monash University.
*Wang, B., and L. W. braile. 1996. “Simultaneous inversion of reflection and refraction seis- mic data and application to field data from the northern Rio Grande rift.” Geophysical Journal International 125 (2): 443–458.
*Weglein, A. B., H. Y. Zhang, A. C. Ramirez, F. Liu, and J. E. M. Lira. 2009. “Clarifying the underlying and fundamental meaning of the approximate linear inversion of seismic dat.” Geophysics 74 (6): 6WCD1–WCD13.

[[Category:Mathematical modeling]]
[[Category:Geological techniques]]
[[Category:Seismology measurement]]</text>
      <sha1>7lz7il0loxpdtnl058zhzv59uzqpydt</sha1>
    </revision>
  </page>
  <page>
    <title>List of things named after Friedrich Bessel</title>
    <ns>0</ns>
    <id>49655161</id>
    <revision>
      <id>778093702</id>
      <parentid>710299981</parentid>
      <timestamp>2017-05-01T02:27:44Z</timestamp>
      <contributor>
        <username>Ema--or</username>
        <id>16005968</id>
      </contributor>
      <comment>/* Mathematics */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="832">This is a (partial) list of things named for [[Friedrich Wilhelm Bessel]], a 19th-century German scholar who worked in astronomy, geodesy and mathematical sciences:
==Astronomy, geodesy, astronomical bodies==

* [[1552 Bessel]]
* Bessel's star; see [[61 Cygni]]
* [[Bessel (crater)]]
* [[Bessel ellipsoid]]
*[[Besselian elements]]
*[[Besselian epoch]]

==Mathematics==
* [[Bessel's correction]]
* [[Bessel's inequality]]
* [[Bessel potential]]
* [[Bessel process]]

===Bessel and related functions===
* [[Bessel beam]]
* [[Bessel filter]]
* [[Bessel function]]
* [[Bessel polynomial]]
*Bessel series, see [[Fourier–Bessel series]]
* [[Bessel window]]
*[[Bessel–Clifford function]]
*[[Fourier–Bessel series]]

[[Category:Lists of things named after mathematicians|Bessel]]
[[Category:Lists of things named after astronomers|B]]</text>
      <sha1>87rw1l9q3qqdoutyo227qxulr7apq4a</sha1>
    </revision>
  </page>
  <page>
    <title>Literal movement grammar</title>
    <ns>0</ns>
    <id>26256312</id>
    <revision>
      <id>863550965</id>
      <parentid>475445803</parentid>
      <timestamp>2018-10-11T14:02:21Z</timestamp>
      <contributor>
        <username>Dewritech</username>
        <id>11498870</id>
      </contributor>
      <minor/>
      <comment>/* top */clean up, added [[CAT:UL|underlinked]] tag</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3785">{{Underlinked|date=October 2018}}

Literal movement grammars (LMGs) are a grammar formalism introduced by Groenink in 1995&lt;ref name="groenink1995"&gt;Groenink, Annius V. 1995. Literal Movement Grammars. In ''Proceedings of the 7th EACL Conference''.&lt;/ref&gt; intended to characterize certain extraposition phenomena of natural language such as topicalization and cross-serial dependencies. LMGs extend the class of [[Context free grammar|CFGs]] by adding introducing pattern-matched function-like rewrite semantics, as well as the operations of variable binding and slash deletion.

==Description==

The basic rewrite operation of an LMG is very similar to that of a CFG, with the addition of "arguments" to the non-terminal symbols. Where a context-free rewrite rule obeys the general schema &lt;math&gt;S \to \alpha&lt;/math&gt; for some non-terminal &lt;math&gt;S&lt;/math&gt; and some string of terminals and/or non-terminals &lt;math&gt;\alpha&lt;/math&gt;, an LMG rewrite rule obeys the general schema &lt;math&gt;X(x_1, ..., x_n) \to \alpha&lt;/math&gt;, where X is a non-terminal with arity n (called a predicate in LMG terminology), and &lt;math&gt;\alpha&lt;/math&gt; is a string of "items", as defined below. The arguments &lt;math&gt;x_i&lt;/math&gt; are strings of terminal symbols and/or variable symbols defining an argument pattern. In the case where an argument pattern has multiple adjacent variable symbols, the argument pattern will match any and all partitions of the actual value that unify. Thus, if the predicate is &lt;math&gt;f(xy)&lt;/math&gt; and the actual pattern is &lt;math&gt;f(ab)&lt;/math&gt;, there are three valid matches: &lt;math&gt;x = \epsilon,\ y = ab;\ x = a,\ y = b;\ x = ab,\ y = \epsilon&lt;/math&gt;. In this way, a single rule is actually a family of alternatives.

An "item" in a literal movement grammar is one of

* &lt;math&gt;f(x_1, \ldots, x_n)&lt;/math&gt;, a predicate of arity n,
* &lt;math&gt;x \text{:}f(x_1, \ldots, x_n)&lt;/math&gt;, a variable binding x to the string produced by &lt;math&gt;f(x_1, ..., x_n)&lt;/math&gt;, or
* &lt;math&gt;f(x_1, \ldots, x_n)/\alpha&lt;/math&gt;, a slash deletion of &lt;math&gt;f(x_1, ..., x_n)&lt;/math&gt; by the string of terminals and/or variables &lt;math&gt;\alpha&lt;/math&gt;.

In a rule like &lt;math&gt;f(x_1, ..., x_m) \to \alpha\ y \text{:} g(z_1, ... z_n)\ \beta&lt;/math&gt;, the variable y is bound to whatever terminal string the g predicate produces, and in &lt;math&gt;\alpha&lt;/math&gt; and &lt;math&gt;\beta&lt;/math&gt;, all occurrences of y are replaced by that string, and &lt;math&gt;\alpha&lt;/math&gt; and &lt;math&gt;\beta&lt;/math&gt; are produced as if terminal string had always been there.

An item &lt;math&gt;x/y&lt;/math&gt;, where x is something that produces a terminal string (either a terminal string itself or some predicate), and y is a string of terminals and/or variables, is rewritten as the empty string (&lt;math&gt;\epsilon&lt;/math&gt;) if and only if &lt;math&gt;g(y_1, ..., y_n) = z&lt;/math&gt;, and otherwise cannot be rewritten at all.

==Example==

LMGs can characterize the non-CF language &lt;math&gt;\{ a^n b^n c^n : n \geq 1 \}&lt;/math&gt; as follows:

:&lt;math&gt;S() \to x\text{:}A()\ B(x)&lt;/math&gt;
:&lt;math&gt;A() \to a\ A()&lt;/math&gt;
:&lt;math&gt;A() \to \epsilon&lt;/math&gt;
:&lt;math&gt;B(xy) \to a/x\ b\ B(y) c&lt;/math&gt;
:&lt;math&gt;B(\epsilon) \to \epsilon&lt;/math&gt;

The derivation for ''aabbcc'', using parentheses also for grouping, is therefore

&lt;math&gt;S() \to x\text{:}A()\ B(x) \to x\text{:}(a\ A())\ B(x) \to x\text{:}(aa\ A())\ B(x) \to x\text{:}aa\ B(x) \to aa\ B(aa)&lt;/math&gt;
: &lt;math&gt;\to aa\ a/a\ b\ B(a)\ c \to aab\ B(a)\ c \to aab\ a/a\ b\ B()\ cc \to aabb\ B()\ cc\ \to aabbcc&lt;/math&gt;

==Computational Power==

Languages generated by LMGs contain the context-free languages as a proper subset, as every CFG is an LMG where all predicates have arity 0 and no production rule contains variable bindings or slash deletions.

==References==

&lt;references/&gt;

{{Formal languages and grammars}}

[[Category:Formal languages]]
[[Category:Grammar frameworks]]</text>
      <sha1>2bf6yrj5i151z0tvgewo4wn3i5zg140</sha1>
    </revision>
  </page>
  <page>
    <title>Logico-linguistic modeling</title>
    <ns>0</ns>
    <id>30109665</id>
    <revision>
      <id>825542036</id>
      <parentid>787180628</parentid>
      <timestamp>2018-02-14T00:18:05Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <comment>adding links to references using [[Google Scholar]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11657">'''Logico-linguistic modeling''' is a method for building knowledge-based systems with a learning capability using [[conceptual model]]s from [[soft systems methodology]], modal predicate logic, and the [[Prolog]] artificial intelligence language.

== Overview==
Logico-linguistic modeling is a six-stage method developed primarily for building [[knowledge-based systems]] (KBS), but it also has application in manual decision support systems and information source analysis. Logico-linguistic models have a superficial similarity to [[John F. Sowa]]'s [[conceptual graphs]];&lt;ref&gt;Sowa, John F. (1984), ''Conceptual Structures:  Information Processing in Mind and Machine'', Addison-Wesley, Reading, MA, USA.&lt;/ref&gt; both use bubble style diagrams, both are concerned with concepts, both can be expressed in logic and both can be used in artificial intelligence. However, logico-linguistic models are very different in both logical form and in their method of construction.
 
Logico-linguistic modeling was developed in order to solve theoretical problems found in the soft systems method for information system design. The main thrust of the research into has been to show how [[soft systems methodology]] (SSM), a method of systems analysis, can be extended into artificial intelligence.

== Background ==

SSM employs three modeling devices i.e. rich pictures, root definitions, and conceptual models of human activity systems. The root definitions and conceptual models are built by stakeholders themselves in an iterative debate organized by a facilitator. The strengths of this method lie, firstly, in its flexibility, the fact that it can address any problem situation, and, secondly, in the fact that the solution belongs to the people in the organization and is not imposed by an outside analyst.&lt;ref name = "source"&gt;Gregory, Frank Hutson and Lau, Sui Pong (1999) [http://logicalgregory.jimdo.com/publications/logical-ssm-for-isa/ Logical Soft Systems Modelling for Information Source Analysis - The Case of Hong Kong Telecom], Journal of the Operational Research Society, vol. 50 (2).&lt;/ref&gt;

Information [[requirements analysis]] (IRA) took the basic SSM method a stage further and showed how the conceptual models could be developed into a detailed information system design.&lt;ref name="Wilson"&gt;Wilson, Brian ''Systems: Concepts, Methodologies and Applications'', John Wiley &amp; Sons Ltd. 1984, 1990. {{ISBN|0-471-92716-3}}&lt;/ref&gt; IRA calls for the addition of two modeling devices: "Information Categories", which show the required information inputs and outputs from the activities identified in an expanded conceptual model; and the "Maltese Cross", a matrix which shows the inputs and outputs from the information categories and shows where new information processing procedures are required. A completed Maltese Cross is sufficient for the detailed design of a transaction processing system.

The initial impetus to the development of logico-linguistic modeling was a concern with the theoretical problem of how an information system can have a connection to the physical world.&lt;ref&gt;Gregory, Frank Hutson (1995) [[s:Mapping Information Systems onto the Real World|Mapping Information Systems onto the Real World]]. Working Paper Series No. WP95/01. Dept. of Information Systems, City University of Hong Kong.&lt;/ref&gt; This is a problem in both IRA and more established methods (such as [[SSADM]]) because none base their information system design on models of the physical world. IRA designs are based on a notional conceptual model and SSADM is based on models of the movement of documents.

The solution to these problems provided a formula that was not limited to the design of transaction processing systems but could be used for the design of KBS with learning capability.&lt;ref name="know"&gt;Gregory, Frank Hutson (1993) SSM for Knowledge Elicitation &amp; Representation, Warwick Business School Research Paper No. 98. Later published in the ''Journal of the Operational Research Society'' (1995) 46, 562-578.&lt;/ref&gt;

== The six stages of logico-linguistic modeling==
[[File:Fig 1. SSM model abstracted from Wilson.jpg|thumb|Fig 1. SSM Conceptual Model]]
The logico-linguistic modeling method comprises six stages.&lt;ref name="know"/&gt;

=== 1. Systems analysis ===

In the first stage logico-linguistic modeling uses SSM for [[systems analysis]]. This stage seeks to structure the problem in the client organization by identifying stakeholders, modelling organizational objectives and discussing possible solutions. At this stage it not assumed that a KBS will be a solution and logico-linguistic modeling often produces solutions that do not require a computerized KBS.

[[Expert systems]] tend to capture the expertise, of individuals in different organizations, on the same topic. By contrast a KBS, produced by logico-linguistic modeling, seeks to capture the expertise of individuals in the same organization on different topics. The emphasis is on the [[elicitation technique|elicitation]] of organizational or group knowledge rather than individual experts. In logico-linguistic modeling the stakeholders become the experts.

The end point of this stage is an SSM style conceptual models such as figure 1.

=== 2. Language creation ===
[[File:Fig 2. Logico-linguistic Model.jpeg|thumb|Fig 2. Logico-linguistic Model]]

According to the theory behind logico-linguistic modeling the SSM conceptual model building process is a Wittgensteinian [[language-game]] in which the stakeholders build a language to describe the problem situation.&lt;ref&gt;Gregory, Frank Hutson (1992) [[s:SSM to Information Systems: A Wittengsteinian Approach|SSM to Information Systems: A Wittengsteinian Approach. Warwick Business School Research Paper No. 65.]] With revisions and additions this paper was published in Journal of Information Systems (1993) 3, pp.&amp;nbsp;149–168.&lt;/ref&gt; The logico-linguistic model expresses this language as a set of definitions, see figure 2.

=== 3. Knowledge elicitation===
After the model of the language has been built putative knowledge about the real world can be added by the stakeholders. Traditional SSM conceptual models contain only one logical connective (a necessary condition). In order to represent causal sequences, "[[sufficient condition]]s" and "[[necessary and sufficient condition]]s" are also required.&lt;ref name="cause2"&gt;Gregory, Frank Hutson (1992) [[s:Cause, Effect, Efficiency &amp; Soft Systems Models|Cause, Effect, Efficiency &amp; Soft Systems Models. Warwick Business School Research Paper No. 42]]. Later published in Journal of the Operational Research Society (1993) 44 (4), pp 149-168&lt;/ref&gt; In logico-linguistic modeling this deficiency is remedied by two addition types of connective. The outcome of stage three is an empirical model, see figure 3.

=== 4. Knowledge representation ===
[[File:Fig 3. Empirical Model.jpeg|thumb|Fig 3. Empirical Model]]

Modal predicate logic (a combination of [[modal logic]] and [[predicate logic]]) is used as the formal method of knowledge representation. The connectives from the language model are logically true (indicated by the "''L''" modal operator) and connective added at the knowledge elicitation stage are possibility true (indicated by the "''M''" modal operator). Before proceeding to stage 5, the models are expressed in logical formulae.

=== 5. Computer code ===

Formulae in predicate logic translate easily into the [[Prolog]] artificial intelligence language. The modality is expressed by two different types of Prolog rules. Rules taken from the language creation stage of  model building process are treated as incorrigible. While rules from the knowledge elicitation stage are marked as hypothetical rules. The system is not confined to decision support but has a built in learning capability.

=== 6. Verification ===

A knowledge based system built using this method verifies itself. [[Verification and Validation (software)|Verification]] takes place when the KBS is used by the clients. It is an ongoing process that continues throughout the life of the system. If the stakeholder beliefs about the real world are mistaken this will be brought out by the addition of Prolog facts that conflict with  the hypothetical rules. It operates in accordance to the classic principle of [[falsifiability]] found in the philosophy of science&lt;ref&gt;Gregory, Frank Hutson (1996) "The need for "Scientific" Information Systems" Proceedings of the Americas Conference on Information Systems, Aug 1996, Association for Information Systems, 1996. pp. 534-536.&lt;/ref&gt;

== Applications ==

=== Knowledge-based computer systems ===
Logico-linguistic modeling has been used to produce fully operational computerized knowledge based systems, such as one for the management of diabetes patients in a hospital out-patients department.&lt;ref&gt;Choi, Mei Yee Sarah (1997) Logico-linguistic Modelling for building a Diabetes Mellitus Patient Management Knowledge Based System. M.A. Dissertation, Department of Information Systems, City University of Hong Kong.&lt;/ref&gt;

=== Manual decision support ===
In other projects the need to move into Prolog was considered unnecessary because the printed logico-linguistic models provided an easy to use guide to decision making. For example, a system for mortgage loan approval&lt;ref&gt;Lee, Kam Shing Clive (1997) The Development of a Knowledge Based System on Mortgage Loan Approval. M.A. Dissertation, Department of Information Systems, City University of Hong Kong.&lt;/ref&gt;

=== Information source analysis ===
In some cases a KBS could not be built because the organization did not have all the knowledge needed to support all their activities. In these cases logico-linguistic modeling showed shortcomings in the supply of information and where more was needed. For example, a planning department in a telecoms company&lt;ref name =  "source"/&gt;

== Criticism ==
While logico-linguistic modeling overcomes the problems found in SSM's transition from conceptual model to computer code, it does so at the expense of increased stakeholder constructed model complexity. The benefits of this complexity are questionable&lt;ref&gt;Klein, J. H. (1994) [http://www.tandfonline.com/doi/abs/10.1057/jors.1994.137 Cognitive processes and operational research: a human information processing perspective]. Journal of the Operational Research Society. Vol. 45, No. 8.&lt;/ref&gt;
and this modeling method may be much harder to use than other methods.&lt;ref&gt;Klein, J. H. (1995) Over-simplistic cognitive science: A response.  Journal of the Operational Research Society. Vol. 46, No. 4. pp. 275-6.&lt;/ref&gt;

This contention has been exemplified by subsequent research. An attempt by researchers to model buying decisions across twelve companies using logico-linguistic modeling required simplification of the models and removal of the modal elements.&lt;ref&gt;Nakswasdi, Suravut (2004) [http://arrow.unisa.edu.au:8080/vital/access/manager/Repository/unisa:44235 Logical Soft Systems for Modeling Industrial Machinery Buying Decisions in Thailand]. Doctor of Business Administration thesis, University of South Australia.&lt;/ref&gt;

== See also ==
* [[Argument map]]
* [[Cognitive map]]
* [[Concept map]]
* [[Fuzzy cognitive map]]
* [[Knowledge representation and reasoning]]
* [[Semantic network]]

== References ==
{{Reflist}}

== Further reading ==
{{commons category}}
* Gregory, Frank Hutson  (1993) "[http://wrap.warwick.ac.uk/2888/ A logical analysis of soft systems modelling: implications for information system design and knowledge based system design]''. PhD thesis, University of Warwick.

[[Category:Knowledge representation]]
[[Category:Systems analysis]]
[[Category:Modal logic]]</text>
      <sha1>jw1of15fsdhfgybqe6np87pa2jkvzdv</sha1>
    </revision>
  </page>
  <page>
    <title>Machine learning</title>
    <ns>0</ns>
    <id>233488</id>
    <revision>
      <id>871390169</id>
      <parentid>871388411</parentid>
      <timestamp>2018-11-30T18:54:08Z</timestamp>
      <contributor>
        <ip>67.180.134.203</ip>
      </contributor>
      <comment>/* Proprietary software with free and open-source editions */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="72462">{{for|the journal|Machine Learning (journal)}}
{{redirect|Statistical learning|statistical learning in linguistics|statistical learning in language acquisition}}
{{Machine learning bar}}

'''Machine learning''' (ML) is the study of [[algorithm]]s and [[mathematical model]]s that [[computer systems]] use to progressively improve their performance on a specific task. Machine learning algorithms build a mathematical model of sample data, known as "[[training data]]", in order to make predictions or decisions without being explicitly programmed to perform the task.{{refn|The definition "without being explicitly programmed" is often attributed to [[Arthur Samuel]], who coined the term "machine learning" in 1959, but the phrase is not found verbatim in this publication, and may be a [[paraphrase]] that appeared later. Confer "Paraphrasing Arthur Samuel (1959), the question is: How can computers learn to solve problems without being explicitly programmed?" in {{Cite conference|url=https://link.springer.com/chapter/10.1007/978-94-009-0279-4_9|title=Automated Design of Both the Topology and Sizing of Analog Electrical Circuits Using Genetic Programming|conference=Artificial Intelligence in Design '96|last=Koza|first=John R.|last2=Bennett|first2=Forrest H.|last3=Andre|first3=David|last4=Keane|first4=Martin A.|date=1996|publisher=Springer, Dordrecht|pages=151–170|language=en|doi=10.1007/978-94-009-0279-4_9}}}}&lt;ref name="bishop2006" /&gt;{{rp|2}} Machine learning algorithms are used in the applications of [[email filtering]], detection of network intruders, and [[computer vision]], where it is infeasible to develop an algorithm of specific instructions for performing the task. Machine learning is closely related to [[computational statistics]], which focuses on making predictions using computers. The study of [[mathematical optimization]] delivers methods, theory and application domains to the field of machine learning. [[Data mining]] is a field of study within machine learning, and focuses on [[exploratory data analysis]] through [[unsupervised learning]].{{refn|Machine learning and pattern recognition "can be viewed as two facets of the same field."&lt;ref name="bishop2006" /&gt;{{rp|vii}}}}&lt;ref&gt;{{cite journal |last=Friedman |first=Jerome H. |authorlink = Jerome H. Friedman|title=Data Mining and Statistics: What's the connection? |journal=Computing Science and Statistics |volume=29 |issue=1 |year=1998 |pages=3–9}}&lt;/ref&gt; In its application across business problems, machine learning is also referred to as [[predictive analytics]].

== Overview ==
The name ''machine learning'' was coined in 1959 by [[Arthur Samuel]].&lt;ref name="Samuel"&gt;{{Cite journal|last=Samuel|first=Arthur|date=1959|title=Some Studies in Machine Learning Using the Game of Checkers|journal=IBM Journal of Research and Development|volume=3|issue=3|pages=210–229|doi=10.1147/rd.33.0210|via=|citeseerx=10.1.1.368.2254}}&lt;/ref&gt; [[Tom M. Mitchell]] provided a widely quoted, more formal definition of the algorithms studied in the machine learning field: "A computer program is said to learn from experience ''E'' with respect to some class of tasks ''T'' and performance measure ''P'' if its performance at tasks in ''T'', as measured by ''P'', improves with experience ''E''."&lt;ref name="Mitchell-1997"&gt;{{cite book
|author=Mitchell, T. 
|title=Machine Learning
|publisher=McGraw Hill
|isbn= 978-0-07-042807-2
|pages=2
|year=1997}}&lt;/ref&gt; This definition of the tasks in which machine learning is concerned offers a fundamentally [[operational definition]] rather than defining the field in cognitive terms. This follows [[Alan Turing]]'s proposal in his paper "[[Computing Machinery and Intelligence]]", in which the question "Can machines think?" is replaced with the question "Can machines do what we (as thinking entities) can do?".&lt;ref&gt;{{Citation |chapterurl=http://eprints.ecs.soton.ac.uk/12954/ |first=Stevan |last=Harnad |authorlink=Stevan Harnad |year=2008 |chapter=The Annotation Game: On Turing (1950) on Computing, Machinery, and Intelligence |editor1-last=Epstein |editor1-first=Robert |editor2-last=Peters |editor2-first=Grace |title=The Turing Test Sourcebook: Philosophical and Methodological Issues in the Quest for the Thinking Computer |location= |publisher=Kluwer |isbn= }}&lt;/ref&gt; In Turing's proposal the various characteristics that could be possessed by a ''thinking machine'' and the various implications in constructing one are exposed.

=== Machine learning tasks ===
{{Anchor|Algorithm types}}

[[File:Svm max sep hyperplane with margin.png|thumb|A [[support vector machine]] is a supervised learning model that divides the data into regions separated by a [[linear classifier|linear boundary]]. Here, the linear boundary divides the black circles from the white.]]
Machine learning tasks are classified into several broad categories. In [[supervised learning]], the algorithm builds a mathematical model of a set of data that contains both the inputs and the desired outputs. For example, if the task were determining whether an image contained a certain object, the [[training data]] for a supervised learning algorithm would include images with and without that object (the input), and each image would have a label (the output) designating whether it contained the object. In special cases, the input may be only partially available, or restricted to special feedback.{{what|date=November 2018}} [[Semi-supervised learning]] algorithms develop mathematical models from incomplete training data, where a portion of the sample inputs are missing the desired output.

[[Statistical classification|Classification]] algorithms and [[regression analysis|regression]] algorithms are types of supervised learning. Classification algorithms are used when the outputs are restricted to a [[discrete number|limited set]] of values. For a classification algorithm that filters emails, the input would be an incoming email, and the output would be the name of the folder in which to file the email. For an algorithm that identifies spam emails, the output would be the prediction of either "spam" or "not spam", represented by the [[Boolean data type|Boolean]] values one and zero. [[Regression analysis|Regression]] algorithms are named for their continuous outputs, meaning they may have any value within a range. Examples of a continuous value are the temperature, length, or price of an object.

In [[unsupervised learning]], the algorithm builds a mathematical model of a set of data which contains only inputs and no desired outputs. Unsupervised learning algorithms are used to find structure in the data, like grouping or [[cluster analysis|clustering]] of data points. Unsupervised learning can discover patterns in the data, and can group the inputs into categories, as in [[feature learning]]. [[Dimensionality reduction]] is the process of reducing the number of "features", or inputs, in a set of data. 

[[Active learning (machine learning)|Active learning]] algorithms access the desired outputs (training labels) for a limited set of inputs based on a budget, and optimize the choice of inputs for which it will acquire training labels. When used interactively, these can be presented to a human user for labeling. [[Reinforcement learning]] algorithms are given feedback in the form of positive or negative reinforcement in a dynamic environment, and are used in [[autonomous vehicle]]s or in learning to play a game against a human opponent.&lt;ref name="bishop2006" /&gt;{{rp|3}} Other specialized algorithms in machine learning include [[topic modeling]], where the computer program is given a set of [[natural language]] documents and finds other documents that cover similar topics. Machine learning algorithms can be used to find the unobservable [[probability density function]] in [[density estimation]] problems. [[Meta learning (computer science)|Meta learning]] algorithms learn their own [[inductive bias]] based on previous experience. In [[developmental robotics]], [[robot learning]] algorithms generate their own sequences of learning experiences, also known as a curriculum, to cumulatively acquire new skills through self-guided exploration and social interaction with humans. These robots use guidance mechanisms such as active learning, maturation, motor synergies, and imitation.{{what|date=November 2018}}

== History and relationships to other fields ==
{{see also|Timeline of machine learning}}
[[Arthur Samuel]], an American pioneer in the field of [[computer gaming]] and [[artificial intelligence]], coined the term "Machine Learning" in 1959 while at [[IBM]]&lt;ref&gt;R. Kohavi and F. Provost, "Glossary of terms," Machine Learning, vol. 30, no. 2–3, pp. 271–274, 1998.&lt;/ref&gt;. 
As a scientific endeavour, machine learning grew out of the quest for artificial intelligence. Already in the early days of AI as an academic discipline, some researchers were interested in having machines learn from data. They attempted to approach the problem with various symbolic methods, as well as what were then termed "[[neural network]]s"; these were mostly [[perceptron]]s and [[ADALINE|other models]] that were later found to be reinventions of the [[generalized linear model]]s of statistics.&lt;ref&gt;{{cite paper |last1=Sarle|first1=Warren|title=Neural Networks and statistical models|journal=|citeseerx=10.1.1.27.699}}&lt;/ref&gt; [[Probability theory|Probabilistic]] reasoning was also employed, especially in automated medical diagnosis.&lt;ref name="aima"&gt;{{cite AIMA|edition=2}}&lt;/ref&gt;{{rp|488}}

However, an increasing emphasis on the [[GOFAI|logical, knowledge-based approach]] caused a rift between AI and machine learning. Probabilistic systems were plagued by theoretical and practical problems of data acquisition and representation.&lt;ref name="aima" /&gt;{{rp|488}} By 1980, [[expert system]]s had come to dominate AI, and statistics was out of favor.&lt;ref name="changing"&gt;{{Cite journal | last1 = Langley | first1 = Pat| title = The changing science of machine learning | doi = 10.1007/s10994-011-5242-y | journal = [[Machine Learning (journal)|Machine Learning]]| volume = 82 | issue = 3 | pages = 275–279 | year = 2011 | pmid =  | pmc = }}&lt;/ref&gt; Work on symbolic/knowledge-based learning did continue within AI, leading to [[inductive logic programming]], but the more statistical line of research was now outside the field of AI proper, in [[pattern recognition]] and [[information retrieval]].&lt;ref name="aima" /&gt;{{rp|708–710; 755}} Neural networks research had been abandoned by AI and [[computer science]] around the same time. This line, too, was continued outside the AI/CS field, as "[[connectionism]]", by researchers from other disciplines including [[John Hopfield|Hopfield]], [[David Rumelhart|Rumelhart]] and [[Geoff Hinton|Hinton]]. Their main success came in the mid-1980s with the reinvention of [[backpropagation]].&lt;ref name="aima" /&gt;{{rp|25}}

Machine learning, reorganized as a separate field, started to flourish in the 1990s. The field changed its goal from achieving artificial intelligence to tackling solvable problems of a practical nature. It shifted focus away from the symbolic approaches it had inherited from AI, and toward methods and models borrowed from statistics and [[probability theory]].&lt;ref name="changing" /&gt; It also benefited from the increasing availability of digitized information, and the ability to distribute it via the [[Internet]].

=== Relation to data mining ===

Machine learning and [[data mining]] often employ the same methods and overlap significantly, but while machine learning focuses on prediction, based on ''known'' properties learned from the training data, [[data mining]] focuses on the [[discovery (observation)|discovery]] of (previously) ''unknown'' properties in the data (this is the analysis step of [[knowledge discovery]] in databases). Data mining uses many machine learning methods, but with different goals; on the other hand, machine learning also employs data mining methods as "unsupervised learning" or as a preprocessing step to improve learner accuracy. Much of the confusion between these two research communities (which do often have separate conferences and separate journals, [[ECML PKDD]] being a major exception) comes from the basic assumptions they work with: in machine learning, performance is usually evaluated with respect to the ability to ''reproduce known'' knowledge, while in knowledge discovery and data mining (KDD) the key task is the discovery of previously ''unknown'' knowledge. Evaluated with respect to known knowledge, an uninformed (unsupervised) method will easily be outperformed by other supervised methods, while in a typical KDD task, supervised methods cannot be used due to the unavailability of training data.

=== Relation to optimization ===

Machine learning also has intimate ties to [[Mathematical optimization|optimization]]: many learning problems are formulated as minimization of some [[loss function]] on a training set of examples. Loss functions express the discrepancy between the predictions of the model being trained and the actual problem instances (for example, in classification, one wants to assign a label to instances, and models are trained to correctly predict the pre-assigned labels of a set of examples). The difference between the two fields arises from the goal of generalization: while optimization algorithms can minimize the loss on a training set, machine learning is concerned with minimizing the loss on unseen samples.&lt;ref&gt;{{cite encyclopedia |last1=Le Roux |first1=Nicolas |first2=Yoshua |last2=Bengio |first3=Andrew |last3=Fitzgibbon |title=Improving First and Second-Order Methods by Modeling Uncertainty |encyclopedia=Optimization for Machine Learning |year=2012 |page=404 |editor1-last=Sra |editor1-first=Suvrit |editor2-first=Sebastian |editor2-last=Nowozin |editor3-first=Stephen J. |editor3-last=Wright |publisher=MIT Press}}&lt;/ref&gt;

=== Relation to statistics ===
Machine learning and [[statistics]] are closely related fields. According to [[Michael I. Jordan]], the ideas of machine learning, from methodological principles to theoretical tools, have had a long pre-history in statistics.&lt;ref name="mi jordan ama"&gt;{{cite web|url=https://www.reddit.com/r/MachineLearning/comments/2fxi6v/ama_michael_i_jordan/ckelmtt?context=3 |title=statistics and machine learning|publisher=reddit|date=2014-09-10|accessdate=2014-10-01|language=|author = [[Michael I. Jordan]]}}&lt;/ref&gt; He also suggested the term [[data science]] as a placeholder to call the overall field.&lt;ref name="mi jordan ama" /&gt;

[[Leo Breiman]] distinguished two statistical modelling paradigms: data model and algorithmic model,&lt;ref&gt;{{cite web|url=http://projecteuclid.org/download/pdf_1/euclid.ss/1009213726|title=Breiman: Statistical Modeling: The Two Cultures (with comments and a rejoinder by the author)|author=Cornell University Library|publisher=|accessdate=8 August 2015}}&lt;/ref&gt; wherein "algorithmic model" means more or less the machine learning algorithms like [[Random forest]].

Some statisticians have adopted methods from machine learning, leading to a combined field that they call ''statistical learning''.&lt;ref name="islr"&gt;{{cite book |author1=Gareth James |author2=Daniela Witten |author3=Trevor Hastie |author4=Robert Tibshirani |title=An Introduction to Statistical Learning |publisher=Springer |year=2013 |url=http://www-bcf.usc.edu/~gareth/ISL/ |page=vii}}&lt;/ref&gt;

== {{anchor|Generalization}} Theory ==
{{Main article|Computational learning theory}}
A core objective of a learner is to generalize from its experience.&lt;ref name="bishop2006"&gt;{{citation|first= C. M. |last= Bishop |authorlink=Christopher M. Bishop |year=2006 |title=Pattern Recognition and Machine Learning |publisher=Springer |isbn=978-0-387-31073-2}}&lt;/ref&gt;&lt;ref&gt;{{Cite Mehryar Afshin Ameet 2012}}&lt;/ref&gt; Generalization in this context is the ability of a learning machine to perform accurately on new, unseen examples/tasks after having experienced a learning data set. The training examples come from some generally unknown probability distribution (considered representative of the space of occurrences) and the learner has to build a general model about this space that enables it to produce sufficiently accurate predictions in new cases.

The computational analysis of machine learning algorithms and their performance is a branch of [[theoretical computer science]] known as [[computational learning theory]]. Because training sets are finite and the future is uncertain, learning theory usually does not yield guarantees of the performance of algorithms. Instead, probabilistic bounds on the performance are quite common. The [[bias–variance decomposition]] is one way to quantify generalization [[Errors and residuals|error]].

For the best performance in the context of generalization, the complexity of the hypothesis should match the complexity of the function underlying the data. If the hypothesis is less complex than the function, then the model has underfit the data. If the complexity of the model is increased in response, then the training error decreases. But if the hypothesis is too complex, then the model is subject to [[overfitting]] and generalization will be poorer.&lt;ref name="alpaydin"&gt;{{Cite book|author=Alpaydin, Ethem|title=Introduction to Machine Learning|url=https://mitpress.mit.edu/books/introduction-machine-learning |year=2010 |publisher=The MIT Press |place=London|isbn= 978-0-262-01243-0 |access-date=4 February 2017 }}&lt;/ref&gt;

In addition to performance bounds, computational learning theorists study the time complexity and feasibility of learning. In computational learning theory, a computation is considered feasible if it can be done in [[Time complexity#Polynomial time|polynomial time]]. There are two kinds of [[time complexity]] results. Positive results show that a certain class of functions can be learned in polynomial time. Negative results show that certain classes cannot be learned in polynomial time.

== Approaches ==

=== Types of learning algorithms ===

The types of machine learning algorithms differ in their approach, the type of data they input and output, and the type of task or problem that they are intended to solve.

==== Supervised and semi-supervised learning ====
{{Main article|Supervised learning}}
Supervised learning algorithms build a mathematical model of a set of data that contains both the inputs and the desired outputs.&lt;ref&gt;{{cite book |last1=Russell |first1=Stuart J. |last2=Norvig |first2=Peter |title=[[Artificial Intelligence: A Modern Approach]] |date=2010 |publisher=Prentice Hall |isbn=9780136042594 |edition=Third}}&lt;/ref&gt; The data is known as [[training data]], and consists of a set of training examples. Each training example has one or more inputs and a desired output, also known as a supervisory signal. In the case of semi-supervised learning algorithms, some of the training examples are missing the desired output. In the mathematical model, each training example is represented by an [[array data structure|array]] or vector, and the training data by a [[Matrix (mathematics)|matrix]]. Through the process of [[iteration]], supervised learning algorithms develop and optimize a [[Function (mathematics)|function]] that can be used to predict the output associated with new inputs.&lt;ref&gt;{{cite book |last1=Mohri |first1=Mehryar |last2=Rostamizadeh |first2=Afshin |last3=Talwalkar |first3=Ameet |title=Foundations of Machine Learning |date=2012 |publisher=The MIT Press |isbn=9780262018258}}&lt;/ref&gt; An optimal function will allow the algorithm to correctly determine the output for inputs that were not a part of the training data. An algorithm that improves the accuracy of its outputs or predictions over time is said to have learned to perform that task.&lt;ref name="Mitchell-1997" /&gt;

Supervised learning algorithms include [[Statistical classification|classification]] and [[Regression analysis|regression]].&lt;ref&gt;{{cite book|last=Alpaydin|first=Ethem|title=Introduction to Machine Learning|date=2010|publisher=MIT Press|isbn=978-0-262-01243-0|page=9|url=https://books.google.com/books?id=7f5bBAAAQBAJ&amp;printsec=frontcover#v=onepage&amp;q=classification&amp;f=false}}&lt;/ref&gt; Classification algorithms are used when the outputs are restricted to a limited set of values, and regression algorithms are used when the outputs may have any numerical value within a range. [[Similarity learning]] is an area of supervised machine learning closely related to regression and classification, but the goal is to learn from examples using a similarity function that measures how similar or related two objects are. It has applications in [[ranking]], [[recommendation systems]], visual identity tracking, face verification, and speaker verification.

==== Unsupervised learning ====
{{Main article|Unsupervised learning}}{{See also|Cluster analysis}}
Unsupervised learning algorithms take a set of data that contains only inputs, and find structure in the data, like grouping or clustering of data points. The algorithms therefore learn from test data that has not been labeled, classified or categorized. Instead of responding to feedback, unsupervised learning algorithms identify commonalities in the data and react based on the presence or absence of such commonalities in each new piece of data. A central application of unsupervised learning is in the field of [[density estimation]] in [[statistics]],&lt;ref name="JordanBishop2004"&gt;{{cite book |first1=Michael I. |last1=Jordan |first2=Christopher M. |last2=Bishop |chapter=Neural Networks |editor=Allen B. Tucker |title=Computer Science Handbook, Second Edition (Section VII: Intelligent Systems) |location=Boca Raton, Florida |publisher=Chapman &amp; Hall/CRC Press LLC |year=2004 |ISBN=1-58488-360-X }}&lt;/ref&gt; though unsupervised learning encompasses other domains involving summarizing and explaining data features.

Cluster analysis is the assignment of a set of observations into subsets (called ''clusters'') so that observations within the same cluster are similar according to one or more predesignated criteria, while observations drawn from different clusters are dissimilar. Different clustering techniques make different assumptions on the structure of the data, often defined by some ''similarity metric'' and evaluated, for example, by ''internal compactness'', or the similarity between members of the same cluster, and ''separation'', the difference between clusters. Other methods are based on ''estimated density'' and ''graph connectivity''.

==== Reinforcement learning ====
{{Main article|Reinforcement learning}}
Reinforcement learning is an area of machine learning concerned with how [[software agent]]s ought to take [[Action selection|actions]] in an environment so as to maximize some notion of cumulative reward. Due to its generality, the field is studied in many other disciplines, such as [[game theory]], [[control theory]], [[operations research]], [[information theory]], [[simulation-based optimization]], [[multi-agent system]]s, [[swarm intelligence]], [[statistics]] and [[genetic algorithm]]s.&lt;ref name="Bert12"&gt;Dimitri P. Bertsekas. "Dynamic Programming and Optimal Control: Approximate Dynamic Programming, Vol.II", Athena Scientific, 2012,[http://www.athenasc.com/dpbook.html]&lt;/ref&gt;&lt;ref name="BertTsi96"&gt;Dimitri P. Bertsekas and John N. Tsitsiklis. "Neuro-Dynamic Programming", Athena Scientific, 1996,[http://www.athenasc.com/ndpbook.html]&lt;/ref&gt; In machine learning, the environment is typically represented as a [[Markov Decision Process]] (MDP). Many reinforcement learning algorithms use [[dynamic programming]] techniques.&lt;ref name=Bert12/&gt;&lt;ref name=BertTsi96/&gt;&lt;ref&gt;{{Cite journal|title=Reinforcement learning and markov decision processes|url=https://link.springer.com/chapter/10.1007/978-3-642-27645-3_1|author1=van Otterlo, M.|author2=Wiering, M.|journal=Reinforcement Learning |pages=3–42 |year=2012 |publisher=Springer Berlin Heidelberg}}&lt;/ref&gt; Reinforcement learning algorithms do not assume knowledge of an exact mathematical model of the MDP, and are used when exact models are infeasible.&lt;ref name=Bert12/&gt;&lt;ref name=BertTsi96/&gt; Reinforcement learning algorithms are used in autonomous vehicles or in learning to play a game against a human opponent.

=== Processes and techniques ===

Various processes, techniques and methods can be applied to one or more types of machine learning algorithms to enhance their performance.

==== Feature learning ====
{{Main article|Feature learning}}

Several learning algorithms aim at discovering better representations of the inputs provided during training.&lt;ref name="pami"&gt;{{cite journal |author1=Y. Bengio |author2=A. Courville |author3=P. Vincent |title=Representation Learning: A Review and New Perspectives |journal=IEEE Trans. PAMI, special issue Learning Deep Architectures |year=2013|doi=10.1109/tpami.2013.50 |volume=35 |pages=1798–1828|arxiv=1206.5538 }}&lt;/ref&gt; Classic examples include [[principal components analysis]] and cluster analysis. Feature learning algorithms, also called representation learning algorithms, often attempt to preserve the information in their input but also transform it in a way that makes it useful, often as a pre-processing step before performing classification or predictions. This technique allows reconstruction of the inputs coming from the unknown data-generating distribution, while not being necessarily faithful to configurations that are implausible under that distribution. This replaces manual [[feature engineering]], and allows a machine to both learn the features and use them to perform a specific task.

Feature learning can be either supervised or unsupervised. In supervised feature learning, features are learned using labeled input data. Examples include [[artificial neural network]]s, [[multilayer perceptron]]s, and supervised [[dictionary learning]]. In unsupervised feature learning, features are learned with unlabeled input data.  Examples include dictionary learning, [[independent component analysis]], [[autoencoder]]s, [[matrix decomposition|matrix factorization]]&lt;ref&gt;{{cite conference |author1=Nathan Srebro |author2=Jason D. M. Rennie |author3=Tommi S. Jaakkola |title=Maximum-Margin Matrix Factorization |conference=[[Conference on Neural Information Processing Systems|NIPS]] |year=2004}}&lt;/ref&gt; and various forms of [[Cluster analysis|clustering]].&lt;ref name="coates2011"&gt;{{cite conference
|last1 = Coates |first1 = Adam
|last2 = Lee |first2 = Honglak
|last3 = Ng |first3 = Andrew Y.
|title = An analysis of single-layer networks in unsupervised feature learning
|conference = Int'l Conf. on AI and Statistics (AISTATS)
|year = 2011
|url = http://machinelearning.wustl.edu/mlpapers/paper_files/AISTATS2011_CoatesNL11.pdf
}}&lt;/ref&gt;&lt;ref&gt;{{cite conference |last1 = Csurka |first1 = Gabriella|last2 = Dance |first2 = Christopher C.|last3 = Fan |first3 = Lixin|last4 = Willamowski |first4 = Jutta|last5 = Bray |first5 = Cédric|title = Visual categorization with bags of keypoints|conference = ECCV Workshop on Statistical Learning in Computer Vision|year = 2004|url = http://www.cs.cmu.edu/~efros/courses/LBMV07/Papers/csurka-eccv-04.pdf}}&lt;/ref&gt;&lt;ref name="jurafsky"&gt;{{cite book |title=Speech and Language Processing |author1=Daniel Jurafsky |author2=James H. Martin |publisher=Pearson Education International |year=2009 |pages=145–146}}&lt;/ref&gt;

[[Manifold learning]] algorithms attempt to do so under the constraint that the learned representation is low-dimensional. [[Sparse coding]] algorithms attempt to do so under the constraint that the learned representation is sparse, meaning that the mathematical model has many zeros. [[Multilinear subspace learning]] algorithms aim to learn low-dimensional representations directly from [[tensor]] representations for multidimensional data, without reshaping them into higher-dimensional vectors.&lt;ref&gt;{{cite journal |first1=Haiping |last1=Lu |first2=K.N. |last2=Plataniotis |first3=A.N. |last3=Venetsanopoulos |url=http://www.dsp.utoronto.ca/~haiping/Publication/SurveyMSL_PR2011.pdf |title=A Survey of Multilinear Subspace Learning for Tensor Data |journal=Pattern Recognition |volume=44 |number=7 |pages=1540–1551 |year=2011 |doi=10.1016/j.patcog.2011.01.004}}&lt;/ref&gt; [[Deep learning]] algorithms discover multiple levels of representation, or a hierarchy of features, with higher-level, more abstract features defined in terms of (or generating) lower-level features. It has been argued that an intelligent machine is one that learns a representation that disentangles the underlying factors of variation that explain the observed data.&lt;ref&gt;{{cite book | title = Learning Deep Architectures for AI | author = [[Yoshua Bengio]] | publisher = Now Publishers Inc. | year = 2009 | isbn = 978-1-60198-294-0 | pages = 1–3 | url = https://books.google.com/books?id=cq5ewg7FniMC&amp;pg=PA3}}&lt;/ref&gt;

Feature learning is motivated by the fact that machine learning tasks such as classification often require input that is mathematically and computationally convenient to process. However, real-world data such as images, video, and sensory data has not yielded to attempts to algorithmically define specific features. An alternative is to discover such features or representations through examination, without relying on explicit algorithms.

==== Sparse dictionary learning ====
{{Main article|Sparse dictionary learning}}
Sparse dictionary learning is a feature learning method where a training example is represented as a linear combination of [[basis function]]s, and is assumed to be a [[sparse matrix]]. The method is [[strongly NP-hard]] and difficult to solve approximately.&lt;ref&gt;{{cite journal |first=A. M. |last=Tillmann |title=On the Computational Intractability of Exact and Approximate Dictionary Learning |journal=IEEE Signal Processing Letters |volume=22 |issue=1 |year=2015 |pages=45–49 |doi=10.1109/LSP.2014.2345761|bibcode=2015ISPL...22...45T |arxiv=1405.6664 }}&lt;/ref&gt; A popular [[heuristic]] method for sparse dictionary learning is the [[K-SVD]] algorithm. Sparse dictionary learning has been applied in several contexts. In classification, the problem is to determine to which classes a previously unseen training example belongs. For a dictionary where each class has already been built, a new training example is associated with the class that is best sparsely represented by the corresponding dictionary. Sparse dictionary learning has also been applied in [[image de-noising]]. The key idea is that a clean image patch can be sparsely represented by an image dictionary, but the noise cannot.&lt;ref&gt;Aharon, M, M Elad, and A Bruckstein. 2006. "K-SVD: An Algorithm for Designing Overcomplete Dictionaries for Sparse Representation." Signal Processing, IEEE Transactions on 54 (11): 4311–4322&lt;/ref&gt;

==== Anomaly detection ====
{{Main article|Anomaly detection}}
In [[data mining]], anomaly detection, also known as outlier detection, is the identification of rare items, events or observations which raise suspicions by differing significantly from the majority of the data.&lt;ref name=":0"&gt;{{Citation|last=Zimek|first=Arthur|title=Outlier Detection|date=2017|url=http://link.springer.com/10.1007/978-1-4899-7993-3_80719-1|work=Encyclopedia of Database Systems|pages=1–5|publisher=Springer New York|language=en|doi=10.1007/978-1-4899-7993-3_80719-1|isbn=9781489979933|access-date=2018-08-16|last2=Schubert|first2=Erich}}&lt;/ref&gt; Typically, the anomalous items represent an issue such as [[bank fraud]], a structural defect, medical problems or errors in a text. Anomalies are referred to as [[outlier]]s, novelties, noise, deviations and exceptions.&lt;ref&gt;{{cite journal | last1 = Hodge | first1 = V. J. | last2 = Austin | first2 = J. | doi = 10.1007/s10462-004-4304-y | title = A Survey of Outlier Detection Methodologies | journal = Artificial Intelligence Review| volume = 22 | issue = 2 | pages = 85–126 | year = 2004 | url = http://eprints.whiterose.ac.uk/767/1/hodgevj4.pdf| pmid =  | pmc = | citeseerx = 10.1.1.318.4023 }}&lt;/ref&gt;

In particular, in the context of abuse and network intrusion detection, the interesting objects are often not rare objects, but unexpected bursts in activity. This pattern does not adhere to the common statistical definition of an outlier as a rare object, and many outlier detection methods (in particular, unsupervised algorithms) will fail on such data, unless it has been aggregated appropriately. Instead, a cluster analysis algorithm may be able to detect the micro-clusters formed by these patterns.&lt;ref&gt;{{cite journal| first=Paul | last=Dokas | first2=Levent |last2=Ertoz |first3=Vipin |last3=Kumar |first4=Aleksandar |last4=Lazarevic |first5=Jaideep |last5=Srivastava |first6=Pang-Ning |last6=Tan | title=Data mining for network intrusion detection | year=2002 | journal=Proceedings NSF Workshop on Next Generation Data Mining | url=http://www.csee.umbc.edu/~kolari1/Mining/ngdm/dokas.pdf}}&lt;/ref&gt;

Three broad categories of anomaly detection techniques exist.&lt;ref name="ChandolaSurvey"&gt;{{cite journal |last1=Chandola |first1=V. |last2=Banerjee |first2=A. |last3=Kumar |first3=V. |year=2009 |title=Anomaly detection: A survey|journal=[[ACM Computing Surveys]]|volume=41|issue=3|pages=1–58|doi=10.1145/1541880.1541882}}&lt;/ref&gt; Unsupervised anomaly detection techniques detect anomalies in an unlabeled test data set under the assumption that the majority of the instances in the data set are normal, by looking for instances that seem to fit least to the remainder of the data set. Supervised anomaly detection techniques require a data set that has been labeled as "normal" and "abnormal" and involves training a classifier (the key difference to many other statistical classification problems is the inherent unbalanced nature of outlier detection). Semi-supervised anomaly detection techniques construct a model representing normal behavior from a given normal training data set, and then test the likelihood of a test instance to be generated by the model.

==== Decision trees ====
{{Main article|Decision tree learning}}
Decision tree learning uses a [[decision tree]] as a [[Predictive modelling|predictive model]] to go from observations about an item (represented in the branches) to conclusions about the item's target value (represented in the leaves). It is one of the predictive modeling approaches used in statistics, data mining and machine learning. Tree models where the target variable can take a discrete set of values are called classification trees; in these tree structures, [[leaf node|leaves]] represent class labels and branches represent [[Logical conjunction|conjunction]]s of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically [[real numbers]]) are called regression trees. In decision analysis, a decision tree can be used to visually and explicitly represent decisions and [[decision making]]. In data mining, a decision tree describes data, but the resulting classification tree can be an input for decision making.

==== Association rules ====
{{Main article|Association rule learning}}{{See also|Inductive logic programming}}
Association rule learning is a [[rule-based machine learning]] method for discovering relationships between variables in large databases. It is intended to identify strong rules discovered in databases using some measure of "interestingness".&lt;ref name="piatetsky"&gt;Piatetsky-Shapiro, Gregory (1991), ''Discovery, analysis, and presentation of strong rules'', in Piatetsky-Shapiro, Gregory; and Frawley, William J.; eds., ''Knowledge Discovery in Databases'', AAAI/MIT Press, Cambridge, MA.&lt;/ref&gt; This rule-based approach generates new rules as it analyzes more data. The ultimate goal, assuming the set of data is large enough, is to help a machine mimic the human brain’s [[feature extraction]] and [[Abstraction|abstract association]] capabilities for data that has not been categorized.&lt;ref&gt;{{Cite web|url=https://deepai.org/machine-learning-glossary-and-terms/association-learning|title=How Does Association Learning Work?|last=|first=|date=|website=deepai.org|access-date=}}&lt;/ref&gt;

Rule-based machine learning is a general term for any machine learning method that identifies, learns, or evolves "rules" to store, manipulate or apply knowledge. The defining characteristic of a rule-based machine learning algorithm is the identification and utilization of a set of relational rules that collectively represent the knowledge captured by the system. This is in contrast to other machine learning algorithms that commonly identify a singular model that can be universally applied to any instance in order to make a prediction.&lt;ref&gt;{{Cite journal|last=Bassel|first=George W.|last2=Glaab|first2=Enrico|last3=Marquez|first3=Julietta|last4=Holdsworth|first4=Michael J.|last5=Bacardit|first5=Jaume|date=2011-09-01|title=Functional Network Construction in Arabidopsis Using Rule-Based Machine Learning on Large-Scale Data Sets|url=http://www.plantcell.org/content/23/9/3101|journal=The Plant Cell|language=en|volume=23|issue=9|pages=3101–3116|doi=10.1105/tpc.111.088153|issn=1532-298X|pmc=3203449|pmid=21896882}}&lt;/ref&gt; Rule-based machine learning approaches include [[learning classifier system]]s, association rule learning, and [[artificial immune system]]s.

Based on the concept of strong rules, [[Rakesh Agrawal (computer scientist)|Rakesh Agrawal]], [[Tomasz Imieliński]] and Arun Swami introduced association rules for discovering regularities between products in large-scale transaction data recorded by [[point-of-sale]] (POS) systems in supermarkets.&lt;ref name="mining"&gt;{{Cite book | last1 = Agrawal | first1 = R. | last2 = Imieliński | first2 = T. | last3 = Swami | first3 = A. | doi = 10.1145/170035.170072 | chapter = Mining association rules between sets of items in large databases | title = Proceedings of the 1993 ACM SIGMOD international conference on Management of data - SIGMOD '93 | pages = 207 | year = 1993 | isbn = 0897915925 | pmid =  | pmc = }}&lt;/ref&gt; For example, the rule &lt;math&gt;\{\mathrm{onions, potatoes}\} \Rightarrow \{\mathrm{burger}\}&lt;/math&gt; found in the sales data of a supermarket would indicate that if a customer buys onions and potatoes together, they are likely to also buy hamburger meat. Such information can be used as the basis for decisions about marketing activities such as promotional [[pricing]] or [[product placement]]s. In addition to [[market basket analysis]], association rules are employed today in application areas including [[Web usage mining]], [[intrusion detection]], [[continuous production]], and [[bioinformatics]]. In contrast with [[sequence mining]], association rule learning typically does not consider the order of items either within a transaction or across transactions.

Learning classifier systems (LCS) are a family of rule-based machine learning algorithms that combine a discovery component, typically a [[genetic algorithm]], with a learning component, performing either [[supervised learning]], [[reinforcement learning]], or [[unsupervised learning]]. They seek to identify a set of context-dependent rules that collectively store and apply knowledge in a [[piecewise]] manner in order to make predictions.&lt;ref&gt;{{Cite journal|last=Urbanowicz|first=Ryan J.|last2=Moore|first2=Jason H.|date=2009-09-22|title=Learning Classifier Systems: A Complete Introduction, Review, and Roadmap|url=http://www.hindawi.com/archive/2009/736398/|journal=Journal of Artificial Evolution and Applications|language=en|volume=2009|pages=1–25|doi=10.1155/2009/736398|issn=1687-6229}}&lt;/ref&gt;

Inductive logic programming (ILP) is an approach to rule-learning using [[logic programming]] as a uniform representation for input examples, background knowledge, and hypotheses. Given an encoding of the known background knowledge and a set of examples represented as a logical database of facts, an ILP system will derive a hypothesized logic program that [[Entailment|entails]] all positive and no negative examples. [[Inductive programming]] is a related field that considers any kind of programming languages for representing hypotheses (and not only logic programming), such as [[Functional programming|functional programs]].

Inductive logic programming is particularly useful in [[bioinformatics]] and [[natural language processing]]. [[Gordon Plotkin]] and [[Ehud Shapiro]] laid the initial theoretical foundation for inductive machine learning in a logical setting.&lt;ref&gt;Plotkin G.D. [https://www.era.lib.ed.ac.uk/bitstream/handle/1842/6656/Plotkin1972.pdf;sequence=1 Automatic Methods of Inductive Inference], PhD thesis, University of Edinburgh, 1970.&lt;/ref&gt;&lt;ref&gt;Shapiro, Ehud Y. [http://ftp.cs.yale.edu/publications/techreports/tr192.pdf Inductive inference of theories from facts], Research Report 192, Yale University, Department of Computer Science, 1981. Reprinted in J.-L. Lassez, G. Plotkin (Eds.), Computational Logic, The MIT Press, Cambridge, MA, 1991, pp. 199–254.&lt;/ref&gt;&lt;ref&gt;Shapiro, Ehud Y. (1983). ''Algorithmic program debugging''. Cambridge, Mass: MIT Press. {{ISBN|0-262-19218-7}}&lt;/ref&gt; Shapiro built their first implementation (Model Inference System) in 1981: a Prolog program that inductively inferred logic programs from positive and negative examples.&lt;ref&gt;Shapiro, Ehud Y. "[http://dl.acm.org/citation.cfm?id=1623364 The model inference system]." Proceedings of the 7th international joint conference on Artificial intelligence-Volume 2. Morgan Kaufmann Publishers Inc., 1981.&lt;/ref&gt; The term ''inductive'' here refers to [[Inductive reasoning|philosophical]] induction, suggesting a theory to explain observed facts, rather than [[mathematical induction|mathematical]] induction, proving a property for all members of a well-ordered set.

=== Models ===

==== Artificial neural networks ====
{{Main article|Artificial neural network}}{{See also|Deep learning}}
[[File:Colored neural network.svg|thumb|300px|An artificial neural network is an interconnected group of nodes, akin to the vast network of [[neuron]]s in a [[brain]]. Here, each circular node represents an [[artificial neuron]] and an arrow represents a connection from the output of one artificial neuron to the input of another.]]
Artificial neural networks (ANNs), or [[Connectionism|connectionist]] systems, are computing systems vaguely inspired by the [[biological neural network]]s that constitute animal [[brain]]s.&lt;ref&gt;{{Cite web|url=https://www.frontiersin.org/research-topics/4817/artificial-neural-networks-as-models-of-neural-information-processing|title=Artificial Neural Networks as Models of Neural Information Processing {{!}} Frontiers Research Topic|language=en|access-date=2018-02-20}}&lt;/ref&gt; The neural network itself is not an algorithm, but rather a framework for many different machine learning algorithms to work together and process complex data inputs.&lt;ref&gt;{{Cite web|url=https://deepai.org/machine-learning-glossary-and-terms/neural-network|title=Build with AI {{!}} DeepAI|website=DeepAI|access-date=2018-10-06}}&lt;/ref&gt; Such systems "learn" to perform tasks by considering examples, generally without being programmed with any task-specific rules.

An ANN is a model based on a collection of connected units or nodes called "[[artificial neuron]]s", which loosely model the [[neuron]]s in a biological [[brain]]. Each connection, like the [[synapse]]s in a biological [[brain]], can transmit information, a "signal", from one artificial neuron to another. An artificial neuron that receives a signal can process it and then signal additional artificial neurons connected to it. In common ANN implementations, the signal at a connection between artificial neurons is a [[real number]], and the output of each artificial neuron is computed by some non-linear function of the sum of its inputs. The connections between artificial neurons are called "edges". Artificial neurons and edges typically have a [[weight (mathematics)|weight]] that adjusts as learning proceeds. The weight increases or decreases the strength of the signal at a connection. Artificial neurons may have a threshold such that the signal is only sent if the aggregate signal crosses that threshold. Typically, artificial neurons are aggregated into layers. Different layers may perform different kinds of transformations on their inputs. Signals travel from the first layer (the input layer), to the last layer (the output layer), possibly after traversing the layers multiple times.

The original goal of the ANN approach was to solve problems in the same way that a [[human brain]] would. However, over time, attention moved to performing specific tasks, leading to deviations from [[biology]]. Artificial neural networks have been used on a variety of tasks, including [[computer vision]], [[speech recognition]], [[machine translation]], [[social network]] filtering, [[general game playing|playing board and video games]] and [[medical diagnosis]].

[[Deep learning]] consists of multiple hidden layers in an artificial neural network. This approach tries to model the way the human brain processes light and sound into vision and hearing. Some successful applications of deep learning are [[computer vision]] and [[speech recognition]].&lt;ref&gt;Honglak Lee, Roger Grosse, Rajesh Ranganath, Andrew Y. Ng. "[http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.149.802&amp;rep=rep1&amp;type=pdf Convolutional Deep Belief Networks for Scalable Unsupervised Learning of Hierarchical Representations]" Proceedings of the 26th Annual International Conference on Machine Learning, 2009.&lt;/ref&gt;

==== Support vector machines ====
{{Main article|Support vector machines}}
Support vector machines (SVMs), also known as support vector networks, are a set of related [[supervised learning]] methods used for classification and regression. Given a set of training examples, each marked as belonging to one of two categories, an SVM training algorithm builds a model that predicts whether a new example falls into one category or the other.&lt;ref name="CorinnaCortes"&gt;{{Cite journal |last1=Cortes |first1=Corinna |authorlink1=Corinna Cortes |last2=Vapnik |first2=Vladimir N. |year=1995 |title=Support-vector networks |journal=[[Machine Learning (journal)|Machine Learning]] |volume=20 |issue=3 |pages=273–297 |doi=10.1007/BF00994018 }}&lt;/ref&gt;  An SVM training algorithm is a non-[[probabilistic classification|probabilistic]], [[binary classifier|binary]], [[linear classifier]], although methods such as [[Platt scaling]] exist to use SVM in a probabilistic classification setting. In addition to performing linear classification, SVMs can efficiently perform a non-linear classification using what is called the [[kernel trick]], implicitly mapping their inputs into high-dimensional feature spaces.

==== Bayesian networks ====
{{Main article|Bayesian network}}
[[Image:SimpleBayesNetNodes.svg|thumb|right|A simple Bayesian network. Rain influences whether the sprinkler is activated, and both rain and the sprinkler influence whether the grass is wet.]]

A Bayesian network, belief network or directed acyclic graphical model is a probabilistic [[graphical model]] that represents a set of [[random variables]] and their [[conditional independence]] with a [[directed acyclic graph]] (DAG). For example, a Bayesian network could represent the probabilistic relationships between diseases and symptoms. Given symptoms, the network can be used to compute the probabilities of the presence of various diseases. Efficient algorithms exist that perform [[inference]] and learning. Bayesian networks that model sequences of variables, like [[speech recognition|speech signals]] or [[peptide sequence|protein sequences]], are called [[dynamic Bayesian network]]s. Generalizations of Bayesian networks that can represent and solve decision problems under uncertainty are called [[influence diagram]]s.

==== Genetic algorithms ====
{{Main article|Genetic algorithm}}
A genetic algorithm (GA) is a [[search algorithm]] and [[heuristic (computer science)|heuristic]] technique that mimics the process of [[natural selection]], using methods such as [[Mutation (genetic algorithm)|mutation]] and [[Crossover (genetic algorithm)|crossover]] to generate new [[Chromosome (genetic algorithm)|genotype]]s in the hope of finding good solutions to a given problem. In machine learning, genetic algorithms were used in the 1980s and 1990s.&lt;ref&gt;{{cite journal |last1=Goldberg |first1=David E. |first2=John H. |last2=Holland |title=Genetic algorithms and machine learning |journal=[[Machine Learning (journal)|Machine Learning]] |volume=3 |issue=2 |year=1988 |pages=95–99 |doi=10.1007/bf00113892}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal |title=Machine Learning, Neural and Statistical Classification |journal=Ellis Horwood Series in Artificial Intelligence |first1=D. |last1=Michie |first2=D. J. |last2=Spiegelhalter |first3=C. C. |last3=Taylor |year=1994 |bibcode=1994mlns.book.....M }}&lt;/ref&gt; Conversely, machine learning techniques have been used to improve the performance of genetic and [[evolutionary algorithm]]s.&lt;ref&gt;{{cite journal |last1=Zhang |first1=Jun |last2=Zhan |first2=Zhi-hui |last3=Lin |first3=Ying |last4=Chen |first4=Ni |last5=Gong |first5=Yue-jiao |last6=Zhong |first6=Jing-hui |last7=Chung |first7=Henry S.H. |last8=Li |first8=Yun |last9=Shi |first9=Yu-hui |title=Evolutionary Computation Meets Machine Learning: A Survey |journal=Computational Intelligence Magazine |year=2011 |volume=6 |issue=4 |pages=68–75 |url=http://ieeexplore.ieee.org/iel5/10207/6052357/06052374.pdf?arnumber=6052374 |doi=10.1109/mci.2011.942584}}&lt;/ref&gt;

== Applications ==
{{list missing criteria|date=April 2018}}
Applications for machine learning include:

{{div col}}
* [[Precision agriculture|Agriculture]]
* [[Automated theorem proving]]&lt;ref&gt;Bridge, James P., Sean B. Holden, and Lawrence C. Paulson. "[https://www.cl.cam.ac.uk/~lp15/papers/Reports/Bridge-ml.pdf Machine learning for first-order theorem proving]." Journal of automated reasoning 53.2 (2014): 141–172.&lt;/ref&gt;&lt;ref&gt;Loos, Sarah, et al. "[https://arxiv.org/pdf/1701.06972 Deep Network Guided Proof Search]." arXiv preprint arXiv:1701.06972 (2017).&lt;/ref&gt;
* [[Adaptive website]]s{{citation needed|date=August 2017}}
* [[Affective computing]]
* [[Bioinformatics]]
* [[Brain–machine interface]]s
* [[Cheminformatics]]
* Classifying [[DNA sequence]]s
* [[Computational anatomy]]
* [[Network simulation|Computer Networks]]
* [[Telecommunication]]
* [[Computer vision]], including [[object recognition]]
* Detecting [[credit-card fraud]]
* [[General game playing]]&lt;ref&gt;Finnsson, Hilmar, and Yngvi Björnsson. "[https://vvvvw.aaai.org/Papers/AAAI/2008/AAAI08-041.pdf Simulation-Based Approach to General Game Playing]." AAAI. Vol. 8. 2008.&lt;/ref&gt;
* [[Information retrieval]]
* [[Internet fraud]] detection&lt;ref name="alpaydin"/&gt;
* [[Computational linguistics]]
* [[Marketing]]
* [[Machine learning control]]
* [[Machine perception]]
* [[Automated medical diagnosis]]&lt;ref name="aima"/&gt;
* [[Computational economics]]
* [[Insurance]]
* [[Natural language processing]]
* [[Natural language understanding]]&lt;ref&gt;Sarikaya, Ruhi, Geoffrey E. Hinton, and Anoop Deoras. "[http://www.cs.utoronto.ca/~hinton/absps/ruhijournal.pdf Application of deep belief networks for natural language understanding]." IEEE/ACM Transactions on Audio, Speech and Language Processing (TASLP) 22.4 (2014): 778–784.&lt;/ref&gt;
* [[Mathematical optimization|Optimization]] and [[metaheuristic]]
* [[Online advertising]]
* [[Recommender system]]s
* [[Robot locomotion]]
* [[Search engines]]
* [[Sentiment analysis]] (or opinion mining)
* [[Sequence mining]]
* [[Software engineering]]
* [[Speech recognition|Speech]] and [[handwriting recognition]]
* [[Financial market]] analysis
* [[Structural health monitoring]]
* [[Syntactic pattern recognition]]
* [[Time series|Time series forecasting]]
* [[User behavior analytics]]
* [[Machine translation]]&lt;ref&gt;{{cite web|url=http://english.yonhapnews.co.kr/news/2017/01/10/0200000000AEN20170110009700320.html?did=2106m|title=AI-based translation to soon reach human levels: industry officials|publisher=Yonhap news agency|accessdate=4 Mar 2017}}&lt;/ref&gt;{{div col end}}

In 2006, the online movie company [[Netflix]] held the first "[[Netflix Prize]]" competition to find a program to better predict user preferences and improve the accuracy on its existing Cinematch movie recommendation algorithm by at least 10%.  A joint team made up of researchers from [[AT&amp;T Labs]]-Research in collaboration with the teams Big Chaos and Pragmatic Theory built an [[Ensemble Averaging|ensemble model]] to win the Grand Prize in 2009 for $1&amp;nbsp;million.&lt;ref&gt;[https://web.archive.org/web/20151110062742/http://www2.research.att.com/~volinsky/netflix/ "BelKor Home Page"] research.att.com&lt;/ref&gt; Shortly after the prize was awarded, Netflix realized that viewers' ratings were not the best indicators of their viewing patterns ("everything is a recommendation") and they changed their recommendation engine accordingly.&lt;ref&gt;{{cite web|url=http://techblog.netflix.com/2012/04/netflix-recommendations-beyond-5-stars.html|title=The Netflix Tech Blog: Netflix Recommendations: Beyond the 5 stars (Part 1)|publisher=|accessdate=8 August 2015}}&lt;/ref&gt; In 2010 The Wall Street Journal wrote about the firm Rebellion Research and their use of machine learning to predict the financial crisis.&lt;ref&gt;{{cite web|url=https://www.wsj.com/articles/SB10001424052748703834604575365310813948080|title=Letting the Machines Decide|author=Scott Patterson|date=13 July 2010|publisher=[[The Wall Street Journal]]|accessdate=24 June 2018}}&lt;/ref&gt; In 2012, co-founder of [[Sun Microsystems]], [[Vinod Khosla]], predicted that 80% of medical doctors jobs would be lost in the next two decades to automated machine learning medical diagnostic software.&lt;ref&gt;{{cite web|url=https://techcrunch.com/2012/01/10/doctors-or-algorithms/|author=Vonod Khosla|publisher=Tech Crunch|title=Do We Need Doctors or Algorithms?|date=January 10, 2012}}&lt;/ref&gt; In 2014, it was reported that a machine learning algorithm had been applied in the field of art history to study fine art paintings, and that it may have revealed previously unrecognized influences between artists.&lt;ref&gt;[https://medium.com/the-physics-arxiv-blog/when-a-machine-learning-algorithm-studied-fine-art-paintings-it-saw-things-art-historians-had-never-b8e4e7bf7d3e When A Machine Learning Algorithm Studied Fine Art Paintings, It Saw Things Art Historians Had Never Noticed], ''The Physics at [[ArXiv]] blog''&lt;/ref&gt;

== Limitations ==
Although machine learning has been transformative in some fields, effective machine learning is difficult because finding patterns is hard and often not enough training data are available; as a result, many machine-learning programs often fail to deliver the expected value.&lt;ref&gt;{{Cite news|url=http://web.archive.org/web/20170320225010/https://www.bloomberg.com/news/articles/2016-11-10/why-machine-learning-models-often-fail-to-learn-quicktake-q-a|title=Why Machine Learning Models Often Fail to Learn: QuickTake Q&amp;A|date=2016-11-10|work=Bloomberg.com|access-date=2017-04-10}}&lt;/ref&gt;&lt;ref&gt;{{Cite news|url=https://hbr.org/2017/04/the-first-wave-of-corporate-ai-is-doomed-to-fail|title=The First Wave of Corporate AI Is Doomed to Fail|date=2017-04-18|work=Harvard Business Review|access-date=2018-08-20}}&lt;/ref&gt;&lt;ref&gt;{{Cite news|url=https://venturebeat.com/2016/09/17/why-the-a-i-euphoria-is-doomed-to-fail/|title=Why the A.I. euphoria is doomed to fail|date=2016-09-18|work=VentureBeat|access-date=2018-08-20|language=en-US}}&lt;/ref&gt; Reasons for this are numerous: lack of (suitable) data, lack of access to the data, data bias, privacy problems, badly chosen tasks and algorithms, wrong tools and people, lack of resources, and evaluation problems.&lt;ref&gt;{{Cite web|url=https://www.kdnuggets.com/2018/07/why-machine-learning-project-fail.html|title=9 Reasons why your machine learning project will fail|website=www.kdnuggets.com|language=en-US|access-date=2018-08-20}}&lt;/ref&gt; Machine learning is also dependent on the availability of large, high-quality datasets.&lt;ref name=":1"&gt;{{Cite journal|last=Mullainathan|first=Sendhil|last2=Obermeyer|first2=Ziad|date=May 2017|title=Does Machine Learning Automate Moral Hazard and Error?|url=https://www.aeaweb.org/articles?id=10.1257/aer.p20171084|journal=American Economic Review|language=en|volume=107|issue=5|pages=476–480|doi=10.1257/aer.p20171084|issn=0002-8282|pmc=5540263|pmid=28781376|via=}}&lt;/ref&gt;

In 2018, a self-driving car from [[Uber]] failed to detect a pedestrian, who got killed in the accident.&lt;ref&gt;{{Cite news|url=https://www.economist.com/the-economist-explains/2018/05/29/why-ubers-self-driving-car-killed-a-pedestrian|title=Why Uber’s self-driving car killed a pedestrian|work=The Economist|access-date=2018-08-20|language=en}}&lt;/ref&gt; Attempts to use machine learning in healthcare with the [[Watson (computer)|IBM Watson]] system failed to deliver even after years of time and billions of investment.&lt;ref&gt;{{Cite news|url=https://www.statnews.com/2018/07/25/ibm-watson-recommended-unsafe-incorrect-treatments/|title=IBM's Watson recommended 'unsafe and incorrect' cancer treatments - STAT|date=2018-07-25|work=STAT|access-date=2018-08-21|language=en-US}}&lt;/ref&gt;&lt;ref&gt;{{Cite news|url=https://www.wsj.com/articles/ibm-bet-billions-that-watson-could-improve-cancer-treatment-it-hasnt-worked-1533961147|title=IBM Has a Watson Dilemma|last=Hernandez|first=Daniela|date=2018-08-11|work=Wall Street Journal|access-date=2018-08-21|last2=Greenwald|first2=Ted|language=en-US|issn=0099-9660}}&lt;/ref&gt; 

===Bias===
{{main|Algorithmic bias}}
Machine learning approaches in particular can suffer from different data biases. In healthcare data, measurement errors can often result in bias of machine learning applications.&lt;ref name=":1" /&gt; A machine learning system trained on your current customers only may not be able to predict the needs of new customer groups that are not represented in the training data. When trained on man-made data, machine learning is likely to pick up the same constitutional and unconscious biases already present in society.&lt;ref&gt;{{Cite journal|last=Garcia|first=Megan|date=2016|title=Racist in the Machine|url=https://read.dukeupress.edu/world-policy-journal/article/33/4/111-117/30942|journal=World Policy Journal|language=en|volume=33|issue=4|pages=111–117|doi=10.1215/07402775-3813015|issn=0740-2775}}&lt;/ref&gt; Language models learned from data have been shown to contain human-like biases.&lt;ref&gt;{{Cite journal|last=Caliskan|first=Aylin|last2=Bryson|first2=Joanna J.|last3=Narayanan|first3=Arvind|date=2017-04-14|title=Semantics derived automatically from language corpora contain human-like biases|url=http://science.sciencemag.org/content/356/6334/183|journal=Science|language=en|volume=356|issue=6334|pages=183–186|doi=10.1126/science.aal4230|issn=0036-8075|pmid=28408601|bibcode=2017Sci...356..183C|arxiv=1608.07187}}&lt;/ref&gt;&lt;ref&gt;{{Citation|last=Wang|first=Xinan|title=An algorithm for L1 nearest neighbor search via monotonic embedding|date=2016|url=http://papers.nips.cc/paper/6227-an-algorithm-for-l1-nearest-neighbor-search-via-monotonic-embedding.pdf|work=Advances in Neural Information Processing Systems 29|pages=983–991|editor-last=Lee|editor-first=D. D.|publisher=Curran Associates, Inc.|access-date=2018-08-20|last2=Dasgupta|first2=Sanjoy|editor2-last=Sugiyama|editor2-first=M.|editor3-last=Luxburg|editor3-first=U. V.|editor4-last=Guyon|editor4-first=I.}}&lt;/ref&gt; Machine learning systems used for criminal risk assessment have been found to be biased against black people.&lt;ref&gt;{{Cite web|url=https://www.propublica.org/article/machine-bias-risk-assessments-in-criminal-sentencing|title=Machine Bias|last=|first=|date=2016-05-23|website=[[ProPublica]]|language=|others=Julia Angwin, Jeff Larson, Lauren Kirchner, Surya Mattu|archive-url=|archive-date=|dead-url=|access-date=2018-08-20}}&lt;/ref&gt;&lt;ref&gt;{{Cite news|url=https://www.nytimes.com/2017/10/26/opinion/algorithm-compas-sentencing-bias.html|title=Opinion {{!}} When an Algorithm Helps Send You to Prison|last=|first=|date=|work=[[New York Times]]|access-date=2018-08-20|language=en}}&lt;/ref&gt; In 2015, Google photos would often tag black people as gorillas,&lt;ref&gt;{{Cite news|url=https://www.bbc.co.uk/news/technology-33347866|title=Google apologises for racist blunder|date=2015-07-01|work=BBC News|access-date=2018-08-20|language=en-GB}}&lt;/ref&gt; and in 2018 this still was not well resolved, but Google reportedly was still using the workaround to remove all gorilla from the training data, and thus was not able to recognize real gorillas at all.&lt;ref&gt;{{Cite news|url=https://www.theverge.com/2018/1/12/16882408/google-racist-gorillas-photo-recognition-algorithm-ai|title=Google ‘fixed’ its racist algorithm by removing gorillas from its image-labeling tech|work=The Verge|access-date=2018-08-20}}&lt;/ref&gt; Similar issues with recognizing non-white people have been found in many other systems.&lt;ref&gt;{{Cite news|url=https://www.nytimes.com/2016/06/26/opinion/sunday/artificial-intelligences-white-guy-problem.html|title=Opinion {{!}} Artificial Intelligence’s White Guy Problem|last=|first=|date=|work=[[New York Times]]|access-date=2018-08-20|language=en}}&lt;/ref&gt; In 2016, Microsoft tested a [[chatbot]] that learned from Twitter, and it quickly picked up racist and sexist language.&lt;ref&gt;{{Cite news|url=https://www.technologyreview.com/s/601111/why-microsoft-accidentally-unleashed-a-neo-nazi-sexbot/|title=Why Microsoft’s teen chatbot, Tay, said lots of awful things online|last=Metz|first=Rachel|work=MIT Technology Review|access-date=2018-08-20|language=en}}&lt;/ref&gt; Because of such challenges, the effective use of machine learning may take longer to be adopted in other domains.&lt;ref&gt;{{Cite news|url=https://www.technologyreview.com/s/603944/microsoft-ai-isnt-yet-adaptable-enough-to-help-businesses/|title=Microsoft says its racist chatbot illustrates how AI isn’t adaptable enough to help most businesses|last=Simonite|first=Tom|work=MIT Technology Review|access-date=2018-08-20|language=en}}&lt;/ref&gt;

== Model assessments ==
Classification machine learning models can be validated by accuracy estimation techniques like the [[Test set|Holdout]] method, which splits the data in a training and test set (conventionally 2/3 training set and 1/3 test set designation) and evaluates the performance of the training model on the test set. In comparison, the N-fold-[[Cross-validation (statistics)|cross-validation]] method randomly splits the data in k subsets where the k-1 instances of the data are used to train the model while the kth instance is used to test the predictive ability of the training model. In addition to the holdout and cross-validation methods, [[Bootstrapping|bootstrap]], which samples n instances with replacement from the dataset, can be used to assess model accuracy.&lt;ref&gt;{{cite journal|last1=Kohavi|first1=Ron|title=A Study of Cross-Validation and Bootstrap for Accuracy Estimation and Model Selection|journal=International Joint Conference on Artificial Intelligence|date=1995|url=http://web.cs.iastate.edu/~jtian/cs573/Papers/Kohavi-IJCAI-95.pdf}}&lt;/ref&gt;

In addition to overall accuracy, investigators frequently report [[sensitivity and specificity]] meaning True Positive Rate (TPR) and True Negative Rate (TNR) respectively. Similarly, investigators sometimes report the [[False Positive Rate]] (FPR) as well as the [[False Negative Rate]] (FNR). However, these rates are ratios that fail to reveal their numerators and denominators. The [[Total Operating Characteristic]] (TOC) is an effective method to express a model's diagnostic ability. TOC shows the numerators and denominators of the previously mentioned rates, thus TOC provides more information than the commonly used [[Receiver Operating Characteristic]] (ROC) and ROC's associated Area Under the Curve (AUC).&lt;ref&gt;{{cite journal|last1=Pontius|first1=Robert Gilmore|last2=Si|first2=Kangping|title=The total operating characteristic to measure diagnostic ability for multiple thresholds| journal=International Journal of Geographical Information Science|volume=28|issue=3|year=2014|pages=570–583|doi=10.1080/13658816.2013.862623}}&lt;/ref&gt;

== Ethics ==
Machine learning poses a host of [[Machine ethics|ethical questions]]. Systems which are trained on datasets collected with biases may exhibit these biases upon use ([[algorithmic bias]]), thus digitizing cultural prejudices.&lt;ref&gt;{{Cite web|url=http://www.nickbostrom.com/ethics/artificial-intelligence.pdf|title=The Ethics of Artificial Intelligence|last=Bostrom|first=Nick|date=2011|website=|publisher=|access-date=11 April 2016 }}&lt;/ref&gt; For example, using job hiring data from a firm with racist hiring policies may lead to a machine learning system duplicating the bias by scoring job applicants against similarity to previous successful applicants.&lt;ref name="Edionwe Outline"&gt;{{cite web|last1=Edionwe|first1=Tolulope|title=The fight against racist algorithms|url=https://theoutline.com/post/1571/the-fight-against-racist-algorithms|website=The Outline|accessdate=17 November 2017}}&lt;/ref&gt;&lt;ref name="Jeffries Outline"&gt;{{cite web|last1=Jeffries|first1=Adrianne|title=Machine learning is racist because the internet is racist|url=https://theoutline.com/post/1439/machine-learning-is-racist-because-the-internet-is-racist|website=The Outline|accessdate=17 November 2017}}&lt;/ref&gt; Responsible [[Data collection|collection of data]] and documentation of algorithmic rules used by a system thus is a critical part of machine learning.

Because language contains biases, machines trained on language ''[[Text corpus|corpora]]'' will necessarily also learn bias.&lt;ref&gt;{{cite web |url=https://freedom-to-tinker.com/2016/08/24/language-necessarily-contains-human-biases-and-so-will-machines-trained-on-language-corpora/ |title=Language necessarily contains human biases, and so will machines trained on language corpora |date=August 24, 2016 |first=Arvind |last=Narayanan |website=Freedom to Tinker}}&lt;/ref&gt;

Other forms of ethical challenges, not related to personal biases, are more seen in health care. There are concerns among health care professionals that these systems might not be designed in the public's interest, but as income generating machines. This is especially true in the United States where there is a perpetual ethical dilemma of improving health care, but also increasing profits. For example, the algorithms could be designed to provide patients with unnecessary tests or medication in which the algorithm's proprietary owners hold stakes in. There is huge potential for machine learning in health care to provide professionals a great tool to diagnose, medicate, and even plan recovery paths for patients, but this will not happen until the personal biases mentioned previously, and these "greed" biases are addressed.&lt;ref&gt;{{cite journal |last=Char |first=D. S. |last2=Shah |first2=N. H. |last3=Magnus |first3=D. |year=2018 |title=Implementing Machine Learning in Health Care—Addressing Ethical Challenges |journal=[[New England Journal of Medicine]] |volume=378 |issue=11 |pages=981–983 |doi=10.1056/nejmp1714229 |pmid=29539284 |pmc=5962261 }}&lt;/ref&gt;

== Software ==
[[Software suite]]s containing a variety of machine learning algorithms include the following :

=== Free and open-source software ===
{{Div col|colwidth=15em}}
* [[Microsoft Cognitive Toolkit|CNTK]]
* [[Deeplearning4j]]
* [[ELKI]]
* [[H2o (Analytics tool)|H2O]]
* [[Apache Mahout|Mahout]]
* [[Mallet (software project)|Mallet]] 
* [[mlpack]]
* [[MXNet]]
* [[OpenNN]]
* [[Orange (software)|Orange]]
* [[scikit-learn]]
* [[Shogun (toolbox)|Shogun]]
* [[Apache Spark#MLlib Machine Learning Library|Spark MLlib]]
* [[TensorFlow]]
* [[Torch (machine learning)|Torch]] / [[PyTorch]]
* [[Weka (machine learning)|Weka]] / [[MOA (Massive Online Analysis)|MOA]]
* [[Yooreeka]]
{{Div col end}}

=== Proprietary software with free and open-source editions ===
{{Div col|colwidth=15em}}
* [[KNIME]]
* [[RapidMiner]]
{{Div col end}}

=== Proprietary software ===
{{Div col|colwidth=15em}}
* [[Amazon Machine Learning]]
* [[Angoss]] KnowledgeSTUDIO
* [[Ayasdi]]
* [[IBM Data Science Experience]]
* [[Google APIs|Google Prediction API]]
* [[SPSS Modeler|IBM SPSS Modeler]]
* [[KXEN Inc.|KXEN Modeler]]
* [[LIONsolver]]
* [[Mathematica]]
* [[MATLAB]]
* [[Azure machine learning studio|Microsoft Azure Machine Learning]]
* [[Neural Designer]]
* [[NeuroSolutions]]
* [[Oracle Data Mining]]
* [[Oracle Cloud#Platform as a Service (PaaS)|Oracle AI Platform Cloud Service]]
* [[RCASE]]
* [[SAS (software)#Components|SAS Enterprise Miner]]
* [[SequenceL]]
* [[Splunk]]
* [[STATISTICA]] Data Miner
{{Div col end}}

== Journals ==
* ''[[Journal of Machine Learning Research]]''
* [[Machine Learning (journal)|''Machine Learning'']]
* [[Neural Computation (journal)|''Neural Computation'']]

== Conferences ==
* [[Conference on Neural Information Processing Systems]]
* [[International Conference on Machine Learning]]

== See also ==
{{Portal|Artificial intelligence|Machine learning}}
{{columns-list|
* [[Automated machine learning]]
* [[Automatic reasoning]]
* [[Big data]]
* [[Computational intelligence]]
* [[Computational neuroscience]]
* [[Ethics of artificial intelligence]]
* [[Existential risk from advanced artificial intelligence]]
* [[Explanation-based learning]]
* [[List of important publications in computer science#Machine learning|Important publications in machine learning]]
* [[Information engineering (field)|Information engineering]]
* [[List of datasets for machine learning research]]
* [[Predictive analytics]]
* [[Quantum machine learning]]
* [[Machine learning in bioinformatics|Machine-learning applications in bioinformatics]]
}}

== References ==
{{Reflist|30em}}

== Further reading ==
{{Refbegin|2}} 
*  Nils J. Nilsson, ''[http://ai.stanford.edu/people/nilsson/mlbook.html Introduction to Machine Learning]''.
* [[Trevor Hastie]], [[Robert Tibshirani]] and [[Jerome H. Friedman]] (2001). ''[https://web.stanford.edu/~hastie/ElemStatLearn/ The Elements of Statistical Learning]'', Springer. {{ISBN|0-387-95284-5}}.
* [[Pedro Domingos]] (September 2015), ''[[The Master Algorithm]]'', Basic Books, {{ISBN|978-0-465-06570-7}} 
* Ian H. Witten and Eibe Frank (2011). ''Data Mining: Practical machine learning tools and techniques'' Morgan Kaufmann, 664pp., {{ISBN|978-0-12-374856-0}}.
* Ethem Alpaydin (2004). ''Introduction to Machine Learning'', MIT Press, {{ISBN|978-0-262-01243-0}}.
* [[David J. C. MacKay]]. ''[http://www.inference.phy.cam.ac.uk/mackay/itila/book.html Information Theory, Inference, and Learning Algorithms]'' Cambridge: Cambridge University Press, 2003. {{ISBN|0-521-64298-1}}
* [[Richard O. Duda]], [[Peter E. Hart]], David G. Stork (2001) ''Pattern classification'' (2nd edition), Wiley, New York, {{ISBN|0-471-05669-3}}.
* [[Christopher Bishop]] (1995). ''Neural Networks for Pattern Recognition'', Oxford University Press. {{ISBN|0-19-853864-2}}.
* Stuart Russell &amp; Peter Norvig, (2002). ''Artificial Intelligence – A Modern Approach''. Prentice Hall, {{ISBN|0-136-04259-7}}. 
* [[Ray Solomonoff]], ''An Inductive Inference Machine'', IRE Convention Record, Section on Information Theory, Part 2, pp., 56–62, 1957.
* [[Ray Solomonoff]], ''[http://world.std.com/~rjs/indinf56.pdf An Inductive Inference Machine]'' A privately circulated report from the 1956 [[Dartmouth workshop|Dartmouth Summer Research Conference on AI]].
{{Refend}}

==External links==
{{Commonscat}}
*[https://web.archive.org/web/20171230081341/http://machinelearning.org:80/ International Machine Learning Society]
* Popular online course by [[Andrew Ng]], at [https://www.coursera.org/learn/machine-learning Coursera]. It uses [[GNU Octave]]. The course is a free version of [[Stanford University]]'s actual course taught by Ng, whose lectures are also [https://see.stanford.edu/Course/CS229 available for free].
*[https://mloss.org/ mloss] is an academic database of open-source machine learning software.
*[https://developers.google.com/machine-learning/crash-course/ Machine Learning Crash Course] by [[Google]]. This is a free course on machine learning through the use of [[TensorFlow]].
*[https://palin.co.in/courses/data-science-with-python-training-in-gurgaon/ Machine Learning with Python Course]

{{Computer science}}
[[Category:Machine learning| ]]
[[Category:Cybernetics]]
[[Category:Learning]]</text>
      <sha1>gpbeo59zm643428dflh8fvt2jol0kgs</sha1>
    </revision>
  </page>
  <page>
    <title>Mimesis (mathematics)</title>
    <ns>0</ns>
    <id>1575127</id>
    <revision>
      <id>605288484</id>
      <parentid>448800663</parentid>
      <timestamp>2014-04-22T12:31:09Z</timestamp>
      <contributor>
        <username>BD2412</username>
        <id>196446</id>
      </contributor>
      <minor/>
      <comment>minor fixes, mostly [[Wikipedia:Disambiguation pages with links|disambig links]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1526">In mathematics, '''mimesis''' is the quality of a numerical method which imitates some properties of the continuum problem. The goal of [[numerical analysis]] is to approximate the continuum, so instead of solving a [[partial differential equation]] one aims to solve a discrete version of the continuum problem. Properties of the continuum problem commonly imitated by numerical methods are [[Conservation law (physics)|conservation law]]s, [[symmetry in physics|solution symmetries]], and fundamental identities and theorems of vector and tensor calculus like the [[divergence theorem]].&lt;ref&gt;{{Citation | last1=Hyman | first1=James M. | last2=Morel | first2=Jim E. | last3=Shashkov | first3=Mikhail | last4=Steinberg | first4=Stanly L. | title=Mimetic finite difference methods for diffusion equations | doi=10.1023/A:1021282912658 | year=2002 | journal=Computational Geosciences | issn=1420-0597 | volume=6 | issue=3 | pages=333–352}}.&lt;/ref&gt;
Both [[finite difference]] or [[finite element method]] can be mimetic; it depends on the properties that the method has.

For example, a mixed finite element method applied to [[Darcy's law|Darcy flows]] strictly [[conservation of mass|conserves the mass]] of the flowing fluid.  &lt;!--there are analogies with heat conduction and elastics in mechanics, but i forget what they are offhand--&gt;

The term ''[[geometric integrator|geometric integration]]'' denotes the same philosophy.

==References==
&lt;references/&gt;

{{Mathapplied-stub}}

[[Category:Numerical differential equations]]</text>
      <sha1>38sz9vx5jal6wk1c6m4yf2sf348gsl2</sha1>
    </revision>
  </page>
  <page>
    <title>Murnaghan–Nakayama rule</title>
    <ns>0</ns>
    <id>43510390</id>
    <revision>
      <id>816514464</id>
      <parentid>809729743</parentid>
      <timestamp>2017-12-21T20:51:40Z</timestamp>
      <contributor>
        <username>Pkbwcgs</username>
        <id>29259446</id>
      </contributor>
      <minor/>
      <comment>v1.43 - [[WP:WCW]] project (Heading starts with one "=" - Heading hierarchy - Spelling and typography)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7364">In [[group theory]], a branch of mathematics, the '''Murnaghan–Nakayama rule''' is a [[combinatorics|combinatorial]] method to compute [[Representation theory of the symmetric group|irreducible character]] values of a [[symmetric group]].&lt;ref&gt;Richard Stanley, ''Enumerative Combinatorics, Vol. 2''&lt;/ref&gt;
There are several generalizations of this rule beyond the representation theory of symmetric groups, but they are not covered here.

The irreducible characters of a group are of interest to mathematicians because they concisely summarize important information about the group, such as the dimensions of the vector spaces in which the elements of the group can be represented by linear transformations that “mix” all the dimensions. For many groups, calculating irreducible character values is very difficult; the existence of simple formulas is the exception rather than the rule.

The Murnaghan–Nakayama rule is a combinatorial rule for computing symmetric group character values &amp;chi;{{su|p=&amp;lambda;|b=&amp;rho;}} using a particular kind of [[Young tableaux]].
Here λ and ρ are both [[integer partition]]s of some integer ''n'', the [[Order (group theory)|order]] of the symmetric group under consideration. The partition λ specifies the irreducible character, while the partition ρ specifies the [[conjugacy class]] on whose group elements the character is evaluated to produce the character value. The partitions are represented as [[Monotonic function|weakly decreasing]] tuples; for example, two of the partitions of 8 are (5,2,1) and (3,3,1,1).

There are two versions of the Murnaghan-Nakayama rule, one non-recursive and one recursive.

==Non-recursive version==

'''Theorem:''' 
:&lt;math&gt;\chi^{\lambda}_\rho = \sum_{T \in BST(\lambda,\rho)} (-1)^{ht(T)}&lt;/math&gt;
where the sum is taken over the set BST(λ,ρ) of all ''border-strip'' tableaux of shape λ and type ρ.
That is, each tableau ''T'' is a tableau such that
* the ''k''-th row of ''T'' has λ&lt;sub&gt;k&lt;/sub&gt; boxes
* the boxes of ''T'' are filled with integers, with the integer ''i'' appearing ρ&lt;sub&gt;i&lt;/sub&gt; times
* the integers in every row and column are [[Monotonic function|weakly increasing]]
* the set of squares filled with the integer ''i'' form a ''border strip'', that is, a connected skew-shape with no 2×2-square.

The ''height'', ''ht''(T), is the sum of the heights of the border strips in ''T''. The height of a border strip is one less than the number of 
rows it touches.

It follows from this theorem that the character values of a symmetric group are integers.

For some combinations of λ and ρ, there are no border-strip tableaux. In this case, there are no terms in the sum and therefore the character value is zero.

===Example===

Consider the calculation of one of the character values for the symmetric group of order 8, when λ is the partition (5,2,1) and ρ is the partition (3,3,1,1). The shape partition λ specifies that the tableau must have three rows, the first having 5 boxes, the second having 2 boxes, and the third having 1 box. The type partition ρ specifies that the tableau must be filled with three 1's, three 2's, one 3, and one 4. There are six such border-strip tableaux:

[[File:MurnaghanNakayama2.svg|frameless|none|600px|Example of the border-strip tableaux involved in computing a particular symmetric group character value using the non-recursive Murnaghan-Nakayama rule.]]

If we call these &lt;math&gt;T_1&lt;/math&gt;, &lt;math&gt;T_2&lt;/math&gt;, &lt;math&gt;T_3&lt;/math&gt;, &lt;math&gt;T_4&lt;/math&gt;, &lt;math&gt;T_5&lt;/math&gt;, and &lt;math&gt;T_6&lt;/math&gt;, then their heights are

&lt;math&gt;
\begin{align}
ht(T_1)=0+1+0+0=1\\
ht(T_2)=1+0+0+0=1\\
ht(T_3)=1+0+0+0=1\\
ht(T_4)=2+0+0+0=2\\
ht(T_5)=2+0+0+0=2\\
ht(T_6)=2+1+0+0=3
\end{align}
&lt;/math&gt;

and the character value is therefore

&lt;math&gt;
\chi^{(5,2,1)}_{(3,3,1,1)}=(-1)^1+(-1)^1+(-1)^1+(-1)^2+(-1)^2+(-1)^3=-1-1-1+1+1-1=-2
&lt;/math&gt;

==Recursive version==

'''Theorem:''' 
:&lt;math&gt;\chi^{\lambda}_{\rho} = \sum_{\xi \in BS(\lambda,\rho_1)} (-1)^{ht(\xi)} \chi^{\lambda\backslash\xi}_{\rho\backslash\rho_1} &lt;/math&gt;
where the sum is taken over the set BS(λ,ρ&lt;sub&gt;1&lt;/sub&gt;) of border strips within the Young diagram of shape λ that have ρ&lt;sub&gt;1&lt;/sub&gt; boxes and whose removal leaves a valid Young diagram. The notation &lt;math&gt;\lambda\backslash\xi&lt;/math&gt; represents the partition that results from removing the border strip ξ from λ. The notation &lt;math&gt;\rho\backslash\rho_1&lt;/math&gt; represents the partition that results from removing the first element ρ&lt;sub&gt;1&lt;/sub&gt; from ρ.

Note that the right-hand side is a sum of characters for symmetric groups that have smaller order than that of the symmetric group we started with on the left-hand side. In other words, this version of the Murnaghan-Nakayama rule expresses a character of the symmetric group S&lt;sub&gt;n&lt;/sub&gt; in terms of the characters of smaller symmetric groups S&lt;sub&gt;k&lt;/sub&gt; with ''k''&lt;''n''.

Applying this rule recursively will result in a tree of character value evaluations for smaller and smaller partitions. Each branch stops for one of two reasons: Either there are no border strips of the required length within the reduced shape, so the sum on the right is zero, or a border strip occupying the entire reduced shape is removed, leaving a Young diagram with no boxes. At this point we are evaluating &amp;chi;{{su|p=&amp;lambda;|b=&amp;rho;}} when both λ and ρ are the empty partition (), and the rule requires that this terminal case be defined as having character &lt;math&gt;\chi^{()}_{()} = 1&lt;/math&gt;.

This recursive version of the Murnaghan-Nakayama rule is especially efficient for computer calculation when one computes character tables for S&lt;sub&gt;k&lt;/sub&gt; for increasing values of ''k'' and stores all of the previously computed character tables.

==Example==

We will again compute the character value with λ=(5,2,1) and ρ=(3,3,1,1).

To begin, consider the Young diagram with shape λ. Since the first part of ρ is 3, look for border strips that consist of 3 boxes. There are two possibilities:

[[File:MurnaghanNakayama3.svg|frameless|none|200px|Example of the border-strip tableaux involved in computing a particular symmetric group character value using the recursive Murnaghan-Nakayama rule.]]

In the first diagram, the border strip has height 0, and removing it produces the reduced shape (2,2,1). In the second diagram, the border strip has height 1, and removing it produces the reduced shape (5). Therefore, one has

&lt;math&gt;\chi^{(5,2,1)}_{(3,3,1,1)}=\chi^{(2,2,1)}_{(3,1,1)}-\chi^{(5)}_{(3,1,1)}&lt;/math&gt;,

expressing a character value of S&lt;sub&gt;8&lt;/sub&gt; in terms of two character values of S&lt;sub&gt;5&lt;/sub&gt;.

Applying the rule again to both terms, one finds

&lt;math&gt;\chi^{(2,2,1)}_{(3,1,1)}=-\chi^{(2)}_{(1,1)}&lt;/math&gt;

and

&lt;math&gt;\chi^{(5)}_{(3,1,1)}=\chi^{(2)}_{(1,1)}&lt;/math&gt;,

reducing to a character value of S&lt;sub&gt;2&lt;/sub&gt;.

Applying again, one finds

&lt;math&gt;\chi^{(2)}_{(1,1)}=\chi^{(1)}_{(1)}&lt;/math&gt;,

reducing to the only character value of S&lt;sub&gt;1&lt;/sub&gt;.

A final application produces the terminal character &lt;math&gt;\chi^{()}_{()} = 1&lt;/math&gt;:

&lt;math&gt;\chi^{(1)}_{(1)}=\chi^{()}_{()}=1&lt;/math&gt;

Working backwards from this known character, the result is &lt;math&gt;\chi^{(5,2,1)}_{(3,3,1,1)}=-2&lt;/math&gt;, as before.

==References==
{{Reflist}}

{{DEFAULTSORT:Murnaghan-Nakayama rule}}
[[Category:Combinatorics]]
[[Category:Representation theory of finite groups]]
[[Category:Symmetry]]</text>
      <sha1>30dofpw8kekcvtl22ydjadfflzg7uga</sha1>
    </revision>
  </page>
  <page>
    <title>Optimal maintenance</title>
    <ns>0</ns>
    <id>2777780</id>
    <revision>
      <id>592664580</id>
      <parentid>592664310</parentid>
      <timestamp>2014-01-27T17:41:38Z</timestamp>
      <contributor>
        <username>Ronz</username>
        <id>7862</id>
      </contributor>
      <comment>Undid revision 592664310 by [[Special:Contributions/Ronz|Ronz]] ([[User talk:Ronz|talk]]) coi but seems ok</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2158">'''Optimal maintenance''' is the discipline within [[operations research]] concerned with maintaining a system in a manner that maximizes [[Profit (economics)|profit]] or minimizes [[cost]]. Cost functions depending on the [[reliability theory|reliability]], [[availability]] and [[maintainability]] characteristics of the system of interest determine the parameters to minimize. Parameters often considered are the cost of failure, the cost per time unit of "downtime" (for example: revenue losses), the cost (per time unit) of [[corrective maintenance]], the cost per time unit of [[preventive maintenance]] and the cost of [[repairable]] system replacement [Cassady and Pohl]. The foundation of any maintenance model relies on the correct description of the underlying deterioration process and failure behavior of the component, and on the relationships between maintained components in the product breakdown (system / sub-system / assembly / sub-assembly...).

Optimal Maintenance strategies are often constructed using stochastic models and focus on finding an optimal [[Planned maintenance|inspection time]] or the optimal acceptable degree of system degradation before maintenance and/or replacement. Cost considerations on an Asset scale may also lead to select a "run-to-failure" approach for specific components.

There are four main survey papers available accomplished to cover the spectrum of optimal maintenance:

* Optimal maintenance models for systems subject to failure–a review by YS Sherif, ML Smith published in Naval Research Logistics Quarterly, 1981.
* C. Valdez-Flores, R.M. Feldman, “A survey of preventive maintenance models for stochastically deteriorating single-unit systems”, Naval Research Logistics, vol 36, 1989 Aug, pp 419-446.
* J.J. McCall, “Maintenance policies for stochastically failing equipment:a survey”, Management Science, vol 11, 1965 Mar, pp 493-524.
* W.P. Pierskalla, J.A. Voelker, “A survey of maintenance models: The control and surveillance of deteriorating systems”, Naval Research Logistics Quarterly, vol 23, 1976 Sep, pp 353-388.

== External links ==



[[Category:Operations research]]</text>
      <sha1>2xu5uobill5ikjha7sw2m1djwfgpf2h</sha1>
    </revision>
  </page>
  <page>
    <title>Order-extension principle</title>
    <ns>0</ns>
    <id>21982763</id>
    <redirect title="Linear extension" />
    <revision>
      <id>838587901</id>
      <parentid>278903629</parentid>
      <timestamp>2018-04-28T00:14:18Z</timestamp>
      <contributor>
        <username>The Transhumanist</username>
        <id>1754504</id>
      </contributor>
      <comment>Add template{{R to section}} to categorize redirect using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="111">#REDIRECT [[Linear extension#Order-extension principle]]

{{R to section}}
[[Category:Mathematical principles]]</text>
      <sha1>5absdp5rb3lptcoypirypdxd9v7bxyh</sha1>
    </revision>
  </page>
  <page>
    <title>Orthogonal symmetric Lie algebra</title>
    <ns>0</ns>
    <id>30505908</id>
    <revision>
      <id>649957892</id>
      <parentid>643987487</parentid>
      <timestamp>2015-03-05T07:13:49Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>more wikilinks</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1234">In [[mathematics]], an '''orthogonal symmetric Lie algebra''' is a pair &lt;math&gt;(\mathfrak{g}, s)&lt;/math&gt; consisting of a real [[Lie algebra]] &lt;math&gt;\mathfrak{g}&lt;/math&gt; and an [[automorphism]] &lt;math&gt;s&lt;/math&gt; of &lt;math&gt;\mathfrak{g}&lt;/math&gt; of order &lt;math&gt;2&lt;/math&gt; such that the [[eigenspace]] &lt;math&gt;\mathfrak{u}&lt;/math&gt; of ''s'' corrsponding to 1 (i.e., the set &lt;math&gt;\mathfrak{u}&lt;/math&gt; of [[Fixed point (mathematics)|fixed points]]) is a [[compact Lie algebra|compact subalgebra]]. If "compactness" is omitted, it is called a '''symmetric Lie algebra'''. An orthogonal symmetric Lie algebra is said to be ''effective'' if &lt;math&gt;\mathfrak{u}&lt;/math&gt; intersects the [[Center (group theory)|center]] of &lt;math&gt;\mathfrak{g}&lt;/math&gt; [[trivial group|trivially]]. In practice, effectiveness is often assumed; we do this in this article as well.

The canonical example is the Lie algebra of a [[symmetric space]], &lt;math&gt;s&lt;/math&gt; being the differential of a symmetry.

Every orthogonal symmetric Lie algebra decomposes into a [[direct sum]] of ideals "of compact type", "of noncompact type" and "of Euclidean type".

== References ==
* S. Helgason, ''Differential geometry, Lie groups, and symmetric spaces''

[[Category:Lie algebras]]


{{math-stub}}</text>
      <sha1>4qwb9yqjfcl9ghst0yojzk7acohrcjj</sha1>
    </revision>
  </page>
  <page>
    <title>Pfaffian orientation</title>
    <ns>0</ns>
    <id>56281176</id>
    <revision>
      <id>846301299</id>
      <parentid>822916895</parentid>
      <timestamp>2018-06-17T20:44:17Z</timestamp>
      <contributor>
        <username>OAbot</username>
        <id>28481209</id>
      </contributor>
      <minor/>
      <comment>[[Wikipedia:OABOT|Open access bot]]: add arxiv identifier to citation with #oabot.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5044">In [[graph theory]], a '''Pfaffian orientation''' of an [[undirected graph]]
&lt;math&gt;G&lt;/math&gt; is an [[Orientation (graph theory)|orientation]] (an assignment of a direction to each edge of the graph) in which every even central cycle is oddly oriented. In this definition, a [[cycle (graph theory)|cycle]] &lt;math&gt;C&lt;/math&gt; is even if it contains an even number of edges. &lt;math&gt;C&lt;/math&gt; is central if the subgraph of &lt;math&gt;G&lt;/math&gt; formed by removing all the vertices of &lt;math&gt;C&lt;/math&gt; has a [[perfect matching]]; central cycles are also sometimes called alternating circuits. And &lt;math&gt;C&lt;/math&gt; is oddly oriented if each of the two orientations of &lt;math&gt;C&lt;/math&gt; is consistent with an odd number of edges in the orientation.{{r|nt|t}}

Pfaffian orientations have been studied in connection with the [[FKT algorithm]] for counting the number of perfect matchings in a given graph. In this algorithm, the orientations of the edges are used to assign the values &lt;math&gt;\pm 1&lt;/math&gt; to the variables in the [[Tutte matrix]] of the graph. Then, the [[Pfaffian]] of this matrix (the [[square root]] of its [[determinant]]) gives the number of perfect matchings. Each perfect matching contributes &lt;math&gt;\pm 1&lt;/math&gt; to the Pfaffian regardless of which orientation is used; the choice of a Pfaffian orientation ensures that these contributions all have the same sign as each other, so that none of them cancel.
This result stands in contrast to the much higher computational complexity of counting matchings in arbitrary graphs.{{r|t}}

A graph is said to be Pfaffian if it has a Pfaffian orientation.
Every [[planar graph]] is Pfaffian.{{r|k}}
An orientation in which each face of a planar graph has an odd number of clockwise-oriented edges is automatically Pfaffian. Such an orientation can be found by starting with an arbitrary orientation of a [[spanning tree]] of the graph.
The remaining edges, not in this tree, form a spanning tree of the [[dual graph]], and their orientations can be chosen according to a bottom-up traversal of the dual spanning tree in order to ensure that each face of the original graph has an odd number of clockwise edges. More generally, every &lt;math&gt;K_{3,3}&lt;/math&gt;-minor-free graph has a Pfaffian orientation. These are the graphs that do not have the [[utility graph]] &lt;math&gt;K_{3,3}&lt;/math&gt; (which is not Pfaffian) as a [[graph minor]]. By [[Wagner's theorem]], the &lt;math&gt;K_{3,3}&lt;/math&gt;-minor-free graphs are formed by gluing together copies of planar graphs and the [[complete graph]] &lt;math&gt;K_5&lt;/math&gt; along shared edges. The same gluing structure can be used to obtain a Pfaffian orientation for these graphs.{{r|l}}

Along with &lt;math&gt;K_{3,3}&lt;/math&gt;, there are infinitely many minimal non-Pfaffian graphs.{{r|nt}} For [[bipartite graph]]s, it is possible to determine whether a Pfaffian orientation exists, and if so find one, in [[polynomial time]].{{r|rst}}

==References==
{{reflist|refs=

&lt;ref name=k&gt;{{citation
 | last = Kasteleyn | first = P. W. | authorlink = Pieter Kasteleyn
 | contribution = Graph theory and crystal physics
 | mr = 0253689
 | pages = 43–110
 | publisher = Academic Press | location = London
 | title = Graph Theory and Theoretical Physics
 | year = 1967}}&lt;/ref&gt;

&lt;ref name=l&gt;{{citation
 | last = Little | first = Charles H. C.
 | journal = Combinatorial mathematics (Proc. Second Australian Conf., Univ. Melbourne, Melbourne, 1973)
 | mr = 0382062
 | pages = 63–72
 | series = Lecture Notes in Mathematics
 | volume = 403
 | publisher = Springer, Berlin
 | title = An extension of Kasteleyn's method of enumerating the 1-factors of planar graphs
 | year = 1974}}&lt;/ref&gt;

&lt;ref name=nt&gt;{{citation
 | last1 = Norine | first1 = Serguei
 | last2 = Thomas | first2 = Robin | author2-link = Robin Thomas (mathematician)
 | doi = 10.1016/j.jctb.2007.12.005
 | issue = 5
 | journal = Journal of Combinatorial Theory
 | mr = 2442595
 | pages = 1038–1055
 | series = Series B
 | title = Minimally non-Pfaffian graphs
 | volume = 98
 | year = 2008}}&lt;/ref&gt;

&lt;ref name=rst&gt;{{citation
 | last1 = Robertson | first1 = Neil | author1-link = Neil Robertson (mathematician)
 | last2 = Seymour | first2 = P. D. | author2-link = Paul Seymour (mathematician)
 | last3 = Thomas | first3 = Robin | author3-link = Robin Thomas (mathematician)
 | doi = 10.2307/121059
 | issue = 3
 | journal = Annals of Mathematics
 | mr = 1740989
 | pages = 929–975
 | series = Second Series
 | title = Permanents, Pfaffian orientations, and even directed circuits
 | volume = 150
 | year = 1999| arxiv = math/9911268}}&lt;/ref&gt;

&lt;ref name=t&gt;{{citation
 | last = Thomas | first = Robin | authorlink = Robin Thomas (mathematician)
 | contribution = A survey of Pfaffian orientations of graphs
 | contribution-url = http://people.math.gatech.edu/~thomas/PAP/pfafsurv.pdf
 | doi = 10.4171/022-3/47
 | mr = 2275714
 | pages = 963–984
 | publisher = Eur. Math. Soc. | location = Zürich
 | title = International Congress of Mathematicians. Vol. III
 | year = 2006}}&lt;/ref&gt;

}}

[[Category:Graph theory objects]]
[[Category:Matching]]</text>
      <sha1>oki426bmlkhd7p79zrcsr8rch06i5ku</sha1>
    </revision>
  </page>
  <page>
    <title>Radon transform</title>
    <ns>0</ns>
    <id>503378</id>
    <revision>
      <id>863895256</id>
      <parentid>859411047</parentid>
      <timestamp>2018-10-13T19:47:54Z</timestamp>
      <contributor>
        <username>Treesmill</username>
        <id>428736</id>
      </contributor>
      <comment>/* Definition */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19171">[[File:Radon transform.png|thumb|Radon transform. Maps ''f'' on the (''x, y'')-domain into ''f'' on the (''α, s'')-domain.]]
[[File:Sinogram - Two Square Indicator Phantom.svg|thumb|right|Radon transform of the [[indicator function]] of two squares shown in the image below.  Lighter regions indicate larger function values.  Black indicates zero.]]
[[File:Sinogram Source - Two Squares Phantom.png|thumb|right|Original function is equal to one on the white region and zero on the dark region.]]
In [[mathematics]], the '''Radon transform''' is the [[integral transform]] which takes a function ''f'' defined on the plane to a function ''Rf'' defined on the (two-dimensional) space of lines in the plane, whose value at a particular line is equal to the [[line integral]] of the function over that line. The transform was introduced in 1917 by [[Johann Radon]],{{sfn|Radon|1917}} who also provided a formula for the inverse transform. Radon further included formulas for the transform in [[three dimensions]], in which the integral is taken over planes (integrating over lines is known as the [[X-ray transform]]).  It was later generalized to higher-dimensional [[Euclidean space]]s, and more broadly in the context of [[integral geometry]].  The [[complex number|complex]] analog of the Radon transform is known as the [[Penrose transform]]. The Radon transform is widely applicable to [[tomography]], the creation of an image from the projection data associated with cross-sectional scans of an object.
{{multiple image
 | align     = right
 | direction = horizontal
 | image1    = SheppLogan_Phantom.svg
 | width1    = 170
 | caption1  = [[Shepp-Logan Phantom|Shepp Logan phantom]]
 | image2    = Shepp logan radon.png 
 | width2    = 42
 | caption2  = Radon transform
 | image3    = Shepp logan iradon.png
 | width3    = 170
 | caption3  = Inverse Radon transform
}}

==Explanation==
If a function &lt;math&gt;f&lt;/math&gt; represents an unknown density, then the Radon transform represents the projection data obtained as the output of a tomographic scan.  Hence the inverse of the Radon transform can be used to reconstruct the original density from the projection data, and thus it forms the mathematical underpinning for [[tomographic reconstruction]], also known as [[iterative reconstruction]].

The Radon transform data is often called a '''sinogram''' because the Radon transform of an off-center point source is a sinusoid.  Consequently, the Radon transform of a number of small objects appears graphically as a number of blurred [[sine wave]]s with different amplitudes and phases.

The Radon transform is useful in [[computed axial tomography]] (CAT scan), [[barcode]] scanners, [[electron microscopy]] of [[macromolecular assemblies]] like [[virus]]es and [[protein complex]]es, [[reflection seismology]] and in the solution of hyperbolic [[partial differential equations]].

==Definition==
Let ''&amp;fnof;''('''x''')&amp;nbsp;=&amp;nbsp;''&amp;fnof;''(''x'',''y'') be a [[Support (mathematics)|compactly supported]] [[continuous function]] on '''R'''&lt;sup&gt;2&lt;/sup&gt;.  The Radon transform, ''R&amp;fnof;'', is a function defined on the space of straight lines ''L'' in '''R'''&lt;sup&gt;2&lt;/sup&gt; by the [[Line integral#Definition|line integral]] along each such line:
:&lt;math&gt;Rf(L) = \int_L f(\mathbf{x})\,|d\mathbf{x}|.&lt;/math&gt;
Concretely, the parametrization of any straight line ''L'' with respect to arc length ''z'' can always be written
:&lt;math&gt;(x(z),y(z)) = \Big( (z\sin\alpha+s\cos\alpha), (-z\cos\alpha+s\sin\alpha)  \Big) \,&lt;/math&gt;
where ''s'' is the distance of ''L'' from the origin and &lt;math&gt;\alpha&lt;/math&gt; is the angle the normal vector to ''L'' makes with the ''x'' axis.  It follows that the quantities (α,''s'') can be considered as coordinates on the space of all lines in '''R'''&lt;sup&gt;2&lt;/sup&gt;, and the Radon transform can be expressed in these coordinates by
:&lt;math&gt;\begin{align}Rf(\alpha,s) &amp;= \int_{-\infty}^{\infty} f(x(z),y(z))\, dz\\ &amp;= \int_{-\infty}^{\infty} f\big(  (z\sin\alpha+s\cos\alpha), (-z\cos\alpha+s\sin\alpha) \big)\, dz\end{align}&lt;/math&gt;

More generally, in the ''n''-dimensional [[Euclidean space]] '''R'''&lt;sup&gt;''n''&lt;/sup&gt;, the Radon transform of a [[compact support|compactly supported]] continuous function ''&amp;fnof;'' is a function ''R&amp;fnof;'' on the space Σ&lt;sub&gt;''n''&lt;/sub&gt; of all [[hyperplane]]s in '''R'''&lt;sup&gt;''n''&lt;/sup&gt;.  It is defined by
:&lt;math&gt;Rf(\xi) = \int_\xi f(\mathbf{x})\, d\sigma(\mathbf{x})&lt;/math&gt;
for ξ&amp;nbsp;∈Σ&lt;sub&gt;''n''&lt;/sub&gt;, where the integral is taken with respect to the natural [[hypersurface]] [[measure (mathematics)|measure]], ''d''σ (generalizing the |''d'''''x'''| term from the 2-dimensional case).  Observe that any element of &amp;Sigma;&lt;sub&gt;''n''&lt;/sub&gt; is characterized as the solution locus of an equation
:&lt;math&gt;\mathbf{x}\cdot\alpha = s&lt;/math&gt;
where &amp;alpha;&amp;nbsp;&amp;isin;&amp;nbsp;''S''&lt;sup&gt;''n''&amp;minus;1&lt;/sup&gt; is a [[unit vector]] and ''s''&amp;nbsp;&amp;isin;&amp;nbsp;'''R'''.  Thus the ''n''-dimensional Radon transform may be rewritten as a function on ''S''&lt;sup&gt;''n''&amp;minus;1&lt;/sup&gt;&amp;times;'''R''' via
:&lt;math&gt;Rf(\alpha,s) = \int_{\mathbf{x}\cdot\alpha = s} f(\mathbf{x})\, d\sigma(\mathbf{x}).&lt;/math&gt;

It is also possible to generalize the Radon transform still further by integrating instead over ''k''-dimensional affine subspaces of '''R'''&lt;sup&gt;''n''&lt;/sup&gt;.  The [[X-ray transform]] is the most widely used special case of this construction, and is obtained by integrating over straight lines.

==Relationship with the Fourier transform==
{{main|Projection-slice theorem}}

[[File:Radon transform via Fourier transform.png|thumb|Computing the 2-dimensional Radon transform in terms of two Fourier transforms.]]

The Radon transform is closely related to the [[Fourier transform]]. We define the one variable Fourier transform here as

:&lt;math&gt;\hat{f}(\omega)=\int_{-\infty}^\infty f(x)e^{-2\pi ix\omega }\,dx.
&lt;/math&gt;

and for a function of a 2-vector &lt;math&gt;\mathbf{x}=(x,y)&lt;/math&gt;,

:&lt;math&gt;
\hat{f}(\mathbf{w})=\int\limits_{-\infty}^{\infty}
\int\limits_{-\infty }^{\infty}  f(\mathbf{x})e^{-2\pi i\mathbf{x}\cdot\mathbf{w}}\,dx\, dy.
&lt;/math&gt;

For convenience, denote &lt;math&gt;\mathcal{R}_\alpha[f](s)= \mathcal{R}[f](\alpha,s)&lt;/math&gt;. The [[Fourier slice theorem]] then states

:&lt;math&gt;
\widehat{\mathcal{R}_{\alpha}[f]}(\sigma)=\hat{f}(\sigma\mathbf{n}(\alpha))
&lt;/math&gt;

where

:&lt;math&gt;\mathbf{n}(\alpha)= (\cos \alpha,\sin\alpha).&lt;/math&gt;

Thus the two-dimensional Fourier transform of the initial function along a line at the inclination angle &lt;math&gt;\alpha&lt;/math&gt; is the one variable Fourier transform of the Radon transform (acquired at angle &lt;math&gt;\alpha&lt;/math&gt;) of that function. This fact can be used to compute both the Radon transform and its inverse.

The result can be generalized into ''n'' dimensions 

:&lt;math&gt;\hat{f}(r\alpha) = \int_{-\infty}^\infty \mathcal{R}f(\alpha,s)e^{-2\pi i sr}\, ds.&lt;/math&gt;

==Dual transform==
The dual Radon transform is a kind of [[Hermitian adjoint|adjoint]] to the Radon transform.  Beginning with a function ''g'' on the space Σ&lt;sub&gt;''n''&lt;/sub&gt;, the dual Radon transform is the function &lt;math&gt;\mathcal{R}^*g&lt;/math&gt; on '''R'''&lt;sup&gt;''n''&lt;/sup&gt; defined by
:&lt;math&gt;\mathcal{R}^*g(x) = \int_{x\in\xi} g(\xi)\,d\mu(\xi).&lt;/math&gt;
The integral here is taken over the set of all hyperplanes incident with the point ''x''&amp;nbsp;∈&amp;nbsp;'''R'''&lt;sup&gt;''n''&lt;/sup&gt;, and the measure ''d''μ is the unique [[probability measure]] on the set &lt;math&gt;\{\xi | x\in\xi\}&lt;/math&gt; invariant under rotations about the point ''x''.

Concretely, for the two-dimensional Radon transform, the dual transform is given by
:&lt;math&gt;\mathcal{R}^*g(x) = \frac{1}{2\pi}\int_{\alpha=0}^{2\pi}g(\alpha,\mathbf{n}(\alpha)\cdot\mathbf{x})\,d\alpha.&lt;/math&gt;
In the context of image processing, the dual transform is commonly called ''backprojection''{{sfn|Roerdink|2001}} as it takes a function defined on each line in the plane and 'smears' or projects it back over the line to produce an image.

===Intertwining property===
Let Δ denote the [[Laplacian]] on '''R'''&lt;sup&gt;''n''&lt;/sup&gt;: 
:&lt;math&gt;\Delta = \frac{\partial^2}{\partial x_1^2}+\cdots+\frac{\partial^2}{\partial x_n^2}.&lt;/math&gt;
This is a natural rotationally invariant second-order [[differential operator]].  On Σ&lt;sub&gt;''n''&lt;/sub&gt;, the "radial" second derivative
:&lt;math&gt;Lf(\alpha,s) \equiv \frac{\partial^2}{\partial s^2} f(\alpha,s)&lt;/math&gt;
is also rotationally invariant. The Radon transform and its dual are [[intertwining operator]]s for these two differential operators in the sense that{{sfn|Helgason|1984|loc=Lemma I.2.1}}
:&lt;math&gt;\mathcal{R}(\Delta f) = L (\mathcal{R}f),\quad \mathcal{R}^* (Lg) = \Delta(\mathcal{R}^*g).&lt;/math&gt;

==Reconstruction approaches==

The process of  ''reconstruction'' produces the image (or function &lt;math&gt;f&lt;/math&gt; in the previous section) from its projection data. ''Reconstruction'' is an [[inverse problem]].

===Radon inversion formula===
In the 2D case, the most commonly used analytical formula to recover &lt;math&gt;f&lt;/math&gt; from its Radon transform is the ''Filtered Backprojection Formula'' or ''Radon Inversion Formula'':

:&lt;math&gt;f(\mathbf{x})=\int^{\pi}_{0}(\mathcal{R}f(\cdot,\theta)*h)(\left\langle\mathbf{x},\mathbf{n}_{\theta} \right\rangle) d\theta&lt;/math&gt;{{sfn|Candès|2016a}}

where &lt;math&gt;h&lt;/math&gt; is such that &lt;math&gt;\hat{h}(k)=|k|&lt;/math&gt;.{{sfn|Candès|2016b}}

The convolution kernel &lt;math&gt;h&lt;/math&gt; is referred to as [[Ramp filter]] in some literature.

===Ill-posedness===
Intuitively, in the ''filtered backprojection'' formula, by analogy with differentiation, for which &lt;math&gt;\left(\widehat{\frac{d}{dx}f}\right)\!(k)=ik\widehat{f}(k)&lt;/math&gt;, we see that the filter performs an operation similar to a derivative. Roughly speaking, then, the filter makes objects ''more'' singular.

A quantitive statement of the ill-posedness of Radon Inversion goes as follows:

We have &lt;math&gt;\widehat{\mathcal{R}^*\mathcal{R}g}(k)=\frac{1}{||\mathbf{k}||}\hat{g}(\mathbf{k})&lt;/math&gt;

where &lt;math&gt;\mathcal{R}^*&lt;/math&gt; is the previously defined [[adjoint]] to the Radon Transform.

Thus for &lt;math&gt;g(\mathbf{x})=e^{i \left\langle\mathbf{k}_0,\mathbf{x}\right\rangle}&lt;/math&gt;, 
:&lt;math&gt; \mathcal{R}^*\mathcal{R}g=\frac{1}{||\mathbf{k_0}||}e^{i \left\langle\mathbf{k}_0,\mathbf{x}\right\rangle}&lt;/math&gt;.

The complex exponential &lt;math&gt;e^{i \left\langle\mathbf{k}_0,\mathbf{x}\right\rangle}&lt;/math&gt; is thus an eigenfunction of &lt;math&gt;\mathcal{R}^*\mathcal{R}&lt;/math&gt; with eigenvalue &lt;math&gt;\frac{1}{||\mathbf{k_0}||}&lt;/math&gt;. Thus the singular values of &lt;math&gt;\mathcal{R}&lt;/math&gt; are &lt;math&gt;\sqrt{\frac{1}{||\mathbf{k}||}}&lt;/math&gt;. Since these singular values tend to 0, &lt;math&gt;\mathcal{R}^{-1}&lt;/math&gt; is unbounded.{{sfn|Candès|2016b}}

===Iterative reconstruction methods===
{{main|Iterative reconstruction}}
Compared with the ''Filtered Backprojection'' method, iterative reconstruction costs large computation time, limiting its practical use. However, due to the ill-posedness of Radon Inversion, the ''Filtered Backprojection'' method may be infeasible in the presence of discontinuity or noise. Iterative reconstruction methods (''e.g.'', [[SAMV (algorithm)|iterative Sparse Asymptotic Minimum Variance]]&lt;ref name=AbeidaZhang&gt;{{cite journal | last=Abeida | first=Habti | last2=Zhang | first2=Qilin | last3=Li | first3=Jian | last4=Merabtine | first4=Nadjim | title=Iterative Sparse Asymptotic Minimum Variance Based Approaches for Array Processing | journal=IEEE Transactions on Signal Processing | publisher=IEEE | volume=61 | issue=4 | year=2013 | issn=1053-587X | doi=10.1109/tsp.2012.2231676 | pages=933–944 | url=https://qilin-zhang.github.io/_pages/pdfs/SAMVpaper.pdf | format=PDF}}&lt;/ref&gt;) could provide metal artifact reduction, noise and dose reduction for the reconstructed result that attract much research interest around the world.

==Inversion formulas==

Explicit and computationally efficient inversion formulas for the Radon transform and its dual are available.  The Radon transform in ''n'' dimensions can be inverted by the formula{{sfn|Helgason|1984|loc=Theorem I.2.13}}
:&lt;math&gt;c_n f = (-\Delta)^{(n-1)/2}R^*Rf\,&lt;/math&gt;
where 
:&lt;math&gt;c_n = (4\pi)^{(n-1)/2}\frac{\Gamma(n/2)}{\Gamma(1/2)}.&lt;/math&gt;
and the power of the Laplacian (&amp;minus;Δ)&lt;sup&gt;(''n''&amp;minus;1)/2&lt;/sup&gt; is defined as a [[pseudodifferential operator]] if necessary by the [[Fourier transform]]
:&lt;math&gt;\left[\mathcal{F}(-\Delta)^{(n-1)/2}\phi\right](\xi) = |2\pi\xi|^{n-1}(\mathcal{F}\phi)(\xi).&lt;/math&gt;

For computational purposes, the power of the Laplacian is commuted with the dual transform ''R''&lt;sup&gt;*&lt;/sup&gt; to give{{sfn|Helgason|1984|loc=Theorem I.2.16}}
:&lt;math&gt;c_nf = \begin{cases}
R^*\frac{d^{n-1}}{ds^{n-1}}Rf &amp; n \rm{\ odd}\\
R^*H_s\frac{d^{n-1}}{ds^{n-1}}Rf &amp; n \rm{\ even}
\end{cases}
&lt;/math&gt;
where ''H''&lt;sub&gt;''s''&lt;/sub&gt; is the [[Hilbert transform]] with respect to the ''s'' variable.  In two dimensions, the operator ''H''&lt;sub&gt;''s''&lt;/sub&gt;''d''/''ds'' appears in image processing as a [[ramp filter]].{{sfn|Nygren|1997}}
One can prove directly from the Fourier slice theorem and change of variables for integration that for a compactly supported continuous function ƒ of two variables
:&lt;math&gt;f = \frac{1}{2}R^{*}H_s\frac{d}{ds}Rf.&lt;/math&gt;
Thus in an image processing context the original image ƒ can be recovered from the 'sinogram' data ''R''ƒ by applying a ramp filter (in the &lt;math&gt;s&lt;/math&gt;  variable) and then back-projecting. As the filtering step can be performed efficiently (for example using [[digital signal processing]] techniques) and the back projection step is simply an accumulation of values in the pixels of the image, this results in a highly efficient, and hence widely used, algorithm.

Explicitly, the inversion formula obtained by the latter method is{{sfn|Roerdink|2001}}
:&lt;math&gt;f(x) = \frac{1}{2}(2\pi)^{1-n}(-1)^{(n-1)/2}\int_{S^{n-1}}\frac{\partial^{n-1}}{\partial s^{n-1}}Rf(\alpha,\alpha\cdot x)\,d\alpha&lt;/math&gt;
if ''n'' is odd, and
:&lt;math&gt;f(x) = (2\pi)^{-n}(-1)^{n/2}\int_{-\infty}^\infty \frac{1}{q}\int_{S^{n-1}}\frac{\partial^{n-1}}{\partial s^{n-1}}Rf(\alpha,\alpha\cdot x + q)\,d\alpha\,dq&lt;/math&gt;
if ''n'' is even.

The dual transform can also be inverted by an analogous formula:
:&lt;math&gt;c_n g = (-L)^{(n-1)/2}R(R^*g).\,&lt;/math&gt;

== See also ==
* [[Periodogram]]
* [[Matched filter]]
* [[Deconvolution]]
* [[X-ray transform]]
* [[Funk transform]]
* The [[Hough transform]], when written in a continuous form, is very similar, if not equivalent, to the Radon transform.{{sfn|van Ginkel|Hendricks|van Vliet|2004}}
* [[Crofton formula|Cauchy-Crofton theorem]] is a closely related formula for computing the length of curves in space.
* [[Fourier transform]]
* [[Fast Fourier transform]]

==Notes==
{{reflist}}

==References==
{{refbegin}}
* {{citation |title = Über die Bestimmung von Funktionen durch ihre Integralwerte längs gewisser Mannigfaltigkeiten |last = Radon |first = Johann |authorlink = Johann Radon |journal = Berichte über die Verhandlungen der Königlich-Sächsischen Akademie der Wissenschaften zu Leipzig, Mathematisch-Physische Klasse [Reports on the proceedings of the Royal Saxonian Academy of Sciences at Leipzig, mathematical and physical section] |location = Leipzig |publisher = Teubner |year = 1917 |issue = 69 |pages = 262–277 }}; ''Translation:'' {{citation |title = On the determination of functions from their integral values along certain manifolds |journal = IEEE Transactions on Medical Imaging |year = 1986 |volume = 5 |pages = 170–176 |author = Radon, J. |author2 = Parks, P.C. (translator) |doi = 10.1109/TMI.1986.4307775 |pmid = 18244009 |issue = 4 }}.
* {{springer|id=t/t092980|title=Tomography|first=J.B.T.M.|last=Roerdink}}.
* {{citation |first = Sigurdur |last = Helgason |authorlink = Sigurður Helgason (mathematician) |title = Groups and Geometric Analysis: Integral Geometry, Invariant Differential Operators, and Spherical Functions |year = 1984 |publisher = Academic Press |isbn = 0-12-338301-3 }}.
* {{cite web |title = Applied Fourier Analysis and Elements of Modern Signal Processing - Lecture 9 |last1 = Candès |first1 = Emmanuel |url = http://statweb.stanford.edu/~candes/math262/Lectures/Lecture09.pdf |date = February 2, 2016a  |ref = harv }}
* {{cite web |title = Applied Fourier Analysis and Elements of Modern Signal Processing - Lecture 10 |last1 = Candès |first1 = Emmanuel |url = http://statweb.stanford.edu/~candes/math262/Lectures/Lecture10.pdf |date = February 4, 2016b  |ref = harv }}
* {{cite web |url = http://www.owlnet.rice.edu/~elec539/Projects97/cult/node2.html |title = Filtered Back Projection |work = Tomographic Reconstruction of SPECT Data |last = Nygren |first = Anders J. |year = 1997 |ref = harv }}
* {{cite web |title = A short introduction to the Radon and Hough transforms and how they relate to each other |last1 = van Ginkel |first1 = M. |last2 = Hendricks |first2 = C.L. Luengo |last3 = van Vliet |first3 = L.J. |year = 2004 |url = http://tnw.home.tudelft.nl/fileadmin/Faculteit/TNW/Over_de_faculteit/Afdelingen/Imaging_Science_and_Technology/Research/Research_Groups/Quantitative_Imaging/Publications/Technical_Reports/doc/mvanginkel_radonandhough_tr2004.pdf |archive-url = https://web.archive.org/web/20160729172119/http://tnw.home.tudelft.nl/fileadmin/Faculteit/TNW/Over_de_faculteit/Afdelingen/Imaging_Science_and_Technology/Research/Research_Groups/Quantitative_Imaging/Publications/Technical_Reports/doc/mvanginkel_radonandhough_tr2004.pdf |archive-date = 2016-07-29 |dead-url = no |ref = harv }}
{{refend}}

==Further reading==
* {{citation |last = Deans |first = Stanley R. |title = The Radon Transform and Some of Its Applications |year = 1983 |publisher = John Wiley &amp; Sons |location = New York }}
* {{Citation |last1 = Helgason |first1 = Sigurdur |title = Geometric analysis on symmetric spaces |publisher = [[American Mathematical Society]] |location = Providence, R.I. |edition = 2nd |series = Mathematical Surveys and Monographs |isbn = 978-0-8218-4530-1 |mr = 2463854 |year = 2008 |volume = 39 }}
* {{citation |last = Herman |first = Gabor T. |authorlink = Gabor Herman |title = Fundamentals of Computerized Tomography: Image Reconstruction from Projections |year = 2009 |publisher = Springer |edition = 2nd |isbn = 978-1-85233-617-2 }}
* {{springer |id = r/r077190 |title = Radon transform |first = R.A. |last = Minlos}}
* {{citation |first = Frank |last = Natterer |title = The Mathematics of Computerized Tomography |series = Classics in Applied Mathematics |volume = 32 |publisher = Society for Industrial and Applied Mathematics |isbn = 0-89871-493-1 }}
* {{citation |first1 = Frank |last1 = Natterer |first2 = Frank |last2 = Wübbeling |title = Mathematical Methods in Image Reconstruction |publisher = Society for Industrial and Applied Mathematics |isbn = 0-89871-472-9 }}

==External links==
* {{MathWorld |urlname = RadonTransform |pagename = Radon Transform}}
* {{cite AV media |date = September 10, 2015 |title = Analytical projection (the Radon transform) |medium = video |institution = [[University of Antwerp]] |series = Part of the "Computed Tomography and the ASTRA Toolbox" course |url = https://www.youtube.com/watch?v=MA2y_2YySq0 }}

[[Category:Integral geometry]]
[[Category:Integral transforms]]</text>
      <sha1>taayh6fmlo21am1vjcbav24ux0ac71v</sha1>
    </revision>
  </page>
  <page>
    <title>Ronald C. Read</title>
    <ns>0</ns>
    <id>18154982</id>
    <revision>
      <id>868363464</id>
      <parentid>839954067</parentid>
      <timestamp>2018-11-11T18:44:40Z</timestamp>
      <contributor>
        <username>Siliconred</username>
        <id>32070437</id>
      </contributor>
      <comment>add age</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4120">{{Infobox scientist
|name              = R. C. Read
|image             = 
|image_size       =
|caption           =
|birth_date        = {{Birth date and age|1924|12|19}}
|birth_place       = [[Croydon, England]]
|residence         = [[Canada]]
|citizenship       = [[Canadian]]
|nationality       =
|ethnicity         =
|field             = [[Graph theory]]
|work_institutions = [[University of Waterloo]]
|alma_mater        = [[University of Cambridge]] and [[University of London]]
|doctoral_advisor  =
|doctoral_students = [[Jorge Urrutia Galicia|Jorge Urrutia]] and [[William Lawrence Kocay]]
|known_for         =
|author_abbrev_bot =
|author_abbrev_zoo =
|Spouse            =
|influences        =
|influenced        =
|prizes            =
|footnotes         =
|signature         =
}}

'''Ronald Cedric Read''' (Ron Read, born December 19, 1924) is a professor emeritus of mathematics at the [[University of Waterloo]], Canada. He has published many books&lt;ref&gt;[https://www.amazon.com/s?ie=UTF8&amp;search-type=ss&amp;index=books&amp;field-author=Ronald%20C.%20Read&amp;page=1 - Books on Amazon]&lt;/ref&gt; and papers, primarily on enumeration of graphs, [[graph isomorphism]], [[chromatic polynomial]]s&lt;!-- mathscinet --&gt;, and particularly, the use of computers in graph-theoretical research. A majority of his later work was done in Waterloo.
Read received his Ph.D. (1959) in graph theory from the [[University of London]].&lt;ref&gt;[http://genealogy.math.ndsu.nodak.edu/id.php?id=42675 - His Ph.D. ]&lt;/ref&gt;

==Life and career==
Ronald Read served in the Royal Navy during [[World War II]], then completed a degree in mathematics at the University of Cambridge before joining [[The University of the West Indies]] in Jamaica as the second founding member of the Mathematics Department there. In 1970 he moved his family to Canada to take up a post as Professor of Mathematics at the University of Waterloo, Ontario, Canada.

While in Jamaica he became interested in cave exploration, and in 1957 he founded the Jamaica Caving Club.

He has had a lifelong interest in the making of string figures and is the inventor of the {{YouTube|BnUa38SSuUs|Olympic Flag String Figure}}.

He is an accomplished musician and plays many instruments including violin, viola, cello, double bass, piano, guitar, lute, and many early music instruments, some of which he has also built. He has diplomas in Theory and in Composition from the Royal Conservatory of Music in Toronto, Canada, and has composed four works for orchestra and several pieces for smaller groups.&lt;ref&gt;[http://www.broadbent-dunn.com/manufacturers.php?manufacturerid=53 - Published music]&lt;/ref&gt;

==Selected papers==
* An Introduction to Chromatic Polynomials. Journal of Combinatorial Theory 4 (1968) 52 - 71.
* Every One A Winner; or How to avoid isomorphism search when cataloguing combinatorial configurations. Annals of Discrete Mathematics 2, North-Holland Publishing Company (1978) 107-120.
* (With P. Rosenstiehl) On the Principal Edge Tripartition of a Graph. Annals of Discrete Mathematics 3, North-Holland Publishing Company, (1978) 195-226.
* (With [[W. T. Tutte]]), Chromatic Polynomials. Selected Topics in Graph Theory, Vol. 3 (1988) 15-42.
* (with G. F. Royle) Chromatic Roots of Families of Graphs. Graph Theory, Combinatorics and Applications. John Wiley (1991) 1009 - 1029
* Prospects for Graph-theoretical Algorithms.  Annals of Discrete Mathematics 55 (1993) 201 - 210.

==Books==
* "Tangrams : 330 Puzzles". New York: Dover Publications (1965) {{isbn|0-486-21483-4}}.
* "A Mathematical Background for Economists and Social Scientists", Prentice Hall series in mathematical economics (1972) {{isbn|0-13-560987-9}}.
* (With Robin J. Wilson) "An Atlas of Graphs". Oxford Science Publications (2005) {{isbn|0-19-853289-X}}.

==References==
{{Reflist}}

{{Authority control}}

{{DEFAULTSORT:Read, Ronald C.}}
[[Category:Graph theorists]]
[[Category:University of Waterloo faculty]]
[[Category:20th-century mathematicians]]
[[Category:British expatriate academics in Canada]]
[[Category:1924 births]]
[[Category:Living people]]
[[Category:Alumni of the University of London]]</text>
      <sha1>8rwd4tgo6u9ephazfuszylisfnm5ndb</sha1>
    </revision>
  </page>
  <page>
    <title>Scott Flansburg</title>
    <ns>0</ns>
    <id>33101260</id>
    <revision>
      <id>844860431</id>
      <parentid>844860155</parentid>
      <timestamp>2018-06-07T17:23:12Z</timestamp>
      <contributor>
        <username>Prestocookie</username>
        <id>11807027</id>
      </contributor>
      <comment>/* Early career */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10523">{{infobox writer &lt;!-- for more information see [[:Template:Infobox Writer/doc]] --&gt;
| name = Scott Flansburg
| image = ScottFlansburg.png
| alt =
| caption = Flansburg in 2011
| pseudonym = The Human Calculator
| birth_date = {{Birth date and age|1963|12|28|mf=y}}
| birth_place = [[Herkimer, New York]]
| occupation = [[Mental calculator]]
| language = English
| nationality = American
| ethnicity =
| citizenship =
| education =
| alma_mater =
| period =
| genre =
| subject =
| movement =
| notableworks = ''Math Magic''&lt;br&gt;''Math Magic for Kids''
| spouse =
| partner =
| children =
| awards = [[Guinness World Record]] 2001
| website = {{URL|http://www.scottflansburg.com/}}
| imagesize = 200px
| birthname =
| influences =
| influenced =
}}
 
'''Scott Flansburg''' (born December 28, 1963) is an American man who is often called a [[mental calculator]]. Dubbed multiple times as "The Human Calculator", he was entered into the ''[[Guinness Book of World Records]]'' for speed of [[mental calculation]]. He is the annual host and ambassador for [[World Maths Day]], and is a math educator and media personality. Flansburg has also published the books ''Math Magic'' and ''Math Magic for Your Kids''.&lt;ref name="books"/&gt;

==Biography==

===Early life===
Scott Flansburg was born on December 28, 1963, in Herkimer, New York. Scott has stated that he was nine years old when he first discovered his [[mental calculator]] abilities. He stated that he wasn't paying attention in math class and his teacher picked him to solve a math equation on the board. He said instead of going right to left in that addition sum, he went left to right and he was able to solve the question. Afterwards he would keep a running tally of his family's groceries at the store, so his father could give the cashier an exact check before the bill had been rung up.&lt;ref name="kiwi"/&gt; In his youth he also began noticing that the shape and number of angles in numbers are clues to their value, and began counting from 0 to 9 on his fingers instead of 1 to 10.&lt;ref name="homes"/&gt;

===Early career===
Flansburg can add, subtract, multiply, divide, and find square and cube roots in his head almost instantly with calculator accuracy. Around 1990 he began using his ability in an entertainment and educational context.&lt;ref name="homehome"/&gt; 

In 1991, Flansburg was involved in the creation of a product called "The Human Calculator System" designed for direct-marketing sales channels which consisted of a study guide and four cassettes teaching his method. He appeared in the informercial series "Amazing Discoveries" hosting by [[Mike Levey]] in the Spring of that year to help sell his product. He is introduced as "The Human Calculator" at the beginning of the program, perhaps for the first time for national media markets.&lt;ref&gt;{{cite web |title=Amazing Discoveries Infomercial: Human Calculator |url=https://www.youtube.com/watch?v=cSB4BvLSrjU |website=YouTube |accessdate=7 June 2018}}&lt;/ref&gt;

He was dubbed "The Human Calculator" by [[Regis Philbin]] after appearing on ''[[Live with Regis and Kelly|Live with Regis and Kathy Lee]]''.&lt;ref name="kiwi"/&gt;

The ''[[Guinness Book of World Records]]'' listed him as "Fastest Human Calculator"&lt;ref name="homehome"/&gt; in 2001 and 2003,&lt;ref name="am"/&gt; after he broke the record for adding the same number to itself more times in 15 seconds than someone could do with a calculator.&lt;ref name="tirblocal"/&gt; In 1999 Flansburg invented a 13-month calendar that uses zero as a day, month, and year alternative to the [[Gregorian calendar]] that he called "The Human Calculator Calendar."&lt;ref name="am"/&gt;

In 1998 he published the book ''Math Magic for Your Kids: Hundreds of Games and Exercises from the Human Calculator to Make Math Fun and Easy''&lt;ref name="kids"/&gt; on [[Harper Paperbacks]]. A revised edition of his book ''Math Magic: How to Master Everyday Math Problems'' was published in 2004.&lt;ref name="homes"/&gt;

===As an educator===
Since about 1990&lt;ref name="homes"/&gt; Flansburg has regularly given lectures and presentations at schools.&lt;ref name="tirblocal"/&gt; He has appeared as a presenter at institutions such as [[NASA]], [[IBM]], [[The Smithsonian Institution]], the [[National Council of Teachers of Mathematics]],&lt;ref name="homehome"/&gt; and the [[Mental Calculation World Cup]]. The latter has described Flansburg as "more an auditory than a visual [mental] calculator."&lt;ref name="cup"/&gt;

According to Flansburg, one of his personal missions is to use education to elevate mathematical confidence and self-esteem in adults and children, stating "Why has it become so socially acceptable to be bad at math? If you were illiterate you wouldn’t say that on TV, but you can say that you are bad at math. We have to change the attitude." He is a proponent of students becoming comfortable with calculation methods instead of relying on table memorization.&lt;ref name="homes"/&gt; Flansburg is the annual host and ambassador for [[World Maths Day]].&lt;ref name="home"/&gt; He is also an official promoter of the American Math Challenge, a competition for students preparing for World Math Day.&lt;ref name="tirblocal"/&gt;

===Media appearances===
Flansburg has appeared on television shows such as ''[[The Oprah Winfrey Show]]'', ''[[The Ellen DeGeneres Show]]'', ''[[The Tonight Show with Jay Leno]]'', ''[[Larry King Live]]''. On April 26, 2009, while on the Japanese primetime show ''Asahi's Otona no Sonata'', he broke his own world record with 37 answers in 15 seconds.&lt;ref name="home"/&gt; He was featured as The Human Calculator in the first episode of ''[[Stan Lee's Superhumans]]'', which aired on [[History (U.S. TV channel)|The History Channel]] on August 5, 2010. Part of the episode analyzed his brain activity.&lt;ref name="tan"/&gt; An [[Functional magnetic resonance imaging|fMRI]] scan while he was doing complex calculations revealed that his brain activity in the [[Brodmann area 44]] region of the [[frontal cortex]] was absent. Instead there was activity somewhat higher from area 44 and closer to the motor cortex.&lt;ref name="electro"/&gt;

In January 2016, the TV show titled ''The Human Calculator'', hosted by Flansburg, premiered on [[H2 (TV network)|H2]].&lt;ref&gt;[http://www.history.com/shows/the-human-calculator The Human Calculator Full Episodes, Video &amp; More] HISTORY&lt;/ref&gt;

==Personal life==
Flansburg resides in [[San Diego, California]].&lt;ref name="tan"/&gt;

==Publications==
*''Math Magic for Your Kids'' (1998)&lt;ref name="kids"/&gt;
*''Math Magic'' (2004)&lt;ref name="mathmagic"/&gt;

== References ==
{{Reflist|2| refs =

&lt;ref name="electro"&gt;{{cite news
| title       = Electro Man: Episode 101
| first       =
| last        =
| url         =
| newspaper   =
| publisher   = ''[[Stan Lee's Superhumans]]''
| date        = August 5, 2011
}}&lt;/ref&gt;

&lt;ref name="kids"&gt;{{cite book
| last = Scott
| first = Flansburg
| authorlink =
| title = Math Magic for Your Kids
| publisher = [[Harper (publisher)|Harper Paperbacks]]
| series =
| year = 1998
| doi =
| isbn = 978-0-06-097731-3
}}&lt;/ref&gt;

&lt;ref name="mathmagic"&gt;{{cite book
| last = Flansburg
| first = Scott
| authorlink =
| title = Math Magic
| publisher = [[Harper Paperbacks]]
| series =
| year = 1993
| doi =
| isbn = 978-0-06-072635-5
}}&lt;/ref&gt;

&lt;ref name="home"&gt;{{cite news| title        =Scott Flansburg: The Human Calculator| first        =| last        =| url        =http://www.scottflansburg.com/thc/| newspaper        =| publisher        =ScottFlansburg.com| date        =| accessdate        =2011-09-09| deadurl        =yes| archiveurl        =https://web.archive.org/web/20110924133705/http://www.scottflansburg.com/thc/| archivedate        =2011-09-24| df        =}}&lt;/ref&gt;

&lt;ref name="tan"&gt;{{cite news
| title       = Featured Superhumans: The Human Calculator
| first       =
| last        =
| url         = http://www.history.com/shows/stan-lees-superhumans/bios/the-human-calculator
| newspaper   =
| publisher   = [[History.com]]
| date        =
| accessdate  = 2011-09-09
}}&lt;/ref&gt;

&lt;ref name="kiwi"&gt;{{cite news
| title       = 'Human calculator' looking for Kiwi mathletes
| first       = Jacqueline
| last        = Smith
| url         = http://www.nzherald.co.nz/nz/news/article.cfm?c_id=1&amp;objectid=10591238
| newspaper   = [[New Zealand Herald]]
| publisher   =
| date        = August 17, 2009
| accessdate  = 2011-09-09
}}&lt;/ref&gt;

&lt;ref name="homehome"&gt;{{cite news
| title       = Meet Scott Flansburg
| first       =
| last        =
| url         = http://scottflansburg.com/thc/meet/
| newspaper   =
| publisher   = ScottFlansburg.com
| date        =
| accessdate  = 2011-09-09
}}&lt;/ref&gt;

&lt;ref name="books"&gt;{{cite news
| title       = Scott Flansburg
| first       =
| last        =
| url         = http://www.harpercollins.com/authors/10924/Scott_Flansburg/index.aspx
| newspaper   =
| publisher   = [[HarperCollins Publishers]]
| date        =
| accessdate  = 2011-09-09
}}&lt;/ref&gt;

&lt;ref name="am"&gt;{{cite news
| title       = Guests: Scott Flansburg
| first       = George
| last        = Noory
| url         = http://www.coasttocoastam.com/guest/flansburg-scott/5903
| newspaper   = [[Coast to Coast AM]]
| publisher   =
| date        = June 16, 2002
| accessdate  = 2011-09-09
}}&lt;/ref&gt;

&lt;ref name="homes"&gt;{{cite news
| title       = Scott Flansburg: The Math King
| first       =
| last        =
| url         = http://www.childrensliteraturenetwork.org/blog/wontok/?p=627
| newspaper   =
| publisher   = Children's Literature Network
| date        = March 21, 2011
| accessdate  = 2011-09-09
}}&lt;/ref&gt;

&lt;ref name="tirblocal"&gt;{{cite news
| title       = Woodlands Academy Mesmerized By The Human Calculator
| first       = Angela
| last        = Reiter
| url         = http://triblocal.com/lake-bluff/community/stories/2010/10/woodlands-academy-mesmerized-by-the-human-calculator/
| newspaper   = Trib Local
| publisher   =
| date        = October 22, 2010
| accessdate  = 2011-09-09
}}&lt;/ref&gt;

&lt;ref name="cup"&gt;{{cite news
| title       = Mental Calculation World Cup 2010
| first       = Mr.
| last        = Brain
| url         = http://memory-sports.com/2010/07/12/mental-calculation-world-cup-2010-%E2%80%93-day-3/
| newspaper   =
| publisher   = [[Mental Calculation World Cup]]
| date        = July 12, 2010
| accessdate  = 2011-09-09
}}&lt;/ref&gt;

}}

==External links==
*{{Official website|http://www.scottflansburg.com/}}

{{Authority control}}

{{DEFAULTSORT:Flansburg, Scott}}
[[Category:Living people]]
[[Category:Mental calculators]]
[[Category:Giftedness]]
[[Category:People from Herkimer, New York]]
[[Category:1963 births]]</text>
      <sha1>8p1iue22j8f78h3r65ix623kqypvn2m</sha1>
    </revision>
  </page>
  <page>
    <title>Significance arithmetic</title>
    <ns>0</ns>
    <id>1237823</id>
    <revision>
      <id>826418218</id>
      <parentid>822560113</parentid>
      <timestamp>2018-02-19T00:28:52Z</timestamp>
      <contributor>
        <username>Softtest123</username>
        <id>3551859</id>
      </contributor>
      <comment>Add internal reference to alternative methods of error reduction/avoidance.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15036">'''Significance arithmetic''' is a set of rules (sometimes called '''significant figure rules''') for approximating the propagation of uncertainty in scientific or statistical calculations. These rules can be used to find the appropriate number of [[significant figures]] to use to represent the result of a calculation.  If a calculation is done without analysis of the uncertainty involved, a result that is written with too many significant figures can be taken to imply a higher [[Arithmetic precision|precision]] than is known, and a result that is written with too few significant figures results in an avoidable loss of precision.  Understanding these rules requires a good understanding of the concept of [[significant figures|significant and insignificant figures]].

The rules of significance arithmetic are an approximation based on statistical rules for dealing with probability distributions.  See the article on [[propagation of uncertainty]] for these more advanced and precise rules.  Significance arithmetic rules rely on the assumption that the number of significant figures in the [[operand]]s gives accurate information about the uncertainty of the operands and hence the uncertainty of the result. For an alternatives see [[interval arithmetic]] and [[floating point error mitigation]].

An important caveat is that significant figures apply only to ''measured'' values. Values known to be exact should be ignored for determining the number of significant figures that belong in the result. Examples of such values include:
* [[integer]] counts (e.g., the number of oranges in a bag)
* definitions of one unit in terms of another (e.g. a minute is 60 seconds)
* actual prices asked or offered, and quantities given in requirement specifications
* legally defined conversions, such as international currency exchange
* scalar operations, such as "tripling" or "halving"
* mathematical constants, such as [[π]] and [[e (mathematical constant)|e]]
Physical constants such as [[Avogadro's number]], however, have a limited number of significant digits, because these constants are known to us only by measurement. On the other hand, c ([[speed of light]]) is exactly 299,792,458&amp;nbsp;m/s by definition.

==Multiplication and division using significance arithmetic==
When multiplying or dividing numbers, the result is [[rounding|rounded]] to the ''number'' of significant figures in the factor with the least significant figures. Here, the ''quantity'' of significant figures in each of the factors is important—not the ''position'' of the significant figures.  For instance, using significance arithmetic rules:

*8 × 8 ≈ 6 &amp;times; 10&lt;sup&gt;1&lt;/sup&gt;
*8 × 8.0 ≈ 6 &amp;times; 10&lt;sup&gt;1&lt;/sup&gt;
*8.0 × 8.0 ≈ 64
*8.02 × 8.02 ≈ 64.3
*8 / 2.0 ≈ 4
*8.6 /2.0012 ≈ 4.3
*2 × 0.8 ≈ 2

If, in the above, the numbers are assumed to be measurements (and therefore probably inexact) then "8" above represents an inexact measurement with only one significant digit.  Therefore, the result of "8 × 8" is rounded to a result with only one significant digit, i.e., "6 &amp;times; 10&lt;sup&gt;1&lt;/sup&gt;" instead of the unrounded "64" that one might expect.  In many cases, the rounded result is less accurate than the non-rounded result; a measurement of "8" has an actual underlying quantity between 7.5 and 8.5. The true square would be in the range between 56.25 and 72.25. So 6 &amp;times; 10&lt;sup&gt;1&lt;/sup&gt; is the best one can give, as other possible answers give a false sense of accuracy.  Further, the 6 &amp;times; 10&lt;sup&gt;1&lt;/sup&gt; is itself confusing (as it might be considered to imply 60 ±5, which is over-optimistic; more accurate would be 64 ±8).

==Addition and subtraction using significance arithmetic==
When adding or subtracting using significant figures rules, results are rounded to the ''position'' of the least significant digit in the most uncertain of the numbers being summed (or subtracted). That is, the result is rounded to the last digit that is significant in ''each'' of the numbers being summed. Here the ''position'' of the significant figures is important, but the ''quantity'' of significant figures is irrelevant.  Some examples using these rules:

{|cellspacing=0 cellpadding=2px
| || || ||1
|-
|style="border-bottom: 1px solid black;"|+ ||style="border-bottom: 1px solid black;"| ||style="border-bottom: 1px solid black;"| ||style="border-bottom: 1px solid black;"|1.1
|-
| || || ||2
|}
*
* 1 is significant to the ones place, 1.1 is significant to the tenths place.  Of the two, the least precise is the ones place.  The answer cannot have any significant figures past the ones place.

{|cellspacing=0 cellpadding=2px
| || || ||1.0
|-
|style="border-bottom: 1px solid black;"|+ ||style="border-bottom: 1px solid black;"| ||style="border-bottom: 1px solid black;"| ||style="border-bottom: 1px solid black;"|1.1
|-
| || || ||2.1
|}

*
* 1.0 and 1.1 are significant to the tenths place, so the answer will also have a number in the tenths place.
100 + 110 ≈ 200
* We see the answer is 200, given the significance to the hundredths place of the 100.  The answer maintains a single digits of significance in the hundreds place, just like the first term in the arithmetic.
100. + 110. = 210.
* 100. and 110. are both significant to the ones place (as indicated by the decimal), so the answer is also significant to the ones place.
 1×10&lt;sup&gt;2&lt;/sup&gt; + 1.1×10&lt;sup&gt;2&lt;/sup&gt; ≈ 2×10&lt;sup&gt;2&lt;/sup&gt;
* 100 is significant up to the hundreds place, while 110 is up to the tens place.  Of the two, the least accurate is the hundreds place.  The answer should not have significant digits past the hundreds place.
 1.0×10&lt;sup&gt;2&lt;/sup&gt; + 111 = 2.1×10&lt;sup&gt;2&lt;/sup&gt;
* 1.0×10&lt;sup&gt;2&lt;/sup&gt; is significant up to the tens place while 111 has numbers up until the ones place.  The answer will have no significant figures past the tens place.
 123.25 + 46.0 + 86.26 ≈ 255.5
* 123.25 and 86.26 are significant until the hundredths place while 46.0 is only significant until the tenths place.  The answer will be significant up until the tenths place.
 100 - 1 ≈ 100
* We see the answer is 100, given the significance to the hundredths place of the 100.  It may seem counter-intuitive, but giving the nature of significant digits dictating precision, we can see how this follows from the standard rules.

==Transcendental functions==
[[Transcendental function]]s have a complicated method for determining the significance of the result. These include the [[logarithm]] function, the [[exponential function]] and the [[trigonometric functions]]. The significance of the result depends on the [[condition number]]. In general, the number of significant figures for the result is equal to the number of significant figures for the input minus the [[order of magnitude]] of the condition number.

The condition number of a differentiable function ''f'' at a point ''x'' is &lt;math&gt;\left|xf'(x)/f(x)\right|;&lt;/math&gt;  see [[Condition number#One variable|Condition number: One variable]] for details. Note that if a function has a zero at a point, its condition number at the point is infinite, as infinitesimal changes in the input can change the output from zero to non-zero, yielding a ratio with zero in the denominator, hence an infinite relative change. The condition number of the mostly used functions are as follows;&lt;ref&gt;http://www.cl.cam.ac.uk/~jrh13/papers/transcendentals.pdf{{full citation needed|date=January 2013}}&lt;/ref&gt; these can be used to compute significant figures for all [[elementary function]]s:

* Exponential function &lt;math&gt;e^x&lt;/math&gt;: &lt;math&gt;|x|&lt;/math&gt;
* Natural logarithm function &lt;math&gt;\ln(x)&lt;/math&gt;: &lt;math&gt;\frac{1}{|\ln(x)|}&lt;/math&gt;
* Sine function &lt;math&gt;\sin(x)&lt;/math&gt;: &lt;math&gt;|x\cot(x)|&lt;/math&gt;
* Cosine function &lt;math&gt;\cos(x)&lt;/math&gt;: &lt;math&gt;|x\tan(x)|&lt;/math&gt;
* Tangent function &lt;math&gt;\tan(x)&lt;/math&gt;: &lt;math&gt;|x(\tan(x)+\cot(x))|&lt;/math&gt;
* Inverse sine function &lt;math&gt;\arcsin(x)&lt;/math&gt;: &lt;math&gt;\left|\frac{x}{\sqrt{1-x^2}\arcsin(x)}\right|&lt;/math&gt;
* Inverse cosine function &lt;math&gt;\arccos(x)&lt;/math&gt;: &lt;math&gt;\left|\frac{x}{\sqrt{1-x^2}\arccos(x)}\right|&lt;/math&gt;
* Inverse tangent function &lt;math&gt;\arctan(x)&lt;/math&gt;: &lt;math&gt;\left|\frac{x}{(1+x^2)\arctan(x)}\right|&lt;/math&gt;

The fact that the number of significant figures for the result is equal to the number of significant figures for the input minus the logarithm of the condition number can be easily derived from first principles. Let &lt;math&gt;\hat{x}&lt;/math&gt; and &lt;math&gt;f(\hat{x})&lt;/math&gt; be the true values and let &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;f(x)&lt;/math&gt; be approximate values with errors &lt;math&gt;\delta x&lt;/math&gt; and &lt;math&gt;\delta f&lt;/math&gt; respectively. Then
we have &lt;math&gt; \hat{x} = x \pm \delta x &lt;/math&gt;, &lt;math&gt; f( \hat{x} ) = f(x) \pm \delta f &lt;/math&gt;, and 
&lt;math&gt; \pm \delta f = f(\hat{x}) - f(x) = f(x \pm \delta x) - f(x) = \frac{f(x \pm \delta x) - f(x)} { \pm \delta x } \cdot (\pm \delta x) \approx \pm  \frac{df(x)}{dx} \delta x &lt;/math&gt;

The significant figures of a number is related to the uncertain error of the number by &lt;math&gt;\delta x \approx x \cdot 10^{1-{\rm(significant ~ figures ~ of ~ x)}}&lt;/math&gt;. Substituting this into the above equation gives:
&lt;math&gt; f(x) \cdot 10^{1-{\rm(significant ~ figures ~ of ~ f(x))}} \approx \frac{df(x)}{dx} x \cdot 10^{1-{\rm(significant ~ figures ~ of ~ x)}} &lt;/math&gt;

&lt;math&gt; 1-{\rm(significant ~ figures ~ of ~ f(x))} \approx \log_{10} \left ( \frac{df(x)}{dx} \frac{x}{f(x)} \cdot 10^{1-{\rm(significant ~ figures ~ of ~ x)}} \right ) = 1-{\rm(significant ~ figures ~ of ~ x)} + \log_{10} \left ( \frac{df(x)}{dx} \frac{x}{f(x)} \right ) &lt;/math&gt;

&lt;math&gt; {\rm(significant ~ figures ~ of ~ f(x))} \approx {\rm(significant ~ figures ~ of ~ x)} - \log_{10} \left ( \frac{df(x)}{dx} \frac{x}{f(x)} \right ) &lt;/math&gt;

==Rounding rules==
Because significance arithmetic involves rounding, it is useful to understand a specific rounding rule that is often used when doing scientific calculations: the [[Rounding#Round half to even|round-to-even rule]] (also called ''banker's rounding'').  It is especially useful when dealing with large data sets.

This rule helps to eliminate the upwards skewing of data when using traditional rounding rules.  Whereas traditional rounding always rounds up when the following digit is 5, bankers sometimes round down to eliminate this upwards bias.

''See the article on [[rounding]] for more information on rounding rules and a detailed explanation of the round-to-even rule.''

==Disagreements about importance==
Significant figures are used extensively in high school and undergraduate courses as a shorthand for the precision with which a measurement is known. However, significant figures are ''not'' a perfect representation of uncertainty, and are not meant to be. Instead, they are a useful tool for avoiding expressing more information than the experimenter actually knows, and for avoiding rounding numbers in such a way as to lose precision.

For example, here are some important differences between significant figure rules and uncertainty:
* Uncertainty is not the same as a mistake.  If the outcome of a particular experiment is reported as 1.234±0.056 it does not mean the observer made a mistake;  it may be that the outcome is inherently statistical, and is best described by the expression indicating a value showing only those digits that are significant, ie the known digits plus one uncertain digit, in this case  1.23±0.06.  To describe that outcome as 1.234 would be incorrect under these circumstances, even though it expresses ''less'' uncertainty.
* Uncertainty is not the same as insignificance, and vice versa. An uncertain number may be highly significant (example: [http://www.av8n.com/physics/uncertainty.htm#sec-extracting signal averaging]). Conversely, a completely certain number may be insignificant.
* Significance is not the same as significant ''digits''. Digit-counting is not as rigorous a way to represent significance as specifying the uncertainty separately and explicitly (such as 1.234±0.056).
* Manual, algebraic [[propagation of uncertainty]]—the nominal topic of this article—is possible, but challenging. Alternative methods include the [http://www.av8n.com/physics/uncertainty.htm#sec-crank3 crank three times] method and the [[Monte Carlo method]]. Another option is [[interval arithmetic]], which can provide a strict upper bound on the uncertainty, but generally it is not a tight upper bound (i.e. it does not provide a ''best estimate'' of the uncertainty).  For most purposes, Monte Carlo is more useful than interval arithmetic {{Citation needed|date=March 2012}}. [[William Kahan|Kahan]] considers significance arithmetic to be unreliable as a form of automated error analysis.&lt;ref name=JavaHurt&gt;{{cite web|url=http://www.cs.berkeley.edu/~wkahan/JAVAhurt.pdf|title=How JAVA's Floating-Point Hurts Everyone Everywhere| author=William Kahan |date=1 March 1998|pages=37–39}}&lt;/ref&gt;

In order to explicitly express the uncertainty in any uncertain result, the uncertainty should be given separately, with an uncertainty interval, and a confidence interval.  The expression 1.23 U95 = 0.06 implies that the true (unknowable) value of the variable is expected to lie in the interval from 1.17 to 1.29 with at least 95% confidence.  If the confidence interval is not specified it has traditionally been assumed to be 95% corresponding to two standard deviations from the mean.  Confidence intervals at one standard deviation (68%) and three standard deviations (99%) are also commonly used.

==See also==
* [[Rounding]]
* [[Propagation of uncertainty]]
* [[Significant figures]]
* [[Accuracy and precision]]
* [[MANIAC III]]
* [[Loss of significance]]

==References==
{{reflist}}

==Further reading==
*{{cite journal |first1=D. B. |last1=Delury |year=1958 |title=Computations with approximate numbers |journal=The Mathematics Teacher |volume=51 |issue=7 |pages=521–30 |jstor=27955748}}
*{{cite journal |first1=E. A. |last1=Bond |year=1931 |title=Significant Digits in Computation with Approximate Numbers |journal=The Mathematics Teacher |volume=24 |issue=4 |pages=208–12 |jstor=27951340}}
* [[ASTM]] E29-06b, Standard Practice for Using Significant Digits in Test Data to Determine Conformance with Specifications

==External links==
* [http://speleotrove.com/decimal/decifaq4.html#signif The Decimal Arithmetic FAQ — Is the decimal arithmetic ‘significance’ arithmetic?]
* [http://www.av8n.com/physics/uncertainty.htm Advanced methods for handling uncertainty] and some explanations of the shortcomings of significance arithmetic and significant figures.
* [http://ostermiller.org/calc/sigfig.html Significant Figures Calculator] – Displays a number with the desired number of significant digits.
*[http://www.av8n.com/physics/uncertainty.htm Measurements and Uncertainties versus Significant Digits or Significant Figures] – Proper methods for expressing uncertainty, including a detailed discussion of the problems with any notion of significant digits.

[[Category:Numerical analysis]]
[[Category:Elementary arithmetic]]</text>
      <sha1>pg1hveiu8b0njq9jzkczumj7app7uxl</sha1>
    </revision>
  </page>
  <page>
    <title>Social genome</title>
    <ns>0</ns>
    <id>48675877</id>
    <revision>
      <id>838974784</id>
      <parentid>828631087</parentid>
      <timestamp>2018-04-30T13:36:04Z</timestamp>
      <contributor>
        <username>Cclutz2</username>
        <id>19126763</id>
      </contributor>
      <minor/>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4076">The '''social genome''' is the collection of data about members of a society that is captured in ever-larger and ever-more complex databases (e.g., government administrative data, operational data, social media data etc.).  Some have used the term [[digital footprint]] to refer to individual traces.

== History ==
There have been two distinct uses of the term.  First, the word Social Genome was used in a letter to the editor submission to Science in response to a seminal article about using big data for social science by King.&lt;ref&gt;{{Cite journal|title = Ensuring the Data-Rich Future of the Social Sciences|url = http://www.sciencemag.org/content/331/6018/719|journal = Science|date = 2011-02-11|issn = 0036-8075|pmid = 21311013|pages = 719–721|volume = 331|issue = 6018|doi = 10.1126/science.1197872|first = Gary|last = King}}&lt;/ref&gt;  The letter&lt;ref&gt;{{Cite journal|url = |title = Dealing with data: governments records|last = Kum|first = Hye-Chung|date = June 10, 2011|journal = Science|doi = 10.1126/science.332.6035.1263-a|pmid = 21659589|last2 = Ahalt|first2 = Stanley|last3 = Carsey|first3 = Thomas M.|authorlink3=Thomas M. Carsey|volume = 332|issue = 6035|page = 1263}}&lt;/ref&gt; was published, but the word social genome was edited out of the letter.  The original submission states, “A well-integrated federated data system of administrative databases updated on an ongoing basis could hold a collective representation of our society, our social genome.” Kum and others continue to use the word since 2011, with it being defined in a peer reviewed article in 2013.&lt;ref&gt;{{Cite journal|title = Privacy-by-Design: Understanding Data Access Models for Secondary Data|journal = AMIA Joint Summits on Translational Science proceedings AMIA Summit on Translational Science|date = 2013-01-01|issn = 2153-4063|pmc = 3845756|pmid = 24303251|pages = 126–130|volume = 2013|first = Hye-Chung|last = Kum|first2 = Stanley|last2 = Ahalt}}&lt;/ref&gt;  It states “Today there is a constant flow of data into, out of, and between ever-larger and ever-more complex databases about people. Together, these digital traces collectively capture our social genome, the footprints of our society.”  In 2014, a vision paper&lt;ref&gt;{{Cite journal|title = Social Genome: Putting Big Data to Work for Population Informatics|url = http://ieeexplore.ieee.org/xpl/login.jsp?tp=&amp;arnumber=6678338&amp;url=http%253A%252F%252Fieeexplore.ieee.org%252Fiel7%252F2%252F5306045%252F06678338.pdf%253Farnumber%253D6678338|journal = Computer|date = 2014-01-01|issn = 0018-9162|pages = 56–63|volume = 47|issue = 1|doi = 10.1109/MC.2013.405|first = Hye-Chung|last = Kum|first2 = A.|last2 = Krishnamurthy|first3 = A.|last3 = Machanavajjhala|first4 = S.C.|last4 = Ahalt}}&lt;/ref&gt; on [[population informatics]] was published which further elaborated on the term.

Second, separately at about the same time, a group of researchers led by the Brookings Institution started the [http://www.brookings.edu/about/centers/ccf/social-genome-project Social Genome Project] which built a data-rich model to map the pathway to the Middle class by tracing the life course from birth until middle age.  The first paper&lt;ref&gt;{{Cite web|title = Pathways to the Middle Class: Balancing Personal and Public Responsibilities|url = http://www.brookings.edu/research/papers/2012/09/20-pathways-middle-class-sawhill-winship|website = The Brookings Institution|date = 2012-09-20|accessdate = 2015-11-28}}&lt;/ref&gt; was published in 2012.

== See also ==
* [[Population informatics]]
* [[Sociogenomics]]
* [[Predictive analytics]]
* [[Social media analytics]]
* [[Computational sociology]]
* [[Comparative genomics]]
* [[Sociology of the Internet]]
* [[Web science]]
* [[Big data]]

== References ==
{{Reflist}}

==External links==
* [http://research.tamhsc.edu/pinformatics/social-genome/ Social Genome page at the Population Informatics Research Group]
* [http://www.brookings.edu/about/centers/ccf/social-genome-project Social Genome Project at the Brookings Institution]

[[Category:Computational science]]
[[Category:Computing and society]]</text>
      <sha1>9rjaqxzdk4d08exnzwi5yz2igjihdm3</sha1>
    </revision>
  </page>
  <page>
    <title>Straight-line program</title>
    <ns>0</ns>
    <id>45235652</id>
    <revision>
      <id>859309473</id>
      <parentid>859309365</parentid>
      <timestamp>2018-09-13T05:49:29Z</timestamp>
      <contributor>
        <username>Cedar101</username>
        <id>374440</id>
      </contributor>
      <minor/>
      <comment>/* Reachability theorem */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13561">In [[mathematics]], more specifically in [[Computer algebra system|computational algebra]], a '''straight-line program''' ('''SLP''') for a finite group ''G'' =&amp;nbsp;&amp;lang;''S''&amp;rang; is a  finite sequence ''L'' of elements of ''G'' such that every element of ''L'' either belongs to ''S'', is the inverse of a preceding element, or the product of two preceding elements. An SLP ''L'' is said to ''compute'' a group element ''g''&amp;nbsp;∈&amp;nbsp;''G'' if ''g''&amp;nbsp;∈&amp;nbsp;''L'', where ''g'' is encoded by a word in ''S'' and its inverses.

Intuitively, an SLP computing some ''g''&amp;nbsp;∈&amp;nbsp;''G'' is  an ''efficient'' way of storing ''g'' as a group word over ''S''; observe that if ''g'' is constructed in ''i'' steps, the word length of ''g'' may be exponential in ''i'', but the length of the corresponding SLP is linear in&amp;nbsp;''i''. This has important applications in [[computational group theory]], by using SLPs to efficiently encode group elements as words over a given generating set.

Straight-line programs were introduced by Babai and Szemerédi in 1984&lt;ref&gt;Babai, László, and Endre Szemerédi. "On the complexity of matrix group problems I." Foundations of Computer Science, 1984. 25th Annual Symposium on Foundations of Computer Science. IEEE, 1984&lt;/ref&gt; as a tool for studying the computational complexity of certain matrix group properties. Babai and Szemerédi prove that every element of a finite group ''G'' has an SLP of length ''O''(log&lt;sup&gt;2&lt;/sup&gt;|''G''|) in every generating set.

An efficient solution to the ''constructive membership problem'' is crucial to many group-theoretic algorithms. It can be stated in terms of SLPs as follows. Given a finite group ''G''&amp;nbsp;=&amp;nbsp;&amp;lang;''S''&amp;rang; and ''g''&amp;nbsp;∈&amp;nbsp;''G'', find a straight-line program computing ''g'' over&amp;nbsp;''S''. The constructive membership problem is often studied in the setting of [[black box group]]s. The elements are encoded by bit strings of a fixed length.  Three ''oracles'' are provided for the group-theoretic functions of multiplication, inversion, and checking for equality with the identity. A ''black box algorithm'' is one which uses only these oracles. Hence, straight-line programs for black box groups are black box algorithms.

Explicit straight-line programs are given for a wealth of finite simple groups in the online [[ATLAS of Finite Groups]].

==Definition==

===Informal Definition===
Let ''G'' be a finite group and let ''S'' be a subset of ''G''.  A  sequence ''L'' = (''g''&lt;sub&gt;1&lt;/sub&gt;,…,''g''&lt;sub&gt;''m''&lt;/sub&gt;) of elements of ''G'' is a ''straight-line program'' over ''S'' if each ''g''&lt;sub&gt;''i''&lt;/sub&gt; can be obtained by one of the following three rules: 
# ''g''&lt;sub&gt;''i''&lt;/sub&gt; ∈ ''S''
# ''g''&lt;sub&gt;''i''&lt;/sub&gt; = ''g''&lt;sub&gt;''j''&lt;/sub&gt; &lt;math&gt;\cdot&lt;/math&gt; ''g''&lt;sub&gt;''k''&lt;/sub&gt; for some ''j'',''k'' &lt; ''i'' 
# ''g''&lt;sub&gt;''i''&lt;/sub&gt; = ''g''{{su|b=''j''|p=−1}} for some ''j'' &lt; ''i''.
The straight-line ''cost'' ''c''(''g''|''S'') of an element ''g'' ∈ ''G'' is the length of a shortest straight-line program over ''S'' computing ''g''. The cost is infinite if ''g'' is not in the subgroup generated by ''S''.

A straight-line program is similar to a derivation in predicate logic. The elements of ''S'' correspond to axioms and the group operations correspond to the rules of inference.

===Formal definition===

Let ''G'' be a finite group and let ''S'' be a subset of ''G''. A ''straight-line program'' of length ''m'' over ''S'' computing some ''g'' ∈ ''G'' is a sequence of expressions (''w''&lt;sub&gt;1&lt;/sub&gt;,…,''w''&lt;sub&gt;''m''&lt;/sub&gt;) such that for each ''i'', ''w''&lt;sub&gt;''i''&lt;/sub&gt; is a symbol for some element of ''S'', or ''w''&lt;sub&gt;''i''&lt;/sub&gt; = (''w''&lt;sub&gt;''j''&lt;/sub&gt;,-1) for some ''j'' &lt; ''i'', or ''w''&lt;sub&gt;''i''&lt;/sub&gt; = (''w''&lt;sub&gt;''j''&lt;/sub&gt;,''w''&lt;sub&gt;''k''&lt;/sub&gt;) for some ''j'',''k'' &lt; ''i'', such that ''w''&lt;sub&gt;''m''&lt;/sub&gt; takes upon the value ''g'' when evaluated in ''G'' in the obvious manner.

The original definition appearing in &lt;ref name = "Seress"&gt;Ákos Seress. (2003). Permutation Group Algorithms. [Online]. Cambridge Tracts in Mathematics. (No. 152). Cambridge: Cambridge University Press.&lt;/ref&gt; requires that ''G'' =&amp;lang;''S''&amp;rang;. The definition presented above is a common generalisation of this.

From a computational perspective, the formal definition of a straight-line program has some advantages. Firstly, a sequence of abstract expressions requires less memory than terms over the generating set. Secondly, it allows straight-line programs to be constructed in one representation of ''G'' and evaluated in another. This is an important feature of some algorithms.&lt;ref name = "Seress"/&gt;

==Examples==

The [[dihedral group]]  D&lt;sub&gt;12&lt;/sub&gt; is  the group of symmetries of a hexagon. It can be generated by a 60 degree rotation ρ and one reflection λ. The leftmost column of the following is a straight-line program for λρ&lt;sup&gt;3&lt;/sup&gt;:

{{col-begin}}
{{col-break}}
# λ
# ρ
# ρ&lt;sup&gt;2&lt;/sup&gt;
# ρ&lt;sup&gt;3&lt;/sup&gt;
# λρ&lt;sup&gt;3&lt;/sup&gt;
{{col-break}}
# λ is a generator.
# ρ is a generator.
# Second rule: (2).(2)
# Second rule: (3).(2)
# Second rule: (1).(4)
{{col-end}}

In S&lt;sub&gt;6&lt;/sub&gt;, the group of permutations on six letters, we can take α=(1 2 3 4 5 6) and β=(1 2) as generators. The leftmost column here is an example of a straight-line program to compute (1 2 3)(4 5 6):

{{col-begin}}
{{col-break}}
# α
# β
# α&lt;sup&gt;2&lt;/sup&gt; 
# α&lt;sup&gt;2&lt;/sup&gt;β
# α&lt;sup&gt;2&lt;/sup&gt;βα
# α&lt;sup&gt;2&lt;/sup&gt;βαβ
# α&lt;sup&gt;2&lt;/sup&gt;βαβα&lt;sup&gt;2&lt;/sup&gt;βαβ 
{{col-break}}
# (1 2 3 4 5 6)
# (1 2)
# (1 3 5)(2 4 6)
# (1 3 5 2 4 6)
# (1 4)(2 5 3 6)
# (1 4 2 5 3 6)
# (1 2 3)(4 5 6)
{{col-break}}
# α is a generator
# β is a generator
# Second rule: (1).(1)
# Second rule: (3).(2)
# Second rule: (4).(1)
# Second rule: (5).(2)
# Second rule: (6).(6)
{{col-end}}

==Applications==

''Short descriptions of finite groups''. Straight-line programs can be used to study compression of finite groups via  [[first-order logic]]. They provide a tool to construct "short" sentences describing ''G'' (i.e. much shorter than |''G''|). In more detail, SLPs are used to prove that every finite simple group has a first-order description of length ''O''(log|''G''|), and every finite group ''G'' has a first-order description of length ''O''(log&lt;sup&gt;3&lt;/sup&gt;|''G''|).&lt;ref&gt;Nies, A., &amp; Tent, K. (2016). Describing finite groups by short first-order sentences. Israel J. Mathematics, to appear. https://arxiv.org/abs/1409.8390&lt;/ref&gt;

''Straight-line programs computing generating sets for maximal subgroups of finite simple groups''. The online ATLAS of Finite Group Representations&lt;ref&gt;http://brauer.maths.qmul.ac.uk/Atlas/v3/&lt;/ref&gt; provides abstract straight-line programs for computing generating sets of maximal subgroups for many finite simple groups.

'''Example''': The group Sz(32), belonging to the infinite family of [[Suzuki groups]], has rank 2 via generators ''a'' and ''b'', where ''a'' has order 2, ''b'' has order 4, ''ab'' has order 5, ''ab''&lt;sup&gt;2&lt;/sup&gt; has order 25 and ''abab''&lt;sup&gt;2&lt;/sup&gt;''ab''&lt;sup&gt;3&lt;/sup&gt; has order 25. The following is a straight-line program that computes a generating set for a maximal subgroup E&lt;sub&gt;32&lt;/sub&gt;&lt;math&gt;\cdot&lt;/math&gt;E&lt;sub&gt;32&lt;/sub&gt;&lt;math&gt;\rtimes&lt;/math&gt;C&lt;sub&gt;31&lt;/sub&gt;. This straight-line program can be found in the online ATLAS of Finite Group Representations.

{{col-begin}}
{{col-break}}
# ''a''
# ''b''
# ''ab''
# ''abb''
# ''ababb''
# ''ababbb''
# (''abb'')&lt;sup&gt;18&lt;/sup&gt;
# (''abb'')&lt;sup&gt;−18&lt;/sup&gt;
# (''abb'')&lt;sup&gt;−18&lt;/sup&gt;''b''
# (''abb'')&lt;sup&gt;−18&lt;/sup&gt;''b''(''abb'')&lt;sup&gt;18&lt;/sup&gt;
# (''ababb'')&lt;sup&gt;14&lt;/sup&gt;
# (''ababb'')&lt;sup&gt;−14&lt;/sup&gt;
# (''ababb'')&lt;sup&gt;−14&lt;/sup&gt;''ababbb''
# (''ababb'')&lt;sup&gt;−14&lt;/sup&gt;''ababbb''(''ababb'')&lt;sup&gt;14&lt;/sup&gt;
{{col-break}}
# ''a'' is a generator.
# ''b'' is a generator.
# Second rule: (1).(2)
# Second rule: (3).(2)
# Second rule: (3).(4)
# Second rule: (5).(2)
# Second rule iterated: (4) multiplied 18 times
# Third rule: (7) inverse
# Second rule: (8).(2)
# Second rule: (9).(7)
# Second rule iterated: (5) multiplied 14 times
# Third rule: (11) inverse
# Second rule: (12).(6)
# Second rule: (13).(11)
{{col-end}}

==Reachability theorem==
The reachability theorem states that, given a finite group ''G'' generated by ''S'', each ''g'' ∈ ''G'' has a maximum cost of {{math|(1&amp;nbsp;+ lg{{mabs|''G''}})&lt;sup&gt;2&lt;/sup&gt;}}. This can be understood as a bound on how hard it is to generate a group element from the generators.

Here the function lg(''x'') is an integer-valued version of  the logarithm function: for ''k''≥1 let lg(''k'') = max{''r'' : 2&lt;sup&gt;r&lt;/sup&gt; ≤ ''k''}.

The idea of the proof is to construct a set ''Z'' = {''z''&lt;sub&gt;1&lt;/sub&gt;,…,''z''&lt;sub&gt;''s''&lt;/sub&gt;} that will work as a new generating set (''s'' will be defined during the process). It is usually larger than ''S'', but any element of ''G'' can be expressed as a word of length at most {{math|2{{mabs|''Z''}}}} over ''Z''. The set ''Z'' is constructed by inductively defining an increasing sequence of sets ''K''(''i'').

Let ''K''(''i'') = {''z''&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;''α''&lt;sub&gt;1&lt;/sub&gt;&lt;/sup&gt;&amp;middot;''z''&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;''α''&lt;sub&gt;2&lt;/sub&gt;&lt;/sup&gt;&amp;middot;…&amp;middot;''z''&lt;sub&gt;''i''&lt;/sub&gt;&lt;sup&gt;''α''&lt;sub&gt;''i''&lt;/sub&gt;&lt;/sup&gt; : ''α''&lt;sub&gt;''j''&lt;/sub&gt; ∈ {0,1}}, where ''z''&lt;sub&gt;''i''&lt;/sub&gt; is the group element added to ''Z'' at the ''i''-th step. Let ''c''(''i'') denote the length of a shortest straight-line program that contains ''Z''(''i'') = {''z''&lt;sub&gt;1&lt;/sub&gt;,…,''z''&lt;sub&gt;''i''&lt;/sub&gt;}. Let ''K''(0) = {1&lt;sub&gt;''G''&lt;/sub&gt;} and ''c''(0)=0. We define the set ''Z'' recursively:
* If ''K''(''i'')&lt;sup&gt;−1&lt;/sup&gt;''K''(''i'') = ''G'', declare ''s'' to take upon the value ''i'' and stop.
* Else, choose some ''z''&lt;sub&gt;''i''+1&lt;/sub&gt; ∈ ''G''\''K''(''i'')&lt;sup&gt;−1&lt;/sup&gt;''K''(''i'') (which is non-empty) that minimises the "cost increase" ''c''(''i''+1) − ''c''(''i'').

By this process, ''Z'' is defined in a way so that any ''g'' ∈ ''G'' can be written as an element of ''K''(''i'')&lt;sup&gt;−1&lt;/sup&gt;''K''(''i''), effectively making it easier to generate from ''Z''.

We now need to verify the following claim to ensure that the process terminates within lg(|''G''|) many steps:

{{math theorem|name=Claim 1|1= If ''i'' &lt; ''s'' then {{math|1={{mabs|''K''(''i''+1)}} = 2{{mabs|''K''(''i'')}}}}.}}

{{Math proof|1= It is immediate that {{math|1={{mabs|''K''(''i''+1)}} ≤ 2{{mabs|''K''(''i'')}}}}. Now suppose for a contradiction that {{math|1={{mabs|''K''(''i''+1)}} &lt; 2{{mabs|''K''(''i'')}}}}. By the pigeonhole principle there are ''k''&lt;sub&gt;1&lt;/sub&gt;,''k''&lt;sub&gt;2&lt;/sub&gt; ∈ ''K''(''i''+1) with ''k''&lt;sub&gt;1&lt;/sub&gt; = ''z''&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;''α''&lt;sub&gt;1&lt;/sub&gt;&lt;/sup&gt;&amp;middot;''z''&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;''α''&lt;sub&gt;2&lt;/sub&gt;&lt;/sup&gt;&amp;middot;…&amp;middot;''z''&lt;sub&gt;''i''+1&lt;/sub&gt;&lt;sup&gt;''α''&lt;sub&gt;''i''+1&lt;/sub&gt;&lt;/sup&gt; = ''z''&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;''β''&lt;sub&gt;1&lt;/sub&gt;&lt;/sup&gt;&amp;middot;''z''&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;''β''&lt;sub&gt;2&lt;/sub&gt;&lt;/sup&gt;&amp;middot;…&amp;middot;''z''&lt;sub&gt;''i''+1&lt;/sub&gt;&lt;sup&gt;''β''&lt;sub&gt;''i''+1&lt;/sub&gt;&lt;/sup&gt; = ''k''&lt;sub&gt;2&lt;/sub&gt; for some ''α''&lt;sub&gt;''j''&lt;/sub&gt;,''β''&lt;sub&gt;''j''&lt;/sub&gt; ∈ {0,1}. Let ''r'' be the largest integer such that ''α''&lt;sub&gt;''r''&lt;/sub&gt; ≠ ''β''&lt;sub&gt;''r''&lt;/sub&gt;. Assume WLOG that ''α''&lt;sub&gt;''r''&lt;/sub&gt; = 1. It follows that ''z''&lt;sub&gt;''r''&lt;/sub&gt; = ''z''&lt;sub&gt;''p''&lt;/sub&gt;&lt;sup&gt;−''α''&lt;sub&gt;''p''&lt;/sub&gt;&lt;/sup&gt;&amp;middot;''z''&lt;sub&gt;''p''-1&lt;/sub&gt;&lt;sup&gt;−''α''&lt;sub&gt;''p''-1&lt;/sub&gt;&lt;/sup&gt;&amp;middot;…&amp;middot;''z''&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;−''α''&lt;sub&gt;1&lt;/sub&gt;&lt;/sup&gt;&amp;middot;''z''&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;''β''&lt;sub&gt;1&lt;/sub&gt;&lt;/sup&gt;&amp;middot;''z''&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;''β''&lt;sub&gt;2&lt;/sub&gt;&lt;/sup&gt;&amp;middot;…&amp;middot;''z''&lt;sub&gt;''q''&lt;/sub&gt;&lt;sup&gt;''β''&lt;sub&gt;''q''&lt;/sub&gt;&lt;/sup&gt;, with ''p'',''q'' &lt; ''r''. Hence ''z''&lt;sub&gt;''r''&lt;/sub&gt; ∈ ''K''(''r''−1)&lt;sup&gt;−1&lt;/sup&gt;''K''(''r''&amp;nbsp;−&amp;nbsp;1), a contradiction.}}

The next claim is used to show that the cost of every group element is within the required bound.

{{math theorem|name=Claim 2| {{math|1=''c''(''i'') ≤ ''i'' &lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;−&amp;nbsp;''i''}}.}}

{{Math proof|1= Since ''c''(0)=0 it suffices to show that ''c''(''i''+1) - ''c''(''i'') ≤ 2''i''. The [[Cayley graph]] of ''G'' is connected and if ''i'' &lt; ''s'', ''K''(''i'')&lt;sup&gt;−1&lt;/sup&gt;''K''(''i'') ≠ ''G'', then there is an element of the form {{math|1=''g''&lt;sub&gt;1&lt;/sub&gt;&amp;middot;''g''&lt;sub&gt;2&lt;/sub&gt; ∈ ''G''&amp;nbsp;\&amp;nbsp;''K''(''i'')&lt;sup&gt;−1&lt;/sup&gt;''K''(''i'')}} with ''g''&lt;sub&gt;1&lt;/sub&gt; ∈ ''K''(''i'')&lt;sup&gt;−1&lt;/sup&gt;''K''(''i'') and ''g''&lt;sub&gt;2&lt;/sub&gt; ∈ ''S''.}}

It takes at most 2''i'' steps to generate ''g''&lt;sub&gt;1&lt;/sub&gt; ∈ ''K''(''i'')&lt;sup&gt;−1&lt;/sup&gt;''K''(''i''). There is no point in generating the element of maximum length, since it is the identity. Hence {{math|1=2''i''&amp;nbsp;−1}} steps suffice. To generate ''g''&lt;sub&gt;1&lt;/sub&gt;&amp;middot;''g''&lt;sub&gt;2&lt;/sub&gt; ∈ ''G''\''K''(''i'')&lt;sup&gt;−1&lt;/sup&gt;''K''(''i''), 2''i'' steps  are sufficient.

We now finish the theorem. Since ''K''(''s'')&lt;sup&gt;−1&lt;/sup&gt;''K''(''s'') = ''G'', any ''g'' ∈ ''G'' can be written in the form ''k''{{su|b=1|p=−1}}&amp;middot;''k''&lt;sub&gt;2&lt;/sub&gt; with ''k''{{su|b=1|p=−1}},''k''&lt;sub&gt;2&lt;/sub&gt; ∈ ''K''(''s''). By Corollary 2, we need at most {{math|''s''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;−&amp;nbsp;''s''}} steps to generate ''Z''(''s'') = ''Z'', and no more than {{math|1=2''s''&amp;nbsp;−&amp;nbsp;1}} steps to generate ''g'' from ''Z''(''s'').

Therefore {{math|1=''c''(''g''{{!}}''S'') ≤&amp;nbsp;''s''&lt;sup&gt;2&lt;/sup&gt; + &amp;nbsp;''s''&amp;nbsp;−&amp;nbsp;1 ≤&amp;nbsp;lg&lt;sup&gt;2&lt;/sup&gt;{{mabs|''G''}} &amp;nbsp;+&amp;nbsp;lg{{mabs|''G''}}&amp;nbsp;−&amp;nbsp;1 ≤&amp;nbsp;(1&amp;nbsp;+&amp;nbsp;lg{{mabs|''G''}})&lt;sup&gt;2&lt;/sup&gt;}}.

==References==
{{reflist}}

[[Category:Algebra]]</text>
      <sha1>mdm5n25m1q2h7l1237puzlz1lpk3r7l</sha1>
    </revision>
  </page>
  <page>
    <title>Tatyana Pavlovna Ehrenfest</title>
    <ns>0</ns>
    <id>3969942</id>
    <revision>
      <id>857404089</id>
      <parentid>706859853</parentid>
      <timestamp>2018-08-31T13:32:14Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>/* References */add authority control, test</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2490">[[File:Tatyana van Aardenne Ehrenfest.jpg|thumb|van Aardenne-Ehrenfest in 1977&lt;br /&gt;Photo courtesy of [[Mathematical Research Institute of Oberwolfach|MFO]]]]
'''Tatyana Pavlovna Ehrenfest''', later '''van Aardenne-Ehrenfest''', ([[Vienna]], October 28, 1905 – [[Dordrecht]], November 29, 1984) was a [[Netherlands|Dutch]] [[mathematician]]. She was the daughter of [[Paul Ehrenfest]] (1880–1933) and [[Tatyana Afanasyeva|Tatyana Alexeyevna Afanasyeva]] (1876–1964).

Tatyana Ehrenfest was born in Vienna, and spent her childhood in [[Saint Petersburg|St Petersburg]]. In 1912 the Ehrenfests moved to [[Leiden]] where her father succeeded [[Hendrik Lorentz|H.A. Lorentz]] as professor at the [[Leiden University|University of Leiden]]. Until 1917 she was home schooled, after that she attended the [[Stedelijk Gymnasium Leiden|Gymnasium]] in Leiden and passed the [[Gymnasium (school)#Final degree|final exams]] in 1922.
She studied [[mathematics]] and [[physics]] at the University of Leiden. In 1928 she went to [[Göttingen]] where she took courses from [[Harald Bohr]] and [[Max Born]]. On December 8, 1931 she obtained her Ph.D. in Leiden.{{ref|thesis}} After that, she was never employed and, in particular, never held any academic position.{{ref|Bruijn}}

Under her married name, Tanja van Aardenne-Ehrenfest, she is known for her contributions to [[De Bruijn sequence]]s, the [[Van der Corput sequence|discrepancy theorem]]{{ref|Weisstein}} and the [[BEST theorem]].

== References ==
# {{note|thesis}} ''Oppervlakken met scharen van gesloten geodetische lijnen'', Thesis, Leiden, 1931.
# {{note|Bruijn}} [[N.G. de Bruijn]], [http://alexandria.tue.nl/repository/freearticles/597575.pdf In memoriam T. van Aardenne-Ehrenfest, 1905–1984], ''Nieuw Archief voor Wiskunde (4)'', Vol.3, (1985) 235–236.
# {{note|Weisstein}} Eric W. Weisstein. [http://mathworld.wolfram.com/DiscrepancyTheorem.html Discrepancy Theorem]. From MathWorld – A Wolfram Web Resource.


{{authority control}}

{{DEFAULTSORT:Ehrenfest, Tatyana}}
[[Category:1905 births]]
[[Category:1984 deaths]]
[[Category:20th-century Dutch mathematicians]]
[[Category:Women mathematicians]]
[[Category:Combinatorialists]]
[[Category:Dutch Jews]]
[[Category:People from Vienna]]
[[Category:People from Leiden]]
[[Category:Dutch people of Austrian descent]]
[[Category:Austrian people of Ukrainian descent]]
[[Category:Austrian people of Russian descent]]


{{Netherlands-scientist-stub}}
{{Europe-mathematician-stub}}</text>
      <sha1>nuvdo3kj1942v8iplx1e9ssi6ocpk78</sha1>
    </revision>
  </page>
  <page>
    <title>Van Wijngaarden grammar</title>
    <ns>0</ns>
    <id>1544998</id>
    <revision>
      <id>800253208</id>
      <parentid>793456001</parentid>
      <timestamp>2017-09-12T10:55:07Z</timestamp>
      <contributor>
        <username>Dewritech</username>
        <id>11498870</id>
      </contributor>
      <comment>/* ALGOL 68 examples */clean up, [[WP:AWB/T|typo(s) fixed]]: Subsequently → Subsequently, using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10783">In [[computer science]], a '''Van Wijngaarden grammar''' (also '''vW-grammar''' or '''W-grammar''') is a [[two-level grammar]] which provides a technique to define potentially infinite [[context-free grammar]]s in a finite number of rules.  The formalism was invented by [[Adriaan van Wijngaarden]]&lt;ref&gt;{{Citation | first = Adriaan | last = van Wijngaarden | url = http://www.eah-jena.de/~kleine/history/languages/VanWijngaarden-MR76.pdf | title = MR 76: Orthogonal design and description of a formal language |  place = [[Amsterdam|Amsterdam, The Netherlands]] | publisher = [[Centrum Wiskunde &amp; Informatica|CWI]] | format = PDF | year = 1965 }}.&lt;/ref&gt; to define rigorously some [[syntactic]] restrictions which previously had to be formulated in [[natural language]], despite their essentially syntactical content. Typical applications are the treatment of [[Grammatical gender|gender]] and [[Grammatical number|number]] in [[natural language]] syntax and the well-definedness of identifiers in [[programming language]]s.

The technique was used and developed in the definition of the [[programming language]] [[ALGOL 68]].  It is an example of the larger class of [[affix grammar]]s.

== Overview ==
A W-grammar consists of a finite set of [[meta]]-rules, which are used to derive (possibly infinitely many) production rules from a finite set of [[wikt:hyper-|hyper]]-rules. Meta-rules are restricted to those defined by a [[context-free grammar]]. Hyper-rules restrict the admissible contexts at the upper level. Essentially, the ''consistent substitution'' used in the derivation process is equivalent to [[unification (computing)|unification]], as in [[Prolog]], as was noted by [[Alain Colmerauer]]{{Where|date=March 2016}}.

For example, the [[Assignment (computer science)|assignment]] &lt;code&gt;x:=1&lt;/code&gt; is only valid if the variable x can contain an integer. Therefore, the context-free syntax &lt;code&gt;variable := value&lt;/code&gt; is incomplete. In a two-level grammar, this might be specified in a context-sensitive manner as &lt;code&gt;REF TYPE variable := TYPE value&lt;/code&gt;. Then &lt;code&gt;ref integer variable := integer value&lt;/code&gt; could be a production rule but &lt;code&gt;ref Boolean variable := integer value&lt;/code&gt; is not a possible production rule. This also means that assigning with incompatible types becomes a syntax error which can be caught at compile-time. Similarly, 
&lt;pre&gt;
STYLE begin token, new LAYER1 preludes, 
       parallel token, new LAYER1 tasks PACK, 
       STYLE end token
&lt;/pre&gt;
allows &lt;code&gt;begin ... end&lt;/code&gt; and &lt;code&gt;{ ... }&lt;/code&gt; but not &lt;code&gt;begin ... }&lt;/code&gt;.

== ALGOL 68 examples ==
Prior to [[ALGOL 68]] the language [[ALGOL 60]] was formalised using the context-free [[Backus–Naur form]].  The appearance of new [[Context-sensitive grammar|context-sensitive]] two-level grammar presented a challenge to some readers of the 1968 [[ALGOL 68]] "Final Report". Subsequently, the final report was revised by Wijngaarden and his colleagues and published as the 1973 ALGOL 68 "Revised Report".

The grammar for ALGOL 68 is officially in the two-level Van Wijngaarden grammar, but a subset has been done in the one-level [[Backus–Naur form]], compare:
* Van Wijngaarden grammar;&lt;ref&gt;{{Citation | url = http://www.fh-jena.de/~kleine/history/languages/Algol68-ReportAttachement.pdf | title = Languages history | contribution = Algol 68 | publisher = FH Jena | place = [[Germany|DE]] | last = Kleine | type = report attachment}}.&lt;/ref&gt;
* Backus–Naur form/[[Yacc]]&lt;ref&gt;{{Citation | url = http://wwwmathlabo.univ-poitiers.fr/~maavl/algol68/syntax68 | title = Algol 68 | contribution = Syntax | publisher = Univ Poitiers | place = [[France|FR]]}}.&lt;/ref&gt;

===ALGOL 68 as in the 1968 Final Report §2.1===
 a) program : open symbol, standard prelude,
      library prelude option, particular program, exit,
      library postlude option, standard postlude, close symbol.
 b) standard prelude : declaration prelude sequence.
 c) library prelude : declaration prelude sequence.
 d) particular program :
      label sequence option, strong CLOSED void clause.
 e) exit : go on symbol, letter e letter x letter i letter t, label symbol.
 f) library postlude : statement interlude.
 g) standard postlude : strong void clause train

===ALGOL 68 as in the 1973 Revised Report §2.2.1, §10.1.1===
 program : strong void new closed clause

 A) EXTERNAL :: standard ; library ; system ; particular.
 B) STOP :: label letter s letter t letter o letter p.

 a) program text : STYLE begin token, new LAYER1 preludes, 
        parallel token, new LAYER1 tasks PACK, 
        STYLE end token.
 b) NEST1 preludes : NEST1 standard prelude with DECS1, 
        NEST1 library prelude with DECSETY2, 
        NEST1 system prelude with DECSETY3, where (NEST1) is
        (new EMPTY new DECS1 DECSETY2 DECSETY3).
 c) NEST1 EXTERNAL prelude with DECSETY1 : 
        strong void NEST1 series with DECSETY1, go on token ; 
        where (DECSETY1) is (EMPTY), EMPTY.
 d) NEST1 tasks : NEST1 system task list, and also token, 
        NEST1 user task PACK list.
 e) NEST1 system task : strong void NEST1 unit.
 f) NEST1 user task : NEST2 particular prelude with DECS, 
        NEST2 particular program PACK, go on token, 
        NEST2 particular postlude, 
        where (NEST2) is (NEST1 new DECS STOP).
 g) NEST2 particular program : 
        NEST2 new LABSETY3 joined label definition
        of LABSETY3, strong void NEST2 new LABSETY3
        ENCLOSED clause.
 h) NEST joined label definition of LABSETY : 
        where (LABSETY) is (EMPTY), EMPTY ; 
        where (LABSETY) is (LAB1 LABSETY1), 
           NEST label definition of LAB1, 
           NEST joined label definition of$ LABSETY1. 
 i) NEST2 particular postlude :
        strong void NEST2 series with STOP.

== Implementations ==
''yo-yo''&lt;ref name = "yo-yo"&gt;{{Citation | url = http://www-users.cs.york.ac.uk/~fisher/software/yoyovwg/ | title = Software | contribution = yo-yo | last = Fisher | first = Anthony | publisher = York | place = [[United Kingdom|UK]]}}.&lt;/ref&gt; parser for van Wijngaarden grammars with example grammars for ''expressions'', ''eva'', ''sal'' and [[Pascal programming language|Pascal]] (the actual [[ISO 7185]] standard for Pascal uses [[extended Backus–Naur form]]).

== History ==
W-grammars are based on the idea of providing the nonterminal symbols of context-free grammars with ''attributes'' (or ''affixes'') that pass information between the nodes of the [[parse tree]], used to constrain the syntax and to specify the semantics.  This idea was well known at the time; e.g. [[Donald Knuth]] visited the ALGOL 68 design committee while developing his own version of it, [[attribute grammar]]s.&lt;ref&gt;{{Citation | first = Donald E | last = Knuth | url = http://www-cs-faculty.stanford.edu/~knuth/papers/gag.tex.gz | title = The genesis of attribute grammars | journal = Proceedings of the international conference on Attribute grammars and their applications | place = [[United States of America|US]] | publisher = Stanford | format = [[gZip]]ed plain text | year = 1990 | pages = 1–12}}.&lt;/ref&gt;  Quite peculiar to W-grammars was their strict treatment of attributes as strings, defined by a context-free grammar, on which concatenation is the only possible operation; in attribute grammars, attributes can be of any data type, and any kind of operation can be applied to them.

After their introduction in the Algol 68 report, W-grammars were widely considered as too powerful and unconstrained to be practical.{{citation needed|date=March 2016}}  This was partly a consequence of the way in which they had been applied; the revised Algol 68 report contained a much more readable grammar, without modifying the W-grammar formalism itself.

Meanwhile, it became clear that W-grammars are indeed too powerful.
They describe precisely all [[recursively enumerable language]]s,&lt;ref&gt;{{cite journal |first=M. |last=Sintzoff |authorlink=Michel Sintzoff |title=Existence of a van Wijngaarden syntax for every recursively enumerable set |journal=Annales de la Société scientifique de Bruxelles |year=1967 |volume=81 |pages=115–118}}&lt;/ref&gt; which makes parsing impossible in general: it is an [[undecidable problem]] to decide whether a given string can be [[String generation|generated]] by a given W-grammar. Their use must be seriously constrained when used for automatic parsing or translation.  Restricted and modified variants of W-grammars were developed to address this, e.g.
* [[Extended Affix Grammar]]s, applied to describe the grammars of [[natural language]] such as English and Spanish)
* [[Q-systems]], also applied to natural language processing
* the [[Compiler Description Language|CDL]] series of languages, applied as [[compiler construction]] languages for [[programming language]]s

== Applications outside of ALGOL 68 ==
Anthony Fisher has written a parser for a large class of W-grammars.&lt;ref name = "yo-yo" /&gt;

[[Dick Grune]] created a [[C (programming language)|C]] program that would generate all possible productions of a 2-level grammar.&lt;ref&gt;{{Citation | url = http://dickgrune.com/CS/Formal_Languages/vw_generator/ | title = A Two-Level Sentence Generator | first = Dick | last = Grune | publisher = VU | place = [[Netherlands|NL]]}}.&lt;/ref&gt;

The applications of [[Extended Affix Grammar|EAG]]s mentioned above can effectively be regarded as applications of W-grammars, since EAGs are so close to W-grammars.

W-grammars have also been proposed for the description of complex human actions in [[ergonomics]].{{Citation needed|date=August 2015}}
* A W-Grammar Description for Ada &lt;!-- &lt;ref&gt;{{Citation | url = http://www.dtic.mil/srch/doc?collection=t3&amp;id=ADA177802 | publisher = DTIC | title = A W-Grammar Description for Ada | place = US}}.&lt;/ref&gt; --&gt; &lt;ref&gt;{{Citation | url = http://www.ntis.gov/search/product.aspx?ABBR=ADA177802 | publisher = DTIC | title = ADA177802: A W-Grammar Description for Ada | place = US}}.&lt;/ref&gt; – "W-grammar description of [[Ada programming language|Ada]] is still useful to computer scientists who need more than a simple understanding of the syntax and rudimentary description of the semantics"

== See also==
*[[Affix grammar]]
*[[Extended Affix Grammar]]
*[[Attribute grammar]]

== References ==

{{reflist}}

== External links==
* {{Citation | url = http://www.bookrags.com/sciences/computerscience/algol-68-wcs.html | contribution = Use in Algol68 | title = Computer science | publisher = Book rags}}.
* {{Citation | url = http://www.cwi.nl/~steven/vw.html | title = VW | type = tutorial introduction | author-link = Steven Pemberton | first = Steven | last = Pemberton | publisher = CWI | place = NL}}.

[[Category:Formal languages]]
[[Category:Parsing]]
[[Category:Compiler construction]]
[[Category:Dutch inventions]]</text>
      <sha1>1nvsyg3ydckeshlidg3nxf0igwvs6za</sha1>
    </revision>
  </page>
  <page>
    <title>Vector calculus identities</title>
    <ns>0</ns>
    <id>3114930</id>
    <revision>
      <id>868571943</id>
      <parentid>867842039</parentid>
      <timestamp>2018-11-13T01:43:00Z</timestamp>
      <contributor>
        <username>Quondum</username>
        <id>12331483</id>
      </contributor>
      <comment>/* See also */ +[[Exterior calculus identities]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="20631">{{See also|Vector algebra relations}}
{{More footnotes|date=August 2017}}
{{Calculus |Vector}}
The following [[Identity (mathematics)|identities]] are important in [[vector calculus]]:

==Operator notations==

===Gradient===
{{main|Gradient}}

In the three-dimensional [[Cartesian coordinate system]], the gradient of some function &lt;math&gt;f(x,y,z)&lt;/math&gt; is given by:

:&lt;math&gt;\operatorname{grad}(f) = \nabla f = \frac{\partial f}{\partial x} \mathbf{i} +
\frac{\partial f}{\partial y}  \mathbf{j} +
\frac{\partial f}{\partial z} \mathbf{k}&lt;/math&gt;

where '''i''', '''j''', '''k''' are the [[standard basis|standard]] [[unit vector]]s.

The gradient of a [[tensor field]], &lt;math&gt;\mathbf{A}&lt;/math&gt;, of order ''n'', is generally written as

:&lt;math&gt;\operatorname{grad}(\mathbf{A}) = \nabla \mathbf{A} &lt;/math&gt;

and is a tensor field of order {{nowrap|''n'' + 1}}. In particular, if the tensor field has order 0 (i.e. a scalar), &lt;math&gt;\psi&lt;/math&gt;, the resulting gradient,

:&lt;math&gt;\operatorname{grad}(\psi) = \nabla \psi&lt;/math&gt;

is a [[vector field]].

===Divergence===
{{main|Divergence}}

In three-dimensional Cartesian coordinates, the divergence of a [[continuously differentiable]] [[vector field]] &lt;math&gt;\mathbf{F} = F_x\mathbf{i} + F_y\mathbf{j} + F_z\mathbf{k}&lt;/math&gt; is defined as the [[scalar (mathematics)|scalar]]-valued function:

:&lt;math&gt;\operatorname{div}\,\mathbf{F} = \nabla\cdot\mathbf{F}
 = \left(
\frac{\partial}{\partial x},
\frac{\partial}{\partial y},
\frac{\partial}{\partial z}
\right)
\cdot (F_x,F_y,F_z)
 = \frac{\partial F_x}{\partial x}
+\frac{\partial F_y}{\partial y}
+\frac{\partial F_z}{\partial z}.
&lt;/math&gt;

The divergence of a [[tensor field]], &lt;math&gt;\mathbf{A}&lt;/math&gt;, of non-zero order ''n'', is generally written as

:&lt;math&gt;\operatorname{div}(\mathbf{A}) = \nabla \cdot \mathbf{A}&lt;/math&gt;

and is a [[Tensor contraction|contraction]] to a tensor field of order {{nowrap|''n'' − 1}}. Specifically, the divergence of a vector is a scalar. The divergence of a higher order tensor field may be found by decomposing the tensor field into a sum of outer products, thereby allowing the use of the identity,

:&lt;math&gt;\nabla \cdot (\mathbf{B} \otimes \hat{\mathbf{A}}) = \hat{\mathbf{A}}(\nabla \cdot \mathbf{B})+(\mathbf{B}\cdot \nabla) \hat{\mathbf{A}}&lt;/math&gt;

where &lt;math&gt; \mathbf{B}\cdot\nabla &lt;/math&gt; is the [[directional derivative]] in the direction of &lt;math&gt; \mathbf{B} &lt;/math&gt; multiplied by its magnitude. Specifically, for the outer product of two vectors,

:&lt;math&gt;\nabla \cdot \left(\mathbf{a} \mathbf{b}^\mathrm{T}\right) = \mathbf{b}(\nabla \cdot \mathbf{a})+(\mathbf{a}\cdot \nabla) \mathbf{b} \ .&lt;/math&gt;

===Curl===
{{main|Curl (mathematics)}}

In [[Cartesian coordinate system|Cartesian coordinates]], for &lt;math&gt;\mathbf{F} = F_x\mathbf{i} + F_y\mathbf{j} + F_z\mathbf{k}&lt;/math&gt;:

&lt;math&gt; \operatorname{curl}\mathbf F=\nabla \times \mathbf{F} = \begin{vmatrix} \mathbf{i} &amp; \mathbf{j} &amp; \mathbf{k} \\
{\frac{\partial}{\partial x}} &amp; {\frac{\partial}{\partial y}} &amp; {\frac{\partial}{\partial z}} \\
  F_x &amp; F_y &amp; F_z \end{vmatrix}&lt;/math&gt;

&lt;math&gt;\nabla \times \mathbf{F} = \left(\frac{\partial F_z}{\partial y}  - \frac{\partial F_y}{\partial z}\right) \mathbf{i} + \left(\frac{\partial F_x}{\partial z} - \frac{\partial F_z}{\partial x}\right) \mathbf{j} + \left(\frac{\partial F_y}{\partial x} - \frac{\partial F_x}{\partial y}\right) \mathbf{k}&lt;/math&gt;

where '''i''', '''j''', and '''k''' are the [[unit vector]]s for the ''x''-, ''y''-, and ''z''-axes, respectively.


For a 3-dimensional vector field &lt;math&gt; \mathbf{v} &lt;/math&gt;, curl is also a 3-dimensional vector field, generally written as:

:&lt;math&gt; \nabla \times \mathbf{v}&lt;/math&gt;

or in [[Einstein notation]] as:

:&lt;math&gt; \varepsilon^{ijk} \frac {\partial v_k}{\partial x^j}&lt;/math&gt;

where &amp;epsilon; is the [[Levi-Civita symbol]].

===Laplacian===
{{main|Laplace operator}}

In '''[[Cartesian coordinates]]''', the Laplacian of a function &lt;math&gt;f(x,y,z)&lt;/math&gt; is

:&lt;math&gt;
\Delta f = \nabla^2 f = (\nabla \cdot \nabla) f = \frac{\partial^2 f}{\partial x^2} + \frac{\partial^2 f}{\partial y^2} + \frac{\partial^2 f}{\partial z^2}.
&lt;/math&gt;

For a [[tensor field]], &lt;math&gt; \mathbf{A} &lt;/math&gt;, the laplacian is generally written as:

:&lt;math&gt;\Delta\mathbf{A} = \nabla^2 \mathbf{A} = (\nabla \cdot \nabla) \mathbf{A}&lt;/math&gt;

and is a tensor field of the same order.

===Special notations===
In ''Feynman subscript notation'',
: &lt;math&gt; \nabla_\mathbf{B} \left( \mathbf{A \cdot B} \right)\ =\ \mathbf{A} \times \left( \nabla \times \mathbf{B} \right) \ +\ \left( \mathbf{A} \cdot \nabla \right) \mathbf{B} &lt;/math&gt;

where the notation ∇&lt;sub&gt;'''B'''&lt;/sub&gt;  means the subscripted gradient operates on only the factor '''B'''.&lt;ref name=Feynman&gt;{{cite book |first1=R. P. |last1=Feynman |first2=R. B. |last2=Leighton |first3=M. |last3=Sands |title=The Feynman Lectures on Physics |publisher = Addison-Wesley |year=1964 |isbn=0-8053-9049-9 |nopp= true |pages= Vol II, p. 27–4}}&lt;/ref&gt;&lt;ref name=Missevitch&gt;{{cite arXiv |eprint=physics/0504223 |first1=A. L. |last1=Kholmetskii |first2=O. V. |last2=Missevitch |title=The Faraday induction law in relativity theory |year=2005 |url=https://arxiv.org/ftp/physics/papers/0504/0504223.pdf |page=4 }}&lt;/ref&gt;

A less general but similar idea is used in ''[[geometric algebra]]'' where the so-called Hestenes ''overdot notation'' is employed.&lt;ref name=Doran&gt;{{cite book |first1=C. |last1=Doran |authorlink1=Chris J. L. Doran |first2=A. |last2=Lasenby |title=Geometric algebra for physicists |year=2003 |publisher=Cambridge University Press |page=169 |isbn=978-0-521-71595-9}}&lt;/ref&gt; The above identity is then expressed as:

: &lt;math&gt; \dot{\nabla} \left( \mathbf{A} \cdot \dot{\mathbf{B}} \right)\ =\ \mathbf{A} \times \left( \nabla \times \mathbf{B} \right)\ +\ \left( \mathbf{A} \cdot \nabla \right) \mathbf{B} &lt;/math&gt;

where overdots define the scope of the vector derivative. The dotted vector, in this case '''B''', is differentiated, while the (undotted) '''A''' is held constant.

For the remainder of this article, Feynman subscript notation will be used where appropriate.

==Properties==
For scalar fields &lt;math&gt;\psi&lt;/math&gt; and &lt;math&gt;\phi&lt;/math&gt;, vector fields &lt;math&gt;\mathbf{A}&lt;/math&gt; and &lt;math&gt;\mathbf{B}&lt;/math&gt;, and cartesian functions &lt;math&gt;f&lt;/math&gt; and &lt;math&gt;g&lt;/math&gt;:

===Distributive properties===

:&lt;math&gt; \nabla ( \psi + \phi ) = \nabla \psi + \nabla \phi &lt;/math&gt;

:&lt;math&gt; \nabla \cdot ( \mathbf{A} + \mathbf{B} ) = \nabla \cdot \mathbf{A} + \nabla \cdot \mathbf{B} &lt;/math&gt;

:&lt;math&gt; \nabla \times ( \mathbf{A} + \mathbf{B} ) = \nabla \times \mathbf{A} + \nabla \times \mathbf{B} &lt;/math&gt;

===Product rule for the gradient===
The gradient of the product of two scalar fields  &lt;math&gt;\psi&lt;/math&gt; and &lt;math&gt;\phi&lt;/math&gt; follows the same form as the [[product rule]] in single variable [[calculus]].
: &lt;math&gt; \nabla (\psi \, \phi) = \phi \,\nabla \psi  + \psi \,\nabla \phi &lt;/math&gt;

===Product of a scalar and a vector===

:&lt;math&gt; \nabla ( \psi \mathbf{A} ) = \nabla \psi \otimes \mathbf{A} \ + \  \psi \ \nabla \mathbf{A}\ &lt;/math&gt;    (gradient of the tensor field &lt;math&gt;  \psi \mathbf{A} &lt;/math&gt; of order 1)

:&lt;math&gt; \nabla \cdot ( \psi \mathbf{A} ) = \psi\ ( \nabla \cdot \mathbf{A}) \ +\  \mathbf{A}\cdot (\nabla \psi) &lt;/math&gt;

:&lt;math&gt; \nabla \times ( \psi \mathbf{A} ) = \psi\ ( \nabla \times \mathbf{A}) \ +\ ( \nabla \psi ) \times \mathbf{A} &lt;/math&gt;

===Quotient rule===
:&lt;math&gt; \nabla\left(\frac{f}{g}\right) = \frac{g\nabla f - (\nabla g)f}{g^2}&lt;/math&gt;
:&lt;math&gt; \nabla \cdot \left(\frac{\mathbf{A}}{g}\right) = \frac{g \nabla \cdot \mathbf{A} - (\nabla g) \cdot \mathbf{A}}{g^2}&lt;/math&gt;
:&lt;math&gt; \nabla \times \left(\frac{\mathbf{A}}{g}\right) = \frac{g \nabla \times \mathbf{A} - (\nabla g) \times \mathbf{A}}{g^2}&lt;/math&gt;

===Chain rule===
:&lt;math&gt;\nabla(f \circ g) = (f' \circ g) \nabla g&lt;/math&gt;
:&lt;math&gt;\nabla(f \circ \mathbf{A}) = (\nabla f \circ \mathbf{A}) \nabla \mathbf{A}&lt;/math&gt;
:&lt;math&gt;\nabla \cdot (\mathbf{A} \circ f) = (\mathbf{A}' \circ f) \cdot \nabla f&lt;/math&gt;
:&lt;math&gt;\nabla \times (\mathbf{A} \circ f) = -(\mathbf{A}' \circ f) \times \nabla f&lt;/math&gt;

===Vector dot product===

:&lt;math&gt;\begin{align}
\nabla(\mathbf{A} \cdot \mathbf{B})
&amp;= (\mathbf{A} \cdot \nabla)\mathbf{B} + (\mathbf{B} \cdot \nabla)\mathbf{A} + \mathbf{A} \times (\nabla \times \mathbf{B}) + \mathbf{B} \times (\nabla \times \mathbf{A}) \\
&amp;= \mathbf{J}^\mathrm{T}_\mathbf{A} \mathbf{B} + \mathbf{J}^\mathrm{T}_\mathbf{B} \mathbf{A} \\
&amp;= \nabla\mathbf{A} \cdot \mathbf{B} + \nabla\mathbf{B} \cdot \mathbf{A} \ .
\end{align}&lt;/math&gt;

where {{math|'''J'''&lt;sub&gt;'''A'''&lt;/sub&gt;}} denotes the [[Jacobian matrix and determinant|Jacobian]] of {{math|'''A'''}}.  For more details, refer to these notes &lt;ref&gt;{{cite book |last1=Kelly |first1=P. |year=2013 |title=Mechanics Lecture Notes Part III: Foundations of Continuum Mechanics |url=http://homepages.engineering.auckland.ac.nz/~pkel015/SolidMechanicsBooks/Part_III/ |publisher=University of Auckland |chapter=Chapter 1.14 Tensor Calculus 1: Tensor Fields |chapter-url=http://homepages.engineering.auckland.ac.nz/~pkel015/SolidMechanicsBooks/Part_III/Chapter_1_Vectors_Tensors/Vectors_Tensors_14_Tensor_Calculus.pdf |accessdate=7 December 2017}}&lt;/ref&gt;

As a special case, when {{math|'''A''' {{=}} '''B'''}},
:&lt;math&gt;\begin{align}
\frac{1}{2} \nabla \left( \mathbf{A}\cdot\mathbf{A} \right)
&amp;= (\mathbf{A} \cdot \nabla) \mathbf{A} + \mathbf{A} \times (\nabla \times \mathbf{A}) \\
&amp;= \mathbf{J}^\mathrm{T}_\mathbf{A} \mathbf A \\
&amp;= \nabla \mathbf{A} \cdot  \mathbf{A} \ .
\end{align}&lt;/math&gt;

===Vector cross product===

:&lt;math&gt; \nabla \cdot (\mathbf{A} \times \mathbf{B})\ =\ (\nabla \times \mathbf{A}) \cdot \mathbf{B} - \mathbf{A} \cdot (\nabla \times \mathbf{B}) &lt;/math&gt;

:&lt;math&gt; \begin{align}\nabla \times (\mathbf{A} \times \mathbf{B}) &amp;\ =\ \mathbf{A}\ (\nabla \cdot \mathbf{B}) - \mathbf{B}\ (\nabla \cdot \mathbf{A}) + (\mathbf{B} \cdot \nabla) \mathbf{A} - (\mathbf{A} \cdot \nabla) \mathbf{B} \\
&amp;\ =\ (\nabla \cdot \mathbf{B}  + \mathbf{B} \cdot \nabla)\mathbf{A} -(\nabla \cdot \mathbf{A} + \mathbf{A} \cdot \nabla )\mathbf{B} \\
&amp;\ =\ \nabla \cdot (\mathbf{B} \mathbf{A}^\mathrm{T}) - \nabla \cdot (\mathbf{A} \mathbf{B}^\mathrm{T})  \\
&amp;\ =\ \nabla \cdot (\mathbf{B} \mathbf{A}^\mathrm{T} - \mathbf{A} \mathbf{B}^\mathrm{T}) \end{align} &lt;/math&gt;

==Second derivatives==

===Curl of the gradient===

The [[Curl (mathematics)|curl]] of the [[gradient]] of ''any'' continuously twice-differentiable [[scalar field]] &lt;math&gt;\ \phi &lt;/math&gt; is always the [[zero vector]]:

:&lt;math&gt;\nabla \times ( \nabla \phi )  = \mathbf{0}&lt;/math&gt;

===Divergence of the curl===
The [[divergence]] of the curl of ''any'' [[vector field]] '''A''' is always zero:
:&lt;math&gt;\nabla \cdot ( \nabla \times \mathbf{A} ) = 0 &lt;/math&gt;

===Divergence of the gradient===
The [[Laplacian]] of a scalar field is the divergence of its gradient:
:&lt;math&gt; \nabla^2 \psi = \nabla \cdot (\nabla \psi) &lt;/math&gt;
The result is a scalar quantity.

===Curl of the curl===
:&lt;math&gt; \nabla \times \left( \nabla \times \mathbf{A} \right) = \nabla(\nabla \cdot \mathbf{A}) - \nabla^{2}\mathbf{A}&lt;/math&gt;
Here,∇&lt;sup&gt;2&lt;/sup&gt; is the [[vector Laplacian]] operating on the vector field '''A'''.

==Summary of important identities==

===Addition and multiplication===
*&lt;math&gt; \mathbf{A}+\mathbf{B}=\mathbf{B}+\mathbf{A} &lt;/math&gt;
*&lt;math&gt; \mathbf{A}\cdot\mathbf{B}=\mathbf{B}\cdot\mathbf{A} &lt;/math&gt;
*&lt;math&gt; \mathbf{A}\times\mathbf{B}=\mathbf{-B}\times\mathbf{A} &lt;/math&gt;
*&lt;math&gt; \left(\mathbf{A}+\mathbf{B}\right)\cdot\mathbf{C}
  =\mathbf{A}\cdot\mathbf{C}+\mathbf{B}\cdot\mathbf{C} &lt;/math&gt;
*&lt;math&gt; \left(\mathbf{A}+\mathbf{B}\right)\times\mathbf{C}
  =\mathbf{A}\times\mathbf{C}+\mathbf{B}\times\mathbf{C} &lt;/math&gt;
*&lt;math&gt; \mathbf{A}\cdot\left(\mathbf{B}\times\mathbf{C}\right)
  =\mathbf{B}\cdot\left(\mathbf{C}\times\mathbf{A}\right)
  =\mathbf{C}\cdot\left(\mathbf{A}\times\mathbf{B}\right)&lt;/math&gt; ([[scalar triple product]])
*&lt;math&gt; \mathbf{A}\times\left(\mathbf{B}\times\mathbf{C}\right)
  =\left(\mathbf{A}\cdot\mathbf{C}\right)\mathbf{B}-\left(\mathbf{A}\cdot\mathbf{B}\right)\mathbf{C} &lt;/math&gt; ([[vector triple product]])
*&lt;math&gt; \left(\mathbf{A}\times\mathbf{B}\right)\times\mathbf{C}
  =\left(\mathbf{A}\cdot\mathbf{C}\right)\mathbf{B}-\left(\mathbf{B}\cdot\mathbf{C}\right)\mathbf{A} &lt;/math&gt; ([[vector triple product]])
*&lt;math&gt; \mathbf{A}\times\left(\mathbf{B}\times\mathbf{C}\right)
  =\left(\mathbf{A}\times\mathbf{B}\right)\times\mathbf{C}\ +\ \mathbf{B}\times\left(\mathbf{A}\times\mathbf{C}\right) &lt;/math&gt; ([[Jacobi identity]])
*&lt;math&gt; \mathbf{A}\times\left(\mathbf{B}\times\mathbf{C}\right)\ +\ \mathbf{C}\times\left(\mathbf{A}\times\mathbf{B}\right)\ +\ \mathbf{B}\times\left(\mathbf{C}\times\mathbf{A}\right)
  = 0 &lt;/math&gt;  ([[Jacobi identity]])
*&lt;math&gt; \left(\mathbf{A}\times\mathbf{B}\right)\cdot\left(\mathbf{C}\times\mathbf{D}\right)=\left(\mathbf{A}\cdot\mathbf{C}\right)\left(\mathbf{B}\cdot\mathbf{D}\right)-\left(\mathbf{B}\cdot\mathbf{C}\right)\left(\mathbf{A}\cdot\mathbf{D}\right) &lt;/math&gt;	 
*&lt;math&gt;
\left(\mathbf{A}\cdot\left(\mathbf{B}\times\mathbf{C}\right)\right)\mathbf{D}=\left(\mathbf{A}\cdot\mathbf{D}\right)\left(\mathbf{B}\times\mathbf{C}\right)+\left(\mathbf{B}\cdot\mathbf{D}\right)\left(\mathbf{C}\times\mathbf{A}\right)+\left(\mathbf{C}\cdot\mathbf{D}\right)\left(\mathbf{A}\times\mathbf{B}\right) &lt;/math&gt;
*&lt;math&gt;
\left(\mathbf{A}\times\mathbf{B}\right)\times\left(\mathbf{C}\times\mathbf{D}\right)
=\left(\mathbf{A}\cdot\left(\mathbf{B}\times\mathbf{D}\right)\right)\mathbf{C}-\left(\mathbf{A}\cdot\left(\mathbf{B}\times\mathbf{C}\right)\right)\mathbf{D}&lt;/math&gt;

===Differentiation===

====Gradient====
*&lt;math&gt; \nabla(\psi+\phi)=\nabla\psi+\nabla\phi &lt;/math&gt;
*&lt;math&gt; \nabla (\psi \, \phi) = \phi \,\nabla \psi  + \psi \,\nabla \phi &lt;/math&gt;
*&lt;math&gt; \nabla ( \psi \mathbf{A} ) = \nabla \psi \otimes \mathbf{A} \ + \  \psi \ \nabla \mathbf{A}\ &lt;/math&gt;
*&lt;math&gt; \nabla\left(\mathbf{A}\cdot\mathbf{B}\right)=\left(\mathbf{A}\cdot\nabla\right)\mathbf{B}+\left(\mathbf{B}\cdot\nabla\right)\mathbf{A}+\mathbf{A}\times\left(\nabla\times\mathbf{B}\right)+\mathbf{B}\times\left(\nabla\times\mathbf{A}\right)=\left(\nabla\mathbf{A}\right)\cdot\mathbf{B}+\mathbf{A}\cdot\left(\nabla\mathbf{B}\right) &lt;/math&gt;

====Divergence====
*&lt;math&gt; \nabla\cdot(\mathbf{A}+\mathbf{B})\ =\ \nabla\cdot\mathbf{A}\,+\,\nabla\cdot\mathbf{B} &lt;/math&gt;
*&lt;math&gt; \nabla\cdot\left(\psi\mathbf{A}\right)\ =\ \psi\,\nabla\cdot\mathbf{A}\,+\,\mathbf{A}\cdot\nabla \psi &lt;/math&gt;
*&lt;math&gt; \nabla\cdot\left(\mathbf{A}\times\mathbf{B}\right)\ =\ \mathbf{B}\cdot (\nabla\times\mathbf{A})\,-\,\mathbf{A}\cdot(\nabla\times\mathbf{B}) &lt;/math&gt;

====Curl====
*&lt;math&gt; \nabla\times(\mathbf{A}+\mathbf{B})\ =\ \nabla\times\mathbf{A}\,+\,\nabla\times\mathbf{B} &lt;/math&gt;
*&lt;math&gt; \nabla\times\left(\psi\mathbf{A}\right)\ =\ \psi\,(\nabla\times\mathbf{A})\,+\,\nabla\psi\times\mathbf{A}&lt;/math&gt;
*&lt;math&gt; \nabla\times\left(\psi\nabla\phi\right)\ = \nabla \psi \times \nabla \phi&lt;/math&gt;
*&lt;math&gt; \nabla\times\left(\mathbf{A}\times\mathbf{B}\right)\ =\ \mathbf{A}\left(\nabla\cdot\mathbf{B}\right)\,-\,\mathbf{B}\left(\nabla\cdot\mathbf{A}\right)\,+\,\left(\mathbf{B}\cdot\nabla\right)\mathbf{A}\,-\,\left(\mathbf{A}\cdot\nabla\right)\mathbf{B} &lt;/math&gt;

====Second derivatives====
[[File:DCG chart.svg|right|thumb|300px|DCG chart:

A simple chart depicting all rules pertaining to second derivatives.
D, C, G, L and CC stand for divergence, curl, gradient, Laplacian and curl of curl, respectively.

Arrows indicate existence of second derivatives. Blue circle in the middle represents curl of curl, whereas the other two red circles(dashed) mean that DD and GG do not exist.
]]

*&lt;math&gt; \nabla\cdot(\nabla\times\mathbf{A})=0 &lt;/math&gt;
*&lt;math&gt; \nabla\times(\nabla\psi)= \mathbf{0} &lt;/math&gt;
*&lt;math&gt; \nabla\cdot(\nabla\psi)=\nabla^{2}\psi &lt;/math&gt;        ([[Laplace operator|scalar Laplacian]])
*&lt;math&gt; \nabla\left(\nabla\cdot\mathbf{A}\right)-\nabla\times\left(\nabla\times\mathbf{A}\right)=\nabla^{2}\mathbf{A} &lt;/math&gt;   ([[vector Laplacian]])
*&lt;math&gt; \nabla\cdot(\phi\nabla\psi)=\phi\nabla^{2}\psi + \nabla\phi\cdot\nabla\psi &lt;/math&gt; 
*&lt;math&gt; \psi\nabla^2\phi-\phi\nabla^2\psi= \nabla\cdot\left(\psi\nabla\phi-\phi\nabla\psi\right)&lt;/math&gt;
*&lt;math&gt; \nabla^2(\phi\psi)=\phi\nabla^2\psi+2\nabla\phi\cdot\nabla\psi+\psi\nabla^2\phi&lt;/math&gt;
*&lt;math&gt; \nabla^2(\psi\mathbf{A})=\mathbf{A}\nabla^2\psi+2(\nabla\psi\cdot\nabla)\mathbf{A}+\psi\nabla^2\mathbf{A}&lt;/math&gt;
*&lt;math&gt; \nabla^2(\mathbf{A}\cdot\mathbf{B})= \mathbf{A}\cdot\nabla^2\mathbf{B} - \mathbf{B}\cdot\nabla^2\mathbf{A} + 2\nabla\cdot((\mathbf{B}\cdot\nabla)\mathbf{A} + \mathbf{B}\times\nabla\times\mathbf{A})&lt;/math&gt; ([[Green's identities|Green's vector identity]])

====Third derivatives====
*&lt;math&gt;\nabla^{2}(\nabla\psi) = \nabla(\nabla\cdot(\nabla\psi)) = \nabla(\nabla^{2}\psi)&lt;/math&gt;
*&lt;math&gt; \nabla^{2}(\nabla\cdot\mathbf{A}) = \nabla\cdot(\nabla(\nabla\cdot\mathbf{A})) =\nabla\cdot(\nabla^{2}\mathbf{A})&lt;/math&gt;
*&lt;math&gt; \nabla^{2}(\nabla\times\mathbf{A}) = -\nabla\times(\nabla\times(\nabla\times\mathbf{A})) = \nabla\times(\nabla^{2}\mathbf{A})&lt;/math&gt;

===Integration===
Below, the curly symbol ∂ means "[[boundary (topology)|boundary of]]".

====Surface–volume integrals====
In the following surface–volume integral theorems, ''V'' denotes a three-dimensional volume with a corresponding two-dimensional [[Boundary (topology)|boundary]] ''S'' = ∂''V'' (a [[closed surface]]):

*{{oiint|intsubscpt=&lt;math&gt;\scriptstyle \partial V&lt;/math&gt;|integrand=&lt;math&gt;\mathbf{A}\cdot d\mathbf{S}=\iiint_V \left(\nabla \cdot \mathbf{A}\right)dV &lt;/math&gt;}} ([[Divergence theorem]])
*{{oiint|intsubscpt=&lt;math&gt;\scriptstyle \partial V&lt;/math&gt;|integrand=&lt;math&gt;\psi d \mathbf{S} = \iiint_V \nabla \psi\, dV&lt;/math&gt;}}
*{{oiint|intsubscpt=&lt;math&gt;\scriptstyle \partial V&lt;/math&gt;|integrand=&lt;math&gt;\left(\hat{\mathbf{n}}\times\mathbf{A}\right)dS=\iiint _{V}\left(\nabla\times\mathbf{A}\right)dV&lt;/math&gt;}}
*{{oiint|intsubscpt=&lt;math&gt;\scriptstyle \partial V&lt;/math&gt;|integrand=&lt;math&gt;\psi\left(\nabla\varphi\cdot\hat{\mathbf{n}}\right)dS = \iiint _{V}\left(\psi\nabla^{2}\varphi+\nabla\varphi\cdot\nabla\psi\right)dV&lt;/math&gt;}} ([[Green's first identity]])
*{{oiint|intsubscpt=&lt;math&gt;\scriptstyle \partial V&lt;/math&gt;|integrand=&lt;math&gt;\left[\left(\psi\nabla\varphi-\varphi\nabla\psi\right)\cdot\hat{\mathbf{n}}\right]dS=&lt;/math&gt;{{oiint|intsubscpt=&lt;math&gt;\scriptstyle \partial V&lt;/math&gt;|integrand=&lt;math&gt;\left[\psi\frac{\partial\varphi}{\partial n}-\varphi\frac{\partial\psi}{\partial n}\right]dS&lt;/math&gt;}} &lt;math&gt;\displaystyle=\iiint_{V}\left(\psi\nabla^{2}\varphi-\varphi\nabla^{2}\psi\right)dV&lt;/math&gt;}} ([[Green's second identity]])

====Curve–surface integrals====
In the following curve–surface integral theorems, ''S'' denotes a 2d open surface with a corresponding 1d boundary ''C'' = ∂''S'' (a [[closed curve]]):

*&lt;math&gt; \oint_{\partial S}\mathbf{A}\cdot d\boldsymbol{\ell}=\iint_{S}\left(\nabla\times\mathbf{A}\right)\cdot d\mathbf{S} &lt;/math&gt; {{pad|2em}} ([[Stokes' theorem]])
*&lt;math&gt; \oint_{\partial S}\psi d\boldsymbol{\ell}=\iint_{S}\left(\hat{\mathbf{n}}\times\nabla\psi\right)dS &lt;/math&gt;

Integration around a closed curve in the [[clockwise]] sense is the negative of the same line integral in the counterclockwise sense (analogous to interchanging the limits in a [[definite integral]]):

:{{intorient|
| preintegral = {{intorient|
| preintegral =
|symbol=oint
| intsubscpt = &lt;math&gt;{\scriptstyle \partial S}&lt;/math&gt;
| integrand = &lt;math&gt;\mathbf{A}\cdot{\rm d}\boldsymbol{\ell}=-&lt;/math&gt;
}}
|symbol=ointctr
| intsubscpt = &lt;math&gt;{\scriptstyle \partial S}&lt;/math&gt;
| integrand = &lt;math&gt;\mathbf{A}\cdot{\rm d}\boldsymbol{\ell}.&lt;/math&gt;
}}

==See also==
* [[Exterior calculus identities]]
* [[Exterior derivative]]
* [[Vector calculus]]
* [[Del in cylindrical and spherical coordinates]]
* [[Comparison of vector algebra and geometric algebra]]

==References==
{{reflist}}

==Further reading==
{{Refbegin}}
* {{cite book | title = Advanced Engineering Electromagnetics | first = Constantine A. | last = Balanis | isbn = 0-471-62194-3 }}
* {{cite book | first = H. M. | last = Schey | title = Div Grad Curl and all that:  An informal text on vector calculus | publisher=W. W. Norton &amp; Company | year= 1997 | isbn = 0-393-96997-5 }}
* {{cite book | first = David J. | last = Griffiths | title = Introduction to Electrodynamics | publisher=Prentice Hall|year=1999|isbn= 0-13-805326-X}}
{{Refend}}

&lt;!--List of vector identities, oldid=343957081--&gt;

[[Category:Vector calculus]]
[[Category:Mathematical identities]]
[[Category:Mathematics-related lists]]

[[bs:Spisak vektorskih identiteta]]
[[eo:Vektoraj identoj]]
[[zh:向量恆等式列表]]</text>
      <sha1>e3g6s6jnbxigci5e4roi0dkz0cut64q</sha1>
    </revision>
  </page>
  <page>
    <title>Weak value</title>
    <ns>0</ns>
    <id>12529188</id>
    <revision>
      <id>869297714</id>
      <parentid>813277820</parentid>
      <timestamp>2018-11-17T18:31:35Z</timestamp>
      <contributor>
        <username>22merlin</username>
        <id>20296737</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="16699">In [[quantum mechanics]] (and [[quantum computation|computation]]), a '''weak value''' is a quantity related to a shift of a measuring device's pointer when usually there is pre- and [[postselection]]. It should not be confused with a [[weak measurement]], which is often defined in conjunction. The weak value was first defined by [[Yakir Aharonov]], [[David Albert]] and [[Lev Vaidman]], published in Physical Review Letters 1988,&lt;ref name="Aharonov1988"&gt;{{cite journal
 | author = Yakir Aharonov | author2=David Z. Albert |author3=Lev Vaidman
 | title = How the result of a measurement of a component of the spin of a spin-1/2 particle can turn out to be 100
 | journal = Physical Review Letters
 | volume = 60
 | issue = 14
 | pages = 1351–1354
 | year = 1988
 | doi = 10.1103/PhysRevLett.60.1351
 | url = http://prl.aps.org/abstract/PRL/v60/i14/p1351_1
 | bibcode = 1988PhRvL..60.1351A
 | pmid=10038016}}&lt;/ref&gt; and is related to the [[two-state vector formalism]]. There is also to way to obtain weak values without postselection&lt;ref&gt;https://arxiv.org/abs/1805.09364&lt;/ref&gt;.

==Definition and Derivation==
There are many excellent review articles on weak values (see e.g.&lt;ref name="KofmanAshhabNori"&gt;{{cite journal
 | author = A. G. Kofman |author2=S. Ashhab |author3=F. Nori
 | title = Nonperturbative theory of weak pre- and post-selected measurements
 | journal = Physics Reports
 | volume = 520
 | issue = 1
 | pages = 43–133
 | year = 2012
 | doi = 10.1016/j.physrep.2012.07.001
 | url = http://www.sciencedirect.com/science/article/pii/S0370157312002050
|arxiv = 1109.6315 |bibcode = 2012PhR...520...43K }}&lt;/ref&gt;&lt;ref name="Tamir2013"&gt;{{cite journal
 | author = Boaz Tamir |author2=Eliahu Cohen
 | title = Introduction to Weak Measurements and Weak Values
 | journal = Quanta
 | volume = 2
 | issue = 1
 | pages = 7–17
 | year = 2013
 | doi = 10.12743/quanta.v2i1.14
 | url = http://quanta.ws/ojs/index.php/quanta/article/view/14
}}&lt;/ref&gt;&lt;ref name="Svensson2013"&gt;{{cite journal
 | author = Bengt E. Y. Svensson
 | title = Pedagogical Review of Quantum Measurement Theory with an Emphasis on Weak Measurements
 | journal = Quanta
 | volume = 2
 | issue = 1
 | pages = 18–49
 | year = 2013
 | doi = 10.12743/quanta.v2i1.12
 | url = http://quanta.ws/ojs/index.php/quanta/article/view/12
}}&lt;/ref&gt;&lt;ref name="DresselMalik2014"&gt;{{cite journal
 | author = J. Dressel |author2=M. Malik |author3=F. M. Miatto  |author4=A. N. Jordan |author5=R. W. Boyd
 | title = Colloquium: Understanding quantum weak values: Basics and applications
 | journal = Reviews of Modern Physics
 | volume = 86
 | issue = 1
 | pages = 307–316
 | year = 2014
 | doi = 10.1103/RevModPhys.86.307
 | url = http://link.aps.org/doi/10.1103/RevModPhys.86.307
|arxiv = 1305.7154 |bibcode = 2014RvMP...86..307D }}&lt;/ref&gt; ) here we briefly cover the basics.

===Definition===
We will denote the initial state of a system as &lt;math&gt;|\psi_i\rangle&lt;/math&gt;, while the final state of the system is denoted as &lt;math&gt;|\psi_f\rangle&lt;/math&gt;. We will refer to the initial and final states of the system as the pre- and post-selected quantum mechanical states. With respect to these state the ''weak value'' of the observable &lt;math&gt;A&lt;/math&gt; is defined as:

&lt;math&gt; A_w = \frac{\langle\psi_f|A|\psi_i\rangle}{\langle\psi_f|\psi_i\rangle}.&lt;/math&gt;

Notice that if &lt;math&gt;|\psi_f\rangle = |\psi_i\rangle&lt;/math&gt; then the weak value is equal to the usual [[Expectation value (quantum mechanics)|expected value]] in the initial state &lt;math&gt;\langle\psi_i|A|\psi_i\rangle&lt;/math&gt; or the final state &lt;math&gt;\langle\psi_f|A|\psi_f\rangle&lt;/math&gt;. In general the weak value quantity is a [[complex number]]. The weak value of the observable becomes large when the post-selected state, &lt;math&gt;|\psi_f\rangle&lt;/math&gt;, approaches being orthogonal to the pre-selected state, &lt;math&gt;|\psi_i\rangle&lt;/math&gt;, i.e. &lt;math&gt;\langle\psi_f|\psi_i\rangle \ll 1&lt;/math&gt;. If &lt;math&gt; A_w &lt;/math&gt; is larger than the largest eigenvalue of &lt;math&gt;A&lt;/math&gt; or smaller than the smallest eigenvalue of &lt;math&gt;A&lt;/math&gt; the weak value is said to be anomalous.

As an example consider a spin 1/2 particle.&lt;ref name="DuckStevensonSudarshan1989"&gt;{{cite journal
 | author = Duck, I. M. |author2=Stevenson, P. M. |author3=Sudarshan, E. C. G.
 | title = The sense in which a "weak measurement" of a spin- extonehalf{} particle's spin component yields a value 100
 | journal = Physical Review D
 | volume = 40
 | issue = 6
 | pages = 2112–2117
 | year = 1989
 | doi = 10.1103/PhysRevD.40.2112
 | url = http://link.aps.org/doi/10.1103/PhysRevD.40.2112
|bibcode = 1989PhRvD..40.2112D }}&lt;/ref&gt; Take &lt;math&gt;A&lt;/math&gt; to be the [[Pauli matrices|Pauli]] Z operator &lt;math&gt;A= \sigma_z &lt;/math&gt; with eigenvalues &lt;math&gt; \pm 1&lt;/math&gt;. Using the initial state

&lt;math&gt; |\psi_i\rangle= \frac{1}{\sqrt{2}}\left(\begin{array}{c}\cos\frac{\alpha}{2}+\sin\frac{\alpha}{2} \\ \cos\frac{\alpha}{2}-\sin\frac{\alpha}{2}\end{array}\right)&lt;/math&gt;

and the final state

&lt;math&gt; |\psi_f\rangle=\frac{1}{\sqrt{2}}\left(\begin{array}{c}1 \\ 1 \end{array}\right)&lt;/math&gt;

we can calculate the weak value to be

&lt;math&gt; A_w = (\sigma_z)_w = \tan\frac{\alpha}{2}&lt;/math&gt;.

For &lt;math&gt;| \alpha |&gt;\frac{\pi}{2} &lt;/math&gt; the weak value is anomalous.

===Derivation===
Here we follow the presentation given by Duck, Stevenson, and [[E. C. George Sudarshan|Sudarshan]],&lt;ref name="DuckStevensonSudarshan1989"/&gt; (with some notational updates from Kofman et al.&lt;ref name="KofmanAshhabNori"/&gt; )which makes explicit when the approximations used to derive the weak value are valid.

Consider a quantum system that you want to measure by coupling an ancillary (also quantum) measuring device. The observable to be measured on the system is &lt;math&gt; A &lt;/math&gt;. The system and ancilla are coupled via the Hamiltonian
&lt;math&gt;
\begin{align}
H = \gamma A \otimes p,
\end{align}
&lt;/math&gt; where the coupling constant is integrated over an interaction time
&lt;math&gt; \gamma = \int_{t_i}^{t_f} g(t) dt \ll 1 &lt;/math&gt; and &lt;math&gt; [q, p] =i &lt;/math&gt; is the canonical commutator. The Hamiltonian generates the unitary

&lt;math&gt;
\begin{align}
U= \exp[-i \gamma A\otimes p].
\end{align}
&lt;/math&gt;

Take the initial state of the ancilla to have a Gaussian distribution

&lt;math&gt;
\begin{align}
|\Phi\rangle = \frac{1}{(2\pi \sigma^2)^{1/4}}\int dq' \exp[-q'^2/4\sigma^2]|q'\rangle,
\end{align}
&lt;/math&gt;

the position wavefunction of this state is

&lt;math&gt;
\begin{align}
\Phi(q) =\langle q|\Phi\rangle = \frac{1}{(2\pi \sigma^2)^{1/4}} \exp[-q^2/4\sigma^2].
\end{align}
&lt;/math&gt;

The initial state of the system is given by &lt;math&gt; |\psi_i\rangle &lt;/math&gt; above; the state &lt;math&gt;|\Psi\rangle&lt;/math&gt;, jointly describing the initial state of the system and ancilla, is given then by:

&lt;math&gt;
\begin{align}
|\Psi\rangle =|\psi_i\rangle \otimes |\Phi\rangle.
\end{align}
&lt;/math&gt;

Next the system and ancilla interact via the unitary &lt;math&gt;U |\Psi\rangle&lt;/math&gt;. After this one performs a [[Projection-valued measure|projective measurement]] of the projectors &lt;math&gt;\{ |\psi_f\rangle\langle \psi_f |, I- |\psi_f\rangle\langle \psi_f |\}&lt;/math&gt; on the system. If we [[Postselection|postselect]] (or [[Conditional probability|condition]]) on getting the outcome &lt;math&gt; |\psi_f\rangle\langle \psi_f |&lt;/math&gt;, then the (unnormalized) final state of the meter is

&lt;math&gt;
\begin{align}
|\Phi_f \rangle
&amp;= \langle \psi_f |U |\psi_i\rangle \otimes |\Phi\rangle\\
&amp;\approx \langle \psi_f |(I\otimes I -i \gamma A\otimes p ) |\psi_i\rangle \otimes|\Phi\rangle \quad (I)\\
&amp;= \langle \psi_f|\psi_i\rangle (1 -i \gamma A_w p ) |\Phi\rangle\\
&amp;\approx \langle \psi_f|\psi_i\rangle \exp(-i \gamma A_w p) |\Phi\rangle. \quad (II)
\end{align}
&lt;/math&gt;

To arrive at this conclusion, we use the first order series expansion of &lt;math&gt;U&lt;/math&gt; on line (I), and we require that&lt;ref name="KofmanAshhabNori"/&gt;&lt;ref name="DuckStevensonSudarshan1989"/&gt;

&lt;math&gt;
\begin{align}
\frac{|\gamma|}{\sigma} \left|\frac{\langle \psi_f |A^n |\psi_i \rangle}{ \langle \psi_f| A |\psi_i \rangle }\right|^{1/(n-1)} \ll 1, \quad (n=2,3,...)
\end{align}
&lt;/math&gt;

On line (II) we use the approximation that &lt;math&gt;e^{-x}\approx 1-x&lt;/math&gt; for small &lt;math&gt;x&lt;/math&gt;. This final approximation is only valid when&lt;ref name="KofmanAshhabNori"/&gt;&lt;ref name="DuckStevensonSudarshan1989"/&gt;

&lt;math&gt;
\begin{align}
|\gamma A_w|/\sigma \ll 1.
\end{align}
&lt;/math&gt;

As &lt;math&gt; p &lt;/math&gt; is the generator of translations, the ancilla's wavefunction is now given by

&lt;math&gt;
\begin{align}
\Phi_f(q) = \Phi(q-\gamma A_w).
\end{align}
&lt;/math&gt;

This is the original wavefunction, shifted by an amount &lt;math&gt; \gamma A_w &lt;/math&gt;. By Busch's theorem&lt;ref name="Busch2009"&gt;{{cite book| author = Paul Busch
 |title = "No Information Without Disturbance": Quantum Limitations of Measurement
 |series=Invited contribution, "Quantum Reality, Relativistic Causality, and Closing the Epistemic Circle: An International Conference in Honour of Abner Shimony", Perimeter Institute, Waterloo, Ontario, Canada, July 18–21, 2006
 |editor1= J. Christian |editor2=W. Myrvold
 |publisher=Springer-Verlag
 |pages = 229–256
 |year = 2009
 |doi = 10.1007/978-1-4020-9107-0
 |url= https://link.springer.com/chapter/10.1007/978-1-4020-9107-0_13
 |issn= 1566-659X
 |arxiv= 0706.3526
 }}&lt;/ref&gt; the system and meter wavefunctions are necessarily disturbed by the measurement. There is a certain sense in which the protocol that allows one to measure the weak value is minimally disturbing,&lt;ref name="Ipsen"&gt;{{cite journal
 | author = Asger C. Ipsen
 | title = Disturbance in weak measurements and the difference between quantum and classical weak values
 | journal = Physical Review A
 | volume = 91
 | issue = 6
 | pages = 062120
 | year = 2015
 | doi = 10.1103/PhysRevA.91.062120
 | url = http://link.aps.org/doi/10.1103/PhysRevA.91.062120
|arxiv = 1409.3538 |bibcode = 2015PhRvA..91f2120I }}&lt;/ref&gt; but there is still disturbance.&lt;ref name="Ipsen"/&gt;

==Applications==

===Quantum metrology and Tomography===
At the end of the original weak value paper&lt;ref name="Aharonov1988"/&gt; the authors suggested weak values could be used in [[quantum metrology]]:
{{Quote box
 |quote = Another striking aspect of this experiment becomes evident when we consider it as a device for measuring
a small gradient of the magnetic field ... yields a tremendous amplification.
 |source = Aharonov, Albert, Vaidman&lt;ref name="Aharonov1988"/&gt;
|align  = center
}}

This suggestion was followed by Hosten and Kwiat&lt;ref name="Hosten2008"&gt;{{cite journal
 | author = O. Hosten |author2=P. Kwiat
 | title = Observation of the spin Hall effect of light via weak measurements
 | journal = Science
 | volume = 319
 | issue = 5864
 | pages =  787–790
 | year = 2008
 | doi = 10.1126/science.1152697
 | url = http://www.sciencemag.org/content/319/5864/787.abstract
|bibcode = 2008Sci...319..787H }}&lt;/ref&gt; and later by Dixon et al.&lt;ref name="BenDixon2009"&gt;{{cite journal
 | author = P. Ben Dixon |author2=David J. Starling |author3=Andrew N. Jordan |author4=John C. Howell
 | title = Ultrasensitive Beam Deflection Measurement via Interferometric Weak Value Amplification
 | journal = Physical Review Letters
 | volume = 102
 | issue = 17
 | pages = 173601
 | year = 2009
 | doi = 10.1103/PhysRevLett.102.173601
 | arxiv = 0906.4828
|bibcode = 2009PhRvL.102q3601D | pmid=19518781}}&lt;/ref&gt; It appears to be an interesting line of research that could result in improved quantum sensing technology.

Additionally in 2011, weak measurements of many photons prepared in the same [[Quantum state#Pure states as rays in a Hilbert space|pure state]], followed by strong measurements of a complementary variable, were used to perform [[quantum tomography]] (i.e. reconstruct the state in which the photons were prepared).&lt;ref&gt;[[Jeff S. Lundeen]], Brandon Sutherland, Aabid Patel, Corey Stewart, Charles Bamber: ''Direct measurement of the quantum wavefunction'', Nature vol.&amp;nbsp;474, pp.&amp;nbsp;188–191, 9. June 2011, {{doi|10.1038/nature10120}}&lt;/ref&gt;

===Quantum foundations===

Weak values have been used to examine some of the paradoxes in the foundations of quantum theory. For example, the research group of Aephraim Steinberg at the [[University of Toronto]] confirmed [[Hardy's paradox]] experimentally using joint weak measurement of the locations of entangled pairs of photons.&lt;ref name="Lundeen2009"&gt;{{cite journal
 | author = J. S. Lundeen |author2=A. M. Steinberg
 | title = Experimental Joint Weak Measurement on a Photon Pair as a Probe of Hardy's Paradox
 | journal = Physical Review Letters
 | volume = 102
 | issue = 2
 | pages = 020404
 | year = 2009
 | doi = 10.1103/PhysRevLett.102.020404
 | arxiv = 0810.4229
 | bibcode = 2009PhRvL.102b0404L
 | pmid=19257252}}&lt;/ref&gt;&lt;ref name="PI2009"&gt;{{cite web
 | url = https://perimeterinstitute.ca/news/hardys-paradox-confirmed-experimentally
 | title = Hardy's paradox confirmed experimentally
 | date= July 2, 2009
 | work =
 | publisher = Perimeter Institute for Theoretical Physics
 | accessdate=June 8, 2013}}&lt;/ref&gt; (also see&lt;ref&gt;{{cite journal | author = Yokota K., Yamamoto T., Koashi M., Imoto N. | year = 2009 | title = Direct observation of Hardy's paradox by joint weak measurement with an entangled photon pair | url = http://www.iop.org/EJ/abstract/1367-2630/11/3/033011/ | journal = New J. Phys. | volume = 11 | issue = | page = 033011 | arxiv = 0809.2224 | bibcode = 2009NJPh...11a3011R | doi = 10.1088/1367-2630/11/1/013011 }}&lt;/ref&gt;)

Building on weak measurements, [[Howard M. Wiseman]] proposed a weak value measurement of the velocity of a quantum particle at a precise position, which he termed its "naïvely observable velocity". In 2010, a first experimental observation of trajectories of a photon in a [[double-slit interferometer]] was reported, which displayed the qualitative features predicted in 2001 by [[Partha Ghose]]&lt;ref&gt;Partha Ghose, A.S. Majumdar, S. Guhab, J. Sau: [http://web.mit.edu/saikat/www/research/files/Bohmian-traj_PLA2001.pdf ''Bohmian trajectories for photons''], Physics Letters A 290 (2001), pp. 205–213, 10 November 2001&lt;/ref&gt; for photons in the [[De Broglie–Bohm theory|de Broglie-Bohm interpretation]].&lt;ref&gt;Sacha Kocsis, Sylvain Ravets, Boris Braverman, Krister Shalm, Aephraim M. Steinberg: Observing the trajectories of a single photon using weak measurement, 19th Australian Institute of Physics (AIP) Congress, 2010 [https://web.archive.org/web/20120324224949/http://www.aip.org.au/Congress2010/Abstracts/Monday%206%20Dec%20-%20Orals/Session_3E/Kocsis_Observing_the_Trajectories.pdf]&lt;/ref&gt;&lt;ref&gt;Sacha Kocsis, Boris Braverman, Sylvain Ravets, Martin J. Stevens, Richard P. Mirin, L. Krister Shalm, Aephraim M. Steinberg: ''Observing the Average Trajectories of Single Photons in a Two-Slit Interferometer'', Science, vol.&amp;nbsp;332 no.&amp;nbsp;6034 pp.&amp;nbsp;1170–1173, 3 June 2011, {{doi|10.1126/science.1202218}} ([http://www.sciencemag.org/content/332/6034/1170.abstract abstract])&lt;/ref&gt;

== Criticisms ==

Criticisms of weak values include philosophical and practical criticisms. Some noted researchers such as [[Asher Peres]], [[Anthony James Leggett|Tony Leggett]], [[David Mermin]], and [[Charles H. Bennett (computer scientist)|Charles H. Bennett]] are critical of weak values also:

* Stephen Parrott questions the meaning and usefulness of weak measurements, as described above.[http://www.math.umb.edu/~sp/papers.html]
* Sokolovski{{clarify|reason=What's his critisism?|date=May 2016}}&lt;ref name="Sokolovski2013"&gt;{{cite journal
 | author = Dmitri Sokolovski
 | title = Are the Weak Measurements Really Measurements?
 | journal = Quanta
 | volume = 2
 | issue = 1
 | pages = 50–57
 | year = 2013
 | doi = 10.12743/quanta.v2i1.15
 | url = http://quanta.ws/ojs/index.php/quanta/article/view/15
}}&lt;/ref&gt;

==Further reading==
* {{cite journal |author=Zeeya Merali |journal=[[Discover (magazine)|Discover]] |title=Back From the Future |date=April 2010 |url=http://discovermagazine.com/2010/apr/01-back-from-the-future |postscript=. A series of quantum experiments shows that measurements performed in the future can influence the present.}}
* {{cite journal |date=June 2, 2011 |title=Quantum physics first: Researchers observe single photons in two-slit interferometer experiment |url=http://www.physorg.com/news/2011-06-quantum-physics-photons-two-slit-interferometer.html |publisher=phys.org}}
* {{cite journal |author=Adrian Cho |title=Furtive Approach Rolls Back the Limits of Quantum Uncertainty |journal=Science |date=5 August 2011|volume=333 |number=6043 |pages=690–693 |doi=10.1126/science.333.6043.690 |pmid=21817029}}

==References==
{{Reflist}}

{{DEFAULTSORT:Weak Measurement}}
[[Category:Quantum mechanics]]
[[Category:Quantum information science]]
[[Category:Quantum measurement]]</text>
      <sha1>f4i8xxkg4dwzkuxy2uspy83bi3wir50</sha1>
    </revision>
  </page>
  <page>
    <title>Wolfgang Heinrich Johannes Fuchs</title>
    <ns>0</ns>
    <id>5397705</id>
    <revision>
      <id>857357772</id>
      <parentid>831641578</parentid>
      <timestamp>2018-08-31T05:13:11Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>Removing from [[Category:German mathematicians]] using [[c:Help:Cat-a-lot|Cat-a-lot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2211">{{Infobox scientist
|name              = Wolfgang Fuchs
|image             =  &lt;!--(filename only)--&gt;
|image_size        = 
|caption           = 
|birth_date        = {{birth date|1915|05|19}}
|birth_place       = 
|death_date        = {{death date and age|1997|02|24|1915|05|19}}
|death_place       = 
|residence         = 
|citizenship       = 
|nationality       = 
|ethnicity         = 
|fields            = 
|workplaces        = [[Cornell University]]
|alma_mater        = [[University of Cambridge]]
|doctoral_advisor  = [[Albert Ingham]]
|academic_advisors = 
|doctoral_students = [[David Drasin]]
|notable_students  = 
|known_for         = 
|author_abbrev_bot = 
|author_abbrev_zoo = 
|influences        = 
|influenced        = 
|awards            = 
|religion          = 
|signature         = &lt;!--(filename only)--&gt;
|footnotes         = 
}}
'''Wolfgang Heinrich Johannes Fuchs''' (May 19, 1915, [[Munich]] – February 24, 1997) was a mathematician specializing in [[complex analysis]]. His main area of research was [[Nevanlinna theory]].

Fuchs received his [[Ph.D.]] in 1941 from the [[University of Cambridge]], under the direction of [[Albert Ingham]].&lt;ref&gt;{{ cite journal
   | last = Anderson
   | first = J. Milne |author2=Drasin, David |author3=Sons, Linda R.
   | title = Wolfgang Heinrich Johannes Fuchs (1915–1997)
   |date=December 1998
   | journal = [[Notices of the American Mathematical Society]]
   | volume = 45
   | issue = 11
   | pages =1472–1478
   | url = http://www.ams.org/notices/199811/mem-fuchs.pdf
   | format = [[PDF]]
   | accessdate = 2007-11-27 }}&lt;/ref&gt; He joined the faculty of [[Cornell University]] in 1950 and spent the rest of his career there.

==See also==
* [[Erdős–Fuchs theorem]]

==References==
&lt;references/&gt;

==External links==
* {{MathGenealogy|id=30624}}

{{Authority control}}

{{DEFAULTSORT:Fuchs, Wolfgang Heinrich Johannes}}
[[Category:20th-century German mathematicians]]
[[Category:Mathematical analysts]]
[[Category:Alumni of the University of Cambridge]]
[[Category:Cornell University faculty]]
[[Category:People from Munich]]
[[Category:1915 births]]
[[Category:1997 deaths]]
[[Category:Guggenheim Fellows]]


{{Germany-mathematician-stub}}</text>
      <sha1>5mbssusnbpdkw9lnlcitg46ixts2k2r</sha1>
    </revision>
  </page>
  <page>
    <title>Zig-zag lemma</title>
    <ns>0</ns>
    <id>19382017</id>
    <revision>
      <id>871048974</id>
      <parentid>847739368</parentid>
      <timestamp>2018-11-28T16:25:03Z</timestamp>
      <contributor>
        <ip>2A01:CB1D:1CC:3700:C066:B37F:70A6:12AC</ip>
      </contributor>
      <comment>/* Statement */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4021">In [[mathematics]], particularly [[homological algebra]], the '''zig-zag lemma''' asserts the existence of a particular [[long exact sequence]] in the [[homology group]]s of certain [[chain complex]]es.  The result is valid in every [[abelian category]].

== Statement ==
In an abelian category (such as the category of [[abelian group]]s or the category of [[vector space]]s over a given [[field (algebra)|field]]), let &lt;math&gt;(\mathcal{A},\partial_{\bullet}), (\mathcal{B},\partial_{\bullet}')&lt;/math&gt; and &lt;math&gt;(\mathcal{C},\partial_{\bullet}'')&lt;/math&gt; be chain complexes that fit into the following [[short exact sequence]]:

: &lt;math&gt;0 \longrightarrow \mathcal{A} \stackrel{\alpha}{\longrightarrow} \mathcal{B} \stackrel{\beta}{\longrightarrow} \mathcal{C}\longrightarrow 0&lt;/math&gt; 

Such a sequence is shorthand for the following [[commutative diagram]]:

[[image:complex_ses_diagram.png|commutative diagram representation of a short exact sequence of chain complexes]]

where the rows are [[exact sequence]]s and each column is a [[chain complex]]. 

The zig-zag lemma asserts that there is a collection of boundary maps

: &lt;math&gt; \delta_n : H_n(\mathcal{C}) \longrightarrow H_{n-1}(\mathcal{A}), &lt;/math&gt;

that makes the following sequence exact:

[[image:complex_les.png|long exact sequence in homology, given by the Zig-Zag Lemma]]

The maps &lt;math&gt;\alpha_*^{ }&lt;/math&gt; and &lt;math&gt;\beta_*^{ }&lt;/math&gt; are the usual maps induced by homology.  The boundary maps &lt;math&gt;\delta_n^{ }&lt;/math&gt; are explained below.  The name of the lemma arises from the "zig-zag" behavior of the maps in the sequence.  In an unfortunate overlap in terminology, this theorem is also commonly known as the "[[snake lemma]]," although there is another result in homological algebra with that name.  The other snake lemma can be used to prove the zig-zag lemma, in a manner different from what is described below.

== Construction of the boundary maps ==
The maps &lt;math&gt;\delta_n^{ }&lt;/math&gt; are defined using a standard diagram chasing argument.  Let &lt;math&gt;c \in C_n&lt;/math&gt; represent a class in &lt;math&gt;H_n(\mathcal{C})&lt;/math&gt;, so &lt;math&gt;\partial_n''(c) = 0&lt;/math&gt;.  Exactness of the row implies that &lt;math&gt;\beta_n^{ }&lt;/math&gt; is surjective, so there must be some &lt;math&gt;b \in B_n&lt;/math&gt; with &lt;math&gt;\beta_n^{ }(b) = c&lt;/math&gt;.  By commutativity of the diagram, 

:&lt;math&gt; \beta_{n-1} \partial_n' (b) = \partial_n'' \beta_n(b) = \partial_n''(c) = 0. &lt;/math&gt;

By exactness, 

:&lt;math&gt;\partial_n'(b) \in \ker \beta_{n-1} = \mathrm{im} \alpha_{n-1}.&lt;/math&gt;

Thus, since &lt;math&gt;\alpha_{n-1}^{}&lt;/math&gt; is injective, there is a unique element &lt;math&gt;a \in A_{n-1}&lt;/math&gt; such that &lt;math&gt;\alpha_{n-1}(a) = \partial_n'(b)&lt;/math&gt;.  This is a cycle, since &lt;math&gt;\alpha_{n-2}^{ }&lt;/math&gt; is injective and

:&lt;math&gt;\alpha_{n-2} \partial_{n-1}(a) = \partial_{n-1}' \alpha_{n-1}(a) = \partial_{n-1}' \partial_n'(b) = 0,&lt;/math&gt;

since &lt;math&gt;\partial^2 = 0&lt;/math&gt;.  That is, &lt;math&gt;\partial_{n-1}(a) \in \ker \alpha_{n-2} = \{0\}&lt;/math&gt;.  This means &lt;math&gt;a&lt;/math&gt; is a cycle, so it represents a class in &lt;math&gt;H_{n-1}(\mathcal{A})&lt;/math&gt;.  We can now define

:&lt;math&gt; \delta_{ }^{ }[c] = [a].&lt;/math&gt;

With the boundary maps defined, one can show that they are well-defined (that is, independent of the choices of ''c'' and ''b'').  The proof uses diagram chasing arguments similar to that above.  Such arguments are also used to show that the sequence in homology is exact at each group.

== See also ==
* [[Mayer–Vietoris sequence]]

==References==

*{{cite book | first = Allen | last = Hatcher | authorlink = Allen Hatcher | year = 2002 | title = Algebraic Topology | publisher  = Cambridge University Press | isbn = 0-521-79540-0 | url = http://www.math.cornell.edu/~hatcher/AT/ATpage.html}}
*{{Lang Algebra}}
*{{cite book | first = James R. | last = Munkres | authorlink = James Munkres | year = 1993 | title = Elements of Algebraic Topology | publisher = Westview Press | location = New York | isbn = 0-201-62728-0}}


[[Category:Homological algebra]]
[[Category:Lemmas]]</text>
      <sha1>qgxz0fnj58pi9k5qflvmji1n70g92cj</sha1>
    </revision>
  </page>
</mediawiki>
