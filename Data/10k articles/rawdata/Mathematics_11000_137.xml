<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Adjugate matrix</title>
    <ns>0</ns>
    <id>202840</id>
    <revision>
      <id>869899144</id>
      <parentid>866274711</parentid>
      <timestamp>2018-11-21T02:54:56Z</timestamp>
      <contributor>
        <ip>75.183.54.212</ip>
      </contributor>
      <comment>/* Relation to exterior algebras */  Changed some \varphis that appeared in the text to match \phi, which was used most often in the section.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="24905">In [[linear algebra]], the '''adjugate''', '''classical adjoint''', or '''adjunct''' of a [[square matrix]] is the [[transpose]] of its [[cofactor matrix]].&lt;ref&gt;{{cite book |first=F. R. |last=Gantmacher |authorlink=Felix Gantmacher |title=The Theory of Matrices |volume=1 |publisher=Chelsea |location=New York |year=1960 |isbn=0-8218-1376-5 |pages=76–89 |url=https://books.google.com/books?id=ePFtMw9v92sC&amp;pg=PA76 }}&lt;/ref&gt;

The adjugate&lt;ref&gt;{{cite book | last=Strang | first=Gilbert | authorlink=Gilbert Strang | title=Linear Algebra and its Applications | edition=3rd| year=1988 | publisher=Harcourt Brace Jovanovich | isbn=0-15-551005-3 | pages=231–232 | chapter=Section 4.4: Applications of determinants}}&lt;/ref&gt; has sometimes been called the "adjoint",&lt;ref&gt;{{cite book|ref=harv|first=Alston S.|last=Householder|title=The Theory of Matrices in Numerical Analysis |publisher=Dover Books on Mathematics|year=2006|authorlink=Alston Scott Householder | isbn=0-486-44972-6 |pages=166–168 }}&lt;/ref&gt; but today the "adjoint" of a matrix normally refers to its corresponding [[Hermitian adjoint|adjoint operator]], which is its [[conjugate transpose]].

== Definition ==
The '''adjugate''' of {{math|'''A'''}} is the [[transpose]] of the [[cofactor matrix]] {{math|'''C'''}} of {{math|'''A'''}},
:&lt;math&gt;\operatorname{adj}(\mathbf{A}) = \mathbf{C}^\mathsf{T}.&lt;/math&gt;

In more detail,  suppose {{math|''R''}} is a [[commutative ring]] and {{math|'''A'''}} is an {{math|''n'' × ''n''}} [[matrix (mathematics)|matrix]] with entries from {{math|''R''}}.  The {{math|(''i'',''j'')}}-''[[minor (linear algebra)|minor]]'' of {{math|'''A'''}}, denoted {{math|'''M'''&lt;sub&gt;''ij''&lt;/sub&gt;}}, is the [[determinant]] of the {{math|(''n''&amp;nbsp;−&amp;nbsp;1) × (''n''&amp;nbsp;−&amp;nbsp;1)}} matrix that results from deleting row {{mvar|i}} and column {{mvar|j}} of {{math|'''A'''}}.  The [[Cofactor (linear algebra)#Inverse of a matrix|cofactor matrix]] of {{math|'''A'''}} is the {{math|''n'' × ''n''}} matrix {{math|'''C'''}} whose {{math|(''i'', ''j'')}} entry is the {{math|(''i'', ''j'')}} ''[[cofactor (linear algebra)|cofactor]]'' of {{math|'''A'''}}, which is the {{math|(''i'', ''j'')}}-minor times a sign factor:
:&lt;math&gt;\mathbf{C} = \left((-1)^{i+j} \mathbf{M}_{ij}\right)_{1 \le i, j \le n}.&lt;/math&gt;
The adjugate of {{math|'''A'''}} is the transpose of {{math|'''C'''}}, that is, the {{math|''n''×''n''}} matrix whose {{math|(''i'',''j'')}} entry is the {{math|(''j'',''i'')}} cofactor of {{math|'''A'''}},
:&lt;math&gt;\operatorname{adj}(\mathbf{A}) = \mathbf{C}^\mathsf{T} = \left((-1)^{i+j} \mathbf{M}_{ji}\right)_{1 \le i, j \le n}.&lt;/math&gt;

The adjugate is defined as it is so that the product of {{math|'''A'''}} with its adjugate yields a [[diagonal matrix]] whose diagonal entries are the determinant {{math|det('''A''')}}.  That is,
:&lt;math&gt;\mathbf{A} \operatorname{adj}(\mathbf{A}) = \operatorname{adj}(\mathbf{A}) \mathbf{A} = \det(\mathbf{A}) \mathbf{I},&lt;/math&gt;
where {{math|'''I'''}} is the {{math|''n''×''n''}} identity matrix.  This is a consequence of the [[Laplace expansion]] of the determinant.

The above formula implies one of the fundamental results in matrix algebra, that {{math|'''A'''}} is invertible if and only if {{math|det('''A''')}} is an invertible element of {{math|''R''}}.  When this holds, the equation above yields
:&lt;math&gt;\begin{align}
\operatorname{adj}(\mathbf{A}) &amp;= \det(\mathbf{A}) \mathbf{A}^{-1}, \\
\mathbf{A}^{-1} &amp;= \det(\mathbf{A})^{-1} \operatorname{adj}(\mathbf{A}).
\end{align}&lt;/math&gt;

== Examples ==

=== 1 × 1 generic matrix ===
The adjugate of any 1×1 matrix is &lt;math&gt;\mathbf{I} = (1)&lt;/math&gt;.

=== 2 × 2 generic matrix ===
The adjugate of the 2×2 matrix
:&lt;math&gt;\mathbf{A} = \begin{pmatrix} {{a}} &amp; {{b}} \\ {{c}}  &amp; {{d}} \end{pmatrix}
&lt;/math&gt;
is
:&lt;math&gt;
\operatorname{adj}(\mathbf{A}) = \begin{pmatrix} {{d}} &amp; {{-b}}\\ {{-c}} &amp; {{a}} \end{pmatrix}.
&lt;/math&gt;
By direct computation,
:&lt;math&gt;\mathbf{A} \operatorname{adj}(\mathbf{A}) = \begin{pmatrix} ad - bc &amp; 0 \\ 0 &amp; ad - bc \end{pmatrix} = (\det \mathbf{A})\mathbf{I}.&lt;/math&gt;
In this case, it is also true that det(adj('''A''')) = det('''A''') and hence that adj(adj('''A''')) = '''A'''.

=== 3 × 3 generic matrix ===
Consider a 3×3 matrix
:&lt;math&gt;
\mathbf{A} = \begin{pmatrix}
a_{11} &amp; a_{12} &amp; a_{13} \\
a_{21} &amp; a_{22} &amp; a_{23} \\
a_{31} &amp; a_{32} &amp; a_{33}
\end{pmatrix}.
&lt;/math&gt;
Its cofactor matrix is
:&lt;math&gt;
\mathbf{C} = \begin{pmatrix}
+\begin{vmatrix} a_{22} &amp; a_{23} \\ a_{32} &amp; a_{33} \end{vmatrix} &amp;
-\begin{vmatrix} a_{21} &amp; a_{23} \\ a_{31} &amp; a_{33} \end{vmatrix} &amp;
+\begin{vmatrix} a_{21} &amp; a_{22} \\ a_{31} &amp; a_{32} \end{vmatrix} \\
 &amp; &amp; \\
-\begin{vmatrix} a_{12} &amp; a_{13} \\ a_{32} &amp; a_{33} \end{vmatrix} &amp;
+\begin{vmatrix} a_{11} &amp; a_{13} \\ a_{31} &amp; a_{33} \end{vmatrix} &amp;
-\begin{vmatrix} a_{11} &amp; a_{12} \\ a_{31} &amp; a_{32} \end{vmatrix} \\
 &amp; &amp; \\
+\begin{vmatrix} a_{12} &amp; a_{13} \\ a_{22} &amp; a_{23} \end{vmatrix} &amp;
-\begin{vmatrix} a_{11} &amp; a_{13} \\ a_{21} &amp; a_{23} \end{vmatrix} &amp;
+\begin{vmatrix} a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22} \end{vmatrix}
\end{pmatrix},
&lt;/math&gt;
where
:&lt;math&gt;
\begin{vmatrix} a_{im} &amp; a_{in} \\ a_{jm} &amp; a_{jn} \end{vmatrix}
=\det\begin{pmatrix} a_{im} &amp; a_{in} \\ a_{jm} &amp; a_{jn} \end{pmatrix}
&lt;/math&gt;.

Its adjugate is the transpose of its cofactor matrix,
:&lt;math&gt;
\operatorname{adj}(\mathbf{A}) = \mathbf{C}^\mathsf{T} = \begin{pmatrix}
+\begin{vmatrix} a_{22} &amp; a_{23} \\ a_{32} &amp; a_{33} \end{vmatrix} &amp;
-\begin{vmatrix} a_{12} &amp; a_{13} \\ a_{32} &amp; a_{33} \end{vmatrix} &amp;
+\begin{vmatrix} a_{12} &amp; a_{13} \\ a_{22} &amp; a_{23} \end{vmatrix} \\
 &amp; &amp; \\
-\begin{vmatrix} a_{21} &amp; a_{23} \\ a_{31} &amp; a_{33} \end{vmatrix} &amp;
+\begin{vmatrix} a_{11} &amp; a_{13} \\ a_{31} &amp; a_{33} \end{vmatrix} &amp;
-\begin{vmatrix} a_{11} &amp; a_{13} \\ a_{21} &amp; a_{23} \end{vmatrix} \\
 &amp; &amp; \\
+\begin{vmatrix} a_{21} &amp; a_{22} \\ a_{31} &amp; a_{32} \end{vmatrix} &amp;
-\begin{vmatrix} a_{11} &amp; a_{12} \\ a_{31} &amp; a_{32} \end{vmatrix} &amp;
+\begin{vmatrix} a_{11} &amp; a_{12} \\ a_{21} &amp; a_{22} \end{vmatrix}
\end{pmatrix}
&lt;/math&gt;.

=== 3 × 3 numeric matrix ===
As a specific example, we have
:&lt;math&gt;\operatorname{adj} \begin{pmatrix}
-3 &amp;  2 &amp; -5 \\
-1 &amp;  0 &amp; -2 \\
 3 &amp; -4 &amp;  1
\end{pmatrix} = \begin{pmatrix}
-8 &amp; 18 &amp; -4 \\
-5 &amp; 12 &amp; -1 \\
 4 &amp; -6 &amp;  2
\end{pmatrix}.
&lt;/math&gt;
It is easy to check the adjugate is the inverse times the determinant, {{math|&amp;minus;6}}.

The {{math|&amp;minus;1}} in the second row, third column of the adjugate was computed as follows.  The (2,3) entry of the adjugate is the (3,2) cofactor of '''A'''.  This cofactor is computed using the submatrix obtained by deleting the third row and second column of the original matrix '''A''',
:&lt;math&gt;\begin{pmatrix}-3&amp;-5\\-1&amp;-2\end{pmatrix}.&lt;/math&gt;
The (3,2) cofactor is a sign times the determinant of this submatrix:
:&lt;math&gt;(-1)^{3+2}\operatorname{det}\begin{pmatrix}-3&amp;-5\\-1&amp;-2\end{pmatrix}=-(-3 \cdot -2 - -5 \cdot -1)=-1,&lt;/math&gt;
and this is the (2,3) entry of the adjugate.

== Properties ==
For any {{math|''n'' × ''n''}} matrix {{math|'''A'''}}, elementary computations show that adjugates enjoy the following properties.
* &lt;math&gt;\operatorname{adj}(\mathbf{0}) = \mathbf{0}&lt;/math&gt; and &lt;math&gt;\operatorname{adj}(\mathbf{I}) = \mathbf{I}&lt;/math&gt;, where &lt;math&gt;\mathbf{0}&lt;/math&gt; and &lt;math&gt;\mathbf{I}&lt;/math&gt; are the zero and identity matrices, respectively.
* &lt;math&gt;\operatorname{adj}(c \mathbf{A}) = c^{n - 1}\operatorname{adj}(\mathbf{A})&lt;/math&gt; for any scalar {{math|''c''}}.
* &lt;math&gt;\operatorname{adj}(\mathbf{A}^\mathsf{T}) = \operatorname{adj}(\mathbf{A})^\mathsf{T}&lt;/math&gt;.
* &lt;math&gt;\det(\operatorname{adj}(\mathbf{A})) = (\det \mathbf{A})^{n-1}&lt;/math&gt;.
* If {{math|''A''}} is invertible, then &lt;math&gt;\operatorname{adj}(\mathbf{A}) = (\det \mathbf{A}) \mathbf{A}^{-1}&lt;/math&gt;.  It follows that:
** {{math|adj('''A''')}} is invertible with inverse {{math|(det '''A''')&lt;sup&gt;&amp;minus;1&lt;/sup&gt; '''A'''}}.
** {{math|1=adj('''A'''{{i sup|&amp;minus;1}}) = adj('''A'''){{i sup|&amp;minus;1}}}}.
* {{math|adj('''A''')}} is entrywise polynomial in {{math|'''A'''}}.  In particular, over the real or complex numbers, the adjugate is a smooth function of the entries of {{math|'''A'''}}.

Over the complex numbers,
* &lt;math&gt;\operatorname{adj}(\bar\mathbf{A}) = \overline{\operatorname{adj}(\mathbf{A})}&lt;/math&gt;, where the bar denotes complex conjugation.
* &lt;math&gt;\operatorname{adj}(\mathbf{A}^*) = \operatorname{adj}(\mathbf{A})^*&lt;/math&gt;, where the asterisk denotes conjugate transpose.

Suppose that {{math|'''B'''}} is another {{math|''n'' × ''n''}} matrix.  Then
:&lt;math&gt;\operatorname{adj}(\mathbf{AB}) = \operatorname{adj}(\mathbf{B})\operatorname{adj}(\mathbf{A}).&lt;/math&gt;
This can be proved in two ways.  One way, valid for any 
commutative ring, is a direct computation using the 
[[Cauchy–Binet formula]].  The other way, valid for the real 
or complex numbers, is to first observe that for invertible 
matrices {{math|'''A'''}} and {{math|'''B'''}},
:&lt;math&gt;\operatorname{adj}(\mathbf{B})\operatorname{adj}(\mathbf{A}) = (\det \mathbf{B})\mathbf{B}^{-1}(\det \mathbf{A})\mathbf{A}^{-1} = (\det \mathbf{AB})(\mathbf{AB})^{-1} = \operatorname{adj}(\mathbf{AB}).&lt;/math&gt;
Because every non-invertible matrix is the limit of invertible matrices, continuity of the adjugate then implies that the formula remains true when one of {{math|'''A'''}} or {{math|'''B'''}} is not invertible.

A corollary of the previous formula is that, for any non-negative integer {{math|''k''}},
:&lt;math&gt;\operatorname{adj}(\mathbf{A}^k) = \operatorname{adj}(\mathbf{A})^k.&lt;/math&gt;
If {{math|'''A'''}} is invertible, then the above formula also holds for negative {{math|''k''}}.

From the identity
:&lt;math&gt;(\mathbf{A} + \mathbf{B})\operatorname{adj}(\mathbf{A} + \mathbf{B})\mathbf{B} = \det(\mathbf{A} + \mathbf{B})\mathbf{B} = \mathbf{B}\operatorname{adj}(\mathbf{A} + \mathbf{B})(\mathbf{A} + \mathbf{B}),&lt;/math&gt;
we deduce
:&lt;math&gt;\mathbf{A}\operatorname{adj}(\mathbf{A} + \mathbf{B})\mathbf{B} = \mathbf{B}\operatorname{adj}(\mathbf{A} + \mathbf{B})\mathbf{A}.&lt;/math&gt;

Suppose that {{math|'''A'''}} commutes with {{math|'''B'''}}.  Multiplying the identity {{math|1='''AB''' = '''BA'''}} on the left and right by {{math|adj('''A''')}} proves that
:&lt;math&gt;\det(\mathbf{A})\operatorname{adj}(\mathbf{A})\mathbf{B} = \det(\mathbf{A})\mathbf{B}\operatorname{adj}(\mathbf{A}).&lt;/math&gt;
If {{math|'''A'''}} is invertible, this implies that {{math|adj('''A''')}} also commutes with {{math|'''B'''}}.  Over the real or complex numbers, continuity implies that {{math|adj('''A''')}} commutes with {{math|'''B'''}} even when {{math|'''A'''}} is not invertible.

Using the above properties and other elementary computations, it is straightforward to show that if {{math|'''A'''}} has one of the following properties, then {{math|adj '''A'''}} does as well:
* Upper triangular,
* Lower triangular,
* Diagonal,
* Orthogonal,
* Unitary,
* Symmetric,
* Hermitian,
* Skew-symmetric,
* Skew-hermitian,
* Normal.

If {{math|'''A'''}} is invertible, then, as noted above, there is a formula for {{math|adj('''A''')}} in terms of the determinant and inverse of {{math|'''A'''}}.  When {{math|'''A'''}} is not invertible, the adjugate satisfies different but closely related formulas.
* If {{math|1=rk('''A''') &amp;le; ''n'' &amp;minus; 2}}, then {{math|1=adj('''A''') = '''0'''}}.
* If {{math|1=rk('''A''') = ''n'' &amp;minus; 1}}, then {{math|1=rk(adj('''A''')) = 1}}.  (Some minor is non-zero, so {{math|adj('''A''')}} is non-zero and hence has rank at least one; the identity {{math|1=adj('''A''')&amp;thinsp;'''A''' = '''0'''}} implies that the dimension of the nullspace of {{math|adj('''A''')}} is at least {{math|''n'' &amp;minus; 1}}, so its rank is at most one.)  It follows that {{math|1=adj('''A''') = &amp;alpha;'''xy'''&lt;sup&gt;T&lt;/sup&gt;}}, where {{math|&amp;alpha;}} is a scalar and {{math|'''x'''}} and {{math|'''y'''}} are vectors such that {{math|1='''Ax''' = '''0'''}} and {{math|1='''A'''&lt;sup&gt;T&lt;/sup&gt;&amp;thinsp;'''y''' = '''0'''}}.

=== Column substitution and Cramer's rule ===
{{see also|Cramer's rule}}

Partition {{math|'''A'''}} into column vectors:
:&lt;math&gt;\mathbf{A} = (\mathbf{a}_1\ \cdots\ \mathbf{a}_n).&lt;/math&gt;
Let {{math|'''b'''}} be a column vector of size {{math|''n''}}.  Fix {{math|1 &amp;le; ''i'' &amp;le; ''n''}} and consider the matrix formed by replacing column {{math|''i''}} of {{math|'''A'''}} by {{math|'''b'''}}:
:&lt;math&gt;(\mathbf{A} \stackrel{i}{\leftarrow} \mathbf{b})\ \stackrel{\text{def}}{=}\ \begin{pmatrix} \mathbf{a}_1 &amp; \cdots &amp; \mathbf{a}_{i-1} &amp; \mathbf{b} &amp; \mathbf{a}_{i+1} &amp; \cdots &amp; \mathbf{a}_n \end{pmatrix}.&lt;/math&gt;
Laplace expand the determinant of this matrix along column {{math|''i''}}.  The result is entry {{math|''i''}} of the product {{math|adj('''A''')'''b'''}}.  Collecting these determinants for the different possible {{math|''i''}} yields an equality of column vectors
:&lt;math&gt;\left(\det(\mathbf{A} \stackrel{i}{\leftarrow} \mathbf{b})\right)_{i=1}^n = \operatorname{adj}(\mathbf{A})\mathbf{b}.&lt;/math&gt;

This formula has the following concrete consequence.  Consider the linear system of equations
:&lt;math&gt;\mathbf{A}\mathbf{x} = \mathbf{b}.&lt;/math&gt;
Assume that {{math|'''A'''}} is non-singular.  Multiplying this system on the left by {{math|adj('''A''')}} and dividing by the determinant yields
:&lt;math&gt;\mathbf{x} = \frac{\operatorname{adj}(\mathbf{A})\mathbf{b}}{\det \mathbf{A}}.&lt;/math&gt;
Applying the previous formula to this situation yields '''Cramer's rule''',
:&lt;math&gt;x_i = \frac{\det(\mathbf{A} \stackrel{i}{\leftarrow} \mathbf{b})}{\det \mathbf{A}},&lt;/math&gt;
where {{math|''x''&lt;sub&gt;''i''&lt;/sub&gt;}} is the {{math|''i''}}th entry of {{math|'''x'''}}.

=== Characteristic polynomial ===
Let the [[characteristic polynomial]] of {{math|'''A'''}} be
:&lt;math&gt;p(s) = \det(s\mathbf{I} - \mathbf{A}) = \sum_{i=0}^n p_i s^i \in R[s].&lt;/math&gt;
The first [[divided difference]] of {{math|''p''}} is a [[symmetric polynomial]] of degree {{math|''n'' &amp;minus; 1}},
:&lt;math&gt;\Delta p(s, t) = \frac{p(s) - p(t)}{s - t} = \sum_{0 \le j + k &lt; n} p_{j+k+1} s^j t^k \in R[s, t].&lt;/math&gt;
Multiply {{math|''s'''''I''' &amp;minus; '''A'''}} by its adjugate.  Since {{math|1=''p''('''A''') = '''0'''}} by the [[Cayley–Hamilton theorem]], some elementary manipulations reveal
:&lt;math&gt;\operatorname{adj}(s\mathbf{I} - \mathbf{A}) = \Delta p(s\mathbf{I}, \mathbf{A}).&lt;/math&gt;

In particular, the [[resolvent formalism|resolvent]] of {{math|'''A'''}} is defined to be
:&lt;math&gt;R(z; \mathbf{A}) = (z\mathbf{I} - \mathbf{A})^{-1},&lt;/math&gt;
and by the above formula, this is equal to
:&lt;math&gt;R(z; \mathbf{A}) = \frac{\Delta p(z\mathbf{I}, \mathbf{A})}{p(z)}.&lt;/math&gt;

=== Jacobi's formula ===
{{main|Jacobi's formula}}
The adjugate also appears in [[Jacobi's formula]] for the [[derivative]] of the [[determinant]].  If {{math|'''A'''(''t'')}} is continuously differentiable, then
:&lt;math&gt;\frac{d(\det \mathbf{A})}{dt}(t) = \operatorname{tr}\left(\operatorname{adj}(\mathbf{A}(t)) \mathbf{A}'(t)\right).&lt;/math&gt;
It follows that the total derivative of the determinant is the transpose of the adjugate:
:&lt;math&gt;d(\det \mathbf{A})_{\mathbf{A}_0} = \operatorname{adj}(\mathbf{A}_0)^{\mathsf{T}}.&lt;/math&gt;

=== Cayley–Hamilton formula ===
{{main|Cayley–Hamilton theorem}}
Let {{math|''p''&lt;sub&gt;'''A'''&lt;/sub&gt;(''t'')}} be the characteristic polynomial of {{math|'''A'''}}.  The [[Cayley–Hamilton theorem]] states that
:&lt;math&gt;p_{\mathbf{A}}(\mathbf{A}) = \mathbf{0}.&lt;/math&gt;
Separating the constant term and multiplying the equation by {{math|adj('''A''')}} gives an expression for the adjugate that depends only on {{math|'''A'''}} and the coefficients of {{math|''p''&lt;sub&gt;'''A'''&lt;/sub&gt;(''t'')}}.  These coefficients can be explicitly represented in terms of traces of powers of {{math|'''A'''}} using complete exponential [[Bell polynomials]].  The resulting formula is
:&lt;math&gt;\operatorname{adj}(\mathbf{A}) = \sum_{s=0}^{n-1} \mathbf{A}^{s} \sum_{k_1, k_2, \ldots, k_{n-1}} \prod_{\ell=1}^{n-1} \frac{(-1)^{k_\ell+1}}{\ell^{k_\ell}k_{\ell}!}\operatorname{tr}(\mathbf{A}^\ell)^{k_\ell},&lt;/math&gt;
where {{math|''n''}} is the dimension of {{math|'''A'''}}, and the sum is taken over {{math|''s''}} and all sequences of {{math|''k&lt;sub&gt;l&lt;/sub&gt;'' ≥ 0}} satisfying the linear [[Diophantine equation]]
:&lt;math&gt;s+\sum_{\ell=1}^{n-1}\ell k_\ell = n - 1.&lt;/math&gt;

For the 2×2 case, this gives
:&lt;math&gt;\operatorname{adj}(\mathbf{A})=\mathbf{I}_2\left(\operatorname{tr} \mathbf{A}\right) - \mathbf{A}.&lt;/math&gt;
For the 3×3 case, this gives
:&lt;math&gt;\operatorname{adj}(\mathbf{A})=\frac 1 2 \mathbf{I}_3\left( (\operatorname{tr}\mathbf{A})^2-\operatorname{tr}\mathbf{A}^2\right) -\mathbf{A}\left(\operatorname{tr} \mathbf{A}\right) + \mathbf{A}^2 .&lt;/math&gt;
For the 4×4 case, this gives
:&lt;math&gt;\operatorname{adj}(\mathbf{A})=
\frac{1}{6}\mathbf{I}_4\left( (\operatorname{tr}\mathbf{A})^{3}-3\operatorname{tr}\mathbf{A}\operatorname{tr}\mathbf{A}^{2}+2\operatorname{tr}\mathbf{A}^{3}\right)
- \frac{1}{2}\mathbf{A}\left( (\operatorname{tr}\mathbf{A})^{2}-\operatorname{tr}\mathbf{A}^{2}\right)
+ \mathbf{A}^2\left(\operatorname{tr}\mathbf{A}\right)
- \mathbf{A}^{3}.&lt;/math&gt;

The same formula follows directly from the terminating step of the [[Faddeev–LeVerrier algorithm]], which efficiently determines the [[characteristic polynomial]] of {{math|'''A'''}}.

== Relation to exterior algebras ==
The adjugate can be viewed in abstract terms using [[exterior algebra]]s.  Let {{math|''V''}} be an {{math|''n''}}-dimensional vector space.  The [[exterior product]] defines a bilinear pairing
:&lt;math&gt;V \times \wedge^{n-1} V \to \wedge^n V.&lt;/math&gt;
Abstractly, &lt;math&gt;\wedge^n V&lt;/math&gt; is isomorphic to {{math|'''R'''}}, and under any such isomorphism the exterior product is a [[perfect pairing]].  Therefore, it yields an isomorphism
:&lt;math&gt;\phi \colon V\ \xrightarrow{\cong}\ \operatorname{Hom}(\wedge^{n-1} V, \wedge^n V).&lt;/math&gt;
Explicitly, this pairing sends {{math|'''v''' ∈ ''V''}} to &lt;math&gt;\phi_{\mathbf{v}}&lt;/math&gt;, where
:&lt;math&gt;\phi_\mathbf{v}(\alpha) = \mathbf{v} \wedge \alpha.&lt;/math&gt;
Suppose that {{math|''T'' : ''V'' &amp;rarr; ''V''}} is a linear transformation.  Pullback by the {{math|(''n'' &amp;minus; 1)}}st exterior power of {{math|''T''}} induces a morphism of {{math|Hom}} spaces.  The '''adjugate''' of {{math|''T''}} is the composite
:&lt;math&gt;V\ \xrightarrow{\phi}\ \operatorname{Hom}(\wedge^{n-1} V, \wedge^n V)\ \xrightarrow{(\wedge^{n-1} T)^*}\ \operatorname{Hom}(\wedge^{n-1} V, \wedge^n V)\ \xrightarrow{\phi^{-1}}\ V.&lt;/math&gt;

If {{math|1=''V'' = '''R'''&lt;sup&gt;''n''&lt;/sup&gt;}} is endowed with its coordinate basis {{math|'''e'''&lt;sub&gt;1&lt;/sub&gt;, ..., '''e'''&lt;sub&gt;''n''&lt;/sub&gt;}}, and if the matrix of {{math|''T''}} in this basis is {{math|'''A'''}}, then the adjugate of {{math|''T''}} is the adjugate of {{math|'''A'''}}.  To see why, give &lt;math&gt;\wedge^{n-1} \mathbf{R}^n&lt;/math&gt; the basis
:&lt;math&gt;\{\mathbf{e}_1 \wedge \dots \wedge \hat\mathbf{e}_k \wedge \dots \wedge \mathbf{e}_n\}_{k=1}^n.&lt;/math&gt;
Fix a basis vector {{math|'''e'''&lt;sub&gt;''i''&lt;/sub&gt;}} of {{math|'''R'''&lt;sup&gt;''n''&lt;/sup&gt;}}.  The image of {{math|'''e'''&lt;sub&gt;''i''&lt;/sub&gt;}} under &lt;math&gt;\phi&lt;/math&gt; is determined by where it sends basis vectors:
:&lt;math&gt;\phi_{\mathbf{e}_i}(\mathbf{e}_1 \wedge \dots \wedge \hat\mathbf{e}_k \wedge \dots \wedge \mathbf{e}_n)
= \begin{cases} (-1)^{i-1} \mathbf{e}_1 \wedge \dots \wedge \mathbf{e}_n, &amp;\text{if}\ k = i, \\ 0 &amp;\text{otherwise.} \end{cases}&lt;/math&gt;
On basis vectors, the {{math|(''n'' &amp;minus; 1)}}st exterior power of {{math|''T''}} is
:&lt;math&gt;\mathbf{e}_1 \wedge \dots \wedge \hat\mathbf{e}_j \wedge \dots \wedge \mathbf{e}_n \mapsto \sum_{k=1}^n (\det A_{jk}) \mathbf{e}_1 \wedge \dots \wedge \hat\mathbf{e}_k \wedge \dots \wedge \mathbf{e}_n.&lt;/math&gt;
Each of these terms maps to zero under &lt;math&gt;\phi_{\mathbf{e}_i}&lt;/math&gt; except the {{math|1=''k'' = ''i''}} term.  Therefore, the pullback of &lt;math&gt;\phi_{\mathbf{e}_i}&lt;/math&gt; is the linear transformation for which
:&lt;math&gt;\mathbf{e}_1 \wedge \dots \wedge \hat\mathbf{e}_j \wedge \dots \wedge \mathbf{e}_n \mapsto (-1)^{i-1} (\det A_{ji}) \mathbf{e}_1 \wedge \dots \wedge \mathbf{e}_n,&lt;/math&gt;
that is, it equals
:&lt;math&gt;\sum_{j=1}^n (-1)^{i+j} (\det A_{ji})\phi_{\mathbf{e}_j}.&lt;/math&gt;
Applying the inverse of &lt;math&gt;\phi&lt;/math&gt; shows that the adjugate of {{math|''T''}} is the linear transformation for which
:&lt;math&gt;\mathbf{e}_i \mapsto \sum_{j=1}^n (-1)^{i+j}(\det A_{ji})\mathbf{e}_j.&lt;/math&gt;
Consequently, its matrix representation is the adjugate of {{math|'''A'''}}.

If {{math|''V''}} is endowed with an inner product and a volume form, then the map {{math|''φ''}} can be decomposed further.  In this case, {{math|''φ''}} can be understood as the composite of the [[Hodge star operator]] and dualization.  Specifically, if {{math|ω}} is the volume form, then it, together with the inner product, determines an isomorphism
:&lt;math&gt;\omega^\vee \colon \wedge^n V \to \mathbf{R}.&lt;/math&gt;
This induces an isomorphism
:&lt;math&gt;\operatorname{Hom}(\wedge^{n-1} \mathbf{R}^n, \wedge^n \mathbf{R}^n) \cong \wedge^{n-1} (\mathbf{R}^n)^\vee.&lt;/math&gt;
A vector {{math|'''v'''}} in {{math|'''R'''&lt;sup&gt;''n''&lt;/sup&gt;}} corresponds to the linear functional
:&lt;math&gt;(\alpha \mapsto \omega^\vee(\mathbf{v} \wedge \alpha)) \in \wedge^{n-1} (\mathbf{R}^n)^\vee.&lt;/math&gt;
By the definition of the Hodge star operator, this linear functional is dual to {{math|*'''v'''}}.  That is, {{math|ω&lt;sup&gt;∨&lt;/sup&gt; ∘ φ}} equals {{math|'''v''' ↦ *'''v'''&lt;sup&gt;∨&lt;/sup&gt;}}.

== Higher adjugates ==
Let {{math|'''A'''}} be an {{math|''n'' × ''n''}} matrix, and fix {{math|''r'' &amp;ge; 0}}.  The '''{{math|''r''}}th higher adjugate''' of {{math|'''A'''}} is an &lt;math&gt;\textstyle\binom{n}{r} \times \binom{n}{r}&lt;/math&gt; matrix, denoted {{math|adj&lt;sub&gt;''r''&lt;/sub&gt; '''A'''}}, whose entries are indexed by size {{math|''r''}} subsets {{math|''I''}} and {{math|''J''}} of {{math|{1, ..., ''m''&lt;nowiki&gt;}&lt;/nowiki&gt;}}.  Let {{math|''I''{{i sup|c}}}} and {{math|''J''{{i sup|c}}}} denote the complements of {{math|''I''}} and {{math|''J''}}, respectively.  Also let &lt;math&gt;\mathbf{A}_{I^c, J^c}&lt;/math&gt; denote the submatrix of {{math|'''A'''}} containing those rows and columns whose indices are in {{math|''I''&lt;sup&gt;c&lt;/sup&gt;}} and {{math|''J''&lt;sup&gt;c&lt;/sup&gt;}}, respectively.  Then the {{math|(''I'', ''J'')}} entry of {{math|adj&lt;sub&gt;''r''&lt;/sub&gt; '''A'''}} is
:&lt;math&gt;(-1)^{\sigma(I) + \sigma(J)}\det \mathbf{A}_{J^c, I^c},&lt;/math&gt;
where {{math|σ(''I'')}} and {{math|σ(''J'')}} are the sum of the elements of {{math|''I''}} and {{math|''J''}}, respectively.

Basic properties of higher adjugates include:
* {{math|1=adj&lt;sub&gt;0&lt;/sub&gt;('''A''') = det '''A'''}}.
* {{math|1=adj&lt;sub&gt;1&lt;/sub&gt;('''A''') = adj '''A'''}}.
* {{math|1=adj&lt;sub&gt;''n''&lt;/sub&gt;('''A''') = 1}}.
* {{math|1=adj&lt;sub&gt;''r''&lt;/sub&gt;('''BA''') = adj&lt;sub&gt;''r''&lt;/sub&gt;('''A''') adj&lt;sub&gt;''r''&lt;/sub&gt;('''B''')}}.
* &lt;math&gt;\operatorname{adj}_r(\mathbf{A})C_r(\mathbf{A}) = C_r(\mathbf{A})\operatorname{adj}_r(\mathbf{A}) = (\det \mathbf{A})I_{\binom{n}{r}}&lt;/math&gt;, where {{math|''C''&lt;sub&gt;''r''&lt;/sub&gt;('''A''')}} denotes the {{math|''r''}}th [[compound matrix]].

Higher adjugates may be defined in abstract algebraic terms in a similar fashion to the usual adjugate, substituting &lt;math&gt;\wedge^r V&lt;/math&gt; and &lt;math&gt;\wedge^{n-r} V&lt;/math&gt; for &lt;math&gt;V&lt;/math&gt; and &lt;math&gt;\wedge^{n-1} V&lt;/math&gt;, respectively.

== Iterated adjugates ==
[[Iterated function|Iteratively]] taking the adjugate of an invertible matrix '''A'''  {{mvar|k}} times yields

:&lt;math&gt;\overbrace{\operatorname{adj}\dotsm\operatorname{adj}}^k(\mathbf{A})=\det(\mathbf{A})^{\frac{(n-1)^k-(-1)^k}n}\mathbf{A}^{(-1)^k}~,&lt;/math&gt;
:&lt;math&gt;\det(\overbrace{\operatorname{adj}\dotsm\operatorname{adj}}^k(\mathbf{A}))=\det(\mathbf{A})^{(n-1)^k}~.&lt;/math&gt;

For example,
:&lt;math&gt;\operatorname{adj}(\operatorname{adj}(\mathbf{A})) = \det(\mathbf{A})^{n - 2} \mathbf{A}.&lt;/math&gt;
:&lt;math&gt;\det(\operatorname{adj}(\operatorname{adj}(\mathbf{A}))) = \det(\mathbf{A})^{(n - 1)^2}.&lt;/math&gt;

== See also ==
* [[Cayley–Hamilton theorem]]
* [[Cramer's rule]]
* [[Trace diagram]]
* [[Jacobi's formula]]
* [[Faddeev–LeVerrier algorithm]]

== References ==
{{Reflist}}

*  Roger A. Horn and Charles R. Johnson (2013), ''Matrix Analysis'', Second Edition. Cambridge University Press, {{ISBN|978-0-521-54823-6}}
*  Roger A. Horn and Charles R. Johnson (1991), ''Topics in Matrix Analysis''. Cambridge University Press, {{ISBN|978-0-521-46713-1}}

== External links ==
* [http://www.ee.ic.ac.uk/hp/staff/dmb/matrix/property.html#adjoint Matrix Reference Manual]
*[http://www.elektro-energetika.cz/calculations/matreg.php?language=english Online matrix calculator (determinant, track, inverse, adjoint, transpose)] Compute Adjugate matrix up to order 8
* {{cite web | url=http://www.wolframalpha.com/input/?i=adjugate+of+{+{+a%2C+b%2C+c+}%2C+{+d%2C+e%2C+f+}%2C+{+g%2C+h%2C+i+}+} | title=&lt;nowiki&gt;adjugate of { { a, b, c }, { d, e, f }, { g, h, i } }&lt;/nowiki&gt; | work=[[Wolfram Alpha]]}}

{{Matrix classes}}

[[Category:Matrix theory]]
[[Category:Linear algebra]]</text>
      <sha1>og7uo7vryb8twvg7zo3o8mklwebrht0</sha1>
    </revision>
  </page>
  <page>
    <title>András Gyárfás</title>
    <ns>0</ns>
    <id>614969</id>
    <revision>
      <id>864611284</id>
      <parentid>815037450</parentid>
      <timestamp>2018-10-18T10:41:24Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v2.0beta9)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1663">'''András Gyárfás''' (born 1945) is a [[Hungary|Hungarian]] [[mathematician]] who specializes in the study of [[graph theory]]. Together with [[Paul Erdős]] he conjectured what is now called the [[Erdős–Gyárfás conjecture]] which states that any [[Graph (discrete mathematics)|graph]] with minimum [[Graph (discrete mathematics)#Definitions|degree]] 3 contains a [[Cycle (graph theory)|simple cycle]] whose length is a [[power of two]]. He and [[David Sumner]] independently formulated the [[Gyárfás–Sumner conjecture]] according to which, for every [[Tree (graph theory)|tree]] ''T'', the ''T''-free graphs are [[χ-bounded]].

Gyárfás began working as a researcher for the Computer and Automation Research Institute of the [[Hungarian Academy of Sciences]] in 1968. He earned a [[candidate degree]] in 1980, and a doctorate (Dr. Math. Sci.) in 1992. He won the Géza Grünwald Commemorative Prize for young researchers of the [[János Bolyai Mathematical Society]] in 1978.&lt;ref&gt;[http://wwwold.sztaki.hu/sztaki/ake/applmath/discret/gyarfas_cv.jhtml Gyárfás's CV], retrieved 2016-07-12.&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
* [https://web.archive.org/web/20080824023918/http://www.sztaki.hu/people/008001049/ András Gyárfás] at the Computer and Automation Research Institute, [[Hungarian Academy of Sciences]]
*[https://scholar.google.com/citations?user=nVmNGo0AAAAJ Google scholar profile]

{{Authority control}}

{{DEFAULTSORT:Gyarfas, Andras}}
[[Category:Hungarian mathematicians]]
[[Category:1945 births]]
[[Category:Combinatorialists]]
[[Category:Living people]]


{{Hungary-scientist-stub}}
{{Europe-mathematician-stub}}</text>
      <sha1>c1hcj6ybzj73fd3630n75m9kqyh0yi2</sha1>
    </revision>
  </page>
  <page>
    <title>Behavior of coupled DEVS</title>
    <ns>0</ns>
    <id>21431954</id>
    <revision>
      <id>841842251</id>
      <parentid>828071067</parentid>
      <timestamp>2018-05-18T12:32:25Z</timestamp>
      <contributor>
        <username>Mhhwang2002</username>
        <id>5725715</id>
      </contributor>
      <minor/>
      <comment>/* View2: Total States = States * Lifespan * Elapsed Times */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9721">{{multiple issues|
{{notability|date=November 2012}}
{{technical|date=November 2012}}
{{context|date=November 2012}}
}}
''DEVS is closed under coupling'' [[Behavior_of_Coupled_DEVS#References|[Zeigper84]]] [[Behavior_of_Coupled_DEVS#References|[ZPK00]]]. In other words, given a [[DEVS#Coupled DEVS|coupled DEVS]] model &lt;math&gt; N &lt;/math&gt;, its behavior is described as an atomic DEVS model &lt;math&gt; M&lt;/math&gt;.  For a given coupled DEVS &lt;math&gt; N &lt;/math&gt;, once we have an equivalent atomic DEVS &lt;math&gt; M &lt;/math&gt;, behavior of &lt;math&gt; M &lt;/math&gt; can be referred to [[Behavior of DEVS|behavior of atomic DEVS]] which is based on [[Timed Event System]].

Similar to [[Behavior of DEVS|behavior of atomic DEVS]], behavior of the Coupled DEVS class is described depending on definition of the total state set and its handling as follows.

== View1: Total States = States * Elapsed Times ==
Given a [[DEVS#Coupled DEVS|coupled DEVS]] model &lt;math&gt; N = &lt;X,Y,D,\{M_i\},C_{xx}, C_{yx}, C_{yy}, Select&gt;&lt;/math&gt;, its behavior is described as an atomic DEVS model &lt;math&gt; M = &lt;X,Y,S,s_0,ta, \delta_{ext}, \delta_{int}, \lambda&gt; &lt;/math&gt;

where
* &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; are the input event set and the output event set, respectively.
* &lt;math&gt;S=\underset{i \in D}\times Q_i&lt;/math&gt; is the partial state set where &lt;math&gt;Q_i=\{(s_i,t_{ei})| s_i \in S_i, t_{ei} \in (\mathbb{T} \cap [0, ta_i(s_i)])\} &lt;/math&gt; is the total state set of component &lt;math&gt; i \in D&lt;/math&gt; (Refer to [[Behavior of DEVS#View 1: total states = states * elapsed times|View1 of Behavior of DEVS]]), where &lt;math&gt; \mathbb{T}=[0,\infty)&lt;/math&gt; is the set of non-negative real numbers.
* &lt;math&gt;s_0=\underset{i \in D}\times q_{0i}&lt;/math&gt; is the initial state set where &lt;math&gt;q_{0i}=(s_{0i},0)&lt;/math&gt; is the total initial state of component &lt;math&gt; i \in D &lt;/math&gt;.
*&lt;math&gt;ta:S \rightarrow \mathbb{T}^\infty &lt;/math&gt; is the time advance function, where &lt;math&gt; \mathbb{T}^\infty=[0,\infty]&lt;/math&gt; is the set of non-negative real numbers plus infinity.Given &lt;math&gt;s=(\ldots, (s_{i},t_{ei}),\ldots)&lt;/math&gt;, &lt;center&gt; &lt;math&gt; ta(s)= \min\{ ta_i(si) - t_{ei}| i \in D\}. 
&lt;/math&gt; &lt;/center&gt;
 
 
*&lt;math&gt;\delta_{ext}:Q \times X \rightarrow S &lt;/math&gt; is the external state function.  Given a total state &lt;math&gt;q=(s,t_e)&lt;/math&gt; where &lt;math&gt;s=(\ldots, (s_{i}, t_{ei}),\ldots), t_e \in (\mathbb{T}\cap [0,ta(s)] )&lt;/math&gt;, and input event &lt;math&gt; x \in X &lt;/math&gt;, the next state is given by &lt;center&gt;&lt;math&gt; \delta_{ext}(q, x)=s'=(\ldots,(s_i', t_{ei}'), \ldots) &lt;/math&gt;&lt;/center&gt;
where
&lt;center&gt; &lt;math&gt; 
(s_i', t_{ei}')= 
\begin{cases}
(\delta_{ext}(s_i, t_{ei}, x_i),0) &amp; \text{if } (x, x_i) \in C_{xx}\\
(s_i, t_{ei}) &amp; \text{otherwise}.
\end{cases}
&lt;/math&gt;&lt;/center&gt;

Given the partial state &lt;math&gt;s=(\ldots,(s_i, t_{ei}),\ldots) \in S &lt;/math&gt;, let &lt;math&gt; IMM(s)=\{i \in D| ta_i(s_i) = ta(s) \} &lt;/math&gt; denote ''the set of imminent components''. The ''firing component'' &lt;math&gt; i^* \in D &lt;/math&gt; which triggers the internal state transition and an output event is determined by &lt;center&gt; &lt;math&gt; i^* = Select(IMM(s)).&lt;/math&gt;&lt;/center&gt;

*&lt;math&gt;\delta_{int}:S \rightarrow S &lt;/math&gt; is the internal state function.  Given a partial state &lt;math&gt; s=(\ldots, (s_{i}, t_{ei}),\ldots)&lt;/math&gt;, the next state is given by &lt;center&gt;&lt;math&gt; \delta_{int}(s)=s'=(\ldots,(s_i', t_{ei}'), \ldots) &lt;/math&gt;&lt;/center&gt;
where
&lt;center&gt; &lt;math&gt; 
(s_i', t_{ei}')= 
\begin{cases}
(\delta_{int}(s_i),0) &amp; \text{if } i = i^*\\
(\delta_{ext}(s_i, t_{ei}, x_i),0) &amp; \text{if } (\lambda_{i^*}(s_{i^*}), x_i) \in C_{yx}\\
(s_i, t_{ei}) &amp; \text{otherwise}.
\end{cases}
&lt;/math&gt;&lt;/center&gt;

*&lt;math&gt;\lambda:S \rightarrow Y^\phi &lt;/math&gt; is the output function.  Given a partial state &lt;math&gt; s=(\ldots, (s_{i}, t_{ei}),\ldots)&lt;/math&gt;,  &lt;center&gt;&lt;math&gt; \lambda(s)=
\begin{cases} 
\phi                          &amp;\text{if } \lambda_{i^*}(s_{i^*})=\phi \\
C_{yy}(\lambda_{i^*}(s_{i^*})) &amp;\text{otherwise}.
\end{cases}
&lt;/math&gt;&lt;/center&gt;

== View2: Total States = States * Lifespan * Elapsed Times ==
Given a [[DEVS#Coupled DEVS|coupled DEVS]] model &lt;math&gt; N = &lt;X,Y,D,\{M_i\},C_{xx}, C_{yx}, C_{yy}, Select&gt;&lt;/math&gt;, its behavior is described as an atomic DEVS model &lt;math&gt; M = &lt;X,Y,S,s_0,ta, \delta_{ext}, \delta_{int}, \lambda&gt; &lt;/math&gt;

where
* &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; are the input event set and the output event set, respectively.
* &lt;math&gt;S=\underset{i \in D}\times Q_i&lt;/math&gt; is the partial state set where &lt;math&gt;Q_i=\{(s_i,t_{si}, t_{ei})| s_i \in S_i, t_{si} \in \mathbb{T}^\infty, t_{ei} \in (\mathbb{T} \cap [0, t_{si}])\} &lt;/math&gt; is the total state set of component &lt;math&gt; i \in D&lt;/math&gt; (Refer to [[Behavior of DEVS#View 2: total states = states * lifespans * elapsed times|View2 of Behavior of DEVS]]).
* &lt;math&gt;s_0=\underset{i \in D}\times q_{0i}&lt;/math&gt; is the initial state set where &lt;math&gt;q_{0i}=(s_{0i},ta_i(s_{0i}),0)&lt;/math&gt; is the total initial state of component &lt;math&gt; i \in D &lt;/math&gt;.
*&lt;math&gt;ta:S \rightarrow \mathbb{T}^\infty &lt;/math&gt; is the time advance function. Given &lt;math&gt;s=(\ldots, (s_{i},t_{si},t_{ei}),\ldots)&lt;/math&gt;, &lt;center&gt; &lt;math&gt; ta(s)= \min\{ t_{si} - t_{ei}| i \in D\}. 
&lt;/math&gt; &lt;/center&gt;
 
 
*&lt;math&gt;\delta_{ext}:Q \times X \rightarrow S \times \{0, 1\} &lt;/math&gt; is the external state function.  Given a total state &lt;math&gt;q=(s,t_s,t_e)&lt;/math&gt; where &lt;math&gt;s=(\ldots, (s_{i}, t_{si},t_{ei}),\ldots), t_s \in \mathbb{T}^\infty, t_e \in (\mathbb{T}\cap [0,t_s] )&lt;/math&gt;, and input event &lt;math&gt; x \in X &lt;/math&gt;, the next state is given by &lt;center&gt;&lt;math&gt; \delta_{ext}(q, x)=((\ldots,(s_i', t_{si}', t_{ei}'), \ldots),b) &lt;/math&gt;&lt;/center&gt;
where
&lt;center&gt; &lt;math&gt; 
(s_i', t_{si}', t_{ei}')= 
\begin{cases}
(s_i', ta_i(s_i'), 0) &amp; \text{if } (x, x_i) \in C_{xx},\delta_{ext}(s_i, t_{si}, t_{ei}, x_i)=(s_i',1)\\
(s_i', t_{si}, t_{ei} ) &amp; \text{if } (x, x_i) \in C_{xx},\delta_{ext}(s_i, t_{si}, t_{ei}, x_i)=(s_i',0)\\
(s_i, t_{si}, t_{ei}) &amp; \text{otherwise}
\end{cases}
&lt;/math&gt;&lt;/center&gt;
and
&lt;center&gt; &lt;math&gt; 
b=
\begin{cases}
1 &amp; \text{if } \exists i \in D: (x, x_i) \in C_{xx},\delta_{ext}(s_i, t_{si}, t_{ei}, x_i)=(s_i',1)\\
0 &amp; \text{otherwise}.
\end{cases}
&lt;/math&gt;&lt;/center&gt;

Given the partial state &lt;math&gt;s=(\ldots,(s_i, t_{si}, t_{ei}),\ldots) \in S &lt;/math&gt;, let &lt;math&gt; IMM(s)=\{i \in D| t_{si} - t_{ei} = ta(s) \} &lt;/math&gt; denote ''the set of imminent components''. The ''firing component'' &lt;math&gt; i^* \in D &lt;/math&gt; which triggers the internal state transition and an output event is determined by &lt;center&gt; &lt;math&gt; i^* = Select(IMM(s)).&lt;/math&gt;&lt;/center&gt;

*&lt;math&gt;\delta_{int}:S \rightarrow S &lt;/math&gt; is the internal state function.  Given a partial state &lt;math&gt; s=(\ldots, (s_{i},t_{si}, t_{ei}),\ldots)&lt;/math&gt;, the next state is given by &lt;center&gt;&lt;math&gt; \delta_{int}(s)=s'=(\ldots,(s_i', t_{si}', t_{ei}'), \ldots) &lt;/math&gt;&lt;/center&gt;
where
&lt;center&gt; &lt;math&gt; 
(s_i', t_{si}', t_{ei}')= 
\begin{cases}
(s_i', ta_i(s_i'),0) &amp; \text{if } i = i^*,\delta_{int}(s_i)=s_i',\\
(s_i', ta_i(s_i'),0)  &amp; \text{if } (\lambda_{i^*}(s_{i^*}), x_i) \in C_{yx},\delta_{ext}(s_i, t_{si}, t_{ei}, x_i)=(s', 1)\\
(s_i', t_{si}, t_{ei})  &amp; \text{if } (\lambda_{i^*}(s_{i^*}), x_i) \in C_{yx},\delta_{ext}(s_i, t_{si}, t_{ei}, x_i)=(s', 0)\\
(s_i, t_{si}, t_{ei}) &amp; \text{otherwise}.
\end{cases}
&lt;/math&gt;&lt;/center&gt;

*&lt;math&gt;\lambda:S \rightarrow Y^\phi &lt;/math&gt; is the output function.  Given a partial state &lt;math&gt; s=(\ldots, (s_{i}, t_{si}, t_{ei}),\ldots)&lt;/math&gt;,  &lt;center&gt;&lt;math&gt; \lambda(s)=
\begin{cases} 
\phi                          &amp;\text{if } \lambda_{i^*}(s_{i^*})=\phi \\
C_{yy}(\lambda_{i^*}(s_{i^*})) &amp;\text{otherwise}.
\end{cases}
&lt;/math&gt;&lt;/center&gt;

== Time Passage ==
Since in a coupled DEVS model with non-empty sub-components, i.e., &lt;math&gt; |D|&gt;0&lt;/math&gt;, the number of clocks which trace their elapsed times are multiple, so time passage of the model is noticeable. 
;For View1  
Given a total state &lt;math&gt; q=(s,t_e) \in Q &lt;/math&gt;  where &lt;math&gt; s = (\ldots,(s_i, t_{ei}),\ldots) &lt;/math&gt;

If [[Event Segment#Unit event segment|unit event segment]] &lt;math&gt; \omega&lt;/math&gt; is  the [[Event Segment#Null event segment|null event segment]], i.e.  &lt;math&gt; \omega=\epsilon_{[t, t+dt]}&lt;/math&gt;, the state trajectory in terms of [[Timed Event System]] is   
&lt;center&gt; &lt;math&gt; \Delta(q, \omega)=((\ldots,(s_i, t_{ei}+dt),\ldots), t_e+dt).&lt;/math&gt; &lt;/center&gt;

; For View2 
Given a total state &lt;math&gt; q=(s,t_s,t_e) \in Q &lt;/math&gt;  where &lt;math&gt; s = (\ldots,(s_i, t_{si}, t_{ei}),\ldots) &lt;/math&gt;

If [[Event Segment#Unit event segment|unit event segment]] &lt;math&gt; \omega&lt;/math&gt; is  the [[Event Segment#Null event segment|null event segment]], i.e.  &lt;math&gt; \omega=\epsilon_{[t, t+dt]}&lt;/math&gt;, the state trajectory in terms of [[Timed Event System]] is   
&lt;center&gt; &lt;math&gt; \Delta(q, \omega)=((\ldots,(s_i,t_{si}, t_{ei}+dt),\ldots),  t_{s}, t_e+dt).&lt;/math&gt; &lt;/center&gt;

== Remarks ==
# The behavior of a couple DEVS network whose all sub-components are [[DEVS#Deterministic DEVS and Non-deterministic DEVS|''deterministic DEVS'']] models can be ''non-deterministic'' if &lt;math&gt; Select(IMM(s))&lt;/math&gt; is ''non-deterministic''.

==See also==
*[[DEVS]]
*[[Behavior of DEVS|Behavior of Atomic DEVS]]
*[[Simulation Algorithms for Coupled DEVS]]
*[[Simulation Algorithms for Atomic DEVS]]

== References ==
* [Zeigler84] {{cite book|author = Bernard Zeigler | year = 1984| title = Multifacetted Modeling and Discrete Event Simulation | publisher = Academic Press, London; Orlando | isbn = 978-0-12-778450-2  }}
* [ZKP00] {{cite book|author1=Bernard Zeigler |author2=Tag Gon Kim |author3=Herbert Praehofer | year = 2000| title = Theory of Modeling and Simulation| publisher = Academic Press, New York  | isbn= 978-0-12-778455-7 |edition=second}}

{{DEFAULTSORT:Behavior Of Coupled Devs}}
[[Category:Automata (computation)]]
[[Category:Formal specification languages]]</text>
      <sha1>4id6c5a2ouuf2wo8w835q1xe861iguf</sha1>
    </revision>
  </page>
  <page>
    <title>Big O notation</title>
    <ns>0</ns>
    <id>44578</id>
    <revision>
      <id>871558449</id>
      <parentid>871557290</parentid>
      <timestamp>2018-12-01T22:36:32Z</timestamp>
      <contributor>
        <username>Sapphorain</username>
        <id>12923157</id>
      </contributor>
      <comment>/* The Hardy–Littlewood definition */ Carried out the deeply silly requirement for better source</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="53394">{{Order-of-approx}}
[[File:Big-O-notation.png|300px|thumb|Example of Big O notation: {{color|#ff0000|''f''(''x'')}} ∈ 
O({{color|#0000ff|''g''(''x'')}}) as there exists ''c''&amp;nbsp;&gt;&amp;nbsp;0 (e.g., ''c''&amp;nbsp;=&amp;nbsp;1) and ''x''&lt;sub&gt;0&lt;/sub&gt; (e.g., ''x''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;5) such that {{color|#ff0000|''f''(''x'')}}&amp;nbsp;≤&amp;nbsp;''c''{{color|#0000ff|''g''(''x'')}} whenever ''x''&amp;nbsp;≥&amp;nbsp;''x''&lt;sub&gt;0&lt;/sub&gt;.]]

'''Big O notation''' is a mathematical notation that describes the [[asymptotic analysis|limiting behavior]] of a [[function (mathematics)|function]] when the [[Argument of a function|argument]] tends towards a particular value or infinity.  It is a member of a family of notations invented by [[Paul Gustav Heinrich Bachmann|Paul Bachmann]],&lt;ref name=Bachmann /&gt; [[Edmund Landau]],&lt;ref name=Landau /&gt; and others, collectively called '''Bachmann–Landau notation''' or '''asymptotic notation'''.

In [[computer science]], big O notation is used to [[Computational complexity theory|classify algorithms]] according to how their running time or space requirements grow as the input size grows.&lt;ref name=quantumcomplexity&gt;{{cite web|last1=Mohr|first1=Austin|title=Quantum Computing in Complexity Theory and Theory of Computation|url=http://www.austinmohr.com/Work_files/complexity.pdf|accessdate=7 June 2014|page=2}}&lt;/ref&gt;  In [[analytic number theory]], big O notation is often used to express a bound on the difference between an [[arithmetic function|arithmetical function]] and a better understood approximation; a famous example of such a difference is the remainder term in the [[prime number theorem]].

Big O notation characterizes functions according to their growth rates: different functions with the same growth rate may be represented using the same O notation.

The letter O is used because the growth rate of a function is also referred to as the '''order of the function'''.  A description of a function in terms of big O notation usually only provides an [[upper bound]] on the growth rate of the function. Associated with big O notation are several related notations, using the symbols {{math|''o'', Ω, ω, and Θ}}, to describe other kinds of bounds on asymptotic growth rates.

Big O notation is also used in many other fields to provide similar estimates.

==Formal definition==
Let ''f'' be a real or complex valued function and ''g'' a real valued function, both defined on some unbounded subset of the [[real number|real positive numbers]], such that ''g(x)'' is strictly positive for all large enough values of ''x''.&lt;ref name=LandauO&gt;{{cite book |first=Edmund |last=Landau |authorlink=Edmund Landau |title=Handbuch der Lehre von der Verteilung der Primzahlen |publisher=B. G. Teubner |date=1909 |location=Leipzig |trans-title=Handbook on the theory of the distribution of the primes |language=de |page=31 | url=https://archive.org/stream/handbuchderlehre01landuoft#page/31/mode/2up}}&lt;/ref&gt; One writes

:&lt;math&gt;f(x)=O(g(x))\text{ as }x\to\infty&lt;/math&gt;

[[if and only if]] for all sufficiently large values of ''x'', the absolute value of ''f''(''x'') is at most a positive constant multiple of ''g''(''x''). That is, ''f''(''x'')&amp;nbsp;=&amp;nbsp;''O''(''g''(''x'')) if and only if there exists a positive real number ''M'' and a real number ''x''&lt;sub&gt;0&lt;/sub&gt; such that

:&lt;math&gt;|f(x)| \le \; M g(x)\text{ for all }x \ge x_0.&lt;/math&gt;

In many contexts, the assumption that we are interested in the growth rate as the variable ''x'' goes to infinity is left unstated, and one writes more simply that

:&lt;math&gt;f(x) = O(g(x)).&lt;/math&gt;

The notation can also be used to describe the behavior of ''f'' near some real number ''a'' (often, ''a''&amp;nbsp;=&amp;nbsp;0): we say

:&lt;math&gt;f(x)=O(g(x))\text{ as }x\to a&lt;/math&gt;

if and only if there exist positive numbers ''δ'' and ''M'' such that

:&lt;math&gt;|f(x)| \le \; M g(x)\text{ when } 0 &lt; |x - a| &lt; \delta.&lt;/math&gt;

As ''g''(''x'') is chosen to be non-zero for values of ''x'' [[Mathematical jargon#sufficiently large|sufficiently close]] to ''a'', both of these definitions can be unified using the [[limit superior]]:

:&lt;math&gt;f(x)=O(g(x))\text{ as }x \to a&lt;/math&gt;

if and only if

:&lt;math&gt;\limsup_{x\to a} \left|\frac{f(x)}{g(x)}\right| &lt; \infty.&lt;/math&gt; &lt;!-- These absolute value bars may be left out. --&gt;

==Example==
In typical usage, the formal definition of ''O'' notation is not used directly; rather, the ''O'' notation for a function ''f'' is derived by the following simplification rules:
*If ''f''(''x'') is a sum of several terms, if there is one with largest growth rate, it can be kept, and all others omitted.
*If ''f''(''x'') is a product of several factors, any constants (terms in the product that do not depend on ''x'') can be omitted.
For example, let ''f''(''x'') = 6''x''&lt;sup&gt;4&lt;/sup&gt;&amp;nbsp;−&amp;nbsp;2''x''&lt;sup&gt;3&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;5, and suppose we wish to simplify this function, using ''O'' notation, to describe its growth rate as ''x'' approaches infinity. This function is the sum of three terms: 6''x''&lt;sup&gt;4&lt;/sup&gt;, −2''x''&lt;sup&gt;3&lt;/sup&gt;, and 5. Of these three terms, the one with the highest growth rate is the one with the largest exponent as a function of ''x'', namely 6''x''&lt;sup&gt;4&lt;/sup&gt;. Now one may apply the second rule: 6''x''&lt;sup&gt;4&lt;/sup&gt; is a product of 6 and ''x''&lt;sup&gt;4&lt;/sup&gt; in which the first factor does not depend on ''x''. Omitting this factor results in the simplified form ''x''&lt;sup&gt;4&lt;/sup&gt;. Thus, we say that ''f''(''x'') is a "big-oh" of (''x''&lt;sup&gt;4&lt;/sup&gt;). Mathematically, we can write ''f''(''x'')&amp;nbsp;=&amp;nbsp;''O''(''x''&lt;sup&gt;4&lt;/sup&gt;).
One may confirm this calculation using the formal definition: let ''f''(''x'')&amp;nbsp;=&amp;nbsp;6''x''&lt;sup&gt;4&lt;/sup&gt;&amp;nbsp;−&amp;nbsp;2''x''&lt;sup&gt;3&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;5 and ''g''(''x'')&amp;nbsp;=&amp;nbsp;''x''&lt;sup&gt;4&lt;/sup&gt;. Applying the [[#Formal definition|formal definition]] from above, the statement that ''f''(''x'')&amp;nbsp;=&amp;nbsp;''O''(''x''&lt;sup&gt;4&lt;/sup&gt;) is equivalent to its expansion,
:&lt;math&gt;|f(x)| \le \; M x^4&lt;/math&gt;
for some suitable choice of ''x''&lt;sub&gt;0&lt;/sub&gt; and ''M'' and for all ''x''&amp;nbsp;&amp;gt;&amp;nbsp;''x''&lt;sub&gt;0&lt;/sub&gt;. To prove this, let ''x''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;1 and ''M''&amp;nbsp;=&amp;nbsp;13. Then, for all ''x''&amp;nbsp;&amp;gt;&amp;nbsp;''x''&lt;sub&gt;0&lt;/sub&gt;:
:&lt;math&gt;\begin{align}|6x^4 - 2x^3 + 5| &amp;\le 6x^4 + |2x^3| + 5\\
                                      &amp;\le 6x^4 + 2x^4 + 5x^4\\
                                      &amp;= 13x^4\end{align}&lt;/math&gt;
so
:&lt;math&gt; |6x^4 - 2x^3 + 5| \le 13 \,x^4 .&lt;/math&gt;

==Usage==
Big O notation has two main areas of application:
* in [[mathematics]], it is commonly used to describe [[Big_O_notation#Infinitesimal_asymptotics|how closely a finite series approximates a given function]], especially in the case of a truncated [[Taylor series]] or [[asymptotic expansion]] 
* in [[computer science]], it is useful in the [[Big_O_notation#Infinite_asymptotics|analysis of algorithms]] 

In both applications, the function ''g''(''x'') appearing within the ''O''(...) is typically chosen to be as simple as possible, omitting constant factors and lower order terms.

There are two formally close, but noticeably different, usages of this notation: 
* [[Infinity|infinite]] asymptotics 
* [[infinitesimal]] asymptotics. 

This distinction is only in application and not in principle, however—the formal definition for the "big O" is the same for both cases, only with different limits for the function argument.

===Infinite asymptotics===
{{comparison computational complexity.svg}}
Big O notation is useful when [[analysis of algorithms|analyzing algorithms]] for efficiency.  For example, the time (or the number of steps) it takes to complete a problem of size ''n'' might be found to be ''T''(''n'') = 4''n''&lt;sup&gt;2&lt;/sup&gt; − 2''n'' + 2.
As ''n'' grows large, the ''n''&lt;sup&gt;2&lt;/sup&gt; [[term (mathematics)|term]] will come to dominate, so that all other terms can be neglected—for instance when ''n'' = 500, the term 4''n''&lt;sup&gt;2&lt;/sup&gt; is 1000 times as large as the 2''n'' term. Ignoring the latter would have negligible effect on the expression's value for most purposes.
Further, the [[coefficient]]s become irrelevant if we compare to any other [[Orders of approximation|order]] of expression, such as an expression containing a term ''n''&lt;sup&gt;3&lt;/sup&gt; or ''n''&lt;sup&gt;4&lt;/sup&gt;.  Even if ''T''(''n'') = 1,000,000''n''&lt;sup&gt;2&lt;/sup&gt;, if ''U''(''n'') = ''n''&lt;sup&gt;3&lt;/sup&gt;, the latter will always exceed the former once ''n'' grows larger than 1,000,000 (''T''(1,000,000) = 1,000,000&lt;sup&gt;3&lt;/sup&gt;= ''U''(1,000,000)). Additionally, the number of steps depends on the details of the machine model on which the algorithm runs, but different types of machines typically vary by only a constant factor in the number of steps needed to execute an algorithm.
So the big O notation captures what remains: we write either
:&lt;math&gt;\ T(n)= O(n^2) &lt;/math&gt;
or
:&lt;math&gt;T(n)\in O(n^2) &lt;/math&gt;
and say that the algorithm has ''order of n&lt;sup&gt;2&lt;/sup&gt;'' time complexity.
Note that "=" is not meant to express "is equal to" in its normal mathematical sense, but rather a more colloquial "is", so the second expression is sometimes considered more accurate (see the "[[#Equals sign|Equals sign]]" discussion below) while the first is considered by some as an [[abuse of notation]].&lt;ref name="Introduction to Algorithms"&gt;Thomas H. Cormen et al., 2001, [http://highered.mcgraw-hill.com/sites/0070131511/ Introduction to Algorithms, Second Edition]{{page needed|date=February 2016}}&lt;/ref&gt;

===Infinitesimal asymptotics===
Big O can also be used to describe the [[Taylor_series#Approximation_error_and_convergence|error term]] in an approximation to a mathematical function. The most significant terms are written explicitly, and then the least-significant terms are summarized in a single big O term.  Consider, for example, the [[Exponential function#Formal definition|exponential series]] and two expressions of it that are valid when ''x'' is small:
:&lt;math&gt;\begin{align}
e^x &amp;=1+x+\frac{x^2}{2!}+\frac{x^3}{3!}+\frac{x^4}{4!}+\dotsb &amp;\text{for all } x\\
    &amp;=1+x+\frac{x^2}{2}+O(x^3)                             &amp;\text{as } x\to 0\\
    &amp;=1+x+O(x^2)                                           &amp;\text{as } x\to 0\\
\end{align}&lt;/math&gt;
The second expression (the one with ''O''(''x''&lt;sup&gt;3&lt;/sup&gt;)) means the absolute-value of the error ''e''&lt;sup&gt;''x''&lt;/sup&gt; − (1 + ''x'' + ''x''&lt;sup&gt;2&lt;/sup&gt;/2) is at most some constant times {{!}}''x''&lt;sup&gt;3&lt;/sup&gt;{{!}} when ''x'' is close enough to&amp;nbsp;0.

==Properties==
If the function ''f'' can be written as a finite sum of other functions, then the fastest growing one determines the order of
''f''(''n''). For example, 
:&lt;math&gt;f(n) = 9 \log n + 5 (\log n)^3 + 3n^2 + 2n^3 = O(n^3) \,, \qquad\text{as } n\to\infty  \,\!.&lt;/math&gt; &lt;!-- note: "\,\!" forces TeX rendering --&gt;
In particular, if a function may be bounded by a polynomial in ''n'', then as ''n'' tends to ''infinity'', one may disregard ''lower-order'' terms of the polynomial. 
Another thing to notice is the sets ''O''(''n''&lt;sup&gt;''c''&lt;/sup&gt;) and ''O''(''c''&lt;sup&gt;''n''&lt;/sup&gt;) are very different. If ''c'' is greater than one, then the latter grows much faster. A function that grows faster than ''n''&lt;sup&gt;''c''&lt;/sup&gt; for any ''c''  is called ''superpolynomial''.  One that grows more slowly than any exponential function of the form ''c''&lt;sup&gt;''n''&lt;/sup&gt; is called ''subexponential''. An algorithm can require time that is both superpolynomial and subexponential; examples of this include the fastest known algorithms for [[integer factorization]] and the function ''n''&lt;sup&gt;log ''n''&lt;/sup&gt;.

We may ignore any powers of ''n'' inside of the logarithms. The set ''O''(log ''n'') is exactly the same as ''O''(log(''n''&lt;sup&gt;''c''&lt;/sup&gt;)). The logarithms differ only by a constant factor (since
log(''n''&lt;sup&gt;''c''&lt;/sup&gt;) = ''c'' log ''n'') and thus the big O notation ignores that. Similarly, logs with different constant bases are equivalent. On the other hand, exponentials with different bases are not of the same order. For example, 2&lt;sup&gt;''n''&lt;/sup&gt; and 3&lt;sup&gt;''n''&lt;/sup&gt; are not of the same order.

Changing units may or may not affect the order of the resulting algorithm. Changing units is equivalent to multiplying the appropriate variable by a constant wherever it appears. For example, if an algorithm runs in the order of ''n''&lt;sup&gt;2&lt;/sup&gt;, replacing ''n'' by ''cn'' means the algorithm runs in the order of ''c''&lt;sup&gt;2&lt;/sup&gt;''n''&lt;sup&gt;2&lt;/sup&gt;, and the big O notation ignores the constant ''c''&lt;sup&gt;2&lt;/sup&gt;. This can be written as ''c''&lt;sup&gt;2&lt;/sup&gt;''n''&lt;sup&gt;2&lt;/sup&gt; = O(''n''&lt;sup&gt;2&lt;/sup&gt;). If, however, an algorithm runs in the order of 2&lt;sup&gt;''n''&lt;/sup&gt;, replacing ''n'' with ''cn'' gives 2&lt;sup&gt;''cn''&lt;/sup&gt; = (2&lt;sup&gt;''c''&lt;/sup&gt;)&lt;sup&gt;n&lt;/sup&gt;. This is not equivalent to 2&lt;sup&gt;''n''&lt;/sup&gt; in general.
Changing variables may also affect the order of the resulting algorithm. For example, if an algorithm's run time is ''O''(''n'') when measured in terms of the number ''n'' of ''digits'' of an input number ''x'', then its run time is ''O''(log ''x'') when measured as a function of the input number ''x'' itself, because ''n'' = ''O''(log ''x'').

===Product===
:&lt;math&gt; f_1 = O(g_1) \text{ and } f_2 = O(g_2)\, \Rightarrow f_1  f_2 = O(g_1  g_2)&lt;/math&gt;
:&lt;math&gt;f\cdot O(g) = O(f g)&lt;/math&gt;

===Sum===
:&lt;math&gt; f_1 = O(g_1) \text{ and }
  f_2= O(g_2)\, \Rightarrow f_1 + f_2 = O(g_1 + g_2)&lt;/math&gt;
This implies &lt;math&gt; f_1 = O(g) \text{ and } f_2 = O(g) \Rightarrow f_1+f_2 \in O(g) &lt;/math&gt;, which means that &lt;math&gt;O(g)&lt;/math&gt; is a [[convex cone]].

===Multiplication by a constant===
:Let ''k'' be constant. Then:
:&lt;math&gt;\ O(|k| g) = O(g)&lt;/math&gt; if ''k'' is nonzero.
:&lt;math&gt;f= O(g) \Rightarrow kf = O(g). &lt;/math&gt;

==Multiple variables==
Big ''O'' (and little o, and Ω...) can also be used with multiple variables.
To define Big ''O'' formally for multiple variables, suppose &lt;math&gt;f&lt;/math&gt; and &lt;math&gt;g&lt;/math&gt; are two functions defined on some subset of &lt;math&gt;\mathbb{R}^n&lt;/math&gt;. We say
:&lt;math&gt;f(\vec{x})\text{ is }O(g(\vec{x}))\text{ as }\vec{x}\to\infty&lt;/math&gt;
if and only if&lt;ref&gt;{{cite book|last1=Cormen|first1=Thomas|last2=Leiserson|first2=Charles|last3=Rivest|first3=Ronald|last4=Stein|first4=Clifford|title=Introduction to Algorithms|date=2009|publisher=MIT|page=53|edition=Third}}&lt;/ref&gt;
:&lt;math&gt;\exists M\,\exists C&gt;0\text{ such that for all }\vec{x} \text{ with } x_i \ge M \text{ for some }i, |f(\vec{x})| \le C |g(\vec{x})|.&lt;/math&gt;
Equivalently, the condition that &lt;math&gt;x_i \geq M&lt;/math&gt; for some &lt;math&gt;i&lt;/math&gt; can be replaced with the condition that &lt;math&gt;\|\vec{x}\|_{\infty} \ge M&lt;/math&gt;, where &lt;math&gt;\|\vec{x}\|_{\infty}&lt;/math&gt; denotes the [[Chebyshev norm]]. For example, the statement
:&lt;math&gt;f(n,m) = n^2 + m^3 + O(n+m) \text{ as } n,m\to\infty&lt;/math&gt;
asserts that there exist constants ''C'' and ''M'' such that
:&lt;math&gt;\forall \|(n, m)\|_{\infty} \geq M:  |g(n,m)| \le C|n+m|,&lt;/math&gt;
where ''g''(''n'',''m'') is defined by
:&lt;math&gt;f(n,m) = n^2 + m^3 + g(n,m).&lt;/math&gt;
Note that this definition allows all of the coordinates of &lt;math&gt;\vec{x}&lt;/math&gt; to increase to infinity. In particular, the statement
:&lt;math&gt;f(n,m) = O(n^m) \text{ as } n,m\to\infty&lt;/math&gt;
(i.e., &lt;math&gt;\exists C\,\exists M\,\forall n\,\forall m\dots&lt;/math&gt;) is quite different from
:&lt;math&gt;\forall m\colon f(n,m) = O(n^m) \text{ as } n\to\infty&lt;/math&gt;
(i.e., &lt;math&gt;\forall m\,\exists C\,\exists M\,\forall n\dots&lt;/math&gt;).

Note that under this definition, the subset on which a function is defined is significant when generalizing statements from the univariate setting to the multivariate setting. For example, if &lt;math&gt;f(n,m)=1&lt;/math&gt; and &lt;math&gt;g(n,m)=n&lt;/math&gt;, then &lt;math&gt;f(n,m)=O(g(n,m))&lt;/math&gt; if we restrict &lt;math&gt;f&lt;/math&gt; and &lt;math&gt;g&lt;/math&gt; to &lt;math&gt;[1,\infty)^2&lt;/math&gt;, but not if they are defined on &lt;math&gt;[0,\infty)^2&lt;/math&gt;.

This is not the only generalization of big O to multivariate functions, and in practice, there is some inconsistency in the choice of definition.&lt;ref&gt;{{cite web|last1=Howell|first1=Rodney|title=On Asymptotic Notation with Multiple Variables|url=http://people.cis.ksu.edu/~rhowell/asymptotic.pdf|accessdate=2015-04-23}}&lt;/ref&gt;

==Matters of notation==

===Equals sign===
The statement "''f''(''x'') is ''O''(''g''(''x''))" as defined above is usually written as ''f''(''x'')&amp;nbsp;=&amp;nbsp;''O''(''g''(''x'')). Some consider this to be an [[abuse of notation]], since the use of the equals sign could be misleading as it suggests a symmetry that this statement does not have. As [[Nicolaas Govert de Bruijn|de Bruijn]] says, ''O''(''x'')&amp;nbsp;=&amp;nbsp;''O''(''x''&lt;sup&gt;2&lt;/sup&gt;) is true but ''O''(''x''&lt;sup&gt;2&lt;/sup&gt;)&amp;nbsp;=&amp;nbsp;''O''(''x'') is not.&lt;ref&gt;{{Cite book| author = [[N. G. de Bruijn]] | title=Asymptotic Methods in Analysis | place=Amsterdam |publisher=North-Holland | year=1958 | pages=5–7 | url=https://books.google.com/books?id=_tnwmvHmVwMC&amp;pg=PA5&amp;vq=%22The+trouble+is%22 | isbn=978-0-486-64221-5}}&lt;/ref&gt; [[Donald Knuth|Knuth]] describes such statements as "one-way equalities", since if the sides could be reversed, "we could deduce ridiculous things like ''n''&amp;nbsp;=&amp;nbsp;''n''&lt;sup&gt;2&lt;/sup&gt; from the identities ''n''&amp;nbsp;=&amp;nbsp;''O''(''n''&lt;sup&gt;2&lt;/sup&gt;) and ''n''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;''O''(''n''&lt;sup&gt;2&lt;/sup&gt;)."&lt;ref name="Concrete Mathematics"&gt;{{Cite book |last1=Graham |first1=Ronald |author1link=Ronald Graham |first2=Donald |last2=Knuth |author2link=Donald Knuth |last3=Patashnik |first3=Oren |author3link=Oren Patashnik |title=Concrete Mathematics |location=Reading, Massachusetts |publisher=Addison–Wesley |edition=2 |date=1994 |page=446 |url=https://books.google.com/books?id=pntQAAAAMAAJ |isbn=978-0-201-55802-9}}&lt;/ref&gt;

For these reasons, it would be more precise to use [[set notation]] and write ''f''(''x'')&amp;nbsp;∈&amp;nbsp;''O''(''g''(''x'')), thinking of ''O''(''g''(''x'')) as the class of all functions ''h''(''x'') such that |''h''(''x'')|&amp;nbsp;≤&amp;nbsp;''C''|''g''(''x'')| for some constant ''C''.&lt;ref name="Concrete Mathematics"/&gt; However, the use of the equals sign is customary.  Knuth pointed out that "mathematicians customarily use the = sign as they use the word 'is' in English: Aristotle is a man, but a man isn't necessarily Aristotle."&lt;ref&gt;{{Cite journal| author=Donald Knuth | title=Teach Calculus with Big O | date=June–July 1998 | journal=[[Notices of the American Mathematical Society]] | volume=45 | issue=6 | page=687 | url=http://www.ams.org/notices/199806/commentary.pdf}} ([http://www-cs-staff.stanford.edu/~knuth/ocalc.tex Unabridged version])&lt;/ref&gt;

===Other arithmetic operators===
Big O notation can also be used in conjunction with other arithmetic operators in more complicated equations.   For example, ''h''(''x'') + ''O''(''f''(''x'')) denotes the collection of functions having the growth of ''h''(''x'') plus a part whose growth is limited to that of ''f''(''x''). Thus,
:&lt;math&gt;g(x) = h(x) + O(f(x))&lt;/math&gt;
expresses the same as
:&lt;math&gt;g(x) - h(x) = O(f(x))\,.&lt;/math&gt;

====Example {{anchor|Example (Matters of notation)}}====
Suppose an [[algorithm]] is being developed to operate on a set of ''n'' elements. Its developers are interested in finding a function ''T''(''n'')  that will express how long the algorithm will take to run (in some arbitrary measurement of time) in terms of the number of elements in the input set. The algorithm works by first calling a subroutine to sort the elements in the set and then perform its own operations. The sort has a known time complexity of ''O''(''n''&lt;sup&gt;2&lt;/sup&gt;), and after the subroutine runs the algorithm must take an additional  55''n''&lt;sup&gt;3&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;2''n''&amp;nbsp;+&amp;nbsp;10 steps before it terminates.  Thus the overall time complexity of the algorithm can be expressed as ''T''(''n'') = 55''n''&lt;sup&gt;3&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''O''(''n''&lt;sup&gt;2&lt;/sup&gt;).
Here the terms 2''n''+10 are subsumed within the faster-growing ''O''(''n''&lt;sup&gt;2&lt;/sup&gt;).  Again, this usage disregards some of the formal meaning of the "=" symbol, but it does allow one to use the big O notation as a kind of convenient placeholder.

===Multiple usages===
In more complicated usage, ''O''(...) can appear in different places in an equation, even several times on each side. For example, the following are true for &lt;math&gt;n\to\infty&lt;/math&gt;
:&lt;math&gt;(n+1)^2 = n^2 + O(n)&lt;/math&gt;
:&lt;math&gt;(n+O(n^{1/2}))(n + O(\log n))^2 = n^3 + O(n^{5/2})&lt;/math&gt;
:&lt;math&gt;n^{O(1)} = O(e^n).&lt;/math&gt;
The meaning of such statements is as follows: for ''any'' functions which satisfy each ''O''(...) on the left side, there are ''some'' functions satisfying each ''O''(...) on the right side, such that substituting all these functions into the equation makes the two sides equal. For example, the third equation above means: "For any function ''f''(''n'') = ''O''(1), there is some function ''g''(''n'')  = ''O''(''e''&lt;sup&gt;''n''&lt;/sup&gt;) such that ''n''&lt;sup&gt;''f''(''n'')&lt;/sup&gt; = ''g''(''n'')." In terms of the "set notation" above, the meaning is that the class of functions represented by the left side is a subset of the class of functions represented by the right side. In this use the "=" is a formal symbol that unlike the usual use of "=" is not a [[symmetric relation]]. Thus for example ''n''&lt;sup&gt;''O''(1)&lt;/sup&gt; = ''O''(''e''&lt;sup&gt;''n''&lt;/sup&gt;) does not imply the false statement ''O''(''e''&lt;sup&gt;''n''&lt;/sup&gt;) = ''n''&lt;sup&gt;''O''(1)&lt;/sup&gt;

===Typesetting===
Big O consists of just an uppercase "O".  Unlike Greek-named Bachmann–Landau notations, it needs no special symbol.  Yet, commonly used calligraphic variants, like &lt;math&gt;\mathcal{O}&lt;/math&gt;, are available, in [[LaTeX]] and derived typesetting systems.&lt;ref&gt;{{cite web |url=https://texblog.org/2014/06/24/big-o-and-related-notations-in-latex/ |title=Big O and related notations in LaTeX |date=24 June 2014 |author=Tom |work=texblog}}&lt;/ref&gt;

==Orders of common functions==
{{Further|Time complexity#Table of common time complexities}}

Here is a list of classes of functions that are commonly encountered when analyzing the running time of an algorithm.  In each case, ''c'' is a positive constant and ''n'' increases without bound. The slower-growing functions are generally listed first.

{| class="wikitable"
|-
!Notation !! Name !! Example
|-
|&lt;math&gt;O(1)&lt;/math&gt; || [[Constant time|constant]] || Determining if a binary number is even or odd; Calculating &lt;math&gt;(-1)^n&lt;/math&gt;; Using a constant-size [[lookup table]]
|-
|&lt;math&gt;O(\log \log n)&lt;/math&gt; || double logarithmic || Number of comparisons spent finding an item using [[interpolation search]] in a sorted array of uniformly distributed values
|-
|&lt;math&gt;O(\log n)&lt;/math&gt; || [[Logarithmic time|logarithmic]] || Finding an item in a sorted array with a [[Binary search algorithm|binary search]] or a balanced search [[Tree data structure|tree]] as well as all operations in a [[Binomial heap]]
|-
|&lt;math&gt;O((\log n)^c)&lt;/math&gt;&lt;br/&gt;&lt;math&gt;\scriptstyle c&gt;1&lt;/math&gt; || [[Polylogarithmic time|polylogarithmic]] || Matrix chain ordering can be solved in polylogarithmic time on a [[parallel random-access machine]].
|-
|&lt;math&gt;O(n^c)&lt;/math&gt;&lt;br/&gt;&lt;math&gt;\scriptstyle 0&lt;c&lt;1&lt;/math&gt; || fractional power || Searching in a [[k-d tree]]
|-
|&lt;math&gt;O(n)&lt;/math&gt; || [[linear time|linear]] || Finding an item in an unsorted list or in an unsorted array; adding two ''n''-bit integers by [[Ripple carry adder|ripple carry]]
|-
|&lt;math&gt;O(n\log^* n)&lt;/math&gt; || n [[log-star]] n || Performing [[Polygon triangulation|triangulation]] of a simple polygon using [[Kirkpatrick–Seidel_algorithm|Seidel's algorithm]], or the [[Proof of O(log*n) time complexity of union–find|union–find algorithm]]. Note that &lt;math&gt;\log^*(n) =
\begin{cases}
 0, &amp; \text{if }n \leq 1 \\
 1 + \log^*(\log n), &amp; \text{if }n&gt;1
\end{cases}&lt;/math&gt;
|-
|&lt;math&gt;O(n\log n)=O(\log n!)&lt;/math&gt; || [[Linearithmic time|linearithmic]], loglinear, or quasilinear || Performing a [[fast Fourier transform]]; Fastest possible [[comparison sort]]; [[heapsort]] and [[merge sort]]
|-
|&lt;math&gt;O(n^2)&lt;/math&gt; || [[quadratic time|quadratic]] || Multiplying two ''n''-digit numbers by a simple algorithm; simple sorting algorithms, such as [[bubble sort]], [[selection sort]] and [[insertion sort]]; (worst case) bound on some usually faster sorting algorithms such as [[quicksort]], [[Shellsort]], and [[tree sort]]
|-
|&lt;math&gt;O(n^c)&lt;/math&gt; || [[polynomial time|polynomial]] or algebraic || [[Tree-adjoining grammar]] parsing; maximum [[Matching (graph theory)|matching]] for [[bipartite graph]]s; finding the [[determinant]] with [[LU decomposition]]
|-
|&lt;math&gt;L_n[\alpha,c] = e^{(c+o(1)) (\ln n)^\alpha (\ln \ln n)^{1-\alpha}}&lt;/math&gt;&lt;br/&gt;&lt;math&gt;\scriptstyle 0 &lt; \alpha &lt; 1&lt;/math&gt; || [[L-notation]] or [[sub-exponential time|sub-exponential]] || Factoring a number using the [[quadratic sieve]] or [[number field sieve]]
|-
|&lt;math&gt;O(c^n)&lt;/math&gt;&lt;br/&gt;&lt;math&gt;\scriptstyle c&gt;1&lt;/math&gt; || [[exponential time|exponential]] || Finding the (exact) solution to the [[travelling salesman problem]] using [[dynamic programming]]; determining if two logical statements are equivalent using [[brute-force search]]
|-
|&lt;math&gt;O(n!)&lt;/math&gt; || [[factorial]] || Solving the [[travelling salesman problem]] via brute-force search; generating all unrestricted permutations of a [[Partially ordered set|poset]]; finding the [[determinant]] with [[Laplace expansion]]; enumerating [[Bell number|all partitions of a set]]
|}
The statement &lt;math&gt;f(n)=O(n!)&lt;/math&gt; is sometimes weakened to &lt;math&gt;f(n)=O\left(n^n\right)&lt;/math&gt; to derive simpler formulas for asymptotic complexity.
For any &lt;math&gt;k&gt;0&lt;/math&gt; and &lt;math&gt;c&gt;0&lt;/math&gt;, &lt;math&gt;O(n^c(\log n)^k)&lt;/math&gt; is a subset of &lt;math&gt;O(n^{c+\varepsilon })&lt;/math&gt; for any &lt;math&gt; \varepsilon &gt;0&lt;/math&gt;, so may be considered as a polynomial with some bigger order.

==Related asymptotic notations==
Big ''O'' is the most commonly used asymptotic notation for comparing functions.{{cn|date=December 2017}} Together with some  other related notations it forms the family of Bachmann–Landau notations.

===Little-o notation=== &lt;!-- [[Little-o notation]] redirects here --&gt;
{{redirect|Little o|the baseball player|Omar Vizquel}}
Intuitively, the assertion "{{math|''f''(''x'')}} is {{math|''o''(''g''(''x''))}}" (read "{{math|''f''(''x'')}} is little-o of {{math|''g''(''x'')}}") means that {{math|''g''(''x'')}} grows much faster than {{math|''f''(''x'')}}. Let as before ''f'' be a real or complex valued function and ''g'' a real valued function, both defined on some unbounded subset of the [[real number|real positive numbers]], such that ''g(x)'' is strictly positive for all large enough values of ''x''. One writes
:&lt;math&gt;f(x)=o(g(x))\text{ as }x\to\infty&lt;/math&gt;
If for every positive constant {{math|''ε''}} there exists a constant {{math|''N''}} such that
:&lt;math&gt;|f(x)|\leq\varepsilon g(x)\qquad\text{for all }x\geq N~.&lt;/math&gt;&lt;ref name=Landausmallo&gt;{{cite book |first=Edmund |last=Landau |authorlink=Edmund Landau |title=Handbuch der Lehre von der Verteilung der Primzahlen |publisher=B. G. Teubner |date=1909 |location=Leipzig |trans-title=Handbook on the theory of the distribution of the primes |language=de |page=61 | url=https://archive.org/stream/handbuchderlehre01landuoft#page/61/mode/2up}}&lt;/ref&gt;  For example, one has
: &lt;math&gt;2x  = o(x^2) &lt;/math&gt; and &lt;math&gt;1/x = o(1).&lt;/math&gt;  

The difference between the earlier [[#Formal definition|definition]] for the big-O notation and the present definition of little-o, is that while the former has to be true for ''at least one'' constant ''M'', the latter must hold for ''every'' positive constant {{math|''ε''}}, however small.&lt;ref name="Introduction to Algorithms"/&gt; In this way, little-o notation makes a ''stronger statement'' than the corresponding big-O notation: every function that is little-o of ''g'' is also big-O of ''g'', but not every function that is big-O of ''g'' is also little-o of ''g''.  For example, &lt;math&gt;2x^2 = O(x^2) &lt;/math&gt; but &lt;math&gt;2x^2 \neq o(x^2). &lt;/math&gt;

As ''g''(''x'') is nonzero, or at least becomes nonzero beyond a certain point, the relation {{math|''f''(''x'')&amp;nbsp;{{=}}&amp;nbsp;''o''(''g''(''x''))}} is equivalent to
:&lt;math&gt;\lim_{x \to \infty}\frac{f(x)}{g(x)}=0&lt;/math&gt; (and this is in fact how Landau&lt;ref name=Landausmallo /&gt; originally defined the little-o notation). 

Little-o respects a number of arithmetic operations.  For example,
: if {{mvar|c}} is a nonzero constant and &lt;math&gt;f = o(g)&lt;/math&gt; then &lt;math&gt;c \cdot f = o(g)&lt;/math&gt;, and
: if &lt;math&gt;f = o(F)&lt;/math&gt; and &lt;math&gt;g = o(G)&lt;/math&gt; then &lt;math&gt; f \cdot g = o(F \cdot G).&lt;/math&gt;
It also satisfies a transitivity relation:
: if &lt;math&gt; f = o(g)&lt;/math&gt; and &lt;math&gt; g = o(h)&lt;/math&gt; then &lt;math&gt; f = o(h).&lt;/math&gt;

===Big Omega notation===

There are two very widespread and incompatible definitions of the statement

:&lt;math&gt;f(x)=\Omega(g(x))\ (x\rightarrow a),&lt;/math&gt;

where ''a'' is some real number, ∞,  or −∞, where ''f'' and ''g'' are real functions defined in a neighbourhood of ''a'', and where ''g'' is positive in this neighbourhood.

The first one (chronologically) is used in [[analytic number theory]], and the other one in [[computational complexity theory]]. When the two subjects meet, this situation is bound to generate confusion.

====The Hardy–Littlewood definition====

In 1914 [[Godfrey Harold Hardy]] and [[John Edensor Littlewood]] introduced the new symbol &lt;math&gt;\Omega&lt;/math&gt;,&lt;ref name="HL"&gt;{{cite journal|last1=Hardy|first1=G. H.|last2=Littlewood|first2=J. E.|title=Some problems of diophantine approximation: Part II. The trigonometrical series associated with the elliptic ϑ-functions|journal=Acta Mathematica|date=1914|volume=37|page=225|doi=10.1007/BF02401834|url=http://projecteuclid.org/download/pdf_1/euclid.acta/1485887376}}&lt;/ref&gt; which is defined as follows:

:&lt;math&gt;f(x)=\Omega(g(x))\ (x\rightarrow\infty)\;\Leftrightarrow\;\limsup_{x \to \infty} \left|\frac{f(x)}{g(x)}\right|&gt; 0.&lt;/math&gt;

Thus &lt;math&gt;f(x)=\Omega(g(x))&lt;/math&gt; is the negation of &lt;math&gt;f(x)=o(g(x))&lt;/math&gt;.

In 1916 the same authors introduced the two new symbols &lt;math&gt;\Omega_R&lt;/math&gt; and &lt;math&gt;\Omega_L&lt;/math&gt;, defined thusly:&lt;ref name="HL2"&gt;G. H. Hardy and J. E. Littlewood, « Contribution to the theory of the Riemann zeta-function and the theory of the distribution of primes », ''[[Acta Mathematica]]'', vol. 41, 1916.&lt;/ref&gt;

:&lt;math&gt;f(x)=\Omega_R(g(x))\ (x\rightarrow\infty)\;\Leftrightarrow\;\limsup_{x \to \infty} \frac{f(x)}{g(x)}&gt; 0&lt;/math&gt;;

:&lt;math&gt;f(x)=\Omega_L(g(x))\ (x\rightarrow\infty)\;\Leftrightarrow\;\liminf_{x \to \infty} \frac{f(x)}{g(x)}&lt; 0. &lt;/math&gt;

These symbols were used by [[Edmund Landau]], with the same meanings, in 1924.&lt;ref name="landau"&gt;E. Landau, "Über die Anzahl der Gitterpunkte in gewissen Bereichen. IV." Nachr. Gesell. Wiss. Gött. Math-phys. Kl. 1924, 137–150.&lt;/ref&gt;  After Landau, the notations were never used again exactly thus; &lt;math&gt;\Omega_R&lt;/math&gt; became &lt;math&gt;\Omega_+&lt;/math&gt; and &lt;math&gt;\Omega_L&lt;/math&gt; became &lt;math&gt;\Omega_-&lt;/math&gt;.{{cn|date=December 2018}}

These three symbols &lt;math&gt;\Omega, \Omega_+, \Omega_-&lt;/math&gt;, as well as &lt;math&gt;f(x)=\Omega_\pm(g(x))&lt;/math&gt; (meaning that &lt;math&gt;f(x)=\Omega_+(g(x))&lt;/math&gt; and &lt;math&gt;f(x)=\Omega_-(g(x))&lt;/math&gt; are both satisfied), are now currently used in [[analytic number theory]].&lt;ref name=Ivic&gt;Aleksandar Ivić. The Riemann zeta-function, chapter 9. John Wiley &amp; Sons 1985.&lt;/ref&gt;&lt;ref&gt;Gérald Tenenbaum, Introduction to analytic and probabilistic number theory, Chapter I.5. American Mathematical Society, Providence RI, 2015.&lt;/ref&gt;

=====Simple examples=====

We have

:&lt;math&gt;\sin x=\Omega(1)\ (x\rightarrow\infty),&lt;/math&gt;

and more precisely

:&lt;math&gt;\sin x=\Omega_\pm(1)\ (x\rightarrow\infty).&lt;/math&gt;

We have

:&lt;math&gt;\sin x+1=\Omega(1)\ (x\rightarrow\infty),&lt;/math&gt;

and more precisely

:&lt;math&gt;\sin x+1=\Omega_+(1)\ (x\rightarrow\infty);&lt;/math&gt;

however

:&lt;math&gt;\sin x+1\not=\Omega_-(1)\ (x\rightarrow\infty).&lt;/math&gt;

====The Knuth definition====

In 1976 [[Donald Knuth]] published a paper to justify his use of the &lt;math&gt;\Omega&lt;/math&gt;-symbol to describe a stronger property. Knuth wrote: "For all the applications I have seen so far in computer science, a stronger requirement […] is much more appropriate". He defined

:&lt;math&gt;f(x)=\Omega(g(x))\Leftrightarrow g(x)=O(f(x))&lt;/math&gt;

with the comment: "Although I have changed Hardy and Littlewood's definition of &lt;math&gt;\Omega&lt;/math&gt;, I feel justified in doing so because their definition is by no means in wide use, and because there are other ways to say what they want to say in the comparatively rare cases when their definition applies."&lt;ref name="knuth"&gt;Donald Knuth. "[http://www.phil.uu.nl/datastructuren/10-11/knuth_big_omicron.pdf Big Omicron and big Omega and big Theta]", SIGACT News, Apr.-June 1976, 18–24.&lt;/ref&gt;

===Family of Bachmann–Landau notations===
{| class="wikitable"
|-
! Notation
! Name&lt;ref name="knuth"/&gt;
! Description
! Formal Definition
! Limit Definition&lt;ref name=Balcázar&gt;{{cite journal|last1=Balcázar|first1=José L.|last2=Gabarró|first2=Joaquim|title=Nonuniform complexity classes specified by lower and upper bounds|journal=RAIRO - Theoretical Informatics and Applications - Informatique Théorique et Applications|volume=23|issue=2|page=180|url=http://archive.numdam.org/article/ITA_1989__23_2_177_0.pdf|accessdate=14 March 2017|language=en|issn=0988-3754}}&lt;/ref&gt;&lt;ref name=Cucker&gt;{{cite book|last1=Cucker|first1=Felipe|last2=Bürgisser|first2=Peter|title=Condition: The Geometry of Numerical Algorithms|date=2013|publisher=Springer|location=Berlin, Heidelberg|isbn=978-3-642-38896-5|pages=467–468|url=https://link.springer.com/book/10.1007%2F978-3-642-38896-5|accessdate=14 March 2017|chapter=A.1  Big Oh, Little Oh, and Other Comparisons}}&lt;/ref&gt;&lt;ref name=Wild&gt;{{cite journal | first1=Paul | last1=Vitányi | authorlink1=Paul Vitanyi | first2=Lambert | last2=Meertens | authorlink2=Lambert Meertens | title=Big Omega versus the wild functions | journal=ACM SIGACT News | volume=16 | issue=4 | date=April 1985 | pages=56–59 | doi=10.1145/382242.382835 | url=http://www.kestrel.edu/home/people/meertens/publications/papers/Big_Omega_contra_the_wild_functions.pdf}}&lt;/ref&gt;&lt;ref name="knuth"/&gt;&lt;ref name="HL"/&gt;
|-
| &lt;math&gt;f(n) = o(g(n))&lt;/math&gt;
| Small O; Small Oh
| {{mvar|f}} is dominated by {{mvar|g}} asymptotically
| &lt;math&gt;\forall k&gt;0 \; \exists n_0 \; \forall n&gt;n_0 \; |f(n)| &lt; k\cdot g(n)&lt;/math&gt;
| &lt;math&gt;\lim_{n \to \infty} \frac{f(n)}{g(n)} = 0&lt;/math&gt;
|-
| &lt;math&gt;f(n) = O(g(n))&lt;/math&gt;
| Big O; Big Oh; Big Omicron
| &lt;math&gt;|f|&lt;/math&gt; is bounded above by {{mvar|g}} (up to constant factor) asymptotically
| &lt;math&gt;\exists k&gt;0 \; \exists n_0 \; \forall n&gt;n_0 \; |f(n)| \leq k\cdot g(n)&lt;/math&gt;
| &lt;math&gt;\limsup_{n \to \infty} \frac{\left|f(n)\right|}{g(n)} &lt; \infty&lt;/math&gt;
|-
| &lt;math&gt;f(n) = \Theta(g(n))&lt;/math&gt;
| Big Theta
| {{mvar|f}} is bounded both above and below by {{mvar|g}} asymptotically
| &lt;math&gt;\exists k_1&gt;0 \; \exists k_2&gt;0 \; \exists n_0 \; \forall n&gt;n_0&lt;/math&gt;&lt;math&gt;k_1\cdot g(n) \leq f(n) \leq k_2\cdot g(n)&lt;/math&gt;
| &lt;math&gt;f(n) = O(g(n))&lt;/math&gt; and &lt;math&gt;f(n) = \Omega(g(n))&lt;/math&gt; (Knuth version)
|-
| &lt;math&gt;f(n)\sim g(n)&lt;/math&gt;
| On the order of
| {{mvar|f}} is equal to {{mvar|g}} asymptotically
| &lt;math&gt;\forall \varepsilon&gt;0\;\exists n_0\;\forall n&gt;n_0\;\left|{f(n) \over g(n)}-1\right|&lt;\varepsilon&lt;/math&gt;
| &lt;math&gt;\lim_{n \to \infty} {f(n) \over g(n)} = 1&lt;/math&gt;
|-
| &lt;math&gt;f(n) = \Omega(g(n))&lt;/math&gt;
| Big Omega in number theory (Hardy–Littlewood)
| &lt;math&gt;|f|&lt;/math&gt; is not dominated by {{mvar|g}} asymptotically
| &lt;math&gt;\exists k&gt;0 \; \forall n_0 \; \exists n&gt;n_0 \; |f(n)| \geq k\cdot g(n)&lt;/math&gt;
| &lt;math&gt;\limsup_{n \to \infty} \left|\frac{f(n)}{g(n)}\right| &gt; 0 &lt;/math&gt;
|-
| &lt;math&gt;f(n) = \Omega(g(n))&lt;/math&gt;
| Big Omega in complexity theory (Knuth)
| {{mvar|f}} is bounded below by {{mvar|g}} asymptotically
| &lt;math&gt;\exists k&gt;0 \; \exists n_0 \; \forall n&gt;n_0 \; f(n) \geq k\cdot g(n)&lt;/math&gt;
| &lt;math&gt;\liminf_{n \to \infty} \frac{f(n)}{g(n)} &gt; 0 &lt;/math&gt;
|-
| &lt;math&gt;f(n) = \omega(g(n))&lt;/math&gt;
| Small Omega
| {{mvar|f}} dominates {{mvar|g}} asymptotically
| &lt;math&gt;\forall k&gt;0 \; \exists n_0 \; \forall n&gt;n_0 \  |f(n)| &gt; k\cdot |g(n)|&lt;/math&gt;
| &lt;math&gt;\lim_{n \to \infty} \left|\frac{f(n)}{g(n)}\right| = \infty&lt;/math&gt;
|}

The limit definitions assume &lt;math&gt;g(n) \neq 0&lt;/math&gt; for sufficiently large {{mvar|n}}. The table is (partly) sorted from smallest to largest, in the sense that o, O, Θ, ∼, (Knuth's version of) Ω, ω on functions correspond to &lt;, ≤, ≈, =, ≥, &gt; on the real line&lt;ref name=Wild/&gt; (the Hardy-Littlewood version of Ω, however, doesn't correspond to any such description). 

Computer science uses the big ''O'', Big Theta Θ, little ''o'', little omega &amp;omega; and Knuth's big Omega Ω notations.&lt;ref&gt;{{Introduction to Algorithms|edition=2|pages=41–50}}&lt;/ref&gt; Analytic number theory often uses the big ''O'', small ''o'', Hardy–Littlewood's big Omega Ω (with or without the +, - or ± subscripts) and &lt;math&gt;\sim&lt;/math&gt; notations.&lt;ref name=Ivic/&gt; The small omega ω notation is not used as often in analysis.&lt;ref&gt;for example it is omitted in: {{cite web|last1=Hildebrand|first1=A.J.|title=Asymptotic Notations|url=http://www.math.uiuc.edu/~ajh/595ama/ama-ch2.pdf|website=Asymptotic Methods in Analysis|publisher=Math 595, Fall 2009|accessdate=14 March 2017}}&lt;/ref&gt;

=== Use in computer science ===
{{details|Analysis of algorithms}}
Informally, especially in computer science, the Big ''O'' notation often is permitted to be somewhat abused to describe an asymptotic tight bound where using Big Theta Θ notation might be more factually appropriate in a given context.{{Citation needed|reason=Wording (weasel-words) suggest a primarily opinion-based posit.|date=May 2015}} For example, when considering a function ''T''(''n'') = 73''n''&lt;sup&gt;3&lt;/sup&gt; + 22''n''&lt;sup&gt;2&lt;/sup&gt; + 58, all of the following are generally acceptable, but tighter bounds (i.e., numbers 2 and 3 below) are usually strongly preferred over looser bounds (i.e., number 1 below).
#''T''(''n'')&amp;nbsp;=&amp;nbsp;''O''(''n''&lt;sup&gt;100&lt;/sup&gt;)
#''T''(''n'')&amp;nbsp;=&amp;nbsp;''O''(''n''&lt;sup&gt;3&lt;/sup&gt;)
#''T''(''n'')&amp;nbsp;=&amp;nbsp;Θ(''n''&lt;sup&gt;3&lt;/sup&gt;)
The equivalent English statements are respectively:
#''T''(''n'') grows asymptotically no faster than ''n''&lt;sup&gt;100&lt;/sup&gt;
#''T''(''n'') grows asymptotically no faster than ''n''&lt;sup&gt;3&lt;/sup&gt;
#''T''(''n'') grows asymptotically as fast as ''n''&lt;sup&gt;3&lt;/sup&gt;.
So while all three statements are true, progressively more information is contained in each. In some fields, however, the big O notation (number 2 in the lists above) would be used more commonly than the Big Theta notation (bullets number 3 in the lists above). For example, if ''T''(''n'') represents the running time of a newly developed algorithm for input size ''n'', the inventors and users of the algorithm might be more inclined to put an upper asymptotic bound on how long it will take to run without making an explicit statement about the lower asymptotic bound.

=== Other notation ===

In their book ''[[Introduction to Algorithms]]'', [[Thomas H. Cormen|Cormen]], [[Charles E. Leiserson|Leiserson]], [[Ronald L. Rivest|Rivest]] and [[Clifford Stein|Stein]] consider the set of functions ''f'' which satisfy 

:&lt;math&gt; f(n) = O(g(n))\  (n\rightarrow\infty).&lt;/math&gt;

In a correct notation this set can for instance be called ''O''(''g''), where

:&lt;math&gt;O(g) = \{ f :&lt;/math&gt; there exist positive constants ''c'' and &lt;math&gt;n_0&lt;/math&gt; such that &lt;math&gt;0 \le f(n) \le cg(n)&lt;/math&gt; for all &lt;math&gt;n \ge n_0 \}&lt;/math&gt;.&lt;ref&gt;{{cite book |  isbn=978-0262533058 | author1=Thomas H. Cormen |author2= Charles E. Leiserson |author3= Ronald L. Rivest | title=Introduction to Algorithms | location=Cambridge/MA | publisher=MIT Press | edition=3rd | year=2009 | page=47 | quote=When we have only an asymptotic upper bound, we use O-notation. For a given function ''g''(''n''), we denote by ''O''(''g''(''n'')) (pronounced “big-oh of ''g'' of ''n''” or sometimes just “oh of ''g'' of ''n''”) the set of functions ''O''(''g''(''n'')) = { ''f''(''n'') : there exist positive constants ''c'' and ''n''&lt;sub&gt;0&lt;/sub&gt; such that 0 ≤ ''f''(''n'') ≤ ''cg''(''n'') for all ''n'' ≥ ''n''&lt;sub&gt;0&lt;/sub&gt;} }}&lt;/ref&gt;

The authors state that the use of equality operator (=) to denote set membership rather than the set membership operator (∈) is an abuse of notation, but that doing so has advantages.&lt;ref name="clrs3"&gt;{{cite book |  isbn=978-0262533058 | author1=Thomas H. Cormen |author2= Charles E. Leiserson |author3= Ronald L. Rivest | title=Introduction to Algorithms | location=Cambridge/MA | publisher=MIT Press | edition=3rd | year=2009 |page=45|quote=Because ''θ''(''g''(''n'')) is a set, we could write "''f''(''n'') ∈ ''θ''(''g''(''n''))" to indicate that ''f''(''n'') is a member of ''θ''(''g''(''n'')). Instead, we will usually write ''f''(''n'') = ''θ''(''g''(''n'')) to express the same notion. You might be confused because we abuse equality in this way, but we shall see later in this section that doing so has its advantages.}}&lt;/ref&gt; Inside an equation or inequality, the use of asymptotic notation stands for an anonymous function in the set ''O''(''g''), which eliminates lower-order terms, and helps to reduce inessential clutter in equations, for example:&lt;ref&gt;{{cite book |  isbn=978-0262533058 | author1=Thomas H. Cormen |author2= Charles E. Leiserson |author3= Ronald L. Rivest | title=Introduction to Algorithms | location=Cambridge/MA | publisher=MIT Press | edition=3rd | year=2009 | page=49 | quote= When the asymptotic notation stands alone (that is, not within a larger formula) on the right-hand side of an equation (or inequality), as in n = O(n²), we have already defined the equal sign to mean set membership: n ∈ O(n²). In general, however, when asymptotic notation appears in a formula, we interpret it as standing for some anonymous function that we do not care to name. For example, the formula 2''n''&lt;sup&gt;2&lt;/sup&gt; + 3''n'' + 1 = 2''n''&lt;sup&gt;2&lt;/sup&gt; + ''θ''(''n'') means that 2''n''&lt;sup&gt;2&lt;/sup&gt; + 3''n'' + 1 = 2''n''&lt;sup&gt;2&lt;/sup&gt; + ''f''(''n''), where ''f''(''n'') is some function in the set ''θ''(''n''). In this case, we let ''f''(''n'') = 3''n'' + 1, which is indeed in ''θ''(''n''). Using asymptotic notation in this manner can help eliminate inessential detail and clutter in an equation.}}&lt;/ref&gt;

:&lt;math&gt; 2n^2 + 3n + 1=2n^2 + O(n).&lt;/math&gt;

===Extensions to the Bachmann–Landau notations===
Another notation sometimes used in computer science is Õ (read ''soft-O''): ''f''(''n'')&amp;nbsp;=&amp;nbsp;''Õ''(''g''(''n'')) is shorthand
for ''f''(''n'')&amp;nbsp;=&amp;nbsp;''O''(''g''(''n'')&amp;nbsp;log&lt;sup&gt;''k''&lt;/sup&gt;&amp;nbsp;''g''(''n'')) for some ''k''. Essentially, it is big O notation, ignoring logarithmic factors because the growth-rate effects of some other super-logarithmic function indicate a growth-rate explosion for large-sized input parameters that is more important to predicting bad run-time performance than the finer-point effects contributed by the logarithmic-growth factor(s). This notation is often used to obviate the "nitpicking" within growth-rates that are stated as too tightly bounded for the matters at hand (since log&lt;sup&gt;''k''&lt;/sup&gt;&amp;nbsp;''n'' is always ''o''(''n''&lt;sup&gt;ε&lt;/sup&gt;) for any constant ''k'' and any ε&amp;nbsp;&gt;&amp;nbsp;0).

Also the [[L-notation|L notation]], defined as
:&lt;math&gt;L_n[\alpha,c]=e^{(c+o(1))(\ln n)^\alpha(\ln\ln n)^{1-\alpha}}&lt;/math&gt;
is convenient for functions that are between [[Time complexity#Polynomial time|polynomial]] and [[Time complexity#Exponential time|exponential]] in terms of &lt;math&gt;\ln n&lt;/math&gt;.

==Generalizations and related usages==
The generalization to functions taking values in any [[normed vector space]] is straightforward (replacing absolute values by norms), where ''f'' and ''g'' need not take their values in the same space. A generalization to functions ''g'' taking values in any [[topological group]] is also possible{{Citation needed|date=May 2017}}.
The "limiting process" ''x''&amp;nbsp;→&amp;nbsp;''x''&lt;sub&gt;o&lt;/sub&gt; can also be generalized by introducing an arbitrary [[filter base]], i.e. to directed [[net (mathematics)|net]]s ''f'' and&amp;nbsp;''g''.
The ''o'' notation can be used to define [[derivative]]s and [[differentiability]] in quite general spaces, and also (asymptotical) equivalence of functions,
:&lt;math&gt; f\sim g \iff (f-g) \in o(g) &lt;/math&gt;
which is an [[equivalence relation]] and a more restrictive notion than the relationship "''f'' is Θ(''g'')" from above. (It reduces to lim ''f'' / ''g'' = 1 if ''f'' and ''g'' are positive real valued functions.)  For example, 2''x'' is Θ(''x''), but 2''x''&amp;nbsp;−&amp;nbsp;''x'' is not ''o''(''x'').

==History (Bachmann–Landau, Hardy, and Vinogradov notations)==

The symbol O was first introduced by number theorist [[Paul Bachmann]] in 1894, in the second volume of his book ''Analytische Zahlentheorie'' ("[[analytic number theory]]"), the first volume of which (not yet containing big O notation) was published in 1892.&lt;ref name=Bachmann&gt;{{cite book |first=Paul |last=Bachmann |authorlink=Paul Bachmann |title=Analytische Zahlentheorie |trans-title=Analytic Number Theory |language=de |volume=2 |location=Leipzig |publisher=Teubner |date=1894 |url=https://archive.org/stream/dieanalytischeza00bachuoft#page/402/mode/2up}}&lt;/ref&gt; The number theorist [[Edmund Landau]] adopted it, and was thus inspired to introduce in 1909 the notation o;&lt;ref name=Landau&gt;{{cite book |first=Edmund |last=Landau |authorlink=Edmund Landau |title=Handbuch der Lehre von der Verteilung der Primzahlen |publisher=B. G. Teubner |date=1909 |location=Leipzig |trans-title=Handbook on the theory of the distribution of the primes |language=de |page=883 | url=https://archive.org/details/handbuchderlehre01landuoft}}&lt;/ref&gt; hence both are now called Landau symbols. These notations were used in applied mathematics during the 1950s for asymptotic analysis.&lt;ref&gt;{{cite book |title=Asymptotic Expansions |last=Erdelyi |first=A. |year=1956 |isbn=978-0486603186}}&lt;/ref&gt; 
The symbol &lt;math&gt;\Omega&lt;/math&gt; (in the sense "is not an ''o'' of") was introduced in 1914 by Hardy and Littlewood.&lt;ref name="HL" /&gt; Hardy and Littlewood also introduced in 1918 the symbols &lt;math&gt;\Omega_R&lt;/math&gt; ("right") and &lt;math&gt;\Omega_L&lt;/math&gt; ("left"),&lt;ref name="HL2" /&gt;  precursors of the modern symbols &lt;math&gt;\Omega_+&lt;/math&gt; ("is not smaller than a small o of") and &lt;math&gt;\Omega_-&lt;/math&gt; ("is not larger than a small o of"). Thus the Omega symbols (with their original meanings) are sometimes also referred to as "Landau symbols". This notation &lt;math&gt;\Omega&lt;/math&gt; became commonly used in number theory at least since the 1950s.&lt;ref name="titchmarsh"&gt;E. C. Titchmarsh, The Theory of the Riemann Zeta-Function (Oxford; Clarendon Press, 1951)&lt;/ref&gt;
In the 1970s the big O was popularized in computer science by [[Donald Knuth]], who introduced the related Theta notation, and proposed a different definition for the Omega notation.&lt;ref name="knuth"/&gt;

Landau never used the Big Theta and small omega symbols.

Hardy's symbols were (in terms of the modern ''O'' notation)
:&lt;math&gt; f \preccurlyeq g\iff f \in O(g) &lt;/math&gt; &amp;nbsp; and &amp;nbsp; &lt;math&gt; f\prec g\iff f\in o(g); &lt;/math&gt;
 
(Hardy however never defined or used the notation &lt;math&gt;\prec\!\!\prec&lt;/math&gt;, nor &lt;math&gt;\ll&lt;/math&gt;, as it has been sometimes reported).
It should also be noted that Hardy introduces  the symbols &lt;math&gt;\preccurlyeq &lt;/math&gt; and &lt;math&gt;\prec &lt;/math&gt; (as well as some other symbols) in his 1910 tract "Orders of Infinity", and makes use of it only in three papers (1910–1913). In his nearly 400 remaining papers and books he consistently uses the Landau symbols O and o.

Hardy's notation is not used anymore. On the other hand, in the 1930s,&lt;ref&gt;See for instance "A new estimate for ''G''(''n'') in Waring's problem" (Russian). Doklady Akademii Nauk SSSR 5, No 5-6 (1934), 249–253. Translated in English in: Selected works / Ivan Matveevič Vinogradov ; prepared by the Steklov Mathematical Institute of the Academy of Sciences of the USSR on the occasion of his 90th birthday. Springer-Verlag, 1985.&lt;/ref&gt; the Russian number theorist  [[Ivan Matveyevich Vinogradov]]  introduced his notation	&lt;math&gt;\ll&lt;/math&gt;, which  has been increasingly used in number theory instead of  the &lt;math&gt;O&lt;/math&gt; notation. We have	
:&lt;math&gt; f\ll g \iff f \in O(g), &lt;/math&gt;	
and frequently both notations are used in the same paper.
	
The big-O originally stands for "order of" ("Ordnung", Bachmann 1894), and is thus a Latin letter. Neither Bachmann nor Landau ever call it "Omicron". The symbol was much later on (1976) viewed by Knuth as a capital [[omicron]],&lt;ref name="knuth"/&gt; probably in reference to his definition of the symbol [[Omega]]. The digit [[0 (number)|zero]] should not be used.

==See also==
* [[Asymptotic expansion]]: Approximation of functions generalizing Taylor's formula
* [[Asymptotically optimal algorithm]]: A phrase frequently used to describe an algorithm that has an upper bound asymptotically within a constant of a lower bound for the problem
* [[Big O in probability notation]]: ''O&lt;sub&gt;p&lt;/sub&gt;'',''o&lt;sub&gt;p&lt;/sub&gt;''
* [[Limit superior and limit inferior]]: An explanation of some of the limit notation used in this article
* [[Nachbin's theorem]]: A precise method of bounding [[complex analytic]] functions so that the domain of convergence of [[integral transform]]s can be stated
* [[Orders of approximation]]

==References and notes==
{{Reflist|30em}}

==Further reading==
* {{cite book | first=G. H. | last=Hardy | authorlink=G. H. Hardy | title=Orders of Infinity: The 'Infinitärcalcül' of Paul du Bois-Reymond | date=1910 | url=https://archive.org/details/ordersofinfinity00harduoft | publisher=[[Cambridge University Press]]}}
* {{cite book | first=Donald | last=Knuth | authorlink=Donald Knuth | title=Fundamental Algorithms | volume=1 | series=The Art of Computer Programming | edition=3rd | publisher=Addison–Wesley | date=1997 | isbn=0-201-89683-4 | section=1.2.11: Asymptotic Representations}}
* {{cite book | first1=Thomas H. | last1=Cormen | authorlink1=Thomas H. Cormen | first2=Charles E. | last2=Leiserson | authorlink2=Charles E. Leiserson | first3=Ronald L. | last3=Rivest | authorlink3=Ronald L. Rivest | first4=Clifford | last4=Stein | authorlink4=Clifford Stein | title=[[Introduction to Algorithms]] | edition=2nd | publisher=MIT Press and McGraw–Hill | date=2001 | isbn=0-262-03293-7 | section=3.1: Asymptotic notation}}
* {{cite book | first=Michael | last=Sipser | authorlink=Michael Sipser | year = 1997 | title = Introduction to the Theory of Computation | publisher = PWS Publishing | isbn = 0-534-94728-X | pages=226–228}}
* {{cite conference | first1=Jeremy | last1=Avigad | first2=Kevin | last2=Donnelly | url=http://www.andrew.cmu.edu/~avigad/Papers/bigo.pdf | format=PDF | title=Formalizing O notation in Isabelle/HOL | doi=10.1007/978-3-540-25984-8_27 | conference=International Joint Conference on Automated Reasoning | date=2004}}
* {{cite web | first=Paul E. | last=Black | url=https://xlinux.nist.gov/dads/HTML/bigOnotation.html | title=big-O notation | work=Dictionary of Algorithms and Data Structures | editor-first=Paul E. | editor-last=Black | publisher=U.S. National Institute of Standards and Technology | date=11 March 2005 | access-date=December 16, 2006}}
* {{cite web | first=Paul E. | last=Black | url=https://xlinux.nist.gov/dads/HTML/littleOnotation.html | title=little-o notation | work=Dictionary of Algorithms and Data Structures | editor-first=Paul E. | editor-last=Black | publisher=U.S. National Institute of Standards and Technology | date=17 December 2004 | access-date=December 16, 2006}}
* {{cite web | first=Paul E. | last=Black | url=https://xlinux.nist.gov/dads/HTML/omegaCapital.html | title=Ω | work=Dictionary of Algorithms and Data Structures | editor-first=Paul E. | editor-last=Black | publisher=U.S. National Institute of Standards and Technology | date=17 December 2004 | access-date=December 16, 2006}}
* {{cite web | first=Paul E. | last=Black | url=https://xlinux.nist.gov/dads/HTML/omega.html | title=ω | work=Dictionary of Algorithms and Data Structures | editor-first=Paul E. | editor-last=Black | publisher=U.S. National Institute of Standards and Technology | date=17 December 2004 | access-date=December 16, 2006}}
* {{cite web | first=Paul E. | last=Black | url=https://xlinux.nist.gov/dads/HTML/theta.html | title=Θ | work=Dictionary of Algorithms and Data Structures | editor-first=Paul E. | editor-last=Black | publisher=U.S. National Institute of Standards and Technology | date=17 December 2004 | access-date=December 16, 2006}}

==External links==
{{wikibooks|Data Structures|Asymptotic Notation#Big-O Notation|Big-O Notation}}
* [http://oeis.org/wiki/Growth_of_sequences Growth of sequences — OEIS (Online Encyclopedia of Integer Sequences) Wiki]
* [https://classes.soe.ucsc.edu/classes/cmps102/Spring04/TantaloAsymp.pdf Introduction to Asymptotic Notations]
* [http://mathworld.wolfram.com/LandauSymbols.html Landau Symbols]
* [http://www.perlmonks.org/?node_id=573138 Big-O Notation – What is it good for]
* [https://stackoverflow.com/questions/487258/what-is-a-plain-english-explanation-of-big-o-notation/50288253#50288253 Big O Notation explained in plain english]
*[https://autarkaw.org/2013/01/30/making-sense-of-the-big-oh/ An example of Big O in accuracy of central divided difference scheme for first derivative] 

{{DEFAULTSORT:Big O Notation}}
[[Category:Mathematical notation]]
[[Category:Asymptotic analysis]]
[[Category:Analysis of algorithms]]</text>
      <sha1>qxifuyqq8v706louejfbnp169yhlwh3</sha1>
    </revision>
  </page>
  <page>
    <title>Boolean delay equation</title>
    <ns>0</ns>
    <id>5300610</id>
    <revision>
      <id>868088920</id>
      <parentid>846502654</parentid>
      <timestamp>2018-11-09T22:42:25Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 0 sources and tagging 1 as dead. #IABot (v2.0beta10)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2899">As a novel type of semi-discrete [[dynamical systems]], '''Boolean delay equations (BDEs)''' are models with Boolean-valued variables that evolve in continuous time.  Since at the present time, most phenomena are too complex to be modeled by [[partial differential equations]] (as continuous infinite-dimensional systems), BDEs are intended as a ([[heuristic]]) first step on the challenging road to further understanding and [[scientific model|modeling]] them.  For instance, one can mention complex problems in [[fluid dynamics]], [[climate]] dynamics, solid-earth [[geophysics]], and many problems elsewhere in [[natural sciences]] where much of the discourse is still [[wikt:conceptual|conceptual]].

==Hopes and promises==
Although in recent centuries, [[differential equation]]s (both ordinary and partial) have extensively served as [[quantitative property|quantitative]] models of vast categories of problems, by the recent greedy and rapid burst of complexities everywhere, the gap between [[quantitative property|quantitative]] and [[qualitative property|qualitative]] modeling and [[reasoning]] techniques is widening.  BDEs offer a formal [[mathematical]] language that is promising to help bridge that gap.

==External links==
*{{cite journal |author=Ghil M, Zaliapin I |title=A Novel Fractal Way: Boolean Delay Equations and Their Applications to the Geosciences |publisher=atmos.ucla.edu |format=PDF |url=http://www.atmos.ucla.edu/tcd/PREPRINTS/BDE_rev05_B.pdf}}
* [http://cdsagenda5.ictp.trieste.it//askArchive.php?categ=a0385&amp;id=a0385s29t12&amp;ifd=14357&amp;down=1&amp;type=lecture_notes  Boolean Delay Equations: A New Type of Dynamical Systems and Its Applications to Climate and Earthquakes]
*{{cite journal |author=Wright DG, Stocker TF, Mysak LA |title=A note on quaternary climate modelling using Boolean delay equations |journal=Climate Dynamics |volume=4 |issue=4 |pages=263–7 |year=1990 |doi=10.1007/BF00211063 |url=http://www.springerlink.com/content/w4m42276x8mk4782/fulltext.pdf |format=PDF|bibcode=1990ClDy....4..263W }}
*{{cite journal |author=Oktem H, Pearson R, Egiazarian K |title=An adjustable aperiodic model class of genomic interactions using continuous time Boolean networks (Boolean delay equations) |journal=Chaos |volume=13 |issue=4 |pages=1167–74 |date=December 2003 |pmid=14604408 |doi=10.1063/1.1608671 |url=http://link.aip.org/link/?cha/13/1167&amp;agg=MEDLINE_CHA |bibcode=2003Chaos..13.1167O }}{{Dead link|date=November 2018 |bot=InternetArchiveBot |fix-attempted=yes }}
*{{cite journal |doi=10.1016/j.physd.2008.07.006 |author=Ghil M, Zaliapin I, Coluzzi B |title=Boolean Delay Equations: A simple way of looking at complex systems |journal=Physica D |volume=237 |pages=2967–86 |year=2008 |arxiv=nlin.CG/0612047 |issue=23 |bibcode=2008PhyD..237.2967G }}

{{DEFAULTSORT:Boolean Delay Equation}}
[[Category:Dynamical systems]]
[[Category:Mathematical modeling]]</text>
      <sha1>p78dxkbgvrspir15r06m25tyiwabxzn</sha1>
    </revision>
  </page>
  <page>
    <title>Bounded complete poset</title>
    <ns>0</ns>
    <id>710299</id>
    <revision>
      <id>867868353</id>
      <parentid>866179068</parentid>
      <timestamp>2018-11-08T14:07:36Z</timestamp>
      <contributor>
        <ip>134.59.11.233</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2873">In the [[mathematics|mathematical]] field of [[order theory]], a [[partially ordered set]] is '''bounded complete''' if all of its [[subset]]s that have some [[upper bound]] also have a [[least upper bound]]. Such a partial order can also be called '''consistently''' or '''coherently complete''' ([[#Visser2004|Visser 2004, p. 182]]), since any upper bound of a set can be interpreted as some [[consistent]] (non-contradictory) piece of information that extends all the information present in the set. Hence the presence of some upper bound in a way guarantees the consistency of a set. Bounded completeness then yields the existence of a least upper bound of any "consistent" subset, which can be regarded as the most general piece of information that captures all the knowledge present within this subset. This view closely relates to the idea of information ordering that one typically finds in [[domain theory]]. 

Formally, a partially ordered set (''P'', ≤) is ''bounded complete'' if the following holds for any subset ''S'' of ''P'':

: If ''S'' has some upper bound, then it also has a least upper bound.

Bounded completeness has various relationships to other completeness properties, which are detailed in the article on [[completeness (order theory)|completeness in order theory]]. Note also that the term ''bounded poset'' is sometimes used to refer to a partially ordered set that has both a [[least element|least]] and a [[greatest element]]. Hence it is important to distinguish between a bounded-complete poset and a bounded [[complete partial order]] (cpo).

For a typical example of a bounded-complete poset, consider the set of all finite decimal numbers starting with "0." (like 0.1, 0.234, 0.122) together with all infinite such numbers (like the decimal representation 0.1111... of 1/9). Now these elements can be ordered based on the [[prefix order]] of words: a decimal number ''n'' is below some other number ''m'' if there is some string of digits w such that ''nw'' = ''m''. For example, 0.2 is below 0.234, since one can obtain the latter by appending the string "34" to 0.2. The infinite decimal numbers are the [[maximal element]]s within this order. In general, subsets of this order do not have least upper bounds: just consider the set {0.1, 0.3}. Looking back at the above intuition, one might say that it is not consistent to assume that some number starts both with 0.1 and with 0.3. However, the order is still bounded complete. In fact, it is even an example of a more specialized class of structures, the [[Scott domain]]s, which provide many other examples for bounded-complete posets.

==References==
* &lt;cite id=Visser2004&gt;Visser, A. (2004) ‘Semantics and the Liar Paradox’ in: D.M. Gabbay and F. Günther (ed.) Handbook of Philosophical Logic, 2nd Edition, Volume 11, pp.&amp;nbsp;149 – 240&lt;/cite&gt;

[[Category:Order theory]]</text>
      <sha1>ck7a5bp6cm5tv8f2v78hshxkl0in2lg</sha1>
    </revision>
  </page>
  <page>
    <title>Brauer algebra</title>
    <ns>0</ns>
    <id>3875858</id>
    <revision>
      <id>814473799</id>
      <parentid>740776713</parentid>
      <timestamp>2017-12-09T01:11:06Z</timestamp>
      <contributor>
        <username>JCW-CleanerBot</username>
        <id>31737083</id>
      </contributor>
      <minor/>
      <comment>/* References */[[User:JCW-CleanerBot#Logic|task]], replaced: journal=[[Annals of Mathematics|Annals of Mathematics. Second Series]] → journal=[[Annals of Mathematics]] |series=Second Series using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3103">In mathematics, a '''Brauer algebra''' is an algebra introduced by {{harvs|txt|authorlink=Richard Brauer|first=Richard|last=Brauer|year=1937|loc=section 5}} used in the [[representation theory]] of the [[orthogonal group]]. It plays the same role that the [[symmetric group]] does for the representation theory of the [[general linear group]] in [[Schur–Weyl duality]].

==Definition==
[[Image:Brauer algebra.jpg|thumb|400px|The product of 2 basis elements ''A'' and ''B'' of the Brauer algebra with ''n''&amp;nbsp;=&amp;nbsp;12]]
The Brauer algebra depends on the choice of a positive integer ''n'' and a number ''d'' (which in practice is often the dimension of the [[fundamental representation]] of an orthogonal group ''O''&lt;sub&gt;''d''&lt;/sub&gt;). The Brauer algebra has dimension (2''n'')!/2&lt;sup&gt;''n''&lt;/sup&gt;''n''! = (2''n''&amp;nbsp;&amp;minus;&amp;nbsp;1)(2''n''&amp;nbsp;&amp;minus;&amp;nbsp;3)&amp;nbsp;···&amp;nbsp;5·3·1 and has a basis consisting of all pairings on a set of 2''n'' elements ''X''&lt;sub&gt;1&lt;/sub&gt;, ..., ''X''&lt;sub&gt;''n''&lt;/sub&gt;, ''Y''&lt;sub&gt;1&lt;/sub&gt;, ..., ''Y''&lt;sub&gt;''n''&lt;/sub&gt; (that is, all [[perfect matching]]s of a [[complete graph]] ''K''&lt;sub&gt;2''n''&lt;/sub&gt;: any two of the 2''n'' elements may be matched to each other, regardless of their symbols). The elements ''X''&lt;sub&gt;''i''&lt;/sub&gt; are usually written in a row, with the elements ''Y''&lt;sub&gt;''i''&lt;/sub&gt; beneath them.  The product of two basis elements ''A'' and ''B'' is obtained by first identifying the endpoints in the bottom row of  ''A '' and the top row of  ''B '' (Figure  ''AB '' in the diagram), then deleting the endpoints in the middle row and joining endpoints in the remaining two rows if they are joined, directly or by a path, in  ''AB '' (Figure  ''AB=nn '' in the diagram).

==The orthogonal group==
If ''O''&lt;sub&gt;''d''&lt;/sub&gt;('''R''')  is the orthogonal group acting on ''V'' = '''R'''&lt;sup&gt;''d''&lt;/sup&gt;, then 
the Brauer algebra has a natural action on the space of polynomials on ''V''&lt;sup&gt;''n''&lt;/sup&gt; commuting with the action of the orthogonal group.

==See also==
*[[Birman–Wenzl algebra]], a deformation of the Brauer algebra.

==References==
*{{Citation | last1=Brauer | first1=Richard | authorlink = Richard Brauer | title=On Algebras Which are Connected with the Semisimple Continuous Groups | jstor=1968843 | publisher=Annals of Mathematics | series=Second Series | year=1937 | journal=[[Annals of Mathematics]] | issn=0003-486X | volume=38 | issue=4 | pages=857–872 | doi=10.2307/1968843}}
*{{Citation | last1=Wenzl | first1=Hans | title=On the structure of Brauer's centralizer algebras | jstor=1971466 | mr=951511 | year=1988 | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=128 | issue=1 | pages=173–193 | doi=10.2307/1971466}}
*{{Citation | last1=Weyl | first1=Hermann | author1-link=Hermann Weyl | title=The Classical Groups: Their Invariants and Representations | url=https://books.google.com/books?id=zmzKSP2xTtYC | accessdate=03/2007/26 | publisher=[[Princeton University Press]] | isbn=978-0-691-05756-9 | mr=0000255 | year=1946}}

[[Category:Representation theory]]
[[Category:Diagram algebras]]</text>
      <sha1>s4sbgau7pzdv1o4pmadi08prwg0pit0</sha1>
    </revision>
  </page>
  <page>
    <title>Centroidal Voronoi tessellation</title>
    <ns>0</ns>
    <id>14087640</id>
    <revision>
      <id>862708533</id>
      <parentid>854800801</parentid>
      <timestamp>2018-10-06T05:13:38Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Removing category Eponymous scientific concepts per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2018 September 22]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3239">In [[geometry]], a '''centroidal Voronoi tessellation (CVT)''' is a special type of Voronoi tessellation or [[Voronoi diagram]].  A Voronoi tessellation is called centroidal when the generating point of each Voronoi cell is also its mean (center of mass).  It can be viewed as an optimal partition corresponding to an optimal distribution of generators. A number of algorithms can be used to generate centroidal Voronoi tessellations, including [[Lloyd's algorithm]] for [[K-means clustering]].

Gersho's conjecture, proven for one and two dimensions, says that "asymptotically speaking, all cells of the optimal CVT, while forming a [[tessellation]], are [[Congruence (geometry)|congruent]] to a basic cell which depends on the dimension."&lt;ref&gt;{{citation|first1=Qiang|last1=Du|first2=Desheng|last2=Wang|title=The Optimal Centroidal Voronoi Tessellations and the Gersho's Conjecture in the Three-Dimensional Space|journal=Computers and Mathematics with Applications|issue=49|year=2005|pages=1355–1373}}&lt;/ref&gt;  In two dimensions, the basic cell for the optimal CVT is a regular [[hexagon]].

Centroidal Voronoi tessellations are useful in [[data compression]], optimal [[Numerical integration|quadrature]], optimal [[Quantization (signal processing)|quantization]], [[Data clustering|clustering]], and optimal mesh generation.&lt;ref name="Du_2"&gt;{{citation|first1=Qiang|last1=Du|first2=Vance|last2=Faber|author2-link=Vance Faber|first3=Max|last3=Gunzburger|title=Centroidal Voronoi Tessellations: Applications and Algorithms|doi=10.1137/S0036144599352836|journal=SIAM Review|volume=41|issue=4|pages=637–676|year=1999|citeseerx=10.1.1.452.2448}}.&lt;/ref&gt; Many [[patterns in nature|patterns seen in nature]] are closely approximated by a Centroidal Voronoi tessellation. Examples of this include the [[Giant's Causeway]], the cells of the [[cornea]],&lt;ref&gt;{{cite journal | last1 = Pigatto | first1 = João Antonio Tadeu | display-authors = etal   | year = 2009 | title = Scanning electron microscopy of the corneal endothelium of ostrich | url = | journal = Cienc. Rural | volume = 39 | issue = 3| pages = 926–929 | doi = 10.1590/S0103-84782009005000001 }}&lt;/ref&gt; and the breeding pits of the male [[tilapia]].&lt;ref name=Du_2/&gt;

A weighted centroidal Voronoi diagrams is a CVT in which each centroid is weighted according to a certain function. For example, a [[grayscale]] image can be used as a density function to weight the points of a CVT, as a way to create digital [[stippling]].&lt;ref&gt;Secord, Adrian. "Weighted voronoi stippling." Proceedings of the 2nd international symposium on Non-photorealistic animation and rendering. ACM, 2002.&lt;/ref&gt;

{{multiple image
 | align     = center
 | direction = horizontal
 | header    = Three centroidal Voronoi tessellations of five points in a square
 | header_align = center
 | header_background = 
 | footer    = 
 | footer_align = right
 | footer_background = 
 | width     = 200

 | image1    = CentroidalVoronoiTessellation1.png
 | alt1      =

 | image2    = CentroidalVoronoiTessellation2.png
 | alt2      =

 | image3    = CentroidalVoronoiTessellation3.png
 | alt3      = 
}}

==References==
{{reflist}}

[[Category:Discrete geometry]]
[[Category:Geometric algorithms]]
[[Category:Diagrams]]</text>
      <sha1>qr8pzuffpn2vc4yoxugogt8e5m9jdk8</sha1>
    </revision>
  </page>
  <page>
    <title>Chihara–Ismail polynomials</title>
    <ns>0</ns>
    <id>32809583</id>
    <revision>
      <id>665398456</id>
      <parentid>665373625</parentid>
      <timestamp>2015-06-04T00:15:35Z</timestamp>
      <contributor>
        <username>Rpyle731</username>
        <id>46515</id>
      </contributor>
      <minor/>
      <comment>stub sort</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1345">In mathematics, the '''Chihara–Ismail polynomials''' are a family of [[orthogonal polynomials]] introduced by {{harvs|txt|last=Chihara|author1-link=Theodore Seio Chihara|last2=Ismail|author2-link=Mourad Ismail|year=1982}}, generalizing the '''van Doorn polynomials''' introduced by {{harvtxt|van Doorn|1981}} and the [[Karlin–McGregor polynomials]]. They have a rather unusual [[Measure (mathematics)|measure]], which is [[Discrete measure|discrete]] except for a single [[limit point]] at 0 with jump 0, and is non-symmetric, but whose support has an infinite number of both positive and negative points.

==References==

*{{Citation | last1=Chihara | first1=Theodore Seio | last2=Ismail | first2=Mourad E. H. | title=Orthogonal polynomials suggested by a queueing model | doi=10.1016/S0196-8858(82)80017-1 | mr=682630 | year=1982 | journal=Advances in Applied Mathematics | issn=0196-8858 | volume=3 | issue=4 | pages=441–462}}
*{{Citation | last1=van Doorn | first1=Erik A. | title=The transient state probabilities for a queueing model where potential customers are discouraged by queue length | mr=611792 | year=1981 | journal=Journal of Applied Probability | issn=0021-9002 | volume=18 | issue=2 | pages=499–506 | doi=10.2307/3213296}}

{{DEFAULTSORT:Chihara-Ismail polynomials}}
[[Category:Orthogonal polynomials]]


{{math-stub}}</text>
      <sha1>3aikynumfar985teh3jz90j0t9ap62f</sha1>
    </revision>
  </page>
  <page>
    <title>Christopher J. Bishop</title>
    <ns>0</ns>
    <id>57910969</id>
    <revision>
      <id>867761551</id>
      <parentid>866982739</parentid>
      <timestamp>2018-11-07T21:00:39Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>[[Category:Fellows of the American Mathematical Society]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4105">'''Christopher Bishop''' is an American mathematician at [[Stony Brook University]]. He is known for his contributions to [[geometric function theory]],&lt;ref&gt;Christopher J. Bishop and Peter Jones, [https://www.jstor.org/stable/1971428?seq=1#page_scan_tab_contents "Harmonic Measure and Arclength"], ''Annals of Mathematics'', November 1990&lt;/ref&gt;&lt;ref&gt;Christopher J. Bishop, [http://annals.math.princeton.edu/2007/166-3/p01 "Conformal welding and Koebe’s theorem"], ''Annals of Mathematics'', 2007&lt;/ref&gt;&lt;ref&gt;Christopher J. Bishop, [https://link.springer.com/article/10.1007/s00222-013-0488-6 "True trees are dense"] ''Inventiones mathematicae'', August 2014&lt;/ref&gt; [[Kleinian group]]s,&lt;ref&gt;Christopher J. Bishop and Peter Jones, [https://projecteuclid.org/euclid.acta/1485891069 "Hausdorff dimension and Kleinian groups"], ''Acta Mathematica'', November 1990&lt;/ref&gt;&lt;ref&gt;Bernd O. Stratmann, [https://link.springer.com/chapter/10.1007/978-3-0348-7891-3_6 "The Exponent of Convergence of Kleinian Groups; on a Theorem of Bishop and Jones."], ''Fractal Geometry and Stochastics'', 2004&lt;/ref&gt;&lt;ref&gt;Christopher J. Bishop, [http://annals.math.princeton.edu/articles/11004 "Divergence groups have the Bowen property."], ''Annals of Mathematics'', 2001&lt;/ref&gt;&lt;ref&gt;Christopher J. Bishop, [https://link.springer.com/article/10.1007/s002220050113 "Geometric exponents and Kleinian groups."], ''Inventiones Mathematicae'', 1997&lt;/ref&gt;&lt;ref&gt;Christopher J. Bishop and Thomas Steeger, [https://projecteuclid.org/euclid.acta/1485890701 "Representation theoretic rigidity in PSL(2, R)."], ''Acta Mathematica'', 1993&lt;/ref&gt; [[complex dynamics]],&lt;ref&gt;Christopher J. Bishop, [https://projecteuclid.org/euclid.acta/1485802411 "Constructing entire functions by quasiconformal folding."], ''Acta Mathematica'', 2015&lt;/ref&gt;&lt;ref&gt;Christopher J. Bishop, [https://link.springer.com/article/10.1007/s00222-017-0770-0 "A transcendental Julia set of dimension 1."], ''Inventiones Mathematicae'', 2018&lt;/ref&gt; and [[computational geometry]];&lt;ref&gt;Christopher J. Bishop, [https://link.springer.com/article/10.1007/s00454-010-9269-9 "Conformal mapping in linear time."], ''Discrete Computational Geometry'', 2010&lt;/ref&gt; and in particular for topics such as fractals, harmonic measure, conformal and quasiconformal mappings and Julia sets. He received his Ph.D. from the University of Chicago in 1987, under the supervision of [[Peter Jones (mathematician)|Peter Jones]].&lt;ref&gt;{{mathgenealogy|id=31346}}&lt;/ref&gt;

Bishop was awarded the 1992 A. P. [[Sloan Foundation fellowship]],&lt;ref&gt;[https://sloan.org/past-fellows "List of past Sloan fellows."]&lt;/ref&gt;, was an invited speaker at the 2018 [[International Congress of Mathematicians]].&lt;ref&gt;[http://www.icm2018.org/portal/en/invited-section-lectures-speakers "List of 2018 ICM speakers."]&lt;/ref&gt; He was included in the 2019 class of fellows of the [[American Mathematical Society]] "for contributions to the theory of harmonic measures, quasiconformal maps and transcendental dynamics".&lt;ref&gt;{{citation|url=https://www.ams.org/profession/ams-fellows/new-fellows|title=2019 Class of the Fellows of the AMS|publisher=[[American Mathematical Society]]|accessdate=2018-11-07}}&lt;/ref&gt;

== Books ==

With [[Yuval Peres]], Bishop is the author of the book ''Fractals in Probability and Analysis'' (Cambridge Studies in Advanced Mathematics 162, 2009).&lt;ref&gt;Reviews of ''Fractals in Probability and Analysis'':
*{{citation|title=Review|first=Tushar|last=Das|journal=MAA Reviews|url=https://www.maa.org/press/maa-reviews/fractals-in-probability-and-analysis|date=November 2017}}
*{{citation|title=none|first=David A.|last=Croydon|journal=Mathematical Reviews|mr=3616046}}&lt;/ref&gt;

== External links ==
*[http://www.math.stonybrook.edu/~bishop/ Home page]

== References ==
{{reflist}}


{{authority control}}

{{DEFAULTSORT:Bishop, Chris}}
[[Category:Stony Brook University faculty]]
[[Category:Sloan Research Fellows]]
[[Category:20th-century American mathematicians]]
[[Category:University of Chicago alumni]]
[[Category:Living people]]
[[Category:Fellows of the American Mathematical Society]]


{{US-mathematician-stub}}</text>
      <sha1>fzzf7s7jid0q1sxa8k4z4htp6oakl5m</sha1>
    </revision>
  </page>
  <page>
    <title>Complete market</title>
    <ns>0</ns>
    <id>2189987</id>
    <revision>
      <id>829206066</id>
      <parentid>829205990</parentid>
      <timestamp>2018-03-07T07:12:57Z</timestamp>
      <contributor>
        <username>Me, Myself, and I are Here</username>
        <id>17619453</id>
      </contributor>
      <comment>Filled in 2 bare reference(s) with [[:en:WP:REFILL|reFill]] ()</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5891">{{Refimprove|date=July 2016}}
In [[economics]], a '''complete market''' (aka '''Arrow-Debreu market'''&lt;ref name="buckle" /&gt; or '''complete system of markets''') is a market with two conditions:

# Negligible [[transaction costs]]&lt;ref name="buckle"&gt;{{cite web|url=https://books.google.com/books?id=4VsrBkOu1FgC&amp;pg=PA46&amp;lpg=PA46&amp;dq=do+complete+markets+allow+transaction+costs&amp;source=bl&amp;ots=WLG8_ebyXg&amp;sig=uRdHIMZqUQhtb-vcKyXHYjP1UZw&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiNwYDyreDLAhVU3GMKHVAJCqUQ6AEIMjAD#v=onepage&amp;q=do+complete+markets+allow+transaction+costs&amp;f=false|title=The UK Financial System: Fourth Edition|first1=Michael J.|last1=Buckle|first2=Mike|last2=Buckle|first3=John|last3=Thompson|date=7 March 2018|publisher=Manchester University Press|via=Google Books}}&lt;/ref&gt; and therefore also [[perfect information]],
# there is a price for every asset in every possible state of the world&lt;ref&gt;{{cite web|url=https://stats.oecd.org/glossary/detail.asp?ID=5900|title=OECD Glossary of Statistical Terms - Complete market Definition|first=OECD Statistics|last=Directorate|website=stats.oecd.org}}&lt;/ref&gt;

In such a market, the complete set of possible bets on future states-of-the-world can be constructed with existing [[asset]]s without [[Frictionless market|friction]]. Here goods are state-contingent; that is, a good includes the time and state of the world in which it is consumed. So for instance, an umbrella tomorrow if it rains is a distinct good from an umbrella tomorrow if it is clear. The study of complete markets is central to state-preference theory. The theory can be traced to the work of [[Kenneth Arrow]] (1964), [[Gérard Debreu]] (1959), Arrow &amp; Debreu (1954) and [[Lionel McKenzie]](1954). Arrow and Debreu were awarded the [[Nobel Memorial Prize in Economics]] (Arrow in 1972, Debreu in 1983), largely for their work in developing the theory of complete markets and applying it to the problem of general equilibrium.

A '''state of the world''' is a complete specification of the values of all relevant variables over the relevant time horizon. A '''state-contingent claim''', or '''state claim''', is a contract whose future payoffs depend on future states of the world. For example, suppose you can bet on the outcome of a coin toss. If you guess the outcome correctly, you will win one dollar, and otherwise you will lose one dollar. A bet on heads is a state claim, with payoff of one dollar if heads is the outcome, and payoff of negative one dollar if tails is the outcome. "Heads" and "tails" are the states of the world in this example. A state-contingent claim can be represented as a payoff vector with one element for each state of the world, e.g. (payoff if heads, payoff if tails). So a bet on heads can be represented as ($1, −$1) and a bet on tails can be represented as (−$1, $1). Notice that by placing one bet on heads and one bet on tails, you have a state-contingent claim of ($0, $0); that is, the payoff is the same regardless of which state of the world occurs.

The bet on a coin toss is a simplistic example but illustrates widely applicable concepts, especially in [[finance]]. If markets are complete, it is possible to arrange a portfolio with any conceivable payoff vector. That is, the state claims available for purchase, represented as payoff vectors, [[Linear span|span]] the payoff space. A ''pure security'' or simple [[contingent claims|contingent claim]] is a state claim that pays off in only one state. Any state-contingent claim can be regarded as a collection of [[State prices|pure securities]]. A system of markets is complete if and only if the number of attainable pure securities equals the number of possible states. Formally, a market is complete with respect to a [[trading strategy]], &lt;math&gt;s&lt;/math&gt;, if there exists a [[self-financing trading strategy]], &lt;math&gt;s_0&lt;/math&gt; such that at any time &lt;math&gt;t&lt;/math&gt;, the returns of the two strategies, &lt;math&gt;s&lt;/math&gt; and &lt;math&gt;s_0&lt;/math&gt;  are equal. This is equivalent to stating that for a complete market, all cash flows for a trading strategy can be replicated using a similar synthetic trading strategy. Because a trading strategy can be simplified into a set of simple [[contingent claims]] (strategies paying 1 in one state and 0 in every other state), a complete market can be generalized as the ability to replicate cash flows of all simple contingent claims.

Often used to describe insurance markets the model of a complete market occurs if [[wiktionary:agent|agent]]s can buy insurance contracts to protect themselves against ''any'' future time and state-of-the-world.

For example, if a market is a finite state market with dimension ''N'', then a complete market would be one where there exist traded assets with payoffs that form a basis for&amp;nbsp;'''R'''&lt;sup&gt;''N''&lt;/sup&gt;.

== Dynamically complete market ==
In order for a market to be complete, it must be possible to ''instantaneously'' enter into any position  regarding any future state of the market.  In contrast, a market is called '''dynamically complete''' if it is possible to construct a self-financing trading strategy that will have the same cash-flow.  In other words, a complete market allows you to  place all of your bet at once, while a dynamically complete market may require that you execute subsequent trades after making your initial investment.  The requirement that the strategy be self-financing means that subsequent trades must be cash-flow neutral (you cannot contribute or withdraw any additional funds).  Any complete market is also dynamically complete.

== See also ==
*[[Incomplete markets]]

== References ==
{{Reflist}}
== Further reading ==
* Mark D. Flood (1991), [http://research.stlouisfed.org/publications/review/91/03/Markets_Mar_Apr1991.pdf "An Introduction to Complete Markets"], [[Federal Reserve Bank of St. Louis]], Review, March/April 1991
[[Category:Mathematical finance]]</text>
      <sha1>2efvnvcn2wpro31ltfi58vfd14jk38w</sha1>
    </revision>
  </page>
  <page>
    <title>Convergence group</title>
    <ns>0</ns>
    <id>52297304</id>
    <revision>
      <id>852311341</id>
      <parentid>840383732</parentid>
      <timestamp>2018-07-28T02:11:14Z</timestamp>
      <contributor>
        <username>AvalerionV</username>
        <id>30927891</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13202">In mathematics, a '''convergence group''' or a '''discrete convergence group''' is a [[group (mathematics)|group]] &lt;math&gt;\Gamma&lt;/math&gt; [[group action|acting]] by [[homeomorphism]]s on a [[compact space|compact]] [[metrizable space]] &lt;math&gt;M&lt;/math&gt; in a way that generalizes the properties of the action of [[Kleinian group]] by [[Möbius transformation]]s on the ideal boundary &lt;math&gt;\mathbb S^2&lt;/math&gt; of the [[Hyperbolic space|hyperbolic 3-space &lt;math&gt;\mathbb H^3 &lt;/math&gt;]].
The notion of a convergence group was introduced by [[Frederick Gehring|Gehring]] and [[Gaven Martin|Martin]] (1987) &lt;ref&gt;F. W. Gehring and G. J. Martin, ''Discrete quasiconformal groups I'', [[Proceedings of the London Mathematical Society]] '''55''' (1987), 331–358&lt;/ref&gt; and has since found wide applications in [[geometric topology]], [[Quasiconformal mapping|quasiconformal analysis]], and [[geometric group theory]].

== Formal definition ==

Let &lt;math&gt;\Gamma&lt;/math&gt; be a group acting by homeomorphisms on a compact metrizable space &lt;math&gt;M&lt;/math&gt;. This action is called a ''convergence action'' or a ''discrete convergence action'' (and then &lt;math&gt;\Gamma&lt;/math&gt; is called a ''convergence group'' or a ''discrete convergence group'' for this action) if for every infinite distinct sequence of elements &lt;math&gt;\gamma_n \in \Gamma&lt;/math&gt; there exist a subsequence &lt;math&gt;\gamma_{n_k}, k=1,2,\dots&lt;/math&gt; and points &lt;math&gt;a,b\in M&lt;/math&gt; such that the maps &lt;math&gt;\gamma_{n_k}\big|_{M\setminus\{a\}}&lt;/math&gt; converge uniformly on compact subsets to the constant map sending &lt;math&gt;M\setminus\{a\}&lt;/math&gt; to &lt;math&gt;b&lt;/math&gt;.  Here converging uniformly on compact subsets means that for every open neighborhood &lt;math&gt;U&lt;/math&gt; of &lt;math&gt;b&lt;/math&gt; in &lt;math&gt;M&lt;/math&gt; and every compact &lt;math&gt; K\subset M\setminus \{a\}&lt;/math&gt; there exists an index &lt;math&gt;k_0\ge 1&lt;/math&gt; such that for every &lt;math&gt;k\ge k_0,&lt;/math&gt; &lt;math&gt; \gamma_{n_k}(K)\subseteq U&lt;/math&gt;. Note that the "poles" &lt;math&gt; a, b\in M&lt;/math&gt; associated with the subsequence  &lt;math&gt;\gamma_{n_k}&lt;/math&gt; are not required  to be distinct.

=== Reformulation in terms of the action on distinct triples ===

The above definition of convergence group admits a useful equivalent reformulation in terms of the action of &lt;math&gt;\Gamma&lt;/math&gt; on the "space of distinct triples" of &lt;math&gt;M&lt;/math&gt;.
For a set &lt;math&gt;M&lt;/math&gt; denote &lt;math&gt;\Theta(M):=M^3\setminus \Delta(M)&lt;/math&gt;, where &lt;math&gt;\Delta(M)=\{(a,b,c)\in M^3\mid \#\{a,b,c\}\le 2\}&lt;/math&gt;. The set &lt;math&gt;\Theta(M)&lt;/math&gt; is called the "space of distinct triples" for &lt;math&gt;M&lt;/math&gt;.

Then the following equivalence is known to hold:&lt;ref name=Bow-config&gt;B. H. Bowditch,  ''Convergence groups and configuration spaces''.  Geometric group theory down under (Canberra, 1996), 23–54, de Gruyter, Berlin, 1999.&lt;/ref&gt;

Let &lt;math&gt;\Gamma&lt;/math&gt; be a group acting by homeomorphisms on a compact metrizable space &lt;math&gt;M&lt;/math&gt; with at least two points. Then this action is a discrete convergence action if and only if the inducted action of &lt;math&gt;\Gamma&lt;/math&gt; on &lt;math&gt;\Theta(M)&lt;/math&gt; is [[properly discontinuous action|properly discontinuous]].

== Examples ==

*The action of a [[Kleinian group]] &lt;math&gt;\Gamma&lt;/math&gt; on &lt;math&gt;\mathbb S^2=\partial \mathbb H^3&lt;/math&gt; by [[Möbius transformation]]s is a convergence group action.
* The action of a [[word-hyperbolic group]] &lt;math&gt;G&lt;/math&gt; by translations on its ideal boundary &lt;math&gt;\partial G&lt;/math&gt; is a convergence group action.
* The action of a [[relatively hyperbolic group]] &lt;math&gt;G&lt;/math&gt; by translations on its Bowditch boundary &lt;math&gt;\partial G&lt;/math&gt; is a convergence group action.
* Let &lt;math&gt;X&lt;/math&gt; be a proper geodesic [[δ-hyperbolic space|Gromov-hyperbolic]] metric space and let &lt;math&gt;\Gamma&lt;/math&gt; be a group acting properly discontinuously by isometries on &lt;math&gt;X&lt;/math&gt;. Then the corresponding boundary action of &lt;math&gt;\Gamma&lt;/math&gt; on &lt;math&gt;\partial X&lt;/math&gt; is a discrete convergence action (Lemma 2.11 of &lt;ref name=Bow-config /&gt;).

== Classification of elements in convergence groups ==

Let &lt;math&gt;\Gamma&lt;/math&gt; be a group acting by homeomorphisms on a compact metrizable space &lt;math&gt;M&lt;/math&gt;with at least three points,  and let &lt;math&gt;\gamma\in\Gamma&lt;/math&gt;. Then it is known (Lemma 3.1 in &lt;ref name=Bow-config /&gt; or Lemma 6.2 in &lt;ref&gt;B. H. Bowditch, ''Treelike structures arising from continua and convergence groups''. [[Memoirs of the American Mathematical Society]] '''139''' (1999), no. 662.&lt;/ref&gt;) that exactly one of the following occurs:

(1) The element &lt;math&gt;\gamma&lt;/math&gt; has finite order in &lt;math&gt;\Gamma &lt;/math&gt;; in this case &lt;math&gt;\gamma&lt;/math&gt;  is called ''elliptic''.

(2) The element &lt;math&gt;\gamma&lt;/math&gt; has infinite order in &lt;math&gt;\Gamma &lt;/math&gt; and the fixed set &lt;math&gt;\operatorname{Fix}_M(\gamma)&lt;/math&gt; is a single point; in this case &lt;math&gt;\gamma&lt;/math&gt;  is called ''parabolic''.

(3) The element &lt;math&gt;\gamma&lt;/math&gt; has infinite order in &lt;math&gt;\Gamma&lt;/math&gt; and the fixed set &lt;math&gt;\operatorname{Fix}_M(\gamma)&lt;/math&gt; consists of two distinct points; in this case &lt;math&gt;\gamma&lt;/math&gt;  is called ''loxodromic''.

Moreover, for every &lt;math&gt;p\ne 0&lt;/math&gt; the elements &lt;math&gt;\gamma&lt;/math&gt; and &lt;math&gt;\gamma^p&lt;/math&gt;have the same type.  Also in cases (2) and (3) &lt;math&gt;\operatorname{Fix}_M(\gamma) = \operatorname{Fix}_M(\gamma^p)&lt;/math&gt; (where &lt;math&gt;p\ne 0&lt;/math&gt;) and the group &lt;math&gt;\langle \gamma\rangle &lt;/math&gt; acts properly discontinuously on &lt;math&gt;M\setminus \operatorname{Fix}_M(\gamma)&lt;/math&gt;. Additionally, if &lt;math&gt;\gamma&lt;/math&gt; is loxodromic, then &lt;math&gt;\langle \gamma\rangle &lt;/math&gt; acts properly discontinuously and cocompactly on &lt;math&gt;M\setminus \operatorname{Fix}_M(\gamma) &lt;/math&gt;.

If  &lt;math&gt;\gamma\in \Gamma&lt;/math&gt; is parabolic with a fixed point &lt;math&gt;a\in M&lt;/math&gt; then for every &lt;math&gt;x\in M&lt;/math&gt; one has &lt;math&gt;\lim_{n\to\infty}\gamma^nx=\lim_{n\to-\infty}\gamma^nx =a&lt;/math&gt;
If &lt;math&gt;\gamma\in \Gamma&lt;/math&gt; is loxodromic, then &lt;math&gt;\operatorname{Fix}_M(\gamma)&lt;/math&gt; can be written as &lt;math&gt;\operatorname{Fix}_M(\gamma)=\{a_-,a_+\}&lt;/math&gt; so that for every &lt;math&gt; x \in  M\setminus \{a_-\} &lt;/math&gt; one has &lt;math&gt;\lim_{n\to\infty}\gamma^nx=a_+&lt;/math&gt; and for every &lt;math&gt; x \in  M\setminus \{a_+\} &lt;/math&gt; one has &lt;math&gt;\lim_{n\to-\infty}\gamma^nx=a_-&lt;/math&gt;, and these convergences are  uniform on compact subsets of &lt;math&gt;M\setminus \{a_-, a_+\}&lt;/math&gt;.

== Uniform convergence groups ==

A discrete convergence action of a group &lt;math&gt;\Gamma&lt;/math&gt; on a compact metrizable space &lt;math&gt;M&lt;/math&gt; is called ''uniform'' (in which case &lt;math&gt;\Gamma&lt;/math&gt; is called a ''uniform convergence group'') if the action of &lt;math&gt;\Gamma&lt;/math&gt; on &lt;math&gt;\Theta(M)&lt;/math&gt; is [[Cocompact group action|co-compact]]. Thus &lt;math&gt;\Gamma&lt;/math&gt; is a uniform convergence group if and only if its action on &lt;math&gt;\Theta(M)&lt;/math&gt; is both properly discontinuous and co-compact.

=== Conical limit points ===

Let &lt;math&gt;\Gamma&lt;/math&gt; act on a compact metrizable space &lt;math&gt;M&lt;/math&gt; as a discrete convergence group. A point &lt;math&gt;x\in M&lt;/math&gt; is called a ''conical limit point'' (sometimes also called a ''radial limit point'' or  a ''point of approximation'') if there exist an infinite sequence of distinct elements &lt;math&gt;\gamma_n\in \Gamma&lt;/math&gt; and distinct points &lt;math&gt;a,b\in M&lt;/math&gt; such that &lt;math&gt;\lim_{n\to\infty}\gamma_n x=a&lt;/math&gt; and for every &lt;math&gt;y\in M\setminus \{x\}&lt;/math&gt; one has &lt;math&gt; \lim_{n\to\infty}\gamma_n y=b&lt;/math&gt;.

An important result of Tukia,&lt;ref&gt;P. Tukia, ''Conical limit points and uniform convergence groups.''
[[Journal für die Reine und Angewandte Mathematik]] ''501'' (1998), 71–98&lt;/ref&gt; also independently obtained by [[Brian Bowditch|Bowditch]],&lt;ref name=Bow-config /&gt;&lt;ref name=Bow-topol&gt;B. Bowditch, [http://www.ams.org/journals/jams/1998-11-03/S0894-0347-98-00264-1/S0894-0347-98-00264-1.pdf ''A topological characterisation of hyperbolic groups.] [[Journal of the American Mathematical Society]] '''11''' (1998), no. 3, 643–667&lt;/ref&gt; states:

A discrete convergence group action of a group &lt;math&gt;\Gamma&lt;/math&gt; on a compact metrizable space &lt;math&gt;M&lt;/math&gt; is uniform if and only if every non-isolated point of &lt;math&gt;M&lt;/math&gt; is a conical limit point.

=== Word-hyperbolic groups and their boundaries ===

It was already observed by Gromov&lt;ref&gt;{{cite book | authorlink=Mikhail Gromov (mathematician) | first=Mikhail | last=Gromov | chapter=Hyperbolic groups | title=Essays in group theory | pages=75–263 | series=Mathematical Sciences Research Institute Publications | volume=8 | publisher=Springer | location=New York | year=1987 | doi=10.1007/978-1-4613-9586-7_3 | mr=919829 | editor-last=Gersten | editor-first=Steve M. | isbn=0-387-96618-8 | ref=harv}}&lt;/ref&gt; that the natural action by translations of a [[word-hyperbolic group]] &lt;math&gt;G&lt;/math&gt; on its boundary &lt;math&gt;\partial G&lt;/math&gt; is a uniform convergence action (see&lt;ref name=Bow-config /&gt; for a formal proof). Bowditch&lt;ref name=Bow-topol /&gt; proved an important converse, thus obtaining a topological characterization of word-hyperbolic groups:

'''Theorem.''' Let &lt;math&gt; G&lt;/math&gt; act as a discrete uniform convergence group on a compact metrizable space &lt;math&gt;M&lt;/math&gt; with no isolated points. Then the group &lt;math&gt;G&lt;/math&gt; is word-hyperbolic and there exists a &lt;math&gt;G&lt;/math&gt;-equivariant homeomorphism &lt;math&gt;M\to \partial G&lt;/math&gt;.

== Convergence actions on the circle ==

An isometric action of a group &lt;math&gt;G&lt;/math&gt; on the [[hyperbolic plane]] &lt;math&gt;\mathbb H^2 &lt;/math&gt; is called [[Geometric group action|''geometric'']] if this action is properly discontinuous and cocompact. Every geometric action of &lt;math&gt;G&lt;/math&gt; on  &lt;math&gt;\mathbb H^2 &lt;/math&gt; induces a uniform convergence action of &lt;math&gt;G&lt;/math&gt; on &lt;math&gt;\mathbb S^1 =\partial H^2\approx \partial G&lt;/math&gt;.
An important result of Tukia (1986),&lt;ref&gt;P. Tukia, ''On quasiconformal groups.'' [[Journal d'Analyse Mathématique]] '''46''' (1986), 318–346.&lt;/ref&gt; [[David Gabai|Gabai]] (1992),&lt;ref&gt;D. Gabai, ''Convergence groups are Fuchsian groups.'' [[Annals of Mathematics]] '''136''' (1992), no. 3, 447–510.&lt;/ref&gt;   Casson–Jungreis (1994),&lt;ref&gt;A. Casson, D. Jungreis, ''Convergence groups and Seifert fibered 3-manifolds.''
[[Inventiones Mathematicae]] '''118''' (1994), no. 3, 441–456.&lt;/ref&gt; and Freden (1995)&lt;ref&gt;E. Freden, ''Negatively curved groups have the convergence property. I.''  Annales Academiae Scientiarum Fennicae. Series A I. Mathematica '''20''' (1995), no. 2, 333–348.&lt;/ref&gt; shows that the converse also holds:

'''Theorem.''' If &lt;math&gt;G&lt;/math&gt; is a group acting as a discrete uniform convergence group on &lt;math&gt;\mathbb S^1&lt;/math&gt; then this action is topologically conjugate to an action induced by a geometric action of &lt;math&gt;G&lt;/math&gt; on &lt;math&gt;\mathbb H^2&lt;/math&gt; by isometries.

Note that whenever &lt;math&gt;G&lt;/math&gt;  acts geometrically on &lt;math&gt;\mathbb H^2 &lt;/math&gt;, the group &lt;math&gt;G&lt;/math&gt; is [[virtually]] a hyperbolic surface group, that is,  &lt;math&gt;G&lt;/math&gt; contains a finite index subgroup isomorphic to the fundamental group of a closed hyperbolic surface.

== Convergence actions on the 2-sphere ==

One of the equivalent reformulations of [[Cannon's conjecture]], originally posed by [[James W. Cannon]] in terms of word-hyperbolic groups with boundaries homeomorphic to &lt;math&gt;\mathbb S^2&lt;/math&gt;,&lt;ref&gt;James W. Cannon, ''The theory of negatively curved spaces and groups''. Ergodic theory, symbolic dynamics, and hyperbolic spaces (Trieste, 1989), 315–369,
Oxford Sci. Publ., Oxford Univ. Press, New York, 1991&lt;/ref&gt; says that if &lt;math&gt;G&lt;/math&gt; is a group acting as a discrete uniform convergence group on &lt;math&gt;\mathbb S^2&lt;/math&gt; then this action is topologically conjugate to an action induced by a [[Geometric group action|geometric action]] of &lt;math&gt;G&lt;/math&gt; on &lt;math&gt;\mathbb H^3&lt;/math&gt; by isometries. This conjecture still remains open.

== Applications and further generalizations ==

* Yaman gave a characterization of [[relatively hyperbolic group]]s in terms of convergence actions,&lt;ref&gt;A. Yaman, ''A topological characterisation of relatively hyperbolic groups.'' [[Journal für die Reine und Angewandte Mathematik]] '''566''' (2004), 41–89&lt;/ref&gt; generalizing Bowditch's characterization of word-hyperbolic groups as uniform convergence groups.
* One can consider more general versions of group actions with "convergence property" without the discreteness assumption.&lt;ref&gt;V. Gerasimov, ''Expansive convergence groups are relatively hyperbolic'', [[Geometric and Functional Analysis (journal)|Geometric and Functional Analysis (GAFA)]]  '''19''' (2009), no. 1, 137–169&lt;/ref&gt;
* The most general version of the notion of [[Cannon–Thurston map]], originally defined in the context of Kleinian and word-hyperbolic groups, can be defined and studied in the context of setting of convergence groups.&lt;ref&gt;W.Jeon, I. Kapovich, C. Leininger, K. Ohshika, [http://www.ams.org/journals/ecgd/2016-20-04/S1088-4173-2016-00294-0/S1088-4173-2016-00294-0.pdf ''Conical limit points and the Cannon-Thurston map.''] [[Conformal Geometry and Dynamics]] '''20''' (2016), 58–80&lt;/ref&gt;

== References ==
{{Reflist}}

[[Category:Group theory]]
[[Category:Dynamical systems]]
[[Category:Geometric topology]]
[[Category:Geometric group theory]]</text>
      <sha1>nwlj82r02k1h255413v9b6gsoogfm8s</sha1>
    </revision>
  </page>
  <page>
    <title>CoreASM</title>
    <ns>0</ns>
    <id>7961605</id>
    <revision>
      <id>870170106</id>
      <parentid>789047948</parentid>
      <timestamp>2018-11-22T22:23:05Z</timestamp>
      <contributor>
        <username>Leschnei</username>
        <id>27335766</id>
      </contributor>
      <comment>/* top */ 1</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3272">'''CoreASM''' is an [[open source]]{{dn|date=November 2018}} project (licensed under [[Academic Free License]] version 3.0) that focuses on the design of a lean executable ASM ([[Abstract State Machines]]) language, in combination with a supporting tool environment for high-level design, experimental validation, and formal verification (where appropriate) of abstract system models.

Abstract state machines are known for their versatility in modeling of algorithms, architectures, languages, protocols, and virtually all kinds of sequential, parallel, and distributed systems. The ASM formalism has been studied extensively by researchers in academia and industry for more than 15 years with the intention to bridge the gap between formal and pragmatic approaches. 

Model-based systems engineering can benefit from abstract executable specifications as a tool for design exploration and experimental validation through simulation and testing. Building on experiences with two generations of ASM tools, a novel executable ASM language, called CoreASM, is being developed (see [https://github.com/CoreASM CoreASM] homepage). 

The CoreASM language emphasizes freedom of experimentation, and supports the evolutionary nature of design as a product of creativity. It is particularly suited to Exploring the problem space for the purpose of writing an initial specification. The CoreASM language allows writing of highly abstract and concise specifications by minimizing the need for encoding in mapping the problem space to a formal model, and by allowing explicit declaration of the parts of the specification that are purposely left abstract. The principle of minimality, in combination with robustness of the underlying mathematical framework, improves modifiability of specifications, while effectively supporting the highly iterative nature of specification and design.

==References==
*  R. Farahbod, V. Gervasi, U. Glässer and M. Memon. ''Design Exploration and Experimental Validation of Abstract Requirements'', Proceedings of the 12th International Working Conference on Requirements Engineering: Foundation for Software Quality (REFSQ'06), June 2006, Luxembourg, Grand-Duchy of Luxembourg, Essener Informatik Beitrage, {{ISBN|3-922602-26-6}}.
* R. Farahbod, V. Gervasi, U. Glässer, and M. Memon. [http://stl.sfu.ca/publications/CoreASM-TR-2006-09.pdf Design and Specification of the CoreASM Execution Engine, Part 1: the Kernel]. Technical Report SFU-CMPT-TR-2006-09, [[Simon Fraser University]], May 2006.
* R. Farahbod, V. Gervasi, and U. Glässer. [http://stl.sfu.ca/publications/CoreASM-ASM2005.pdf CoreASM: An extensible ASM execution engine]. In D. Beauquier, [[E. Börger]] and A. Slissenko (Eds.), Proc. 12th International Workshop on Abstract State Machines, Paris, March 2005, pages 153–165
* ... [https://github.com/CoreASM/coreasm.core/wiki/Documentation further references and documentation]

==External links==
* [https://github.com/CoreASM The CoreASM Project at [[GitHub]]]
* [https://github.com/CoreASM/coreasm.core/wiki The CoreASM wiki]
* [http://www.eecs.umich.edu/gasm/ Abstract State Machines homepage]

[[Category:Formal specification languages]]
[[Category:Formal methods tools]]
[[Category:Software using the Academic Free License]]</text>
      <sha1>fsamyev44scfak7lvxef88fie6so1sd</sha1>
    </revision>
  </page>
  <page>
    <title>Discrete tomography</title>
    <ns>0</ns>
    <id>6778984</id>
    <revision>
      <id>797508192</id>
      <parentid>793224366</parentid>
      <timestamp>2017-08-27T13:51:31Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[Wikipedia:Bots/Requests for approval/KolbertBot|HTTP→HTTPS]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11008">[[File:Discrete tomography.png|thumb|A discrete tomography reconstruction problem for two vertical and horizontal directions (left), together with its (non-unique) solution (right). The task is to color some of the white points black so that the number of black points in the rows and columns match the blue numbers.]]'''Discrete tomography'''&lt;ref name="ref4"&gt;
Herman, G. T. and Kuba, A., Discrete Tomography: Foundations, Algorithms, and Applications, Birkhäuser Boston, 1999
&lt;/ref&gt;&lt;ref name="ref5"&gt;
Herman, G. T. and Kuba, A., Advances in Discrete Tomography and Its Applications, Birkhäuser Boston, 2007
&lt;/ref&gt; focuses on the problem of reconstruction of [[binary image]]s (or finite subsets of the [[integer lattice]]) from a small number of their [[projection (mathematics)|projection]]s.

In general, [[tomography]] deals with the problem of determining shape and dimensional information of an object from a set of projections. From the mathematical point of view, the object corresponds to a [[function (mathematics)|function]] and the problem posed is to reconstruct this function from its [[integral]]s or sums over subsets of its [[Domain of a function|domain]]. In general, the tomographic inversion problem may be continuous or discrete. In continuous tomography both the
domain and the range of the function are continuous and line integrals are used. In discrete tomography the domain of the function may be either discrete or continuous, and the range of the function is a finite set of real, usually nonnegative numbers. In continuous tomography when a large number of projections is available, accurate reconstructions can be made by many different algorithms.
It is typical for discrete tomography that only a few projections (line sums) are used. In this case, conventional techniques all fail. A special case of discrete tomography deals with the problem of the reconstruction of
a binary image from a small number of projections. The name ''discrete tomography'' is due to [[Larry Shepp]], who organized the first meeting devoted to this topic ([[DIMACS]] Mini-Symposium on Discrete Tomography, September 19, 1994, [[Rutgers University]]).

==Theory==
Discrete tomography has strong connections with other mathematical fields, such as [[number theory]],&lt;ref name="ref11"&gt;R.J. Gardner, P. Gritzmann, Discrete tomography: determination of finite sets by X-rays, Trans. Amer. Math. Soc. 349 (1997), no. 6, 2271-2295.&lt;/ref&gt;&lt;ref name="ref9"&gt;L. Hajdu, R. Tijdeman, Algebraic aspects of discrete tomography, J. reine angew. Math. 534 (2001), 119-128.&lt;/ref&gt;&lt;ref name="ref10"&gt;A. Alpers, R. Tijdeman, The Two-Dimensional Prouhet-Tarry-Escott Problem, Journal of Number Theory, 123 (2), pp. 403-412, 2007 [http://www-m9.ma.tum.de/Allgemeines/AndreasAlpersPublications].&lt;/ref&gt; [[discrete mathematics]],&lt;ref name="ref14"&gt;S. Brunetti, A. Del Lungo, P. Gritzmann, S. de Vries, On the reconstruction of binary and permutation matrices under (binary) tomographic constraints. Theoret. Comput. Sci. 406 (2008), no. 1-2, 63-71.&lt;/ref&gt;&lt;ref name="ref13"&gt;A. Alpers, P. Gritzmann, On Stability, Error Correction, and Noise Compensation in Discrete Tomography, SIAM Journal on Discrete Mathematics  20 (1), pp. 227-239, 2006 [http://www-m9.ma.tum.de/Allgemeines/AndreasAlpersPublications]&lt;/ref&gt;&lt;ref name="ref12"&gt;P. Gritzmann, B. Langfeld, On the index of Siegel grids and its application to the tomography of quasicrystals. European J. Combin. 29 (2008), no. 8, 1894-1909.&lt;/ref&gt; [[Complex systems|complexity theory]]&lt;ref name="ref15"&gt;R.J. Gardner, P. Gritzmann, D. Prangenberg, On the computational complexity of reconstructing lattice sets from their X-rays. Discrete Math. 202 (1999), no. 1-3, 45-71.&lt;/ref&gt;&lt;ref name="ref16"&gt;C. Dürr, F. Guiñez, M. Matamala, Reconstructing 3-Colored Grids from Horizontal and Vertical Projections Is NP-hard. ESA 2009: 776-787.&lt;/ref&gt; and [[combinatorics]].&lt;ref name="ref17"&gt;H.J. Ryser, Matrices of zeros and ones, Bull. Amer. Math. Soc. 66 1960 442-464.&lt;/ref&gt;&lt;ref name="ref18"&gt;D. Gale, A theorem on flows in networks, Pacific J. Math. 7 (1957), 1073-1082.&lt;/ref&gt;&lt;ref name="ref19"&gt;E. Barcucci, S. Brunetti, A. Del Lungo, M. Nivat, Reconstruction of lattice sets from their horizontal, vertical and diagonal X-rays, Discrete Mathematics 241(1-3): 65-78 (2001).&lt;/ref&gt; In fact, a number of discrete tomography problems were first discussed as combinatorial problems. In 1957, [[H. J. Ryser]] found a necessary and sufficient condition for a pair of vectors being the two orthogonal projections of a discrete set. In the proof of his theorem, Ryser also described a reconstruction algorithm, the very first reconstruction algorithm for a general discrete set from two orthogonal projections. In the same year, [[David Gale]] found the same consistency conditions, but in connection with the [[flow network|network flow]] problem.&lt;ref name=CMC27&gt;{{cite book | last=Brualdi | first=Richard A. | title=Combinatorial matrix classes | series=Encyclopedia of Mathematics and Its Applications | volume=108 | location=Cambridge | publisher=[[Cambridge University Press]] | year=2006 | isbn=0-521-86565-4 | zbl=1106.05001 | page=27 }}&lt;/ref&gt; Another result of Ryser's is the definition of the switching operation by which discrete sets having the same projections can be transformed into each other.

The problem of reconstructing a binary image from a small number of projections generally leads to a large number of solutions. It is desirable to limit the class of possible solutions to only those that are typical of the class of the images which contains the image being reconstructed by using a priori information, such as convexity or connectedness.

===Theorems===
* Reconstructing (finite) planar lattice sets from their 1-dimensional X-rays is an [[NP-hard]] problem if the X-rays are taken from &lt;math&gt; m\geq 3 &lt;/math&gt; lattice directions (for &lt;math&gt; m=2 &lt;/math&gt; the problem is in P).&lt;ref name="ref15" /&gt;
*The reconstruction problem is highly unstable for &lt;math&gt; m\geq 3 &lt;/math&gt; (meaning that a small perturbation of the X-rays may lead to completely different reconstructions)&lt;ref name="ref23" /&gt; and stable for &lt;math&gt; m=2 &lt;/math&gt;, see.&lt;ref name="ref23"&gt;A. Alpers, P. Gritzmann, L. Thorens, Stability and Instability in Discrete Tomography, Lecture Notes in Computer Science 2243; Digital and Image Geometry (Advanced Lectures), G. Bertrand, A. Imiya, R. Klette (Eds.), pp. 175-186, Springer-Verlag, 2001.&lt;/ref&gt;&lt;ref name="ref24"&gt;A. Alpers, S. Brunetti, On the Stability of Reconstructing Lattice Sets from X-rays Along Two Directions, Lecture Notes in Computer Science 3429; Digital Geometry for Computer Imagery,  E. Andres, G. Damiand, P. Lienhardt (Eds.), pp. 92-103, Springer-Verlag, 2005.&lt;/ref&gt;&lt;ref name="ref25"&gt;B. van Dalen, Stability results for uniquely determined sets from two directions in discrete tomography, Discrete Mathematics 309(12): 3905-3916 (2009).&lt;/ref&gt;
*Coloring a grid using &lt;math&gt; k &lt;/math&gt; colors with the restriction that each row and each column has a specific number of cells of each color is  known as the &lt;math&gt;(k-1)&lt;/math&gt;−atom problem in the discrete tomography community. The problem is NP-hard for &lt;math&gt; k \geq 3 &lt;/math&gt;, see.&lt;ref name="ref16" /&gt;

For further results see.&lt;ref name="ref4" /&gt;&lt;ref name="ref5" /&gt;

==Algorithms==
Among the reconstruction methods one can find [[algebraic reconstruction technique]]s (e.g., DART&lt;ref name="ref8" /&gt; 
&lt;ref&gt;W. van Aarle, K J. Batenburg, and J. Sijbers, Automatic parameter estimation for the Discrete Algebraic Reconstruction Technique (DART), IEEE Transactions on Image Processing, 2012 [https://dx.doi.org/10.1109/TIP.2012.2206042]&lt;/ref&gt; or &lt;ref name="ref8b"&gt;K. J. Batenburg, and J. Sijbers, "Generic iterative subset algorithms for discrete tomography", Discrete Applied Mathematics, vol. 157, no. 3, pp. 438-451, 2009&lt;/ref&gt;), [[greedy algorithm]]s (see &lt;ref name="ref22"&gt;P. Gritzmann, S. de Vries, M. Wiegelmann, Approximating binary images from discrete X-rays, SIAM J. Optim. 11 (2000), no. 2, 522-546.&lt;/ref&gt; for approximation guarantees), and [[Monte Carlo algorithm]]s.&lt;ref name="ref20" /&gt;&lt;ref name="ref21" /&gt;

==Applications==
Various algorithms have been applied in [[image processing]]
,&lt;ref name="ref8"&gt;
Batenburg, Joost; Sijbers, Jan - DART: A practical reconstruction algorithm for discrete tomography - In: IEEE transactions on image processing, Vol. 20, Nr. 9, p. 2542-2553, (2011). {{doi|10.1109/TIP.2011.2131661}}
&lt;/ref&gt; [[medicine]], 
three-dimensional statistical data security problems, computer
tomograph assisted engineering and design, [[electron microscopy]]
&lt;ref name="ref6"&gt;
S. Bals, K. J. Batenburg, J. Verbeeck, J. Sijbers and G. Van Tendeloo, "Quantitative 3D reconstruction of catalyst particles for bamboo-like carbon-nanotubes", Nano Letters, Vol. 7, Nr. 12, p. 3669-3674, (2007) {{doi|10.1021/nl071899m}}
&lt;/ref&gt;&lt;ref name="ref7"&gt;
Batenburg J., S. Bals, Sijbers J., C. Kubel, P.A. Midgley, J.C. Hernandez, U. Kaiser, E.R. Encina, E.A. Coronado and G. Van Tendeloo, "3D imaging of nanomaterials by discrete tomography", Ultramicroscopy, Vol. 109, p. 730-740, (2009) {{doi|10.1016/j.ultramic.2009.01.009}}
&lt;/ref&gt; and [[materials science]], including the [[3DXRD]] microscope.&lt;ref name="ref20"&gt;A. Alpers, H.F. Poulsen, E. Knudsen, G.T. Herman, A Discrete Tomography Algorithm for Improving the Quality of 3DXRD Grain Maps, Journal of Applied Crystallography  39, pp. 582-588, 2006 [http://www-m9.ma.tum.de/Allgemeines/AndreasAlpersPublications].&lt;/ref&gt;&lt;ref name="ref21"&gt;L. Rodek, H.F. Poulsen, E. Knudsen, G.T. Herman, A stochastic algorithm for reconstruction of grain maps of moderately deformed specimens based on X-ray diffraction, Journal of Applied Crystallography 40:313-321, 2007.&lt;/ref&gt;&lt;ref name="ref21b"&gt;K. J. Batenburg, J. Sijbers, H. F. Poulsen, and E. Knudsen, "DART: A Robust Algorithm for Fast Reconstruction of 3D Grain Maps", Journal of Applied Crystallography, vol. 43, pp. 1464-1473, 2010&lt;/ref&gt;

A form of discrete tomography also forms the basis of [[nonogram]]s, a type of [[logic puzzle]] in which information about the rows and columns of a digital image is used to reconstruct the image.&lt;ref&gt;{{Cite book| url=https://books.google.com/books?id=K98BAAAACAAJ | title=Games Magazine Presents Paint by Numbers | publisher=[[Random House]] | year=1994 | ISBN=0-8129-2384-7}}&lt;/ref&gt;

==See also==
*[[Geometric tomography]]

==References==
{{reflist}}

== External links ==
*[https://web.archive.org/web/20111007015951/http://astra.ua.ac.be/euroDT/index.php/Main_Page Euro DT (a Discrete Tomography Wiki site for researchers)]
*[http://www-desir.lip6.fr/~durrc/Xray/Complexity/ Tomography applet by Christoph Dürr]
*[http://visielab.uantwerpen.be/publications/tomographic-segmentation-and-discrete-tomography-quantitative-analysis-transmission PhD thesis on discrete tomography (2012): Tomographic segmentation and discrete tomography for quantitative analysis of transmission tomography data]

[[Category:Applied mathematics]]
[[Category:Digital geometry]]</text>
      <sha1>1y736xnmxo23y1r2hbef8kofqkmoc25</sha1>
    </revision>
  </page>
  <page>
    <title>Edith Cohen</title>
    <ns>0</ns>
    <id>58795079</id>
    <revision>
      <id>869228285</id>
      <parentid>869225930</parentid>
      <timestamp>2018-11-17T06:47:18Z</timestamp>
      <contributor>
        <username>Duncan.Hull</username>
        <id>3507210</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4870">{{Infobox scientist
| honorific_prefix  = 
| name              = 
| honorific_suffix  = 
| native_name       = 
| native_name_lang  = 
| image             = &lt;!--(filename only, i.e. without "File:" prefix)--&gt;
| image_size        = 
| image_upright     = 
| alt               = 
| caption           = 
| birth_name        = &lt;!-- if different from "name" --&gt;
| birth_date        = {{birth date and age|1966|05|21}}
| birth_place       = 
| fields            = {{Plainlist|
* [[Algorithm]]s&lt;ref name=gs/&gt;
* [[Computer network|Networking]]&lt;ref name=gs/&gt;
* [[Data Mining]]&lt;ref name=gs/&gt;
* [[Mathematical optimization|Optimization]]&lt;ref name=gs/&gt;}}
| workplaces        = [[IBM Research - Almaden]]&lt;br&gt;[[Google Inc]]&lt;br&gt;[[Bell Labs]]
| patrons           = 
| education         = 
| alma_mater        = [[Tel Aviv University]]&lt;br&gt;[[Stanford University]] (PhD)
| thesis_title      = Combinatorial Algorithms for Optimization Problems
| thesis_url        = http://www.worldcat.org/oclc/753884177
| thesis_year       = 1992
| doctoral_advisor  = [[Andrew V. Goldberg]]&lt;ref name=mgp/&gt;
| academic_advisors = 
| doctoral_students = 
| notable_students  = 
| known_for         = 
| influences        = [[Nimrod Megiddo]]&lt;ref name=mgp/&gt;
| influenced        = 
| awards            = [[ACM Fellow]] (2017)&lt;ref name=facm/&gt;
| spouse            = &lt;!--(or | spouses = )--&gt;
| partner           = &lt;!--(or | partners = )--&gt;
| children          = 
| signature         = &lt;!--(filename only)--&gt;
| signature_alt     = 
| website           = {{URL|cohenwang.com/edith}}}}'''Edith Cohen''' (born May 21, 1966) is an Israeli and American{{r|cv}} computer scientist specializing in [[data mining]] and [[algorithm]]s for [[big data]].&lt;ref name=gs&gt;{{Google scholar id}}&lt;/ref&gt; She is also known for her research on [[peer-to-peer]] networks. She works for [[Google]] in [[Mountain View, California]], and as a visiting professor at [[Tel Aviv University]] in Israel.

==Education==
Cohen is originally from [[Tel Aviv]], where her father was a banker.{{r|idit}}
She earned a bachelor's degree in 1985 and a master's degree in 1986 from [[Tel Aviv University]]; her master's thesis was supervised by Michael Tarsi.{{r|cv}} She moved to [[Stanford University]] for her doctoral studies, and completed her Ph.D. in 1991 with [[Andrew V. Goldberg]] as her [[doctoral advisor]] and [[Nimrod Megiddo]] as an unofficial mentor. Her dissertation was ''Combinatorial Algorithms for Optimization Problems''.{{r|cv|mgp|goog}}

==Career and research==
Cohen was a student researcher at [[IBM Research - Almaden]] from 1987 to 1991, and a researcher at [[Bell Labs]] and its successor [[AT&amp;T Labs]] from 1991 to 2012. In 2012, she became a visiting professor at Tel Aviv University, and began working for [[Microsoft Research]], as a visitor for one year and then as a principal researcher.{{r|cv}} She has been associated with Google since 2015.{{r|goog}}

===Awards and honors===
Cohen won the William R. Bennett prize of the [[IEEE Communications Society]] in 2007 with [[David Applegate]], for their work on robust network [[routing]].{{r|cv|bennett}}
She was nominated an [[ACM Fellow]] in 2017 "for contributions to the design of efficient algorithms for networking and big data".{{r|facm}}

==References==
{{reflist|refs=

&lt;ref name=bennett&gt;{{citation|url=https://www.comsoc.org/about/memberprograms/comsoc-awards/bennett|title=The IEEE Communications Society William R. Bennett prize|publisher=[[IEEE Communications Society]]|accessdate=2018-10-17}}&lt;/ref&gt;

&lt;ref name=cv&gt;{{citation|url=http://www.cohenwang.com/edith/cv.pdf|title=Edith Cohen Curriculum vitae|year=2014|accessdate=2018-10-17}}&lt;/ref&gt;

&lt;ref name=facm&gt;{{citation|url=https://www.acm.org/media-center/2017/december/fellows-2017|title=ACM Recognizes 2017 Fellows for Making Transformative Contributions and Advancing Technology in the Digital Age|publisher=[[Association for Computing Machinery]]|date=December 11, 2017|accessdate=2018-10-17}}&lt;/ref&gt;

&lt;ref name=goog&gt;{{citation|url=https://ai.google/research/people/EdithCohen|title=Edith Cohen @GoogleAI|publisher=Google|website=ai.google|accessdate=2018-10-17}}&lt;/ref&gt;

&lt;ref name=idit&gt;{{citation|url=http://cohen-wang.com/idit.html|title=Edith Cohen (E-deet)|first=Edith|last=Cohen|date=May 2000|accessdate=2018-10-17}}&lt;/ref&gt;

&lt;ref name=mgp&gt;{{mathgenealogy}}&lt;/ref&gt;

}}


{{Authority control}}
{{DEFAULTSORT:Cohen, Edith}}
[[Category:1966 births]]
[[Category:Living people]]
[[Category:American computer scientists]]
[[Category:Israeli computer scientists]]
[[Category:Israeli women computer scientists]]
[[Category:Theoretical computer scientists]]
[[Category:Tel Aviv University alumni]]
[[Category:Stanford University alumni]]
[[Category:Scientists at Bell Labs]]
[[Category:Tel Aviv University faculty]]
[[Category:Google employees]]
[[Category:Fellows of the Association for Computing Machinery]]</text>
      <sha1>qflaaxpaucwx2jy1u2tet9xn3mobycc</sha1>
    </revision>
  </page>
  <page>
    <title>Equioscillation theorem</title>
    <ns>0</ns>
    <id>32446743</id>
    <revision>
      <id>749502385</id>
      <parentid>663181132</parentid>
      <timestamp>2016-11-14T17:54:58Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <minor/>
      <comment>1 archive template merged to {{[[template:webarchive|webarchive]]}} ([[User:Green_Cardamom/Webarchive_template_merge|WAM]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1486">{{multiple issues|
{{context|date=March 2012}}
{{no footnotes|date=March 2012}}
}}
The '''equioscillation theorem''' concerns the [[Approximation theory|approximation]] of [[continuous function]]s using [[polynomial]]s when the merit function is the maximum difference ([[uniform norm]]). Its discovery is attributed to [[Pafnuty Chebyshev|Chebyshev]].

== Statement ==
Let &lt;math&gt;f&lt;/math&gt; be a continuous function from &lt;math&gt;[a,b]&lt;/math&gt; to &lt;math&gt;\mathbf{R}&lt;/math&gt;. Among all the polynomials of degree &lt;math&gt;\le n&lt;/math&gt;, the polynomial &lt;math&gt;g&lt;/math&gt; minimizes the uniform norm of the difference &lt;math&gt; || f - g || _\infty &lt;/math&gt; if and only if there are &lt;math&gt;n+2&lt;/math&gt; points &lt;math&gt;a \le x_0 &lt; x_1 &lt; \cdots &lt; x_{n+1} \le b&lt;/math&gt; such that &lt;math&gt;f(x_i) - g(x_i) = \sigma (-1)^i || f - g || _\infty&lt;/math&gt; where &lt;math&gt;\sigma = \pm 1&lt;/math&gt;.

== Algorithms ==
Several [[minimax approximation algorithm]]s are available, the most common being the [[Remez algorithm]].

== References ==
* {{webarchive |url=https://web.archive.org/web/20110702221651/http://www.math.uiowa.edu/~jeichhol/qual%20prep/Notes/cheb-equiosc-thm_2007.pdf |date=July 2, 2011 |title=Notes on how to prove Chebyshev’s equioscillation theorem }}
* [http://www.maa.org/publications/periodicals/loci/joma/the-chebyshev-equioscillation-theorem The Chebyshev Equioscillation Theorem by Robert Mayans]

[[Category:Polynomials]]
[[Category:Numerical analysis]]
[[Category:Theorems in analysis]]


{{mathanalysis-stub}}</text>
      <sha1>h7nd927j6wwmqgdqbtyad4uzct3kz9o</sha1>
    </revision>
  </page>
  <page>
    <title>Erlangen program</title>
    <ns>0</ns>
    <id>243382</id>
    <revision>
      <id>855381032</id>
      <parentid>855324522</parentid>
      <timestamp>2018-08-17T21:35:25Z</timestamp>
      <contributor>
        <username>Rgdboer</username>
        <id>92899</id>
      </contributor>
      <minor/>
      <comment>/* Abstract returns from the Erlangen program */ lk Linear fractional transformation</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14352">The '''Erlangen program''' is a method of characterizing [[geometry|geometries]] based on [[group theory]] and [[projective geometry]]. It was published by [[Felix Klein]] in 1872 as ''Vergleichende Betrachtungen über neuere geometrische Forschungen.'' It is named after the [[University of Erlangen-Nuremberg|University Erlangen-Nürnberg]], where Klein worked.

By 1872, [[non-Euclidean geometry|non-Euclidean geometries]] had emerged, but without a way to determine their hierarchy and relationships. Klein's method was fundamentally innovative in three ways:

:* Projective geometry was emphasized as the unifying frame for all other geometries considered by him. In particular, [[Euclidean geometry]] was more restrictive than [[affine geometry]], which in turn is more restrictive than projective geometry.

:* Klein proposed that [[group theory]], a branch of mathematics that uses algebraic methods to abstract the idea of [[symmetry]], was the most useful way of organizing geometrical knowledge; at the time it had already been introduced into the [[theory of equations]] in the form of [[Galois theory]].

:* Klein made much more explicit the idea that each geometrical language had its own, appropriate concepts, thus for example projective geometry rightly talked about [[conic section]]s, but not about [[circle]]s or [[angle]]s because those notions were not invariant under [[projective transformation]]s (something familiar in [[geometrical perspective]]). The way the multiple languages of geometry then came back together could be explained by the way [[subgroup]]s of a [[symmetry group]] related to each other.

Later, [[Élie Cartan]] generalized Klein's homogeneous model spaces to [[Cartan connection]]s on certain principal bundles, which generalized [[Riemannian geometry]].

==The problems of nineteenth century geometry==

Since [[Euclid]], geometry had meant the geometry of [[Euclidean space]] of two dimensions ([[Euclidean plane geometry|plane geometry]]) or of three dimensions ([[solid geometry]]). In the first half of the nineteenth century there had been several developments complicating the picture. Mathematical applications required geometry of [[higher dimensions|four or more dimensions]]; the close scrutiny of the foundations of the traditional Euclidean geometry had revealed the independence of the [[parallel postulate]] from the others, and [[non-Euclidean geometry]] had been born. Klein proposed an idea that all these new geometries are just special cases of the [[projective geometry]], as already developed by [[Jean-Victor Poncelet|Poncelet]], [[August Ferdinand Möbius|Möbius]], [[Arthur Cayley|Cayley]] and others. Klein also strongly suggested to mathematical ''physicists'' that even a moderate cultivation of the projective purview might bring substantial benefits to them.

With every geometry, Klein associated an underlying [[symmetry group|group of symmetries]]. The hierarchy of geometries is thus mathematically represented as a hierarchy of these [[group (mathematics)|groups]], and hierarchy of their [[invariant (mathematics)|invariants]]. For example, lengths, angles and areas are preserved with respect to the [[Euclidean geometry|Euclidean group]] of symmetries, while only the [[incidence structure]] and the [[cross-ratio]] are preserved under the most general [[projective geometry|projective transformations]]. A concept of [[parallel (geometry)|parallel]]ism, which is preserved in [[affine geometry]], is not meaningful in [[projective geometry]]. Then, by abstracting the underlying [[group (mathematics)|groups]] of symmetries from the geometries, the relationships between them can be re-established at the group level. Since the group of affine geometry is a [[subgroup]] of the group of projective geometry, any notion invariant in projective geometry is ''a priori'' meaningful in affine geometry; but not the other way round. If you add required symmetries, you have a more powerful theory but fewer concepts and theorems (which will be deeper and more general).

==Homogeneous spaces==

In other words, the "traditional spaces" are [[homogeneous space]]s; but not for a uniquely determined group. Changing the group changes the appropriate geometric language.

In today's language, the groups concerned in classical geometry are all very well known as [[Lie group]]s: the [[classical groups]]. The specific relationships are quite simply described, using technical language.

===Examples===

For example, the group of [[projective geometry]] in ''n'' real-valued dimensions is the symmetry group of ''n''-dimensional real [[projective space]] (the [[general linear group]] of degree {{nowrap|''n'' + 1}}, quotiented by [[Scalar matrix|scalar matrices]]). The [[affine group]] will be the subgroup respecting (mapping to itself, not fixing pointwise) the chosen [[hyperplane at infinity]]. This subgroup has a known structure ([[semidirect product]] of the [[general linear group]] of degree ''n'' with the subgroup of [[translation (geometry)|translation]]s). This description then tells us which properties are 'affine'. In Euclidean plane geometry terms, being a parallelogram is affine since affine transformations always take one parallelogram to another one. Being a circle is not affine since an affine shear will take a circle into an ellipse.

To explain accurately the relationship between affine and Euclidean geometry, we now need to pin down the group of Euclidean geometry within the affine group. The [[Euclidean group]] is in fact (using the previous description of the affine group) the semi-direct product of the orthogonal (rotation and reflection) group with the translations.  (See [[Klein geometry]] for more details.)

==Influence on later work==

The long-term effects of the Erlangen program can be seen all over pure mathematics (see tacit use at [[congruence (geometry)]], for example); and the idea of transformations and of synthesis using groups of [[symmetry (physics)|symmetry]] has become standard in [[physics]].

When [[topology]] is routinely described in terms of properties [[invariant (mathematics)|invariant]] under [[homeomorphism]], one can see the underlying idea in operation. The groups involved will be infinite-dimensional in almost all cases – and not [[Lie group]]s – but the philosophy is the same. Of course this mostly speaks to the pedagogical influence of Klein. Books such as those by [[H.S.M. Coxeter]] routinely used the Erlangen program approach to help 'place' geometries. In pedagogic terms, the program became [[transformation geometry]], a mixed blessing in the sense that it builds on stronger intuitions than the style of [[Euclid]], but is less easily converted into a [[logical system]].

In his book ''Structuralism'' (1970) [[Jean Piaget]] says, "In the eyes of contemporary structuralist mathematicians, like [[Nicolas Bourbaki|Bourbaki]], the Erlangen Program amounts to only a partial victory for structuralism, since they want to subordinate all mathematics, not just geometry, to the idea of [[mathematical structure|structure]]."

For a geometry and its group, an element of the group is sometimes called a [[motion (geometry)|motion]] of the geometry. For example, one can learn about the [[Poincaré half-plane model]] of [[hyperbolic geometry]] through a development based on [[hyperbolic motion]]s. Such a development enables one to methodically prove the [[ultraparallel theorem]] by successive motions.

==Abstract returns from the Erlangen program==
Quite often, it appears there are two or more distinct [[Geometry|geometries]] with [[isomorphic]] [[automorphism group]]s. There arises the question of reading the Erlangen program from the ''abstract'' group, to the geometry.

One example: [[oriented]] (i.e., [[Reflection (mathematics)|reflections]] not included) [[elliptic geometry]] (i.e., the surface of an [[n-sphere|''n''-sphere]] with opposite points identified) and [[oriented]] [[spherical geometry]] (the same [[nonEuclidean geometry|non-Euclidean geometry]], but with opposite points not identified) have [[isomorphic]] [[automorphism group]], [[Special orthogonal group|SO(''n''+1)]] for even ''n''. These may appear to be distinct. It turns out, however, that the geometries are very closely related, in a way that can be made precise.

To take another example, [[Elliptic geometry|elliptic geometries]] with different [[Radius of curvature (mathematics)|radii of curvature]] have isomorphic automorphism groups. That does not really count as a critique as all such geometries are isomorphic. General [[Riemannian geometry]] falls outside the boundaries of the program.

[[Complex numbers|Complex]], [[dual numbers|dual]] and [[split-complex number|double (aka split-complex) numbers]] appear as homogeneous spaces SL(2,'''R''')/H for the group [[SL2(R)|SL(2,'''R''')]] and its subgroups H=A, N, K &lt;ref name="raw"&gt;{{cite book |last=Kisil |first=Vladimir V. |year=2012 |title=Geometry of Möbius transformations. Elliptic, parabolic and hyperbolic actions of SL(2,R) | location=London |publisher=Imperial College Press|page=xiv+192 |isbn=978-1-84816-858-9 | doi=10.1142/p835}}&lt;/ref&gt;. The group SL(2,'''R''') acts on these homogeneous spaces by [[linear fractional transformation]]s and a large portion of the respective geometries can be obtained in a uniform way from the Erlangen programme.

Some further notable examples have come up in physics.

Firstly, ''n''-dimensional [[hyperbolic geometry]], ''n''-dimensional [[de Sitter space]] and (''n''−1)-dimensional [[inversive geometry]] all have isomorphic automorphism groups,

:&lt;math&gt;\mathrm{O}(n,1)/\mathrm{C}_2,\ &lt;/math&gt;

the [[orthochronous Lorentz group]], for {{nowrap|''n'' ≥ 3}}. But these are apparently distinct geometries. Here some interesting results enter, from the physics. It has been shown that physics models in each of the three geometries are "dual" for some models.

Again, ''n''-dimensional [[anti-de Sitter space]] and (''n''−1)-dimensional [[conformal space]] with "Lorentzian" signature (in contrast with [[conformal space]] with "Euclidean" signature, which is identical to [[inversive geometry]], for three dimensions or greater) have isomorphic automorphism groups, but are distinct geometries. Once again, there are models in physics with "dualities" between both [[Geometry|spaces]]. See [[AdS/CFT]] for more details.

The covering group of SU(2,2) is isomorphic to the covering group of SO(4,2), which is the symmetry group of a 4D conformal Minkowski space and a 5D anti-de Sitter space and a complex four-dimensional [[twistor space]].

The Erlangen program can therefore still be considered fertile, in relation with dualities in physics.

In the seminal paper which introduced [[Category theory|categories]], [[Saunders Mac Lane]] and S. Eilenberg stated: "This may be regarded as a continuation of the Klein Erlanger Program, in the sense that a geometrical space with its group of transformations is generalized to a category with its algebra of mappings"&lt;ref&gt;S. Eilenberg and S. Mac Lane, ''A general theory of natural equivalences'', Trans. Amer. Math. Soc., 58:231–294, 1945. (p. 237); the point is elaborated in Jean-Pierre Marquis (2009), ''From a Geometrical Point of View: A Study of the History of Category Theory'', Springer, {{ISBN|978-1-4020-9383-8}}&lt;/ref&gt;

Relations of the Erlangen program with work of C. [[Ehresmann]] on [[groupoids]] in geometry is considered in the article below by  Pradines.&lt;ref&gt;Jean Pradines, ''In [[Ehresmann]]'s footsteps: from group geometries to [[groupoid]] geometries'' (English summary) Geometry and topology of manifolds, 87–157, Banach Center Publ., 76, Polish Acad. Sci., Warsaw, 2007.&lt;/ref&gt;

In mathematical logic, the Erlangen Program also served as an inspiration for [[Alfred Tarski]] in his analysis of [[Alfred Tarski#Work on logical notions|logical notions]].&lt;ref&gt;Luca Belotti, ''Tarski on Logical Notions'', Synthese, 404-413, 2003.&lt;/ref&gt;

==References==
{{wikibooks|Geometry|Groups}}
{{reflist}}

*Klein, Felix (1872) "A comparative review of recent researches in geometry". Complete English Translation is here https://arxiv.org/abs/0807.3161.
*Sharpe, Richard W. (1997) ''Differential geometry: Cartan's generalization of Klein's Erlangen program'' Vol. 166. Springer.
*[[Heinrich Guggenheimer]] (1977) ''Differential Geometry'', Dover, New York, {{ISBN|0-486-63433-7}}.
:Covers the work of Lie, Klein and Cartan. On p. 139 Guggenheimer sums up the field by noting, "A Klein geometry is the theory of geometric invariants of a transitive transformation group (Erlangen program, 1872)".

* Thomas Hawkins (1984) "The ''Erlanger Program'' of Felix Klein: Reflections on Its Place In the History of Mathematics", [[Historia Mathematica]] 11:442&amp;ndash;70.
* {{springer|title=Erlangen program|id=p/e036190}}
*  Lizhen Ji and Athanase Papadopoulos (editors) (2015) ''Sophus Lie and Felix Klein: The Erlangen program and its impact in mathematics and physics'', IRMA Lectures in Mathematics and Theoretical Physics 23, European Mathematical Society Publishing House, Zürich.
*[[Felix Klein]] (1872) "Vergleichende Betrachtungen über neuere geometrische Forschungen" ('A comparative review of recent researches in geometry'), Mathematische Annalen, 43 (1893) pp.&amp;nbsp;63–100 (Also: Gesammelte Abh. Vol. 1, Springer, 1921, pp.&amp;nbsp;460–497).
:An English translation by [[Mellen Haskell]] appeared in ''Bull. N. Y. Math. Soc'' 2 (1892–1893): 215–249.

:The original German text of the Erlangen Program can be viewed at the University of Michigan online collection at [http://www.hti.umich.edu/cgi/t/text/text-idx?c=umhistmath;idno=ABN7632], and also at [http://www.xs4all.nl/~jemebius/ErlangerProgramm.htm] in HTML format.
:A central information page on the Erlangen Program maintained by [[John Baez]] is at [http://math.ucr.edu/home/baez/erlangen/].

*[[Felix Klein]] (2004) ''Elementary Mathematics from an Advanced Standpoint: Geometry'', Dover, New York, {{ISBN|0-486-43481-8}}
:(translation of ''Elementarmathematik vom höheren Standpunkte aus'', Teil II: Geometrie, pub. 1924 by Springer). Has a section on the Erlangen Program.


{{DEFAULTSORT:Erlangen Program}}
[[Category:Classical geometry]]
[[Category:Symmetry]]
[[Category:Group theory]]
[[Category:Homogeneous spaces]]
[[Category:Erlangen]]</text>
      <sha1>4owgxgzq9naviz87u4mda4ia15dze8b</sha1>
    </revision>
  </page>
  <page>
    <title>Four color theorem</title>
    <ns>0</ns>
    <id>10949</id>
    <revision>
      <id>871526880</id>
      <parentid>871503296</parentid>
      <timestamp>2018-12-01T18:18:37Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>Undid revision 871503296 by [[Special:Contributions/JoyceJonathan2|JoyceJonathan2]] ([[User talk:JoyceJonathan2|talk]]) I think keeping one example that's more familiar to the western English-speaking audience is helpful</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="41903">[[File:Four Colour Map Example.svg|thumb|Example of a four-colored map]][[File:Map of United States vivid colors shown.png|thumb|A four-coloring of a map of the states of the United States (ignoring lakes).]]
In [[mathematics]], the '''four color theorem''', or the '''four color map theorem''', states that, given any separation of a plane into [[wikt:contiguity|contiguous]] regions, producing a figure called a ''map'', no more than four colors are required to color the regions of the map so that no two adjacent regions have the same color. ''Adjacent'' means that two regions share a common boundary curve segment, not merely a corner where three or more regions meet.&lt;ref&gt;From {{harvtxt|Gonthier|2008}}: "Definitions: A planar map is a set of pairwise disjoint subsets of the plane, called regions. A simple map is one whose regions are connected open sets. Two regions of a map are adjacent if their respective closures have a common point that is not a corner of the map. A point is a corner of a map if and only if it belongs to the closures of at least three regions. Theorem: The regions of any simple planar map can be colored with only four colors, in such a way that any two adjacent regions have different colors."&lt;/ref&gt;

Unlike the [[five color theorem]], a theorem that states that five colors are enough to color a map, which was proved in the 1800s, the four color theorem was proved in 1976 by [[Kenneth Appel]] and [[Wolfgang Haken]], but only after many false proofs and counterexamples. It was the first major [[theorem]] to be [[computer-assisted proof#List of theorems proved with the help of computer programs|proved using a computer]]. Initially, their proof was not accepted by all mathematicians because the [[computer-assisted proof]] was [[Non-surveyable proof|infeasible for a human to check by hand]].{{sfnp|Swart|1980}} Since then the proof has gained wide acceptance, although some doubters remain.{{sfnp|Wilson|2014|loc=216–222}}

To dispel any remaining doubts about the Appel–Haken proof, a simpler proof using the same ideas and still relying on computers was published in 1997 by Robertson, Sanders, Seymour, and Thomas. Additionally, in 2005, the theorem was proved by [[Georges Gonthier]] with general-purpose [[proof assistant|theorem-proving software]].

==Precise formulation of the theorem==
[[File:World map with four colours.svg|thumb|A map of the world using just four colors for countries. The map also uses a fifth unique color (white) for oceans and lakes. This could be eliminated by recoloring, but then some [[landlocked country|landlocked countries]] would share color with the ocean, and some lakes and the ocean would differ.]]
The intuitive statement of the four color theorem, i.e. "that given any separation of a plane into contiguous regions, called a map, the regions can be colored using at most four colors so that no two adjacent regions have the same color", needs to be interpreted appropriately to be correct.

First, all corners, points that belong to (technically, are in the closure of) three or more countries, must be ignored. In addition, bizarre maps (using regions of finite area but infinite perimeter) can require more than four colors.{{sfnp|Hudson|2003}} Second, for the purpose of the theorem, every "country" has to be a [[connected space|connected region]], or [[Geographic contiguity|contiguous]]. In the real world, [[Enclave and exclave|this is not true]] (e.g., the [[Upper Peninsula of Michigan|Upper]] and [[Lower Peninsula of Michigan]], [[Nakhchivan Autonomous Republic|Nakhchivan]] as part of [[Azerbaijan]], and [[Kaliningrad Oblast|Kaliningrad]] as part of Russia are not contiguous). Because all the territory of a particular country must be the same color, four colors may not be sufficient. For instance, consider a simplified map:

[[File:4CT Inadequacy Example.svg|center]]

In this map, the two regions labeled ''A'' belong to the same country, and must be the same color. This map then requires five colors, since the two ''A'' regions together are contiguous with four other regions, each of which is contiguous with all the others. A similar construction also applies if a single color is used for all bodies of water, as is usual on real maps. For maps in which more than one country may have multiple disconnected regions, six or more colors might be required.

A simpler statement of the theorem uses [[graph theory]]. The set of regions of a map can be represented more abstractly as an [[undirected graph]] that has a [[vertex (graph theory)|vertex]] for each region and an [[edge (graph theory)|edge]] for every pair of regions that share a boundary segment. This graph is [[planar graph|planar]] (it is important to note that we are talking about the graphs that have some limitations according to the map they are transformed from only): it can be drawn in the plane without crossings by placing each vertex at an arbitrarily chosen location within the region to which it corresponds, and by drawing the edges as curves that lead without crossing within each region from the vertex location to each shared boundary point of the region. Conversely any planar graph can be formed from a map in this way. In graph-theoretic terminology, the four-color theorem states that the vertices of every planar graph can be [[graph coloring|colored]] with at most four colors so that no two adjacent vertices receive the same color, or for short, "every planar graph is four-colorable".&lt;ref&gt;{{harvtxt|Thomas|1998|p=849}}; {{harvtxt|Wilson|2014}}).&lt;/ref&gt;

[[File:Four Colour Planar Graph.svg|center]]

==History==&lt;!-- This section is linked from [[Mathematics]] --&gt;

===Early proof attempts===
[[File:DeMorganFourColour.png|thumb|Letter of De Morgan to [[William Rowan Hamilton|Hamilton]], 23 Oct. 1852]]
As far as is known, the conjecture was first proposed on October 23, 1852&lt;ref name=MacKenzie&gt;Donald MacKenzie, ''Mechanizing Proof: Computing, Risk, and Trust'' (MIT Press, 2004) p103&lt;/ref&gt; when [[Francis Guthrie]], while trying to color the map of counties of England, noticed that only four different colors were needed. At the time, Guthrie's brother, Frederick, was a student of [[Augustus De Morgan]] (the former advisor of Francis) at [[University College London]]. Francis inquired with Frederick regarding it, who then took it to De Morgan (Francis Guthrie graduated later in 1852, and later became a professor of mathematics in South Africa). According to De Morgan:

&lt;blockquote&gt;"A student of mine [Guthrie] asked me to day to give him a reason for a fact which I did not know was a fact—and do not yet. He says that if a figure be any how divided and the compartments differently colored so that figures with any portion of common boundary ''line'' are differently colored—four colors may be wanted but not more—the following is his case in which four colors ''are'' wanted. Query cannot a necessity for five or more be invented…" {{Harv|Wilson|2014|p=18}}&lt;/blockquote&gt;

"F.G.", perhaps one of the two Guthries, published the question in ''[[Athenaeum (British magazine)|The Athenaeum]]'' in 1854,&lt;ref&gt;{{harvtxt|F. G.|1854}}; {{harvtxt|McKay|2012}}&lt;/ref&gt; and De Morgan posed the question again in the same magazine in 1860.&lt;ref name=Athenaeum1860&gt;{{citation|journal=[[Athenaeum (British magazine)|The Athenaeum]]|date=April 14, 1860|first=Augustus|last=De Morgan (anonymous)|authorlink=Augustus De Morgan|pages=501–503|title=The Philosophy of Discovery, Chapters Historical and Critical. By W. Whewell.}}&lt;/ref&gt; Another early published reference by {{harvs|first=Arthur|last=Cayley|authorlink=Arthur Cayley|year=1879|txt}} in turn credits the conjecture to De Morgan.

There were several early failed attempts at proving the theorem. De Morgan believed that it followed from a simple fact about four regions, though he didn't believe that fact could be derived from more elementary facts.
&lt;blockquote&gt;This arises in the following way. We never need four colors in a neighborhood unless there be four counties, each of which has boundary lines in common with each of the other three. Such a thing cannot happen with four areas unless one or more of them be inclosed by the rest; and the color used for the inclosed county is thus set free to go on with. Now this principle, that four areas cannot each have common boundary with all the other three without inclosure, is not, we fully believe, capable of demonstration upon anything more evident and more elementary; it must stand as a postulate.&lt;ref name=Athenaeum1860 /&gt;&lt;/blockquote&gt;

One alleged proof was given by [[Alfred Kempe]] in 1879, which was widely acclaimed;&lt;ref name="rouse_ball_1960"&gt;[[W. W. Rouse Ball]] (1960) ''The Four Color Theorem'', in Mathematical Recreations and Essays, Macmillan, New York, pp 222–232.&lt;/ref&gt; another was given by [[Peter Guthrie Tait]] in 1880. It was not until 1890 that Kempe's proof was shown incorrect by [[Percy Heawood]], and in 1891, Tait's proof was shown incorrect by [[Julius Petersen]]—each false proof stood unchallenged for 11 years.{{sfnp|Thomas|1998|p=848}}

In 1890, in addition to exposing the flaw in Kempe's proof, Heawood proved the [[five color theorem]] and generalized the four color conjecture to surfaces of arbitrary genus.{{sfnp|Heawood|1890}}

Tait, in 1880, showed that the four color theorem is equivalent to the statement that a certain type of graph (called a [[snark (graph theory)|snark]] in modern terminology) must be non-[[planar graph|planar]].{{sfnp|Tait|1880}}

In 1943, [[Hugo Hadwiger]] formulated the [[Hadwiger conjecture (graph theory)|Hadwiger conjecture]],{{sfnp|Hadwiger|1943}} a far-reaching generalization of the four-color problem that still remains unsolved.

===Proof by computer===
During the 1960s and 1970s German mathematician [[Heinrich Heesch]] developed methods of using computers to search for a proof. Notably he was the first to use [[discharging method (discrete mathematics)|discharging]] for proving the theorem, which turned out to be important in the unavoidability portion of the subsequent Appel&amp;ndash;Haken proof. He also expanded on the concept of reducibility and, along with Ken Durre, developed a computer test for it. Unfortunately, at this critical juncture, he was unable to procure the necessary supercomputer time to continue his work.{{sfnp|Wilson|2014}}

Others took up his methods and his computer-assisted approach. While other teams of mathematicians were racing to complete proofs, [[Kenneth Appel]] and [[Wolfgang Haken]] at the [[University of Illinois at Urbana–Champaign|University of Illinois]] announced, on June 21, 1976,&lt;ref&gt;Gary Chartrand and Linda Lesniak, ''Graphs &amp; Digraphs'' (CRC Press, 2005) p.221&lt;/ref&gt; that they had proved the theorem. They were assisted in some algorithmic work by [[John A. Koch]].{{sfnp|Wilson|2014}}

If the four-color conjecture were false, there would be at least one map with the smallest possible number of regions that requires five colors. The proof showed that such a minimal counterexample cannot exist, through the use of two technical concepts:&lt;ref&gt;{{harvtxt|Wilson|2014}}; {{harvtxt|Appel|Haken|1989}}; {{harvtxt|Thomas|1998|pp=852–853}}&lt;/ref&gt;

# An ''unavoidable set'' is a set of configurations such that every map that satisfies some necessary conditions for being a minimal non-4-colorable [[maximal planar graph|triangulation]] (such as having minimum degree 5) must have at least one configuration from this set.
# A ''reducible configuration'' is an arrangement of countries that cannot occur in a minimal counterexample. If a map contains a reducible configuration, then the map can be reduced to a smaller map. This smaller map has the condition that if it can be colored with four colors, then the original map can also. This implies that if the original map cannot be colored with four colors the smaller map can't either and so the original map is not minimal.

Using mathematical rules and procedures based on properties of reducible configurations, Appel and Haken found an unavoidable set of reducible configurations, thus proving that a minimal counterexample to the four-color conjecture could not exist. Their proof reduced the infinitude of possible maps to 1,936 reducible configurations (later reduced to 1,476) which had to be checked one by one by computer and took over a thousand hours. This reducibility part of the work was independently double checked with different programs and computers. However, the unavoidability part of the proof was verified in over 400 pages of [[microfiche]], which had to be checked by hand with the assistance of Haken's daughter [[Dorothea Blostein]] {{Harv|Appel|Haken|1989}}.

Appel and Haken's announcement was widely reported by the news media around the world, and the math department at the [[University of Illinois]] used a postmark stating "Four colors suffice." At the same time the unusual nature of the proof—it was the first major theorem to be proved with extensive computer assistance—and the complexity of the human-verifiable portion aroused considerable controversy {{Harv|Wilson|2014}}.

In the early 1980s, rumors spread of a flaw in the Appel&amp;ndash;Haken proof. Ulrich Schmidt at [[RWTH Aachen]] examined Appel and Haken's proof for his master's thesis {{Harv|Wilson|2014|loc=225}}. He had checked about 40% of the unavoidability portion and found a significant error in the discharging procedure {{Harv|Appel|Haken|1989}}. In 1986, Appel and Haken were asked by the editor of ''[[Mathematical Intelligencer]]'' to write an article addressing the rumors of flaws in their proof. They responded that the rumors were due to a "misinterpretation of [Schmidt's] results" and obliged with a detailed article {{Harv|Wilson|2014|loc=225–226}}. Their [[Masterpiece|magnum opus]], ''Every Planar Map is Four-Colorable'', a book claiming a complete and detailed proof (with a microfiche supplement of over 400 pages), appeared in 1989 and explained Schmidt's discovery and several further errors found by others {{Harv|Appel|Haken|1989}}.

===Simplification and verification===
Since the proving of the theorem, efficient algorithms have been found for 4-coloring maps requiring only [[Big O notation|O]](''n''&lt;sup&gt;2&lt;/sup&gt;) time, where ''n'' is the number of vertices. In 1996, [[Neil Robertson (mathematician)|Neil Robertson]], [[Daniel P. Sanders]], [[Paul Seymour (mathematician)|Paul Seymour]], and [[Robin Thomas (mathematician)|Robin Thomas]] created a [[polynomial time|quadratic-time]] algorithm, improving on a [[quartic function|quartic]]-time algorithm based on Appel and Haken’s proof&lt;ref&gt;{{harvtxt|Thomas|1995}}; {{harvtxt|Robertson|Sanders|Seymour|Thomas|1996}}).&lt;/ref&gt; This new proof is similar to Appel and Haken's but more efficient because it reduces the complexity of the problem and requires checking only 633 reducible configurations. Both the unavoidability and reducibility parts of this new proof must be executed by computer and are impractical to check by hand.{{sfnp|Thomas|1998|pp=852–853}} In 2001, the same authors announced an alternative proof, by proving the [[snark theorem]].&lt;ref&gt;{{harvtxt|Thomas|1999}}; {{harvtxt|Pegg|Melendez|Berenguer|Sendra|2002}}).&lt;/ref&gt;

In 2005, [[Benjamin Werner]] and [[Georges Gonthier]] formalized a proof of the theorem inside the [[Coq]] proof assistant. This removed the need to trust the various computer programs used to verify particular cases; it is only necessary to trust the Coq kernel.{{sfnp|Gonthier|2008}}

==Summary of proof ideas==
The following discussion is a summary based on the introduction to ''Every Planar Map is Four Colorable'' {{Harv|Appel|Haken|1989}}. Although flawed, Kempe's original purported proof of the four color theorem provided some of the basic tools later used to prove it. The explanation here is reworded in terms of the modern graph theory formulation above.

Kempe's argument goes as follows. First, if planar regions separated by the graph are not ''[[triangulation (geometry)|triangulated]]'', i.e. do not have exactly three edges in their boundaries, we can add edges without introducing new vertices in order to make every region triangular, including the unbounded outer region. If this [[planar graph|triangulated graph]] is colorable using four colors or fewer, so is the original graph since the same coloring is valid if edges are removed. So it suffices to prove the four color theorem for triangulated graphs to prove it for all planar graphs, and without loss of generality we assume the graph is triangulated.

Suppose ''v'', ''e'', and ''f'' are the number of vertices, edges, and regions (faces). Since each region is triangular and each edge is shared by two regions, we have that 2''e'' = 3''f''. This together with [[Euler characteristic#Planar graphs|Euler's formula]], ''v'' − ''e'' + ''f'' = 2, can be used to show that 6''v'' − 2''e'' = 12. Now, the ''degree'' of a vertex is the number of edges abutting it. If ''v''&lt;sub&gt;''n''&lt;/sub&gt; is the number of vertices of degree ''n'' and ''D'' is the maximum degree of any vertex,
:&lt;math&gt;6v - 2e = 6\sum_{i=1}^D v_i - \sum_{i=1}^D iv_i = \sum_{i=1}^D (6 - i)v_i = 12.&lt;/math&gt;
But since 12 &gt; 0 and 6 − ''i'' ≤ 0 for all ''i'' ≥ 6, this demonstrates that there is at least one vertex of degree 5 or less.

If there is a graph requiring 5 colors, then there is a ''minimal'' such graph, where removing any vertex makes it four-colorable. Call this graph ''G''. Then ''G'' cannot have a vertex of degree 3 or less, because if ''d''(''v'') ≤ 3, we can remove ''v'' from ''G'', four-color the smaller graph, then add back ''v'' and extend the four-coloring to it by choosing a color different from its neighbors.

Kempe also showed correctly that ''G'' can have no vertex of degree 4. As before we remove the vertex ''v'' and four-color the remaining vertices. If all four neighbors of ''v'' are different colors, say red, green, blue, and yellow in clockwise order, we look for an alternating path of vertices colored red and blue joining the red and blue neighbors. Such a path is called a [[Kempe chain]]. There may be a Kempe chain joining the red and blue neighbors, and there may be a Kempe chain joining the green and yellow neighbors, but not both, since these two paths would necessarily intersect, and the vertex where they intersect cannot be colored. Suppose it is the red and blue neighbors that are not chained together. Explore all vertices attached to the red neighbor by red-blue alternating paths, and then reverse the colors red and blue on all these vertices. The result is still a valid four-coloring, and ''v'' can now be added back and colored red.

This leaves only the case where ''G'' has a vertex of degree 5; but Kempe's argument was flawed for this case. Heawood noticed Kempe's mistake and also observed that if one was satisfied with proving only five colors are needed, one could run through the above argument (changing only that the minimal counterexample requires 6 colors) and use Kempe chains in the degree 5 situation to prove the [[five color theorem]].

In any case, to deal with this degree 5 vertex case requires a more complicated notion than removing a vertex. Rather the form of the argument is generalized to considering ''configurations'', which are connected subgraphs of ''G'' with the degree of each vertex (in G) specified. For example, the case described in degree 4 vertex situation is the configuration consisting of a single vertex labelled as having degree 4 in ''G''. As above, it suffices to demonstrate that if the configuration is removed and the remaining graph four-colored, then the coloring can be modified in such a way that when the configuration is re-added, the four-coloring can be extended to it as well. A configuration for which this is possible is called a ''reducible configuration''. If at least one of a set of configurations must occur somewhere in G, that set is called ''unavoidable''. The argument above began by giving an unavoidable set of five configurations (a single vertex with degree 1, a single vertex with degree 2, ..., a single vertex with degree 5) and then proceeded to show that the first 4 are reducible; to exhibit an unavoidable set of configurations where every configuration in the set is reducible would prove the theorem.

Because ''G'' is triangular, the degree of each vertex in a configuration is known, and all edges internal to the configuration are known, the number of vertices in ''G'' adjacent to a given configuration is fixed, and they are joined in a cycle. These vertices form the ''ring'' of the configuration; a configuration with ''k'' vertices in its ring is a ''k''-ring configuration, and the configuration together with its ring is called the ''ringed configuration''. As in the simple cases above, one may enumerate all distinct four-colorings of the ring; any coloring that can be extended without modification to a coloring of the configuration is called ''initially good''. For example, the single-vertex configuration above with 3 or less neighbors were initially good. In general, the surrounding graph must be systematically recolored to turn the ring's coloring into a good one, as was done in the case above where there were 4 neighbors; for a general configuration with a larger ring, this requires more complex techniques. Because of the large number of distinct four-colorings of the ring, this is the primary step requiring computer assistance.

Finally, it remains to identify an unavoidable set of configurations amenable to reduction by this procedure. The primary method used to discover such a set is the [[discharging method (discrete mathematics)|method of discharging]]. The intuitive idea underlying discharging is to consider the planar graph as an electrical network. Initially positive and negative "electrical charge" is distributed amongst the vertices so that the total is positive.

Recall the formula above:

:&lt;math&gt;\sum_{i=1}^D (6 - i)v_i = 12.&lt;/math&gt;

Each vertex is assigned an initial charge of 6-deg(''v''). Then one "flows" the charge by systematically redistributing the charge from a vertex to its neighboring vertices according to a set of rules, the ''discharging procedure''. Since charge is preserved, some vertices still have positive charge. The rules restrict the possibilities for configurations of positively charged vertices, so enumerating all such possible configurations gives an unavoidable set.

As long as some member of the unavoidable set is not reducible, the discharging procedure is modified to eliminate it (while introducing other configurations). Appel and Haken's final discharging procedure was extremely complex and, together with a description of the resulting unavoidable configuration set, filled a 400-page volume, but the configurations it generated could be checked mechanically to be reducible. Verifying the volume describing the unavoidable configuration set itself was done by peer review over a period of several years.

A technical detail not discussed here but required to complete the proof is ''[[immersion (mathematics)|immersion]] reducibility''.

==False disproofs==
The four color theorem has been notorious for attracting a large number of false proofs and disproofs in its long history. At first, ''[[The New York Times]]'' refused as a matter of policy to report on the Appel–Haken proof, fearing that the proof would be shown false like the ones before it {{Harv|Wilson|2014}}. Some alleged proofs, like Kempe's and Tait's mentioned above, stood under public scrutiny for over a decade before they were refuted. But many more, authored by amateurs, were never published at all.

{{double image|right|4CT Non-Counterexample 1.svg|150|4CT Non-Counterexample 2.svg|150|The map (left) has been colored with five colors, but for example four of the ten regions can be changed to obtain a coloring with only four colors (right).}}

Generally, the simplest, though invalid, counterexamples attempt to create one region which touches all other regions. This forces the remaining regions to be colored with only three colors. Because the four color theorem is true, this is always possible; however, because the person drawing the map is focused on the one large region, they fail to notice that the remaining regions can in fact be colored with three colors.

This trick can be generalized: there are many maps where if the colors of some regions are selected beforehand, it becomes impossible to color the remaining regions without exceeding four colors. A casual verifier of the counterexample may not think to change the colors of these regions, so that the counterexample will appear as though it is valid.

Perhaps one effect underlying this common misconception is the fact that the color restriction is not [[transitive relation|transitive]]: a region only has to be colored differently from regions it touches directly, not regions touching regions that it touches. If this were the restriction, planar graphs would require arbitrarily large numbers of colors.

Other false disproofs violate the assumptions of the theorem in unexpected ways, such as using a region that consists of multiple disconnected parts, or disallowing regions of the same color from touching at a point.

==Three-coloring==

While every planar map can be colored with four colors, it is [[NP-complete]] in [[computational complexity theory|complexity]] to decide whether an arbitrary planar map can be colored with just three colors.&lt;ref&gt;{{Citation
 | last1 = Dailey | first1 = D. P.
 | title = Uniqueness of colorability and colorability of planar 4-regular graphs are NP-complete
 | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]
 | volume = 30
 | pages = 289–293
 | year = 1980
 | doi = 10.1016/0012-365X(80)90236-8
 | issue = 3
}}&lt;/ref&gt;

==Generalizations==
[[File:Torus with seven colours.svg|thumb|300px|By joining the single arrows together and the double arrows together, one obtains a [[torus]] with seven mutually touching regions; therefore seven colors are necessary]]
[[File:Projection color torus.png|480px|thumb|This construction shows the torus divided into the maximum of seven regions, each one of which touches every other.]]
The four-color theorem applies not only to finite planar graphs, but also to [[infinite graph]]s that can be drawn without crossings in the plane, and even more generally to infinite graphs (possibly with an uncountable number of vertices) for which every finite subgraph is planar. To prove this, one can combine a proof of the theorem for finite planar graphs with the [[De Bruijn–Erdős theorem (graph theory)|De Bruijn–Erdős theorem]] stating that, if every finite subgraph of an infinite graph is ''k''-colorable, then the whole graph is also ''k''-colorable {{harvtxt|Nash-Williams|1967}}. This can also be seen as an immediate consequence of [[Kurt Gödel]]'s [[compactness theorem]] for [[first-order logic]], simply by expressing the colorability of an infinite graph with a set of logical formulae.

One can also consider the coloring problem on surfaces other than the plane ([[#Reference-Mathworld-Map coloring|Weisstein]]). The problem on the [[sphere]] or [[cylinder]] is equivalent to that on the plane. For closed (orientable or non-orientable) surfaces with positive [[genus (mathematics)|genus]], the maximum number ''p'' of colors needed depends on the surface's [[Euler characteristic]] χ according to the formula
:&lt;math&gt;p=\left\lfloor\frac{7 + \sqrt{49 - 24 \chi}}{2}\right\rfloor,&lt;/math&gt;
where the outermost brackets denote the [[floor function]].

Alternatively, for an [[orientable]] surface the formula can be given in terms of the genus of a surface, ''g'':
::&lt;math&gt;p=\left\lfloor\frac{7 + \sqrt{1 + 48g }}{2}\right\rfloor&lt;/math&gt; ([[#Reference-Mathworld-Map coloring|Weisstein]]).

[[File:Szilassi polyhedron 3D model.svg|thumb|left|150px|Interactive Szilassi polyhedron model with each face a different color. In [http://upload.wikimedia.org/wikipedia/commons/8/8f/Szilassi_polyhedron_3D_model.svg the SVG image], move the mouse to rotate it.&lt;ref&gt;Branko Grünbaum, Lajos Szilassi, [http://www.diale.org/pdf/csaszar.pdf ''Geometric Realizations of Special Toroidal Complexes''], Contributions to Discrete Mathematics, Volume 4, Number 1, Pages 21-39, ISSN 1715-0868&lt;/ref&gt;]]

This formula, the [[Heawood conjecture]], was conjectured by [[P. J. Heawood]] in 1890 and proved by [[Gerhard Ringel]] and [[John William Theodore Youngs|J. W. T. Youngs]] in 1968. The only exception to the formula is the [[Klein bottle]], which has Euler characteristic 0 (hence the formula gives p = 7) and requires only 6 colors, as shown by [[Philip Franklin|P. Franklin]] in 1934 ([[#Reference-Mathworld-Map coloring|Weisstein]]).

For example, the [[torus]] has Euler characteristic χ = 0 (and genus ''g'' = 1) and thus ''p'' = 7, so no more than 7 colors are required to color any map on a torus. This upper bound of 7 is sharp: certain [[toroidal polyhedron|toroidal polyhedra]] such as the [[Szilassi polyhedron]] require seven colors.

[[File:Tietze-Moebius.svg|thumb|[[Heinrich Franz Friedrich Tietze|Tietze's]] subdivision of a [[Möbius strip]] into six mutually adjacent regions, requiring six colors. The vertices and edges of the subdivision form an embedding of [[Tietze's graph]] onto the strip.]]
A [[Möbius strip]] requires six colors {{harv|Tietze|1910}} as do [[1-planar graph]]s (graphs drawn with at most one simple crossing per edge) {{harv|Borodin|1984}}. If both the vertices and the faces of a planar graph are colored, in such a way that no two adjacent vertices, faces, or vertex-face pair have the same color, then again at most six colors are needed {{harv|Borodin|1984}}.

There is no obvious extension of the coloring result to three-dimensional solid regions. By using a set of ''n'' flexible rods, one can arrange that every rod touches every other rod. The set would then require ''n'' colors, or ''n''+1 if you consider the empty space that also touches every rod. The number ''n'' can be taken to be any integer, as large as desired. Such examples were known to Fredrick Guthrie in 1880 {{Harv|Wilson|2014}}. Even for axis-parallel [[cuboid]]s (considered to be adjacent when two cuboids share a two-dimensional boundary area) an unbounded number of colors may be necessary ({{harvnb|Reed|Allwright|2008}}; {{harvtxt|Magnant|Martin|2011}}).
{{clear|left}}

==Relation to other areas of mathematics==

[[Dror Bar-Natan]] gave a statement concerning [[Lie algebras]] and [[Vassiliev invariant]]s which is equivalent to the four color theorem.{{sfnp|Bar-Natan|1997}}

==Use outside of mathematics==
Despite the motivation from [[map coloring|coloring political maps of countries]], the theorem is not of particular interest to [[cartographers]]. According to an article by the math historian [[Kenneth May]], "Maps utilizing only four colors are rare, and those that do usually require only three. Books on cartography and the history of mapmaking do not mention the four-color property" {{Harv|Wilson|2014|loc=2}}. The theorem also does not guarantee the usual cartographic requirement that non-contiguous regions of the same country (such as Great Britain and Northern Ireland) be colored identically.

==See also==
{{Portal|Discrete mathematics|Mathematics}}
* [[Apollonian network]]
* [[Graph coloring]]
* [[Grötzsch's theorem]]: [[triangle-free graph|triangle-free]] planar graphs are 3-colorable.
* [[Hadwiger–Nelson problem]]: how many colors are needed to color the plane so that no two points at unit distance apart have the same color?

==Notes==
&lt;references /&gt;

==References==

{{refbegin|30em}}
* {{Citation |last=Allaire |first=Frank |year=1978 |chapter=Another proof of the four colour theorem. I. |title= Proceedings, 7th Manitoba Conference on Numerical Mathematics and Computing, Congr. Numer. |volume = 20 |pages=3–72 | mr=0535003 | publisher=Utilitas Mathematica Publishing, Inc. | location=Winnipeg, Man. | isbn=0-919628-20-6 | editor1=D. McCarthy | editor2=H. C. Williams}}
* {{Citation |last=Appel |first=Kenneth |last2=Haken |first2=Wolfgang |year=1977 |title=Every Planar Map is Four Colorable. I. Discharging |journal=Illinois Journal of Mathematics| volume=21 |pages=429–490| issue=3 | mr=0543792 | url=http://projecteuclid.org/euclid.ijm/1256049011}}
* {{Citation |last=Appel |first=Kenneth |last2=Haken |first2=Wolfgang |last3=Koch |first3=John |year=1977 |title=Every Planar Map is Four Colorable. II. Reducibility |journal=Illinois Journal of Mathematics| volume=21 |pages=491–567| mr=0543793 | issue=3 | url=http://projecteuclid.org/euclid.ijm/1256049012}}
* {{Citation| doi=10.1038/scientificamerican1077-108| last=Appel |first=Kenneth |last2=Haken| first2=Wolfgang |date=October 1977 |title=Solution of the Four Color Map Problem |periodical=Scientific American|volume=237 |pages=108–121|isbn= |issue=4 |bibcode=1977SciAm.237d.108A}}
* {{Citation |last=Appel |first=Kenneth |last2=Haken |first2=Wolfgang |year=1989 |title=Every Planar Map is Four-Colorable |publisher=American Mathematical Society |place=Providence, RI |isbn = 0-8218-5103-9 | mr=1025335 | series=Contemporary Mathematics | volume=98 | others=With the collaboration of J. Koch. | doi=10.1090/conm/098}}
*{{citation
 | last = Bar-Natan | first = Dror | authorlink = Dror Bar-Natan
 | arxiv = q-alg/9606016
 | doi = 10.1007/BF01196130
 | issue = 1
 | journal = Combinatorica
 | mr = 1466574
 | pages = 43–52
 | title = Lie algebras and the four color theorem
 | volume = 17
 | year = 1997}}
* {{Citation |last=Bernhart|first=Frank R.|year=1977|title= A digest of the four color theorem|periodical= Journal of Graph Theory|volume=1|pages=207–225|doi= 10.1002/jgt.3190010305| issue=3 | mr=0465921}}
* {{citation
 |last = Borodin |first = O. V.
 |issue = 41
 |journal = Metody Diskretnogo Analiza
 |mr = 832128
 |pages = 12–26, 108
 |title = Solution of the Ringel problem on vertex-face coloring of planar graphs and coloring of 1-planar graphs
 |year = 1984}}.
* {{Citation |last=Cayley |first=Arthur |title=On the colourings of maps |periodical=Proc. Royal Geographical Society |volume=1 |year=1879 |pages=259–261 |doi=10.2307/1799998 |jstor=1799998 |issue=4 |publisher=Blackwell Publishing}}
* {{Citation |last=Fritsch |first=Rudolf |last2=Fritsch |first2=Gerda |year=1998 |title=The Four Color Theorem: History, Topological Foundations and Idea of Proof |publisher=Springer |place=New York |isbn = 978-0-387-98497-1 | others=Translated from the 1994 German original by Julie Peschke. | mr=1633950 | doi=10.1007/978-1-4612-1720-6}}
*{{citation|url=https://books.google.com/books?id=Mm1IAAAAYAAJ&amp;pg=PA726|journal=[[Athenaeum (British magazine)|The Athenaeum]]|date=June 10, 1854|page=726|author=F. G.|title=Tinting Maps}}.
* {{Citation |last=Gonthier |first=Georges |title=A computer-checked proof of the four colour theorem |url=http://research.microsoft.com/en-us/um/people/gonthier/4colproof.pdf |publisher=unpublished |year=2005}}
* {{Citation |last=Gonthier |first=Georges |title=Formal Proof—The Four-Color Theorem |journal=[[Notices of the American Mathematical Society]] |volume=55 |year=2008|url=http://www.ams.org/notices/200811/tx081101382p.pdf |issue=11 |pages=1382–1393 | mr=2463991}}
* {{citation |last=Hadwiger |first=Hugo |author1-link=Hugo Hadwiger |title=Über eine Klassifikation der Streckenkomplexe |year=1943 |journal=Vierteljschr. Naturforsch. Ges. Zürich |volume=88 |pages=133–143}}
* {{Citation|last=Heawood |first=P. J. |title=Map-Colour Theorem |periodical= Quarterly Journal of Mathematics, Oxford |volume= 24 |year = 1890 |pages = 332–338 |authorlink=Percy John Heawood}}
*{{citation |title=Four Colors Do Not Suffice
 |first=Hud|last= Hudson
 |journal=The American Mathematical Monthly
 |volume=110
 |number=5
 |date=May 2003
 |pages=417–423
 |jstor=3647828
 |doi=10.2307/3647828 }}
* {{Citation|last=Kempe|first=A. B. |title=On the Geographical Problem of the Four Colours |journal = American Journal of Mathematics,  The Johns Hopkins University Press
|volume= 2 |issue= 3|year = 1879|pages = 193–220 |authorlink=Alfred Kempe}}
* {{citation|first1=C.|last1=Magnant|first2=D. M.|last2=Martin|title=Coloring rectangular blocks in 3-space|journal=Discussiones Mathematicae Graph Theory|volume=31|issue=1|year=2011|pages=161–170|url=http://www.discuss.wmie.uz.zgora.pl/php/discuss.php?ip=&amp;url=plik&amp;nIdA=21787&amp;sTyp=HTML&amp;nIdSesji=-1|doi=10.7151/dmgt.1535}}
*{{citation|first = Brendan D.|last=McKay|authorlink=Brendan McKay|title = A note on the history of the four-colour conjecture |arxiv = 1201.2852 |year = 2012|bibcode=2012arXiv1201.2852M}}
* {{citation
 |last = Nash-Williams |first = C. St. J. A. |author-link = Crispin Nash-Williams
 |journal = Journal of Combinatorial Theory
 |mr = 0214501
 |pages = 286–301
 |title = Infinite graphs—a survey
 |volume = 3
 |year = 1967
 |doi=10.1016/s0021-9800(67)80077-2}}.
* {{Citation |last=O'Connor |last2=Robertson |title=The Four Colour Theorem |url=http://www-groups.dcs.st-and.ac.uk/~history/HistTopics/The_four_colour_theorem.html |publisher=[[MacTutor archive]] |year=1996}}
* {{citation|last=Pegg|first=Ed, Jr.|authorlink=Ed Pegg, Jr. |last2=Melendez |first2=J. |last3=Berenguer |first3=R. |last4=Sendra |first4=J. R. |last5=Hernandez |first5=A. |last6=Del Pino |first6=J. |title=Book Review: The Colossal Book of Mathematics|journal=Notices of the American Mathematical Society|volume=49|issue=9|year=2002|pages=1084–1086|url=http://www.ams.org/notices/200209/rev-pegg.pdf|doi=10.1109/TED.2002.1003756|bibcode=2002ITED...49.1084A}}
* {{citation|last1=Reed|first1=Bruce|author1-link = Bruce Reed (mathematician)|last2=Allwright|first2=David|title=Painting the office|journal=Mathematics-in-Industry Case Studies|volume=1|year=2008|pages=1–8|url=http://www.micsjournal.ca/index.php/mics/article/view/5}}
* {{Citation |last=Ringel |first= G.|last2=Youngs |first2=J.W.T. |title=Solution of the Heawood Map-Coloring Problem|periodical= Proc. Natl. Acad. Sci. USA|year=1968 |pages=438–445 |issue=2|volume = 60 |doi=10.1073/pnas.60.2.438 |pmc=225066 |pmid=16591648|bibcode = 1968PNAS...60..438R }}
* {{Citation |last=Robertson |first=Neil |last2=Sanders |first2=Daniel P. |last3=Seymour |first3=Paul |last4=Thomas |first4=Robin |contribution=Efficiently four-coloring planar graphs |title = Proceedings of the 28th ACM Symposium on Theory of Computing (STOC 1996)|year=1996 |pages=571–575|doi = 10.1145/237814.238005 | mr=1427555}}
* {{Citation |doi=10.1006/jctb.1997.1750 |last=Robertson |first=Neil |last2=Sanders |first2=Daniel P. |last3=Seymour |first3=Paul |last4=Thomas |first4=Robin |title=The Four-Colour Theorem |year=1997 |periodical=J. Combin. Theory Ser. B|volume=70|pages=2–44|issue=1 | mr=1441258}}
* {{Citation |last=Saaty|first=Thomas |author-link=Thomas L. Saaty| last2=Kainen|first2=Paul|author2-link=Paul Chester Kainen| title = The Four Color Problem: Assaults and Conquest| isbn = 0-486-65092-8 |journal=Science |year=1986 |volume=202 |issue=4366 |page=424 |publisher=Dover Publications |location=New York |doi=10.1126/science.202.4366.424 |bibcode=1978Sci...202..424S}}
* {{Citation |last=Swart |first=Edward Reinier |year=1980 |title=The philosophical implications of the four-color problem |periodical=American Mathematical Monthly |volume=87 |pages=697–702 |url=http://www.maa.org/programs/maa-awards/writing-awards/the-philosophical-implications-of-the-four-color-problem |doi=10.2307/2321855 |issue=9 |jstor=2321855 |publisher=Mathematical Association of America | mr=0602826}}
* {{Citation |last=Thomas|first=Robin |title=An Update on the Four-Color Theorem |periodical=[[Notices of the American Mathematical Society]] |url = http://www.ams.org/notices/199807/thomas.pdf |year=1998 |volume=45|pages=848–859|authorlink=Robin Thomas (mathematician) |issue=7 | mr=1633714}}
* {{Citation |last=Thomas|first=Robin|title=The Four Color Theorem|url= http://people.math.gatech.edu/~thomas/FC/fourcolor.html|year=1995}}
* {{citation| first=Heinrich|last=Tietze|authorlink=Heinrich Franz Friedrich Tietze |url=http://gdz.sub.uni-goettingen.de/no_cache/dms/load/img/?IDDOC=244202 |title=Einige Bemerkungen zum Problem des Kartenfärbens auf einseitigen Flächen|trans-title=Some remarks on the problem of map coloring on one-sided surfaces| journal= [[German Mathematical Society|DMV]] Annual Report| volume=19 |year= 1910|pages=155–159}}
* {{citation|chapter=Recent Excluded Minor Theorems for Graphs |first=Robin|last=Thomas |pages=201–222 | title=Surveys in combinatorics, 1999 | mr=1725004 | doi=10.1017/CBO9780511721335 | series=London Mathematical Society Lecture Note Series | volume=267 | publisher=Cambridge University Press | location=Cambridge | isbn=0-521-65376-2  | year=1999 | editor1-last=Lamb | editor1-first=John D. | editor2-last=Preece | editor2-first=D. A.}}
*{{Citation|authorlink=Peter Guthrie Tait|first=P. G.|last= Tait|title=Remarks on the colourings of maps|journal=Proc. R. Soc. Edinburgh|volume=10|year=1880|pages=729}}
* {{Citation |last=Wilson |first=Robin |authorlink=Robin Wilson (mathematician) |title=Four Colors Suffice | publisher=Princeton University Press |place=Princeton, NJ |series=Princeton Science Library |year=2014 | origyear=2002 |isbn =978-0-691-15822-8 | mr=3235839}}
{{refend}}

==External links==
{{Commons category|Four-color theorem}}
* {{springer|title=Four-colour problem|id=p/f040970}}
* {{MathWorld|title=Blanuša snarks|urlname=BlanusaSnarks}}
* {{MathWorld|title=Map coloring|urlname=MapColoring}}
* [http://mathoverflow.net/questions/189097/generalizations-of-the-four-color-theorem List of generalizations of the four color theorem] on [[MathOverflow]]

[[Category:Graph coloring]]
[[Category:Planar graphs]]
[[Category:Theorems in graph theory]]
[[Category:Computer-assisted proofs]]</text>
      <sha1>98a977i8rkp6xxq1ue7k1aqlv2kzdwj</sha1>
    </revision>
  </page>
  <page>
    <title>Functional predicate</title>
    <ns>0</ns>
    <id>174908</id>
    <revision>
      <id>811952598</id>
      <parentid>798019798</parentid>
      <timestamp>2017-11-25T00:46:22Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <minor/>
      <comment>/* Introducing new function symbols */ linking</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7921">{{Unreferenced|date=December 2009}}
In [[formal logic]] and related branches of [[mathematics]], a '''functional predicate''', or '''function symbol''', is a logical symbol that may be applied to an object term to produce another object term.
Functional predicates are also sometimes called ''mappings'', but that term has other meanings as well.
In a [[model (logic)|model]], a function symbol will be modelled by a [[function (mathematics)|function]].

Specifically, the symbol ''F'' in a [[formal language]] is a functional symbol if, [[given any]] symbol ''X'' representing an object in the language, ''F''(''X'') is again a symbol representing an object in that language.
In [[typed logic]], ''F'' is a functional symbol with ''domain'' type '''T''' and ''codomain'' type '''U''' if, given any symbol ''X'' representing an object of type '''T''', ''F''(''X'') is a symbol representing an object of type '''U'''.
One can similarly define function symbols of more than one variable, analogous to functions of more than one variable; a function symbol in [[0 (number)|zero]] variables is simply a [[Logical constant|constant]] symbol.

Now consider a model of the formal language, with the types '''T''' and '''U''' modelled by [[Set (mathematics)|sets]] ['''T'''] and ['''U'''] and each symbol ''X'' of type '''T''' modelled by an element [''X''] in ['''T'''].
Then ''F'' can be modelled by the set
:&lt;math&gt;[F]:=\big\{([X],[F(X)]):[X]\in[\mathbf{T}]\big\},&lt;/math&gt;
which is simply a [[function (mathematics)|function]] with domain ['''T'''] and codomain ['''U'''].
It is a requirement of a consistent model that [''F''(''X'')] = [''F''(''Y'')] whenever [''X''] = [''Y''].

==Introducing new function symbols==
In a treatment of [[predicate logic]] that allows one to introduce new predicate symbols, one will also want to be able to introduce new function symbols. Given the function symbols ''F'' and ''G'', one can introduce a new function symbol ''F'' ∘ ''G'', the ''[[function composition|composition]]'' of ''F'' and ''G'', satisfying (''F'' ∘ ''G'')(''X'') = ''F''(''G''(''X'')), [[for all]] ''X''.
Of course, the right side of this equation doesn't make sense in typed logic unless the domain type of ''F'' matches the codomain type of ''G'', so this is required for the composition to be defined.

One also gets certain function symbols automatically.
In untyped logic, there is an ''identity predicate'' id that satisfies id(''X'') = ''X'' for all ''X''.
In typed logic, given any type '''T''', there is an identity predicate id&lt;sub&gt;'''T'''&lt;/sub&gt; with domain and codomain type '''T'''; it satisfies id&lt;sub&gt;'''T'''&lt;/sub&gt;(''X'') = ''X'' for all ''X'' of type '''T'''.
Similarly, if '''T''' is a [[subtype]] of '''U''', then there is an inclusion predicate of domain type '''T''' and codomain type '''U''' that satisfies the same equation; there are additional function symbols associated with other ways of constructing new types out of old ones.

Additionally, one can define functional predicates after proving an appropriate [[theorem]].
(If you're working in a [[formal system]] that doesn't allow you to introduce new symbols after proving theorems, then you will have to use relation symbols to get around this, as in the next section.)
Specifically, if you can prove that for every ''X'' (or every ''X'' of a certain type), [[there exists]] a [[unique (mathematics)|unique]] ''Y'' satisfying some condition ''P'', then you can introduce a function symbol ''F'' to indicate this.
Note that ''P'' will itself be a relational [[predicate (logic)|predicate]] involving both ''X'' and ''Y''.
So if there is such a predicate ''P'' and a theorem:
: For all ''X'' of type '''T''', for some unique ''Y'' of type '''U''', ''P''(''X'',''Y''),
then you can introduce a function symbol ''F'' of domain type '''T''' and codomain type '''U''' that satisfies:
: For all ''X'' of type '''T''', for all ''Y'' of type '''U''', ''P''(''X'',''Y'') [[if and only if]] ''Y'' = ''F''(''X'').

==Doing without functional predicates==
Many treatments of predicate logic don't allow functional predicates, only relational [[predicate (logic)|predicate]]s.
This is useful, for example, in the context of proving [[metalogic]]al theorems (such as [[Gödel's incompleteness theorem]]s), where one doesn't want to allow the introduction of new functional symbols (nor any other new symbols, for that matter).
But there is a method of replacing functional symbols with relational symbols wherever the former may occur; furthermore, this is algorithmic and thus suitable for applying most metalogical theorems to the result.

Specifically, if ''F'' has domain type '''T''' and [[codomain]] type '''U''', then it can be replaced with a predicate ''P'' of type ('''T''','''U''').
Intuitively, ''P''(''X'',''Y'') means ''F''(''X'') = ''Y''.
Then whenever ''F''(''X'') would appear in a statement, you can replace it with a new symbol ''Y'' of type '''U''' and include another statement ''P''(''X'',''Y'').
To be able to make the same deductions, you need an additional proposition:
: [[For all]] ''X'' of type '''T''', for some [[unique (mathematics)|unique]] ''Y'' of type '''U''', ''P''(''X'',''Y'').
(Of course, this is the same proposition that had to be proved as a theorem before introducing a new function symbol in the previous section.)

Because the elimination of functional predicates is both convenient for some purposes and possible, many treatments of formal logic do not deal explicitly with function symbols but instead use only relation symbols; another way to think of this is that a functional predicate is a ''special kind of'' predicate, specifically one that satisfies the proposition above.
This may seem to be a problem if you wish to specify a proposition [[schema (logic)|schema]] that applies only to functional predicates ''F''; how do you know ahead of time whether it satisfies that condition?
To get an equivalent formulation of the schema, first replace anything of the form ''F''(''X'') with a new variable ''Y''.
Then [[universally quantify]] over each ''Y'' immediately after the corresponding ''X'' is introduced (that is, after ''X'' is quantified over, or at the beginning of the statement if ''X'' is free), and guard the quantification with ''P''(''X'',''Y'').
Finally, make the entire statement a [[material conditional|material consequence]] of the uniqueness condition for a functional predicate above.

Let us take as an example the [[axiom schema of replacement]] in [[Zermelo–Fraenkel set theory]].
(This example uses [[mathematical symbols]].)
This schema states (in one form), for any functional predicate ''F'' in one variable:
:&lt;math&gt;\forall A, \exists B, \forall C, C \in A \rightarrow F(C)\in B.&lt;/math&gt; 
First, we must replace ''F''(''C'') with some other variable ''D'': 
:&lt;math&gt;\forall A, \exists B, \forall C, C \in A\rightarrow D \in B.&lt;/math&gt; 
Of course, this statement isn't correct; ''D'' must be quantified over just after ''C'': 
:&lt;math&gt;\forall A, \exists B, \forall C, \forall D, C \in A \rightarrow D\in B.&lt;/math&gt; 
We still must introduce ''P'' to guard this quantification: 
:&lt;math&gt;\forall A, \exists B, \forall C, \forall D, P(C,D) \rightarrow (C \in A \rightarrow D \in B).&lt;/math&gt; 
This is almost correct, but it applies to too many predicates; what we actually want is: 
:&lt;math&gt;(\forall X, \exists ! Y, P(X,Y))\rightarrow (\forall A, \exists B, \forall C, \forall D, P(C,D)\rightarrow (C \in A \rightarrow D \in B)).&lt;/math&gt; 
This version of the axiom schema of replacement is now suitable for use in a formal language that doesn't allow the introduction of new function symbols. Alternatively, one may interpret the original statement as a statement in such a formal language; it was merely an abbreviation for the statement produced at the end.

==See also==
*[[Logical connective]]
*[[Logical constant]]

{{DEFAULTSORT:Functional Predicate}}
[[Category:Model theory]]</text>
      <sha1>mhu3yi21c39jfdl99s97r0abo4kja9l</sha1>
    </revision>
  </page>
  <page>
    <title>Georg Cantor's first set theory article</title>
    <ns>0</ns>
    <id>22697171</id>
    <revision>
      <id>870801742</id>
      <parentid>870798941</parentid>
      <timestamp>2018-11-27T02:37:54Z</timestamp>
      <contributor>
        <username>RJGray</username>
        <id>8268674</id>
      </contributor>
      <minor/>
      <comment>/* Everywhere dense */ ones -&gt; one</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="89545">{{good article}}
[[File:Georg Cantor3.jpg|thumb|Georg Cantor, c. 1870.]]

[[Georg Cantor]] published his first set theory article in 1874, and it contains the first theorems of transfinite [[set theory]], which studies [[infinite set]]s and their properties.&lt;ref&gt;{{harvnb|Ferreirós|2007|p=171}}.&lt;/ref&gt; One of these theorems is "Cantor's revolutionary discovery" that the [[set (mathematics)|set]] of all [[real number]]s is [[uncountable set|uncountably]], rather than [[countable set|countably]], infinite.&lt;ref&gt;{{harvnb|Dauben|1993|p=4}}.&lt;/ref&gt; This theorem is proved using '''Cantor's first uncountability proof''', which differs from the more familiar proof using his [[Cantor's diagonal argument|diagonal argument]]. The title of the article, "'''On a Property of the Collection of All Real Algebraic Numbers'''" ("''Ueber eine Eigenschaft des Inbegriffes aller reellen algebraischen Zahlen''"), refers to its first theorem: the set of real [[algebraic numbers]] is countable. In 1879, Cantor modified his uncountability proof by using the [[topological]] notion of a set being [[dense set|dense]] in an interval.

Cantor's 1874 article also contains a proof of the existence of [[transcendental number]]s. As early as 1930, mathematicians have disagreed on whether this proof is [[constructive proof|constructive or non-constructive]].&lt;ref&gt;"[Cantor's method is] a method that incidentally, contrary to a widespread interpretation, is fundamentally constructive and not merely existential." ({{harvnb|Fraenkel|1930|p=237}}; English translation: {{harvnb|Gray|1994|p=823}}.)&lt;/ref&gt; Books as recent as 2014 and 2015 indicate that this disagreement has not been resolved.&lt;ref&gt;"Cantor's proof of the existence of transcendental numbers is not just an existence proof. It can, at least in principle, be used to construct an explicit transcendental number." ({{harvnb|Sheppard|2014|p=131}}.) "Meanwhile Georg Cantor, in 1874, had produced a revolutionary proof of the existence of transcendental numbers, without actually constructing any." ({{harvnb|Stewart|2015|p=285}}.)&lt;/ref&gt; Since Cantor's proof either constructs transcendental numbers or does not, an analysis of his article can determine whether his proof is constructive or non-constructive.&lt;ref&gt;{{harvnb|Gray|1994|pp=819&amp;ndash;821}}.&lt;/ref&gt; Cantor's correspondence with [[Richard Dedekind]] shows the development of his ideas and reveals that he had a choice between two proofs, one that uses the uncountability of the real numbers and one that does not.

Historians of mathematics have examined Cantor's article and the circumstances in which it was written. For example, they have discovered that Cantor was advised to leave out his uncountability theorem in the article he submitted; he added it during [[Galley proof|proofreading]]. They have traced this and other facts about the article to the influence of [[Karl Weierstrass]] and [[Leopold Kronecker]]. Historians have also studied Dedekind's contributions to the article, including his contributions to the theorem on the countability of the real algebraic numbers. In addition, they have looked at the article's legacy, which includes the impact that the uncountability theorem and the concept of countability have had on mathematics.

==The article==
Cantor's article is short, less than four and a half pages.&lt;ref&gt;In letter to Dedekind dated December 25, 1873, Cantor states that he has written and submitted "a short paper" titled ''On a Property of the Collection of All Real Algebraic Numbers''. {{harvnb|Noether|Cavaillès|1937|p=17}}; English translation: {{harvnb|Ewald|1996|p=847}}.&lt;/ref&gt; It begins with a discussion of the real algebraic numbers and a statement of his first theorem: The set of real algebraic numbers can be put into [[one-to-one correspondence]] with the set of positive integers.&lt;ref name=Cantor1874&gt;{{harvnb|Cantor|1874}}. English translation: {{harvnb|Ewald|1996|pp=840&amp;ndash;843}}.&lt;/ref&gt; Cantor restates this theorem in terms more familiar to mathematicians of his time: The set of real algebraic numbers can be written as an infinite [[sequence]] in which each number appears only once.&lt;ref&gt;{{harvnb|Gray|1994|p=828}}.&lt;/ref&gt;

Cantor's second theorem works with a [[closed interval]] [''a'',&amp;nbsp;''b''], which is the set of real numbers ≥&amp;nbsp;''a'' and ≤&amp;nbsp;''b''. The theorem states: Given any sequence of real numbers ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ''x''&lt;sub&gt;3&lt;/sub&gt;, … and any interval [''a'',&amp;nbsp;''b''], there is a number in [''a'',&amp;nbsp;''b''] that is not contained in the given sequence. Hence, there are infinitely many such numbers.&lt;ref name=Ewald840_841&gt;{{harvnb|Cantor|1874|p=259}}. English translation: {{harvnb|Ewald|1996|pp=840&amp;ndash;841}}.&lt;/ref&gt;

The first part of this theorem implies the "Hence" part. For example, let [0,&amp;nbsp;1] be the interval, and consider its [[pairwise disjoint]] subintervals {{nowrap|[0,&amp;nbsp;{{sfrac|1|2}}],}} {{nowrap|[{{sfrac|3|4}},&amp;nbsp;{{sfrac|7|8}}],}} {{nowrap|[{{sfrac|15|16}},&amp;nbsp;{{sfrac|31|32}}],}} …. Applying the first part of the theorem to each subinterval produces infinitely many numbers in [0,&amp;nbsp;1] that are not contained in the given sequence.

Cantor observes that combining his two theorems yields a new proof of [[Liouville number|Liouville's theorem]] that every interval [''a'',&amp;nbsp;''b''] contains infinitely many [[transcendental number]]s.&lt;ref name=Ewald840_841 /&gt;

Cantor then remarks that his second theorem is:
{{quote|the reason why collections of real numbers forming a so-called continuum (such as, all real numbers which are ≥&amp;nbsp;0 and ≤&amp;nbsp;1) cannot correspond one-to-one with the collection (ν) [the collection of all positive integers]; thus I have found the clear difference between a so-called continuum and a collection like the totality of real algebraic numbers.''&lt;ref&gt;{{harvnb|Cantor|1874|p=259}}. English translation: {{harvnb|Gray|1994|p=820}}.&lt;/ref&gt;}}

This remark contains Cantor's uncountability theorem, which only states that an interval [''a'',&amp;nbsp;''b''] cannot be put into one-to-one correspondence with the set of positive integers. It does not state that this interval is an infinite set of larger [[cardinality]] than the set of positive integers. Cardinality is defined in Cantor's next article, which was published in 1878.&lt;ref&gt;{{harvnb|Cantor|1878|p=242}}.&lt;/ref&gt;

{{Anchor|Proof of Cantor's uncountability theorem}}
{| class="wikitable collapsible collapsed"
! style="background: f5f5f5;" |'''Proof of Cantor's uncountability theorem'''
|- style="text-align: left; vertical-align: top; background: white" 
| style="padding-left: 1em; padding-right: 1em" | Cantor does not explicitly prove his uncountability theorem, which follows easily from his second theorem. To prove it, we use [[proof by contradiction]]. Assume that the interval [''a'',&amp;nbsp;''b''] can be put into one-to-one correspondence with the set of positive integers, or equivalently: The real numbers in [''a'',&amp;nbsp;''b''] can be written as a sequence in which each real number appears only once. Applying Cantor's second theorem to this sequence and [''a'',&amp;nbsp;''b''] produces a real number in [''a'',&amp;nbsp;''b''] that does not belong to the sequence. This contradicts the original assumption, and proves the uncountability theorem.&lt;ref&gt;Our proof is nearly the same as the proof of Corollary 2 in {{harvnb|Gray|1994|p=820}}. The only difference is that we specify the contradiction.&lt;/ref&gt;
|}

Cantor only states his uncountability theorem. He does not use it in any proofs.&lt;ref name=Cantor1874 /&gt;

==The proofs==

===First theorem===
[[File:Algebraicszoom.png|thumb|Algebraic numbers on the [[complex plane]] colored by polynomial degree. (red&amp;nbsp;=&amp;nbsp;1, green&amp;nbsp;=&amp;nbsp;2, blue&amp;nbsp;=&amp;nbsp;3, yellow&amp;nbsp;=&amp;nbsp;4). Points become smaller as the integer polynomial coefficients become larger.]]

To prove that the set of real algebraic numbers is countable, define the ''height'' of a [[polynomial]] of [[degree of a polynomial|degree]] ''n'' with integer [[coefficient]]s as: ''n''&amp;nbsp;−&amp;nbsp;1&amp;nbsp;+&amp;nbsp;|''a''&lt;sub&gt;0&lt;/sub&gt;|&amp;nbsp;+&amp;nbsp;|''a''&lt;sub&gt;1&lt;/sub&gt;|&amp;nbsp;+&amp;nbsp;…&amp;nbsp;+&amp;nbsp;|''a''&lt;sub&gt;''n''&lt;/sub&gt;|, where ''a''&lt;sub&gt;0&lt;/sub&gt;, ''a''&lt;sub&gt;1&lt;/sub&gt;, …, ''a''&lt;sub&gt;''n''&lt;/sub&gt; are the coefficients of the polynomial. Order the polynomials by their height, and order the real [[Root of a polynomial|root]]s of polynomials of the same height by numeric order. Since there are only a finite number of roots of polynomials of a given height, these orderings put the real algebraic numbers into a sequence. Cantor went a step further and produced a sequence in which each real algebraic number appears just once. He did this by only using polynomials that are [[irreducible polynomial|irreducible]] over the integers.&lt;ref&gt;{{harvnb|Cantor|1874|pp=259&amp;ndash;260}}. English translation: {{harvnb|Ewald|1996|p=841}}.&lt;/ref&gt; The table below contains the beginning of Cantor's enumeration.

{| class="wikitable collapsible collapsed" style="border:1px solid black; margin:auto;"
! colspan="3"| '''Cantor's enumeration of the real algebraic numbers'''
|- style="text-align: center;"
| Real algebraic&lt;br /&gt;number || &lt;br /&gt;Polynomial || Height of&lt;br /&gt;polynomial
|-
| style="padding-left: 4em;" | ''x''&lt;sub&gt;1&lt;/sub&gt; = 0 || align="right" | ''x''{{spaces|6}} || align="center" | 1
|-
| style="padding-left: 4em;" | ''x''&lt;sub&gt;2&lt;/sub&gt; = −1 || align="right" | ''x'' + 1{{spaces|6}} || align="center" | 2
|-
| style="padding-left: 4em;" | ''x''&lt;sub&gt;3&lt;/sub&gt; = 1 || align="right" | ''x'' − 1{{spaces|6}} || align="center" | 2
|-
| style="padding-left: 4em;" | ''x''&lt;sub&gt;4&lt;/sub&gt; = −2 || align="right" | ''x'' + 2{{spaces|6}} || align="center" | 3
|-
| style="padding-left: 4em;" | ''x''&lt;sub&gt;5&lt;/sub&gt; = −{{sfrac|1|2}} || align="right" | 2''x'' + 1{{spaces|6}} || align="center" | 3
|-
| style="padding-left: 4em;" | ''x''&lt;sub&gt;6&lt;/sub&gt; = {{sfrac|1|2}} || align="right" | 2''x'' − 1{{spaces|6}} || align="center" | 3
|-
| style="padding-left: 4em;" | ''x''&lt;sub&gt;7&lt;/sub&gt; = 2 || align="right" | ''x'' − 2{{spaces|6}} || align="center" | 3
|-
| style="padding-left: 4em;" | ''x''&lt;sub&gt;8&lt;/sub&gt; = −3 || align="right" | ''x'' + 3{{spaces|6}} || align="center" | 4
|-
| style="padding-left: 4em;" | ''x''&lt;sub&gt;9&lt;/sub&gt; = {{sfrac|−1 − {{radic|5}}|2}} || align="right" | ''x''&lt;sup&gt;2&lt;/sup&gt; + ''x'' − 1{{spaces|6}} || align="center" | 4
|-
| style="padding-left: 3.54em;" | ''x''&lt;sub&gt;10&lt;/sub&gt; = −{{radic|2}} || align="right" | ''x''&lt;sup&gt;2&lt;/sup&gt; − 2{{spaces|6}} || align="center" | 4
|-
| style="padding-left: 3.54em;" | ''x''&lt;sub&gt;11&lt;/sub&gt; = −{{sfrac|1|{{radic|2}}}} || align="right" | 2''x''&lt;sup&gt;2&lt;/sup&gt; − 1{{spaces|6}} || align="center" | 4
|-
| style="padding-left: 3.54em;" | ''x''&lt;sub&gt;12&lt;/sub&gt; = {{sfrac|1 − {{radic|5}}|2}} || align="right" | ''x''&lt;sup&gt;2&lt;/sup&gt; − ''x'' − 1{{spaces|6}} || align="center" | 4
|-
| style="padding-left: 3.54em;" | ''x''&lt;sub&gt;13&lt;/sub&gt; = −{{sfrac|1|3}} || align="right" | 3''x'' + 1{{spaces|6}} || align="center" | 4
|-
| style="padding-left: 3.54em;" | ''x''&lt;sub&gt;14&lt;/sub&gt; = {{sfrac|1|3}} || align="right" | 3''x'' − 1{{spaces|6}} || align="center" | 4
|-
| style="padding-left: 3.54em; white-space: nowrap;"| ''x''&lt;sub&gt;15&lt;/sub&gt; = {{sfrac|−1 + {{radic|5}}|2}}{{spaces|3}} || align="right" style="padding-left: 1.25em; white-space: nowrap;"| ''x''&lt;sup&gt;2&lt;/sup&gt; + ''x'' − 1{{spaces|6}} || align="center" | 4
|-
| style="padding-left: 3.54em;" | ''x''&lt;sub&gt;16&lt;/sub&gt; = {{sfrac|1|{{radic|2}}}} || align="right" | 2''x''&lt;sup&gt;2&lt;/sup&gt; − 1{{spaces|6}} || align="center" | 4
|-
| style="padding-left: 3.54em;" | ''x''&lt;sub&gt;17&lt;/sub&gt; = {{radic|2}} || align="right" | ''x''&lt;sup&gt;2&lt;/sup&gt; − 2{{spaces|6}} || align="center" | 4
|-
| style="padding-left: 3.54em;" | ''x''&lt;sub&gt;18&lt;/sub&gt; = {{sfrac|1 + {{radic|5}}|2}} || align="right" | ''x''&lt;sup&gt;2&lt;/sup&gt; − ''x'' − 1{{spaces|6}} || align="center" | 4
|-
| style="padding-left: 3.58em;" | ''x''&lt;sub&gt;19&lt;/sub&gt; = 3 || align="right" | ''x'' − 3{{spaces|6}} || align="center" | 4
|}

===Second theorem===
Only the first part of Cantor's second theorem needs to be proved. It states: Given any sequence of real numbers ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ''x''&lt;sub&gt;3&lt;/sub&gt;,&amp;nbsp;… and any interval [''a'',&amp;nbsp;''b''], there is a number in [''a'',&amp;nbsp;''b''] that is not contained in the given sequence. We simplify Cantor's proof by using [[open interval]]s. The open interval (''a'',&amp;nbsp;''b'') is the set of real numbers greater than&amp;nbsp;''a'' and less than&amp;nbsp;''b''.

To find a number in [''a'',&amp;nbsp;''b''] that is not contained in the given sequence, construct two sequences of real numbers as follows: Find the first two numbers of the given sequence that are in (''a'',&amp;nbsp;''b''). Denote the smaller of these two numbers by ''a''&lt;sub&gt;1&lt;/sub&gt; and the larger by ''b''&lt;sub&gt;1&lt;/sub&gt;.  Similarly, find the first two numbers of the given sequence that are in (''a''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;1&lt;/sub&gt;). Denote the smaller by ''a''&lt;sub&gt;2&lt;/sub&gt; and the larger by ''b''&lt;sub&gt;2&lt;/sub&gt;. Continuing this procedure generates a sequence of intervals (''a''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;1&lt;/sub&gt;), (''a''&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;2&lt;/sub&gt;), (''a''&lt;sub&gt;3&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;3&lt;/sub&gt;),&amp;nbsp;… such that each interval in the sequence contains all succeeding intervals—that is, it generates a sequence of [[nested intervals]]. This implies that the sequence ''a''&lt;sub&gt;1&lt;/sub&gt;, ''a''&lt;sub&gt;2&lt;/sub&gt;, ''a''&lt;sub&gt;3&lt;/sub&gt;,&amp;nbsp;… is increasing and the sequence ''b''&lt;sub&gt;1&lt;/sub&gt;, ''b''&lt;sub&gt;2&lt;/sub&gt;, ''b''&lt;sub&gt;3&lt;/sub&gt;,&amp;nbsp;… is decreasing.&lt;ref&gt;{{harvnb|Cantor|1874|pp=260&amp;ndash;261}}. English translation: {{harvnb|Ewald|1996|pp=841&amp;ndash;842}}.&lt;/ref&gt;

Either the number of intervals generated is finite or infinite. If finite, let (''a''&lt;sub&gt;''N''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''N''&lt;/sub&gt;) be the last interval. If infinite, take the [[Limit of a sequence|limit]]s ''a''&lt;sub&gt;∞&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;lim&lt;sub&gt;''n''&amp;nbsp;→&amp;nbsp;∞&lt;/sub&gt;&amp;nbsp;''a''&lt;sub&gt;''n''&lt;/sub&gt; and ''b''&lt;sub&gt;∞&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;lim&lt;sub&gt;''n''&amp;nbsp;→&amp;nbsp;∞&lt;/sub&gt;&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;. Since ''a''&lt;sub&gt;''n''&lt;/sub&gt;&amp;nbsp;&lt;&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt; for all ''n'', either ''a''&lt;sub&gt;∞&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''b''&lt;sub&gt;∞&lt;/sub&gt; or ''a''&lt;sub&gt;∞&lt;/sub&gt;&amp;nbsp;&lt;&amp;nbsp;''b''&lt;sub&gt;∞&lt;/sub&gt;. Thus, there are three cases to consider:{{Anchor|Cases}}
[[File:Cantor's first uncountability proof Case 1 svg.svg|thumb|350px|Case 1: Last interval (''a''&lt;sub&gt;''N''&lt;/sub&gt;, ''b''&lt;sub&gt;''N''&lt;/sub&gt;)]]
[[File:Cantor's first uncountability proof Case 2 svg.svg|thumb|350px|Case 2: ''a''&lt;sub&gt;∞&lt;/sub&gt; = ''b''&lt;sub&gt;∞&lt;/sub&gt;]]
[[File:Cantor's first uncountability proof Case 3 svg.svg|thumb|350px|Case 3: ''a''&lt;sub&gt;∞&lt;/sub&gt; &lt; ''b''&lt;sub&gt;∞&lt;/sub&gt;]]

{{plainlist|indent=1}}
* Case 1: There is a last interval (''a''&lt;sub&gt;''N''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''N''&lt;/sub&gt;). Since at most one ''x''&lt;sub&gt;''n''&lt;/sub&gt; can be in this interval, every ''y'' in this interval except ''x''&lt;sub&gt;''n''&lt;/sub&gt; (if it exists) is not contained in the given sequence.
* Case 2: ''a''&lt;sub&gt;∞&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''b''&lt;sub&gt;∞&lt;/sub&gt;. Then ''a''&lt;sub&gt;∞&lt;/sub&gt; is not contained in the given sequence since for all ''n''{{space|hair}}: ''a''&lt;sub&gt;∞&lt;/sub&gt; belongs to the interval (''a''&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;), but as Cantor observes, ''x''&lt;sub&gt;''n''&lt;/sub&gt; does not.
{{Anchor|x_n not in}}
{| class="wikitable collapsible collapsed"
! style="background: f5f5f5;" |'''Proof that for all&amp;nbsp;''n''{{space|hair}}:{{space|hair}} ''x''&lt;sub&gt;''n''&lt;/sub&gt; &amp;notin; (''a''&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;)
|- style="text-align: left; vertical-align: top; background: white" 
| style="padding-left: 1em; padding-right: 1em" | This is implied by the stronger result: For all&amp;nbsp;''n'', (''a''&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;) excludes ''x''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;…&amp;nbsp;,&amp;nbsp;''x''&lt;sub&gt;2''n''&lt;/sub&gt;, which is proved by [[mathematical induction|induction]]. Basis step: If ''a''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''x''&lt;sub&gt;''j''&lt;/sub&gt;, ''b''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''x''&lt;sub&gt;''k''&lt;/sub&gt;, and ''E''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;max(''j'',&amp;nbsp;''k''), then (''a''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;1&lt;/sub&gt;) excludes ''x''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;…&amp;nbsp;,&amp;nbsp;''x''&lt;sub&gt;''E''&lt;sub&gt;1&lt;/sub&gt;&lt;/sub&gt;. Also, ''E''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;≥&amp;nbsp;2 since the interval (''a''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;1&lt;/sub&gt;) excludes its two endpoints. Inductive step: Assume that (''a''&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;) excludes ''x''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;…&amp;nbsp;,&amp;nbsp;''x''&lt;sub&gt;''E''&lt;sub&gt;''n''&lt;/sub&gt;&lt;/sub&gt; and ''E''&lt;sub&gt;''n''&lt;/sub&gt;&amp;nbsp;≥&amp;nbsp;2''n''. If ''a''&lt;sub&gt;''n''+1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''x''&lt;sub&gt;''j''&lt;/sub&gt;, ''b''&lt;sub&gt;''n''+1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''x''&lt;sub&gt;''k''&lt;/sub&gt;, and ''E''&lt;sub&gt;''n''+1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;max(''j'',&amp;nbsp;''k''), then (''a''&lt;sub&gt;''n''+1&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''+1&lt;/sub&gt;) excludes ''x''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;…&amp;nbsp;,&amp;nbsp;''x''&lt;sub&gt;''E''&lt;sub&gt;''n''+1&lt;/sub&gt;&lt;/sub&gt;. Also, ''E''&lt;sub&gt;''n''+1&lt;/sub&gt;&amp;nbsp;≥&amp;nbsp;''E''&lt;sub&gt;''n''&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;2&amp;nbsp;≥ 2''n''&amp;nbsp;+&amp;nbsp;2&amp;nbsp;= 2(''n''&amp;nbsp;+&amp;nbsp;1) since the interval (''a''&lt;sub&gt;''n''+1&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''+1&lt;/sub&gt;) excludes the numbers that (''a''&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;) excludes plus the two endpoints ''a''&lt;sub&gt;''n''+1&lt;/sub&gt; and ''b''&lt;sub&gt;''n''+1&lt;/sub&gt;. Therefore, for all ''n''{{space|hair}}: (''a''&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;) excludes ''x''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;…&amp;nbsp;,&amp;nbsp;''x''&lt;sub&gt;2''n''&lt;/sub&gt;. (This proof is similar to a proof that Cantor published in 1879. The main difference is that Cantor's proof is embedded in a larger proof and uses notation from it. Our proof does not depend on this notation.&lt;ref group=proof name=p /&gt;)
|}
* Case 3: ''a''&lt;sub&gt;∞&lt;/sub&gt;&amp;nbsp;&lt;&amp;nbsp;''b''&lt;sub&gt;∞&lt;/sub&gt;. Then every ''y'' in [''a''&lt;sub&gt;∞&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;∞&lt;/sub&gt;] is not contained in the given sequence since for all ''n''{{space|hair}}: ''y'' belongs to (''a''&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;) but ''x''&lt;sub&gt;''n''&lt;/sub&gt; does not.&lt;ref name=Ewald842&gt;{{harvnb|Cantor|1874|p=261}}. English translation: {{harvnb|Ewald|1996|p=842}}.&lt;/ref&gt;
{{endplainlist}}

The proof is complete since, in all cases, at least one real number in [''a'',&amp;nbsp;''b''] has been found that is not contained in the given sequence.{{efn-ua|The difference between our proof and Cantor's is that he generates the sequence of closed intervals [''a''&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;]. To find ''a''&lt;sub&gt;''n''&amp;nbsp;+&amp;nbsp;1&lt;/sub&gt; and ''b''&lt;sub&gt;''n''&amp;nbsp;+&amp;nbsp;1&lt;/sub&gt;, he must also use the open intervals (''a''&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;). By generating a sequence of open intervals, we avoid working with the closed intervals.}}

Cantor's proofs are constructive and have been used to write a [[computer program]] that generates the digits of a transcendental number. This program applies Cantor's construction to a sequence containing all the real algebraic numbers between 0 and 1. The article that discusses this program gives some of its output, which shows how the construction generates a transcendental.&lt;ref&gt;{{harvnb|Gray|1994|p=822}}.&lt;/ref&gt;

===Example of Cantor's construction===
An example illustrates how Cantor's construction works. Consider the sequence: {{sfrac|1|2}}, {{sfrac|1|3}}, {{sfrac|2|3}}, {{sfrac|1|4}}, {{sfrac|3|4}}, {{sfrac|1|5}}, {{sfrac|2|5}}, {{sfrac|3|5}}, {{sfrac|4|5}}, … This sequence is obtained by ordering the [[rational number]]s in (0,&amp;nbsp;1) by increasing denominators, ordering those with the same denominator by increasing numerators, and omitting [[reducible fraction]]s.{{efn-ua|This example is nearly the same as an exercise in {{harvnb|Gray|1994|p=823}}. The only difference is that this sequence contains only [[irreducible fraction]]s, while Gray's sequence includes the reducible fractions of the initial interval.}} The table below shows the first five steps of the construction. The table's first column contains the intervals (''a''&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;). The second column lists the terms visited during the search for the first two terms in (''a''&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;). These two terms are in red.

{| class="wikitable" style="border:1px solid black; margin:auto;"
|+'''Generating a number using Cantor's construction'''
! '''Interval !! Finding the next interval !! Interval (decimal)'''
|-
|&lt;math&gt;\left(\;\frac{1}{3}, \;\;\!\frac{1}{2}\;\right)&lt;/math&gt; 
|| &lt;math&gt;\;\frac{2}{3}, \;\! \frac{1}{4}, \;\! \frac{3}{4}, \;\! \frac{1}{5}, \;\! {\color{red}\frac{2}{5},}\;\! \;\! \frac{3}{5}, \;\! \frac{4}{5}, \;\! \frac{1}{6}, \;\! \frac{5}{6}, \;\! \frac{1}{7}, \;\! \frac{2}{7}, \;\! {\color{red}\frac{3}{7}} &lt;/math&gt;
|| &lt;math&gt;\left(0.3333\dots,\; 0.5000\dots\right)&lt;/math&gt; 
|-
|&lt;math&gt;\left(\;\frac{2}{5}, \;\;\!\frac{3}{7}\;\right)&lt;/math&gt;  
|| &lt;math&gt;\;\frac{4}{7}, \;\dots,\; \frac{1}{12}, {\color{red}\frac{5}{12},}\;\! \frac{7}{12}, \;\dots,\; \frac{6}{17}, {\color{red}\frac{7}{17}}&lt;/math&gt;
|| &lt;math&gt;\left(0.4000\dots,\; 0.4285\dots\right)&lt;/math&gt;
|-
|&lt;math&gt;\left(\frac{7}{17}, \frac{5}{12}\right)&lt;/math&gt;  
|| &lt;math&gt;\frac{8}{17}, \;\!\dots,\;\! \frac{11}{29}, {\color{red}\frac{12}{29},}\;\! \frac{13}{29}, \;\dots,\; \frac{16}{41}, {\color{red}\frac{17}{41}}&lt;/math&gt;
|| &lt;math&gt;\left(0.4117\dots,\; 0.4166\dots\right)&lt;/math&gt;
|-
|&lt;math&gt;\left(\frac{12}{29}, \frac{17}{41}\right)&lt;/math&gt; 
|| &lt;math&gt;\frac{18}{41}, \;\!\dots,\;\! \frac{27}{70}, {\color{red}\frac{29}{70},}\;\! \frac{31}{70}, \;\dots,\; \frac{40}{99}, {\color{red}\frac{41}{99}}&lt;/math&gt;
|| &lt;math&gt;\left(0.4137\dots,\; 0.4146\dots\right)&lt;/math&gt; 
|-
|&lt;math&gt;\left(\frac{41}{99}, \frac{29}{70}\right)&lt;/math&gt;  
|| &lt;math&gt;\frac{43}{99}, \dots, \frac{69}{169}, {\color{red}\frac{70}{169},}\;\! \frac{71}{169}, \,\dots, \frac{98}{239}, {\color{red}\frac{99}{239}}&lt;/math&gt;
|| &lt;math&gt;\left(0.4141\dots,\; 0.4142\dots\right)&lt;/math&gt;
|}

Since the sequence contains all the rational numbers in (0,&amp;nbsp;1), the construction generates an [[irrational number]], which turns out to be {{sqrt|2}}&amp;nbsp;&amp;minus;&amp;nbsp;1.

{{Anchor|Number generated}}
{| class="wikitable collapsible collapsed"
! style="background: f5f5f5;" |'''Proof that the number generated is {{sqrt|2}} &amp;minus; 1'''
|- style="text-align: left; vertical-align: top; background: white" 
| style="padding-left: 1em; padding-right: 1em" |The proof uses [[Farey sequence]]s and [[continued fractions]]. The Farey sequence &lt;math&gt;F_n&lt;/math&gt; is the increasing sequence of [[completely reduced fraction]]s whose denominators are &lt;math&gt;\leq n.&lt;/math&gt; If &lt;math&gt;\frac{a}{b}&lt;/math&gt; and &lt;math&gt;\frac{c}{d}&lt;/math&gt; are adjacent in a Farey sequence, the lowest denominator fraction between them is their [[mediant (mathematics)|mediant]] &lt;math&gt;\frac{a+c}{b+d}.&lt;/math&gt; This mediant is adjacent to both &lt;math&gt;\frac{a}{b}&lt;/math&gt; and &lt;math&gt;\frac{c}{d}&lt;/math&gt; in the Farey sequence &lt;math&gt;F_{b+d}.&lt;/math&gt;&lt;ref&gt;{{harvnb|LeVeque|1956|pp=154&amp;ndash;155}}.&lt;/ref&gt;

Cantor's construction produces mediants because the rational numbers were sequenced by increasing denominator. The first interval in the table is &lt;math&gt;(\frac{1}{3}, \frac{1}{2}).&lt;/math&gt; Since &lt;math&gt;\frac{1}{3}&lt;/math&gt; and &lt;math&gt;\frac{1}{2}&lt;/math&gt; are adjacent in &lt;math&gt;F_3,&lt;/math&gt; their mediant &lt;math&gt;\frac{2}{5}&lt;/math&gt; is the first fraction in the sequence between &lt;math&gt;\frac{1}{3}&lt;/math&gt; and &lt;math&gt;\frac{1}{2}.&lt;/math&gt; Hence, &lt;math&gt;\frac{1}{3} &lt; \frac{2}{5} &lt; \frac{1}{2}.&lt;/math&gt; In this inequality, &lt;math&gt;\frac{1}{2}&lt;/math&gt; has the smallest denominator, so the second fraction is the mediant of &lt;math&gt;\frac{2}{5}&lt;/math&gt; and &lt;math&gt;\frac{1}{2},&lt;/math&gt; which equals &lt;math&gt;\frac{3}{7}.&lt;/math&gt; This implies: &lt;math&gt;\frac{1}{3} &lt; \frac{2}{5} &lt; \frac{3}{7} &lt; \frac{1}{2}.&lt;/math&gt; Therefore, the next interval is &lt;math&gt;(\frac{2}{5}, \frac{3}{7}).&lt;/math&gt;

We will prove that the endpoints of the intervals converge to the continued fraction &lt;math&gt;[0; 2, 2, \dots].&lt;/math&gt; This continued fraction is the limit of its convergents:
:&lt;math&gt;\frac{p_n}{q_n} = [0; 2, \dots, 2]\;\;(n\;2\text{'s}).&lt;/math&gt;

The &lt;math&gt;p_n&lt;/math&gt; and &lt;math&gt;q_n&lt;/math&gt; sequences satisfy the equations:&lt;ref&gt;{{harvnb|LeVeque|1956|p=174}}.&lt;/ref&gt;
:&lt;math&gt;p_0 = 0\;\;\;\;\;\;\;p_1 = 1\;\;\;\;\;\;\;p_{n+1} = 2p_n + p_{n-1} \text{ for } n \geq 1&lt;/math&gt;
:&lt;math&gt;q_0 = 1\;\;\;\;\;\;\;q_1 = 2\;\;\;\;\;\;\;q_{n+1} = 2q_n + q_{n-1} \text{ for } n \geq 1&lt;/math&gt;

First, we prove by induction that for odd ''n'', the ''n''-th interval in the table is: 
:&lt;math&gt;\left(\frac{p_n + p_{n-1}}{q_n + q_{n-1}}, \frac{p_n}{q_n}\right)\!,&lt;/math&gt; 
and for even ''n'', the interval's endpoints are reversed: &lt;math&gt;\left(\frac{p_n}{q_n}, \frac{p_n + p_{n-1}}{q_n + q_{n-1}}\right)\!.&lt;/math&gt;

This is true for the first interval since:
:&lt;math&gt;\left(\frac{p_1 + p_0}{q_1 + q_0}, \frac{p_1}{q_1}\right) = \left(\frac{1}{3}, \frac{1}{2}\right)\!.&lt;/math&gt;

Assume that the inductive hypothesis is true for the ''k''-th interval. If ''k'' is odd, this interval is:
:&lt;math&gt;\left(\frac{p_k + p_{k-1}}{q_k + q_{k-1}}, \frac{p_k}{q_k}\right)\!.&lt;/math&gt;

The mediant of its endpoints &lt;math&gt;\frac{2p_k + p_{k-1}}{2q_k + q_{k-1}} = \frac{p_{k+1}}{q_{k+1}}&lt;/math&gt; is the first fraction in the sequence between these endpoints.&lt;br /&gt;&lt;br /&gt;

Hence, &lt;math&gt;\frac{p_k + p_{k-1}}{q_k + q_{k-1}} &lt; \frac{p_{k+1}}{q_{k+1}} &lt; \frac{p_k}{q_k}.&lt;/math&gt;&lt;br /&gt;&lt;br /&gt;

In this inequality, &lt;math&gt;\frac{p_k}{q_k}&lt;/math&gt; has the smallest denominator, so the second fraction is the mediant of &lt;math&gt;\frac{p_{k+1}}{q_{k+1}}&lt;/math&gt; and &lt;math&gt;\frac{p_k}{q_k},&lt;/math&gt; which equals &lt;math&gt;\frac{p_{k+1} + p_k}{p_{k+1} + q_k}.&lt;/math&gt;&lt;br /&gt;&lt;br /&gt;

This implies: &lt;math&gt;\frac{p_k + p_{k-1}}{q_k + q_{k-1}} &lt; \frac{p_{k+1}}{q_{k+1}} &lt; \frac{p_{k+1} + p_k}{p_{k+1} + q_k} &lt; \frac{p_k}{q_k}.&lt;/math&gt;&lt;br /&gt;&lt;br /&gt;

Therefore, the (''k''&amp;nbsp;+&amp;nbsp;1)-st interval is &lt;math&gt;\left(\frac{p_{k+1}}{q_{k+1}}, \frac{p_{k+1} + p_k}{p_{k+1} + q_k}\right)\!.&lt;/math&gt;&lt;br /&gt;&lt;br /&gt;

This is the desired interval; &lt;math&gt;\frac{p_{k+1}}{q_{k+1}}&lt;/math&gt; is the left endpoint because ''k''&amp;nbsp;+&amp;nbsp;1 is even. Thus, the inductive hypothesis is true for the (''k''&amp;nbsp;+&amp;nbsp;1)-st interval. For even ''k'', the proof is similar. This completes the inductive proof.

Since the right endpoints of the intervals are decreasing and every other endpoint is &lt;math&gt;\frac{p_{2n-1}}{q_{2n-1}},&lt;/math&gt; their limit equals
&lt;math&gt;\lim_{n \to \infty} \frac{p_n}{q_n}.&lt;/math&gt; The left endpoints have the same limit because they are increasing and every other endpoint is &lt;math&gt;\frac{p_{2n}}{q_{2n}}.&lt;/math&gt; As mentioned above, this limit is the continued fraction &lt;math&gt;[0; 2, 2, \dots],&lt;/math&gt; which equals &lt;math&gt;\sqrt{2}-1.&lt;/math&gt;&lt;ref&gt;{{harvnb|Weisstein|2003|p=541}}.&lt;/ref&gt;
|}

== Cantor's second uncountability proof ==
=== Everywhere dense ===
In 1879, Cantor published a new uncountability proof that modifies his 1874 proof. He first defines the [[topological]] notion of a point set ''P'' being "everywhere [[dense set|dense]] in an interval" (which is quite often shortened to "dense in an interval"):{{efn-ua|Cantor was not the first to define "everywhere dense" but his terminology was adopted with or without the "everywhere" (everywhere dense: {{harvnb|Arkhangel'skii|Fedorchuk|1990|p=15}}; dense: {{harvnb|Kelley|1991|p=49}}). In 1870, [[Hermann Hankel]] had defined this concept using different terminology: "a multitude of points … ''fill the segment'' if no interval, however small, can be given within the segment in which one does not find at least one point of that multitude" ({{harvnb|Ferreirós|2007|pp=155}}). Hankel was building on [[Peter Gustav Lejeune Dirichlet]]'s 1829 article that contains the [[Dirichlet function]], a non-([[Riemann integral|Riemann]]) [[integrable function]] whose value is 0 for [[rational number]]s and 1 for [[irrational number]]s. ({{harvnb|Ferreirós|2007|p=149}}.)}}

:If ''P'' lies partially or completely in the interval [α,&amp;nbsp;β], then the remarkable case can happen that ''all'' intervals [γ,&amp;nbsp;δ] contained in [α,&amp;nbsp;β], ''no matter how small,'' contain points of ''P''.  In such a case, we will say that ''P'' is ''everywhere dense in the interval'' [α,&amp;nbsp;β].{{efn-ua|The original German text from {{harvnb|Cantor|1879|p=2}} (Cantor's closed set notation (α&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β) is translated to [α,&amp;nbsp;β]):&lt;br&gt;
Liegt ''P'' theilweise oder ganz im Intervalle (α&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β), so kann der bemerkenswerthe Fall eintreten, dass ''jedes noch so kleine'' in (α&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β) enthaltene Intervall (γ&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;δ) Punkte von ''P'' enthält. In einem solchen Falle wollen wir sagen, dass ''P'' ''im Intervalle'' (α&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β) ''überall-dicht'' sei.}}

We will use ''a'',&amp;nbsp;''b'', ''c'',&amp;nbsp;''d'' rather than α,&amp;nbsp;β, γ,&amp;nbsp;δ. Cantor assumes that an interval [''c'',&amp;nbsp;''d''] satisfies ''d''&amp;nbsp;&gt;&amp;nbsp;''c''.

Since our discussion of Cantor's 1874 proof was simplified using by open intervals rather than closed intervals, the same simplification is used here. This requires an equivalent definition of everywhere dense: A set ''P'' is everywhere dense in the interval [''a'',&amp;nbsp;''b''] if and only if every subinterval (''c'',&amp;nbsp;''d'') of [''a'',&amp;nbsp;''b''] contains at least one point of ''P''.&lt;ref&gt;{{harvnb|Arkhangel'skii|Fedorchuk|1990|p=16}}.&lt;/ref&gt;

Cantor did not specify how many points of ''P'' a subinterval (''c'',&amp;nbsp;''d'') must contain. He did not need to specify this because assuming that every subinterval contains at least one point of ''P'' implies that they contain infinitely many points of ''P''. This is proved by generating a sequence of points belonging to both ''P'' and (''c'',&amp;nbsp;''d''). Since ''P'' is dense in [''a'',&amp;nbsp;''b''], the subinterval (''c'',&amp;nbsp;''d'') contains at least one point ''x''&lt;sub&gt;1&lt;/sub&gt; of ''P''. Now consider the subinterval (''x''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''d''). It contains at least one point ''x''&lt;sub&gt;2&lt;/sub&gt; of ''P'', which satisfies ''x''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;&gt;&amp;nbsp;x&lt;sub&gt;1&lt;/sub&gt;. In general, after generating ''x''&lt;sub&gt;''n''&lt;/sub&gt;, the subinterval (x&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''d'') is used to obtain the point ''x''&lt;sub&gt;''n''&amp;nbsp;+&amp;nbsp;1&lt;/sub&gt;, which satisfies ''x''&lt;sub&gt;''n''&amp;nbsp;+&amp;nbsp;1&lt;/sub&gt;&amp;nbsp;&gt;&amp;nbsp;''x''&lt;sub&gt;''n''&lt;/sub&gt;. The points ''x''&lt;sub&gt;''n''&lt;/sub&gt; are all unique and belong to both ''P'' and (''c'',&amp;nbsp;''d'').

=== Cantor's 1879 proof ===
Cantor's 1879 proof is the same as his 1874 proof except for a new proof of first part of his [[#Second theorem|second theorem]]: Given any sequence ''P'' of real numbers ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ''x''&lt;sub&gt;3&lt;/sub&gt;,&amp;nbsp;… and any interval [''a'',&amp;nbsp;''b''], there is a number in [''a'',&amp;nbsp;''b''] that is not contained in the sequence ''P''. The new proof has only two cases.&lt;ref group=proof name=p&gt;Since Cantor's proof has not been published in English, an English translation is given alongside the original German text, which is from {{harvnb|Cantor|1879|pp=5–7}}. The translation starts one sentence before the proof because this sentence mentions Cantor's 1874 proof. Cantor states it was printed in Borchardt's Journal. Crelle’s Journal was also called Borchardt’s Journal from 1856-1880 when [[Carl Wilhelm Borchardt]] edited the journal ({{harvnb|Audin|2011|p=80}}). Square brackets are used to identify this mention of Cantor's earlier proof, to clarify the translation, and to provide page numbers. Also, "Mannichfaltigkeit" (manifold) has been translated to "set" and Cantor's notation for closed sets (α&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β) has been translated to [α,&amp;nbsp;β]. Cantor changed his terminology from Mannichfaltigkeit to Menge (set) in his 1883 article, which introduced sets of [[ordinal number]]s ({{harvnb|Kanamori|2012|p=5}}). Currently in mathematics, a [[manifold]] is type of [[topological space]].

{| class="wikitable collapsible collapsed"
|-
! style="width: 47%;" | English translation
! style="width: 53%;" | German text
|- 
|[Page 5]&lt;br&gt;
&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;But this contradicts a very general theorem, which we have 
proved with full rigor in Borchardt's Journal, Vol. 77, page 260; namely, the following theorem:&lt;br&gt;
{{space|5}}''"If one has a simply [countably] infinite sequence''&lt;br&gt; 
{{space|12}}ω&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;ω&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;,&amp;nbsp;ω&lt;sub&gt;ν&lt;/sub&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&lt;br&gt;
''of real, unequal numbers that proceed according to some rule, then in every given interval [α,&amp;nbsp;β] a number η (and thus infinitely many of them) can be specified that does not occur in this sequence (as a member of it)."''&lt;br&gt;

{{space|5}}In view of the great interest in this theorem, not only in the present discussion, but also in many other arithmetical as well as analytical relations, it might not be superfluous if we develop the argument followed there [Cantor's 1874 proof] more clearly here by using simplifying modifications.&lt;br&gt;

{{space|5}}Starting with the sequence:&lt;br&gt;
{{space|12}}ω&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;ω&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;,&amp;nbsp;ω&lt;sub&gt;ν&lt;/sub&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&lt;br&gt;
(which we give [denote by] the symbol (ω)) and an arbitrary interval [α,&amp;nbsp;β], where α&amp;nbsp;&lt;&amp;nbsp;β, we will now demonstrate that in this interval a real number η can be found that does ''not'' occur in (ω).&lt;br&gt;

{{space|5}}I. We first notice that if our set (ω) is ''not everywhere dense'' in the interval [α,&amp;nbsp;β], then within this interval another interval [γ,&amp;nbsp;δ] must be present, all of whose numbers do not belong to (ω). From the interval [γ,&amp;nbsp;δ], one can then choose any number for η. It lies in the interval [α,&amp;nbsp;β] and definitely does ''not'' occur in our sequence (ω). Thus, this case presents no special considerations and we can move on to the ''more difficult'' case.

{{space|5}}II. Let the set (ω) be ''everywhere dense'' in the interval [α,&amp;nbsp;β]. In this case, every interval [γ,δ] located in [α,β], however small, contains numbers of our sequence (ω). To show that, ''nevertheless,'' numbers η in the interval [α,&amp;nbsp;β] exist that do not occur in (ω), we employ the following observation.

{{space|5}}Since some numbers in our sequence:&lt;br&gt;
{{space|12}}ω&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;ω&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;,&amp;nbsp;ω&lt;sub&gt;ν&lt;/sub&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&lt;br&gt;

|[Seite 5]&lt;br&gt;
&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;Dem widerspricht aber ein sehr allgemeiner Satz, welchen wir in Borchardt's Journal, Bd. 77, pag. 260, mit aller Strenge bewiesen haben, nämlich der folgende Satz:

{{space|5}}''"Hat man eine einfach unendliche Reihe''&lt;br&gt;
{{space|12}}ω&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;ω&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;,&amp;nbsp;ω&lt;sub&gt;ν&lt;/sub&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&lt;br&gt;
''von reellen, ungleichen Zahlen, die nach irgend einem Gesetz fortschreiten, so lässt sich in jedem vorgegebenen, Intervalle (α&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β) eine Zahl η (und folglich lassen sich deren unendlich viele) angeben, welche nicht in jener Reihe (als Glied derselben) vorkommt."''

{{space|5}}In Anbetracht des grossen Interesses, welches sich an diesen Satz, nicht blos bei der gegenwärtigen Erörterung, sondern auch in vielen anderen sowohl arithmetischen, wie analytischen Beziehungen, knüpft, dürfte es nicht überflüssig sein, wenn wir die dort befolgte Beweisführung [Cantors 1874 Beweis], unter Anwendung vereinfachender Modificationen, hier deutlicher entwickeln.

{{space|5}}Unter Zugrundelegung der Reihe:&lt;br&gt;
{{space|12}}ω&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;ω&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;,&amp;nbsp;ω&lt;sub&gt;ν&lt;/sub&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&lt;br&gt;
(welcher wir das Zeichen (ω) beilegen) und eines beliebigen Intervalles (α&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β), wo α&amp;nbsp;&lt;&amp;nbsp;β ist, soll also nun gezeigt werden, dass in diesem Intervalle eine reelle Zahl η gefunden werden kann, welche in (ω) ''nicht'' vorkommt.

{{space|5}}I. Wir bemerken zunächst, dass wenn unsre Mannichfaltigkeit (ω) in dem Intervall (α&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β)  ''nicht überall-dicht'' ist, innerhalb dieses Intervalles ein anderes (γ&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;δ) vorhanden sein muss, dessen Zahlen sämmtlich nicht zu (ω) gehören; man kann alsdann für η irgend eine Zahl des Intervalls (γ&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;δ) wählen, sie liegt im Intervalle (α&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β) und kommt sicher in unsrer Reihe (ω) ''nicht'' vor. Dieser Fall bietet daher keinerlei besondere Umstände; und wir können zu dem ''schwierigeren'' übergehen.

{{space|5}}II. Die Mannichfaltigkeit (ω) sei im Intervalle (α&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β) ''überall-dicht''. In diesem Falle enthält jedes, noch so kleine in (α&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β) gelegene Intervall (γ&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;δ) Zahlen unserer Reihe (ω). Um zu zeigen, dass ''nichtsdestoweniger'' Zahlen η im Intervalle (α&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β) existiren, welche in (ω) nicht vorkommen, stellen wir die folgende Betrachtung an.

{{space|5}}Da in unserer Reihe:&lt;br&gt;
{{space|12}}ω&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;ω&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;,&amp;nbsp;ω&lt;sub&gt;ν&lt;/sub&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&lt;br&gt;

|-
|[Page 6]&lt;br&gt;
definitely occur ''within'' the interval [α,&amp;nbsp;β], one of these numbers must have the ''least index,'' let it be ω&lt;sub&gt;κ&lt;sub&gt;1&lt;/sub&gt;&lt;/sub&gt;, and another: ω&lt;sub&gt;κ&lt;sub&gt;2&lt;/sub&gt;&lt;/sub&gt; with the next larger index.

{{space|5}}Let the smaller of the two numbers  ω&lt;sub&gt;κ&lt;sub&gt;1&lt;/sub&gt;&lt;/sub&gt;, ω&lt;sub&gt;κ&lt;sub&gt;2&lt;/sub&gt;&lt;/sub&gt; be denoted by α', the larger by β'. (Their equality is impossible because we assumed that our sequence consists of nothing but unequal numbers.)

{{space|5}}Then according to the definition:&lt;br&gt;
{{space|12}}α&amp;nbsp;&lt;&amp;nbsp;α'&amp;nbsp;&lt;&amp;nbsp;β'&amp;nbsp;&lt;&amp;nbsp;β{{space|hair}},&lt;br&gt;
furthermore:&lt;br&gt;                           
{{space|17}}κ&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;&lt;&amp;nbsp;κ&lt;sub&gt;2&lt;/sub&gt;{{space|hair}};&lt;br&gt;
and it should also be noted that all numbers ω&lt;sub&gt;μ&lt;/sub&gt; of our sequence, 
for which μ ≤ κ&lt;sub&gt;2&lt;/sub&gt;, do ''not'' lie in the interior of the interval [α',&amp;nbsp;β'], as is immediately clear from the definition of the numbers κ&lt;sub&gt;1&lt;/sub&gt;, κ&lt;sub&gt;2&lt;/sub&gt;. Similarly, let ω&lt;sub&gt;κ&lt;sub&gt;3&lt;/sub&gt;&lt;/sub&gt; and ω&lt;sub&gt;κ&lt;sub&gt;4&lt;/sub&gt;&lt;/sub&gt; be the two numbers of our sequence with smallest indices that fall in the ''interior'' of the interval [α',&amp;nbsp;β'] and let the smaller of the numbers ω&lt;sub&gt;κ&lt;sub&gt;3&lt;/sub&gt;&lt;/sub&gt;, ω&lt;sub&gt;κ&lt;sub&gt;4&lt;/sub&gt;&lt;/sub&gt; be denoted by α&lt;nowiki&gt;''&lt;/nowiki&gt;, the larger by β&lt;nowiki&gt;''&lt;/nowiki&gt;.&lt;br&gt;

{{space|5}}Then one has: &lt;br&gt; 
{{space|12}}α'&amp;nbsp;&lt; α&lt;nowiki&gt;''&lt;/nowiki&gt;&amp;nbsp;&lt;&amp;nbsp;β&lt;nowiki&gt;''&lt;/nowiki&gt;&amp;nbsp;&lt;&amp;nbsp;β'{{space|hair}},&lt;br&gt;
{{space|14}}κ&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;&lt;&amp;nbsp;κ&lt;sub&gt;3&lt;/sub&gt;&amp;nbsp;&lt;&amp;nbsp;κ&lt;sub&gt;4&lt;/sub&gt;{{space|hair}};&lt;br&gt;
and one sees that all numbers ω&lt;sub&gt;μ&lt;/sub&gt; of our sequence, for which μ&amp;nbsp;≤&amp;nbsp;κ&lt;sub&gt;4&lt;/sub&gt;, do ''not'' fall into the ''interior'' of the interval [α&lt;nowiki&gt;''&lt;/nowiki&gt;,&amp;nbsp;β&lt;nowiki&gt;''&lt;/nowiki&gt;].

{{space|5}} After one has followed this rule to reach an interval {{nowrap|[α&lt;sup&gt;(ν - 1)&lt;/sup&gt;, β&lt;sup&gt;(ν - 1)&lt;/sup&gt;]}}, the next interval is produced by selecting the first two (i. e. with lowest indices) numbers of our sequence (ω) (let them be {{nowrap|ω&lt;sub&gt;κ&lt;sub&gt;2ν - 1&lt;/sub&gt;&lt;/sub&gt;}} and ω&lt;sub&gt;κ&lt;sub&gt;2ν&lt;/sub&gt;&lt;/sub&gt;) that fall into the ''interior'' of {{nowrap|[α&lt;sup&gt;(ν - 1)&lt;/sup&gt;,&amp;nbsp;β&lt;sup&gt;(ν - 1)&lt;/sup&gt;]}}. Let the smaller of these two numbers be denoted  by α&lt;sup&gt;(ν)&lt;/sup&gt;, the larger by β&lt;sup&gt;(ν)&lt;/sup&gt;.

{{space|5}}The interval [α&lt;sup&gt;(ν)&lt;/sup&gt;,&amp;nbsp;β&lt;sup&gt;(ν)&lt;/sup&gt;] then lies in the ''interior'' of all preceding intervals and has the ''specific'' relation with our sequence (ω) that all numbers ω&lt;sub&gt;μ&lt;/sub&gt;, for which μ ≤ κ&lt;sub&gt;2ν&lt;/sub&gt;, ''definitely do not lie in its interior''. Since obviously:&lt;br&gt;
{{space|12}}{{nowrap|κ&lt;sub&gt;1&lt;/sub&gt; &lt; κ&lt;sub&gt;2&lt;/sub&gt; &lt; κ&lt;sub&gt;3&lt;/sub&gt; &lt;  . . . , ω&lt;sub&gt;κ&lt;sub&gt;2ν – 2&lt;/sub&gt;&lt;/sub&gt; &lt;/sub&gt; &lt; ω&lt;sub&gt;κ&lt;sub&gt;2ν – 1&lt;/sub&gt;&lt;/sub&gt; &lt; ω&lt;sub&gt;κ&lt;sub&gt;2ν&lt;/sub&gt;&lt;/sub&gt;{{space|hair}}, . . . }}&lt;br&gt;
and these numbers, as indices, are ''whole'' numbers, so:&lt;br&gt;
{{space|12}}κ&lt;sub&gt;2ν&lt;/sub&gt;&amp;nbsp;≥&amp;nbsp;2ν{{space|hair}},&lt;br&gt;
and hence:&lt;br&gt;
{{space|12}}ν&amp;nbsp;&lt;&amp;nbsp;κ&lt;sub&gt;2ν&lt;/sub&gt;{{space|hair}};&lt;br&gt;
thus, we can certainly say (and this is sufficient for the following):&lt;br&gt;

{{space|5}}''That if ν is an arbitrary whole number, the [real] quantity ω&lt;sub&gt;ν&lt;/sub&gt; lies outside the interval [α&lt;sup&gt;(ν)&lt;/sup&gt;&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β&lt;sup&gt;(ν)&lt;/sup&gt;].''

|[Seite 6]&lt;br&gt;
sicher Zahlen ''innerhalb'' des Intervalls (α&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β) vorkommen, so muss eine von diesen Zahlen den ''kleinsten Index'' haben, sie sei ω&lt;sub&gt;κ&lt;sub&gt;1&lt;/sub&gt;&lt;/sub&gt;, und eine andere: ω&lt;sub&gt;κ&lt;sub&gt;2&lt;/sub&gt;&lt;/sub&gt; mit dem nächst grösseren Index behaftet sein.

{{space|5}}Die kleinere der beiden Zahlen ω&lt;sub&gt;κ&lt;sub&gt;1&lt;/sub&gt;&lt;/sub&gt;, ω&lt;sub&gt;κ&lt;sub&gt;2&lt;/sub&gt;&lt;/sub&gt; werde mit α', die grössere mit β' bezeichnet. (Ihre Gleichheit ist ausgeschlossen, weil wir voraussetzten, dass unsere Reihe aus lauter ungleichen Zahlen besteht.)

{{space|5}}Es ist alsdann der Definition nach:&lt;br&gt;
{{space|12}}α&amp;nbsp;&lt;&amp;nbsp;α'&amp;nbsp;&lt;&amp;nbsp;β'&amp;nbsp;&lt;&amp;nbsp;β{{space|hair}},&lt;br&gt;
ferner:&lt;br&gt;                           
{{space|17}}κ&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;&lt;&amp;nbsp;κ&lt;sub&gt;2&lt;/sub&gt;{{space|hair}};&lt;br&gt;
und ausserdem ist zu bemerken, dass alle Zahlen ω&lt;sub&gt;μ&lt;/sub&gt; unserer Reihe, 
für welche μ ≤ κ&lt;sub&gt;2&lt;/sub&gt;, ''nicht'' im Innern des Intervalls (α'&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β') liegen, wie aus der Bestimmung der Zahlen κ&lt;sub&gt;1&lt;/sub&gt;, κ&lt;sub&gt;2&lt;/sub&gt; sofort erhellt. Ganz ebenso mögen ω&lt;sub&gt;κ&lt;sub&gt;3&lt;/sub&gt;&lt;/sub&gt;, ω&lt;sub&gt;κ&lt;sub&gt;4&lt;/sub&gt;&lt;/sub&gt; die beiden mit den kleinsten Indices versehenen Zahlen unserer Reihen [see note 1 below] sein, welche in das ''Innere'' des Intervalls (α'&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β') fallen und die kleinere der Zahlen ω&lt;sub&gt;κ&lt;sub&gt;3&lt;/sub&gt;&lt;/sub&gt;, ω&lt;sub&gt;κ&lt;sub&gt;4&lt;/sub&gt;&lt;/sub&gt; werde mit α&lt;nowiki&gt;''&lt;/nowiki&gt;, die grössere mit β&lt;nowiki&gt;''&lt;/nowiki&gt; bezeichnet.&lt;br&gt;

{{space|5}}Man hat alsdann: &lt;br&gt; 
{{space|12}}α'&amp;nbsp;&lt; α&lt;nowiki&gt;''&lt;/nowiki&gt;&amp;nbsp;&lt;&amp;nbsp;β&lt;nowiki&gt;''&lt;/nowiki&gt;&amp;nbsp;&lt;&amp;nbsp;β'{{space|hair}},&lt;br&gt;
{{space|14}}κ&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;&lt;&amp;nbsp;κ&lt;sub&gt;3&lt;/sub&gt;&amp;nbsp;&lt;&amp;nbsp;κ&lt;sub&gt;4&lt;/sub&gt;{{space|hair}};&lt;br&gt;
und man erkennt, dass alle Zahlen ω&lt;sub&gt;μ&lt;/sub&gt; unserer Reihe, für welche μ&amp;nbsp;≤&amp;nbsp;κ&lt;sub&gt;4&lt;/sub&gt; ''nicht'' in das ''Innere'' des Intervalls (α&lt;nowiki&gt;''&lt;/nowiki&gt;&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β&lt;nowiki&gt;''&lt;/nowiki&gt;) fallen.

{{space|5}}Nachdem man unter Befolgung des gleichen Gesetzes zu einem Intervall {{nowrap|(α&lt;sup&gt;(ν - 1)&lt;/sup&gt;, . . . β&lt;sup&gt;(ν - 1)&lt;/sup&gt;)}} gelangt ist, ergiebt sich das folgende Intervall dadurch aus demselben, dass man die beiden ersten (d. h. mit niedrigsten Indices versehenen) Zahlen unserer Reihe (ω) aufstellt (sie seien ω&lt;sub&gt;κ&lt;sub&gt;2ν – 1&lt;/sub&gt;&lt;/sub&gt; und ω&lt;sub&gt;κ&lt;sub&gt;2ν&lt;/sub&gt;&lt;/sub&gt;), welche in das ''Innere'' von {{nowrap|(α&lt;sup&gt;(ν – 1)&lt;/sup&gt; . . . β&lt;sup&gt;(ν – 1)&lt;/sup&gt;)}} fallen; die kleinere dieser beiden Zahlen werde mit α&lt;sup&gt;(ν)&lt;/sup&gt;, die grössere mit β&lt;sup&gt;(ν)&lt;/sup&gt; bezeichnet.

{{space|5}}Das Intervall (α&lt;sup&gt;(ν)&lt;/sup&gt;&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β&lt;sup&gt;(ν)&lt;/sup&gt;) liegt alsdann im ''Innern'' aller vorangegangenen Intervalle und hat zu unserer Reihe (ω) die ''eigenthümliche'' 
Beziehung, dass alle Zahlen ω&lt;sub&gt;μ&lt;/sub&gt;, für welche μ ≤ κ&lt;sub&gt;2ν&lt;/sub&gt; ''sicher nicht in seinem Innern'' liegen.  Da offenbar:&lt;br&gt;
{{space|12}}{{nowrap|κ&lt;sub&gt;1&lt;/sub&gt; &lt; κ&lt;sub&gt;2&lt;/sub&gt; &lt; κ&lt;sub&gt;3&lt;/sub&gt; &lt;  . . . , ω&lt;sub&gt;κ&lt;sub&gt;2ν – 2&lt;/sub&gt;&lt;/sub&gt; &lt;/sub&gt; &lt; ω&lt;sub&gt;κ&lt;sub&gt;2ν – 1&lt;/sub&gt;&lt;/sub&gt; &lt; ω&lt;sub&gt;κ&lt;sub&gt;2ν&lt;/sub&gt;&lt;/sub&gt;{{space|hair}}, . . . }}

und diese Zahlen, als Indices, ''ganze'' Zahlen sind, so ist:&lt;br&gt;
{{space|12}}κ&lt;sub&gt;2ν&lt;/sub&gt;&amp;nbsp;≥&amp;nbsp;2ν{{space|hair}},&lt;br&gt;
und daher:&lt;br&gt;
{{space|12}}ν&amp;nbsp;&lt;&amp;nbsp;κ&lt;sub&gt;2ν&lt;/sub&gt;{{space|hair}};&lt;br&gt;
wir können daher, und dies ist für das Folgende ausreichend, gewiss sagen:&lt;br&gt;

{{space|5}}''Dass, wenn ν eine beliebige ganze Zahl ist, die Grösse ω&lt;sub&gt;ν&lt;/sub&gt; ausserhalb des Intervalls (α&lt;sup&gt;(ν)&lt;/sup&gt;&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β&lt;sup&gt;(ν)&lt;/sup&gt;) liegt.''

|-
|[Page 7]&lt;br&gt;
{{space|5}}Since the numbers α',  α&lt;nowiki&gt;''&lt;/nowiki&gt;,  α&lt;nowiki&gt;'''&lt;/nowiki&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;., α&lt;sup&gt;(ν)&lt;/sup&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;. are continually increasing by value while simultaneously being enclosed in the interval [α,&amp;nbsp;β], they have, by a well-known fundamental theorem of the theory of magnitudes [see note 2 below], a limit that we denote by A, so that:&lt;br&gt;
{{space|12}}{{nowrap|A {{=}} Lim α&lt;sup&gt;(ν)&lt;/sup&gt;  for  ν {{=}} ∞.}}

{{space|5}}The same applies to the numbers β', β&lt;nowiki&gt;''&lt;/nowiki&gt;, β&lt;nowiki&gt;'''&lt;/nowiki&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;., β&lt;sup&gt;(ν)&lt;/sup&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;., which are continually decreasing and likewise lying in the interval [α,&amp;nbsp;β]. We call their limit B, so that:&lt;br&gt; 
{{space|12}}{{nowrap|B {{=}} Lim β&lt;sup&gt;(ν)&lt;/sup&gt;  for  ν {{=}} ∞.}}

{{space|5}}Obviously, one has:&lt;br&gt;
{{space|12}}{{nowrap|α&lt;sup&gt;(ν)&lt;/sup&gt; &lt; A ≤ B &lt; β&lt;sup&gt;(ν)&lt;/sup&gt;.}}

{{space|5}}But it is easy to see that the case A&amp;nbsp;&lt;&amp;nbsp;B can ''not'' occur here since otherwise every number ω&lt;sub&gt;ν&lt;/sub&gt; of our sequence would lie ''outside'' of the interval [A,&amp;nbsp;B] by lying outside the interval [α&lt;sup&gt;(ν)&lt;/sup&gt;,&amp;nbsp;β&lt;sup&gt;(ν)&lt;/sup&gt;]. So our sequence (ω) would ''not'' be ''everywhere dense'' in the interval [α,&amp;nbsp;β], contrary 
to the assumption.

{{space|5}}Thus, there only remains the case A&amp;nbsp;=&amp;nbsp;B and now it is demonstrated that the number:&lt;br&gt; 
{{space|12}}η&amp;nbsp;=&amp;nbsp;A&amp;nbsp;=&amp;nbsp;B&lt;br&gt; 
does ''not'' occur in our sequence (ω).

{{space|5}}If it were a member of our sequence, such as the ν&lt;sup&gt;th&lt;/sup&gt;, then one would have: η = ω&lt;sub&gt;ν&lt;/sub&gt;.

{{space|5}}But the latter equation is not possible for any value of ν because η is in the ''interior'' of the interval [α&lt;sup&gt;(ν)&lt;/sup&gt;,&amp;nbsp;β&lt;sup&gt;(ν)&lt;/sup&gt;], but ω&lt;sub&gt;ν&lt;/sub&gt; lies ''outside'' of it.

|[Seite 7]&lt;br&gt;
{{space|5}}Da die Zahlen α', α&lt;nowiki&gt;''&lt;/nowiki&gt;, α&lt;nowiki&gt;'''&lt;/nowiki&gt;,,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;., α&lt;sup&gt;(ν)&lt;/sup&gt;,,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;. ihrer Grösse nach fortwährend wachsen, dabei jedoch im Intervalle (α&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β) eingeschlossen sind, so haben sie, nach einem bekannten Fundamentalsatze der Grössenlehre, eine Grenze, die wir mit A bezeichnen, so dass:&lt;br&gt;
{{space|12}}{{nowrap|A {{=}} Lim α&lt;sup&gt;(ν)&lt;/sup&gt;  für  ν {{=}} ∞.}}

{{space|5}}Ein Gleiches gilt für die Zahlen β', β&lt;nowiki&gt;''&lt;/nowiki&gt;, β&lt;nowiki&gt;'''&lt;/nowiki&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;., β&lt;sup&gt;(ν)&lt;/sup&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.  welche fortwährend abnehmen und dabei ebenfalls im Intervalle (α&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β) liegen; wir nennen ihre Grenze B, so dass:&lt;br&gt;
{{space|12}}{{nowrap|B {{=}} Lim β&lt;sup&gt;(ν)&lt;/sup&gt;  für  ν {{=}} ∞.}}

{{space|5}}Man hat offenbar:&lt;br&gt;
{{space|12}}{{nowrap|α&lt;sup&gt;(ν)&lt;/sup&gt; &lt; A ≤ B &lt; β&lt;sup&gt;(ν)&lt;/sup&gt;.}}

{{space|5}}Es ist aber leicht zu sehen, dass der Fall A&amp;nbsp;&lt;&amp;nbsp;B hier ''nicht'' vorkommen kann; da sonst jede Zahl ω&lt;sub&gt;ν&lt;/sub&gt;, unserer Reihe ''ausserhalb'' des Intervalles (A&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;B) liegen würde, indem ω&lt;sub&gt;ν&lt;/sub&gt;, ausserhalb des Intervalls (α&lt;sup&gt;(ν)&lt;/sup&gt; . . . β&lt;sup&gt;(ν)&lt;/sup&gt;) gelegen ist; unsere Reihe (ω) wäre im Intervall (α&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;β) ''nicht überalldicht,'' gegen die Voraussetzung.

{{space|5}}Es bleibt daher nur der Fall A&amp;nbsp;=&amp;nbsp; B übrig und es zeigt sich nun, dass die Zahl:&lt;br&gt;
{{space|12}}{{nowrap|η {{=}} A {{=}} B}}&lt;br&gt;
in unserer Reihe (ω) ''nicht'' vorkommt.

{{space|5}}Denn, würde sie ein Glied unserer Reihe sein, etwa das ν&lt;sup&gt;te&lt;/sup&gt;, so hätte man: η = ω&lt;sub&gt;ν&lt;/sub&gt;.

{{space|5}}Die letztere Gleichung ist aber für keinen Werth von v möglich, weil η im ''Innern'' des Intervalls [α&lt;sup&gt;(ν)&lt;/sup&gt;,&amp;nbsp;β&lt;sup&gt;(ν)&lt;/sup&gt;], ω&lt;sub&gt;ν&lt;/sub&gt; aber ''ausserhalb'' desselben liegt.
|-
| colspan="2" | Note 1:  This is the only occurrence of "unserer Reihen" ("our sequences") in the proof. There is only one sequence involved in Cantor's proof and everywhere else "Reihe" ("sequence") is used, so it is most likely a typographical error and should be "unserer Reihe" ("our sequence"), which is how it has been translated.&lt;br&gt;
Note 2: Grössenlehre, which has been translated as "the theory of magnitudes", is a term used by 19th century German mathematicians that refers to the theory of [[Discrete mathematics|discrete]] and [[Linear continuum|continuous]] magnitudes. ({{harvnb|Ferreirós|2007|pp=41-42, 202}}.)
|}&lt;/ref&gt;

In the first case, ''P'' is not dense in [''a'',&amp;nbsp;''b'']. By definition, ''P'' is dense if and only if for all {{nowrap|(''c'', ''d'') ⊆ [''a'',&amp;nbsp;''b'']}}, there is an ''x''&amp;nbsp;∈&amp;nbsp;''P'' such that {{nowrap|''x'' ∈ (''c'', ''d'')}}. Taking the negation of each side of the "if and only if" produces: ''P'' is not dense in [''a'',&amp;nbsp;''b''] if and only if there exists a {{nowrap|(''c'', ''d'') ⊆ [''a'',&amp;nbsp;''b'']}} such that for all ''x''&amp;nbsp;∈&amp;nbsp;''P'', we have {{nowrap|''x'' ∉ (''c'', ''d'')}}. Thus, every number in (''c'',&amp;nbsp;''d'') is not contained in the sequence ''P''.&lt;ref group=proof name=p /&gt; This case handles [[#Cases|cases 1 and 3]] of Cantor's 1874 proof.

In the second case, ''P'' is dense in [''a'',&amp;nbsp;''b'']. The denseness of ''P'' is used to [[recursively define]] a nested sequence of intervals that excludes all elements of ''P''. The definition begins with
''a''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''a'' and ''b''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''b''. The definition's inductive case starts with the interval (''a''&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;), which because of the denseness of ''P'' contains infinitely many elements of ''P''. From these elements of ''P'', we take the two with smallest indices and denote the least of these two numbers by ''a''&lt;sub&gt;''n''&amp;nbsp;+&amp;nbsp;1&lt;/sub&gt; and the greatest by ''b''&lt;sub&gt;''n''&amp;nbsp;+&amp;nbsp;1&lt;/sub&gt;.
Cantor proved that for all&amp;nbsp;''n''{{space|hair}}:{{space|hair}} {{nowrap|''x''&lt;sub&gt;''n''&lt;/sub&gt; ∉ (''a''&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;)}}.&lt;ref group=proof name=p /&gt; We proved this in a [[#x_n not in|previous section]].

The sequence ''a''&lt;sub&gt;''n''&lt;/sub&gt; is increasing and bounded above by ''b'', so it has a limit ''A'', which satisfies ''a''&lt;sub&gt;''n''&lt;/sub&gt;&amp;nbsp;&lt;&amp;nbsp;''A''. The sequence ''b''&lt;sub&gt;''n''&lt;/sub&gt; is decreasing and bounded below by ''a'', so it has a limit ''B'', which satisfies ''B''&amp;nbsp;&lt;&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;. Also, ''a''&lt;sub&gt;''n''&lt;/sub&gt;&amp;nbsp;&lt;&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt; implies ''A''&amp;nbsp;≤&amp;nbsp;''B''. Therefore, {{nowrap|''a''&lt;sub&gt;''n''&lt;/sub&gt; &lt; ''A'' ≤ ''B'' &lt; ''b''&lt;sub&gt;''n''&lt;/sub&gt;}}. If ''A''&amp;nbsp;&lt;&amp;nbsp;''B'', then for every ''n'': ''x''&lt;sub&gt;''n''&lt;/sub&gt;&amp;nbsp;∉&amp;nbsp;(''A'',&amp;nbsp;''B'') because ''x''&lt;sub&gt;''n''&lt;/sub&gt; is not in the larger interval (''a''&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;). This contradicts ''P'' being dense in [''a'',&amp;nbsp;''b'']. Therefore, ''A''&amp;nbsp;=&amp;nbsp;''B''. Since for all ''n'': {{nowrap|''A'' ∈ (''a''&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;)}} but {{nowrap|''x''&lt;sub&gt;''n''&lt;/sub&gt; ∉ (''a''&lt;sub&gt;''n''&lt;/sub&gt;,&amp;nbsp;''b''&lt;sub&gt;''n''&lt;/sub&gt;)}}, the limit ''A'' is a real number that is not contained in the sequence ''P''.&lt;ref group=proof name=p /&gt; This case handles [[#Cases|case 2]] of Cantor's 1874 proof.

Cantor's new proof first takes care of the easy case of the sequence ''P'' not being dense in the interval. Then it deals with the more difficult case of ''P'' being dense. This division into cases not only indicates which sequences are most difficult to handle, but it also reveals the important role denseness plays in the proof.&lt;ref group=proof name=p /&gt;

In the [[#Example of Cantor's construction|Example of Cantor's construction]], each successive nested interval excludes  rational numbers for two different reasons. It will exclude the finitely many rationals visited in the search for the first two rationals within the interval (these two rationals will have the least indices). These rationals are then used to form an interval that excludes the rationals visited in the search along with infinitely many more rationals. However, it still contains infinitely many rationals since our sequence of rationals is dense in [0, 1]. Forming this interval from the two rationals with the least indices guarantees that this interval excludes an initial segment of our sequence that contains at least two more elements than the preceding initial segment. Since the denseness of our sequence guarantees that this process never ends, all rationals will be excluded.&lt;ref group=proof name=p /&gt; Because of the ordering of the rationals in our sequence, the [[Intersection (set theory)|intersection]] of the nested intervals is the [[#Number generated|set {{{sqrt|2}}&amp;nbsp;&amp;minus;&amp;nbsp;1}]].

==The development of Cantor's ideas==
The development leading to Cantor's 1874 article appears in the correspondence between Cantor and [[Richard Dedekind]]. On November 29, 1873, Cantor asked Dedekind whether the collection of positive integers and the collection of positive real numbers "can be corresponded so that each individual of one collection corresponds to one and only one individual of the other?" Cantor added that collections having such a correspondence include the collection of positive rational numbers, and collections of the form (''a''&lt;sub&gt;''n''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''n''&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;.&amp;nbsp;.&amp;nbsp;.&amp;nbsp;,&amp;nbsp;''n''&lt;sub&gt;''ν''&lt;/sub&gt;&lt;/sub&gt;) where ''n''&lt;sub&gt;1&lt;/sub&gt;, ''n''&lt;sub&gt;2&lt;/sub&gt;, .&amp;nbsp;.&amp;nbsp;. , ''n''&lt;sub&gt;''ν''&lt;/sub&gt;, and ''ν'' are positive integers.&lt;ref&gt;{{harvnb|Noether|Cavaillès|1937|pp=12&amp;ndash;13}}. English translation: {{harvnb|Gray|1994|p=827}}; {{harvnb|Ewald|1996|p=844}}.&lt;/ref&gt;

Dedekind replied that he was unable to answer Cantor's question, and said that it "did not deserve too much effort because it has no particular practical interest." Dedekind also sent Cantor a proof that the set of algebraic numbers is countable.&lt;ref name=Noether18&gt;{{harvnb|Noether|Cavaillès|1937|p=18}}. English translation: {{harvnb|Ewald|1996|p=848}}.&lt;/ref&gt;

{{Anchor|Cantor's December 2nd letter}}
On December 2, Cantor responded that his question does have interest: "It would be nice if it could be answered; for example, provided that it could be answered ''no'', one would have a new proof of [[Liouville number|Liouville's theorem]] that there are transcendental numbers."&lt;ref&gt;{{harvnb|Noether|Cavaillès|1937|p=13}}. English translation: {{harvnb|Gray|1994|p=827}}.&lt;/ref&gt;

On December 7, Cantor sent Dedekind a proof by contradiction that the set of real numbers is uncountable. Cantor starts by assuming the real numbers can be written as a sequence. Then he applies a construction to this sequence to produce a real number not in the sequence, thus contradicting his assumption.&lt;ref&gt;{{harvnb|Noether|Cavaillès|1937|pp=14&amp;ndash;15}}. English translation: {{harvnb|Ewald|1996|pp=845&amp;ndash;846}}.&lt;/ref&gt; The letters of December 2 and 7 lead to a non-constructive proof of the existence of transcendental numbers.

On December 9, Cantor announced the theorem that allowed him to construct transcendental numbers as well as prove the uncountability of the set of real numbers:

{{quote|I show directly that if I start with a sequence&lt;br /&gt;(I)&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp; ''ω''&lt;sub&gt;1&lt;/sub&gt;, ''ω''&lt;sub&gt;2&lt;/sub&gt;, … , ''ω''&lt;sub&gt;''n''&lt;/sub&gt;, …&lt;br /&gt;I can determine, in ''every'' given interval [''α'',&amp;nbsp;''β''], a number ''η'' that is not included in (I).&lt;ref&gt;{{harvnb|Noether|Cavaillès|1937|p=16}}. English translation: {{harvnb|Gray|1994|p=827}}.&lt;/ref&gt;}}

This is the second theorem in Cantor's article. It comes from realizing that his construction can be applied to any sequence, not just to sequences that supposedly enumerate the real numbers. So Cantor had a choice between two proofs that demonstrate the existence of transcendental numbers: one proof is constructive, but the other is not. We now compare the proofs assuming that we have a sequence consisting of all the real algebraic numbers.

The constructive proof applies Cantor's construction to this sequence and the interval [''a'',&amp;nbsp;''b''] to produce a transcendental number in this interval.&lt;ref name=Ewald840_841 /&gt;

The non-constructive proof uses two proofs by contradiction:
# The proof by contradiction used to prove the uncountability theorem (see [[#Proof of Cantor's uncountability theorem|Proof of Cantor's uncountability theorem]]).
# The proof by contradiction used to prove the existence of transcendental numbers from the countability of the real algebraic numbers and the uncountability of real numbers. [[#Cantor's December 2nd letter|Cantor's December 2nd letter]] mentions this existence proof but does not contain it. Here is a proof: Assume that there are no transcendental numbers in [''a'',&amp;nbsp;''b'']. Then all the numbers in [''a'',&amp;nbsp;''b''] are algebraic. This implies that they form a [[subsequence]] of the sequence of all real algebraic numbers, which contradicts Cantor's uncountability theorem. Thus, the assumption that there are no transcendental numbers in [''a'',&amp;nbsp;''b''] is false. Therefore, there is a transcendental number in [''a'',&amp;nbsp;''b''].{{efn-ua|The beginning of our proof is derived from the proof below by restricting the numbers in this proof to the interval [''a'',&amp;nbsp;''b'']. However, we derive the contradiction by using a subsequence because Cantor was using sequences in his 1873 work on countability.&lt;br&gt;'''''German text:'' Satz 68. Es gibt transzendente Zahlen.'''&lt;br&gt;Gäbe es nämlich keine transzendenten Zahlen, so wären alle Zahlen algebraisch, das Kontinuum also identisch mit der Menge aller algebraischen Zahlen. Das ist aber unmöglich, weil die Menge aller algebraischen Zahlen abzählbar ist, das Kontinuum aber nicht.&lt;ref&gt;{{harvnb|Perron|1921|p=162}}.&lt;/ref&gt;&lt;br&gt;'''''Translation:'' Theorem 68. There are transcendental numbers.'''&lt;br&gt; If there were no transcendental numbers, then all numbers would be algebraic. Hence, the [[continuum (set theory)|continuum]] would be identical to the set of all algebraic numbers. However, this is impossible because the set of all algebraic numbers is countable, but the continuum is not.}}

Cantor chose to publish the constructive proof, which not only produces a transcendental number but is also shorter and avoids two proofs by contradiction. The non-constructive proof from Cantor's correspondence is simpler than the one above because it works with all the real numbers rather than the interval [''a'',&amp;nbsp;''b'']. This eliminates the subsequence step and all occurrences of [''a'',&amp;nbsp;''b''] in the second proof by contradiction.&lt;ref name=Ewald840_841 /&gt;

==The disagreement about Cantor's existence proof==
[[File:Oskar Perron.jpg|thumb|upright=0.93|Oskar Perron]]
The correspondence containing Cantor's non-constructive reasoning was published in 1937. By then, other mathematicians had rediscovered its non-constructive proof. As early as 1921, this proof was attributed to Cantor and criticized for not producing any transcendental numbers.&lt;ref&gt;{{harvnb|Gray|1994|pp=827&amp;ndash;828}}.&lt;/ref&gt;  In that year, [[Oskar Perron]] stated: "… Cantor's proof for the existence of transcendental numbers has, along with its simplicity and elegance, the great disadvantage that it is only an existence proof; it does not enable us to actually specify even a single transcendental number."&lt;ref&gt;{{harvnb|Perron|1921|p=162}}. English translation: {{harvnb|Gray|1994|p=828}}.&lt;/ref&gt;

[[File:Adolf Abraham Halevi Fraenkel.jpg|thumb|upright=0.93|Abraham Fraenkel]]
Some mathematicians have attempted to correct this misunderstanding of Cantor's work. In 1930, the [[set theory|set theorist]] [[Abraham Fraenkel]] stated that Cantor's method is "… a method that incidentally, contrary to a widespread interpretation, is fundamentally constructive and not merely existential."&lt;ref&gt;{{harvnb|Fraenkel|1930|p=237}}. English translation: {{harvnb|Gray|1994|p=823}}.&lt;/ref&gt; In 1972, [[Irving Kaplansky]] wrote: "It is often said that Cantor's proof is not 'constructive,' and so does not yield a tangible transcendental number. This remark is not justified. If we set up a definite listing of all algebraic numbers … and then apply the [[Cantor's diagonal argument|diagonal procedure]] …, we get a perfectly definite transcendental number (it could be computed to any number of decimal places)."&lt;ref&gt;{{harvnb|Kaplansky|1972|p=25}}.&lt;/ref&gt;

Cantor's diagonal argument has often replaced his 1874 construction in expositions of his proof. The diagonal argument is constructive and produces a more efficient computer program than his 1874 construction. Using it, a computer program has been written that computes the digits of a transcendental number in [[polynomial time]]. The program that uses Cantor's 1874 construction requires at least [[sub-exponential time]].{{efn-ua|The program using the diagonal method produces &lt;math&gt;n&lt;/math&gt; digits in [[Big O notation#Use in computer science|&lt;math&gt;{\color{Blue}O}(n^2 \log^2 n \log \log n)&lt;/math&gt;]] steps, while the program using the 1874 method requires at least &lt;math&gt;O(2^{\sqrt[3]{n}})&lt;/math&gt; steps to produce &lt;math&gt;n&lt;/math&gt; digits. ({{harvnb|Gray|1994|pp=822&amp;ndash;823}}.)}}

The disagreement about Cantor's proof occurs because two groups of mathematicians are talking about different proofs: the constructive one that Cantor published and the non-constructive one that was later rediscovered.  The opinion that Cantor's proof is non-constructive appears in some books that were quite successful as measured by the length of time new editions or reprints appeared—for example: [[Eric Temple Bell|Eric Temple Bell's]] ''[[Men of Mathematics]]'' (1937; still being reprinted), [[Godfrey Hardy]] and [[E. M. Wright|E. M. Wright's]] ''An Introduction to the [[Theory of numbers|Theory of Numbers]]'' (1938; 2008 6th edition), [[Garrett Birkhoff]] and [[Saunders Mac Lane|Saunders Mac Lane's]] ''A Survey of [[Modern Algebra]]'' (1941; 1997 5th edition), and [[Michael Spivak|Michael Spivak's]] ''[[Calculus]]'' (1967; 2008 4th edition).&lt;ref&gt;{{harvnb|Bell|1937|pp=568&amp;ndash;569}}; {{harvnb|Hardy|Wright|1938|p=159}} (6th ed., pp. 205&amp;ndash;206); {{harvnb|Birkhoff|Mac Lane|1941|p=392}}, (5th ed., pp. 436&amp;ndash;437); {{harvnb|Spivak|1967|pp=369&amp;ndash;370}} (4th ed., pp. 448&amp;ndash;449).&lt;/ref&gt; Since these books view Cantor's proof as non-constructive, they do not mention his constructive proof. On the other hand, the quotations above from Fraenkel and Kaplansky show that they knew Cantor's work can be used non-constructively. The disagreement about Cantor's proof shows no sign of being resolved: since 2014, at least two books have appeared stating that Cantor's proof is constructive, and at least four have appeared stating that his proof does not construct any (or a single) transcendental.&lt;ref&gt;Proof is constructive: {{harvnb|Dasgupta|2014|p=107}}; {{harvnb|Sheppard|2014|pp=131&amp;ndash;132}}. Proof is non-constructive: {{harvnb|Jarvis|2014|p=18}}; {{harvnb|Chowdhary|2015|p=19}}; {{harvnb|Stewart|2015|p=285}}; {{harvnb|Stewart|Tall|2015|p=333}}.&lt;/ref&gt;

Asserting that Cantor gave a non-constructive proof can lead to erroneous statements about the [[history of mathematics]]. In ''A Survey of Modern Algebra,'' Birkhoff and Mac Lane state: "Cantor's argument for this result [Not every real number is algebraic] was at first rejected by many mathematicians, since it did not exhibit any specific transcendental number." Birkhoff and Mac Lane are talking about the non-constructive proof.&lt;ref&gt;{{harvnb|Birkhoff|Mac Lane|1941|p=392}}, (5th ed., pp. 436&amp;ndash;437).&lt;/ref&gt; Cantor's proof produces transcendental numbers, and there appears to be no evidence that his argument was rejected.&lt;ref&gt;{{harvnb|Gray|1994|p=828}}.&lt;/ref&gt; Even [[Leopold Kronecker]], who had strict views on what is acceptable in mathematics and who could have delayed publication of Cantor's article, did not delay it.&lt;ref&gt;{{harvnb|Edwards|1989}}; {{harvnb|Gray|1994|p=828}}.&lt;/ref&gt; In fact, applying Cantor's construction to the sequence of real algebraic numbers produces a limiting process that Kronecker accepted—namely, it determines a number to any required degree of accuracy.&lt;ref&gt;{{harvnb|Edwards|1989|pp=74&amp;ndash;75}}.&lt;/ref&gt;{{efn-ua|Kronecker's opinion was: "Definitions must contain the means of reaching a decision in a finite number of steps, and existence proofs must be conducted so that the quantity in question can be calculated with any required degree of accuracy."&lt;ref&gt;{{harvnb|Burton|1995|p=595}}.&lt;/ref&gt; So Kronecker would accept Cantor's argument as a valid existence proof, but he would not accept its conclusion that transcendental numbers exist. For Kronecker, they do not exist because their definition contains no means for deciding in a finite number of steps whether or not a given number is transcendental.&lt;ref&gt;{{harvnb|Dauben|1979|p=69}}.&lt;/ref&gt; To prove that Cantor's construction calculates numbers to any required degree of accuracy, we need to prove: Given a ''k'', an ''n'' can be computed such that 
{{nowrap|''b''&lt;sub&gt;''n''&lt;/sub&gt; – ''a&lt;sub&gt;n&lt;/sub&gt;'' ≤ {{sfrac|1|''k''}}}} where (''a&lt;sub&gt;n&lt;/sub&gt;'',&amp;nbsp;''b&lt;sub&gt;n&lt;/sub&gt;'') is the {{nowrap|''n''-th}} interval of Cantor's construction. An example of how to prove this is given in {{harvnb|Gray|1994|p=822}}. Cantor's diagonal argument provides an accuracy of 10&lt;sup&gt;−''n''&lt;/sup&gt; after ''n'' real algebraic numbers have been calculated because each of these numbers generates one digit of the transcendental number.&lt;ref&gt;{{harvnb|Gray|1994|p=824}}.&lt;/ref&gt;}}

== The influence of Weierstrass and Kronecker on Cantor's article ==
[[File:Karl Weierstrass.jpg|thumb|upright=0.93|Karl Weierstrass]]
[[File:Leopold Kronecker 1865.jpg|thumb|upright=0.93|Leopold Kronecker, 1865]]
Historians of mathematics have discovered the following facts about Cantor's article "On a Property of the Collection of All Real Algebraic Numbers":

* Cantor's uncountability theorem was left out of the article he submitted. He added it during [[Galley proof|proofreading]].&lt;ref name=Ferreiros184&gt;{{harvnb|Ferreirós|2007|p=184}}.&lt;/ref&gt;
* The article's title refers to the set of real algebraic numbers. The main topic in Cantor's correspondence was the set of real numbers.&lt;ref&gt;{{harvnb|Noether|Cavaillès|1937|pp=12&amp;ndash;16}}. English translation: {{harvnb|Ewald|1996|pp=843&amp;ndash;846}}.&lt;/ref&gt;
* The proof of Cantor's second theorem came from Dedekind. However, it omits Dedekind's explanation of why the limits ''a''&lt;sub&gt;∞&lt;/sub&gt; and ''b''&lt;sub&gt;∞&lt;/sub&gt; exist.&lt;ref&gt;{{harvnb|Dauben|1979|p=67}}.&lt;/ref&gt;
* Cantor restricted his first theorem to the set of real algebraic numbers. The proof he was using demonstrates the countability of the set of all algebraic numbers.&lt;ref name=Noether18 /&gt;

To explain these facts, historians have pointed to the influence of Cantor's former professors, [[Karl Weierstrass]] and Leopold Kronecker. Cantor discussed his results with Weierstrass on December 23, 1873.&lt;ref name=Noether16_17&gt;{{harvnb|Noether|Cavaillès|1937|pp=16&amp;ndash;17}}. English translation: {{harvnb|Ewald|1996|p=847}}.&lt;/ref&gt; Weierstrass was first amazed by the concept of countability, but then found the countability of the set of real algebraic numbers useful.&lt;ref&gt;{{harvnb|Grattan-Guinness|1971|p=124}}.&lt;/ref&gt; Cantor did not want to publish yet, but Weierstrass felt that he must publish at least his results concerning the algebraic numbers.&lt;ref name=Noether16_17 /&gt;

From his correspondence, it appears that Cantor only discussed his article with Weierstrass. However, Cantor told Dedekind: "The restriction which I have imposed on the published version of my investigations is caused in part by local circumstances …"&lt;ref name=Noether16_17 /&gt; Cantor biographer [[Joseph Dauben]] believes that "local circumstances" refers to Kronecker who, as a member of the editorial board of ''[[Crelle's Journal]]'', had delayed publication of an 1870 article by [[Eduard Heine]], one of Cantor's colleagues. Cantor would submit his article to ''Crelle's Journal''.&lt;ref&gt;{{harvnb|Dauben|1979|pp=67, 308&amp;ndash;309}}.&lt;/ref&gt;

Weierstrass advised Cantor to leave his uncountability theorem out of the article he submitted, but Weierstrass also told Cantor that he could add it as a marginal note during proofreading, which he did.&lt;ref name=Ferreiros184 /&gt; It appears in a remark at the end of the article's introduction.&lt;ref&gt;See "[[#The article|The article]]" section. Also: {{harvnb|Cantor|1874|p=259}}; English translation: {{harvnb|Ewald|1996|p=841}}.&lt;/ref&gt; The opinions of Kronecker and Weierstrass both played a role here. Kronecker did not accept infinite sets, and it seems that Weierstrass did not accept that two infinite sets could be so different, with one being countable and the other not.&lt;ref&gt;{{harvnb|Ferreirós|2007|pp=184&amp;ndash;185, 245}}.&lt;/ref&gt; Weierstrass changed his opinion later.&lt;ref&gt;"It is unclear when his attitude changed, but there is evidence that by the mid-1880s he was accepting the conclusion that infinite sets are of different powers [cardinalities]." {{harv|Ferreirós|2007|p=185.}}&lt;/ref&gt; Without the uncountability theorem, the article needed a title that did not refer to this theorem. Cantor chose ''Ueber eine Eigenschaft des Inbegriffes aller reellen algebraischen Zahlen''&lt;!-- Spelled "Ueber", not "Über", in the title, despite the standard use of umlauts in the text of the article. --&gt; ("On a Property of the Collection of All Real Algebraic Numbers"), which refers to the countability of the set of real algebraic numbers, the result that Weierstrass found useful.&lt;ref&gt;{{harvnb|Ferreirós|2007|p=177}}.&lt;/ref&gt;

Kronecker's influence appears in the proof of Cantor's second theorem. Cantor used Dedekind's version of the proof except he left out why the limits ''a''&lt;sub&gt;∞&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;lim&lt;sub&gt;''n''&amp;nbsp;→&amp;nbsp;∞&lt;/sub&gt;&amp;nbsp;''a&lt;sub&gt;n&lt;/sub&gt;'' and 
''b''&lt;sub&gt;∞&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;lim&lt;sub&gt;''n''&amp;nbsp;→&amp;nbsp;∞&lt;/sub&gt;&amp;nbsp;''b&lt;sub&gt;n&lt;/sub&gt;'' exist. Dedekind had used his "principle of continuity" to prove they exist. This principle (which is equivalent to the [[least upper bound property]] of the real numbers) comes from Dedekind's construction of the real numbers, a construction Kronecker did not accept.&lt;ref&gt;{{harvnb|Dauben|1979|pp=67&amp;ndash;68}}.&lt;/ref&gt;

Cantor restricted his first theorem to the set of real algebraic numbers even though Dedekind had sent him a proof that handled all algebraic numbers.&lt;ref name=Noether18 /&gt; Cantor did this for expository reasons and because of "local circumstances."&lt;ref&gt;{{harvnb|Ferreirós|2007|p=183}}.&lt;/ref&gt; This restriction simplifies the article because the second theorem works with real sequences. Hence, the construction in the second theorem can be applied directly to the enumeration of the real algebraic numbers to produce "an effective procedure for the calculation of transcendental numbers." This procedure would be acceptable to Weierstrass.&lt;ref&gt;{{harvnb|Ferreirós|2007|p=185}}.&lt;/ref&gt;

==Dedekind's contributions to Cantor's article==
[[File:Dedekind.jpeg|thumb|upright=0.93|Richard Dedekind, c. 1870.]]

Since 1856, Dedekind had developed theories involving infinitely many infinite sets—for example: [[Ideal (ring theory)|ideal]]s, which he used in [[algebraic number theory]], and [[Dedekind cut]]s, which he used to construct the real numbers. This work enabled him to understand and contribute to Cantor's work.&lt;ref&gt;{{harvnb|Ferreirós|2007|pp=109&amp;ndash;111, 172&amp;ndash;174.}}&lt;/ref&gt;

Dedekind's first contribution concerns the theorem that the set of real algebraic numbers is countable. Cantor is usually given credit for this theorem, but the mathematical historian José Ferreirós calls it "Dedekind's theorem."&lt;ref&gt;{{harvnb|Ferreirós|1993|p=350}}.&lt;/ref&gt; Their correspondence reveals what each mathematician contributed to the theorem.

In his letter introducing the concept of countability, Cantor stated without proof that the set of positive rational numbers is countable, as are sets of the form (''a''&lt;sub&gt;''n''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''n''&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;…,&amp;nbsp;''n''&lt;sub&gt;''ν''&lt;/sub&gt;&lt;/sub&gt;) where ''n''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''n''&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;…,&amp;nbsp;''n&lt;sub&gt;ν''&lt;/sub&gt;, and ''ν'' are positive integers.&lt;ref&gt;{{harvnb|Noether|Cavaillès|1937|pp=12&amp;ndash;13}}. English translation: {{harvnb|Ewald|1996|p=844}}.&lt;/ref&gt; Cantor's second result uses indexed numbers: a set of the form (''a''&lt;sub&gt;''n''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''n''&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;…,&amp;nbsp;''n''&lt;sub&gt;''ν''&lt;/sub&gt;&lt;/sub&gt;) is the range of a function from the ''ν'' indices to the set of real numbers. His second result implies his first: let ''ν''&amp;nbsp;=&amp;nbsp;2 and ''a''&lt;sub&gt;''n''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''n''&lt;sub&gt;2&lt;/sub&gt;&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;{{sfrac|''n''&lt;sub&gt;1&lt;/sub&gt;|''n''&lt;sub&gt;2&lt;/sub&gt;}}. The function can be quite general—for example, ''a''&lt;sub&gt;''n''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''n''&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;''n''&lt;sub&gt;3&lt;/sub&gt;,&amp;nbsp;''n''&lt;sub&gt;4&lt;/sub&gt;,&amp;nbsp;''n''&lt;sub&gt;5&lt;/sub&gt;&lt;/sub&gt;&amp;nbsp;= {{nowrap|({{sfrac|''n''&lt;sub&gt;1&lt;/sub&gt;|''n''&lt;sub&gt;2&lt;/sub&gt;}})}}&lt;sup&gt;{{nowrap|{{sfrac|1|''n''&lt;sub&gt;3&lt;/sub&gt;}}&lt;/sup&gt;}}&amp;nbsp;+&amp;nbsp;{{nowrap|tan({{sfrac|''n''&lt;sub&gt;4&lt;/sub&gt;|''n''&lt;sub&gt;5&lt;/sub&gt;}}).}}

Dedekind replied with a proof of the theorem that the set of all algebraic numbers is countable.&lt;ref name=Noether18 /&gt; To obtain this result from Cantor's theorem about indexed numbers, Dedekind had to remove the restriction to positive integer indices and realize that the ordering produced can order the polynomials that have integer coefficients.

In his reply, Cantor did not claim to have proved Dedekind's result. He did indicate how he proved his theorem about indexed numbers: "Your proof that (''n'') [the set of positive integers] can be correlated one-to-one with the field of all algebraic numbers is approximately the same as the way I prove my contention in the last letter. I take ''n''&lt;sub&gt;1&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''n''&lt;sub&gt;2&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;···&amp;nbsp;+&amp;nbsp;''n''&lt;sub&gt;''ν''&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;&lt;math&gt;\mathfrak{N}&lt;/math&gt; and order the elements accordingly."&lt;ref&gt;{{harvnb|Noether|Cavaillès|1937|p=13}}. English translation: {{harvnb|Ewald|1996|p=845}}.&lt;/ref&gt; Cantor's ordering cannot handle indices that are 0.&lt;ref&gt;{{harvnb|Ferreirós|2007|p=179}}.&lt;/ref&gt;

Dedekind's second contribution is his proof of Cantor's second theorem. Dedekind sent Cantor this proof in reply to Cantor's letter that announced the uncountability theorem and proved it using infinitely many sequences. Before Dedekind's proof arrived, Cantor wrote that he had found a simpler proof that did not use infinitely many sequences.&lt;ref&gt;{{harvnb|Noether|Cavaillès|1937|pp=14&amp;ndash;16, 19}}. English translation: {{harvnb|Ewald|1996|pp=845&amp;ndash;847, 849}}.&lt;/ref&gt; So Cantor had a choice of proofs and chose to publish Dedekind's.

Cantor thanked Dedekind privately for his help: "… your comments (which I value highly) and your manner of putting some of the points were of great assistance to me."&lt;ref name=Noether16_17 /&gt; However, he did not mention Dedekind's help in his article. In previous articles, he had acknowledged help received from Kronecker, Weierstrass, Heine, and [[Hermann Schwarz]]. Cantor's failure to mention Dedekind's contributions damaged his relationship with Dedekind. Dedekind stopped replying to his letters and did not resume the correspondence until October 1876.&lt;ref&gt;{{harvnb|Ferreirós|1993|pp=349&amp;ndash;350}}; {{harvnb|Ferreirós|2007|pp=185&amp;ndash;186}}.&lt;/ref&gt;

==The legacy of Cantor's article==
Cantor's article introduced the uncountability theorem and the concept of countability. Both would lead to significant developments in mathematics. The uncountability theorem demonstrated that one-to-one correspondences can be used to analyze infinite sets. In 1878, Cantor used them to define and compare cardinalities. He also constructed one-to-one correspondences to prove that the [[n-dimensional Euclidean space|''n''-dimensional spaces]] '''R'''&lt;sup&gt;''n''&lt;/sup&gt; (where '''R''' is the set of real numbers) and the set of irrational numbers have the same cardinality as '''R'''.&lt;ref&gt;{{harvnb|Cantor|1878|pp=245&amp;ndash;254}}.&lt;/ref&gt;{{efn-ua|Cantor's method of constructing a one-to-one correspondence between the set of irrational numbers and '''R''' can be used to construct one between the set of transcendental numbers and '''R'''.&lt;ref&gt;{{harvnb|Cantor|1879|p=4}}.&lt;/ref&gt; The construction begins with the set of transcendental numbers ''T'' and removes a countable [[subset]] {''t&lt;sub&gt;n&lt;/sub&gt;''} (for example, {{nowrap|''t&lt;sub&gt;n&lt;/sub&gt;'' {{=}} {{sfrac|''[[e (mathematical constant)|e]]''|''n''}}}}). Let this set be ''T''&lt;sub&gt;0&lt;/sub&gt;. Then ''T''&amp;nbsp;&lt;nowiki&gt;=&lt;/nowiki&gt; &amp;nbsp;''T''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;∪&amp;nbsp;{''t&lt;sub&gt;n&lt;/sub&gt;''}&amp;nbsp;&lt;nowiki&gt;=&lt;/nowiki&gt; ''T''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;∪&amp;nbsp;{''t''&lt;sub&gt;{{nowrap|2''n'' – 1}}&lt;/sub&gt;}&amp;nbsp;∪&amp;nbsp;{''t''&lt;sub&gt;2''n''&lt;/sub&gt;}, and '''R'''&amp;nbsp;&lt;nowiki&gt;=&lt;/nowiki&gt;&amp;nbsp;''T''&amp;nbsp;∪&amp;nbsp;{''a''&lt;sub&gt;''n''&lt;/sub&gt;}&amp;nbsp;&lt;nowiki&gt;=&lt;/nowiki&gt; ''T''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;∪&amp;nbsp;{''t''&lt;sub&gt;''n''&lt;/sub&gt;}&amp;nbsp;∪&amp;nbsp;{''a''&lt;sub&gt;''n''&lt;/sub&gt;} where ''a''&lt;sub&gt;''n''&lt;/sub&gt; is the sequence of real algebraic numbers. So both ''T'' and '''R''' are the union of three pairwise disjoint sets: ''T''&lt;sub&gt;0&lt;/sub&gt; and two countable sets. A one-to-one correspondence between ''T'' and '''R''' is given by the function: {{nowrap|''g''(''t'') {{=}} ''t''}} if ''t''&amp;nbsp;∈&amp;nbsp;''T''&lt;sub&gt;0&lt;/sub&gt;, {{nowrap|''g''(''t''&lt;sub&gt;2''n'' – 1&lt;/sub&gt;) {{=}} ''t&lt;sub&gt;n&lt;/sub&gt;''}}, and {{nowrap|''g''(''t''&lt;sub&gt;2''n''&lt;/sub&gt;)&amp;nbsp; {{=}} ''a&lt;sub&gt;n&lt;/sub&gt;''.}}}}

In 1883, Cantor extended the [[natural numbers]] with his infinite [[ordinal number|ordinals]]. This extension was necessary for his work on the [[Cantor–Bendixson theorem]]. Cantor discovered other uses for the ordinals—for example, he used sets of ordinals to produce an infinity of sets having different infinite cardinalities.&lt;ref&gt;{{harvnb|Ferreirós|2007|pp=267&amp;ndash;273}}.&lt;/ref&gt; His work on infinite sets together with Dedekind's set-theoretical work created set theory.&lt;ref&gt;{{harvnb|Ferreirós|2007|pp=xvi, 320&amp;ndash;321, 324}}.&lt;/ref&gt;

The concept of countability led to countable operations and objects that are used in various areas of mathematics. For example, in 1878, Cantor introduced countable [[Union (set theory)|union]]s of sets.&lt;ref&gt;{{harvnb|Cantor|1878|p=243}}.&lt;/ref&gt; In the 1890s, [[Émile Borel]] used countable unions in his [[Borel measure|theory of measure]], and [[René-Louis Baire|René Baire]] used countable ordinals to define his [[Baire function|classes of functions]].&lt;ref&gt;{{harvnb|Hawkins|1970|pp=103&amp;ndash;106, 127}}.&lt;/ref&gt; Building on the work of Borel and Baire, [[Henri Lebesgue]] created his theories of [[Lebesgue measure|measure]] and [[Lebesgue integration|integration]], which were published from 1899 to 1901.&lt;ref&gt;{{harvnb|Hawkins|1970|pp=118, 120&amp;ndash;124, 127}}.&lt;/ref&gt;

Countable [[Model theory|models]] are used in set theory. In 1922, [[Thoralf Skolem]] proved that if conventional [[Set theory#Axiomatic set theory|axioms of set theory]] are [[consistent]], then they have a countable model. Since this model is countable, its set of real numbers is countable. This consequence is called [[Skolem's paradox]], and Skolem explained why it does not contradict Cantor's uncountability theorem: although there is a one-to-one correspondence between this set and the set of positive integers, no such one-to-one correspondence is a member of the model. Thus the model considers its set of real numbers to be uncountable, or more precisely, the [[first-order logic|first-order sentence]] that says the set of real numbers is uncountable is true within the model.&lt;ref&gt;{{harvnb|Ferreirós|2007|pp=362&amp;ndash;363}}.&lt;/ref&gt; In 1963, [[Paul Cohen]] used countable models to prove his [[Independence (mathematical logic)|independence]] theorems.&lt;ref&gt;{{harvnb|Cohen|1963|pp=1143&amp;ndash;1144}}.&lt;/ref&gt;

==See also==
*[[Cantor's theorem]]

==Notes==
{{notelist-ua}}

==Note: Cantor's 1879 proof==
{{reflist|group=proof}}

==References==
{{reflist|2}}

==Bibliography==
* {{Citation | ref=harv
  | last1 = Arkhangel'skii
  | first1 = A. V.
  | last2 = Fedorchuk
  | first2 = V. V.
  | contribution=The basic concepts and constructions of general topology
  | editor-last1 = Arkhangel'skii
  | editor-first1=  A. V.
  | editor-last2 = Pontryagin
  | editor-first2 = L. S. (eds.)
  | title = General Topology I
  | pages = 1–90
  | publisher = Springer-Verlag
  | year = 1990
  | isbn = 978-0-387-18178-3}}.
* {{Citation | ref=harv
  | first = Michèle
  | last = Audin
  | title = Remembering Sofya Kovalevskaya
  | publisher = Springer
  | year = 2011
  | isbn = 978-0-85729-928-4}}.
* {{Citation | ref=harv
  | first = Eric Temple
  | last = Bell
  | title = Men of Mathematics
  | publisher = Simon &amp; Schuster
  | year = 1937
  | isbn = 0-671-62818-6}}.
* {{Citation | ref=harv
  | first1 = Garrett 
  | last1 = Birkhoff
  | first2 = Saunders
  | last2 = Mac Lane
  | title = A Survey of Modern Algebra
  | publisher = Macmillan
  | year = 1941
  | isbn = 978-1-56881-068-3}}.
* {{Citation | ref=harv
  | last = Burton
  | first = David M.
  | title = Burton's History of Mathematics
  | publisher = William C. Brown
  | year = 1995
  | edition = 3rd 
  | isbn = 0-697-16089-0}}.
* {{Citation | ref=harv
  | first = Georg
  | last = Cantor
  | title = Ueber eine Eigenschaft des Inbegriffes aller reellen algebraischen Zahlen
  | url = http://www.digizeitschriften.de/main/dms/img/?PPN=GDZPPN002155583
  | volume = 77
  | pages = 258–262
  | journal = [[Journal für die Reine und Angewandte Mathematik]]
  | year = 1874
  | doi=10.1515/crll.1874.77.258}}.
* {{Citation | ref=harv
  | first = Georg
  | last = Cantor
  | title = Ein Beitrag zur Mannigfaltigkeitslehre
  | url = http://www.digizeitschriften.de/dms/img/?PID=GDZPPN002156806
  | volume = 84
  | pages = 242–258
  | journal = Journal für die Reine und Angewandte Mathematik
  | year = 1878}}.
* {{Citation | ref=harv 
  | first = Georg
  | last = Cantor
  | title = Ueber unendliche, lineare Punktmannichfaltigkeiten. 1.
  | url = http://www.digizeitschriften.de/dms/img/?PID=GDZPPN002244853
  | volume = 15
  | pages = 1—7 
  | journal = [[Mathematische Annalen]]
  | year = 1879
  | doi=10.1007/bf01444101}}.
* {{Citation | ref=harv
  | last = Chowdhary
  | first = K. R.
  | title = Fundamentals of Discrete Mathematical Structures
  | publisher = PHI Learning
  | year = 2015
  | edition = 3rd 
  | isbn = 978-81-203-5074-8}}.
* {{Citation | ref=harv
  | first = Paul J.
  | last = Cohen 
  | year = 1963
  | title = The Independence of the Continuum Hypothesis
  | journal = [[Proceedings of the National Academy of Sciences of the United States of America]]
  | volume = 50
  | pages = 1143–1148
  | doi = 10.1073/pnas.50.6.1143
  | pmid=16578557
  | pmc=221287}}.
* {{Citation | ref=harv
  | last = Dasgupta
  | first = Abhijit
  | title = Set Theory: With an Introduction to Real Point Sets
  | publisher = Springer
  | year = 2014 
  | isbn = 978-1-4614-8853-8}}.
* {{Citation | ref=harv
  | last = Dauben
  | first = Joseph
  | title = Georg Cantor: His Mathematics and Philosophy of the Infinite
  | publisher = Harvard University Press
  | year = 1979
  | isbn = 0-674-34871-0}}.
* {{Citation | ref=harv
  | last = Dauben
  | first = Joseph
  | title = Georg Cantor and the Battle for Transfinite Set Theory
  | url = http://acmsonline.org/home2/wp-content/uploads/2016/05/Dauben-Cantor.pdf
  | journal = 9th ACMS Conference Proceedings
  | year = 1993}}.
* {{Citation | ref=harv
  | last = Edwards
  | first = Harold M.
  | authorlink = Harold Edwards (mathematician)
  | chapter = Kronecker's Views on the [[Foundations of Mathematics]]
  | editor-last1 = Rowe 
  | editor-first1= David E.
  | editor-last2 = McCleary
  | editor-first2 = John (eds.)
  | title = The History of Modern Mathematics, Volume 1
  | pages = 67–77
  | publisher = Academic Press
  | year = 1989
  | isbn = 0-12-599662-4}}. 
* {{Citation | ref=harv
  | last = Ewald
  | first = William B. (ed.)
  | title = From [[Immanuel Kant]] to [[David Hilbert]]: A Source Book in the Foundations of Mathematics, Volume 2
  | publisher = Oxford University Press
  | year = 1996
  | isbn = 0-19-850536-1}}.
* {{Citation | ref=harv
  | last = Ferreirós
  | first = José 
  | title = On the relations between Georg Cantor and Richard Dedekind
  | url = http://www.sciencedirect.com/science/article/pii/S031508608371030X
  | journal = [[Historia Mathematica]]
  | volume = 20
  | pages = 343–363
  | year = 1993
  | doi = 10.1006/hmat.1993.1030}}.
* {{Citation | ref=harv
  | last = Ferreirós
  | first = José 
  | title = Labyrinth of Thought: A History of Set Theory and Its Role in Mathematical Thought
  | publisher = Birkhäuser
  | year = 2007
  | edition = 2nd revised
  | isbn = 3-7643-8349-6}}.
* {{Citation | ref=harv
  | first = Abraham 
  | last = Fraenkel
  | title = Georg Cantor
  | url = http://www.digizeitschriften.de/main/dms/img/?PPN=PPN37721857X_0039&amp;DMDID=dmdlog27
  | journal = Jahresbericht der Deutschen Mathematiker-Vereinigung
  | volume = 39
  | pages = 189–266
  | year = 1930}}.
* {{Citation | ref=harv
  | first = Ivor
  | last = Grattan-Guinness
  | authorlink = Ivor Grattan-Guinness
  | title = The Correspondence between Georg Cantor and Philip Jourdain
  | url = http://www.digizeitschriften.de/main/dms/img/?PPN=GDZPPN002136651
  | journal = Jahresbericht der Deutschen Mathematiker-Vereinigung
  | volume = 73
  | pages = 111–130
  | year = 1971}}.
* {{Citation | ref=harv
  | first = Robert 
  | last = Gray
  | title = Georg Cantor and Transcendental Numbers
  | url = http://www.maa.org/sites/default/files/pdf/upload_library/22/Ford/Gray819-832.pdf
  | journal = [[American Mathematical Monthly]]
  | volume = 101
  | pages = 819–832
  | year = 1994
  | doi=10.2307/2975129
  | mr=1300488
  | zbl=0827.01004}}.
* {{Citation | ref=harv
  | last1 = Hardy 
  | first1 = Godfrey 
  | last2 = Wright 
  | first2 = E. M. 
  | title = An Introduction to the Theory of Numbers
  | publisher = Clarendon Press
  | year = 1938
  | isbn = 978-0-19-921985-8}}.
* {{Citation | ref=harv
  | last = Hawkins
  | first = Thomas
  | authorlink = Thomas W. Hawkins, Jr.
  | title = Lebesgue's Theory of Integration
  | publisher = University of Wisconsin Press
  | year = 1970
  | isbn = 0-299-05550-7}}.
* {{Citation | ref=harv
  | last = Jarvis
  | first = Frazer 
  | title = Algebraic Number Theory
  | publisher = Springer
  | year = 2014
  | isbn = 978-3-319-07544-0}}.
* {{Citation| ref=harv
  | authorlink=Akihiro Kanamori 
  | last=Kanamori 
  | first=Akihiro  
  | chapter=Set Theory from Cantor to Cohen  
  | chapter-url=http://math.bu.edu/people/aki/16.pdf 
  | editor-first=Dov M. 
  | editor-last=Gabbay 
  | editor2-first=Akihiro 
  | editor2-last=Kanamori 
  | editor3-first=John H. 
  | editor3-last=Woods 
  | title=Sets and Extensions in the Twentieth Century
  | publisher=Cambridge University Press 
  | publication-date=2012 
  | pages=1—71 
  | isbn=978-0-444-51621-3}}.
* {{Citation | ref=harv
  | last = Kaplansky
  | first = Irving
  | title = Set Theory and Metric Spaces
  | publisher = Allyn and Bacon
  | year = 1972
  | isbn = 0-8284-0298-1}}.
* {{Citation | ref=harv
  | last = Kelley
  | first = John L.
  | authorlink = John L. Kelley
  | title = General Topology
  | publisher=Springer
  | year = 1991 
  | isbn = 3-540-90125-6}}. 
* {{Citation | ref=harv
  | last = LeVeque 
  | first = William J.
  | authorlink = William J. LeVeque 
  | title = Topics in Number Theory, Volume I
  | publisher = Addison-Welsey
  | year = 1956
  | isbn = 978-0-486-42539-9}}. (Reprinted by Dover Publications, 2002.)
* {{Citation | ref=harv
  | last = Noether
  | first = Emmy 
  | authorlink = Emmy Noether
  | last2 = Cavaillès
  | first2 = Jean (eds.)
  | authorlink2 = Jean Cavaillès
  | title = Briefwechsel Cantor-Dedekind
  | publisher = Hermann
  | year = 1937}}.
* {{Citation | ref=harv
  | last = Perron
  | first = Oskar
  | title = Irrationalzahlen
  | url = https://archive.org/stream/irrationalzahlen00perruoft#page/n5/mode/2up
  | publisher = de Gruyter
  | year = 1921}}.
* {{Citation | ref=harv
  | last = Sheppard 
  | first = Barnaby 
  | title = The Logic of Infinity
  | publisher = Cambridge University Press
  | year = 2014
  | isbn = 978-1-107-67866-8}}.
* {{Citation | ref=harv
  | last = Spivak
  | first = Michael 
  | title = Calculus
  | url = https://archive.org/details/Calculus_643
  | publisher = W. A. Benjamin 
  | year = 1967
  | isbn = 978-0914098911}}.
* {{Citation | ref=harv
  | last = Stewart 
  | first = Ian 
  | title = Galois Theory
  | authorlink = Ian Stewart (mathematician)
  | publisher = CRC Press
  | year = 2015
  | edition = 4th
  | isbn = 978-1-4822-4582-0}}.
* {{Citation | ref=harv
  | last1 = Stewart
  | first1 = Ian 
  | last2 = Tall 
  | first2 = David 
  | authorlink2 = David Tall
  | title = The Foundations of Mathematics
  | publisher = Oxford University Press
  | year = 2015
  | edition = 2nd
  | isbn = 978-0-19-870644-1}}.
* {{Citation | ref=harv
  | contribution = Continued Fraction
  | title = CRC Concise Encyclopedia of Mathematics
  | editor-last = Weisstein
  | editor-first = Eric W.
  | editor-link = Eric W. Weisstein
  | publisher = Chapman &amp; Hall/CRC
  | year = 2003
  | isbn = 1-58488-347-2}}.

[[Category:History of mathematics]]
[[Category:Set theory]]
[[Category:Real analysis]]
[[Category:Georg Cantor]]</text>
      <sha1>s1npnech7x1chew39pv0ittks4sb95e</sha1>
    </revision>
  </page>
  <page>
    <title>HAS-V</title>
    <ns>0</ns>
    <id>9626709</id>
    <revision>
      <id>580827561</id>
      <parentid>529770507</parentid>
      <timestamp>2013-11-09T00:17:25Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>/* References */[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (9615)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="977">'''HAS-V''' is a [[cryptographic hash function]] with a variable output length. HAS-V is a hash function based on a [[block cipher]]. The hash function can produce hash values with lengths from 16 to 40 [[bytes]].

==Specifications==
Digest Size: 128-320 [[bit]]s
Max message length: &lt;2&lt;sup&gt;64&lt;/sup&gt; bits
Compression Function: 1024-bit message block, 320-bit chaining variable

The hash function was developed by Nan Kyoung Park, Joon Ho Hwang and Pil Joong Lee,&lt;ref&gt;Designers - [http://www.users.zetnet.co.uk/hopwood/crypto/scan/md.html Standard Cryptographic Algorithm Naming.]&lt;/ref&gt; and was released in 2000.

==See also==
* [[One-way compression function]] - Describes how hash functions can be built from block ciphers.

==External links==
* [http://www.springerlink.com/index/9W1A3XCMC12FVYMG.pdf HAS-V: A New Hash Function with Variable Output Length]

==References==
{{reflist}}

{{Cryptography navbox|hash}}

[[Category:Cryptographic hash functions]]


{{crypto-stub}}</text>
      <sha1>bcwxe5p7sm2ibt6or5qbbaerqbo2cpe</sha1>
    </revision>
  </page>
  <page>
    <title>Henry Coddington</title>
    <ns>0</ns>
    <id>20383103</id>
    <revision>
      <id>783493985</id>
      <parentid>771941860</parentid>
      <timestamp>2017-06-02T17:38:36Z</timestamp>
      <contributor>
        <username>Magic links bot</username>
        <id>30707369</id>
      </contributor>
      <minor/>
      <comment>Replace [[Help:Magic links|magic links]] with templates per [[Special:Permalink/772743896#Future of magic links|local RfC]] and [[:mw:Requests for comment/Future of magic links|MediaWiki RfC]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3205">'''Henry Coddington''' (1798/9, [[Oldbridge]], [[County Meath]] &amp;mdash; 3 March 1845, [[Rome]]) was an English natural philosopher, fellow and tutor of [[Trinity College, Cambridge]] and [[Church of England]] clergyman.&lt;ref&gt;[http://www.oxforddnb.com/index/101005793/ Henry Coddington] in [[Oxford Dictionary of National Biography]]&lt;/ref&gt;

==Life==
Henry Coddington was the son of Latham Coddington, Rector of [[Timolin]], [[Kildare]]. 
Admitted to [[Trinity College, Cambridge]] in 1816, Coddingtion graduated BA as [[List of Wranglers of the University of Cambridge|Senior Wrangler]] in 1820,&lt;ref name=Venn&gt;{{acad|id=CDNN816H|name=Coddington, Henry}}&lt;/ref&gt; and first Smith's prizeman; proceeded M.A. in 1823, and obtained a fellowship and sub-tutorship in his college. He retired to the college living of Ware in Hertfordshire, and in the discharge of his clerical duties burst a blood-vessel, thereby fatally injuring his health.{{sfn|Clerke|1887}}

Coddington was vicar of [[Ware, Hertfordshire]] from 1832 to 1845.&lt;ref name=Venn/&gt;
Advised to try a southern climate, he travelled abroad, and died at [[Rome]] 3 March 1845.{{sfn|Clerke|1887}}

==Family==
He married a daughter of Dr. Batten, principal of [[Haileybury College]], and left seven children.{{sfn|Clerke|1887}}

==Legacy==
He wrote chiefly on [[optics]], in particular ''An Elementary Treatise on Optics''.&lt;ref&gt;[https://books.google.com/books?id=VpwIAAAAIAAJ&amp;dq=%22Henry+Coddington%22&amp;printsec=frontcover&amp;source=bl&amp;ots=EqxHcRL9RO&amp;sig=s2U2rnsuaaxkh9PA0bTfgk70AZM&amp;oi=book_result&amp;ct=result&amp;hl=no ''An Elementary Treatise on Optics'']&lt;/ref&gt; He also made the [[Coddington magnifier]] popular. He was elected a [[Fellow of the Royal Society]] in February, 1829.&lt;ref&gt;{{cite web | url= http://www2.royalsociety.org/DServe/dserve.exe?dsqIni=Dserve.ini&amp;dsqApp=Archive&amp;dsqCmd=Show.tcl&amp;dsqDb=Persons&amp;dsqPos=0&amp;dsqSearch=%28Surname%3D%27coddington%27%29| title = Library and Archive catalogue |publisher= Royal Society|accessdate= 23 December 2010}}&lt;/ref&gt;

==Awards==
His name occurs on the first list of members of the [[British Association]]. He was one of the earliest members of the [[Royal Astronomical Society]],&lt;ref&gt;{{Cite web|url=http://articles.adsabs.harvard.edu/cgi-bin/nph-iarticle_query?1846MNRAS...7Q..48.|title=1846MNRAS...7Q..48. Page 48|website=articles.adsabs.harvard.edu|access-date=2017-02-05}}&lt;/ref&gt; was a fellow of the [[Royal Geographical Society]] and [[Royal Society]], and sat on the council of the latter body in 1831-2.{{sfn|Clerke|1887}}

==References==
{{reflist}}
;Attribution
{{DNB|wstitle=Coddington, Henry |first=Agnes Mary|last=Clerke|volume=11|pages=202-203}}

==Sources==
* [[W. W. Rouse Ball]], ''A History of the Study of Mathematics at Cambridge University'', 1889, repr. [[Cambridge University Press]], 2009, {{ISBN|978-1-108-00207-3}}, p.&amp;nbsp;131

==External links==
*{{worldcat id|lccn-n87-106437}}

{{Authority control}}

{{DEFAULTSORT:Coddington, Henry}}
[[Category:1798 births]]
[[Category:1845 deaths]]
[[Category:English physicists]]
[[Category:Alumni of Trinity College, Cambridge]]
[[Category:Fellows of Trinity College, Cambridge]]
[[Category:Senior Wranglers]]
[[Category:Fellows of the Royal Society]]</text>
      <sha1>2ywtia94nugmgqn2g6pttfllq359a0j</sha1>
    </revision>
  </page>
  <page>
    <title>Hewitt–Savage zero–one law</title>
    <ns>0</ns>
    <id>6595367</id>
    <revision>
      <id>740848779</id>
      <parentid>738050678</parentid>
      <timestamp>2016-09-23T17:59:42Z</timestamp>
      <contributor>
        <username>Mrcanstatman</username>
        <id>29245914</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4561">The '''Hewitt–Savage zero–one law''' is a [[theorem]] in [[probability theory]], similar to [[Kolmogorov's zero–one law]] and the [[Borel–Cantelli lemma]], that specifies that a certain type of event will either [[almost surely]] happen or almost surely not happen. It is sometimes known as the '''Hewitt–Savage law for symmetric events'''. It is named after [[Edwin Hewitt]] and [[Leonard Jimmie Savage]].&lt;ref&gt;{{cite journal | last1 = Hewitt | first1 = E. | authorlink1 = Edwin Hewitt | last2=Savage | first2=L. J. | authorlink2=Leonard Jimmie Savage | title = Symmetric measures on Cartesian products | journal = Trans. Amer. Math. Soc. | volume = 80 | year = 1955 | pages = 470–501 | doi=10.1090/s0002-9947-1955-0076206-8 }}&lt;/ref&gt;

==Statement of the Hewitt–Savage zero–one law==
Let &lt;math&gt;\left\{ X_n \right\}_{n = 1}^\infty&lt;/math&gt; be a [[sequence]] of [[independent and identically-distributed random variables]] taking values in a set &lt;math&gt;\mathbb{X}&lt;/math&gt;.  The Hewitt–Savage zero–one law says that any event whose occurrence or non-occurrence is determined by the values of these random variables and whose occurrence or non-occurrence is unchanged by finite [[permutation]]s of the indices, has [[probability]] either 0 or 1 (a "finite" permutation is one that leaves all but finitely many of the indices fixed).

Somewhat more abstractly, define the ''exchangeable [[sigma algebra]]'' or ''sigma algebra of symmetric events'' &lt;math&gt;\mathcal{E}&lt;/math&gt; to be the set of events (depending on the sequence of variables &lt;math&gt;\left\{ X_n \right\}_{n = 1}^\infty&lt;/math&gt;) which are invariant under [[Wikt:finite|finite]] [[permutation]]s of the indices in the sequence &lt;math&gt;\left\{ X_n \right\}_{n = 1}^\infty&lt;/math&gt;. Then &lt;math&gt;A \in \mathcal{E} \implies \mathbb{P} (A) \in \{ 0, 1 \}&lt;/math&gt;.

Since any finite permutation can be written as a product of [[transposition (mathematics)|transposition]]s, if we wish to check whether or not an event &lt;math&gt;A&lt;/math&gt; is symmetric (lies in &lt;math&gt;\mathcal{E}&lt;/math&gt;), it is enough to check if its occurrence is unchanged by an arbitrary transposition &lt;math&gt;(i, j)&lt;/math&gt;, &lt;math&gt;i, j \in \mathbb{N}&lt;/math&gt;.

==Examples==

===Example 1===
Let the sequence &lt;math&gt;\left\{ X_n \right\}_{n = 1}^\infty&lt;/math&gt; take values in &lt;math&gt;[0, \infty)&lt;/math&gt;. Then the event that the series &lt;math&gt;\sum_{n = 1}^\infty X_n&lt;/math&gt; converges (to a finite value) is a symmetric event in &lt;math&gt;\mathcal{E}&lt;/math&gt;, since its occurrence is unchanged under transpositions (for a finite re-ordering, the convergence or divergence of the series—and, indeed, the numerical value of the sum itself—is independent of the order in which we add up the terms). Thus, the series either converges almost surely or diverges almost surely. If we assume in addition that the common [[expected value]] &lt;math&gt;\mathbb{E}[X_n] &gt; 0&lt;/math&gt; (which essentially means that &lt;math&gt;\mathbb{P}(X_n = 0 ) &lt; 1 &lt;/math&gt; because of the random variables' non-negativity), we may conclude that

:&lt;math&gt;\mathbb{P} \left( \sum_{n = 1}^\infty X_n = + \infty \right) = 1,&lt;/math&gt;

i.e. the series diverges almost surely. This is a particularly simple application of the Hewitt–Savage zero–one law. In many situations, it can be easy to apply the Hewitt–Savage zero–one law to show that some event has probability 0 or 1, but surprisingly hard to determine ''which'' of these two extreme values is the correct one.

===Example 2===
Continuing with the previous example, define

: &lt;math&gt;S_N= \sum_{n = 1}^N X_n,&lt;/math&gt;

which is the position at step ''N'' of a [[random walk]] with the [[iid]] increments ''X''&lt;sub&gt;''n''&lt;/sub&gt;. The event {&amp;nbsp;''S''&lt;sub&gt;''N''&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;0&amp;nbsp;infinitely&amp;nbsp;often&amp;nbsp;} is invariant under finite permutations. Therefore, the zero–one law is applicable and one infers that the probability of a random walk with real iid increments visiting the origin infinitely often is either one or zero.  Visiting the origin infinitely often is a tail event with respect to the sequence (''S''&lt;sub&gt;''N''&lt;/sub&gt;), but ''S''&lt;sub&gt;''N''&lt;/sub&gt; are not independent and therefore the [[Kolmogorov's zero–one law]] is not directly applicable here.&lt;ref&gt;This example is from {{cite book | last = Shiryaev | first = A.| authorlink = Albert Shiryaev | title = Probability Theory | edition = Second | publisher = Springer-Verlag | location = New York | year = 1996 |pages=381–82 }}&lt;/ref&gt;

==References==
{{Reflist}}

{{DEFAULTSORT:Hewitt-Savage zero-one law}}
[[Category:Probability theorems]]
[[Category:Covering lemmas]]</text>
      <sha1>80htzuk9rp3mfctzgbr335n1tcwvewj</sha1>
    </revision>
  </page>
  <page>
    <title>History of compiler construction</title>
    <ns>0</ns>
    <id>21310186</id>
    <revision>
      <id>871674241</id>
      <parentid>871060680</parentid>
      <timestamp>2018-12-02T18:33:55Z</timestamp>
      <contributor>
        <ip>82.2.1.89</ip>
      </contributor>
      <comment>Fix link to refer directly to the pdf</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="43107">{{Use dmy dates|date=February 2013}}
{{Hide in print|{{History of computing}}}}
In [[computing]], a [[compiler]] is a [[computer program]] that transforms [[source code]] written in a [[programming language]] or computer language (the ''source language''), into another computer language (the ''target language'', often having a binary form known as ''[[object code]]'' or ''[[machine code]]''). The most common reason for transforming source code is to create an [[executable]] program.

Any program written in a [[high-level programming language]] must be translated to object code before it can be executed, so all programmers using such a language use a compiler or an [[interpreter (computing)|interpreter]]. Thus, compilers are very important to programmers. Improvements to a compiler may lead to a large number of improved executable programs.

The [[PQCC|Production Quality Compiler-Compiler]], in the late 1970s, introduced the principles of compiler organization that are still widely used today (e.g., a front-end handling syntax and sematics and a back-end generating machine code).

== First compilers ==

Software for early computers was primarily written in [[assembly language]]. It is usually more productive for a programmer to use a high-level language, and programs written in a high-level language can be [[software reuse|reused]] [[Software portability|on different kinds of computers]]. Even so, it took a while for compilers to become established, because they generated code that did not perform as well as hand-written assembler, they were daunting development projects in their own right, and the very limited [[Computer storage|memory]] capacity of early computers created many technical problems for practical compiler implementations.

The first compiler was written by [[Corrado Böhm]], in 1951, for his [http://e-collection.library.ethz.ch/eserv/eth:32719/eth-32719-02.pdf PhD thesis].  The term ''compiler'' was coined by [[Grace Hopper]].&lt;ref name="wikles1968"&gt;[[Maurice V. Wilkes]]. 1968. Computers Then and Now. Journal of the Association for Computing Machinery, 15(1):1–7, January. p. 3 (a comment in brackets added by editor), "(I do not think that the term compiler was then [1953] in general use, although it had in fact been introduced by Grace Hopper.)"&lt;/ref&gt;&lt;ref name="computerhistory.org"&gt;[http://www.computerhistory.org/events/lectures/cobol_06121997/index.shtml] The World's First COBOL Compilers   {{webarchive |url=https://web.archive.org/web/20111013021915/http://www.computerhistory.org/events/lectures/cobol_06121997/index.shtml |date=13 October 2011 }}&lt;/ref&gt;, referring to her [[A-0 system]] which functioned as a loader or [[Linker (computing)|linker]], not the modern notion of a compiler. The [[FORTRAN]]&lt;!-- ###here (only), upper-case FORTRAN is correct, as it was the name used at the time, and on IBM's early compilers ###--&gt; team led by [[John W. Backus]] at [[IBM]] introduced the first commercially available compiler, in 1957, which took 18 person-years to create.&lt;ref&gt;Backus et al. "The FORTRAN automatic coding system", Proc. AFIPS 1957 Western Joint Computer Conf., Spartan Books, Baltimore 188–198&lt;/ref&gt;

The first [[ALGOL 58]] compiler was completed by the end of 1958 by [[Friedrich L. Bauer]], Hermann Bottenbruch, [[Heinz Rutishauser]], and [[Klaus Samelson]] for the [[Z22 (computer)|Z22]] computer. Bauer et al. had been working on compiler technology for the ''Sequentielle Formelübersetzung'' (i.e. ''sequential formula translation'') in the previous years.

By 1960, an extended Fortran compiler, ALTAC, was available on the [[Philco]] 2000, so it is probable that a Fortran program was compiled for both IBM and Philco [[computer architecture]]s in mid-1960.&lt;ref&gt;[http://portal.acm.org/citation.cfm?id=808498] Rosen, Saul. ''ALTAC, FORTRAN, and compatibility''. Proceedings of the 1961 16th ACM national meeting&lt;/ref&gt; The first known demonstrated [[cross-platform]] high-level language was [[COBOL]]. In a demonstration in December 1960, a COBOL program was compiled and executed on both the [[UNIVAC II]] and the [[RCA]] 501.&lt;ref name="computerhistory.org" /&gt;

== Self-hosting compilers ==
{{distinguish|Compiler compiler}}
Like any other software, there are benefits from implementing a compiler in a high-level language. In particular, a compiler can be [[Self-hosting|self-hosted]] – that is, written in the programming language it compiles. Building a self-hosting compiler is a [[bootstrapping (compilers)|bootstrapping]] problem, i.e. the first such compiler for a language must be either hand written machine code or compiled by a compiler written in another language, or compiled by running the compiler in an [[Interpreter (computing)|interpreter]].

=== Corrado Böhm PhD dissertation ===

Corrado Böhm developed a language, a machine, and a translation method for compiling that language on the machine in his  PhD dissertation dated 1951. He not only described a complete compiler, but also defined for the first time that compiler in its own language. The language was interesting in itself, because every statement (including input statements, output statements and control statements) was a special case of an [[Assignment (computer science)|assignment statement]].

=== NELIAC ===

The '''Navy Electronics Laboratory International [[ALGOL]] Compiler''' or [[NELIAC]] was a [[programming language dialect|dialect]] and compiler implementation of the [[ALGOL 58]] [[programming language]] developed by the [[Naval Electronics Laboratory]] in 1958.

NELIAC was the brainchild of [[Harry Huskey]] — then Chairman of the [[Association for Computing Machinery|ACM]] and a well known [[computer science|computer scientist]] (and later academic supervisor of [[Niklaus Wirth]]), and supported by Maury Halstead, the head of the computational center at NEL. The earliest version was implemented on the prototype [[AN/USQ-17|USQ-17]] computer (called the Countess) at the laboratory. It was the world's first self-compiling compiler - the compiler was first coded in simplified form in assembly language (the ''bootstrap''), then re-written in its own language and compiled by the bootstrap, and finally re-compiled by itself, making the bootstrap obsolete.

=== Lisp ===
Another early [[self-hosting]] compiler was written for [[Lisp programming language|Lisp]] by Tim Hart and Mike Levin at [[Massachusetts Institute of Technology|MIT]] in 1962.&lt;ref&gt;[ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-039.pdf T. Hart and M. Levin "The New Compiler", AIM-39] CSAIL Digital Archive – Artificial Intelligence Laboratory Series&lt;/ref&gt; They wrote a Lisp compiler in Lisp, testing it inside an existing Lisp interpreter. Once they had improved the compiler to the point where it could compile its own source code, it was self-hosting.&lt;ref name="LEVIN" /&gt;

:''The compiler as it exists on the standard compiler tape is a machine language program that was obtained by having the [[S-expression]] definition of the compiler work on itself through the interpreter.'' (AI Memo 39)&lt;ref name="LEVIN"&gt;{{cite web | title=AI Memo 39-The new compiler|author1=Tim Hart |author2=Mike Levin | url=ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-039.pdf | accessdate=2008-05-23}}&lt;/ref&gt;
This technique is only possible when an interpreter already exists for the very same language that is to be compiled. It borrows directly from the notion of running a program on itself as input, which is also used in various proofs in [[theoretical computer science]], such as the proof that the [[halting problem]] is [[Undecidable problem|undecidable]].

=== Forth ===
[[Forth (programming language)|Forth]] is an example of a self-hosting compiler. The [[Forth (programming language)#Self-compilation and cross compilation|self compilation and cross compilation]] features of Forth are commonly confused with [[metacompilation]] and [[metacompiler]]s.{{fact|date=July 2017}} Like [[Lisp (programming language)|Lisp]], Forth is an [[extensible programming]] language. It is the [[extensible programming]] language features of Forth and Lisp that enable them to generate new versions of themselves or port themselves to new environments.

== Context-free grammars and parsers ==

A [[Parsing|parser]] is an important component of a compiler. It parses the source code of a computer programming language to create some form of internal representation. Programming languages tend to be specified in terms of a [[context-free grammar]] because fast and efficient parsers can be written for them. Parsers can be written by hand or generated by a [[parser generator]]. A context-free grammar provides a simple and precise mechanism for describing how programming language constructs are built from smaller [[block (programming)|blocks]]. The formalism of context-free grammars was developed in the mid-1950s by [[Noam Chomsky]].&lt;ref name="chomsky1956"&gt;{{cite journal
 | last = Chomsky | first = Noam | title = Three models for the description of language
 | journal = Information Theory, IEEE Transactions
 | volume = 2 | issue = 3 | pages = 113–124
 | publisher = | date = Sep 1956
 | url = http://ieeexplore.ieee.org/iel5/18/22738/01056813.pdf?isnumber=22738&amp;prod=STD&amp;arnumber=1056813&amp;arnumber=1056813&amp;arSt=+113&amp;ared=+124&amp;arAuthor=+Chomsky%2C+N.
 | doi = 10.1109/TIT.1956.1056813| id = | accessdate = 2007-06-18}}&lt;/ref&gt;

Block structure was introduced into computer programming languages by the ALGOL project (1957–1960), which, as a consequence, also featured a context-free grammar to describe the resulting ALGOL syntax.

Context-free grammars are simple enough to allow the construction of efficient parsing algorithms which, for a given string, determine whether and how it can be generated from the grammar. If a programming language designer is willing to work within some limited subsets of context-free grammars, more efficient parsers are possible.

=== LR parsing ===

{{Main|LR parser}}

The [[LR parser]] (left to right) was invented by [[Donald Knuth]] in 1965 in a paper, "On the Translation of Languages from Left to Right". An '''LR parser''' is a parser that reads input from '''L'''eft to right (as it would appear if visually displayed) and produces a [[Rightmost derivation|'''R'''ightmost derivation]].  The term '''LR(''k'') parser''' is also used, where ''k'' refers to the number of unconsumed [[Parsing#Lookahead|lookahead]] input symbols that are used in making parsing decisions.

Knuth proved that LR(''k'') grammars can be parsed with an execution time essentially proportional to the length of the program, and that every LR(''k'') grammar for ''k''&amp;nbsp;&amp;gt;&amp;nbsp;1 can be mechanically transformed into an LR(1) grammar for the same language. In other words, it is only necessary to have one symbol lookahead to parse any [[deterministic context-free grammar]](DCFG).&lt;ref&gt;{{cite web|last=Knuth|first=Donald|title=On the Translation of Languages from Left to Right|url=http://www.cs.dartmouth.edu/~mckeeman/cs48/mxcom/doc/knuth65.pdf|accessdate=29 May 2011}}&lt;/ref&gt;

Korenjak (1969) was the first to show parsers for programming languages could be produced using these techniques.&lt;ref&gt;Korenjak, A. “A Practical Method for Constructing LR(k) Processors,” Communications of the ACM, Vol. 12, No. 11, 1969&lt;/ref&gt; Frank DeRemer devised the more practical [[Simple LR parser|Simple LR]] (SLR) and [[LALR parser|Look-ahead LR]] (LALR) techniques, published in his PhD dissertation at MIT in 1969.&lt;ref&gt;DeRemer, F. Practical Translators for LR(k) Languages. PhD dissertation, MIT, 1969.&lt;/ref&gt;&lt;ref&gt;DeRemer, F. “Simple LR(k) Grammars,” Communications of the ACM, Vol. 14, No. 7, 1971.&lt;/ref&gt; This was an important breakthrough, because LR(k) translators, as defined by Donald Knuth, were much too large for implementation on computer systems in the 1960s and 1970s.

In practice, LALR offers a good solution; the added power of LALR(1) parsers over SLR(1) parsers (that is, LALR(1) can parse more complex grammars than SLR(1)) is useful, and, though LALR(1) is not comparable with LL(1) (LALR(1) cannot parse all LL(1) grammars), most LL(1) grammars encountered in practice can be parsed by LALR(1). LR(1) grammars are more powerful again than LALR(1); however, an LR(1) grammar requires a [[canonical LR parser]] which would be extremely large in size and is not considered practical. The syntax of many [[programming language]]s are defined by grammars that can be parsed with an LALR(1) parser, and for this reason LALR parsers are often used by compilers to perform syntax analysis of source code.

A [[recursive ascent parser]] implements an LALR parser using mutually-recursive functions rather than tables. Thus, the parser is ''directly encoded'' in the host language similar to [[recursive descent]]. Direct encoding usually yields a parser which is faster than its table-driven equivalent&lt;ref name=Pennello86&gt;{{cite news|title=Very fast LR parsing|author=Thomas J Pennello|year=1986|journal=ACM SIGPLAN Notices|volume=21|issue=7|url=http://portal.acm.org/citation.cfm?id=13310.13326}}&lt;/ref&gt; for the same reason that compilation is faster than interpretation. It is also (in principle) possible to hand edit a recursive ascent parser, whereas a tabular implementation is nigh unreadable to the average human.

Recursive ascent was first described by Thomas Pennello in his article "Very fast LR parsing" in 1986.&lt;ref name=Pennello86 /&gt; The technique was later expounded upon by G.H. Roberts&lt;ref&gt;{{cite web|title=Recursive ascent: an LR analog to recursive descent|year=1988|author=G.H. Roberts|url=http://portal.acm.org/citation.cfm?id=47907.47909}}&lt;/ref&gt; in 1988 as well as in an article by Leermakers, Augusteijn, Kruseman Aretz&lt;ref&gt;{{cite web|title=A functional LR parser|author=Leermakers, Augusteijn, Kruseman Aretz|year=1992|url=http://portal.acm.org/citation.cfm?id=146986.146994}}&lt;/ref&gt; in 1992 in the journal ''Theoretical Computer Science''.

=== LL parsing ===

{{Main|LL parser}}

An [[LL parser]] parses the input from '''L'''eft to right, and constructs a [[Context-free grammar#Derivations and syntax trees|'''L'''eftmost derivation]] of the sentence (hence LL, as opposed to LR). The class of grammars which are parsable in this way is known as the ''LL grammars''. LL grammars are an even more restricted class of context-free grammars than LR grammars. Nevertheless, they are of great interest to compiler writers, because such a parser is simple and efficient to implement.

LL(k) grammars can be parsed by a [[recursive descent parser]] which is usually coded by hand, although a notation such as [[META II]] might alternatively be used.

The design of ALGOL sparked investigation of recursive descent, since the ALGOL language itself is recursive. The concept of recursive descent parsing was discussed in the January 1961 issue of [[Communications of the ACM|CACM]] in separate papers by A.A. Grau and [[Edgar T. Irons|Edgar T. "Ned" Irons]].
&lt;ref&gt;A.A. Grau, "Recursive processes and ALGOL translation", Commun. ACM,  4, No. 1, pp. 10–15. Jan. 1961&lt;/ref&gt;
&lt;ref&gt;[[Edgar T. Irons]], "A syntax-directed compiler for ALGOL 60", Commun. ACM,  4, No. 1, Jan. 1961, pp. 51–55.&lt;/ref&gt;
Richard Waychoff and colleagues also implemented recursive descent in the [[Burroughs Corporation|Burroughs]] ALGOL compiler in March 1961,&lt;ref&gt;{{cite web |url=http://www.ianjoyner.name/Files/Waychoff.pdf |title=Stories of the B5000 and People Who Were There}}&lt;/ref&gt; the two groups used different approaches but were in at least informal contact.&lt;ref&gt;{{cite web |url=http://hdl.handle.net/11299/107105 |title=The Burroughs B5000 Conference, Charles Babbage Institute}}&lt;/ref&gt;

The idea of LL(1) grammars was introduced by Lewis and Stearns (1968).&lt;ref&gt;P. M. Lewis, R. E. Stearns, "Syntax directed transduction," focs, pp.21–35, 7th Annual Symposium on Switching and Automata Theory (SWAT 1966), 1966&lt;/ref&gt;&lt;ref&gt;Lewis, P. and Stearns, R. “Syntax-Directed Transduction,” Journal of the ACM, Vol. 15, No. 3, 1968.&lt;/ref&gt;

Recursive descent was popularised by [[Niklaus Wirth]] with [[PL/0]], an [[educational programming language]] used to teach compiler construction in the 1970s.&lt;ref&gt;{{cite web |url=http://www.246.dk/pl0.html |title=The PL/0 compiler/interpreter}}&lt;/ref&gt;

LR parsing can handle a larger range of languages than [[LL parsing]], and is also better at error reporting, i.e. it detects syntactic errors when the input does not conform to the grammar as soon as possible.

=== Earley parser ===

In 1970, [[Jay Earley]] invented what came to be known as the [[Earley parser]]. Earley parsers are appealing because they can parse all [[context-free language]]s reasonably efficiently.&lt;ref&gt;J. Earley, [http://portal.acm.org/citation.cfm?doid=362007.362035 "An efficient context-free parsing algorithm"], ''Communications of the Association for Computing Machinery'', '''13''':2:94-102, 1970.&lt;/ref&gt;

== Grammar description languages ==

John Backus proposed "metalinguistic formulas"&lt;ref&gt;{{cite journal
| last = Backus
| first = J. W.
| author-link = John W. Backus
| year = 1959
| title = The syntax and semantics of the proposed international algebraic language of the Zurich ACM-GAMM Conference
| url = http://www.softwarepreservation.org/projects/ALGOL/paper/Backus-Syntax_and_Semantics_of_Proposed_IAL.pdf/view
| work = Proceedings of the International Conference on Information Processing
| publisher = UNESCO
| pages = 125–132
}}&lt;/ref&gt;&lt;ref&gt;{{cite web
| website = Compiler Basics
| last = Farrell
| first = James A.
| title = Extended Backus Naur Form
| date = August 1995
| url = http://www.cs.man.ac.uk/~pjj/farrell/comp2.html#EBNF
| accessdate = 11 May 2011
| postscript= .
}}&lt;/ref&gt;
to describe the syntax of the new programming language IAL, known today as [[ALGOL 58]] (1959). Backus's work was based on the [[Post canonical system]] devised by [[Emil Post]].

Further development of ALGOL led to [[ALGOL 60]]; in its report (1963), [[Peter Naur]] named Backus's notation '''Backus normal form''' (BNF), and simplified it to minimize the character set used. However, Donald Knuth argued that BNF should rather be read as [[Backus–Naur form]],&lt;ref&gt;Donald E. Knuth, "Backus Normal Form vs. Backus Naur Form", Commun. ACM, 7(12):735–736, 1964.&lt;/ref&gt; and that has become the commonly accepted usage.

[[Niklaus Wirth]] defined [[extended Backus–Naur form]] (EBNF), a refined version of BNF, in the early 1970s for PL/0. [[Augmented Backus–Naur form]] (ABNF) is another variant. Both EBNF and ABNF are widely used to specify the grammar of programming languages, as the inputs to parser generators, and in other fields such as defining communication protocols.

== Parser generators ==
{{Hatnote|For a more complete list, which also includes LL, SLR, GLR and LR parser generators, see [[Comparison of parser generators]]}}

A [[parser generator]] generates the lexical-analyser portion of a compiler. It is a program that takes a description of a [[formal grammar]] of a specific programming language and produces a parser for that language.  That parser can be used in a compiler for that specific language. The parser detects and identifies the reserved words and symbols of the specific language from a stream of text and returns these as tokens to the code which implements the syntactic validation and translation into object code.  This second part of the compiler can also be created by a ''compiler-compiler'' using a formal rules-of-precedence syntax-description as input.

The first ''compiler-compiler'' to use that name was written by [[Tony Brooker]] in 1960 and was used to create compilers for the [[Atlas Computer (Manchester)|Atlas]] computer at the University of Manchester, including the [[Atlas Autocode]] compiler.  However it was rather different from modern compiler-compilers, and today would probably be described as being somewhere between a highly customisable generic compiler and an [[Extensible programming|extensible-syntax language]].  The name 'compiler-compiler' was far more appropriate for Brooker's system than it is for most modern compiler-compilers, which are more accurately described as parser generators.  It is almost certain that the "Compiler Compiler" name has entered common use due to [[Yacc]] rather than Brooker's work being remembered.{{Citation needed|date=February 2007}}

In the early 1960s, Robert McClure at [[Texas Instruments]] invented a compiler-compiler called TMG, the name taken from "transmogrification".&lt;ref&gt;{{cite web|url=http://www.reocities.com/ResearchTriangle/2363/tmg011.html|title=TMG Meta Compiler|work=reocities.com}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://hopl.murdoch.edu.au/showlanguage.prx?exp=242 |title=Archived copy |accessdate=2011-06-30 |deadurl=yes |archiveurl=https://web.archive.org/web/20070921161049/http://hopl.murdoch.edu.au/showlanguage.prx?exp=242 |archivedate=21 September 2007 |df=dmy-all }}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://portal.acm.org/citation.cfm?id=806050&amp;dl=ACM&amp;coll=DL&amp;CFID=29658196&amp;CFTOKEN=62044584|title=Programming languages for non-numeric processing—1|work=acm.org}}&lt;/ref&gt;&lt;ref&gt;R. M. McClure, ''TMG—A Syntax Directed Compiler'' Proc. 20th ACM National Conf. (1965), pp. 262–274.&lt;/ref&gt; In the following years TMG was [[Porting|ported]] to several [[UNIVAC]] and IBM mainframe computers.

The [[Multics]] project, a joint venture between [[MIT]] and [[Bell Labs]], was one of the first to develop an [[operating system]] in a high level language. [[PL/I]] was chosen as the language, but an external supplier could not supply a working compiler.&lt;ref&gt;{{cite web|url=http://multicians.org/pl1.html|title=Multics PL/I|work=multicians.org}}&lt;/ref&gt;  The Multics team developed their own subset dialect of [[PL/I]] known as '''Early PL/I''' (EPL) as their implementation language in 1964. TMG was ported to [[GE-600 series]] and used to develop EPL by [[Douglas McIlroy]], [[Robert Morris (cryptographer)|Robert Morris]], and others.

Not long after [[Ken Thompson]] wrote the first version of [[Unix]] for the [[PDP-7]] in 1969, Doug McIlroy created the new system's first higher-level language: an implementation of McClure's TMG.&lt;ref&gt;{{cite web|url=http://cm.bell-labs.com/who/dmr/chist.html |title=Archived copy |accessdate=2011-08-03 |deadurl=yes |archiveurl=https://archive.is/20150110214721/http://cm.bell-labs.com/who/dmr/chist.html |archivedate=10 January 2015 |df=dmy-all }} Dennis M. Ritchie. ''The Development of the C Language''&lt;/ref&gt; TMG was also the compiler definition tool used by Ken Thompson to write the compiler for the [[B programming language|B language]] on his PDP-7 in 1970. B was the immediate ancestor of [[C programming language|C]].

An early [[LALR parser generator]] was called "TWS", created by Frank DeRemer and Tom Pennello.

=== XPL ===

[[XPL]] is a dialect of the [[PL/I]] [[programming language]], used for the development of compilers for computer languages. It was designed and implemented in 1967 by a team with [[William M. McKeeman]], [[Jim Horning|James J. Horning]], and [[David Wortman|David B. Wortman]] at [[Stanford University]] and the [[University of California, Santa Cruz]]. It was first announced at the 1968 [[Fall Joint Computer Conference]] in San Francisco.&lt;ref&gt;McKeeman, William Marshall; Horning, James J.; and Wortman, David B., ''A Compiler Generator'' (1971),  {{ISBN|978-0-13-155077-3}}.&lt;/ref&gt;&lt;ref&gt;Computer Science Department, [[University of Toronto]], [http://www.cs.toronto.edu/XPL/ "The XPL Programming Language"]&lt;/ref&gt;

XPL featured a relatively simple [[compiler-compiler|translator writing system]] dubbed [[XPL#ANALYZER|ANALYZER]], based upon a [[Bottom-up parsing|bottom-up compiler]] precedence parsing technique called [[Shift-reduce parser#MSP|MSP]] (mixed strategy precedence).  XPL was bootstrapped through Burroughs Algol onto the [[IBM System/360]] computer.  (Some subsequent versions of XPL used on [[University of Toronto]] internal projects utilized an SLR(1) parser, but those implementations have never been distributed).

=== Yacc ===

[[Yacc]] is a [[parser generator]] (loosely, [[compiler-compiler]]), not to be confused with [[Lex programming tool|lex]], which is a [[lexical analysis|lexical analyzer]] frequently used as a first stage by Yacc. Yacc was developed by [[Stephen C. Johnson]] at [[AT&amp;T]] for the [[Unix]] operating system.&lt;ref&gt;Johnson, S.C., “Yacc – Yet Another Compiler Compiler”, Computing Science Technical Report 32, AT&amp;T Bell Labs, 1975&lt;/ref&gt; The name is an acronym for "[[Yet Another]] [[Compiler Compiler]]." It generates an LALR(1) compiler based on a grammar written in a notation similar to Backus–Naur form.

Johnson worked on Yacc in the early 1970s at [[Bell Labs]].&lt;ref&gt;{{cite web|url=http://www.techworld.com.au/article/252319/a-z_programming_languages_yacc/|title=The A-Z of Programming Languages: YACC|first=Naomi|last=Hamilton|work=TechWorld}}&lt;/ref&gt; He was familiar with TMG and its influence can be seen in Yacc and the design of the C programming language. Because Yacc was the default compiler generator on most Unix systems, it was widely distributed and used. Derivatives such as [[GNU Bison]] are still in use.

The compiler generated by Yacc requires a [[lexical analyzer]]. Lexical analyzer generators, such as [[Lex programming tool|lex]] or [[Flex lexical analyser|flex]] are widely available. The [[IEEE]] [[POSIX]] P1003.2 standard defines the functionality and requirements for both Lex and Yacc.

== Metacompilers ==
{{Main|metacompiler}}

Metacompilers differ from parser generators, taking as input a [[computer program|program]] written in a [[metalanguage#Metaprogramming|metalanguage]]. Their input consists grammar analyzing formula and code production transforms that output executable code. Many can be programmed in their own metalanguage enabling them to compile themselves, making them self-hosting extensible language compilers.

Many metacompilers build on the work of [[Metacompiler#Schorre metalanguages|Dewey Val Schorre]]. His [[META II]] compiler, first released in 1964, was the first documented metacompiler. Able to define its own language and others, META II accepted [[formal grammar#Analytic grammars|syntax formula]] having imbedded [[Code generation (compiler)|output (code production)s]]. It also translated to one of the earliest instances of a [[virtual machine]]. Lexical analysis was performed by built token recognizing functions: .ID, .STRING, and .NUMBER. Quoted strings in syntax formula recognize lexemes that are not kept.&lt;ref&gt;{{cite web|url=http://portal.acm.org/citation.cfm?id=808896&amp;dl=ACM&amp;coll=&amp;CFID=15151515&amp;CFTOKEN=6184618|title=META II a syntax-oriented compiler writing language|work=acm.org}}&lt;/ref&gt;

[[TREE-META]], a second generation Schorre metacompiler, appeared around 1968. It extended the capabilities of META II, adding unparse rules separating code production from the grammar analysis. Tree transform operations in the syntax formula produce [[abstract syntax tree]]s that the unparse rules operate on. The unparse tree pattern matching provided [[peephole optimization]] ability.

[[Metacompiler#CWIC|CWIC]], described in a 1970 ACM publication is a third generation Schorre metacompiler that added lexing rules and backtracking operators to the grammar analysis. [[LISP 2]] was married with the unparse rules of TREEMETA in the CWIC generator language. With LISP 2 processing, CWIC can generate fully optimized code. CWIC also provided binary code generation into named code sections. Single and multipass compiles could be implemented using CWIC.

CWIC compiled to 8 bit byte addressable machine code instructions primarily designed to produce IBM System/360 code.

Later generations are not publicly documented. One important feature would be the abstraction of the target processor instruction set, generating to a pseudo machine instruction set, macros, that could be separately defined or mapped to a real machine's instructions. Optimizations applying to sequential instructions could then be applied to the pseudo instruction before their expansion to target machine code.

== Cross compilation ==

A [[Cross compilation|cross compiler]] runs in one environment but produces object code for another. Cross compilers are used for embedded development, where the target computer has limited capabilities.

An early example of cross compilation was AIMICO, where a FLOW-MATIC program on a UNIVAC II was used to generate assembly language for the [[IBM 705]], which was then assembled on the IBM computer.&lt;ref name="computerhistory.org" /&gt;

The [[ALGOL 68C]] compiler generated ''ZCODE'' output, that could then be either compiled into the local machine code by a ''ZCODE'' translator or run interpreted.  ''ZCODE'' is a register-based intermediate language.  This ability to interpret or compile ''ZCODE'' encouraged the porting of ALGOL 68C to numerous different computer platforms.

== Optimizing compilers ==

[[Compiler optimization]] is the process of improving the quality of object code without changing the results it produces.

The developers of the first FORTRAN compiler aimed to generate code that was ''better'' than the average
hand-coded assembler, so that customers would actually use their product. In one of the first
real compilers, they often succeeded.&lt;ref&gt;{{cite web|url=http://compilers.iecc.com/comparch/article/97-10-017|title=Comp.compilers: Re: History and evolution of compilers|work=iecc.com}}&lt;/ref&gt;

Later compilers, like IBM's Fortran IV compiler, placed more priority on good diagnostics and executing more quickly, at the expense of object code optimization. It wasn't until the IBM System/360 series that IBM provided two separate compilers: a fast executing code checker, and a slower optimizing one.

[[Frances E. Allen]], working alone and jointly with [[John Cocke]], introduced many of the concepts for optimization. Allen's 1966 paper, ''Program Optimization,''&lt;ref&gt;F.E. Allen.
Program optimization.
In Mark I. Halpern and Christopher J. Shaw, editors, Annual Review in Automatic Programming, volume 5, pages 239–307. Pergamon Press, New York, 1969.&lt;/ref&gt; introduced the use of [[Graph (data structure)|graph data structures]] to encode program content for optimization.&lt;ref&gt;Frances E. Allen and John Cocke.
Graph theoretic constructs for program control flow analysis.
Technical Report IBM Res. Rep. RC 3923, IBM T.J. Watson Research Center, Yorktown Heights, NY, 1972.&lt;/ref&gt; Her 1970 papers, ''Control Flow Analysis''&lt;ref&gt;Frances E. Allen.
Control flow analysis.
ACM SIGPLAN Notices, 5(7):1–19, July 1970.&lt;/ref&gt; and ''A Basis for Program Optimization''&lt;ref&gt;Frances E. Allen.
A basis for program optimization.
In Proc. IFIP Congress 71, pages 385–390. North-Holland, 1972.&lt;/ref&gt; established ''intervals'' as the context for efficient and effective data flow analysis and optimization. Her 1971 paper with Cocke, ''A Catalogue of Optimizing Transformations'',&lt;ref&gt;Frances E. Allen and John Cocke.
A catalogue of optimizing transformations.
In R. Rustin, editor, Design and Optimization of Compilers, pages 1–30. Prentice-Hall, 1971.&lt;/ref&gt; provided the first description and systematization of optimizing transformations. Her 1973 and 1974 papers on interprocedural data flow analysis extended the analysis to whole programs.&lt;ref&gt;Frances E. Allen.
Interprocedural data flow analysis.
In Proc. IFIP Congress 74, pages 398–402. North-Holland, 1974.&lt;/ref&gt;&lt;ref&gt;Frances E. Allen.
A method for determining program data relationships.
In Andrei Ershov and Valery A. Nepomniaschy, editors, Proc. International Symposium on Theoretical Programming, Novosibirsk, USSR, August 1972, volume 5 of Lecture Notes in Computer Science, pages 299–308. Springer-Verlag, 1974.&lt;/ref&gt; Her 1976 paper with Cocke describes one of the two main analysis strategies used in optimizing compilers today.&lt;ref&gt;Frances E. Allen and John Cocke.
A program data flow analysis procedure.
Communications of the ACM, 19(3):137–147, March 1976.&lt;/ref&gt;

Allen developed and implemented her methods as part of compilers for the [[IBM 7030 Stretch]]-[[IBM 7950 Harvest|Harvest]] and the experimental [[ACS-1|Advanced Computing System]]. This work established the feasibility and structure of modern machine- and language-independent optimizers. She went on to establish and lead the PTRAN project on the automatic parallel execution of FORTRAN programs.&lt;ref&gt;Vivek Sarkar. The PTRAN Parallel Programming System.  In Parallel Functional Programming Languages and Compilers, edited by B. Szymanski, ACM Press Frontier Series, pages 309–391, 1991.&lt;/ref&gt; Her PTRAN team developed new parallelism detection schemes and created the concept of the program dependence graph, the primary structuring method used by most parallelizing compilers.

''Programming Languages and their Compilers'' by John Cocke and [[Jacob T. Schwartz]], published early in 1970, devoted more than 200 pages to optimization algorithms. It included many of the now familiar techniques such as [[Partial redundancy elimination|redundant code elimination]] and [[strength reduction]].&lt;ref&gt;John Cocke and Jacob T. Schwartz, Programming Languages and their Compilers. Courant Institute of Mathematical Sciences, New York University, April 1970.&lt;/ref&gt;

=== Peephole Optimization ===

[[Peephole optimizations|Peephole optimization]] is a very simple but effective optimization technique. It was invented by [[William M. McKeeman]] and published in 1965 in CACM.&lt;ref&gt;[[William M. McKeeman|McKeeman, W.M.]] Peephole optimization. Commun. ACM 8, 7 (July 1965), 443–444&lt;/ref&gt; It was used in the XPL compiler that McKeeman helped develop.

=== Capex COBOL Optimizer ===
[[Capex Corporation]] developed the "COBOL Optimizer" in the mid 1970s for [[COBOL]]. This type of optimizer depended, in this case, upon knowledge of 'weaknesses' in the standard IBM COBOL compiler, and actually replaced (or [[Patch (computing)|patched]]) sections of the object code with more efficient code. The replacement code might replace a linear [[Lookup table|table lookup]] with a [[binary search]] for example or sometimes simply replace a relatively 'slow' instruction with a known faster one that was otherwise functionally equivalent within its context. This technique is now known as "[[Strength reduction]]". For example, on the IBM System/360 hardware the '''CLI''' instruction was, depending on the particular model, between twice and 5 times as fast as a '''CLC''' instruction for single byte comparisons.&lt;ref&gt;http://www.bitsavers.org/pdf/ibm/360/A22_6825-1_360instrTiming.pdf&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://portal.acm.org/citation.cfm?id=358732&amp;dl=GUIDE&amp;dl=ACM|title=Software engineering for the Cobol environment|work=acm.org}}&lt;/ref&gt;

Modern compilers typically provide optimization options, so programmers can choose whether or not to execute an optimization pass.

== Diagnostics ==

When a compiler is given a syntactically incorrect program, a good, clear error message is helpful. From the perspective of the compiler writer, it is often difficult to achieve.

The [[WATFIV]] Fortran compiler was developed at the [[University of Waterloo]], Canada in the late 1960s. It was designed to give better error messages than IBM's Fortran compilers of the time. In addition, WATFIV was far more usable, because it combined compiling, [[Linker (computing)|linking]] and execution into one step, whereas IBM's compilers had three separate components to run.

=== PL/C ===
[[PL/C]] was a computer programming language developed at Cornell University in the early 1970s. While PL/C was a subset of IBM's PL/I language, it was designed with the specific goal of being used for teaching programming. The two researchers and academic teachers who designed PL/C were Richard W. Conway and Thomas R. Wilcox. They submitted the famous article "Design and implementation of a diagnostic compiler for PL/I" published in the Communications of ACM in March 1973.&lt;ref&gt;CACM March 1973 pp 169–179.&lt;/ref&gt;

PL/C eliminated some of the more complex features of PL/I, and added extensive debugging and error recovery facilities. The PL/C compiler had the unusual capability of never failing to compile any program, through the use of extensive automatic correction of many syntax errors and by converting any remaining syntax errors to output statements.

== Just in Time compilation ==
{{Main|Just-in-time compilation}}
Just in time compilation ([[Just-in-time compilation|JIT]]) is the generation of executable code [[On the fly|on-the-fly]] or as close as possible to its actual execution, to take advantage of run time [[Software metric|metrics]] or other performance enhancing options.

== Intermediate representation ==
{{Main|Intermediate language}}

Most modern compilers have a lexer and parser that produce an intermediate representation of the program. The intermediate representation is a simple sequence of operations which can be used by an optimizer and a [[Code generation (compiler)|code generator]] which produces instructions in the [[Machine code|machine language]] of the target processor. Because the code generator uses an intermediate representation, the same code generator can be used for many different high level languages.

There are many possibilities for the intermediate representation. [[Three-address code]], also known as a ''quadruple'' or ''quad'' is a common form, where there is an operator, two operands, and a result. Two-address code or ''triples'' have a stack to which results are written, in contrast to the explicit variables of three-address code.

[[Static Single Assignment]] (SSA) was developed by [[Ron Cytron]], [[Jeanne Ferrante]], [[Barry Rosen (computer scientist)|Barry K. Rosen]], [[Mark N. Wegman]], and [[F. Kenneth Zadeck]], researchers at [[International Business Machines|IBM]] in the 1980s.&lt;ref name="Cytron_1991"&gt;{{cite journal 
|title=Efficiently computing static single assignment form and the control dependence graph 
|author1=Cytron, Ron |author2=Ferrante, Jeanne |author3=Rosen, Barry K. |author4=Wegman, Mark N. |author5=Zadeck, F. Kenneth |journal=ACM Transactions on Programming Languages and Systems |volume=13 |year=1991 |pages=451&amp;ndash;490 |url=http://www.cs.utexas.edu/~pingali/CS380C/2010/papers/ssaCytron.pdf |issue=4 
|doi=10.1145/115372.115320 }}&lt;/ref&gt; In SSA, a variable is given a value only once. A new variable is created rather than modifying an existing one. SSA simplifies optimization and code generation.

== Code Generation ==
{{Main|Code generation (compiler)}}
A code generator generates machine language instructions for the target processor.

=== Register Allocation ===
[[Sethi–Ullman algorithm]] or Sethi-Ullman numbering is a method to minimise the number of registers needed to hold variables.

== Notable compilers ==
{{See also|ALGOL 60#ALGOL 60 implementations timeline{{!}}ALGOL 60: ALGOL 60 implementations timeline|List of compilers}}
* [[Amsterdam Compiler Kit]] by [[Andrew Tanenbaum]] and Ceriel Jacobs
* Berkeley Pascal, created by Ken Thompson in 1975. [[Bill Joy]] and others at University of California, Berkeley added improvements
* [[GNU Compiler Collection]], formerly the GNU C Compiler. First created by [[Richard Stallman]] in 1987, GCC is the major compiler used to build [[Linux]].
* [[LLVM]], formerly known as the ''Low Level Virtual Machine''
* [[Small-C]] by Ron Cain and James E Hendrix
* [[Turbo Pascal]], created by [[Anders Hejlsberg]], first released in 1983.
* [[WATFOR]], created at the [[University of Waterloo]]. One of the first popular educational compilers, although now largely obsolete.
{{expand list|date=October 2012}}

== See also ==
* [[History of programming languages]]
* [[Lex programming tool|Lex]] (and [[Flex lexical analyser]]), the token parser commonly used in conjunction with yacc (and Bison).
* [[Backus–Naur form|BNF]], a [[metasyntax]] used to express [[context-free grammar]]: that is, a formal way to describe formal languages.
* [[Self-interpreter]], an interpreter written in a language it can interpret.

== References ==
{{reflist|2}}

== Further reading ==
* [[John Backus|Backus, John]], et al., [http://archive.computerhistory.org/resources/text/Fortran/102663113.05.01.acc.pdf "The FORTRAN Automatic Coding System"], Proceedings of the Western Joint Computer Conference, Los Angeles, California, February 1957. Describes the design and implementation of the first FORTRAN compiler by the IBM team.
* Knuth, D. E., ''RUNCIBLE-algebraic translation on a limited computer'', Communications of the ACM, Vol. 2, p.&amp;nbsp;18, (Nov. 1959).
* Irons, Edgar T., ''A syntax directed compiler for ALGOL 60'', Communications of the ACM, Vol. 4, p.&amp;nbsp;51. (Jan. 1961)
* {{cite techreport|first=Edsger W.|last=Dijkstra|authorlink=Edsger Dijkstra|title="ALGOL 60 Translation: An ALGOL 60 Translator for the X1 and Making a Translator for ALGOL 60|year=1961|institution=Mathematisch Centrum|location=Amsterdam|number=35|url=http://www.cs.utexas.edu/users/EWD/MCReps/MR35.PDF}}
* [[Melvin Conway|Conway, Melvin E.]], ''Design of a separable transition-diagram compiler'', Communications of the ACM, Volume 6,  Issue 7  (July 1963)
* [[Robert Floyd|Floyd, R. W.]], ''Syntactic analysis and operator precedence'', Journal of the ACM, Vol. 10, p.&amp;nbsp;316. (July 1963).
*Cheatham, T. E., and Sattley, K., ''Syntax directed compilation'', SJCC p.&amp;nbsp;31. (1964).
* [[Brian Randell|Randell, Brian]]; Russell, Lawford John, ''ALGOL 60 Implementation: The Translation and Use of ALGOL 60 Programs on a Computer'', Academic Press, 1964
* {{cite journal | last1 = Knuth | first1 = D. E. | authorlink = Donald Knuth | title = On the translation of languages from left to right | doi = 10.1016/S0019-9958(65)90426-2 | journal = Information and Control | volume = 8 | issue = 6 | pages = 607–639 | date=July 1965 | url = http://www.cs.dartmouth.edu/~mckeeman/cs48/mxcom/doc/knuth65.pdf | accessdate=29 May 2011 | ref = harv }}
* [[John Cocke|Cocke, John]]; [[Jack Schwartz|Schwartz, Jacob T.]], ''Programming Languages and their Compilers: Preliminary Notes'', [[Courant Institute of Mathematical Sciences]] technical report, [[New York University]], 1969.
* [[Friedrich L. Bauer|Bauer, Friedrich L.]]; Eickel, Jürgen (Eds.), ''Compiler Construction, An Advanced Course'', 2nd ed. Lecture Notes in Computer Science 21, Springer 1976, {{ISBN|3-540-07542-9}}
* [[David Gries|Gries, David]], ''Compiler Construction for Digital Computers'', New York : Wiley, 1971. {{ISBN|0-471-32776-X}}

== External links ==
* [https://web.archive.org/web/20120328052718/http://www.dickgrune.com/Summaries/CS/CompilerConstruction-1979.html Compiler Construction before 1980] — Annotated literature list by [[Dick Grune]]
*{{cite journal|title=A HISTORY OF WRITING COMPILERS|journal=Computers and Automation|date=Dec 1962|volume=XI|issue=12|pages=8-10, 12, 14, 24-25|url=http://bitsavers.trailing-edge.com/pdf/computersAndAutomation/196212.pdf}}

{{Parsers}}

[[Category:Compilers]]
[[Category:History of software]]
[[Category:History of computer science]]
[[Category:Parsing algorithms]]</text>
      <sha1>qx3jeev5ak7ctl4yqocz4uod7q9ycx6</sha1>
    </revision>
  </page>
  <page>
    <title>Hub (network science)</title>
    <ns>0</ns>
    <id>46897719</id>
    <revision>
      <id>871218044</id>
      <parentid>858203859</parentid>
      <timestamp>2018-11-29T17:51:24Z</timestamp>
      <contributor>
        <username>CitationCleanerBot</username>
        <id>15270283</id>
      </contributor>
      <minor/>
      <comment>Various citation &amp; identifier cleanup, plus AWB genfixes (arxiv version pointless when published)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12431">In [[network science]], a '''hub''' is a [[Node (networking)|node]] with a number of links that greatly exceeds the average. Emergence of hubs is a consequence of a scale-free property of networks.&lt;ref&gt;[http://barabasi.com/networksciencebook/content/book_chapter_2.pdf Barabási, Albert-László. ''Network Science: Graph Theory''., p. 27]&lt;/ref&gt; While hubs cannot be observed in a random network, they are expected to emerge in [[scale-free network]]s. The uprise of hubs in scale-free networks is associated with power-law distribution. Hubs have a significant impact on the [[network topology]]. Hubs can be found in many real networks, such as Brain Network or Internet.
[[File:Network representation of brain connectivity.JPG|thumb|Network representation of brain connectivity. Hubs are highlighted|150px|right]]
[[File:Internet map 4096.png|thumb|Partial map of the Internet based on the January 15, 2005. Hubs are highlighted|150px|right]]A hub is a component of a network with a high-degree [[Vertex (graph theory)|node]]. Hubs have a significantly larger number of links in comparison with other nodes in the network. The number of links ([[Degree (graph theory)|degrees]]) for a hub in a scale-free network is much higher than for the biggest node in a random network, keeping the size ''N'' of the network and average degree ''&lt;k&gt;'' constant. The existence of hubs is the biggest difference between random networks and scale-free networks. In random networks, the degree ''k'' is comparable for every node; it is therefore not possible for hubs to emerge. In scale-free networks, a few nodes (hubs) have a high degree ''k'' while the other nodes have a small number of links.

== Emergence ==
[[Image:Scale-free network sample.png|thumb|Example of a random network and a scale-free network|400px|right|Random network (a) and scale-free network (b). In the scale-free network, the larger hubs are highlighted.]]

Emergence of hubs can be explained by the difference between scale-free networks and random networks. Scale-free networks ([[Barabási–Albert model]]) are different from random networks ([[Erdős–Rényi model]]) in two aspects: (a) growth, (b) preferential attachment.&lt;ref name=RMP&gt;{{Cite journal
 | url = http://www.nd.edu/~networks/Publication%20Categories/03%20Journal%20Articles/Physics/StatisticalMechanics_Rev%20of%20Modern%20Physics%2074,%2047%20(2002).pdf
 | authorlink1 = Réka Albert | first1 = Réka | last1 = Albert
 | authorlink2 = Albert-László Barabási | first2 = Albert-László | last2 = Barabási
 | title = Statistical mechanics of complex networks
 | journal = [[Reviews of Modern Physics]]
 | volume = 74
 | pages = 47–97
 | year = 2002
 | doi = 10.1103/RevModPhys.74.47
 | bibcode=2002RvMP...74...47A
|arxiv = cond-mat/0106096 }}&lt;/ref&gt;
* (a) Scale-free networks assume a continuous growth of the number of nodes ''N'', compared to random networks which assume a fixed number of nodes. In scale-free networks the degree of the largest hub rises polynomially with the size of the network. Therefore, the degree of a hub can be high in a scale-free network. In random networks the degree of the largest node rises logaritmically (or slower) with N, thus the hub number will be small even in a very large network.
* (b) A new node in a scale-free network has a tendency to link to a node with a higher degree, compared to a new node in a random network which links itself to a random node. This process is called [[preferential attachment]]. The tendency of a new node to link to a node with a high degree ''k'' is characterized by [[Power law|power-law distribution]] (also known as rich-gets-richer process). This idea was introduced by [[Vilfredo Pareto]] and it explained why a small percentage of the population earns most of the money. This process is present in networks as well, for example 80 percent of web links point to 15 percent of webpages. The emergence of scale-free networks is not typical only of networks created by human action, but also of such networks as metabolic networks or illness networks.&lt;ref&gt;Barabási, Albert-László. ''Network Science: The Scale-Free Property''., p. 8.[http://barabasi.com/networksciencebook/content/book_chapter_2.pdf]&lt;/ref&gt; This phenomenon may be explained by the example of hubs on the World Wide Web such as Facebook or Google. These webpages are very well known and therefore the tendency of other webpages pointing to them is much higher than linking to random small webpages.

The mathematical explanation for [[Barabási–Albert model]]:

[[File:Barabasi Albert model.gif|thumb|300px|The steps of the growth of the network according to the Barabasi–Albert model (&lt;math&gt;m_0=m=2&lt;/math&gt;)]]
The network begins with an initial connected network of &lt;math&gt;m_0&lt;/math&gt; nodes.

New nodes are added to the network one at a time. Each new node is connected to &lt;math&gt;m \le m_0&lt;/math&gt; existing nodes with a probability that is proportional to the number of links that the existing nodes already have. Formally, the probability &lt;math&gt;p_i&lt;/math&gt; that the new node is connected to node &lt;math&gt;i&lt;/math&gt; is&lt;ref name="RMP"/&gt;

: &lt;math&gt;p_i = \frac{k_i}{\sum_j k_j},&lt;/math&gt;

where &lt;math&gt;k_i&lt;/math&gt; is the degree of the node &lt;math&gt;i&lt;/math&gt; and the sum is taken over all pre-existing nodes &lt;math&gt;j&lt;/math&gt; (i.e. the denominator results in twice the current number of edges in the network).

Emergence of hubs in networks is also related to time. In scale-free networks, nodes which emerged earlier have a higher chance of becoming a hub than latecomers. This phenomenon is called first-mover advantage and it explains why some nodes become hubs and some do not. However, in a real network, the time of emergence is not the only factor that influences the size of the hub. For example, Facebook emerged 8 years later after Google became the largest hub on the World Wide Web and yet in 2011 Facebook became the largest hub of WWW. Therefore, in real networks the growth and the size of a hub depends also on various attributes such as popularity, quality or the aging of a node.

== Attributes ==
There are several attributes of Hubs in a scale-free networks

=== Shortening the path lengths in a network ===
The more observable hubs are in a network, the more they shrink a distances between nodes. In a scale-free networks hubs serve as bridges between the small degree nodes.&lt;ref&gt;Barabási, Albert-László. ''Network Science: The Scale-Free Property''., p. 23.[http://barabasi.com/networksciencebook/content/book_chapter_4.pdf]&lt;/ref&gt; Since the distance of two random nodes in a scale-free networks is small, we refer to scale-free networks as "small" or "ultra small". While a difference between path distance in a various small networks may not be noticeable, the difference in a path distance between large random network and scale-free network is remarkable.

Average path length in scale-free networks:
&lt;math&gt;\ell\sim\frac{\ln N}{\ln \ln N}.&lt;/math&gt;

=== Aging of hubs (nodes) ===
The phenomenon present in a real networks, when older hubs are shadowed in a network. This phenomenon is responsible for changes in evolution and topology of networks.&lt;ref&gt;[http://barabasi.com/networksciencebook/content/book_chapter_6.pdf Barabási, Albert-László. ''Network Science: Evolving Networks''., p. 3]&lt;/ref&gt; The example of aging phenomenon may be the case of Facebook overtaking the position of the largest hub on the Web where Google was the largest node since 2000.{{Citation needed|date=May 2016}}

=== Robustness and Attack Tolerance ===
During the random failure of nodes or targeted attack hubs are key components of the network. During the random failure of nodes in network hubs are responsible for exceptional robustness of network.&lt;ref&gt;{{cite journal|title=Resilience of the Internet to Random Breakdowns|
journal=Phys. Rev. Lett.|
year=2000|
first=Reoven|
last=Cohen|
first2=K. |last2=Erez |first3=D. |last3=ben-Avraham |first4=S.|last4=Havlin|authorlink4=Shlomo Havlin|
volume=85|
pages=4626–8|
doi= 10.1103/PhysRevLett.85.4626|
url=http://link.aps.org/doi/10.1103/PhysRevLett.85.4626
|bibcode=2000PhRvL..85.4626C|arxiv = cond-mat/0007048 |pmid=11082612}}
&lt;/ref&gt; The chance that a random failure would delete the hub is very small, because hubs coexists with a large number of small degree nodes. The removal of small degree nodes does not have a large effect on integrity of network. Even though the random removal would hit the hub, the chance of fragmantation of network is very small because the remaining hubs would hold the network together. In this case, hubs are the strength of a scale-free networks.

During a targeted attack on hubs, the integrity of a network will fall apart relatively fast. Since small nodes are predominantly linked to hubs, the targeted attack on the largest hubs results in destroys the network in a short period of time. The financial market meltdown in 2008 is an example of such a network failure, when bankruptcy of the largest players (hubs) led to a continuous breakdown of the whole system.&lt;ref&gt;{{Cite journal|author1=S. V. Buldyrev |author2=R. Parshani |author3=G. Paul |author4=H. E. Stanley |author5=S. Havlin |title = Catastrophic cascade of failures in interdependent networks|journal = Nature |volume = 464 |pages = 1025–28 |year = 2010 | doi=10.1038/nature08932 |issue=7291|url=http://havlin.biu.ac.il/Publications.php?keyword=Catastrophic+cascade+of+failures+in+interdependent+networks&amp;year=*&amp;match=all |pmid=20393559|arxiv=1012.0206 |bibcode=2010Natur.464.1025B }}&lt;/ref&gt; On the other hand, it may have a positive effect when removing hubs in a terrorist network; targeted node deletion may destroy the whole terrorist group. The attack tolerance of a network may be increased by connecting its peripheral nodes, however it requires to double the number of links.

=== Degree correlation ===
The perfect degree correlation means that each degree-k node is connected only to the same degree-k nodes. Such connectivity of nodes decide the topology of networks, which has an effect on robustness of network, the attribute discussed above. If the number of links between the hubs is the same as would be expected by chance, we refer to this network as Neutral Network. If hubs tend to connected to each other while avoiding linking to small-degree nodes we refer to this network as Assortative Network. This network is relatively resistant against attacks, because hubs form a core group, which is more reduntant against hub removal. If hubs avoid connecting to each other while linking to small-degree nodes, we refer to this network as Disassortative Network. This network has a hub-and-spoke character. Therefore, if we remove the hub in this type of network, it may damage or destroy the whole network.

=== Spreading phenomenon ===
The hubs are also responsible for effective spreading of material on network. In an analysis of disease spreading or information flow, hubs are referred to as super-spreaders. Super-spreaders may have a positive impact, such as effective information flow, but also devastating in a case of epidemic spreading such as H1N1 or AIDS. The mathematical models such as model of H1H1 Epidemic prediction &lt;ref&gt;{{Cite journal|first1=Duygu |last1=Balcan |first2=Hao |last2=Hu |first3=Bruno |last3=Goncalves |first4=Paolo |last4=Bajardi |first5=Chiara |last5=Poletto |first6=Jose J.|last6=Ramasco|first7=Daniela|last7=Paolotti|first8=Nicola|last8=Perra |first9=Michele |last9=Tizzoni |first10=Wouter |last10=Van den Broeck|first11=Vittoria |last11=Colizza|first12=Alessandro |last12=Vespignani|date=14 September 2009|title=Seasonal transmission potential and activity peaks of the new influenza A(H1N1): a Monte Carlo likelihood analysis based on human mobility|journal=BMC Medicine |volume=7 |issue=45 |page=29 |doi=10.1186/1741-7015-7-45 |id= |pmid=19744314 |pmc=2755471 |arxiv=0909.2417}}&lt;/ref&gt; may allow us to predict the spread of diseases based on human mobility networks, infectiousness, or social interactions among humans. Hubs are also important in the eradication of disease. In a scale-free network hubs are most likely to be infected, because of the large number of connections they have. After the hub is infected, it broadcasts the disease to the nodes it is linked to. Therefore, the selective immunization of hubs may be the cost-effective strategy in eradication of spreading disease.

== References ==
{{Reflist}}

[[Category:Network theory]]</text>
      <sha1>puf1axb972ztqocwf03phtxloh9vzzb</sha1>
    </revision>
  </page>
  <page>
    <title>Idempotent analysis</title>
    <ns>0</ns>
    <id>59158118</id>
    <revision>
      <id>870813098</id>
      <parentid>870639452</parentid>
      <timestamp>2018-11-27T04:25:54Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="609">In [[mathematical analysis]], '''idempotent analysis''' is the study of [[idempotent semiring]]s, such as the [[tropical semiring]]. The lack of an additive inverse in the semiring is compensated somewhat by the idempotent rule&amp;nbsp;''A''&amp;nbsp;+&amp;nbsp;''A''&amp;nbsp;=&amp;nbsp;''A''.

== References ==
{{reflist}}
{{refbegin}}
* {{cite arXiv |author-link= |eprint=math/0507014v1 |title= The Maslov dequantization, idempotent and tropical mathematics: A brief introduction|class= |last1= Litvinov|first1= G. L.|year= 2005}}
{{refend}}

{{Abstract-algebra-stub}}
{{mathanalysis-stub}}
[[Category:Idempotent analysis| ]]</text>
      <sha1>hw7qrn349grp1z212becpisr6q3sv7k</sha1>
    </revision>
  </page>
  <page>
    <title>Joseph Wedderburn</title>
    <ns>0</ns>
    <id>2969494</id>
    <revision>
      <id>841939412</id>
      <parentid>821286327</parentid>
      <timestamp>2018-05-19T03:03:13Z</timestamp>
      <contributor>
        <username>Hmains</username>
        <id>508734</id>
      </contributor>
      <minor/>
      <comment>standard quote handling in WP;standard Apostrophe/quotation marks in WP;add/change/refine category; MOS fixes using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11505">{{EngvarB|date=August 2014}}
{{Use dmy dates|date=August 2014}}
{{Infobox scientist
|name              = Joseph Wedderburn
|image             = Wedderburn.jpeg
|image_size       = 300px
|caption           = Joseph Henry Maclagan Wedderburn (1882–1948)
|birth_date        = {{birth date|df=yes|1882|02|02}} 
|birth_place       = [[Forfar]], Angus, Scotland 
|death_date        = {{death date and age|df=yes|1948|10|09|1882|02|02}}
|death_place       = [[Princeton, New Jersey]], US
|residence         = US
|citizenship       = American
|nationality       = Scottish
|ethnicity         = &lt;!--not needed, but keep as template placeholder--&gt;
|fields            = Mathematician
|workplaces        = [[Princeton University]]
|alma_mater        = [[University of Edinburgh]]
|doctoral_advisor  = [[George Chrystal]]
|academic_advisors = 
|doctoral_students = [[Merrill Flood]]&lt;br&gt;[[Nathan Jacobson]]
|notable_students  = 
|known_for         = [[Wedderburn-Etherington number]]&lt;br&gt;[[Artin–Wedderburn theorem]]
|author_abbrev_bot = 
|author_abbrev_zoo = 
|influences        = 
|influenced        = 
|awards            = MacDougall-Brisbane Gold Medal,&lt;br&gt; [[Fellow of the Royal Society]]&lt;ref name="frs"&gt;{{Cite journal | last1 = Taylor | first1 = H. S. | title = Joseph Henry Maclagen Wedderburn. 1882-1948 | doi = 10.1098/rsbm.1949.0016 | journal = [[Obituary Notices of Fellows of the Royal Society]] | volume = 6 | issue = 18 | pages = 618–626 | year = 1949 | jstor = 768943| pmid =  | pmc = }}&lt;/ref&gt;
|religion          = 
|signature         =  &lt;!--(filename only)--&gt;
|footnotes         = 
}}
'''Joseph Henry Maclagan Wedderburn''' [[Fellow of the Royal Society of Edinburgh|FRSE]] [[Fellow of the Royal Society of London|FRS]] (2 February 1882, Forfar, [[Angus, Scotland]] – 9 October 1948, [[Princeton, New Jersey]]) was a Scottish mathematician, who taught at [[Princeton University]] for most of his career. A significant [[abstract algebra|algebraist]], he proved that a finite [[division algebra]] is a [[field (mathematics)|field]], and part of the [[Artin–Wedderburn theorem]] on [[simple algebra]]s. He also worked on [[group theory]] and [[matrix algebra]].&lt;ref&gt;{{MacTutor Biography|id=Wedderburn}}&lt;/ref&gt;
&lt;ref&gt;{{MathGenealogy|id=282}}&lt;/ref&gt;

His younger brother was the lawyer [[Ernest Wedderburn]].

==Life==
Joseph Wedderburn was the tenth of fourteen children of Alexander Wedderburn of Pearsie, a physician, and Anne Ogilvie. Educated at [[Forfar Academy]] and [[George Watson's College]], Edinburgh, in 1898 he entered the [[University of Edinburgh]]. In 1903, he published his first three papers, worked as an assistant in the Physical Laboratory of the University, obtained an [[M.A. (Scotland)|MA]] degree with [[First Class Honours]] in mathematics, and was elected a Fellow of the [[Royal Society of Edinburgh]], upon the proposal of [[George Chrystal]], James Gordon MacGregor, [[Cargill Gilston Knott]] and William Peddie.

He then studied briefly at the [[University of Leipzig]] and the [[University of Berlin]], where he met the algebraists [[Ferdinand Georg Frobenius|Frobenius]] and [[Issai Schur|Schur]]. A [[Carnegie Corporation of New York|Carnegie Scholarship]] allowed him to spend the 1904–1905 academic year at the [[University of Chicago]] where he worked with [[Oswald Veblen]], [[E. H. Moore]], and most importantly, [[Leonard Dickson]], who was to become the most important American algebraist of his day.

Returning to Scotland in 1905, Wedderburn worked for four years at the [[University of Edinburgh]] as an assistant to [[George Chrystal]], who supervised his [[D.Sc]], awarded in 1908 for a thesis titled ''On Hypercomplex Numbers''. He gained a PhD in algebra from the University of Edinburgh in 1908.&lt;ref&gt;{{Cite journal|last=J.H.|first=Maclagan-Wedderburn,|date=1908|title=Theory of linear associative algebras|url=http://hdl.handle.net/1842/19081|language=en}}&lt;/ref&gt; From 1906 to 1908, Wedderburn edited the ''[[Proceedings of the Edinburgh Mathematical Society]]''. In 1909, he returned to the United States to become a Preceptor in Mathematics at [[Princeton University]]; his colleagues included [[Luther P. Eisenhart]], [[Oswald Veblen]], [[Gilbert Ames Bliss]], and [[George Birkhoff]].

Upon the outbreak of the [[First World War]], Wedderburn enlisted in the British Army as a private. He was the first person at Princeton to volunteer for that war, and had the longest war service of anyone on the staff. He served with the [[Seaforth Highlanders]] in France, as [[Lieutenant (British Army and Royal Marines)|Lieutenant]] (1914), then as [[Captain (British Army and Royal Marines)|Captain]] of the 10th Battalion (1915–18). While a Captain in the Fourth Field Survey Battalion of the [[Royal Engineers]] in France, he devised [[sound-ranging]] equipment to locate enemy artillery.

He returned to Princeton after the war, becoming Associate Professor in 1921 and editing the ''[[Annals of Mathematics]]'' until 1928.  While at Princeton, he supervised only three PhDs, one of them being [[Nathan Jacobson]]. In his later years, Wedderburn became an increasingly solitary figure and may even have suffered from depression. His isolation after his 1945 early retirement was such that his death from a heart attack was not noticed for several days. His [[Nachlass]] was destroyed, as per his instructions.

Wedderburn received the [[MacDougall-Brisbane Gold Medal and Prize]] from the [[Royal Society of Edinburgh]] in 1921, and was elected to the [[Royal Society of London]] in 1933.&lt;ref name="frs"/&gt;

As to why Wedderburn never married:

{{quote|It seems that an old Scottish tradition required that a man, before marrying, accumulate savings equal to a certain percentage of his annual income. In Wedderburn's case his income had gone up so rapidly that he had never been able to accomplish this.|Hooke, 1984}}

==Work==
In all, Wedderburn published about 40 books and papers, making important advances in the theory of rings, algebras and matrix theory.

In 1905, Wedderburn published a paper that included three claimed proofs of a theorem stating that a noncommutative [[Finite set|finite]] [[division ring]] could not exist. The proofs all made clever use of the interplay between the [[group scheme|additive group]] of a finite [[division algebra]] ''A'', and the [[multiplicative group]] ''A''* = ''A''-&lt;nowiki/&gt;{0}. Parshall (1983) notes that the first of these three proofs had a gap not noticed at the time. Meanwhile, Wedderburn's Chicago colleague Dickson also found a proof of this result but, believing Wedderburn's first proof to be correct, Dickson acknowledged Wedderburn's priority. But Dickson also noted that Wedderburn constructed his second and third proofs only after having seen Dickson's proof. Parshall concludes that Dickson should be credited with the first correct proof.

A corollary to this theorem yields the complete structure of all finite [[projective geometry]]. In their paper on "Non-Desarguesian and non-Pascalian geometries" in the 1907 ''[[Transactions of the American Mathematical Society]]'', Wedderburn and [[Oswald Veblen|Veblen]] showed that in these geometries, [[Pascal's theorem]] is a consequence of [[Desargues' theorem]]. They also constructed finite projective geometries which are neither "Desarguesian" nor "Pascalian" (the terminology is [[David Hilbert|Hilbert]]'s).

Wedderburn's best-known paper was his sole-authored "On hypercomplex numbers," published in the 1907 [[Proceedings of the London Mathematical Society]], and for which he was awarded the D.Sc. the following year. This paper gives a complete classification of [[simple algebra|simple]] and [[semisimple algebra]]s. He then showed that every [[semisimple algebra]] finite-dimensional can be constructed as a direct sum of [[simple algebra]]s and that every [[simple algebra]] is [[isomorphic]] to a [[matrix algebra]] for some [[division ring]]. The [[Artin–Wedderburn theorem]] generalises this result, with the ascending chain condition.

His best known book is his ''[http://www.ams.org/online_bks/coll17/ Lectures on Matrices]'' (1934),&lt;ref&gt;{{cite journal|author=MacDuffee, C. C.|authorlink=Cyrus Colton MacDuffee|title=Wedderburn on Matrices|journal=Bull. Amer. Math. Soc.|year=1935|volume=41|issue=7|pages=471–472|url=http://projecteuclid.org/euclid.bams/1183498283|doi=10.1090/s0002-9904-1935-06117-8}}&lt;/ref&gt; which Jacobson praised as follows:

{{quote|That this was the result of a number of years of painstaking labour is evidenced by the bibliography of 661 items (in the revised printing) covering the period 1853 to 1936. The work is, however, not a compilation of the literature, but a synthesis that is Wedderburn's own. It contains a number of original contributions to the subject.|Nathan Jacobson, quoted in Taylor 1949}}

About Wedderburn's teaching:
{{quote|He was apparently a very shy man and much preferred looking at the blackboard to looking at the students. He had the [[galley proof]]s from his book "Lectures on Matrices" pasted to cardboard for durability, and his "lecturing" consisted of reading this out loud while simultaneously copying it onto the blackboard.|Hooke, 1984}}

== See also ==
*[[Hypercomplex number]]s
*[[Wedderburn–Etherington number]]
*[[Wedderburn's little theorem]]
*[[Division ring|Wedderburn's theorem (division ring)]]
*[[Simple ring|Wedderburn's theorem (simple ring)]]

==References==
{{reflist}}

==Further reading==
*{{cite journal|author=Artin, Emil|title=The influence of J. H. M. Wedderburn on the development of modern algebra|journal=Bull. Amer. Math. Soc.|year=1950|volume=56|issue=1, Part 1|pages=65–72|url=http://projecteuclid.org/euclid.bams/1183514454|authorlink=Emil Artin|doi=10.1090/s0002-9904-1950-09346-x}}
*Robert Hooke (1984) [https://web.archive.org/web/20150310072127/http://www.princeton.edu/~mudd/finding_aids/mathoral/pmc21.htm Recollections of Princeton, 1939–1941]
*[[Karen Parshall]] (1983) "In pursuit of the finite division algebra theorem and beyond: Joseph H M Wedderburn, [[Leonard Dickson]], and [[Oswald Veblen]]," ''Archives of International History of Science 33'': 274–99.
* Karen Parshall (1985) "Joseph H. M. Wedderburn and the structure theory of algebras," ''Archive for History of Exact Sciences 32'': 223–349.
* Karen Parshall (1992) "New Light on the Life and Work of Joseph Henry Maclagan Wedderburn (1882–1948)," in Menso Folkerts ''et al.'' (eds.): ''Amphora: Festschrift für [[Hans Wußing]] zu seinem 65. Geburtstag'', Birkhäuser Verlag, 523–537.

{{Authority control}}

{{DEFAULTSORT:Wedderburn, Joseph}}
[[Category:1882 births]]
[[Category:1948 deaths]]
[[Category:20th-century American mathematicians]]
[[Category:People from Forfar]]
[[Category:People from Edinburgh]]
[[Category:People educated at Forfar Academy]]
[[Category:People educated at George Watson's College]]
[[Category:Alumni of the University of Edinburgh]]
[[Category:Leipzig University alumni]]
[[Category:Humboldt University of Berlin alumni]]
[[Category:University of Chicago alumni]]
[[Category:Academics of the University of Edinburgh]]
[[Category:Princeton University faculty]]
[[Category:Fellows of the Royal Society of Edinburgh]]
[[Category:Fellows of the Royal Society]]
[[Category:Seaforth Highlanders officers]]
[[Category:Royal Engineers officers]]
[[Category:Algebraists]]
[[Category:British Army personnel of World War I]]
[[Category:Scottish emigrants to the United States]]
[[Category:Scottish mathematicians]]</text>
      <sha1>ncml18fhcxejbt965e6m3mtn18pq7fs</sha1>
    </revision>
  </page>
  <page>
    <title>Ken Ono</title>
    <ns>0</ns>
    <id>9753326</id>
    <revision>
      <id>867812077</id>
      <parentid>864927745</parentid>
      <timestamp>2018-11-08T04:21:51Z</timestamp>
      <contributor>
        <username>Alaney2k</username>
        <id>209266</id>
      </contributor>
      <minor/>
      <comment>/* top */US =&gt; Americans</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11521">{{Distinguish|Ken Bono}}
{{BLP sources|date=March 2013}}

{{ Infobox scientist
| name              = Ken Ono
| image             = TIFF.jpg
| image_size        =
| caption           = Ken Ono in 2015 at the Toronto International Film Festival
| birth_date        = {{birth date and age|1968|3|20|df=y}}
| birth_place       =
| death_date        =
| death_place       =
| nationality       = [[United States]]
| fields            = [[Mathematics]]
| work_institutions =[[Emory University]]&lt;br&gt; [[University of Wisconsin–Madison]]
| alma_mater        = [[University of California, Los Angeles|UCLA]]&lt;br&gt;[[University of Chicago]]
| doctoral_advisor  = [[Basil Gordon]]
| doctoral_students =
| notable_students  = [[Daniel Kane (mathematician)|Daniel Kane]]&lt;br&gt;[[Karl Mahlburg]]&lt;br&gt;[[Robert Schneider]]&lt;br&gt; [[Gwynneth Coogan]]
| known_for         =
}}
'''Ken Ono''' (born 20 March 1968) is a [[Japanese people|Japanese]]-[[Americans|American]] [[mathematician]] who specializes in [[number theory]], especially in [[integer partition]]s, [[modular form]]s,  [[Umbral moonshine]], and the fields of interest to [[Srinivasa Ramanujan]]. He was the Manasse Professor of Letters and Science and the Hilldale Professor of Mathematics at the [[University of Wisconsin–Madison]].

He is currently the [[Asa Griggs Candler]] Professor of Mathematics at [[Emory University]] and the Vice President of the [[American Mathematical Society]].

==Early life and education==
Ono is the son of mathematician [[Takashi Ono (mathematician)|Takashi Ono]], who emigrated from Japan to the United States after World War II. His older brother, immunologist and university president [[Santa J. Ono]], was born while Takashi Ono was in Canada working at the [[University of British Columbia]], but by the time Ken Ono was born the family had returned to the US for a position at the [[University of Pennsylvania]].&lt;ref&gt;{{citation|url=http://magazine.uc.edu/issues/0413/ono.html|title=Getting to know Ono|journal=UC Magazine|publisher=University of Cincinnati|date=April 2013|first=John|last=Bach}}. Although primarily a profile of Ono's brother, this article also includes some details of Ono's early family life.&lt;/ref&gt; In the 1980s, Ono attended [[Towson High School]], but he dropped out.  He later enrolled at the [[University of Chicago]] without a high school diploma. There he raced bicycles, and he was a member of the [[Pepsi]]–[[Miyata (bicycle)|Miyata]] Cycling Team.

He received his BA from the [[University of Chicago]] in 1989, where he was a member of the [[Psi Upsilon]] fraternity. He earned his PhD in 1993 at [[University of California, Los Angeles|UCLA]] where his advisor was [[Basil Gordon]].&lt;ref&gt;{{MathGenealogy|id=39997}}&lt;/ref&gt; Initially he planned to study medicine, but later switched to mathematics. He attributes his interest in mathematics to his father.&lt;ref&gt;{{Cite web|url = http://gonitsora.com/in-conversation-with-prof-ken-ono/|title = In conversation with Prof. Ken Ono: Gonit Sora|date = 23 February 2015|accessdate = 16 March 2015|website = [[Gonit Sora]]|publisher = |last = Saikia|first = Manjil}}&lt;/ref&gt;

==Career and research==
Ono's contributions include several monographs and over 160 research and popular articles in [[number theory]], [[combinatorics]], and [[algebra]]. He is considered to be an expert in the theory of [[integer partition]]s and [[modular form]]s.  In 2000 he greatly expanded [[Ramanujan]]'s theory of [[Ramanujan's congruences|partition congruences]], and in work with [[Kathrin Bringmann]] he has made important contributions to the theory of [[Maass form]]s, functions which include Ramanujan's [[mock theta function]]s as examples. In 2007 [[Don Zagier]] gave a Seminar Bourbaki address on the work of Bringmann, Ono, and [[Zwegers]] on the mock theta functions. The 2009 [[SASTRA]] Ramanujan Prize, awarded to a young mathematician under the age of 32, was awarded to Kathrin Bringmann for this joint work with Ono. Recently he and his collaborators have announced a proof of the famous Umbral Moonshine Conjecture.

Ono has received many awards for his research. In April 2000 he received the Presidential Career Award ([[PECASE]]) from [[Bill Clinton]] in a ceremony at the White House, and in June 2005 he received the [[National Science Foundation]] Director's Distinguished Teaching Scholar Award at the [[National Academy of Science]]. He has also won a [[Sloan Fellowship]], a Packard Fellowship, and a [[Guggenheim Fellowship]]. In 2012 he became a fellow of the [[American Mathematical Society]].&lt;ref&gt;[http://www.ams.org/profession/fellows-list List of Fellows of the American Mathematical Society], retrieved 2013-03-20.&lt;/ref&gt;

In 2011 and 2015 Ono gave TED talks.&lt;ref&gt;{{cite web | title = -Infinity to Infinity, TED|
url=http://tedxtalks.ted.com/video/TEDxEmory-Dr-Ken-Ono-Infinity-t}}&lt;/ref&gt;&lt;ref&gt;{{cite web | title = Live mathematically, but not by the numbers, TED|
url=https://www.youtube.com/watch?v=20zfJZYlDDE}}&lt;/ref&gt;

In a joint work with [[Jan Bruinier]], he recently discovered a finite formula for computing partition numbers.&lt;ref&gt;{{cite web|last=Kavassalis|first=Sarah|title=Finite formula found for partition numbers|url=http://blogs.plos.org/badphysics/2011/01/20/ono/|work=The Language of Bad Physics|accessdate=1 March 2011}}&lt;/ref&gt;

He stars in the 2013 docudrama "The genius of Srinivasa Ramanujan".&lt;ref&gt;{{cite web | title=The genius of Srinivasa Ramanujan, IMDB.com| url=https://www.imdb.com/title/tt2861842/?ref_=fn_al_tt_1}}&lt;/ref&gt;

He is profiled in the May 2014 issue of [[Scientific American]].&lt;ref&gt;{{cite web | title=The Oracle, Scientific American|
url=http://www.scientificamerican.com/article/one-of-srinivasa-ramanujans-neglected-manuscripts-has-helped-solve-long-standing-mathematical-mysteries/}}&lt;/ref&gt; He was an Associate Producer and the mathematical consultant for the movie ''[[The Man Who Knew Infinity (film)|The Man Who Knew Infinity]]'' based on [[The Man Who Knew Infinity|Ramanujan's biography]] written by [[Robert Kanigel]].&lt;ref&gt;{{Cite web|url = http://gonitsora.com/in-conversation-with-prof-ken-ono/|title = In conversation with Prof. Ken Ono: Gonit Sora|date = 23 February 2015|accessdate = 16 March 2015|website = [[Gonit Sora]]|publisher = |last = Saikia|first = Manjil}}&lt;/ref&gt;

==A framework for the Rogers–Ramanujan identities==
In April 2014 Ono announced that he and two others had found a framework for the Rogers–Ramanujan identities and their arithmetic properties, solving a long-standing mystery stemming from the work of Ramanujan. The findings yield a treasure trove of algebraic numbers and formulas to access them. Ono's co-authors for this work were S. Ole Warnaar of the University of Queensland and Michael Griffin, an Emory University graduate student. Their work made world news that year and was ranked 15th among the top 100 stories of 2014 in science in [[Discover (magazine)|''Discover'' magazine]].&lt;ref&gt;{{cite web | title = Mother lode of mathematical identities discovered, Discover| url=http://discovermagazine.com/2015/jan-feb/15-a-beautiful-find}}&lt;/ref&gt;

After 15 years of focusing on the Rogers–Ramanujan identities, Warnaar had found a way to embed them into a much larger class of similar identities using [[representation theory]]. When Ono saw Warnaar's work, "It just clicked," Ono recalls. "Now we can extract infinitely many functions whose values are algebraic numbers." &lt;ref&gt;{{cite journal| first1=Michael J. | last1=Griffin | first2=Ken | last2=Ono | first3=S. Ole | last3=Warnaar | title=A framework of Rogers–Ramanujan identities and their arithmetic properties | arxiv=1401.7718 | year=2014 | doi=10.1215/00127094-3449994 | journal=Duke Mathematical Journal}}&lt;/ref&gt;

==Personal life==
In recent years, Ono has resumed athletic training as a runner, swimmer and cyclist; since 2012, he has competed in triathlons as a member of Team USA. Ken Ono now lives in Atlanta, Georgia, with his wife Erika, daughter Aspen, and son Sage.

==Honors and awards==

* National Security Agency Young Investigator (1997)
* National Science Foundation CAREER Award  (1998)
* Alfred P. Sloan Foundation Research Fellowship (1999)
* David and Lucile Packard Research Fellowship (1999)
* Presidential Early Career Award (awarded by Clinton) (2000)
* National Science Foundation CBMS Distinguished Lecturer (2003)
* John S. Guggenheim Fellowship (2003)
* Manasse Professor of Letters and Science, U. Wisconsin (2004–2011)
* National Science Foundation Director's Distinguished Teaching Scholar Award (2005)
* Hilldale Professor of Mathematics, U. Wisconsin (2008–2011)
* Candler Professor of Mathematics, Emory University (2010–present)
* Fellow of the American Mathematical Society (2013)
* Albert E. Levy Award for Scientific Research (2014)
* MAA George Polya Distinguished Lecturer (2016–2017)
* International Science Film Festival Technical Award (2017)
* Eleanor Main Graduate Mentor Award (2017)
* Prose Award (Best Scholarly Book in Mathematics), Awarded by the American Publishers, (2018)

==Editorial boards==

Ono is on the editorial board of several journals:
* [https://link.springer.com/journal/26 Annals of Combinatorics]
* [[Bulletin of the American Mathematical Society]]
* [http://intlpress.com/site/pub/pages/journals/items/cntp/_home/_main/  Communications in Number Theory and Physics]
* [http://www.integers-ejcnt.org/ Integers]
* [http://ijmm.dixiewpublishing.com/ International Journal of Modern Mathematics]
* [http://www.worldscinet.com/ijnt/mkt/editorial.shtml The International Journal of Number Theory]
* [http://www.m-hikari.com/imf.html The International Mathematical Forum]
* [http://www.involvemath.org Involve]
*[https://www.novapublishers.com/catalog/product_info.php?products_id=7065 Journal of Combinatorics and Number Theory]
* [https://link.springer.com/journal/11139  The Ramanujan Journal]
* [https://www.springer.com/mathematics/journal/40687  Research in the Mathematical Sciences] (Editor-in-Chief)
* [https://www.springer.com/mathematics/numbers/journal/40993 Research in Number Theory] (Editor-in-Chief)

==See also==
*[[Ramanujan's ternary quadratic form]]

==References==
{{Reflist}}

==External links==
* [http://www.mathcs.emory.edu/~ono Ken Ono's homepage]
* [http://gonitsora.com/national-mathematics-day-conversation-prof-ken-ono/ Conversation with Ken Ono] at [[Gonit Sora]].
* [http://scienceandfilm.org/articles/2698/ken-ono-robert-schneider-why-ramanujan-matters Ken Ono on ''The Man Who Knew Infinity'' and why Ramanujan Matters]
* {{cite book|last1= Ono |first1= Ken |authorlink1= Ken Ono |last2= Aczel|first2= Amir D. |authorlink2= Amir Aczel |title= My Search for Ramanujan: How I Learned to Count |date= 2016-04-13 |publisher= [[Springer Science+Business Media|Springer]] |isbn= 978-3319255668 |url= https://www.amazon.com/My-Search-Ramanujan-Learned-Count/dp/3319255665#reader_3319255665}}

{{Authority control}}

{{DEFAULTSORT:Ono, Ken}}
[[Category:1968 births]]
[[Category:Living people]]
[[Category:Combinatorialists]]
[[Category:Number theorists]]
[[Category:20th-century American mathematicians]]
[[Category:21st-century American mathematicians]]
[[Category:Towson High School alumni]]
[[Category:University of Chicago alumni]]
[[Category:Emory University faculty]]
[[Category:University of Wisconsin–Madison faculty]]
[[Category:American scientists of Japanese descent]]
[[Category:Guggenheim Fellows]]
[[Category:Fellows of the American Mathematical Society]]
[[Category:Place of birth missing (living people)]]</text>
      <sha1>q9zi481r44uol8oczuzr5ea2n9xcwfe</sha1>
    </revision>
  </page>
  <page>
    <title>Kolmogorov equations</title>
    <ns>0</ns>
    <id>29973465</id>
    <revision>
      <id>852793312</id>
      <parentid>852793166</parentid>
      <timestamp>2018-07-31T11:52:04Z</timestamp>
      <contributor>
        <ip>129.240.43.144</ip>
      </contributor>
      <comment>/* An example from biology */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4727">In [[probability theory]], '''Kolmogorov equations''', including '''[[Kolmogorov forward equations (disambiguation)|Kolmogorov forward equations]]'''&lt;!--intentional link to DAB page--&gt; and '''[[Kolmogorov backward equations]]''', characterize ''[[Stochastic process|stochastic processes]]''. In particular, they describe how the probability that a stochastic process is in a certain state changes over time.

==Diffusion Processes vs. Jump Processes==

Writing in 1931, [[Andrei Kolmogorov]] started from the theory of discrete time Markov processes, which are described by the [[Chapman-Kolmogorov equation]], and sought to derive a theory of continuous time Markov processes by extending this equation.  He found that there are two kinds of continuous time Markov processes, depending on the assumed behavior over small intervals of time:

If you assume that "in a small time interval there is an overwhelming probability that the state will remain unchanged; however, if it changes, the change may be radical",&lt;ref name=f49 /&gt; then you are led to what are called [[jump process]]es.

The other case leads to processes such as those "represented by [[Itō diffusion|diffusion]] and by [[Brownian motion]]; there it is certain that some change will occur in any time interval, however small; only, here it is certain that the changes during small time intervals will be also small".&lt;ref name=f49 /&gt;

For each of these two kinds of processes, Kolmogorov derived a forward and a backward system of equations (four in all).

==History==

The equations are named after [[Andrei Kolmogorov]] since they were highlighted in his 1931 foundational work.&lt;ref name="k31"&gt;Andrei Kolmogorov, "Über die analytischen Methoden in der Wahrscheinlichkeitsrechnung" (On Analytical Methods in the Theory of Probability), 1931, [http://www.springerlink.com/content/v724507673277262/fulltext.pdf]&lt;/ref&gt;

[[William Feller]], in 1949, used the names "forward equation" and "backward equation" for his more general version of the Kolmogorov's pair,
in both jump and diffusion processes.&lt;ref name="f49"&gt;Feller, W. (1949) [http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;handle=euclid.bsmsp/1166219215 "On the Theory of Stochastic Processes, with Particular Reference to Applications"], [http://projecteuclid.org/euclid.bsmsp/1166219193 ''Proceedings of the (First) Berkeley Symposium on Mathematical Statistics and Probability''] pp 403-432.&lt;/ref&gt; Much later, in 1956, he referred to the equations for the jump process as "Kolmogorov forward equations" and "Kolmogorov backward equations".&lt;ref name=f57&gt;William Feller, 1957. On Boundaries and Lateral Conditions for the Kolmogorov Differential Equations [https://www.jstor.org/stable/1970064]&lt;/ref&gt;

Other authors, such as [[Motoo Kimura]] referred to the diffusion (Fokker–Planck) equation as Kolmogorov forward equation, a name that has persisted.&lt;ref name=m57&gt;Kimura, Motoo (1957) "Some Problems of Stochastic Processes in Genetics", ''The Annals of Mathematical Statistics'', 28 (4), 882-901 {{jstor|2237051}}&lt;/ref&gt;

==The modern view==

*In the context of a [[continuous-time Markov process]] with [[Jump process|jumps]], see [[Kolmogorov equations (Markov jump process)]]. In particular, in [[natural science]]s the forward equation is also known as [[master equation]].
*In the context of a [[diffusion]] process, for the backward Kolmogorov equations see [[Kolmogorov backward equations (diffusion)]]. The forward Kolmogorov equation is also known as [[Fokker–Planck equation]].

==An example from biology==

One example from biology is given below:&lt;ref name="Logan"&gt;Logan, J. David and Wolesensky, Willian R. Mathematical methods in biology. Pure and Applied Mathematics: a Wiley-interscience Series of Texts, Monographs, and Tracts. John Wiley&amp; Sons, Inc. 2009. pp. 325-327.&lt;/ref&gt;

: &lt;math&gt;p_n' (t)= (n-1)\beta p_{n-1}(t) - n\beta p_{n}(t) &lt;/math&gt;

This equation is applied to model [[population growth]] with [[birth]]. Where &lt;math&gt; n &lt;/math&gt; is the population index, with reference the initial population, &lt;math&gt; \beta &lt;/math&gt; is the birth rate, and finally &lt;math&gt;p_n(t)=Pr(N(t)=n)&lt;/math&gt;, i.e. the [[probability]] of achieving a certain [[population size]].

The analytical solution is:&lt;ref name="Logan"/&gt;

: &lt;math&gt; p_n(t)= (n-1)\beta\mathrm{e}^{-n\beta t} \int_0^t \! p_{n-1}(s)\,\mathrm{e}^{n\beta s}\mathrm{d}s &lt;/math&gt;

This is a formula for the density &lt;math&gt;p_n(t)&lt;/math&gt; in terms of the preceding ones, i.e. &lt;math&gt;p_{n-1}(t)&lt;/math&gt;.

== References ==
{{reflist}}

[[Category:Articles created via the Article Wizard]]
[[Category:Markov processes]]
[[Category:Stochastic models]]
[[Category:Mathematical and theoretical biology]]
[[Category:Population models]]</text>
      <sha1>nl4cg0u8ou3nsgp8tnwyp9cdtzyio1p</sha1>
    </revision>
  </page>
  <page>
    <title>Linear map</title>
    <ns>0</ns>
    <id>18102</id>
    <revision>
      <id>869201715</id>
      <parentid>869200949</parentid>
      <timestamp>2018-11-17T01:43:58Z</timestamp>
      <contributor>
        <ip>89.107.5.192</ip>
      </contributor>
      <comment>inline</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="33571">{{Redirect|Linear transformation|fractional linear transformations|Möbius transformation}}
{{distinguish|linear function}}
In [[mathematics]], a '''linear map''' (also called a '''linear mapping''', '''linear [[Transformation (function)|transformation]]''' or, in some contexts, '''linear function''') is a [[Map (mathematics)|mapping]] {{math|''V'' → ''W''}} between two [[Module (mathematics)|module]]s (including [[vector space]]s) that preserves (in the sense defined below) the operations of addition and [[scalar (mathematics)|scalar]] multiplication.  

An important special case is when {{math|1=''V'' = ''W''}}, in which case the map is called a '''linear operator''',&lt;ref&gt;Linear transformations of {{mvar|V}} into {{mvar|V}} are often called ''linear operators'' on {{mvar|V}} {{harvnb|Rudin|1976|page=207}}&lt;/ref&gt; or an [[endomorphism]] of&amp;nbsp;{{math|''V''}}. Sometimes the term ''[[linear function]]'' has the same meaning as ''linear map'', while in [[analytic geometry]] it does not.

A linear map always [[map (mathematics)|maps]] linear subspaces onto linear subspaces (possibly of a lower dimension);&lt;ref&gt;{{harvnb|Rudin|1991|page=14}}&lt;br&gt; Here are some properties of linear mappings &lt;math display="inline"&gt;\Lambda: X \to Y&lt;/math&gt; whose proofs are so easy that we omit them; it is assumed that &lt;math display="inline"&gt;A \subset X&lt;/math&gt; and &lt;math display="inline"&gt;B \subset Y&lt;/math&gt;:
{{ordered list
 | list-style-type=lower-alpha
 | &lt;math display="inline"&gt;\Lambda 0 = 0.&lt;/math&gt;
 | If {{mvar|A}} is a subspace (or a [[convex set]], or a [[balanced set]]) the same is true of &lt;math display="inline"&gt;\Lambda(A)&lt;/math&gt;
 | If {{mvar|B}} is a subspace (or a convex set, or a balanced set) the same is true of &lt;math display="inline"&gt;\Lambda^{-1}(B)&lt;/math&gt;
 | In particular, the set:
: &lt;math&gt;\Lambda^{-1}(\{0\}) = \{x \in X: \Lambda x = 0\} = {N}(\Lambda)&lt;/math&gt;

is a subspace of {{mvar|X}}, called the ''null space'' of &lt;math display="inline"&gt;\Lambda&lt;/math&gt;.
}}&lt;/ref&gt; for instance it maps a plane through the [[origin (geometry)|origin]] to a [[plane (geometry)|plane]], [[straight line]] or [[point (geometry)|point]]. Linear maps can often be represented as [[matrix (mathematics)|matrices]], and simple examples include [[rotation and reflection linear transformations]].

In the language of [[abstract algebra]], a linear map is a [[module homomorphism]].  In the language of [[category theory]] it is a [[morphism]] in the [[category of modules]] over a given [[Ring (mathematics)|ring]].

== Definition and first consequences ==
Let &lt;math display="inline"&gt;V&lt;/math&gt; and &lt;math display="inline"&gt;W&lt;/math&gt; be vector spaces over the same [[field (mathematics)|field]] &lt;math display="inline"&gt;\mathbf{K}.&lt;/math&gt; A function &lt;math display="inline"&gt;f : V \to W&lt;/math&gt; is said to be a ''linear map'' if for any two vectors &lt;math display="inline"&gt;\mathbf{u}, \mathbf{v} \in V&lt;/math&gt; and any scalar &lt;math display="inline"&gt;c \in \mathbf{K}&lt;/math&gt; the following two conditions are satisfied:

{|
|-
| style="padding:0 20pt"|&lt;math&gt;f(\mathbf{u}+\mathbf{v}) = f(\mathbf{u})+f(\mathbf{v})&lt;/math&gt;
| [[Additive map|additivity]] / operation of addition
|-
| style="padding:0 20pt"|&lt;math&gt;f(c \mathbf{u}) = c f(\mathbf{u})&lt;/math&gt;
| [[Homogeneous function|homogeneity]] of degree 1 / operation of scalar multiplication
|}

Thus, a linear map is said to be ''operation preserving''.  In other words, it does not matter whether you apply the linear map before or after the operations of addition and scalar multiplication.

This is equivalent to requiring the same for any linear combination of vectors, i.e. that for any vectors &lt;math display="inline"&gt;\mathbf{u}_1, \ldots, \mathbf{u}_n \in V&lt;/math&gt; and scalars &lt;math display="inline"&gt;c_1, \ldots, c_n \in \mathbf{K},&lt;/math&gt; the following equality holds:&lt;ref&gt;{{harvnb|Rudin|1991|page=14}}. Suppose now that {{mvar|X}} and {{mvar|Y}} are vector spaces ''over the same scalar field''. A mapping &lt;math display="inline"&gt;\Lambda: X \to Y&lt;/math&gt; is said to be ''linear'' if &lt;math display="inline"&gt;\Lambda(\alpha x + \beta y) = \alpha\Lambda x + \beta\Lambda y&lt;/math&gt; for all &lt;math display="inline"&gt;x, y \in X&lt;/math&gt; and all scalars &lt;math display="inline"&gt;\alpha&lt;/math&gt; and &lt;math display="inline"&gt;\beta&lt;/math&gt;. Note that one often writes &lt;math display="inline"&gt;\Lambda x&lt;/math&gt;, rather than &lt;math display="inline"&gt;\Lambda(x)&lt;/math&gt;, when &lt;math display="inline"&gt;\Lambda&lt;/math&gt; is linear.&lt;/ref&gt;&lt;ref&gt;{{harvnb|Rudin|1976|page=206}}. A mapping {{mvar|A}} of a vector space {{mvar|X}} into a vector space {{mvar|Y}} is said to be a ''linear transformation'' if: &lt;math display="inline"&gt;A\left(\bf{x}_1 + \bf{x}_2\right) = A\bf{x}_1 + A\bf{x}_2,\  A(c\bf{x}) = cA\bf{x}&lt;/math&gt; for all &lt;math display="inline"&gt;\bf{x}, \bf{x}_1, \bf{x}_2 \in X&lt;/math&gt; and all scalars {{mvar|c}}. Note that one often writes &lt;math display="inline"&gt;A\bf{x}&lt;/math&gt; instead of &lt;math display="inline"&gt;A(\bf {x})&lt;/math&gt; if {{mvar|A}} is linear.&lt;/ref&gt;

: &lt;math&gt;f\left(c_1 \mathbf{u}_1 + \cdots + c_n \mathbf{u}_n\right) = c_1 f\left(\mathbf{u}_1\right) + \cdots + c_n f\left(\mathbf{u}_n\right).&lt;/math&gt;

Denoting the zero elements of the vector spaces &lt;math display="inline"&gt;V&lt;/math&gt; and &lt;math display="inline"&gt;W&lt;/math&gt; by &lt;math display="inline"&gt;\mathbf{0}_V&lt;/math&gt; and &lt;math display="inline"&gt;\mathbf{0}_W&lt;/math&gt; respectively, it follows that &lt;math display="inline"&gt;f\left(\mathbf{0}_V\right) = \mathbf{0}_W.&lt;/math&gt; Let &lt;math display="inline"&gt;c = 0&lt;/math&gt; and &lt;math display="inline"&gt;\mathbf{v} \in V&lt;/math&gt; in the equation for homogeneity of degree 1:

:&lt;math&gt;f\left(\mathbf{0}_V\right) = f\left(0\mathbf{v}\right) = 0f(\mathbf{v}) = \mathbf{0}_W.&lt;/math&gt;

Occasionally, &lt;math display="inline"&gt;V&lt;/math&gt; and &lt;math display="inline"&gt;W&lt;/math&gt; can be considered to be vector spaces over different fields.  It is then necessary to specify which of these ground fields is being used in the definition of "linear". If &lt;math display="inline"&gt;V&lt;/math&gt; and &lt;math display="inline"&gt;W&lt;/math&gt; are considered as spaces over the field &lt;math display="inline"&gt;\mathbf{K}&lt;/math&gt; as above, we talk about &lt;math display="inline"&gt;\mathbf{K}&lt;/math&gt;-linear maps. For example, the [[complex conjugate|conjugation]] of [[complex numbers]] is an &lt;math display="inline"&gt;\mathbf{R}&lt;/math&gt;-linear map &lt;math display="inline"&gt;\mathbf{C} \to \mathbf{C}&lt;/math&gt;, but it is not &lt;math display="inline"&gt;\mathbf{C}&lt;/math&gt;-linear.

A linear map &lt;math display="inline"&gt;V \to \mathbf{K}&lt;/math&gt; with &lt;math display="inline"&gt;\mathbf{K}&lt;/math&gt; viewed as a vector space over itself is called a [[linear functional]].&lt;ref&gt;{{harvnb|Rudin|1991|page=14}}. Linear mappings of {{mvar|X}} onto its scalar field are called ''linear functionals''.&lt;/ref&gt;

These statements generalize to any left-module &lt;math display="inline"&gt;{}_R M&lt;/math&gt; over a ring &lt;math display="inline"&gt;R&lt;/math&gt; without modification, and to any right-module upon reversing of the scalar multiplication.

== Examples ==
* The zero map {{nowrap|''x'' ↦ 0}} between two left-modules (or two right-modules) over the same ring is always linear.
* The [[identity function|identity map]] on any module is a linear operator.
* Any [[homothecy]] centered in the origin of a vector space, &lt;math display="inline"&gt;v \mapsto cv&lt;/math&gt; where ''c'' is a scalar, is a linear operator.  This does not hold in general for modules, where such a map might only be [[semilinear map|semilinear]].
* For real numbers, the map {{nowrap|''x'' ↦ ''x''&lt;sup&gt;2&lt;/sup&gt;}} is not linear.
* For real numbers, the map {{nowrap|''x'' ↦ ''x'' + 1}} is not linear (but is an [[affine transformation]]; {{nowrap|1=''y'' = ''x'' + 1}} is a [[linear equation]], as the term is used in [[analytic geometry]].)
* If ''A'' is a real {{nowrap|''m'' × ''n''}} [[matrix (mathematics)|matrix]], then ''A'' defines a linear map from  '''R'''&lt;sup&gt;''n''&lt;/sup&gt; to '''R'''&lt;sup&gt;''m''&lt;/sup&gt; by sending the [[column vector]] {{nowrap|'''x''' ∈ '''R'''&lt;sup&gt;''n''&lt;/sup&gt;}} to the column vector {{nowrap|''A'''''x''' ∈ '''R'''&lt;sup&gt;''m''&lt;/sup&gt;}}. Conversely, any linear map between [[finite-dimensional]] vector spaces can be represented in this manner; see the [[#Matrices|following section]].
* [[Derivative|Differentiation]] defines a linear map from the space of all differentiable functions to the space of all functions. It also defines a linear operator on the space of all [[smooth function]]s.
* The (definite) [[integral]] over some [[interval (mathematics)|interval]] ''I'' is a linear map from the space of all real-valued integrable functions on ''I'' to '''R'''.
* The (indefinite) [[integral]] (or [[antiderivative]]) with a fixed starting point defines a linear map from the space of all real-valued integrable functions on '''R''' to the space of all real-valued, differentiable, functions on '''R'''. Without a fixed starting point, an exercise in group theory will show that the antiderivative maps to the [[quotient space]] of the differentiables over the [[equivalence relation]], "differ by a constant", which yields an identity class of the constant valued functions &lt;math display="inline"&gt;\left(\,\int\!:\  I(\Re) \ \to\  D(\Re)/\Re\,\right)&lt;/math&gt;.
* If ''V'' and ''W'' are finite-dimensional vector spaces over a field ''F'', then functions that send linear maps {{nowrap|''f'' : ''V'' → ''W''}} to {{nowrap|dim&lt;sub&gt;''F''&lt;/sub&gt;(''W'') × dim&lt;sub&gt;''F''&lt;/sub&gt;(''V'')}} matrices in the way described in the sequel are themselves linear maps (indeed [[linear isomorphism]]s).
* The [[expected value]] of a [[Random variable#Definition|random variable]] (which is in fact a function, and as such a member of a vector space) is linear, as for random variables ''X'' and ''Y'' we have {{nowrap|1=E[''X'' + ''Y''] = E[''X''] + E[''Y'']}} and {{nowrap|1=E[''aX''] = ''a''E[''X'']}}, but the [[variance]] of a random variable is not linear.

&lt;gallery widths=300 heights=200&gt;
File:Streckung eines Vektors.gif|The function &lt;math display="inline"&gt;f:\R^2 \to \R^2&lt;/math&gt; with &lt;math display="inline"&gt;f(x, y) = (2x, y)&lt;/math&gt; is a linear map. This function scales the &lt;math display="inline"&gt;x&lt;/math&gt; component of a vector by the factor &lt;math display="inline"&gt;2&lt;/math&gt;.
File:Streckung der Summe zweier Vektoren.gif|The function is additive: It doesn't matter whether first vectors are added and then mapped or whether they are mapped and finally added: &lt;math display="inline"&gt;f(a + b) = f(a) + f(b)&lt;/math&gt;
File:Streckung homogenitaet Version 3.gif|The function is homogeneous: It doesn't matter whether a vector is first scaled and then mapped or first mapped and then scaled: &lt;math display="inline"&gt;f(\lambda a) = \lambda f(a)&lt;/math&gt;
&lt;/gallery&gt;

== Matrices ==
{{main|Transformation matrix}}

If ''V'' and ''W'' are [[finite-dimensional]] vector spaces and a [[basis of a vector space|basis]] is defined for each vector space, then every linear map from ''V'' to ''W'' can be represented by a [[matrix (mathematics)|matrix]].&lt;ref&gt;{{harvnb|Rudin|1976|page=210}}

Suppose &lt;math display="inline"&gt;\left\{\bf{x}_1, \ldots, \bf{x}_n\right\}&lt;/math&gt; and &lt;math display="inline"&gt;\left\{\bf{y}_1, \ldots, \bf{y}_m\right\}&lt;/math&gt; are bases of vector spaces {{mvar|X}} and {{mvar|Y}}, respectively. Then every &lt;math display="inline"&gt;A \in L(X, Y)&lt;/math&gt; determines a set of numbers &lt;math display="inline"&gt;a_{i,j}&lt;/math&gt; such that
: &lt;math&gt;A\bf{x}_j = \sum_{i=1}^m a_{i,j}\bf{y}_i\quad (1 \leq j \leq n).&lt;/math&gt;

It is convenient to represent these numbers in a rectangular array of {{mvar|m}} rows and {{mvar|n}} columns, called an {{mvar|m}} ''by'' {{mvar|n}} ''matrix'':
: &lt;math&gt;[A] = \begin{bmatrix}
  a_{1,1} &amp; a_{1,2} &amp; \ldots &amp; a_{1,n} \\
  a_{2,1} &amp; a_{2,2} &amp; \ldots &amp; a_{2,n} \\ 
   \vdots &amp;  \vdots &amp; \ddots &amp;  \vdots \\
  a_{m,1} &amp; a_{m,2} &amp; \ldots &amp; a_{m,n}
\end{bmatrix}&lt;/math&gt;

Observe that the coordinates &lt;math display="inline"&gt;a_{i,j}&lt;/math&gt; of the vector &lt;math display="inline"&gt;A{\bf x}_j&lt;/math&gt; (with respect to the basis &lt;math display="inline"&gt;\{\bf{y}_1, \ldots, \bf{y}_m\}&lt;/math&gt;) appear in the ''j''&lt;sup&gt;th&lt;/sup&gt; column of &lt;math display="inline"&gt;[A]&lt;/math&gt;. The vectors &lt;math display="inline"&gt;A{\bf x}_j&lt;/math&gt; are therefore sometimes called the ''column vectors'' of &lt;math display="inline"&gt;[A]&lt;/math&gt;. With this terminology, the ''range'' of {{mvar|A}} ''is spanned by the column vectors of &lt;math display="inline"&gt;[A]&lt;/math&gt;''.
&lt;/ref&gt; This is useful because it allows concrete calculations. Matrices yield examples of linear maps: if ''A'' is a real {{nowrap|''m'' × ''n''}} matrix, then {{nowrap|1=''f''('''x''') = ''A'''''x'''}} describes a linear map {{nowrap|'''R'''&lt;sup&gt;''n''&lt;/sup&gt; → '''R'''&lt;sup&gt;''m''&lt;/sup&gt;}} (see [[Euclidean space]]).

Let {'''v'''&lt;sub&gt;1&lt;/sub&gt;, …, '''v'''&lt;sub&gt;''n''&lt;/sub&gt;} be a basis for ''V''.  Then every vector '''v''' in ''V'' is uniquely determined by the coefficients ''c''&lt;sub&gt;1&lt;/sub&gt;, …, ''c''&lt;sub&gt;''n''&lt;/sub&gt; in the field '''R''':

: &lt;math&gt;c_1 \mathbf{v}_1 + \cdots + c_n \mathbf{v}_n.&lt;/math&gt;

If {{nowrap|''f'' : ''V'' → ''W''}} is a linear map,

: &lt;math&gt;f\left(c_1 \mathbf{v}_1 + \cdots + c_n \mathbf{v}_n\right) = c_1 f\left(\mathbf{v}_1\right) + \cdots + c_n f\left(\mathbf{v}_n\right),&lt;/math&gt;

which implies that the function ''f'' is entirely determined by the vectors ''f''('''v'''&lt;sub&gt;1&lt;/sub&gt;), …, ''f''('''v'''&lt;sub&gt;''n''&lt;/sub&gt;). Now let {{nowrap|{'''w'''&lt;sub&gt;1&lt;/sub&gt;, …, '''w'''&lt;sub&gt;''m''&lt;/sub&gt;} }} be a basis for ''W''.  Then we can represent each vector ''f''('''v'''&lt;sub&gt;''j''&lt;/sub&gt;) as

: &lt;math&gt;f\left(\mathbf{v}_j\right) = a_{1j} \mathbf{w}_1 + \cdots + a_{mj} \mathbf{w}_m.&lt;/math&gt;

Thus, the function ''f'' is entirely determined by the values of ''a''&lt;sub&gt;''ij''&lt;/sub&gt;. If we put these values into an {{nowrap|''m'' × ''n''}} matrix ''M'', then we can conveniently use it to compute the vector output of ''f'' for any vector in ''V''.  To get ''M'', every column ''j'' of ''M'' is a vector

: &lt;math&gt;\begin{pmatrix} a_{1j} &amp; \cdots &amp; a_{mj} \end{pmatrix}^\textsf{T}&lt;/math&gt;

corresponding to ''f''('''v'''&lt;sub&gt;''j''&lt;/sub&gt;) as defined above. To define it more clearly, for some column ''j'' that corresponds to the mapping ''f''('''v'''&lt;sub&gt;''j''&lt;/sub&gt;),

: &lt;math&gt;\mathbf{M} = \begin{pmatrix}
  \ \cdots &amp; a_{1j} &amp; \cdots\  \\
           &amp; \vdots &amp;          \\
           &amp; a_{mj} &amp;
\end{pmatrix}&lt;/math&gt;

where '''M''' is the matrix of ''f''. In other words, every column {{nowrap|1=''j'' = 1, …, ''n''}} has a corresponding vector ''f''('''v'''&lt;sub&gt;''j''&lt;/sub&gt;) whose coordinates ''a''&lt;sub&gt;1''j''&lt;/sub&gt;, …, ''a''&lt;sub&gt;''mj''&lt;/sub&gt; are the elements of column ''j''. A single linear map may be represented by many matrices. This is because the values of the elements of a matrix depend on the bases chosen.

The matrices of a linear transformation can be represented visually:

# Matrix for &lt;math display="inline"&gt;T&lt;/math&gt; relative to &lt;math display="inline"&gt;B&lt;/math&gt;: &lt;math display="inline"&gt;A&lt;/math&gt;
# Matrix for &lt;math display="inline"&gt;T&lt;/math&gt; relative to &lt;math display="inline"&gt;B'&lt;/math&gt;: &lt;math display="inline"&gt;A'&lt;/math&gt;
# Transition matrix from &lt;math display="inline"&gt;B'&lt;/math&gt; to &lt;math display="inline"&gt;B&lt;/math&gt;: &lt;math display="inline"&gt;P&lt;/math&gt;
# Transition matrix from &lt;math display="inline"&gt;B&lt;/math&gt; to &lt;math display="inline"&gt;B'&lt;/math&gt;: &lt;math display="inline"&gt;P^{-1}&lt;/math&gt;

[[File:Linear_transformation_visualization.svg|The relationship between matrices in a linear transformation]]

Such that starting in the bottom left corner &lt;math display="inline"&gt;\left[\vec{v}\right]_{B'}&lt;/math&gt; and looking for the bottom right corner &lt;math display="inline"&gt;\left[T\left(\vec{v}\right)\right]_{B'}&lt;/math&gt;, one would left-multiply—that is, &lt;math display="inline"&gt;A'\left[\vec{v}\right]_{B'} = \left[T\left(\vec{v}\right)\right]_{B'}&lt;/math&gt;. The equivalent method would be the "longer" method going clockwise from the same point such that &lt;math display="inline"&gt;\left[\vec{v}\right]_{B'}&lt;/math&gt; is left-multiplied with &lt;math display="inline"&gt;P^{-1}AP&lt;/math&gt;, or &lt;math display="inline"&gt;P^{-1}AP\left[\vec{v}\right]_{B'} = \left[T\left(\vec{v}\right)\right]_{B'}&lt;/math&gt;.

== Examples of linear transformation matrices ==
In two-[[dimension]]al space '''R'''&lt;sup&gt;2&lt;/sup&gt; linear maps are described by [[2 × 2 real matrices]]. These are some examples:

* [[Rotation (mathematics)|rotation]]
** by 90 degrees counterclockwise:
**: &lt;math&gt;\mathbf{A} = \begin{pmatrix} 0 &amp; -1\\ 1 &amp; 0\end{pmatrix}&lt;/math&gt;
** by an angle ''θ'' counterclockwise:
**: &lt;math&gt;\mathbf{A} = \begin{pmatrix} \cos\theta &amp; -\sin\theta \\ \sin\theta &amp; \cos\theta \end{pmatrix}&lt;/math&gt;
* [[Reflection (mathematics)|reflection]]
** about the ''x'' axis:
**: &lt;math&gt;\mathbf{A} = \begin{pmatrix} 1 &amp; 0\\ 0 &amp; -1\end{pmatrix}&lt;/math&gt;
** about the ''y'' axis:
**: &lt;math&gt;\mathbf{A} = \begin{pmatrix}-1 &amp; 0\\ 0 &amp; 1\end{pmatrix}&lt;/math&gt;
* [[Scaling (geometry)|scaling]] by 2 in all directions:
*: &lt;math&gt;\mathbf{A} = \begin{pmatrix} 2 &amp; 0\\ 0 &amp; 2\end{pmatrix} = 2\mathbf{I}&lt;/math&gt;
* [[shear mapping|horizontal shear mapping]]:
*: &lt;math&gt;\mathbf{A} = \begin{pmatrix} 1 &amp; m\\ 0 &amp; 1\end{pmatrix}&lt;/math&gt;
* [[squeeze mapping]]:
*: &lt;math&gt;\mathbf{A} = \begin{pmatrix} k &amp; 0\\ 0 &amp; \frac{1}{k}\end{pmatrix}&lt;/math&gt;
* [[Projection (linear algebra)|projection]] onto the ''y'' axis:
*: &lt;math&gt;\mathbf{A} = \begin{pmatrix} 0 &amp; 0\\ 0 &amp; 1\end{pmatrix}.&lt;/math&gt;

== Forming new linear maps from given ones ==
The composition of linear maps is linear: if {{nowrap|''f'' : ''V'' → ''W''}} and {{nowrap|''g'' : ''W'' → ''Z''}} are linear, then so is their [[Relation composition|composition]] {{nowrap|''g'' ∘ ''f'' : ''V'' → ''Z''}}. It follows from this that the [[class (set theory)|class]] of all vector spaces over a given field ''K'', together with ''K''-linear maps as [[morphism]]s, forms a [[category (mathematics)|category]].

The [[inverse function|inverse]] of a linear map, when defined, is again a linear map.

If {{nowrap|''f''&lt;sub&gt;1&lt;/sub&gt; : ''V'' → ''W''}} and {{nowrap|''f''&lt;sub&gt;2&lt;/sub&gt; : ''V'' → ''W''}} are linear, then so is their [[pointwise]] sum {{nowrap|''f''&lt;sub&gt;1&lt;/sub&gt; + ''f''&lt;sub&gt;2&lt;/sub&gt;}} (which is defined by {{nowrap|1=(''f''&lt;sub&gt;1&lt;/sub&gt; + ''f''&lt;sub&gt;2&lt;/sub&gt;)('''x''') = ''f''&lt;sub&gt;1&lt;/sub&gt;('''x''') + ''f''&lt;sub&gt;2&lt;/sub&gt;('''x'''))}}.

If {{nowrap|''f'' : ''V'' → ''W''}} is linear and ''a'' is an element of the ground field ''K'', then the map ''af'', defined by {{nowrap|1=(''af'')('''x''') = ''a''(''f''('''x'''))}}, is also linear.

Thus the set {{nowrap|''L''(''V'', ''W'')}} of linear maps from ''V'' to ''W'' itself forms a vector space over ''K'', sometimes denoted {{nowrap|Hom(''V'', ''W'')}}.  Furthermore, in the case that {{nowrap|1=''V'' = ''W''}}, this vector space (denoted End(''V'')) is an [[associative algebra]] under [[composition of maps]], since the composition of two linear maps  is again a linear map, and the composition of maps is always associative.  This case is discussed in more detail below.

Given again the finite-dimensional case, if bases have been chosen, then the composition of linear maps corresponds to the [[matrix multiplication]], the addition of linear maps corresponds to the [[matrix addition]], and the multiplication of linear maps with scalars corresponds to the multiplication of matrices with scalars.

== Endomorphisms and automorphisms ==
{{main|Endomorphism|Automorphism}}
A linear transformation ''f'': ''V'' → ''V'' is an [[endomorphism]] of ''V''; the set of all such endomorphisms End(''V'')  together with addition, composition and scalar multiplication as defined above forms an [[associative algebra]] with identity element over the field ''K'' (and in particular a [[ring (algebra)|ring]]). The multiplicative identity element of this algebra is the [[identity function|identity map]] id: ''V'' → ''V''.

An endomorphism of ''V'' that is also an [[isomorphism]] is called an [[automorphism]] of ''V''. The composition of two automorphisms is again an automorphism, and the set of all automorphisms of ''V'' forms a [[group (math)|group]], the [[automorphism group]] of ''V'' which is denoted by Aut(''V'') or GL(''V''). Since the automorphisms are precisely those [[endomorphisms]] which possess inverses under composition, Aut(''V'') is the group of [[Unit (ring theory)|units]] in the ring End(''V'').

If ''V'' has finite dimension ''n'', then End(''V'') is [[isomorphism|isomorphic]] to the [[associative algebra]] of all ''n'' × ''n'' matrices with entries in ''K''. The automorphism group of ''V'' is [[group isomorphism|isomorphic]] to the  [[general linear group]] GL(''n'', ''K'') of all ''n'' × ''n'' invertible matrices with entries in ''K''.

== Kernel, image and the rank–nullity theorem ==
{{main|Kernel (linear operator)|Image (mathematics)|Rank of a matrix}}
If ''f'' : ''V'' → ''W'' is linear, we define the [[kernel (linear operator)|kernel]] and the [[image (mathematics)|image]] or [[range (mathematics)|range]] of ''f'' by

: &lt;math&gt;\begin{align}
  \operatorname{\ker}(f) &amp;= \{\,x \in V: f(x) = 0\,\} \\
    \operatorname{im}(f) &amp;= \{\,w \in W: w = f(x), x \in V\,\}
\end{align}&lt;/math&gt;

ker(''f'') is a [[Linear subspace|subspace]] of ''V'' and im(''f'') is a subspace of ''W''.  The following [[dimension]] formula is known as the [[rank–nullity theorem]]:

: &lt;math&gt;\dim(\ker( f )) + \dim(\operatorname{im}( f )) = \dim( V ).&lt;/math&gt;&lt;ref&gt;{{harvnb|Horn|Johnson|2013|loc=0.2.3 Vector spaces associated with a matrix or linear transformation, p. 6}}&lt;/ref&gt;

The number dim(im(''f'')) is also called the ''rank of f'' and written as rank(''f''), or sometimes, ρ(''f''); the number dim(ker(''f'')) is called the ''nullity of f'' and written as null(''f'') or ν(''f''). If ''V'' and ''W'' are finite-dimensional, bases have been chosen and ''f'' is represented by the matrix ''A'', then the rank and nullity of ''f'' are equal to the [[rank of a matrix|rank]] and [[Kernel (matrix)#Subspace properties|nullity]] of the matrix ''A'', respectively.

== Cokernel ==
{{main|Cokernel}}

A subtler invariant of a linear transformation &lt;math display="inline"&gt;f: V \to W&lt;/math&gt; is the [[cokernel|''co''kernel]], which is defined as

: &lt;math&gt;\operatorname{coker}(f) := W/f(V) = W/\operatorname{im}(f).&lt;/math&gt;

This is the ''dual'' notion to the kernel: just as the kernel is a ''sub''space of the ''domain,'' the co-kernel is a [[quotient space (linear algebra)|''quotient'' space]] of the ''target.''
Formally, one has the [[exact sequence]]

: &lt;math&gt;0 \to \ker(f) \to V \to W \to \operatorname{coker}(f) \to 0.&lt;/math&gt;

These can be interpreted thus: given a linear equation ''f''('''v''') = '''w''' to solve,

* the kernel is the space of ''solutions'' to the ''homogeneous'' equation ''f''('''v''') = 0, and its dimension is the number of ''degrees of freedom'' in a solution, if it exists;
* the co-kernel is the space of ''constraints'' that must be satisfied if the equation is to have a solution, and its dimension is the number of constraints that must be satisfied for the equation to have a solution.

The dimension of the co-kernel and the dimension of the image (the rank) add up to the dimension of the target space. For finite dimensions, this means that the dimension of the quotient space ''W''/''f''(''V'') is the dimension of the target space minus the dimension of the image.

As a simple example, consider the map ''f'': '''R'''&lt;sup&gt;2&lt;/sup&gt; → '''R'''&lt;sup&gt;2&lt;/sup&gt;, given by ''f''(''x'', ''y'') = (0, ''y''). Then for an equation ''f''(''x'', ''y'') = (''a'', ''b'') to have a solution, we must have ''a'' = 0 (one constraint), and in that case the solution space is (''x'', ''b'')  or equivalently stated, (0, ''b'') + (''x'', 0), (one degree of freedom). The kernel may be expressed as the subspace (''x'', 0) &lt; ''V'': the value of ''x'' is the freedom in a solution – while the cokernel may be expressed via the map ''W'' → '''R''', &lt;math display="inline"&gt; (a, b) \mapsto (a):&lt;/math&gt; given a vector (''a'', ''b''), the value of ''a'' is the ''obstruction'' to there being a solution.

An example illustrating the infinite-dimensional case is afforded by the map ''f'': '''R'''&lt;sup&gt;∞&lt;/sup&gt; → '''R'''&lt;sup&gt;∞&lt;/sup&gt;, &lt;math display="inline"&gt;\left\{a_n\right\} \mapsto \left\{b_n\right\}&lt;/math&gt; with ''b''&lt;sub&gt;1&lt;/sub&gt; = 0 and ''b''&lt;sub&gt;''n'' + 1&lt;/sub&gt; = ''a&lt;sub&gt;n&lt;/sub&gt;'' for ''n'' &gt; 0. Its image consists of all sequences with first element 0, and thus its cokernel consists of the classes of sequences with identical first element. Thus, whereas its kernel has dimension 0 (it maps only the zero sequence to the zero sequence), its co-kernel has dimension 1. Since the domain and the target space are the same, the rank and the dimension of the kernel add up to the same [[cardinal number#Cardinal addition|sum]] as the rank and the dimension of the co-kernel ( &lt;math display="inline"&gt;\aleph_0 + 0 = \aleph_0 + 1&lt;/math&gt; ), but in the infinite-dimensional case it cannot be inferred that the kernel and the co-kernel of an [[endomorphism]] have the same dimension (0 ≠ 1). The reverse situation obtains for the map ''h'': '''R'''&lt;sup&gt;∞&lt;/sup&gt; → '''R'''&lt;sup&gt;∞&lt;/sup&gt;, &lt;math display="inline"&gt;\left\{a_n\right\} \mapsto \left\{c_n\right\}&lt;/math&gt; with ''c&lt;sub&gt;n&lt;/sub&gt;'' = ''a''&lt;sub&gt;''n'' + 1&lt;/sub&gt;. Its image is the entire target space, and hence its co-kernel has dimension 0, but since it maps all sequences in which only the first element is non-zero to the zero sequence, its kernel has dimension 1.

=== Index ===
For a linear operator with finite-dimensional kernel and co-kernel, one may define  ''index'' as:

: &lt;math&gt;\operatorname{ind}(f) := \dim(\ker(f)) - \dim(\operatorname{coker}(f)),&lt;/math&gt;

namely the degrees of freedom minus the number of constraints.

For a transformation between finite-dimensional vector spaces, this is just the difference dim(''V'') − dim(''W''), by rank–nullity. This gives an indication of how many solutions or how many constraints one has: if mapping from a larger space to a smaller one, the map may be onto, and thus will have degrees of freedom even without constraints. Conversely, if mapping from a smaller space to a larger one, the map cannot be onto, and thus one will have constraints even without degrees of freedom.

The index of an operator is precisely the [[Euler characteristic]] of the 2-term complex 0 → ''V'' → ''W'' → 0. In [[operator theory]], the index of [[Fredholm operator]]s is an object of study, with a major result being the [[Atiyah–Singer index theorem]].&lt;ref&gt;{{SpringerEOM|title=Index theory|id=Index_theory&amp;oldid=23864|first=Victor|last=Nistor}}: "The main question in index theory is to provide index formulas for classes of Fredholm operators ... Index theory has become a subject on its own only after M. F. Atiyah and I. Singer published their index theorems"&lt;/ref&gt;

== Algebraic classifications of linear transformations ==
No classification of linear maps could hope to be exhaustive. The following incomplete list enumerates some important classifications that do not require any additional structure on the vector space.

Let ''V'' and ''W'' denote vector spaces over a field, ''F''. Let ''T'': ''V'' → ''W'' be a linear map.

* ''T'' is said to be ''[[injective]]'' or a ''[[monomorphism]]'' if any of the following equivalent conditions are true:
** ''T'' is [[injective|one-to-one]] as a map of [[set (mathematics)|sets]].
** ker''T'' = {0&lt;sub&gt;''V''&lt;/sub&gt;}
** ''T'' is [[monic morphism|monic]] or left-cancellable, which is to say, for any vector space ''U'' and any pair of linear maps ''R'': ''U'' → ''V'' and ''S'': ''U'' → ''V'', the equation ''TR'' = ''TS'' implies ''R'' = ''S''.
** ''T'' is [[inverse (ring theory)|left-invertible]], which is to say there exists a linear map ''S'': ''W'' → ''V'' such that ''ST'' is the [[Identity function|identity map]] on ''V''.
* ''T'' is said to be ''[[surjective]]'' or an ''[[epimorphism]]'' if any of the following equivalent conditions are true:
** ''T'' is [[surjective|onto]] as a map of sets.
** [[cokernel|coker]] ''T'' = {0&lt;sub&gt;''W''&lt;/sub&gt;}
** ''T'' is [[epimorphism|epic]] or right-cancellable, which is to say, for any vector space ''U'' and any pair of linear maps ''R'': ''W'' → ''U'' and ''S'': ''W'' → ''U'', the equation ''RT'' = ''ST'' implies ''R'' = ''S''.
** ''T'' is [[inverse (ring theory)|right-invertible]], which is to say there exists a linear map ''S'': ''W'' → ''V'' such that ''TS'' is the [[Identity function|identity map]] on ''W''.
* ''T'' is said to be an ''[[isomorphism]]'' if it is both left- and right-invertible. This is equivalent to ''T'' being both one-to-one and onto (a [[bijection]] of sets) or also to ''T'' being both epic and monic, and so being a [[bimorphism]].
* If ''T'': ''V'' → ''V'' is an endomorphism, then:
** If, for some positive integer ''n'', the ''n''-th iterate of ''T'', ''T&lt;sup&gt;n&lt;/sup&gt;'', is identically zero, then ''T'' is said to be [[nilpotent]].
** If ''T''&lt;sup&gt;2&lt;/sup&gt; = ''T'', then ''T'' is said to be [[idempotent]]
** If ''T'' = ''kI'', where ''k'' is some scalar, then ''T'' is said to be a scaling transformation or scalar multiplication map; see [[scalar matrix]].

== Change of basis ==
{{main|Basis (linear algebra)|Change of basis}}
Given a linear map which is an [[endomorphism]] whose matrix is ''A'', in the basis ''B'' of the space it transforms vector coordinates [u] as [v] = ''A''[u]. As vectors change with the inverse of ''B'' (vectors are [[Covariance and contravariance of vectors|contravariant]]) its inverse transformation is [v] = ''B''[v'].

Substituting this in the first expression
:&lt;math&gt;B\left[v'\right] = AB\left[u'\right]&lt;/math&gt;

hence
:&lt;math&gt;\left[v'\right] = B^{-1}AB\left[u'\right] = A'\left[u'\right].&lt;/math&gt;

Therefore, the matrix in the new basis is ''A′'' = ''B''&lt;sup&gt;−1&lt;/sup&gt;''AB'', being ''B'' the matrix of the given basis.

Therefore, linear maps are said to be 1-co- 1-contra-[[covariance and contravariance of vectors|variant]] objects, or type (1, 1) [[tensor]]s.

== Continuity ==
{{main|Discontinuous linear map}}

A ''linear transformation'' between [[topological vector space]]s, for example [[normed space]]s, may be [[continuous function (topology)|continuous]].  If its domain and codomain are the same, it will then be a [[continuous linear operator]].  A linear operator on a normed linear space is continuous if and only if it is [[bounded operator|bounded]], for example, when the domain is finite-dimensional.&lt;ref&gt;{{harvnb|Rudin|1991|page=15}}

'''1.18 Theorem''' ''Let &lt;math display="inline"&gt;\Lambda&lt;/math&gt; be a linear functional on a topological vector space {{mvar|X}}. Assume &lt;math display="inline"&gt;\Lambda x \neq 0&lt;/math&gt; for some &lt;math display="inline"&gt;x \in X&lt;/math&gt;. Then each of the following four properties implies the other three:
{{ordered list
 | list-style-type=lower-alpha
 | &lt;math display="inline"&gt;\Lambda&lt;/math&gt; is continuous
 | The null space &lt;math display="inline"&gt;N(\Lambda)&lt;/math&gt; is closed.
 | &lt;math display="inline"&gt;N(\Lambda)&lt;/math&gt; is not dense in {{mvar|X}}.
 | &lt;math display="inline"&gt;\Lambda&lt;/math&gt; is bounded in some neighbourhood {{mvar|V}} of 0.
}}&lt;/ref&gt;  An infinite-dimensional domain may have [[discontinuous linear operator]]s.

An example of an unbounded, hence discontinuous, linear transformation is differentiation on the space of smooth functions equipped with the supremum norm (a function with small values can have a derivative with large values, while the derivative of 0 is 0).  For a specific example, sin(''nx'')/''n'' converges to 0, but its derivative cos(''nx'') does not, so differentiation is not continuous at 0 (and by a variation of this argument, it is not continuous anywhere).

== Applications ==
A specific application of linear maps is for geometric transformations, such as those performed in [[computer graphics]], where the translation, rotation and scaling of 2D or 3D objects is performed by the use of a [[transformation matrix]]. Linear mappings also are used as a mechanism for describing change: for example in calculus correspond to derivatives; or in relativity, used as a device to keep track of the local transformations of reference frames.

Another application of these transformations is in [[compiler optimizations]] of nested-loop code, and in [[parallelizing compiler]] techniques.

== See also ==
{{Wikibooks|Linear Algebra/Linear Transformations}}
* [[Affine transformation]]
* [[Linear equation]]
* [[Bounded operator]]
* [[Antilinear map]]
* [[Semilinear map]]
* [[Continuous linear operator]]
* [[Bent function]]

== Notes==
{{reflist}}

== References ==
* {{Cite book | last1=Halmos | first1=Paul R. | author1-link=Paul R. Halmos | title=Finite-Dimensional Vector Spaces | publisher=[[Springer-Verlag]] | location=New York | isbn=0-387-90093-4 | year=1974}}
* {{Cite book | last1=Horn | first1=Roger A. | last2=Johnson | first2=Charles R. | title=Matrix Analysis |edition=Second |publisher=[[Cambridge University Press]] | isbn=978-0-521-83940-2 | year=2013 }}
* {{Citation | last1=Lang | first1=Serge | author1-link=Serge Lang | title=Linear Algebra | publisher=[[Springer-Verlag]] | location=New York | isbn=0-387-96412-6 |edition=Third | year=1987}}
* {{cite book| last=Rudin | first=Walter| author1-link=Walter Rudin| title=Principles of Mathematical Analysis |edition=Third |publisher=[[McGraw-Hill]]|year=1976|isbn=0-07-085613-3 }}
* {{cite book| last=Rudin | first=Walter| author1-link=Walter Rudin| title=Functional Analysis |edition=Second | publisher=[[McGraw-Hill]] |year=1991 |isbn=0-07-054236-8 }}

{{linear algebra}}
{{tensors}}

[[Category:Abstract algebra]]
[[Category:Functions and mappings]]
[[Category:Linear algebra]]
[[Category:Transformation (function)]]</text>
      <sha1>no68071ipxbkbmlm549675qzqyg40pf</sha1>
    </revision>
  </page>
  <page>
    <title>List of works by Nicolas Minorsky</title>
    <ns>0</ns>
    <id>40658915</id>
    <revision>
      <id>867730859</id>
      <parentid>842558366</parentid>
      <timestamp>2018-11-07T17:04:15Z</timestamp>
      <contributor>
        <username>Citation bot</username>
        <id>7903804</id>
      </contributor>
      <minor/>
      <comment>Alter: journal, isbn. Add: pmid, issue. Removed accessdate with no specified URL. Removed parameters. You can [[WP:UCB|use this bot]] yourself. [[WP:DBUG|Report bugs here]]. | [[WP:UCB|User-activated]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="30109">List of works by [[Nicolas Minorsky]].&lt;ref name="Flügge-Lotz"&gt;{{cite journal |author = Flügge-Lotz, I. |year = 1971|title=Memorial to N. Minorsky|journal=IEEE Transactions on Automatic Control, Vol. AC-16, No. 4}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |author=Bennet, Stuart |year=1984 |title=Nicolas Minorsky and the Automatic Steering of Ships |journal=Control Systems Magazine |volume= |issue= |pages=10–15|doi= |pmid= |pmc= |url= |accessdate= }}&lt;/ref&gt;

==Books==
* {{cite book
| title        = Introduction to non-linear mechanics: Topological methods, analytical methods, non-linear resonance, relaxation oscillations
| first        = N.
| last         = Minorsky
| publisher    = J.W. Edwards
| series=
| pages        =
| year         = 1947
| asin         = B0007DXRVY
}}
* {{cite book
| title        = Influence d'Henri Poincaré sur l'évolution moderne de la théorie des oscillations non linéaires 
| trans-title  = Influence of Henri Poincaré on the modern development of the theory of nonlinear oscillations
| language     = French
| first1       = Nicolai 
| last1        = Minorsky 
| publisher    = Paris, Gauthier-Villars
| series       = Le livre du centenaire de la naissance de Henri Poincaré, 1854-1954
| pages        = 120–126
| year         = 1955
| oclc         = 10571426
}}
* {{cite book
| title        = Dynamics and Nonlinear Mechanics: The Theory of Oscillations 
| first        = N. 
| last         = Minorsky 
| publisher    = Wiley
| series       = Surveys in Applied Mathematics
| pages        = 110–206
| year         = 1958
| asin         = B0006AVKQW
}}
* {{cite book
| title        = On some aspects of non-linear oscillations
| first        = N. 
| last         = Minorsky 
| editor       = G. Szego
| publisher    = Stanford Univ Pr (April 1962)
| series       = Studies in Mathematical Analysis and Related Topics
| year         = 1962
| isbn         = 978-0804701402
| display-editors= etal
}}
* {{cite book
| title        = Théorie des Oscillations
| trans-title  = Theory of Oscillations
| language     = French
| first        = N.
| last         = Minorsky
| series       = Memorial des Sciences Mathematiques, Fascicule CLXIII
| year         = 1967
| asin         = B000LQD7KI
}}
* {{cite book
| title        = Drgania nieliniowe
| trans-title  = Nonlinear oscillations
| language     = Polish
| first1       = Nicolai
| last1        = Minorsky
| first2       = Richard B.
| last2        = Hetnarski
| first3       = Zenon
| last3        = Mróz
| publisher    = Warszawa : Państwowe Wydawnictwo Naukowe
| year         = 1967
| oclc         = 749191659
}}

==Papers==
* {{cite journal
| author       = Minorsky, N.
| date         = May 1922
| title        = Directional stability of automatically steered bodies
| journal      = [[Journal of the American Society of Naval Engineers|J. Amer. Soc of Naval Engineers]]
| volume       = 34
| issue         = 2
| pages        = 280–309
| doi          = 10.1111/j.1559-3584.1922.tb04958.x 
}}
* {{cite journal
| author       = Minorsky, N.
| date         = February 1927
| title        = Phenomenon of direct-current self excitation in vacuum tubes circuits and in applications
| journal      = [[Journal of the Franklin Institute|J. Franklin Inst.]]
| volume       = 203
| issue        = 2
| pages        = 181–209
| doi          = 10.1016/S0016-0032(27)92437-5
| url          = http://www.sciencedirect.com/science/article/pii/S0016003227924375
| accessdate   = 9 July 2014
}}
* {{cite journal
| author       = Minorsky, N.
| date         = April 1928
| title        = La rotation de l'arc électrique dans champ magnétique radial
| language     = French
| trans-title  = The rotation of an arc in a radial magnetic field 
| journal      = J. Phys. Radium
| volume       = 9
| issue        = 4
| pages        = 127–136 
| doi          = 10.1051/jphysrad:0192800904012700
}}
* {{cite journal
| author       = Minorsky, N.
| year         = 1928
| title        = Mesure de la vitesse d'un aéronef par rapport au sol en l'absence supposée de tout repère extérieur
| language     = French
| trans-title  = Measuring the speed of an aircraft relative to the ground in the absence of any assumed approximate
| journal      = L'Aéronautique
| volume       = 113
}}
* {{cite journal
| author       = Minorsky, N.
| date         = May 1930
| title        = Automatic steering test
| journal      = J. Amer. Soc. Nav. Eng.
| volume       = 42
| issue        = 2
| pages        = 285–310
| doi          = 10.1111/j.1559-3584.1930.tb05036.x
}}
* {{cite journal
| author       = Minorsky, N.
| date         = June 1930
| title        = Electronic conduction and ionization in crossed electric and magnetic fields
| journal      = Journal of the Franklin Institute
| volume       = 209
| issue        = 6
| pages        = 757–775
| doi          = 10.1016/S0016-0032(30)91472-X
| issn         = 0016-0032
| url          = http://www.sciencedirect.com/science/article/pii/S001600323091472X
| accessdate   = 8 July 2014
}}
* {{cite journal
| author       = Minorsky, N.
| date         = August 1934
| title        = Ship stabilization by activated tank: An experimental investigation
| journal      = The Engineer 
| volume       = 158
| page         = 154
}}
* {{cite journal
| author       = Minorsky, N.
| date         = 27 January 1936
| title        = Une méthode d'intégration de quelques équations différentielles par un procédé électrique
| language     = French
| trans-title  = A method of integration of the differential equations by some electrical method
| journal      = [[C. R. Acad. Sci.]]
| volume       = 202
| issue        = 
| pages        = 293–295 
| url          = http://gallica.bnf.fr/ark:/12148/bpt6k3154f/f293.image.r= 
| accessdate   = 8 July 2014
}}
* {{cite journal
| author       = Minorsky, N.
| date         = 30 May 1936
| title        = Application des circuits électroniques à l'intégration graphique de quelques équations différentielles
| language     = French
| trans-title  = Application of the electronic circuits to the graphic integration of some differential equations
| journal      = Rev Gén Elec.  
| volume       = 34
| issue        = 
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = March 1937
| title        = The principles and practice of automatic control
| journal      = The Engineer  
| volume       = 9
| issue        = 
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = September 1941
| title        = Note on the angular motion of ships
| journal      = Journal of Applied Mechanics
| volume       = 45
| issue        = 
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = November 1941
| title        = Control Problems
| journal      = Journal of the Franklin Institute
| volume       = 232
| issue        = 5
| pages        = 451–487
| doi          = 10.1016/S0016-0032(41)90069-8
| url          = http://www.sciencedirect.com/science/article/pii/S0016003241900698
| accessdate   = 9 July 2014
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = December 1941
| title        = Control Problems (cont.)
| journal      = Journal of the Franklin Institute
| volume       = 232
| issue        = 6
| pages        = 519–551 
| doi          = 10.1016/S0016-0032(41)90178-3 
| url          = http://www.sciencedirect.com/science/article/pii/S0016003241901783 
| accessdate   = 9 July 2014
}}
* {{cite journal
| author       = Minorsky, N.
| date         = June 1942
| title        = Self-excited in dynamical systems possessing retarded actions
| journal      = J. Appl. Mech.
| volume       = 9
| issue        = 
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = 15 October 1944 
| title        = On mechanical self-excited oscillations
| journal      = Proceedings of the National Academy of Sciences of the United States of America
| volume       = 30
| issue        = 10
| pages        = 308–314 
| doi          = 10.1073/pnas.30.10.308
| pmid         = 16578134 
| url          = http://www.pnas.org/content/30/10/308.full.pdf+html 
| accessdate   = 9 July 2014
| pmc= 1078718}}
* {{cite journal 
| author       = Minorsky, N.
| date         = 1 November 1945
| title        = On non-linear phenomenon of self-rolling
| journal      = Proceedings of the National Academy of Sciences of the United States of America
| volume       = 31
| issue        = 11
| pages        = 346–349 
| doi          = 10.1073/pnas.31.11.346
| pmid         = 16578177
| url          = http://www.pnas.org/content/31/11/346.full.pdf+html?sid=94c11ce3-ea20-43cc-9834-c5e0b0ff635c 
| accessdate   = 9 July 2014
| pmc= 1078842
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = July 1945
| title        = On parametric excitation
| journal      = J. Franklin Inst. 
| volume       = 240
| issue        = 1
| pages        = 25–46 
| doi          = 10.1016/0016-0032(45)90217-1 
| url          = http://www.sciencedirect.com/science/article/pii/0016003245902171 
| accessdate   = 9 July 2014
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = February 1947
| title        = A dynamical analogue
| journal      = J. Franklin Inst. 
| volume       = 243
| issue        = 2
| pages        = 131–149 
| doi          = 10.1016/0016-0032(74)90312-3 
| url          = http://www.sciencedirect.com/science/article/pii/0016003274903123 
| accessdate   = 9 July 2014
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = October 1947
| title        = Experiments with activated tanks
| journal      = ASME 
| issue        = 
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = April 1948
| title        = Self-excited mechanical oscillations
| journal      = Journal of Applied Physics  
| volume       = 19
| issue        = 4
| pages        = 332–338 
| doi          = 10.1063/1.1715068 
| url          = http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5118794&amp;isnumber=5118792
| issn         = 0021-8979
| accessdate   = 9 July 2014
}}
* {{cite journal 
| author       = Minorsky, N.
| year         = {{date|April 1948}}
| title        = Sur une classe d'oscillations auto-entretenues 
| language     = French
| trans-title  = On a class of self-sustained oscillations
| journal      = C. R. Acad. Sci.
| series       = Méchanique
| volume       = 226
| issue        = 
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = January 1949
| title        = Sur l'oscillateur de van der Pol 
| language     = French
| trans-title  = On the van der Pol oscillator
| journal      = C. R. Acad. Sci.
| series       = Physique mathématique
| volume       = 228
| issue        = 
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = September 1949
| title        = Energy fluctuations in a van der Pol oscillator 
| journal      = J. Franklin Inst.
| volume       = 248
| issue        = 3
| pages        = 205–223 
| doi          = 10.1016/0016-0032(49)90210-0  
| url          = http://www.sciencedirect.com/science/article/pii/0016003249902100
| accessdate   = 9 July 2014
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = December 1950
| title        = Meccanica non-lineare
| language     = Italian
| trans-title  = Non-Linear Mechanics
| journal      = Bollettino Dell'Unione Matematica Italiana
| series       = 3
| volume       = 5
| issue        = 
| pages        = 313–330 
| url          = http://www.bdim.eu/item?id=BUMI_1950_3_5_3-4_313_0 
| accessdate   = 9 July 2014
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = 18 December 1950
| title        = Sur l'excitation paramétrique 
| language     = French
| trans-title  = On the parametric excitation
| journal      = C. R. Acad. Sci.
| series       = Méchanique
| volume       = 231
| issue        = 
| pages        = 1417–1419 
| url          = http://gallica.bnf.fr/ark:/12148/bpt6k3183z/f1417.image.langEN 
| accessdate   = 9 July 2014
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = January 1951
| title        = Parametric excitation 
| journal      = J. Appl. Phys. 
| volume       = 22
| issue        = 1
| pages        = 49–54 
| doi          = 10.1063/1.1699819 
| url          = http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=5119480&amp;isnumber=5119471 
| accessdate   = 10 July 2014
}}
* {{cite journal 
| author       = Minorsky, N.
| year         = 1951 
| title        = Sur une équation différentielle de la physique 
| language     = French
| trans-title  = On a differential equation of the physical
| journal      = C. R. Acad. Sci.
| series       = Méchanique
| volume       = 232
| issue        = 
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = 1 October 1951
| title        = Sur le pendule entretenu par un courant alternatif 
| language     = French
| trans-title  = On the clock maintained by an alternating current
| journal      = C. R. Acad. Sci.
| series       = Méchanique
| volume       = 233
| issue        = 
| pages        = 728, 729
| url          = http://gallica.bnf.fr/ark:/12148/bpt6k3185k/f728.image.langEN 
| accessdate   = 10 July 2014
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = March 1951
| title        = Modern nonlinear trends in engineering 
| journal      = Applied Mechanics Reviews
| volume       = 4
| issue        = 
| issn         = 0003-6900
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = June 1951
| title        = Sur l'oscillateur non linéaire de Mathieu
| language     = French
| trans-title  = The Mathieu nonlinear oscillator
| journal      = C. R. Acad. Sci. 
| volume       = 232
| issue        = 
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = January 1952
| title        = Sur l'interaction des oscillations non linéaires
| language     = French
| trans-title  = On the interaction of nonlinear oscillations
| journal      = C. R. Acad. Sci.
| volume       = 234
| issue        = 
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = March 1952
| title        = Sur les systèmes à l'action retardée le pendule entretenu par un courant alternatif 
| language     = French
| trans-title  = Systems to share the delayed clock maintained by an alternating current
| journal      = C. R. Acad. Sci.
| series       = Méchanique
| volume       = 234
| issue        = 
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = July 1952
| title        = Stationary solutions of certain nonlinear differential equations 
| journal      = J. Franklin Inst.
| volume       = 254
| issue        = 1
| pages        = 21–42 
| doi          = 10.1016/0016-0032(52)90003-3 
| url          = http://www.sciencedirect.com/science/article/pii/0016003252900033 
| accessdate   = 10 July 2014
}}
* {{cite journal 
| author       = Minorsky, N.
| date         = September 1952
| title        = Sur des systèmes oscillatoires contenant des paramètres à inertie
| language     = French
| trans-title  = On oscillatory systems containing inertia parameters
| journal      = C. R. Acad. Sci.
| volume       = 235
| issue        = 
}}
* {{cite journal 
|author        = Minorsky, N.
|date          = August 1953
|title         = On interaction of non-linear oscillations 
|journal       = J. Franklin Inst.
|volume        = 256
|issue         = 2
|pages         = 147–165 
|doi           = 10.1016/0016-0032(53)90941-7 
|url           = http://www.sciencedirect.com/science/article/pii/0016003253909417 
|accessdate    = 10 July 2014
}}
* {{cite journal |author=Minorsky, N.|year=1953 |title=Sur l'extinction asynchrone|journal=C. R. Acad. Sci.|series=|volume=237|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal 
| author       = Minorsky, N.
| year         = {{date|December 1953}}
| title        = Sur quelques applications des équations differentielles aux différences 
| language     = French
| trans-title  = On some applications of differential equations to differences
| journal      = Rendiconti del Seminario Matematico e Fisico di Milano 
| volume       = 23
| issue        = 1
| pages        = 36–47
| doi          = 10.1007/BF02922522 
}}
* {{cite journal |author=Minorsky, N.|year=1953 |title=Sur les systèmes non linéaires à deux degrés de liberté |journal=Rend. Semin. Mal. Torino|series=|volume=13|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1953 |title=Sur l'interaction des oscillation non linéaires |journal=Rendiconti del Seminario Matematico e Fisico di Milano|series=|volume=25|issue=1|pages=145–163 |doi=10.1007/BF02923816 |pmid= |pmc= }}
* {{cite journal |author=Minorsky, N.|year=1954 |title=La méthode stroboscopique et ses applications|journal=Bull. S.F.M.|series=|volume=13|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1954 |title=Sur les systèmes non linéaires à deux degrés de liberté|journal=C. R. Acad. Sci.|series=|volume=238|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1955 |title=On synchronous actions |journal=J. Franklin Inst.|series=|volume=259|issue=3|pages=209–219 |doi=10.1016/0016-0032(55)90825-5 |pmid= |pmc= |url=http://www.sciencedirect.com/science/article/pii/0016003255908255 |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1955 |title=Sur la méthode stroboscopique et ses applications |journal=Proc. 8th Int. Congr. Theorel. Appl. Mech. |series=|volume=|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1955 |title=Sur la résonance non linéaires |journal=C. R. Acad. Sci.|series=|volume=240|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1955 |title=Sur l'espace paramétrique de l'équation de M. Liénard |journal=C. R. Acad. Sci.|series=Méchanique|volume=240|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
&lt;!-- https://link.springer.com/article/10.1007%2FBF02923816 --&gt;
* {{cite journal |author=Minorsky, N.|year=1957 |title=Structure topolgique de l'équation de M. Liénard |journal=J. Phys. Phys. Appl.|series=|volume=18 |issue=s12|pages=121–130 |doi=10.1051/jphysap:019570018012012100 |pmid= |pmc= }}
*{{cite journal |author=Minorsky, N.|year=1958 |title=Equations différentielles - Sur l'excitation paramétrique |journal=C. R. Acad. Sci.|series=|volume=247|issue= |pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1958 |title=Sur la synchronisation |journal=C. R. Acad. Sci.|series=Méchanique|volume=247|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1959 |title=Sur les phénomènes paramétriques |journal=Atti. Accad. Sci. Ist. Bologna, Cl. Sci. Fis|series=|volume=247|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1959 |title=Sur les phénomènes paramétriques |journal=Rend. Semin. Mat. Fis. Milano|series=11|volume=6|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1959 |title=Sur l'action asynchrone |journal=C. R. Acad. Sci.|series=Méchanique|volume=248|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1959 |title=Sur l'interaction des oscillations non linéaires |journal=C. R. Acad. Sci.|series=Méchanique|volume=250|issue= |pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1960 |title=Méthode stroboscopique et ses applications |journal=Cah. Phys.|series=|volume=119|issue= |pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1960 |title=Theoretical aspects of nonlinear oscillations |journal=Circuit Theory, IRE Transactions on |series=|volume=7|issue=4|pages=368–381 |doi=10.1109/TCT.1960.1086717 |pmid= |pmc= |url=http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1086717&amp;isnumber=23617 |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1961 |title=Sur quelques aspects des oscillation non linéaires |journal=C. R. Acad. Sci.|series=|volume=253|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1962 |title=Sur les oscillation quasi discontinues |journal=C. R. Acad. Sci.|series=|volume=255|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1962 |title=Sur la résonance non linéaires |journal=C. R. Acad. Sci.|series=|volume=254|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1962 |title=Sur la résonance non linéaires |journal=C. R. Acad. Sci.|series=|volume=254|issue= |pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1963 |title=Sur la méthode stroboscopique |journal=C. R. Acad. Sci.|series=|volume=256|issue= |pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1963 |title=Existence et stabilité de certaines solutions périodiques multiformes d'une équation de Duffing |journal=C. R. Acad. Sci.|series=|volume=256|issue= |pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1964 |title=Sur la synchronisation|journal=C. R. Acad. Sci.|series=|volume=259|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1965 |title=Sur les oscillations quasi discontinues |journal=C. R. Acad. Sci.|series=|volume=261|issue= |pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1965 |title=Sur les phénomènes paramétriques |journal=C. R. Acad. Sci.|series=|volume=261|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1965 |title=Sur ll'interaction des oscillations non linéaires |journal=C. R. Acad. Sci.|series=|volume=261|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1965 |title=Sur les oscillations quasi discontinues |journal=C. R. Acad. Sci.|series=Méchanique|volume=261|issue= |pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1966 |title=Sur la méthode stroboscopique |journal=C. R. Acad. Sci.|series=|volume=263|issue=|pages= |doi= |pmid= |pmc= |url= |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1967 |title=Comments "On asynchronous quenching"  |journal=Automatic Control, IEEE Transactions on |series=|volume=12|issue=2|pages=225–227 |doi=10.1109/TAC.1967.1098559 |pmid= |pmc= |url=http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=1098559&amp;isnumber=24094
 |accessdate= }}
* {{cite journal |author=Minorsky, N.|year=1968 |title=Sur les équations différentielles aux différences |journal=C. R. Acad. Sci.|series=|volume=266|issue= |pages= |doi= |pmid= |pmc= |url= |accessdate= }}

==Conferences==
* {{cite conference |first=N. |last=Minorsky | authorlink=Nicolas Minorsky | title=Self-excited oscillations in systems possessing retarded actions | conference=7th Int. Congr. Appl. Math, London, England |year=1948 |url=}}
* {{cite conference |first=N. |last=Minorsky | authorlink=Nicolas Minorsky | title=Sur le phénomène Béthenod | conference=Actes Colloq. Int. Vibrations linéaires, Porqurolles, France |year=1951 |url=}}
* {{cite conference |first=N. |last=Minorsky | authorlink=Nicolas Minorsky | title=Non-linear control systems | conference=Conf. Automat. Contr., London:Butterworth |year=1952 |url=}}
* {{cite conference |first=N. |last=Minorsky | authorlink=Nicolas Minorsky | title=Sur la méthode stroboscopique | conference=Mem. lett. Accad. Sci. Ist., Bologna |year=1952 |url=}}
* {{cite conference |first=N. |last=Minorsky | authorlink=Nicolas Minorsky | title=Sur quelques oscillatoires contenant les paramètres à inertie | conference=Ann. Fac. Sci. Univ. d'Aix, Marseilles, France |year=1952 |url=}}
&lt;!--  https://link.springer.com/article/10.1007%2FBF02923816#page-1 --&gt;
* {{cite conference |first=N. |last=Minorsky | authorlink=Nicolas Minorsky | title=Nouvelles méthodes de la théorie des oscillations | conference=Conf. Semin. Mat. Univ. Bari., |year=1957 |url=}}
* {{cite conference |first=N. |last=Minorsky | authorlink=Nicolas Minorsky | title=On synchronization | conference=Int. Union Theor. Appl. Mech. Symp. Nonlinear Vibrations, Kiev, U.S.S.R |year=1961 |url=}}
* {{cite conference |first=N. |last=Minorsky | authorlink=Nicolas Minorsky | title=On synchronization | conference=Tr. Mizh. Symp. po Nelin. Kolebelina, vol. 1, Kiev, U.S.S.R |year=1963 |url=}}
* {{cite conference |first=N. |last=Minorsky | authorlink=Nicolas Minorsky | title=Les vibrations forcées dans les systèmes non linéaires | conference=Coll. Int. Centre Nat. Rech. Sci., no. 148, Marseilles, France |year=1964 |url=}}
* {{cite conference |first=N. |last=Minorsky | authorlink=Nicolas Minorsky | title=Sur les systèmes non autonomes | conference=Colloques Inl. Centre Nat. Rech. Sci., vol 148 |year=1965 |url=}}

==Patents==
*  {{ cite patent
 | country = US
 | number = 1306552 A
 | status = patent
 | title = Gyrometer
 | pubdate = 1919-06-10
 | gdate = 
 | fdate = 1918-06-17
 | pridate = 
 | inventor = N. Minorsky
 | invent1 = N. Minorsky
 | invent2 = 
 | assign1 = The Sperry Gyroscope company
 | assign2 = 
 | class = G01C19/42
}}
*  {{ cite patent
 | country = US
 | number = 1372184 A
 | status = patent
 | title = Angular-velocity-indicating apparatus
 | pubdate = 1921-03-22
 | gdate = 
 | fdate = 1918-12-26
 | pridate = 1918-12-26 
 | inventor = N. Minorsky
 | invent1 = N. Minorsky
 | invent2 = 
 | assign1 = N. Minorsky
 | assign2 = 
 | class = G01C19/42
}}
*  {{ cite patent
 | country = US
 | number = 1436280 A
 | status = patent
 | title = Automatic steering device
 | pubdate = 1922-11-21 
 | gdate = 
 | fdate = 1918-11-02 
 | pridate = 1918-11-02 
 | inventor = N. Minorsky
 | invent1 = N. Minorsky
 | invent2 = 
 | assign1 = N. Minorsky
 | assign2 = 
 | class = G05D1/02C
}}
*  {{ cite patent
 | country = US
 | number = 1633822 A
 | status = patent
 | title = System of motor control
 | pubdate = 1927-06-28
 | gdate = 
 | fdate = 1925-09-01
 | pridate = 1925-09-01 
 | inventor = N. Minorsky
 | invent1 = N. Minorsky
 | invent2 = 
 | assign1 = The General Electric Company
 | assign2 = 
 | class = H02P5/50
}}
*  {{ cite patent
 | country = US
 | number = 1703280 A
 | status = patent
 | title = Directional stabilizer
 | pubdate = 1929-02-26
 | gdate = 
 | fdate = 1922-09-21
 | pridate = 1922-09-21
 | inventor = N. Minorsky
 | invent1 = N. Minorsky
 | invent2 = 
 | assign1 = N. Minorsky
 | assign2 = 
 | class = 
}}
*  {{ cite patent
 | country = US
 | number = 1703317 A
 | status = patent
 | title = Automatic steering device
 | pubdate = 1929-02-26
 | gdate = 
 | fdate = 1925-04-08
 | pridate = 1925-04-08
 | inventor = N. Minorsky
 | invent1 = N. Minorsky
 | invent2 = 
 | assign1 = N. Minorsky
 | assign2 = 
 | class = 
}}
*  {{ cite patent
 | country = US
 | number = 1840911 A
 | status = patent
 | title = [[Earth inductor compass|Induction compass]]
 | pubdate = 1932-01-12
 | gdate = 
 | fdate = 1931-07-08
 | pridate = 1931-07-08
 | inventor = N. Minorsky
 | invent1 = N. Minorsky
 | invent2 = 
 | assign1 = N. Minorsky
 | assign2 = 
 | class = 
}}
*  {{ cite patent
 | country = US
 | number = 1853069 A
 | status = patent
 | title = Stabilizing apparatus
 | pubdate = 1932-04-12
 | gdate = 
 | fdate = 1931-06-15
 | pridate = 1931-06-15
 | inventor = N. Minorsky
 | invent1 = N. Minorsky
 | invent2 = 
 | assign1 = N. Minorsky
 | assign2 = 
 | class = 
}}
*  {{ cite patent
 | country = US
 | number = RE19038 E
 | status = patent
 | title = [[Earth inductor compass|Induction compass]]
 | pubdate = 1934-01-02
 | gdate = 
 | fdate = 1925-07-08
 | pridate = 1925-07-08
 | inventor = N. Minorsky
 | invent1 = N. Minorsky
 | invent2 = 
 | assign1 = Bendix Aviation Corporation
 | assign2 = 
 | class = 
}}
*  {{ cite patent
 | country = US
 | number = 1950946 A
 | status = patent
 | title = Navigational instrument
 | pubdate = 1934-03-13
 | gdate = 
 | fdate = 1930-02-28
 | pridate = 1930-02-28
 | inventor = N. Minorsky
 | invent1 = N. Minorsky
 | invent2 = 
 | assign1 = Pioneer Instr. Co. Inc.
 | assign2 = 
 | class = 
}}
*  {{ cite patent
 | country = US
 | number = 2017072 A
 | status = patent
 | title = Stabilizing apparatus
 | pubdate = 1935-10-15
 | gdate = 
 | fdate = 1932-04-11 
 | pridate = 1932-04-11 
 | inventor = N. Minorsky
 | invent1 = N. Minorsky
 | invent2 = 
 | assign1 = N. Minorsky
 | assign2 = 
 | class = 
}}
*  {{ cite patent
 | country = US
 | number = 2202162 A
 | status = patent
 | title = Antirolling stabilization of ships
 | pubdate = 1940-05-28
 | gdate = 
 | fdate = 1936-11-21 
 | pridate = 1936-11-21 
 | inventor = N. Minorsky
 | invent1 = N. Minorsky
 | invent2 = 
 | assign1 = N. Minorsky
 | assign2 = 
 | class = 
}}
*  {{ cite patent
 | country = US
 | number = 2071759 A
 | status = patent
 | title = Electron discharge tube system
 | pubdate = 1937-02-23
 | gdate = 
 | fdate = 1926-06-25
 | pridate = 1926-06-25 
 | inventor = N. Minorsky
 | invent1 = N. Minorsky
 | invent2 = 
 | assign1 = RCA Corp.
 | assign2 = 
 | class = 
}}
*  {{ cite patent
 | country = US
 | number = 2449563 A
 | status = patent
 | title = Balancing machine
 | pubdate = 1948-09-21
 | gdate = 
 | fdate = 1943-05-04
 | pridate = 1943-05-04 
 | inventor = N. Minorsky
 | invent1 = N. Minorsky
 | invent2 = 
 | assign1 = Gyro Balance Corp.
 | assign2 = 
 | class = 
}}
*  {{ cite patent
 | country = US
 | number = 2590029 A
 | status = patent
 | title = Torque amplifying system
 | pubdate = 1952-03-18
 | gdate = 
 | fdate = 1952-03-18
 | pridate = 1945-10-26 
 | inventor = N. Minorsky
 | invent1 = N. Minorsky
 | invent2 = 
 | assign1 = Lear Inc.
 | assign2 = 
 | class = 
}}

==References==
{{reflist}}

{{DEFAULTSORT:Minorsky, Nicolas}}
[[Category:Mathematics-related lists]]
[[Category:Bibliographies by writer]]</text>
      <sha1>ptzjsdvr0s9l9vy11h02zh1e7ecjwgi</sha1>
    </revision>
  </page>
  <page>
    <title>Livingstone graph</title>
    <ns>0</ns>
    <id>45209715</id>
    <revision>
      <id>852405309</id>
      <parentid>852405056</parentid>
      <timestamp>2018-07-28T19:29:34Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Removing category Graphs of radius 4 per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2018 July 21]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1601">{{Orphan|date=June 2016}}

{{infobox graph
 | name = Livingstone graph
 | image = 
 | image_caption = 
 | namesake = 
 | vertices = 266
 | edges = 1463
 | automorphisms= 175560 ([[Janko group J1|J&lt;sub&gt;1&lt;/sub&gt;]])
 | girth = 5
 | radius = 4
 | diameter = 4
 | chromatic_number = 
 | chromatic_index = 
 | properties = [[Symmetric graph|Symmetric]]&lt;br&gt;[[Distance-transitive graph|Distance-transitive]]&lt;br&gt;[[Primitive graph|Primitive]]
}}

In the [[mathematics|mathematical]] field of [[graph theory]], the '''Livingstone graph''' is a [[distance-transitive graph]] with 266 vertices and 1463 edges. It is the largest distance-transitive graph with degree 11.&lt;ref&gt;{{MathWorld|urlname=LivingstoneGraph|title=Livingstone Graph}}&lt;/ref&gt;

== Algebraic properties ==

The [[automorphism group]] of the Livingstone graph is the [[Sporadic group|sporadic]] [[simple group]] [[Janko group J1|J&lt;sub&gt;1&lt;/sub&gt;]], and the stabiliser of a point is [[Projective linear group|PSL(2,11)]]. As the stabiliser is maximal in J&lt;sub&gt;1&lt;/sub&gt;, it acts primitively on the graph.

As the Livingstone graph is distance-transitive, PSL(2,11) acts transitively on the set of 11 vertices adjacent to a reference vertex ''v'', and also on the set of 12 vertices at distance 4 from ''v''. The second action is equivalent to the standard action of PSL(2,11) on the projective line over '''F'''&lt;sub&gt;11&lt;/sub&gt;; the first is equivalent to an exceptional action on 11 points, related to the [[Block design#Paley biplane|Paley biplane]].

== References ==
{{reflist}}

[[Category:Individual graphs]]
[[Category:Regular graphs]]


{{combin-stub}}</text>
      <sha1>d48kx6qxnr0fe46ocno6z25p623vbxu</sha1>
    </revision>
  </page>
  <page>
    <title>Locally discrete collection</title>
    <ns>0</ns>
    <id>17950868</id>
    <revision>
      <id>790451805</id>
      <parentid>787182040</parentid>
      <timestamp>2017-07-13T20:51:08Z</timestamp>
      <contributor>
        <username>The Banner</username>
        <id>8144267</id>
      </contributor>
      <minor/>
      <comment>v1.39 - Repaired 2 links to disambiguation pages - [[WP:DPL|(You can help)]] - [[Basis]], [[Regular]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1997">In [[mathematics]], particularly [[topology]], collections of subsets are said to be locally discrete if they look like they have precisely one element from a local point of view. The study of locally discrete collections is worthwhile as [[Bing's metrization theorem]] shows.

==Formal definition==

Let ''X'' be a [[topological space]]. A collection {G&lt;sub&gt;a&lt;/sub&gt;} of subsets of ''X'' is said to be locally discrete, if each point of the space has a [[neighbourhood]] intersecting at most one element of the collection. A collection of subsets of ''X'' is said to be countably locally discrete, if it is the countable union of locally discrete collections.

==Properties and examples==

1. Locally discrete collections are always [[Locally finite collection|locally finite]]. See the page on local finiteness.

2. If a collection of subsets of a topological space X is locally discrete, it must satisfy the property that each point of the space belongs to at most one element of the collection. This means that only collections of pairwise disjoint sets can be locally discrete. 

3. A [[Hausdorff space]] cannot have a locally discrete basis unless it is itself discrete. The same property holds for a [[T1 space|T&lt;sub&gt;1&lt;/sub&gt; space]].

4. The following is known as Bing's metrization theorem:

A space ''X'' is metrizable iff it is regular and has a basis that is countably locally discrete.

5. A [[countable set|countable]] collection of sets is necessarily countably locally discrete. Therefore, if X is a metrizable space with a [[Second countable|countable basis]], one implication of Bing's metrization theorem holds. In fact, Bing's metrization theorem is almost a corollary of the [[Nagata-Smirnov metrization theorem|Nagata-Smirnov theorem]].

==See also==

*[[Locally finite collection]]
*[[Nagata-Smirnov metrization theorem]]
*[[Bing metrization theorem]]

==References==

*James Munkres (1999). Topology, 2nd edition, Prentice Hall. {{ISBN|0-13-181629-2}}.

[[Category:Topology]]</text>
      <sha1>n3535tq78wr03rxpezoo19dyf0ouc80</sha1>
    </revision>
  </page>
  <page>
    <title>Locally integrable function</title>
    <ns>0</ns>
    <id>4258398</id>
    <revision>
      <id>863383909</id>
      <parentid>863383674</parentid>
      <timestamp>2018-10-10T12:48:55Z</timestamp>
      <contributor>
        <ip>143.107.45.1</ip>
      </contributor>
      <comment>/* Lp is a subspace of L1,loc for all p ≥ 1 */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="28194">In [[mathematics]], a '''locally integrable function''' (sometimes also called '''locally summable function''')&lt;ref&gt;According to {{harvtxt|Gel'fand|Shilov|1964|p=3}}.&lt;/ref&gt; is a [[function (mathematics)|function]] which is integrable (so its integral is finite) on every [[compact subset]] of its [[domain (mathematics)#Domain of a function|domain of definition]]. The importance of such functions lies in the fact that their [[function space]] is similar to [[Lp space|{{math|''L''&lt;sub&gt;''p''&lt;/sub&gt;}} spaces]], but its members are not required to satisfy any growth restriction on their behavior at infinity: in other words, locally integrable functions can grow arbitrarily fast at infinity, but are still manageable in a way similar to ordinary integrable functions.

== Definition ==

===Standard definition===
{{EquationRef|1|Definition 1}}.&lt;ref name="ScVl"&gt;See for example {{Harv|Schwartz|1998|p=18}} and {{Harv|Vladimirov|2002|p=3}}.&lt;/ref&gt; Let {{math|Ω}} be an [[open set]] in  the [[Euclidean space]] {{math|ℝ''&lt;sup&gt;n&lt;/sup&gt;''}} and  {{math|''f'' : Ω → ℂ}} be a [[Lebesgue measure|Lebesgue]] [[measurable function]]. If {{math|''f''}} on {{math|Ω}} is such that

:&lt;math&gt; \int_K | f |\, \mathrm{d}x &lt;+\infty,&lt;/math&gt;

i.e. its [[Lebesgue integral]] is finite on all [[compact set|compact subsets]] {{math|''K''}} of {{math|Ω}},&lt;ref&gt;Another slight variant of this definition, chosen by {{harvtxt|Vladimirov|2002|p=1}}, is to require only that {{math|''K'' ⋐ Ω}} (or, using the notation of {{harvtxt|Gilbarg|Trudinger|2001|p=9}}, {{math|''K'' ⊂⊂ Ω}}), meaning that {{math|''K''}} ''is strictly included in'' {{math|Ω}} i.e. it is a set having compact [[Closure (topology)|closure]] [[subset|strictly included]] in the given ambient set.&lt;/ref&gt; then {{math|''f''}}&amp;thinsp; is called ''locally integrable''. The [[Set (mathematics)|set]] of all such functions is denoted by {{math|''L''&lt;sub&gt;1,loc&lt;/sub&gt;(Ω)}}:

:&lt;math&gt;L_{1,\mathrm{loc}}(\Omega)=\bigl\{f:\Omega\to\mathbb{C}\text{ measurable}\,\big|\, f|_K \in L_1(K)\ \forall\, K \subset \Omega,\, K \text{ compact}\bigr\},&lt;/math&gt;

where {{math|''f''&amp;thinsp;&lt;nowiki&gt;|&lt;/nowiki&gt;&lt;sub&gt;''K''&lt;/sub&gt;}} denotes the [[restriction of a function|restriction]] of {{math|''f''}}&amp;thinsp; to the set {{math|''K''}}.

The classical definition of a locally integrable function involves only [[Measure theory|measure theoretic]] and [[Topological space|topological]]&lt;ref&gt;The notion of compactness must obviously be defined on the given abstract measure space.&lt;/ref&gt; concepts and can be carried over abstract to [[Complex number|complex-valued]] functions on a topological [[measure space]] {{math|(''X'',&amp;thinsp;Σ,&amp;thinsp;''μ'')}}:&lt;ref&gt;This is the approach developed for example by {{harvtxt|Cafiero|1959|pp=285–342}} and by {{harvtxt|Saks|1937|loc = chapter I}}, without dealing explicitly with the locally integrable case.&lt;/ref&gt; however, since the most common application of such functions is to [[Distribution (mathematics)|distribution theory]] on Euclidean spaces,&lt;ref name="ScVl"/&gt; all the definitions in this and the following sections deal explicitly only with this important case.

===An alternative definition===
{{EquationRef|2|Definition 2}}.&lt;ref&gt;See for example {{Harv|Strichartz|2003|pp=12–13}}.&lt;/ref&gt; Let {{math|Ω}} be an open set in the Euclidean space {{math|ℝ''&lt;sup&gt;n&lt;/sup&gt;''}}. Then a [[Function (mathematics)|function]] {{math|''f'' : Ω → ℂ}} such that

:&lt;math&gt; \int_\Omega | f \varphi|\, \mathrm{d}x &lt;+\infty,&lt;/math&gt;

for each [[test function]] {{math|''φ'' ∈ {{SubSup|C|c|∞}}(Ω)}} is called ''locally integrable'', and the set of such functions is denoted  by {{math|''L''&lt;sub&gt;1,loc&lt;/sub&gt;(Ω)}}. Here {{math|{{SubSup|C|c|∞}}(Ω)}} denotes the set of all infinitely differentiable functions {{math|''φ'' : Ω → ℝ}} with [[Support (mathematics)#Compact support|compact support]] contained in {{math|Ω}}.

This definition has its roots in the approach to measure and integration theory based on the concept of [[Continuous linear functional#Continuous linear functionals|continuous linear functional]] on a [[topological vector space]], developed by [[Nicolas Bourbaki]] and his school:&lt;ref&gt;This approach was praised by {{harvtxt|Schwartz|1998|pp=16–17}} who remarked also its usefulness, however using {{EquationNote|1|Definition&amp;nbsp;1}} to define locally integrable functions.&lt;/ref&gt; it is also the one adopted by {{Harvtxt|Strichartz|2003}} and by {{Harvtxt|Maz'ya|Shaposhnikova|2009|p=34}}.&lt;ref&gt;Be noted that Maz'ya and Shaposhnikova define explicitly only the "localized" version of the [[Sobolev space]] {{math|''W''&lt;sup&gt;''k'',''p''&lt;/sup&gt;(Ω)}}, nevertheless explicitly asserting that the same method is used to define localized versions of all other [[Banach space]]s used in the cited book: in particular,  {{math|''L''&lt;sub&gt;''p'',loc&lt;/sub&gt;(Ω)}} is introduced on page 44.&lt;/ref&gt; This "distribution theoretic" definition is equivalent to the standard one, as the following lemma proves:

{{EquationRef|3|Lemma 1}}. A given function {{math|''f'' : Ω → ℂ}} is locally integrable according to {{EquationNote|1|Definition&amp;nbsp;1}} if and only if it is locally integrable according to {{EquationNote|2|Definition&amp;nbsp;2}}, i.e.

:&lt;math&gt; \int_K | f |\, \mathrm{d}x &lt;+\infty \quad \forall\, K \subset \Omega,\, K \text{ compact} \quad \Longleftrightarrow \quad 
\int_\Omega | f \varphi|\, \mathrm{d}x &lt;+\infty \quad \forall\, \varphi \in C^\infty_{\mathrm{c}}(\Omega).&lt;/math&gt;

&lt;div style="clear:both;width:95%;" class="NavFrame"&gt;
&lt;div class="NavHead" style="background-color:#FFFAF0; text-align:left; font-size:larger;"&gt;Proof of {{EquationNote|3|Lemma&amp;nbsp;1}}&lt;/div&gt;
&lt;div class="NavContent" style="text-align:left;display:none;"&gt;

'''If part''':  Let {{math|''φ'' ∈ {{SubSup|C|c|∞}}(Ω)}} be a test function. It is [[Extreme value theorem|bounded]] by its [[supremum norm]] {{math|&lt;nowiki&gt;||&lt;/nowiki&gt;''φ''&lt;nowiki&gt;||&lt;/nowiki&gt;&lt;sub&gt;∞&lt;/sub&gt;}}, measurable, and has a [[Support (mathematics)#Compact support|compact support]], let's call it {{math|''K''}}. Hence

:&lt;math&gt;\int_\Omega | f \varphi|\, \mathrm{d}x = \int_K |f|\,|\varphi|\, \mathrm{d}x \le\|\varphi\|_\infty\int_K | f |\, \mathrm{d}x&lt;\infty&lt;/math&gt;

by {{EquationNote|1|Definition&amp;nbsp;1}}.

'''Only if part''': Let {{math|''K''}} be a compact subset of the open set {{math|Ω}}. We will first construct a test function {{math|''φ&lt;sub&gt;K&lt;/sub&gt;'' ∈ {{SubSup|C|c|∞}}(Ω)}} which majorises the [[indicator function]] {{math|''χ&lt;sub&gt;K&lt;/sub&gt;''}} of {{math|''K''}}.
The [[Distance#Distances between sets and between a point and a set|usual set distance]]&lt;ref&gt;Not to be confused with the [[Hausdorff distance]].&lt;/ref&gt; between {{math|''K''}} and the [[Boundary (topology)|boundary]] {{math|∂Ω}} is strictly greater than zero, i.e.

:&lt;math&gt;\Delta:=d(K,\partial\Omega)&gt;0,&lt;/math&gt;

hence it is possible to choose a [[real number]] {{math|''δ''}} such that {{math|Δ &gt; 2''δ'' &gt; 0}} (if {{math|∂Ω}} is the empty set, take {{math|Δ {{=}} ∞}}). Let {{math|''K&lt;sub&gt;δ&lt;/sub&gt;''}} and {{math|''K''&lt;sub&gt;2''δ''&lt;/sub&gt;}} denote the [[Closure (topology)#Closure of a set|closed]] [[Neighbourhood (mathematics)#In a metric space|{{math|''δ''}}-neighborhood]] and {{math|2''δ''}}-neighborhood of {{math|''K''}}, respectively.  They are likewise compact and satisfy

:&lt;math&gt;K\subset K_\delta\subset K_{2\delta}\subset\Omega,\qquad d(K_\delta,\partial\Omega)=\Delta-\delta&gt;\delta&gt;0.&lt;/math&gt;

Now use [[convolution]] to define the function {{math|''φ&lt;sub&gt;K&lt;/sub&gt;'' : Ω → ℝ}} by

:&lt;math&gt;\varphi_K(x)={\chi_{K_\delta}\ast\varphi_\delta(x)}=
\int_{\mathbb{R}^n}\chi_{K_\delta}(y)\,\varphi_\delta(x-y)\,\mathrm{d}y,&lt;/math&gt;

where {{math|''φ&lt;sub&gt;δ&lt;/sub&gt;''}} is a [[mollifier]] constructed by using the [[Mollifier#Concrete example|standard positive symmetric one]]. Obviously {{math|''φ&lt;sub&gt;K&lt;/sub&gt;''}} is non-negative in the sense that {{math|''φ&lt;sub&gt;K&lt;/sub&gt;'' ≥ 0}}, infinitely differentiable, and its support is contained in {{math|''K''&lt;sub&gt;2''δ''&lt;/sub&gt;}}, in particular it is a test function. Since {{math|''φ&lt;sub&gt;K&lt;/sub&gt;''(''x'') {{=}} 1}} for all {{math|''x'' ∈ ''K''}}, we have that {{math|''χ&lt;sub&gt;K&lt;/sub&gt;'' ≤ ''φ&lt;sub&gt;K&lt;/sub&gt;''}}.

Let {{math|''f''}}&amp;thinsp; be a locally integrable function according to {{EquationNote|2|Definition&amp;nbsp;2}}. Then

:&lt;math&gt;\int_K|f|\,\mathrm{d}x=\int_\Omega|f|\chi_K\,\mathrm{d}x
\le\int_\Omega|f|\varphi_K\,\mathrm{d}x&lt;\infty.
&lt;/math&gt;

Since this holds for every compact subset {{math|''K''}} of {{math|Ω}}, the function {{math|''f''}}&amp;thinsp; is locally integrable according to {{EquationNote|1|Definition&amp;nbsp;1}}. □
&lt;/div&gt;
&lt;/div&gt;

===Generalization: locally ''p''-integrable functions===
{{EquationRef|4|Definition 3}}.&lt;ref name="Vlp3"&gt;See for example {{Harv|Vladimirov|2002|p=3}} and {{harv|Maz'ya|Poborchi|1997|p=4}}.&lt;/ref&gt; Let {{math|Ω}} be an open set in  the Euclidean space ℝ''&lt;sup&gt;n&lt;/sup&gt;'' and  {{math|''f'' : Ω → }}ℂ be a Lebesgue measurable function. If, for a given {{math|''p''}} with {{math|1 ≤ ''p'' ≤ +∞}}, {{math|''f''}} satisfies

:&lt;math&gt; \int_K | f|^p \,\mathrm{d}x &lt;+\infty,&lt;/math&gt;

i.e., it belongs to [[Lp space|{{math|''L''&lt;sub&gt;''p''&lt;/sub&gt;(''K'')}}]] for all [[compact set|compact subsets]] {{math|''K''}} of {{math|Ω}}, then {{math|''f''}} is called ''locally'' {{math|''p''}}-''integrable'' or also {{math|''p''}}-''locally integrable''.&lt;ref name="Vlp3"/&gt; The [[Set (mathematics)|set]] of all such functions is denoted by {{math|''L''&lt;sub&gt;''p'',loc&lt;/sub&gt;(Ω)}}:

:&lt;math&gt;L_{p,\mathrm{loc}}(\Omega)=\left\{f:\Omega\to\mathbb{C}\text{ measurable }\left|\ f\in L_p(K),\ \forall\, K \subset \Omega, K \text{ compact}\right.\right\}.&lt;/math&gt;

An alternative definition, completely analogous to the one given for locally integrable functions, can also be given for locally {{math|''p''}}-integrable functions: it can also be and proven equivalent to the one in this section.&lt;ref&gt;As remarked in the previous section, this is the approach adopted by {{harvtxt|Maz'ya|Shaposhnikova|2009}}, without developing the elementary details.&lt;/ref&gt; Despite their apparent higher generality, locally {{math|''p''}}-integrable functions form a subset of locally integrable functions for every {{math|''p''}} such that {{math|1 &lt; ''p'' ≤ +∞}}.&lt;ref&gt;Precisely, they form a [[vector subspace]] of {{math|''L''&lt;sub&gt;1,loc&lt;/sub&gt;(Ω)}}: see {{EquationNote|7|Corollary&amp;nbsp;1}} to {{EquationNote|6|Theorem&amp;nbsp;2}}.&lt;/ref&gt;

=== Notation ===
Apart from the different [[glyph]]s which may be used for the uppercase "L",&lt;ref&gt;See for example {{Harv|Vladimirov|2002|p=3}}, where a calligraphic '''&amp;#x2112;''' is used.&lt;/ref&gt; there are few variants for the notation of the set of locally integrable functions
*&lt;math&gt;L^p_{\mathrm{loc}}(\Omega),&lt;/math&gt; adopted by {{harv|Hörmander|1990|p=37}}, {{Harv|Strichartz|2003|pp=12–13}} and {{Harv|Vladimirov|2002|p=3}}.
*&lt;math&gt;L_{p,\mathrm{loc}}(\Omega),&lt;/math&gt; adopted by {{harv|Maz'ya|Poborchi|1997|p=4}} and {{Harvtxt|Maz'ya|Shaposhnikova|2009|p=44}}.
*&lt;math&gt;L_p(\Omega,\mathrm{loc}),&lt;/math&gt; adopted by {{harv|Maz'ja|1985|p=6}} and {{harv|Maz'ya|2011|p=2}}.

== Properties ==

===''L''&lt;sub&gt;''p'',loc&lt;/sub&gt; is a complete metric space for all ''p'' ≥ 1===
{{EquationRef|5|Theorem 1}}.&lt;ref&gt;See {{harv|Gilbarg|Trudinger|1998|p=147}}, {{harv|Maz'ya|Poborchi|1997|p=5}} for a statement of this results, and also the brief notes in {{harv|Maz'ja|1985|p=6}} and {{harv|Maz'ya|2011|p=2}}.&lt;/ref&gt; {{math|''L''&lt;sub&gt;''p'',loc&lt;/sub&gt;}} is a [[Complete metric space|complete metrizable space]]: its topology can be generated by the following [[Metric (mathematics)|metric]]:
:&lt;math&gt;d(u,v)=\sum_{k\geq 1}\frac{1}{2^k}\frac{\Vert u - v\Vert_{p,\omega_k}}{1+\Vert u - v\Vert_{p,\omega_k}}\qquad u, v\in L_{p,\mathrm{loc}}(\Omega),&lt;/math&gt;
where {{math|{''ω''&lt;sub&gt;''k''&lt;/sub&gt;}&lt;sub&gt;''k''≥1&lt;/sub&gt;}} is a family of non empty open sets such that
* {{math|''ω''&lt;sub&gt;''k''&lt;/sub&gt; ⊂⊂ ''ω''&lt;sub&gt;''k''+1&lt;/sub&gt;}}, meaning that {{math|''ω''&lt;sub&gt;''k''&lt;/sub&gt;}} ''is strictly included in'' {{math|''ω''&lt;sub&gt;''k''+1&lt;/sub&gt;}} i.e. it is a set having compact closure strictly included in the set of higher index.
* {{math|∪&lt;sub&gt;''k''&lt;/sub&gt;''ω''&lt;sub&gt;''k''&lt;/sub&gt; {{=}} Ω}}.
* &lt;math&gt;\scriptstyle{\Vert\cdot\Vert_{p,\omega_k}}\to\mathbb{R}^+&lt;/math&gt;, ''k'' ∈ ℕ is an [[indexed family]] of [[seminorm]]s, defined as
::&lt;math&gt; {\Vert u \Vert_{p,\omega_k}} = \left (\int_{\omega_k} | u(x)|^p \,\mathrm{d}x\right)^{1/p}\qquad\forall\, u\in L_{p,\mathrm{loc}}(\Omega).&lt;/math&gt;

In references {{harv|Gilbarg|Trudinger|1998|p=147}}, {{harv|Maz'ya|Poborchi|1997|p=5}}, {{harv|Maz'ja|1985|p=6}} and {{harv|Maz'ya|2011|p=2}}, this theorem is stated but not proved on a formal basis:&lt;ref&gt;{{harvtxt|Gilbarg|Trudinger|1998|p=147}} and {{harvtxt|Maz'ya|Poborchi|1997|p=5}} only sketch very briefly the method of proof, while in {{harv|Maz'ja|1985|p=6}} and {{harv|Maz'ya|2011|p=2}} it is assumed as a known result, from which the subsequent development starts.&lt;/ref&gt; a complete proof of a more general result, which includes it, is found in {{harv|Meise|Vogt|1997|p=40}}.

===''L''&lt;sub&gt;''p''&lt;/sub&gt; is a subspace of ''L''&lt;sub&gt;1,loc&lt;/sub&gt; for all ''p'' ≥ 1===
{{EquationRef|6|Theorem 2}}. Every function {{math|''f''}} belonging to {{math|''L''&lt;sub&gt;''p''&lt;/sub&gt;(Ω)}}, {{math|1 ≤ ''p'' ≤ +∞}}, where {{math|Ω}} is an [[open subset]] of ℝ''&lt;sup&gt;n&lt;/sup&gt;'', is locally integrable.

'''Proof'''. The case {{math|''p'' {{=}} 1}} is trivial, therefore in the sequel of the proof it is assumed that {{math|1 &lt; ''p'' ≤ +∞}}. Consider the [[Indicator function|characteristic function]] {{math|''χ''&lt;sub&gt;''K''&lt;/sub&gt;}} of a compact subset {{math|''K''}} of  {{math|Ω}}: then, for {{math|''p'' ≤ +∞}},

:&lt;math&gt;\left|{\int_\Omega|\chi_K|^q\,\mathrm{d}x}\right|^{1/q}=\left|{\int_K \mathrm{d}x}\right|^{1/q}=|K|^{1/q}&lt;+\infty,&lt;/math&gt;

where
*{{math|''q''}} is a [[positive number]] such that {{math|1/''p'' + 1/''q''}} = {{math|1}} for a given {{math|1 ≤ ''p'' ≤ +∞}}
*{{math|&lt;nowiki&gt;|&lt;/nowiki&gt;''K''&lt;nowiki&gt;|&lt;/nowiki&gt;}} is the [[Lebesgue measure]] of the [[compact set]] {{math|''K''}}
Then by [[Hölder's inequality]], the [[Product (mathematics)|product]] {{math|''fχ''&lt;sub&gt;''K''&lt;/sub&gt;}} is [[Integrable function|integrable]] i.e. belongs to {{math|''L''&lt;sub&gt;1&lt;/sub&gt;(Ω)}} and

:&lt;math&gt;{\int_K|f|\,\mathrm{d}x}={\int_\Omega|f\chi_K|\,\mathrm{d}x}\leq\left|{\int_\Omega|f|^p\,\mathrm{d}x}\right|^{1/p}\left|{\int_K \mathrm{d}x}\right|^{1/q}=\|f\|_p|K|^{1/q}&lt;+\infty,&lt;/math&gt;

therefore

:&lt;math&gt;f\in L_{1,\mathrm{loc}}(\Omega).&lt;/math&gt;

Note that since the following inequality is true

:&lt;math&gt;{\int_K|f|\,\mathrm{d}x}={\int_\Omega|f\chi_K|\,\mathrm{d}x}\leq\left|{\int_K|f|^p \,\mathrm{d}x}\right|^{1/p}\left|{\int_K \mathrm{d}x}\right|^{1/q}=\|f\|_p|K|^{1/q}&lt;+\infty,&lt;/math&gt;

the theorem is true also for functions {{math|''f''}} belonging only to the space of locally {{math|''p''}}-integrable functions, therefore the theorem implies also the following result.

{{EquationRef|7|Corollary 1}}. Every function &lt;math&gt; f &lt;/math&gt; in &lt;math&gt;L_{p,loc}(\Omega)&lt;/math&gt;, &lt;math&gt; 1&lt;p\leq\infty &lt;/math&gt;, is locally integrable, i. e. belongs to &lt;math&gt; L_{1,loc}(\Omega) &lt;/math&gt;.

&lt;b&gt;Note:&lt;/b&gt; If &lt;math&gt; \Omega &lt;/math&gt; is an [[open subset]] of &lt;math&gt; \mathbb{R}^n&lt;/math&gt; that is also bounded, then one has the standard inclusion &lt;math&gt; L_p(\Omega) \subset L_1(\Omega)&lt;/math&gt; which makes sense given the above inclusion &lt;math&gt; L_1(\Omega)\subset L_{1,loc}(\Omega)&lt;/math&gt;. But the first of these statements is not true if &lt;math&gt; \Omega &lt;/math&gt; is not bounded; then it is still true that &lt;math&gt; L_p(\Omega) \subset L_{1,loc}(\Omega)&lt;/math&gt; for any &lt;math&gt;p&lt;/math&gt;, but not that &lt;math&gt; L_p(\Omega)\subset L_1(\Omega) &lt;/math&gt;. To see this, one typically considers the function &lt;math&gt; u(x)=1 &lt;/math&gt;, which is in &lt;math&gt; L_{\infty}(\mathbb{R}^n) &lt;/math&gt; but not in &lt;math&gt; L_p(\mathbb{R}^n)&lt;/math&gt; for any finite &lt;math&gt;p&lt;/math&gt;.

=== ''L''&lt;sub&gt;1,loc&lt;/sub&gt; is the space of densities of absolutely continuous measures===

{{EquationRef|7|Theorem 3}}. A function {{math|''f''}} is the [[Density function (measure theory)|density]] of an [[Absolute continuity#Absolute continuity of measures|absolutely continuous measure]] if and only if &lt;math&gt; f\in L_{1,loc}&lt;/math&gt;.

The proof of this result is sketched by {{harv|Schwartz|1998|p=18}}. Rephrasing its statement, this theorem asserts that every locally integrable function defines an absolutely continuous measure and conversely that every absolutely continuous measures defines a locally integrable function: this is also, in the abstract measure theory framework, the form of the important [[Radon–Nikodym theorem]] given by [[Stanisław Saks]] in his treatise.&lt;ref&gt;According to {{harvtxt|Saks|1937|p=36}}, "''If {{math|E}} is a set of finite measure, or, more generally the sum of a sequence of sets of finite measure ''(''{{math|μ}}'')'', then, in order that an additive function of a set ''({{math|&amp;#x1D51B;}})'' on {{math|E}} be absolutely continuous on {{math|E}}, it is necessary and sufficient that this function of a set be the indefinite integral of some integrable function of a point of {{math|E}}''". Assuming ({{math|''μ''}}) to be the Lebesgue measure, the two statements can be seen to be equivalent.&lt;/ref&gt;

==Examples==
*The constant function {{math|1}} defined on the real line is locally integrable but not globally integrable. More generally, [[constant (mathematics)|constants]], [[continuous function]]s&lt;ref&gt;See for example {{harv|Hörmander|1990|p=37}}.&lt;/ref&gt; and [[integrable function]]s are locally integrable.&lt;ref&gt;See {{harv|Strichartz|2003|p=12}}.&lt;/ref&gt;
* The function
:: &lt;math&gt;
f(x)=
\begin{cases}
1/x &amp;x\neq 0,\\
0 &amp; x=0,
\end{cases}
&lt;/math&gt;
: is not locally integrable in {{math|''x'' {{=}} 0}}: it is indeed locally integrable near this point since its integral over every compact set not including it is finite. Formally speaking, {{math|1/''x'' ∈ ''L''&lt;sub&gt;1,loc&lt;/sub&gt;}}(ℝ&amp;nbsp;\&amp;nbsp;0):&lt;ref&gt;See {{harv|Schwartz|1998|p=19}}.&lt;/ref&gt; however, this function can be extended to a distribution on the whole ℝ as a [[Cauchy principal value]].&lt;ref&gt;See {{Harv|Vladimirov|2002|pp=19–21}}.&lt;/ref&gt;
* The preceding example raises a question: does every function which is locally integrable in {{math|Ω}} ⊊ ℝ admit an extension to the whole ℝ as a distribution? The answer is negative, and a counterexample is provided by the following function:
:: &lt;math&gt;
f(x)= 
\begin{cases}
e^{1/x} &amp;x\neq 0,\\
0 &amp; x=0,
\end{cases}
&lt;/math&gt;
: does not define any distribution on ℝ.&lt;ref&gt;See {{Harv|Vladimirov|2002|p=21}}.&lt;/ref&gt;  
* The following example, similar to the preceding one, is a function belonging to {{math|''L''&lt;sub&gt;1,loc&lt;/sub&gt;}}(ℝ&amp;nbsp;\&amp;nbsp;0) which serves as an elementary [[counterexample]] in the application of the theory of distributions to [[differential operator]]s with [[Irregular singularity|irregular singular coefficients]]:
:: &lt;math&gt;
f(x)= 
\begin{cases}
k_1 e^{1/x^2} &amp;x&gt;0,\\
0 &amp; x=0,\\
k_2 e^{1/x^2} &amp;x&lt;0,
\end{cases}
&lt;/math&gt;
:where {{math|''k''&lt;sub&gt;1&lt;/sub&gt;}} and {{math|''k''&lt;sub&gt;2&lt;/sub&gt;}} are [[Complex number|complex constants]], is a general solution of the following elementary [[Fuchsian differential equation|non-Fuchsian differential equation]] of first order 
::&lt;math&gt;x^3\frac{\mathrm{d}f}{\mathrm{d}x}+2f=0.&lt;/math&gt;
:Again it does not define any distribution on the whole ℝ, if {{math|''k''&lt;sub&gt;1&lt;/sub&gt;}} or {{math|''k''&lt;sub&gt;2&lt;/sub&gt;}} are not zero: the only distributional global solution of such equation is therefore the zero distribution, and this shows how, in this branch of the theory of differential equations, the methods of the theory of distributions cannot be expected to have the same success achieved in other branches of the same theory, notably in the theory of linear differential equations with constant coefficients.&lt;ref&gt;For a brief discussion of this example, see {{harv|Schwartz|1998|pp=131–132}}.&lt;/ref&gt;

== Applications ==

Locally integrable functions play a prominent role in [[Distribution (mathematics)|distribution theory]] and they occur in the definition of various classes of [[function (mathematics)|functions]] and [[function space]]s, like [[Bounded variation|functions of bounded variation]]. Moreover, they appear in the [[Radon–Nikodym theorem]] by characterizing the absolutely continuous part of every measure.

== See also ==
*[[Compact set]]
*[[Distribution (mathematics)]]
*[[Lebesgue's density theorem]]
*[[Lebesgue differentiation theorem]]
*[[Lebesgue integral]]
*[[Lp space]]

==Notes==
{{reflist|29em}}

==References==
*{{Citation
| last = Cafiero
| first = Federico
| author-link = Federico Cafiero
| title = Misura e integrazione
| place = [[Rome|Roma]]
| publisher = Edizioni Cremonese
| year = 1959
| series = Monografie matematiche del [[Consiglio Nazionale delle Ricerche]]
| volume = 5
| pages =  VII+451
| id = 
| mr = 0215954 
| zbl = 0171.01503
| language = Italian
}}. ''Measure and integration'' (as the English translation of the title reads) is a definitive monograph on integration and measure theory: the treatment of the limiting behavior of the integral of various kind of [[sequences]] of measure-related structures (measurable functions, [[measurable set]]s, measures and their combinations) is somewhat conclusive.
*{{Citation
 | last1 = Gel'fand
 | first1 = I. M. 
 | author-link = Israel Gelfand
 | last2 = Shilov
 | first2 = G. E.
 | author2-link = Georgiy Shilov
 | title = Generalized functions. Vol. I: Properties and operations
 | place = New York–London
 | publisher = [[Academic Press]]
 | origyear = 1958
 | year = 1964
 | edition =
 | pages = xviii+423
 | url = https://books.google.com/books?id=QoWBSgAACAAJ
 | isbn = 978-0-12-279501-5 
 | mr = 0166596
 | zbl = 0115.33101}}. Translated from the original 1958 Russian edition by Eugene Saletan, this is an important monograph on the theory of [[generalized functions]], dealing both with distributions and analytic functionals.
*{{Citation
 | last = Gilbarg
 | first = David
 | author-link = David Gilbarg
 | last2 = Trudinger
 | first2 = Neil S. 
 | author2-link = Neil Trudinger
 | title = Elliptic partial differential equations of second order
 | place = Berlin – Heidelberg – New York
 | publisher = [[Springer Verlag]]
 | series = Classics in Mathematics
 | origyear = 1998
 | year = 2001
 | edition = Revised 3rd printing of 2nd 
 | pages = xiv+517
 | language =
 | url = https://books.google.com/books?id=eoiGTf4cmhwC&amp;printsec=frontcover&amp;hl=it&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=true
 | doi =
 | id =
 | isbn = 3-540-41160-7
 | mr = 1814364 
 | zbl = 1042.35002
}}.
*{{Citation
| last = Hörmander
| first = Lars
| author-link = Lars Hörmander
| title = The analysis of linear partial differential operators I
| place = [[Berlin]]-[[Heidelberg]]-[[New York City]]
| publisher = [[Springer-Verlag]]
| year = 1990
| series = Grundlehren der Mathematischen Wissenschaft
| volume = 256
| edition = 2nd
| pages = xii+440
| url = 
| doi = 
| id = 
| mr = 1065136 
| zbl= 0712.35001
| isbn = 0-387-52343-X}} (available also as {{ISBN|3-540-52343-X}}).
*{{Citation
| last = Maz'ja
| first = Vladimir G. 
| authorlink = Vladimir Gilelevich Maz'ya
| title = Sobolev Spaces
| publisher = [[Springer-Verlag]]
| place = Berlin–Heidelberg–New York
| year = 1985
| pages = xix+486
| isbn = 3-540-13589-8
| id = 
| mr = 817985 
| zbl = 0692.46023
}} (available also as {{ISBN|0-387-13589-8}}).
*{{Citation
 | last = Maz'ya
 | first = Vladimir G. 
 | authorlink = Vladimir Gilelevich Maz'ya
 | title = Sobolev Spaces. With Applications to Elliptic Partial Differential Equations.
 | place = Berlin–Heidelberg–New York
 | publisher = [[Springer Verlag]]
 | series = Grundlehren der Mathematischen Wissenschaften 
 | volume = 342
 | origyear = 1985
 | year = 2011
 | edition = 2nd revised and augmented
 | pages = xxviii+866
 | language =
 | doi =
 | id =
 | isbn = 978-3-642-15563-5
 | mr = 2777530 
 | zbl = 1217.46002
}}.
*{{Citation
| last = Maz'ya
| first = Vladimir G. 
| authorlink = Vladimir Gilelevich Maz'ya
| last2 = Poborchi
| first2 = Sergei V.
| author2-link =
| title = Differentiable Functions on Bad Domains
| publisher = [[World Scientific]]
| place = Singapore–New Jersey–London–Hong Kong
| year = 1997
| pages = xx+481
| isbn = 981-02-2767-1
| id = 
| mr = 1643072
| zbl = 0918.46033
}}.
*{{Citation
  | last = Maz'ya
  | first = Vladimir G.
  | author-link = Vladimir Gilelevich Maz'ya
  | last2 = Shaposhnikova
  | first2 = Tatyana O.
  | author2-link = Tatyana Shaposhnikova
  | title = Theory of Sobolev multipliers. With applications to differential and integral operators
  | place = [[Heidelberg]]
  | publisher = [[Springer-Verlag]]
  | series = Grundlehren der Mathematischen Wissenschaft
  | volume = 337
  | origyear =
  | year = 2009
  | pages = xiii+609
  | language = 
  | url = https://books.google.com/books?id=QN8uP6Mn0yQC&amp;printsec=frontcover#v=onepage&amp;q&amp;f=true
  | doi = 
  | id = 
  | isbn = 978-3-540-69490-8
  | mr = 2457601
  | zbl = 1157.46001 
}}.
*{{Citation
 | last = Meise
 | first = Reinhold
 | author-link =
 | last2 = Vogt
 | first2 = Dietmar
 | author2-link =
 | title = Introduction to Functional Analysis
 | place = Oxford
 | publisher = [[Clarendon Press]]
 | series = Oxford Graduate Texts in Mathematics
 | volume = 2
 | year = 1997
 | pages = x+437
 | url =
 | doi =
 | id =
 | isbn = 0-19-851485-9
 | mr = 1483073
 | zbl = 0924.46002 
}}.
*{{Citation
| last = Saks
| first = Stanisław
| author-link = Stanisław Saks
| title = Theory of the Integral
| place = [[Warszawa]]-[[Lwów]]
| publisher = G.E. Stechert &amp; Co.
| year = 1937
| series = [[Monografie Matematyczne]]
| volume = 7
| edition = 2nd
| pages = VI+347
| url = http://pldml.icm.edu.pl/mathbwn/element/bwmeta1.element.dl-catalog-42a56b61-37f4-4c6b-a42b-ea95a98e407a?q=0f71728c-851f-486a-bc43-e8507297cea3$1&amp;qt=IN_PAGE
| jfm = 63.0183.05 
| mr = 0167578
| zbl = 0017.30004}}. English translation by [[Laurence Chisholm Young]], with two additional notes by [[Stefan Banach]]: the [[Mathematical Reviews]] number refers to the [[Dover Publications]] 1964 edition, which is basically a reprint.
*{{Citation
 | first = Laurent
 | last = Schwartz
 | authorlink = Laurent Schwartz
 | title = Théorie des distributions
 | place = Paris 
 | publisher = Hermann Éditeurs 
 | series = Publications de l'Institut de Mathématique de l'Université de Strasbourg
 | volume = No. IX–X
 | origyear = 1966
 | year = 1998
 | edition = Nouvelle
 | pages = xiii+420
 | language = French
 | url =
 | isbn = 2-7056-5551-4
 | mr = 0209834 
 | zbl = 0149.09501
}}.
*{{Citation
| last = Strichartz
| first = Robert S.
| title = A Guide to Distribution Theory and Fourier Transforms
| place = [[River Edge, NJ]]
| publisher = [[World Scientific|World Scientific Publishers]]
| year = 2003
| edition = 2nd printing
| pages = x+226
| url = https://books.google.com/books?id=T7vEOGGDCh4C&amp;printsec=frontcover&amp;dq=A+Guide+to+Distribution+Theory+and+Fourier+Transforms#v=onepage&amp;q=&amp;f=false
| doi = 
| mr = 2000535
| zbl = 1029.46039
| isbn = 981-238-430-8
}}.
*{{Citation
| last = Vladimirov
| first = V. S.
| author-link = Vasilii Sergeevich Vladimirov
| title = Methods of the theory of generalized functions
| place = London–New York
| publisher = [[Taylor &amp; Francis]]
| pages = XII+353
| series = Analytical Methods and Special Functions
| volume = 6
| year = 2002
| url = https://books.google.com/?id=hlumB8fkX0UC&amp;pg=PR1&amp;dq=Methods+of+the+theory+of+generalized+functions
| id = 
| mr = 2012831
| zbl = 1078.46029
| isbn = 0-415-27356-0}}. A monograph on the theory of [[generalized function]]s written with an eye towards their applications to [[several complex variables]] and [[mathematical physics]], as is customary for the Author.

== External links ==
*{{MathWorld |title=Locally integrable|author=Rowland, Todd|urlname=LocallyIntegrable}}
*{{springer
 | title=Locally integrable function
 | id= L/l060460
 | last= Vinogradova
 | first=I.A.
 }}

{{PlanetMath attribution|id=4430|title=Locally integrable function}}

[[Category:Measure theory]]
[[Category:Integral calculus]]
[[Category:Types of functions]]</text>
      <sha1>dv2ta0nyon2uc2hyhwa8ohxybe8oqbr</sha1>
    </revision>
  </page>
  <page>
    <title>Minimal ideal</title>
    <ns>0</ns>
    <id>34233472</id>
    <revision>
      <id>799903406</id>
      <parentid>799902463</parentid>
      <timestamp>2017-09-10T14:23:04Z</timestamp>
      <contributor>
        <username>Quondum</username>
        <id>12331483</id>
      </contributor>
      <comment>/* Properties */ which → that (less chance of ambiguous interpretation)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5589">In the branch of [[abstract algebra]] known as [[ring theory]], a '''minimal right ideal''' of a [[ring (mathematics)|ring]] ''R'' is a nonzero [[right ideal]] which contains no other nonzero right ideal. Likewise a '''minimal left ideal''' is a nonzero left ideal of ''R'' containing no other nonzero left ideals of ''R'', and a '''minimal ideal''' of ''R'' is a nonzero ideal containing no other nonzero two-sided ideal of ''R''. {{harv|Isaacs|2009|loc=p.&amp;nbsp;190}}

Said another way, minimal right ideals are [[minimal element]]s of the [[poset]] of nonzero right ideals of ''R'' ordered by inclusion.  The reader is cautioned that outside of this context, some posets of ideals may admit the zero ideal, and so zero could potentially be a minimal element in that poset. This is the case for the poset of [[prime ideal]]s of a ring, which may include the zero ideal as a [[minimal prime ideal]].

==Definition==
The definition of a minimal right ideal ''N'' of a module ''R'' is equivalent to the following conditions:
*If ''K'' is a right ideal of ''R'' with {{nowrap|{0} ⊆ ''K'' ⊆ ''N''}}, then either {{nowrap|1=''K'' = {0}{{null}}}} or {{nowrap|1=''K'' = ''N''}}.
*''N'' is a [[simple module|simple]] right ''R'' module.

Minimal right ideals are the [[duality (mathematics)|dual notion]] to the idea of [[maximal ideal|maximal right ideals]].

==Properties==
Many standard facts on minimal ideals can be found in standard texts such as {{harv|Anderson|Fuller|1992}}, {{harv|Isaacs|2009}}, {{harv|Lam|2001}}, and {{harv|Lam|1999}}.

* In a [[ring with unity]], [[maximal ideal|maximal right ideals]] always exist. In contrast, minimal right, left, or two-sided ideals in a ring need not exist.
* The right [[Socle (mathematics)#Socle of a module|socle of a ring]] &lt;math&gt;\mathrm{soc}(R_R)&lt;/math&gt; is an important structure defined in terms of the minimal right ideals of ''R''.
* Rings for which every right ideal contains a minimal right ideal are exactly the rings with an essential right socle.
* Any right [[Artinian ring]] or right [[Kasch ring]] has a minimal right ideal.
* [[domain (ring theory)|Domains]] that are not [[division ring]]s have no minimal right ideals.
* In rings with unity, minimal right ideals are necessarily [[principal ideal|principal right ideals]], because for any nonzero ''x'' in a minimal right ideal ''N'', the set ''xR'' is a nonzero right ideal of ''R'' inside ''N'', and so {{nowrap|1=''xR'' = ''N''}}.
* '''Brauer's lemma:''' Any minimal right ideal ''N'' in a ring ''R'' satisfies {{nowrap|1=''N''&lt;sup&gt;2&lt;/sup&gt; = {0}{{null}}}} or {{nowrap|1=''N'' = ''eR''}} for some [[idempotent element (ring theory)|idempotent element]] of ''R''. {{harv|Lam|2001|loc=p.&amp;nbsp;162}}
* If ''N''&lt;sub&gt;1&lt;/sub&gt; and ''N''&lt;sub&gt;2&lt;/sub&gt; are nonisomorphic minimal right ideals of ''R'', then the product {{nowrap|1=''N''&lt;sub&gt;1&lt;/sub&gt;''N''&lt;sub&gt;2&lt;/sub&gt; = {0}.}}
* If ''N''&lt;sub&gt;1&lt;/sub&gt; and ''N''&lt;sub&gt;2&lt;/sub&gt; are distinct minimal ideals of a ring ''R'', then {{nowrap|1=''N''&lt;sub&gt;1&lt;/sub&gt;''N''&lt;sub&gt;2&lt;/sub&gt; = {0}.}}
* A [[simple ring]] with a minimal right ideal is a [[semisimple ring]].
* In a [[semiprime ring]], there exists a minimal right ideal if and only if there exists a minimal left ideal. {{harv|Lam|2001|loc=p.&amp;nbsp;174}}

==Generalization==
A nonzero submodule ''N'' of a right module ''M'' is called a '''minimal submodule''' if it contains no other nonzero submodules of ''M''. Equivalently, ''N'' is a nonzero submodule of ''M'' which is a [[simple module]]. This can also be extended to [[bimodules]] by calling a nonzero sub-bimodule ''N'' a '''minimal sub-bimodule''' of ''M'' if ''N'' contains no other nonzero sub-bimodules.

If the module ''M'' is taken to be the right ''R'' module ''R''&lt;sub&gt;''R''&lt;/sub&gt;, then clearly the minimal submodules are exactly the minimal right ideals of ''R''. Likewise, the minimal left ideals of ''R'' are precisely the minimal submodules of the left module &lt;sub&gt;''R''&lt;/sub&gt;''R''. In the case of two-sided ideals, we see that the minimal ideals of ''R'' are exactly the minimal sub-bimodules of the bimodule &lt;sub&gt;''R''&lt;/sub&gt;''R''&lt;sub&gt;''R''&lt;/sub&gt;.

Just as with rings, there is no guarantee that minimal submodules exist in a module. Minimal submodules can be used to define the [[Socle (mathematics)#Socle of a module|socle of a module]].

==References==
{{Reflist}}
*{{citation |last1=Anderson |first1=Frank W.  |last2=Fuller |first2=Kent R.  |title=Rings and categories of modules   |series=[[Graduate Texts in Mathematics]]   |volume=13   |edition=2   |publisher=Springer-Verlag  |place=New York   |year=1992   |pages=x+376   |isbn=0-387-97845-3  |mr=1245487 }}
*{{citation |last=Isaacs |first=I. Martin  |title=Algebra: a graduate course |series=[[Graduate Studies in Mathematics]] |volume=100 |origyear=1994 |publisher=American Mathematical Society |place=Providence, RI |year=2009 |pages=xii+516 |isbn=978-0-8218-4799-2 |MR=2472787}}
*{{citation | last=Lam | first=Tsit-Yuen | title=Lectures on modules and rings | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Graduate Texts in Mathematics No. 189 | isbn=978-0-387-98428-5 | mr=1653294 | year=1999}}
*{{citation  |last=Lam |first=T. Y.   |title=A first course in noncommutative rings  |series=Graduate Texts in Mathematics   |volume=131   |edition=2   |publisher=Springer-Verlag   |place=New York   |year=2001   |pages=xx+385   |isbn=0-387-95183-0   |mr=1838439 }}

==External links==
* http://www.encyclopediaofmath.org/index.php/Minimal_ideal

&lt;!--- Categories ---&gt;
[[Category:Abstract algebra]]
[[Category:Ring theory]]
[[Category:Ideals]]</text>
      <sha1>59q4vfinhquh7opb6vb734q74mv5gi0</sha1>
    </revision>
  </page>
  <page>
    <title>Noncommutative unique factorization domain</title>
    <ns>0</ns>
    <id>24087239</id>
    <revision>
      <id>786594359</id>
      <parentid>696205440</parentid>
      <timestamp>2017-06-20T11:58:55Z</timestamp>
      <contributor>
        <username>CBM</username>
        <id>1108292</id>
      </contributor>
      <minor/>
      <comment>Manually reviewed edit to replace magic words per [[Special:PermanentLink/772743896#Future_of_magic_links|local rfc]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="758">{{Orphan|date=February 2013}}

In [[mathematics]], the '''noncommutative unique factorization domain''' is the [[noncommutative counterpart]] of the commutative or classical [[unique factorization domain]] (UFD).

==Example==
*The [[ring (mathematics)|ring]] of integral [[quaternions]]. If the coefficients a&lt;sub&gt;0&lt;/sub&gt;, a&lt;sub&gt;1&lt;/sub&gt;, a&lt;sub&gt;2&lt;/sub&gt;, a&lt;sub&gt;3&lt;/sub&gt; are integers or halves of odd integers of a rational quaternion a = a&lt;sub&gt;0&lt;/sub&gt; + a&lt;sub&gt;1&lt;/sub&gt;i + a&lt;sub&gt;2&lt;/sub&gt;j + a&lt;sub&gt;3&lt;/sub&gt;k then the quaternion is integral.

==References==
* R. Sivaramakrishnan, ''Certain number-theoretic episodes in algebra'', CRC Press, 2006, {{isbn|0-8247-5895-1}}

==Notes==
{{reflist}}

[[Category:Ring theory]]
[[Category:Number theory]]


{{numtheory-stub}}</text>
      <sha1>bslxv466gedfxquhbwgrgxgqah5ruyu</sha1>
    </revision>
  </page>
  <page>
    <title>Nucleic acid secondary structure</title>
    <ns>0</ns>
    <id>27157933</id>
    <revision>
      <id>863118847</id>
      <parentid>846701840</parentid>
      <timestamp>2018-10-08T20:44:04Z</timestamp>
      <contributor>
        <ip>130.108.104.143</ip>
      </contributor>
      <comment>/* Secondary structure prediction */ typo correction: do-&gt;to</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17670">{{DNA RNA structure}}
'''Nucleic acid secondary structure''' is the [[basepair]]ing interactions within a single [[nucleic acid]] polymer or between two polymers. It can be represented as a list of bases which are paired in a nucleic acid molecule.&lt;!--
--&gt;&lt;ref&gt;{{cite journal |last1=Dirks |first1=Robert M. |authorlink= |author2=Lin, Milo; Winfree, Erik &amp; Pierce, Niles A. |year=2004 |title=Paradigms for computational nucleic acid design |journal=[[Nucleic Acids Research]] |volume=32 |issue=4 |pages=1392–1403|doi=10.1093/nar/gkh291|pmid=14990744 |pmc=390280}}&lt;/ref&gt;
The secondary structures of biological [[DNA]]'s and [[RNA]]'s tend to be different: biological DNA mostly exists as fully [[base pair]]ed double helices, while biological RNA is single stranded and often forms complex and intricate base-pairing interactions due to its increased ability to form [[hydrogen bond]]s stemming from the extra [[hydroxyl]] group in the [[ribose]] sugar.

In a non-biological context, secondary structure is a vital consideration in the [[nucleic acid design]] of nucleic acid structures for [[DNA nanotechnology]] and [[DNA computing]], since the pattern of basepairing ultimately determines the overall structure of the molecules.

==Fundamental concepts==

===Base pairing===

{{multiple image
|image2=GC_base_pair_jypx3.png
|image1=AT_base_pair_jypx3.png
|width=200
|direction=vertical
|footer=''Top'', an AT base pair demonstrating two intermolecular hydrogen bonds; ''bottom'', a GC base pair demonstrating three intermolecular [[hydrogen bond]]s.
}}

{{Main article|Base pair}}

In [[molecular biology]], two [[nucleotide]]s on opposite [[complementarity (molecular biology)|complementary]] [[DNA]] or [[RNA]] strands that are connected via [[hydrogen bond]]s are called a base pair (often abbreviated bp). In the canonical Watson-Crick base pairing, [[adenine]] (A) forms a base pair with [[thymine]] (T) and [[guanine]] (G) forms one with [[cytosine]] (C) in DNA. In RNA, [[thymine]] is replaced by [[uracil]] (U). Alternate hydrogen bonding patterns, such as the [[wobble base pair]] and [[Hoogsteen base pair]], also occur&amp;mdash;particularly in RNA&amp;mdash;giving rise to complex and functional [[nucleic acid tertiary structure|tertiary structures]].  Importantly, pairing is the mechanism by which [[codon]]s on [[messenger RNA]] molecules are recognized by [[anticodon]]s on [[transfer RNA]] during protein [[translation (genetics)|translation]]. Some DNA- or RNA-binding enzymes can recognize specific base pairing patterns that identify particular regulatory regions of genes.
[[Hydrogen bond]]ing is the chemical mechanism that underlies the base-pairing rules described above. Appropriate geometrical correspondence of hydrogen bond donors and acceptors allows only the "right" pairs to form stably. DNA with high GC-content is more stable than DNA with low [[GC-content]], but contrary to popular belief, the hydrogen bonds do not stabilize the DNA significantly and stabilization is mainly due to [[stacking (chemistry)|stacking]] interactions.&lt;ref name ="Yakovchuk2006"&gt;{{cite journal | last1 = Yakovchuk | first1 = Peter | last2 = Protozanova | first2 = Ekaterina | last3 = Frank-Kamenetskii | first3 = Maxim D. | year = 2006 | title = Base-stacking and base-pairing contributions into thermal stability of the DNA double helix | journal = Nucleic Acids Research | volume = 34 | issue = 2| pages = 564–574 | doi = 10.1093/nar/gkj454 | pmid = 16449200 | pmc=1360284}}&lt;/ref&gt;

The larger [[nucleobase]]s, adenine and guanine, are members of a class of doubly ringed chemical structures called [[purine]]s; the smaller nucleobases, cytosine and thymine (and uracil), are members of a class of singly ringed chemical structures called [[pyrimidine]]s. Purines are only complementary with pyrimidines: pyrimidine-pyrimidine pairings are energetically unfavorable because the molecules are too far apart for hydrogen bonding to be established; purine-purine pairings are energetically unfavorable because the molecules are too close, leading to overlap repulsion. The only other possible pairings are GT and AC; these pairings are mismatches because the pattern of hydrogen donors and acceptors do not correspond. The GU [[wobble base pair]], with two hydrogen bonds, does occur fairly often in [[RNA]].

===Nucleic acid hybridization===
{{Main article|Nucleic acid thermodynamics}}

Hybridization is the process of [[Complementarity (molecular biology)|complementary]] [[base pair]]s binding to form a [[double helix]].  Melting is the process by which the interactions between the strands of the double helix are broken, separating the two nucleic acid strands. These bonds are weak, easily separated by gentle heating, [[enzyme]]s, or physical force. Melting occurs preferentially at certain points in the nucleic acid.&lt;ref name="Breslauer1986"&gt;{{cite journal |doi=10.1073/pnas.83.11.3746 |vauthors=Breslauer KJ, Frank R, Blöcker H, Marky LA |title=Predicting DNA duplex stability from the base sequence |journal=PNAS |volume=83 |issue=11 |pages=3746–3750 |year=1986 |pmid=3459152 |pmc=323600|bibcode=1986PNAS...83.3746B }}&lt;/ref&gt; '''T''' and '''A''' rich sequences are more easily melted than '''C''' and '''G''' rich regions. Particular base steps are also susceptible to DNA melting, particularly '''T A''' and '''T G''' base steps.&lt;ref&gt;{{cite web |url=http://www.owczarzy.net/tm.htm |title=DNA melting temperature - How to calculate it? |accessdate=2008-10-02 |author=Richard Owczarzy |date=2008-08-28 |work=High-throughput DNA biophysics |publisher=owczarzy.net}}&lt;/ref&gt; These mechanical features are reflected by the use of sequences such as '''[[TATA box|TATAA]]''' at the start of many genes to assist RNA polymerase in melting the DNA for transcription.

Strand separation by gentle heating, as used in [[PCR]], is simple providing the molecules have fewer than about 10,000 base pairs (10 kilobase pairs, or 10 kbp). The intertwining of the DNA strands makes long segments difficult to separate. The cell avoids this problem by allowing its DNA-melting enzymes ([[helicase]]s) to work concurrently with [[topoisomerase]]s, which can chemically cleave the phosphate backbone of one of the strands so that it can swivel around the other. [[Helicase]]s unwind the strands to facilitate the advance of sequence-reading enzymes such as [[DNA polymerase]].

==Secondary structure motifs==
[[File:A-DNA, B-DNA and Z-DNA.png|thumb|The main nucleic acid [[Nucleic acid double helix#Helix geometries|helix structures]] (A-, B- and Z-form)]]
Nucleic acid secondary structure is generally divided into helices (contiguous base pairs), and various kinds of loops (unpaired nucleotides surrounded by helices).  Frequently these elements, or combinations of them, are further classified into additional categories including, for example, [[tetraloop]]s, [[pseudoknot]]s, and [[stem-loop]]s.

=== Double helix ===

{{Main article|Nucleic acid double helix}}

The double helix is an important [[Nucleic acid tertiary structure|tertiary structure]] in nucleic acid molecules which is intimately connected with the molecule's secondary structure. A double helix is formed by regions of many consecutive base pairs.

The nucleic acid double helix is a spiral polymer, usually right-handed, containing two [[nucleotide]] strands which [[base pairing|base pair]] together.  A single turn of the helix constitutes about ten nucleotides, and contains a major groove and minor groove, the major groove being wider than the minor groove.&lt;ref name="Alberts"&gt;{{cite book|author=Alberts|year=1994|title=The Molecular Biology of the Cell|isbn=978-0-8153-4105-5|publisher=Garland Science|location=New York|display-authors=etal}}&lt;/ref&gt; Given the difference in widths of the major groove and minor groove, many proteins which bind to DNA do so through the wider major groove.&lt;ref&gt;{{cite journal |vauthors=Pabo C, Sauer R |title=Protein-DNA recognition |journal=Annu Rev Biochem |volume=53 |pages=293–321 |year=1984 |pmid=6236744 | doi = 10.1146/annurev.bi.53.070184.001453}}&lt;/ref&gt;   Many double-helical forms are possible; for DNA the three biologically relevant forms are [[A-DNA]], [[B-DNA]], and [[Z-DNA]], while RNA double helices have structures similar to the A form of DNA.

=== Stem-loop structures ===

[[File:Stem-loop.svg|thumb|An RNA [[stem-loop]] secondary structure]]

{{Main article|Stem-loop}}

The secondary structure of nucleic acid molecules can often be uniquely decomposed into stems and loops. The [[stem-loop]] structure (also often referred to as an "hairpin"), in which a base-paired helix ends in a short unpaired loop, is extremely common and is a building block for larger structural motifs such as cloverleaf structures, which are four-helix junctions such as those found in [[transfer RNA]]. Internal loops (a short series of unpaired bases in a longer paired helix) and bulges (regions in which one strand of a helix has "extra" inserted bases with no counterparts in the opposite strand) are also frequent.

There are many secondary structure elements of functional importance to biological RNA's; some famous examples are the [[Intrinsic termination|Rho-independent terminator]] stem-loops and the [[Transfer RNA#Structure|tRNA cloverleaf]]. Active research is on-going to determine the secondary structure of RNA molecules, with approaches including both [[Nucleic acid structure determination|experimental]] and [[Nucleic acid structure prediction|computational]] methods (see also the [[List of RNA structure prediction software]]).

=== Pseudoknots ===

{{Main article|Pseudoknot}}

[[File:Pseudoknot.svg|thumb|300px|An RNA [[pseudoknot]] structure. For example, the RNA component of human [[telomerase]].&lt;ref name="Chen"&gt;{{Cite journal | doi = 10.1073/pnas.0502259102 |vauthors=Chen JL, Greider CW | year = 2005 | title = Functional analysis of the pseudoknot structure in human telomerase RNA | journal = Proc Natl Acad Sci USA | volume = 102 | issue = 23| pages = 8080–5 | pmid=15849264 | pmc=1149427| bibcode=2005PNAS..102.8080C }}&lt;/ref&gt;]]

A pseudoknot is a nucleic acid secondary structure containing at least two [[stem-loop]] structures in which half of one stem is [[intercalation (biochemistry)|intercalated]] between the two halves of another stem.  Pseudoknots fold into knot-shaped three-dimensional conformations but are not true [[knot (mathematics)|topological knots]].  The [[base pair]]ing in pseudoknots is not well nested; that is, base pairs occur that "overlap" one another in sequence position. This makes the presence of general pseudoknots in nucleic acid sequences impossible to [[Nucleic acid structure prediction|predict]] by the standard method of [[dynamic programming]], which uses a recursive scoring system to identify paired stems and consequently cannot detect non-nested base pairs with common algorithms. However, limited subclasses of pseudoknots can be predicted using modified dynamic programs.&lt;ref&gt;{{cite journal | vauthors = Rivas E, Eddy SR | year = 1999 | title = A dynamic programming algorithm for RNA structure prediction including pseudoknots | url = | journal = J Mol Biol | volume = 285 | issue = | pages = 2053–2068 | doi=10.1006/jmbi.1998.2436 | pmid=9925784| arxiv = physics/9807048}}&lt;/ref&gt;
Newer structure prediction techniques such as [[stochastic context-free grammar]]s are also unable to consider pseudoknots.

Pseudoknots can form a variety of structures with catalytic activity&lt;ref&gt;{{Cite journal|last=Staple|first=David W.|last2=Butcher|first2=Samuel E.|date=2005-06-14|title=Pseudoknots: RNA Structures with Diverse Functions|url=http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.0030213|journal=PLOS Biol|volume=3|issue=6|pages=e213|doi=10.1371/journal.pbio.0030213|issn=1545-7885|pmc=1149493|pmid=15941360}}&lt;/ref&gt; and several important biological processes rely on RNA molecules that form pseudoknots. For example, the RNA component of the human [[telomerase]] contains a pseudoknot that is critical for its activity.&lt;ref name="Chen"/&gt; The hepatitis delta virus ribozyme is a well known example of a catalytic RNA with a pseudoknot in its active site.&lt;ref&gt;{{Cite journal|last=Doudna|first=Jennifer A.|last2=Ferré-D'Amaré|first2=Adrian R.|last3=Zhou|first3=Kaihong|title= Crystal structure of a hepatitis delta virus ribozyme|url=http://www.nature.com/doifinder/10.1038/26912|journal=Nature|volume=395|issue=6702|pages=567–574|doi=10.1038/26912|pmid=9783582|date=October 1998|bibcode=1998Natur.395..567F}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Lai|first=Michael M. C.|date=1995-06-01|title=The Molecular Biology of Hepatitis Delta Virus|url=http://www.annualreviews.org/doi/10.1146/annurev.bi.64.070195.001355|journal=Annual Review of Biochemistry|volume=64|issue=1|pages=259–286|doi=10.1146/annurev.bi.64.070195.001355|issn=0066-4154}}&lt;/ref&gt; Though DNA can also form pseudoknots, they are generally not present in standard [[Physiological condition|physiological conditions]].

==Secondary structure prediction==

{{Main article|Nucleic acid structure prediction}}
{{Further|List of RNA structure prediction software}}

Most methods for nucleic acid secondary structure prediction rely on a nearest neighbor thermodynamic model.&lt;ref name="R1" &gt;{{cite journal |vauthors=Xia T, ((SantaLucia J Jr)), Burkard ME, Kierzek R, Schroeder SJ, Jiao X, Cox C, Turner DH |title=Thermodynamic parameters for an expanded nearest-neighbor model for formation of RNA duplexes with Watson-Crick base pairs |journal=Biochemistry |volume=37 |pages=14719–35 |date=October 1998 |pmid=9778347 |doi= 10.1021/bi9809425|url=http://pubs.acs.org/doi/abs/10.1021/bi9809425}}&lt;/ref&gt;&lt;ref name="R2" &gt;{{cite journal |vauthors=Mathews DH, Disney MD, Childs JL, Schroeder SJ, Zuker M, Turner DH |title=Incorporating chemical modification constraints into a dynamic programming algorithm for prediction of RNA secondary structure |journal=PNAS |volume=101 |issue=19 |pages=7287–92 |date=May 2004 |pmid=15123812 |doi= 10.1073/pnas.0401799101|url=http://www.pnas.org/content/101/19/7287.long|accessdate= |pmc=409911|bibcode=2004PNAS..101.7287M}}&lt;/ref&gt; A common method to determine the most probable structures given a sequence of [[Nucleotide|nucleotides]] makes use of a [[dynamic programming]] algorithm that seeks to find structures with low free energy&lt;ref&gt;{{Cite journal|last=Zuker|first=M.|date=1989-04-07|title=On finding all suboptimal foldings of an RNA molecule|url=http://science.sciencemag.org/content/244/4900/48|journal=Science|language=en|volume=244|issue=4900|pages=48–52|doi=10.1126/science.2468181|issn=0036-8075|pmid=2468181|bibcode=1989Sci...244...48Z}}&lt;/ref&gt;. Dynamic programming algorithms often forbid [[pseudoknot]]s, or other cases in which base pairs are not fully nested, as considering these structures becomes computationally very expensive for even small nucleic acid molecules. Other methods, such as [[stochastic context-free grammar]]s can also be used to predict nucleic acid secondary structure.

For many RNA molecules, the secondary structure is highly important to the correct function of the RNA &amp;mdash; often more so than the actual sequence. This fact aids in the analysis of [[non-coding RNA]] sometimes termed "RNA genes". One application of [[bioinformatics]] uses predicted RNA secondary structures in searching a [[genome]] for noncoding but functional forms of RNA. For example, [[miRNA|microRNA]]s have canonical long stem-loop structures interrupted by small internal loops.

RNA secondary structure applies in [[RNA splicing]] in certain species. In humans and other tetrapods, it has been shown that without the [[U2AF2]] protein, the splicing process is inhibited. However, in [[Zebrafish#RNA Splicing|zebrafish]] and other [[teleosts]] the [[RNA splicing]] process can still occur on certain genes in the absence of U2AF2. This may be because 10% of genes in zebrafish have alternating TG and AC base pairs at the 3' splice site (3'ss) and 5' splice site (5'ss) respectively on each intron, which alters the secondary structure of the RNA. This suggests that secondary structure of RNA can influence splicing, potentially without the use of proteins like U2AF2 that have been thought to be required for splicing to occur.&lt;ref name="BrownRNA"&gt;{{cite journal|last1=Lin|first1=Chien-Ling|last2=Taggart|first2=Allison J.|last3=Lim|first3=Kian Huat|last4=Cygan|first4=Kamil J.|last5=Ferraris|first5=Luciana|last6=Creton|first6=Robert|last7=Huang|first7=Yen-Tsung|last8=Fairbrother|first8=William G.|title=RNA structure replaces the need for U2AF2 in splicing|journal=Genome Research|date=13 November 2015|volume=26|issue=1|pages=12–23|pmid= 26566657|doi=10.1101/gr.181008.114|pmc=4691745}}&lt;/ref&gt;

==See also==
{{Portal|Molecular and cellular biology}}
*[[DNA nanotechnology]]
*[[Molecular models of DNA]]
*[[DiProDB]]. The database is designed to collect and analyse thermodynamic, structural and other dinucleotide properties.

==References==&lt;!-- JMolBiol143:49; JMolBiol300:891 --&gt;
{{Reflist|2}}

==External links==
*[https://web.archive.org/web/20070226124940/http://humphry.chem.wesleyan.edu:8080/MDDNA/ MDDNA: Structural Bioinformatics of DNA]
*[http://www.biomolecular-modeling.com/Abalone/index.html Abalone] &amp;mdash; Commercial software for DNA modeling
*[http://mmb.pcb.ub.es/DNAlive DNAlive: a web interface to compute DNA physical properties]. Also allows cross-linking of the results with the UCSC [[Genome browser]] and DNA dynamics.

{{Biomolecular structure}}

{{DEFAULTSORT:Nucleic Acid Secondary Structure}}
[[Category:DNA]]
[[Category:Biophysics]]
[[Category:Molecular geometry|Molecular structure]]
[[Category:RNA]]</text>
      <sha1>6ik134jlqsd5ccatozrfdibcoa4w1oz</sha1>
    </revision>
  </page>
  <page>
    <title>Optional stopping theorem</title>
    <ns>0</ns>
    <id>17593652</id>
    <revision>
      <id>868858908</id>
      <parentid>849634484</parentid>
      <timestamp>2018-11-14T22:22:34Z</timestamp>
      <contributor>
        <username>Texvc2LaTeXBot</username>
        <id>33995001</id>
      </contributor>
      <minor/>
      <comment>Replacing deprecated latex syntax [[mw:Extension:Math/Roadmap]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10139">In [[probability theory]], the '''optional stopping theorem''' (or '''[[Joseph L. Doob|Doob]]'s optional sampling theorem''') says that, under certain conditions, the [[expected value]] of a [[martingale (probability theory)|martingale]] at a [[stopping time]] is equal to its initial expected value. Since martingales can be used to model the wealth of a gambler participating in a fair game, the optional stopping theorem says that, on average, nothing can be gained by stopping play based on the information obtainable so far (i.e., without looking into the future). Certain conditions are necessary for this result to hold true. In particular, the theorem applies to [[Martingale (betting system)|doubling strategies]].

The optional stopping theorem is an important tool of [[mathematical finance]] in the context of the [[fundamental theorem of asset pricing]].

== Statement of theorem ==
A discrete-time version of the theorem is given below:

Let {{math|''X'' {{=}} (''X&lt;sub&gt;t&lt;/sub&gt;'')&lt;sub&gt;''t''∈ℕ&lt;sub&gt;0&lt;/sub&gt;&lt;/sub&gt;}} be a discrete-time [[Martingale (probability theory)|martingale]] and {{math|''τ''}} a [[stopping time]] with values in {{math|ℕ&lt;sub&gt;0&lt;/sub&gt; ∪ {∞}}}, both with respect to a [[Filtration (probability theory)|filtration]] {{math|({{mathcal|F}}&lt;sub&gt;''t''&lt;/sub&gt;)&lt;sub&gt;''t''∈ℕ&lt;sub&gt;0&lt;/sub&gt;&lt;/sub&gt;}}. Assume that one of the following three conditions holds:
:({{EquationRef|a}}) The stopping time {{math|''τ''}} is [[almost surely]] bounded, i.e., there exists a [[mathematical constant|constant]] {{math|''c'' ∈ ℕ}} such that {{math|''τ'' ≤ ''c''}} a.s.
:({{EquationRef|b}}) The stopping time {{math|''τ''}} has finite expectation and the conditional expectations of the absolute value of the martingale increments  are almost surely bounded, more precisely, &lt;math&gt;\mathbb{E}[\tau]&lt;\infty&lt;/math&gt; and there exists a constant {{math|''c''}} such that &lt;math&gt;\mathbb{E}\bigl[|X_{t+1}-X_t|\,\big\vert\,{\mathcal F}_t\bigr]\le c&lt;/math&gt; almost surely on the event {{math|{''τ'' &gt; ''t''}}} for all {{math|''t'' ∈ ℕ&lt;sub&gt;0&lt;/sub&gt;}}.
:({{EquationRef|c}}) There exists a constant {{math|''c''}} such that {{math|{{!}}''X''&lt;sub&gt;''t''∧''τ''&lt;/sub&gt;{{!}} ≤ ''c''}} a.s. for all {{math|''t'' ∈ ℕ&lt;sub&gt;0&lt;/sub&gt;}} where {{math|∧}} denotes the [[join and meet|minimum operator]].
Then {{math|''X&lt;sub&gt;τ&lt;/sub&gt;''}} is an almost surely well defined random variable and &lt;math&gt;\mathbb{E}[X_{\tau}]=\mathbb{E}[X_0].&lt;/math&gt;

Similarly, if the stochastic process {{math|''X''}} is a [[submartingale]] or a [[supermartingale]] and one of the above conditions holds, then
:&lt;math&gt;\mathbb{E}[X_\tau]\ge\mathbb{E}[X_0],&lt;/math&gt;
for a submartingale, and
:&lt;math&gt;\mathbb{E}[X_\tau]\le\mathbb{E}[X_0],&lt;/math&gt;
for a supermartingale.

=== Remark ===
Under condition&amp;nbsp;({{EquationNote|c}}) it is possible that {{math|''τ'' {{=}} ∞}} happens with positive probability. On this event {{math|''X&lt;sub&gt;τ&lt;/sub&gt;''}} is defined as the almost surely existing pointwise limit of {{math|''X''}}, see the proof below for details.

==Applications==
*The optional stopping theorem can be used to prove the impossibility of successful betting strategies for a gambler with a finite lifetime (which gives condition&amp;nbsp;({{EquationNote|a}})) and a house limit on bets (condition&amp;nbsp;({{EquationNote|b}})).  Suppose that the gambler can wager up to ''c'' dollars on a fair coin flip at times 1, 2, 3, etc., winning his wager if the coin comes up heads and losing it if the coin comes up tails.  Suppose further that he can quit whenever he likes, but cannot predict the outcome of gambles that haven't happened yet.  Then the gambler's fortune over time is a martingale, and the time {{math|''τ''}} at which he decides to quit (or goes broke and is forced to quit) is a stopping time.  So the theorem says that {{math|E[''X&lt;sub&gt;τ&lt;/sub&gt;''] {{=}} E[''X''&lt;sub&gt;0&lt;/sub&gt;]}}.  In other words, the gambler leaves with the same amount of money ''on average'' as when he started.  (The same result holds if the gambler, instead of having a house limit on individual bets, has a finite limit on his line of credit or how far in debt he may go, though this is easier to show with another version of the theorem.)&lt;ref&gt;{{cite web |url=https://www.scribd.com/doc/28125042/Unit-Betting-System |title=Archived copy |accessdate=2017-09-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20160305022324/https://www.scribd.com/doc/28125042/Unit-Betting-System |archivedate=2016-03-05 |df= }}&lt;/ref&gt;
*Suppose a [[random walk]] starting at {{math|''a'' ≥ 0}} that goes up or down by one with equal probability on each step.  Suppose further that the walk stops if it reaches {{math|0}} or {{math|''m'' ≥ ''a''}}; the time at which this first occurs is a stopping time.  If it is known that the expected time at which the walk ends is finite (say, from [[Markov chain]] theory), the optional stopping theorem predicts that the expected stop position is equal to the initial position {{math|''a''}}.  Solving {{math|''a'' {{=}} ''pm'' + (1 – ''p'')0}} for the probability {{math|''p''}} that the walk reaches {{math|''m''}} before {{math|0}} gives {{math|''p'' {{=}} ''a''/''m''}}.
*Now consider a random walk {{math|''X''}} that starts at {{math|0}} and stops if it reaches {{math|–''m''}} or {{math|+''m''}}, and use the {{math|''Y&lt;sub&gt;n&lt;/sub&gt;'' {{=}} ''X''&lt;sub&gt;''n''&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; – ''n''}} martingale from the [[Martingale (probability theory)#Examples of martingales|examples section]]. If {{math|''τ''}} is the time at which {{math|''X''}} first reaches {{math|±''m''}}, then {{math|0 {{=}} E[''Y''&lt;sub&gt;0&lt;/sub&gt;] {{=}} E[''Y&lt;sub&gt;τ&lt;/sub&gt;''] {{=}} ''m''&lt;sup&gt;2&lt;/sup&gt; – E[τ]}}. This gives {{math|E[''τ''] {{=}} ''m''&lt;sup&gt;2&lt;/sup&gt;}}.
*Care must be taken, however, to ensure that one of the conditions of the theorem hold. For example, suppose the last example had instead used a 'one-sided' stopping time, so that stopping only occurred at {{math|+''m''}}, not at {{math|−''m''}}. The value of {{math|''X''}} at this stopping time would therefore be {{math|''m''}}. Therefore, the expectation value {{math|E[''X&lt;sub&gt;τ&lt;/sub&gt;'']}} must also be {{math|''m''}}, seemingly in violation of the theorem which would give {{math|E[''X&lt;sub&gt;τ&lt;/sub&gt;''] {{=}} 0}}. The failure of the optional stopping theorem shows that all three of the conditions fail.

==Proof==
Let {{math|''X&amp;thinsp;&lt;sup&gt;τ&lt;/sup&gt;''}} denote the [[stopped process]], it is also a martingale (or a submartingale or supermartingale, respectively). Under condition&amp;nbsp;({{EquationNote|a}}) or&amp;nbsp;({{EquationNote|b}}), the random variable {{math|''X&lt;sub&gt;τ&lt;/sub&gt;''}} is well defined. Under condition&amp;nbsp;({{EquationNote|c}}) the stopped process {{math|''X&amp;thinsp;&lt;sup&gt;τ&lt;/sup&gt;''}} is bounded, hence by Doob's [[martingale convergence theorem]] it converges a.s. pointwise to a random variable which we call {{math|''X&lt;sub&gt;τ&lt;/sub&gt;''}}.

If condition&amp;nbsp;({{EquationNote|c}}) holds, then the stopped process {{math|''X&amp;thinsp;&lt;sup&gt;τ&lt;/sup&gt;''}} is bounded by the constant random variable {{math|''M'' :{{=}} ''c''}}. Otherwise, writing the stopped process as

:&lt;math&gt;X_t^\tau=X_0+\sum_{s=0}^{\tau \land t-1}(X_{s+1}-X_s),\quad t\in{\mathbb N}_0,&lt;/math&gt;

gives {{math|{{!}}''X''&lt;sub&gt;''t''&lt;/sub&gt;&lt;sup&gt;''τ''&lt;/sup&gt;{{!}} ≤ ''M''}} for all {{math|''t'' ∈ ℕ&lt;sub&gt;0&lt;/sub&gt;}}, where

:&lt;math&gt;M:=|X_0|+\sum_{s=0}^{\tau-1}|X_{s+1}-X_s|=|X_0|+\sum_{s=0}^\infty|X_{s+1}-X_s|\cdot\mathbf{1}_{\{\tau&gt;s\}}&lt;/math&gt;.

By the [[monotone convergence theorem]]

:&lt;math&gt;\mathbb{E}[M]=\mathbb{E}[|X_0|]+\sum_{s=0}^\infty \mathbb{E}\bigl[|X_{s+1}-X_s|\cdot\mathbf{1}_{\{\tau&gt;s\}}\bigr]&lt;/math&gt;.

If condition&amp;nbsp;({{EquationNote|a}}) holds, then this series only has a finite number of non-zero terms, hence {{math|''M''}} is integrable.

If condition&amp;nbsp;({{EquationNote|b}}) holds, then we continue by inserting a [[conditional expectation]] and using that the event {{math|{''τ'' &gt; ''s''}}} is known at time {{math|''s''}} (note that {{math|''τ''}} is assumed to be a stopping time with respect to the filtration), hence

:&lt;math&gt;\begin{align}\mathbb{E}[M]
&amp;=\mathbb{E}[|X_0|]+\sum_{s=0}^\infty \mathbb{E}\bigl[\underbrace{\mathbb{E}\bigl[|X_{s+1}-X_s|\big|{\mathcal F}_s\bigr]\cdot\mathbf{1}_{\{\tau&gt;s\}}}_{\le\,c\,\mathbf{1}_{\{\tau&gt;s\}}\text{ a.s. by (b)}}\bigr]\\
&amp;\le\mathbb{E}[|X_0|]+c\sum_{s=0}^\infty\mathbb{P}(\tau&gt;s)\\
&amp;=\mathbb{E}[|X_0|]+c\,\mathbb{E}[\tau]&lt;\infty,\\
\end{align}&lt;/math&gt;

where a [[Expected value#Discrete distribution taking only non-negative integer values|representation of the expected value of non-negative integer-valued random variables]] is used for the last equality.

Therefore, under any one of the three conditions in the theorem, the stopped process is dominated by an integrable random variable {{math|''M''}}. Since the stopped process {{math|''X&amp;thinsp;&lt;sup&gt;τ&lt;/sup&gt;''}} converges almost surely to {{math|''X&lt;sub&gt;τ&lt;/sub&gt;''}}&amp;thinsp;, the [[dominated convergence theorem]] implies

:&lt;math&gt;\mathbb{E}[X_\tau]=\lim_{t\to\infty}\mathbb{E}[X_t^\tau].&lt;/math&gt;

By the martingale property of the stopped process,

:&lt;math&gt;\mathbb{E}[X_t^\tau]=\mathbb{E}[X_0],\quad t\in{\mathbb N}_0,&lt;/math&gt;

hence

:&lt;math&gt;\mathbb{E}[X_\tau]=\mathbb{E}[X_0].&lt;/math&gt;

Similarly, if {{math|''X''}} is a submartingale or supermartingale, respectively, change the equality in the last two formulas to the appropriate inequality.

==References==
{{refimprove|date=February 2012}}
{{Reflist}}
# {{cite book | last1=Grimmett | first1=Geoffrey R. | last2=Stirzaker | first2=David R. | title=Probability and Random Processes | publisher=Oxford University Press | edition=3rd | year=2001 | pages=491–495 | isbn=9780198572220}}
# {{cite book | last1=Bhattacharya | first1=Rabi | last2=Waymire | first2=Edward C. | title=A Basic Course in Probability Theory | publisher=Springer | year=2007 | pages=43–45 | isbn=978-0-387-71939-9}}

==External links==
* [http://math.mit.edu/~sheffield/martingalenote.pdf Doob's Optional Stopping Theorem]

[[Category:Probability theorems]]
[[Category:Statistical theorems]]
[[Category:Articles containing proofs]]
[[Category:Martingale theory]]</text>
      <sha1>kpx8v2iqgp8c8p8p7q760ykiilkks7r</sha1>
    </revision>
  </page>
  <page>
    <title>Otto Schilling</title>
    <ns>0</ns>
    <id>31260972</id>
    <revision>
      <id>713261622</id>
      <parentid>683454211</parentid>
      <timestamp>2016-04-02T23:43:51Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <minor/>
      <comment>clean up; http-&gt;https (see [[WP:VPR/Archive 127#RfC: Should we convert existing Google and Internet Archive links to HTTPS?|this RfC]]) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3728">'''Otto Franz Georg Schilling''' (3 November 1911 – 20 June 1973) was a German-American mathematician known as one of the leading [[algebra]]ists of his time.&lt;ref&gt;{{citation|title=Modern Algebra and the Rise of Mathematical Structures|first=Leo|last=Corry|publisher=Springer|year=2004|isbn=9783764370022|page=222|url=https://books.google.com/books?id=WdGbeyehoCoC&amp;pg=PA222}}.&lt;/ref&gt;

He was born in [[Apolda]] and studied in the 1930s with the [[Universität Jena]] and the [[Universität Göttingen]] under [[Emmy Noether]]. After Noether was forced to leave Germany by the Nazis, he found a new advisor in [[Helmut Hasse]],&lt;ref name="a3"&gt;{{citation|title=A3 &amp; His Algebra: How a Boy from Chicago's West Side Became a Force in American Mathematics|first=Nancy|last=Albert-Goldberg|publisher=iUniverse|year=2005|isbn=9780595328178|page=122|url=https://books.google.com/books?id=Id-v3q__QfoC&amp;pg=PA122}}.&lt;/ref&gt; and obtained his Ph.D. from [[Marburg University]] in 1934 on the thesis ''Über gewisse Beziehungen zwischen der Arithmetik hyperkomplexer Zahlsysteme und algebraischer Zahlkörper''.&lt;ref name="mg"/&gt; He then was [[post doc]] at [[Trinity College, Cambridge]] before moving to [[Institute for Advanced Study]] 1935–37 and the [[Johns Hopkins University]] 1937–39. He became an instructor with the [[University of Chicago]] in 1939,&lt;ref name="a3"/&gt; promoted to assistant professor 1943, associate 1945 and full professor in 1958. In 1961 he moved to [[Purdue University]]. He died in [[Highland Park, Illinois]]. His students were, among others, the game theorist [[Anatol Rapoport]] and the mathematician [[Harley Flanders]].&lt;ref name="mg"&gt;{{Mathgenealogy|id=6367}}&lt;/ref&gt;

==Articles==
*{{cite journal|title=Regular normal extensions over complete fields|journal=Trans. Amer. Math. Soc.|year=1940|volume=47|issue=3|pages=440–454|mr=0001970|doi=10.1090/s0002-9947-1940-0001970-2}}
*with [[Saunders Mac Lane]]: {{cite journal|title=A formula for the direct products of crossed product algebras|journal=Bull. Amer. Math. Soc.|year=1942|volume=48|issue=2|pages=108–114|mr=0006152|doi=10.1090/s0002-9904-1942-07613-0}}
*{{cite journal|title=On a special class of abelian functions|journal=Bull. Amer. Math. Soc.|year=1945|volume=51|issue=2|pages=133–136|mr=0011291|doi=10.1090/s0002-9904-1945-08293-7}}
*{{cite journal|title=Noncommutative valuations|journal=Bull. Amer. Math. Soc.|year=1945|volume=51|issue=4|pages=297–304|mr=0011684|doi=10.1090/s0002-9904-1945-08339-6}}
*{{cite journal|title=Ideal theory on open Riemann surfaces|journal=Bull. Amer. Math. Soc.|year=1946|volume=52|issue=11, Part 1|pages=945–963|mr=0019733|doi=10.1090/s0002-9904-1946-08669-3}}

==Books==
*{{cite book|title=Theory of Valuations|first=Otto|last=Schilling|year=1950|publisher=American Mathematical Society|isbn=0821815040}}&lt;ref&gt;{{cite journal|author=Hochschild, G.|authorlink=Gerhard Hochschild|title=Review: ''The theory of valuations'', by O. F. G. Schilling|journal=Bull. Amer. Math. Soc.|year=1951|volume=57|issue=1, Part 1|pages=91–94|url=http://www.ams.org/journals/bull/1951-57-01/S0002-9904-1951-09464-1/|doi=10.1090/s0002-9904-1951-09464-1}}&lt;/ref&gt;
*{{cite book|title=Basic abstract algebra|first=Otto|last=Schilling|first2=William Stephen|last2=Piper|year=1975|publisher=[[Allyn &amp; Bacon]]|isbn=0205042732|postscript= (394 pages)}}

==References==
{{reflist}}

{{Use dmy dates|date=March 2011}}
{{Authority control}}

{{DEFAULTSORT:Schilling, Otto}}
[[Category:German mathematicians]]
[[Category:Algebraists]]
[[Category:People from Apolda]]
[[Category:Purdue University faculty]]
[[Category:University of Chicago faculty]]
[[Category:1911 births]]
[[Category:1973 deaths]]
[[Category:20th-century mathematicians]]</text>
      <sha1>exawmzwive1qclriv8lq26z9ooyeu8x</sha1>
    </revision>
  </page>
  <page>
    <title>Parabolic coordinates</title>
    <ns>0</ns>
    <id>428111</id>
    <revision>
      <id>840136674</id>
      <parentid>790740430</parentid>
      <timestamp>2018-05-07T23:46:09Z</timestamp>
      <contributor>
        <ip>155.41.40.96</ip>
      </contributor>
      <comment>/* Three-dimensional scale factors */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7281">[[Image:Parabolic coords.svg|thumb|right|384px]]
'''Parabolic coordinates''' are a two-dimensional [[orthogonal coordinates|orthogonal]] [[coordinate system]] in which the [[Coordinate system#Coordinate line|coordinate lines]] are [[confocal]] [[parabola]]s. [[Parabolic cylindrical coordinates|A three-dimensional version]] of parabolic coordinates is obtained by rotating the two-dimensional [[coordinate system|system]] about the symmetry axis of the parabolas.  

Parabolic coordinates have found many applications, e.g., the treatment of the [[Stark effect]] and the [[potential theory]] of the edges.

== Two-dimensional parabolic coordinates ==

Two-dimensional parabolic coordinates &lt;math&gt;(\sigma, \tau)&lt;/math&gt; are defined by the equations, in terms of cartesian coordinates:

:&lt;math&gt;
x = \sigma \tau
&lt;/math&gt;

:&lt;math&gt;
y = \frac{1}{2} \left( \tau^{2} - \sigma^{2} \right)
&lt;/math&gt;

The curves of constant &lt;math&gt;\sigma&lt;/math&gt; form confocal parabolae

:&lt;math&gt;
2y = \frac{x^{2}}{\sigma^{2}} - \sigma^{2}
&lt;/math&gt;

that open upwards (i.e., towards &lt;math&gt;+y&lt;/math&gt;), whereas the curves of constant &lt;math&gt;\tau&lt;/math&gt; form confocal parabolae

:&lt;math&gt;
2y = -\frac{x^{2}}{\tau^{2}} + \tau^{2}
&lt;/math&gt;

that open downwards (i.e., towards &lt;math&gt;-y&lt;/math&gt;).  The foci of all these parabolae are located at the origin.

==Two-dimensional scale factors==

The scale factors for the parabolic coordinates &lt;math&gt;(\sigma, \tau)&lt;/math&gt; are equal

:&lt;math&gt;
h_{\sigma} = h_{\tau} = \sqrt{\sigma^{2} + \tau^{2}}
&lt;/math&gt;

Hence, the infinitesimal element of area is

:&lt;math&gt;
dA = \left( \sigma^{2} + \tau^{2} \right) d\sigma d\tau
&lt;/math&gt;

and the [[Laplacian]] equals

:&lt;math&gt;
\nabla^{2} \Phi = \frac{1}{\sigma^{2} + \tau^{2}} 
\left(  \frac{\partial^{2} \Phi}{\partial \sigma^{2}} + 
\frac{\partial^{2} \Phi}{\partial \tau^{2}} \right)
&lt;/math&gt;

Other differential operators such as &lt;math&gt;\nabla \cdot \mathbf{F}&lt;/math&gt; 
and &lt;math&gt;\nabla \times \mathbf{F}&lt;/math&gt; can be expressed in the coordinates &lt;math&gt;(\sigma, \tau)&lt;/math&gt; by substituting 
the scale factors into the general formulae 
found in [[orthogonal coordinates]].

==Three-dimensional parabolic coordinates==

[[Image:Parabolic coordinates 3D.png|thumb|right|300px|[[Coordinate system#Coordinate surface|Coordinate surfaces]] of the three-dimensional parabolic coordinates.  The red paraboloid corresponds to τ=2, the blue paraboloid corresponds to σ=1, and the yellow half-plane corresponds to φ=-60°.  The three surfaces intersect at the point '''P''' (shown as a black sphere) with [[Cartesian coordinate system|Cartesian coordinates]] roughly (1.0, -1.732, 1.5).]]

The two-dimensional parabolic coordinates form the basis for two sets of three-dimensional [[orthogonal coordinates]]. The [[parabolic cylindrical coordinates]] are produced by projecting in the &lt;math&gt;z&lt;/math&gt;-direction.
Rotation about the symmetry axis of the parabolae produces a set of 
confocal paraboloids, the coordinate system of tridimensional parabolic coordinates. Expressed in terms of cartesian coordinates:

:&lt;math&gt;
x = \sigma \tau \cos \varphi
&lt;/math&gt;

:&lt;math&gt;
y = \sigma \tau \sin \varphi
&lt;/math&gt;

:&lt;math&gt;
z = \frac{1}{2} \left(\tau^{2} - \sigma^{2} \right)
&lt;/math&gt;

where the parabolae are now aligned with the &lt;math&gt;z&lt;/math&gt;-axis,
about which the rotation was carried out.  Hence, the azimuthal angle &lt;math&gt;\phi&lt;/math&gt; is defined

:&lt;math&gt;
\tan \varphi = \frac{y}{x}
&lt;/math&gt;

The surfaces of constant &lt;math&gt;\sigma&lt;/math&gt; form confocal paraboloids

:&lt;math&gt;
2z = \frac{x^{2} + y^{2}}{\sigma^{2}} - \sigma^{2}
&lt;/math&gt;

that open upwards (i.e., towards &lt;math&gt;+z&lt;/math&gt;) whereas the surfaces of constant &lt;math&gt;\tau&lt;/math&gt; form confocal paraboloids 

:&lt;math&gt;
2z = -\frac{x^{2} + y^{2}}{\tau^{2}} + \tau^{2}
&lt;/math&gt;

that open downwards (i.e., towards &lt;math&gt;-z&lt;/math&gt;).  The foci of all these paraboloids are located at the origin.

The [[Riemannian manifold|Riemannian]] [[metric tensor]] associated with this coordinate system is

:&lt;math&gt; g_{ij} = \begin{bmatrix} \sigma^2+\tau^2 &amp; 0 &amp; 0\\0 &amp; \sigma^2+\tau^2 &amp; 0\\0 &amp; 0  &amp; \sigma^2\tau^2 \end{bmatrix} &lt;/math&gt;

==Three-dimensional scale factors==

The three dimensional scale factors are:

:&lt;math&gt;h_{\sigma} = \sqrt{\sigma^2+\tau^2}&lt;/math&gt;
:&lt;math&gt;h_{\tau}   = \sqrt{\sigma^2+\tau^2}&lt;/math&gt;
:&lt;math&gt;h_{\varphi} = \sigma\tau&lt;/math&gt;

It is seen that the scale factors &lt;math&gt;h_{\sigma}&lt;/math&gt; and &lt;math&gt;h_{\tau}&lt;/math&gt; are the same as in the two-dimensional case. The infinitesimal volume element is then

:&lt;math&gt;
dV = h_\sigma h_\tau h_\varphi\, d\sigma\,d\tau\,d\varphi = \sigma\tau \left( \sigma^{2} + \tau^{2} \right)\,d\sigma\,d\tau\,d\varphi
&lt;/math&gt;

and the Laplacian is given by

:&lt;math&gt;
\nabla^2 \Phi = \frac{1}{\sigma^{2} + \tau^{2}} 
\left[
\frac{1}{\sigma} \frac{\partial}{\partial \sigma} 
\left( \sigma \frac{\partial \Phi}{\partial \sigma} \right) +
\frac{1}{\tau} \frac{\partial}{\partial \tau} 
\left( \tau \frac{\partial \Phi}{\partial \tau} \right)\right] +
\frac{1}{\sigma^2\tau^2}\frac{\partial^2 \Phi}{\partial \varphi^2}
&lt;/math&gt;

Other differential operators such as &lt;math&gt;\nabla \cdot \mathbf{F}&lt;/math&gt; 
and &lt;math&gt;\nabla \times \mathbf{F}&lt;/math&gt; can be expressed in the coordinates &lt;math&gt;(\sigma, \tau, \phi)&lt;/math&gt; by substituting 
the scale factors into the general formulae 
found in [[orthogonal coordinates]].

== See also ==

* [[Parabolic cylindrical coordinates]]
* [[Orthogonal coordinate system]]
* [[Curvilinear coordinates]]

==Bibliography==
*{{cite book | author = [[Philip M. Morse|Morse PM]], [[Herman Feshbach|Feshbach H]] | year = 1953 | title = Methods of Theoretical Physics, Part I | publisher = McGraw-Hill | location = New York | isbn = 0-07-043316-X|lccn=52011515 | pages = 660}}
*{{cite book | author = [[Henry Margenau|Margenau H]], Murphy GM | year = 1956 | title = The Mathematics of Physics and Chemistry | publisher = D. van Nostrand | location = New York | pages = 185&amp;ndash;186 | lccn = 55010911 }}
*{{cite book | author = Korn GA, Korn TM |year = 1961 | title = Mathematical Handbook for Scientists and Engineers | publisher = McGraw-Hill | location = New York | id = ASIN B0000CKZX7 | pages = 180 | lccn = 59014456}}
*{{cite book | author = Sauer R, Szabó I | year = 1967 | title = Mathematische Hilfsmittel des Ingenieurs | publisher = Springer Verlag | location = New York | pages = 96 | lccn = 67025285}}  
*{{cite book | author = Zwillinger D | year = 1992 | title = Handbook of Integration | publisher = Jones and Bartlett | location = Boston, MA | isbn = 0-86720-293-9 | pages = 114}}  Same as Morse &amp; Feshbach (1953), substituting ''u''&lt;sub&gt;''k''&lt;/sub&gt; for ξ&lt;sub&gt;''k''&lt;/sub&gt;.
*{{cite book | author = Moon P, Spencer DE | year = 1988 | chapter = Parabolic Coordinates (μ, ν, ψ) | title = Field Theory Handbook, Including Coordinate Systems, Differential Equations, and Their Solutions | edition = corrected 2nd ed., 3rd print | publisher = Springer-Verlag | location = New York | pages = 34&amp;ndash;36 (Table 1.08) | isbn = 978-0-387-18430-2}}

==External links==
* {{springer|title=Parabolic coordinates|id=p/p071170}}
*[http://mathworld.wolfram.com/ParabolicCoordinates.html MathWorld description of parabolic coordinates]

{{Orthogonal coordinate systems}}

[[Category:Coordinate systems]]

&lt;!--[[it:Coordinate paraboliche]] deleted --&gt;</text>
      <sha1>qoz63ebrqw885hd7owlob2vw8ok142d</sha1>
    </revision>
  </page>
  <page>
    <title>Polish notation</title>
    <ns>0</ns>
    <id>25056</id>
    <revision>
      <id>865892411</id>
      <parentid>862108121</parentid>
      <timestamp>2018-10-26T21:19:52Z</timestamp>
      <contributor>
        <username>Zjk7</username>
        <id>19916134</id>
      </contributor>
      <comment>clarifying in which sense the notations of Bocheński and Łukasiewicz are compatible and in which sense they are incompatible</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="16720">{{About|a prefix notation in mathematics and computer sciences|the similarly named logic|Łukasiewicz logic}}
{{Operator notation sidebar |logo=[[File:Prefix-dia.svg|125px]]}}

'''Polish notation''' ('''PN'''), also known as '''normal Polish notation''' ('''NPN'''),&lt;ref name="Jorke_1989"&gt;{{cite book |author-first1=Günter |author-last1=Jorke |author-first2=Bernhard |author-last2=Lampe |author-first3=Norbert |author-last3=Wengel |title=Arithmetische Algorithmen der Mikrorechentechnik |edition=1 |trans-title=Arithmetic algorithms in microcomputers |publisher=[[:de:VEB Verlag Technik|VEB Verlag Technik]] |location=Berlin, Germany |date=1989 |language=German |isbn=3341005153 |id={{EAN|9783341005156}}. MPN 5539165. License 201.370/4/89 |url=https://books.google.com/books/about/Arithmetische_Algorithmen_der_Mikroreche.html?id=DqYWAQAAMAAJ |access-date=2015-12-01}}&lt;/ref&gt; '''Łukasiewicz notation''', '''Warsaw notation''', '''Polish prefix notation''' or simply '''prefix notation''', is a mathematical notation in which [[Operation (mathematics)|operators]] ''precede'' their [[operand]]s, in contrast to (the more common) [[infix notation]] (in which operators are placed ''between'' operands), as well as to [[reverse Polish notation]] (RPN, in which operators ''follow'' their operands). It does not need any parentheses as long as each operator has a fixed [[arity|number of operands]]. The description "Polish" refers to the [[nationality]] of [[logician]] [[Jan Łukasiewicz]],&lt;ref name="Łukasiewicz_1957"&gt;{{cite book |author-last=Łukasiewicz |author-first=Jan |author-link=Jan Łukasiewicz |title=Aristotle's Syllogistic from the Standpoint of Modern Formal Logic |publisher=[[Oxford University Press]] |date=1957}} (Reprinted by Garland Publishing in 1987. {{isbn|0-8240-6924-2}})&lt;/ref&gt; who invented Polish notation in 1924.&lt;ref&gt;{{cite journal |author-first=Charles Leonard |author-last=Hamblin |author-link=Charles Leonard Hamblin |date=1962 |title=Translation to and from Polish notation |journal=[[Computer Journal]] |volume=5 |issue=3 |pages=210–213 |url=http://comjnl.oxfordjournals.org/content/5/3/210.full.pdf |doi=10.1093/comjnl/5.3.210}}&lt;/ref&gt;&lt;ref&gt;{{cite book |title=Algorithms for RPN calculators |author-first=John A. |author-last=Ball |date=1978 |edition=1 |publisher=[[Wiley-Interscience]], [[John Wiley &amp; Sons, Inc.]] |location=Cambridge, Massachusetts, USA |isbn=0-471-03070-8}}&lt;/ref&gt;

The term ''Polish notation'' is sometimes taken (as the opposite of ''infix notation'') to also include reverse Polish notation.&lt;ref&gt;{{cite book |title=Data structures and other objects using Java |edition=3rd |author-first=Michael |author-last=Main |publisher=[[Pearson PLC]] [[Addison-Wesley]] |date=2006 |isbn=978-0-321-37525-4 |page=334 |url=https://books.google.com/books?id=Tok_AQAAIAAJ}}&lt;/ref&gt;

When Polish notation is used as a syntax for mathematical expressions by [[programming language]] [[Interpreter (computing)|interpreter]]s, it is readily parsed into [[abstract syntax tree]]s and can, in fact, define a [[Bijection|one-to-one representation]] for the same. Because of this, [[Lisp (programming language)|Lisp]] ([[#Implementations|see below]]) and related programming languages define their entire syntax in terms of prefix notation (and others use postfix notation).

A quotation from a paper by [[Jan Łukasiewicz]], ''Remarks on Nicod's Axiom and on "Generalizing Deduction"'', page 180, states how the notation was invented:
&lt;blockquote&gt;I came upon the idea of a parenthesis-free notation in 1924. I used that notation for the first time in my article Łukasiewicz(1), p. 610, footnote.&lt;/blockquote&gt;

The reference cited by Łukasiewicz is apparently a lithographed report in [[Polish language|Polish]]. The referring paper by Łukasiewicz ''Remarks on Nicod's Axiom and on "Generalizing Deduction"'' was reviewed by [[Henry Pogorzelski|Henry A. Pogorzelski]] in the ''Journal of Symbolic Logic'' in 1965.&lt;ref&gt;Pogorzelski, Henry A., [https://www.jstor.org/stable/2269644 "Reviewed work(s): Remarks on Nicod's Axiom and on "Generalizing Deduction" by Jan Łukasiewicz; Jerzy Słupecki; Państwowe Wydawnictwo Naukowe"], ''The Journal of Symbolic Logic'', Vol. 30, No. 3 (September 1965), pp. 376–377. The original paper by Łukasiewicz was published in [[Warsaw]] in 1961 in a volume edited by Jerzy Słupecki.&lt;/ref&gt; [[Heinrich Behmann]], editor in 1924 of the article of [[Moses Schönfinkel]]&lt;ref&gt;"Über die Bausteine der mathematischen Logik", ''Mathematische Annalen'' '''92''', pages 305-316. Translated by Stefan Bauer-Mengelberg as "On the building blocks of mathematical logic" in [[Jean van Heijenoort]], 1967. ''A Source Book in Mathematical Logic, 1879-1931''. [[Harvard University Press]]: 355-66.&lt;/ref&gt; already had the idea of eliminating parentheses in logic formulas.

[[Alonzo Church]] mentions this notation in his classic book on [[mathematical logic]] as worthy of remark in notational systems even contrasted to [[Alfred North Whitehead|Alfred Whitehead]] and [[Bertrand Russell]]'s logical notational exposition and work in [[Principia Mathematica]].&lt;ref&gt;{{cite book |author-first=Alonzo |author-last=Church |title=Introduction to Mathematical Logic |location=Princeton, New Jersey, USA |publisher=[[Princeton University Press]] |date=1944 |page=38 |quote=[…] Worthy of remark is the parenthesis-free notation of Jan Łukasiewicz. In this the letters N, A, C, E, K are used in the roles of negation, disjunction, implication, equivalence, conjunction respectively. […]}}&lt;/ref&gt;

In Łukasiewicz's 1951 book, ''Aristotle's Syllogistic from the Standpoint of Modern Formal Logic'', he mentions that the principle of his notation was to write the [[Function symbol|functor]]s before the [[Argument of a function|argument]]s to avoid brackets and that he had employed his notation in his logical papers since 1929.&lt;ref&gt;Łukasiewicz, (1951) ''Aristotle's Syllogistic from the Standpoint of Modern Formal Logic'', Chapter IV "Aristotle's System in Symbolic Form" (section on "Explanation of the Symbolism"), p. 78 and on.&lt;/ref&gt; He then goes on to cite, as an example, a 1930 paper he wrote with [[Alfred Tarski]] on the [[Propositional calculus|sentential calculus]].&lt;ref&gt;Łukasiewicz, Jan; Tarski, Alfred, "Untersuchungen über den Aussagenkalkül" ["Investigations into the sentential calculus"], ''Comptes Rendus des Séances de la Société des Sciences et des Lettres de Varsovie'', Vol. 23 (1930) Cl. III, pp. 31–32.&lt;/ref&gt;

While no longer used much in logic,&lt;ref&gt;{{citation |contribution=Mhy bib I fail logic? Dyslexia in the teaching of logic |author-first=Xóchitl |author-last=Martínez Nava |pages=162–169 |title=Tools for Teaching Logic: Third International Congress, TICTTL 2011, Salamanca, Spain, June 1-4, 2011, Proceedings |volume=6680 |series=Lecture Notes in Artificial Intelligence |publisher=[[Springer Nature]] |date=2011-06-01 |isbn=9783642213496 |editor-first1=Patrick |editor-last1=Blackburn |editor-first2=Hans |editor-last2=van Ditmarsch |editor-first3=Maria |editor-last3=Manzano |editor3-link=María Manzano|editor-first4=Fernando |editor-last4=Soler-Toscano |doi=10.1007/978-3-642-21350-2_19 |url=https://books.google.com/books?id=be-pTR5TmZIC&amp;pg=PA166 |quote=[…] Polish or prefix notation has come to disuse given the difficulty that using it implies. […]}}&lt;/ref&gt; Polish notation has since found a place in [[computer science]].

==Explanation==
The expression for adding the numbers 1 and 2 is written in Polish notation as {{nowrap|+ 1 2}} (pre-fix), rather than as {{nowrap|1 + 2}} (in-fix). In more complex expressions, the operators still precede their operands, but the operands may themselves be expressions including again operators and their operands. For instance, the expression that would be written in conventional infix notation as
: {{nowrap|(5 − 6) × 7}}
can be written in Polish notation as
: {{nowrap|× (− 5 6) 7}}
Assuming a given [[arity]] of all involved operators (here the "−" denotes the binary operation of subtraction, not the unary function of sign-change), any well formed prefix representation thereof is unambiguous, and brackets within the prefix expression are unnecessary. As such, the above expression can be further simplified to
: {{nowrap|× − 5 6 7}}

The processing of the product is deferred until its two operands are available (i.e., 5 minus 6, and 7). As with ''any'' notation, the innermost expressions are evaluated first, but in Polish notation this "innermost-ness" can be conveyed by the sequence of operators and operands rather than by bracketing.

In the conventional infix notation parentheses are required to override the standard [[Order of operations|precedence rules]], since, referring to the above example, moving them
: {{nowrap|5 − (6 × 7)}}
or removing them
: {{nowrap|5 − 6 × 7}}
changes the meaning and the result of the expression. This version is written in Polish notation as
: {{nowrap|− 5 × 6 7}}.

When dealing with non-commutative operations, like division or subtraction, it is necessary to coordinate the sequential arrangement of the operands with the definition of how the operator takes its arguments, i.e., from left to right.  For example, {{nowrap|÷ 10 5}}, with 10 left to 5, has the meaning of 10 ÷ 5 (read as "divide 10 by 5"), or {{nowrap|- 7 6}}, with 7 left to 6, has the meaning of 7 - 6 (read as "subtract from 7 the operand 6").

==Evaluation algorithm==
Prefix/postfix notation is especially popular for its innate ability to express the intended order of operations without the need for parentheses and other precedence rules, as are usually employed with [[infix notation]]. Instead, the notation uniquely indicates which operator to evaluate first. The operators are assumed to have a fixed [[arity]] each, and all necessary operands are assumed to be explicitly given. A valid prefix expression always starts with an operator and ends with an operand. Evaluation can either proceed from left to right, or in the opposite direction. Starting at the left, the input string, consisting of tokens denoting operators or operands, is pushed token for token on a [[Stack (abstract data type)|stack]], until the top entries of the stack contain the number of operands that fits to the top most operator (immediately beneath). This group of tokens at the stacktop (the last stacked operator and the according number of operands) is replaced by the result of executing the operator on these/this operand(s). Then the processing of the input continues in this manner. The rightmost operand in a valid prefix expression thus empties the stack, except for the result of evaluating the whole expression. When starting at the right, the pushing of tokens is performed similarly, just the evaluation is triggered by an operator, finding the appropriate number of operands that fits its arity already at the stacktop. Now the leftmost token of a valid prefix expression must be an operator, fitting to the number of operands in the stack, which again yields the result. As can be seen from the description, a [[Deterministic pushdown automaton|push-down store]] with no capability of arbitrary stack inspection suffices to implement this [[parsing]].

The above sketched stack manipulation works –with mirrored input– also for expressions in [[reverse Polish notation]].

==Polish notation for logic==
The table below shows the core of [[Jan Łukasiewicz]]'s notation for [[sentential logic]].&lt;ref&gt;{{citation |title=Routledge Encyclopedia of Philosophy, Volume 8 |author-first=Edward |author-last=Craig |publisher=[[Taylor &amp; Francis]] |date=1998 |isbn=9780415073103 |page=496 |url=https://books.google.com/books?id=mxpFwcAplaAC&amp;pg=PA496}}.&lt;/ref&gt; Some letters in the Polish notation table stand for particular words in [[Polish language|Polish]], as shown:

{| class=wikitable
|-
!Concept!!Conventional&lt;br/ &gt; notation!!Polish&lt;br/ &gt; notation!!Polish&lt;br/ &gt;term
|-
|- style="border-top:3px solid #999;"
|-
|[[Negation]]||&lt;math&gt;\neg\varphi&lt;/math&gt;||&lt;math&gt;\mathrm N\varphi&lt;/math&gt;||''negacja''
|-
|[[Logical conjunction|Conjunction]]||&lt;math&gt;\varphi\land\psi&lt;/math&gt;||&lt;math&gt;\mathrm K\varphi\psi&lt;/math&gt;||''koniunkcja''
|-
|[[Disjunction]]||&lt;math&gt;\varphi\lor\psi&lt;/math&gt;||&lt;math&gt;\mathrm A\varphi\psi&lt;/math&gt;||''alternatywa''
|-
|[[Material conditional]]||&lt;math&gt;\varphi\to\psi&lt;/math&gt;||&lt;math&gt;\mathrm C\varphi\psi&lt;/math&gt;||''implikacja''
|-
|[[Biconditional]]||&lt;math&gt;\varphi\leftrightarrow\psi&lt;/math&gt;||&lt;math&gt;\mathrm E\varphi\psi&lt;/math&gt;||''ekwiwalencja''
|-
|[[Falsum]]||&lt;math&gt;\bot&lt;/math&gt;||&lt;math&gt;\mathrm O&lt;/math&gt;||''fałsz''
|-
|[[Sheffer stroke]]||&lt;math&gt;\varphi\mid\psi &lt;/math&gt;||&lt;math&gt;\mathrm D\varphi\psi&lt;/math&gt;||''dysjunkcja''
|-
|[[Modal logic|Possibility]]||&lt;math&gt;\Diamond\varphi&lt;/math&gt;||&lt;math&gt;\mathrm M\varphi&lt;/math&gt;||''możliwość''
|-
|[[Modal logic|Necessity]]||&lt;math&gt;\Box\varphi&lt;/math&gt;||&lt;math&gt;\mathrm L\varphi&lt;/math&gt;||''konieczność''
|-
|[[Universal quantification|Universal quantifier]]||&lt;math&gt;\forall p\,\varphi&lt;/math&gt;||&lt;math&gt;\Pi p\,\varphi&lt;/math&gt;||''kwantyfikator ogólny''
|-
|[[Existential quantification|Existential quantifier]]||&lt;math&gt;\exists p\,\varphi&lt;/math&gt;||&lt;math&gt;\Sigma p\,\varphi&lt;/math&gt;||''kwantyfikator szczegółowy''
|}

Note that the quantifiers ranged over propositional values in Łukasiewicz's work on many-valued logics.

[[Józef Maria Bocheński|Bocheński]] introduced a system of Polish notation that names all 16 binary [[logical connective|connectives]] of classical propositional logic. For classical propositional logic, it is a compatible extension of the notation of Łukasiewicz. But the notations are incompatible in the sense that Bocheński uses L and M (for nonimplication and converse nonimplication) in propositional logic and Łukasiewicz uses L and M in modal logic.&lt;ref&gt;Bocheński, Józef Maria (1959). A Precis of Mathematical Logic, translated by Otto Bird from the French and German editions, D. Reidel: Dordrecht, Holland.&lt;/ref&gt;

==Implementations==
Prefix notation has seen wide application in [[Lisp (programming language)|Lisp]] [[s-expressions]], where the brackets are required since the operators in the language are themselves data ([[first-class function]]s). Lisp functions may also be [[Variadic function|variadic]]. The [[Tcl]] programming language, much like Lisp also uses Polish notation through the mathop library. The Ambi&lt;ref&gt;https://code.google.com/p/ambi/&lt;/ref&gt; programming language uses Polish notation for arithmetic operations and program construction.

Postfix notation is used in many [[stack-oriented programming language]]s like [[PostScript]] and [[Forth (programming language)|Forth]]. [[CoffeeScript]] syntax also allows functions to be called using prefix notation, while still supporting the unary postfix syntax common in other languages.

The number of return values of an expression equals the difference between the number of operands in an expression and the total arity of the operators minus the total number of return values of the operators.

Polish notation, usually in postfix form, is the chosen notation of certain [[calculator]]s, notably from [[HP calculators|Hewlett-Packard]].&lt;ref&gt;{{cite web |url=http://h20331.www2.hp.com/Hpsub/downloads/35_02_RPN_Mode.pdf |title=HP calculators {{!}} '''HP 35s''' RPN Mode |publisher=[[Hewlett-Packard]]}}&lt;/ref&gt; At a lower level, postfix operators are used by some [[stack machines]] such as the [[Burroughs large systems]].

==See also==
* [[Reverse Polish notation]]
* [[Function application]]
* [[Lambda calculus]]
* [[Currying]]
* [[Lisp (programming language)]]
** [[S-expression]]
* [[Polish School of Mathematics]]
* [[Hungarian notation]]
* [[Verb–subject–object]] (VSO)
* [[Verb–object–subject]] (VOS)

==References==
{{reflist}}

==Further reading==
* {{Cite book |author-last=Łukasiewicz |author-first=Jan |author-link=Jan Łukasiewicz |title=Aristotle's Syllogistic from the Standpoint of Modern Formal Logic |publisher=[[Oxford University Press]] |date=1957}}
* {{cite book |author-last=Łukasiewicz |author-first=Jan |author-link=Jan Łukasiewicz |title=Philosophische Bemerkungen zu mehrwertigen Systemen des Aussagenkalküls |language=German |trans-title=Philosophical Remarks on Many-Valued Systems of Propositional Logics |journal=Comptes Rendus des Séances de la Société des Sciences et des Lettres de Varsovie |volume=23 |pages=51–77 |date=1930}} Translated by H. Weber in Storrs McCall, ''Polish Logic 1920-1939'', [[Clarendon Press]]: Oxford (1967).

{{DEFAULTSORT:Polish Notation}}
[[Category:Mathematical notation]]
[[Category:Polish inventions]]
[[Category:Science and technology in Poland]]
[[Category:Operators (programming)]]
[[Category:Logical expressions]]</text>
      <sha1>mxbq6iodwjo1tmh5ze8wsch9nfr21bb</sha1>
    </revision>
  </page>
  <page>
    <title>Product category</title>
    <ns>0</ns>
    <id>4587226</id>
    <revision>
      <id>810046056</id>
      <parentid>810045994</parentid>
      <timestamp>2017-11-13T01:52:33Z</timestamp>
      <contributor>
        <username>Siddharthist</username>
        <id>28122572</id>
      </contributor>
      <minor/>
      <comment>/* References */ Add {{Reflist}} so notes don't gather on the bottom of the page</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3125">{{Redirect|Product of categories|the operation on objects of a category|Product (category theory)}}

In the mathematical field of [[category theory]], the '''product''' of two categories ''C'' and ''D'', denoted {{nowrap| ''C'' × ''D''}} and called a '''product category''', is an extension of the concept of the [[Cartesian product]] of two sets. Product categories are used to define [[Functor#Bifunctors and multifunctors|bifunctors and multifunctors]].{{sfn|Mac Lane|1978}}

==Definition==
The product category {{nowrap| ''C'' × ''D''}} has:
*as objects:
*:pairs of objects {{nowrap| (''A'', ''B'')}}, where ''A'' is an object of ''C'' and ''B'' of ''D'';
*as arrows from {{nowrap| (''A''&lt;sub&gt;1&lt;/sub&gt;, ''B''&lt;sub&gt;1&lt;/sub&gt;)}} to  {{nowrap| (''A''&lt;sub&gt;2&lt;/sub&gt;, ''B''&lt;sub&gt;2&lt;/sub&gt;)}}:
*:pairs of arrows {{nowrap| (''f'', ''g'')}}, where {{nowrap| ''f'' : ''A''&lt;sub&gt;1&lt;/sub&gt; → ''A''&lt;sub&gt;2&lt;/sub&gt;}} is an arrow of ''C'' and {{nowrap| ''g'' : ''B''&lt;sub&gt;1&lt;/sub&gt; → ''B''&lt;sub&gt;2&lt;/sub&gt;}} is an arrow of ''D'';
*as composition, component-wise composition from the contributing categories:
*:{{nowrap|1= (''f''&lt;sub&gt;2&lt;/sub&gt;, ''g''&lt;sub&gt;2&lt;/sub&gt;) &lt;small&gt;o&lt;/small&gt; (''f''&lt;sub&gt;1&lt;/sub&gt;, ''g''&lt;sub&gt;1&lt;/sub&gt;) = (''f''&lt;sub&gt;2&lt;/sub&gt; &lt;small&gt;o&lt;/small&gt; ''f''&lt;sub&gt;1&lt;/sub&gt;, ''g''&lt;sub&gt;2&lt;/sub&gt; &lt;small&gt;o&lt;/small&gt; ''g''&lt;sub&gt;1&lt;/sub&gt;)}};
*as identities, pairs of identities from the contributing categories:
*:1&lt;sub&gt;(''A'', ''B'')&lt;/sub&gt; = (1&lt;sub&gt;''A''&lt;/sub&gt;, 1&lt;sub&gt;''B''&lt;/sub&gt;).

==Relation to other categorical concepts==
For [[small category|small categories]], this is the same as the action on objects of the [[categorical product]] in the category '''[[Category of small categories|Cat]]'''. A [[functor]] whose domain is a product category is known as a [[bifunctor]]. An important example is the [[Hom functor]], which has the product of the [[Dual (category theory)|opposite]] of some category with the original category as domain:

:Hom : ''C''&lt;sup&gt;op&lt;/sup&gt; × ''C'' → '''Set'''.

==Generalization to several arguments==
Just as the binary Cartesian product is readily generalized to an [[Cartesian product#n-ary product|''n''-ary Cartesian product]], binary product of two categories can be generalized, completely analogously, to a product of ''n'' categories. The product operation on categories is commutative and associative, [[up to isomorphism]], and so this generalization brings nothing new from a theoretical point of view.

==References==
{{Reflist}}
* Definition 1.6.5 in {{Cite book| publisher = Cambridge University Press| isbn = 0-521-44178-1| volume = Volume 1| last = Borceux| first = Francis| title = Handbook of categorical algebra| series = Encyclopedia of mathematics and its applications 50-51, 53 [i.e. 52]| date = 1994 | page=22 }}
* {{nlab|id=product+category|title=Product category}}
* {{Cite book|url=https://www.worldcat.org/oclc/851741862|title=Categories for the Working Mathematician|last=Mac Lane|first=Saunders|date=1978|publisher=Springer New York|year=|isbn=1441931236|edition=Second|location=New York, NY|pages=49–51|oclc=851741862}}

{{Category theory}}

[[Category:Category theory]]

{{Categorytheory-stub}}</text>
      <sha1>tqnwi3ag1a0e4trxo71i7guiyssnjqn</sha1>
    </revision>
  </page>
  <page>
    <title>Proofs involving the addition of natural numbers</title>
    <ns>0</ns>
    <id>2219011</id>
    <revision>
      <id>782052077</id>
      <parentid>781623254</parentid>
      <timestamp>2017-05-24T17:57:17Z</timestamp>
      <contributor>
        <username>Magic links bot</username>
        <id>30707369</id>
      </contributor>
      <minor/>
      <comment>Replace [[Help:Magic links|magic links]] with templates per [[Special:Permalink/772743896#Future of magic links|local RfC]] and [[:mw:Requests for comment/Future of magic links|MediaWiki RfC]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4690">This article contains [[mathematical proof]]s for some properties of [[addition]] of the [[natural numbers]]: the additive identity, commutativity, and associativity. These proofs are used in the article [[Addition of natural numbers]].

==Definitions==
This article will use the [[Peano axioms#Addition|Peano axioms]] for the definitions of addition of the natural numbers, and the [[successor function]] S(a). In particular:

{|
|-
| '''A1:''' || ''a'' + 0 = ''a''
|-
| '''A2:''' || ''a'' + S(''b'') = S(''a'' + ''b'')
|}

For the proof of commutativity, it is useful to define another natural number closely related to the successor function, namely "1". We define 1 to be the successor of 0, in other words,

:1 = S(0).

Note that for all natural numbers ''a'',

{|
|-
| || S(''a'') 
|-
| = || S(''a'' + 0) || [by A1]
|-
| = || ''a'' + S(0) || [by A2]
|-
| = || ''a'' + 1 || [by Def. of 1]
|}

== Proof of associativity ==

We prove [[associativity]] by first fixing natural numbers ''a'' and ''b'' and applying [[mathematical induction|induction]] on the natural number ''c''.

For the base case ''c'' = 0,

: (''a''+''b'')+0 = ''a''+''b'' = ''a''+(''b''+0)

Each equation follows by definition [A1]; the first with ''a'' + ''b'', the second with ''b''.

Now, for the induction. We assume the induction hypothesis, namely we assume that for some natural number ''c'',

: (''a''+''b'')+''c'' = ''a''+(''b''+''c'')

Then it follows,

{|
|-
| || (''a'' + ''b'') + ''S''(''c'') 
|-
|= || ''S''((''a'' + ''b'') + ''c'') || [by A2]
|-
| = || ''S''(''a'' + (''b'' + ''c'')) || [by the induction hypothesis] 
|-
| = || ''a'' + ''S''(''b'' + ''c'') || [by A2]
|-
| = || ''a'' + (''b'' + ''S''(''c'')) || [by A2]
|}

In other words, the induction hypothesis holds for ''S''(''c''). Therefore, the induction on ''c'' is complete.

== Proof of identity element ==

Definition [A1] states directly that 0 is a [[mathematical identity|right identity]].
We prove that 0 is a [[mathematical identity|left identity]] by induction on the natural number ''a''.

For the base case ''a'' = 0, 0 + 0 = 0 by definition [A1].
Now we assume the induction hypothesis, that 0 + ''a'' = ''a''.
Then
{|
|-
| || 0 + ''S''(''a'')
|-
| = || ''S''(0 + ''a'') || [by A2]
|-
| = || ''S''(''a'') || [by the induction hypothesis]
|}
This completes the induction on ''a''.

== Proof of commutativity ==

We prove [[commutativity]] (''a'' + ''b'' = ''b'' + ''a'') by applying induction on the natural number ''b''. First we prove the base cases ''b'' = 0 and ''b'' = ''S''(0) = 1 (i.e. we prove that 0 and 1 commute with everything).

The base case ''b'' = 0 follows immediately from the identity element property (0 is an [[mathematical identity|additive identity]]), which has been proved above:
''a'' + 0 = ''a'' = 0 + ''a''.

Next we will prove the base case ''b'' = 1, that 1 commutes with everything, i.e. for all natural numbers ''a'', we have ''a'' + 1 = 1 + ''a''. We will prove this by induction on ''a'' (an induction proof within an induction proof). We have proved that 0 commutes with everything, so in particular, 0 commutes with 1: for ''a'' = 0, we have 0 + 1 = 1 + 0. Now, suppose ''a'' + 1 = 1 + ''a''. Then

{|
|-
| || ''S''(''a'') + 1
|-
| = || ''S''(''a'') + ''S''(0) || [by Def. of 1]
|-
| = || ''S''(''S''(''a'') + 0) || [by A2]
|-
| = || ''S''((''a'' + 1) + 0) || [as shown [[#Definitions|above]]]
|-
| = || ''S''(''a'' + 1) || [by A1]
|-
| = || ''S''(1 + ''a'') || [by the induction hypothesis]
|-
| = || 1 + ''S''(''a'') || [by A2]
|}

This completes the induction on ''a'', and so we have proved the base case ''b'' = 1. Now, suppose that for all natural numbers ''a'', we have ''a'' + ''b'' = ''b'' + ''a''. We must show that for all natural numbers ''a'', we have ''a'' + ''S''(''b'') = ''S''(''b'') + ''a''. We have

{|
|-
| || ''a'' + ''S''(''b'')
|-
| = || ''a'' + (''b'' + 1) || [as shown [[#Definitions|above]]]
|-
| = || (''a'' + ''b'') + 1 || [by associativity]
|-
| = || (''b'' + ''a'') + 1 || [by the induction hypothesis]
|-
| = || ''b'' + (''a'' + 1) || [by associativity]
|-
| = || ''b'' + (1 + ''a'') || [by the base case ''b'' = 1]
|-
| = || (''b'' + 1) + ''a'' || [by associativity]
|-
| = || ''S''(''b'') + ''a'' || [as shown [[#Definitions|above]]]
|}

This completes the induction on ''b''.

== See also==
*[[Binary operation]]
*[[Mathematical proofs|Proof]]
*[[Mathematical ring|Ring]]

== References ==
&lt;references/&gt;
*[[Edmund Landau]], Foundations of Analysis, Chelsea Pub Co. {{ISBN|0-8218-2693-X}}.

{{DEFAULTSORT:Addition Of Natural Numbers/Proofs}}
[[Category:Article proofs]]
[[Category:Abstract algebra]]
[[Category:Elementary algebra]]
[[Category:Binary operations]]</text>
      <sha1>gu53dokgxj3md02d9u94r1ih3ewag5l</sha1>
    </revision>
  </page>
  <page>
    <title>Regression analysis</title>
    <ns>0</ns>
    <id>826997</id>
    <revision>
      <id>867840688</id>
      <parentid>867661822</parentid>
      <timestamp>2018-11-08T10:11:36Z</timestamp>
      <contributor>
        <ip>131.181.158.31</ip>
      </contributor>
      <comment>/* References */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="37022">{{Regression bar}}
{{Machine learning bar}}
In [[statistical model]]ing, '''regression analysis''' is a set of statistical processes for [[Estimation theory|estimating]] the relationships among variables. It includes many techniques for modeling and analyzing several variables, when the focus is on the relationship between a [[dependent variable]] and one or more [[independent variable]]s (or 'predictors'). More specifically, regression analysis helps one understand how the typical value of the dependent variable  (or 'criterion variable') changes when any one of the independent variables is varied, while the other independent variables are held fixed.

Most commonly, regression analysis estimates the [[conditional expectation]] of the dependent variable given the independent variables – that is, the [[average value]] of the dependent variable when the independent variables are fixed. Less commonly, the focus is on a [[quantile]], or other [[location parameter]] of the conditional distribution of the dependent variable given the independent variables. In all cases, a [[function (mathematics)|function]] of the independent variables called the '''regression function''' is to be estimated. In regression analysis, it is also of interest to characterize the variation of the dependent variable around the prediction of the  regression function using a [[probability distribution]]. A related but distinct approach is Necessary Condition Analysis&lt;ref&gt;[http://www.erim.eur.nl/centres/necessary-condition-analysis/ Necessary Condition Analysis]&lt;/ref&gt; (NCA), which estimates the maximum (rather than average) value of the dependent variable for a given value of the independent variable (ceiling line rather than central line) in order to identify what value of the independent variable is [[Necessity and sufficiency|necessary but not sufficient]] for a given value of the dependent variable.

Regression analysis is widely used for [[prediction]] and [[forecasting|forecast]]ing, where its use has substantial overlap with the field of [[machine learning]]. Regression analysis is also used to understand which among the independent variables are related to the dependent variable, and to explore the forms of these relationships. In restricted circumstances, regression analysis can be used to infer [[causality|causal relationships]] between the independent and dependent variables. However this can lead to illusions or false relationships, so caution is advisable.

Many techniques for carrying out regression analysis have been developed. Familiar methods such as [[linear regression]] and [[ordinary least squares]] regression are [[parametric statistics|parametric]], in that the regression function is defined in terms of a finite number of unknown [[parameter]]s that are estimated from the [[data]]. [[Nonparametric regression]] refers to techniques that allow the regression function to lie in a specified set of [[function (mathematics)|functions]], which may be [[dimension|infinite-dimensional]].

The performance of regression analysis methods in practice depends on the form of the [[data collection|data generating process]], and how it relates to the regression approach being used. Since the true form of the data-generating process is generally not known, regression analysis often depends to some extent on making assumptions about this process. These assumptions are sometimes testable if a sufficient quantity of data is available. Regression models for prediction are often useful even when the assumptions are moderately violated, although they may not perform optimally. However, in many applications, especially with small [[effect size|effects]] or questions of [[causality]] based on [[observational study|observational data]], regression methods can give misleading results.&lt;ref&gt;David A. Freedman, ''Statistical Models: Theory and Practice'', Cambridge University Press (2005)&lt;/ref&gt;&lt;ref&gt;R. Dennis Cook; Sanford Weisberg [https://www.jstor.org/stable/270724 Criticism and Influence Analysis in Regression], ''Sociological Methodology'', Vol. 13. (1982), pp. 313–361&lt;/ref&gt;

In a narrower sense, regression may refer specifically to the estimation of continuous response (dependent) variables, as opposed to the discrete response variables used in [[statistical classification|classification]].&lt;ref&gt;{{cite book |author=Christopher M. Bishop |year=2006 |title=Pattern Recognition and Machine Learning |publisher=Springer |quote=Cases [...] in which the aim is to assign each input vector to one of a finite number of discrete categories, are called ''classification'' problems. If the desired output consists of one or more continuous dependent variables, then the task is called ''regression''. |page=3}}&lt;/ref&gt; The case of a continuous dependent variable may be more specifically referred to as ''metric regression'' to distinguish it from related problems.&lt;ref&gt;{{cite journal |title=ROC analysis in [[ordinal regression]] learning |first1=Willem |last1=Waegeman |first2=Bernard |last2=De Baets |first3=Luc |last3=Boullart |doi=10.1016/j.patrec.2007.07.019 |year=2008 |journal=Pattern Recognition Letters |volume=29 |pages=1–9}}&lt;/ref&gt;

==History==
The earliest form of regression was the [[method of least squares]], which was published by [[Adrien Marie Legendre|Legendre]] in 1805,&lt;ref name="Legendre"&gt;[[Adrien-Marie Legendre|A.M. Legendre]]. [https://books.google.com/books?id=FRcOAAAAQAAJ ''Nouvelles méthodes pour la détermination des orbites des comètes''], Firmin Didot, Paris, 1805. “Sur la Méthode des moindres quarrés” appears as an appendix.&lt;/ref&gt; and by [[Carl Friedrich Gauss|Gauss]] in 1809.&lt;ref name="Gauss"&gt;[[Carl Friedrich Gauss|C.F. Gauss]]. ''Theoria Motus Corporum Coelestium in Sectionibus Conicis Solem Ambientum''. (1809)&lt;/ref&gt; Legendre and Gauss both applied the method to the problem of determining, from astronomical observations, the orbits of bodies about the Sun (mostly comets, but also later the then newly discovered minor planets&lt;!-- Legendre's first example is applied to [[C/1769 P1]] (Messier) --&gt;). Gauss published a further development of the theory of least squares in 1821,&lt;ref name="Gauss2"&gt;C.F. Gauss. [https://books.google.com/books?id=ZQ8OAAAAQAAJ&amp;printsec=frontcover&amp;dq=Theoria+combinationis+observationum+erroribus+minimis+obnoxiae&amp;as_brr=3#v=onepage&amp;q=&amp;f=false ''Theoria combinationis observationum erroribus minimis obnoxiae'']. (1821/1823)&lt;/ref&gt; including a version of the [[Gauss–Markov theorem]].

The term "regression" was coined by [[Francis Galton]] in the nineteenth century to describe a biological phenomenon. The phenomenon was that the heights of descendants of tall ancestors tend to regress down towards a normal average (a phenomenon also known as [[regression toward the mean]]).&lt;ref&gt;
{{cite book
  | last = Mogull
  | first = Robert G.
  | title = Second-Semester Applied Statistics
  | publisher = Kendall/Hunt Publishing Company
  | year = 2004
  | page = 59
  | isbn = 0-7575-1181-3
}}&lt;/ref&gt;&lt;ref&gt;{{cite journal | last=Galton | first=Francis | journal=Statistical Science | year=1989 | title=Kinship and Correlation (reprinted 1989) | volume=4 | jstor=2245330 | pages=80–86 | publisher=Institute of Mathematical Statistics | issue=2 | doi=10.1214/ss/1177012581}}&lt;/ref&gt;
For Galton, regression had only this biological meaning,&lt;ref&gt;[[Francis Galton]]. "Typical laws of heredity", Nature 15 (1877), 492–495, 512–514, 532–533. ''(Galton uses the term "reversion" in this paper, which discusses the size of peas.)''&lt;/ref&gt;&lt;ref&gt;Francis Galton. Presidential address, Section H, Anthropology. (1885) ''(Galton uses the term "regression" in this paper, which discusses the height of humans.)''&lt;/ref&gt; but his work was later extended by [[Udny Yule]] and [[Karl Pearson]] to a more general statistical context.&lt;ref&gt;{{cite journal | doi=10.2307/2979746 | last=Yule | first=G. Udny | authorlink=G. Udny Yule | title=On the Theory of Correlation | journal=Journal of the Royal Statistical Society | year= 1897 | pages=812&amp;ndash;54 | jstor=2979746 | volume=60 | issue=4 | publisher=Blackwell Publishing}}&lt;/ref&gt;&lt;ref&gt;{{cite journal | doi=10.1093/biomet/2.2.211 | authorlink=Karl Pearson | last=Pearson | first=Karl |author2=Yule, G.U. |author3=Blanchard, Norman |author4= Lee,Alice  | title=The Law of Ancestral Heredity | journal=[[Biometrika]] | year=1903 | jstor=2331683 | pages=211–236 | volume=2 | issue=2 | publisher=Biometrika Trust}}&lt;/ref&gt; In the work of Yule and Pearson, the [[joint distribution]] of the response and explanatory variables is assumed to be [[Normal distribution|Gaussian]]. This assumption was weakened by [[Ronald A. Fisher|R.A. Fisher]] in his works of 1922 and 1925.&lt;ref&gt;{{cite journal | last=Fisher | first=R.A. | title=The goodness of fit of regression formulae, and the distribution of regression coefficients | journal=Journal of the Royal Statistical Society | volume=85 | pages=597–612 | year=1922 | doi=10.2307/2341124 | jstor=2341124 | issue=4 | publisher=Blackwell Publishing}}&lt;/ref&gt;&lt;ref name="FisherR1954Statistical"&gt;{{Cite book
 | author = [[Ronald A. Fisher]]
 | title = Statistical Methods for Research Workers
 | publisher = Oliver and Boyd
 | location = [[Edinburgh]]
 | year = 1954
 | edition = Twelfth
 | url = http://psychclassics.yorku.ca/Fisher/Methods/
 | isbn = 0-05-002170-2
}}&lt;/ref&gt;&lt;ref&gt;{{cite journal | last=Aldrich | first=John | journal=Statistical Science | year=2005 | title=Fisher and Regression | volume=20 | issue=4 | pages=401&amp;ndash;417 | jstor=20061201 | doi=10.1214/088342305000000331}}&lt;/ref&gt; Fisher assumed that the [[conditional distribution]] of the response variable is Gaussian, but the joint distribution need not be. In this respect, Fisher's assumption is closer to Gauss's formulation of 1821.

In the 1950s and 1960s, economists used electromechanical desk "calculators" to calculate regressions. Before 1970, it sometimes took up to 24 hours to receive the result from one regression.&lt;ref&gt;Rodney Ramcharan. [http://www.imf.org/external/pubs/ft/fandd/2006/03/basics.htm Regressions: Why Are Economists Obessessed with Them?] March 2006. Accessed 2011-12-03.&lt;/ref&gt;

Regression methods continue to be an area of active research. In recent decades, new methods have been developed for [[robust regression]], regression involving correlated responses such as [[time series]] and [[growth curve (statistics)|growth curve]]s, regression in which the predictor (independent variable) or response variables are curves, images, graphs, or other complex data objects, regression methods accommodating various types of missing data, [[nonparametric regression]], [[Bayesian statistics|Bayesian]] methods for regression, regression in which the predictor variables are measured with error, regression with more predictor variables than observations, and causal inference with regression.

==Regression models==

Regression models involve the following parameters and variables:
*The '''unknown parameters''', denoted as &lt;math&gt;\beta&lt;/math&gt;, which may represent a [[scalar (physics)|scalar]] or a [[Euclidean vector|vector]].
*The '''independent variables''', &lt;math&gt;X&lt;/math&gt;.
*The '''dependent variable''', &lt;math&gt;Y&lt;/math&gt;.

In various [[List of fields of application of statistics|fields of application]], different terminologies are used in place of [[dependent and independent variables]].

A regression model relates &lt;math&gt;Y&lt;/math&gt; to a function of &lt;math&gt;X&lt;/math&gt; and &lt;math&gt; \beta&lt;/math&gt;.

:&lt;math&gt;Y \approx f (X, \beta).&lt;/math&gt;

The approximation is usually formalized as &lt;math&gt;\operatorname E(Y|X)=f(X,  \beta)&lt;/math&gt;. To carry out regression analysis, the form of the function &lt;math&gt;f&lt;/math&gt; must be specified. Sometimes the form of this function is based on knowledge about the relationship between &lt;math&gt;Y&lt;/math&gt; and &lt;math&gt;X&lt;/math&gt; that does not rely on the data. If no such knowledge is available, a flexible or convenient form for &lt;math&gt;f&lt;/math&gt; is chosen.

Assume now that the vector of unknown parameters &lt;math&gt;\beta&lt;/math&gt; is of length &lt;math&gt;k&lt;/math&gt;. In order to perform a regression analysis the user must provide information about the dependent variable &lt;math&gt;Y&lt;/math&gt;:
*If &lt;math&gt;N&lt;/math&gt; data points of the form &lt;math&gt;(Y, X)&lt;/math&gt; are observed, where &lt;math&gt;N &lt; k&lt;/math&gt;, most classical approaches to regression analysis cannot be performed: since the system of equations defining the regression model is underdetermined, there are not enough data to recover &lt;math&gt; \beta&lt;/math&gt;.
*If exactly &lt;math&gt;N=k&lt;/math&gt; data points are observed, and the function &lt;math&gt;f&lt;/math&gt; is linear, the equations &lt;math&gt;Y=f(X,  \beta)&lt;/math&gt; can be solved exactly rather than approximately.  This reduces to solving a set of &lt;math&gt;N&lt;/math&gt; equations with &lt;math&gt;N&lt;/math&gt; unknowns (the elements of &lt;math&gt; \beta)&lt;/math&gt;, which has a unique solution as long as the &lt;math&gt;X&lt;/math&gt; are linearly independent. If &lt;math&gt;f&lt;/math&gt; is nonlinear, a solution may not exist, or many solutions may exist.
*The most common situation is where &lt;math&gt;N &gt; k&lt;/math&gt; data points are observed. In this case, there is enough information in the data to estimate a unique value for &lt;math&gt;\beta&lt;/math&gt; that best fits the data in some sense, and the regression model when applied to the data can be viewed as an [[overdetermined system]] in &lt;math&gt; \beta&lt;/math&gt;.

In the last case, the regression analysis provides the tools for:
#Finding a solution for unknown parameters &lt;math&gt; \beta&lt;/math&gt; that will, for example, minimize the distance between the measured and predicted values of the dependent variable &lt;math&gt;Y&lt;/math&gt; (also known as method of [[least squares]]).
#Under certain statistical assumptions, the regression analysis uses the surplus of information to provide statistical information about the unknown parameters &lt;math&gt; \beta&lt;/math&gt; and predicted values of the dependent variable &lt;math&gt;Y&lt;/math&gt;.

===Necessary number of independent measurements===
Consider a regression model which has three unknown parameters, &lt;math&gt;\beta_0&lt;/math&gt;, &lt;math&gt;\beta_1&lt;/math&gt;, and &lt;math&gt;\beta_2&lt;/math&gt;. Suppose an experimenter performs 10 measurements all at exactly the same value of independent variable vector &lt;math&gt;X&lt;/math&gt; (which contains the independent variables &lt;math&gt;X_1&lt;/math&gt;, &lt;math&gt;X_2&lt;/math&gt;, and &lt;math&gt;X_3&lt;/math&gt;). In this case, regression analysis fails to give a unique set of estimated values for the three unknown parameters; the experimenter did not provide enough information. The best one can do is to estimate the average value and the standard deviation of the dependent variable &lt;math&gt;Y&lt;/math&gt;. Similarly, measuring at two different values of &lt;math&gt;X&lt;/math&gt; would give enough data for a regression with two unknowns, but not for three or more unknowns.

If the experimenter had performed measurements at three different values of the independent variable vector &lt;math&gt; X&lt;/math&gt;, then regression analysis would provide a unique set of estimates for the three unknown parameters in &lt;math&gt; \beta&lt;/math&gt;.

In the case of [[Regression analysis#Linear regression|general linear regression]], the above statement is equivalent to the requirement that the matrix &lt;math&gt;X^\top X&lt;/math&gt; is [[Invertible matrix|invertible]].

When the number of measurements, &lt;math&gt;N&lt;/math&gt;, is larger than the number of unknown parameters, &lt;math&gt;k&lt;/math&gt;, and the measurement errors &lt;math&gt;\epsilon_i&lt;/math&gt; are normally distributed then ''the excess of information'' contained in &lt;math&gt;(N - k)&lt;/math&gt; measurements is used to make statistical predictions about the unknown parameters. This excess of information is referred to as the [[Degrees of freedom (statistics)#Linear regression|degrees of freedom]] of the regression.

==Underlying assumptions==
Classical [[statistical assumption|assumptions]] for regression analysis include:
*The sample is representative of the population for the inference prediction.
*The error is a [[random variable]] with a mean of zero conditional on the explanatory variables.
*The independent variables are measured with no error. (Note: If this is not so, modeling may be done instead using [[errors-in-variables model]] techniques).
* The independent variables (predictors) are [[linearly independent]], i.e. it is not possible to express any predictor as a linear combination of the others.
* The errors are [[uncorrelated]], that is, the [[Covariance matrix|variance–covariance matrix]] of the errors is [[Diagonal matrix|diagonal]] and each non-zero element is the variance of the error.
*The variance of the error is constant across observations ([[homoscedasticity]]). If not, [[weighted least squares]] or other methods might instead be used.
These are sufficient conditions for the least-squares estimator to possess desirable properties; in particular, these assumptions imply that the parameter estimates will be [[bias of an estimator|unbiased]], [[consistent estimator|consistent]], and [[efficient (statistics)|efficient]] in the class of linear unbiased estimators. It is important to note that actual data rarely satisfies the assumptions. That is, the method is used even though the assumptions are not true. Variation from the assumptions can sometimes be used as a measure of how far the model is from being useful. Many of these assumptions may be relaxed in more advanced treatments. Reports of statistical analyses usually include analyses of tests on the sample data and methodology for the fit and usefulness of the model.

Independent and dependent variables often refer to values measured at point locations. There may be spatial trends and spatial autocorrelation in the variables that violate statistical assumptions of regression. Geographic weighted regression is one technique to deal with such data.&lt;ref&gt;{{cite book|last3=Charlton|first=A. Stewart|last= Fotheringham|first2= Chris |last2=Brunsdon|first3= Martin|title=Geographically weighted regression: the analysis of spatially varying relationships|year=2002|publisher=John Wiley|location=Chichester, England|isbn=978-0-471-49616-8|edition=Reprint}}&lt;/ref&gt; Also, variables may include values aggregated by areas. With aggregated data the [[modifiable areal unit problem]] can cause extreme variation in regression parameters.&lt;ref&gt;{{cite journal|last=Fotheringham|first=AS|author2=Wong, DWS |title=The modifiable areal unit problem in multivariate statistical analysis|journal=Environment and Planning A|date=1 January 1991|volume=23|issue=7|pages=1025–1044|doi=10.1068/a231025}}&lt;/ref&gt; When analyzing data aggregated by political boundaries, postal codes or census areas results may be very distinct with a different choice of units.

==Linear regression==
{{Main|Linear regression}}
{{Hatnote|See [[simple linear regression]] for a derivation of these formulas and a numerical example}}
In linear regression, the model specification is that the dependent variable, &lt;math&gt; y_i &lt;/math&gt; is a [[linear combination]] of the ''parameters'' (but need not be linear in the ''independent variables''). For example, in [[simple linear regression]] for modeling &lt;math&gt; n &lt;/math&gt; data points there is one independent variable: &lt;math&gt; x_i &lt;/math&gt;, and two parameters, &lt;math&gt;\beta_0&lt;/math&gt; and &lt;math&gt;\beta_1&lt;/math&gt;:

:straight line: &lt;math&gt;y_i=\beta_0 +\beta_1 x_i +\varepsilon_i,\quad i=1,\dots,n.\!&lt;/math&gt;

In multiple linear regression, there are several independent variables or functions of independent variables.

Adding a term in &lt;math&gt;x_i^2&lt;/math&gt; to the preceding regression gives:

:parabola: &lt;math&gt;y_i=\beta_0 +\beta_1 x_i +\beta_2 x_i^2+\varepsilon_i,\ i=1,\dots,n.\!&lt;/math&gt;

This is still linear regression; although the expression on the right hand side is quadratic in the independent variable &lt;math&gt;x_i&lt;/math&gt;, it is linear in the parameters &lt;math&gt;\beta_0&lt;/math&gt;, &lt;math&gt;\beta_1&lt;/math&gt; and &lt;math&gt;\beta_2.&lt;/math&gt;

In both cases, &lt;math&gt;\varepsilon_i&lt;/math&gt; is an error term and the subscript &lt;math&gt;i&lt;/math&gt; indexes a particular observation.

Returning our attention to the straight line case: Given a random sample from the population, we estimate the population parameters and obtain the sample linear regression model:

: &lt;math&gt; \widehat{y}_i = \widehat{\beta}_0 + \widehat{\beta}_1 x_i. &lt;/math&gt;

The [[errors and residuals in statistics|residual]], &lt;math&gt; e_i = y_i - \widehat{y}_i &lt;/math&gt;, is the difference between the value of the dependent variable predicted by the model, &lt;math&gt; \widehat{y}_i&lt;/math&gt;, and the true value of the dependent variable, &lt;math&gt;y_i&lt;/math&gt;. One method of estimation is [[ordinary least squares]]. This method obtains parameter estimates that minimize the sum of squared [[errors and residuals in statistics|residuals]], [[Residual sum of squares|SSR]]:

:&lt;math&gt;SSR=\sum_{i=1}^n e_i^2. \, &lt;/math&gt;

Minimization of this function results in a set of [[Linear least squares (mathematics)|normal equations]], a set of simultaneous linear equations in the parameters, which are solved to yield the parameter estimators, &lt;math&gt;\widehat{\beta}_0, \widehat{\beta}_1&lt;/math&gt;.

[[Image:Linear regression.svg|thumb|right|300px|Illustration of linear regression on a data set.]]

In the case of simple regression, the formulas for the least squares estimates are

:&lt;math&gt;\widehat{\beta}_1=\frac{\sum(x_i-\bar{x})(y_i-\bar{y})}{\sum(x_i-\bar{x})^2}\text{ and }\widehat{\beta}_0=\bar{y}-\widehat{\beta}_1\bar{x}&lt;/math&gt;

where &lt;math&gt;\bar{x}&lt;/math&gt; is the [[Arithmetic mean|mean]] (average) of the &lt;math&gt;x&lt;/math&gt; values and &lt;math&gt;\bar{y}&lt;/math&gt; is the mean of the &lt;math&gt;y&lt;/math&gt; values.

Under the assumption that the population error term has a constant variance, the estimate of that variance is given by:

: &lt;math&gt; \hat{\sigma}^2_\varepsilon = \frac{SSR}{n-2}.\,&lt;/math&gt;

This is called the [[mean square error]] (MSE) of the regression. The denominator is the sample size reduced by the number of model parameters estimated from the same data, &lt;math&gt;(n-p)&lt;/math&gt; for &lt;math&gt;p&lt;/math&gt; [[regressor]]s or &lt;math&gt;(n-p-1)&lt;/math&gt; if an intercept is used.&lt;ref&gt;Steel, R.G.D, and Torrie, J. H., ''Principles and Procedures of Statistics with Special Reference to the Biological Sciences.'', [[McGraw Hill]], 1960, page 288.&lt;/ref&gt; In this case, &lt;math&gt;p=1&lt;/math&gt; so the denominator is &lt;math&gt;n-2&lt;/math&gt;.

The [[standard error (statistics)|standard error]]s of the parameter estimates are given by

:&lt;math&gt;\hat\sigma_{\beta_1}=\hat\sigma_{\varepsilon} \sqrt{\frac{1}{\sum(x_i-\bar x)^2}}&lt;/math&gt;

:&lt;math&gt;\hat\sigma_{\beta_0}=\hat\sigma_{\varepsilon} \sqrt{\frac{1}{n} + \frac{\bar{x}^2}{\sum(x_i-\bar x)^2}}=\hat\sigma_{\beta_1} \sqrt{\frac{\sum x_i^2}{n}}&lt;/math&gt;.

Under the further assumption that the population error term is normally distributed, the researcher can use these estimated standard errors to create [[confidence interval]]s and conduct [[hypothesis test]]s about the [[population parameter]]s.

===General linear model===
{{Hatnote|For a derivation, see [[linear least squares (mathematics)|linear least squares]]}}
{{Hatnote|For a numerical example, see [[linear regression]]}}
In the more general multiple regression model, there are &lt;math&gt;p&lt;/math&gt; independent variables:

: &lt;math&gt; y_i = \beta_1 x_{i1} + \beta_2 x_{i2} + \cdots + \beta_p x_{ip} + \varepsilon_i, \, &lt;/math&gt;

where &lt;math&gt;x_{ij}&lt;/math&gt; is the &lt;math&gt;i&lt;/math&gt;-th observation on the &lt;math&gt;j&lt;/math&gt;-th independent variable.
If the first independent variable takes the value 1 for all &lt;math&gt;i&lt;/math&gt;, &lt;math&gt;x_{i1} = 1&lt;/math&gt;, then &lt;math&gt;\beta_1&lt;/math&gt; is called the [[regression intercept]].

The least squares parameter estimates are obtained from &lt;math&gt;p&lt;/math&gt; normal equations. The residual can be written as

:&lt;math&gt;\varepsilon_i=y_i -  \hat\beta_1 x_{i1} - \cdots - \hat\beta_p x_{ip}.&lt;/math&gt;

The '''normal equations''' are

:&lt;math&gt;\sum_{i=1}^n \sum_{k=1}^p x_{ij}x_{ik}\hat \beta_k=\sum_{i=1}^n x_{ij}y_i,\  j=1,\dots,p.\,&lt;/math&gt;

In matrix notation, the normal equations are written as

:&lt;math&gt;\mathbf{(X^\top X )\hat{\boldsymbol{\beta}}= {}X^\top Y},\,&lt;/math&gt;

where the &lt;math&gt;ij&lt;/math&gt; element of &lt;math&gt;\mathbf X&lt;/math&gt; is &lt;math&gt;x_{ij}&lt;/math&gt;, the &lt;math&gt;i&lt;/math&gt; element of the column vector &lt;math&gt;Y&lt;/math&gt; is &lt;math&gt;y_i&lt;/math&gt;, and the &lt;math&gt;j&lt;/math&gt; element of &lt;math&gt;\hat \boldsymbol \beta&lt;/math&gt; is &lt;math&gt;\hat \beta_j&lt;/math&gt;. Thus &lt;math&gt;\mathbf X&lt;/math&gt; is &lt;math&gt;n \times p&lt;/math&gt;, &lt;math&gt;Y&lt;/math&gt; is &lt;math&gt;n \times 1&lt;/math&gt;, and &lt;math&gt;\hat \boldsymbol \beta&lt;/math&gt; is &lt;math&gt;p \times 1&lt;/math&gt;. The solution is

:&lt;math&gt;\mathbf{\hat{\boldsymbol{\beta}}= {}(X^\top X )^{-1}X^\top Y}.\,&lt;/math&gt;

===Diagnostics===
{{main|Regression diagnostics}}
{{Category see also|Regression diagnostics}}
Once a regression model has been constructed, it may be important to confirm the [[goodness of fit]] of the model and the [[statistical significance]] of the estimated parameters. Commonly used checks of goodness of fit include the [[R-squared]], analyses of the pattern of [[errors and residuals in statistics|residuals]] and hypothesis testing. Statistical significance can be checked by an [[F-test]] of the overall fit, followed by [[t-test]]s of individual parameters.

Interpretations of these diagnostic tests rest heavily on the model assumptions. Although examination of the residuals can be used to invalidate a model, the results of a [[t-test]] or [[F-test]] are sometimes more difficult to interpret if the model's assumptions are violated. For example, if the error term does not have a normal distribution, in small samples the estimated parameters will not follow normal distributions and complicate inference. With relatively large samples, however, a [[central limit theorem]] can be invoked such that hypothesis testing may proceed using asymptotic approximations.

===Limited dependent variables===

[[Limited dependent variable]]s, which are response variables that are [[categorical variable]]s or are variables constrained to fall only in a certain range, often arise in [[econometrics]].

The response variable may be non-continuous ("limited" to lie on some subset of the real line). For binary (zero or one) variables, if analysis proceeds with least-squares linear regression, the model is called the [[linear probability model]]. Nonlinear models for binary dependent variables include the [[probit model|probit]] and [[logistic regression|logit model]]. The [[multivariate probit]] model is a standard method of estimating a joint relationship between several binary dependent variables and some independent variables. For [[categorical variable]]s with more than two values there is the [[multinomial logit]]. For [[ordinal variable]]s with more than two values, there are the [[ordered logit]] and [[ordered probit]] models. [[Censored regression model]]s may be used when the dependent variable is only sometimes observed, and [[Heckman correction]] type models may be used when the sample is not randomly selected from the population of interest. An alternative to such procedures is linear regression based on [[polychoric correlation]] (or polyserial correlations) between the categorical variables. Such procedures differ in the assumptions made about the distribution of the variables in the population. If the variable is positive with low values and represents the repetition of the occurrence of an event, then count models like the [[Poisson regression]] or the [[negative binomial]] model may be used.

==Nonlinear regression==
{{Main|Nonlinear regression}}

When the model function is not linear in the parameters, the sum of squares must be minimized by an iterative procedure. This introduces many complications which are summarized in [[least squares#Differences between linear and nonlinear least squares|Differences between linear and non-linear least squares]].

==Interpolation and extrapolation==
[[File:CurveWeightHeight.png|thumb|right|370px|In the middle, the interpolated straight line represents the best balance between the points above and below this line. The dotted lines represent the two extreme lines. The first curves represent the estimated values. The outer curves represent a prediction for a new measurement.&lt;ref&gt;{{cite book |last=Rouaud |first=Mathieu |title=Probability, Statistics and Estimation|year=2013 |page=60 |url=http://www.incertitudes.fr/book.pdf }}&lt;/ref&gt;]]
Regression models predict a value of the ''Y'' variable given known values of the ''X'' variables. Prediction ''within'' the range of values in the dataset used for model-fitting is known informally as [[interpolation]]. Prediction ''outside'' this range of the data is known as [[extrapolation]]. Performing extrapolation relies strongly on the regression assumptions. The further the extrapolation goes outside the data, the more room there is for the model to fail due to differences between the assumptions and the sample data or the true values.

It is generally advised {{Citation needed|date=February 2010}} that when performing extrapolation, one should accompany the estimated value of the dependent variable with a prediction interval that represents the uncertainty. Such intervals tend to expand rapidly as the values of the independent variable(s) moved outside the range covered by the observed data.

For such reasons and others, some tend to say that it might be unwise to undertake extrapolation.&lt;ref&gt;Chiang, C.L, (2003) ''Statistical methods of analysis'', World Scientific. {{isbn|981-238-310-7}} - [https://books.google.com/books?id=BuPNIbaN5v4C&amp;lpg=PA274&amp;dq=regression%20extrapolation&amp;pg=PA274#v=onepage&amp;q=regression%20extrapolation&amp;f=false page 274 section 9.7.4 "interpolation vs extrapolation"]&lt;/ref&gt;

However, this does not cover the full set of modeling errors that may be made: in particular, the assumption of a particular form for the relation between ''Y'' and ''X''. A properly conducted regression analysis will include an assessment of how well the assumed form is matched by the observed data, but it can only do so within the range of values of the independent variables actually available. This means that any extrapolation is particularly reliant on the assumptions being made about the structural form of the regression relationship. Best-practice advice here{{Citation needed|date=March 2011}} is that a linear-in-variables and linear-in-parameters relationship should not be chosen simply for computational convenience, but that all available knowledge should be deployed in constructing a regression model. If this knowledge includes the fact that the dependent variable cannot go outside a certain range of values, this can be made use of in selecting the model – even if the observed dataset has no values particularly near such bounds. The implications of this step of choosing an appropriate functional form for the regression can be great when extrapolation is considered. At a minimum, it can ensure that any extrapolation arising from a fitted model is "realistic" (or in accord with what is known).

==Power and sample size calculations==
There are no generally agreed methods for relating the number of observations versus the number of independent variables in the model. One rule of thumb suggested by Good and Hardin is &lt;math&gt;N=m^n&lt;/math&gt;, where &lt;math&gt;N&lt;/math&gt; is the sample size, &lt;math&gt;n&lt;/math&gt; is the number of independent variables and &lt;math&gt;m&lt;/math&gt; is the number of observations needed to reach the desired precision if the model had only one independent variable.&lt;ref&gt;{{cite book |last1=Good |first1=P. I. |author1-link=Phillip Good|last2=Hardin |first2=J. W. |title=Common Errors in Statistics (And How to Avoid Them)|publisher=Wiley|edition=3rd|location=Hoboken, New Jersey|year=2009|page=211|isbn=978-0-470-45798-6}}&lt;/ref&gt; For example, a researcher is building a linear regression model using a dataset that contains 1000 patients (&lt;math&gt;N&lt;/math&gt;). If the researcher decides that five observations are needed to precisely define a straight line (&lt;math&gt;m&lt;/math&gt;), then the maximum number of independent variables the model can support is 4, because

&lt;math&gt;\frac{\log{1000}}{\log{5}}=4.29&lt;/math&gt;.

==Other methods==
Although the parameters of a regression model are usually estimated using the method of least squares, other methods which have been used include:
* [[Bayesian method]]s, e.g. [[Bayesian linear regression]]
* Percentage regression, for situations where reducing ''percentage'' errors is deemed more appropriate.&lt;ref&gt;{{cite journal| ssrn=1406472 |title=Least Squares Percentage Regression |last=Tofallis |first=C. |journal=Journal of Modern Applied Statistical Methods |volume=7 |year=2009 |pages=526–534| doi=10.2139/ssrn.1406472}}&lt;/ref&gt;
* [[Least absolute deviations]], which is more robust in the presence of outliers, leading to [[quantile regression]]
* [[Nonparametric regression]], requires a large number of observations and is computationally intensive
* Distance metric learning, which is learned by the search of a meaningful distance metric in a given input space.&lt;ref&gt;{{cite journal |url=http://pages.cs.wisc.edu/~huangyz/caip09_Long.pdf |title=Human age estimation by metric learning for regression problems |author=YangJing Long |journal=Proc. International Conference on Computer Analysis of Images and Patterns |year=2009 |pages=74–82 |deadurl=yes |archiveurl=https://web.archive.org/web/20100108055346/http://pages.cs.wisc.edu/~huangyz/caip09_Long.pdf |archivedate=2010-01-08 |df= }}&lt;/ref&gt;

==Software==
{{Main|List of statistical packages}}
All major statistical software packages perform [[least squares]] regression analysis and inference. [[Simple linear regression]] and multiple regression using least squares can be done in some [[spreadsheet]] applications and on some calculators. While many statistical software packages can perform various types of nonparametric and robust regression, these methods are less standardized; different software packages implement different methods, and a method with a given name may be implemented differently in different packages. Specialized regression software has been developed for use in fields such as survey analysis and neuroimaging.

==See also==
{{Portal|Statistics}}
{{Colbegin}}
* [[Curve fitting]]
* [[Estimation Theory]]
* [[Forecasting]]
* [[Fraction of variance unexplained]]
* [[Function approximation]]
* [[Generalized linear models]]
* [[Kriging]] (a linear least squares estimation algorithm)
* [[Local regression]]
* [[Modifiable areal unit problem]]
* [[Multivariate adaptive regression splines]]
* [[Multivariate normal distribution]]
* [[Pearson product-moment correlation coefficient]]
*[[Quasi-variance]]
* [[Prediction interval]]
* [[Regression validation]]
* [[Robust regression]]
* [[Segmented regression]]
* [[Signal processing]]
* [[Stepwise regression]]
* [[Trend estimation]]
{{Colend}}

== References ==
{{Reflist}}

==Further reading==
* [[William Kruskal|William H. Kruskal]] and [[Judith Tanur|Judith M. Tanur]], ed. (1978), "Linear Hypotheses," ''International Encyclopedia of Statistics''. Free Press,  v. 1,
:Evan J. Williams, "I. Regression," pp. 523–41.
:[[Julian C. Stanley]], "II. Analysis of Variance," pp. 541–554.
* [[D.V. Lindley|Lindley, D.V.]] (1987). "Regression and correlation analysis," [[New Palgrave: A Dictionary of Economics]], v. 4, pp.&amp;nbsp;120–23.
* Birkes, David and [[Yadolah Dodge|Dodge, Y.]], ''Alternative Methods of Regression''. {{isbn|0-471-56881-3}}
* Chatfield, C. (1993) "Calculating Interval Forecasts," ''Journal of Business and Economic Statistics,'' '''11'''. pp.&amp;nbsp;121–135.
* {{cite book
|title = Applied Regression Analysis
|edition = 3rd
|last1= Draper |first1=N.R. |last2=Smith |first2=H.
|publisher = John Wiley
|year = 1998
|isbn = 0-471-17082-8}}
* Fox, J. (1997).  ''Applied Regression Analysis, Linear Models and Related Methods.'' Sage
* Hardle, W., ''Applied Nonparametric Regression'' (1990), {{isbn|0-521-42950-1}}
* Meade, N. and T. Islam (1995) [http://onlinelibrary.wiley.com/doi/10.1002/for.3980140502/abstract "Prediction Intervals for Growth Curve Forecasts"] ''Journal of Forecasting,'' '''14''', pp.&amp;nbsp;413–430.
* A. Sen, M. Srivastava, ''Regression Analysis &amp;mdash; Theory, Methods, and Applications'', Springer-Verlag, Berlin, 2011 (4th printing).
* T. Strutz: ''Data Fitting and Uncertainty (A practical introduction to weighted least squares and beyond)''. Vieweg+Teubner, {{isbn|978-3-8348-1022-9}}.
* Malakooti, B. (2013). Operations and Production Systems with Multiple Objectives. John Wiley &amp; Sons.

==External links==
{{Commons category|Regression analysis}}
* {{springer|title=Regression analysis|id=p/r080620}}
* [http://jeff560.tripod.com/r.html Earliest Uses: Regression] – basic history and references
* [http://www.vias.org/simulations/simusoft_regrot.html Regression of Weakly Correlated Data] – how linear regression mistakes can appear when Y-range is much smaller than X-range

{{least squares and regression analysis}}
{{Statistics|correlation|state=collapsed}}
{{Quantitative forecasting methods}}
{{Public health}}

{{Authority control}}

{{DEFAULTSORT:Regression Analysis}}
[[Category:Regression analysis| ]]
[[Category:Estimation theory]]
[[Category:Actuarial science]]</text>
      <sha1>39fjf6y9k5r9zwcn1us1m04olgql95m</sha1>
    </revision>
  </page>
  <page>
    <title>Residuated Boolean algebra</title>
    <ns>0</ns>
    <id>12435391</id>
    <revision>
      <id>777417470</id>
      <parentid>607163820</parentid>
      <timestamp>2017-04-27T01:57:55Z</timestamp>
      <contributor>
        <username>Cedar101</username>
        <id>374440</id>
      </contributor>
      <minor/>
      <comment>/* Converse */ ˘</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8486">In [[mathematics]], a '''residuated Boolean algebra''' is a [[residuated lattice]] whose lattice structure is that of a [[Boolean algebra (structure)|Boolean algebra]]. Examples include Boolean algebras with the monoid taken to be conjunction, the set of all formal languages over a given alphabet Σ under concatenation, the set of all binary relations on a given set ''X'' under relational composition, and more generally the power set of any equivalence relation, again under relational composition. The original application was to [[relation algebra]]s as a finitely axiomatized generalization of the binary relation example, but there exist interesting examples of residuated Boolean algebras that are not relation algebras, such as the language example.

==Definition==
A '''residuated Boolean algebra''' is an algebraic structure (''L'', ∧, ∨, ¬, 0, 1, •, '''I''', \, /) such that
: (i) (''L'', &amp;and;, &amp;or;, •, '''I''', \, /) is a [[residuated lattice]], and
:(ii) (''L'', &amp;and;, &amp;or;, &amp;not;, 0, 1) is a Boolean algebra.

An equivalent signature better suited to the [[relation algebra]] application is (''L'', ∧, ∨, ¬, 0, 1, •, '''I''', ▷, ◁) where the unary operations ''x''\ and ''x''▷ are intertranslatable in the manner of [[De Morgan's laws]] via
:''x''\''y'' = &amp;not;(''x''&amp;#9655;&amp;not;''y''), &amp;nbsp; ''x''&amp;#9655;''y'' = &amp;not;(''x''\&amp;not;''y''), &amp;nbsp; and dually /''y'' and &amp;#9665;''y'' as
: ''x''/''y'' = &amp;not;(&amp;not;''x''&amp;#9665;''y''), &amp;nbsp; ''x''&amp;#9665;''y'' = &amp;not;(&amp;not;''x''/''y''),

with the residuation axioms in the [[residuated lattice]] article reorganized accordingly (replacing ''z'' by ¬''z'') to read
:(''x''&amp;#9655;''z'')&amp;and;''y'' = 0 &amp;nbsp; &amp;hArr; &amp;nbsp; (''x''•''y'')&amp;and;''z'' = 0 &amp;nbsp; &amp;hArr; &amp;nbsp; (''z''&amp;#9665;''y'')&amp;and;''x'' = 0

This [[De Morgan's laws|De Morgan dual]] reformulation is motivated and discussed in more detail in the section below on conjugacy.

Since residuated lattices and Boolean algebras are each definable with finitely many equations, so are residuated Boolean algebras, whence they form a finitely axiomatizable [[Variety (universal algebra)|variety]].

==Examples==
# Any Boolean algebra, with the monoid multiplication • taken to be conjunction and both residuals taken to be [[material conditional|material implication]] ''x''→''y''. Of the remaining 15 binary Boolean operations that might be considered in place of conjunction for the monoid multiplication, only five meet the monotonicity requirement, namely 0, 1, ''x'', ''y'', and ''x''∨''y''. Setting ''y'' = ''z'' = 0 in the residuation axiom ''y'' ≤ ''x''\''z'' &amp;nbsp; ⇔ &amp;nbsp; ''x''•''y'' ≤ ''z'', we have 0 ≤ ''x''\0 &amp;nbsp; ⇔ &amp;nbsp; ''x''•0 ≤ 0, which is falsified by taking ''x'' = 1 when ''x''•''y'' = 1, ''x'', or ''x''∨''y''. The dual argument for ''z''/''y'' rules out ''x''•''y'' = ''y''. This just leaves ''x''•''y'' = 0 (a constant binary operation independent of ''x'' and ''y''), which satisfies almost all the axioms when the residuals are both taken to be the constant operation ''x''/''y'' = ''x''\''y'' = 1. The axiom it fails is ''x''•'''I''' = ''x'' = '''I'''•''x'', for want of a suitable value for '''I'''. Hence conjunction is the only binary Boolean operation making the monoid multiplication that of a residuated Boolean algebra.
# The power set 2&lt;sup&gt;''X''²&lt;/sup&gt; made a Boolean algebra as usual with ∩, ∪ and complement relative to ''X''², and made a monoid with relational composition. The monoid unit '''I''' is the identity relation {(''x'',''x'')|''x'' ∈ ''X''}. The right residual ''R''\''S'' is defined by ''x''(''R''\''S'')''y'' if and only if for all ''z'' in ''X'', ''zRx'' implies ''zSy''. Dually the left residual ''S''/''R'' is defined by ''y''(''S''/''R'')''x'' if and only if for all ''z'' in ''X'', ''xRz'' implies ''ySz''.
# The power set 2&lt;sup&gt;Σ*&lt;/sup&gt; made a Boolean algebra as for example 2, but with language concatenation for the monoid. Here the set Σ is used as an alphabet while Σ* denotes the set of all finite (including empty) words over that alphabet. The concatenation ''LM'' of languages ''L'' and ''M'' consists of all words ''uv'' such that ''u'' ∈ ''L'' and ''v'' ∈ ''M''. The monoid unit is the language {ε} consisting of just the empty word ε. The right residual ''M''\''L'' consists of all words ''w'' over Σ such that ''Mw'' ⊆ ''L''. The left residual ''L''/''M'' is the same with ''wM'' in place of ''Mw''.

==Conjugacy==
The De Morgan duals ▷ and ◁ of residuation arise as follows. Among residuated lattices, Boolean algebras are special by virtue of having a complementation operation ¬. This permits an alternative expression of the three inequalities
:''y'' &amp;le; ''x''\''z'' &amp;nbsp; &amp;hArr; &amp;nbsp; ''x''•''y'' &amp;le; ''z'' &amp;nbsp; &amp;hArr; &amp;nbsp; ''x'' &amp;le; ''z''/''y''

in the axiomatization of the two residuals in terms of disjointness, via the equivalence ''x'' ≤ ''y'' ⇔ ''x''∧¬''y'' = 0. Abbreviating ''x''∧''y'' = 0 to ''x'' # ''y'' as the expression of their disjointness, and substituting ¬''z'' for ''z'' in the axioms, they become with a little Boolean manipulation
:&amp;not;(''x''\&amp;not;''z'') # ''y'' &amp;nbsp; &amp;hArr; &amp;nbsp; ''x''•''y'' # ''z'' &amp;nbsp; &amp;hArr; &amp;nbsp; &amp;not;(&amp;not;''z''/''y'') # ''x''

Now ¬(''x''\¬''z'') is reminiscent of [[De Morgan's laws|De Morgan duality]], suggesting that ''x''\ be thought of as a unary operation ''f'', defined by ''f''(y) = ''x''\''y'', that has a De Morgan dual ¬''f''(¬''y''), analogous to ∀''x''φ(''x'') = ¬∃''x''¬φ(''x''). Denoting this dual operation as ''x''▷, we define ''x''▷''z'' as ¬(''x''\¬''z''). Similarly we define another operation ''z''◁''y'' as ¬(¬''z''/''y''). By analogy with ''x''\ as the residual operation associated with the operation ''x''•, we refer to ''x''▷ as the conjugate operation, or simply '''conjugate''', of ''x''•. Likewise ◁''y'' is the '''conjugate''' of •''y''. Unlike residuals, conjugacy is an equivalence relation between operations: if ''f'' is the conjugate of ''g'' then ''g'' is also the conjugate of ''f'', i.e. the conjugate of the conjugate of ''f'' is ''f''. Another advantage of conjugacy is that it becomes unnecessary to speak of right and left conjugates, that distinction now being inherited from the difference between ''x''• and •''x'', which have as their respective conjugates ''x''▷ and ◁''x''.  (But this advantage accrues also to residuals when ''x''\ is taken to be the residual operation to ''x''•.)

All this yields (along with the Boolean algebra and monoid axioms) the following equivalent axiomatization of a residuated Boolean algebra.
:''y'' # ''x''&amp;#9655;''z'' &amp;nbsp; &amp;hArr; &amp;nbsp; ''x''•''y'' # ''z'' &amp;nbsp; &amp;hArr; &amp;nbsp; ''x'' # ''z''&amp;#9665;''y''

With this signature it remains the case that this axiomatization can be expressed as finitely many equations.

==Converse==
In examples 2 and 3 it can be shown that ''x''▷'''I''' = '''I'''◁''x''. In example 2 both sides equal the converse ''x''˘ of ''x'', while in example 3 both sides are '''I''' when ''x'' contains the empty word and 0 otherwise. In the former case ''x''˘ = ''x''. This is impossible for the latter because ''x''▷'''I''' retains hardly any information about ''x''. Hence in example 2 we can substitute ''x''˘ for ''x'' in ''x''▷'''I''' = ''x''˘ = '''I'''◁''x'' and cancel (soundly) to give
:''x''˘&amp;#9655;'''I''' = ''x'' =  '''I'''&amp;#9665;''x''˘.

''x''˘ = ''x'' can be proved from these two equations. [[Alfred Tarski|Tarski]]'s notion of a '''[[relation algebra]]''' can be defined as a residuated Boolean algebra having an operation ''x''˘ satisfying these two equations.

The cancellation step in the above is not possible for example 3, which therefore is not a relation algebra, ''x''˘ being uniquely determined as ''x''▷'''I'''.

Consequences of this axiomatization of converse include ''x''˘ = ''x'', ¬(''x''˘) = (¬''x'')˘, (''x''∨''y'')˘ = ''x''˘∨''y''˘, and (''x''•''y'')˘ = ''y''˘•''x''˘.

== References ==
* Bjarni Jónsson and Constantine Tsinakis, Relation algebras as residuated Boolean algebras, Algebra Universalis, 30 (1993) 469-478.
* Peter Jipsen, ''[http://www1.chapman.edu/~jipsen/dissertation/ Computer aided investigations of relation algebras]'', Ph.D. Thesis, Vanderbilt University, May 1992.

[[Category:Boolean algebra]]
[[Category:Mathematical logic]]
[[Category:Fuzzy logic]]
[[Category:Algebraic logic]]</text>
      <sha1>2vir0rodrwu0oy2v97gs37ubrzoerlz</sha1>
    </revision>
  </page>
  <page>
    <title>Reversed compound agent theorem</title>
    <ns>0</ns>
    <id>35068788</id>
    <revision>
      <id>844257337</id>
      <parentid>793431938</parentid>
      <timestamp>2018-06-03T18:59:22Z</timestamp>
      <contributor>
        <username>QYYZ</username>
        <id>33343951</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2423">{{redirect|Rcat|the usage of redirect categories on Wikipedia|WP:RCAT}}
In [[probability theory]], the '''reversed compound agent theorem''' ('''RCAT''') is a set of [[sufficient]] conditions for a [[stochastic process]] expressed in the [[PEPA]] language to have a [[product form stationary distribution]]&lt;ref name="rcat"&gt;{{Cite journal | last1 = Harrison | first1 = P. G. | authorlink = Peter G. Harrison| title = Turning back time in Markovian process algebra | doi = 10.1016/S0304-3975(02)00375-4 | url = http://pubs.doc.ic.ac.uk/rcat/| journal = Theoretical Computer Science | volume = 290 | issue = 3 | pages = 1947–2013 | year = 2003 | pmid =  | pmc = }}&lt;/ref&gt; (assuming that the process is stationary&lt;ref&gt;{{Cite journal | last1 = Harrison | first1 = P. G. | authorlink = Peter G. Harrison| title = Process Algebraic Non-product-forms | doi = 10.1016/j.entcs.2006.03.012 | journal = Electronic Notes in Theoretical Computer Science | volume = 151 | issue = 3 | pages = 61–06 | year = 2006 | url = http://pubs.doc.ic.ac.uk/separable-pasm/separable-pasm.pdf| pmid =  | pmc = }}&lt;/ref&gt;&lt;ref name="rcat" /&gt;). The theorem shows that product form solutions in [[Jackson's theorem (queueing theory)|Jackson's theorem]],&lt;ref name="rcat" /&gt; the [[BCMP theorem]]&lt;ref&gt;{{Cite journal | last1 = Harrison | first1 = P. G. | authorlink = Peter G. Harrison| doi = 10.1016/j.laa.2004.02.020 | url = http://pubs.doc.ic.ac.uk/ercat/| title = Reversed processes, product forms and a non-product form | journal = Linear Algebra and its Applications | volume = 386 | pages = 359–381| year = 2004 | pmid =  | pmc = }}&lt;/ref&gt; and [[G-network]]s are based on the same fundamental mechanisms.&lt;ref&gt;{{Cite book | last1 = Hillston | first1 = J. | authorlink1 = Jane Hillston| chapter = Process Algebras for Quantitative Analysis | doi = 10.1109/LICS.2005.35 | title = 20th Annual IEEE Symposium on Logic in Computer Science (LICS' 05) | pages = 239–248 | year = 2005 | isbn = 0-7695-2266-1 | url = http://www.dcs.ed.ac.uk/pepa/quantitativeanalysis.pdf| pmid =  | pmc = }}&lt;/ref&gt;

The theorem identifies a reversed process using [[Kelly's lemma]], from which the stationary distribution can be computed.&lt;ref name="rcat" /&gt;

==References==
{{reflist}}

==External links==
* [http://pubs.doc.ic.ac.uk/rcat-pepa-product-form/ RCAT: From PEPA to Product form] a short introduction to RCAT

[[Category:Probability theorems]]


{{Probability-stub}}</text>
      <sha1>sujgyb10ccjyo1j7ujr4zijml07jp0q</sha1>
    </revision>
  </page>
  <page>
    <title>Rodrigues' rotation formula</title>
    <ns>0</ns>
    <id>1525933</id>
    <revision>
      <id>870144350</id>
      <parentid>848701229</parentid>
      <timestamp>2018-11-22T18:21:13Z</timestamp>
      <contributor>
        <username>Boris Breuer</username>
        <id>12963327</id>
      </contributor>
      <minor/>
      <comment>/* Statement */ Defining terms.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12838">:''This article is about the Rodrigues' rotation formula, which is distinct from the related  [[Euler–Rodrigues parameters]] and [[SO(4)#The Euler–Rodrigues formula for 3D rotations|The Euler–Rodrigues formula for 3D rotation]].''

In the theory of [[three-dimensional rotation]], '''Rodrigues' rotation formula''', named after [[Olinde Rodrigues]], is an efficient algorithm for rotating a [[vector (geometric)|vector]] in space, given an [[axis angle|axis]] and [[angle of rotation]]. By extension, this can be used to transform all three [[basis vector]]s to compute a [[rotation matrix]] in [[Rotation group SO(3)|{{math|SO(3)}}]], the group of all rotation matrices, from an [[axis–angle representation]]. In other words, the Rodrigues' formula provides an algorithm to compute the [[matrix exponential|exponential map]] from {{math|'''so'''(3)}}, the [[Lie algebra]] of {{math|SO(3)}}, to {{math|SO(3)}} without actually computing the full matrix exponential.

==Statement==
If {{math|'''v'''}} is a vector in {{math|ℝ&lt;sup&gt;3&lt;/sup&gt;}} and {{math|'''k'''}} is a [[unit vector]] describing an axis of rotation about which {{math|'''v'''}} rotates  by an angle {{mvar|θ}} according to the [[Right hand rule#Direction associated with a rotation|right hand rule]], the Rodrigues formula for the rotated vector {{math|'''v'''&lt;sub&gt;rot&lt;/sub&gt;}} is
{{Equation box 1
|indent =:
|equation =&lt;math&gt;\mathbf{v}_\mathrm{rot} = \mathbf{v} \cos\theta + (\mathbf{k} \times \mathbf{v})\sin\theta + \mathbf{k} ~(\mathbf{k} \cdot \mathbf{v}) (1 - \cos\theta)\,.&lt;/math&gt;
|cellpadding= 6
|border
|border colour = #0073CF
|bgcolor=#F9FFF7}}

An alternative statement is to write the axis vector as a [[cross product]] {{math|'''a''' × '''b'''}} of any two nonzero vectors {{math|'''a'''}} and {{math|'''b'''}} which define the plane of rotation, and the sense of the angle {{math|''θ''}} is measured away from {{math|'''a'''}} and towards {{math|'''b'''}}. Letting {{math|''α''}} denote the angle between these vectors, the two angles {{math|''θ''}} and {{math|''α''}} are not necessarily equal, but they are measured in the same sense. Then the unit axis vector can be written

:&lt;math&gt;\mathbf{k} = \frac{\mathbf{a}\times\mathbf{b}}{|\mathbf{a}\times\mathbf{b}|} = \frac{\mathbf{a}\times\mathbf{b}}{|\mathbf{a}||\mathbf{b}|\sin\alpha}\,. &lt;/math&gt;

This form may be more useful when two vectors defining a plane are involved. An example in physics is the [[Thomas precession]] which includes the rotation given by Rodrigues' formula, in terms of two non-collinear boost velocities, and the axis of rotation is perpendicular to their plane.

== Derivation ==

[[Image:Rodrigues-formula.svg|300px|right|thumb|Rodrigues' rotation formula rotates {{math|'''v'''}} by an angle {{math|''θ''}} around vector {{math|''k''}} by decomposing it into its components parallel and perpendicular to {{math|''k''}}, and rotating only the perpendicular component.]] 
[[File:Orthogonal decomposition unit vector rodrigues rotation formula.svg|350px|thumb|Vector geometry of Rodrigues' rotation formula, as well as the decomposition into parallel and perpendicular components.]]

Let {{math|'''k'''}} be a [[unit vector]] defining a rotation axis, and let {{math|'''v'''}} be any vector to rotate about {{math|'''k'''}} by angle {{math|''θ''}} ([[right hand rule]], anticlockwise in the figure).

Using the [[dot product|dot]] and [[cross product]]s, the vector {{math|'''v'''}} can be decomposed into components parallel and perpendicular to the axis {{math|'''k'''}},

:&lt;math&gt; \mathbf{v} = \mathbf{v}_\parallel + \mathbf{v}_\perp \,, &lt;/math&gt;

where the component parallel to {{math|'''k'''}} is

:&lt;math&gt; \mathbf{v}_\parallel = (\mathbf{v} \cdot \mathbf{k}) \mathbf{k} &lt;/math&gt;

called the [[vector projection]] of {{math|'''v'''}} on {{math|'''k'''}}, and the component perpendicular to {{math|'''k'''}} is

:&lt;math&gt;\mathbf{v}_{\perp} = \mathbf{v} - \mathbf{v}_{\parallel} = \mathbf{v} - (\mathbf{k} \cdot \mathbf{v}) \mathbf{k} = - \mathbf{k}\times(\mathbf{k}\times\mathbf{v})&lt;/math&gt;

called the [[vector rejection]] of {{math|'''v'''}} from {{math|'''k'''}}. 

The vector {{math|'''k''' × '''v'''}} can be viewed as a copy of {{math|'''v'''&lt;sub&gt;⊥&lt;/sub&gt;}} rotated anticlockwise by 90° about {{math|'''k'''}}, so their magnitudes are equal but directions are perpendicular. Likewise the vector {{math|'''k''' × ('''k''' × '''v''')}} a copy of {{math|'''v'''&lt;sub&gt;⊥&lt;/sub&gt;}} rotated anticlockwise through {{math|180°}} about {{math|'''k'''}}, so that {{math|'''k''' × ('''k''' × '''v''')}} and {{math|'''v'''&lt;sub&gt;⊥&lt;/sub&gt;}} are equal in magnitude but in opposite directions (i.e. they are negatives of each other, hence the minus sign). Expanding the [[vector triple product]] establishes the connection between the parallel and perpendicular components, for reference the formula is {{math|'''a''' × ('''b''' × '''c''') {{=}} ('''a''' · '''c''')'''b''' − ('''a''' · '''b''')'''c'''}} given any three vectors {{math|'''a'''}}, {{math|'''b'''}}, {{math|'''c'''}}.

The component parallel to the axis will not change magnitude nor direction under the rotation,

:&lt;math&gt; \mathbf{v}_{\parallel\mathrm{rot}} = \mathbf{v}_\parallel \,, &lt;/math&gt;

only the perpendicular component will change direction but retain its magnitude, according to 

:&lt;math&gt;\begin{align} \left|\mathbf{v}_{\perp\mathrm{rot}}\right| &amp;= \left|\mathbf{v}_\perp\right| \,, \\
\mathbf{v}_{\perp\mathrm{rot}} &amp;= \cos\theta \mathbf{v}_\perp + \sin\theta \mathbf{k}\times\mathbf{v}_\perp \,, \end{align}&lt;/math&gt;

and since {{math|'''k'''}} and {{math|'''v'''&lt;sub&gt;∥&lt;/sub&gt;}} are parallel, their cross product is zero {{math|'''k''' × '''v'''&lt;sub&gt;∥&lt;/sub&gt; {{=}} '''0'''}}, so that

:&lt;math&gt; \mathbf{k} \times \mathbf{v}_\perp = \mathbf{k} \times \left(\mathbf{v} - \mathbf{v}_{\parallel}\right) = \mathbf{k} \times \mathbf{v} - \mathbf{k} \times \mathbf{v}_{\parallel} = \mathbf{k} \times \mathbf{v}  &lt;/math&gt;

and it follows

:&lt;math&gt; \mathbf{v}_{\perp\mathrm{rot}} = \cos\theta \mathbf{v}_\perp + \sin\theta \mathbf{k}\times\mathbf{v} \,. &lt;/math&gt;

This rotation is correct since the vectors {{math|'''v'''&lt;sub&gt;⊥&lt;/sub&gt;}} and {{math|'''k''' × '''v'''}} have the same length, and {{math|'''k''' × '''v'''}} is {{math|'''v'''&lt;sub&gt;⊥&lt;/sub&gt;}} rotated anticlockwise through {{math|90°}} about {{math|'''k'''}}. An appropriate scaling of {{math|'''v'''&lt;sub&gt;⊥&lt;/sub&gt;}} and {{math|'''k''' × '''v'''}} using the [[trigonometric functions]] [[sine]] and [[cosine]] gives the rotated perpendicular component. The form of the rotated component is similar to the radial vector in 2D planar [[polar coordinates]] {{math|(''r'', ''θ'')}} in the [[Cartesian coordinates|Cartesian basis]] 

:&lt;math&gt; \mathbf{r} = r\cos\theta \mathbf{e}_x + r\sin\theta \mathbf{e}_y \,, &lt;/math&gt;

where {{math|'''e'''&lt;sub&gt;''x''&lt;/sub&gt;}}, {{math|'''e'''&lt;sub&gt;''y''&lt;/sub&gt;}} are [[unit vector]]s in their indicated directions.

Now the full rotated vector is

:&lt;math&gt; \mathbf{v}_{\mathrm{rot}} = \mathbf{v}_{\parallel\mathrm{rot}} + \mathbf{v}_{\perp\mathrm{rot}} \,, &lt;/math&gt;

By substituting the definitions of {{math|'''v'''&lt;sub&gt;∥rot&lt;/sub&gt;}} and {{math|'''v'''&lt;sub&gt;⊥rot&lt;/sub&gt;}} in the equation results in

:&lt;math&gt; \begin{align}
\mathbf{v}_{\mathrm{rot}} 
&amp; = \mathbf{v}_\parallel + \cos\theta \, \mathbf{v}_\perp + \sin\theta \, \mathbf{k}\times\mathbf{v} \\
&amp; = \mathbf{v}_\parallel + \cos\theta \left(\mathbf{v} - \mathbf{v}_\parallel\right) + \sin\theta \, \mathbf{k}\times\mathbf{v} \\
&amp; = \cos\theta \, \mathbf{v} + (1 - \cos\theta) \mathbf{v}_\parallel + \sin\theta \, \mathbf{k}\times\mathbf{v} \\
&amp; = \cos\theta \, \mathbf{v} + (1 - \cos\theta ) (\mathbf{k} \cdot \mathbf{v})\mathbf{k} + \sin\theta \, \mathbf{k}\times\mathbf{v}
\end{align} &lt;/math&gt;

== Matrix notation ==

Representing {{math|'''v'''}} and {{math|'''k''' × '''v'''}} as [[row and column vectors|column matrices]], the cross product can be expressed as a [[matrix product]]

:&lt;math&gt;\begin{bmatrix} (\mathbf{k}\times\mathbf{v})_x \\ (\mathbf{k}\times\mathbf{v})_y \\ (\mathbf{k}\times\mathbf{v})_z \end{bmatrix} = \begin{bmatrix} k_y v_z - k_z v_y \\ k_z v_x - k_x v_z \\ k_x v_y - k_y v_x \end{bmatrix} = \begin{bmatrix} 0 &amp; -k_z &amp; k_y \\ k_z &amp; 0 &amp; -k_x \\ -k_y &amp; k_x &amp; 0 \end{bmatrix} \begin{bmatrix} v_x \\ v_y \\ v_z \end{bmatrix} \,. &lt;/math&gt;

Letting {{math|'''K'''}} denote the "[[Cross_product#Conversion_to_matrix_multiplication|cross-product matrix]]" for the unit vector {{math|'''k'''}},
: &lt;math&gt;\mathbf{K}=  
\left[\begin{array}{ccc}
0 &amp; -k_z &amp; k_y \\
k_z &amp; 0 &amp; -k_x \\
-k_y &amp; k_x &amp; 0
\end{array}\right]\,,
&lt;/math&gt;
the matrix equation is, symbolically,
: &lt;math&gt;\mathbf{K}\mathbf{v} = \mathbf{k}\times\mathbf{v} &lt;/math&gt;
for any vector {{math|'''v'''}}. (In fact, {{math|'''K'''}} is the unique matrix with this property. It has eigenvalues 0 and {{math|±''i''}}). 

Iterating the cross product on the right is equivalent to multiplying by the cross product matrix on the left, in particular
: &lt;math&gt;\mathbf{K}(\mathbf{K}\mathbf{v}) = \mathbf{K}^2\mathbf{v} = \mathbf{k}\times(\mathbf{k}\times\mathbf{v}) \,. &lt;/math&gt;

Moreover, since {{math|'''k'''}} is a unit vector, {{math|'''K'''}}  has unit [[Matrix norm|2-norm]]. The previous rotation formula in matrix language is therefore
&lt;!--PLEASE  Do NOT "correct" this formula unsoundly, before you appreciate the logic and function of the preceding one.  Discuss first.    --&gt;
:&lt;math&gt;\mathbf{v}_{\mathrm{rot}} = \mathbf{v} + (\sin\theta) \mathbf{K}\mathbf{v} + (1-\cos\theta)\mathbf{K}^2\mathbf{v} \,,\quad \|\mathbf{K}\|_2 = 1\,.&lt;/math&gt;
Note the coefficient of the leading term is ''now'' 1, in this notation.

Factorizing the {{math|'''v'''}} allows the compact expression
:&lt;math&gt;
\begin{align}
\mathbf{v}_{\mathrm{rot}} &amp;= \mathbf{R}\mathbf{v} 
\end{align}
&lt;/math&gt;
where 
{{Equation box 1
|indent =:
|equation =  &lt;math&gt;   \mathbf{R} = \mathbf{I} + (\sin\theta) \mathbf{K} + (1-\cos\theta)\mathbf{K}^2  &lt;/math&gt;
|cellpadding= 6
|border
|border colour = #0073CF
|bgcolor=#F9FFF7}}

is the [[rotation matrix]] through an angle {{mvar|θ}} counterclockwise about the axis {{math|'''k'''}}, and {{math|'''I'''}} the {{nowrap|3 × 3}} [[identity matrix]]. This matrix {{math|'''R'''}} is an element of the rotation group {{math|SO(3)}} of {{math|ℝ&lt;sup&gt;3&lt;/sup&gt;}}, and {{math|'''K'''}} is an element of the [[Lie algebra]] {{math|'''so'''(3)}} generating that Lie group (note that {{math|'''K'''}} is skew-symmetric, which characterizes {{math|'''so'''(3)}}). In terms of the matrix exponential, 
:&lt;math&gt; \mathbf{R} = \exp (\theta\mathbf{K})\,.&lt;/math&gt;
To see that the last identity holds, one notes that
:&lt;math&gt;\mathbf{R}(\theta) \mathbf{R}(\phi)= \mathbf{R} (\theta+\phi), \quad \mathbf{R}(0) = \mathbf{I}\,, &lt;/math&gt;
characteristic of a [[one-parameter subgroup]], i.e. exponential, and that the formulas match for infinitesimal {{mvar|θ}}.

For an alternative derivation based on this exponential relationship, see [[Axis–angle representation#Exponential map from so(3) to SO(3)|exponential map from {{math|'''so'''(3)}} to {{math|SO(3)}}]]. For the inverse mapping, see [[Axis–angle representation#Log map from SO(3) to so(3)|log map from {{math|SO(3)}} to {{math|'''so'''(3)}}]].

== See also ==
* [[Axis angle]]
* [[Rotation (mathematics)]]
* [[SO(3)]] and [[SO(4)]]
* [[Euler–Rodrigues formula]]

== References ==
*[[Leonhard Euler]], "Problema algebraicum ob affectiones prorsus singulares memorabile", ''Commentatio 407 Indicis Enestoemiani, Novi Comm. Acad. Sci. Petropolitanae''  '''15''' (1770), 75–106.
*[[Olinde Rodrigues]], "Des lois géometriques qui regissent les déplacements d' un systéme solide dans l' espace, et de la variation des coordonnées provenant de ces déplacement considérées indépendant des causes qui peuvent les produire", ''J. Math. Pures Appl.'' '''5''' (1840), 380–440.
*Don Koks, (2006) ''Explorations in Mathematical Physics'', Springer Science+Business Media,LLC. {{isbn|0-387-30943-8}}. Ch.4, pps 147 et seq. ''A Roundabout Route to Geometric Algebra'

== External links ==
* {{MathWorld |title=Rodrigues' Rotation Formula |urlname=RodriguesRotationFormula}}
* Johan E. Mebius, [https://arxiv.org/abs/math/0701759  Derivation of the Euler-Rodrigues formula for three-dimensional rotations from the general formula for four-dimensional rotations.], ''arXiv General Mathematics'' 2007.
* For another descriptive example see http://chrishecker.com/Rigid_Body_Dynamics#Physics_Articles, Chris Hecker, physics section, part 4. "The Third Dimension" – on page 3, section ``Axis and Angle'', http://chrishecker.com/images/b/bb/Gdmphys4.pdf

{{DEFAULTSORT:Rodrigues' Rotation Formula}}
[[Category:Rotation in three dimensions]]
[[Category:Euclidean geometry]]
[[Category:Orientation (geometry)]]

[[fr:Rotation vectorielle#Cas général]]</text>
      <sha1>cwkhk0mvgyle6y8diyo8mfrbbqctrg2</sha1>
    </revision>
  </page>
  <page>
    <title>Synchronous frame</title>
    <ns>0</ns>
    <id>21666983</id>
    <revision>
      <id>869475845</id>
      <parentid>863577328</parentid>
      <timestamp>2018-11-18T21:24:08Z</timestamp>
      <contributor>
        <username>Colonies Chris</username>
        <id>577301</id>
      </contributor>
      <comment>/* Synchronous coordinates */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="29796">A reference frame in which the time [[coordinate]] defines [[proper time]] for all co-moving observers is called "synchronous". It is built by choosing some [[time-like]] [[hypersurface]] as an origin, such that has in every point a [[normal (geometry) | normal]] along the time line (lies inside the [[light cone]] with an apex in that point); all interval elements on this hypersurface are [[space-like]]. A family of [[geodesics]] normal to this hypersurface are drawn and defined as the time coordinates with a beginning at the hypersurface.

Such a construct, and hence, choice of synchronous frame, is always possible though it is not unique. It allows any transformation of space coordinates that does not depend on time and, additionally, a transformation brought about by the arbitrary choice of hypersurface used for this geometric construct.

== Synchronization over a curved space ==
[[Einstein synchronisation | Synchronization]] of clocks located at different space points means that events happening at different places can be measured as simultaneous if those clocks show the same times. In the special relativity theory, the space distance element ''dl'' is defined as the intervals between two very close events that occur at the same moment of time. In the general relativity theory this cannot be done, that is, one cannot define ''dl'' by just substituting ''dx''&lt;sup&gt;0&lt;/sup&gt; = 0 in the metric. The reason for this is the different dependence between [[proper time]] and time coordinate ''x''&lt;sup&gt;0&lt;/sup&gt; in different points of space.
[[File:Synchronization.svg|250px|left|thumb|'''Figure 1.''' Synchronization of clocks in curved space through light signals.]]
To find ''dl'' in this case, one can first synchronize time over the whole space in the following way (Fig. 1): Bob sends a light signal from some space point ''B'' with coordinates ''x''&lt;sup&gt;α&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''dx''&lt;sup&gt;α&lt;/sup&gt; to Alice who is at a very close point ''A'' with coordinates ''x''&lt;sup&gt;α&lt;/sup&gt; and then Alice immediately reflects the signal back to Bob. The time necessary for this operation (measured by Bob), multiplied by ''c'' is, obviously, the doubled distance between [[Alice and Bob]].

The squared interval, with separated space and time coordinates, is:
{{NumBlk|:|&lt;math&gt;ds^2 = g_{\alpha\beta}\, dx^\alpha\, dx^\beta + 2g_{0\alpha}\, dx^0\, dx^\alpha + g_{00} \left ( dx^0 \right )^2,&lt;/math&gt;|{{EquationRef|eq. 1}}}}
where a repeated Greek index within a term means summation by values 1, 2, 3. The interval between the events of signal arrival in point ''A'' and its immediate reflection back is zero (two events in the same time at the same point). Equation ''ds''&lt;sup&gt;2&lt;/sup&gt; = 0 solved for ''dx''&lt;sup&gt;0&lt;/sup&gt; gives two roots:
: &lt;math&gt;dx^{0(1)} = \frac{1}{g_{00}} \left ( -g_{0\alpha}\, dx^\alpha - \sqrt{\left ( g_{0\alpha}g_{0\beta} - g_{\alpha \beta}g_{00} \right )\, dx^\alpha \,dx^\beta} \right ),&lt;/math&gt;
{{NumBlk|:|&lt;math&gt;dx^{0(2)} = \frac{1}{g_{00}} \left ( -g_{0\alpha}\, dx^\alpha + \sqrt{\left ( g_{0\alpha}g_{0\beta} - g_{\alpha \beta}g_{00} \right ) \,dx^\alpha \,dx^\beta} \right ),&lt;/math&gt;|{{EquationRef|eq. 2}}}}
which correspond to the propagation of the signal in both directions between Alice and Bob. If ''x''&lt;sup&gt;0&lt;/sup&gt; is the moment of arrival/reflection of the signal to/from Alice, the moments of signal departure from Bob and its arrival back to Bob correspond, respectively, to ''x''&lt;sup&gt;0&lt;/sup&gt; + ''dx''&lt;sup&gt;0 (1)&lt;/sup&gt; and ''x''&lt;sup&gt;0&lt;/sup&gt; + ''dx''&lt;sup&gt;0 (2)&lt;/sup&gt;. The thick lines on Fig. 1 are the world lines of Alice and Bob with coordinates ''x''&lt;sup&gt;α&lt;/sup&gt; and ''x''&lt;sup&gt;α&lt;/sup&gt; + ''dx''&lt;sup&gt;α&lt;/sup&gt;, respectively, while the red lines are the world lines of the signals. Fig. 1 supposes that ''dx''&lt;sup&gt;0 (2)&lt;/sup&gt; is positive and ''dx''&lt;sup&gt;0 (1)&lt;/sup&gt; is negative, which, however, is not necessarily the case: ''dx''&lt;sup&gt;0 (1)&lt;/sup&gt; and ''dx''&lt;sup&gt;0 (2)&lt;/sup&gt; may have the same sign. The fact that in the latter case the value ''x''&lt;sup&gt;0&lt;/sup&gt; (Alice) in the moment of signal arrival at Alice's position may be less than the value ''x''&lt;sup&gt;0&lt;/sup&gt; (Bob) in the moment of signal departure from Bob does not contain a contradiction because clocks in different points of space are not supposed to be synchronized. It is clear that the full "time" interval between departure and arrival of the signal in Bob's place is

: &lt;math&gt;dx^{0(2)} - dx^{0(1)} = \frac{2}{g_{00}} \sqrt{\left ( g_{0\alpha}g_{0\beta} - g_{\alpha \beta}g_{00} \right ) \,dx^\alpha \,dx^\beta}.&lt;/math&gt;

The respective proper time interval is obtained from the above relationship by multiplication by &lt;math&gt;\sqrt{g_{00}}/c&lt;/math&gt;, and the distance ''dl'' between the two points – by additional multiplication by ''c''/2. As a result:

{{NumBlk|:|&lt;math&gt;dl^2 = \left ( -g_{\alpha \beta} + \frac{g_{0\alpha}g_{0\beta}}{g_{00}} \right )\, dx^\alpha \,dx^\beta.&lt;/math&gt;|{{EquationRef|eq. 3}}}}
This is the required relationship that defines distance through the space coordinate elements.

It is obvious that such synchronization should be done by exchange of light signals between points. Consider again propagation of signals between infinitesimally close points ''A'' and ''B'' in Fig. 1. The clock reading in ''B'' which is simultaneous with the moment ''x''&lt;sup&gt;0&lt;/sup&gt; in ''A'' lies in the middle between the moments of sending and receiving the signal in ''B''; this is the moment
:&lt;math&gt;x^0 + \Delta x^0 = x^0 + \tfrac{1}{2} \left ( dx^{0(2)} + dx^{0(1)} \right ).&lt;/math&gt;
Substitute here {{EquationNote|eq. 2}} to find the difference in "time" ''x''&lt;sup&gt;0&lt;/sup&gt; between two simultaneous events occurring in infinitesimally close points as
{{NumBlk|:|&lt;math&gt;\Delta x^0 = -\frac{g_{0 \alpha}\, dx^\alpha}{g_{00}} \equiv g_\alpha \,dx^\alpha.&lt;/math&gt;|{{EquationRef|eq. 4}}}}
This relationship allows clock synchronization in any infinitesimally small space volume. By continuing such synchronization further from point ''A'', one can synchronize clocks, that is, determine simultaneity of events along any open line. The synchronization condition can be written in another form by multiplying {{EquationNote|eq. 4}} by ''g''&lt;sub&gt;00&lt;/sub&gt; and bringing terms to the left hand side  
{{NumBlk|:|&lt;math&gt;\Delta x_0 = g_{0i} dx^i = 0&lt;/math&gt;|{{EquationRef|eq. 5}}}}
or, the "covariant differential" ''dx''&lt;sub&gt;0&lt;/sub&gt; between two infinitesimally close points should be zero.

However, it is impossible, in general, to synchronize clocks along a closed contour: starting out along the contour and returning to the starting point one would obtain a Δ''x''&lt;sup&gt;0&lt;/sup&gt; value different from zero. Thus, unambiguous synchronization of clocks over the whole space is impossible. An exception are reference frames in which all components ''g''&lt;sub&gt;0α&lt;/sub&gt; are zeros.

Note the inability to synchronize all clocks is a property of the reference frame and not of the spacetime itself. It is always possible in infinitely many ways in any gravitational field to choose the reference frame so that the three ''g''&lt;sub&gt;0α&lt;/sub&gt; become zeros and thus enable a complete synchronization of clocks. To this class are assigned cases where ''g''&lt;sub&gt;0α&lt;/sub&gt; can be made zeros by a simple change in the time coordinate which does not involve a choice of a system of objects that define the space coordinates.

In the special relativity theory, too, proper time elapses differently for clocks moving relatively to each other. In general relativity, proper time is different even in the same reference frame at different points of space. This means that the interval of proper time between two events occurring at some space point and the time interval between the events simultaneous with those at another space point are, in general, different from one another.

== Space metric tensor ==

Rewrite {{EquationNote|eq. 3}} in the form

{{NumBlk|:|&lt;math&gt;dl^2 = \gamma_{\alpha \beta} \,dx^\alpha\, dx^\beta,&lt;/math&gt;|{{EquationRef|eq. 6}}}}

where

{{NumBlk|:|&lt;math&gt;\gamma_{\alpha \beta} = -g_{\alpha \beta} + \frac{g_{0\alpha}g_{0\beta}}{g_{00}}&lt;/math&gt;|{{EquationRef|eq. 7}}}}

is the three-dimensional metric tensor that determines the metric, that is, the geometrical properties of space. Equations {{EquationNote|eq. 7}} give the relationships between the metric of the three-dimensional space and the metric of the four-dimensional spacetime.

In general, however, the metric ''g&lt;sub&gt;ik&lt;/sub&gt;'' depends on ''x''&lt;sup&gt;0&lt;/sup&gt; so that the space metric {{EquationNote|eq. 7}} changes with time. Therefore, it doesn't make sense to integrate ''dl'': this integral depends on the choice of world line between the two points on which it is taken. It follows that in general relativity the distance between two bodies cannot be determined in general; this distance is determined only for infinitesimally close points. Distance can be determined also for finite space regions only in such reference frames in which ''g&lt;sub&gt;ik&lt;/sub&gt;'' does not depend on time and therefore the integral '''∫'''''dl'' along the space curve acquires some definite sense.

The tensor&amp;nbsp;–γ&lt;sub&gt;αβ&lt;/sub&gt; is inverse to the contravariant 3-dimensional tensor ''g''&lt;sup&gt;αβ&lt;/sup&gt;. Indeed, writing equation ''g&lt;sup&gt;ik&lt;/sup&gt;g&lt;sub&gt;kl&lt;/sub&gt;'' = &lt;math&gt;\scriptstyle{\delta_l^i}&lt;/math&gt; in components, one has:

: &lt;math&gt;g^{\alpha \beta}g_{\beta \gamma} + g^{\alpha 0}g_{0 \gamma} = \delta_\gamma^\alpha,&lt;/math&gt;

{{NumBlk|:|&lt;math&gt;g^{\alpha \beta}g_{\beta 0} + g^{\alpha 0}g_{00} = 0,&lt;/math&gt;|{{EquationRef|eqs. 8}}}}

: &lt;math&gt;g^{0 \beta}g_{\beta 0} + g^{00}g_{00} = 1.&lt;/math&gt;

Determine ''g''&lt;sup&gt;α0&lt;/sup&gt; from the second equation and substitute in the first to obtain

{{NumBlk|:|&lt;math&gt;-g^{\alpha \beta} \gamma_{\beta \gamma} = \delta_\gamma^\alpha,&lt;/math&gt;|{{EquationRef|eq. 9}}}}

which was to be demonstrated. This result can be presented otherwise by saying that ''g''&lt;sup&gt;αβ&lt;/sup&gt; are components of a contravariant 3-dimensional tensor corresponding to metric {{EquationNote|eq. 7}}:

{{NumBlk|:|&lt;math&gt;\gamma^{\alpha \beta} = -g^{\alpha \beta}.&lt;/math&gt;|{{EquationRef|eq. 10}}}}

The determinants ''g'' and γ composed of elements ''g&lt;sub&gt;ik&lt;/sub&gt;'' and γ&lt;sub&gt;αβ&lt;/sub&gt;, respectively, are related to each other by the simple relationship:

{{NumBlk|:|&lt;math&gt;-g = g_{00}\gamma.&lt;/math&gt;|{{EquationRef|eq. 11}}}}

In many applications, it is convenient to define a 3-dimensional vector '''g''' with covariant components

{{NumBlk|:|&lt;math&gt;g_\alpha = -\frac{g_{0 \alpha}}{g_{00}}.&lt;/math&gt;|{{EquationRef|eq. 12}}}}

Considering '''g''' as a vector in space with metric {{EquationNote|eq. 7}}, its contravariant components can be written as ''g''&lt;sup&gt;α&lt;/sup&gt; = γ&lt;sup&gt;αβ&lt;/sup&gt;''g''&lt;sub&gt;β&lt;/sub&gt;. Using {{EquationNote|eq. 11}} and the second of {{EquationNote|eqs. 8}}, it is easy to see that

{{NumBlk|:|&lt;math&gt;g^\alpha = \gamma^{\alpha \beta} g_\beta = -g^{0 \alpha}.&lt;/math&gt;|{{EquationRef|eq. 13}}}}

From the third of {{EquationNote|eqs. 8}}, it follows

{{NumBlk|:|&lt;math&gt;g^{00} = \frac{1}{g_{00}} - g_\alpha g^\alpha.&lt;/math&gt;|{{EquationRef|eq. 14}}}}

==Synchronous coordinates==

As concluded from {{EquationNote|eq. 5}}, the condition that allows clock synchronization in different space points is that metric tensor components ''g''&lt;sub&gt;0α&lt;/sub&gt; are zeros. If, in addition, ''g''&lt;sub&gt;00&lt;/sub&gt; = 1, then the time coordinate ''x''&lt;sup&gt;0&lt;/sup&gt; = ''t'' is the proper time in each space point (with ''c'' = 1). A reference frame that satisfies the conditions
{{NumBlk|:|&lt;math&gt;g_{00} = 1, \quad g_{0 \alpha} = 0,&lt;/math&gt;|{{EquationRef|eq. 15}}}}
is called ''synchronous frame''. The interval element in this system is given by the expression
{{NumBlk|:|&lt;math&gt;ds^2 = dt^2 - g_{\alpha \beta}\, dx^\alpha\, dx^\beta,&lt;/math&gt;|{{EquationRef|eq. 16}}}}
with the spatial metric tensor components identical (with opposite sign) to the components ''g''&lt;sub&gt;αβ&lt;/sub&gt;:
{{NumBlk|:|&lt;math&gt;\gamma_{\alpha \beta} = -g_{\alpha \beta}.&lt;/math&gt;|{{EquationRef|eq. 17}}}}
[[File:Time lines.svg|300px|left|thumb|'''Figure 2.''' A synchronous reference frame built with the choice of the timelike hypersurface ''t'' = const (teal color). Only one spatial coordinate ''x''&lt;sup&gt;1&lt;/sup&gt; = ''x'' is shown. The four observers have the same proper times ''x''&lt;sup&gt;0&lt;/sup&gt; = ''t'' which are normal to the hypersurface in their locally flat spacetimes (shown by the [[light cones]]). The unit vector ''n''&lt;sup&gt;0&lt;/sup&gt; = ''u''&lt;sup&gt;0&lt;/sup&gt; = 1 is shown in yellow. There are no spatial components (''u''&lt;sup&gt;α&lt;/sup&gt; = 0) so the common proper time is a geodesic line with a beginning at the hypersurface and a positive direction (red arrows).]]
In synchronous frame time, time lines are normal to the hypersurfaces ''t'' = const. Indeed, the unit four-vector normal to such a hypersurface ''n&lt;sub&gt;i&lt;/sub&gt;'' = ∂''t''/∂''x&lt;sup&gt;i&lt;/sup&gt;'' has covariant components ''n''&lt;sub&gt;α&lt;/sub&gt; = 0, ''n''&lt;sub&gt;0&lt;/sub&gt; = 1. The respective contravariant components with the conditions {{EquationNote|eq. 15}} are again ''n''&lt;sup&gt;α&lt;/sup&gt; = 0, ''n''&lt;sup&gt;0&lt;/sup&gt; = 1.

The components of the unit normal coincide with those of the four-vector ''u&lt;sup&gt; i&lt;/sup&gt;'' = ''dx&lt;sup&gt;i&lt;/sup&gt;/ds'' which is tangent to the world line ''x''&lt;sup&gt;1&lt;/sup&gt;, ''x''&lt;sup&gt;2&lt;/sup&gt;, ''x''&lt;sup&gt;3&lt;/sup&gt; = const. The ''u&lt;sup&gt; i&lt;/sup&gt;'' with components ''u''&lt;sup&gt;α&lt;/sup&gt; = 0, ''u''&lt;sup&gt;0&lt;/sup&gt; = 1 automatically satisfies the [[geodesics in general relativity|geodesic equations]]:
:&lt;math&gt;\frac{du^i}{ds} + \Gamma_{kl}^i u^k u^l = \Gamma_{00}^i = 0,&lt;/math&gt;
since, from the conditions {{EquationNote|eq. 15}}, the Christoffel symbols &lt;math&gt;\Gamma_{00}^{\alpha}&lt;/math&gt; and &lt;math&gt;\Gamma_{00}^0&lt;/math&gt; vanish identically. Therefore, in the synchronous reference system the time lines are geodesics in the spacetime.

These properties can be used to construct synchronous frame in any spacetime (Fig. 2). To this end, choose some [[spacelike]] [[hypersurface]] as an origin, such that has in every point a normal along the time line (lies inside the [[light cone]] with an apex in that point); all interval elements on this hypersurface are space-like. Then draw a family of geodesics normal to this hypersurface. Choose these lines as time coordinate lines and define the time coordinate ''t'' as the length ''s'' of the geodesic measured with a beginning at the hypersurface; the result is a synchronous frame.

An analytic transformation to synchronous frame can be done with the use of the [[Hamilton&amp;ndash;Jacobi equation]]. The principle of this method is based on the fact that particle trajectories in gravitational fields are geodesics. The [[Hamilton–Jacobi equation#HJE in a gravitational field|Hamilton&amp;ndash;Jacobi equation]] for a particle (whose mass is set equal to unity) in a gravitational field is
{{NumBlk|:|&lt;math&gt;g^{ik} \frac{\partial S}{\partial x^i} \frac{\partial S}{\partial x^k} = 1,&lt;/math&gt;|{{EquationRef|eq. 18a}}}}
where ''S'' is the action. Its complete integral has the form:
{{NumBlk|:|&lt;math&gt;S = f \left ( \xi^{\alpha}, x^i \right ) + A \left ( \xi^{\alpha} \right ),&lt;/math&gt;|{{EquationRef|eq. 18b}}}}
where ''f'' is a function of the four coordinates ''x&lt;sup&gt;i&lt;/sup&gt;'' and the three parameters ξ&lt;sup&gt;α&lt;/sup&gt;; the constant ''A'' is treated as an arbitrary function of the three ξ&lt;sup&gt;α&lt;/sup&gt;. With such a representation for ''S'' the equations for the trajectory of the particle can be obtained by equating the derivatives ∂''S''/∂ξ&lt;sup&gt;α&lt;/sup&gt; to zero, i.e. 
{{NumBlk|:|&lt;math&gt;\frac{\partial f}{\partial \xi^{\alpha}} = - \frac{\partial A}{\partial \xi^{\alpha}}.&lt;/math&gt;|{{EquationRef|eq. 18c}}}}
For each set of assigned values of the parameters ξ&lt;sup&gt;α&lt;/sup&gt;, the right sides of equations {{EquationNote|18a-18c}} have definite constant values, and the world line determined by these equations is one of the possible trajectories of the particle. Choosing the quantities ξ&lt;sup&gt;α&lt;/sup&gt;, which are constant along the trajectory, as new space coordinates, and the quantity ''S'' as the new time coordinate, one obtains a synchronous reference system; the transformation from the old coordinates to the new ones is given by equations {{EquationNote|18b-18c}}. In fact, it is guaranteed that for such a transformation the time lines will be geodesics and will be normal to the hypersurfaces ''S'' = const. The latter point is obvious from the mechanical analogy: the four-vector ∂''S''/∂''x''&lt;sup&gt;''i''&lt;/sup&gt; which is normal to the hypersurface coincides in mechanics with the four-momentum of the particle, and therefore coincides in direction with its four-velocity ''u''&lt;sup&gt; ''i''&lt;/sup&gt; i.e. with the four-vector tangent to the trajectory. Finally the condition ''g''&lt;sub&gt;00&lt;/sub&gt; = 1 is obviously satisfied, since the derivative −''dS''/''ds'' of the action along the trajectory is the mass of the particle, which was set equal to 1; therefore |''dS''/''ds''| = 1.

The gauge conditions {{EquationNote|eq. 15}} do not fix the coordinate system completely and therefore are not a fixed [[gauge theory|gauge]], as the spacelike hypersurface at &lt;math&gt;t=0&lt;/math&gt; can be chosen arbitrarily. One still have the freedom of performing some coordinate transformations containing four arbitrary functions depending on the three spatial variables ''x''&lt;sup&gt;α&lt;/sup&gt;, which are easily worked out in infinitesimal form:
{{NumBlk|:|&lt;math&gt;x^i = \acute{x}^i + \xi^i(\acute{x}).&lt;/math&gt;|{{EquationRef|eq. 18}}}}
Here, the collections of the four old coordinates (''t'', ''x''&lt;sup&gt;α&lt;/sup&gt;) and four new coordinates &lt;math&gt;(\acute{t}, \acute{x}^{\alpha})&lt;/math&gt; are denoted by the symbols ''x'' and &lt;math&gt;\acute{x}&lt;/math&gt;, respectively. The functions &lt;math&gt;\xi^i(\acute{x})&lt;/math&gt; together with their first derivatives are infinitesimally small quantities. After such a transformation, the four-dimensional interval takes the form:
{{NumBlk|:|&lt;math&gt;ds^2 = g_{ik}(x)dx^i dx^k = g^{(new)}_{ik} (\acute{x})d\acute{x}^i d\acute{x}^k,&lt;/math&gt;|{{EquationRef|eq. 19}}}}
where
{{NumBlk|:|&lt;math&gt;g^{(new)}_{ik} (\acute{x}) = g_{ik} (\acute{x}) + g_{il} (\acute{x}) \frac{\partial \xi^l (\acute{x})}{\partial \acute{x}^k} + g_{kl} (\acute{x}) \frac{\partial \xi^l (\acute{x})}{\partial \acute{x}^i} + \frac{\partial g_{ik} (\acute{x})}{\partial \acute{x}^l} \xi^l (\acute{x}).&lt;/math&gt;|{{EquationRef|eq. 20}}}}
In the last formula, the &lt;math&gt;g_{ik}(\acute{x})&lt;/math&gt; are the same functions ''g''&lt;sub&gt;''ik''&lt;/sub&gt;(''x'') in which ''x'' should simply be replaced by &lt;math&gt;\acute{x}&lt;/math&gt;. If one wishes to preserve the gauge {{EquationNote|eq. 15}} also for the new metric tensor &lt;math&gt;g^{(new)}_{ik} (\acute{x})&lt;/math&gt; in the new coordinates &lt;math&gt;\acute{x}&lt;/math&gt;, it is necessary to impose the following restrictions on the functions &lt;math&gt;\xi^i (\acute{x})&lt;/math&gt;:
{{NumBlk|:|&lt;math&gt;\frac{\partial \xi^0 (\acute{x})}{\partial \acute{x}} = 0, \quad g_{\alpha \beta} (\acute{x}) \frac{\partial \xi^{\beta} (\acute{x})}{\partial \acute{t}} - \frac{\partial \xi^0 (\acute{x})}{\partial \acute{x}^{\alpha}} = 0.&lt;/math&gt;|{{EquationRef|eq. 21}}}}
The solutions of these equations are:
{{NumBlk|:|&lt;math&gt;\xi^0 = f^0 \left (\acute{x}^1, \acute{x}^2, \acute{x}^3 \right ), \quad \xi^{\alpha} = \int{g^{\alpha \beta} (\acute{x}) \frac{\partial f^0 \left (\acute{x}^1, \acute{x}^2, \acute{x}^3 \right )}{\partial \acute{x}^{\beta}} d \acute{x}^0 } + f^{\alpha} \left (\acute{x}^1, \acute{x}^2, \acute{x}^3 \right ),&lt;/math&gt;|{{EquationRef|eq. 22}}}}
where ''f''&lt;sup&gt;0&lt;/sup&gt; and ''f''&lt;sup&gt;α&lt;/sup&gt; are four arbitrary functions depending only on the spatial coordinates &lt;math&gt;\acute{x}^{\alpha}&lt;/math&gt;.

For a more elementary geometrical explanation, consider Fig. 2. First, the synchronous time line ξ&lt;sup&gt;0&lt;/sup&gt; = ''t'' can be chosen arbitrarily (Bob's, Carol's, Dana's or any of an infinitely many observers). This makes one arbitrarily chosen function: &lt;math&gt;\xi^0 = f^0 \left (\acute{x}^1, \acute{x}^2, \acute{x}^3 \right )&lt;/math&gt;. Second, the initial hypersurface can be chosen in infinitely many ways. Each of these choices changes three functions: one function for each of the three spatial coordinates &lt;math&gt;\xi^{\alpha} = f^{\alpha} \left (\acute{x}^1, \acute{x}^2, \acute{x}^3 \right )&lt;/math&gt;. Altogether, four (= 1 + 3) functions are arbitrary.

When discussing general solutions ''g''&lt;sub&gt;αβ&lt;/sub&gt; of the field equations in synchronous gauges, it is necessary to keep in mind that the gravitational potentials ''g''&lt;sub&gt;αβ&lt;/sub&gt; contain, among all possible arbitrary functional parameters present in them, four arbitrary functions of 3-space just representing the gauge freedom and therefore of no direct physical significance.

Another problem with the reference system is that [[caustic (optics)|caustic]]s can occur which cause the gauge choice to break down. These problems have caused some difficulties doing [[cosmological perturbation theory]] in this system, but the problems are now well understood. Synchronous coordinates are generally considered the most efficient reference system for doing calculations, and are used in many modern cosmology codes, such as [[CMBFAST]]. They are also useful for solving theoretical problems in which a spacelike hypersurface needs to be fixed, as with spacelike [[gravitational singularity|singularities]].

==Einstein equations in synchronous frame==
Introduction of a synchronous frame allows one to separate the operations of space and time differentiation in the [[Einstein field equations]]. To make them more concise, the notation
{{NumBlk|:|&lt;math&gt;\varkappa_{\alpha \beta} = \frac{\partial \gamma_{\alpha \beta}}{\partial t}&lt;/math&gt;|{{EquationRef|eq. 23}}}}
is introduced for the time derivatives of the three-dimensional metric tensor; these quantities also form a three-dimensional tensor. In the synchronous frame &lt;math&gt;\varkappa_{\alpha \beta}&lt;/math&gt; is proportional to the [[second fundamental form]] (shape tensor). All operations of shifting indices and covariant differentiation of the tensor &lt;math&gt;\varkappa_{\alpha \beta}&lt;/math&gt; are done in three-dimensional space with the metric γ&lt;sub&gt;αβ&lt;/sub&gt;. This does not apply to operations of shifting indices in the space components of the four-tensors ''R&lt;sub&gt;ik&lt;/sub&gt;'', ''T&lt;sub&gt;ik&lt;/sub&gt;''. Thus ''T''&lt;sub&gt;α&lt;/sub&gt;&lt;sup&gt;β&lt;/sup&gt; must be understood to be ''g''&lt;sup&gt;βγ&lt;/sup&gt;''T''&lt;sub&gt;γα&lt;/sub&gt; + ''g''&lt;sup&gt;β0&lt;/sup&gt;''T''&lt;sub&gt;0α&lt;/sub&gt;, which reduces to ''g''&lt;sup&gt;βγ&lt;/sup&gt;''T''&lt;sub&gt;γα&lt;/sub&gt; and differs in sign from γ&lt;sup&gt;βγ&lt;/sup&gt;''T''&lt;sub&gt;γα&lt;/sub&gt;. The sum &lt;math&gt;\varkappa_{\alpha}^{\alpha}&lt;/math&gt; is the logarithmic derivative of the determinant γ ≡ |γ&lt;sub&gt;αβ&lt;/sub&gt;| = − ''g'':
{{NumBlk|:|&lt;math&gt;\varkappa_{\alpha}^{\alpha} = \gamma^{\alpha \beta} \frac{\partial \gamma_{\alpha \beta}}{\partial t} = \frac{\partial}{\partial t} \ln{(\gamma)}.&lt;/math&gt;|{{EquationRef|eq. 24}}}}
Then for the complete set of [[Christoffel symbols]] &lt;math&gt;\Gamma_{kl}^i&lt;/math&gt; one obtains:
{{NumBlk|:|&lt;math&gt;\Gamma_{00}^{0} = \Gamma_{00}^{\alpha} = \Gamma_{0 \alpha}^0 = 0, \quad \Gamma_{\alpha \beta}^0 = \frac{1}{2} \varkappa_{\alpha \beta}, \quad \Gamma_{0 \beta}^{\alpha} = \frac{1}{2} \varkappa_{\beta}^{\alpha}, \quad \Gamma_{\alpha \beta}^{\gamma} = \lambda_{\alpha \beta}^{\gamma}&lt;/math&gt;|{{EquationRef|eq. 25}}}}
where &lt;math&gt;\lambda_{\alpha \beta}^{\gamma}&lt;/math&gt; are the three-dimensional Christoffel symbols constructed from γ&lt;sub&gt;αβ&lt;/sub&gt;:
{{NumBlk|:|&lt;math&gt;\lambda_{\alpha \beta}^{\gamma} = \frac{1}{2} \gamma^{\gamma \mu} \left ( \gamma_{\mu \alpha,\beta} + \gamma_{\mu \beta,\beta} - \gamma_{\alpha \beta,\mu} \right ),&lt;/math&gt;|{{EquationRef|eq. 26}}}}
where the comma denotes partial derivative by the respective coordinate.

With the Christoffel symbols {{EquationNote|eq. 25}}, the components ''R&lt;sup&gt;i&lt;/sup&gt;&lt;sub&gt;k&lt;/sub&gt;'' = ''g&lt;sup&gt;il&lt;/sup&gt;R&lt;sub&gt;lk&lt;/sub&gt;'' of the [[Ricci tensor]] can be written in the form:
{{NumBlk|:|&lt;math&gt;R_0^0=-\frac{1}{2} \dot{\varkappa}-\frac{1}{4} \varkappa_{\alpha}^{\beta} \varkappa_{\beta}^{\alpha},&lt;/math&gt;|{{EquationRef|eq. 27}}}}
{{NumBlk|:|&lt;math&gt;R_{\alpha}^{0}=\frac{1}{2} \left (\varkappa_{\alpha;\beta}^{\beta} - \varkappa_{,\alpha} \right ),&lt;/math&gt;|{{EquationRef|eq. 28}}}}
{{NumBlk|:|&lt;math&gt;R_{\alpha}^{\beta}=-\frac{1}{2 \sqrt{\gamma}} \dot{\left (\sqrt{\gamma} \varkappa_{\alpha}^{\beta} \right )} - P_{\alpha}^{\beta}.&lt;/math&gt;|{{EquationRef|eq. 29}}}}
Dots on top denote time differentiation, semicolons (";") denote covariant differentiation which in this case is performed with respect to the three-dimensional metric γ&lt;sub&gt;αβ&lt;/sub&gt; with three-dimensional Christoffel symbols &lt;math&gt;\lambda_{\alpha \beta}^{\gamma}&lt;/math&gt;, &lt;math&gt;\varkappa \equiv \varkappa_{\alpha}^{\alpha}&lt;/math&gt;, and ''P''&lt;sub&gt;α&lt;/sub&gt;&lt;sup&gt;β&lt;/sup&gt; is a three-dimensional Ricci tensor constructed from &lt;math&gt;\lambda_{\alpha \beta}^{\gamma}&lt;/math&gt;:
{{NumBlk|:|&lt;math&gt;P_{\alpha}^{\beta} = \gamma^{\beta \gamma} P_{\gamma \alpha}, \quad P_{\alpha \beta} = \lambda_{\alpha \beta, \gamma}^{\gamma} - \lambda_{\gamma \alpha, \beta}^{\gamma} + \lambda_{\alpha \beta}^{\gamma} \lambda_{\gamma \mu}^{\mu} - \lambda_{\alpha \mu}^{\gamma} \lambda_{\beta \gamma}^{\mu}.&lt;/math&gt;|{{EquationRef|eq. 30}}}}
It follows from {{EquationNote|eq. 27–29}} that the Einstein equations &lt;math&gt;R_i^k = 8 \pi k \left ( T_i^k - \frac{1}{2} \delta_i^k T \right )&lt;/math&gt; (with the components of the energy-momentum tensor ''T''&lt;sub&gt;0&lt;/sub&gt;&lt;sup&gt;0&lt;/sup&gt; = −''T''&lt;sub&gt;00&lt;/sub&gt;, ''T''&lt;sub&gt;α&lt;/sub&gt;&lt;sup&gt;0&lt;/sup&gt; = −''T''&lt;sub&gt;0α&lt;/sub&gt;, ''T''&lt;sub&gt;α&lt;/sub&gt;&lt;sup&gt;β&lt;/sup&gt; = γ&lt;sup&gt;βγ&lt;/sup&gt;''T''&lt;sub&gt;γα&lt;/sub&gt;) become in a synchronous coordinate system:
{{NumBlk|:|&lt;math&gt;R_0^0=-\frac{1}{2} \dot{\varkappa}-\frac{1}{4} \varkappa_{\alpha}^{\beta} \varkappa_{\beta}^{\alpha} = 8 \pi k \left ( T_0^0 - \frac{1}{2} T \right ),&lt;/math&gt;|{{EquationRef|eq. 31}}}}
{{NumBlk|:|&lt;math&gt;R_{\alpha}^{0}=\frac{1}{2} \left (\varkappa_{\alpha;\beta}^{\beta} - \varkappa_{,\alpha} \right ) = 8 \pi k T_{\alpha}^0,&lt;/math&gt;|{{EquationRef|eq. 32}}}}
{{NumBlk|:|&lt;math&gt;R_{\alpha}^{\beta}=-\frac{1}{2 \sqrt{\gamma}} \dot{\left (\sqrt{\gamma} \varkappa_{\alpha}^{\beta} \right )} - P_{\alpha}^{\beta} = 8 \pi k \left ( T_{\alpha}^{\beta} - \frac{1}{2} \delta_{\alpha}^{\beta} T \right ).&lt;/math&gt;|{{EquationRef|eq. 33}}}}
A characteristic feature of synchronous reference systems is that they are not stationary: the [[gravitational field]] cannot be constant in such a system. In a constant field &lt;math&gt;\varkappa_{\alpha \beta}&lt;/math&gt; would become zero. But in the presence of matter the disappearance of all &lt;math&gt;\varkappa_{\alpha \beta}&lt;/math&gt; would contradict {{EquationNote|eq. 31}} (which has a right side different from zero). In empty space from {{EquationNote|eq. 33}} follows that all ''P''&lt;sub&gt;αβ&lt;/sub&gt;, and with them all the components of the three-dimensional curvature tensor ''P''&lt;sub&gt;αβγδ&lt;/sub&gt; ([[Riemann tensor]]) vanish, i.e. the field vanishes entirely (in a synchronous system with a [[Euclidean metric|Euclidean spatial metric]] the space-time is flat).

At the same time the matter filling the space cannot in general be at rest relative to the synchronous reference frame. This is obvious from the fact that particles of matter within which there are pressures generally move along lines that are not geodesics; the [[world line]] of a particle at rest is a time line, and thus is a geodesic in the synchronous reference system. An exception is the case of dust (''p'' = 0). Here the particles interacting with one another will move along geodesic lines; consequently, in this case the condition for a synchronous reference system does not contradict the condition that it be comoving with the matter. Even in this case, in order to be able to choose a synchronously comoving system of reference, it is still necessary that the matter move without rotation. In the comoving system the contravariant components of the velocity are ''u''&lt;sup&gt;0&lt;/sup&gt; = 1, ''u''&lt;sup&gt;α&lt;/sup&gt; = 0. If the reference system is also synchronous, the covariant components must satisfy ''u''&lt;sub&gt;0&lt;/sub&gt; = 1, ''u''&lt;sub&gt;α&lt;/sub&gt; = 0, so that its four-dimensional [[curl (mathematics)|curl]] must vanish:
:&lt;math&gt;u_{i;k} - u_{k;i} \equiv \frac{\partial u_i}{\partial x^k} - \frac{\partial u_k}{\partial x^i} = 0.&lt;/math&gt;
But this tensor equation must then also be valid in any other reference frame. Thus, in a synchronous but not comoving system the condition curl '''v''' = 0 for the three-dimensional velocity '''v''' is additionally needed. For other [[equations of state]] a similar situation can occur only in special cases when the pressure gradient vanishes in all or in certain directions.

==See also==
*[[Normal coordinates]]

== References ==

*{{Citation |last=Landau|first=Lev D.|first2=Evgeny M.|last2=Lifshitz|author-link=Lev Landau|author2-link=Evgeny Lifshitz| title=Classical Theory of Fields (7th Russian ed.) | location=Moscow | publisher=Nauka | year=1988 | isbn=5-02-014420-7}} Vol. 2 of the Course of Theoretical Physics.
*{{cite book | author=Carroll, Sean M. | title=Spacetime and Geometry: An Introduction to General Relativity | location=San Francisco | publisher=[[Addison-Wesley]] | date = 2004 | isbn=0-8053-8732-3}}.  See ''section 7.2''.
*{{cite journal | title = Cosmological perturbation theory in the synchronous and conformal Newtonian gauges | author = C.-P. Ma | author2 = E. Bertschinger | last-author-amp = yes | journal = Astrophys. J. | volume = 455 | pages = 7–25 | date = 1995 | doi = 10.1086/176550 | bibcode=1995ApJ...455....7M|arxiv = astro-ph/9506072 }}
*{{cite book | author=[[Lev Landau|Landau, L.D.]] and [[Evgeny Lifshitz|Lifshitz, E.M.]] | title=The Classical Theory of Fields | location=England | publisher=Elsevier Butterworth Heinemann | date=1972 |isbn=0-7506-2768-9}}. See section 97.

[[Category:General relativity]]
[[Category:Coordinate systems]]
[[Category:Frames of reference]]
[[Category:Coordinate charts in general relativity]]
[[Category:Physical cosmology]]</text>
      <sha1>sqfbx8wne2ui6p8d06x1nrd1ix20oi2</sha1>
    </revision>
  </page>
  <page>
    <title>Van Wijngaarden transformation</title>
    <ns>0</ns>
    <id>12480332</id>
    <revision>
      <id>848562655</id>
      <parentid>750611566</parentid>
      <timestamp>2018-07-02T18:51:40Z</timestamp>
      <contributor>
        <username>LilHelpa</username>
        <id>8024439</id>
      </contributor>
      <minor/>
      <comment>typo and general fixes using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3214">In [[mathematics]] and [[numerical analysis]], in order to accelerate convergence of an [[alternating series]], [[Euler transform|Euler's transform]] can be computed as follows.

Compute a row of partial sums :
:&lt;math&gt;s_{0,k} = \sum_{n=0}^k(-1)^n a_n &lt;/math&gt;
and form rows of averages between neighbors, 
:&lt;math&gt; \, s_{j+1,k} = \frac{s_{j,k}+s_{j,k+1}}2 &lt;/math&gt;
The first column &lt;math&gt;\scriptstyle s_{j,0}&lt;/math&gt; then contains the partial sums of the Euler transform.

[[Adriaan van Wijngaarden]]'s contribution was to point out that it is better not to carry this procedure through to the very end, but to stop two-thirds of the way.&lt;ref&gt;[[Adriaan van Wijngaarden|A. van Wijngaarden]], in:  Cursus:  Wetenschappelijk Rekenen B, Proces Analyse, Stichting Mathematisch Centrum, (Amsterdam, 1965) pp. 51-60&lt;/ref&gt; If  &lt;math&gt;\scriptstyle  a_0,a_1, \ldots, a_{12} &lt;/math&gt; are available, then &lt;math&gt; \scriptstyle s_{8,4} &lt;/math&gt; is almost always a better approximation to the sum than &lt;math&gt;\scriptstyle  s\, _{12,0}. &lt;/math&gt;

[[Leibniz formula for pi]], &lt;math&gt;\scriptstyle 1 - \frac 1 3 + \frac  1 5 -  \frac 1 7 + \cdots = \frac \pi 4 = 0.7853981\ldots &lt;/math&gt;, gives the partial sum &lt;math&gt;\scriptstyle  \,s_{0,12} = 0.8046006... (+2.4\%)&lt;/math&gt;, the Euler transform partial sum &lt;math&gt;\scriptstyle  \,s_{12,0} = 0.7854002... (+2.6 \times 10^{-6})&lt;/math&gt; and the van Wijngaarden result &lt;math&gt;\scriptstyle  \,s_{8,4} = 0.7853982... (+4.7 \times 10^{-8})&lt;/math&gt; (relative errors are in round brackets).

&lt;small&gt; 
 1.00000000 0.66666667 0.86666667 0.72380952 0.83492063 0.74401154 0.82093462 0.75426795 0.81309148 0.76045990 0.80807895 0.76460069 '''0.80460069'''
 0.83333333 0.76666667 0.79523810 0.77936508 0.78946609 0.78247308 0.78760129 0.78367972 0.78677569 0.78426943 0.78633982 0.78460069 
 0.80000000 0.78095238 0.78730159 0.78441558 0.78596959 0.78503719 0.78564050 0.78522771 0.78552256 0.78530463 0.78547026 
 0.79047619 0.78412698 0.78585859 0.78519259 0.78550339 0.78533884 0.78543410 0.78537513 0.78541359 0.78538744 
 0.78730159 0.78499278 0.78552559 0.78534799 0.78542111 0.78538647 0.78540462 0.78539436 0.78540052 
 0.78614719 0.78525919 0.78543679 0.78538455 0.78540379 0.78539555 0.78539949 0.78539744 
 0.78570319 0.78534799 0.78541067 0.78539417 0.78539967 0.78539752 0.78539847 
 0.78552559 0.78537933 0.78540242 0.78539692 0.78539860 0.78539799 
 0.78545246 0.78539087 0.78539967 0.78539776 '''0.78539829''' 
 0.78542166 0.78539527 0.78539871 0.78539803 
 0.78540847 0.78539699 0.78539837 
 0.78540273 0.78539768     
 '''0.78540021''' 
&lt;/small&gt;                   
This table results from the [[J (programming language)|J]] formula  'b11.8'8!:2-:&amp;(}:+}.)^:n+/\(_1^n)*%1+2*n=.i.13 In many cases the diagonal terms do not converge in one cycle so process of averaging is to be repeated with diagonal terms by bringing them in a row. This will be needed in a geometric series with ratio -4. This process of successive averaging of the average of partial sum can be replaced by using formula to calculate the diagonal term.

== References ==
{{reflist}}

== See also==
[[Euler summation]]

{{DEFAULTSORT:Van Wijngaarden Transformation}}
[[Category:Mathematical series]]
[[Category:Numerical analysis]]</text>
      <sha1>gb9m670tkupsjwci1s58tj2pqijviwg</sha1>
    </revision>
  </page>
  <page>
    <title>Weak interpretability</title>
    <ns>0</ns>
    <id>655334</id>
    <revision>
      <id>856604786</id>
      <parentid>612106359</parentid>
      <timestamp>2018-08-26T11:34:32Z</timestamp>
      <contributor>
        <username>Omnipaedista</username>
        <id>8524693</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2354">In [[mathematical logic]], '''weak interpretability''' is a notion of translation of logical theories,  introduced together with [[interpretability]] by [[Alfred Tarski]] in 1953.

Assume T and S are [[theory (mathematical logic)|formal theories]]. Slightly simplified, T is said to be '''weakly interpretable''' in S if, and only if, the language of T can be [[translation|translate]]d into the [[language]] of S in such a way that the translation of every [[theorem]] of T is consistent with S. Of course, there are some natural conditions on admissible translations here, such as the necessity for a translation to preserve the [[logic]]al structure of [[formula]]s.

A generalization of weak interpretability, [[tolerance (in logic)|tolerance]], was introduced by [[Giorgi Japaridze]] in 1992. 

==See also==
*[[Interpretability logic]]

==References==
*{{citation
 | last = Tarski | first = Alfred | authorlink = Alfred Tarski
 | location = Amsterdam
 | mr = 0058532
 | publisher = North-Holland Publishing Company
 | series = Studies in Logic and the Foundations of Mathematics
 | title = Undecidable theories
 | year = 1953}}. Written in collaboration with [[Andrzej Mostowski]] and [[Raphael M. Robinson]].
*{{citation
 | last = Dzhaparidze | first = Giorgie | authorlink = Giorgi Japaridze
 | doi = 10.1016/0168-0072(93)90201-N
 | issue = 1-2
 | journal = Annals of Pure and Applied Logic
 | mr = 1218658
 | pages = 113–160
 | title = A generalized notion of weak interpretability and the corresponding modal logic
 | volume = 61
 | year = 1993}}.
*{{citation
 | last = Dzhaparidze | first = Giorgie | authorlink = Giorgi Japaridze
 | doi = 10.1007/BF00370116
 | issue = 2
 | journal = Studia Logica
 | mr = 1185914
 | pages = 249–277
 | title = The logic of linear tolerance
 | volume = 51
 | year = 1992}}
*{{citation
 | last1 = Japaridze | first1 = Giorgi | author1-link = Giorgi Japaridze
 | last2 = de Jongh | first2 = Dick | author2-link = Dick de Jongh
 | editor-last = Buss | editor-first = Samuel R. | editor-link = Samuel Buss
 | contribution = The logic of provability
 | doi = 10.1016/S0049-237X(98)80022-0
 | location = Amsterdam
 | mr = 1640331
 | pages = 475–546
 | publisher = North-Holland
 | series = Stud. Logic Found. Math.
 | title = Handbook of Proof Theory
 | volume = 137
 | year = 1998}}

[[Category:Proof theory]]</text>
      <sha1>6w4zj397rt7ntcqb94v6rghaebv102v</sha1>
    </revision>
  </page>
</mediawiki>
