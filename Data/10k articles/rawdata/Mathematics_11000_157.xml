<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>ABACABA pattern</title>
    <ns>0</ns>
    <id>55464594</id>
    <revision>
      <id>870221561</id>
      <parentid>870220975</parentid>
      <timestamp>2018-11-23T08:32:11Z</timestamp>
      <contributor>
        <username>Hyacinth</username>
        <id>17171</id>
      </contributor>
      <comment>{{redirects|ABACABA||Sonata rondo form}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3566">{{merge to|Sesquipower|discuss=Talk:Sesquipower#Proposed merge with Abacaba pattern|date=October 2017}}
{{redirects|ABACABA||Sonata rondo form}}
[[File:Binary number dabacaba pattern.png|100px|thumb|DABACABA patterns in (3-[[bit]]) [[binary number]]s]]

The '''ABACABA pattern''' is a [[recursion|recursive]] [[fractal]] [[pattern]] that shows up in many places in the real world (such as in geometry, art, music, poetry, number systems, literature and higher dimensions).&lt;ref&gt;{{Cite news|url=http://digitaleditions.walsworthprintgroup.com/display_article.php?id=1300604|title=ABACABA Amazing Pattern, Amazing Connections|access-date=2017-10-06|language=en}}&lt;/ref&gt;&lt;ref&gt;{{Cite news|url=http://chicagogeekguy.com/exploring-abacaba/|title=Exploring Fractals with ABACABA – Chicago Geek Guy|date=2016-04-21|work=Chicago Geek Guy|access-date=2017-10-06|language=en-US}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=http://archive.bridgesmathart.org/2011/bridges2011-89.pdf|title=Abacaba! – Using a mathematical pattern to connect art, music, poetry and literature|last=Naylor|first=Mike|date=|website=|archive-url=|archive-date=|dead-url=|access-date=October 6, 2017}}&lt;/ref&gt;&lt;ref name=":0"&gt;{{Cite book|url=https://books.google.com/books?id=3SB60Wavy6MC&amp;pg=PA53&amp;lpg=PA53&amp;dq=Abacaba+pattern+-wikipedia.org&amp;source=bl&amp;ots=r-p9ejSugU&amp;sig=Anwy4PGFPj2JatdlBZJon9mK5t4&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjUgdWVhunWAhXplVQKHQf9CYYQ6AEIZDAM#v=onepage&amp;q=Abacaba%20pattern%20-wikipedia.org&amp;f=false|title=Magic Words: A Dictionary|last=Conley|first=Craig|date=2008-10-01|publisher=Weiser Books|isbn=9781609250508|language=en}}&lt;/ref&gt; Patterns often show a '''D'''ABACABA type subset.

== Generating the pattern ==
In order to [[iterated function system|generate]] the next sequence, first take the previous pattern, add the next letter from the alphabet, and then repeat the previous pattern. The first few steps are listed here.&lt;ref name=":0" /&gt;
{{clear}}
# &lt;center&gt;'''A'''&lt;/center&gt;
# &lt;center&gt;A'''B'''A&lt;/center&gt;
# &lt;center&gt;ABA'''C'''ABA&lt;/center&gt;
# &lt;center&gt;ABACABA'''D'''ABACABA&lt;/center&gt;
# &lt;center&gt;ABACABADABACABA'''E'''ABACABADABACABA&lt;/center&gt;
# &lt;center&gt;ABACABADABACABAEABACABADABACABA'''F'''ABACABADABACABAEABACABADABACABA&lt;/center&gt;

==Gallery==
{{Gallery
| title        = 
| align        =
| footer       =
| style        =
| state        =
| height       =
| width        =
| cellwidth    =
| captionstyle =
| File:ABACABA Sierpinski triangle.png
 | alt1=
 | [[Sierpinski triangle]]&lt;!--3rd iteration--&gt;:&lt;br/&gt; &lt;center&gt;ABA'''C'''ABA&lt;/center&gt;
| File:Measuring - Fractions of an inch.svg
 | alt2=
 | [[Ruler]], excluding 1 and 2:&lt;br/&gt; &lt;center&gt;ABACABA'''D'''ABACABA&lt;/center&gt;
| File:Cantor4 abacaba pattern.png
 | alt3=
 | [[Cantor set]]&lt;!--4th iteration--&gt;:&lt;br/&gt; &lt;center&gt;ABACABA'''D'''ABACABA&lt;/center&gt;
| File:Ex 001.png
 | alt4=
 | [[Binary tree]]/upside down [[family tree]]&lt;!--4th iteration--&gt;:&lt;br/&gt; &lt;center&gt;ABACABA'''D'''ABACABA&lt;/center&gt;
| File:KochCurve1 abacaba pattern.png
 | alt5=
 | [[Koch curve]]: &lt;math&gt;n=1&lt;/math&gt; is A'''B'''A, &lt;math&gt;n=2&lt;/math&gt; is ABA'''C'''ABA, and &lt;math&gt;n=3&lt;/math&gt;: ABACABA'''D'''ABACABA
| File:Solfege subdivision de la ronde a la croche.svg
 | alt6=
 | [[Metric hierarchy]]&lt;!--4th iteration--&gt;:&lt;br/&gt; &lt;center&gt;ABACABA'''D'''ABACABA&lt;/center&gt;
| File:Arrière du Château de Chambord (41).JPG
 | alt7=
 | &lt;center&gt;ABA'''C'''ABA&lt;ref&gt;Wright, Craig (2012). ''The Essential Listening to Music'', p.45. Cengage Learning. {{ISBN|9781111342029}}.&lt;/ref&gt;&lt;/center&gt;
}}

== See also ==
* [[Farey sequence]]
* [[Rondo]]
* [[Sesquipower]]

== References ==
{{Reflist}}

[[Category:Fractals]]


{{Math-stub}}</text>
      <sha1>meu8efntuz0iherqmlvqlydqvkqh90y</sha1>
    </revision>
  </page>
  <page>
    <title>Askey scheme</title>
    <ns>0</ns>
    <id>32649081</id>
    <revision>
      <id>797492811</id>
      <parentid>744269799</parentid>
      <timestamp>2017-08-27T11:19:54Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[Wikipedia:Bots/Requests for approval/KolbertBot|HTTP→HTTPS]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6391">In mathematics, the '''Askey scheme''' is a way of organizing [[orthogonal polynomials]] of hypergeometric or basic hypergeometric type into a hierarchy. For the classical orthogonal polynomials discussed in {{harvtxt|Andrews|Askey|1985}}, the Askey scheme  was first drawn by {{harvtxt|Labelle|1985}} and by {{harvs|txt|authorlink=Richard Askey|last=Askey|last2=Wilson|year=1985}}, and has since been extended by {{harvtxt|Koekoek|Swarttouw|1998}} and {{harvtxt|Koekoek|Lesky|Swarttouw|2010}} to cover basic orthogonal polynomials.

==Askey scheme for hypergeometric orthogonal polynomials==

{{harvtxt|Koekoek|Lesky|Swarttouw|2010|loc=p.183}} give the following version of the Askey scheme:

;&lt;sub&gt;4&lt;/sub&gt;F&lt;sub&gt;3&lt;/sub&gt;: [[Wilson polynomials|Wilson]] | [[Racah polynomials|Racah]]
;&lt;sub&gt;3&lt;/sub&gt;F&lt;sub&gt;2&lt;/sub&gt;: [[continuous dual Hahn polynomials|Continuous dual Hahn]] | [[continuous Hahn polynomials|Continuous Hahn]] | [[Hahn polynomials|Hahn]] | [[dual Hahn polynomials|dual Hahn]]
;&lt;sub&gt;2&lt;/sub&gt;F&lt;sub&gt;1&lt;/sub&gt;: [[Meixner–Pollaczek polynomials|Meixner–Pollaczek]] | [[Jacobi polynomials|Jacobi]] | [[Pseudo Jacobi polynomials|Pseudo Jacobi]] | [[Meixner polynomials|Meixner]] | [[Krawtchouk polynomials|Krawtchouk]]
;&lt;sub&gt;2&lt;/sub&gt;F&lt;sub&gt;0&lt;/sub&gt;/&lt;sub&gt;1&lt;/sub&gt;F&lt;sub&gt;1&lt;/sub&gt;: [[Laguerre polynomials|Laguerre]] | [[Bessel polynomials|Bessel]] | [[Charlier polynomials|Charlier]]
;&lt;sub&gt;1&lt;/sub&gt;F&lt;sub&gt;0&lt;/sub&gt;: [[Hermite polynomials|Hermite]]

==Askey scheme for basic hypergeometric orthogonal polynomials==

{{harvtxt|Koekoek|Lesky|Swarttouw|2010|loc=p.413}} give the following scheme for basic hypergeometric orthogonal polynomials:
 
;&lt;sub&gt;4&lt;/sub&gt;&lt;math&gt;\phi&lt;/math&gt;&lt;sub&gt;3&lt;/sub&gt;: [[Askey–Wilson polynomials|Askey–Wilson]] | [[q-Racah polynomials|q-Racah]]
;&lt;sub&gt;3&lt;/sub&gt;&lt;math&gt;\phi&lt;/math&gt;&lt;sub&gt;2&lt;/sub&gt;: [[continuous dual q-Hahn polynomials|Continuous dual q-Hahn]] | [[continuous q-Hahn polynomials|Continuous q-Hahn]] | [[Big q-Jacobi polynomials|Big q-Jacobi]] | [[q-Hahn polynomials|q-Hahn]] | [[dual q-Hahn polynomials|dual q-Hahn]]
;&lt;sub&gt;2&lt;/sub&gt;&lt;math&gt;\phi&lt;/math&gt;&lt;sub&gt;1&lt;/sub&gt;: [[Al-Salam–Chihara polynomials|Al-Salam–Chihara]] | [[q-Meixner–Pollaczek polynomials|q-Meixner–Pollaczek]] | [[Continuous q-Jacobi polynomials|Continuous q-Jacobi]] | [[Big q-Laguerre polynomials|Big q-Laguerre]] | [[Little q-Jacobi polynomials|Little q-Jacobi]] | [[q-Meixner polynomials|q-Meixner]] | [[Quantum q-Krawtchouk polynomials|Quantum q-Krawtchouk]] | [[q-Krawtchouk polynomials|q-Krawtchouk]] | [[Affine q-Krawtchouk polynomials|Affine q-Krawtchouk]] | [[Dual q-Krawtchouk polynomials|Dual q-Krawtchouk]]
;&lt;sub&gt;2&lt;/sub&gt;&lt;math&gt;\phi&lt;/math&gt;&lt;sub&gt;0&lt;/sub&gt;/&lt;sub&gt;1&lt;/sub&gt;&lt;math&gt;\phi&lt;/math&gt;&lt;sub&gt;1&lt;/sub&gt;: [[Continuous big q-Hermite polynomials|Continuous big q-Hermite]] | [[Continuous q-Laguerre polynomials|Continuous q-Laguerre]] | [[Little q-Laguerre polynomials|Little q-Laguerre]] | [[q-Laguerre polynomials|q-Laguerre]] | [[q-Bessel polynomials|q-Bessel]] | [[q-Charlier polynomials|q-Charlier]] | [[Al-Salam–Carlitz polynomials|Al-Salam–Carlitz I]] | [[Al-Salam–Carlitz polynomials|Al-Salam–Carlitz II]]
;&lt;sub&gt;1&lt;/sub&gt;&lt;math&gt;\phi&lt;/math&gt;&lt;sub&gt;0&lt;/sub&gt;: [[Continuous q-Hermite polynomials|Continuous q-Hermite]] | [[Stieltjes–Wigert polynomials|Stieltjes–Wigert]] | [[Discrete q-Hermite polynomials|Discrete q-Hermite I]] | [[Discrete q-Hermite polynomials|Discrete q-Hermite II]]

==References==

*{{Citation | last1=Andrews | first1=George E. | last2=Askey | first2=Richard | editor1-last=Brezinski | editor1-first=C. | editor2-last=Draux | editor2-first=A. | editor3-last=Magnus | editor3-first=Alphonse P. | editor4-last=Maroni | editor4-first=Pascal | editor5-last=Ronveaux | editor5-first=A. | title=Polynômes orthogonaux et applications.  Proceedings of the Laguerre symposium held at Bar-le-Duc, October 15–18, 1984.  | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Lecture Notes in Math. | isbn=978-3-540-16059-5  | doi=10.1007/BFb0076530 | mr=838970 | year=1985 | volume=1171 | chapter=Classical orthogonal polynomials | chapterurl=https://dx.doi.org/10.1007/BFb0076530 | pages=36–62}}
*{{Citation | last1=Askey | first1=Richard | last2=Wilson | first2=James | title=Some basic hypergeometric orthogonal polynomials that generalize Jacobi polynomials | url=https://books.google.com/books?id=9q9o03nD_xsC | isbn=978-0-8218-2321-7 | mr=783216 | year=1985 | journal=Memoirs of the American Mathematical Society | issn=0065-9266 | volume=54 | issue=319 | pages=iv+55 | doi=10.1090/memo/0319}}
*{{Citation | last1=Koekoek | first1=Roelof | last2=Swarttouw | first2=René F. | title=The Askey-scheme of hypergeometric orthogonal polynomials and its q-analogue|url = http://homepage.tudelft.nl/11r49/askey/|year=1998  |publisher=Delft University of Technology, Faculty of Information Technology and Systems, Department of Technical Mathematics and Informatics
|volume= 98-17}}
*{{Citation | last1=Koekoek | first1=Roelof | last2=Lesky | first2=Peter A. | last3=Swarttouw | first3=René F. | title=Hypergeometric orthogonal polynomials and their q-analogues | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Springer Monographs in Mathematics | isbn=978-3-642-05013-8 | doi=10.1007/978-3-642-05014-5 | mr=2656096 | year=2010}}
*{{Citation | last1=Koornwinder | first1=Tom H. | title=Orthogonal polynomials and their applications (Segovia, 1986) | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Lecture Notes in Math. | isbn=978-3-540-19489-7  | doi=10.1007/BFb0083353 | mr=973421 | year=1988 | volume=1329 | chapter=Group theoretic interpretations of Askey's scheme of hypergeometric orthogonal polynomials | pages=46–72}}
*{{Citation | last1=Labelle | first1=Jacques | editor1-last=Brezinski | editor1-first=C. | editor2-last=Draux | editor2-first=A. | editor3-last=Magnus | editor3-first=Alphonse P. | editor4-last=Maroni | editor4-first=Pascal | editor5-last=Ronveaux | editor5-first=A. | title=Polynômes Orthogonaux et Applications. Proceedings of the Laguerre Symposium held at Bar-le-Duc | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Lecture Notes in Math. | isbn=978-3-540-16059-5  | doi=10.1007/BFb0076527 | mr=838967 | year=1985 | volume=1171 | chapter=Tableau d'Askey | pages=xxxvi–xxxvii}}

[[Category:Orthogonal polynomials]]
[[Category:Hypergeometric functions]]
[[Category:Q-analogs]]</text>
      <sha1>eveo82ba66qttac7qvsbcwn20xj5z7u</sha1>
    </revision>
  </page>
  <page>
    <title>Assignment problem</title>
    <ns>0</ns>
    <id>140592</id>
    <revision>
      <id>849295415</id>
      <parentid>849295358</parentid>
      <timestamp>2018-07-08T00:25:21Z</timestamp>
      <contributor>
        <username>Headbomb</username>
        <id>1461430</id>
      </contributor>
      <comment>/* Further reading */ ce</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7632">{{more citations needed|date=August 2012}}
The '''assignment problem''' is one of the fundamental [[combinatorial optimization]] problems in the branch of [[Optimization (mathematics)|optimization]] or [[operations research]] in [[mathematics]]. It consists of finding a maximum weight [[Matching (graph theory)|matching]] (or minimum weight perfect matching) in a [[weighted graph|weighted]] [[bipartite graph]].

In its most general form, the problem is as follows:
:The problem instance has a number of ''agents'' and a number of ''tasks''. Any agent can be assigned to perform any task, incurring some ''cost'' that may vary depending on the agent-task assignment. It is required to perform all tasks by assigning exactly one agent to each task and exactly one task to each agent in such a way that the ''total cost'' of the assignment is minimized.

If the numbers of agents and tasks are equal and the total cost of the assignment for all tasks is equal to the sum of the costs for each agent (or the sum of the costs for each task, which is the same thing in this case), then the problem is called the ''linear assignment problem''. Commonly, when speaking of the ''assignment problem'' without any additional qualification, then the ''linear assignment problem'' is meant.

== Algorithms and generalizations ==
The [[Hungarian algorithm]] is one of many [[algorithm]]s that have been devised that solve the linear assignment problem within time bounded by a polynomial expression of the number of agents. Other algorithms include adaptations of the primal [[simplex algorithm]], and the [[auction algorithm]].

The assignment problem is a special case of the [[transportation problem]], which is a special case of the [[minimum cost flow problem]], which in turn is a special case of a [[linear program]].  While it is possible to solve any of these problems using the [[simplex algorithm]], each specialization has more efficient algorithms designed to take advantage of its special structure.

When a number of agents and tasks is very large, a [[parallel algorithm]] with randomization can be applied. The problem of finding minimum weight maximum matching can be converted to finding a minimum weight perfect matching. A [[bipartite graph]] can be extended to a complete bipartite graph by adding artificial edges with large weights. These weights should exceed the weights of all existing matchings to prevent appearance of artificial edges in the possible solution. As shown by {{harvtxt|Mulmuley|Vazirani|Vazirani|1987}}, the problem of minimum weight perfect matching is converted to finding minors in the [[adjacency matrix]] of a graph. Using the [[isolation lemma]], a minimum weight perfect matching in a graph can be found with probability at least ½. For a graph with n vertices, it requires &lt;math&gt; O(\log^2(n)) &lt;/math&gt; time.

==Example==
Suppose that a taxi firm has three taxis (the agents) available, and three customers (the tasks) wishing to be picked up as soon as possible. The firm prides itself on speedy pickups, so for each taxi the "cost" of picking up a particular customer will depend on the time taken for the taxi to reach the pickup point. The solution to the assignment problem will be whichever combination of taxis and customers results in the least total cost.

However, the assignment problem can be made rather more flexible than it first appears. In the above example, suppose that there are four taxis available, but still only three customers. Then a fourth dummy task can be invented, perhaps called "sitting still doing nothing", with a cost of 0 for the taxi assigned to it. The assignment problem can then be solved in the usual way and still give the best solution to the problem.

Similar adjustments can be done in order to allow more tasks than agents, tasks to which multiple agents must be assigned (for instance, a group of more customers than will fit in one taxi), or maximizing profit rather than minimizing cost.

==Formal mathematical definition==

The formal definition of the '''assignment problem''' (or '''linear assignment problem''') is

:Given two sets, ''A'' and ''T'', of equal size, together with a [[weight function]] ''C'' : ''A'' &amp;times; ''T'' &amp;rarr; '''[[real number|R]]'''. Find a [[bijection]] ''f'' : ''A'' &amp;rarr; ''T'' such that the [[Loss function|cost function]]:

::&lt;math&gt;\sum_{a\in A}C(a,f(a))&lt;/math&gt;
is minimized.

Usually the weight function is viewed as a square real-valued [[matrix (mathematics)|matrix]] ''C'', so that the cost function is written down as:

:&lt;math&gt;\sum_{a\in A}C_{a,f(a)}&lt;/math&gt;

The problem is "linear" because the cost function to be optimized as well as all the constraints contain only linear terms.

The problem can be expressed as a standard [[linear program]] with the objective function

:&lt;math&gt;\sum_{i\in A}\sum_{j\in T}C(i,j)x_{ij}&lt;/math&gt;

subject to the constraints

:&lt;math&gt;\sum_{j\in T}x_{ij}=1\text{ for }i\in A, \, &lt;/math&gt;

:&lt;math&gt;\sum_{i\in A}x_{ij}=1\text{ for }j\in T, \, &lt;/math&gt;

:&lt;math&gt;x_{ij}\ge 0\text{ for }i,j\in A,T. \, &lt;/math&gt;

The variable &lt;math&gt;x_{ij}&lt;/math&gt; represents the assignment of agent &lt;math&gt;i&lt;/math&gt; to task &lt;math&gt;j&lt;/math&gt;, taking value 1 if the assignment is done and 0 otherwise.  This formulation allows also fractional variable values, but there is always an optimal solution where the variables take integer values.  This is because the constraint matrix is [[Unimodular matrix#Total unimodularity|totally unimodular]].  The first constraint requires that every agent is assigned to exactly one task, and the second constraint requires that every task is assigned exactly one agent.

==See also==
*[[Auction algorithm]]
*[[Generalized assignment problem]]
*[[Linear bottleneck assignment problem]]
*[[Monge-Kantorovich transportation problem]], a more general formulation
*[[National Resident Matching Program]]
*[[Quadratic assignment problem]]
*[[Secretary problem]]
*[[Stable marriage problem]]
*[[Stable roommates problem]]
*[[Weapon target assignment problem]]

==References==
* {{Cite journal
| volume = 7
| issue = 1
| pages = 105–113
| last1 = Mulmuley | first1 = Ketan | authorlink1 = Ketan Mulmuley
| last2 = Vazirani | first2 = Umesh | authorlink2 = Umesh Vazirani
| last3 = Vazirani | first3 = Vijay | authorlink3 = Vijay Vazirani
| title = Matching is as easy as matrix inversion
| journal = Combinatorica
| year = 1987
| ref=harv
| url=http://www.springerlink.com/content/r4rw2x4l46476708/
| doi = 10.1007/BF02579206}}

== Further reading ==
*{{cite journal |last1=Munkres |first1=James |title=Algorithms for the Assignment and Transportation Problems |journal=Journal of the Society for Industrial and Applied Mathematics |date=1957 |volume=5 |issue=1 |pages=32–38 |jstor=2098689}}
*{{cite book | last=Brualdi | first=Richard A. | title=Combinatorial matrix classes | series=Encyclopedia of Mathematics and Its Applications | volume=108 | location=Cambridge | publisher=[[Cambridge University Press]] | year=2006 | isbn=0-521-86565-4 | zbl=1106.05001 }}
* {{cite book | authorlink = Rainer Burkard | first = Rainer | last = Burkard |author2=M. Dell'Amico|author3=S. Martello | year = 2012 | title = Assignment Problems (Revised reprint) | publisher = SIAM | isbn = 978-1-61197-222-1 }}
* {{cite book | authorlink = Dimitri Bertsekas | first = Dimitri | last = Bertsekas | year = 1998 | title = Network Optimization: Continuous and Discrete Models | publisher = Athena Scientific | isbn = 1-886529-02-7 }}

[[Category:Combinatorial optimization]]
[[Category:Matching]]
[[Category:Polynomial-time problems]]
[[Category:Linear programming]]

[[de:Zuordnungsproblem]]</text>
      <sha1>51034z0a98mdz6vw82jfr867id7vxm7</sha1>
    </revision>
  </page>
  <page>
    <title>Bird's-eye view</title>
    <ns>0</ns>
    <id>2872234</id>
    <revision>
      <id>860934995</id>
      <parentid>860795043</parentid>
      <timestamp>2018-09-24T02:10:56Z</timestamp>
      <contributor>
        <username>Smasongarrison</username>
        <id>16185737</id>
      </contributor>
      <comment>/* top */copy edit with [[Wikipedia:AutoWikiBrowser/General_fixes|General fixes]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5569">{{Other uses}}
{{Redirect|Aerial view|the album by Blackmail|Aerial View}}
{{Redirect|Overhead view|the video game perspective|Top-down perspective}}
{{More citations needed|date=March 2017}}
{{Views}}
A '''bird's-eye view''' is an elevated view of an object from above, with a [[Perspective (graphical)|perspective]] as though the [[observation|observer]] were a [[bird]], often used in the making of [[blueprints]], [[floor plan]]s, and [[map]]s.

It can be an [[aerial photography|aerial photograph]], but also a drawing. Before manned flight was common, the term "bird's eye" was used to distinguish views drawn from direct observation at high locations (for example a mountain or tower), from those constructed from an imagined (bird's) perspectives. Bird's eye views as a genre have existed since classical times. The last great flourishing of them was in the mid-to-late 19th century, when bird's eye view prints were popular in the United States and Europe.

== Terminology ==
The terms '''aerial view''' and '''aerial viewpoint''' are also sometimes used synonymous with bird's-eye view. The term ''aerial view'' can refer to any view from a great height, even at a wide angle, as for example when looking sideways from an airplane window or from a mountain top. '''Overhead view''' is fairly synonymous with ''bird's-eye view'' but tends to imply a less lofty vantage point than the latter term. For example, in computer and video games, an "overhead view" of a character or situation often places the vantage point only a few feet (a meter or two) above human height. See [[top-down perspective]].

Recent technological and networking developments have made [[satellite image]]s more accessible. Microsoft [[Bing Maps]] offers direct overhead satellite photos of the entire planet but also offers a feature named [[Bing Maps#Bird's-eye view|Bird's eye view]] in some locations. The ''Bird's Eye'' photos are angled at 40 degrees rather than being straight down. Satellite imaging programs and photos have been described as offering a viewer the opportunity to "fly over" and observe the world from this specific angle.

In [[filmmaking]] and [[video production]], a '''bird's-eye shot''' refers to a shot looking directly down on the subject. The perspective is very foreshortened, making the subject appear short and squat. This shot can be used to give an overall [[establishing shot]] of a scene, or to emphasise the smallness or insignificance of the subjects. These shots are normally used for battle scenes or establishing where the character is. It is shot by lifting the camera up by hands or by hanging it off something strong enough to support it. When a scene needs a large area shot, it is a [[crane shot]].

== Gallery ==
&lt;gallery mode="packed" heights="160"&gt;
Paris moderne. Les Tuileries, le Louvre, et la rue de Rivoli, vue prise du Jardin des Tuileries.jpg|Bird's-eye view drawing of [[Paris]] in 1850
File:Baku-London flight.jpg|A view of central London
File:Flying above the ALMA Site.jpg|Flying above the [[ESO]]'s [[Atacama Large Millimeter Array]] site.
File:A-Y- P Exposition bird's eye view postcard.jpg|Bird's Eye View drawing of the Alaska-Yukon-Pacific Exposition 1909
File:Disneyland Anaheim.jpg|Aerial view of Disneyland in 2004
File:Janneke Viegers, Schiphol Airport.jpg|Painting of Schiphol Airport by the Dutch artist Janneke Viegers
&lt;/gallery&gt;

== Bird's-flight view ==
[[File:Copperplate map Moorfields.jpg|thumb|300px|Part of the [[Copperplate map of London|"Copperplate" map of London]], surveyed between 1553 and 1559, depicting a bird's-flight view of the [[Moorfields]] area]]
A distinction is sometimes drawn between a bird's-eye view and a '''bird's-flight view''', or "view-plan in [[Isometric projection|isometrical projection]]".&lt;ref&gt;{{cite book |first=Herbert |last=Hurst |title=Oxford Topography: an essay |chapter=Introduction |pages=1–12 (4–5) |place=Oxford |series=[[Oxford Historical Society]] |publisher=Oxford Historical Society |volume=39 |year=1899 |url=https://archive.org/stream/oxfordtopography00hursrich#page/4/mode/2up }}&lt;/ref&gt; Whereas a bird's-eye view shows a scene from a single viewpoint (real or imagined) in true [[Perspective (graphical)|perspective]], including, for example, the [[Perspective (graphical)#Foreshortening|foreshortening]] of more distant features, a bird's-flight view combines a vertical plan of ground-level features with perspective views of buildings and other standing features, all presented at roughly the same scale.&lt;ref&gt;{{cite journal |first=William |last=Ravenhill |title=Bird's-eye view &amp; bird's-flight view |journal=The Map Collector |volume=35 |year=1986 |pages=36–7 }}&lt;/ref&gt; The landscape appears "as it would unfold itself to any one passing over it, as in a balloon, at a height sufficient to abolish sharpness of perspective, and yet low enough to allow of distinct view of the scene beneath".&lt;ref&gt;Hurst 1899, p. 4.&lt;/ref&gt; The technique was popular among local surveyors and cartographers of the sixteenth and early seventeenth centuries.

==See also==
{{wiktionary}}
{{commons category}}
* [[Aerial landscape art]]
* [[Aerial perspective (disambiguation)]]
* [[Aerial photography]]
* [[Bird's-eye view map]]
* [[Camera angle]]
* [[Cinematic techniques]]
* [[Filmmaking]]
* [[Google Earth]]
* [[Pictometry]]
* [[Plans (drawings)]]
* [[Top-down perspective]]
* [[Video production]]
* [[Worm's-eye view]]

== References ==
{{reflist}}

{{Cinematic techniques}}

[[Category:Technical drawing]]
[[Category:Cartography]]
[[Category:Methods of representation]]</text>
      <sha1>tew1pq7c58s4s55l5baxolnncg3702n</sha1>
    </revision>
  </page>
  <page>
    <title>Category of modules</title>
    <ns>0</ns>
    <id>11231812</id>
    <revision>
      <id>828550466</id>
      <parentid>811946477</parentid>
      <timestamp>2018-03-03T07:38:35Z</timestamp>
      <contributor>
        <username>Cadairidris</username>
        <id>31958920</id>
      </contributor>
      <comment>/* Generalizations */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3443">In algebra, given a [[ring (mathematics)|ring]] ''R'', the '''category of left modules''' over ''R'' is the category whose objects are all left [[module (mathematics)|modules]] over ''R'' and whose morphisms are all [[module homomorphism]]s between left ''R''-modules. For example, when ''R'' is the ring of integers '''Z''', it is the same thing as the [[category of abelian groups]]. The '''category of right modules''' is defined in a similar way.

'''Note:''' Some authors use the term '''[[module category]]''' for the category of modules; this term can be ambiguous since it could also refer to a category with a [[monoidal-category action]].&lt;ref&gt;{{cite web|url=http://ncatlab.org/nlab/show/module+category|title=module category in nLab|work=ncatlab.org}}&lt;/ref&gt;

== Properties ==
The category of left modules (or that of right modules) is an [[abelian category]]. The category has [[enough projectives]]&lt;ref&gt;trivially since any module is a quotient of a free module.&lt;/ref&gt; and [[enough injectives]].&lt;ref&gt;{{harvnb|Dummit–Foote|loc=Ch. 10, Theorem 38.}}&lt;/ref&gt; [[Mitchell's embedding theorem]] states every abelian category arises as a full subcategory of the category of modules.

[[Projective limit]]s and [[inductive limit]]s exist in the category of (say left) modules.&lt;ref&gt;{{harvnb|Bourbaki|loc=§ 6.}}&lt;/ref&gt;

Over a commutative ring, together with the [[tensor product of modules]] ⊗, the category of modules is a [[symmetric monoidal category]].

== Example: the category of vector spaces ==
The [[category (mathematics)|category]] '''K-Vect''' (some authors use '''Vect'''&lt;sub&gt;''K''&lt;/sub&gt;) has all [[vector space]]s over a fixed [[Field (mathematics)|field]] ''K'' as [[object (category theory)|objects]] and [[linear transformation|''K''-linear transformations]] as [[morphism]]s. Since vector spaces over ''K'' (as a field) are the same thing as [[module (algebra)|module]]s over the [[ring (mathematics)|ring]] ''K'', '''K-Vect''' is a special case of '''R-Mod''', the category of left ''R''-modules.

Much of [[linear algebra]] concerns the description of '''K-Vect'''. For example, the [[dimension theorem for vector spaces]] says that the [[isomorphism class]]es in '''K-Vect''' correspond exactly to the [[cardinal number]]s, and that '''K-Vect''' is [[equivalence of categories|equivalent]] to the [[subcategory]] of '''K-Vect''' which has as its objects the free vector spaces ''K''&lt;sup&gt;''n''&lt;/sup&gt;, where ''n'' is any cardinal number.

== Generalizations ==
The category of [[sheaves of modules]] over a [[ringed space]] also has enough injectives (though not always enough projectives).

== See also ==
*[[Algebraic K-theory]] (the important invariant of the category of modules.)
*[[Category of rings]]
*[[Derived category]]
*[[Module spectrum]]
* [[Category of graded vector spaces]]
* [[Category of abelian groups]]
* [[category of representations]]

== References ==
{{reflist}}
*Bourbaki, ''Algèbre''; "Algèbre linéaire."
*Dummit, David; Foote, Richard. ''Abstract Algebra''.
*{{cite book |first=Saunders |last=Mac Lane |authorlink=Saunders Mac Lane|title=Categories for the Working Mathematician | edition=second |date=September 1998 |publisher=Springer |isbn=0-387-98403-8 | zbl=0906.18001 | volume=5 | series=[[Graduate Texts in Mathematics]] }}

== External links ==
*http://ncatlab.org/nlab/show/Mod

[[Category:Categories in category theory|Vector spaces]]
[[Category:Linear algebra]]


{{algebra-stub}}</text>
      <sha1>diroawkfnaflqgnrbo6qqx4v8l53s5i</sha1>
    </revision>
  </page>
  <page>
    <title>Chainstore paradox</title>
    <ns>0</ns>
    <id>2610129</id>
    <revision>
      <id>860418390</id>
      <parentid>860418341</parentid>
      <timestamp>2018-09-20T15:11:23Z</timestamp>
      <contributor>
        <username>Volunteer1234</username>
        <id>30845620</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9140">The '''chainstore paradox''' is an apparent game theory paradox involving the chain store game, where a "deterrence strategy" appears optimal instead of the [[backward induction]] strategy of standard [[game theory]] reasoning.

==The chain store game==
A [[monopolist]] (Player A) has branches in 20 towns. He faces 20 potential competitors, one in each town, who will be able to choose {{small caps|in}} or {{small caps|out}}. They do so in sequential order and one at a time. If a potential competitor chooses {{small caps|out}}, he receives a payoff of 1, while A receives a payoff of 5. If he chooses {{small caps|in}}, he will receive a payoff of either 2 or 0, depending on the response of Player A to his action. Player A, in response to a choice of {{small caps|in}}, must choose one of two pricing strategies, {{small caps|cooperative}} or {{small caps|aggressive}}. If he chooses {{small caps|cooperative}}, both player A and the competitor receive a payoff of 2, and if A chooses {{small caps|aggressive}}, each player receives a payoff of 0.

These outcomes lead to two theories for the game, the induction (game theoretically correct version) and the deterrence theory (weakly dominated theory):

===Induction theory===

Consider the decision to be made by the 20th and final competitor, of whether to choose {{small caps|in}} or {{small caps|out}}. He knows that if he chooses {{small caps|in}}, Player A receives a higher payoff from choosing cooperate than aggressive, and being the last period of the game, there are no longer any future competitors whom Player A needs to intimidate from the market. Knowing this, the 20th competitor enters the market, and Player A will cooperate (receiving a payoff of 2 instead of 0).

The outcome in the final period is set in stone, so to speak. Now consider period 19, and the potential competitor's decision. He knows that A will cooperate in the next period, regardless of what happens in period 19. Thus, if player 19 enters, an aggressive strategy will be unable to deter player 20 from entering. Player 19 knows this and chooses {{small caps|in}}. Player A chooses {{small caps|cooperate}}.

Of course, this process of [[backward induction]] holds all the way back to the first competitor. Each potential competitor chooses {{small caps|in}}, and Player A always cooperates. A receives a payoff of 40 (2×20) and each competitor receives 2.

===Deterrence theory===
This theory states that Player A will be able to get payoff of higher than 40. Suppose Player A finds the induction argument convincing. He will decide how many periods at the end to play such a strategy, say 3. In periods 1–17, he will decide to always be aggressive against the choice of IN. If all of the potential competitors know this, it is unlikely potential competitors 1–17 will bother the [[chain store]], thus risking the safe payout of 1 ("A" will not retaliate if they choose "{{small caps|out}}"). If a few do test the chain store early in the game, and see that they are greeted with the aggressive strategy, the rest of the competitors are likely not to test any further. Assuming all 17 are deterred, Player A receives 91 (17×5 + 2×3). Even if as many as 10 competitors enter and test Player A's will, Player A will still receive a payoff of 41 (10×0+ 7×5 + 3×2), which is better than the induction (game theoretically correct) payoff.

===The chain store paradox===
If Player A follows the game theory payoff matrix to achieve the optimal payoff, they will have a lower payoff than with the "deterrence" strategy. This creates an apparent game theory paradox: game theory states that induction strategy should be optimal, but it looks like "deterrence strategy" is optimal instead.

The "deterrence strategy" is not a [[Subgame perfect equilibrium]]: It relies on the [[non-credible threat]] of responding to {{small caps|in}} with {{small caps|aggressive}}.  A rational player will not carry out a non-credible threat, but the paradox is that it nevertheless seems to benefit Player A to carry out the threat.

===Selten's response===
[[Reinhard Selten]]'s response to this apparent [[paradox]] is to argue that the idea of "deterrence", while [[irrational]] by the standards of [[Game Theory]], is in fact an acceptable idea by the rationality that individuals actually employ. Selten argues that individuals can make decisions of three levels: Routine, Imagination, and Reasoning.

===Complete information?===
Game theory is based on the idea that each matrix is modeled with the assumption of [[complete information]]: that "every player knows the payoffs and strategies available to other players," where the word "payoff" is descriptive of behavior—what the player is trying to maximize.  If, in the first town, the competitor enters and the monopolist is aggressive, the second competitor has observed that the monopolist is not, from the standpoint of common knowledge of payoffs and strategies, maximizing the assumed payoffs; expecting the monopolist to do so in this town seems dubious.

If competitors place even a very small probability on the possibility that the monopolist is spiteful, and places intrinsic value on being (or appearing) aggressive, and the monopolist knows this, then even if the monopolist has payoffs as described above, responding to entry in an early town with aggression will be optimal if it increases the probability that later competitors place on the monopolist's being spiteful.

==Selten's levels of decision making==
===The routine level===
The individuals use their past experience of the results of decisions to guide their response to choices in the present. "The underlying criteria of similarity between decision situations are crude and sometimes inadequate". (Selten)

===The imagination level===
The individual tries to visualize how the selection of different alternatives may influence the probable course of future events. This level employs the routine level within the procedural decisions. This method is similar to a computer simulation.

===The reasoning level===
The individual makes a conscious effort to analyze the situation in a rational way, using both past experience and logical thinking. This mode of decision uses simplified models whose assumptions are products of imagination, and is the only method of reasoning permitted and expected by game theory.

==Decision-making process==
===The predecision===
One chooses which method (routine, imagination or reasoning) to use for the problem, and this decision itself is made on the routine level.

===The final decision===
Depending on which level is selected, the individual begins the decision procedure. The individual then arrives at a (possibly different) decision for each level available (if we have chosen imagination, we would arrive at a routine decision and possible and imagination decision). Selten argues that individuals can always reach a routine decision, but perhaps not the higher levels. Once the individuals have all their levels of decision, they can decide which answer to use...the Final Decision. The final decision is made on the routine level and governs actual behavior.

==The economy of decision effort==
Decision effort is a scarce commodity, being both time consuming and mentally taxing. Reasoning is more costly than Imagination, which, in turn is more costly than Routine. The highest level activated is not always the most accurate since the individual may be able to reach a good decision on the routine level, but makes serious computational mistakes on higher levels, especially Reasoning.

Selten finally argues that strategic decisions, like those made by the monopolist in the chainstore paradox, are generally made on the level of Imagination, where deterrence is a reality, due to the complexity of Reasoning, and the great inferiority of Routine (it does not allow the individual to see herself in the other player's position). Since Imagination cannot be used to visualize more than a few stages of an extensive form game (like the Chain-store game) individuals break down games into "the beginning" and "towards the end". Here, deterrence is a reality, since it is reasonable "in the beginning", yet is not convincing "towards the end".

==See also==
*[[Repeated game]]
*[[Traveler's dilemma]]
*[[Unexpected hanging paradox]]

==References==
* {{cite book |first=Peter C. |last=Ordeshook |authorlink=Peter Ordeshook |chapter=Reputation and the Chain-Store Paradox |title=A Political Theory Primer |location= |publisher=Routledge |year=1992 |isbn=0-415-90241-X |pages=247–249 |chapterurl=https://books.google.com/books?id=VkRHzeIJTz4C&amp;pg=PA247 }}
* {{cite journal |last=Selten |first=Reinhard |year=1978 |authorlink=Reinhard Selten |title=The chain store paradox |journal=[[Theory and Decision]] |volume=9 |issn=0040-5833 |issue= 2 |pages=127–159 |doi=10.1007/BF00131770 }}

==Further reading==
*[http://jtp.sagepub.com/cgi/content/abstract/11/1/5 Relation of Chain Store Paradox to Constitutional Politics in Canada]

{{Decision theory paradoxes}}

[[Category:Game theory]]
[[Category:Decision-making paradoxes]]</text>
      <sha1>0o27dawld63uaqv2yz5ic6zjbgc3kzu</sha1>
    </revision>
  </page>
  <page>
    <title>Clobber</title>
    <ns>0</ns>
    <id>5143496</id>
    <revision>
      <id>859930538</id>
      <parentid>783701980</parentid>
      <timestamp>2018-09-17T06:43:06Z</timestamp>
      <contributor>
        <username>Jabberjaw</username>
        <id>9895903</id>
      </contributor>
      <comment>clean up</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2633">{{other uses}}
{{more footnotes|date=December 2012}}
{{Chess diagram 5x6
| tright
| 
|oo|xo|oo|xo|oo
|xo|oo|xo|oo|xo
|oo|xo|oo|xo|oo
|xo|oo|xo|oo|xo
|oo|xo|oo|xo|oo
|xo|oo|xo|oo|xo
| Starting position for 5×6 Clobber
}}

'''Clobber''' is an [[abstract strategy game]] invented in 2001 by [[combinatorial game theory|combinatorial game theorists]] [[Michael H. Albert]], J.P. Grossman and Richard Nowakowski. It has subsequently been studied by [[Elwyn Berlekamp]] and [[Erik Demaine]] among others. Since 2005, it has been one of the events in the [[Computer Olympiad]].

==Details==
Clobber is best played with two players and takes an average of 15 minutes to play. It is suggested for ages 8 and up. It is typically played on a rectangular white and black checkerboard. Players take turns to move one of their own pieces onto an orthogonally adjacent opposing piece, removing it from the game. The winner of the game is the player who makes the last move (i.e. whose opponent cannot move).

To start the game, each of the squares on the checkerboard is occupied by a stone. White stones are placed on the white squares and black stones on the black squares. To move, the player must pick up one of his or her own stones and "clobber" an opponent's stone on an adjacent square, either horizontally or vertically. Once the opponent's stone is clobbered, it must then be removed from the board and replaced by the stone that was moved. The player who, on their turn, is unable to move, loses the game.&lt;ref name=Clobber&gt;{{cite web|title=Clobber {{!}} Board Game |url=http://boardgamegeek.com/boardgame/23864/clobber|publisher=Board Game Geek|accessdate=12 February 2013}}&lt;/ref&gt;

==Variants==
In computational play (e.g., [[Computer Olympiad]]), clobber is generally played on a 10x10 board. There are also variations in the initial layout of the pieces. Another variant is ''Cannibal Clobber'', where a stone may not only capture stones of the opponent but also other stones of its owner. An advantage of Cannibal Clobber over Clobber is that a player may not only win, but win by a non-trivial margin. Cannibal Clobber was proposed in the summer of 2003 by Ingo Althoefer.

==References==
{{Reflist}}

==External links==
* ''[https://www.sciencenews.org/article/getting-clobbered Getting Clobbered]'' article at ''[[Science News]]''
* {{bgg|23864|Clobber}}
* ''[http://www.althofer.de/clobber-2011.html Clobber in the Olympiad]'' Pictorial report on the Clobber competition in Tilburg 2011

[[Category:Board games introduced in 2001]]
[[Category:Combinatorial game theory]]
[[Category:Abstract strategy games]]


{{board-game-stub}}</text>
      <sha1>qysowdrxiq4t4leg8k5pu6kj2osymx8</sha1>
    </revision>
  </page>
  <page>
    <title>Computer number format</title>
    <ns>0</ns>
    <id>48662</id>
    <revision>
      <id>837258426</id>
      <parentid>837211389</parentid>
      <timestamp>2018-04-19T18:02:57Z</timestamp>
      <contributor>
        <username>Wtshymanski</username>
        <id>139104</id>
      </contributor>
      <comment>rv v Undid revision 837211389 by [[Special:Contributions/223.176.143.114|223.176.143.114]] ([[User talk:223.176.143.114|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17447">{{multiple issues|
{{Cleanup-rewrite|date=February 2013
}}
{{Refimprove|date=February 2010}}
}}

A '''computer number format''' is the internal representation of numeric values in digital [[computer]] and [[calculator]] hardware and software.&lt;ref&gt;
{{cite book
 | title = Inside the machine: an illustrated introduction to microprocessors and computer architecture
 | author = Jon Stokes
 | publisher = No Starch Press
 | year = 2007
 | isbn = 978-1-59327-104-6
 | page = 66
 | url = https://books.google.com/books?id=Q1zSIarI8xoC&amp;pg=PA66
 }}&lt;/ref&gt;  Normally, numeric values are stored as groupings of [[bit]]s, named for the number of bits that compose them. The encoding between numerical values and bit patterns is chosen for convenience of the operation of the computer; the bit format used by the computer's instruction set generally requires conversion for external use such as printing and display. Different types of processors may have different internal representations of numerical values.  Different conventions are used for integer and real numbers. Most calculations are carried out with number formats that fit into a processor register, but some software systems allow representation of arbitrarily large numbers using multiple words of memory.

==Binary number representation==
{{See also|Integer (computer science)}}
Computers represent data in sets of binary digits.  The representation is composed of bits, which in turn are grouped into larger sets such as bytes.

{| class="wikitable" style="float: right; clear:right; margin-left:1em"
|+ Table 1: Binary to Octal
|-
! Binary String !! Octal value
|-
| 000 || 0
|-
| 001 || 1
|-
| 010 || 2
|-
| 011 || 3
|-
| 100 || 4
|-
| 101 || 5
|-
| 110 || 6
|-
| 111 || 7
|}

{| class="wikitable" style="float: right; clear:right; margin-left:1em"
|+ Table 2: Number of values for a bit string.
|-
! Length of Bit String (b) !! Number of Possible Values (N)
|-
| 1 || 2
|-
| 2 || 4
|-
| 3 || 8
|-
| 4 || 16
|-
| 5 || 32
|-
| 6 || 64
|-
| 7 || 128
|-
| 8 || 256
|-
| 9 || 512
|-
| 10 || 1024
|-
| ... || 
|-
| &lt;math&gt;b&lt;/math&gt; || &lt;math&gt;2^b=N&lt;/math&gt;
|}

A ''[[bit]]'' is a [[Binary numeral system|binary]] [[Numerical digit|digit]] that represents one of two [[state (computer science)|states]].  The concept of a bit can be understood as a value of either ''1'' or ''0'', ''on'' or ''off'', ''yes'' or ''no'', ''true'' or ''false'', or [[encoding|encoded]] by a switch or [[Toggle switch|toggle]] of some kind.

While a single bit, on its own, is able to represent only two values, a [[Bit string|string of bits]] may be used to represent larger values.  For example, a string of three bits can represent up to eight distinct values as illustrated in Table 1.

As the number of bits composing a string increases, the number of possible ''0'' and ''1'' combinations increases [[Exponentiation|exponentially]].  While a single bit allows only two value-combinations and two bits combined can make four separate values and so on.  The amount of possible combinations doubles with each binary digit added as illustrated in Table 2.

Groupings with a specific number of bits are used to represent varying things and have specific names.

A ''[[byte]]'' is a bit string containing the number of bits needed to represent a [[Character (computing)|character]].  On most modern computers, this is an eight bit string.  Because the definition of a byte is related to the number of bits composing a character, some older computers have used a different bit length for their byte.&lt;ref&gt;{{cite web|title=byte definition|url=http://catb.org/~esr/jargon/html/B/byte.html|accessdate=24 April 2012}}&lt;/ref&gt;  In many [[Computer Architecture|computer architectures]], the byte is used to [[Byte addressing|address]]&lt;!-- Find an external source for this --&gt; specific areas of memory.  For example, even though 64-bit processors may address memory sixty-four bits at a time, they may still split that memory into eight-bit pieces.  This is called byte-addressable memory.  Historically, many [[CPU]]s read data in some multiple of eight bits.&lt;ref&gt;{{cite web|title=Microprocessor and CPU (Central Processing Unit)|url=http://www.networkdictionary.com/hardware/mc.php|publisher=Network Dictionary|accessdate=1 May 2012}}&lt;/ref&gt;    Because the byte size of eight bits is so common, but the definition is not standardized, the term [[Octet (computing)|octet]] is sometimes used to explicitly describe an eight bit sequence.

A ''[[nibble]]'' (sometimes ''nybble''), is a number composed of four bits.&lt;ref&gt;{{cite web|title=nybble definition|url=http://catb.org/~esr/jargon/html/N/nybble.html|accessdate=3 May 2012}}&lt;/ref&gt;  Being a [[half-byte]], the nibble was named as a play on words.  A person may need several nibbles for one bite from something; similarly, a nybble is a part of a byte.  Because four bits allow for sixteen values, a nibble is sometimes known as a [[hexadecimal digit]].&lt;ref&gt;{{cite web|title=Nybble|url=http://www.techterms.com/definition/nybble|publisher=TechTerms.com|accessdate=3 May 2012}}&lt;/ref&gt;&lt;!-- A single digit in [[Binary_coded_decimal|binary-coded decimal]] can be stored in a nibble. --&gt;

==Octal and hex number display==
{{See also|Base64}}
Octal and hex are convenient ways to represent binary numbers, as used by computers. Computer engineers often need to write out binary quantities, but in practice writing out a binary number such as 1001001101010001 is tedious and prone to errors. Therefore, binary quantities are written in a base-8, or "octal", or, much more commonly, a base-16, "hexadecimal" or "hex", number format.  In the decimal system, there are 10 digits, 0 through 9, which combine to form numbers. In an octal system, there are only 8 digits, 0 through 7.  That is, the value of an octal "10" is the same as a decimal "8", an octal "20" is a decimal "16", and so on.   In a hexadecimal system, there are 16 digits, 0 through 9 followed, by convention, with A through F.  That is, a hex "10" is the same as a decimal "16" and a hex "20" is the same as a decimal "32".  An example and comparison of numbers in different bases is described in the chart below.

When typing numbers, formatting characters are used to describe the number system, for example 000_0000B or 0b000_00000 for binary and 0F8H or 0xf8 for hexadecimal numbers.
  
===Converting between bases===

{| class="wikitable" style="float: right; clear:right; margin-left:1em"
|+ Table 3: Comparison of Values in Different Bases
|-
! Decimal Value !! Binary Value !! Octal Value !! Hexadecimal Value
|-
| 0 || 000000 || 00 || 00
|-
| 1 || 000001 || 01 || 01
|-
| 2 || 000010 || 02 || 02
|-
| 3 || 000011 || 03 || 03
|-
| 4 || 000100 || 04 || 04
|-
| 5 || 000101 || 05 || 05
|-
| 6 || 000110 || 06 || 06
|-
| 7 || 000111 || 07 || 07
|-
| 8 || 001000 || 10 || 08
|-
| 9 || 001001 || 11 || 09
|-
| 10 || 001010 || 12 || 0A
|-
| 11 || 001011 || 13 || 0B
|-
| 12 || 001100 || 14 || 0C
|-
| 13 || 001101 || 15 || 0D
|-
| 14 || 001110 || 16 || 0E
|-
| 15 || 001111 || 17 || 0F
|}

{{Main | Positional_notation#Base_conversion| l1=Positional notation (Base conversion) }}

Each of these number systems are positional systems, but while decimal weights are powers of 10, the octal weights are powers of 8 and the hex weights are powers of 16. To convert from hex or octal to decimal, for each digit one multiplies the value of the digit by the value of its position and then adds the results. For example:

&lt;math&gt;\text{octal } 756&lt;/math&gt;

&lt;math&gt;= (7 * 8^2) + (5 * 8^1) + (6 * 8^0)&lt;/math&gt;

&lt;math&gt;= (7 * 64) + (5 * 8) + (6 * 1)&lt;/math&gt;

&lt;math&gt;= 448 + 40 + 6&lt;/math&gt;

&lt;math&gt;= \text{decimal } 494&lt;/math&gt;

&lt;math&gt;\text{hex } 3b2&lt;/math&gt;

&lt;math&gt;= (3 * 16^2) + (11 * 16^1) + (2 * 16^0)&lt;/math&gt;

&lt;math&gt;= (3 * 256) + (11 * 16) + (2 * 1)&lt;/math&gt;

&lt;math&gt;= 768 + 176 + 2&lt;/math&gt;

&lt;math&gt;= \text{decimal } 946&lt;/math&gt;

==Representing fractions in binary==

===Fixed-point numbers===

[[Fixed-point arithmetic|Fixed-point]] formatting can be useful to represent fractions in binary.

The number of bits needed for the precision and range desired must be chosen to store the fractional and integer parts of a number. For instance, using a 32-bit format, 16 bits may be used for the integer and 16 for the fraction.

The eight's bit is followed by the four's bit, then the two's bit, then the one's bit.  The fractional bits continue the pattern set by the integer bits.  The next bit is the half's bit, then the quarter's bit, then the ⅛'s bit, and so on.  For example:
{| class="toccolours"
|- style="text-align:center"
|       || ||  || || integer bits   || fractional bits
|-
| 0.500 ||=||  {{frac|1|2}}||=||colspan=2| 00000000 00000000.10000000 00000000
|-
| 1.250 ||=||{{frac|1|1|4}}||=||colspan=2| 00000000 00000001.01000000 00000000
|-
| 7.375 ||=||{{frac|7|3|8}}||=||colspan=2| 00000000 00000111.01100000 00000000
|}
This form of encoding cannot represent some values in binary. For example, the fraction &lt;math&gt;\tfrac{1}{5}&lt;/math&gt;, 0.2 in decimal, the closest approximations would be as follows:
{| class="toccolours"
|-
|  13107 / 65536 ||=|| 00000000 00000000.00110011 00110011 ||=|| 0.1999969... in decimal
|-
|  13108 / 65536 ||=|| 00000000 00000000.00110011 00110100 ||=|| 0.2000122... in decimal
|}

Even if more digits are used, an exact representation is impossible. The number &lt;math&gt;\tfrac{1}{3}&lt;/math&gt;, written in decimal as 0.333333333..., continues indefinitely. If prematurely terminated, the value would not represent &lt;math&gt;\tfrac{1}{3}&lt;/math&gt; precisely.

===Floating-point numbers===

While both unsigned and signed integers are used in digital systems, even a 32-bit integer is not enough to handle all the range of numbers a calculator can handle, and that's not even including fractions. To approximate the greater range and precision of [[real number]]s, we have to abandon signed integers and fixed-point numbers and go to a "[[floating point|floating-point]]" format.

In the decimal system, we are familiar with floating-point numbers of the form ([[scientific notation]]):

: 1.1030402 &amp;times; 10&lt;sup&gt;5&lt;/sup&gt; = 1.1030402 &amp;times; 100000 = 110304.02

or, more compactly:

: 1.1030402E5

which means "1.1030402 times 1 followed by 5 zeroes". We have a certain numeric value (1.1030402) known as a "[[significand]]", multiplied by a power of 10 (E5, meaning 10&lt;sup&gt;5&lt;/sup&gt; or 100,000), known as an "[[exponentiation|exponent]]". 
If we have a negative exponent, that means the number is multiplied by a 1 that many places to the right of the decimal point. For example:

: 2.3434E-6 = 2.3434 &amp;times; 10&lt;sup&gt;−6&lt;/sup&gt; = 2.3434 &amp;times; 0.000001 = 0.0000023434

The advantage of this scheme is that by using the exponent we can get a much wider range of numbers, even if the number of digits in the significand, or the "numeric precision", is much smaller than the range. 
Similar binary floating-point formats can be defined for computers. There is a number of such schemes, the most popular has been defined by [[Institute of Electrical and Electronics Engineers]] (IEEE). The [[IEEE floating point|IEEE 754-2008]] standard specification defines a 64 bit floating-point format with:

* an 11-bit binary exponent, using "excess-1023" format. Excess-1023 means the exponent appears as an unsigned binary integer from 0 to 2047; subtracting 1023 gives the actual signed value
* a 52-bit significand, also an unsigned binary number, defining a fractional value with a leading implied "1"
* a sign bit, giving the sign of the number.

Let's see what this format looks like by showing how such a number would be stored in 8 bytes of memory:

{| class="wikitable"
|-
| byte 0: || S || x10 || x9 || x8 || x7 || x6 || x5 || x4
|-
| byte 1: || x3 || x2 || x1 || x0 || m51 || m50 || m49 || m48
|-
| byte 2: || m47 || m46 || m45 || m44 || m43 || m42 || m41 || m40
|-
| byte 3: || m39 || m38 || m37 || m36 || m35 || m34 || m33 || m32
|-
| byte 4: || m31 || m30 || m29 || m28 || m27 || m26 || m25 || m24
|-
| byte 5: || m23 || m22 || m21 || m20 || m19 || m18 || m17 || m16
|-
| byte 6: || m15 || m14 || m13 || m12 || m11 || m10 || m9 || m8
|-
| byte 7: || m7 || m6 || m5 || m4 || m3 || m2 || m1 || m0
|}

where "S" denotes the sign bit, "x" denotes an exponent bit, and "m" denotes a significand bit. Once the bits here have been extracted, they are converted with the computation:

:   &amp;lt;sign&amp;gt; &amp;times; (1 + &amp;lt;fractional significand&amp;gt;) &amp;times; 2&lt;sup&gt;&amp;lt;exponent&amp;gt; - 1023&lt;/sup&gt;

This scheme provides numbers valid out to about 15 decimal digits, with the following range of numbers:
{|
|-
!
! maximum
! minimum
|-
! positive
| 1.797693134862231E+308
| 4.940656458412465E-324
|-
! negative
| -4.940656458412465E-324
| -1.797693134862231E+308
|}

The specification also defines several special values that are not defined numbers, and are known as ''[[NaN]]s'', for "Not A Number". These are used by programs to designate invalid operations and the like. 
 
Some programs also use 32-bit floating-point numbers. The most common scheme uses a 23-bit significand with a sign bit, plus an 8-bit exponent in "excess-127" format, giving seven valid decimal digits.

{| class="wikitable"
|-
|  byte 0: ||  S  ||  x7 ||  x6 ||  x5 ||  x4 ||  x3 ||  x2 ||  x1
|-
|  byte 1: ||  x0 || m22 || m21 || m20 || m19 || m18 || m17 || m16
|-
|  byte 2: || m15 || m14 || m13 || m12 || m11 || m10 ||  m9 ||  m8
|-
|  byte 3: ||  m7 ||  m6 ||  m5 ||  m4 ||  m3 ||  m2 ||  m1 ||  m0
|}
The bits are converted to a numeric value with the computation:

: &amp;lt;sign&amp;gt; &amp;times; (1 + &amp;lt;fractional significand&amp;gt;) &amp;times; 2&lt;sup&gt;&amp;lt;exponent&amp;gt; - 127&lt;/sup&gt;

leading to the following range of numbers:
{|
|-
!
! maximum
! minimum
|-
! positive
| 3.402823E+38
| 2.802597E-45
|-
! negative
| -2.802597E-45
| -3.402823E+38
|}

Such floating-point numbers are known as "reals" or "floats" in general, but with a number of  variations:

A 32-bit float value is sometimes called a "real32" or a "single", meaning "single-precision floating-point value".

A 64-bit float is sometimes called a "real64" or a "double", meaning "double-precision floating-point value".

The relation between numbers and bit patterns is chosen for convenience in computer manipulation; eight bytes stored in computer memory may represent a 64-bit real, two 32-bit reals, or four signed or unsigned integers, or some other kind of data that fits into eight bytes. The only difference is how the computer interprets them. If the computer stored four unsigned integers and then read them back from memory as a 64-bit real, it almost always would be a perfectly valid real number, though it would be junk data.

Only a finite range of real numbers can be represented with a given number of bits. Arithmetic operations can overflow or underflow, producing a value too large or too small to be represented.

The representation has a limited precision. For example, only 15 decimal digits can be represented with a 64-bit real. If a very small floating-point number is added to a large one, the result is just the large one. The small number was too small to even show up in 15 or 16 digits of resolution, and the computer effectively discards it. Analyzing the effect of limited precision is a well-studied problem. Estimates of the magnitude of round-off errors and methods to limit their effect on large calculations are part of any large computation project. The precision limit is different from the range limit, as it affects the significand, not the exponent.

The significand is a binary fraction that doesn't necessarily perfectly match a decimal fraction. In many cases a sum of reciprocal powers of 2 does not matches a specific decimal fraction, and the results of computations will be slightly off. For example, the decimal fraction "0.1" is equivalent to an infinitely repeating binary fraction: 0.000110011 ...&lt;ref&gt;{{cite web|last=Goebel|first=Greg|title=Computer Numbering Format|url=http://www.vectorsite.net/tsfloat.html|accessdate=10 September 2012}}&lt;/ref&gt;

==Numbers in programming languages==

Programming in [[assembly language]] requires the programmer to keep track of the representation of numbers. Where the processor does not support a required mathematical operation, the programmer must work out a suitable algorithm and instruction sequence to carry out the operation; on some microprocessors, even integer multiplication must be done in software.

High-level [[programming language]]s such as [[Lisp (programming language)|LISP]] and [[Python (programming language)|Python]] offer an abstract number that may be an expanded type such as ''rational'', ''bignum'', or ''complex''. Mathematical operations are carried out by library routines provided by the implementation of the language. A given mathematical symbol in the source code, by [[operator overloading]],  will invoke different object code appropriate to the representation of the numerical type; mathematical operations on any number—whether signed, unsigned, rational, floating-point, fixed-point, integral, or complex—are written exactly the same way.

Some languages, such as [[REXX]] and [[Java (programming language)|Java]], provide decimal floating points operations, which provide rounding errors of a different form.

==See also==

* [[Binary-coded decimal]]
* [[Binary numeral system]]
* [[Gray code]]
* [[Hexadecimal]]
* [[Numeral system]]
* [[Octal]]

==Notes and references==

{{vectorsite}}

{{Reflist}}

[[Category:Computer arithmetic]]
[[Category:Numeral systems]]</text>
      <sha1>ptegozfd5c56uznwhj2k7sqw88cgqb6</sha1>
    </revision>
  </page>
  <page>
    <title>Content validity</title>
    <ns>0</ns>
    <id>2229899</id>
    <revision>
      <id>844581099</id>
      <parentid>758932493</parentid>
      <timestamp>2018-06-05T20:07:22Z</timestamp>
      <contributor>
        <ip>2601:54A:300:1F8E:5C2B:29FD:834F:D355</ip>
      </contributor>
      <comment>/* On content validity */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6536">In [[psychometrics]], '''content validity''' (also known as '''logical validity''') refers to the extent to which a measure represents all facets of a given construct. For example, a [[depression (mood)|depression]] scale may lack content validity if it only assesses the [[affective]] dimension of depression but fails to take into account the [[behavioral]] dimension. An element of subjectivity exists in relation to determining content validity, which requires a degree of agreement about what a particular [[personality trait]] such as [[extraversion]] represents. A disagreement about a personality trait will prevent the gain of a high content validity.&lt;ref&gt;{{cite book | last = Pennington | first = Donald | authorlink = Donald Pennington | title = Essential Personality | publisher = [[Edward Arnold (publisher)|Arnold]] | year = 2003 | doi = | isbn = 0-340-76118-0 | page = 37}}&lt;/ref&gt;

==On content validity==
Content validity is different from [[face validity]], which refers not to what the test actually measures, but to what it superficially appears to measure. Face validity assesses whether the test "looks valid" to the examinees who take it, the administrative personnel who decide on its use, and other technically untrained observers. Content validity requires the use of recognized subject matter experts to evaluate whether test items assess defined content and more rigorous [[statistical test]]s than does the assessment of face validity. Content validity is most often addressed in academic and vocational testing, where test items need to reflect the knowledge actually required for a given topic area (e.g., history) or job skill (e.g., accounting). In clinical settings, content validity refers to the correspondence between test items and the symptom content of a syndrome.

One widely used method of measuring content validity was developed by C. H. Lawshe. It is essentially a method for gauging agreement among raters or judges regarding how essential a particular item is. Lawshe (1975) proposed that each of the subject matter expert raters (SMEs) on the judging panel respond to the following question for each item: "Is the skill or knowledge measured by this item 'essential,' 'useful, but not essential,' or 'not necessary' to the performance of the construct?" According to Lawshe, if more than half the panelists indicate that an item is essential, that item has at least some content validity. Greater levels of content validity exist as larger numbers of panelists agree that a particular item is essential. Using these assumptions, Lawshe developed a formula termed the content validity ratio:
&lt;math&gt;CVR = (n_e - N/2)/(N/2)&lt;/math&gt;
where &lt;math&gt;CVR=&lt;/math&gt; content validity ratio, &lt;math&gt;n_e=&lt;/math&gt; number of SME panelists indicating "essential", &lt;math&gt;N=&lt;/math&gt; total number of SME panelists. This formula yields values which range from +1 to -1; positive values indicate that at least half the SMEs rated the item as essential. The mean CVR across items may be used as an indicator of overall test content validity.

Lawshe (1975) provided a table of critical values for the CVR by which a test evaluator could determine, for a pool of SMEs of a given size, the size of a calculated CVR necessary to exceed chance expectation. This table had been calculated for Lawshe by his friend, Lowell Schipper. Close examination of this published table revealed an anomaly. In Schipper's table, the critical value for the CVR increases monotonically from the case of 40 SMEs (minimum value = .29) to the case of 9 SMEs (minimum value = .78) only to unexpectedly drop at the case of 8 SMEs (minimum value = .75) before hitting its ceiling value at the case of 7 SMEs (minimum value = .99). However, it is important to understand when applying the formula to 8 raters, the result from 7 Essential and 1 other rating yields a CVR of .75. If .75 was not the critical value, then you would need 8 of 8 raters of Essential that would yield a CVR of 1.00.  In that case, to be consistent with the ascending order of CVRs the value for 8 raters would have to be 1.00.  That would violate the same principle because you would have the "perfect" value required for 8 raters, but not for ratings at other numbers of raters at either higher or lower than 8 raters.  Whether this departure from the table's otherwise monotonic progression was due to a calculation error on Schipper's part or an error in typing or type setting is unclear. Wilson, Pan, and Schumsky (2012), seeking to correct the error, found no explanation in Lawshe's writings nor any publications by Schipper describing how the table of critical values was computed. Wilson and colleagues determined that the Schipper values were close approximations to the normal approximation to the binomial distribution.  By comparing Schipper's values to the newly calculated binomial values, they also found that Lawshe and Schipper had erroneously labeled their published table as representing a one-tailed test when in fact the values mirrored the binomial values for a two-tailed test. Wilson and colleagues published a recalculation of critical values for the content validity ratio providing critical values in unit steps at multiple alpha levels.

The table of values is the following one:

N° of panelists          Min. Value
 
       5              .99
       6              .99
       7              .99
       8              .75
       9              .78
      10              .62
      11              .59
      12              .56
      20              .42
      30              .33
      40              .29
 
   From:http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.460.9380&amp;rep=rep1&amp;type=pdf

==See also==
* [[Construct validity]]
* [[Criterion validity]]
* [[Test validity]]
* [[Validity (statistics)]]
* [[Face Validity]]

==References==
{{reflist}}
*Lawshe, C.H. (1975). A quantitative approach to content validity. ''Personnel Psychology, 28'', 563–575. doi:10.1111/j.1744-6570.1975.tb01393.x
*Wilson, F.R., Pan, W., &amp; Schumsky, D.A. (2012). Recalculation of the critical values for Lawshe’s content validity ratio. ''Measurement and Evaluation in Counseling and Development, 45''(3), 197-210. doi:10.1177/0748175612440286

==External links==
*[[Wikibooks:Handbook of Management Scales|Handbook of Management Scales]], a Wikibook containing previously used multi-item scales to measure constructs in empirical management research literature. For many scales, content validity is discussed.

[[Category:Validity (statistics)]]</text>
      <sha1>toh5g6mnm4vqyhhzri3govut69q5ee1</sha1>
    </revision>
  </page>
  <page>
    <title>Deletion channel</title>
    <ns>0</ns>
    <id>36017008</id>
    <revision>
      <id>731430763</id>
      <parentid>701637542</parentid>
      <timestamp>2016-07-25T09:20:44Z</timestamp>
      <contributor>
        <ip>95.113.227.76</ip>
      </contributor>
      <comment>/* Formal description */ improved structure minimally</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2377">A '''deletion channel''' is a [[communications channel]] model used in [[coding theory]] and [[information theory]]. In this model, a transmitter sends a [[bit]] (a zero or a one), and the receiver either receives the bit (with probability &lt;math&gt;p&lt;/math&gt;) or does not receive anything without being notified that the bit was dropped (with probability &lt;math&gt;1-p&lt;/math&gt;). Determining the [[capacity (information theory)|capacity]] of the deletion channel is an open problem.&lt;ref&gt;{{citation
 | last = Mitzenmacher | first = Michael | authorlink = Michael Mitzenmacher
 | doi = 10.1214/08-PS141
 | journal = Probability Surveys
 | mr = 2525669
 | pages = 1–33
 | title = A survey of results for deletion channels and related synchronization channels
 | volume = 6
 | year = 2009}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last1 = Kanoria | first1 = Yashodhan
 | last2 = Montanari | first2 = Andrea
 | doi = 10.1109/TIT.2013.2262020
 | issue = 10
 | journal = IEEE Transactions on Information Theory
 | mr = 3106824
 | pages = 6192–6219
 | title = Optimal coding for the binary deletion channel with small deletion probability
 | volume = 59
 | year = 2013}}.&lt;/ref&gt;

The deletion channel should not be confused with the [[binary erasure channel]] which is much simpler to analyze.

== Formal description ==

Let &lt;math&gt;p&lt;/math&gt; be the deletion probability, &lt;math&gt;0 &lt; p &lt; 1&lt;/math&gt;. The [[independent and identically distributed|iid]] binary deletion channel is defined as follows:

Given a input sequence of &lt;math&gt;n&lt;/math&gt; bits &lt;math&gt;(X_i)&lt;/math&gt; as input, each bit in &lt;math&gt;X_n&lt;/math&gt; can be deleted with probability &lt;math&gt;p&lt;/math&gt;. The deletion positions are unknown to the sender and the receiver. The output sequence &lt;math&gt;(Y_i)&lt;/math&gt; is the sequence of the &lt;math&gt;(X_i)&lt;/math&gt; which were not deleted, in the correct order and with no errors.

== Capacity ==

{{unsolved|computer science|What is the capacity of a deletion channel?}}

The [[channel capacity|capacity]] of the binary deletion channel (as an [[analytical expression]] of the deletion rate &lt;math&gt;p&lt;/math&gt;) is unknown. It has a [[mathematical expression]]{{citation needed|date=January 2016}}. Several upper and lower bounds are known.

==External links==
* [https://github.com/JarekDuda/DeletionChannelPracticalCorrection Implementation of correction for deletion channel]

==References==
{{reflist}}

[[Category:Coding theory]]</text>
      <sha1>2am3qfyfru57jrey1jfhdaj80pv1xyx</sha1>
    </revision>
  </page>
  <page>
    <title>Differential invariant</title>
    <ns>0</ns>
    <id>25412108</id>
    <revision>
      <id>863897521</id>
      <parentid>854942231</parentid>
      <timestamp>2018-10-13T20:07:23Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>[[Elizabeth Mansfield (mathematician)]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6033">In [[mathematics]], a '''differential invariant''' is an [[invariant theory|invariant]] for the [[group action|action]] of a [[Lie group]] on a space that involves the [[derivative]]s of graphs of functions in the space. Differential invariants are fundamental in [[projective differential geometry]], and the [[curvature]] is often studied from this point of view.&lt;ref&gt;{{harvnb|Guggenheimer|1977}}&lt;/ref&gt; Differential invariants were introduced in special cases by [[Sophus Lie]] in the early 1880s and studied by [[Georges Henri Halphen]] at the same time. {{harvtxt|Lie|1884}} was the first general work on differential invariants, and established the relationship between differential invariants, invariant [[differential equation]]s, and [[invariant differential operator]]s.

Differential invariants are contrasted with geometric invariants.  Whereas differential invariants can involve a distinguished choice of independent variables (or a parameterization), geometric invariants do not.  [[Élie Cartan]]'s [[method of moving frames]] is a refinement that, while less general than Lie's methods of differential invariants, always yields invariants of the geometrical kind.

==Definition==
The simplest case is for differential invariants for one independent variable ''x'' and one dependent variable ''y''. Let ''G'' be a [[Lie group]] acting on '''R'''&lt;sup&gt;2&lt;/sup&gt;.  Then ''G'' also acts, locally, on the space of all graphs of the form ''y''&amp;nbsp;=&amp;nbsp;''&amp;fnof;''(''x''). Roughly speaking, a ''k''-th order differential invariant is a function
:&lt;math&gt;I\left(x,y,\frac{dy}{dx},\dots,\frac{d^ky}{dx^k}\right)&lt;/math&gt;
depending on ''y'' and its first ''k'' derivatives with respect to ''x'', that is invariant under the action of the group.

The group can act on the higher-order derivatives in a nontrivial manner that requires computing the ''prolongation'' of the group action.  The action of ''G'' on the first derivative, for instance, is such that the [[chain rule]] continues to hold: if
:&lt;math&gt;(\overline{x},\overline{y}) = g\cdot(x,y),&lt;/math&gt;
then
:&lt;math&gt;g\cdot\left(x,y,\frac{dy}{dx}\right) \stackrel{\text{def}}{=} \left(\overline{x},\overline{y},\frac{d\overline{y}}{d\overline{x}}\right).&lt;/math&gt;
Similar considerations apply for the computation of higher prolongations.  This method of computing the prolongation is impractical, however, and it is much simpler to work infinitesimally at the level of [[Lie algebra]]s and the [[Lie derivative]] along the ''G'' action.

More generally, differential invariants can be considered for mappings from any [[smooth manifold]] ''X'' into another smooth manifold ''Y'' for a Lie group acting on the [[Cartesian product]] ''X''&amp;times;''Y''.  The graph of a mapping ''X''&amp;nbsp;&amp;rarr;&amp;nbsp;''Y'' is a submanifold of ''X''&amp;times;''Y'' that is everywhere transverse to the fibers over ''X''.  The group ''G'' acts, locally, on the space of such graphs, and induces an action on the ''k''-th prolongation ''Y''&lt;sup&gt;(''k'')&lt;/sup&gt; consisting of graphs passing through each point modulo the relation of ''k''-th order contact.  A differential invariant is a function on ''Y''&lt;sup&gt;(''k'')&lt;/sup&gt; that is invariant under the prolongation of the group action.

==Applications==
* Differential invariants can be applied to the study of systems of [[partial differential equations]]: seeking [[similarity solution]]s that are invariant under the action of a particular group can reduce the dimension of the problem (i.e. yield a "reduced system").&lt;ref&gt;{{harvnb|Olver|1994|loc=Chapter 3}}&lt;/ref&gt;
* [[Noether's theorem]] implies the existence of differential invariants corresponding to every differentiable symmetry of a [[calculus of variations|variational problem]].
*[[Fluid dynamics|Flow characteristics]] using [[computer vision]]&lt;ref&gt;{{cite book |first=Peter |last=Olver |first2=Guillermo |last2=Sapiro |first3=Allen |last3=Tannenbaum |title=Geometry-Driven Diffusion in Computer Vision |pages=255–306 |chapter=Differential Invariant Signatures and Flows in Computer Vision: A Symmetry Group Approach |year=1994 |series=Computational Imaging and Vision |volume=1 |publisher=Springer |location=Dordrecht |doi=10.1007/978-94-017-1699-4_11 |isbn=90-481-4461-2 }}&lt;/ref&gt;
*[[Geometric integrator|Geometric integration]]

==See also==
*[[Cartan's equivalence method]]

==Notes==
{{reflist}}

==References==
*{{Citation | last1=Guggenheimer | first1=Heinrich | authorlink1=Heinrich Guggenheimer|title=Differential Geometry | publisher=[[Dover Publications]] | location=New York | isbn=978-0-486-63433-3 | year=1977}}.
*{{citation | last=Lie|first=Sophus|authorlink=Sophus Lie|contribution=Über Differentialinvarianten|title=Gesammelte Adhandlungen|volume=6|publisher=B.G. Teubner|publication-place=Leipzig|year=1884|pages=95&amp;ndash;138}}; English translation: {{citation|last1=Ackerman|first1=M|last2=Hermann|first2=R|title=Sophus Lie's 1884 Differential Invariant Paper|publisher=Math Sci Press|publication-place=Brookline, Mass.|year=1975}}.
*{{Citation | last1=Olver | first1=Peter J. |author1-link=Peter J. Olver | title=Applications of Lie groups to differential equations | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | isbn=978-0-387-94007-6 | year=1993}}.
*{{Citation | last1=Olver | first1=Peter J. |author1-link=Peter J. Olver | title=Equivalence, Invariants, and Symmetry | publisher=[[Cambridge University Press]] | isbn=978-0-521-47811-3 | year=1995}}.
*{{citation|url=http://www.kent.ac.uk/ims/personal/elm2/FrameBook2Jun09.pdf |title=A Practical Guide to the Invariant Calculus |first=Elizabeth Louise |last=Mansfield |authorlink=Elizabeth Mansfield (mathematician)|year=2009 }}{{dead link|date=December 2016 |bot=InternetArchiveBot |fix-attempted=yes }}; to be published by Cambridge 2010, {{ISBN|978-0-521-85701-7}}.

==External links==
*[http://www.physics.ucla.edu/~cwp/articles/noether.trans/english/mort186.html Invariant Variation Problems]

[[Category:Differential geometry]]
[[Category:Invariant theory]]
[[Category:Projective geometry]]</text>
      <sha1>pk1xh6cst29xqo5rclvo3r81id7ejl8</sha1>
    </revision>
  </page>
  <page>
    <title>Ed Scheinerman</title>
    <ns>0</ns>
    <id>39950449</id>
    <revision>
      <id>770931398</id>
      <parentid>766519518</parentid>
      <timestamp>2017-03-18T14:01:54Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Moving category Princeton University alumni, 1980–89 to [[:Category:Princeton University alumni]] per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2017 February 24]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4439">'''Edward R. Scheinerman''' is an American mathematician, specializing in [[graph theory]] and [[order theory]]. He is a professor of applied mathematics, statistics, and computer science at [[Johns Hopkins University]].&lt;ref name="folio"&gt;[http://folio.jhu.edu/faculty/Edward%20R._Scheinerman Faculty profile], Johns Hopkins University, retrieved 2013-07-12.&lt;/ref&gt; His contributions to mathematics include [[Scheinerman's conjecture]], now proven, stating that every [[planar graph]] may be represented as an [[intersection graph]] of [[line segment]]s.&lt;ref&gt;{{citation
 | last1 = Chalopin | first1 = J.
 | last2 = Gonçalves | first2 = D.
 | contribution = Every planar graph is the intersection graph of segments in the plane
 | title = [[Symposium on Theory of Computing|ACM Symposium on Theory of Computing]]
 | contribution-url = http://www.csie.ntu.edu.tw/~hil/bib/ChalopinG09.pdf
 | year = 2009}}.&lt;/ref&gt;

Scheinerman did his undergraduate studies at [[Brown University]], graduating in 1980, and earned his Ph.D. in 1984 from [[Princeton University]] under the supervision of [[Douglas West (mathematician)|Douglas B. West]].&lt;ref name="folio"/&gt;&lt;ref&gt;{{mathgenealogy|id=6748}}&lt;/ref&gt; He joined the Johns Hopkins faculty in 1984, and since 2000 he has been an administrator there, serving as department chair, associate dean, and (since 2008) vice dean for education.&lt;ref name="folio"/&gt;

He is a two-time winner of the [[Mathematical Association of America]]'s [[Lester R. Ford Award]] for expository writing, in 1991 for his paper "Random intervals" with Joyce Justicz and [[Peter Winkler]], and in 2001 for his paper "When Close is Close Enough".&lt;ref&gt;[http://www.maa.org/awards/ford.html List of Ford Award winners], MAA, retrieved 2013-07-12.&lt;/ref&gt; In 1992 he became a fellow of the [[Institute of Combinatorics and its Applications]],&lt;ref name="folio"/&gt; and in 2012 he became a fellow of the [[American Mathematical Society]].&lt;ref&gt;[http://www.ams.org/profession/fellows-list List of Fellows of the American Mathematical Society], retrieved 2013-07-12.&lt;/ref&gt;

==Selected publications==
;Books
*''Invitation to Dynamical Systems'' (Prentice Hall, 1996, [https://books.google.com/books/about/Invitation_to_Dynamical_Systems.html?id=jJOAqSB9ntAC reprinted by Dover Publications, 2012]).
*''Fractional Graph Theory'' (With Daniel Ullman, Wiley, 1997, [https://books.google.com/books/about/Fractional_Graph_Theory.html?id=zzFxD8kPWigC reprinted by Dover Publications, 2011]).&lt;ref&gt;Review of ''Fractional Graph Theory'', {{MR|1481157}} and {{MR|2963519}}.&lt;/ref&gt;
*''[https://books.google.com/books?id=Pc0KAAAAQBAJ Mathematics: A Discrete Introduction].'' (Brooks/Cole, 2000; [https://books.google.com/books/about/Mathematics_A_Discrete_Introduction.html?id=DZBHGD2sEYwC 3rd edition], Cengage Learning, 2012).
*[https://books.google.com/books/about/C++_for_Mathematicians.html?id=qsfzxExOVTsC ''C++ for mathematicians : an introduction for students and professionals''] (Chapman &amp; Hall/CRC, 2006).
*''[https://books.google.com/books/about/The_Mathematics_Lover_s_Companion.html?id=XncsvgAACAAJ The Mathematics Lover's Companion: Masterpieces for Everyone]'' (Yale University Press, 2017).
&lt;!-- ''Mathematical Notation: A Guide for Engineers and Scientists'' intentionally omitted as self-published --&gt;

;Papers
*{{citation|first=E. R.|last=Scheinerman|title=When close is close enough|journal=[[American Mathematical Monthly]]|volume=107|issue=6|date=June 2000|pages=489–499|url=http://mathdl.maa.org/mathDL/22/?pa=content&amp;sa=viewDocument&amp;nodeId=2890|doi=10.2307/2589344}}.
*{{citation|first1=Joyce|last1=Justicz|first2=Edward R.|last2=Scheinerman|first3=Peter|last3=Winkler|author3-link=Peter Winkler|title=Random intervals|journal=[[American Mathematical Monthly]]|volume=97|year=1990|pages=881–889|url=http://mathdl.maa.org/mathDL/22/?pa=content&amp;sa=viewDocument&amp;nodeId=2921|doi=10.2307/2324324}}.

==References==
{{reflist}}

==External links==
*[http://www.ams.jhu.edu/ers/ Home page]

{{Authority control}}

{{DEFAULTSORT:Scheinerman, Edward R.}}
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]
[[Category:20th-century American mathematicians]]
[[Category:21st-century American mathematicians]]
[[Category:Graph theorists]]
[[Category:Brown University alumni]]
[[Category:Princeton University alumni]]
[[Category:Johns Hopkins University faculty]]
[[Category:Fellows of the American Mathematical Society]]</text>
      <sha1>qz1u0nob35e916h83okhv01xi4r1c5i</sha1>
    </revision>
  </page>
  <page>
    <title>Edge chromatic number</title>
    <ns>0</ns>
    <id>744291</id>
    <redirect title="Edge coloring" />
    <revision>
      <id>313416309</id>
      <parentid>16554462</parentid>
      <timestamp>2009-09-12T18:58:46Z</timestamp>
      <contributor>
        <username>Miym</username>
        <id>8436643</id>
      </contributor>
      <minor/>
      <comment>cat</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="57">#REDIRECT [[Edge coloring]]
[[Category:Graph invariants]]</text>
      <sha1>6mqj2qoy1u6yepr2nxwy4hl7kiy53ns</sha1>
    </revision>
  </page>
  <page>
    <title>Entropic vector</title>
    <ns>0</ns>
    <id>17519721</id>
    <revision>
      <id>854255965</id>
      <parentid>848052701</parentid>
      <timestamp>2018-08-10T01:09:06Z</timestamp>
      <contributor>
        <username>CalliopeMuse</username>
        <id>33865157</id>
      </contributor>
      <minor/>
      <comment>/* The Shannon inequality and &amp;Gamma;n */ typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7479">{{Orphan|date=July 2013}}
{{merge|Inequalities in information theory|date=October 2017}}

The '''entropic vector''' or '''entropic function''' is a concept arising in [[information theory]]. [[Claude Shannon|Shannon]]'s [[information entropy]] measures and their associated identities and inequalities (both constrained and unconstrained) have received a lot of attention over the past from the time Shannon introduced his concept of Information Entropy. A lot of inequalities and identities have been found and are available in standard Information Theory texts. But recent researchers have laid focus on trying to find all possible identities and inequalities (both constrained and unconstrained) on such entropies and characterize them. Entropic vector lays down the basic framework for such a study.

==Definition==

Let  &lt;math&gt;X_1, X_2,\dots,X_n&lt;/math&gt; be random variables, with &lt;math&gt;n \in N&lt;/math&gt;

A vector ''h'' in &lt;math&gt;R^{2^n-1}&lt;/math&gt; is an entropic vector of order &lt;math&gt;n&lt;/math&gt; if and only if there exists a tuple &lt;math&gt;\overrightarrow{X}=X_1,X_2,\ldots,X_n&lt;/math&gt; with associated vector &lt;math&gt;h_{\overrightarrow{X}}&lt;/math&gt; defined by &lt;math&gt;h_{\overrightarrow{X}}(I)=H(X_I)=H(X_{i_1},X_{i_2},\dots,X_{i_k})&lt;/math&gt; where &lt;math&gt;I=\{i_1,i_2,\dots,i_k\}&lt;/math&gt; y &lt;math&gt;h=h_{\overrightarrow{x}}&lt;/math&gt;. The set of all entropic vectors of order &lt;math&gt;n&lt;/math&gt; is denoted by &lt;math&gt;\Gamma_n^*&lt;/math&gt;

All the properties of entropic functions can be transposed to entropic vectors:

&lt;math&gt;  H :P_n \rightarrow R^+ &lt;/math&gt;  is continuous

Given a deterministic random variable &lt;math&gt;x &lt;/math&gt;, we have &lt;math&gt;H(x)=0&lt;/math&gt;

Given &lt;math&gt; \alpha \in R^+ &lt;/math&gt;, there exists a random variable &lt;math&gt;x &lt;/math&gt; such that &lt;math&gt;H(x)=\alpha &lt;/math&gt;

Given &lt;math&gt;P &lt;/math&gt; a probability distribution on &lt;math&gt;[n]&lt;/math&gt;, we have &lt;math&gt;H(P)\leq \log_2 n &lt;/math&gt;

==Example==

Let ''X'',''Y'' be two independent random variables with [[discrete uniform distribution]] over the set &lt;math&gt;\{0,1\}&lt;/math&gt;. Then 
:&lt;math&gt; 
H \left (X \right ) = H(Y) = 1, I \left (X;Y \right ) = 0
&lt;/math&gt;

It follows that

&lt;math&gt;
H(X,Y)= H(X) + H(Y) - I \left (X;Y \right ) = 2
&lt;/math&gt;

The entropic vector is thus :

&lt;math&gt; 
v = \left ( 1,1,2 \right )^T \in \Gamma_2^*
&lt;/math&gt;

== The region &amp;Gamma;&lt;sub&gt;''n''&lt;/sub&gt;&lt;sup&gt;*&lt;/sup&gt; ==

=== The Shannon inequality and &amp;Gamma;&lt;sub&gt;''n''&lt;/sub&gt; ===

The entropy satisfies the properties

:&lt;math&gt; 
1) \quad H(\empty) = 0
&lt;/math&gt;

:&lt;math&gt;
 2) \quad \alpha \subseteq \beta: H(\alpha) \leq H(\beta)
&lt;/math&gt;

The Shannon inequality is

:&lt;math&gt; 
3) \quad H(X_\alpha) + H(X_\beta) \geq H(X_{\alpha\cup\beta}) + H(X_{\alpha\cap\beta})
&lt;/math&gt;

The entropy vector that satisfies the linear combination of this region is called   &lt;math&gt;\Gamma_n&lt;/math&gt;.
 
The region &lt;math&gt;\Gamma_n^*&lt;/math&gt;  has been studied recently, the cases for ''n''&amp;nbsp;=&amp;nbsp;1,&amp;nbsp;2,&amp;nbsp;3

:&lt;math&gt; 
L_n=\Gamma_n= \Gamma_n^* =\overline{\Gamma_n}^*
&lt;/math&gt;
:&lt;math&gt; 
L_n^o=\Gamma_n^o =\overline{\Gamma_n}^{*o}= \langle \mathrm{Shannon}_n\rangle ^+
&lt;/math&gt;

if and only if ''n''&amp;nbsp;&amp;isin;&amp;nbsp;{1,&amp;nbsp;2,&amp;nbsp;3}
[[File:Cone diagram.jpg|thumb|Cone diagram]]
It is difficult harder con the case &lt;math&gt; n \geq4  &lt;/math&gt;, the number of inequalities given by monotone and submodularity properties increase when we increase ''n'', however the relationship among entropic vectors, polymatroids, are an object of study for the information theory and there are other ways to characterize those relationships mentioned

The most important results for the characterization of &lt;math&gt;\Gamma_n^*&lt;/math&gt; is not precisely about these set, but its topological clousure i.e. the set &lt;math&gt;\overrightarrow{\Gamma_n^*}&lt;/math&gt;, which says that &lt;math&gt;\overrightarrow{\Gamma_n^*}&lt;/math&gt; is a [[convex cone]], other interesting characterization is that &lt;math&gt;\overrightarrow{\Gamma_n^*}=\Gamma_n&lt;/math&gt; (&lt;math&gt;\Gamma_n&lt;/math&gt; is the set of vectors that satisfy Shannon-type inequalities) for &lt;math&gt;n \leq 3&lt;/math&gt;, in other words the set of entropy vector is completely characterized by Sahnnon's Inequalities,&lt;ref&gt;{{cite conference |first=Terence |last=Chan |first2=Dongning |last2=Duo |first3=Raymondo |last3=Yeung |title=Entropy functions and determinant inequalities |conference=2012 IEEE International Symposium on Information Theory | year=2012}}&lt;/ref&gt; for the case ''n''&amp;nbsp;=&amp;nbsp;4 fails this property,&lt;ref name="matus2007"/&gt;&lt;ref&gt;{{cite conference |first=R. |last=Dougherty |first2=C. |last2=Freiling|author2-link=Chris Freiling |first3=K. |last3=Zeger |title=Six New Non-Shannon Information Inequalities |conference=2006 IEEE International Symposium on Information Theory | year=2006}}&lt;/ref&gt; particularly by the [[Ingleton's inequality]].&lt;ref&gt;{{cite conference |first=A. |last=Ingleton |title=Representation of matroids |journal=n Combinatorial Mathematics and its Applications |year=1971}}&lt;/ref&gt;

:&lt;math&gt; 
L_n \subseteq \overline{\Gamma_n}^* \subseteq \Gamma_n
&lt;/math&gt;
:&lt;math&gt; 
\Gamma_n^o \subseteq \overline{\Gamma_n}^{*o} \subseteq L_n^o
&lt;/math&gt;

:&lt;math&gt; 
\Gamma_n^o= \langle \mathrm{Shannon}_n\rangle^+
&lt;/math&gt;

==The Matus theorem==
In 1998 Zhang and Yeung proved a new non-Shannon's inequality&lt;ref&gt;{{cite journal |first=Z. |last=Zhang |first2=R.W. |last2=Yeung |title=On Characterization of Entropy Function via Information Inequalities |journal=IEEE Trans. Inf. Theory |volume=44 |pages=1440–1452 |year=1998}}&lt;/ref&gt;
:&lt;math&gt;
I(X_3,X_4) = I(X_4,X_2) + I(X_1:X_3,X_4) + 3I(X_3:X_4|X_1) + I(X_3:X_4| X_2)
&lt;/math&gt;
and in 2007 Matus proved that &lt;math&gt; \overline{\Gamma_4^*} &lt;/math&gt; is not
polihedral.&lt;ref name="matus2007"&gt;{{cite conference |first=F. |last=Matus |title=Infinitely many information inequalities |conference=2007 IEEE International Symposium on Information Theory | year=2007}}&lt;/ref&gt;

==Entropy and groups==

===Group-charactizable vectors and quasi-uniform distribution===
One way to charactize &lt;math&gt;\Gamma_n^*&lt;/math&gt; is by looking at some special distributions.\\
Definition: A group characterizable vector h is also denoted to be&lt;math&gt;  2^n\rightarrow R&lt;/math&gt;

such that there exists a group &lt;math&gt; G &lt;/math&gt; and subgroups  &lt;math&gt; G_1, G_2,\dots, G_n&lt;/math&gt;  and for  &lt;math&gt; \alpha \subset n&lt;/math&gt;

:&lt;math&gt; 
H(\alpha) = \frac{|Gi|}{|G|}
&lt;/math&gt;

if &lt;math&gt;  G_\alpha  &lt;/math&gt; is not   &lt;math&gt; \quad \quad  \empty &lt;/math&gt; and 0 otherwise. &lt;math&gt;  G= \cap_{i\in \alpha} G_i&lt;/math&gt; .

Definition: &lt;math&gt;\gamma^n&lt;/math&gt; is the set of all group charactizable vectors is &lt;math&gt; n &lt;/math&gt;, and we can describe better the set &lt;math&gt;\Gamma^n&lt;/math&gt;

Theorem: &lt;math&gt;\gamma^n \subset \Gamma^n&lt;/math&gt;

== Open problem ==

Given a vector &lt;math&gt;v \in R^{2^n -1}&lt;/math&gt;, is it possible to say if there exists &lt;math&gt; n &lt;/math&gt; random variables such that their joint entropies are given by &lt;math&gt;v&lt;/math&gt;? It turns out that for &lt;math&gt;n=2,3&lt;/math&gt; the problem has been solved. But for &lt;math&gt; n \geq 4&lt;/math&gt;, it still remains unsolved. Defining the set of all such vectors &lt;math&gt;v \in R^{2^n -1}&lt;/math&gt; that can be constructed from a set of &lt;math&gt;n&lt;/math&gt; random variables as &lt;math&gt;{\Gamma}^{*}_n&lt;/math&gt;, we see that a complete characterization of this space remains an unsolved mystery.

==References==

&lt;references/&gt;
* Thomas M. Cover, Joy A. Thomas. ''Elements of information theory'' New York: Wiley, 1991. {{ISBN|0-471-06259-6}}
* Raymond Yeung. ''A First Course in Information Theory'', Chapter 12, ''Information Inequalities'', 2002, Print {{ISBN|0-306-46791-7}}

[[Category:Information theory]]</text>
      <sha1>7bkgtv3wpck0ysrdhngm8bhv3r8e26u</sha1>
    </revision>
  </page>
  <page>
    <title>Equivalence (formal languages)</title>
    <ns>0</ns>
    <id>34615237</id>
    <revision>
      <id>840270618</id>
      <parentid>840268293</parentid>
      <timestamp>2018-05-08T20:28:33Z</timestamp>
      <contributor>
        <username>Jochen Burghardt</username>
        <id>17350134</id>
      </contributor>
      <comment>/* Context-free grammar example */ oops, had 2nd grammar wrong (unable to generate e.g. "1+1+1"); fixed grammar and syntax tree picture</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6790">
In formal language theory, '''weak equivalence''' of two [[formal grammars|grammar]]s means they generate the same set of strings, i.e. that the [[formal language]] they generate is the same. In [[compiler theory]] the notion is distinguished from '''strong''' (or '''structural''') '''equivalence''', which additionally means that the two [[parse tree]]s{{clarify|reason=In the Chomsky hierarchy of grammars, parse trees can be defined only for context-free grammars (and below). For example, the derivation of 'aaabbbccc' shown in 'Context-sensitive grammar#Examples' doesn't correspond to a tree.|date=February 2014}} are reasonably similar in that the same semantic interpretation can be assigned to both.&lt;ref name="Reghizzi2009"&gt;{{cite book|author=Stefano Crespi Reghizzi|title=Formal Languages and Compilation|url=https://books.google.com/books?id=AxH6cWm61i0C&amp;pg=PA57|year=2009|publisher=Springer|isbn=978-1-84882-049-4|page=57}}&lt;/ref&gt;

Vijay-Shanker and Weir (1994)&lt;ref name="vijayshankarAndWeir1995"&gt;Vijay-Shanker, K. and Weir, David J. 1994. ''The Equivalence of Four Extensions of Context-Free Grammars''. Mathematical Systems Theory 27(6): 511-546.&lt;/ref&gt; demonstrates that [[Indexed grammar|Linear Indexed Grammars]], [[Combinatory categorial grammar|Combinatory Categorial Grammars]], [[Tree-adjoining grammar|Tree-adjoining Grammars]], and [[Head grammar|Head Grammars]] are weakly equivalent formalisms,{{clarify|reason=In the previous paragraph, weak equivalence was introduced as a relation between two grammars. In the current paragraph, it is used a relation between two 'formalisms', i.e. two classes of grammars.|date=February 2014}} in that they all define the same string languages.

On the other hand, if the two grammars generate the same set of derivation trees (or more generally, the same set of abstract syntactic objects), then the two languages are strongly equivalent. Chomsky (1963)&lt;ref name="Chomsky.1963"/&gt; introduces the notion of strong equivalence, and argues that only strong equivalence is relevant when comparing grammar formalisms. Kornai and Pullum (1990)&lt;ref name="kornaiAndPullum1990"&gt;Kornai, A. and Pullum, G. K. 1990. ''The X-bar Theory of Phrase Structure''. Language, 66:24-50.&lt;/ref&gt; and Miller (1994)&lt;ref name="miller1994"&gt;Miller, Philip H. 1999. ''Strong Generative Capacity''. CSLI publications.&lt;/ref&gt; offer a refined notion of strong equivalence that allows for isomorphic relationships between the syntactic analyses given by different formalisms. Yoshinaga, Miyao, and Tsujii (2002)&lt;ref name="yoshinagaMiyaoAndTsujii2002"&gt;[http://www-tsujii.is.s.u-tokyo.ac.jp/~yoshinag/papers/yoshinag-tagplus6.pdf Yoshinaga, N., Miyao Y., and Tsujii, J. 2002. ''A formal proof of strong equivalence for a grammar conversion from LTAG to HPSG-style''. In the Proceedings of the TAG+6 Workshop:187-192. Venice, Italy.]&lt;/ref&gt; offers a proof of the strong equivalency of the [[Tree-adjoining grammar|LTAG]] and [[Head-driven phrase structure grammar|HPSG]] formalisms.

==Context-free grammar example==
[[File:Parse Tree Derivations ETF.svg|300px|thumb|''Left:'' one of the parse trees of the string "1+2*3" with the first grammar. ''Right:'' the only parse tree of that string with the second grammar.]]
As an example, consider the following two context-free grammars,&lt;ref group=note&gt;with the [[start symbol (formal languages)|start symbol]] "&lt;nowiki&gt;&lt;expression&gt;&lt;/nowiki&gt;"&lt;/ref&gt; given in [[Backus-Naur form]]:

&lt;source lang="bnf"&gt;
&lt;expression&gt; ::= &lt;expression&gt; "+" &lt;expression&gt; | &lt;expression&gt; "-" &lt;expression&gt;
               | &lt;expression&gt; "*" &lt;expression&gt; | &lt;expression&gt; "/" &lt;expression&gt; 
               | "x" | "y" | "z"   |   "1" | "2" | "3"   |   "(" &lt;expression&gt; ")"
&lt;/source&gt;

&lt;source lang="bnf"&gt;
&lt;expression&gt; ::= &lt;term&gt;   | &lt;expression&gt; "+" &lt;term&gt;   | &lt;expression&gt; "-" &lt;term&gt;
&lt;term&gt;       ::= &lt;factor&gt; |       &lt;term&gt; "*" &lt;factor&gt; |       &lt;term&gt; "/" &lt;factor&gt;
&lt;factor&gt;     ::= "x" | "y" | "z"   |   "1" | "2" | "3"   |   "(" &lt;expression&gt; ")"
&lt;/source&gt;

Both grammars generate the same set of strings, viz. the set of all arithmetical expressions that can be built from the variables "x", "y", "z", the constants "1", "2", "3", the operators "+", "-", "*", "/", and parantheses "(" and ")".
However, a [[concrete syntax tree]] of the second grammar always reflects the usual [[order of operations]], while a tree from the first grammar need not.

For the example string "1+2*3", the right part of the picture shows its unique parse tree with the second grammar;&lt;ref group=note&gt;using the abbreviation "E", "T", and "F" for &lt;nowiki&gt;&lt;expression&gt;&lt;/nowiki&gt;, &lt;nowiki&gt;&lt;term&gt;&lt;/nowiki&gt;, and &lt;nowiki&gt;&lt;factor&gt;&lt;/nowiki&gt;, respectively&lt;/ref&gt; evaluating this tree in [[postfix traversal|postfix order]] will yield the proper value, 7.
In contrast, the left picture part shows one of the parse trees for that string with the first grammar; evaluating it in postfix order will yield 9.

Since the second grammar cannot generate a tree corresponding to the left picture part, while the first grammar can, both grammars are not strongly equivalent.

==Generative capacity==
In linguistics, the '''weak generative capacity'''&lt;!---in bold, since this term redirects here---&gt; of a [[generative grammar|grammar]] is defined as the set of all strings generated by it,&lt;ref group=note&gt;for context-free grammars: see [[Context-free_grammar#Context-free language]] for a formal definition&lt;/ref&gt; while a grammar's '''strong generative capacity'''&lt;!---in bold, since this term redirects here---&gt; refers to the set of "structural descriptions"&lt;ref group=note&gt;for context-free grammars: [[concrete syntax tree]]s&lt;/ref&gt; generated by it.&lt;ref name="Bach.Miller.2003"&gt;{{cite book | contribution-url=http://www.kornai.com/MatLing/gencapfin.pdf | author1=Emmon Bach |author2= Philip Miller | contribution=Generative Capacity | doi=10.1093/acref/9780195139778.001.0001 | isbn=9780195139778 | editor=William J. Frawley | title=International Encyclopedia of Linguistics | publisher=Oxford University Press | edition=2nd | year=2003 }}&lt;/ref&gt;
As a consequence, two grammars are considered weakly equivalent if their weak generative capacities coincide; similar for strong equivalence.
The notion of ''generative capacity'' was introduced by [[Noam Chomsky]] in 1963.&lt;ref name="Chomsky.1963"&gt;{{cite book | author=Noam Chomsky | contribution=Formal properties of grammar | pages=323&amp;mdash;418 | editor1=R.D. Luce | editor2= R.R. Bush | editor3= E. Galanter | title=Handbook of Mathematical Psychology | location=New York | publisher=Wiley | series= | volume=II | year=1963 |url=https://archive.org/details/handbookofmathem017893mbp }}&lt;/ref&gt;&lt;ref name="Bach.Miller.2003"/&gt;

==Notes==
{{reflist|group=note}}

==References==
{{Reflist}}


{{DEFAULTSORT:Equivalence}}
[[Category:Formal languages]]</text>
      <sha1>peycxdone43yk38m3r7op21nury72pn</sha1>
    </revision>
  </page>
  <page>
    <title>Fitch notation</title>
    <ns>0</ns>
    <id>7316682</id>
    <revision>
      <id>827587308</id>
      <parentid>815189980</parentid>
      <timestamp>2018-02-25T16:16:25Z</timestamp>
      <contributor>
        <ip>2003:65:E80D:E95:88CA:4213:F2AE:1645</ip>
      </contributor>
      <comment>Dead link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3781">'''Fitch notation''', also known as '''Fitch diagrams''' (named after [[Frederic Fitch]]), is a notational system for constructing [[formal proof]]s used in [[sentential logic|sentential logics]] and [[predicate logic|predicate logics]]. Fitch-style proofs arrange the sequence of sentences that make up the proof into rows. A unique feature of Fitch notation is that the degree of indentation of each row conveys which assumptions are active for that step.

== Example ==

Each row in a Fitch-style proof is either:
* an assumption or subproof assumption.
* a sentence justified by the citation of (1) a rule of inference and (2) the prior line or lines of the proof that license that rule.

Introducing a new assumption increases the level of indentation, and begins a new vertical "scope" bar that continues to indent subsequent lines until the assumption is discharged. This mechanism immediately conveys which assumptions are active for any given line in the proof, without the assumptions needing to be rewriten on every line (as with sequent-style proofs).

The following example displays the main features of Fitch notation:

&lt;pre&gt;
0 |__                        [assumption, want P iff not not P]
1 |   |__ P                  [assumption, want not not P]
2 |   |   |__ not P          [assumption, for reductio]
3 |   |   |   contradiction  [contradiction introduction: 1, 2]
4 |   |   not not P          [negation introduction: 2]
  |
5 |   |__ not not P          [assumption, want P]
6 |   |   P                  [negation elimination: 5]
  |
7 |   P iff not not P        [biconditional introduction: 1 - 4, 5 - 6]
&lt;/pre&gt;
0.  The null assumption, ''i.e.'', we are proving a [[tautology (logic)|tautology]]&lt;br/&gt;
1.  Our first subproof: we assume the l.h.s. to show the r.h.s. follows&lt;br/&gt;
2.  A subsubproof: we are free to assume what we want. Here we aim for a [[reductio ad absurdum]]&lt;br/&gt;
3.  We now have a contradiction&lt;br/&gt;
4.  We are allowed to prefix the statement that "caused" the contradiction with a not&lt;br/&gt;
5.  Our second subproof: we assume the r.h.s. to show the l.h.s. follows&lt;br/&gt;
6.  We invoke the rule that allows us to remove an even number of nots from a statement prefix&lt;br/&gt;
7.  From 1 to 4 we have shown if P then not not P, from 5 to 6 we have shown P if not not P; hence we are allowed to introduce the biconditional

== See also ==

* [[Natural deduction]]

== References ==

* [[Frederic Brenton Fitch]], ''Symbolic Logic: An introduction'', Ronald Press Co., 1952.
* [[Jon Barwise]] and [[John Etchemendy]], [[Language, Proof and Logic]] [https://web.archive.org/web/20130903114953/http://ssdi.di.fct.unl.pt/~pb/cadeiras/lc/0102/lpl%20textbook.pdf ''1st edition as PDF''], Seven Bridges Press and CSLI, 1999.

== External links ==
* [http://plato.stanford.edu/entries/fitch-paradox/ Fitch's Paradox of Knowability]
* [http://logik.phl.univie.ac.at/~chris/gateway/formular-uk-fitch.html An online Java application for proof building]
* [http://www.proofmood.com/ A Web implementation of Fitch proof system (propositional and first-order) at www.proofmod.com]
* [http://japeforall.org.uk The Jape general-purpose proof assistant] (see [[Jape (software)|Jape]])
* [http://www.logicmatters.net/latex-for-logicians/nd/ Resources for typesetting proofs in Fitch notation with LaTeX] (see [[LaTeX]])
* [https://mrieppel.github.io/fitchjs/ FitchJS: An open source web app to construct proofs in Fitch notation (and export to LaTeX)]
&lt;!-- This page is about fitch calculus, rather than the notational system, but helps illustrate the difference. This Wikipedia entry should do a better job of that too.
* [http://www.cc.utah.edu/~nahaj/logic/structures/systems/fitch.html Fitch Calculus]
--&gt;

[[Category:Philosophical logic]]
[[Category:Logical calculi]]</text>
      <sha1>9nln4ffyrbwrmdl4fhy4it8ridf4ao6</sha1>
    </revision>
  </page>
  <page>
    <title>Forward anonymity</title>
    <ns>0</ns>
    <id>11343398</id>
    <revision>
      <id>851352029</id>
      <parentid>847621872</parentid>
      <timestamp>2018-07-21T18:05:58Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v485)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5662">{{multiple issues|
{{expert needed|1=Cryptography|date=June 2013}}
{{cleanup|reason=more links, context, proper definition first, link to parent topic per WP:LEAD|date=June 2013}}
{{context|date=June 2013}}
{{refimprove|date=July 2007}}
}}
'''Forward anonymity''', analogous to [[forward secrecy]], in [[computer security]] and [[cryptography]] is the property which prevents an attacker who has recorded past communications from discovering the identities of the participants, even after the fact.

When speaking of forward secrecy, system designers attempt to prevent an attacker who has recorded past communications from discovering the contents of said communications later on. One example of a system which satisfies the perfect forward secrecy property is one in which a compromise of one key by an attacker (and consequent [[decryption]] of messages encrypted with that key) does not undermine the security of previously used keys. Forward secrecy does not refer to protecting the content of the message, but rather to the protection of keys used to decrypt messages.

One example of a system which uses forward anonymity is a [[public key cryptography]] system, where the public key is well-known and used to [[encrypt]] a message, and an unknown private key is used to decrypt it. In this system, one of the keys is always said to be compromised, but messages and their participants are still unknown by anyone without the corresponding private key.

==History==
Originally introduced by  [[Whitfield Diffie]], [[Paul van Oorschot]], and Michael James Wiener to describe a property of STS ([[station-to-station protocol]]) involving a long term secret, either a private key or a shared password.&lt;ref&gt;{{cite journal |last1=Diffie |first1=Whitfield |last2=Van Oorschot |first2=Paul C. |last3=Wiener |first3=Michael J. |title=Authentication and authenticated key exchanges |journal=Designs, Codes and Cryptography |date=June 1992 |volume=2 |issue=2 |pages=107–125 |doi=10.1007/BF00124891 |url=http://people.scs.carleton.ca/~paulv/papers/sts-final.pdf}}&lt;/ref&gt;

==Public Key Cryptography==
Public Key Cryptography is a common form of a forward anonymous system. It is used to pass encrypted messages, preventing any information about the message from being discovered if the message is intercepted by an attacker. It uses two keys, a public key and a private key. The public key is published, and is used by anyone to encrypt a [[plaintext]] message. The Private key is not well known, and is used to decrypt [[cyphertext]]. Public key cryptography is known as an asymmetric decryption algorithm because of different keys being used to perform opposing functions. Public key cryptography is popular because, while it is computationally easy to create a pair of keys, it is extremely difficult to determine the private key knowing only the public key. Therefore, the public key being well known does not allow messages which are intercepted to be decrypted. This is a forward anonymous system because one compromised key (the public key) does not compromise the anonymity of the system.

==Web of Trust==
A variation of the public key cryptography system is a [[Web of trust]], where each user has both a public and private key. Messages sent are encrypted using the intended recipient's public key, and only this recipient's private key will decrypt the message. They are also signed with the senders private key. This creates added security where it becomes more difficult for an attacker to pretend to be a user, as the lack of a private key signature indicates a non-trusted user.

==Limitations==
A forward anonymous system does not necessarily mean a wholly secure system. A successful [[cryptanalysis]] of a message or sequence of messages can still decode the information without the use of a private key or long term secret.

==News==
Forward anonymity, along with other cryptography related properties, received a burst of media attention after the leak of classified information by [[Edward Snowden]], beginning in June, 2013, which indicated that the [[NSA]] and [[FBI]] had practices of asking companies to leave in back doors for them, allowing the companies and agencies to decrypt information stored on phones and other devices more easily, with the intention of allowing them to more easily find and arrest various criminals, while occasionally mistakenly targeting innocent civilians. They especially publicized the aid this practice provided in catching predatory [[pedophiles]].&lt;ref&gt;{{cite news |last1=Pagliery |first1=Jose |title=FBI director: iPhones shields pedophiles from cops |url=http://money.cnn.com/2014/10/13/technology/security/fbi-apple/ |publisher=CNN |date=14 October 2014 |dead-url=no |archiveurl=https://web.archive.org/web/20180116194600/http://money.cnn.com/2014/10/13/technology/security/fbi-apple/ |archivedate=2018-01-16}}&lt;/ref&gt; Opponents to this practice argue that leaving in a back door to law enforcement increases the risk of attackers being able to decrypt information, as well as questioning its legality under the [[US Constitution]], specifically being a form of illegal [[Search and Seizure]].&lt;ref&gt;{{cite news |last1=van den Heuvel |first1=Katrina |last2=Cohen |first2=Stephen |title=Edward Snowden: A ‘Nation’ Interview |url=http://www.thenation.com/article/186129/snowden-exile-exclusive-interview |work=[[The Nation]] |date=28 October 2014 |dead-url=no |archiveurl=https://web.archive.org/web/20150204105726/http://www.thenation.com/article/186129/snowden-exile-exclusive-interview |archivedate=2015-02-04}}&lt;/ref&gt;

==References==
{{Reflist}}

[[Category:Cryptography]]
[[Category:Computer security]]</text>
      <sha1>silpzkxniidmhapculu4qewvd46f36o</sha1>
    </revision>
  </page>
  <page>
    <title>Fractal in soil mechanics</title>
    <ns>0</ns>
    <id>15495829</id>
    <revision>
      <id>823674198</id>
      <parentid>814803213</parentid>
      <timestamp>2018-02-02T18:17:50Z</timestamp>
      <contributor>
        <username>Lewis Fausak</username>
        <id>32983802</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1736">{{Multiple issues|
{{POV-check|date=July 2008}}
{{original research|date=April 2008}}
}}

The '''fractal approach to soil mechanics''' is a new line of thought. It was first raised in "Fractal Character Of Grain-Size Distribution Of Expansion Soils" by Yongfu Xu and Songyu Liu, published in 1999, by Fractals. There are several problems in [[soil mechanics]] which can be dealt by applying a fractal approach. One of these problems is the determination of soil-water-characteristic curve (also called ([[water retention curve]]) and/or [[capillary pressure]] curve). It is a time-consuming process considering usual laboratory experiments. Many scientists have been involved in making mathematical models of soil-water-characteristic curve (SWCC) in which constants are related to the fractal dimension of pore size distribution or particle size distribution of the soil. After the great mathematician [[Benoît Mandelbrot]]—father of [[Fractal|fractal mathematics]]—showed the world fractals, Scientists of [[Agronomy]], [[Agricultural engineering]] and Earth Scientists have developed more fractal-based models. 
It is noteworthy that all of these models have been used to extract [[hydraulic]] properties of soils and the potential capabilities of fractal mathematics to investigate mechanical properties of soils. Therefore, it is really important to use such physically based models to promote our understanding of the mechanics of the soils. It can be of great help for researchers in the area of unsaturated soil mechanics. Mechanical parameters can also be driven from such models and of course it needs further works and researches.

[[Category:Soil mechanics]]
[[Category:Fractals]]


{{soil-sci-stub}}
{{Hydrology-stub}}</text>
      <sha1>9r1d44lhdmqbzy0bbxni238rcseb0ff</sha1>
    </revision>
  </page>
  <page>
    <title>Gardner–Salinas braille codes</title>
    <ns>0</ns>
    <id>4595591</id>
    <revision>
      <id>854291491</id>
      <parentid>851663450</parentid>
      <timestamp>2018-08-10T08:04:18Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <minor/>
      <comment>&amp;ang -&gt; ∠, etc. for readability</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11692">The '''Gardner–Salinas''' [[braille code]]s are a method of encoding mathematical and scientific notation linearly using [[braille cell]]s for tactile reading by the visually impaired. The most common form of Gardner–Salinas braille is the 8-cell variety, commonly called ''GS8''. There is also a corresponding 6-cell form called ''GS6''.&lt;ref name=OSU&gt;{{cite web |title=Index of Topics in Braille Section |url=http://dots.physics.orst.edu/gs_index.html |work=Oregon State University Science Access Project Braille topics. |accessdate=2012-04-29 |deadurl=yes |archiveurl=https://web.archive.org/web/20120420124208/http://dots.physics.orst.edu/gs_index.html |archivedate=2012-04-20 |df= }}&lt;/ref&gt;

The codes were developed as a replacement for [[Nemeth Braille]] by [[John A. Gardner]], a physicist at [[Oregon State University]], and Norberto Salinas, an Argentinian mathematician.

The Gardner–Salinas braille codes are an example of a compact human-readable [[markup language]]. The syntax is based on the [[LaTeX]] system for scientific typesetting.{{Citation needed|date=September 2014}}

{{See also|Braille|Braille music}}

==Table of Gardner–Salinas 8-dot (GS8) braille==

The set of lower-case letters, the period, comma, semicolon, colon, exclamation mark, apostrophe, and opening and closing double quotes are the same as in Grade-2 [[English Braille]].&lt;ref name=OSU/&gt;

=== Digits ===

{| class="wikitable" style="text-align:center"
|- 
! Symbol !! 0 !! 1 !! 2 !! 3 !! 4 !! 5 !! 6 !! 7 !! 8 !! 9
|-
! [[Braille]]
| {{Braille Cell|type=8|346}} || {{Braille Cell|type=8|16}} || {{Braille Cell|type=8|126}} || {{Braille Cell|type=8|146}} || {{Braille Cell|type=8|1456}} || {{Braille Cell|type=8|156}} || {{Braille Cell|type=8|1246}} || {{Braille Cell|type=8|12456}} || {{Braille Cell|type=8|1256}} || {{Braille Cell|type=8|246}} 
|}

Apart from 0, this is the same as the [[Antoine notation]] used in [[French Braille|French]] and [[Luxembourgish Braille]].

=== Upper-case letters ===

GS8 upper-case letters are indicated by the same cell as standard English braille (and GS8) lower-case letters, with dot #7 added.

{| class="wikitable" style="text-align:center"
|- 
! Symbol !! A !! B !! C !! D !! E !! F !! G !! H !! I !! J !! K !! L !! M
|-
! [[Braille]]
| {{Braille Cell|type=8|17}} || {{Braille Cell|type=8|127}} || {{Braille Cell|type=8|147}} || {{Braille Cell|type=8|1457}} || {{Braille Cell|type=8|157}} || {{Braille Cell|type=8|1247}} || {{Braille Cell|type=8|12457}} || {{Braille Cell|type=8|1257}} || {{Braille Cell|type=8|247}} || {{Braille Cell|type=8|2457}} || {{Braille Cell|type=8|137}} || {{Braille Cell|type=8|1237}} || {{Braille Cell|type=8|1347}} 
|- 
! Symbol !! N !! O !! P !! Q !! R !! S !! T !! U !! V !! W !! X !! Y !! Z
|-
! [[Braille]]
| {{Braille Cell|type=8|13457}} || {{Braille Cell|type=8|1357}} || {{Braille Cell|type=8|12347}} || {{Braille Cell|type=8|123457}} || {{Braille Cell|type=8|12357}} || {{Braille Cell|type=8|2347}} || {{Braille Cell|type=8|23457}} || {{Braille Cell|type=8|1367}} || {{Braille Cell|type=8|12367}} || {{Braille Cell|type=8|24567}} || {{Braille Cell|type=8|13467}} || {{Braille Cell|type=8|134567}} || {{Braille Cell|type=8|13567}} 
|}

Compare [[Luxembourgish Braille]].

=== Greek letters ===
Dot 8 is added to the letter forms of [[International Greek Braille]] to derive Greek letters: 

{| class="wikitable" style="text-align:center"
|- 
! Symbol !! α !! β !! γ !! δ !! ε !! ζ !! η !! θ !! ι !! κ !! λ !! μ 
|-
! [[Braille]]
| {{Braille Cell|type=8|18}} || {{Braille Cell|type=8|128}} || {{Braille Cell|type=8|12458}} || {{Braille Cell|type=8|1458}} || {{Braille Cell|type=8|158}} || {{Braille Cell|type=8|13568}} || {{Braille Cell|type=8|1568}} || {{Braille Cell|type=8|14568}} || {{Braille Cell|type=8|248}} || {{Braille Cell|type=8|138}} || {{Braille Cell|type=8|1238}} || {{Braille Cell|type=8|1348}} 
|- 
! Symbol !! ν !! ξ !! ο !! π !! ρ !! σ !! τ !! υ !! φ !! χ !! ψ !! ω
|-
! [[Braille]]
| {{Braille Cell|type=8|13458}} || {{Braille Cell|type=8|13468}} || {{Braille Cell|type=8|1358}} || {{Braille Cell|type=8|12348}} || {{Braille Cell|type=8|12358}} || {{Braille Cell|type=8|2348}} || {{Braille Cell|type=8|23458}} || {{Braille Cell|type=8|1368}} || {{Braille Cell|type=8|1248}} || {{Braille Cell|type=8|123468}} || {{Braille Cell|type=8|134568}} || {{Braille Cell|type=8|24568}} 
|- 
! Symbol !! Γ !! Δ !! Θ !! Λ !! Ξ !! Π !! Σ !! Υ !! Φ !! Ψ !! Ω 
|-
! [[Braille]]
| {{Braille Cell|type=8|124578}} || {{Braille Cell|type=8|14578}} || {{Braille Cell|type=8|145678}} || {{Braille Cell|type=8|12378}} || {{Braille Cell|type=8|134678}} || {{Braille Cell|type=8|123478}} || {{Braille Cell|type=8|23478}} || {{Braille Cell|type=8|13678}} || {{Braille Cell|type=8|12478}} || {{Braille Cell|type=8|1345678}} || {{Braille Cell|type=8|245678}} 
|}

=== Characters differing from English Braille ===

{| class="wikitable" style="text-align:center"
|- 
!rowspan=2| Symbol !!colspan=2| Parentheses !!colspan=2| Brackets !!colspan=2| single quote !! &amp;nbsp;
|-
! open !! close !! open !! close !! open !! close !! ?
|-
! [[Braille]]
| {{Braille Cell|type=8|238}} || {{Braille Cell|type=8|457}} &lt;!--Seems like it should be 567. Needs to be confirmed.--&gt; || {{Braille Cell|type=8|23678}} || {{Braille Cell|type=8|35678}} || {{Braille Cell|type=8|378}} || {{Braille Cell|type=8|3}} || {{Braille Cell|type=8|2356}} 
|}

=== ASCII symbols and mathematical operators ===

{| class="wikitable" style="text-align:center"
|- 
! Symbol !! " !! $ !! &amp; !! @ !! \ !! ^ !! _ !! ` !! { !! } !! ~
|-
! [[Braille]]
| {{Braille Cell|type=8|2578}} || {{Braille Cell|type=8|34567}} || {{Braille Cell|type=8|12346}} || {{Braille Cell|type=8|348}} || {{Braille Cell|type=8|1678}} || {{Braille Cell|type=8|57}} || {{Braille Cell|type=8|28}} || {{Braille Cell|type=8|378}} || {{Braille Cell|type=8|1235678}} || {{Braille Cell|type=8|2345678}} || {{Braille Cell|type=8|678}} 
|- 
! Symbol !! # !! % !! * !! + !! / !! &lt; !! = !! &gt; !! × !! · !! ÷
|-
! [[Braille]]
| {{Braille Cell|type=8|3456}} || {{Braille Cell|type=8|2345}} || {{Braille Cell|type=8|3458}} || {{Braille Cell|type=8|257}} || {{Braille Cell|type=8|3478}} || {{Braille Cell|type=8|358}} || {{Braille Cell|type=8|235678}} || {{Braille Cell|type=8|267}} || {{Braille Cell|type=8|278}} || {{Braille Cell|type=8|258}} || {{Braille Cell|type=8|578}} 
|}

=== Text symbols ===

{| class="wikitable" style="text-align:center"
|- 
! Symbol !! • !! © !! † !! ‡ !! £
|-
! [[Braille]]
| {{Braille Cell|type=8|123568}} || {{Braille Cell|type=8|148}} || {{Braille Cell|type=8|124568}} || {{Braille Cell|type=8|1234568}} || {{Braille Cell|type=8|34568}} 
|}

=== Math and science symbols ===

{| class="wikitable" style="text-align:center"
|- 
! Symbol !! ∼ !! ≈ !! ≡ !! ∝ !! &amp;#124;x&amp;#124; (abs. value) !! ∞ !! ∫ !! line ∫ !! closed ∫
|-
! [[Braille]]
| {{Braille Cell|type=8|23567}} || {{Braille Cell|type=8|23568}} || {{Braille Cell|type=8|12345678}} || {{Braille Cell|type=8|3678}} || {{Braille Cell|type=8|12568}} || {{Braille Cell|type=8|1258}} || {{Braille Cell|type=8|234678}} || {{Braille Cell|type=8|23467}} || {{Braille Cell|type=8|23468}} 
|}

{| class="wikitable" style="text-align:center"
|- 
! Symbol !! 〈 !! 〉 !! ∇ !! ∂ !! ... !! → !! ← !! ○ !! ℵ !! ∈ !! ⇌
|-
! [[Braille]]
| {{Braille Cell|type=8|24678}} || {{Braille Cell|type=8|13578}} || {{Braille Cell|type=8|14678}} || {{Braille Cell|type=8|1468}} || {{Braille Cell|type=8|368}} || {{Braille Cell|type=8|2367}} || {{Braille Cell|type=8|3568}} || {{Braille Cell|type=8|2458}} || {{Braille Cell|type=8|15678}} || {{Braille Cell|type=8|1578}} || {{Braille Cell|type=8|234567}} 
|}

=== Markup ===

{| class="wikitable" style="text-align:center"
|- 
! Symbol !! Superscript !! Subscript !! Left-superscript !! Left-subscript !! Begin fraction !! Denominator !! End fraction !! Over *
|-
! [[Braille]]
| {{Braille Cell|type=8|35}} || {{Braille Cell|type=8|26}} || {{Braille Cell|type=8|3578}} || {{Braille Cell|type=8|2678}} || {{Braille Cell|type=8|12356}} || {{Braille Cell|type=8|34}} || {{Braille Cell|type=8|23456}} || {{Braille Cell|type=8|345}} 
|}

&lt;nowiki&gt;*&lt;/nowiki&gt; Encodes the fraction-slash for the single adjacent digits/letters as numerator and denominator.

{| class="wikitable" style="text-align:center"
|- 
!rowspan=2| Symbol !!rowspan=2| √ !!colspan=2| complex radicand * !!colspan=2| displayed equation ** !!colspan=2| math expressions ** !!colspan=2| hyperlink **
|-
| Open || Close || Open || Close || Open || Close || Open || Close 
|-
! [[Braille]]
| {{Braille Cell|type=8|2468}} || {{Braille Cell|type=8|2378}} || {{Braille Cell|type=8|5678}} || {{Braille Cell|type=8|123678}} || {{Braille Cell|type=8|345678}} || {{Braille Cell|type=8|23578}} || {{Braille Cell|type=8|25678}} || {{Braille Cell|type=8|23578}} || {{Braille Cell|type=8|25678}} 
|}

&lt;nowiki&gt;*&lt;/nowiki&gt; Used for any &gt; 1 digit radicand.

&lt;nowiki&gt;**&lt;/nowiki&gt; Used for markup to represent inkprint text.

{| class="wikitable" style="text-align:center"
|- 
!rowspan=2| Symbol !!colspan=4| Array !! &amp;nbsp; !! &amp;nbsp; !! &amp;nbsp;
|-
! Begin !! End !! End element !! End line !! vert. stack !! horiz. combo !! superposition
|-
! [[Braille]]
| {{Braille Cell|type=8|38}} || {{Braille Cell|type=8|67}} || {{Braille Cell|type=8|27}} || {{Braille Cell|type=8|237}} || {{Braille Cell|type=8|3467}} || {{Braille Cell|type=8|34678}} || {{Braille Cell|type=8|3468}} 
|}

{| class="wikitable" style="text-align:center"
|- 
!rowspan=2| Symbol !!colspan=2| Misc. Symbol * !! &amp;nbsp; !! &amp;nbsp; !!colspan=4| Modifiers
|-
! Begin !! End !! Quantity !! Markup indicator !! Inverted !! Stroke/Not !! Variant !! Large
|-
! [[Braille]]
| {{Braille Cell|type=8|37}} || {{Braille Cell|type=8|7}} || {{Braille Cell|type=8|123456}} || {{Braille Cell|type=8|367}} || {{Braille Cell|type=8|78}} || {{Braille Cell|type=8|347}} || {{Braille Cell|type=8|168}} || {{Braille Cell|type=8|1234567}} 
|}

=== Typeface indicators ===
{| class="wikitable" style="text-align:center"
|- 
! Symbol !! Script !! Bold !! Italic !! Underline !! Definable font 1 !! Fractur !! Roman !! Underline
|-
! [[Braille]]
| {{Braille Cell|type=8|48}} || {{Braille Cell|type=8|458}} || {{Braille Cell|type=8|468}} || {{Braille Cell|type=8|4568}} || {{Braille Cell|type=8|478}} || {{Braille Cell|type=8|4578}} || {{Braille Cell|type=8|4678}} || {{Braille Cell|type=8|45678}} 
|}

=== Shape symbols ===
{| class="wikitable" style="text-align:center"
|- 
! Symbol !! ⊥ !! ∥ !! ∠ !! Open Square !! ⊿
|-
! [[Braille]]
| {{Braille Cell|type=8|8|2378}} || {{Braille Cell|type=8|568|237}} || {{Braille Cell|type=8|8|3578}} || {{Braille Cell|type=8|568|25678}} || {{Braille Cell|type=8|568|378}} 
|}

=== Set theory ===
{| class="wikitable" style="text-align:center"
|- 
! Symbol !! ∪ !! ∩ !! ⊂ !! ⊃
|-
! [[Braille]]
| {{Braille Cell|type=8|568|5678}} || {{Braille Cell|type=8|568|2568}} || {{Braille Cell|type=8|568|2578}} || {{Braille Cell|type=8|58|25678}} 
|}

&lt;!-- ===  ===
{| class="wikitable" style="text-align:center"
|- 
! Symbol !! 
|-
! [[Braille]]
| {{Braille Cell|type=8|}} |
|}
--&gt;

== References ==

{{reflist}}
*[http://www-cgi.cnn.com/TECH/9511/new_braille/index.html Blind physicist creates better Braille] — a [[Cable News Network|CNN]] news item, November 9, 1995
*[http://www.ams.org/notices/200210/comm-morin.pdf The world of blind mathematicians] — article in ''Notices of the [[American Mathematical Society|AMS]]'', November 2002

{{Braille}}

{{DEFAULTSORT:Gardner-Salinas Braille}}
[[Category:Braille symbols]]
[[Category:8-dot braille scripts]]
[[Category:Mathematical notation]]</text>
      <sha1>036jiquibd3hffvea59p5vl6smrj0nw</sha1>
    </revision>
  </page>
  <page>
    <title>Garside element</title>
    <ns>0</ns>
    <id>20247295</id>
    <revision>
      <id>871170825</id>
      <parentid>784879785</parentid>
      <timestamp>2018-11-29T11:46:29Z</timestamp>
      <contributor>
        <username>Stephanie (Oxford)</username>
        <id>3131340</id>
      </contributor>
      <comment>/* Garside monoid and Garside group */ Some information on Garside the person added</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2821">{{Orphan|date=May 2014}}

In [[mathematics]], a '''Garside element''' is an element of an [[algebraic structure]] such as a [[monoid]] that has several desirable properties.

Formally, if ''M'' is a monoid, then an element &amp;Delta; of ''M'' is said to be a '''Garside element''' if the set of all right divisors of &amp;Delta;,
:&lt;math&gt;\{ r \in M \mid \mbox{for some } x \in M, \Delta = x r \},&lt;/math&gt;
is the same set as the set of all left divisors of &amp;Delta;,
:&lt;math&gt;\{ \ell \in M \mid \mbox{for some } x \in M, \Delta = \ell x \},&lt;/math&gt;
and this set [[generating set|generates]] ''M''.

A Garside element is in general not unique: any power of a Garside element is again a Garside element.

==Garside monoid and Garside group==
A '''Garside monoid''' is a monoid with the following properties:
* Finitely generated and atomic;
* [[Cancellative]];
* The [[Partially ordered set|partial order]] relations of divisibility are [[Lattice (order)|lattices]];
* There exists a Garside element.

A Garside monoid satisfies the [[Ore condition for multiplicative sets]] and hence embeds in its group of fractions: such a group is a '''Garside group'''.  A Garside group is [[biautomatic group|biautomatic]] and hence has soluble [[Word problem for groups|word problem]] and [[conjugacy problem]].  Examples of such groups include [[braid group]]s and, more generally, [[Artin group]]s of [[Artin group of finite type|finite Coxeter type]].&lt;ref name=dehorneyparis&gt;{{citation | last1=Dehornoy | first1=Patrick | last2=Paris | first2=Luis | title=Gaussian groups and Garside groups, two generalisations of Artin groups | journal=Proceedings of the London Mathematical Society | volume=79 | number=3 | pages=569–604 | year=1999 | doi=10.1112/s0024611599012071}}&lt;/ref&gt;

The name was coined by Dehornoy and Paris&lt;ref name=dehorneyparis/&gt; to mark the work of Frank Arnold Garside (1915-1988), a teacher at Magdalen College School, Oxford who served as Lord Mayor of Oxford in 1984-5, on the conjugacy problem for braid groups.&lt;ref&gt;{{citation | last=Garside | first=F.A. | title=The braid group and other groups | journal=Q. J. Math., Oxf. II. Ser. | volume=20 | pages=235–254 | year=1969 | doi=10.1093/qmath/20.1.235}}&lt;/ref&gt;

==References==
{{reflist}}
* Benson Farb, ''Problems on mapping class groups and related topics'' (Volume 74 of Proceedings of symposia in pure mathematics) AMS Bookstore, 2006, {{ISBN|0-8218-3838-5}}, p.&amp;nbsp;357
* Patrick Dehornoy, "Groupes de Garside", ''Ann .Sci. Ecole Norm. Sup. (4)'' '''35''' (2002) 267-306.  [[Mathematics Reviews|MR]] 2003f:20067.
* Matthieu Picantin, "Garside monoids vs divisibility monoids", ''Math. Structures Comput. Sci.'' '''15''' (2005) 231-242.  [[Mathematics Reviews|MR]] 2006d:20102.

[[Category:Abstract algebra]]
[[Category:Semigroup theory]]


{{Abstract-algebra-stub}}</text>
      <sha1>e6i7pw81fk9i05rywp4gk8egtp8owyi</sha1>
    </revision>
  </page>
  <page>
    <title>Gentleware</title>
    <ns>0</ns>
    <id>24139370</id>
    <revision>
      <id>753134433</id>
      <parentid>712594866</parentid>
      <timestamp>2016-12-05T10:45:27Z</timestamp>
      <contributor>
        <ip>91.222.250.74</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1187">'''Gentleware AG''' is a software company headquartered in [[Hamburg]], [[Germany]]. Gentleware was founded in 2000 with software based in the [[open source]] project [[ArgoUML]]. The company is best known for [[Poseidon for UML]], the world's most downloaded commercial [[UML tool]], with over 1,200,000 copies distributed to over 100 countries.&lt;ref name="company"&gt;{{
cite web
|author=Gentleware AG 
|title=Company Overview
|url=http://www.gentleware.com/company.html
|accessdate=2009-08-27}}&lt;/ref&gt;

Since 2004 [[Gentleware Ukraine]], has been the center of the development of the German company Gentleware AG and provides complete development, support and sales of such products as Poseidon for UML, Apollo for Eclipse, Poseidon for DSLs.

== References ==
{{reflist}}

==External links==
* [http://www.gentleware.com/ Gentleware AG corporate web]
* [http://rozdoum.com/blog_articles/items/rozdoum-announces-merge-with-gentleware-ukraine.html Gentleware Ukraine announces merge with Rozdoum]
{{UML}}
{{SysML}}
{{Software engineering}}

[[Category:Software companies of Germany]]
[[Category:SysML Partners]]
[[Category:Companies based in Hamburg]]
[[Category:Unified Modeling Language]]</text>
      <sha1>6o4axxx6snt925pewa83xjuy7pjgpdn</sha1>
    </revision>
  </page>
  <page>
    <title>Gul (design)</title>
    <ns>0</ns>
    <id>26758463</id>
    <revision>
      <id>785306489</id>
      <parentid>758089286</parentid>
      <timestamp>2017-06-12T19:59:55Z</timestamp>
      <contributor>
        <username>Magic links bot</username>
        <id>30707369</id>
      </contributor>
      <minor/>
      <comment>Replace [[Help:Magic links|magic links]] with templates per [[Special:Permalink/772743896#Future of magic links|local RfC]] and [[:mw:Requests for comment/Future of magic links|MediaWiki RfC]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5806">[[File:Konya 18th carpet with Memling gul design.jpg|thumb|[[Konya]] 18th carpet with Memling gul design. There is a row of triangular [[amulet]] [[kilim motifs|motifs]] (Muska) at top and bottom; each of the four lower guls has a star motif (Yıldız) at its centre.]]
[[File:Oriental rugs, antique and modern (1922) (14780627715).jpg|thumb|[[Turkmens|Turkmen]] carpet with 3 central gul medallions]]

A '''gul''' (also written gol, göl and gül) is a medallion-like design element typical of traditional hand-[[Weaving|woven]] carpets from [[Central Asia|Central]] and [[West Asia]]. In Turkmen weavings they are often repeated to form the pattern in the main field.

==Shape==

Guls are medallions, often [[octagon]]al, and often somewhat angular on a generally octagonal plan, though they can be somewhat rounded within the constraints of carpet-weaving, and some are lozenge-shaped ([[rhombus]]es). They usually have either [[rotational symmetry#n-fold rotational symmetry|twofold rotational symmetry]] or mirror [[reflection symmetry]] (often both left/right and up/down).&lt;ref&gt;{{cite web |title=Rug Gallery. Carpet Turkey (Kurdish) 18th century |url=http://mathforum.org/geometry/rugs/gallery/08.html |publisher=MathForum |accessdate=28 January 2016 |date=2012}}&lt;/ref&gt;

Guls were historically described in the West as being elephant's foot motifs. Other Western guesses held that the gul was a drawing of a round Turkmen tent, with lines between tents representing irrigation canals; or that the emblem was a [[totem]]ic bird. None of these descriptions have any basis in weaving tradition or culture.&lt;ref name=ThompsonElephant&gt;{{cite book |last1=Thompson |first1=Jon |title=Carpets from the Tents, Cottages and Workshops of Asia |date=1988 |publisher=Barrie &amp; Jenkins |isbn=0-7126-2501-1 |page=156}}&lt;/ref&gt;

==Etymology==

The term gul, gol, göl or gül is used widely across Central and West Asia, and among carpet specialists in the West. It may derive from the [[Persian language|Persian]] word gol, which means flower or rose,&lt;ref name=LittlePersia&gt;{{cite web |title=Rug Layouts and Designs|url=http://www.little-persia.com/?action=RugDesign#gul |publisher=Little Persia |accessdate=28 January 2016 |date=2015}}&lt;/ref&gt; or from the Turkish word gül which similarly means a rose or roundel.&lt;ref name=Thompson&gt;{{cite book |last1=Thompson |first1=Jon |title=Carpets from the Tents, Cottages and Workshops of Asia |date=1988 |publisher=Barrie &amp; Jenkins |isbn=0-7126-2501-1 |page=163}}&lt;/ref&gt;

==Usage==

In [[Turkmens|Turkmen]] weavings, such as bags and rugs, guls are often repeated to form the basic pattern in the main field (excluding the border).&lt;ref name=Thompson/&gt;&lt;ref name=Arastan&gt;{{cite web|title=Carpet Motifs: A Beginner’s Guide|url=http://arastan.com/journey/carpet-motifs-beginners-guide|publisher=Arastan|accessdate=28 January 2016|date=9 May 2012}}&lt;/ref&gt; &lt;!--This use of guls is occasionally found also in Turkish carpets, though they more often have one or a few guls arranged centrally in the main field.--&gt;

The different Turkmen tribes such as [[Teke tribe|Tekke]], [[Salor]], [[Ersari]] and [[Yomut]] traditionally wove a variety of guls, some of ancient design, but gul designs were often used by more than one tribe, and by non-Turkmens.&lt;ref name=Thompson/&gt;&lt;ref name=Arastan/&gt;

Western authors have used comparison of the "design vocabulary" of tribal guls, reproduced on traditional rugs, in studying the [[ethnogenesis]] of Asian peoples.&lt;ref name=Mace&gt;{{cite book |author1=Mace, Ruth |author2=Holden, Clare J. |author3=Shennan, Stephen |date=2005 |url=https://books.google.com/books?id=5YP_p5eS898C |title=The evolution of cultural diversity: a phylogenetic approach |publisher=Routledge |isbn=1-84472-099-3 |pages=118–120}}&lt;/ref&gt;

==In Western culture==
[[File:Hans Memling 065.jpg|thumb|Triptych by [[Hans Memling]], 1479, with a gul-patterned carpet at the Virgin's feet]]
{{further|Oriental carpets in Renaissance painting}}

Western artists including [[Hans Memling]] depicted [[oriental carpet]]s from Turkish Anatolia with guls in several of his paintings, to the extent that these are known as [[Memling carpet]]s. These guls often contain star or (hooked) dragon [[Kilim motifs|motifs]] as found on 15th century [[Konya]] carpets.&lt;ref name=Howe&gt;{{cite web |last1=Howe |first1=R. John |title=The "Memling" Gul Motif, The Lecture |url=https://rjohnhowe.wordpress.com/2010/10/14/the-memling-gul-motif-the-lecture/ |publisher=R. John Howe &lt;!--carpet expert--&gt; |accessdate=28 January 2016 |date=2 October 2010}}&lt;/ref&gt; The presence of the hooked motif defines a "Memling carpet".&lt;ref&gt;King, Donald and [[David Sylvester|Sylvester, David]] eds. ''The Eastern Carpet in the Western World, From the 15th to the 17th century'', [[Arts Council of Great Britain]], London, 1983, {{ISBN|0-7287-0362-9}}. page 57&lt;/ref&gt; The artists [[Lorenzo Lotto]] and [[Hans Holbein the Younger|Hans Holbein]] who similarly depicted Anatolian carpets also have the varieties they painted named after them.&lt;ref&gt;{{cite web|title=Turkish carpet|url=http://www.kilimcollection.com/about-turkish-kilim/9/turkish-carpet/|publisher=Kilim Collection|accessdate=28 January 2016|date=2016}}&lt;/ref&gt;

==See also==
* [[Islamic geometric patterns]]
* [[Kilim motifs]]
* [[Flag of Turkmenistan]] &lt;!--features guls--&gt;

==References==
{{reflist}}

==Further reading==
* Louise W. Mackie, Jon Thompson (1980). ''[https://books.google.com/books?id=jzTrAAAAMAAJ Turkmen, tribal carpets and traditions]''. [[Textile Museum (Washington, D.C.)]].

{{Ornaments}}
{{Islamic art}}

[[Category:Ornaments]]
[[Category:Symmetry]]
[[Category:Asian culture]]
[[Category:National symbols of Turkmenistan]]
[[Category:Turkic rugs and carpets]]
[[Category:Visual motifs]]
[[Category:Textile patterns]]
[[Category:Islamic art]]</text>
      <sha1>2aftdboyubeg1v3nhyknujgbhmbxlvi</sha1>
    </revision>
  </page>
  <page>
    <title>Hardware random number generator</title>
    <ns>0</ns>
    <id>160506</id>
    <revision>
      <id>867411555</id>
      <parentid>866778088</parentid>
      <timestamp>2018-11-05T15:20:52Z</timestamp>
      <contributor>
        <username>The Anome</username>
        <id>76</id>
      </contributor>
      <comment>more</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="32775">{{more citations needed|date=June 2014}}
[[File:Sun-crypto-accelerator-1000.jpg|thumb|right|This [[SSL acceleration|SSL Accelerator]] [[Hardware acceleration|computer card]] uses a hardware random number generator to generate [[cryptographic key]]s to encrypt data sent over computer networks.]]

In [[computing]], a '''hardware random number generator''' ('''HRNG''') or '''true random number generator''' ('''TRNG''') is a device that generates [[Random number generation|random numbers]] from a [[physical process]], rather than by means of an [[algorithm]].  Such devices are often based on microscopic phenomena that generate low-level, [[statistically random]] "noise" signals, such as [[thermal noise]], the [[photoelectric effect]], involving a [[beam splitter]], and other [[quantum]] phenomena. These [[stochastic]] processes are, in theory, completely unpredictable, and the theory's assertions of unpredictability are subject to experimental test. This is in contrast to the common paradigm of pseudo-random number generation commonly implemented in [[Computer program|computer programs]] or [[Cryptographic accelerator|cryptographic hardware]].

A hardware random number generator typically consists of a [[transducer]] to convert some aspect of the physical phenomena to an electrical signal, an [[amplifier]] and other electronic circuitry to increase the amplitude of the random fluctuations to a measurable level, and some type of [[analog to digital converter]] to convert the output into a digital number, often a simple binary digit 0 or 1. By repeatedly sampling the randomly varying signal, a series of random numbers is attained.

The main application for electronic hardware random number generators is in [[cryptography]], where they are used to generate random [[cryptographic key]]s to transmit data securely.  They are widely used in Internet encryption protocols such as [[Secure Sockets Layer]] (SSL).

Random number generators can also be built from "random" macroscopic processes, using devices such as [[coin flipping]], [[dice]], [[roulette]] wheels and [[lottery machine]]s. The presence of unpredictability in these phenomena can be justified by the theory of [[Instability|unstable]] [[dynamical system]]s and [[chaos theory]]. Even though macroscopic processes are deterministic under [[Newtonian mechanics]], the output of a well-designed device like a roulette wheel cannot be predicted in practice, because it depends on the sensitive, micro-details of the [[initial conditions]] of each use.

Although dice have been mostly used in [[gambling]], and as "randomizing" elements in games (e.g. [[role playing game]]s), the [[Victorian era|Victorian]] scientist [[Francis Galton]] described a way to use dice to explicitly generate random numbers for scientific purposes in 1890.&lt;ref&gt;{{cite journal|last=Galton|first=Francis|title=Dice for statistical experiments|journal=Nature|date=1890|volume=42|pages=13–14|url=http://galton.org/essays/1890-1899/galton-1890-dice.pdf|accessdate=14 May 2014|doi=10.1038/042013a0|bibcode=1890Natur..42...13G}}&lt;/ref&gt;

Hardware random number generators generally produce only a limited number of random bits per second.  In order to increase the available output data rate, they are often used to generate the "[[Random seed|seed]]" for a faster [[cryptographically secure pseudorandom number generator]], which then generates a [[pseudorandom]] output sequence at a much higher data rate.

== Uses ==
{{See also|Applications of randomness}}

Unpredictable random numbers were first investigated in the context of [[gambling]], and many randomizing devices such as [[dice]], [[shuffling playing cards]], and [[roulette]] wheels, were first developed for such use. Fairly produced random numbers are vital to electronic gambling and ways of creating them are sometimes regulated by governmental gaming commissions.

Random numbers are also used for non-gambling purposes, both where their use is mathematically important, such as sampling for [[opinion poll]]s, and in situations where fairness is approximated by [[randomization]], such as selecting [[juror]]s and [[Conscription|military draft lotteries]].

=== Cryptography ===
The major use for hardware random number generators is in the field of [[data encryption]], for example to create random [[cryptographic key]]s to encrypt data.  They are a more secure alternative to [[pseudorandom number generator]]s (PRNGs), software programs commonly used in computers to generate "random" numbers.  PRNGs use a [[Deterministic algorithm|deterministic]] [[algorithm]] to produce numerical sequences.  Although these pseudorandom sequences pass [[Randomness tests|statistical pattern tests for randomness]], by knowing the algorithm and the conditions used to initialize it, called the "seed", the output can be predicted.   Because the sequence of numbers produced by a PRNG is in principle predictable, data encrypted with pseudorandom numbers is potentially vulnerable to [[cryptanalysis]].  Hardware random number generators produce sequences of numbers that are assumed not to be predictable, and therefore provide the greatest security when used to encrypt data.

== Early work==
One early way of producing random numbers was by a variation of the same machines used to play [[keno]] or select [[lottery]] numbers.  These mixed numbered ping-pong balls with blown air, perhaps combined with mechanical agitation, and used some method to withdraw balls from the mixing chamber ({{US patent| 4786056}}).  This method gives reasonable results in some senses, but the random numbers generated by this means are expensive. The method is inherently slow, and is unusable for most computing applications.

On 29 April 1947, [[RAND Corporation]] began generating random digits with an "electronic roulette wheel", consisting of a random frequency pulse source of about 100,000 pulses per second gated once per second with a constant frequency pulse and fed into a five-bit binary counter. Douglas Aircraft built the equipment, implementing Cecil Hasting’s suggestion (RAND P-113)&lt;ref&gt;{{citation | url = http://www.rand.org/pubs/papers/P113 | publisher = Rand Corporation | title = P-113 | series = Papers}}.&lt;/ref&gt; for a noise source (most likely the well known behavior of the 6D4 miniature gas [[thyratron]] tube, when placed in a magnetic field&lt;ref&gt;{{citation | title = Electrical Noise Generators | last = Cobine, Curry | journal = Proceedings of the I.R.E. | year = 1947 | issue = September 1947 | pages = 875&amp;ndash;9}}&lt;/ref&gt;).  Twenty of the 32 possible counter values were mapped onto the 10 decimal digits and the other 12 counter values were discarded.&lt;ref&gt;{{citation | url = http://www.rand.org/pubs/monograph_reports/MR1418/index2.html | publisher = Rand Corporation | title = Monograph report}}.&lt;/ref&gt;

The results of a long run from the RAND machine, filtered and tested, were converted into a table, which was published in 1955 in the book ''[[A Million Random Digits with 100,000 Normal Deviates]]''.  The RAND table was a significant breakthrough in delivering random numbers because such a large and carefully prepared table had never before been available. It has been a useful source for simulations, modeling, and for deriving the arbitrary constants in cryptographic algorithms to demonstrate that the constants had not been selected maliciously. The block ciphers [[Khufu and Khafre]] are among the applications which use the RAND table.&lt;ref&gt;{{cite book|last1=Schneier|first1=Bruce|title=Applied Cryptography|publisher=John Wiley &amp; Sons, Inc.|isbn=0-471-11709-9|page=423|edition=Second|chapter=Other Stream Ciphers and Real Random-Sequence Generators}}&lt;/ref&gt; ''See:'' [[Nothing up my sleeve number]]s.

== Physical phenomena with random properties ==

=== Quantum random properties ===
There are two fundamental sources of practical [[Quantum mechanics|quantum mechanical]] physical randomness: quantum mechanics at the atomic or sub-atomic level and [[thermal noise]] (some of which is quantum mechanical in origin). Quantum mechanics predicts that certain physical phenomena, such as the [[nuclear decay]] of atoms,&lt;ref&gt;"Each nucleus decays spontaneously, at random, in accordance with the blind workings of chance." ''Q for Quantum'', [[John Gribbin]]&lt;/ref&gt; are [[Statistical interpretation|fundamentally random]] and cannot, in principle, be predicted (for a discussion of empirical verification of quantum unpredictability, see [[Bell test experiments]]).  And, because we live at a temperature above [[absolute zero]], every system has some random variation in its state; for instance, molecules of gases composing air are constantly bouncing off each other in a random way (''see'' [[statistical mechanics]].)  This randomness is a quantum phenomenon as well (''see'' [[phonon]]).

Because the outcome of quantum-mechanical events cannot be predicted even in principle, they are the ‘[[wikt:gold standard|gold standard]]’ for random number generation.  Some quantum phenomena used for random number generation include:

* [[Shot noise]], a quantum mechanical noise source in electronic circuits. A simple example is a lamp shining on a photodiode.  Due to the [[uncertainty principle]], arriving photons create noise in the circuit.  Collecting the noise for use poses some problems, but this is an especially simple random noise source.  However, shot noise energy is not always well distributed throughout the bandwidth of interest.  Gas diode and thyratron electron tubes in a crosswise magnetic field can generate substantial noise energy (10 volts or more into high impedance loads) but have a very peaked energy distribution and require careful filtering to achieve flatness across a broad spectrum.&lt;ref&gt;{{citation | publisher =  Sylvania | title = 6D4 electron tube reference}}.&lt;/ref&gt;
* A [[nuclear decay]] radiation source, detected by a [[Geiger counter]] attached to a PC.
* [[Photon]]s travelling through a [[Beam splitter|semi-transparent mirror]].  The [[mutually exclusive events]] (reflection/transmission) are detected and associated to ‘0’ or ‘1’ bit values respectively.
* [[Electronic amplifier|Amplification]] of the signal produced on the base of a [[P-n junction#Reverse bias|reverse-biased]] [[transistor]].  The emitter is saturated with electrons and occasionally they will [[Quantum tunneling|tunnel]] through the [[band gap]] and exit via the base.  This signal is then [[Electronic amplifier|amplified]] through a few more [[transistor]]s and the result fed into a [[Schmitt trigger]].
* [[Spontaneous parametric down-conversion]] leading to binary phase state selection in a degenerate [[optical parametric oscillator]].&lt;ref&gt;{{cite journal|last=Marandi|first=A.|author2=N. C. Leindecker |author3=K. L. Vodopyanov |author4=R. L. Byer |journal=Opt. Express|volume=20|pages= 19322–19330|year=2012|title=All-optical quantum random bit generation from intrinsically binary phase of parametric oscillators|doi=10.1364/OE.20.019322|url=http://www.opticsinfobase.org/oe/abstract.cfm?uri=oe-20-17-19322|arxiv=1206.0815|bibcode=2012OExpr..2019322M}}&lt;/ref&gt;
* Fluctuations in [[vacuum energy]] measured through [[homodyne detection]].&lt;ref&gt;{{cite journal|last=Symul|first=T.|author2=S. M. Assad |author3=P. K. Lam|journal=Appl. Phys. Lett.|volume=98|year=2011|title=Real time demonstration of high bitrate quantum random number generation with coherent laser light|doi=10.1063/1.3597793|arxiv=1107.4438|bibcode=2011ApPhL..98w1103S}}&lt;/ref&gt;{{third-party inline|date=February 2016}}

===[[Classical physics|Classical]] random properties ===
&lt;!--Apparently phenomena without quantum-random properties were added to the section "Physical phenomena with quantum-random properties". Thus this section was split but by someone who doesn't know which phenomena contain quantum randomness. If you can tell them apart, please put the phenomena in the right section and then delete this paragraph.--&gt;

Thermal phenomena are easier to detect. They are somewhat vulnerable to attack by lowering the temperature of the system,&lt;ref name="dube" /&gt; though most systems will stop operating at temperatures low enough to reduce noise by a factor of two (e.g., ~150 K). Some of the thermal phenomena used include:

* [[Johnson-Nyquist noise|Thermal noise]] from a [[resistor]], amplified to provide a random voltage source.&lt;ref name="dube"&gt;{{cite book|author=Roger R. Dube|title=Hardware-based Computer Security Techniques to Defeat Hackers: From Biometrics to Quantum Cryptography|url=https://books.google.com/books?id=e0IpysH6-vUC|year=2008|publisher=John Wiley &amp; Sons|isbn=978-0-470-42547-3|pages=47–50|chapter=Hardware Key Generation|quote="Thermal noise does not have its origins in a quantum mechanical process"}}&lt;/ref&gt;
* [[Avalanche breakdown|Avalanche noise]] generated from an [[avalanche diode]], or [[Zener breakdown]] noise from a reverse-biased [[Zener diode]].
* [[Noise (radio)|Atmospheric noise]], detected by a radio receiver attached to a PC (though much of it, such as lightning noise, is not properly thermal noise, but most likely a [[Chaos theory|chaotic]] phenomenon).

In the absence of quantum effects or thermal noise, other phenomena that tend to be random, although in ways not easily characterized by laws of physics, can be used. When several such sources are combined carefully (as in, for example, the [[Yarrow algorithm]] or [[Fortuna (PRNG)|Fortuna]] [[Cryptographically secure pseudorandom number generator|CSPRNG]]s), enough entropy can be collected for the  creation of cryptographic keys and [[cryptographic nonce|nonces]], though generally at restricted rates. The advantage is that this approach needs, in principle, no special hardware.  The disadvantage is that a sufficiently knowledgeable attacker can surreptitiously modify the software or its inputs, thus reducing the randomness of the output, perhaps substantially. The primary source of randomness typically used in such approaches is the precise timing of the [[interrupt]]s caused by mechanical input/output devices, such as keyboards and [[disk drive]]s, various system information counters, etc.

This last approach must be implemented carefully and may be subject to attack if it is not. For instance, the forward-security of the generator in Linux 2.6.10 kernel could be broken with 2&lt;sup&gt;64&lt;/sup&gt; or 2&lt;sup&gt;96&lt;/sup&gt; time complexity.&lt;ref&gt;{{citation | url = http://eprint.iacr.org/2006/086.pdf | title = Analysis of the Linux Random Number Generator | format = PDF | publisher = IACR}}&lt;/ref&gt;

==== Clock drift ====
{{further|Clock drift#Random number generators}}

Another variable physical phenomenon that is easy to measure is clock drift.
There are several ways to measure and use clock drift as a source of randomness.

The [[Intel]] 82802 Firmware Hub (FWH) chip included a hardware RNG&lt;ref&gt;Intel Corporation [http://download.intel.com/design/chipsets/designex/29065701.pdf ''Intel® 810 Chipset Design Guide, June 1999''] Ch. 1.3.5, p. 1-10.&lt;/ref&gt; using two free running oscillators, one fast and one slow. A thermal noise source (non-commonmode noise from two diodes) is used to modulate the frequency of the slow oscillator, which then triggers a measurement of the fast oscillator. That output is then debiased using a [[John von Neumann|von Neumann]] type decorrelation step (see below). The output rate of this device is somewhat less than 100,000 bit/s. This chip was an optional component of the 840 chipset family that supported an earlier Intel bus. It is not included in modern PCs.

All [[VIA C3]] microprocessors have included a hardware RNG on the processor chip since 2003. Instead of using thermal noise, raw bits are generated by using four freerunning oscillators which are designed to run at different rates. The output of two are XORed to control the bias on a third oscillator, whose output clocks the output of the fourth oscillator to produce the raw bit. Minor variations in temperature, silicon characteristics, and local electrical conditions cause continuing oscillator speed variations and thus produce the entropy of the raw bits. To further ensure randomness, there are actually two such RNGs on each chip, each positioned in different environments and rotated on the silicon. The final output is a mix of these two generators. The raw output rate is tens to hundreds of megabits per second, and the whitened rate is a few megabits per second. User software can access the generated random bit stream using new non-privileged machine language instructions.

A software implementation of a related idea on ordinary hardware is included in CryptoLib,&lt;ref&gt;{{cite journal|last=Lacy|first=J.B. |author2=D.P. Mitchell |author3=W.M. Schell|journal=Proc. 4th USENIX Security Symp.|pages=1–17|year=1993|title=CryptoLib: Cryptography in Software|url=http://www.mentallandscape.com/papers_usenix93.pdf}}&lt;/ref&gt; a cryptographic routine library. The algorithm is called ''[[truerand]]''. Most modern computers have two crystal oscillators, one for the real-time clock and one for the primary CPU clock; truerand exploits this fact. It uses an operating system service that sets an alarm, running off the real-time clock. One subroutine sets that alarm to go off in one clock tick (usually 1/60th of a second). Another then enters a while loop waiting for the alarm to trigger. Since the alarm will not always trigger in exactly one tick, the least significant bits of a count of loop iterations, between setting the alarm and its trigger, will vary randomly, possibly enough for some uses. Truerand doesn't require additional hardware, but in a multi-tasking system great care must be taken to avoid non-randomizing interference from other processes (e.g., in the suspension of the counting loop process as the operating system scheduler starts and stops assorted processes).

The [[RdRand]] opcode will return values from an onboard hardware random number generator. It is present in Intel [[Ivy Bridge (microarchitecture)|Ivy Bridge]] processors and AMD64 processors since 2015.&lt;ref&gt;
{{cite web
 |url        = http://support.amd.com/TechDocs/24594.pdf
 |title      = AMD64 Architecture Programmer’s Manual Volume 3: General-Purpose and System Instructions
 |date       = June 2015
 |website    = AMD Developer Guides, Manuals &amp; ISA Documents
 |publisher  = 
 |accessdate = 16 October 2015
}}
&lt;/ref&gt;

== Dealing with bias ==
The bit-stream from such systems is prone to be biased, with either 1s or 0s predominating.{{citation needed|date=October 2017}}  There are two approaches to dealing with bias and other artifacts. The first is to design the RNG to minimize bias inherent in the operation of the generator. One method to correct this feeds back the generated bit stream, filtered by a low-pass filter, to adjust the bias of the generator.  By the [[central limit theorem]], the feedback loop will tend to be well-adjusted '[[Asymptotically almost surely|almost all the time]]'. Ultra-high speed random number generators often use this method. Even then, the numbers generated are usually somewhat biased.

=== Software whitening  ===
{{main article|Randomness extractor}}

A second approach to coping with bias is to reduce it after generation  (in software or hardware). Even if the above hardware bias reduction steps have been taken, the bit-stream should still be assumed to contain bias and correlation. There are several techniques for reducing bias and correlation, often called "[[decorrelation|whitening]]" algorithms, by analogy with the related problem of producing white noise from a correlated signal.
There is another way, the dynamic-statics test, which makes a statics randomness check in each random number block dynamically.  This can be done usably in a short time, 1 gigabyte per second or more.
In this method, if one block shall be determined as a doubtful one, the block is disregarded and canceled.
This method is requested in the draft of ANSI(X9F1).

[[John von Neumann]] invented a simple algorithm to fix simple bias and reduce correlation. It considers two bits at a time (non-overlapping), taking one of three actions: when two successive bits are equal, they are discarded; a sequence of 1,0 becomes a 1; and a sequence of 0,1 becomes a zero. It thus represents a [[falling edge]] with a 1, and a [[rising edge]] with a 0.  This eliminates simple bias, and is easy to implement as a computer program or in digital logic. This technique works no matter how the bits have been generated. It cannot assure randomness in its output, however.  What it can do (with significant numbers of discarded bits) is transform a biased random bit stream into an unbiased one.

Another technique for improving a near random bit stream is to [[XOR|exclusive-or]] the bit stream with the output of a high-quality [[cryptographically secure pseudorandom number generator]] such as [[Blum Blum Shub]] or a strong [[stream cipher]].  This can improve decorrelation and digit bias at low cost; it can be done by hardware, such as an FPGA, which is faster than doing it by software.

A related method which reduces bias in a near random bit stream is to take two or more uncorrelated near random bit streams, and [[exclusive or]] them together.  Let the probability of a bit stream producing a 0 be 1/2&amp;nbsp;+&amp;nbsp;''e'', where −1/2&amp;nbsp;≤&amp;nbsp;''e''&amp;nbsp;≤&amp;nbsp;1/2. Then ''e'' is the bias of the bitstream.  If two uncorrelated bit streams with bias ''e'' are exclusive-or-ed together, then the bias of the result will be 2''e''².  This may be repeated with more bit streams (see also the [[Piling-up lemma]]).

Some designs apply cryptographic [[hash function]]s such as [[MD5]], [[SHA-1]], or [[RIPEMD-160]] or even a [[Cyclic redundancy check|CRC]] function to all or part of the bit stream, and then use the output as the random bit stream.  This is attractive, partly because it is relatively fast compared to some other methods, but depends significantly on qualities in the hash output for which there may be little theoretical basis.

Many physical phenomena can be used to generate bits that are highly biased, but each bit is independent from the others.
A Geiger counter (with a sample time longer than the tube recovery time) or a semi-transparent mirror photon detector both generate bit streams that are mostly "0" (silent or transmission) with the occasional "1" (click or reflection).
If each bit is independent from the others, the Von Neumann strategy generates one random, unbiased output bit for each of the rare "1" bits in such a highly biased bit stream.
Whitening techniques such as the Advanced Multi-Level Strategy (AMLS)&lt;ref&gt;{{citation | doi = 10.1214/aos/1176348543 | first = Yuval | last = Peres | title = Iterating Von Neumann's Procedure for Extracting Random Bits | journal = Annals of Statistics | volume = 20 | issue = 1 |date=March 1992 | pages = 590&amp;ndash;97}}.&lt;/ref&gt; can extract more output bits – output bits that are just as random and unbiased – from such a highly biased bit stream.&lt;ref&gt;{{citation | author-link = Paul Crowley (cryptographer) | first = Paul | last = Crowley | url = http://www.ciphergoth.org/crypto/unbiasing/ | title = Generating random binary data from Geiger counters | publisher = Cipher Goth}}.&lt;/ref&gt;

===PRNG with periodically refreshed random key===
&lt;!-- This is also software and not hardware, thus, should be moved to Pseudo Random number generator. --&gt;

Other designs use what are believed to be true random bits as the [[key (cryptography)|key]] for a high quality [[block cipher]] algorithm, taking the encrypted output as the random bit stream. Care must be taken in these cases to select an appropriate [[Block cipher modes of operation|block mode]], however. In some implementations, the PRNG is run for a limited number of digits, while the hardware generating device produces a new seed.

== Using observed events ==
Software engineers without true random number generators often try to develop them by measuring physical events available to the software. An example is measuring the time between user keystrokes, and then taking the least significant bit (or two or three) of the count as a random digit. A similar approach measures task-scheduling, network hits, disk-head seek times and other internal events. One Microsoft design includes a very long list of such internal values (see the [[Cryptographically secure pseudorandom number generator|CSPRNG]] article). Even [[lava lamp]]s have been used as the physical devices to be monitored (see [[Lavarand]]).

The method is risky when it uses computer-controlled events because a clever, malicious attacker might be able to predict a cryptographic key by controlling the external events. It is also risky because the supposed user-generated event (e.g., keystrokes) can be [[Spoofing attack|spoofed]] by a sufficiently ingenious attacker, allowing control of the "random values" used by the cryptography.

However, with sufficient care, a system can be designed that produces cryptographically secure random numbers from the sources of randomness available in a modern computer. The basic design is to maintain an "entropy pool" of random bits that are assumed to be unknown to an attacker.  New randomness is added whenever available (for example, when the user hits a key) and an estimate of the number of bits in the pool that cannot be known to an attacker is kept. Some of the strategies in use include:

* When random bits are requested, return that many bits derived from the entropy pool (by a cryptographic hash function, say) and decrement the estimate of the number of random bits remaining in the pool.  If not enough unknown bits are available, wait until enough are available.  This is the top-level design of the "[[/dev/random]]" device in Linux, written by [[Theodore Ts'o]] and used in many other Unix-like operating systems. It provides  high-quality random numbers so long as the estimates of the input randomness are sufficiently cautious.  The Linux "/dev/urandom" device is a simple modification which disregards estimates of input randomness, and is therefore rather less likely to have high entropy as a result.
* Maintain a [[stream cipher]] with a key and [[Initialization vector]] (IV) obtained from an entropy pool.  When enough bits of entropy have been collected, replace both key and IV with new random values and decrease the estimated entropy remaining in the pool.  This is the approach taken by the [[Yarrow algorithm|yarrow]] library. It provides resistance against some attacks and conserves hard-to-obtain entropy.

== Problems ==
It is very easy to misconstruct hardware or software devices which attempt to generate random numbers.  Also, most 'break' silently, often producing decreasingly random numbers as they degrade. A physical example might be the rapidly decreasing radioactivity of the smoke detectors mentioned earlier. Failure modes in such devices are plentiful and are complicated, slow, and hard to detect.

Because many entropy sources are often quite fragile, and fail silently, statistical tests on their output should be performed continuously. Many, but not all, such devices include some such tests into the software that reads the device.

=== Attacks ===
{{main article | random number generator attack }}

Just as with other components of a cryptography system, a software random number generator should be designed to resist [[random number generator attack|certain attacks]]. Defending against these attacks is difficult.

The random number generator used for cryptographic purposes in version 1.1 of the [[Netscape]] browser was vulnerable, and was promptly fixed in version 2.0.

=== Estimating entropy ===
{{See also|Entropy estimation}}

There are mathematical techniques for estimating the [[information entropy|entropy]] of a sequence of symbols. None are so reliable that their estimates can be fully relied upon; there are always assumptions which may be very difficult to confirm. These are useful for determining if there is enough entropy in a seed pool, for example, but they cannot, in general, distinguish between a true random source and a pseudorandom generator.

=== Performance test ===
Hardware random number generators should be constantly monitored for proper operation. RFC 4086, [[Federal Information Processing Standard|FIPS]] [[FIPS 140|Pub 140-2]] and [[NIST]] Special Publication 800-90b&lt;ref&gt;[http://csrc.nist.gov/publications/drafts/800-90/draft-sp800-90b.pdf Elaine Barker and John Kelsey,'' Recommendation for the Entropy Sources Used for Random Bit Generation,'' NIST SP 800-90b]&lt;/ref&gt; include tests which can be used for this. Also see the documentation for the New Zealand cryptographic software library [[cryptlib]].

Since many practical designs rely on a hardware source as an input, it will be useful to at least check that the source is still operating. Statistical tests can often detect failure of a noise source, such as a radio station transmitting on a channel thought to be empty, for example. Noise generator output should be sampled for testing before being passed through a "whitener." Some whitener designs can pass statistical tests with no random input. While detecting a large deviation from perfection would be a sign that a true random noise source has become degraded, small deviations are normal and can be an indication of proper operation. Correlation of bias in the inputs to a generator design with other parameters (e.g., internal temperature, bus voltage) might be additionally useful as a further check. Unfortunately, with currently available (and foreseen) tests, passing such tests is not enough to be sure the output sequences are random. A carefully chosen design, verification that the manufactured device implements that design and continuous physical security to insure against tampering may all be needed in addition to testing for high value uses.

== See also ==
* [[AN/CYZ-9]]
* [[Bell test experiments]]
* [[/dev/random]]
* [[Premium Bond#ERNIE|ERNIE]]
* [[List of random number generators]]
* [[Lottery machine]]
* [[Pseudorandom number generator]] (PRNG)
* [[Random number generation]]
* [[Randomness extractor]]
* [[RdRand]]

==References==
{{Reflist|30em}}

===General references===
{{Refbegin}}
* {{citation | last = Brown | first = George W | title = History of Rand’s Million Digits |date=June 1949 | url = http://www.rand.org/pubs/papers/P113 | series = papers | issue = P-113 | publisher = [[RAND Corporation]]}}.
* {{citation | last = Brown | first = Bernice | title = Some Tests of the Randomness of a Million Digits |date=October 1948 | url = http://www.rand.org/pubs/papers/P44 | series = Papers | issue = P-44 | publisher = [[RAND Corporation]]}}.
* {{citation | title = Electron Tube Data handbook | year = 1957 | chapter = Tube type 6D4| publisher = Sylvania }}.
* {{citation | title = A Million Random Digits with 100,000 Normal Deviates | url = http://www.rand.org/publications/classics/randomdigits/ | publisher = [[RAND Corporation]]}}.
* {{citation | last = Galton | first = Francis | title = Dice for statistical experiments | url = http://www.mugu.com/galton/statistician.html| year = 1890 | author-link = Francis Galton | journal = Nature | volume =  42 | pages = 13&amp;ndash;4 |doi=10.1038/042013a0| bibcode = 1890Natur..42...13G }}.
* {{citation | title = Randomness and Genuine Random Number Generator With Self-testing Functions | url = https://geant4.web.cern.ch/geant4/results/papers/QMD-MC2010.pdf | location = Japan | publisher = LE Tech RNG| format = PDF }}.

{{Refend}}

==External links==
* {{citation | url = http://www.ietf.org/rfc/rfc4086.txt | title = RFC 4086 on Randomness Recommendations for Security (replaces earlier RFC 1750) | publisher = IETF}}.
* {{citation | url = http://www.cryptography.com/public/pdf/IntelRNG.pdf | publisher = [[Intel]] | format = PDF | title = The Intel Random Number Generator}}.
* {{citation | url = http://www.entropykey.co.uk/tech | publisher = Simtec | title = Entropy Key | quote = uses P-N semiconductor junctions reverse biassed with a high enough voltage to bring them near to, but not beyond, breakdown in order to generate noise}}.
* {{citation | url = http://www.randomserver.dyndns.org/client/random.php | publisher = randomserver | title = Download Random Numbers | quote = uses a TRNG9803 hardware random number generator}}.
*[https://www.protegost.com/ ProtegoST SG100], ProtegoST, "Hardware Random Number Generator "Based on quantum physics random number source from a zener diode".

{{DEFAULTSORT:Hardware Random Number Generator}}
[[Category:Cryptography]]
[[Category:Random number generation]]
[[Category:Computer peripherals]]

[[de:Zufallszahlengenerator#Physikalischer Zufallszahlengenerator]]</text>
      <sha1>h8xhs316d8ca4xou9cgfgw203winf69</sha1>
    </revision>
  </page>
  <page>
    <title>Hypergeometric distribution</title>
    <ns>0</ns>
    <id>180841</id>
    <revision>
      <id>865712631</id>
      <parentid>863324043</parentid>
      <timestamp>2018-10-25T17:32:29Z</timestamp>
      <contributor>
        <ip>157.193.240.131</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="22950">&lt;!-- EDITORS! Please see [[Wikipedia:WikiProject Probability#Standards]] for a discussion
of standards used for probability distribution articles such as this one. --&gt;
{{Infobox probability distribution
| name       = Hypergeometric
| type       = mass
| pdf_image  = [[File:HypergeometricPDF.png|300px|Hypergeometric PDF plot]]
| cdf_image  = [[File:HypergeometricCDF.png|300px|Hypergeometric CDF plot]]
| parameters = &lt;math&gt;\begin{align}N&amp;\in \left\{0,1,2,\dots\right\} \\
                                 K&amp;\in \left\{0,1,2,\dots,N\right\} \\
                                 n&amp;\in \left\{0,1,2,\dots,N\right\}\end{align}\,&lt;/math&gt;
| support  = &lt;math&gt;\scriptstyle{k\, \in\, \left\{\max{(0,\, n+K-N)},\, \dots,\, \min{(n,\, K )}\right\}}\,&lt;/math&gt;
| pdf      = &lt;math&gt;{{{K \choose k} {{N-K} \choose {n-k}}}\over {N \choose n}}&lt;/math&gt;
| cdf      = &lt;math&gt;1-{{{n \choose {k+1}}{{N-n} \choose {K-k-1}}}\over {N \choose K}} \,_3F_2\!\!\left[\begin{array}{c}1,\ k+1-K,\ k+1-n \\ k+2,\ N+k+2-K-n\end{array};1\right],&lt;/math&gt; where &lt;math&gt;\,_pF_q&lt;/math&gt; is the [[generalized hypergeometric function]]
| mean     = &lt;math&gt;n {K\over N}&lt;/math&gt;
| median   =
| mode     = &lt;math&gt;\left \lceil \frac{(n+1)(K+1)}{N+2} \right \rceil-1, \left \lfloor \frac{(n+1)(K+1)}{N+2} \right \rfloor&lt;/math&gt;
| variance = &lt;math&gt;n{K\over N}{(N-K)\over N}{N-n\over N-1}&lt;/math&gt;
| skewness = &lt;math&gt;\frac{(N-2K)(N-1)^\frac{1}{2}(N-2n)}{[nK(N-K)(N-n)]^\frac{1}{2}(N-2)}&lt;/math&gt;
| kurtosis = &lt;math&gt; \left.\frac{1}{n K(N-K)(N-n)(N-2)(N-3)}\cdot\right.&lt;/math&gt;
&lt;math&gt;\Big[(N-1)N^{2}\Big(N(N+1)-6K(N-K)-6n(N-n)\Big)+{}&lt;/math&gt;
&lt;math&gt;{}+6 n K (N-K)(N-n)(5N-6)\Big]&lt;/math&gt;
| entropy =
| mgf     = &lt;math&gt;\frac{{N-K \choose n} \scriptstyle{\,_2F_1(-n, -K; N - K - n + 1; e^{t}) } }
                         {{N \choose n}}  \,\!&lt;/math&gt;
| char = &lt;math&gt;\frac{{N-K \choose n} \scriptstyle{\,_2F_1(-n, -K; N - K - n + 1; e^{it}) }}
{{N \choose n}} &lt;/math&gt;
}}
In [[probability theory]] and [[statistics]], the '''hypergeometric distribution''' is a [[Probability distribution#Discrete probability distribution|discrete probability distribution]] that describes the probability of &lt;math&gt;k&lt;/math&gt; successes (random draws for which the object drawn has a specified feature) in &lt;math&gt;n&lt;/math&gt; draws, ''without'' replacement, from a finite [[population]] of size &lt;math&gt;N&lt;/math&gt; that contains exactly &lt;math&gt;K&lt;/math&gt; objects with that feature, wherein each draw is either a success or a failure. In contrast, the [[binomial distribution]] describes the probability of &lt;math&gt;k&lt;/math&gt; successes in &lt;math&gt;n&lt;/math&gt; draws ''with'' replacement.

In [[statistics]], the '''hypergeometric test''' uses the hypergeometric distribution to calculate the statistical significance of having drawn a specific &lt;math&gt;k&lt;/math&gt; successes (out of &lt;math&gt;n&lt;/math&gt; total draws) from the aforementioned population. The test is often used to identify which sub-populations are over- or under-represented in a sample. This test has a wide range of applications. For example, a marketing group could use the test to understand their customer base by testing a set of known customers for over-representation of various demographic subgroups (e.g., women, people under 30).

==Definition==
The following conditions characterize the hypergeometric distribution:
* The result of each draw (the elements of the population being sampled) can be classified into one of [[Binary variable|two mutually exclusive categories]] (e.g. Pass/Fail or  Employed/Unemployed).
* The probability of a success changes on each draw, as each draw decreases the population (''[[sampling without replacement]]'' from a finite population).

A [[random variable]] &lt;math&gt;X&lt;/math&gt; follows the hypergeometric distribution if its [[probability mass function]] (pmf) is given by&lt;ref&gt;{{Cite book
| edition   = Third
| publisher = Duxbury Press
| last      = Rice
| first     = John A.
| title     = Mathematical Statistics and Data Analysis
| year      = 2007
| page      = 42
}}&lt;/ref&gt;

:&lt;math&gt; p_X(k) = \Pr(X = k) 
= \frac{\binom{K}{k} \binom{N - K}{n-k}}{\binom{N}{n}},&lt;/math&gt;

where

*&lt;math&gt;N&lt;/math&gt; is the population size,
*&lt;math&gt;K&lt;/math&gt; is the number of success states in the population,
*&lt;math&gt;n&lt;/math&gt; is the number of draws (i.e. quantity drawn in each trial),
*&lt;math&gt;k&lt;/math&gt; is the number of observed successes,
*&lt;math display="inline"&gt;\textstyle {a \choose b}&lt;/math&gt; is a [[binomial coefficient]].

The {{Abbr|pmf|probability mass function}} is positive when &lt;math&gt;\max(0, n+K-N) \leq k \leq \min(K,n)&lt;/math&gt;.

A random variable distributed hypergeometrically with parameters &lt;math&gt;N&lt;/math&gt;, &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;n&lt;/math&gt; is written &lt;math display="inline"&gt;X \sim \operatorname{Hypergeometric}(N,K,n)&lt;/math&gt; and has [[probability mass function]] &lt;math display="inline"&gt; p_X(k)&lt;/math&gt; above.

==Combinatorial identities==

As required, we have

:&lt;math display="block"&gt; \sum_{0\leq k\leq n} { {K \choose k} { N-K \choose n-k} \over {N \choose n} } 
= 1,&lt;/math&gt;

which essentially follows from [[Vandermonde's identity]] from [[combinatorics]].

Also note that

:&lt;math&gt; {{K \choose k} {N-K \choose n-k}\over {N \choose n}} = 
 {{{n \choose k} {{N-n} \choose {K-k}}} \over {N \choose K}},&lt;/math&gt;

which follows from the symmetry of the problem, but it can also be shown by expressing the binomial coefficients in terms of factorials and rearranging the latter.

== Application and example ==

The classical application of the hypergeometric distribution is '''sampling without replacement'''. Think of an [[urn problem|urn]] with two types of [[marbles]], red ones and green ones. Define drawing a green marble as a success and drawing a red marble as a failure (analogous to the binomial distribution). If the variable ''N'' describes the number of '''all marbles in the urn''' (see contingency table below) and ''K'' describes the number of '''green marbles''', then ''N''&amp;nbsp;−&amp;nbsp;''K'' corresponds to the number of '''red marbles'''. In this example, ''X'' is the [[random variable]] whose outcome is ''k'', the number of green marbles actually drawn in the experiment. This situation is illustrated by the following [[contingency table]]:
&lt;!-- Formatting problem: tables overlap in Firefox with low resolution unless aligned by right. Please keep align=right!
{| class="wikitable" style="float:right; margin-left:1em"
|-
!
! drawn
! not drawn
! total
|-
| align="right" | '''defective'''
| align="right" | ''k''
| align="right" | ''K'' − ''k''
| align="right" | ''K''
|-
| align="right" | '''non-defective'''
| align="right" | ''n'' − ''k''
| align="right" | ''N − K − n + k''
| align="right" | ''N − K''
|-
| align="right" | '''total'''
td align="right"&gt;''n''
| align="right" | ''N − n''
| align="right" | ''N''
|} {{Clearright}}--&gt;
{| class="wikitable" style="text-align:center"
! || drawn || not drawn || total
|-
| align="right" | '''green marbles''' || ''k'' || ''K'' − ''k'' || ''K''
|-
| align="right" | '''red marbles''' || ''n'' − ''k'' || ''N + k − n − K'' || ''N − K''
|-
| align="right" | '''total''' || ''n'' || ''N − n'' || ''N''
|-
|}

Now, assume (for example) that there are 5 green and 45 red marbles in the urn. Standing next to the urn, you close your eyes and draw 10 marbles without replacement. What is the probability that exactly 4 of the 10 are green? ''Note that although we are looking at success/failure, the data are not accurately modeled by the [[binomial distribution]], because the probability of success on each trial is not the same, as the size of the remaining population changes as we remove each marble.''

This problem is summarized by the following contingency table:
{| class="wikitable" style="text-align:center"
|-
! !! drawn !! not drawn !! total
|-
| align="right" | '''green marbles'''
| ''k'' = '''4'''
| ''K'' − ''k'' = '''1'''
| ''K'' = '''5'''
|-
| align="right" | '''red marbles'''
| ''n'' − ''k'' = '''6'''
| ''N + k − n − K'' = '''39'''
| ''N − K'' = '''45'''
|-
| align="right" | '''total'''
| ''n'' = '''10'''
| ''N − n'' = '''40'''
| ''N'' = '''50'''
|}

The probability of drawing exactly ''k'' green marbles can be calculated by the formula

:&lt;math&gt; P(X=k) = f(k;N,K,n) = {{{K \choose k} {{N-K} \choose {n-k}}}\over {N \choose n}}.&lt;/math&gt;

Hence, in this example calculate

:&lt;math&gt; P(X=4) = f(4;50,5,10) = {{{5 \choose 4} {{45} \choose {6}}}\over {50 \choose 10}} = {5\cdot 8145060\over 10272278170} = 0.003964583\dots. &lt;/math&gt;

Intuitively we would expect it to be even more unlikely that all 5 green marbles will be among the 10 drawn.

:&lt;math&gt; P(X=5) = f(5;50,5,10) = {{{5 \choose 5} {{45} \choose {5}}}\over {50 \choose 10}} = {1\cdot 1221759
\over 10272278170} = 0.0001189375\dots, &lt;/math&gt;

As expected, the probability of drawing 5 green marbles is roughly 35 times less likely than that of drawing 4.

===Application to auditing elections===
[[File:Election Samples.png|thumb|Samples used for election audits and resulting chance of missing a problem]]
[[Election audits]] typically test a sample of machine-counted precincts to see if recounts by hand or machine match the original counts. Mismatches result in either a report or a larger recount. The sampling rates are usually defined by law, not statistical design, so for a legally defined sample size ''n'', what is the probability of missing a problem which is present in ''K'' precincts, such as a hack or bug? This is the probability that ''k''&amp;nbsp;=&amp;nbsp;0. Bugs are often obscure, and a hacker can minimize detection by affecting only a few precincts, which will still affect close elections, so a plausible scenario is for ''K'' to be on the order of 5% of ''N''. Audits typically cover 1% to 10% of precincts (often 3%),&lt;ref name="vvstates"&gt;{{Cite web |url=https://www.verifiedvoting.org/state-audit-laws/ |title=State Audit Laws |date=2017-02-10 |website=Verified Voting |language=en-US |access-date=2018-04-02}}&lt;/ref&gt;&lt;ref name="ncsl"&gt;{{Cite web |url=http://www.ncsl.org/research/elections-and-campaigns/post-election-audits635926066.aspx#state |title=Post-Election Audits |last=National Conference of State Legislatures |website=www.ncsl.org |language=en-US |access-date=2018-04-02}}&lt;/ref&gt; so they have a high chance of missing a problem. For example if a problem is present in 5 of 100 precincts, a 3% sample has 86% probability that ''k''&amp;nbsp;=&amp;nbsp;0 so the problem would not be noticed, and only 14% probability of the problem appearing in the sample (positive ''k''):

: &lt;math&gt;
\begin{align}
\Pr(X = 0) &amp; = \frac{\binom{\text{Hack}}{0} \binom{N - \text{Hack}}{n-0}}{\binom{N}{n}} =  \frac{\binom{N - \text{Hack}}{n}}{\binom{N}{n}} = \frac{\frac{(N-\text{Hack})!}{n!(N-\text{Hack}-n)!}}{\frac{N!}{n!(N-n)!}} = \frac{\frac{(N-\text{Hack})!}{(N-\text{Hack}-n)!}}{\frac{N!}{(N-n)!}} \\[8pt]
&amp; =  \frac{\binom{100-5}{3}}{\binom{100}{3}} = \frac{\frac{(100-5)!}{(100-5-3)!}}{\frac{100!}{(100-3)!}} = \frac{\frac{95!}{92!}}{\frac{100!}{97!}} = \frac{95\times94\times93}{100\times99\times98} = 86\%
\end{align}
&lt;/math&gt;

The sample would need 45 precincts in order to have probability under 5% that ''k''&amp;nbsp;=&amp;nbsp;0 in the sample, and thus have probability over 95% of finding the problem:

: &lt;math&gt;P(X = 0) =  \frac{\binom{100-5}{45}}{\binom{100}{45}} = \frac{\frac{95!}{50!}}{\frac{100!}{55!}} = \frac{95\times94\times \cdots \times51}{100\times99\times \cdots \times56} = \frac{55\times54\times53\times52\times51}{100\times99\times98\times97\times96} = 4.6\%&lt;/math&gt;

=== Application to Texas hold'em poker ===
In [[hold'em]] poker players make the best hand they can combining the two cards in their hand with the 5 cards (community cards) eventually turned up on the table. The deck has 52 and there are 13 of each suit.
For this example assume a player has 2 clubs in the hand and there are 3 cards showing on the table, 2 of which are also clubs. The player would like to know the probability of one of the next 2 cards to be shown being a club to complete the [[Flush (poker)|flush]].&lt;br /&gt;
(Note that the probability calculated in this example assumes no information is known about the cards in the other players' hands; however, experienced poker players may consider how the other players place their bets (check, call, raise, or fold) in considering the probability for each scenario. Strictly speaking, the approach to calculating success probabilities outlined here is accurate in a scenario where there is just one player at the table; in a multiplayer game this probability might be adjusted somewhat based on the betting play of the opponents.)

There are 4 clubs showing so there are 9 still unseen. There are 5 cards showing (2 in the hand and 3 on the table) so there are &lt;math&gt;52-5=47&lt;/math&gt; still unseen.

The probability that one of the next two cards turned is a club can be calculated using hypergeometric with &lt;math&gt;k=1, n=2, K=9&lt;/math&gt; and &lt;math&gt;N=47&lt;/math&gt;. (about 31.6%)

The probability that both of the next two cards turned are clubs can be calculated using hypergeometric with &lt;math&gt;k=2, n=2, K=9&lt;/math&gt; and &lt;math&gt;N=47&lt;/math&gt;. (about 3.3%)

The probability that neither of the next two cards turned are clubs can be calculated using hypergeometric with &lt;math&gt;k=0, n=2, K=9&lt;/math&gt; and &lt;math&gt;N=47&lt;/math&gt;. (about 65.0%)

== Symmetries ==
Swapping the roles of green and red marbles:
: &lt;math&gt; f(k;N,K,n) = f(n-k;N,N-K,n)&lt;/math&gt;

Swapping the roles of drawn and not drawn marbles:
: &lt;math&gt; f(k;N,K,n) = f(K-k;N,K,N-n)&lt;/math&gt;

Swapping the roles of green and drawn marbles:
: &lt;math&gt; f(k;N,K,n) = f(k;N,n,K) &lt;/math&gt;

== Hypergeometric test ==
The '''hypergeometric test''' uses the hypergeometric distribution to measure the statistical significance of having drawn a sample consisting of a specific number of &lt;math&gt;k&lt;/math&gt; successes (out of &lt;math&gt;n&lt;/math&gt; total draws) from a population of size &lt;math&gt;N&lt;/math&gt; containing &lt;math&gt;K&lt;/math&gt; successes. In a test for over-representation of successes in the sample, the hypergeometric p-value is calculated as the probability of randomly drawing &lt;math&gt;k&lt;/math&gt; or more successes from the population in &lt;math&gt;n&lt;/math&gt; total draws. In a test for under-representation, the p-value is the probability of randomly drawing &lt;math&gt;k&lt;/math&gt; or fewer successes.

=== Relationship to Fisher's exact test ===
[[File:Youngronaldfisher2.JPG|thumb|right|200px|Biologist and statistician [[Ronald Fisher]]]]
{{see also|Fisher's noncentral hypergeometric distribution}}
The test based on the hypergeometric distribution (hypergeometric test) is identical to the corresponding one-tailed version of [[Fisher's exact test]]&lt;ref&gt;{{cite journal| first1=I.|last1= Rivals|first2= L. |last2=Personnaz | first3= L. |last3=Taing |first4= M.-C |last4=Potier| title=Enrichment or depletion of a GO category within a class of genes: which test? |volume= 23|journal= Bioinformatics |year=2007 |pages= 401–407|pmid=17182697| doi=10.1093/bioinformatics/btl633| issue=4|url=https://hal-espci.archives-ouvertes.fr/hal-00801557/document}}&lt;/ref&gt; ). Reciprocally, the p-value of a two-sided Fisher's exact test can be calculated as the sum of two appropriate hypergeometric tests (for more information see&lt;ref&gt;{{cite web| author=K. Preacher and N. Briggs| title=Calculation for Fisher's Exact Test: An interactive calculation tool for Fisher's exact probability test for 2 x 2 tables (interactive page) | url=http://quantpsy.org/fisher/fisher.htm}}&lt;/ref&gt; ).

== Order of draws ==
The probability of drawing any set of green and red marbles (the hypergeometric distribution) depends only on the  numbers of green and red marbles, not on the order in which they appear; i.e., it is an [[exchangeable random variables|exchangeable]] distribution. As a result, the probability of drawing a green marble in the &lt;math&gt;i^{\text{th}}&lt;/math&gt; draw is&lt;ref&gt;http://www.stat.yale.edu/~pollard/Courses/600.spring2010/Handouts/Symmetry%5BPolyaUrn%5D.pdf&lt;/ref&gt;

:&lt;math&gt; P(G_i) = \frac{K}{N}.&lt;/math&gt;

This is an ex ante probability—that is, it is based on not knowing the results of the previous draws.

== Related distributions ==

Let &lt;math&gt;X\sim\operatorname{Hypergeometric}(K,N,n)&lt;/math&gt; and &lt;math&gt;p=K/N&lt;/math&gt;.

*If &lt;math&gt;n=1&lt;/math&gt; then &lt;math&gt;X&lt;/math&gt; has a [[Bernoulli distribution]] with parameter &lt;math&gt;p&lt;/math&gt;.
*Let &lt;math&gt;Y&lt;/math&gt; have a [[binomial distribution]] with parameters &lt;math&gt;n&lt;/math&gt; and &lt;math&gt;p&lt;/math&gt;; this models the number of successes in the analogous sampling problem ''with'' replacement.  If &lt;math&gt;N&lt;/math&gt; and &lt;math&gt;K&lt;/math&gt; are large compared to &lt;math&gt;n&lt;/math&gt;, and &lt;math&gt;p&lt;/math&gt; is not close to 0 or 1, then &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt; have similar distributions, i.e., &lt;math&gt;P(X \le k) \approx P(Y \le k)&lt;/math&gt;.
*If &lt;math&gt;n&lt;/math&gt; is large, &lt;math&gt;N&lt;/math&gt; and &lt;math&gt;K&lt;/math&gt; are large compared to &lt;math&gt;n&lt;/math&gt;, and &lt;math&gt;p&lt;/math&gt; is not close to 0 or 1, then
::&lt;math&gt;P(X \le k) \approx \Phi \left( \frac{k-n p}{\sqrt{n p (1-p)}} \right)&lt;/math&gt;

where &lt;math&gt;\Phi&lt;/math&gt; is the [[Standard normal distribution#Cumulative distribution function|standard normal distribution function]]
*If the probabilities of drawing a green or red marble are not equal (e.g. because green marbles are bigger/easier to grasp than red marbles) then &lt;math&gt;X&lt;/math&gt; has a [[noncentral hypergeometric distribution]]
*The [[beta-binomial distribution]] is a [[conjugate prior]] for the hypergeometric distribution.

The following table describes four distributions related to the number of successes in a sequence of draws: 
{| class="wikitable"
|-
!  !! With replacements !! No replacements
|-
| Given number of draws || [[binomial distribution]] || hypergeometric distribution
|-
| Given number of failures || [[negative binomial distribution]] || [[negative hypergeometric distribution]]
|}

==Tail bounds==
Let &lt;math&gt;X \sim \operatorname{Hypergeometric}(K,N,n)&lt;/math&gt; and &lt;math&gt;p=K/N&lt;/math&gt;. Then we can derive the following bounds:&lt;ref&gt;{{citation
 | last = Hoeffding | first = Wassily
 | journal = [[Journal of the American Statistical Association]]
 | volume= 58
 | number= 301
 | pages=13–30
 | title = Probability inequalities for sums of bounded random variables
 | year = 1963
 | doi=10.2307/2282952}}.&lt;/ref&gt;

:&lt;math&gt;\begin{align}
\Pr[X\le (p - t)n]
&amp;\le e^{-n\text{D}(p-t\parallel p)} \le e^{-2t^2n}\\
\Pr[X\ge (p+t)n]
&amp;\le e^{-n\text{D}(p+t\parallel p)} \le e^{-2t^2n}\\
\end{align}\!&lt;/math&gt;

where

:&lt;math&gt; D(a\parallel b)=a\log\frac{a}{b}+(1-a)\log\frac{1-a}{1-b}&lt;/math&gt;

is the [[Kullback-Leibler divergence]] and it is used that &lt;math&gt;D(a, b) \ge 2(a-b)^2&lt;/math&gt;.&lt;ref name="wordpress.com"&gt;{{cite web|url=https://ahlenotes.wordpress.com/2015/12/08/hypergeometric_tail/|title=Another Tail of the Hypergeometric Distribution|author=|date=8 December 2015|website=wordpress.com|accessdate=19 March 2018}}&lt;/ref&gt;

If ''n'' is larger than ''N''/2, it can be useful to apply symmetry to "invert" the bounds, which give you the following:
&lt;ref name="wordpress.com"/&gt;
&lt;ref&gt;{{citation
 | last = Serfling | first = Robert
 | journal = [[The Annals of Statistics]]
 | pages = 39–48
 | title = Probability inequalities for the sum in sampling without replacement
 | year = 1974}}.&lt;/ref&gt;

:&lt;math&gt;\begin{align}
\Pr[X\le (p - t)n]
&amp;\le e^{-(N-n)\text{D}(p+\tfrac{tn}{N-n}||p)} \le e^{-2 t^2 n \tfrac{n}{N-n}}\\
\\
\Pr[X\ge (p+t)n]
&amp;\le e^{-(N-n)\text{D}(p-\tfrac{tn}{N-n}||p)} \le e^{-2 t^2 n \tfrac{n}{N-n}}\\
\end{align}\!&lt;/math&gt;

== Multivariate hypergeometric distribution ==
{{Infobox probability distribution
| name       = Multivariate hypergeometric distribution
| type       = mass
| pdf_image  =
| cdf_image  =
| parameters = &lt;math&gt;c \in \mathbb{N} = \lbrace 0, 1, \ldots \rbrace&lt;/math&gt;&lt;br /&gt;&lt;math&gt;(K_1,\ldots,K_c) \in \mathbb{N}^c&lt;/math&gt;&lt;br /&gt;&lt;math&gt;N = \sum_{i=1}^c K_i&lt;/math&gt;&lt;br /&gt;&lt;math&gt;n \in \lbrace 0,\ldots,N\rbrace&lt;/math&gt;
| support    = &lt;math&gt;\left\{ \mathbf{k} \in \mathbb{Z}_{0+}^c \, : \, \forall i\ k_i \le K_i , \sum_{i=1}^{c} k_i = n \right\}&lt;/math&gt;
| pdf        = &lt;math&gt;\frac{\prod_{i=1}^c \binom{K_i}{k_i}}{\binom{N}{n}}&lt;/math&gt;
| cdf        =
| mean       = &lt;math&gt;\operatorname E(X_i) = \frac{n K_i}{N}&lt;/math&gt;
| median     =
| mode       =
| variance   = &lt;math&gt;\operatorname{Var}(X_i) = \frac{K_i}{N} \left(1-\frac{K_i}{N}\right) n \frac{N-n}{N-1} &lt;/math&gt;&lt;br /&gt;&lt;math&gt;\operatorname{Cov}(X_i,X_j) = -\frac{n K_i K_j}{N^2} \frac{N-n}{N-1} &lt;/math&gt;
| skewness   =
| kurtosis   =
| entropy    =
| mgf        =
| char       =
}}

The model of an [[urn problem|urn]] with green and red marbles can be extended to the case where there are more than two colors of marbles. If there are ''K''&lt;sub&gt;''i''&lt;/sub&gt; marbles of color ''i'' in the urn and you take ''n'' marbles at random without replacement, then the number of marbles of each color in the sample (''k''&lt;sub&gt;1&lt;/sub&gt;,''k''&lt;sub&gt;2&lt;/sub&gt;,...,''k''&lt;sub&gt;''c''&lt;/sub&gt;) has the multivariate hypergeometric distribution.  This has the same relationship to the [[multinomial distribution]] that the hypergeometric distribution has to the binomial distribution—the multinomial distribution is the "with-replacement" distribution and the multivariate hypergeometric is the "without-replacement" distribution.

The properties of this distribution are given in the adjacent table, where ''c'' is  the number of different colors and &lt;math&gt;N=\sum_{i=1}^c K_i&lt;/math&gt; is the total number of marbles.

=== Example ===
Suppose there are 5 black, 10 white, and 15 red marbles in an urn.  If six marbles are chosen without replacement, the probability that exactly two of each color are chosen is

:&lt;math&gt; P(2\text{ black}, 2\text{ white}, 2\text{ red}) = {{{5 \choose 2}{10 \choose 2} {15 \choose 2}}\over {30 \choose 6}} = 0.079575596816976&lt;/math&gt;

== See also ==
* [[Noncentral hypergeometric distributions]]
* [[Negative hypergeometric distribution]]
* [[Multinomial distribution]]
* [[Sampling (statistics)]]
* [[Generalized hypergeometric function]]
* [[Coupon collector's problem]]
* [[Geometric distribution]]
* [[Keno]]

{{more footnotes|date=August 2011}}

==Notes==
&lt;references/&gt;

== References ==
*{{cite journal|doi=10.1016/j.jda.2006.01.001|title=HyperQuick algorithm for discrete hypergeometric distribution|year=2007|last1=Berkopec|first1=Aleš|journal=Journal of Discrete Algorithms|volume=5|issue=2|pages=341}}
* {{Cite web|last=Skala|first= M. |year=2011|url=http://ansuz.sooke.bc.ca/professional/hypergeometric.pdf |title=Hypergeometric tail inequalities: ending the insanity}} unpublished note

== External links ==
* [http://demonstrations.wolfram.com/TheHypergeometricDistribution/ The Hypergeometric Distribution] and [http://demonstrations.wolfram.com/BinomialApproximationToAHypergeometricRandomVariable/ Binomial Approximation to a Hypergeometric Random Variable] by Chris Boucher, [[Wolfram Demonstrations Project]].
* {{MathWorld |title=Hypergeometric Distribution |urlname=HypergeometricDistribution}}

{{ProbDistributions|discrete-finite}}

{{DEFAULTSORT:Hypergeometric Distribution}}
[[Category:Discrete distributions]]
[[Category:Factorial and binomial topics]]</text>
      <sha1>bblhy6edlxvpfdepdy8685bz6w3wvkw</sha1>
    </revision>
  </page>
  <page>
    <title>Infinite difference method</title>
    <ns>0</ns>
    <id>55573467</id>
    <revision>
      <id>817436917</id>
      <parentid>806827196</parentid>
      <timestamp>2017-12-28T10:44:56Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v481)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1208">In [[mathematics]], '''infinite difference methods''' are [[numerical methods]] for solving [[differential equations]] by approximating them with [[Recurrence relation#Relationship to difference equations narrowly defined|difference equations]], in which [[Finite difference#Generalizations|infinite differences approximate]] the [[derivative]]s.

==See also==
* [[Infinite element method]]
* [[Finite difference]]
* [[Finite difference time domain]]

==References==
{{Reflist}}
* [http://www.sciencedirect.com/science/article/pii/S0022072803001037 Simulation of ion transfer under conditions of natural convection by the finite difference method]
* {{cite book|author1=Han, Houde|author2=Wu, Xiaonan|title=Artificial Boundary Method|year=2013|publisher=Springer|isbn=978-3-642-35464-9|at=Chapter 6: Discrete Artificial Boundary Conditions|url=https://www.springer.com/la/book/9783642354632}}.
* [http://www.khuisf.ac.ir/prof/Images/Uploaded_Files/3445-8593-4-PB%5B6699598%5D.PDF Genetic Algorithm and Numerical Solution]

{{Authority control}}

{{DEFAULTSORT:Infinite Difference Method}}
[[Category:Finite differences]]
[[Category:Numerical differential equations]]

{{Numerical PDE}}

{{Applied-math-stub}}</text>
      <sha1>i0kwyijpv2iydrwb9wcbrxhjdadygvm</sha1>
    </revision>
  </page>
  <page>
    <title>Jean-Yves Béziau</title>
    <ns>0</ns>
    <id>3661969</id>
    <revision>
      <id>871000395</id>
      <parentid>870783543</parentid>
      <timestamp>2018-11-28T08:40:11Z</timestamp>
      <contributor>
        <username>Sarah Rosenberg</username>
        <id>28302821</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4071">{{COI|date=May 2016}}
{{BLP sources|date=January 2016}}
[[File:jyb-jerusalem-2016.jpg|thumb|right|460px| Jean-Yves Béziau in [[Jerusalem]], January 2016]]

'''Jean-Yves Beziau''' ({{lang-fr|bezjo|lang}}; born January 15, 1965 in [[Orléans, France]]) is a professor and researcher of the Brazilian Research Council — [[CNPq]] — at the [[University of Brazil]] in [[Rio de Janeiro]].

==Career==
Beziau works in the field of [[logic]]—in particular, [[paraconsistent logic]], the [[square of opposition]] and [[universal logic]].  He holds a Maîtrise in [[Philosophy]] from [[University of Paris 1|Pantheon-Sorbonne University]], a DEA in Philosophy from Pantheon-Sorbonne University, a [[PhD]] in Philosophy from the [[University of São Paulo]], a [[Master's degree|MSc]] and a PhD in Logic and Foundations of [[Computer Science]] from [[University of Paris 7|Denis Diderot University]].

Béziau is the editor-in-chief of the journal ''[[Logica Universalis]]''&lt;ref&gt;{{Cite web|title = Logica Universalis|url = https://link.springer.com/journal/11787|website = SpringerLink|access-date = 2016-04-11}}&lt;/ref&gt; and of the ''South American Journal of Logic'' - an online, open-access journal - as well as of the [[Springer Science+Business Media|Springer]] book series ''Studies in Universal Logic.''&lt;ref&gt;http://www.sa-logic.org/&lt;/ref&gt;&lt;ref&gt;https://www.springer.com/series/7391?detailsPage=titles&lt;/ref&gt; He is also the editor of ''College Publication's'' book series ''Logic PhDs''&lt;ref&gt;http://www.collegepublications.co.uk/lphd/&lt;/ref&gt;

He has launched four major international series of events: UNILOG (World Congress and School on Universal Logic), SQUARE (World Congress on the Square of Opposition), WOCOLOR (World Congress on Logic and Religion), LIQ (Logic in Question). Such events have included major figures of mathematics, philosophy, computer science and religion such as [[Saul Kripke]], [[Michal Heller]], [[Laurent Lafforgue]], [[Yuri Gurevich]], [[Pierre_Cartier_(mathematician) | Pierre Cartier]] &lt;ref&gt;https://www.uni-log.org/&lt;/ref&gt;,
&lt;ref&gt;http://en.uw.edu.pl/congress-on-logic-and-religion/&lt;/ref&gt;.
 

== Selected publications ==
* "What is paraconsistent logic?"  In D. Batens et al. (eds.), ''Frontiers of Paraconsistent Logic'', Research Studies Press, Baldock, 2000, pp.&amp;nbsp;95–111.  {{ISBN|0-86380-253-2}}
* ''Handbook of Paraconsistency'' (ed. with [[Walter Carnielli]] and  [[Dov Gabbay]]).  London: College Publication, 2007.  {{ISBN|978-1-904987-73-4}}
* "Semantic computation of truth based on associations already learned" (with [[Patrick Suppes]]), ''Journal of Applied Logic'', 2 (2004), pp.&amp;nbsp;457–467.
* "From paraconsistent logic to universal logic", ''Sorites'',  12 (2001), pp.&amp;nbsp;5–32.
* ''Logica Universalis: Towards a General Theory of Logic'' (ed.)  Basel: Birkhäuser Verlag, 2005, Second Edition 2007.  {{ISBN|3-7643-7259-1}}
* "Logic is not logic", 'Abstracta' 6 (2010), pp.&amp;nbsp;73–102.
*  "The power of the hexagon", ''Logica Universalis'', 6 (2012), pp.&amp;nbsp;1–43.
*  "History of truth-values",  in D.M.Gabbay and J.Woods (eds) ''Handbook of the History of Logic'', Vol. 11 - Logic: a history of its central concepts, Elsevier, Amsterdam, 2012, pp.&amp;nbsp;233–305
* ''The Square of Opposition: a General Framework for Cognition'' (ed. with Gillman Payette).  Bern: Peter Lang, 2012.  {{ISBN|978-3-0343-0537-2}}
* "The new rising of the square of opposition", in J.-Y.Béziau and D.Jacquette (eds), ''Around and Beyond the Square of Opposition'', Birkhäuser, Basel, 2012, pp.&amp;nbsp;6–24.
* ''La pointure du symbole'' (ed.)  Paris: Petra, 2014.  {{ISBN|978-2-84743-048-6}}
*  "The relativity and universality of logic", ''Synthese'', 192 (2015), pp 1939–1954.

==References==
{{Reflist}}

==External links==
* [http://www.jyb-logic.org/ Jean-Yves Beziau's personal homepage]

{{Authority control}}

{{DEFAULTSORT:Beziau, Jean-Yves}}
[[Category:1965 births]]
[[Category:Mathematical logicians]]
[[Category:Living people]]
[[Category:Logicians]]
[[Category:People from Orléans]]
[[Category:Paraconsistent logic]]</text>
      <sha1>kjadcg1vqftf63qigr5ep1iyup83hxl</sha1>
    </revision>
  </page>
  <page>
    <title>Juris Hartmanis</title>
    <ns>0</ns>
    <id>305842</id>
    <revision>
      <id>823314131</id>
      <parentid>814317850</parentid>
      <timestamp>2018-01-31T14:30:06Z</timestamp>
      <contributor>
        <username>The Transhumanist</username>
        <id>1754504</id>
      </contributor>
      <comment>/* top */Fix typos/general, [[WP:AWB/T|typo(s) fixed]]: award winning → award-winning using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6236">{{BLP sources|date=January 2013}}
{{Infobox scientist
| name                    = Juris Hartmanis
| image                   = Juris_Hartmanis(2002).jpg
| image_size              = 150px
| caption                 = 
| birth_date              = {{Birth date and age|1928|7|5|mf=y}}
| birth_place             = [[Riga]], [[Latvia]]
| death_date              = 
| death_place             = 
| residence               = 
| citizenship             =
| nationality             = 
| ethnicity               =
| field                   = [[Computer Science]]
| work_institution        = [[General Electric]]&lt;br&gt;[[Cornell University]]
| alma_mater              = [[California Institute of Technology]]
| doctoral_advisor        = 
| doctoral_students       = [[Allan Borodin]] &lt;br&gt; [[Dexter Kozen]]
| known_for               = 
| author_abbreviation_bot = 
| author_abbreviation_zoo = 
| prizes                  = [[Turing Award]] &lt;small&gt;(1993)&lt;/small&gt;
| religion                = 
| footnotes               = 
}}
'''Juris Hartmanis''' (born July 5, 1928) is a prominent [[computer scientist]] and [[computational theorist]] who, with [[Richard Stearns (computer scientist)|Richard E. Stearns]], received the 1993 [[Association for Computing Machinery|ACM]] [[Turing Award]] "in recognition of their seminal paper which established the foundations for the field of [[computational complexity theory]]".

Hartmanis was born in [[Latvia]]. He was a son of {{illm|Mārtiņš Hartmanis|lv}},&lt;ref&gt;In the Baltic languages, own-names are not lexical constants but have different grammatical forms. ''Hartmanis'' must be understood as Hartman-is, whereby ''Hartman'' is the stem of the own-name, whereas the suffix ''-is'' indicates a masculine grammatical form in the Latvian language. In a similar manner, for example, the philosopher Kant is known as Kant''as'' in the Lithuanian language.&lt;/ref&gt; a general in the Latvian Army, and sister of poet [[Astrid Ivask]]. After the Soviet Union [[Soviet occupation of Latvia in 1940|occupied Latvia in 1940]], Mārtiņš Hartmanis was arrested by Soviets and died in a prison. At the end of [[World War II]], the wife and children of Mārtiņš Hartmanis left Latvia as refugees, fearing for their safety if the Soviet Union took over Latvia again.

They first moved to [[Germany]], where Juris Hartmanis received the equivalent of a master's degree in physics from the [[University of Marburg]]. Then he moved to the [[United States]], where he received master's degree in applied mathematics at the [[University of Kansas City]] (now known as the [[University of Missouri-Kansas City]]) in 1951 and a [[Doctor of Philosophy|Ph.D.]] in mathematics from [[Caltech]] under the supervision of [[Robert P. Dilworth]] in 1955. The [[University of Missouri-Kansas City]] honored him with Honorary Doctor of Humane Letters in May 1999.

After teaching at [[Cornell University]] and [[Ohio State University]], Hartmanis joined the [[General Electric]] Research Laboratory in 1958. While at General Electric, he developed many principles of computational complexity theory. In 1965, he became a professor at [[Cornell University]]. At Cornell, he was one of founders and the first chairman of its [[computer science]] department (which was one of the first computer science departments in the world). Hartmanis is a [[Fellow]] of the [[Association for Computing Machinery]] and of the [[American Mathematical Society]]&lt;ref&gt;[http://www.ams.org/profession/fellows-list List of Fellows of the American Mathematical Society], retrieved 2013-01-19.&lt;/ref&gt; and a member of the [[National Academy of Engineering]] and [[National Academy of Sciences]].&lt;ref&gt;[http://www.nasonline.org/news-and-multimedia/news/2013_04_30_NAS_Election.html National Academy of Sciences Members and Foreign Associates Elected] {{webarchive|url=https://web.archive.org/web/20130527095103/http://www.nasonline.org/news-and-multimedia/news/2013_04_30_NAS_Election.html |date=2013-05-27 }}, [[National Academy of Sciences]], April 30, 2013.&lt;/ref&gt;

He is best known for his Turing-award-winning paper with Richard Stearns, in which he introduced [[time complexity]] classes ''[[DTIME|TIME (f(n))]]'' and proved the [[time hierarchy theorem]]. Another paper by Hartmanis from 1977, with Leonard Berman, introduced the still-unsolved [[Berman–Hartmanis conjecture]] that all NP-complete languages are polynomial time isomorphic.

==Selected publications==
*{{citation
 | last1 = Berman | first1 = L.
 | last2 = Hartmanis | first2 = J.
 | issue = 2
 | journal = SIAM Journal on Computing
 | mr = 0455536
 | pages = 305–322
 | title = On isomorphisms and density of NP and other complete sets
 | volume = 6 |doi=10.1137/0206023
 |url=http://epubs.siam.org/doi/abs/10.1137/0206023
 | year = 1977}}.
*{{citation
 | last1 = Hartmanis | first1 = J.
 | last2 = Stearns | first2 = R. E. | author2-link = Richard Stearns (computer scientist)
 | journal = [[Transactions of the American Mathematical Society]]
 | pages = 285–306
 | title = On the computational complexity of algorithms
 | volume = 117
 | year = 1965
 | mr = 0170805
 | jstor = 1994208 | doi=10.2307/1994208}}.

==References==
{{reflist}}

==External links==
*[http://www.cs.cornell.edu/annual_report/00-01/bios.htm#hartmanis Hartmanis biography at Cornell]
*{{MathGenealogy|id=10404}}

{{Authority control}}
{{Turing award}}

{{DEFAULTSORT:Hartmanis, Juris}}
[[Category:American computer scientists]]
[[Category:Latvian computer scientists]]
[[Category:Theoretical computer scientists]]
[[Category:1928 births]]
[[Category:Living people]]
[[Category:Fellows of the Association for Computing Machinery]]
[[Category:Fellows of the American Mathematical Society]]
[[Category:Members of the United States National Academy of Engineering]]
[[Category:Members of the United States National Academy of Sciences]]
[[Category:Turing Award laureates]]
[[Category:Cornell University faculty]]
[[Category:California Institute of Technology alumni]]
[[Category:Latvian emigrants to the United States]]
[[Category:People from Riga]]
[[Category:Latvian World War II refugees]]
[[Category:20th-century American engineers]]
[[Category:20th-century American scientists]]
[[Category:Santa Fe Institute people]]</text>
      <sha1>mdw85a9at2rdnoit69wwovtl64xb49b</sha1>
    </revision>
  </page>
  <page>
    <title>Kunita–Watanabe inequality</title>
    <ns>0</ns>
    <id>33744493</id>
    <revision>
      <id>854858740</id>
      <parentid>795813612</parentid>
      <timestamp>2018-08-14T08:12:14Z</timestamp>
      <contributor>
        <username>Beland</username>
        <id>57939</id>
      </contributor>
      <minor/>
      <comment>&amp;ocirc -&gt; ô for readability</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1333">In [[stochastic calculus]], the '''Kunita–Watanabe inequality''' is a generalization of the [[Cauchy–Schwarz inequality]] to integrals of [[stochastic processes]].

== Statement of the theorem ==
Let ''M'', ''N'' be continuous [[local martingale]]s and ''H'', ''K'' [[measurable]] processes. Then

: &lt;math&gt; \int_0^t \left| H_s \right| \left| K_s \right| \left| \mathrm{d} \langle M,N \rangle_s \right| \leq  \sqrt{\int_0^t  H_s^2  \,\mathrm{d} \langle M \rangle_s} \sqrt{\int_0^t K_s^2 \,\mathrm{d} \langle N \rangle_s} &lt;/math&gt;

where the brackets indicates the [[Quadratic variation|quadratic variation and quadratic covariation]] operators. The integrals are understood in the [[Lebesgue–Stieltjes integration|Lebesgue–Stieltjes]] sense.

==References==
{{reflist}}
*{{Cite book | title=Diffusions, Markov Processes and Martingales |volume=II, Itô; Calculus | page = 50 | first1=L. C. G. | last1=Rogers | first2= D. | last2=Williams | authorlink2=David Williams (mathematician) | authorlink1=Chris Rogers (mathematician) | year=1987 | publisher=Cambridge University Press |isbn=0-521-77593-0 |doi=10.1017/CBO9780511805141 |url=https://books.google.com/books?id=bDQy-zoHWfcC&amp;pg=PA50 |postscript=&lt;!--None--&gt;}}

{{DEFAULTSORT:Kunita-Watanabe theorem}}
[[Category:Probability theorems]]
[[Category:Probabilistic inequalities]]</text>
      <sha1>qof9sipab4adj7zz1mgv90iz1fssx3p</sha1>
    </revision>
  </page>
  <page>
    <title>Linear bounded automaton</title>
    <ns>0</ns>
    <id>1811962</id>
    <revision>
      <id>819472194</id>
      <parentid>816791793</parentid>
      <timestamp>2018-01-09T15:32:13Z</timestamp>
      <contributor>
        <username>Tea2min</username>
        <id>36029</id>
      </contributor>
      <comment>/* LBA and context-sensitive languages */ Prefer redirect over link to anchor/section header.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6439">In [[computer science]], a '''linear bounded automaton''' (plural '''linear bounded automata''', abbreviated '''LBA''') is a restricted form of [[Turing machine]].

== Operation ==
A linear bounded automaton is a [[nondeterministic Turing machine]] that satisfies the following three conditions:

* Its input alphabet includes two special symbols, serving as left and right endmarkers.
* Its transitions may not print other symbols over the endmarkers.
* Its transitions may neither move to the left of the left endmarker nor to the right of the right endmarker.&lt;ref name="Hopcroft.Ullman.1979"&gt;{{cite book| author1=John E. Hopcroft |author2=Jeffrey D. Ullman| author1link=John E. Hopcroft| author2link=Jeffrey D. Ullman|title=Introduction to Automata Theory, Languages, and Computation| year=1979| publisher=Addison-Wesley| isbn=0-201-02988-X}}&lt;/ref&gt;{{rp|225}}
In other words: 
instead of having potentially infinite tape on which to compute, computation is restricted to the portion of the tape containing the input plus the two tape squares holding the endmarkers. 

An alternative, '''less restrictive definition''' is as follows:
* Like a [[Turing machine]], an LBA possesses a tape made up of cells that can contain symbols from a [[finite set|finite]] [[alphabet (computer science)|alphabet]], a head that can read from or write to one cell on the tape at a time and can be moved, and a finite number of states. 
* An LBA differs from a [[Turing machine]] in that while the tape is initially considered to have unbounded length, only a finite contiguous portion of the tape, whose length is a [[linear function]] of the length of the initial input, can be accessed by the read/write head; hence the name ''linear bounded automaton''.&lt;ref name="Hopcroft.Ullman.1979"/&gt;{{rp|225}}

This limitation makes an LBA a somewhat more accurate model of a real-world [[computer]] than a Turing machine, whose definition assumes unlimited tape.

The strong and the weaker definition lead to the same computational abilities of the respective automaton classes,&lt;ref name="Hopcroft.Ullman.1979"/&gt;{{rp|225}} due to the ''[[linear speedup theorem]]''.

==LBA and context-sensitive languages==
Linear bounded automata are [[acceptor (finite-state machine)|acceptor]]s for the class of [[context-sensitive language]]s.&lt;ref name="Hopcroft.Ullman.1979"/&gt;{{rp|225-226}}  The only restriction placed on [[Formal_grammar|grammars]] for such languages is that no production maps a string to a shorter string.  Thus no derivation of a string in a context-sensitive language can contain a [[sentential form]] longer than the string itself.  Since there is a one-to-one correspondence between linear-bounded automata and such grammars, no more tape than that occupied by the original string is necessary for the string to be recognized by the automaton.

==History==
In 1960, [[John Myhill]] introduced an automaton model today known as deterministic linear bounded automaton.&lt;ref&gt;{{cite report | author=John Myhill | authorlink=John Myhill | title=Linear Bounded Automata | institution=Wright Patterson AFB, Wright Air Development Division, Ohio | type=WADD Technical Note | number=60-165  | date=June 1960 }}&lt;/ref&gt; In 1963, Peter S. Landweber proved that the languages accepted by deterministic LBAs are context-sensitive.&lt;ref&gt;{{cite journal | author=P.S. Landweber | title=Three Theorems on Phrase Structure Grammars of Type 1 | journal=[[Information and Control]] | volume=6 | number=2 | pages=131&amp;mdash;136 | url=http://www.sciencedirect.com/science/article/pii/S0019995863901694/pdf?md5=15d5b2836e2e59a70ed02fb4074b63f9&amp;pid=1-s2.0-S0019995863901694-main.pdf | year=1963 | doi=10.1016/s0019-9958(63)90169-4}}&lt;/ref&gt; In 1964, [[S.-Y. Kuroda]] introduced the more general model of (nondeterministic) linear bounded automata, noted that Landweber's proof also works for nondeterministic linear bounded automata, and showed that the languages accepted by them are precisely the context-sensitive languages.&lt;ref&gt;{{cite journal | author=Sige-Yuki Kuroda | authorlink=Sige-Yuki Kuroda |title=Classes of languages and linear-bounded automata | journal=Information and Control | volume=7 | number=2 | pages=207&amp;mdash;223 | url=http://www.sciencedirect.com/science/article/pii/S0019995864901202/pdf?md5=0a10da0e20ccd77c5d61461764f42e5f&amp;pid=1-s2.0-S0019995864901202-main.pdf | date=Jun 1964 | doi=10.1016/s0019-9958(64)90120-2}}&lt;/ref&gt;&lt;ref&gt;{{cite book|author=Willem J. M. Levelt| authorlink=Willem Levelt| title=An Introduction to the Theory of Formal Languages and Automata|url=https://books.google.com/books?id=tFvtwGYNe7kC&amp;pg=PA126|year=2008|publisher=John Benjamins Publishing|isbn=90-272-3250-4|pages=126–127}}&lt;/ref&gt;

==LBA problems==
In his seminal paper, Kuroda also stated two research challenges, which subsequently became famously known as the "LBA problems": The first LBA problem is whether the class of languages accepted by LBA is equal to the class of languages accepted by deterministic LBA. This problem can be phrased succinctly in the language of [[computational complexity theory]] as:
:'''First LBA problem''': Is '''[[NSPACE]]'''(O(n)) = '''[[DSPACE]]'''(O(n))?
The second LBA problem is whether the class of languages accepted by LBA is closed under complement.
:'''Second LBA problem''': Is '''[[NSPACE]]'''(O(n)) = '''co-[[NSPACE]]'''(O(n))?
As observed already by Kuroda, a negative answer to the second LBA problem would imply a negative answer to the first problem. But the second LBA problem has an affirmative answer, which is implied by the [[Immerman–Szelepcsényi theorem]] proved 20 years after the problem was raised. As of today, the first LBA problem still remains open.

==References==
&lt;references/&gt;

== External links ==
* [https://web.archive.org/web/20070205070159/http://www.cs.uky.edu/~lewis/texts/theory/automata/lb-auto.pdf Linear Bounded Automata] by [https://web.archive.org/web/20070109012311/http://www.cs.uky.edu/~lewis/ Forbes D. Lewis]
* [http://www.cs.uiowa.edu/~fleck/PartIIIxpar/sld006.htm Linear Bounded Automata] slides, part of [http://www.cs.uiowa.edu/~fleck/PartIIIxpar/ Context-sensitive Languages] by [http://www.cs.uiowa.edu/~fleck Arthur C. Fleck]
* [http://www.seas.upenn.edu/~cit596/notes/dave/chomsky2.html Linear-Bounded Automata], part of Theory of Computation syllabus, by David Matuszek

{{Formal languages and grammars}}

[[Category:Automata (computation)]]
[[Category:Models of computation]]</text>
      <sha1>hkvweaoxcvjygfj6rejtdfhg3lr04al</sha1>
    </revision>
  </page>
  <page>
    <title>List of integrals of inverse trigonometric functions</title>
    <ns>0</ns>
    <id>234969</id>
    <revision>
      <id>847301681</id>
      <parentid>847301455</parentid>
      <timestamp>2018-06-24T10:04:37Z</timestamp>
      <contributor>
        <ip>106.51.202.53</ip>
      </contributor>
      <comment>/* Arccosecant function integration formulas */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5789">{{Trigonometry}}
The following is a list of [[indefinite integral]]s ([[antiderivative]]s) of expressions involving the [[inverse trigonometric function]]s. For a complete list of integral formulas, see [[lists of integrals]].

* The inverse trigonometric functions are also known as the "arc functions".
* ''C'' is used for the arbitrary [[constant of integration]] that can only be determined if something about the value of the integral at some point is known. Thus each function has an infinite number of antiderivatives.
* There are three common notations for inverse trigonometric functions.  The arcsine function, for instance, could be written as ''sin&lt;sup&gt;&amp;minus;1&lt;/sup&gt;'', ''asin'', or, as is used on this page, ''arcsin''.
* For each inverse trigonometric integration formula below there is a corresponding formula in the [[list of integrals of inverse hyperbolic functions]].

== Arcsine function integration formulas ==

:&lt;math&gt;\int\arcsin(x)\,dx=
  x\arcsin(x)+
 {\sqrt{1-x^2}}+C&lt;/math&gt;

:&lt;math&gt;\int\arcsin(ax)\,dx=
x\arcsin(ax)+
  \frac{\sqrt{1-a^2x^2}}{a}+C&lt;/math&gt;

:&lt;math&gt;\int x\arcsin(ax)\,dx=
  \frac{x^2\arcsin(ax)}{2}-
  \frac{\arcsin(ax)}{4\,a^2}+
  \frac{x\sqrt{1-a^2x^2}}{4\,a}+C&lt;/math&gt;

:&lt;math&gt;\int x^2\arcsin(ax)\,dx=
  \frac{x^3\arcsin(ax)}{3}+
  \frac{\left(a^2x^2+2\right)\sqrt{1-a^2x^2}}{9\,a^3}+C&lt;/math&gt;

:&lt;math&gt;\int x^m\arcsin(ax)\,dx=
  \frac{x^{m+1}\arcsin(ax)}{m+1}\,-\,
  \frac{a}{m+1}\int \frac{x^{m+1}}{\sqrt{1-a^2x^2}}\,dx\quad(m\ne-1)&lt;/math&gt;

:&lt;math&gt;\int\arcsin(ax)^2\,dx=
  -2x+x\arcsin(ax)^2+
  \frac{2\sqrt{1-a^2x^2}\arcsin(ax)}{a}+C&lt;/math&gt;

:&lt;math&gt;\int\arcsin(ax)^n\,dx=
  x\arcsin(ax)^n\,+\,
  \frac{n\sqrt{1-a^2x^2}\arcsin(ax)^{n-1}}{a}\,-\,
  n\,(n-1)\int\arcsin(ax)^{n-2}\,dx&lt;/math&gt;

:&lt;math&gt;\int\arcsin(ax)^n\,dx=
  \frac{x\arcsin(ax)^{n+2}}{(n+1)\,(n+2)}\,+\,
  \frac{\sqrt{1-a^2x^2}\arcsin(ax)^{n+1}}{a\,(n+1)}\,-\,
  \frac{1}{(n+1)\,(n+2)}\int\arcsin(ax)^{n+2}\,dx\quad(n\ne-1,-2)&lt;/math&gt;

== Arccosine function integration formulas ==
:&lt;math&gt;\int\arccos(x)\,dx=
  x\arccos(x)-
  {\sqrt{1-x^2}}+C&lt;/math&gt;

:&lt;math&gt;\int\arccos(ax)\,dx=
  x\arccos(ax)-
  \frac{\sqrt{1-a^2x^2}}{a}+C&lt;/math&gt;

:&lt;math&gt;\int x\arccos(ax)\,dx=
  \frac{x^2\arccos(ax)}{2}-
  \frac{\arccos(ax)}{4\,a^2}-
  \frac{x\sqrt{1-a^2x^2}}{4\,a}+C&lt;/math&gt;

:&lt;math&gt;\int x^2\arccos(ax)\,dx=
  \frac{x^3\arccos(ax)}{3}-
  \frac{\left(a^2x^2+2\right)\sqrt{1-a^2x^2}}{9\,a^3}+C&lt;/math&gt;

:&lt;math&gt;\int x^m\arccos(ax)\,dx=
  \frac{x^{m+1}\arccos(ax)}{m+1}\,+\,
  \frac{a}{m+1}\int \frac{x^{m+1}}{\sqrt{1-a^2x^2}}\,dx\quad(m\ne-1)&lt;/math&gt;

:&lt;math&gt;\int\arccos(ax)^2\,dx=
  -2x+x\arccos(ax)^2-
  \frac{2\sqrt{1-a^2x^2}\arccos(ax)}{a}+C&lt;/math&gt;

:&lt;math&gt;\int\arccos(ax)^n\,dx=
  x\arccos(ax)^n\,-\,
  \frac{n\sqrt{1-a^2x^2}\arccos(ax)^{n-1}}{a}\,-\,
  n\,(n-1)\int\arccos(ax)^{n-2}\,dx&lt;/math&gt;

:&lt;math&gt;\int\arccos(ax)^n\,dx=
  \frac{x\arccos(ax)^{n+2}}{(n+1)\,(n+2)}\,-\,
  \frac{\sqrt{1-a^2x^2}\arccos(ax)^{n+1}}{a\,(n+1)}\,-\,
  \frac{1}{(n+1)\,(n+2)}\int\arccos(ax)^{n+2}\,dx\quad(n\ne-1,-2)&lt;/math&gt;

== Arctangent function integration formulas ==
:&lt;math&gt;\int\arctan(x)\,dx=
  x\arctan(x)-
  \frac{\ln\left(x^2+1\right)}{2}+C&lt;/math&gt;

:&lt;math&gt;\int\arctan(ax)\,dx=
  x\arctan(ax)-
  \frac{\ln\left(a^2x^2+1\right)}{2\,a}+C&lt;/math&gt;

:&lt;math&gt;\int x\arctan(ax)\,dx=
  \frac{x^2\arctan(ax)}{2}+
  \frac{\arctan(ax)}{2\,a^2}-\frac{x}{2\,a}+C&lt;/math&gt;

:&lt;math&gt;\int x^2\arctan(ax)\,dx=
  \frac{x^3\arctan(ax)}{3}+
  \frac{\ln\left(a^2x^2+1\right)}{6\,a^3}-\frac{x^2}{6\,a}+C&lt;/math&gt;

:&lt;math&gt;\int x^m\arctan(ax)\,dx=
  \frac{x^{m+1}\arctan(ax)}{m+1}-
  \frac{a}{m+1}\int \frac{x^{m+1}}{a^2x^2+1}\,dx\quad(m\ne-1)&lt;/math&gt;

== Arccotangent function integration formulas ==

:&lt;math&gt;\int\arccot(x)\,dx=
  x\arccot(x)+
  \frac{\ln\left(x^2+1\right)}{2}+C&lt;/math&gt;

:&lt;math&gt;\int\arccot(ax)\,dx=
  x\arccot(ax)+
  \frac{\ln\left(a^2x^2+1\right)}{2\,a}+C&lt;/math&gt;

:&lt;math&gt;\int x\arccot(ax)\,dx=
  \frac{x^2\arccot(ax)}{2}+
  \frac{\arccot(ax)}{2\,a^2}+\frac{x}{2\,a}+C&lt;/math&gt;

:&lt;math&gt;\int x^2\arccot(ax)\,dx=
  \frac{x^3\arccot(ax)}{3}-
  \frac{\ln\left(a^2x^2+1\right)}{6\,a^3}+\frac{x^2}{6\,a}+C&lt;/math&gt;

:&lt;math&gt;\int x^m\arccot(ax)\,dx=
  \frac{x^{m+1}\arccot(ax)}{m+1}+
  \frac{a}{m+1}\int \frac{x^{m+1}}{a^2x^2+1}\,dx\quad(m\ne-1)&lt;/math&gt;

== Arcsecant function integration formulas ==

:&lt;math&gt;\int\arcsec(x)\,dx=
  x\arcsec(x)-\operatorname{arcosh}|x|+C&lt;/math&gt;

:&lt;math&gt;\int\arcsec(ax)\,dx=
  x\arcsec(ax)-
  \frac{1}{a}\,\operatorname{arcosh}|ax|+C&lt;/math&gt;
:&lt;math&gt;\int x\arcsec(ax)\,dx=
  \frac{x^2\arcsec(ax)}{2}-
  \frac{x}{2\,a}\sqrt{1-\frac{1}{a^2x^2}}+C&lt;/math&gt;

:&lt;math&gt;\int x^2\arcsec(ax)\,dx=
  \frac{x^3\arcsec(ax)}{3}\,-\,
  \frac{\operatorname{arcosh}|ax|}{6\,a^3}\,-\,
  \frac{x^2}{6\,a}\sqrt{1-\frac{1}{a^2x^2}}\,+\,C&lt;/math&gt;

:&lt;math&gt;\int x^m\arcsec(ax)\,dx=
  \frac{x^{m+1}\arcsec(ax)}{m+1}\,-\,
  \frac{1}{a\,(m+1)}\int \frac{x^{m-1}}{\sqrt{1-\frac{1}{a^2x^2}}}\,dx\quad(m\ne-1)&lt;/math&gt;

== Arccosecant function integration formulas ==
:&lt;math&gt;\int\arccsc(x)\,dx= 
  x\arccsc(x) \, + \,
 \ln\left(\left|x\right|+\sqrt{x^2-1}\right)\,+\,C=
  x\arccsc(x)\,+\,
 \operatorname{arcosh}|x|\,+\,C &lt;/math&gt;

:&lt;math&gt;\int\arccsc(ax)\,dx=
  x\arccsc(ax)+
  \frac{1}{a}\,\operatorname{arctanh}\,\sqrt{1-\frac{1}{a^2x^2}}+C&lt;/math&gt;

:&lt;math&gt;\int x\arccsc(ax)\,dx=
  \frac{x^2\arccsc(ax)}{2}+
  \frac{x}{2\,a}\sqrt{1-\frac{1}{a^2x^2}}+C&lt;/math&gt;

:&lt;math&gt;\int x^2\arccsc(ax)\,dx=
  \frac{x^3\arccsc(ax)}{3}\,+\,
  \frac{1}{6\,a^3}\,\operatorname{arctanh}\,\sqrt{1-\frac{1}{a^2x^2}}\,+\,
  \frac{x^2}{6\,a}\sqrt{1-\frac{1}{a^2x^2}}\,+\,C&lt;/math&gt;

:&lt;math&gt;\int x^m\arccsc(ax)\,dx=
  \frac{x^{m+1}\arccsc(ax)}{m+1}\,+\,
  \frac{1}{a\,(m+1)}\int \frac{x^{m-1}}{\sqrt{1-\frac{1}{a^2x^2}}}\,dx\quad(m\ne-1)&lt;/math&gt;

{{Lists of integrals}}

[[Category:Integrals|Arc functions]]
[[Category:Mathematics-related lists|Integrals of arc functions]]</text>
      <sha1>4va88xwe37c6y6tuq1jw08g6olcamoe</sha1>
    </revision>
  </page>
  <page>
    <title>List of things named after Alexander Grothendieck</title>
    <ns>0</ns>
    <id>39402871</id>
    <revision>
      <id>815543185</id>
      <parentid>796053164</parentid>
      <timestamp>2017-12-15T13:49:47Z</timestamp>
      <contributor>
        <username>Omnipaedista</username>
        <id>8524693</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1495">The mathematician [[Alexander Grothendieck]] (1928–2014) is the [[eponym]] of many things.

== Mathematics ==
*[[Ax–Grothendieck theorem]]
*[[Birkhoff–Grothendieck theorem]]
*[[Brieskorn–Grothendieck resolution]]
*[[Grothendieck category]]
*[[Grothendieck's connectedness theorem]]
*[[Grothendieck connection]]
*[[Grothendieck construction]]
*[[Coherent duality|Grothendieck duality]]
*[[Grothendieck existence theorem]]
*[[Grothendieck fibration]]
*[[Grothendieck's Galois theory]]
*[[Grothendieck group]]
*[[Grothendieck inequality]] or [[Grothendieck constant]]
*[[Grothendieck–Katz p-curvature conjecture]]
*[[Grothendieck local duality]]
*[[Grothendieck's monodromy theorem]]
*[[Grothendieck's mysterious functor]]
*[[Grothendieck–Ogg–Shafarevich formula]]
*[[Grothendieck period conjecture]]
*[[Grothendieck prime]]
*[[Grothendieck's relative point of view]]
*[[Grothendieck–Riemann–Roch theorem]]
*[[Grothendieck's Séminaire de géométrie algébrique]]
*[[Grothendieck's six operators]]
*[[Grothendieck space]]
*[[Grothendieck spectral sequence]]
*[[Grothendieck–Teichmüller group]]
*[[Grothendieck–Teichmüller theory]]
*[[Grothendieck trace formula]]
*[[Grothendieck topology]]
*[[Grothendieck universe]]
*[http://imag.edu.umontpellier.fr Institut Montpelliérain Alexander Grothendieck]
*[[Tarski–Grothendieck set theory]]

{{DEFAULTSORT:List Of Topics Named After Alexander Grothendieck}}
[[Category:Lists of things named after mathematicians|Grothendieck]]</text>
      <sha1>buzeokxb2ewsy560pdau1gvqjc80hc2</sha1>
    </revision>
  </page>
  <page>
    <title>Markov–Krein theorem</title>
    <ns>0</ns>
    <id>39643921</id>
    <revision>
      <id>806314053</id>
      <parentid>753395092</parentid>
      <timestamp>2017-10-21T04:50:54Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>[[S. Lynne Stokes]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2393">In [[probability theory]], the '''Markov–Krein theorem''' gives the best upper and lower bounds on the expected values of certain functions of a random variable where only the first moments of the random variable are known.&lt;ref&gt;{{cite journal | last1 = Stokes | first1 = S. Lynne |author1-link = S. Lynne Stokes | first2 = Mary H. L | last2 = Mulry-Liggan | title = Estimation of Interviewer Variance for Categorical Variables | journal = Journal of Official Statistics | volume = 3 | year = 1987 | pages = 389–401 | url = https://www.amstat.org/sections/srms/Proceedings/papers/1984_133.pdf | accessdate = 11 June 2013}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | last1 = Brockett | first1 = P. L. | last2 = Kahane | first2 = Y. | doi = 10.1287/mnsc.38.6.851 | title = Risk, Return, Skewness and Preference | journal = Management Science | volume = 38 | issue = 6 | pages = 851 | year = 1992 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | last1 = Simar | first1 = L.| title = Maximum Likelihood Estimation of a Compound Poisson Process | doi = 10.1214/aos/1176343651 | journal = [[The Annals of Statistics]]| volume = 4 | issue = 6 | pages = 1200 | year = 1976 | jstor = 2958588| pmid =  | pmc = }}&lt;/ref&gt;&lt;ref&gt;{{cite book | first1= S.|last1= Karlin | authorlink1 = Samuel Karlin | first2 =  W. J. | last2 = Studden | title = Tchebycheff Systems, with Applications in Analysis and Statistics | publisher = Interscience | location = New York | year = 1966 | page= 82}}&lt;/ref&gt; The result is named after [[Andrey Markov]] and [[Mark Krein]].&lt;ref&gt;{{cite journal | last1= Kreĭn | first1= M. G.| authorlink1 = Mark Krein| title = The ideas of P. L. Čebyšev and A. A. Markov in the theory of limiting values of integrals and their further development | journal = Amer. Math. Soc. Transl. |volume = 2 | number= 12| year= 1959 |pages= 1–121|mr=0113106 }}&lt;/ref&gt;

The theorem can be used to bound average response times in the [[M/G/k queue]]ing system.&lt;ref&gt;{{Cite journal | last1 = Gupta | first1 = V. | last2 = Osogami | first2 = T. | doi = 10.1007/s11134-011-9248-8 | title = On Markov–Krein characterization of the mean waiting time in M/G/K and other queueing systems | journal = Queueing Systems | volume = 68 | issue = 3–4 | pages = 339 | year = 2011 | pmid =  | pmc = }}&lt;/ref&gt;

==References==

{{Reflist}}

{{DEFAULTSORT:Markov-Krein theorem}}
[[Category:Probability theorems]]


{{probability-stub}}</text>
      <sha1>h01n5kncpnblkxossmfadtkap81nf54</sha1>
    </revision>
  </page>
  <page>
    <title>Mathematical programming with equilibrium constraints</title>
    <ns>0</ns>
    <id>18894568</id>
    <revision>
      <id>786530085</id>
      <parentid>667519531</parentid>
      <timestamp>2017-06-20T01:37:27Z</timestamp>
      <contributor>
        <username>CBM</username>
        <id>1108292</id>
      </contributor>
      <minor/>
      <comment>Manually reviewed edit to replace magic words per [[Special:PermanentLink/772743896#Future_of_magic_links|local rfc]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1417">'''Mathematical programming with equilibrium constraints (MPEC)''' is the study of 
constrained [[optimization (mathematics)|optimization]] problems  where the constraints include [[variational inequalities]] or  [[complementarity theory|complementarities]]. MPEC is related to the [[Stackelberg game]]. 

MPEC is used in the study of [[engineering design]], [[economic equilibrium]] and [[multilevel games]].

MPEC is difficult to deal with because its [[feasible region]] is not necessarily convex or even connected.

==References==
&lt;references/&gt;
* Z.-Q. Luo, J.-S. Pang and D. Ralph: ''Mathematical Programs with Equilibrium Constraints''. Cambridge University Press, 1996, {{isbn|0-521-57290-8}}.
* B. Baumrucker, J. Renfro, L. T. Biegler, MPEC problem formulations and solution strategies with chemical engineering applications, Computers &amp; Chemical Engineering, 32 (12) (2008) 2903-2913.
* A. U. Raghunathan, M. S. Diaz, L. T. Biegler, An MPEC formulation for dynamic optimization of distillation operations, Computers &amp; Chemical Engineering, 28 (10) (2004) 2037-2052.

==External links==
* [http://apmonitor.com/wiki/index.php/Apps/MpecExamples MPEC examples] such as SIGN, ABS, MIN, and MAX
* [http://apmonitor.com/me575/index.php/Main/LogicalConditions Formulating logical statements] as continuously differentiable nonlinear programming problems

[[Category:Mathematical optimization]]

{{mathapplied-stub}}</text>
      <sha1>m0i612xkb8vn5doygppzih4s28k2dd3</sha1>
    </revision>
  </page>
  <page>
    <title>Maximal semilattice quotient</title>
    <ns>0</ns>
    <id>9758445</id>
    <revision>
      <id>665980498</id>
      <parentid>418095000</parentid>
      <timestamp>2015-06-08T02:58:03Z</timestamp>
      <contributor>
        <username>K9re11</username>
        <id>19647483</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1671">{{Expert-subject|date=March 2009}}

In [[abstract algebra]], a branch of [[mathematics]], a '''maximal semilattice quotient''' is a [[commutative monoid]] derived from another commutative monoid by making certain elements [[equivalence relation|equivalent]] to each other.

Every commutative monoid can be endowed with its ''algebraic'' [[preorder]]ing ≤ . By definition, ''x&amp;le; y'' holds, if there exists ''z'' such that ''x+z=y''. Further, for ''x, y'' in ''M'', let &lt;math&gt;x\propto y&lt;/math&gt; hold, if there exists a positive integer ''n'' such that ''x≤ ny'', and let &lt;math&gt;x\asymp y&lt;/math&gt; hold, if &lt;math&gt;x\propto y&lt;/math&gt; and &lt;math&gt;y\propto x&lt;/math&gt;. The [[binary relation]] &lt;math&gt;\asymp&lt;/math&gt; is a [[Congruence relation|monoid congruence]] of ''M'', and the quotient monoid &lt;math&gt;M/{\asymp}&lt;/math&gt; is the ''maximal semilattice quotient'' of ''M''.

This terminology can be explained by the fact that the canonical projection ''p'' from ''M'' onto &lt;math&gt;M/{\asymp}&lt;/math&gt; is universal among all monoid homomorphisms from ''M'' to a (&amp;or;,0)-[[semilattice]], that is, for any (&amp;or;,0)-semilattice ''S'' and any monoid homomorphism ''f: M→ S'', there exists a unique (&amp;or;,0)-homomorphism &lt;math&gt;g\colon M/{\asymp}\to S&lt;/math&gt; such that ''f=gp''.

If ''M'' is a [[refinement monoid]], then &lt;math&gt;M/{\asymp}&lt;/math&gt; is a [[Distributivity (order theory)|distributive semilattice]].

==References==

A.H. Clifford and G.B. Preston, The Algebraic Theory of Semigroups. Vol. I. Mathematical Surveys, No. '''7''', American Mathematical Society, Providence, R.I. 1961. xv+224 p.

{{DEFAULTSORT:Maximal Semilattice Quotient}}
[[Category:Lattice theory]]


{{algebra-stub}}</text>
      <sha1>lo53c3w7ydm233mzq55fsqugh6u17st</sha1>
    </revision>
  </page>
  <page>
    <title>Microscopic traffic flow model</title>
    <ns>0</ns>
    <id>9518854</id>
    <revision>
      <id>784191213</id>
      <parentid>734612049</parentid>
      <timestamp>2017-06-06T23:56:05Z</timestamp>
      <contributor>
        <username>Rayman60</username>
        <id>11276832</id>
      </contributor>
      <comment>Added {{[[Template:original research|original research]]}} tag to article ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4425">{{original research|date=June 2017}}
{{Unreferenced|date=February 2007}}
'''Microscopic traffic flow models''' are a class of [[Scientific modeling|scientific models]] of [[traffic flow|vehicular traffic dynamics]].

In contrast to [[Macroscopic traffic flow model|macroscopic models]], microscopic traffic flow models simulate single vehicle-driver units, so the dynamic variables of the models represent microscopic properties like the position and velocity of single vehicles.

==Car-following models==
Also known as ''time-continuous models'', all car-following models have in common that they are defined by [[ordinary differential equations]] describing the complete dynamics of the vehicles' positions &lt;math&gt;x_\alpha&lt;/math&gt; and velocities &lt;math&gt;v_\alpha&lt;/math&gt;. It is assumed that the input stimuli of the drivers are restricted to their own velocity &lt;math&gt;v_\alpha&lt;/math&gt;, the net distance (bumper-to-bumper distance) &lt;math&gt;s_\alpha = x_{\alpha-1} - x_\alpha - l_{\alpha-1}&lt;/math&gt; to the leading vehicle &lt;math&gt;\alpha-1&lt;/math&gt; (where &lt;math&gt;l_{\alpha-1}&lt;/math&gt; denotes the vehicle length), and the velocity &lt;math&gt;v_{\alpha-1}&lt;/math&gt; of the leading vehicle. The [[equation of motion]] of each vehicle is characterized by an acceleration function that depends on those input stimuli:

:&lt;math&gt;\ddot{x}_\alpha(t) = \dot{v}_\alpha(t) = F(v_\alpha(t), s_\alpha(t), v_{\alpha-1}(t))&lt;/math&gt;

In general, the driving behavior of a single driver-vehicle unit &lt;math&gt;\alpha&lt;/math&gt; might not merely depend on the immediate leader &lt;math&gt;\alpha-1&lt;/math&gt; but on the &lt;math&gt;n_a&lt;/math&gt; vehicles in front. The equation of motion in this more generalized form reads:

:&lt;math&gt;\dot{v}_\alpha(t) = f(x_\alpha(t), v_\alpha(t), x_{\alpha-1}(t), v_{\alpha-1}(t), \ldots, x_{\alpha-n_a}(t), v_{\alpha-n_a}(t))&lt;/math&gt;

===Examples of car-following models===
* [[Optimal velocity model]] (OVM)
* [[Velocity difference model]] (VDIFF)
* [[VISSIM|Wiedemann model]] (1974)
* [[Intelligent driver model]] (IDM, 1999)
** [http://volkhin.com/RoadTrafficSimulator/ RoadTrafficSimulator - visualization of the model]
* [[Gipps' model]] (Gipps, 1981)

==Cellular automaton models==
[[Cellular automaton]] (CA) models use integer variables to describe the dynamical properties of the system. The road is divided into sections of a certain length &lt;math&gt;\Delta x&lt;/math&gt; and the time is [[discretization|discretized]] to steps of &lt;math&gt;\Delta t&lt;/math&gt;. Each road section can either be occupied by a vehicle or empty and the dynamics are given by update rules of the form:

:&lt;math&gt;v_\alpha^{t+1} = f(s_\alpha^t, v_\alpha^t, v_{\alpha-1}^t, \ldots)&lt;/math&gt;
:&lt;math&gt;x_\alpha^{t+1} = x_\alpha^t + v_\alpha^{t+1}&lt;/math&gt;

(the simulation time &lt;math&gt;t&lt;/math&gt; is measured in units of &lt;math&gt;\Delta t&lt;/math&gt; and the vehicle positions &lt;math&gt;x_\alpha&lt;/math&gt; in units of &lt;math&gt;\Delta x&lt;/math&gt;).

The time scale is typically given by the reaction time of a human driver, &lt;math&gt;\Delta t = 1 \text{s}&lt;/math&gt;. With &lt;math&gt;\Delta t&lt;/math&gt; fixed, the length of the road sections determines the granularity of the model. At a complete standstill, the average road length occupied by one vehicle is approximately 7.5 meters. Setting &lt;math&gt;\Delta x&lt;/math&gt; to this value leads to a model where one vehicle always occupies exactly one section of the road and a velocity of 5 corresponds to &lt;math&gt;5 \Delta x/\Delta t = 135 \text{km/h}&lt;/math&gt;, which is then set to be the maximum velocity a driver wants to drive at. However, in such a model, the smallest possible acceleration would be &lt;math&gt;\Delta x/(\Delta t)^2 = 7.5 \text{m}/\text{s}^2&lt;/math&gt; which is unrealistic. Therefore, many modern CA models use a finer spatial discretization, for example &lt;math&gt;\Delta x = 1.5 \text{m}&lt;/math&gt;, leading to a smallest possible acceleration of &lt;math&gt;1.5 \text{m}/\text{s}^2&lt;/math&gt;.

Although cellular automaton models lack the accuracy of the time-continuous car-following models, they still have the ability to reproduce a wide range of traffic phenomena. Due to the simplicity of the models, they are numerically very efficient and can be used to simulate large road networks in realtime or even faster.

===Examples of CA models===
* [[Rule 184]]
* [[Biham–Middleton–Levine traffic model]]
* [[Nagel–Schreckenberg model]] (NaSch, 1992)

==See also==
*[[Microsimulation]]

{{DEFAULTSORT:Microscopic Traffic Flow Model}}
[[Category:Road traffic management]]
[[Category:Mathematical modeling]]</text>
      <sha1>q9s0y4gzxp2aiyl32xzlil0ome1w73f</sha1>
    </revision>
  </page>
  <page>
    <title>Nixon diamond</title>
    <ns>0</ns>
    <id>1579841</id>
    <revision>
      <id>745021483</id>
      <parentid>745021457</parentid>
      <timestamp>2016-10-18T21:17:35Z</timestamp>
      <contributor>
        <username>Widefox</username>
        <id>1588193</id>
      </contributor>
      <comment>Added {{[[Template:no footnotes|no footnotes]]}} tag to article ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2282">{{no footnotes|date=October 2016}}
In [[non-monotonic logic|nonmonotonic reasoning]], the '''Nixon diamond''' is a scenario in which default assumptions lead to mutually inconsistent conclusions. The scenario is:

* usually, [[Quakers]] are [[pacifist]]
* usually, [[Republican Party (United States)|Republicans]] are not pacifist
* [[Richard Nixon]] is both a Quaker and a Republican

Since Nixon is a Quaker, one could assume that he is a pacifist; since he is Republican, however, one could also assume he is not a pacifist. The problem is how a [[formal logic]] of nonmonotonic reasoning should deal with such cases. Two approaches can be adopted:

; skeptical : since Nixon can neither be proved to be a pacifist nor the contrary, no conclusion is drawn;
; credulous : since Nixon can be proved to be a pacifist in at least one case, he is believed to be a pacifist; however, since he can also be proved not to be a pacifist, he is also believed not to be a pacifist.

The credulous approach can allow proving both something and its contrary. For this reason, the skeptical approach is often preferred. Another solution to this problem is to attach priorities to default assumptions; for example, the fact that “usually, Republicans are not pacifist” can be assumed more likely than “usually, Quakers are pacifist”, leading to the conclusion that Nixon is not pacifist.

The name ''diamond'' comes from the fact that such a scenario, when expressed in [[inheritance network]]s, is a [[rhombus|diamond shape]]. This example is mentioned for the first time by Reiter and Criscuolo in a slightly different form where the person that is both a Republican and a Quaker is a John instead of Richard Nixon.

==See also==
* [[Default logic]]
* [[Multiple inheritance|Multiple Inheritance]]

==References==
* W. Marek and M. Truszczynski (1993). ''Nonmonotonic Logics: Context-Dependent Reasoning''. Springer.
* R. Reiter and G. Criscuolo (1981). On interacting defaults. In '' Proceedings of the Seventh International Joint Conference on Artificial Intelligence (IJCAI'81)'', pages 94–100.

==External links==
* [http://www.cs.sfu.ca/~bbastani/personal/courses/823/review_of_john_horty.htm Review of John Horty paper on Floating Conclusions]

[[Category:Non-classical logic]]</text>
      <sha1>7ksgx1ab2hds5yxc73nuzms67ll9elm</sha1>
    </revision>
  </page>
  <page>
    <title>Outline of probability</title>
    <ns>0</ns>
    <id>387878</id>
    <revision>
      <id>860177045</id>
      <parentid>836963816</parentid>
      <timestamp>2018-09-18T21:00:24Z</timestamp>
      <contributor>
        <username>Colonies Chris</username>
        <id>577301</id>
      </contributor>
      <minor/>
      <comment>/* See also */minor fixes</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7663">'''[[Probability]]''' is a measure of the likeliness that an event will occur. Probability is used to quantify an attitude of mind towards some proposition of whose truth we are not certain. The proposition of interest is usually of the form "A specific event will occur." The attitude of mind is of the form "How certain are we that the event will occur?" The certainty we adopt can be described in terms of a numerical measure and this number, between 0 and 1 (where 0 indicates impossibility and 1 indicates certainty), we call probability. Probability theory is used extensively in [[statistics]], [[mathematics]], [[science]] and [[philosophy]] to draw conclusions about the likelihood of potential events and the underlying mechanics of complex systems.

{{TOC limit|limit=2}}

==Introduction==
* [[Probability]] and [[randomness]].

==Basic probability==
(Related topics: [[set theory]], [[simple theorems in the algebra of sets]])

===Events===
* [[Event (probability theory)|Events in probability theory]]
* [[Elementary event]]s, [[sample space]]s, [[Venn diagram]]s
* [[Mutually exclusive|Mutual exclusivity]]

===Elementary probability===
* The [[axioms of probability]]
* [[Boole's inequality]]

===Meaning of probability===
* [[Probability interpretations]]
* [[Bayesian probability]]
* [[Frequency probability]]

===Calculating with probabilities===
* [[Conditional probability]]
* The [[law of total probability]]
* [[Bayes' theorem]]

===Independence===
* [[Independence (probability theory)]]

==[[Probability theory]]==
(Related topics: [[measure theory]])

===Measure-theoretic probability===
* [[Sample space]]s, [[sigma-algebra|σ-algebras]] and [[probability measure]]s
* [[Probability space]]
** [[Sample space]]
** [[Standard probability space]]
** [[Random element]]
*** [[Random compact set]]
** [[Dynkin system]]
* [[Probability axioms]]
* [[Event (probability theory)]]
** [[Complementary event]]
* [[Elementary event]]
* "[[Almost surely]]"

===Independence===
* [[Independence (probability theory)]]
* The [[Borel–Cantelli lemma]]s and [[Kolmogorov's zero–one law]]

===Conditional probability===
* [[Conditional probability]]
* [[Conditioning (probability)]]
* [[Conditional expectation]]
* [[Conditional probability distribution]]
* [[Regular conditional probability]]
* [[Disintegration theorem]]
* [[Bayes' theorem]]
* [[Rule of succession]]
* [[Conditional independence]]
* [[Conditional event algebra]]
** [[Goodman–Nguyen–van Fraassen algebra]]

==[[Random variable]]s==
===Discrete and continuous random variables===
* [[Discrete random variable]]s: [[Probability mass function]]s
* [[Continuous random variable]]s: [[Probability density function]]s
* [[Normalizing constant]]s
* [[Cumulative distribution function]]s
* [[Joint distribution|Joint]], [[marginal distribution|marginal]] and [[Conditional distribution|conditional]] distributions

===Expectation===
* [[expected value|Expectation]] (or [[mean]]), [[variance]] and [[covariance]]
** [[Jensen's inequality]]
* General [[Moment about the mean|moments about the mean]]
* [[Correlation|Correlated]] and [[uncorrelated]] random variables
* [[Conditional expectation]]:
** [[law of total expectation]], [[law of total variance]]
* [[Fatou's lemma]] and the [[monotone convergence theorem|monotone]] and [[dominated convergence theorem|dominated]] convergence theorems
* [[Markov's inequality]] and [[Chebyshev's inequality]]

===Independence===
* [[Statistical independence#Independent random variables|Independent random variables]]

===Some common distributions===
* Discrete:
** [[Constant random variable|constant]] (see also [[degenerate distribution]]),
** [[Bernoulli distribution|Bernoulli]] and [[Binomial distribution|binomial]],
** [[Negative binomial distribution|negative binomial]],
** [[Uniform distribution (discrete)|(discrete) uniform]],
** [[geometric distribution|geometric]],
** [[Poisson distribution|Poisson]], and
** [[hypergeometric distribution|hypergeometric]].
* Continuous:
** [[Uniform distribution (continuous)|(continuous) uniform]],
** [[exponential distribution|exponential]],
** [[gamma distribution|gamma]],
** [[beta distribution|beta]],
** [[normal distribution|normal]] (or ''Gaussian'') and [[multivariate normal distribution|multivariate normal]],
** [[chi-squared distribution|χ-squared]] (or chi-squared),
** [[F-distribution]],
** [[Student's t-distribution]], and
** [[Cauchy distribution|Cauchy]].

===Some other distributions===
* [[Cantor distribution|Cantor]]
* [[Fisher–Tippett distribution|Fisher–Tippett]] (or ''Gumbel'')
* [[Pareto distribution|Pareto]]
* [[Benford's law]]

===Functions of random variables===
* [[Sums of random variables]]
* General [[functions of random variables]]
* [[Borel's paradox]]

==Generating functions==
(Related topics: [[integral transform]]s)

===Common generating functions===
* [[Probability-generating function]]s
* [[Moment-generating function]]s
* [[Laplace transform]]s and [[Laplace–Stieltjes transform]]s
* [[Characteristic function (probability theory)|Characteristic function]]s

===Applications===
* [[Central limit theorem#Proof|A proof of the central limit theorem]]
* [[Random sums of random variables]]

==Convergence of random variables==
(Related topics: [[Modes of convergence (annotated index)|convergence]])

===Modes of convergence===
* [[Convergence in distribution]] and [[convergence in probability]],
* Convergence in [[Convergence of random variables#Convergence in mean|mean]], [[Convergence of random variables#Convergence in mean square|mean square]] and [[Convergence of random variables#Convergence in rth mean|''r''th mean]]
* [[Almost sure convergence]]
* [[Skorokhod's representation theorem]]

===Applications===
* [[Central limit theorem]] and [[Law of large numbers|Laws of large numbers]]
** [[Illustration of the central limit theorem]] and a [[concrete illustration of the central limit theorem|'concrete' illustration]]
** [[Berry–Esseen theorem|Berry–Esséen theorem]]
* [[Law of the iterated logarithm]]

==Stochastic processes==
===Some common [[stochastic process]]es===
* [[Random walk]]
* [[Poisson process]]
* [[Compound Poisson process]]
* [[Wiener process]]
* [[Geometric Brownian motion]]
* [[Fractional Brownian motion]]
* [[Brownian bridge]]
* [[Ornstein–Uhlenbeck process]]
* [[Gamma process]]

===Markov processes===
* [[Markov property]]
* [[Branching process]]
** [[Galton–Watson process]]
* [[Markov chain]]
** [[Examples of Markov chains]]
* [[Population process]]es
* Applications to [[queueing theory]]
** [[Erlang distribution]]

===Stochastic differential equations===
* [[Stochastic calculus]]
* [[Diffusion]]s
** [[Brownian motion]]
** [[Wiener equation]]
** [[Wiener process]]

===[[Time series]]===
* [[Moving average model|Moving-average]] and [[autoregressive process|autoregressive]] processes
* [[Correlation function]] and [[autocorrelation]]

===[[Martingale (probability theory)|Martingales]]===
* [[Martingale central limit theorem]]
* [[Azuma's inequality]]

==See also==
* [[Catalog of articles in probability theory]]
* [[Glossary of probability and statistics]]
* [[Notation in probability and statistics]]
* [[List of mathematical probabilists]]
* [[List of probability distributions]]
* [[List of probability topics]]
* [[List of scientific journals in probability]]
* [[Timeline of probability and statistics]]
* [[Topic outline of statistics]]

{{outline footer}}

{{DEFAULTSORT:Outline of probability}}
[[Category:Wikipedia outlines|Probability]]
[[Category:Probability|*]]
[[Category:Mathematics-related lists|Probability]]
[[Category:Statistics-related lists|Probability]]</text>
      <sha1>dvxq7z7dy6mqvkmyegj73qy8k3u73nv</sha1>
    </revision>
  </page>
  <page>
    <title>Parabolic Lie algebra</title>
    <ns>0</ns>
    <id>10538204</id>
    <revision>
      <id>695747688</id>
      <parentid>563937137</parentid>
      <timestamp>2015-12-18T08:28:22Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>/* Bibliography */[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (11757)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1811">In [[algebra]], a '''parabolic Lie algebra''' &lt;math&gt;\mathfrak p&lt;/math&gt; is a subalgebra of a [[semisimple Lie algebra]] &lt;math&gt;\mathfrak g&lt;/math&gt; satisfying one of the following two conditions:
* &lt;math&gt;\mathfrak p&lt;/math&gt; contains a maximal [[solvable Lie algebra|solvable]] subalgebra (a [[Borel subalgebra]]) of &lt;math&gt;\mathfrak g&lt;/math&gt;;
* the [[Killing form|Killing perp]] of &lt;math&gt;\mathfrak p&lt;/math&gt; in &lt;math&gt;\mathfrak g&lt;/math&gt; is the [[Nilradical of a Lie algebra|nilradical]] of &lt;math&gt;\mathfrak p&lt;/math&gt;.
These conditions are equivalent over an [[algebraically closed]] [[field (mathematics)|field]] of [[characteristic zero]], such as the [[complex numbers]]. If the field &lt;math&gt;\mathbb F&lt;/math&gt; is not algebraically closed, then the first condition is replaced by the assumption that
* &lt;math&gt;\mathfrak p\otimes_{\mathbb F}\overline{\mathbb F}&lt;/math&gt; contains a Borel subalgebra of &lt;math&gt; \mathfrak g\otimes_{\mathbb F}\overline{\mathbb F}&lt;/math&gt;
where &lt;math&gt;\overline{\mathbb F}&lt;/math&gt; is the [[algebraic closure]] of &lt;math&gt;\mathbb F&lt;/math&gt;.

==See also==

* [[Generalized flag variety]]

==Bibliography==
* {{citation|first1=Robert J.|last1=Baston|first2=Michael G.|last2=Eastwood|authorlink2=Michael Eastwood|title=The Penrose Transform: its Interaction with Representation Theory|publisher=Oxford University Press|year=1989}}.
* {{Fulton-Harris}}
* {{citation|doi=10.2307/2372388|first=Alexander|last=Grothendieck|authorlink=Alexander Grothendieck|year=1957|title=Sur la classification des fibrés holomorphes sur la sphère de Riemann|journal=Amer. J. Math.|volume=79|issue=1|pages=121–138|jstor=2372388}}.
* {{citation | first=J.|last=Humphreys | title=Linear Algebraic Groups |  location=New York | publisher=Springer | year=1972 | isbn=0-387-90108-6}}

[[Category:Lie algebras]]


{{algebra-stub}}</text>
      <sha1>2e31qcwqbf4jc2tetbpt3tz2439ayyr</sha1>
    </revision>
  </page>
  <page>
    <title>Patlak plot</title>
    <ns>0</ns>
    <id>4921531</id>
    <revision>
      <id>863076250</id>
      <parentid>863076143</parentid>
      <timestamp>2018-10-08T15:26:39Z</timestamp>
      <contributor>
        <username>Fnielsen</username>
        <id>325</id>
      </contributor>
      <comment>/* References */ Further literature</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4181">{{Expert needed|Science|date=November 2008}}

A '''Patlak plot''' (sometimes called '''Gjedde–Patlak plot''', '''Patlak–Rutland plot''', or '''Patlak analysis''')&lt;ref name="Patlak1983"&gt;{{cite journal |author1=C. S. Patlak |author2=R. G. Blasberg |author3=J. D. Fenstermacher | title=Graphical evaluation of blood-to-brain transfer constants from multiple-time uptake data | journal=[[Journal of Cerebral Blood Flow and Metabolism]] | volume=3 | issue=1 | pages=1-7 |date=March 1983 | doi=10.1038/jcbfm.1983.1 | pmid=6822610}}&lt;/ref&gt;&lt;ref name="Patlak1985"&gt;{{cite journal |author1=C.S. Patlak |author2=R.G. Blasberg | title=Graphical evaluation of blood-to-brain transfer constants from multiple-time uptake data. Generalizations | journal=[[Journal of Cerebral Blood Flow and Metabolism]] | volume=5 | issue=4 | pages=584-590 |date=April 1985 | doi=10.1038/jcbfm.1985.87 | pmid=4055928}}&lt;/ref&gt; is a [[Graph of a function|graphical]] analysis technique based on the [[Compartment (pharmacokinetics)|compartment]] model that uses [[linear regression]] to identify and analyze [[pharmacokinetics]] of tracers involving irreversible uptake, such as in the case of [[deoxyglucose]].&lt;ref&gt;{{cite journal | author=A. Gjedde | title=High- and low-affinity transport of D-glucose from blood to brain | journal=[[Journal of Neurochemistry]] | volume=36 | issue=4 | pages=1463-1471 |date=April 1981 | doi=10.1111/j.1471-4159.1981.tb00587.x}}&lt;/ref&gt;&lt;ref&gt;{{cite journal | author=A. Gjedde | title=Calculation of glucose phosphorylation from brain uptake of glucose analogs in vivo: A re-examination | journal=[[Brain Research Reviews]] | volume=4 | issue=2 | pages=237-274 |date=June 1982 | doi=10.1016/0165-0173(82)90018-2}}&lt;/ref&gt; It is used for the evaluation of [[nuclear medicine]] [[medical imaging|imaging]] data after the injection of a [[radioopaque]] or [[radioactive tracer]].

The method is model-independent because it does not depend on any specific compartmental model configuration for the tracer, and the minimal assumption is that the behavior of the tracer can be approximated by two compartments &amp;ndash; a "central" (or reversible) compartment that is in rapid equilibrium with [[blood plasma|plasma]], and a "peripheral" (or irreversible) compartment, where tracer enters without ever leaving during the time of the measurements.&lt;ref name="Patlak1983" /&gt;&lt;ref name="Patlak1985" /&gt;  The amount of tracer in the [[region of interest]] is accumulating according to the equation:

: &lt;math&gt;R(t) = K \int_0^t C_p(\tau) \, d\tau + V_0  C_p(t)&lt;/math&gt;

where &lt;math&gt;t&lt;/math&gt; represents time after tracer injection, &lt;math&gt;R(t)&lt;/math&gt; is the amount of tracer in [[region of interest]], &lt;math&gt;C_p(t)&lt;/math&gt; is the concentration of tracer in plasma or blood, &lt;math&gt;K&lt;/math&gt; is the [[Clearance (medicine)|clearance]] determining the rate of entry into the peripheral (irreversible) compartment, and &lt;math&gt;V_0&lt;/math&gt; is the [[distribution volume]] of the tracer in the central compartment. The first [[Term (mathematics)|term]] of the right-hand side represents tracer in the peripheral compartment, and the second term tracer in the central compartment.

By [[division (mathematics)|dividing]] both sides by &lt;math&gt;C_p(t)&lt;/math&gt;, one obtains:

: &lt;math&gt;{R(t) \over C_p(t)} = K {\int_0^t C_p(\tau) \, d\tau \over C_p(t)} + V_0&lt;/math&gt;

The unknown constants &lt;math&gt;K&lt;/math&gt; and &lt;math&gt;V_0&lt;/math&gt; can be obtained by [[linear regression]] from a [[Graph of a function|graph]] of &lt;math&gt;{R(t) \over C_p(t)}&lt;/math&gt; against &lt;math&gt; \int_0^t C_p(\tau) \, d\tau / C_p(t)&lt;/math&gt;.

== See also ==
* [[Logan plot]]
* [[Positron emission tomography]]
* [[Multi-compartment model]]
* [[Binding potential]]
* [[Deconvolution]]
* [[Albert Gjedde]]

== References ==
{{Reflist}}

=== Further literature ===
* {{Cite Q | Q48779416 }}

==External links==
* PMOD, [https://web.archive.org/web/20070311063807/http://www.pmod.com/technologies/doc/pkin/2326.htm Patlak Plot], PMOD Kinetic Modeling Tool (PKIN).
* ''[http://www.turkupetcentre.net/modelling/guide/patlak_plot.html Gjedde–Patlak plot]'', [[Turku PET Centre]].

[[Category:Mathematical modeling]]
[[Category:Systems theory]]
[[Category:Plots (graphics)]]</text>
      <sha1>husqpl0mffkj00fi95ua7ucx2v80r9d</sha1>
    </revision>
  </page>
  <page>
    <title>Planar projection</title>
    <ns>0</ns>
    <id>16568536</id>
    <revision>
      <id>688458573</id>
      <parentid>625897764</parentid>
      <timestamp>2015-11-01T01:42:27Z</timestamp>
      <contributor>
        <username>Maxtremus</username>
        <id>3140864</id>
      </contributor>
      <comment>/* Map uses */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1849">{{unreferenced|date=September 2012}}
{{views}}
'''Planar projections''' are the subset of 3D graphical projections constructed by linearly mapping points in three-dimensional space to points on a two-dimensional '''projection plane'''. The projected point on the plane is chosen such that it is [[Line (geometry)|collinear]] with the corresponding three-dimensional point and the '''centre of projection'''. The lines connecting these points are commonly referred to as '''projectors'''. 

The centre of projection can be thought of as the location of the observer while the plane of projection is the surface on which the two dimensional projected image of the scene is recorded or from which it is viewed (e.g., photographic negative, photographic print, computer monitor). When the centre of projection is at a finite distance from the projection plane, a [[Perspective (graphical)|perspective]] projection is obtained. When the centre of projection is at infinity, all the projectors are parallel, and the corresponding subset of planar projections are referred to as '''parallel projections'''.

==Mathematical formulation==
Mathematically, planar projections are [[linear transformations]] acting on a point in three-dimensional space &lt;math&gt;\mathbf{a}_{x,y,z}&lt;/math&gt; to give a point &lt;math&gt;\mathbf{b}_{u,v}&lt;/math&gt; on the projection plane. These transformations consist of various [[function composition|compositions]] of the five transformations: [[orthographic projection]], [[rotation (mathematics)|rotation]], [[Shear mapping|shear]], [[Translation (geometry)|translation]] and [[Projective transformation|perspective]].

==Map uses==
It is also used in maps to show the planet Earth and other planets or objects in space. This is good for maps of close-up areas.

{{Computer graphics}}

[[Category:Graphical projections]]

{{geometry-stub}}</text>
      <sha1>itm77i59tsr5a4itku3luif0fraj961</sha1>
    </revision>
  </page>
  <page>
    <title>SC (complexity)</title>
    <ns>0</ns>
    <id>18449916</id>
    <revision>
      <id>760087432</id>
      <parentid>760087381</parentid>
      <timestamp>2017-01-14T22:23:34Z</timestamp>
      <contributor>
        <ip>128.189.133.101</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2453">In [[computational complexity theory]], '''SC''' (Steve's Class, named after [[Stephen Cook]])&lt;ref&gt;{{ComplexityZoo|SC|S#sc}}&lt;/ref&gt; is the [[complexity class]] of problems solvable by a [[deterministic Turing machine]] in polynomial time (class '''[[P_(complexity)|P]]''') and polylogarithmic space (class '''[[PolyL]]''') (that is, [[Big O notation|O]]((log ''n'')&lt;sup&gt;k&lt;/sup&gt;) space for some constant ''k''). It may also be called '''DTISP(poly, polylog)''', where '''DTISP''' stands for ''deterministic time and space''. Note that the definition of '''SC''' differs from '''P''' &amp;cap; '''PolyL''', since for the former, it is required that a single algorithm runs in both polynomial time and polylogarithmic space; while for the latter, two separate algorithms will suffice: one that runs in polynomial time, and another that runs in polylogarithmic space. (It is unknown whether '''SC''' and '''P''' &amp;cap; '''PolyL''' are equivalent).

'''[[Deterministic context-free language|DCFL]]''', the strict subset of [[context-free language]]s recognized by [[deterministic pushdown automaton|deterministic pushdown automata]], is contained in '''SC''', as shown by Cook in 1979.&lt;ref&gt;S. A. Cook. Deterministic CFL's are accepted simultaneously in polynomial time and log squared space. Proceedings of ACM STOC'79, pp. 338&amp;ndash;345. 1979.&lt;/ref&gt;

It is open if directed [[st-connectivity]] is in '''SC''', although it is known to be in '''P''' &amp;cap; '''PolyL''' (because of a DFS algorithm and [[Savitch's theorem]]). This question is equivalent to '''[[NL (complexity)|NL]]''' ⊆ '''SC'''.

'''[[RL (complexity)|RL]]''' and '''[[BPL (complexity)|BPL]]''' are classes of problems acceptable by [[probabilistic Turing machines]] in logarithmic space and polynomial time. [[Noam Nisan]] showed in 1992 the weak [[derandomization]] result that both are contained in '''SC'''.&lt;ref&gt;{{citation
 | last = Nisan | first = Noam | author-link = Noam Nisan
 | contribution = RL ⊆ SC
 | doi = 10.1145/129712.129772
 | location = Victoria, British Columbia, Canada
 | pages = 619–623
 | title = Proceedings of the 24th ACM Symposium on Theory of computing (STOC '92)
 | year = 1992}}.&lt;/ref&gt; In other words, given ''polylogarithmic'' space, a deterministic machine can simulate ''logarithmic'' space probabilistic algorithms.

== References ==
{{reflist}}
{{comp-sci-theory-stub}}

{{ComplexityClasses}}

{{DEFAULTSORT:Sc (Complexity)}}
[[Category:Complexity classes]]</text>
      <sha1>tau4inykz7n6jto0z1qj46vz747l9h5</sha1>
    </revision>
  </page>
  <page>
    <title>Satake isomorphism</title>
    <ns>0</ns>
    <id>32319857</id>
    <revision>
      <id>851755601</id>
      <parentid>814475526</parentid>
      <timestamp>2018-07-24T11:44:19Z</timestamp>
      <contributor>
        <ip>145.116.185.219</ip>
      </contributor>
      <comment>/* Statement */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2126">In mathematics, the '''Satake isomorphism''', introduced by {{harvs|txt|authorlink=Ichirō Satake|last=Satake|year=1963}}, identifies the [[Hecke algebra of a locally compact group|Hecke algebra]] of a [[reductive group]] over a [[local field]] with a ring of invariants of the [[Weyl group]].
The '''geometric Satake equivalence''' is a geometric version of the Satake isomorphism, introduced by {{harvtxt|Mirković|Vilonen|2007}}.

==Statement==
Let ''G'' be a [[Group of Lie type|Chevalley group]], ''K'' be a non-Archimedean local field and ''O'' be its ring of integers. Then the Satake isomorphism identifies the Grothendieck group of complex representations of the [[Langlands dual]] &lt;math&gt;{}^L G&lt;/math&gt; of ''G'', with the ring of ''G(O)'' invariant compactly supported functions on the affine Grassmannian. In formulas:

:&lt;math&gt; \mathbb C_c [  G(K) / G(O) ]^{G(O)} \cong K_0({}^L G-Rep).&lt;/math&gt;

Here ''G(O)'' acts on ''G(K)'' / ''G(O)'' by multiplication from the left.

==Notes==
{{Reflist}}

==References==
* {{Citation | last1=Gross | first1=Benedict H. |authorlink=Benedict Gross| title=Galois representations in arithmetic algebraic geometry (Durham, 1996) | publisher=[[Cambridge University Press]] | series=London Math. Soc. Lecture Note Ser. | doi=10.1017/CBO9780511662010.006 | mr=1696481 | year=1998 | volume=254 | chapter=On the Satake isomorphism | pages=223–237}}
* {{Citation | last1=Mirković | first1=I. | last2=Vilonen | first2=K. | title=Geometric Langlands duality and representations of algebraic groups over commutative rings | doi=10.4007/annals.2007.166.95 | mr=2342692 | year=2007 | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=166 | issue=1 | pages=95–143 |arxiv=math/0401222}}
* {{Citation | last1=Satake | first1=Ichirō |authorlink=Ichirō Satake| title=Theory of spherical functions on reductive algebraic groups over p-adic fields | url=http://www.numdam.org/item?id=PMIHES_1963__18__5_0 | mr=0195863 | year=1963 | journal=[[Publications Mathématiques de l'IHÉS]] | issn=1618-1913 | issue=18 | pages=5–69}}

[[Category:Representation theory]]</text>
      <sha1>05z7ptqzk17hk1tirsba1bmypjq1f9t</sha1>
    </revision>
  </page>
  <page>
    <title>Separable σ-algebra</title>
    <ns>0</ns>
    <id>58580585</id>
    <redirect title="Sigma-algebra" />
    <revision>
      <id>861258270</id>
      <timestamp>2018-09-26T05:22:27Z</timestamp>
      <contributor>
        <username>NikelsenH</username>
        <id>18120325</id>
      </contributor>
      <comment>[[WP:AES|←]]Redirected page to [[Sigma-algebra#Separable σ-algebras]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="76">#REDIRECT [[sigma-algebra#Separable σ-algebras]]

[[Category:Set families]]</text>
      <sha1>ekow7wnv8sum1v7me2lf5drnzlcchy9</sha1>
    </revision>
  </page>
  <page>
    <title>Teresa W. Haynes</title>
    <ns>0</ns>
    <id>53697574</id>
    <revision>
      <id>865607872</id>
      <parentid>865607623</parentid>
      <timestamp>2018-10-25T00:28:17Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>cats</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2482">'''Teresa W. Haynes''' is an American professor of mathematics and statistics at [[East Tennessee State University]] known for her research in [[graph theory]] and particularly on [[dominating set]]s.

== Education and career==
Haynes earned three degrees from [[Eastern Kentucky University]]: a B.S in Mathematics and Education in 1975, M.A. in Mathematics and Education in 1978, and M.S in Mathematical Sciences in 1984.&lt;ref name=cv&gt;{{citation|url=http://faculty.etsu.edu/haynes/vita.pdf|title=Curriculum vitae|accessdate=2018-10-24}}&lt;/ref&gt; She completed her Ph.D in Computer Science in 1988 from the [[University of Central Florida]]. Her dissertation was ''On &lt;math&gt;k&lt;/math&gt;-&lt;math&gt;\gamma&lt;/math&gt;-Insensitive Domination'' and was supervised by Robert C. Brigham.&lt;ref&gt;{{mathgenealogy|id=140763}}&lt;/ref&gt;

Haynes worked as a mathematics teacher from 1975 to 1978 and as a telephone engineer from 1978 to 1981. She became a mathematics and computer science instructor at [[Pikeville College]] in 1981, and moved to [[Prestonburg Community College]] in 1983.
After completing her doctorate in 1988, she became an assistant professor at East Tennessee State, and she was promoted to full professor there in 1999.&lt;ref name=cv/&gt;

== Books ==
Haynes is the author of two books on [[dominating set]]s in [[graph theory]]:
* ''Fundamentals of Domination in Graphs'', Marcel Dekker, Inc., New York, 1998 (with Stephen Hedetniemi and Peter Slater; {{ISBN|978-0824700331}})&lt;ref&gt;{{citation|title=Review of ''Fundamentals of Domination in Graphs''|first=Christine M.|last=Mynhardt|journal=Mathematical Reviews|year=2001|mr=1605684}}&lt;/ref&gt;
* ''Domination in Graphs: Advanced Topics'', Marcel Dekker, Inc., New York, 1998 (edited by Teresa Haynes, Stephen Hedetniemi and Peter Slater; {{ISBN|978-0824700348}})&lt;ref&gt;{{citation|title=Review of ''Domination in Graphs: Advanced Topics''|first=Gary|last=MacGillivray|journal=Mathematical Reviews|year=2000|mr=1605685}}&lt;/ref&gt;

== References ==
{{reflist}}

==External links==
*{{Google Scholar id|tPu9j7cAAAAJ}}

{{authority control}}

{{DEFAULTSORT:Haynes, Teresa}}
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]
[[Category:American women mathematicians]]
[[Category:Mathematicians from Tennessee]]
[[Category:Eastern Kentucky University alumni]]
[[Category:University of Central Florida alumni]]
[[Category:University of Pikeville faculty]]
[[Category:East Tennessee State University faculty]]
[[Category:Graph theorists]]</text>
      <sha1>2ol0pf3wwuyumc64klqm5qc83q6ho07</sha1>
    </revision>
  </page>
  <page>
    <title>Three-process view</title>
    <ns>0</ns>
    <id>8328262</id>
    <revision>
      <id>805568134</id>
      <parentid>805568067</parentid>
      <timestamp>2017-10-16T06:37:51Z</timestamp>
      <contributor>
        <username>Tony1</username>
        <id>332841</id>
      </contributor>
      <comment>[[User:Ohconfucius/script|Script]]-assisted fixes: per [[MOS:NUM]], [[MOS:CAPS]], [[MOS:LINK]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1121">{{Use dmy dates|date=October 2017}}
The '''three-process view''' is a [[psychology|psychological]] term coined by [[Janet E. Davidson]] and [[Robert Sternberg]].

According to this concept, there are three kinds of insight: selective-encoding, selective-comparison, and selective-combination.&lt;ref name= "tpv"&gt; Davidson, 1995,2003&lt;br&gt;
Sternberg, R. J., &amp; Davidson, J. E.  (Eds.).  (1984).  Conceptions of giftedness. New York: [[Cambridge University Press]]. [[Google Book Search|Google Books]] at [https://books.google.com/books?id=K-KBooxRCQ0C&amp;dq=%22Sternberg%22+%22Conceptions+of+Giftedness%22+]&lt;/ref&gt;

'''Selective-encoding insight''' – Distinguishing what is important in a problem and what is irrelevant.  (i.e. filter) &lt;br /&gt;
'''Selective-comparison insight''' – Identifying information by finding a connection between acquired [[knowledge]] and [[experience]]. &lt;br /&gt;
'''Selective-combination insight''' – Identifying a problem through understanding the different components and putting everything together.

==References==
{{reflist}}

{{DEFAULTSORT:Three-Process View, The}}
[[Category:Information theory]]</text>
      <sha1>61jdjgjptbu0ic1yehy5q7kswmv8tdl</sha1>
    </revision>
  </page>
  <page>
    <title>Triangular network coding</title>
    <ns>0</ns>
    <id>36145695</id>
    <revision>
      <id>841531184</id>
      <parentid>540180654</parentid>
      <timestamp>2018-05-16T12:04:54Z</timestamp>
      <contributor>
        <username>OAbot</username>
        <id>28481209</id>
      </contributor>
      <minor/>
      <comment>[[Wikipedia:OABOT|Open access bot]]: add arxiv identifier to citation with #oabot.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2951">In [[coding theory]], '''triangular network coding (TNC)''' is a [[network coding]] based packet coding scheme introduced by {{harvtxt|Qureshi|Foh|Cai|2012}}.&lt;ref&gt;{{citation
 | last1 = Qureshi | first1 = Jalaluddin
 | last2 = Foh | first2 = Chuan Heng
 | last3 = Cai | first3 = Jianfei
 | doi = 10.1109/SECON.2012.6275780
 | mr = 
 | issue = 
 | journal = IEEE SECON
 | pages = 134–142
 | title = Optimal Solution for the Index Coding Problem Using Network Coding over GF(2)
 | volume = 
 | year = 2012| arxiv = 1209.6539}}.&lt;/ref&gt;
Previously, packet coding for network coding was done using linear network coding (LNC). The drawback of LNC over large [[finite field]] is that it resulted in high encoding and decoding [[Big O notation|computational complexity]]. While linear encoding and decoding over [[GF(2)]] alleviates the concern of high computational complexity, coding over GF(2) comes at the tradeoff cost of degrading throughput performance.

Triangular network coding therefore essentially addresses the high encoding and decoding computational complexity without degrading the throughput performance, with [[code rate]] comparable to that of linear network coding.

==Coding and decoding==

[[File:TNC, coding 4 packets together..PNG|frame|An example of coding four packets using TNC. Bit ''b''&lt;sub&gt;''i'',''k''&lt;/sub&gt; &amp;isin; {0,1} is the ''i''&lt;sup&gt;th&lt;/sup&gt; bit of the ''k''&lt;sup&gt;th&lt;/sup&gt; packet. Each packet has original length of ''B'' bits. The resulting coded packet has length ''B''&amp;nbsp;+&amp;nbsp;3 bits. Information about the number of redundant '0' bits added at the head of each packet is included in the coded packet's header.]]

In TNC, coding is performed in two stages. First redundant "0" bits are selectively added at the head and tail of each packet such that all packets are of uniform bit length. Then the packets are [[Exclusive or|XOR coded]], bit-by-bit. The "0" bits are added in such a way that these redundant "0" bits added to each packet generate a [[Triangular matrix|triangular pattern]].

In essence, the TNC decoding process, like the LNC decoding process involves [[Gaussian elimination]]. However, since the packets in TNC have been coded in such a manner that the resulting coded packets are in triangular pattern, the computational process of ''triangularization,''&lt;ref name=fraleigh95&gt;J. B. Fraleigh, and R. A. Beauregard, Linear Algebra. Chapter 10, Addison-Wesley Publishing Company, 1995.&lt;/ref&gt; with complexity of &lt;math&gt;O(n^3)&lt;/math&gt;, where &lt;math&gt;n&lt;/math&gt; is the number of packets, can be bypassed. The receiver now only needs to perform ''back-substitution,''&lt;ref name=fraleigh95&gt;J. B. Fraleigh, and R. A. Beauregard, Linear Algebra. Chapter 10, Addison-Wesley Publishing Company, 1995.&lt;/ref&gt; with complexity given as &lt;math&gt;O(n^2)&lt;/math&gt; for each bit location.

==References==
{{reflist}}

[[Category:Coding theory]]
[[Category:Finite fields]]
[[Category:Information theory]]

{{telecommunications-stub}}</text>
      <sha1>qpjspaugtibdmtp2ami3v81jibjq0f1</sha1>
    </revision>
  </page>
  <page>
    <title>Unary operation</title>
    <ns>0</ns>
    <id>161006</id>
    <revision>
      <id>871194780</id>
      <parentid>845716552</parentid>
      <timestamp>2018-11-29T15:43:20Z</timestamp>
      <contributor>
        <username>IntegralPython</username>
        <id>33883676</id>
      </contributor>
      <comment>expanded and clarified - combined sections into single "examples" section and added a trigonometry example.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5234">{{Refimprove|date=March 2010}}
In [[mathematics]], a '''unary operation''' is an [[Operation (mathematics)|operation]] with only one [[operand]], i.e. a single input. This is in contrast to [[binary operation]]s, which use two operands. An example is the function {{nowrap|''f'' : ''A'' → ''A''}}, where ''A'' is a [[Set (mathematics)|set]]. The function ''f'' is a unary operation on ''A''.

Common notations are [[prefix notation]] (e.g. [[+]], [[−]], [[¬]]), [[postfix notation]] (e.g. [[factorial]] n!), functional notation (e.g. sin ''x'' or sin(''x'')), and [[superscript]]s (e.g. [[transpose]] ''A''&lt;sup&gt;T&lt;/sup&gt;). Other notations exist as well. For example, in the case of the [[square root]], a horizontal bar extending the square root sign over the argument can indicate the extent of the argument.

==Examples==
===Unary negative and positive===
As unary operations have only one operand they are evaluated before other operations containing them. Here is an example using negation:

:3 − −2

Here, the first '−' represents the binary subtraction operation, while the second '−' represents the unary negation of the 2 (or '−2' could be taken to mean the integer −2). Therefore, the expression is equal to:

:3 − (−2) = 5

Technically there is also a unary positive but it is not needed since we assume a value to be positive:

:(+2) = 2

Unary positive does not change the sign of a negative operation:

:(+(−2)) = (−2)

In this case a unary negative is needed to change the sign:

:(−(−2)) = (+2)

===Trigonometry===
In [[trigonometry]] the functions &lt;math&gt;sin(x)&lt;/math&gt;, &lt;math&gt;cos(x)&lt;/math&gt;, &lt;math&gt;tan(x)&lt;/math&gt;, and the other trigonometric functions are unary operations. This is because it is possible to input only one term with the operation and retrieve a result, compared with operations, such as addition, where two different terms are needed to compute a result.

===Examples from programming languages===
====C family of languages====
In the [[C (programming language)|C]] family of languages, the following operators are unary:&lt;ref&gt;{{cite book |url=http://www-01.ibm.com/support/docview.wss?uid=swg27002103&amp;aid=1 |website=www-01.ibm.com |page=109 |chapter=Chapter 5. Expressions and Operators |title=C/C++ Language Reference |version=Version 6.0 |archiveurl=https://web.archive.org/web/20121016081612/http://www-01.ibm.com/support/docview.wss?uid=swg27002103&amp;aid=1 |archivedate=2012-10-16}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.sanfoundry.com/c-tutorials-different-unary-operators-operate-operands/ |title=Unary Operators - C Tutorials - Sanfoundry |website=www.sanfoundry.com}}&lt;/ref&gt;

*[[Increment and decrement operators|Increment]]: &lt;code&gt;++&lt;span style="color:gray;"&gt;x&lt;/span&gt;&lt;/code&gt;, &lt;code&gt;&lt;span style="color:gray;"&gt;x&lt;/span&gt;++&lt;/code&gt;
*[[Increment and decrement operators|Decrement]]: &lt;code&gt;−−&lt;span style="color:gray;"&gt;x&lt;/span&gt;&lt;/code&gt;, &lt;code&gt;&lt;span style="color:gray;"&gt;x&lt;/span&gt;−−&lt;/code&gt;
*[[Reference (computer science)|Address]]: &lt;code&gt;&amp;&lt;span style="color:gray;"&gt;x&lt;/span&gt;&lt;/code&gt;
*[[Indirection]]: &lt;code&gt;*&lt;span style="color:gray;"&gt;x&lt;/span&gt;&lt;/code&gt;
*Positive: &lt;code&gt;+&lt;span style="color:gray;"&gt;x&lt;/span&gt;&lt;/code&gt;
*Negative: &lt;code&gt;−&lt;span style="color:gray;"&gt;x&lt;/span&gt;&lt;/code&gt;
*[[Signed number representations|Ones' complement]]: &lt;code&gt;~&lt;span style="color:gray;"&gt;x&lt;/span&gt;&lt;/code&gt;
*[[Negation|Logical negation]]: &lt;code&gt;!&lt;span style="color:gray;"&gt;x&lt;/span&gt;&lt;/code&gt;
*[[Sizeof]]: &lt;code&gt;sizeof &lt;span style="color:gray;"&gt;x&lt;/span&gt;, sizeof(&lt;span style="color:gray;"&gt;type-name&lt;/span&gt;)&lt;/code&gt;
*[[Type conversion|Cast]]: &lt;code&gt;(''type-name'') ''&lt;span style="color:gray;"&gt;cast-expression&lt;/span&gt;''&lt;/code&gt;

====Unix Shell (Bash)====
In the Unix/Linux shell (bash/sh), ''''$'''' is a unary operator when used for parameter expansion, replacing the name of a variable by its (sometimes modified) value. For example:

* Simple expansion:  &lt;code&gt;$&lt;span style="color:gray;"&gt;x&lt;/span&gt;&lt;/code&gt;
* Complex expansion:  &lt;code&gt;${#&lt;span style="color:gray;"&gt;x&lt;/span&gt;}&lt;/code&gt;

====Windows PowerShell====
*Increment: &lt;code&gt;++&lt;span style="color:gray;"&gt;$x&lt;/span&gt;&lt;/code&gt;, &lt;code&gt;&lt;span style="color:gray;"&gt;$x&lt;/span&gt;++&lt;/code&gt;
*Decrement: &lt;code&gt;−−&lt;span style="color:gray;"&gt;$x&lt;/span&gt;&lt;/code&gt;, &lt;code&gt;&lt;span style="color:gray;"&gt;$x&lt;/span&gt;−−&lt;/code&gt;
*Positive: &lt;code&gt;+&lt;span style="color:gray;"&gt;$x&lt;/span&gt;&lt;/code&gt;
*Negative: &lt;code&gt;−&lt;span style="color:gray;"&gt;$x&lt;/span&gt;&lt;/code&gt;
*Logical negation: &lt;code&gt;-not &lt;span style="color:gray;"&gt;$x&lt;/span&gt;&lt;/code&gt;
*[[Execution (computing)|Invoke]] in current [[Scope (programming)|scope]]: &lt;code&gt;.&lt;span style="color:gray;"&gt;$x&lt;/span&gt;&lt;/code&gt;
*Invoke in new scope: &lt;code&gt;&amp;&lt;span style="color:gray;"&gt;$x&lt;/span&gt;&lt;/code&gt;
*Cast: &lt;code&gt;[''type-name''] ''&lt;span style="color:gray;"&gt;cast-expression&lt;/span&gt;''&lt;/code&gt;
*Cast: &lt;code&gt;+&lt;span style="color:gray;"&gt;$x&lt;/span&gt;&lt;/code&gt;
*Array: &lt;code&gt;,&lt;span style="color:gray;"&gt;$array&lt;/span&gt;&lt;/code&gt;

==See also==
* [[Binary operation]]
* [[Ternary operation]]
* [[Arity]]
* [[Operation (mathematics)]]
* [[Operator (programming)]]

==References==
{{Reflist}}
*{{MathWorld|title=Unary Operation|urlname=UnaryOperation|author=Matt Insall}}

{{DEFAULTSORT:Unary Operation}}
[[Category:Elementary algebra]]
[[Category:Operators (programming)]]
[[Category:Unary operations| ]]</text>
      <sha1>3kw3bfgc4yf3pqieuuupiqew5ov5t94</sha1>
    </revision>
  </page>
  <page>
    <title>Unrestricted algorithm</title>
    <ns>0</ns>
    <id>54117020</id>
    <revision>
      <id>847631025</id>
      <parentid>838275164</parentid>
      <timestamp>2018-06-26T18:21:59Z</timestamp>
      <contributor>
        <username>CitationCleanerBot</username>
        <id>15270283</id>
      </contributor>
      <minor/>
      <comment>cleanup, added [[CAT:UL|underlinked]] tag</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1769">{{Multiple issues|
{{Underlinked|date=June 2018}}
{{notability|date=May 2017}}
}}

An '''unrestricted algorithm''' is an [[algorithm]] for the computation of a mathematical function that puts no restrictions on the range of the argument or on the precision that may be demanded in the result.&lt;ref name="Clenshaw"&gt;{{cite journal|last1=C.W. Clenshaw and F. W. J. Olver|title=An unrestricted algorithm for the exponential function|journal=SIAM Journal on Numerical Analysis|date=April 1980|volume= 17|issue=2|pages=310–331|jstor=2156615}}&lt;/ref&gt; The idea of such an algorithm was put forward by C. W. Clenshaw and F. W. J. Olver in a paper published in 1980.&lt;ref name=Clenshaw/&gt;&lt;ref name="Brent"&gt;{{cite book|last1=Richard P Brent|chapter=Unrestricted algorithms for elementary and special functions|title=Information Processing |volume=80  |editor=S. H. Lavington |publisher=North-Holland, Amsterdam|date=1980|pages=613–619|arxiv=1004.3621}}&lt;/ref&gt;

In the problem of developing algorithms for computing the values of a real-valued function of a real variable, say ''g''(''x''), in "restricted" algorithms, the error that can be tolerated in the result is specified in advance. An interval on the real line would also be specified for values where in the values of function are to be evaluated. Different algorithms may have to applied for evaluating functions outside the interval. An unrestricted algorithm envisages a situation in which a user may stipulate the value of ''x'' and also the precision required in ''g''(''x''), quite arbitrarily. The algorithm should then produce an acceptable result without failure.&lt;ref name = Clenshaw/&gt;

==References==
{{reflist}}

[[Category:Numerical analysis]]
[[Category:Algorithms|*]]
[[Category:Theoretical computer science]]</text>
      <sha1>fi7by8h26am0biohaan4rl0f9kbfazn</sha1>
    </revision>
  </page>
  <page>
    <title>Waldspurger's theorem</title>
    <ns>0</ns>
    <id>35154167</id>
    <revision>
      <id>851004657</id>
      <parentid>729893566</parentid>
      <timestamp>2018-07-19T12:31:04Z</timestamp>
      <contributor>
        <ip>5.2.200.163</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="734">In [[mathematics]], '''Waldspurger's theorem''', introduced by {{harvs|txt|authorlink=Jean-Loup Waldspurger|first=Jean-Loup |last=Waldspurger|year=1981}}, is a result that identifies [[Fourier coefficient]]s of [[modular form]]s of half-integral weight ''k''+1/2 with the value of a [[L-function|L-series]] at ''s''=''k''/2.

==References==

*{{Citation | last1=Waldspurger | first1=Jean-Loup | title=Sur les coefficients de Fourier des formes modulaires de poids demi-entier |mr=646366 | year=1981 | journal=Journal de Mathématiques Pures et Appliquées. Neuvième Série | issn=0021-7824 | volume=60 | issue=4 | pages=375–484}}


[[Category:Modular forms]]
[[Category:Zeta and L-functions]]
[[Category:Theorems in number theory]]</text>
      <sha1>onwbvdsgw5624uhocpuw105sx7xtsl5</sha1>
    </revision>
  </page>
  <page>
    <title>Wu's method of characteristic set</title>
    <ns>0</ns>
    <id>13284111</id>
    <revision>
      <id>797948353</id>
      <parentid>786595496</parentid>
      <timestamp>2017-08-30T03:22:01Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <minor/>
      <comment>adding a link using [[Google Scholar]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11016">{{More footnotes|date=November 2012}}
'''[[Wu Wenjun|Wenjun Wu]]'s method''' is an algorithm for solving [[systems of polynomial equations|multivariate polynomial equations]] introduced in the late 1970s by the Chinese mathematician [[Wu Wenjun|Wen-Tsun Wu]]. This method is based on the mathematical concept of '''characteristic set''' introduced in the late 1940s by [[Joseph Ritt|J.F. Ritt]]. It is fully independent of the [[Gröbner basis]] method, introduced by [[Bruno Buchberger]] (1965), even if Gröbner bases may be used to compute characteristic sets.&lt;ref&gt;{{cite book|editor-last=Corrochano|editor-first=Eduardo Bayro|editor2-first=Garret |editor2-last=Sobczyk|title=Geometric algebra with applications in science and engineering|page=110 |year=2001|publisher=Birkhäuser|location=Boston, Mass|isbn=9780817641993}}&lt;/ref&gt;&lt;ref&gt;P. Aubry, D. Lazard, M. Moreno Maza (1999). [http://www.sciencedirect.com/science/article/pii/S0747717199902699 On the theories of triangular sets]. Journal of Symbolic Computation, 28(1–2):105–124&lt;/ref&gt;

Wu's method is powerful for [[automatic theorem proving|mechanical theorem proving]] in [[elementary geometry]], and provides a complete decision process for certain classes of problem.
It has been used in research in his laboratory (KLMM, Key Laboratory of Mathematics Mechanization in Chinese Academy of Science) and around the world.  The main trends of research on Wu's method concern [[systems of polynomial equations]] of positive dimension and [[differential algebra]] where [[Joseph Ritt|Ritt]]'s results have been made effective.&lt;ref&gt;Hubert, E. ''Factorisation Free Decomposition Algorithms in Differential Algebra.'' Journal of Symbolic Computation, (May 2000): 641–662.&lt;/ref&gt;&lt;ref&gt;[[Maple (software)]] package '''diffalg'''.&lt;/ref&gt; Wu's method has been applied in various scientific fields, like biology, computer vision, robot kinematics and especially automatic proofs in geometry&lt;ref&gt;Chou, Shang-Ching; Gao, Xiao Shan; Zhang, Jing Zhong. ''Machine proofs in geometry''. World Scientific, 1994.&lt;/ref&gt;

==Informal description==
'''Wu's method''' uses [[polynomial]] division to solve problems of the form:

: &lt;math&gt; \forall x, y, z, \dots I(x, y, z, \dots) \implies f(x, y, z, \dots) \, &lt;/math&gt;

where ''f'' is a [[polynomial equation]] and ''I'' is a [[Logical conjunction|conjunction]] of [[polynomial equation]]s. The algorithm is complete for such problems over the [[complex domain]].

The core idea of the algorithm is that you can divide one polynomial by another to give a remainder. Repeated division results in either the remainder vanishing (in which case the ''I'' implies ''f'' statement is true), or an irreducible remainder is left behind (in which case the statement is false).

More specifically, for an [[ideal (ring theory)|ideal]] ''I'' in the [[ring (mathematics)|ring]] ''k''[''x''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;...,&amp;nbsp;''x''&lt;sub&gt;''n''&lt;/sub&gt;] over a field ''k'', a (Ritt) characteristic set ''C'' of ''I'' is composed of a set of polynomials in ''I'', which is in triangular shape: polynomials in ''C'' have distinct main variables (see the formal definition below). Given a characteristic set ''C'' of ''I'', one can decide if a polynomial ''f'' is zero modulo ''I''. That is, the membership test is checkable for ''I'', provided a characteristic set of ''I''.

==Ritt characteristic set==

A Ritt characteristic set is a finite set of polynomials in triangular form of an ideal. This triangular set satisfies
certain minimal condition with respect to the Ritt ordering, and it preserves many interesting geometrical properties
of the ideal. However it may not be its system of generators.

===Notation===

Let R be the multivariate polynomial ring ''k''[''x''&lt;sub&gt;1&lt;/sub&gt;, ..., ''x''&lt;sub&gt;''n''&lt;/sub&gt;] over a field ''k''.
The variables are ordered linearly according to their subscript: ''x''&lt;sub&gt;1&lt;/sub&gt; &lt; ... &lt; ''x''&lt;sub&gt;''n''&lt;/sub&gt;.
For a non-constant polynomial ''p'' in R, the greatest variable effectively presenting in ''p'', called '''main variable''' or '''class''', plays a particular role:
''p'' can be naturally regarded as a univariate polynomial in its main variable ''x''&lt;sub&gt;''k''&lt;/sub&gt; with coefficients in ''k''[''x''&lt;sub&gt;1&lt;/sub&gt;, ..., ''x''&lt;sub&gt;''k''−1&lt;/sub&gt;].
The degree of p as a univariate polynomial in its main variable is also called its main degree.

===Triangular set===

A set ''T'' of non-constant polynomials is called a '''triangular set''' if all polynomials in ''T'' have distinct main variables.  This generalizes triangular [[systems of linear equations]] in a natural way.

===Ritt ordering===

For two non-constant polynomials ''p'' and ''q'', we say ''p'' is smaller than ''q'' with respect to '''Ritt ordering''' and written as ''p''&amp;nbsp;&lt;&lt;sub&gt;''r''&lt;/sub&gt;&amp;nbsp;''q'', if one of the following assertions holds:
:(1)  the main variable of ''p'' is smaller than the main variable of ''q'', that is, mvar(''p'')&amp;nbsp;&lt;&amp;nbsp;mvar(''q''),
:(2) ''p'' and ''q'' have the same main variable, and the main degree of ''p'' is less than the '''main degree''' of&amp;nbsp;''q'', that is, mvar(''p'')&amp;nbsp;=&amp;nbsp;mvar(''q'') and mdeg(''p'')&amp;nbsp;&lt;&amp;nbsp;mdeg(''q'').

In this way, (''k''[''x''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;...,&amp;nbsp;''x''&lt;sub&gt;''n''&lt;/sub&gt;],&lt;&lt;sub&gt;''r''&lt;/sub&gt;) forms a [[well partial order]]. However, the Ritt ordering is not a [[total order]]:
there exist polynomials p and q such that neither ''p''&amp;nbsp;&lt;&lt;sub&gt;''r''&lt;/sub&gt;&amp;nbsp;''q'' nor ''p''&amp;nbsp;&amp;gt;&lt;sub&gt;''r''&lt;/sub&gt;&amp;nbsp;''q''. In this case, we say that ''p'' and ''q'' are not comparable.
Note that the Ritt ordering is comparing the '''rank''' of ''p'' and ''q''. The rank, denoted by rank(''p''), of a non-constant polynomial ''p'' is defined to be a power of
its main variable: mvar(''p'')&lt;sup&gt;mdeg(''p'')&lt;/sup&gt; and ranks are compared by comparing first the variables and then, in case of equality of the variables, the degrees.

===Ritt ordering on triangular sets===

A crucial generalization on Ritt ordering is to compare triangular sets.
Let ''T''&amp;nbsp;=&amp;nbsp;{&amp;nbsp;''t''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;...,&amp;nbsp;''t''&lt;sub&gt;''u''&lt;/sub&gt;} and ''S''&amp;nbsp;=&amp;nbsp;{&amp;nbsp;''s''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;...,&amp;nbsp;''s''&lt;sub&gt;''v''&lt;/sub&gt;} be two triangular sets
such that polynomials in ''T'' and ''S'' are sorted increasingly according to their main variables.
We say ''T'' is smaller than ''U'' w.r.t. Ritt ordering if one of the following assertions holds
:(1) there exists ''k''&amp;nbsp;≤&amp;nbsp;min(''u'',&amp;nbsp;''v'') such that rank(''t''&lt;sub&gt;''i''&lt;/sub&gt;)&amp;nbsp;=&amp;nbsp;rank(''s''&lt;sub&gt;''i''&lt;/sub&gt;) for 1&amp;nbsp;≤&amp;nbsp;''i''&amp;nbsp;&lt;&amp;nbsp;''k'' and ''t''&lt;sub&gt;''k''&lt;/sub&gt;&amp;nbsp;&lt;&lt;sub&gt;''r''&lt;/sub&gt;&amp;nbsp;''s''&lt;sub&gt;''k''&lt;/sub&gt;,
:(2) ''u''&amp;nbsp;&gt;&amp;nbsp;''v'' and rank(''t''&lt;sub&gt;''i''&lt;/sub&gt;)&amp;nbsp;=&amp;nbsp;rank(''s''&lt;sub&gt;''i''&lt;/sub&gt;) for 1&amp;nbsp;≤&amp;nbsp;''i''&amp;nbsp;≤&amp;nbsp;''v''.

Also, there exists incomparable triangular sets w.r.t Ritt ordering.

===Ritt characteristic set===

Let I be a non-zero ideal of k[x&lt;sub&gt;1&lt;/sub&gt;, ..., x&lt;sub&gt;n&lt;/sub&gt;]. A subset T of I is a '''Ritt characteristic set''' of I if one of the following conditions holds:
:(1) T consists of a single nonzero constant of k,
:(2) T is a triangular set and T is minimal w.r.t Ritt ordering in the set of all triangular sets contained in I.

A polynomial ideal may possess (infinitely) many characteristic sets, since Ritt ordering is a partial order.

==Wu characteristic set==

The Ritt–Wu process, first devised by Ritt, subsequently modified by Wu, computes not a Ritt characteristic but an extended one, called Wu characteristic set or ascending chain.

A non-empty subset T of the ideal &lt;F&gt; generated by F is a '''Wu characteristic set''' of F if one of the following condition holds

:(1) T = {a} with a being a nonzero constant,
:(2) T is a triangular set and there exists a subset G of &lt;F&gt; such that &lt;F&gt; = &lt;G&gt; and every polynomial in G is [[regular chain|pseudo-reduced]] to zero with respect to T.

Note that Wu characteristic set is defined to the set F of polynomials, rather to the ideal &lt;F&gt; generated by F. Also it can be shown that a Ritt characteristic set T of &lt;F&gt; is a Wu characteristic set of F. Wu characteristic sets can be computed by Wu's algorithm CHRST-REM, which only requires pseudo-remainder computations and no factorizations are needed.

Wu's characteristic set method has exponential complexity; improvements in computing efficiency by weak chains, [[regular chain]]s, saturated chain were introduced&lt;ref&gt;Chou S C, Gao X S; Ritt–Wu's decomposition algorithm and geometry theorem proving. Proc of CADE, 10 LNCS, #449, Berlin, Springer Verlag, 1990 207–220.&lt;/ref&gt;

==Decomposing algebraic varieties==

An application is an algorithm for solving systems of algebraic equations by means of characteristic sets. More precisely, given a finite subset F of polynomials, there is an algorithm to compute characteristic sets T&lt;sub&gt;1&lt;/sub&gt;, ...,
T&lt;sub&gt;e&lt;/sub&gt; such that:

:&lt;math&gt;V(F) = W(T_1)\cup \cdots \cup W(T_e), \, &lt;/math&gt;

where W(T&lt;sub&gt;i&lt;/sub&gt;) is the difference of V(T&lt;sub&gt;i&lt;/sub&gt;) and V(h&lt;sub&gt;i&lt;/sub&gt;), here h&lt;sub&gt;i&lt;/sub&gt; is the product of initials of the polynomials in T&lt;sub&gt;i&lt;/sub&gt;.

==See also==
*[[Regular chain]]
*[[Mathematics-Mechanization Platform]]
*[[RegularChains]]

==References==
&lt;references/&gt;

*P. Aubry, M. Moreno Maza (1999) Triangular Sets for Solving Polynomial Systems: a Comparative Implementation of Four Methods. J. Symb. Comput. 28(1–2): 125–154
*David A. Cox, John B. Little, Donal O'Shea.  Ideals, Varieties, and Algorithms.  2007.
*{{cite web|last=Hua-Shan|first=Liu |title=WuRittSolva: Implementation of Wu-Ritt Characteristic Set Method |url=http://library.wolfram.com/infocenter/MathSource/5716/ |work=Wolfram Library Archive |publisher=Wolfram |date=24 August 2005 |accessdate=17 November 2012}}
*{{cite book|last=Heck|first=André|title=Introduction to Maple|year=2003|publisher=Springer|location=New York|isbn=9780387002309|pages=105, 508|edition=3.}}
*Ritt, J. (1966). Differential Algebra. New York, Dover Publications.
*Dongming Wang (1998). Elimination Methods. Springer-Verlag, Wien, Springer-Verlag
*Dongming Wang (2004). Elimination Practice, Imperial College Press, London {{isbn|1-86094-438-8}}
*Wu, W. T. (1984). Basic principles of mechanical theorem proving in elementary geometries. J. Syst. Sci. Math. Sci., 4, 207–35
*Wu, W. T. (1987). A zero structure theorem for polynomial equations solving. MM Research Preprints, 1, 2–12
*{{cite journal|last=Xiaoshan|first=Gao|author2=Chunming, Yuan |author3=Guilin, Zhang |title=Ritt-Wu's characteristic set method for ordinary difference polynomial systems with arbitrary ordering|journal=Acta Mathematica Scientia|year=2009|volume=29|issue=4|pages=1063–1080|doi=10.1016/S0252-9602(09)60086-2}}

==External links==
*[http://www.mmrc.iss.ac.cn/~dwang/wsolve.htm  wsolve Maple package]
*[http://www.mmrc.iss.ac.cn/~lzhi/Research/wuritt.html The Characteristic Set Method]

[[Category:Computer algebra]]
[[Category:Algebraic geometry]]
[[Category:Commutative algebra]]
[[Category:Polynomials]]</text>
      <sha1>mkehkp38w0jbbnwe79n00nj8u0o7vzv</sha1>
    </revision>
  </page>
</mediawiki>
