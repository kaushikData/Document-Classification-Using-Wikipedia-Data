<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Alternating algebra</title>
    <ns>0</ns>
    <id>2497875</id>
    <revision>
      <id>828112059</id>
      <parentid>790740259</parentid>
      <timestamp>2018-02-28T17:02:32Z</timestamp>
      <contributor>
        <username>CBM</username>
        <id>1108292</id>
      </contributor>
      <minor/>
      <comment>Rm category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1207">In [[mathematics]], an '''alternating algebra''' is a {{math|'''Z'''}}-[[graded algebra]] for which {{math|1=''xy'' = (−1){{sup|deg(''x'')deg(''y'')}}''yx''}} for all nonzero [[homogeneous element]]s {{math|''x''}} and {{math|''y''}} (i.e. it is an [[Graded-commutative ring|anticommutative algebra]]) and has the further property that {{math|1=''x''&lt;sup&gt;2&lt;/sup&gt; = 0}} for every homogeneous element {{math|''x''}} of odd degree.{{refn|{{cite book|author=Nicolas Bourbaki|year=1998|title=Algebra I|publisher=[[Springer Science+Business Media]]|page=482}}}}

==Properties==
* The algebra formed as the [[Direct sum of modules|direct sum]] of the homogeneous subspaces of even degree of an anticommutative algebra {{math|''A''}} is a subalgebra contained in the centre of {{math|''A''}}, and is thus commutative.
* If 2 is not a [[zero divisor]] in the base ring {{math|''R''}} of an anticommutative algebra {{math|''A''}}, then the algebra {{math|''A''}} is necessarily alternating.

==See also==
* [[Alternating multilinear map]]
* [[Exterior algebra]]
* [[Graded symmetric algebra]]&lt;!-- this should be example, no? --&gt;

==References==
{{reflist}}

{{Abstract-algebra-stub}}
[[Category:Algebraic geometry]]</text>
      <sha1>5s2ujthunqv2nhzsqp91l1w2wkycuk6</sha1>
    </revision>
  </page>
  <page>
    <title>Anisotropy</title>
    <ns>0</ns>
    <id>1264</id>
    <revision>
      <id>867158186</id>
      <parentid>866905730</parentid>
      <timestamp>2018-11-04T01:21:24Z</timestamp>
      <contributor>
        <username>Myoglobin</username>
        <id>27752372</id>
      </contributor>
      <comment>Undid revision 866905730 by [[Special:Contributions/27.34.104.42|27.34.104.42]] ([[User talk:27.34.104.42|talk]]) v, with vandalism acknowledged in the notes of the anonymous editor</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15025">{{Refimprove|date=May 2017}}
{{Use dmy dates|date=April 2013}}
[[File:WMAP 2010.png|thumb|300px|[[Wilkinson Microwave Anisotropy Probe|WMAP]] image of the (extremely tiny) anisotropies in the [[cosmic background radiation]]]]
'''Anisotropy''' {{IPAc-en|ˌ|æ|n|ɪ|ˈ|s|ɒ|t|r|ə|p|i}}, {{IPAc-en|ˌ|æ|n|aɪ|ˈ|s|ɒ|t|r|ə|p|i}} is the property of being directionally dependent, which implies different properties in different directions, as opposed to [[isotropy]]. It can be defined as a difference, when measured along different axes, in a material's [[Physical property|physical]] or [[List of materials properties#Mechanical properties|mechanical properties]] ([[absorbance]], [[refractive index]], [[Electrical resistivity and conductivity|conductivity]], [[tensile strength]], etc.) 

An example of anisotropy is light coming through a [[polarizer]]. Another is [[wood]], which is easier to split along its [[wood grain|grain]] than across it.

==Fields of interest==

===Computer graphics===

In the field of [[computer graphics]], an anisotropic surface changes in appearance as it rotates about its [[Normal (geometry)|geometric normal]], as is the case with [[velvet]].

[[Anisotropic filtering]] (AF) is a method of enhancing the image quality of textures on surfaces that are far away and steeply angled with respect to the point of view. Older techniques, such as [[bilinear filtering|bilinear]] and [[trilinear filtering]], do not take into account the angle a surface is viewed from, which can result in [[aliasing]] or blurring of textures. By reducing detail in one direction more than another, these effects can be reduced.

=== Chemistry ===
A chemical anisotropic [[filter (chemistry)|filter]], as used to filter particles, is a filter with increasingly smaller interstitial spaces in the direction of filtration so that the [[Anatomical terms of location#Proximal and distal|proximal]] [[region]]s filter out larger particles and [[Anatomical terms of location#Proximal and distal|distal]] regions increasingly remove smaller particles, resulting in greater flow-through and more efficient filtration.

In [[Nuclear magnetic resonance spectroscopy|NMR spectroscopy]], the orientation of nuclei with respect to the applied magnetic field determines their [[chemical shift]].  In this context, anisotropic systems refer to the electron distribution of molecules with abnormally high electron density, like the pi system of [[benzene]]. This abnormal electron density affects the applied magnetic field and causes the observed chemical shift to change.

In [[fluorescence spectroscopy]], the [[fluorescence anisotropy]], calculated from the [[polarization (waves)|polarization]] properties of fluorescence from samples excited with plane-polarized light, is used, e.g., to determine the shape of a macromolecule.
Anisotropy measurements reveal the average angular displacement of the fluorophore that occurs between absorption and subsequent emission of a photon.

=== Real-world imagery===
Images of a gravity-bound or man-made environment are particularly anisotropic in the orientation domain, with more image structure located at orientations parallel with or orthogonal to the direction of gravity (vertical and horizontal).

===Physics===&lt;!-- This section is linked from [[Birefringence]] --&gt;
[[File:Plasma-lamp 2.jpg|thumb|300px|right|A [[plasma globe|plasma lamp]] displaying the nature of [[Plasma (physics)|plasmas]], in this case, the phenomenon of "filamentation"]]

[[Physicists]] from [[University of California, Berkeley]] reported about their detection of the cosine anisotropy in [[cosmic microwave background radiation]] in 1977. Their experiment demonstrated the [[Doppler shift]] caused by the movement of the earth with respect to the [[Big Bang|early Universe matter]], the source of the radiation.&lt;ref&gt;{{cite web | url=http://www.muller.lbl.gov/COBE-early_history/anisotropy-PRL.pdf | title=Detection of Anisotropy in the Cosmic Blackbody Radiation | publisher=[[Lawrence Berkeley Laboratory]] and [[Space Sciences Laboratory]], [[University of California, Berkeley]] | date=5 October 1977 | accessdate=15 September 2013 | author=Smoot G. F. |author2=Gorenstein M. V. |author3= [[Richard A. Muller|Muller R. A.]] |last-author-amp=yes }}&lt;/ref&gt; Cosmic anisotropy has also been seen in the alignment of galaxies' rotation axes and polarisation angles of quasars.

Physicists use the term anisotropy to describe direction-dependent properties of materials. [[Magnetic anisotropy]], for example, may occur in a [[plasma (physics)|plasma]], so that its magnetic field is oriented in a preferred direction. Plasmas may also show "filamentation" (such as that seen in [[lightning]] or a [[plasma globe]]) that is directional.

An ''anisotropic liquid'' has the fluidity of a normal liquid, but has an average structural order relative to each other along the molecular axis, unlike water or [[chloroform]], which contain no structural ordering of the molecules. [[Liquid crystals]] are examples of anisotropic liquids.

Some materials [[heat conduction|conduct heat]] in a way that is isotropic, that is independent of spatial orientation around the heat source.  Heat conduction is more commonly anisotropic, which implies that detailed geometric modeling of typically diverse materials being thermally managed is required. The materials used to transfer and reject heat from the heat source in [[electronics]] are often anisotropic.&lt;ref name="Nature8April2013"&gt;{{cite web|last1=Tian|first1=Xiaojuan|last2=Itkis|first2=Mikhail E|last3=Bekyarova|first3=Elena B|last4=Haddon|first4=Robert C|title=Anisotropic Thermal and Electrical Properties of Thin Thermal Interface Layers of Graphite Nanoplatelet-Based Composites|url=http://www.nature.com/articles/srep01710|website=Nature.com|accessdate=11 August 2016|archiveurl=https://web.archive.org/web/20160811183641/http://www.nature.com/articles/srep01710|archivedate=11 August 2016|date=8 April 2013}}&lt;/ref&gt;

Many [[crystal]]s are anisotropic to [[light]] ("optical anisotropy"), and exhibit properties such as [[birefringence]]. [[Crystal optics]] describes light propagation in these media. An "axis of anisotropy" is defined as the axis along which isotropy is broken (or an axis of symmetry, such as normal to crystalline layers). Some materials can have multiple such [[optical axis|optical axes]].

===Geophysics and geology===
[[Seismic anisotropy]] is the variation of seismic wavespeed with direction. Seismic anisotropy is an indicator of long range order in a material, where features smaller than the seismic [[wavelength]] (e.g., crystals, cracks, pores, layers or inclusions) have a dominant alignment. This alignment leads to a directional variation of [[Elasticity (physics)|elasticity]] wavespeed. Measuring the effects of anisotropy in seismic data can provide important information about processes and mineralogy in the Earth; indeed, significant seismic anisotropy has been detected in the Earth's [[Crust (geology)|crust]], [[mantle (geology)|mantle]] and [[inner core]].

[[Geological]] formations with distinct layers of [[sedimentary]] material can exhibit electrical anisotropy; [[electrical conductivity]] in one direction (e.g. parallel to a layer), is different from that in another (e.g. perpendicular to a layer). This property is used in the gas and [[oil exploration]] industry to identify [[hydrocarbon]]-bearing sands in sequences of [[sand]] and [[shale]]. Sand-bearing hydrocarbon assets have high [[resistivity]] (low conductivity), whereas shales have lower resistivity. [[Formation evaluation]] instruments measure this conductivity/resistivity and the results are used to help find oil and gas in wells.

The [[hydraulic conductivity]] of [[aquifer]]s is often anisotropic for the same reason. When calculating groundwater flow to [[drainage|drains]]&lt;ref&gt;R.J.Oosterbaan, 1997, The energy balance of groundwater flow applied to subsurface drainage in anisotropic soils by pipes or ditches with entrance resistance. On line: [http://www.waterlog.info/pdf/enerart.pdf]. The corresponding free EnDrain program can be downloaded from: [http://www.waterlog.info/endrain.htm].&lt;/ref&gt; or to [[Water well|wells]],&lt;ref&gt;R.J.Oosterbaan, 2002, Subsurface drainage by (tube)wells, 9 pp. On line: [http://www.waterlog.info/pdf/wellspac.pdf]. The corresponding free WellDrain program can be downloaded from: [http://www.waterlog.info/weldrain.htm]&lt;/ref&gt; the difference between horizontal and vertical permeability must be taken into account, otherwise the results may be subject to error.

Most common rock-forming [[mineral]]s are anisotropic, including [[quartz]] and [[feldspar]]. Anisotropy in minerals is most reliably seen in their [[optical mineralogy|optical properties]]. An example of an isotropic mineral is [[garnet]].

===Medical acoustics===
Anisotropy is also a well-known property in medical ultrasound imaging describing a different resulting [[echogenicity]] of soft tissues, such as tendons, when the angle of the transducer is changed. Tendon fibers appear hyperechoic (bright) when the transducer is perpendicular to the tendon, but can appear hypoechoic (darker) when the transducer is angled obliquely. This can be a source of interpretation error for inexperienced practitioners.

===Material science and engineering===
Anisotropy, in Material Science, is a material's directional dependence of a physical property. Most materials exhibit anisotropic behavior. An example would be the dependence of [[Young's modulus]] on the direction of load.&lt;ref&gt;{{cite book
  |last = Kocks
  |first = U.F.
  |title = Texture and Anisotropy: Preferred Orientations in Polycrystals and their effect on Materials Properties
  |publisher = [[Cambridge]]
  |year = 2000
  |isbn = 9780521794206}}&lt;/ref&gt;
In such a case anisotropy could be effectively measured directly from its stiffness tensor independently of its origin which may for instance be its texture, randomness of internal composition or defects.
&lt;ref&gt;{{cite journal
| last       = Sokołowski
| first      = Damian
| last2      = Kamiński
| first2     = Marcin
| date       = 2018
| title      = Homogenization of carbon/polymer composites with anisotropic distribution of particles and stochastic interface defects
| url        = https://link.springer.com/article/10.1007/s00707-018-2174-7
| journal    = Acta Mechanica
| volume     = 229
| issue      = 9
| pages      = 3727–3765
| doi        = 10.1007/s00707-018-2174-7
| access-date = 21 August 2018
}}&lt;/ref&gt;
[[Texture (crystalline)|Texture]] patterns are often produced during manufacturing of the material.  In the case of rolling, "stringers" of texture are produced in the direction of rolling, which can lead to vastly different properties in the rolling and transverse directions. 
Some materials, such as wood and fibre-reinforced [[composite material|composites]] are very anisotropic, being much stronger along the grain/fibre than across it. Metals and alloys tend to be more isotropic, though they can sometimes exhibit significant anisotropic behaviour. This is especially important in processes such as [[Deep drawing|deep-drawing]].

Wood is a naturally anisotropic (but often simplified to be [[transversely isotropic]]) material.  Its properties vary widely when measured with or against the growth grain.  For example, wood's strength and hardness is different for the same sample measured in different orientations.

In the Mechanics of Continuum Materials, isotropy and anisotropy are rigorously described through the symmetry group of the constitutive relation.&lt;ref&gt;{{Cite book|url=https://link.springer.com/10.1007/978-3-662-10388-3|title=The Non-Linear Field Theories of Mechanics - Springer|last=Truesdell|first=Clifford|last2=Noll|first2=Walter|language=en|doi=10.1007/978-3-662-10388-3}}&lt;/ref&gt;

===Microfabrication===

Anisotropic etching techniques (such as [[deep reactive ion etching]]) are used in microfabrication processes to create well defined microscopic features with a high [[aspect ratio]].  These features are commonly used in [[microelectromechanical systems|MEMS]] and [[microfluidic]] devices, where the anisotropy of the features is needed to impart desired optical, electrical, or physical properties to the device. Anisotropic etching can also refer to certain chemical etchants used to etch a certain material preferentially over certain crystallographic planes (e.g., KOH etching of [[silicon]] [100] produces pyramid-like structures)

===Neuroscience===

[[Diffusion tensor imaging]] is an [[magnetic resonance imaging|MRI]] technique that involves measuring the fractional anisotropy of the random motion ([[Brownian motion]]) of water molecules in the brain.  Water molecules located in [[White matter|fiber tracts]] are more likely to be anisotropic, since they are restricted in their movement (they move more in the dimension parallel to the fiber tract rather than in the two dimensions orthogonal to it), whereas water molecules dispersed in the rest of the brain have less restricted movement and therefore display more isotropy.  This difference in fractional anisotropy is exploited to create a map of the fiber tracts in the brains of the individual.

===Atmospheric radiative transfer===
[[Radiance]] fields (see [[BRDF]]) from a reflective surface are often not isotropic in nature. This makes calculations of the total energy being reflected from any scene a difficult quantity to calculate. In [[remote sensing]] applications, anisotropy functions can be derived for specific scenes, immensely simplifying the calculation of the net reflectance or (thereby) the net [[irradiance]] of a scene.
For example, let the [[BRDF]] be &lt;math&gt;\gamma(\Omega_i, \Omega_v)&lt;/math&gt; where 'i' denotes incident direction and 'v' denotes viewing direction (as if from a satellite or other instrument). And let P be the Planar Albedo, which represents the total reflectance from the scene.
:&lt;math&gt;P(\Omega_i) = \int_{\Omega_v}\gamma(\Omega_i, \Omega_v)\hat{n}\cdot d\hat\Omega_v&lt;/math&gt;
:&lt;math&gt;A(\Omega_i, \Omega_v) = \frac{\gamma(\Omega_i, \Omega_v)}{P(\Omega_i)}&lt;/math&gt;

It is of interest because, with knowledge of the anisotropy function as defined, a measurement of the [[BRDF]] from a single viewing direction (say, &lt;math&gt;\Omega_v&lt;/math&gt;) yields a measure of the total scene reflectance (Planar Albedo) for that specific incident geometry (say, &lt;math&gt;\Omega_i&lt;/math&gt;).

==See also==
*[[Circular symmetry]]

==References==
{{Reflist}}

== External links ==
{{Wiktionary|anisotropy}}
*[http://www.knitty.com/ISSUEsummer05/FEATsum05TBP.html "Gauge, and knitted fabric generally, is an anisotropic phenomenon"]
*[https://web.archive.org/web/20100303080919/http://aluminium.matter.org.uk/content/html/eng/default.asp?catid=99&amp;pageid=1028022659 "Overview of Anisotropy"]
*[http://www.doitpoms.ac.uk/tlplib/anisotropy/index.php DoITPoMS Teaching and Learning Package: "Introduction to Anisotropy"]

{{Authority control}}

[[Category:Orientation (geometry)]]</text>
      <sha1>l2zenikrx3qlm5kxu4l4suyi9uwjnb9</sha1>
    </revision>
  </page>
  <page>
    <title>Beck–Fiala theorem</title>
    <ns>0</ns>
    <id>39400139</id>
    <revision>
      <id>857848254</id>
      <parentid>836289903</parentid>
      <timestamp>2018-09-03T12:19:57Z</timestamp>
      <contributor>
        <ip>109.226.20.85</ip>
      </contributor>
      <comment>/* Statement */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3023">In mathematics, the '''Beck–Fiala theorem''' is a major theorem in [[discrepancy theory]] due to [[József Beck]] and [[Tibor Fiala]]. Discrepancy is concerned with coloring elements of a ground set such that each set in a certain set system is as balanced as possible, i.e., has approximately the same number of elements of each color. The Beck–Fiala theorem is concerned with the case where each element doesn't appear many times across all sets. The theorem guarantees that if each element appears at most {{mvar|t}} times, then the elements can be colored so that the imbalance is at most {{math|2''t'' &amp;minus; 1}}.

==Statement==
Formally, given a universe

: &lt;math&gt;[n] = \{1, \ldots , n\}&lt;/math&gt;

and a collection of subsets

: &lt;math&gt; S_1, S_2, \ldots, S_m \subseteq [n]&lt;/math&gt;

such that for each &lt;math&gt; i \in [n]&lt;/math&gt;,

: &lt;math&gt; \vert \{j; i \in S_j \} \vert \leq t,&lt;/math&gt;

then one can find an assignment
: &lt;math&gt;x : [n] \rightarrow \{-1,+1\}&lt;/math&gt;

such that

: &lt;math&gt;|x(S_j)| \leq 2t-1, \forall j \text{ where }x(S_j) := \sum_{i \in S_j} x_i.&lt;/math&gt;

== Proof sketch ==

The proof is based on a simple linear-algebraic argument. Start with &lt;math&gt; x_i = 0&lt;/math&gt; for all elements and call all variables active in the beginning.

Consider only sets with &lt;math&gt;\vert S_j \vert &gt; t&lt;/math&gt;. Since each element appears at most &lt;math&gt;t&lt;/math&gt; times in a set, there are less than &lt;math&gt;n&lt;/math&gt; such sets. Now, enforce linear constraints &lt;math&gt;\sum_{i \in S_j} x_i = 0&lt;/math&gt; for them. Since it is a non-trivial linear subspace of &lt;math&gt;\mathbf{R}^n&lt;/math&gt; with fewer constraints than variables, there is a non-zero solution. Normalize this solution, and at least one of the values is either &lt;math&gt;+1,-1&lt;/math&gt;. Set this value and inactivate this variable. Now, ignore the sets with less than &lt;math&gt;t&lt;/math&gt; active variables. And repeat the same procedure enforcing the linear constraints that the sum of active variables of each remaining set is still the same. By the same counting argument, there is a non-trivial solution, so one can take linear combinations of this with the original one until some element becomes &lt;math&gt;+1,-1&lt;/math&gt;. Repeat until all variables are set.

Once a set is ignored, the sum of the values of its variables is zero and there are at most &lt;math&gt;t&lt;/math&gt; unset variables. The change in those can increase &lt;math&gt;x(S_j)&lt;/math&gt; to at most &lt;math&gt;2t-1&lt;/math&gt;.

== References ==

*{{cite  journal
  | title = "Integer-making" theorems
  | journal =  Discrete Applied Mathematics
  | volume =  3
  | issue =  1
  | year = 1981
  | pages = 1-8
  | doi = 10.1016/0166-218X(81)90022-6
  | author =  József Beck and Tibor Fiala }}
*{{cite book |title=The Discrepancy Method: Randomness and Complexity |last=Chazelle |first=Bernard |authorlink=Bernard Chazelle |coauthors= |year=2000 |publisher=Cambridge University Press |location=New York |isbn=0-521-77093-9 |pages= |url=http://www.cs.princeton.edu/~chazelle/book.html}}

{{DEFAULTSORT:Beck-Fiala theorem}}
[[Category:Mathematical theorems]]</text>
      <sha1>6lgcxhix34chjjn3hb8hvjv3c05hynh</sha1>
    </revision>
  </page>
  <page>
    <title>Ben Andrews (mathematician)</title>
    <ns>0</ns>
    <id>25371909</id>
    <revision>
      <id>847031858</id>
      <parentid>791038344</parentid>
      <timestamp>2018-06-22T12:52:38Z</timestamp>
      <contributor>
        <username>Melcous</username>
        <id>20472590</id>
      </contributor>
      <comment>[[WP:POSTNOM]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2060">'''Ben Andrews''' is a [[Senior Fellow]] at the [[Centre for Mathematics and its Applications]] at the [[Australian National University]].&lt;ref&gt;
{{cite web
| url = http://wwwmaths.anu.edu.au/~andrews/
| title = ANU - Mathematical Sciences Institute (MSI) - People - Ben Andrews
| publisher = wwwmaths.anu.edu.au
| accessdate = 2009-12-09
| archiveurl= https://web.archive.org/web/20091030080603/http://wwwmaths.anu.edu.au/~andrews/| archivedate= 30 October 2009 &lt;!--DASHBot--&gt;| deadurl= no}}&lt;/ref&gt;
His PhD Thesis, 1993, was on ''Evolving Convex Hypersurfaces'' at Australian National University.&lt;ref&gt;
{{cite web
| url = http://projecteuclid.org/DPubS?verb=Display&amp;version=1.0&amp;service=UI&amp;handle=euclid.jdg/1214458106&amp;page=record
| title = Andrews: Contraction of convex hypersurfaces by their affine normal
| publisher = projecteuclid.org
| accessdate = 2009-12-09
}}&lt;/ref&gt; In 2003, he received the [[Australian Mathematical Society Medal]], along with Andrew Hassell, for distinguished research in the mathematical sciences.&lt;ref&gt;
{{cite web
| url = http://www.austms.org.au/The+Australian+Mathematical+Society+Medal
| title = Aust MS : The Australian Mathematical Society Medal
| publisher = www.austms.org.au
| accessdate = 2009-12-09
| archiveurl = https://web.archive.org/web/20091210145608/http://www.austms.org.au/The%2BAustralian%2BMathematical%2BSociety%2BMedal
| archivedate = 10 December 2009
| deadurl = no
| df = 
}}&lt;/ref&gt; In 2012 he became a fellow of the [[American Mathematical Society]].&lt;ref&gt;[http://www.ams.org/profession/fellows-list List of Fellows of the American Mathematical Society], retrieved 2012-11-03.&lt;/ref&gt;

== References ==
{{reflist}}

{{Authority control}}
{{DEFAULTSORT:Andrews, Ben}}
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]
[[Category:20th-century Australian mathematicians]]
[[Category:21st-century Australian mathematicians]]
[[Category:Australian National University faculty]]
[[Category:Fellows of the American Mathematical Society]]


{{Australia-scientist-stub}}
{{mathematician-stub}}</text>
      <sha1>7q7j457ruz55tb0pvogxc9q82ssnrrm</sha1>
    </revision>
  </page>
  <page>
    <title>Boolean algebras canonically defined</title>
    <ns>0</ns>
    <id>6318542</id>
    <revision>
      <id>841529586</id>
      <parentid>841514293</parentid>
      <timestamp>2018-05-16T11:49:25Z</timestamp>
      <contributor>
        <username>OAbot</username>
        <id>28481209</id>
      </contributor>
      <minor/>
      <comment>[[Wikipedia:OABOT|Open access bot]]: add pmc identifier to citation with #oabot.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="59647">{{no footnotes|date=May 2015}}
:''Boolean algebras have been formally defined variously as a kind of lattice and as a kind of ring. This article presents them,  equally formally, as simply the models of the equational theory of two values, and observes the equivalence of both the lattice and ring definitions to this more elementary one.''



[[Boolean algebra]] is a mathematically rich branch of [[abstract algebra]]. Just as [[group theory]] deals with [[Group (mathematics)|groups]], and [[linear algebra]] with [[vector spaces]], [[Boolean algebras]] are models of the [[equational theory]] of the two values 0 and 1 (whose interpretation need not be numerical). Common to Boolean algebras, groups, and vector spaces is the notion of an [[algebraic structure]], a [[Set (mathematics)|set]] closed under zero or more [[operation (mathematics)|operations]]  satisfying certain equations.

Just as there are basic examples of groups, such as the group '''Z''' of [[integer]]s and the [[permutation group]] ''S''&lt;sub&gt;''n''&lt;/sub&gt; of [[permutation]]s of ''n'' objects, there are also basic examples of Boolean algebra such as the following.
* The algebra of [[binary digit]]s or bits 0 and 1 under the logical operations including disjunction, conjunction, and negation. Applications include the [[propositional calculus]] and the theory of [[digital circuits]].
* The [[algebra of sets]] under the set operations including [[Union (set theory)|union]], [[intersection (set theory)|intersection]], and [[Complement (set theory)|complement]]. Applications include any area of mathematics for which sets form a natural [[foundation of mathematics|foundation]].
Boolean algebra thus permits applying the methods of [[abstract algebra]] to [[mathematical logic]], [[digital logic]], and the set-theoretic [[foundations of mathematics]].

Unlike [[Group (mathematics)|groups]] of finite [[order (group theory)|order]], which exhibit complexity and diversity and whose [[first-order logic|first-order]] theory is [[Decidability (logic)|decidable]] only in special cases, all finite Boolean algebras share the same theorems and have a decidable first-order theory. Instead the intricacies of Boolean algebra are divided between the structure of infinite algebras and the [[algorithm]]ic complexity of their [[syntax|syntactic]] structure.

==Definition==
Boolean algebra treats the [[equational theory]] of the maximal two-element [[finitary]] algebra, called the '''Boolean prototype''', and the models of that theory, called '''Boolean algebras'''. These terms are defined as follows.

An [[Universal algebra|algebra]] is a [[Indexed family|family]] of operations on a set, called the underlying set of the algebra. We take the underlying set of the Boolean prototype to be {0,1}.

An algebra is '''[[finitary]]''' when each of its operations takes only finitely many arguments. For the prototype each argument of an operation is either 0 or 1, as is the result of the operation. The maximal such algebra consists of all finitary operations on {0,1}.

The number of arguments taken by each operation is called the [[arity]] of the operation. An operation on {0,1} of arity ''n'', or ''n''-ary operation, can be applied to any of 2&lt;sup&gt;''n''&lt;/sup&gt; possible values for its ''n'' arguments. For each choice of arguments the operation may return 0 or 1, whence there are 2&lt;sup&gt;&lt;span&gt;2&lt;sup&gt;''n''&lt;/sup&gt;&lt;/span&gt;&lt;/sup&gt; ''n''-ary operations.

The prototype therefore has two operations taking no arguments, called zeroary or '''nullary''' operations, namely zero and one. It has four [[unary operation]]s, two of which are constant operations, another is the identity, and the most commonly used one, called ''negation'', returns the opposite of its argument: 1 if 0, 0 if 1. It has sixteen [[binary operation]]s; again two of these are constant, another returns its first argument, yet another returns its second, one is called ''conjunction'' and returns 1 if both arguments are 1 and otherwise 0, another is called ''disjunction'' and returns 0 if both arguments are 0 and otherwise 1, and so on. The number of (''n''+1)-ary operations in the prototype is the square of the number of ''n''-ary operations, so there are 16&lt;sup&gt;2&lt;/sup&gt; = 256 ternary operations, 256&lt;sup&gt;2&lt;/sup&gt; = 65,536 quaternary operations, and so on.

A [[Indexed family|family]] is indexed by an [[index set]]. In the case of a family of operations forming an algebra, the indices are called '''operation symbols''', constituting the '''language''' of that algebra. The operation indexed by each symbol is called the denotation or '''[[interpretation (model theory)|interpretation]]''' of that symbol. Each operation symbol specifies the arity of its interpretation, whence all possible interpretations of a symbol have the same arity. In general it is possible for an algebra to interpret distinct symbols with the same operation, but this is not the case for the prototype, whose symbols are in one-one correspondence with its operations. The prototype therefore has 2&lt;sup&gt;&lt;span&gt;2&lt;sup&gt;''n''&lt;/sup&gt;&lt;/span&gt;&lt;/sup&gt; ''n''-ary operation symbols, called the '''Boolean operation symbols''' and forming the language of Boolean algebra. Only a few operations have conventional symbols, such as ¬ for negation, ∧ for conjunction, and ∨ for disjunction. It is convenient to consider the ''i''-th ''n''-ary symbol to be &lt;sup&gt;''n''&lt;/sup&gt;''f''&lt;sub&gt;''i''&lt;/sub&gt; as done below in the section on [[Boolean algebras canonically defined#Truth tables|truth tables]].

An [[equational theory]] in a given language consists of equations between terms built up from variables using symbols of that language. Typical equations in the language of Boolean algebra are ''x''∧''y'' = ''y''∧''x'', ''x''∧''x'' = ''x'', ''x''∧¬''x'' = ''y''∧¬''y'', and ''x''∧''y'' = ''x''.

An algebra '''[[satisfiability|satisfies]]''' an equation when the equation holds for all possible values of its variables in that algebra when the operation symbols are interpreted as specified by that algebra. The laws of Boolean algebra are the equations in the language of Boolean algebra satisfied by the prototype. The first three of the above examples are Boolean laws, but not the fourth since 1∧0 ≠ 1.

The [[equational theory]] of an algebra is the set of all equations satisfied by the algebra. The laws of Boolean algebra therefore constitute the equational theory of the Boolean prototype.

A [[model (model theory)|model of a theory]] is an algebra interpreting the operation symbols in the language of the theory and satisfying the equations of the theory.
: ''A Boolean algebra is any model of the laws of Boolean algebra.''

That is, a Boolean algebra is a set and a family of operations thereon interpreting the Boolean operation symbols and satisfying the same laws as the Boolean prototype.

If we define a homologue of an algebra to be a model of the equational theory of that algebra, then a Boolean algebra can be defined as any homologue of the prototype.

'''Example 1'''. The Boolean prototype is a Boolean algebra, since trivially it satisfies its own laws. It is thus the prototypical Boolean algebra. We did not call it that initially in order to avoid any appearance of circularity in the definition.

==Basis==
The operations need not be all explicitly stated. A ''basis'' is any set from which the remaining operations can be obtained by composition. A "Boolean algebra" may be defined from any of several different bases. Three bases for Boolean algebra are in common use, the lattice basis, the ring basis, and the [[Sheffer stroke]] or NAND basis. These bases impart respectively a logical, an arithmetical, and a parsimonious character to the subject.
* The [[lattice (order)|lattice]] basis originated in the 19th century with the work of [[George Boole|Boole]], [[Charles Sanders Peirce|Peirce]], and others seeking an algebraic formalization of logical thought processes.
* The [[Boolean ring|ring]] basis emerged in the 20th century with the work of [[Ivan Ivanovich Zhegalkin|Zhegalkin]] and [[Marshall Stone|Stone]] and became the basis of choice for algebraists coming to the subject from a background in [[abstract algebra]]. Most treatments of Boolean algebra assume the lattice basis, a notable exception being [[Paul Halmos|Halmos]][1963] whose linear algebra background evidently endeared the ring basis to him.
* Since all finitary operations on {0,1} can be defined in terms of the [[Sheffer stroke]] NAND (or its dual NOR), the resulting economical basis has become the basis of choice for analyzing [[digital circuit]]s, in particular [[gate array]]s in [[digital electronics]].

The common elements of the lattice and ring bases are the constants 0 and 1, and an [[associative]] [[commutative]] [[binary operation]], called [[meet (mathematics)|meet]] ''x''∧''y'' in the lattice basis, and [[multiplication]] ''xy'' in the ring basis. The distinction is only terminological. The lattice basis has the further operations of [[join (mathematics)|join]], ''x''∨''y'', and [[Complement (order theory)|complement]], ¬''x''. The ring basis has instead the arithmetic operation ''x''⊕''y'' of [[addition]] (the symbol ⊕ is used in preference to + because the latter is sometimes given the Boolean reading of join).

To be a basis is to yield all other operations by composition, whence any two bases must be intertranslatable. The lattice basis translates ''x''∨''y'' to the ring basis as ''x''⊕''y''⊕''xy'', and ¬''x'' as ''x''⊕1. Conversely the ring basis translates ''x''⊕''y'' to the lattice basis as (''x''∨''y'')∧¬(''x''∧''y'').

Both of these bases allow Boolean algebras to be defined via a subset of the equational properties of the Boolean operations. For the lattice basis, it suffices to define a Boolean algebra as a [[distributive lattice]] satisfying ''x''∧¬''x'' = 0 and ''x''∨¬''x'' = 1, called a [[Complemented lattice|complemented]] distributive lattice. The ring basis turns a Boolean algebra into a [[Boolean ring]], namely a ring satisfying ''x''&lt;sup&gt;2&lt;/sup&gt; = ''x''.

[[Emil Post]] gave a necessary and sufficient condition for a set of operations to be a basis for the nonzeroary Boolean operations. A ''nontrivial'' property is one shared by some but not all operations making up a basis. Post listed five nontrivial properties of operations, identifiable with the five [[Post's class]]es, each preserved by composition, and showed that a set of operations formed a basis if, for each property, the set contained an operation lacking that property. (The converse of Post's theorem, extending "if" to "[[iff|if and only if]]," is the easy observation that a property from among these five holding of every operation in a candidate basis will also hold of every operation formed by composition from that candidate, whence by nontriviality of that property the candidate will fail to be a basis.) Post's five properties are:
* [[Monotonic function|monotone]], no 0-1 input transition can cause a 1-0 output transition;
* [[affine transformation|affine]], representable with [[Zhegalkin polynomial]]s that lack bilinear or higher terms, e.g. ''x''⊕''y''⊕1 but not ''xy'';
* [[De Morgan's laws|self-dual]], so that complementing all inputs complements the output, as with ''x'', or the [[median operator]] ''xy''⊕''yz''⊕''zx'', or their negations;
* [[strict function|strict]] (mapping the all-zeros input to zero);
* costrict (mapping all-ones to one).
The [[Sheffer stroke|NAND]] (dually NOR) operation lacks all these, thus forming a basis by itself.

==Truth tables==
The finitary operations on {0,1} may be exhibited as [[truth table]]s, thinking of 0 and 1 as the [[truth value]]s '''false''' and '''true'''. They can be laid out in a uniform and application-independent way that allows us to name, or at least number, them individually. These names provide a convenient shorthand for the Boolean operations. The names of the ''n''-ary operations are binary numbers of 2&lt;sup&gt;''n''&lt;/sup&gt; bits. There being 2&lt;sup&gt;2&lt;span&gt;&lt;sup&gt;''n''&lt;/sup&gt;&lt;/span&gt;&lt;/sup&gt; such operations, one cannot ask for a more succinct nomenclature. Note that each finitary operation can be called a [[switching function]].

This layout and associated naming of operations is illustrated here in full for arities from 0 to 2.

&lt;div&gt;
&lt;center&gt;
::{| border="0" style="border:4px"
|+ '''Truth tables for the Boolean operations of arity up to 2'''
|- valign="top"
|
{| class="wikitable" style="text-align:center"
|+ Constants
|-
!&lt;math&gt;{}^0\!f_0&lt;/math&gt;
!&lt;math&gt;{}^0\!f_1&lt;/math&gt;
|-
|0 || 1
|}
|
{| class="wikitable" style="text-align:center"
|+ Unary operations
|-
!&lt;math&gt;x_0&lt;/math&gt;
!&lt;math&gt;{}^1\!f_0&lt;/math&gt;
!&lt;math&gt;{}^1\!f_1&lt;/math&gt;
!&lt;math&gt;{}^1\!f_2&lt;/math&gt;
!&lt;math&gt;{}^1\!f_3&lt;/math&gt;
|-
!0
|0 || 1 || 0 || 1
|-
!1
|0 || 0 || 1 || 1
|}
|-
| colspan="5" |
{| class="wikitable" style="text-align:center"
|+ Binary operations
|-
!&lt;math&gt;x_0&lt;/math&gt;
!&lt;math&gt;x_1&lt;/math&gt;
!&lt;math&gt;{}^2\!f_0&lt;/math&gt;
!&lt;math&gt;{}^2\!f_1&lt;/math&gt;
!&lt;math&gt;{}^2\!f_2&lt;/math&gt;
!&lt;math&gt;{}^2\!f_3&lt;/math&gt;
!&lt;math&gt;{}^2\!f_4&lt;/math&gt;
!&lt;math&gt;{}^2\!f_5&lt;/math&gt;
!&lt;math&gt;{}^2\!f_6&lt;/math&gt;
!&lt;math&gt;{}^2\!f_7&lt;/math&gt;
!&lt;math&gt;{}^2\!f_8&lt;/math&gt;
!&lt;math&gt;{}^2\!f_9&lt;/math&gt;
!&lt;math&gt;{}^2\!f_{10}&lt;/math&gt;
!&lt;math&gt;{}^2\!f_{11}&lt;/math&gt;
!&lt;math&gt;{}^2\!f_{12}&lt;/math&gt;
!&lt;math&gt;{}^2\!f_{13}&lt;/math&gt;
!&lt;math&gt;{}^2\!f_{14}&lt;/math&gt;
!&lt;math&gt;{}^2\!f_{15}&lt;/math&gt;
|-
!0
!0
|0 || 1 || 0 || 1 || 0 || 1 || 0 || 1 || 0 || 1 || 0 || 1 || 0 || 1 || 0 || 1
|-
!1
!0
|0 || 0 || 1 || 1 || 0 || 0 || 1 || 1 || 0 || 0 || 1 || 1 || 0 || 0 || 1 || 1
|-
!0
!1
|0 || 0 || 0 || 0 || 1 || 1 || 1 || 1 || 0 || 0 || 0 || 0 || 1 || 1 || 1 || 1
|-
!1
!1
|0 || 0 || 0 || 0 || 0 || 0 || 0 || 0 || 1 || 1 || 1 || 1 || 1 || 1 || 1 || 1
|}
|}
&lt;/center&gt;
&lt;/div&gt;

These tables continue at higher arities, with 2&lt;sup&gt;''n''&lt;/sup&gt; rows at arity ''n'', each row giving a valuation or binding of the ''n'' variables ''x''&lt;sub&gt;0&lt;/sub&gt;,...''x''&lt;sub&gt;''n''−1&lt;/sub&gt; and each column headed &lt;sup&gt;''n''&lt;/sup&gt;''f''&lt;sub&gt;''i''&lt;/sub&gt; giving the value &lt;sup&gt;''n''&lt;/sup&gt;''f''&lt;sub&gt;''i''&lt;/sub&gt;(''x''&lt;sub&gt;0&lt;/sub&gt;,...,''x''&lt;sub&gt;''n''−1&lt;/sub&gt;) of the ''i''-th ''n''-ary operation at that valuation. The operations include the variables, for example &lt;sup&gt;1&lt;/sup&gt;''f''&lt;sub&gt;2&lt;/sub&gt; is ''x''&lt;sub&gt;0&lt;/sub&gt; while &lt;sup&gt;2&lt;/sup&gt;''f''&lt;sub&gt;10&lt;/sub&gt; is ''x''&lt;sub&gt;0&lt;/sub&gt; (as two copies of its unary counterpart) and &lt;sup&gt;2&lt;/sup&gt;''f''&lt;sub&gt;12&lt;/sub&gt; is ''x''&lt;sub&gt;1&lt;/sub&gt; (with no unary counterpart). Negation or complement ¬''x''&lt;sub&gt;0&lt;/sub&gt; appears as &lt;sup&gt;1&lt;/sup&gt;''f''&lt;sub&gt;1&lt;/sub&gt; and again as &lt;sup&gt;2&lt;/sup&gt;''f''&lt;sub&gt;5&lt;/sub&gt;, along with &lt;sup&gt;2&lt;/sup&gt;''f''&lt;sub&gt;3&lt;/sub&gt; (¬''x''&lt;sub&gt;1&lt;/sub&gt;, which did not appear at arity 1), disjunction or union ''x''&lt;sub&gt;0&lt;/sub&gt;∨''x''&lt;sub&gt;1&lt;/sub&gt; as &lt;sup&gt;2&lt;/sup&gt;''f''&lt;sub&gt;14&lt;/sub&gt;, conjunction or intersection ''x''&lt;sub&gt;0&lt;/sub&gt;∧''x''&lt;sub&gt;1&lt;/sub&gt; as &lt;sup&gt;2&lt;/sup&gt;''f''&lt;sub&gt;8&lt;/sub&gt;, implication ''x''&lt;sub&gt;0&lt;/sub&gt;→''x''&lt;sub&gt;1&lt;/sub&gt; as &lt;sup&gt;2&lt;/sup&gt;''f''&lt;sub&gt;13&lt;/sub&gt;, exclusive-or symmetric difference ''x''&lt;sub&gt;0&lt;/sub&gt;⊕''x''&lt;sub&gt;1&lt;/sub&gt;  as &lt;sup&gt;2&lt;/sup&gt;''f''&lt;sub&gt;6&lt;/sub&gt;, set difference ''x''&lt;sub&gt;0&lt;/sub&gt;−''x''&lt;sub&gt;1&lt;/sub&gt; as &lt;sup&gt;2&lt;/sup&gt;''f''&lt;sub&gt;2&lt;/sub&gt;, and so on.

As a minor detail important more for its form than its content, the operations of an algebra are traditionally organized as a list. Although we are here indexing the operations of a Boolean algebra by the finitary operations on {0,1}, the truth-table presentation above serendipitously orders the operations first by arity and second by the layout of the tables for each arity. This permits organizing the set of all Boolean operations in the traditional list format. The list order for the operations of a given arity is determined by the following two rules.

: (i) The ''i''-th row in the left half of the table is the binary representation of ''i'' with its least significant or 0-th bit on the left ("little-endian" order, originally proposed by [[Alan Turing]], so it would not be unreasonable to call it Turing order).

: (ii)  The ''j''-th column in the right half of the table is the binary representation of ''j'', again in little-endian order. In effect the subscript of the operation ''is'' the truth table of that operation. By analogy with [[Gödel number]]ing of computable functions one might call this numbering of the Boolean operations the Boole numbering.

When programming in C or Java, bitwise disjunction is denoted &lt;tt&gt;''x''|''y''&lt;/tt&gt;, conjunction &lt;tt&gt;''x''&amp;''y''&lt;/tt&gt;, and negation &lt;tt&gt;~''x''&lt;/tt&gt;. A program can therefore represent for example the operation ''x''∧(''y''∨''z'') in these languages as &lt;tt&gt;''x''&amp;(''y''|''z'')&lt;/tt&gt;, having previously set &lt;tt&gt;''x''&amp;nbsp;= 0xaa&lt;/tt&gt;, &lt;tt&gt;''y''&amp;nbsp;= 0xcc&lt;/tt&gt;, and &lt;tt&gt;''z''&amp;nbsp;= 0xf0&lt;/tt&gt; (the "&lt;tt&gt;0x&lt;/tt&gt;" indicates that the following constant is to be read in hexadecimal or base 16), either by assignment to variables or defined as macros. These one-byte (eight-bit) constants correspond to the columns for the input variables in the extension of the above tables to three variables. This technique is almost universally used in raster graphics hardware to provide a flexible variety of ways of combining and masking images, the typical operations being ternary and acting simultaneously on source, destination, and mask bits.

==Examples==

===Bit vectors===
'''Example 2'''. All [[bit vector]]s of a given length form a Boolean algebra "pointwise", meaning that any ''n''-ary Boolean operation can be applied to ''n'' bit vectors one bit position at a time. For example, the ternary OR of three bit vectors each of length 4 is the bit vector of length 4 formed by oring the three bits in each of the four bit positions, thus 0100∨1000∨1001&amp;nbsp;= 1101. Another example is the truth tables above for the ''n''-ary operations, whose columns are all the bit vectors of length 2&lt;sup&gt;''n''&lt;/sup&gt; and which therefore can be combined pointwise whence the ''n''-ary operations form a Boolean algebra.
This works equally well for bit vectors of finite and infinite length, the only rule being that the bit positions all be indexed by the same set in order that "corresponding position" be well defined.

The '''[[atom (order theory)|atoms]]''' of such an algebra are the bit vectors containing exactly one 1. In general the atoms of a Boolean algebra are those elements ''x'' such that ''x''∧''y'' has only two possible values, ''x'' or 0.

===Power set algebra===
'''Example 3'''. The  '''power set algebra''', the set 2&lt;sup&gt;''W''&lt;/sup&gt; of all subsets of a given set ''W''. This is just Example 2 in disguise, with ''W'' serving to index the bit positions. Any subset ''X'' of ''W'' can be viewed as the bit vector having 1's in just those bit positions indexed by elements of ''X''. Thus the all-zero vector is the empty subset of ''W'' while the all-ones vector is ''W'' itself, these being the constants 0 and 1 respectively of the power set algebra. The counterpart of disjunction ''x''∨''y'' is union ''X''∪''Y'', while that of conjunction ''x''∧''y'' is intersection ''X''∩''Y''. Negation ¬''x'' becomes ~''X'', complement relative to ''W''. There is also set difference ''X''∖''Y''&amp;nbsp;= ''X''∩~''Y'', symmetric difference (''X''∖''Y'')∪(''Y''∖''X''), ternary union ''X''∪''Y''∪''Z'', and so on. The atoms here are the singletons, those subsets with exactly one element.

Examples 2 and 3 are special cases of a general construct of algebra called [[direct product]], applicable not just to Boolean algebras but all kinds of algebra including groups, rings, etc. The direct product of any family ''B''&lt;sub&gt;i&lt;/sub&gt; of Boolean algebras where ''i'' ranges over some index set ''I'' (not necessarily finite or even countable) is a Boolean algebra consisting of all ''I''-tuples (...''x''&lt;sub&gt;i&lt;/sub&gt;,...) whose ''i''-th element is taken from ''B''&lt;sub&gt;''i''&lt;/sub&gt;. The operations of a direct product are the corresponding operations of the constituent algebras acting within their respective coordinates; in particular operation &lt;sup&gt;''n''&lt;/sup&gt;''f''&lt;sub&gt;''j''&lt;/sub&gt; of the product operates on ''n'' ''I''-tuples by applying operation &lt;sup&gt;''n''&lt;/sup&gt;''f''&lt;sub&gt;''j''&lt;/sub&gt; of ''B''&lt;sub&gt;''i''&lt;/sub&gt; to the ''n'' elements in the ''i''-th coordinate of the ''n'' tuples, for all ''i'' in ''I''.

When all the algebras being multiplied together in this way are the same algebra ''A'' we call the direct product a ''direct power'' of ''A''. The Boolean algebra of all 32-bit bit vectors is the two-element Boolean algebra raised to the 32nd power, or power set algebra of a 32-element set, denoted 2&lt;sup&gt;32&lt;/sup&gt;. The Boolean algebra of all sets of integers is 2&lt;sup&gt;'''Z'''&lt;/sup&gt;. All Boolean algebras we have exhibited thus far have been direct powers of the two-element Boolean algebra, justifying the name "power set algebra".

===Representation theorems===
It can be shown that every finite Boolean algebra is [[isomorphic]] to some power set algebra. Hence the cardinality (number of elements) of a finite Boolean algebra is a power of 2, namely one of 1,2,4,8,...,2&lt;sup&gt;''n''&lt;/sup&gt;,... This is called a '''representation theorem''' as it gives insight into the nature of finite Boolean algebras by giving a [[representation (mathematics)|representation]] of them as power set algebras.

This representation theorem does not extend to infinite Boolean algebras: although every power set algebra is a Boolean algebra, not every Boolean algebra need be isomorphic to a power set algebra. In particular, whereas there can be no [[countably infinite]] power set algebras (the smallest infinite power set algebra is the power set algebra 2&lt;sup&gt;''N''&lt;/sup&gt; of sets of natural numbers, [[Cantor's diagonal argument|shown]] by [[Georg Cantor|Cantor]] to be [[uncountable]]), there exist various countably infinite Boolean algebras.

To go beyond power set algebras we need another construct. A [[subalgebra]] of an algebra ''A'' is any subset of ''A'' closed under the operations of ''A''. Every subalgebra of a Boolean algebra ''A'' must still satisfy the equations holding of ''A'', since any violation would constitute a violation for ''A'' itself. Hence every subalgebra of a Boolean algebra is a Boolean algebra.

A [[subalgebra]] of a power set algebra is called a [[field of sets]]; equivalently a field of sets is a set of subsets of some set ''W'' including the empty set and ''W'' and closed under finite union and complement with respect to ''W'' (and hence also under finite intersection). Birkhoff's [1935] representation theorem for Boolean algebras states that every Boolean algebra is isomorphic to a field of sets. Now [[Birkhoff's HSP theorem]] for varieties can be stated as, every class of models of the equational theory of a class ''C'' of algebras is the Homomorphic image of a [[Subalgebra]] of a [[direct product|direct Product]] of algebras of ''C''. Normally all three of H, S, and P are needed; what the first of these two Birkhoff theorems shows is that for the special case of the variety of Boolean algebras [[Homomorphism]] can be replaced by [[Isomorphism]]. Birkhoff's HSP theorem for varieties in general therefore becomes Birkhoff's ISP theorem for the [[variety (universal algebra)|variety]] of Boolean algebras.

===Other examples===
It is convenient when talking about a set ''X'' of natural numbers to view it as a sequence ''x''&lt;sub&gt;0&lt;/sub&gt;,''x''&lt;sub&gt;1&lt;/sub&gt;,''x''&lt;sub&gt;2&lt;/sub&gt;,... of bits, with ''x''&lt;sub&gt;''i''&lt;/sub&gt;&amp;nbsp;= 1 if and only if ''i'' ∈ ''X''. This viewpoint will make it easier to talk about [[subalgebra]]s of the power set algebra 2&lt;sup&gt;''N''&lt;/sup&gt;, which this viewpoint makes the Boolean algebra of all sequences of bits. It also fits well with the columns of a truth table: when a column is read from top to bottom it constitutes a sequence of bits, but at the same time it can be viewed as the set of those valuations (assignments to variables in the left half of the table) at which the function represented by that column evaluates to 1.

'''Example 4'''. ''Ultimately constant sequences''. Any Boolean combination of ultimately constant sequences is ultimately constant; hence these form a Boolean algebra. We can identify these with the integers by viewing the ultimately-zero sequences as nonnegative binary numerals (bit 0 of the sequence being the low-order bit) and the ultimately-one sequences as negative binary numerals (think [[two's complement]] arithmetic with the all-ones sequence being −1). This makes the integers a Boolean algebra, with union being bit-wise OR and complement being ''−x−1''. There are only countably many integers, so this infinite Boolean algebra is countable. The atoms are the powers of two, namely 1,2,4,.... Another way of describing this algebra is as the set of all finite and cofinite sets of natural numbers, with the ultimately all-ones sequences corresponding to the cofinite sets, those sets omitting only finitely many natural numbers.

'''Example 5'''. ''Periodic sequence''. A sequence is called ''periodic'' when there exists some number ''n'' &amp;gt; 0, called a witness to periodicity, such that ''x''&lt;sub&gt;''i''&lt;/sub&gt;&amp;nbsp;= ''x''&lt;sub&gt;''i''+''n''&lt;/sub&gt; for all ''i'' ≥ 0. The period of a periodic sequence is its least witness. Negation leaves period unchanged, while the disjunction of two periodic sequences is periodic, with period at most the least common multiple of the periods of the two arguments (the period can be as small as 1, as happens with the union of any sequence and its complement). Hence the periodic sequences form a Boolean algebra.

Example 5 resembles Example 4 in being countable, but differs in being atomless. The latter is because the conjunction of any nonzero periodic sequence ''x'' with a sequence of greater period is neither 0 nor ''x''. It can be shown that all countably infinite atomless Boolean algebras are isomorphic, that is, up to isomorphism there is only one such algebra.

'''Example 6'''. ''Periodic sequence with period a power of two''. This is a proper [[subalgebra]] of Example 5 (a proper subalgebra equals the intersection of itself with its algebra). These can be understood as the finitary operations, with the first period of such a sequence giving the truth table of the operation it represents. For example, the truth table of ''x''&lt;sub&gt;0&lt;/sub&gt; in the table of binary operations, namely &lt;sup&gt;2&lt;/sup&gt;''f''&lt;sub&gt;10&lt;/sub&gt;, has period 2 (and so can be recognized as using only the first variable) even though 12 of the binary operations have period 4. When the period is 2&lt;sup&gt;''n''&lt;/sup&gt; the operation only depends on the first ''n'' variables, the sense in which the operation is finitary. This example is also a countably infinite atomless Boolean algebra. Hence Example 5 is isomorphic to a proper subalgebra of itself! Example 6, and hence Example 5, constitutes the free Boolean algebra on countably many generators, meaning the Boolean algebra of all finitary operations on a countably infinite set of generators or variables.

'''Example 7'''. ''Ultimately periodic sequences'', sequences that become periodic after an initial finite bout of lawlessness. They constitute a proper extension of Example 5 (meaning that Example 5 is a proper [[subalgebra]] of Example 7) and also of Example 4, since constant sequences are periodic with period one. Sequences may vary as to when they settle down, but any finite set of sequences will all eventually settle down no later than their slowest-to-settle member, whence ultimately periodic sequences are closed under all Boolean operations and so form a Boolean algebra. This example has the same atoms and coatoms as Example 4, whence it is not atomless and therefore not isomorphic to Example 5/6. However it contains an infinite atomless [[subalgebra]], namely Example 5, and so is not isomorphic to Example 4, every [[subalgebra]] of which must be a Boolean algebra of finite sets and their complements and therefore atomic. This example is isomorphic to the direct product of Examples 4 and 5, furnishing another description of it.

'''Example 8'''. The [[direct product]] of a Periodic Sequence (Example 5) with any finite but nontrivial Boolean algebra. (The trivial one-element Boolean algebra is the unique finite atomless Boolean algebra.) This resembles Example 7 in having both atoms and an atomless [[subalgebra]], but differs in having only finitely many atoms. Example 8 is in fact an infinite family of examples, one for each possible finite number of atoms.

These examples by no means exhaust the possible Boolean algebras, even the countable ones. Indeed, there are uncountably many nonisomorphic countable Boolean algebras, which Jussi Ketonen [1978] classified completely in terms of invariants representable by certain hereditarily countable sets.

==Boolean algebras of Boolean operations==
The ''n''-ary Boolean operations themselves constitute a power set algebra 2&lt;sup&gt;''W''&lt;/sup&gt;, namely when ''W'' is taken to be the set of 2&lt;sup&gt;''n''&lt;/sup&gt; valuations of the ''n'' inputs. In terms of the naming system of operations &lt;sup&gt;''n''&lt;/sup&gt;''f''&lt;sub&gt;''i''&lt;/sub&gt; where ''i'' in binary is a column of a truth table, the columns can be combined with Boolean operations of any arity to produce other columns present in the table. That is, we can apply any Boolean operation of arity ''m'' to ''m'' Boolean operations of arity ''n'' to yield a Boolean operation of arity ''n'', for any ''m'' and ''n''.

The practical significance of this convention for both software and hardware is that ''n''-ary Boolean operations can be represented as words of the appropriate length. For example, each of the 256 ternary Boolean operations can be represented as an unsigned byte. The available logical operations such as AND and OR can then be used to form new operations. If we take ''x'', ''y'', and ''z'' (dispensing with subscripted variables for now) to be 10101010, 11001100, and 11110000 respectively (170, 204, and 240 in decimal, 0xaa, 0xcc, and 0xf0 in hexadecimal), their pairwise conjunctions are ''x''∧''y''&amp;nbsp;= 10001000, ''y''∧''z''&amp;nbsp;= 11000000, and ''z''∧''x''&amp;nbsp;= 10100000, while their pairwise disjunctions are ''x''∨''y''&amp;nbsp;= 11101110, ''y''∨''z''&amp;nbsp;= 11111100, and ''z''∨''x''&amp;nbsp;= 11111010. The disjunction of the three conjunctions is 11101000, which also happens to be the conjunction of three disjunctions. We have thus calculated, with a dozen or so logical operations on bytes, that the two ternary operations
: (''x''∧''y'')∨(''y''∧''z'')∨(''z''∧''x'')

and
: (''x''∨''y'')∧(''y''∨''z'')∧(''z''∨''x'')

are actually the same operation. That is, we have proved the equational identity
: (''x''∧''y'')∨(''y''∧''z'')∨(''z''∧''x'')&amp;nbsp;= (''x''∨''y'')∧(''y''∨''z'')∧(''z''∨''x''),

for the two-element Boolean algebra. By the definition of "Boolean algebra" this identity must therefore hold in every Boolean algebra.

This ternary operation incidentally formed the basis for Grau's [1947] ternary Boolean algebras, which he axiomatized in terms of this operation and negation. The operation is symmetric, meaning that its value is independent of any of the 3!&amp;nbsp;= 6 permutations of its arguments. The two halves of its truth table 11101000 are the truth tables for ∨, 1110, and ∧, 1000, so the operation can be phrased as '''if''' ''z'' '''then''' ''x''∨''y'' '''else''' ''x''∧''y''. Since it is symmetric it can equally well be phrased as either of '''if''' ''x'' '''then''' ''y''∨''z'' '''else''' ''y''∧''z'', or '''if''' ''y'' '''then''' ''z''∨''x'' '''else''' ''z''∧''x''. Viewed as a labeling of the 8-vertex 3-cube, the upper half is labeled 1 and the lower half 0; for this reason it has been called the [[median operator]], with the evident generalization to any odd number of variables (odd in order to avoid the tie when exactly half the variables are 0).

==Axiomatizing Boolean algebras==
The technique we just used to prove an identity of Boolean algebra can be generalized to all identities in a systematic way that can be taken as a sound and complete [[axiomatization]] of, or [[axiomatic system]] for, the equational laws of [[Boolean logic]]. The customary formulation of an axiom system consists of a set of axioms that "prime the pump" with some initial identities, along with a set of inference rules for inferring the remaining identities from the axioms and previously proved identities. In principle it is desirable to have finitely many axioms; however as a practical matter it is not necessary since it is just as effective to have a finite [[axiom schema]] having infinitely many instances each of which when used in a proof can readily be verified to be a legal instance, the approach we follow here.

Boolean identities are assertions of the form ''s''&amp;nbsp;= ''t'' where ''s'' and ''t'' are ''n''-ary terms, by which we shall mean here terms whose variables are limited to ''x''&lt;sub&gt;''0''&lt;/sub&gt; through ''x''&lt;sub&gt;''n-1''&lt;/sub&gt;. An ''n''-ary '''term''' is either an atom or an application. An application &lt;sup&gt;''m''&lt;/sup&gt;''f''&lt;sub&gt;''i''&lt;/sub&gt;(''t''&lt;sub&gt;0&lt;/sub&gt;,...,''t''&lt;sub&gt;''m''-1&lt;/sub&gt;) is a pair consisting of an ''m''-ary operation &lt;sup&gt;''m''&lt;/sup&gt;''f''&lt;sub&gt;''i''&lt;/sub&gt; and a list or ''m''-tuple (''t''&lt;sub&gt;0&lt;/sub&gt;,...,''t''&lt;sub&gt;''m''-1&lt;/sub&gt;) of ''m'' ''n''-ary terms called '''operands'''.

Associated with every term is a natural number called its '''height'''. Atoms are of zero height, while applications are of height one plus the height of their highest operand.

Now what is an atom? Conventionally an atom is either a constant (0 or 1) or a variable ''x''&lt;sub&gt;''i''&lt;/sub&gt; where 0 ≤ ''i'' &amp;lt; ''n''. For the proof technique here it is convenient to define atoms instead to be ''n''-ary operations &lt;sup&gt;''n''&lt;/sup&gt;''f''&lt;sub&gt;''i''&lt;/sub&gt;, which although treated here as atoms nevertheless mean the same as ordinary terms of the exact form &lt;sup&gt;''n''&lt;/sup&gt;''f''&lt;sub&gt;''i''&lt;/sub&gt;(''x''&lt;sub&gt;0&lt;/sub&gt;,...,''x''&lt;sub&gt;''n''-1&lt;/sub&gt;) (exact in that the variables must listed in the order shown without repetition or omission). This is not a restriction because atoms of this form include all the ordinary atoms, namely the constants 0 and 1, which arise here as the ''n''-ary operations &lt;sup&gt;''n''&lt;/sup&gt;''f''&lt;sub&gt;0&lt;/sub&gt; and &lt;sup&gt;''n''&lt;/sup&gt;''f''&lt;sub&gt;−1&lt;/sub&gt; for each ''n'' (abbreviating 2&lt;sup&gt;2&lt;span&gt;&lt;sup&gt;''n''&lt;/sup&gt;&lt;/span&gt;&lt;/sup&gt;−1 to −1), and the variables ''x''&lt;sub&gt;0&lt;/sub&gt;,...,''x''&lt;sub&gt;''n''-1&lt;/sub&gt; as can be seen from the truth tables where ''x''&lt;sub&gt;0&lt;/sub&gt; appears as both the unary operation &lt;sup&gt;1&lt;/sup&gt;''f''&lt;sub&gt;2&lt;/sub&gt; and the binary operation &lt;sup&gt;2&lt;/sup&gt;''f''&lt;sub&gt;10&lt;/sub&gt; while ''x''&lt;sub&gt;1&lt;/sub&gt; appears as &lt;sup&gt;2&lt;/sup&gt;''f''&lt;sub&gt;12&lt;/sub&gt;.

The following axiom schema and three inference rules axiomatize the Boolean algebra of ''n''-ary terms.
: '''A1'''. &lt;sup&gt;''m''&lt;/sup&gt;''f''&lt;sub&gt;''i''&lt;/sub&gt;(&lt;sup&gt;''n''&lt;/sup&gt;''f''&lt;sub&gt;''j''&lt;span&gt;&lt;sub&gt;0&lt;/sub&gt;&lt;/span&gt;&lt;/sub&gt;,...,&lt;sup&gt;''n''&lt;/sup&gt;''f''&lt;sub&gt;''j''&lt;span&gt;&lt;sub&gt;''m''-1&lt;/sub&gt;&lt;/span&gt;&lt;/sub&gt;)&amp;nbsp;= &lt;sup&gt;''n''&lt;/sup&gt;''f''&lt;sub&gt;''i''o''ĵ''&lt;/sub&gt; where (''i''&lt;small&gt;o&lt;/small&gt;''ĵ'')&lt;sub&gt;''v''&lt;/sub&gt;&amp;nbsp;= ''i''&lt;sub&gt;''ĵ''&lt;span&gt;&lt;sub&gt;''v''&lt;/sub&gt;&lt;/span&gt;&lt;/sub&gt;, with ''ĵ'' being ''j'' transpose, defined by (''ĵ''&lt;sub&gt;''v''&lt;/sub&gt;)&lt;sub&gt;''u''&lt;/sub&gt;&amp;nbsp;= (''j''&lt;sub&gt;''u''&lt;/sub&gt;)&lt;sub&gt;''v''&lt;/sub&gt;.
: '''R1'''. With no premises infer ''t''&amp;nbsp;= ''t''.
: '''R2'''. From ''s''&amp;nbsp;= ''u'' and ''t''&amp;nbsp;= ''u'' infer ''s''&amp;nbsp;= ''t'' where ''s'', ''t'', and ''u'' are ''n''-ary terms.
: '''R3'''. From ''s''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;= ''t''&lt;sub&gt;0&lt;/sub&gt;,...,''s''&lt;sub&gt;''m''-1&lt;/sub&gt;&amp;nbsp;= ''t''&lt;sub&gt;''m''-1&lt;/sub&gt; infer &lt;sup&gt;''m''&lt;/sup&gt;''f''&lt;sub&gt;''i''&lt;/sub&gt;(''s''&lt;sub&gt;0&lt;/sub&gt;,...,''s''&lt;sub&gt;''m''-1&lt;/sub&gt;)&amp;nbsp;= &lt;sup&gt;''m''&lt;/sup&gt;''f''&lt;sub&gt;''i''&lt;/sub&gt;(''t''&lt;sub&gt;0&lt;/sub&gt;,...,''t''&lt;sub&gt;''m''-1&lt;/sub&gt;), where all terms ''s''&lt;sub&gt;i&lt;/sub&gt;, ''t''&lt;sub&gt;i&lt;/sub&gt; are ''n''-ary.

The meaning of the side condition on '''A1''' is that ''i''&lt;small&gt;o&lt;/small&gt;''ĵ'' is that 2&lt;sup&gt;''n''&lt;/sup&gt;-bit number whose ''v''-th bit is the ''ĵ''&lt;sub&gt;''v''&lt;/sub&gt;-th bit of ''i'', where the ranges of each quantity are ''u'': ''m'', ''v'': 2&lt;sup&gt;''n''&lt;/sup&gt;, ''j''&lt;sub&gt;''u''&lt;/sub&gt;: 2&lt;sup&gt;2&lt;span&gt;&lt;sup&gt;''n''&lt;/sup&gt;&lt;/span&gt;&lt;/sup&gt;, and ''ĵ''&lt;sub&gt;''v''&lt;/sub&gt;: 2&lt;sup&gt;''m''&lt;/sup&gt;. (So ''j'' is an ''m''-tuple of 2&lt;sup&gt;''n''&lt;/sup&gt;-bit numbers while ''ĵ'' as the transpose of ''j'' is a 2&lt;sup&gt;''n''&lt;/sup&gt;-tuple of ''m''-bit numbers. Both ''j'' and ''ĵ'' therefore contain ''m''2&lt;sup&gt;''n''&lt;/sup&gt; bits.)

'''A1''' is an axiom schema rather than an axiom by virtue of containing '''metavariables''', namely ''m'', ''i'', ''n'', and ''j&lt;sub&gt;0&lt;/sub&gt;'' through ''j&lt;sub&gt;m-1&lt;/sub&gt;''. The actual axioms of the axiomatization are obtained by setting the metavariables to specific values. For example, if we take ''m''&amp;nbsp;= ''n''&amp;nbsp;= ''i''&amp;nbsp;= ''j''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;= 1, we can compute the two bits of ''i''&lt;small&gt;o&lt;/small&gt;''ĵ'' from ''i''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;= 0 and ''i''&lt;sub&gt;0&lt;/sub&gt;&amp;nbsp;= 1, so ''i''&lt;small&gt;o&lt;/small&gt;''ĵ''&amp;nbsp;= 2 (or 10 when written as a two-bit number). The resulting instance, namely &lt;sup&gt;1&lt;/sup&gt;''f''&lt;sub&gt;1&lt;/sub&gt;(&lt;sup&gt;1&lt;/sup&gt;''f''&lt;sub&gt;1&lt;/sub&gt;)&amp;nbsp;= &lt;sup&gt;1&lt;/sup&gt;''f''&lt;sub&gt;2&lt;/sub&gt;, expresses the familiar axiom ¬¬''x''&amp;nbsp;= ''x'' of double negation. Rule '''R3''' then allows us to infer ¬¬¬''x''&amp;nbsp;= ¬''x'' by taking ''s&lt;sub&gt;0&lt;/sub&gt;'' to be &lt;sup&gt;1&lt;/sup&gt;''f''&lt;sub&gt;1&lt;/sub&gt;(&lt;sup&gt;1&lt;/sup&gt;''f''&lt;sub&gt;1&lt;/sub&gt;) or ¬¬''x''&lt;sub&gt;0&lt;/sub&gt;, ''t&lt;sub&gt;0&lt;/sub&gt;'' to be &lt;sup&gt;1&lt;/sup&gt;''f''&lt;sub&gt;2&lt;/sub&gt; or ''x''&lt;sub&gt;0&lt;/sub&gt;, and &lt;sup&gt;''m''&lt;/sup&gt;''f''&lt;sub&gt;''i''&lt;/sub&gt; to be &lt;sup&gt;''1''&lt;/sup&gt;''f''&lt;sub&gt;''1''&lt;/sub&gt; or ¬.

For each ''m'' and ''n'' there are only finitely many axioms instantiating '''A1''', namely 2&lt;sup&gt;2&lt;span&gt;&lt;sup&gt;''m''&lt;/sup&gt;&lt;/span&gt;&lt;/sup&gt; × (2&lt;sup&gt;2&lt;span&gt;&lt;sup&gt;''n''&lt;/sup&gt;&lt;/span&gt;&lt;/sup&gt;)&lt;sup&gt;''m''&lt;/sup&gt;. Each instance is specified by 2&lt;sup&gt;''m''&lt;/sup&gt;+''m''2&lt;sup&gt;''n''&lt;/sup&gt; bits.

We treat '''R1''' as an inference rule, even though it is like an axiom in having no premises, because it is a domain-independent rule along with '''R2''' and '''R3''' common to all equational axiomatizations, whether of groups, rings, or any other variety. The only entity specific to Boolean algebras is axiom schema '''A1'''. In this way when talking about different equational theories we can push the rules to one side as being independent of the particular theories, and confine attention to the axioms as the only part of the axiom system characterizing the particular equational theory at hand.

This axiomatization is complete, meaning that every Boolean law ''s''&amp;nbsp;= ''t'' is provable in this system. One first shows by induction on the height of ''s'' that every Boolean law for which ''t'' is atomic is provable, using '''R1''' for the base case (since distinct atoms are never equal) and '''A1''' and '''R3''' for the induction step (''s'' an application). This proof strategy amounts to a recursive procedure for evaluating ''s'' to yield an atom. Then to prove ''s''&amp;nbsp;= ''t'' in the general case when ''t'' may be an application, use the fact that if ''s''&amp;nbsp;= ''t'' is an identity then ''s'' and ''t'' must evaluate to the same atom, call it ''u''. So first prove ''s''&amp;nbsp;= ''u'' and ''t''&amp;nbsp;= ''u'' as above, that is, evaluate ''s'' and ''t'' using '''A1''', '''R1''', and '''R3''', and then invoke '''R2''' to infer ''s''&amp;nbsp;= ''t''.

In '''A1''', if we view the number ''n''&lt;sup&gt;''m''&lt;/sup&gt; as the function type ''m''→''n'', and ''m''&lt;sub&gt;''n''&lt;/sub&gt; as the application ''m''(''n''), we can reinterpret the numbers ''i'', ''j'', ''ĵ'', and ''i''&lt;small&gt;o&lt;/small&gt;''ĵ'' as functions of type ''i'':&amp;nbsp;(''m''→2)→2, ''j'':&amp;nbsp;''m''→((''n''→2)→2), ''ĵ'':&amp;nbsp;(''n''→2)→(''m''→2), and ''i''&lt;small&gt;o&lt;/small&gt;''ĵ'':&amp;nbsp;(''n''→2)→2. The definition (''i''&lt;small&gt;o&lt;/small&gt;''ĵ'')&lt;sub&gt;''v''&lt;/sub&gt;&amp;nbsp;= ''i''&lt;sub&gt;''ĵ''&lt;span&gt;&lt;sub&gt;''v''&lt;/sub&gt;&lt;/span&gt;&lt;/sub&gt; in '''A1''' then translates to (''i''&lt;small&gt;o&lt;/small&gt;''ĵ'')(''v'')&amp;nbsp;= ''i''(''ĵ''(''v'')), that is, ''i''&lt;small&gt;o&lt;/small&gt;''ĵ'' is defined to be composition of ''i'' and ''ĵ'' understood as functions. So the content of '''A1''' amounts to defining term application to be essentially composition, modulo the need to transpose the ''m''-tuple ''j'' to make the types match up suitably for composition. This composition is the one in Lawvere's previously mentioned category of power sets and their functions. In this way we have translated the commuting diagrams of that category, as the equational theory of Boolean algebras, into the equational consequences of '''A1''' as the logical representation of that particular composition law.

==Underlying lattice structure==
Underlying every Boolean algebra ''B'' is a [[partially ordered set]] or '''poset''' (''B'',≤). The '''partial order''' relation is defined by ''x'' ≤ ''y'' just when ''x''&amp;nbsp;= ''x''∧''y'', or equivalently when ''y''&amp;nbsp;= ''x''∨''y''. Given a set ''X'' of elements of a Boolean algebra, an '''upper bound''' on ''X'' is an element ''y'' such that for every element ''x'' of ''X'', ''x'' ≤ ''y'', while a lower bound on ''X'' is an element ''y'' such that for every element ''x'' of ''X'', ''y'' ≤ ''x''.

A '''sup''' ([[supremum]]) of ''X'' is a least upper bound on ''X'', namely an upper bound on ''X'' that is less or equal to every upper bound on ''X''. Dually an '''inf''' ([[infimum]]) of ''X'' is a greatest lower bound on ''X''. The sup of ''x'' and ''y'' always exists in the underlying poset of a Boolean algebra, being ''x''∨''y'', and likewise their inf exists, namely ''x''∧''y''. The empty sup is 0 (the bottom element) and the empty inf is 1 (top). It follows that every finite set has both a sup and an inf. Infinite subsets of a Boolean algebra may or may not have a sup and/or an inf; in a power set algebra they always do.

Any poset (''B'',≤) such that every pair ''x'',''y'' of elements has both a sup and an inf is called a '''[[lattice (order)|lattice]]'''. We write ''x''∨''y'' for the sup and ''x''∧''y'' for the inf. The underlying poset of a Boolean algebra always forms a lattice. The lattice is said to be '''distributive''' when ''x''∧(''y''∨''z'')&amp;nbsp;= (''x''∧''y'')∨(''x''∧''z''), or equivalently when ''x''∨(''y''∧''z'')&amp;nbsp;= (''x''∨''y'')∧(''x''∨''z''), since either law implies the other in a lattice. These are laws of Boolean algebra whence the underlying poset of a Boolean algebra forms a distributive lattice.

Given a lattice with a bottom element 0 and a top element 1, a pair ''x'',''y'' of elements is called '''complementary''' when ''x''∧''y''&amp;nbsp;= 0 and ''x''∨''y''&amp;nbsp;= 1, and we then say that ''y'' is a complement of ''x'' and vice versa. Any element ''x'' of a distributive lattice with top and bottom can have at most one complement. When every element of a lattice has a complement the lattice is called complemented. It follows that in a complemented distributive lattice, the complement of an element always exists and is unique, making complement a unary operation. Furthermore, every complemented distributive lattice forms a Boolean algebra, and conversely every Boolean algebra forms a complemented distributive lattice. This provides an alternative definition of a Boolean algebra, namely as any complemented distributive lattice. Each of these three properties can be axiomatized with finitely many equations, whence these equations taken together constitute a finite axiomatization of the equational theory of Boolean algebras.

In a class of algebras defined as all the models of a set of equations, it is usually the case that some algebras of the class satisfy more equations than just those needed to qualify them for the class. The class of Boolean algebras is unusual in that, with a single exception, every Boolean algebra satisfies exactly the Boolean identities and no more. The exception is the one-element Boolean algebra, which necessarily satisfies every equation, even ''x''&amp;nbsp;= ''y'', and is therefore sometimes referred to as the inconsistent Boolean algebra.

==Boolean homomorphisms==
A Boolean [[homomorphism]] is a function ''h'': ''A''→''B'' between Boolean algebras ''A'', ''B'' such that for every Boolean operation &lt;sup&gt;''m''&lt;/sup&gt;''f''&lt;sub&gt;''i''&lt;/sub&gt;,
: ''h''(&lt;sup&gt;''m''&lt;/sup&gt;''f''&lt;sub&gt;''i''&lt;/sub&gt;(''x''&lt;sub&gt;0&lt;/sub&gt;,...,''x''&lt;sub&gt;''m''−1&lt;/sub&gt;))&amp;nbsp;= &lt;sup&gt;''m''&lt;/sup&gt;''f''&lt;sub&gt;''i''&lt;/sub&gt;(''h''(''x''&lt;sub&gt;0&lt;/sub&gt;),...,''h''(''x''&lt;sub&gt;''m''−1&lt;/sub&gt;)).

The [[category (mathematics)|category]] '''Bool''' of Boolean algebras has as objects all Boolean algebras and as morphisms the Boolean homomorphisms between them.

There exists a unique homomorphism from the two-element Boolean algebra '''2''' to every Boolean algebra, since homomorphisms must preserve the two constants and those are the only elements of '''2'''. A Boolean algebra with this property is called an '''initial''' Boolean algebra. It can be shown that any two initial Boolean algebras are isomorphic, so up to isomorphism '''2''' is ''the'' initial Boolean algebra.

In the other direction, there may exist many homomorphisms from a Boolean algebra ''B'' to '''2'''. Any such homomorphism partitions ''B'' into those elements mapped to 1 and those to 0. The subset of ''B'' consisting of the former is called an [[ultrafilter]] of ''B''. When ''B'' is finite its ultrafilters pair up with its atoms; one atom is mapped to 1 and the rest to 0. Each ultrafilter of ''B'' thus consists of an atom of ''B'' and all the elements above it; hence exactly half the elements of ''B'' are in the ultrafilter, and there as many ultrafilters as atoms.

For infinite Boolean algebras the notion of ultrafilter becomes considerably more delicate. The elements greater or equal than an atom always form an ultrafilter but so do many other sets; for example in the Boolean algebra of finite and cofinite sets of integers the cofinite sets form an ultrafilter even though none of them are atoms. Likewise the powerset of the integers has among its ultrafilters the set of all subsets containing a given integer; there are countably many of these "standard" ultrafilters, which may be identified with the integers themselves, but there are uncountably many more "nonstandard" ultrafilters. These form the basis for [[nonstandard analysis]], providing representations for such classically inconsistent objects as infinitesimals and delta functions.

==Infinitary extensions==
Recall the definition of sup and inf from the section above on the underlying partial order of a Boolean algebra. A [[complete Boolean algebra]] is one every subset of which has both a sup and an inf, even the infinite subsets. Gaifman [1964] and [[Alfred W. Hales|Hales]] [1964] independently showed that infinite [[free object|free]] [[complete Boolean algebra]]s do not exist. This suggests that a logic with set-sized-infinitary operations may have class-many terms—just as a logic with finitary operations may have infinitely many terms.

There is however another approach to introducing infinitary Boolean operations: simply drop "finitary" from the definition of Boolean algebra. A model of the equational theory of the algebra of ''all'' operations on {0,1} of arity up to the cardinality of the model is called a complete atomic Boolean algebra, or ''CABA''.  (In place of this awkward restriction on arity we could allow any arity, leading to a different awkwardness, that the signature would then be larger than any set, that is, a proper class. One benefit of the latter approach is that it simplifies the definition of homomorphism between CABAs of different [[cardinality]].)  Such an algebra can be defined equivalently as a [[complete Boolean algebra]] that is '''atomic''', meaning that every element is a sup of some set of atoms. Free CABAs exist for all cardinalities of a set ''V'' of [[generating set of an algebra|generators]], namely the [[power set]] algebra 2&lt;sup&gt;2&lt;span&gt;&lt;sup&gt;''V''&lt;/sup&gt;&lt;/span&gt;&lt;/sup&gt;, this being the obvious generalization of the finite free Boolean algebras. This neatly rescues infinitary Boolean logic from the fate the Gaifman–Hales result seemed to consign it to.

The nonexistence of [[free object|free]] [[complete Boolean algebra]]s can be traced to failure to extend the equations of Boolean logic suitably to all laws that should hold for infinitary conjunction and disjunction, in particular the neglect of distributivity in the definition of complete Boolean algebra. A complete Boolean algebra is called '''completely distributive''' when arbitrary conjunctions distribute over arbitrary disjunctions and vice versa. A Boolean algebra is a CABA if and only if it is complete and completely distributive, giving a third definition of CABA. A fourth definition is as any Boolean algebra isomorphic to a power set algebra.

A complete homomorphism is one that preserves all sups that exist, not just the finite sups, and likewise for infs. The category '''CABA''' of all CABAs and their complete homomorphisms is dual to the category of sets and their functions, meaning that it is equivalent to the opposite of that category (the category resulting from reversing all morphisms). Things are not so simple for the category '''Bool''' of Boolean algebras and their homomorphisms, which [[Marshall Stone]] showed in effect (though he lacked both the language and the conceptual framework to make the duality explicit) to be dual to the category of [[totally disconnected space|totally disconnected]] [[compact Hausdorff space]]s, subsequently called [[Stone space]]s.

Another infinitary class intermediate between Boolean algebras and [[complete Boolean algebra]]s is the notion of a [[sigma-algebra]]. This is defined analogously to complete Boolean algebras, but with [[supremum|sups]] and [[infimum|infs]] limited to countable arity. That is, a [[sigma-algebra]] is a Boolean algebra with all countable sups and infs. Because the sups and infs are of bounded [[cardinality]], unlike the situation with [[complete Boolean algebra]]s, the Gaifman-Hales result does not apply and [[free object|free]] [[sigma-algebra]]s do exist. Unlike the situation with CABAs however, the free countably generated sigma algebra is not a power set algebra.

==Other definitions of Boolean algebra==
We have already encountered several definitions of Boolean algebra, as a model of the equational theory of the two-element algebra, as a complemented distributive lattice, as a Boolean ring, and as a product-preserving functor from a certain category (Lawvere). Two more definitions worth mentioning are:.

; [[Marshall Stone|Stone]] (1936): A Boolean algebra is the set of all [[clopen set]]s of a [[topological space]]. It is no limitation to require the space to be a totally disconnected compact [[Hausdorff space]], or [[Stone space]], that is, every Boolean algebra arises in this way, up to [[isomorphism]]. Moreover, if the two Boolean algebras formed as the clopen sets of two Stone spaces are isomorphic, so are the Stone spaces themselves, which is not the case for arbitrary topological spaces. This is just the reverse direction of the duality mentioned earlier from Boolean algebras to [[Stone space]]s. This definition is fleshed out by the next definition.

; Johnstone (1982): A Boolean algebra is a [[filtered colimit]] of finite Boolean algebras.

(The circularity in this definition can be removed by replacing "finite Boolean algebra" by "finite power set" equipped with the Boolean operations standardly interpreted for power sets.)

To put this in perspective, infinite sets arise as filtered colimits of finite sets, infinite CABAs as filtered limits of finite power set algebras, and infinite Stone spaces as filtered limits of finite sets. Thus if one starts with the finite sets and asks how these generalize to infinite objects, there are two ways: "adding" them gives ordinary or inductive sets while "multiplying" them gives [[Stone space]]s or [[profinite set]]s. The same choice exists for finite power set algebras as the duals of finite sets: addition yields Boolean algebras as inductive objects while multiplication yields CABAs or power set algebras as profinite objects.

A characteristic distinguishing feature is that the underlying topology of objects so constructed, when defined so as to be [[Hausdorff space|Hausdorff]], is [[discrete space|discrete]] for inductive objects and [[compact space|compact]] for profinite objects. The topology of finite Hausdorff spaces is always both discrete and compact, whereas for infinite spaces "discrete"' and "compact" are mutually exclusive. Thus when generalizing finite algebras (of any kind, not just Boolean) to infinite ones, "discrete" and "compact" part company, and one must choose which one to retain. The general rule, for both finite and infinite algebras, is that finitary algebras are discrete, whereas their duals are compact and feature infinitary operations. Between these two extremes, there are many intermediate infinite Boolean algebras whose topology is neither discrete nor compact.

==See also==
{{col-begin}}
{{col-break}}
* [[Boolean domain]]
* [[Boolean function]]
* [[Boolean-valued function]]
* [[Boolean-valued model]]
* [[Cartesian closed category]]
* [[Closed monoidal category]]
* [[Complete Boolean algebra]]
* [[Topos#Elementary topoi (topoi in logic)|Elementary topos]]
{{col-break}}
* [[Field of sets]]
* [[Filter (mathematics)]]
* [[Finitary boolean function]]
* [[Free Boolean algebra]]
* [[Functional completeness]]
* [[Ideal (order theory)]]
* [[Lattice (order)]]
* [[Lindenbaum–Tarski algebra]]
{{col-break}}
* [[Monoidal category]]
* [[Propositional calculus]]
* [[Robbins algebra]]
* [[Truth table]]
* [[Ultrafilter]]
* [[Universal algebra]]
{{col-end}}

==References==
&lt;!-- Use [[WP:CITET|templates]] for consistency --&gt;
* {{cite journal
 | last = Birkhoff
 | first = Garrett
 | authorlink = Garrett Birkhoff
 | title = On the structure of abstract algebras
 | journal = Proc. Camb. Phil. Soc.
 | volume = 31
 | pages = 433–454
 | year = 1935
 | issn = 0008-1981
 | doi=10.1017/s0305004100013463}}
* {{cite book
 | last = Boole
 | first = George
 | authorlink = George Boole
 | title = An Investigation of the Laws of Thought
 | publisher = Prometheus Books
 | origyear = 1854
 | year = 2003
 | isbn = 978-1-59102-089-9 }}
* {{cite book
 | last = Dwinger
 | first = Philip
 | title = Introduction to Boolean algebras
 | publisher = Physica Verlag
 | location = Würzburg
 | year = 1971 }}
* {{cite journal
 | last = Gaifman
 | first = Haim
 | title = Infinite Boolean Polynomials, I
 | journal = Fundamenta Mathematicae
 | volume = 54
 | pages = 229–250
 | year = 1964
 | issn = 0016-2736 }}
* {{Cite book
 | last1 = Givant
 | first1 = Steven
 | first2 = Paul
 | last2 = Halmos
 | year = 2009
 | title = Introduction to Boolean Algebras
 | series = [[Undergraduate Texts in Mathematics]]
 | publisher = Springer
 | isbn = 978-0-387-40293-2
 | postscript =}}.
* {{cite journal
 | last = Grau
 | first = A.A.
 | title = Ternary Boolean algebra
 | journal = Bull. Am. Math. Soc.
 | volume = 33
 | pages = 567–572
 | year = 1947
 | doi = 10.1090/S0002-9904-1947-08834-0
 | issue = 6 }}
* {{cite journal
 | last = Hales
 | first = Alfred W. | authorlink = Alfred W. Hales
 | title = On the Non-Existence of Free Complete Boolean Algebras
 | journal = Fundamenta Mathematicae
 | volume = 54
 | pages = 45–66
 | year = 1964
 | issn = 0016-2736 }}
* {{cite book
 | last = Halmos
 | first = Paul
 | authorlink = Paul Halmos
 | title = Lectures on Boolean Algebras
 | publisher = van Nostrand
 | year = 1963
 | isbn = 0-387-90094-2 }}
* --------, and Givant, Steven (1998) ''Logic as Algebra''. Dolciani Mathematical Exposition, No. 21. [[Mathematical Association of America]].
* {{cite book
 | last = Johnstone
 | first = Peter T.
 | title = Stone Spaces
 | authorlink = Peter Johnstone (mathematician)
 | publisher = Cambridge University Press
 | year = 1982
 | location = Cambridge, UK
 | isbn = 978-0-521-33779-3 }}
* {{cite journal
 | last = Ketonen
 | first = Jussi
 | title = The structure of countable Boolean algebras
 | jstor = 1970929
 | journal = Annals of Mathematics
 | volume = 108
 | issue = 1
 | pages = 41–89
 | year = 1978
 | doi = 10.2307/1970929 }}
* Koppelberg, Sabine (1989) "General Theory of Boolean Algebras" in Monk, J. Donald, and Bonnet, Robert, eds., ''Handbook of Boolean Algebras, Vol. 1''. North Holland. {{ISBN|978-0-444-70261-6}}.
&lt;!-- The following alternatives do not work; still searching for good template
* {{cite book
  | last = Stone
  | first = Marshall
  | chapter = General Theory of Boolean Algebras
  | title = The Theory of Representations for Boolean Algebras
  | publisher = North Holland
  | year = 1936
  | location = Amsterdam
  | isbn = 978-0-444-70261-6 --&gt; &lt;!--0-444-70261-X--&gt;
&lt;!--
* {{cite conference
 | first = Koppelberg
 | last = Sabine
 | year = 1989
 | title = General Theory of Boolean Algebras
 | booktitle = Handbook of Boolean Algebras, Vol. 1
 | editor = J. Donald Monk with Robert Bonnet
 | publisher = North Holland
 | location = Amsterdam
 | pages = 
 | isbn = 978-0-444-70261-6
}}--&gt;
* [[Charles Sanders Peirce|Peirce, C. S.]] (1989) ''Writings of Charles S. Peirce: A Chronological Edition: 1879–1884''. Kloesel, C. J. W., ed. Indianapolis: Indiana University Press. {{ISBN|978-0-253-37204-8}}.
* {{cite journal
 | last = Lawvere
 | first = F. William
 | authorlink = William Lawvere
 | title = Functorial semantics of algebraic theories
 | journal = Proceedings of the National Academy of Sciences
 | volume = 50
 | issue = 5
 | pages = 869–873
 | year = 1963
 | url = http://www.tac.mta.ca/tac/reprints/articles/5/tr5abs.html
 | doi = 10.1073/pnas.50.5.869 | pmc = 221940
 }}
* {{cite book
 | last = Schröder
 | first = Ernst
 | authorlink = Ernst Schröder
 | title = Vorlesungen über die Algebra der Logik (exakte Logik), I–III&lt;!--, III.1--&gt;
 | publisher = B.G. Teubner
 | location = Leipzig
 | date = 1890–1910&lt;!--1895--&gt;
 }}
* {{cite book
 | last = Sikorski
 | first = Roman
 | authorlink = Roman Sikorski
 | title = Boolean Algebras
 | publisher = Springer-Verlag
 | location = Berlin
 | edition = 3rd.
 | year = 1969
 | isbn = 978-0-387-04469-9 }}
* {{cite journal
 | authorlink = Marshall Harvey Stone
 | journal = Transactions of the American Mathematical Society
 | volume = 40
 | issue = 1
 | pages = 37–111
 | jstor = 1989664
 | issn = 0002-9947
 | doi = 10.2307/1989664
 | title = The Theory of Representation for Boolean Algebras
 | year = 1936
 | author = Stone, M. H. }}
* [[Alfred Tarski|Tarski, Alfred]] (1983). ''Logic, Semantics, Metamathematics'', Corcoran, J., ed. Hackett. 1956 1st edition edited and translated by J. H. Woodger, Oxford Uni. Press. Includes English translations of the following two articles:
** {{cite journal
 | last = Tarski
 | first = Alfred
 | authorlink = Alfred Tarski
 | title = Sur les classes closes par rapport à certaines opérations élémentaires
 | journal = Fundamenta Mathematicae
 | volume = 16
 | pages = 195–97
 | year = 1929
 | issn = 0016-2736 }}
** {{cite journal
 | last = Tarski
 | first = Alfred
 | authorlink = Alfred Tarski
 | title = Zur Grundlegung der Booleschen Algebra, I
 | journal = Fundamenta Mathematicae
 | volume = 24
 | pages = 177–98
 | year = 1935
 | issn = 0016-2736 }}
* {{cite book
 | last = Vladimirov
 | first = D.A.
 | title = булевы алгебры (Boolean algebras, in Russian, German translation Boolesche Algebren 1974)
  | publisher = Nauka (German translation Akademie-Verlag)
 | year = 1969 }}

[[Category:Articles with inconsistent citation formats]]
[[Category:Boolean algebra]]</text>
      <sha1>d56fxwvni0cblrjbnhobmfj0v516c6u</sha1>
    </revision>
  </page>
  <page>
    <title>Codes for electromagnetic scattering by cylinders</title>
    <ns>0</ns>
    <id>24392847</id>
    <revision>
      <id>844559538</id>
      <parentid>797660477</parentid>
      <timestamp>2018-06-05T17:49:15Z</timestamp>
      <contributor>
        <ip>168.122.8.213</ip>
      </contributor>
      <comment>/* Codes for electromagnetic scattering by a single homogeneous cylinder */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2615">'''Codes for electromagnetic scattering by cylinders''' – this article list codes for electromagnetic scattering by a cylinder.

Majority of existing codes for calculation of electromagnetic scattering by a single cylinder are based on [[Mie theory]], which is an analytical solution of [[Maxwell's equations]] in terms of infinite series.&lt;ref name=BohrenHuffman&gt;Bohren, Craig F.  and Donald R. Huffman, Title Absorption and scattering of light by small particles,  New York : Wiley, 1998, 530 p., {{ISBN|0-471-29340-7}}, {{ISBN|978-0-471-29340-8}} (second edition).&lt;/ref&gt;

==Classification==
The compilation contains information about the electromagnetic scattering by cylindrical particles, relevant links, and applications.&lt;ref&gt;T. Wreidt, Light scattering theories and computer codes, Journal of Quantitative Spectroscopy and Radiative Transfer, 110, 833–843, 2009.&lt;/ref&gt;

===Codes for electromagnetic scattering by a single homogeneous cylinder===
{| class="wikitable"
|- style="background-color: #efefef;"
! Year !! Name !! Authors !! References !! Language !! Short description
|- 
|1983
| BHCYL
| Craig F. Bohren and Donald R. Huffman
| &lt;ref name=BohrenHuffman/&gt; 
| Fortran
| Mie solution (infinite series) to scattering, absorption and phase function of electromagnetic waves by a homogeneous cylinder.

|- 
| 1992
| SCAOBLIQ2.FOR
| H. A. Yousif and E. Boutros
| &lt;ref&gt;H. A. Yousif and E. Boutros, A FORTRAN code for the scattering of EM-plane waves by an infinitely long cylinder at oblique incidence", Comput. Phys. Commun. 69, 406–414 (1992).&lt;/ref&gt;
| Fortran
| Cylinder, oblique incidence.

|- 
| 2002
| Mackowski
| D. Mackowski
| 
| Fortran
| Cylinder, oblique incidence.

|- 
| 2008
| [http://www.thecomputationalphysicist.com jMie2D]
| Jeffrey M. McMahon
| 
| C++
| Mie solution. Open-source software.

|- 
| 2015
| [https://nanohub.org/tools/nwabsorption nwabsorption]
| Sarath Ramadurgam
| 
| MATLAB
| Computes various optical properties of a single nanowire with up to 2 shell layers using Mie-formalism.

|}

==Relevant scattering codes==
* [[Discrete dipole approximation codes]]
* [[Codes for electromagnetic scattering by spheres]]

==See also==
* [[Computational electromagnetics]]
* [[List of atmospheric radiative transfer codes]]

==External links==
*[http://scatterlib.wikidot.com SCATTERLIB: Collection of light scattering codes]

==References==
&lt;references/&gt;

{{DEFAULTSORT:Codes for electromagnetic scattering by spheres}}
[[Category:Science-related lists]]
[[Category:Computational science]]
[[Category:Scattering, absorption and radiative transfer codes]]
[[Category:Scattering]]</text>
      <sha1>e0px6v33ag3swlfjw7pfwsbvsm2ifpx</sha1>
    </revision>
  </page>
  <page>
    <title>Composite number</title>
    <ns>0</ns>
    <id>82289</id>
    <revision>
      <id>870971345</id>
      <parentid>870971314</parentid>
      <timestamp>2018-11-28T03:30:51Z</timestamp>
      <contributor>
        <username>Donner60</username>
        <id>12744454</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/2001:569:7D23:8C00:C906:EFF3:1830:9787|2001:569:7D23:8C00:C906:EFF3:1830:9787]] ([[User talk:2001:569:7D23:8C00:C906:EFF3:1830:9787|talk]]) ([[WP:HG|HG]]) (3.4.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6339">[[File:Composite number Cuisenaire rods 10.png|thumb|Demonstration, with [[Cuisenaire rod]]s, of the divisors of the composite number 10]]
[[File:Primes-vs-composites.svg | thumb | right | Comparison of prime and composite numbers]]

A '''composite number''' is a [[positive integer]] that can be formed by multiplying two smaller positive integers. Equivalently, it is a positive integer that has at least one [[divisor]] other than 1 and itself.&lt;ref&gt;{{harvtxt|Pettofrezzo|Byrkit|1970|pp=23–24}}&lt;/ref&gt;&lt;ref&gt;{{harvtxt|Long|1972|p=16}}&lt;/ref&gt; Every positive integer is composite, [[prime number|prime]], or the [[Unit (ring theory)|unit]]&amp;nbsp;1, so the composite numbers are exactly the numbers that are not prime and not a unit.&lt;ref&gt;{{harvtxt|Fraleigh|1976|pp=198,266}}&lt;/ref&gt;&lt;ref&gt;{{harvtxt|Herstein|1964|p=106}}&lt;/ref&gt;

For example, the integer [[14 (number)|14]] is a composite number because it is the product of the two smaller integers [[2 (number)|2]]&amp;nbsp;&amp;times;&amp;nbsp;[[7 (number)|7]]. Likewise, the integers 2 and 3 are not composite numbers because each of them can only be divided by one and itself.

The composite numbers up to 150 are
:4, 6, 8, 9, 10, 12, 14, 15, 16, 18, 20, 21, 22, 24, 25, 26, 27, 28, 30, 32, 33, 34, 35, 36, 38, 39, 40, 42, 44, 45, 46, 48, 49, 50, 51, 52, 54, 55, 56, 57, 58, 60, 62, 63, 64, 65, 66, 68, 69, 70, 72, 74, 75, 76, 77, 78, 80, 81, 82, 84, 85, 86, 87, 88, 90, 91, 92, 93, 94, 95, 96, 98, 99, 100, 102, 104, 105, 106, 108, 110, 111, 112, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 128, 129, 130, 132, 133, 134, 135, 136, 138, 140, 141, 142, 143, 144, 145, 146, 147, 148, 150. {{OEIS|id=A002808}}

Every composite number can be written as the product of two or more (not necessarily distinct) primes.&lt;ref&gt;{{harvtxt|Long|1972|p=16}}&lt;/ref&gt; For example, the composite number [[299 (number)|299]] can be written as 13 × 23, and the composite number [[360 (number)|360]] can be written as 2&lt;sup&gt;3&lt;/sup&gt; × 3&lt;sup&gt;2&lt;/sup&gt; × 5; furthermore, this representation is unique up to the order of the factors. This fact is called the [[fundamental theorem of arithmetic]].&lt;ref&gt;{{harvtxt|Fraleigh|1976|p=270}}&lt;/ref&gt;&lt;ref&gt;{{harvtxt|Long|1972|p=44}}&lt;/ref&gt;&lt;ref&gt;{{harvtxt|McCoy|1968|p=85}}&lt;/ref&gt;&lt;ref&gt;{{harvtxt|Pettofrezzo|Byrkit|1970|p=53}}&lt;/ref&gt;

There are several known [[primality test]]s that can determine whether a number is prime or composite, without necessarily revealing the factorization of a composite input.

==Types==

One way to classify composite numbers is by counting the number of prime factors. A composite number with two prime factors is a [[semiprime]] or 2-almost prime (the factors need not be distinct, hence squares of primes are included). A composite number with three distinct prime factors is a [[sphenic number]]. In some applications, it is necessary to differentiate between composite numbers with an odd number of distinct prime factors and those with an even number of distinct prime factors. For the latter 
:&lt;math&gt;\mu(n) = (-1)^{2x} = 1&lt;/math&gt;

(where μ is the [[Möbius function]] and ''x'' is half the total of prime factors), while for the former

:&lt;math&gt;\mu(n) = (-1)^{2x + 1} = -1.&lt;/math&gt;

However, for prime numbers, the function also returns −1 and &lt;math&gt;\mu(1) = 1&lt;/math&gt;. For a number ''n'' with one or more repeated prime factors,

:&lt;math&gt;\mu(n) = 0&lt;/math&gt;.&lt;ref&gt;{{harvtxt|Long|1972|p=159}}&lt;/ref&gt;

If ''all'' the prime factors of a number are repeated it is called a [[powerful number]] (All [[perfect power]]s are powerful numbers). If ''none'' of its prime factors are repeated, it is called [[Square-free integer|squarefree]]. (All prime numbers and 1 are squarefree.)

For example, [[72 (number)|72]] = 2&lt;sup&gt;3&lt;/sup&gt; × 3&lt;sup&gt;2&lt;/sup&gt;, all the prime factors are repeated, so 72 is a powerful number. [[42 (number)|42]] = 2 × 3 × 7, none of the prime factors are repeated, so 42 is squarefree.

Another way to classify composite numbers is by counting the number of divisors. All composite numbers have at least three divisors. In the case of squares of primes, those divisors are &lt;math&gt;\{1, p, p^2\}&lt;/math&gt;. A number ''n'' that has more divisors than any ''x'' &lt; ''n'' is a [[highly composite number]] (though the first two such numbers are 1 and 2).

Composite numbers have also been called "rectangular numbers", but that name can also refer to the [[pronic number]]s, numbers that are the product of two consecutive integers.

Yet another way to classify composite numbers is to determine whether all prime factors are either all below or all above some fixed (prime) number. Such numbers are called [[smooth number]]s and [[rough number]]s, respectively.

==See also==
* [[Table of prime factors]]
* [[Integer factorization]]
* [[Canonical representation of a positive integer]]
* [[Sieve of Eratosthenes]]

==Notes==
{{reflist}}

==References==
* {{ citation | first1 = John B. | last1 = Fraleigh | year = 1976 | isbn = 0-201-01984-1 | title = A First Course In Abstract Algebra | edition = 2nd | publisher = [[Addison-Wesley]] | location = Reading }}
* {{ citation | first1 = I. N. | last1 = Herstein | author-link=Israel Nathan Herstein | year = 1964 | isbn = 978-1114541016 | title = Topics In Algebra | publisher = [[Blaisdell Publishing Company]] | location = Waltham }}
* {{ citation | first1 = Calvin T. | last1 = Long | year = 1972 | title = Elementary Introduction to Number Theory | edition = 2nd | publisher = [[D. C. Heath and Company]] | location = Lexington | lccn = 77-171950 }}
* {{ citation | first1 = Neal H. | last1 = McCoy | year = 1968 | title = Introduction To Modern Algebra, Revised Edition | publisher = [[Allyn and Bacon]] | location = Boston | lccn = 68-15225 }}
* {{ citation | first1 = Anthony J. | last1 = Pettofrezzo | first2 = Donald R. | last2 = Byrkit | year = 1970 | title = Elements of Number Theory | publisher = [[Prentice Hall]] | location = Englewood Cliffs | lccn = 77-81766 }}

== External links ==
* [http://naturalnumbers.org/composites.html Lists of composites with prime factorization (first 100, 1,000, 10,000, 100,000, and 1,000,000)]
* [http://www.divisorplot.com/index.html Divisor Plot (patterns found in large composite numbers)]

{{Divisor classes}}

[[Category:Prime numbers| Composite]]
[[Category:Integer sequences]]
[[Category:Arithmetic]]
[[Category:Elementary number theory]]</text>
      <sha1>gzc8yq4gh1jcxyr539uzb53hhp1tmjt</sha1>
    </revision>
  </page>
  <page>
    <title>Connection (algebraic framework)</title>
    <ns>0</ns>
    <id>24628685</id>
    <revision>
      <id>822457151</id>
      <parentid>786997881</parentid>
      <timestamp>2018-01-26T13:53:16Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v481)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4753">Geometry of [[Quantum mechanics|quantum systems]] (e.g.,
[[noncommutative geometry]] and [[supergeometry]]) is mainly
phrased in algebraic terms of [[module (mathematics)|modules]] and
[[algebras]]. '''Connections''' on modules are
generalization of a linear [[connection (vector bundle)|connection]] on a smooth [[vector bundle]] &lt;math&gt;E\to
X&lt;/math&gt; written as a [[Koszul connection]] on the
&lt;math&gt;C^\infty(X)&lt;/math&gt;-module of sections of &lt;math&gt;E\to
X&lt;/math&gt;.&lt;ref&gt;Koszul (1950)&lt;/ref&gt;

== Commutative algebra ==

Let &lt;math&gt;A&lt;/math&gt; be a commutative [[ring (mathematics)|ring]]
and &lt;math&gt;P&lt;/math&gt; an ''A''-[[module (mathematics)|module]]. There are different equivalent definitions
of a connection on &lt;math&gt;P&lt;/math&gt;.&lt;ref&gt;Koszul (1950), Mangiarotti
(2000)&lt;/ref&gt; Let &lt;math&gt;D(A)&lt;/math&gt; be the module of [[derivation (abstract algebra)|derivations]] of a ring &lt;math&gt;A&lt;/math&gt;. A
connection on an ''A''-module &lt;math&gt;P&lt;/math&gt; is defined
as an ''A''-module morphism

: &lt;math&gt; \nabla:D(A)\ni u\to \nabla_u\in \mathrm{Diff}_1(P,P)&lt;/math&gt;

such that the first order [[differential calculus over commutative algebras|differential operator]]s &lt;math&gt;\nabla_u&lt;/math&gt; on
&lt;math&gt;P&lt;/math&gt; obey the Leibniz rule

: &lt;math&gt;\nabla_u(ap)=u(a)p+a\nabla_u(p), \quad a\in A, \quad p\in
P.&lt;/math&gt;

Connections on a module over a commutative ring always exist.

The curvature of the connection &lt;math&gt;\nabla&lt;/math&gt; is defined as
the zero-order differential operator

: &lt;math&gt;R(u,u')=[\nabla_u,\nabla_{u'}]-\nabla_{[u,u']} \, &lt;/math&gt;

on the module &lt;math&gt;P&lt;/math&gt; for all &lt;math&gt;u,u'\in D(A)&lt;/math&gt;.

If &lt;math&gt;E\to X&lt;/math&gt; is a vector bundle, there is one-to-one
correspondence between [[connection (vector bundle)|linear
connections]] &lt;math&gt;\Gamma&lt;/math&gt; on &lt;math&gt;E\to X&lt;/math&gt; and the
connections &lt;math&gt;\nabla&lt;/math&gt; on the
&lt;math&gt;C^\infty(X)&lt;/math&gt;-module of sections of &lt;math&gt;E\to
X&lt;/math&gt;. Strictly speaking, &lt;math&gt;\nabla&lt;/math&gt; corresponds to
the [[covariant derivative|covariant differential]] of a
connection on &lt;math&gt;E\to X&lt;/math&gt;.

== Graded commutative algebra ==

The notion of a connection on modules over commutative rings is
straightforwardly extended to modules over a [[superalgebra|graded
commutative algebra]].&lt;ref&gt;Bartocci (1991), Mangiarotti
(2000)&lt;/ref&gt; This is the case of
[[supergeometry|superconnections]] in [[supergeometry]] of
[[graded manifold]]s and [[supergeometry|supervector bundles]].
Superconnections always exist.

== Noncommutative algebra ==

If &lt;math&gt;A&lt;/math&gt; is a noncommutative ring, connections on left
and right ''A''-modules are defined similarly to those on
modules over commutative rings.&lt;ref&gt;Landi (1997)&lt;/ref&gt; However
these connections need not exist.

In contrast with connections on left and right modules, there is a
problem how to define a connection on an
''R''-''S''-[[bimodule]] over noncommutative rings
''R'' and ''S''. There are different definitions
of such a connection.&lt;ref&gt;Dubois-Violette
(1996), Landi (1997)&lt;/ref&gt; Let us mention one of them. A connection on an
''R''-''S''-bimodule &lt;math&gt;P&lt;/math&gt; is defined as a bimodule
morphism

: &lt;math&gt; \nabla:D(A)\ni u\to \nabla_u\in \mathrm{Diff}_1(P,P)&lt;/math&gt;

which obeys the Leibniz rule

: &lt;math&gt;\nabla_u(apb)=u(a)pb+a\nabla_u(p)b +apu(b), \quad a\in R,
\quad b\in S, \quad p\in P.&lt;/math&gt;

== See also ==

*[[Connection (vector bundle)]] 
*[[Connection (mathematics)]]
*[[Noncommutative geometry]]
*[[Supergeometry]] 
*[[Differential calculus over commutative algebras]]

== Notes ==

{{reflist}}

== References ==

* Koszul, J., Homologie et cohomologie des algebres de Lie,''Bulletin de la Societe Mathematique'' '''78''' (1950) 65
* Koszul, J., ''Lectures on Fibre Bundles and Differential Geometry'' (Tata University, Bombay, 1960)
* Bartocci, C., Bruzzo, U., Hernandez Ruiperez, D., ''The Geometry of Supermanifolds'' (Kluwer Academic Publ., 1991) {{isbn|0-7923-1440-9}}
* Dubois-Violette, M., Michor, P., Connections on central bimodules in noncommutative differential geometry, ''J. Geom. Phys.'' '''20''' (1996) 218. [https://arxiv.org/abs/q-alg/9503020 arXiv:q-alg/9503020v2]
* Landi, G., ''An Introduction to Noncommutative Spaces and their Geometries'', Lect. Notes Physics, New series m: Monographs, '''51''' (Springer, 1997) ArXiv [https://arxiv.org/abs/hep-th/9701078 eprint], iv+181 pages.
* Mangiarotti, L., [[Gennadi Sardanashvily|Sardanashvily, G.]], ''Connections in Classical and Quantum Field Theory'' (World Scientific, 2000) {{isbn|981-02-2013-8}}

== External links ==
* [[Gennadi Sardanashvily|Sardanashvily, G.]], ''Lectures on Differential Geometry of Modules and Rings'' (Lambert Academic Publishing, Saarbrücken, 2012); [http://xxx.lanl.gov/abs/0910.1515 arXiv: 0910.1515]

[[Category:Connection (mathematics)]]
[[Category:Noncommutative geometry]]</text>
      <sha1>n692lu1gtq6k9j12nhw2v9q1dccvfkf</sha1>
    </revision>
  </page>
  <page>
    <title>Convex set</title>
    <ns>0</ns>
    <id>6292</id>
    <revision>
      <id>863047897</id>
      <parentid>863047875</parentid>
      <timestamp>2018-10-08T11:14:02Z</timestamp>
      <contributor>
        <ip>150.214.60.163</ip>
      </contributor>
      <comment>/* Star-convex (star-shaped) sets */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19725">{{merge from|Complex convexity|discuss=Talk:Convex set#Proposed merge with Complex convexity|date=September 2018}}
[[File:Convex polygon illustration1.svg|right|thumb|Illustration of a convex set which looks somewhat like a deformed circle. The (black) line segment joining points x and y lies completely within the (green) set. Since this is true for any points x and y within the set that we might choose, the set is convex.]]
[[File:Convex polygon illustration2.svg|right|thumb|Illustration of a non-convex set. Since the red part of the (black and red) line-segment joining the points x and y lies ''outside'' of the (green) set, the set is non-convex.]]

In [[convex geometry]], a '''convex set''' is a subset of an [[affine space]] that is [[Closure (mathematics)|closed]] under [[convex combination]]s.&lt;ref&gt;{{cite book|last1=Bachem|first1=Achim|last2=Kern|first2=Walter|title=Linear Programming Duality: An Introduction to Oriented Matroids|publisher=Springer Science &amp; Business Media|isbn=9783642581526|page=13|url=https://books.google.com/books?id=w10OBwAAQBAJ&amp;lpg=PA13&amp;pg=PA13#v=onepage&amp;q=convex%20set&amp;f=false|accessdate=5 April 2017|language=en}}&lt;/ref&gt; More specifically, in a [[Euclidean space]], a '''convex region''' is a [[Region (mathematics)|region]] where, for every pair of points within the region, every point on the [[straight line]] segment that joins the pair of points is also within the region.&lt;ref&gt;{{cite book|last1=Morris|first1=Carla C.|last2=Stark|first2=Robert M.|title=Finite Mathematics: Models and Applications|publisher=John Wiley &amp; Sons|isbn=9781119015383|page=121|url=https://books.google.com/books?id=ZgJyCgAAQBAJ&amp;lpg=PA121&amp;pg=PA121#v=onepage&amp;q=convex%20region&amp;f=false|accessdate=5 April 2017|language=en}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last1=Kjeldsen|first1=Tinne Hoff|title=History of Convexity and Mathematical Programming|journal=Proceedings of the International Congress of Mathematicians|issue=ICM 2010|pages=3233–3257|doi=10.1142/9789814324359_0187|url=http://www.mathunion.org/ICM/ICM2010.4/Main/icm2010.4.3233.3257.pdf|accessdate=5 April 2017}}&lt;/ref&gt; For example, a solid [[cube (geometry)|cube]] is a convex set, but anything that is hollow or has an indent, for example, a [[crescent]] shape, is not convex.

The [[boundary (topology)|boundary]] of a convex set is always a [[convex curve]]. The intersection of all convex sets containing a given subset {{mvar|A}} of Euclidean space is called the [[convex hull]] of {{mvar|A}}. It is the smallest convex set containing {{mvar|A}}.

A [[convex function]] is a [[real-valued function]] defined on an [[interval (mathematics)|interval]] with the property that its [[epigraph (mathematics)|epigraph]] (the set of points on or above the [[graph of a function|graph]] of the function) is a convex set. [[Convex minimization]] is a subfield of [[mathematical optimization|optimization]] that studies the problem of minimizing convex functions over convex sets. The branch of mathematics devoted to the study of properties of convex sets and convex functions is called [[convex analysis]].

The notion of a convex set can be generalized as described below.

== In vector spaces ==
[[File:Convex supergraph.svg|right|thumb|A [[convex function|function]] is convex if and only if its [[Epigraph (mathematics)|epigraph]], the region (in green) above its [[graph of a function|graph]] (in blue), is a convex set.]]
Let {{mvar|S}} be a [[vector space]] over the [[real number]]s, or, more generally, over some [[ordered field]]. This includes Euclidean spaces. A [[set (mathematics)|set]] {{mvar|C}} in {{mvar|S}} is said to be '''convex''' if, for all {{mvar|x}} and {{mvar|y}} in {{mvar|C}} and all {{mvar|t}} in the [[interval (mathematics)|interval]] {{math|(0, 1)}}, the point {{math|(1 − ''t'')''x'' + ''ty''}} also belongs to {{mvar|C}}. In other words, every point on the [[line segment]] connecting {{mvar|x}} and {{mvar|y}} is in {{mvar|C}}. This implies that a convex set in a [[real number|real]] or [[complex number|complex]] [[topological vector space]] is [[path-connected]], thus [[connected space|connected]].
Furthermore, {{mvar|C}} is strictly convex if every point on the line segment connecting {{mvar|x}} and {{mvar|y}} other than the endpoints is inside the [[Interior (topology)|interior]] of {{mvar|C}}.

A set {{mvar|C}} is called [[absolutely convex]] if it is convex and [[balanced set|balanced]].

The convex [[subset]]s of {{math|'''R'''}} (the set of real numbers) are the intervals of {{math|'''R'''}}. Some examples of convex subsets of the [[Euclidean space|Euclidean plane]] are solid [[regular polygon]]s, solid triangles, and intersections of solid triangles. Some examples of convex subsets of a [[Euclidean space|Euclidean 3-dimensional space]] are the [[Archimedean solid]]s and the [[Platonic solid]]s. The [[Kepler-Poinsot polyhedra]] are examples of non-convex sets.

=== Non-convex set ===
:''"Concave set" redirects here.
A set that is not convex is called a ''non-convex set''. A [[polygon]] that is not a [[convex polygon]] is sometimes called a [[concave polygon]],&lt;ref&gt;{{citation |first=Jeffrey J. |last=McConnell |year=2006 |title=Computer Graphics: Theory Into Practice |isbn=0-7637-2250-2 |page=130}}.&lt;/ref&gt; and some sources more generally use the term ''concave set'' to mean a non-convex set,&lt;ref&gt;{{MathWorld|title=Concave|id=Concave}}&lt;/ref&gt; but most authorities prohibit this usage.&lt;ref&gt;{{citation|title=Analytical Methods in Economics|first=Akira|last=Takayama|publisher=University of Michigan Press|year=1994|isbn=9780472081356|url=https://books.google.com/books?id=_WmZA0MPlmEC&amp;pg=PA54|page=54|quote=An often seen confusion is a "concave set". Concave and convex functions designate certain classes of functions, not of sets, whereas a convex set designates a certain class of sets, and not a class of functions. A "concave set" confuses sets with functions.}}&lt;/ref&gt;&lt;ref&gt;{{citation|title=An Introduction to Mathematical Analysis for Economic Theory and Econometrics|first1=Dean|last1=Corbae|first2=Maxwell B.|last2=Stinchcombe|first3= Juraj|last3=Zeman|publisher=Princeton University Press|year=2009|isbn=9781400833085|url=https://books.google.com/books?id=j5P83LtzVO8C&amp;pg=PT347|page=347|quote=There is no such thing as a concave set.}}&lt;/ref&gt;

The [[Complement (set theory)|complement]] of a convex set, such as the [[epigraph (mathematics)|epigraph]] of a [[concave function]], is sometimes called a ''reverse convex set'', especially in the context of [[mathematical optimization]].&lt;ref&gt;{{citation | last = Meyer | first = Robert | journal = SIAM Journal on Control and Optimization | mr = 0312915 | pages = 41–54 | title = The validity of a family of optimization methods | volume = 8 | year = 1970}}.&lt;/ref&gt;

== Properties ==
If {{mvar|S}} is a convex set in {{mvar|n}}-dimensional space, then for any collection of {{mvar|r}}, {{math|''r'' &gt; 1}}, {{mvar|n}}-dimensional vectors {{math|''u''&lt;sub&gt;1&lt;/sub&gt;, ..., ''u&lt;sub&gt;r&lt;/sub&gt;''}} in {{mvar|S}}, and for any [[negative number|nonnegative number]]s {{math|''λ''&lt;sub&gt;1&lt;/sub&gt;, ..., ''λ&lt;sub&gt;r&lt;/sub&gt;''}} such that {{math|''λ''&lt;sub&gt;1&lt;/sub&gt; + ... + ''λ&lt;sub&gt;r&lt;/sub&gt;'' {{=}} 1}}, then one has:
:&lt;math&gt;\sum_{k=1}^r\lambda_k u_k \in S.&lt;/math&gt;
A vector of this type is known as a [[convex combination]] of {{math|''u''&lt;sub&gt;1&lt;/sub&gt;, ..., ''u&lt;sub&gt;r&lt;/sub&gt;''}}.

=== Intersections and unions ===
The collection of convex subsets of a vector space has the following properties:&lt;ref name="Soltan" &gt;Soltan, Valeriu, ''Introduction to the Axiomatic Theory of Convexity'', Ştiinţa, [[Chişinău]], 1984 (in Russian).
&lt;/ref&gt;&lt;ref name="Singer" &gt;{{cite book|last=Singer|first=Ivan|title=Abstract convex analysis|series=Canadian Mathematical Society series of monographs and advanced texts|publisher=John Wiley&amp;nbsp;&amp;&amp;nbsp;Sons, Inc.|location=New&amp;nbsp;York|year= 1997|pages=xxii+491|isbn=0-471-16015-6|mr=1461544}}&lt;/ref&gt;
#The [[empty set]] and the whole vector-space are convex.
#The intersection of any collection of convex sets is convex.
#The ''[[union (sets)|union]]'' of a [[Total order#Chains|non-decreasing]] [[net (mathematics)|sequence]] of convex subsets is a convex set. For the preceding property of unions of non-decreasing sequences of convex sets, the restriction to nested sets is important: The union of two convex sets need ''not'' be convex.

=== Closed convex sets ===
[[closed set|Closed]] convex sets are convex sets that contain all their [[limit points]]. They can be characterised as the intersections of ''closed [[Half-space (geometry)|half-space]]s'' (sets of point in space that lie on and to one side of a [[hyperplane]]).

From what has just been said, it is clear that such intersections are convex, and they will also be closed sets. To prove the converse, i.e., every convex set may be represented as such intersection, one needs the [[supporting hyperplane theorem]] in the form that for a given closed convex set {{mvar|C}} and point {{mvar|P}} outside it, there is a closed half-space {{mvar|H}} that contains {{mvar|C}} and not {{mvar|P}}. The supporting hyperplane theorem is a special case of the [[Hahn–Banach theorem]] of [[functional analysis]].

=== Convex sets and rectangles ===
Let ''C'' be a convex body in the plane. We can inscribe a rectangle ''r'' in ''C'' such that a [[Homothetic transformation|homothetic]] copy ''R'' of ''r'' is circumscribed about ''C''. The positive homothety ratio is at most 2 and:&lt;ref&gt;{{Cite journal | doi = 10.1007/BF01263495| title = Approximation of convex bodies by rectangles| journal = Geometriae Dedicata| volume = 47| pages = 111| year = 1993| last1 = Lassak | first1 = M. }}&lt;/ref&gt;
:&lt;math&gt;\tfrac{1}{2} \cdot\text{Area}(R) \leq \text{Area}(C) \leq 2\cdot \text{Area}(r)&lt;/math&gt;

== Convex hulls and Minkowski sums ==

=== Convex hulls ===
{{Main|convex hull}}
Every subset {{mvar|A}} of the vector space is contained within a smallest convex set (called the [[convex hull]] of {{mvar|A}}), namely the intersection of all convex sets containing {{mvar|A}}. The convex-hull operator Conv() has the characteristic properties of a [[closure operator|hull operator]]:
:{| border="0"
|-
| ''extensive''
| {{math|''S''&amp;nbsp;⊆&amp;nbsp;Conv(''S'')}},
|-
| ''[[Monotone function#Monotonicity in order theory|non-decreasing]]''
| {{math|''S''&amp;nbsp;⊆&amp;nbsp;''T''}} implies that {{math|Conv(''S'')&amp;nbsp;⊆&amp;nbsp;Conv(''T'')}}, and
|-
| ''[[idempotence|idempotent]]''
| {{math|Conv(Conv(''S'')) {{=}} Conv(''S'')}}.
|}
The convex-hull operation is needed for the set of convex sets to form a &lt;!-- complete  --&gt;[[lattice (order)|lattice]], in which the [[join and meet|"''join''" operation]] is the convex hull of the union of two convex sets
: {{math|Conv(''S'') ∨ Conv(''T'') {{=}} Conv(''S''&amp;nbsp;∪&amp;nbsp;''T'') {{=}} Conv(Conv(''S'')&amp;nbsp;∪&amp;nbsp;Conv(''T''))}}.
The intersection of any collection of convex sets is itself convex, so the convex subsets of a (real or complex) vector space form a complete [[lattice (order)|lattice]].

=== Minkowski addition ===
{{Main|Minkowski addition}}
[[File:Minkowski sum graph - vector version.svg|thumb|alt=Three squares are shown in the nonnegative quadrant of the Cartesian plane. The square {{math|''Q''&lt;sub&gt;1&lt;/sub&gt; {{=}} [0, 1] × [0, 1]}} is green. The square {{math|''Q''&lt;sub&gt;2&lt;/sub&gt; {{=}} [1, 2] × [1, 2]}} is brown, and it sits inside the turquoise square {{math|1=Q&lt;sub&gt;1&lt;/sub&gt;+Q&lt;sub&gt;2&lt;/sub&gt;{{=}}[1,3]×[1,3]}}.|[[Minkowski addition]] of sets. The &lt;!-- [[Minkowski addition|Minkowski]]&amp;nbsp; --&gt;[[sumset|sum]] of the squares&amp;nbsp;Q&lt;sub&gt;1&lt;/sub&gt;=[0,1]&lt;sup&gt;2&lt;/sup&gt; and&amp;nbsp;Q&lt;sub&gt;2&lt;/sub&gt;=[1,2]&lt;sup&gt;2&lt;/sup&gt; is the square&amp;nbsp;Q&lt;sub&gt;1&lt;/sub&gt;+Q&lt;sub&gt;2&lt;/sub&gt;=[1,3]&lt;sup&gt;2&lt;/sup&gt;.]]

In a real vector-space, the ''[[Minkowski addition|Minkowski sum]]'' of two (non-empty) sets, {{math|''S''&lt;sub&gt;1&lt;/sub&gt;}} and {{math|''S''&lt;sub&gt;2&lt;/sub&gt;}}, is defined to be the [[sumset|set]] {{math|''S''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;''S''&lt;sub&gt;2&lt;/sub&gt;}} formed by the addition of vectors element-wise from the summand-sets
:{{math|''S''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;''S''&lt;sub&gt;2&lt;/sub&gt; {{=}} {''x''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;''x''&lt;sub&gt;2&lt;/sub&gt; : ''x''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;∈&amp;nbsp;''S''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;∈&amp;nbsp;''S''&lt;sub&gt;2&lt;/sub&gt;} }}.
More generally, the ''Minkowski sum'' of a finite family of (non-empty) sets {{math|''S&lt;sub&gt;n&lt;/sub&gt;''}} is &lt;!-- defined to be --&gt; the set &lt;!-- of vectors --&gt; formed by element-wise addition of vectors&lt;!--  from the summand-sets --&gt;
:&lt;math&gt; \sum_n S_n = \left \{ \sum_n x_n : x_n \in S_n \right \}.&lt;/math&gt;

For Minkowski&amp;nbsp;addition, the ''zero set''&amp;nbsp;{{math|{0} }} containing only the [[null vector|zero&amp;nbsp;vector]]&amp;nbsp;{{math|0}} has [[identity element|special importance]]: For every non-empty subset&amp;nbsp;S of a vector space
:{{math|''S''&amp;nbsp;+&amp;nbsp;{0} {{=}} ''S''}};
in algebraic terminology, {{math|{0} }} is the [[identity element]] of Minkowski addition (on the collection of non-empty sets).&lt;ref&gt;The [[empty set]] is important in Minkowski addition, because the empty set annihilates every other subset: For every subset {{mvar|S}} of a vector space, its sum with the empty set is empty: {{math|''S'' + ∅ {{=}} ∅}}.&lt;/ref&gt;

=== Convex hulls of Minkowski sums ===
Minkowski addition behaves well with respect to the operation of taking convex hulls, as shown by the following proposition:

Let {{math|''S''&lt;sub&gt;1&lt;/sub&gt;, ''S''&lt;sub&gt;2&lt;/sub&gt;}} be subsets of a real vector-space, the [[convex hull]] of their Minkowski sum is the Minkowski sum of their convex hulls
:{{math|Conv(''S''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;''S''&lt;sub&gt;2&lt;/sub&gt;) {{=}} Conv(''S''&lt;sub&gt;1&lt;/sub&gt;) + Conv(''S''&lt;sub&gt;2&lt;/sub&gt;)}}.

This result holds more generally for each finite collection of non-empty sets:

:&lt;math&gt;\text{Conv}\left ( \sum_n S_n \right ) = \sum_n \text{Conv} \left (S_n \right).&lt;/math&gt;

In mathematical terminology, the [[operation (mathematics)|operation]]s of Minkowski summation and of forming [[convex hull]]s are [[commutativity|commuting]] operations.&lt;ref&gt;Theorem&amp;nbsp;3 (pages&amp;nbsp;562–563): {{cite news|first1=M.|last1=Krein|authorlink1=Mark Krein|first2=V.|last2=Šmulian|year=1940|title=On regularly convex sets in the space conjugate to a Banach space|journal=Annals of Mathematics |series=Second Series| volume=41 |pages=556–583|jstor=1968735|doi=10.2307/1968735}}&lt;/ref&gt;&lt;ref name="Schneider"&gt;For the commutativity of [[Minkowski sum|Minkowski addition]] and [[convex hull|convexification]], see Theorem&amp;nbsp;1.1.2 (pages&amp;nbsp;2–3) in Schneider; this reference discusses much of the literature on the [[convex hull]]s of [[Minkowski addition|Minkowski]] [[sumset]]s in its "Chapter&amp;nbsp;3 Minkowski addition" (pages&amp;nbsp;126–196): {{cite book|last=Schneider|first=Rolf|title=Convex bodies: The Brunn–Minkowski theory|series=Encyclopedia of mathematics and its applications|volume=44|publisher=Cambridge&amp;nbsp;University Press| location=Cambridge | year=1993| pages=xiv+490 |isbn=0-521-35220-7|mr=1216521}}&lt;/ref&gt;

=== Minkowski sums of convex sets ===
The Minkowski sum of two compact convex sets is compact. The sum of a compact convex set and a closed convex set is closed.&lt;ref&gt;Lemma&amp;nbsp;5.3: {{cite book|first1=C.D.|last1= Aliprantis|first2=K.C.| last2=Border|title=Infinite Dimensional Analysis, A Hitchhiker's Guide| publisher=Springer| location=Berlin|year=2006|isbn=978-3-540-29587-7}}&lt;/ref&gt;

== Generalizations and extensions for convexity ==
The notion of convexity in the Euclidean space may be generalized by modifying the definition in some or other aspects. The common name "generalized convexity" is used, because the resulting objects retain certain properties of convex sets.

=== Star-convex (Star-shaped) sets ===
{{main|Star domain}}
Let {{mvar|C}} be a set in a real or complex vector space. {{mvar|C}} is '''star convex (star-shaped)''' if there exists an {{math|''x''&lt;sub&gt;0&lt;/sub&gt;}} in {{mvar|C}} such that the line segment from {{math|''x''&lt;sub&gt;0&lt;/sub&gt;}} to any point {{mvar|y}} in {{mvar|C}} is contained in {{mvar|C}}. Hence a non-empty convex set is always star-convex but a star-convex set is not always convex.

=== Orthogonal convexity ===
{{main|Orthogonal convex hull}}
An example of generalized convexity is '''orthogonal convexity'''.&lt;ref&gt;Rawlins G.J.E. and Wood D, "Ortho-convexity and its generalizations",  in: ''Computational Morphology'', 137-152. [[Elsevier]], 1988.&lt;/ref&gt;

A set {{mvar|S}} in the Euclidean space is called '''orthogonally convex''' or '''ortho-convex''', if any segment parallel to any of the coordinate axes connecting two points of {{mvar|S}} lies totally within {{mvar|S}}. It is easy to prove that an intersection of any collection of orthoconvex sets is orthoconvex. Some other properties of convex sets are  valid as well.

=== Non-Euclidean geometry ===
The definition of a convex set and a convex hull extends naturally to geometries which are not Euclidean by defining a [[geodesic convexity|geodesically convex set]] to be one that contains the [[geodesic]]s joining any two points in the set.

=== Order topology ===
Convexity can be extended for a space {{mvar|X}} endowed with the [[order topology]], using the [[total order]] {{math|&lt;}} of the space.&lt;ref&gt;[[James Munkres|Munkres, James]]; ''Topology'', Prentice Hall; 2nd edition (December 28, 1999). {{ISBN|0-13-181629-2}}.&lt;/ref&gt;

Let {{math|''Y'' ⊆ ''X''}}. The subspace {{mvar|Y}} is a convex set if for each pair of points {{math|''a'', ''b''}} in {{mvar|Y}} such that {{math|''a'' &lt; ''b''}}, the interval {{math|(''a'', ''b'') {{=}} {''x'' ∈ ''X'' : ''a'' &lt; ''x'' &lt; ''b''} }} is contained in {{mvar|Y}}. That is, {{mvar|Y}} is convex if and only if for all {{math|''a'', ''b''}} in {{mvar|Y}}, {{math|''a'' &lt; ''b''}} implies {{math|(''a'', ''b'') ⊆ ''Y''}}.

=== Convexity spaces ===
The notion of convexity may be generalised to other objects, if certain properties of convexity are selected as [[axiom]]s.

Given a set {{mvar|X}}, a '''convexity''' over {{mvar|X}} is a collection {{math|''𝒞''}} of subsets of {{mvar|X}} satisfying the following axioms:&lt;ref name="Soltan"/&gt;&lt;ref name="Singer"/&gt;&lt;ref name="vanDeVel" &gt;{{cite book|last=van De Vel|first=Marcel L. J.|title=Theory of convex structures|series=North-Holland Mathematical Library|publisher=North-Holland Publishing Co.|location=Amsterdam|year= 1993|pages=xvi+540|isbn=0-444-81505-8|mr=1234493}}&lt;/ref&gt;

#The empty set and {{mvar|X}} are in {{math|''𝒞''}}
#The intersection of any collection from {{math|''𝒞''}} is in {{math|''𝒞''}}.
#The union of a [[Total order|chain]] (with respect to the [[inclusion relation]]) of elements of {{math|''𝒞''}} is in {{math|''𝒞''}}.

The elements of {{math|''𝒞''}} are called convex sets and the pair {{math|(''X'', ''𝒞'')}} is called a '''convexity space'''. For the ordinary convexity, the first two axioms hold, and the third one is trivial.

For an alternative definition of abstract convexity, more suited to [[discrete geometry]], see the ''convex geometries'' associated with [[antimatroid]]s.

== See also ==
{{Div col|colwidth=25em}}
* [[Brouwer fixed-point theorem]]
* [[Complex convexity]]
* [[Convex body]]
* [[Convex metric space]]
* [[Carathéodory's theorem (convex hull)]]
* [[Choquet theory]]
* [[Helly's theorem]]
* [[Holomorphically convex hull]]
* [[Pseudoconvexity]]
* [[Radon's theorem]]
* [[Shapley–Folkman lemma]]
{{Div col end}}

== References ==
{{reflist|30em}}

== External links ==
{{Wiktionary}}
* {{springer|title=Convex subset|id=p/c026380}}
* [http://www.fmf.uni-lj.si/~lavric/lauritzen.pdf Lectures on Convex Sets], notes by Niels Lauritzen, at [[Aarhus University]], March 2010.

{{Functional Analysis}}

{{Authority control}}

{{DEFAULTSORT:Convex Set}}
[[Category:Convex geometry]]
[[Category:Convex analysis]]</text>
      <sha1>5ucgog84bwxjeiakuorfebmrfnsvz3f</sha1>
    </revision>
  </page>
  <page>
    <title>Covariant classical field theory</title>
    <ns>0</ns>
    <id>3687308</id>
    <revision>
      <id>822419952</id>
      <parentid>783598494</parentid>
      <timestamp>2018-01-26T07:51:54Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v481)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2296">In [[mathematical physics]], '''covariant classical field theory''' represents [[classical field theory|classical field]]s by [[Section (fiber bundle)|section]]s of [[fiber bundle]]s, and their dynamics is phrased in the context of a [[finite-dimensional]] space of [[field (physics)|fields]]. Nowadays, it is well known that{{cn|date=February 2016}} [[jet bundle]]s and the [[variational bicomplex]] are the correct domain for such a description. The Hamiltonian variant of covariant classical field theory is the [[covariant Hamiltonian field theory]] where momenta correspond to derivatives of field variables with respect to all world coordinates. [[Non-autonomous mechanics]] is formulated as covariant classical field theory on [[fiber bundles]] over the time axis ℝ.

==See also==
*[[Classical field theory]]
*[[Exterior algebra]]
*[[Lagrangian system]]
*[[Variational bicomplex]]
*[[Quantum field theory]]
*[[Non-autonomous mechanics]]
*[[Higgs field (classical)]]

==References==
* Saunders, D.J., "The Geometry of Jet Bundles", Cambridge University Press, 1989, {{ISBN|0-521-36948-7}}
* Bocharov, A.V. [et al.] "Symmetries and conservation laws for differential equations of mathematical physics", Amer. Math. Soc., Providence, RI, 1999, {{ISBN|0-8218-0958-X}}  
* De Leon, M., Rodrigues, P.R., "Generalized Classical Mechanics and Field Theory", Elsevier Science Publishing, 1985, {{ISBN|0-444-87753-3}}
* Griffiths, P.A., "Exterior Differential Systems and the Calculus of Variations", Boston: Birkhäuser, 1983, {{ISBN|3-7643-3103-8}}
* Gotay, M.J., Isenberg, J., Marsden, J.E., Montgomery R., ''[https://arxiv.org/pdf/physics/9801019 Momentum Maps and Classical Fields Part I: Covariant Field Theory]'', November 2003
* Echeverria-Enriquez, A., Munoz-Lecanda, M.C., Roman-Roy,M., ''[https://arxiv.org/abs/dg-ga/9505004 Geometry of Lagrangian First-order Classical Field Theories]'', May 1995
* Giachetta, G., Mangiarotti, L., [[Gennadi Sardanashvily|Sardanashvily, G.]], "Advanced Classical Field Theory", World Scientific, 2009, {{ISBN|978-981-283-895-7}} ([http://xxx.lanl.gov/abs/0811.0331 arXiv: 0811.0331v2]) 

[[Category:Differential topology]]
[[Category:Differential equations]]
[[Category:Fiber bundles]]
[[Category:Theoretical physics]]
[[Category:Lagrangian mechanics]]</text>
      <sha1>i9knqsq2cs9ul26zzj6odbrgym9x31t</sha1>
    </revision>
  </page>
  <page>
    <title>Credibility theory</title>
    <ns>0</ns>
    <id>4152503</id>
    <revision>
      <id>855463666</id>
      <parentid>845375562</parentid>
      <timestamp>2018-08-18T13:50:04Z</timestamp>
      <contributor>
        <ip>197.101.7.70</ip>
      </contributor>
      <comment>I changed the introduction to include an example and give readers a better intuition on what the topic is about. I also added a document to the further reading section that has helped me better understand the topic.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10913">{{Technical|date=January 2012}}
{{About|Credibility theory in actuary science|credibility in general|Credibility|application of the concept in [[institutional theory]]|Credibility thesis}}

'''Credibility theory''' is a form of statistical inference used to forecast an uncertain future event developed by [[Thomas Bayes]]. It may be used when you have multiple estimates of a future event, and you would like to combine these estimates in such a way to get a more accurate and relevant estimate. This is typically used by [[Actuary|actuaries]] working for insurance companies when determining the premium values. For example, in [[Group insurance|group health insurance]] an insurer is interested in calculating the risk premium, &lt;math&gt;RP&lt;/math&gt;, (i.e. the theoretical expected claims amount) for a particular employer in the coming year. The insurer will likely have an estimate of historic overall claims experience, &lt;math&gt;x&lt;/math&gt;, as well as a more specific estimate for the employer in question, &lt;math&gt;y&lt;/math&gt;. Assigning a credibility factor, &lt;math&gt;z&lt;/math&gt;, to the overall claims experience (and the reciprocal to employer experience) allows the insurer to get a more accurate estimate of the risk premium in the following manner:  

&lt;math display="block"&gt;RP = xz + y(1-z).&lt;/math&gt;The credibility factor is derived by calculating the [[Maximum likelihood estimates|maximum likelihood estimate]] which would minimise the error of estimate. Assuming the variance of &lt;math&gt;x&lt;/math&gt;and &lt;math&gt;y&lt;/math&gt;are known quantities taking on the values &lt;math&gt;u&lt;/math&gt;and &lt;math&gt;v&lt;/math&gt;respectively, it can be shown that &lt;math&gt;z&lt;/math&gt;should be equal to:  

&lt;math display="block"&gt;z = v / (u+v).&lt;/math&gt;Therefore, the more uncertainty the estimate has, the lower is its credibility.  

==Types of Credibility==
In Bayesian credibility, we separate each class (B) and assign them a probability (Probability of B).  Then we find how likely our experience (A) is within each class (Probability of A given B). Next, we find how likely our experience was over all classes (Probability of A).  Finally, we can find the probability of our class given our experience.  So going back to each class, we weight each statistic with the probability of the particular class given the experience.

Bühlmann credibility works by looking at the Variance across the population.  More specifically, it looks to see how much of the Total Variance is attributed to the Variance of the Expect Values of each class (Variance of the Hypothetical Mean), and how much is attributed to the Expected Variance over all classes (Expected Value of the Process Variance).  Say we have a basketball team with a high number of points per game.  Sometimes they get 128 and other times they get 130 but always one of the two.  Compared to all basketball teams this is a relatively low variance, meaning that they will contribute very little to the Expect Value of the Process Variance.  Also, their unusually high point totals greatly increases the variance of the population, meaning that if the league booted them out, they'd have a much more predictable point total for each team (lower variance). So, this team is definitely unique (they contribute greatly to the Variance of the Hypothetical Mean).  So we can rate this team's experience with a fairly high credibility.  They often/always score a lot (low Expected Value of Process Variance) and not many teams score as much as them (high Variance of Hypothetical Mean).

==A simple example==

Suppose there are two coins in a box.  One has heads on both sides and the other is a normal coin with 50:50 likelihood of heads or tails.  You need to place a wager on the outcome after one is randomly drawn and flipped.

The odds of heads is .5 * 1 + .5 * .5 = .75.  This is because there is a .5 chance of selecting the heads-only coin with 100% chance of heads and .5 chance of the fair coin with 50% chance.

Now the same coin is reused and you are asked to bet on the outcome again.

If the first flip was tails, there is a 100% chance you are dealing with a fair coin, so the next flip has a 50% chance of heads and 50% chance of tails.

If the first flip was heads, we must calculate the conditional probability that the chosen coin was heads-only as well as the conditional probability that the coin was fair, after which we can calculate the conditional probability of heads on the next flip. The probability that it came from a heads-only coin given that the first flip was heads is the probability of selecting a heads-only coin times the probability of heads for that coin divided by the initial probability of heads on the first flip, or .5 * 1 / .75 = 2/3.  The probability that it came from a fair coin given that the first flip was heads is the probability of selecting a fair coin times the probability of heads for that coin divided by the initial probability of heads on the first flip, or .5 * .5 / .75 = 1/3.  Finally, The conditional probability of heads on the next flip given that the first flip was heads is the conditional probability of a heads-only coin times the probability of heads for a heads-only coin plus the conditional probability of a fair coin times the probability of heads for a fair coin, or 2/3 * 1 + 1/3 * .5 = 5/6 ≈ .8333

==Actuarial credibility==

'''Actuarial credibility''' describes an approach used by [[actuary|actuaries]] to improve [[statistical]] estimates. Although the approach can be formulated in either a [[frequentist]] or [[Bayesian statistics|Bayesian]] statistical setting, the latter is often preferred because of the ease of recognizing more than one source of randomness through both "sampling" and "prior" information. In a typical application, the actuary has an estimate X based on a small set of data, and an estimate M based on a larger but less relevant set of data. The credibility estimate is ZX + (1-Z)M,&lt;ref&gt;{{cite web|url=http://w4.stern.nyu.edu/emplibrary/Klugman |title= A brief introduction to credibility theory and an example featuring race-based insurance premiums}}&lt;/ref&gt; where Z is a number between 0 and 1 (called the "credibility weight" or "credibility factor") calculated to balance the [[sampling error]] of X against the possible lack of relevance (and therefore modeling error) of M.

When an [[insurance]] company calculates the premium it will charge, it divides the policy holders into groups. For example, it might divide motorists by age, sex, and type of car; a young man driving a fast car being considered a high risk, and an old woman driving a small car being considered a low risk. The division is made balancing the two requirements that the risks in each group are sufficiently similar and the group sufficiently large that a [[Statistical significance|meaningful statistical]] analysis of the claims experience can be done to calculate the premium. This compromise means that none of the groups contains only identical risks. The problem is then to devise a way of combining the experience of the group with the experience of the individual risk to calculate the premium better. Credibility theory provides a solution to this problem.

For [[Actuary|actuaries]], it is important to know credibility theory in order to calculate a premium for a group of [[insurance contract]]s. The goal is to set up an experience rating system to determine next year's premium, taking into account not only the individual experience with the group, but also the collective experience.

There are two extreme positions. One is to charge everyone the same premium estimated by the overall mean &lt;math&gt;\overline{X}&lt;/math&gt; of the data. This makes sense only if the portfolio is homogeneous, which means that all risks cells have identical mean claims. However, if the portfolio is heterogeneous, it is not a good idea to charge a premium in this way (overcharging "good" people and undercharging "bad" risk people) since the "good" risks will take their business elsewhere, leaving the insurer with only "bad" risks. This is an example of [[adverse selection]].

The other way around is to charge to group &lt;math&gt;j&lt;/math&gt;  its own average claims, being &lt;math&gt;\overline{X_j}&lt;/math&gt; as premium charged to the insured. These methods are used if the portfolio is heterogeneous, provided a fairly large claim experience. To compromise these two extreme positions, we take the [[Weighted mean|weighted average]] of the two extremes:

: &lt;math&gt;C = z_j\overline{X_j} + (1 - z_j) \overline{X}\,&lt;/math&gt;

&lt;math&gt;z_j&lt;/math&gt; has the following intuitive meaning: it expresses how ''"credible"'' (acceptability) the individual of cell &lt;math&gt;j&lt;/math&gt; is. If it is high, then use higher &lt;math&gt;z_j&lt;/math&gt; to attach a larger weight to charging the &lt;math&gt;\overline{X_j}&lt;/math&gt;, and in this case, &lt;math&gt;z_j&lt;/math&gt; is called a credibility factor, and such a premium charged is called a credibility premium.

If the group were completely homogeneous then it would be reasonable to set &lt;math&gt;z_j=0&lt;/math&gt;, while if the group were completely heterogeneous then it would be reasonable to set &lt;math&gt;z_j=1&lt;/math&gt;.  Using intermediate values is reasonable to the extent that both individual and group history is useful in inferring future individual behavior.

For example, an actuary has an accident and payroll historical data for a shoe factory suggesting a rate of 3.1 accidents per million dollars of payroll. She has industry statistics (based on all shoe factories) suggesting that the rate is 7.4 accidents per million. With a credibility, Z, of 30%, she would estimate the rate for the factory as 30%(3.1) + 70%(7.4) = 6.1 accidents per million.

==References==
{{more footnotes|date=January 2012}}
{{reflist}}

==Further reading==
*Behan, Donald F. (2009) [http://www.actuary.com/seac/handouts/200906_03c_Credibility_Theory.pdf "Statistical Credibility Theory"], Southeastern Actuarial Conference, June 18, 2009
*Longley-Cook, L.H. (1962) An introduction to credibility theory PCAS, 49, 194-221.
*{{cite book |last1= Mahler|first1= Howard C. |last2= Dean|first2= Curtis Gary|year= 2001 |chapter= Chapter 8: Credibility |chapterurl= http://people.stat.sfu.ca/~cltsai/ACMA315/Ch8_Credibility.pdf |chapter-format= PDF|editor1-last= [[Casualty Actuarial Society]] |title= Foundations of Casualty Actuarial Science|edition= 4th |publisher= [[Casualty Actuarial Society]] |pages= 485–659 |isbn= 978-0-96247-622-8|access-date= June 25, 2015}}
*Whitney, A.W. (1918) The Theory of Experience Rating, Proceedings of the Casualty Actuarial Society, 4, 274-292 (This is one of the original casualty actuarial papers dealing with credibility. It uses Bayesian techniques, although the author uses the now archaic "inverse probability" terminology.)
*Venter, Gary G. (2005) "[https://www.casact.org/pubs/forum/03wforum/03wf621.pdf Credibility Theory for Dummies]"

{{DEFAULTSORT:Credibility Theory}}
[[Category:Actuarial science]]
[[Category:Credit risk]]</text>
      <sha1>06daeo549p7tlhbk3rpi1hpmufvo0w1</sha1>
    </revision>
  </page>
  <page>
    <title>Drinfeld upper half plane</title>
    <ns>0</ns>
    <id>35205725</id>
    <revision>
      <id>635261316</id>
      <parentid>585990492</parentid>
      <timestamp>2014-11-24T17:15:11Z</timestamp>
      <contributor>
        <username>K9re11</username>
        <id>19647483</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1531">In [[mathematics]], the '''Drinfeld upper half plane''' is a [[rigid analytic space]] analogous to the usual [[upper half plane]] for function fields, introduced by {{harvs|txt|authorlink=Vladimir Drinfeld|last=Drinfeld|year=1976}}. 
It is defined to be '''P'''&lt;sup&gt;1&lt;/sup&gt;('''C''')\'''P'''&lt;sup&gt;1&lt;/sup&gt;('''F'''&lt;sub&gt;∞&lt;/sub&gt;), where '''F''' is a function field of a curve over a [[finite field]], '''F'''&lt;sub&gt;∞&lt;/sub&gt; its completion at ∞, and '''C''' the completion of the [[algebraic closure]] of '''F'''&lt;sub&gt;∞&lt;/sub&gt;.

The analogy with the usual upper half plane arises from the fact that the [[global function field]] '''F''' is analogous to the rational numbers '''Q'''. Then, '''F'''&lt;sub&gt;∞&lt;/sub&gt; is the real numbers '''R''' and the algebraic closure of '''F'''&lt;sub&gt;∞&lt;/sub&gt; is the complex numbers '''C''' (which are already complete). Finally, '''P'''&lt;sup&gt;1&lt;/sup&gt;('''C''') is the [[Riemann sphere]], so '''P'''&lt;sup&gt;1&lt;/sup&gt;('''C''')\'''P'''&lt;sup&gt;1&lt;/sup&gt;('''R''') is the upper half plane ''together with'' the lower half plane.

==References==

*{{Citation | last1=Drinfeld | first1=V. G. | title=Coverings of p-adic symmetric domains | mr=0422290  | year=1976 | journal=Akademija Nauk SSSR. Funkcional'nyi Analiz i ego Priloženija | issn=0374-1990 | volume=10 | issue=2 | pages=29–40}}
*{{Citation | last1=Genestier | first1=Alain | title=Espaces symétriques de Drinfeld | mr=1393015  | year=1996 | journal=Astérisque | issn=0303-1179 | issue=234 | pages=124}}

[[Category:Automorphic forms]]


{{mathanalysis-stub}}</text>
      <sha1>3yj5weyv9t3495hvb0qiz8h6xe79iki</sha1>
    </revision>
  </page>
  <page>
    <title>Enrico Arbarello</title>
    <ns>0</ns>
    <id>28094731</id>
    <revision>
      <id>857359518</id>
      <parentid>830995480</parentid>
      <timestamp>2018-08-31T05:22:36Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>Copying from [[Category:20th-century Italian mathematicians]] to [[Category:Italian mathematicians]] using [[c:Help:Cat-a-lot|Cat-a-lot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2076">{{BLP sources|date=August 2010}}
{{Infobox scientist
| name              = Enrico Arbarello
| image             = &lt;!--(filename only)--&gt;
| image_size        = 
| caption           = 
| birth_date        = 1945
| birth_place       = 
| death_date        = 
| death_place       = 
| nationality       = [[Italy]]
| fields            = [[Mathematics]]
| workplaces        = [[Sapienza University of Rome|University of Rome]]&lt;br&gt;[[Scuola Normale Superiore di Pisa]]
| alma_mater        = [[Columbia University]]
| doctoral_advisor  = [[Lipman Bers]]&lt;br&gt;[[Herbert Clemens]]
| doctoral_students = 
| known_for         = 
| awards            = 
}}
'''Enrico Arbarello''' is an [[Italy|Italian]] [[mathematician]] who is a leading expert in [[algebraic geometry]].

He earned a [[Ph.D.]] at [[Columbia University]] in New York in 1973. He was a visiting scholar at the [[Institute for Advanced Study]] from 1993-94.&lt;ref&gt;[http://www.ias.edu/people/cos/frontpage?page=4 Institute for Advanced Study: A Community of Scholars] {{webarchive|url=https://web.archive.org/web/20130106144338/http://www.ias.edu/people/cos/frontpage?page=4 |date=2013-01-06 }}&lt;/ref&gt;  He is now a Mathematics Professor at [[Sapienza University of Rome]].
In 2012 he became a fellow of the [[American Mathematical Society]].&lt;ref&gt;[http://www.ams.org/profession/fellows-list List of Fellows of the American Mathematical Society], retrieved 2012-11-03.&lt;/ref&gt;

==References==
{{reflist}}

== External links ==
*[http://www.mat.uniroma1.it/people/arbarello/ University site]
*{{MathGenealogy |id=19992 }}

{{authority control}}
{{DEFAULTSORT:Arbarello, Enrico}}
[[Category:Living people]]
[[Category:Columbia University alumni]]
[[Category:Institute for Advanced Study visiting scholars]]
[[Category:20th-century Italian mathematicians]]
[[Category:Italian mathematicians]]
[[Category:21st-century Italian mathematicians]]
[[Category:Fellows of the American Mathematical Society]]
[[Category:1945 births]]
[[Category:Algebraic geometers]]
[[Category:Sapienza University of Rome faculty]]


{{Italy-mathematician-stub}}</text>
      <sha1>keupr6izmri4nu6k08al4dfcnguw8cg</sha1>
    </revision>
  </page>
  <page>
    <title>Examples of vector spaces</title>
    <ns>0</ns>
    <id>1463006</id>
    <revision>
      <id>864661446</id>
      <parentid>864660272</parentid>
      <timestamp>2018-10-18T17:12:19Z</timestamp>
      <contributor>
        <ip>208.66.211.213</ip>
      </contributor>
      <comment>/* Finite vector spaces */ projective geometry is great, but anyone who does it knows what a finite vector space is already.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14051">{{Unreferenced|date=November 2009}}
This page lists some '''examples of vector spaces'''. See [[vector space]] for the definitions of terms used on this page. See also: [[dimension (vector space)|dimension]], [[basis (linear algebra)|basis]].

''Notation''. We will let '''F''' denote an arbitrary [[field (mathematics)|field]] such as the [[real number]]s '''R''' or the [[complex number]]s '''C'''. See also: [[table of mathematical symbols]].

==Trivial or zero vector space==
{{main|Zero vector space}}
The simplest example of a vector space is the trivial one: {'''0'''}, which contains only the zero vector (see axiom 3 of [[vector space]]s). Both vector addition and scalar multiplication are trivial. A [[basis (linear algebra)|basis]] for this vector space is the [[empty set]], so that {0} is the 0-dimensional vector space over '''F'''. Every vector space over '''F''' contains a subspace [[Isomorphism|isomorphic]] to this one.

The zero vector space is different from the [[null space]] of a linear operator F, which is the [[Kernel (linear algebra)|kernel]] of F.

==Field==
The next simplest example is the field '''F''' itself. Vector addition is just field addition, and scalar multiplication is just field multiplication. This property can be used to prove that a field is a vector space. Any non-zero element of '''F''' serves as a basis so '''F''' is a 1-dimensional vector space over itself.

The field is a rather special vector space; in fact it is the simplest example of a '''[[algebra over a field|commutative algebra]]''' over '''F'''.  Also, '''F''' has just two [[Linear subspace|subspaces]]: {0} and '''F''' itself.

==Coordinate space==
{{Main|Coordinate space}}
The original example of a vector space, which the axiomatic definition generalizes, is the following. For any [[Positive number|positive]] [[integer]] ''n'', the set of all ''n''-tuples of elements of '''F''' forms an ''n''-dimensional vector space over '''F''' sometimes called ''[[coordinate space]]'' and denoted '''F'''&lt;sup&gt;''n''&lt;/sup&gt;. An element of '''F'''&lt;sup&gt;''n''&lt;/sup&gt; is written
:&lt;math&gt;x = (x_1, x_2, \ldots, x_n) &lt;/math&gt;
where each ''x''&lt;sub&gt;''i''&lt;/sub&gt; is an element of '''F'''. The operations on '''F'''&lt;sup&gt;''n''&lt;/sup&gt; are defined by
:&lt;math&gt;x + y = (x_1 + y_1, x_2 + y_2, \ldots, x_n + y_n) &lt;/math&gt;
:&lt;math&gt;\alpha x = (\alpha x_1, \alpha x_2, \ldots, \alpha x_n) &lt;/math&gt;
:&lt;math&gt;0 = (0, 0, \ldots, 0) &lt;/math&gt;
:&lt;math&gt;-x = (-x_1, -x_2, \ldots, -x_n) &lt;/math&gt;
Commonly, '''F''' is the field of [[real number]]s, in which case we obtain [[real coordinate space]] '''R'''&lt;sup&gt;''n''&lt;/sup&gt;. The field of [[complex number]]s gives [[complex coordinate space]] '''C'''&lt;sup&gt;''n''&lt;/sup&gt;. The ''a + bi'' form of a complex number shows that '''C''' itself is a two-dimensional real vector space with coordinates  (''a'',''b''). Similarly, the [[quaternion]]s and the [[octonion]]s are respectively four- and eight-dimensional real vector spaces, and '''C'''&lt;sup&gt;''n''&lt;/sup&gt; is a ''2n''-dimensional real vector space.

The vector space '''F'''&lt;sup&gt;''n''&lt;/sup&gt; has a [[standard basis]]:
:&lt;math&gt;e_1 = (1, 0, \ldots, 0) &lt;/math&gt;
:&lt;math&gt;e_2 = (0, 1, \ldots, 0) &lt;/math&gt;
:&lt;math&gt;\vdots &lt;/math&gt;
:&lt;math&gt;e_n = (0, 0, \ldots, 1) &lt;/math&gt;
where 1 denotes the multiplicative identity in '''F'''.

==Infinite coordinate space==
Let '''F'''&lt;sup&gt;∞&lt;/sup&gt; denote the space of [[infinite sequence]]s of elements from '''F''' such that only ''finitely'' many elements are nonzero. That is, if we write an element of '''F'''&lt;sup&gt;∞&lt;/sup&gt; as
:&lt;math&gt;x = (x_1, x_2, x_3, \ldots) &lt;/math&gt;
then only a finite number of the ''x''&lt;sub&gt;''i''&lt;/sub&gt; are nonzero (i.e., the coordinates become all zero after a certain point). Addition and scalar multiplication are given as in finite coordinate space. The dimensionality of '''F'''&lt;sup&gt;∞&lt;/sup&gt; is [[countably infinite]]. A standard basis consists of the vectors ''e''&lt;sub&gt;''i''&lt;/sub&gt; which contain a 1 in the ''i''-th slot and zeros elsewhere.  This vector space is the [[coproduct]] (or [[direct sum of modules|direct sum]]) of countably many copies of the vector space '''F'''.

Note the role of the finiteness condition here. One could consider arbitrary sequences of elements in '''F''', which also constitute a vector space with the same operations, often denoted by '''F'''&lt;sup&gt;'''N'''&lt;/sup&gt; - see [[Examples of vector spaces#Function spaces|below]].  '''F'''&lt;sup&gt;'''N'''&lt;/sup&gt; is the ''[[product (category theory)|product]]'' of countably many copies of '''F'''.

By Zorn's lemma, '''F'''&lt;sup&gt;'''N'''&lt;/sup&gt; has a basis (there is no obvious basis).   There are [[uncountably infinite]] elements in the basis.  Since the dimensions are different,  '''F'''&lt;sup&gt;'''N'''&lt;/sup&gt; is ''not'' isomorphic to '''F'''&lt;sup&gt;∞&lt;/sup&gt;.  It is worth noting that '''F'''&lt;sup&gt;'''N'''&lt;/sup&gt; is (isomorphic to) the [[dual space]] of '''F'''&lt;sup&gt;∞&lt;/sup&gt;, because a linear map ''T'' from '''F'''&lt;sup&gt;∞&lt;/sup&gt; to '''F''' is determined uniquely by its values ''T''(''e&lt;sub&gt;i&lt;/sub&gt;'') on the basis elements of  '''F'''&lt;sup&gt;∞&lt;/sup&gt;, and these values can be arbitrary. Thus one sees that a vector space need not be isomorphic to its dual if it is infinite dimensional, in contrast to the finite dimensional case.

==Product of vector spaces==
Starting from ''n'' vector spaces, or a countably infinite collection of them, each with the same field, we can define the product space like above.

==Matrices==
Let '''F'''&lt;sup&gt;''m''×''n''&lt;/sup&gt; denote the set of ''m''×''n'' [[matrix (mathematics)|matrices]] with entries in '''F'''. Then '''F'''&lt;sup&gt;''m''×''n''&lt;/sup&gt; is a vector space over '''F'''. Vector addition is just matrix addition and scalar multiplication is defined in the obvious way (by multiplying each entry by the same scalar). The zero vector is just the [[zero matrix]]. The [[dimension (vector space)|dimension]] of '''F'''&lt;sup&gt;''m''×''n''&lt;/sup&gt; is ''mn''. One possible choice of basis is the matrices with a single entry equal to 1 and all other entries 0.

When ''m'' = ''n'' the [[square matrix|matrix is square]] and [[matrix multiplication]] of two such matrices produces a third. This vector space of dimension ''n''&lt;sup&gt;2&lt;/sup&gt; forms an [[algebra over a field]].

==Polynomial vector spaces==
===One variable===

The set of [[polynomial]]s with coefficients in '''F''' is a vector space over '''F''', denoted '''F'''[''x'']. Vector addition and scalar multiplication are defined in the obvious manner. If the [[Degree of a polynomial|degree of the polynomials]] is unrestricted then the dimension of '''F'''[''x''] is [[countably infinite]]. If instead one restricts to polynomials with degree less than or equal to ''n'', then we have a vector space with dimension ''n''&amp;nbsp;+&amp;nbsp;1.

One possible basis for '''F'''[''x''] is a [[monomial basis]]: the coordinates of a polynomial with respect to this basis are its [[coefficient]]s, and the map sending a polynomial to the sequence of its coefficients is a [[linear isomorphism]] from '''F'''[''x''] to the infinite coordinate space '''F'''&lt;sup&gt;∞&lt;/sup&gt;.

The vector space of polynomials with real coefficients and degree less than or equal to ''n'' is denoted by '''P'''&lt;sub&gt;''n''&lt;/sub&gt;.

===Several variables===
The set of [[polynomial]]s in several variables with coefficients in '''F''' is vector space over '''F''' denoted '''F'''[''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, …, ''x''&lt;sub&gt;''r''&lt;/sub&gt;]. Here ''r'' is the number of variables.

:''See also'': [[Polynomial ring]]

==Function spaces==
:''See main article at [[Function space]], especially the functional analysis section.''
Let ''X'' be a non-empty arbitrary set and ''V'' an arbitrary vector space over '''F'''. The space of all [[function (mathematics)|function]]s from ''X'' to ''V'' is a vector space over '''F''' under [[pointwise]] addition and multiplication. That is, let ''f'' : ''X'' → ''V'' and ''g'' : ''X'' → ''V'' denote two functions, and let ''α''∈'''F'''. We define
:&lt;math&gt;(f + g)(x) = f(x) + g(x) &lt;/math&gt;
:&lt;math&gt;(\alpha f)(x) = \alpha f(x) &lt;/math&gt;
where the operations on the right hand side are those in ''V''. The zero vector is given by the constant function sending everything to the zero vector in ''V''. The space of all functions from ''X'' to ''V'' is commonly denoted ''V''&lt;sup&gt;''X''&lt;/sup&gt;.

If ''X'' is finite and ''V'' is finite-dimensional then ''V''&lt;sup&gt;''X''&lt;/sup&gt; has dimension |''X''|(dim ''V''), otherwise the space is infinite-dimensional (uncountably so if ''X'' is infinite).

Many of the vector spaces that arise in mathematics are subspaces of some function space. We give some further examples.

===Generalized coordinate space===

Let ''X'' be an arbitrary set. Consider the space of all functions from ''X'' to '''F''' which vanish on all but a finite number of points in ''X''. This space is a vector subspace of '''F'''&lt;sup&gt;''X''&lt;/sup&gt;, the space of all possible functions from ''X'' to '''F'''. To see this, note that the union of two finite sets is finite, so that the sum of two functions in this space will still vanish outside a finite set.

The space described above is commonly denoted ('''F'''&lt;sup&gt;''X''&lt;/sup&gt;)&lt;sub&gt;0&lt;/sub&gt; and is called ''generalized coordinate space'' for the following reason. If ''X'' is the set of numbers between 1 and ''n'' then this space is easily seen to be equivalent to the coordinate space '''F'''&lt;sup&gt;''n''&lt;/sup&gt;. Likewise, if ''X'' is the set of [[natural number]]s, '''N''', then this space is just '''F'''&lt;sup&gt;∞&lt;/sup&gt;.

A canonical basis for ('''F'''&lt;sup&gt;''X''&lt;/sup&gt;)&lt;sub&gt;0&lt;/sub&gt; is the set of functions {δ&lt;sub&gt;''x''&lt;/sub&gt; | ''x'' ∈ ''X''} defined by
:&lt;math&gt;\delta_x(y) = \begin{cases}1 \quad x = y \\ 0 \quad x \neq y\end{cases}&lt;/math&gt;
The dimension of ('''F'''&lt;sup&gt;''X''&lt;/sup&gt;)&lt;sub&gt;0&lt;/sub&gt; is therefore equal to the [[cardinality]] of ''X''. In this manner we can construct a vector space of any dimension over any field. Furthermore, ''every vector space is isomorphic to one of this form''. Any choice of basis determines an isomorphism by sending the basis onto the canonical one for ('''F'''&lt;sup&gt;''X''&lt;/sup&gt;)&lt;sub&gt;0&lt;/sub&gt;.

Generalized coordinate space may also be understood as the [[direct sum of modules|direct sum]] of |''X''| copies of '''F''' (i.e. one for each point in ''X''):
:&lt;math&gt;(\mathbf F^X)_0 = \bigoplus_{x\in X}\mathbf F.&lt;/math&gt;
The finiteness condition is built into the definition of the direct sum. Contrast this with the [[direct product]] of |''X''| copies of '''F''' which would give the full function space '''F'''&lt;sup&gt;''X''&lt;/sup&gt;.

===Linear maps===
An important example arising in the context of [[linear algebra]] itself is the vector space of [[linear map]]s. Let ''L''(''V'',''W'') denote the set of all linear maps from ''V'' to ''W'' (both of which are vector spaces over '''F'''). Then ''L''(''V'',''W'') is a subspace of ''W''&lt;sup&gt;''V''&lt;/sup&gt; since it is closed under addition and scalar multiplication.

Note that L('''F'''&lt;sup&gt;''n''&lt;/sup&gt;,'''F'''&lt;sup&gt;''m''&lt;/sup&gt;) can be identified with the space of matrices '''F'''&lt;sup&gt;''m''×''n''&lt;/sup&gt; in a natural way.  In fact, by choosing appropriate bases for finite-dimensional spaces V and W, L(V,W) can also be identified with '''F'''&lt;sup&gt;''m''×''n''&lt;/sup&gt;.  This identification normally depends on the choice of basis.

===Continuous functions===
If ''X'' is some [[topological space]], such as the [[unit interval]] [0,1], we can consider the space of all [[continuous function]]s from ''X'' to '''R'''. This is a vector subspace of '''R'''&lt;sup&gt;''X''&lt;/sup&gt;  since the sum of any two continuous functions is continuous and scalar multiplication is continuous.

===Differential equations===
The subset of the space of all functions from '''R''' to '''R''' consisting of (sufficiently differentiable) functions that satisfy a certain [[differential equation]] is a subspace of '''R'''&lt;sup&gt;'''R'''&lt;/sup&gt; if the equation is linear. This is because [[derivative|differentiation]] is a linear operation, i.e., &lt;nowiki&gt;(a f + b g)' = a f' + b g', where ' is the differentiation operator.&lt;/nowiki&gt;

==Field extensions==
Suppose '''K''' is a [[Field extension|subfield]] of '''F''' (cf. [[field extension]]). Then '''F''' can be regarded as a vector space over '''K''' by restricting scalar multiplication to elements in '''K''' (vector addition is defined as normal). The dimension of this vector space is called the ''degree'' of the extension. For example the [[complex number]]s '''C''' form a two-dimensional vector space over the real numbers '''R'''. Likewise, the [[real numbers]] '''R''' form an (uncountably) infinite-dimensional vector space over the [[rational number]]s '''Q'''.

If ''V'' is a vector space over '''F''' it may also be regarded as vector space over '''K'''. The dimensions are related by the formula
:dim&lt;sub&gt;'''K'''&lt;/sub&gt;''V'' = (dim&lt;sub&gt;'''F'''&lt;/sub&gt;''V'')(dim&lt;sub&gt;'''K'''&lt;/sub&gt;'''F''')
For example '''C'''&lt;sup&gt;''n''&lt;/sup&gt;, regarded as a vector space over the reals, has dimension 2''n''.

==Finite vector spaces==
Apart from the trivial case of a [[zero-dimensional space]] over any field, a vector space over a field '''F''' has a finite number of elements if and only if '''F''' is a [[finite field]] and the vector space has a finite dimension. Thus we have '''F'''&lt;sub&gt;''q''&lt;/sub&gt;, the unique finite field (up to [[isomorphism]]) with ''q'' elements. Here ''q'' must be a power of a [[prime number|prime]] (''q'' = ''p''&lt;sup&gt;''m''&lt;/sup&gt; with ''p'' prime). Then any ''n''-dimensional vector space ''V'' over '''F'''&lt;sub&gt;''q''&lt;/sub&gt; will have ''q''&lt;sup&gt;''n''&lt;/sup&gt; elements. Note that the number of elements in ''V'' is also the power of a prime (because a power of a prime power is again a prime power). The primary example of such a space is the coordinate space ('''F'''&lt;sub&gt;''q''&lt;/sub&gt;)&lt;sup&gt;''n''&lt;/sup&gt;.

These vector spaces are of critical importance in the [[representation theory]] of [[finite group]]s, [[number theory]], and [[cryptography]].

==References==
{{Reflist}}


{{DEFAULTSORT:Examples Of Vector Spaces}}
[[Category:Mathematical examples|Vector spaces]]
[[Category:Vector spaces| ]]</text>
      <sha1>7c3jqlja44te41f7rwa9d4zv8l7xtlz</sha1>
    </revision>
  </page>
  <page>
    <title>Exponential mechanism (differential privacy)</title>
    <ns>0</ns>
    <id>22999896</id>
    <revision>
      <id>842419706</id>
      <parentid>840085057</parentid>
      <timestamp>2018-05-22T09:42:04Z</timestamp>
      <contributor>
        <ip>2001:DA8:201:3410:1BE8:5E79:7FA7:C196</ip>
      </contributor>
      <comment>put a space between two words</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="16200">{{Inappropriate person|date=December 2015}}
The '''exponential mechanism''' is a technique for designing differentially private algorithms. It was developed by Frank McSherry &lt;ref&gt;[http://www.frankmcsherry.org Frank McSherry]&lt;/ref&gt; and Kunal Talwar.&lt;ref&gt;[http://www.kunaltalwar.org Kunal Talwar]&lt;/ref&gt; [[Differential privacy]] is a technique for releasing statistical information about a database without revealing information about its individual entries.

Most of the initial research in the field of differential privacy revolved around real-valued functions which have relatively low [[Differential privacy|sensitivity]] to change in the data of a single individual and whose usefulness is not hampered by small additive perturbations. A natural question is what happens in the situation when one wants to preserve more general sets of properties. The exponential mechanism helps to extend the notion of differential privacy to address these issues. Moreover, it describes a class of mechanisms that includes all possible differentially private mechanisms.

== The exponential mechanism &lt;ref&gt;[http://research.microsoft.com/pubs/65075/mdviadp.pdf F.McSherry and K.Talwar. Mechanism Design via Differential Privacy. Proceedings of the 48th Annual Symposium of Foundations of Computer Science, 2007.]&lt;/ref&gt; ==

=== Algorithm ===
In very generic terms a privacy mechanism maps a set of &lt;math&gt;n\,\!&lt;/math&gt; inputs from domain &lt;math&gt;\mathcal{D}\,\!&lt;/math&gt;, to a range &lt;math&gt;\mathcal{R}\,\!&lt;/math&gt;. The map may be randomized, in which case each element of the domain &lt;math&gt;D\,\!&lt;/math&gt; corresponds to the probability distribution over the range &lt;math&gt;R\,\!&lt;/math&gt;. The privacy mechanism makes no assumption about the nature of &lt;math&gt;\mathcal{D}\,\!&lt;/math&gt; and &lt;math&gt;\mathcal{R}\,\!&lt;/math&gt; apart from a base [[Measure (mathematics)|measure]] &lt;math&gt;\mu\,\!&lt;/math&gt; on &lt;math&gt;\mathcal{R}\,\!&lt;/math&gt;. Let us define a function &lt;math&gt;q:\mathcal{D}^n\times\mathcal{R}\rightarrow\mathbb{R}\,\!&lt;/math&gt;. Intuitively this function assigns a score to the pair &lt;math&gt;(d,r)\,\!&lt;/math&gt;, where &lt;math&gt;d\in\mathcal{D}^n\,\!&lt;/math&gt; and &lt;math&gt;r\in\mathcal{R}\,\!&lt;/math&gt;. The score reflects the appeal of the pair &lt;math&gt;(d,r)\,\!&lt;/math&gt;, i.e. the higher the score, the more appealing the pair is.  
Given the input &lt;math&gt;d\in\mathcal{D}^n\,\!&lt;/math&gt;, the mechanism's objective is to return an &lt;math&gt;r\in\mathcal{R}\,\!&lt;/math&gt; such that the function &lt;math&gt;q(d,r)\,\!&lt;/math&gt; is approximately maximized. To achieve this, set up the mechanism &lt;math&gt;\mathcal{E}_q^\varepsilon(d)\,\!&lt;/math&gt; as follows: &lt;br&gt;
'''Definition:''' For any function &lt;math&gt;q:(\mathcal{D}^n\times\mathcal{R})\rightarrow\mathbb{R}\,\!&lt;/math&gt;, and a base measure &lt;math&gt;\mu\,\!&lt;/math&gt; over &lt;math&gt;\mathcal{R}\,\!&lt;/math&gt;, define:
:&lt;math&gt;\mathcal{E}_q^\varepsilon(d):=\,\!&lt;/math&gt; Choose &lt;math&gt;r\,\!&lt;/math&gt; with probability proportional to &lt;math&gt;e^{\varepsilon q(d,r)}\times\mu(r)\,\!&lt;/math&gt;, where &lt;math&gt;d\in\mathcal{D}^n,r\in R\,\!&lt;/math&gt;.
This definition implies the fact that the probability of returning an &lt;math&gt;r\,\!&lt;/math&gt; increases exponentially with the increase in the value of &lt;math&gt;q(d,r)\,\!&lt;/math&gt;. Ignoring the base measure &lt;math&gt;\mu\,\!&lt;/math&gt; then the value &lt;math&gt;r\,\!&lt;/math&gt; which maximizes &lt;math&gt;q(d,r)\,\!&lt;/math&gt; has the highest probability. Moreover, this mechanism is differentially private. Proof of this claim will follow. One technicality that should be kept in mind is that in order to properly define &lt;math&gt;\mathcal{E}_q^\varepsilon(d)\,\!&lt;/math&gt; the &lt;math&gt;\int_r e^{\varepsilon q(d,r)}\times\mu(r)\,\!&lt;/math&gt; should be finite.

'''Theorem (differential privacy):''' &lt;math&gt;\mathcal{E}_q^\varepsilon(d)\,\!&lt;/math&gt; gives &lt;math&gt;(2\varepsilon\Delta q)\,\!&lt;/math&gt;-differential privacy.

Proof: The probability density of &lt;math&gt;\mathcal{E}_q^\varepsilon(d)\,\!&lt;/math&gt; at &lt;math&gt;r\,\!&lt;/math&gt; equals

:&lt;math&gt;\frac{e^{\varepsilon q(d,r)}\mu(r)}{\int e^{\varepsilon q(d,r)} \mu(r) \, dr}.\,\!&lt;/math&gt;

Now, if a single change in &lt;math&gt;d\,\!&lt;/math&gt; changes &lt;math&gt;q\,\!&lt;/math&gt; by at most &lt;math&gt;\Delta q\,\!&lt;/math&gt; then the numerator can change at most by a factor of &lt;math&gt;e^{\varepsilon\Delta q}\,\!&lt;/math&gt; and the denominator minimum by a factor of &lt;math&gt;e^{-\varepsilon\Delta q}\,\!&lt;/math&gt;. Thus, the ratio of the new probability density (i.e. with new &lt;math&gt;d\,\!&lt;/math&gt;) and the earlier one is at most &lt;math&gt;\exp(2\varepsilon\Delta q)\,\!&lt;/math&gt;.

=== Accuracy ===

We would ideally want the random draws of &lt;math&gt;r\,\!&lt;/math&gt; from the mechanism &lt;math&gt;\mathcal{E}_q^\varepsilon(d)\,\!&lt;/math&gt; to nearly maximize &lt;math&gt;q(d,r)\,\!&lt;/math&gt;. If we consider &lt;math&gt;\max_rq(d,r)\,\!&lt;/math&gt; to be &lt;math&gt;OPT\,\!&lt;/math&gt; then we can show that the probability of the mechanism deviating from &lt;math&gt;OPT\,\!&lt;/math&gt; is low, as long as there is a sufficient mass (in terms of &lt;math&gt;\mu&lt;/math&gt;) of values &lt;math&gt;r\,\!&lt;/math&gt; with value &lt;math&gt;q\,\!&lt;/math&gt; close to the optimum.

'''Lemma:''' Let &lt;math&gt;S_{t}=\{r:q(d,r)&gt;OPT-t\}\,\!&lt;/math&gt; and &lt;math&gt;\bar{S}_{2t}=\{r:q(d,r)\leq OPT-2t\}\,\!&lt;/math&gt;, we have &lt;math&gt;p(\bar{S}_{2t})\,\!&lt;/math&gt; is at most &lt;math&gt;\exp(-\varepsilon t)/\mu(S_{t})\,\!&lt;/math&gt;. The probability is taken over &lt;math&gt;R\,\!&lt;/math&gt;.

Proof: The probability &lt;math&gt;p(\bar{S}_{2t})\,\!&lt;/math&gt; is at most &lt;math&gt;p(\bar{S}_{2t})/p(S_t)\,\!&lt;/math&gt;, as the denominator can be at most one. Since both the probabilities have the same normalizing term so,

:&lt;math&gt;\frac{p(\bar{S}_{2t})}{p(S_t)} = \frac{\int_{\bar{S}_{2t}}\exp(\varepsilon q(d,r))\mu(r) \, dr}{\int_{S_t} \exp(\varepsilon q(d,r))\mu(r) \, dr} \leq \exp(-\varepsilon t) \frac{\mu(\bar{S}_{2t})}{\mu(S_t)}. &lt;/math&gt;

The value of &lt;math&gt;\mu(\bar{S}_{2t})\,\!&lt;/math&gt; is at most one, and so this bound implies the lemma statement.

'''Theorem (Accuracy):''' For those values of &lt;math&gt;t\geq \ln \left( \frac{OPT}{t\mu(S_t)} \right) / \varepsilon\,\!&lt;/math&gt;, we have &lt;math&gt; E[q(d,\mathcal{E}_q^\varepsilon(d))]\geq OPT-3t\,\!&lt;/math&gt;.

Proof: It follows from the previous lemma that the probability of the score being at least &lt;math&gt;OPT-2t\,\!&lt;/math&gt; is &lt;math&gt;1-\exp(-\varepsilon t)/\mu(S_{t})\,\!&lt;/math&gt;. By hypothesis, &lt;math&gt;t\geq \ln\left(\frac{OPT}{t\mu(S_t)}\right)/\varepsilon\,\!&lt;/math&gt;. Substituting the value of &lt;math&gt;t\,\!&lt;/math&gt; we get this probability to be at least &lt;math&gt;1-t/OPT\,\!&lt;/math&gt;. Multiplying with &lt;math&gt;OPT-2t\,\!&lt;/math&gt; yields the desired bound.

We can assume &lt;math&gt;\mu(A)\,\!&lt;/math&gt; for &lt;math&gt;A\subseteq \mathcal{R}\,\!&lt;/math&gt; to be less than or equal to one in all the computations, because we can always normalize with &lt;math&gt;\mu(\mathcal{R})\,\!&lt;/math&gt; .

== Example application of the exponential mechanism &lt;ref&gt;[http://www.cs.cmu.edu/~alroth/Papers/dataprivacy.pdf Avrim Blum,Katrina Ligett,Aaron Roth. A Learning Theory Approach to Non-Iteractive Database Privacy.In Proceedings of the 40th annual ACM symposium on Theory of computing, 2008]&lt;/ref&gt;==

Before we get into the details of the example let us define some terms which we will be using extensively throughout our discussion.

'''Definition (global sensitivity):''' The global sensitivity of a query &lt;math&gt;Q\,\!&lt;/math&gt; is its maximum difference when evaluated on two neighbouring datasets &lt;math&gt;D_1,D_2\in\mathcal{D}^n\,\!&lt;/math&gt;:

: &lt;math&gt; GS_Q = \max_{D_1,D_2:d(D_1,D_2)=1}|(Q(D_1)-Q(D_2))|.\,\!&lt;/math&gt;

'''Definition:''' A predicate query &lt;math&gt;Q_\varphi\,\!&lt;/math&gt; for any predicate &lt;math&gt;\varphi\,\!&lt;/math&gt; is defined to be

:&lt;math&gt;Q_{\varphi}=\frac{|\{x\in D:\varphi(x)\}|}{|D|}.\,\!&lt;/math&gt;

Note that &lt;math&gt;GS_{Q_\varphi}\leq 1/n\,\!&lt;/math&gt; for any predicate &lt;math&gt;\varphi\,\!&lt;/math&gt;.

=== Release mechanism ===

The following is due to [http://www.cs.cmu.edu/~avrim/ Avrim Blum], [[Katrina Ligett]] and [http://www.cs.cmu.edu/~alroth/ Aaron Roth].

'''Definition (Usefulness):''' A [http://cryptowiki.cse.psu.edu/mediawiki/index.php/CSE546-Spring-2009/Differential-Privacy mechanism] &lt;math&gt;\mathcal{A}\,\!&lt;/math&gt; is &lt;math&gt;(\alpha,\delta)\,\!&lt;/math&gt;-useful for queries in class &lt;math&gt;H\,\!&lt;/math&gt; with probability &lt;math&gt;1-\delta\,\!&lt;/math&gt;, if &lt;math&gt;\forall h\in H\,\!&lt;/math&gt; and every dataset &lt;math&gt;D\,\!&lt;/math&gt;, for &lt;math&gt;\widehat{D}=\mathcal{A}(D)\,\!&lt;/math&gt;, &lt;math&gt;|Q_h(\widehat{D})-Q_h(D)|\leq \alpha\,\!&lt;/math&gt;.

Informally, it means that with high probability the query &lt;math&gt;Q_{h}\,\!&lt;/math&gt; will behave in a similar way on the original dataset &lt;math&gt;D\,\!&lt;/math&gt; and on the synthetic dataset &lt;math&gt;\widehat{D}\,\!&lt;/math&gt;. &lt;br&gt;
Let us consider a common problem in Data Mining. Assume there is a database &lt;math&gt;D\,\!&lt;/math&gt; with &lt;math&gt;n\,\!&lt;/math&gt; entries. Each entry consist of &lt;math&gt;k\,\!&lt;/math&gt;-tuples of the form &lt;math&gt;(x_1,x_2,\dots,x_k)\,\!&lt;/math&gt; where &lt;math&gt;x_{i}\in\{0,1\}\,\!&lt;/math&gt;. Now, a user wants to learn a [[Half-space (geometry)|linear halfspace]] of the form &lt;math&gt;\pi_1 x_1 + \pi_2 x_2+\cdots+\pi_{k-1}x_{k-1}\geq x_{k}\,\!&lt;/math&gt;. In essence the user wants to figure out the values of &lt;math&gt;\pi_1,\pi_2,\dots,\pi_{k-1}\,\!&lt;/math&gt; such that maximum number of tuples in the database satisfy the inequality. The algorithm we describe below can generate a synthetic database &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; which will allow the user to learn (approximately) the same linear half-space while querying on this synthetic database. The motivation for such an algorithm being that the new database will be generated in a differentially private manner and thus assure privacy to the individual records in the database &lt;math&gt;D\,\!&lt;/math&gt;.

In this section we show that it is possible to release a dataset which is useful for concepts from a polynomial [[VC dimension|VC-Dimension]] class and at the same time adhere to &lt;math&gt;\varepsilon\,\!&lt;/math&gt;-differential privacy as long as the size of the original dataset is at least polynomial on the [[VC dimension|VC-Dimension]] of the concept class. To state formally:

'''Theorem:''' For any class of functions &lt;math&gt;H\,\!&lt;/math&gt; and any dataset &lt;math&gt;D\subset \{0,1\}^k\,\!&lt;/math&gt; such that
:&lt;math&gt;|D|\geq O\left(\frac{k\cdot \operatorname{VCDim}(H)\log(1/\alpha)}{\alpha^3\varepsilon}+\frac{\log(1/\delta)}{\alpha\varepsilon}\right)\,\!&lt;/math&gt;
we can output an &lt;math&gt;(\alpha,\delta)\,\!&lt;/math&gt;-useful dataset &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; that preserves &lt;math&gt;\varepsilon\,\!&lt;/math&gt;-differential privacy. As we had mentioned earlier the algorithm need not be efficient.

One interesting fact is that the algorithm which we are going to develop generates a synthetic dataset whose size is independent of the original dataset; in fact, it only depends on the [[VC dimension|VC-dimension]] of the concept class and the parameter &lt;math&gt;\alpha\,\!&lt;/math&gt;. The algorithm outputs a dataset of size &lt;math&gt;\tilde{O}(\operatorname{VCDim}(H)/\alpha^2)\,\!&lt;/math&gt;

We borrow the [[Uniform convergence (combinatorics)|Uniform Convergence Theorem]] from [[combinatorics]] and state a corollary of it which aligns to our need.

'''Lemma:''' Given any dataset &lt;math&gt;D\,\!&lt;/math&gt; there exists a dataset &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; of size &lt;math&gt;=O(\operatorname{VCDim}(H)\log(1/\alpha))/\alpha^2\,\!&lt;/math&gt; such that &lt;math&gt;\max_{h\in H}|Q_{h}(D)-Q_{h}(\widehat{D})|\leq \alpha/2\,\!&lt;/math&gt;.

Proof:

We know from the uniform convergence theorem that

: &lt;math&gt;
\begin{align}
&amp; \Pr\left[ \, \left|Q_h(D)-Q_h(\widehat{D})\right| \geq \frac \alpha 2 \text{ for some } h\in H \right] \\[5pt]
\leq {} &amp; 2 \left( \frac{em}{\operatorname{VCDim}(H)} \right)^{\operatorname{VCDim}(H)} \cdot e^{-\alpha^2 m/8},
\end{align}
&lt;/math&gt;

where probability is over the distribution of the dataset. 
Thus, if the RHS is less than one then we know for sure that the data set &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; exists. To bound the RHS to less than one we need &lt;math&gt;m\geq\lambda(\operatorname{VCDim}(H)\log(m/\operatorname{VCDim}(H))/\alpha^2)\,\!&lt;/math&gt;, where &lt;math&gt;\lambda\,\!&lt;/math&gt; is some positive constant. Since we stated earlier that we will output a dataset of size &lt;math&gt;\tilde{O}(\operatorname{VCDim}(H)/\alpha^2)\,\!&lt;/math&gt;, so using this bound on &lt;math&gt;m\,\!&lt;/math&gt; we get &lt;math&gt;m\geq\lambda(\operatorname{VCDim}(H) \log( 1/\alpha) / \alpha^2) \,\!&lt;/math&gt;. Hence the lemma.

Now we invoke the exponential mechanism.

'''Definition:''' For any function &lt;math&gt;q:((\{0,1\}^k)^n \times(\{0,1\}^k)^m) \rightarrow \mathbb{R}\,\!&lt;/math&gt; and input dataset &lt;math&gt;D\,\!&lt;/math&gt;, the exponential mechanism outputs each dataset &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; with probability proportional to &lt;math&gt;e^{q(D,\widehat{D})\varepsilon n/2}\,\!&lt;/math&gt;.

From the exponential mechanism we know this preserves &lt;math&gt;(\varepsilon nGS_q)\,\!&lt;/math&gt;-differential privacy. Let's get back to the proof of the Theorem.

We define &lt;math&gt;(q(D),q(\widehat{D}))=-\max_{h\in H}|Q_h(D)-Q_h(\widehat{D})|\,\!&lt;/math&gt;.

To show that the mechanism satisfies the &lt;math&gt;(\alpha,\delta)\,\!&lt;/math&gt;-usefulness, we should show that it outputs some dataset &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; with &lt;math&gt;q(D,\widehat{D})\geq -\alpha\,\!&lt;/math&gt; with probability &lt;math&gt;1-\delta\,\!&lt;/math&gt;. 
There are at most &lt;math&gt;2^{km}\,\!&lt;/math&gt; output datasets and the probability that &lt;math&gt;q(D,\widehat{D})\leq -\alpha\,\!&lt;/math&gt; is at most proportional to &lt;math&gt;e^{-\varepsilon\alpha n/2}\,\!&lt;/math&gt;. Thus by union bound, the probability of outputting any such dataset &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; is at most proportional to &lt;math&gt;2^{km}e^{-\varepsilon\alpha n/2}\,\!&lt;/math&gt;.  
Again, we know that there exists some dataset &lt;math&gt;\widehat{D}\in(\{0,1\}^k)^m\,\!&lt;/math&gt; for which &lt;math&gt;q(D,\widehat{D})\geq -\alpha/2\,\!&lt;/math&gt;. Therefore, such a dataset is output with probability at least proportional to &lt;math&gt;e^{-\alpha\varepsilon n/4}\,\!&lt;/math&gt;.

Let &lt;math&gt;A:=\,\!&lt;/math&gt; the event that the exponential mechanism outputs some dataset &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; such that &lt;math&gt;q(D,\widehat{D})\geq-\alpha/2\,\!&lt;/math&gt;.

&lt;math&gt;B:=\,\!&lt;/math&gt; the event that the exponential mechanism outputs some dataset &lt;math&gt;\widehat{D}\,\!&lt;/math&gt; such that &lt;math&gt;q(D,\widehat{D})\leq-\alpha\,\!&lt;/math&gt;.
:&lt;math&gt;\therefore \frac{\Pr[A]}{\Pr[B]}\geq \frac{e^{-\alpha\varepsilon n/4}}{2^{km}e^{-\alpha\varepsilon n/2}}=\frac{e^{\alpha\varepsilon n/4}}{2^{km}}.\,\!&lt;/math&gt;
Now setting this quantity to be at least &lt;math&gt;1/\delta\geq(1-\delta)/\delta\,\!&lt;/math&gt;, we find that it suffices to have

:&lt;math&gt;n\geq\frac 4 {\varepsilon\alpha}\left(km+\ln\frac 1 \delta \right)\geq O\left(\frac{d\cdot \operatorname{VCDim}(H)\log(1/\alpha)}{\alpha^3\varepsilon}+\frac{\log(1/\delta)}{\alpha\varepsilon}\right).\,\!&lt;/math&gt;

And hence we prove the theorem.

== The exponential mechanism in other domains ==

In the above example of the usage of exponential mechanism, one can output a synthetic dataset in a differentially private manner and can use the dataset to answer queries with good accuracy. Other private mechanisms, such as posterior sampling,&lt;ref&gt;[https://arxiv.org/abs/1306.1066 Christos Dimitrakakis, Blaine Nelson, Aikaterini Mitrokotsa, Benjamin Rubinstein. Robust and Private Bayesian Inference. Algorithmic Learning Theory 2014]&lt;/ref&gt; which returns parameters rather than datasets, can be made equivalent to the exponential one.&lt;ref&gt;[https://arxiv.org/abs/1502.07645 Yu-Xiang Wang, Stephen E. Fienberg, Alex Smola Privacy for Free: Posterior Sampling and Stochastic Gradient Monte Carlo. International Conference on Machine Learning, 2015.]&lt;/ref&gt;

Apart from the setting of privacy, the exponential mechanism has also been studied in the context of [[auction theory]] and [[Statistical classification|classification algorithms]].&lt;ref&gt;[https://arxiv.org/abs/0803.0924v2 Shiva Prasad Kasiviswanathan, Homin K. Lee, Kobbi Nissim,Sofya Raskhodnikova, Adam Smith. What Can We Learn Privately? Proceedings of the 2008 49th Annual IEEE Symposium on Foundations of Computer Science.]&lt;/ref&gt; In the case of auctions the exponential mechanism helps to achieve a ''truthful'' auction setting.

==References==
{{Reflist}}

==External links==
* [http://www.cis.upenn.edu/~aaroth/Papers/privacybook.pdf The Algorithmic Foundations of Differential Privacy] by Cynthia Dwork and Aaron Roth, 2014.

{{DEFAULTSORT:Exponential Mechanism (Differential Privacy)}}
[[Category:Information privacy]]
[[Category:Theory of cryptography]]
[[Category:Applied probability]]</text>
      <sha1>cw1va2rydx55jbx2xhw6qwrd2roypx5</sha1>
    </revision>
  </page>
  <page>
    <title>Gisbert Hasenjaeger</title>
    <ns>0</ns>
    <id>41568237</id>
    <revision>
      <id>869451419</id>
      <parentid>863725670</parentid>
      <timestamp>2018-11-18T18:20:59Z</timestamp>
      <contributor>
        <username>Scope creep</username>
        <id>346308</id>
      </contributor>
      <comment>/* Construction of Turing Machines */ link fix</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13977">{{Infobox scientist
| name = Gisbert F. R. Hasenjaeger
| image = [[File:Gisbert Hasenjaeger as a soldier in World War 2.jpg]]
| caption = Picture of Gisbert Hasenjaeger in his identity papers during his time at [[Oberkommando der Wehrmacht Chiffrierabteilung|OKW/Chi]]
| birth_date = {{Birth-date|June 1, 1919|June 1, 1919}}
| birth_place = [[Hildesheim]]
| death_date = {{Death date and age|2006|09|2|1919|6|1|mf=y}}
| death_place = [[Münster|Münster, Westphalia]]
| residence = 
| citizenship = [[Germans|German]]
| nationality = 
| ethnicity = 
| field = [[Mathematics]]&lt;br/&gt;[[Logic]]
| work_institutions = [[University of Münster|Münster University]]&lt;br/&gt;[[University of Bonn]] &lt;br/&gt;[[Princeton University|University of Princeton]]
| alma_mater = [[University of Münster|Münster University]]
| doctoral_advisor = [[Heinrich Scholz]]
| doctoral_students = [[Alexander Prestel]]&lt;br/&gt;[[Ronald Jensen]]
| known_for = Testing the [[Enigma machine|Enigma]] encryption machine for cryptographic weaknesses.&lt;br/&gt;Developing a proof of the [[Gödel's completeness theorem|completeness]] theorem in 1949.
| influences = [[Alan Turing]]
| influenced = 
| prizes = 
| footnotes = 
| signature = 
}}

'''Gisbert F. R. Hasenjaeger''' (June 1, 1919 – September 2, 2006) was a German [[Mathematics|mathematical]] [[logic]]ian. Independently and simultaneously with [[Leon Henkin]] in 1949, he developed a new proof of the [[Gödel's completeness theorem|completeness]] theorem of [[Kurt Gödel]] for [[predicate logic]].&lt;ref name="Münster"&gt;{{cite web |url=http://wwwmath.uni-muenster.de/historie/kapitel7.pdf  |title=Past Professors at Münster University|publisher=wwmath.uni-muenster.de |format=PDF|accessdate=6 January 2014}}&lt;/ref&gt;&lt;ref name="Enc"&gt;{{cite web |url=http://wwwmath.uni-muenster.de/logik/Veroeffentlichungen/etc/Hasenjaeger/haselaud.html |title=Laudatio anläßlich der Erneuerung der Doktorurkunde |publisher=WWU Münster Mathematik: Logik |accessdate=17 February 2014}}&lt;/ref&gt; He worked as an assistant to [[Heinrich Scholz]] at Section IVa of [[Oberkommando der Wehrmacht Chiffrierabteilung]], and was responsible for the security of the [[Enigma machine]].&lt;ref&gt;{{cite journal|last1=Schmeh|first1=Klaus|title=Enigma’s Contemporary Witness: Gisbert Hasenjaeger|journal=Cryptologia|date=15 September 2009|volume=33|issue=4|pages=343–346|doi=10.1080/01611190903186003|url=http://www.tandfonline.com/doi/full/10.1080/01611190903186003|accessdate=12 August 2017|publisher=Taylor and Francis|language=English|issn=0161-1194}}&lt;/ref&gt;

==Personal life==
Gisbert Hasenjaeger went to high school in [[Mülheim]], where his father {{Interlanguage link multi|Edwin Renatus Hasenjaeger|de|vertical-align=sup}} was a lawyer and local politician. After completing school in 1936, Gisbert volunteered for labor service. He was drafted for military service in [[World War II]], and fought as an artillerist in the [[Eastern Front (World War II)|Russian campaign]], where he was badly wounded in January 1942. After his recovery, in October 1942 [[Heinrich Scholz]]&lt;ref&gt;Hasenjaeger knew Scholz since his school days and corresponded with him during his time as a [[Conscription|conscript]].&lt;/ref&gt; got him an employment in the [[Cipher Department of the High Command of the Wehrmacht]] (OKW/Chi), where he was the youngest member at 24. He attended a [[cryptography]] training course by [[Erich Hüttenhain]], and was put into the recently founded Section IVa "Security check of own Encoding Procedures" under [[Karl Stein (mathematician)|Karl Stein]], who assigned him the security check of the [[Enigma machine]].&lt;ref name="Heise"&gt;{{cite web |url = http://www.heise.de/tp/artikel/20/20750/1.html |title = Enigma Contemporary Witness - Enigma Vulnerability Part 3 |publisher = Klaus Schmeh |date = 29 August 2005 |website = [[Heise online]] |accessdate= 2 March 2014 }}&lt;/ref&gt;&lt;ref name="Bauer.2000"&gt;{{cite book | isbn=978-3-540-67931-8 | author=Friedrich L. Bauer | title=Entzifferte Geheimnisse &amp;mdash; Methoden und Maximen der Kryptologie | location=Heidelberg | publisher=Springer | edition=3 | year=2000 }} Cited from [[:de:Gisbert Hasenjaeger#Einzelnachweise|German Wikipedia]]&lt;/ref&gt; At the end of the war as OKW/Chi disintegrated, Hasenjaeger managed to escape [[TICOM]], the United States effort to roundup and seize captured German intelligence people and material.&lt;ref name=Heise/&gt;

From the end of 1945, he studied mathematics and especially mathematical logic with [[Heinrich Scholz]] at the [[Münster University|Westfälische Wilhelms-Universität]] University in Münster.  In 1950 received his doctorate ''Topological studies on the semantics and syntax of an extended predicate calculus'' and completed his habilitation in 1953.&lt;ref name="Enc"/&gt;

In Münster, he worked as an assistant to Scholz and later co-author, to write the textbook ''Fundamentals of Mathematical Logic'' in ''Springer's Grundlehren series'' (Yellow series of Springer-Verlag),  which he published in 1961 fully 6 years after Scholz's death. In 1962 he became professor at the [[University of Bonn]], where he was Director of the newly created Department of Logic.&lt;ref name="Enc" /&gt;

In 1962, Dr. Hasenjaeger left Münster University to take a full professorship at Bonn University, where he was established Director of the newly established Department of Logic and Basic Research. In 1964/65 he spent a year at [[Princeton University]] at the [[Institute for Advanced Study]]&lt;ref&gt;{{cite web |url=https://www.ias.edu/scholars/gisbert-hasenjaeger |title=IAS - Gisbert Hasenjeager |last1= |first1= |date= |website=www.ias.edu |publisher=IAS |access-date=20 July 2016}}&lt;/ref&gt; His doctoral students at Bonn included [[Ronald Jensen|Ronald B. Jensen]], his most famous pupil.&lt;ref name="Enc" /&gt;

He became professor emeritus in 1984.

==Work==

===Safety Testing the Enigma Machine===
In Oct 1942, after starting work at [[Oberkommando der Wehrmacht Chiffrierabteilung|OKW/Chi]], Hasenjaeger was trained in cryptology, given by the mathematician, [[Erich Hüttenhain]], who was widely considered the most important German cryptologist of his time.  Hasenjaeger was put into a newly formed department, whose principal responsibility was the defensive testing and security control of their own methods and devices.&lt;ref name="Heise" /&gt;&lt;ref name="TuringBook"&gt;{{cite book |last=Cooper|first=S. Barry |last2=Leeuwen|first2=J. van |date=3 Jun 2013 |title= Alan Turing: His Work and Impact: His Work and Impact |url= |location= |publisher=Elsevier Science |page=936  |isbn= 978-0-12-386980-7 }}&lt;!--|accessdate=15 March 2014 --&gt;&lt;/ref&gt; Hasenjaeger was ordered, by the mathematician [[Karl Stein (mathematician)|Karl Stein]] who was also conscripted at OKW/Chi, to examine the [[Enigma machine]] for cryptologic weaknesses, while Stein was to examine the [[Siemens and Halske T52]] and the [[Lorenz cipher|Lorenz SZ-42]].&lt;ref name="TuringBook" /&gt; The Enigma machine that Hasenjaeger examined was a variation that worked with 3 rotors and had no plug board. Germany sold this version to neutral countries to accrue foreign exchange. Hasenjaeger was presented with a 100 character encrypted message for analysis and found a weakness which enabled the identification of the correct wiring rotors and also the appropriate rotor positions, to decrypt the messages. Further success eluded him however. He crucially failed to identify the most important weakness of the Enigma machine: the lack of fixed points (letters encrypting to themselves) due to the reflector. Hasenjaeger could take some comfort from the fact that even [[Alan Turing]] missed this weakness. Instead the honour was attributed to [[Gordon Welchman]], who used the knowledge to decrypt several hundred thousand Enigma messages during the war.&lt;ref name="Heise" /&gt;&lt;ref name="TuringBook" /&gt; In fact fixed points were earlier used by Polish codebreaker, Henryk Zygalski, as the basis for his method of attack on Enigma cipher, referred to by the Poles as "Zygalski sheets" ([[Zygalski sheets]]) (płachty Zygalskiego) and by the British as the "Netz method".

===Proof of Gödel's completeness theorem===
It was while Hasenjaeger was working at [[Münster University|Westfälische Wilhelms-Universität]] University in Münster in the period between 1946 and 1953 that Hasenjaeger made a most amazing discovery - a [[Mathematical proof|proof]] of [[Kurt Gödel]]'s [[Gödel's completeness theorem|Gödel's completeness]] [[theorem]] for full [[predicate logic]] with identity and function symbols.&lt;ref name="Enc" /&gt; Gödel's proof of 1930 for predicate logic did not automatically establish a procedure for the general case. When he had solved the problem in late 1949, he was frustrated to find that a young American mathematician [[Leon Henkin]], had also created a proof.&lt;ref name="Enc" /&gt;  Both construct from extension of a [[Herbrand structure|term model]], which is then the model for the initial theory. Although the Henkin proof was considered by Hasenjaeger and his peers to more flexible, Hasenjaeger' is considered simpler and more transparent.&lt;ref name="Enc" /&gt;

Hasenjaeger continued to refine his proof through to 1953 when he made a breakthrough. According to the mathematicians [[Alfred Tarski]], [[Stephen Cole Kleene]] and [[Andrzej Mostowski]], the [[Arithmetical hierarchy]] of formulas is the set of arithmetical propositions that are true in the standard model, but not arithmetically definable. So, what does the concept of [[Logical truth|truth]] for the term model mean, the results for the recursively [[Axiomatic system|axiomatized]] [[Peano axioms|Peano arithmetic]] from the Hasenjaeger method? The result was the [[truth predicate]] is well arithmetically, it is even &lt;math&gt;\Delta^0_2&lt;/math&gt;.&lt;ref name="Enc" /&gt;  So far down in the arithmetic hierarchy, and that goes for any recursively axiomatized (countable, consistent) theories. Even if you are true in all the [[natural numbers]] &lt;math&gt;\Pi^0_1&lt;/math&gt; formulas to the axioms.

This classic proof is a very early, original application of the arithmetic hierarchy theory to a general-logical problem. It appeared in 1953 in the ''[[Journal of Symbolic Logic]]''.&lt;ref name="Symbolic"&gt;{{cite journal |first=G. |last=Hasenjaeger |title=Eine Bemerkung zu Henkin's Beweis für die Vollständigkeit des Prädikatenkalküls der ersten Stufe |journal=Journal of Symbolic Logic |year=1953 |volume=18 |issue=1 |pages=42–48 |doi=10.2307/2266326 }} Gödel proof.&lt;/ref&gt;

==Construction of Turing Machines==

In 1963, Hasenjaeger built an [[Universal Turing machine]] out of old telephone relays. Although Hasenjaeger's work on UTMs was largely unknown and he never published any details of the machinery during his lifetime, his family decided to donate the machine to the [[Heinz Nixdorf Museum]] in [[Paderborn]], [[Germany]], after his death.&lt;ref name="Heinz"&gt;{{cite journal |arxiv=1304.0053|title=Wang's B machines are efficiently universal, as is Hasenjaeger's small universal electromechanical toy |journal=Journal of Complexity |volume=30 |issue=5 |date=October 2014 |pages=634–646|bibcode=2013arXiv1304.0053N}}&lt;/ref&gt;&lt;ref name="UTM"&gt;{{cite web |url=http://www.computing-conference.ugent.be/file/21 |title=Hasenjaeger's electromechanical small universal Turing machine is time efficient.|publisher=Department of History an Philosophy Universiteit Gent|format=PDF|accessdate=18 March 2014}}&lt;/ref&gt;&lt;!-- Deleted image removed:  [[File:Universal Turing machine, built by Gisbert Hasenjaeger.jpg|thumb|left|alt=A picture of a small hand built Turing machine.|Gisbert Hasenjaeger [[Universal Turing machine]]|Turing Machine]]  --&gt;''In a academic paper presented at the International Conference of History and Philosophy of Computing''&lt;ref&gt;http://www.computing-conference.ugent.be/&lt;/ref&gt; Rainer Glaschick, Turlough Neary, Damien Woods, Niall Murphy had examined Hasenjaeger's UTM machine at the request of Hasenjaeger family and found that the UTM was remarkably small and efficiently [[Turing completeness|universal]]. Hasenjaeger UTM contained 3-tapes, 4 states, 2 symbols and was an evolution of ideas from [[Edward F. Moore]]'s first universal machine and [[Hao Wang (academic)|Hao Wang]]'s [[Wang B-machine|B-machine]]. Hasenjaeger went on to build a small efficient Wang B-machine simulator. This was again proven by the team assembled by Rainer Glaschick to be efficiently [[Turing completeness|universal]].

===Comments on the Enigma Machine weakness===
It was only in the 1970s that Hasenjaeger learned that the Enigma Machine had been so comprehensively broken.&lt;ref name="Heise" /&gt; It impressed him that Alan Turing himself, considered one of the greatest mathematicians of the 20th century, had worked on breaking the device. The fact that the Germans had so comprehensively underestimated the weaknesses of the device, in contrast to Turing and Welchmans work, was seen by Hasenjaeger today as entirely positive. Hasenjaeger stated:

:''Would it not been so, then the war would have lasted probably longer and the first atomic bomb had not fallen on Japan, but on Germany.''&lt;ref name="Heise" /&gt;

{{Portal|Biography|Logic|Cryptography}}

==References==
{{reflist}}

==Further reading==
* Rebecca Ratcliffe: Searching for Security. The German Investigations into Enigma's security. In: Intelligence and National Security 14 (1999) Issue 1 (Special Issue) S.146–167.
* Rebecca Ratcliffe: How Statistics led the Germans to believe Enigma Secure and Why They Were Wrong: neglecting the practical Mathematics of Ciper machines Add:. Brian J. angle (eds.) The German Enigma Cipher Machine. Artech House: Boston, London of 2005.

{{Authority control}}

{{DEFAULTSORT:Hasenjaeger, Gisbert}}
[[Category:1919 births]]
[[Category:2006 deaths]]
[[Category:20th-century German mathematicians]]
[[Category:21st-century German mathematicians]]
[[Category:German logicians]]
[[Category:Mathematical logicians]]
[[Category:German male writers]]
[[Category:German cryptographers]]</text>
      <sha1>96xm9csn3ai3p36f0ca01rmxfc1g7ug</sha1>
    </revision>
  </page>
  <page>
    <title>Harnack's principle</title>
    <ns>0</ns>
    <id>1433747</id>
    <revision>
      <id>708034831</id>
      <parentid>708034312</parentid>
      <timestamp>2016-03-03T06:05:29Z</timestamp>
      <contributor>
        <username>Baking Soda</username>
        <id>26883893</id>
      </contributor>
      <comment>/* top */ link to [[open set]], [[connected set]]; formatting: 3x whitespace (using [[User:Cameltrader#Advisor.js|Advisor.js]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1201">In [[complex analysis]], '''Harnack's principle''' or '''Harnack's theorem''' is one of several closely related theorems about the convergence of sequences of [[harmonic function]]s, that follow from [[Harnack's inequality]].

If the [[function (mathematics)|functions]] &lt;math&gt; u_1(z)&lt;/math&gt;, &lt;math&gt; u_2(z)&lt;/math&gt;, ... are harmonic in an [[Open set|open]] [[Connected set|connected]] [[subset]] &lt;math&gt;G&lt;/math&gt; of the [[complex plane]] '''C''', and
:&lt;math&gt;u_1(z) \le u_2(z) \le ...&lt;/math&gt;
in every point of &lt;math&gt;G&lt;/math&gt;, then the [[Limit of a sequence|limit]]
:&lt;math&gt; \lim_{n\to\infty}u_n(z)&lt;/math&gt;
either is infinite in every point of the [[domain (mathematics)|domain]]  &lt;math&gt;G&lt;/math&gt; or it is finite in every point of the domain, in both cases uniformly in each [[compact space|compact]] subset of &lt;math&gt;G&lt;/math&gt;. In the latter case, the function
:&lt;math&gt; u(z) = \lim_{n\to\infty}u_n(z)&lt;/math&gt;
is harmonic in the set &lt;math&gt; G&lt;/math&gt;.

==References==
*{{springer|id=h/h046620|title=Harnack theorem|first=L.I.|last= Kamynin}}
*{{PlanetMath attribution|id=6657|title=Harnack's principle}}

[[Category:Harmonic functions]]
[[Category:Theorems in complex analysis]]
[[Category:Mathematical principles]]</text>
      <sha1>8rjukaop418dur6crcw8wb9vjsifhww</sha1>
    </revision>
  </page>
  <page>
    <title>Henry Ainslie</title>
    <ns>0</ns>
    <id>14468160</id>
    <revision>
      <id>825357639</id>
      <parentid>741199255</parentid>
      <timestamp>2018-02-12T23:03:18Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v481)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2034">{{Use dmy dates|date=September 2016}}
{{Use British English|date=September 2016}}
'''Henry Ainslie''' (21 March 1760 – 1834) was a physician.  Educated at [[Hawkshead Grammar School]] and then [[Pembroke College, Cambridge]] (where he graduated [[Senior Wrangler]] and was second in the [[Smith Prize]]), he became a fellow of Pembroke in 1782, and a Fellow of the [[Royal College of Physicians]] in 1795.&lt;ref&gt;{{acad|id=ANSY777H|name=Ainslie, Henry}}&lt;/ref&gt;&lt;ref&gt;Norman Moore, ‘Ainslie, Henry (1760–1834)’, rev. Patrick Wallis, Oxford Dictionary of National Biography, Oxford University Press, 2004; online edn, May 2008 http://www.oxforddnb.com/view/article/235, accessed 18 Sept 2008&lt;/ref&gt; He was a Junior Commissioner for madhouses in 1797 and 1798, and a Senior Commissioner in 1809 and 1817.  
[[File:Mrs Henry Ainslie with her child, by George Romney.jpg|thumb|120px|Mrs Henry Ainslie with her child ([[George Romney (painter)|George Romney]])]]
In 1785 he married Agnes Ford of Monk Coniston (an estate near [[Coniston Water]] in the [[English Lake District]]) in the church at [[Colton, Cumbria|Colton]]. Agnes Ford was the daughter of Richard Ford, founder of the Newland Company, later known as [[Harrison Ainslie]].  The couple owned [[Grizedale Hall|Ford Lodge]] at [[Grizedale]] and planted many thousands of larch trees in the valley and on the surrounding hills and moorland which effectively started [[Grizedale Forest]]. They had a son [[Montague Ainslie]].
==External links==

*[https://www.bbc.co.uk/arts/yourpaintings/paintings/henry-ainslie-17601834-195511] Portrait at Pembroke College

==References==
{{reflist}}
{{DNB Poster|Ainslie, Henry|Henry Ainslie}}

{{DEFAULTSORT:Ainslie, Henry}}
[[Category:Alumni of Pembroke College, Cambridge]]
[[Category:Fellows of Pembroke College, Cambridge]]
[[Category:Senior Wranglers]]
[[Category:1760 births]]
[[Category:1834 deaths]]
[[Category:People educated at Hawkshead Grammar School]]
[[Category:19th-century English medical doctors]]


{{England-med-bio-stub}}</text>
      <sha1>fcmq1cdjy3w6zq9zarfoc54zb3y3yuq</sha1>
    </revision>
  </page>
  <page>
    <title>Influential observation</title>
    <ns>0</ns>
    <id>38818738</id>
    <revision>
      <id>866501878</id>
      <parentid>866385021</parentid>
      <timestamp>2018-10-30T18:43:08Z</timestamp>
      <contributor>
        <username>CAPTAIN RAJU</username>
        <id>25523690</id>
      </contributor>
      <minor/>
      <comment>Reverted 1 edit by [[Special:Contributions/24.138.254.164|24.138.254.164]] identified as test/vandalism using [[WP:STiki|STiki]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5628">[[File:Anscombe's quartet 3.svg|right|400px|thumb|In [[Anscombe's quartet]] the two [[dataset]]s on the bottom both contain influential points. All four sets are identical when examined using simple summary statistics, but vary considerably when graphed. If one point were removed, the line would look very different.]]
In [[statistics]], an '''influential observation''' is an observation for a [[Estimation theory|statistical calculation]] whose deletion from the dataset would noticeably change the [[result]] of the calculation.&lt;ref&gt;{{citation|title=Elementary Statistics for Geographers|first1=James E.|last1=Burt|first2=Gerald M.|last2=Barber|first3=David L.|last3=Rigby|publisher=Guilford Press|year=2009|isbn=9781572304840|page=513|url=https://books.google.com/books?id=p7YMOPuu8ugC&amp;pg=PA513}}.&lt;/ref&gt; In particular, in [[regression analysis]] an influential point is one whose deletion has a large effect on the parameter estimates.&lt;ref name = "Everitt"/&gt;

== Assessment ==
Various methods have been proposed for measuring influence.&lt;ref&gt;{{cite web |first=Larry |last=Winner |title=Influence Statistics, Outliers, and Collinearity Diagnostics |work= |date=March 25, 2002 |url=http://stat.ufl.edu/~winner/sta6127/influence.doc }}&lt;/ref&gt;&lt;ref&gt;{{cite book |last=Belsley |first=David A. |last2=Kuh |first2=Edwin |last3=Welsh |first3=Roy E. | year=1980 |title=Regression Diagnostics: Identifying Influential Data and Sources of Collinearity |publisher=[[John Wiley &amp; Sons]] |location=New York |series=Wiley Series in Probability and Mathematical Statistics |isbn=0-471-05856-4 |pages=11–16 |url=https://books.google.com/books?id=GECBEUJVNe0C&amp;pg=PA11 }}&lt;/ref&gt; Assume an estimated regression &lt;math&gt;\mathbf{y} = \mathbf{X} \mathbf{b} + \mathbf{e}&lt;/math&gt;, where &lt;math&gt;\mathbf{y}&lt;/math&gt; is an ''n''×1 column vector for the response variable, &lt;math&gt;\mathbf{X}&lt;/math&gt; is the ''n''×''k'' [[design matrix]] of explanatory variables (including a constant), &lt;math&gt;\mathbf{e}&lt;/math&gt; is the ''n''×1 residual vector, and &lt;math&gt;\mathbf{b}&lt;/math&gt; is a ''k''×1 vector of estimates of some population parameter &lt;math&gt;\mathbf{\beta} \in \mathbb{R}^{k}&lt;/math&gt;. Also define &lt;math&gt;\mathbf{H} \equiv \mathbf{X} \left(\mathbf{X}^{\mathsf{T}} \mathbf{X} \right)^{-1} \mathbf{X}^{\mathsf{T}}&lt;/math&gt;, the [[projection matrix]] of &lt;math&gt;\mathbf{X}&lt;/math&gt;. Then we have the following measures of influence:

# &lt;math&gt;\text{DFBETA}_{i} \equiv \mathbf{b} - \mathbf{b}_{(-i)} = \frac{\left( \mathbf{X}^{\mathsf{T}} \mathbf{X} \right)^{-1} \mathbf{x}_{i}^{\mathsf{T}} e_{i}}{1 - h_{i}}&lt;/math&gt;, where &lt;math&gt;\mathbf{b}_{(-i)}&lt;/math&gt; denotes the coefficients estimated with the ''i''-th row &lt;math&gt;\mathbf{x}_{i}&lt;/math&gt; of &lt;math&gt;\mathbf{X}&lt;/math&gt; deleted, &lt;math&gt;h_{i} = \mathbf{x}_{i} \left( \mathbf{X}^{\mathsf{T}} \mathbf{X} \right)^{-1} \mathbf{x}_{i}^{\mathsf{T}}&lt;/math&gt; denotes the ''i''-th row of &lt;math&gt;\mathbf{H}&lt;/math&gt;. Thus DFBETA measures the difference in each parameter estimate with and without the influential point. There is a DFBETA for each point and each observation (if there are ''N'' points and ''k'' variables there are N·k DFBETAs).&lt;ref&gt;{{cite web |title=Outliers and DFBETA |url=http://www.albany.edu/faculty/kretheme/PAD705/SupportMat/DFBETA.pdf |dead-url=no |archivedate=May 11, 2013 |archiveurl=https://web.archive.org/web/20130511013229/http://www.albany.edu/faculty/kretheme/PAD705/SupportMat/DFBETA.pdf }}&lt;/ref&gt;
# [[DFFITS]]
# [[Cook's distance|Cook's ''D'']] measures the effect of removing a data point on all the parameters combined.&lt;ref name="Everitt"&gt;{{cite book | last = Everitt | first = Brian | title = The Cambridge Dictionary of Statistics | publisher = Cambridge University Press | location = Cambridge, UK New York | year = 1998 | isbn = 0-521-59346-8 }} &lt;/ref&gt;

== Outliers, leverage and influence ==

An [[outlier]] may be defined as a surprising data point. [[Leverage (statistics)|Leverage]] is a measure of how much the estimated value of the [[dependent variable]] changes when the point is removed. There is one value of leverage for each data point.&lt;ref&gt;{{cite web |first=Clifford |last=Hurvich |title=Simple Linear Regression VI: Leverage and Influence |publisher=NYU Stern |url=http://pages.stern.nyu.edu/~churvich/Undergrad/Handouts2/31-Reg6.pdf |dead-url=no |archivedate=September 21, 2006 |archiveurl=https://web.archive.org/web/20060921160749/http://pages.stern.nyu.edu/~churvich/Undergrad/Handouts2/31-Reg6.pdf }}&lt;/ref&gt; Data points with high leverage force the regression line to be close to the point.&lt;ref name = "Everitt"/&gt; In Anscombe's quartet, only the bottom right image has a point with high leverage.

== See also ==
* [[Outlier]]
* [[Leverage (statistics)|Leverage]]
** [[Partial leverage]]
* [[Regression analysis]]
* [[Cook's distance#Detecting highly influential observations]]
* [[Anomaly detection]]

== References ==
{{reflist}}

== Further reading ==
* {{cite journal |first=Catherine |last=Dehon |first2=Marjorie |last2=Gassner |first3=Vincenzo |last3=Verardi |title=Beware of ‘Good’ Outliers and Overoptimistic Conclusions |journal=Oxford Bulletin of Economics and Statistics |volume=71 |issue=3 |pages=437–452 |year=2009 |doi=10.1111/j.1468-0084.2009.00543.x }}
* {{cite book |first=Peter |last=Kennedy |authorlink=Peter Kennedy (economist) |chapter=Robust Estimation |title=A Guide to Econometrics |location=Cambridge |publisher=The MIT Press |edition=Fifth |year=2003 |isbn=0-262-61183-X |pages=372–388 |chapterurl=https://books.google.com/books?id=B8I5SP69e4kC&amp;pg=PA372 }}

[[Category:Actuarial science]]
[[Category:Regression diagnostics]]
[[Category:Robust statistics]]</text>
      <sha1>kmskjiv9uk2ytk28a9xmby7zdd2q8x3</sha1>
    </revision>
  </page>
  <page>
    <title>Integration by parts</title>
    <ns>0</ns>
    <id>147252</id>
    <revision>
      <id>868012469</id>
      <parentid>868012293</parentid>
      <timestamp>2018-11-09T12:24:08Z</timestamp>
      <contributor>
        <ip>112.79.220.237</ip>
      </contributor>
      <comment>/* Applications */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="31826">{{Calculus |Integral}}

In [[calculus]], and more generally in [[mathematical analysis]], '''integration by parts''' or '''partial integration''' is a process that finds the [[integral (mathematics)|integral]] of a [[product (mathematics)|product]] of functions in terms of the integral of their derivative and antiderivative. It is frequently used to transform the antiderivative of a product of functions into an antiderivative for which a solution can be more easily found. The rule can be derived in one line simply by integrating the [[product rule]] of [[derivative|differentiation]].

If {{math|''u''}} {{=}} {{math|''u''(''x'')}} and {{math|''du''}} {{=}} {{math|''u''′(''x'')&amp;nbsp;''dx''}}, while {{math|''v''}} {{=}} {{math|''v''(''x'')}} and {{math|''dv''}} {{=}} {{math|''v''′(''x'')&amp;nbsp;''dx''}}, then integration by parts states that:

:&lt;math&gt;\begin{align}
\int_a^b u(x) v'(x) \, dx 
&amp;= [u(x) v(x)]_a^b - \int_a^b u'(x) v(x) dx\\
&amp;= u(b) v(b) - u(a) v(a) - \int_a^b u'(x) v(x) \, dx 
\end{align}&lt;/math&gt;

or more compactly:

:&lt;math&gt;\int u \, dv=uv-\int v \, du.\!&lt;/math&gt;

Mathematician [[Brook Taylor]] discovered integration by parts, first publishing the idea in [[1715 in science|1715]].&lt;ref name="Brook Taylor biography, St. Andrews"&gt;{{cite web |url=http://www-history.mcs.st-andrews.ac.uk/Biographies/Taylor.html |title=Brook Taylor |work=History.MCS.St-Andrews.ac.uk |accessdate= May 25, 2018}}&lt;/ref&gt;&lt;ref name="Brook Taylor biography, Stetson"&gt;{{cite web |url=https://www2.stetson.edu/~efriedma/periodictable/html/Tl.html |title=Brook Taylor |work=Stetson.edu |accessdate= May 25, 2018}}&lt;/ref&gt; More general formulations of integration by parts exist for the [[Riemann–Stieltjes integral#Properties and relation to the Riemann integral|Riemann–Stieltjes]] and [[Lebesgue–Stieltjes integral#Integration by parts|Lebesgue–Stieltjes integrals]]. The discrete analogue for sequences is called [[summation by parts]].

==Theorem==

===Product of two functions===
The theorem can be derived as follows. Suppose ''u''(''x'') and ''v''(''x'') are two [[continuously differentiable]] [[function (mathematics)|functions]]. The [[product rule]] states (in [[Leibniz's notation]]):

:&lt;math&gt;\frac{d}{dx}\Big(u(x)v(x)\Big) = v(x) \frac{d}{dx}\left(u(x)\right) + u(x) \frac{d}{dx}\left(v(x)\right).\!&lt;/math&gt;

Integrating both sides with respect to ''x'',

:&lt;math&gt;\int \frac{d}{dx}\left(u(x)v(x)\right)\,dx = \int u'(x)v(x)\,dx + \int u(x)v'(x)\,dx &lt;/math&gt;

then applying the definition of [[indefinite integral]],

:&lt;math&gt;u(x)v(x) = \int u'(x)v(x)\,dx + \int u(x)v'(x)\,dx&lt;/math&gt;

:&lt;math&gt;\int u(x)v'(x)\,dx = u(x)v(x) - \int u'(x)v(x)\,dx&lt;/math&gt;

gives the formula for '''integration by parts'''.

Since ''du'' and ''dv'' are [[differentials of a function]] of one variable ''x'',

:&lt;math&gt;du=u'(x)dx \quad dv=v'(x)dx &lt;/math&gt;

:&lt;math&gt;\int u(x)\,dv = u(x)v(x) - \int v(x)\,du&lt;/math&gt;

The original integral ∫''uv''′&amp;nbsp;''dx'' contains ''v''′ ([[derivative]] of ''v''); in order to apply the theorem, ''v'' ([[antiderivative]] of ''v''′) must be found, and then the resulting integral ∫''vu''′&amp;nbsp;''dx'' must be evaluated.

===Extension to other cases===
It is not actually necessary for ''u'' and ''v'' to be continuously differentiable. Integration by parts works if ''u'' is [[absolutely continuous]] and the function designated ''{{math|v'}}'' is [[Lebesgue integrable]] (but not necessarily continuous).&lt;ref&gt;{{cite web|title=Integration by parts|url=https://www.encyclopediaofmath.org/index.php/Integration_by_parts|website=Encyclopedia of Mathematics}}&lt;/ref&gt; (If ''v''' has a point of discontinuity then its antiderivative ''v'' may not have a derivative at that point.)

If the interval of integration is not [[compact space|compact]], then it is not necessary for ''u'' to be absolutely continuous in the whole interval or for ''v''&amp;nbsp;' to be Lebesgue integrable in the interval, as a couple of examples (in which ''u'' and ''v'' are continuous and continuously differentiable) will show. For instance, if

:&lt;math&gt;u(x)=\exp(x)/x^2,\,v'(x)=\exp(-x)&lt;/math&gt;

''u'' is not absolutely continuous on the interval {{closed-open|1, +∞}}, but nevertheless

:&lt;math&gt;\int_1^\infty u(x)v'(x)\,dx = \left[u(x)v(x)\right]_1^\infty - \int_1^\infty u'(x)v(x)\,dx&lt;/math&gt;

so long as &lt;math&gt;\left[u(x)v(x)\right]_1^\infty&lt;/math&gt; is taken to mean the limit of &lt;math&gt;u(L)v(L)-u(1)v(1)&lt;/math&gt; as &lt;math&gt;L\to\infty&lt;/math&gt; and so long as the two terms on the right-hand side are finite. This is only true if we choose &lt;math&gt;v(x)=-\exp(-x).&lt;/math&gt; Similarly, if

:&lt;math&gt;u(x)=\exp(-x),\,v'(x)=x^{-1}\sin(x)&lt;/math&gt;

''v''' is not Lebesgue integrable on the interval {{closed-open|1, +∞}}, but nevertheless

:&lt;math&gt;\int_1^\infty u(x)v'(x)\,dx = \left[u(x)v(x)\right]_1^\infty - \int_1^\infty u'(x)v(x)\,dx&lt;/math&gt;
with the same interpretation.

One can also easily come up with similar examples in which ''u'' and ''v'' are '''''not''''' continuously differentiable.


Further, if &lt;math&gt;f(x)&lt;/math&gt; is a function of bounded variation on the segment &lt;math&gt;[a,b],&lt;/math&gt; and &lt;math&gt;\varphi(x)&lt;/math&gt; is differentiable on &lt;math&gt;[a,b],&lt;/math&gt; then

&lt;math&gt;\int\limits_{a}^{b}f(x)\varphi'(x)\,dx=-\int\limits_{-\infty}^{+\infty}\widetilde\varphi(x)\,d(\widetilde\chi_{[a,b]}(x)\widetilde f(x)),&lt;/math&gt;

where by &lt;math&gt;d(\chi_{[a,b]}(x)\widetilde f(x))&lt;/math&gt; I denoted the signed measure corresponding to the function of bounded variation &lt;math&gt;\chi_{[a,b]}(x)f(x)&lt;/math&gt;, and functions &lt;math&gt;\widetilde f, \widetilde \varphi&lt;/math&gt; are extensions of &lt;math&gt;f, \varphi&lt;/math&gt;
to &lt;math&gt;\mathbb{R},&lt;/math&gt; which are respectively of bounded variation and differentiable.

===Product of many functions===

Integrating the product rule for three multiplied functions, ''u''(''x''), ''v''(''x''), ''w''(''x''), gives a similar result:

:&lt;math&gt;\int_a^b u v \, dw = [u v w]^{b}_{a} - \int_a^b u w \, dv - \int_a^b v w \, du.&lt;/math&gt;

In general, for ''n'' factors

:&lt;math&gt;\frac{d}{dx} \left(\prod_{i=1}^n u_i(x) \right)= \sum_{j=1}^n \prod_{i\neq j}^n u_i(x) \frac{du_j(x)}{dx}, &lt;/math&gt;

which leads to

:&lt;math&gt; \Bigl[ \prod_{i=1}^n u_i(x) \Bigr]_a^b = \sum_{j=1}^n \int_a^b \prod_{i\neq j}^n u_i(x) \, du_j(x), &lt;/math&gt;

where the [[product (mathematics)|product]] is of all functions except for the one differentiated in the same term.

==Visualization==
[[Image:Integration by parts v2.svg|thumb|280px |Graphical interpretation of the theorem. The pictured curve is parametrized by the variable t.]]
Consider a parametric curve by (''x'', ''y'') = (''f''(''t''), ''g''(''t'')). Assuming that the curve is locally [[Injective function|one-to-one]] and [[Locally integrable function|integrable]], we can define
:&lt;math&gt;x(y) = f(g^{-1}(y))&lt;/math&gt;
:&lt;math&gt;y(x) = g(f^{-1}(x))&lt;/math&gt;

The area of the blue region is

:&lt;math&gt;A_1=\int_{y_1}^{y_2}x(y)dy&lt;/math&gt;

Similarly, the area of the red region is
:&lt;math&gt;A_2=\int_{x_1}^{x_2}y(x)dx&lt;/math&gt;

The total area ''A''&lt;sub&gt;1&lt;/sub&gt; + ''A''&lt;sub&gt;2&lt;/sub&gt; is equal to the area of the bigger rectangle, ''x''&lt;sub&gt;2&lt;/sub&gt;''y''&lt;sub&gt;2&lt;/sub&gt;, minus the area of the smaller one, ''x''&lt;sub&gt;1&lt;/sub&gt;''y''&lt;sub&gt;1&lt;/sub&gt;:

:&lt;math&gt;\overbrace{\int_{y_1}^{y_2}x(y)dy}^{A_1}+\overbrace{\int_{x_1}^{x_2}y(x)dx}^{A_2}=\biggl.x \cdot y(x)\biggl|_{x_1}^{x_2} = \biggl.y \cdot x(y)\biggl|_{y_1}^{y_2}.&lt;/math&gt;
Or, in terms of ''t'',
:&lt;math&gt;\int_{t_1}^{t_2}x(t)dy(t) + \int_{t_1}^{t_2}y(t)dx(t) = \biggl. x(t)y(t) \biggl|_{t_1}^{t_2}&lt;/math&gt;
Or, in terms of indefinite integrals, this can be written as
:&lt;math&gt;\int xdy + \int y dx = xy&lt;/math&gt;
Rearranging:
:&lt;math&gt;\int xdy = xy - \int y dx&lt;/math&gt;
Thus integration by parts may be thought of as deriving the area of the blue region from the area of rectangles and that of the red region.

This visualization also explains why integration by parts may help find the integral of an inverse function ''f''&lt;sup&gt;−1&lt;/sup&gt;(''x'') when the integral of the function ''f''(''x'') is known. Indeed, the functions ''x''(''y'') and ''y''(''x'') are inverses, and the integral ∫''x dy'' may be calculated as above from knowing the integral ∫''y dx''. In particular, this explains use of integration by parts to integrate [[logarithm]] and [[inverse trigonometric function]]s.

==Applications==

===Strategy===
Integration by parts is a [[heuristic]] rather than a purely mechanical process for solving integrals; given a single function to integrate, the typical strategy is to carefully separate this single function into a product of two functions ''u''(''x'')''v''(''x'') such that the residual integral from the integration by parts formula is easier to evaluate than the single function. The following form is useful in illustrating the best strategy to take:

:&lt;math&gt;\int uv\ dx = u \int v\ dx - \int\left(u' \int v\ dx \right)\ dx.&lt;/math&gt;

Note that on the right-hand side, ''u'' is differentiated and ''v'' is integrated; consequently it is useful to choose ''u'' as a function that simplifies when differentiated, or to choose ''v'' as a function that simplifies when integrated. As a simple example, consider:

:&lt;math&gt;\int\frac{\ln(x)}{x^2}\ dx\ .&lt;/math&gt;

Since the derivative of ln(''x'') is {{sfrac|1|''x''}}, one makes (ln(''x'')) part ''u''; since the antiderivative of {{sfrac|1|''x''&lt;sup&gt;2&lt;/sup&gt;}} is -{{sfrac|1|''x''}}, one makes {{sfrac|1|''x''&lt;sup&gt;2&lt;/sup&gt;}}''dx'' part ''dv''. The formula now yields:

:&lt;math&gt;\int\frac{\ln(x)}{x^2}\ dx = -\frac{\ln(x)}{x} - \int \biggl(\frac1{x}\biggr) \biggl(-\frac1{x}\biggr)\ dx\ .&lt;/math&gt;

The antiderivative of −{{sfrac|1|''x''&lt;sup&gt;2&lt;/sup&gt;}} can be found with the [[power rule]] and is {{sfrac|1|''x''}}.

Alternatively, one may choose ''u'' and ''v'' such that the product ''u''' (∫''v dx'') simplifies due to cancellation. For example, suppose one wishes to integrate:

:&lt;math&gt;\int\sec^2(x)\cdot\ln\Big(\bigl|\sin(x)\bigr|\Big)\ dx.&lt;/math&gt;

If we choose ''u''(''x'') = ln(|sin(''x'')|) and ''v''(''x'') = sec&lt;sup&gt;2&lt;/sup&gt;x, then ''u'' differentiates to 1/ tan ''x'' using the [[chain rule]] and ''v'' integrates to tan ''x''; so the formula gives:

:&lt;math&gt;\int\sec^2(x)\cdot\ln\Big(\bigl|\sin(x)\bigr|\Big)\ dx=\tan(x)\cdot\ln\Big(\bigl|\sin(x)\bigr|\Big)-\int\tan(x)\cdot\frac1{\tan(x)}dx\ .&lt;/math&gt;

The integrand simplifies to 1, so the antiderivative is ''x''. Finding a simplifying combination frequently involves experimentation.

In some applications, it may not be necessary to ensure that the integral produced by integration by parts has a simple form; for example, in [[numerical analysis]], it may suffice that it has small magnitude and so contributes only a small error term. Some other special techniques are demonstrated in the examples below.

====Polynomials and trigonometric functions====

In order to calculate

:&lt;math&gt;I=\int x\cos(x)\ dx\ ,&lt;/math&gt;

let:
:&lt;math&gt;u = x\ \Rightarrow\ du = dx&lt;/math&gt;
:&lt;math&gt;dv = \cos(x)\ dx\ \Rightarrow\ v = \int\cos(x)\ dx = \sin(x)&lt;/math&gt;

then:

:&lt;math&gt;
\begin{align}
  \int x\cos(x)\ dx &amp; = \int u\ dv \\
  &amp; = u\cdot v - \int v \, du \\
  &amp; = x\sin(x) - \int \sin(x)\ dx \\
  &amp; = x\sin(x) + \cos(x) + C,
\end{align}
\!&lt;/math&gt;

where ''C'' is a [[constant of integration]].

For higher powers of ''x'' in the form

:&lt;math&gt;\int x^n e^x\ dx,\ \int x^n\sin(x)\ dx,\ \int x^n\cos(x)\ dx\ ,&lt;/math&gt;

repeatedly using integration by parts can evaluate integrals such as these; each application of the theorem lowers the power of ''x'' by one.

====Exponentials and trigonometric functions====

An example commonly used to examine the workings of integration by parts is

:&lt;math&gt;I=\int e^x\cos(x)\ dx.&lt;/math&gt;

Here, integration by parts is performed twice. First let

:&lt;math&gt;u = \cos(x)\ \Rightarrow\ du = -\sin(x)\ dx&lt;/math&gt;
:&lt;math&gt;dv = e^x\ dx\ \Rightarrow\ v = \int e^x\ dx = e^x&lt;/math&gt;

then:

:&lt;math&gt;\int e^x\cos(x)\ dx = e^x\cos(x) + \int e^x\sin(x)\ dx.&lt;/math&gt;

Now, to evaluate the remaining integral, we use integration by parts again, with:

:&lt;math&gt;u = \sin(x)\ \Rightarrow\ du = \cos(x)\ dx&lt;/math&gt;
:&lt;math&gt;dv = e^x\ dx\ \Rightarrow\ v = \int e^x\ dx = e^x.&lt;/math&gt;

Then:

:&lt;math&gt;\int e^x\sin(x)\ dx = e^x\sin(x) - \int e^x\cos(x)\ dx.&lt;/math&gt;

Putting these together,

:&lt;math&gt;\int e^x\cos(x)\ dx = e^x\cos(x) + e^x\sin(x) - \int e^x\cos(x)\ dx.&lt;/math&gt;

The same integral shows up on both sides of this equation. The integral can simply be added to both sides to get

:&lt;math&gt;2\int e^x\cos(x)\ dx = e^x\bigl(\sin(x)+\cos(x)\bigr) + C&lt;/math&gt;

which rearranges to:

:&lt;math&gt;\int e^x\cos(x)\ dx = \frac{e^x\bigl(\sin(x)+\cos(x)\bigr)}{2} + C'&lt;/math&gt;

where again ''C'' (and ''C''&lt;nowiki&gt;'&lt;/nowiki&gt; = ''C''/2) is a [[constant of integration]].

A similar method is used to find the [[integral of secant cubed]].

====Functions multiplied by unity====

Two other well-known examples are when integration by parts is applied to a function expressed as a product of 1 and itself. This works if the derivative of the function is known, and the integral of this derivative times ''x'' is also known.

The first example is ∫ ln(''x'') d''x''. We write this as:

:&lt;math&gt;I=\int\ln(x)\cdot 1\ dx\ .&lt;/math&gt;

Let:

:&lt;math&gt;u = \ln(x)\ \Rightarrow\ du = \frac{dx}{x}&lt;/math&gt;
:&lt;math&gt;dv = dx\ \Rightarrow\ v = x&lt;/math&gt;

then:

:&lt;math&gt;
\begin{align}
\int \ln(x)\ dx &amp; = x\ln(x) - \int\frac{x}{x}\ dx \\
&amp; = x\ln(x) - \int 1\ dx \\
&amp; = x\ln(x) - x + C
\end{align}
&lt;/math&gt;

where ''C'' is the [[constant of integration]].

The second example is the [[inverse tangent]] function arctan(''x''):

:&lt;math&gt;I=\int\arctan(x)\ dx.&lt;/math&gt;

Rewrite this as

:&lt;math&gt;\int\arctan(x)\cdot 1\ dx.&lt;/math&gt;

Now let:

:&lt;math&gt;u = \arctan(x)\ \Rightarrow\ du = \frac{dx}{1+x^2}&lt;/math&gt;

:&lt;math&gt;dv = dx\ \Rightarrow\ v = x&lt;/math&gt;

then

:&lt;math&gt;
\begin{align}
\int\arctan(x)\ dx
&amp; = x\cdot\arctan(x) - \int\frac{x}{1+x^2}\ dx \\[8pt]
&amp; = x\cdot\arctan(x) - \frac{\ln(1+x^2)}{2} + C
\end{align}
&lt;/math&gt;

using a combination of the [[inverse chain rule method]] and the [[natural logarithm integral condition]].

===LIATE rule===
A [[rule of thumb]] proposed by [[Herbert Kasube]] advises that whichever function comes first in the following list should be chosen as ''u'':&lt;ref&gt;{{Cite journal |jstor=2975556 |first=Herbert E. |last=Kasube |title=A Technique for Integration by Parts |journal=[[The American Mathematical Monthly]] |volume=90 |issue=3 |year=1983 |pages=210–211 |doi=10.2307/2975556}}&lt;/ref&gt;
:'''L''' – [[logarithmic function]]s: &lt;math&gt;\ln(x),\ \log_b(x),&lt;/math&gt; etc.
:'''I''' – [[inverse trigonometric function]]s:  &lt;math&gt;\arctan(x),\ \arcsec(x),&lt;/math&gt; etc.
:'''A''' – [[Polynomial|algebraic functions]]: &lt;math&gt;x^2,\ 3x^{50},&lt;/math&gt; etc.
:'''T''' – [[trigonometric functions]]: &lt;math&gt;\sin(x),\ \tan(x),&lt;/math&gt; etc.
:'''E''' – [[exponential function]]s: &lt;math&gt;e^x,\ 19^x,&lt;/math&gt; etc.

The function which is to be ''dv'' is whichever comes last in the list: functions lower on the list have easier [[antiderivative]]s than the functions above them. The rule is sometimes written as "DETAIL" where ''D'' stands for ''dv''.

To demonstrate the LIATE rule, consider the integral

:&lt;math&gt;\int x \cdot \cos(x) \,dx.&lt;/math&gt;

Following the LIATE rule, ''u'' = ''x'', and ''dv'' = cos(''x'') ''dx'', hence ''du'' = ''dx'', and ''v'' = sin(''x''), which makes the integral become
:&lt;math&gt;x \cdot \sin(x) - \int 1 \sin(x) \,dx,&lt;/math&gt;
which equals
:&lt;math&gt;x \cdot \sin(x) + \cos(x) + C.&lt;/math&gt;

In general, one tries to choose ''u'' and ''dv'' such that ''du'' is simpler than ''u'' and ''dv'' is easy to integrate. If instead cos(''x'') was chosen as ''u'', and ''x dx'' as ''dv'', we would have the integral

:&lt;math&gt;\frac{x^2}2 \cos(x) + \int \frac{x^2}2 \sin(x) \,dx,&lt;/math&gt;

which, after recursive application of the integration by parts formula, would clearly result in an infinite recursion and lead nowhere.

Although a useful rule of thumb, there are exceptions to the LIATE rule. A common alternative is to consider the rules in the "ILATE" order instead. Also, in some cases, polynomial terms need to be split in non-trivial ways. For example, to integrate

:&lt;math&gt;\int x^3 e^{x^2} \,dx,&lt;/math&gt;

one would set

:&lt;math&gt;u = x^2, \quad dv = x \cdot e^{x^2} \,dx,&lt;/math&gt;

so that

:&lt;math&gt;du = 2x \,dx, \quad v = \frac{e^{x^2}}{2}.&lt;/math&gt;

Then

:&lt;math&gt;\int x^3 e^{x^2} \,dx = \int (x^2) \left(xe^{x^2}\right) \,dx = \int u \,dv
= uv - \int v \,du = \frac{x^2 e^{x^2}}{2} - \int x e^{x^2} \,dx.&lt;/math&gt;

Finally, this results in
:&lt;math&gt;\int x^3 e^{x^2} \,dx = \frac{e^{x^2}(x^2 - 1)}{2} + C.&lt;/math&gt;

Integration by parts is often used as a tool to prove theorems in [[mathematical analysis]].

===Use in special functions===

The [[gamma function]] is an example of a [[special function]], defined as an [[improper integral]] for {{math|''z'' &amp;gt; 0}}. Integration by parts illustrates it to be an extension of the [[factorial]]:

:&lt;math&gt;\begin{align}
\Gamma(z) &amp; = \int_0^\infty e^{-x} x^{z-1} \,dx \\
 &amp; = - \int_0^\infty x^{z-1} \, d\left(e^{-x}\right) \\
 &amp; = - \left[e^{-x} x^{z-1}\right]_0^\infty + \int_0^\infty e^{-x} \, d\left(x^{z-1}\right) \\
 &amp; = 0 + \int_0^\infty \left(z-1\right) x^{z-2} e^{-x} \, dx\\
 &amp; = (z-1)\Gamma(z-1).
\end{align} &lt;/math&gt;

Since

:&lt;math&gt;\Gamma(1) = \int_0^\infty e^{-x} \, dx = 1,&lt;/math&gt;

for integer {{mvar|z}}, applying this formula repeatedly gives the [[factorial]] (denoted by the !):

:&lt;math&gt;\Gamma(z+1) = z!&lt;/math&gt;

===Use in harmonic analysis===

Integration by parts is often used in [[harmonic analysis]], particularly [[Fourier analysis]], to show [[Riemann-Lebesgue lemma|that quickly oscillating integrals with sufficiently smooth integrands decay quickly]]. The most common example of this is its use in showing that the decay of function's Fourier transform depends on the smoothness of that function, as described below.

====Fourier transform of derivative====

If ''f'' is a ''k''-times continuously differentiable function and all derivatives up to the ''k''th one decay to zero at infinity, then its [[Fourier transform]] satisfies

:&lt;math&gt;(\mathcal{F}f^{(k)})(\xi) = (2\pi i\xi)^k \mathcal{F}f(\xi),&lt;/math&gt;

where {{nowrap|''f''&lt;sup&gt;(''k'')&lt;/sup&gt;}} is the ''k''th derivative of ''f''. (The exact constant on the right depends on the [[Fourier transform#Other conventions|convention of the Fourier transform used]].) This is proved by noting that

:&lt;math&gt;\frac{d}{dy} e^{-2\pi iy\xi} = -2\pi i\xi e^{-2\pi iy\xi},&lt;/math&gt;

so using integration by parts on the Fourier transform of the derivative we get

:&lt;math&gt;\begin{align}
(\mathcal{F}f')(\xi) &amp;= \int_{-\infty}^\infty e^{-2\pi iy\xi} f'(y)\,dy \\
&amp;=\left[e^{-2\pi iy\xi} f(y)\right]_{-\infty}^\infty - \int_{-\infty}^\infty (-2\pi i\xi e^{-2\pi iy\xi}) f(y)\,dy \\
&amp;=2\pi i\xi \int_{-\infty}^\infty e^{-2\pi iy\xi} f(y)\,dy \\
&amp;=2\pi i\xi \mathcal{F}f(\xi).
\end{align}&lt;/math&gt;

Applying this [[Mathematical induction|inductively]] gives the result for general ''k''. A similar method can be used to find the [[Laplace transform]] of a derivative of a function.

====Decay of Fourier transform====

The above result tells us about the decay of the Fourier transform, since it follows that if ''f'' and {{nowrap|''f''&lt;sup&gt;(''k'')&lt;/sup&gt;}} are integrable then

:&lt;math&gt;\vert\mathcal{F}f(\xi)\vert \leq \frac{I(f)}{1+\vert 2\pi\xi\vert^k}&lt;/math&gt;, where &lt;math&gt;I(f)=\int_{-\infty}^\infty\Bigl(\vert f(y)\vert + \vert f^{(k)}(y)\vert\Bigr) dy&lt;/math&gt;.

In other words, if ''f'' satisfies these conditions then its Fourier transform decays at infinity at least as quickly as {{nowrap|1/{{!}}''ξ''{{!}}&lt;sup&gt;''k''&lt;/sup&gt;}}. In particular, if {{nowrap|''k'' ≥ 2}} then the Fourier transform is integrable.

The proof uses the fact, which is immediate from the [[Fourier transform#Definition|definition of the Fourier transform]], that
:&lt;math&gt;\vert\mathcal{F}f(\xi)\vert \leq \int_{-\infty}^\infty \vert f(y) \vert \,dy.&lt;/math&gt;
Using the same idea on the equality stated at the start of this subsection gives
:&lt;math&gt;\vert(2\pi i\xi)^k \mathcal{F}f(\xi)\vert \leq \int_{-\infty}^\infty \vert f^{(k)}(y) \vert \,dy.&lt;/math&gt;
Summing these two inequalities and then dividing by {{nowrap|1 + {{!}}2''πξ''&lt;sup&gt;''k''&lt;/sup&gt;{{!}}}} gives the stated inequality.

===Use in operator theory===

One use of integration by parts in [[operator theory]] is that it shows that the {{nowrap|-∆}} (where ∆ is the [[Laplace operator]]) is a [[positive operator]] on {{nowrap|''L''&lt;sup&gt;2&lt;/sup&gt;}} (see [[Lp space|''L''&lt;sup&gt;''p''&lt;/sup&gt; space]]). If ''f'' is smooth and compactly supported then, using integration by parts, we have

:&lt;math&gt;\begin{align}
\langle -\Delta f, f \rangle_{L^2} &amp;= -\int_{-\infty}^\infty f''(x)\overline{f(x)}\,dx \\
&amp;=-\left[f'(x)\overline{f(x)}\right]_{-\infty}^\infty + \int_{-\infty}^\infty f'(x)\overline{f'(x)}\,dx \\
&amp;=\int_{-\infty}^\infty \vert f'(x)\vert^2\,dx \geq 0.
\end{align}&lt;/math&gt;

===Other applications===
&lt;!---INCLUDING DERIVATIONS HERE WOULD BE TOO LENGTHLY, IDEALLY KEEP THIS AS A LIST---&gt;
* For determining [[boundary condition]]s in [[Sturm–Liouville theory]]
* Deriving the [[Euler–Lagrange equation]] in the [[calculus of variations]]

==Repeated integration by parts==
Considering a second derivative of &lt;math&gt;v&lt;/math&gt; in the integral on the LHS of the formula for partial integration suggests a repeated application to the integral on the RHS:
:&lt;math&gt;\int u v''dx = uv' - \int u'v'dx = uv' - \left( u'v - \int u''vdx \right).&lt;/math&gt;

Extending this concept of repeated partial integration to derivatives of degree {{mvar|n}} leads to
:&lt;math&gt;\int u^{(0)} v^{(n)}dx = u^{(0)} v^{(n-1)} - u^{(1)}v^{(n-2)} + u^{(2)}v^{(n-3)} - \cdots + (-1)^{n-1}u^{(n-1)} v^{(0)} + (-1)^n \int u^{(n)} v^{(0)} dx.&lt;/math&gt;

This concept may be useful when the successive integrals of &lt;math&gt;v^{(n)}&lt;/math&gt; are readily available (e.g., plain exponentials or sine and cosine, as in  [[Laplace transform|Laplace]] or [[Fourier transform]]s), and when the {{mvar|n}}th derivative of &lt;math&gt;u&lt;/math&gt; vanishes (e.g., as a polynomial function with degree &lt;math&gt;(n-1)&lt;/math&gt;). The latter condition stops the repeating of partial integration, because the RHS-integral vanishes.

In the course of the above repetition of partial integrations the integrals 
:&lt;math&gt;\int u^{(0)} v^{(n)}dx \quad&lt;/math&gt; and &lt;math&gt;\quad \int u^{(\ell)} v^{(n-\ell)}dx \quad&lt;/math&gt; and &lt;math&gt;\quad \int u^{(m)} v^{(n-m)}dx \quad\text{ for } 1 \le m,\ell \le n&lt;/math&gt;
get related. This may be interpreted as arbitrarily "shifting" derivatives between &lt;math&gt;v&lt;/math&gt; and &lt;math&gt;u&lt;/math&gt; within the integrand, and proves useful, too (see [[Rodrigues' formula]]).

===Tabular integration by parts===
The essential process of the above formula can be summarized in a table; the resulting method is called "tabular integration"&lt;ref&gt;{{Cite journal |url=http://elib.mi.sanu.ac.rs/files/journals/tm/21/tm1125.pdf |first=Sanjay K. |last=Khattri |title=FOURIER SERIES AND LAPLACE TRANSFORM THROUGH TABULAR INTEGRATION |journal=[[The Teaching of Mathematics]] |volume=XI |issue=2 |year=2008 |pages=97–103}}&lt;/ref&gt; and was featured in the film ''[[Stand and Deliver]]''.&lt;ref&gt;{{Cite journal |url=https://www.maa.org/sites/default/files/pdf/mathdl/CMJ/Horowitz307-311.pdf |first=David |last=Horowitz |title=Tabular Integration by Parts |journal=[[The College Mathematics Journal]] |volume=21 |issue=4 |year=1990 |pages=307–311 |doi=10.2307/2686368 |jstor=2686368}}&lt;/ref&gt;

For example, consider the integral

:&lt;math&gt;\int x^3 \cos x \,dx \quad&lt;/math&gt; and take &lt;math&gt;\quad u^{(0)} = x^3, \quad v^{(n)} = \cos x.&lt;/math&gt;

Begin to list in column '''A''' the function &lt;math&gt;u^{(0)} = x^3&lt;/math&gt; and its subsequent derivatives &lt;math&gt;u^{(i)}&lt;/math&gt; until zero is reached. Then list in column '''B''' the function &lt;math&gt;v^{(n)} = \cos x&lt;/math&gt; and its subsequent integrals &lt;math&gt;v^{(n-i)}&lt;/math&gt; until the size of column '''B''' is the same as that of column '''A'''. The result is as follows:

:{| class="wikitable" style="text-align:center"
!# i !! Sign !! A: derivatives ''u''&lt;sup&gt;(i)&lt;/sup&gt;  !! B: integrals ''v''&lt;sup&gt;(''n''−''i'')&lt;/sup&gt;
|-
| 0 || + || &lt;math&gt;x^3&lt;/math&gt; || &lt;math&gt;\cos x&lt;/math&gt;
|-
| 1 || − || &lt;math&gt;3x^2&lt;/math&gt; || &lt;math&gt;\sin x&lt;/math&gt;
|-
| 2 || + || &lt;math&gt;6x&lt;/math&gt; || &lt;math&gt;-\cos x&lt;/math&gt;
|-
| 3 || − || &lt;math&gt;6&lt;/math&gt; || &lt;math&gt;-\sin x&lt;/math&gt;
|-
| 4 || + || &lt;math&gt;0&lt;/math&gt; || &lt;math&gt;\cos x&lt;/math&gt;
|}

The product of the entries in {{nowrap|row {{mvar|i}}}} of columns '''A''' and '''B''' together with the respective sign give the relevant integrals in {{nowrap|step {{mvar|i}}}} in the course of repeated integration by parts. {{nowrap|Step {{math|''i'' {{=}} 0}}}} yields the original integral. For the complete result in {{nowrap|step {{math|''i'' &gt; 0}}}} the {{nowrap|{{mvar|i}}th integral}} must be added to all the previous products ({{math|0 ≤ ''j'' &lt; ''i''}}) of the {{nowrap|{{mvar|j}}th entry}} of column A and the {{nowrap|{{math|(''j'' + 1)}}st entry}} of column B (i.e., multiply the 1st entry of column A with the 2nd entry of column B, the 2nd entry of column A with the 3rd entry of column B, etc ...) with the given {{nowrap|{{mvar|j}}th sign.}} This process comes to a natural halt, when the product, which yields the integral, is zero ({{math|''i'' {{=}} 4}} in the example).  The complete result is the following (notice the alternating signs in each term):

:&lt;math&gt;\underbrace{(+1)(x^3)(\sin x)}_{j=0} + \underbrace{(-1)(3x^2)(-\cos x)}_{j=1} + \underbrace{(+1)(6x)(-\sin x)}_{j=2} +\underbrace{(-1)(6)(\cos x)}_{j=3}+ \underbrace{\int(+1)(0)(\cos x) \,dx}_{i=4: \;\to \;C}.&lt;/math&gt;

This yields

:&lt;math&gt;\underbrace{\int x^3 \cos x \,dx}_{\text{step 0}} = x^3\sin x + 3x^2\cos x - 6x\sin x - 6\cos x + C. &lt;/math&gt;

The repeated partial integration also turns out useful, when in the course of respectively differentiating and integrating the functions &lt;math&gt;u^{(i)}&lt;/math&gt; and  &lt;math&gt;v^{(n-i)}&lt;/math&gt;  their product results in a multiple of the original integrand. In this case the repetition may also be terminated with this index {{mvar|i.}}This can happen, expectably, with exponentials and trigonometric functions. As an example consider

:&lt;math&gt;\int e^x \cos x \,dx. &lt;/math&gt;

:{| class="wikitable" style="text-align:center"
!# i !! Sign !! A: derivatives ''u''&lt;sup&gt;(i)&lt;/sup&gt; !! B: integrals ''v''&lt;sup&gt;(''n''−''i'')&lt;/sup&gt;
|-
| 0 || + || &lt;math&gt;e^x&lt;/math&gt; || &lt;math&gt;\cos x&lt;/math&gt;
|-
| 1 || − || &lt;math&gt;e^x&lt;/math&gt; || &lt;math&gt;\sin x&lt;/math&gt;
|-
| 2 || + || &lt;math&gt;e^x&lt;/math&gt; || &lt;math&gt;-\cos x&lt;/math&gt;
|}

In this case the product of the terms in columns '''A''' and '''B''' with the appropriate sign for index {{math|''i'' {{=}} 2}} yields the negative of the original integrand (compare {{nowrap|rows {{math|''i'' {{=}} 0}}}} {{nowrap|and {{math|''i'' {{=}} 2}}).}}

:&lt;math&gt; \underbrace{\int e^x \cos x \,dx}_{\text{step 0}} = \underbrace{(+1)(e^x)(\sin x)}_{j=0} + \underbrace{(-1)(e^x)(-\cos x)}_{j=1} + \underbrace{\int(+1)(e^x)(-\cos x) \,dx}_{i= 2}. &lt;/math&gt;

Observing that the integral on the RHS can have its own constant of integration &lt;math&gt;C'&lt;/math&gt;, and bringing the abstract integral to the other side, gives

:&lt;math&gt; 2 \int e^x \cos x \,dx = e^x\sin x + e^x\cos x + C', &lt;/math&gt;

and finally:

:&lt;math&gt;\int e^x \cos x \,dx = \frac 12 \left(e^x ( \sin x + \cos x ) \right) + C,&lt;/math&gt;

where ''C'' = ''C''′/2.

==Higher dimensions==
The formula for integration by parts can be extended to functions of several variables. These derivations are analogous to the one given above: a fundamental theorem of calculus is substituted into an appropriate product rule. There are several such pairings possible in multivariate calculus.&lt;ref&gt;{{Cite web|url=http://www.math.nagoya-u.ac.jp/~richard/teaching/s2016/Ref2.pdf|title=The Calculus of Several Variables|last=Rogers|first=Robert C.|date=September 29, 2011|website=|archive-url=|archive-date=|dead-url=}}&lt;/ref&gt; For example, we may begin with a product rule for divergence followed by the divergence theorem.

A [[Vector calculus identities#Properties|product rule for divergence]]:

:&lt;math&gt;\nabla \cdot ( \varphi \vec v ) = \varphi\ ( \nabla \cdot \vec v) \ +\  \vec v\cdot (\nabla \varphi)&lt;/math&gt;

Instead of an interval we integrate over an ''n''-dimensional domain &lt;math&gt;
 \Omega
&lt;/math&gt;: 

:&lt;math&gt;
\int_\Omega \varphi\, \operatorname{div} \vec v \, dV = \int_{\Omega} \nabla\cdot (\varphi\, \vec v) dV - \int_\Omega \vec v\cdot \operatorname{grad} \varphi \, dV
&lt;/math&gt;

After substitution using the [[divergence theorem]] we arrive at:

:&lt;math&gt;
 \int_\Omega \varphi\, \operatorname{div} \vec v \, dV = \int_{\partial \Omega} \varphi\, \vec v \cdot d \vec S - \int_\Omega \vec v\cdot \operatorname{grad} \varphi \, dV.
&lt;/math&gt;

More specifically, suppose Ω is an [[Open set|open]] [[bounded set|bounded subset]] of ℝ&lt;sup&gt;''n''&lt;/sup&gt; with a [[piecewise smooth]] [[boundary (topology)|boundary]] Γ. If ''u'' and ''v'' are two [[smooth function|continuously differentiable]] functions on the [[Closure (topology)|closure]] of Ω, then the formula for integration by parts is
:&lt;math&gt;\int_{\Omega} \frac{\partial u}{\partial x_i} v \,d\Omega = \int_{\Gamma} u v \, \hat\nu_i \,d\Gamma - \int_{\Omega} u \frac{\partial v}{\partial x_i} \, d\Omega,&lt;/math&gt;
where &lt;math&gt;\hat{\mathbf{\nu}}&lt;/math&gt; is the outward unit [[surface normal]] to Γ, &lt;math&gt;\hat\nu_i&lt;/math&gt; is its ''i''-th component, and ''i'' ranges from 1 to ''n''. In vector form, the equation reads
:&lt;math&gt;\int_{\Omega}  v \nabla u \,d\Omega = \int_{\Gamma} u v \, \mathbf{\hat\nu} \,d\Gamma - \int_{\Omega} u \nabla v \, d\Omega,&lt;/math&gt;

Replacing ''v'' in the component formula with ''v''&lt;sub&gt;''i''&lt;/sub&gt; and summing over ''i'' gives the vector formula
:&lt;math&gt; \int_{\Omega} \nabla u \cdot \mathbf{v}\, d\Omega = \int_{\Gamma} u (\mathbf{v}\cdot \hat{\nu})\, d\Gamma - \int_\Omega u\, \nabla\cdot\mathbf{v}\, d\Omega,&lt;/math&gt;
where '''v''' is a vector-valued function with components ''v''&lt;sub&gt;1&lt;/sub&gt;, ..., ''v''&lt;sub&gt;''n.''&lt;/sub&gt;

For &lt;math&gt;\mathbf{v}=\nabla v&lt;/math&gt; where &lt;math&gt;v\in C^2(\bar{\Omega})&lt;/math&gt;, one gets
:&lt;math&gt; \int_{\Omega} \nabla u \cdot \nabla v\, d\Omega = \int_{\Gamma} u\, \nabla v\cdot\hat{\nu}\, d\Gamma - \int_\Omega u\, \nabla^2 v\, d\Omega,&lt;/math&gt;
which is the [[Green's identities|first Green's identity]].

The [[Differentiability class|regularity]] requirements of the theorem can be relaxed. For instance, the boundary Γ need only be [[Lipschitz continuous]]. In the first formula above, only ''u'', ''v'' ∈ ''H''&lt;sup&gt;1&lt;/sup&gt;(Ω) is necessary (where ''H''&lt;sup&gt;1&lt;/sup&gt; is a [[Sobolev space]]); the other formulas have similarly relaxed requirements.

==See also==
* [[Lebesgue–Stieltjes integral#Integration by parts|Integration by parts for the Lebesgue–Stieltjes integral]]
* [[Quadratic variation#Semimartingales|Integration by parts]] for [[semimartingale]]s, involving their quadratic covariation.
* [[Integration by substitution]]
* [[Legendre transformation]]

==Notes==
&lt;references /&gt;

==References==
* {{Cite book| first=Lawrence C. | last=Evans | title=Partial Differential Equations | publisher=American Mathematical Society | location=Providence, Rhode Island | year=1998 | isbn=0-8218-0772-2}}
* {{Cite book| first1=Todd | last1=Arbogast|author1-link= Todd Arbogast |last2=Bona|first2=Jerry | title=Methods of Applied Mathematics | url=http://www.math.utexas.edu/users/arbogast/appMath08.pdf |format=PDF| year=2005}}
* {{Cite journal| last = Horowitz | first = David | title = Tabular Integration by Parts | journal = The College Mathematics Journal | volume = 21 | issue = 4 | pages = 307–311 |date=September 1990 | doi = 10.2307/2686368 | jstor = 2686368}}

==External links==
{{wikibooks|Calculus|Integration techniques/Integration by Parts|Integration by parts}}
* {{springer|title=Integration by parts|id=p/i051730}}
* [http://mathworld.wolfram.com/IntegrationbyParts.html Integration by parts—from MathWorld]

[[Category:Integral calculus]]

[[es:Métodos de integración#Método de integración por partes]]</text>
      <sha1>os62yugxd515s2bvq0uxtu0qkq2zuw9</sha1>
    </revision>
  </page>
  <page>
    <title>Lagrangian (field theory)</title>
    <ns>0</ns>
    <id>40752010</id>
    <revision>
      <id>870520137</id>
      <parentid>870152311</parentid>
      <timestamp>2018-11-25T10:12:02Z</timestamp>
      <contributor>
        <ip>193.248.50.209</ip>
      </contributor>
      <comment>/* Action */It was general non-sense.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="22540">'''Lagrangian field theory''' is a formalism in [[classical field theory]]. It is the field-theoretic analogue of [[Lagrangian mechanics]]. Lagrangian mechanics is used for discrete particles each with a finite number of [[degrees of freedom (physics and chemistry)|degrees of freedom]]. Lagrangian field theory applies to continua and fields, which have an infinite number of degrees of freedom.

This article uses &lt;math&gt;\mathcal{L}&lt;/math&gt; for the Lagrangian density, and ''L'' for the Lagrangian. 

The Lagrangian mechanics formalism was generalized further to handle [[classical field theory|field theory]]. In field theory, the independent variable is replaced by an event in [[spacetime]] (''x'', ''y'', ''z'', ''t''), or more generally still by a point ''s'' on a [[manifold]]. The dependent variables (''q'') are replaced by the value of a field at that point in spacetime &lt;math&gt;\varphi (x, y, z, t)&lt;/math&gt; so that the [[equations of motion]] are obtained by means of an [[action (physics)|action]] principle, written as:

:&lt;math&gt;\frac{\delta \mathcal{S}}{\delta \varphi_i} = 0,\,&lt;/math&gt;

where the ''action'', &lt;math&gt;\mathcal{S}&lt;/math&gt;, is a [[functional (mathematics)|functional]] of the dependent variables &lt;math&gt;\varphi_i (s) &lt;/math&gt; with their derivatives and ''s'' itself

:&lt;math&gt;\mathcal{S}\left[\varphi_i\right] 
= 
\int{ \mathcal{L} \left(\varphi_i (s), 
\left\{ \frac{\partial\varphi_i(s)}{\partial s^\alpha} \right\}, 
\{ s^\alpha \} \right) \, \mathrm{d}^n s }&lt;/math&gt;,

where the brackets denote &lt;math&gt;\{\cdot~\forall\alpha\}&lt;/math&gt;;
and ''s'' = {''s&lt;sup&gt;α&lt;/sup&gt;''} denotes the [[Set (mathematics)|set]] of ''n'' [[independent variable]]s of the system, including the time variable, and is indexed by ''α'' = 1, 2, 3,..., ''n''. Notice that the calligraphic typeface, &lt;math&gt;\mathcal{L}&lt;/math&gt;, is used to denote volume density, where volume is the integral measure of the domain of the field function, i.e. &lt;math&gt;\mathrm{d}^n s&lt;/math&gt;.

==Definitions==

In Lagrangian field theory, the Lagrangian as a function of [[generalized coordinates]] is replaced by a Lagrangian density, a function of the fields in the system and their derivatives, and possibly the space and time coordinates themselves. In field theory, the independent variable ''t'' is replaced by an event in spacetime (''x'', ''y'', ''z'', ''t'') or still more generally by a point ''s'' on a manifold. 

Often, a "Lagrangian density" is simply referred to as a "Lagrangian".

===Scalar fields===

For one scalar field &lt;math&gt;\varphi&lt;/math&gt;, the Lagrangian density will take the form:&lt;ref group=nb&gt;It is a standard abuse of notation to abbreviate all the derivatives and coordinates in the Lagrangian density as follows:
:&lt;math&gt;\mathcal{L} (\varphi, \partial_\mu \varphi, x_\mu)&lt;/math&gt;
see [[four-gradient]]. The {{math|''μ''}} is an index which takes values 0 (for the time coordinate), and 1, 2, 3 (for the spatial coordinates), so strictly only one derivative or coordinate would be present. In general, all the spatial and time derivatives will appear in the Lagrangian density, for example in Cartesian coordinates, the Lagrangian density has the full form:
:&lt;math&gt;\mathcal{L} \left(\varphi, \frac{\partial \varphi}{\partial x}, \frac{\partial \varphi}{\partial y}, \frac{\partial \varphi}{\partial z}, \frac{\partial \varphi}{\partial t}, x,y,z,t\right)&lt;/math&gt;
Here we write the same thing, but using ∇ to abbreviate all spatial derivatives as a vector.&lt;/ref&gt;&lt;ref name=Mandl&gt;Mandl F., Shaw G., ''Quantum Field Theory'', chapter 2&lt;/ref&gt; 

:&lt;math&gt;\mathcal{L}(\varphi, \nabla\varphi, \partial \varphi/\partial t , \mathbf{x},t)&lt;/math&gt;

For many scalar fields

:&lt;math&gt;\mathcal{L}(\varphi_1, \nabla\varphi_1, \partial \varphi_1/\partial t ,\ldots,\varphi_2, \nabla\varphi_2, \partial \varphi_2/\partial t ,\ldots, \mathbf{x},t)&lt;/math&gt;

===Vector fields, tensor fields, spinor fields===

The above can be generalized for [[vector field]]s, [[tensor field]]s, and [[spinor field]]s. In physics [[fermion]]s are described by spinor fields and [[boson]]s by tensor fields.

===Action===

The [[time integral]] of the Lagrangian is called the [[action (physics)|action]] denoted by ''S''. In field theory, a distinction is occasionally made between the '''Lagrangian''' ''L'', of which the time integral is the action

:&lt;math&gt;\mathcal{S} = \int L \, \mathrm{d}t \,,&lt;/math&gt;

and the '''Lagrangian density''' &lt;math&gt;\mathcal{L}&lt;/math&gt;, which one integrates over all [[spacetime]] to get the action:

:&lt;math&gt;\mathcal{S} [\varphi] = \int \mathcal{L} (\varphi,\nabla\varphi,\partial\varphi/\partial t , \mathbf{x},t) \, \mathrm{d}^3 \mathbf{x} \, \mathrm{d}t .&lt;/math&gt;

The spatial [[volume integral]] of the Lagrangian density is the Lagrangian, in 3d

:&lt;math&gt;L = \int \mathcal{L} \, \mathrm{d}^3 \mathbf{x} \,.&lt;/math&gt;

Note, in the presence of gravity or when using general curvilinear coordinates, the Lagrangian density &lt;math&gt;\mathcal{L}&lt;/math&gt; will include a factor of {{math|{{sqrt|''g''}}}}, making it a [[tensor density|scalar density]]. This procedure ensures that the action &lt;math&gt;\mathcal{S}&lt;/math&gt; is invariant under general coordinate transformations.

===Mathematical formalism===

Suppose we have an ''n''-dimensional [[manifold]], ''M'', and a target manifold, ''T''. Let &lt;math&gt;\mathcal{C}&lt;/math&gt; be the configuration space of [[smooth function]]s from ''M'' to ''T''.

In field theory, ''M'' is the [[spacetime]] manifold and the target space is the set of values the fields can take at any given point. For example, if there are &lt;math&gt;m&lt;/math&gt; [[real number|real]]-valued [[scalar field]]s, &lt;math&gt;\varphi_1, \dots, \varphi_m&lt;/math&gt;, then the target manifold is &lt;math&gt;\mathbb{R}^m&lt;/math&gt;. If the field is a real [[vector field]], then the target manifold is [[isomorphic]] to &lt;math&gt;\mathbb{R}^n&lt;/math&gt;. Note that there is also an elegant formalism{{which|date=September 2016}} for this, using [[tangent bundle]]s over ''M''.

Consider a [[Functional (mathematics)|functional]], 
:&lt;math&gt;\mathcal{S}:\mathcal{C}\rightarrow \mathbb{R}&lt;/math&gt;,
called the [[Action (physics)|action]].

In order for the action to be [[Functional (mathematics)#Locality|local]], we need additional restrictions on the [[action (physics)|action]]. If &lt;math&gt;\varphi\ \in\ \mathcal{C}&lt;/math&gt;, we assume &lt;math&gt;\mathcal{S}[\varphi]&lt;/math&gt; is the [[integral]] over ''M'' of a function of &lt;math&gt;\varphi&lt;/math&gt;, its [[derivative]]s and the position called the '''Lagrangian''', &lt;math&gt;\mathcal{L}(\varphi,\partial\varphi,\partial\partial\varphi, ...,x)&lt;/math&gt;. In other words,

:&lt;math&gt;\forall\varphi\in\mathcal{C}, \ \ \mathcal{S}[\varphi]\equiv\int_M \mathcal{L} \big( \varphi(x),\partial\varphi(x),\partial\partial\varphi(x), ...,x \big) \mathrm{d}^nx .&lt;/math&gt;

It is assumed below, in addition, that the Lagrangian depends on only the field value and its first derivative but not the higher derivatives.

Given [[boundary condition]]s, basically a specification of the value of &lt;math&gt;\varphi&lt;/math&gt; at the [[Boundary (topology)|boundary]] if ''M'' is [[Compact space|compact]] or some limit on &lt;math&gt;\varphi&lt;/math&gt; as ''x'' → ∞ (this will help in doing [[integration by parts]]), the [[subspace topology|subspace]] of &lt;math&gt;\mathcal{C}&lt;/math&gt; consisting of functions, &lt;math&gt;\varphi&lt;/math&gt;, such that all [[functional derivative]]s of ''S'' at &lt;math&gt;\varphi&lt;/math&gt; are zero and &lt;math&gt;\varphi&lt;/math&gt; satisfies the given boundary conditions is the subspace of [[on shell]] solutions.

From this we get: 

:&lt;math&gt;0 = \frac{\delta\mathcal{S}}{\delta\varphi} = \int_M \left(-\partial_\mu
 \left(\frac{\partial\mathcal{L}}{\partial(\partial_\mu\varphi)}\right)+ \frac{\partial\mathcal{L}}{\partial\varphi}\right) \mathrm{d}^nx .&lt;/math&gt;

The left hand side is the [[functional derivative]] of the [[action (physics)|action]] with respect to &lt;math&gt;\varphi&lt;/math&gt;.

Hence we get the [[Euler–Lagrange equations]] (due to the [[boundary condition]]s):

:&lt;math&gt;\frac{\partial\mathcal{L}}{\partial\varphi}=\partial_\mu
 \left(\frac{\partial\mathcal{L}}{\partial(\partial_\mu\varphi)}\right) .&lt;/math&gt;

==Examples==

To go with the section on test particles above, here are the equations for the fields in which they move. The equations below pertain to the fields in which the test particles described above move and allow the calculation of those fields. The equations below will not give you the equations of motion of a test particle in the field but will instead give you the potential (field) induced by quantities such as mass or charge density at any point &lt;math&gt;(\mathbf{x},t)&lt;/math&gt;. For example, in the case of Newtonian gravity, the Lagrangian density integrated over spacetime gives you an equation which, if solved, would yield &lt;math&gt;\Phi (\mathbf{x},t)&lt;/math&gt;. This &lt;math&gt;\Phi (\mathbf{x},t)&lt;/math&gt;, when substituted back in equation ({{EquationNote|1}}), the Lagrangian equation for the test particle in a Newtonian gravitational field, provides the information needed to calculate the acceleration of the particle.

===Newtonian gravity===

The density &lt;math&gt;\mathcal{L}&lt;/math&gt; has units of J·m&lt;sup&gt;−3&lt;/sup&gt;. The interaction term ''mΦ'' is replaced by a term involving a continuous mass density ''ρ'' in kg·m&lt;sup&gt;−3&lt;/sup&gt;. This is necessary because using a point source for a field would result in mathematical difficulties. The resulting Lagrangian for the classical gravitational field is:

:&lt;math&gt;\mathcal{L}(\mathbf{x},t)= - \rho (\mathbf{x},t) \Phi (\mathbf{x},t) - {1 \over 8 \pi G} (\nabla \Phi (\mathbf{x},t))^2 &lt;/math&gt;

where ''G'' in m&lt;sup&gt;3&lt;/sup&gt;·kg&lt;sup&gt;−1&lt;/sup&gt;·s&lt;sup&gt;−2&lt;/sup&gt; is the [[gravitational constant]]. Variation of the integral with respect to ''Φ'' gives:

:&lt;math&gt;\delta \mathcal{L}(\mathbf{x},t) = - \rho (\mathbf{x},t) \delta\Phi (\mathbf{x},t) - {2 \over 8 \pi G} (\nabla \Phi (\mathbf{x},t)) \cdot (\nabla \delta\Phi (\mathbf{x},t)) .&lt;/math&gt;

Integrate by parts and discard the total integral. Then divide out by ''δΦ'' to get:

:&lt;math&gt;0 = - \rho (\mathbf{x},t) + {1 \over 4 \pi G} \nabla \cdot \nabla \Phi (\mathbf{x},t) &lt;/math&gt;

and thus

:&lt;math&gt;4 \pi G \rho (\mathbf{x},t) = \nabla^2 \Phi (\mathbf{x},t) &lt;/math&gt;

which yields [[Gauss's law for gravity]].

===Einstein gravity===
{{main|Einstein–Hilbert action}}
The Lagrange density for general relativity in the presence of matter fields is
:&lt;math&gt;\mathcal{L}_\text{GR}=\mathcal{L}_\text{EH}+\mathcal{L}_\text{matter}=\frac{c^4}{16\pi G}\left(R-2\Lambda\right)+\mathcal{L}_\text{matter}&lt;/math&gt;
&lt;math&gt;R&lt;/math&gt; is the [[curvature scalar]], which is the [[Ricci tensor]] contracted with the [[metric tensor]], and the [[Ricci tensor]] is the [[Riemann tensor]] contracted with a [[Kronecker delta]]. The integral of &lt;math&gt; \mathcal{L}_\text{EH}&lt;/math&gt; is known as the [[Einstein-Hilbert action]]. The Riemann tensor is the [[tidal force]] tensor, and is constructed out of [[Christoffel symbols]] and derivatives of Christoffel symbols, which are the gravitational force field. Plugging this Lagrangian into the Euler-Lagrange equation and taking the metric tensor &lt;math&gt; g_{\mu\nu}&lt;/math&gt; as the field, we obtain the [[Einstein field equations]] 
:&lt;math&gt; R_{\mu\nu}-\frac{1}{2}Rg_{\mu\nu}+g_{\mu\nu}\Lambda=\frac{8\pi G}{c^4}T_{\mu\nu} &lt;/math&gt;
The last tensor is the [[energy momentum tensor]] and is defined by
:&lt;math&gt;T_{\mu\nu} \equiv \frac{-2}{\sqrt{-g}}\frac{\delta (\mathcal{L}_{\mathrm{matter}} \sqrt{-g}) }{\delta g^{\mu\nu}} = -2 \frac{\delta \mathcal{L}_\mathrm{matter}}{\delta g^{\mu\nu}} + g_{\mu\nu} \mathcal{L}_\mathrm{matter}.&lt;/math&gt;
&lt;math&gt;g&lt;/math&gt; is the determinant of the metric tensor when regarded as a matrix. &lt;math&gt;\Lambda&lt;/math&gt; is the [[cosmological constant]]. Generally, in general relativity, the integration measure of the action of Lagrange density is &lt;math&gt;\sqrt{-g}d^4x &lt;/math&gt;. This makes the integral coordinate independent, as the root of the metric determinant is equivalent to the [[Jacobian determinant]]. The minus sign is a consequence of the metric signature (the determinant by itself is negative).&lt;ref&gt;{{cite book|last1=Zee|first1=A.|title=Einstein gravity in a nutshell|date=2013|publisher=Princeton University Press|location=Princeton|isbn=9780691145587|pages=344–390}}&lt;/ref&gt;

===Electromagnetism in special relativity===
{{main|Covariant formulation of classical electromagnetism}}

The interaction terms 
:&lt;math&gt;- q \phi (\mathbf{x}(t),t) + q \dot{\mathbf{x}}(t) \cdot \mathbf{A} (\mathbf{x}(t),t)&lt;/math&gt;
are replaced by terms involving a continuous charge density ρ in A·s·m&lt;sup&gt;−3&lt;/sup&gt; and current density &lt;math&gt;\mathbf{j}&lt;/math&gt; in A·m&lt;sup&gt;−2&lt;/sup&gt;. The resulting Lagrangian for the electromagnetic field is:

:&lt;math&gt;\mathcal{L}(\mathbf{x},t) = - \rho (\mathbf{x},t) \phi (\mathbf{x},t) + \mathbf{j} (\mathbf{x},t) \cdot \mathbf{A} (\mathbf{x},t) + {\epsilon_0 \over 2} {E}^2 (\mathbf{x},t) - {1 \over {2 \mu_0}} {B}^2 (\mathbf{x},t) .&lt;/math&gt;

Varying this with respect to ϕ, we get

:&lt;math&gt;0 = - \rho (\mathbf{x},t) + \epsilon_0 \nabla \cdot \mathbf{E} (\mathbf{x},t) &lt;/math&gt;

which yields [[Gauss' law]].

Varying instead with respect to &lt;math&gt;\mathbf{A}&lt;/math&gt;, we get

:&lt;math&gt;0 = \mathbf{j} (\mathbf{x},t) + \epsilon_0 \dot{\mathbf{E}} (\mathbf{x},t) - {1 \over \mu_0} \nabla   \times \mathbf{B} (\mathbf{x},t) &lt;/math&gt;

which yields [[Ampère's law]].

Using [[tensor notation]], we can write all this more compactly. The term &lt;math&gt; - \rho \phi (\mathbf{x},t) + \mathbf{j} \cdot \mathbf{A}    &lt;/math&gt; is actually the inner product of two [[four-vector]]s. We package the charge density into the current 4-vector and the potential into the potential 4-vector. These two new vectors are
:&lt;math&gt; j^\mu = (\rho,\mathbf{j})\quad\text{and}\quad A_\mu = (-\phi,\mathbf{A}) &lt;/math&gt;
We can then write the interaction term as
:&lt;math&gt; - \rho \phi  + \mathbf{j} \cdot \mathbf{A}  = j^\mu A_\mu &lt;/math&gt;
Additionally, we can package the E and B fields into what is known as the [[electromagnetic tensor]] &lt;math&gt; F_{\mu\nu} &lt;/math&gt;.
We define this tensor as
:&lt;math&gt;  F_{\mu\nu}=\partial_\mu A_\nu-\partial_\nu A_\mu &lt;/math&gt;
The term we are looking out for turns out to be
:&lt;math&gt; {\epsilon_0 \over 2} {E}^2 - {1 \over {2 \mu_0}} {B}^2 =  -\frac{1}{4\mu_0} F_{\mu\nu}F^{\mu\nu}= -\frac{1}{4\mu_0} F_{\mu\nu} F_{\rho\sigma}\eta^{\mu\rho}\eta^{\nu\sigma}&lt;/math&gt;
We have made use of the [[Minkowski metric]] to raise the indices on the EMF tensor. In this notation, Maxwell's equations are
:&lt;math&gt; \partial_\mu F^{\mu\nu}=-\mu_0 j^\nu\quad\text{and}\quad \epsilon^{\mu\nu\lambda\sigma}\partial_\nu F_{\lambda\sigma}=0 &lt;/math&gt;
where ε is the [[Levi-Civita tensor]]. So the Lagrange density for electromagnetism in special relativity written in terms of Lorentz vectors and tensors is
:&lt;math&gt; \mathcal{L}(x) = j^\mu(x) A_\mu(x) - \frac{1}{4\mu_0} F_{\mu\nu}(x) F^{\mu\nu}(x) &lt;/math&gt;
In this notation it is apparent that classical electromagnetism is a Lorentz-invariant theory. By the [[equivalence principle]], it becomes simple to extend the notion of electromagnetism to curved spacetime.&lt;ref&gt;{{cite book|last1=Zee|first1=A.|title=Einstein gravity in a nutshell|date=2013|publisher=Princeton University Press|location=Princeton|isbn=9780691145587|pages=244–253}}&lt;/ref&gt;&lt;ref&gt;{{cite book|last1=Mexico|first1=Kevin Cahill, University of New|title=Physical mathematics|date=2013|publisher=Cambridge University Press|location=Cambridge|isbn=9781107005211|edition=Repr.}}&lt;/ref&gt;

===Electromagnetism in general relativity===
{{main|Maxwell's equations in curved spacetime}}
The Lagrange density of electromagnetism in general relativity also contains the Einstein-Hilbert action from above. The pure electromagnetic Lagrangian is precisely a matter Lagrangian &lt;math&gt; \mathcal{L}_\text{matter}&lt;/math&gt;. The Lagrangian is

:&lt;math&gt;\begin{align}\mathcal{L}(x) &amp;= j^\mu (x) A_\mu (x) 
 - {1 \over 4\mu_0} F_{\mu \nu}(x) F_{\rho\sigma}(x) g^{\mu\rho}(x) g^{\nu\sigma}(x) + \frac{c^4}{16\pi G}R(x)\\ &amp;= \mathcal{L}_\text{Maxwell} + \mathcal{L}_\text{Einstein-Hilbert}.\end{align}&lt;/math&gt;

This Lagrangian is obtained by simply replacing the Minkowski metric in the above flat Lagrangian with a more general (possibly curved) metric &lt;math&gt; g_{\mu\nu}(x)&lt;/math&gt;. We can generate the Einstein Field Equations in the presence of an EM field using this lagrangian. The energy-momentum tensor is
:&lt;math&gt;  T^{\mu\nu}(x)=\frac{2}{\sqrt{-g(x)}}\frac{\delta}{\delta g_{\mu\nu}(x)}\mathcal{S}_\text{Maxwell}=\frac{1}{\mu_{0}}\left(F^{\mu}_{\text{ }\lambda}(x)F^{\nu\lambda}(x)-\frac{1}{4}g^{\mu\nu}(x)F_{\rho\sigma}(x)F^{\rho\sigma}(x)\right) &lt;/math&gt;
It can be shown that this energy momentum tensor is traceless, i.e. that
:&lt;math&gt;  T=g_{\mu\nu}T^{\mu\nu}=0 &lt;/math&gt;
If we take the trace of both sides of the Einstein Field Equations, we obtain
:&lt;math&gt;  R=-\frac{8\pi G}{c^4}T &lt;/math&gt;
So the tracelessness of the energy momentum tensor implies that the curvature scalar in an electromagnetic field vanishes. The Einstein equations are then
:&lt;math&gt; R^{\mu\nu}=\frac{8\pi G}{c^4}\frac{1}{\mu_{0}}\left(F^{\mu}_{\text{ }\lambda}(x)F^{\nu\lambda}(x)-\frac{1}{4}g^{\mu\nu}(x)F_{\rho\sigma}(x)F^{\rho\sigma}(x)\right) &lt;/math&gt;
Additionally, Maxwell's equations are
:&lt;math&gt; D_{\mu}F^{\mu\nu}=-\mu_0 j^\nu  &lt;/math&gt;
where &lt;math&gt;D_\mu&lt;/math&gt; is the [[covariant derivative]]. For free space, we can set the current tensor equal to zero, &lt;math&gt; j^\mu=0  &lt;/math&gt;. Solving both Einstein and Maxwell's equations around a spherically symmetric mass distribution in free space leads to the [[Reissner–Nordström black hole|Reissner–Nordström charged black hole]], with the defining line element (written in [[natural units]] and with charge Q):&lt;ref&gt;{{cite book|last1=Zee|first1=A.|title=Einstein gravity in a nutshell|date=2013|publisher=Princeton University Press|location=Princeton|isbn=9780691145587|pages=381–383, 477–478}}&lt;/ref&gt;

:&lt;math&gt; \mathrm{d}s^2=\left(1-\frac{2M}{r}+\frac{Q^2}{r^2}\right)\mathrm{d}t^2- \left(1-\frac{2M}{r}+\frac{Q^2}{r^2}\right)^{-1}\mathrm{d}r^2 -r^2\mathrm{d}\Omega^2&lt;/math&gt;

One possible way of unifying the electromagnetic and gravitational Lagrangians (by using a fifth dimension) is given by [[Kaluza-Klein theory]].

===Electromagnetism using differential forms===
Using [[differential forms]], the electromagnetic action ''S'' in vacuum on a (pseudo-) Riemannian manifold &lt;math&gt;\mathcal M&lt;/math&gt; can be written (using [[natural units]], {{nowrap|1=''c'' = ''ε''&lt;sub&gt;0&lt;/sub&gt; = 1}}) as
:&lt;math&gt;\mathcal S[\mathbf{A}] = -\int_{\mathcal{M}} \left(\frac{1}{2}\,\mathbf{F} \wedge \star\mathbf{F} + \mathbf{A} \wedge\star \mathbf{J}\right) .&lt;/math&gt;
Here, '''A''' stands for the electromagnetic potential 1-form, '''J''' is the current 1-form, '''F''' is the field strength 2-form and the star denotes the [[Hodge star]] operator. This is exactly the same Lagrangian as in the section above, except that the treatment here is coordinate-free; expanding the integrand into a basis yields the identical, lengthy expression. Note that with forms, an additional integration measure is not necessary because forms have coordinate differentials built in. Variation of the action leads to
:&lt;math&gt;\mathrm{d} {\star}\mathbf{F} = {\star}\mathbf{J} .&lt;/math&gt;
These are Maxwell's equations for the electromagnetic potential. Substituting {{nowrap|1='''F''' = d'''A'''}} immediately yields the equation for the fields,
:&lt;math&gt;\mathrm{d}\mathbf{F} = 0&lt;/math&gt;
because '''F''' is an [[exact form]].

===Dirac Lagrangian===
The Lagrangian density for a [[Dirac field]] is:&lt;ref&gt;Itzykson-Zuber, eq. 3-152&lt;/ref&gt;

:&lt;math&gt;\mathcal{L} = i \hbar c \bar \psi {\partial}\!\!\!/\ \psi - mc^2 \bar\psi \psi&lt;/math&gt;

where ''ψ'' is a [[Dirac spinor]] ([[annihilation operator]]), &lt;math&gt;\bar \psi = \psi^\dagger \gamma^0&lt;/math&gt; is its [[Dirac adjoint]] ([[creation operator]]), and &lt;math&gt;{\partial}\!\!\!/&lt;/math&gt; is [[Feynman slash notation]] for &lt;math&gt;\gamma^\sigma \partial_\sigma\!&lt;/math&gt;.

===Quantum electrodynamic Lagrangian===
The Lagrangian density for [[Quantum electrodynamics|QED]] is:

:&lt;math&gt;\mathcal{L}_{\mathrm{QED}} = i\hbar c \bar \psi {D}\!\!\!\!/\ \psi - mc^2 \bar\psi \psi - {1 \over 4\mu_0} F_{\mu \nu} F^{\mu \nu}&lt;/math&gt;

where &lt;math&gt;F^{\mu \nu}\!&lt;/math&gt; is the [[electromagnetic tensor]], ''D'' is the [[gauge covariant derivative]], and &lt;math&gt;{D}\!\!\!\!/&lt;/math&gt; is [[Feynman slash notation|Feynman notation]] for &lt;math&gt;\gamma^\sigma D_\sigma\!&lt;/math&gt; with &lt;math&gt; D_\sigma = \partial_\sigma - i e A_\sigma &lt;/math&gt; where &lt;math&gt;A_\sigma&lt;/math&gt; is the [[electromagnetic four-potential]].

===Quantum chromodynamic Lagrangian===
The Lagrangian density for [[quantum chromodynamics]] is:&lt;ref&gt;{{cite web|url=http://www.fuw.edu.pl/~dobaczew/maub-42w/node9.html|title=Quantum Chromodynamics (QCD)|author=|date=|website=www.fuw.edu.pl|accessdate=12 April 2018}}&lt;/ref&gt;&lt;ref&gt;{{cite web |first=E. R. |last=Hilf |title=Semiclassical QCD-Lagrangian for Nuclear Physics |url=http://smallsystems.isn-oldenburg.de/Docs/THEO3/publications/semiclassical.qcd.prep.pdf }}&lt;/ref&gt;&lt;ref&gt;{{cite web |first=Volker |last=Sluka |date=January 10, 2005 |title=Talk |url=http://www-zeus.physik.uni-bonn.de/~brock/teaching/jets_ws0405/seminar09/sluka_quark_gluon_jets.pdf |archivedate=June 26, 2007 |archiveurl=https://web.archive.org/web/20070626211526/http://www-zeus.physik.uni-bonn.de/~brock/teaching/jets_ws0405/seminar09/sluka_quark_gluon_jets.pdf }}&lt;/ref&gt;

:&lt;math&gt;\mathcal{L}_{\mathrm{QCD}} = \sum_n \left ( i\hbar c\bar\psi_n{D}\!\!\!\!/\ \psi_n - m_n c^2 \bar\psi_n \psi_n \right) - {1\over 4} G^\alpha {}_{\mu\nu} G_\alpha {}^{\mu\nu}&lt;/math&gt;

where ''D'' is the QCD [[gauge covariant derivative#Quantum chromodynamics|gauge covariant derivative]], 
''n'' = 1, 2, ...6 counts the quark types, and &lt;math&gt;G^\alpha {}_{\mu\nu}\!&lt;/math&gt; is the [[gluon field strength tensor]].

==See also==
{{colbegin}}
*[[Calculus of variations]]
*[[Covariant classical field theory]]
*[[Einstein–Maxwell–Dirac equations]]
*[[Euler–Lagrange equation]]
*[[Functional derivative]]
*[[Functional integral]]
*[[Generalized coordinates]]
*[[Hamiltonian mechanics]]
*[[Hamiltonian field theory]]
*[[Kinetic term]]
*[[Lagrangian and Eulerian coordinates]]
*[[Lagrangian mechanics]]
*[[Lagrangian point]]
*[[Lagrangian system]]
*[[Noether's theorem]]
*[[Onsager–Machlup function]]
*[[Principle of least action]]
*[[Scalar field theory]]
{{colend}}

==Notes==
{{reflist|group=nb}}

==Citations==
{{reflist|2}}

[[Category:Theoretical physics]]
[[Category:Mathematical physics]]
[[Category:Classical field theory]]
[[Category:Calculus of variations]]
[[Category:Quantum field theory]]</text>
      <sha1>lbp8nf7uzkha5mxvrt5ufexi8lhaxcl</sha1>
    </revision>
  </page>
  <page>
    <title>Lapped transform</title>
    <ns>0</ns>
    <id>39747050</id>
    <revision>
      <id>866176877</id>
      <parentid>815778622</parentid>
      <timestamp>2018-10-28T19:36:00Z</timestamp>
      <contributor>
        <username>JCW-CleanerBot</username>
        <id>31737083</id>
      </contributor>
      <minor/>
      <comment>/* top */[[User:JCW-CleanerBot#Logic|task]], replaced: IEEE Trans. on Signal Processing → IEEE Transactions on Signal Processing (2)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2731">In [[signal processing]], a '''lapped transform''' is a type of [[linear transformation|linear]] [[discrete transform|discrete block transformation]] where the [[basis function]]s of the transformation overlap the block boundaries, yet the number of coefficients overall resulting from a series of overlapping block transforms remains the same as if a non-overlapping block transform had been used.&lt;ref&gt;{{cite paper | first = H. S. | last = Malvar | title = Signal Processing with Lapped Transforms | publisher = Artech House | date = 1992 }}&lt;/ref&gt;&lt;ref&gt;{{cite paper | citeseerx = 10.1.1.91.7148 | title = Lapped Transforms | first = Ricardo L. | last = de Queiroz }}&lt;/ref&gt;&lt;ref&gt;{{cite journal | first = H. S. | last = Malvar | url = http://research.microsoft.com/pubs/102075/malvar_elt_tsp1192.pdf | title = Extended Lapped Transforms: Properties, Applications, and Fast Algorithms | journal = IEEE Transactions on Signal Processing | volume = 40 | issue = 11 | pages = 2703–2714 | date = November 1992 }}&lt;/ref&gt;&lt;ref&gt;{{cite journal | first1 = Trac D. | last1 = Tran | first2 = Jie | last2 = Liang | first3 = Chengjie | last3 = Tu | url = http://thanglong.ece.jhu.edu/Tran/Pub/prepost.pdf | title = Lapped Transform via Time-Domain Pre- and Post-Filtering | journal = IEEE Transactions on Signal Processing | volume = 51 | issue = 6 | date = June 2003 | accessdate = 2013-06-22 | deadurl = yes | archiveurl = https://web.archive.org/web/20160304123116/http://thanglong.ece.jhu.edu/Tran/Pub/prepost.pdf | archivedate = 2016-03-04 | df =  }}&lt;/ref&gt;

Lapped transforms substantially reduce the blocking artifacts that otherwise occur with [[block transform coding]] techniques, in particular those using the [[discrete cosine transform]]. The best known example is the [[modified discrete cosine transform]] used in the [[MP3]], [[Vorbis]], [[Advanced Audio Coding|AAC]], and [[Opus (codec)|Opus]] [[audio codec]]s.&lt;ref name=introducingdaala&gt;{{cite web|url=http://people.xiph.org/~xiphmont/demo/daala/demo1.shtml|title=Next generation video: Introducing Daala|publisher=xiph.org|date=June 20, 2013}}&lt;/ref&gt;

Although the best-known application of lapped transforms has been for audio coding, they have also been used for video and image coding and various other applications. They are used in video coding for coding [[I-frames]] in [[VC-1]] and for image coding in the [[JPEG XR]] format. More recently, a form of lapped transform has also been used in the development of the [[Daala (video codec)|Daala]] [[video coding format]].&lt;ref name=introducingdaala/&gt;

== References ==
{{reflist}}

[[Category:Digital signal processing]]
[[Category:Linear algebra]]
[[Category:Discrete transforms]]


{{linear-algebra-stub}}
{{electronics-stub}}</text>
      <sha1>6ampj5p3zst7b6ietlzzq1h3fjddeqc</sha1>
    </revision>
  </page>
  <page>
    <title>Laurence Chisholm Young</title>
    <ns>0</ns>
    <id>9620408</id>
    <revision>
      <id>838498642</id>
      <parentid>836372354</parentid>
      <timestamp>2018-04-27T12:12:07Z</timestamp>
      <contributor>
        <username>Tom.Reding</username>
        <id>9784415</id>
      </contributor>
      <minor/>
      <comment>+{{[[Template:Authority control|Authority control]]}} (8 sources from Wikidata), [[WP:GenFixes]] on, using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12554">{{Infobox scientist
| name              = Laurence Chisholm Young
| image             = Cairns Daniell Young Stouffer Mitchell Stouffer Kenedy Zurich1932.tif
| image_size        = 
| alt               = 
| caption           = L. Ch. Young (standing right) at the [[International Congress of Mathematicians|ICM]] 1932
| birth_date        = {{Birth date|1905|07|14}}
| birth_place       = [[Göttingen]]
| death_date        = {{Death date and age|2000|12|24|1905|07|14}} 
| death_place       = [[Madison, Wisconsin]]
| resting_place     = 
| resting_place_coordinates = &lt;!-- {{Coord|LAT|LONG|type:landmark|display=inline,title}} --&gt;
| residence         = 
| citizenship       = 
| nationality       = 
| fields            = 
| workplaces        = {{plainlist|
*[[Trinity College, Cambridge]]
*[[University of Cape Town]]
*[[University of Wisconsin–Madison]]
*[[University of Campinas]]
*[[Instituto Nacional de Matemática Pura e Aplicada|IMPA]]}}
| alma_mater        = [[Cambridge University]]
| thesis_title      = 
| thesis_url        = 
| thesis_year       = 
| doctoral_advisor  = 
| academic_advisors = 
| doctoral_students = [[Wendell Fleming]]
| notable_students  = 
| known_for         = [[Calculus of variations]], [[real analysis]]
| author_abbrev_bot = 
| author_abbrev_zoo = 
| influences        = 
| influenced        = 
| awards            = {{plainlist|
*Isaac Newton Studentship (1930)
*[[Honorary degree]] from [[Paris Dauphine University]] (1984)}}
| signature         = &lt;!--(filename only)--&gt;
| signature_alt     = 
| website           = &lt;!-- {{URL|www.example.com}} --&gt;
| footnotes         = 
}}

'''Laurence Chisholm Young''' (14 July 1905 – 24 December 2000) was an American [[mathematician]] known for his contributions to [[measure theory]], the [[calculus of variations]], [[optimal control theory]], and [[potential theory]].  He was the son of [[William Henry Young]] and [[Grace Chisholm Young]], both prominent mathematicians.

The concept of [[Young measure]] is named after him: he also introduced the concept of the [[generalized curve]]&lt;ref&gt;{{harv|Young|1937}}.&lt;/ref&gt; and a concept of generalized surface&lt;ref&gt;{{harv|Young|1951}}.&lt;/ref&gt; which later evolved in the concept of [[varifold]].&lt;ref&gt;In his commemorative papers describing the research of Almgren, {{harvs|txt|first1=Brian|last1=White|author1-link= Brian White (mathematician)|year=1997|year2=1998|loc1=p.1452, footnote 1|loc2=p.682, footnote 1}} writes that these are "''essentially the same class of surfaces''". He notes also that Young himself used the same term in a somewhat different context i.e. in {{harvs|first=L. C.|last=Young|year1=1942|year2=1942a}}.&lt;/ref&gt;&lt;ref&gt;See also the [http://www.dam.brown.edu/people/documents/Geometic_000.pdf 2015 unpublished essay] of his pupil [[Wendell Fleming]].&lt;/ref&gt; The Young integral also is named after him and has now been generalised in the theory of [[rough paths]].&lt;ref&gt;{{harv|Young|1936}}.&lt;/ref&gt;

==Life and academic career==
Laurence Chisholm Young was born in [[Göttingen]],&lt;ref name="mr2001"&gt;{{harv|Turner|Rabinowitz|Rudin|2001}}.&lt;/ref&gt; the fifth of the six children of [[William Henry Young]] and [[Grace Chisholm Young]].&lt;ref name="FW413" &gt;{{harv|Fleming|Wiegand|2004|p=413}}.&lt;/ref&gt;

==Selected publications==
===Books===
*{{Citation
|last = Young
|first = L. C. 
|author-link =
|title = The Theory of Integration
|place = Cambridge
|publisher = [[Cambridge University Press]]
|series = Cambridge Tracts in Mathematics and Mathematical Physics
|volume = 21
|year = 1927
|pages = viii + 53
|url = https://archive.org/details/theoryofintegrat032534mbp
|doi =
|jfm = 53.0207.19
}}, available from the [[Internet Archive|Internet archive]].
*{{Citation
|last = Young
|first = L. C.
|author-link = 
|title = Lectures on the Calculus of Variations and Optimal Control
|place = Philadelphia–London–Toronto
|publisher = [[W. B. Saunders]]
|year = 1969
|pages = xi+331
|url = https://books.google.com/books?id=YQpRAAAAMAAJ&amp;hl=en
|doi =
|id =
|isbn =
|mr = 0259704
|zbl = 0177.37801
}}.
*{{Citation
|last = Young
|first = Laurence
|author-link =
|title =  Mathematicians and their times. History of mathematics and mathematics of history
|place = Amsterdam–New York
|publisher = [[Elsevier|North-Holland Publishing Co.]]
|series =  North-Holland Mathematics Studies, '''48''' / Notas de Matemática [Mathematical Notes], '''76'''
|volume =
|year = 1981
|pages = x+344
|language =
|url =https://books.google.com/books?id=NWKc7mYOBhUC&amp;lpg=PP1&amp;hl=it&amp;pg=PP1#v=onepage&amp;q&amp;f=true
|doi =
|id =
|isbn = 0-444-86135-1|mr = 0629980
|zbl = 0446.01028
}}.

===Papers===
*{{citation
|last=Young
|first=L. C.
|title=An inequality of the Hölder type, connected with Stieltjes integration
|journal=[[Acta Mathematica]]
|volume=67
|year=1936
|issue=1
|pages=251–282
|doi=10.1007/bf02401743
|jfm =62.0250.02
|zbl =0016.10404
|subscription=yes
}}.
*{{Citation
|last = Young
|first = L. C.
|author-link =
|title = Generalized curves and the existence of an attained absolute minimum in the Calculus of Variations
|journal = [[Comptes Rendus des Séances de la Société des Sciences et des Lettres de Varsovie]]
|volume = XXX
|series =Classe III
|issue = 7–9
|pages = 211–234
|year=1937
|url=http://rcin.org.pl/dlibra/editions-content?id=69114
|jfm =63.1064.01
|zbl =0019.21901
}}, memoir presented by [[Stanisław Saks]] at the session of 16 December 1937 of the [[Warsaw Society of Sciences and Letters]]. The free [[PDF]] copy is made available by the [http://rcin.org.pl/dlibra RCIN –Digital Repository of the Scientifics Institutes].
*{{Citation
|last = Young
|first = L. C.
|author-link =
|title = Generalized Surfaces in the Calculus of Variations
|journal = [[Annals of Mathematics]]
|volume = 43
|series =Second Series
|issue = 1
|pages = 84–103
|date=January 1942
|jstor=1968882
|doi=10.2307/1968882
|jfm =68.0227.03
|mr =0006023
|zbl =0063.09081
|subscription=yes
}}.
*{{Citation
|last = Young
|first = L. C.
|author-link =
|title = Generalized Surfaces in the Calculus of Variations. II
|journal = [[Annals of Mathematics]]
|volume = 43
|series =Second Series
|issue = 3
|pages = 530–544
|date=July 1942a
|jstor=1968809
|doi=10.2307/1968809
|mr =0006832
|zbl =0063.08362
|subscription=yes
}}.
*{{Citation
|last = Young
|first = L. C.
|author-link = Laurence Chisholm Young
|title = Surfaces parametriques generalisees
|journal = [[Bulletin de la Société Mathématique de France]]
|volume = 79
|pages = 59–84
|year=1951
|url =http://www.numdam.org/item?id=BSMF_1951__79__59_0
|mr = 46421
|zbl = 0044.10203
}}.
*{{Citation
|last = Young
|first = L. C.
|author-link =
|title = A variantional Algoritm
|language =
|journal = [[Rivista di Matematica della Università di Parma]]
|series = (1)
|volume = 5
|pages = 255–268
|year = 1954
|url =http://rivista.math.unipr.it/fulltext/1954-5/1954-5-268.pdf
|id =
|mr =81437
|zbl =0059.09605
}}.
*{{Citation
|last = Young
|first = L. C.
|author-link =
|title =  Partial area – I
|language =
|journal = [[Rivista di Matematica della Università di Parma]]
|series = (1)
|volume = 10
|pages = 103–113
|year = 1959
|url =http://rivista.math.unipr.it/fulltext/1959-10/1959-10-103.pdf
|id =
|mr =141760
|zbl =0107.27402
}}.
*{{Citation
|last = Young
|first = L. C.
|author-link =
|title = Partial area. Part. II: Contours on hypersurfaces
|language =
|journal = [[Rivista di Matematica della Università di Parma]]
|series = (1)
|volume = 10
|pages = 171–182
|year = 1959a
|url =http://rivista.math.unipr.it/fulltext/1959-10/1959-10-171.pdf
|id =
|mr =141761
|zbl =0107.27402
}}.
*{{Citation
|last = Young
|first = L. C.
|author-link =
|title = Partial area. Part III: Symmetrization and the isoperimetric and least area problems
|language =
|journal = [[Rivista di Matematica della Università di Parma]]
|series = (1)
|volume = 10
|pages = 257–263
|year = 1959b
|url =http://rivista.math.unipr.it/fulltext/1959-10/1959-10-257.pdf
|id =
|mr =141762
|zbl =0107.27402
}}.
*{{Citation
|first = Laurence C.
|last = Young
|author-link =
|editor-last = Roxin
|editor-first = Emilio O. 
|editor-link =
|contribution = Remarks and personal reminiscences
|contribution-url =
|title = Modern optimal control: a conference in honor of Solomon Lefschetz and Joseph P. LaSalle
|series = Lecture Notes in Pure and Applied Mathematics
|volume = 119
|year = 1989
|pages = 421–433
|place = New York
|publisher = [[Marcel Dekker]]
|url =https://books.google.com/books?id=-ejQW0mFf00C&amp;printsec=frontcover&amp;hl=it#v=onepage&amp;q&amp;f=true
|doi =
|id = 
|mr = 1013226
}}.

==See also==
*[[Bounded variation]]
*[[Caccioppoli set]]
*[[Measure theory]]
*[[Varifold]]

==Notes==
{{Reflist|30em}}

==References==
{{refbegin}}

===Biographical and general references===
*{{Citation
| last = Fleming
| first = Wendell H. 
| author-link = Wendell Fleming
| last2 = Wiegand
| first2 = Sylvia M.
| author2-link =Sylvia Wiegand
| title =  Laurence Chisholm Young (1905-2000)
| journal = [[Bulletin of the London Mathematical Society]]
| volume = 36
| issue = 3
| pages = 413–424
| date =
| year = 2004
| doi = 10.1112/S0024609303002959
| mr = 2038729
| zbl = 1050.01519
}}
*{{Citation
|last = Aubin
|first = Jean–Pierre
|author-link =
|last2 =
|first2 =
|author2-link =
|title = Eloge du Professeur L. C. Young, Docteur Honoris Causa de l'Université Paris-Dauphine
|journal = [[Gazette des Mathématiciens]]
|volume = No. 27
|pages = 98–112
|year = 1985
|language = French
|url =
|doi =
|id =
|mr = 0803575
|zbl =
}}, including a reply by L. C. Young himself (pages 109–112).
*{{Citation
| last =Turner
| first =Robert
| author-link =
| last2 =Rabinowitz
| first2 =Paul
| author2-link =Paul Rabinowitz
| last3 =Rudin
| first3 =Mary Ellen 
| author3-link =Mary Ellen Rudin
| title =On the death of Professor Emeritus Laurence Chisholm Young
| series =Memorial Resolution of the Faculty of the University of Wisconsin Madison
| volume =Faculty Document 1554
| pages =1
| date = 5 March 2001
| url =https://www.secfac.wisc.edu/senate/2001/0305/1554(mem_res).pdf
}}.

===Scientific references===
*{{Citation
|last = Màlek
|first = Josef
|author-link =
|last2 = Nečas
|first2 = Jindřich
|author2-link =
|last3 = Rokyta
|first3 = Mirko
|last4 = Růžička
|first4 = Michael
|title = Weak and measure-valued solutions to evolutionary PDEs
|place = London–Weinheim–New York–Tokyo–Melbourne–Madras
|publisher = [[Chapman &amp; Hall]]/[[CRC Press]]
|year = 1996
|series = Applied Mathematics and Mathematical Computation
|volume = 13
|url = https://books.google.com/books?id=30_PBBzwSfAC&amp;printsec=frontcover&amp;dq=Weak+and+measure-valued+solutions+to+evolutionary+PDEs
|isbn = 0-412-57750-X
|pages = xii+317
|mr = 1409366
|zbl = 0851.35002}}. One of the most complete monographs on the theory of [[Young measure]]s, strongly oriented to applications in continuum mechanics of fluids.
*{{Citation
|last1 = Roubicek
|first1 = Tomas
|author1-link = 
|title = Relaxation in optimization theory and variational calculus
|url = https://www.degruyter.com/view/product/145977
|year = 1997 
|place = Berlin
|isbn = 978-3-11-081191-9 
|publisher = [[De Gruyter]]
}}. A thorough scrutiny of [[Young measure]]s and their various generalization is in Chapter 3 from the perspective of [[convex compactification]]s. 
*{{Citation
|last1 = White
|first1 = Brian 
|author1-link = Brian White (mathematician)
|title = The Mathematics of F. J. Almgren Jr.
|url = http://www.ams.org/notices/199711/index.html
|year = 1997 
|journal = [[Notices of the American Mathematical Society]] 
|issn = 0002-9920 
|volume = 44 
|issue = 11 
|pages = 1451–1456 
|mr = 1488574
|zbl = 0908.01017
}}.
*{{Citation
|last=White
|first=Brian
|title=The mathematics of F. J. Almgren, Jr.
|year=1998
|journal=[[The Journal of Geometric Analysis]]
|issn=1050-6926
|volume=8
|issue=5
|pages=681–702
|doi=10.1007/BF02922665
|mr=1731057
|zbl=0955.01020
}}. An extended version of {{harv|White|1997}} with a list of Almgren's publications.
{{refend}}

==External links==
*{{MacTutor Biography|id=Young_Laurence}}
*[http://www.math.wisc.edu/oldhome/news/2001/young.htm Obituary on University of Wisconsin web site]
*{{MathGenealogy|id=8140}}

{{Authority control}}

{{DEFAULTSORT:Young, Laurence Chisholm}}
[[Category:20th-century mathematicians]]
[[Category:Alumni of Trinity College, Cambridge]]
[[Category:Mathematical analysts]]
[[Category:1905 births]]
[[Category:2000 deaths]]
[[Category:Variational analysts]]
[[Category:Historians of mathematics]]
[[Category:Instituto Nacional de Matemática Pura e Aplicada researchers]]</text>
      <sha1>kw6ufrudvqqmiylxurzwurrbtzz1v7c</sha1>
    </revision>
  </page>
  <page>
    <title>Leabra</title>
    <ns>0</ns>
    <id>7049330</id>
    <revision>
      <id>814843194</id>
      <parentid>804487877</parentid>
      <timestamp>2017-12-11T07:14:17Z</timestamp>
      <contributor>
        <username>Nyq</username>
        <id>1893804</id>
      </contributor>
      <minor/>
      <comment>Decapitalized common nouns</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6799">'''Leabra''' stands for '''local, error-driven and associative, biologically realistic algorithm'''. It is a [[Computer simulation|model]] of [[learning]] which is a balance between [[Hebbian learning|Hebbian]] and [[error-driven learning]] with other [[neural network|network]]-derived characteristics.  This model is used to mathematically predict outcomes based on inputs and previous learning influences. This model is heavily influenced by and contributes to neural network designs and models. This algorithm is the default algorithm in [[Emergent (software)|''emergent'']] (successor of PDP++) when making a new project, and is extensively used in various simulations.

[[Hebbian learning]] is performed using [[conditional principal components analysis]] (CPCA) algorithm with correction factor for sparse expected activity levels.

[[Error-driven learning]] is performed using [[GeneRec]], which is a generalization of the [[recirculation algorithm]], and approximates [[Almeida–Pineda recurrent backpropagation]]. The symmetric, midpoint version of GeneRec is used, which is equivalent to the [[contrastive Hebbian learning]] algorithm (CHL). See O'Reilly (1996; Neural Computation) for more details.

The activation function is a point-neuron approximation with both discrete [[Spiking neural network|spiking]] and continuous rate-code output.

Layer or unit-group level inhibition can be computed directly using a [[Winner-take-all (computing)|k-winners-take-all]] (KWTA) function, producing sparse distributed representations.

The net input is computed as an average, not a sum, over connections, based on normalized, sigmoidally transformed weight values, which are subject to scaling on a connection-group level to alter relative contributions.  Automatic scaling is performed to compensate for differences in expected activity level in the different projections.

Documentation about this algorithm can be found in the book "Computational Explorations in Cognitive Neuroscience: Understanding the Mind by Simulating the Brain" published by MIT press.&lt;ref&gt;{{cite book |author=O'Reilly, R. C., Munakata, Y. |title=Computational explorations in cognitive neuroscience |publisher=MIT Press|location=Cambridge |year=2000 |isbn=0-19-510491-9 }}&lt;/ref&gt; and in the [http://grey.colorado.edu/emergent/index.php/Leabra Emergent Documentation]

== Overview of the leabra algorithm ==
The pseudocode for Leabra is given here, showing exactly how the
pieces of the algorithm described in more detail in the subsequent
sections fit together.

&lt;code&gt;
 Iterate over minus and plus phases of settling for each event.
  o At start of settling, for all units:
    - Initialize all state variables (activation, v_m, etc).
    - Apply external patterns (clamp input in minus, input &amp; output in
      plus).
    - Compute net input scaling terms (constants, computed
      here so network can be dynamically altered).
    - Optimization: compute net input once from all static activations
      (e.g., hard-clamped external inputs).
  o During each cycle of settling, for all non-clamped units:
    - Compute excitatory netinput (g_e(t), aka eta_j or net)
       -- sender-based optimization by ignoring inactives.
    - Compute [[Winner-take-all (computing)|kWTA]] inhibition for each layer, based on g_i^Q:
      * Sort units into two groups based on g_i^Q: top k and
        remaining k+1 -&gt; n.
      * If basic, find k and k+1th highest
        If avg-based, compute avg of 1 -&gt; k &amp; k+1 -&gt; n.
      * Set inhibitory conductance g_i from g^Q_k and g^Q_k+1
    - Compute point-neuron activation combining excitatory input and
      inhibition
  o After settling, for all units, record final settling activations
    as either minus or plus phase (y^-_j or y^+_j).
 After both phases update the weights (based on linear current
    weight values), for all connections:
  o Compute [[error-driven learning|error-driven]] weight changes with [[Contrastive Hebbian learning|CHL]] with soft weight bounding
  o Compute [[Hebbian learning|Hebbian]] weight changes with [[Conditional principal components analysis|CPCA]] from plus-phase activations
  o Compute net weight change as weighted sum of error-driven and Hebbian
  o Increment the weights according to net weight change.
&lt;/code&gt;

==Implementations==
[https://grey.colorado.edu/emergent/index.php/Main_Page Emergent] is the original implementation of Leabra, written in C++ and highly optimized. This is the fastest implementation, suitable for constructing large networks. Although ''emergent'' has a graphical user interface, it is very complex and has a steep learning curve.

If you want to understand the algorithm in detail, it will be easier to read non-optimzed code. For this purpose, check out the [https://grey.colorado.edu/svn/emergent/emergent/trunk/Matlab/ MATLAB version]. There is also an [https://cran.r-project.org/web/packages/leabRa/ R version] available, that can be easily installed via &lt;code&gt;install.packages("leabRa")&lt;/code&gt; in R and has a [https://cran.r-project.org/web/packages/leabRa/vignettes/leabRa.html short introduction] to how the package is used. The MATLAB and R versions are not suited for constructing very large networks, but they can be installed quickly and (with some programming background) are easy to use. Furthermore, they can also be adapted easily.

==Special algorithms==
* '''Temporal differences and general dopamine modulation'''. [[Temporal difference learning|Temporal differences (TD)]] is widely used as a [[Computer simulation|model]] of [[midbrain]] [[dopaminergic]] firing.
* '''Primary value learned value (PVLV)'''. [[PVLV]] simulates behavioral and neural data on [[Classical conditioning|Pavlovian conditioning]] and the [[midbrain]] [[dopaminergic]] [[neurons]] that fire in proportion to unexpected rewards (an alternative to [[Temporal difference learning|TD]]).
* '''Prefrontal cortex basal ganglia working memory (PBWM)'''. [[PBWM]] uses [[PVLV]] to train [[prefrontal cortex]] [[working memory]] updating system, based on the biology of the prefrontal cortex and [[basal ganglia]].

==References==
{{reflist}}

==External links==
* [http://grey.colorado.edu/emergent/index.php/Leabra Emergent about Leabra]
* [http://archive.cnbc.cmu.edu/Resources/PDP++/manual/pdp-user_235.html PDP++ about Leabra]
* O'Reilly, R.C. (1996). The Leabra Model of Neural Interactions and Learning in the Neocortex. Phd Thesis, Carnegie Mellon University, Pittsburgh, PA [ftp://grey.colorado.edu/pub/oreilly/thesis/oreilly_thesis.all.pdf PDF]
* [https://cran.r-project.org/web/packages/leabRa/ R version of Leabra]
* [https://cran.r-project.org/web/packages/leabRa/vignettes/leabRa.html Vignette for R version of Leabra]

[[Category:Machine learning algorithms]]
[[Category:Artificial neural networks]]</text>
      <sha1>pqzc3bf6ia9n252s0edk0zosc4on2jb</sha1>
    </revision>
  </page>
  <page>
    <title>Littlewood–Offord problem</title>
    <ns>0</ns>
    <id>1976420</id>
    <revision>
      <id>849583072</id>
      <parentid>849580671</parentid>
      <timestamp>2018-07-10T00:27:51Z</timestamp>
      <contributor>
        <username>Davemck</username>
        <id>326639</id>
      </contributor>
      <minor/>
      <comment>rmv duplicate parm</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2662">In [[mathematics|mathematical]] field of [[combinatorial geometry]], the '''Littlewood–Offord problem''' is the problem of determining the number of [[subsum]]s of a set of [[vector space|vector]]s that fall in a given [[convex set]]. More formally, if ''V'' is a vector space of [[dimension (vector space)|dimension]] ''d'', the problem is to determine, given a finite subset of vectors ''S'' and a convex subset ''A'', the number of subsets of ''S'' whose [[summation]] is in ''A''.

The first [[upper bound]] for this problem was proven (for ''d'' = 1 and ''d'' = 2) in 1938 by [[John Edensor Littlewood]] and [[Albert Cyril Offord|A. Cyril Offord]].&lt;ref&gt;{{cite journal|last=Littlewood|first=J.E.|last2=Offord|first2=A.C.|title=On the number of real roots of a random algebraic equation (III)|journal=Rec. Math. (Mat. Sbornik) N.S.|year=1943|volume=12 |issue=54|pages=277–286}}&lt;/ref&gt; This '''Littlewood–Offord lemma''' states that if ''S'' is a set of ''n'' real or complex numbers of [[absolute value]] at least one and ''A'' is any [[Disk (mathematics)|disc]] of [[radius]] one, then not more than &lt;math&gt; \Big( c \, \log n / \sqrt{n} \Big) \, 2^n &lt;/math&gt; of the 2&lt;sup&gt;''n''&lt;/sup&gt; possible subsums of ''S'' fall into the disc.

In 1945 [[Paul Erdős]] improved the upper bound for ''d'' = 1 to
:&lt;math&gt;{n \choose \lfloor{n/2}\rfloor} \approx 2^n \, \frac{1}{\sqrt{n}}&lt;/math&gt;
using  [[Sperner's theorem]].&lt;ref name="bollobas"&gt;{{cite book|last=Bollobás|first=Béla|title=Combinatorics|year=1986|publisher=Cambridge|isbn=0-521-33703-8}}&lt;/ref&gt; This bound is sharp; equality is attained when all vectors in ''S'' are equal. In 1966, Kleitman showed that the same bound held for complex numbers. In 1970, he extended this to the setting when ''V'' is a [[normed space]].&lt;ref name="bollobas" /&gt;

Suppose ''S'' = {''v''&lt;sub&gt;1&lt;/sub&gt;, …, ''v''&lt;sub&gt;''n''&lt;/sub&gt;}. By subtracting
: &lt;math&gt;\frac{1}{2} \sum_{i = 1}^n v_i&lt;/math&gt;
from each possible subsum (that is, by changing the origin and then scaling by a factor of 2), the Littlewood–Offord problem is equivalent to the problem of determining the number of sums of the form
: &lt;math&gt;\sum_{i = 1}^n \epsilon_i v_i&lt;/math&gt;
that fall in the target set ''A'', where &lt;math&gt;\epsilon_i&lt;/math&gt; takes the value 1 or &amp;minus;1. This makes the problem into a [[probabilistic]] one, in which the question is of the distribution of these ''[[random vector]]s'', and what can be said knowing nothing more about the ''v''&lt;sub&gt;''i''&lt;/sub&gt;.

==References==
&lt;references/&gt;

{{DEFAULTSORT:Littlewood-Offord problem}}
[[Category:Combinatorics]]
[[Category:Probability problems]]
[[Category:Lemmas]]
[[Category:Mathematical problems]]</text>
      <sha1>pqlkk43krq0ilpkgwj0z95ns3n0r15h</sha1>
    </revision>
  </page>
  <page>
    <title>Loss–DiVincenzo quantum computer</title>
    <ns>0</ns>
    <id>8481660</id>
    <revision>
      <id>865971376</id>
      <parentid>822445014</parentid>
      <timestamp>2018-10-27T11:27:49Z</timestamp>
      <contributor>
        <username>RAM256</username>
        <id>34984326</id>
      </contributor>
      <minor/>
      <comment>add reference link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3543">{{primary sources|date=October 2015}}
[[Image:DoubleQuantumDot.jpg|thumb|250px|right|A double [[quantum dot]].  Each electron spin S&lt;sub&gt;L&lt;/sub&gt; or S&lt;sub&gt;R&lt;/sub&gt; define one quantum two-level system, or ''[[qubit]]'' in the Loss-DiVincenzo proposal. A narrow gate between the two dots can modulate the coupling, allowing [[Swap (computer science)|swap]] operations.]]
The '''Loss–DiVincenzo quantum computer''' (or spin-qubit quantum computer) is a scalable [[semiconductor]]-based [[quantum computer]]  proposed by [[Daniel Loss]] and [[David P. DiVincenzo]] in 1997.&lt;ref&gt;D. Loss and D. P. DiVincenzo, "Quantum computation with quantum dots", [https://doi.org/10.1103/PhysRevA.57.120 ''Phys. Rev. A'' '''57''', p120 (1998)]; [https://arxiv.org/abs/cond-mat/9701055 on arXiv.org in Jan. 1997]&lt;/ref&gt;  The proposal was to use as [[qubits]] the intrinsic spin-1/2 degree of freedom of individual electrons confined to [[quantum dots]]. This was done in a way that fulfilled [[DiVincenzo's_criteria|DiVincenzo Criteria]] for a scalable quantum computer,&lt;ref&gt;D. P. DiVincenzo, in Mesoscopic Electron Transport, Vol. 345 of NATO  Advanced Study Institute, Series E: Applied Sciences, edited by L. Sohn, L. Kouwenhoven, and G. Schoen (Kluwer, Dordrecht, 1997); [https://arxiv.org/abs/cond-mat/9612126 on arXiv.org in Dec. 1996]&lt;/ref&gt; namely:

* identification of well-defined qubits;
* reliable state preparation;
* low decoherence;
* accurate quantum gate operations and
* strong quantum measurements.

A candidate for such a quantum computer is a [[lateral quantum dot]] system.

==Implementation of the two-qubit gate==

The Loss–DiVincenzo quantum computer operates, basically, using inter-dot gate voltage for implementing [[Swap (computer science)]] operations and local magnetic fields (or any other local spin manipulation) for implementing the [[Controlled NOT gate]] (CNOT gate). 

The Swap operation is achieved by applying a pulsed inter-dot gate voltage, so the exchange constant in the Heisenberg Hamiltonian becomes time-dependent:

:&lt;math&gt;H_s(t) = J(t)\vec{S}_L \cdot \vec{S}_R .&lt;/math&gt;

This description is only valid if:

*the level spacing in the quantum-dot &lt;math&gt;\Delta E &lt;/math&gt; is much greater than &lt;math&gt;\; kT &lt;/math&gt;;
*the pulse time scale &lt;math&gt;\tau_s &lt;/math&gt; is greater than &lt;math&gt;\hbar / \Delta E &lt;/math&gt;, so there is no time for transitions to higher orbital levels to happen and
*the [[Quantum decoherence|decoherence]] time &lt;math&gt;\Gamma ^{-1} &lt;/math&gt; is longer than &lt;math&gt;\tau_s &lt;/math&gt;.

From the pulsed Hamiltonian follows the time evolution operator

:&lt;math&gt;U_s(t) = \mathbf{T} \exp\{-i\int_0^t dt' H_s(t')\}.&lt;/math&gt;

We can choose a specific duration of the pulse such that the integral in time over &lt;math&gt;J(t)&lt;/math&gt; gives &lt;math&gt;J_0 \tau_s = \pi (\text{mod}2\pi),&lt;/math&gt; and &lt;math&gt;U_s&lt;/math&gt; becomes the Swap operator &lt;math&gt;U_s (J_0 \tau_s = \pi) \equiv U_{sw}&lt;/math&gt;.

The XOR gate may be achieved by combining &lt;math&gt;\sqrt{\text{swap}}&lt;/math&gt; (square root of Swap) operations with individual spin operations:

:&lt;math&gt;U_{XOR} = e^{i\frac{\pi}{2}S_L^z}e^{-i\frac{\pi}{2}S_R^z}U_{sw}^{1/2}
e^{i \pi S_L^z}U_{sw}^{1/2}.&lt;/math&gt;

This operator gives a conditional phase for the state in the basis of &lt;math&gt;\vec{S}_L + \vec{S}_R&lt;/math&gt;.

==References==
{{reflist}}

==External links==
*[http://quantumtheory.physik.unibas.ch/ Condensed Matter Theory at the University of Basel]

{{quantum_computing}}

{{DEFAULTSORT:Loss-DiVincenzo quantum computer}}
[[Category:Quantum information science]]
[[Category:Quantum dots]]</text>
      <sha1>hhvhl0q8jl1x7mut7ihqyrm8g5z7ghn</sha1>
    </revision>
  </page>
  <page>
    <title>Mathemagician</title>
    <ns>0</ns>
    <id>2164767</id>
    <revision>
      <id>846580918</id>
      <parentid>825374768</parentid>
      <timestamp>2018-06-19T17:05:29Z</timestamp>
      <contributor>
        <username>Peaceray</username>
        <id>11630810</id>
      </contributor>
      <comment>/* Notable mathemagicians */ Converting a [[WP:BAREURL|bare URL]] to a [[WP:CT|citation template]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2420">A '''mathemagician''' is a [[mathematician]] who is also a [[Magician (illusion)|magician]].

The name "mathemagician" was probably first applied to [[Martin Gardner]], but has since been used to describe many mathematician/magicians, including [[Arthur T. Benjamin]],&lt;ref&gt;Albers, Donald J. "Art Benjamin - Mathemagician." ''Math Horizons'', November 1998, 14-18.&lt;/ref&gt; [[Persi Diaconis]],&lt;ref&gt;''[https://www.jstor.org/stable/25678003?origin=JSTOR-pdf&amp;seq=1#page_scan_tab_contents Professor of &lt;s&gt;Magic&lt;/s&gt; Mathematics]'' by Don Albers and Persi Diaconis, ''[[Math Horizons]]'' Vo. 2, No 3 (February 1995), pp. 11-15&lt;/ref&gt; and [[Colm Mulcahy]].&lt;ref&gt;[http://www.mathaware.org/mam/2014/committee/ Mathematics Awareness Month 2014: Mathematics, Magic, and Mystery] Committee Members&lt;/ref&gt;  Diaconis has suggested that the reason so many mathematicians are magicians is that "inventing a magic trick and inventing a theorem are very similar activities."&lt;ref&gt;Diaconis, Persi. Quoted in: Albers, Donald J. "Professor of &lt;s&gt;Magic&lt;/s&gt; Mathematics." ''Math Horizons'', February 1995, 11-15.&lt;/ref&gt;

A great number of self-working [[mentalism]] tricks rely on mathematical principles. [[Max Maven]] often utilizes this type of magic in his performance.

== Notable mathemagicians ==
* [[Arthur T. Benjamin]]
* [[Persi Diaconis]]
* [[Richard Feynman]]
* [[Karl Fulves]]
* [[Martin Gardner]]
* [[Ronald Graham]]
* [[Colm Mulcahy]]
* [[Raymond Smullyan]]
* [[Bernard Meulenbroek]]&lt;ref&gt;{{Cite web|url=http://mathemagician.tudelft.nl/|title=MatheMagician|website=mathemagician.tudelft.nl|access-date=2018-06-19}}&lt;/ref&gt;

== References ==
{{reflist}}

== Further reading ==
* Diaconis, Persi &amp; Graham, Ron. ''[http://press.princeton.edu/titles/9510.html Magical Mathematics: The Mathematical Ideas That Animate Great Magic Tricks]'' Princeton University Press, 2012. {{ISBN|0691169772}} 
* Fulves, Karl. ''Self-working Number Magic'', New York London : Dover Constable, 1983. {{ISBN|0486243915}}
* Gardner, Martin. ''Mathematics, Magic and Mystery'', [[Dover Publications|Dover]], 1956{{isbn|0-486-20335-2}}
* Ron Graham, Ron. ''[http://www.math.ucsd.edu/~ronspubs/13_05_juggling.pdf Juggling Mathematics and Magic]'' University of California, San Diego

[[Category:Magicians]]
[[Category:Magic (illusion)]]
[[Category:Entertainment]]
[[Category:Deception]]
[[Category:Education terminology]]
[[Category:Mathematical science occupations]]</text>
      <sha1>rqvmoog0iso3ksj7oefk0jkzyc5q38y</sha1>
    </revision>
  </page>
  <page>
    <title>Mathematical sculpture</title>
    <ns>0</ns>
    <id>47304362</id>
    <revision>
      <id>841882651</id>
      <parentid>798318434</parentid>
      <timestamp>2018-05-18T17:56:24Z</timestamp>
      <contributor>
        <username>D.T.Ladisa</username>
        <id>31875730</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="955">[[File:Bathsheba Grossman geometric art.jpg|thumb|upright|Mathematical sculpture by [[Bathsheba Grossman]], 2007]]

A '''mathematical sculpture''' is a sculpture which uses [[mathematics]] as an essential conception.&lt;ref&gt;{{cite web |url= http://pages.towson.edu/gsarhang/67%20-%2074.pdf|title=A Proposal for the Classification of Mathematical Sculpture |publisher=[[Towson University]]  |accessdate=July 22, 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite news |title=Mathematical Sculptures Made Out Of Office Supplies (PHOTOS) |url= http://www.huffingtonpost.com/2011/12/26/mathematical-sculptures-made-out-of-office-supplies_n_1170370.html |date=December 26, 2011 |work=[[Huffington Post]]}}&lt;/ref&gt; [[Helaman Ferguson]], [[George W. Hart]], [[Bathsheba Grossman]], [[Peter Forakis]] and [[Jacobus Verhoeff]] are well-known [[mathematical artist|mathematical sculptors]].

==References==
{{Reflist}}

{{Mathematical art}}

[[Category:Mathematics and art]]
[[Category:Sculpture]]</text>
      <sha1>etr3nk37lqn492xr1aaodp6y7925ivh</sha1>
    </revision>
  </page>
  <page>
    <title>Mike Young (Neighbours)</title>
    <ns>0</ns>
    <id>4889209</id>
    <revision>
      <id>845972843</id>
      <parentid>835685278</parentid>
      <timestamp>2018-06-15T12:01:19Z</timestamp>
      <contributor>
        <username>Mitch Ames</username>
        <id>6326132</id>
      </contributor>
      <comment>/* Reception */ uncapitalise</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14739">{{DISPLAYTITLE:Mike Young (''Neighbours'')}}
{{Use dmy dates|date=April 2011}}
{{Infobox soap character 
| series = Neighbours
| image = Mike Young Neighbours.jpg
| imagesize = 200px
| name = Mike Young
| portrayer = [[Guy Pearce]]
| creator = [[Ray Kolle]]
| introducer = [[Reg Watson]]
| first = {{Start date|1986|01|20|df=y}}
| last = {{End date|1989|12|06|df=y}}
| years = 1986–1989
| classification = [[List of past Neighbours characters#Y|Former; regular]]
| born = 14 June 1968&lt;ref&gt;{{cite episode|title=Episode 983|series=Neighbours|serieslink=Neighbours|credits=Executive producer: Don Battye; Director: Andrew Friedman|network=[[Network Ten]]|airdate=14 June 1989}}&lt;/ref&gt;
| home = [[Philippines]]
| occupation = {{plainlist|
*Student
*Waiter
*Maths teacher
}}
| father = [[List of Neighbours characters (1986)#David Young|David Young]]
| mother = [[List of Neighbours characters (1986)#Barbara Young|Barbara Young]]
}}
'''Mike Young''' is a fictional character from the Australian soap opera ''[[Neighbours]]'', played by [[Guy Pearce]]. He made his first on-screen appearance on 20 January 1986. Mike's storylines included being physically abused by his father, moving in with [[Des Clarke (Neighbours)|Des]] and [[Daphne Clarke]], making friends with [[Charlene Robinson|Charlene Mitchell]] and [[Scott Robinson (Neighbours)|Scott Robinson]], his relationship with [[Jane Harris (Neighbours)|Jane Harris]] and becoming a teacher. Mike departed Erinsborough to be with his mother on 6 December 1989.

==Creation and casting==
When he was 18, Pearce's drama teacher advised him to write to TV companies asking for auditions and he wrote to the [[Reg Grundy Organisation|Grundy Organisation]], which produced ''[[Neighbours]]'' at the time.&lt;ref name="Talk"&gt;{{cite web|url=http://www.talktalk.co.uk/entertainment/film/biography/artist/guy-pearce/biography/34?page=2|title=Guy Pearce – Biography Pg2|publisher=[[TalkTalk Group]]|accessdate=4 April 2010 |archiveurl=https://www.webcitation.org/5omxBgDSZ?url=http://www.talktalk.co.uk/entertainment/film/biography/artist/guy-pearce/biography/34?page%3D2|dead-url=no|archivedate=6 April 2010 }}&lt;/ref&gt; Pearce was then cast as the troubled and lonely Mike Young. Following his last [[Year Twelve#Australia|year 12]] exam, he began filming on 3 December 1985.&lt;ref name="Talk"/&gt;&lt;ref&gt;{{cite news|url=https://www.theguardian.com/film/2002/apr/12/australia.artsfeatures|title=Love thy neighbour|last=Barkham|first=Patrick|date=12 April 2002|work=[[The Guardian]]|publisher=[[Guardian Media Group|Guardian Media Group plc]]|accessdate=4 April 2010 |archiveurl=https://www.webcitation.org/5omxV2mfX?url=http://www.guardian.co.uk/film/2002/apr/12/australia.artsfeatures|dead-url=no|archivedate=6 April 2010 }}&lt;/ref&gt;&lt;ref&gt;{{cite news|url=http://www.smh.com.au/executive-style/culture/whos-that-guy-20090518-b8ne.html|title=Who's that Guy?|last=Astle |first=David|date=3 March 2008|work=[[The Sydney Morning Herald]]|publisher=[[Fairfax Media]]|accessdate=4 April 2010 |archiveurl=https://www.webcitation.org/5on415J2S?url=http://www.smh.com.au/executive-style/culture/whos-that-guy-20090518-b8ne.html|dead-url=yes|archivedate=6 April 2010 }}&lt;/ref&gt; Pearce's debut was in episode number 171, the first episode broadcast on [[Network Ten]] following the show's move from [[Seven Network]].&lt;ref name="Talk"/&gt;

In 1989, the ''Neighbours'' producers did not want Pearce to play [[Errol Flynn]] in a biopic film and Pearce decided to leave the show.&lt;ref&gt;{{cite web|url=http://www.talktalk.co.uk/entertainment/film/biography/artist/guy-pearce/biography/34?page=3|title=Guy Pearce – Biography Pg3|publisher=[[TalkTalk Group]]|accessdate=4 April 2010 |archiveurl=https://www.webcitation.org/5on4EbjMt?url=http://www.talktalk.co.uk/entertainment/film/biography/artist/guy-pearce/biography/34?page%3D3|dead-url=no|archivedate=6 April 2010 }}&lt;/ref&gt; Of being a part of ''Neighbours'', Pearce said "I experienced hysteria at a pretty high pitch with that show".&lt;ref&gt;{{cite news|url=http://www.theage.com.au/articles/2004/11/12/1100227561103.html?from=storyrhs|title=He is his own Guy|date=13 November 2004|work=[[The Age]]|publisher=[[Fairfax Media]]|accessdate=4 April 2010}}&lt;/ref&gt; Pearce also added "I'm not embarrassed by having done it now, not at all. It was an amazing experience, an amazing opportunity. And I was only 18."&lt;ref&gt;{{cite news|url=http://www.sundaysun.co.uk/whats-on-newcastle-north-east/cinema-film-reviews/2003/09/05/straight-talking-honesty-79310-13372383/|title=Straight talking honesty|last=Ebner|first=Sarah|date=5 September 2003|work=[[Sunday Sun]]|publisher=[[Trinity Mirror]]|accessdate=4 April 2010 |archiveurl=https://www.webcitation.org/5vxBrVoI3?url=http://www.sundaysun.co.uk/whats-on-newcastle-north-east/cinema-film-reviews/2003/09/05/straight-talking-honesty-79310-13372383/|dead-url=no|archivedate=23 January 2011 }}&lt;/ref&gt;

==Storylines==
Mike and his mother, [[List of Neighbours characters (1986)#Barbara Young|Barbara]] ([[Rona McLeod]]; Diana Greentree), lived in fear of being beaten and abused by Mike's father, [[List of Neighbours characters (1986)#David Young|David]] (Stewart Faichney), throughout Mike's young life. This meant Mike grew up into a lonely, quiet young man who did not socialise much until he began attending [[Erinsborough#Erinsborough High School|Erinsborough High School]] and becomes good friends with [[Charlene Robinson|Charlene Mitchell]] ([[Kylie Minogue]]) and [[Scott Robinson (Neighbours)|Scott Robinson]] ([[Jason Donovan]]).&lt;ref&gt;{{cite web|url=http://www.whatsontv.co.uk/soaps/neighbours/photos/1/11769/2|title=The amazing career of Neighbours star Guy Pearce in photos|date=13 July 2009|work=[[What's on TV]]|publisher=([[IPC Media]])|accessdate=4 April 2010|deadurl=yes|archiveurl=https://web.archive.org/web/20120305005340/http://www.whatsontv.co.uk/soaps/neighbours/photos/1/11769/2|archivedate=5 March 2012|df=dmy-all}}&lt;/ref&gt;

Through his friendship with Scott, Mike comes to [[Ramsay Street]] and befriends Scott's neighbour, [[Daphne Clarke]] ([[Elaine Smith (actress)|Elaine Smith]]), who cares for him, later giving him a part-time job at her coffee shop. Mike opens up to Daphne and [[Des Clarke (Neighbours)|Des]] ([[Paul Keane]]) about his problems at home with his father and Daphne tries to get Mike and Barbara away from David. However, Barbara is too scared of her husband and she does not want to leave, but Mike is determined to go and moves in with Des and Daphne when they offer to become his legal guardians. Mike briefly dates Scott's cousin [[Nikki Dennison]] (Charlene Fenn), but he later falls for [[Jane Harris (Neighbours)|Jane Harris]] ([[Annie Jones (actress)|Annie Jones]]) when she moves in with her grandmother, [[Nell Mangel]] ([[Vivean Gray]]). Jane falls for Mike straight away, but it takes a while before Mike realises his feelings due to Jane's plain image. [[Helen Daniels]] ([[Anne Haddy]]) and Daphne give her a makeover for a school dance, which consists of replacing her glasses with contacts, a new haircut and makeup.&lt;ref&gt;{{cite web|url=http://tv.uk.msn.com/photos/photos.aspx?cp-documentid=149729015&amp;page=7|title=TV's Neighbours: where are they now? – Guy Pearce|last=Cooper|first=Lorna|date=17 March 2010|work=[[MSN|MSN TV]]|publisher=([[Microsoft]])|accessdate=4 April 2010}}&lt;/ref&gt; Mike likes her new image and they begin dating. Mrs Mangel is not happy that her granddaughter is dating Mike and when she receives letters about Mike's reputation with other girls, Mrs Mangel stops Jane from seeing him. Daphne eventually catches school bully, [[Sue Parker]] (Kate Gorman), posting the letters and Sue explains that she is jealous of Mike and Jane's relationship. Mrs Mangel then lets Jane and Mike continue dating. When Nikki returns to Ramsay Street, Mike helps comfort her when she discovers her mother is ill, leading Jane to become jealous of their friendship. Mike becomes jealous when [[Shane Ramsay]] ([[Peter O'Brien (actor)|Peter O'Brien]]) shows an attraction to Jane, but she tells Mike that he is the only one for her.

Following their final exams, Jane focuses on her modelling career and Mike decides to become a teacher. As they are leading separate lives, Jane and Mike split up amicably and remain friends. Not long after, Daphne is killed in a car crash and Mike is left feeling guilty as he had not been around for a few weeks. Mike is angry and upset and he finds the two men who had crashed into Daphne's car. He attacks them and is later arrested. When Mike finishes university, he gets a job teaching Maths at Erinsborough High. Mike becomes close to one of his students, Jessie Ross (Michelle Kearley), who admits that she has an abusive father too. Mike confronts her father, Ted (Doug Bennett), but soon learns that her mother Adele (Marian Sinclair) is the one abusing her. Mike becomes close to Jessie and they share a kiss, which is witnessed by principal [[Kenneth Muir (Neighbours)|Kenneth Muir]] (Roger Boyce). Mr Muir suspends Mike, who decides to leave Erinsborough for a while.

On his return, Mike finds Des and Jane have forged a strong friendship. Mike becomes moody as he settles back in. When Jenny Owens ([[Danielle Carter (actress)|Danielle Carter]]) comes to see him, it is revealed that Mike and Jenny took a ride on his motorbike and had an accident. Jenny fell from the bike and was left paralysed and using a wheelchair for the rest of her life. Mike blames himself for Jenny's condition and could not bring himself to accept that the event was an accident. Jenny eventually convinces him that it was not his fault. At the same time, Des and Jane begin dating and Mike is disgusted with the both of them. He refuses to accept the relationship and leaves Erinsborough again. He returns in the middle of Des and Jane's engagement party and he interrupts it. Mike eventually accepts that Des and Jane love each other and gives them his blessing. Mike begins to feel like there is not much left for him in Erinsborough and when he hears that his mother had been in a plane crash, and with his father long dead, Mike decides to leave Ramsay Street and join her to help her recovery.&lt;ref&gt;{{cite web|url=http://www.whatsontv.co.uk/soaps/neighbours/photos/1/11769/5|title=The amazing career of Neighbours star Guy Pearce in photos Pg5|date=13 July 2009|work=[[What's on TV]]|publisher=([[IPC Media]])|accessdate=4 April 2010|deadurl=yes|archiveurl=https://web.archive.org/web/20120305005352/http://www.whatsontv.co.uk/soaps/neighbours/photos/1/11769/5|archivedate=5 March 2012|df=dmy-all}}&lt;/ref&gt;

==Reception==
A writer for the [[BBC]]'s ''Neighbours'' website stated that Mike's most notable moment was when he saw Jane's "transformation at the school dance."&lt;ref&gt;{{cite web|url=http://www.bbc.co.uk/neighbours/whoswho/characterbiogs/index.shtml?content/_mikeyoung/page1 |title=Character: Mike Young |publisher=[[BBC]] |accessdate=26 January 2013 |archiveurl=https://web.archive.org/web/20040811025748/http://www.bbc.co.uk/neighbours/whoswho/characterbiogs/index.shtml?content%2F_mikeyoung%2Fpage1 |archivedate=11 August 2004 |deadurl=yes |df= }}&lt;/ref&gt; In 2010, to celebrate Neighbours' 25th anniversary [[Sky (United Kingdom)|Sky]], a British [[satellite broadcasting|satellite broadcasting company]], profiled 25 characters of which they believed were the most memorable in the series history.&lt;ref name="a"&gt;{{cite news|url=http://tv.sky.com/neighbours |title=Neighbours: 25 Top Characters |work=[[Sky Television plc|tv.sky.com]] |publisher=[[British Sky Broadcasting]] |author= |date=2010 |accessdate=26 April 2010 |deadurl=yes |archiveurl=https://web.archive.org/web/20100323092809/http://tv.sky.com/neighbours |archivedate=23 March 2010 |df= }}&lt;/ref&gt; Mike is in the list and describing him they state: "The Erinsborough economy is studied the world over for being the only one in the world that can sustained on just four professions: you can work in 'business', or be a journalist, a doctor, or a teacher. Mike chose the latter, probably because he was the nice, fairly quiet boy out of the legendary original set of teens on the show. His romance with plain Jane Harris unfortunately ended up with her engaged to his father figure of a friend, Des, one of many reasons why he left the show to care for his sick mother."&lt;ref name="a"/&gt; ''[[Heat (magazine)|Heat]]'' magazine called Mike "cheesy, but gorgeous".&lt;ref&gt;{{cite news|url=http://www.heatworld.com/Entertainment/Films/2008/03/The-teen-crush-that-just-wont-go-away/|title=The teen crush that just won't go away…|date=26 March 2008|work=[[Heat (magazine)|heatworld.com]]|publisher=[[Bauer Media Group]]|accessdate=11 September 2010}}&lt;/ref&gt; Lorna Cooper of [[MSN|MSN TV]] has listed Mike as one of soap opera's forgotten characters and claims he is a favourite out of the golden era of the serial.&lt;ref name="Lorna"&gt;{{cite web|url=http://tv.uk.msn.com/photos/photos.aspx?cp-documentid=154313087&amp;page=1|title=Soap's forgotten characters|last=Cooper|first=Lorna|date=20 September 2010|work=[[MSN]]|publisher=([[Microsoft]])|accessdate=23 January 2011 |archiveurl=https://www.webcitation.org/5vwU8fZDF?url=http://tv.uk.msn.com/photos/photos.aspx?cp-documentid%3D154313087%26page%3D1|dead-url=no|archivedate=23 January 2011 }}&lt;/ref&gt; [[Orange UK]] describe Mike as one of the serial's "hottest spunks".&lt;ref name=ora&gt;{{cite web|title=Neighbours stars – where are they now?|url=http://www.orange.co.uk/entertainment/television/pics/2345_3.htm|publisher=([[Orange (telecommunications)|Orange]])|accessdate=23 January 2011 |archiveurl=https://www.webcitation.org/5vwWHcd3u?url=http://www.orange.co.uk/entertainment/television/pics/2345_3.htm|dead-url=no|archivedate=23 January 2011}}&lt;/ref&gt; [[LoveFilm]] describe Mike's storylines as serious and give him the nickname "motorbike Mike".&lt;ref name=motorbike&gt;{{cite web|title=Top 10 TV Stars Turned Movie Stars|url=http://www.lovefilm.com/features/top-lists/top_tvstarsturnedmoviestars|work=[[LoveFilm]]|publisher=([[Amazon.com]])|accessdate=23 January 2011 |archiveurl=https://www.webcitation.org/5vxBRZHzz?url=http://www.lovefilm.com/features/top-lists/top_tvstarsturnedmoviestars|dead-url=yes|archivedate=23 January 2011}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
*[https://web.archive.org/web/20040811025748/http://www.bbc.co.uk/neighbours/whoswho/characterbiogs/index.shtml?content%2F_mikeyoung%2Fpage1 Character profile] at [[BBC Online]]

{{Neighbours}}
{{Neighbours characters|past}}

{{DEFAULTSORT:Young, Mike}}
[[Category:Neighbours characters]]
[[Category:Fictional schoolteachers]]
[[Category:Fictional mathematicians]]
[[Category:Fictional characters introduced in 1986]]
[[Category:Fictional domestic abuse victims]]</text>
      <sha1>m7as4ojq50is2d8zfcxrrtbvp8b4uad</sha1>
    </revision>
  </page>
  <page>
    <title>Model-driven integration</title>
    <ns>0</ns>
    <id>13099198</id>
    <revision>
      <id>776024078</id>
      <parentid>771500517</parentid>
      <timestamp>2017-04-18T14:30:49Z</timestamp>
      <contributor>
        <username>MrOllie</username>
        <id>6908984</id>
      </contributor>
      <comment>/* External links */ rm advert</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1167">In [[software design]], '''model-driven integration''' is a subset of [[model-driven architecture]] (MDA) which focuses purely on solving Application Integration problems using executable [[Unified Modeling Language]] (UML).

==External links==
*{{cite journal |url=http://www.tdan.com/view-articles/4989 |title=Model Driven Information Architecture |last=Noggle |first=Brian J |author2=M Lang |date=1 April 2002|journal=The Data Administration Newsletter}}
*{{cite journal |url=http://csdl2.computer.org/persagen/DLAbsToc.jsp?resourcePath=/dl/mags/so/&amp;toc=comp/mags/so/2003/05/s5toc.xml&amp;DOI=10.1109/MS.2003.1231153 |title=Model-Driven Integration Using Existing Models |publisher=IEEE Computer Society |last=Denno |first=Peter |author2=MP Steves |author3=D Libes |author4=EJ Barkmeyer  |date=Sep–Oct 2003 |volume=20 |journal=Software |issue=5 |pages=59–63 |doi=10.1109/MS.2003.1231153}}
* [http://www.metada.com/products/integration/case_csmw.html "Model-Driven Integration in Financial Services"] case-study by [http://www.metada.com Metada], 2008

{{DEFAULTSORT:Model-Driven Integration}}
[[Category:Systems engineering]]
[[Category:Unified Modeling Language]]</text>
      <sha1>eesv6w4i6isqkec4dcdws5g4kc3jk26</sha1>
    </revision>
  </page>
  <page>
    <title>Network theory</title>
    <ns>0</ns>
    <id>766409</id>
    <revision>
      <id>868613752</id>
      <parentid>868613702</parentid>
      <timestamp>2018-11-13T10:06:51Z</timestamp>
      <contributor>
        <ip>72.86.37.38</ip>
      </contributor>
      <comment>/* Implementations */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="28785">{{about||the theory regarding the regulation of the adaptive immune system|immune network theory|the sociological theory|social network}}
[[File:Small Network.png|thumb|right|A small example network with eight vertices and ten edges]]
{{Network science}}

'''Network theory''' is the study of [[Graph (discrete mathematics)|graphs]] as a representation of either [[symmetric relation]]s or [[directed graph|asymmetric relations]] between discrete objects. In [[computer science]] and [[network science]], network theory is a part of [[graph theory]]: a network can be defined as a graph in which nodes and/or edges have attributes (e.g. names).

Network theory has applications in many disciplines including [[statistical physics]], [[particle physics]], computer science, [[electrical engineering]]&lt;ref&gt;{{Cite journal|last=Saleh|first=Mahmoud|last2=Esa|first2=Yusef|last3=Mohamed|first3=Ahmed|date=2018-05-29|title=Applications of Complex Network Analysis in Electric Power Systems|url=http://www.mdpi.com/1996-1073/11/6/1381|journal=Energies|language=en|volume=11|issue=6|pages=1381|doi=10.3390/en11061381}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=https://ieeexplore.ieee.org/document/8191215/|title=Optimal microgrids placement in electric distribution systems using complex network framework - IEEE Conference Publication|website=ieeexplore.ieee.org|language=en-US|access-date=2018-06-07}}&lt;/ref&gt;, [[biology]],&lt;ref&gt;{{Cite journal|last=Habibi|first=Iman|last2=Emamian|first2=Effat S.|last3=Abdi|first3=Ali|date=2014-01-01|title=Quantitative analysis of intracellular communication and signaling errors in signaling networks|url=https://dx.doi.org/10.1186/s12918-014-0089-z|journal=BMC Systems Biology|volume=8|pages=89|doi=10.1186/s12918-014-0089-z|issn=1752-0509|pmc=4255782|pmid=25115405}}&lt;/ref&gt; [[economics]], [[finance]], [[operations research]], [[climatology]] and [[sociology]]. Applications of network theory include [[Logistics|logistical]] networks, the [[World Wide Web]], Internet, [[gene regulatory network]]s, metabolic networks, [[social networks]], [[epistemological]] networks, etc.; see [[List of network theory topics]] for more examples.

Euler's solution of the [[Seven Bridges of Königsberg|Seven Bridges of Königsberg problem]] is considered to be the first true proof in the theory of networks.&lt;ref&gt;{{cite journal |last=Newman |first=M. E. J. |title=The structure and function of complex networks |url=http://www-personal.umich.edu/~mejn/courses/2004/cscs535/review.pdf |publisher=Department of Physics, University of Michigan}}&lt;/ref&gt;

== Network optimization ==
[[File:Network Partition for Optimization.svg|left|alt=Network Optimization|thumb|Break down a NP-hard network optimization task into subtasks by discarding of the most irrelevant interactions in network.&lt;ref name="Network partition"&gt;{{cite journal|author1=Ignatov, D.Yu.|author2=Filippov, A.N.|author3=Ignatov, A.D.|author4=Zhang, X.|title=Automatic Analysis, Decomposition and Parallel Optimization of Large Homogeneous Networks|journal=Proc. ISP RAS|date=2016|volume=28|pages=141–152|doi=10.15514/ISPRAS-2016-28(6)-10|arxiv=1701.06595|url=https://arxiv.org/pdf/1701.06595.pdf}}&lt;/ref&gt;]]

Network problems that involve finding an optimal way of doing something are studied under the name  [[combinatorial optimization]]. Examples include [[flow network|network flow]], [[shortest path problem]], [[transport problem]], [[transshipment problem]], [[Facility location problem|location problem]], [[Matching (graph theory)|matching problem]], [[assignment problem]], [[packing problem]], [[routing| routing problem]], [[critical path analysis]] and [[PERT]] (Program Evaluation &amp; Review Technique). In order to break a [[NP-hardness|NP-hard]] task of network optimization down into subtasks the network is decomposed into relatively independent subnets.&lt;ref name="Network partition"/&gt;

== Network analysis ==

=== Electric network analysis ===
The electric power systems analysis could be conducted using network theory from two main points of view: 

(1) an abstract perspective (i.e., as a graph consists from nodes and edges), regardless of the electric power aspects (e.g., transmission line impedances). Most of these studies focus only on the abstract structure of the power grid using node degree distribution and betweenness distribution, which introduces substantial insight regarding the vulnerability assessment of the grid. Through these types of studies, the category of the grid structure could be identified from the complex network perspective (e.g., single-scale, scale-free). This classification might help the electric power system engineers in the planning stage or while upgrading the infrastructure (e.g., add a new transmission line) to maintain a proper redundancy level in the transmission system.&lt;ref&gt;{{Cite journal|last=Saleh|first=Mahmoud|last2=Esa|first2=Yusef|last3=Mohamed|first3=Ahmed|date=2018-05-29|title=Applications of Complex Network Analysis in Electric Power Systems|url=http://www.mdpi.com/1996-1073/11/6/1381|journal=Energies|language=en|volume=11|issue=6|pages=1381|doi=10.3390/en11061381}}&lt;/ref&gt;

(2) weighted graphs that blend an abstract understanding of complex network theories and electric power systems properties.&lt;ref&gt;{{Cite web|url=https://ieeexplore.ieee.org/document/8191215/|title=Optimal microgrids placement in electric distribution systems using complex network framework - IEEE Conference Publication|website=ieeexplore.ieee.org|language=en-US|access-date=2018-06-07}}&lt;/ref&gt;

===Social network analysis===
[[File:Social Network Analysis Visualization.png|thumb|right|Visualization of social network analysis&lt;ref&gt;{{Cite journal | volume = 10| issue = 3| last = Grandjean| first = Martin| title = La connaissance est un réseau| journal =Les Cahiers du Numérique| accessdate = 2014-10-15| date = 2014| pages = 37–54| url = http://www.cairn.info/resume.php?ID_ARTICLE=LCN_103_0037| doi=10.3166/lcn.10.3.37-54}}&lt;/ref&gt;]]'''[[Social network analysis]]''' examines the structure of relationships between social entities.&lt;ref&gt;[[Wasserman, Stanley]] and Katherine Faust. 1994. ''Social Network Analysis: Methods and Applications.'' Cambridge: Cambridge University Press. Rainie, Lee and [[Barry Wellman]], ''Networked: The New Social Operating System.'' Cambridge, MA: [[MIT]] Press, 2012.
&lt;/ref&gt; These entities are often persons, but may also be [[Group (sociology)|groups]], [[organizations]], [[nation states]], [[web sites]], or [[scientometrics|scholarly publications]].

Since the 1970s, the empirical study of networks has played a central role in social science, and many of the [[Mathematics|mathematical]] and [[Statistics|statistical]] tools used for studying networks have been first developed in [[sociology]].&lt;ref name="Newman"&gt;Newman, M.E.J. ''Networks: An Introduction.'' Oxford University Press. 2010&lt;/ref&gt;  Amongst many other applications, social network analysis has been used to understand the [[diffusion of innovations]], news and rumors.  Similarly, it has been used to examine the spread of both [[epidemiology|diseases]] and [[Medical sociology|health-related behaviors]].  It has also been applied to the [[Economic sociology|study of markets]], where it has been used to examine the role of trust {{Citation needed|date=March 2015}} in [[Social exchange|exchange relationships]] and of social mechanisms in setting prices.  Similarly, it has been used to study recruitment into [[political movement]]s and social organizations.  It has also been used to conceptualize scientific disagreements as well as academic prestige.  More recently, network analysis (and its close cousin [[traffic analysis]]) has gained a significant use in military intelligence, for uncovering insurgent networks of both hierarchical and [[leaderless resistance|leaderless]] nature.{{citation needed|date=July 2015}}

===Biological network analysis===
{{see also|Metabolic network|proteome|metabolome|omics}}
With the recent explosion of publicly available high throughput [[biological data]], the analysis of molecular networks has gained significant interest.&lt;ref&gt;{{Cite journal|last=Habibi|first=Iman|last2=Emamian|first2=Effat S.|last3=Abdi|first3=Ali|date=2014-10-07|title=Advanced Fault Diagnosis Methods in Molecular Networks|url=http://journals.plos.org/plosone/article?id=10.1371/journal.pone.0108830|journal=PLOS ONE|volume=9|issue=10|pages=e108830|doi=10.1371/journal.pone.0108830|issn=1932-6203|pmc=4188586|pmid=25290670|bibcode=2014PLoSO...9j8830H}}&lt;/ref&gt; The type of analysis in this context is closely related to social network analysis, but often focusing on local patterns in the network. For example, [[network motif]]s are small subgraphs that are over-represented in the network. Similarly, [[Network motif#Activity motifs|activity motifs]] are patterns in the attributes of nodes and edges in the network that are over-represented given the network structure. The analysis of [[biological network]]s with respect to diseases has led to the development of the field of [[network medicine]].&lt;ref&gt;{{cite journal | last1 = Barabási | first1 = A. L. | last2 = Gulbahce | first2 = N. | last3 = Loscalzo | first3 = J. | year = 2011 | title = Network medicine: a network-based approach to human disease | journal = Nature Reviews Genetics | volume = 12 | issue = 1| pages = 56–68 | doi=10.1038/nrg2918 | pmid=21164525 | pmc=3140052}}&lt;/ref&gt;  Recent examples of application of network theory in biology include applications to understanding the [[cell cycle]].&lt;ref&gt;{{cite journal | last1 = Jailkhani | first1 = N. | last2 = Ravichandran | first2 = N. | last3 = Hegde | first3 = S. R. | last4 = Siddiqui | first4 = Z. | last5 = Mande | first5 = S. C. | last6 = Rao | first6 = K. V. | year = | title = Delineation of key regulatory elements identifies points of vulnerability in the mitogen-activated signaling network | url = | journal = Genome Research | volume = 21 | issue = 12| pages = 2067–81 | doi=10.1101/gr.116145.110| pmc = 3227097 }}&lt;/ref&gt;
The interactions between physiological systems like brain, heart, eyes, etc. can be regarded as a physiological network.&lt;ref name="BashanBartsch2012"&gt;{{cite journal|last1=Bashan|first1=Amir|last2=Bartsch|first2=Ronny P.|last3=Kantelhardt|first3=Jan. W.|last4=Havlin|first4=Shlomo|last5=Ivanov|first5=Plamen Ch.|title=Network physiology reveals relations between network topology and physiological function|journal=Nature Communications|volume=3|year=2012|pages=702|issn=2041-1723|doi=10.1038/ncomms1705|pmid=22426223|pmc=3518900|arxiv=1203.0242|bibcode=2012NatCo...3E.702B}}&lt;/ref&gt;

=== Narrative network analysis ===
[[File:Tripletsnew2012.png|thumb|right|Narrative network of US Elections 2012&lt;ref name="Reference"&gt;Automated analysis of the US presidential elections using Big Data and network analysis; S Sudhahar, GA Veltri, N Cristianini; Big Data &amp; Society 2 (1), 1–28, 2015&lt;/ref&gt;]]
The automatic parsing of ''[[Text corpus|textual corpora]]'' has enabled the extraction of actors and their relational networks on a vast scale. The resulting [[narrative network]]s, which can contain thousands of nodes, are then analysed by using tools from Network theory to identify the key actors, the key communities or parties, and general properties such as robustness or structural stability of the overall network, or centrality of certain nodes.&lt;ref&gt;Network analysis of narrative content in large corpora; S Sudhahar, G De Fazio, R Franzosi, N Cristianini; Natural Language Engineering, 1–32, 2013&lt;/ref&gt; This automates the approach introduced by Quantitative Narrative Analysis,&lt;ref&gt;Quantitative Narrative Analysis; Roberto Franzosi; Emory University © 2010&lt;/ref&gt; whereby subject-verb-object triplets are identified with pairs of actors linked by an action, or pairs formed by actor-object.&lt;ref name="Reference" /&gt;

===Link analysis===
[[Link analysis]] is a subset of network analysis, exploring associations between objects. An example may be examining the addresses of suspects and victims, the telephone numbers they have dialed and financial transactions that they have partaken in during a given timeframe, and the familial relationships between these subjects as a part of police investigation. Link analysis here provides the crucial relationships and associations between very many objects of different types that are not apparent from isolated pieces of information. Computer-assisted or fully automatic computer-based link analysis is increasingly employed by [[bank]]s and [[insurance]] agencies in [[fraud]] detection, by telecommunication operators in telecommunication network analysis, by medical sector in [[epidemiology]] and [[pharmacology]], in law enforcement [[Criminal procedure|investigation]]s, by [[search engine]]s for [[relevance]] rating (and conversely by the [[search engine spammer|spammers]] for [[spamdexing]] and by business owners for [[search engine optimization]]), and everywhere else where relationships between many objects have to be analyzed. Links are  also derived from similarity of time behavior in both nodes. Examples include climate networks where the links between two locations (nodes) are determined for example, by the similarity of the rainfall or temperature fluctuations in both sites.&lt;ref name="TsonisSwanson2006"&gt;{{cite journal|last1=Tsonis|first1=Anastasios A.|last2=Swanson|first2=Kyle L.|last3=Roebber|first3=Paul J.|title=What Do Networks Have to Do with Climate?|journal=Bulletin of the American Meteorological Society|volume=87|issue=5|year=2006|pages=585–595|issn=0003-0007|doi=10.1175/BAMS-87-5-585|bibcode=2006BAMS...87..585T}}&lt;/ref&gt;&lt;ref name="YamasakiGozolchiani2008"&gt;{{cite journal|last1=Yamasaki|first1=K.|last2=Gozolchiani|first2=A.|last3=Havlin|first3=S.|title=Climate Networks around the Globe are Significantly Affected by El Niño|journal=Physical Review Letters|volume=100|issue=22|year=2008|issn=0031-9007|doi=10.1103/PhysRevLett.100.228501|bibcode=2008PhRvL.100v8501Y|pmid=18643467|page=228501}}&lt;/ref&gt;&lt;ref name="Boers2014"&gt;{{cite journal|last1=Boers|first1=N.|last2=Bookhagen|first2=B.|last3=Barbosa|first3=H.M.J.|last4=Marwan|first4=N.|last5=[[Jürgen Kurths|Kurths]]|first5=J.|title=Prediction of extreme floods in the eastern Central Andes based on a complex networks approach|journal=Nature Communications|volume=5|year=2014|issn=2041-1723|doi=10.1038/ncomms6199|pages=5199|pmid=25310906|bibcode=2014NatCo...5E5199B}}&lt;/ref&gt;

====Network robustness====
The structural robustness of networks is studied using [[percolation theory]].&lt;ref&gt;{{cite book |title= Complex Networks: Structure, Robustness and Function |author1=R. Cohen |author2=S. Havlin |year= 2010 |publisher= Cambridge University Press |url= http://havlin.biu.ac.il/Shlomo%20Havlin%20books_com_net.php}}&lt;/ref&gt; When a  critical fraction of nodes (or links) is removed the network becomes fragmented into small disconnected clusters. This phenomenon is called percolation,&lt;ref&gt;{{cite book |title= Fractals and Disordered Systems |author1=A. Bunde |author2=S. Havlin |year= 1996 |publisher= Springer |url= http://havlin.biu.ac.il/Shlomo%20Havlin%20books_fds.php}}&lt;/ref&gt;  and it represents an order-disorder type of [[phase transition]] with [[critical exponents]]. Percolation theory can predict the size of the largest component (called giant component), the critical threshold and the critical exponents.

====Web link analysis====
Several [[Web search]] [[ranking]] algorithms use link-based centrality metrics, including [[Google]]'s [[PageRank]], Kleinberg's [[HITS algorithm]], the [[CheiRank]] and [[TrustRank]] algorithms. Link analysis is also conducted in information science and communication science in order to understand and extract information from the structure of collections of web pages. For example, the analysis might be of the interlinking between politicians' web sites or blogs. Another use is for classifying pages according to their mention in other pages.&lt;ref&gt;{{cite journal|last=Attardi|first=G.|author2=S. Di Marco |author3=D. Salvi |title=Categorization by Context|journal=Journal of Universal Computer Science|year=1998|volume=4|issue=9|pages=719–736|url=http://www.jucs.org/jucs_4_9/categorisation_by_context/Attardi_G.pdf}}&lt;/ref&gt;

===Centrality measures===
Information about the relative importance of nodes and edges in a graph can be obtained through [[centrality]] measures, widely used in disciplines like [[sociology]]. For example, [[eigenvector centrality]] uses the [[eigenvectors]] of the [[adjacency matrix]] corresponding to a network, to determine nodes that tend to be frequently visited. Formally established measures of centrality are [[degree centrality]], [[closeness centrality]], [[betweenness centrality]], [[eigenvector centrality]], [[subgraph centrality]] and [[Katz centrality]]. The purpose or objective of analysis generally determines the type of centrality measure to be used. For example, if one is interested in dynamics on networks or the robustness of a network to node/link removal, often the [[dynamical importance]]&lt;ref&gt;{{cite journal|last=Restrepo|first=Juan|author2= E. Ott|author3= B. R. Hunt|title=Characterizing the Dynamical Importance of Network Nodes and Links|journal=Phys. Rev. Lett.|year=2006|volume=97|pages=094102|url=http://prl.aps.org/abstract/PRL/v97/i9/e094102| doi = 10.1103/PhysRevLett.97.094102|issue=9|pmid=17026366 |arxiv=cond-mat/0606122|bibcode=2006PhRvL..97i4102R}}&lt;/ref&gt;  of a node is the most relevant centrality measure.For a centrality measure based on k-core analysis see ref.&lt;ref name="CarmiHavlin2007"&gt;{{cite journal|last1=Carmi|first1=S.|last2=Havlin|first2=S.|last3=Kirkpatrick|first3=S.|last4=Shavitt|first4=Y.|last5=Shir|first5=E.|title=A model of Internet topology using k-shell decomposition|journal=Proceedings of the National Academy of Sciences|volume=104|issue=27|year=2007|pages=11150–11154|issn=0027-8424|doi=10.1073/pnas.0701175104|pmid=17586683|pmc=1896135|arxiv=cs/0607080|bibcode=2007PNAS..10411150C}}&lt;/ref&gt;

===Assortative and disassortative mixing===
{{see|Assortative mixing}}
These concepts are used to characterize the linking preferences of hubs in a network. Hubs are nodes which have a large number of links. Some hubs tend to link to other hubs while others avoid connecting to hubs and prefer to connect to nodes with low connectivity. We say a hub is assortative when it tends to connect to other hubs. A disassortative hub avoids connecting to other hubs. If hubs have connections with the expected random probabilities, they are said to be neutral. There are three methods to quantify degree correlations.

===Recurrence networks===
The recurrence matrix of a [[recurrence plot]] can be considered as the adjacency matrix of an undirected and unweighted network. This allows for the analysis of time series by network measures. Applications range from detection of regime changes over characterizing dynamics to synchronization analysis.&lt;ref name="marwan2009"&gt;{{cite journal|last1=Marwan|first1=N.|last2=Donges|first2=J.F.|last3=Zou|first3=Y.|last4=Donner|first4=R.V.|last5=Kurths|first5=J.|title=Complex network approach for recurrence analysis of time series|journal=Physics Letters A|volume=373|issue=46|year=2009|pages=4246–4254|issn=0375-9601|doi=10.1016/j.physleta.2009.09.042|arxiv=0907.3368|bibcode=2009PhLA..373.4246M}}&lt;/ref&gt;&lt;ref name="donner2011"&gt;{{cite journal|last1=Donner|first1=R.V.|last2=Heitzig|first2=J.|last3=Donges|first3=J.F.|last4=Zou|first4=Y.|last5=Marwan|first5=N.|last6=Kurths|first6=J.|title=The Geometry of Chaotic Dynamics – A Complex Network Perspective|journal=European Physical Journal B|volume=84|year=2011|pages=653–672|issn=1434-6036|doi=10.1140/epjb/e2011-10899-1|arxiv=1102.1853|bibcode=2011EPJB...84..653D}}&lt;/ref&gt;&lt;ref name="feldhoff2013"&gt;{{cite journal|last1=Feldhoff|first1=J.H.|last2=Donner|first2=R.V.|last3=Donges|first3=J.F.|last4=Marwan|first4=N.|last5=Kurths|first5=J.|title=Geometric signature of complex synchronisation scenarios|journal=Europhysics Letters|volume=102|issue=3|year=2013|pages=30007|issn=1286-4854|doi=10.1209/0295-5075/102/30007|arxiv=1301.0806|bibcode=2013EL....10230007F}}&lt;/ref&gt;

== Spread ==
Content in a [[complex network]] can spread via two major methods: conserved spread and non-conserved spread.&lt;ref&gt;Newman, M., Barabási, A.-L., Watts, D.J. [eds.] (2006) The Structure and Dynamics of Networks. Princeton, N.J.: Princeton University Press.&lt;/ref&gt;  In conserved spread, the total amount of content that enters a complex network remains constant as it passes through.  The model of conserved spread can best be represented by a pitcher containing a fixed amount of water being poured into a series of funnels connected by tubes .  Here, the pitcher represents the original source and the water is the content being spread.  The funnels and connecting tubing represent the nodes and the connections between nodes, respectively.  As the water passes from one funnel into another, the water disappears instantly from the funnel that was previously exposed to the water.  In non-conserved spread, the amount of content changes as it enters and passes through a complex network.  The model of non-conserved spread can best be represented by a continuously running faucet running through a series of funnels connected by tubes.  Here, the amount of water from the original source is infinite. Also, any funnels that have been exposed to the water continue to experience the water even as it passes into successive funnels.  The non-conserved model is the most suitable for explaining the transmission of most [[infectious diseases]], neural excitation, information and rumors, etc.

== Interdependent networks ==
An interdependent network is a system of coupled networks where nodes of one or more networks depend on nodes in other networks. Such dependencies are enhanced by the developments in modern technology. Dependencies may lead to cascading failures between the networks and a relatively small failure can lead to a catastrophic breakdown of the system. Blackouts are a fascinating demonstration of the important role played by the dependencies between networks. A recent study developed a framework to study the cascading failures in an interdependent networks system.&lt;ref&gt;{{Cite journal|author1=S. V. Buldyrev |author2=R. Parshani |author3=G. Paul |author4=H. E. Stanley |author5=S. Havlin |title = Catastrophic cascade of failures in interdependent networks|journal = Nature |volume = 464 |pages = 1025–28 |year = 2010 | doi=10.1038/nature08932 |issue=7291|url=http://havlin.biu.ac.il/Publications.php?keyword=Catastrophic+cascade+of+failures+in+interdependent+networks&amp;year=*&amp;match=all |pmid=20393559|arxiv=0907.1182 |bibcode=2010Natur.464.1025B }}&lt;/ref&gt;&lt;ref&gt;{{cite journal|author1=Jianxi Gao|author2=Sergey V. Buldyrev|author3=Shlomo Havlin|author4=H. Eugene Stanley|title=Robustness of a Network of Networks|journal=Phys. Rev. Lett.|year=2011|volume=107|pages=195701|url=http://havlin.biu.ac.il/Publications.php?keyword=Robustness+of+a+Tree-like+Network+of+Interdependent+Networks&amp;year=*&amp;match=all | doi = 10.1103/PhysRevLett.107.195701|issue=19|pmid=22181627 |arxiv=1010.5829|bibcode=2011PhRvL.107s5701G}}&lt;/ref&gt;

== Implementations ==
*[http://igraph.sourceforge.net igraph], an [[open source]] [[C (programming language)|C]] library for the analysis of large-scale complex networks, with interfaces to [[R (programming language)|R]], [[Python (programming language)|Python]] and [[Ruby (programming language)|Ruby]].
*[[Graph-tool]] and [[NetworkX]], [[Free Software|free]] and efficient Python modules for manipulation and statistical analysis of networks.
*[[Orange (software)|Orange]], an [[open-source software|open-source]] [[data mining]] software suite with its Network add-on.
*[http://pajek.imfm.si/doku.php Pajek], program for (large) network analysis and visualization.
*[http://www.graphmatcher.com GraphMatcher], a [[Java (programming language)|Java]] program to align two or more networks.
*[[Tulip (software)|Tulip]], a free data mining and visualization software dedicated to the analysis and visualization of relational data.
*[http://semoss.org/ SEMOSS], an [[Resource Description Framework|RDF]]-based [[Open-source software|open source]] context-aware analytics tool written in [[Java (programming language)|Java]] leveraging the [[SPARQL]] query language.
*[http://graphstream-project.org/ GraphStream] is a [[Java (programming language)|Java]] library for the modeling and analysis of dynamic graphs. You can generate, import, export, measure, layout and visualize them.
*[http://www.datawalk.com DataWalk] a program for correlating (large) data sets for performing network analysis and visualization with reusable workflows, alerts, and risk scoring.

== See also ==
{{div col|colwidth=20em}}
*[[Complex network]]
*[[Congestion game]]
*[[Quantum complex network]]
*[[Dual-phase evolution]]
*[[Percolation]]
*[[Network partition]]
*[[Network science]]
*[[Network theory in risk assessment]]
*[[Network topology]]
*[[Network management|Network analyzer]]
*[[Seven Bridges of Königsberg]]
*[[Small-world networks]]
*[[Social network]]
*[[Scale-free networks]]
*[[Network dynamics]]
*[[Sequential dynamical system]]s
*[[Pathfinder network]]s
*[[Human disease network]]
*[[Biological network]]
*[[Network medicine]]
{{div col end}}

== References ==
{{Reflist}}

== Books ==
*S.N. Dorogovtsev and J.F.F. Mendes, ''Evolution of Networks: from biological networks to the Internet and WWW'', Oxford University Press, 2003, {{isbn|0-19-851590-1}}
*G. Caldarelli,  "Scale-Free Networks", Oxford University Press, 2007, {{isbn|978-0-19-921151-7}}
*A. Barrat, M. Barthelemy, A. Vespignani,  "Dynamical Processes on Complex Networks", Cambridge University Press, 2008, {{isbn|978-0521879507}}
*E. Estrada,  "The Structure of Complex Networks: Theory and Applications", Oxford University Press, 2011, {{isbn|978-0-199-59175-6}}
*K. Soramaki and S. Cook, "Network Theory and Financial Risk", Risk Books, 2016 {{ISBN|978-1782722199}} &lt;ref&gt;{{Cite book|url=https://www.worldcat.org/oclc/973733901|title=Network theory and financial risk|last=Kimmo.|first=Soramäki,|date=2016|publisher=Risk Books|others=Cook, Samantha (Statistician)|isbn=978-1782722199|location=[London]|oclc=973733901}}&lt;/ref&gt;
*V. Latora, V. Nicosia, G. Russo, "Complex Networks: Principles, Methods and Applications", Cambridge University Press, 2017, {{isbn|978-1107103184}}

== External links ==
{{wikiquote}}
*[http://netwiki.amath.unc.edu/ netwiki] Scientific wiki dedicated to network theory
*[http://www.networkcultures.org/networktheory/ New Network Theory] International Conference on 'New Network Theory'
*[http://nwb.slis.indiana.edu/ Network Workbench]: A Large-Scale Network Analysis, Modeling and Visualization Toolkit
* [https://www.slideshare.net/DmitryIgnatovPhD/network-optimization-82005426 Optimization of the Large Network] &lt;span style="font-size:70%"&gt;[[doi:10.13140/RG.2.2.20183.06565/6]]&lt;/span&gt;
*[http://www.orgnet.com/SocialLifeOfRouters.pdf Network analysis of computer networks]
*[http://www.orgnet.com/orgnetmap.pdf Network analysis of organizational networks]
*[http://firstmonday.org/htbin/cgiwrap/bin/ojs/index.php/fm/article/view/941/863 Network analysis of terrorist networks]
*[http://www.orgnet.com/AJPH2007.pdf Network analysis of a disease outbreak]
*[http://linkanalysis.wlv.ac.uk/ Link Analysis: An Information Science Approach] (book)
*[http://gephi.org/2008/how-kevin-bacon-cured-cancer/ Connected: The Power of Six Degrees] (documentary)
* {{cite journal | last1 = Kitsak | first1 = M. | last2 = Gallos | first2 = L. K. | last3 = Havlin | first3 = S. | last4 = Liljeros | first4 = F. | last5 = Muchnik | first5 = L. | last6 = Stanley | first6 = H. E. | last7 = Makes | first7 = H.A. | year = 2010 | title = Influential Spreaders in Networks | url = http://havlin.biu.ac.il/Publications.php?keyword=Identification+of+influential+spreaders+in+complex+networks++&amp;year=*&amp;match=all | journal = Nature Physics | volume = 6 | issue = | page = 888 | doi=10.1038/nphys1746| arxiv = 1001.5285 | bibcode = 2010NatPh...6..888K }}
*[http://havlin.biu.ac.il/course4.php A short course on complex networks]
*[http://barabasilab.neu.edu/courses/phys5116/ A course on complex network analysis by Albert-László Barabási]
*[http://www.risk.net/type/technical-paper/source/journal-of-network-theory-in-finance/ The Journal of Network Theory in Finance]
*[https://www.informs.org/About-INFORMS/History-and-Traditions/OR-Methodologies/Networks-and-Graphs Network theory in Operations Research] from the Institute for Operations Research and the Management Sciences (INFORMS)

&lt;!--Eponymous category:--&gt;
[[Category:Network theory| ]]
&lt;!--Other categories:--&gt;
[[Category:Networks]]
[[Category:Graph theory]]

[[fi:Verkkoteoria]]</text>
      <sha1>iyxihh1u60jwtc7opw6m063rrx6farl</sha1>
    </revision>
  </page>
  <page>
    <title>Online codes</title>
    <ns>0</ns>
    <id>1145820</id>
    <revision>
      <id>863263112</id>
      <parentid>796355337</parentid>
      <timestamp>2018-10-09T18:11:28Z</timestamp>
      <contributor>
        <ip>73.71.156.214</ip>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4089">In [[computer science]], '''online codes''' are an example of [[rateless erasure codes]]. These codes can encode a message into a number of symbols such that knowledge of any fraction of them allows one to recover the original message (with high probability). ''Rateless'' codes produce an arbitrarily large number of symbols which can be broadcast until the receivers have enough symbols.

[[Image:online_codes_highlevel.png|frame|High level view of the use of ''online codes'']]

The online encoding [[algorithm]] consists of several phases. First the message is split into ''n'' fixed size message blocks. Then the ''outer encoding'' is an [[erasure code]] which produces auxiliary blocks that are appended to the message blocks to form a composite message.

From this the inner encoding generates check blocks. Upon receiving a certain number of check blocks some fraction of the composite message can be recovered. Once enough has been recovered the outer decoding can be used to recover the original message.

==Detailed discussion==
Online codes are parameterised by the block size and two scalars, ''q'' and ''ε''. The authors suggest ''q''=3 and ε=0.01. These parameters set the balance between the complexity and performance of the encoding. A message of ''n'' blocks can be recovered, [[with high probability]], from (1+3ε)''n'' check blocks. The probability of failure is (ε/2)&lt;sup&gt;q+1&lt;/sup&gt;.

===Outer encoding===
Any erasure code may be used as the outer encoding, but the author of online codes suggest the following.

For each message block, pseudo-randomly choose ''q'' auxiliary blocks 
(from a total of 0.55''q''ε''n'' auxiliary blocks) to attach it to. Each auxiliary block is then the XOR of all the message blocks which have been attached to it.

===Inner encoding===
[[Image:online_codes_time_against_blocks_graph.png|frame|A graph of check blocks received against number of message blocks fixed for a 10000 block message.]]

The inner encoding takes the composite message and generates a stream of check blocks. A check block is the XOR of all the blocks from the composite message that it is attached to.

The ''degree'' of a check block is the number of blocks that it is attached to. The degree is determined by sampling a random distribution, ''p'', which is defined as:

:&lt;math&gt;F=\left\lceil\frac{\ln(\epsilon^2/4)}{\ln(1-\epsilon/2)}\right\rceil&lt;/math&gt;

:&lt;math&gt;p_1=1-\frac{1+1/F}{1+\epsilon}&lt;/math&gt;

:&lt;math&gt;p_i=\frac{(1-p_1)F}{(F-1)i(i-1)}&lt;/math&gt; for &lt;math&gt;2\le i\le F&lt;/math&gt;

Once the degree of the check block is known, the blocks from the composite message which it is attached to are chosen uniformly.

===Decoding===
Obviously the decoder of the inner stage must hold check blocks which it cannot currently decode. A check block can only be decoded when all but one of the blocks which it is attached to are known. The graph to the left shows the progress of an inner decoder. The x-axis plots the number of check blocks received and the dashed line shows the number of check blocks which cannot currently be used. This climbs almost linearly at first as many check blocks with degree &amp;gt; 1 are received but unusable. At a certain point,  some of the check blocks are suddenly usable, resolving more blocks which then causes more check blocks to be usable. Very quickly the whole file can be decoded.

As the graph also shows the inner decoder falls just shy of decoding everything for a little while after having received ''n'' check blocks. The outer encoding ensures that a few elusive blocks from the inner decoder are not an issue, as the file can be recovered without them.

==External links==
* [https://cs.nyu.edu/media/publications/TR2002-833.pdf Original paper]
* [http://pdos.csail.mit.edu/~petar/papers/maymounkov-bigdown-lncs.ps Rateless Codes and Big Downloads] (A more accessible paper by the same author)
* [http://pdos.csail.mit.edu/~petar/pubs.html Papers by Petar Maymounkov]
* [http://rubyforge.org/projects/archipelago/ A Ruby project hosted at RubyForge containing a Ruby library for Online Coding]

[[Category:Coding theory]]</text>
      <sha1>3nijc9xbzzcgyo21120h5vzmcbn1rwq</sha1>
    </revision>
  </page>
  <page>
    <title>Palm calculus</title>
    <ns>0</ns>
    <id>31776869</id>
    <revision>
      <id>845887678</id>
      <parentid>454221614</parentid>
      <timestamp>2018-06-14T20:16:30Z</timestamp>
      <contributor>
        <ip>2601:445:437F:FE66:A97B:E721:C3FA:28DD</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2527">{{Expert-subject|Probability|date=May 2011}}

In the study of [[stochastic process]]es, '''Palm calculus''', named after Swedish [[teletraffic engineering|teletrafficist]] [[Conny Palm]], is the study of the relationship between [[probability|probabilities]] conditioned on a specified event and time-average probabilities.  A Palm probability or Palm [[expected value|expectation]], often denoted &lt;math&gt;P^0(\cdot)&lt;/math&gt; or &lt;math&gt;E^0[\cdot]&lt;/math&gt;, is a probability or expectation conditioned on a specified event occurring at time 0.

==Little's formula==
A simple example of a formula from Palm calculus is [[Little's law]] &lt;math&gt;L=\lambda W&lt;/math&gt;, which states that the time-average number of users (''L'') in a system is equal to the product of the rate (&lt;math&gt;\lambda&lt;/math&gt;) at which users arrive and the Palm-average waiting time (''W'') that a user spends in the system.  That is, the average ''W'' gives equal weight to the waiting time of all customers, rather than being the time-average of "the waiting times of the customers currently in the system".

==Feller's paradox==
An important example of the use of Palm probabilities is Feller's paradox, often associated with the analysis of an [[Pollaczek–Khinchine formula|M/G/1 queue]].  This states that the (time-)average time between the previous and next points in a [[point process]] is greater than the expected interval between points.  The latter is the Palm expectation of the former, conditioning on the event that a point occurs at the time of the observation.  This paradox occurs because large intervals are given greater weight in the time average than small intervals.
&lt;!--
==PASTA==
A useful result in Palm calculus is that "Poisson arrivals see time averages" (PASTA).  That means that, if the event being conditioned on is a point in a [[Poisson process]] (independent of the process being observed), then the distinction between Palm probabilities and time average probabilities can be ignored.
--&gt;

==References==
*{{cite journal|last=Le Boudec|first=Jean-Yves|year=2007|title=Understanding the simulation of mobility models with Palm calculus|journal=[[Performance Evaluation]]|volume=64|issue=2|pages=126&amp;ndash;147|doi=10.1016/j.peva.2006.03.001|url=http://lcawww.epfl.ch/Publications/LeBoudec/LeBoudecV04.pdf}}
*Palm, C. (1943) "Intensitätsschwankungen im Fernsprechverkehr" ''Ericsson Techniks'', No. 44 {{MR|11402}}

[[Category:Queueing theory]]
[[Category:Stochastic calculus]]
[[Category:Telecommunication theory]]
{{probability-stub}}</text>
      <sha1>n3ozq655bng5dfyk3vv0hk7oml5s8o0</sha1>
    </revision>
  </page>
  <page>
    <title>Parareal</title>
    <ns>0</ns>
    <id>47661313</id>
    <revision>
      <id>870948788</id>
      <parentid>870934921</parentid>
      <timestamp>2018-11-28T00:04:26Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor/>
      <comment>Dating maintenance tags: {{Citation needed}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="23436">'''Parareal''' is a [[parallel algorithm]] from [[numerical analysis]] and used for the solution of [[initial value problem]]s.&lt;ref&gt;{{cite journal
| last       = Lions
| first      =  Jacques-Louis
| last2      = Maday
| first2     = Yvon
| last3      = Turinici
| first3     = Gabriel
| date       = 2015
| title      = A "parareal" in time discretization of PDE's
| journal    = Comptes Rendus de l'Académie des Sciences, Série I
| volume     = 332
| issue      = 7
| pages      = 661–668
| bibcode    =2001CRASM.332..661L 
| doi        = 10.1016/S0764-4442(00)01793-6
| url      =  https://hal.archives-ouvertes.fr/hal-00798372/file/CRAS_01_lions_maday_turinici.pdf
}}&lt;/ref&gt;
It has been introduced in 2001 by Lions, Maday and Turinici. Since then, it has become one of the most widely studied parallel-in-time integration methods {{Citation needed|date=November 2018}}.

== Parallel-in-time integration methods ==

In contrast to e.g. [[Runge–Kutta methods|Runge-Kutta]] or [[Linear multistep method|multi-step]] methods, some of the computations in Parareal can be performed [[Parallel computing|in parallel]] and
Parareal is therefore one example of a ''parallel-in-time'' integration method.
While historically most efforts to parallelize the [[Numerical partial differential equations|numerical solution]] of [[partial differential equation]]s focussed on the spatial discretization, in view of the challenges from [[exascale computing]], parallel methods for [[temporal discretization]] have been identified as a possible way to increase concurrency in [[List of numerical analysis software|numerical software]].&lt;ref&gt;{{Cite report
 | author     = Jack Dongarra
 |author2=Jeffrey Hittinger |author3=John Bell |author4=Luis Chacon |author5=Robert Falgout |author6=Michael Heroux |author7=Paul Hovland |author8=Esmond Ng |author9=Clayton Webster |author10=Stefan Wild
 | date       = March 2014
 | title      = Applied Mathematics Research for Exascale Computing
 | url        = http://science.energy.gov/~/media/ascr/pdf/research/am/docs/EMWGreport.pdf
 | publisher  = US Department of Energy
 | accessdate = August 2015
}}&lt;/ref&gt;
Because Parareal computes the numerical solution for multiple time steps in parallel, it is categorized as a ''parallel across the steps'' method.&lt;ref&gt;{{cite journal
| last       = Burrage
| first      =  Kevin
| date       = 1997
| title      = Parallel methods for ODEs
| url        =
| journal    = Advances in Computational Mathematics
| volume     = 7
| issue      = 1–2
| pages      = 1–31
| bibcode    = 
| doi        = 10.1023/A:1018997130884
}}&lt;/ref&gt;
This is in contrast to approaches using ''parallelism across the method'' like parallel Runge-Kutta&lt;ref&gt;{{Cite journal|last=Iserles|first=A.|last2=NøRSETT|first2=S. P.|date=1990-10-01|title=On the Theory of Parallel Runge—Kutta Methods|url=http://imajna.oxfordjournals.org/content/10/4/463|journal=IMA Journal of Numerical Analysis|language=en|volume=10|issue=4|pages=463–488|doi=10.1093/imanum/10.4.463|issn=0272-4979}}&lt;/ref&gt; or extrapolation methods,&lt;ref&gt;{{Cite journal|last=Ketcheson|first=David|last2=Waheed|first2=Umair bin|date=2014-06-13|title=A comparison of high-order explicit Runge–Kutta, extrapolation, and deferred correction methods in serial and parallel|url=http://msp.org/camcos/2014/9-2/p01.xhtml|journal=Communications in Applied Mathematics and Computational Science|volume=9|issue=2|pages=175–200|doi=10.2140/camcos.2014.9.175|issn=2157-5452|arxiv=1305.6165}}&lt;/ref&gt; where independent stages can be computed in parallel or ''parallel across the system'' methods like waveform relaxation.

== History ==

Parareal can be derived as both a [[multigrid method]] in time method or as [[Direct multiple shooting method|multiple shooting]] along the time axis.&lt;ref name="gander2007"&gt;{{cite journal
| last       = Gander
| first      =  Martin J.
| last2      = Vandewalle
| first2     = Stefan
| date       = 2007
| title      = Analysis of the Parareal Time‐Parallel Time‐Integration Method
| journal    = SIAM Journal on Scientific Computing
| volume     = 29
| issue      = 2
| pages      = 556–578
| bibcode    = 
| doi        = 10.1137/05064607X
| citeseerx      =  10.1.1.154.6042
}}&lt;/ref&gt;
Both ideas, multigrid in time as well as adopting multiple shooting for time integration, go back to the 1980s and 1990s.&lt;ref&gt;{{cite book
| last       = Hackbusch
| first      =  Wolfgang
| date       = 1985
| title      = Parabolic multi-grid methods
| url        = http://dl.acm.org/citation.cfm?id=4673.4714
| journal    = Computing Methods in Applied Sciences and Engineering, VI
| volume     = 
| issue      = 
| pages      = 189–197 
| bibcode    = 
| doi        = 
| access-date= August 2015
| isbn      =  9780444875976
}}&lt;/ref&gt;&lt;ref&gt;
{{cite journal
| last       = Kiehl
| first      =  Martin
| date       = 1994
| title      = Parallel multiple shooting for the solution of initial value problems
| journal    = Parallel Computing
| volume     = 20
| issue      = 3
| pages      =  275–295
| bibcode    = 
| doi        = 10.1016/S0167-8191(06)80013-X
}}&lt;/ref&gt;
Parareal is a widely studied method and has been used and modified for a range of different applications.&lt;ref&gt;{{cite conference
 | last = Gander
 | first = Martin J.
 | authorlink =
 | title = 50 years of Time Parallel Time Integration
 | publisher = Springer International Publishing
 | series = Contributions in Mathematical and Computational Sciences
 | volume = 9
 | edition = 1
 | date = 2015
 | location =
 | pages =
 | language =
 | url =
 | doi = 10.1007/978-3-319-23321-5
 | id =
 | isbn = 978-3-319-23321-5
 | mr =
 | zbl =
 | jfm = }}&lt;/ref&gt;
Ideas to parallelize the solution of initial value problems go back even further: the first paper proposing a parallel-in-time integration method appeared in 1964.&lt;ref&gt;
{{cite journal
| last       = Nievergelt
| first      = Jürg
| date       = 1964
| title      =  Parallel methods for integrating ordinary differential equations
| journal    = Communications of the ACM
| volume     = 7
| issue      =  12
| pages      =  731–733
| bibcode    = 
| doi        = 10.1145/355588.365137
}}&lt;/ref&gt;

== Algorithm ==
[[File:Parareal Animation.ogg|thumb|708x708px|Visualization of the Parareal algorithm. The coarse propagator here is labelled &lt;math&gt;\bar{\varphi}&lt;/math&gt; whereas the fine propagator is labelled &lt;math&gt;\varphi&lt;/math&gt;.]]
Parareal solves an initial value problem of the form

&lt;math&gt; \dot{y}(t) = f(y(t), t), \quad y(t_0) = y_0 \quad \text{with} \quad t_0 \leq t \leq T.&lt;/math&gt;

Here, the right hand side &lt;math&gt;f&lt;/math&gt; can correspond to the spatial discretization of a partial differential equation in a [[method of lines]] approach.

Parareal now requires a [[Domain decomposition methods|decomposition]] of the time interval &lt;math&gt;[t_0, T]&lt;/math&gt; into &lt;math&gt;P&lt;/math&gt; so-called time slices &lt;math&gt;[t_j, t_{j+1}]&lt;/math&gt; such that

&lt;math&gt; [t_0, T] = [t_0, t_1] \cup [t_1, t_2] \cup \ldots \cup [t_{P-1}, t_{P} ].&lt;/math&gt;

Each time slice is assigned to one processing unit when parallelizing the algorithm, so that &lt;math&gt;P&lt;/math&gt; is equal to the number of processing units used for Parareal: in an [[Message Passing Interface|MPI]] based code for example, this would be the number of processes, while in an [[OpenMP]] based code, &lt;math&gt;P&lt;/math&gt; would be equal to the number of [[Thread (computing)|threads]].

Parareal is based on the iterative application of two methods for [[Numerical methods for ordinary differential equations|integration of ordinary differential equations]].
One, commonly labelled &lt;math&gt;\mathcal{F}&lt;/math&gt;, should be of high accuracy and computational cost while the other, typically labelled &lt;math&gt;\mathcal{G}&lt;/math&gt;, must be computationally cheap but can be much less accurate. 
Typically, some form of Runge-Kutta method is chosen for both coarse and fine integrator, where &lt;math&gt;\mathcal{G}&lt;/math&gt; might be of lower order and use a larger time step than &lt;math&gt;\mathcal{F}&lt;/math&gt;.
If the initial value problem stems from the discretization of a PDE, &lt;math&gt;\mathcal{G}&lt;/math&gt; can also use a coarser spatial discretization, but this can negatively impact convergence unless high order interpolation is used.&lt;ref&gt;{{Cite journal|last=Ruprecht|first=Daniel|date=2014-12-01|title=Convergence of Parareal with spatial coarsening|journal=PAMM|language=en|volume=14|issue=1|pages=1031–1034|doi=10.1002/pamm.201410490|issn=1617-7061|url=http://eprints.whiterose.ac.uk/90536/1/paper.pdf}}&lt;/ref&gt;
The result of numerical integration with one of these methods over a time slice &lt;math&gt;[t_{j}, t_{j+1}]&lt;/math&gt; for some starting value &lt;math&gt;y_j&lt;/math&gt; given at &lt;math&gt;t_j&lt;/math&gt; is then written as

&lt;math&gt; y = \mathcal{F}(y_j, t_j, t_{j+1}) &lt;/math&gt;   or   &lt;math&gt;y = \mathcal{G}(y_j, t_j, t_{j+1}) &lt;/math&gt;.

Serial time integration with the fine method would then correspond to a step-by-step computation of

&lt;math&gt; y_{j+1} = \mathcal{F}(y_j, t_j, t_{j+1}), \quad j=0, \ldots, P-1.&lt;/math&gt;

Parareal instead uses the following iteration

&lt;math&gt; y_{j+1}^{k+1} = \mathcal{G}(y^{k+1}_j, t_j, t_{j+1}) + \mathcal{F}(y^k_j, t_j, t_{j+1}) - \mathcal{G}(y^k_j, t_j, t_{j+1}), \quad j=0, \ldots, P-1, \quad k=0, \ldots, K-1,&lt;/math&gt;

where &lt;math&gt;k&lt;/math&gt; is the iteration counter. 
As the iteration converges and &lt;math&gt;y^{k+1}_j - y^k_j \to 0&lt;/math&gt;, the terms from the coarse method cancel out and Parareal reproduces the solution that is obtained by the serial execution of the fine method only.
It can be shown that Parareal converges after a maximum of &lt;math&gt;P&lt;/math&gt; iterations.&lt;ref name="gander2007" /&gt;
For Parareal to provide speedup, however, it has to converge in a number of iterations significantly smaller than the number of time slices, that is &lt;math&gt;K \ll P&lt;/math&gt;.

In the Parareal iteration, the computationally expensive evaluation of &lt;math&gt;\mathcal{F}(y^k_j, t_j, t_{j+1})&lt;/math&gt; can be performed in parallel on &lt;math&gt;P&lt;/math&gt; processing units.
By contrast, the dependency of &lt;math&gt;y^{k+1}_{j+1}&lt;/math&gt; on &lt;math&gt;\mathcal{G}(y^{k+1}_j, t_j, t_{j+1})&lt;/math&gt; means that the coarse correction has to be computed in serial order.

===Speedup===
Under some assumptions, a simple theoretical model for the [[Amdahl's law|speedup]] of Parareal can be derived.&lt;ref&gt;{{cite journal
| last       = Minion
| first      = Michael L.
| date       = 2010
| title      =  A Hybrid Parareal Spectral Deferred Corrections Method
| journal    = Communications in Applied Mathematics and Computational Science
| volume     = 5
| issue      =  2
| pages      = 265–301
| bibcode    = 
| doi        = 10.2140/camcos.2010.5.265
}}&lt;/ref&gt;
Although in applications these assumptions can be too restrictive, the model still is useful to illustrate the trade offs that are involved in obtaining speedup with Parareal.

First, assume that every time slice &lt;math&gt;[t_j, t_{j+1}]&lt;/math&gt; consists of exactly &lt;math&gt;N_f&lt;/math&gt; steps of the fine integrator and of &lt;math&gt;N_c&lt;/math&gt; steps of the coarse integrator.
This includes in particular the assumption that all time slices are of identical length and that both coarse and fine integrator use a constant step size over the full simulation.
Second, denote by &lt;math&gt;\tau_f&lt;/math&gt; and &lt;math&gt;\tau_c&lt;/math&gt; the computing time required for a single step of the fine and coarse methods, respectively, and assume that both are constant.
This is typically not exactly true when an [[Temporal discretization#Implicit Time Integration|implicit]] method is used, because then runtimes vary depending on the number of iterations required by the [[Iterative method|iterative solver]].

Under these two assumptions, the runtime for the fine method integrating over &lt;math&gt;P&lt;/math&gt; time slices can be modelled as

&lt;math&gt; c_{\text{fine}} = P N_{f} \tau_f. &lt;/math&gt;

The runtime of Parareal using &lt;math&gt;P&lt;/math&gt; processing units and performing &lt;math&gt;K&lt;/math&gt; iterations is

&lt;math&gt; c_{\text{parareal}} = (K+1) P N_{c} \tau_c + K N_{f} \tau_f. &lt;/math&gt;

Speedup of Parareal then is

&lt;math&gt; S_{p} = \frac{c_{\text{fine}}}{c_{\text{parareal}}} = \frac{1}{ (K+1) \frac{N_c}{N_f} \frac{\tau_c}{\tau_f} + \frac{K}{P}} \leq \min\left\{ \frac{N_f \tau_f}{N_c \tau_c}, \frac{P}{K}  \right\}.&lt;/math&gt;

These two bounds illustrate the trade off that has to be made in choosing the coarse method: on the one hand, it has to be cheap and/or use a much larger time step to make the first bound as large as possible, on the other hand the number of iterations &lt;math&gt;K&lt;/math&gt; has to be kept low to keep the second bound large.
In particular, [[Speedup#Additional Details|Parareal's parallel efficiency]] is bounded by

&lt;math&gt; E_{p} = \frac{S_p}{P} \leq \frac{1}{K},&lt;/math&gt;

that is by the inverse of the number of required iterations.

=== Instability for imaginary eigenvalues ===
The vanilla version of Parareal has issues for problems with imaginary [[Eigenvalues and eigenvectors|eigenvalues]].&lt;ref name="gander2007" /&gt; It typically only converges toward the very last iterations, that is as &lt;math&gt;k&lt;/math&gt; approaches &lt;math&gt;P&lt;/math&gt;, and the speedup &lt;math&gt; S_p&lt;/math&gt; is always going to be smaller than one.  So either the number of iterations is small and Parareal is unstable or, if &lt;math&gt;k&lt;/math&gt; is large enough to make Parareal stable, no speedup is possible. This also means that Parareal is typically unstable for [[Hyperbolic partial differential equation|hyperbolic]] equations.&lt;ref&gt;{{Cite book|title=Stability of the Parareal Algorithm|last=Staff|first=Gunnar Andreas|last2=Rønquist|first2=Einar M.|date=2005-01-01|publisher=Springer Berlin Heidelberg|isbn=9783540225232|editor-last=Barth|editor-first=Timothy J.|series=Lecture Notes in Computational Science and Engineering|pages=449–456|language=en|doi=10.1007/3-540-26825-1_46|editor-last2=Griebel|editor-first2=Michael|editor-last3=Keyes|editor-first3=David E.|editor-last4=Nieminen|editor-first4=Risto M.|editor-last5=Roose|editor-first5=Dirk|editor-last6=Schlick|editor-first6=Tamar|editor-last7=Kornhuber|editor-first7=Ralf|editor-last8=Hoppe|editor-first8=Ronald|editor-last9=Périaux|editor-first9=Jacques}}&lt;/ref&gt; Even though the formal analysis by Gander and Vandewalle covers only linear problems with constant coefficients, the problem also arises when Parareal is applied to the nonlinear [[Navier-stokes equations|Navier-Stokes equations]] when the [[viscosity]] coefficient becomes too small and the [[Reynolds number]] too large.&lt;ref&gt;{{Cite book|title=Convergence of Parareal for the Navier-Stokes Equations Depending on the Reynolds Number|last=Steiner|first=Johannes|last2=Ruprecht|first2=Daniel|last3=Speck|first3=Robert|last4=Krause|first4=Rolf|date=2015-01-01|publisher=Springer International Publishing|isbn=9783319107042|editor-last=Abdulle|editor-first=Assyr|series=Lecture Notes in Computational Science and Engineering|pages=195–202|language=en|doi=10.1007/978-3-319-10705-9_19|editor-last2=Deparis|editor-first2=Simone|editor-last3=Kressner|editor-first3=Daniel|editor-last4=Nobile|editor-first4=Fabio|editor-last5=Picasso|editor-first5=Marco|citeseerx = 10.1.1.764.6242}}&lt;/ref&gt; Different approaches exist to stabilise Parareal,&lt;ref&gt;{{Cite journal|last=Dai|first=X.|last2=Maday|first2=Y.|date=2013-01-01|title=Stable Parareal in Time Method for First- and Second-Order Hyperbolic Systems|journal=SIAM Journal on Scientific Computing|volume=35|issue=1|pages=A52–A78|doi=10.1137/110861002|issn=1064-8275}}&lt;/ref&gt;&lt;ref name=":0"&gt;{{Cite journal|last=Farhat|first=Charbel|last2=Cortial|first2=Julien|last3=Dastillung|first3=Climène|last4=Bavestrello|first4=Henri|date=2006-07-30|title=Time-parallel implicit integrators for the near-real-time prediction of linear structural dynamic responses|journal=International Journal for Numerical Methods in Engineering|language=en|volume=67|issue=5|pages=697–724|doi=10.1002/nme.1653|issn=1097-0207|bibcode=2006IJNME..67..697F}}&lt;/ref&gt;&lt;ref name=":1"&gt;{{Cite book|title=On the Use of Reduced Basis Methods to Accelerate and Stabilize the Parareal Method|last=Chen|first=Feng|last2=Hesthaven|first2=Jan S.|last3=Zhu|first3=Xueyu|date=2014-01-01|publisher=Springer International Publishing|isbn=9783319020891|editor-last=Quarteroni|editor-first=Alfio|series=MS&amp;A - Modeling, Simulation and Applications|pages=187–214|language=en|doi=10.1007/978-3-319-02090-7_7|editor-last2=Rozza|editor-first2=Gianluigi|url = http://infoscience.epfl.ch/record/190666}}&lt;/ref&gt; one being Krylov-subspace enhanced Parareal.

== Variants ==
There are multiple algorithms that are directly based or at least inspired by the original Parareal algorithm.

=== Krylov-subspace enhanced Parareal ===
Early on it was recognised that for linear problems information generated by the fine method &lt;math&gt;\mathcal{F}_{\delta t}&lt;/math&gt; can be used to improve the accuracy of the coarse method &lt;math&gt;\mathcal{G}_{\Delta t}&lt;/math&gt;.&lt;ref name=":0" /&gt; Originally, the idea was formulated for the parallel implicit time-integrator PITA,&lt;ref&gt;{{Cite journal|last=Farhat|first=Charbel|last2=Chandesris|first2=Marion|date=2003-11-07|title=Time-decomposed parallel time-integrators: theory and feasibility studies for fluid, structure, and fluid–structure applications|journal=International Journal for Numerical Methods in Engineering|language=en|volume=58|issue=9|pages=1397–1434|doi=10.1002/nme.860|issn=1097-0207|bibcode=2003IJNME..58.1397F}}&lt;/ref&gt; a method closely related to Parareal but with small differences in how the correction is done. In every iteration &lt;math&gt;k&lt;/math&gt; the result &lt;math&gt;\mathcal{F}_{\delta t}(y^k_j)&lt;/math&gt; is computed for values &lt;math&gt;u^k_j \in \mathbb{R}^d&lt;/math&gt; for &lt;math&gt;j=0, \ldots, P-1&lt;/math&gt;. Based on this information, the [[Vector space|subspace]]

&lt;math&gt;
S_k := \left\{ y^{k'}_j : 0 \leq k' \leq k, j=0, \ldots, P-1 \right\} &lt;/math&gt;

is defined and updated after every Parareal iteration.&lt;ref&gt;{{Cite journal|last=Gander|first=M.|last2=Petcu|first2=M.|title=Analysis of a Krylov subspace enhanced parareal algorithm for linear problems|journal=ESAIM: Proceedings|volume=25|pages=114–129|doi=10.1051/proc:082508|year=2008}}&lt;/ref&gt; Denote as &lt;math&gt; P_k&lt;/math&gt; the [[orthogonal projection]] from &lt;math&gt; \mathbb{R}^d&lt;/math&gt; to &lt;math&gt; S_k&lt;/math&gt;. Then, replace the coarse method with the improved integrator &lt;math&gt; \mathcal{K}_{\Delta t}(y) = \mathcal{F}_{\delta t}(P_k y) + \mathcal{G}_{\Delta t}((I-P_k)y)&lt;/math&gt;.

As the number of iterations increases, the space &lt;math&gt; S_k&lt;/math&gt; will grow and the modified propagator &lt;math&gt; \mathcal{K}_{\Delta t}&lt;/math&gt; will become more accurate. This will lead to faster convergence. This version of Parareal can also stably integrate linear hyperbolic partial differential equations.&lt;ref&gt;{{Cite journal|last=Ruprecht|first=D.|last2=Krause|first2=R.|date=2012-04-30|title=Explicit parallel-in-time integration of a linear acoustic-advection system|url=http://www.sciencedirect.com/science/article/pii/S0045793012000709|journal=Computers &amp; Fluids|volume=59|pages=72–83|doi=10.1016/j.compfluid.2012.02.015|arxiv=1510.02237}}&lt;/ref&gt; An extension to nonlinear problems based on the reduced basis method exists as well.&lt;ref name=":1" /&gt;

=== Hybrid Parareal spectral deferred corrections ===
A method with improved parallel efficiency based on a combination of Parareal with spectral deferred corrections (SDC) &lt;ref&gt;{{Cite journal|last=Dutt|first=Alok|last2=Greengard|first2=Leslie|last3=Rokhlin|first3=Vladimir|date=2000-06-01|title=Spectral Deferred Correction Methods for Ordinary Differential Equations|journal=BIT Numerical Mathematics|language=en|volume=40|issue=2|pages=241–266|doi=10.1023/A:1022338906936|issn=0006-3835}}&lt;/ref&gt; has been proposed by M. Minion.&lt;ref&gt;{{Cite journal|last=Minion|first=Michael|date=2011-01-05|title=A hybrid parareal spectral deferred corrections method|url=http://msp.org/camcos/2010/5-2/p05.xhtml|journal=Communications in Applied Mathematics and Computational Science|volume=5|issue=2|pages=265–301|doi=10.2140/camcos.2010.5.265|issn=2157-5452}}&lt;/ref&gt; It limits the choice for coarse and fine integrator to SDC, sacrificing flexibility for improved parallel efficiency. Instead of the limit of &lt;math&gt;1/K&lt;/math&gt;, the bound on parallel efficiency in the hybrid method becomes

&lt;math&gt;E_p \leq \frac{K_s}{K_p}&lt;/math&gt;

with &lt;math&gt;K_s&lt;/math&gt; being the number of iterations of the serial SDC base method and &lt;math&gt;K_p&lt;/math&gt; the typically greater number of iterations of the parallel hybrid method. The Parareal-SDC hybrid has been further improved by addition of a ''full approximation scheme''  as used in nonlinear [[Multigrid method|multigrid]]. This led to the development of the ''parallel full approximation scheme in space and time'' (PFASST).&lt;ref&gt;{{Cite journal|last=Emmett|first=Matthew|last2=Minion|first2=Michael|date=2012-03-28|title=Toward an efficient parallel in time method for partial differential equations|url=http://msp.org/camcos/2012/7-1/p04.xhtml|journal=Communications in Applied Mathematics and Computational Science|volume=7|issue=1|pages=105–132|doi=10.2140/camcos.2012.7.105|issn=2157-5452}}&lt;/ref&gt; Performance of PFASST has been studied for PEPC, a [[Barnes–Hut simulation|Barnes-Hut]] tree code based particle solver developed at [[Jülich Supercomputing Centre|Juelich Supercomputing Centre]]. Simulations using all 262,144 cores on the IBM [[Blue Gene|BlueGene]]/P system JUGENE showed that PFASST could produce additional speedup beyond saturation of the spatial tree parallelisation.&lt;ref&gt;{{Cite book|last=Speck|first=R.|last2=Ruprecht|first2=D.|last3=Krause|first3=R.|last4=Emmett|first4=M.|last5=Minion|first5=M.|last6=Winkel|first6=M.|last7=Gibbon|first7=P.|date=2012-11-01|title=A massively space-time parallel N-body solver|url=http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=6468522|journal=High Performance Computing, Networking, Storage and Analysis (SC), 2012 International Conference for|pages=1–11|doi=10.1109/SC.2012.6|isbn=978-1-4673-0805-2}}&lt;/ref&gt;

=== Multigrid reduction in time (MGRIT) ===
The multigrid reduction in time method (MGRIT) generalises the interpretation of Parareal as a multigrid-in-time algorithms to multiple levels using different smoothers.&lt;ref&gt;{{Cite journal|last=Falgout|first=R.|last2=Friedhoff|first2=S.|last3=Kolev|first3=T.|last4=MacLachlan|first4=S.|last5=Schroder|first5=J.|date=2014-01-01|title=Parallel Time Integration with Multigrid|journal=SIAM Journal on Scientific Computing|volume=36|issue=6|pages=C635–C661|doi=10.1137/130944230|issn=1064-8275|citeseerx=10.1.1.701.2603}}&lt;/ref&gt; It is a more general approach but for a specific choice of parameters it is equivalent to Parareal. The [http://computation.llnl.gov/projects/parallel-time-integration-multigrid XBraid] library implementing MGRIT is being developed by [[Lawrence Livermore National Laboratory]].

=== ParaExp ===
ParaExp uses [[exponential integrator]]s within Parareal.&lt;ref&gt;{{Cite journal|last=Gander|first=M.|last2=Güttel|first2=S.|date=2013-01-01|title=PARAEXP: A Parallel Integrator for Linear Initial-Value Problems|journal=SIAM Journal on Scientific Computing|volume=35|issue=2|pages=C123–C142|doi=10.1137/110856137|issn=1064-8275|citeseerx=10.1.1.800.5938}}&lt;/ref&gt; While limited to linear problems, it can produce almost optimal parallel speedup.

== External links ==
*[https://www.parallel-in-time.org/ parallel-in-time.org]

==References==
{{Reflist}}

[[Category:Numerical analysis]]
[[Category:Numerical differential equations]]
[[Category:Parallel computing]]
[[Category:Computational science]]</text>
      <sha1>illnwvq6z6c55qhzwjx9sg3csh8qu76</sha1>
    </revision>
  </page>
  <page>
    <title>Perturbation (astronomy)</title>
    <ns>0</ns>
    <id>1490148</id>
    <revision>
      <id>863077283</id>
      <parentid>858547792</parentid>
      <timestamp>2018-10-08T15:34:49Z</timestamp>
      <contributor>
        <username>Sushant savla</username>
        <id>3708758</id>
      </contributor>
      <comment>Converted Image to SVG</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="20219">{{merge from|Orbital_perturbation_analysis|discuss=Talk:Perturbation_(astronomy)#Merge_from_https://en.wikipedia.org/wiki/Orbital_perturbation_analysis|date=August 2018}}

[[File:Moon perturbation diagram.svg|thumb|300px|alt=Vector diagram of the Sun's perturbations on the Moon. When the gravitational force of the Sun common to both the Earth and the Moon is subtracted, what is left is the perturbations.|The perturbing forces of the [[Sun]] on the [[Moon]] at two places in its [[orbit]]. The blue arrows represent the [[Euclidean vector|direction and magnitude]] of the gravitational force on the [[Earth]]. Applying this to both the Earth's and the Moon's position does not disturb the positions relative to each other. When it is subtracted from the force on the Moon (black arrows), what is left is the perturbing force (red arrows) on the Moon relative to the Earth. Because the perturbing force is different in direction and magnitude on opposite sides of the orbit, it produces a change in the shape of the orbit.]]

In [[astronomy]], '''perturbation''' is the complex motion of a [[astronomical object|massive body]] subject to forces other than the [[gravity|gravitational]] attraction of a single other [[mass]]ive [[physical body|body]].&lt;ref&gt;Bate, Mueller, White (1971): ch. 9, p. 385.&lt;/ref&gt; The other forces can include a third (fourth, fifth, etc.) body, [[drag (physics)|resistance]], as from an [[atmosphere]], and the off-center attraction of an [[oblate spheroid|oblate]] or otherwise misshapen body.&lt;ref name="moulton"/&gt;

== Introduction ==
The study of perturbations began with the first attempts to predict planetary motions in the sky. In ancient times the causes were a mystery. [[Isaac Newton|Newton]], at the time he formulated his laws of [[Newton's laws of motion|motion]] and of [[Newton's law of universal gravitation|gravitation]], applied them to the first analysis of perturbations,&lt;ref name="moulton"/&gt; recognizing the complex difficulties of their calculation.&lt;ref&gt;Newton in 1684 wrote: "By reason of the deviation of the Sun from the center of gravity, the centripetal force does not always tend to that immobile center, and hence the planets neither move exactly in ellipses nor revolve twice in the same orbit. Each time a planet revolves it traces a fresh orbit, as in the motion of the Moon, and each orbit depends on the combined motions of all the planets, not to mention the action of all these on each other. But to consider simultaneously all these causes of motion and to define these motions by exact laws admitting of easy calculation exceeds, if I am not mistaken, the force of any human mind." (quoted by Prof G E Smith (Tufts University), in [http://google.com/search?q=cache:8RItNNOcJJoJ:www.stanford.edu/dept/cisst/SmithPowerpointTalk1.ppt "Three Lectures on the Role of Theory in Science"] 1. Closing the loop: Testing Newtonian Gravity, Then and Now); and Prof R F Egerton (Portland State University, Oregon) after quoting the same passage from Newton concluded: [http://physics.pdx.edu/~egertonr/ph311-12/newton.htm "Here, Newton identifies the "many body problem" which remains unsolved analytically."] {{webarchive |url=https://web.archive.org/web/20050310192531/http://physics.pdx.edu/~egertonr/ph311-12/newton.htm |date=2005-03-10}}&lt;/ref&gt; Many of the great mathematicians since then have given attention to the various problems involved; throughout the 18th and 19th centuries there was demand for accurate tables of the position of the [[Moon]] and [[planet]]s for [[marine navigation]].

The complex motions of gravitational perturbations can be broken down. The hypothetical motion that the body follows under the gravitational effect of one other body only is typically a [[conic section]], and can be readily described with the methods of [[geometry]]. This is called a [[two-body problem]], or an unperturbed [[Kepler orbit|Keplerian orbit]]. The differences between that and the actual motion of the body are '''perturbations''' due to the additional gravitational effects of the remaining body or bodies. If there is only one other significant body then the perturbed motion is a [[three-body problem]]; if there are multiple other bodies it is an [[n-body problem|''n''-body problem]]. A general analytical solution (a mathematical expression to predict the positions and motions at any future time) exists for the two-body problem; when more than two bodies are considered analytic solutions exist only for special cases. Even the two-body problem becomes insoluble if one of the bodies is irregular in shape.&lt;ref name="roy"&gt;Roy (1988): ch. 6, 7.&lt;/ref&gt;

[[File:Mercury perturbation comparison.png|thumb|300px|alt=Plot of Mercury's position in its orbit, with and without perturbations from various planets. The perturbations cause Mercury to move in looping paths around its unperturbed position.|[[Mercury (planet)|Mercury]]'s orbital longitude and latitude, as perturbed by [[Venus]], [[Jupiter]] and all of the planets of the [[Solar System]], at intervals of 2.5 days. Mercury would remain centered on the crosshairs if there were no perturbations.]]

Most systems that involve multiple gravitational attractions present one primary body which is dominant in its effects (for example, a [[star]], in the case of the star and its planet, or a planet, in the case of the planet and its satellite). The gravitational effects of the other bodies can be treated as perturbations of the hypothetical unperturbed motion of the planet or [[satellite]] around its primary body.

== Mathematical analysis ==

=== General perturbations ===
In methods of '''general perturbations''', general differential equations, either of motion or of change in the [[orbital elements]], are solved analytically, usually by [[series expansion]]s. The result is usually expressed in terms of algebraic and trigonometric functions of the orbital elements of the body in question and the perturbing bodies. This can be applied generally to many different sets of conditions, and is not specific to any particular set of gravitating objects.&lt;ref&gt;Bate, Mueller, White (1971): p. 387; sec. 9.4.3, p. 410.&lt;/ref&gt; Historically, general perturbations were investigated first. The classical methods are known as ''variation of the elements'', ''[[variation of parameters]]'' or ''variation of the constants of integration''. In these methods, it is considered that the body is always moving in a [[conic section]], however the conic section is constantly changing due to the perturbations. If all perturbations were to cease at any particular instant, the body would continue in this (now unchanging) conic section indefinitely; this conic is known as the [[osculating orbit]] and its [[orbital elements]] at any particular time are what are sought by the methods of general perturbations.&lt;ref name="moulton"/&gt;

General perturbations takes advantage of the fact that in many problems of [[celestial mechanics]], the two-body orbit changes rather slowly due to the perturbations; the two-body orbit is a good first approximation. General perturbations is applicable only if the perturbing forces are about one order of magnitude smaller, or less, than the gravitational force of the primary body.&lt;ref name="roy"/&gt; In the [[Solar System]], this is usually the case; [[Jupiter]], the second largest body, has a mass of about 1/1000 that of the [[Sun]].

General perturbation methods are preferred for some types of problems, as the source of certain observed motions are readily found. This is not necessarily so for special perturbations; the motions would be predicted with similar accuracy, but no information on the configurations of the perturbing bodies (for instance, an [[orbital resonance]]) which caused them would be available.&lt;ref name="roy"/&gt;

=== Special perturbations ===
In methods of '''special perturbations''', numerical datasets, representing values for the positions, velocities and accelerative forces on the bodies of interest, are made the basis of [[numerical integration]] of the differential [[equations of motion]].&lt;ref&gt;Bate, Mueller, White (1971), pp. 387–409.&lt;/ref&gt; In effect, the positions and velocities are perturbed directly, and no attempt is made to calculate the curves of the orbits or the [[orbital elements]].&lt;ref name=moulton&gt;Moulton (1914): ch. IX&lt;/ref&gt;

Special perturbations can be applied to any problem in [[celestial mechanics]], as it is not limited to cases where the perturbing forces are small.&lt;ref name="roy"/&gt; Once applied only to comets and minor planets, special perturbation methods are now the basis of the most accurate machine-generated [[Fundamental ephemeris|planetary ephemerides]] of the great astronomical almanacs.&lt;ref name="moulton"/&gt;&lt;ref&gt;See, for instance, [[Jet Propulsion Laboratory Development Ephemeris]].&lt;/ref&gt; Special perturbations are also used for [[Orbit Modeling|modeling]] an orbit with computers.

==== Cowell's formulation ====
[[File:Cowells method.png|thumb|Cowell's method. Forces from all perturbing bodies (black and gray) are summed to form the total force on body ''i'' (red), and this is numerically integrated starting from the initial position (the ''epoch of osculation'').]]

Cowell's formulation (so named for [[Philip Herbert Cowell|Philip H. Cowell]], who, with A.C.D. Cromellin, used a similar method to predict the return of Halley's comet) is perhaps the simplest of the special perturbation methods.&lt;ref&gt;{{cite web |url=http://adsabs.harvard.edu/full/1911GOAMM..71O...1C |last1=Cowell |first1=P. H. |last2=Crommelin |first2=A. C. D. |title=Investigation of the Motion of Halley's Comet from 1759 to 1910 |publisher=Neill &amp; Co. |location=Bellevue, for His Majesty's Stationery Office |date=1910 |bibcode=1911GOAMM..71O...1C}}&lt;/ref&gt; In a system of &lt;math&gt;n&lt;/math&gt; mutually interacting bodies, this method mathematically solves for the [[Newton's law of universal gravitation|Newtonian]] forces on body &lt;math&gt;i&lt;/math&gt; by summing the individual interactions from the other &lt;math&gt;j&lt;/math&gt; bodies:
:&lt;math&gt;\mathbf{\ddot{r}}_i = \sum_{\underset{j \ne i}{j=1}}^n {Gm_j (\mathbf{r}_j-\mathbf{r}_i) \over r_{ij}^3}&lt;/math&gt;

where &lt;math&gt;\mathbf{\ddot{r}}_i&lt;/math&gt; is the [[acceleration]] vector of body &lt;math&gt;i&lt;/math&gt;, &lt;math&gt;G&lt;/math&gt; is the [[gravitational constant]], &lt;math&gt;m_j&lt;/math&gt; is the [[mass]] of body &lt;math&gt;j&lt;/math&gt;, &lt;math&gt;\mathbf{r}_i&lt;/math&gt; and &lt;math&gt;\mathbf{r}_j&lt;/math&gt; are the [[position vector]]s of objects &lt;math&gt;i&lt;/math&gt; and &lt;math&gt;j&lt;/math&gt; respectively, and &lt;math&gt;r_{ij}&lt;/math&gt; is the distance from object &lt;math&gt;i&lt;/math&gt; to object &lt;math&gt;j&lt;/math&gt;. All [[Euclidean vector#Physics|vectors]] being referred to the [[Center of mass#Astronomy|barycenter]] of the system. This equation is resolved into components in &lt;math&gt;x&lt;/math&gt;, &lt;math&gt;y&lt;/math&gt;, and &lt;math&gt;z&lt;/math&gt; and these are integrated numerically to form the new velocity and position vectors. This process is repeated as many times as necessary. The advantage of Cowell's method is ease of application and programming. A disadvantage is that when perturbations become large in magnitude (as when an object makes a close approach to another) the errors of the method also become large.&lt;ref name="danby"&gt;
{{cite book 
|last = Danby 
|first = J.M.A. 
| title = Fundamentals of Celestial Mechanics 
|publisher = Willmann-Bell, Inc. 
|edition = second |isbn = 0-943396-20-4 
|date=1988}}, chapter 11.&lt;/ref&gt; 
However, for many problems in [[celestial mechanics]], this is never the case. Another disadvantage is that in systems with a dominant central body, such as the [[Sun]], it is necessary to carry many [[Significant figures|significant digits]] in the [[arithmetic]] because of the large difference in the forces of the central body and the perturbing bodies, although with modern [[computer]]s this is not nearly the limitation it once was.&lt;ref&gt;
{{cite book 
|last = Herget
|first = Paul 
|title = The Computation of Orbits
|publisher = privately published by the author
|date=1948}}, p. 91 ff.&lt;/ref&gt;

==== Encke's method ====
[[File:Enckes method-vector.svg|thumb|Encke's method. Greatly exaggerated here, the small difference  δ'''r''' (blue) between the osculating, unperturbed orbit (black) and the perturbed orbit (red), is numerically integrated starting from the initial position (the ''epoch of osculation'').]]

Encke's method begins with the [[osculating orbit]] as a reference and integrates numerically to solve for the variation from the reference as a function of time.&lt;ref&gt;
{{cite book 
|url=https://books.google.com/books?id=t5VCAQAAMAAJ&amp;source=gbs_navlinks_s
|last = Encke
|first = J. F.
|title = Über die allgemeinen Störungen der Planeten 
|work=Berliner Astronomisches Jahrbuch für 1857
|date=1854
|pages=319–397}}&lt;/ref&gt; 
Its advantages are that perturbations are generally small in magnitude, so the integration can proceed in larger steps (with resulting lesser errors), and the method is much less affected by extreme perturbations. Its disadvantage is complexity; it cannot be used indefinitely without occasionally updating the osculating orbit and continuing from there, a process known as ''rectification''.&lt;ref name="danby"/&gt; Encke's method is similar to the general perturbation method of variation of the elements, except the rectification is performed at discrete intervals rather than continuously.&lt;ref&gt;
Battin (1999), sec. 10.2.&lt;/ref&gt;

Letting &lt;math&gt;\boldsymbol{\rho}&lt;/math&gt; be the [[Position vector|radius vector]] of the [[osculating orbit]], &lt;math&gt;\mathbf{r}&lt;/math&gt; the radius vector of the perturbed orbit, and &lt;math&gt;\delta \mathbf{r}&lt;/math&gt; the variation from the osculating orbit,

{{NumBlk|:|&lt;math&gt;\delta \mathbf{r} = \mathbf{r} - \boldsymbol{\rho}&lt;/math&gt;, and the [[Equations of motion|equation of motion]] of &lt;math&gt;\delta \mathbf{r}&lt;/math&gt; is simply|{{EquationRef|1}}}}

{{NumBlk|:|&lt;math&gt;\ddot{\delta \mathbf{r}} = \mathbf{\ddot{r}} - \boldsymbol{\ddot{\rho}}&lt;/math&gt;.|{{EquationRef|2}}}}

&lt;math&gt;\mathbf{\ddot{r}}&lt;/math&gt; and &lt;math&gt;\boldsymbol{\ddot{\rho}}&lt;/math&gt; are just the equations of motion of &lt;math&gt;\mathbf{r}&lt;/math&gt; and &lt;math&gt;\boldsymbol{\rho},&lt;/math&gt;

{{NumBlk|:|&lt;math&gt;\mathbf{\ddot{r}} = \mathbf{a}_{\text{per}} - {\mu \over r^3} \mathbf{r}&lt;/math&gt; for the perturbed orbit and |{{EquationRef|3}}}}

{{NumBlk|:|&lt;math&gt;\boldsymbol{\ddot{\rho}} = - {\mu \over \rho^3} \boldsymbol{\rho}&lt;/math&gt; for the unperturbed orbit,|{{EquationRef|4}}}}

where &lt;math&gt;\mu = G(M+m)&lt;/math&gt; is the [[Standard gravitational parameter|gravitational parameter]] with &lt;math&gt;M&lt;/math&gt; and &lt;math&gt;m&lt;/math&gt; the [[mass]]es of the central body and the perturbed body, &lt;math&gt;\mathbf{a}_{\text{per}}&lt;/math&gt; is the perturbing [[acceleration]], and &lt;math&gt;r&lt;/math&gt; and &lt;math&gt;\rho&lt;/math&gt; are the magnitudes of &lt;math&gt;\mathbf{r}&lt;/math&gt; and &lt;math&gt;\boldsymbol{\rho}&lt;/math&gt;.

Substituting from equations ({{EquationNote|3}}) and ({{EquationNote|4}}) into equation ({{EquationNote|2}}),

{{NumBlk|:|&lt;math&gt;\ddot{\delta \mathbf{r}} = \mathbf{a}_{\text{per}} + \mu \left( {\boldsymbol{\rho} \over \rho^3} - {\mathbf{r} \over r^3} \right),&lt;/math&gt; |{{EquationRef|5}}}}

which, in theory, could be integrated twice to find &lt;math&gt;\delta \mathbf{r}&lt;/math&gt;. Since the osculating orbit is easily calculated by two-body methods, &lt;math&gt;\boldsymbol{\rho}&lt;/math&gt; and &lt;math&gt;\delta \mathbf{r}&lt;/math&gt; are accounted for and &lt;math&gt;\mathbf{r}&lt;/math&gt; can be solved. In practice, the quantity in the brackets, &lt;math&gt; {\boldsymbol{\rho} \over \rho^3} - {\mathbf{r} \over r^3} &lt;/math&gt;, is the difference of two nearly equal vectors, and further manipulation is necessary to avoid the need for extra [[Significant figures|significant digits]].&lt;ref&gt;
Bate, Mueller, White (1971), sec. 9.3.&lt;/ref&gt;&lt;ref&gt;
Roy (1988), sec. 7.4.&lt;/ref&gt; 
Encke's method was more widely used before the advent of modern [[computer]]s, when much orbit computation was performed on [[Calculating machine|mechanical calculating machines]].

== Periodic nature ==
[[File:Eccentricity rocky planets.jpg|thumb|300px|[http://www.orbitsimulator.com/gravity/articles/what.html Gravity Simulator] plot of the changing [[orbital eccentricity]] of [[Mercury (planet)|Mercury]], [[Venus]], [[Earth]], and [[Mars]] over the next 50,000 years. The 0 point on this plot is the year 2007.]]

In the Solar System, many of the disturbances of one planet by another are periodic, consisting of small impulses each time a planet passes another in its orbit. This causes the bodies to follow motions that are periodic or quasi-periodic &amp;ndash; such as the Moon in its [[Lunar theory|strongly perturbed]] [[Orbit of the Moon|orbit]], which is the subject of [[lunar theory]]. This periodic nature led to the [[discovery of Neptune]] in 1846 as a result of its perturbations of the orbit of [[Uranus]].

On-going mutual perturbations of the planets cause long-term quasi-periodic variations in their [[orbital element]]s, most apparent when two planets' orbital periods are nearly in sync. For instance, five orbits of [[Jupiter]] (59.31 years) is nearly equal to two of [[Saturn]] (58.91 years). This causes large perturbations of both, with a period of 918 years, the time required for the small difference in their positions at [[Conjunction (astronomy and astrology)|conjunction]] to make one complete circle, first discovered by [[Pierre-Simon Laplace|Laplace]].&lt;ref name="moulton"/&gt; [[Venus]] currently has the orbit with the least [[Orbital eccentricity|eccentricity]], i.e. it is the closest to [[Circle|circular]], of all the planetary orbits. In 25,000 years' time, [[Earth]] will have a more circular (less eccentric) orbit than Venus. It has been shown that long-term periodic disturbances within the [[Solar System]] can become chaotic over very long time scales; under some circumstances one or more [[planet]]s can cross the orbit of another, leading to collisions.&lt;ref&gt;see references at [[Stability of the Solar System]]&lt;/ref&gt;

The orbits of many of the minor bodies of the Solar System, such as [[comet]]s, are often heavily perturbed, particularly by the gravitational fields of the [[gas giant]]s. While many of these perturbations are periodic, others are not, and these in particular may represent aspects of [[chaotic motion]]. For example, in April 1996, [[Jupiter]]'s gravitational influence caused the [[Orbital period|period]] of [[Comet Hale–Bopp]]'s orbit to decrease from 4,206 to 2,380 years, a change that will not revert on any periodic basis.&lt;ref name=perturb&gt;{{cite web |date=1997-04-10 |title=Comet Hale–Bopp Orbit and Ephemeris Information |publisher=JPL/NASA |author=Don Yeomans |url=http://www2.jpl.nasa.gov/comet/ephemjpl8.html |access-date=2008-10-23}}&lt;/ref&gt;

== See also ==
* [[Nereid (moon)|Nereid]] one of the outer moons of Neptune with a high [[orbital eccentricity]] of ~0.75 and is frequently perturbed
* [[Osculating orbit]]
* [[Orbital resonance]]
* [[Stability of the Solar System]]
* [[Formation and evolution of the Solar System]]
* [[Orbit modeling]]
* [[Proper orbital elements]]

== References ==
;Bibliography
*{{cite book |last1=Bate |first1=Roger R. |last2=Mueller |first2=Donald D. |last3=White |first3=Jerry E. |title=Fundamentals of Astrodynamics |publisher=[[Dover Publications]] |location=New York |isbn=0-486-60061-0 |date=1971}}
*{{cite book |url=https://books.google.com/books?id=jqM5AAAAMAAJ&amp;printsec=frontcover&amp;source=gbs_ge_summary_r&amp;cad=0#v=onepage&amp;q&amp;f=false |title=An Introduction to Celestial Mechanics |edition=2nd revised |last=Moulton |first=Forest Ray |date=1914}}
*{{cite book |last=Roy |first=A. E. |title=Orbital Motion |publisher=Institute of Physics Publishing |edition=3rd |isbn=0-85274-229-0 |date=1988}}

;Footnotes
{{Reflist}}

== External links ==
*[https://web.archive.org/web/20070907013516/http://main.chemistry.unina.it/~alvitagl/solex/MarsDist.html Solex] (by Aldo Vitagliano) predictions for the position/orbit/close approaches of Mars
*[https://books.google.com/books?id=snK4AAAAIAAJ&amp;source=gbs_navlinks_s Gravitation] Sir George Biddell Airy's 1884 book on gravitational motion and perturbations, using little or no math.(at [https://books.google.com/books Google books])

{{Orbits}}

[[Category:Orbital perturbations|*]]
[[Category:Dynamical systems]]
[[Category:Dynamics of the Solar System]]</text>
      <sha1>hgr7zk461ltgu0x1zke74kxbkoronfl</sha1>
    </revision>
  </page>
  <page>
    <title>Physical information</title>
    <ns>0</ns>
    <id>243627</id>
    <revision>
      <id>871049361</id>
      <parentid>870715420</parentid>
      <timestamp>2018-11-28T16:28:17Z</timestamp>
      <contributor>
        <username>BeardedMenace52</username>
        <id>26718987</id>
      </contributor>
      <comment>Reworded introductory paragraph</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14667">{{lead extra info|date=March 2018}}
'''Physical information''' is a form of [[information]]. In [[physics]], it refers to the information of a [[physical system]]. Physical information is an important concept used in a number of fields of study in physics. For example, in [[quantum mechanics]], the form of physical information known as quantum information is used in many descriptions of quantum phenomena, such as [[observer (quantum physics)|quantum observation]], [[quantum entanglement]] and the [[causality|causal]] relationship between quantum objects that carry out either or both close and long-range interactions with one another.

In a somewhat general sense, the information of a given entity can be interpreted as its identity. As such, its information can be perceived to be the representation of the specification of its existence and thus, to be serving as the full description of each of the properties (real or potentialized) that are responsible for the entity’s existence. This description, of course, is one that, in a sense, is completely divorced from both any and all forms of language.

When clarifying the subject of information, care should be taken to distinguish between the following specific cases:{{citation needed|date=February 2015}}
* The phrase '''instance of information''' refers to the specific [[Instantiation principle|instantiation]] of information (identity, form, essence) that is associated with the being of ''a particular example'' of a thing. (This allows for the reference to separate instances of information that happen to share identical patterns.)
* A '''holder of information''' is a variable or mutable instance that can have different forms at different times (or in different situations).
* A '''piece of information''' is a particular fact about a thing's identity or properties, i.e., a portion of its instance.
* A '''pattern of information''' (or ''form'') is the pattern or content of an instance or piece of information. Many separate pieces of information may share the same form. We can say that those pieces are ''perfectly correlated'' or say that they are ''copies'' of each other, as in copies of a book.
* An '''embodiment of information''' is the thing whose essence is a given instance of information.
* A '''representation of information''' is an encoding of some pattern of information within some other pattern or instance.
* An '''interpretation of information''' is a decoding of a pattern of information as being a representation of another specific pattern or fact.
* A '''subject of information''' is the thing that is identified or described by a given instance or piece of information. (Most generally, a thing that is a subject of information could be either abstract or concrete; either mathematical or physical.)
* An '''amount of information''' is a quantification of ''how large'' a given instance, piece, or pattern of information is, or how much of a given system's information content (its instance) has a given attribute, such as being known or unknown. Amounts of information are most naturally characterized in [[logarithm]]ic units.

As the above usages are all conceptually distinct from each other, overloading the word "information" (by itself) to denote (or connote) several of these concepts simultaneously can lead to confusion. Accordingly, this article uses more detailed phrases, such as those shown in bold above, whenever the intended meaning is not made clear by the context.

==Classical versus quantum information==
The instance of information that is contained in a physical system is generally considered to specify
that system's "true" ''state''.  (A [[Scientific realism|realist]] would assert that a physical system ''always'' has a true state of some sort—whether classical or quantum—even though, in many practical situations, the system's true state may be largely unknown.)

When discussing the information that is contained in physical systems according to modern [[quantum physics]], we must distinguish between classical information and [[quantum information]].  Quantum information specifies the complete quantum state [[Vector (mathematics and physics)|vector]] (or equivalently, wavefunction) of a system, whereas classical information, roughly speaking, only picks out a definite (pure) quantum state if we are already given a prespecified set of distinguishable ([[orthogonal]]) quantum states to choose from; such a set forms a [[Basis (linear algebra)|basis]] for the [[vector space]] of all the possible pure quantum states (see [[pure state]]).  Quantum information could thus be expressed by providing (1) a choice of a basis such that the actual quantum state is equal to one of the basis vectors, together with (2) the classical information specifying which of these basis vectors is the actual one.  (However, the quantum information by itself does not include a specification of the basis, indeed, an uncountable number of different bases will include any given state vector.)

Note that the amount of classical information in a quantum system gives the maximum amount of information that can actually be measured and extracted from that quantum system for use by external classical (decoherent) systems, since only basis states are operationally distinguishable from each other.  The impossibility of differentiating between non-orthogonal states is a fundamental principle of quantum mechanics,{{Citation needed|date=August 2010}} equivalent to [[Werner Heisenberg|Heisenberg]]'s [[uncertainty principle]].{{Citation needed|date=August 2010}}  Because of its more general utility, the remainder of this article will deal primarily with classical information, although [[quantum information theory]] does also have some potential applications ([[quantum computing]], [[quantum cryptography]], [[quantum teleportation]]) that are currently being actively explored by both theorists and experimentalists.&lt;ref&gt;Michael A. Nielsen and Isaac L. Chuang, ''Quantum Computation and Quantum Information'', Cambridge University Press, 2000.&lt;/ref&gt;

==Quantifying classical physical information==
An amount of (classical) physical information may be quantified, as in [[information theory]], as follows.&lt;ref name="shannon"&gt;Claude E. Shannon and Warren Weaver, ''Mathematical Theory of Communication'', University of Illinois Press, 1963.&lt;/ref&gt;  For a system ''S'', defined abstractly in such a way that it has ''N'' distinguishable states (orthogonal quantum states) that are consistent with its description, the amount of information ''I''(''S'') contained in the system's state can be said to be log(''N'').  The logarithm is selected for this definition since it has the advantage that this measure of information content is additive when concatenating independent, unrelated subsystems; e.g., if subsystem ''A'' has ''N'' distinguishable states (''I''(''A'') = log(''N'') information content) and an independent subsystem ''B'' has ''M'' distinguishable states (''I''(''B'') = log(''M'') information content), then the concatenated system has ''NM'' distinguishable states and an information content ''I''(''AB'') = log(''NM'') = log(''N'') + log(''M'') = ''I''(''A'') + ''I''(''B'').  We expect information to be additive from our everyday associations with the meaning of the word, e.g., that two pages of a book can contain twice as much information as one page.

The base of the logarithm used in this definition is arbitrary, since it affects the result by only a multiplicative constant, which determines the unit of information that is implied.  If the log is taken base 2, the unit of information is the binary digit or bit (so named by [[John Tukey]]); if we use a natural logarithm instead, we might call the resulting unit the "[[nat (unit)|nat]]."  In magnitude, a nat is apparently identical to [[Boltzmann's constant]] ''k'' or the [[ideal gas constant]] ''R'', although these particular quantities are usually reserved to measure physical information that happens to be entropy, and that are expressed in physical units such as joules per [[kelvin]], or kilocalories per mole-kelvin.

==Physical information and entropy==
An easy way to understand the underlying unity between physical (as in thermodynamic) [[entropy]] and information-theoretic entropy is as follows: &lt;blockquote&gt;Entropy is simply that portion of the (classical) physical information contained in a system of interest (whether it is an entire physical system, or just a subsystem delineated by a set of possible messages) whose identity (as opposed to amount) is unknown (from the point of view of a particular knower).&lt;/blockquote&gt;This informal characterization corresponds to both von Neumann's formal definition of the entropy of a mixed quantum state (which is just a statistical mixture of pure states; see [[Quantum statistical mechanics#Von Neumann entropy|von Neumann entropy]]), as well as [[Claude Shannon]]'s definition of the entropy of a [[probability distribution]] over classical signal states or messages (see [[information entropy]]).&lt;ref name="shannon" /&gt;  Incidentally, the credit for Shannon's entropy formula (though not for its use in an [[information theory]] context) really belongs to [[Boltzmann]], who derived it much earlier for use in his [[H-theorem]] of statistical mechanics.&lt;ref&gt;[[Carlo Cercignani]], ''Ludwig Boltzmann: The Man Who Trusted Atoms'', Oxford University Press, 1998.&lt;/ref&gt;  (Shannon himself references Boltzmann in his monograph.&lt;ref name="shannon" /&gt;)

Furthermore, even when the state of a system ''is'' known, we can say that the information in the system is still ''effectively'' entropy if that information is effectively incompressible, that is, if there are no known or feasibly determinable correlations or redundancies between different pieces of information within the system.  Note that this definition of entropy can even be viewed as equivalent to the previous one (unknown information) if we take a meta-perspective, and say that for observer ''A'' to "know" the state of system ''B'' means simply that there is a definite correlation between the state of observer ''A'' and the state of system ''B''; this correlation could thus be used by a meta-observer (that is, whoever is discussing the overall situation regarding A's state of knowledge about B) to compress his own description of the joint system ''AB''.&lt;ref name="mpf"&gt;Michael P. Frank, "Physical Limits of Computing", ''Computing in Science and Engineering'', '''4'''(3):16-25, May/June 2002. http://www.cise.ufl.edu/research/revcomp/physlim/plpaper.html&lt;/ref&gt;

Due to this connection with [[algorithmic information theory]],&lt;ref&gt;W. H. Zurek, "Algorithmic randomness, physical entropy, measurements, and the demon of choice," in (Hey 1999), pp. 393-410, and reprinted in (Leff &amp; Rex 2003), pp. 264-281.&lt;/ref&gt; entropy can be said to be that portion of a system's information capacity which is "used up," that is, unavailable for storing new information (even if the existing information content were to be compressed).  The rest of a system's information capacity (aside from its entropy) might be called ''extropy'', and it represents the part of the system's information capacity which is potentially still available for storing newly derived information.  The fact that physical entropy is basically "used-up storage capacity" is a direct concern in the engineering of computing systems; e.g., a computer must first remove the entropy from a given physical subsystem (eventually expelling it to the environment, and emitting heat) in order for that subsystem to be used to store some newly computed information.&lt;ref name="mpf" /&gt;

==Extreme physical information==
{{Main|Extreme physical information}}
In a theory developed by [[B. Roy Frieden]],&lt;ref&gt;[[B. Roy Frieden]] and R.A. Gatenby - ''Power laws of complex systems from extreme physical information'', ''Phys. Rev. E'' '''72,''' 036101, 2005&lt;/ref&gt;&lt;ref&gt;[[B. Roy Frieden]] and B.H. Soffer - ''Information-theoretic significance of the Wigner distribution'', ''Phys. Rev. A'', to be published 2006&lt;/ref&gt;&lt;ref&gt;[[B. Roy Frieden]] and B.H. Soffer, "Lagrangians of physics and the game of Fisher-information transfer," Phys. Rev. E 52, 2274-2286, 1995.&lt;/ref&gt;&lt;ref&gt;[[B. Roy Frieden]], ''Science from Fisher Information'', Cambridge University Press, 2004.&lt;/ref&gt; "physical information" is defined as the loss of [[Fisher information]] that is incurred during the observation of a physical effect. Thus, if the effect has an intrinsic information level ''J'' but is observed at information level ''I'', the physical information is defined to be the difference ''I'' &amp;minus; ''J''. This defines an ''information [[Lagrangian mechanics|Lagrangian]]''. Frieden's ''principle of [[extreme physical information]]'' or EPI states that extremalizing ''I'' &amp;minus; ''J'' by varying the system probability amplitudes gives the correct amplitudes for most or even all physical theories. The EPI principle was recently proven.&lt;ref&gt;[[B. Roy Frieden]] and R.A. Gatenby - ''Principle of maximum Fisher information from Hardy's axioms applied to statistical systems'', ''Phys. Rev. E'' '''88,''' 042144, 2013&lt;/ref&gt;{{dubious|date=February 2015}} It follows from a system of mathematical axioms of L. Hardy defining all{{dubious|date=February 2015}} known physics.&lt;ref&gt;Hardy, L. - ''Probability theories in general and quantum theory in particular'', Studies in History and Philosophy of Modern Physics '''34,''' 381-393, 2003&lt;/ref&gt;

==See also==
* [[Digital physics]]
* [[Entropy in thermodynamics and information theory]]
* [[History of information theory]]
* [[Information entropy]]
* [[Information theory]]
* [[Logarithmic scale]]
* [[Logarithmic units]]
* [[Reversible computing]] (for relations between information and energy)
* [[Philosophy of information]]
* [[Entropy|Thermodynamic entropy]]

==References==
&lt;references /&gt;

==Further reading==
&lt;!-- Note that the following two books are also cited, Harvard-style, in the References section above, since they are accessible sources where one may find the Zurek article.  They also provide many other good articles relating to this topic.  I did these references this way because apparently you can't do nested citations (citing one reference within another) using the ref tag. -Mpfrank --&gt;

* J. G. Hey, ed., ''Feynman and Computation: Exploring the Limits of Computers'', Perseus, 1999.
* Harvey S. Leff and Andrew F. Rex, ''Maxwell's Demon 2: Entropy, Classical and Quantum Information, Computing'', Institute of Physics Publishing, 2003.

{{DEFAULTSORT:Physical Information}}
[[Category:Information theory]]
[[Category:Concepts in physics]]</text>
      <sha1>q8jtd0p036a7g2lpr4aq9zf8ljscnhn</sha1>
    </revision>
  </page>
  <page>
    <title>Primorial</title>
    <ns>0</ns>
    <id>422786</id>
    <revision>
      <id>863030409</id>
      <parentid>854779478</parentid>
      <timestamp>2018-10-08T08:06:50Z</timestamp>
      <contributor>
        <username>PMajer</username>
        <id>3113030</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11868">{{Distinguish|primordial (disambiguation)}}
[[Image:Primorial pn plot.png|thumb|300px|{{math|''p&lt;sub&gt;n&lt;/sub&gt;''#}} as a function of {{math|''n''}}, plotted logarithmically.]]
[[Image:Primorial n plot.png|thumb|300px|{{math|''n''#}} as a function of {{math|''n''}} (red dots), compared to {{math|''n''!}}. Both plots are logarithmic.]]

In [[mathematics]], and more particularly in [[number theory]], '''primorial''' is a [[Function (mathematics)|function]] from [[natural number]]s to natural numbers similar to the [[factorial]] function, but rather than successively multiplying positive integers, only [[prime number]]s are multiplied.

There are two conflicting definitions that differ in the interpretation of the argument: the first interprets the argument as an index into the sequence of prime numbers (so that the function is [[strictly increasing]]), while the second interprets the argument as a bound on the prime numbers to be multiplied (so that the function value at any [[composite number]] is the same as at its predecessor). The rest of this article uses the former interpretation.

The name "primorial", coined by [[Harvey Dubner]], draws an analogy to ''primes'' similar to the way the name "factorial" relates to ''factors''.

== Definition for primorial numbers ==

For the {{mvar|n}}th prime number {{mvar|p&lt;sub&gt;n&lt;/sub&gt;}}, the primorial {{math|''p&lt;sub&gt;n&lt;/sub&gt;''#}} is defined as the product of the first {{mvar|n}} primes:&lt;ref name="mathworld"&gt;{{Mathworld | urlname=Primorial | title=Primorial}}&lt;/ref&gt;&lt;ref name="OEIS A002110"&gt;{{OEIS|id=A002110}}&lt;/ref&gt;

:&lt;math&gt;p_n\# \equiv \prod_{k=1}^n p_k&lt;/math&gt;,

where {{mvar|p&lt;sub&gt;k&lt;/sub&gt;}} is the {{mvar|k}}th prime number. For instance, {{math|''p''&lt;sub&gt;5&lt;/sub&gt;#}} signifies the product of the first 5 primes:

:&lt;math&gt;p_5\# = 2 \times 3 \times 5 \times 7 \times 11 = 2310.&lt;/math&gt;

The first five primorials {{math|''p&lt;sub&gt;n&lt;/sub&gt;''#}} are:

:[[2 (number)|2]], [[6 (number)|6]], [[30 (number)|30]], [[210 (number)|210]], 2310 {{OEIS|id=A002110}}.

The sequence also includes {{math|''p''&lt;sub&gt;0&lt;/sub&gt;# {{=}} 1}} as [[empty product]]. Asymptotically, primorials {{math|''p&lt;sub&gt;n&lt;/sub&gt;''#}} grow according to:

:&lt;math&gt;p_n\# = e^{(1 + o(1)) n \log n},&lt;/math&gt;

where {{math|''o''( )}} is [[Little O notation]].&lt;ref name="OEIS A002110" /&gt;

== Definition for natural numbers ==
In general for a positive integer {{mvar|n}}, a primorial {{math|''n''#}} can also be defined, namely as the product of those primes ≤ {{mvar|n}}:&lt;ref name="mathworld" /&gt;&lt;ref name="OEIS A034386"&gt;{{OEIS|id=A034386}}&lt;/ref&gt;

:&lt;math&gt;n\# \equiv \prod_{i=1}^{\pi(n)} p_i = p_{\pi(n)}\# &lt;/math&gt;,

where {{math|''π''(''n'')}} is the [[prime-counting function]] {{OEIS|id=A000720}}, giving the number of primes ≤ {{mvar|n}}. This is equivalent to:

:&lt;math&gt;n\# = 
\begin{cases}
    1 &amp; \text{if }n = 0,\ 1 \\
    (n-1)\# \times n &amp; \text{if } n \text{ is prime} \\
    (n-1)\# &amp; \text{if } n \text{ is composite}.
\end{cases}&lt;/math&gt;

For example, 12# represents the product of those primes ≤ 12:

:&lt;math&gt;12\# = 2 \times 3 \times 5 \times 7 \times 11= 2310.&lt;/math&gt;

Since {{math|''π''(12) {{=}} 5}}, this can be calculated as:

:&lt;math&gt;12\# = p_{\pi(12)}\# = p_5\# = 2310.&lt;/math&gt;

Consider the first 12 values of {{math|''n''#}}:

:1, 2, 6, 6, 30, 30, 210, 210, 210, 210, 2310, 2310.

We see that for composite {{mvar|n}} every term {{math|''n''#}} simply duplicates the preceding term {{math|(''n'' − 1)#}}, as given in the definition. In the above example we have {{math|12# {{=}} ''p''&lt;sub&gt;5&lt;/sub&gt;# {{=}} 11#}} since 12 is a composite number.

Primorials are related to the first [[Chebyshev function]], written {{math|''ϑ''(''n'')}} or {{math|''θ''(''n'')}} according to:

:&lt;math&gt;\ln (n\#) = \vartheta(n).&lt;/math&gt;&lt;ref&gt;{{Mathworld | urlname=ChebyshevFunctions | title=Chebyshev Functions}}&lt;/ref&gt;

Since {{math|''ϑ''(''n'')}} asymptotically approaches {{math|''n''}} for large values of {{math|''n''}}, primorials therefore grow according to:
:&lt;math&gt;n\# = e^{(1+o(1))n}.&lt;/math&gt;

The idea of multiplying all known primes occurs in some proofs of the [[infinitude of the prime numbers]], where it is used to derive the existence of another prime.

== Applications and properties ==
Primorials play a role in the search for [[Primes in arithmetic progression|prime numbers in additive arithmetic progressions]]. For instance, 
{{val|2236133941}}&amp;nbsp;+&amp;nbsp;23# results in a prime, beginning a sequence of thirteen primes found by repeatedly adding 23#, and ending with {{val|5136341251}}. 23# is also the common difference in arithmetic progressions of fifteen and sixteen primes.

Every [[highly composite number]] is a product of primorials (e.g. [[360 (number)|360]] = {{nowrap|2 × 6 × 30}}).&lt;ref&gt;{{Cite OEIS|sequencenumber=A002182|name=Highly composite numbers}}&lt;/ref&gt;

Primorials are all [[square-free integer]]s, and each one has more distinct [[prime factor]]s than any number smaller than it. For each primorial {{mvar|n}}, the fraction {{math|{{sfrac|''φ''(''n'')|''n''}}}} is smaller than it for any lesser integer, where {{mvar|φ}} is the [[Euler totient function]].

Any [[completely multiplicative function]] is defined by its values at primorials, since it is defined by its values at primes, which can be recovered by division of adjacent values.

Base systems corresponding to primorials (such as base 30, not to be confused with the [[Mixed Radix#Primorial number system|primorial number system]]) have a lower proportion of [[repeating fraction]]s than any smaller base.

Every primorial is a [[sparsely totient number]].&lt;ref&gt;{{cite journal | last1=Masser | first1=D.W. | author1-link=David Masser | last2=Shiu | first2=P. | title=On sparsely totient numbers | journal=Pac. J. Math. | volume=121 | pages=407–426 | year=1986 | issn=0030-8730 | zbl=0538.10006 | url=http://projecteuclid.org/euclid.pjm/1102702441 | mr=819198 | doi=10.2140/pjm.1986.121.407}}&lt;/ref&gt;

The {{mvar|n}}-compositorial of a [[composite number]] {{mvar|n}} is the product of all composite numbers up to and including {{mvar|n}}.&lt;ref name="Wells 2011"&gt;{{cite book|last1=Wells|first1=David|title=Prime Numbers: The Most Mysterious Figures in Math|date=2011|publisher=John Wiley &amp; Sons|isbn=9781118045718|page=29|url=https://books.google.com/books?id=1MTcYrbTdsUC&amp;pg=PA29&amp;lpg=PA29&amp;dq=Compositorial+primorial&amp;source=bl&amp;ots=9G82GeQoX9&amp;sig=SX44vw-N4d3M1s9bVM-W4TyItXI&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwiYtIKE5cXLAhUBU2MKHZLfARkQ6AEIRjAG#v=onepage&amp;q=Compositorial%20primorial&amp;f=false|accessdate=16 March 2016}}&lt;/ref&gt; The {{mvar|n}}-compositorial is equal to the {{mvar|n}}-[[factorial]] divided by the primorial {{math|''n''#}}. The compositorials are
:[[1 (number)|1]], [[4 (number)|4]], [[24 (number)|24]], [[192 (number)|192]], 1728, {{val|17280}}, {{val|207360}}, {{val|2903040}}, {{val|43545600}}, {{val|696729600}}, ...&lt;ref&gt;{{Cite OEIS|sequencenumber=A036691|name=Compositorial numbers: product of first n composite numbers.}}&lt;/ref&gt;

== Appearance ==

The [[Riemann zeta function]] at positive integers greater than one can be expressed&lt;ref name=mezo/&gt; by using the primorial function and [[Jordan's totient function]] {{math|''J&lt;sub&gt;k&lt;/sub&gt;''(''n'')}}:
: &lt;math&gt; \zeta(k)=\frac{2^k}{2^k-1}+\sum_{r=2}^\infty\frac{(p_{r-1}\#)^k}{J_k(p_r\#)},\quad k=2,3,\dots &lt;/math&gt;

== Table of primorials ==

{| class="wikitable" style="text-align:right"
|-
! rowspan="2" | {{mvar|n}}
! rowspan="2" | {{math|''n''#}}
! rowspan="2" | {{mvar|p&lt;sub&gt;n&lt;/sub&gt;}}
! rowspan="2" | {{math|''p&lt;sub&gt;n&lt;/sub&gt;''#}}&lt;ref&gt;http://planetmath.org/TableOfTheFirst100Primorials&lt;/ref&gt;
! colspan="2" | [[Primorial prime]]?
|-
! ''p&lt;sub&gt;n&lt;/sub&gt;''# + 1&lt;ref&gt;{{Cite OEIS|sequencenumber=A014545|name=Primorial plus 1 prime indices}}&lt;/ref&gt;
! ''p&lt;sub&gt;n&lt;/sub&gt;''# − 1&lt;ref&gt;{{Cite OEIS|sequencenumber=A057704|name=Primorial - 1 prime indices}}&lt;/ref&gt;
|-
| 0
| 1
| {{n/a}}
| [[Empty product|1]]
| Yes  
| No
|-
| 1
| 1
| 2
| 2
| Yes
| No
|-
| 2
| 2
| 3
| 6
| Yes
| Yes
|-
| 3
| 6
| 5
| 30
| Yes
| Yes
|-
| 4
| 6
| 7
| 210
| Yes
| No
|-
| 5
| 30
| 11
| {{val|2310|fmt=gaps}}
| Yes
| Yes
|-
| 6
| 30
| 13
| {{val|30030}}
| No
| Yes
|-
| 7
| 210
| 17
| {{val|510510}}
| No
| No
|-
| 8
| 210
| 19
| {{val|9699690}}
| No
| No
|-
| 9
| 210
| 23
| {{val|223092870}}
| No
| No
|-
| 10
| 210
| 29
| {{val|6469693230}}
| No
| No
|-
| 11
| {{val|2310|fmt=gaps}}
| 31
| {{val|200560490130}}
| Yes
| No
|-
| 12
| {{val|2310|fmt=gaps}}
| 37
| {{val|7420738134810}}
| No
| No
|-
| 13
| {{val|30030}}
| 41
| {{val|304250263527210}}
| No
| Yes
|-
| 14
| {{val|30030}}
| 43
| {{val|13082761331670030}}
| No
| No
|-
| 15
| {{val|30030}}
| 47
| {{val|614889782588491410}}
| No
| No
|-
| 16
| {{val|30030}}
| 53
| {{val|32589158477190044730}}
| No
| No
|-
| 17
| {{val|510510}}
| 59
| {{val|1922760350154212639070}}
| No
| No
|-
| 18
| {{val|510510}}
| 61
| {{val|117288381359406970983270}}
| No
| No
|-
| 19
| {{val|9699690}}
| 67
| {{val|7858321551080267055879090}}
| No
| No
|-
| 20
| {{val|9699690}}
| 71
| {{val|557940830126698960967415390}}
| No
| No
|-
| 21
| {{val|9699690}}
| 73
| {{val|40729680599249024150621323470}}
| No
| No
|-
| 22
| {{val|9699690}}
| 79
| {{val|3217644767340672907899084554130}}
| No
| No
|-
| 23
| {{val|223092870}}
| 83
| {{val|267064515689275851355624017992790}}
| No
| No
|-
| 24
| {{val|223092870}}
| 89
| {{val|23768741896345550770650537601358310}}
| No
| No
|-
| 25
| {{val|223092870}}
| 97
| {{val|2305567963945518424753102147331756070}}
| No
| No
|-
| 26
| {{val|223092870}}
| 101
| {{val|232862364358497360900063316880507363070}}
| No
| No
|-
| 27
| {{val|223092870}}
| 103
| {{val|23984823528925228172706521638692258396210}}
| No
| No
|-
| 28
| {{val|223092870}}
| 107
| {{val|2566376117594999414479597815340071648394470}}
| No
| No
|-
| 29
| {{val|6469693230}}
| 109
| {{val|279734996817854936178276161872067809674997230}}
| No
| No
|-
| 30
| {{val|6469693230}}
| 113
| {{val|31610054640417607788145206291543662493274686990}}
| No
| No
|-
| 31
| {{val|200560490130}}
| 127
| {{val|4014476939333036189094441199026045136645885247730}}
| No
| No
|-
| 32
| {{val|200560490130}}
| 131
| {{val|525896479052627740771371797072411912900610967452630}}
| No
| No
|-
| 33
| {{val|200560490130}}
| 137
| {{val|72047817630210000485677936198920432067383702541010310}}
| No
| No
|-
| 34
| {{val|200560490130}}
| 139
| {{val|10014646650599190067509233131649940057366334653200433090}}
| No
| No
|-
| 35
| {{val|200560490130}}
| 149
| {{val|1492182350939279320058875736615841068547583863326864530410}}
| No
| No
|-
| 36
| {{val|200560490130}}
| 151
| {{val|225319534991831177328890236228992001350685163362356544091910}}
| No
| No
|-
| 37
| {{val|7420738134810}}
| 157
| {{val|35375166993717494840635767087951744212057570647889977422429870}}
| No
| No
|-
| 38
| {{val|7420738134810}}
| 163
| {{val|5766152219975951659023630035336134306565384015606066319856068810}}
| No
| No
|-
| 39
| {{val|7420738134810}}
| 167
| {{val|962947420735983927056946215901134429196419130606213075415963491270}}
| No
| No
|-
| 40
| {{val|7420738134810}}
| 173
| {{val|166589903787325219380851695350896256250980509594874862046961683989710}}
| No
| No
|}

== See also ==
* [[Bonse's inequality]]
* [[Chebyshev function]]
* [[Mixed Radix#Primorial number system|Primorial number system]]
* [[Primorial prime]]

== Notes ==
{{reflist|refs=
&lt;ref name=mezo&gt;
{{Cite journal 
| last1 = Mező | first1 = István
| title = The Primorial and the Riemann zeta function 
| journal = The American Mathematical Monthly 
| volume = 120
| issue = 4 
| pages = 321 
| year = 2013 
}}&lt;/ref&gt;
}}

== References ==
* {{cite journal | last1 = Dubner | first1 = Harvey | year = 1987 | title = Factorial and primorial primes | url = | journal = [[Journal of Recreational Mathematics|J. Recr. Math.]] | volume = 19 | issue = | pages = 197–203 }}

[[Category:Integer sequences]]
[[Category:Factorial and binomial topics]]
[[Category:Prime numbers]]</text>
      <sha1>iswcokl3bvr9nz56qsd1hviuit820qx</sha1>
    </revision>
  </page>
  <page>
    <title>Resource selection function</title>
    <ns>0</ns>
    <id>54252064</id>
    <revision>
      <id>858572703</id>
      <parentid>853013522</parentid>
      <timestamp>2018-09-08T04:16:21Z</timestamp>
      <contributor>
        <username>Citation bot</username>
        <id>7903804</id>
      </contributor>
      <minor/>
      <comment>Alter: url. Add: jstor, citeseerx. Removed parameters. You can [[WP:UCB|use this bot]] yourself. [[WP:DBUG|Report bugs here]]. | [[WP:UCB|User-activated]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3812">'''Resource selection functions''' (RSFs) are a class of functions that are used in [[spatial ecology]] to assess which habitat characteristics are important to a specific population or species of animal, by assessing the a probability of that animal using a certain resource proportional to the availability of that resource in the environment.&lt;ref&gt;{{Cite journal|last=Boyce|first=Mark S|last2=Vernier|first2=Pierre R|last3=Nielsen|first3=Scott E|last4=Schmiegelow|first4=Fiona K. A|date=2002-11-30|title=Evaluating resource selection functions|url=http://www.sciencedirect.com/science/article/pii/S0304380002002004|journal=Ecological Modelling|volume=157|issue=2–3|pages=281–300|doi=10.1016/S0304-3800(02)00200-4|citeseerx=10.1.1.471.8823}}&lt;/ref&gt;

== Modeling ==
Resource Selection Functions require two types of data: location information for the wildlife in question, and data on the resources available across the study area. Resources can include a broad range of environmental and geographical variables, including categorical variables such as land cover type, or continuous variables such as average rainfall over a given time period. A variety of methods are used for modeling RSFs, with [[logistic regression]] being commonly used.&lt;ref&gt;{{Cite book|url=https://books.google.com/?id=rBXnBwAAQBAJ&amp;pg=PR9#v=onepage&amp;q&amp;f=false|title=Resource Selection by Animals: Statistical Design and Analysis for Field Studies|last=Manly|first=B. F.|last2=McDonald|first2=L.|last3=Thomas|first3=Dana|last4=McDonald|first4=Trent L.|last5=Erickson|first5=Wallace P.|date=2007-05-08|publisher=Springer Science &amp; Business Media|isbn=9780306481512|language=en}}&lt;/ref&gt;

RSFs can be fit to data where animal presence is known, but absence is not, such as for species where several individuals within a study area are fitted with a GPS collar, but some individuals may be present without collars.
When this is the case, buffers of various distances are generated around known presence points, with a number of available points generated within each buffer, which represent areas where the animal could have been, but it is unknown whether they actually were.&lt;ref&gt;{{Cite journal|last=Johnson|first=Chris J.|last2=Nielsen|first2=Scott E.|last3=Merrill|first3=Evelyn H.|last4=Mcdonald|first4=Trent L.|last5=Boyce|first5=Mark S.|date=2006-04-01|title=Resource Selection Functions Based on Use–Availability Data: Theoretical Motivation and Evaluation Methods|journal=Journal of Wildlife Management|volume=70|issue=2|pages=347–357|doi=10.2193/0022-541X(2006)70[347:RSFBOU]2.0.CO;2|issn=0022-541X}}&lt;/ref&gt;  These models can be fit using binomial generalized linear models or binomial generalized linear mixed models, with the resources, or environmental and geographic data, as explanatory variables.

== Scale ==
Resource selection functions can be modeled at a variety of spatial scales, depending on the species and the scientific question being studied. (insert one more sentence on scale)

Most RSFs address one of the following scales, which were defined by Douglas Johnson in 1980 and are still used today:&lt;ref&gt;{{Cite journal|last=Johnson|first=Douglas H.|date=1980-02-01|title=The Comparison of Usage and Availability Measurements for Evaluating Resource Preference|journal=Ecology|language=en|volume=61|issue=1|pages=65–71|doi=10.2307/1937156|issn=1939-9170|jstor=1937156}}&lt;/ref&gt;
* '''First order selection:''' The entire range of a species
* '''Second order selection:''' The home range of an individual or group of animals
* '''Third order selection:''' Resource or habitat usage within an individual's or group's home range 
* '''Fourth order selection:''' The procurement of specific resources, such as food, at specific sites

== References ==
{{reflist}}

[[Category:Ecology]]
[[Category:Mathematical modeling]]</text>
      <sha1>ac05xinkx8wxwm60pfgbnpngz9a7h04</sha1>
    </revision>
  </page>
  <page>
    <title>SWAG (silver, wine, art and gold)</title>
    <ns>0</ns>
    <id>39759555</id>
    <revision>
      <id>834010419</id>
      <parentid>788528303</parentid>
      <timestamp>2018-04-03T13:25:22Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v485)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2397">'''SWAG''' is an [[Asset allocation#Asset classes|asset class]] comprising silver, wine, art and gold, identified by economist [[Joe Roseman (economist)|Joe Roseman]] in his 2011 ''[[Investment Week]]'' article, "SWAG: The Industry's Latest Acronym". He describes these as transportable and easy to store physical assets with no income stream (and so no exposure to income tax) and no incumbent debt, whose performance appears unrelated to the performance of equity markets; and he notes that none of these factors would be affected by sovereign default.&lt;ref name=Roseman2011&gt;{{cite journal | url=http://www.investmentweek.co.uk/investment-week/feature/2111592/swag-industrys-acronym | date=23 September 2011 | journal = Investment Week | title = SWAG: The Industry's Latest Acronym }}&lt;/ref&gt;

Because of their scarcity, desirability, durability and stability and the independence of their price from stock market prices they can add genuine diversity to an asset portfolio.&lt;ref&gt;{{cite news |last= Kocialkowska |first= Kamila |date= 16 November 2012 &lt;!-- 10:51 --&gt; |title= Investment Art: A Beginner's Guide|url= http://www.newstatesman.com/cultural-capital/2012/11/investment-art-beginners-guide|newspaper= New Statesman|location= |publisher= |accessdate= 8 December 2013}}&lt;/ref&gt;

==References==
{{reflist}}

==Further reading==
*{{cite book|author=Joe Roseman|title=SWAG: Alternative Investments for the Coming Decade|url=https://books.google.com/books?id=r8YFuwAACAAJ|accessdate=24 June 2013|date=1 May 2012|publisher=Grosvenor House Publishing Limited|isbn=978-1-78148-518-7}}

==External links==
&lt;!-- I've applied for whitelisting the first item http://moneyweek.com/alternative-investments-silver-wine-art-gold-22100/--&gt;
*Review: Partridge M (2012) {{plain link|name="Which 'Swag' Assets Should You Buy?"|url=http://www.moneyweek.com/alternative-investments-silver-wine-art-gold-22100/}} ''Money Week''
*Review: Saft J (2012) {{plain link|name="Adding SWAG to Your Portfolio" |url=https://www.reuters.com/article/2012/08/09/us-wealth-swag-saft-idUSBRE8781FE20120809}} ''Reuters''
*Review: Evans R (2012) {{plain link|url=https://www.telegraph.co.uk/finance/personalfinance/investing/gold/9497714/Is-it-time-for-investors-to-bag-some-swag.html|name="Is it Time for Investors to Bag Some Swag?"}} ''The Telegraph''


[[Category:Commodities used as an investment]]
[[Category:Actuarial science]]</text>
      <sha1>q6x03yrk8lv0sijrrw98r25i5mz4m5x</sha1>
    </revision>
  </page>
  <page>
    <title>Sartaj Sahni</title>
    <ns>0</ns>
    <id>855345</id>
    <revision>
      <id>850545841</id>
      <parentid>839997147</parentid>
      <timestamp>2018-07-16T15:06:53Z</timestamp>
      <contributor>
        <username>GreenC bot</username>
        <id>27823944</id>
      </contributor>
      <comment>Rescued 1 archive link; reformat 1 link. [[User:GreenC/WaybackMedic_2.1|Wayback Medic 2.1]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7161">{{Infobox scientist
|name              = Sartaj Sahni
|image             = Sartaj Sahni.jpg
|caption           = Prof. Sartaj Sahni in 2015.
|birth_date        = {{birth date and age|1949|07|22}}
|birth_place       = [[Pune]], India
|nationality       = United States
|alma_mater        = [[Indian Institute of Technology]], [[Cornell University]]
|doctoral_advisor  = [[Ellis Horowitz]]
|doctoral_students = [[Teofilo F. Gonzalez]]
|known_for         = [[Data structure]]s, [[Algorithm]]s
|field             = [[Computer science]]
|work_institutions = [[University of Florida]]
|prizes            = [[IEEE Computer Society]] [[Taylor L. Booth Education Award]], 1997 &lt;br /&gt; IEEE Computer Society W. Wallace McDowell Award, 2003 &lt;br /&gt; [[Association for Computing Machinery|ACM]] Karl Karlstrom Outstanding Educator Award, 2003
}}

Professor '''Sartaj Kumar Sahni''' (born July 22, 1949, in [[Poona]], India) is a [[computer scientist]] based in the United States, and is one of the pioneers in the field of [[data structure]]s. He is a distinguished professor in the Department of Computer and Information Science and Engineering at the [[University of Florida]].&lt;ref&gt;[http://www.cise.ufl.edu/people/faculty/sahni/ Faculty profile], CISE, U. of Florida, accessed 2011-10-10.&lt;/ref&gt;

==Education==
Sahni received his [[BTech]] degree in [[electrical engineering]] from the [[Indian Institute of Technology Kanpur]].&lt;ref name="iitk"/&gt; Following this, he undertook his graduate studies at [[Cornell University]] in the USA, earning a [[PhD degree]] in 1973, under the supervision of [[Ellis Horowitz]].&lt;ref&gt;{{mathgenealogy|name=Sartaj Kumar Sahni|id=46660}}.&lt;/ref&gt;

==Research and publications==
Sahni has published over 280 research papers and written 15 textbooks.&lt;ref&gt;{{DBLP|name=Sartaj Sahni}}&lt;/ref&gt; His research publications are on the design and analysis of efficient [[algorithms]], [[data structures]], [[parallel computing]], interconnection networks, design automation, and medical algorithms.

With his advisor Ellis Horowitz, Sahni wrote two widely used textbooks, ''Fundamentals of Computer Algorithms'' and ''Fundamentals of Data Structures''. He has also written highly cited research papers on the [[NP-complete]]ness of [[approximation algorithm|approximately solving]] certain optimization problems,&lt;ref&gt;{{citation
 | last1 = Sahni | first1 = Sartaj
 | last2 = Gonzalez | first2 = Teofilo | author2-link = Teofilo F. Gonzalez
 | issue = 3
 | journal = Journal of the Association for Computing Machinery
 | mr = 0408313
 | pages = 555–565
 | title = P-complete approximation problems
 | volume = 23
 | year = 1976
 | doi=10.1145/321958.321975}}.&lt;/ref&gt; on [[open shop scheduling]],&lt;ref&gt;{{citation
 | last1 = Gonzalez | first1 = Teofilo | author1-link = Teofilo F. Gonzalez
 | last2 = Sahni | first2 = Sartaj
 | issue = 4
 | journal = Journal of the Association for Computing Machinery
 | mr = 0429089
 | pages = 665–679
 | title = Open shop scheduling to minimize finish time
 | volume = 23
 | year = 1976
 | doi=10.1145/321978.321985}}.&lt;/ref&gt; on [[parallel algorithm]]s for [[matrix multiplication]] and their application in [[graph theory]],&lt;ref&gt;{{citation
 | last1 = Dekel | first1 = Eliezer
 | last2 = Nassimi | first2 = David
 | last3 = Sahni | first3 = Sartaj
 | doi = 10.1137/0210049
 | issue = 4
 | journal = SIAM Journal on Computing
 | mr = 635424
 | pages = 657–675
 | title = Parallel matrix and graph algorithms
 | volume = 10
 | year = 1981}}.&lt;/ref&gt; and on improved [[exponential time]] exact algorithms for the [[subset sum problem]],&lt;ref&gt;{{citation
 | last1 = Horowitz | first1 = Ellis
 | last2 = Sahni | first2 = Sartaj
 | doi = 10.1145/321812.321823
 | journal = Journal of the Association for Computing Machinery
 | mr = 0354006
 | pages = 277–292
 | title = Computing partitions with applications to the knapsack problem
 | volume = 21
 | year = 1969}}.&lt;/ref&gt; among his many other research results.

==Awards and honors==
In 1997, Sahni was awarded the [[IEEE Computer Society]]'s [[Taylor L. Booth Education Award]]&lt;ref&gt;[http://www.computer.org/portal/web/awards/taylorbooth Past recipients for Taylor L. Booth Education Award], IEEE Computer Society, accessed 2011-10-10.&lt;/ref&gt; and in 2003 he was awarded the IEEE Computer Society [[McDowell Award]].&lt;ref&gt;[http://awards.computer.org/ana/award/viewPastRecipients.action?id=23 Past recipients for W. Wallace McDowell Award], IEEE Computer Society, accessed 2011-10-10.&lt;/ref&gt; Sahni was also awarded the 2003 Karl V. Karlstrom Outstanding Educator Award of the [[Association for Computing Machinery]].&lt;ref&gt;[http://awards.acm.org/homepage.cfm?awd=142 Karl V. Karlstrom Outstanding Educator Award] {{webarchive|url=https://web.archive.org/web/20120402212417/http://awards.acm.org/homepage.cfm?awd=142 |date=2012-04-02 }}, ACM, accessed 2011-10-10.&lt;/ref&gt;

Prof. Sahni is a member of the [[European Academy of Sciences]].&lt;ref&gt;[http://www.eurasc.org/members/members.asp?cognome=s List of the members of the European Academy of Sciences], accessed 2011-10-10.&lt;/ref&gt; He was elected as a [[Fellow]] of the [[Institute of Electrical and Electronics Engineers]] in 1988,&lt;ref&gt;[http://www.ieee.org/membership_services/membership/fellows/sfellows.html Alphabetical Listing of Fellows] {{webarchive|url=https://web.archive.org/web/20110820074035/http://www.ieee.org/membership_services/membership/fellows/sfellows.html |date=2011-08-20 }}, IEEE, accessed 2011-10-10.&lt;/ref&gt; and of the [[Association for Computing Machinery]] in 1996;&lt;ref&gt;[http://fellows.acm.org/homepage.cfm?alpha=S&amp;srt=alpha ACM Fellows listing], accessed 2011-10-10.&lt;/ref&gt; he is also a fellow of the [[American Association for the Advancement of Science]].&lt;ref&gt;[http://www.aaas.org/aboutaaas/fellows/ AAAS Fellows], accessed 2011-10-10.&lt;/ref&gt; He is a Distinguished Alumnus of the Indian Institute of Technology, Kanpur.&lt;ref name="iitk"&gt;[http://www.iitk.ac.in/infocell/Archive/dirjuly3/alumnus_awards.html Distinguished Alumnus Awards-2000], IIT Kanpur, accessed 2011-10-10.&lt;/ref&gt;

Sahni was given the Honorary Professor Award of [[Asia University (Taiwan)]] in 2009.&lt;ref&gt;[http://www.cise.ufl.edu/news/NA00141/index.shtml Distinguished Professor and Chair Sartaj Sahni receives the Honorary Professor Award from Asia University, Taiwan], University of Florida, CISE, June 9, 2009. Sahni Accessed 2011-10-10.&lt;/ref&gt;

==References==
{{Reflist|colwidth=30em}}

==External links==
* [http://www.cise.ufl.edu/~sahni/ Sartaj K. Sahni home page]
* {{GoogleScholar|id=gMQWxE0AAAAJ}}

{{Authority control}}

{{DEFAULTSORT:Sahni, Sartaj}}
[[Category:1949 births]]
[[Category:Living people]]
[[Category:Indian Institute of Technology Kanpur alumni]]
[[Category:Cornell University alumni]]
[[Category:Indian computer scientists]]
[[Category:American computer scientists]]
[[Category:Theoretical computer scientists]]
[[Category:Computer science writers]]
[[Category:Indian emigrants to the United States]]
[[Category:Fellows of the American Association for the Advancement of Science]]
[[Category:Fellows of the Association for Computing Machinery]]
[[Category:Fellow Members of the IEEE]]
[[Category:University of Florida faculty]]</text>
      <sha1>ddztnjy655aba46kevkekupyakcoysq</sha1>
    </revision>
  </page>
  <page>
    <title>Schreier coset graph</title>
    <ns>0</ns>
    <id>24673785</id>
    <revision>
      <id>855110262</id>
      <parentid>843934094</parentid>
      <timestamp>2018-08-16T01:02:12Z</timestamp>
      <contributor>
        <username>Davyker</username>
        <id>27243682</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3817">In the area of [[mathematics]] called [[combinatorial group theory]], the '''Schreier coset graph''' is a [[Graph (discrete mathematics)|graph]] associated with a [[group (mathematics)|group]] ''G'', a [[Generating set of a group|generating set]] { ''x''&lt;sub&gt;''i''&lt;/sub&gt; : ''i'' in ''I'' }, and a [[subgroup]] ''H'' ≤ ''G''.  

The graph is named after [[Otto Schreier]], who used the term “Nebengruppenbild”.&lt;ref&gt;{{cite journal|url= https://link.springer.com/article/10.1007/BF02952517#citeas |first1=Otto |last1=Schreier |title=Die Untergruppen der freien Gruppen |journal=Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg |volume=5 |issue=1 |date=December 1927 |pages=161–183 |doi=10.1007/BF02952517|accessdate=2018-06-01}}&lt;/ref&gt; An equivalent definition [[Todd–Coxeter algorithm|was made]] in an early paper of Todd and Coxeter.&lt;ref&gt;{{cite journal|url= https://www.cambridge.org/core/services/aop-cambridge-core/content/view/S0013091500008221 |first1=J.A |last1=Todd |first2=H.S.M. |last2=Coxeter |title=A practical method for enumerating cosets of a finite abstract group |journal=Proceedings of the Edinburgh Mathematical Society |volume=5 |issue=1 |date=October 1936 |pages=26-34 |doi=10.1017/S0013091500008221 |accessdate=2018-03-05}}&lt;/ref&gt; 



==Description==
The [[vertex (graph theory)|vertices of the graph]] are the right [[coset]]s ''Hg'' = { ''hg'' : ''h'' in ''H'' } for ''g'' in ''G''.  

The [[edge (graph theory)|edges of the graph]] are of the form (''Hg'',''Hgx&lt;sub&gt;i&lt;/sub&gt;'').  

The [[Cayley graph]] of the group ''G'' with { ''x''&lt;sub&gt;''i''&lt;/sub&gt; : ''i'' in ''I'' } is the Schreier coset graph for ''H'' = { 1&lt;sub&gt;G&lt;/sub&gt; },{{harv|Gross|Tucker|1987|p=73}}.  

A [[spanning tree]] of a Schreier coset graph corresponds to a Schreier transversal, as in [[Schreier's subgroup lemma]], {{harv|Conder|2003}}.

The book "Categories and Groupoids" listed below relates this to the theory of covering morphisms of [[groupoid]]s.  A subgroup ''H'' of a group ''G'' determines a covering morphism of groupoids &lt;math&gt; p: K \rightarrow G &lt;/math&gt;  and if ''X'' is a generating set for ''G'' then its inverse image under ''p'' is the Schreier graph of ''(G,X)''.

==Applications==
The graph is useful to understand [[coset enumeration]] and the [[Todd–Coxeter algorithm]].  

Coset graphs can be used to form large [[permutation representation]]s of groups and were used by [[Graham Higman]] to show that the [[alternating group]]s of large enough degree are [[Hurwitz group]]s, {{harv|Conder|2003}}.

Every [[vertex-transitive graph]] is a coset graph.

== References ==
{{reflist}}

* {{Citation | last1=Magnus | first1=W. | last2=Karrass | first2=A. |last3=Solitar | first3=D. | title=Combinatorial Group Theory | publisher=Dover | year=1976 }}
* {{Citation | last1=Conder | first1=Marston |authorlink=Marston Conder| title=Groups St. Andrews 2001 in Oxford. Vol. I | publisher=[[Cambridge University Press]] | series=London Math. Soc. Lecture Note Ser. |mr=2051519 | year=2003 | volume=304 | chapter=Group actions on graphs, maps and surfaces with maximum symmetry | pages=63–91}}
* {{Citation | last1=Gross | first1=Jonathan L. | last2=Tucker | first2=Thomas W. | title=Topological graph theory | publisher=[[John Wiley &amp; Sons]] | location=New York | series=Wiley-Interscience Series in Discrete Mathematics and Optimization | isbn=978-0-471-04926-5 |mr=898434 | year=1987}}
* [https://arxiv.org/abs/0911.2915 Schreier graphs of the Basilica group Authors: Daniele D'Angeli, Alfredo Donno, Michel Matter, Tatiana Nagnibeda]
* Philip J. Higgins, Categories and Groupoids, van Nostrand, New York, Lecture Notes, 1971, [http://www.tac.mta.ca/tac/reprints/articles/7/tr7abs.html Republished as TAC Reprint, 2005] 


{{algebra-stub}}

[[Category:Combinatorial group theory]]</text>
      <sha1>mylwq5cs9y84bjukpl36v2rcj147mth</sha1>
    </revision>
  </page>
  <page>
    <title>Shmuel Agmon</title>
    <ns>0</ns>
    <id>32542224</id>
    <revision>
      <id>842798710</id>
      <parentid>842725840</parentid>
      <timestamp>2018-05-24T18:45:57Z</timestamp>
      <contributor>
        <username>BrownHairedGirl</username>
        <id>754619</id>
      </contributor>
      <minor/>
      <comment>remove [[:Category:Jewish mathematicians]]. [[WP:G4]] per [[:WP:Categories for discussion/Log/2007 May 14#Category:Jewish_mathematicians]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4610">[[File:Shmuel Agmon.jpeg|thumb|right|200px|Shmuel Agmon in [[Nice]] (1970)]]

'''Shmuel Agmon''' ({{lang-he|שמואל אגמון}}) (born 2 February 1922 in [[Tel-Aviv]]) is an [[Israel]]i [[mathematician]]. He is known for his work in [[mathematical analysis|analysis]] and [[partial differential equations]].

==Work==
Agmon's contributions to partial differential equations include [[Agmon's method]] for proving exponential decay of eigenfunctions for elliptic operators.&lt;ref&gt;{{cite book|last=Agmon|first=Sh.|title=Lectures on exponential decay of solutions of second-order elliptic equations: bounds on eigenfunctions of ''N''-body Schrödinger operators|series=Mathematical Notes|volume=29|publisher=Princeton University Press|location=Princeton, NJ|isbn=0-691-08318-5}}&lt;/ref&gt;

==Awards==
Agmon was awarded the 1991 [[Israel Prize]] in mathematics.&lt;ref&gt;[http://www.education.gov.il/pras-israel/list4.htm List of Israel Prize recipients (Hebrew)] {{webarchive|url=https://web.archive.org/web/20090604190810/http://www.education.gov.il/pras-israel/list4.htm |date=2009-06-04 }}&lt;/ref&gt; He received the 2007 [[EMET Prize]] "''for paving new paths in the study of partial-elliptical differential equations and their problematic language and for advancing the knowledge in the field, as well as his essential contribution to the development of the Spectral Theory and the Distribution Theory of Schrödinger Operators.''"&lt;ref name=emet&gt;[http://www.emetprize.org/english/Product.aspx?Product=22&amp;Year=2007 Emet prize for science art and culture] {{webarchive|url=https://archive.is/20130414174246/http://www.emetprize.org/english/Product.aspx?Product=22&amp;Year=2007 |date=2013-04-14 }} retrieved 17/09/2011&lt;/ref&gt; He has also received the Weizmann Prize and the Rothschild Prize.&lt;ref name=emet/&gt; In 2012 he became a fellow of the [[American Mathematical Society]].&lt;ref&gt;[http://www.ams.org/profession/fellows-list List of Fellows of the American Mathematical Society], retrieved 2012-11-03.&lt;/ref&gt;

==Selected works==
*{{cite journal|title=Functions of exponential type in an angle and singularities of Taylor series|journal=Trans. Amer. Math. Soc.|year=1951|volume=70|pages=492–508|mr=0041222|doi=10.1090/s0002-9947-1951-0041222-5}}
*with [[Lipman Bers]]: {{cite journal|title=The expansion theorem for pseudo-analytic functions|journal=Proc. Amer. Math. Soc.|year=1952|volume=3|pages=757–764|mr=0057349|doi=10.1090/s0002-9939-1952-0057349-4}}
*{{cite journal|title=Complex variable Tauberians|journal=Trans. Amer. Math. Soc.|year=1953|volume=74|pages=444–481|mr=0054079|doi=10.1090/s0002-9947-1953-0054079-5}}
*{{cite journal|title=Maximum theorems for solutions of higher order elliptic equations|journal=Bull. Amer. Math. Soc.|year=1960|volume=66|issue=2|pages=77–80|mr=0124618|doi=10.1090/s0002-9904-1960-10402-8}}
*{{cite book|title=Lectures on elliptic boundary value problems|publisher=Van Nostrand|year=1965|nopp=yes|pages=iii+291 p.}}; {{cite book|title=2nd edition|publisher=AMS Chelsea Pub.|year=2010|isbn=978-0-8218-4910-1}}
*{{cite book|title=Unicité et convexité dans les problèmes différentiels|publisher=Presses de l'Université de Montréal|year=1966|nopp=yes|pages=156 p.}}
*{{cite book|title=Spectral properties of Schrödinger operators and scattering theory|publisher=Scuola normale superiore di Pisa|year=1975}}
*{{cite book|title=Lectures on exponential decay of solutions of second-order elliptic equations: bounds on eigenfunctions of N-body Schrödinger operators|publisher=Princeton University Press|year=1982|isbn=0-691-08318-5}}&lt;ref&gt;{{cite journal|author=Deift, Percy|authorlink=Percy Deift|title=Review: ''Lectures on exponential decay of solutions of second-order elliptic equations: bounds on eigenfunctions of N-body Schrödinger operators'', by Shmuel Agmon|journal=Bull. Amer. Math. Soc. (N.S.)|year=1985|volume=12|issue=1|pages=165–169|url=http://www.ams.org/journals/bull/1985-12-01/S0273-0979-1985-15332-7/S0273-0979-1985-15332-7.pdf|doi=10.1090/s0273-0979-1985-15332-7}}&lt;/ref&gt;

==See also==
*[[Agmon's inequality]]

==References==
{{Reflist}}

==External links==
*{{MathGenealogy|id=9347}}

{{authority control}}

{{DEFAULTSORT:Agmon, Shmuel}}
[[Category:Israeli mathematicians]]
[[Category:Mathematical analysts]]
[[Category:Israel Prize in mathematics recipients]]
[[Category:Members of the Israel Academy of Sciences and Humanities]]
[[Category:Fellows of the American Mathematical Society]]
[[Category:PDE theorists]]
[[Category:1922 births]]
[[Category:Living people]]
[[Category:21st-century mathematicians]]
[[Category:20th-century Israeli mathematicians]]</text>
      <sha1>3ivadi7zkznhwslzx6l8yh5qgegc96p</sha1>
    </revision>
  </page>
  <page>
    <title>Short-time Fourier transform</title>
    <ns>0</ns>
    <id>436912</id>
    <revision>
      <id>858573360</id>
      <parentid>851913716</parentid>
      <timestamp>2018-09-08T04:23:52Z</timestamp>
      <contributor>
        <username>Enterprisey</username>
        <id>16663390</id>
      </contributor>
      <comment>Added {{[[:Template:merge from|merge from]]}} tag to article ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="18996">{{merge from|Rectangular mask short-time Fourier transform|discuss=Talk:Short-time Fourier transform#Proposed merge with Rectangular mask short-time Fourier transform|date=September 2018}}
The '''short-time Fourier transform''' ('''STFT'''), is a [[List of Fourier-related transforms|Fourier-related transform]] used to determine the sinusoidal frequency and phase content of local sections of a signal as it changes over time.&lt;ref&gt;{{cite journal |author1=Sejdić E. |author2=Djurović I. |author3=Jiang J. | year = 2009 | title = Time-frequency feature representation using energy concentration: An overview of recent advances | url = | journal = Digital Signal Processing | volume = 19 | issue = 1| pages = 153–183 | doi=10.1016/j.dsp.2007.12.004}}&lt;/ref&gt;  In practice, the procedure for computing STFTs is to divide a longer time signal into shorter segments of equal length and then compute the Fourier transform separately on each shorter segment.  This reveals the Fourier spectrum on each shorter segment.  One then usually plots the changing spectra as a function of time.

[[File:Impact STFT.png|thumb|400 px| Example of short time Fourier transforms used to determine time of impact from audio signal.]]

== STFT ==

=== Continuous-time STFT ===
Simply, in the continuous-time case, the function to be transformed is multiplied by a [[window function]] which is nonzero for only a short period of time.  The [[Fourier transform]] (a one-dimensional function) of the resulting signal is taken as the window is slid along the time axis, resulting in a two-dimensional representation of the signal.  Mathematically, this is written as:

:&lt;math&gt;\mathbf{STFT}\{x(t)\}(\tau,\omega) \equiv X(\tau, \omega) = \int_{-\infty}^{\infty} x(t) w(t-\tau) e^{-j \omega t} \, dt &lt;/math&gt;

where ''w''(''t'') is the [[window function]], commonly a [[Window function#Hann window|Hann window]] or [[Window function#Gaussian window|Gaussian window]] centered around zero, and ''x''(''t'') is the signal to be transformed{{cn|date=June 2016}}  (note the difference between ''w'' and ω). ''X''(τ,ω) is essentially the Fourier Transform of ''x''(''t'')''w''(''t''-τ), a [[complex function]] representing the phase and magnitude of the signal over time and frequency. Often [[phase unwrapping]] is employed along either or both the time axis, τ, and frequency axis, ω, to suppress any [[jump discontinuity]] of the phase result of the STFT.  The time index τ is normally considered to be "''slow''" time and usually not expressed in as high resolution as time ''t''.

=== Discrete-time STFT ===
{{See also|Modified discrete cosine transform}}
In the discrete time case, the data to be transformed could be broken up into chunks or frames (which usually overlap each other, to reduce artifacts at the boundary). Each chunk is [[Fourier transform]]ed, and the complex result is added to a matrix, which records magnitude and phase for each point in time and frequency.  This can be expressed as:

:&lt;math&gt;\mathbf{STFT}\{x[n]\}(m,\omega)\equiv X(m,\omega) = \sum_{n=-\infty}^{\infty} x[n]w[n-m]e^{-j \omega n} &lt;/math&gt;

likewise, with signal ''x''[''n''] and window ''w''[''n''].  In this case, ''m'' is discrete and ω is continuous, but in most typical applications the STFT is performed on a computer using the [[Fast Fourier Transform]], so both variables are discrete and [[Quantization (signal processing)|quantized]].

The [[magnitude (mathematics)|magnitude]] squared of the STFT yields the [[spectrogram]] representation of the Power Spectral Density of the function:

:&lt;math&gt;\operatorname{spectrogram}\{x(t)\}(\tau, \omega) \equiv |X(\tau, \omega)|^2 &lt;/math&gt;

See also the [[modified discrete cosine transform]] (MDCT), which is also a Fourier-related transform that uses overlapping windows.

====Sliding DFT====

If only a small number of ω are desired, or if the STFT is desired to be evaluated for every shift ''m'' of the window, then the STFT may be more efficiently evaluated using a [[sliding DFT]] algorithm.&lt;ref&gt;E. Jacobsen and R. Lyons, [http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=1184347 The sliding DFT], ''Signal Processing Magazine'' vol. 20, issue 2, pp. 74–80 (March 2003).&lt;/ref&gt;

== Inverse STFT ==

The STFT is [[invertible function|invertible]], that is, the original signal can be recovered from the transform by the Inverse STFT. The most widely accepted way of inverting the STFT is by using the [[Overlap–add method|overlap-add (OLA) method]], which also allows for modifications to the STFT complex spectrum. This makes for a versatile signal processing method,&lt;ref&gt;{{cite journal
 | author = Jont B. Allen
 |date=June 1977
 | title = Short Time Spectral Analysis, Synthesis, and Modification by Discrete Fourier Transform
 | journal = IEEE Transactions on Acoustics, Speech, and Signal Processing
 | volume = ASSP-25
 | number = 3
 
 | pages = 235–238
 | url = http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1162950
}}&lt;/ref&gt; referred to as the ''overlap and add with modifications'' method.

=== Continuous-time STFT ===

Given the width and definition of the window function ''w''(''t''), we initially require the area of the window function to be scaled so that

:&lt;math&gt; \int_{-\infty}^{\infty} w(\tau) \, d\tau  = 1.&lt;/math&gt;

It easily follows that

:&lt;math&gt; \int_{-\infty}^{\infty} w(t-\tau) \, d\tau  = 1 \quad \forall \ t &lt;/math&gt;

and

:&lt;math&gt; x(t) = x(t) \int_{-\infty}^{\infty} w(t-\tau) \, d\tau  = \int_{-\infty}^{\infty} x(t) w(t-\tau) \, d\tau. &lt;/math&gt;

The continuous Fourier Transform is

:&lt;math&gt; X(\omega) = \int_{-\infty}^{\infty} x(t) e^{-j \omega t} \, dt. &lt;/math&gt;

Substituting ''x''(''t'') from above:

:&lt;math&gt; X(\omega) = \int_{-\infty}^{\infty} \left[ \int_{-\infty}^{\infty} x(t) w(t-\tau) \, d\tau \right] \, e^{-j \omega t} \, dt &lt;/math&gt;

:::&lt;math&gt; = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x(t) w(t-\tau) \, e^{-j \omega t} \, d\tau \, dt. &lt;/math&gt;

Swapping order of integration:

:&lt;math&gt; X(\omega) = \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} x(t) w(t-\tau) \, e^{-j \omega t} \, dt \, d\tau &lt;/math&gt;

:::&lt;math&gt; = \int_{-\infty}^{\infty} \left[ \int_{-\infty}^{\infty} x(t) w(t-\tau) \, e^{-j \omega t} \, dt \right] \, d\tau &lt;/math&gt;

:::&lt;math&gt; = \int_{-\infty}^{\infty} X(\tau, \omega) \, d\tau. &lt;/math&gt;

So the Fourier Transform can be seen as a sort of phase coherent sum of all of the STFTs of ''x''(''t'').  Since the inverse Fourier transform is

:&lt;math&gt; x(t)  = \frac{1}{2 \pi} \int_{-\infty}^{\infty} X(\omega) e^{+j \omega t} \, d\omega, &lt;/math&gt;

then ''x''(''t'') can be recovered from ''X''(τ,ω) as

:&lt;math&gt; x(t)  = \frac{1}{2 \pi} \int_{-\infty}^{\infty} \int_{-\infty}^{\infty} X(\tau, \omega) e^{+j \omega t} \, d\tau \, d\omega. &lt;/math&gt;

or

:&lt;math&gt; x(t)  = \int_{-\infty}^{\infty} \left[ \frac{1}{2 \pi} \int_{-\infty}^{\infty} X(\tau, \omega) e^{+j \omega t} \, d\omega \right] \, d\tau. &lt;/math&gt;

It can be seen, comparing to above that windowed "grain" or "wavelet" of ''x''(''t'') is

:&lt;math&gt; x(t) w(t-\tau)  = \frac{1}{2 \pi} \int_{-\infty}^{\infty} X(\tau, \omega) e^{+j \omega t} \, d\omega. &lt;/math&gt;

the inverse Fourier transform of ''X''(τ,ω) for τ fixed.

== Resolution issues ==
{{further|Gabor limit}}

One of the pitfalls of the STFT is that it has a fixed resolution.  The width of the windowing function relates to how the signal is represented—it determines whether there is good frequency resolution (frequency components close together can be separated) or good time resolution (the time at which frequencies change).  A wide window gives better frequency resolution but poor time resolution.  A narrower window gives good time resolution but poor frequency resolution.  These are called narrowband and wideband transforms, respectively.

[[Image:STFT - windows.png|frame|none|Comparison of STFT resolution. Left has better time resolution, and right has better frequency resolution.]]

This is one of the reasons for the creation of the [[wavelet transform]] and [[multiresolution analysis]], which can give good time resolution for high-frequency events and good frequency resolution for low-frequency events, the combination best suited for many real signals.

This property is related to the [[Werner Heisenberg|Heisenberg]] [[uncertainty principle]], but not directly – see [[Gabor limit]] for discussion.  The product of the standard deviation in time and frequency is limited.  The boundary of the uncertainty principle (best simultaneous resolution of both) is reached with a Gaussian window function, as the Gaussian minimizes the [[Fourier uncertainty principle]].  This is called the [[Gabor transform]] (and with modifications for multiresolution becomes the [[Morlet wavelet]] transform).

One can consider the STFT for varying window size as a two-dimensional domain (time and frequency), as illustrated in the example below, which can be calculated by varying the window size. However, this is no longer a strictly time–frequency representation – the kernel is not constant over the entire signal.

=== Example ===

Using the following sample signal &lt;math&gt;x(t)&lt;/math&gt; that is composed of a set of four sinusoidal waveforms joined together in sequence.  Each waveform is only composed of one of four frequencies (10, 25, 50, 100 [[hertz|Hz]]).  The definition of &lt;math&gt;x(t)&lt;/math&gt; is:

:&lt;math&gt;x(t)=\begin{cases}
\cos (2 \pi 10 t)  &amp; 0\,\mathrm{s}  \le t &lt; 5  \,\mathrm{s} \\
\cos (2 \pi 25 t)  &amp; 5\,\mathrm{s}  \le t &lt; 10\,\mathrm{s} \\
\cos (2 \pi 50 t)  &amp; 10\,\mathrm{s} \le t &lt; 15\,\mathrm{s} \\
\cos (2 \pi 100 t) &amp; 15\,\mathrm{s} \le t &lt; 20\,\mathrm{s} \\
\end{cases}&lt;/math&gt;

Then it is sampled at 400&amp;nbsp;Hz. The following spectrograms were produced:

{|
|-
|[[Image:STFT colored spectrogram 25ms.png|thumb|300px|25 ms window]]
|[[Image:STFT colored spectrogram 125ms.png|thumb|300px|125 ms window]]
|-
|[[Image:STFT colored spectrogram 375ms.png|thumb|300px|375 ms window]]
|[[Image:STFT colored spectrogram 1000ms.png|thumb|300px|1000 ms window]]
|-
|}

{{clear}}

The 25 ms window allows us to identify a precise time at which the signals change but the precise frequencies are difficult to identify.  At the other end of the scale, the 1000 ms window allows the frequencies to be precisely seen but the time between frequency changes is blurred.

=== Explanation ===

It can also be explained with reference to the sampling and [[Nyquist frequency]].

Take a window of ''N''  samples from an arbitrary real-valued signal at sampling rate ''f''&lt;sub&gt;s&lt;/sub&gt; .  Taking the Fourier transform produces ''N'' complex coefficients.  Of these coefficients only half are useful (the last ''N/2'' being the complex conjugate of the first ''N/2'' in reverse order, as this is a real valued signal).

These ''N/2'' coefficients represent the frequencies 0 to ''f''&lt;sub&gt;s&lt;/sub&gt;/2 (Nyquist) and two consecutive coefficients are spaced apart by
''f''&lt;sub&gt;s&lt;/sub&gt;/''N'' Hz.

To increase the frequency resolution of the window the frequency spacing of the coefficients needs to be reduced.  There are only two variables, but decreasing ''f''&lt;sub&gt;s&lt;/sub&gt; (and keeping ''N'' constant) will cause the window size to increase — since there are now fewer samples per unit time.  The other alternative is to increase ''N'', but this again causes the window size to increase.  So any attempt to increase the frequency resolution causes a larger window size and therefore a reduction in time resolution—and vice versa.

==Rayleigh frequency==
As the Nyquist frequency is a limitation in the maximum frequency that can be meaningfully analysed, so is the Rayleigh frequency a limitation on the minimum frequency.

Rayleigh frequency is the minimum frequency that can be resolved by a finite duration time window.&lt;ref&gt;https://physics.ucsd.edu/neurophysics/publications/Cold%20Spring%20Harb%20Protoc-2014-Kleinfeld-pdb.top081075.pdf&lt;/ref&gt;&lt;ref&gt;http://fieldtrip.fcdonders.nl/faq/what_does_padding_not_sufficient_for_requested_frequency_resolution_mean&lt;/ref&gt;

Given a time window that is Τ seconds long, the minimum frequency that can be resolved is 1/Τ Hz.

Rayleigh frequency is important to consider in applications of the short-time Fourier transform (STFT), such as in analysing neural signals&lt;ref&gt;{{cite journal | pmc = 2441488 | pmid=18293071 | doi=10.1007/s10827-007-0066-2 | volume=25 | title=Biased competition through variations in amplitude of gamma-oscillations | year=2008 | journal=J Comput Neurosci | pages=89–107 |vauthors=Zeitler M, Fries P, Gielen S }}&lt;/ref&gt;&lt;ref&gt;http://www.jneurosci.org/content/30/20/7078.full&lt;/ref&gt;

==Application==
[[File:Short time fourier transform.PNG|thumb|upright=1.2|right|An STFT being used to analyze an audio signal across time]]
STFTs as well as standard Fourier transforms and other tools are frequently used to analyze music. The [[spectrogram]] can, for example, show frequency on the horizontal axis, with the lowest frequencies at left, and the highest at the right. The height of each bar (augmented by color) represents the [[amplitude]] of the frequencies within that band. The depth dimension represents time, where each new bar was a separate distinct transform. Audio engineers use this kind of visual to gain information about an audio sample, for example, to locate the frequencies of specific noises (especially when used with greater frequency resolution) or to find frequencies which may be more or less resonant in the space where the signal was recorded. This information can be used for [[Equalization (audio)|equalization]] or tuning other audio effects.

== Implementation ==
Original function

:&lt;math&gt;X(t,f) = \int^\infty_{-\infty}w(t-\tau) x(\tau) e^{-j 2 \pi f \tau} d\tau&lt;/math&gt;

Converting into the discrete form:

:&lt;math&gt;t = n\Delta_t, f=m\Delta_f, \tau =p\Delta_t&lt;/math&gt;

:&lt;math&gt;X(n\Delta_t, m\Delta_f)=\sum^\infty_{-\infty}w((n-p)\Delta_t)x(p\Delta_t)e^{-j 2 \pi p m \Delta_t \Delta_f}\Delta_t&lt;/math&gt;

Suppose that 

:&lt;math&gt;w(t) \cong 0 \text{ for } |t|&gt;B, \frac{B}{\Delta_t}=Q&lt;/math&gt;

Then we can write the original function into

:&lt;math&gt;X(n\Delta_t, m\Delta_f)= \sum^{n+Q}_{p=n-Q}w((n-p)\Delta_t)x(p\Delta_t)e^{-j 2 \pi p m \Delta_t \Delta_f}\Delta_t&lt;/math&gt;

=== Direct implementation ===

==== Constraints ====
a. Nyquist criterion(Avoiding the aliasing effect):

:&lt;math&gt;\Delta_t &lt; \frac{1}{2\Omega}&lt;/math&gt;, where &lt;math&gt;\Omega&lt;/math&gt; is the bandwidth of &lt;math&gt;x(\tau) w(t-\tau)&lt;/math&gt;

=== FFT-based method ===

==== Constraint ====
a. &lt;math&gt;\Delta_t \Delta_f = \tfrac{1}{N}&lt;/math&gt;, where &lt;math&gt;N&lt;/math&gt; is an integer

b. &lt;math&gt;N \geq 2Q+1&lt;/math&gt;

c. Nyquist criterion (Avoiding the aliasing effect):

:&lt;math&gt;\Delta_t &lt; \frac{1}{2\Omega}&lt;/math&gt;, &lt;math&gt;\Omega&lt;/math&gt; is the bandwidth of &lt;math&gt;x(\tau) w(t-\tau)&lt;/math&gt;

:&lt;math&gt;X(n\Delta_t, m\Delta_f) = \sum_{p=n-Q}^{n+Q} w((n - p)\Delta_t)x(p\Delta_t) e^{-\frac{2\pi jpm}{N}}\Delta_t&lt;/math&gt;

:&lt;math&gt;\text{if we have } q = p - (n - Q), \text{ then } p = (n - Q) + q&lt;/math&gt;

:&lt;math&gt; X(n\Delta_t, m\Delta_f) = \Delta_t e^{\frac{2\pi j(Q - n)m}{N}} \sum_{q=0}^{N-1} x_1(q)e^{-\frac{2\pi jqm}{N}}&lt;/math&gt;

:&lt;math&gt;\text{where } x_1(q) = \begin{cases} w((Q - q)\Delta_t)x((n - Q + q)\Delta_t) &amp;  0 \leq q \leq 2Q\\ 0 &amp; 2Q &lt; q &lt; N \end{cases}&lt;/math&gt;

=== Recursive method ===

==== Constraint ====
a. &lt;math&gt;\Delta_t \Delta_f = \tfrac{1}{N}&lt;/math&gt;, where &lt;math&gt;N&lt;/math&gt; is an integer

b. &lt;math&gt;N \geq 2Q+1&lt;/math&gt;

c. Nyquist criterion (Avoiding the aliasing effect):

:&lt;math&gt;\Delta_t &lt; \frac{1}{2\Omega}&lt;/math&gt;, &lt;math&gt;\Omega&lt;/math&gt; is the bandwidth of &lt;math&gt;x(\tau) w(t-\tau)&lt;/math&gt;

d. Only for implementing the rectangular-STFT

Rectangular window imposes the constraint
:&lt;math&gt;w((n - p)\Delta_t) = 1 &lt;/math&gt;
Substitution gives:
:&lt;math&gt;
\begin{align}
X(n\Delta_t, m\Delta_f) &amp;= \sum_{p=n-Q}^{n+Q} w((n - p)\Delta_t)&amp;x(p\Delta_t) e^{-\frac{j2\pi pm}{N}}\Delta_t \\
                        &amp;= \sum_{p=n-Q}^{n+Q}                   &amp;x(p\Delta_t) e^{-\frac{j2\pi pm}{N}}\Delta_t \\
\end{align}
&lt;/math&gt;

Change of variable {{math|''n''-1}} for {{math|''n''}}:
:&lt;math&gt;
X((n-1)\Delta_t, m\Delta_f) = \sum_{p=n-1-Q}^{n-1+Q} x(p\Delta_t) e^{-\frac{j2\pi pm}{N}}\Delta_t 
&lt;/math&gt;

Calculate &lt;math&gt;X(\min{n}\Delta_t, m\Delta_f)&lt;/math&gt; by the ''N''-point FFT:

:&lt;math&gt;X(n_0\Delta_t, m\Delta_f) = \Delta_t e^{\frac{j 2\pi(Q-n_0)m}{N}} \sum_{q=0}^{N-1} x_1(q) e^{-j\frac{2\pi qm}{N}}, \qquad n_0=\min{(n)}&lt;/math&gt;

where

:&lt;math&gt; x_1(q) = \begin{cases} x((n - Q + q)\Delta_t) &amp;  q \leq 2Q\\ 0 &amp; q &gt;2Q \end{cases}&lt;/math&gt;

Applying the recursive formula to calculate &lt;math&gt;X(n\Delta_t, m\Delta_f)&lt;/math&gt;

:&lt;math&gt;X(n\Delta_t, m\Delta_f) = X((n-1)\Delta_t, m\Delta_f) - x((n - Q -1)\Delta_t) e^{-\frac{j 2\pi(n-Q-1)m}{N}}\Delta_t + x((n+Q)\Delta_t)e^{-\frac{j 2\pi(n+Q)m}{N}}\Delta_t&lt;/math&gt;

=== Chirp Z transform ===

==== Constraint ====

:&lt;math&gt;\exp{(-j2\pi pm\Delta_t\Delta_f)} = \exp{(-j\pi p^2\Delta_t\Delta_f)} \cdot \exp{(j\pi(p-m)^2\Delta_t\Delta_f)}\cdot \exp{(-j\pi m^2\Delta_t\Delta_f)}&lt;/math&gt;

so 

:&lt;math&gt;X(n\Delta_t, m\Delta_f) = \Delta_t \sum_{p=n-Q}^{n+Q} w((n-p)\Delta_t)x(p\Delta_t)e^{-j2\pi pm\Delta_t\Delta_f}&lt;/math&gt;

:&lt;math&gt;X(n\Delta_t, m\Delta_f) = \Delta_t e^{-j2\pi m^2\Delta_t\Delta_f} \sum_{p=n-Q}^{n+Q} w((n-p)\Delta_t)x(p\Delta_t)e^{-j\pi p^2\Delta_t\Delta_f} e^{j\pi (p-m)^2\Delta_t\Delta_f} &lt;/math&gt;

=== Implementation comparison ===
{| class="wikitable"
!Method
!Complexity
|-
|Direct implementation
|&lt;math&gt;O(TFQ)&lt;/math&gt;
|-
|FFT-based
|&lt;math&gt;O(TN \log_2 N)
&lt;/math&gt;
|-
|Recursive
|&lt;math&gt;O(TF)&lt;/math&gt;
|-
|Chirp Z transform
|&lt;math&gt;O(TN \log_2 N)
&lt;/math&gt;
|}

== See also ==
* [[Spectral density estimation]]
* [[Time-frequency representation]]
* [[Reassignment method]]

Other time-frequency transforms:
* [[Cone-shape distribution function]]
* [[Constant-Q transform]]
* [[Fractional Fourier transform]]
* [[Gabor transform]]
* [[Newland transform]]
* [[S transform]]
* [[Wavelet transform]]
* [[Chirplet transform]]

==References==

&lt;references/&gt;

== External links ==
* [http://tfd.sourceforge.net/ DiscreteTFDs – software for computing the short-time Fourier transform and other time-frequency distributions]
*[http://www.atmos.ucla.edu/tcd/ssa/ Singular Spectral Analysis - MultiTaper Method Toolkit] - a free software program to analyze short, noisy time series.
*[http://www.spectraworks.com kSpectra Toolkit for Mac OS X from SpectraWorks]
*[https://www.researchgate.net/publication/3091384_Time-stretched_short-time_Fourier_transform Time stretched short time Fourier transform for time frequency analysis of ultra wideband signals]
*[http://www.mathworks.fr/matlabcentral/fileexchange/33451-stft-mdct-and-inverses-onset-and-pitch-detection A BSD-licensed Matlab class to perform STFT and inverse STFT]
*[http://ltfat.sourceforge.net/ LTFAT - A free (GPL) Matlab / Octave toolbox to work with short-time Fourier transforms and time-frequency analysis]

{{DEFAULTSORT:Short-Time Fourier Transform}}
[[Category:Fourier analysis]]
[[Category:Time–frequency analysis]]
[[Category:Transforms]]
[[Category:Signal processing]]</text>
      <sha1>o8ple3k4opk6cxkbiwly93pu77hw22s</sha1>
    </revision>
  </page>
  <page>
    <title>Spatial network</title>
    <ns>0</ns>
    <id>1708671</id>
    <revision>
      <id>859954928</id>
      <parentid>853078588</parentid>
      <timestamp>2018-09-17T11:42:28Z</timestamp>
      <contributor>
        <username>Lanouchi</username>
        <id>32552622</id>
      </contributor>
      <minor/>
      <comment>Added figures 1 and 2</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13865">{{Network Science}}

A '''spatial network''' (sometimes also '''[[Geometric graph theory|geometric graph]]''') is a [[Graph (discrete mathematics)|graph]] in which the [[Vertex (graph theory)|vertices]] or [[Edge (graph theory)|edges]] are ''spatial elements'' associated with [[Geometry|geometric]] objects, i.e. the nodes are located in a space equipped with a certain [[Metric (mathematics)|metric]].&lt;ref name="Bart"&gt;M. Barthelemy, "Spatial Networks", Physics Reports 499:1-101 (2011) ( https://arxiv.org/abs/1010.0302 ).&lt;/ref&gt;&lt;ref name="Bart2"&gt;M. Barthelemy, "Morphogenesis of Spatial Networks", Springer (2018).&lt;/ref&gt; The simplest mathematical realization is a [[Lattice graph|lattice]] or a [[random geometric graph]], where nodes are distributed uniformly at random over a two-dimensional plane; a pair of nodes are connected if the [[Euclidean distance]] is smaller than a given neighborhood radius. [[Transport network|Transportation and mobility networks]], [[Internet]], [[cellular network|mobile phone networks]], [[electrical grid|power grids]], [[social network|social and contact networks]] and [[neural network]]s are all examples where the underlying space is relevant and where the graph's [[topology]] alone does not contain all the information. Characterizing and understanding the structure, resilience and the evolution of spatial networks is crucial for many different fields ranging from urbanism to epidemiology.

== Examples ==
An urban spatial network can be constructed by abstracting intersections as nodes and streets as links, which is referred to as, [[Transportation network (graph theory)|transportation network]]. Beijing traffic was studied as a dynamic network and its percolation properties have been found useful to identify systematic bottlenecks.&lt;ref&gt;{{Cite journal|last=Li|first=D.|last2=Fu|first2=B.|last3=Wang|first3=Y.|last4=Lu|first4=G.|last5=Berezin|first5=Y.|last6=Stanley|first6=H.E.|last7=Havlin|first7=S.|date=2015|title=Percolation transition in dynamical traffic network with evolving critical bottlenecks|url=|journal=PNAS|volume=112|pages=669|via=|bibcode=2015PNAS..112..669L|doi=10.1073/pnas.1419185112|pmc=4311803}}&lt;/ref&gt;

One might think of the 'space map' as being the negative image of the standard map, with the open space cut out of the background buildings or walls.&lt;ref name="SocLo"&gt;Hillier B, Hanson J, 1984, The social logic of space (Cambridge University Press, Cambridge, UK).&lt;/ref&gt;

==Characterizing spatial networks==
The following aspects are some of the characteristics to examine a spatial network:&lt;ref name="Bart" /&gt;
* Planar networks
In many applications, such as rail, roads, and other transportation networks the network is assumed to be [[planar graph|planar]]. Planar networks build up an important group out of the spatial networks, but not all spatial networks are planar. Indeed, the airline passenger
networks is a non-planar example: All airports in the world are connected through direct flights. 
* The way it is embedded in space
There are examples of networks, which seem to be not "directly" embedded in space. Social networks for instance
connect individuals through friendship relations. But in this case, space intervenes in the fact that the connection
probability between two individuals usually decreases with the distance between them.
* Voronoi tessellation
A spatial network can be represented by a [[Voronoi diagram]], which is a way of dividing space into a number of regions.  The dual graph for a Voronoi diagram corresponds to the [[Delaunay triangulation]] for the same set of points.
Voronoi tessellations are interesting for spatial networks in the sense that they provide a natural representation model
to which one can compare a real world network.
* Mixing space and topology

[[File:Lattice network 2d.png|alt=Lattice network in two dimensions|thumb|Fig. 1. Lattice network in two dimensions. The balls are the nodes the edges connecting neighboring nodes are the links.]]
[[File:Spatially interdependent networks.png|alt=Spatially interdependent networks|thumb|Fig. 2. Spatially interdependent networks. Two square lattices A and B, where in each lattice a node has two types of links: connectivity links and dependency links. Every node is connected (with connectivity links) to its four nearest neighbors within the same lattice via connectivity links and a fraction of nodes in each network have dependency links to the other network. If a node fail in one network its dependent node in the other network will also fail, even if it is still connected to its network via connectivity links.]]
Examining the topology of the nodes and edges itself is another way to characterize networks.  The distribution of [[Degree (graph theory)|degree]] of the nodes is often considered, regarding the structure of edges it is useful to find the [[Minimum spanning tree]], or the generalization, the [[Steiner tree]] and the [[relative neighborhood graph]].
[[File:Lattice-layout-z15.png|thumb|Fig. 3:  Spatially embedded multiplex networks. The nodes occupy regular locations in two-dimensional lattice  while the links in each layer (blue and green) have lengths
that are exponentially distributed with characteristic length
ζ = 3 and are connected at random with degree k=4.]]

==Lattice networks==
 
Lattice networks (see Fig. 1) are useful models  for spatial embedded networks. Many physical phenomenas have been studied on these structures. Examples include Ising model for spontaneous magnetization,&lt;ref name="McCoyWu1968"&gt;{{cite journal|last1=McCoy|first1=Barry M.|last2=Wu|first2=Tai Tsun|title=Theory of a Two-Dimensional Ising Model with Random Impurities. I. Thermodynamics|journal=Physical Review|volume=176|issue=2|year=1968|pages=631–643|issn=0031-899X|doi=10.1103/PhysRev.176.631|bibcode=1968PhRv..176..631M}}&lt;/ref&gt; diffusion phenomena modeled as random walks&lt;ref name="MasoliverMontero2003"&gt;{{cite journal|last1=Masoliver|first1=Jaume|last2=Montero|first2=Miquel|last3=Weiss|first3=George H.|title=Continuous-time random-walk model for financial distributions|journal=Physical Review E|volume=67|issue=2|year=2003|issn=1063-651X|doi=10.1103/PhysRevE.67.021112|arxiv=cond-mat/0210513|bibcode=2003PhRvE..67b1112M}}&lt;/ref&gt;
and percolation.&lt;ref name="BundeHavlin1996"&gt;{{cite journal|last1=Bunde|first1=Armin|last2=Havlin|first2=Shlomo|year=1996|doi=10.1007/978-3-642-84868-1|title=Fractals and Disordered Systems}}&lt;/ref&gt; Recently to model the resilience of  interdependent infrastructures which are spatially embedded a model of interdependent lattice networks was introduced (see Fig. 2) and analyzed&lt;ref name="LiBashan2012"&gt;{{cite journal|last1=Li|first1=Wei|last2=Bashan|first2=Amir|last3=Buldyrev|first3=Sergey V.|last4=Stanley|first4=H. Eugene|last5=Havlin|first5=Shlomo|title=Cascading Failures in Interdependent Lattice Networks: The Critical Role of the Length of Dependency Links|journal=Physical Review Letters|volume=108|issue=22|year=2012|issn=0031-9007|doi=10.1103/PhysRevLett.108.228702|pmid=23003664|pages=228702|arxiv=1206.0224|bibcode=2012PhRvL.108v8702L}}&lt;/ref&gt;
.&lt;ref name="BashanBerezin2013"&gt;{{cite journal|last1=Bashan|first1=Amir|last2=Berezin|first2=Yehiel|last3=Buldyrev|first3=Sergey V.|last4=Havlin|first4=Shlomo|title=The extreme vulnerability of interdependent spatially embedded networks|journal=Nature Physics|volume=9|issue=10|year=2013|pages=667–672|issn=1745-2473|doi=10.1038/nphys2727|arxiv=1206.2062|bibcode=2013NatPh...9..667B}}&lt;/ref&gt; A spatial multiplex model was introduced
by Danziger et al&lt;ref name="DanzigerShekhtman2016"&gt;{{cite journal|last1=Danziger|first1=Michael M.|last2=Shekhtman|first2=Louis M.|last3=Berezin|first3=Yehiel|last4=Havlin|first4=Shlomo|title=The effect of spatiality on multiplex networks|journal=EPL|volume=115|issue=3|year=2016|pages=36002|issn=0295-5075|doi=10.1209/0295-5075/115/36002|bibcode=2016EL....11536002D}}&lt;/ref&gt; and was analyzed further by Vaknin et al.&lt;ref name="VakninDanziger2017"&gt;{{cite journal|last1=Vaknin|first1=Dana|last2=Danziger|first2=Michael M|last3=Havlin|first3=Shlomo|title=Spreading of localized attacks in spatial multiplex networks|journal=New Journal of Physics|volume=19|issue=7|year=2017|pages=073037|issn=1367-2630|doi=10.1088/1367-2630/aa7b09|arxiv=1704.00267|bibcode=2017NJPh...19g3037V}}&lt;/ref&gt; For the model see Fig. 3.

==Probability and spatial networks ==
In the "real" world many aspects of networks are not deterministic - randomness plays an important role. For example, new links, representing friendships, in social networks are in a certain manner random. Modelling spatial networks in respect of stochastic operations is consequent. In many cases the [[spatial Poisson process]] is used to approximate data sets of processes on spatial networks. Other stochastic aspects of interest are: 
* The [[Poisson line process]]
* Stochastic geometry: the [[Erdős–Rényi model|Erdős–Rényi graph]]
* [[Percolation theory]]

==Approach from the theory of space syntax==
Another definition of spatial network derives from the theory of [[space syntax]]. It can be notoriously difficult to decide what a spatial element should be in complex spaces involving large open areas or many interconnected paths. The originators of space syntax, Bill Hillier and Julienne Hanson use [[axial line]]s and [[convex space]]s as the spatial elements. Loosely, an axial line is the 'longest line of sight and access' through open space, and a convex space the 'maximal convex polygon' that can be drawn in open space. Each of these elements is defined by the geometry of the local boundary in different regions of the space map. Decomposition of a space map into a complete set of intersecting axial lines or overlapping convex spaces produces the axial map or overlapping convex map respectively. Algorithmic definitions of these maps exist, and this allows the mapping from an arbitrary shaped space map to a network amenable to graph mathematics to be carried out in a relatively well defined manner. Axial maps are used to analyse [[urban network]]s, where the system generally comprises linear segments, whereas convex maps are more often used to analyse [[building|building plans]] where space patterns are often more convexly articulated, however both convex and axial maps may be used in either situation.

Currently, there is a move within the space syntax community to integrate better with [[geographic information system]]s (GIS), and much of the [[Spatial network analysis software|software]] they produce interlinks with commercially available GIS systems.

==History==
While networks and graphs were already for a long time the subject
of many studies in [[mathematics]], physics, mathematical sociology,
[[computer science]], spatial networks have been studied intensively during the 1970s in quantitative geography. Objects of studies in geography are inter alia locations, activities and flows of individuals, but also networks evolving in time and space.&lt;ref name="NetAnal"&gt;P. Haggett and R.J. Chorley. ''Network analysis in geog-
raphy''. Edward Arnold, London, 1969.&lt;/ref&gt; Most of the important problems such
as the location of nodes of a network, the evolution of
transportation networks and their interaction with population
and activity density are addressed in these earlier
studies. On the other side, many important points still remain unclear, partly because at that time datasets of large networks and larger computer capabilities were lacking.  
Recently, spatial networks have been the subject of studies in [[Statistics]], to connect probabilities and stochastic processes with networks in the real world.&lt;ref name="David"&gt;http://www.stat.berkeley.edu/~aldous/206-SNET/index.html&lt;/ref&gt;

==See also==
* [[Hyperbolic geometric graph]]
* [[Spatial network analysis software]]
* [[Cascading Failures]]
* [[Complex network]]
* [[Planar graphs]]
* [[Percolation Theory]]
* [[Random graphs]]
* [[Topological graph theory]]
* [[Chemical graph]]

==References==
{{reflist}}
&lt;!--- After listing your sources please cite them using inline citations and place them after the information they cite. Please see http://en.wikipedia.org/wiki/Wikipedia:REFB for instructions on how to add citations. ---&gt;
*{{cite journal
 |last        = Bandelt
 |first       = Hans-Jürgen
 |author2     = Chepoi, Victor
 |url         = http://www.lif-sud.univ-mrs.fr/%7Echepoi/survey_cm_bis.pdf
 |format      = PDF
 |title       = Metric graph theory and geometry: a survey
 |journal     = Contemp. Math.
 |year        = 2008
 |pages       = to appear
 |deadurl     = yes
 |archiveurl  = https://web.archive.org/web/20061125112119/http://www.lif-sud.univ-mrs.fr/~chepoi/survey_cm_bis.pdf
 |archivedate = 2006-11-25
 |df          = 
}}

*{{cite book
  | title = Towards a Theory of Geometric Graphs
  | year = 2004
  | publisher = Contemporary Mathematics, no. 342, American Mathematical Society
  |last=Pach |first=János |display-authors=etal
  | authorlink1 = János Pach
  }}

*{{cite conference
 |author      = [[Tomaž Pisanski|Pisanski, Tomaž]]; [[Milan Randić|Randić, Milan]]
 |title       = Bridges between geometry and graph theory
 |date        = 2000
 |url         = http://www.ijp.si/ftp/pub/preprints/ps/98/pp595.ps
 |booktitle   = Geometry at Work: Papers in Applied Geometry
 |editor      = Gorini, C. A. (Ed.)
 |location    = Washington, DC
 |publisher   = Mathematical Association of America
 |pages       = 174–194
 |deadurl     = yes
 |archiveurl  = https://web.archive.org/web/20070927192355/http://www.ijp.si/ftp/pub/preprints/ps/98/pp595.ps
 |archivedate = 2007-09-27
 |df          = 
}}

{{DEFAULTSORT:Spatial Network}}
[[Category:Geometric graph theory|*]]
[[Category:Architectural theory]]
[[Category:Environmental design]]
[[Category:Environmental psychology]]
[[Category:Application-specific graphs]]
[[Category:Graph theory]]</text>
      <sha1>bv1intyvsyxqpbztu5jv68nixaxt19f</sha1>
    </revision>
  </page>
  <page>
    <title>TNSDL</title>
    <ns>0</ns>
    <id>26077527</id>
    <revision>
      <id>790008575</id>
      <parentid>777541246</parentid>
      <timestamp>2017-07-11T00:47:03Z</timestamp>
      <contributor>
        <username>SJ Defender</username>
        <id>19403234</id>
      </contributor>
      <minor/>
      <comment>Disambiguated: [[asynchronous]] → [[asynchronous communication]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9855">'''TNSDL''' stands for TeleNokia [[Specification and Description Language]]. TNSDL is based on the [[ITU-T]] [[Specification and Description Language|SDL]]-88 language. It is used exclusively at [[Nokia Networks]], primarily for developing applications for [[telephone exchange]]s.

==Purpose==
TNSDL is a general-purpose [[procedural programming language]]. It is especially well-suited for developing highly concurrent, distributed systems.&lt;ref name="FM'99"&gt;{{cite book|title=FM'99 - Formal Methods: World Congress on Formal Methods, 1999, Proceedings|date=1999|publisher=Springer|isbn=3540665870|url=https://books.google.com/books?id=C_ErOSAokFcC&amp;dq=TNSDL|editor1=Jeannette M. Wing |editor2=Jim Woodcook |editor3=Jim Davies }}&lt;/ref&gt;

It was originally designed for programming circuit switched exchanges. As the world shifted towards packet-switched and internet-based telecommunication, TNSDL turned out to be an excellent fit for developing internet servers, too.

==Design==
TNSDL is a very simple, easy-to-learn programming language.

===Basics===
TNSDL is a [[Strongly typed programming language|strongly typed]] [[procedural programming language]]. Its basic capabilities are comparable to the [[C (programming language)|C]] and [[Pascal (programming language)|Pascal]] languages.

===Multi-processing===
In TNSDL processes are created by the CREATE command. (It is somewhat similar to the [[POSIX]] [[Fork (operating system)|fork]] or [[POSIX Threads|pthread_create]] commands.) The CREATE command creates either an operating system process or a [[Cooperative multitasking|cooperative task]].

The process model can be selected by configuration. The source code itself does not reflect which scheduling method is used. Still, to avoid certain [[race condition]]s, developers may need to be prepared for parallel execution. TNSDL explicitly supports [[critical section]]s to be marked in the code.

In case of [[cooperative multitasking]] a program is scheduled as one operating system process. When a cooperative thread enters the state of waiting for asynchronous input, another thread of the program may run.

===Message passing===
The feature of TNSDL is the [[actor model]]. Processes are meant to be designed as [[event-driven finite state machine]]s. [[Inter-process communication]] is done by asynchronous [[message passing]]. The OUTPUT command sends a message, while INPUT statements define the expected messages.

Timers, from TNSDL perspective, are delayed messages. Just like ordinary messages, timer expiration is handled by the INPUT statement. The SET command starts and the RESET command cancels a timer.

State machines can be optionally used, for example, to prevent accepting certain input messages at some stage of the processing.

The following code piece demonstrates a server, which receives a query signal (message), contacts a database process to obtain the necessary data and finally sends an answer signal.

&lt;source lang="c"&gt;
DCL WITHWARMING /* Data to be live-migrated (on platforms supporting "warming") */
  query_process pid; /* PID of query_signal sender */

CONSTANT time_to_wait = 10; /* Timeout of database response */

TIMER db_timeout_timer; /* Timer of database response */

STATE idle; /* Idle state, wait for query signal */
  INPUT query_signal(DCL input_data);
    DCL
      db_query db_query_type;     /* Local variable, stored on stack. */
    TASK query_process := SENDER; /* Sender address saved to specific memory area, which is preserved even on software update.*/
    TASK db_query.field1 := some_procedure(input_data),
         db_query.field2 := input_data.field1;
    OUTPUT db_request_signal(db_query) TO db_process; /* Send request to database process */
    SET(NOW + time_to_wait, db_timeout_timer);        /* Start database response timer */
    NEXTSTATE wait_db;                                /* Enter wait_db state where database response is expected */
ENDSTATE idle;

STATE wait_db;
  INPUT db_response_signal(DCL answer_data);
    RESET(db_timeout_timer) COMMENT 'Database answered in time';
    OUTPUT answer_signal(answer_data.records) TO query_process;
    NEXTSTATE idle;

  INPUT db_timeout_timer; /* Timeout */
    OUTPUT error_signal(error_constant) TO query_process;
    NEXTSTATE idle;
ENDSTATE wait_db;
&lt;/source&gt;

Comments:
* The state machine prevents any new query_signal to be processed while waiting for the database program to answer.
* WITHWARMING means that when another computer takes over the role of the current one, the marked data (variable) will be copied to the new computer. Therefore, if hardware change or software update happens while waiting for the database to answer, the address of the query sender will not be lost, and the answer can be delivered properly. It is not supported on all platforms, though.

TNSDL allows inputs to be tied to several or all of the states. An input signal may have state-specific behavior, if needed.

&lt;source lang="c"&gt;
STATE idle COMMENT 'Idle state';
  INPUT are_you_busy;
    OUTPUT no TO SENDER;
    NEXTSTATE -; /* No state change */
  /* ... other input handlers */
ENDSTATE idle;

STATE *(idle) COMMENT 'Any state, except idle';
  INPUT are_you_busy;
    OUTPUT yes TO SENDER;
    NEXTSTATE -; /* No state change */
ENDSTATE *(idle);

STATE * COMMENT 'Any state';
  INPUT are_you_alive;
    OUTPUT yes TO SENDER;
    NEXTSTATE -; /* No state change */
ENDSTATE *;
&lt;/source&gt;

===Differences from SDL-88===

Nokia has made several modifications to the language,&lt;ref&gt;{{cite journal|title=Dynamical analysis of SDL programs with Predicate/Transition nets|publisher=Helsinki University of Technology, Digital Systems Laboratory|first=Tero |last=Jyrinki|date=1997|page=22}}&lt;/ref&gt; mainly including simplifications and additions, such as:

* Features like channels and signal routes have been replaced by other mechanisms.
* The concepts of modules and services were added in TNSDL (the service concept of SDL-88 is similar to the subautomaton feature of TNSDL). 
* Some elements have been renamed (for example Priority Inputs are called Input Internals in TNSDL). 
* In TNSDL the MACRO feature has been omitted and a WHILE construct has been added to allow loops in a structured way without using JOINs.

==Compiling==
TNSDL is not directly [[compiler|compiled]] to machine code. Instead, TNSDL programs are translated to [[C (programming language)|C language]] source code. The responsibility of TNSDL is to allow message handling, state machine definitions, synchronizing parallel execution, "data warming" etc. easily and safely coded. The task of processor-specific code generation and low-level optimization is delegated to the C compiler used.

After translating TNSDL to C, any standard-compliant C compiler, linker, coverage measurement and profiling tool can be used. To make source-level debugging possible TNSDL puts line number references to the generated C code.

TNSDL code can call routines implemented in other languages, if objects or libraries are present for them. Even [[C (programming language)|C language macros]] can be used, if C header files are present. External declarations must be made available to the TNSDL translator.

The TNSDL translator is a proprietary tool. A source code (reachability) analyser has also been developed specifically for TNSDL.&lt;ref&gt;{{cite journal|title=Emma: A Tool For Analysis of SDL Programs|citeseerx = 10.1.1.30.3240|first1 = Nisse | last1 = Husberg | first2 = Markus | last2 = Malmqvist | first3 = Tero | last3 = Jyrinki|date=1996}}&lt;/ref&gt;

==Use==
TNSDL is commonly used on the [[DX 200]], IPA 2800 and Linux platforms for high-performance, high-availability applications.

TNSDL is an actively used and developed programming language used by thousands of developers (in 2010).{{Citation needed|date=April 2014}}

TNSDL is mainly used at [[Nokia Networks]] for developing software for [[SGSN]]s, [[Base station controller|BSC]]s, [[Mobile Switching Center|mobile switching centers]], [[application server]]s both in traditional setups and as virtual network functions (VNF) of [[Network function virtualization|NFV]] solutions.

== Similar programming languages ==
Despite the difference in syntax, probably one of the closest relative of TNSDL is [[Go (programming language)|Go language]]. Both languages has [[Light-weight process|light-weight processes]] in their focus. Go's channel are similar to TNSDL INPUTs and Go's select statement on channels allows very similar program design. There are differences, though. TNSDL uses [[asynchronous communication|asynchronous]] [[message passing]] between [[Actor model|actors]], while channels in Go can either be [[synchronous]] or asynchronous (buffered). TNSDL allows message passing between processes running on the same or separate computer nodes. In that aspect TNSDL is a relative of [[Erlang (programming language)|Erlang]].

Even though in TNSDL one can define operators for types and protect structure attributes to be accessible via those operators only, TNSDL is not an [[Object-oriented programming|object-oriented]] language. In that aspect it belongs to the family of non-OOP [[procedural programming]] languages, such as [[C (programming language)|C language]].

==History==
1980s: In the beginning, [[ITU-T]] [[Specification and Description Language|SDL]] had a graphical syntax. Textual syntax was introduced later. A corresponding graphical tool and code generator were developed within [[Nokia]].

1990: [[ITU-T]] [[Specification and Description Language|SDL]] shifted towards text-based representation. Based on the SDL-88 specification TNSDL was born. TNSDL is a simplified and heavily customized variant of SDL-88.

==References==
{{Reflist}}

[[Category:Formal specification languages]]
[[Category:Procedural programming languages]]
[[Category:Programming languages created in the 1980s]]</text>
      <sha1>q5eo66jupgdi5rqy6kfhm1qtoetlows</sha1>
    </revision>
  </page>
  <page>
    <title>Telephone number (mathematics)</title>
    <ns>0</ns>
    <id>35258497</id>
    <revision>
      <id>850385624</id>
      <parentid>850102077</parentid>
      <timestamp>2018-07-15T14:51:20Z</timestamp>
      <contributor>
        <username>TheMagikBOT</username>
        <id>24649005</id>
      </contributor>
      <minor/>
      <comment>Added page protection template where none existed</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14583">{{pp|small=yes}} 
[[File:K4 matchings.svg|thumb|The [[complete graph]] {{math|''K''&lt;sub&gt;4&lt;/sub&gt;}} has ten matchings, corresponding to the value {{math|1=''T''(4)&amp;nbsp;=&amp;nbsp;10}} of the fourth telephone number.]]
In [[mathematics]], the '''telephone numbers''' or the '''involution numbers''' are a [[integer sequence|sequence of integers]] that count the ways {{mvar|n}} telephone lines can be connected to each other, where each line can be connected to at most one other line. These numbers also describe the number of [[matching (graph theory)|matchings]] (the [[Hosoya index]]) of a [[complete graph]] on {{mvar|n}} vertices, the number of  [[permutation]]s on {{mvar|n}} elements that are [[involution (mathematics)|involution]]s, the sum of absolute values of coefficients of the [[Hermite polynomial]]s, the number of standard [[Young tableau]]x with {{mvar|n}} cells, and the sum of the degrees of the [[irreducible representation]]s of the [[symmetric group]]. Involution numbers were first studied in 1800 by [[Heinrich August Rothe]], who gave a [[recurrence equation]] by which they may be calculated,&lt;ref name="knuth"&gt;{{citation
 | last = Knuth | first = Donald E. | author-link = Donald Knuth
 | location = Reading, Mass.
 | mr = 0445948
 | pages = 65–67
 | publisher = Addison-Wesley
 | title = [[The Art of Computer Programming]], Volume 3: Sorting and Searching
 | year = 1973}}.&lt;/ref&gt; giving the values (starting from {{math|1=''n''&amp;nbsp;=&amp;nbsp;0}})
:1, 1, 2, 4, 10, 26, 76, 232, 764, 2620, 9496, ... {{OEIS|A000085}}.

==Applications==
[[John Riordan (mathematician)|John Riordan]] provides the following explanation for these numbers: suppose that a telephone service has {{mvar|n}} subscribers, any two of whom may be connected to each other by a telephone call. How many different patterns of connection are possible? For instance, with three subscribers, there are three ways of forming a single telephone call, and one additional pattern in which no calls are being made, for a total of four patterns.&lt;ref&gt;{{citation
 | last = Riordan | first = John | author-link = John Riordan (mathematician)
 | pages = 85–86
 | publisher = Dover
 | title = Introduction to Combinatorial Analysis
 | year = 2002}}.&lt;/ref&gt; For this reason, the numbers counting how many patterns are possible are sometimes called the telephone numbers.&lt;ref&gt;{{citation
 | last1 = Peart | first1 = Paul
 | last2 = Woan | first2 = Wen-Jin
 | issue = 2
 | journal = Journal of Integer Sequences
 | mr = 1778992
 | at = Article 00.2.1
 | title = Generating functions via Hankel and Stieltjes matrices
 | url = http://www.emis.ams.org/journals/JIS/VOL3/PEART/peart1.pdf
 | volume = 3
 | year = 2000}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last = Getu | first = Seyoum
 | doi = 10.2307/2690455
 | issue = 1
 | journal = Mathematics Magazine
 | mr = 1092195
 | pages = 45–53
 | title = Evaluating determinants via generating functions
 | volume = 64
 | year = 1991}}.&lt;/ref&gt;

Every pattern of pairwise connections between {{mvar|n}} subscribers defines an [[involution (mathematics)|involution]], a [[permutation]] of the subscribers that is its own inverse, in which two subscribers who are making a call to each other are swapped with each other and all remaining subscribers stay in place. Conversely, every possible involution has the form of a set of pairwise swaps of this type. Therefore, the telephone numbers also count involutions. The problem of counting involutions was the original [[combinatorial enumeration]] problem studied by Rothe in 1800&lt;ref name="knuth"/&gt; and these numbers have also been called involution numbers.&lt;ref name="sbdhp"&gt;{{citation
 | last1 = Solomon | first1 = A. I.
 | last2 = Blasiak | first2 = P.
 | last3 = Duchamp | first3 = G.
 | last4 = Horzela | first4 = A.
 | last5 = Penson | first5 = K.A.
 | editor1-last = Gruber | editor1-first = Bruno J.
 | editor2-last = Marmo | editor2-first = Giuseppe
 | editor3-last = Yoshinaga | editor3-first = Naotaka
 | arxiv = quant-ph/0310174
 | contribution = Combinatorial physics, normal order and model Feynman graphs
 | doi = 10.1007/1-4020-2634-X_25
 | pages = 527–536
 | publisher = Kluwer Academic Publishers
 | title = Symmetries in Science XI
 | year = 2005}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last1 = Blasiak | first1 = P.
 | last2 = Dattoli | first2 = G.
 | last3 = Horzela | first3 = A.
 | last4 = Penson | first4 = K. A.
 | last5 = Zhukovsky | first5 = K.
 | issue = 1
 | journal = Journal of Integer Sequences
 | mr = 2377567
 | at = Article 08.1.1
 | title = Motzkin numbers, central trinomial coefficients and hybrid polynomials
 | url = http://www.cs.uwaterloo.ca/journals/JIS/VOL11/Penson/penson131.html
 | volume = 11
 | year = 2008}}.&lt;/ref&gt;

In [[graph theory]], a subset of the edges of a graph that touches each vertex at most once is called a [[matching (graph theory)|matching]]. The number of different matchings of a given graph is important in [[chemical graph theory]], where the graphs model molecules and the number of matchings is known as the [[Hosoya index]]. The largest possible Hosoya index of an {{mvar|n}}-vertex graph is given by the [[complete graph]]s, for which any pattern of pairwise connections is possible; thus, the Hosoya index of a complete graph on {{mvar|n}} vertices is the same as the {{mvar|n}}th telephone number.&lt;ref&gt;{{citation
 | last1 = Tichy | first1 = Robert F.
 | last2 = Wagner | first2 = Stephan
 | doi = 10.1089/cmb.2005.12.1004
 | issue = 7
 | journal = [[Journal of Computational Biology]]
 | pages = 1004–1013
 | title = Extremal problems for topological indices in combinatorial chemistry
 | url = http://www.math.tugraz.at/fosp/pdfs/tugraz_main_0052.pdf
 | volume = 12
 | year = 2005}}.&lt;/ref&gt;

[[File:Young tableaux for 541 partition.svg|thumb|A standard Young tableau]]
A [[Ferrers diagram]] is a geometric shape formed by a collection of {{mvar|n}} squares in the plane, grouped into a [[polyomino]] with a horizontal top edge, a vertical left edge, and a single monotonic chain of horizontal and vertical bottom and right edges. A standard [[Young tableau]] is formed by placing the numbers from 1 to {{mvar|n}} into these squares in such a way that the numbers increase from left to right and from top to bottom throughout the tableau.
According to the [[Robinson–Schensted correspondence]], permutations correspond one-for-one with ordered pairs of standard [[Young tableau]]x. Inverting a permutation corresponds to swapping the two tableaux, and so the self-inverse permutations correspond to single tableaux, paired with themselves.&lt;ref name="b"&gt;A direct bijection between involutions and tableaux, inspired by the recurrence relation for the telephone numbers, is given by {{citation
 | last = Beissinger | first = Janet Simpson
 | doi = 10.1016/0012-365X(87)90024-0
 | issue = 2
 | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]
 | mr = 913181
 | pages = 149–163
 | title = Similar constructions for Young tableaux and involutions, and their application to shiftable tableaux
 | volume = 67
 | year = 1987}}.&lt;/ref&gt; Thus, the telephone numbers also count the number of Young tableaux with {{mvar|n}} squares.&lt;ref name="knuth"/&gt; In [[representation theory]], the Ferrers diagrams correspond to the [[irreducible representation]]s of the [[symmetric group]] of permutations, and the Young tableaux with a given shape form a basis of the irreducible representation with that shape. Therefore, the telephone numbers give the sum of the degrees of the irreducible representations.

{{Chess diagram
| tright
|
|__|__|__|rl|__|__|__|__
|__|__|__|__|__|__|rl|__
|__|__|rl|__|__|__|__|__
|rl|__|__|__|__|__|__|__
|__|__|__|__|rl|__|__|__
|__|__|__|__|__|__|__|rl
|__|rl|__|__|__|__|__|__
|__|__|__|__|__|rl|__|__
|A diagonally symmetric non-attacking placement of eight rooks on a chessboard
}}
In the [[Mathematical chess problem|mathematics of chess]], the telephone numbers count the number of ways to place {{mvar|n}} rooks on an {{math|''n''&amp;nbsp;×&amp;nbsp;''n''}} [[chessboard]] in such a way that no two rooks attack each other (the so-called [[Eight queens puzzle|eight rooks puzzle]]), and in such a way that the configuration of the rooks is symmetric under a diagonal reflection of the board. Via the [[Pólya enumeration theorem]], these numbers form one of the key components of a formula for the overall number of "essentially different" configurations of {{mvar|n}} mutually non-attacking rooks, where two configurations are counted as essentially different if there is no symmetry of the board that takes one into the other.&lt;ref&gt;{{citation
 | last = Holt | first = D. F.
 | issue = 404
 | journal = The Mathematical Gazette
 | jstor = 3617799
 | pages = 131–134
 | title = Rooks inviolate
 | volume = 58
 | year = 1974}}.&lt;/ref&gt;

==Mathematical properties==
===Recurrence===
The telephone numbers satisfy the [[recurrence relation]]
:&lt;math&gt;T(n) = T(n-1) + (n-1)T(n-2),&lt;/math&gt;
first published in 1800 by [[Heinrich August Rothe]], by which they may easily be calculated.&lt;ref name="knuth"/&gt;
One way to explain this recurrence is to partition the {{math|''T''(''n'')}} connection patterns of the {{mvar|n}} subscribers to a telephone system into the patterns in which the first subscriber is not calling anyone else, and the patterns in which the first subscriber is making a call. There are {{math|''T''(''n''&amp;nbsp;−&amp;nbsp;1)}} connection patterns in which the first subscriber is disconnected, explaining the first term of the recurrence. If the first subscriber is connected to someone else, there are {{math|''n''&amp;nbsp;−&amp;nbsp;1}} choices for which other subscriber they are connected to, and {{math|''T''(''n''&amp;nbsp;−&amp;nbsp;2)}} patterns of connection for the remaining {{math|''n''&amp;nbsp;−&amp;nbsp;2}} subscribers, explaining the second term of the recurrence.&lt;ref name="chm"/&gt;

===Summation formula and approximation===
The telephone numbers may be expressed exactly as a [[summation]]
:&lt;math&gt;T(n) = \sum_{k=0}^{\lfloor n/2\rfloor}\binom{n}{2k}(2k-1)!! = \sum_{k=0}^{\lfloor n/2\rfloor}\frac{n!}{2^k (n-2k)! k!}.&lt;/math&gt;
In each term of this sum, {{mvar|k}} gives the number of matched pairs, the [[binomial coefficient]] &lt;math&gt;\binom{n}{2k}&lt;/math&gt; counts the number of ways of choosing the {{math|2''k''}} elements to be matched, and the [[double factorial]] {{math|1=(2''k'' &amp;minus; 1)!! = (2''k'')!/(2&lt;sup&gt;''k''&lt;/sup&gt;''k''!)}} is the product of the odd integers up to its argument and counts the number of ways of completely matching the {{math|2''k''}} selected elements.&lt;ref name="knuth"/&gt;&lt;ref name="chm"/&gt; It follows from the summation formula and [[Stirling's approximation]] that, [[Asymptotic analysis|asymptotically]], 
:&lt;math&gt;T(n) \sim \left(\frac{n}{e}\right)^{n/2} \frac{e^{\sqrt{n}}}{(4e)^{1/4}}\,.&lt;/math&gt;&lt;ref name="knuth"/&gt;&lt;ref name="chm"&gt;{{citation
 | last1 = Chowla | first1 = S. | author1-link = Sarvadaman Chowla
 | last2 = Herstein | first2 = I. N. | author2-link = Israel Nathan Herstein
 | last3 = Moore | first3 = W. K.
 | doi = 10.4153/CJM-1951-038-3
 | journal = [[Canadian Journal of Mathematics]]
 | mr = 0041849
 | pages = 328–334
 | title = On recursions connected with symmetric groups. I
 | volume = 3
 | year = 1951}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last1 = Moser | first1 = Leo | author1-link = Leo Moser
 | last2 = Wyman | first2 = Max
 | doi = 10.4153/CJM-1955-021-8
 | journal = [[Canadian Journal of Mathematics]]
 | mr = 0068564
 | pages = 159–168
 | title = On solutions of {{math|1=''x&lt;sup&gt;d&lt;/sup&gt;''&amp;nbsp;=&amp;nbsp;1}} in symmetric groups
 | volume = 7
 | year = 1955}}.&lt;/ref&gt;

===Generating function===
The [[exponential generating function]] of the telephone numbers is
:&lt;math&gt;\sum_{n=0}^{\infty}\frac{T(n)x^n}{n!}=\exp\left(\frac{x^2}{2}+x\right).&lt;/math&gt;&lt;ref name="chm"/&gt;&lt;ref name="gfgt"&gt;{{citation
 | last1 = Banderier | first1 = Cyril
 | last2 = Bousquet-Mélou | first2 = Mireille | author2-link = Mireille Bousquet-Mélou
 | last3 = Denise | first3 = Alain
 | last4 = Flajolet | first4 = Philippe | author4-link = Philippe Flajolet
 | last5 = Gardy | first5 = Danièle
 | last6 = Gouyou-Beauchamps | first6 = Dominique
 | arxiv = math/0411250
 | doi = 10.1016/S0012-365X(01)00250-3
 | issue = 1-3
 | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]
 | mr = 1884885
 | pages = 29–55
 | title = Generating functions for generating trees
 | volume = 246
 | year = 2002}}.&lt;/ref&gt;
In other words, the telephone numbers may be read off as the coefficients of the [[Taylor series]] of {{math|exp(''x''&lt;sup&gt;2&lt;/sup&gt;/2 + ''x'')}}, and the {{mvar|n}}th telephone number is the value at zero of the {{mvar|n}}th derivative of this function.
This function is closely related to the exponential generating function of the [[Hermite polynomial]]s, which are the [[matching polynomial]]s of the complete graphs.&lt;ref name="gfgt"/&gt;
The sum of absolute values of the coefficients of the {{mvar|n}}th (probabilist) Hermite polynomial is the {{mvar|n}}th telephone number, and the telephone numbers can also be realized as certain special values of the Hermite polynomials:&lt;ref name="sbdhp"/&gt;&lt;ref name="gfgt"/&gt;
:&lt;math&gt;T(n)=\frac{\mathit{He}_n(i)}{i^n}.&lt;/math&gt;

===Prime factors===
For large values of {{mvar|n}}, the {{mvar|n}}th telephone number is divisible by a large [[power of two]], {{math|2&lt;sup&gt;''n''/4 + ''O''(1)&lt;/sup&gt;}}.

More precisely, the [[p-adic order|2-adic order]] (the number of factors of two in the [[prime factorization]]) of {{math|''T''(4''k'')}} and of {{math|''T''(4''k'' + 1)}} is {{mvar|k}}; for {{math|''T''(4''k'' + 2)}} it is {{math|''k'' + 1}}, and for {{math|''T''(4''k'' + 3)}} it is {{math|''k'' + 2}}.&lt;ref&gt;{{citation
 | last1 = Kim | first1 = Dongsu
 | last2 = Kim | first2 = Jang Soo
 | doi = 10.1016/j.jcta.2009.08.002
 | issue = 8
 | journal = [[Journal of Combinatorial Theory]] | series = Series A
 | mr = 2677675
 | pages = 1082–1094
 | title = A combinatorial approach to the power of 2 in the number of involutions
 | volume = 117
 | year = 2010}}.&lt;/ref&gt;

For any prime number {{mvar|p}}, one can test whether there exists a telephone number divisible by {{mvar|p}} by computing the recurrence for the sequence of telephone numbers, modulo {{mvar|p}}, until either reaching zero or [[Cycle detection|detecting a cycle]]. The primes that divide at least one telephone number are
:2, 5, 13, 19, 23, 29, 31, 43, 53, 59, ... {{OEIS| A264737}}

==References==
{{reflist|colwidth=30em}}

[[Category:Integer sequences]]
[[Category:Enumerative combinatorics]]
[[Category:Factorial and binomial topics]]
[[Category:Matching]]
[[Category:Permutations]]</text>
      <sha1>6aov1mzf476qnunz9hgayjvmw3e2u74</sha1>
    </revision>
  </page>
  <page>
    <title>Ulam's game</title>
    <ns>0</ns>
    <id>32177562</id>
    <revision>
      <id>866101657</id>
      <parentid>830343637</parentid>
      <timestamp>2018-10-28T07:49:00Z</timestamp>
      <contributor>
        <username>Erel Segal</username>
        <id>7637243</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2383">{{for|Ulam's topological game where players alternate choosing binary digits|binary game}}
'''Ulam's game''', or the '''Rényi–Ulam game''', is a mathematical game similar to the popular game of [[twenty questions]] where one attempts to guess an unnamed object with [[yes–no question]]s, but where some of the answers may be wrong.&lt;ref&gt;{{cite web|title=How to Play Ulam's Game|url=http://math.iit.edu/~rellis/papers/9how.pdf|accessdate=13 June 2013}}&lt;/ref&gt;  {{harvs|txt|first=Alfréd|last=Rényi|authorlink=Alfréd Rényi|year=1961}} introduced the game, though his paper was overlooked for many years. Rényi [69] reported the following story about the Jew [[Bar Kochba]] in 135&amp;nbsp;CE, who defended his fortress against the Romans. It is also said that Bar Kochba sent out a scout to the Roman camp who was captured and tortured, having his tongue cut out. He escaped from captivity and reported back to Bar Kochba, but being unable to talk, he could not tell in words what he had seen. Bar Kochba accordingly asked him questions which he could answer by nodding or shaking his head. Thus he acquired from his mute scout the information he needed to defend the fortress. {{harvs|txt|first=Stanislaw|last=Ulam|authorlink=Stanislaw Ulam|year=1976|loc=p. 281}} rediscovered the game, presenting the idea that there are a million objects and the answer to one question can be wrong. {{harvtxt|Pelc|2002}} gave a survey of similar games and their relation to [[information theory]].

==References==
&lt;references /&gt;

*{{Citation | last1=Pelc | first1=Andrzej | title=Searching games with errors---fifty years of coping with liars | doi=10.1016/S0304-3975(01)00303-6 | mr=1871067 | year=2002 | journal=[[Theoretical Computer Science (journal)|Theoretical Computer Science]] | issn=0304-3975 | volume=270 | issue=1 | pages=71–109}}
*{{Citation | last1=Rényi | first1=Alfréd | title=On a problem in information theory | language=Hungarian | mr=0143666 | year=1961 | journal=Magyar Tud. Akad. Mat. Kutató Int. Közl. | volume=6 | pages=505–516}}
*{{Citation | last1=Ulam | first1=S. M. | title=Adventures of a mathematician | url=https://books.google.com/books?id=U2_zEZOHdU4C | publisher=Charles Scribner's sons | isbn=978-0-520-07154-4 | mr=0485098 | year=1976}}

{{mathematics-stub}}

[[Category:Mathematical games]]
[[Category:Information theory]]
[[Category:Guessing games]]</text>
      <sha1>tvwic6wxj3hvz0n8c944l1u0n1lfa96</sha1>
    </revision>
  </page>
  <page>
    <title>X-ray transform</title>
    <ns>0</ns>
    <id>18732683</id>
    <revision>
      <id>796104327</id>
      <parentid>791437642</parentid>
      <timestamp>2017-08-18T14:24:55Z</timestamp>
      <contributor>
        <username>Xsk8rat</username>
        <id>259788</id>
      </contributor>
      <minor/>
      <comment>changed "scattering" to "attenuating" as this more cl;osely represents the process of the tomographic scan.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3182">In [[mathematics]], the '''X-ray transform''' (also called '''John transform''') is an [[integral transform]] introduced by [[Fritz John]] in 1938&lt;ref&gt;{{cite journal|last=Fritz|first=John|title=The ultrahyperbolic differential equation with four independent variables|journal=Duke Mathematical Journal|year=1938|volume=4|pages=300–322|doi=10.1215/S0012-7094-38-00423-5|url=http://projecteuclid.org/euclid.dmj/1077490637|accessdate=23 January 2013}}&lt;/ref&gt; that is one of the cornerstones of modern [[integral geometry]]. It is very closely related to the [[Radon transform]], and coincides with it in two dimensions.  In higher dimensions, the X-ray transform of a function is defined by integrating over [[line (geometry)|line]]s rather than over [[hyperplane]]s as in the Radon transform.  The X-ray transform derives its name from X-ray [[tomography]] because the X-ray transform of a function ''&amp;fnof;'' represents the attenuation data of a tomographic scan through an inhomogeneous medium whose density is represented by the function ''&amp;fnof;''.  Inversion of the X-ray transform is therefore of practical importance because it allows one to reconstruct an unknown density ''&amp;fnof;'' from its known attenuation data.

In detail, if ''&amp;fnof;'' is a [[compact support|compactly supported]] [[continuous function]] on the [[Euclidean space]] '''R'''&lt;sup&gt;''n''&lt;/sup&gt;, then the X-ray transform of ''&amp;fnof;'' is the function ''X&amp;fnof;'' defined on the set of all lines in '''R'''&lt;sup&gt;''n''&lt;/sup&gt; by
:&lt;math&gt;Xf(L) = \int_L f = \int_{\mathbf{R}} f(x_0+t\theta)dt&lt;/math&gt;
where ''x''&lt;sub&gt;0&lt;/sub&gt; is an initial point on the line and θ is a unit vector giving the direction of the line ''L''.  The latter integral is not regarded in the oriented sense: it is the integral with respect to the 1-dimensional [[Lebesgue measure]] on the Euclidean line ''L''.

The X-ray transform satisfies an [[ultrahyperbolic wave equation]] called [[John's equation]].

The [[Gauss hypergeometric function]] can be written as an X-ray transform {{harv|Gelfand|Gindikin|Graev|2003|loc=2.1.2}}.

==References==
{{Reflist}}
*{{springer|first=Carlos A.|last=Berenstein|title=X-ray transform|id=X/x120030}}.
*{{Citation | last1=Gelfand | first1=I. M. | last2=Gindikin | first2=S. G. | last3=Graev | first3=M. I. | title=Selected topics in integral geometry | origyear=2000 | url=https://books.google.com/books?isbn=0821829327 | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Translations of Mathematical Monographs | isbn=978-0-8218-2932-5 | mr=2000133 | year=2003 | volume=220}}
*{{Citation | last1=Helgason | first1=Sigurdur | title=Geometric analysis on symmetric spaces | publisher=[[American Mathematical Society]] | location=Providence, R.I. | edition=2nd | series=Mathematical Surveys and Monographs | isbn=978-0-8218-4530-1 | mr=2463854 | year=2008 | volume=39}}
*{{Citation | last1=Helgason | first1=Sigurdur | url=http://www-math.mit.edu/~helgason/Radonbook.pdf | title=The Radon Transform | publisher=[[Birkhauser]] | location=Boston, M.A. | edition=2nd | series=Progress in Mathematics | year=1999}}

[[Category:Integral geometry]]
[[Category:Integral transforms]]</text>
      <sha1>9p66h8e8kofuazoqybefxtq0ayduttx</sha1>
    </revision>
  </page>
  <page>
    <title>Yurii Vladimirovich Egorov</title>
    <ns>0</ns>
    <id>50758843</id>
    <revision>
      <id>863197771</id>
      <parentid>848632910</parentid>
      <timestamp>2018-10-09T09:18:33Z</timestamp>
      <contributor>
        <username>JChMathae</username>
        <id>29105708</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5346">'''Yurii''' (or '''Yuri''') '''Vladimirovich Egorov''' (Юрий Владимирович Егоров, born 14 July 1938 in Moscow, died October 2018 in Toulouse) was a Russian-Soviet mathematician who specializes in differential equations.

==Biography==
In 1960 he completed his undergraduate studies at the Mechanics and Mathematics Faculty of [[Moscow State University]] (MSU). In 1963 from MSU he received his Ph.D. with the thesis "''Некоторые задачи теории оптимального управления в бесконечномерных пространствах''" ("Some Problems of Optimal Control Theory in Infinite-Dimensional Spaces"). In 1970 from MSU he received his Russian doctorate of sciences ([[Doctor nauk|Doctor Nauk]]) with thesis: "''О локальных свойствах псевдодифференциальных операторов главного типа''" ("Local Properties of Pseudodifferential Operators of Principal Type"). He was employed at MSU from 1961 to 1992, and  he was a full professor in the Department of Differential Equations of the Mechanics and Mathematics Faculty there from 1973 to 1992. Since 1992 he has been a professor of mathematics at [[Paul Sabatier University]] (Toulouse III).

Egorov's research deals with differential equations and applications in mathematical physics, spectral theory, and optimal control theory. In 1970 he was an Invited Speaker of the [[International Congress of Mathematicians|ICM]] in Nice.&lt;ref&gt;Egorov, Yu V. [http://www.mathunion.org/ICM/ICM1970.2/Main/icm1970.2.0717.0722.ocr.pdf "On the local solvability of pseudodifferential equations."] In ''Actes du Congrès International des Mathématiciens'', Tome 2, pp. 717–722. 1970.&lt;/ref&gt;

==Awards==
* 1981 — Lomonosov Memorial Prize (established in 1944) — for his series of publications on "Субэллиптические операторы и их применения к исследованию краевых задач" (Subelliptic operators and their applications to the study of boundary value problems)
* 1988 — USSR State Prize (with several co-authors) — for their series of publications (1958–1985) on "Исследования краевых задач для дифференциальных операторов и их приложения в математической физике" (Research on boundary value problems and their applications in mathematical physics)
* 1998 — Petrovsky Award (jointly with V. A. Kondratiev) for their series of publications on "Исследование спектра эллиптических операторов" (The study of the spectra of elliptic operators)

==Selected publications==

===Articles===
*[http://www.mathnet.ru/php/archive.phtml?wshow=paper&amp;jrnid=rm&amp;paperid=5554&amp;option_lang=eng "The canonical transformations of pseudodifferential operators."] ''Uspekhi Matematicheskikh Nauk'' 24, no. 5 (1969): 235–236. 
*[http://iopscience.iop.org/article/10.1070/RM1971v026n02ABEH003823/pdf "On the solubility of differential equations with simple characteristics."] Russian Mathematical Surveys 26, no. 2 (1971): 113.
*with Mikhail Aleksandrovich Shubin: [http://www.mathnet.ru/php/archive.phtml?wshow=paper&amp;jrnid=intf&amp;paperid=111&amp;option_lang=eng "Linear partial differential equations. Foundations of the classical theory."] Itogi Nauki i Tekhniki. Seriya" Sovremennye Problemy Matematiki. Fundamental'nye Napravleniya" 30 (1988): 5–255.
*[http://iopscience.iop.org/article/10.1070/RM1990v045n05ABEH002683/meta "A contribution to the theory of generalized functions."] Russian Mathematical Surveys 45, no. 5 (1990): 1.
*with Vladimir Aleksandrovich Kondrat'ev and [[Olga Oleinik|Olga Arsen'evna Oleynik]]: [http://www.turpion.org/php/paper.phtml?journal_id=sm&amp;paper_id=304  "Asymptotic behaviour of the solutions of non-linear elliptic and parabolic systems in tube domains."] Sbornik: Mathematics 189, no. 3 (1998): 359–382.
*Victor A. Galaktionov, Vladimir A. Kondratiev, and Stanislav I. Pohozaev: [http://www.sciencedirect.com/science/article/pii/S0764444200001245 "On the necessary conditions of global existence to a quasilinear inequality in the half-space."] Comptes Rendus de l'Académie des Sciences-Series I-Mathematics 330, no. 2 (2000): 93–98.

===Books===
*with Vladimir A. Kondratiev: {{cite book|title=On spectral theory of elliptic operators|location=Basel; Boston|publisher=Birkhäuser Verlag|year=1996|postscript=; x+328 pages|series=Operator theory, advances and applications ; vol. 89|url=https://books.google.com/books?id=3Mcv_DpgY_8C}}
*with Bert-Wolfgang Schulze: {{cite book|title=Pseudo-differential operators, singularities, applications|location=Basel; Boston|publisher=Birkhäuser Verlag|year=1997|postscript=; xiii+349 pages|series=Operator theory, advances and applications ; vol. 93|url=https://books.google.com/books/about/Pseudo_Differential_Operators_Singularit.html?id=dDsUz1cO4aYC}}

==References==
{{reflist}}

{{Authority control}}

{{DEFAULTSORT:Egorov, Yurii Vladimirovich}}
[[Category:Moscow State University alumni]]
[[Category:Moscow State University faculty]]
[[Category:Russian mathematicians]]
[[Category:Soviet mathematicians]]
[[Category:Mathematical analysts]]
[[Category:PDE theorists]]
[[Category:1938 births]]
[[Category:Living people]]</text>
      <sha1>o7tngn6gyok4k8y5btfgsd9omcdx53y</sha1>
    </revision>
  </page>
  <page>
    <title>Zygmunt Zawirski</title>
    <ns>0</ns>
    <id>14650127</id>
    <revision>
      <id>811376010</id>
      <parentid>741841036</parentid>
      <timestamp>2017-11-21T06:11:16Z</timestamp>
      <contributor>
        <username>Omnipaedista</username>
        <id>8524693</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2723">'''Zygmunt Zawirski''' (29 September 1882 – 2 April 1948) was a [[Poland|Polish]] [[philosopher]] and [[logician]].

His main field of study was [[philosophy of physics]], [[history of science]], [[multi-valued logic]] and relation of multi-valued logic to calculus of probability.

==Biography==
Zawirski was born on 29 September 1882 in the village of Berezowica Mała ([[Berzovytsia Mala]]) near [[Zbarazh]] (now [[Ukraine]]). In 1928 he became a professor of the [[Adam Mickiewicz University in Poznań]] and in 1937 professor of the [[Jagiellonian University]] in [[Kraków]]. In 1936 he became an editor of ''Kwartalnik Filozoficzny'' ("Philosophical Quarterly"). After 1945, he was president of the [[Krakowskie Towarzystwo Filozoficzne]] ("Kraków Philosophical Society").

He died on 2 April 1948 in [[Końskie]], Poland.

==Notable works==
* {{cite book
|last=—.
|title=L’évolution de la notion du temps
|year=1936
|language=French
|publisher=Gebethner et Wolff
|location=Cracovie, etc.
|oclc=10599561
}}

==References==
* {{cite web
|title=Zawirski Zygmunt
|work=[[Internetowa encyklopedia PWN]]
|url=http://encyklopedia.pwn.pl/haslo.php?id=4000699
|accessdate=2007-12-10
|language=Polish
|publisher=[[Wydawnictwo Naukowe PWN]]
}}
* {{cite web
|title=Zawirski Zygmunt
|work=[[WIEM Encyklopedia]]
|url=http://portalwiedzy.onet.pl/139,,,,zawirski_zygmunt,haslo.html
|language=Polish
|accessdate=2007-12-10
}}

==Further reading==
* {{cite book
|last=Piesko
|first=Maria
|title=Naukowa metafizyka Zygmunta Zawirskiego
|year=2004
|language=Polish
|publisher=OBI, Biblos
|location=Kraków, Tarnów
|isbn=83-7332-207-8
|oclc=69577155
}}
* {{cite book
|last=Szumilewicz-Lachman
|first=Irena
|title=Zygmunt Zawirski: His Life and Work. With Selected Writings on Time, Logic &amp; the Methodology of Science
|year=1994
|publisher=Kluwer Academic
|location=Boston
|isbn=0-7923-2566-4
|oclc=29223536
}}
* {{cite book
|last=Zawirski
|first=Zygmunt
|author2=Sepioło, Michał (ed.); Bednarczyk, Andrzej (ed.)
|title=O stosunku metafizyki do nauk
|year=2003
|language=Polish
|publisher=Wydział Filozofii i Socjologii Uniwersytetu Warszawskiego
|location=Warsaw
|isbn=83-87963-20-8
|oclc=69528304
}}

{{Authority control}}

{{DEFAULTSORT:Zawirski, Zygmunt}}
[[Category:1882 births]]
[[Category:1948 deaths]]
[[Category:Jagiellonian University faculty]]
[[Category:Historians of science]]
[[Category:Mathematical logicians]]
[[Category:Adam Mickiewicz University in Poznań faculty]]
[[Category:Polish historians]]
[[Category:Polish logicians]]
[[Category:Polish philosophers]]
[[Category:People from Zbarazh Raion]]
[[Category:19th-century philosophers]]
[[Category:20th-century historians]]


{{Poland-mathematician-stub}}</text>
      <sha1>73ju6x8oixfbp345garcety740bwfka</sha1>
    </revision>
  </page>
</mediawiki>
