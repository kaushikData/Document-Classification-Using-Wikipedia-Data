<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>233 (number)</title>
    <ns>0</ns>
    <id>2959451</id>
    <revision>
      <id>832584247</id>
      <parentid>823313795</parentid>
      <timestamp>2018-03-26T21:16:29Z</timestamp>
      <contributor>
        <username>DePiep</username>
        <id>199625</id>
      </contributor>
      <minor/>
      <comment>save text from invisible. replace SloanesRef: use {{Cite OEIS}} (via [[WP:JWB]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1127">{{Infobox number
| number = 233
| prime = yes
}}
'''233''' ('''two hundred [and] thirty-three''') is the [[natural number]] following [[232 (number)|232]] and preceding [[234 (number)|234]].

233 is a [[prime number]],&lt;ref&gt;{{Cite OEIS|A000040|name=The prime numbers}}&lt;/ref&gt;
233 is a [[Sophie Germain prime]],&lt;ref&gt;{{Cite OEIS|A005384|name=Sophie Germain primes p: 2p+1 is also prime}}&lt;/ref&gt;
a [[Pillai prime]],&lt;ref&gt;{{Cite OEIS|A063980|name=Pillai primes}}&lt;/ref&gt;
and a [[Ramanujan prime]].&lt;ref&gt;{{Cite OEIS|A104272|name=Ramanujan primes}}&lt;/ref&gt;
It is a [[Fibonacci number]],&lt;ref&gt;{{Cite OEIS|A000045|name=Fibonacci numbers}}&lt;/ref&gt;
one of the [[Fibonacci prime]]s.&lt;ref&gt;{{Cite OEIS|A005478|name=Prime Fibonacci numbers}}&lt;/ref&gt;

There are exactly 233 [[maximal planar graph]]s with ten vertices,&lt;ref&gt;{{Cite OEIS|A000109|name=Number of simplicial polyhedra with n nodes}}&lt;/ref&gt;
and 233 connected [[topological space]]s with four points.&lt;ref&gt;{{Cite OEIS|A001929|name=Number of connected topologies on n labeled points}}&lt;/ref&gt;

==References==
{{reflist}}

{{Integers|2}}

{{DEFAULTSORT:233 (Number)}}
[[Category:Integers]]


{{Num-stub}}</text>
      <sha1>qkhxdo7idxt4t79aubrkub736lf83vu</sha1>
    </revision>
  </page>
  <page>
    <title>4D vector</title>
    <ns>0</ns>
    <id>50761824</id>
    <revision>
      <id>813823470</id>
      <parentid>780332714</parentid>
      <timestamp>2017-12-05T12:18:24Z</timestamp>
      <contributor>
        <username>WereSpielChequers</username>
        <id>4071608</id>
      </contributor>
      <minor/>
      <comment>/* Software support */[[WP:AWB/T|Typo fixing]], replaced: Theses →  These using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4407">{{distinguish|four-vector}}
In [[computer science]], a '''4D vector''' is a 4-component [[Vector (mathematics)|vector]] [[data type]]. Uses include [[homogeneous coordinates]] for [[3-dimensional space]] in [[computer graphics]], and ''red green blue alpha'' ([[RGBA]]) values for [[bitmap]] images with a color and [[alpha channel]] (as such they are widely used in computer graphics). They may also represent [[quaternion]]s (useful for rotations) although the algebra they define is different.

== Computer hardware support ==
&lt;!-- Todo: some earlier workstations must have had similar vector engines. --&gt;
Some [[microprocessor]]s have hardware support for 4D vectors with instructions dealing with 4 [[SIMD lane|lane]] ''single instruction, multiple data'' ([[SIMD]]) instructions, usually with a [[128-bit]] data path and [[32-bit floating point]] fields.&lt;ref&gt;{{cite web|title=intel SSE intrinsics|url=https://software.intel.com/sites/landingpage/IntrinsicsGuide/#techs=SSE}}&lt;/ref&gt;

Specific instructions (e.g., 4 element [[dot product]]) may facilitate the use of one 128-bit register to represent a 4D vector. For example, in chronological order: [[Hitachi SH4]], [[PowerPC]] VMX128 extension,&lt;ref&gt;{{cite web|title=Putting It All Together: Anatomy of the XBox 360 Game Console (see VMX128 dot product)|url=https://www.cis.upenn.edu/~milom/cis371-Spring11/lectures/15_xbox.pdf}}&lt;/ref&gt; and Intel [[x86]] SSE4.&lt;ref&gt;{{cite web|title=intel SSE4 dot product|url=https://software.intel.com/en-us/node/583111}}&lt;/ref&gt;

Some 4-element vector engines (e.g., the [[PS2 vector unit]]s) went further with the ability to broadcast components as multiply sources, and [[cross product]] support.&lt;ref&gt;{{cite web|title=VU0 user manual|url=https://www.dropbox.com/s/e3lsv80kb1jb6sh/VU0E.PDF}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=feasibility study on using the playstation 2 for scientific computing|url=http://gamehacking.org/faqs/VPUThesis.pdf}}&lt;/ref&gt;  Earlier generations of [[graphics processing unit]] (GPU) shader pipelines used ''[[very long instruction word]]'' (VLIW) [[instruction set]]s tailored for similar operations.

== Software support ==
SIMD use for 4D vectors can be conveniently wrapped in a ''[[vector maths library]]'' (commonly implemented in [[C (programming language)|C]] or [[C++]])&lt;ref&gt;{{cite web|title=sce vectormath|url=https://github.com/erwincoumans/sce_vectormath}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=GLM (vector maths library)|url=http://glm.g-truc.net/0.9.7/index.html}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Microsoft DirectX Maths|url=https://msdn.microsoft.com/en-us/library/windows/desktop/ee415652(v=vs.85).aspx}}&lt;/ref&gt; 
commonly used in [[video game development]], along with [[4×4 matrix]] support. These are distinct from more general [[linear algebra libraries]] in other domains focussing on [[Matrix (mathematics)|matrices]] of arbitrary size. Such libraries sometimes support 3D vectors padded to 4D or loading 3D data into 4D registers, with arithmetic mapped efficiently to SIMD operations by per platform [[intrinsic function]] implementations. There is choice between [[AOS and SOA]] approaches given the availability of 4 element registers, versus SIMD instructions that are usually tailored toward homogenous data.

[[Shading language]]s for [[graphics processing unit]] (GPU) programming usually have a 4D datatypes (along with 2D, 3D) with x-y-z-w accessors including ''[[Permutation|permutes]]'' or ''swizzle'' access, e.g., allowing easy swapping of RGBA or ARGB formats, accessing two 2D vectors packed into one 4D vector, etc.&lt;ref&gt;{{cite web|title=GLSL data types &amp; swizzling|url=https://www.opengl.org/wiki/Data_Type_(GLSL)#Swizzling}}&lt;/ref&gt; Modern GPUs have since moved to scalar [[single instruction, multiple threads]] (SIMT) pipelines (for more efficiency in ''[[general-purpose computing on graphics processing units]]'' (GPGPU)) but still support this programming model.&lt;ref&gt;{{cite web|title=AMD graphics core next|url=http://www.anandtech.com/show/4455/amds-graphics-core-next-preview-amd-architects-for-compute/4}}&lt;/ref&gt;

== See also ==
* [[Euclidean space]]
* [[Four-dimensional space]]
* [[Quaternion]]
* [[Dimension]]
* [[RGBA color space]]
* [[Tesseract]]
* [[4×4 matrix]]

==References==
{{Reflist|30em}}

{{DEFAULTSORT:4D Vector}}
[[Category:Concepts in physics]]
[[Category:Mathematical structures]]
[[Category:Vectors (mathematics and physics)]]
[[Category:Vector spaces]]</text>
      <sha1>dz9zqvl1n8ue7h364ft2up3qoxiqf1o</sha1>
    </revision>
  </page>
  <page>
    <title>Asymptotology</title>
    <ns>0</ns>
    <id>19216612</id>
    <revision>
      <id>858193670</id>
      <parentid>858191679</parentid>
      <timestamp>2018-09-05T17:47:21Z</timestamp>
      <contributor>
        <username>TheSeven</username>
        <id>2480707</id>
      </contributor>
      <minor/>
      <comment>/* Asymptotic uncertainty principle */  copyedits</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10336">'''Asymptotology''' has been defined as “the art of dealing with applied mathematical systems in [[limiting case (mathematics)|limiting case]]s”&lt;ref&gt;Kruskal M.D., Asymptotology, in ''Mathematical Models in [[Physical science|Physical Sciences]]'' (eds. S. Drobot and P. A. Viebrock), Proceedings of the conference at the University of Notre Dame, 1962, (Prentice-Hall, Englewood Cliffs, N.J., 1963) 17-48. [http://w3.pppl.gov/~hammett/work/2009/Kruskal62_Asymptotology_MATT160_ocr.pdf (preprint version)]
&lt;/ref&gt; as well as “the science about the synthesis of simplicity and exactness by means of localization.&lt;ref&gt;Barantsev R.G. Asymptotic versus classical mathematics // Topics in Math. Analysis.  Singapore e.a.: 1989, 49–64.&lt;/ref&gt;

==Principles==

The field of [[asymptotics]] is normally first encountered in school [[geometry]] with the introduction of the [[asymptote]], a line to which a curve tends at infinity.  The word Ασύμπτωτος (asymptotos) in Greek means non-coincident and puts strong emphasis on the point that approximation does not turn into coincidence.  It is a salient feature of asymptotics, but this property alone does not entirely cover the idea of [[asymptotics]] and, etymologically, the term seems to be quite insufficient.

==Perturbation theory, small and large parameters==

In [[physics]] and other fields of [[science]], one frequently comes across problems of an asymptotic nature, such as damping, orbiting, [[asymptotic stability|stabilization]] of a perturbed motion, etc.  Their solutions lend themselves to [[asymptotic analysis]] ([[perturbation theory]]), which is widely used in modern [[applied mathematics]], [[mechanics]] and [[physics]].  But asymptotic methods put a claim on being more than a part of classical mathematics.  [[K. Friedrichs]] said: “Asymptotic description is not only a convenient tool in the mathematical analysis of nature, it has some more fundamental significance”. [[Martin Kruskal|M. Kruskal]] introduced the special term asymptotology, defined above, and called for a formalization of the accumulated experience to convert the art of asymptotology to a science.
	A general term is capable of possessing significant heuristic value.  In his essay "The Future of Mathematics",&lt;ref&gt;[http://www-groups.dcs.st-and.ac.uk/~history/Extras/Poincare_Future.html The Future of Mathematics]&lt;/ref&gt; [[H. Poincaré]] wrote the following.
{{quote| text=
The invention of a new word will often be sufficient to bring out the relation, and the word will be creative.... It is hardly possible to believe what economy of thought, as Mach used to say, can be effected by a well-chosen term.... Mathematics is the art of giving the same name to different things.... When language has been well chosen, one is astonished to find that all demonstrations made for a known object apply immediately to many new objects: nothing requires to be changed, not even the terms, since the names have become the same.... The bare fact, then, has sometimes no great interest ... it only acquires a value when some more careful thinker perceives the connection it brings out, and symbolizes it by a term.
}}

In addition, “the success of ‘[[cybernetics]]’, ‘[[attractor]]s’ and ‘[[catastrophe theory]]’ illustrates the fruitfulness of word creation as scientific research”.&lt;ref&gt;V. Arnol’d&lt;/ref&gt;

Almost every physical theory, formulated in the most general manner, is rather difficult from a mathematical point of view.  Therefore, both at the genesis of the theory and its further development, the simplest limiting cases, which allow analytical solutions, are of particular importance.  In those limits, the number of equations usually decreases, their order reduces, nonlinear equations can be replaced by linear ones, the initial system becomes averaged in a certain sense, and so on.

All these idealizations, different as they may seem, increase the degree of symmetry of the mathematical model of the phenomenon under consideration.

==Asymptotic approach==

In essence, the asymptotic approach to a complex problem consists in treating the insufficiently symmetrical governing system as close to a certain symmetrical one as possible.

In attempting to obtain a better approximation of the exact solution to the given problem, it is crucial that the determination of corrective solutions, which depart from the limit case, be much simpler than directly investigating the governing system.  At first sight, the possibilities of such an approach seem restricted to varying the parameters determining the system only within a narrow range.  However, experience in the investigation of different physical problems shows that if the system’s parameters have changed sufficiently and the system has deviated far from the symmetrical limit case, another limit system, often with less obvious symmetries can be found, to which an asymptotic analysis is also applicable.  This allows one to describe the system’s behavior on the basis of a small number of limit cases over the whole range of parameter variations.  Such an approach corresponds to the maximum level of intuition, promotes further insights, and eventually leads to the formulation of new physical concepts.  It is also important that [[asymptotic methods]] help to establish the connection between different physical theories.
The aim of the asymptotic approach is to simplify the object.  This simplification is attained by decreasing the vicinity of the singularity under consideration.  It is typical that the accuracy of asymptotic expansions grows with localization.  Exactness and simplicity are commonly regarded as mutually exclusive notions.  When tending to simplicity, we sacrifice exactness, and trying to achieve exactness, we expect no simplicity.  Under localization, however, the antipodes converge; the contradiction is resolved in a synthesis called asymptotics.  In other words, simplicity and exactness are coupled by an “uncertainty principle” relation while the domain size serves as a small parameter&amp;nbsp;– a measure of uncertainty.

==Asymptotic uncertainty principle==

Let us illustrate the “asymptotic uncertainty principle”.  Take the expansion of the function &lt;math&gt;f(x)&lt;/math&gt; in an asymptotic sequence &lt;math&gt;{\phi_n(x)}&lt;/math&gt;: &lt;br /&gt;
&lt;math&gt;f(x) = \sum_{n=0}^{\infty} a_n \phi_n(x)&lt;/math&gt;, &lt;math&gt;x&lt;/math&gt; → &lt;math&gt;0&lt;/math&gt;.

A partial sum of the series is designated by &lt;math&gt;S_N(x)&lt;/math&gt;, and the exactness of approximation at a given &lt;math&gt;N&lt;/math&gt; is estimated by &lt;math&gt;\Delta_N(x) = |f(x) - S_N(x)|&lt;/math&gt;.  Simplicity is characterized here by the number &lt;math&gt;N&lt;/math&gt; and the locality by the length of interval &lt;math&gt;x&lt;/math&gt;.

Based on known properties of the [[asymptotic expansion]], we consider the pair wise interrelation of values &lt;math&gt;x&lt;/math&gt;, &lt;math&gt;N&lt;/math&gt;, and &lt;math&gt;\Delta&lt;/math&gt;.  At a fixed &lt;math&gt;x&lt;/math&gt; the expansion initially converges, i.e., the exactness increases at the cost of simplicity.  If we fix &lt;math&gt;N&lt;/math&gt;, the exactness and the interval size begin to compete.  The smaller the interval, the given value of &lt;math&gt;\Delta&lt;/math&gt; is reached more simply.

We illustrate these regularities using a simple example.  Consider the exponential integral function: &lt;br /&gt;
&lt;math&gt;\operatorname{Ei}(y) = \int_{-\infty}^y e^{\zeta} \zeta^{-1} d{\zeta},  y &lt; 0&lt;/math&gt;.

Integrating by parts, we obtain the following asymptotic expansion &lt;br /&gt;
&lt;math&gt;\operatorname{Ei}(y) \sim e^y \sum_{n=1}^{\infty} (n-1)! y^{-n}, \; y&lt;/math&gt; → &lt;math&gt;-\infty&lt;/math&gt;.

Put &lt;math&gt;f(x) = -e-y \operatorname{Ei}(y)&lt;/math&gt;, &lt;math&gt;y = -x-1&lt;/math&gt;. Calculating the partial sums of this series and the values &lt;math&gt;\Delta_N(x)&lt;/math&gt; and &lt;math&gt;f(x)&lt;/math&gt; for different &lt;math&gt;x&lt;/math&gt; yields:

  &lt;math&gt;x&lt;/math&gt;	&lt;math&gt;f(x)&lt;/math&gt;     &lt;math&gt;\Delta_1&lt;/math&gt;      &lt;math&gt;\Delta_2&lt;/math&gt;      &lt;math&gt;\Delta_3&lt;/math&gt;      &lt;math&gt;\Delta_4&lt;/math&gt;      &lt;math&gt;\Delta_5&lt;/math&gt;	&lt;math&gt;\Delta_6&lt;/math&gt;      &lt;math&gt;\Delta_7&lt;/math&gt;
  1/3	0.262	0.071	0.040	0.034	0.040	0.060	0.106	0.223
  1/5	0.171	0.029	0.011	0.006	0.004	0.0035	0.0040	0.0043
  1/7	0.127	0.016	0.005	0.002	0.001	0.0006	0.0005	0.0004

Thus, at a given &lt;math&gt;x&lt;/math&gt;, the exactness first increases with the growth of &lt;math&gt;N&lt;/math&gt; and then decreases (so one has an asymptotic expansion).  For a given &lt;math&gt;N&lt;/math&gt;, one may observe an improvement of exactness with diminishing &lt;math&gt;x&lt;/math&gt;.

Finally, is it worth using asymptotic analysis if [[computers]] and [[numerical methods]] have reached such an advanced state?  As [[David Crighton|D. G. Crighton]] has mentioned,&lt;ref&gt;Crighton, D. G., "Asymptotics&amp;nbsp;– an indispensible complement to thought, computation and experiment in Applied Mathematical modelling".  In ''Proceedings of the Seventh Eur. Conf. Math. in Industry (March 2–6, 1993, Montecatini Terme)''. A.Fasano, M.Primicerio (eds.) Stuttgart: B.G. Teubner, 3-19.&lt;/ref&gt;

{{quote| text= Design of computational or experimental schemes without the guidance of asymptotic information is wasteful at best, dangerous at worst, because of the possible failure to identify crucial (stiff) features of the process and their localization in coordinate and parameter space.  Moreover, all experience suggests that asymptotic solutions are useful numerically far beyond their nominal range of validity, and can often be used directly, at least at a preliminary product design stage, for example, saving the need for accurate computation until the final design stage where many variables have been restricted to narrow ranges.}}

==Notes==
{{Reflist}}

==References==
* Andrianov I.V., Manevitch L.I. ''Asymptotology: Ideas, Methods, and Applications''. [[Kluwer Academic Publishers]], 2002.
* Dewar R.L. "Asymptotology&amp;nbsp;– a cautionary tale", ''[[ANZIAM Journal]]'', 2002, 44, 33–40. {{doi|10.1017/S1446181100007884}}
* Friedrichs K.O. "Asymptotic phenomena in mathematical physics", ''[[Bulletin of the American Mathematical Society]]'', 1955, 61, 485–504.
* Segel L.A. "The importance of asymptotic analysis in Applied Mathematics", ''[[American Mathematical Monthly]]'', 1966, 73, 7–14.
* White R.B. ''Asymptotic Analysis of Differential Equations'', Revised Edition, London: [[Imperial College Press]], 2010.

[[Category:Applied mathematics]]
[[Category:Asymptotic analysis]]</text>
      <sha1>kgjd277sjvtdobirfuvncjhebwhaba0</sha1>
    </revision>
  </page>
  <page>
    <title>BCH code</title>
    <ns>0</ns>
    <id>40779</id>
    <revision>
      <id>858916828</id>
      <parentid>858650629</parentid>
      <timestamp>2018-09-10T13:58:59Z</timestamp>
      <contributor>
        <ip>38.21.29.83</ip>
      </contributor>
      <comment>Undid revision 858650629 by [[Special:Contributions/14.139.155.214|14.139.155.214]] ([[User talk:14.139.155.214|talk]]); no justification was provided for deletion of article content; possible vandalism.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="43868">In [[coding theory]], the '''BCH codes''' or '''Bose&amp;ndash;Chaudhuri&amp;ndash;Hocquenghem codes''' form a class of [[cyclic code|cyclic]] [[Error detection and correction|error-correcting codes]] that are constructed using [[polynomial]]s over a [[finite field]] (also called ''[[Finite field|Galois field]]''). BCH codes were invented in 1959 by French mathematician [[Alexis Hocquenghem]], and independently in 1960 by [[Raj Chandra Bose|Raj Bose]] and [[D.K. Ray-Chaudhuri|D. K. Ray-Chaudhuri]].&lt;ref&gt;{{Harvnb|Reed|Chen|1999|p=189}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Hocquenghem|1959}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Bose|Ray-Chaudhuri|1960}}&lt;/ref&gt; The name ''Bose&amp;ndash;Chaudhuri&amp;ndash;Hocquenghem'' (and the acronym ''BCH'') arises from the initials of the inventors' surnames (mistakenly, in the case of Ray-Chaudhuri).

One of the key features of BCH codes is that during code design, there is a precise control over the number of symbol errors correctable by the code. In particular, it is possible to design binary BCH codes that can correct multiple bit errors. Another advantage of BCH codes is the ease with which they can be decoded, namely, via an [[Abstract algebra|algebraic]] method known as [[syndrome decoding]]. This simplifies the design of the decoder for these codes, using small low-power electronic hardware.

BCH codes are used in applications such as satellite communications,&lt;ref&gt;{{cite web|title=Phobos Lander Coding System: Software and Analysis|url=http://ipnpr.jpl.nasa.gov/progress_report/42-94/94V.PDF|accessdate=25 February 2012}}&lt;/ref&gt; [[compact disc]] players, [[DVD]]s, [[Disk storage|disk drives]], [[solid-state drive]]s&lt;ref&gt;{{cite web|title=Sandforce SF-2500/2600 Product Brief|url=http://www.sandforce.com/index.php?id=133&amp;parentId=2&amp;top=1|accessdate=25 February 2012}}&lt;/ref&gt; and [[Bar codes|two-dimensional bar codes]].

== Definition and illustration ==

=== Primitive narrow-sense BCH codes ===
Given a [[prime number]] {{mvar|q}} and [[prime power]] {{math|''q''&lt;sup&gt;''m''&lt;/sup&gt;}} with positive integers {{mvar|m}} and {{mvar|d}} such that {{math|''d'' ≤ ''q''&lt;sup&gt;''m''&lt;/sup&gt; − 1}}, a primitive narrow-sense BCH code over the [[finite field]] (or Galois field) {{math|GF(''q'')}} with code length {{math|''n'' {{=}} ''q''&lt;sup&gt;''m''&lt;/sup&gt; − 1}} and [[Block code#The distance d|distance]] at least {{mvar|d}} is constructed by the following method.

Let {{mvar|α}} be a [[Primitive element (finite field)|primitive element]] of {{math|GF(''q''&lt;sup&gt;''m''&lt;/sup&gt;)}}.
For any positive integer {{mvar|i}}, let {{math|''m''&lt;sub&gt;''i''&lt;/sub&gt;(''x'')}} be the [[minimal polynomial (field theory)|minimal polynomial]] with coefficients in {{math|GF(''q'')}} of {{math|α&lt;sup&gt;''i''&lt;/sup&gt;}}.
The [[generator polynomial]] of the BCH code is defined as the [[least common multiple]] {{math|''g''(''x'') {{=}} lcm(''m''&lt;sub&gt;1&lt;/sub&gt;(''x''),…,''m''&lt;sub&gt;''d'' − 1&lt;/sub&gt;(''x''))}}.
It can be seen that {{math|''g''(''x'')}} is a polynomial with coefficients in {{math|GF(''q'')}} and divides {{math|''x''&lt;sup&gt;''n''&lt;/sup&gt; − 1}}.
Therefore, the polynomial code defined by {{math|''g''(''x'')}} is a cyclic code.

==== Example ====
Let {{math|''q''{{=}}2}} and {{math|''m''{{=}}4}} (therefore {{math|''n''{{=}}15}}). We will consider different values of {{mvar|d}}. For {{math|GF(16) {{=}} GF(2&lt;sup&gt;4&lt;/sup&gt;)}} based on the polynomial {{math|x&lt;sup&gt;4&lt;/sup&gt;+x+1}} with primitive root {{math|α{{=}}x+0}} there are minimum polynomials {{math|m&lt;sub&gt;i&lt;/sub&gt;(x)}} with coefficients in {{math|GF(2)}} satisfying
:&lt;math&gt;m_i(\alpha^i) \bmod (x^4+x+1) = 0.&lt;/math&gt;
The minimal polynomials of the fourteen powers of {{math|α}} are
:&lt;math&gt;m_1(x) = m_2(x) = m_4(x) = m_8(x) = x^4+x+1,\,&lt;/math&gt;
:&lt;math&gt;m_3(x) = m_6(x) = m_9(x) = m_{12}(x) =x^4+x^3+x^2+x+1,\,&lt;/math&gt;
:&lt;math&gt;m_5(x) = m_{10}(x) = x^2+x+1,\,&lt;/math&gt;
:&lt;math&gt;m_7(x) = m_{11}(x) = m_{13}(x) = m_{14}(x) = x^4+x^3+1.\,&lt;/math&gt;

The BCH code with &lt;math&gt;d=2,3&lt;/math&gt; has generator polynomial

&lt;math&gt;g(x) = {\rm lcm}(m_1(x), m_2(x)) = m_1(x) = x^4+x+1.\,&lt;/math&gt;

It has minimal [[Hamming distance]] at least 3 and corrects up to one error. Since the generator polynomial is of degree 4, this code has 11 data bits and 4 checksum bits.

The BCH code with &lt;math&gt;d=4,5&lt;/math&gt; has generator polynomial

&lt;math&gt;
\begin{align}
g(x) &amp;{} = {\rm lcm}(m_1(x),m_2(x),m_3(x),m_4(x)) = m_1(x) m_3(x) \\
&amp; {} = (x^4+x+1)(x^4+x^3+x^2+x+1) = x^8+x^7+x^6+x^4+1.
\end{align}
&lt;/math&gt;

It has minimal Hamming distance at least 5 and corrects up to two errors. Since the generator polynomial is of degree 8, this code has 7 data bits and 8 checksum bits.

The BCH code with &lt;math&gt;d=6,7&lt;/math&gt; has generator polynomial

&lt;math&gt;
\begin{align}
g(x) &amp; {} = {\rm lcm}(m_1(x),m_2(x),m_3(x),m_4(x),m_5(x),m_6(x)) = m_1(x) m_3(x) m_5(x) \\
&amp; {} = (x^4+x+1)(x^4+x^3+x^2+x+1)(x^2+x+1) = x^{10}+x^8+x^5+x^4+x^2+x+1.
\end{align}
&lt;/math&gt;

It has minimal Hamming distance at least 7 and corrects up to three errors. Since the generator polynomial is of degree 10, this code has 5 data bits and 10 checksum bits. (This particular generator polynomial has a real-world application, in the format patterns of the [[QR code]].)

The BCH code with &lt;math&gt;d=8&lt;/math&gt; and higher has generator polynomial

&lt;math&gt;
\begin{align}
g(x) &amp; {} = {\rm lcm}(m_1(x),m_2(x),...,m_{14}(x))  = m_1(x) m_3(x) m_5(x) m_7(x)\\
&amp; {} = (x^4+x+1)(x^4+x^3+x^2+x+1)(x^2+x+1)(x^4+x^3+1) = x^{14}+x^{13}+x^{12}+\cdots+x^2+x+1.
\end{align}
&lt;/math&gt;

This code has minimal Hamming distance 15 and corrects 7 errors. It has 1 data bit and 14 checksum bits. In fact, this code has only two codewords: 000000000000000 and 111111111111111.

=== General BCH codes ===
General BCH codes differ from primitive narrow-sense BCH codes in two respects.

First, the requirement that &lt;math&gt;\alpha&lt;/math&gt; be a primitive element of &lt;math&gt;\mathrm{GF}(q^m)&lt;/math&gt; can be relaxed. By relaxing this requirement, the code length changes from &lt;math&gt;q^m - 1&lt;/math&gt; to &lt;math&gt;\mathrm{ord}(\alpha),&lt;/math&gt; the [[Order (group theory)|order]] of the element &lt;math&gt;\alpha.&lt;/math&gt;

Second, the consecutive roots of the generator polynomial may run from &lt;math&gt;\alpha^c,\ldots,\alpha^{c+d-2}&lt;/math&gt; instead of &lt;math&gt;\alpha,\ldots,\alpha^{d-1}.&lt;/math&gt;

'''Definition.''' Fix a finite field &lt;math&gt;GF(q),&lt;/math&gt; where &lt;math&gt;q&lt;/math&gt; is a prime power. Choose positive integers &lt;math&gt;m,n,d,c&lt;/math&gt; such that &lt;math&gt;2\leq d\leq n,&lt;/math&gt; &lt;math&gt;{\rm gcd}(n,q)=1,&lt;/math&gt;  and &lt;math&gt;m&lt;/math&gt; is the [[multiplicative order]] of &lt;math&gt;q&lt;/math&gt; modulo &lt;math&gt;n.&lt;/math&gt;

As before, let &lt;math&gt;\alpha&lt;/math&gt; be a [[primitive nth root of unity|primitive &lt;math&gt;n&lt;/math&gt;th root of unity]] in &lt;math&gt;GF(q^m),&lt;/math&gt; and let &lt;math&gt;m_i(x)&lt;/math&gt; be the [[minimal polynomial (field theory)|minimal polynomial]] over &lt;math&gt;GF(q)&lt;/math&gt; of &lt;math&gt;\alpha^i&lt;/math&gt; for all &lt;math&gt;i.&lt;/math&gt;
The generator polynomial of the BCH code is defined as the [[least common multiple]] &lt;math&gt;g(x) = {\rm lcm}(m_c(x),\ldots,m_{c+d-2}(x)).&lt;/math&gt;

'''Note:''' if &lt;math&gt;n=q^m-1&lt;/math&gt; as in the simplified definition, then &lt;math&gt;{\rm gcd}(n,q)&lt;/math&gt; is 1, and the order of &lt;math&gt;q&lt;/math&gt; modulo &lt;math&gt;n&lt;/math&gt; is &lt;math&gt;m.&lt;/math&gt;
Therefore, the simplified definition is indeed a special case of the general one.

=== Special cases ===
* A BCH code with &lt;math&gt;c=1&lt;/math&gt; is called a ''narrow-sense BCH code''.
* A BCH code with &lt;math&gt;n=q^m-1&lt;/math&gt; is called ''primitive''.

The generator polynomial &lt;math&gt;g(x)&lt;/math&gt; of a BCH code has coefficients from &lt;math&gt;\mathrm{GF}(q).&lt;/math&gt;
In general, a cyclic code over &lt;math&gt;\mathrm{GF}(q^p)&lt;/math&gt; with &lt;math&gt;g(x)&lt;/math&gt; as the generator polynomial is called a BCH code over &lt;math&gt;\mathrm{GF}(q^p).&lt;/math&gt;
The BCH code over &lt;math&gt;\mathrm{GF}(q^m)&lt;/math&gt; and generator polynomial &lt;math&gt;g(x)&lt;/math&gt; with successive powers of &lt;math&gt;\alpha&lt;/math&gt; as roots is one type of [[Reed–Solomon code]] where the decoder (syndromes) alphabet is the same as the channel (data and generator polynomial) alphabet, all elements of &lt;math&gt;\mathrm{GF}(q^m)&lt;/math&gt; .&lt;ref&gt;{{Harvnb|Gill|n.d.|p=3}}&lt;/ref&gt; The other type of Reed Solomon code is an [[Reed%E2%80%93Solomon_error_correction#Reed_&amp;_Solomon's_original_view:_The_codeword_as_a_sequence_of_values | original view Reed Solomon code]] which is not a BCH code.

== Properties ==

The generator polynomial of a BCH code has degree at most &lt;math&gt;(d-1)m&lt;/math&gt;. Moreover, if &lt;math&gt;q=2&lt;/math&gt; and &lt;math&gt;c=1&lt;/math&gt;, the generator polynomial has degree at most &lt;math&gt;dm/2&lt;/math&gt;.
{{Collapse top|title=Proof}}
Each minimal polynomial &lt;math&gt;m_i(x)&lt;/math&gt; has degree at most &lt;math&gt;m&lt;/math&gt;.
Therefore, the least common multiple of &lt;math&gt;d-1&lt;/math&gt; of them has degree at most &lt;math&gt;(d-1)m&lt;/math&gt;.
Moreover, if &lt;math&gt;q=2,&lt;/math&gt; then &lt;math&gt;m_i(x) = m_{2i}(x)&lt;/math&gt; for all &lt;math&gt;i&lt;/math&gt;.
Therefore, &lt;math&gt;g(x)&lt;/math&gt; is the least common multiple of at most &lt;math&gt;d/2&lt;/math&gt; minimal polynomials &lt;math&gt;m_i(x)&lt;/math&gt; for odd indices &lt;math&gt;i,&lt;/math&gt; each of degree at most &lt;math&gt;m&lt;/math&gt;.
{{Collapse bottom}}

A BCH code has minimal Hamming distance at least &lt;math&gt;d&lt;/math&gt;.
{{Collapse top|title=Proof}}
Suppose that &lt;math&gt;p(x)&lt;/math&gt; is a code word with fewer than &lt;math&gt;d&lt;/math&gt; non-zero terms. Then

: &lt;math&gt;p(x) = b_1x^{k_1} + \cdots + b_{d-1}x^{k_{d-1}},\text{ where }k_1&lt;k_2&lt;\cdots&lt;k_{d-1}.&lt;/math&gt;

Recall that &lt;math&gt;\alpha^c,\ldots,\alpha^{c+d-2}&lt;/math&gt; are roots of &lt;math&gt;g(x),&lt;/math&gt; hence of &lt;math&gt;p(x)&lt;/math&gt;.
This implies that &lt;math&gt;b_1,\ldots,b_{d-1}&lt;/math&gt; satisfy the following equations, for each &lt;math&gt;i \in \{c, \dotsc, c+d-2\}&lt;/math&gt;:

: &lt;math&gt;p(\alpha^i) = b_1\alpha^{ik_1} + b_2\alpha^{ik_2} + \cdots + b_{d-1}\alpha^{ik_{d-1}} = 0.&lt;/math&gt;

In matrix form, we have
:&lt;math&gt;\begin{bmatrix}
\alpha^{ck_1} &amp; \alpha^{ck_2} &amp; \cdots &amp; \alpha^{ck_{d-1}} \\
\alpha^{(c+1)k_1} &amp; \alpha^{(c+1)k_2} &amp; \cdots &amp; \alpha^{(c+1)k_{d-1}} \\
\vdots &amp; \vdots &amp;&amp; \vdots \\
\alpha^{(c+d-2)k_1} &amp; \alpha^{(c+d-2)k_2} &amp; \cdots &amp; \alpha^{(c+d-2)k_{d-1}} \\
\end{bmatrix}
\begin{bmatrix}
b_1 \\ b_2 \\ \vdots \\ b_{d-1}
\end{bmatrix}
=
\begin{bmatrix}
0 \\ 0 \\ \vdots \\ 0
\end{bmatrix}.
&lt;/math&gt;

The determinant of this matrix equals
:&lt;math&gt;\left(\prod_{i=1}^{d-1}\alpha^{ck_i}\right)\det\begin{pmatrix}
1 &amp; 1 &amp; \cdots &amp; 1 \\
\alpha^{k_1} &amp; \alpha^{k_2} &amp; \cdots &amp; \alpha^{k_{d-1}} \\
\vdots &amp; \vdots &amp;&amp; \vdots \\
\alpha^{(d-2)k_1} &amp; \alpha^{(d-2)k_2} &amp; \cdots &amp; \alpha^{(d-2)k_{d-1}} \\
\end{pmatrix} = \left(\prod_{i=1}^{d-1}\alpha^{ck_i}\right) \det(V).&lt;/math&gt;

The matrix &lt;math&gt;V&lt;/math&gt; is seen to be a [[Vandermonde matrix]], and its determinant is
:&lt;math&gt;\det(V) = \prod_{1\le i&lt;j\le d-1} (\alpha^{k_j}-\alpha^{k_i}),&lt;/math&gt;
which is non-zero. It therefore follows that &lt;math&gt;b_1,\ldots,b_{d-1}=0,&lt;/math&gt; hence &lt;math&gt;p(x)=0.&lt;/math&gt;
{{Collapse bottom}}

A BCH code is cyclic. {{Collapse top|title=Proof}}
A polynomial code of length &lt;math&gt;n&lt;/math&gt; is cyclic if and only if its generator polynomial divides &lt;math&gt;x^n-1.&lt;/math&gt;
Since &lt;math&gt;g(x)&lt;/math&gt; is the minimal polynomial with roots &lt;math&gt;\alpha^c,\ldots,\alpha^{c+d-2},&lt;/math&gt; it suffices to check that each of &lt;math&gt;\alpha^c,\ldots,\alpha^{c+d-2}&lt;/math&gt; is a root of &lt;math&gt;x^n-1.&lt;/math&gt;
This follows immediately from the fact that &lt;math&gt;\alpha&lt;/math&gt; is, by definition, an &lt;math&gt;n&lt;/math&gt;th root of unity.
{{Collapse bottom}}

== Encoding ==
{{Empty section|date=March 2013}}

== Decoding ==
There are many algorithms for decoding BCH codes. The most common ones follow this general outline:
# Calculate the syndromes ''s&lt;sub&gt;j&lt;/sub&gt;'' for the received vector &lt;!-- there are d syndromes --&gt;
# Determine the number of errors ''t'' and the error locator polynomial ''Λ(x)'' from the syndromes &lt;!-- Gill uses ν for actual errors; some references use ''t'' as max number of correctable errors. --&gt;
# Calculate the roots of the error location polynomial to find the error locations ''X&lt;sub&gt;i&lt;/sub&gt;''
# Calculate the error values ''Y&lt;sub&gt;i&lt;/sub&gt;'' at those error locations &lt;!-- also known as e_i --&gt;
# Correct the errors

During some of these steps, the decoding algorithm may determine that the received vector has too many errors and cannot be corrected. For example, if an appropriate value of ''t'' is not found, then the correction would fail. In a truncated (not primitive) code, an error location may be out of range. If the received vector has more errors than the code can correct, the decoder may unknowingly produce an apparently valid message that is not the one that was sent.

===Calculate the syndromes===
The received vector &lt;math&gt;R&lt;/math&gt; is the sum of the correct codeword &lt;math&gt;C&lt;/math&gt; and an unknown error vector &lt;math&gt;E.&lt;/math&gt;
The syndrome values are formed by considering &lt;math&gt;R&lt;/math&gt; as a polynomial and evaluating it at &lt;math&gt;\alpha^c,\ldots,\alpha^{c+d-2}.&lt;/math&gt;
Thus the syndromes are&lt;ref&gt;{{Harvnb|Lidl|Pilz|1999|p=229}}&lt;/ref&gt;
:&lt;math&gt;s_j = R(\alpha^{j}) = C(\alpha^{j}) + E(\alpha^{j})&lt;/math&gt;
for &lt;math&gt;j = c&lt;/math&gt; to &lt;math&gt;c+d-2.&lt;/math&gt;
Since &lt;math&gt;\alpha^{j}&lt;/math&gt; are the zeros of &lt;math&gt;g(x),&lt;/math&gt; of which
&lt;math&gt;C(x)&lt;/math&gt; is a multiple, &lt;math&gt;C(\alpha^{j}) = 0.&lt;/math&gt;
Examining the syndrome values thus isolates the error vector so one can begin to solve for it.

If there is no error, &lt;math&gt;s_j = 0&lt;/math&gt; for all &lt;math&gt;j.&lt;/math&gt;
If the syndromes are all zero, then the decoding is done.

===Calculate the error location polynomial===
If there are nonzero syndromes, then there are errors. The decoder needs to figure out how many errors and the location of those errors.

If there is a single error, write this as &lt;math&gt;E(x) = e\,x^i,&lt;/math&gt;
where &lt;math&gt;i&lt;/math&gt; is the location of the error and &lt;math&gt;e&lt;/math&gt; is its magnitude. Then the first two syndromes are
:&lt;math&gt;s_c = e\,\alpha^{c\,i}&lt;/math&gt;
:&lt;math&gt;s_{c+1} = e\,\alpha^{(c+1)\,i} = \alpha^i s_c&lt;/math&gt;
so together they allow us to calculate &lt;math&gt;e&lt;/math&gt; and provide some information about &lt;math&gt;i&lt;/math&gt; (completely determining it in the case of Reed–Solomon codes).

If there are two or more errors,
:&lt;math&gt;E(x) = e_1 x^{i_1} + e_2 x^{i_2} + \cdots \, &lt;/math&gt;
It is not immediately obvious how to begin solving the resulting syndromes for the unknowns &lt;math&gt;e_k&lt;/math&gt; and &lt;math&gt;i_k.&lt;/math&gt;
First step is finding locator polynomial
:&lt;math&gt;\Lambda(x)=\prod_{j=1}^t (x\alpha^{i_j}-1)&lt;/math&gt; compatible with computed syndromes and with minimal possible &lt;math&gt;t.&lt;/math&gt;

Two popular algorithms for this task are:
# [[BCH code#Peterson–Gorenstein–Zierler algorithm|Peterson–Gorenstein–Zierler algorithm]]
# [[Berlekamp–Massey algorithm]]

====Peterson–Gorenstein–Zierler algorithm====
&lt;!-- this confuses t (max number of errors that can be corrected) with ν (actual number of errors) --&gt;
Peterson's algorithm is the step 2 of the generalized BCH decoding procedure. Peterson's algorithm is used to calculate the error locator polynomial coefficients  &lt;math&gt; \lambda_1 , \lambda_2, \dots, \lambda_{v} &lt;/math&gt; of a polynomial

: &lt;math&gt; \Lambda(x) = 1 + \lambda_1 x + \lambda_2 x^2 + \cdots + \lambda_v x^v .&lt;/math&gt;

Now the procedure of the Peterson–Gorenstein–Zierler algorithm.&lt;ref&gt;{{Harvnb|Gorenstein|Peterson|Zierler|1960}}&lt;/ref&gt; Expect we have at least 2''t'' syndromes ''s''&lt;sub&gt;''c''&lt;/sub&gt;,...,''s''&lt;sub&gt;''c''+2''t''−1&lt;/sub&gt;.
Let ''v''&amp;nbsp;=&amp;nbsp;''t''.
* Start by generating the &lt;math&gt;S_{v\times v}&lt;/math&gt; matrix with elements that are syndrome values
::&lt;math&gt;S_{v \times v}=\begin{bmatrix}s_c&amp;s_{c+1}&amp;\dots&amp;s_{c+v-1}\\
s_{c+1}&amp;s_{c+2}&amp;\dots&amp;s_{c+v}\\
\vdots&amp;\vdots&amp;\ddots&amp;\vdots\\
s_{c+v-1}&amp;s_{c+v}&amp;\dots&amp;s_{c+2v-2}\end{bmatrix}.
&lt;/math&gt;
* Generate a &lt;math&gt;c_{v \times 1}&lt;/math&gt; vector with elements
::&lt;math&gt;C_{v \times 1}=\begin{bmatrix}s_{c+v}\\
s_{c+v+1}\\
\vdots\\
s_{c+2v-1}\end{bmatrix}.
&lt;/math&gt;
* Let &lt;math&gt;\Lambda&lt;/math&gt; denote the unknown polynomial coefficients, which are given by
::&lt;math&gt;\Lambda_{v \times 1} = \begin{bmatrix}\lambda_{v}\\
\lambda_{v-1}\\
\vdots\\
\lambda_{1}\end{bmatrix}.
&lt;/math&gt;
* Form the matrix equation
::&lt;math&gt;S_{v \times v} \Lambda_{v \times 1}  = -C_{v \times 1\,} .&lt;/math&gt;
* If the determinant of matrix &lt;math&gt;S_{v \times v}&lt;/math&gt; is nonzero, then we can actually find an inverse of this matrix and solve for the values of unknown &lt;math&gt;\Lambda&lt;/math&gt; values.
* If &lt;math&gt; \det(S_{v \times v}) = 0 ,&lt;/math&gt;  then follow
        if &lt;math&gt;v = 0&lt;/math&gt;
        then
              declare an empty error locator polynomial
              stop Peterson procedure.
        end
        set &lt;math&gt; v \leftarrow v -1&lt;/math&gt;
        continue from the beginning of Peterson's decoding by making smaller &lt;math&gt;S_{v \times v}&lt;/math&gt;
* After you have values of &lt;math&gt;\Lambda&lt;/math&gt;, you have with you the error locator polynomial.
* Stop Peterson procedure.

===Factor error locator polynomial===
Now that you have the &lt;math&gt;\Lambda(x)&lt;/math&gt; polynomial, its roots can be found in the form &lt;math&gt;\Lambda(x)=(\alpha^{i_1} x - 1) (\alpha^{i_2} x  - 1) \cdots (\alpha^{i_v} x - 1)&lt;/math&gt; by brute force for example using the [[Chien search]] algorithm. The exponential
powers of the primitive element &lt;math&gt;\alpha&lt;/math&gt; will yield the positions where errors occur in the received word; hence the name 'error locator' polynomial.

The zeros of Λ(''x'') are ''α''&lt;sup&gt;−''i''&lt;sub&gt;1&lt;/sub&gt;&lt;/sup&gt;, ..., ''α''&lt;sup&gt;−''i''&lt;sub&gt;''v''&lt;/sub&gt;&lt;/sup&gt;.

===Calculate error values===
Once the error locations are known, the next step is to determine the error values at those locations. The error values are then used to correct the received values at those locations to recover the original codeword.

For the case of binary BCH, (with all characters readable) this is trivial; just flip the bits for the received word at these positions, and we have the corrected code word. In the more general case, the error weights &lt;math&gt;e_j&lt;/math&gt; can be determined by solving the linear system

: &lt;math&gt;
\begin{align}
s_c &amp; = e_1 \alpha^{c\,i_1} + e_2 \alpha^{c\,i_2} + \cdots \\
s_{c+1} &amp; = e_1 \alpha^{(c + 1)\,i_1} + e_2 \alpha^{(c + 1)\,i_2} + \cdots \\
&amp; {}\ \vdots
\end{align}
&lt;/math&gt;

==== Forney algorithm ====
However, there is a more efficient method known as the [[Forney algorithm]].

Let

:&lt;math&gt;S(x)=s_c+s_{c+1}x+s_{c+2}x^2+\cdots+s_{c+d-2}x^{d-2}.&lt;/math&gt;
:&lt;math&gt;v\leqslant d-1, \lambda_0\neq 0 \qquad \Lambda(x)=\sum_{i=0}^v\lambda_ix^i=\lambda_0 \prod_{k=0}^{v} (\alpha^{-i_k}x-1).&lt;/math&gt;

And the error evaluator polynomial&lt;ref name="Gill-Forney"&gt;{{Harvnb|Gill|n.d.|p=47}}&lt;/ref&gt;

:&lt;math&gt;\Omega(x) \equiv S(x) \Lambda(x) \bmod{x^{d-1}}&lt;/math&gt;

Finally:

:&lt;math&gt;\Lambda'(x) = \sum_{i=1}^v i \cdot \lambda_i x^{i-1},&lt;/math&gt;

where

:&lt;math&gt;i\cdot x := \sum_{k=1}^i x.&lt;/math&gt;

Than if syndromes could be explained by an error word, which could be nonzero only on positions &lt;math&gt;i_k&lt;/math&gt;, then error values are
:&lt;math&gt;e_k=-{\alpha^{i_k}\Omega(\alpha^{-i_k})\over \alpha^{c\cdot i_k}\Lambda'(\alpha^{-i_k})}.&lt;/math&gt;

For narrow-sense BCH codes, ''c'' = 1, so the expression simplifies to:
:&lt;math&gt;e_k=-{\Omega(\alpha^{-i_k})\over \Lambda'(\alpha^{-i_k})}.&lt;/math&gt;

==== Explanation of Forney algorithm computation ====
It is based on [[Lagrange polynomial|Lagrange interpolation]] and techniques of [[generating function]]s.

Consider &lt;math&gt;S(x)\Lambda(x), &lt;/math&gt; and for the sake of simplicity suppose &lt;math&gt;\lambda_k=0&lt;/math&gt; for &lt;math&gt;k&gt;v,&lt;/math&gt; and &lt;math&gt;s_k=0&lt;/math&gt; for &lt;math&gt;k&gt;c+d-2.&lt;/math&gt; Then

:&lt;math&gt;S(x)\Lambda(x)=\sum_{j=0}^{\infty}\sum_{i=0}^j s_{j-i+1}\lambda_i x^j.&lt;/math&gt;

:&lt;math&gt; \begin{align}
S(x)\Lambda(x) &amp;=S(x) \left \{ \lambda_0\prod_{\ell=1}^v \left (\alpha^{i_\ell}x-1 \right ) \right \} \\
&amp;= \left \{ \sum_{i=0}^{d-2}\sum_{j=1}^v e_j\alpha^{(c+i)\cdot i_j} x^i \right \} \left \{ \lambda_0\prod_{\ell=1}^v \left (\alpha^{i_\ell}x-1 \right ) \right \} \\
&amp;=  \left \{ \sum_{j=1}^v e_j \alpha^{c i_j}\sum_{i=0}^{d-2} \left (\alpha^{i_j} \right )^i x^i \right \} \left \{ \lambda_0\prod_{\ell=1}^v \left (\alpha^{i_\ell}x-1 \right ) \right \} \\
&amp;=  \left \{ \sum_{j=1}^v e_j \alpha^{c i_j} \frac{\left (x \alpha^{i_j} \right )^{d-1}-1}{x \alpha^{i_j}-1} \right \} \left \{ \lambda_0 \prod_{\ell=1}^v \left (\alpha^{i_\ell}x-1 \right ) \right \} \\
&amp;= \lambda_0 \sum_{j=1}^v e_j\alpha^{c i_j} \frac{ \left (x\alpha^{i_j} \right)^{d-1}-1}{x\alpha^{i_j}-1} \prod_{\ell=1}^v \left (\alpha^{i_\ell}x-1 \right ) \\
&amp;=\lambda_0  \sum_{j=1}^v e_j\alpha^{c i_j} \left ( \left (x\alpha^{i_j} \right)^{d-1}-1 \right ) \prod_{\ell\in\{1,\cdots,v\}\setminus\{j\}} \left (\alpha^{i_\ell}x-1 \right )
\end{align}&lt;/math&gt;

We want to compute unknowns &lt;math&gt;e_j,&lt;/math&gt; and we could simplify the context by removing the &lt;math&gt;(x\alpha^{i_j})^{d-1}&lt;/math&gt; terms. This leads to the error evaluator polynomial
:&lt;math&gt;\Omega(x) \equiv S(x) \Lambda(x) \bmod{x^{d-1}}.&lt;/math&gt;

Thanks to &lt;math&gt;v\leqslant d-1&lt;/math&gt; we have
:&lt;math&gt;\Omega(x) = -\lambda_0\sum_{j=1}^v e_j\alpha^{c i_j} \prod_{\ell\in\{1,\cdots,v\}\setminus\{j\}} \left (\alpha^{i_\ell}x-1 \right ).&lt;/math&gt;

Thanks to &lt;math&gt;\Lambda&lt;/math&gt; (the Lagrange interpolation trick) the sum degenerates to only one summand for &lt;math&gt;x = \alpha^{-i_k}&lt;/math&gt;

:&lt;math&gt;\Omega \left (\alpha^{-i_k} \right )=-\lambda_0e_k\alpha^{c\cdot i_k}\prod_{\ell\in\{1,\cdots,v\}\setminus\{k\}} \left (\alpha^{i_\ell}\alpha^{-i_k}-1 \right ). &lt;/math&gt;

To get &lt;math&gt;e_k&lt;/math&gt; we just should get rid of the product. We could compute the product directly from already computed roots &lt;math&gt;\alpha^{-i_j}&lt;/math&gt; of &lt;math&gt;\Lambda,&lt;/math&gt; but we could use simpler form.

As [[formal derivative]]

:&lt;math&gt;\Lambda'(x)=\lambda_0\sum_{j=1}^v \alpha^{i_j}\prod_{\ell\in\{1,\cdots,v\}\setminus\{j\}} \left (\alpha^{i_\ell}x-1 \right ),&lt;/math&gt;
we get again only one summand in

:&lt;math&gt;\Lambda'(\alpha^{-i_k})=\lambda_0\alpha^{i_k}\prod_{\ell\in\{1,\cdots,v\}\setminus\{k\}} \left (\alpha^{i_\ell}\alpha^{-i_k}-1 \right ).&lt;/math&gt;

So finally
:&lt;math&gt;e_k=- \frac{\alpha^{i_k}\Omega \left (\alpha^{-i_k} \right )}{\alpha^{c\cdot i_k}\Lambda' \left (\alpha^{-i_k} \right )}.&lt;/math&gt;

This formula is advantageous when one computes the formal derivative of &lt;math&gt;\Lambda&lt;/math&gt; form

:&lt;math&gt;\Lambda(x)=\sum_{i=1}^v\lambda_ix^i&lt;/math&gt;

yielding:
:&lt;math&gt;\Lambda'(x) = \sum_{i=1}^v i \cdot \lambda_i x^{i-1},&lt;/math&gt;

where

:&lt;math&gt;i\cdot x := \sum_{k=1}^i x.&lt;/math&gt;

=== Decoding based on extended Euclidean algorithm ===
An alternate process of finding both the polynomial Λ and the error locator polynomial is based on Yasuo Sugiyama's adaptation of the [[Extended Euclidean algorithm]].&lt;ref&gt;Yasuo Sugiyama, Masao Kasahara, Shigeichi Hirasawa, and Toshihiko Namekawa. A method for solving key equation for decoding Goppa codes. Information and Control, 27:87–99, 1975.&lt;/ref&gt; Correction of unreadable characters could be incorporated to the algorithm easily as well.

Let &lt;math&gt;k_1, ... ,k_k&lt;/math&gt; be positions of unreadable characters. One creates polynomial localising these positions &lt;math&gt;\Gamma(x)=\prod_{i=1}^k(x\alpha^{k_i}-1).&lt;/math&gt;
Set values on unreadable positions to 0 and compute the syndromes.

As we have already defined for the Forney formula let &lt;math&gt;S(x)=\sum_{i=0}^{d-2}s_{c+i}x^i.&lt;/math&gt;

Let us run extended Euclidean algorithm for locating least common divisor of polynomials &lt;math&gt;S(x)\Gamma(x)&lt;/math&gt; and &lt;math&gt;x^{d-1}.&lt;/math&gt;
The goal is not to find the least common divisor, but a polynomial &lt;math&gt;r(x)&lt;/math&gt; of degree at most &lt;math&gt;\lfloor (d+k-3)/2\rfloor&lt;/math&gt; and polynomials &lt;math&gt;a(x), b(x)&lt;/math&gt; such that &lt;math&gt;r(x)=a(x)S(x)\Gamma(x)+b(x)x^{d-1}.&lt;/math&gt;
Low degree of &lt;math&gt;r(x)&lt;/math&gt; guarantees, that &lt;math&gt;a(x)&lt;/math&gt; would satisfy extended (by &lt;math&gt;\Gamma&lt;/math&gt;) defining conditions for &lt;math&gt;\Lambda.&lt;/math&gt;

Defining &lt;math&gt;\Xi(x)=a(x)\Gamma(x)&lt;/math&gt; and using &lt;math&gt;\Xi&lt;/math&gt; on the place of &lt;math&gt;\Lambda(x)&lt;/math&gt; in the Fourney formula will give us error values.

The main advantage of the algorithm is that it meanwhile computes &lt;math&gt;\Omega(x)=S(x)\Xi(x)\bmod x^{d-1}=r(x)&lt;/math&gt; required in the Forney formula.

==== Explanation of the decoding process ====
The goal is to find a codeword which differs from the received word minimally as possible on readable positions. When expressing the received word as a sum of nearest codeword and error word, we are trying to find error word with minimal number of non-zeros on readable positions. Syndrom &lt;math&gt;s_i&lt;/math&gt; restricts error word by condition

:&lt;math&gt;s_i=\sum_{j=0}^{n-1}e_j\alpha^{ij}.&lt;/math&gt;

We could write these conditions separately or we could create polynomial

:&lt;math&gt;S(x)=\sum_{i=0}^{d-2}s_{c+i}x^i&lt;/math&gt;

and compare coefficients near powers &lt;math&gt;0&lt;/math&gt; to &lt;math&gt;d-2.&lt;/math&gt;

:&lt;math&gt;S(x) \stackrel{\{0,\cdots,\,d-2\}}{=} E(x)=\sum_{i=0}^{d-2}\sum_{j=0}^{n-1}e_j\alpha^{ij}\alpha^{cj}x^i.&lt;/math&gt;

Suppose there is unreadable letter on position &lt;math&gt;k_1,&lt;/math&gt; we could replace set of syndromes &lt;math&gt;\{s_c,\cdots,s_{c+d-2}\}&lt;/math&gt; by set of syndromes &lt;math&gt;\{t_c,\cdots,t_{c+d-3}\}&lt;/math&gt; defined by equation &lt;math&gt;t_i=\alpha^{k_1}s_i-s_{i+1}.&lt;/math&gt; Suppose for an error word all restrictions by original set &lt;math&gt;\{s_c,\cdots,s_{c+d-2}\}&lt;/math&gt; of syndromes hold,
than

:&lt;math&gt;t_i=\alpha^{k_1}s_i-s_{i+1}=\alpha^{k_1}\sum_{j=0}^{n-1}e_j\alpha^{ij}-\sum_{j=0}^{n-1}e_j\alpha^j\alpha^{ij}=\sum_{j=0}^{n-1}e_j(\alpha^{k_1}-\alpha^j)\alpha^{ij}.&lt;/math&gt;

New set of syndromes restricts error vector

:&lt;math&gt;f_j=e_j(\alpha^{k_1}-\alpha^j)&lt;/math&gt;

the same way the original set of syndromes restricted the error vector &lt;math&gt;e_j.&lt;/math&gt; Note, that except the coordinate &lt;math&gt;k_1,&lt;/math&gt; where we have &lt;math&gt;f_{k_1}=0,&lt;/math&gt; an &lt;math&gt;f_j&lt;/math&gt; is zero, if &lt;math&gt;e_j = 0.&lt;/math&gt; For the goal of locating error positions we could change the set of syndromes in the similar way to reflect all unreadable characters. This shortens the set of syndromes by &lt;math&gt;k.&lt;/math&gt;

In polynomial formulation, the replacement of syndromes set &lt;math&gt;\{s_c,\cdots,s_{c+d-2}\}&lt;/math&gt; by syndromes set &lt;math&gt;\{t_c,\cdots,t_{c+d-3}\}&lt;/math&gt; leads to

:&lt;math&gt;T(x)=\sum_{i=0}^{d-3}t_{c+i}x^i=\alpha^{k_1}\sum_{i=0}^{d-3}s_{c+i}x^i-\sum_{i=1}^{d-2}s_{c+i}x^{i-1}.&lt;/math&gt;

Therefore,

:&lt;math&gt;xT(x) \stackrel{\{1,\cdots,\,d-2\}}{=} \left (x\alpha^{k_1}-1 \right )S(x).&lt;/math&gt;

After replacement of &lt;math&gt;S(x)&lt;/math&gt; by &lt;math&gt;S(x)\Gamma(x)&lt;/math&gt;, one would require equation for coefficients near powers &lt;math&gt;k,\cdots,d-2.&lt;/math&gt;

One could consider looking for error positions from the point of view of eliminating influence of given positions similarly as for unreadable characters. If we found &lt;math&gt;v&lt;/math&gt; positions such that eliminating their influence leads to obtaining set of syndromes consisting of all zeros, than there exists error vector with errors only on these coordinates.
If &lt;math&gt;\Lambda(x)&lt;/math&gt; denotes the polynomial eliminating the influence of these coordinates, we obtain

:&lt;math&gt;S(x)\Gamma(x)\Lambda(x) \stackrel{\{k+v,\cdots, d-2\}}{=} 0.&lt;/math&gt;

In Euclidean algorithm, we try to correct at most &lt;math&gt;\tfrac{1}{2}(d-1-k)&lt;/math&gt; errors (on readable positions), because with bigger error count there could be more codewords in the same distance from the received word. Therefore, for &lt;math&gt;\Lambda(x)&lt;/math&gt; we are looking for, the equation must hold for coefficients near powers starting from

:&lt;math&gt;k+ \left \lfloor \tfrac{1}{2} (d-1-k) \right \rfloor.&lt;/math&gt;

In Forney formula, &lt;math&gt;\Lambda(x)&lt;/math&gt; could be multiplied by a scalar giving the same result.

It could happen that the Euclidean algorithm finds &lt;math&gt;\Lambda(x)&lt;/math&gt; of degree higher than &lt;math&gt;\tfrac{1}{2}(d-1-k)&lt;/math&gt; having number of different roots equal to its degree, where the Fourney formula would be able to correct errors in all its roots, anyway correcting such many errors could be risky (especially with no other restrictions on received word). Usually after getting  &lt;math&gt;\Lambda(x)&lt;/math&gt; of higher degree, we decide not to correct the errors. Correction could fail in the case &lt;math&gt;\Lambda(x)&lt;/math&gt; has roots with higher multiplicity or the number of roots is smaller than its degree. Fail could be detected as well by Forney formula returning error outside the transmitted alphabet.

===Correct the errors===
Using the error values and error location, correct the errors and form a corrected code vector by subtracting error values at error locations.

===Decoding examples===

==== Decoding of binary code without unreadable characters ====
Consider a BCH code in GF(2&lt;sup&gt;4&lt;/sup&gt;) with &lt;math&gt;d=7&lt;/math&gt; and &lt;math&gt;g(x) = x^{10} + x^8 + x^5 + x^4 + x^2 + x + 1&lt;/math&gt;. (This is used in [[QR code]]s.) Let the message to be transmitted be &lt;nowiki&gt;[1 1 0 1 1]&lt;/nowiki&gt;, or in polynomial notation, &lt;math&gt;M(x) = x^4 + x^3 + x + 1.&lt;/math&gt;
The "checksum" symbols are calculated by dividing &lt;math&gt;x^{10} M(x)&lt;/math&gt; by &lt;math&gt;g(x)&lt;/math&gt; and taking the remainder, resulting in &lt;math&gt;x^9 + x^4 + x^2&lt;/math&gt; or &lt;nowiki&gt;[ 1 0 0 0 0 1 0 1 0 0 ]&lt;/nowiki&gt;. These are appended to the message, so the transmitted codeword is &lt;nowiki&gt;[ 1 1 0 1 1 1 0 0 0 0 1 0 1 0 0 ]&lt;/nowiki&gt;.

Now, imagine that there are two bit-errors in the transmission, so the received codeword is [ 1 {{color|red|0}} 0 1 1 1 0 0 0 {{color|red|1}} 1 0 1 0 0 ]. In polynomial notation:
:&lt;math&gt;R(x) = C(x) + x^{13} + x^5 = x^{14} + x^{11} + x^{10} + x^9 + x^5 + x^4 + x^2&lt;/math&gt;
In order to correct the errors, first calculate the syndromes. Taking &lt;math&gt;\alpha = 0010,&lt;/math&gt; we have &lt;math&gt;s_1 = R(\alpha^1) = 1011,&lt;/math&gt; &lt;math&gt;s_2 = 1001,&lt;/math&gt; &lt;math&gt;s_3 = 1011,&lt;/math&gt; &lt;math&gt;s_4 = 1101,&lt;/math&gt; &lt;math&gt;s_5 = 0001,&lt;/math&gt; and &lt;math&gt;s_6 = 1001.&lt;/math&gt;
Next, apply the Peterson procedure by row-reducing the following augmented matrix.
:&lt;math&gt;\left [ S_{3 \times 3} | C_{3 \times 1} \right ] =
\begin{bmatrix}s_1&amp;s_2&amp;s_3&amp;s_4\\
s_2&amp;s_3&amp;s_4&amp;s_5\\
s_3&amp;s_4&amp;s_5&amp;s_6\end{bmatrix} =
\begin{bmatrix}1011&amp;1001&amp;1011&amp;1101\\
1001&amp;1011&amp;1101&amp;0001\\
1011&amp;1101&amp;0001&amp;1001\end{bmatrix} \Rightarrow
\begin{bmatrix}0001&amp;0000&amp;1000&amp;0111\\
0000&amp;0001&amp;1011&amp;0001\\
0000&amp;0000&amp;0000&amp;0000\end{bmatrix}&lt;/math&gt;
Due to the zero row, {{math|''S''&lt;sub&gt;3×3&lt;/sub&gt;}} is singular, which is no surprise since only two errors were introduced into the codeword.
However, the upper-left corner of the matrix is identical to {{closed-closed|''S''&lt;sub&gt;2×2&lt;/sub&gt; &lt;nowiki&gt;|&lt;/nowiki&gt; ''C''&lt;sub&gt;2×1&lt;/sub&gt;}}, which gives rise to the solution &lt;math&gt;\lambda_2 = 1000,&lt;/math&gt; &lt;math&gt;\lambda_1 = 1011.&lt;/math&gt;
The resulting error locator polynomial is &lt;math&gt;\Lambda(x) = 1000 x^2 + 1011 x + 0001,&lt;/math&gt; which has zeros at &lt;math&gt;0100 = \alpha^{-13}&lt;/math&gt; and &lt;math&gt;0111 = \alpha^{-5}.&lt;/math&gt;
The exponents of &lt;math&gt;\alpha&lt;/math&gt; correspond to the error locations.
There is no need to calculate the error values in this example, as the only possible value is 1.

==== Decoding with unreadable characters ====
Suppose the same scenario, but the received word has two unreadable characters [ 1 {{color|red|0}} 0 ? 1 1 ? 0 0 {{color|red|1}} 1 0 1 0 0 ]. We replace the unreadable characters by zeros while creating the polynom reflecting their positions &lt;math&gt;\Gamma(x)=(\alpha^8x-1)(\alpha^{11}x-1).&lt;/math&gt; We compute the syndromes &lt;math&gt;s_1=\alpha^{-7}, s_2=\alpha^{1}, s_3=\alpha^{4}, s_4=\alpha^{2}, s_5=\alpha^{5},&lt;/math&gt; and &lt;math&gt;s_6=\alpha^{-7}.&lt;/math&gt; (Using log notation which is independent on GF(2&lt;sup&gt;4&lt;/sup&gt;) isomorphisms. For computation checking we can use the same representation for addition as was used in previous example. Hexadecimal description of the powers of &lt;math&gt;\alpha&lt;/math&gt; are consecutively 1,2,4,8,3,6,C,B,5,A,7,E,F,D,9 with the addition based on bitwise xor.)

Let us make syndrome polynomial

:&lt;math&gt;S(x)=\alpha^{-7}+\alpha^{1}x+\alpha^{4}x^2+\alpha^{2}x^3+\alpha^{5}x^4+\alpha^{-7}x^5,&lt;/math&gt;

compute

:&lt;math&gt;S(x)\Gamma(x)=\alpha^{-7}+\alpha^{4}x+\alpha^{-1}x^2+\alpha^{6}x^3+\alpha^{-1}x^4+\alpha^{5}x^5+\alpha^{7}x^6+\alpha^{-3}x^7.&lt;/math&gt;

Run the extended Euclidean algorithm:

:&lt;math&gt;\begin{align}
\begin{pmatrix}S(x)\Gamma(x)\\ x^6\end{pmatrix} &amp;= \begin{pmatrix}\alpha^{-7} +\alpha^{4}x+ \alpha^{-1}x^2+ \alpha^{6}x^3+ \alpha^{-1}x^4+\alpha^{5}x^5 +\alpha^{7}x^6+ \alpha^{-3}x^7 \\ x^6\end{pmatrix} \\ [6pt]
&amp;=\begin{pmatrix}\alpha^{7}+\alpha^{-3}x&amp;1\\ 1&amp;0\end{pmatrix} \begin{pmatrix}x^6\\ \alpha^{-7} +\alpha^{4}x +\alpha^{-1}x^2 +\alpha^{6}x^3 +\alpha^{-1}x^4 +\alpha^{5}x^5 +2\alpha^{7}x^6 +2\alpha^{-3}x^7\end{pmatrix} \\ [6pt]
&amp;=\begin{pmatrix}\alpha^{7}+\alpha^{-3}x&amp;1\\ 1&amp;0\end{pmatrix} \begin{pmatrix}\alpha^4+\alpha^{-5}x&amp;1\\ 1&amp;0\end{pmatrix} \times \\
&amp;\qquad \times \begin{pmatrix} \alpha^{-7}+\alpha^{4}x+\alpha^{-1}x^2+\alpha^{6}x^3+\alpha^{-1}x^4+\alpha^{5}x^5\\ \alpha^{-3} +(\alpha^{-7}+\alpha^{3})x+(\alpha^{3}+\alpha^{-1})x^2+(\alpha^{-5}+\alpha^{-6})x^3+(\alpha^3+\alpha^{1})x^4+ 2\alpha^{-6}x^5+ 2x^6\end{pmatrix} \\ [6pt]
&amp;=\begin{pmatrix}(1+\alpha^{-4})+(\alpha^{1}+\alpha^{2})x+\alpha^{7}x^2&amp;\alpha^{7}+\alpha^{-3}x \\ \alpha^4+\alpha^{-5}x&amp;1\end{pmatrix} \begin{pmatrix} \alpha^{-7} +\alpha^{4}x +\alpha^{-1}x^2+ \alpha^{6}x^3+\alpha^{-1}x^4+\alpha^{5}x^5\\ \alpha^{-3}+\alpha^{-2}x+\alpha^{0}x^2+ \alpha^{-2}x^3+\alpha^{-6}x^4\end{pmatrix} \\ [6pt]
&amp;=\begin{pmatrix}\alpha^{-3}+\alpha^{5}x+\alpha^{7}x^2&amp;\alpha^{7}+\alpha^{-3}x \\ \alpha^4+\alpha^{-5}x&amp;1\end{pmatrix} \begin{pmatrix}\alpha^{-5}+\alpha^{-4}x&amp;1\\ 1&amp;0 \end{pmatrix} \times \\
&amp;\qquad \times \begin{pmatrix} \alpha^{-3}+\alpha^{-2}x+\alpha^{0}x^2+ \alpha^{-2}x^3+\alpha^{-6}x^4\\ (\alpha^{7}+\alpha^{-7})+ (2\alpha^{-7}+\alpha^{4})x+ (\alpha^{-5}+\alpha^{-6}+\alpha^{-1})x^2+ (\alpha^{-7}+\alpha^{-4}+\alpha^{6})x^3+ (\alpha^{4}+\alpha^{-6}+\alpha^{-1})x^4+ 2\alpha^{5}x^5\end{pmatrix} \\ [6pt]
&amp;=\begin{pmatrix} \alpha^{7}x+\alpha^{5}x^2+\alpha^{3}x^3&amp;\alpha^{-3}+\alpha^{5}x+\alpha^{7}x^2\\ \alpha^{3}+\alpha^{-5}x+\alpha^{6}x^2&amp;\alpha^4+\alpha^{-5}x\end{pmatrix}
\begin{pmatrix} \alpha^{-3}+\alpha^{-2}x+\alpha^{0}x^2+ \alpha^{-2}x^3+\alpha^{-6}x^4\\ \alpha^{-4}+\alpha^{4}x+\alpha^{2}x^2+ \alpha^{-5}x^3\end{pmatrix}.
\end{align}&lt;/math&gt;

We have reached polynomial of degree at most 3, and as

:&lt;math&gt;\begin{pmatrix}-(\alpha^4+\alpha^{-5}x)&amp;\alpha^{-3}+\alpha^{5}x+\alpha^{7}x^2\\ \alpha^{3}+\alpha^{-5}x+\alpha^{6}x^2&amp;-(\alpha^{7}x+\alpha^{5}x^2+\alpha^{3}x^3)\end{pmatrix} \begin{pmatrix} \alpha^{7}x+\alpha^{5}x^2+\alpha^{3}x^3&amp;\alpha^{-3}+\alpha^{5}x+\alpha^{7}x^2\\ \alpha^{3}+\alpha^{-5}x+\alpha^{6}x^2&amp;\alpha^4+\alpha^{-5}x\end{pmatrix} =\begin{pmatrix}1&amp;0\\ 0&amp;1\end{pmatrix},&lt;/math&gt;

we get

:&lt;math&gt; \begin{pmatrix}-(\alpha^4+\alpha^{-5}x)&amp;\alpha^{-3}+\alpha^{5}x+\alpha^{7}x^2\\ \alpha^{3}+\alpha^{-5}x+\alpha^{6}x^2&amp;-(\alpha^{7}x+\alpha^{5}x^2+\alpha^{3}x^3)\end{pmatrix}
\begin{pmatrix}S(x)\Gamma(x)\\ x^6\end{pmatrix}= \begin{pmatrix} \alpha^{-3}+\alpha^{-2}x+\alpha^{0}x^2+\alpha^{-2}x^3+\alpha^{-6}x^4\\ \alpha^{-4}+\alpha^{4}x +\alpha^{2}x^2+\alpha^{-5}x^3 \end{pmatrix}. &lt;/math&gt;

Therefore,

:&lt;math&gt; S(x)\Gamma(x)(\alpha^{3}+\alpha^{-5}x+\alpha^{6}x^2)- (\alpha^{7}x+\alpha^{5}x^2+\alpha^{3}x^3)x^6= \alpha^{-4}+\alpha^{4}x+\alpha^{2}x^2+\alpha^{-5}x^3. &lt;/math&gt;

Let &lt;math&gt;\Lambda(x)=\alpha^{3}+\alpha^{-5}x+\alpha^{6}x^2.&lt;/math&gt; Don't worry that &lt;math&gt;\lambda_0\neq 1.&lt;/math&gt; Find by brute force a root of &lt;math&gt;\Lambda.&lt;/math&gt; The roots are &lt;math&gt;\alpha^2,&lt;/math&gt; and &lt;math&gt;\alpha^{10}&lt;/math&gt; (after finding for example &lt;math&gt;\alpha^2&lt;/math&gt; we can divide &lt;math&gt;\Lambda&lt;/math&gt; by corresponding monom &lt;math&gt;(x-\alpha^2)&lt;/math&gt; and the root of resulting monom could be found easily).

Let

:&lt;math&gt;\Xi(x)=\Gamma(x)\Lambda(x)=\alpha^3+\alpha^4x^2+\alpha^2x^3+\alpha^{-5}x^4 &lt;/math&gt;
:&lt;math&gt;\Omega(x)=S(x)\Xi(x) \equiv \alpha^{-4}+\alpha^4x+\alpha^2x^2+\alpha^{-5}x^3 \bmod{x^6}&lt;/math&gt;

Let us look for error values using formula

:&lt;math&gt;e_j=-\frac{\Omega \left (\alpha^{-i_j} \right )}{\Xi' \left (\alpha^{-i_j} \right )},&lt;/math&gt;

where &lt;math&gt;\alpha^{-i_j}&lt;/math&gt; are roots of &lt;math&gt;\Xi(x).&lt;/math&gt; &lt;math&gt;\Xi'(x)=\alpha^{2}x^2.&lt;/math&gt; We get

:&lt;math&gt;\begin{align}
e_1 &amp;=-\frac{\Omega(\alpha^4)}{\Xi'(\alpha^{4})} =\frac{\alpha^{-4}+\alpha^{-7}+\alpha^{-5}+\alpha^{7}}{\alpha^{-5}} =\frac{\alpha^{-5}}{\alpha^{-5}}=1 \\
e_2 &amp;=-\frac{\Omega(\alpha^7)}{\Xi'(\alpha^{7})}=\frac{\alpha^{-4}+\alpha^{-4}+\alpha^{1}+\alpha^{1}}{\alpha^{1}}=0 \\
e_3 &amp;=-\frac{\Omega(\alpha^{10})}{\Xi'(\alpha^{10})}=\frac{\alpha^{-4}+\alpha^{-1}+\alpha^{7}+\alpha^{-5}}{\alpha^{7}}=\frac{\alpha^{7}}{\alpha^{7}}=1 \\
e_4 &amp;=-\frac{\Omega(\alpha^{2})}{\Xi'(\alpha^{2})}=\frac{\alpha^{-4}+\alpha^{6}+\alpha^{6}+\alpha^{1}}{\alpha^{6}}=\frac{\alpha^{6}}{\alpha^{6}}=1
\end{align}&lt;/math&gt;

Fact, that &lt;math&gt;e_3=e_4=1,&lt;/math&gt; should not be surprising.

Corrected code is therefore [ 1 {{color|green|1}} 0 {{color|green|1}} 1 1 {{color|green|0}} 0 0 {{color|green|0}} 1 0 1 0 0].

==== Decoding with unreadable characters with a small number of errors ====
Let us show the algorithm behaviour for the case with small number of errors. Let the received word is [ 1 {{color|red|0}} 0 ? 1 1 ? 0 0 0 1 0 1 0 0 ].

Again, replace the unreadable characters by zeros while creating the polynom reflecting their positions &lt;math&gt;\Gamma(x)=(\alpha^8x-1)(\alpha^{11}x-1).&lt;/math&gt;
Compute the syndromes &lt;math&gt;s_1=\alpha^{4}, s_2=\alpha^{-7}, s_3=\alpha^{1}, s_4=\alpha^{1}, s_5=\alpha^{0},&lt;/math&gt; and &lt;math&gt;s_6=\alpha^{2}.&lt;/math&gt;
Create syndrome polynomial

:&lt;math&gt;S(x)=\alpha^{4}+\alpha^{-7}x+\alpha^{1}x^2+\alpha^{1}x^3+\alpha^{0}x^4+\alpha^{2}x^5,&lt;/math&gt;
:&lt;math&gt;S(x)\Gamma(x)=\alpha^{4}+\alpha^{7}x+\alpha^{5}x^2+\alpha^{3}x^3+\alpha^{1}x^4+\alpha^{-1}x^5+\alpha^{-1}x^6+\alpha^{6}x^7.&lt;/math&gt;

Let us run the extended Euclidean algorithm:

&lt;math&gt;\begin{align}
\begin{pmatrix}S(x)\Gamma(x)\\ x^6\end{pmatrix} &amp;= \begin{pmatrix} \alpha^{4}+\alpha^{7}x+\alpha^{5}x^2 +\alpha^{3}x^3+\alpha^{1}x^4+\alpha^{-1}x^5 +\alpha^{-1}x^6+\alpha^{6}x^7\\ x^6\end{pmatrix} \\
&amp;=\begin{pmatrix}\alpha^{-1}+\alpha^{6}x&amp;1\\ 1&amp;0\end{pmatrix} \begin{pmatrix}x^6\\ \alpha^{4}+\alpha^{7}x+\alpha^{5}x^2+\alpha^{3}x^3+\alpha^{1}x^4+\alpha^{-1}x^5+2\alpha^{-1}x^6+2\alpha^{6}x^7
\end{pmatrix} \\
&amp;=\begin{pmatrix}\alpha^{-1}+\alpha^{6}x&amp;1\\ 1&amp;0\end{pmatrix} \begin{pmatrix}\alpha^{3}+\alpha^{1}x&amp;1\\ 1&amp;0\end{pmatrix} \begin{pmatrix} \alpha^{4} +\alpha^{7}x+ \alpha^{5}x^2 +\alpha^{3}x^3+\alpha^{1}x^4+\alpha^{-1}x^5\\ \alpha^{7}+(\alpha^{-5}+\alpha^{5})x+ 2\alpha^{-7}x^2+2\alpha^{6}x^3+ 2\alpha^{4}x^4+ 2\alpha^{2}x^5+ 2x^6 \end{pmatrix} \\
&amp;=\begin{pmatrix}(1+\alpha^{2})+(\alpha^{0}+\alpha^{-6})x+\alpha^{7}x^2&amp;\alpha^{-1}+\alpha^{6}x\\ \alpha^{3}+\alpha^{1}x&amp;1\end{pmatrix} \begin{pmatrix} \alpha^{4}+\alpha^{7}x+ \alpha^{5}x^2 +\alpha^{3}x^3+ \alpha^{1}x^4+\alpha^{-1}x^5\\ \alpha^{7}+\alpha^{0}x \end{pmatrix}
\end{align} &lt;/math&gt;

We have reached polynomial of degree at most 3, and as
: &lt;math&gt;
\begin{pmatrix}-1&amp;\alpha^{-1}+\alpha^{6}x\\
\alpha^{3}+\alpha^{1}x&amp;-(\alpha^{-7}+\alpha^{7}x+\alpha^{7}x^2)\end{pmatrix}
\begin{pmatrix}\alpha^{-7}+\alpha^{7}x+\alpha^{7}x^2&amp;\alpha^{-1}+\alpha^{6}x\\
\alpha^{3}+\alpha^{1}x&amp;1\end{pmatrix}
=\begin{pmatrix}1&amp;0\\ 0&amp;1\end{pmatrix},
&lt;/math&gt;
we get
: &lt;math&gt;
\begin{pmatrix}-1&amp;\alpha^{-1}+\alpha^{6}x\\
\alpha^{3}+\alpha^{1}x&amp;-(\alpha^{-7}+\alpha^{7}x+\alpha^{7}x^2)\end{pmatrix}
\begin{pmatrix}S(x)\Gamma(x)\\ x^6\end{pmatrix}=
\begin{pmatrix}\alpha^{4}+\alpha^{7}x+\alpha^{5}x^2+\alpha^{3}x^3+\alpha^{1}x^4+\alpha^{-1}x^5\\
\alpha^{7}+\alpha^{0}x
\end{pmatrix}.
&lt;/math&gt;

Therefore,
: &lt;math&gt; S(x)\Gamma(x)(\alpha^{3}+\alpha^{1}x)- (\alpha^{-7}+\alpha^{7}x+\alpha^{7}x^2)x^6= \alpha^{7}+\alpha^{0}x. &lt;/math&gt;

Let &lt;math&gt;\Lambda(x)=\alpha^{3}+\alpha^{1}x.&lt;/math&gt; Don't worry that &lt;math&gt;\lambda_0\neq 1.&lt;/math&gt; The root of &lt;math&gt;\Lambda(x)&lt;/math&gt; is &lt;math&gt;\alpha^{3-1}.&lt;/math&gt;

Let 
:&lt;math&gt;\Xi(x)=\Gamma(x)\Lambda(x)=\alpha^{3}+\alpha^{-7}x+\alpha^{-4}x^2+\alpha^{5}x^3,&lt;/math&gt; 
:&lt;math&gt;\Omega(x)=S(x)\Xi(x) \equiv \alpha^{7}+\alpha^{0}x \bmod{x^6}&lt;/math&gt;

Let us look for error values using formula &lt;math&gt;e_j=-\Omega(\alpha^{-i_j})/\Xi'(\alpha^{-i_j}),&lt;/math&gt; where &lt;math&gt;\alpha^{-i_j}&lt;/math&gt; are roots of polynomial &lt;math&gt;\Xi(x).&lt;/math&gt;
&lt;math&gt;\Xi'(x)=\alpha^{-7}+\alpha^{5}x^2.&lt;/math&gt;
We get

:&lt;math&gt;\begin{align}
e_1 &amp;=-\frac{\Omega(\alpha^4)}{\Xi'(\alpha^{4})}=\frac{\alpha^{7}+\alpha^{4}}{\alpha^{-7}+\alpha^{-2}}= \frac{\alpha^{3}}{\alpha^{3}}=1 \\
e_2 &amp;=-\frac{\Omega(\alpha^7)}{\Xi'(\alpha^{7})}=\frac{\alpha^{7}+\alpha^{7}}{\alpha^{-7}+\alpha^{4}}= 0 \\
e_3 &amp;=-\frac{\Omega(\alpha^2)}{\Xi'(\alpha^2)}= \frac{\alpha^{7}+\alpha^{2}}{\alpha^{-7}+\alpha^{-6}}=\frac{\alpha^{-3}}{\alpha^{-3}}=1
\end{align}&lt;/math&gt;

The fact that &lt;math&gt;e_3=1&lt;/math&gt; should not be surprising.

Corrected code is therefore [ 1 {{color|green|1}} 0 {{color|green|1}} 1 1 {{color|green|0}} 0 0 0 1 0 1 0 0].

==Citations==
{{reflist|30em}}

== References ==

===Primary sources===
* {{Citation
 |first= A.
 |last= Hocquenghem
 |author-link= Alexis Hocquenghem
 |title=Codes correcteurs d'erreurs
 |language= French
 |journal= Chiffres
 |location= Paris
 |volume=2
 |pages= 147–156
 |date= September 1959
 |issn=
 |doi=}}
* {{Citation
 |first= R. C.
 |last= Bose
 |author-link= R. C. Bose
 |first2= D. K.
 |last2= Ray-Chaudhuri
 |author2-link= D. K. Ray-Chaudhuri
 |title= On A Class of Error Correcting Binary Group Codes
 |journal= [[Information and Control]]
 |volume= 3
 |issue=1
 |pages= 68–79
 |date= March 1960
 |issn= 0890-5401
 |doi=10.1016/s0019-9958(60)90287-4}}

===Secondary sources===
* {{Citation|last=Gill |first=John |title=EE387 Notes #7, Handout #28 |date=n.d. |accessdate=April 21, 2010 |pages=42–45 |publisher=Stanford University |url=http://www.stanford.edu/class/ee387/handouts/notes7.pdf |doi= }}{{dead link|date=August 2016|bot=medic}}{{cbignore|bot=medic}} Course notes are apparently being redone for 2012: http://www.stanford.edu/class/ee387/
* {{Citation
 |last= Gorenstein
 |first= Daniel
 |authorlink= Daniel Gorenstein
 |last2= Peterson
 |first2= W. Wesley
 |authorlink2= W. Wesley Peterson
 |last3= Zierler
 |first3 = Neal
 |authorlink3= Neal Zierler
 |title= Two-Error Correcting Bose-Chaudhuri Codes are Quasi-Perfect
 |journal= Information and Control
 |volume= 3
 |issue= 3
 |pages= 291–294
 |year= 1960
 |doi= 10.1016/s0019-9958(60)90877-9}}
* {{Citation
 |first= Rudolf
 |last= Lidl
 |first2= Günter
 |last2= Pilz
 |title= Applied Abstract Algebra
 |edition= 2nd
 |publisher= John Wiley
 |year= 1999
 |url=
 |isbn=
 |doi=}}
* {{Citation
 |first= Irving S.
 |last= Reed
 |authorlink= Irving S. Reed
 |first2= Xuemin
 |last2= Chen
 |title= Error-Control Coding for Data Networks
 |location= Boston, MA
 |publisher= [[Kluwer Academic Publishers]]
 |year= 1999
 |isbn= 0-7923-8528-4
 |doi=}}

==Further reading==
* {{Citation |last1=Blahut |first1=Richard E. |author-link1=Richard Blahut |title=Algebraic Codes for Data Transmission |edition=2nd |publisher=[[Cambridge University Press]] |year=2003 |isbn=0-521-55374-1}}
* {{Citation
 |first= W. J.
 |last= Gilbert
 |first2= W. K.
 |last2= Nicholson
 |title= Modern Algebra with Applications
 |edition= 2nd
 |publisher= John Wiley
 |year= 2004
 |url=
 |isbn=
 |doi=}}
* {{Citation
 |first= S.
 |last= Lin
 |first2= D.
 |last2= Costello
 |title= Error Control Coding: Fundamentals and Applications
 |publisher= Prentice-Hall
 |location= Englewood Cliffs, NJ
 |year= 2004
 |isbn=
 |doi= }}
* {{Citation
 |first= F. J.
 |last=MacWilliams
 |first2= N. J. A.
 |last2= Sloane
 |authorlink2= N. J. A. Sloane
 |title= The Theory of Error-Correcting Codes
 |location= New York, NY
 |publisher= North-Holland Publishing Company
 |year= 1977
 |isbn=
 |doi=}}
* {{Citation
 |first        = Atri
 |last         = Rudra
 |title        = CSE 545, Error Correcting Codes: Combinatorics, Algorithms and Applications
 |publisher    = University at Buffalo
 |url          = http://www.cse.buffalo.edu/~atri/courses/coding-theory/
 |accessdate   = April 21, 2010
 |doi          = 
 |archive-url  = https://web.archive.org/web/20100702120650/http://www.cse.buffalo.edu/~atri/courses/coding-theory/
 |archive-date = 2010-07-02
 |dead-url     = yes
 |df           = 
}}

{{DEFAULTSORT:Bch Code}}
[[Category:Error detection and correction]]
[[Category:Finite fields]]
[[Category:Coding theory]]</text>
      <sha1>q44d265lflhllghvxzbu08lkc6bspxs</sha1>
    </revision>
  </page>
  <page>
    <title>Bass diffusion model</title>
    <ns>0</ns>
    <id>6837693</id>
    <revision>
      <id>867488571</id>
      <parentid>823680910</parentid>
      <timestamp>2018-11-06T01:34:38Z</timestamp>
      <contributor>
        <username>Jdcox1999</username>
        <id>4431633</id>
      </contributor>
      <minor/>
      <comment>Updated google scholar citation count.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8800">The '''Bass Model''' or '''Bass Diffusion Model''' was developed by [[Frank Bass]].  It consists of a simple [[differential equation]] that describes the process of how new products get adopted in a population. The model presents a rationale of how current adopters and potential adopters of a new product interact. The basic premise of the model is that adopters can be classified as [[innovators]] or as imitators and the speed and timing of adoption depends on their degree of innovativeness and the degree of imitation among adopters. The Bass model has been widely used in [[forecasting]], especially new products' [[sales forecasting]] and [[technology forecasting]]. Mathematically, the basic Bass diffusion is a [[Riccati equation]] with constant coefficients.

In 1969, Frank Bass published his paper on a new product growth model for consumer durables.&lt;ref&gt;{{cite journal |journal=Management Science |accessdate=September 22, 2016 |volume=50 |issue=12 |pages=1833–1840 |url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.460.1976&amp;rep=rep1&amp;type=pdf |title=Comments on "A New Product Growth for Model Consumer Durables": The Bass Model |first=Frank M. |last=Bass |year=2004 |quote=Perhaps the first thing to notice about the paper that has come to be known as the "Bass Model" (Bass 1969) is the title. It contains a typo. The paper was published with the title: "A New Product Growth for Model Consumer Durables." The correct title should be: "A New Product Growth Model for Consumer Durables." |doi=10.1287/mnsc.1040.0300}}&lt;/ref&gt;{{rp|1833}}&lt;ref name="Bass1969"&gt;{{cite journal |last=Bass |first=Frank |authorlink=Frank Bass  |year=1969 |title= A new product growth for model consumer durables |journal=Management Science |volume=15 |issue=5 |pages=215–227 |url= |accessdate= |doi=10.1287/mnsc.15.5.215}}&lt;/ref&gt;  Prior to this, [[Everett Rogers]] published ''Diffusion of Innovations'', a highly influential work that described the different stages of product adoption. Bass contributed some mathematical ideas to the concept.&lt;ref name="MgtScienceDec2004"&gt;''Management Science'' 50 Number 12 Supplement, Dec 2004 {{issn|0025-1909}} p1833-1840&lt;/ref&gt;

==Model formulation==
:&lt;math&gt;\frac{f(t)}{1-F(t)} = p + q F(t)&lt;/math&gt; &lt;ref name="Bass1969" /&gt;

Where:
* &lt;math&gt;\ f(t) &lt;/math&gt; is the change of the installed base fraction
* &lt;math&gt;\ F(t) &lt;/math&gt; is the installed base fraction
* &lt;math&gt;\ p &lt;/math&gt; is the coefficient of innovation
* &lt;math&gt;\ q &lt;/math&gt; is the coefficient of imitation

Sales &lt;math&gt;\ S(t) &lt;/math&gt; is the rate of change of installed base (i.e. adoption) &lt;math&gt;\ f(t) &lt;/math&gt; multiplied by the ultimate market potential &lt;math&gt;\ m &lt;/math&gt;:

:&lt;math&gt;\ S(t)=mf(t) &lt;/math&gt;
:&lt;math&gt;\ S(t)=m{ \frac{(p+q)^2}{p}} \frac{e^{-(p+q)t}}{(1+\frac{q}{p}e^{-(p+q)t})^2} &lt;/math&gt; &lt;ref name="Bass1969" /&gt;

The time of peak sales &lt;math&gt;\ t^* &lt;/math&gt;

: &lt;math&gt;\ t^*=\frac{\ln q - \ln p}{p+q} &lt;/math&gt;  &lt;ref name="Bass1969" /&gt;

===Explanation===
The coefficient ''p'' is called the coefficient of innovation, external influence or advertising effect.  The coefficient q is called the coefficient of imitation, internal influence or word-of-mouth effect.

Typical values of ''p'' and ''q'' when time ''t'' is measured in years:&lt;ref name="Empirical Generalizations"&gt;{{cite journal |last=Mahajan |first=Vijay |author2=Muller, Eitan |author3=Bass, Frank |year=1995 |title=Diffusion of new products: Empirical generalizations and managerial uses |journal=Marketing Science |volume=14 |issue=3 |pages=G79–G88 |url= |accessdate= |doi=10.1287/mksc.14.3.G79 }}&lt;/ref&gt;
*The average value of ''p'' has been found to be 0.03, and is often less than 0.01
*The average value of ''q'' has been found to be 0.38, with a typical range between 0.3 and 0.5

[[image:Bass adopters.svg]]
[[image:Bass new adopters.svg]]

==Extensions to the model==
===Generalised Bass model (with pricing)===

Bass found that his model fit the data for almost all product introductions, despite a wide range of managerial decision variables, e.g. pricing and advertising.  This means that decision variables can shift the Bass curve in time, but that the shape of the curve is always similar.

Although many extensions of the model have been proposed, only one of these reduces to the Bass model under ordinary circumstances.&lt;ref name=Bass1994&gt;{{cite journal|last=Bass|first=Frank M.|author2=Trichy V. Krishnan |author3=Dipak C. Jain |title=Why the Bass Model Fits without Decision Variables|journal=Marketing Science|date=1994|volume=13|issue=2|pages=203–223|doi=10.1287/mksc.13.3.203}}&lt;/ref&gt; [[image:Bass diffusion model.svg|right]] This model was developed in 1994 by Frank Bass, Trichy Krishnan and Dipak Jain:

:&lt;math&gt;\frac{f(t)}{1-F(t)} = (p + {q}F(t)) x(t)&lt;/math&gt;

where &lt;math&gt;\ x(t) &lt;/math&gt; is a function of percentage change in price and other variables

===Successive generations===
Technology products succeed one another in generations.  Norton and Bass extended the model in 1987 for sales of products with continuous repeat purchasing.  The formulation for three generations is as follows:&lt;ref&gt;{{cite journal|last=Norton|first=John A.|author2=Frank M. Bass|title=A Diffusion Theory Model of Adoption and Substitution for Successive Generations of High-Technology Products|journal=Management Science|date=1987|volume=33|issue=9|pages=1069–1086|doi=10.1287/mnsc.33.9.1069}}&lt;/ref&gt; 

: &lt;math&gt;\ S_{1,t} = F(t_1) m_1 (1-F(t_2)) &lt;/math&gt;
: &lt;math&gt;\ S_{2,t} = F(t_2) (m_2 + F(t_1) m_1 ) (1-F(t_3)) &lt;/math&gt;
: &lt;math&gt;\ S_{3,t} = F(t_3) (m_3 + F(t_2) (m_2 + F(t_1) m_1 )) &lt;/math&gt;

where 
* &lt;math&gt;\ m_i = a_i M_i &lt;/math&gt;
* &lt;math&gt;\ M_i &lt;/math&gt; is the incremental number of ultimate adopters of the ''i''th generation product
* &lt;math&gt;\ a_i &lt;/math&gt; is the average (continuous) repeat buying rate among adopters of the ''i''th generation product
* &lt;math&gt;\ t_i &lt;/math&gt; is the time since the introduction of the ''i''th generation product
* &lt;math&gt;\ F(t_i) = \frac{1-e^{-(p+q)t_i}}{1+\frac{q}{p} e^{-(p+q)t_i}} &lt;/math&gt;

It has been found that the p and q terms are generally the same between successive generations.

===Relationship with other s-curves===
There are two special cases of the Bass diffusion model.

*The first special case occurs when q=0, when the model reduces to the [[Exponential distribution]].
*The second special case reduces to the [[logistic distribution]], when p=0.

The Bass model is a special case of the Gamma/[[shifted Gompertz distribution]] (G/SG): Bemmaor&lt;ref&gt;{{Cite book | last=Bemmaor | given=Albert C. | year= 1994 |pages=201–223| chapter=Modeling the Diffusion of New Durable Goods: Word-of-Mouth Effect Versus Consumer Heterogeneity | editor=G. Laurent, G.L. Lilien &amp; B. Pras | title=Research Traditions in Marketing | publisher=Kluwer Academic Publishers | place=Boston| isbn=0-7923-9388-0}}&lt;/ref&gt; (1994) 

'''Use in online social networks'''
The rapid, recent (as of early 2007) growth in online social networks (and other [[virtual communities]]) has led to an increased use of the Bass diffusion model.  The Bass diffusion model is used to estimate the size and growth rate of these social networks. The work by Christian Bauchkage and co-authors&lt;ref&gt;{{cite arXiv |last= |first= |author-link= |eprint=1406.6529 |title=Strong Regularities in Growth and Decline of Popularity of Social Media Services|class= cs.SI|date= |last1= Bauckhage|first1= Christian|last2= Kersting|first2= Kristian|year= 2014}}&lt;/ref&gt; shows that the Bass model provides a more pessimistic picture of the future than alternative model(s) such as the Weibull distribution and the shifted Gompertz distribution.

==Adoption of this Model ==
The model is one of the most cited empirical generalizations in marketing; as of October 2018 the paper "A New Product Growth for Model Consumer Durables" published in Management Science had (approximately) 8499 citations in Google Scholar&lt;ref&gt;https://scholar.google.com/scholar?hl=en&amp;as_sdt=0%2C45&amp;q=A+New+Product+Growth+for+Model+Consumer+Durables&amp;btnG=&lt;/ref&gt;. 

This model has been widely influential in marketing and management science.  In 2004 it was selected as one of the ten most frequently cited papers in the 50-year history of ''Management Science''.&lt;ref name=MgtScienceDec2004 /&gt;  It was ranked number five, and the only marketing paper in the list.  It was subsequently reprinted in the December 2004 issue of Management Science.&lt;ref name=MgtScienceDec2004 /&gt;

==See also==
*[[Diffusion of innovation]]
*[[Forecasting]]
*[[Shifted Gompertz distribution]]
*[[Lazy User Model]]

==References==
{{Reflist}}

== External links ==
* [http://www.bassbasement.org/BassModel/ Frank M. Bass Official Website]

{{DEFAULTSORT:Bass Diffusion Model}}
[[Category:Applied mathematics]]
[[Category:Innovation]]
[[Category:Market segmentation]]
[[Category:Product lifecycle management]]</text>
      <sha1>kjv1hzso790mjtp712pcaiaby9ejtzk</sha1>
    </revision>
  </page>
  <page>
    <title>Bernoulli polynomials of the second kind</title>
    <ns>0</ns>
    <id>58580000</id>
    <revision>
      <id>863164940</id>
      <parentid>863164567</parentid>
      <timestamp>2018-10-09T03:04:18Z</timestamp>
      <contributor>
        <ip>31.172.236.233</ip>
      </contributor>
      <comment>Minor edit</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8538">The '''Bernoulli polynomials of the second kind'''&lt;ref name="jordan2" /&gt;&lt;ref name="jordan"&gt;{{cite book|last1=Jordan |first1=Charles |title=The Calculus of Finite Differences (3rd Edition) |year=1965 |publisher=Chelsea Publishing Company}}&lt;/ref&gt; {{math|''&amp;psi;&lt;sub&gt;n''&lt;/sub&gt;(''x'')}}, also known as the '''Fontana-Bessel polynomials'''&lt;ref name="blagouchine"&gt; {{citation |last1= Blagouchine|first1= Iaroslav V.|title= Three notes on Ser's and Hasse's representations for the zeta-functions |journal= Integers (Electronic Journal of Combinatorial Number Theory) |date= 2018 |volume= 18A|issue= #A3|pages= 1–45|url=http://math.colgate.edu/~integers/sjs3/sjs3.pdf}} [https://arxiv.org/abs/1606.02044 arXiv]&lt;/ref&gt;, are the polynomials defined by the following generating function:

: &lt;math&gt;
\frac{z(1+z)^x}{\ln(1+z)}= \sum_{n=0}^\infty z^n \psi_n(x) ,\qquad |z|&lt;1.
&lt;/math&gt;

The first five polynomials are:

:&lt;math&gt;
\begin{array}{l}
\displaystyle \psi_0(x)=1  \\[2mm]
\displaystyle \psi_1(x)=x+\frac12  \\[2mm]
\displaystyle \psi_2(x)=\frac12x^2-\frac{1}{12}\\[2mm]
\displaystyle \psi_3(x)=\frac16x^3-\frac14x^2+\frac{1}{24}\\[2mm]
\displaystyle \psi_4(x)=\frac{1}{24}x^4-\frac16x^3+\frac16x^2 -\frac{19}{720}
\end{array}
&lt;/math&gt;

Some authors define these polynomials slightly differently&lt;ref name="roman"&gt;{{cite book|last1=Roman |first1=S. |title=The Umbral Calculus |year=1984 |publisher=New York: Academic Press}}&lt;/ref&gt;&lt;ref&gt;{{cite book|last1=Weisstein |first1= Eric W. |title=Bernoulli Polynomial of the Second Kind |publisher=From MathWorld--A Wolfram Web Resource | url=http://mathworld.wolfram.com/BernoulliPolynomialoftheSecondKind.html}}&lt;/ref&gt;

: &lt;math&gt;
\frac{z(1+z)^x}{\ln(1+z)}= \sum_{n=0}^\infty \frac{z^n}{n!} \psi^*_n(x) ,\qquad |z|&lt;1,
&lt;/math&gt;

so that 

:&lt;math&gt;
\psi^*_n(x)= \psi_n(x)\, n!
&lt;/math&gt;

and may also use a different notation for them (the most used alternative notation is {{math|''b&lt;sub&gt;n''&lt;/sub&gt;(''x'')}}).

The Bernoulli polynomials of the second kind were largely studied by the Hungarian mathematician Charles Jordan,&lt;ref name="jordan2"&gt; {{citation |last1= Jordan|first1= Charles|title= Sur des polynomes analogues aux polynomes de Bernoulli, et sur des formules de sommation analogues à celle de Maclaurin-Euler |journal= Acta Sci. Math. (Szeged) |date= 1928 |volume= 4 |pages= 130–150}}&lt;/ref&gt;&lt;ref name="jordan" /&gt; but their history may also be traced back to the much earlier works&lt;ref name="blagouchine" /&gt;.

==Integral representations==
The Bernoulli polynomials of the second kind may be represented via these integrals&lt;ref name="jordan2" /&gt;&lt;ref name="jordan" /&gt;

: &lt;math&gt;
\psi_{n}(x) = \int\limits_x^{x+1}\! \binom{u}{n}\,  du = \int\limits_0^1 \binom{x+u}{n}\,  du
&lt;/math&gt;

as well as&lt;ref name="blagouchine" /&gt; 

:&lt;math&gt;
\begin{array}{l}
\displaystyle \psi_{n}(x)=\frac{(-1)^{n+1}}{\pi} 
\int\limits_0^\infty  \frac{\pi \cos\pi x - \sin\pi x \ln z}{(1+z)^n} \cdot\frac{z^x  dz}{\ln^2 z +\pi^2}
,\qquad  -1\leq x\leq n-1\, \\[3mm]
\displaystyle \psi_{n}(x)=\frac{(-1)^{n+1}}{\pi} 
\int\limits_{-\infty}^{+\infty} \frac{\pi \cos\pi x - v\sin\pi x }{\,(1+e^v)^n} 
\cdot\frac{e^{v(x+1)} }{v^2 +\pi^2}\, dv ,\qquad  -1\leq x\leq n-1\,
\end{array}
&lt;/math&gt;

These polynomials are, therefore, up to a constant, the [[antiderivative]] of the [[binomial coefficient]] and also that of the [[falling factorial]].&lt;ref name="jordan2" /&gt;&lt;ref name="jordan" /&gt;&lt;ref name="blagouchine" /&gt;

==Explicit formula==
For an arbitrary {{math|''n''}}, these polynomials may be computed explicitly via the following summation formula&lt;ref name="jordan2" /&gt;&lt;ref name="jordan" /&gt;&lt;ref name="blagouchine" /&gt;

:&lt;math&gt;
\psi_{n}(x) =
\frac{1}{(n-1)!}\sum_{l=0}^{n-1} \frac{s(n-1,l)}{l+1} x^{l+1} + G_{n},\qquad n=1,2,3,\ldots
&lt;/math&gt;

where where {{math|''s''(''n'',''l'')}} are the signed [[Stirling numbers of the first kind]] and {{math|''G''&lt;sub&gt;''n''&lt;/sub&gt;}} are the [[Gregory coefficients]].

==Recurrence formula==
The Bernoulli polynomials of the second kind satisfy the recurrence relation&lt;ref name="jordan2" /&gt;&lt;ref name="jordan" /&gt;

:&lt;math&gt;
\psi_{n}(x+1) - \psi_{n}(x) = \psi_{n-1}(x)
&lt;/math&gt;

or equivalently 

:&lt;math&gt;
\Delta\psi_{n}(x) = \psi_{n-1}(x)
&lt;/math&gt;

The repeated difference produces&lt;ref name="jordan2" /&gt;&lt;ref name="jordan" /&gt;

:&lt;math&gt;
\Delta^m\psi_{n}(x) = \psi_{n-m}(x)
&lt;/math&gt;

==Symmetry property==
The main property of the symmetry reads&lt;ref name="jordan" /&gt;&lt;ref name="roman" /&gt;

:&lt;math&gt;
\psi_{n}(\tfrac12n-1+x) = (-1)^n\psi_{n}(\tfrac12n-1-x) 
&lt;/math&gt;

==Some further properties and particular values==
Some properties and particular values of these polynomials include 
:&lt;math&gt;
\begin{array}{l}
\displaystyle \psi_n(0)=G_n  \\[2mm]
\displaystyle \psi_n(1)=G_{n-1} + G_{n} \\[2mm]
\displaystyle \psi_n(-1)= (-1)^{n+1} \sum_{m=0}^n  |G_m| = (-1)^n C_n\\[2mm]
\displaystyle \psi_n(n-2)=-|G_n|  \\[2mm]
\displaystyle \psi_n(n-1)= (-1)^n \psi_n(-1) = 1- \sum_{m=1}^n  |G_m|\\[2mm]
\displaystyle \psi_{2n}(n-1)=M_{2n} \\[2mm]
\displaystyle \psi_{2n}(n-1+y)=\psi_{2n}(n-1-y) \\[2mm]
\displaystyle \psi_{2n+1}(n-\tfrac12+y)=-\psi_{2n+1}(n-\tfrac12-y) \\[2mm]
\displaystyle \psi_{2n+1}(n-\tfrac12)=0 
\end{array}
&lt;/math&gt;

where {{math|''C''&lt;sub&gt;''n''&lt;/sub&gt;}} are the ''Cauchy numbers of the second kind'' and {{math|''M''&lt;sub&gt;''n''&lt;/sub&gt;}} are the ''central difference coefficients''.&lt;ref name="jordan2" /&gt;&lt;ref name="jordan" /&gt;&lt;ref name="blagouchine" /&gt;

==Expansion into a Newton series==
The expansion of the Bernoulli polynomials of the second kind into a Newton series reads&lt;ref name="jordan2" /&gt;&lt;ref name="jordan" /&gt;

:&lt;math&gt;
\psi_{n}(x) = G_0 \binom{x}{n} + G_1 \binom{x}{n-1} + G_2 \binom{x}{n-2} + \ldots + G_n
&lt;/math&gt;

==Some series involving the Bernoulli polynomials of the second kind==
The [[digamma function]] {{math|&amp;Psi;(''x'')}} may be expanded into a series with the Bernoulli polynomials of the second kind
in the following way&lt;ref name="blagouchine" /&gt;

:&lt;math&gt;
\Psi(v)=\ln(v+a) + \sum_{n=1}^\infty\frac{(-1)^n\psi_{n}(a)\,(n-1)!}{(v)_{n}},\qquad \Re(v)&gt;-a,
&lt;/math&gt;

and hence&lt;ref name="blagouchine" /&gt; 

&lt;math&gt;\gamma= -\ln(a+1) - \sum_{n=1}^\infty\frac{(-1)^n \psi_{n}(a)}{n},\qquad \Re(a)&gt;-1 &lt;/math&gt;

and 

:&lt;math&gt;\gamma=\sum_{n=1}^\infty\frac{(-1)^{n+1}}{2n}\Big\{\psi_{n}(a)+ \psi_{n}\Big(-\frac{a}{1+a}\Big)\Big\},
\quad a&gt;-1&lt;/math&gt;

where {{math|''&amp;gamma;''}} is [[Euler's constant]]. Furthermore, we also have&lt;ref name="blagouchine" /&gt;

: &lt;math&gt;
\Psi(v)= \frac{1}{v+a-\tfrac12}\left\{\ln\Gamma(v+a) + v - \frac12\ln2\pi - \frac12 + \sum_{n=1}^\infty\frac{(-1)^n \psi_{n+1}(a)}{(v)_{n}}(n-1)!\right\},\qquad \Re(v)&gt;-a, 
&lt;/math&gt;

where {{math|&amp;Gamma;(''x'')}} is the [[gamma function]]. The [[Hurwitz zeta function|Hurwitz]] and [[Riemann zeta function|Riemann zeta functions]] may be expanded into these
polynomials as follows&lt;ref name="blagouchine" /&gt;

: &lt;math&gt;
\zeta(s,v)= \frac{(v+a)^{1-s} }{s-1} +  \sum_{n=0}^\infty (-1)^n \psi_{n+1}(a)
\sum_{k=0}^{n} (-1)^k \binom{n}{k} (k+v)^{-s}
&lt;/math&gt;

and

: &lt;math&gt;
\zeta(s)= \frac{(a+1)^{1-s} }{s-1} +  \sum_{n=0}^\infty (-1)^n \psi_{n+1}(a)
\sum_{k=0}^{n} (-1)^k \binom{n}{k} (k+1)^{-s}
&lt;/math&gt;

and also

: &lt;math&gt;
\zeta(s) =1 + \frac{(a+2)^{1-s}}{s-1} +  \sum_{n=0}^\infty (-1)^n \psi_{n+1}(a)
\sum_{k=0}^{n} (-1)^k \binom{n}{k} (k+2)^{-s}
&lt;/math&gt;

The Bernoulli polynomials of the second kind are also involved in the following relationship&lt;ref name="blagouchine" /&gt;

: &lt;math&gt;
\big(v+a-\tfrac{1}{2}\big)\zeta(s,v) = -\frac{\zeta(s-1,v+a)}{s-1} + \zeta(s-1,v) +  
 \sum_{n=0}^\infty (-1)^n \psi_{n+2}(a) \sum_{k=0}^{n} (-1)^k \binom{n}{k} (k+v)^{-s} 
&lt;/math&gt;

between the zeta functions, as well as in various formulas for the [[Stieltjes constants]], e.g.&lt;ref name="blagouchine" /&gt;

: &lt;math&gt;
\gamma_m(v)=-\frac{\ln^{m+1}(v+a)}{m+1} + \sum_{n=0}^\infty (-1)^n \psi_{n+1}(a)
\sum_{k=0}^{n} (-1)^k \binom{n}{k}\frac{\ln^m (k+v)}{k+v}
&lt;/math&gt;

and 

: &lt;math&gt;
 \gamma_m(v)=\frac{1}{\tfrac{1}{2}-v-a}
\left\{\frac{(-1)^m}{m+1}\,\zeta^{(m+1)}(0,v+a)- (-1)^m \zeta^{(m)}(0,v)
- \sum_{n=0}^\infty (-1)^n \psi_{n+2}(a) 
\sum_{k=0}^{n} (-1)^k \binom{n}{k}\frac{\ln^m (k+v)}{k+v}\right\} 
&lt;/math&gt;

which are both valid for &lt;math&gt;\Re(a) &gt; -1&lt;/math&gt; and &lt;math&gt;v\in\mathbb{C}\setminus\!\{0,-1,-2,\ldots\}&lt;/math&gt;.

== See also==
* [[Bernoulli polynomials]]
* [[Stirling polynomials]]
* [[Gregory coefficients]]
* [[Bernoulli numbers]]
* [[Difference polynomials]]
* [[Poly-Bernoulli number]]
* [[Mittag-Leffler polynomials]]

== References ==
{{Reflist|2}}

[[Category:Polynomials]]
[[Category:Number theory]]
[[Category:Number theory]]

== Mathematics ==</text>
      <sha1>0byr56341336kefc09dolp8r6mm4rme</sha1>
    </revision>
  </page>
  <page>
    <title>Bipartite double cover</title>
    <ns>0</ns>
    <id>21241712</id>
    <revision>
      <id>742022224</id>
      <parentid>727034693</parentid>
      <timestamp>2016-10-01T05:19:44Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>[[Category:Bipartite graphs]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9794">In [[graph theory]], the '''bipartite double cover''' of an [[undirected graph]] ''G'' is a [[bipartite graph|bipartite]] [[covering graph]] of ''G'', with twice as many vertices as ''G''. It can be constructed as the [[tensor product of graphs]], ''G'' × ''K''&lt;sub&gt;2&lt;/sub&gt;. It is also called the '''Kronecker double cover''', '''canonical double cover''' or simply the '''bipartite double''' of ''G''.

It should not be confused with a [[cycle double cover conjecture|cycle double cover]] of a graph, a family of cycles that includes each edge twice.

==Construction==
The bipartite double cover of ''G'' has two vertices ''u&lt;sub&gt;i&lt;/sub&gt;'' and ''w&lt;sub&gt;i&lt;/sub&gt;'' for each vertex ''v&lt;sub&gt;i&lt;/sub&gt;'' of ''G''. Two vertices ''u&lt;sub&gt;i&lt;/sub&gt;'' and ''w&lt;sub&gt;j&lt;/sub&gt;'' are connected by an edge in the double cover if and only if ''v&lt;sub&gt;i&lt;/sub&gt;'' and ''v&lt;sub&gt;j&lt;/sub&gt;'' are connected by an edge in ''G''. For instance, below is an illustration of a bipartite double cover of a non-bipartite graph ''G''. In the illustration, each vertex in the tensor product is shown using a color from the first term of the product (''G'') and a shape from the second term of the product (''K''&lt;sub&gt;2&lt;/sub&gt;); therefore, the vertices ''u&lt;sub&gt;i&lt;/sub&gt;'' in the double cover are shown as circles while the vertices ''w&lt;sub&gt;i&lt;/sub&gt;'' are shown as squares.

:[[Image:Covering-graph-2.svg]]

The bipartite double cover may also be constructed using adjacency matrices (as described below) or as the derived graph of a [[voltage graph]] in which each edge of ''G'' is labeled by the nonzero element of the two-element [[group (mathematics)|group]].

==Examples==
The bipartite double cover of the [[Petersen graph]] is the [[Desargues graph]]: ''K''&lt;sub&gt;2&lt;/sub&gt; × ''G''(5,2) = ''G''(10,3).

The bipartite double cover of a [[complete graph]] ''K&lt;sub&gt;n&lt;/sub&gt;'' is a [[crown graph]] (a [[complete bipartite graph]] ''K&lt;sub&gt;n&lt;/sub&gt;''&lt;sub&gt;,''n''&lt;/sub&gt; minus a [[perfect matching]]). In particular, the bipartite double cover of the graph of a [[tetrahedron]], ''K''&lt;sub&gt;4&lt;/sub&gt;, is the graph of a [[cube]].

The bipartite double cover of an odd-length [[cycle graph]] is a cycle of twice the length, while the bipartite double of any bipartite graph (such as an even length cycle, shown in the following example) is formed by two disjoint copies of the original graph.

:[[Image:Covering-graph-1.svg]]

==Matrix interpretation==
If an undirected graph ''G'' has a matrix ''A'' as its [[adjacency matrix]], then the adjacency matrix of the double cover of ''G'' is
:&lt;math&gt;\left[\begin{matrix}0&amp;A\\A^T&amp;0\end{matrix}\right],&lt;/math&gt;
and the [[biadjacency matrix]] of the double cover of ''G'' is just ''A'' itself. That is, the conversion from a graph to its double cover can be performed simply by reinterpreting ''A'' as a biadjacency matrix instead of as an adjacency matrix. More generally, the reinterpretation the adjacency matrices of [[directed graph]]s as biadjacency matrices provides a [[bijective proof|combinatorial equivalence]] between directed graphs and balanced bipartite graphs.&lt;ref&gt;{{harvtxt|Dulmage|Mendelsohn|1958}}; {{harvtxt|Brualdi|Harary|Miller|1980}}.&lt;/ref&gt;

==Properties==
The bipartite double cover of any graph ''G'' is a [[bipartite graph]]; both parts of the bipartite graph have one vertex for each vertex of ''G''. A bipartite double cover is [[connected graph|connected]] if and only if ''G'' is connected and non-bipartite.&lt;ref&gt;{{harvtxt|Brualdi|Harary|Miller|1980}}, Theorem 3.4.&lt;/ref&gt;

The bipartite double cover is a special case of a ''double cover'' (a 2-fold [[covering graph]]). A double cover in graph theory can be viewed as a special case of a [[Double cover (topology)|topological double cover]].

If ''G'' is a non-bipartite [[symmetric graph]], the double cover of ''G'' is also a symmetric graph; several known [[cubic graph|cubic]] symmetric graphs may be obtained in this way. For instance, the double cover of ''K''&lt;sub&gt;4&lt;/sub&gt; is the graph of a cube; the double cover of the Petersen graph is the Desargues graph; and the double cover of the graph of the [[dodecahedron]] is a 40-vertex symmetric cubic graph.&lt;ref&gt;{{harvtxt|Feng|Kutnar|Malnič|Marušic|2008}}.&lt;/ref&gt;

It is possible for two different graphs to have [[graph isomorphism|isomorphic]] bipartite double covers. For instance, the Desargues graph is not only the bipartite double cover of the Petersen graph, but is also the bipartite double cover of a different graph that is not isomorphic to the Petersen graph.&lt;ref name="ip08"&gt;{{harvtxt|Imrich|Pisanski|2008}}.&lt;/ref&gt; Not every bipartite graph is a bipartite double cover of another graph; for a bipartite graph ''G'' to be the bipartite cover of another graph, it is necessary and sufficient that the [[graph automorphism|automorphism]]s of ''G'' include an [[Involution (mathematics)|involution]] that maps each vertex to a distinct and non-adjacent vertex.&lt;ref name="ip08"/&gt; For instance, the graph with two vertices and one edge is bipartite but is not a bipartite double cover, because it has no non-adjacent pairs of vertices to be mapped to each other by such an involution; on the other hand, the graph of the cube is a bipartite double cover, and has an involution that maps each vertex to the diametrally opposite vertex. An alternative characterization of the bipartite graphs that may be formed by the bipartite double cover construction was obtained by {{harvtxt|Sampathkumar|1975}}.

==Other double covers==
In general, a graph may have multiple double covers that are different from the bipartite double cover.&lt;ref&gt;{{harvtxt|Waller|1976}}.&lt;/ref&gt; In the following figure, the graph ''C'' is a double cover of the graph ''H'':
# The graph ''C'' is a ''covering graph'' of ''H'': there is a surjective local isomorphism ''f'' from ''C'' to ''H'', the one indicated by the colours. For example, ''f'' maps both blue nodes in ''C'' to the blue node in ''H''. Furthermore, let ''X'' be the [[Neighbourhood (graph theory)|neighbourhood]] of a blue node in ''C'' and let ''Y'' be the neighbourhood of the blue node in ''H''; then the restriction of ''f'' to ''X'' is a bijection from ''X'' to ''Y''. In particular, the degree of each blue node is the same. The same applies to each colour.
# The graph ''C'' is a ''double'' cover (or ''2-fold cover'' or ''2-lift'') of ''H'': the preimage of each node in ''H'' has size 2. For example, there are exactly 2 nodes in ''C'' that are mapped to the blue node in ''H''.
However, ''C'' is not a ''bipartite'' double cover of ''H'' or any other graph; it is not a bipartite graph. 

If we replace one triangle by a square in ''H  ''the resulting graph has four distinct double covers. Two of them are bipartite but only one of them is the Kronecker cover.

:[[Image:Covering-graph-4.svg]]

As another example, the graph of the [[icosahedron]] is a double cover of the complete graph ''K''&lt;sub&gt;6&lt;/sub&gt;; to obtain a covering map from the icosahedron to ''K''&lt;sub&gt;6&lt;/sub&gt;, map each pair of opposite vertices of the icosahedron to a single vertex of ''K''&lt;sub&gt;6&lt;/sub&gt;. However, the icosahedron is not bipartite, so it is not the bipartite double cover of ''K''&lt;sub&gt;6&lt;/sub&gt;. Instead, it can be obtained as the [[orientable double cover]] of an [[graph embedding|embedding]] of ''K''&lt;sub&gt;6&lt;/sub&gt; on the [[projective plane]].

==See also==
*[[Bipartite half]]

==Notes==
{{reflist|2}}

==References==
*{{citation
 | last1 = Brualdi | first1 = Richard A.
 | last2 = Harary | first2 = Frank | author2-link = Frank Harary
 | last3 = Miller | first3 = Zevi
 | doi = 10.1002/jgt.3190040107
 | mr = 558453
 | issue = 1
 | journal = Journal of Graph Theory
 | pages = 51–73
 | title = Bigraphs versus digraphs via matrices
 | volume = 4
 | year = 1980}}.
*{{citation
 | doi = 10.4153/CJM-1958-052-0
 | last1 = Dulmage | first1 = A. L.
 | last2 = Mendelsohn | first2 = N. S. | author2-link = Nathan Mendelsohn
 | mr = 0097069
 | journal = Canadian Journal of Mathematics
 | pages = 517–534
 | title = Coverings of bipartite graphs
 | volume = 10
 | year = 1958}}. The “coverings” in the title of this paper refer to the [[vertex cover]] problem, not to bipartite double covers. However, {{harvtxt|Brualdi|Harary|Miller|1980}} cite this paper as the source of the idea of reinterpreting the adjacency matrix as a biadjacency matrix.
*{{citation
 | last1 = Feng | first1 = Yan-Quan
 | last2 = Kutnar | first2 = Klavdija
 | last3 = Malnič | first3 = Aleksander
 | last4 = Marušic | first4 = Dragan
 | doi = 10.1016/j.jctb.2007.07.001
 | arxiv = math.CO/0701722 |mr=2389602 | issue = 2
 | journal = Journal of Combinatorial Theory, Series B
 | pages = 324–341
 | title = On 2-fold covers of graphs
 | volume = 98
 | year = 2008}}.
*{{citation
 | first1 = Wilfried | last1 = Imrich
 | last2 = Pisanski | first2 = Tomaž | author2-link = Tomaž Pisanski
 | mr = 2419215
 | title = Multiple Kronecker covering graphs
 | journal = European Journal of Combinatorics
 | volume = 29 | issue = 5 | year = 2008 | pages = 1116–1122
 | doi = 10.1016/j.ejc.2007.07.001}}.
*{{citation
 | doi = 10.1017/S1446788700020619
 | last = Sampathkumar | first = E. | author1-link = E. Sampathkumar
 | mr = 0389681
 | issue = 3
 | journal = Journal of the Australian Mathematical Society, Series A, Pure Mathematics and Statistics
 | pages = 268–273
 | title = On tensor product graphs
 | volume = 20
 | year = 1975}}.
*{{citation
 | doi = 10.1017/S0004972700025053
 | last = Waller | first = Derek A.
 | mr = 0406876
 | issue = 2
 | journal = Bulletin of the Australian Mathematical Society
 | pages = 233–248
 | title = Double covers of graphs
 | volume = 14
 | year = 1976}}.

==External links==
*{{mathworld | title = Bipartite Double Graph | urlname = BipartiteDoubleGraph}}

[[Category:Graph operations]]
[[Category:Bipartite graphs]]</text>
      <sha1>gg1nctgd41okkddnmr77nmas8uvfv6g</sha1>
    </revision>
  </page>
  <page>
    <title>Birkhoff's representation theorem</title>
    <ns>0</ns>
    <id>20489385</id>
    <revision>
      <id>869311874</id>
      <parentid>785834261</parentid>
      <timestamp>2018-11-17T20:28:59Z</timestamp>
      <contributor>
        <username>Ira Leviton</username>
        <id>25046916</id>
      </contributor>
      <minor/>
      <comment>Fixed a typo found with [[Wikipedia:Typo_Team/moss]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="20977">:''This is about [[lattice theory]]. For other similarly named results, see [[Birkhoff's theorem (disambiguation)]].''
In [[mathematics]], '''Birkhoff's representation theorem''' for distributive lattices states that the elements of any [[finite set|finite]] [[distributive lattice]] can be represented as [[finite set]]s, in such a way that the lattice operations correspond to [[Union (set theory)|unions]] and [[Intersection (set theory)|intersections]] of sets. The theorem can be interpreted as providing a [[Bijection|one-to-one correspondence]] between distributive lattices and [[partial order]]s, between [[Knowledge space|quasi-ordinal knowledge spaces]] and [[preorder]]s, or between [[finite topological space]]s and preorders. It is named after [[Garrett Birkhoff]], who published a proof of it in 1937.&lt;ref name="birkhoff"&gt;{{harvtxt|Birkhoff|1937}}.&lt;/ref&gt;

The name “Birkhoff's representation theorem” has also been applied to two other results of Birkhoff, one from 1935 on the [[Boolean algebras canonically defined#Representation theorems|representation of Boolean algebras]] as families of sets closed under union, intersection, and complement (so-called ''fields of sets'', closely related to the ''rings of sets'' used by Birkhoff to represent distributive lattices), and [[Birkhoff's HSP theorem]] representing algebras as products of irreducible algebras. Birkhoff's representation theorem has also been called the '''fundamental theorem for finite distributive lattices'''.&lt;ref name="stanley"&gt;{{harv|Stanley|1997}}.&lt;/ref&gt;

== Understanding the theorem ==
Many lattices can be defined in such a way that the elements of the lattice are represented by sets, the join operation of the lattice is represented by set union, and the meet operation of the lattice is represented by set intersection. For instance, the [[Boolean lattice]] defined from the family of all subsets of a finite set has this property. More generally any [[finite topological space]] has a lattice of sets as its family of open sets. Because set unions and intersections obey the [[distributive law]], any lattice defined in this way is a distributive lattice. Birkhoff's theorem states that in fact ''all'' finite distributive lattices can be obtained this way, and later generalizations of Birkhoff's theorem state a similar thing for infinite distributive lattices.

==Examples==
[[File:Birkhoff120.svg|thumb|upright=1.8|The distributive lattice of divisors of 120, and its representation as sets of prime powers.]]
Consider the [[divisor]]s of some composite number, such as (in the figure) 120,  partially ordered by divisibility. Any two divisors of 120, such as 12 and 20, have a unique [[Greatest common divisor|greatest common factor]] 12&amp;nbsp;∧&amp;nbsp;20&amp;nbsp;=&amp;nbsp;4, the largest number that divides both of them, and a unique [[least common multiple]] 12&amp;nbsp;∨&amp;nbsp;20&amp;nbsp;=&amp;nbsp;60; both of these numbers are also divisors of 120. These two operations ∨ and ∧ satisfy the [[distributive law]], in either of two equivalent forms: (''x''&amp;nbsp;∧&amp;nbsp;''y'')&amp;nbsp;∨&amp;nbsp;''z''&amp;nbsp;=&amp;nbsp;(''x''&amp;nbsp;∨&amp;nbsp;''z'')&amp;nbsp;∧&amp;nbsp;(''y''&amp;nbsp;∨&amp;nbsp;''z'') and (''x''&amp;nbsp;∨&amp;nbsp;''y'')&amp;nbsp;∧&amp;nbsp;''z''&amp;nbsp;=&amp;nbsp;(''x''&amp;nbsp;∧&amp;nbsp;''z'')&amp;nbsp;∨&amp;nbsp;(''y''&amp;nbsp;∧&amp;nbsp;''z''), for all ''x'', ''y'', and ''z''. Therefore, the divisors form a finite [[distributive lattice]].

One may associate each divisor with the set of [[prime power]]s that divide it: thus, 12 is associated with the set {2,3,4}, while 20 is associated with the set {2,4,5}. Then 12&amp;nbsp;∧&amp;nbsp;20&amp;nbsp;=&amp;nbsp;4 is associated with the set {2,3,4}&amp;nbsp;∩&amp;nbsp;{2,4,5}&amp;nbsp;=&amp;nbsp;{2,4}, while 12&amp;nbsp;∨&amp;nbsp;20&amp;nbsp;=&amp;nbsp;60 is associated with the set {2,3,4}&amp;nbsp;∪&amp;nbsp;{2,4,5}&amp;nbsp;=&amp;nbsp;{2,3,4,5}, so the join and meet operations of the lattice correspond to union and intersection of sets. 

The prime powers 2, 3, 4, 5, and 8 appearing as elements in these sets may themselves be partially ordered by divisibility; in this smaller partial order, 2 ≤ 4 ≤ 8 and there are no order relations between other pairs. The 16 sets that are associated with divisors of 120 are the [[lower set]]s of this smaller partial order, subsets of elements such that if ''x'' ≤ ''y'' and ''y'' belongs to the subset, then ''x'' must also belong to the subset. From any lower set ''L'', one can recover the associated divisor by computing the least common multiple of the prime powers in ''L''. Thus, the partial order on the five prime powers 2, 3, 4, 5, and 8 carries enough information to recover the entire original 16-element divisibility lattice.

Birkhoff's theorem states that this relation between the operations ∧ and ∨ of the lattice of divisors and the operations ∩ and ∪ of the associated sets of prime powers is not coincidental, and not dependent on the specific properties of prime numbers and divisibility: the elements of any finite distributive lattice may be associated with lower sets of a partial order in the same way.

As another example, the application of Birkhoff's theorem to the family of [[subset]]s of an ''n''-element set, partially ordered by inclusion, produces the [[free distributive lattice]] with ''n'' generators. The number of elements in this lattice is given by the [[Dedekind number]]s.

==The partial order of join-irreducibles==
In a lattice, an element ''x'' is ''join-irreducible'' if ''x'' is not the join of a finite set of other elements. Equivalently, ''x'' is join-irreducible if it is neither the bottom element of the lattice (the join of zero elements) nor the join of any two smaller elements. For instance, in the lattice of divisors of 120, there is no pair of elements whose join is 4, so 4 is join-irreducible. An element ''x'' is ''join-prime'' if, whenever ''x''&amp;nbsp;≤&amp;nbsp;''y''&amp;nbsp;∨&amp;nbsp;''z'', either ''x''&amp;nbsp;≤&amp;nbsp;''y'' or ''x''&amp;nbsp;≤&amp;nbsp;''z''. In the same lattice, 4 is join-prime: whenever lcm(''y'',''z'') is divisible by 4, at least one of ''y'' and ''z'' must itself be divisible by 4.

In any lattice, a join-prime element must be join-irreducible. Equivalently, an element that is not join-irreducible is not join-prime. For, if an element ''x'' is not join-irreducible, there exist smaller ''y'' and ''z'' such that ''x''&amp;nbsp;=&amp;nbsp;''y''&amp;nbsp;∨&amp;nbsp;''z''. But then ''x''&amp;nbsp;≤&amp;nbsp;''y''&amp;nbsp;∨&amp;nbsp;''z'', and ''x'' is not less than or equal to either ''y'' or ''z'', showing that it is not join-prime.

There exist lattices in which the join-prime elements form a proper subset of the join-irreducible elements, but in a distributive lattice the two types of elements coincide. For, suppose that ''x'' is join-irreducible, and that ''x''&amp;nbsp;≤&amp;nbsp;''y''&amp;nbsp;∨&amp;nbsp;''z''. This inequality is equivalent to the statement that ''x''&amp;nbsp;=&amp;nbsp;''x''&amp;nbsp;∧&amp;nbsp;(''y''&amp;nbsp;∨&amp;nbsp;''z''), and by the distributive law ''x''&amp;nbsp;=&amp;nbsp;(''x''&amp;nbsp;∧&amp;nbsp;''y'')&amp;nbsp;∨&amp;nbsp;(''x''&amp;nbsp;∧&amp;nbsp;''z''). But since ''x'' is join-irreducible, at least one of the two terms in this join must be ''x'' itself, showing that either ''x''&amp;nbsp;=&amp;nbsp;''x''&amp;nbsp;∧&amp;nbsp;''y'' (equivalently ''x''&amp;nbsp;≤&amp;nbsp;''y'') or ''x''&amp;nbsp;=&amp;nbsp;''x''&amp;nbsp;∧&amp;nbsp;''z'' (equivalently ''x''&amp;nbsp;≤&amp;nbsp;''z'').

The lattice ordering on the subset of join-irreducible elements forms a [[partial order]]; Birkhoff's theorem states that the lattice itself can be recovered from the lower sets of this partial order.

==Birkhoff's theorem==
[[File:Birkhoff representation theorem.gif|thumb|upright=1.2|Distributive example lattice, with join-irreducible elements a,...,g (shadowed nodes). The lower set a node corresponds to by Birkhoff's isomorphism is shown in blue.]]
In any partial order, the [[lower set]]s form a lattice in which the lattice's partial ordering is given by set inclusion, the join operation corresponds to set union, and the meet operation corresponds to set intersection, because unions and intersections preserve the property of being a lower set. Because set unions and intersections obey the distributive law, this is a distributive lattice. Birkhoff's theorem states that any finite distributive lattice can be constructed in this way.

:'''Theorem'''. Any finite distributive lattice ''L'' is isomorphic to the lattice of lower sets of the partial order of the join-irreducible elements of ''L''.

That is, there is a one-to-one order-preserving correspondence between elements of ''L'' and lower sets of the partial order. The lower set corresponding to an element ''x'' of ''L'' is simply the set of join-irreducible elements of ''L'' that are less than or equal to ''x'', and the element of ''L'' corresponding to a lower set ''S'' of join-irreducible elements is the join of ''S''.

For any lower set ''S'' of join-irreducible elements, let ''x'' be the join of ''S'', and let ''T'' be the  lower set of the join-irreducible elements less than or equal to ''x''. Then ''S''&amp;nbsp;=&amp;nbsp;''T''. For, every element of ''S'' clearly belongs to ''T'', and any join-irreducible element less than or equal to ''x'' must (by join-primality) be less than or equal to one of the members of ''S'', and therefore must (by the assumption that ''S'' is a lower set) belong to ''S'' itself. Conversely, for any element ''x'' of ''L'', let ''S'' be the join-irreducible elements less than or equal to ''x'', and let ''y'' be the join of ''S''. Then ''x''&amp;nbsp;=&amp;nbsp;''y''. For, as a join of elements less than or equal to ''x'', ''y'' can be no greater than ''x'' itself, but if ''x'' is join-irreducible then ''x'' belongs to ''S'' while if ''x'' is the join of two or more join-irreducible items then they must again belong to ''S'', so ''y''&amp;nbsp;≥&amp;nbsp;''x''. Therefore, the correspondence is one-to-one and the theorem is proved.

==Rings of sets and preorders==
{{harvtxt|Birkhoff|1937}} defined a ''ring of sets'' to be a [[family of sets]] that is [[Closure (mathematics)|closed]] under the operations of set unions and set intersections; later, motivated by applications in [[mathematical psychology]], {{harvtxt|Doignon|Falmagne|1999}} called the same structure a ''quasi-ordinal [[knowledge space]]''. If the sets in a ring of sets are ordered by inclusion, they form a distributive lattice. The elements of the sets may be given a [[preorder]] in which ''x''&amp;nbsp;≤&amp;nbsp;''y'' whenever some set in the ring contains ''x'' but not ''y''. The ring of sets itself is then the family of lower sets of this preorder, and any preorder gives rise to a ring of sets in this way.

==Functoriality==
Birkhoff's theorem, as stated above, is a correspondence between individual partial orders and distributive lattices. However, it can also be extended to a correspondence between order-preserving functions of partial orders and [[lattice homomorphism|bounded homomorphisms]] of the corresponding distributive lattices. The direction of these maps is reversed in this correspondence.

Let '''2''' denote the partial order on the two-element set {0, 1}, with the order relation 0 &amp;lt; 1, and (following Stanley) let ''J(P)'' denote the distributive lattice of lower sets of a finite partial order ''P''. Then the elements of ''J(P)'' correspond one-for-one to the order-preserving functions from ''P'' to '''2'''.&lt;ref name="stanley"/&gt; For, if ƒ is such a function, ƒ&lt;sup&gt;−1&lt;/sup&gt;(0) forms a lower set, and conversely if ''L'' is a lower set one may define an order-preserving function ƒ&lt;sub&gt;''L''&lt;/sub&gt; that maps ''L'' to 0 and that maps the remaining elements of ''P'' to 1. If ''g'' is any order-preserving function from ''Q'' to ''P'', one may define a function ''g''* from ''J(P)'' to ''J(Q)'' that uses the [[composition of functions]] to map any element ''L'' of ''J(P)'' to  ƒ&lt;sub&gt;''L''&lt;/sub&gt;&amp;nbsp;∘&amp;nbsp;''g''. This composite function maps ''Q'' to '''2''' and therefore corresponds to an element ''g''*(''L'')&amp;nbsp;=&amp;nbsp;(ƒ&lt;sub&gt;''L''&lt;/sub&gt;&amp;nbsp;∘&amp;nbsp;''g'')&lt;sup&gt;−1&lt;/sup&gt;(0) of ''J(Q)''. Further, for any ''x'' and ''y'' in ''J(P)'', ''g''*(''x''&amp;nbsp;∧&amp;nbsp;''y'')&amp;nbsp;=&amp;nbsp;''g''*(''x'')&amp;nbsp;∧&amp;nbsp;''g''*(''y'') (an element of ''Q'' is mapped by ''g'' to the lower set ''x''&amp;nbsp;∩&amp;nbsp;''y'' if and only if belongs both to the set of elements mapped to ''x'' and the set of elements mapped to ''y'') and symmetrically ''g''*(''x''&amp;nbsp;∨&amp;nbsp;''y'')&amp;nbsp;=&amp;nbsp;''g''*(''x'')&amp;nbsp;∨&amp;nbsp;''g''*(''y''). Additionally, the bottom element of ''J(P)'' (the function that maps all elements of ''P'' to 0) is mapped by ''g''* to the bottom element of ''J(Q)'', and the top element of ''J(P)'' is mapped by ''g''* to the top element of ''J(Q)''. That is, ''g''* is a homomorphism of bounded lattices.

However, the elements of ''P'' themselves correspond one-for-one with bounded lattice homomorphisms from ''J(P)'' to '''2'''. For, if ''x'' is any element of ''P'', one may define a bounded lattice homomorphism ''j&lt;sub&gt;x&lt;/sub&gt;'' that maps all lower sets containing ''x'' to 1 and all other lower sets to 0. And, for any lattice homomorphism from ''J(P)'' to '''2''', the elements of ''J(P)'' that are mapped to 1 must have a unique minimal element ''x'' (the meet of all elements mapped to 1), which must be join-irreducible (it cannot be the join of any set of elements mapped to 0), so every lattice homomorphism has the form ''j&lt;sub&gt;x&lt;/sub&gt;'' for some ''x''. Again, from any bounded lattice homomorphism ''h'' from ''J(P)'' to ''J(Q)'' one may use  composition of functions to define an order-preserving map ''h''* from ''Q'' to ''P''. It may be verified that ''g''**&amp;nbsp;=&amp;nbsp;''g'' for any order-preserving map ''g'' from ''Q'' to ''P'' and that and ''h''**&amp;nbsp;=&amp;nbsp;''h'' for any bounded lattice homomorphism ''h'' from ''J(P)'' to ''J(Q)''.

In [[category theory|category theoretic]] terminology, ''J'' is a [[Hom-functor|contravariant hom-functor]] ''J''&amp;nbsp;=&amp;nbsp;Hom(—,'''2''') that defines a [[Equivalence of categories|duality of categories]] between, on the one hand, the category of finite partial orders and order-preserving maps, and on the other hand the category of finite distributive lattices and bounded lattice homomorphisms.

==Generalizations==
In an infinite distributive lattice, it may not be the case that the lower sets of the join-irreducible elements are in one-to-one correspondence with lattice elements. Indeed, there may be no join-irreducibles at all. This happens, for instance, in the lattice of all natural numbers, ordered with the reverse of the usual divisibility ordering (so ''x''&amp;nbsp;≤&amp;nbsp;''y'' when ''y'' divides ''x''): any number ''x'' can be expressed as the join of numbers ''xp'' and ''xq'' where ''p'' and ''q'' are distinct [[prime number]]s. However, elements in infinite distributive lattices may still be represented as sets via [[Stone's representation theorem]] for distributive lattices, a form of [[Stone duality]] in which each lattice element corresponds to a [[compact space|compact]] [[open set]] in a certain [[topological space]]. This generalized representation theorem can be expressed as a [[category theory|category-theoretic]] [[Equivalence of categories|duality]] between distributive lattices and [[spectral space]]s (sometimes called coherent spaces, but not the same as the [[coherent space|coherent spaces in linear logic]]), topological spaces in which the compact open sets are closed under intersection and form a [[Base (topology)|base]] for the topology.&lt;ref&gt;{{harvtxt|Johnstone|1982}}.&lt;/ref&gt; [[Hilary Priestley]] showed that Stone's representation theorem could be interpreted as an extension of the idea of representing lattice elements by lower sets of a partial order, using Nachbin's idea of ordered topological spaces. Stone spaces with an additional partial order linked with the topology via [[Priestley space|Priestley separation axiom]] can also be used to represent bounded distributive lattices. Such spaces are known as [[Priestley space]]s. Further, certain [[bitopological space]]s, namely [[pairwise Stone space]]s, generalize Stone's original approach by utilizing ''two'' topologies on a set to represent an abstract distributive lattice. Thus, Birkhoff's representation theorem extends to the case of infinite (bounded) distributive lattices in at least three different ways, summed up in [[duality theory for distributive lattices]].

Birkhoff's representation theorem may also be generalized to finite structures other than distributive lattices. In a distributive lattice, the self-dual median operation&lt;ref&gt;{{harvtxt|Birkhoff|Kiss|1947}}.&lt;/ref&gt;
:&lt;math&gt;m(x,y,z)=(x\vee y)\wedge(x\vee z)\wedge(y\vee z)=(x\wedge y)\vee(x\wedge z)\vee(y\wedge z)&lt;/math&gt;
gives rise to a [[median algebra]], and the covering relation of the lattice forms a [[median graph]]. Finite median algebras and median graphs have a dual structure
as the set of solutions of a [[2-satisfiability]] instance; {{harvtxt|Barthélemy|Constantin|1993}} formulate this structure equivalently as the family of initial [[Independent set (graph theory)|stable sets]] in a [[mixed graph]].&lt;ref&gt;A minor difference between the 2-SAT and initial stable set formulations is that the latter presupposes the choice of a fixed base point from the median graph that corresponds to the empty initial stable set.&lt;/ref&gt; For a distributive lattice, the corresponding mixed graph has no undirected edges, and the initial stable sets are just the lower sets of the [[transitive closure]] of the graph. Equivalently, for a distributive lattice, the [[implication graph]] of the 2-satisfiability instance can be partitioned into two [[Connected component (graph theory)|connected components]], one on the positive variables of the instance and the other on the negative variables; the transitive closure of the positive component is the underlying partial order of the distributive lattice.

Another result analogous to Birkhoff's representation theorem, but applying to a broader class of lattices, is the theorem of {{harvtxt|Edelman|1980}} that any finite join-distributive lattice may be represented as an [[antimatroid]], a family of sets closed under unions but in which closure under intersections has been replaced by the property that each nonempty set has a removable element.

==Notes==
{{reflist|2}}

==References==
*{{citation
 | last1 = Barthélemy | first1 = J.-P.
 | last2 = Constantin | first2 = J.
 | doi = 10.1016/0012-365X(93)90140-O
 | issue = 1–3
 | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]
 | pages = 49–63
 | title = Median graphs, parallelism and posets
 | volume = 111
 | year = 1993}}.
*{{citation
 | last = Birkhoff | first = Garrett | authorlink = Garrett Birkhoff
 | doi = 10.1215/S0012-7094-37-00334-X
 | issue = 3
 | journal = Duke Mathematical Journal
 | pages = 443–454
 | title = Rings of sets
 | volume = 3
 | year = 1937}}.
*{{citation
 | last1 = Birkhoff | first1 = Garrett | author1-link = Garrett Birkhoff
 | last2 = Kiss | first2 = S. A.
 | mr = 0021540
 | issue = 1
 | journal = Bulletin of the American Mathematical Society
 | pages = 749–752
 | title = A ternary operation in distributive lattices
 | url = http://projecteuclid.org/euclid.bams/1183510977 | doi = 10.1090/S0002-9904-1947-08864-9
 | volume = 53
 | year = 1947}}.
*{{citation
 | last1 = Doignon | first1 = J.-P.
 | last2 = Falmagne | first2 = J.-Cl. | author2-link = Jean-Claude Falmagne
 | isbn = 3-540-64501-2
 | publisher = Springer-Verlag
 | title = Knowledge Spaces
 | year = 1999}}.
*{{citation
 | last = Edelman | first = Paul H.
 | doi = 10.1007/BF02482912
 | issue = 1
 | journal = Algebra Universalis
 | pages = 290–299
 | title = Meet-distributive lattices and the anti-exchange closure
 | volume = 10
 | year = 1980}}.
*{{citation
 | last = Johnstone | first = Peter | author-link = Peter Johnstone (mathematician)
 | isbn = 978-0-521-33779-3
 | publisher = Cambridge University Press
 | title = Stone Spaces
 | contribution = II.3 Coherent locales
 | pages = 62–69
 | year = 1982}}.
*{{citation
 | doi = 10.1112/blms/2.2.186
 | last = Priestley | first = H. A.
 | journal = Bulletin of the London Mathematical Society
 | pages = 186–190
 | title = Representation of distributive lattices by means of ordered Stone spaces
 | issue = 2
 | volume = 2
 | year = 1970}}.
*{{citation
 | doi = 10.1112/plms/s3-24.3.507
 | last = Priestley | first = H. A.
 | journal = Proceedings of the London Mathematical Society
 | pages = 507–530
 | title = Ordered topological spaces and the representation of distributive lattices
 | issue = 3
 | volume = 24
 | year = 1972}}.
*{{citation
 | last = Stanley | first = R. P. | author-link = Richard P. Stanley
 | title = Enumerative Combinatorics, Volume I
 | series = Cambridge Studies in Advanced Mathematics 49
 | publisher = Cambridge University Press
 | year = 1997
 | pages = 104–112}}.

[[Category:Lattice theory]]
[[Category:Theorems in algebra]]</text>
      <sha1>o187t5m05ahxk8t1frybyhbutajrwan</sha1>
    </revision>
  </page>
  <page>
    <title>Cantamath</title>
    <ns>0</ns>
    <id>5584076</id>
    <revision>
      <id>749439748</id>
      <parentid>733433117</parentid>
      <timestamp>2016-11-14T09:50:56Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.2.7.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2575">{{notability|date=July 2013}}
'''Cantamath''' is a mathematics competition competed in Christchurch, [[Canterbury, New Zealand]] by years 6 to 10 students. {{cn|date=August 2016}}

There are two sections, the Competition section and the Project section. {{cn|date=August 2016}}

The sponsors of Cantamath are [[Casio]], [[Trimble Navigation]], [[Every Educaid]], [[Mathletics (educational software)|Mathletics]] and [[University of Canterbury]]. {{cn|date=August 2016}}

==Team Competition section==

In the Team Competition section, each participating school sends in four selected student mathematicians per year level. The participants compete against other schools in the Christchurch [[Horncastle Arena]]. It's a speed competition and takes 30 minutes. There are 20 questions for each team to complete, the aim being for each team to answer all questions the fastest. One of the four team members is a runner who runs to a judge to check if the answer to their current question is right. Each question is worth 5 points, allowing a maximum score of 100. A team can only attempt one question at a time and have to keep working on it until they get it right. Passing is allowed, but no points will be received for that question, as well as preventing the team from returning to that question. {{cn|date=August 2016}}

The winning team gets a badge and a prize from Casio. {{cn|date=August 2016}}

==Project section==
In the Project section, the student submits a project on a certain topic. Projects can be awarded with an Excellence or Highly Commended award, depending on their quality.  There is also an Outstanding award for the best few projects in the display section. {{cn|date=August 2016}}

The categories include:
*Computer Generated Design (CGD): This consists of a picture made in programs. CGDs might include polygons, curves, and circles.
*Publicity Motif: This is a design for the following year's poster. One poster will be chosen and will be the poster for that year.
*Mathematical Poster: A poster which is based on the current year's theme.
*Geometrical Design: This is a hand-done [[A4 paper]] project which should include geometric shapes and curves.
*Mathematical Models: A 3D model. The design may be static or mobile.
Varies other exist, and may be found at the Cantamath site. {{cn|date=August 2016}}

==References==
{{Reflist}}

==External links==
*[https://web.archive.org/web/20080827171502/http://www.canterburymaths.org.nz/cantamath/index.htm Cantamath]

[[Category:Education in Canterbury, New Zealand]]
[[Category:Mathematics competitions]]</text>
      <sha1>5lbo4jbebzsnsgyso32frhbisyshp08</sha1>
    </revision>
  </page>
  <page>
    <title>Cauchy–Schwarz inequality</title>
    <ns>0</ns>
    <id>38128</id>
    <revision>
      <id>871071777</id>
      <parentid>869955078</parentid>
      <timestamp>2018-11-28T19:13:10Z</timestamp>
      <contributor>
        <ip>2A02:A443:CCAF:1:5154:526D:6E93:CFE</ip>
      </contributor>
      <comment>/* Titu's Lemma */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="26259">In [[mathematics]], the '''Cauchy–Schwarz inequality''', also known as the '''Cauchy–Bunyakovsky–Schwarz inequality''', is a useful [[Inequality (mathematics)|inequality]] encountered in many different settings, such as [[linear algebra]], [[mathematical analysis|analysis]], [[probability theory]], [[vector algebra]] and other areas. It is considered to be one of the most important inequalities in all of mathematics.&lt;ref name="Steele"&gt;{{cite web |last=Steele |first=J. Michael |year=2004 |title=The Cauchy–Schwarz Master Class: an Introduction to the Art of Mathematical Inequalities |publisher=The Mathematical Association of America |isbn=978-0521546775 |page=1 |url=http://www-stat.wharton.upenn.edu/~steele/Publications/Books/CSMC/CSMC_index.html |quote=...there is no doubt that this is one of the most widely used and most important inequalities in all of mathematics.}}&lt;/ref&gt;

The inequality for sums was published by {{harvs|first=Augustin-Louis|last=Cauchy|authorlink=Augustin-Louis Cauchy|year=1821|txt=yes}}, while the corresponding inequality for integrals was first proved by
{{harvs|txt=yes|authorlink=Viktor Bunyakovsky |year=1859|first=Viktor|last=Bunyakovsky}}. The modern proof of the integral inequality was given by {{harvs|txt=yes|authorlink=Hermann Amandus Schwarz|first=Hermann Amandus|last=Schwarz|year=1888}}.&lt;ref name="Steele" /&gt;

== Statement of the inequality ==
The Cauchy–Schwarz inequality states that for all vectors &lt;math&gt;u&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; of an [[inner product space]] it is true that
: &lt;math&gt;|\langle \mathbf{u},\mathbf{v}\rangle| ^2 \leq \langle \mathbf{u},\mathbf{u}\rangle \cdot \langle \mathbf{v},\mathbf{v}\rangle,&lt;/math&gt;
where &lt;math&gt;\langle\cdot,\cdot\rangle&lt;/math&gt; is the [[inner product]]. Examples of inner products include the real and complex [[dot product]], see the [[Inner product space#Examples|examples in inner product]].  Equivalently, by taking the square root of both sides, and referring to the [[inner product space#Norms on inner product spaces|norms]] of the vectors, the inequality is written as&lt;ref name=Strang5&gt;{{cite book |last=Strang |first=Gilbert |date=19 July 2005 |title=Linear Algebra and its Applications |edition=4th |chapter=3.2 |publisher=Cengage Learning |location=Stamford, CT |isbn=978-0030105678 |pages=154–155}}&lt;/ref&gt;&lt;ref name=":0"&gt;{{cite book |last1=Hunter |first1=John K. |last2=Nachtergaele |first2=Bruno |year=2001 |title=Applied Analysis |publisher=World Scientific |isbn=981-02-4191-7 |url=https://books.google.com/books?id=oOYQVeHmNk4C}}&lt;/ref&gt;
: &lt;math&gt;|\langle \mathbf{u},\mathbf{v}\rangle| \leq \|\mathbf{u}\|  \|\mathbf{v}\|.&lt;/math&gt;
Moreover, the two sides are equal if and only if &lt;math&gt;\mathbf{u}&lt;/math&gt; and &lt;math&gt;\mathbf{v}&lt;/math&gt; are [[linear independence|linearly dependent]] (meaning they are [[parallel (geometry)|parallel]]: one of the vector's magnitudes is zero, or one is a scalar multiple of the other).&lt;ref&gt;{{cite book |last1=Bachmann |first1=George |last2=Narici |first2=Lawrence |last3=Beckenstein |first3=Edward |date=2012-12-06 |title=Fourier and Wavelet Analysis |publisher=Springer Science &amp; Business Media |isbn=9781461205050 |page=14 |url=https://books.google.com/books?id=PkHhBwAAQBAJ}}&lt;/ref&gt;&lt;ref&gt;{{cite book |last=Hassani |first=Sadri |year=1999 |title=Mathematical Physics: A Modern Introduction to Its Foundations |publisher=Springer |isbn=0-387-98579-4 |page=29 |quote=Equality holds iff &lt;c&amp;#124;c&gt;=0 or &amp;#124;c&gt;=0. From the definition of &amp;#124;c&gt;, we conclude that &amp;#124;a&gt; and &amp;#124;b&gt; must be proportional.}}&lt;/ref&gt;

If &lt;math&gt;u_1,\ldots, u_n\in\mathbb C&lt;/math&gt; and &lt;math&gt;v_1,\ldots, v_n\in\mathbb C&lt;/math&gt;, and the inner product is the standard complex inner product, then the inequality may be restated more explicitly as follows (where the bar notation is used for [[Complex conjugate|complex conjugation]]): 
: &lt;math&gt;|u_1\bar{v}_1 + \cdots + u_n \bar{v}_n|^2 \leq (|u_1|^2 + \cdots + |u_n|^2) (|v_1|^2 + \cdots + |v_n|^2)&lt;/math&gt;
or 
: &lt;math&gt;\left| \sum_{i=1}^n u_i \bar{v}_i \right|^2 \leq \sum_{j=1}^n |u_j|^2 \sum_{k=1}^n |v_k|^2.&lt;/math&gt;

==Proofs==

=== First proof ===
Let &lt;math&gt;u&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; be arbitrary vectors in a vector space over &lt;math&gt;\mathbb F&lt;/math&gt; with an inner product, where &lt;math&gt;\mathbb F&lt;/math&gt; is the field of real or complex numbers. We prove the inequality
: &lt;math&gt;\big| \langle u,v \rangle \big| \leq \|u\| \|v\|&lt;/math&gt;
and that equality holds if and only if either &lt;math&gt;u&lt;/math&gt; or &lt;math&gt;v&lt;/math&gt; is a multiple of the other (which includes the special case that either is the zero vector).

If &lt;math&gt;v = 0&lt;/math&gt;, it is clear that we have equality, and in this case &lt;math&gt;u&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; are also linearly dependent, regardless of &lt;math&gt;u&lt;/math&gt;, so the theorem is true. Similarly if &lt;math&gt;u = 0&lt;/math&gt;. We henceforth assume that &lt;math&gt;v&lt;/math&gt; is nonzero.

Let
: &lt;math&gt;z= u-u_v= u-\frac {\langle u, v \rangle} {\langle v, v \rangle} v.&lt;/math&gt;
Then, by linearity of the inner product in its first argument, one has
: &lt;math&gt;\langle z, v \rangle = \left\langle u -\frac {\langle u, v \rangle} {\langle v, v \rangle} v, v \right\rangle = \langle u, v \rangle - \frac {\langle u, v \rangle} {\langle v, v \rangle} \langle v, v \rangle = 0.&lt;/math&gt;
Therefore,  &lt;math&gt;z&lt;/math&gt; is a vector orthogonal to the vector &lt;math&gt;v&lt;/math&gt; (Indeed, &lt;math&gt;z&lt;/math&gt;  is the [[vector projection|projection]] of &lt;math&gt;u&lt;/math&gt;  onto the plane orthogonal to &lt;math&gt;v&lt;/math&gt; .) We can thus apply the [[Pythagorean theorem#Inner product spaces|Pythagorean theorem]] to
: &lt;math&gt;u= \frac {\langle u, v \rangle} {\langle v, v \rangle} v+z&lt;/math&gt;
which gives
: &lt;math&gt;\|u\|^2 = \left|\frac{\langle u, v \rangle}{\langle v, v \rangle}\right|^2 \|v\|^2 + \|z\|^2 = \frac{|\langle u, v \rangle|^2}{\|v\|^2} + \|z\|^2 \geq \frac{|\langle u, v \rangle|^2}{\|v\|^2}&lt;/math&gt;
and, after multiplication by &lt;math&gt;\| v \|^2&lt;/math&gt; and taking square root, we get the Cauchy–Schwarz inequality.
Moreover, if the relation &lt;math&gt;\geq&lt;/math&gt; in the above expression is actually an equality, then &lt;math&gt;\| z \|^2 = 0&lt;/math&gt; and hence &lt;math&gt;z = 0&lt;/math&gt;; the definition of &lt;math&gt;z&lt;/math&gt; then establishes a relation of linear dependence between &lt;math&gt;u&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt;. On the other hand, if &lt;math&gt;u&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; are linearly dependent, then there exists &lt;math&gt;c \in \mathbb{F}&lt;/math&gt; such that &lt;math&gt;u=c \cdot v&lt;/math&gt; (since &lt;math&gt;v \ne 0&lt;/math&gt;). Then 
: &lt;math&gt;|\langle u, v \rangle| = |\langle c \cdot v, v \rangle| = \left| c\|v\|^2 \right| = |c|\|v\|^2=\|c \cdot v\|\|v\|=\|u\|\|v\|.&lt;/math&gt;
This establishes the theorem.

=== Second proof ===

Let &lt;math&gt;u&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; be arbitrary vectors in an inner product space over &lt;math&gt;\mathbb C&lt;/math&gt;.

In the special case &lt;math&gt;v = 0&lt;/math&gt; the theorem is trivially true. Now assume that &lt;math&gt;v \ne 0&lt;/math&gt;. Let &lt;math&gt; \lambda \in \mathbb{C}&lt;/math&gt; be given by &lt;math&gt;\lambda = \langle u, v \rangle / \|v\|^2&lt;/math&gt;, then

: &lt;math&gt;
\begin{align}
0 &amp; \leq \| u - \lambda \cdot v \|^2 \\
 &amp; = \langle u, u \rangle - \langle \lambda \cdot v, u \rangle - \langle u,\lambda \cdot v \rangle + \langle \lambda \cdot v, \lambda \cdot v \rangle   \\
 &amp; =  \langle u, u \rangle - \lambda \langle v, u \rangle - \overline{\lambda} \langle u, v \rangle + \lambda \overline{\lambda} \langle v, v \rangle \\
 &amp; =  \|u\|^2 - \lambda \overline{\langle u, v \rangle} - \overline{\lambda} \langle u, v \rangle + \lambda \overline{\lambda} \|v\|^2 \\
 &amp; = \|u\|^2 -  \frac{|\langle u, v \rangle|^2}{\|v\|^2} - \frac{|\langle u, v \rangle|^2}{\|v\|^2} + \frac{|\langle u, v \rangle|^2}{\|v\|^2} \\
&amp; = \|u\|^2 -  \frac{|\langle u, v \rangle|^2}{\|v\|^2}. 
\end{align}
&lt;/math&gt;

Therefore, &lt;math&gt;0 \leq \|u\|^2 -  \frac{|\langle u, v \rangle|^2}{\|v\|^2}&lt;/math&gt;, or &lt;math&gt;|\langle u, v \rangle| \leq \|u\| \|v\|&lt;/math&gt;.

If the inequality holds as an equality, then &lt;math&gt;\| u - \lambda \cdot v \| = 0&lt;/math&gt;, and so &lt;math&gt;u - \lambda \cdot v = 0&lt;/math&gt;, thus &lt;math&gt;u&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; are linearly dependent. On the other hand, if &lt;math&gt;u&lt;/math&gt; and &lt;math&gt;v&lt;/math&gt; are linearly dependent, then &lt;math&gt;|\langle u, v \rangle| = \|u\| \|v\|&lt;/math&gt;, as shown in the first proof.

=== More proofs ===
There are many different proofs&lt;ref&gt;{{cite journal |last1=Wu |first1=Hui-Hua |last2=Wu |first2=Shanhe |date=April 2009 |title=Various proofs of the Cauchy-Schwarz inequality |journal=Octogon Mathematical Magazine |issn=1222-5657 |isbn=978-973-88255-5-0 |volume=17 |issue=1 |pages=221–229 |url=http://www.uni-miskolc.hu/~matsefi/Octogon/volumes/volume1/article1_19.pdf |access-date=18 May 2016}}&lt;/ref&gt; of the Cauchy–Schwarz inequality other than the above two examples.&lt;ref name="Steele" /&gt;&lt;ref name=":0" /&gt; When consulting other sources, there are often two sources of confusion. First, some authors define {{math|⟨⋅,⋅⟩}} to be linear in the [[Inner product space#Alternative definitions, notations and remarks|second argument]] rather than the first. Second, some proofs are only valid when the field is &lt;math&gt;\mathbb R&lt;/math&gt; and not &lt;math&gt;\mathbb C&lt;/math&gt;.&lt;ref&gt;{{cite book |last1=Aliprantis |first1=Charalambos D. |last2=Border |first2=Kim C. |date=2007-05-02 |title=Infinite Dimensional Analysis: A Hitchhiker's Guide |publisher=Springer Science &amp; Business Media |isbn=9783540326960 |url=https://books.google.com/books?id=4hIq6ExH7NoC}}&lt;/ref&gt;

==Special cases==

===Titu's Lemma ===
Titu's lemma (named after [[Titu Andreescu]], also known as T2 Lemma, Engel's form, or Sedrakyan's inequality) states that for positive reals, we have

:&lt;math&gt; \frac{\left( \sum_{i=1}^n u_i \right)^2 }{\sum_{i=1}^n v_i} \leq \sum_{i=1}^n \frac{u_i^2}{v_i}. &lt;/math&gt;

It is a direct consequence of the Cauchy-Schwarz inequality, obtained upon substituting &lt;math&gt;u_i' = \frac{u_i}{\sqrt{v_i}}&lt;/math&gt; and &lt;math&gt;v_i' =  \sqrt{v_i}.&lt;/math&gt; This form is especially helpful when the inequality involves fractions where the numerator is a perfect square.

===R&lt;sup&gt;2&lt;/sup&gt; (ordinary two-dimensional space)===
In the usual 2-dimensional space with the [[dot product]], let &lt;math&gt;v = (v_1,v_2)&lt;/math&gt; and &lt;math&gt;u = (u_1,u_2)&lt;/math&gt;. The Cauchy–Schwarz inequality is that
:&lt;math&gt;\langle u, v \rangle^2 = (\|u\| \|v\| \cos \theta)^2 \leq \|u\|^2 \|v\|^2,&lt;/math&gt;
where &lt;math&gt;\theta&lt;/math&gt; is the [[angle]] between &lt;math&gt;u&lt;/math&gt; and &lt;math&gt;v.&lt;/math&gt;

The form above is perhaps the easiest in which to understand the inequality, since the square of the cosine can be at most 1, which occurs when the vectors are in the same or opposite directions. It can also be restated in terms of the vector coordinates &lt;math&gt;v_1, v_2, u_1&lt;/math&gt; and &lt;math&gt;u_2&lt;/math&gt; as
: &lt;math&gt;(u_1v_1 + u_2v_2)^2 \leq (u_1^2 + u_2^2) (v_1^2 + v_2^2),&lt;/math&gt;
where equality holds if and only if the vector &lt;math&gt;(u_1,u_2)&lt;/math&gt; is in the same or opposite direction as the vector &lt;math&gt;(v_1, v_2),&lt;/math&gt; or if one of them is the zero vector.

=== R&lt;sup&gt;''n''&lt;/sup&gt; (''n''-dimensional Euclidean space)===
In [[Euclidean space]] &lt;math&gt;\mathbb R ^n&lt;/math&gt; with the standard inner product, the Cauchy–Schwarz inequality is
: &lt;math&gt;\left(\sum_{i=1}^n u_i v_i\right)^2\leq \left(\sum_{i=1}^n u_i^2\right) \left(\sum_{i=1}^n v_i^2\right)&lt;/math&gt;

The Cauchy–Schwarz inequality can be proved using only ideas from elementary algebra in this case. 
Consider the following quadratic polynomial in &lt;math&gt;x&lt;/math&gt;
: &lt;math&gt;0 \leq (u_1 x + v_1)^2 + \cdots + (u_n x + v_n)^2 = \left( \sum u_i^2 \right) x^2 + 2 \left( \sum u_i v_i \right) x + \sum v_i^2.&lt;/math&gt;
Since it is nonnegative, it has at most one real root for &lt;math&gt;x&lt;/math&gt;, hence its [[discriminant]] is less than or equal to zero. That is,
: &lt;math&gt;\left(\sum ( u_i v_i ) \right)^2 - \sum {u_i^2}  \sum {v_i^2} \le 0,&lt;/math&gt;
which yields the Cauchy–Schwarz inequality.

===L&lt;sup&gt;2&lt;/sup&gt;===
For the inner product space of [[square-integrable]] complex-valued [[function (mathematics)|functions]], one has
: &lt;math&gt;\left|\int_{\mathbb{R}^n} f(x) \overline{g(x)}\,dx\right|^2 \leq \int_{\mathbb{R}^n} |f(x)|^2\,dx  \int_{\mathbb{R}^n} |g(x)|^2 \,dx.&lt;/math&gt;
A generalization of this is the [[Hölder inequality]].

== Applications ==

=== Analysis ===
The [[triangle inequality]] for the standard norm is often shown as a consequence of the Cauchy–Schwarz inequality, as follows: given vectors ''x'' and ''y'':
: &lt;math&gt;\begin{align}
\|x + y\|^2 &amp; = \langle x + y, x + y \rangle \\
&amp; = \|x\|^2 + \langle x, y \rangle + \langle y, x \rangle + \|y\|^2 \\
&amp; = \|x\|^2 + 2 \operatorname{Re} \langle x, y \rangle + \|y\|^2\\
&amp; \le \|x\|^2 + 2|\langle x, y \rangle| + \|y\|^2 \\
&amp; \le \|x\|^2 + 2\|x\|\|y\| + \|y\|^2 \\
&amp; = (\|x\| + \|y\|)^2
\end{align}&lt;/math&gt;

Taking square roots gives the triangle inequality.

The Cauchy–Schwarz inequality is used to prove that the inner product is a [[continuous function]] with respect to the [[topology]] induced by the inner product itself.&lt;ref&gt;{{cite book |last1=Bachman |first1=George |last2=Narici |first2=Lawrence |date=2012-09-26 |title=Functional Analysis |publisher=Courier Corporation |isbn=9780486136554 |pages=141|url=https://books.google.com/books?id=_lTDAgAAQBAJ}}&lt;/ref&gt;&lt;ref&gt;{{cite book |last=Swartz |first=Charles |date=1994-02-21 |title=Measure, Integration and Function Spaces |publisher=World Scientific |isbn=9789814502511 |pages=236 |url=https://books.google.com/books?id=SsbsCgAAQBAJ}}&lt;/ref&gt;

=== Geometry ===
The Cauchy–Schwarz inequality allows one to extend the notion of "angle between two vectors" to any [[real numbers|real]] inner-product space by defining:&lt;ref&gt;{{cite book |last=Ricardo |first=Henry |date=2009-10-21 |title=A Modern Introduction to Linear Algebra |publisher=CRC Press |isbn=9781439894613 |pages=18 |url=https://books.google.com/books?id=s7bMBQAAQBAJ}}&lt;/ref&gt;&lt;ref&gt;{{cite book |last1=Banerjee |first1=Sudipto |last2=Roy |first2=Anindya |date=2014-06-06 |title=Linear Algebra and Matrix Analysis for Statistics |publisher=CRC Press |isbn=9781482248241 |pages=181 |url=https://books.google.com/books?id=WDTcBQAAQBAJ}}&lt;/ref&gt;
: &lt;math&gt;\cos\theta_{xy}=\frac{\langle x,y\rangle}{\|x\| \|y\|}.&lt;/math&gt;
The Cauchy–Schwarz inequality proves that this definition is sensible, by showing that the right-hand side lies in the interval [&amp;minus;1,&amp;nbsp;1] and justifies the notion that (real) [[Hilbert space]]s are simply generalizations of the [[Euclidean space]]. It can also be used to define an angle in [[complex numbers|complex]] [[inner-product space]]s, by taking the absolute value or the real part of the right-hand side,&lt;ref&gt;{{cite book |last=Valenza |first=Robert J. |date=2012-12-06 |title=Linear Algebra: An Introduction to Abstract Mathematics |publisher=Springer Science &amp; Business Media |isbn=9781461209010 |pages=146 |url=https://books.google.com/books?id=7x8MCAAAQBAJ}}&lt;/ref&gt;&lt;ref&gt;{{cite book |last=Constantin |first=Adrian |date=2016-05-21 |title=Fourier Analysis with Applications |publisher=Cambridge University Press |isbn=9781107044104 |pages=74 |url=https://books.google.com/books?id=JnMZDAAAQBAJ}}&lt;/ref&gt; as is done when extracting a metric from [[Fidelity of quantum states|quantum fidelity]].

===Probability theory===
&lt;!-- For the multivariate case,{{clarify|reason=define GE operator here|date=July 2011}}&lt;ref&gt;{{cite journal| last=Gautam | first=Tripathi |title=A matrix extension of the Cauchy-Schwarz inequality|journal=Economics Letters|date=4 December 1998|url=http://web2.uconn.edu/tripathi/published-papers/cs.pdf|doi=10.1016/s0165-1765(99)00014-2}}&lt;/ref&gt;
: &lt;math&gt;\operatorname{Var}(Y) \ge \operatorname{Cov} (Y,X) \operatorname{Var}^{-1}(X) \operatorname{Cov}(X,Y)&lt;/math&gt;
This inequality means that the difference is semidefinite positive. --&gt;

Let ''X'', ''Y'' be [[random variable]]s, then the covariance inequality&lt;ref&gt;{{cite book |last=Mukhopadhyay |first=Nitis |date=2000-03-22 |title=Probability and Statistical Inference |publisher=CRC Press |isbn=9780824703790 |pages=150 |url=https://books.google.com/books?id=TMSnGkr_DxwC}}&lt;/ref&gt;&lt;ref&gt;{{cite book |last=Keener |first=Robert W. |date=2010-09-08 |title=Theoretical Statistics: Topics for a Core Course |publisher=Springer Science &amp; Business Media |isbn=9780387938394 |pages=71 |url=https://books.google.com/books?id=aVJmcega44cC}}&lt;/ref&gt; is given by
: &lt;math&gt;\operatorname{Var}(Y)\ge\frac{\operatorname{Cov}(Y,X)\operatorname{Cov}(Y,X)}{\operatorname{Var}(X)}.&lt;/math&gt;
After defining an inner product on the set of random variables using the expectation of their product,
: &lt;math&gt;\langle X, Y \rangle := \operatorname{E}(X Y),&lt;/math&gt;
then the Cauchy–Schwarz inequality becomes
: &lt;math&gt;|\operatorname{E}(XY)|^2 \leq \operatorname{E}(X^2) \operatorname{E}(Y^2).&lt;/math&gt;
To prove the covariance inequality using the Cauchy–Schwarz inequality, let &lt;math&gt;\mu = \operatorname{E}(X)&lt;/math&gt; and &lt;math&gt;\nu = \operatorname{E}(Y)&lt;/math&gt;, then
: &lt;math&gt;\begin{align}
|\operatorname{Cov}(X,Y)|^2 &amp;= |\operatorname{E}( (X - \mu)(Y - \nu) )|^2 \\
&amp;= | \langle X - \mu, Y - \nu \rangle |^2\\
&amp;\leq \langle X - \mu, X - \mu \rangle \langle Y - \nu, Y - \nu \rangle \\
&amp; = \operatorname{E}( (X-\mu)^2 ) \operatorname{E}( (Y-\nu)^2 ) \\
&amp; = \operatorname{Var}(X) \operatorname{Var}(Y),
\end{align}&lt;/math&gt;

where &lt;math&gt;\operatorname{Var}&lt;/math&gt; denotes [[variance]], and &lt;math&gt;\operatorname{Cov}&lt;/math&gt; denotes [[covariance]].

== Generalizations ==
Various generalizations of the Cauchy–Schwarz inequality exist in the context of [[operator theory]], e.g. for operator-convex functions and [[operator algebra]]s, where the domain and/or range are replaced by a [[C*-algebra]] or [[W*-algebra]].

An inner product can be used to define a [[positive linear functional]]. For example, given a Hilbert space &lt;math&gt;L^2(m), m&lt;/math&gt; being a finite measure, the standard inner product gives rise to a positive functional &lt;math&gt;\varphi&lt;/math&gt; by &lt;math&gt;\varphi (g) = \langle g, 1 \rangle&lt;/math&gt;.  Conversely, every positive linear functional &lt;math&gt;\varphi&lt;/math&gt; on &lt;math&gt;L^2(m)&lt;/math&gt; can be used to define an  inner product &lt;math&gt;\langle f, g \rangle _\varphi := \varphi(g^*f)&lt;/math&gt;, where &lt;math&gt;g^*&lt;/math&gt; is the [[Pointwise product|pointwise]] [[complex conjugate]] of &lt;math&gt;g&lt;/math&gt;. In this language, the Cauchy–Schwarz inequality becomes&lt;ref&gt;{{cite book |last1=Faria |first1=Edson de |last2=Melo |first2=Welington de |date=2010-08-12 |title=Mathematical Aspects of Quantum Field Theory |publisher=Cambridge University Press |isbn=9781139489805 |pages=273 |url=https://books.google.com/books?id=u9M9PFLNpMMC}}&lt;/ref&gt;
: &lt;math&gt;| \varphi(g^*f) |^2 \leq \varphi(f^*f) \varphi(g^*g),&lt;/math&gt;
which extends verbatim to positive functionals on C*-algebras:

'''Theorem''' (Cauchy–Schwarz inequality for  positive functionals on C*-algebras):&lt;ref&gt;{{cite book |last=Lin |first=Huaxin |date=2001-01-01 |title=An Introduction to the Classification of Amenable C*-algebras |publisher=World Scientific |isbn=9789812799883 |pages=27 |url=https://books.google.com/books?id=2qru8d7BCAAC}}&lt;/ref&gt;&lt;ref&gt;{{cite book |last=Arveson |first=W. |date=2012-12-06 |title=An Invitation to C*-Algebras |publisher=Springer Science &amp; Business Media |isbn=9781461263715 |pages=28 |url=https://books.google.com/books?id=d5TqBwAAQBAJ}}&lt;/ref&gt; If &lt;math&gt;\varphi&lt;/math&gt; is a positive linear functional on a C*-algebra &lt;math&gt;A,&lt;/math&gt; then for all &lt;math&gt;a, b \in A&lt;/math&gt;, &lt;math&gt;|\varphi(b^*a)|^2 \leq \varphi(b^*b)\varphi(a^*a)&lt;/math&gt;.

The next two theorems are further examples in operator algebra.

'''Theorem''' (Kadison–Schwarz inequality,&lt;ref&gt;{{cite book |last=Størmer |first=Erling |date=2012-12-13 |title=Positive Linear Maps of Operator Algebras |series=Springer Monographs in Mathematics |publisher=Springer Science &amp; Business Media |isbn=9783642343698 |url=https://books.google.com/books?id=lQtKAIONqwIC}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |last=Kadison |first=Richard V. |date=1952-01-01 |title=A Generalized Schwarz Inequality and Algebraic Invariants for Operator Algebras |jstor=1969657 |journal=Annals of Mathematics |doi=10.2307/1969657 |volume=56 |number=3 |pages=494–503}}&lt;/ref&gt; named after [[Richard Kadison]]): If &lt;math&gt;\varphi&lt;/math&gt; is a unital positive map, then for every [[normal operator|normal element]] &lt;math&gt;a&lt;/math&gt; in its domain, we have &lt;math&gt;\varphi(a^*a) \ge \varphi(a^*) \varphi(a)&lt;/math&gt; and &lt;math&gt;\varphi(a^*a) \ge \varphi(a) \varphi(a^*)&lt;/math&gt;.

This extends the fact &lt;math&gt;\varphi(a^*a) \cdot 1 \ge \varphi(a)^* \varphi(a) = |\varphi(a)|^2&lt;/math&gt;, when &lt;math&gt;\varphi&lt;/math&gt; is a linear functional. The case when &lt;math&gt;a&lt;/math&gt; is self-adjoint, i.e. &lt;math&gt;a = a^*,&lt;/math&gt; is sometimes known as '''Kadison's inequality'''.

'''Theorem''' (Modified Schwarz inequality for 2-positive maps):&lt;ref&gt;{{cite book |last=Paulsen |first=Vern |year=2002 |title=Completely Bounded Maps and Operator Algebras |series=Cambridge Studies in Advanced Mathematics |volume=78 |publisher=Cambridge University Press |isbn=9780521816694 |page=40 |url=https://books.google.com/books?id=VtSFHDABxMIC&amp;pg=PA40}}&lt;/ref&gt; For a 2-positive map &lt;math&gt;\varphi&lt;/math&gt; between C*-algebras, for all &lt;math&gt;a, b&lt;/math&gt; in its domain,
: &lt;math&gt;\varphi(a)^*\varphi(a) \leq \Vert\varphi(1)\Vert\varphi(a^*a), \text{ and }&lt;/math&gt;
: &lt;math&gt;\Vert\varphi(a^*b)\Vert^2 \leq \Vert\varphi(a^*a)\Vert \cdot \Vert\varphi(b^*b)\Vert.&lt;/math&gt;
 
Another generalization is a refinement obtained by interpolating between both sides the Cauchy-Schwarz inequality:  

'''Theorem''' (Callebaut's Inequality)&lt;ref&gt;{{cite journal |last1=Callebaut|first1=D.K. |date=1965 |title=Generalization of the Cauchy–Schwarz inequality   |journal=J. Math. Anal. Appl. |volume=12  |pages=491-494 |doi=10.1016/0022-247X(65)90016-8 |url=https://www.sciencedirect.com/science/article/pii/0022247X65900168}}&lt;/ref&gt; 
For reals &lt;math&gt;0\leqslant s\leqslant t\leqslant 1&lt;/math&gt;, 
: &lt;math&gt;\Bigl(\sum_{i=1}^{n}a_{i}b_{i} \Bigr) ^2\leqslant\sum_{i=1}^{n}a_{i}^{1+s}b_{i}^{1-s}\sum_{i=1}^{n}a_{i}^{1-s}b_{i}^{1+s} \leqslant\sum_{i=1}^{n}a_{i}^{1+t}b_{i}^{1-t}\sum_{i=1}^{n}a_{i}^{1-t}b_{i}^{1+t} \leqslant \sum_{i=1}^{n}a_{i}^2\sum_{i=1}^{n}b_{i}^2 . &lt;/math&gt;

It can be easily proven by Hölder's inequality.&lt;ref&gt;{{cite book  |title=Callebaut's  inequality |publisher=Entry in the AoPS Wiki |url=https://artofproblemsolving.com/wiki/index.php?title=Callebaut%27s_Inequality}}&lt;/ref&gt; There are also non commutative versions for operators and tensor products of matrices.&lt;ref&gt;{{cite book |last1= Moslehian |first1=M.S. |last2=Matharu |first2=J.S. |last3=Aujla |first3=J.S. |date=2011 |title=Non-commutative Callebaut inequality |publisher=ArXiv |url=https://arxiv.org/pdf/1112.3003.pdf}}&lt;/ref&gt;

== See also ==
* [[Bessel's inequality]]
* [[Hölder's inequality]]
* [[Jensen's inequality]]
* [[Kunita–Watanabe inequality]]
* [[Minkowski inequality]]

==Notes==

{{Reflist|30em}}

==References==
{{refbegin}}
* {{citation|first=J. M.|last=Aldaz|first2=S.|last2=Barza|first3=M.|last3=Fujii|first4=M. S.|last4=Moslehian|title=Advances in Operator Cauchy—Schwarz inequalities and their reverses|journal=Annals of Functional Analysis|volume=6|year=2015|issue=3|pages=275–295|doi=10.15352/afa/06-3-20}}
* {{springer|id=b/b017770|title=Bunyakovskii inequality|first=V. I.|last=Bityutskov}}
* {{citation|first=V.|last=Bunyakovsky |authorlink=Viktor Yakovlevich Bunyakovsky|title=Sur quelques inegalités concernant les intégrales aux différences finies|journal=Mem. Acad. Sci. St. Petersbourg|volume=7|issue= 1|year=1859|pages=9|url=http://www-stat.wharton.upenn.edu/~steele/Publications/Books/CSMC/bunyakovsky.pdf|format=PDF}}
* {{citation|first=A.-L.|last=Cauchy|title= Sur les formules qui résultent de l'emploie du signe et sur &gt; ou &lt;, et sur les moyennes entre plusieurs quantités |journal= Cours d'Analyse, 1er Partie: Analyse algébrique 1821; OEuvres ser.2 III 373-377|year=1821}}
* {{citation|first=S. S.|last=Dragomir|title=A survey on Cauchy–Bunyakovsky–Schwarz type discrete inequalities|journal=Journal of Inequalities in Pure and Applied Mathematics|volume=4|issue=3|year=2003|pages=142 pp|url=http://jipam.vu.edu.au/article.php?sid=301|deadurl=yes|archiveurl=https://web.archive.org/web/20080720034744/http://jipam.vu.edu.au/article.php?sid=301|archivedate=2008-07-20|df=}}
*{{Citation | last1=Grinshpan | first1=A. Z. | title=General inequalities, consequences, and applications | doi=10.1016/j.aam.2004.05.001 | year=2005 | 
journal=Advances in Applied Mathematics | volume=34 | issue=1 | pages=71–100 }}
* {{citation|first=R. V.|last=Kadison|authorlink=Richard V. Kadison|title= A generalized Schwarz inequality and algebraic invariants for operator algebras|journal=Annals of Mathematics|volume=56|year=1952|doi=10.2307/1969657|pages=494–503|jstor=1969657|issue=3}}.
* {{Citation|last=Lohwater|first=Arthur|title=Introduction to Inequalities|publisher=Online e-book in PDF fomat|url=http://www.mediafire.com/?1mw1tkgozzu|year=1982|isbn=}}
* {{citation|first=V.|last=Paulsen|title=Completely Bounded Maps and Operator Algebras|publisher=Cambridge University Press|year= 2003}}.
* {{citation|first=H. A.|last=Schwarz|year=1888|pages=318|journal=Acta Societatis Scientiarum Fennicae|volume=XV|title=Über ein Flächen kleinsten Flächeninhalts betreffendes Problem der Variationsrechnung|url=http://www-stat.wharton.upenn.edu/~steele/Publications/Books/CSMC/Schwarz.pdf|format=PDF}}
* {{springer|title=Cauchy inequality|id=C/c020880|first=E. D.|last=Solomentsev}}
* {{citation|url=http://www-stat.wharton.upenn.edu/~steele/Publications/Books/CSMC/CSMC_index.html|first=J. M.|last=Steele|title=The Cauchy–Schwarz Master Class|publisher=Cambridge University Press|year=2004|isbn=0-521-54677-X}}
{{refend}}

==External links==
* [http://jeff560.tripod.com/c.html Earliest Uses: The entry on the Cauchy–Schwarz inequality has some historical information.]
* [http://people.revoledu.com/kardi/tutorial/LinearAlgebra/LinearlyIndependent.html#LinearlyIndependentVectors Example of application of Cauchy–Schwarz inequality to determine Linearly Independent Vectors] Tutorial and Interactive program.

{{Functional Analysis}}

{{DEFAULTSORT:Cauchy-Schwarz inequality}}
[[Category:Inequalities]]
[[Category:Linear algebra]]
[[Category:Operator theory]]
[[Category:Articles containing proofs]]
[[Category:Mathematical analysis]]
[[Category:Probabilistic inequalities]]</text>
      <sha1>e974un13oosvrdlhipac30520p2acbi</sha1>
    </revision>
  </page>
  <page>
    <title>Club set</title>
    <ns>0</ns>
    <id>1251559</id>
    <revision>
      <id>826845014</id>
      <parentid>783483559</parentid>
      <timestamp>2018-02-21T09:09:06Z</timestamp>
      <contributor>
        <username>Trovatore</username>
        <id>310173</id>
      </contributor>
      <comment>{{short description}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4790">{{short description|set theory concept}}
In [[mathematics]], particularly in [[mathematical logic]] and [[set theory]], a '''club set''' is a subset of a [[limit ordinal]] which is [[closed set|closed]] under the [[order topology]], and is unbounded (see below) relative to the limit ordinal.  The name ''club'' is a contraction of "closed and unbounded".

== Formal definition ==
Formally, if &lt;math&gt;\kappa&lt;/math&gt; is a limit ordinal, then a set &lt;math&gt;C\subseteq\kappa&lt;/math&gt; is ''closed'' in &lt;math&gt;\kappa&lt;/math&gt; [[if and only if]] for every &lt;math&gt;\alpha&lt;\kappa&lt;/math&gt;, if &lt;math&gt;\sup(C\cap \alpha)=\alpha\ne0&lt;/math&gt;, then &lt;math&gt;\alpha\in C&lt;/math&gt;.  Thus, if the [[Limit of a sequence|limit of some sequence]] from &lt;math&gt;C&lt;/math&gt; is less than &lt;math&gt;\kappa&lt;/math&gt;, then the limit is also in &lt;math&gt;C&lt;/math&gt;.

If &lt;math&gt;\kappa&lt;/math&gt; is a limit ordinal and &lt;math&gt;C\subseteq\kappa&lt;/math&gt; then &lt;math&gt;C&lt;/math&gt; is '''unbounded''' in &lt;math&gt;\kappa&lt;/math&gt; if for any &lt;math&gt;\alpha&lt;\kappa&lt;/math&gt;, there is some &lt;math&gt;\beta\in C&lt;/math&gt; such that &lt;math&gt;\alpha&lt;\beta&lt;/math&gt;.

If a set is both closed and unbounded, then it is a '''club set'''. Closed [[proper class]]es are also of interest (every proper class of ordinals is unbounded in the class of all ordinals).

For example, the set of all [[countable]] limit ordinals is a club set with respect to the [[first uncountable ordinal]]; but it is not a club set with respect to any higher limit ordinal, since it is neither closed nor unbounded.
The set of all limit ordinals &lt;math&gt;\alpha&lt;\kappa&lt;/math&gt; is closed unbounded in &lt;math&gt;\kappa   &lt;/math&gt;  (&lt;math&gt;\kappa   &lt;/math&gt; regular).   In fact a  club set is nothing else but the range of   a [[normal function]]  (i.e. increasing and continuous).

More generally, if &lt;math&gt;X&lt;/math&gt; is a nonempty set and &lt;math&gt;\lambda&lt;/math&gt; is a cardinal, then &lt;math&gt;C\subseteq[X]^\lambda&lt;/math&gt; is ''club'' if every union of a subset of &lt;math&gt;C&lt;/math&gt; is in &lt;math&gt;C&lt;/math&gt; and every subset of &lt;math&gt;X&lt;/math&gt; of cardinality less than &lt;math&gt;\lambda&lt;/math&gt; is contained in some element of &lt;math&gt;C&lt;/math&gt; (see [[stationary set]]).

== The closed unbounded filter ==
Let &lt;math&gt;\kappa \,&lt;/math&gt; be a limit ordinal of uncountable [[cofinality]] &lt;math&gt;\lambda \,.&lt;/math&gt; For some &lt;math&gt;\alpha &lt; \lambda \,&lt;/math&gt;, let &lt;math&gt;\langle C_\xi : \xi &lt; \alpha\rangle \,&lt;/math&gt; be a sequence of closed unbounded subsets of &lt;math&gt;\kappa \,.&lt;/math&gt; Then &lt;math&gt;\bigcap_{\xi &lt; \alpha} C_\xi \,&lt;/math&gt; is also closed unbounded. To see this, one can note that an intersection of closed sets is always closed, so we just need to show that this intersection is unbounded. So fix any &lt;math&gt;\beta_0 &lt; \kappa \,,&lt;/math&gt; and for each ''n''&lt;&amp;omega; choose from each &lt;math&gt;C_\xi \,&lt;/math&gt; an element &lt;math&gt;\beta_{n+1}^\xi &gt; \beta_{n} \,,&lt;/math&gt; which is possible because each is unbounded. Since this is a collection of fewer than &lt;math&gt;\lambda \,&lt;/math&gt; ordinals, all less than &lt;math&gt;\kappa \,,&lt;/math&gt; their least upper bound must also be less than &lt;math&gt;\kappa \,,&lt;/math&gt; so we can call it &lt;math&gt;\beta_{n+1} \,.&lt;/math&gt; This process generates a countable sequence &lt;math&gt;\beta_0,\beta_1,\beta_2,\dots \,.&lt;/math&gt; The limit of this sequence must in fact also be the limit of the sequence &lt;math&gt;\beta_0^\xi,\beta_1^\xi,\beta_2^\xi,\dots \,,&lt;/math&gt; and since each &lt;math&gt;C_\xi \,&lt;/math&gt; is closed and &lt;math&gt;\lambda \,&lt;/math&gt; is uncountable, this limit must be in each &lt;math&gt;C_\xi \,,&lt;/math&gt; and therefore this limit is an element of the intersection that is above &lt;math&gt;\beta_0 \,,&lt;/math&gt; which shows that the intersection is unbounded. QED.

From this, it can be seen that if &lt;math&gt;\kappa \,&lt;/math&gt; is a regular cardinal, then &lt;math&gt;\{S \subset \kappa : \exists C \subset S \text{ such that } C \text{ is closed unbounded in } \kappa\} \,&lt;/math&gt; is a non-principal &lt;math&gt;\kappa \,&lt;/math&gt;-complete [[filter (mathematics)|filter]] on &lt;math&gt;\kappa \,.&lt;/math&gt;

If &lt;math&gt;\kappa \,&lt;/math&gt; is a regular cardinal then club sets are also closed under [[diagonal intersection]].

In fact, if &lt;math&gt;\kappa \,&lt;/math&gt; is regular and &lt;math&gt;\mathcal{F} \,&lt;/math&gt; is any filter on &lt;math&gt;\kappa \,,&lt;/math&gt; closed under diagonal intersection, containing all sets of the form &lt;math&gt;\{\xi &lt; \kappa : \xi \geq \alpha\} \,&lt;/math&gt; for &lt;math&gt;\alpha &lt; \kappa \,,&lt;/math&gt; then &lt;math&gt;\mathcal{F} \,&lt;/math&gt; must include all club sets.

==See also==
*[[Club filter]]
*[[Stationary set]]
*[[Clubsuit]]

==References==
* Jech, Thomas, 2003. ''Set Theory: The Third Millennium Edition, Revised and Expanded''.  Springer.  {{ISBN|3-540-44085-2}}.
* Levy, A. (1979) ''Basic Set Theory'', Perspectives in Mathematical Logic, Springer-Verlag. Reprinted 2002, Dover. {{ISBN|0-486-42079-5}}
* {{PlanetMath attribution|id=3227|title=Club}}

[[Category:Set theory]]
[[Category:Ordinal numbers]]</text>
      <sha1>assm11nd1wgvp0ww6onmvia5jqw84vc</sha1>
    </revision>
  </page>
  <page>
    <title>Compensated CORDIC</title>
    <ns>0</ns>
    <id>49010509</id>
    <redirect title="CORDIC" />
    <revision>
      <id>699006488</id>
      <parentid>698333850</parentid>
      <timestamp>2016-01-09T17:33:09Z</timestamp>
      <contributor>
        <username>Matthiaspaul</username>
        <id>13467261</id>
      </contributor>
      <comment>+cat</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="198">#redirect [[CORDIC#Compensated CORDIC]] {{R to related topic}}

[[Category:Numerical analysis]]
[[Category:Trigonometry]]
[[Category:Digit-by-digit algorithms]]
[[Category:Shift-and-add algorithms]]</text>
      <sha1>33njhmi4c9qoz7lahxbd9u8c1n7tx54</sha1>
    </revision>
  </page>
  <page>
    <title>Corecursion</title>
    <ns>0</ns>
    <id>1338683</id>
    <revision>
      <id>869405619</id>
      <parentid>869404965</parentid>
      <timestamp>2018-11-18T11:20:43Z</timestamp>
      <contributor>
        <username>DesolateReality</username>
        <id>4591330</id>
      </contributor>
      <comment>/* Factorial */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="28637">{{distinguish|Mutual recursion}}

In [[computer science]], '''corecursion''' is a type of operation that is [[Dual (category theory)|dual]] to [[recursion (computer science)|recursion]]. Whereas recursion works analytically, starting on data further from a base case and breaking it down into smaller data and repeating until one reaches a base case, corecursion works synthetically, starting from a base case and building it up, iteratively producing data further removed from a base case. Put simply, corecursive algorithms use the data that they themselves produce, bit by bit, as they become available, and needed, to produce further bits of data. A similar but distinct concept is ''[[Generative_recursion#Structural_versus_generative_recursion|generative recursion]]'' which may lack a definite "direction" inherent in corecursion and recursion.

Where recursion allows programs to operate on arbitrarily complex data, so long as they can be reduced to simple data (base cases), corecursion allows programs to produce arbitrarily complex and potentially infinite data structures, such as [[stream (computing)|stream]]s, so long as it can be produced from simple data (base cases) in a sequence of ''finite'' steps. Where recursion may not terminate, never reaching a base state, corecursion starts from a base state, and thus produces subsequent steps deterministically, though it may proceed indefinitely (and thus not terminate under strict evaluation), or it may consume more than it produces and thus become non-''productive''. Many functions that are traditionally analyzed as recursive can alternatively, and arguably more naturally, be interpreted as corecursive functions that are terminated at a given stage, for example [[recurrence relation]]s such as the factorial.

Corecursion can produce both [[Finite set|finite]] and [[Infinite set|infinite]] [[data structure]]s as results, and may employ [[self-reference|self-referential]] data structures. Corecursion is often used in conjunction with [[lazy evaluation]], to produce only a finite subset of a potentially infinite structure (rather than trying to produce an entire infinite structure at once). Corecursion is a particularly important concept in [[functional programming]], where corecursion and [[codata (computer science)|codata]] allow [[total language]]s to work with infinite data structures.

== Examples ==
Corecursion can be understood by contrast with recursion, which is more familiar. While corecursion is primarily of interest in functional programming, it can be illustrated using imperative programming, which is done below using the [[Generator (computer programming)|generator]] facility in Python. In these examples local variables are used, and [[Assignment (computer science)|assigned values]] imperatively (destructively), though these are not necessary in corecursion in pure functional programming. In pure functional programming, rather than assigning to local variables, these computed values form an invariable sequence, and prior values are accessed by self-reference (later values in the sequence reference earlier values in the sequence to be computed). The assignments simply express this in the imperative paradigm and explicitly specify where the computations happen, which serves to clarify the exposition.

=== Factorial ===
A classic example of recursion is computing the [[factorial]], which is defined recursively by ''0! := 1'' and ''n! := n × (n - 1)!''.

To ''recursively'' compute its result on a given input, a recursive function calls (a copy of) ''itself'' with a different ("smaller" in some way) input and uses the result of this call to construct its result. The recursive call does the same, unless the ''base case'' has been reached. Thus a [[call stack]] develops in the process. For example, to compute ''fac(3)'', this recursively calls in turn ''fac(2)'', ''fac(1)'', ''fac(0)'' ("winding up" the stack), at which point recursion terminates with ''fac(0) = 1'', and then the stack unwinds in reverse order and the results are calculated on the way back along the call stack to the initial call frame ''fac(3)'' that uses the result of ''fac(2) = 2'' to calculate the final result as ''3 × 2 = 3 × fac(2) =: fac(3)'' and finally return ''fac(3) = 6''. In this example a function returns a single value.

This stack unwinding can be explicated, defining the factorial ''corecursively'', as an [[Iteration|iterator]], where one ''starts'' with the case of &lt;math&gt;1 =: 0!&lt;/math&gt;, then from this starting value constructs factorial values for increasing numbers ''1, 2, 3...'' as in the above recursive definition with "time arrow" reversed, as it were, by reading it ''backwards'' as {{nobreak|&lt;math&gt;n! \times (n+1) =: (n+1)!&lt;/math&gt;.}} The corecursive algorithm thus defined produces a ''stream'' of ''all'' factorials. This may be concretely implemented as a [[Generator (computer programming)|generator]]. Symbolically, noting that computing next factorial value requires keeping track of both ''n'' and ''f'' (a previous factorial value), this can be represented as:
:&lt;math&gt;n, f = (0, 1) : (n + 1, f \times (n+1))&lt;/math&gt;
or in [[Haskell (programming language)|Haskell]], 
&lt;source lang=haskell&gt;
  (\(n,f) -&gt; (n+1, f*(n+1))) `iterate` (0,1)
&lt;/source&gt;
meaning, "starting from &lt;math&gt;n, f = 0, 1&lt;/math&gt;, on each step the next values are calculated as &lt;math&gt;n+1, f \times (n+1)&lt;/math&gt;". This is mathematically equivalent and almost identical to the recursive definition, but the &lt;math&gt;+1&lt;/math&gt; emphasizes that the factorial values are being built ''up'', going forwards from the starting case, rather than being computed after first going backwards, ''down'' to the base case, with a &lt;math&gt;-1&lt;/math&gt; decrement. Note also that the direct output of the corecursive function does not simply contain the factorial &lt;math&gt;n!&lt;/math&gt; values, but also includes for each value the auxiliary data of its index ''n'' in the sequence, so that any one specific result can be selected among them all, as and when needed.

Note the connection with [[denotational semantics]], where the [[Denotational semantics#Denotations of recursive programs|denotations of recursive programs]] is built up corecursively in this way.

In Python, a recursive factorial function can be defined as:{{efn|Not validating input data.}}
&lt;source lang=python&gt;
def factorial(n):
    if n == 0:
        return 1
    else:
        return n * factorial(n - 1)
&lt;/source&gt;
This could then be called for example as &lt;code&gt;factorial(5)&lt;/code&gt; to compute ''5!''.

A corresponding corecursive generator can be defined as:
&lt;source lang=python&gt;
def factorials():
    n, f = 0, 1
    while True:
        yield f
        n, f = n + 1, f * (n + 1)
&lt;/source&gt;
This generates an infinite stream of factorials in order; a finite portion of it can be produced by:
&lt;source lang=python&gt;
def n_factorials(k):
    n, f = 0, 1
    while n &lt;= k:
        yield f
        n, f = n + 1, f * (n + 1)
&lt;/source&gt;
This could then be called to produce the factorials up to ''5!'' via:
&lt;source lang=python&gt;
for f in n_factorials(5):
    print(f)
&lt;/source&gt;
If we're only interested in a certain factorial, just the last value can be taken, or we can fuse the production and the access into one function,
&lt;source lang=python&gt;
def nth_factorial(k):
    n, f = 0, 1
    while n &lt; k:
        n, f = n + 1, f * (n + 1)
    yield f
&lt;/source&gt;
As can be readily seen here, this is practically equivalent (just by substituting &lt;code&gt;return&lt;/code&gt; for the only &lt;code&gt;yield&lt;/code&gt; there) to the accumulator argument technique for [[tail call|tail recursion]], unwound into an explicit loop. Thus it can be said that the concept of corecursion is an explication of the embodiment of iterative computation processes by recursive definitions, where applicable.

=== Fibonacci sequence ===
In the same way, the [[Fibonacci sequence]] can be represented as:
:&lt;math&gt;a, b = (0, 1) : (b, a+b)&lt;/math&gt;
Note that because the Fibonacci sequence is a [[recurrence relation]] of order 2, the corecursive relation must track two successive terms, with the &lt;math&gt;(b, -)&lt;/math&gt; corresponding to shift forward by one step, and the &lt;math&gt;(-, a+b)&lt;/math&gt; corresponding to computing the next term. This can then be implemented as follows (using [[parallel assignment]]):
&lt;source lang=python&gt;
def fibonacci_sequence():
    a, b = 0, 1
    while True:
        yield a
        a, b = b, a + b
&lt;/source&gt;
In Haskell, &lt;source lang=haskell&gt; map fst ( (\(a,b) -&gt; (b,a+b)) `iterate` (0,1) )&lt;/source&gt;

=== Tree traversal ===
[[Tree traversal]] via a [[depth-first]] approach is a classic example of recursion. Dually, [[breadth-first]] traversal can very naturally be implemented via corecursion.

Without using recursion or corecursion specifically, one may traverse a tree by starting at the root node, placing its child nodes in a data structure, then iterating by removing node after node from the data structure while placing each removed node's children back into that data structure.{{efn|More elegantly, one can start by placing the root node itself in the data structure and then starting the process.}} If the data structure is a [[Stack (abstract data type)|stack]] (LIFO), this yields depth-first traversal, and  if the data structure is a [[Queue (abstract data type)|queue]] (FIFO), this yields breadth-first traversal.

Using recursion, a (post-order){{efn|Post-order is to make "leaf node is base case" explicit for exposition, but the same analysis works for pre-order or in-order.}} depth-first traversal can be implemented by starting at the root node and recursively traversing each child subtree in turn (the subtree based at each child node) – the second child subtree does not start processing until the first child subtree is finished. Once a leaf node is reached or the children of a branch node have been exhausted, the node itself is visited (e.g., the value of the node itself is outputted). In this case, the call stack (of the recursive functions) acts as the stack that is iterated over.

Using corecursion, a breadth-first traversal can be implemented by starting at the root node, outputting its value,{{efn|Breadth-first traversal, unlike depth-first, is unambiguous, and visits a node value before processing children.}} then breadth-first traversing the subtrees – i.e., passing on the ''whole list'' of subtrees to the next step (not a single subtree, as in the recursive approach) – at the next step outputting the value of all of their root nodes, then passing on their child subtrees, etc.{{efn|Technically, one may define a breadth-first traversal on an ordered, disconnected set of trees – first the root node of each tree, then the children of each tree in turn, then the grandchildren in turn, etc.}} In this case the generator function, indeed the output sequence itself, acts as the queue. As in the factorial example (above), where the auxiliary information of the index (which step one was at, ''n'') was pushed forward, in addition to the actual output of ''n''!, in this case the auxiliary information of the remaining subtrees is pushed forward, in addition to the actual output. Symbolically:
:&lt;math&gt;v, t = ([], FullTree) : (RootValues, ChildTrees)&lt;/math&gt;
meaning that at each step, one outputs the list of values of root nodes, then proceeds to the child subtrees. Generating just the node values from this sequence simply requires discarding the auxiliary child tree data, then flattening the list of lists (values are initially grouped by level (depth); flattening (ungrouping) yields a flat linear list). In Haskell, 
&lt;source lang=haskell&gt; concatMap fst ( (\(v, t) -&gt; (rootValues t, childTrees t)) `iterate` ([], fullTree) )&lt;/source&gt;

These can be compared as follows. The recursive traversal handles a ''leaf node'' (at the ''bottom'') as the base case (when there are no children, just output the value), and ''analyzes'' a tree into subtrees, traversing each in turn, eventually resulting in just leaf nodes – actual leaf nodes, and branch nodes whose children have already been dealt with (cut off ''below''). By contrast, the corecursive traversal handles a ''root node'' (at the ''top'') as the base case (given a node, first output the value), treats a tree as being ''synthesized'' of a root node and its children, then produces as auxiliary output a list of subtrees at each step, which are then the input for the next step – the child nodes of the original root are the root nodes at the next step, as their parents have already been dealt with (cut off ''above''). Note also that in the recursive traversal there is a distinction between leaf nodes and branch nodes, while in the corecursive traversal there is no distinction, as each node is treated as the root node of the subtree it defines.

Notably, given an infinite tree,{{efn|Assume fixed [[branching factor]] (e.g., binary), or at least bounded, and balanced (infinite in every direction).}} the corecursive breadth-first traversal will traverse all nodes, just as for a finite tree, while the recursive depth-first traversal will go down one branch and not traverse all nodes, and indeed if traversing post-order, as in this example (or in-order), it will visit no nodes at all, because it never reaches a leaf. This shows the usefulness of corecursion rather than recursion for dealing with infinite data structures.

In Python, this can be implemented as follows.{{efn|First defining a tree class, say via:
&lt;source lang=python&gt;
class Tree:
    def __init__(self, value, left=None, right=None):
        self.value = value
        self.left  = left
        self.right = right

    def __str__(self):
        return str(self.value)
&lt;/source&gt;
and initializing a tree, say via:
&lt;source lang=python&gt;
t = Tree(1, Tree(2, Tree(4), Tree(5)), Tree(3, Tree(6), Tree(7)))
&lt;/source&gt;
In this example nodes are labeled in breadth-first order:
     1
  2     3
 4 5   6 7
}}
The usual post-order depth-first traversal can be defined as:{{efn|Intuitively, the function iterates over subtrees (possibly empty), then once these are finished, all that is left is the node itself, whose value is then returned; this corresponds to treating a leaf node as basic.}}
&lt;source lang=python&gt;
def df(node):
    if node is not None:
        df(node.left)
        df(node.right)
        print(node.value)
&lt;/source&gt;
This can then be called by &lt;code&gt;df(t)&lt;/code&gt; to print the values of the nodes of the tree in post-order depth-first order.

The breadth-first corecursive generator can be defined as:{{efn|1=Here the argument (and loop variable) is considered as a whole, possible infinite tree, represented by (identified with) its root node (tree = root node), rather than as a potential leaf node, hence the choice of variable name.}}
&lt;source lang=python&gt;
def bf(tree):
    tree_list = [tree]
    while tree_list:
        new_tree_list = []
        for tree in tree_list:
            if tree is not None:
                yield tree.value
                new_tree_list.append(tree.left)
                new_tree_list.append(tree.right)
        tree_list = new_tree_list
&lt;/source&gt;
This can then be called to print the values of the nodes of the tree in breadth-first order:
&lt;source lang=python&gt;
for i in bf(t):
    print(i)
&lt;/source&gt;

==Definition==
{{technical|section|date=November 2010}}
[[Initial and terminal objects|Initial data type]]s can be defined as being the [[least fixpoint]] ([[up to isomorphism]]) of some type equation; the [[isomorphism]] is then given by an [[Initial algebra|initial]] [[F-algebra|algebra]]. Dually, final (or terminal) data types can be defined as being the [[greatest fixpoint]] of a type equation; the isomorphism is then given by a final [[F-coalgebra|coalgebra]].

If the domain of discourse is the [[category of sets]] and total functions, then final data types may contain infinite, [[Non-well-founded set theory|non-wellfounded]] values, whereas initial types do not.&lt;ref&gt;Barwise and Moss 1996.&lt;/ref&gt;&lt;ref&gt;Moss and Danner 1997.&lt;/ref&gt; On the other hand, if the domain of discourse is the category of [[complete partial order]]s and [[Scott continuity|continuous functions]], which corresponds roughly to the [[Haskell (programming language)|Haskell]] programming language, then final types coincide with initial types, and the corresponding final coalgebra and initial algebra form an isomorphism.&lt;ref&gt;Smyth and Plotkin 1982.&lt;/ref&gt;

Corecursion is then a technique for recursively defining functions whose range (codomain) is a final data type, dual to the way that ordinary [[recursion]] recursively defines functions whose domain is an initial data type.&lt;ref&gt;Gibbons and Hutton 2005.&lt;/ref&gt;&lt;!--G&amp;H ascribe this definition to Barwise and Moss--&gt;

The discussion below provides several examples in Haskell that distinguish corecursion. Roughly speaking, if one were to port these definitions to the category of sets, they would still be corecursive. This informal usage is consistent with existing textbooks about Haskell.&lt;ref&gt;Doets and van Eijck 2004.&lt;/ref&gt; Also note that the examples used in this article predate the attempts to define corecursion and explain what it is.

==Discussion==
{{cleanup|section|reason=mix of discussion, examples, and related concepts|date=July 2012}}
{{Expert needed|Computer science|2=section|date=November 2010}}
The rule for ''primitive corecursion'' on [[codata (computer science)|codata]] is the dual to that for [[primitive recursion]] on data. Instead of descending on the argument by [[pattern-matching]] on its constructors (that ''were called up before'', somewhere, so we receive a ready-made datum and get at its constituent sub-parts, i.e. "fields"), we ascend on the result by filling-in its "destructors" (or "observers", that ''will be called afterwards'', somewhere - so we're actually calling a constructor, creating another bit of the result to be observed later on). Thus corecursion ''creates'' (potentially infinite) codata, whereas ordinary recursion ''analyses'' (necessarily finite) data.  Ordinary recursion might not be applicable to the codata because it might not terminate.  Conversely, corecursion is not strictly necessary if the result type is data, because data must be finite.

In "Programming with streams in Coq: a case study: the Sieve of Eratosthenes"&lt;ref&gt;Leclerc and Paulin-Mohring, 1994&lt;/ref&gt; we find
&lt;source lang=haskell&gt;
hd (conc a s) = a               
tl (conc a s) = s

(sieve p s) = if div p (hd s) then sieve p (tl s)
              else conc (hd s) (sieve p (tl s))

hd (primes s) = (hd s)          
tl (primes s) = primes (sieve (hd s) (tl s))
&lt;/source&gt;
where primes "are obtained by applying the primes operation to the stream (Enu 2)". Following the above notation, the sequence of primes (with a throwaway 0 prefixed to it) and numbers streams being progressively sieved, can be represented as 
:&lt;math&gt;p, s = (0, [2..]) : (hd(s), sieve(hd(s),tl(s)))&lt;/math&gt;
or in Haskell, 
&lt;source lang=haskell&gt;
(\(p, s@(h:t)) -&gt; (h, sieve h t)) `iterate` (0, [2..])
&lt;/source&gt;
The authors discuss how the definition of &lt;code&gt;sieve&lt;/code&gt; is not guaranteed always to be ''productive'', and could become stuck e.g. if called with &lt;code&gt;[5,10..]&lt;/code&gt; as the initial stream.

Here is another example in Haskell. The following definition produces the list of [[Fibonacci numbers]] in linear time:
&lt;source lang=haskell&gt;
fibs = 0 : 1 : zipWith (+) fibs (tail fibs)
&lt;/source&gt;

This infinite list depends on lazy evaluation;  elements are computed on an as-needed basis, and only finite prefixes are ever explicitly represented in memory. This feature allows algorithms on parts of codata to terminate; such techniques are an important part of Haskell programming.

This can be done in Python as well:&lt;ref&gt;Hettinger 2009.&lt;/ref&gt;
&lt;source lang=python&gt;
from itertools import tee, chain, islice, imap

def add(x, y):
    return x + y

def fibonacci():
    def deferred_output():
        for i in output:
            yield i
    result, c1, c2 = tee(deferred_output(), 3)
    paired = imap(add, c1, islice(c2, 1, None))
    output = chain([0, 1], paired)
    return result

for i in islice(fibonacci(), 20):
    print(i)
&lt;/source&gt;

The definition of &lt;code&gt;zipWith&lt;/code&gt; can be inlined, leading to this:

&lt;source lang=haskell&gt;
fibs = 0 : 1 : next fibs
  where
    next (a: t@(b:_)) = (a+b):next t
&lt;/source&gt;

This example employs a self-referential ''data structure''.   Ordinary recursion makes use of self-referential ''functions'',   but does not accommodate self-referential data.   However,  this is not essential to the Fibonacci example.    It can be rewritten as follows:

&lt;source lang=haskell&gt;
fibs = fibgen (0,1)
fibgen (x,y) = x : fibgen (y,x+y) 
&lt;/source&gt;

This employs only self-referential ''function'' to construct the result. If it were used with strict list constructor it would be an example of runaway recursion, but with [[lazy evaluation|non-strict]] list constructor this guarded recursion gradually produces an indefinitely defined list.

{{anchor|corecursive queue breadth-first tree traversal}}Corecursion need not produce an infinite object; a corecursive queue&lt;ref&gt;Allison 1989; Smith 2009.&lt;/ref&gt; is a particularly good example of this phenomenon.   The following definition produces a [[Breadth-first search|breadth-first traversal]] of a binary tree in linear time:

&lt;source lang=haskell&gt;
data Tree a b = Leaf a  |  Branch b (Tree a b) (Tree a b)

bftrav :: Tree a b -&gt; [Tree a b]
bftrav tree = queue
  where
    queue = tree : gen 1 queue

    gen  0   p                 =         []           
    gen len (Leaf   _     : s) =         gen (len-1) s 
    gen len (Branch _ l r : s) = l : r : gen (len+1) s  
&lt;/source&gt;

This definition takes an initial tree and produces a list of subtrees.   This list serves dual purpose as both the queue and the result (&lt;small&gt;''&lt;code&gt;gen len p&lt;/code&gt;''&lt;/small&gt; produces its output &lt;small&gt;''&lt;code&gt;len&lt;/code&gt;''&lt;/small&gt; notches after its input back-pointer, &lt;small&gt;''&lt;code&gt;p&lt;/code&gt;''&lt;/small&gt;, along the &lt;small&gt;''&lt;code&gt;queue&lt;/code&gt;''&lt;/small&gt;).  It is finite if and only if the initial tree is finite.  The length of the queue must be explicitly tracked in order to ensure termination;  this can safely be elided if this definition is applied only to infinite trees.  &lt;!-- Even if the result is finite, this example depends on lazy evaluation due to the use of self-referential data structures.

why the above was removed: it isn't true: the Haskell code can be translated into Prolog quite straightforwardly. Prolog is not a lazy language. But it does have tail recursion modulo cons. Which is emulatable in Scheme, C, etc.

bftrav(Tree,Q):- Q=[Tree|R], bfgen(1,Q,R).
bfgen(0,_,[]):- !.
bfgen(N,[leaf(_)        |P], R       ):- N2 is N-1, bfgen(N2,P,R).
bfgen(N,[branch(_,Lt,Rt)|P],[Lt,Rt|R]):- N2 is N+1, bfgen(N2,P,R).
--&gt;

Another particularly good example gives a solution to the problem of breadth-first labeling.&lt;ref&gt;Jones and Gibbons 1992.&lt;/ref&gt; The function &lt;code&gt;label&lt;/code&gt; visits every node in a binary tree in a breadth first fashion,  and replaces each label with an integer,  each subsequent integer is bigger than the last by one.   This solution employs a self-referential data structure,  and the binary tree can be finite or infinite.

&lt;source lang=haskell&gt;
label :: Tree a b -&gt; Tree Int Int 
label t = t′
    where
    (t′, ns) = go t (1:ns)

    go :: Tree a b    -&gt; [Int]  -&gt; (Tree Int Int, [Int])
    go   (Leaf   _    ) (n:ns) = (Leaf   n       , n+1 : ns  )
    go   (Branch _ l r) (n:ns) = (Branch n l′ r′ , n+1 : ns′′)
                                where
                                  (l′, ns′ ) = go l ns
                                  (r′, ns′′) = go r ns′
&lt;/source&gt;

An [[apomorphism]] (such as an [[anamorphism]], such as [[Unfold (higher-order function)|unfold]]) is a form of corecursion in the same way that a [[paramorphism]] (such as a [[catamorphism]], such as [[Fold (higher-order function)|fold]]) is a form of recursion.

The [[Coq]] proof assistant supports corecursion and [[coinduction]] using the CoFixpoint command.

== History ==
Corecursion, referred to as ''circular programming,'' dates at least to {{Harv|Bird|1984}}, who credits [[John Hughes (computer scientist)|John Hughes]] and [[Philip Wadler]]; more general forms were developed in {{Harv|Allison|1989}}. The original motivations included producing more efficient algorithms (allowing 1 pass over data in some cases, instead of requiring multiple passes) and implementing classical data structures, such as doubly linked lists and queues, in functional languages.

== See also ==
* [[Bisimulation]]
* [[Coinduction]]
* [[Recursion]]
* [[Anamorphism]]

== Notes ==
{{notelist}}

== References ==
{{Reflist|2}}
{{refbegin}}
* {{Cite journal | last = Bird | first = Richard Simpson | authorlink = Richard Bird (computer scientist)| title = Using circular programs to eliminate multiple traversals of data | doi = 10.1007/BF00264249 | journal = Acta Informatica | volume = 21 | issue = 3 | pages = 239–250 | year = 1984 | pmid =  | pmc = }}
* {{cite journal
| doi = 10.1002/spe.4380190202
| author = Lloyd Allison
| date = April 1989
| title = Circular Programs and Self-Referential Structures
| url = http://www.csse.monash.edu.au/~lloyd/tildeFP/1989SPE/
| journal = Software Practice and Experience
| volume = 19
| issue = 2
| pages = 99–109
}}
* {{cite techreport
| author = Geraint Jones and [[Jeremy Gibbons]]
| title = Linear-time breadth-first tree algorithms: An exercise in the arithmetic of folds and zips
| institution = Dept of Computer Science, University of Auckland
| year = 1992
| url  = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.32.5446
}}
* {{cite book
|author1=[[Jon Barwise]] |author2=Lawrence S Moss | title   = Vicious Circles
| url     = http://www.press.uchicago.edu/presssite/metadata.epl?mode=synopsis&amp;bookkey=3630257
| publisher = Center for the Study of Language and Information
| date    = June 1996
| isbn    = 978-1-57586-009-1
}}
* {{cite journal
| doi  = 10.1093/jigpal/5.2.231
|author1=Lawrence S Moss |author2=Norman Danner | title   = On the Foundations of Corecursion
| url     = http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.40.4243
| journal = Logic Journal of the IGPL
| volume  = 5
| issue  = 2
| pages   = 231–257
| year    = 1997
}}
* {{cite book
|author1=Kees Doets |author2=Jan van Eijck | title     = The Haskell Road to Logic, Maths, and Programming
| url       = http://homepages.cwi.nl/~jve/HR/
| publisher = King's College Publications
| date      = May 2004
| isbn      = 978-0-9543006-9-2
}}
* {{cite journal
| author = David Turner
| date = 2004-07-28
| title = Total Functional Programming
| url = http://www.jucs.org/jucs_10_7/total_functional_programming
| journal = Journal of Universal Computer Science
| volume = 10
| issue = 7
| pages = 751–768
| doi = 10.3217/jucs-010-07-0751
| authorlink = David Turner (computer scientist)
}}
* {{cite journal
|author1=Jeremy Gibbons |author2=Graham Hutton | title   = Proof methods for corecursive programs
| journal = Fundamenta Informaticae Special Issue on Program Transformation
| volume  = 66
| issue   = 4
| pages   = 353–366
| date    = April 2005
| url     = http://www.cs.nott.ac.uk/~gmh/bib.html#corecursion
}}
* {{citation
| author = Leon P Smith
| date = 2009-07-29
| title = Lloyd Allison's Corecursive Queues:  Why Continuations Matter
| url = http://themonadreader.wordpress.com/2009/07/29/issue-14/
| journal = The Monad Reader
| issue = 14
| pages = 37–68
}}
* {{cite web
| author = Raymond Hettinger
| title  = Recipe 576961: Technique for cyclical iteration
| url    = http://code.activestate.com/recipes/576961/
| date   = 2009-11-19
}}
* {{cite journal
| author = M. B. Smyth and [[Gordon Plotkin|G. D. Plotkin]]
| year = 1982
| title = The Category-Theoretic Solution of Recursive Domain Equations
| journal = [[SIAM Journal on Computing]]
| volume = 11
| issue = 4
| pages = 761–783
| doi = 10.1137/0211062
}}
* {{cite book
|author1=Leclerc, Francois  |author2=Paulin-Mohring, Christine
| title = Programming with Streams in Coq: A Case Study: the Sieve of Eratosthenes
| series = Types for Proofs and Programs: International Workshop TYPES '93. 
| year = 1993
| isbn = 3-540-58085-9
| pages = 191–212
| url = http://dl.acm.org/citation.cfm?id=189973.189981
| publisher = Springer-Verlag New York, Inc.
}}
{{refend}}

[[Category:Theoretical computer science]]
[[Category:Self-reference]]
[[Category:Articles with example Haskell code]]
[[Category:Articles with example Python code]]
[[Category:Functional programming]]
[[Category:Category theory]]
[[Category:Recursion]]</text>
      <sha1>dkczg2vw6z739sxrt6wedjqr8qz6ewg</sha1>
    </revision>
  </page>
  <page>
    <title>Curve orientation</title>
    <ns>0</ns>
    <id>3633706</id>
    <revision>
      <id>769058314</id>
      <parentid>747138958</parentid>
      <timestamp>2017-03-07T09:39:31Z</timestamp>
      <contributor>
        <username>Blah</username>
        <id>24964259</id>
      </contributor>
      <minor/>
      <comment>/* Practical considerations */ typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8526">{{unref|date=September 2013}}
{{about|closed curves|the orientation of open curves|Differential geometry of curves#Tangent vectors}}
In [[mathematics]], a '''positively oriented curve''' is a planar [[simple closed curve]] (that is, a curve in the plane whose starting point is also the end point and which has no other self-intersections) such that when traveling on it one always has the curve interior to the left (and consequently, the curve exterior to the right). If in the above definition one interchanges left and right, one obtains a '''negatively oriented curve'''.

Crucial to this definition is the fact that every simple closed curve admits a well-defined interior; that follows from the [[Jordan curve theorem]].

All simple closed curves can be classified as negatively oriented ([[clockwise]]), positively oriented ([[counterclockwise]]), or [[orientability|non-orientable]].  The [[Inner/Outer labeling|inner loop]] of a beltway road in the United States (or other countries where people drive on the right side of the road) would be an example of a negatively oriented (clockwise) curve.  A [[circle]] oriented [[counterclockwise]] is an example of a positively oriented curve. The same circle oriented clockwise would be a negatively oriented curve.

The concept of ''orientation'' of a curve is just a particular case of the notion of [[orientation (mathematics)|orientation]] of a [[manifold]] (that is, besides orientation of a curve one may also speak of orientation of a [[Surface (topology)|surface]], [[hypersurface]], etc.). Here, the interior and the exterior of a curve both inherit the usual orientation of the plane. The positive orientation on the curve is then the orientation it inherits as the boundary of its interior; the negative orientation is inherited from the exterior.

== Orientation of a simple polygon ==

[[Image:determining orientation.png|right|Selecting reference points.]]

In two dimensions, given an ordered set of three or more connected vertices (points) (such as in [[Connect the dots|connect-the-dots]]) which forms a [[simple polygon]], the orientation of the resulting [[polygon]] is directly related to the [[Angle#Positive and negative angles|sign of the angle]] at any [[Vertex (geometry)|vertex]] of the [[convex hull]] of the polygon, for example, of the angle ABC in the picture. In computations, the sign of the smaller angle formed by a pair of vectors is typically determined by the sign of the [[cross  product]] of the vectors. The latter one may be calculated  as the sign of the [[determinant]] of their orientation matrix. In the particular case when the two vectors are defined by two [[line segment]]s with common endpoint, such as the sides BA and BC of the angle ABC in our example, the orientation matrix may be defined as follows: 
:&lt;math&gt;\mathbf{O} = \begin{bmatrix}
1 &amp; x_A &amp; y_A \\
1 &amp; x_B &amp; y_B \\
1 &amp; x_C &amp; y_C \end{bmatrix}.&lt;/math&gt;

A formula for its determinant may be obtained, e.g., using the method of [[cofactor expansion]]:

: &lt;math&gt;\begin{align}
\det(O) &amp;= 1\begin{vmatrix}x_B &amp; y_B \\
x_C &amp; y_C \end{vmatrix}
-x_A \begin{vmatrix} 1 &amp; y_B \\
1 &amp; y_C \end{vmatrix}
+y_A \begin{vmatrix} 1 &amp; x_B \\
1 &amp; x_C \end{vmatrix} \\
&amp;= x_B y_C-y_B x_C-x_A y_C+x_A y_B+y_A x_C-y_A x_B \\
&amp;= (x_B y_C+x_A y_B+y_A x_C)-(y_A x_B+y_B x_C+x_A y_C).
\end{align}
&lt;/math&gt;

If the determinant is negative, then the polygon is oriented clockwise.  If the determinant is positive, the polygon is oriented counterclockwise.  The determinant  is non-zero if points A, B, and C are non-[[collinear]].  In the above example, with points ordered A, B, C, etc., the determinant is negative, and therefore the polygon is clockwise.

===Practical considerations===
In practical applications, the following considerations are commonly taken into an account.

One does not need to construct the convex hull of a polygon to find a suitable vertex. A common choice is the vertex of the polygon with the smallest X-coordinate. If there are several of them, the one with the smallest Y-coordinate is picked. It is guaranteed to be the vertex of the convex hull of the polygon. Alternatively, the vertex with the smallest Y-coordinate among the ones with the largest X-coordinates or the vertex with the smallest X-coordinate among the ones with the largest Y-coordinates (or any other of 8 "smallest, largest" X/Y combinations) will do as well. 

If the orientation of a [[convex polygon]] is sought, then, of course, any vertex may be picked.

For numerical reasons, the following equivalent formula for the determinant is commonly used:

: &lt;math&gt;
\det(O) = (x_B-x_A)(y_C-y_A)-(x_C-x_A)(y_B-y_A)
&lt;/math&gt;

The latter formula has four multiplications less. What is more important in computer computations involved in most practical applications, such as [[computer graphics]] or [[Computer-aided design|CAD]], the absolute values of the multipliers  are usually smaller (e.g., when A, B, C are within the same [[quadrant (plane geometry)|quadrant]]), thus giving a smaller [[numerical error]] or, in the extreme cases, avoiding the [[arithmetic overflow]].

When it is not known in advance that the sequence of points defines a simple polygon, the following things must be kept in mind. 

For a [[self-intersecting polygon]] ([[complex polygon]]) (or for any self-intersecting curve) there is no natural notion of the "interior", hence the orientation is not defined. At the same time, in [[geometry]] and [[computer graphics]] there are a number of concepts to replace the notion of the "interior" for closed non-simple curves; see, e.g., "[[flood fill]]" and "[[winding number]]".

In "mild" cases of self-intersection, with [[degeneracy (mathematics)|degenerate]] vertices when three consecutive points are allowed be on the same [[straight line]] and form a zero-degree angle, the concept of "interior" still makes sense, but an extra care must be taken in selection of the tested angle.  In the given example, imagine point A to lie on segment BC. In this situation the angle ABC and its determinant will be 0, hence useless. A solution is to test consecutive corners along the polygon (BCD, DEF,...) until a non-zero determinant is found (unless all points lie on the same [[straight line]]). (Notice that the points C, D, E are on the same line and form a 180-degree angle with zero determinant.)

== Local concavity ==
Once the orientation of a polygon formed from an ordered set of vertices is known, the [[concave polygon|concavity]] of a local region of the polygon can be determined using a second orientation matrix.  This matrix is composed of three consecutive vertices which are being examined for concavity.  For example, in the polygon pictured above, if we wanted to know whether the sequence of points F-G-H is [[Concave set|concave]], [[Convex set|convex]], or collinear (flat), we construct the matrix

:&lt;math&gt;\mathbf{O} = \begin{bmatrix}
1 &amp; x_{F} &amp; y_{F} \\
1 &amp; x_{G} &amp; y_{G} \\
1 &amp; x_{H} &amp; y_{H}\end{bmatrix}.&lt;/math&gt;

If the determinant of this matrix is 0, then the sequence is collinear - neither concave nor convex.  If the determinant has the same sign as that of the orientation matrix for the entire polygon, then the sequence is convex.  If the signs differ, then the sequence is concave.  In this example, the polygon is negatively oriented, but the determinant for the points F-G-H is positive, and so the sequence F-G-H is concave.

The following table illustrates rules for determining whether a sequence of points is convex, concave, or flat:

{| class="wikitable"
|-
! width="200"|
! Negatively oriented polygon (clockwise)
! Positively oriented polygon (counterclockwise)
|-
| determinant of orientation matrix for local points is negative
| convex sequence of points
| concave sequence of points
|-
| determinant of orientation matrix for local points is positive
| concave sequence of points
| convex sequence of points
|-
| determinant of orientation matrix for local points is 0
| collinear sequence of points
| collinear sequence of points
|}

==See also==

* [[Differential geometry of curves]]
* [[Orientability]]
* [[Convex hull]]

==References==
{{Reflist}}&lt;!--added above External links/Sources by script-assisted edit--&gt;

==External links==
* http://www.math.hmc.edu/faculty/gu/curves_and_surfaces/curves/_topology.html
* [http://mathworld.wolfram.com/CurveOrientation.html Curve orientation] at [[MathWorld]]

{{DEFAULTSORT:Curve Orientation}}
[[Category:Curves]]
[[Category:Orientation (geometry)]]
[[Category:Polygons]]</text>
      <sha1>9e83e8fcgs97l0l967l2kjw8cwamtea</sha1>
    </revision>
  </page>
  <page>
    <title>Difference quotient</title>
    <ns>0</ns>
    <id>241863</id>
    <revision>
      <id>827855526</id>
      <parentid>827855382</parentid>
      <timestamp>2018-02-27T03:30:29Z</timestamp>
      <contributor>
        <ip>2604:6000:6345:1B00:6570:8B25:3DF5:D6B</ip>
      </contributor>
      <comment>Undid revision 827855382 by [[Special:Contributions/2604:6000:6345:1B00:6570:8B25:3DF5:D6B]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="20382">{{broader|Finite difference}}
{{POV-check|date=April 2015}}

In single-variable [[calculus]], the '''difference quotient''' is usually the name for the expression

:&lt;math&gt; \frac{f(x+h) - f(x)}{h} &lt;/math&gt;

which when taken to the [[Limit of a function|limit]] as ''h'' approaches 0 gives the [[derivative]] of the [[Function (mathematics)|function]] ''f''.&lt;ref name="LaxTerrell2013"&gt;{{cite book|author1=Peter D. Lax|author2=Maria Shea Terrell|title=Calculus With Applications|year=2013|publisher=Springer|isbn=978-1-4614-7946-8|page=119}}&lt;/ref&gt;&lt;ref name="HockettBock2005"&gt;{{cite book|author1=Shirley O. Hockett|author2=David Bock|title=Barron's how to Prepare for the AP Calculus|year=2005|publisher=Barron's Educational Series|isbn=978-0-7641-2382-5|page=44}}&lt;/ref&gt;&lt;ref name="Ryan2010"&gt;{{cite book|author=Mark Ryan|title=Calculus Essentials For Dummies|year=2010|publisher=John Wiley &amp; Sons|isbn=978-0-470-64269-6|pages=41–47}}&lt;/ref&gt;&lt;ref name="NealGustafson2012"&gt;{{cite book|author1=Karla Neal|author2=R. Gustafson|author3=Jeff Hughes|title=Precalculus|year=2012|publisher=Cengage Learning|isbn=0-495-82662-6|page=133}}&lt;/ref&gt; The name of the expression stems from the fact that it is the [[quotient]] of the [[Difference (mathematics)|difference]] of values of the function by the difference of the corresponding values of its argument (the latter is (''x''+''h'')-''x''=''h'' in this case).&lt;ref name="Comenetz2002"&gt;{{cite book|author=Michael Comenetz|title=Calculus: The Elements|year=2002|publisher=World Scientific|isbn=978-981-02-4904-5|pages=71–76 and 151–161}}&lt;/ref&gt;&lt;ref name="Pasch2010"&gt;{{cite book|author=Moritz Pasch|title=Essays on the Foundations of Mathematics by Moritz Pasch|year=2010|publisher=Springer|isbn=978-90-481-9416-2|page=157}}&lt;/ref&gt; The difference quotient is a measure of the '''[[average]] rate of change''' of the function over an [[Interval (mathematics)|interval]] (in this case, an interval of length ''h'').&lt;ref name="WilsonAdamson2008"&gt;{{cite book|author1=Frank C. Wilson|author2=Scott Adamson|title=Applied Calculus|year=2008|publisher=Cengage Learning|isbn=0-618-61104-5|page=177}}&lt;/ref&gt;&lt;ref name="RubySellers2014"/&gt;{{rp|237}}&lt;ref name="HungerfordShaw2008"&gt;{{cite book|author1=Thomas Hungerford|author2=Douglas Shaw|title=Contemporary Precalculus: A Graphing Approach|year=2008|publisher=Cengage Learning|isbn=0-495-10833-2|pages=211–212}}&lt;/ref&gt; The limit of the difference quotient (i.e., the derivative) is thus the [[instantaneous]] rate of change.&lt;ref name="HungerfordShaw2008"/&gt;

By a slight change in notation (and viewpoint), for an interval [''a'', ''b''], the difference quotient

:&lt;math&gt; \frac{f(b) - f(a)}{b-a}&lt;/math&gt;

is called&lt;ref name="Comenetz2002"/&gt; the mean (or average) value of the derivative of ''f'' over the interval [''a'', ''b'']. This name is justified by the [[mean value theorem]], which states that for a [[differentiable function]] ''f'', its derivative ''f′'' reaches its [[Mean of a function|mean value]] at some point in the interval.&lt;ref name="Comenetz2002"/&gt; Geometrically, this difference quotient measures the [[slope]] of the [[secant line]] passing through the points with coordinates (''a'', ''f''(''a'')) and  (''b'', ''f''(''b'')).&lt;ref name="Krantz2014"&gt;{{cite book|author=Steven G. Krantz|title=Foundations of Analysis|year=2014|publisher=CRC Press|isbn=978-1-4822-2075-9|page=127}}&lt;/ref&gt;

Difference quotients are used as approximations in [[numerical differentiation]],&lt;ref name="RubySellers2014"&gt;{{cite book|author1=Tamara Lefcourt Ruby|author2=James Sellers|author3=Lisa Korf |author4=Jeremy Van Horn |author5=Mike Munn|title=Kaplan AP Calculus AB &amp; BC 2015|year=2014|publisher=Kaplan Publishing|isbn=978-1-61865-686-5|page=299}}&lt;/ref&gt; but they have also been subject of criticism in this application.&lt;ref name="GriewankWalther2008"&gt;{{cite book|author1=Andreas Griewank|author2=Andrea Walther|title=Evaluating Derivatives: Principles and Techniques of Algorithmic Differentiation, Second Edition|url=https://books.google.com/books?id=qMLUIsgCwvUC&amp;pg=PA2|year=2008|publisher=SIAM|isbn=978-0-89871-659-7|pages=2–}}&lt;/ref&gt;

The difference quotient is sometimes also called the '''Newton quotient'''&lt;ref name="Krantz2014"/&gt;&lt;ref name="Lang1968"&gt;{{cite book|author=[[Serge Lang]]|title=Analysis 1|year=1968|publisher=Addison-Wesley Publishing Company|page=56}}&lt;/ref&gt;&lt;ref name="Hahn1994"&gt;{{cite book|author=Brian D. Hahn|title=Fortran 90 for Scientists and Engineers|year=1994|publisher=Elsevier|isbn=978-0-340-60034-4|page=276}}&lt;/ref&gt;&lt;ref name="ClaphamNicholson2009"&gt;{{cite book|author1=Christopher Clapham|author2=James Nicholson|title=The Concise Oxford Dictionary of Mathematics|year=2009|publisher=Oxford University Press|isbn=978-0-19-157976-9|page=313}}&lt;/ref&gt; (after [[Isaac Newton]]) or '''Fermat's difference quotient''' (after [[Pierre de Fermat]]).&lt;ref&gt;Donald C. Benson, ''A Smoother Pebble: Mathematical Explorations'', Oxford University Press, 2003, p. 176.&lt;/ref&gt;

==Overview==
The typical notion of the difference quotient discussed above is a particular case of a more general concept. The primary vehicle of [[calculus]] and other higher mathematics is the [[Function (mathematics)|function]].  Its "input value" is its ''argument'', usually a point ("P") expressible on a graph.  The difference between two points, themselves, is known as their [[Delta (letter)|Delta]] (Δ''P''), as is the difference in their function result, the particular notation being determined by the direction of formation:
*Forward difference:&amp;nbsp; Δ''F''(''P'') = ''F''(''P'' + Δ''P'') − ''F''(''P'');
*Central difference:&amp;nbsp; δF(P) = F(P + ½ΔP) − F(P − ½ΔP);
*Backward difference: ∇F(P) = F(P) − F(P − ΔP).
The general preference is the forward orientation, as F(P) is the base, to which differences (i.e., "ΔP"s) are added to it.  Furthermore,

*If |ΔP| is ''finite'' (meaning measurable), then ΔF(P) is known as a '''[[finite difference]]''', with specific denotations of DP and DF(P);
*If |ΔP| is ''[[infinitesimal]]'' (an infinitely small amount—''&lt;math&gt;\iota&lt;/math&gt;''—usually expressed in standard analysis as a limit: &lt;math&gt;\lim_{\Delta P\rightarrow 0}\,\!&lt;/math&gt;), then ΔF(P) is known as an '''infinitesimal difference''', with specific denotations of dP and dF(P) (in calculus graphing, the point is almost exclusively identified as "x" and F(x) as "y").

The function difference divided by the point difference is known as "difference quotient":

:&lt;math&gt;\frac{\Delta F(P)}{\Delta P}=\frac{F(P+\Delta P)-F(P)}{\Delta P}=\frac{\nabla F(P+\Delta P)}{\Delta P}.\,\!&lt;/math&gt;

If ΔP is infinitesimal, then the difference quotient is a ''[[derivative]]'', otherwise it is a ''[[divided differences|divided difference]]'':

:&lt;math&gt; \text{If } |\Delta P| = \mathit{ \iota}: \quad \frac{\Delta F(P)}{\Delta P}=\frac{dF(P)}{dP}=F'(P)=G(P);\,\!&lt;/math&gt;

:&lt;math&gt; \text{If } |\Delta P| &gt; \mathit{ \iota}: \quad \frac{\Delta F(P)}{\Delta P}=\frac{DF(P)}{DP}=F[P,P+\Delta P].\,\!&lt;/math&gt;

==Defining the point range==
Regardless if ΔP is infinitesimal or finite, there is (at least—in the case of the derivative—theoretically) a point range, where the boundaries are P&amp;nbsp;±&amp;nbsp;(0.5)&amp;nbsp;ΔP (depending on the orientation—ΔF(P), δF(P) or ∇F(P)):
:LB = Lower Boundary; &amp;nbsp; UB = Upper Boundary;
Derivatives can be regarded as functions themselves, harboring their own derivatives. Thus each function is home to sequential degrees ("higher orders") of derivation, or ''differentiation''.  This property can be generalized to all difference quotients.&lt;br&gt;
As this sequencing requires a corresponding boundary splintering, it is practical to break up the point range into smaller, equi-sized sections, with each section being marked by an intermediary point (''P''&lt;sub&gt;''i''&lt;/sub&gt;), where LB = ''P''&lt;sub&gt;0&lt;/sub&gt; and UB = ''P''&lt;sub&gt;''ń''&lt;/sub&gt;, the ''n''th  point, equaling the degree/order:
&lt;!--Improperly formatted formulae--&gt;
   LB =  P&lt;sub&gt;0&lt;/sub&gt;  = P&lt;sub&gt;0&lt;/sub&gt; + 0Δ&lt;sub&gt;1&lt;/sub&gt;P     = P&lt;sub&gt;ń&lt;/sub&gt; − (Ń-0)Δ&lt;sub&gt;1&lt;/sub&gt;P;
         P&lt;sub&gt;1&lt;/sub&gt;  = P&lt;sub&gt;0&lt;/sub&gt; + 1Δ&lt;sub&gt;1&lt;/sub&gt;P     = P&lt;sub&gt;ń&lt;/sub&gt; − (Ń-1)Δ&lt;sub&gt;1&lt;/sub&gt;P;
         P&lt;sub&gt;2&lt;/sub&gt;  = P&lt;sub&gt;0&lt;/sub&gt; + 2Δ&lt;sub&gt;1&lt;/sub&gt;P     = P&lt;sub&gt;ń&lt;/sub&gt; − (Ń-2)Δ&lt;sub&gt;1&lt;/sub&gt;P;
         P&lt;sub&gt;3&lt;/sub&gt;  = P&lt;sub&gt;0&lt;/sub&gt; + 3Δ&lt;sub&gt;1&lt;/sub&gt;P     = P&lt;sub&gt;ń&lt;/sub&gt; − (Ń-3)Δ&lt;sub&gt;1&lt;/sub&gt;P;
             ↓      ↓        ↓       ↓
        P&lt;sub&gt;ń-3&lt;/sub&gt; = P&lt;sub&gt;0&lt;/sub&gt; + (Ń-3)Δ&lt;sub&gt;1&lt;/sub&gt;P = P&lt;sub&gt;ń&lt;/sub&gt; − 3Δ&lt;sub&gt;1&lt;/sub&gt;P;
        P&lt;sub&gt;ń-2&lt;/sub&gt; = P&lt;sub&gt;0&lt;/sub&gt; + (Ń-2)Δ&lt;sub&gt;1&lt;/sub&gt;P = P&lt;sub&gt;ń&lt;/sub&gt; − 2Δ&lt;sub&gt;1&lt;/sub&gt;P;
        P&lt;sub&gt;ń-1&lt;/sub&gt; = P&lt;sub&gt;0&lt;/sub&gt; + (Ń-1)Δ&lt;sub&gt;1&lt;/sub&gt;P = P&lt;sub&gt;ń&lt;/sub&gt; − 1Δ&lt;sub&gt;1&lt;/sub&gt;P;
   UB = P&lt;sub&gt;ń-0&lt;/sub&gt; = P&lt;sub&gt;0&lt;/sub&gt; + (Ń-0)Δ&lt;sub&gt;1&lt;/sub&gt;P = P&lt;sub&gt;ń&lt;/sub&gt; − 0Δ&lt;sub&gt;1&lt;/sub&gt;P = P&lt;sub&gt;ń&lt;/sub&gt;;

   ΔP = Δ&lt;sub&gt;1&lt;/sub&gt;P = P&lt;sub&gt;1&lt;/sub&gt; − P&lt;sub&gt;0&lt;/sub&gt; = P&lt;sub&gt;2&lt;/sub&gt; − P&lt;sub&gt;1&lt;/sub&gt; = P&lt;sub&gt;3&lt;/sub&gt; − P&lt;sub&gt;2&lt;/sub&gt; = ... = P&lt;sub&gt;ń&lt;/sub&gt; − P&lt;sub&gt;ń-1&lt;/sub&gt;;

   ΔB = UB − LB = P&lt;sub&gt;ń&lt;/sub&gt; − P&lt;sub&gt;0&lt;/sub&gt; = Δ&lt;sub&gt;ń&lt;/sub&gt;P = ŃΔ&lt;sub&gt;1&lt;/sub&gt;P.

==The primary difference quotient (''Ń'' = 1)==
:&lt;math&gt;\frac{\Delta F(P_0)}{\Delta P}=\frac{F(P_{\acute{n}})-F(P_0)}{\Delta_{\acute{n}}P}=\frac{F(P_1)-F(P_0)}{\Delta _1P}=\frac{F(P_1)-F(P_0)}{P_1-P_0}.\,\!&lt;/math&gt;

===As a derivative===
:The difference quotient as a derivative needs no explanation, other than to point out that, since P&lt;sub&gt;0&lt;/sub&gt; essentially equals P&lt;sub&gt;1&lt;/sub&gt; = P&lt;sub&gt;2&lt;/sub&gt; = ... = P&lt;sub&gt;ń&lt;/sub&gt; (as the differences are infinitesimal), the [[Leibniz notation]] and derivative expressions do not distinguish P to P&lt;sub&gt;0&lt;/sub&gt; or P&lt;sub&gt;ń&lt;/sub&gt;:

:::&lt;math&gt;\frac{dF(P)}{dP}=\frac{F(P_1)-F(P_0)}{dP}=F'(P)=G(P).\,\!&lt;/math&gt;
There are [[Derivative#Notation for differentiation|other derivative notations]], but these are the most recognized, standard designations.

===As a divided difference===
:A divided difference, however, does require further elucidation, as it equals the average derivative between and including LB and UB:

:: &lt;math&gt;
\begin{align}
P_{(tn)} &amp; =LB+\frac{TN-1}{UT-1}\Delta B \ =UB-\frac{UT-TN}{UT-1}\Delta B; \\[10pt]
&amp; {} \qquad {\color{white}.}(P_{(1)}=LB,\  P_{(ut)}=UB){\color{white}.} \\[10pt]
F'(P_\tilde{a}) &amp; =F'(LB &lt; P &lt; UB)=\sum_{TN=1}^{UT=\infty}\frac{F'(P_{(tn)})}{UT}.
\end{align}
&lt;/math&gt;

:In this interpretation, P&lt;sub&gt;ã&lt;/sub&gt; represents a function extracted, average value of P (midrange, but usually not exactly midpoint), the particular valuation depending on the function averaging it is extracted from.  More formally, P&lt;sub&gt;ã&lt;/sub&gt; is found in the [[mean value theorem]] of calculus, which says:

::''For any function that is continuous on [LB,UB] and differentiable on (LB,UB) there exists some P&lt;sub&gt;ã&lt;/sub&gt; in the interval (LB,UB) such that the secant joining the endpoints of the interval [LB,UB] is parallel to the tangent at P&lt;sub&gt;ã&lt;/sub&gt;.''

:Essentially, P&lt;sub&gt;ã&lt;/sub&gt; denotes some value of P between LB and UB—hence,

::&lt;math&gt;P_\tilde{a}:=LB &lt; P &lt; UB=P_0 &lt; P &lt; P_\acute{n} \,\!&lt;/math&gt;

:which links the mean value result with the divided difference:

:: &lt;math&gt;
\begin{align}
\frac{DF(P_0)}{DP} &amp; = F[P_0,P_1]=\frac{F(P_1)-F(P_0)}{P_1-P_0}=F'(P_0 &lt; P &lt; P_1)=\sum_{TN=1}^{UT=\infty}\frac{F'(P_{(tn)})}{UT}, \\[8pt]
&amp; = \frac{DF(LB)}{DB}=\frac{\Delta F(LB)}{\Delta B}=\frac{\nabla F(UB)}{\Delta B}, \\[8pt]
&amp; = F[LB,UB]=\frac{F(UB)-F(LB)}{UB-LB}, \\[8pt]
&amp; =F'(LB &lt; P &lt; UB)=G(LB &lt; P &lt; UB).
\end{align}
&lt;/math&gt;

:As there is, by its very definition, a tangible difference between LB/P&lt;sub&gt;0&lt;/sub&gt; and UB/P&lt;sub&gt;ń&lt;/sub&gt;, the Leibniz and derivative expressions ''do'' require divarication of the function argument.

==Higher-order difference quotients==

===Second order===

: &lt;math&gt;
\begin{align}
\frac{\Delta^2F(P_0)}{\Delta_1P^2} &amp; =\frac{\Delta F'(P_0)}{\Delta_1P}=\frac{\frac{\Delta F(P_1)}{\Delta_1P}-\frac{\Delta F(P_0)}{\Delta_1P}}{\Delta_1P}, \\[10pt]
&amp; =\frac{\frac{F(P_2)-F(P_1)}{\Delta_1P}-\frac{F(P_1)-F(P_0)}{\Delta_1P}}{\Delta_1P}, \\[10pt]
&amp; =\frac{F(P_2)-2F(P_1)+F(P_0)}{\Delta_1P^2};
\end{align}
&lt;/math&gt;

: &lt;math&gt;
\begin{align}
\frac{d^2F(P)}{dP^2} &amp; = \frac{dF'(P)}{dP}=\frac{F'(P_1)-F'(P_0)}{dP}, \\[10pt]
&amp; =\ \frac{dG(P)}{dP}=\frac{G(P_1)-G(P_0)}{dP}, \\[10pt]
&amp; =\frac{F(P_2)-2F(P_1)+F(P_0)}{dP^2}, \\[10pt]
&amp; =F''(P)=G'(P)=H(P)
\end{align}
&lt;/math&gt;

: &lt;math&gt;
\begin{align}
\frac{D^2F(P_0)}{DP^2} &amp; =\frac{DF'(P_0)}{DP}=\frac{F'(P_1 &lt; P &lt; P_2)-F'(P_0 &lt; P &lt; P_1)}{P_1-P_0}, \\[10pt]
&amp; {\color{white}.} \qquad \ne\frac{F'(P_1)-F'(P_0)}{P_1-P_0}, \\[10pt]
&amp; =F[P_0,P_1,P_2]=\frac{F(P_2)-2F(P_1)+F(P_0)}{(P_1-P_0)^2}, \\[10pt]
&amp; =F''(P_0 &lt; P &lt; P_2)=\sum_{TN=1}^\infty \frac{F''(P_{(tn)})}{UT}, \\[10pt]
&amp; =G'(P_0 &lt; P &lt; P_2)=H(P_0 &lt; P &lt; P_2).
\end{align}
&lt;/math&gt;

===Third order===

: &lt;math&gt;
\begin{align}
\frac{\Delta^3F(P_0)}{\Delta_1P^3} &amp; = \frac{\Delta^2 F'(P_0)}{\Delta_1P^2}=\frac{\Delta F''(P_0)}{\Delta_1P}
=\frac{\frac{\Delta F'(P_1)}{\Delta_1P}-\frac{\Delta F'(P_0)}{\Delta_1P}}{\Delta_1P}, \\[10pt]
&amp; =\frac{\frac{\frac{\Delta F(P_2)}{\Delta_1P}-\frac{\Delta F'(P_1)}{\Delta_1P}}{\Delta_1P}-
\frac{\frac{\Delta F'(P_1)}{\Delta_1P}-\frac{\Delta F'(P_0)}{\Delta_1P}}{\Delta_1P}}{\Delta_1P}, \\[10pt]
&amp; =\frac{\frac{F(P_3)-2F(P_2)+F(P_1)}{\Delta_1P^2}-\frac{F(P_2)-2F(P_1)+F(P_0)}{\Delta_1P^2}}{\Delta_1P}, \\[10pt]
&amp; =\frac{F(P_3)-3F(P_2)+3F(P_1)-F(P_0)}{\Delta_1P^3};
\end{align}
&lt;/math&gt;

: &lt;math&gt;
\begin{align}
\frac{d^3F(P)}{dP^3} &amp; =\frac{d^2F'(P)}{dP^2}=\frac{dF''(P)}{dP}=\frac{F''(P_1)-F''(P_0)}{dP}, \\[10pt]
&amp; =\frac{d^2G(P)}{dP^2}\ =\frac{dG'(P)}{dP}\ =\frac{G'(P_1)-G'(P_0)}{dP}, \\[10pt]
&amp; {\color{white}.}\qquad\qquad\ \ =\frac{dH(P)}{dP}\ =\frac{H(P_1)-H(P_0)}{dP}, \\[10pt]
&amp; =\frac{G(P_2)-2G(P_1)+G(P_0)}{dP^2}, \\[10pt]
&amp; =\frac{F(P_3)-3F(P_2)+3F(P_1)-F(P_0)}{dP^3}, \\[10pt]
&amp; =F'''(P)=G''(P)=H'(P)=I(P);
\end{align}
&lt;/math&gt;

: &lt;math&gt;
\begin{align}
\frac{D^3F(P_0)}{DP^3} &amp; =\frac{D^2F'(P_0)}{DP^2}=\frac{DF''(P_0)}{DP}=\frac{F''(P_1 &lt; P &lt; P_3)-F''(P_0 &lt; P &lt; P_2)}{P_1-P_0}, \\[10pt]
&amp; {\color{white}.}\qquad\qquad\qquad\qquad\qquad\ \ \ne\frac{F''(P_1)-F''(P_0)}{P_1-P_0}, \\[10pt]
&amp; =\frac{\frac{F'(P_2 &lt; P &lt; P_3)-F'(P_1 &lt; P &lt; P_2)}{P_1-P_0}-\frac{F'(P_1 &lt; P &lt; P_2)-F'(P_0 &lt; P &lt; P_1)}{P_1-P_0}}{P_1-P_0}, \\[10pt]
&amp; =\frac{F'(P_2 &lt; P &lt; P_3)-2F'(P_1 &lt; P &lt; P_2)+F'(P_0 &lt; P &lt; P_1)}{(P_1-P_0)^2}, \\[10pt]
&amp; =F[P_0,P_1,P_2,P_3]=\frac{F(P_3)-3F(P_2)+3F(P_1)-F(P_0)}{(P_1-P_0)^3}, \\[10pt]
&amp; =F'''(P_0 &lt; P &lt; P_3)=\sum_{TN=1}^{UT=\infty}\frac{F'''(P_{(tn)})}{UT}, \\[10pt]
&amp; =G''(P_0 &lt; P &lt; P_3)\ =H'(P_0 &lt; P &lt; P_3)=I(P_0 &lt; P &lt; P_3).
\end{align}
&lt;/math&gt;

===''Ń''th order===

: &lt;math&gt;
\begin{align}
\Delta^\acute{n}F(P_0) &amp; =F^{(\acute{n}-1)}(P_1)-F^{(\acute{n}-1)}(P_0), \\[10pt]
&amp; =\frac{F^{(\acute{n}-2)}(P_2)-F^{(\acute{n}-2)}(P_1)}{\Delta_1P}-\frac{F^{(\acute{n}-2)}(P_1)-F^{(\acute{n}-2)}(P_0)}{\Delta_1P}, \\[10pt]
&amp; =\frac{\frac{F^{(\acute{n}-3)}(P_3)-F^{(\acute{n}-3)}(P_2)}{\Delta_1P}-\frac{F^{(\acute{n}-3)}(P_2)-F^{(\acute{n}-3)}(P_1)}{\Delta_1P}}{\Delta_1P} \\[10pt]
&amp; {\color{white}.}\qquad -\frac{\frac{F^{(\acute{n}-3)}(P_2)-F^{(\acute{n}-3)}(P_1)}{\Delta_1P}-\frac{F^{(\acute{n}-3)}(P_1)-F^{(\acute{n}-3)}(P_0)}{\Delta_1P}}{\Delta_1P}, \\[10pt]
&amp; = \cdots
\end{align}
&lt;/math&gt;

: &lt;math&gt;
\begin{align}
\frac{\Delta^\acute{n}F(P_0)}{\Delta_1P^\acute{n}} &amp; =\frac{\sum_{I=0}^{\acute{N}}{-1\choose\acute{N}-I}{\acute{N}\choose I}F(P_0+I\Delta_1P)}{\Delta_1P^\acute{n}}; \\[10pt]
&amp; \frac{\nabla^\acute{n}F(P_\acute{n})}{\Delta_1P^\acute{n}} \\[10pt]
&amp; =\frac{\sum_{I=0}^{\acute{N}}{-1\choose I}{\acute{N}\choose I}F(P_\acute{n}-I\Delta_1P)}{\Delta_1P^\acute{n}};
\end{align}
&lt;/math&gt;

: &lt;math&gt;
\begin{align}
\frac{d^\acute{n}F(P_0)}{dP^\acute{n}} &amp; =\frac{d^{\acute{n}-1}F'(P_0)}{dP^{\acute{n}-1}}
=\frac{d^{\acute{n}-2}F''(P_0)}{dP^{\acute{n}-2}}
=\frac{d^{\acute{n}-3}F'''(P_0)}{dP^{\acute{n}-3}}=\cdots=\frac{d^{\acute{n}-r}F^{(r)}(P_0)}{dP^{\acute{n}-r}},
 \\[10pt]
&amp; =\frac{d^{\acute{n}-1}G(P_0)}{dP^{\acute{n}-1}} \\[10pt]
&amp; =\frac{d^{\acute{n}-2}G'(P_0)}{dP^{\acute{n}-2}}=\ \frac{d^{\acute{n}-3}G''(P_0)}{dP^{\acute{n}-3}}=\cdots=\frac{d^{\acute{n}-r}G^{(r-1)}(P_0)}{dP^{\acute{n}-r}}, \\[10pt]
&amp; {\color{white}.}\qquad\qquad\qquad=\frac{d^{\acute{n}-2}H(P_0)}{dP^{\acute{n}-2}}
=\ \frac{d^{\acute{n}-3}H'(P_0)}{dP^{\acute{n}-3}}=\cdots=\frac{d^{\acute{n}-r}H^{(r-2)}(P_0)}{dP^{\acute{n}-r}}, \\
&amp; {\color{white}.}\qquad\qquad\qquad\qquad\qquad\qquad\ =\ \frac{d^{\acute{n}-3}I(P_0)}{dP^{\acute{n}-3}}
=\cdots=\frac{d^{\acute{n}-r}I^{(r-3)}(P_0)}{dP^{\acute{n}-r}}, \\[10pt]
&amp; =F^{(\acute{n})}(P)=G^{(\acute{n}-1)}(P)=H^{(\acute{n}-2)}(P)=I^{(\acute{n}-3)}(P)=\cdots
\end{align}
&lt;/math&gt;

: &lt;math&gt;
\begin{align}
\frac{D^\acute{n}F(P_0)}{DP^\acute{n}} &amp; =F[P_0,P_1,P_2,P_3,\ldots,P_{\acute{n}-3},P_{\acute{n}-2},P_{\acute{n}-1},P_\acute{n}], \\[10pt]
&amp; =F^{(\acute{n})}(P_0 &lt; P &lt; P_\acute{n})=\sum_{TN=1}^{UT=\infty}\frac{F^{(\acute{n})}(P_{(tn)})}{UT}
 \\[10pt]
&amp; =F^{(\acute{n})}(LB &lt; P &lt; UB)=G^{(\acute{n}-1)}(LB &lt; P &lt; UB)= \cdots 
\end{align}
&lt;/math&gt;

==Applying the divided difference==
The quintessential application of the divided difference is in the presentation of the definite integral, which is nothing more than a finite difference:

: &lt;math&gt;
\begin{align}
\int_{LB}^{UB} G(p) \, dp &amp; = \int_{LB}^{UB} F'(p) \, dp=F(UB)-F(LB), \\[10pt]
&amp; =F[LB,UB]\Delta B, \\[10pt]
&amp; =F'(LB &lt; P &lt; UB)\Delta B, \\[10pt]
&amp; =\ G(LB &lt; P &lt; UB)\Delta B.
\end{align}
&lt;/math&gt;

Given that the mean value, derivative expression form provides all of the same information as the classical integral notation, the mean value form may be the preferable expression, such as in writing venues that only support/accept standard [[ASCII]] text, or in cases that only require the average derivative (such as when finding the average radius in an elliptic integral).
This is especially true for definite integrals that technically have (e.g.) 0 and either &lt;math&gt;\pi\,\!&lt;/math&gt; or &lt;math&gt;2\pi\,\!&lt;/math&gt; as boundaries, with the same divided difference found as that with boundaries of 0 and &lt;math&gt;\begin{matrix}\frac{\pi}{2}\end{matrix}&lt;/math&gt; (thus requiring less averaging effort):

: &lt;math&gt;
\begin{align}
\int_0^{2\pi} F'(p) \, dp &amp; =4\int_0^{\frac{\pi}{2}} F'(p)\, dp=F(2\pi)-F(0)=4(F(\begin{matrix}\frac{\pi}{2}\end{matrix})-F(0)), \\[10pt]
&amp; =2\pi F[0,2\pi]=2\pi F'(0 &lt; P &lt; 2\pi), \\[10pt]
&amp; =2\pi F[0,\begin{matrix}\frac{\pi}{2}\end{matrix}] =2\pi F'(0 &lt; P &lt; \begin{matrix}\frac{\pi}{2}\end{matrix}).
\end{align}
&lt;/math&gt;

This also becomes particularly useful when dealing with ''iterated'' and [[multiple integral|''multiple integral''s]] (ΔA = AU − AL, ΔB = BU − BL, ΔC = CU − CL):

: &lt;math&gt;
\begin{align}
&amp; {} \qquad \int_{CL}^{CU}\int_{BL}^{BU} \int_{AL}^{AU} F'(r,q,p)\,dp\,dq\,dr \\[10pt]
&amp; =\sum_{T\!C=1}^{U\!C=\infty}\left(\sum_{T\!B=1}^{U\!B=\infty}
\left(\sum_{T\!A=1}^{U\!A=\infty}F^{'}(R_{(tc)}:Q_{(tb)}:P_{(ta)})\frac{\Delta A}{U\!A}\right)\frac{\Delta B}{U\!B}\right)\frac{\Delta C}{U\!C}, \\[10pt]
&amp; = F'(C\!L &lt; R &lt; CU:BL &lt; Q &lt; BU:AL &lt; P &lt;\!AU)
\Delta A\,\Delta B\,\Delta C.
\end{align}
&lt;/math&gt;

Hence,

: &lt;math&gt;F'(R,Q:AL &lt; P &lt; AU)=\sum_{T\!A=1}^{U\!A=\infty}
\frac{F'(R,Q:P_{(ta)})}{U\!A};\,\!&lt;/math&gt;

and
:&lt;math&gt;F'(R:BL &lt; Q &lt; BU:AL &lt; P &lt; AU)=\sum_{T\!B=1}^{U\!B=\infty}\left(\sum_{T\!A=1}^{U\!A=\infty}\frac{F'(R:Q_{(tb)}:P_{(ta)})}{U\!A}\right)\frac{1}{U\!B}.\,\!&lt;/math&gt;

==See also==
*[[Divided differences]]
*[[Fermat theory]]
*[[Newton polynomial]]
*[[Rectangle method]]
*[[Quotient rule]]
*[[Symmetric difference quotient]]

==References==
{{reflist|2}}

==External links==
*[http://cis.stvincent.edu/carlsond/ma109/diffquot.html Saint Vincent College: Br. David Carlson, O.S.B.—''MA109 The Difference Quotient'']
*[http://web.mat.bham.ac.uk/D.F.M.Hermans/msmxg6/ln/lnotes78.html University of Birmingham: Dirk Hermans—''Divided Differences'']
*Mathworld:
**[http://mathworld.wolfram.com/DividedDifference.html  ''Divided Difference'']
**[http://mathworld.wolfram.com/Mean-ValueTheorem.html ''Mean-Value Theorem'']
*[http://www.cs.wisc.edu/wpis/abstracts/tr1415r.abs.html University of Wisconsin: [[Thomas W. Reps]] and Louis B. Rall—''Computational Divided Differencing and Divided-Difference Arithmetics'']
*[http://giraldi.org/derivata/derivata.html Interactive simulator on difference quotient to explain the derivative]

{{Isaac Newton}}

[[Category:Differential calculus]]
[[Category:Numerical analysis]]</text>
      <sha1>t4zva2lr9iljn22uh4dr3s72990n56r</sha1>
    </revision>
  </page>
  <page>
    <title>Dynamic equation</title>
    <ns>0</ns>
    <id>18991816</id>
    <revision>
      <id>630007533</id>
      <parentid>456353085</parentid>
      <timestamp>2014-10-17T17:49:27Z</timestamp>
      <contributor>
        <username>Loraof</username>
        <id>22399950</id>
      </contributor>
      <comment>change redirect to dab page</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="251">In [[mathematics]], '''dynamic equation''' can refer to:

*[[difference equation]] in discrete time
*[[differential equation]] in continuous time
*[[time scale calculus]] in combined discrete and continuous time

{{dab}}
[[Category:Dynamical systems]]</text>
      <sha1>5q8kx9zdr7tlk0mhprnfjx1p2mt0ijz</sha1>
    </revision>
  </page>
  <page>
    <title>Esquisse d'un Programme</title>
    <ns>0</ns>
    <id>19575137</id>
    <revision>
      <id>866210915</id>
      <parentid>863715121</parentid>
      <timestamp>2018-10-29T00:13:56Z</timestamp>
      <contributor>
        <ip>186.0.35.252</ip>
      </contributor>
      <comment>update link to project</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12038">'''"Esquisse d'un Programme"''' (Sketch of a Programme) is a famous proposal for long-term mathematical research made by the German-born, French mathematician [[Alexander Grothendieck]] in 1984.&lt;ref&gt;Scharlau, Winifred (September 2008), written at Oberwolfach, Germany, "Who is Alexander Grothendieck", ''Notices of the American Mathematical Society'' (Providence, RI: American Mathematical Society) 55(8): 930–941, {{issn|1088-9477}}, {{OCLC|34550461}}, http://www.ams.org/notices/200808/tx080800930p.pdf&lt;/ref&gt; He pursued the sequence of logically linked ideas in his important project proposal from 1984 until 1988, but his proposed research continues to date to be of major interest in several branches of advanced mathematics. Grothendieck's vision provides inspiration today for several developments in mathematics such as the extension and generalization of [[Galois theory]], which is currently being extended based on his original proposal.

==Brief history==
Submitted in 1984, the ''Esquisse d'un Programme''&lt;ref&gt;Alexander Grothendieck, 1984. "[http://webusers.imj-prg.fr/~leila.schneps/grothendieckcircle/EsquisseFr.pdf Esquisse d'un Programme]", (1984 manuscript), finally published in Schneps and Lochak (1997, I), pp.5-48; English transl., ibid., pp. 243-283. {{MR|1483107}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://matematicas.unex.es/~navarro/res/esquisseeng.pdf|title=Sketch of a programme ''(English translation, hosted by the University of Extremadura)''|accessdate=October 28, 2012}}&lt;/ref&gt; was a proposal submitted by Alexander Grothendieck for a position at the [[French National Centre for Scientific Research|Centre National de la Recherche Scientifique]]. The proposal was not successful, but Grothendieck obtained a special position where, while keeping his affiliation at the university of Montpellier, he was paid by the CNRS and released of his teaching obligations. Grothendieck held this position from 1984 till 1988.&lt;ref&gt;Rehmeyer, Julie (May 9, 2008), "Sensitivity to the Harmony of Things", ''Science News''&lt;/ref&gt;&lt;ref&gt;Jackson, Allyn (November, 2004) "Comme Appelé du Néant - As if summoned from the void: the life of Alexandre Grothendieck", Notices of the AMS&lt;/ref&gt; This proposal was not formally published until 1997, because the author "could not be found, much less his permission requested".&lt;ref&gt;Schneps and Lochak (1997, I) p.1&lt;/ref&gt; The outlines of ''[[dessin d'enfant|dessins d'enfants]]'', or "children's  drawings", and "[[Anabelian geometry]]", that are contained in this manuscript continue to inspire research; thus, "'''Anabelian geometry''' is a proposed theory in [[mathematics]], describing the way the [[algebraic fundamental group]] ''G'' of an [[algebraic variety]] ''V'', or some related geometric object, determines how ''V'' can be mapped into another geometric object ''W'', under the assumption that ''G'' is '''not'''  an [[abelian group]], in the sense of being strongly [[noncommutative]]. The word ''anabelian'' (an [[alpha privative]] ''an-'' before ''abelian'') was introduced in ''Esquisse d'un Programme''. While the work of Grothendieck was for many years unpublished, and unavailable through the traditional formal scholarly channels, the formulation and predictions of the proposed theory received much attention, and some alterations, at the hands of a number of mathematicians. Those who have researched in this area have obtained some expected and related results, and in the 21st century the beginnings of such a theory started to be available."''

==Abstract of Grothendieck's programme==
("''Sommaire''")

*1. The Proposal and enterprise ("Envoi"). 
*2. "[[Oswald Teichmüller|Teichmüller]]'s Lego-game and the [[Galois group]] of &lt;span style="text-decoration:overline"&gt;Q&lt;/span&gt; over Q"  ("Un jeu de “Lego-Teichmüller” et le groupe de [[Galois]] de &lt;span style="text-decoration:overline"&gt;Q&lt;/span&gt; sur Q").
*3. [[Number fields]] associated with [[dessins d'enfant]]". ("Corps de nombres associés à un dessin d’enfant").
*4. [[Regular polyhedron|Regular polyhedra]] over [[finite field]]s ("Polyèdres réguliers sur les corps finis").
*5. [[General topology]] or a '[[Moderated Topology]]' ("Haro sur la topologie dite 'générale', et réflexions heuristiques vers une topologie dite 'modérée").
*6. [[Differentiable theories]] and [[moderated theories]] ("Théories différentiables" (à la Nash) et “théories modérées").
*7. [[Pursuing Stacks]] ("À la Poursuite des Champs").&lt;ref&gt;{{cite web |url=http://www.bangor.ac.uk/r.brown/pstacks.htm |title=Archived copy |accessdate=2008-10-03 |deadurl=yes |archiveurl=https://archive.is/20120722054515/http://www.bangor.ac.uk/r.brown/pstacks.htm |archivedate=2012-07-22 |df= }}&lt;/ref&gt;
*8. [[Two-dimensional geometry]] ("Digressions de géométrie bidimensionnelle").&lt;ref&gt;Cartier, Pierre (2001), "A mad day's work: from Grothendieck to Connes and Kontsevich The evolution of concepts of space and symmetry", ''Bull. Amer. Math. Soc.'' '''38'''(4): 389–408, &lt;http://www.ams.org/bull/2001-38-04/S0273-0979-01-00913-2/S0273-0979-01-00913-2.pdf&gt;. An English translation of Cartier (1998)&lt;/ref&gt;
*9. Summary of proposed studies ("Bilan d’une activité enseignante").
*10. Epilogue.
*Notes

Suggested further reading for the interested mathematical reader is provided
in the ''References'' section.

===Extensions of Galois's theory for groups: Galois groupoids, categories and functors===
[[Galois]] has developed a powerful, fundamental [[algebraic theory]] in mathematics that provides very efficient computations for certain algebraic problems by utilizing the algebraic concept of [[Group (mathematics)|groups]], which is now known as the theory of [[Galois group]]s; such computations were not possible before, and also in many cases are much more effective than the 'direct' calculations without using groups.&lt;ref&gt;Cartier, Pierre (1998), "La Folle Journée, de Grothendieck à Connes et Kontsevich — Évolution des Notions d'Espace et de Symétrie", ''Les Relations entre les Mathématiques et la Physique Théorique — Festschrift for the 40th anniversary of the ''IHÉS'', Institut des Hautes Études Scientifiques'', pp. 11–19&lt;/ref&gt; To begin with, Alexander Grothendieck stated in his proposal:'' "Thus, the group of Galois is realized as the [[automorphism group]] of a concrete, [[pro-finite group]] which respects certain structures that are essential to this group."'' This fundamental, Galois group theory in mathematics has been considerably expanded, at first to [[groupoids]]- as proposed in Alexander Grothendieck's ''Esquisse d' un Programme'' (''EdP'')- and now already partially carried out for groupoids; the latter are now further developed beyond groupoids to categories by several groups of mathematicians. Here, we shall focus only on the well-established and fully validated extensions of Galois' theory. Thus, EdP also proposed and anticipated, along previous Alexander Grothendieck's ''[[IHÉS]]'' seminars ([[SGA1]] to [[SGA4]]) held in the 1960s, the development of even more powerful extensions of the original Galois's theory for groups by utilizing categories, [[functors]] and [[natural transformations]], as well as further expansion of the manifold of ideas presented in Alexander Grothendieck's ''[[Descent theory|Descent Theory]]''. The notion of [[Motive (algebraic geometry)|motive]] has also been pursued actively. This was developed into the [[motivic Galois group]], [[Grothendieck topology]] and Grothendieck category
.&lt;ref&gt;http://planetmath.org/encyclopedia/GrothendieckCategory.html&lt;/ref&gt; Such developments were recently extended in [[algebraic topology]] ''via'' [[representable functor]]s and the [[fundamental groupoid functor]].

==See also==
{{Portal|Category theory|Mathematics}}
* [[Grothendieck's Galois theory]]
* [[Grothendieck's Séminaire de géométrie algébrique]]
* [[Anabelian geometry]]

==References==
{{Reflist}}

===Related works by Alexander Grothendieck===
*[[Alexander Grothendieck]]. 1971, [[Revêtements Étales]] et [[Groupe Fondamental]] ([[SGA1]]), chapter VI: ''[[Catégories fibrées et descente]]'', Lecture Notes in Math. 224, Springer-Verlag: Berlin. 
*Alexander Grothendieck. 1957, Sur quelques points d'algèbre homologique,'' Tohoku Mathematics Journal'', '''9''', 119-221.
*Alexander Grothendieck and [[Jean Dieudonné]].: 1960,'' [[Éléments de géométrie algébrique]]''., Publ. ''[[IHÉS|Inst. des Hautes Études Scientifiques]],'' ''([[IHÉS]])'', '''4'''. 
*Alexander Grothendieck et al.,1971. [[Grothendieck's Séminaire de géométrie algébrique|Séminaire de Géométrie Algébrique du Bois-Marie]], Vol. 1-7, Berlin: Springer-Verlag.
*Alexander Grothendieck. 1962. ''Séminaires en Géométrie Algébrique du Bois-Marie'', Vol. '''2''' - [[Cohomologie Locale des Faisceaux Cohèrents]] et [[Théorèmes de Lefschetz Locaux et Globaux]]., pp.&amp;nbsp;287. (''with an additional contributed exposé by Mme. Michele Raynaud''). (Typewritten manuscript available in French; see also a brief summary in English References Cited: 
**[[Jean-Pierre Serre]]. 1964. [[Cohomologie Galoisienne]], Springer-Verlag: Berlin. 
**[[J. L. Verdier]]. 1965. [[Algèbre homologiques et Catégories derivées]]. [[North Holland Publ. Cie]]).
*Alexander Grothendieck et al. Séminaires en Géometrie Algèbrique- 4, Tome 1, Exposé 1 (or the Appendix to Exposée 1, by `[[N. Bourbaki]]) for more detail and a large number of results. [[AG4]] is freely available in French; also available is an extensive Abstract in English.
*Alexander Grothendieck, 1984. [http://people.math.jussieu.fr/~leila/grothendieckcircle/EsquisseFr.pdf  "Esquisse d'un Programme"], (1984 manuscript), finally published in "[[Geometric Galois Actions]]", L. Schneps, P. Lochak, eds., ''London Math. Soc. Lecture Notes'' '''242''', [[Cambridge University Press]], 1997, pp.&amp;nbsp;5-48; [http://matematicas.unex.es/~navarro/res/esquisseeng.pdf English transl.], ibid., pp.&amp;nbsp;243-283. {{MR|1483107}}.
*Alexander Grothendieck, "[[La longue marche in à travers la théorie de Galois.]]" = "The Long March Towards/Across the Theory of [[Galois]]", 1981 manuscript, [[University of Montpellier]] preprint series 1996, edited by J. Malgoire.

===Other related publications===
*{{citation|first=Leila|last=Schneps|authorlink=Leila Schneps|year=1994|title=The [[Grothendieck]] Theory of [[Dessin d'Enfant|Dessins d'Enfants]]|series=London Mathematical Society Lecture Note Series|publisher=[[Cambridge University Press]]}}.
*{{Citation | editor1-last=Schneps | editor1-first=Leila | editor2-last=Lochak | editor2-first=Pierre | title=Geometric Galois Actions I: Around Grothendieck's Esquisse D'un Programme  | publisher=Cambridge University Press | series=London Mathematical Society Lecture Note Series | volume=242 | isbn=978-0-521-59642-8 | year=1997}}
*{{Citation | editor1-last=Schneps | editor1-first=Leila | editor2-last=Lochak | editor2-first=Pierre | title=Geometric Galois Actions II: The Inverse Galois Problem, Moduli Spaces and Mapping Class Groups | publisher=Cambridge University Press | series=London Mathematical Society Lecture Note Series | volume=243 | isbn=978-0-521-59641-1 | year=1997}}
*{{citation|first1=David|last1=Harbater|first2=Leila|last2=Schneps|title=[[Fundamental group]]s of [[Moduli space|moduli]] and the [[Grothendieck–Teichmüller group]]|journal=Trans. Amer. Math. Soc.|volume=352|year=2000|pages=3117–3148|doi=10.1090/S0002-9947-00-02347-3}}.

==External links==
* [http://planetphysics.org/encyclopedia/QuantumFundamentalGroupoid3.html Fundamental Groupoid Functors], Planet Physics.

* [http://www.neverendingbooks.org/the-best-rejected-proposal-ever The best rejected proposal ever], Never Ending Books, Lieven le Bruyn

* [https://github.com/carmonamateo/NotesAnabeliennes Notes Anabéliennes], A. Grothendieck.

[[Category:Abstract algebra]]
[[Category:Group theory]]
[[Category:Algebraic geometry]]
[[Category:Category theory]]
[[Category:Algebraic topology]]
[[Category:Mathematics papers]]
[[Category:1983 documents]]</text>
      <sha1>efaj7kbsejiahc45vm0bl3tvwez8xx2</sha1>
    </revision>
  </page>
  <page>
    <title>Eulerian coherent structure</title>
    <ns>0</ns>
    <id>51380802</id>
    <revision>
      <id>846582676</id>
      <parentid>841549243</parentid>
      <timestamp>2018-06-19T17:17:33Z</timestamp>
      <contributor>
        <username>Bibcode Bot</username>
        <id>14394459</id>
      </contributor>
      <minor/>
      <comment>Adding 0 [[arXiv|arxiv eprint(s)]], 1 [[bibcode|bibcode(s)]] and 0 [[digital object identifier|doi(s)]]. Did it miss something? Report bugs, errors, and suggestions at [[User talk:Bibcode Bot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2360">{{multiple issues|1=
{{Orphan|date=August 2016}}
{{Primary source|date=September 2016}}
{{Single source|date=September 2016}}
{{COI|date=September 2016}}
{{Notability|date=September 2016}}
}}
In applied mathematics, '''objective Eulerian coherent structures (OECSs)''' are the instantaneously most influential [[surface (mathematics)|surfaces]] or [[curve]]s that exert a major influence on nearby [[Phase_space#Introduction|trajectories]] in a [[dynamical system]] over short time-scales,&lt;ref name=Serra2016OECSs&gt;{{cite journal|last1=Serra M.|first1=Haller G.|title=Objective Eulerian coherent structures|journal=Chaos|volume=26|issue=5|pages=053110|doi=10.1063/1.4951720|url=http://scitation.aip.org/content/aip/journal/chaos/26/5/10.1063/1.4951720|year=2016|arxiv=1512.02112|bibcode=2016Chaos..26e3110S}}&lt;/ref&gt; and are the short-time limit of [[Lagrangian coherent structure]]s (LCSs). Such influence can be of different types, but OECSs invariably create a short-term coherent trajectory pattern for which they serve as a theoretical centerpiece. While LCSs are intrinsically tied to a specific finite time interval, OECSs can be computed at any time instant regardless of the multiple and generally unknown time scales of the system. 

In observations of tracer patterns in nature, one readily identifies short-term variability in material structures such as emerging and dissolving coherent features. However, it is often the underlying structure creating these features that is of interest. While individual tracer trajectories forming coherent patterns are generally sensitive with respect to changes in their initial conditions and the system parameters, OECSs are robust and reveal the instantaneous time-varying skeleton of complex dynamical systems.&lt;ref name=Serra2016OECSs/&gt; Despite OECSs are defined for general dynamical systems, their role in creating coherent patterns is perhaps most readily observable in fluid flows. Therefore, OECSs are suitable in a number of applications ranging from flow control to environmental assessment such as now-casting or short-term forecasting of pattern evolution, where quick operational decisions need to be made. Examples include floating debris, oil spills, surface drifters, and control of unsteady flow separation.

==References==
{{Reflist}}

[[Category:Dynamical systems]]
[[Category:Fluid dynamics]]</text>
      <sha1>eubfxlr6bs90e9mw3zg9v1k6cpr1jkg</sha1>
    </revision>
  </page>
  <page>
    <title>Even and odd functions</title>
    <ns>0</ns>
    <id>592151</id>
    <revision>
      <id>871501189</id>
      <parentid>871493358</parentid>
      <timestamp>2018-12-01T14:22:31Z</timestamp>
      <contributor>
        <username>D.Lazard</username>
        <id>12336988</id>
      </contributor>
      <minor/>
      <comment>/* Even–odd decomposition */ fixing bf</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12188">[[File:Sintay SVG.svg|thumb|The [[sine function]] and all of its [[Taylor polynomial]]s are odd functions. This image shows &lt;math&gt;\sin(x)&lt;/math&gt; and its Taylor approximations, polynomials of degree 1, 3, 5, 7, 9, 11 and 13.]]
[[File:Développement limité du cosinus.svg|thumb|The [[cosine function]] and all of its [[Taylor polynomials]] are even functions. This image shows &lt;math&gt;\cos(x)&lt;/math&gt; and its Taylor approximation of degree 4.]]

In [[mathematics]], '''even functions''' and '''odd functions''' are [[function (mathematics)|function]]s which satisfy particular [[symmetry]] relations, with respect to taking [[additive inverse]]s. They are important in many areas of [[mathematical analysis]], especially the theory of [[power series]] and [[Fourier series]]. They are named for the [[parity (mathematics)|parity]] of the powers of the [[power function]]s which satisfy each condition: the function &lt;math&gt;f(x)=x^n&lt;/math&gt; is an even function if &lt;math&gt;n&lt;/math&gt; is an even integer, and it is an odd function if &lt;math&gt;n&lt;/math&gt; is an odd integer.

==Definition and examples==

Evenness or oddness are generally considered for [[real function]]s, that is real-valued functions of a real variable. However the concepts may be, more generally defined for functions whose [[domain of a function|domain]] and [[codomain]] both have an [[additive inverse]]. This includes [[abelian group|additive groups]], all [[ring (algebra)|ring]]s, all [[field (mathematics)|field]]s, and all [[vector space]]s. Thus, for example, a real function, as could a complex-valued function of a vector variable, and so on.

The given examples are real functions, to illustrate the [[symmetry]] of their graphs.

===Even functions===
[[Image:Function x^2.svg|right|thumb|&lt;math&gt;f(x)=x^2&lt;/math&gt; is an example of an even function.]]
Let &lt;math&gt;f(x)&lt;/math&gt; be a [[real number|real]]-valued function of a real variable. Then &lt;math&gt;f&lt;/math&gt; is '''even''' if the following equation holds for all &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;-x&lt;/math&gt; in the domain of &lt;math&gt;f&lt;/math&gt;:&lt;ref&gt;Gelfand 2002, p. 11&lt;/ref&gt;

:&lt;math&gt;
f(x) = f(-x),
&lt;/math&gt;

or

:&lt;math&gt;
f(x) - f(-x) = 0.
&lt;/math&gt;

Geometrically speaking, the graph face of an even function is [[symmetry|symmetric]] with respect to the ''y''-axis, meaning that its [[graph of a function|graph]] remains unchanged after [[reflection (mathematics)|reflection]] about the ''y''-axis.

Examples of even functions are:
*[[Absolute value]] &lt;math&gt;|x|&lt;/math&gt;
*&lt;math&gt;x^2&lt;/math&gt;
*&lt;math&gt;x^4&lt;/math&gt;
*[[trigonometric function|cosine]] &lt;math&gt;\cos(x)&lt;/math&gt;
*[[hyperbolic function|hyperbolic cosine]] &lt;math&gt;\cosh(x)&lt;/math&gt;

===Odd functions===
[[Image:Function-x3.svg|right|thumb|&lt;math&gt;f(x)=x^3&lt;/math&gt; is an example of an odd function.]]
Again, let &lt;math&gt;f(x)&lt;/math&gt; be a [[real number|real]]-valued function of a real variable. Then &lt;math&gt;f&lt;/math&gt; is '''odd''' if the following equation holds for all &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;-x&lt;/math&gt; in the domain of &lt;math&gt;f&lt;/math&gt;:&lt;ref&gt;Gelfand 2002, p. 72&lt;/ref&gt;

:&lt;math&gt;
-f(x) = f(-x),
&lt;/math&gt;

or

:&lt;math&gt;
f(x) + f(-x) = 0.
&lt;/math&gt;

Geometrically, the graph of an odd function has rotational symmetry with respect to the [[Origin (mathematics)|origin]], meaning that its [[graph of a function|graph]] remains unchanged after [[coordinate rotation|rotation]] of 180 [[degree (angle)|degree]]s about the origin.

Examples of odd functions are:
*&lt;math&gt;f(x)=x&lt;/math&gt;
*&lt;math&gt;f(x)=x^3&lt;/math&gt;
*[[sine|sine]]&lt;math&gt;f(x)=\sin(x)&lt;/math&gt;
*[[hyperbolic function|hyperbolic sine]] &lt;math&gt;f(x)=\sinh(x)&lt;/math&gt;
*[[error function|error function]] &lt;math&gt;f(x)=\operatorname{erf}(x)&lt;/math&gt;

[[Image:Function-x3plus1.svg|right|thumb|&lt;math&gt;f(x)=x^3+1&lt;/math&gt; is neither even nor odd.]]

==Basic properties==

===Uniqueness===
* If a function is even and odd, it is equal to 0 everywhere it is defined.
* If a function is odd, the [[absolute value]] of that function is an even function.

===Addition and subtraction===
* The [[addition|sum]] of two even functions is even.
* The sum of two odd functions is odd.
* The [[subtraction|difference]] between two odd functions is odd.
* The difference between two even functions is even.
* The sum of an even and odd function is neither even nor odd, unless one of the functions is equal to zero over the given [[Domain of a function|domain]].

===Multiplication and division===
* The [[multiplication|product]] of two even functions is an even function.
* The product of two odd functions is an even function.
* The product of an even function and an odd function is an odd function.
* The [[Division (mathematics)|quotient]] of two even functions is an even function.
* The quotient of two odd functions is an even function.
* The quotient of an even function and an odd function is an odd function.

===Composition===
* The [[function composition|composition]] of two even functions is even.
* The composition of two odd functions is odd.
* The composition of an even function and an odd function is even.
* The composition of any function with an even function is even (but not vice versa).

==Even–odd decomposition==
Every function may be uniquely decomposed as the sum of an even and an odd function, which are called respectively the '''even part''' and the '''odd part''' of the function. In fact, if one defines
: &lt;math&gt;\begin{align}
f_\text{e}(x) &amp;= \frac {f(x)+f(-x)} 2,\\
f_\text{o}(x) &amp;= \frac {f(x)-f(-x)} 2,
\end{align}&lt;/math&gt;
then &lt;math&gt;f_\text{e}&lt;/math&gt; is even, &lt;math&gt;f_\text{o}&lt;/math&gt; is odd, and
: &lt;math&gt;f(x)=f_\text{e}(x) + f_\text{o}(x).&lt;/math&gt;

Conversely, if
:&lt;math&gt;f(x)=g(x)+h(x),&lt;/math&gt;
where {{mvar|g}} is even and {{mvar|h}} is odd, then &lt;math&gt;g=f_\text{e}&lt;/math&gt; and &lt;math&gt;h=f_\text{o},&lt;/math&gt;  since
: &lt;math&gt;\begin{align}
2f_\text{e}(x) &amp;=f(x)+f(-x)= g(x) + g(-x) +h(x) +h(-x) = 2g(x),\\
2f_\text{o}(x) &amp;=f(x)-f(-x)= g(x) - g(-x) +h(x) -h(-x) = 2h(x).
\end{align}&lt;/math&gt;

For example, the [[hyperbolic cosine]] and the [[hyperbolic sine]] may be defined as the even and odd parts of the exponential function, as the first one is an even function, the second one is odd, and 
:&lt;math&gt;e^x=\cosh x +\sinh x.&lt;/math&gt;

==Further algebraic properties==
* Any [[linear combination]] of even functions is even, and the even functions form a [[vector space]] over the [[real number|real]]s. Similarly, any linear combination of odd functions is odd, and the odd functions also form a vector space over the reals. In fact, the vector space of ''all'' real functions is the [[direct sum of vector spaces|direct sum]] of the [[linear subspace|subspace]]s of even and odd functions. This is a more abstract way for expressing the property of the preceding section.

*The even functions form a [[algebra over a field|commutative algebra]] over the reals. However, the odd functions do ''not'' form an algebra over the reals, as they are not [[Closure (mathematics)|closed]] under multiplication.

==Calculus properties==

A function's being odd or even does not imply [[differentiable function|differentiability]], or even [[continuous function|continuity]]. For example, the [[Dirichlet function]] is even, but is nowhere continuous. 

In the following, properties involving [[derivative]]s, [[Fourier series]], [[Taylor series]], and so on suppose that these concepts are defined of the functions that are considered.

===Basic calculus properties===
* The [[derivative]] of an even function is odd.
* The derivative of an odd function is even.
* The [[integral]] of an odd function from &amp;minus;''A'' to +''A'' is zero (where ''A'' is finite, and the function has no vertical asymptotes between &amp;minus;''A'' and ''A'').  For an odd function that is integrable over a symmetric interval, e.g. &lt;math&gt;[-A,A]&lt;/math&gt; the result of the integral over that interval is identically zero, e.g. &lt;math&gt;\int_{-A}^{A} f(x) dx = 0&lt;/math&gt;.  &lt;ref&gt;{{cite web|url=http://mathworld.wolfram.com/OddFunction.html|title=Odd Function|first=Weisstein, Eric|last=W.|date=|website=mathworld.wolfram.com}}&lt;/ref&gt;

* The integral of an even function from &amp;minus;''A'' to +''A'' is twice the integral from 0 to +''A'' (where ''A'' is finite, and the function has no vertical asymptotes between &amp;minus;''A'' and ''A''.  This also holds true when ''A'' is infinite, but only if the integral converges).
&lt;math&gt;\int_{-A}^{A} f(x) dx = 2\int_{0}^{A} f(x) dx&lt;/math&gt;.

===Series===
* The [[Maclaurin series]] of an even function includes only even powers.
* The Maclaurin series of an odd function includes only odd powers.
* The [[Fourier series]] of a [[periodic function|periodic]] even function includes only [[trigonometric function|cosine]] terms.
* The Fourier series of a periodic odd function includes only [[trigonometric function|sine]] terms.

==Harmonics==
In [[signal processing]], [[harmonic distortion]] occurs when a [[sine wave]] signal is sent through a memoryless [[nonlinear system]], that is, a system whose output at time &lt;math&gt;t&lt;/math&gt; only depends on the input at time &lt;math&gt;t&lt;/math&gt; and does not depend on the input at any previous times. Such a system is described by a response function &lt;math&gt;V_\text{out}(t) = f(V_\text{in}(t))&lt;/math&gt;. The type of [[harmonic]]s produced depend on the response function &lt;math&gt;f&lt;/math&gt;:&lt;ref&gt;{{Cite web|url=http://www.uaudio.com/webzine/2005/october/content/content2.html|title=Ask the Doctors: Tube vs. Solid-State Harmonics|last=Berners|first=Dave|date=October 2005|website=UA WebZine|publisher=Universal Audio|access-date=2016-09-22|quote=To summarize, if the function f(x) is odd, a cosine input will produce no even harmonics. If the function f(x) is even, a cosine input will produce no odd harmonics (but may contain a DC component). If the function is neither odd nor even, all harmonics may be present in the output.}}&lt;/ref&gt;
* When the response function is even, the resulting signal will consist of only even harmonics of the input sine wave; &lt;math&gt;0f, 2f, 4f, 6f, \dots &lt;/math&gt;
** The [[fundamental frequency|fundamental]] is also an odd harmonic, so will not be present.
** A simple example is a [[full-wave rectifier]].
** The &lt;math&gt;0f&lt;/math&gt; component represents the DC offset, due to the one-sided nature of even-symmetric transfer functions.
* When it is odd, the resulting signal will consist of only odd harmonics of the input sine wave; &lt;math&gt;1f, 3f, 5f, \dots &lt;/math&gt;
** The output signal will be half-wave [[symmetric]].
** A simple example is [[clipping (audio)|clipping]] in a symmetric [[Electronic amplifier|push-pull amplifier]].
* When it is asymmetric, the resulting signal may contain either even or odd harmonics; &lt;math&gt;1f, 2f, 3f, \dots &lt;/math&gt;
** Simple examples are a half-wave rectifier, and clipping in an asymmetrical [[class-A amplifier]].

Note that this does not hold true for more complex waveforms.  A [[sawtooth wave]] contains both even and odd harmonics, for instance.  After even-symmetric full-wave rectification, it becomes a [[triangle wave]], which, other than the DC offset, contains only odd harmonics.

==Case of complex-valued functions==
The definitions for even and odd symmetry for [[Complex number|complex-valued]] functions of a real argument are similar to the real case but involve [[complex conjugation]].

===Even symmetry===
A complex-valued function of a real argument &lt;math&gt;f: \mathbb{R} \mapsto \mathbb{C}&lt;/math&gt; is called even symmetric if:
:&lt;math&gt;f(z)=\overline{f(-z)} \quad \forall z\in\mathbb{R}&lt;/math&gt;

===Odd symmetry===
A complex-valued function of a real argument &lt;math&gt;f: \mathbb{R} \mapsto \mathbb{C}&lt;/math&gt; is called odd symmetric if:
:&lt;math&gt;f(z)=-\overline{f(-z)} \quad \forall z\in\mathbb{R}&lt;/math&gt;

==See also==
*[[Hermitian function]] for a generalization in complex numbers
*[[Taylor series]]
*[[Fourier series]]
*[[Holstein–Herring method]]
*[[Parity (physics)]]

==Notes==
{{reflist}}

==References==
*{{Citation |last=Gelfand |first=I. M. |last2=Glagoleva |first2=E. G. |last3=Shnol |first3=E. E. |author-link=Israel Gelfand |year=2002 | origyear=1969 |title=Functions and Graphs |publisher=Dover Publications |publication-place=Mineola, N.Y |page= |url=http://store.doverpublications.com/0486425649.html |accessdate= }}

[[Category:Calculus]]
[[Category:Parity (mathematics)]]
[[Category:Types of functions]]</text>
      <sha1>g1cxkgd920qt85hicipmcmu4ksxijv1</sha1>
    </revision>
  </page>
  <page>
    <title>Exterior derivative</title>
    <ns>0</ns>
    <id>220782</id>
    <revision>
      <id>866242191</id>
      <parentid>866202186</parentid>
      <timestamp>2018-10-29T04:44:08Z</timestamp>
      <contributor>
        <username>JackozeeHakkiuz</username>
        <id>29347601</id>
      </contributor>
      <minor/>
      <comment>/* Invariant formulations of operators in vector calculus */ Style</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19326">{{Calculus |Multivariable}}

On a [[differentiable manifold]], the '''exterior derivative''' extends the concept of the [[pushforward (differential)|differential]] of a function to [[differential form]]s of higher degree. The exterior derivative was first described in its current form by [[Élie Cartan]] in 1899; it allows for a natural, metric-independent generalization of [[Stokes' theorem]], [[Gauss's theorem]], and [[Green's theorem]] from vector calculus.

If a {{math|''k''}}-form is thought of as measuring the flux through an infinitesimal {{math|''k''}}-[[Parallelepiped#Parallelotope|parallelotope]], then its exterior derivative can be thought of as measuring the net flux through the boundary of a {{math|(''k'' + 1)}}-parallelotope.

== Definition ==
The exterior derivative of a [[differential form]] of degree {{math|''k''}} is a differential form of degree {{math|''k'' + 1.}}

If {{math|&amp;thinsp;''f''&amp;thinsp;}} is a [[Smoothness|smooth function]] (a {{math|0}}-form), then the exterior derivative of {{math|&amp;thinsp;''f''&amp;thinsp;}} is the [[Pushforward (differential)|differential]] of {{math|&amp;thinsp;''f''&amp;thinsp;}}. That is, {{math|''df''&amp;thinsp;}} is the unique [[1-form|{{math|1}}-form]] such that for every smooth [[vector field]] {{math|''X''}}, {{math|1=''df''&amp;thinsp;(''X'') = ''d''&lt;sub&gt;''X''&lt;/sub&gt;&amp;thinsp;''f''&amp;thinsp;}}, where {{math|''d''&lt;sub&gt;''X''&lt;/sub&gt;&amp;thinsp;''f''&amp;thinsp;}} is the [[directional derivative]] of {{math|&amp;thinsp;''f''&amp;thinsp;}} in the direction of {{math|''X''}}.

There are a variety of equivalent definitions of the exterior derivative of a general {{math|''k''}}-form.

===In terms of axioms===
The exterior derivative is defined to be the unique {{math|ℝ}}-linear mapping from {{math|''k''}}-forms to {{math|(''k'' + 1)}}-forms satisfying the following properties:

# {{math|''df''&amp;thinsp;}} is the [[Differential_of_a_function|differential]] of {{math|&amp;thinsp;''f''&amp;thinsp;}}, for {{math|0}}-forms  ([[Smoothness|smooth functions]]) {{math|&amp;thinsp;''f''&amp;thinsp;}}.
# {{math|1=''d''(''df''&amp;thinsp;) = 0}} for any {{math|0}}-form (smooth function) {{math|&amp;thinsp;''f''&amp;thinsp;}}.
# {{math|1=''d''(''α'' ∧ ''β'') = ''dα'' ∧ ''β'' + (−1){{sup|''p''}} (''α'' ∧ ''dβ'')}} where {{mvar|α}} is a {{math|''p''}}-form. That is to say, {{math|''d''}} is an [[derivation (algebra)|antiderivation]] of degree {{math|1}} on the [[exterior algebra]] of differential forms.

The second defining property holds in more generality: in fact, {{math|1=''d''(''dα'') = 0}} for any {{math|''k''}}-form {{mvar|α}}; more succinctly, {{math|1=''d''{{i sup|2}} = 0}}. The third defining property implies as a special case that if {{math|&amp;thinsp;''f''&amp;thinsp;}} is a function and {{mvar|α}} a {{math|''k''}}-form, then {{math|1=''d''(&amp;thinsp;''fα'') = ''d''(&amp;thinsp;''f'' ∧ ''α'') = ''df''&amp;thinsp; ∧ ''α'' + &amp;thinsp;''f''&amp;thinsp; ∧ ''dα''}} because functions are {{math|0}}-forms, and scalar multiplication and the exterior product are equivalent when one of the arguments is a scalar.

===In terms of local coordinates===
Alternatively, one can work entirely in a [[local coordinate system]] {{math|(''x''{{sup|1}}, ..., ''x''{{i sup|''n''}})}}. The coordinate differentials {{math|''dx''{{sup|1}}, ..., ''dx''{{i sup|''n''}}}} form a basis of the space of one-forms, each associated with a coordinate. Given a [[multi-index]] {{math|1=''I'' = (''i''{{sub|1}}, ..., ''i''{{sub|''k''}})}} with {{math|1 ≤ ''i''{{sub|''p''}} ≤ ''n''}} for {{math|1 ≤ ''p'' ≤ ''k''}} (and denoting {{math|''dx''{{i sup|''i''{{sub|1}}}} ∧ ... ∧ ''dx''{{i sup|''i''{{sub|''k''}}}}}} with an [[abuse of notation]] {{math|1=''dx''{{i sup|''I''}}}}), the exterior derivative of a (simple) {{math|''k''}}-form

:&lt;math&gt;\varphi = g\,dx^I = g\,dx^{i_1}\wedge dx^{i_2}\wedge\cdots\wedge dx^{i_k}&lt;/math&gt;

over {{math|ℝ{{sup|''n''}}}} is defined as

:&lt;math&gt;d{\varphi} =  \frac{\partial g}{\partial x^i} dx^i \wedge dx^I&lt;/math&gt;

(using [[Einstein notation]]).  The definition of the exterior derivative is extended [[linear]]ly to a general {{math|''k''}}-form

:&lt;math&gt;\omega = f_I dx^I,&lt;/math&gt;

where each of the components of the multi-index {{math|''I''}} run over all the values in {{math|{1, ..., ''n''}&lt;nowiki/&gt;}}. Note that whenever {{math|''i''}} equals one of the components of the multi-index {{math|''I''}} then {{math|1=''dx''{{i sup|''i''}} ∧ ''dx''{{i sup|''I''}} = 0}} (see [[Exterior product]]).

The definition of the exterior derivative in local coordinates follows from the preceding [[#In terms of axioms|definition in terms of axioms]]. Indeed, with the {{math|''k''}}-form {{math|''φ''}} as defined above,

:&lt;math&gt;\begin{align}
d{\varphi} &amp;= d\left (g\,dx^{i_1} \wedge \cdots \wedge dx^{i_k} \right ) \\
                   &amp;= dg  \wedge \left (dx^{i_1} \wedge \cdots \wedge dx^{i_k} \right ) + g\,d\left (
                      dx^{i_1}\wedge \cdots \wedge dx^{i_k} \right ) \\
                   &amp;= dg \wedge dx^{i_1} \wedge \cdots \wedge dx^{i_k} + g \sum_{p=1}^k (-1)^{p-1} dx^{i_1}
                      \wedge \cdots \wedge dx^{i_{p-1}} \wedge d^2x^{i_p} \wedge dx^{i_{p+1}} \wedge \cdots \wedge d
                      x^{i_k} \\
                     &amp;= dg \wedge dx^{i_1} \wedge \cdots \wedge dx^{i_k} \\
                     &amp;= \frac{\partial g}{\partial x^i} dx^i \wedge dx^{i_1} \wedge \cdots \wedge dx^{i_k} \\
\end{align}&lt;/math&gt;

Here, we have interpreted {{math|''g''}} as a {{math|0}}-form, and then applied the properties of the exterior derivative.

This result extends directly to the general {{math|''k''}}-form {{math|''ω''}} as

:&lt;math&gt;d{\omega} = \frac{\partial f_I}{\partial x^i} dx^i \wedge dx^I .&lt;/math&gt;

In particular, for a {{math|1}}-form {{math|''ω''}}, the components of {{math|''dω''}} in local coordinates are
:&lt;math&gt;(d\omega)_{ij} = \partial_i \omega_j - \partial_j \omega_i. &lt;/math&gt;

===In terms of invariant formula===
Alternatively, an explicit formula can be given for the exterior derivative of a {{math|''k''}}-form {{math|''ω''}}, when paired with {{math|''k'' + 1}} arbitrary smooth [[vector field]]s {{math|''V''&lt;sub&gt;0&lt;/sub&gt;,''V''&lt;sub&gt;1&lt;/sub&gt;, ..., ''V''&lt;sub&gt;''k''&lt;/sub&gt;}}:

:&lt;math&gt;d\omega(V_0,...,V_k) = \sum_i(-1)^{i} V_i \left( \omega \left (V_0, \ldots, \hat V_i, \ldots,V_k \right )\right) +\sum_{i&lt;j}(-1)^{i+j}\omega \left (\left [V_i, V_j \right ], V_0, \ldots, \hat V_i, \ldots, \hat V_j, \ldots, V_k \right )&lt;/math&gt;

where {{math|[''V&lt;sub&gt;i&lt;/sub&gt;'', ''V&lt;sub&gt;j&lt;/sub&gt;'']}} denotes the [[Lie bracket of vector fields|Lie bracket]] and a hat denotes the omission of that element:

:&lt;math&gt;\omega \left (V_0, \ldots, \hat V_i, \ldots,V_k \right ) = \omega \left (V_0, \ldots, V_{i-1}, V_{i+1}, \ldots, V_k \right ).&lt;/math&gt;

In particular, for {{math|1}}-forms we have: {{math|1=''dω''(''X'', ''Y'') = ''X''(''ω''(''Y'')) − ''Y''(''ω''(''X'')) − ''ω''([''X'', ''Y''])}}, where {{math|''X''}} and {{math|''Y''}} are vector fields, {{math|''X''(''ω''(''Y''))}} is the scalar field defined by the vector field {{math|''X'' ∈ Γ(''TM'')}} applied as a differential operator ("directional derivative along ''X''") to the scalar field defined by applying {{math|''ω'' ∈ Γ{{sup|∗}}(''TM'')}} as a covector field to the vector field {{math|''Y'' ∈ Γ(''TM'')}} and likewise for {{math|''Y''(''ω''(''X''))}}.

'''Note:''' Some authors (e.g., Kobayashi–Nomizu and Helgason) use a formula that differs by a factor of {{math|{{sfrac|''k'' + 1}}}}:
:&lt;math&gt;\begin{align}d\omega(V_0,...,V_k) &amp;= {1 \over k+1} \sum_i(-1)^{i} V_i \left( \omega \left (V_0, \ldots, \hat V_i, \ldots,V_k \right )\right) \\
&amp;+ {1 \over k+1}  \sum_{i&lt;j}(-1)^{i+j}\omega \left (\left [V_i, V_j \right ], V_0, \ldots, \hat V_i, \ldots, \hat V_j, \ldots, V_k \right ).\end{align}&lt;/math&gt;

== Examples ==
'''Example 1.''' Consider {{math|1=''σ'' = ''u''&amp;thinsp;''dx''{{i sup|1}} ∧ ''dx''{{i sup|2}}}} over a {{math|1}}-form basis {{math|''dx''{{i sup|1}}, ..., ''dx''{{i sup|''n''}}}}. The exterior derivative is:

:&lt;math&gt;\begin{align}
  d\sigma &amp;= du \wedge dx^1 \wedge dx^2 \\
                    &amp;= \left(\sum_{i=1}^n \frac{\partial u}{\partial x^i} dx^i\right) \wedge dx^1 \wedge dx^2 \\
                    &amp;= \sum_{i=3}^n \left( \frac{\partial u}{\partial x^i} dx^i \wedge dx^1 \wedge dx^2 \right )
\end{align}&lt;/math&gt;

The last formula follows easily from the properties of the [[exterior product]]. Namely, {{math|1=''dx''{{i sup|''i''}} ∧ ''dx''{{i sup|''i''}} = 0}}.

'''Example 2.''' Let {{math|1=''σ'' = ''u''&amp;thinsp;''dx'' + ''v''&amp;thinsp;''dy''}} be a {{math|1}}-form defined over {{math|ℝ{{sup|2}}}}. By applying the above formula to each term (consider {{math|1=''x''{{i sup|1}} = ''x''}} and {{math|1=''x''{{i sup|2}} = ''y''}}) we have the following sum,

:&lt;math&gt;\begin{align}
d\sigma 
    &amp;= \left( \sum_{i=1}^2 \frac{\partial u}{\partial x^i} dx^i \wedge dx \right) + \left( \sum_{i=1}^2 \frac{\partial v}{\partial x^i} dx^i \wedge dy \right) \\
    &amp;= \left(\frac{\partial{u}}{\partial{x}} dx \wedge dx + \frac{\partial{u}}{\partial{y}} dy \wedge dx\right) + \left(\frac{\partial{v}}{\partial{x}} dx \wedge dy + \frac{\partial{v}}{\partial{y}} dy \wedge dy\right) \\
    &amp;= 0 - \frac{\partial{u}}{\partial{y}} dx \wedge dy + \frac{\partial{v}}{\partial{x}} dx \wedge dy + 0 \\
    &amp;= \left(\frac{\partial{v}}{\partial{x}} - \frac{\partial{u}}{\partial{y}}\right) dx \wedge dy
\end{align}&lt;/math&gt;

== Stokes' theorem on manifolds ==
{{main|Stokes' theorem}}

If {{math|''M''}} is a compact smooth orientable {{math|''n''}}-dimensional manifold with boundary, and {{math|''ω''}} is an {{math|(''n'' − 1)}}-form on {{math|''M''}}, then the generalized form of [[Stokes' theorem]] states that:

:&lt;math&gt;\int_M d\omega = \int_{\partial{M}} \omega&lt;/math&gt;

Intuitively, if one thinks of {{math|''M''}} as being divided into infinitesimal regions, and one adds the flux through the boundaries of all the regions, the interior boundaries all cancel out, leaving the total flux through the boundary of {{math|''M''}}.

== Further properties ==

===Closed and exact forms===
{{main article|Closed and exact forms}}
A {{math|''k''}}-form {{math|''ω''}} is called ''closed'' if {{math|1=''dω'' = 0}}; closed forms are the [[Kernel (algebra)|kernel]] of {{math|''d''}}. {{math|''ω''}} is called ''exact'' if {{math|1=''ω'' = ''dα''}} for some {{math|(''k'' − 1)}}-form {{math|''α''}}; exact forms are the [[Image (mathematics)|image]] of {{math|''d''}}. Because {{math|1=''d''{{i sup|2}} = 0}}, every exact form is closed. The [[Poincaré lemma]] states that in a contractible region, the converse is true.

===de Rham cohomology===
Because the exterior derivative {{math|''d''}} has the property that {{math|1=''d''{{i sup|2}} = 0}}, it can be used as the [[Cochain complex|differential]] (coboundary) to define [[de Rham cohomology]] on a manifold. The {{math|''k''}}-th de Rham cohomology (group) is the vector space of closed {{math|''k''}}-forms modulo the exact {{math|''k''}}-forms; as noted in the previous section, the Poincaré lemma states that these vector spaces are trivial for a contractible region, for {{math|''k'' &gt; 0}}. For [[smooth manifold]]s, integration of forms gives a natural homomorphism from the de Rham cohomology to the singular cohomology over {{math|ℝ}}. The theorem of de Rham shows that this map is actually an isomorphism, a far-reaching generalization of the Poincaré lemma. As suggested by the generalized Stokes' theorem, the exterior derivative is the "dual" of the [[Chain complex#Formal definition|boundary map]] on singular simplices.

===Naturality===
The exterior derivative is natural in the technical sense: if {{math|&amp;thinsp;''f'' : ''M'' → ''N''}} is a smooth map and {{math|Ω{{sup|''k''}}}} is the contravariant smooth [[functor]] that assigns to each manifold the space of {{math|''k''}}-forms on the manifold, then the following diagram commutes

:[[Image:Exteriorderivnatural.png|none]]

so {{math|1=''d''(&amp;thinsp;''f''{{i sup|∗}}''ω'') = &amp;thinsp;''f''{{i sup|∗}}''dω''}}, where {{math|&amp;thinsp;''f''{{i sup|∗}}}} denotes the [[pullback (differential geometry)|pullback]] of {{math|&amp;thinsp;''f''&amp;thinsp;}}. This follows from that {{math|&amp;thinsp;''f''{{i sup|∗}}''ω''(·)}}, by definition, is {{math|''ω''(&amp;thinsp;''f''&lt;sub&gt;∗&lt;/sub&gt;(·))}}, {{math|&amp;thinsp;''f''&lt;sub&gt;∗&lt;/sub&gt;}} being the [[Pushforward (differential)|pushforward]] of {{math|&amp;thinsp;''f''&amp;thinsp;}}. Thus {{math|''d''}} is a [[natural transformation]] from {{math|Ω{{sup|''k''}}}} to {{math|Ω{{sup|''k''+1}}}}.

== Exterior derivative in vector calculus ==
Most [[vector calculus]] operators are special cases of, or have close relationships to, the notion of exterior differentiation.

===Gradient===
A [[smooth function]] {{math|&amp;thinsp;''f'' : ''M'' → ℝ}} on a real differentiable manifold {{math|''M''}} is a {{math|0}}-form. The exterior derivative of this {{math|0}}-form is the {{math|1}}-form {{math|''df''}}.

When an inner product {{math|{{langle}}·,·{{rangle}}}} is defined, the [[gradient]] {{math|∇''f''&amp;thinsp;}} of a function {{math|&amp;thinsp;''f''&amp;thinsp;}} is defined as the unique vector in {{math|''V''}} such that its inner product with any element of {{math|''V''}} is the directional derivative of {{math|&amp;thinsp;''f''&amp;thinsp;}} along the vector, that is such that

:&lt;math&gt;\langle \nabla f, \cdot \rangle = df = \sum_{i=1}^n \frac{\partial f}{\partial x^i}\, dx^i .&lt;/math&gt;

That is,
:&lt;math&gt;\nabla f = (df)^\sharp = \sum_{i=1}^n \frac{\partial f}{\partial x^i}\, (dx^i)^\sharp ,&lt;/math&gt;
where {{math|{{music|sharp}}}} denotes the [[musical isomorphism]] {{math|{{music|sharp}} : ''V''{{sup|∗}} → ''V''}} mentioned earlier that is induced by the inner product.

The {{math|1}}-form {{math|''df''&amp;thinsp;}} is a section of the [[cotangent bundle]], that gives a local linear approximation to {{math|&amp;thinsp;''f''&amp;thinsp;}} in the cotangent space at each point.

===Divergence===
A vector field {{math|1=''V'' = (''v''&lt;sub&gt;1&lt;/sub&gt;, ''v''&lt;sub&gt;2&lt;/sub&gt;, ... ''v&lt;sub&gt;n&lt;/sub&gt;'')}} on {{math|ℝ{{sup|''n''}}}} has a corresponding {{math|(''n'' − 1)}}-form

:&lt;math&gt;\begin{align}
\omega_V &amp;= v_1 \left (dx^2 \wedge \cdots \wedge dx^n \right) - v_2 \left (dx^1 \wedge dx^3  \wedge \cdots \wedge dx^n \right ) + \cdots + (-1)^{n-1}v_n \left (dx^1 \wedge \cdots \wedge dx^{n-1} \right) \\
&amp;=\sum_{p=1}^n (-1)^{(p-1)}v_p \left (dx^1 \wedge \cdots \wedge dx^{p-1} \wedge \widehat{dx^{p}} \wedge dx^{p+1} \wedge \cdots \wedge dx^n \right )
\end{align}&lt;/math&gt;

where &lt;math&gt;\widehat{dx^{p}}&lt;/math&gt; denotes the omission of that element.

(For instance, when {{math|1=''n'' = 3}}, i.e. in three-dimensional space, the {{math|2}}-form {{math|''ω&lt;sub&gt;V&lt;/sub&gt;''}} is locally the [[scalar triple product]] with {{math|''V''}}.)  The integral of {{math|''ω&lt;sub&gt;V&lt;/sub&gt;''}} over a hypersurface is the [[flux]] of {{math|''V''}} over that hypersurface.

The exterior derivative of this {{math|(''n'' − 1)}}-form is the {{math|''n''}}-form

:&lt;math&gt;d\omega _V = \operatorname{div} V \left (dx^1 \wedge dx^2 \wedge \cdots \wedge dx^n \right ).&lt;/math&gt;

===Curl===
A vector field {{math|''V''}} on {{math|ℝ{{sup|''n''}}}} also has a corresponding {{math|1}}-form

:&lt;math&gt;\eta_V = v_1  dx^1 + v_2 dx^2 + \cdots + v_n dx^n.&lt;/math&gt;,

Locally, {{math|''η&lt;sub&gt;V&lt;/sub&gt;''}} is the dot product with {{math|''V''}}. The integral of {{math|''η&lt;sub&gt;V&lt;/sub&gt;''}} along a path is the [[Mechanical work|work]] done against {{math|−''V''}} along that path.

When {{math|1=''n'' = 3}}, in three-dimensional space, the exterior derivative of the {{math|1}}-form {{math|''η&lt;sub&gt;V&lt;/sub&gt;''}} is the {{math|2}}-form

:&lt;math&gt;d\eta_V = \omega_{\operatorname{curl} V}.&lt;/math&gt;

===Invariant formulations of operators in vector calculus===
The standard [[vector calculus]] operators can be generalized for any [[pseudo-Riemannian manifold]], and written in coordinate-free notation as follows:

:&lt;math&gt;
\begin{array}{rcccl}
      \operatorname{grad} f &amp;\equiv&amp; \nabla f        &amp;=&amp; \left( d f \right)^\sharp \\
      \operatorname{div} F  &amp;\equiv&amp; \nabla \cdot F  &amp;=&amp; {\star d {\star} ( F^\flat )} \\
      \operatorname{curl} F &amp;\equiv&amp; \nabla \times F &amp;=&amp; \left( {\star} d ( F^\flat ) \right)^\sharp \\
      \Delta f              &amp;\equiv&amp; \nabla^2 f      &amp;=&amp; {\star} d {\star} d f \\
                            &amp;      &amp; \nabla^2 F      &amp;=&amp; \left(d{\star}d{\star}(F^{\flat}) - {\star}d{\star}d(F^{\flat})\right)^{\sharp} , \\
\end{array}
&lt;/math&gt;

where {{math|⋆}} is the [[Hodge dual|Hodge star operator]], {{music|flat}} and {{music|sharp}} are the [[musical isomorphism]]s, {{mvar|&amp;thinsp;f&amp;thinsp;}} is a [[scalar field]] and {{mvar|F}} is a [[vector field]].

Note that the expression for &lt;math&gt;\operatorname{curl}&lt;/math&gt; makes sense only in three dimensions, since it requires &lt;math&gt;\sharp&lt;/math&gt; to act on &lt;math&gt;{\star} d( F^\flat )&lt;/math&gt;, which is a form of degree &lt;math&gt;n-2&lt;/math&gt;.

== See also ==
*[[Exterior covariant derivative]]
*[[de Rham complex]]
*[[Discrete exterior calculus]]
*[[Green's theorem]]
*[[Lie derivative]]
*[[Stokes' theorem]]
*[[Fractal derivative]]

== Notes ==
{{reflist}}

== References ==
* {{cite journal | last =Cartan | first =Élie | author-link =Élie Cartan
 | title =Sur certaines expressions différentielles et le problème de Pfaff
 | journal =Annales Scientifiques de l'École Normale Supérieure |series=Série 3
 | volume =16 | pages =239–332 | publisher =Gauthier-Villars | location =Paris | date =1899 | language =French
 | url =http://www.numdam.org/item?id=ASENS_1899_3_16__239_0
 | issn =0012-9593 | jfm =30.0313.04 | access-date = 2 Feb 2016}}
* {{cite book |author=Conlon, Lawrence |title=Differentiable manifolds |publisher=Birkhäuser |location=Basel, Switzerland |year=2001 |pages= 239 |isbn=0-8176-4134-3 |oclc= |doi=}}
* {{cite book |author=Darling, R. W. R. |title=Differential forms and connections |publisher=Cambridge University Press |location=Cambridge, UK |year=1994 |pages=35 |isbn=0-521-46800-0 |oclc= |doi=}}
* {{cite book |author=Flanders, Harley |title=Differential forms with applications to the physical sciences |publisher=Dover Publications |location=New York |year=1989 |pages=20 |isbn=0-486-66169-5 |oclc= |doi=}}
* {{cite book|url=https://archive.org/details/LoomisL.H.SternbergS.AdvancedCalculusRevisedEditionJonesAndBartlett|title=Advanced Calculus|first=Lynn H.|last2=Sternberg|first2=Shlomo|publisher=Jones and Bartlett|year=1989|isbn=0-486-66169-5|location=Boston|pages=304–473 (ch. 7–11)|chapter=|doi=|oclc=|author=Loomis}}
* {{cite book |author=Ramanan, S. |title=Global calculus |publisher=American Mathematical Society |location=Providence, Rhode Island |year=2005 |pages=54 |isbn=0-8218-3702-8 |oclc= |doi=}}
* {{cite book | last =Spivak | first =Michael | author-link =Michael Spivak | title =[[Calculus on Manifolds (book)|Calculus on Manifolds]]
 | publisher =Westview Press | date =1971 | location =Boulder, Colorado | url = | doi = | isbn =9780805390216 }}
* {{citation|last=Warner|first= Frank W.|title= Foundations of differentiable manifolds and Lie groups|series= Graduate Texts in Mathematics|volume= 94|publisher= Springer|year=1983|isbn= 0-387-90894-3}}

{{Tensors}}

[[Category:Differential forms]]
[[Category:Differential operators]]
[[Category:Generalizations of the derivative]]</text>
      <sha1>9ajb4b1d4vse0eiy42bp2m9j70farbo</sha1>
    </revision>
  </page>
  <page>
    <title>Fairfield Experiment</title>
    <ns>0</ns>
    <id>56835885</id>
    <revision>
      <id>830627796</id>
      <parentid>830522241</parentid>
      <timestamp>2018-03-16T00:16:04Z</timestamp>
      <contributor>
        <username>Leutha</username>
        <id>305208</id>
      </contributor>
      <comment>added [[Category:Govan]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1529">The '''Fairfield experiment''' was an experiment in industrial relations carried out at the [[Fairfield Shipbuilding and Engineering Company]], [[Glasgow]] during the 1960s. The experiment was agreed by [[George Brown, Baron George-Brown|George Brown]], the [[First Secretary]] in [[Harold Wilson]]'s cabinet, in 1966. The company was facing closure, but Brown agreed to provide £1M of state cash to enable the [[Trade Unions]], the management and the shareholders to try out new ways of [[industrial management]].&lt;ref name="Our Glasgow"&gt;{{cite book|last1=Dudgeon|first1=Piers|title=Our Glasgow: Memories of Life in Disappearing Britain|date=2012|publisher=Headline|isbn=9780755364466|url=https://books.google.co.uk/books?id=91tnKrJjWBkC&amp;pg=PT183&amp;lpg=PT183&amp;dq=Fairfield+Experiment&amp;source=bl&amp;ots=XoxLUdW58I&amp;sig=ngWnE64ZKEl4UDXZ7k2J3xW9IKw&amp;hl=en&amp;sa=X&amp;ved=0ahUKEwjKtYOgvevZAhXlLMAKHZz_BGwQ6AEIeTAJ#v=onepage&amp;q=Fairfield%20Experiment&amp;f=false|accessdate=14 March 2018|language=en}}&lt;/ref&gt;

==''The Bowler and the Bunnet''==
{{main|The Bowler and the Bunnet}}
''[[The Bowler and the Bunnet]]'' was a film directed by [[Sean Connery]] and written by [[Cliff Hanley]] about the Fairfield Experiment.&lt;ref name="SSF"&gt;{{cite web|title=Scottish Studies Foundation, The Bowler and the Bunnet|url=http://www.scottishstudies.com/100bowlerandthebunnet-part2.html|website=www.scottishstudies.com|publisher=Scottish Studies Foundation|accessdate=1 March 2018}}&lt;/ref&gt;

==References==
{{reflist}}

[[Category:Operations research]]
[[Category:Govan]]</text>
      <sha1>gbdfk262grbcvk8m5n5xiqt9qvcfih5</sha1>
    </revision>
  </page>
  <page>
    <title>Field of sets</title>
    <ns>0</ns>
    <id>1018676</id>
    <revision>
      <id>859078225</id>
      <parentid>855455556</parentid>
      <timestamp>2018-09-11T15:29:12Z</timestamp>
      <contributor>
        <ip>197.234.164.85</ip>
      </contributor>
      <comment>/* Topological fields of sets */ modal companions</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="18109">{{redirect|Set algebra|the basic properties and laws of sets|Algebra of sets}}
{{expert|mathematics|talk=Confusing}}

In [[mathematics]] a '''field of sets''' is a pair &lt;math&gt;\langle X, \mathcal{F} \rangle&lt;/math&gt; where &lt;math&gt;X &lt;/math&gt; is a [[Set (mathematics)|set]] and &lt;math&gt;\mathcal{F}&lt;/math&gt; is an '''algebra over &lt;math&gt;X&lt;/math&gt;''' i.e., a non-empty subset of the [[power set]] of &lt;math&gt;X &lt;/math&gt; closed under the [[intersection (set theory)|intersection]] and [[union (set theory)|union]] of pairs of sets and under [[complement (set theory)|complements]] of individual sets. In other words, &lt;math&gt;\mathcal{F}&lt;/math&gt; forms a [[subalgebra]] of the [[power set]] [[Boolean algebra (structure)|Boolean algebra]] of &lt;math&gt;X &lt;/math&gt;. (Many authors refer to &lt;math&gt;\mathcal{F}&lt;/math&gt; itself as a field of sets.) Elements of &lt;math&gt;X&lt;/math&gt; are called '''points''' and those of &lt;math&gt;\mathcal{F}&lt;/math&gt; are called '''complexes''' and are said to be the '''admissible sets''' of &lt;math&gt;X &lt;/math&gt;. 

Fields of sets should not be confused with [[field (mathematics)|field]]s in [[ring theory]] nor with [[field (physics)|fields in physics]]. Similarly the term "algebra over &lt;math&gt;X&lt;/math&gt;" is used in the sense of a [[Boolean algebra]] and should not be confused with [[algebra over a field|algebras over fields or rings]] in ring theory.

Fields of sets play an essential role in the [[representation theory]] of Boolean algebras. Every Boolean algebra can be represented as a field of sets.

== Fields of sets in the representation theory of Boolean algebras ==

=== Stone representation ===
Every finite Boolean algebra can be represented as a whole power set - the power set of its set of [[atomic (order theory)|atoms]]; each element of the Boolean algebra corresponds to the set of atoms below it (the join of which is the element). This '''power set representation''' can be constructed more generally for any [[Complete Boolean algebra|complete]] [[atomic (order theory)|atomic]] Boolean algebra.

In the case of Boolean algebras which are not complete and atomic we can still generalize the power set representation by considering fields of sets instead of whole power sets. To do this we first observe that the atoms of a finite Boolean algebra correspond to its [[ultrafilter]]s and that an atom is below an element of a finite Boolean algebra if and only if that element is contained in the ultrafilter corresponding to the atom. This leads us to construct a representation of a Boolean algebra by taking its set of ultrafilters and forming complexes by associating with each element of the Boolean algebra the set of ultrafilters containing that element. This construction does indeed produce a representation of the Boolean algebra as a field of sets and is known as the '''Stone representation'''. It is the basis of [[Stone's representation theorem for Boolean algebras]] and an example of a completion procedure in [[order theory]] based on [[Ideal (order theory)#Applications|ideal]]s or [[Filter (mathematics)|filter]]s, similar to [[Dedekind cut]]s.

Alternatively one can consider the set of [[homomorphism]]s onto the two element Boolean algebra and form complexes by associating each element of the Boolean algebra with the set of such homomorphisms that map it to the top element. (The approach is equivalent as the ultrafilters of a Boolean algebra are precisely the pre-images of the top elements under these homomorphisms.) With this approach one sees that Stone representation can also be regarded as a generalization of the representation of finite Boolean algebras by [[truth table]]s.

=== Separative and compact fields of sets: towards Stone duality ===

* A field of sets is called '''separative''' (or '''differentiated''') if and only if for every pair of distinct points there is a complex containing one and not the other.
* A field of sets is called '''compact''' if and only if for every proper [[filter (mathematics)|filter]] over &lt;math&gt;X\ &lt;/math&gt; the intersection of all the complexes contained in the filter is non-empty.

These definitions arise from considering the [[topological space|topology]] generated by the complexes of a field of sets. Given a field of sets &lt;math&gt;\mathbf{X}= \langle X, \mathcal{F} \rangle&lt;/math&gt; the complexes form a [[base (topology)|base]] for a topology. We denote by &lt;math&gt;T(\mathbf{X})&lt;/math&gt; the corresponding topological space, &lt;math&gt;\langle X, \mathcal{T} \rangle&lt;/math&gt; where &lt;math&gt;\mathcal{T}&lt;/math&gt; is the topology formed by taking arbitrary unions of complexes. Then

* &lt;math&gt;T(\mathbf{X})&lt;/math&gt; is always a [[zero-dimensional space]].
* &lt;math&gt;T(\mathbf{X})&lt;/math&gt; is a [[Hausdorff space]] if and only if &lt;math&gt;\mathbf{X}&lt;/math&gt; is separative.
* &lt;math&gt;T(\mathbf{X})&lt;/math&gt; is a [[compact space]] with compact open sets &lt;math&gt;\mathcal{F}&lt;/math&gt; if and only if &lt;math&gt;\mathbf{X}&lt;/math&gt; is compact.
* &lt;math&gt;T(\mathbf{X})&lt;/math&gt; is a [[Boolean space]] with clopen sets &lt;math&gt;\mathcal{F}&lt;/math&gt; if and only if &lt;math&gt;\mathbf{X}&lt;/math&gt; is both separative and compact (in which case it is described as being '''descriptive''')

The Stone representation of a Boolean algebra is always separative and compact; the corresponding Boolean space is known as the [[Stone space]] of the Boolean algebra. The clopen sets of the Stone space are then precisely the complexes of the Stone representation. The area of mathematics known as [[Stone duality]] is founded on the fact that the Stone representation of a Boolean algebra can be recovered purely from the corresponding Stone space whence a [[Duality (mathematics)|duality]] exists between Boolean algebras and Boolean spaces.

== Fields of sets with additional structure ==

=== Sigma algebras and measure spaces ===

If an algebra over a set is closed under [[countable]] [[intersection (set theory)|intersections]] and countable [[union (set theory)|unions]], it is called a [[sigma algebra]] and the corresponding field of sets is called a '''measurable space'''. The complexes of a measurable space are called '''measurable sets'''. The Loomis-Sikorski theorem provides a Stone-type duality between countably complete Boolean algebras (which may be called '''abstract sigma algebras''') and measurable spaces.

A '''measure space''' is a triple &lt;math&gt;\langle X, \mathcal{F}, \mu  \rangle&lt;/math&gt; where &lt;math&gt;\langle X, \mathcal{F} \rangle&lt;/math&gt; is a measurable space and &lt;math&gt;\mu&lt;/math&gt; is a [[measure theory|measure]] defined on it. If &lt;math&gt;\mu&lt;/math&gt; is in fact a [[probability theory|probability measure]] we speak of a '''probability space''' and call its underlying measurable space a '''sample space'''. The points of a sample space are called '''samples''' and represent potential outcomes while the measurable sets (complexes) are called '''events''' and represent properties of outcomes for which we wish to assign probabilities. (Many use the term '''sample space''' simply for the underlying set of a probability space, particularly in the case where every subset is an event.) Measure spaces and probability spaces play a foundational role in [[measure theory]] and [[probability theory]] respectively.

In applications to [[Physics]] we often deal with measure spaces and probability spaces derived from rich mathematical structures such as [[inner product space]]s or [[topological group]]s which already have a topology associated with them - this should not be confused with the topology generated by taking arbitrary unions of complexes.

=== Topological fields of sets ===

A '''topological field of sets''' is a triple &lt;math&gt;\langle X, \mathcal{T}, \mathcal{F} \rangle&lt;/math&gt; where &lt;math&gt;\langle X, \mathcal{T} \rangle&lt;/math&gt; is a [[topological space]] and &lt;math&gt;\langle X, \mathcal{F} \rangle&lt;/math&gt; is a field of sets which is closed under the [[closure operator]] of &lt;math&gt;\mathcal{T}&lt;/math&gt; or equivalently under the [[interior operator]] i.e. the closure and interior of every complex is also a complex. In other words, &lt;math&gt;\mathcal{F}&lt;/math&gt; forms a subalgebra of the power set [[interior algebra]] on &lt;math&gt;\langle X, \mathcal{T} \rangle&lt;/math&gt;.

Topological fields of sets play a fundamental role in the representation theory of interior algebras and [[Heyting algebra]]s. These two classes of algebraic structures provide the [[Algebraic semantics (mathematical logic)|algebraic semantics]] for the [[modal logic]] ''S4'' (a formal mathematical abstraction of [[epistemic|epistemic logic]]) and [[intuitionistic logic]] respectively. Topological fields of sets representing these algebraic structures provide a related topological [[semantics of logic|semantics]] for these logics.

Every interior algebra can be represented as a topological field of sets with the underlying Boolean algebra of the interior algebra corresponding to the complexes of the topological field of sets and the interior and closure operators of the interior algebra corresponding to those of the topology. Every [[Heyting algebra]] can be represented by a topological field of sets with the underlying lattice of the Heyting algebra corresponding to the lattice of complexes of the topological field of sets that are open in the topology. Moreover the topological field of sets representing a Heyting algebra may be chosen so that the open complexes generate all the complexes as a Boolean algebra. These related representations provide a well defined mathematical apparatus for studying the relationship between truth modalities (possibly true vs necessarily true, studied in modal logic) and notions of provability and refutability (studied in intuitionistic logic) and is thus deeply connected to the theory of [[modal companion]]s of [[intermediate logic]]s.

Given a topological space the [[topology glossary|clopen]] sets trivially form a topological field of sets as each clopen set is its own interior and closure. The Stone representation of a Boolean algebra can be regarded as such a topological field of sets, however in general the topology of a topological field of sets can differ from the topology generated by taking arbitrary unions of complexes and in general the complexes of a topological field of sets need not be open or closed in the topology.

==== Algebraic fields of sets and Stone fields ====

A topological field of sets is called '''algebraic''' if and only if there is a base for its topology consisting of complexes.

If a topological field of sets is both compact and algebraic then its topology is compact and its compact open sets are precisely the open complexes. Moreover, the open complexes form a base for the topology.

Topological fields of sets that are separative, compact and algebraic are called '''Stone fields''' and provide a generalization of the Stone representation of Boolean algebras. Given an interior algebra we can form the Stone representation of its underlying Boolean algebra and then extend this to a topological field of sets by taking the topology generated by the complexes corresponding to the [[Interior algebra#Open and closed elements|open elements]] of the interior algebra (which form a base for a topology). These complexes are then precisely the open complexes and the construction produces a Stone field representing the interior algebra - the '''Stone representation'''. (The topology of the Stone representation is also known as the '''McKinsey-Tarski Stone topology''' after the mathematicians who first generalized Stone's result for Boolean algebras to interior algebras and should not be confused with the Stone topology of the underlying Boolean algebra of the interior algebra which will be a finer topology).

=== Preorder fields ===

A '''preorder field''' is a triple &lt;math&gt;\langle X, \leq , \mathcal{F} \rangle&lt;/math&gt; where &lt;math&gt;\langle X, \leq \rangle&lt;/math&gt; is a [[preorder|preordered set]] and &lt;math&gt;\langle X, \mathcal{F}\rangle &lt;/math&gt; is a field of sets.

Like the topological fields of sets, preorder fields play an important role in the representation theory of interior algebras. Every interior algebra can be represented as a preorder field with its interior and closure operators corresponding to those of the [[Alexandrov topology]] induced by the preorder. In other words,

:&lt;math&gt;\mbox{Int}(S) = \{x \in X :&lt;/math&gt; there exists a &lt;math&gt;y \in S &lt;/math&gt; with &lt;math&gt;y \leq x \}&lt;/math&gt; and
:&lt;math&gt;\mbox{Cl}(S) = \{ x \in X :&lt;/math&gt; there exists a &lt;math&gt;y \in S &lt;/math&gt; with &lt;math&gt;x \leq y \}&lt;/math&gt; for all &lt;math&gt;S \in \mathcal{F}&lt;/math&gt;

Similarly to topological fields of sets, preorder fields arise naturally in modal logic where the points represent the ''possible worlds'' in the [[Kripke semantics]] of a theory in the modal logic ''S4'', the preorder represents the accessibility relation on these possible worlds in this semantics, and the complexes represent sets of possible worlds in which individual sentences in the theory hold, providing a representation of the [[Lindenbaum–Tarski algebra]] of the theory. They are a special case of the [[general frame|general modal frames]] which are fields of sets with an additional accessibility relation providing representations of modal algebras.

==== Algebraic and canonical preorder fields ====

A preorder field is called '''algebraic''' (or '''tight''') if and only if it has a set of complexes &lt;math&gt;\mathcal{A}&lt;/math&gt; which determines the preorder in the following manner: &lt;math&gt;x \leq y&lt;/math&gt; if and only if for every complex &lt;math&gt;S \in \mathcal{A}&lt;/math&gt;, &lt;math&gt;x \in S&lt;/math&gt; implies &lt;math&gt;y \in S&lt;/math&gt;. The preorder fields obtained from ''S4'' theories are always algebraic, the complexes determining the preorder being the sets of possible worlds in which the sentences of the theory closed under necessity hold.

A separative compact algebraic preorder field is said to be '''canonical'''. Given an interior algebra, by replacing the topology of its Stone representation with the corresponding [[specialization (pre)order|canonical preorder]] (specialization preorder) we obtain a representation of the interior algebra as a canonical preorder field. By replacing the preorder by its corresponding [[Alexandrov topology#Duality with preordered sets|Alexandrov topology]] we obtain an alternative representation of the interior algebra as a topological field of sets. (The topology of this "'''Alexandrov representation'''" is just the [[Alexandrov topology#Categorical description of the duality|Alexandrov bi-coreflection]] of the topology of the Stone representation.) While representation of modal algebras by general modal frames is possible for any normal modal algebra, it is only in the case of interior algebras (which correspond to the modal logic ''S4'') that the general modal frame corresponds to topological field of sets in this manner.

=== Complex algebras and fields of sets on relational structures===

The representation of interior algebras by preorder fields can be generalized to a representation theorem for arbitrary (normal) [[Boolean algebras with operators]]. For this we consider structures &lt;math&gt;\langle X, ( R_i )_I, \mathcal{F} \rangle &lt;/math&gt; where &lt;math&gt;\langle X, ( R_i )_I \rangle &lt;/math&gt; is a [[relational structure]] i.e. a set with an indexed family of [[relation (mathematics)|relation]]s defined on it, and &lt;math&gt;\langle X, \mathcal{F} \rangle &lt;/math&gt; is a field of sets. The '''complex algebra''' (or '''algebra of complexes''') determined by a field of sets &lt;math&gt;\mathbf{X} = \langle X, ( R_i )_I, \mathcal{F} \rangle &lt;/math&gt; on a relational structure, is the Boolean algebra with operators

:&lt;math&gt;\mathcal{C}(\mathbf{X}) = \langle \mathcal{F}, \cap, \cup, \prime, \empty, X, ( f_i )_I \rangle&lt;/math&gt;

where for all &lt;math&gt;i \in I&lt;/math&gt;, if &lt;math&gt;R_i\ &lt;/math&gt; is a relation of arity &lt;math&gt;n+1&lt;/math&gt;, then &lt;math&gt;f_i\ &lt;/math&gt; is an operator of arity &lt;math&gt;n&lt;/math&gt; and for all &lt;math&gt;S_1,...,S_n \in \mathcal{F}&lt;/math&gt;

:&lt;math&gt;f_i(S_1,...,S_n) = \{ x \in X :&lt;/math&gt; there exist &lt;math&gt;x_1 \in S_1 ,...,x_n \in S_n&lt;/math&gt; such that &lt;math&gt;R_i(x_1,...,x_n,x) \}\ &lt;/math&gt;

This construction can be generalized to fields of sets on arbitrary [[algebraic structure]]s having both [[Operation (mathematics)|operators]] and relations as operators can be viewed as a special case of relations. If &lt;math&gt;\mathcal{F}&lt;/math&gt; is the whole power set of &lt;math&gt;X\ &lt;/math&gt; then &lt;math&gt;\mathcal{C}(\mathbf{X})&lt;/math&gt; is called a '''full complex algebra''' or '''power algebra'''.

Every (normal) Boolean algebra with operators can be represented as a field of sets on a relational structure in the sense that it is [[isomorphism|isomorphic]] to the complex algebra corresponding to the field.

(Historically the term '''complex''' was first used in the case where the algebraic structure was a [[group (mathematics)|group]] and has its origins in 19th century [[group theory]] where a subset of a group was called a '''complex'''.)

== See also==

* [[List of Boolean algebra topics]]
* [[Algebra of sets]]
* [[Sigma algebra]]
* [[Measure theory]]
* [[Probability theory]]
* [[Interior algebra]]
* [[Alexandrov topology]]
* [[Stone's representation theorem for Boolean algebras]]
* [[Stone duality]]
* [[Boolean ring]]
* [[Preordered field]]
* [[General frame]]

==References==

* [[Robert Goldblatt|Goldblatt, R.]], ''Algebraic Polymodal Logic: A Survey'', Logic Journal of the IGPL, Volume 8, Issue 4, p.&amp;nbsp;393-450, July 2000
* Goldblatt, R., ''Varieties of complex algebras'', Annals of Pure and Applied Logic, 44, p.&amp;nbsp;173-242, 1989
* {{cite book
 | last = Johnstone
 | first = Peter T.
 | authorlink = Peter T. Johnstone
 | year = 1982
 | title = Stone spaces
 | edition = 3rd
 | publisher = Cambridge University Press
 | location = Cambridge
 | isbn = 0-521-33779-8
}}
* Naturman, C.A., ''Interior Algebras and Topology'', Ph.D. thesis, University of Cape Town Department of Mathematics, 1991
* Patrick Blackburn, Johan F.A.K. van Benthem, Frank Wolter ed., ''Handbook of Modal Logic, Volume 3 of Studies in Logic and Practical Reasoning'', Elsevier, 2006

==External links==
*{{springer|title=Algebra of sets|id=p/a011400}}

[[Category:Boolean algebra]]
[[Category:Set families]]</text>
      <sha1>42mnf042x9yjefhz9rhcipzpc25x8tl</sha1>
    </revision>
  </page>
  <page>
    <title>Fly algorithm</title>
    <ns>0</ns>
    <id>52188012</id>
    <revision>
      <id>866843074</id>
      <parentid>865274567</parentid>
      <timestamp>2018-11-01T22:22:54Z</timestamp>
      <contributor>
        <username>BattyBot</username>
        <id>15996738</id>
      </contributor>
      <comment>Removed [[Template:Multiple issues]] and [[WP:AWB/GF|General fixes]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="31476">{{Evolutionary algorithms}}
{{COI|date=July 2018}}

== History ==

The Fly Algorithm is a type of [[cooperative coevolution]] based on the Parisian approach.&lt;ref name=Collet2009&gt;{{cite book |title=Optimization in Signal and Image Processing|chapter=Artificial evolution and the Parisian approach: applications in the processing of signals and images|last2=Louchet,|first2=Jean|last1=Collet|first1=Pierre|publisher=Wiley-ISTE|date=Oct 2009|isbn=9781848210448|editor1-last=Siarry|editor1-first=Patrick}}&lt;/ref&gt; The Fly Algorithm has first been developed in 1999 in the scope of the application of [[Evolutionary algorithms]] to [[computer stereo vision]].&lt;ref name=LouchetRFIA2000&gt;{{cite conference |title=L’algorithme des mouches : une stratégie d’évolution individuelle appliquée en stéréovision|last1=Louchet|first1=Jean|conference=Reconnaissance des Formes et Intelligence Artificielle (RFIA2000)|date=Feb 2000}}&lt;/ref&gt;&lt;ref name=Louchet2000ICPR&gt;{{cite conference |title=Stereo analysis using individual evolution strategy|last1=Louchet|first1=Jean|publisher=IEEE|conference=Proceedings of 15th International Conference on  Pattern Recognition, 2000 (ICPR’00)|pages=908–911|date=Sep 2000|doi=10.1109/ICPR.2000.905580|isbn=0-7695-0750-6|location=Barcelona, Spain}}&lt;/ref&gt; Unlike the classical image-based approach to stereovision, which extracts image primitives then matches them in order to obtain 3-D information, the Fly Agorithm is based on the direct exploration of the 3-D space of the scene. A fly is defined as a 3-D point described by its coordinates (''x'', ''y'', ''z''). Once a random population of flies has been created in a search space corresponding to the field of view of the cameras, its evolution (based on the Evolutionary Strategy paradigm) used a [[fitness function]] that evaluates how likely the fly is lying on the visible surface of an object, based on the consistency of its image projections. To this end, the fitness function uses the grey levels, colours and/or textures of the calculated fly's projections.

The first application field of the Fly Algorithm has been stereovision.&lt;ref name=LouchetRFIA2000 /&gt;&lt;ref name=Louchet2000ICPR /&gt;&lt;ref name=Louchet2001&gt;{{cite journal |title=Using an Individual Evolution Strategy for Stereovision|last1=Louchet|first1=Jean|publisher=Kluwer Academic Publishers|pages=101–109|date=Jun 2001|doi=10.1023/A:1011544128842|volume=2|issue=2|journal=Genetic Programming and Evolvable Machines}}&lt;/ref&gt;&lt;ref name=Boumaza2003&gt;{{cite conference |title=Mobile robot sensor fusion using flies|last1=Boumaza|first1=Amine|last2=Louchet|first2=Jean|publisher=Springer|conference=European Conference on Genetic Programming (EuroGP 2003)|pages=357–367|date=Apr 2003|doi=10.1007/3-540-36605-9_33|isbn=978-3-540-00976-4|location=Essex, UK|volume=2611|book-title=Lecture Notes on Computer Science}}&lt;/ref&gt; While classical `image priority' approaches use matching features from the stereo images in order to build a 3-D model, the Fly Algorithm directly explores the 3-D space and uses image data to evaluate the validity of 3-D hypotheses. A variant called the "Dynamic Flies" defines the fly as a 6-uple (''x'', ''y'', ''z'', ''x’'', ''y’'', ''z’'') involving the fly's velocity.&lt;ref name=Louchet2002&gt;{{cite book |title=Apprentissage Automatique et Evolution Artificielle|chapter=L’algorithme des mouches dynamiques: guider un robot par évolution artificielle en temps réel|last1=Louchet,|first1=Jean|last2=Guyon|first2=Maud|last3=Lesot|first3=Marie-Jeanne|last4=Boumaza|first4=Amine|publisher=Hermes Sciences Publications|pages=|date=Mar 2002|isbn=274620360X|language=French|url=http://jean.louchet.free.fr/publis/2001ECA.pdf|editor1-last=Lattaud|editor1-first=Claude}}&lt;/ref&gt;&lt;ref name=Louchet2002PatterRec&gt;{{cite journal |title=Dynamic Flies: a new pattern recognition tool applied to stereo sequence processing|last1=Louchet,|first1=Jean|last2=Guyon|first2=Maud|last3=Lesot|first3=Marie-Jeanne|last4=Boumaza|first4=Amine|publisher=Elsevier Science B.V.|pages=335–345|date=Jan 2002|doi=10.1016/S0167-8655(01)00129-5|volume=23|issue=1–3|journal=Pattern Recognition Letters|url=http://jean.louchet.free.fr/publis/2001PRL.pdf}}&lt;/ref&gt; The velocity components are not explicitly taken into account in the fitness calculation but are used in the flies' positions updating and are subject to similar genetic operators (mutation, crossover).

The application of Flies to obstacle avoidance in vehicles&lt;ref name=Bomaza2001EvoIASP&gt;{{cite conference |title=Dynamic Flies: Using Real-time evolution in Robotics|last1=Boumaza|first1=Amine|last2=Louchet|first2=Jean|publisher=Springer|conference=Artificial Evolution in Image Analysis and Signal Processing (EVOIASP2001)|pages=288–297|date=Apr 2001|doi=10.1007/3-540-45365-2_30|isbn=978-3-540-41920-4|location=Como, Italy|volume=2037|book-title=Lecture Notes on Computer Science}}&lt;/ref&gt; exploits the fact that the population of flies is a time compliant, quasi-continuously evolving representation of the scene to directly generate vehicle control signals from the flies. The use of the Fly Algorithm is not strictly restricted to stereo images, as other sensors may be added (e.g. acoustic proximity sensors, etc.) as additional terms to the fitness function being optimised. Odometry information can also be used to speed up the updating of flies' positions, and conversely the flies positions can be used to provide localisation and mapping information.&lt;ref name=Louchet2009EvoIASP&gt;{{cite conference |title=Flies Open a Door to SLAM.|last1=Louchet|first1=Jean|last2=Sapin|first2=Emmanuel|publisher=Springer|conference=Applications of Evolutionary Computation (EvoApplications 2009)|pages=385–394|date=2009|doi= 10.1007/978-3-642-01129-0_43|location=Tübingen, Germany|volume=5484|book-title=Lecture Notes in Computer Science}}&lt;/ref&gt;

Another application field of the Fly Algorithm is reconstruction for emission Tomography in [[nuclear medicine]]. The Fly Algorithm has been successfully applied in [[single-photon emission computed tomography]]&lt;ref name=Bousquet2007EA&gt;{{cite conference |title=Fully Three-Dimensional Tomographic Evolutionary Reconstruction in Nuclear Medicine|last1=Bousquet|first1=Aurélie|last2=Louchet|first2=Jean-Marie|last3=Rocchisani|first3=Jean|publisher=Springer, Heidelberg|conference=Proceedings of the 8th international conference on Artificial Evolution (EA’07)|pages=231–242|date=Oct 2007|doi=10.1007/978-3-540-79305-2_20|isbn=978-3-540-79304-5|location=Tours, France|volume=4926|book-title=Lecture Notes in Computer Science|url=http://jean.louchet.free.fr/publis/EA07Bousquet.pdf|}}&lt;/ref&gt; and [[positron emission tomography]]&lt;ref name=Vidal2009EA&gt;{{cite conference |title=Artificial evolution for 3D PET reconstruction|last1=Vidal|first1=Franck P.|last2=Lazaro-Ponthus|first2=Delphine|last3=Legoupil|first3=Samuel|first4=Jean|first5=Évelyne|first6=Jean-Marie|last4=Louchet|last5=Lutton|last6=Rocchisani|publisher=Springer, Heidelberg|conference=Proceedings of the 9th international conference on Artificial Evolution (EA’09)|pages=37–48|date=Oct 2009|doi=10.1007/978-3-642-14156-0_4|isbn=978-3-642-14155-3|location=Strasbourg, France|volume=5975|book-title=Lecture Notes in Computer Science|url=http://fly4pet.fpvidal.net/pdf/Vidal2009EA.pdf|}}&lt;/ref&gt;
.&lt;ref name=Vidal2009MIC&gt;{{cite conference |title=PET reconstruction using a cooperative coevolution strategy in LOR space|last1=Vidal|first1=Franck P.|last2=Louchet|first2=Jean|last3=Lutton|first3=Évelyne|last4=Rocchisani|first4=Jean-Marie|publisher=IEEE|conference=Medical Imaging Conference (MIC)|pages=3363–3366|date=Oct-Nov 2009|doi=10.1109/NSSMIC.2009.5401758|location=Orlando, Florida|book-title=IEEE Nuclear Science Symposium Conference Record (NSS/MIC), 2009}}&lt;/ref&gt; Here, each fly is considered a photon emitter and its fitness is based on the conformity of the simulated illumination of the sensors with the actual pattern observed on the sensors. Within this application, the fitness function has been re-defined to use the new concept of 'marginal evaluation'. Here, the fitness of one individual is calculated as its (positive or negative) contribution to the quality of the global population. It is based on the [[Cross-validation (statistics)|leave-one-out cross-validation]] principle. A ''global fitness function'' evaluates the quality of the population as a whole; only then the fitness of an individual (a fly) is calculated as the difference between the global fitness values of the population with and without the particular fly whose ''individual fitness function''  has to be evaluated.&lt;ref name=Vidal2010EvoIASP&gt;{{cite conference |title=New genetic operators in the Fly Algorithm: application to medical PET image reconstruction|last1=Vidal|first1=Franck P.|last2=Louchet|first2=Jean|last4=Lutton|first4=Évelyne|last3=Rocchisani|first3=Jean-Marie|publisher=Springer, Heidelberg|conference=European Workshop on Evolutionary Computation in Image Analysis and Signal Processing (EvoIASP’10)|pages=292–301|date=Apr 2010|doi=10.1007/978-3-642-12239-2_30|isbn=978-3-642-12238-5|location=Istanbul, Turkey|volume=6024|book-title=Lecture Notes in Computer Science|url=http://fly4pet.fpvidal.net/pdf/Vidal2010EvoIASP.pdf|}}&lt;/ref&gt;&lt;ref name=Vidal2010PPSN&gt;{{cite conference |title=Threshold selection, mitosis and dual mutation in cooperative coevolution: application to medical 3D tomography|last1=Vidal|first1=Franck P.|last2=Lutton|first2=Évelyne|last3=Louchet|first3=Jean|last4=Rocchisani|first4=Jean-Marie|publisher=Springer, Heidelberg|conference=International Conference on Parallel Problem Solving From Nature (PPSN'10)|pages=414–423|date=Sep 2010|doi=10.1007/978-3-642-15844-5_42|location=Krakow, Poland|volume=6238|book-title=Lecture Notes in Computer Science|url=http://fly4pet.fpvidal.net/pdf/Vidal2010PPSN.pdf|}}&lt;/ref&gt; In &lt;ref name=Abbood2017SWEVO&gt;&gt;{{cite journal |title=Voxelisation in the 3-D Fly Algorithm for PET|last1=Ali Abbood|first1=Zainab|last2=Lavauzelle|first2=Julien|last3=Lutton|first3=Évelyne|last4=Rocchisani|first4=Jean-Marie|last5=Louchet|first5=Jean|last6=Vidal|first6=Franck P.|publisher=Elsevier|page=???|date=2017|doi=10.1016/j.swevo.2017.04.001|issn=2210-6502|volume=???|issue=???|journal=Swarm and Evolutionary Computation|url=http://fly4pet.fpvidal.net/pdf/Abbood2017SWEVO.pdf|}}&lt;/ref&gt; the fitness of each fly is considered as a `level of confidence'. It is used during the voxelisation process to tweak the fly's individual footprint using implicit modelling (such as [[metaballs]]). It produces smooth results that are more accurate.

More recently it has been used in digital art to generate mosaic-like images or spray paint.&lt;ref name=Abbood2017EvoIASP&gt;{{cite conference |title=Evolutionary Art Using the Fly Algorithm|last1=Ali Abbood|first1=Zainab|last2=Amlal|first2=Othman|last3=Vidal|first3=Franck P.|publisher=Springer|conference=Applications of Evolutionary Computation (EvoApplications 2017)|pages=455–470|date=Apr 2017|doi=10.1007/978-3-319-55849-3_30|location=Amsterdam, The Netherlands|volume=10199|book-title=Lecture Notes in Computer Science|url=http://fly4pet.fpvidal.net/pdf/Abbood017EvoIASP.pdf|}}&lt;/ref&gt; Examples of images can be found on [https://www.youtube.com/playlist?list=PLR_5kWb9r72g-XnYdrdJhRle8aGtqX6CE YouTube]
&lt;!--ref&gt;{{cite conference |title=|last1=|first1=|last2=|first2=|last3=|first3=|last4=|first4=|publisher=|conference=|pages=|date=|doi=|isbn=|location=|volume=|book-title=|url=|}}&lt;/ref--&gt;
&lt;!-- 
&lt;ref name=Vidal2010&gt;{{cite journal |title=Flies for PET: An artificial evolution strategy for image reconstruction in nuclear medicine|last1=Vidal|first1=Franck P.|last4=Lutton|first4=Évelyne|last2=Louchet|first2=Jean|last3=Rocchisani|first3=Jean-Marie|publisher=American Association of Physicists in Medicine|page=3139|date=2010|doi=10.1118/1.3468200|isbn=|volume=37|issue=6|journal=Medical Physics|url=http://fly4pet.fpvidal.net/pdf/Vidal2010MedPhys-A.pdf|}}&lt;/ref&gt;
--&gt;

== Parisian Evolution ==

Here, the population of individuals is considered as a ''society'' where the individuals collaborate toward a common goal. 
This is implemented using an evolutionary algorithm that includes all the common [[genetic operators]] (e.g. mutation, cross-over, selection). 
The main difference is in the fitness function. 
Here two levels of fitness function are used:
* A local fitness function to assess the performance of a given individual (usually used during the selection process).
* A global fitness function to assess the performance of the whole population. Maximising (or minimising depending on the problem considered) this global fitness is the goal of the population.
In addition, a diversity mechanism is required to avoid individuals gathering in only a few areas of the search space. 
Another difference is in the extraction of the problem solution once the evolutionary loop terminates. In classical evolutionary approaches, the best individual corresponds to the solution and the rest of the population is discarded. 
Here, all the individuals (or individuals of a sub-group of the population) are collated to build the problem solution.
The way the fitness functions are constructed and the way the solution extraction is made are of course problem-dependent.

Examples of Parisian Evolution applications include:
* [http://evelyne.lutton.free.fr/FlyAlgo.html The Fly algorithm].
* [http://evelyne.lutton.free.fr/TextRetrieval.html Text-mining].
* [http://evelyne.lutton.free.fr/HandGesture.html Hand gesture recognition].
* [http://evelyne.lutton.free.fr/Cheese.html Modelling complex interactions in industrial agrifood process].
* [http://fpvidal.net/fly4pet/fly4pet.php Positron Emission Tomography reconstruction].

== Disambiguation ==

=== Parisian approach ''vs'' [[cooperative coevolution]] ===

[[Cooperative coevolution]] is a broad class of [[evolutionary algorithm]]s where a complex problem is solved by decomposing it into subcomponents that are solved independently. 
The Parisian approach shares many similarities with the [[cooperative coevolution|cooperative coevolutionary algorithm]]. The Parisian approach makes use of a single-population whereas multi-species may be used in [[cooperative coevolution|cooperative coevolutionary algorithm]]. 
Similar internal evolutionary engines are considered in classical [[evolutionary algorithm]], [[cooperative coevolution|cooperative coevolutionary algorithm]] and Parisian evolution. 
The difference between [[cooperative coevolution|cooperative coevolutionary algorithm]] and Parisian evolution resides in the population's semantics. 
[[cooperative coevolution|Cooperative coevolutionary algorithm]] divides a big problem into sub-problems (groups of individuals) and solves them separately toward the big problem.&lt;ref name=Mesejo2015&gt;{{cite journal |title=Artificial Neuron – Glia Networks Learning Approach Based on Cooperative Coevolution|last1=Mesejo|first1=Pablo|last2=Ibanez|first2=Oscar|last3=Fernandez-blanco|first3=Enrique|last4=Cedron|first4=Francisco|last5=Pazos|first5=Alejandro|last6=Porto-pazos|first6=Ana|pages=1550012|date=2015|doi=10.1142/S0129065715500124|volume=25|issue=4|journal=International Journal of Neural Systems}}&lt;/ref&gt; There is no interaction/breeding between individuals of the different sub-populations, only with individuals of the same sub-population. 
However, Parisian [[evolutionary algorithms]] solve a whole problem as a big component. 
All population's individuals cooperate together to drive the whole population toward attractive areas of the search space.

=== Fly Algorithm ''vs'' [[particle swarm optimisation]] ===
[[Cooperative coevolution]] and [[particle swarm optimisation|particle swarm optimisation (PSO)]] share many similarities. [[particle swarm optimisation|PSO]] is inspired by the social behaviour of bird flocking or fish schooling.&lt;ref name=Kennedy1995&gt;{{cite conference |conference=Proceedings of IEEE International Conference on Neural Networks|title=Particle swarm optimization|last1=Kennedy,|first1=J|last2=Eberhart|first2=R|publisher=IEEE|date=1995|doi=10.1109/ICNN.1995.488968|pages=1942–1948}}&lt;/ref&gt;&lt;ref name=Shi1998&gt;{{cite conference |conference=Proceedings of IEEE International Conference on Evolutionary Computation|title=A modified particle swarm optimizer|last1=Shi,|first1=Y|last2=Eberhart|first2=R|publisher=IEEE|date=1998|doi=10.1109/ICEC.1998.699146|pages=69–73}}&lt;/ref&gt; 
It was initially introduced as a tool for realistic animation in computer graphics. 
It uses complex individuals that interact with each other in order to build visually realistic collective behaviours through adjusting the individuals' behavioural rules (which may use random generators). 
In mathematical optimisation, every particle of the swarm somehow follows its own random path biased toward the best particle of the swarm. 
In the Fly Algorithm, the flies aim at building spatial representations of a scene from actual sensor data; flies do not communicate or explicitly cooperate, and do not use any behavioural model.

Both algorithms are search methods that start with a set of random solutions, which are iteratively corrected toward a global optimum. 
However, the solution of the optimisation problem in the Fly Algorithm is the population (or a subset of the population): The flies implicitly collaborate to build the solution. In [[particle swarm optimisation|PSO]] the solution is a single particle, the one with the best fitness. Another main difference between the Fly Algorithm and with [[particle swarm optimisation|PSO]] is that the Fly Algorithm is not based on any behavioural model but only builds a geometrical representation.

== Applications of the Fly Algorithnm ==
* [[Computer stereo vision]] &lt;ref name=LouchetRFIA2000 /&gt;&lt;ref name=Louchet2000ICPR /&gt;&lt;ref name=Louchet2001 /&gt;&lt;ref name=Boumaza2003 /&gt;
* [[Obstacle avoidance]] &lt;ref name=Louchet2002 /&gt;&lt;ref name=Bomaza2001EvoIASP /&gt;&lt;ref name=Louchet2002PatterRec /&gt;
* [[Simultaneous localization and mapping|Simultaneous localization and mapping (SLAM)]] &lt;ref name=Louchet2009EvoIASP /&gt;
* [[Single-photon emission computed tomography|Single-photon emission computed tomography (SPECT)]] reconstruction &lt;ref name=Bousquet2007EA /&gt;
* [[Positron emission tomography|Positron emission tomography (PET)]] reconstruction &lt;ref name=Vidal2009EA /&gt;&lt;ref name=Vidal2009MIC /&gt;&lt;ref name=Vidal2010EvoIASP /&gt;&lt;ref name=Vidal2010PPSN /&gt;&lt;ref name=Abbood2017SWEVO /&gt;&lt;ref name=Abbood2017EA&gt;{{cite conference |title=Basic, Dual, Adaptive, and Directed Mutation Operators in the Fly Algorithm|last1=Abbood|first1=Zainab Ali|last2=Vidal|first2=Franck P.|conference=13th Biennal International Conference on Artificial Evolution (EA-2017)|pages=106–119|date=2017|location=Paris, France|book-title=Lecture Notes in Computer Science|ISBN=978-2-9539267-7-4}}&lt;/ref&gt;
* [[Digital art]] &lt;ref name=Abbood2017EvoIASP /&gt;&lt;ref name=Abbood2017ArtAndScience&gt;{{cite journal |title=Fly4Arts: Evolutionary Digital Art with the Fly Algorithm|last1=Abbood|first1=Zainab Ali|last2=Vidal|first2=Franck P.|publisher=ISTE OpenScience|pages=1–6|date=Oct 2017|doi=10.21494/ISTE.OP.2017.0177|volume=17- 1|issue=1|journal=Art and Science}}&lt;/ref&gt;

&lt;!--== Methodology ==
=== Fitness functions ===
=== Initialisation ===
=== Selection ===
=== Genetic operators ===
=== Stopping criteria ===
=== Extraction of the solution ===
--&gt;

== Example: Tomography reconstruction ==
&lt;!-- === Pseudocode === --&gt;

{{multiple image
 | width = 100
 | right
 | image1=
 | caption1=Example of image to reconstruct &lt;math&gt;(f)&lt;/math&gt;, which is unknown.
 | image2=sinogram-hot_spheres.png
 | caption2=Sinogram &lt;math&gt;(Y)&lt;/math&gt; of &lt;math&gt;f&lt;/math&gt;, which is known.
 | image3=
 | caption3=Image reconstructed using the Fly algorithm &lt;math&gt;(\hat{f})&lt;/math&gt;.
 | image4=
 | caption4=Sinogram &lt;math&gt;(\hat{Y})&lt;/math&gt; of &lt;math&gt;\hat{f}&lt;/math&gt;.
 | footer=Example of reconstruction of a hot rod phantom using the Fly Algorithm. 
}}

Tomography reconstruction is an [[inverse problem]] that is often [[ill-posed]] due to missing data and/or noise. The answer to the inverse problem is not unique, and in case of extreme noise level it may not even exist. The input data of a reconstruction algorithm may be given as the [[Radon transform]] or sinogram &lt;math&gt;\left(Y\right)&lt;/math&gt; of the data to reconstruct &lt;math&gt;\left(f\right)&lt;/math&gt;. &lt;math&gt;f&lt;/math&gt; is unknown; &lt;math&gt;Y&lt;/math&gt; is known. 
The data acquisition in tomography can be modelled as:

&lt;math&gt;
Y = P[f] + \epsilon
&lt;/math&gt;

where &lt;math&gt;P&lt;/math&gt; is the system matrix or projection operator and &lt;math&gt;\epsilon&lt;/math&gt; corresponds to some [[Shot noise|Poisson noise]]. 
In this case the reconstruction corresponds to the inversion of the [[Radon transform]]:

&lt;math&gt;
f = P^{-1}[Y]
&lt;/math&gt;

Note that &lt;math&gt;P^{-1}&lt;/math&gt; can account for noise, acquisition geometry, etc. 
The Fly Algorithm is an example of [[iterative reconstruction]]. Iterative methods in [[tomographic reconstruction]] are relatively easy to model:

&lt;math&gt;
    \hat{f} = \operatorname{arg\,min} || Y - \hat{Y}||^2_2
&lt;/math&gt;

where &lt;math&gt;\hat{f}&lt;/math&gt; is an estimate of &lt;math&gt;f&lt;/math&gt;, that minimises an error metrics (here [[Norm (mathematics)|{{math|''ℓ&lt;sup&gt;2&lt;/sup&gt;}}-norm]], but other error metrics could be used) between &lt;math&gt;Y&lt;/math&gt; and &lt;math&gt;\hat{Y}&lt;/math&gt;. Note that a [[Regularization (mathematics)|regularisation term]] can be introduced to prevent overfitting and to smooth noise whilst preserving edges. 
Iterative methods can be implemented as follows:
[[File:Iterative-algorithm.svg|512x122px|thumb|right|Iterative correction in tomography reconstruction.]]
   (i) The reconstruction starts using an initial estimate of the image (generally a constant image),
   (ii) Projection data is computed from this image,
   (iii) The estimated projections are compared with the measured projections,
   (iv) Corrections are made to correct the estimated image, and
   (v) The algorithm iterates until convergence of the estimated and measured projection sets.

The [[pseudocode]] below is a step-by-step description of the Fly Algorithm for [[tomographic reconstruction]]. The algorithm follows the steady-state paradigm. For illustrative purposes, advanced genetic operators, such as mitosis, dual mutation, etc.&lt;ref&gt;{{cite conference |title=Threshold selection, mitosis and dual mutation in cooperative co-evolution: Application to medical 3D tomography|last1=Vidal|first1=Franck P.|last2=Lutton|first2=Évelyne|last3=Louchet|first3=Jean|last4=Rocchisani|first4=Jean-Marie|publisher=Springer Berlin / Heidelberg|conference=Parallel Problem Solving from Nature - PPSN XI|pages=414–423|date=Sep 2010|doi=10.1007/978-3-642-15844-5_42|isbn=978-3-642-15843-8|location=Kraków, Poland|volume=6238|book-title=Lecture Notes in Computer Science|url=http://www.fpvidal.net/fly4pet/pdf/Vidal2010PPSN.pdf|}}&lt;/ref&gt;&lt;ref&gt;{{cite conference |title=Basic, Dual, Adaptive, and Directed Mutation Operators in the Fly Algorithm|last1=Ali Abbood|first1=Zainab|last2=Vidal|first2=Franck P.|publisher=Springer-Verlag|conference=13th Biennal International Conference on Artificial Evolution|date=Oct 2017|location=Paris, France|book-title=Lecture Notes in Computer Science}}&lt;/ref&gt; are ignored. A [[JavaScript]] implementation can be found on [http://www.fpvidal.net/fly4pet/demo.php Fly4PET].

&lt;!-- [[File:Fly4pet.svg|512x616px|thumb|right|Flowchart of the Fly Algorithm for Positron Emission Tomography.]] --&gt;
&lt;!-- [[File:evolutionary_PET_reconstructions.png|299x501px|thumb|right|Example of evolutionary reconstructions at successive resolutions (with ''N'' the number of flies and ''TS'' the time-stamp in minutes).]]--&gt;

 '''algorithm''' fly-algorithm '''is'''
     '''input:''' number of flies (''N''), 
            input projection data (''p''&lt;sub&gt;''reference''&lt;/sub&gt;)
     
     '''output:''' the fly population (''F''), 
             the projections estimated from ''F'' (''p''&lt;sub&gt;''estimated''&lt;/sub&gt;)
             the 3-D volume corresponding to the voxelisation of ''F'' (''V''&lt;sub&gt;''F''&lt;/sub&gt;)
     
     '''postcondition:''' the difference between ''p''&lt;sub&gt;''estimated''&lt;/sub&gt; and ''p''&lt;sub&gt;reference&lt;/sub&gt; is minimal.
     
     '''START'''
     
  1.   ''// Initialisation''
  2.   ''// Set the position of the ''N'' flies, i.e. create initial guess''
  3.   '''for each''' fly ''i'' '''in''' fly population ''F'' '''do'''
  4.       ''F''(''i'')&lt;sub&gt;x&lt;/sub&gt; &amp;larr; random(0, 1)
  5.       ''F''(''i'')&lt;sub&gt;y&lt;/sub&gt; &amp;larr; random(0, 1)
  6.       ''F''(''i'')&lt;sub&gt;z&lt;/sub&gt; &amp;larr; random(0, 1)
  7.       Add ''F''(''i'')'s projection in ''p''&lt;sub&gt;''estimated''&lt;/sub&gt;
  8.   
  9.   ''// Compute the population's performance (i.e. the global fitness)''
 10.   ''G''&lt;sub&gt;''fitness''&lt;/sub&gt;(''F'') &amp;larr; ''Error''&lt;sub&gt;metrics&lt;/sub&gt;(''p''&lt;sub&gt;''reference''&lt;/sub&gt;, ''p''&lt;sub&gt;''estimated''&lt;/sub&gt;)
 11.    
 12.   ''f''&lt;sub&gt;''kill''&lt;/sub&gt; &amp;larr; Select a random fly of ''F''
 13.    
 14.   Remove ''f''&lt;sub&gt;''kill''&lt;/sub&gt;'s contribution from ''p''&lt;sub&gt;''estimated''&lt;/sub&gt;
 15.    
 16.   ''// Compute the population's performance without ''f''&lt;sub&gt;''kill''&lt;/sub&gt;''
 17.   ''G''&lt;sub&gt;''fitness''&lt;/sub&gt;(''F''-&lt;nowiki/&gt;{''f''&lt;sub&gt;''kill''&lt;/sub&gt;}) &amp;larr; ''Error''&lt;sub&gt;metrics&lt;/sub&gt;(''p''&lt;sub&gt;''reference''&lt;/sub&gt;, ''p''&lt;sub&gt;''estimated''&lt;/sub&gt;)
 18.    
 19.   // Compare the performances, i.e. compute the fly's local fitness
 20.   ''L''&lt;sub&gt;''fitness''&lt;/sub&gt;(''f''&lt;sub&gt;''kill''&lt;/sub&gt;) &amp;larr; ''G''&lt;sub&gt;''fitness''&lt;/sub&gt;(''F''-&lt;nowiki/&gt;{''f''&lt;sub&gt;''kill''&lt;/sub&gt;}) - ''G''&lt;sub&gt;''fitness''&lt;/sub&gt;(''F'')
 21.    
 22.   '''If''' the local fitness is greater than 0, // Thresholded-selection of a bad fly that can be killed
 23.       '''then''' go to Step 26.   ''// ''f''&lt;sub&gt;''kill''&lt;/sub&gt; is a good fly (the population's performance is better when ''f''&lt;sub&gt;''kill''&lt;/sub&gt; is included): we should not kill it''
 24.       '''else''' go to Step 28.   ''// ''f''&lt;sub&gt;''kill''&lt;/sub&gt; is a bad fly (the population's performance is worse when ''f''&lt;sub&gt;''kill''&lt;/sub&gt; is included): we can get rid of it''
 25.    
 26.   Restore the fly's contribution, then go to Step 12.
 27.    
 28.   Select a genetic operator
 29.    
 30.   '''If''' the genetic operator is mutation,
 31.       '''then''' go to Step 34.
 32.       '''else''' go to Step 50.
 33.    
 34.   ''f''&lt;sub&gt;''reproduce''&lt;/sub&gt; &amp;larr; Select a random fly of ''F''
 35.    
 14.   Remove ''f''&lt;sub&gt;''reproduce''&lt;/sub&gt;'s contribution from ''p''&lt;sub&gt;''estimated''&lt;/sub&gt;
 37.    
 38.   ''// Compute the population's performance without ''f''&lt;sub&gt;''reproduce''&lt;/sub&gt;''
 39.   ''G''&lt;sub&gt;''fitness''&lt;/sub&gt;(''F''-&lt;nowiki/&gt;{''f''&lt;sub&gt;''reproduce''&lt;/sub&gt;}) &amp;larr; ''Error''&lt;sub&gt;metrics&lt;/sub&gt;(''p''&lt;sub&gt;''reference''&lt;/sub&gt;, ''p''&lt;sub&gt;''estimated''&lt;/sub&gt;)
 40.    
 41.   // Compare the performances, i.e. compute the fly's local fitness
 42.   ''L''&lt;sub&gt;''fitness''&lt;/sub&gt;(''f''&lt;sub&gt;''reproduce''&lt;/sub&gt;) &amp;larr; ''G''&lt;sub&gt;''fitness''&lt;/sub&gt;(''F''-&lt;nowiki/&gt;{''f''&lt;sub&gt;''reproduce''&lt;/sub&gt;}) - ''G''&lt;sub&gt;''fitness''&lt;/sub&gt;(''F'')
 43.    
 44.   Restore the fly's contribution'
 45.    
 46.   '''If''' the local fitness is lower than or equal to 0, // Thresholded-selection of a good fly that can reproduce
 47.       '''else''' go to Step 34.   ''// ''f''&lt;sub&gt;''kill''&lt;/sub&gt; is a bad fly: we should not allow it to reproduce''
 48.       '''then''' go to Step 53.   ''// ''f''&lt;sub&gt;''kill''&lt;/sub&gt; is a good fly: we can allow it to reproduce''
 49.    
 50.   ''// New blood / Immigration''
 51.   Replace ''f''&lt;sub&gt;''kill''&lt;/sub&gt; by a new fly with a random position, go to Step 57.
 52.    
 53.   ''// Mutation''
 54.   Copy ''f''&lt;sub&gt;''reproduce''&lt;/sub&gt; into ''f''&lt;sub&gt;''kill''&lt;/sub&gt;
 55.   Slightly and randomly alter ''f''&lt;sub&gt;''kill''&lt;/sub&gt;'s position
 56.    
 57.   Add the new fly's contribution to the population
 58.    
 59.   '''If''' stop the reconstruction,
 60.       '''then''' go to Step 63.
 61.       '''else''' go to Step 10.
 62.    
 63.   ''// Extract solution''
 64.   ''V''&lt;sub&gt;''F''&lt;/sub&gt; &amp;larr; voxelisation of ''F''
 65.    
 66.   '''return''' ''V''&lt;sub&gt;''F''&lt;/sub&gt;
    
    '''END'''

== Example: Digital arts ==
&lt;!-- [[File:Initial_random_fly_population.png|200x200px|thumb|left|Image of the initial population (random fly positions).]] --&gt;

{{multiple image
 | width = 200
 | right
 | image1=evolutionary_search_Y_Lliwedd_flies.gif
 | caption1=Evolutionary search.
 | image2=Y_Lliwedd_flies.png
 | caption2=Image reconstructed after optimisation using a set of stripes as the pattern for each tile.
}}

In this example, an input image is to be approximated by a set of tiles (for example as in an ancient [[mosaic]]). A tile has an orientation (angle θ), a three colour components (R, G, B), a size (w, h) and a position (x, y, z). If there are ''N'' tiles, there are 9''N'' unknown floating point numbers to guess. In other words for 5,000 tiles, there are 45,000 numbers to find. Using a classical evolutionary algorithm where the answer of the optimisation problem is the best individual, the genome of an individual would made of 45,000 genes. This approach would be extremely costly in term of complexity and computing time. The same applies for any classical optimisation algorithm. Using the Fly Algorithm, every individual mimics a tile and can be individually evaluated using its local fitness to assess its contribution to the population's performance (the global fitness). Here an individual has 9 genes instead of 9''N'', and there are ''N'' individuals. It can be solved as a reconstruction problem as follows:

&lt;math&gt;reconstruction = \operatorname{arg\,min} \overset{x&lt;W}{\underset{y=0}{\sum}}\overset{j&lt;H}{\underset{j=0}{\sum}}|input(x,y) - P[F](x,y)|&lt;/math&gt;

where &lt;math&gt;input&lt;/math&gt; is the input image, &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt; are the pixel coordinates along the horizontal and vertical axis respectively, &lt;math&gt;W&lt;/math&gt; and &lt;math&gt;H&lt;/math&gt; are the image width and height in number of pixels respectively, &lt;math&gt;F&lt;/math&gt; is the fly population, and &lt;math&gt;P&lt;/math&gt; is a projection operator that creates an image from flies. This projection operator &lt;math&gt;P&lt;/math&gt; can take many forms. In her work, Z. Ali Aboodd &lt;ref name=Abbood2017EvoIASP /&gt; uses [[OpenGL]] to generate different effects (e.g. mosaics, or spray paint). For speeding up the evaluation of the fitness functions, [[OpenCL]] is used too.
The algorithm starts with a population &lt;math&gt;F&lt;/math&gt; that is randomly generated (see Line 3 in the algorithm above). &lt;math&gt;F&lt;/math&gt; is then assessed using the global fitness to compute &lt;math&gt;G_{fitness}(F) = \overset{x&lt;W}{\underset{y=0}{\sum}}\overset{j&lt;H}{\underset{j=0}{\sum}}|input(x,y) - P[F](x,y)|&lt;/math&gt; (see Line 10). &lt;math&gt;G_{fitness}&lt;/math&gt; is an error metrics, it has to be minimised.

== See also ==

* [[Mathematical optimization]]
* [[Metaheuristic]]
* [[Search algorithm]]
* [[Stochastic optimization]]
* [[Evolutionary computation]]
* [[Evolutionary algorithm]]
* [[Genetic algorithm]]
* [[Mutation (genetic algorithm)]]
* [[Crossover (genetic algorithm)]]
* [[Selection (genetic algorithm)]]

== References ==
&lt;!-- Inline citations added to your article will automatically display here. See https://en.wikipedia.org/wiki/WP:REFB for instructions on how to add citations. --&gt;
{{reflist}}

[[Category:Optimization algorithms and methods]]
[[Category:Genetic algorithms]]
[[Category:Evolutionary algorithms]]
[[Category:Heuristics]]
[[Category:Nature-inspired metaheuristics]]
[[Category:Evolutionary computation]]</text>
      <sha1>e7yy9985zjmkpyb2eull9w0aezzimsh</sha1>
    </revision>
  </page>
  <page>
    <title>Gabor atom</title>
    <ns>0</ns>
    <id>6765164</id>
    <revision>
      <id>869175598</id>
      <parentid>865594792</parentid>
      <timestamp>2018-11-16T22:04:54Z</timestamp>
      <contributor>
        <ip>162.18.172.11</ip>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2923">In applied [[mathematics]], '''Gabor atoms''', or '''Gabor functions''', are [[function (mathematics)|functions]] used in the analysis proposed by [[Dennis Gabor]] in 1946 in which a family of functions is built from translations and modulations of a generating function.

==Overview==
In 1946,&lt;ref&gt;{{Cite journal|last=Gabor|first=D.|title=Theory of communication. Part 1: The analysis of information|journal=Journal of the Institution of Electrical Engineers - Part III: Radio and Communication Engineering|volume=93|issue=26|pages=429–441|doi=10.1049/ji-3-2.1946.0074|year=1946}}&lt;/ref&gt; [[Dennis Gabor]] suggested the idea of using a granular system to produce [[sound]]. In his work, Gabor discussed the problems with [[Fourier analysis]]. Although he found the mathematics to be correct, it did not reflect the behaviour of sound in the world, because sounds, such as the sound of a siren, have variable frequencies over time. Another problem was the underlying supposition, as we use sine waves analysis, that the signal under concern has infinite duration even though sounds in real life have limited duration – see [[time–frequency analysis]]. Gabor applied ideas from [[quantum physics]] to sound, allowing an analogy between sound and quanta. He proposed a mathematical method to reduce Fourier analysis into cells. His research aimed at the information transmission through communication channels. Gabor saw in his atoms a possibility to transmit the same information but using less data. Instead of transmitting the signal itself it would be possible to transmit only the coefficients which represent the same signal using his atoms.

==Mathematical definition==
The Gabor function is defined by

:&lt;math&gt;g_{\ell,n}(x) = g(x - a\ell)e^{2\pi ibnx}, \quad -\infty &lt; \ell,n &lt; \infty,&lt;/math&gt;

where ''a'' and ''b'' are constants and ''g'' is a fixed function in [[Square-integrable function|''L''&lt;sup&gt;2&lt;/sup&gt;('''R''')]], such that ||''g''||&amp;nbsp;=&amp;nbsp;1.  Depending on &lt;math&gt;a&lt;/math&gt;, &lt;math&gt;b&lt;/math&gt;, and &lt;math&gt;g&lt;/math&gt;, a Gabor system may be a basis for ''L''&lt;sup&gt;2&lt;/sup&gt;('''R'''), which is defined by translations and modulations.  This is similar to a wavelet system, which may form a basis through dilating and translating a mother wavelet.

==See also==
*[[Gabor filter]]
*[[Gabor wavelet]]
*[[Fourier analysis]]
*[[Wavelet]]
*[[Morlet wavelet]]

==References==
{{Reflist}}

==Further reading==
*Hans G. Feichtinger, Thomas Strohmer: "Gabor Analysis and Algorithms", Birkhäuser, 1998;   {{ISBN|0-8176-3959-4}}
*Hans G. Feichtinger, Thomas Strohmer: "Advances in Gabor Analysis",  Birkhäuser,  2003; {{ISBN|0-8176-4239-0}}
*Karlheinz Gröchenig:  "Foundations of Time-Frequency Analysis", Birkhäuser,  2001; {{ISBN|0-8176-4022-3}}

==External links==
*[http://www.nuhag.eu NuHAG homepage] [Numerical Harmonic Analysis Group]

{{DEFAULTSORT:Gabor Atom}}
[[Category:Wavelets]]
[[Category:Fourier analysis]]</text>
      <sha1>a13wmb1a4i0fqodosmllkc8r0d6oy9s</sha1>
    </revision>
  </page>
  <page>
    <title>Gould's sequence</title>
    <ns>0</ns>
    <id>51516730</id>
    <revision>
      <id>832584942</id>
      <parentid>784549129</parentid>
      <timestamp>2018-03-26T21:20:40Z</timestamp>
      <contributor>
        <username>DePiep</username>
        <id>199625</id>
      </contributor>
      <minor/>
      <comment>save text from invisible. replace SloanesRef: use {{Cite OEIS}} (via [[WP:JWB]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8486">[[File:Pascal triangle small.png|thumb|right|300px|Pascal's triangle, rows 0 through 7. The number of odd integers in row ''i'' is the ''i''-th number in Gould's sequence.]]
[[File:Gould sawtooth.svg|thumb|The [[Self-similarity|self-similar]] sawtooth shape of Gould's sequence]]
'''Gould's sequence''' is an [[integer sequence]] named after [[Henry W. Gould]]  that counts the [[odd number]]s in each row of [[Pascal's triangle]]. It consists only of [[power of two|powers of two]], and begins:&lt;ref name="oeis"&gt;{{Cite OEIS|A001316|name=Gould's sequence}}&lt;/ref&gt;&lt;ref name="ptw"&gt;{{citation
 | last1 = Pólya | first1 = George | author1-link = George Pólya
 | last2 = Tarjan | first2 = Robert E. | author2-link = Robert Tarjan
 | last3 = Woods | first3 = Donald R.
 | isbn = 9780817649531
 | page = 21
 | publisher = Springer
 | series = Progress in Computer Science and Applied Logic
 | title = Notes on Introductory Combinatorics
 | url = https://books.google.com/books?id=K-bDBAAAQBAJ&amp;pg=PA21
 | volume = 4
 | year = 2009}}.&lt;/ref&gt;
:1, 2, 2, 4, 2, 4, 4, 8, 2, 4, 4, 8, 4, 8, 8, 16, 2, 4, 4, 8, 4, 8, 8, 16, 4, 8, 8, 16, 8, 16, 16, 32, ... {{OEIS|A001316}}
For instance, the sixth number in the sequence is 4, because there are four odd numbers in the sixth row of Pascal's triangle (the four bold numbers in the sequence '''1''', '''5''', 10, 10, '''5''', '''1''').

==Additional interpretations==
The {{mvar|n}}th value in the sequence (starting from {{math|1=''n'' = 0}}) gives the highest power of 2 that divides the [[central binomial coefficient]] &lt;math&gt;\tbinom{2n}{n}&lt;/math&gt;, and it gives the numerator of &lt;math&gt;2^n/n!&lt;/math&gt; (expressed as a fraction in lowest terms).&lt;ref name="oeis"/&gt;

[[File:PascalungeradeDreiecke.svg|thumb|upright=1.35|[[Sierpinski triangle]] generated by [[Rule 90]], or by marking the positions of odd numbers in [[Pascal's triangle]]. Gould's sequence counts the number of live cells in each row of this pattern.]]
Gould's sequence also gives the number of live cells in the {{mvar|n}}th generation of the [[Rule 90]] [[cellular automaton]] starting from a single live cell.&lt;ref name="oeis"/&gt;&lt;ref name="w"&gt;{{citation
 | last = Wolfram | first = Stephen | authorlink = Stephen Wolfram
 | doi = 10.2307/2323743
 | issue = 9
 | journal = [[American Mathematical Monthly]]
 | mr = 764797
 | pages = 566–571
 | title = Geometry of binomial coefficients
 | volume = 91
 | year = 1984}}.&lt;/ref&gt;
It has a characteristic growing [[sawtooth wave|sawtooth]] shape that can be used to recognize physical processes that behave similarly to Rule 90.&lt;ref name="cns04"&gt;{{citation|first1=Jens Christian|last1=Claussen|first2=Jan|last2=Nagler|first3=Heinz Georg|last3=Schuster|title=Sierpinski signal generates 1∕''f''&lt;sup&gt;&amp;nbsp;''α''&lt;/sup&gt; spectra|journal=Physical Review E|volume=70|year=2004|page=032101|doi=10.1103/PhysRevE.70.032101|arxiv=cond-mat/0308277|bibcode = 2004PhRvE..70c2101C }}.&lt;/ref&gt;

==Related sequences==
The [[binary logarithm]]s (exponents in the powers of two) of Gould's sequence themselves form an integer sequence,
:0, 1, 1, 2, 1, 2, 2, 3, 1, 2, 2, 3, 2, 3, 3, 4, 1, 2, 2, 3, 2, 3, 3, 4, 2, 3, 3, 4, 3, 4, 4, 5, ... {{OEIS|A000120}}
in which the {{mvar|n}}th value gives the [[Hamming weight|number of nonzero bits]] in the [[binary representation]] of the number {{mvar|n}}, sometimes written in mathematical notation as &lt;math&gt;\#_1(n)&lt;/math&gt;.&lt;ref name="oeis"/&gt;&lt;ref name="ptw"/&gt; Equivalently, the {{mvar|n}}th value in Gould's sequence is
:&lt;math&gt;2^{\#_1(n)}.&lt;/math&gt;
Taking the sequence of exponents modulo two gives the [[Thue–Morse sequence]].&lt;ref&gt;{{citation
 | last = Northshield | first = Sam
 | journal = Congressus Numerantium
 | mr = 2597704
 | pages = 35–52
 | title = Sums across Pascal's triangle mod 2
 | url = http://digitalcommons.plattsburgh.edu/cgi/viewcontent.cgi?article=1008&amp;context=mathematics_facpubs
 | volume = 200
 | year = 2010}}.&lt;/ref&gt;

The [[partial sum]]s of Gould's sequence,
:0, 1, 3, 5, 9, 11, 15, 19, 27, 29, 33, 37, 45, 49, 57, 65, 81, 83, 87, 91, 99, 103, 111, ... {{OEIS|A006046}}
count all odd numbers in the first {{mvar|n}} rows of Pascal's triangle. These numbers grow proportionally to &lt;math&gt;n^{\log_2 3}\approx n^{1.585}&lt;/math&gt;,
but with a constant of proportionality that oscillates between 0.812556... and 1, periodically as a function of {{math|log ''n''}}.&lt;ref&gt;{{citation
 | last = Harborth | first = Heiko | authorlink = Heiko Harborth
 | doi = 10.2307/2041936
 | issue = 1
 | journal = Proceedings of the American Mathematical Society
 | mr = 0429714
 | pages = 19–22
 | title = Number of odd binomial coefficients
 | volume = 62
 | year = 1976}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last = Larcher | first = G.
 | doi = 10.1007/BF00052108
 | issue = 3
 | journal = Acta Mathematica Hungarica
 | mr = 1397551
 | pages = 183–203
 | title = On the number of odd binomial coefficients
 | volume = 71
 | year = 1996}}.&lt;/ref&gt;

==Recursive construction and self-similarity==
The first {{math|2&lt;sup&gt;''i''&lt;/sup&gt;}} values in Gould's sequence may be constructed by recursively constructing the first {{math|2&lt;sup&gt;''i'' &amp;minus; 1&lt;/sup&gt;}} values, and then concatenating the doubles of the first {{math|2&lt;sup&gt;''i'' &amp;minus; 1&lt;/sup&gt;}} values. For instance, concatenating the first four values 1, 2, 2, 4 with their doubles 2, 4, 4, 8 produces the first eight values. Because of this doubling construction, the first occurrence of each power of two {{math|2&lt;sup&gt;''i''&lt;/sup&gt;}} in this sequence is at position {{math|2&lt;sup&gt;''i''&lt;/sup&gt; &amp;minus; 1}}.&lt;ref name="oeis"/&gt;

Gould's sequence, the sequence of its exponents, and the Thue–Morse sequence are all [[Self-similarity|self-similar]]: they have the property that the subsequence of values at even positions in the whole sequence equals the original sequence, a property they also share with some other sequences such as [[Calkin–Wilf tree|Stern's diatomic sequence]].&lt;ref name="w"/&gt;&lt;ref&gt;{{citation|url=https://oeis.org/selfsimilar.html|title=Some Self-Similar Integer Sequences|first=Michael|last=Gilleland|publisher=OEIS|accessdate=2016-09-10}}.&lt;/ref&gt;&lt;ref&gt;{{citation|title=Fractal Horizons|location=New York|publisher=St. Martin's Press|year=1996|editor-first=Clifford A.|editor-last=Pickover|editorlink=Clifford A. Pickover|first=Manfred|last=Schroeder|contribution=Fractals in Music|pages=207–223}}. As cited by Gilleland.&lt;/ref&gt; In Gould's sequence, the values at odd positions are double their predecessors, while in the sequence of exponents, the values at odd positions are one plus their predecessors.

==History==
The sequence is named after [[Henry W. Gould]], who studied it in the early 1960s. However, the fact that these numbers are powers of two, with the exponent of the {{mvar|n}}th number equal to the number of ones in the [[binary representation]] of {{mvar|n}}, was already known to [[James Whitbread Lee Glaisher|J. W. L. Glaisher]] in 1899.&lt;ref&gt;{{citation
 | last = Granville | first = Andrew | authorlink = Andrew Granville
 | doi = 10.2307/2324898
 | issue = 4
 | journal = [[American Mathematical Monthly]]
 | mr = 1157222
 | pages = 318–331
 | title = Zaphod Beeblebrox's brain and the fifty-ninth row of Pascal's triangle
 | volume = 99
 | year = 1992}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last = Glaisher | first = J. W. L. | author-link = James Whitbread Lee Glaisher
 | journal = [[The Quarterly Journal of Pure and Applied Mathematics]]
 | pages = 150–156
 | title = On the residue of a binomial-theorem coefficient with respect to a prime modulus
 | url = https://books.google.com/books?id=j7sKAAAAIAAJ&amp;pg=PA150
 | volume = 30
 | year = 1899}}. See in particular the final paragraph of p.&amp;nbsp;156.&lt;/ref&gt;

Proving that the numbers in Gould's sequence are powers of two was given as a problem in the 1956 [[William Lowell Putnam Mathematical Competition]].&lt;ref&gt;{{citation
 | editor1-last = Gleason | editor1-first = Andrew M. | editor1-link = Andrew M. Gleason
 | editor2-last = Greenwood | editor2-first = R. E.
 | editor3-last = Kelly | editor3-first = Leroy Milton | editor3-link = Leroy Milton Kelly
 | isbn = 9780883854624
 | page = 46
 | publisher = Mathematical Association of America
 | title = The William Lowell Putnam Mathematical Competition: Problems and Solutions: 1938–1964
 | url = https://books.google.com/books?id=1zmhHT7lIc0C&amp;pg=PA46
 | year = 1980}}.&lt;/ref&gt;

==References==
{{reflist|30em}}

[[Category:Integer sequences]]
[[Category:Factorial and binomial topics]]
[[Category:Fractals]]
[[Category:Scaling symmetries]]</text>
      <sha1>cf9ogkd60rhim058nghrto5sylf9jp7</sha1>
    </revision>
  </page>
  <page>
    <title>Hamid Naderi Yeganeh</title>
    <ns>0</ns>
    <id>45717431</id>
    <revision>
      <id>867750915</id>
      <parentid>861375365</parentid>
      <timestamp>2018-11-07T19:40:57Z</timestamp>
      <contributor>
        <username>Sdfehyj</username>
        <id>24623434</id>
      </contributor>
      <comment>photo updated</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12112">{{Infobox artist
| name = Hamid Naderi Yeganeh
| image = Hamid Naderi Yeganeh 2018.jpg
| imagesize = 
| caption = Naderi Yeganeh and a digital print of his artwork "A Bird in Flight"
| birth_name = 
| birth_date = {{Birth date|1990|7|26}}
| birth_place = [[Qom]], [[Iran]]
| nationality = Iranian
| field = [[mathematical art]]
| training = 
| movement = 
| works = [[A Bird in Flight]], [[Boat (drawing)|Boat]]
| patrons = 
| awards = 
| influenced by = 
}}
'''Hamid Naderi Yeganeh''' ({{lang-fa|حمید نادری یگانه}}; born July 26, 1990 in [[Iran]]&lt;ref&gt;{{cite web|url=http://www.qom.ac.ir/portal/Home/ShowPage.aspx?Object=NEWS&amp;ID=1079d504-908b-4fc7-8528-be5bbdf5d8bb&amp;LayoutID=3cf0a557-996e-418e-8ddd-c62b96a15f88&amp;CategoryID=d715ec66-0a1c-4acb-9b78-2bdacb866338|title=دانشگاه قم/مصاحبه با آقای...|publisher=[[University of Qom]]|language=Persian|accessdate=April 20, 2015}}&lt;/ref&gt;) is an [[Iranian peoples|Iranian]] [[mathematical artist]].&lt;ref&gt;{{cite news |title=These Delicate Drawings Are The Handiwork Of A Very Smart Computer |url= http://www.huffingtonpost.com/entry/these-delicate-drawings-are-the-handiwork-of-a-very-smart-computer_55d24edce4b07addcb43c429 |date=August 18, 2015 |first=Maddie |last=Crum   |work=[[The Huffington Post]]}}&lt;/ref&gt;&lt;ref  name="articlecosmos"&gt;{{cite news |title=The art and beauty of mathematics |url=https://cosmosmagazine.com/mathematics/art-and-beauty-mathematics  |date=February 29, 2016 |first=Belinda |last=Smith   |work=[[Cosmos (magazine)|Cosmos]]}}&lt;/ref&gt;&lt;ref&gt;{{cite news |title=The Chess Master |url= http://wordplay.blogs.nytimes.com/2015/01/05/moriconi/ |date=January 5, 2015 |first=Gary |last=Antonick   |work=[[The New York Times]] (blog)}}&lt;/ref&gt; He is known for using mathematical formulas to create drawings of real-life objects, intricate illustrations, animations, [[fractal]]s and [[tessellation]]s.&lt;ref  name="sciamblog"&gt;{{cite news |title=How to Draw with Math |url=https://blogs.scientificamerican.com/guest-blog/how-to-draw-with-math/ |date=January 9, 2017 |first=Hamid |last=Naderi Yeganeh |work=[[Scientific American]] (blog)}}&lt;/ref&gt;&lt;ref&gt;{{cite news |title=7 times mathematics became art and blew our minds |url=http://www.sciencealert.com/7-times-mathematics-became-art-and-blew-our-minds |date=February 19, 2016 |first=Fiona |last=MacDonald |work=Science Alert}}&lt;/ref&gt;&lt;ref&gt;{{cite news |title=Next da Vinci? Math genius using formulas to create fantastical works of art |url= http://www.cnn.com/2015/09/17/arts/math-art/ |date=September 18, 2015 |first=Stephy |last=Chung |work=[[CNN]]}}&lt;/ref&gt; His artwork ''9,000 Ellipses'' was used as the background cover image of ''[[The American Mathematical Monthly]] – November 2017''.&lt;ref&gt;{{cite journal | date = November 2017| title = About the Cover| jstor = 10.4169/amer.math.monthly.124.9.772 | journal = [[The American Mathematical Monthly]] | volume = 124 | pages = 772 }}&lt;/ref&gt;&lt;ref&gt;{{cite news |title=Iranian Math Whiz Da Vinci Design on Cover of U.S. Mathematics Monthly |url=http://kayhan.ir/en/news/46202/iranian-math-whiz-da-vinci-design-on-cover-of-us-mathematics-monthly |date=November 6, 2017  |work=[[Kayhan]]}}&lt;/ref&gt;&lt;ref&gt;{{cite news |title=These Beautiful Images Are Created By Drawing Ellipses |url=https://www.huffingtonpost.com/hamid-naderi-yeganeh/these-beautiful-images-ar_b_8445078.html |date=November 10, 2016 |first=Hamid |last=Naderi Yeganeh |work=[[Huffington Post]] (blog)}}&lt;/ref&gt;

==Works==
===Drawings of real-life objects===
[[File:A Bird in Flight by Hamid Naderi Yeganeh 2016.jpg|300px|thumb|left|''[[A Bird in Flight]]'', by Hamid Naderi Yeganeh is an example of drawing real-life objects with math.&lt;ref&gt;{{cite web |url= http://www.ams.org/mathimagery/displayimage.php?album=40&amp;pid=684#top_display_media|title="A Bird in Flight (2016)," by Hamid Naderi Yeganeh|publisher=[[American Mathematical Society]] |date=March 23, 2016 |accessdate=March 29, 2017}}&lt;/ref&gt;]]

Naderi Yeganeh has introduced two methods to draw real-life objects with mathematical formulas.&lt;ref name="articlecosmos"/&gt; In the first method, he creates tens of thousands of computer-generated mathematical figures to find a few interesting shapes accidentally.&lt;ref&gt;{{cite news |title=Math Is Beautiful |url= http://www.sciencefriday.com/articles/math-is-beautiful/ |date=January 19, 2016 |first=Lauren |last=Young   |work=[[Science Friday]]}}&lt;/ref&gt; For example, by using this method, he found some shapes that resemble  birds, fishes and sailing boats.&lt;ref name="articleguardian"&gt;{{cite news |title=Catch of the day: mathematician nets weird, complex fish |url= https://www.theguardian.com/science/alexs-adventures-in-numberland/2015/feb/24/catch-of-the-day-mathematician-nets-weird-complex-fish |date= February 24, 2015 |first=Alex |last=Bellos   |work=[[The Guardian]]}}&lt;/ref&gt;&lt;ref&gt;{{cite news |title=Mathematically Precise Crosshatching |url= http://blogs.scientificamerican.com/symbiartic/mathematically-precise-crosshatching/ |date=August 6, 2015 |first=Glendon |last=Mellow   |work=[[Scientific American]] (blog)}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=https://plus.maths.org/content/not-bird|title=This is not a bird (or a moustache) |publisher=[[Plus Magazine]] |date= January 8, 2015|accessdate=April 22, 2015}}&lt;/ref&gt; In the second method, he draws a real life object with a step-by-step process. In each step, he tries to find out which mathematical formulas will produce the drawing.&lt;ref  name="sciamblog"/&gt;&lt;ref name="articlecosmos"/&gt; For example, by using this method, he drew birds in flight, butterflies, human faces and plants using [[trigonometric functions]].&lt;ref&gt;{{cite news |title=Drawing Birds in Flight With Mathematics |url=http://www.huffingtonpost.com/hamid-naderi-yeganeh/mathematical-birds_b_8876904.html |date=January 12, 2016 |first=Hamid |last=Naderi Yeganeh |work=[[Huffington Post]] (blog)}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.ams.org/mathimagery/displayimage.php?album=40&amp;pid=678#top_display_media |title="Butterfly (1)," by Hamid Naderi Yeganeh|publisher=[[American Mathematical Society]] |date=March 23, 2016 |accessdate=January 23, 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite news |title=Drawing Human Faces With Mathematics |url=http://www.huffingtonpost.com/hamid-naderi-yeganeh/human-faces_b_9274054.html |date=March 8, 2016 |first=Hamid |last=Naderi Yeganeh |work=[[Huffington Post]] (blog)}}&lt;/ref&gt;&lt;ref  name="sciamblog"/&gt;

===Fractals and tessellations===
He has designed some fractals and tessellations inspired by the [[continents]].&lt;ref&gt;{{cite news |title=The Tax Collector |url= http://wordplay.blogs.nytimes.com/2015/04/13/finkel-4/ |date=April 13, 2015 |first=Gary |last=Antonick   |work=[[The New York Times]] (blog)}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://mathmunch.org/2015/04/15/continents-math-explorers-club-and-i-use-math-for/|title=Continents, Math Explorers’ Club, and "I use math for…" |publisher=mathmunch.org |date=April 2015 |accessdate=April 20, 2015}}&lt;/ref&gt; For example, in 2015, he described the fractal Africa with an Africa-like octagon and its lateral inversion.&lt;ref name="articleguardian"/&gt;&lt;ref&gt;{{cite web |url=http://education.lms.ac.uk/2016/08/hamid-naderi-yeganeh-fractal-africa/|title=Hamid Naderi Yeganeh: Fractal Africa |publisher=The De Morgan Forum – [[London Mathematical Society]] |date=September 21, 2016 |accessdate=October 10, 2016}}&lt;/ref&gt;

== Views ==
Naderi Yeganeh believes that there are an infinite number of ways of using mathematical tools in art.&lt;ref&gt;{{Cite book|url=|title=Art N Math|last=Cook|first=Katherine|last2=Finkel|first2=Dan|date=2018-03-13|publisher=Center on Contemporary Art|year=|isbn=9780999081921|location=|pages=|language=English}}&lt;/ref&gt; He says, "I don’t think computer-made art clashes with human creativity, but it can change the role of artists.”&lt;ref&gt;{{Cite news|url=https://www.csmonitor.com/Technology/2018/0507/Can-an-algorithm-be-art|title=Can an algorithm be art?|date=2018-05-07|first=Han |last=Zhao |work=[[Christian Science Monitor]]|access-date=2018-05-09|issn=0882-7729}}&lt;/ref&gt;

==Education==
Naderi Yeganeh received his bachelor's degree in mathematics from the [[University of Qom]].&lt;ref&gt;{{cite web |url=http://www.huffingtonpost.com/hamid-naderi-yeganeh |title=
Hamid Naderi Yeganeh |publisher=[[Huffington Post]] |date= |language=English| accessdate=September 8, 2016}}&lt;/ref&gt; He won a gold medal at the 38th [[Iranian Mathematical Society]]’s Mathematics Competition in May 2014 and a silver medal at the 39th [[Iranian Mathematical Society|IMS]]’s Mathematics Competition in May 2015.&lt;ref&gt;{{cite web |url= http://math-art.eu/Newsletters/EsmaNewsletter2015-02.pdf|title=ESMA Newsletter February 2015 |publisher=ESMA European Society for Mathematics and the Arts |date=February 2015 |accessdate=March 19, 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.math.wustl.edu/News2015/News2015_Feb_Yeganeh.html|title=Math Art: Hamid Naderi Yeganeh |publisher=[[Washington University in St. Louis]] |date=February 2015 |accessdate=March 19, 2015}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url= http://ims.ir/files/competitions/39-yazd/stu_result_final.pdf|title=سی ونهمین مسابقه ریاضی دانشجویی کشور  |publisher=[[Iranian Mathematical Society]] |date=May 2015 |language=Persian| accessdate=May 18, 2015}}&lt;/ref&gt;

== Exhibitions and conferences ==

* '''Art ∩ Math''', [[Center on Contemporary Art]] (Seattle, WA), March 1 - April 14, 2018.&lt;ref&gt;{{Cite web|url=https://cocaseattle.org/exhibitions/artandmath|title=Art ∩ Math|website=CoCA Seattle|language=en-US|access-date=2018-06-04}}&lt;/ref&gt;
* '''The Intersection of Art + Math''', Schack Art Center (Everett, WA), April 26 - June 2, 2018.&lt;ref&gt;{{Cite web|url=http://www.schack.org/exhibits/the-intersection-of-art-math-1/|title=The Intersection of Art + Math {{!}} Exhibit {{!}} Schack Art Center|website=www.schack.org|language=en|access-date=2018-06-04}}&lt;/ref&gt;
*'''LASER Talks in Tehran''', [[Leonardo, the International Society for the Arts, Sciences and Technology]] (Tehran, Iran), August 10, 2018.&lt;ref&gt;{{Cite news|url=https://www.leonardo.info/civicrm/event/info?id=219&amp;reset=1|title=LASER Talks in Tehran|last=|first=|date=|work=[[Leonardo/ISAST]]|access-date=2018-08-13|language=en}}&lt;/ref&gt;

==Gallery==
Below are some examples of Yeganeh's mathematical figures:
{|style="margin: 0 auto;"|align=left
| [[File:Heart by Hamid Naderi Yeganeh.jpg|170px|thumb|upright|Heart&lt;br/&gt; by Hamid Naderi Yeganeh&lt;br/&gt; 2014&lt;ref&gt;{{cite web |url= http://www.ams.org/mathimagery/displayimage.php?album=40&amp;pid=569#top_display_media|title=Hamid Naderi Yeganeh, "Heart" (November 2014) |publisher=[[American Mathematical Society]] |date=November 2014 |accessdate=March 29, 2015}}&lt;/ref&gt;]]
| [[File:A Bird in Flight by Hamid Naderi Yeganeh.jpg|300px|thumb|upright|[[A Bird in Flight]]&lt;br/&gt; by Hamid Naderi Yeganeh&lt;br/&gt;  2015&lt;ref&gt;{{cite web |url= http://www.ams.org/mathimagery/displayimage.php?album=40&amp;pid=616#top_display_media|title="A Bird in Flight (2015)," by Hamid Naderi Yeganeh|publisher=[[American Mathematical Society]] |date=September 16, 2015 |accessdate=October 9, 2015}}&lt;/ref&gt;]]
| [[File:Boat by Hamid Naderi Yeganeh.jpg|170px|thumb|upright|[[Boat (drawing)|Boat]]&lt;br/&gt; by Hamid Naderi Yeganeh&lt;br/&gt; 2015&lt;ref&gt;{{cite web |url= http://www.ams.org/mathimagery/displayimage.php?album=40&amp;pid=617#top_display_media|title="Boat," by Hamid Naderi Yeganeh|publisher=[[American Mathematical Society]] |date=September 16, 2015 |accessdate=October 9, 2015}}&lt;/ref&gt;]]
|}

==References==
{{Reflist}}

==External links==
*[http://www.ams.org/mathimagery/thumbnails.php?album=40 Mathematical Concepts Illustrated by Hamid Naderi Yeganeh] at [[American Mathematical Society]]
*[http://www.huffingtonpost.com/author/naderiyeganeh-606 Hamid Naderi Yeganeh's blog posts] in The Huffington Post

{{Mathematical art}}

{{authority control}}

{{DEFAULTSORT:Naderi Yeganeh, Hamid}}
[[Category:Mathematical artists]]
[[Category:Digital artists]]
[[Category:Living people]]
[[Category:Iranian designers]]
[[Category:Iranian contemporary artists]]
[[Category:1990 births]]
[[Category:Iranian mathematicians]]
[[Category:People from Qom]]
[[Category:Digital art]]</text>
      <sha1>4doy1opkh11uruavlrkds1kffq2crcj</sha1>
    </revision>
  </page>
  <page>
    <title>Isogonal</title>
    <ns>0</ns>
    <id>9427744</id>
    <revision>
      <id>544683180</id>
      <parentid>470908835</parentid>
      <timestamp>2013-03-16T16:58:24Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q3155506]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="451">'''Isogonal''' is a [[mathematics|mathematical]] term which means "having similar angles". It occurs in several contexts:

*'''[[Isogonal figure|Isogonal]]''' polygon, polyhedron, polytope or tiling.
*'''[[Isogonal trajectory]]''' in curve theory.
*'''[[Isogonal conjugate]]''' in triangle geometry.

An '''Isogonal''' is also the name for a line connecting points at which the [[magnetic declination]] is the same.

{{disambig}}
[[Category:Geometry]]</text>
      <sha1>n4i7bd0kpmkbvzuw5datqzsslz8icga</sha1>
    </revision>
  </page>
  <page>
    <title>Lov Grover</title>
    <ns>0</ns>
    <id>296472</id>
    <revision>
      <id>869760457</id>
      <parentid>863486970</parentid>
      <timestamp>2018-11-20T05:23:12Z</timestamp>
      <contributor>
        <username>Alaney2k</username>
        <id>209266</id>
      </contributor>
      <minor/>
      <comment>US =&gt; Americans</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3493">{{BLP sources|date=February 2018}}
'''Lov Kumar Grover''' (born 1961) is an [[India]]n-[[Americans|American]] computer scientist. He is the originator of the [[Grover's algorithm|Grover database search algorithm]] used in [[quantum computing]].&lt;ref&gt;{{cite news|title=Quantum Leap in Searching |url=https://www.wired.com/science/discoveries/news/2000/05/36574 |accessdate=19 July 2010 |newspaper=Wired |date=25 July 2000 |deadurl=yes |archiveurl=https://web.archive.org/web/20110703044742/http://www.wired.com/science/discoveries/news/2000/05/36574 |archivedate=July 3, 2011 }}&lt;/ref&gt;  Grover's 1996 algorithm won renown as the second major algorithm proposed for [[quantum computing]] (after [[Shor's algorithm|Shor's 1994 algorithm]]),&lt;ref&gt;https://www.wired.com/story/wired-guide-to-quantum-computing/&lt;/ref&gt;&lt;ref&gt;https://www.wired.com/story/the-ongoing-battle-between-quantum-and-classical-computers/ The Ongoing Battle Between Quantum And Classical Computers, by Ariel Bleicher&lt;/ref&gt; and in 2017 was finally implemented in a scalable physical quantum system.&lt;ref&gt;https://www.technologyreview.com/s/604068/quantum-computing-now-has-a-powerful-search-tool/ Quantum Computing Now Has a Powerful Search Tool&lt;/ref&gt;  [[Grover's algorithm]] has been the subject of numerous popular science articles.&lt;ref&gt;https://hackaday.com/2018/02/07/quantum-searching-in-your-browser/ Quantum Searching In Your Browser by Al Williams&lt;/ref&gt;&lt;ref&gt;https://www.infoq.com/articles/quantum-computing-applications-three&lt;/ref&gt;  Grover has been ranked as the 9th most prominent computer scientist from India.&lt;ref&gt;https://www.ranker.com/list/famous-computer-scientists-from-india/reference&lt;/ref&gt;

Grover received his bachelor's degree from the Indian Institute of Technology, Delhi in 1981{{citation needed|date=February 2018}} and his PhD in Electrical engineering from Stanford University in 1985.&lt;ref&gt;http://adsabs.harvard.edu/abs/1985PhDT........18G&lt;/ref&gt;&lt;ref&gt;https://ieeexplore.ieee.org/abstract/document/1072775&lt;/ref&gt; He then went to Bell Laboratories, where he worked for an assistant professor at Cornell University from 1987 to 1995.{{citation needed|date=February 2018}}

==Publications==
* Grover L.K.: ''[https://arxiv.org/abs/quant-ph/9605043 A fast quantum mechanical algorithm for database search]'', Proceedings, 28th Annual ACM Symposium on the Theory of Computing, (May 1996) p.&amp;nbsp;212
* Grover L.K.: ''[https://arxiv.org/abs/quant-ph/0109116 From Schrödinger's equation to quantum search algorithm]'', American Journal of Physics, 69(7): 769-777, 2001. Pedagogical review of the algorithm and its history.
* Grover L.K.: [https://cryptome.org/qc-grover.htm QUANTUM COMPUTING: How the weird logic of the subatomic world could make it possible for machines to calculate millions of times faster than they do today] ''The Sciences'', July/August 1999, pp.&amp;nbsp;24–30.
* [https://web.archive.org/web/20140201230754/http://www.bell-labs.com/user/feature/archives/lkgrover/ What's a Quantum Phone Book?], Lov Grover, Lucent Technologies

==References==
{{reflist}}

{{Authority control}}

{{DEFAULTSORT:Grover, Lov}}
[[Category:Living people]]
[[Category:Theoretical computer scientists]]
[[Category:Indian computer scientists]]
[[Category:American computer scientists]]
[[Category:Scientists at Bell Labs]]
[[Category:1961 births]]
[[Category:Indian Institute of Technology Delhi alumni]]
[[Category:American people of Indian descent]]
[[Category:Quantum information scientists]]


{{compu-scientist-stub}}</text>
      <sha1>tqi804nq2d9ulus711ir4fkau74jotb</sha1>
    </revision>
  </page>
  <page>
    <title>Maximal common divisor</title>
    <ns>0</ns>
    <id>24971513</id>
    <revision>
      <id>701943233</id>
      <parentid>607438406</parentid>
      <timestamp>2016-01-27T14:09:51Z</timestamp>
      <contributor>
        <ip>14.139.222.66</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="857">In [[abstract algebra]], particularly [[ring theory]], '''maximal common divisors''' are an abstraction of the [[number theory]] concept of [[greatest common divisor]] (GCD). This definition is slightly more general than GCDs, and may exist in rings in which GCDs do not. Halter-Koch (1998) provides the following definition.&lt;ref name="Ideal Systems"&gt;{{cite book|title=Ideal systems|publisher=Marcel Dekker|first=Franz|last=Halter-Koch|year=1998|isbn=0-8247-0186-0}}&lt;/ref&gt;

''d''&amp;nbsp;∈&amp;nbsp;''H'' is a maximal common divisor of a subset, ''B''&amp;nbsp;⊂&amp;nbsp;''H'', if the following criteria are met:
# ''d''|''b'' for all ''b''&amp;nbsp;∈&amp;nbsp;''B''
# Suppose ''c''&amp;nbsp;∈&amp;nbsp;''H'' ''d''|''c'' and ''c''|''b'' for all ''b''&amp;nbsp;∈&amp;nbsp;''B''. Then &lt;math&gt;c \simeq d&lt;/math&gt;.

==References==
{{Reflist}}

[[Category:Abstract algebra]]


{{algebra-stub}}</text>
      <sha1>c96wgq036xhnnfonzlt0vs0xw55y4wl</sha1>
    </revision>
  </page>
  <page>
    <title>Michelle L. Wachs</title>
    <ns>0</ns>
    <id>44926697</id>
    <revision>
      <id>871170335</id>
      <parentid>871170322</parentid>
      <timestamp>2018-11-29T11:40:30Z</timestamp>
      <contributor>
        <username>Dorintosh</username>
        <id>31140333</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/Twinspires1|Twinspires1]] ([[User talk:Twinspires1|talk]]) ([[WP:HG|HG]]) (3.4.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5429">[[File:Michelle L Wachs Berkeley 2013.jpg|thumb|[[Berkeley, California|Berkeley]], 2013]]
'''Michelle Lynn Wachs''' is an American mathematician who specializes in [[algebraic combinatorics]] and works as a professor of mathematics at the [[University of Miami]].&lt;ref&gt;[http://www.math.miami.edu/~wachs/ Michelle Wachs], Univ. of Miami, retrieved 2015-01-02.&lt;/ref&gt;

==Contributions==
Wachs and her advisor [[Adriano Garsia]] are the namesakes of  the [[Garsia–Wachs algorithm]] for [[optimal binary search tree]]s, which they published in 1977.{{r|knuth}}{{ran|A}}
She is also known for her research on [[Shelling (topology)|shellings]] for [[simplicial complex]]es,{{ran|F}} [[partially ordered set]]s,{{ran|C}} and [[Coxeter group]]s,{{ran|B}} and on [[random permutation statistics]]{{ran|E}} and [[Partition of a set|set partition]] statistics.{{ran|D}}

==Education==
Wachs earned her doctorate in 1977 from the [[University of California, San Diego]], under the supervision of [[Adriano Garsia]]. Her dissertation was ''Discrete Variational Techniques in Finite Mathematics''.{{r|mgp}}

==Recognition==
In 2012 Wachs became one of the inaugural fellows of the [[American Mathematical Society]].{{r|fams}} In 2013 she and her husband, mathematician Gregory Galloway (the chair of the mathematics department at Miami) were recognized as Simons Fellows.{{r|simons}} A conference in her honor was held in January 2015 at the University of Miami.{{r|conf}}

==Selected publications==
{{rma|A|{{citation
 | last1 = Garsia | first1 = Adriano M. | author1-link = Adriano Garsia
 | last2 = Wachs | first2 = Michelle L.
 | doi = 10.1137/0206045
 | issue = 4
 | journal = [[SIAM Journal on Computing]]
 | mr = 0520738
 | pages = 622–642
 | title = A new algorithm for minimum cost binary trees
 | volume = 6
 | year = 1977}}}}

{{rma|B|{{citation
 | last1 = Björner | first1 = Anders | author1-link = Anders Björner
 | last2 = Wachs | first2 = Michelle
 | doi = 10.1016/0001-8708(82)90029-9
 | issue = 1
 | journal = [[Advances in Mathematics]]
 | mr = 644668
 | pages = 87–100
 | title = Bruhat order of Coxeter groups and shellability
 | volume = 43
 | year = 1982}}}}

{{rma|C|{{citation
 | last1 = Björner | first1 = Anders | author1-link = Anders Björner
 | last2 = Wachs | first2 = Michelle
 | doi = 10.2307/1999359
 | issue = 1
 | journal = [[Transactions of the American Mathematical Society]]
 | mr = 690055
 | pages = 323–341
 | title = On lexicographically shellable posets
 | volume = 277
 | year = 1983}}}}

{{rma|D|{{citation
 | last1 = Wachs | first1 = Michelle
 | last2 = White | first2 = Dennis
 | doi = 10.1016/0097-3165(91)90020-H
 | issue = 1
 | journal = [[Journal of Combinatorial Theory]]
 | mr = 1082841
 | pages = 27–46
 | series = Series A
 | title = &lt;math&gt;p,q&lt;/math&gt;-Stirling numbers and set partition statistics
 | volume = 56
 | year = 1991}}}}

{{rma|E|{{citation
 | last1 = Björner | first1 = Anders | author1-link = Anders Björner
 | last2 = Wachs | first2 = Michelle L.
 | doi = 10.1016/0097-3165(91)90075-R
 | issue = 1
 | journal = [[Journal of Combinatorial Theory]]
 | mr = 1119703
 | pages = 85–114
 | series = Series A
 | title = Permutation statistics and linear extensions of posets
 | volume = 58
 | year = 1991}}}}

{{rma|F|{{citation
 | last1 = Björner | first1 = Anders | author1-link = Anders Björner
 | last2 = Wachs | first2 = Michelle L.
 | doi = 10.1090/S0002-9947-96-01534-6
 | issue = 4
 | journal = [[Transactions of the American Mathematical Society]]
 | mr = 1333388
 | pages = 1299–1327
 | title = Shellable nonpure complexes and posets I
 | volume = 348
 | year = 1996}}; Part II, ''Trans. AMS'' 349 (10): 3945–3975, 1997, {{doi|10.1090/S0002-9947-97-01838-2}}, {{MR|1401765}}}}

==References==
{{reflist|refs=

&lt;ref name=conf&gt;[http://www.math.miami.edu/~galloway/wachsfest.html A Conference to Celebrate The Mathematics of Michelle Wachs], retrieved 2015-01-02;[http://www.ams.org/meetings/calendar/2015_jan5-9_coralgables.html The Mathematics of Michelle Wachs], [[American Mathematical Society]], retrieved 2015-01-02.&lt;/ref&gt;

&lt;ref name=fams&gt;[http://www.ams.org/profession/fellows-list List of Fellows of the American Mathematical Society], retrieved 2015-01-02.&lt;/ref&gt;

&lt;ref name=knuth&gt;{{citation
 | last = Knuth | first = Donald E. | author-link = Donald Knuth
 | contribution = Algorithm G (Garsia–Wachs algorithm for optimum binary trees)
 | edition = 2nd
 | pages = 451–453
 | publisher = Addison–Wesley
 | title = [[The Art of Computer Programming]], Vol. 3: Sorting and Searching
 | year = 1998}}. See also History and bibliography, pp. 453–454.&lt;/ref&gt;

&lt;ref name=mgp&gt;{{mathgenealogy|id=26898}}&lt;/ref&gt;

&lt;ref name=simons&gt;{{citation|title=Math professors recognized with national fellowship|newspaper=The Miami Hurricane|date=February 17, 2013|first=Hannah|last=Meister|url=http://www.themiamihurricane.com/2013/02/17/math-professors-recognized-with-national-fellowship/}}.&lt;/ref&gt;

}}

{{Authority control}}

{{DEFAULTSORT:Wachs, Michelle Lynn}}
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]
[[Category:20th-century American mathematicians]]
[[Category:21st-century American mathematicians]]
[[Category:Women mathematicians]]
[[Category:Combinatorialists]]
[[Category:University of California, San Diego alumni]]
[[Category:University of Miami faculty]]
[[Category:Fellows of the American Mathematical Society]]</text>
      <sha1>8bx94h6i09mofl6ckggbfrucpozzvws</sha1>
    </revision>
  </page>
  <page>
    <title>Microsoft Mathematics</title>
    <ns>0</ns>
    <id>16520361</id>
    <revision>
      <id>822936937</id>
      <parentid>785237756</parentid>
      <timestamp>2018-01-29T10:48:48Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.6.2)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7524">{{Infobox Software
| name                   = Microsoft Mathematics 
| logo                   = 
| screenshot             = [[File:Microsoft Math.png|300px|Microsoft Math Screenshot]]
| caption                = Screenshot of Microsoft Mathematics 4.0 in [[Windows 7]]
| latest release version = 4.0.1108.0000
| latest release date    = {{start date and age|2011|1|11}}
| latest preview version = 
| latest preview date    = 
| developer              = [[Microsoft]]
| operating_system       = [[Windows XP]] with [[Windows XP Service Pack 3|Service Pack 3]] and later
| status                 = Active
| genre                  = [[Mathematical software]]
| license                = [[Freeware]]
| website                = {{URL|1=http://www.microsoft.com/download/details.aspx?id=15702}}
}}
[[File:MSMath.png|thumb|Microsoft Math 3: Graphing mode]]

'''Microsoft Mathematics''' (formerly ''Microsoft Math'') is a freely downloadable educational program, designed for [[Microsoft Windows]], that allows users to solve math and science problems. Developed and maintained by [[Microsoft]], it is primarily targeted at students as a learning tool.&lt;ref name=TechGenieMicrosoftMathematicsFourPointZero&gt;{{cite news|last=Sharma|first=Trishna|title=Microsoft Releases Mathematics 4.0|url=http://news.techgenie.com/softwares/microsoft-releases-mathematics-4-0/|newspaper=TechGenie|date=11 January 2011|deadurl=yes|archiveurl=https://web.archive.org/web/20150223212014/http://news.techgenie.com/softwares/microsoft-releases-mathematics-4-0/|archivedate=23 February 2015|df=}}&lt;/ref&gt;

A related [[freeware]] add-in, called ''Microsoft Mathematics Add-In for [[Microsoft Word|Word]] and [[Microsoft OneNote|OneNote]]'', is also available from Microsoft and offers comparable functionality (Word 2007 or higher is required).&lt;ref&gt;{{Cite web
  |url = http://www.microsoft.com/downloads/en/details.aspx?FamilyID=ca620c50-1a56-49d2-90bd-b2e505b3bf09&amp;displaylang=en
  |title = Download details: Microsoft Mathematics Add-In for Word and OneNote.
  |work = Microsoft Download Center
  |publisher = Microsoft Corporation
  |date = 17 October 2010
  |accessdate = 29 November 2010
}}&lt;/ref&gt;&lt;ref&gt;{{Cite web
  |url = http://blogs.technet.com/b/nzedu/archive/2010/08/16/free-microsoft-mathematics-add-in-for-word-and-onenote-released.aspx
  |title = Free: Microsoft Mathematics Add-In for Word and OneNote released
  |work = Microsoft New Zealand Education blog
  |publisher = Microsoft Corporation
  |date = 15 August 2010
  |accessdate = 29 November 2010
}}&lt;/ref&gt;

Microsoft Math has received 2008 Award of Excellence from Tech &amp; Learning Magazine.&lt;ref&gt;{{Cite web
  |url = http://www.techlearning.com/article/15112
  |title = Awards of Excellence Winners 2008
  |work = Tech &amp; Learning Magazine
  |date = 3 January 2009
  |accessdate = 29 November 2010
}}&lt;/ref&gt;

==Features==
Microsoft Math contains features that are designed to assist in solving mathematics, science, and tech-related problems, as well as to educate the user. The application features such tools as a [[graphing calculator]] and a [[unit converter]]. It also includes a [[triangle]] solver, and an [[equation]] solver that provides step-by-step solutions to each problem.

The standalone version of Microsoft Math 3.0 also has support for [[calculus]] and [[Handwriting recognition|Ink Handwriting]], allowing the user to write out problems by hand and have them recognized by Microsoft Math.

==Versions==
* '''Microsoft Math 1.0''' – Available only in [[Microsoft Student]] 2006
* '''Microsoft Math 2.0''' – Available only in Microsoft Student 2007
* '''Microsoft Math 3.0''' – Full-featured version available as a standalone purchasable product and a scaled-down version called ''Encarta Calculator'' available as part of [[Microsoft Student]] 2008. The full-featured standalone version exclusively includes calculus support, digital ink recognition features and a special display mode for [[video projector]]s. The standalone version is also the first version of Microsoft Math to require [[product activation]].&lt;ref&gt;{{Cite web |title=Activate your Microsoft games and mapping programs (Revision 9.1) |work=Microsoft Support |publisher=Microsoft Corporation |date=25 June 2010 |url=http://support.microsoft.com/default.aspx/kb/927007 |accessdate=29 November 2010}}&lt;/ref&gt;
* '''Microsoft Mathematics 4.0''' – This version was released in 32-bit and 64-bit editions as a free download in January 2011.&lt;ref&gt;{{cite web |title=Microsoft Mathematics 4.0 released for free download |work=TechNet Blogs |publisher=Microsoft |date=2011-01-12 |url= http://blogs.technet.com/b/nzedu/archive/2011/01/12/microsoft-mathematics-4-0-released-for-free-download.aspx}}&lt;/ref&gt; It features a [[Ribbon (computing)|Ribbon]] interface.

==System requirements==
System requirements for Microsoft Math are:&lt;ref&gt;{{Cite web
  |url = http://www.microsoft.com/learningspace/Math.aspx
  |title = Microsoft Math
  |work = Microsoft Learning Space
  |publisher = Microsoft Corporation
  |accessdate = 29 November 2010
}}&lt;/ref&gt;
{| class="wikitable"
|-
! 
! Minimum requirements
! Recommended requirements
|-
| [[Central processing unit|Processor]]
| [[Pentium compatible processor|Pentium]] 500&amp;nbsp;MHz or equivalent
| Pentium 1&amp;nbsp;GHz or equivalent
|-
| [[Operating system]]
| colspan="2"|&lt;center&gt;Microsoft [[Windows XP]] [[Windows XP#Service Pack 3|SP3]] or later&lt;/center&gt;
|-
| [[Random access memory|RAM]]
| 256 MB
| 512 MB
|-
| [[Hard drive]]
| colspan="2"|&lt;center&gt;65 MB free space&lt;/center&gt;
|-
| Graphics
| VGA-capable or better [[video card|video &lt;br/&gt;card]] with 800 x 600 resolution
| VGA-capable or better video &lt;br/&gt;card with 1024 x 768 resolution
|-
| Other requirements
|colspan="2"|&lt;center&gt;[[.NET Framework]] 3.5 SP1&lt;/center&gt;
|}

==Windows Phone app==
{{Main|Microsoft Mobile Services#Microsoft Math}}

In 2015 Microsoft released a similar branded mobile application for [[Windows Phone]] named '''Microsoft Math''' (alternatively called ''Nokia Mobile-Mathematics'' or ''Nokia Momaths'') specifically for South African and Tanzanian students which has no relation with the earlier Microsoft Mathematics product.&lt;ref name=NewsforMathematics&gt;{{cite news|last=Mathematics|first=News for|title=Nokia Mobile-Mathematics: A solution for mathematic failure in Tanzania?|url=http://newsformathematics.blogspot.com/2014/10/nokia-mobile-mathematics-solution-for.html|newspaper=News for Mathematics|date=5 January 2015}}&lt;/ref&gt;&lt;ref name=Schoolnet&gt;{{cite news|last=Net|first=School|title=Microsoft Math|url=http://www.schoolnet.org.za/schoolnet-at-work/projects/nokia-momath/|newspaper=Schoolnet South Africa|date=19 January 2015}}&lt;/ref&gt;&lt;ref name=WinBeta&gt;{{cite news|last=Al-Riyami|first=Fahad|title=Microsoft releases new app to make learning math exciting|url=http://www.winbeta.org/news/microsoft-releases-new-app-make-learning-math-exciting|newspaper=WinBeta|date=17 February 2015}}&lt;/ref&gt;

==See also==
*[[Grapher]]
*Math.com
*[[Mathematica]]
*[[Symbolab]]
*[[Wolfram Alpha]]

==References==
{{reflist}}

==External links==
*{{Official website|http://www.microsoft.com/math/default.mspx}}
*[http://www.microsoft.com/download/details.aspx?id=15702 Download details: Microsoft Mathematics 4.0]
*[http://www.microsoft.com/download/details.aspx?id=17786 Download details: Microsoft Mathematics Add-In for Word and OneNote]


{{Microsoft}}
{{Nokia services}}

[[Category:Educational math software]]
[[Category:Science education software]]
[[Category:Mathematical software]]
[[Category:Nokia services]]</text>
      <sha1>nf3sa0cxl4fzpj9frji5xvclvnhb553</sha1>
    </revision>
  </page>
  <page>
    <title>Moritz Epple</title>
    <ns>0</ns>
    <id>56087242</id>
    <revision>
      <id>822899294</id>
      <parentid>816584426</parentid>
      <timestamp>2018-01-29T03:51:47Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v481)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6517">[[File:Moritz Epple.jpeg|thumb|Moritz Epple in [[Mathematisches Forschungsinstitut Oberwolfach|Oberwolfach]], 2010]]
'''Moritz Epple''' (7 May 1960, [[Stuttgart]]) is a German mathematician and historian of science.

==Biography==
Epple studied mathematics, philosophy and physics at the [[University of Tübingen]], where he received in 1987 his bachelor's degree (''Diplom'') in physics and in 1991 his Ph.D. (''[[Promotion (Germany)|Promotion]])'' in mathematical physics.&lt;ref&gt;{{MathGenealogy|id=85247}}&lt;/ref&gt; He then became an assistant in the history of mathematics and natural sciences at the [[University of Mainz]], where he received in 1998 his ''[[Habilitation]]''. From 2001 to 2003 he was the head of the department of history of the natural sciences and technology at the [[University of Stuttgart]]. Since 2003 he has been a professor at the [[Goethe University of Frankfurt]] and head of the working group for the modern history of science at the historic seminary there. He was a visiting professor at several academic institutions including the [[Dibner Institute for the History of Science and Technology]] of  [[Massachusetts Institute of Technology]] (MIT) and at the [[Max-Planck-Gesellschaft]] in Berlin.

His habilitation thesis on the history of knot theory was published in 1999  under the title ''Die Entstehung der Knotentheorie – Kontexte und Konstruktionen einer modernen mathematischen Theorie'' (with 2nd edition in 2013). He also wrote the article on knot theory in the book ''History of Topology'' edited by [[Ioan James]]. He has done research on the history of mathematical analysis, for example the article ''Geschichte der Grundlagen der Analysis 1860 – 1930'' in the 1999 book ''Geschichte der Analysis'' edited by Jahnke; Epple wrote on, among other topics, [[Luitzen Egbertus Jan Brouwer]] and applied mathematical research in Germany during WW II. His historical research has also dealt with the epistemological works of [[Felix Hausdorff]] and Jewish mathematicians in German-speaking academic culture.

From 2000 to 2001 he was a Heisenberg Fellow. He was a board member of the ''Deutschen Gesellschaft für Geschichte der Medizin, Naturwissenschaft und Technik'' and a co-editor of ''NTM Zeitschrift für Geschichte der Wissenschaften, Technik und Medizin''. Since 2013 he has been a co-editor of the journal ''Science in Context''. In 2002 at the [[International Congress of Mathematicians|ICM]] in Beijing he was an Invited Speaker  with talk ''From Quaternions to cosmology – spaces of constant curvature 1873–1925''. In 2015 Epple with his Frankfurt team received the media prize of the Deutsche Mathematiker-Vereinigung for the exhibition "Transcending Tradition".&lt;ref&gt;"Transcending Tradition" cf. [https://dmv.mathematik.de/index.php/aktuell-presse/presseinformationen/26-ablage/698-medienpreis-mathematik-2015-fuer-moritz-epple-und-team Pressemitteilung der DMV] and the [http://www.gj-math.de/ website of the exhibition].&lt;/ref&gt; On 26 November 2016, Epple was elected a member of the [[Deutsche Akademie der Naturforscher Leopoldina]].

==Selected publications==
* ''Die Entstehung der Knotentheorie: Kontexte und Konstruktionen einer modernen mathematischen Theorie''. Vieweg Verlag: Wiesbaden, 1999.&lt;ref&gt;{{cite journal|author=Schirrmacher, Arne|title=Review: ''Die Entstehung der Knotentheorie'' by Moritz Epple|journal=Historia Mathematica|year=2002|volume=29|issue=1|pages=70–72|doi=10.1006/hmat.2001.2309}}&lt;/ref&gt;
* ''Genies, Ideen, Institutionen, mathematische Werkstätten. Formen der Mathematikgeschichte'', Mathematische Semesterberichte vol. 47, 2000. pp.&amp;nbsp;131–163.
* ''Rechnen, Messen, Führen. Kriegsforschung am Kaiser-Wilhelm-Institut für Strömungsforschung, 1937–1945'', in: Helmut Maier (ed.): Rüstungsforschung im Nationalsozialismus: Organisation, Mobilisierung und Entgrenzung der Technikwissenschaften. Wallstein: Göttingen, 2002, pp.&amp;nbsp;305–356.
* ''From Quaternions to Cosmology: Spaces of Constant Curvature, ca. 1873–1925'', in: Proceedings of the International Congress of Mathematicians, Beijing 2002, Vol. III: Invited Lectures, pp.&amp;nbsp;935–945.
* ''Knot invariants in Vienna and Princeton during the 1920s: Epistemic Configurations of Mathematical Research'', Science in Context 17 (2004), pp.&amp;nbsp;131–164.
* ''Orbits of asteroids, a braid and the first link invariant'', Mathematical Intelligencer, 1998, No.1, p.&amp;nbsp;48
* ''Felix Hausdorff's Considered Empiricism'', in: José Ferreiros, Jeremy J. Gray (ed.): The Architecture of Modern Mathematics: Essays in History and Philosophy. Oxford University Press: Oxford, 2006, pp.&amp;nbsp;263–289.
* Jüdische Mathematiker in der deutschsprachigen akademischen Kultur. ed. von Moritz Epple und Birgit Bergmann. Springer Verlag: Heidelberg, 2008.
** English transition: Bergmann, Epple, Ruti Ungar (eds.) ''Transcending Tradition: Jewish Mathematicians in German-Speaking Academic Culture'', Springer Verlag: Heidelberg, 2012.&lt;ref&gt;{{cite journal|author=Senechal, Marjorie|title=Review: ''Transcending Tradition: Jewish Mathematicians in German-Speaking Academic Culture'' edited by Birgit Bergman, Moritz Epple &amp; Ruti Ungar|url=http://www.ams.org/notices/201302/rnoti-p209.pdf|date=February 2003|journal=Notices of the American Mathematical Society|volume=60|issue=2|pages=209–213}}&lt;/ref&gt;
* [https://www.jstor.org/stable/10.1086/661622 ''Between Timelessness and Historiality: On the Dynamics of the Epistemic Objects of Mathematics''], Isis 102 (2011), pp.&amp;nbsp;481–493.
* as editor with [[Johannes Fried]], Raphael Gross and Janus Gudian: ''„Politisierung der Wissenschaft.“ Jüdische Wissenschaftler und ihrer Gegner an der Universität Frankfurt am Main vor und nach 1933.'' Verlag Wallstein, Göttingen 2016, {{ISBN|978-3-8353-1438-2}}.

==Sources==
*Leopoldina Neugewählte Mitglieder 2016, Leopoldina, Halle (Saale) 2017, S. 12 ([http://www.leopoldina.org/uploads/tx_leopublication/Neugew%C3%A4hlte_Mitglieder_2016.pdf PDF])

==References==
&lt;references/&gt;

==External links==
* {{DNB-Portal|136956610}}
* [http://web.uni-frankfurt.de/fb08/HS/wg/ Home page for Prof. Dr. Moritz Epple]
* [http://www.gj-math.de/ Exhibition: "Transcending Tradition: Jewish Mathematicians in German-Speaking Academic Culture"]

{{authority control}}

{{DEFAULTSORT:Epple, Moritz}}
[[Category:University of Tübingen alumni]]
[[Category:Goethe University Frankfurt faculty]]
[[Category:Historians of mathematics]]
[[Category:1960 births]]
[[Category:Living people]]</text>
      <sha1>eq2wiaou1ydbypve44u7st3q5xtzah1</sha1>
    </revision>
  </page>
  <page>
    <title>Multiplicity-one theorem</title>
    <ns>0</ns>
    <id>25976893</id>
    <revision>
      <id>871342875</id>
      <parentid>867672124</parentid>
      <timestamp>2018-11-30T12:27:05Z</timestamp>
      <contributor>
        <username>Opqsp2n</username>
        <id>29618572</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5458">In the mathematical theory of [[automorphic representation]]s, a '''multiplicity-one theorem''' is a result about the [[representation theory]] of an [[adelic algebraic group|adelic]] [[reductive algebraic group]]. The multiplicity in question is the number of times a given abstract [[group representation]] is realised in a certain space, of [[square-integrable function]]s, given in a concrete way.

A multiplicity one theorem may also refer to a result about the [[Restricted representation|restriction]] of a [[Group representation|representation]] of a [[Group (mathematics)|group]] G to a [[Subgroup|subgroup]] H. In that context, the pair (G, H) is called a strong [[Gelfand pair]]. 

==Definition==
Let ''G'' be a reductive algebraic group over a [[number field]] ''K'' and let '''A''' denote the [[adele ring|adele]]s of ''K''. Let ''Z'' denote the [[center of a group|centre]] of ''G'' and let ω be a [[continuous (mathematics)|continuous]] [[character (mathematics)|unitary character]] from ''Z''(''K'')\Z('''A''')&lt;sup&gt;&amp;times;&lt;/sup&gt; to '''C'''&lt;sup&gt;&amp;times;&lt;/sup&gt;. Let ''L''&lt;sup&gt;2&lt;/sup&gt;&lt;sub&gt;0&lt;/sub&gt;(''G''(''K'')/''G''('''A'''), ω) denote the [[cuspidal representation|space of cusp forms with central character &amp;omega;]] on ''G''('''A'''). This space decomposes into a [[direct sum of Hilbert spaces]]
:&lt;math&gt;L^2_0(G(K)\backslash G(\mathbf{A}),\omega)=\hat{\bigoplus}_{(\pi,V_\pi)}m_\pi V_\pi&lt;/math&gt;
where the sum is over [[irreducible representation|irreducible]] [[subrepresentation]]s and ''m''&lt;sub&gt;π&lt;/sub&gt; are non-negative [[integer]]s.

The group of adelic points of ''G'', ''G''('''A'''), is said to satisfy the '''multiplicity-one property''' if any [[smooth representation|smooth]] irreducible [[admissible representation]] of ''G''('''A''') occurs with multiplicity at most one in the space of [[cusp form]]s of central character ω, i.e. ''m''&lt;sub&gt;π&lt;/sub&gt; is 0 or 1 for all such π.

==Results==
The fact that the [[general linear group]], ''GL''(''n''), has the multiplicity-one property was proved by {{harvtxt|Jacquet|Langlands|1970}} for ''n''&amp;nbsp;=&amp;nbsp;2 and independently by {{harvtxt|Piatetski-Shapiro|1979}} and {{harvs|txt|authorlink=Joseph Shalika|last=Shalika|year=1974}} for ''n''&amp;nbsp;&gt;&amp;nbsp;2 using the uniqueness of the [[Whittaker model]]. Multiplicity-one also holds for [[Special linear group|''SL''(2)]], but not for ''SL''(''n'') for ''n''&amp;nbsp;&gt;&amp;nbsp;2 {{harv|Blasius|1994}}.

==Strong multiplicity one theorem==

The strong multiplicity one theorem of {{harvtxt|Piatetski-Shapiro|1979}} and {{harvtxt|Jacquet|Shalika|1981}} states that two cuspidal automorphic representations of the general linear group are isomorphic if their local components are isomorphic for all but a finite number of places.

==References==

*{{Citation | last1=Blasius | first1=Don | title=On multiplicities for  SL(n) | doi=10.1007/BF02937513 | mr=1303497 | year=1994 | journal=Israel Journal of Mathematics | issn=0021-2172 | volume=88 | issue=1 | pages=237–251}}
*{{Citation | last1=Cogdell | first1=James W. | editor1-last=Cogdell | editor1-first=James W. | editor2-last=Kim | editor2-first=Henry H. | editor3-last=Murty | editor3-first=Maruti Ram | title=Lectures on automorphic L-functions | url=https://books.google.com/books?id=jb3ZCp0-MQsC | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Fields Inst. Monogr. | isbn=978-0-8218-3516-6  | mr=2071506 | year=2004 | volume=20 | chapter=Lectures on L-functions, converse theorems, and functoriality for  GL&lt;sub&gt;n&lt;/sub&gt; | chapterurl=http://www.math.osu.edu/~cogdell/ | pages=1–96}}
*{{Citation
| last=Jacquet
| first=Hervé
| last2=Langlands
| first2=Robert
| title=Automorphic forms on GL(2)
| series=Lecture Notes in Mathematics
| publisher=Springer-Verlag
| volume=114
| year=1970
}}
*{{Citation | last1=Jacquet | first1=H. | last2=Shalika | first2=J. A. | title=On Euler products and the classification of automorphic representations. I | doi=10.2307/2374103 | mr=618323 | year=1981 | journal=[[American Journal of Mathematics]] | issn=0002-9327 | volume=103 | issue=3 | pages=499–558}} {{Citation | last1=Jacquet | first1=H. | last2=Shalika | first2=J. A. | title=On Euler products and the classification of automorphic representations. II | jstor=2374050 | mr=618323 | year=1981 | journal=[[American Journal of Mathematics]] | issn=0002-9327 | volume=103 | issue=4 | pages=777–815 | doi=10.2307/2374050}}
*{{Citation | last1=Piatetski-Shapiro | first1=I. I. | editor1-last=Borel | editor1-first=Armand | editor1-link=Armand Borel | editor2-last=Casselman. | editor2-first=W. | title=Automorphic forms, representations and L-functions (Proc. Sympos. Pure Math., Oregon State Univ., Corvallis, Ore., 1977), Part 1 | url=http://www.ams.org/publications/online-books/pspum331-index | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Proc. Sympos. Pure Math., XXXIII | isbn=978-0-8218-1435-2  | mr=546599 | year=1979 | chapter=Multiplicity one theorems | pages=209–212}}
*{{Citation | last1=Shalika | first1=J. A. | title=The multiplicity one theorem for GL&lt;sub&gt;n&lt;/sub&gt; | jstor=1971071 | mr=0348047 | year=1974 | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=100 | pages=171–193 | doi=10.2307/1971071}}

[[Category:Representation theory of groups]]
[[Category:Automorphic forms]]
[[Category:Theorems in number theory]]
[[Category:Theorems in representation theory]]</text>
      <sha1>e2anrk2yw78ql2b163wm6p1irigdn4g</sha1>
    </revision>
  </page>
  <page>
    <title>Multiplier (Fourier analysis)</title>
    <ns>0</ns>
    <id>995746</id>
    <revision>
      <id>858757516</id>
      <parentid>858536829</parentid>
      <timestamp>2018-09-09T11:44:08Z</timestamp>
      <contributor>
        <username>Latex-yow</username>
        <id>27692366</id>
      </contributor>
      <minor/>
      <comment>/* On the Euclidean Space */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="23540">{{More footnotes|date=February 2016}}
In [[Fourier analysis]], a '''multiplier operator''' is a type of [[linear operator]], or transformation of [[mathematical function|functions]]. These operators act on a function by altering its [[Fourier transform]]. Specifically they multiply the Fourier transform of a function by a specified function known as the '''multiplier''' or '''symbol'''. Occasionally, the term ''multiplier operator'' itself is shortened simply to ''multiplier''.&lt;ref&gt;{{harvnb|Duoandikoetxea|2001}}, Section 3.5.&lt;/ref&gt; In simple terms, the multiplier reshapes the frequencies involved in any function. This class of operators turns out to be broad: general theory shows that a translation-invariant operator on a [[group (mathematics)|group]] which obeys some (very mild) regularity conditions can be expressed as a multiplier operator, and conversely.&lt;ref&gt;{{harvnb|Stein|1970}}, Chapter II.&lt;/ref&gt; Many familiar operators, such as [[translation]]s and [[differentiation (mathematics)|differentiation]], are multiplier operators, although there are many more complicated examples such as the [[Hilbert transform]].
 
In [[signal processing]], a multiplier operator is called a "[[Filter (signal processing)|filter]]", and the multiplier is the filter's [[frequency response]] (or [[transfer function]]).

In the wider context, multiplier operators are special cases of spectral multiplier operators, which arise from the [[functional calculus]] of an operator (or family of commuting operators). They are also special cases of [[pseudo-differential operator]]s, and more generally [[Fourier integral operator]]s. There are natural questions in this field that are still open, such as characterizing the ''L&lt;sup&gt;p&lt;/sup&gt;'' bounded multiplier operators (see below).

Multiplier operators are unrelated to [[Lagrange multiplier]]s, except that they both involve the multiplication operation.

''For the necessary background on the [[Fourier transform]], see that page. Additional important background may be found on the pages [[operator norm]] and [[lp space|''L&lt;sup&gt;p&lt;/sup&gt;'' space]].''

==Examples==
In the setting of [[periodic function]]s defined on the [[unit circle]], the Fourier transform of a function is simply the sequence of its [[Fourier coefficient]]s. To see that differentiation can be realized as multiplier, consider the Fourier series for the derivative of a periodic function &lt;math&gt;f(t).&lt;/math&gt; After using [[integration by parts]] in the definition of the Fourier coefficient we have that

:&lt;math&gt;\mathcal{F}(f')(n)=\int_{-\pi}^\pi f'(t)e^{-int}\,dt=\int_{-\pi}^\pi (i n) f(t)e^{-int}\,dt = in\cdot\mathcal{F}(f)(n)&lt;/math&gt;.

So, formally, it follows that the Fourier series for the derivative is simply the Fourier series for &lt;math&gt;f&lt;/math&gt; multiplied by a factor &lt;math&gt; i n &lt;/math&gt;. This is the same as saying that differentiation is a multiplier operator with multiplier ''in''.

An example of a multiplier operator acting on functions on the real line is the [[Hilbert transform]]. It can be shown that the Hilbert transform is a multiplier operator whose multiplier is given by the m(''ξ'')&amp;nbsp;=&amp;nbsp;−''i''&amp;nbsp;sgn(''ξ''), where sgn is the [[sign function|signum function]].

Finally another important example of a multiplier is the [[indicator function|characteristic function]] of the unit cube in &lt;math&gt;\R^n&lt;/math&gt; which arises in the study of "partial sums" for the Fourier transform (see [[Convergence of Fourier series]]).

==Definition==
Multiplier operators can be defined on any group ''G'' for which the Fourier transform is also defined (in particular, on any [[locally compact abelian group]]). The general definition is as follows. If &lt;math&gt;f:G\to\C&lt;/math&gt; is a sufficiently [[regular function]], let &lt;math&gt;\hat f: \hat G \to \C&lt;/math&gt; denote its Fourier transform (where &lt;math&gt;\hat G&lt;/math&gt; is the [[Pontryagin dual]] of ''G''). Let &lt;math&gt;m: \hat G \to \C&lt;/math&gt; denote another function, which we shall call the ''multiplier''. Then the multiplier operator &lt;math&gt;T = T_m&lt;/math&gt; associated to this symbol ''m'' is defined via the formula

:&lt;math&gt; \widehat{Tf}(\xi) := m(\xi) \hat{f}(\xi).&lt;/math&gt;

In other words, the Fourier transform of ''Tf'' at a frequency ξ is given by the Fourier transform of ''f'' at that frequency, multiplied by the value of the multiplier at that frequency. This explains the terminology "multiplier".

Note that the above definition only defines Tf implicitly; in order to recover ''Tf'' explicitly one needs to invert the Fourier transform. This can be easily done if both ''f'' and ''m'' are sufficiently smooth and integrable. One of the major problems in the subject is to determine, for any specified multiplier ''m'', whether the corresponding Fourier multiplier operator continues to be well-defined when ''f'' has very low regularity, for instance if it is only assumed to lie in an ''L&lt;sup&gt;p&lt;/sup&gt;'' space. See the discussion on the "boundedness problem" below. As a bare minimum, one usually requires the multiplier ''m'' to be bounded and [[measurable]]; this is sufficient to establish boundedness on &lt;math&gt;L^2&lt;/math&gt; but is in general not strong enough to give boundedness on other spaces.

One can view the multiplier operator ''T'' as the composition of three operators, namely the Fourier transform, the operation of pointwise multiplication by ''m'', and then the inverse Fourier transform. Equivalently, ''T'' is the conjugation of the pointwise multiplication operator by the Fourier transform. Thus one can think of multiplier operators as operators which are diagonalized by the Fourier transform.

== Multiplier operators on common groups ==

We now specialize the above general definition to specific groups ''G''. First consider the unit circle &lt;math&gt;G = \R / 2\pi\Z;&lt;/math&gt; functions on ''G'' can thus be thought of as 2π-periodic functions on the real line. In this group, the Pontryagin dual is the group of integers, &lt;math&gt;\hat G = \Z.&lt;/math&gt; The Fourier transform (for sufficiently regular functions ''f'') is given by

:&lt;math&gt;\hat f(n) := \frac{1}{2\pi} \int_0^{2\pi} f(t) e^{-int} dt &lt;/math&gt;

and the inverse Fourier transform is given by

:&lt;math&gt;f(t) = \sum_{n=-\infty}^\infty \hat f(n) e^{int}.&lt;/math&gt;

A multiplier in this setting is simply a sequence &lt;math&gt;(m_n)_{n=-\infty}^\infty&lt;/math&gt; of numbers, and the operator &lt;math&gt;T = T_m&lt;/math&gt; associated to this multiplier is then given by the formula

:&lt;math&gt;(Tf)(t):=\sum_{n=-\infty}^{\infty}m_n \widehat{f}(n)e^{int},&lt;/math&gt;

at least for sufficiently well-behaved choices of the multiplier &lt;math&gt;(m_n)_{n=-\infty}^\infty&lt;/math&gt; and the function ''f''.

Now let ''G'' be a [[Euclidean space]] &lt;math&gt;G = \R^n&lt;/math&gt;. Here the dual group is also Euclidean, &lt;math&gt;\hat G = \R^n,&lt;/math&gt; and the Fourier and inverse Fourier transforms are given by the formulae

:&lt;math&gt;\hat f(\xi) := \int_{\R^n} f(x) e^{-2\pi i x \cdot \xi} dx&lt;/math&gt;
:&lt;math&gt;f(x) = \int_{\R^n} \hat f(\xi) e^{2\pi i x \cdot \xi} d\xi.&lt;/math&gt;

A multiplier in this setting is a function &lt;math&gt;m: \R^n \to \C,&lt;/math&gt; and the associated multiplier operator &lt;math&gt;T = T_m&lt;/math&gt; is defined by

:&lt;math&gt;Tf(x) := \int_{\R^n} m(\xi) \hat f(\xi) e^{2\pi i x \cdot \xi} d\xi,&lt;/math&gt;

again assuming sufficiently strong regularity and boundedness assumptions on the multiplier and function.

In the sense of [[Distribution (mathematics)|distribution]]s, there is no difference between multiplier operators and [[convolution operator]]s; every multiplier ''T'' can also be expressed in the form ''Tf'' = ''f*K'' for some distribution ''K'', known as the ''[[convolution kernel]]'' of ''T''. In this view, translation by an amount ''x''&lt;sub&gt;0&lt;/sub&gt; is convolution with a [[Dirac delta function]] δ(·&amp;nbsp;−&amp;nbsp;''x''&lt;sub&gt;0&lt;/sub&gt;), differentiation is convolution with δ'. Further examples are given in the [[#Further Examples|table below]].

==Diagrams==

[[File:fourier multiplier diagram.png|640px]]

==Further examples==

===On the Unit Circle===
The following table shows some common examples of multiplier operators on the unit circle &lt;math&gt;G = \R/2\pi \Z.&lt;/math&gt;

{| border="1" cellpadding="2"
!Name
!Multiplier &lt;math&gt;m_n&lt;/math&gt;
!Operator &lt;math&gt;Tf(t)&lt;/math&gt;
!Kernel &lt;math&gt;K(t)&lt;/math&gt;
|-
|Identity operator
|1
|''f''(''t'')
|[[Dirac delta function]] &lt;math&gt;\delta(t)&lt;/math&gt;
|-
|Multiplication by a constant ''c''
|''c''
|''cf''(''t'')
|&lt;math&gt;c\delta(t)&lt;/math&gt;
|-
|Translation by ''s''
|&lt;math&gt;e^{ins}&lt;/math&gt;
|''f''(''t''&amp;nbsp;−&amp;nbsp;''s'')
|&lt;math&gt;\delta(t-s)&lt;/math&gt;
|-
|[[derivative|Differentiation]]
|''in''
|&lt;math&gt;f'(t)&lt;/math&gt;
|&lt;math&gt;\delta'(t)&lt;/math&gt;
|-
|''k''-fold differentiation
|&lt;math&gt;(in)^k&lt;/math&gt;
|&lt;math&gt;f^{(k)}(t)&lt;/math&gt;
|&lt;math&gt;\delta^{(k)}(t)&lt;/math&gt;
|-
|Constant coefficient [[differential operator]]
|&lt;math&gt;P(in)&lt;/math&gt;
|&lt;math&gt;P\left(\frac{d}{dt}\right) f(t)&lt;/math&gt;
|&lt;math&gt;P\left(\frac{d}{dt}\right) \delta(t)&lt;/math&gt;
|-
|[[Fractional derivative]] of order &lt;math&gt;\alpha&lt;/math&gt;
|&lt;math&gt;|n|^\alpha&lt;/math&gt;
|&lt;math&gt;\left|\frac{d}{dt}\right|^\alpha f(t)&lt;/math&gt;
|&lt;math&gt;\left|\frac{d}{dt}\right|^\alpha \delta(t)&lt;/math&gt;
|-
|Mean value
|&lt;math&gt;1_{n = 0}&lt;/math&gt;
|&lt;math&gt;\frac{1}{2\pi} \int_0^{2\pi} f(t)\, dt&lt;/math&gt;
|1
|-
|Mean-free component
|&lt;math&gt;1_{n \neq 0}&lt;/math&gt;
|&lt;math&gt;f(t) - \frac{1}{2\pi} \int_0^{2\pi} f(t)\, dt&lt;/math&gt;
|&lt;math&gt;\delta(t) - 1&lt;/math&gt;
|-
|Integration (of mean-free component)
|&lt;math&gt;\frac{1}{in} 1_{n \neq 0}&lt;/math&gt;
|&lt;math&gt;\frac{1}{2\pi} \int_{0}^{2\pi} (\pi-s) f(t-s) ds&lt;/math&gt;
|[[Sawtooth function]] &lt;math&gt;\frac{1}{2}\left(1 - \left\{ \frac{t}{2\pi}\right\}\right)&lt;/math&gt;
|-
|Periodic [[Hilbert transform]] ''H''
|&lt;math&gt;1_{n\geq 0} - 1_{n&lt;0}&lt;/math&gt;
|&lt;math&gt;Hf := p.v. \frac{1}{\pi} \int_{-\pi}^{\pi} \frac{f(s)}{e^{i(t-s)}-1} ds&lt;/math&gt;
|&lt;math&gt;p.v. 2 \frac{f(s)}{e^{i(t-s)}-1} ds&lt;/math&gt;
|-
|Dirichlet summation &lt;math&gt;D_N&lt;/math&gt;
|&lt;math&gt;1_{-N \leq n \leq N}&lt;/math&gt;
|&lt;math&gt;\sum_{n=-N}^N \hat f(n) e^{int}&lt;/math&gt;
|[[Dirichlet kernel]] &lt;math&gt;\sin((N+\tfrac12)t) / \sin(t/2)&lt;/math&gt;
|-
|Fejér summation &lt;math&gt;F_N&lt;/math&gt;
|&lt;math&gt;\left(1 - \frac{|n|}{N}\right) 1_{-N \leq n \leq N}&lt;/math&gt;
|&lt;math&gt;\sum_{n=-N}^N \left(1 - \frac{|n|}{N}\right) \hat f(n) e^{int}&lt;/math&gt;
|[[Fejér kernel]] &lt;math&gt;\frac{1}{N} (\sin(Nt/2) / \sin(t/2))^2&lt;/math&gt;
|-
|General multiplier
|&lt;math&gt;m_n&lt;/math&gt;
|&lt;math&gt;\sum_{n=-\infty}^\infty m_n \hat f(n) e^{int}&lt;/math&gt;
|&lt;math&gt;T\delta(t) = \sum_{n=-\infty}^\infty m_n e^{int}&lt;/math&gt;
|-
|General [[convolution]] operator
|&lt;math&gt;\hat K(n)&lt;/math&gt;
|&lt;math&gt;f*K(t) := \frac{1}{2\pi} \int_0^{2\pi} f(s) K(t-s) ds&lt;/math&gt;
|&lt;math&gt;K(t)&lt;/math&gt;
|}

===On the Euclidean Space===
The following table shows some common examples of multiplier operators on Euclidean space &lt;math&gt;G = \R^n&lt;/math&gt;.

{| border="1" cellpadding="2"
!Name
!Multiplier &lt;math&gt;m(\xi)&lt;/math&gt;
!Operator &lt;math&gt;Tf(x)&lt;/math&gt;
!Kernel &lt;math&gt;K(x)&lt;/math&gt;
|-
|Identity operator
|1
|''f''(''x'')
|&lt;math&gt;\delta(x)&lt;/math&gt;
|-
|Multiplication by a constant ''c''
|''c''
|''cf''(''x'')
|&lt;math&gt;c\delta(x)&lt;/math&gt;
|-
|Translation by ''y''
|&lt;math&gt;e^{2\pi iy \cdot \xi}&lt;/math&gt;
|&lt;math&gt;f(x-y)&lt;/math&gt;
|&lt;math&gt;\delta(x-y)&lt;/math&gt;
|-
|Derivative &lt;math&gt;\frac{d}{dx}&lt;/math&gt; (one dimension only)
|&lt;math&gt;2\pi i \xi&lt;/math&gt;
|&lt;math&gt;\frac{d f}{d x}(x)&lt;/math&gt;
|&lt;math&gt;\delta'(x)&lt;/math&gt;
|-
|Partial derivative &lt;math&gt;\frac{\partial}{\partial x_j}&lt;/math&gt;
|&lt;math&gt;2\pi i \xi_j&lt;/math&gt;
|&lt;math&gt;\frac{\partial f}{\partial x_j}(x)&lt;/math&gt;
|&lt;math&gt;\frac{\partial \delta}{\partial x_j}(x)&lt;/math&gt;
|-
|[[Laplacian]] &lt;math&gt;\Delta&lt;/math&gt;
|&lt;math&gt;-4\pi^2 |\xi|^2&lt;/math&gt;
|&lt;math&gt;\Delta f(x)&lt;/math&gt;
|&lt;math&gt;\Delta \delta(x)&lt;/math&gt;
|-
|Constant coefficient differential operator &lt;math&gt;P(\nabla)&lt;/math&gt;
|&lt;math&gt;P(i\xi)&lt;/math&gt;
|&lt;math&gt;P(\nabla) f(x)&lt;/math&gt;
|&lt;math&gt;P(\nabla) \delta(x)&lt;/math&gt;
|-
|Fractional derivative of order &lt;math&gt;\alpha&lt;/math&gt;
|&lt;math&gt;(2\pi |\xi|)^\alpha&lt;/math&gt;
|&lt;math&gt;(-\Delta)^{\frac{\alpha}{2}} f(x)&lt;/math&gt;
|&lt;math&gt;(-\Delta)^{\frac{\alpha}{2}} \delta(x)&lt;/math&gt;
|-
|[[Riesz potential]] of order &lt;math&gt;\alpha&lt;/math&gt;
|&lt;math&gt;(2\pi |\xi|)^{-\alpha}&lt;/math&gt;
|&lt;math&gt;(-\Delta)^{-\frac{\alpha}{2}} f(x)&lt;/math&gt;
|&lt;math&gt;(-\Delta)^{-\frac{\alpha}{2}} \delta(x) = c_{n,\alpha} |x|^{\alpha-n}&lt;/math&gt;
|-
|[[Bessel potential]] of order &lt;math&gt;\alpha&lt;/math&gt;
|&lt;math&gt;\left (1 + 4\pi^2 |\xi|^2 \right )^{-\frac{\alpha}{2}}&lt;/math&gt;
|&lt;math&gt;(1-\Delta)^{-\frac{\alpha}{2}} f(x)&lt;/math&gt;
|&lt;math&gt;\frac{1}{(4\pi)^{\frac{\alpha}{2}}\Gamma(\frac{\alpha}{2})}\int_0^\infty e^{-\frac{\pi|x|^2}{s}}e^{-\frac{s}{4\pi}}s^{\frac{-n+\alpha}{2}} \frac{ds}{s}&lt;/math&gt;
|-
|Heat flow operator &lt;math&gt;\exp(t\Delta)&lt;/math&gt;
|&lt;math&gt;\exp \left (-4\pi^2 t |\xi|^2 \right )&lt;/math&gt;
|&lt;math&gt;\exp(t\Delta) f(x) = \frac{1}{(4\pi t)^{\frac{n}{2}}} \int_{\R^n} e^{-\frac{|x-y|^2}{4t}} f(y) dy&lt;/math&gt;
|[[Heat kernel]] &lt;math&gt;\frac{1}{(4\pi t)^{\frac{n}{2}}} e^{-\frac{|x|^2}{4t}}&lt;/math&gt;
|-
|[[Schrödinger equation]] evolution operator &lt;math&gt;\exp(it\Delta)&lt;/math&gt;
|&lt;math&gt;\exp \left (-i4\pi^2 t |\xi|^2 \right )&lt;/math&gt;
|&lt;math&gt;\exp(it\Delta) f(x) = \frac{1}{(4\pi it)^{\frac{n}{2}}} \int_{\R^n} e^{i\frac{|x-y|^2}{4t}} f(y) dy&lt;/math&gt;
|Schrödinger kernel &lt;math&gt;\frac{1}{(4\pi it)^{\frac{n}{2}}} e^{i\frac{|x|^2}{4t}}&lt;/math&gt;
|-
|[[Hilbert transform]] ''H'' (one dimension only)
|&lt;math&gt;-i\sgn(\xi)&lt;/math&gt;
|&lt;math&gt;Hf := p.v. \frac{1}{\pi} \int_{-\infty}^\infty \frac{f(y)}{x-y} dy&lt;/math&gt;
|&lt;math&gt;p.v. \frac{1}{\pi s}&lt;/math&gt;
|-
|[[Riesz transform]]s ''R&lt;sub&gt;j&lt;/sub&gt;''
|&lt;math&gt;-i\frac{\xi_j}{|\xi|}&lt;/math&gt;
|&lt;math&gt;R_jf := p.v. c_n \int_{\R^n} \frac{f(y)(x_j-y_j)}{|x-y|^n} dy&lt;/math&gt;
|&lt;math&gt;p.v. \frac{c_nx_j}{|x|^n},\quad c_n=\frac{\Gamma((n+1)/2)}{\pi^{(n+1)/2}}&lt;/math&gt;
|-
|Partial Fourier integral &lt;math&gt;S^0_R&lt;/math&gt; (one dimension only)
|&lt;math&gt;1_{-R \leq \xi \leq R}&lt;/math&gt;
|&lt;math&gt;\int_{-R}^R \hat f(\xi) e^{2\pi ix\xi} dx&lt;/math&gt;
|&lt;math&gt;\frac{\sin(2\pi R x)}{\pi x}&lt;/math&gt;
|-
|Disk multiplier &lt;math&gt;S^0_R&lt;/math&gt; 
|&lt;math&gt;1_{|\xi| \leq R}&lt;/math&gt;
|&lt;math&gt;\int_{|\xi| \leq R} \hat f(\xi) e^{2\pi ix\xi} dx&lt;/math&gt;
|&lt;math&gt;|x|^{-\frac{n}{2}} J_{\frac{n}{2}}(2\pi |x|)&lt;/math&gt; (''J'' is a [[Bessel function]]) 
|-
|[[Bochner–Riesz operator]]s &lt;math&gt;S^\delta_R&lt;/math&gt;
|&lt;math&gt; \left (1 - \frac{|\xi|^2}{R^2} \right )_+^\delta&lt;/math&gt;
|&lt;math&gt;\int_{|\xi| \leq R} \left(1 - \frac{|\xi|^2}{R^2}\right)^\delta \hat f(\xi)e^{2\pi i x\cdot\xi}\ d\xi&lt;/math&gt;
|&lt;math&gt;\int_{|\xi| \leq R} \left(1 - \frac{|\xi|^2}{R^2}\right)^\delta e^{2\pi i x\cdot\xi}\,d\xi&lt;/math&gt;
|-
|General multiplier
|&lt;math&gt;m(\xi)&lt;/math&gt;
|&lt;math&gt;\int_{R^n} m(\xi) \hat f(\xi) e^{2\pi i x \cdot \xi} d\xi&lt;/math&gt;
|&lt;math&gt;\int_{R^n} m(\xi) e^{2\pi i x \cdot \xi}\ d\xi&lt;/math&gt;
|-
|General convolution operator
|&lt;math&gt;\hat K(\xi)&lt;/math&gt;
|&lt;math&gt;f*K(x) := \int_{\R^n} f(y) K(x-y)\, dy&lt;/math&gt;
|&lt;math&gt;K(x)&lt;/math&gt;
|}

===General considerations===
The map &lt;math&gt;m \mapsto T_m&lt;/math&gt; is a [[homomorphism]] of [[C*-algebra]]s. This follows because the sum of two multiplier operators &lt;math&gt;T_m&lt;/math&gt; and &lt;math&gt;T_{m'}&lt;/math&gt; is a multiplier operators with multiplier &lt;math&gt;m+m'&lt;/math&gt;, the composition of these two multiplier operators is a multiplier operator with multiplier &lt;math&gt;mm',&lt;/math&gt; and the [[adjoint]] of a multiplier operator &lt;math&gt;T_m&lt;/math&gt; is another multiplier operator with multiplier &lt;math&gt;\overline{m}&lt;/math&gt;.

In particular, we see that any two multiplier operators [[commutative operation|commute]] with each other. It is known that multiplier operators are translation-invariant. Conversely, one can show that any translation-invariant linear operator which is bounded on ''L''&lt;sup&gt;2&lt;/sup&gt;(''G'') is a multiplier operator.

==The ''L&lt;sup&gt;p&lt;/sup&gt;'' boundedness problem==
The ''L&lt;sup&gt;p&lt;/sup&gt;'' boundedness problem (for any particular ''p'') for a given group ''G'' is, stated simply, to identify the multipliers ''m'' such that the corresponding multiplier operator is bounded from ''L&lt;sup&gt;p&lt;/sup&gt;''(''G'') to ''L&lt;sup&gt;p&lt;/sup&gt;''(''G''). Such multipliers are usually simply referred to as "''L&lt;sup&gt;p&lt;/sup&gt;'' multipliers". Note that as multiplier operators are always linear, such operators are bounded if and only if they are [[continuous linear operator|continuous]]. This problem is considered to be extremely difficult in general, but many special cases can be treated. The problem depends greatly on ''p'', although there is a [[Dual space|duality relationship]]: if &lt;math&gt;1/p + 1/q = 1&lt;/math&gt; and 1 ≤ ''p'', ''q'' ≤ ∞, then a multiplier operator is bounded on ''L&lt;sup&gt;p&lt;/sup&gt;'' if and only if it is bounded on ''L&lt;sup&gt;q&lt;/sup&gt;''.

The [[Riesz-Thorin theorem]] shows that if a multiplier operator is bounded on two different ''L&lt;sup&gt;p&lt;/sup&gt;'' spaces, then it is also bounded on all intermediate spaces. Hence we get that the space of multipliers is smallest for ''L''&lt;sup&gt;1&lt;/sup&gt; and ''L''&lt;sup&gt;∞&lt;/sup&gt; and grows as one approaches ''L''&lt;sup&gt;2&lt;/sup&gt;, which has the largest multiplier space.

===Boundedness on ''L''&lt;sup&gt;2&lt;/sup&gt;===
This is the easiest case. [[Parseval's theorem]] allows to solve this problem completely and obtain that a function ''m'' is an ''L''&lt;sup&gt;2&lt;/sup&gt;(''G'') multiplier if and only if it is bounded and measurable.

===Boundedness on ''L''&lt;sup&gt;1&lt;/sup&gt; or ''L''&lt;sup&gt;&amp;infin;&lt;/sup&gt;===
This case is more complicated than the [[Hilbert space|Hilbertian]] (''L''&lt;sup&gt;2&lt;/sup&gt;) case, but is fully resolved. The following is true:

'''Theorem''': ''In the [[euclidean space]] &lt;math&gt;\R^n&lt;/math&gt; a function &lt;math&gt;m(\xi)&lt;/math&gt; is an'' ''L''&lt;sup&gt;1&lt;/sup&gt; ''multiplier (equivalently an ''L''&lt;sup&gt;∞&lt;/sup&gt; multiplier) if and only if there exists a finite [[Borel measure]] μ such that'' ''m'' ''is the Fourier transform of μ.''

(The "if" part is a simple calculation. The "only if" part here is more complicated.)

===Boundedness on ''L''&lt;sup&gt;''p''&lt;/sup&gt; for 1 &lt; ''p'' &lt; &amp;infin;===

In this general case, necessary and sufficient conditions for boundedness have not been established, even for Euclidean space or the unit circle. However, several necessary conditions and several sufficient conditions are known. For instance it is known that in order for a multiplier operator to be bounded on even a single ''L&lt;sup&gt;p&lt;/sup&gt;'' space, the multiplier must be bounded and measurable (this follows from the characterisation of ''L''&lt;sup&gt;2&lt;/sup&gt; multipliers above and the inclusion property). However, this is not sufficient except when ''p'' = 2.

Results that give sufficient conditions for boundedness are known as '''multiplier theorems'''. Two such results are given below.

==== Marcinkiewicz multiplier theorem ====
Let &lt;math&gt;m: \R \to \R&lt;/math&gt; be a bounded function that is [[continuously differentiable]] on every set of the form &lt;math&gt;(-2^{j+1}, -2^j) \cup (2^j, 2^{j+1})&lt;/math&gt;{{clarify|reason=And are jump discontinuities at 2^(-j) allowed?|date=February 2017}} for &lt;math&gt;j \in \Z&lt;/math&gt; and has derivative such that

:&lt;math&gt;\sup_{j \in \Z} \left( \int_{-2^{j+1}}^{-2^j} |m'(\xi)| \, d\xi + \int_{2^j}^{2^{j+1}} |m'(\xi)| \, d\xi \right) &lt; \infty.&lt;/math&gt;

Then ''m'' is an ''L&lt;sup&gt;p&lt;/sup&gt;'' multiplier for all 1 &lt; ''p'' &lt; ∞.

==== Mikhlin multiplier theorem ====
Let ''m'' be a bounded function on &lt;math&gt;\R^n&lt;/math&gt; which is smooth except possibly at the origin, and such that the function &lt;math&gt;|x|^k |\nabla^k m|&lt;/math&gt; is bounded for all integers &lt;math&gt;0 \leq k \leq n/2+1&lt;/math&gt;: then ''m'' is an ''L&lt;sup&gt;p&lt;/sup&gt;'' multiplier for all 1 &lt; ''p'' &lt; ∞.

This is a special case of the Hörmander-Mikhlin multiplier theorem.

The proofs of these two theorems are fairly tricky, involving techniques from [[Calderón–Zygmund lemma|Calderón–Zygmund theory]] and the [[Marcinkiewicz theorem|Marcinkiewicz interpolation theorem]]: for the original proof, see {{Harvtxt|Mikhlin|1956}} or {{Harvtxt|Mikhlin|1965|pp=225&amp;ndash;240}}.

===Examples===
Translations are bounded operators on any ''L&lt;sup&gt;p&lt;/sup&gt;''. Differentiation is not bounded on any ''L&lt;sup&gt;p&lt;/sup&gt;''. The [[Hilbert transform]] is bounded only for ''p'' strictly between 1 and ∞. The fact that it is unbounded on ''L''&lt;sup&gt;∞&lt;/sup&gt; is easy, since it is well known that the Hilbert transform of a step function is unbounded. Duality gives the same for ''p'' = 1. However, both the Marcinkiewicz and Mikhlin multiplier theorems show that the Hilbert transform is bounded in ''L&lt;sup&gt;p&lt;/sup&gt;'' for all 1 &lt; ''p'' &lt; ∞.

Another interesting case on the unit circle is when the sequence &lt;math&gt;(x_n)&lt;/math&gt; that is being proposed as a multiplier is constant for ''n'' in each of the sets &lt;math&gt;\{2^n, \ldots,2^{n+1}-1\}&lt;/math&gt; and &lt;math&gt;\{-2^{n+1}+1,\ldots,-2^n\}.&lt;/math&gt; From the Marcinkiewicz multiplier theorem (adapted to the context of the unit circle) we see that any such sequence (also assumed to be bounded, of course){{clarify|reason=Hope this is right.|date=February 2017}} is a multiplier for every 1 &lt; ''p'' &lt; ∞.

In one dimension, the disk multiplier operator &lt;math&gt;S^0_R&lt;/math&gt;(see table above) is bounded on ''L&lt;sup&gt;p&lt;/sup&gt;'' for every 1 &lt; ''p'' &lt; ∞. However, in 1972, [[Charles Fefferman]] showed the surprising result that in two and higher dimensions the disk multiplier operator &lt;math&gt;S^0_R&lt;/math&gt; is unbounded on ''L&lt;sup&gt;p&lt;/sup&gt;'' for every ''p'' ≠ 2. The corresponding problem for Bochner&amp;ndash;Riesz multipliers is only partially solved; see also [[Bochner&amp;ndash;Riesz operator]] and [[Bochner&amp;ndash;Riesz conjecture]].

== See also ==
*[[Calderón–Zygmund lemma]]
*[[Marcinkiewicz theorem]]
*[[Singular integrals]]
*[[Singular integral operators of convolution type]]

== Notes==
{{reflist|2}}

== References ==
* {{citation|last=Duoandikoetxea|first=Javier|title=Fourier Analysis|publisher=American Mathematical Society|year=2001|isbn=0-8218-2172-5}}
* {{citation|first=Loukas|last= Grafakos|year=2008|title=Classical Fourier Analysis|edition=2nd|publisher= Springer|isbn= 978-0-387-09431-1}}
* {{citation|first=Yitzhak|last= Katznelson|authorlink=Yitzhak Katznelson|year=2004|title=An Introduction to Harmonic Analysis| publisher=Cambridge University Press|isbn= 978-0-521-54359-0}}
* {{citation|title=Estimates for translation invariant operators in L&lt;sup&gt;p&lt;/sup&gt; spaces| first=Lars| last= Hörmander|authorlink=Lars Hörmander| journal =Acta Mathematica|year= 1960|volume= 104|pages=93–140|url=http://www.springerlink.com/content/6473j22054706403/?MUD=MP|doi=10.1007/bf02547187}}
* {{citation|first=Lars|last= Hörmander|authorlink=Lars Hörmander|title=The analysis of linear partial differential operators, I. Distribution theory and Fourier analysis|edition=2nd|publisher= Springer-Verlag|isbn= 3-540-52343-X|year=1990}}
* {{citation| last = Mikhlin| first = Solomon G. |authorlink=Solomon Mikhlin| title = On the multipliers of Fourier integrals | journal = [[Doklady Akademii Nauk SSSR]] | volume = 109 | pages = 701–703 | year = 1956| zbl = 0073.08402}} (in [[Russian language|Russian]]). 
* {{citation| last = Mikhlin| first = Solomon G.|authorlink=Solomon Mikhlin| title = Multidimensional singular integrals and integral equations| publisher = [[Pergamon Press]] | year = 1965 | series = International Series of Monographs in Pure and Applied Mathematics | volume = 83| zbl = 0129.07701}}. This contains a comprehensive survey of all results known at the time of publication, including a sketch of the history.
* {{citation|first=Walter|last= Rudin|authorlink=Walter Rudin|year=1962|title=Fourier Analysis on Groups|publisher= Interscience}}
* {{citation|first=Elias M.|last= Stein|authorlink=Elias Stein|year=1970|title=Singular Integrals and Differentiability Properties of Functions| publisher =Princeton University Press}}
* {{citation|first=Alberto|last= Torchinsky|year=2004|title=Real-Variable Methods in Harmonic Analysis| publisher=Dover|isbn= 0-486-43508-3}}

{{DEFAULTSORT:Multiplier (Fourier Analysis)}}
[[Category:Fourier analysis]]</text>
      <sha1>6wag80aeruzov4s0mpio0usqq3uzkxx</sha1>
    </revision>
  </page>
  <page>
    <title>Néron model</title>
    <ns>0</ns>
    <id>9190726</id>
    <revision>
      <id>841678233</id>
      <parentid>829019775</parentid>
      <timestamp>2018-05-17T09:58:15Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>/* References */Journal cites, added 1 DOI</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5571">In [[algebraic geometry]], the '''Néron model''' (or '''Néron minimal model''', or '''minimal model''')
for an [[abelian variety]] ''A&lt;sub&gt;K&lt;/sub&gt;'' defined over the field of fractions ''K''  of a Dedekind domain ''R'' is the "push-forward" of ''A&lt;sub&gt;K&lt;/sub&gt;'' from Spec(''K'') to Spec(''R''), in other words the "best possible"  group scheme ''A&lt;sub&gt;R&lt;/sub&gt;'' defined over ''R'' corresponding to ''A&lt;sub&gt;K&lt;/sub&gt;''.

They were introduced by {{harvs|txt|authorlink=André Néron|first=André |last=Néron|year1=1961|year2=1964}} for abelian varieties over the quotient field of a Dedekind domain ''R'' with perfect residue fields, and {{harvtxt|Raynaud|1966}} extended this construction to semiabelian varieties over all Dedekind domains.

==Definition==

Suppose that ''R'' is a [[Dedekind domain]] with field of fractions ''K'', and suppose that ''A&lt;sub&gt;K&lt;/sub&gt;'' is  a smooth separated scheme over ''K'' (such as an abelian variety). Then a  '''Néron model''' of ''A&lt;sub&gt;K&lt;/sub&gt;'' is defined to be a  [[smooth morphism|smooth]] [[Separated morphism|separated]] scheme ''A&lt;sub&gt;R&lt;/sub&gt;'' over ''R'' with fiber ''A&lt;sub&gt;K&lt;/sub&gt;'' that is universal in the following sense. 
:If ''X'' is a smooth separated scheme over ''R'' then any ''K''-morphism from ''X''&lt;sub&gt;''K''&lt;/sub&gt; to  ''A&lt;sub&gt;K&lt;/sub&gt;'' can be extended to a unique ''R''-morphism from ''X'' to ''A&lt;sub&gt;R&lt;/sub&gt;'' '''(Néron mapping property)'''. 
In particular, the canonical map &lt;math&gt;A_R(R)\to A_K(K)&lt;/math&gt; is an isomorphism. If a Néron model exists then it is unique up to unique isomorphism.

In terms of sheaves, any scheme ''A'' over Spec(''K'') represents a sheaf on the category of schemes smooth over Spec(''K'') with the smooth Grothendieck topology, and this has a pushforward by the injection map from Spec(''K'') to Spec(''R''), which is a sheaf over Spec(''R''). If this pushforward is representable by a scheme, then this scheme is the Néron model of ''A''.

In general the scheme ''A&lt;sub&gt;K&lt;/sub&gt;''  need not have any Néron model. 
For abelian varieties ''A&lt;sub&gt;K&lt;/sub&gt;'' Néron models exist and are unique (up to unique isomorphism) and are commutative quasi-projective  [[group scheme]]s over ''R''. The fiber of a Néron model over a [[closed point]] of Spec(''R'') is a smooth commutative [[algebraic group]], but need not be an abelian variety: for example, it may be disconnected or a torus. Néron models exist as well for certain commutative groups other than abelian varieties such as tori, but these are only locally of finite type. Néron models do not exist for the additive group.

== Properties ==
* The formation of Néron models commutes with products.
* The formation of Néron models commutes with étale base change.
* An [[Abelian scheme]] ''A''&lt;sub&gt;''R''&lt;/sub&gt; is the Néron model of its generic fibre.

==The Néron model of an elliptic curve==

The Néron model of an elliptic curve ''A''&lt;sub&gt;''K''&lt;/sub&gt; over ''K'' can be constructed as follows. First form the minimal model over ''R'' in the sense of algebraic (or arithmetic) surfaces. This is a regular proper surface over ''R'' but is not in general smooth over ''R'' or a group scheme over ''R''. Its subscheme of smooth points over ''R'' is the Néron model, which is a smooth group scheme over ''R'' but not necessarily proper over ''R''. The fibers in general may have several irreducible components, and to form the Néron model one discards all multiple components, all points where two components intersect, and all singular points of the components.

[[Tate's algorithm]] calculates the [[Elliptic surface|special fiber]] of the Néron model of an elliptic curve, or more precisely the fibers of the minimal surface containing the Néron model.

==References==
* {{Citation | last1=Artin | first1=Michael | author1-link=Michael Artin | editor1-last=Cornell | editor1-first=G. | editor2-last=Silverman | editor2-first=Joseph H. | editor2-link=Joseph H. Silverman | title=Arithmetic geometry (Storrs, Conn., 1984) | publisher=[[Springer-Verlag]] | location=Berlin, New York |mr=861977 | year=1986 | chapter=Néron models | pages=213–230}}
* {{Citation | last1=Bosch | first1=Siegfried | last2=Lütkebohmert | first2=Werner | last3=Raynaud | first3=Michel | author3-link= Michel Raynaud | title=Néron models | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=[[Ergebnisse der Mathematik und ihrer Grenzgebiete]] (3)| isbn=978-3-540-50587-7 |mr=1045822 | year=1990 | volume=21 | doi=10.1007/978-3-642-51438-8}}
*{{springer|id=N/n066290|authorlink=I.V. Dolgachev|author=I.V. Dolgachev|title=Néron model }}
*{{citation|last=Néron|first= André |title=Modèles p-minimaux des variétés abéliennes. |series=Séminaire Bourbaki|volume= 7 |year=1961|issue= 227|mr= 1611194 | zbl= 0132.41402
|url= http://www.numdam.org/item?id=SB_1961-1962__7__65_0}}
* {{Citation | last1=Néron | first1=André | author1-link= André Néron | title=Modèles minimaux des variétes abèliennes sur les corps locaux et globaux | url=http://www.numdam.org/item?id=PMIHES_1964__21__5_0 |mr=0179172 | year=1964 | journal=[[Publications Mathématiques de l'IHÉS]] | volume=21 | pages=5–128 | doi=10.1007/BF02684271}}
*{{citation|mr=0194421|last=Raynaud|first= Michel|title=Modèles de Néron|journal=Comptes Rendus de l'Académie des Sciences, Série A-B|volume= 262 |year=1966 |pages=A345–A347}} 
*W. Stein, [http://wstein.org/edu/Fall2003/252/lectures/10-20-03/10-20-03-Neron_models.pdf What are Néron models?] (2003)

{{DEFAULTSORT:Neron model}}
[[Category:Algebraic geometry]]
[[Category:Number theory]]</text>
      <sha1>s1vg6t58gok3udc3io4wovbhlzrabaj</sha1>
    </revision>
  </page>
  <page>
    <title>Pathway Systems</title>
    <ns>0</ns>
    <id>44075827</id>
    <revision>
      <id>677468647</id>
      <parentid>633829531</parentid>
      <timestamp>2015-08-23T13:48:29Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor/>
      <comment>typos, replaced: each others' → each other's using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1891">{{multiple issues|
{{notability|Companies|date=October 2014}}
{{refimprove|date=October 2014}}
}}

'''Pathway Systems International Inc.''' &lt;ref name="Pathway Systems"&gt;http://search.sunbiz.org/Inquiry/CorporationSearch/ByName&lt;/ref&gt; is an American corporation headquartered in [[Orlando]], [[Florida]], 
that designs, develops and sells relationship modeling software.

Although capable of modeling any system of inter-related objects, the software sold under the name '''Blueprints™''', is focused primarily on the [[Information Technology]] (IT) segment of the market.  Any physical or virtual part of an IT system can be visualized with relationship dependencies exposed in a graphical context.

The philosophy of Blueprints™ is to effect assemblage around a common model to transform "tribal knowledge" into shared awareness.  In doing so, '''Blueprints™''' provides both local and remote individuals the ability to share the same perspective, seeing each other's actions, resulting in collaboration in real-time.

The company was incorporated by Daniel Evenson in 2008.  The genesis of '''Pathway Systems''' was the need within the IT industry for a system of visual documentation that was fast, easy to use, and easy to maintain.  
The IT Dependency Mapping functionality of '''Blueprints™''' is delivered as a [[SaaS]] or [[virtual appliance]].

Although Dependency Modeling and Dependency Mapping are used interchangeably within the IT industry, modeling in the case of '''Blueprints™''' connotes a more dynamic approach to representing business process as effected by software and hardware systems. An [http://www.drjournal-digital.com/drjournal/2009fall?pg=99#pg51 article] describing one small part of the advantages of good dependency modeling is covered within the Fall 2009 Disaster Recovery Journal.

==References==
{{Reflist}}

[[Category:Unified Modeling Language]]</text>
      <sha1>dtq2wzl7bsq4yv8ouzwsiz9q0xuif00</sha1>
    </revision>
  </page>
  <page>
    <title>Perfect obstruction theory</title>
    <ns>0</ns>
    <id>54594963</id>
    <revision>
      <id>846713229</id>
      <parentid>791724307</parentid>
      <timestamp>2018-06-20T12:55:54Z</timestamp>
      <contributor>
        <username>Bibcode Bot</username>
        <id>14394459</id>
      </contributor>
      <minor/>
      <comment>Adding 0 [[arXiv|arxiv eprint(s)]], 1 [[bibcode|bibcode(s)]] and 0 [[digital object identifier|doi(s)]]. Did it miss something? Report bugs, errors, and suggestions at [[User talk:Bibcode Bot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3912">In algebraic geometry, given a [[Deligne–Mumford stack]] ''X'', a '''perfect obstruction theory''' for ''X'' consists of:
# a [[perfect complex|perfect]] two-term complex &lt;math&gt;E = [E^{-1} \to E^0]&lt;/math&gt; in the [[derived category]] &lt;math&gt;D(\text{Qcoh}-\mathcal{O}_X)&lt;/math&gt; of quasi-coherent étale sheaves on ''X'', and
# a morphism &lt;math&gt;\varphi: E \to \textbf{L}_X&lt;/math&gt;, where &lt;math&gt;\textbf{L}_X&lt;/math&gt; is the [[cotangent complex]] of ''X'', that induces an isomorphism on &lt;math&gt;h^0&lt;/math&gt; and an epimorphism on &lt;math&gt;h^{-1}&lt;/math&gt;.

The notion was introduced by {{harv|Behrend–Fantechi|1997}} for an application to the intersection theory on moduli stacks; in particular, to define a [[virtual fundamental class]].

== Examples ==
=== Schemes ===
Consider a regular embedding &lt;math&gt;i : Y \to W&lt;/math&gt; fitting into a cartesian square
:&lt;math&gt;
\begin{matrix}
X &amp; \xrightarrow{j} &amp; V \\
g \downarrow &amp; &amp; \downarrow f \\
Y &amp; \xrightarrow{i} &amp; W
\end{matrix}
&lt;/math&gt;
where &lt;math&gt;V,W&lt;/math&gt; are smooth. Then, the complex
:&lt;math&gt;E^\bullet = [g^*N_{Y/W}^{\vee} \to j^*\Omega_V]&lt;/math&gt; (in degrees &lt;math&gt;-1, 0&lt;/math&gt;)
forms a perfect obstruction theory for ''X''.&lt;ref&gt;{{harvnb|Behrend–Fantechi|1997|loc=§ 6}}&lt;/ref&gt; The map comes from the composition
:&lt;math&gt;g^*N_{V/W}^\vee \to g^*i^*\Omega_W =j^*f^*\Omega_W \to j^*\Omega_V&lt;/math&gt;
This is a perfect obstruction theory because the complex comes equipped with a map to &lt;math&gt;\mathbf{L}_X^\bullet&lt;/math&gt; coming from the maps &lt;math&gt;g^*\mathbf{L}_Y^\bullet \to \mathbf{L}_X^\bullet&lt;/math&gt; and &lt;math&gt;j^*\mathbf{L}_V^\bullet \to \mathbf{L}_X^\bullet&lt;/math&gt;. Note that the associated virtual fundamental class is &lt;math&gt;[X,E^\bullet] = i^![V]&lt;/math&gt;
==== Example 1 ====
Consider a smooth projective variety &lt;math&gt;Y \subset \mathbb{P}^n&lt;/math&gt;. If we set &lt;math&gt;V = W&lt;/math&gt;, then the perfect obstruction theory in &lt;math&gt;D^{[-1,0]}(X)&lt;/math&gt; is
:&lt;math&gt;[N_{X/\mathbb{P}^n}^\vee \to \Omega_{\mathbb{P}^n}]&lt;/math&gt;
and the associated virtual fundamental class is
:&lt;math&gt;[X,E^\bullet] = i^![\mathbb{P}^n]&lt;/math&gt;
In particular, if &lt;math&gt;Y&lt;/math&gt; is a smooth local complete intersection then the perfect obstruction theory is the cotangent complex (which is the same as the truncated cotangent complex).

=== Deligne–Mumford stacks ===
The previous construction works too with Deligne–Mumford stacks.

== Symmetric obstruction theory ==
By definition, a '''symmetric obstruction theory''' is a perfect obstruction theory together with nondegenerate symmetric bilinear form.

Example: Let ''f'' be a regular function on a smooth variety (or stack). Then the set of critical points of ''f'' carries a symmetric obstruction theory in a canonical way.

Example: Let ''M'' be a complex symplectic manifold. Then the (scheme-theoretic) [[Lagrangian intersection|intersection]] of [[Lagrangian submanifold]]s of ''M'' carries a canonical symmetric obstruction theory.

== Notes ==
{{reflist}}

== References ==
* {{Cite arXiv|last=Behrend|first=K.|eprint=math/0507523v2|title=Donaldson–Thomas invariants via microlocal geometry|year=2005}}
* {{Cite journal|last=Behrend|first=K.|last2=Fantechi|first2=B.|date=1997-03-01|title=The intrinsic normal cone|url=https://link.springer.com/article/10.1007/s002220050136|journal=Inventiones mathematicae|language=en|volume=128|issue=1|pages=45–88|doi=10.1007/s002220050136|issn=0020-9910|bibcode=1997InMat.128...45B}}
* {{Cite web|url=https://mathoverflow.net/questions/206804/understanding-the-obstruction-cone-of-a-symmetric-obstruction-theory/211932#211932|title=Understanding the obstruction cone of a symmetric obstruction theory|last=Oesinghaus|first=Jakob |website=[[MathOverflow]]|date=2015-07-20|access-date=2017-07-19}}

== See also ==
* [[Behrend function]]
* [[Gromov–Witten invariant]]

[[Category:Differential topology]]
[[Category:Symplectic geometry]]
[[Category:Hamiltonian mechanics]]
[[Category:Smooth manifolds]]</text>
      <sha1>nnvxenp8bh5j3qq7jj3k77ileg9j6fi</sha1>
    </revision>
  </page>
  <page>
    <title>Plumbing drawing</title>
    <ns>0</ns>
    <id>23032386</id>
    <revision>
      <id>810119398</id>
      <parentid>810117395</parentid>
      <timestamp>2017-11-13T12:54:47Z</timestamp>
      <contributor>
        <ip>2.50.191.32</ip>
      </contributor>
      <comment>Pipes</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1617">A '''plumbing drawing''', a type of [[technical drawing]], shows the system of piping for fresh water going into the building and waste going out, both solid and liquid.
It also includes fuel gas drawings. Mainly plumbing drawing consist of Water supply system drawings, Drainage system drawings, Irrigation system drawings, Storm water system drawings. In water supply system drawing there will be hot water piping and cold water piping and hot water return piping also. 
In drainage system drawings there will be waste piping , Soil piping and vent piping. 
The set of drawing of each system like water supply , drainage etc is consist of Plans, Riser diagram, Installation details, Legends, Notes. Every pipes should me marked with pipe sizes. If the drawing is detailed , fixture units also should be marked along with the pipe. If it is shop drawing, sections also should be shown where there pipes are crossing. In shop drawings pipe sizes should be marked with the text and size should be shown with double line. Each pipes with different purposes will be displayed with different colors for ease of understanding. Drainage pipes should be shown with slope. For water supply , pump capacity and number of pumps will be attached as drawing file. For drainage,  manhole schedule which consist of each manhole name, Invert level, Cover level , Depth are also attached as drawing file.
 
== See also ==
* [[Architectural drawing]]
* [[Electrical drawing]]
* [[Engineering drawing]]
* [[Mechanical systems drawing]]
* [[Structural drawing]]
* [[Working drawing]]

{{engineering-stub}}
[[Category:Technical drawing]]</text>
      <sha1>nrrljgcusq8ua300osyivgx6x8rhihh</sha1>
    </revision>
  </page>
  <page>
    <title>Projection method (fluid dynamics)</title>
    <ns>0</ns>
    <id>20542241</id>
    <revision>
      <id>850861023</id>
      <parentid>838658488</parentid>
      <timestamp>2018-07-18T12:44:45Z</timestamp>
      <contributor>
        <ip>185.52.247.41</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7714">The '''projection method''' is an effective means of [[Numerical analysis|numerically]] solving time-dependent [[incompressible flow|incompressible fluid-flow]] problems. It was originally introduced by [[Alexandre Chorin]] in 1967&lt;ref&gt;
{{Citation 
 | surname1 = Chorin
 | given1 = A. J.
 | title = The numerical solution of the Navier-Stokes equations for an incompressible fluid
 | journal = Bull. Am. Math. Soc. 
 | volume = 73
 | year = 1967
 | pages = 928&amp;ndash;931
 | url =http://math.berkeley.edu/~chorin/chorin67.pdf
}}&lt;/ref&gt;&lt;ref&gt;
{{Citation 
 | surname1 = Chorin
 | given1 = A. J.
 | title = Numerical Solution of the Navier-Stokes Equations
 | journal = Math. Comp.
 | volume = 22
 | year = 1968
 | pages = 745&amp;ndash;762
 | url =
 | doi=10.1090/s0025-5718-1968-0242392-2
}}&lt;/ref&gt;
as an efficient means of solving the incompressible [[Navier-Stokes equation]]s. The key advantage of the projection method is that the computations of the velocity and the pressure fields are decoupled.

==The algorithm==
The algorithm of the projection method is based on the [[Helmholtz decomposition]] (sometimes called Helmholtz-Hodge decomposition) of any vector field into a [[solenoidal field|solenoidal]] part and an [[irrotational field|irrotational]] part. Typically, the algorithm consists of two stages. In the first stage, an intermediate velocity that does not satisfy the incompressibility constraint is computed at each time step. In the second, the pressure is used to project the intermediate velocity onto a space of divergence-free velocity field to get the next update of velocity and pressure.

==Helmholtz&amp;ndash;Hodge decomposition==
The theoretical background of projection type method is the decomposition theorem of [[Olga Aleksandrovna Ladyzhenskaya|Ladyzhenskaya]] sometimes referred to as Helmholtz&amp;ndash;Hodge Decomposition or simply as Hodge decomposition. It states that the vector field &lt;math&gt;\mathbf{u}&lt;/math&gt; defined on a [[simply connected space|simply connected]] domain can be uniquely decomposed into a divergence-free ([[Solenoidal vector field|solenoidal]]) part &lt;math&gt;\mathbf{u}_{\text{sol}}&lt;/math&gt; and an [[Conservative vector field#Irrotational vector fields|irrotational]] part &lt;math&gt;\mathbf{u}_{\text{irrot}}&lt;/math&gt;.
.&lt;ref&gt;{{cite book | title = A Mathematical Introduction to Fluid Mechanics | author1 = Chorin, A. J. | author2 = J. E. Marsden | edition = 3rd | publisher = [[Springer Science+Business Media|Springer-Verlag]] | year = 1993 | isbn = 0-387-97918-2}}&lt;/ref&gt;

Thus, 
:&lt;math&gt;
  \mathbf{u} = \mathbf{u}_{\text{sol}} + \mathbf{u}_{\text{irrot}} = \mathbf{u}_{\text{sol}} + \nabla \phi
&lt;/math&gt;

since &lt;math&gt;\nabla \times \nabla \phi = 0&lt;/math&gt; for some scalar function, &lt;math&gt;\,\phi&lt;/math&gt;. Taking the
divergence of equation yields

:&lt;math&gt;
  \nabla\cdot \mathbf{u} = \nabla^2 \phi \qquad ( \text{since,} \; \nabla\cdot \mathbf{u}_{\text{sol}} = 0 )
&lt;/math&gt;

This is a [[Poisson equation]] for the scalar function &lt;math&gt;\,\phi&lt;/math&gt;. If the vector field &lt;math&gt;\mathbf{u}&lt;/math&gt; is known, the above equation can be solved for the scalar function &lt;math&gt;\,\phi&lt;/math&gt; and the divergence-free part of &lt;math&gt;\mathbf{u}&lt;/math&gt; can be extracted using the relation

:&lt;math&gt;
  \mathbf{u}_{\text{sol}} = \mathbf{u} - \nabla \phi
&lt;/math&gt;

This is the essence of solenoidal projection method for solving incompressible
Navier&amp;ndash;Stokes equations.

==Chorin's projection method==
The incompressible Navier-Stokes equation (differential form of momentum equation) may be written as
:&lt;math&gt;
  \frac {\partial \mathbf{u}} {\partial t} + (\mathbf{u}\cdot\nabla)\mathbf{u} = - \frac {1}{\rho} \nabla p + \nu \nabla^2 \mathbf{u}
&lt;/math&gt;

In [[Alexandre Chorin|Chorin]]'s original version of the projection method, one first computes an intermediate velocity, &lt;math&gt;\mathbf{u}^*&lt;/math&gt;, explicitly using the momentum equation by ignoring the pressure gradient term:

:&lt;math&gt;
  \quad (1) \qquad \frac {\mathbf{u}^* - \mathbf{u}^n} {\Delta t} = -(\mathbf{u}^n \cdot\nabla) \mathbf{u}^n + \nu \nabla^2 
  \mathbf{u}^n
&lt;/math&gt;

where &lt;math&gt;\mathbf{u}^n&lt;/math&gt; is the velocity at &lt;math&gt;\,n&lt;/math&gt;&lt;sup&gt;th&lt;/sup&gt; time step. In the second half of the algorithm, the ''projection'' step, we correct the intermediate velocity to obtain the final solution of the time step &lt;math&gt;\mathbf{u}^{n+1}&lt;/math&gt;:

:&lt;math&gt;
  \quad (2) \qquad \mathbf{u}^{n+1} = \mathbf{u}^* - \frac {\Delta t}{\rho} \, \nabla p ^{n+1}
&lt;/math&gt;

One can rewrite this equation in the form of a time step as

:&lt;math&gt;

  \frac {\mathbf{u}^{n+1} - \mathbf{u}^*} {\Delta t} = - \frac {1}{\rho} \, \nabla p ^{n+1}
&lt;/math&gt;

to make clear that the algorithm is really just an [[operator splitting]] approach in which one considers the viscous forces (in the first half step) and the pressure forces (in the second half step) separately.

Computing the right-hand side of the second half step requires knowledge of the pressure, &lt;math&gt;\,p&lt;/math&gt;, at the&lt;math&gt;\,(n+1)&lt;/math&gt; time level. This is obtained by taking the [[divergence]] and requiring that &lt;math&gt;\nabla\cdot \mathbf{u}^{n+1} = 0&lt;/math&gt;, which is the divergence (continuity) condition, thereby deriving the following Poisson equation for &lt;math&gt;\,p^{n+1}&lt;/math&gt;,
:&lt;math&gt;
  \nabla ^2 p^{n+1} = \frac {\rho} {\Delta t} \, \nabla\cdot \mathbf{u}^*
&lt;/math&gt;
It is instructive to note that the equation written as
:&lt;math&gt;
  \mathbf{u}^* = \mathbf{u}^{n+1} + \frac {\Delta t}{\rho} \, \nabla p ^{n+1}
&lt;/math&gt;
is the standard Hodge decomposition if boundary condition for &lt;math&gt;\,p&lt;/math&gt; on the domain boundary, &lt;math&gt;\partial \Omega&lt;/math&gt; are &lt;math&gt;\nabla p^{n+1}\cdot \mathbf{n} = 0&lt;/math&gt;. In practice, this condition is responsible for the errors this method shows close to the boundary of the domain since the real pressure (i.e., the pressure in the exact solution of the Navier-Stokes equations) does not satisfy such boundary conditions.

For the explicit method, the boundary condition for &lt;math&gt;\mathbf{u}^*&lt;/math&gt; in equation (1) is natural. If &lt;math&gt;\mathbf{u}\cdot \mathbf{n} = 0&lt;/math&gt; on &lt;math&gt;\partial \Omega&lt;/math&gt;, is prescribed, then the space of divergence-free vector fields will be orthogonal to the space of irrotational vector fields, and from equation (2) one has
:&lt;math&gt;
  \frac {\partial p^{n+1}} {\partial n} = 0   \qquad \text{on} \quad \partial \Omega
&lt;/math&gt;
The explicit treatment of the boundary condition may be circumvented by using a [[staggered grid]] and requiring that &lt;math&gt;\nabla\cdot \mathbf{u}^{n+1}&lt;/math&gt; vanish at the pressure nodes that are adjacent to the boundaries.

A distinguishing feature of Chorin's projection method is that the velocity field is forced to satisfy a discrete continuity constraint at the end of each time step.

== General method ==

Typically the projection method operates as a two-stage fractional step scheme, a method which uses multiple calculation steps for each numerical time-step. In many projection algorithms, the steps are split as follows:

# First the system is progressed in time to a mid-time-step position, solving the above transport equations for mass and momentum using a suitable advection method. This is denoted the ''predictor'' step.
# At this point an initial projection may be implemented such that the mid-time-step velocity field is enforced as divergence free.
# The ''corrector'' part of the algorithm is then progressed. These use the time-centred estimates of the velocity, density, etc. to form final time-step state.
# A final projection is then applied to enforce the divergence restraint on the velocity field. The system has now been fully updated to the new time.

== References ==
{{reflist}}

[[Category:Computational fluid dynamics]]
[[Category:Mathematical physics]]</text>
      <sha1>3xnj4c384x4cz118225dzhjm69n94h7</sha1>
    </revision>
  </page>
  <page>
    <title>Quasi-empirical method</title>
    <ns>0</ns>
    <id>159735</id>
    <revision>
      <id>794442606</id>
      <parentid>650220554</parentid>
      <timestamp>2017-08-08T01:49:59Z</timestamp>
      <contributor>
        <username>Cryptopocalypse</username>
        <id>30116816</id>
      </contributor>
      <minor/>
      <comment>/* See also */ Fix capitalization</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3080">{{unreferenced|date=September 2008}}
'''Quasi-empirical methods''' are [[methodology|methods]] applied in [[science]] and [[mathematics]] to achieve [[epistemology]] similar to that of [[empiricism]] (thus ''[[wikt:quasi-#Prefix|quasi-]] + empirical'') when [[experience]] cannot [[falsifiability|falsify]] the ideas involved. [[Empirical research]] relies on [[empirical evidence]], and its empirical methods involve [[experiment]]ation and disclosure of apparatus for [[reproducibility]], by which scientific findings are [[verification and validation|validated]] by other scientists. Empirical methods are studied extensively in the [[philosophy of science]], but they cannot be used directly in fields whose hypotheses cannot be falsified by real experiment (for example, [[mathematics]], [[philosophy]], [[theology]], and [[ideology]]). Because of such [[empirical limits in science]], the [[scientific method]] must rely not only on empirical methods but sometimes also on quasi-empirical ones. The prefix ''[[wikt:quasi-#Prefix|quasi-]]'' came to denote methods that are "almost" or "socially approximate" an ideal of truly empirical methods.

It is unnecessary to find all counterexamples to a theory; all that is required to disprove a theory logically is one counterexample. The converse does not prove a theory; [[Bayesian inference]] simply makes a theory more likely, by weight of evidence.

One can argue that no science is capable of finding all counter-examples to a theory, therefore, no science is strictly empirical, it's all quasi-empirical. But usually, the term "quasi-empirical" refers to the means of choosing problems to focus on (or ignore), selecting prior work on which to build an argument or proof, notations for informal claims, peer review and acceptance, and incentives to discover, ignore, or correct errors.  These are common to both [[science]] and [[mathematics]], and do not include experimental method.

[[Albert Einstein]]'s discovery of the [[general relativity]] theory relied upon [[thought experiments]] and [[mathematics]]. Empirical methods only became relevant when confirmation was sought. Furthermore, some empirical confirmation was found only some time after the general acceptance of the theory.

Thought experiments are almost standard procedure in [[philosophy]], where a conjecture is tested out in the imagination for possible effects on experience; when these are thought to be implausible,  unlikely to occur, or not actually occurring, then the conjecture may be either rejected or amended.  [[Logical positivism]] was a perhaps extreme version of this practice, though this claim is open to debate.

Post-20th-century [[philosophy of mathematics]] is mostly concerned with [[quasi-empiricism in mathematics|quasi-empirical mathematical methods]], especially as reflected in the actual [[mathematical practice]] of working mathematicians.  

==See also==
* [[Philosophy of science]]

[[Category:Scientific method]]
[[Category:Philosophy of science]]
[[Category:Philosophy of mathematics]]
[[Category:Thought experiments]]</text>
      <sha1>ab9ajd6sabxh48ds8czldvjsfhttjhv</sha1>
    </revision>
  </page>
  <page>
    <title>Second moment method</title>
    <ns>0</ns>
    <id>18422596</id>
    <revision>
      <id>841546136</id>
      <parentid>790776379</parentid>
      <timestamp>2018-05-16T14:05:55Z</timestamp>
      <contributor>
        <username>OAbot</username>
        <id>28481209</id>
      </contributor>
      <minor/>
      <comment>[[Wikipedia:OABOT|Open access bot]]: add arxiv identifier to citation with #oabot.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8549">{{Cleanup|date=January 2009}}

In mathematics, the '''second moment method''' is a technique used in [[probability theory]] and [[analysis]] to show that a [[random variable]] has positive probability of being positive. More generally, the "moment method" consists of bounding the probability that a random variable fluctuates far from its mean, by using its moments.&lt;ref&gt;{{cite web |url=http://terrytao.wordpress.com/2008/06/18/the-strong-law-of-large-numbers/ |title=The strong law of large numbers|author=[[Terence Tao]]|accessdate=2009-02-10 |work=What’s new?|date=2008-06-18 }}&lt;/ref&gt;

The method is often quantitative, in that one can often deduce a lower bound on the probability that the random variable is larger than some constant times its expectation. The method involves comparing the second [[moment (mathematics)|moment]] of random variables to the square of the first moment.

==First moment method==
The first moment method is a simple application of [[Markov's inequality]] for integer-valued variables.  For a '''non-negative''', '''integer-valued''' random variable ''X'', we may want to prove that ''X'' = 0  with high probability. To obtain an upper bound for P(''X'' &gt; 0), and thus a lower bound for P(''X'' = 0), we first note that since ''X'' takes only integer values, P(''X'' &gt; 0) = P(''X'' ≥ 1).  Since ''X'' is non-negative we can now apply [[Markov's inequality]] to obtain P(''X'' ≥ 1) ≤ E[''X'']. Combining these we have P(''X'' &gt; 0) ≤ E[''X'']; the first moment method is simply the use of this inequality.

==Second moment method==
In the other direction, E[''X''] being "large" does not directly imply that P(''X'' = 0) is small. However, we can often use the second moment to derive such a conclusion, using [[Cauchy–Schwarz inequality]].

'''Theorem''': If ''X''&amp;nbsp;≥&amp;nbsp;0 is a [[random variable]] with
finite variance, then

:&lt;math&gt;
\operatorname{P}( X &gt; 0 )
\ge \frac{(\operatorname{E}[X])^2}{\operatorname{E}[X^2]}.
&lt;/math&gt;

'''Proof''': Using the [[Cauchy–Schwarz inequality]], we have
:&lt;math&gt;
\operatorname{E}[X] = \operatorname{E}[ X \, \mathbf{1}_{\{ X &gt; 0 \}} ] \le \operatorname{E}[X^2]^{1/2} \operatorname{P}( X &gt; 0)^{1/2}.
&lt;/math&gt;
Solving for &lt;math&gt;\operatorname{P}( X &gt; 0)&lt;/math&gt;, the desired inequality then follows. ∎

The method can also be used on distributional limits of random variables. Furthermore, the estimate of the previous theorem can be refined by means of the so-called  [[Paley–Zygmund inequality]]. Suppose that ''X&lt;sub&gt;n&lt;/sub&gt;'' is a sequence of non-negative real-valued random variables which [[converge in law]] to a random variable ''X''. If there are finite positive constants ''c''&lt;sub&gt;1&lt;/sub&gt;, ''c''&lt;sub&gt;2&lt;/sub&gt; such that

:&lt;math&gt;E \left [X_n^2 \right ] \le c_1 E[X_n]^2&lt;/math&gt;
:&lt;math&gt;E \left [X_n \right ]\ge c_2&lt;/math&gt;

hold for every ''n'', then it follows from the [[Paley–Zygmund inequality]] that for every ''n'' and θ in (0, 1)

:&lt;math&gt; P (X_n \geq c_2 \theta) \geq \frac{(1-\theta)^2}{c_1}.&lt;/math&gt;

Consequently, the same inequality is satisfied by ''X''.

==Example application of method==

===Setup of problem===
The [[Bernoulli bond percolation]] [[Glossary of graph theory#Subgraphs|subgraph]] of a graph ''G'' at parameter ''p'' is a random subgraph obtained from ''G'' by deleting every edge of ''G'' with probability 1−''p'', independently. The [[binary tree|infinite complete binary tree]] ''T'' is an infinite [[tree (graph theory)|tree]] where one vertex (called the root) has two neighbors and every other vertex has three neighbors. The second moment method can be used to show that at every parameter ''p'' ∈ (1/2, 1] with positive probability the connected component of the root in the percolation subgraph of ''T'' is infinite.

===Application of method===
Let ''K'' be the percolation component of the root, and let ''T&lt;sub&gt;n&lt;/sub&gt;'' be the set of vertices of ''T'' that are at distance ''n'' from the root. Let ''X&lt;sub&gt;n&lt;/sub&gt;'' be the number of vertices in ''T&lt;sub&gt;n&lt;/sub&gt;'' ∩ ''K''. To prove that ''K'' is infinite with positive probability, it is enough to show that &lt;math&gt;\limsup_{n\to\infty}1_{X_n&gt;0}&gt;0&lt;/math&gt; with positive probability. By the [[reverse Fatou lemma]], it suffices to show that &lt;math&gt;\inf_{n\to\infty}P(X_n&gt;0)&gt;0&lt;/math&gt;. The [[Cauchy–Schwarz inequality]] gives
:&lt;math&gt;E[X_n]^2\le E[X_n^2] \, E\left [(1_{X_n&gt;0})^2\right ]=E[X_n^2]\,P(X_n&gt;0).&lt;/math&gt;
Therefore, it is sufficient to show that
:&lt;math&gt; \inf_n \frac{E \left[ X_n \right ]^2}{E \left[ X_n^2 \right ]}&gt;0\,,&lt;/math&gt;
that is, that the second moment is bounded from above by a constant times the first moment squared (and both are nonzero). In many applications of the second moment method, one is not able to calculate the moments precisely, but can nevertheless establish this inequality.

In this particular application, these moments can be calculated. For every specific ''v'' in ''T&lt;sub&gt;n&lt;/sub&gt;'',

:&lt;math&gt;P(v\in K)=p^n. &lt;/math&gt;

Since &lt;math&gt;|T_n|=2^n&lt;/math&gt;, it follows that

:&lt;math&gt;E[X_n]=2^n\,p^n&lt;/math&gt;

which is the first moment. Now comes the second moment calculation.

:&lt;math&gt;E\!\left[X_n^2 \right ] = E\!\left[\sum_{v\in T_n} \sum_{u\in T_n}1_{v\in K}\,1_{u\in K}\right] = \sum_{v\in T_n} \sum_{u\in T_n} P(v,u\in K).&lt;/math&gt;

For each pair ''v'', ''u'' in ''T&lt;sub&gt;n&lt;/sub&gt;'' let ''w(v, u)'' denote the vertex in ''T'' that is farthest away from the root and lies on the simple path in ''T'' to each of the two vertices ''v'' and ''u'', and let ''k(v, u)'' denote the distance from ''w'' to the root. In order for ''v'', ''u'' to both be in ''K'', it is necessary and sufficient for the three simple paths from ''w(v, u)'' to ''v'', ''u'' and the root to be in ''K''. Since the number of edges contained in the union of these three paths is 2''n'' − ''k(v, u)'', we obtain

:&lt;math&gt;P(v,u\in K)= p^{2n-k(v,u)}.&lt;/math&gt;

The number of pairs ''(v, u)'' such that ''k(v, u)'' = ''s'' is equal to &lt;math&gt;2^s\,2^{n-s}\,2^{n-s-1}=2^{2n-s-1}&lt;/math&gt;, for ''s'' = 0, 1, ..., ''n''. Hence,

:&lt;math&gt;E\!\left[X_n^2 \right ] = \sum_{s=0}^n 2^{2n-s-1} p^{2n-s} = \frac 12\,(2p)^n \sum_{s=0}^n (2p)^s = \frac12\,(2p)^n \, \frac{(2p)^{n+1}-1}{2p-1} \le \frac p{2p-1} \,E[X_n]^2,&lt;/math&gt;

which completes the proof.

===Discussion===
*The choice of the random variables ''X''&lt;sub&gt;''n''&lt;/sub&gt; was rather natural in this setup. In some more difficult applications of the method, some ingenuity might be required in order to choose the random variables ''X''&lt;sub&gt;''n''&lt;/sub&gt; for which the argument can be carried through.
*The [[Paley–Zygmund inequality]] is sometimes used instead of the [[Cauchy–Schwarz inequality]] and may occasionally give more refined results.
*Under the (incorrect) assumption that the events ''v'', ''u'' in ''K'' are always independent, one has &lt;math&gt;P(v,u\in K)=P(v\in K)\,P(u\in K)&lt;/math&gt;, and the second moment is equal to the first moment squared. The second moment method typically works in situations in which the corresponding events or random variables are “nearly independent".
*In this application, the random variables ''X''&lt;sub&gt;''n''&lt;/sub&gt; are given as sums
:: &lt;math&gt;X_n=\sum_{v\in T_n}1_{v\in K}.&lt;/math&gt;
: In other applications, the corresponding useful random variables are [[integral]]s
:: &lt;math&gt;X_n=\int f_n(t)\,d\mu(t),&lt;/math&gt;
: where the functions ''f''&lt;sub&gt;''n''&lt;/sub&gt; are random. In such a situation, one considers the product measure ''&amp;mu;''&amp;nbsp;&amp;times;&amp;nbsp;''&amp;mu;'' and calculates
:: &lt;math&gt; \begin{align}
E \left[X_n^2 \right ] &amp; = E\left[\int\int f_n(x)\,f_n(y)\,d\mu(x)\,d\mu(y)\right ] \\
&amp; = E\left[ \int\int E\left[f_n(x)\,f_n(y)\right]\,d\mu(x)\,d\mu(y)\right ],
\end{align}&lt;/math&gt;
:where the last step is typically justified using [[Fubini's theorem]].

==References==
*{{Citation | last1=Burdzy | first1=Krzysztof | last2=Adelman | first2=Omer | last3=Pemantle | first3=Robin | title=Sets avoided by Brownian motion | hdl=1773/2194  | year=1998 | journal=Annals of Probability | volume=26 | issue=2 | pages=429–464 | doi=10.1214/aop/1022855639| arxiv=math/9701225 }}
*{{Citation | last1=Lyons | first1=Russell | title=Random walk, capacity, and percolation on trees | year=1992 | journal=Annals of Probability | volume=20 | issue=4 | pages=2043–2088 | doi=10.1214/aop/1176989540}}
*{{Citation | last1=Lyons | first1=Russell | last2=Peres | first2=Yuval | title=Probability on trees and networks | url=http://mypage.iu.edu/~rdlyons/prbtree/prbtree.html}}
&lt;references/&gt;

[[Category:Probabilistic inequalities]]
[[Category:Articles containing proofs]]
[[Category:Moment (mathematics)]]</text>
      <sha1>46qh045k9ytiyw8n7t40jwccu8cwz6s</sha1>
    </revision>
  </page>
  <page>
    <title>Semidefinite embedding</title>
    <ns>0</ns>
    <id>1187410</id>
    <revision>
      <id>824797385</id>
      <parentid>732162500</parentid>
      <timestamp>2018-02-09T15:24:26Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Mathematical optimization]]; added [[Category:Optimization algorithms and methods]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6901">'''Semidefinite embedding''' (SDE) or '''maximum variance unfolding (MVU)''' is an [[algorithm]] in [[computer science]] that uses [[semidefinite programming]] to perform [[non-linear dimensionality reduction]] of high-dimensional [[coordinate vector|vector]]ial input data. MVU can be viewed as a non-linear generalization of [[Principal component analysis]].

Non-linear dimensionality reduction algorithms attempt to map high-dimensional data onto a low-dimensional [[Euclidean space|Euclidean]] [[vector space]]. Maximum variance Unfolding is a member of the [[manifold learning]] family, which also include algorithms such as [[isomap]] and [[locally linear embedding]]. In manifold learning, the input data is assumed to be sampled from a low dimensional [[manifold]] that is embedded inside of a higher-dimensional vector space. The main intuition behind MVU is to exploit the local linearity of manifolds and create a mapping that preserves local neighbourhoods at every point of the underlying manifold. 

MVU creates a mapping from the high dimensional input vectors to some low dimensional Euclidean vector space in the following steps:

A [[neighbourhood (topology)|neighbourhood]] graph is created. Each input is connected with its k-nearest input vectors (according to Euclidean distance metric) and all k-nearest neighbors are connected with each other. If the data is sampled well enough, the resulting graph is a discrete approximation of the underlying manifold. 

The neighbourhood graph is "unfolded" with the help of semidefinite programming. Instead of learning the output vectors directly, the semidefinite programming aims to find an inner product matrix that maximizes the pairwise distances between any two inputs that are not connected in the neighbourhood graph while preserving the nearest neighbors distances. 

The low-dimensional embedding is finally obtained by application of [[multidimensional scaling]] on the learned inner product matrix.

The steps of applying semidefinite programming followed by a linear dimensionality reduction step to recover a low-dimensional embedding into a Euclidean space were first proposed by Linial, London, and Rabinovich.

==Optimization Formulation==

Let &lt;math&gt;X \,\!&lt;/math&gt; be the original input and &lt;math&gt;Y\,\!&lt;/math&gt; be the embedding. If &lt;math&gt;i,j\,\!&lt;/math&gt; are two neighbors, then the local isometry constraint that needs to be satisfied is:

:&lt;math&gt;|X_{i}-X_{j}|^{2}=|Y_{i}-Y_{j}|^{2}\,\!&lt;/math&gt;

Let &lt;math&gt;G, K\,\!&lt;/math&gt; be the [[Gramian matrix|Gram matrices]] of &lt;math&gt; X \,\!&lt;/math&gt; and &lt;math&gt; Y \,\!&lt;/math&gt; (i.e.: &lt;math&gt;G_{ij}=X_i \cdot X_j,K_{ij}=Y_i \cdot Y_j \,\!&lt;/math&gt;). We can express the above constraint for every neighbor points &lt;math&gt;i,j\,\!&lt;/math&gt; in term of &lt;math&gt;G, K\,\!&lt;/math&gt;:

:&lt;math&gt;G_{ii}+G_{jj}-G_{ij}-G_{ji}=K_{ii}+K_{jj}-K_{ij}-K_{ji}\,\!&lt;/math&gt;

In addition, we also want to constrain the embedding &lt;math&gt; Y \,\!&lt;/math&gt; to center at the origin:

&lt;math&gt;\sum_{i}Y_{i}=0\Leftrightarrow(\sum_{i}Y_{i}) \cdot (\sum_{i}Y_{i})=0\Leftrightarrow\sum_{i,j}Y_{i} \cdot Y_{j}=0\Leftrightarrow\sum_{i,j}K_{ij}=0&lt;/math&gt;

As described above, except the distances of neighbor points are preserved, the algorithm aims to maximize the pairwise distance of every pair of points. The objective function to be maximized is:

&lt;math&gt;T(Y)=\dfrac{1}{2N}\sum_{i,j}|Y_{i}-Y_{j}|^{2}&lt;/math&gt;

Intuitively, maximizing the function above is equivalent to pulling the points as far away from each other as possible and therefore "unfold" the manifold. The local isometry constraint prevents the objective function from going to infinity. Proof:

Let &lt;math&gt;\tau = max \{\eta_{ij}|Y_{i}-Y_{j}|^2\} \,\!&lt;/math&gt; where &lt;math&gt; \eta_{ij} = 1 \,\!&lt;/math&gt; if i and j are neighbors and &lt;math&gt; \eta_{ij} = 0 \,\!&lt;/math&gt; otherwise.

Since the graph has N points, the distance between any two points &lt;math&gt;|Y_{i}-Y_{j}|^2 \leq N \tau \,\!&lt;/math&gt;. We can then bound the objective function as follow:

:&lt;math&gt;T(Y)=\dfrac{1}{2N}\sum_{i,j}|Y_{i}-Y_{j}|^{2} \leq \dfrac{1}{2N}\sum_{i,j}(N\tau)^2 = \dfrac{N^3\tau^2}{2} \,\!&lt;/math&gt;

The objective function can be rewritten purely in the form of the Gram matrix:

:&lt;math&gt; 
\begin{align}
  T(Y) &amp;{}=  \dfrac{1}{2N}\sum_{i,j}|Y_{i}-Y_{j}|^{2} \\
          &amp;{}= \dfrac{1}{2N}(\sum_{i,j}(Y_{i}^2+Y_{j}^2-Y_{i} \cdot Y_{j} - Y_{j} \cdot Y_{i})\\
          &amp;{}= \dfrac{1}{2N}(\sum_{i,j}Y_{i}^2+\sum_{i,j}Y_{j}^2-\sum_{i,j}Y_{i} \cdot Y_{j} -\sum_{i,j}Y_{j} \cdot Y_{i})\\  
          &amp;{}= \dfrac{1}{2N}(\sum_{i,j}Y_{i}^2+\sum_{i,j}Y_{j}^2-0 -0)\\
          &amp;{}= \dfrac{1}{N}(\sum_{i}Y_{i}^2)=\dfrac{1}{N}(Tr(K))\\
\end{align}
\,\!&lt;/math&gt;

Finally, the optimization can be formulated as:

'''Maximize''' &lt;math&gt; Tr(K) \,\!&lt;/math&gt;

'''Subject to''' &lt;math&gt; K \succeq 0\,\!&lt;/math&gt; and
&lt;math&gt; \forall i,j \,\!&lt;/math&gt; where &lt;math&gt; \eta_{ij} =1, G_{ii}+G_{jj}-G_{ij}-G_{ji}=K_{ii}+K_{jj}-K_{ij}-K_{ji} \,\!&lt;/math&gt; 

After the Gram matrix &lt;math&gt;K \,\!&lt;/math&gt; is learned by semidefinite programming, the output &lt;math&gt;Y \,\!&lt;/math&gt; can be obtained via [[Cholesky decomposition]]. In particular, the Gram matrix can be written as &lt;math&gt; K_{ij}=\sum_{\alpha = 1}^{N}(\lambda_{\alpha } V_{\alpha i} V_{\alpha j}) \,\!&lt;/math&gt; where &lt;math&gt; V_{\alpha i} \,\!&lt;/math&gt; is the i-th element of eigenvector &lt;math&gt; V_{\alpha} \,\!&lt;/math&gt; of the eigenvalue &lt;math&gt; \lambda_{\alpha } \,\!&lt;/math&gt;.

It follows that the &lt;math&gt; \alpha \,\!&lt;/math&gt;-th element of the output &lt;math&gt; Y_i \,\!&lt;/math&gt; is &lt;math&gt; \sqrt{\lambda_{\alpha }} V_{\alpha i} \,\!&lt;/math&gt;.

==See also==
* [[Locally linear embedding]]
* [[Isometry (mathematics) (disambiguation)|Isometry (mathematics)]]
* [[Local Tangent Space Alignment]]
* [[Riemannian manifold]]
* [[Energy minimization]]

==References==
*[http://repository.upenn.edu/cgi/viewcontent.cgi?article=1000&amp;context=cis_papers Unsupervised learning of image manifolds by semidefinite programming] K. Q. Weinberger and L. K. Saul (2004). In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR-04), Washington D.C.
*[http://www.springerlink.com/content/t21q747q278qx4x1/ Unsupervised learning of image manifolds by semidefinite programming] K. Q. Weinberger and L. K. Saul (2005), International Journal of Computer Vision - In Special Issue: Computer Vision and Pattern Recognition-CVPR 2005 Guest Editor(s): [[Aaron Bobick]], [[Rama Chellappa]], [[Larry S. Davis|Larry Davis]], pages 77–90, Volume 70, Number 1, [[Springer Netherlands]]
*[http://citeseer.ist.psu.edu/170127.html The geometry of graphs and some of its algorithmic applications], [[Nathan Linial]], [[Eran London]], [[Yuri Rabinovich]], [[IEEE]] Symposium on Foundations of Computer Science.

==External links==
*[http://www.cse.wustl.edu/~kilian/code/code.html MVU Matlab code online]

[[Category:Computational statistics]]
[[Category:Dimension reduction]]
[[Category:Optimization algorithms and methods]]</text>
      <sha1>1ya0uv4pf4byzk2v8cyzq6i1l16v0ne</sha1>
    </revision>
  </page>
  <page>
    <title>Semiperfect magic cube</title>
    <ns>0</ns>
    <id>372393</id>
    <revision>
      <id>742845018</id>
      <parentid>665111043</parentid>
      <timestamp>2016-10-06T04:33:07Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor/>
      <comment>/* References */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="669">{{expert-subject|1=Combinatorics|date=June 2015}}
In [[mathematics]], a '''semiperfect magic cube''' is a [[magic cube]] that is not a [[perfect magic cube]], i.e., a magic cube for which the [[cross section (geometry)|cross section]] diagonals do not necessarily sum up to the cube's [[magic constant]].

==References==
*{{citation|title=The Zen of Magic Squares, Circles, and Stars: An Exhibition of Surprising Structures across Dimensions|first=Clifford A.|last=Pickover|authorlink=Clifford A. Pickover|publisher=Princeton University Press|year=2003|page=98|url=https://books.google.com/books?id=-mu8O8RMG6QC&amp;pg=PA98}}.

[[Category:Magic squares]]



{{combin-stub}}</text>
      <sha1>h75lh27k7j9ufhkle28lmtvm3jfpkhz</sha1>
    </revision>
  </page>
  <page>
    <title>Signedness</title>
    <ns>0</ns>
    <id>1609200</id>
    <revision>
      <id>792830866</id>
      <parentid>768863111</parentid>
      <timestamp>2017-07-29T00:03:17Z</timestamp>
      <contributor>
        <username>RandomDSdevel</username>
        <id>9668018</id>
      </contributor>
      <minor/>
      <comment>/* In programming languages */ Improved a sentence grammar, style, and clarity.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3307">{{refimprove|date=December 2009}}
In computing, '''signedness''' is a property of [[data type]]s representing [[number]]s in computer programs. A numeric variable is ''signed'' if it can represent both [[positive number|positive]] and [[negative number|negative]] numbers, and ''unsigned'' if it can only represent [[non-negative]] numbers (zero or positive numbers).

As [[Sign (mathematics)|''signed'']] numbers can represent negative numbers, they lose a range of positive numbers that can only be represented with ''unsigned'' numbers of the same size (in bits) because half the possible [[value (programming)|values]] are non-positive values (so if an 8-bit is signed, positive unsigned values 128 to 255 are gone while -128 to 127 are present). Unsigned variables can dedicate all the possible values to the positive number range.

For example, a [[two's complement]] signed [[16-bit]] integer can hold the values −32768 to 32767 inclusively, while an unsigned 16 bit integer can hold the values 0 to [[65535 (number)|65535]]. For this [[Signed number representations|sign representation]] method, the leftmost bit ([[most significant bit]]) denotes whether the value is positive or negative (0 for positive, 1 for negative).

== In programming languages ==
For most architectures, there is no signed–unsigned type distinction in the [[machine language]]. Nevertheless, [[computer arithmetic|arithmetic]] instructions usually set different [[status register|CPU flags]] such as the [[carry flag]] for unsigned arithmetic and the [[overflow flag]] for signed. Those values can be taken into account by subsequent [[branch instruction|branch]] or arithmetic commands.

The [[C programming language]], along with its derivatives, implements a signedness for all [[integer (computing)|integer data types]], as well as for [[character (computing)|"character"]]. The &lt;tt&gt;unsigned&lt;/tt&gt; modifier defines the type to be unsigned. The default integer signedness is signed, but can be set explicitly with &lt;tt&gt;signed&lt;/tt&gt; modifier. Integer [[literal (computer programming)|literals]] can be made unsigned with &lt;tt&gt;U&lt;/tt&gt; suffix. For example, &lt;tt&gt;[[Hexadecimal|0x]]&lt;/tt&gt;&lt;tt&gt;FFFFFFFF&lt;/tt&gt; gives −1, but &lt;tt&gt;0xFFFFFFFFU&lt;/tt&gt; gives 4,294,967,295 for 32-bit code.

Compilers often issue a warning when comparisons are made between signed and unsigned numbers or when one is [[type conversion|cast]] to the other. These are potentially dangerous operations as the ranges of the signed and unsigned types are different.

==See also==
*[[Sign bit]]
*[[Signed number representations]]
*[[Sign (mathematics)]]
* [[Binary Angular Measurement System]], an example of semantics where signedness does not matter

== External links ==
* {{cite web |url=http://dev.mysql.com/doc/refman/5.0/en/numeric-type-overview.html |title=Numeric Type Overview |year=2011 |work=[[MySQL]] 5.0 Reference Manual |publisher=mysql.com |accessdate=6 January 2012}}
* {{Citation
| url   = https://www.securecoding.cert.org/confluence/display/c/INT02-C.+Understand+integer+conversion+rules
| title = Understand integer conversion rules
| work  = CERT C Coding Standard
| publisher  = [[Computer emergency response team]]
| accessdate = December 31, 2015
| ref = none
}}

{{Data types}}

[[Category:Computer arithmetic]]
[[Category:Data types]]</text>
      <sha1>2jskrgiutba9kedvv9zube6lq1plhkn</sha1>
    </revision>
  </page>
  <page>
    <title>Smallest-circle problem</title>
    <ns>0</ns>
    <id>14355284</id>
    <revision>
      <id>868534006</id>
      <parentid>867565095</parentid>
      <timestamp>2018-11-12T20:24:10Z</timestamp>
      <contributor>
        <username>Motmahp</username>
        <id>5439527</id>
      </contributor>
      <comment>Edited algorithm comments, removed newline that divided algorithm into two blocks.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15265">{{Merge from|1-center problem|date=November 2017}}
[[Image:Smallest circle problem.svg|thumb|right|300px|Some instances of the smallest bounding circle.]]

The '''smallest-circle problem''' or '''minimum covering circle problem''' is a [[mathematical problem]] of computing the smallest [[circle]] that contains all of a given [[set (mathematics)|set]] of [[point (geometry)|points]] in the [[Euclidean plane]]. The corresponding problem in [[n-dimensional space|''n''-dimensional space]], the smallest [[bounding sphere|bounding-sphere]] problem, is to compute the smallest [[n-sphere|''n''-sphere]] that contains all of a given set of points.&lt;ref name="eh-mcsp-72"&gt;{{citation|first1=J.|last1=Elzinga|first2=D. W.|last2=Hearn|title=The minimum covering sphere problem|journal=[[Management Science (journal)|Management Science]]|volume=19|pages=96–104|year=1972|doi=10.1287/mnsc.19.1.96}}&lt;/ref&gt; The smallest-circle problem was initially proposed by the English mathematician [[James Joseph Sylvester]] in 1857.&lt;ref&gt;{{citation|first=J. J.|last=Sylvester|authorlink=James Joseph Sylvester|title=A question in the geometry of situation|journal=[[Quarterly Journal of Mathematics]]|volume=1|page=79|year=1857}}.&lt;/ref&gt;

The smallest-circle problem in [[the plane]] is an example of a [[facility location]] problem (the [[1-center problem]]) in which the location of a new facility must be chosen to provide service to a number of customers, minimizing the farthest distance that any customer must travel to reach the new facility.&lt;ref&gt;{{citation|last1=Francis|first1=R. L.|first2=L. F.|last2=McGinnis|first3=J. A.|last3=White|title=Facility Layout and Location: An Analytical Approach|edition=2nd|publisher=Prentice–Hall, Inc.|location=Englewood Cliffs, N.J.|year=1992}}.&lt;/ref&gt; Both the smallest circle problem in the plane, and the smallest bounding sphere problem in any higher-dimensional space of bounded dimension are solvable in [[linear time]].

==Characterization==
Most of the geometric approaches for the problem look for points that lie on the boundary of the minimum circle and are based on the following simple facts:
* The minimum covering circle is unique.
* The minimum covering circle of a set ''S'' can be determined by at most three points in ''S'' which lie on the boundary of the circle. If it is determined by only two points, then the [[line segment]] joining those two points must be a [[diameter]] of the minimum circle. If it is determined by three points, then the triangle consisting of those three points is not [[obtuse triangle|obtuse]].

{{Collapse top|title=Proof that the minimum covering disk is unique}}
Let {{var|P}} be any set of points in the plane, and suppose that there are two smallest enclosing disks of {{var|P}}, with centers at &lt;math&gt;\vec{z}_1&lt;/math&gt; and &lt;math&gt;\vec{z}_2&lt;/math&gt;. Let {{var|r}} be their shared radius, and let &lt;math&gt;2a&lt;/math&gt; be the distance between their centers. Then since {{var|P}} is a subset of both disks it is a subset of their intersection. However, their intersection is contained within the disk with center &lt;math&gt;\frac 12 (\vec{z}_1+\vec{z}_2)&lt;/math&gt; and radius &lt;math&gt;\sqrt{r^2-a^2}&lt;/math&gt;, as shown in the following image:

:[[Image:The smallest enclosing disk of points in the plane is unique.svg|300px]]

Since {{var|r}} is minimal, we must have &lt;math&gt;\sqrt{r^2-a^2}=r&lt;/math&gt;, meaning &lt;math&gt;a=0&lt;/math&gt;, so the disks are identical.{{sfn|Welzl|1991|p=2}}
{{Collapse bottom}}

==Linear-time solutions==
As [[Nimrod Megiddo]] showed,&lt;ref&gt;{{citation
 | last = Megiddo | first = Nimrod | authorlink = Nimrod Megiddo
 | doi = 10.1137/0212052
 | issue = 4
 | journal = [[SIAM Journal on Computing]]
 | mr = 721011
 | pages = 759–776
 | title = Linear-time algorithms for linear programming in {{math|'''R'''&lt;sup&gt;3&lt;/sup&gt;}} and related problems
 | volume = 12
 | year = 1983}}.&lt;/ref&gt; the minimum enclosing circle can be found in linear time, and the same linear time bound also applies to the [[smallest enclosing sphere]] in Euclidean spaces of any constant dimension.

[[Emo Welzl]]&lt;ref&gt;{{citation
 | last = Welzl | first = Emo | authorlink = Emo Welzl
 | editor-last = Maurer | editor-first = H.
 | doi = 10.1007/BFb0038202
 | contribution = Smallest enclosing disks (balls and ellipsoids)
 | volume = 555
 | pages = 359–370
 | publisher = Springer-Verlag
 | series = Lecture Notes in Computer Science
 | title = New Results and New Trends in Computer Science
 | year = 1991| isbn = 978-3-540-54869-0 | citeseerx = 10.1.1.46.1450 }}.&lt;/ref&gt; proposed a simple randomized algorithm for the
minimum covering circle problem that runs in expected &lt;math&gt;O(N)&lt;/math&gt; time, based on a [[linear programming]] algorithm of [[Raimund Seidel]]. This algorithm is presented below.

Subsequently, the smallest-circle problem was included in a general class of [[LP-type problem]]s that can be solved by algorithms like Welzl's based on linear programming. As a consequence of membership in this class, it was shown that the dependence on the dimension of the constant factor in the &lt;math&gt;O(N)&lt;/math&gt; time bound, which was factorial for Seidel's method, could be reduced to subexponential, while still maintaining only linear dependence on ''N''.&lt;ref&gt;{{citation
 | last1 = Matoušek | first1 = Jiří | author1-link = Jiří Matoušek (mathematician)
 | last2 = Sharir | first2 = Micha | author2-link = Micha Sharir
 | last3 = Welzl | first3 = Emo
 | doi = 10.1007/BF01940877
 | journal = [[Algorithmica]]
 | pages = 498–516
 | title = A subexponential bound for linear programming
 | url = http://www.inf.ethz.ch/personal/emo/PublFiles/SubexLinProg_ALG16_96.pdf
 | volume = 16
 | issue = 4–5 | year = 1996| citeseerx = 10.1.1.46.5644 }}.&lt;/ref&gt;

=== Welzl's algorithm ===

The algorithm is [[recursive algorithm|recursive]], and takes as arguments two (finite) sets of points {{var|P}} and {{var|R}}; it computes the smallest enclosing circle of the [[Union (set theory)|union]] of {{var|P}} and {{var|R}}, as long as every point of {{var|R}} is one of the boundary points of the eventual smallest enclosing circle. Thus, the original smallest enclosing circle problem can be solved by calling the algorithm with {{var|P}} equal to the set of points to be enclosed and {{var|R}} equal to the [[empty set]]; as the algorithm calls itself recursively, it will enlarge the set {{var|R}} passed into the recursive calls until it includes all the boundary points of the circle.

The algorithm processes the points of {{var|P}} in a random order, maintaining as it does the set {{var|S}} of processed points and the smallest circle {{var|D}} that encloses the union {{var|S}} ∪ {{var|R}}. At each step, it tests whether the next point {{var|p}} to be processed is in {{var|D}}; if not, the algorithm replaces the {{var|D}} by the result of a recursive call of the algorithm on the sets {{var|S}} and {{var|R}} ∪ {{var|p}}. Whether the circle was replaced or not, {{var|p}} is then included in the set {{var|S}}. Processing each point, therefore, consists of testing in constant time whether the point belongs to a single circle and possibly performing a recursive call to the algorithm. It can be shown that the overall time is linear.

 '''algorithm''' welzl:{{sfn|Welzl|1991}}
     '''input:''' Finite sets {{var|P}} and {{var|R}} of points in the plane
     '''output:''' Minimal disk enclosing {{var|P}} with {{var|R}} on the boundary, or undefined if no such disk exists
     '''if''' {{var|P}} is empty '''or''' |{{var|R}}| ≥ 3:
         '''if''' |{{var|R}}| = 1:
             ({{var|P}} is empty, and smallest "circle" containing the one point of {{var|R}} has radius zero)
             {{var|p}} := {{var|R}}[0]
             return circle({{var|p}}, 0)
         '''else if''' |{{var|R}}| = 2:
             ({{var|P}} is empty, and smallest circle containing the two points of {{var|R}} is centered at midpoint)
             {{var|p0}} := {{var|R[0]}}
             {{var|p1}} := {{var|R[1]}}
             {{var|center}} := midpoint({{var|p0}}, {{var|p1}})
             {{var|diameter}} := distance({{var|p0}}, {{var|p1}})
             return circle({{var|center}}, {{var|diameter}} / 2)
         '''else if''' the points of {{var|R}} are cocircular:
             return the circle they determine
         '''else''':
             return '''undefined'''
     '''choose''' {{var|p}} in {{var|P}} ([[Randomly selected|randomly]] and [[Uniform distribution (continuous)|uniformly]])
     D := welzl({{var|P}} - { {{var|p}} }, {{var|R}})
     '''if''' {{var|p}} is in {{var|D}}:
         return {{var|D}}
     return welzl({{var|P}} - { {{var|p}} }, {{var|R}} ∪ { {{var|p}} })

==Other algorithms==
Prior to Megiddo's result showing that  the smallest-circle problem may be solved in linear time,  several algorithms of higher complexity appeared in the literature. A naive algorithm solves the problem in time O(''n''&lt;sup&gt;4&lt;/sup&gt;) by testing the circles determined by all pairs and triples of points.
* An algorithm of Chrystal and Peirce applies a [[Local search (optimization)|local optimization]] strategy that maintains two points on the boundary of an enclosing circle and repeatedly shrinks the circle, replacing the pair of boundary points, until an optimal circle is found. Chakraborty and Chaudhuri&lt;ref&gt;{{citation|first1=R. K.|last1=Chakraborty|first2=P. K.|last2=Chaudhuri|title=Note on geometrical solutions for some minimax location problems|journal=[[Transportation Science]]|volume=15|issue=2|pages=164–166|year=1981|doi=10.1287/trsc.15.2.164}}.&lt;/ref&gt; propose a linear-time method for selecting a suitable initial circle and a pair of boundary points on that circle. Each step of the algorithm includes as one of the two boundary points a new vertex of the [[convex hull]], so if the hull has ''h'' vertices this method can be implemented to run in time O(''nh'').
* Elzinga and Hearn&lt;ref&gt;{{citation|first1=J.|last1=Elzinga|first2=D. W.|last2=Hearn|title=Geometrical solutions for some minimax location problems|journal=[[Transportation Science]]|volume=6|issue=4|pages=379–394|year=1972|doi=10.1287/trsc.6.4.379}}.&lt;/ref&gt; described an algorithm which maintains a covering circle for a subset of the points. At each step, a point not covered by the current sphere is used to find a larger sphere that covers a new subset of points, including the point found. Although its worst case running time is O(''h''&lt;sup&gt;3&lt;/sup&gt;''n''), the authors report that it ran in linear time in their experiments.  The complexity of the method has been analyzed by Drezner and Shelah.&lt;ref&gt;{{citation|first1=Z.|last1=Drezner|first2=S.|last2=Shelah|title=On the complexity of the Elzinga–Hearn algorithm for the 1-center problem|journal=[[Mathematics of Operations Research]]|volume=12|issue=2|pages=255–261|year=1987|jstor=3689688|doi=10.1287/moor.12.2.255}}.&lt;/ref&gt; Both Fortran and C codes are available from {{harvtxt|Hearn|Vijay|Nickel|1995}}.&lt;ref&gt;{{citation|first1= D. W.|last1=Hearn|first2=J.|last2=Vijay|first3=S.|last3=Nickel|title=Codes of geometrical algorithms for the (weighted) minimum circle problem|journal=European Journal of Operational Research|volume=80|pages=236–237|year=1995|doi=10.1016/0377-2217(95)90075-6}}.&lt;/ref&gt;
* The smallest sphere problem can be formulated as a [[quadratic program]]&lt;ref name="eh-mcsp-72"/&gt; defined by a system of linear constraints with a convex quadratic objective function. Therefore, any feasible direction algorithm can give the solution of the problem.&lt;ref&gt;{{citation|first=S. K.|last=Jacobsen|title=An algorithm for the minimax Weber problem|journal=European Journal of Operational Research|volume=6|issue=2|pages=144–148|year=1981|doi=10.1016/0377-2217(81)90200-9}}.&lt;/ref&gt; Hearn and Vijay&lt;ref name="hv-eawmcp-82"&gt;{{citation|first1=D. W.|last1=Hearn|first2=J.|last2=Vijay|title=Efficient algorithms for the (weighted) minimum circle problem|journal=[[Operations Research (journal)|Operations Research]]|volume=30|issue=4|pages=777–795|year=1982|doi=10.1287/opre.30.4.777}}.&lt;/ref&gt; proved that the feasible direction approach chosen by Jacobsen is equivalent to the Chrystal–Peirce algorithm.
* The dual to this quadratic program may also be formulated explicitly;&lt;ref&gt;{{citation|first1=J.|last1=Elzinga|first2=D. W.|last2=Hearn|first3=W. D.|last3=Randolph|title=Minimax multifacility location with Euclidean distances|journal=[[Transportation Science]]|volume=10|issue=4|pages=321–336|year=1976|doi=10.1287/trsc.10.4.321}}.&lt;/ref&gt; an algorithm of Lawson&lt;ref&gt;{{citation|first=C. L.|last=Lawson|title=The smallest covering cone or sphere|journal=[[SIAM Review]]|volume=7|issue=3|doi=10.1137/1007084|pages=415–417|year=1965}}.&lt;/ref&gt; can be described in this way as a primal dual algorithm.&lt;ref name="hv-eawmcp-82"/&gt;
* Shamos and Hoey&lt;ref&gt;{{citation|first1=M. I.|last1=Shamos|author1-link=Michael Ian Shamos|first2=D.|last2=Hoey|contribution=Closest point problems|title=Proceedings of 16th Annual IEEE Symposium on Foundations of Computer Science|pages=151–162|year=1975|doi=10.1109/SFCS.1975.8|title-link=Symposium on Foundations of Computer Science}}.&lt;/ref&gt; proposed an O(''n''&amp;nbsp;log&amp;nbsp;''n'') time algorithm for the problem based on the observation that the center of the smallest enclosing circle must be a vertex of the farthest-point [[Voronoi diagram]] of the input point set.

==Weighted variants of the problem==

The weighted version of the minimum covering circle problem takes as input a set of points in a Euclidean space, each with weights; the goal is to find a single point that minimizes the maximum weighted distance to any point. The original minimum covering circle problem can be recovered by setting all weights to the same number. As with the unweighted problem, the weighted problem may be solved in linear time in any space of bounded dimension, using approaches closely related to bounded dimension linear programming algorithms, although slower algorithms are again frequent in the literature.&lt;ref name="hv-eawmcp-82"/&gt;&lt;ref&gt;{{citation|first=N.|last=Megiddo|authorlink=Nimrod Megiddo|title=The weighted Euclidean 1-center problem|journal=[[Mathematics of Operations Research]]|volume=8|issue=4|pages=498–504|year=1983|doi=10.1287/moor.8.4.498}}.&lt;/ref&gt;&lt;ref&gt;{{citation|first1=N.|last1=Megiddo|author1-link=Nimrod Megiddo|first2=E.|last2=Zemel|title=An ''O''(''n''&amp;nbsp;log&amp;nbsp;''n'') randomizing algorithm for the weighted Euclidean 1-center problem|journal=Journal of Algorithms|volume=7|issue=3|pages=358–368|year=1986|doi=10.1016/0196-6774(86)90027-1}}.&lt;/ref&gt;

== See also ==
* [[Bounding sphere]]
* [[1-center problem]]
* [[Circumscribed circle]]
* [[Closest string]]

==References==
{{reflist|colwidth=30em}}

==External links==
* [http://www.inf.ethz.ch/personal/gaertner/miniball.html Bernd Gärtner's smallest enclosing ball code]
* [http://www.cgal.org/Manual/latest/doc_html/cgal_manual/Bounding_volumes/Chapter_main.html CGAL] the ''Min_sphere_of_spheres'' package of the ''Computational Geometry Algorithms Library'' (CGAL)
* [https://github.com/hbf/miniball Miniball] an open-source implementation of an algorithm for the smallest enclosing ball problem for low and moderately high dimensions

[[Category:Computational geometry]]
[[Category:Combinatorial optimization]]
[[Category:Circles]]</text>
      <sha1>fnitlyu25tv908bis2yrrv0bnlufxwz</sha1>
    </revision>
  </page>
  <page>
    <title>Superrationality</title>
    <ns>0</ns>
    <id>277125</id>
    <revision>
      <id>867186455</id>
      <parentid>862237275</parentid>
      <timestamp>2018-11-04T06:01:12Z</timestamp>
      <contributor>
        <ip>2601:199:680:6121:E0BA:638B:7F32:F724</ip>
      </contributor>
      <comment>/* Probabilistic strategies */  changed a semantically meaningfully confusing pronoun to a disambiguated version.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9907">In [[economics]] and [[game theory]], a participant is considered to have '''superrationality''' (or '''renormalized rationality''') if they have [[perfect rationality]] (and thus maximize their own utility) but assume that all other players are superrational too and that a superrational individual will always come up with the same strategy as any other superrational thinker when facing the same problem. Applying this definition, a superrational player playing against a superrational opponent in a [[prisoner's dilemma]] will cooperate while a rationally self-interested player would defect. 

This [[decision rule]] is not a mainstream model within [[game theory]] and was suggested by [[Douglas Hofstadter]] in his article, series, and book ''[[Metamagical Themas]]''&lt;ref name=":0"&gt;{{cite journal|last=Hofstadter|first=Douglas|date=June 1983|title=Dilemmas for Superrational Thinkers, Leading Up to a Luring Lottery|url=http://www.gwern.net/docs/1985-hofstadter#dilemmas-for-superrational-thinkers-leading-up-to-a-luring-lottery|journal=[[Scientific American]]|volume=248|number=6}} – reprinted in: {{Cite book|title=[[Metamagical Themas]]|last=Hofstadter|first=Douglas|publisher=Basic Books|year=1985|isbn=0-465-04566-9|pages=737–755}}&lt;/ref&gt; as an alternative type of rational [[decision making]] different from the widely accepted [[game-theoretic]] one. Superrationality is a form of [[Immanuel Kant]]'s [[categorical imperative]].&lt;ref&gt;{{cite journal |title=Reviews |first=Paul J. |last=Campbell |journal=Mathematics Magazine |volume=57 |issue=1 |date=January 1984 |pages=51–55 |jstor=2690298}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |title=Volunteer's Dilemma |first=Andreas |last=Diekmann |journal=The Journal of Conflict Resolution |volume=29 |issue=4 |date=December 1985 |pages=605–610 |doi=10.1177/0022002785029004003 |jstor=174243}}&lt;/ref&gt;{{clarify|date=March 2018}} Hofstadter provided this definition: "Superrational thinkers, by recursive definition, include in their calculations the fact that they are in a group of superrational thinkers."&lt;ref name=":0" /&gt;

Note that contrary to the [[Homo reciprocans]], the superrational thinker will not always play the equilibrium that maximizes the total social utility, and is thus not a [[Philanthropy|philanthropist]].

==Prisoner's dilemma==
The idea of superrationality is that two logical thinkers analyzing the same problem will think of the same correct answer. For example, if two people are both good at math and both have been given the same complicated problem to do, both will get the same right answer. In math, knowing that the two answers are going to be the same doesn't change the value of the problem, but in game theory, knowing that the answer will be the same might change the answer itself.

The [[prisoner's dilemma]] is usually framed in terms of jail sentences for criminals, but it can be stated equally well with cash prizes instead. Two players are each given the choice to cooperate (C) or to defect (D). The players choose without knowing what the other is going to do. If both cooperate, each will get $100. If they both defect, they each get $1. If one cooperates and the other defects, then the defecting player gets $200, while the cooperating player gets nothing.

The four outcomes and the payoff to each player are listed below

{| class="wikitable"
|-
! !! Player B cooperates !! Player B defects
|-
! Player A cooperates
| Both get $100|| Player A: $0 &lt;br/&gt;Player B: $200
|-
! Player A defects
| Player A: $200&lt;br/&gt; Player B: $0 || Both get $1
|}

One valid way for the players to reason is as follows:
# Assuming the other player defects, if I cooperate I get nothing and if I defect I get a dollar.
# Assuming the other player cooperates, I get $100 if I cooperate and $200 if I defect.
# So whatever the other player does, my payoff is increased by defecting, if only by one dollar.

The conclusion is that the rational thing to do is to defect. This type of reasoning defines game-theoretic rationality, and two game-theoretic rational players playing this game both defect and receive a dollar each.

Superrationality is an alternative method of reasoning. First, it is assumed that the answer to a symmetric problem will be the same for all the superrational players. Thus the sameness is taken into account ''before'' knowing what the strategy will be. The strategy is found by maximizing the payoff to each player, assuming that they all use the same strategy. Since the superrational player knows that the other superrational player will do the same thing, whatever that might be, there are only two choices for two superrational players. Both will cooperate or both will defect depending on the value of the superrational answer. Thus the two superrational players will both cooperate, since this answer maximizes their payoff. Two superrational players playing this game will each walk away with $100.

Note that a superrational player playing against a game-theoretic rational player will defect, since the strategy only assumes that the superrational players will agree. {{Citation needed span|text=A superrational player playing against a player of uncertain superrationality will sometimes defect and sometimes cooperate, based on the probability of the other player being superrational.|date=September 2013}}

Although standard game theory assumes common knowledge of rationality, it does so in a different way. The game theoretic analysis maximizes payoffs by allowing each player to change strategies independently of the others, even though in the end, it assumes that the answer in a symmetric game will be the same for all. This is the definition of a game theoretic [[Nash equilibrium]], which defines a stable strategy as one where no player can improve the payoffs by unilaterally changing course. The superrational equilibrium in a symmetric game is one where all the players' strategies are forced to be the same before the maximization step. (There is no agreed upon extension of the concept of superrationality to asymmetric games.)

Some argue that superrationality implies a kind of [[magical thinking]] in which each player supposes that their decision to cooperate will cause the other player to cooperate, despite the fact that there is no communication. Hofstadter points out that the concept of "choice" doesn't apply when the player's goal is to figure something out, and that the decision does not cause the other player to cooperate, but rather same logic leads to same answer independent of communication or cause and effect. This debate is over whether it is reasonable for human beings to act in a superrational manner, not over what superrationality means, and is similar to arguments about whether it is reasonable for humans to act in a 'rational' manner, as described by game theory (wherein they can figure out what other players will or have done by asking themselves, what would I do if I was them, and applying [[backwards induction]] and [[iterated elimination of dominated strategies]]).

==Probabilistic strategies==
For simplicity, the foregoing account of superrationality ignored [[mixed strategy|mixed strategies]]: the possibility that the best choice could be to flip a coin, or more generally to choose different outcomes with some probability. In the [[prisoner's dilemma]], it is superrational to cooperate with probability 1 even when mixed strategies are admitted, because the average payoff when one player cooperates and the other defects is the same as when both cooperate, and so defecting increases the risk of both defecting, which decreases the expected payout. But in some cases, the superrational strategy is mixed.

For example, if the payoffs in are as follows:

:: CC – $100/$100
:: CD – $0/$1,000,000
:: DC – $1,000,000/$0
:: DD – $1/$1

So that defecting has a huge reward, the superrational strategy is defecting with a probability of 499,900/999,899 or a little over 49.995%. As the reward increases to infinity, the probability only approaches 1/2 further, and the losses for adopting the simpler strategy of 1/2 (which are already minimal) approach 0. In a less extreme example, if the payoff for one cooperator and one defector was $400 and $0, respectively, the superrational mixed strategy world be defecting with probability 100/299 or about 1/3.

In similar situations with more players, using a randomising device can be essential. One example discussed by Hofstadter is the [[platonia dilemma]]: an eccentric trillionaire contacts 20 people, and tells them that if one and only one of them sends him or her a telegram (assumed to cost nothing) by noon the next day, that person will receive a billion dollars. If they receive more than one telegram, or none at all, no one will get any money, and communication between players is forbidden. In this situation, the superrational thing to do (if it is known that all 20 are superrational) is to send a telegram with probability p=1/20 — that is, each recipient essentially rolls a [[Icosahedron|20-sided die]] and only sends a telegram if it comes up "1".  This maximizes the probability that exactly one telegram is received.

Notice though that this is not the solution in a conventional game-theoretical analysis. Twenty game-theoretically rational players would each send in telegrams and therefore receive nothing. This is because sending telegrams is the [[dominant strategy]]; if an individual player sends telegrams they have a chance of receiving money, but if they send no telegrams they cannot get anything. (If all telegrams were guaranteed to arrive, they would only send one, and no one would expect to get any money.)

==See also==
* [[Perfect rationality]]
* [[Prisoner's dilemma]]
* [[Newcomb's problem]]
* [[Evidential decision theory]]

==References==
{{Reflist}}
{{Douglas Hofstadter}}

[[Category:Game theory]]
[[Category:Philosophical concepts]]</text>
      <sha1>83kjzv2fo4n1d4h3uaju1tjcswrhnyu</sha1>
    </revision>
  </page>
  <page>
    <title>Symmetric set</title>
    <ns>0</ns>
    <id>22260858</id>
    <revision>
      <id>833564763</id>
      <parentid>590021907</parentid>
      <timestamp>2018-04-01T10:41:07Z</timestamp>
      <contributor>
        <username>PhamNhatThien</username>
        <id>33432853</id>
      </contributor>
      <comment>/* Examples */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1148">In mathematics, a nonempty subset ''S'' of a [[group (mathematics)|group]] ''G'' is said to be '''symmetric''' if
:&lt;math&gt;S=S^{-1}&lt;/math&gt;
where &lt;math&gt;S^{-1} = \{ x^{-1} : x \in S \}&lt;/math&gt;. In other words, ''S'' is symmetric if &lt;math&gt;x^{-1} \in S&lt;/math&gt; whenever &lt;math&gt;x \in S&lt;/math&gt;.

If ''S'' is a subset of a [[vector space]], then ''S'' is said to be symmetric if it is symmetric with respect to the additive group structure of the vector space; that is, if &lt;math&gt;S = -S = \{ -x : x \in S \}&lt;/math&gt;.

==Examples==
* In '''R''', examples of symmetric sets are intervals of the type &lt;math&gt;(-k, k)&lt;/math&gt; with &lt;math&gt;k &gt; 0&lt;/math&gt;, and the sets '''Z''' and &lt;math&gt;\{ -1, 1 \}&lt;/math&gt;.
* Any vector subspace in a vector space is a symmetric set.
* If ''S'' is any subset of a group, then &lt;math&gt;S\cup S^{-1}&lt;/math&gt; and &lt;math&gt;S^{-1}\cap S&lt;/math&gt; are symmetric sets.

==References==
*R. Cristescu, Topological vector spaces, Noordhoff International Publishing, 1977.
*W. Rudin, Functional Analysis, McGraw-Hill Book Company, 1973.

{{PlanetMath attribution|id=4528|title=symmetric set}}

{{Functional Analysis}}

[[Category:Set theory]]

{{settheory-stub}}</text>
      <sha1>4gdjxwtf6haexwir3unv2xd5fou9dnt</sha1>
    </revision>
  </page>
  <page>
    <title>Vladimir Berkovich</title>
    <ns>0</ns>
    <id>34421039</id>
    <revision>
      <id>852076844</id>
      <parentid>842799361</parentid>
      <timestamp>2018-07-26T13:29:11Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 0 sources and tagging 1 as dead. #IABot (v2.0beta2)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2067">{{Infobox scientist
| name = Vladimir G. Berkovich
| image = Vladimir Berkovich.jpg
| image_size =
| caption =
| birth_date =
| birth_place =
| death_date =
| death_place =
| nationality = Israeli
| fields = [[Mathematics]]
| workplaces = [[Weizmann Institute of Science]]
| alma_mater =
| doctoral_advisor = [[Yuri I. Manin]]
| doctoral_students =
| known_for = [[Berkovich space]]
| awards =
}}
'''Vladimir Berkovich''' is a mathematician at the [[Weizmann Institute of Science]] who introduced [[Berkovich space]]s. His Ph.D. advisor was [[Yuri I. Manin]].  Berkovich was a visiting scholar at the [[Institute for Advanced Study]] in 1991-92 and again in the summer of 2000.&lt;ref&gt;[http://www.ias.edu/people/cos/index.html Institute for Advanced Study: A Community of Scholars]{{Dead link|date=July 2018 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;

In 2012 he became a fellow of the [[American Mathematical Society]].&lt;ref&gt;[http://www.ams.org/profession/fellows-list List of Fellows of the American Mathematical Society], retrieved 2012-11-10.&lt;/ref&gt;

==Selected publications==
* {{Citation | last1=Berkovich | first1=Vladimir G. | title=Spectral theory and analytic geometry over non-Archimedean fields | url=https://books.google.com/books/about/Spectral_theory_and_analytic_geometry_ov.html?id=6x5PIl2-DkkC | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys and Monographs | isbn=978-0-8218-1534-2 |mr=1070709 | year=1990 | volume=33}}

==References==
{{reflist}}

==External links==
* [http://www.wisdom.weizmann.ac.il/~vova/ Personal page of Vladimir Berkovich] at the [[Weizmann Institute of Science]]
* {{MathGenealogy |id=62856}}

{{Authority control}}

{{DEFAULTSORT:Berkovich, Vladimir}}
[[Category:Israeli mathematicians]]
[[Category:Living people]]
[[Category:Institute for Advanced Study visiting scholars]]
[[Category:Weizmann Institute faculty]]
[[Category:20th-century Israeli mathematicians]]
[[Category:21st-century mathematicians]]
[[Category:Fellows of the American Mathematical Society]]</text>
      <sha1>5b7co9xcv866rfeeyv7tu87rops8zts</sha1>
    </revision>
  </page>
  <page>
    <title>West Coast Number Theory</title>
    <ns>0</ns>
    <id>37083491</id>
    <revision>
      <id>813777644</id>
      <parentid>734807752</parentid>
      <timestamp>2017-12-05T05:27:57Z</timestamp>
      <contributor>
        <username>Jonboy</username>
        <id>25410842</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2119">'''West Coast Number Theory (WCNT)''', a meeting that has also been known variously as the '''Western Number Theory Conference''' and the '''Asilomar Number Theory meeting''', is an annual gathering of number theorists first organized by [[Derrick Henry Lehmer|D. H.]] and [[Emma Lehmer]] at the [[Asilomar Conference Grounds]] in 1969.&lt;ref&gt;[http://bancroft.berkeley.edu/Exhibits/Math/intro.html The Lehmers at Berkeley]&lt;/ref&gt; In his tribute to D.&amp;nbsp;H.&amp;nbsp;Lehmer, John Brillhart stated that "There is little doubt that one of [Dick and Emma's] most enduring contributions to the world of mathematicians is their founding of the West Coast Number Theory Meeting [an annual event] in 1969".&lt;ref&gt;J. Brillhart in Acta Arith. 62 (1992), 207–213&lt;/ref&gt; To date, the conference remains an active meeting of young and experienced number theorists alike.

==History==
'''West Coast Number Theory''' has been held at a variety of locations throughout western North America. Typically, odd years are held in Pacific Grove, CA. Until 2013, this was always at the [[Asilomar Conference Grounds]], though more recent meetings have moved to the Lighthouse Lodge, just up the road.

*1969 Asilomar
*1970 Tucson
*1971 Asilomar
*1972 Claremont
*1973 Los Angeles
*1974 Los Angeles
*1975 Asilomar
*1976 San Diego
*1977 Los Angeles
*1978 Santa Barbara
*1979 Asilomar
*1980 Tucson
*1981 Santa Barbara
*1982 San Diego
*1983 Asilomar
*1984 Asilomar
*1985 Asilomar
*1986 Tucson
*1987 Asilomar
*1988 Las Vegas
*1989 Asilomar
*1990 Asilomar
*1991 Asilomar
*1992 Corvallis
*1993 Asilomar
*1994 San Diego
*1995 Asilomar
*1996 Las Vegas
*1997 Asilomar
*1998 San Francisco
*1999 Asilomar
*2000 San Diego
*2001 Asilomar
*2002 San Francisco
*2003 Asilomar
*2004 Las Vegas
*2005 Asilomar
*2006 Ensenada
*2007 Asilomar
*2008 Fort Collins
*2009 Asilomar
*2010 Orem
*2011 Asilomar
*2012 Asilomar
*2013 Asilomar
* 2014 Pacific Grove
* 2015 Pacific Grove
* 2016 Pacific Grove
* 2017 Pacific Grove

==References==
&lt;references /&gt;

==External links==
* [http://westcoastnumbertheory.org West Coast Number Theory page]

[[Category:Mathematics conferences]]</text>
      <sha1>jaed3aj1tjk3g1j4wj6gtc4h8w9sgv8</sha1>
    </revision>
  </page>
  <page>
    <title>Wolfram Alpha</title>
    <ns>0</ns>
    <id>21903944</id>
    <revision>
      <id>867578030</id>
      <parentid>862001132</parentid>
      <timestamp>2018-11-06T16:49:09Z</timestamp>
      <contributor>
        <username>Floridada</username>
        <id>32352586</id>
      </contributor>
      <comment>updated November alexa rank</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="21437">{{Use mdy dates|date=October 2014}}
{{Infobox website
| name                 = Wolfram Alpha
| logo                 = Wolfram Alpha December 2016.svg
| caption              = Wolfram Alpha is based on the computational platform [[Mathematica]], written by British scientist [[Stephen Wolfram]] in 1988.
| url                  = {{URL|https://www.wolframalpha.com/}}
| commercial           = Yes
| type                 = [[Answer engine]]
| registration         = Optional
| owner                = Wolfram Alpha LLC
| author               = [[Wolfram Research]]
| alexa                = {{DecreasePositive}} 4,028 ({{as of|2018|11|06|alt=November 2018}})&lt;ref name="alexa"&gt;{{cite web|url= http://www.alexa.com/siteinfo/wolframalpha.com |title= Wolframalpha.com Site Info | publisher= [[Alexa Internet]] |accessdate= 2018-11-06 }}&lt;/ref&gt;
| num_employees        = 200 (as of 2012)
| programming_language = [[Wolfram Language]]
| launch date          = {{start date and age|2009|5|18}}&lt;ref name="launch date"&gt;{{cite web |author=The Wolfram&amp;#124;Alpha Launch Team |url=http://blog.wolframalpha.com/2009/05/08/so-much-for-a-quiet-launch/ |title=So Much for A Quiet Launch |work=Wolfram&amp;#124;Alpha Blog |publisher=Wolfram Alpha |date=May 8, 2009 |accessdate=2013-02-09}}&lt;/ref&gt; (official launch)&lt;br&gt;{{start date|2009|5|15}}&lt;ref name="updated launch detail"&gt;{{cite web |author=The Wolfram&amp;#124;Alpha Launch Team |url=http://blog.wolframalpha.com/2009/05/12/going-live-and-webcasting-it/ |work=Wolfram&amp;#124;Alpha Blog |title=Going Live—and Webcasting It |publisher=Wolfram Alpha |date=May 12, 2009 |accessdate=2013-02-09}}&lt;/ref&gt; (public launch)
| current status       = Active
}}

'''Wolfram Alpha''' (also styled '''WolframAlpha''', and '''Wolfram|Alpha''') is a computational knowledge engine&lt;ref name=Guardiandatasource&gt;{{cite news |title=Where does Wolfram Alpha get its information? |author=Bobbie Johnson |publisher=The Guardian |date=May 21, 2009 |accessdate=2013-03-08 |url=https://www.theguardian.com/technology/2009/may/21/1 }}&lt;/ref&gt; or [[answer engine]] developed by Wolfram Alpha LLC, a subsidiary of [[Wolfram Research]]. It is an online service that answers factual queries directly by computing the answer from externally sourced "curated data",&lt;ref&gt;{{Cite web|title = About Wolfram{{!}}Alpha: Making the World's Knowledge Computable|url = http://www.wolframalpha.com/about.html|website=wolframalpha.com|accessdate = 2015-11-25}}&lt;/ref&gt; rather than providing a list of documents or web pages that might contain the answer as a [[search engine]] might.&lt;ref&gt;{{cite news |url=https://www.theguardian.com/technology/2009/mar/09/search-engine-google |title=British search engine 'could rival Google' |last=Johnson |first=Bobbie |date=March 9, 2009 |work=The Guardian |publisher=Guardian News and Media |location=UK |accessdate=2013-02-09}}&lt;/ref&gt;

Wolfram Alpha, which was released on May 18, 2009, is based on Wolfram's earlier flagship product [[Wolfram Mathematica]], a computational platform or toolkit that encompasses computer algebra, symbolic and numerical computation, visualization, and statistics capabilities.&lt;ref name="launch date" /&gt; Additional data is gathered from both academic and commercial websites such as the CIA's ''[[The World Factbook]]'', the [[United States Geological Survey]], a Cornell University Library publication called ''[https://www.allaboutbirds.org/ All About Birds]'', ''[[Chambers Biographical Dictionary]]'', [[Dow Jones]], the ''[[Catalogue of Life]]'',&lt;ref name=Guardiandatasource /&gt; [[CrunchBase]],&lt;ref name=techcrunch&gt;{{cite news |last=Dillet |first=Romain |title=Wolfram Alpha Makes CrunchBase Data Computable Just In Time For Disrupt SF |url=https://techcrunch.com/2012/09/07/wolfram-alpha-makes-crunchbase-data-computable-just-in-time-for-disrupt/ |publisher=TechCrunch |date=September 7, 2012 |accessdate=2013-02-09}}&lt;/ref&gt; [[Best Buy]],&lt;ref&gt;{{cite news |last=Golson |first=Jordan |title=Wolfram Delivers Siri-Enabled Shopping Results From Best Buy |url=http://www.macrumors.com/2011/12/16/wolfram-delivers-siri-enabled-shopping-results-from-best-buy/ |publisher=MacRumors |date=December 16, 2011 |accessdate=2013-02-09}}&lt;/ref&gt; the [[Federal Aviation Administration|FAA]]&lt;ref&gt;{{cite news |last=Barylick |first=Chris |title=Wolfram Alpha search engine now tracks flight paths, trajectory information |url=https://www.engadget.com/2011/11/19/wolfram-alpha-search-engine-now-tracks-flight-paths-trajectory/ |publisher=Engadget |date=November 19, 2011 |accessdate=2013-02-09}}&lt;/ref&gt; and optionally a user's Facebook account.

== Overview ==
Users submit queries and computation requests via a text field.  Wolfram Alpha then computes answers and relevant visualizations from a [[knowledge base]] of [[Data curation|curated]], [[structured data]] that come from other sites and books. The site "use[s] a portfolio of automated and manual methods, including statistics, visualization, source cross-checking, and expert review."&lt;ref&gt;{{cite web |title=Data in Wolfram&amp;#124;Alpha |url=http://www.wolframalpha.com/faqs5.html |website=Wolfram Alpha |accessdate=4 August 2015}}&lt;/ref&gt; The curated data makes Alpha different from [[semantic search]] engines, which index a large number of answers and then try to match the question to one.

Wolfram Alpha can only provide robust query results based on computational facts, not queries on the social sciences, cultural studies or even many questions about history where responses require more subtlety and complexity. It is able to respond to particularly-phrased [[natural language understanding|natural language]] fact-based questions such as "Where was [[Mary Robinson]] born?" or more complex questions such as "How old was [[Queen Elizabeth II]] in 1974?" It displays its "Input interpretation" of such a question, using standardized phrases such as "age | of Queen Elizabeth II (royalty) | in 1974", the answer of which is "Age at start of 1974: 47 years", and a biography link. Wolfram Alpha does not answer queries which require a narrative response such as "What is the difference between the [[Julian calendar|Julian]] and the [[Gregorian calendar|Gregorian]] calendars?" but will answer factual or computational questions such as "June 1 in Julian calendar".

Mathematical symbolism can be parsed by the engine, which typically responds with more than the numerical results. For example, "lim(x-&gt;0) (sin x)/x" yields the correct [[limit (functions)|limit]]ing value of 1, as well as a plot, up to 235 terms ({{as of|2013|lc=y}}) of the [[Taylor series]], and (for registered users) a possible derivation using [[L'Hôpital's rule]]. It is also able to perform calculations on data using more than one source. For example, "What is the [[List of countries by GDP (nominal) per capita|fifty-second smallest]] country by [[GDP per capita]]?" yields [[Nicaragua]], $1160 per year.

== Technology ==
Wolfram Alpha is written in 15 million lines of [[Wolfram Language]] code&lt;ref&gt;{{cite web |author=WolframResearch |url=https://www.youtube.com/watch?v=56ISaies6Ws#t=927s |title=Stephen Wolfram: The Background and Vision of Mathematica |publisher=Youtube.com |date=October 10, 2011 |accessdate=2013-02-09}}&lt;/ref&gt; and runs on more than 10,000 CPUs.&lt;ref&gt;{{cite news |first=Frederic |last=Lardinois |url=http://readwrite.com/2009/04/25/wolframalpha_our_first_impressions |title=Wolfram&amp;#124;Alpha: Our First Impressions |date=April 25, 2009 |publisher=ReadWriteWeb |accessdate=2013-02-09}}&lt;/ref&gt;&lt;ref&gt;{{cite news |first=Stephen |last=Wolfram |url=http://blog.wolframalpha.com/2009/05/15/wolframalpha-is-launching-made-possible-by-mathematica/ |title=Wolfram&amp;#124;Alpha Is Launching: Made Possible by ''Mathematica'' |work=WolframAlpha Blog |publisher=Wolfram Alpha |date=May 15, 2009 |accessdate=2013-02-09}}&lt;/ref&gt; The database currently includes hundreds of datasets, such as "All Current and Historical Weather." The datasets have been accumulated over several years.&lt;ref&gt;{{cite web |title=Taking a first bite out of Wolfram Alpha | first=Jane Fae | last=Ozimek |work=The Register |date=May 18, 2009 |url=https://www.theregister.co.uk/2009/05/18/wolfram_alpha/ |accessdate=2013-02-09}}&lt;/ref&gt; The curated (as distinct from auto-generated) datasets are checked for quality either by a scientist or other expert in a relevant field, or someone acting in a clerical capacity who simply verifies that the datasets are "acceptable".&lt;ref name=semanticabyss&gt;{{cite web |title=The Semantic Abyss - Plumbing the Semantic Web: Exploring the depths of the semantic gap between the Semantic Web and real world users and consumers |url=http://semanticabyss.blogspot.ca/2009/03/what-is-curated-data.html |year=2009 |author=Jack Krupansky}}&lt;/ref&gt;{{unreliable source?|date=September 2015}}

One example of a live dataset that Wolfram Alpha can use is the profile of a [[Facebook]] user, through inputting the "facebook report" query. If the user authorizes Facebook to share his or her account details with the Wolfram site, Alpha can generate a "personal analytics" report containing the age distribution of friends, the frequency of words used in status updates and other detailed information.&lt;ref name=techland&gt;{{cite news |first=Thomas E. |last=Weber |url=http://techland.time.com/2012/09/05/wolfram-alphas-facebook-analytics-tool-digs-deep-into-your-social-life/ |title=Wolfram Alpha's Facebook Analytics Tool Digs Deep into Your Social Life |work=Tech |publisher=Time Magazine |date=September 5, 2012 |accessdate=2013-02-09}}&lt;/ref&gt; Within two weeks of launching the Facebook analytics service, 400,000 users had used it.&lt;ref&gt;{{cite news |last=R. |first=A. |title=Visualising Facebook Who am I? |url=https://www.economist.com/blogs/graphicdetail/2012/09/visualising-facebook |publisher=The Economist |work=Graphic detail |date=September 21, 2012 |accessdate=2013-02-09}}&lt;/ref&gt; Downloadable query results are behind a pay wall but summaries are accessible to free accounts.&lt;ref name=publiclibraries&gt;{{cite web |url=http://publiclibrariesonline.org/2013/03/a-wolf-or-a-ram-what-is-wolfram-alpha/ |title=A Wolf or a Ram? What is Wolfram Alpha? |author=Joanna Nelson |date=March 4, 2013 |publisher=Public Libraries Online }}&lt;/ref&gt;

== Licensing partners ==
Wolfram Alpha has been used to power some searches in the [[Microsoft]] [[Bing (search engine)|Bing]] and [[DuckDuckGo]] search engines.&lt;ref&gt;{{cite news |first=Tom |last=Krazit |url=http://news.cnet.com/8301-30684_3-10315117-265.html |title=Bing strikes licensing deal with Wolfram Alpha |publisher=CNET |date=August 21, 2009 |accessdate=2013-02-09}}&lt;/ref&gt;&lt;ref&gt;{{cite web |author=The Wolfram&amp;#124;Alpha Team |date=April 18, 2011 |url=http://blog.wolframalpha.com/2011/04/18/wolframalpha-and-duckduckgo-partner-on-api-binding-and-search-integration/ |title=Wolfram&amp;#124;Alpha and DuckDuckGo Partner on API Binding and Search Integration |work=Wolfram&amp;#124;Alpha Blog |publisher=Wolfram Alpha |accessdate=2013-02-09}}&lt;/ref&gt; With the first release on July 21, 2017, [[Brave (web browser)|Brave]] web browser features Wolfram Alpha as one of its default search engines.&lt;ref&gt;{{cite web|title=Brave Browser Github page|url=https://github.com/brave/browser-laptop|website=Github|accessdate=10 August 2017}}&lt;/ref&gt;  For factual [[question answering]], it is also queried by Apple's [[Siri (software)|Siri]], Samsung's [[S Voice]], as well as Dexetra's [[speech recognition]] software for the [[Android (operating system)|Android]] platform, Iris, and the voice control software on [[BlackBerry 10]].&lt;ref&gt;{{cite web|url=http://www.berryreview.com/2013/10/21/blackberry-teams-up-with-wolfram-alpha-for-blackberry-10-voice-control/|title=BlackBerry Teams Up with Wolfram Alpha For BlackBerry 10 Voice Control|work=BerryReview}}&lt;/ref&gt;

== History ==
Launch preparations began on May 15, 2009 at 7&amp;nbsp;pm [[Central Daylight Time (North America)#Central Daylight Time|CDT]] and were broadcast live on [[Justin.tv]]. The plan was to publicly launch the service a few hours later, with expected issues due to extreme load. The service was officially launched on May 18, 2009.&lt;ref name="BBC"&gt;{{cite news |url=http://news.bbc.co.uk/1/hi/technology/8052798.stm |title=Wolfram 'search engine' goes live |publisher=BBC News |date=May 18, 2009 |accessdate=2013-02-09}}&lt;/ref&gt;

Wolfram Alpha has received mixed reviews.&lt;ref name="spivack"&gt;{{cite web |first=Nova |last=Spivack |title=Wolfram Alpha is Coming – and It Could be as Important as Google |date=March 7, 2009 |url=http://www.novaspivack.com/uncategorized/wolfram-alpha-is-coming-and-it-could-be-as-important-as-google |accessdate=2013-02-09 |publisher=Nova Spivack – Minding the Planet}}&lt;/ref&gt;&lt;ref&gt;{{cite news |first=Ryan |last=Singel |title=Wolfram&amp;#124;Alpha Fails the Cool Test |date=May 18, 2009 |url=https://www.wired.com/epicenter/2009/05/wolframalpha-fails-the-cool-test/ |publisher=Wired |accessdate=2013-02-09}}&lt;/ref&gt; Wolfram Alpha advocates point to its potential, some even stating that how it determines results is more important than current usefulness.&lt;ref name="spivack"/&gt;

On December 3, 2009, an [[iPhone]] app was introduced. Some users&lt;ref name="ios-price"&gt;{{cite web |first=MG |last=Siegler |url=https://techcrunch.com/2009/12/03/wolfram-alpha-iphone-app/ |title=Nice Try, Wolfram Alpha. Still Not Paying $50 For Your App. |publisher=TechCrunch |date=December 3, 2009 |accessdate=2013-02-09}}&lt;/ref&gt; considered the initial $50 price of the [[iOS]] app unnecessarily high, since the same features could be freely accessed by using a web browser instead. They also complained about the simultaneous removal of the mobile formatting option for the site.&lt;ref name="mobile-format"&gt;{{cite news |url=http://www.tuaw.com/2009/12/03/wolframalpha-iphone-formatted-web-page-no-longer-available/ |first=TJ |last=Luoma |title=WolframAlpha iPhone-formatted web page no longer available |publisher=TUAW |date=December 3, 2009 |accessdate=2013-02-09}}&lt;/ref&gt; Wolfram responded by lowering the price to $2, offering a refund to existing customers&lt;ref name="refund"&gt;{{cite web|last=Broida |first=Rick |url=http://reviews.cnet.com/8301-19512_7-10471978-233.html |title=Get Wolfram Alpha app for $1.99-and a refund if you paid more |publisher=CNET |date=April 1, 2010 |accessdate=2012-02-28}}&lt;/ref&gt; and re-instating the mobile site.

On October 6, 2010, an Android version of the app was released&lt;ref&gt;{{cite news |url=https://techcrunch.com/2010/10/06/wolframalphas-android-app-now-available/ |title=Wolfram Alpha's Android app now available |first=Leena |last=Rao |publisher=TechCrunch |date=October 6, 2010 |accessdate=2013-02-09}}&lt;/ref&gt; and it is now available for Kindle Fire and Nook. (The Nook version is not available outside the US). A further 71 apps are available which use the Wolfram Alpha engine for specialized tasks.&lt;ref&gt;{{cite web |url=http://products.wolframalpha.com/mobile/ |title=Wolfram&amp;#124;Alpha: Mobile &amp; Tablet Apps |year=2013 |accessdate=2013-02-09 |publisher=Wolfram Alpha}}&lt;/ref&gt;

On June 18, 2018, the [[Japanese language|Japanese]] version of Wolfram Alpha was released.&lt;ref name="Japanese version"&gt;{{cite web |first=Hideto |last=Tarai |title="WolframAlpha" which answers any difficult calculations and questions, the Japanese version is released |date=June 19, 2018 |url=https://forest.watch.impress.co.jp/docs/news/1128271.html |accessdate=June 19, 2018 |publisher=Windows Forest}}&lt;/ref&gt;

== Wolfram Alpha Pro ==
On February 8, 2012, Wolfram Alpha Pro was released,&lt;ref name="WAProAnnounce"&gt;{{cite news |first=Stephen |last=Wolfram |url=http://blog.wolframalpha.com/2012/02/08/announcing-wolframalpha-pro/ |title=Announcing Wolfram&amp;#124;Alpha Pro |date=February 8, 2012 |work=Wolfram&amp;#124;Alpha Blog |publisher=Wolfram Alpha |accessdate=2013-02-09}}&lt;/ref&gt; offering users additional features for a monthly subscription fee. A key feature is the ability to upload many common file types and data—including raw tabular data, images, audio, XML, and dozens of specialized scientific, medical, and mathematical formats—for automatic analysis. Other features include an extended keyboard, interactivity with [[Computable Document Format|CDF]], data downloads, in-depth step by step solution, the ability to customize and save graphical and tabular results&lt;ref name="Hachman"&gt;{{cite news |last=Hachman |first=Mark |title=Data Geeks, Meet Wolfram Alpha Pro |publisher=[[PC Magazine]] |date=February 7, 2012 |url=https://www.pcmag.com/article2/0,2817,2399911,00.asp |accessdate=2012-02-15}}&lt;/ref&gt; and extra computation time.&lt;ref name="WAProAnnounce" /&gt;

Along with new premium features, Wolfram Alpha Pro led to some changes in the free version of the site:
* An increase in advertisements on the free site.
* Text and PDF export options now require the user to set up a free account&lt;ref name="WAProAnnounce" /&gt; even though they existed before the introduction of Wolfram Alpha accounts.&lt;ref&gt;{{cite web|url=http://hplusmagazine.com/2009/06/24/users-guide-wolframalpha/|title=A User's Guide to Wolfram Alpha|first=Surfdaddy|last=Orca|publisher=H+ Magazine|date=2009-06-24|accessdate=2013-04-24}}&lt;/ref&gt;
* The option to request extra time for a long calculation is no longer available to free users.&lt;ref name="extra-time-before"&gt;{{cite web|url=http://web.mst.edu/~jkmq53/school/Fall_2011/English_160/files/Marlowe_Usability_Test.docx|title=Wolfram Alpha Usability Test Survey|first=James|last=Marlowe|year=2011|accessdate=2013-04-24}}&lt;/ref&gt;&lt;ref name="WAProAnnounce" /&gt;
* Step-by-step solving of math problems is limited to three steps for free users (previously uncapped). It has since been reduced to only one step with a preview of the second.&lt;ref name="StepByStep"&gt;{{cite web|url=http://blog.wolframalpha.com/2009/12/01/step-by-step-math/|title=Step-by-Step Math}}&lt;/ref&gt;

== Copyright claims ==
''[[InfoWorld]]'' published an article&lt;ref name="copyright"&gt;{{cite web |last=McAllister |first=Neil |url=http://www.infoworld.com/d/developer-world/how-wolfram-alpha-could-change-software-248 |title=How Wolfram Alpha could change software |publisher=InfoWorld |date=July 29, 2009 |accessdate=2012-02-28}}&lt;/ref&gt; warning readers of the potential implications of giving an automated website proprietary rights to the data it generates. [[Free software movement|Free software]] advocate [[Richard Stallman]] also opposes the idea of recognizing the site as a copyright holder and suspects that Wolfram would not be able to make this case under existing copyright law.&lt;ref name="fsf"&gt;{{cite mailing list |url=http://lists.essential.org/pipermail/a2k/2009-August/004865.html |title=How Wolfram Alpha's Copyright Claims Could Change Software |date=August 4, 2009 |accessdate=2012-02-17 |mailinglist=Access 2 Knowledge|archiveurl=https://web.archive.org/web/20130428041345/http://lists.essential.org/pipermail/a2k/2009-August/004865.html |archivedate=April 28, 2013 |last=Stallman |first=Richard |authorlink=Richard Stallman}}&lt;/ref&gt;

== See also ==
* [[Commonsense knowledge problem]]
* [[Artificial general intelligence|Strong AI]]
* [[Watson (computer)]]

== References ==
{{Reflist|colwidth=30em}}

== Further reading ==
* [https://www.bloomberg.com/news/articles/2009-03-08/wolframalpha-a-new-way-to-search Wolfram Alpha: A New Way To Search?], Stephen Wildstrom, ''BusinessWeek'', March 9, 2009.
* [http://www.informationweek.com/news/internet/search/showArticle.jhtml?articleID=215801388&amp;subSection=News Stephen Wolfram's Answer To Google: If Wolfram/Alpha works as advertised, it will be able to do something Google can't: provide answers that don't already exist in indexed documents.] by Thomas Claburn, ''InformationWeek'', March 10, 2009.
* [http://bits.blogs.nytimes.com/2009/03/09/better-search-doesnt-mean-beating-google/ Better Search Doesn’t Mean Beating Google] by Saul Hansell, ''The New York Times'', March 9, 2009.
* [http://www.pcworld.com/article/160904/wolfram_alpha_will_take_your_questions_any_questions.html Wolfram Alpha will Take Your Questions – Any Questions], Ian Paul, ''PC World'', March 9, 2009.
* [http://www.hplusmagazine.com/articles/ai/wolframalpha-searching-truth Wolfram Alpha: Searching for Truth: Stephen Wolfram talks with Rudy Rucker about his Upcoming Release] by [[Rudy Rucker]], ''H+ Magazine''.
*  [http://www.boston.com/business/technology/articles/2009/05/05/a_hungry_little_number_cruncher/ "A hungry little number cruncher: Wolfram Alpha search tool mines databases to yield math-based replies"] by [[Hiawatha Bray]], ''[[The Boston Globe]]'', May 5, 2009
* [http://newsbreaks.infotoday.com/NewsBreaks/Wolfram-Alpha-Semantic-Search-Is-Born-53892.asp "Wolfram Alpha: Semantic Search is Born" by [[Woody Evans]], May 21, 2009.]

== External links ==
* {{official website|mobile=http://m.wolframalpha.com/}}

{{Wolfram Research|state=uncollapsed}}
{{computable knowledge}}
{{Intelligent personal assistant software}}

[[Category:Agent-based software]]
[[Category:Computer algebra systems]]
[[Category:Educational math software]]
[[Category:Educational websites]]
[[Category:Information retrieval systems]]
[[Category:Internet properties established in 2009]]
[[Category:Mathematics education]]
[[Category:Natural language processing software]]
[[Category:Open educational resources]]
[[Category:Physics education]]
[[Category:Semantic Web]]
[[Category:Software calculators]]
[[Category:Virtual assistants]]
[[Category:Web analytics]]
[[Category:Websites which mirror Wikipedia]]
[[Category:Wolfram Research]]</text>
      <sha1>hr3cxr3f2sikolj439b743lpt62f2aa</sha1>
    </revision>
  </page>
</mediawiki>
