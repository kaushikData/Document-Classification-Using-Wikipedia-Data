<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>155 (number)</title>
    <ns>0</ns>
    <id>856626</id>
    <revision>
      <id>810505613</id>
      <parentid>807071475</parentid>
      <timestamp>2017-11-15T17:48:29Z</timestamp>
      <contributor>
        <username>Llammakey</username>
        <id>21878292</id>
      </contributor>
      <comment>/* In the military */ fixed links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3704">{{example farm|date=February 2013}}
{{Infobox number
| number = 155
| divisor = 1, 5, 31, 155
}}
'''155''' ('''one hundred [and] fifty-five''') is the [[natural number]] following [[154 (number)|154]] and preceding [[156 (number)|156]].

== In mathematics ==
'''155''' is:
*a [[composite number]]
*a [[semiprime]].
*a [[deficient number]], since &lt;math&gt;1+5+31=36&lt;155.&lt;/math&gt;
*[[Odious number|odious]], since its [[binary expansion]] &lt;math&gt;10011011_2&lt;/math&gt; has a total of 5 ones in it.

There are 155 primitive [[permutation group]]s of degree 81. {{OEIS2C|id=A000019}}

If one adds up all the primes from the least through the greatest prime factors of 155, that is, 5 and 31, the result is 155. {{OEIS|id=A055233}}  Only three other "small" semiprimes (10, 39, and 371) share this attribute.

==In the military==
* {{USS|Capable|AM-155}} was a [[United States Navy]] ''Admirable''-class minesweeper during [[World War II]]
* {{USS|Cole|DD-155}} was a United States Navy ''Wickes''-class destroyer during World War II
* {{USS|General A. W. Brewster|AP-155}} was a United States Navy ''General G. O. Squier''-class transport during World War II
* {{USS|Hopping|DE-155}} was a United States Navy ''Buckley''-class destroyer escort during World War II
* {{USS|Lycoming|APA-155}} was a United States Navy {{sclass-|Haskell|attack transport}} during World War II
* {{USS|Mustang|IX-155}} was a United States Navy four-master wooden [[schooner]] during World War II

==In sports==

* The [[maximum break#Breaks_exceeding_147|maximum possible score]] in a single break in [[snooker]], with a free ball at the start of the break ([[147 (number)|147]] is the highest possible without a free ball)

==In transportation==
* [[Alfa Romeo 155]], a [[compact executive car]] produced from 1992–1998
* [https://web.archive.org/web/20021002191528/http://transit.metrokc.gov/tops/bus/schedules/s155_0_.html Seattle Bus Route 155]
* [http://www.londonbusroutes.net/times/155.htm London Bus Route 155]
* The [[British Rail Class 155]] is a [[diesel multiple unit]] [[United Kingdom|British]] [[train]]
* [[Blériot 155]] was a [[France|French]] [[airliner]] of the 1920s
* [[155th Street (Manhattan)|155th Street, Manhattan, New York]]
* 155th Street New York City Subway stations:
** [[155th Street (IND Concourse Line)]]
** [[155th Street (IND Eighth Avenue Line)]]
** [[155th Street (IRT Ninth Avenue Line)]] (former)

==In other fields==
'''155''' is also:
* The year [[155|AD 155]] or [[155 BC]]
* 155 is the number for the International Operator in the United Kingdom
* 155 AH is a year in the [[Islamic calendar]] that corresponds to 771 &amp;ndash; 772 [[Common Era|CE]]
* [[155 Scylla]] is a [[Asteroid belt|main belt]] [[asteroid]]
* [[Europium-155]] is a [[radioisotope]] or [[Europium]] and [[fission product]] with a [[half-life]] of 4.76 years
* [[155 (song)|155]], a 2007 song by the band [[+44 (band)|+44]]
* The dialing code for [[Obihiro, Japan]]
* The [[atomic number]] of an element temporarily called [http://www.flw.com/datatools/periodic/001.php?id=155 unquintquinttium]
* [[155 North Wacker]] is a 46-story [[skyscraper]] under [[construction]] in [[Chicago]]
* [[Wolseley No. 155, Saskatchewan]] is a rural municipality in [[Canada]]

==See also==
* [[List of highways numbered 155]]
* [[United Nations Security Council Resolution 155]]
* [[List of United States Supreme Court cases, volume 155|United States Supreme Court cases, Volume 155]]
* [[Psalms 152–155]]
* [[McHenry County, Illinois]] [[Community High School District 155]]

== References ==
{{Reflist}}

==External links==
{{Commons category|155 (number)}}


{{DEFAULTSORT:155 (Number)}}
{{Integers|1}}
[[Category:Integers]]
{{Num-stub}}</text>
      <sha1>9aywxnb4nr8ekss597py9rgtgz78m4r</sha1>
    </revision>
  </page>
  <page>
    <title>Alan Baker (philosopher)</title>
    <ns>0</ns>
    <id>39448532</id>
    <revision>
      <id>862883854</id>
      <parentid>828062565</parentid>
      <timestamp>2018-10-07T09:38:33Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v2.0beta9)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4622">{{other people||Alan Baker (disambiguation){{!}}Alan Baker}}
'''Alan R. Baker''' is a professor of Philosophy in [[Swarthmore College]] ([[Pennsylvania]], United States), specializing in the [[philosophy of mathematics]] and the [[philosophy of science]]. He is also a former U.S. [[shogi]] champion and created the only active shogi club at an American university.

==Academic career==
Baker did his undergraduate studies at the [[University of Cambridge]], earning a bachelor's degree in philosophy with first class honours in 1991. He then moved to the U.S. for graduate school, earning a master's degree in 1995 and a Ph.D. in 1999, both in philosophy from [[Princeton University]]. His doctoral supervisors were [[Paul Benacerraf]] and Gideon Rosen. After working as an assistant professor at [[Xavier University]], he moved to Swarthmore in 2003.&lt;ref name="cv"&gt;[http://www.swarthmore.edu/Documents/academics/philosophy/Baker%20CV%201%20October%202012.pdf Curriculum vitae (October 2012)] {{Webarchive|url=https://web.archive.org/web/20150924112411/http://www.swarthmore.edu/Documents/academics/philosophy/Baker%20CV%201%20October%202012.pdf |date=2015-09-24 }}, retrieved 2014-04-08.&lt;/ref&gt;&lt;ref&gt;{{mathgenealogy|id=111419|name=Alan R. Baker}}&lt;/ref&gt;

Philosophically, Baker is a [[mathematical realism|mathematical realist]] who has used examples from [[evolutionary biology]] to show the necessity of mathematics in scientific reasoning.&lt;ref&gt;{{citation|title=Inference to the Best Explanation and Mathematical Realism|first=Sorin Ioan|last=Bangu|journal=Synthese|volume=160|issue=1|year=2008|pages=13–20|doi=10.1007/s11229-006-9070-8}}.&lt;/ref&gt;

In 2005 the ''[[New York Times]]'' published an excerpt from the exam from his ''“Introduction to Metaphysics and Epistemology”'' course in its ''“pop quiz”'' column.&lt;ref name=NYTimes2005-11-06a&gt;
{{cite news 
| url         = https://www.nytimes.com/2005/11/06/education/edlife/quiz.1.html?_r=0
| title       = Pop Quiz: I Think, Therefore I Pass 
| publisher   = [[New York Times]]
| author      = Alan Baker
| date        = 2005-11-06
| page        = 
| location    = 
| isbn        = 
| archiveurl  = https://web.archive.org/web/20140328234157/http://www.nytimes.com/2005/11/06/education/edlife/quiz.1.html?_r=0
| archivedate = 2014-03-28
| deadurl     = No 
| quote       = 
}}
&lt;/ref&gt;&lt;ref name=NYTimes2005-11-06b&gt;
{{cite news 
| url         = https://www.nytimes.com/2005/11/06/education/edlife/quizanswers.html
| title       = Quiz answers
| publisher   = [[New York Times]]
| author      = Alan Baker
| date        = 2005-11-06
| page        = 
| location    = 
| isbn        = 
| archiveurl  = https://web.archive.org/web/20121014012037/http://www.nytimes.com/2005/11/06/education/edlife/quizanswers.html
| archivedate = 2012-10-14
| accessdate  = 2014-03-28
| deadurl     = No 
| quote       = 
}}
&lt;/ref&gt;

== Shogi ==
Alan Baker has played [[shogi]] since 1996 and holds 29th place on the FESA list as of June 1, 2014.&lt;ref&gt;[http://www.shogi.net/fesa/index.php?mid=5&amp;listid=2013-06-01 FESA list for 1 jun 2014]&lt;/ref&gt; His highest place on the FESA list was 19th, on January 1, 2009.

In 2005 he founded a shogi club at Swarthmore College, outside [[Philadelphia]], which is the only active college-based shogi club in the U.S.&lt;ref name=Swarthmore /&gt;

Tournament results:

* 2008: Champion of the 13th U.S. [[Shogi]] championship.&lt;ref name=Swarthmore&gt;{{citation|url=http://www.swarthmore.edu/feature-stories-archive-2007-2008/shogi-and-me.xml|title=Shogi and me|publisher=Swarthmore College Feature Stories Archive|date=May 14, 2008}}.&lt;/ref&gt; 
* 2008: 3rd place at individual tournament of 4th [[International Shogi Forum]] ([[Tendō, Yamagata|Tendō]]).&lt;ref&gt;[http://shogi.by/tournaments/33/ 4th International Shogi Forum]{{ref-ru}}&lt;/ref&gt;
* 2009: 2nd place at British Open Shogi Championship.&lt;ref&gt;[http://www.shogi.net/fesa/index.php?mid=5&amp;player=Alan+Baker Alan Baker: FESA profile]&lt;/ref&gt;
* 2014: 1st place at group B of individual tournament of 6th International Shogi Forum ([[Shizuoka, Shizuoka|Shizuoka]])&lt;ref&gt;[http://www.shogi.or.jp/topics/event/2014/12/_in_9.html 6th International Shogi Forum, Shizuoka]&lt;/ref&gt;

== References ==
{{Reflist}}

{{DEFAULTSORT:Baker, Alan}}
[[Category:21st-century American philosophers]]
[[Category:Philosophers from Pennsylvania]]
[[Category:American shogi players]]
[[Category:Living people]]
[[Category:Year of birth missing (living people)]]
[[Category:Philosophers of mathematics]]
[[Category:Philosophers of science]]
[[Category:Philosophical realism]]
[[Category:Swarthmore College faculty]]


{{US-philosopher-stub}}</text>
      <sha1>bpd2d7mosbkad5be6gyqd1eta40vzjl</sha1>
    </revision>
  </page>
  <page>
    <title>Alvis–Curtis duality</title>
    <ns>0</ns>
    <id>32376723</id>
    <revision>
      <id>745448933</id>
      <parentid>637269306</parentid>
      <timestamp>2016-10-21T05:54:11Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor/>
      <comment>/* References */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4392">In [[mathematics]], '''Alvis–Curtis duality''' is a [[Duality (mathematics)|duality operation]] on the [[Character (mathematics)|characters]] of a [[reductive group]] over a [[finite field]], introduced by {{harvs|last=Curtis|first=Charles W.|authorlink=Charles W. Curtis|txt|year=1980}} and studied by his student {{harvs|last=Alvis|first=Dean|txt|year=1979}}. {{harvs|txt|last=Kawanaka|year1=1981|year2=1982}} introduced a similar duality operation for Lie algebras.

Alvis–Curtis duality has order 2 and is an isometry on generalized characters.

{{harvtxt|Carter|1985|loc=8.2}} discusses Alvis–Curtis duality in detail.

==Definition==

The dual ζ* of a character ζ of a finite group ''G'' with a split [[BN-pair]] is defined to be
:&lt;math&gt;\zeta^*=\sum_{J\subseteq R}(-1)^J\zeta^G_{P_J}&lt;/math&gt;
Here the sum is over all subsets ''J'' of the set ''R'' of simple roots of the Coxeter system of ''G''. The character ζ{{su|p=|b=''P''&lt;sub&gt;''J''&lt;/sub&gt;}} is the '''truncation''' of ζ to the parabolic subgroup ''P''&lt;sub&gt;''J''&lt;/sub&gt; of the subset ''J'', given by restricting ζ to ''P''&lt;sub&gt;''J''&lt;/sub&gt; and then taking the space of invariants of the unipotent radical of ''P''&lt;sub&gt;''J''&lt;/sub&gt;, and  ζ{{su|p=''G''|b=''P''&lt;sub&gt;''J''&lt;/sub&gt;}} is the induced representation of ''G''. (The operation of  truncation is the adjoint functor of [[parabolic induction]].)

==Examples==

*The dual of the trivial character 1 is the [[Steinberg character]].
*{{harvtxt|Deligne|Lusztig|1983}} showed that the dual of a [[Deligne–Lusztig character]] ''R''{{su|b=T|p=θ}} is ε&lt;sub&gt;''G''&lt;/sub&gt;ε&lt;sub&gt;''T''&lt;/sub&gt;''R''{{su|b=T|p=θ}}.
*The dual of a [[cuspidal character]] χ is (–1)&lt;sup&gt;|Δ|&lt;/sup&gt;χ, where Δ is the set of simple roots.
*The dual of the [[Gelfand–Graev character]] is the character taking value |''Z''&lt;sup&gt;''F''&lt;/sup&gt;|''q''&lt;sup&gt;''l''&lt;/sup&gt; on the regular unipotent elements and vanishing elsewhere.

==References==

*{{Citation | last1=Alvis | first1=Dean | title=The duality operation in the character ring of a finite Chevalley group | doi=10.1090/S0273-0979-1979-14690-1 | mr=546315 | year=1979 | journal=American Mathematical Society. Bulletin. New Series | issn=0002-9904 | volume=1 | issue=6 | pages=907–911}}
*{{Citation | last1=Carter | first1=Roger W. | author1-link=Roger Carter (mathematician) | title=Finite groups of Lie type. Conjugacy classes and complex characters.  | url=https://books.google.com/books?id=LvvuAAAAMAAJ | publisher=[[John Wiley &amp; Sons]] | location=New York | series=Pure and Applied Mathematics (New York) | isbn=978-0-471-90554-7 | mr=794307 | year=1985}}
*{{Citation | last1=Curtis | first1=Charles W. | authorlink = Charles W. Curtis | title=Truncation and duality in the character ring of a finite group of Lie type | doi=10.1016/0021-8693(80)90185-4 | mr=563231 | year=1980 | journal=[[Journal of Algebra]] | issn=0021-8693 | volume=62 | issue=2 | pages=320–332}}
*{{Citation | last1=Deligne | first1=Pierre | author1-link=Pierre Deligne | last2=Lusztig | first2=George | title=Duality for representations of a reductive group over a finite field | doi=10.1016/0021-8693(82)90023-0 | mr=644236  | year=1982 | journal=[[Journal of Algebra]] | issn=0021-8693 | volume=74 | issue=1 | pages=284–291}}
*{{Citation | last1=Deligne | first1=Pierre | author1-link=Pierre Deligne | last2=Lusztig | first2=George | title=Duality for representations of a reductive group over a finite field. II | doi=10.1016/0021-8693(83)90202-8 | mr=700298  | year=1983 | journal=[[Journal of Algebra]] | issn=0021-8693 | volume=81 | issue=2 | pages=540–545}}
*{{Citation | last1=Kawanaka | first1=Noriaki | title=Fourier transforms of nilpotently supported invariant functions on a finite simple Lie algebra | url=http://projecteuclid.org/getRecord?id=euclid.pja/1195516260 | mr=637555 | year=1981 | journal=Japan Academy. Proceedings. Series A. Mathematical Sciences | issn=0386-2194 | volume=57 | issue=9 | pages=461–464 | doi=10.3792/pjaa.57.461}}
*{{Citation | last1=Kawanaka | first1=N. | title=Fourier transforms of nilpotently supported invariant functions on a simple Lie algebra over a finite field | doi=10.1007/BF01389363 | mr=679766 | year=1982 | journal=[[Inventiones Mathematicae]] | issn=0020-9910 | volume=69 | issue=3 | pages=411–435}}

{{DEFAULTSORT:Alvis-Curtis duality}}
[[Category:Representation theory]]
[[Category:Duality theories]]</text>
      <sha1>fzjjdgzc11privwyfuvlkmspa3ebz44</sha1>
    </revision>
  </page>
  <page>
    <title>Calabi conjecture</title>
    <ns>0</ns>
    <id>5263012</id>
    <revision>
      <id>869272793</id>
      <parentid>869163347</parentid>
      <timestamp>2018-11-17T15:23:38Z</timestamp>
      <contributor>
        <username>Hassanjolany1984</username>
        <id>35057713</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11319">In mathematics, the '''Calabi conjecture''' was a conjecture about the existence of certain "nice" [[Riemannian metric]]s on certain [[complex manifold]]s, made by {{harvs |txt |authorlink=Eugenio Calabi |first=Eugenio |last=Calabi |year =1954 |year2=1957}} and proved by {{harvs |txt |authorlink=Shing-Tung Yau |first=Shing-Tung |last= Yau |year1=1977 |year2= 1978}}. Yau received the [[Fields Medal]] in 1982 in part for this proof.

The Calabi conjecture states that a [[compact space|compact]] [[Kähler manifold]] has a unique Kähler metric in the same class whose [[Ricci form]] is any given 2-form representing the first [[Chern class]]. In particular if the first Chern class vanishes there is a unique Kähler metric in the same class with vanishing [[Ricci curvature]]; these are called [[Calabi&amp;ndash;Yau manifold]]s.

More formally, the Calabi conjecture states:
:If ''M'' is a [[compact space|compact]] [[Kähler manifold]] with Kähler metric &lt;math&gt;g&lt;/math&gt; and Kähler form &lt;math&gt;\omega&lt;/math&gt;, and ''R'' is any [[differential form|(1,1)-form]] representing the manifold's first [[Chern class]], then there exists a unique Kähler metric &lt;math&gt;\tilde{g}&lt;/math&gt; on ''M'' with Kähler form &lt;math&gt;\tilde{\omega}&lt;/math&gt; such that &lt;math&gt;\omega&lt;/math&gt; and &lt;math&gt;\tilde{\omega}&lt;/math&gt; represent the same class in [[cohomology]] H&lt;sup&gt;2&lt;/sup&gt;(''M'','''R''') and the [[Ricci form]] of &lt;math&gt;\tilde{\omega}&lt;/math&gt; is ''R''.

The Calabi conjecture is closely related to the question of which Kähler manifolds have [[Kähler&amp;ndash;Einstein metric]]s.

==Kähler&amp;ndash;Einstein metrics==
A  conjecture closely related to the Calabi conjecture states that if a  compact Kähler variety has a negative, zero,  or positive first Chern class then it has a [[Kähler&amp;ndash;Einstein metric]] in the same class as its Kähler metric, unique up to rescaling.  
This was proved for negative first Chern classes independently by [[Thierry Aubin]] and [[Shing-Tung Yau]] in 1976. When the Chern class is zero it was proved by Yau as an easy consequence of the Calabi conjecture.

It was  disproved for positive first Chern classes by Yau, who observed that the [[complex projective plane]] blown up at 2 points has no Kähler&amp;ndash;Einstein metric and so is a counterexample. Also even when Kähler&amp;ndash;Einstein metric exists it need not be unique.  There has been a lot of further work on the positive first Chern class case. A necessary condition for the existence of a Kähler&amp;ndash;Einstein metric is that the Lie algebra of [[Holomorphic function|holomorphic]] vector fields is reductive.  Yau conjectured that when the first Chern class is positive, a Kähler variety has a [[Kähler&amp;ndash;Einstein metric]] if and only if it is stable in the sense of [[geometric invariant theory]].

The case of complex surfaces has been settled by [[Gang Tian]]. The complex surfaces with positive Chern class are either a product of two copies of a projective line (which obviously has a Kähler&amp;ndash;Einstein metric) or a blowup of the projective plane in at most 8 points in "general position", in the sense that no 3 lie on a line and no 6 lie on a quadric.  The projective plane has a Kähler&amp;ndash;Einstein metric, and the projective plane blown up in 1 or 2 points does not, as the Lie algebra of holomorphic vector fields is not reductive.
Tian showed that the projective plane blown up in 3, 4, 5, 6, 7, or 8 points in general position has a  Kähler&amp;ndash;Einstein metric.

==Outline of the proof of the Calabi conjecture==

Calabi transformed the Calabi conjecture  into  a non&amp;ndash;linear partial differential equation of complex [[Monge–Ampère equation|Monge&amp;ndash;Ampere]] type, and showed that this equation has at most one solution, thus establishing the uniqueness of the required Kähler metric.

Yau proved the Calabi conjecture by constructing a solution of  this equation using the [[continuity method]]. This involves first solving an easier equation, and then showing that a solution to the easy equation can be continuously deformed to a solution of the hard equation. The hardest part of Yau's solution is proving certain [[a priori estimate]]s for the derivatives of solutions.

===Transformation of the Calabi conjecture to a differential equation===

Suppose that ''M'' is a complex compact manifold with a Kahler form ω.
Any other Kahler form in the same class is of the form
:&lt;math&gt;\omega+dd'\phi&lt;/math&gt;
for some smooth function φ on ''M'', unique up to addition of a constant. The Calabi conjecture is therefore equivalent to the following problem:

:Let ''F''=''e''&lt;sup&gt;''f''&lt;/sup&gt; be a positive smooth function on ''M'' with average value 1. Then there is a smooth real function &amp;phi; with
::&lt;math&gt;(\omega+dd'\phi)^m = e^f\omega^m&lt;/math&gt;
:and &amp;phi; is unique up to addition of a constant.

This is an equation of complex Monge&amp;ndash;Ampere type for a single function φ.
It is a particularly hard partial differential equation to solve, as it is non-linear in the terms of highest order. 
It is trivial to solve it when ''f''=0, as φ=0 is a solution. The idea of the continuity method is to show that it can be solved for all ''f'' by showing that the set of ''f'' for which it can be solved is both open and closed. Since the set of ''f'' for which it can be solved is non-empty, and the set of all ''f'' is connected, this shows that it can be solved for all ''f''.

The map from smooth functions to smooth functions taking φ to  ''F'' defined by 
::&lt;math&gt;F=(\omega+dd'\phi)^m/\omega^m&lt;/math&gt;
is neither injective nor surjective. It is not injective because adding a constant to φ does not change ''F'', and it is not surjective 
because ''F'' must be positive and have average value 1. So we consider the map restricted to functions φ that are normalized to have average value 0, and ask if this map is an isomorphism onto the set of positive ''F''=''e''&lt;sup&gt;''f''&lt;/sup&gt; with average value 1. Calabi and Yau proved that it is indeed an isomorphism. This is done in several steps, described below.

===Uniqueness of the solution===

Proving that the solution is unique involves showing that if 
:&lt;math&gt;(\omega+dd'\varphi_1)^m = (\omega+dd'\varphi_2)^m&lt;/math&gt;
then φ&lt;sub&gt;1&lt;/sub&gt; and φ&lt;sub&gt;2&lt;/sub&gt; differ by a constant
(so must be the same if they are both normalized to have average value 0). 
Calabi proved this by showing that the average value of
:&lt;math&gt;|d(\varphi_1-\varphi_2)|^2&lt;/math&gt;
is given by an expression that is at most 0. As it is obviously at least 0, it must be 0, so 
:&lt;math&gt;d(\varphi_1-\varphi_2) = 0&lt;/math&gt;
which in turn forces φ&lt;sub&gt;1&lt;/sub&gt; and φ&lt;sub&gt;2&lt;/sub&gt; to differ by a constant.

===The set of ''F'' is open===

Proving that the set of possible ''F'' is open (in the set of smooth functions with average value 1) involves showing that if it is possible to solve the equation for some ''F'', then it is possible to solve it for all sufficiently close ''F''. Calabi proved this by using the [[implicit function theorem]] for [[Banach space]]s: in order to apply this, the main step is to show that the ''linearization'' of the differential operator above is invertible.

===The set of ''F'' is closed===
This is the hardest part of the proof, and was the part done by Yau.
Suppose that ''F'' is in the closure of the image of possible
functions φ. This means that there is a sequence of 
functions φ&lt;sub&gt;1&lt;/sub&gt;, φ&lt;sub&gt;2&lt;/sub&gt;, ...
such that the corresponding functions ''F''&lt;sub&gt;1&lt;/sub&gt;, ''F''&lt;sub&gt;2&lt;/sub&gt;,...
converge to ''F'', and the problem is to show that some subsequence of the φs converges to a solution φ. In order to do this, Yau finds some [[a priori bound]]s for the functions φ&lt;sub&gt;''i''&lt;/sub&gt; and their higher derivatives
in terms of the higher derivatives of log(''f''&lt;sub&gt;''i''&lt;/sub&gt;). Finding these bounds requires a long sequence of hard estimates, each improving slightly on the previous estimate. The bounds Yau gets are enough to show that the functions φ&lt;sub&gt;''i''&lt;/sub&gt; all lie in a compact subset of a suitable Banach space of functions, so it is possible to find a convergent subsequence.
This subsequence converges to a function φ with image ''F'', which 
shows that the set of possible images ''F'' is closed.

==References==
*T. Aubin, ''Nonlinear Analysis on Manifolds, Monge&amp;ndash;Ampère Equations'' {{ISBN|0-387-90704-1}} This gives a proof of the Calabi conjecture and of Aubin's results on Kaehler&amp;ndash;Einstein metrics.
*{{Citation | last1=Bourguignon | first1=Jean-Pierre | title=Séminaire Bourbaki, 30e année (1977/78) | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Lecture Notes in Math. | doi=10.1007/BFb0069970 | year=1979 | volume=710 | chapter=Premières formes de Chern des variétés kählériennes compactes [d'après E. Calabi, T. Aubin et S. T. Yau] | pages=1–21 | mr=554212 | isbn=978-3-540-09243-8}} This gives a survey of the work of Aubin and Yau.
*{{Citation | last1=Calabi | first1=Eugenio | chapter=The space of Kähler metrics | title=Proc. Internat. Congress Math. Amsterdam | volume=2 | year=1954 | pages=206&amp;ndash;207 | url=http://mathunion.org/ICM/ICM1954.2/ | chapter-url=http://mathunion.org/ICM/ICM1954.2/Main/icm1954.2.0206.0207.ocr.pdf | access-date=2011-01-30 | archive-url=https://web.archive.org/web/20110717144747/http://mathunion.org/ICM/ICM1954.2/# | archive-date=2011-07-17 | dead-url=yes | df= }}
*{{Citation | last1=Calabi | first1=Eugenio | editor1-last=Fox | editor1-first=Ralph H. | editor2-last=Spencer | editor2-first=D. C. | editor3-last=Tucker | editor3-first=A. W. | title=Algebraic geometry and topology. A symposium in honor of S. Lefschetz | url=https://books.google.com/books?id=n_ZQAAAAMAAJ | publisher=[[Princeton University Press]] | series=Princeton Mathematical Series | mr=0085583 | year=1957 | volume=12 | chapter=On Kähler manifolds with vanishing canonical class | pages=78–89}}
*Dominic D. Joyce ''Compact Manifolds with Special Holonomy'' (Oxford Mathematical Monographs) {{ISBN|0-19-850601-5}} This gives a simplified proof of the Calabi conjecture.
*G. Tian,  [http://www.springerlink.com/content/k6w204w55607k5t2/ ''On Calabi's conjecture for complex surfaces with positive first Chern class.''] ''Invent. Math.'' 101 (1990), no. 1, 101&amp;ndash;172.
*{{Citation | last1=Yau | first1=Shing Tung | title=Calabi's conjecture and some new results in algebraic geometry | url=http://www.pnas.org/content/74/5/1798 | year=1977 | journal=[[Proceedings of the National Academy of Sciences|Proceedings of the National Academy of Sciences of the United States of America]] | issn=0027-8424 | volume=74 | issue=5 | pages=1798–1799 | mr=0451180 | doi=10.1073/pnas.74.5.1798| pmc=431004 }}
*{{Citation | last1=Yau | first1=Shing Tung | title=On the Ricci curvature of a compact Kähler manifold and the complex Monge-Ampère equation. I | doi=10.1002/cpa.3160310304 | mr=480350 | year=1978 | journal=[[Communications on Pure and Applied Mathematics]] | volume=31 | issue=3 | pages=339–411}}

==External links==
*{{citation |last=Yau |first=S. T. |url=http://www.scholarpedia.org/article/Calabi-Yau_manifold |title=Calabi-Yau manifold |publisher=Scholarpedia |doi=10.4249/scholarpedia.6524 |year=2009 |journal=Scholarpedia |volume=4 |issue=8 |pages=6524}}

[[Category:Complex manifolds]]
[[Category:Theorems in differential geometry]]</text>
      <sha1>j171p5fhembug3jb6c1jkgo5xvq5egv</sha1>
    </revision>
  </page>
  <page>
    <title>Central limit theorem</title>
    <ns>0</ns>
    <id>39406</id>
    <revision>
      <id>870766994</id>
      <parentid>870123124</parentid>
      <timestamp>2018-11-26T21:48:06Z</timestamp>
      <contributor>
        <ip>128.61.182.71</ip>
      </contributor>
      <comment>/* Classical CLT */ Add missing period: i.i.d -&gt; i.i.d.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="65015">In [[probability theory]], the '''central limit theorem''' ('''CLT''') establishes that, in some situations, when [[Statistical independence|independent random variables]] are added, their properly normalized sum tends toward a [[normal distribution]] (informally a "''bell curve''") even if the original variables themselves are not normally distributed.  The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions.

For example, suppose that a [[Sample (statistics)|sample]] is obtained containing a large number of [[Random variate|observations]], each observation being randomly generated in a way that does not depend on the values of the other observations, and that the [[arithmetic mean]] of the observed values is computed. If this procedure is performed many times, the central limit theorem says that the [[Probability distribution|distribution]] of the average will be closely approximated by a [[normal distribution]].  A simple example of this is that if one [[Coin flipping|flips a coin many times]] the probability of getting a given number of heads in a series of flips will approach a normal curve, with mean equal to half the total number of flips in each series. (In the limit of an infinite number of flips, it will equal a normal curve.)

The central limit theorem has a number of variants. In its common form, the random variables must be [[identically distributed]]. In variants, convergence of the mean to the normal distribution also occurs for non-identical distributions or for non-independent observations, given that they comply with certain conditions.

The earliest version of this theorem, that the [[normal distribution]] may be used as an approximation to the [[binomial distribution]], is now known as the [[de Moivre–Laplace theorem]].

In more general usage, a central limit theorem is any of a set of [[Weak convergence of measures|weak-convergence]] theorems in probability theory. They all express the fact that a sum of many [[Independent and identically distributed random variables|independent and identically distributed]] (i.i.d.) random variables, or alternatively, random variables with specific types of dependence, will tend to be distributed according to one of a small set of ''[[attractor]] distributions''. When the variance of the i.i.d. variables is finite, the attractor distribution is the normal distribution. In contrast, the sum of a number of [[Independent and identically distributed random variables|i.i.d. random variables]] with [[power law]] tail distributions decreasing as {{math|{{abs|''x''}}&lt;sup&gt;−''α'' − 1&lt;/sup&gt;}} where {{math|0 &lt; ''α'' &lt; 2}} (and therefore having infinite variance) will tend to an alpha-[[stable distribution]] with stability parameter (or index of stability) of {{mvar|α}} as the number of variables grows.&lt;ref&gt;{{Cite book |first1=Johannes |last1=Voit |page=124 |year=2003|title=The Statistical Mechanics of Financial Markets|publisher=Springer-Verlag|isbn=3-540-00978-7}}&lt;/ref&gt;

==Independent sequences==
[[File:Central limit thm.png|400px|thumb|right|A distribution being "smoothed out" by [[summation]], showing original [[Probability density function|density of distribution]] and three subsequent summations; see [[Illustration of the central limit theorem]] for further details.]]
[[File:IllustrationCentralTheorem.png|400px|thumb|right|Whatever the form of the population distribution, the sampling distribution tends to a Gaussian, and its dispersion is given by the Central Limit Theorem.&lt;ref&gt;{{cite book |last=Rouaud |first=Mathieu |title=Probability, Statistics and Estimation|year=2013 |page=10 |url=http://www.incertitudes.fr/book.pdf }}&lt;/ref&gt;]]

===Classical CLT===
Let {{math|{''X''&lt;sub&gt;1&lt;/sub&gt;, …, ''X&lt;sub&gt;n&lt;/sub&gt;''}}} be a [[random sample]] of size {{mvar|n}} — that is, a sequence of [[independent and identically distributed]] (i.i.d.) random variables drawn from a distribution of [[expected value]] given by {{mvar|µ}} and finite [[variance]] given by {{math|''σ''&lt;sup&gt;2&lt;/sup&gt;}}. Suppose we are interested in the [[sample mean|sample average]]

:&lt;math&gt;S_n := \frac{X_1+\cdots+X_n}{n}&lt;/math&gt;

of these random variables. By the [[law of large numbers]], the sample averages [[Convergence in probability|converge in probability]] and [[Almost sure convergence|almost surely]] to the expected value {{mvar|µ}} as {{math|''n'' → ∞}}. The classical central limit theorem describes the size and the distributional form of the stochastic fluctuations around the deterministic number {{mvar|µ}} during this convergence. More precisely, it states that as {{mvar|n}} gets larger, the distribution of the difference between the sample average {{mvar|S&lt;sub&gt;n&lt;/sub&gt;}} and its limit {{mvar|µ}}, when multiplied by the factor {{math|{{sqrt|''n''}}}} (that is {{math|{{sqrt|''n''}}(''S&lt;sub&gt;n&lt;/sub&gt;'' − ''µ'')}}), approximates the [[normal distribution]] with mean 0 and variance {{math|''σ''&lt;sup&gt;2&lt;/sup&gt;}}. For large enough {{mvar|n}}, the distribution of {{mvar|S&lt;sub&gt;n&lt;/sub&gt;}} is close to the normal distribution with mean {{mvar|µ}} and variance {{math|''σ''&lt;sup&gt;2&lt;/sup&gt;}}/{{math|''n''}}. The usefulness of the theorem is that the distribution of {{math|{{sqrt|''n''}}(''S&lt;sub&gt;n&lt;/sub&gt;'' − ''µ'')}} approaches normality regardless of the shape of the distribution of the individual {{mvar|X&lt;sub&gt;i&lt;/sub&gt;}}. Formally, the theorem can be stated as follows:

&lt;blockquote&gt;'''Lindeberg–Lévy CLT.''' Suppose {{math|{''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;, …}}} is a sequence of [[independent and identically distributed|i.i.d.]] random variables with {{math|E[''X&lt;sub&gt;i&lt;/sub&gt;''] {{=}} ''µ''}} and {{math|Var[''X&lt;sub&gt;i&lt;/sub&gt;''] {{=}} ''σ''&lt;sup&gt;2&lt;/sup&gt; &lt; ∞}}. Then as {{mvar|n}} approaches infinity, the random variables {{math|{{sqrt|''n''}}(''S&lt;sub&gt;n&lt;/sub&gt;'' − ''µ'')}} [[convergence in distribution|converge in distribution]] to a [[normal distribution|normal]] {{math|''N''(0,''σ''&lt;sup&gt;2&lt;/sup&gt;)}}:&lt;ref&gt;Billingsley (1995, p.&amp;nbsp;357)&lt;/ref&gt;

:&lt;math&gt;\sqrt{n}\left(S_n - \mu\right)\ \xrightarrow{d}\ N\left(0,\sigma^2\right).&lt;/math&gt;&lt;/blockquote&gt;

In the case {{math|''σ'' &gt; 0}}, convergence in distribution means that the [[cumulative distribution function]]s of {{math|{{sqrt|''n''}}(''S&lt;sub&gt;n&lt;/sub&gt;'' − ''µ'')}} converge pointwise to the cdf of the {{math|''N''(0, ''σ''&lt;sup&gt;2&lt;/sup&gt;)}} distribution: for every real number&amp;nbsp;{{mvar|z}},

:&lt;math&gt;\lim_{n\to\infty} \Pr\left[\sqrt{n}(S_n-\mu) \le z\right] = \Phi\left(\frac{z}{\sigma}\right) ,&lt;/math&gt;

where {{math|Φ(''x'')}} is the standard normal cdf evaluated at&amp;nbsp;{{mvar|x}}. Note that the convergence is uniform in {{mvar|z}} in the sense that

:&lt;math&gt;\lim_{n\to\infty}\sup_{z\in\R}\left|\Pr\left[\sqrt{n}(S_n-\mu) \le z\right] - \Phi\left(\frac{z}{\sigma}\right)\right| = 0,&lt;/math&gt;

where sup denotes the least upper bound (or [[supremum]]) of the set.&lt;ref&gt;Bauer (2001, Theorem 30.13, p.199)&lt;/ref&gt;

===Lyapunov CLT===
The theorem is named after Russian mathematician [[Aleksandr Lyapunov]]. In this variant of the central limit theorem the random variables {{mvar|X&lt;sub&gt;i&lt;/sub&gt;}} have to be independent, but not necessarily identically distributed. The theorem also requires that random variables {{math|{{abs|''X&lt;sub&gt;i&lt;/sub&gt;''}}}} have [[moment (mathematics)|moment]]s of some order {{math|(2 + ''δ'')}}, and that the rate of growth of these moments is limited by the Lyapunov condition given below.

&lt;blockquote&gt;'''Lyapunov CLT.'''&lt;ref&gt;Billingsley (1995, p.362)&lt;/ref&gt; Suppose {{math|{''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;, …}}} is a sequence of independent random variables, each with finite expected value {{mvar|μ&lt;sub&gt;i&lt;/sub&gt;}} and variance {{math|''σ''{{su|b=''i''|p=2}}}}. Define

:&lt;math&gt;s_n^2 = \sum_{i=1}^n \sigma_i^2&lt;/math&gt;

If for some {{math|''δ'' &gt; 0}}, ''Lyapunov’s condition''

: &lt;math&gt;\lim_{n\to\infty} \frac{1}{s_{n}^{2+\delta}} \sum_{i=1}^{n} \operatorname{E}\left[|X_{i} - \mu_{i}|^{2+\delta}\right] = 0&lt;/math&gt;

is satisfied, then a sum of {{math|{{sfrac|''X&lt;sub&gt;i&lt;/sub&gt;'' − ''μ&lt;sub&gt;i&lt;/sub&gt;''|''s&lt;sub&gt;n&lt;/sub&gt;''}}}} converges in distribution to a standard normal random variable, as {{mvar|n}} goes to infinity:

: &lt;math&gt;\frac{1}{s_n} \sum_{i=1}^{n} \left(X_i - \mu_i\right) \ \xrightarrow{d}\ N(0,1).&lt;/math&gt;&lt;/blockquote&gt;

In practice it is usually easiest to check Lyapunov's condition for {{math|''δ'' {{=}} 1}}.

If a sequence of random variables satisfies Lyapunov's condition, then it also satisfies Lindeberg's condition. The converse implication, however, does not hold.

===Lindeberg CLT===
{{Main|Lindeberg's condition}}

In the same setting and with the same notation as above, the Lyapunov condition can be replaced with the following weaker one (from [[Jarl Waldemar Lindeberg|Lindeberg]] in 1920).

Suppose that for every {{math|''ε'' &gt; 0}}

:&lt;math&gt;  \lim_{n \to \infty} \frac{1}{s_n^2}\sum_{i = 1}^{n} \operatorname{E}\left[(X_i - \mu_i)^2 \cdot \mathbf{1}_{\{ | X_i - \mu_i | &gt; \varepsilon s_n \}}  \right] = 0&lt;/math&gt;

where {{math|'''1'''&lt;sub&gt;{…}&lt;/sub&gt;}} is the [[indicator function]]. Then the distribution of the standardized sums

:&lt;math&gt;\frac{1}{s_n}\sum_{i = 1}^n \left( X_i - \mu_i \right)&lt;/math&gt;

converges towards the standard normal distribution {{math|''N''(0,1)}}.

===Multidimensional CLT===
Proofs that use characteristic functions can be extended to cases where each individual {{math|X&lt;sub&gt;i&lt;/sub&gt;}} is a [[random vector]] in {{math|'''ℝ'''&lt;sup&gt;''k''&lt;/sup&gt;}}, with mean vector {{math|'''μ''' {{=}} E(''X&lt;sub&gt;i&lt;/sub&gt;'')}} and [[covariance matrix]] {{math|'''Σ'''}} (among the components of the vector), and these random vectors are independent and identically distributed. Summation of these vectors is being done componentwise. The multidimensional central limit theorem  states that when scaled, sums converge to a [[multivariate normal distribution]].&lt;ref&gt;{{Cite book|last = Van der Vaart|first = A. W.|title = Asymptotic statistics|year = 1998| publisher = Cambridge University Press  | location = New York  | isbn = 978-0-521-49603-2|lccn = 98015176| ref = CITEREFvan_der_Vaart1998}}&lt;/ref&gt;

Let

:&lt;math&gt;\mathbf{X}_i=\begin{bmatrix} X_{i(1)} \\ \vdots \\ X_{i(k)} \end{bmatrix}&lt;/math&gt;

be the {{mvar|k}}-vector. The bold in {{math|'''X'''&lt;sub&gt;''i''&lt;/sub&gt;}} means that it is a random vector, not a random (univariate) variable. Then the [[summation|sum]] of the random vectors will be

:&lt;math&gt;\begin{bmatrix} X_{1(1)} \\ \vdots \\ X_{1(k)} \end{bmatrix}+\begin{bmatrix} X_{2(1)} \\ \vdots \\ X_{2(k)} \end{bmatrix}+\cdots+\begin{bmatrix} X_{n(1)} \\ \vdots \\ X_{n(k)} \end{bmatrix} = \begin{bmatrix} \sum_{i=1}^{n} \left [ X_{i(1)} \right ] \\ \vdots \\ \sum_{i=1}^{n} \left [ X_{i(k)} \right ] \end{bmatrix} = \sum_{i=1}^{n} \mathbf{X}_i&lt;/math&gt;

and the average is

:&lt;math&gt; \frac{1}{n} \sum_{i=1}^{n} \mathbf{X}_i= \frac{1}{n}\begin{bmatrix} \sum_{i=1}^{n} X_{i(1)} \\ \vdots \\ \sum_{i=1}^{n} X_{i(k)} \end{bmatrix} = \begin{bmatrix} \bar X_{i(1)} \\ \vdots \\ \bar X_{i(k)} \end{bmatrix}=\mathbf{\bar X_n}&lt;/math&gt;

and therefore

:&lt;math&gt;\frac{1}{\sqrt{n}} \sum_{i=1}^{n} \left [\mathbf{X}_i - \operatorname{E}\left ( X_i\right ) \right ]=\frac{1}{\sqrt{n}}\sum_{i=1}^{n} ( \mathbf{X}_i - \boldsymbol\mu ) = \sqrt{n}\left(\overline{\mathbf{X}}_n - \boldsymbol\mu\right). &lt;/math&gt;

The multivariate central limit theorem states that

:&lt;math&gt;\sqrt{n}\left(\overline{\mathbf{X}}_n - \boldsymbol\mu\right)\ \stackrel{D}{\rightarrow}\ N_k(0,\boldsymbol\Sigma)&lt;/math&gt;

where the [[covariance matrix]] {{math|'''Σ'''}} is equal to

:&lt;math&gt; \boldsymbol\Sigma=\begin{bmatrix}
{\operatorname{Var} \left (X_{1(1)} \right)} &amp; \operatorname{Cov} \left (X_{1(1)},X_{1(2)} \right) &amp; \operatorname{Cov} \left (X_{1(1)},X_{1(3)} \right) &amp; \cdots &amp; \operatorname{Cov} \left (X_{1(1)},X_{1(k)} \right) \\
\operatorname{Cov} \left (X_{1(2)},X_{1(1)} \right) &amp; \operatorname{Var} \left (X_{1(2)} \right) &amp; \operatorname{Cov} \left(X_{1(2)},X_{1(3)} \right) &amp; \cdots &amp; \operatorname{Cov} \left(X_{1(2)},X_{1(k)} \right) \\
\operatorname{Cov}\left (X_{1(3)},X_{1(1)} \right) &amp; \operatorname{Cov} \left (X_{1(3)},X_{1(2)} \right) &amp; \operatorname{Var} \left (X_{1(3)} \right) &amp; \cdots &amp; \operatorname{Cov} \left (X_{1(3)},X_{1(k)} \right) \\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\operatorname{Cov} \left (X_{1(k)},X_{1(1)} \right) &amp; \operatorname{Cov} \left (X_{1(k)},X_{1(2)} \right) &amp; \operatorname{Cov} \left (X_{1(k)},X_{1(3)} \right) &amp; \cdots &amp; \operatorname{Var} \left (X_{1(k)} \right) \\
\end{bmatrix}.&lt;/math&gt;

The rate of convergence is given by the following [[Berry–Esseen theorem|Berry–Esseen]] type result:

&lt;blockquote&gt;'''Theorem.'''&lt;ref&gt;Ryan O’Donnell (2014, Theorem 5.38) http://www.contrib.andrew.cmu.edu/~ryanod/?p=866&lt;/ref&gt; Let &lt;math&gt;X_1,\dots, X_n&lt;/math&gt; be independent &lt;math&gt;R^d&lt;/math&gt;-valued random vectors, each having mean zero. Write &lt;math&gt;S =\sum^n_{i=1}X_i&lt;/math&gt; and assume &lt;math&gt;\Sigma = \operatorname{Cov}[S]&lt;/math&gt; is invertible. Let &lt;math&gt;Z \sim N(0,\Sigma)&lt;/math&gt; be a &lt;math&gt;d&lt;/math&gt;-dimensional Gaussian with the same mean and covariance matrix as &lt;math&gt;S&lt;/math&gt;. Then for all convex sets &lt;math&gt;U \subseteq R^d&lt;/math&gt;,

:&lt;math&gt;|\Pr[S \in U]-\Pr[Z \in U]| \le C d^{1/4} \gamma,&lt;/math&gt;

where &lt;math&gt;C&lt;/math&gt; is a universal constant, &lt;math&gt;\gamma = \sum^n_{i=1} \operatorname{E} [\| \Sigma^{-1/2}X_i\|^3_2]&lt;/math&gt;, and &lt;math&gt;\|\cdot\|_2&lt;/math&gt; denotes the Euclidean norm on &lt;math&gt;R^d&lt;/math&gt;.
&lt;/blockquote&gt;

It is unknown whether the factor &lt;math&gt;d^{1/4}&lt;/math&gt; is necessary.&lt;ref&gt;{{cite journal |first=V. |last=Bentkus |title=A Lyapunov-type Bound in &lt;math&gt;\mathbb{R}^d&lt;/math&gt; |journal=Theory Probab. Appl. |volume=49 |year=2005 |issue=2 |pages=311–323 |doi=10.1137/S0040585X97981123 }}&lt;/ref&gt;

===Generalized theorem===
{{Main|Stable distribution#A generalized central limit theorem}}

The central limit theorem states that the sum of a number of independent and identically distributed random variables with finite variances will tend to a [[normal distribution]] as the number of variables grows. A generalization due to [[Boris Vladimirovich Gnedenko|Gnedenko]] and [[Andrey Nikolaevich Kolmogorov|Kolmogorov]] states that the sum of a number of random variables with a power-law tail ([[Pareto distribution|Paretian tail]]) distributions decreasing as {{math|{{abs|''x''}}&lt;sup&gt;−''α'' − 1&lt;/sup&gt;}} where {{math|0 &lt; ''α'' &lt; 2}} (and therefore having infinite variance) will tend to a stable distribution {{math|''f''(''x'';''α'',0,''c'',0)}} as the number of summands grows.&lt;ref name=Voit2003a&gt;{{cite book|first=Johannes |last=Voit|year=2003 |title=The Statistical Mechanics of Financial Markets |series=Texts and Monographs in Physics |publisher=Springer-Verlag |isbn=3-540-00978-7 |chapter=Section 5.4.3 |chapterurl=https://books.google.com/books?id=6zUlh_TkWSwC }}&lt;/ref&gt;&lt;ref&gt;{{cite book |first=B. V. |last=Gnedenko |first2=A. N. |last2=Kolmogorov |title=Limit distributions for sums of independent random variables |location=Cambridge |publisher=Addison-Wesley |year=1954 |url=https://books.google.com/books/about/Limit_distributions_for_sums_of_independ.html?id=rYsZAQAAIAAuJ }}&lt;/ref&gt; If {{math|''α'' &gt; 2}} then the sum converges to a [[stable distribution]] with stability parameter equal to 2, i.e. a Gaussian distribution.&lt;ref name=Uchaikin&gt;{{cite book |first=Vladimir V. |last=Uchaikin |first2=V. M. |last2=Zolotarev |year=1999 |title=Chance and stability: stable distributions and their applications |location= |publisher=VSP |isbn=90-6764-301-7 |pages=61–62 }}&lt;/ref&gt;

==Dependent processes==

===CLT under weak dependence===
A useful generalization of a sequence of independent, identically distributed random variables is a [[Mixing (mathematics)|mixing]] random process in discrete time; "mixing" means, roughly, that random variables temporally far apart from one another are nearly independent. Several kinds of mixing are used in ergodic theory and probability theory. See especially [[Mixing (mathematics)#Mixing in stochastic processes|strong mixing]] (also called α-mixing) defined by {{math|''α''(''n'') → 0}} where {{math|''α''(''n'')}} is so-called [[Mixing (mathematics)#Mixing in stochastic processes|strong mixing coefficient]].

A simplified formulation of the central limit theorem under strong mixing is:&lt;ref&gt;Billingsley (1995, Theorem 27.5)&lt;/ref&gt;

&lt;blockquote&gt;'''Theorem.''' Suppose that {{math|''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;, …}} is stationary and {{mvar|α}}-mixing with {{math|''α''&lt;sub&gt;''n''&lt;/sub&gt; {{=}} ''O''(''n''&lt;sup&gt;−5&lt;/sup&gt;)}} and that {{math|E(''X&lt;sub&gt;n&lt;/sub&gt;'') {{=}} 0}} and {{math|E(''X''{{su|b=''n''|p=12}}) &lt; ∞}}. Denote {{math|''S&lt;sub&gt;n&lt;/sub&gt;'' {{=}} ''X''&lt;sub&gt;1&lt;/sub&gt; + … + ''X&lt;sub&gt;n&lt;/sub&gt;''}}, then the limit

:&lt;math&gt; \sigma^2 = \lim_n \frac{\operatorname{E}\left(S_n^2\right)}{n} &lt;/math&gt;

exists, and if {{math|''σ'' ≠ 0}} then {{math|{{sfrac|''S&lt;sub&gt;n&lt;/sub&gt;''|''σ''{{sqrt|''n''}}}}}} converges in distribution to {{math|''N''(0,1)}}.&lt;/blockquote&gt;

In fact,

:&lt;math&gt;\sigma^2  = \operatorname{E}\left(X_1^2\right) + 2 \sum_{k=1}^{\infty} \operatorname{E}\left(X_1 X_{1+k}\right),&lt;/math&gt;

where the series converges absolutely.

The assumption {{math|''σ'' ≠ 0}} cannot be omitted, since the asymptotic normality fails for {{math|''X&lt;sub&gt;n&lt;/sub&gt;'' {{=}} ''Y&lt;sub&gt;n&lt;/sub&gt;'' − ''Y''&lt;sub&gt;''n'' − 1&lt;/sub&gt;}} where {{math|Y&lt;sub&gt;n&lt;/sub&gt;}} are another [[stationary sequence]].

There is a stronger version of the theorem:&lt;ref&gt;Durrett (2004, Sect. 7.7(c), Theorem 7.8)&lt;/ref&gt; the assumption {{math|E(''X''{{su|b=''n''|p=12}}) &lt; ∞}} is replaced with {{math|E({{abs|''X&lt;sub&gt;n&lt;/sub&gt;''}}&lt;sup&gt;2 + ''δ''&lt;/sup&gt;) &lt; ∞}}, and the assumption {{math|''α&lt;sub&gt;n&lt;/sub&gt;'' {{=}} ''O''(''n''&lt;sup&gt;−5&lt;/sup&gt;)}} is replaced with

:&lt;math&gt;\sum_n \alpha_n^{\frac\delta{2(2+\delta)}} &lt; \infty.&lt;/math&gt;

Existence of such {{math|''δ'' &gt; 0}} ensures the conclusion. For encyclopedic treatment of limit theorems under mixing conditions see {{harv|Bradley|2007}}.

===Martingale difference CLT===
{{Main|Martingale central limit theorem}}
&lt;blockquote&gt;'''Theorem'''. Let a [[Martingale (probability theory)|martingale]] {{mvar|M&lt;sub&gt;n&lt;/sub&gt;}} satisfy
* &lt;math&gt; \frac1n \sum_{k=1}^n \operatorname{E} \left(\left(M_k-M_{k-1}\right)^2 | M_1,\dots,M_{k-1}\right) \to 1 &lt;/math&gt;  in probability as {{math|''n'' → ∞}},
* for every {{math|''ε'' &gt; 0}}, &lt;math&gt; \frac1n \sum_{k=1}^n \operatorname{E} \left( \left(M_k-M_{k-1}\right)^2; |M_k-M_{k-1}| &gt; \varepsilon \sqrt n \right) \to 0 &lt;/math&gt;  as {{math|''n'' → ∞}},
then {{math|{{sfrac|''M&lt;sub&gt;n&lt;/sub&gt;''|{{sqrt|''n''}}}}}} converges in distribution to {{math|''N''(0,1)}} as {{math|''n'' → ∞}}.&lt;ref&gt;Durrett (2004, Sect. 7.7, Theorem 7.4)&lt;/ref&gt;&lt;ref&gt;Billingsley (1995, Theorem 35.12)&lt;/ref&gt;&lt;/blockquote&gt;

''Caution:'' The [[restricted expectation]]{{clarify|reason=give source for this style of notation, else write using indicator functions|date=April 2012}} {{math|E(''X'' ; ''A'')}} should not be confused with the conditional expectation {{math|E(''X'' {{!}} ''A'') {{=}} {{sfrac|E(''X'' ; ''A'')|'''P'''(''A'')}}}}.

==Remarks==

===Proof of classical CLT===
For a theorem of such fundamental importance to [[statistics]] and [[applied probability]], the central limit theorem has a remarkably simple proof using [[characteristic function (probability theory)|characteristic functions]].&lt;ref&gt;{{cite web|url=https://jhupbooks.press.jhu.edu/content/introduction-stochastic-processes-physics|title=An Introduction to Stochastic Processes in Physics|website=jhupbooks.press.jhu.edu|access-date=2016-08-11}}&lt;/ref&gt; It is similar to the proof of the (weak) [[Proof of the law of large numbers|law of large numbers]].

As stated above, suppose {{math|{''X''&lt;sub&gt;1&lt;/sub&gt;, …, ''X&lt;sub&gt;n&lt;/sub&gt;''}}} are independent and identically distributed random variables, each with mean {{mvar|µ}} and finite variance {{math|''σ''&lt;sup&gt;2&lt;/sup&gt;}}. The sum {{math|''X''&lt;sub&gt;1&lt;/sub&gt; + … + ''X&lt;sub&gt;n&lt;/sub&gt;''}} has [[Linearity of expectation|mean]] {{mvar|nµ}} and [[Variance#Sum of uncorrelated variables (Bienaymé formula)|variance]] {{math|''nσ''&lt;sup&gt;2&lt;/sup&gt;}}. Consider the random variable
: &lt;math&gt;Z_n \ =\ \frac{X_1+\cdots+X_n - n \mu}{\sqrt{n \sigma^2}} \ =\ \sum_{i=1}^n \frac{X_i - \mu}{\sqrt{n \sigma^2}} \ =\ \sum_{i=1}^n \frac{1}{\sqrt{n}} Y_i,&lt;/math&gt;
where in the last step we defined the new random variables {{math|''Y&lt;sub&gt;i&lt;/sub&gt;'' {{=}} {{sfrac|''X&lt;sub&gt;i&lt;/sub&gt;'' − ''μ''|''σ''}}}}, each with zero mean and unit variance ({{math|var(''Y'') {{=}} 1}}). The [[Characteristic function (probability theory)|characteristic function]] of {{mvar|Z&lt;sub&gt;n&lt;/sub&gt;}} is given by

:&lt;math&gt;\varphi_{Z_n}\!(t) \ =\ \varphi_{\sum_{i=1}^n {\frac{1}{\sqrt{n}}Y_i}}\!(t) \ =\ \varphi_{Y_1}\!\!\left(\frac{t}{\sqrt{n}}\right) \varphi_{Y_2}\!\! \left(\frac{t}{\sqrt{n}}\right)\cdots \varphi_{Y_n}\!\! \left(\frac{t}{\sqrt{n}}\right) \ =\ \left[\varphi_{Y_1}\!\!\left(\frac{t}{\sqrt{n}}\right)\right]^n,
&lt;/math&gt;

where in the last step we used the fact that all of the {{mvar|Y&lt;sub&gt;i&lt;/sub&gt;}} are identically distributed. The characteristic function of {{math|''Y''&lt;sub&gt;1&lt;/sub&gt;}} is, by [[Taylor's theorem]],

:&lt;math&gt;\varphi_{Y_1}\!\!\left(\frac{t}{\sqrt{n}}\right) \ =\ 1 - \frac{t^2}{2n} + o\!\!\left(\frac{t^2}{n}\right), \quad \bigg(\frac{t}{\sqrt{n}}\bigg) \rightarrow 0&lt;/math&gt;

where {{math|''o''(''t''&lt;sup&gt;2&lt;/sup&gt;)}} is "[[Little-o notation|little {{mvar|o}} notation]]" for some function of {{mvar|t}} that goes to zero more rapidly than {{math|''t''&lt;sup&gt;2&lt;/sup&gt;}}. By the limit of the [[exponential function]] ({{math|''e''&lt;sup&gt;''x''&lt;/sup&gt;{{=}} lim(1 + {{sfrac|''x''|''n''}})&lt;sup&gt;''n''&lt;/sup&gt;}}), the characteristic function of {{mvar|Z&lt;sub&gt;n&lt;/sub&gt;}} equals

:&lt;math&gt;\varphi_{Z_n}(t) = \left(1 - \frac{t^2}{2n} + o\left(\frac{t^2}{n}\right) \right)^n \rightarrow e^{-\frac12 t^2}, \quad n \rightarrow \infty.&lt;/math&gt;

Note that all of the higher order terms vanish in the limit {{math|''n'' → ∞}}. The right hand side equals the characteristic function of a standard normal distribution {{math|''N''(0,1)}}, which implies through [[Lévy continuity theorem|Lévy's continuity theorem]] that the distribution of {{mvar|Z&lt;sub&gt;n&lt;/sub&gt;}} will approach {{math|''N''(0,1)}} as {{math|''n'' → ∞}}. Therefore, the sum {{math|''X''&lt;sub&gt;1&lt;/sub&gt; + … + ''X&lt;sub&gt;n&lt;/sub&gt;''}} will approach that of the normal distribution {{math|''N''(''nµ'',''nσ''&lt;sup&gt;2&lt;/sup&gt;)}}, and the [[sample mean|sample average]]

:&lt;math&gt;S_n = \frac{X_1+\cdots+X_n}{n}&lt;/math&gt;

converges to the normal distribution {{math|''N''(''µ'',{{sfrac|''σ''&lt;sup&gt;2&lt;/sup&gt;|''n''}})}}, from which the central limit theorem follows.

===Convergence to the limit===
The central limit theorem gives only an [[asymptotic distribution]]. As an approximation for a finite number of observations, it provides a reasonable approximation only when close to the peak of the normal distribution; it requires a very large number of observations to stretch into the tails.{{citation needed|reason=Not immediately obvious, I didn't find a source via google|date=July 2016}}

The convergence in the central limit theorem is [[uniform convergence|uniform]] because the limiting cumulative distribution function is continuous. If the third central [[Moment (mathematics)|moment]] {{math|E((''X''&lt;sub&gt;1&lt;/sub&gt; − ''μ'')&lt;sup&gt;3&lt;/sup&gt;)}} exists and is finite, then the speed of convergence is at least on the order of {{math|{{sfrac|1|{{sqrt|''n''}}}}}} (see [[Berry–Esseen theorem]]). [[Stein's method]]&lt;ref name="stein1972"&gt;{{Cite journal| last = Stein |first=C. |authorlink=Charles Stein (statistician)| title = A bound for the error in the normal approximation to the distribution of a sum of dependent random variables| journal = Proceedings of the Sixth Berkeley Symposium on Mathematical Statistics and Probability| pages= 583–602| year = 1972| mr=402873 | zbl = 0278.60026| url=http://projecteuclid.org/euclid.bsmsp/1200514239
}}&lt;/ref&gt; can be used not only to prove the central limit theorem, but also to provide bounds on the rates of convergence for selected metrics.&lt;ref&gt;{{Cite book|  title = Normal approximation by Stein's method|  publisher = Springer|  year = 2011|last1=Chen |first1=L. H. Y. |last2=Goldstein |first2=L. |last3=Shao |first3=Q. M. |isbn = 978-3-642-15006-7}}&lt;/ref&gt;

The convergence to the normal distribution is monotonic, in the sense that the [[information entropy|entropy]] of {{mvar|Z&lt;sub&gt;n&lt;/sub&gt;}} increases [[monotonic function|monotonically]] to that of the normal distribution.&lt;ref name=ABBN/&gt;

The central limit theorem applies in particular to sums of independent and identically distributed [[discrete random variable]]s.  A sum of [[discrete random variable]]s is still a [[discrete random variable]], so that we are confronted with a sequence of [[discrete random variable]]s whose cumulative probability distribution function converges towards a cumulative probability distribution function corresponding to a continuous variable (namely that of the [[normal distribution]]).  This means that if we build a [[histogram]] of the realisations of the sum of {{mvar|n}} independent identical discrete variables, the curve that joins the centers of the upper faces of the rectangles forming the histogram converges toward a Gaussian curve as {{mvar|n}} approaches infinity, this relation is known as [[de Moivre–Laplace theorem]]. The [[binomial distribution]] article details such an application of the central limit theorem in the simple case of a discrete variable taking only two possible values.

===Relation to the law of large numbers===
[[Law of large numbers|The law of large numbers]] as well as the central limit theorem are partial solutions to a general problem: "What is the limiting behaviour of {{math|S&lt;sub&gt;{{mvar|n}}&lt;/sub&gt;}} as {{mvar|n}} approaches infinity?" In mathematical analysis, [[asymptotic series]] are one of the most popular tools employed to approach such questions.

Suppose we have an asymptotic expansion of {{math|''f''(''n'')}}:

: &lt;math&gt;f(n)= a_1 \varphi_{1}(n)+a_2 \varphi_{2}(n)+O\big(\varphi_{3}(n)\big) \qquad  (n \rightarrow \infty).&lt;/math&gt;

Dividing both parts by {{math|''φ''&lt;sub&gt;1&lt;/sub&gt;(''n'')}} and taking the limit will produce {{math|''a''&lt;sub&gt;1&lt;/sub&gt;}}, the coefficient of the highest-order term in the expansion, which represents the rate at which {{math|''f''(''n'')}} changes in its leading term.

: &lt;math&gt;\lim_{n\to\infty}\frac{f(n)}{\varphi_{1}(n)}=a_1.&lt;/math&gt;

Informally, one can say: "{{math|''f''(''n'')}} grows approximately as {{math|''a''&lt;sub&gt;1&lt;/sub&gt;''φ''&lt;sub&gt;1&lt;/sub&gt;(''n'')}}". Taking the difference between {{math|''f''(''n'')}} and its approximation and then dividing by the next term in the expansion, we arrive at a more refined statement about {{math|''f''(''n'')}}:

: &lt;math&gt;\lim_{n\to\infty}\frac{f(n)-a_1 \varphi_{1}(n)}{\varphi_{2}(n)}=a_2 .&lt;/math&gt;

Here one can say that the difference between the function and its approximation grows approximately as {{math|''a''&lt;sub&gt;2&lt;/sub&gt;''φ''&lt;sub&gt;2&lt;/sub&gt;(''n'')}}.  The idea is that dividing the function by appropriate normalizing functions, and looking at the limiting behavior of the result, can tell us much about the limiting behavior of the original function itself.

Informally, something along these lines happens when the sum, {{mvar|S&lt;sub&gt;n&lt;/sub&gt;}}, of independent identically distributed random variables, {{math|''X''&lt;sub&gt;1&lt;/sub&gt;, …, ''X&lt;sub&gt;n&lt;/sub&gt;''}}, is studied in classical probability theory.{{Citation needed|date=April 2012}}  If each {{mvar|X&lt;sub&gt;i&lt;/sub&gt;}} has finite mean {{mvar|μ}}, then by the law of large numbers, {{math|{{sfrac|''S&lt;sub&gt;n&lt;/sub&gt;''|''n''}} → ''μ''}}.&lt;ref&gt;{{cite book|last=Rosenthal |first=Jeffrey Seth |date=2000 |title=A First Look at Rigorous Probability Theory |publisher=World Scientific |ISBN=981-02-4322-7 |at=Theorem 5.3.4, p. 47}}&lt;/ref&gt;  If in addition each {{mvar|X&lt;sub&gt;i&lt;/sub&gt;}} has finite variance {{math|''σ''&lt;sup&gt;2&lt;/sup&gt;}}, then by the central limit theorem,

: &lt;math&gt; \frac{S_n-n\mu}{\sqrt{n}} \rightarrow \xi ,&lt;/math&gt;

where {{mvar|ξ}} is distributed as {{math|''N''(0,''σ''&lt;sup&gt;2&lt;/sup&gt;)}}.  This provides values of the first two constants in the informal expansion

: &lt;math&gt;S_n \approx \mu n+\xi \sqrt{n}. &lt;/math&gt;

In the case where the {{mvar|X&lt;sub&gt;i&lt;/sub&gt;}} do not have finite mean or variance, convergence of the shifted and rescaled sum can also occur with different centering and scaling factors:

:&lt;math&gt;\frac{S_n-a_n}{b_n} \rightarrow \Xi,&lt;/math&gt;

or informally

: &lt;math&gt;S_n \approx a_n+\Xi b_n. &lt;/math&gt;

Distributions {{math|Ξ}} which can arise in this way are called ''[[stable distribution|stable]]''.&lt;ref&gt;{{cite book|last=Johnson |first=Oliver Thomas |date=2004 |title=Information Theory and the Central Limit Theorem |publisher=Imperial College Press |ISBN= 1-86094-473-6 |page= 88}}&lt;/ref&gt;  Clearly, the normal distribution is stable, but there are also other stable distributions, such as the [[Cauchy distribution]], for which the mean or variance are not defined.  The scaling factor {{mvar|b&lt;sub&gt;n&lt;/sub&gt;}} may be proportional to {{mvar|n&lt;sup&gt;c&lt;/sup&gt;}}, for any {{math|''c'' ≥ {{sfrac|1|2}}}}; it may also be multiplied by a [[slowly varying function]] of {{mvar|n}}.&lt;ref name=Uchaikin /&gt;&lt;ref&gt;{{cite book|last1=Borodin |first1=A. N. |last2=Ibragimov |first2=I. A. |last3=Sudakov |first3=V. N. |date=1995 |title=Limit Theorems for Functionals of Random Walks |publisher=AMS Bookstore |ISBN= 0-8218-0438-3 |at=Theorem 1.1, p. 8}}&lt;/ref&gt;

The [[law of the iterated logarithm]] specifies what is happening "in between" the [[law of large numbers]] and the central limit theorem. Specifically it says that the normalizing function {{math|{{sqrt|''n'' log log ''n''}}}}, intermediate in size between {{mvar|n}} of the law of large numbers and {{math|{{sqrt|''n''}}}} of the central limit theorem, provides a non-trivial limiting behavior.

===Alternative statements of the theorem===

====Density functions====
The [[probability density function|density]] of the sum of two or more independent variables is the [[convolution]] of their densities (if these densities exist).  Thus the central limit theorem can be interpreted as a statement about the properties of density functions under convolution: the convolution of a number of density functions tends to the normal density as the number of density functions increases without bound. These theorems require stronger hypotheses than the forms of the central limit theorem given above. Theorems of this type are often called local limit theorems. See Petrov&lt;ref&gt;{{Cite book|last=Petrov|first=V. V.|title=Sums of Independent Random Variables|year=1976|publisher=Springer-Verlag|location=New York-Heidelberg|at=ch. 7}}&lt;/ref&gt; for a particular local limit theorem for sums of [[independent and identically distributed random variables]].

====Characteristic functions====
Since the [[characteristic function (probability theory)|characteristic function]] of a convolution is the product of the characteristic functions of the densities involved, the central limit theorem has yet another restatement: the product of the characteristic functions of a number of density functions becomes close to the characteristic function of the normal density as the number of density functions increases without bound, under the conditions stated above. Specifically, an appropriate scaling factor needs to be applied to the argument of the characteristic function.

An equivalent statement can be made about [[Fourier transform]]s, since the characteristic function is essentially a Fourier transform.

===Calculating the variance===
Let {{mvar|S&lt;sub&gt;n&lt;/sub&gt;}} be the sum of {{mvar|n}} random variables. Many central limit theorems provide conditions such that {{math|{{mvar|S&lt;sub&gt;n&lt;/sub&gt;}}/{{sqrt|Var({{mvar|S&lt;sub&gt;n&lt;/sub&gt;}})}}}} converges in distribution to {{math|''N''(0,1)}} (the normal distribution with mean 0, variance 1) as {{math|{{mvar|n}}→ ∞}}.  In some cases, it is possible to find a constant {{mvar|σ&lt;sup&gt;2&lt;/sup&gt;}}  and function {{mvar|f(n)}} such that {{math|{{mvar|S&lt;sub&gt;n&lt;/sub&gt;}}/(σ{{sqrt|{{mvar|n⋅f}}({{mvar|n}})}})}} converges in distribution to {{math|''N''(0,1)}} as {{math|{{mvar|n}}→ ∞}}.

&lt;blockquote&gt;'''Lemma.'''&lt;ref&gt;{{cite journal|last1=Hew|first1=Patrick Chisan|title=Asymptotic distribution of rewards accumulated by alternating renewal processes|journal=Statistics and Probability Letters|date=2017|volume=129|pages=355–359|doi=10.1016/j.spl.2017.06.027}}&lt;/ref&gt; Suppose &lt;math&gt;X_1, X_2, \dots&lt;/math&gt; is a sequence of real-valued and strictly stationary random variables with &lt;math&gt;\mathbb{E}(X_i) = 0&lt;/math&gt; for all &lt;math&gt;i&lt;/math&gt;, &lt;math&gt;g : [0,1] \rightarrow \mathbb{R}&lt;/math&gt;, and &lt;math&gt;S_n = \sum_{i=1}^{n} g(\tfrac{i}{n}) X_i&lt;/math&gt;. Construct

:&lt;math&gt;\sigma^2 = \mathbb{E}(X_1^2) + 2\sum_{i=1}^{\infty} \mathbb{E}(X_1 X_{1+i})&lt;/math&gt;

# If &lt;math&gt;\sum_{i=1}^{\infty} \mathbb{E}(X_1 X_{1+i})&lt;/math&gt; is absolutely convergent, &lt;math&gt;\left| \int_0^1 g(x)g'(x) \, dx\right| &lt; \infty&lt;/math&gt;, and &lt;math&gt;0 &lt; \int_0^1 (g(x))^2 dx &lt; \infty&lt;/math&gt; then &lt;math&gt;\mathrm{Var}(S_n)/(n \gamma_n) \rightarrow \sigma^2&lt;/math&gt; as &lt;math&gt;n \rightarrow \infty&lt;/math&gt; where &lt;math&gt;\gamma_n = \frac{1}{n}\sum_{i=1}^{n} (g(\tfrac{i}{n}))^2&lt;/math&gt;.
# If in addition &lt;math&gt;\sigma &gt; 0&lt;/math&gt; and &lt;math&gt;S_n/\sqrt{\mathrm{Var}(S_n)}&lt;/math&gt; converges in distribution to &lt;math&gt;\mathcal{N}(0,1)&lt;/math&gt; as &lt;math&gt;n \rightarrow \infty&lt;/math&gt; then &lt;math&gt;S_n/(\sigma\sqrt{n \gamma_n})&lt;/math&gt; also converges in distribution to &lt;math&gt;\mathcal{N}(0,1)&lt;/math&gt; as &lt;math&gt;n \rightarrow \infty&lt;/math&gt;.
&lt;/blockquote&gt;

==Extensions==

===Products of positive random variables===
The [[logarithm]] of a product is simply the sum of the logarithms of the factors.  Therefore, when the logarithm of a product of random variables that take only positive values approaches a normal distribution, the product itself approaches a [[log-normal distribution]].  Many physical quantities (especially mass or length, which are a matter of scale and cannot be negative) are the products of different [[random]] factors, so they follow a log-normal distribution.  This multiplicative version of the central limit theorem is sometimes called [[Gibrat's law]].

Whereas the central limit theorem for sums of random variables requires the condition of finite variance, the corresponding theorem for products requires the corresponding condition that the density function be square-integrable.&lt;ref name=Rempala/&gt;

==Beyond the classical framework==
Asymptotic normality, that is, [[Convergence in distribution|convergence]] to the normal distribution after appropriate shift and rescaling, is a phenomenon much more general than the classical framework treated above, namely, sums of independent random variables (or vectors). New frameworks are revealed from time to time; no single unifying framework is available for now.

===Convex body===
&lt;blockquote&gt;'''Theorem.'''  There exists a sequence {{math|''ε&lt;sub&gt;n&lt;/sub&gt;'' ↓ 0}} for which the following holds. Let {{math|''n'' ≥ 1}}, and let random variables {{math|''X''&lt;sub&gt;1&lt;/sub&gt;, …, ''X&lt;sub&gt;n&lt;/sub&gt;''}} have a [[Logarithmically concave function|log-concave]] [[Joint density function|joint density]] {{mvar|f}} such that {{math|''f''(''x''&lt;sub&gt;1&lt;/sub&gt;, …, ''x&lt;sub&gt;n&lt;/sub&gt;'') {{=}} ''f''({{abs|''x''&lt;sub&gt;1&lt;/sub&gt;}}, …, {{abs|''x&lt;sub&gt;n&lt;/sub&gt;''}})}} for all {{math|''x''&lt;sub&gt;1&lt;/sub&gt;, …, ''x&lt;sub&gt;n&lt;/sub&gt;''}},  and {{math|E(''X''{{su|b=''k''|p=2}}) {{=}} 1}} for all {{math|''k'' {{=}} 1, …, ''n''}}. Then the distribution of

:&lt;math&gt; \frac{X_1+\cdots+X_n}{\sqrt n} &lt;/math&gt;

is {{mvar|ε&lt;sub&gt;n&lt;/sub&gt;}}-close to {{math|''N''(0,1)}} in the [[Total variation distance of probability measures|total variation distance]].&lt;ref&gt;Klartag (2007, Theorem 1.2)&lt;/ref&gt;&lt;/blockquote&gt;

These two {{mvar|ε&lt;sub&gt;n&lt;/sub&gt;}}-close distributions have densities (in fact, log-concave densities), thus, the total variance distance between them is the integral of the absolute value of the difference between the densities. Convergence in total variation is stronger than weak convergence.

An important example of a log-concave density is a function constant inside a given convex body and vanishing outside; it corresponds to the uniform distribution on the convex body, which explains the term "central limit theorem for convex bodies".

Another example: {{math|''f''(''x''&lt;sub&gt;1&lt;/sub&gt;, …, ''x&lt;sub&gt;n&lt;/sub&gt;'') {{=}} const · exp( − ({{abs|''x''&lt;sub&gt;1&lt;/sub&gt;}}&lt;sup&gt;''α''&lt;/sup&gt; + … + {{abs|''x&lt;sub&gt;n&lt;/sub&gt;''}}&lt;sup&gt;''α''&lt;/sup&gt;)&lt;sup&gt;''β''&lt;/sup&gt;)}} where {{math|''α'' &gt; 1}} and {{math|''αβ'' &gt; 1}}. If {{math|''β'' {{=}} 1}} then {{math|''f''(''x''&lt;sub&gt;1&lt;/sub&gt;, …, ''x&lt;sub&gt;n&lt;/sub&gt;'')}} factorizes into {{math|const · exp (−{{abs|''x''&lt;sub&gt;1&lt;/sub&gt;}}&lt;sup&gt;''α''&lt;/sup&gt;) … exp(−{{abs|''x&lt;sub&gt;n&lt;/sub&gt;''}}&lt;sup&gt;''α''&lt;/sup&gt;), }} which means {{math|''X''&lt;sub&gt;1&lt;/sub&gt;, …, ''X&lt;sub&gt;n&lt;/sub&gt;''}} are independent. In general, however, they are dependent.

The condition {{math|''f''(''x''&lt;sub&gt;1&lt;/sub&gt;, …, ''x&lt;sub&gt;n&lt;/sub&gt;'') {{=}} ''f''({{abs|''x''&lt;sub&gt;1&lt;/sub&gt;}}, …, {{abs|''x&lt;sub&gt;n&lt;/sub&gt;''}})}} ensures that {{math|''X''&lt;sub&gt;1&lt;/sub&gt;, …, ''X&lt;sub&gt;n&lt;/sub&gt;''}} are of zero mean and [[uncorrelated]];{{Citation needed|date=June 2012}} still, they need not be independent, nor even [[Pairwise independence|pairwise independent]].{{Citation needed|date=June 2012}} By the way, pairwise independence cannot replace independence in the classical central limit theorem.&lt;ref&gt;Durrett (2004, Section 2.4, Example 4.5)&lt;/ref&gt;

Here is a [[Berry–Esseen theorem|Berry–Esseen]] type result.

&lt;blockquote&gt;'''Theorem.''' Let {{math|''X''&lt;sub&gt;1&lt;/sub&gt;, …, ''X&lt;sub&gt;n&lt;/sub&gt;''}} satisfy the assumptions of the previous theorem, then &lt;ref&gt;Klartag (2008, Theorem 1)&lt;/ref&gt;

: &lt;math&gt; \left| \mathbb{P} \left( a \le \frac{ X_1+\cdots+X_n }{ \sqrt n } \le b \right) - \frac1{\sqrt{2\pi}} \int_a^b \mathrm{e}^{-\frac12 t^2} \, dt \right| \le \frac{C}{n} &lt;/math&gt;

for all {{math|''a'' &lt; ''b''}}; here {{mvar|C}} is a [[mathematical constant|universal (absolute) constant]]. Moreover, for every {{math|''c''&lt;sub&gt;1&lt;/sub&gt;, …, ''c&lt;sub&gt;n&lt;/sub&gt;'' ∈ '''ℝ'''}} such that {{math|''c''{{su|b=1|p=2}} + … + ''c''{{su|b=''n''|p=2}} {{=}} 1}},

: &lt;math&gt; \left| \mathbb{P} \left( a \le c_1 X_1+\cdots+c_n X_n \le b \right) - \frac1{\sqrt{2\pi}} \int_a^b \mathrm{e}^{-\frac12 t^2} \, dt \right| \le C \left( c_1^4+\dots+c_n^4 \right). &lt;/math&gt;&lt;/blockquote&gt;

The distribution of {{math|{{sfrac|''X''&lt;sub&gt;1&lt;/sub&gt; + … + ''X&lt;sub&gt;n&lt;/sub&gt;''|{{sqrt|''n''}}}}}} need not be approximately normal (in fact, it can be uniform).&lt;ref&gt;Klartag (2007, Theorem 1.1)&lt;/ref&gt; However, the distribution of {{math|''c''&lt;sub&gt;1&lt;/sub&gt;''X''&lt;sub&gt;1&lt;/sub&gt; + … + ''c&lt;sub&gt;n&lt;/sub&gt;X&lt;sub&gt;n&lt;/sub&gt;''}} is close to {{math|''N''(0,1)}} (in the total variation distance) for most vectors {{math|(''c''&lt;sub&gt;1&lt;/sub&gt;, …, ''c&lt;sub&gt;n&lt;/sub&gt;'')}} according to the uniform distribution on the sphere {{math|''c''{{su|b=1|p=2}} + … + ''c''{{su|b=''n''|p=2}} {{=}} 1}}.

===Lacunary trigonometric series===
&lt;blockquote&gt;'''Theorem''' ([[Raphaël Salem|Salem]]–[[Antoni Zygmund|Zygmund]]): Let {{mvar|U}} be a random variable distributed uniformly on {{math|(0,2π)}}, and {{math|''X&lt;sub&gt;k&lt;/sub&gt;'' {{=}} ''r&lt;sub&gt;k&lt;/sub&gt;'' cos(''n&lt;sub&gt;k&lt;/sub&gt;U'' + ''a&lt;sub&gt;k&lt;/sub&gt;'')}}, where
* {{mvar|n&lt;sub&gt;k&lt;/sub&gt;}} satisfy the lacunarity condition: there exists {{math|''q'' &gt; 1}} such that {{math|''n''&lt;sub&gt;''k'' + 1&lt;/sub&gt; ≥ ''qn''&lt;sub&gt;''k''&lt;/sub&gt;}} for all {{mvar|k}},
* {{mvar|r&lt;sub&gt;k&lt;/sub&gt;}} are such that
:: &lt;math&gt; r_1^2 + r_2^2 + \cdots = \infty \quad\text{ and }\quad \frac{ r_k^2 }{ r_1^2+\cdots+r_k^2 } \to 0, &lt;/math&gt;
* {{math|0 ≤ ''a''&lt;sub&gt;''k''&lt;/sub&gt; &lt; 2π}}.
Then&lt;ref name=Zygmund/&gt;&lt;ref&gt;Gaposhkin (1966, Theorem 2.1.13)&lt;/ref&gt;

: &lt;math&gt; \frac{ X_1+\cdots+X_k }{ \sqrt{r_1^2+\cdots+r_k^2} } &lt;/math&gt;

converges in distribution to {{math|''N''(0, {{sfrac|1|2}})}}.&lt;/blockquote&gt;

===Gaussian polytopes===
&lt;blockquote&gt;'''Theorem:'''  Let {{math|''A''&lt;sub&gt;1&lt;/sub&gt;, …, ''A''&lt;sub&gt;''n''&lt;/sub&gt;}} be independent random points on the plane {{math|'''ℝ'''&lt;sup&gt;2&lt;/sup&gt;}} each having the two-dimensional standard normal distribution. Let {{mvar|K&lt;sub&gt;n&lt;/sub&gt;}} be the [[convex hull]] of these points, and {{mvar|X&lt;sub&gt;n&lt;/sub&gt;}} the area of {{mvar|K&lt;sub&gt;n&lt;/sub&gt;}} Then&lt;ref&gt;Bárány &amp; Vu (2007, Theorem 1.1)&lt;/ref&gt;

: &lt;math&gt; \frac{ X_n - \mathrm{E} (X_n) }{ \sqrt{\operatorname{Var} (X_n)} } &lt;/math&gt;

converges in distribution to {{math|''N''(0,1)}} as {{mvar|n}} tends to infinity.&lt;/blockquote&gt;

The same also holds in all dimensions greater than 2.

The [[convex polytope|polytope]] {{mvar|K&lt;sub&gt;n&lt;/sub&gt;}} is called a Gaussian random polytope.

A similar result holds for the number of vertices (of the Gaussian polytope), the number of edges, and in fact, faces of all dimensions.&lt;ref&gt;Bárány &amp; Vu (2007, Theorem 1.2)&lt;/ref&gt;

===Linear functions of orthogonal matrices===
A linear function of a matrix {{math|'''M'''}} is a linear combination of its elements (with given coefficients), {{math|'''M''' ↦ tr('''AM''')}} where {{math|'''A'''}} is the matrix of the coefficients; see [[Trace (linear algebra)#Inner product]].

A random [[orthogonal matrix]] is said to be distributed uniformly, if its distribution is the normalized [[Haar measure]] on the [[orthogonal group]] {{math|O(''n'','''ℝ''')}}; see [[Rotation matrix#Uniform random rotation matrices]].

&lt;blockquote&gt;'''Theorem.''' Let {{math|'''M'''}} be a random orthogonal {{math|''n'' × ''n''}} matrix distributed uniformly, and {{math|'''A'''}} a fixed {{math|''n'' × ''n''}} matrix such that {{math|tr('''AA'''*) {{=}} ''n''}}, and let {{math|''X'' {{=}} tr('''AM''')}}. Then&lt;ref name=Meckes/&gt; the distribution of {{mvar|X}} is close to {{math|''N''(0,1)}} in the total variation metric up to{{clarify|reason=what does up to mean here|date=June 2012}} {{math|{{sfrac|2{{sqrt|3}}|''n'' − 1}}}}.&lt;/blockquote&gt;

===Subsequences===
&lt;blockquote&gt;'''Theorem.'''  Let random variables {{math|''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;, … ∈ ''L''&lt;sub&gt;2&lt;/sub&gt;(Ω)}} be such that {{math|''X&lt;sub&gt;n&lt;/sub&gt;'' → 0}} [[Weak convergence (Hilbert space)|weakly]] in {{math|''L''&lt;sub&gt;2&lt;/sub&gt;(Ω)}} and {{math|''X''{{su|b=''n''|2}} → 1}} weakly in {{math|''L''&lt;sub&gt;1&lt;/sub&gt;(Ω)}}. Then there exist integers {{math|''n''&lt;sub&gt;1&lt;/sub&gt; &lt; ''n''&lt;sub&gt;2&lt;/sub&gt; &lt; …}} such that
:&lt;math&gt; \frac{ X_{n_1}+\cdots+X_{n_k} }{ \sqrt k }&lt;/math&gt;
converges in distribution to {{math|''N''(0,1)}} as {{mvar|k}} tends to infinity.&lt;ref&gt;Gaposhkin (1966, Sect. 1.5)&lt;/ref&gt;&lt;/blockquote&gt;

===Random walk on a crystal lattice===

The central limit theorem may be established for the simple [[random walk]] on a crystal lattice (an infinite-fold abelian covering graph over a finite graph), and is used for design of crystal structures.
&lt;ref&gt;{{cite book |last1=Kotani |first1=M. |last2=Sunada |first2=Toshikazu |author-link2=Toshikazu Sunada |date=2003 |title=Spectral geometry of crystal lattices |publisher=Contemporary Math |volume=338 |pp=271–305 |isbn=978-0-8218-4269-0}}&lt;/ref&gt;&lt;ref&gt;{{cite book |author-link=Toshikazu Sunada |last=Sunada |first=Toshikazu |date=2012 |title=Topological Crystallography – With a View Towards Discrete Geometric Analysis|series=Surveys and Tutorials in the Applied Mathematical Sciences |volume=6 |publisher=Springer |isbn=978-4-431-54177-6}}&lt;/ref&gt;

==Applications and examples==
===Simple example===
[[File:Dice sum central limit theorem.svg|thumb|250px|Comparison of probability density functions, {{math|**''p''(''k'')}} for the sum of {{mvar|n}} fair 6-sided dice to show their convergence to a normal distribution with increasing {{mvar|n}}, in accordance to the central limit theorem. In the bottom-right graph, smoothed profiles of the previous graphs are rescaled, superimposed and compared with a normal distribution (black curve).]]

[[File:Central Limit Theorem.png|Central Limit Theorem|thumb|640px|Another simulation using the binomial distribution. Random 0s and 1s were generated, and then their means calculated for sample sizes ranging from 1 to 512. Note that as the sample size increases the tails become thinner and the distribution becomes more concentrated around the mean.]]

A simple example of the central limit theorem is rolling a large number of identical, unbiased dice. The distribution of the sum (or average) of the rolled numbers will be well approximated by a normal distribution. Since real-world quantities are often the balanced sum of many unobserved random events, the central limit theorem also provides a partial explanation for the prevalence of the normal probability distribution. It also justifies the approximation of large-sample [[statistic]]s to the normal distribution in controlled experiments.

[[File:Empirical CLT - Figure - 040711.jpg|none|thumb|500px|This figure demonstrates the central limit theorem.  The sample means are generated using a random number generator, which draws numbers between 0 and 100 from a uniform probability distribution.  It illustrates that increasing sample sizes result in the 500 measured sample means being more closely distributed about the population mean (50 in this case).  It also compares the observed distributions with the distributions that would be expected for a normalized Gaussian distribution, and shows the [[Pearson's chi-squared test|chi-squared]] values that quantify the goodness of the fit (the fit is good if the reduced [[Pearson's chi-squared test|chi-squared]] value is less than or approximately equal to one).  The input into the normalized Gaussian function is the mean of sample means (~50) and the mean sample standard deviation divided by the square root of the sample size (~28.87/{{math|{{sqrt|''n''}}}}), which is called the standard deviation of the mean (since it refers to the spread of sample means).]]

===Real applications===
Published literature contains a number of useful and interesting examples and applications relating to the central limit theorem.&lt;ref&gt;Dinov, Christou &amp; Sánchez (2008)&lt;/ref&gt; One source&lt;ref&gt;{{cite web|url=http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_GCLT_Applications |title=SOCR EduMaterials Activities GCLT Applications - Socr |website=Wiki.stat.ucla.edu |date=2010-05-24 |accessdate=2017-01-23}}&lt;/ref&gt; states the following examples:
*The probability distribution for total distance covered in a [[random walk]] (biased or unbiased) will tend toward a [[normal distribution]].
*Flipping a large number of coins will result in a normal distribution for the total number of heads (or equivalently total number of tails).

From another viewpoint, the central limit theorem explains the common appearance of the "bell curve" in [[density estimation|density estimates]] applied to real world data. In cases like electronic noise, examination grades, and so on, we can often regard a single measured value as the weighted average of a large number of small effects. Using generalisations of the central limit theorem, we can then see that this would often (though not always) produce a final distribution that is approximately normal.

In general, the more a measurement is like the sum of independent variables with equal influence on the result, the more normality it exhibits. This justifies the common use of this distribution to stand in for the effects of unobserved variables in models like the [[linear model]].

==Regression==
[[Regression analysis]] and in particular [[ordinary least squares]] specifies that a [[dependent variable]] depends according to some function upon one or more [[independent variable]]s, with an additive [[Errors and residuals in statistics|error term]]. Various types of statistical inference on the regression assume that the error term is normally distributed. This assumption can be justified by assuming that the error term is actually the sum of a large number of independent error terms; even if the individual error terms are not normally distributed, by the central limit theorem their sum can be well approximated by a normal distribution.

===Other illustrations===
{{Main|Illustration of the central limit theorem}}
Given its importance to statistics, a number of papers and computer packages are available that demonstrate the convergence involved in the central limit theorem.&lt;ref name="Marasinghe"&gt;{{cite journal|last1=Marasinghe |first1=M. |last2=Meeker |first2=W. |last3=Cook |first3=D. |last4=Shin |first4=T. S. |date=Aug 1994 |title=Using graphics and simulation to teach statistical concepts |series=Paper presented at the Annual meeting of the American Statistician Association, Toronto, Canada}}&lt;/ref&gt;

==History==
Dutch mathematician [[Henk Tijms]] writes:&lt;ref name=Tijms/&gt;

{{quote|The central limit theorem has an interesting history. The first version of this theorem was postulated by the French-born mathematician [[Abraham de Moivre]] who, in a remarkable article published in 1733, used the normal distribution to approximate the distribution of the number of heads resulting from many tosses of a fair coin. This finding was far ahead of its time, and was nearly forgotten until the famous French mathematician [[Pierre-Simon Laplace]] rescued it from obscurity in his monumental work ''Théorie analytique des probabilités'', which was published in 1812.  Laplace expanded De Moivre's finding by approximating the binomial distribution with the normal distribution. But as with De Moivre, Laplace's finding received little attention in his own time. It was not until the nineteenth century was at an end that the importance of the central limit theorem was discerned, when, in 1901, Russian mathematician [[Aleksandr Lyapunov]] defined it in general terms and proved precisely how it worked mathematically. Nowadays, the central limit theorem is considered to be the unofficial sovereign of probability theory.}}

Sir [[Francis Galton]] described the Central Limit Theorem in this way:&lt;ref&gt;{{cite book|last=Galton|first= F. |date=1889 |title=Natural Inheritance |url=http://galton.org/cgi-bin/searchImages/galton/search/books/natural-inheritance/pages/natural-inheritance_0073.htm |page= 66}}&lt;/ref&gt;

{{quote|I know of scarcely anything so apt to impress the imagination as the wonderful form of cosmic order expressed by the "Law of Frequency of Error". The law would have been personified by the Greeks and deified, if they had known of it. It reigns with serenity and in complete self-effacement, amidst the wildest confusion. The huger the mob, and the greater the apparent anarchy, the more perfect is its sway. It is the supreme law of Unreason. Whenever a large sample of chaotic elements are taken in hand and marshalled in the order of their magnitude, an unsuspected and most beautiful form of regularity proves to have been latent all along.}}

The actual term "central limit theorem" (in German: "zentraler Grenzwertsatz") was first used by [[George Pólya]] in 1920 in the title of a paper.&lt;ref name=Polya1920&gt;{{Cite journal|last=Pólya|first=George|authorlink=George Pólya|year=1920|title=Über den zentralen Grenzwertsatz der Wahrscheinlichkeitsrechnung und das Momentenproblem|trans-title=On the central limit theorem of probability calculation and the problem of moments |journal=[[Mathematische Zeitschrift]]|volume=8|pages=171–181|language=German|url=http://www-gdz.sub.uni-goettingen.de/cgi-bin/digbib.cgi?PPN266833020_0008|doi=10.1007/BF01206525|issue=3–4}}&lt;/ref&gt;&lt;ref name=LC1986/&gt; Pólya referred to the theorem as "central" due to its importance in probability theory. According to Le Cam, the French school of probability interprets the word ''central'' in the sense that "it describes the behaviour of the centre of the distribution as opposed to its tails".&lt;ref name=LC1986/&gt; The abstract of the paper ''On the central limit theorem of calculus of probability and the problem of moments'' by Pólya&lt;ref name=Polya1920/&gt; in 1920 translates as follows.

{{quote|text=The occurrence of the Gaussian probability density {{math|1 {{=}} ''e''&lt;sup&gt;−''x''&lt;sup&gt;2&lt;/sup&gt;&lt;/sup&gt;}} in repeated experiments, in errors of measurements, which result in the combination of very many and very small elementary errors, in diffusion processes etc., can be explained, as is well-known, by the very same limit theorem, which plays a central role in the calculus of probability. The actual discoverer of this limit theorem is to be named Laplace; it is likely that its rigorous proof was first given by Tschebyscheff and its sharpest formulation can be found, as far as I am aware of, in an article by Liapounoff. […] }}

A thorough account of the theorem's history, detailing Laplace's foundational work, as well as [[Augustin Louis Cauchy|Cauchy]]'s, [[Friedrich Bessel|Bessel]]'s and [[Siméon Denis Poisson|Poisson]]'s contributions, is provided by Hald.&lt;ref name=Hald/&gt; Two historical accounts, one covering the development from Laplace to Cauchy, the second the contributions by [[Richard von Mises|von Mises]], [[George Pólya|Pólya]], [[Jarl Waldemar Lindeberg|Lindeberg]], [[Paul Lévy (mathematician)|Lévy]], and [[Harald Cramér|Cramér]] during the 1920s, are given by Hans Fischer.&lt;ref name=Fischer/&gt; Le Cam describes a period  around 1935.&lt;ref name=LC1986/&gt; Bernstein&lt;ref name=Bernstein/&gt; presents a historical discussion focusing on the work of [[Pafnuty Chebyshev]] and his students [[Andrey Markov]] and [[Aleksandr Lyapunov]] that led to the first proofs of the CLT in a general setting.

Through the 1930s, progressively more general proofs of the Central Limit Theorem were presented. Many natural systems were found to exhibit [[Normal distribution|Gaussian distributions]] — a typical example being height distributions for humans. When statistical methods such as analysis of variance became established in the early 1900s, it became increasingly common to assume underlying Gaussian distributions.&lt;ref&gt;{{cite book|last=Wolfram|first=Stephen|title=A New Kind of Science|publisher=Wolfram Media, Inc.|year=2002|page=977|isbn=1-57955-008-8}}&lt;/ref&gt;

A curious footnote to the history of the Central Limit Theorem is that a proof of a result similar to the 1922 Lindeberg CLT was the subject of [[Alan Turing]]'s 1934 Fellowship Dissertation for [[King's College, Cambridge|King's College]] at the [[University of Cambridge]].  Only after submitting the work did Turing learn it had already been proved. Consequently, Turing's dissertation was not published.&lt;ref&gt;{{cite book|last=Hodges |first=Andrew | authorlink=Andrew Hodges | date=1983 |title=[[Alan Turing: The Enigma]] |location=London |publisher=Burnett Books |pages= 87–88 |isbn=0091521300}}&lt;/ref&gt;&lt;ref name=Zabell/&gt;&lt;ref name=Aldrich/&gt;

==See also==
* [[Asymptotic equipartition property]]
* [[Asymptotic distribution]]
* [[Benford's law]] – Result of extension of CLT to product of random variables.
* [[Berry–Esseen theorem]]
* [[Central limit theorem for directional statistics]] – Central limit theorem applied to the case of directional statistics
* [[Delta method]] – to compute the limit distribution of a function of a random variable.
* [[Erdős–Kac theorem]] – connects the number of prime factors of an integer with the normal probability distribution
* [[Fisher–Tippett–Gnedenko theorem]] – limit theorem for extremum values (such as {{math|max{''X&lt;sub&gt;n&lt;/sub&gt;''}}})
* [[Tweedie distribution|Tweedie convergence theorem]] – A theorem that can be considered to bridge between the central limit theorem and the [[Poisson convergence theorem]]&lt;ref name="Jørgensen-1997"&gt;{{cite book| last= Jørgensen|first= Bent | year = 1997| title = The Theory of Dispersion Models| publisher = Chapman &amp; Hall | isbn = 978-0412997112}}&lt;/ref&gt;
* [[Irwin–Hall distribution]]
* [[Bates distribution]]
* [[Normal distribution]]

==Notes==
{{reflist|30em|refs=

&lt;ref name=ABBN&gt;{{Citation| first1= S.|last1= Artstein |author1-link=Shiri Artstein| first2= K. |last2= Ball |author2-link=Keith Martin Ball|first3= F. |last3= Barthe|author3-link=Franck Barthe|first4= A. |last4= Naor|author4-link=Assaf Naor |year=2004 |url=http://www.ams.org/jams/2004-17-04/S0894-0347-04-00459-X/home.html |title=Solution of Shannon's Problem on the Monotonicity of Entropy |journal=Journal of the American Mathematical Society  |volume=17 |pages= 975–982| doi= 10.1090/S0894-0347-04-00459-X| issue= 4 }}&lt;/ref&gt;

&lt;ref name=Aldrich&gt;{{cite journal|last=Aldrich |first=John |date=2009 |title=England and Continental Probability in the Inter-War Years |journal=Electronic Journ@l for History of Probability and Statistics |volume=5|issue=2 |url=http://www.jehps.net/decembre2009.html |at=Section 3}}&lt;/ref&gt;

&lt;ref name=Bernstein&gt;{{cite book|authorlink=Sergei Natanovich Bernstein|last=Bernstein|first=S. N. |date=1945 |contribution=On the work of P. L. Chebyshev in Probability Theory |title=Nauchnoe Nasledie P. L. Chebysheva. Vypusk Pervyi: Matematika |language=Russian |trans-title=The Scientific Legacy of P. L. Chebyshev. Part I: Mathematics |editor-first=S. N.|editor-last= Bernstein. |publisher=Academiya Nauk SSSR |location=Moscow &amp; Leningrad |page=174}}&lt;/ref&gt;

&lt;ref name=Fischer&gt;{{cite book|last=Fischer|first=Hans|title=A History of the Central Limit Theorem: From Classical to Modern Probability Theory|series=Sources and Studies in the History of Mathematics and Physical Sciences|year=2011|publisher=Springer|isbn=978-0-387-87856-0|doi=10.1007/978-0-387-87857-7|location=New York|zbl=1226.60004|mr=2743162}} (Chapter 2: The Central Limit Theorem from Laplace to Cauchy: Changes in Stochastic Objectives and in Analytical Methods, Chapter 5.2: The Central Limit Theorem in the Twenties)&lt;/ref&gt;

&lt;ref name=LC1986&gt;{{cite journal| authorlink=Lucien Le Cam|last=Le Cam |first= Lucien|year=1986 |url=http://projecteuclid.org/euclid.ss/1177013818 |title=The central limit theorem around 1935  |journal=Statistical Science |volume=1|issue=1|pages=78–91|doi=10.2307/2245503}}&lt;/ref&gt;

&lt;ref name=Hald&gt;{{cite book|last=Hald |first=Andreas |url=http://www.gbv.de/dms/goettingen/229762905.pdf |title=A History of Mathematical Statistics from 1750 to 1930|isbn=978-0471179122|website=Gbv.de|at=chapter 17}}&lt;/ref&gt;

&lt;ref name=Meckes&gt;{{Cite journal|last=Meckes|first= Elizabeth|year=2008|title=Linear functions on the classical matrix groups|journal=Transactions of the American Mathematical Society|volume=360|pages=5355–5366|doi=10.1090/S0002-9947-08-04444-9|issue=10 |arxiv=math/0509441 }}&lt;/ref&gt;

&lt;ref name=Rempala&gt;{{cite journal | last1 = Rempala | first1 = G. | last2 = Wesolowski | first2 = J. | year = 2002 | title = Asymptotics of products of sums and ''U''-statistics | url = http://www.math.washington.edu/~ejpecp/EcpVol7/paper5.pdf | format = PDF | journal = Electronic Communications in Probability | volume = 7 | issue = | pages = 47–54 | doi=10.1214/ecp.v7-1046}}&lt;/ref&gt;

&lt;ref name=Tijms&gt;{{Cite book | first=Tijms |last= Henk |year=2004 |title= Understanding Probability: Chance Rules in Everyday Life|location= Cambridge |publisher= Cambridge University Press | isbn= 0-521-54036-4 | page=169}}&lt;/ref&gt;

&lt;ref name=Zabell&gt;{{cite book|last=Zabell |first=S. L. |date=2005 |title=Symmetry and Its Discontents: Essays on the History of Inductive Probability |publisher=Cambridge University Press |ISBN= 0-521-44470-5 |page=199}}&lt;/ref&gt;

&lt;ref name=Zygmund&gt;{{cite book|last=Zygmund|first=Antoni|authorlink=Antoni Zygmund|orig-year=1959|title=Trigonometric Series|publisher=Cambridge University Press|date=2003 |ISBN= 0-521-89053-5 |at=vol. II, sect. XVI.5, Theorem 5-5}}&lt;/ref&gt;
}}

==References==
*{{cite journal|last1=Bárány|first1=Imre|authorlink1=Imre Bárány|last2=Vu|first2=Van|year=2007|title=Central limit theorems for Gaussian polytopes|journal=Annals of Probability|publisher=Institute of Mathematical Statistics|volume=35|issue=4|pages=1593–1621|doi=10.1214/009117906000000791 |arxiv=math/0610192 }}
*{{cite book|last=Bauer|first=Heinz|title=Measure and Integration Theory|publisher=de Gruyter|location=Berlin|year=2001|isbn=3110167190}}
*{{cite book|last=Billingsley|first=Patrick|title=Probability and Measure|edition=3rd|publisher=John Wiley &amp; Sons|year=1995|isbn=0-471-00710-2}}
*{{cite book|last=Bradley|first=Richard|title=Introduction to Strong Mixing Conditions |edition=1st|year=2007|isbn=0-9740427-9-X|publisher=Kendrick Press|location=Heber City, UT}}
*{{cite journal|last=Bradley|first=Richard|title=Basic Properties of Strong Mixing Conditions. A Survey and Some Open Questions
|journal=Probability Surveys|year=2005|volume=2|pages=107–144 |arxiv=math/0511078v1 |doi=10.1214/154957805100000104 |url=https://arxiv.org/pdf/math/0511078.pdf}}
*{{cite journal|last1=Dinov|first1=Ivo|last2=Christou|first2=Nicolas| last3=Sanchez|first3=Juana |year=2008|title=Central Limit Theorem: New SOCR Applet and Demonstration Activity|journal=Journal of Statistics Education|publisher=ASA|volume=16|issue=2 |url=http://www.amstat.org/publications/jse/v16n2/dinov.html|doi=10.1080/10691898.2008.11889560}}
*{{Cite book|last=Durrett|first=Richard|authorlink=Rick Durrett|title=Probability: theory and examples|edition=3rd|year=2004|publisher= Cambridge University Press|isbn=0521765390}}
*{{cite journal|last=Gaposhkin|first=V. F.|year=1966|title=Lacunary series and independent functions|journal=Russian Mathematical Surveys|volume=21|issue=6|pages=1–82| doi=10.1070/RM1966v021n06ABEH001196|bibcode=1966RuMaS..21....1G}}.
* &lt;cite id=CITEREFKlartag2007&gt;{{cite journal|last=Klartag |first=Bo'az |date=2007 |title=A central limit theorem for convex sets |journal=Inventiones Mathematicae |volume=168 |pages=91–131 |doi=10.1007/s00222-006-0028-8 |arxiv=math/0605014|bibcode=2007InMat.168...91K }}&lt;/cite&gt;
* &lt;cite id=CITEREFKlartag2008&gt;{{cite journal|last=Klartag |first=Bo'az |date=2008 |title=A Berry–Esseen type inequality for convex bodies with an unconditional basis |journal=Probability Theory and Related Fields |doi=10.1007/s00440-008-0158-6 |arxiv=0705.0832 |volume=145 |pages=1–33}}&lt;/cite&gt;

==External links==
{{commons category}}
* Simplified, step-by-step explanation of the classical  [http://www.quantumfieldtheory.info/CentralLimitTheorem.pdf  Central Limit Theorem.] with histograms at every step.
* Hands-on explanation of the  [https://www.khanacademy.org/math/probability/statistics-inferential/sampling_distribution/v/central-limit-theorem Central Limit Theorem in tutorial videos from Khan Academy], with many examples
* [http://blog.vctr.me/posts/central-limit-theorem.html Central Limit Theorem Visualized in D3] interactive HTML5 simulation of flipping coins.
*{{springer|title=Central limit theorem|id=p/c021180}}
*[https://statistical-engineering.com/clt-summary/ Animated examples of the CLT]
*[http://www.vias.org/simulations/simusoft_cenlimit.html Central Limit Theorem] interactive simulation to experiment with various parameters
*[http://ccl.northwestern.edu/curriculum/ProbLab/CentralLimitTheorem.html CLT in NetLogo (Connected Probability &amp;mdash; ProbLab)] interactive simulation with a variety of modifiable parameters
*[http://wiki.stat.ucla.edu/socr/index.php/SOCR_EduMaterials_Activities_GeneralCentralLimitTheorem General Central Limit Theorem Activity] &amp; corresponding [http://www.socr.ucla.edu/htmls/SOCR_Experiments.html SOCR CLT Applet] (Select the Sampling Distribution CLT Experiment from the drop-down list of [http://wiki.stat.ucla.edu/socr/index.php/About_pages_for_SOCR_Experiments SOCR Experiments])
*[http://www.indiana.edu/~jkkteach/ExcelSampler/ Generate sampling distributions in Excel] Specify arbitrary population, [[sample size]], and sample statistic.
* MIT OpenCourseWare Lecture 18.440 ''Probability and Random Variables'', Spring 2011, Scott Sheffield [https://web.archive.org/web/20120529212657/http://ocw.mit.edu/courses/mathematics/18-440-probability-and-random-variables-spring-2011/lecture-notes/MIT18_440S11_Lecture31.pdf Another proof.] Retrieved 2012-04-08.
*[http://www.causeweb.org CAUSEweb.org] is a site with many resources for teaching statistics including the Central Limit Theorem
* [http://demonstrations.wolfram.com/TheCentralLimitTheorem/ The Central Limit Theorem] by Chris Boucher, [[Wolfram Demonstrations Project]].
* {{MathWorld |title=Central Limit Theorem |urlname=CentralLimitTheorem}}
* [https://web.archive.org/web/20081102151742/http://animation.yihui.name/prob%3Acentral_limit_theorem Animations for the Central Limit Theorem] by Yihui Xie using the [[R (programming language)|R]] package [https://cran.r-project.org/package=animation animation]
* Teaching demonstrations of the CLT: clt.examp function in {{cite book|author=Greg Snow|year=2012|title= TeachingDemos: Demonstrations for teaching and learning. R package version 2.8.|url= https://cran.r-project.org/package=TeachingDemos|ref=harv}}

{{statistics}}

{{DEFAULTSORT:Central Limit Theorem}}
[[Category:Probability theorems]]
[[Category:Statistical theorems]]
[[Category:Articles containing proofs]]
[[Category:Central limit theorem| ]]
[[Category:Asymptotic theory (statistics)]]</text>
      <sha1>9yt5w9ij5gq9altvguneqpjdedzd9iv</sha1>
    </revision>
  </page>
  <page>
    <title>Characterization of probability distributions</title>
    <ns>0</ns>
    <id>53165511</id>
    <revision>
      <id>868848183</id>
      <parentid>863655328</parentid>
      <timestamp>2018-11-14T20:58:51Z</timestamp>
      <contributor>
        <username>Texvc2LaTeXBot</username>
        <id>33995001</id>
      </contributor>
      <minor/>
      <comment>Replacing deprecated latex syntax [[mw:Extension:Math/Roadmap]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7112">In mathematics in general, a [[characterization theorem]] says that a particular object – a function, a space, etc. – is the only one that possesses properties specified in the theorem. A '''characterization of a probability distribution''' accordingly states that it is the only [[probability distribution]] that satisfies specified conditions. More precisely, the model of characterization of
probability distribution was described by [https://ru.wikipedia.org/wiki/%D0%97%D0%BE%D0%BB%D0%BE%D1%82%D0%B0%D1%80%D1%91%D0%B2,_%D0%92%D0%BB%D0%B0%D0%B4%D0%B8%D0%BC%D0%B8%D1%80_%D0%9C%D0%B8%D1%85%D0%B0%D0%B9%D0%BB%D0%BE%D0%B2%D0%B8%D1%87 V.M. Zolotarev] &lt;ref&gt;[https://www.researchgate.net/publication/231029496 V.M. Zolotarev (1976). Metric distances in spaces of random variables and their distributions. Matem. Sb., 101 (143), 3 (11)(1976)]&lt;/ref&gt; in such manner. On the probability space we define the space &lt;math&gt; \mathcal{X}=\{ X \} &lt;/math&gt; of random variables with values in measurable metric space &lt;math&gt;(U,d_{u})&lt;/math&gt; and the space &lt;math&gt; \mathcal{Y}=\{ Y \} &lt;/math&gt; of random variables with values in measurable metric space &lt;math&gt;(V,d_{v})&lt;/math&gt;. By characterizations of probability distributions we understand general problems of description of some set &lt;math&gt; \mathcal{C}&lt;/math&gt; in the space &lt;math&gt; \mathcal{X}&lt;/math&gt; by extracting the sets &lt;math&gt; \mathcal{A} \subseteq \mathcal{X} &lt;/math&gt; and &lt;math&gt; \mathcal{B} \subseteq \mathcal{Y} &lt;/math&gt; which describe the properties of random variables &lt;math&gt; X \in\mathcal{A}&lt;/math&gt; and their images &lt;math&gt; Y=\mathbf{F}X \in \mathcal{B} &lt;/math&gt;, obtained by means of a specially chosen mapping &lt;math&gt; \mathbf{F}:\mathcal{X} \to \mathcal{Y} &lt;/math&gt;.
&lt;br&gt;The description of the properties of the random variables &lt;math&gt;X&lt;/math&gt; and of their images &lt;math&gt; Y=\mathbf{F}X &lt;/math&gt; is equivalent to the indication of the set &lt;math&gt; \mathcal{A} \subseteq \mathcal{X} &lt;/math&gt; from which  &lt;math&gt;X&lt;/math&gt;  must be taken and of the set &lt;math&gt; \mathcal{B} \subseteq \mathcal{Y} &lt;/math&gt; into which its image must fall. So, the set which interests us appears therefore in the following form:
:&lt;math&gt;
X\in\mathcal{A},   \mathbf{F} X \in \mathcal{B} \Leftrightarrow X \in \mathcal{C}, i.e. \mathcal{C} = \mathbf{F}^{-1} \mathcal{B},
&lt;/math&gt;

where &lt;math&gt; \mathbf{F}^{-1} \mathcal{B}&lt;/math&gt; denotes the complete inverse image of &lt;math&gt; \mathcal{B}&lt;/math&gt; in &lt;math&gt; \mathcal{A}&lt;/math&gt;. This is the general model of characterization of probability distribution. Some examples of characterization theorems:

* The assumption that two linear (or non-linear) statistics are identically distributed (or independent, or have a constancy regression and so on) can be used to characterize various populations.&lt;ref name=populations&gt;A. M. Kagan, Yu. V. Linnik and C. Radhakrishna Rao (1973). [https://www.researchgate.net/publication/44720749_Characterization_problems_in_mathematical_statistics_by_A_M_Kagan_Yu_V_Linnik_and_C_Radhakrishna_Raotranslated_from_Russian_text_by_B_Ramachandran Characterization Problems in Mathematical Statistics]. John Wiley and Sons, New York, XII+499 pages.&lt;/ref&gt; For example, according to [[George Pólya|George Pólya's]] &lt;ref&gt;[[George Pólya|Pólya, Georg (1923).]][http://eudml.org/doc/167748 "Herleitung des Gaußschen Fehlergesetzes ans einer Funktionalgleichung".] Mathematische Zeitschrift. 18: 96–108. {{ISSN|0025-5874}}; 1432–1823.&lt;/ref&gt; characterization theorem, if &lt;math&gt;X_1&lt;/math&gt; and &lt;math&gt;X_2&lt;/math&gt; are [[independence (probability theory)|independent]] [[identically distributed]] [[random variable]]s with finite [[variance]], then the statistics &lt;math&gt; S_1 = X_1 &lt;/math&gt; and &lt;math&gt;  S_2 = \cfrac{X_1 + X_2}{\sqrt{2}}&lt;/math&gt; are identically distributed if and only if &lt;math&gt; X_1 &lt;/math&gt; and &lt;math&gt; X_2 &lt;/math&gt; have a &lt;u&gt;normal distribution&lt;/u&gt; with zero mean. In this case 
::&lt;math&gt;
\mathbf{F} = \begin{bmatrix}
1 &amp; 0  \\
1/\sqrt{2} &amp; 1/\sqrt{2}
\end{bmatrix}
&lt;/math&gt;,
:&lt;math&gt; \mathcal{A}&lt;/math&gt; is a set of random two-dimensional column-vectors with independent identically distributed components, &lt;math&gt; \mathcal{B}&lt;/math&gt; is a set of random two-dimensional column-vectors with identically distributed components and &lt;math&gt; \mathcal{C}&lt;/math&gt; is a set of two-dimensional column-vectors with independent identically distributed normal components.
* According to generalized [[George Pólya|George Pólya's]] characterization theorem (without condition on finiteness of variance &lt;ref name=populations/&gt;) if &lt;math&gt;X_1 , X_2 , \dots, X_n&lt;/math&gt; are non-degenerate independent identically distributed random variables, statistics &lt;math&gt;X_1&lt;/math&gt; and &lt;math&gt; a_1X_1 + a_2X_2 + \dots + a_nX_n&lt;/math&gt; are identically distributed and &lt;math&gt;\left | a_j \right \vert &lt; 1, a_1^2 + a_2^2 + \dots + a_n^2 = 1 &lt;/math&gt;, then &lt;math&gt; X_j &lt;/math&gt; is normal random variable for any &lt;math&gt; j, j=1,2, \dots, n &lt;/math&gt;. In this case
::&lt;math&gt;
\mathbf{F} = \begin{bmatrix}
1 &amp; 0 &amp; \dots &amp; 0\\
a_1 &amp; a_2 &amp; \dots &amp; a_n
\end{bmatrix}
&lt;/math&gt;,

:&lt;math&gt; \mathcal{A}&lt;/math&gt; is a set of random ''n''-dimensional column-vectors with independent identically distributed components,  &lt;math&gt; \mathcal{B}&lt;/math&gt; is a set of random two-dimensional column-vectors with identically distributed components and &lt;math&gt; \mathcal{C}&lt;/math&gt; is a set of ''n''-dimensional column-vectors with independent identically distributed normal components.&lt;ref&gt;[[Romanas Januškevičius|R. Yanushkevichius.]][https://www.academia.edu/24648381/R.Yanushkevichius._Stability_characterizations_of_distributions._1  Stability for characterizations of distributions.] Vilnius, Mokslas, 1991.&lt;/ref&gt;
* All probability distributions on the half-line &lt;math&gt;\left [ 0, \infty \right )&lt;/math&gt; that are [[memorylessness|memoryless]] are [[exponential distribution]]s. "Memoryless" means that if &lt;math&gt;X&lt;/math&gt; is a random variable with such a distribution, then for any numbers &lt;math&gt; 0 &lt; y &lt; x &lt;/math&gt; ,
:: &lt;math&gt; \Pr(X &gt; x\mid X&gt;y) = \Pr(X&gt;x-y) &lt;/math&gt;.
&lt;br&gt;
Verification of conditions of characterization theorems in practice is possible only with some error &lt;math&gt;\epsilon &lt;/math&gt;, i.e., only to a certain degree of accuracy.&lt;ref&gt;[[Romanas Januškevičius|R. Yanushkevichius.]][https://www.academia.edu/24648382/R.Yanushkevichius._Stability_characterizations_of_distributions._2 Stability characterizations of some probability distributions.] Saarbrücken, LAP LAMBERT Academic Publishing, 2014.&lt;/ref&gt; Such a situation is observed, for instance, in the cases where a sample of finite size is considered. That is why there arises the following natural question. Suppose that the conditions of the characterization theorem are fulfilled not exactly but only approximately. May we assert that the conclusion of the theorem is also fulfilled approximately? The theorems in which the problems of this kind are considered are called stability characterizations of probability distributions.

== See also ==

* [[Characterization (mathematics)]]

== References ==
{{reflist}}

[[Category:Probability theorems]]
[[Category:Statistical theorems]]
[[Category:Characterization of probability distributions]]</text>
      <sha1>1sxbtzgqj3nejw0fjm57frbzvg6tslh</sha1>
    </revision>
  </page>
  <page>
    <title>Cohomotopy group</title>
    <ns>0</ns>
    <id>682693</id>
    <revision>
      <id>840668776</id>
      <parentid>840668133</parentid>
      <timestamp>2018-05-11T10:50:27Z</timestamp>
      <contributor>
        <username>Turgidson</username>
        <id>1747755</id>
      </contributor>
      <comment>/* Properties */ more tex</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3447">{{refimprove|date=July 2014}}

In [[mathematics]], particularly [[algebraic topology]], '''cohomotopy sets''' are particular [[category theory|contravariant functors]] from the [[category theory|category]] of pointed [[topological space]]s and point-preserving [[continuous function|continuous]] maps to the category of [[Set (mathematics)|sets]] and [[Function (mathematics)|functions]]. They are [[duality (mathematics)|dual]] to the [[homotopy groups]], but less studied.

==Overview==
The ''p''-th cohomotopy set of a pointed [[topological space]] ''X'' is defined by

:&lt;math&gt;\pi^p(X) = [X,S^p]&lt;/math&gt;

the set of pointed [[homotopy]] classes of continuous mappings from &lt;math&gt;X&lt;/math&gt; to the ''p''-[[hypersphere|sphere]] &lt;math&gt;S^p&lt;/math&gt;.  For ''p=1'' this set has an [[abelian group]] structure, and, provided &lt;math&gt;X&lt;/math&gt; is a [[CW-complex]], is isomorphic to the first [[cohomology]] group &lt;math&gt;H^1(X)&lt;/math&gt;, since the circle &lt;math&gt;S^1&lt;/math&gt; is an [[Eilenberg–MacLane space]] of type &lt;math&gt;K(\mathbb{Z},1)&lt;/math&gt;. In fact, it is a theorem of [[Heinz Hopf]] that if &lt;math&gt;X&lt;/math&gt; is a [[CW-complex]] of dimension at most ''p'', then  &lt;math&gt;[X,S^p]&lt;/math&gt; is in bijection with the ''p''-th cohomology group &lt;math&gt;H^p(X)&lt;/math&gt;.

The set &lt;math&gt;[X,S^p]&lt;/math&gt; also has a natural group structure if &lt;math&gt;X&lt;/math&gt; is a [[suspension (topology)|suspension]] &lt;math&gt;\Sigma Y&lt;/math&gt;, such as a sphere &lt;math&gt;S^q&lt;/math&gt; for &lt;math&gt;q \ge 1&lt;/math&gt;.

If ''X'' is not homotopy equivalent to a CW-complex, then &lt;math&gt;H^1(X)&lt;/math&gt; might not be isomorphic to &lt;math&gt;[X,S^1]&lt;/math&gt;. A counterexample is given by the [[Warsaw circle]], whose first cohomology group vanishes, but admits a map to &lt;math&gt;S^1&lt;/math&gt; which is not homotopic to a constant map &lt;ref&gt;[http://math.ucr.edu/~res/math205B-2012/polishcircle.pdf Polish Circle]. Retrieved July 17, 2014.&lt;/ref&gt;

==Properties==
Some basic facts about cohomotopy sets, some more obvious than others:

* &lt;math&gt;\pi^p(S^q) = \pi_q(S^p)&lt;/math&gt; for all ''p'' and ''q''.
* For &lt;math&gt;q= p + 1&lt;/math&gt; or &lt;math&gt;p +2 \ge 4&lt;/math&gt;, the group &lt;math&gt;\pi^p(S^q)&lt;/math&gt; is equal to &lt;math&gt;\mathbb{Z}_2&lt;/math&gt;. (To prove this result, [[Lev Pontryagin]] developed the concept of framed [[cobordism]].)
* If &lt;math&gt;f,g\colon X \to S^p&lt;/math&gt; has &lt;math&gt;||f(x) - g(x)|| &lt; 2&lt;/math&gt; for all ''x'', then &lt;math&gt;[f] = [g]&lt;/math&gt;, and the homotopy is smooth if ''f'' and ''g'' are.
* For &lt;math&gt;X&lt;/math&gt; a compact smooth manifold, &lt;math&gt;\pi^p(X)&lt;/math&gt; is isomorphic to the set of homotopy classes of [[smooth function|smooth]] maps &lt;math&gt;X \to S^p&lt;/math&gt;; in this case, every continuous map can be uniformly approximated by a smooth map and any homotopic smooth maps will be smoothly homotopic.
* If &lt;math&gt;X&lt;/math&gt; is an &lt;math&gt;m&lt;/math&gt;-[[manifold]], then &lt;math&gt;\pi^p(X)=0&lt;/math&gt; for &lt;math&gt;p &gt; m&lt;/math&gt;.
* If &lt;math&gt;X&lt;/math&gt; is an &lt;math&gt;m&lt;/math&gt;-[[manifold#Manifold with boundary|manifold with boundary]], the set &lt;math&gt;\pi^p(X,\partial X)&lt;/math&gt; is [[natural isomorphism|canonically]] in [[bijection]] with the set of cobordism classes of [[codimension]]-''p'' framed submanifolds of the [[Interior (topology)|interior]] &lt;math&gt;X \setminus \partial X&lt;/math&gt;.
* The [[stable cohomotopy group]] of &lt;math&gt;X&lt;/math&gt; is the [[colimit]]
:&lt;math&gt;\pi^p_s(X) = \varinjlim_k{[\Sigma^k X, S^{p+k}]}&lt;/math&gt;
:which is an [[abelian group]].

==References==
{{Reflist}}

{{DEFAULTSORT:Cohomotopy Group}}
[[Category:Homotopy theory]]

{{topology-stub}}</text>
      <sha1>i2f7dl3lvf876noamoj13m8hr1wuxd7</sha1>
    </revision>
  </page>
  <page>
    <title>Complex-valued function</title>
    <ns>0</ns>
    <id>4415007</id>
    <revision>
      <id>859377929</id>
      <parentid>843485929</parentid>
      <timestamp>2018-09-13T17:26:59Z</timestamp>
      <contributor>
        <ip>177.142.22.210</ip>
      </contributor>
      <comment>word choice</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4010">{{refimprove|date=June 2013}}
[[Image:Exponentials_of_complex_number_within_unit_circle.svg|thumb|right|320px|An [[exponentiation|exponential]] function {{math|''A''&lt;sup&gt;''n''&lt;/sup&gt;}} of a discrete ([[integer]]) variable {{mvar|n}}, similar to [[geometric progression]]]]&lt;!-- I detest clueless Fr.wikipedia with their plain-ISO-8859-text-only notation: can somebody find an image of the same, but with a good symbolic notation? --&gt;
In mathematics, a '''complex-valued function''' (not to be confused with '''complex variable function''') is a [[function (mathematics)|function]] whose [[value (mathematics)| values]] are [[complex number]]s. Its domain does not necessarily have any [[mathematical structure|structure]] related to complex numbers. Most important uses of such functions [[#Complex analysis|in complex analysis]] and [[#Functional analysis|in functional analysis]] are explained below.

A [[vector space]] and a [[commutative algebra]] of functions over complex numbers can be defined [[real-valued function#In general|in the same way as for real-valued functions]]. Also, any complex-valued function {{mvar|f}} on an arbitrary [[set (mathematics)|set]] {{mvar|X}} can be considered as an [[ordered pair]] of two [[real-valued function]]s: {{math|([[Real part|Re]]''f'', [[Imaginary part|Im]]''f'')}} or, alternatively, as a real-valued function {{mvar|φ}} on {{math|''X'' [[Cartesian product|×]] [[finite set|{0, 1}]]}} (the [[disjoint union]] of [[2 (number)|two]] copies of {{mvar|X}}) such that for any {{mvar|x}}:
[[Image:Euler's formula.svg|thumb|right|[[Euler's formula]] features a complex-valued function of a ''[[real number|real]]'' variable {{mvar|φ}}]]
:{{math|1=Re ''f''(''x'') = ''F''(''x'', 0)|size=120%}}
:{{math|1=Im''f''(''x'') = ''F''(''x'', 1)|size=120%}}
Some properties of complex-valued functions (such as [[measurable function|measurability]] and [[continuous function|continuity]]) are nothing more than [[real-valued function#Measurable|corresponding properties of real-valued functions]].

== Complex analysis ==
{{main|Complex analysis}}
Complex analysis considers [[holomorphic function]]s on [[complex manifold]]s, such as [[Riemann surface]]s. The property of [[analytic continuation]] makes them very dissimilar from [[smooth function]]s, for example. Namely, if a function defined in a [[neighborhood (mathematics)|neighborhood]] can be continued to a wider [[domain (mathematical analysis)|domain]], then this continuation is [[unique (mathematics)|unique]].

As real functions, any holomorphic function is infinitely smooth and [[analytic function|analytic]]. But there is much less freedom in construction of a holomorphic function than in one of a smooth function.
{{expand section|date=June 2013}}

== Functional analysis ==
Complex-valued [[Lp space|L&lt;sup&gt;2&lt;/sup&gt; spaces]] on [[measure (mathematics)|sets with a measure]] have a particular importance because they are [[Hilbert space]]s. They often appear in [[functional analysis]] (for example, in relation with [[Fourier transform]]) and [[operator theory]]. A major user of such spaces is [[quantum mechanics]], as [[wave function]]s.

The sets on which the complex-valued L&lt;sup&gt;2&lt;/sup&gt; is constructed have the potential to be more exotic than their real-valued analog. For example, complex-valued [[function space]]s are used in some branches of [[p-adic analysis|{{mvar|p}}-adic analysis]] for algebraic reasons: complex numbers form an [[algebraically closed field]] (which facilitates operator theory), whereas neither real numbers nor {{mvar|p}}-adic numbers are not.

Also, complex-valued [[continuous function]]s are an important example in the theory of [[C*-algebra]]s: see [[Gelfand representation]].

==See also==
* [[Function of a complex variable]], the dual concept
&lt;!--==Footnotes==
{{reflist}}--&gt;

==External links==
{{MathWorld |title=Complex Function |id=ComplexFunction}}

[[Category:Complex analysis]]
[[Category:Types of functions]]
[[Category:Functional analysis]]</text>
      <sha1>jv8v3gwxtdcg6obg2hwyjt6demhjima</sha1>
    </revision>
  </page>
  <page>
    <title>Computable number</title>
    <ns>0</ns>
    <id>6206</id>
    <revision>
      <id>859234069</id>
      <parentid>859227662</parentid>
      <timestamp>2018-09-12T17:34:41Z</timestamp>
      <contributor>
        <username>Deacon Vorbis</username>
        <id>29330520</id>
      </contributor>
      <comment>/* Implementation */ rm inline EL</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="18313">In [[mathematics]], '''computable numbers''' are the [[real numbers]] that can be computed to within any desired precision by a finite, terminating [[algorithm]]. They are also known as the '''recursive numbers''' or the '''computable reals''' or '''recursive reals'''.

Equivalent definitions can be given using [[μ-recursive function]]s, [[Turing machines]], or [[lambda calculus|λ-calculus]] as the formal representation of algorithms. The computable numbers form a [[real closed field]] and can be used in the place of real numbers for many, but not all, mathematical purposes.

==Informal definition using a Turing machine as example==
In the following, [[Marvin Minsky]] defines the numbers to be computed in a manner similar to those defined by [[Alan Turing]] in 1936; i.e., as "sequences of digits interpreted as decimal fractions" between 0 and 1:
 
:"A computable number [is] one for which there is a Turing machine which, given ''n'' on its initial tape, terminates with the ''nth'' digit of that number [encoded on its tape]." (Minsky 1967:159)

The key notions in the definition are (1) that some ''n'' is specified at the start, (2) for any ''n'' the computation only takes a finite number of steps, after which the machine produces the desired output and terminates.

An alternate form of (2) – the machine successively prints all n of the digits on its tape, halting after printing the n&lt;sup&gt;th&lt;/sup&gt; – emphasizes Minsky's observation: (3) That by use of a Turing machine, a ''finite'' definition – in the form of the machine's table – is being used to define what is a potentially-''infinite'' string of decimal digits.

This is however not the modern definition which only requires the result be accurate to within any given accuracy. The informal definition above is subject to a rounding problem called the [[table-maker's dilemma]] whereas the modern definition is not.

==Formal definition==
A [[real number]] ''a'' is '''computable''' if it can be approximated by some [[computable function]] &lt;math&gt;f:\mathbb{N}\to\mathbb{Z}&lt;/math&gt; in the following manner: given any positive [[integer]] ''n'', the function produces an integer ''f''(''n'') such that:

:&lt;math&gt;{f(n)-1\over n} \leq a \leq {f(n)+1\over n}.&lt;/math&gt;

There are two similar definitions that are equivalent:
*There exists a computable function which, given any positive rational [[error bound]] &lt;math&gt;\varepsilon&lt;/math&gt;, produces a [[rational number]] ''r'' such that &lt;math&gt;|r - a| \leq \varepsilon.&lt;/math&gt;
*There is a computable sequence of rational numbers &lt;math&gt;q_i&lt;/math&gt; converging to &lt;math&gt;a&lt;/math&gt; such that &lt;math&gt;|q_i - q_{i+1}| &lt; 2^{-i}\,&lt;/math&gt; for each ''i''.

There is another equivalent definition of computable numbers via computable [[Dedekind cut]]s. A '''computable Dedekind cut''' is a computable function &lt;math&gt;D\;&lt;/math&gt; which when provided with a rational number &lt;math&gt;r&lt;/math&gt; as input returns &lt;math&gt;D(r)=\mathrm{true}\;&lt;/math&gt; or &lt;math&gt;D(r)=\mathrm{false}\;&lt;/math&gt;, satisfying the following conditions:
:&lt;math&gt;\exists r D(r)=\mathrm{true}\;&lt;/math&gt;
:&lt;math&gt;\exists r D(r)=\mathrm{false}\;&lt;/math&gt;
:&lt;math&gt;(D(r)=\mathrm{true}) \wedge (D(s)=\mathrm{false}) \Rightarrow r&lt;s\;&lt;/math&gt;
:&lt;math&gt;D(r)=\mathrm{true} \Rightarrow \exist s&gt;r, D(s)=\mathrm{true}.\;&lt;/math&gt;
An example is given by a program ''D'' that defines the [[cube root]] of 3. Assuming &lt;math&gt;q&gt;0\;&lt;/math&gt; this is defined by:
:&lt;math&gt;p^3&lt;3 q^3 \Rightarrow D(p/q)=\mathrm{true}\;&lt;/math&gt;
:&lt;math&gt;p^3&gt;3 q^3 \Rightarrow D(p/q)=\mathrm{false}.\;&lt;/math&gt;

A real number is computable if and only if there is a computable Dedekind cut ''D'' corresponding to it. The function ''D'' is unique for each computable number (although of course two different programs may provide the same function).

A [[complex number]] is called computable if its real and imaginary parts are computable.

==Properties==

===Countable but not computably enumerable===
While the set of real numbers is [[uncountable]], the set of computable numbers is only [[countable]] and thus [[almost all]] real numbers are not computable. That the computable numbers are [[countable|at most countable]] intuitively comes from the fact that they are produced by Turing machines, of which there are only countably many.  More precisely, assigning a [[Gödel number]] to each Turing machine definition produces a subset &lt;math&gt;S&lt;/math&gt; of the [[natural numbers]] corresponding to the computable numbers and identifies a [[surjection]] from &lt;math&gt;S&lt;/math&gt; to the computable numbers, which shows that the computable numbers are [[subcountable]].  Moreover, for any computable number &lt;math&gt;x,&lt;/math&gt; the [[well ordering principle]] provides that there is a minimal element in &lt;math&gt;S&lt;/math&gt; which corresponds to &lt;math&gt;x&lt;/math&gt;, and therefore there exists a subset &lt;math&gt;S_1\subset S&lt;/math&gt; consisting of the minimal elements, on which the map is a [[bijection]].  The inverse of this bijection is an [[Injective function|injection]] into the natural numbers of the computable numbers, proving that they are countable.

The set &lt;math&gt;S&lt;/math&gt; of these Gödel numbers, however, is not [[computably enumerable]] (nor consequently is &lt;math&gt;S_1&lt;/math&gt;), even though the computable reals are themselves ordered. This is because there is no algorithm to determine which Gödel numbers correspond to Turing machines that produce computable reals. In order to produce a computable real, a Turing machine must compute a [[total function]], but the corresponding [[decision problem]] is in [[Turing degree]] '''0&amp;prime;&amp;prime;'''. Consequently, there is no surjective [[computable function]] from the natural numbers to the computable reals, and [[Cantor's diagonal argument]] cannot be used [[Constructivism (mathematics)|constructively]] to demonstrate uncountably many of them.

===Properties as a field===
The arithmetical operations on computable numbers are themselves computable in the sense that whenever real numbers ''a'' and ''b'' are computable then the following real numbers are also computable: ''a + b'', ''a - b'', ''ab'', and ''a/b'' if ''b'' is nonzero.
These operations are actually ''uniformly computable''; for example, there is a Turing machine which on input (''A'',''B'',&lt;math&gt;\epsilon&lt;/math&gt;) produces output ''r'', where ''A'' is the description of a Turing machine approximating ''a'', ''B'' is the description of a Turing machine approximating ''b'', and ''r'' is an &lt;math&gt;\epsilon&lt;/math&gt; approximation of ''a''+''b''.

The fact that computable real numbers form a field was first proved by [[Henry Gordon Rice]] (1954).

Computable reals do not form however a [[computable algebra|computable field]], because the definition of the latter notion requires effective equality.

===Non-computability of the ordering===
The order relation on the computable numbers is not computable. Let ''A'' be the description of a Turing machine approximating the number &lt;math&gt;a&lt;/math&gt;. Then there is no Turing machine which on input ''A'' outputs "YES" if &lt;math&gt;a &gt; 0&lt;/math&gt; and "NO" if &lt;math&gt;a \le 0.&lt;/math&gt; To see why, suppose the machine described by ''A'' keeps outputting 0 as &lt;math&gt;\epsilon&lt;/math&gt; approximations. It is not clear how long to wait before deciding that the machine will ''never'' output an approximation which forces ''a'' to be positive. Thus the machine will eventually have to guess that the number will equal 0, in order to produce an output; the sequence may later become different from 0. This idea can be used to show that the machine is incorrect on some sequences if it computes a total function. A similar problem occurs when the computable reals are represented as [[Dedekind cut]]s. The same holds for the equality relation : the equality test is not computable.

While the full order relation is not computable, the restriction of it to pairs of unequal numbers is computable. That is, there is a program that takes as input two Turing machines ''A'' and ''B'' approximating numbers ''a'' and ''b'', where ''a'' ≠ ''b'', and outputs whether &lt;math&gt;a &lt; b&lt;/math&gt; or &lt;math&gt;a &gt; b.&lt;/math&gt; It is sufficient to use ''ε''-approximations where &lt;math&gt; \varepsilon &lt; |b-a|/2,&lt;/math&gt; so by taking increasingly small ε (approaching 0), one eventually can decide whether &lt;math&gt;a &lt; b&lt;/math&gt; or &lt;math&gt;a &gt; b.&lt;/math&gt;

===Other properties===
The computable real numbers do not share all the properties of the real numbers used in analysis. For example, the least upper bound of a bounded increasing computable sequence of computable real numbers need not be a computable real number (Bridges and Richman, 1987:58). A sequence with this property is known as a [[Specker sequence]], as the first construction is due to E. Specker (1949). Despite the existence of counterexamples such as these, parts of calculus and real analysis can be developed in the field of computable numbers, leading to the study of [[computable analysis]].

Every computable number is [[definable number|definable]], but not vice versa. There are many definable, noncomputable real numbers, including:
*any number that encodes the solution of the [[halting problem]] (or any other [[undecidable problem]]) according to a chosen encoding scheme.
*[[Chaitin's constant]], &lt;math&gt;\Omega&lt;/math&gt;, which is a type of real number that is [[Turing degree|Turing equivalent]] to the halting problem.
Both of these examples in fact define an infinite set of definable, uncomputable numbers, one for each [[Universal Turing machine]].
A real number is computable if and only if the set of natural numbers it represents (when written in binary and viewed as a characteristic function) is computable.

Every computable number is [[arithmetical number|arithmetical]].

The set of computable real numbers (as well as every countable, [[densely ordered]] subset of computable reals without ends) is [[order-isomorphic]] to the set of rational numbers.

==Digit strings and the Cantor and Baire spaces==

Turing's original paper defined computable numbers as follows:

:A real number is computable if its digit sequence can be produced by some algorithm or Turing machine. The algorithm takes an integer &lt;math&gt;n \ge 1&lt;/math&gt; as input and produces the &lt;math&gt;n&lt;/math&gt;-th digit of the real number's decimal expansion as output.

(Note that the decimal expansion of ''a'' only refers to the digits following the decimal point.)

Turing was aware that this definition is equivalent to the &lt;math&gt;\epsilon&lt;/math&gt;-approximation definition given above. The argument proceeds as follows: if a number is computable in the Turing sense, then it is also computable in the &lt;math&gt;\epsilon&lt;/math&gt; sense: if &lt;math&gt;n &gt; \log_{10} (1/\epsilon)&lt;/math&gt;, then the first ''n'' digits of the decimal expansion for ''a'' provide an &lt;math&gt;\epsilon&lt;/math&gt; approximation of ''a''. For the converse, we pick an &lt;math&gt;\epsilon&lt;/math&gt; computable real number ''a'' and generate increasingly precise approximations until the ''n''th digit after the decimal point is certain. This always generates a decimal expansion equal to ''a'' but it may improperly end in an infinite sequence of 9's in which case it must have a finite (and thus computable) proper decimal expansion.

Unless certain topological properties of the real numbers are relevant it is often more convenient to deal with elements of &lt;math&gt;2^{\omega}&lt;/math&gt; (total 0,1 valued functions) instead of reals numbers in &lt;math&gt;[0,1]&lt;/math&gt;. The members of &lt;math&gt;2^{\omega}&lt;/math&gt; can be identified with binary decimal expansions but since the decimal expansions &lt;math&gt;.d_1d_2\ldots d_n0111\ldots&lt;/math&gt; and &lt;math&gt;.d_1d_2\ldots d_n10&lt;/math&gt; denote the same real number the interval &lt;math&gt;[0,1]&lt;/math&gt; can only be bijectively (and homeomorphically under the subset topology) identified with the subset of &lt;math&gt;2^{\omega}&lt;/math&gt; not ending in all 1's.

Note that this property of decimal expansions means it's impossible to effectively identify computable real numbers defined in terms of a decimal expansion and those defined in the &lt;math&gt;\epsilon&lt;/math&gt; approximation sense. Hirst has shown there is no algorithm which takes as input the description of a Turing machine which produces &lt;math&gt;\epsilon&lt;/math&gt; approximations for the computable number ''a'', and produces as output a Turing machine which enumerates the digits of ''a'' in the sense of Turing's definition (see Hirst 2007). Similarly it means that the arithmetic operations on the computable reals are not effective on their decimal representations as when adding decimal numbers, in order to produce one digit it may be necessary to look arbitrarily far to the right to determine if there is a carry to the current location. This lack of uniformity is one reason that the contemporary definition of computable numbers uses &lt;math&gt;\epsilon&lt;/math&gt; approximations rather than decimal expansions.

However, from a computational or measure theoretic perspective the two structures &lt;math&gt;2^{\omega}&lt;/math&gt; and &lt;math&gt;[0,1]&lt;/math&gt; are essentially identical, and computability theorists often refer to members of &lt;math&gt;2^{\omega}&lt;/math&gt; as reals. While &lt;math&gt;[0,1]&lt;/math&gt; &lt;math&gt;2^{\omega}&lt;/math&gt; is [[totally disconnected space|totally disconnected]] for questions about &lt;math&gt;\Pi^0_1&lt;/math&gt; classes or randomness it's much less messy to work in &lt;math&gt;2^{\omega}&lt;/math&gt;.

Elements of &lt;math&gt;\omega^{\omega}&lt;/math&gt; are sometimes called reals as well and though containing a homeomorphic image of &lt;math&gt;\mathbb{R}&lt;/math&gt; &lt;math&gt;\omega^{\omega}&lt;/math&gt; in addition to being totally disconnected isn't even locally compact. This leads to genuine differences in the computational properties. For instance the &lt;math&gt;x \in \mathbb{R}&lt;/math&gt; satisfying &lt;math&gt;\forall(n \in \omega)\phi(x,n)&lt;/math&gt; with &lt;math&gt;\phi(x,n)&lt;/math&gt; quantifier free must be computable while the unique &lt;math&gt;x \in \omega^{\omega}&lt;/math&gt; satisfying a universal formula can be arbitrarily high in the hyperarithmetic hierarchy.

==Can computable numbers be used instead of the reals?==

The computable numbers include many of the specific real numbers which appear in practice, including all real [[algebraic number]]s, as well as ''e'', &lt;math&gt;\pi&lt;/math&gt;, and many other [[transcendental number]]s. Though the computable reals exhaust those reals we can calculate or approximate, the assumption that all reals are computable leads to substantially different conclusions about the real numbers. The question naturally arises of whether it is possible to dispose of the full set of reals and use computable numbers for all of mathematics. This idea is appealing from a [[constructivism (mathematics)|constructivist]] point of view, and has been pursued by what [[Errett Bishop|Bishop]] and Richman call the ''Russian school'' of constructive mathematics.

To actually develop analysis over computable numbers, some care must be taken. For example, if one uses the classical definition of a sequence, the set of computable numbers is not closed under the basic operation of taking the [[supremum]] of a [[bounded sequence]] (for example, consider a [[Specker sequence]]). This difficulty is addressed by considering only sequences which have a computable [[modulus of convergence]]. The resulting mathematical theory is called [[computable analysis]].

==Implementation==

There are some computer packages that work with computable real numbers, representing the real numbers as programs computing approximations.  One example is the [[RealLib]] package.

==See also==
*[[Definable number]]
*[[Semicomputable function]]
*[[Transcomputational problem]]

==References==

*Oliver Aberth 1968, ''Analysis in the Computable Number Field'', Journal of the Association for Computing Machinery (JACM), vol 15, iss 2, pp 276–299. This paper describes the development of the calculus over the computable number field.
*Errett Bishop and Douglas Bridges, ''Constructive Analysis'', Springer, 1985, {{ISBN|0-387-15066-8}}
*Douglas Bridges and Fred Richman. ''Varieties of Constructive Mathematics'', Oxford, 1987.
*Jeffry L. Hirst, Representations of reals in reverse mathematics, Bulletin of the Polish Academy of Sciences, Mathematics, 55, (2007) 303&amp;ndash;316.
*[[Marvin Minsky]] 1967, ''Computation: Finite and Infinite Machines'', Prentice-Hall, Inc. Englewood Cliffs, NJ. No ISBN. Library of Congress Card Catalog No. 67-12342. His chapter §9 "The Computable Real Numbers" expands on the topics of this article.
*E. Specker, "Nicht konstruktiv beweisbare Sätze der Analysis" J. Symbol. Logic, 14 (1949) pp.&amp;nbsp;145–158
*{{Citation | last= Turing | first= A.M. | publication-date = 1937 | year = 1936 | title = On Computable Numbers, with an Application to the Entscheidungsproblem | periodical = Proceedings of the London Mathematical Society | series = 2 | volume = 42 | issue= 1 | pages = 230–65 | url = http://www.abelard.org/turpap2/tp2-ie.asp | doi= 10.1112/plms/s2-42.1.230 }} (and {{Citation | last = Turing | first = A.M. | publication-date = 1937 | title = On Computable Numbers, with an Application to the Entscheidungsproblem: A correction | periodical = Proceedings of the London Mathematical Society | series = 2 | volume = 43 | issue = 6 | pages = 544–6 | doi = 10.1112/plms/s2-43.6.544 | year = 1938 }}). Computable numbers (and Turing's a-machines) were introduced in this paper; the definition of computable numbers uses infinite decimal sequences.
*Klaus Weihrauch 2000, ''Computable analysis'', Texts in theoretical computer science, [[Springer Science+Business Media|Springer]], {{ISBN|3-540-66817-9}}. §1.3.2 introduces the definition by [[nested sequences of intervals]] converging to the singleton real. Other representations are discussed in §4.1.
*Klaus Weihrauch, ''[http://eccc.uni-trier.de/static/books/A_Simple_Introduction_to_Computable_Analysis_Fragments_of_a_Book/ A simple introduction to computable analysis]''
* H. Gordon Rice. "Recursive real numbers." Proceedings of the American Mathematical Society 5.5 (1954): 784-791.
* V. Stoltenberg-Hansen, J. V. Tucker "Computable Rings and Fields" in ''Handbook of computability theory'' edited by E.R. Griffor. Elsevier 1999

Computable numbers were defined independently by Turing, Post and Church. See ''The Undecidable'', ed. Martin Davis, for further original papers.

{{Number systems}}

{{DEFAULTSORT:Computable Number}}
[[Category:Computability theory]]
[[Category:Theory of computation]]</text>
      <sha1>oebc0m1ypcnr1lgx6nnqh544n0u3jfv</sha1>
    </revision>
  </page>
  <page>
    <title>Conrad Wolfram</title>
    <ns>0</ns>
    <id>23170285</id>
    <revision>
      <id>861406394</id>
      <parentid>858605509</parentid>
      <timestamp>2018-09-27T05:20:51Z</timestamp>
      <contributor>
        <ip>128.218.43.108</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11387">{{EngvarB|date=September 2014}}
{{Use dmy dates|date=September 2014}}
{{Infobox person
| name          = Conrad Wolfram
| image         = Transmediale-2010-Conrad Wolfram.jpg
| image_size    = 250px
| caption       = Conrad Wolfram in [[transmediale]] 10 talking about [[Wolfram Alpha]].
| birth_date    = {{birth date and age|df=yes|1970|06|10}}
| birth_place = {{nowrap|Oxford, England, United Kingdom}}
| residence     = United Kingdom
| nationality   = British
| occupation    = Strategic and international director, [[Wolfram Research]]
| alma_mater    = [[Pembroke College, Cambridge]]
| mother             =  [[Sybil Wolfram]]
| father             =  [[Hugo Wolfram]]
| known_for = Education reform
| website = {{startplainlist|class=nowrap}}
* {{URL|http://www.conradwolfram.com}}
* {{URL|twitter.com/conradwolfram}}
{{endplainlist}}
}}

'''Conrad Wolfram''' (born 10 June 1970) is a British [[Technology|technologist]] and businessman known for his work in information technology and its application.&lt;ref&gt;[https://www.simple-talk.com/opinion/geek-of-the-week/conrad-wolfram-geek-of-the-week/ Conrad Wolfram: Geek of the Week] 6 Feb 2015&lt;/ref&gt;&lt;ref&gt;[https://www.theguardian.com/technology/pda/2010/jan/25/google-bing-wolframalpha-search-engine Bing and WolframAlpha catching up with Google in search engine battle?]&lt;/ref&gt; In 2012, The Observer placed him at number 11 in its list of Britain's 50 New Radicals.&lt;ref&gt;[https://www.theguardian.com/theobserver/series/britain-s-new-radicals Britain's 40 New Radicals] The Observer&lt;/ref&gt;&lt;ref&gt;[http://www.nesta.org.uk/news_and_features/britains_new_radicals/conrad_wolfram Britain's 50 New Radicals] Nesta&lt;/ref&gt;

==Biography==
Wolfram's father [[Hugo Wolfram]] was a textile manufacturer and novelist (''Into a Neutral Country'') and his mother [[Sybil Wolfram]] was a professor of philosophy at the [[University of Oxford]]. He is the younger brother of [[Stephen Wolfram]].

Born in [[Oxford]], England, in 1970, Wolfram was educated at [[Dragon School]], [[Eton College]] and [[Pembroke College, Cambridge]],&lt;ref&gt;[http://www.news-gazette.com/business/2008/03/09/going_global_adds_up__champaignbasedWolfram giving away software in push into China], ''[[The News-Gazette (Champaign-Urbana)|The News Gazette]]'', 9 March 2008.&lt;/ref&gt; from which he holds an MA degree in [[Natural Sciences]] and Mathematics. He learned to program on a [[BBC Micro]].&lt;ref&gt;[https://www.bbc.co.uk/news/technology-15969065 The BBC Microcomputer and me, 30 years down the line] BBC News, 1 December 2011&lt;/ref&gt; He is married to primary care [[ophthalmology]] [[Consultant (medicine)|consultant]] Stella Hornby and has a daughter Sophia Wolfram.

==Mathematics education reform==
Wolfram has been a proponent of '[[Computer-Based Math]]'—a reform of mathematics education to "rebuild the curriculum assuming computers exist."&lt;ref&gt;[http://www.huffingtonpost.com/robyn-shulman-/have-we-gotten-math-educa_b_9655258.html Have We Gotten Math Education All Wrong] Huffington Post 11 April 2006&lt;/ref&gt;&lt;ref&gt;[https://www.ft.com/content/bcfb1f30-6cf9-11e5-8171-ba1968cf791a Stop teaching kids to add up - maths is more important] Financial Times, 7 October 2015&lt;/ref&gt;
&lt;ref&gt;[https://www.telegraph.co.uk/education/6719451/We-need-to-base-maths-lessons-on-computers.html We need to base maths lessons on computers], ''[[The Daily Telegraph]]'', 3 December 2009.&lt;/ref&gt;&lt;ref&gt;[https://www.nytimes.com/2013/02/11/world/europe/11iht-educside11.html Closing the gap between modern life and the math curriculum] New York Times, 10 February 2013&lt;/ref&gt; and is the founder of [http://computerbasedmath.org/ www.computerbasedmath.org].&lt;ref&gt;[http://computerbasedmath.org/ computerbasedmath.org].&lt;/ref&gt;&lt;ref&gt;[http://www.newstatesman.com/blogs/michael-brooks/2012/06/despair-dissenting-government-expert  The despair of the dissenting government expert] The New Statesman&lt;/ref&gt;

He argues, "There are a few cases where it is important to do calculations by hand, but these are small fractions of cases. The rest of the time you should assume that students should use a computer just like everyone does in the real world.".&lt;ref&gt;[http://www.channel4.com/news/articles/science_technology/introducing+the+apossatnavapos+maths+exam/3257252 Introducing the 'sat-nav' maths exam], ''[[Channel 4 News]]'', [[Channel 4]], UK&lt;/ref&gt; And that "School mathematics is very disconnected from mathematics used to solve problems in the real world".&lt;ref&gt;[http://economia.elpais.com/economia/2016/04/24/actualidad/1461527206_970734.html, Students fleeing mathematics] El Pais, Spain,&lt;/ref&gt; In an interview with the Guardian he described the replacement of hand calculation by computer use as "democratising expertise".&lt;ref&gt;[https://www.theguardian.com/technology/2009/sep/02/wolfram-alpha-online-search-engine-calculations WolframAlpha Online Search Engine Calculations], ''[[The Guardian]]'', 2 September 2009.&lt;/ref&gt; He argues that "A good guide to how and what you should do with a computer in the classroom is what you'd do with it outside. As much as possible, use real-world tools in the classroom in an open-ended way not special education-only closed-ended approaches." &lt;ref&gt;[https://edtechdigest.wordpress.com/2010/12/10/interview-doing-the-math-with-conrad-wolfram/ edtech digest] 10 Dec 2010&lt;/ref&gt;

In 2009, he spoke about education reform at the [[TEDx]] Conference at the [[EU Parliament]].&lt;ref&gt;[https://www.youtube.com/watch?v=TsvPE1EqwQ8&amp;feature=channel I calculate therefore I am], Conrad Wolfram, TedX Brussels, YouTube, 2009.&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://tedxbrussels.eu/site/speakers.html |title=Speakers at TEDxBurssels 2009 |publisher=Tedxbrussels.eu |date= |accessdate=16 November 2010 |deadurl=yes |archiveurl=https://web.archive.org/web/20110720172601/http://tedxbrussels.eu/site/speakers.html |archivedate=20 July 2011 |df=dmy-all }}&lt;/ref&gt; and again at [[TED (conference)|TED Global 2010]] where he argued that "Maths should be more practical and more conceptual, but less mechanical,"&lt;ref&gt;[https://www.wired.co.uk/news/archive/2010-07/19/conrad-wolfram-maths-curriculum Conrad Wolfram wants to reboot the maths curriculum] {{webarchive |url=https://web.archive.org/web/20100721135433/http://www.wired.co.uk/news/archive/2010-07/19/conrad-wolfram-maths-curriculum |date=21 July 2010 }}, ''[[Wired Magazine]]'', 19 July 2010.&lt;/ref&gt; and that "Calculating is the machinery of math - a means to an end."

In August 2012, he was a member of the judging panel at the [[Festival of Code]], the culmination of [[Young Rewired State]] 2012.&lt;ref&gt;[https://youngrewiredstate.org/past/festival-of-code-yrs2013/sponsors-and-judges-yrs2013 Past Judges] youngrewiredstate.org&lt;/ref&gt; Wolfram is also part of Flooved advisory board.&lt;ref&gt;[https://www.bloomberg.com/research/stocks/private/committees.asp?privcapId=247145889 Company Overview of Flooved Limited, Bloomberg. Retrieved 3/10/17.]&lt;/ref&gt;

==Work==
Conrad Wolfram founded Wolfram Research Europe Ltd.&lt;ref&gt;[http://www.channel4.com/news/articles/science_technology/wolfram+alpha+goes+live/3148457 Wolfram Alpha goes live], ''[[Channel 4 News]]'', [[Channel 4]], UK.&lt;/ref&gt; in 1991 and remains its CEO.&lt;ref&gt;{{cite web |first=Jamillah |last=Knowles |url=http://www.bbc.co.uk/blogs/podsandblogs/2009/05/guatemala_wolfram_alpha_acquin.shtml |title=Guatemala, Wolfram Alpha, Acquine and news |publisher=[[BBC Radio 5 Live|Radio 5]], [[BBC]], UK |date=15 September 2009 |accessdate=16 November 2010}}&lt;/ref&gt; In 1996, he additionally became Strategic and International Director&lt;ref&gt;[http://www.hindu.com/biz/2009/05/25/stories/2009052550071400.htm A computational knowledge engine for factual queries], ''[[The Hindu]]'', India, 25 May 2009.&lt;/ref&gt; of [[Wolfram Research]], Inc., making him also responsible for Wolfram Research Asia Ltd, and communications such as the wolfram.com website.

Wolfram Research was founded by his brother&lt;ref&gt;[https://www.telegraph.co.uk/scienceandtechnology/technology/5319890/Can-Wolfram-Alpha-take-on-Google.html Can Wolfram Take on Google?], ''[[The Daily Telegraph]]'', UK.&lt;/ref&gt; [[Stephen Wolfram]], the maker of [[Mathematica]] software and the [[Wolfram Alpha]] knowledge engine.&lt;ref&gt;[https://www.independent.co.uk/life-style/gadgets-and-tech/news/anthony-doesburg-wolfram-promises-new-way-to-probe-the-web-1694293.html Wolfram promises new way to probe the web], ''[[The Independent]]'', UK.&lt;/ref&gt;

Conrad Wolfram has led the effort to move the use of Mathematica from pure computation system to development and deployment engine,&lt;ref&gt;[http://insidehpc.com/2008/12/10/wolfram-releases-free-mathematica-player-7/ Wolfram Releases Mathematica Player 7], [http://insidehpc.com/ Inside HPC], 10 December 2008.&lt;/ref&gt;&lt;ref&gt;[http://www.ddj.com/hpc-high-performance-computing/212900768 Wolfram Releases Mathematic Player Pro 7], ''[[Dr Dobbs Journal]]''.&lt;/ref&gt; instigating technology such as the Mathematica Player family and web Mathematica and by pushing greater automation within the system.&lt;ref&gt;[http://www.mathematica-journal.com/issue/v8i4/newproducts/webmathematicaj.html webMathematica Japanese Edition: Unique Product Brings Computation to the Web], ''[[Mathematica Journal]]'', volume 8, issue 4.&lt;/ref&gt;

He has also led the focus on interactive publishing technology&lt;ref&gt;[https://www.nytimes.com/2011/12/18/business/online-textbooks-aim-to-make-science-leap-from-the-page.html?_r=3 Making Science Leap From the Page] The New York Times, 17 December 2011&lt;/ref&gt; with the stated aim of "making new applications as everyday as new documents"&lt;ref&gt;[http://140.177.205.236/2007/10/14/the-day-that-documents-and-applications-merged/ The day that documents and applications merged], Wolfram Research.&lt;/ref&gt; claiming that "If a picture is worth a thousand words, an interactive document is worth a thousand pictures."&lt;ref&gt;[http://www.wolfram.com/solutions/interactivedeployment/basics.html Interactive deployment], Wolfram Research.&lt;/ref&gt; These technologies converged to form the [[Computable Document Format]]&lt;ref&gt;[https://www.telegraph.co.uk/technology/news/8561619/Wolfram-Alpha-creator-plans-to-delete-the-PDF.html Wolfram Alpha Creator plans to delete the PDF] The Telegraph (UK)&lt;/ref&gt; which Wolfram says can "transfer knowledge in a much higher-bandwidth way".&lt;ref&gt;[https://www.wired.co.uk/news/archive/2012-06/28/conrad-wolfram-computation Conrad Wolfram: computation will release data overload] {{webarchive |url=https://web.archive.org/web/20120701000524/http://www.wired.co.uk/news/archive/2012-06/28/conrad-wolfram-computation |date=1 July 2012 }} Wired 28 June 2012&lt;/ref&gt;

==References==
{{reflist|colwidth=30em}}

==External links==
* [http://www.conradwolfram.com/ Conrad Wolfram personal website]
* [http://computerbasedmath.org/ computerbasedmath.org]
* [http://blog.wolfram.com/ Wolfram Research blog to which Conrad Wolfram contributes]
* {{TED speaker}}
{{Wolfram Research}}
{{Authority control}}

{{DEFAULTSORT:Wolfram, Conrad}}
[[Category:1970 births]]
[[Category:Living people]]
[[Category:People from Oxford]]
[[Category:People educated at The Dragon School]]
[[Category:People educated at Eton College]]
[[Category:Alumni of Pembroke College, Cambridge]]
[[Category:Businesspeople in information technology]]
[[Category:English businesspeople]]
[[Category:English Jews]]
[[Category:People associated with King's College London]]
[[Category:Wolfram Research people]]
[[Category:Mathematics education]]</text>
      <sha1>2dn8hdbou7rgsazcmoa28cy79ij5bbh</sha1>
    </revision>
  </page>
  <page>
    <title>Daniel Biss</title>
    <ns>0</ns>
    <id>29462016</id>
    <revision>
      <id>867970691</id>
      <parentid>861043897</parentid>
      <timestamp>2018-11-09T04:13:27Z</timestamp>
      <contributor>
        <username>Abcbalbuena</username>
        <id>2046264</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="30687">{{Infobox officeholder
|name         = Daniel Biss
|image        = File:Daniel Biss 2012 (cropped).jpg
|state_senate = Illinois
|district     = 9th
|term_start   = January 2013
|term_end     = 
|predecessor  = [[Jeffrey Schoenberg]]
|successor    = 
|state_house1 = Illinois
|district1    = 17th
|term_start1  = May 2011
|term_end1    = January 2013
|predecessor1 = [[Elizabeth Coulson]]
|successor1   = [[Laura Fine]]
|birth_name   = Daniel Kálmán Biss
|birth_date   = {{birth date and age|1977|8|27}}
|birth_place  = [[Akron, Ohio]], U.S.
|death_date   = 
|death_place  = 
|party        = [[Democratic Party (United States)|Democratic]]
|spouse       = Karin Steinbrueck
|children     = 2
|education    = [[Harvard University]] {{small|([[Bachelor of Arts|BA]])}}&lt;br&gt;[[Massachusetts Institute of Technology]] {{small|([[Master of Arts|MA]], [[Doctor of Philosophy|PhD]])}}
|profession = Mathematician
|website      = {{url|danielbiss.com|Official website}}
|signature =BissSig.png
}}
'''Daniel Kálmán Biss'''&lt;ref&gt;{{cite web | url=http://news.harvard.edu/gazette/1998/05.28/HoopesPrizesAwa.html | title=Hoopes Prizes Awarded to Undergraduates and Thesis Advisers | work=Harvard Gazette | date=28 May 1998 | accessdate=2 June 2016}}&lt;/ref&gt; (born August 27, 1977)&lt;ref&gt;{{cite web | url=http://math.mit.edu/~gracelyo/18904/BissCoveringSpaceArticle.pdf | title=A Generalized Approach to the Fundamental Group | publisher=Mathematical Association of America | work=The American Mathematical Monthly | date=October 2000 | accessdate=2 June 2016 | author=Biss, Daniel K.}}&lt;/ref&gt; is an American politician serving as the member of the [[Illinois Senate]] for the 9th district since January 2013. The district includes Chicago's northern suburbs, including [[Evanston, Illinois|Evanston]], [[Glencoe, Illinois|Glencoe]], [[Glenview, Cook County, Illinois|Glenview]], [[Morton Grove]], [[Northbrook, Illinois|Northbrook]], [[Northfield, Illinois|Northfield]], [[Skokie, Illinois|Skokie]], [[Wilmette]], and [[Winnetka, Illinois|Winnetka]]. Biss first ran for office in 2008 and was a member of the [[Illinois House of Representatives]] from 2011 to 2013. He was also a candidate in the [[Democratic Party (United States)|Democratic Party]] primary for [[Governor of Illinois]] in the [[Illinois gubernatorial election, 2018|2018 election]].&lt;ref&gt;{{cite|url=http://www.chicagotribune.com/news/local/politics/ct-daniel-biss-illinois-governor-race-met-0320-20170319-story.html|title=State Sen. Daniel Biss announces Democratic bid for governor}}&lt;/ref&gt;

Prior to pursuing a political career, Biss was an Assistant Professor of Mathematics at the [[University of Chicago]] from 2002 to 2008.

==Personal life and education==
Biss was born into a Jewish [[Israelis|Israeli]] family&lt;ref&gt;[http://www.jpost.com/BDS-THREAT/Jewish-candidate-for-Illinois-governor-drops-running-mate-over-BDS-504506 Jewish candidate for Illinois governor drops running mate over BDS], ''Jerusalem Post'',
 September 7, 2017.&lt;/ref&gt;&lt;ref&gt;[[David Weigel]], [https://www.washingtonpost.com/news/powerpost/wp/2017/09/07/in-illinois-a-democrat-chooses-a-socialist-running-mate-then-dumps-him/ In Illinois, a Democrat chooses a socialist running mate, then dumps him], ''Washington Post'', September 7, 2017.&lt;/ref&gt; of musicians: his brother is the noted pianist [[Jonathan Biss]], his parents are the violinists [[Paul Biss]] and [[Miriam Fried]], and his grandmother was the Russian-born cellist [[Raya Garbousova]].&lt;ref&gt;{{cite news|title=Jonathan Biss: A Super, Human, Musical Mission|url=https://www.sfcv.org/events-calendar/artist-spotlight/jonathan-biss-a-super-human-musical-mission|accessdate=September 5, 2013|newspaper=San Francisco Classical Voice|author=Swinkels, Niels|date=June 12, 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite news|title=Jonathan Biss|url=http://www.timeout.com/newyork/opera-classical/jonathan-biss|accessdate=September 5, 2013|newspaper=Time Out New York|author=Giovetti, Olivia|date=January 18, 2011}}&lt;/ref&gt;

Biss received an undergraduate degree from [[Harvard University]], graduating ''[[summa cum laude]]'' in 1998, and a Ph.D. at [[Massachusetts Institute of Technology|MIT]] in 2002, both in mathematics.&lt;ref name=szpiro&gt;{{cite book|title=A mathematical medley: fifty easy pieces on mathematics|author=Szpiro, George G.|authorlink= George Szpiro|pages=97–99|year=2010|publisher=American Mathematical Society|chapter=20: Brilliant but Fallible|url=https://books.google.com/books?id=mXWr00HurmMC|chapterurl=https://books.google.com/books?id=mXWr00HurmMC&amp;pg=PA97|isbn=9780821890646}}&lt;/ref&gt; He won the 1999 [[Morgan Prize]] for outstanding research as an undergraduate, and was a [[Clay Mathematics Institute|Clay Research Fellow]] from 2002 to 2007.&lt;ref&gt;[http://www.claymath.org/fas/research_fellows/Biss Daniel Biss], [[Clay Mathematics Institute]]&lt;/ref&gt; His doctoral advisor was [[Michael J. Hopkins]]. He was a visiting scholar at the [[Institute for Advanced Study]] in the fall of 2003.&lt;ref&gt;[http://www.ias.edu/people/cos/users/daniel Institute for Advanced Study: A Community of Scholars]&lt;/ref&gt;

==Mathematics career==
Prior to full-time pursuit of a political career, Biss was an Assistant Professor of Mathematics at the [[University of Chicago]] from 2002 to 2008.&lt;ref name=JPost&gt;{{cite web |url=http://www.jpost.com/American-Politics/Spot-the-differences-between-the-two-Jewish-candidates-for-Illinois-governor-543636 |title=
Spot the differences between the two Jewish candidates for Illinois governor |work=[[The Jerusalem Post]] |date=February 26, 2018 |accessdate=March 29, 2018}}&lt;/ref&gt;

Biss wrote the key mathematical formula and an explanatory appendix for the popular 2006 [[Young adult fiction|young adult]] novel ''[[An Abundance of Katherines]],'' by [[Vlogbrothers]] co-creator and author [[John Green (author)|John Green]], and is a close friend of the author. The appendix explains the mathematics concepts used in the plot, and a book reviewer for the [[American Mathematical Society]] called it well written, saying "Biss does a stand-up job of communicating the basic concepts underlying the math woven throughout the novel. It is natural for the reader to wonder to what extent Biss himself associates with Colin [the book's main character] and to what extent the relationship between Colin and Hassan approximate that of Green and Biss."&lt;ref&gt;{{cite journal|last1=Nir|first1=Oaz|year=2008|title=Book Review: An Abundance of Katherines|journal=Notices of the [[American Mathematical Society|AMS]]|volume=55|issue=9|pages=1096–98|publisher=American Mathematical Society|doi=|url=http://www.ams.org/notices/200809/tx080901096p.pdf|accessdate=June 8, 2011}}&lt;/ref&gt;&lt;ref&gt;{{cite book|title=[[An Abundance of Katherines]]|last=Green|first=John|authorlink=John Green (author)|publisher=Dutton|year=2006|isbn=0-525-47688-1}}&lt;/ref&gt;&lt;ref&gt;{{cite web |last=Green |first=John |url=https://www.questia.com/library/journal/1G1-172908548/printz-award-honor-speech-john-green |title=Printz Award Honor Speech: John Green |work=Young Adult Library Services |access-date=March 16, 2018}}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://www.johngreenbooks.com/an-abundance-of-katherines-faq/ |title=FAQ: Q: How did Daniel Biss's mathematical help factor into the writing process? Did he write the formula before you began the novel or vice versa? |first=John |last=Green |website=Author John Green official website |access-date=March 26, 2018 }}&lt;/ref&gt;

At least four of the mathematics papers that Biss published in academic journals were later discovered to contain major errors. Mathematician Nikolai Mnëv published a report in 2007 that there was a "serious flaw" in two of Biss' works published in ''[[Annals of Mathematics]]'' and ''[[Advances in Mathematics]]'' in 2003, saying "unfortunately this simple mistake destroys the main theorems of both papers".&lt;ref&gt;{{cite arxiv |eprint=0709.1291|last1=Mnev|first1=N|title=On D.K. Biss' papers "The homotopy type of the matroid Grassmannian" and "Oriented matroids, complex manifolds, and a combinatorial model for BU"|class=math.CO|year=2007}}&lt;/ref&gt; In 2008 and 2009, Biss acknowledged the flaw and published [[erratum]] reports for the two papers, thanking Mnëv for drawing his attention to the error.&lt;ref name=szpiro/&gt;&lt;ref&gt;{{Cite journal|last=Biss|first=Daniel K.|date=July 2009|title=Erratum to 'The homotopy type of the matroid Grassmannian'|url=http://annals.math.princeton.edu/wp-content/uploads/annals-v170-n1-p15-p.pdf|journal=[[Annals of Mathematics]]|series=2nd|volume=170-1|page=493|doi=10.4007/annals.2009.170.493|pmid=|accessdate=May 19, 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |first=Daniel |last=Biss |url=https://www.sciencedirect.com/science/article/pii/S0001870808003575/pdfft?md5=28ff0bb45e05ece5a95da1aaf4af2f6d&amp;pid=1-s2.0-S0001870808003575-main.pdf |title=Erratum to 'Oriented matroids, complex manifolds, and a combinatorial model for BU' [Adv. Math. 179 (2) (2003) 250–290] |journal=[[Advances in Mathematics]] |volume=221 |issue=2 |page=681 |date=June 1, 2009 |access-date=March 26, 2018}}&lt;/ref&gt; He and a co-author also acknowledged in 2009 that there was a "fatal error" in a paper they had published in ''[[Inventiones Mathematicae]]'' in 2006, thanking mathematicians Masatoshi Sato and Tom Church for helping to explain the problem.&lt;ref&gt;{{cite journal |url=https://link.springer.com/article/10.1007%2Fs00222-009-0202-x |title=Erratum to {{'}}''Kg'' is not finitely generated' |first1=Daniel K. |last=Biss |first2=Benson |last2=Farb |journal=[[Inventiones Mathematicae]] |volume=178 |issue=1 |page=229 |date=October 2009 |access-date=March 26, 2018|doi=10.1007/s00222-009-0202-x |arxiv=math/0405386 |bibcode=2009InMat.178..229B }}&lt;/ref&gt; Another of his papers published in ''[[Topology and its Applications]]'' was formally retracted by the publisher in 2017, fifteen years after its 2002 publication, with the journal saying "This article has been retracted at the request of the Editors-in-Chief after receiving a complaint about anomalies in this paper. The editors solicited further independent reviews which indicated that the definitions in the paper are ambiguous and most results are false. The author was contacted and does not dispute these findings."&lt;ref&gt;{{Cite journal|url=https://www.sciencedirect.com/science/article/pii/S0166864116303455 |title=Retraction notice to 'The topological fundamental group and generalized covering spaces' [Topol. Appl. 124 (3) (2002) 355–371] |journal=[[Topology and its Applications]] |publisher=[[Elsevier]] |date=15 February 2017 |volume=217 |page=116 |access-date=March 25, 2018}}&lt;/ref&gt; The journal said they had identified twelve specific errors in the paper, but clarified that they had concluded that the paper's findings were merely inaccurate, not fraudulent.&lt;ref name=Pi&gt;{{cite newspaper |url=https://chicago.suntimes.com/news/are-gov-hopeful-bisss-claims-of-math-prowess-pi-in-the-sky/ |title=Are gov hopeful Biss's claims of math prowess pi in the sky? |newspaper=[[Chicago Sun-Times]] |date=September 29, 2017 |access-date=March 26, 2018 |first=Tina |last=Sfondeles}}&lt;/ref&gt;&lt;ref name=Watch1&gt;{{cite web |url=https://retractionwatch.com/2017/02/13/journal-retracts-paper-state-senator-former-mathematician/ |title=Journal retracts paper by state senator (and former mathematician) |date=February 13, 2017 |work=[[Retraction Watch]] |access-date=March 26, 2018}}&lt;/ref&gt;&lt;ref name=Watch2&gt;{{cite web |url=https://retractionwatch.com/2017/02/23/false-results-retracted-paper-senator-inaccurate-not-fraudulent-say-editors/ |title='False' results in retracted paper by senator are inaccurate, not fraudulent, say editors |work=[[Retraction Watch]] |date=February 23, 2017 |access-date=March 26, 2018}}&lt;/ref&gt; When contacted by the journal, Biss had responded saying "Thank you for writing. I am no longer in mathematics and so don't feel equipped to fully evaluate these claims. I certainly do not dispute them. If you would like to publish a retraction to that effect, that would seem to me to be an appropriate approach."&lt;ref name=Pi/&gt;&lt;ref name=Watch2/&gt;

When the 2017 retraction and the previously identified errors were reported by the ''[[Chicago Sun-Times]]'' in September 2017, his campaign blamed operatives for the perceived front-runner for the [[Democratic Party (United States)|Democratic Party]] candidate for governor of Illinois, [[J. B. Pritzker]], for raising it as a political issue. They said "Whether it was training at MIT or the University of Chicago, Daniel has had dozens of academic papers reviewed by his peers and published. In a few cases, further research has found that the case posited in the original article didn't stand up, and he revised his findings."&lt;ref name=Pi/&gt; They referred to the raising of the issue as "silly opposition research".&lt;ref name=Pi/&gt;

==Illinois House of Representatives==

===Committee assignments===
*Appropriations – Elementary &amp; Secondary Education
*Personnel &amp; Pensions
*Consumer Protection
*Small Business Empowerment &amp; Workforce Development
*International Trade &amp; Commerce 
*Bio-Technology
*Appropriations – Higher Education&lt;ref&gt;{{cite web|url=http://www.ilga.gov/house//Rep.asp?MemberID=1778 |title=Representative Daniel Biss (D) |publisher=www.ilga.gov |date= |accessdate={{date|2011-02-22}}}}&lt;/ref&gt;

==Electoral history==
{| class="wikitable" style="margin:0.5em ; font-size:95%"
|+ Illinois State Representative: Results 2008–2010&lt;ref name="results"&gt;{{cite web |url=http://www.cookcountyclerk.com/elections/results/Pages/default.aspx |title=Election Results |accessdate=July 8, 2013 |publisher=Cook County Board of Elections }}&lt;/ref&gt;
!|Year
!
!|Democrat
!|Votes
!|Pct
!
!|Republican
!|Votes
!|Pct
|-
|[[Illinois House of Representatives elections, 2008|2008]]
|
|{{Party shading/Democratic}} |Daniel Biss
|{{Party shading/Democratic}} align="right" |25,959
|{{Party shading/Democratic}} |48.52%
|
|{{Party shading/Republican}} |'''[[Elizabeth Coulson]]'''
|{{Party shading/Republican}} align="right" |'''27,540'''
|{{Party shading/Republican}} |'''51.48%'''
|-
|[[Illinois House of Representatives elections, 2010|2010]]
|
|{{Party shading/Democratic}} |'''Daniel K. Biss'''
|{{Party shading/Democratic}} align="right" |'''23,134'''
|{{Party shading/Democratic}} |'''54.78%'''
|
|{{Party shading/Republican}} |Hamilton Chang
|{{Party shading/Republican}} align="right" |19,096
|{{Party shading/Republican}} |45.22%
|}

{| class="wikitable" style="margin:0.5em ; font-size:95%"
|+ Illinois State Senate: Results 2012&lt;ref name="results"/&gt;
!|Year
!
!|Democrat
!|Votes
!|Pct
!
!|Republican
!|Votes
!|Pct
|-
|[[Illinois Senate elections, 2012|2012]]
|
|{{Party shading/Democratic}} |'''Daniel Biss'''
|{{Party shading/Democratic}} align="right" |'''68,064'''
|{{Party shading/Democratic}} |'''66.63%'''
|
|{{Party shading/Republican}} |Glenn Farkas
|{{Party shading/Republican}} align="right" |34,081
|{{Party shading/Republican}} |33.37%
|}

==Political views==
[[File:Stop Brett Kavanaugh Rally Downtown Chicago Illinois (43406990865).jpg|thumb|right|Biss at an anti Kavanaugh rally in 2018]]
According to a 2008 [[Political Courage Test]], Daniel Biss supports carbon emissions limits. Biss is pro-choice, supporting legal abortion. He supports allowing high school graduates to pay in-state tuition at public universities regardless of immigration status, as well as state funding to raise the salaries of teachers.&lt;ref&gt;{{cite web | url = http://votesmart.org/candidate/political-courage-test/101726/daniel-biss/ | title = Daniel Biss' Issue Positions | work = [[Project Vote Smart]] | year = 2008 | publisher = One Common Ground | location = Philipsburg, Montana }}&lt;/ref&gt; He received a 7% rating by the [[National Rifle Association|NRA]] in 2010.&lt;ref&gt;{{cite web | url = http://votesmart.org/candidate/101726/daniel-biss?categoryId=37&amp;type=R | title = Daniel K. Biss' Political Summary on Issue: Guns | work = [[Project Vote Smart]] | accessdate = November 25, 2015 | publisher = One Common Ground | location = Philipsburg, Montana }}&lt;/ref&gt; Biss has expressed support of [[labor union]]s&lt;ref&gt;{{cite news release | url = http://danielbiss.com/releases/Kickoff%20Press%20Release_091409.pdf | title = Biss Kicks Off Campaign With 150 Strong | date = September 14, 2009 | archiveurl = https://web.archive.org/web/20140221021013/http://danielbiss.com/releases/Kickoff%20Press%20Release_091409.pdf | archivedate=February 21, 2014 | publisher = Daniel Biss for State Representative | location = [[Evanston, Illinois]] }}&lt;/ref&gt; and has received $20,000 from [[AFSCME]], the second largest donation to a state legislator.&lt;ref&gt;{{cite news | url = http://www.journalstandard.com/x1696634795/Wage-increases-slow-but-not-campaign-contributions | title = Wage increases slow, but not campaign contributions | work = [[Journal Standard]] | edition = online | location = Freeport, Illinois | date = November 9, 2011 | agency = Illinois Statehouse News | accessdate = November 25, 2015 }}&lt;/ref&gt; Biss also supports legalizing marijuana in Illinois.&lt;ref&gt;{{Cite web|url=https://capitolfax.com/2017/03/29/where-do-the-candidates-stand-on-marijuana/|title=Capitol Fax.com - Your Illinois News Radar » Where do the candidates stand on marijuana?|website=capitolfax.com|access-date=2017-04-03}}&lt;/ref&gt;

In 2013, Biss cosponsored SB 1, a bill that aimed to limit the annual growth of retirement annuities within state employee's pension plans in an attempt to reduce debts in the state retirement system.&lt;ref&gt;{{cite web |url=http://votesmart.org/bill/16386/46516/amends-state-employee-pension-plans |title=SB 1 - Amends State Employee Pension Plans - Key Vote | work = [[Project Vote Smart]] | publisher = One Common Ground | location = Philipsburg, Montana |accessdate=5 September 2015}}&lt;/ref&gt; In May 2015, the Illinois Supreme Court found the law unconstitutional.&lt;ref&gt;{{cite news |url=http://www.chicagotribune.com/news/local/politics/ct-illinois-pension-law-court-ruling-20150508-story.html |title=Illinois Supreme Court rules landmark pension law unconstitutional |newspaper=Chicago Tribune |date=8 May 2015 |last1=Pearson |first1=Rick |last2=Geiger |first2=Kim |accessdate=5 September 2015}}&lt;/ref&gt; In rejecting the constitutionality of SB-1, the Illinois Supreme Court stated: "These modifications to pension benefits unquestionably diminish the value of the retirement annuities the members…were promised when they joined the pension system. Accordingly, based on the plain language of the Act, these annuity-reducing provisions contravene the pension protection clause’s absolute prohibition against diminishment of pension benefits and exceed the General Assembly’s authority," the ruling states.&lt;ref&gt;http://chicago.suntimes.com/politics/pension-reforms-illinois-supreme-court/&lt;/ref&gt; Later, Biss acknowledged that his work on SB1 was an error saying, "I decided this was the least bad of the bad options. I allowed myself to think we couldn't do better." Biss is now in support of fully honoring the pension payments by instituting progressive tax reforms to fully fund them.&lt;ref&gt;{{Cite news|url=http://www.chicagobusiness.com/article/20180202/BLOGS02/180209967/illinois-governor-candidate-dan-biss-on-taxes-amazon-hq2-lasalle|title=Biss' populist play: Tax LaSalle Street and rethink Amazon HQ2|work=Crain's Chicago Business|access-date=2018-03-05}}&lt;/ref&gt;

In March 2017, Biss sponsored SB1424, a bill proposing a system of matching state funds for small-donor political contributions &lt;ref&gt;{{Cite web|url=http://www.ilga.gov/legislation/BillStatus.asp?GA=100&amp;DocTypeID=SB&amp;DocNum=1424&amp;GAID=14&amp;SessionID=91&amp;LegID=103770|title=Illinois General Assembly - Bill Status for SB1424|website=www.ilga.gov|access-date=2017-03-20}}&lt;/ref&gt;  and SB 780, a bill proposing to elect a number of statewide offices by [[Instant-runoff voting|ranked-choice ballot]].&lt;ref&gt;{{Cite web|url=http://www.ilga.gov/legislation/BillStatus.asp?GA=100&amp;DocTypeID=SB&amp;DocNum=780&amp;GAID=14&amp;SessionID=91&amp;LegID=102091|title=Illinois General Assembly - Bill Status for SB0780|website=www.ilga.gov|access-date=2017-03-20}}&lt;/ref&gt; He also co-sponsored SB 1933, a bill by State Sen. Andy Manar to allow for automatic voter registration when applying for an Illinois drivers’ license.&lt;ref&gt;{{Cite web|url=http://www.ilga.gov/legislation/BillStatus.asp?GA=100&amp;DocTypeID=SB&amp;DocNum=1933&amp;GAID=14&amp;SessionID=91&amp;LegID=105253|title=Illinois General Assembly - Bill Status for SB1933|website=www.ilga.gov|access-date=2017-03-20}}&lt;/ref&gt;

Biss supports [[universal health care]] and advocates specifically for a state-level [[single-payer healthcare]] system.&lt;ref&gt;{{cite news|last1=Miller|first1=Rich|title=Biss, Pawar respond *** Pritzker outlines Medicaid buy-in idea|url=https://capitolfax.com/2017/08/01/pritzker-outlines-medicaid-buy-in-idea/|accessdate=12 August 2017|work=capitolfax.com|date=1 August 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=The truth about single-payer|url=https://www.danielbiss.com/single-payer/|website=Daniel Biss for Governor|accessdate=12 August 2017|date=5 August 2017}}&lt;/ref&gt; In June 2017, Biss voted to reinforce the [[Affordable Care Act]] in Illinois by preventing insurance companies from discriminating against customers with [[pre-existing conditions]].&lt;ref&gt;{{cite news|last1=Dugyala|first1=Rishika|title=Biss joins fight to prevent denial of pre-existing conditions in Illinois|url=https://dailynorthwestern.com/2017/06/14/city/biss-joins-fight-to-prevent-denial-of-pre-existing-conditions-in-illinois/|accessdate=12 August 2017|work=The Daily Northwestern|date=14 June 2017}}&lt;/ref&gt;

==Campaigns==
Biss ran for a seat in the Illinois State House of Representatives in 2008, losing to [[Republican Party (United States)|Republican]] [[Elizabeth Coulson]]. Starting in 2009, he then worked as a policy adviser to [[Rod Blagojevich]], the Democratic governor of Illinois.&lt;ref name=JPost/&gt;

[[File:Biss For Illinois 2017 campaign logo.png|thumb|Biss 2018 gubernatorial campaign logo.]]
On November 10, 2011, Biss announced his intent to run for the Illinois Senate seat held by retiring Senator [[Jeffrey Schoenberg]].&lt;ref&gt;{{Cite news |title=State Rep. Biss to seek Schoenberg's state Senate seat |first=Patrick |last=Svitek |date=November 10, 2011 |publisher=The Daily Northwestern |url=http://www.dailynorthwestern.com/city/state-rep-biss-to-seek-schoenberg-s-senate-seat-1.2669291}}&lt;/ref&gt; He won the election on November 6, 2012, receiving over 66% of the vote.&lt;ref name=chang&gt;{{cite news | first = Chi-an | last = Chang | date = November 7, 2012 | title = Biss Wins 9th State Senate District Race | publisher = [[Patch Media]] | url = http://patch.com/illinois/winnetka/9th-state-senate-district-results-biss-vs-farkas | accessdate = 2016-09-26}}&lt;/ref&gt;

Biss announced a run for [[Illinois Comptroller]] in the [[Illinois Comptroller special election, 2016|2016 special election]]&lt;ref&gt;{{cite news|last1=Pearson|first1=Rick|title=City Clerk Mendoza gets major union backing in state comptroller bid|url=http://www.chicagotribune.com/news/local/politics/ct-susana-mendoza-comptroller-endorsement-met0924-20150923-story.html|accessdate=3 October 2015|publisher=Chicago Tribune|date=22 September 2015}}&lt;/ref&gt; but dropped out and endorsed opponent [[Susana Mendoza]].&lt;ref&gt;{{cite news release | url = http://www.danielbiss.com/comptroller-1 | first = Daniel | last = Biss | title = Comptroller Campaign Update | work = Biss for Illinois | location = [[Evanston, Illinois]] | accessdate = 2015-11-25 }}&lt;/ref&gt;

On March 20, 2017, Biss announced his candidacy for the Democratic nomination for [[Governor of Illinois]] for the [[Illinois gubernatorial election, 2018|2018 election]] on a [[Facebook Live]] video, attacking incumbent governor [[Bruce Rauner]] and Illinois House Speaker [[Mike Madigan]].&lt;ref&gt;{{cite news|last1=Pearson|first1=Rick|title=State Sen. Daniel Biss announces Democratic bid for governor|url=http://www.chicagotribune.com/news/local/politics/ct-daniel-biss-illinois-governor-race-met-0320-20170319-story.html|accessdate=20 March 2017|work=chicagotribune.com|agency=Chicago Tribune|date=March 20, 2017}}&lt;/ref&gt; Biss joined a growing field of Democratic contenders, including businessman [[Christopher G. Kennedy|Chris Kennedy]] and Chicago alderman [[Ameya Pawar]].&lt;ref&gt;{{cite news|last1=Schulte|first1=Sarah|title=State Sen. Biss says he's running for Illinois governor|url=http://abc7chicago.com/politics/state-sen-biss-says-hes-running-for-illinois-governor/1809229/|accessdate=20 March 2017|work=ABC7 Chicago|date=20 March 2017}}&lt;/ref&gt;

Biss briefly named Chicago alderman and [[Democratic Socialists of America]] member [[Carlos Ramirez-Rosa]] as his gubernatorial running mate, but dropped him from the ticket after just six days after Ramirez-Rosa expressed some support for the [[Boycott, Divestment and Sanctions]] boycott movement against [[Israel]].&lt;ref&gt;{{cite news|last1=Smith|first1=Ryan|title=Ramirez-Rosa dumped off the Biss ticket in six days; denies flip-flop on Israel issue|url=https://www.chicagoreader.com/Bleader/archives/2017/09/07/ramirez-rosa-dumped-off-the-biss-ticket-in-six-days-denies-flip-flop-on-israel-issue|accessdate=9 October 2017|work=Chicago Reader}}&lt;/ref&gt; Biss later announced his selection of [[Rockford, Illinois|Rockford]]-based [[Illinois House of Representatives|state representative]] [[Litesa Wallace]], a single mother and former [[social worker]].&lt;ref&gt;{{cite news|last1=Vinicky|first1=Amanda|title=Daniel Biss Replaces Ramirez-Rosa with Rep. Litesa Wallace|url=http://chicagotonight.wttw.com/2017/09/08/daniel-biss-replaces-ramirez-rosa-rep-litesa-wallace|accessdate=9 October 2017|work=Chicago Tonight {{!}} WTTW}}&lt;/ref&gt;&lt;ref&gt;{{cite news|title=Biss formally announces Litesa Wallace as new running mate|url=https://capitolfax.com/2017/09/08/biss-formally-announces-litesa-wallace-as-new-running-mate/|accessdate=9 October 2017|work=capitolfax.com|date=8 September 2017}}&lt;/ref&gt;

Biss had been endorsed by many of his colleagues in the Illinois General Assembly, high profile academics and activists including Nobel laureate [[Richard Thaler]] and presidential candidate [[Lawrence Lessig]], [[National Nurses United]], the largest organization of registered nurses in the United States, and [[Our Revolution]], the successor organization to [[Bernie Sanders]]' [[Bernie Sanders presidential campaign, 2016|2016 presidential campaign]].&lt;ref&gt;{{cite web|title=Who's on Team Biss?|url=https://www.danielbiss.com/team-biss-endorsements/|website=Daniel Biss for Governor|publisher=Biss for Illinois|accessdate=22 January 2018|date=6 November 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite news|last1=Pedersen|first1=Brendan|title=Biss Lands Major Progressive Group's Endorsement|url=https://www.nbcchicago.com/blogs/ward-room/our-revolution-illinois-endorses-sen-daniel-biss-471698764.html|accessdate=30 January 2018|work=NBC Chicago|date=30 January 2018}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Our Revolution Endorses Daniel Biss for Governor of Illinois|url=https://ourrevolution.com/press/our-revolution-endorses-daniel-biss-governor-illinois/|website=Our Revolution|accessdate=16 March 2018|date=27 February 2018}}&lt;/ref&gt; Biss received two-thirds of preferential votes from Illinois members of the progressive advocacy group [[MoveOn.org]].&lt;ref&gt;{{cite news|last1=Payne|first1=Benjamin|title=Daniel Biss Lands MoveOn.org Endorsement For Illinois Governor|url=http://wvik.org/post/daniel-biss-lands-moveonorg-endorsement-illinois-governor#stream/0|accessdate=30 January 2018|agency=WVIK|date=25 January 2018}}&lt;/ref&gt; 

On March 20, 2018, Biss lost the Democratic primary to [[J.B. Pritzker]]. He earned  26.70% of the total vote, behind Prizker with 45.13% and ahead of [[Christopher G. Kennedy|C.G. Kennedy]] with 24.37%. 
Biss carried two counties, [[McLean County, Illinois|McLean]] and [[Champaign County, Illinois|Champaign]].&lt;ref&gt;{{cite web |title=Official Canvas, General Primary Election, March 20, 2018 |url=https://www.elections.il.gov/Downloads/ElectionInformation/VoteTotals/2018GPOfficialVote.pdf |publisher=Illinois State Board of Elections}}&lt;/ref&gt;

==Rust Belt Rising==
On September 18, 2018, Biss announced in an email to supporters that he has accepted the position of executive director of the nonprofit [http://rustbeltrising.org Rust Belt Rising], which aims to train and support Democratic candidates in the [[Great Lakes region|Great Lakes states]].&lt;ref&gt;{{cite news |last1=Meadows |first1=Jonah |title=Sen. Daniel Biss To Head Democratic Candidate Training Nonprofit |url=https://patch.com/illinois/evanston/sen-daniel-biss-head-democratic-candidate-training-nonprofit |accessdate=19 September 2018 |work=Evanston, IL Patch |date=18 September 2018}}&lt;/ref&gt; Biss has stated he intends to fully serve out his state senate term, which expires in January 2019.&lt;ref&gt;{{cite news |title=Biss to take over helm of Rust Belt Rising |url=https://capitolfax.com/2018/09/17/biss-to-take-over-helm-of-rust-belt-rising/ |accessdate=19 September 2018 |work=capitolfax.com |date=18 September 2018}}&lt;/ref&gt;

==See also==
* [[Illinois House of Representatives elections, 2010]]

==References==
{{Reflist|30em}}

==External links==
*[http://my.ilga.gov/Member/Index/2020?tab=1&amp;chamber=S Biography, bills and committees] at the 98th Illinois General Assembly
**By session: [http://www.ilga.gov/senate/Senator.asp?GA=98&amp;MemberID=2020 98th], [http://www.ilga.gov/house/Rep.asp?GA=97&amp;MemberID=1778 97th]
*[http://senatorbiss.com/ Illinois State Senator Daniel Biss] legislative website
*[http://www.illinoissenatedemocrats.com/index.php/sen-biss-home Senator Daniel Biss] at Illinois Senate Democrats
*[http://danielbiss.com/ Daniel Biss for Illinois] campaign website
{{CongLinks | votesmart = 101726 }}
*{{MathGenealogy|67217}}

{{Illinois State Senators}}
{{Authority control}}

{{DEFAULTSORT:Biss, Daniel}}
[[Category:1977 births]]
[[Category:20th-century American mathematicians]]
[[Category:21st-century American mathematicians]]
[[Category:21st-century American politicians]]
[[Category:American people of Romanian-Jewish descent]]
[[Category:American people of Russian-Jewish descent]]
[[Category:Green brothers]]
[[Category:Harvard University alumni]]
[[Category:Illinois Democrats]]
[[Category:Illinois state senators]]
[[Category:Institute for Advanced Study visiting scholars]]
[[Category:Jewish American politicians]]
[[Category:Living people]]
[[Category:Massachusetts Institute of Technology alumni]]
[[Category:Members of the Illinois House of Representatives]]
[[Category:Morgan Prize winners]]
[[Category:People from Bloomington, Illinois]]
[[Category:People from Evanston, Illinois]]
[[Category:Politicians from Akron, Ohio]]
[[Category:Topologists]]
[[Category:Mathematicians from Ohio]]
[[Category:Mathematician politicians]]</text>
      <sha1>0w0l83wdc1d8p454u0xrpctj61frylh</sha1>
    </revision>
  </page>
  <page>
    <title>Delta encoding</title>
    <ns>0</ns>
    <id>320283</id>
    <revision>
      <id>832279277</id>
      <parentid>827113988</parentid>
      <timestamp>2018-03-24T23:57:40Z</timestamp>
      <contributor>
        <username>Bkell</username>
        <id>32452</id>
      </contributor>
      <comment>/* Sample C code */ need `unsigned char` to avoid undefined behavior in case `char` is signed, because signed integer overflow is undefined behavior in C</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8576">{{Distinguish|Elias delta coding|delta modulation}}

'''Delta encoding''' is a way of storing or transmitting [[data]] in the form of ''[[Data differencing|differences]]'' (deltas) between sequential data rather than complete files; more generally this is known as [[data differencing]]. Delta encoding is sometimes called '''delta compression''', particularly where archival histories of changes are required (e.g., in [[revision control software]]).

The differences are recorded in discrete files called "deltas" or "diffs". In situations where differences are small – for example, the change of a few words in a large document or the change of a few records in a large table – delta encoding greatly reduces data redundancy. Collections of unique deltas are substantially more space-efficient than their non-encoded equivalents.

From a logical point of view the difference between two data values is the information required to obtain one value from the other – see [[relative entropy]]. The difference between identical values (under some [[equivalence relation|equivalence]]) is often called ''0'' or the [[neutral element]].

==Simple example==
Perhaps the simplest example is storing values of bytes as differences (deltas) between sequential values, rather than the values themselves. So, instead of 2, 4, 6, 9, 7, we would store 2, 2, 2, 3, −2. This reduces the [[variance]] (range) of the values when neighbor samples are correlated, enabling a lower bit usage for the same data. [[Interchange File Format|IFF]] [[8SVX]] sound format applies this encoding to raw sound data before applying compression to it. Unfortunately, not even all 8-bit sound [[sampling (signal processing)|samples]] compress better when delta encoded, and the usability of delta encoding is even smaller for 16-bit and better samples. Therefore, compression algorithms often choose to delta encode only when the compression is better than without. However, in video compression, delta frames can considerably reduce frame size and are used in virtually every video compression [[codec]].

==Definition==
A delta can be defined in 2 ways, ''symmetric delta'' and ''directed delta''. A ''symmetric delta'' can be expressed as:&lt;blockquote&gt;&lt;math&gt;\Delta(v_1, v_2) = (v_1 \setminus v_2) \cup (v_2 \setminus v_1)&lt;/math&gt;&lt;/blockquote&gt;where &lt;math&gt;v_1&lt;/math&gt; and &lt;math&gt;v_2&lt;/math&gt; represent two versions.

A ''directed delta'', also called a change, is a sequence of (elementary) change operations which, when applied to one version &lt;math&gt;v_1&lt;/math&gt;, yields another version &lt;math&gt;v_2&lt;/math&gt; (note the correspondence to [[transaction log]]s in databases).

===Variants===
A variation of delta encoding which encodes differences between the [[Prefix (computer science)|prefixes]] or [[Suffix (computer science)|suffixes]] of [[string (computer science)|strings]] is called [[incremental encoding]]. It is particularly effective for sorted lists with small differences between strings, such as a list of [[word]]s from a [[dictionary]].

==Implementation issues==
The nature of the data to be encoded influences the effectiveness of a particular compression algorithm.

Delta encoding performs best when data has small or constant variation; for an unsorted data set, there may be little to no compression possible with this method.

In delta encoded transmission over a network where only a single copy of the file is available at each end of the communication channel, special [[error-correction|error control codes]] are used to detect which parts of the file have changed since its previous version.
For example, [[rsync]] uses a rolling checksum algorithm based on Mark Adler's [[adler-32]] checksum.

==Sample C code==
The following [[C (programming language)|C]] code performs a simple form of delta encoding and decoding: &lt;!-- Yes, it is possible to write smaller/faster functions; but these are given because they are easily understandable --&gt;

&lt;source lang="c"&gt;
void delta_encode(unsigned char *buffer, int length)
{
    unsigned char last = 0;
    for (int i = 0; i &lt; length; i++)
    {
        unsigned char current = buffer[i];
        buffer[i] = current - last;
        last = current;
    }
}

void delta_decode(unsigned char *buffer, int length)
{
    unsigned char last = 0;
    for (int i = 0; i &lt; length; i++)
    {
        unsigned char delta = buffer[i];
        buffer[i] = delta + last;
        last = buffer[i];
    }
}&lt;/source&gt;

==Examples==

===Delta encoding in HTTP===
Another instance of use of delta encoding is RFC 3229, "Delta encoding in HTTP", which proposes that [[HTTP]] servers should be able to send updated Web pages in the form of differences between versions (deltas), which should decrease Internet traffic, as most pages change slowly over time, rather than being completely rewritten repeatedly:

{{quote|This document describes how delta encoding can be supported as a compatible extension to HTTP/1.1.

Many HTTP (Hypertext Transport Protocol) requests cause the retrieval of slightly modified instances of resources for which the client already has a cache entry. Research has shown that such modifying updates are frequent, and that the modifications are typically much smaller than the actual entity. In such cases, HTTP would make more efficient use of network bandwidth if it could transfer a minimal description of the changes, rather than the entire new instance of the resource.
}}

=== Delta copying ===
''Delta copying'' is a fast way of copying a file that is partially changed, when a previous version is present on the destination location. With delta copying, only the changed part of a file is copied. It is usually used in [[backup]] or [[file copying]] software, often to save [[Bandwidth (computing)|bandwidth]] when copying between computers over a private network or the internet.&lt;ref&gt;http://www.2brightsparks.com/bb/viewtopic.php?t=4473&lt;/ref&gt;&lt;ref&gt;https://www.bvckup2.com/support/forum/topic/739&lt;/ref&gt;&lt;ref&gt;http://www.eggheadcafe.com/software/aspnet/33678264/delta-copying.aspx&lt;/ref&gt;

===Online backup===
{{main|Online backup services}}

Many of the [[online backup services]] adopt this methodology, often known simply as ''deltas'', in order to give their users previous versions of the same file from previous backups. This reduces associated costs, not only in the amount of data that has to be stored as differing versions (as the whole of each changed version of a file has to be offered for users to access), but also those costs in the uploading (and sometimes the downloading) of each file that has been updated (by just the smaller delta having to be used, rather than the whole file).

===Git===
{{main|Git (software)}}

The Git source code control system employs delta compression in an auxiliary "[http://git-scm.com/docs/git-repack git repack]" operation. Objects in the repository that have not yet been delta-compressed ("loose objects") are compared against a heuristically chosen subset of all other objects, and the common data and differences are concatenated into a "pack file" which is then compressed using conventional methods. In common use cases, where source or data files are changed incrementally between commits, this can result in significant space savings. The repack operation is typically performed as part of the "[http://git-scm.com/docs/git-gc git gc]" process, which is triggered automatically when the numbers of loose objects or pack files exceed configured thresholds.

===VCDIFF===
{{main|VCDIFF}}

One general format for delta encoding is VCDIFF, described in RFC 3284. [[Free software]] implementations include [[Xdelta]] and open-vcdiff.

===GDIFF===
Generic Diff Format (GDIFF) is another delta encoding format. It was submitted to [[W3C]] in 1997.&lt;ref&gt;[http://www.w3.org/TR/NOTE-gdiff-19970901 Generic Diff Format Specification]&lt;/ref&gt; In many cases, VCDIFF has better compression rate than GDIFF.

===Diff===
{{main|Diff}}
Diff is a file comparison program, which is mainly used for text files.

===bsdiff===
[http://www.daemonology.net/bsdiff/ Bsdiff] is a binary diff program using [[Suffix array|suffix sorting]].

==See also==
* [[Data differencing]]
* [[Interleaved deltas]]
* [[Source Code Control System]]
* [[String-to-string correction problem]]
* [[Xdelta]]: open-source delta encoder

==References==
{{reflist}}

==External links==
* RFC 3229 – Delta Encoding in HTTP

{{Compression methods}}
{{Version control software}}

{{DEFAULTSORT:Delta Encoding}}
[[Category:Lossless compression algorithms]]
[[Category:Data differencing]]
[[Category:Articles with example C code]]</text>
      <sha1>sulbot7pexb9lbhjq9x2wk8lxor1hog</sha1>
    </revision>
  </page>
  <page>
    <title>Derived type</title>
    <ns>0</ns>
    <id>1758864</id>
    <revision>
      <id>697723880</id>
      <parentid>696937979</parentid>
      <timestamp>2016-01-01T13:10:14Z</timestamp>
      <contributor>
        <username>Christian75</username>
        <id>1306352</id>
      </contributor>
      <comment>One entry per line</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="513">In [[computer science]], '''derived type''' can mean:

* a [[composite data type]], one built out of other types
* a [[subtype]]
* a [[derived class]]

[[Category:Data types]]
[[Category:Type theory]]

{{Disambiguation}}

{{Short pages monitor}}&lt;!-- This long comment was added to the page to prevent it from being listed on Special:Shortpages. It and the accompanying monitoring template were generated via Template:Long comment. Please do not remove the monitor template without removing the comment as well.--&gt;</text>
      <sha1>oruxzmkamsfo364vxnmxs25dh00vmg4</sha1>
    </revision>
  </page>
  <page>
    <title>Eikonal approximation</title>
    <ns>0</ns>
    <id>12181958</id>
    <revision>
      <id>865153400</id>
      <parentid>803850683</parentid>
      <timestamp>2018-10-22T02:53:14Z</timestamp>
      <contributor>
        <ip>75.118.45.132</ip>
      </contributor>
      <comment>Switch one line from unicode math symbols to &lt;math&gt; for improved look.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4587">In [[theoretical physics]], the '''eikonal approximation''' ([[Greek language|Greek]] εἰκών for likeness, icon or image) is an approximative method useful in wave [[scattering]] equations which occur in [[optics]], [[seismology]], [[quantum mechanics]], [[quantum electrodynamics]], and [[Scattering amplitude#Partial wave expansion|partial wave expansion]].

==Informal description==
The main advantage that the eikonal approximation offers is that the equations reduce to a [[differential equation]] in a single variable. This reduction into a single variable is the result of the straight line approximation or the eikonal approximation which allows us to choose the straight line as a special direction.

==Relation to the WKB approximation==
The early steps involved in the eikonal approximation in quantum mechanics are very closely related to the [[WKB approximation]] for one-dimensional waves. The WKB method, like the eikonal approximation, reduces the equations into a differential equation in a single variable. But the difficulty with the WKB approximation is that this variable is described by the trajectory of the particle which, in general, is complicated.

==Formal description==

Making use of WKB approximation we can write the wave function of the scattered system in terms of [[action (physics)|action]] ''S'':

:&lt;math&gt;\Psi=e^{iS/{\hbar}} &lt;/math&gt;

Inserting the [[wavefunction]] Ψ in the [[Schrödinger equation]] without the presence of a magnetic field we obtain

:&lt;math&gt; -\frac{{\hbar}^2}{2m} {\nabla}^2 \Psi= (E-V) \Psi&lt;/math&gt;

:&lt;math&gt; -\frac{{\hbar}^2}{2m} {\nabla}^2 {e^{iS/{\hbar}}}=(E-V) e^{iS/{\hbar}}&lt;/math&gt;

:&lt;math&gt;\frac{1}{2m} {(\nabla S)}^2 - \frac{i\hbar}{2m}{\nabla}^2 S= E-V&lt;/math&gt;

We write ''S'' as a [[power series]] in ''ħ''

:&lt;math&gt;S= S_0 + \frac {\hbar}{i} S_1 + ...&lt;/math&gt;

For the zero-th order:

:&lt;math&gt; \frac{1}{2m}  {(\nabla S_0)}^2 = E-V&lt;/math&gt;

If we consider the one-dimensional case then &lt;math&gt;{\nabla}^2 \rightarrow {\partial_z}^2&lt;/math&gt;.

We obtain a [[differential equation]] with the [[Boundary value problem|boundary condition]]:

:&lt;math&gt;\frac{S(z=z_0)}{\hbar}= k z_0&lt;/math&gt;

for &lt;math&gt;V \rightarrow 0&lt;/math&gt;, &lt;math&gt;z \rightarrow -\infty&lt;/math&gt;.

:&lt;math&gt;\frac{d}{dz}\frac{S_0}{\hbar}= \sqrt{k^2 - 2mV/{\hbar}^2}&lt;/math&gt;

:&lt;math&gt;\frac{S_0(z)}{\hbar}= kz - \frac{m}{{\hbar}^2 k} \int_{-\infty}^{z}{V dz'} &lt;/math&gt;

==See also==
* [[Eikonal equation]]
* [[Correspondence principle]]
* [[Principle of least action]]

==References==

===Notes===
* [http://www.nhn.ou.edu/~shajesh/eikonal/sp.pdf]''Eikonal Approximation'' K. V. Shajesh Department of Physics and Astronomy, University of Oklahoma

===Further reading===

* {{cite book|title=Comparison of exact solution with Eikonal approximation for elastic heavy ion scattering|edition=3rd|author=R.R. Dubey|location=|publisher=NASA|year=1995 |isbn=|url = https://books.google.com/books?id=NwgVAQAAIAAJ&amp;q=Eikonal+approximation&amp;dq=Eikonal+approximation&amp;hl=en&amp;sa=X&amp;ei=LCnkUOP8HfDa0QW-34GIBA&amp;ved=0CDwQ6AEwAQ}}
* {{cite book|title=Eikonal approximation in partial wave version|edition=3rd|author1=W. Qian |author2=H. Narumi |author3=N. Daigaku. P. Kenkyūjo |location=Nagoya|publisher=|year=1989|isbn=|url = https://books.google.com/books?id=5RdRAAAAMAAJ&amp;q=Eikonal+approximation&amp;dq=Eikonal+approximation&amp;hl=en&amp;sa=X&amp;ei=LCnkUOP8HfDa0QW-34GIBA&amp;ved=0CDYQ6AEwAA}}
*{{cite news
 |author1=M. Lévy |author2=J. Sucher | year = 1969
 | location = Maryland, USA
 | publisher = 
 | journal = Phys. Rev.
 | title = Eikonal Approximation in Quantum Field Theory
 | arxiv = 
 | url = http://prola.aps.org/abstract/PR/v186/i5/p1656_1
 | doi = 10.1103/PhysRev.186.1656
| bibcode =1969PhRv..186.1656L}}
*{{cite news
 |author=I. T. Todorov 
 |year=1970 
 |location=New Jersey, USA 
 |publisher= 
 |journal=Phys. Rev. D
 |title=Quasipotential Equation Corresponding to the Relativistic Eikonal Approximation 
 |arxiv= 
 |url=http://prd.aps.org/abstract/PRD/v3/i10/p2351_1
 |archive-url=https://archive.is/20130223145535/http://prd.aps.org/abstract/PRD/v3/i10/p2351_1
 |dead-url=yes
 |archive-date=2013-02-23
 |doi=10.1103/PhysRevD.3.2351 
|bibcode=1971PhRvD...3.2351T}}
*{{cite news
 | author = D.R. Harrington
 | year = 1969
 | location = New Jersey, USA
 | publisher = 
 | journal = Phys. Rev.
 | title = Multiple Scattering, the Glauber Approximation, and the Off-Shell Eikonal Approximation
 | arxiv = 
 | url = http://prola.aps.org/abstract/PR/v184/i5/p1745_1
 | doi = 10.1103/PhysRev.184.1745
| bibcode = 1969PhRv..184.1745H}}

[[Category:Theoretical physics]]
[[Category:Mathematical analysis]]</text>
      <sha1>t2359khyaxfh0ll1cc9hdta4w7k65ej</sha1>
    </revision>
  </page>
  <page>
    <title>Elementary class</title>
    <ns>0</ns>
    <id>1284226</id>
    <revision>
      <id>868852489</id>
      <parentid>834195478</parentid>
      <timestamp>2018-11-14T21:31:22Z</timestamp>
      <contributor>
        <username>Texvc2LaTeXBot</username>
        <id>33995001</id>
      </contributor>
      <minor/>
      <comment>Replacing deprecated latex syntax [[mw:Extension:Math/Roadmap]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8761">In [[model theory]], a branch of [[mathematical logic]], an '''elementary class''' (or '''axiomatizable class''') is a [[class (set theory)|class]] consisting of all [[structure (mathematical logic)|structures]] satisfying a fixed [[first-order logic|first-order]] [[theory (mathematical logic)|theory]].

== Definition ==
A [[class (set theory)|class]] ''K'' of [[structure (mathematical logic)|structures]] of a [[signature (logic)|signature]] σ is called an '''elementary class''' if there is a [[first-order logic|first-order]] [[theory (mathematical logic)|theory]] ''T'' of signature σ, such that ''K'' consists of all models of ''T'', i.e., of all σ-structures that satisfy ''T''. If ''T'' can be chosen as a theory consisting of a single first-order sentence, then ''K'' is called a '''basic elementary class'''.

More generally, ''K'' is a [[pseudoelementary class|pseudo-elementary class]] if there is a first-order theory ''T'' of a signature that extends σ, such that ''K'' consists of all σ-structures that are [[reduct]]s to σ of models of ''T''. In other words, a class ''K'' of σ-structures is pseudo-elementary [[iff]] there is an elementary class ''K&lt;nowiki&gt;'&lt;/nowiki&gt;'' such that ''K'' consists of precisely the reducts to σ of the structures in ''K&lt;nowiki&gt;'&lt;/nowiki&gt;''.

For obvious reasons, elementary classes are also called '''axiomatizable in first-order logic''', and basic elementary classes are called '''finitely axiomatizable in first-order logic'''. These definitions extend to other logics in the obvious way, but since the first-order case is by far the most important, '''axiomatizable''' implicitly refers to this case when no other logic is specified.

== Conflicting and alternative terminology ==
While the above is nowadays standard terminology in [[model theory|"infinite" model theory]], the slightly different earlier definitions are still in use in [[finite model theory]], where an elementary class may be called a '''Δ-elementary class''', and the terms '''elementary class''' and '''first-order axiomatizable class''' are reserved for basic elementary classes (Ebbinghaus et al. 1994, Ebbinghaus and Flum 2005). Hodges calls elementary classes '''axiomatizable classes''', and he refers to basic elementary classes as '''definable classes'''. He also uses the respective synonyms '''EC class''' and '''EC&lt;math&gt;_\Delta&lt;/math&gt; class''' (Hodges, 1993).

There are good reasons for this diverging terminology. The [[signature (logic)|signature]]s that are considered in general model theory are often infinite, while a single [[first-order logic|first-order]] [[sentence (mathematical logic)|sentence]] contains only finitely many symbols. Therefore, basic elementary classes are atypical in infinite model theory. Finite model theory, on the other hand, deals almost exclusively with finite signatures. It is easy to see that for every finite signature σ and for every class ''K'' of σ-structures closed under isomorphism there is an elementary class &lt;math&gt;K'&lt;/math&gt; of σ-structures such that ''K'' and &lt;math&gt;K'&lt;/math&gt; contain precisely the same finite structures. Hence elementary classes are not very interesting for finite model theorists.

== Easy relations between the notions ==

Clearly every basic elementary class is an elementary class, and every elementary class is a pseudo-elementary class. Moreover, as an easy consequence of the [[compactness theorem]], a class of σ-structures is basic elementary if and only if it is elementary and its complement is also elementary.

== Examples ==
=== A basic elementary class ===
Let σ be a signature consisting only of a [[unary function]] symbol ''f''.  The class ''K'' of σ-structures in which ''f'' is [[injection (mathematics)|one-to-one]] is a basic elementary class. This is witnessed by the theory ''T'', which consists only of the single sentence 
:&lt;math&gt;\forall x\forall y( (f(x)=f(y)) \to (x=y) )&lt;/math&gt;.

=== An elementary, basic pseudoelementary class that is not basic elementary ===
Let σ be an arbitrary signature. The class ''K'' of all infinite σ-structures is elementary. To see this, consider the sentences

:&lt;math&gt;\rho_2={}&lt;/math&gt; "&lt;math&gt;\exist x_1\exist x_2(x_1 \not =x_2)&lt;/math&gt;",

:&lt;math&gt;\rho_3={}&lt;/math&gt; "&lt;math&gt;\exist x_1\exist x_2\exist x_3((x_1 \not =x_2) \land (x_1 \not =x_3) \land (x_2 \not =x_3))&lt;/math&gt;",

and so on. (So the sentence &lt;math&gt;\rho_n&lt;/math&gt; says that there are at least ''n'' elements.) The infinite σ-structures are precisely the models of the theory

:&lt;math&gt;T_\infty=\{\rho_2, \rho_3, \rho_4, \dots\}&lt;/math&gt;.

But ''K'' is not a basic elementary class. Otherwise the infinite σ-structures would be precisely those that satisfy a certain first-order sentence τ. But then the set
&lt;math&gt;\{\neg\tau, \rho_2, \rho_3, \rho_4, \dots\}&lt;/math&gt; would be inconsistent. By the [[compactness theorem]], for some natural number ''n'' the set &lt;math&gt;\{\neg\tau, \rho_2, \rho_3, \rho_4, \dots, \rho_n\}&lt;/math&gt; would be inconsistent. But this is absurd, because this theory is satisfied by any σ-structure with &lt;math&gt;n+1&lt;/math&gt; or more elements.

However, there is a basic elementary class ''K&lt;nowiki&gt;'&lt;/nowiki&gt;'' in the signature σ' = σ &lt;math&gt;\cup&lt;/math&gt; {''f''}, where ''f'' is a unary function symbol, such that ''K'' consists exactly of the reducts to σ of σ'-structures in ''K&lt;nowiki&gt;'&lt;/nowiki&gt;''. ''K&lt;nowiki&gt;'&lt;/nowiki&gt;'' is axiomatised by the single sentence &lt;math&gt;(\forall x\forall y(f(x) = f(y) \rightarrow x=y) \land \exists y\neg\exists x(y = f(x))),&lt;/math&gt;, which expresses that ''f'' is injective but not surjective. Therefore, ''K'' is elementary and what could be called basic pseudo-elementary, but not basic elementary.

=== Pseudo-elementary class that is non-elementary ===
Finally, consider the signature σ consisting of a single unary relation symbol ''P''. Every σ-structure is [[partition of a set|partitioned]] into two subsets: Those elements for which ''P'' holds, and the rest. Let ''K'' be the class of all σ-structures for which these two subsets have the same [[cardinality]], i.e., there is a bijection between them. This class is not elementary, because a σ-structure in which both the set of realisations of ''P'' and its complement are countably infinite satisfies precisely the same first-order sentences as a σ-structure in which one of the sets is countably infinite and the other is uncountable.

Now consider the signature &lt;math&gt;\sigma'&lt;/math&gt;, which consists of ''P'' along with a unary function symbol ''f''. Let &lt;math&gt;K'&lt;/math&gt; be the class of all &lt;math&gt;\sigma'&lt;/math&gt;-structures such that ''f'' is a bijection and ''P'' holds for ''x'' [[iff]] ''P'' does not hold for ''f(x)''. &lt;math&gt;K'&lt;/math&gt; is clearly an elementary class, and therefore ''K'' is an example of a pseudo-elementary class that is not elementary.

=== Non-pseudo-elementary class===
Let σ be an arbitrary signature. The class ''K'' of all finite σ-structures is not elementary, because (as shown above) its complement is elementary but not basic elementary. Since this is also true for every signature extending σ, ''K'' is not even a pseudo-elementary class.

This example demonstrates the limits of expressive power inherent in [[first-order logic]] as opposed to the far more expressive [[second-order logic]]. Second-order logic, however, fails to retain many desirable properties of first-order logic, such as the [[Gödel's_completeness_theorem|completeness]] and [[compactness theorem|compactness]] theorems.

== References ==

* {{Citation | last1=Chang | first1=Chen Chung | last2=Keisler | first2=H. Jerome | author2-link=Howard Jerome Keisler | title=Model Theory | origyear=1973 | publisher=[[Elsevier]] | edition=3rd | series=Studies in Logic and the Foundations of Mathematics | isbn=978-0-444-88054-3 | year=1990}}
* {{Citation | last1=Ebbinghaus | first1=Heinz-Dieter | last2=Flum | first2=Jörg | title=Finite model theory | origyear=1995 | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-3-540-28787-2 | year=2005 | pages=360}}
* {{Citation | last1=Ebbinghaus | first1=Heinz-Dieter | last2=Flum | first2=Jörg | last3=Thomas | first3=Wolfgang | title=Mathematical Logic | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | isbn=978-0-387-94258-2 | year=1994}}
* {{Citation | last1=Hodges | first1=Wilfrid | author1-link=Wilfrid Hodges | title=A shorter model theory | publisher=[[Cambridge University Press]] | isbn=978-0-521-58713-6 | year=1997}}
* {{Citation | last1=Poizat | first1=Bruno | title=A Course in Model Theory: An Introduction to Contemporary Mathematical Logic | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-0-387-98655-5 | year=2000}}

{{DEFAULTSORT:Elementary Class}}
[[Category:Model theory]]</text>
      <sha1>7b75071u6jyptyweol8v0bmyoaii3mr</sha1>
    </revision>
  </page>
  <page>
    <title>Embedded pushdown automaton</title>
    <ns>0</ns>
    <id>14345961</id>
    <revision>
      <id>859390022</id>
      <parentid>729553493</parentid>
      <timestamp>2018-09-13T19:07:40Z</timestamp>
      <contributor>
        <username>JCW-CleanerBot</username>
        <id>31737083</id>
      </contributor>
      <minor/>
      <comment>[[User:JCW-CleanerBot#Logic|task]], replaced: J. Comput. System Sci. → J. Comput. Syst. Sci.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9824">An '''embedded pushdown automaton''' or '''EPDA''' is a [[computational model]] for parsing languages generated by [[tree-adjoining grammar]]s (TAGs). It is similar to the [[context-free grammar]]-parsing [[pushdown automaton]], except that instead of using a plain [[stack (data structure)|stack]] to store symbols, it has a stack of iterated stacks that store symbols, giving TAGs a generative capacity between context-free grammars and [[context-sensitive grammar]]s, or a subset of the [[mildly context-sensitive grammar]]s.
Embedded pushdown automata should not be confused with [[nested stack automata]]  which have more computational power.{{citation needed|reason=This claim is currently supported only by the order in which both notions appear in the 'Automata theory: formal languages and formal grammars' overview table below.|date=February 2014}}

==History and applications==
EPDAs were first described by K. Vijay-Shanker in his 1988 doctoral thesis.&lt;ref&gt;{{cite journal |last=Vijay-Shanker |first=K. |authorlink= |date=January 1988 |title= A Study of Tree-Adjoining Grammars |journal=Ph.D. Thesis |publisher= [[University of Pennsylvania]] |volume= |issue= |pages= |id= |url= http://repository.upenn.edu/dissertations/AAI8804974 |accessdate= |quote= }}&lt;/ref&gt; They have since been applied to more complete descriptions of classes of mildly context-sensitive grammars and have had important roles in refining the [[Chomsky hierarchy]]. Various subgrammars, such as the [[linear indexed grammar]], can thus be defined.&lt;ref&gt;{{cite journal |last=Weir |first=David J. |year=1994 |title=Linear Iterated Pushdowns|journal=Computational Intelligence|volume=10 |issue= 4|pages=431–439 |url=http://www.sussex.ac.uk/Users/davidw/resources/papers/ci94.pdf|accessdate=2012-10-20|doi=10.1111/j.1467-8640.1994.tb00007.x}}&lt;/ref&gt; EPDAs are also beginning to play an important role in natural language processing.

While natural languages have traditionally been analyzed using context-free grammars (see [[transformational-generative grammar]] and [[computational linguistics]]), this model does not work well for languages with crossed dependencies, such as Dutch, situations for which an EPDA is well suited. A detailed linguistic analysis is available in Joshi, Schabes (1997).&lt;ref name="Joshi.Schabes.1997"&gt;{{cite journal |last=Joshi |first=Aravind K. |authorlink= |author2=Yves Schabes |year=1997 |title= Tree-Adjoining Grammars |journal=Handbook of Formal Languages |publisher=Springer |volume=3 |issue= |pages=69–124 |url= http://www.seas.upenn.edu/~joshi/joshi-schabes-tag-97.pdf |accessdate= 2014-02-07 |quote= }}&lt;/ref&gt;

==Theory==
An EPDA is a finite state machine with a set of stacks that can be themselves accessed through the ''embedded stack''. Each stack contains elements of the ''stack alphabet'' &lt;math&gt;\,\Gamma&lt;/math&gt;, and so we define an element of a stack by &lt;math&gt;\,\sigma_i \in \Gamma^*&lt;/math&gt;, where the star is the [[Kleene closure]] of the alphabet.

Each stack can then be defined in terms of its elements, so we denote the &lt;math&gt;\,j&lt;/math&gt;th stack in the automaton using a double-dagger symbol: &lt;math&gt;\,\Upsilon_j = \ddagger\sigma_j = \{\sigma_{j,k}, \sigma_{j,k-1}, \ldots, \sigma_{j,1} \}&lt;/math&gt;,{{clarify|reason=Usually, curly braces build a set (where element ordering and repetition is ignored, e.g. {a,c,a,b} denotes the same set as {a,b,c}), while in a stack, ordering and repetition does matter. Probably, the notation used here is wrong, or at least misleading.|date=February 2014}} where &lt;math&gt;\,\sigma_{j, k}&lt;/math&gt; would be the next accessible symbol in the stack. The ''embedded stack'' of &lt;math&gt;\,m&lt;/math&gt; stacks can thus be denoted by &lt;math&gt;\,\{\Upsilon_j \} = \{\ddagger\sigma_m,\ddagger\sigma_{m-1}, \ldots, \ddagger\sigma_1 \} \in (\ddagger\Gamma^+)^*&lt;/math&gt;.{{clarify|reason=In the sentence before, (upsilon sub j) was explained to equal (ddagger (sigma sub j)), hence {upsilon sub j} would equal {ddagger (sigma sub j)}; however, now it is claimed to equal {ddagger (sigma sub m),...,ddagger (sigma sub 1)} instead. Moreover, in the rightmost term, it should be explained whether (((ddagger Gamma) sup +) sup *) or ((ddagger (Gamma sup +)) sup *) is meant, and what ddagger applied to the stack alphabet Gamma,or to (Gamma sup +), is supposed to mean. Finally, is should be explicitly mentioned that +, as I guessed, denotes the Kleene plus.|date=February 2014}}

We define an EPDA by the septuple (7-tuple)

:&lt;math&gt;\,M = (Q, \Sigma, \Gamma, \delta, q_0, Q_\textrm{F}, \sigma_0)&lt;/math&gt; where

* &lt;math&gt;\,Q&lt;/math&gt; is a finite set of ''states'';
* &lt;math&gt;\,\Sigma&lt;/math&gt; is the finite set of the ''input alphabet'';
* &lt;math&gt;\,\Gamma&lt;/math&gt; is the finite ''stack alphabet'';
* &lt;math&gt;\,q_0 \in Q&lt;/math&gt; is the ''start state'';
* &lt;math&gt;\,Q_\textrm{F} \subseteq Q&lt;/math&gt; is the set of ''final states'';
* &lt;math&gt;\,\sigma_0 \in \Gamma&lt;/math&gt; is the ''initial stack symbol''
* &lt;math&gt;\,\delta : Q \times \Sigma \times \Gamma \rightarrow S&lt;/math&gt; is the ''transition function'', where &lt;math&gt;\,S&lt;/math&gt; are finite subsets of &lt;math&gt;\,Q\times (\ddagger\Gamma^+)^* \times \Gamma^* \times (\ddagger\Gamma^+)^*&lt;/math&gt;.

Thus the transition function takes a state, the next symbol of the input string, and the top symbol of the current stack and generates the next state, the stacks to be pushed and popped onto the ''embedded stack'', the pushing and popping of the current stack, and the stacks to be considered the current stacks in the next transition. More conceptually, the ''embedded stack'' is pushed and popped, the current stack is optionally pushed back onto the ''embedded stack'', and any other stacks one would like are pushed on top of that, with the last stack being the one read from in the next iteration. Therefore, stacks can be pushed both above and below the current stack.

A given configuration is defined by

:&lt;math&gt;\,C(M) = \{q,\Upsilon_m \ldots \Upsilon_1, x_1, x_2\} \in Q\times (\ddagger\Gamma^+)^* \times \Sigma^* \times \Sigma^*&lt;/math&gt;

where &lt;math&gt;\,q&lt;/math&gt; is the current state, the &lt;math&gt;\,\Upsilon&lt;/math&gt;s are the stacks in the ''embedded stack'', with &lt;math&gt;\,\Upsilon_m&lt;/math&gt; the current stack, and for an input string &lt;math&gt;\,x=x_1 x_2 \in \Sigma^*&lt;/math&gt;, &lt;math&gt;\,x_1&lt;/math&gt; is the portion of the string already processed by the machine and &lt;math&gt;\,x_2&lt;/math&gt; is the portion to be processed, with its head being the current symbol read. Note that the empty string &lt;math&gt;\,\epsilon \in \Sigma&lt;/math&gt; is implicitly defined as a terminating symbol, where if the machine is at a final state when the empty string is read, the entire input string is ''accepted'', and if not it is ''rejected''. Such ''accepted'' strings are elements of the language

:&lt;math&gt;\,L(M) = \left\{ x | \{q_0,\Upsilon_0,\epsilon,x\} \rightarrow_M^* \{q_\textrm{F},\Upsilon_m \ldots \Upsilon_1, x, \epsilon\} \right\}&lt;/math&gt;

where &lt;math&gt;\,q_\textrm{F} \in Q_\textrm{F}&lt;/math&gt; and &lt;math&gt;\,\rightarrow_M^*&lt;/math&gt; defines the transition function applied over as many times as necessary to parse the string.

An informal description of EPDA can also be found in Joshi, Schabes (1997),&lt;ref name="Joshi.Schabes.1997"/&gt; Sect.7, p.&amp;nbsp;23-25.

== ''k''-order EPDA and the Weir hierarchy ==
{{cleanup|section|reason=needs rewrite based on Kallmeyer p. 199|date=August 2014}}
A more precisely defined hierarchy of languages that correspond to the mildly context-sensitive class was defined by David J. Weir.&lt;ref&gt;{{Citation
  | last=Weir
  | first=D. J.
  | year=1992
  | title =A geometric hierarchy beyond context-free languages
  | journal = Theoretical computer science
  | volume = 104
  | issue = 2
  | pages = 235–261
  | url = http://www.sciencedirect.com/science/article/pii/030439759290124X/pdf?md5=f60b5b05ab0465019c8284932874b711&amp;pid=1-s2.0-030439759290124X-main.pdf
  | doi=10.1016/0304-3975(92)90124-X
  | postscript=.
}}&lt;/ref&gt;
Based on the work of Nabil A. Khabbaz,&lt;ref&gt;{{cite thesis| type=Ph.D.| author=Nabil Anton Khabbaz| title=Generalized context-free languages| year=1972| publisher=University of Iowa}}&lt;/ref&gt;&lt;ref&gt;{{cite journal| author=Nabil Anton Khabbaz| title=A geometric hierarchy of languages| journal=J. Comput. Syst. Sci.| year=1974| volume=8| issue=2| pages=142–157| url=http://www.sciencedirect.com/science/article/pii/S0022000074800528/pdf?md5=628f9105af6c4854d039d6ec28c89a73&amp;pid=1-s2.0-S0022000074800528-main.pdf| doi=10.1016/s0022-0000(74)80052-8}}&lt;/ref&gt;
Weir's Control Language Hierarchy is a containment {{clarify span|hierarchy of countable set of language classes|date=August 2014}} where the ''Level-1'' is defined as context-free, and ''Level-2'' is the class of tree-adjoining and the other three grammars.

Following are some of the properties of Level-''k'' languages in the hierarchy:
*Level-''k'' languages are properly contained in the Level-(''k''&amp;nbsp;+&amp;nbsp;1) language class
*Level-''k'' languages can be parsed in &lt;math&gt;O(n^{3\cdot2^{k-1}})&lt;/math&gt; time
*Level-''k'' contains the language &lt;math&gt;\{a_1^n \dotso a_{2^k}^n|n\geq0\}&lt;/math&gt;, but not &lt;math&gt;\{a_1^n \dotso a_{2^{k+1}}^n|n\geq0\}&lt;/math&gt;
*Level-''k'' contains the language &lt;math&gt;\{w^{2^{k-1}}|w\in\{a,b\}^*\}&lt;/math&gt;, but not &lt;math&gt;\{w^{2^{k-1}+1}|w\in\{a,b\}^*\}&lt;/math&gt;
Those properties correspond well (at least for small ''k''&amp;nbsp;&gt;&amp;nbsp;1) to the conditions of mildly context-sensitive languages imposed by Joshi, and as ''k'' gets bigger, the language class becomes, in a sense, less mildly context-sensitive.

== See also ==
* [[combinatory categorial grammar]]

==References==
{{reflist}}

==Further reading==
* {{cite book|author=Laura Kallmeyer|title=Parsing Beyond Context-Free Grammars|year=2010|publisher=Springer Science &amp; Business Media|isbn=978-3-642-14846-0}}

{{Formal languages and grammars}}

[[Category:Models of computation]]
[[Category:Automata (computation)]]</text>
      <sha1>1t105eg5tqsuqcz8gu6qdblgsjaiaje</sha1>
    </revision>
  </page>
  <page>
    <title>Entropy (information theory)</title>
    <ns>0</ns>
    <id>15445</id>
    <revision>
      <id>871058360</id>
      <parentid>867732860</parentid>
      <timestamp>2018-11-28T17:38:29Z</timestamp>
      <contributor>
        <username>Chris the speller</username>
        <id>525927</id>
      </contributor>
      <minor/>
      <comment>number fmt</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="59356">{{other uses|Entropy (disambiguation)}}
{{multiple issues|{{technical|date=November 2017}}
{{more citations needed|date=April 2012}}}}
{{Use dmy dates|date=July 2013}}

[[File:Entropy flip 2 coins.jpg|thumb|300px|Two bits of entropy: In the case of two fair coin tosses, the information entropy in bits is the base-2 logarithm of the number of possible outcomes; with two coins there are four possible outcomes, and two bits of entropy. Generally, information entropy is the average amount of information conveyed by an event, when considering all possible outcomes.]]

'''Information entropy''' is the [[expected value|average]] rate at which [[information]] is produced by a [[stochastic]] source of data.

The measure of information entropy associated with each possible data value is the negative [[logarithm]] of the [[probability mass function]] for the value: &lt;math&gt; S = - \sum_i P_i\ln{P_i} &lt;/math&gt; &lt;ref name=pathriaBook&gt;{{cite book |last1=Pathria |first1=R. K. |last2=Beale | first2=Paul |date=2011 |title=Statistical Mechanics (Third Edition) |url=https://books.google.com/books/about/Statistical_Mechanics.html?id=KdbJJAXQ-RsC&amp;printsec=frontcover&amp;source=kp_read_button#v=onepage&amp;q&amp;f=false |publisher=Academic Press |page=51 |isbn=978-0123821881}}&lt;/ref&gt;. Thus, when the data source has a lower-probability value (i.e., when a low-probability event occurs), the event carries more "information" ("surprisal") than when the source data has a higher-probability value. The amount of information conveyed by each event defined in this way becomes a [[random variable]] whose expected value is the information entropy. Generally, ''entropy'' refers to disorder or uncertainty, and the definition of entropy used in information theory is directly analogous to the [[Entropy (statistical thermodynamics)|definition used]] in [[statistical thermodynamics]]. The concept of information entropy was introduced by [[Claude Shannon]] in his 1948 paper "[[A Mathematical Theory of Communication]]".&lt;ref name=shannonPaper&gt;{{cite journal |last=Shannon |first=Claude E. |authorlink=Claude Shannon |title=[[A Mathematical Theory of Communication]] |journal=[[Bell System Technical Journal]] |volume=27 |issue=3 |date=July–October 1948 |doi=10.1002/j.1538-7305.1948.tb01338.x}} ([https://web.archive.org/web/20120615000000*/https://www.alcatel-lucent.com/bstj/vol27-1948/articles/bstj27-3-379.pdf PDF], archived from [http://www.alcatel-lucent.com/bstj/vol27-1948/articles/bstj27-3-379.pdf here])&lt;/ref&gt;{{rp|379–423}}

The basic model of a [[data communication]] system is composed of three elements, a source of data, a [[communication channel]], and a receiver, and – as expressed by Shannon – the "fundamental problem of communication" is for the receiver to be able to identify what data was generated by the source, based on the signal it receives through the channel.&lt;ref&gt;{{cite journal |url=http://math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf |title=A Mathematical Theory of Communication |first=Claude E. |last=Shannon |journal=[[Bell System Technical Journal]] |volume=27 |year=1948}}, July and October&lt;/ref&gt;{{rp|379–423 and 623–656}} The entropy provides an absolute limit on the shortest possible average length of a [[lossless]] [[data compression|compression]] encoding of the data produced by a source, and if the entropy of the source is less than the [[channel capacity]] of the communication channel, the data generated by the source can be reliably communicated to the receiver (at least in theory, possibly neglecting some practical considerations such as the complexity of the system needed to convey the data and the amount of time it may take for the data to be conveyed).

Information entropy is typically measured in [[bit]]s (alternatively called "[[shannon (unit)|shannon]]s") or sometimes in "natural units" ([[nat (unit)|nat]]s) or decimal digits (called "dits", "bans", or "hartleys"). The unit of the measurement depends on the base of the logarithm that is used to define the entropy.

The logarithm of the probability distribution is useful as a measure of entropy because it is additive for independent sources. For instance, the entropy of a fair coin toss is 1 bit, and the entropy of {{math|''m''}} tosses is {{math|''m''}} bits. In a straightforward representation, {{math|log&lt;sub&gt;2&lt;/sub&gt;(''n'')}} bits are needed to represent a variable that can take one of {{math|''n''}} values if {{math|''n''}} is a power of 2. If these values are equally probable, the entropy (in bits) is equal to this number. If one of the values is more probable to occur than the others, an observation that this value occurs is less informative than if some less common outcome had occurred. Conversely, rarer events provide more information when observed. Since observation of less probable events occurs more rarely, the net effect is that the entropy (thought of as average information) received from non-uniformly distributed data is always less than or equal to {{math|log&lt;sub&gt;2&lt;/sub&gt;(''n'')}}. Entropy is zero when one outcome is certain to occur. The entropy quantifies these considerations when a probability distribution of the source data is known. The ''meaning'' of the events observed (the meaning of ''messages'') does not matter in the definition of entropy. Entropy only takes into account the probability of observing a specific event, so the information it encapsulates is information about the underlying probability distribution, not the meaning of the events themselves.

==Introduction==
The basic idea of information theory is the more one knows about a topic, the less new information one is apt to get about it. If an event is very probable, it is no surprise when it happens and thus provides little new information. Inversely, if the event was improbable, it is much more informative that the event happened. Therefore, the [[self-information|information content]] is an increasing function of the inverse of the probability of the event (1/p). Now, if more events may happen, entropy measures the average  information content you can expect to get if one of the events actually happens. This implies that casting a die has more entropy than tossing a coin because each outcome of the die has smaller probability than each outcome of the coin.

Thus, entropy is a measure of ''unpredictability'' of the state, or equivalently, of its ''average information content''. To get an intuitive understanding of these terms, consider the example of a political poll. Usually, such polls happen because the outcome of the poll is not already known. In other words, the outcome of the poll is relatively ''unpredictable'', and actually performing the poll and learning the results gives some new ''information''; these are just different ways of saying that the ''a priori'' entropy of the poll results is large. Now, consider the case that the same poll is performed a second time shortly after the first poll. Since the result of the first poll is already known, the outcome of the second poll can be predicted well and the results should not contain much new information; in this case the ''a priori'' entropy of the second poll result is small relative to that of the first.

Now consider the example of a coin toss. Assuming the probability of heads is the same as the probability of tails, then the entropy of the coin toss is as high as it could be. This is because there is no way to predict the outcome of the coin toss ahead of time: if we have to choose, the best we can do is predict that the coin will come up heads, and this prediction will be correct with probability 1/2. Such a coin toss has one bit of entropy since there are two possible outcomes that occur with equal probability, and learning the actual outcome contains one bit of information. In contrast, a coin toss using a coin that has two heads and no tails has zero entropy since the coin will always come up heads, and the outcome can be predicted perfectly. Analogously, a binary event with equiprobable outcomes has a Shannon entropy of &lt;math&gt;\log_2 2=1&lt;/math&gt; bit. Similarly, one [[Ternary numeral system|trit]] with equiprobable values contains &lt;math&gt;\log_2 3&lt;/math&gt; (about 1.58496) bits of information because it can have one of three values.

English text, treated as a string of characters, has fairly low entropy, i.e., is fairly predictable. Even if we do not know exactly what is going to come next, we can be fairly certain that, for example, 'e' will be far more common than 'z', that the combination 'qu' will be much more common than any other combination with a 'q' in it, and that the combination 'th' will be more common than 'z', 'q', or 'qu'. After the first few letters one can often guess the rest of the word. English text has between 0.6 and 1.3 bits of entropy per character of the message.&lt;ref name="Schneier, B page 234"&gt;Schneier, B: ''Applied Cryptography'', Second edition, John Wiley and Sons.&lt;/ref&gt;{{rp|234}}

If a [[Data compression|compression]] scheme is lossless—that is, you can always recover the entire original message by decompressing—then a compressed message has the same quantity of information as the original, but communicated in fewer characters. That is, it has more information, or a higher entropy, per character. This means a compressed message has less redundancy. Roughly speaking, [[Shannon's source coding theorem]] says that a lossless compression scheme cannot compress messages, on average, to have ''more'' than one bit of information per bit of message, but that any value ''less'' than one bit of information per bit of message can be attained by employing a suitable coding scheme. The entropy of a message per bit multiplied by the length of that message is a measure of how much total information the message contains.

Intuitively, imagine that we wish to transmit sequences comprising the 4 characters 'A', 'B', 'C', and 'D'. Thus, a message to be transmitted might be 'ABADDCAB'. Information theory gives a way of calculating the smallest possible amount of information that will convey this. If all 4 letters are equally likely (25%), we can do no better (over a binary channel) than to have 2 bits encode (in binary) each letter: 'A' might code as '00', 'B' as '01', 'C' as '10', and 'D' as '11'. Now suppose 'A' occurs with 70% probability, 'B' with 26%, and 'C' and 'D' with 2% each. We could assign variable length codes, so that receiving a '1' tells us to look at another bit unless we have already received 2 bits of sequential 1s. In this case, 'A' would be coded as '0' (one bit), 'B' as '10', and 'C' and 'D' as '110' and '111'. It is easy to see that 70% of the time only one bit needs to be sent, 26% of the time two bits, and only 4% of the time 3 bits. On average, then, fewer than 2 bits are required since the entropy is lower (owing to the high prevalence of 'A' followed by 'B' – together 96% of characters). The calculation of the sum of probability-weighted log probabilities measures and captures this effect.

Shannon's theorem also implies that no lossless compression scheme can shorten ''all'' messages. If some messages come out shorter, at least one must come out longer due to the [[pigeonhole principle]]. In practical use, this is generally not a problem, because we are usually only interested in compressing certain types of messages, for example English documents as opposed to gibberish text, or digital photographs rather than noise, and it is unimportant if a compression algorithm makes some unlikely or uninteresting sequences larger.

==Definition==
Named after [[H-theorem|Boltzmann's Η-theorem]], Shannon defined the entropy {{math|&amp;Eta;}} (Greek capital letter [[eta]]) of a [[discrete random variable]] &lt;math display="inline"&gt;X&lt;/math&gt; with possible values &lt;math display="inline"&gt;\left\{x_1, \ldots, x_n \right\}&lt;/math&gt; and [[probability mass function]] &lt;math display="inline"&gt;\mathrm{P}(X)&lt;/math&gt; as:

:&lt;math&gt;\Eta(X) = \mathbb{E}[\operatorname{I}(X)]
= \mathbb{E}[-\log(\mathrm{P}(X))].&lt;/math&gt;

Here &lt;math&gt;\mathbb{E}&lt;/math&gt; is the [[expected value|expected value operator]], and {{math|I}} is the [[self-information|information content]] of {{math|''X''}}.&lt;ref&gt;{{cite book|author=Borda, Monica|title=Fundamentals in Information Theory and Coding|publisher=Springer|year=2011|isbn=978-3-642-20346-6|url=https://books.google.com/books?id=Lyte2yl1SPAC&amp;pg=PA11}}&lt;/ref&gt;{{rp|11}}&lt;ref&gt;{{cite book|authors=Han, Te Sun &amp; Kobayashi, Kingo|title=Mathematics of Information and Coding|publisher=American Mathematical Society|year=2002|isbn=978-0-8218-4256-0|url=https://books.google.com/books?id=VpRESN24Zj0C&amp;pg=PA19}}&lt;/ref&gt;{{rp|19–20}}
{{math|I(''X'')}} is itself a random variable.

The entropy can explicitly be written as
:&lt;math&gt;\Eta(X) = \sum_{i=1}^n {\mathrm{P}(x_i)\,\mathrm{I}(x_i)} = -\sum_{i=1}^n {\mathrm{P}(x_i) \log_b \mathrm{P}(x_i)},&lt;/math&gt;

where {{math|''b''}} is the [[base (exponentiation)|base]] of the [[logarithm]] used. Common values of {{math|''b''}} are 2, [[e (mathematical constant)|Euler's number {{math|''e''}}]], and 10, and the corresponding units of entropy are the [[bit (unit)|bit]]s for {{math|''b'' {{=}} 2}}, [[Nat (unit)|nats]] for {{math|''b'' {{=}} ''e''}}, and [[ban (unit)|ban]]s for {{math|''b'' {{=}} 10}}.&lt;ref&gt;Schneider, T.D, [http://alum.mit.edu/www/toms/paper/primer/primer.pdf Information theory primer with an appendix on logarithms], National Cancer Institute, 14 April 2007.&lt;/ref&gt;

In the case of {{math|P(''x''&lt;sub&gt;''i''&lt;/sub&gt;) {{=}} 0}} for some {{math|''i''}}, the value of the corresponding summand {{math|0 log&lt;sub&gt;''b''&lt;/sub&gt;(0)}} is taken to be {{math|0}}, which is consistent with the [[limit of a function|limit]]:

:&lt;math&gt;\lim_{p\to0+}p\log (p) = 0.&lt;/math&gt;

One may also define the [[conditional entropy]] of two events {{math|''X''}} and {{math|''Y''}} taking values {{math|''x''&lt;sub&gt;''i''&lt;/sub&gt;}} and {{math|''y''&lt;sub&gt;''j''&lt;/sub&gt;}} respectively, as

:&lt;math&gt; \Eta(X|Y)=-\sum_{i,j}p(x_{i},y_{j})\log\frac{p(x_{i},y_{j})}{p(y_{j})}&lt;/math&gt;

where {{math|''p''(''x''&lt;sub&gt;''i''&lt;/sub&gt;, ''y''&lt;sub&gt;''j''&lt;/sub&gt;)}} is the probability that {{math|''X'' {{=}} ''x''&lt;sub&gt;''i''&lt;/sub&gt;}} and {{math|''Y'' {{=}} ''y''&lt;sub&gt;''j''&lt;/sub&gt;}}. This quantity should be understood as the amount of randomness in the random variable {{math|''X''}} given the event {{math|''Y''}}.

==Example==
[[File:Binary entropy plot.svg|thumbnail|right|200px|Entropy {{math|Η(''X'')}} (i.e. the [[expected value|expected]] [[self-information|surprisal]]) of a coin flip, measured in bits, graphed versus the bias of the coin {{math|Pr(''X'' {{=}} 1)}}, where {{math|''X'' {{=}} 1}} represents a result of heads.&lt;br/&gt;&lt;br/&gt;Here, the entropy is at most 1 bit, and to communicate the outcome of a coin flip (2 possible values) will require an average of at most 1 bit (exactly 1 bit for a fair coin). The result of a fair die (6 possible values) would require on average log&lt;sub&gt;2&lt;/sub&gt;6 bits.]]
{{Main|Binary entropy function|Bernoulli process}}
Consider tossing a coin with known, not necessarily fair, probabilities of coming up heads or tails; this can be modelled as a [[Bernoulli process]].

The entropy of the unknown result of the next toss of the coin is maximized if the coin is fair (that is, if heads and tails both have equal probability 1/2). This is the situation of maximum uncertainty as it is most difficult to predict the outcome of the next toss; the result of each toss of the coin delivers one full [[bit]] of information. This is because
:&lt;math&gt;\begin{align}
\Eta(X) &amp;= -\sum_{i=1}^n {\mathrm{P}(x_i) \log_b \mathrm{P}(x_i)} 
\\ &amp;= -\sum_{i=1}^2 {\frac{1}{2}\log_2{\frac{1}{2}}} 
\\ &amp;= -\sum_{i=1}^2 {\frac{1}{2} \cdot (-1)} = 1
\end{align}&lt;/math&gt;

However, if we know the coin is not fair, but comes up heads or tails with probabilities {{math|''p''}} and {{math|''q''}}, where {{math|''p'' ≠ ''q''}}, then there is less uncertainty. Every time it is tossed, one side is more likely to come up than the other. The reduced uncertainty is quantified in a lower entropy: on average each toss of the coin delivers less than one full [[bit]] of information. For example, if {{math|''p''}}=0.7, then
:&lt;math display="block"&gt;\begin{align}
\Eta(X) &amp;=  - p \log_2 (p) - q \log_2 (q)
\\ &amp;= - 0.7 \log_2 (0.7) - 0.3 \log_2 (0.3) 
\\ &amp;\approx - 0.7 \cdot (-0.515) - 0.3 \cdot (-1.737) 
\\ &amp;= 0.8816 &lt; 1
\end{align}&lt;/math&gt;

The extreme case is that of a double-headed coin that never comes up tails, or a double-tailed coin that never results in a head. Then there is no uncertainty. The entropy is zero: each toss of the coin delivers no new information as the outcome of each coin toss is always certain.

Entropy can be normalized by dividing it by information length. This ratio is called [[metric entropy]] and is a measure of the randomness of the information.

==Rationale==
To understand the meaning of {{math|-∑ ''p''&lt;sub&gt;''i''&lt;/sub&gt; log(''p''&lt;sub&gt;''i''&lt;/sub&gt;)}}, first define an information function {{math|I}} in terms of an event {{math|''i''}} with probability {{math|''p''&lt;sub&gt;''i''&lt;/sub&gt;}}. The amount of information acquired due to the observation of event {{math|''i''}} follows from Shannon's solution of the fundamental [[Entropy (information theory)#Characterization|properties]] of [[Information content|information]]:&lt;ref&gt;{{cite book |last=Carter |first=Tom |date=March 2014|title=An introduction to information theory and entropy |url= http://csustan.csustan.edu/~tom/Lecture-Notes/Information-Theory/info-lec.pdf|location=Santa Fe|publisher= |page= |isbn= |access-date=4 August 2017}}&lt;/ref&gt;
# {{math|I(''p'')}} is [[monotonically decreasing]] in {{math|''p''}} – an increase in the probability of an event decreases the information from an observed event, and vice versa.
# {{math|I(''p'') ≥ 0}} – information is a [[non-negative]] quantity.
# {{math|I(1) {{=}} 0}} – events that always occur do not communicate information.
# {{math|I(''p''&lt;sub&gt;1&lt;/sub&gt; ''p''&lt;sub&gt;2&lt;/sub&gt;) {{=}} I(''p''&lt;sub&gt;1&lt;/sub&gt;) + I(''p''&lt;sub&gt;2&lt;/sub&gt;)}} – information due to [[independent events]] is additive.

The last is a crucial property. It states that joint probability of independent sources of information communicates as much information as the two individual events separately. Particularly, if the first event can yield one of {{math|''n''}} [[equiprobable]] outcomes and another has one of {{math|''m''}} equiprobable outcomes then there are {{math|''mn''}} possible outcomes of the joint event. This means that if {{math|log&lt;sub&gt;2&lt;/sub&gt;(''n'')}} bits are needed to encode the first value and {{math|log&lt;sub&gt;2&lt;/sub&gt;(''m'')}} to encode the second, one needs {{math|log&lt;sub&gt;2&lt;/sub&gt;(''mn'') {{=}} log&lt;sub&gt;2&lt;/sub&gt;(''m'') + log&lt;sub&gt;2&lt;/sub&gt;(''n'')}} to encode both. Shannon discovered that the proper choice of function to quantify information, preserving this additivity, is logarithmic, i.e.,

:&lt;math&gt;\mathrm{I}(p) = \log\left(\tfrac{1}{p}\right) = -\log(p):&lt;/math&gt;

let &lt;math display="inline"&gt;I&lt;/math&gt; be the information function which one assumes to be twice continuously differentiable, one has:

&lt;math&gt;\begin{array}{lcl}
I(p_1 p_2) &amp;=&amp; I(p_1) +  I(p_2) \\
p_2 I'(p_1 p_2) &amp;=&amp; I'(p_1) \\
I'(p_1 p_2) + p_1 p_2 I''(p_1 p_2) &amp;=&amp; 0 \\
I'(u) + u I''(u) &amp;=&amp; 0 \\
(u \mapsto u I'(u))' &amp;=&amp; 0
\end{array}&lt;/math&gt;

This [[differential equation]] leads to the solution &lt;math&gt;I(u) = k \log u&lt;/math&gt; for any &lt;math&gt;k \in \mathbb{R}&lt;/math&gt;. Condition 2. leads to &lt;math&gt;k &lt; 0&lt;/math&gt; and especially, &lt;math&gt;k &lt;/math&gt; can be chosen on the form &lt;math&gt;k = - 1/\log x&lt;/math&gt; with &lt;math&gt;x &gt; 1&lt;/math&gt;, which is equivalent to choosing a specific [[Base of a logarithm|base for the logarithm]]. The different [[units of information]] ([[Bit|bits]] for the [[binary logarithm]] {{math|log&lt;sub&gt;2&lt;/sub&gt;}}, [[Nat (unit)|nats]] for the [[natural logarithm]] {{math|ln}}, [[Ban (unit)|bans]] for the [[decimal logarithm]] {{math|log&lt;sub&gt;10&lt;/sub&gt;}} and so on) are [[Proportionality (mathematics)|constant multiples]] of each other. For instance, in case of a fair coin toss, heads provides {{math|log&lt;sub&gt;2&lt;/sub&gt;(2) {{=}} 1}} bit of information, which is approximately 0.693&amp;nbsp;nats or 0.301&amp;nbsp;decimal digits. Because of additivity, {{math|''n''}} tosses provide {{math|''n''}} bits of information, which is approximately {{math|0.693''n''}} nats or {{math|0.301''n''}} decimal digits.

If there is a distribution where event {{math|''i''}} can happen with probability {{math|''p''&lt;sub&gt;''i''&lt;/sub&gt;}}, and it is sampled {{math|''N''}} times with an outcome {{math|''i''}} occurring {{math|''n''&lt;sub&gt;''i''&lt;/sub&gt; {{=}} ''N'' ''p''&lt;sub&gt;''i''&lt;/sub&gt;}} times, the total amount of information we have received is 
:&lt;math&gt;\sum_i {n_i \mathrm{I}(p_i)} = -\sum_i {N p_i \log{p_i}}&lt;/math&gt;. 
The ''[[average]]'' amount of information that we receive per event is therefore
:&lt;math&gt;-\sum_i {p_i \log {p_i}}.&lt;/math&gt;

==Aspects==

===Relationship to thermodynamic entropy===
{{Main|Entropy in thermodynamics and information theory}}
The inspiration for adopting the word ''entropy'' in information theory came from the close resemblance between Shannon's formula and very similar known formulae from [[statistical mechanics]].

In [[statistical thermodynamics]] the most general formula for the thermodynamic [[entropy]] {{math|''S''}} of a [[thermodynamic system]] is the [[Gibbs entropy]],
:&lt;math&gt;S = - k_\text{B} \sum p_i \ln p_i \,&lt;/math&gt;
where {{math|''k''&lt;sub&gt;B&lt;/sub&gt;}} is the [[Boltzmann constant]], and {{math|''p''&lt;sub&gt;''i''&lt;/sub&gt;}} is the probability of a [[Microstate (statistical mechanics)|microstate]]. The Gibbs entropy was defined by [[J. Willard Gibbs]] in 1878 after earlier work by [[Ludwig Boltzmann|Boltzmann]] (1872).&lt;ref&gt;Compare: Boltzmann, Ludwig (1896, 1898). Vorlesungen über Gastheorie : 2 Volumes – Leipzig 1895/98 UB: O 5262-6. English version: Lectures on gas theory. Translated by Stephen G. Brush (1964) Berkeley: University of California Press; (1995) New York: Dover {{isbn|0-486-68455-5}}&lt;/ref&gt;

The Gibbs entropy translates over almost unchanged into the world of [[quantum physics]] to give the [[von Neumann entropy]], introduced by [[John von Neumann]] in 1927,
:&lt;math&gt;S = - k_\text{B} \,{\rm Tr}(\rho \ln \rho) \,&lt;/math&gt;
where ρ is the [[density matrix]] of the quantum mechanical system and Tr is the [[Trace (linear algebra)|trace]].

At an everyday practical level the links between information entropy and thermodynamic entropy are not evident. Physicists and chemists are apt to be more interested in ''changes'' in entropy as a system spontaneously evolves away from its initial conditions, in accordance with the [[second law of thermodynamics]], rather than an unchanging probability distribution. And, as the minuteness of [[Boltzmann's constant]] {{math|''k''&lt;sub&gt;B&lt;/sub&gt;}} indicates, the changes in {{math|''S'' / ''k''&lt;sub&gt;B&lt;/sub&gt;}} for even tiny amounts of substances in chemical and physical processes represent amounts of entropy that are extremely large compared to anything in [[data compression]] or [[signal processing]]. Furthermore, in classical thermodynamics the entropy is defined in terms of macroscopic measurements and makes no reference to any probability distribution, which is central to the definition of information entropy.

The connection between thermodynamics and what is now known as information theory was first made by [[Ludwig Boltzmann]] and expressed by his [[Boltzmann's entropy formula|famous equation]]:

:&lt;math&gt;S=k_\text{B} \ln(W)&lt;/math&gt;

where ''S'' is the thermodynamic entropy of a particular macrostate (defined by thermodynamic parameters such as temperature, volume, energy, etc.), ''W'' is the number of microstates (various combinations of particles in various energy states) that can yield the given macrostate, and ''k&lt;sub&gt;B&lt;/sub&gt;'' is [[Boltzmann's constant]]. It is assumed that each microstate is equally likely, so that the probability of a given microstate is ''p&lt;sub&gt;i&lt;/sub&gt; = 1/W''. When these probabilities are substituted into the above expression for the Gibbs entropy (or equivalently ''k&lt;sub&gt;B&lt;/sub&gt;'' times the Shannon entropy), Boltzmann's equation results. In information theoretic terms, the information entropy of a system is the amount of "missing" information needed to determine a microstate, given the macrostate.

In the view of [[Edwin Thompson Jaynes|Jaynes]] (1957), thermodynamic entropy, as explained by [[statistical mechanics]], should be seen as an ''application'' of Shannon's information theory: the thermodynamic entropy is interpreted as being proportional to the amount of further Shannon information needed to define the detailed microscopic state of the system, that remains uncommunicated by a description solely in terms of the macroscopic variables of classical thermodynamics, with the constant of proportionality being just the [[Boltzmann constant]]. For example, adding heat to a system increases its thermodynamic entropy because it increases the number of possible microscopic states of the system that are consistent with the measurable values of its macroscopic variables, thus making any complete state description longer. (See article: ''[[maximum entropy thermodynamics]]''). [[Maxwell's demon]] can (hypothetically) reduce the thermodynamic entropy of a system by using information about the states of individual molecules; but, as [[Rolf Landauer|Landauer]] (from 1961) and co-workers have shown, to function the demon himself must increase thermodynamic entropy in the process, by at least the amount of Shannon information he proposes to first acquire and store; and so the total thermodynamic entropy does not decrease (which resolves the paradox). [[Landauer's principle]] imposes a lower bound on the amount of heat a computer must generate to process a given amount of information, though modern computers are far less efficient.

===Entropy as information content===
{{Main|Shannon's source coding theorem}}
Entropy is defined in the context of a probabilistic model. Independent fair coin flips have an entropy of 1 bit per flip. A source that always generates a long string of B's has an entropy of 0, since the next character will always be a 'B'.

The entropy rate of a data source means the average number of [[bit]]s per symbol needed to encode it. Shannon's experiments with human predictors show an information rate between 0.6 and 1.3 bits per character in English;&lt;ref&gt;{{cite web| url=http://marknelson.us/2006/08/24/the-hutter-prize/ | title=The Hutter Prize | accessdate=2008-11-27 | date=24 August 2006 | author=Mark Nelson}}&lt;/ref&gt; the [[PPM compression algorithm]] can achieve a compression ratio of 1.5 bits per character in English text.

From the preceding example, note the following points:

# The amount of entropy is not always an integer number of bits.
# Many data bits may not convey information. For example, data structures often store information redundantly, or have identical sections regardless of the information in the data structure.

Shannon's definition of entropy, when applied to an information source, can determine the minimum channel capacity required to reliably transmit the source as encoded binary digits (see caveat below in italics). The formula can be derived by calculating the mathematical expectation of the ''amount of information'' contained in a digit from the information source.  ''See also'' [[Shannon–Hartley theorem]].

Shannon's entropy measures the information contained in a message as opposed to the portion of the message that is determined (or predictable). ''Examples of the latter include redundancy in language structure or statistical properties relating to the occurrence frequencies of letter or word pairs, triplets etc.'' See [[Markov chain]].

===Entropy as a measure of diversity===
{{Main|Diversity index}}
Entropy is one of several ways to measure diversity.  Specifically, Shannon entropy is the logarithm of {{math|&lt;sup&gt;1&lt;/sup&gt;D}}, the [[true diversity]] index with parameter equal to 1.

===Data compression===
{{Main|Data compression}}
Entropy effectively bounds the performance of the strongest lossless compression possible, which can be realized in theory by using the [[typical set]] or in practice using [[Huffman coding|Huffman]], [[LZW|Lempel–Ziv]] or [[arithmetic coding]]. See also [[Kolmogorov complexity]]. In practice, compression algorithms deliberately include some judicious redundancy in the form of [[checksum]]s to protect against errors.

=== World's technological capacity to store and communicate information ===
A 2011 study in ''[[Science (journal)|Science]]'' estimates the world's technological capacity to store and communicate optimally compressed information normalized on the most effective compression algorithms available in the year 2007, therefore estimating the entropy of the technologically available sources.&lt;ref name="HilbertLopez2011"&gt;[http://www.sciencemag.org/content/332/6025/60 "The World's Technological Capacity to Store, Communicate, and Compute Information"], Martin Hilbert and Priscila López (2011), [[Science (journal)|Science]], 332(6025); free access to the article through here: martinhilbert.net/WorldInfoCapacity.html&lt;/ref&gt; {{rp|60–65}}
{| class="wikitable"
|+
All figures in entropically compressed [[exabytes]]
|-
! Type of Information !! 1986 !! 2007 
|-
| Storage || 2.6  || 295 
|-
| Broadcast || 432 || 1900 
|-
| Telecommunications || 0.281 || 65 
|}
The authors estimate humankind technological capacity to store information (fully entropically compressed) in 1986 and again in 2007. They break the information into three categories—to store information on a medium, to receive information through a one-way [[broadcast]] networks, or to exchange information through two-way [[telecommunication]] networks.&lt;ref name="HilbertLopez2011"/&gt;

===Limitations of entropy as information content===
There are a number of entropy-related concepts that mathematically quantify information content in some way:
* the '''[[self-information]]''' of an individual message or symbol taken from a given probability distribution,
* the '''entropy''' of a given probability distribution of messages or symbols, and
* the '''[[entropy rate]]''' of a [[stochastic process]].
(The "rate of self-information" can also be defined for a particular sequence of messages or symbols generated by a given stochastic process: this will always be equal to the entropy rate in the case of a [[stationary process]].) Other [[quantities of information]] are also used to compare or relate different sources of information.

It is important not to confuse the above concepts. Often it is only clear from context which one is meant. For example, when someone says that the "entropy" of the English language is about 1 bit per character, they are actually modeling the English language as a stochastic process and talking about its entropy ''rate''. Shannon himself used the term in this way.

However, if we use very large blocks, then the estimate of per-character entropy rate may become artificially low. This is because in reality, the probability distribution of the sequence is not knowable exactly; it is only an estimate. For example, suppose one considers the text of every book ever published as a sequence, with each symbol being the text of a complete book. If there are {{math|''N''}} published books, and each book is only published once, the estimate of the probability of each book is {{math|1/''N''}}, and the entropy (in bits) is {{math|−log&lt;sub&gt;2&lt;/sub&gt;(1/''N'') {{=}} log&lt;sub&gt;2&lt;/sub&gt;(''N'')}}. As a practical code, this corresponds to assigning each book a [[ISBN|unique identifier]] and using it in place of the text of the book whenever one wants to refer to the book. This is enormously useful for talking about books, but it is not so useful for characterizing the information content of an individual book, or of language in general: it is not possible to reconstruct the book from its identifier without knowing the probability distribution, that is, the complete text of all the books. The key idea is that the complexity of the probabilistic model must be considered. [[Kolmogorov complexity]] is a theoretical generalization of this idea that allows the consideration of the information content of a sequence independent of any particular probability model; it considers the shortest [[Computer program|program]] for a [[universal computer]] that outputs the sequence. A code that achieves the entropy rate of a sequence for a given model, plus the codebook (i.e. the probabilistic model), is one such program, but it may not be the shortest.

For example, the Fibonacci sequence is 1, 1, 2, 3, 5, 8, 13, …. Treating the sequence as a message and each number as a symbol, there are almost as many symbols as there are characters in the message, giving an entropy of approximately {{math|log&lt;sub&gt;2&lt;/sub&gt;(''n'')}}. So the first 128 symbols of the Fibonacci sequence has an entropy of approximately 7 bits/symbol. However, the sequence can be expressed using a formula [{{math|F(''n'') {{=}} F(''n''−1) + F(''n''−2)}} for {{math|''n'' {{=}} 3, 4, 5, …}}, {{math|F(1) {{=}}1}}, {{math|F(2) {{=}} 1}}] and this formula has a much lower entropy and applies to any length of the Fibonacci sequence.

===Limitations of entropy in cryptography===
In [[cryptanalysis]], entropy is often roughly used as a measure of the unpredictability of a cryptographic key, though its real [[Uncertainty principle|uncertainty]] is unmeasurable. For example, a 128-bit key that is uniformly randomly generated has 128 bits of entropy. It also takes (on average) &lt;math&gt;2^{128-1}&lt;/math&gt; guesses to break by brute force. However, entropy fails to capture the number of guesses required if the possible keys are not chosen uniformly.&lt;ref&gt;{{cite conference |first1=James |last1=Massey |year=1994 |title=Guessing and Entropy |booktitle=Proc. IEEE International Symposium on Information Theory |url=http://www.isiweb.ee.ethz.ch/archive/massey_pub/pdf/BI633.pdf |accessdate=31 December 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite conference |first1=David |last1=Malone|first2=Wayne |last2=Sullivan |year=2005 |title=Guesswork is not a Substitute for Entropy |booktitle=Proceedings of the Information Technology &amp; Telecommunications Conference |url=http://www.maths.tcd.ie/~dwmalone/p/itt05.pdf |accessdate=31 December 2013}}&lt;/ref&gt; Instead, a measure called ''guesswork'' can be used to measure the effort required for a brute force attack.&lt;ref&gt;{{cite conference |first1=John |last1=Pliam |year=1999 |title=Guesswork and variation distance as measures of cipher security|booktitle=International Workshop on Selected Areas in Cryptography |url=https://link.springer.com/chapter/10.1007/3-540-46513-8_5 |accessdate=23 October 2016}}&lt;/ref&gt;

Other problems may arise from non-uniform distributions used in cryptography. For example, consider a 1,000,000-digit binary [[one-time pad]] using exclusive or. If the pad has 1,000,000 bits of entropy, it is perfect. If the pad has 999,999 bits of entropy, evenly distributed (each individual bit of the pad having 0.999999 bits of entropy) it may provide good security. But if the pad has 999,999 bits of entropy, where the first bit is fixed and the remaining 999,999 bits are perfectly random, then the first bit of the ciphertext will not be encrypted at all.

===Data as a Markov process===
A common way to define entropy for text is based on the [[Markov model]] of text. For an order-0 source (each character is selected independent of the last characters), the binary entropy is:

:&lt;math&gt;\Eta(\mathcal{S}) = - \sum p_i \log_2 p_i ,&lt;/math&gt;

where {{math|''p''&lt;sub&gt;''i''&lt;/sub&gt;}} is the probability of {{math|''i''}}. For a first-order [[Markov source]] (one in which the probability of selecting a character is dependent only on the immediately preceding character), the '''[[entropy rate]]''' is:

:&lt;math&gt;\Eta(\mathcal{S}) = - \sum_i p_i \sum_j  \  p_i (j) \log_2 p_i (j) ,&lt;/math&gt; {{citation needed|date=April 2013}}

where {{math|''i''}} is a '''state''' (certain preceding characters) and &lt;math&gt;p_i(j)&lt;/math&gt; is the probability of {{math|''j''}} given {{math|''i''}} as the previous character.

For a second order Markov source, the entropy rate is

:&lt;math&gt;\Eta(\mathcal{S}) = -\sum_i p_i \sum_j p_i(j) \sum_k p_{i,j}(k)\ \log_2 \  p_{i,j}(k) .&lt;/math&gt;

==={{math|''b''}}-ary entropy===
In general the '''{{math|''b''}}-ary entropy''' of a source &lt;math&gt;\mathcal{S}&lt;/math&gt; {{math|{{=}} (''S'', ''P'')}} with [[source alphabet]] {{math|''S'' {{=}} {''a''&lt;sub&gt;1&lt;/sub&gt;, …, ''a''&lt;sub&gt;''n''&lt;/sub&gt;}}} and [[discrete probability distribution]] {{math|''P'' {{=}} {''p''&lt;sub&gt;1&lt;/sub&gt;, …, ''p''&lt;sub&gt;''n''&lt;/sub&gt;}}} where {{math|''p''&lt;sub&gt;''i''&lt;/sub&gt;}} is the probability of {{math|''a''&lt;sub&gt;''i''&lt;/sub&gt;}} (say {{math|''p''&lt;sub&gt;''i''&lt;/sub&gt; {{=}} ''p''(''a''&lt;sub&gt;''i''&lt;/sub&gt;))}} is defined by:

:&lt;math&gt; \Eta_b(\mathcal{S}) = - \sum_{i=1}^n p_i \log_b p_i ,&lt;/math&gt;

Note: the {{math|''b''}} in "{{math|''b''}}-ary entropy" is the number of different symbols of the ''ideal alphabet'' used as a standard yardstick to measure source alphabets. In information theory, two symbols are [[necessary and sufficient]] for an alphabet to encode information. Therefore, the default is to let {{math|''b'' {{=}} 2}} ("binary entropy"). Thus, the entropy of the source alphabet, with its given empiric probability distribution, is a number equal to the number (possibly fractional) of symbols of the "ideal alphabet", with an optimal probability distribution, necessary to encode for each symbol of the source alphabet. Also note that "optimal probability distribution" here means a [[Uniform distribution (discrete)|uniform distribution]]: a source alphabet with {{math|''n''}} symbols has the highest possible entropy (for an alphabet with {{math|''n''}} symbols) when the probability distribution of the alphabet is uniform. This optimal entropy turns out to be {{math|log&lt;sub&gt;''b''&lt;/sub&gt;(''n'')}}.

==Efficiency==
A source alphabet with non-uniform distribution will have less entropy than if those symbols had uniform distribution (i.e. the "optimized alphabet"). This deficiency in entropy can be expressed as a ratio called efficiency{{Cite quote|date=July 2014}}:

:&lt;math&gt;\eta(X) = -\sum_{i=1}^n \frac{p(x_i) \log_b (p(x_i))}{\log_b (n)}=\log_n (\prod_{i=1}^n p(x_i)^{-p(x_i)})&lt;/math&gt;{{Clarify|date=July 2014}}

Efficiency has utility in quantifying the effective use of a [[communication channel]]. This formulation is also referred to as the normalized entropy, as the entropy is divided by the maximum entropy &lt;math&gt;{\log_b (n)}&lt;/math&gt;.  Furthermore, the efficiency is indifferent to choice of (positive) base {{math|''b''}}, as indicated by the insensitivity within the final logarithm above thereto.

==Characterization==
Shannon entropy is [[characterization (mathematics)|characterized]] by a small number of criteria, listed below. Any definition of entropy satisfying these assumptions has the form
:&lt;math&gt;-K\sum_{i=1}^np_i\log (p_i)&lt;/math&gt;
where {{math|''K''}} is a constant corresponding to a choice of measurement units.

In the following, {{math|''p''&lt;sub&gt;''i''&lt;/sub&gt; {{=}} Pr(''X'' {{=}} ''x''&lt;sub&gt;''i''&lt;/sub&gt;)}} and {{math|Η&lt;sub&gt;''n''&lt;/sub&gt;(''p''&lt;sub&gt;1&lt;/sub&gt;, …, ''p''&lt;sub&gt;''n''&lt;/sub&gt;) {{=}} Η(''X'')}}.

===Continuity===
The measure should be [[continuous function|continuous]], so that changing the values of the probabilities by a very small amount should only change the entropy by a small amount.

===Symmetry===
The measure should be unchanged if the outcomes {{math|''x''&lt;sub&gt;''i''&lt;/sub&gt;}} are re-ordered.
:&lt;math&gt;\Eta_n\left(p_1, p_2, \ldots \right) = \Eta_n\left(p_2, p_1, \ldots \right)&lt;/math&gt; etc.

===Maximum===
The measure should be maximal if all the outcomes are equally likely (uncertainty is highest when all possible events are equiprobable).
:&lt;math&gt; \Eta_n(p_1,\ldots,p_n) \le \Eta_n\left(\frac{1}{n}, \ldots, \frac{1}{n}\right) = \log_b (n).&lt;/math&gt;

For equiprobable events the entropy should increase with the number of outcomes.
:&lt;math&gt;\Eta_n\bigg(\underbrace{\frac{1}{n}, \ldots, \frac{1}{n}}_{n}\bigg) = \log_b(n) &lt; \log_b (n+1) = \Eta_{n+1}\bigg(\underbrace{\frac{1}{n+1}, \ldots, \frac{1}{n+1}}_{n+1}\bigg).&lt;/math&gt;

For continuous random variables, the multivariate Gaussian is the distribution with maximum [[differential entropy]].

===Additivity===
The amount of entropy should be independent of how the process is regarded as being divided into parts.

This last functional relationship characterizes the entropy of a system with sub-systems. It demands that the entropy of a system can be calculated from the entropies of its sub-systems if the interactions between the sub-systems are known.

Given an ensemble of {{math|''n''}} uniformly distributed elements that are divided into {{math|''k''}} boxes (sub-systems) with {{math|''b''&lt;sub&gt;1&lt;/sub&gt;, ..., ''b''&lt;sub&gt;''k''&lt;/sub&gt;}} elements each, the entropy of the whole ensemble should be equal to the sum of the entropy of the system of boxes and the individual entropies of the boxes, each weighted with the probability of being in that particular box.

For [[positive integers]] {{math|''b''&lt;sub&gt;''i''&lt;/sub&gt;}} where {{math|''b''&lt;sub&gt;1&lt;/sub&gt; + … + ''b''&lt;sub&gt;''k''&lt;/sub&gt; {{=}} ''n''}},
:&lt;math&gt;\Eta_n\left(\frac{1}{n}, \ldots, \frac{1}{n}\right) = \Eta_k\left(\frac{b_1}{n}, \ldots, \frac{b_k}{n}\right) = \sum_{i=1}^k \frac{b_i}{n} \, \Eta_{b_i}\left(\frac{1}{b_i}, \ldots, \frac{1}{b_i}\right).&lt;/math&gt;

Choosing {{math|''k'' {{=}} ''n''}}, {{math|''b''&lt;sub&gt;1&lt;/sub&gt; {{=}} … {{=}} ''b''&lt;sub&gt;''n''&lt;/sub&gt; {{=}} 1}} this implies that the entropy of a certain outcome is zero: {{math|Η&lt;sub&gt;1&lt;/sub&gt;(1) {{=}} 0}}. This implies that the efficiency of a source alphabet with {{math|''n''}} symbols can be defined simply as being equal to its {{math|''n''}}-ary entropy. See also [[Redundancy (information theory)]].

==Further properties==
The Shannon entropy satisfies the following properties, for some of which it is useful to interpret entropy as the amount of information learned (or uncertainty eliminated) by revealing the value of a random variable {{math|''X''}}:

* Adding or removing an event with probability zero does not contribute to the entropy:
::&lt;math&gt;\Eta_{n+1}(p_1,\ldots,p_n,0) = \Eta_n(p_1,\ldots,p_n)&lt;/math&gt;.
* The entropy of a discrete random variable is a non-negative number:
::&lt;math&gt;\Eta(X) \ge 0&lt;/math&gt;.&lt;ref name=cover1991&gt;{{cite book |author1=Thomas M. Cover |author2=Joy A. Thomas |title=Elements of Information Theory |publisher=Wiley |location=Hoboken, New Jersey |year= |isbn=0-471-24195-4}}&lt;/ref&gt;{{rp|15}}
* It can be confirmed using the [[Jensen inequality]] that
::&lt;math&gt;\Eta(X) = \operatorname{E}\left[\log_b \left( \frac{1}{p(X)}\right) \right] \leq \log_b \left( \operatorname{E}\left[ \frac{1}{p(X)} \right] \right) = \log_b(n)&lt;/math&gt;.&lt;ref name=cover1991 /&gt;{{rp|29}}
:This maximal entropy of {{math|log&lt;sub&gt;''b''&lt;/sub&gt;(''n'')}} is effectively attained by a source alphabet having a uniform probability distribution: uncertainty is maximal when all possible events are equiprobable.
* The entropy or the amount of information revealed by evaluating {{math|(''X'',''Y'')}} (that is, evaluating {{math|''X''}} and {{math|''Y''}} simultaneously) is equal to the information revealed by conducting two consecutive experiments: first evaluating the value of {{math|''Y''}}, then revealing the value of {{math|''X''}} given that you know the value of {{math|''Y''}}. This may be written as
::&lt;math&gt; \Eta(X,Y)=\Eta(X|Y)+\Eta(Y)=\Eta(Y|X)+\Eta(X).&lt;/math&gt;
* If &lt;math&gt;Y=f(X)&lt;/math&gt; where &lt;math&gt;f&lt;/math&gt; is a function, then &lt;math&gt;H(f(X)|X) = 0&lt;/math&gt;. Applying the previous formula to &lt;math&gt;H(X,f(X))&lt;/math&gt; yields
::&lt;math&gt; \Eta(X)+\Eta(f(X)|X)=\Eta(f(X))+\Eta(X|f(X)),&lt;/math&gt; 
:so &lt;math&gt;H(f(X)|X) \le H(X)&lt;/math&gt;{{math|Η(''f''(''X'')) ≤ Η(''X'')}}, thus the entropy of a variable can only decrease when the latter is passed through a function.
* If {{math|''X''}} and {{math|''Y''}} are two independent random variables, then knowing the value of {{math|''Y''}} doesn't influence our knowledge of the value of {{math|''X''}} (since the two don't influence each other by independence):
::&lt;math&gt; \Eta(X|Y)=\Eta(X).&lt;/math&gt;
* The entropy of two simultaneous events is no more than the sum of the entropies of each individual event, and are equal if the two events are independent. More specifically, if {{math|''X''}} and {{math|''Y''}} are two random variables on the same probability space, and {{math|(''X'', ''Y'')}} denotes their Cartesian product, then
::&lt;math&gt; \Eta(X,Y)\leq \Eta(X)+\Eta(Y).&lt;/math&gt;
* The entropy &lt;math&gt;\Eta(p)&lt;/math&gt; is [[Concave function|concave]] in the probability mass function &lt;math&gt;p&lt;/math&gt;, i.e.
::&lt;math&gt;\Eta(\lambda p_1 + (1-\lambda) p_2) \ge \lambda \Eta(p_1) + (1-\lambda) \Eta(p_2)&lt;/math&gt;
for all probability mass functions &lt;math&gt;p_1,p_2&lt;/math&gt; and &lt;math&gt; 0 \le \lambda \le 1&lt;/math&gt;.&lt;ref name=cover1991 /&gt;{{rp|32}}

==Extending discrete entropy to the continuous case==

===Differential entropy===
{{Main|Differential entropy}}

The Shannon entropy is restricted to random variables taking discrete values. The corresponding formula for a continuous random variable with [[probability density function]] {{math|''f''(''x'')}} with finite or infinite support &lt;math&gt;\mathbb X&lt;/math&gt; on the real line is defined by analogy, using the above form of the entropy as an expectation:

:&lt;math&gt;h[f] = \operatorname{E}[-\ln (f(x))] = -\int_\mathbb X f(x) \ln (f(x))\, dx.&lt;/math&gt;

This formula is usually referred to as the '''continuous entropy''', or [[differential entropy]]. A precursor of the continuous entropy {{math|''h''[''f'']}} is the expression for the functional {{math|''Η''}} in the [[H-theorem]] of [[Boltzmann]].

Although the analogy between both functions is suggestive, the following question must be set: is the differential entropy a valid extension of the Shannon discrete entropy? Differential entropy lacks a number of properties that the Shannon discrete entropy has&amp;nbsp;– it can even be negative&amp;nbsp;– and thus corrections have been suggested, notably [[limiting density of discrete points]].

To answer this question, we must establish a connection between the two functions:

We wish to obtain a generally finite measure as the [[bin size]] goes to zero. In the discrete case, the bin size is the (implicit) width of each of the {{math|''n''}} (finite or infinite) bins whose probabilities are denoted by {{math|''p''&lt;sub&gt;''n''&lt;/sub&gt;}}. As we generalize to the continuous domain, we must make this width explicit.

To do this, start with a continuous function {{math|''f''}} discretized into bins of size &lt;math&gt;\Delta&lt;/math&gt;.
&lt;!-- Figure: Discretizing the function $ f$ into bins of width $ \Delta$ \includegraphics[width=\textwidth]{function-with-bins.eps} --&gt;&lt;!-- The original article this figure came from is at http://planetmath.org/shannonsentropy but it is broken there too --&gt;
By the mean-value theorem there exists a value {{math|''x''&lt;sub&gt;''i''&lt;/sub&gt;}} in each bin such that

:&lt;math&gt;f(x_i) \Delta = \int_{i\Delta}^{(i+1)\Delta} f(x)\, dx&lt;/math&gt;

and thus the integral of the function {{math|''f''}} can be approximated (in the Riemannian sense) by

:&lt;math&gt;\int_{-\infty}^{\infty} f(x)\, dx = \lim_{\Delta \to 0} \sum_{i = -\infty}^{\infty} f(x_i) \Delta&lt;/math&gt;

where this limit and "bin size goes to zero" are equivalent.

We will denote

:&lt;math&gt;\Eta^{\Delta} := - \sum_{i=-\infty}^{\infty} f(x_i)  \Delta \log \left(  f(x_i)  \Delta \right)&lt;/math&gt;

and expanding the logarithm, we have

:&lt;math&gt;\Eta^{\Delta} = - \sum_{i=-\infty}^{\infty}  f(x_i)  \Delta \log (f(x_i)) -\sum_{i=-\infty}^{\infty} f(x_i) \Delta \log (\Delta).&lt;/math&gt;

As Δ → 0, we have

:&lt;math&gt;\begin{align}
\sum_{i=-\infty}^{\infty} f(x_i) \Delta &amp;\to \int_{-\infty}^{\infty} f(x)\, dx = 1 \\
\sum_{i=-\infty}^{\infty} f(x_i) \Delta \log (f(x_i)) &amp;\to \int_{-\infty}^{\infty} f(x) \log f(x)\, dx.
\end{align}&lt;/math&gt;

But note that {{math|log(Δ) → −∞}} as {{math|Δ → 0}}, therefore we need a special definition of the differential or continuous entropy:

:&lt;math&gt;h[f] = \lim_{\Delta \to 0} \left(\Eta^{\Delta} + \log \Delta\right) = -\int_{-\infty}^{\infty} f(x) \log f(x)\,dx,&lt;/math&gt;

which is, as said before, referred to as the '''differential entropy'''. This means that the differential entropy ''is not'' a limit of the Shannon entropy for {{math|''n'' → ∞}}. Rather, it differs from the limit of the Shannon entropy by an infinite offset (see also the article on [[information dimension]])

===Limiting density of discrete points===
{{Main|Limiting density of discrete points}}

It turns out as a result that, unlike the Shannon entropy, the differential entropy is ''not'' in general a good measure of uncertainty or information. For example, the differential entropy can be negative; also it is not invariant under continuous co-ordinate transformations. This problem may be illustrated by a change of units when ''x'' is a dimensioned variable. ''f(x)'' will then have the units of ''1/x''. The argument of the logarithm must be dimensionless, otherwise it is improper, so that the differential entropy as given above  will be improper. If ''&amp;Delta;'' is some "standard" value of ''x'' (i.e. "bin size") and therefore has the same units, then a modified differential entropy may be written in proper form as:

:&lt;math&gt;
H=\int_{-\infty}^\infty f(x) \log(f(x)\,\Delta)\,dx
&lt;/math&gt;

and the result will be the same for any choice of units for ''x''. In fact, the limit of discrete entropy as &lt;math&gt; N \rightarrow \infty &lt;/math&gt; would also include a term of &lt;math&gt; \log(N)&lt;/math&gt;, which would in general be infinite. This is expected, continuous variables would typically have infinite entropy when discretized. The [[limiting density of discrete points]] is really a measure of how much easier a distribution is to describe than a distribution that is uniform over its quantization scheme.

===Relative entropy===
{{main|Generalized relative entropy}}
Another useful measure of entropy that works equally well in the discrete and the continuous case is the '''relative entropy''' of a distribution. It is defined as the [[Kullback–Leibler divergence]] from the distribution to a reference measure {{math|''m''}} as follows. Assume that a probability distribution {{math|''p''}} is [[absolutely continuous]] with respect to a measure {{math|''m''}}, i.e. is of the form {{math|''p''(''dx'') {{=}} ''f''(''x'')''m''(''dx'')}} for some non-negative {{math|''m''}}-integrable function {{math|''f''}} with {{math|''m''}}-integral 1, then the relative entropy can be defined as
:&lt;math&gt;D_{\mathrm{KL}}(p \| m ) = \int \log (f(x)) p(dx) = \int f(x)\log (f(x)) m(dx) .&lt;/math&gt;

In this form the relative entropy generalises (up to change in sign) both the discrete entropy, where the measure {{math|''m''}} is the [[counting measure]], and the differential entropy, where the measure {{math|''m''}} is the [[Lebesgue measure]]. If the measure {{math|''m''}} is itself a probability distribution, the relative entropy is non-negative, and zero if {{math|''p'' {{=}} ''m''}} as measures. It is defined for any measure space, hence coordinate independent and invariant under co-ordinate reparameterizations if one properly takes into account the transformation of the measure {{math|''m''}}. The relative entropy, and implicitly entropy and differential entropy, do depend on the "reference" measure {{math|''m''}}.

==Use in combinatorics==
Entropy has become a useful quantity in [[combinatorics]].

===Loomis–Whitney inequality===
A simple example of this is an alternate proof of the [[Loomis–Whitney inequality]]: for every subset {{math|''A'' ⊆ '''Z'''&lt;sup&gt;''d''&lt;/sup&gt;}}, we have
:&lt;math&gt; |A|^{d-1}\leq \prod_{i=1}^{d} |P_{i}(A)|&lt;/math&gt;
where {{math|''P''&lt;sub&gt;''i''&lt;/sub&gt;}} is the [[orthogonal projection]] in the {{math|''i''}}th coordinate:
:&lt;math&gt; P_{i}(A)=\{(x_{1}, \ldots, x_{i-1}, x_{i+1}, \ldots, x_{d}) : (x_{1}, \ldots, x_{d})\in A\}.&lt;/math&gt;

The proof follows as a simple corollary of [[Shearer's inequality]]: if {{math|''X''&lt;sub&gt;1&lt;/sub&gt;, …, ''X''&lt;sub&gt;''d''&lt;/sub&gt;}} are random variables and {{math|''S''&lt;sub&gt;1&lt;/sub&gt;, …, ''S''&lt;sub&gt;''n''&lt;/sub&gt;}} are subsets of {{math|{1, …, ''d''}}} such that every integer between 1 and {{math|''d''}} lies in exactly {{math|''r''}} of these subsets, then
:&lt;math&gt; \Eta[(X_{1}, \ldots ,X_{d})]\leq \frac{1}{r}\sum_{i=1}^{n}\Eta[(X_{j})_{j\in S_{i}}]&lt;/math&gt;
where &lt;math&gt; (X_{j})_{j\in S_{i}}&lt;/math&gt; is the Cartesian product of random variables {{math|''X''&lt;sub&gt;''j''&lt;/sub&gt;}} with indexes {{math|''j''}} in {{math|''S''&lt;sub&gt;''i''&lt;/sub&gt;}} (so the dimension of this vector is equal to the size of {{math|''S''&lt;sub&gt;''i''&lt;/sub&gt;}}).

We sketch how Loomis–Whitney follows from this: Indeed, let {{math|''X''}} be a uniformly distributed random variable with values in {{math|''A''}} and so that each point in {{math|''A''}} occurs with equal probability. Then (by the further properties of entropy mentioned above) {{math|Η(''X'') {{=}} log{{abs|''A''}}}}, where {{math|{{abs|''A''}}}} denotes the cardinality of {{math|''A''}}. Let {{math|''S''&lt;sub&gt;''i''&lt;/sub&gt; {{=}} {1, 2, …, ''i''−1, ''i''+1, …, ''d''}}}. The range of &lt;math&gt;(X_{j})_{j\in S_{i}}&lt;/math&gt; is contained in {{math|''P''&lt;sub&gt;''i''&lt;/sub&gt;(''A'')}} and hence &lt;math&gt; \Eta[(X_{j})_{j\in S_{i}}]\leq \log |P_{i}(A)|&lt;/math&gt;. Now use this to bound the right side of Shearer's inequality and exponentiate the opposite sides of the resulting inequality you obtain.

===Approximation to binomial coefficient===
For integers {{math|0 &lt; ''k'' &lt; ''n''}} let {{math|''q'' {{=}} ''k''/''n''}}. Then
:&lt;math&gt;\frac{2^{n\Eta(q)}}{n+1} \leq \tbinom nk \leq 2^{n\Eta(q)},&lt;/math&gt;
where 
:&lt;math&gt;\Eta(q) = -q \log_2(q) - (1-q) \log_2(1-q).&lt;/math&gt;&lt;ref&gt;Aoki, New Approaches to Macroeconomic Modeling.&lt;/ref&gt;{{rp|43}}

Here is a sketch proof. Note that &lt;math&gt;\tbinom nk q^{qn}(1-q)^{n-nq}&lt;/math&gt; is one term of the expression
:&lt;math&gt;\sum_{i=0}^n \tbinom ni q^i(1-q)^{n-i} = (q + (1-q))^n = 1.&lt;/math&gt; 
Rearranging gives the upper bound. For the lower bound one first shows, using some algebra, that it is the largest term in the summation. But then,
:&lt;math&gt;\binom nk q^{qn}(1-q)^{n-nq} \geq \frac{1}{n+1}&lt;/math&gt;
since there are {{math|''n'' + 1}} terms in the summation. Rearranging gives the lower bound.

A nice interpretation of this is that the number of binary strings of length {{math|''n''}} with exactly {{math|''k''}} many 1's is approximately &lt;math&gt;2^{n\Eta(k/n)}&lt;/math&gt;.&lt;ref&gt;Probability and Computing, M. Mitzenmacher and E. Upfal, Cambridge University Press&lt;/ref&gt;

==See also==
{{Portal|Statistics|Cryptography}}
{{colbegin}}
*[[Conditional entropy]]
*[[Cross entropy]] – is a measure of the average number of bits needed to identify an event from a set of possibilities between two probability distributions
*[[Diversity index]] – alternative approaches to quantifying diversity in a probability distribution
*[[Entropy (arrow of time)]]
*[[Entropy encoding]] – a coding scheme that assigns codes to symbols so as to match code lengths with the probabilities of the symbols.
*[[Entropy estimation]]
*[[Entropy power inequality]]
*[[Entropy rate]]
*[[Fisher information]]
*[[Graph entropy]]
*[[Hamming distance]]
*[[History of entropy]]
*[[History of information theory]]
*[[Information geometry]]
*[[Joint entropy]] – is the measure how much entropy is contained in a joint system of two random variables.
*[[Kolmogorov–Sinai entropy]] in [[dynamical system]]s
*[[Levenshtein distance]]
*[[Mutual information]]
*[[Negentropy]]
*[[Perplexity]]
*[[Qualitative variation]] – other measures of [[statistical dispersion]] for [[nominal distributions]]
*[[Quantum relative entropy]] – a measure of distinguishability between two quantum states.
*[[Rényi entropy]] – a generalization of Shannon entropy; it is one of a family of functionals for quantifying the diversity, uncertainty or randomness of a system.
*[[Randomness]]
*[[Shannon index]]
*[[Theil index]]
*[[Typoglycemia]]
{{colend}}

==References==
{{Reflist}}
{{PlanetMath attribution|id=968|title=Shannon's entropy}}

==Further reading==

===Textbooks on information theory===
* Arndt, C. (2004), ''Information Measures: Information and its Description in Science and Engineering'', Springer, {{isbn|978-3-540-40855-0}}
* [[Thomas M. Cover|Cover, T. M.]], Thomas, J. A. (2006), ''Elements of information theory'', 2nd Edition. Wiley-Interscience. {{isbn|0-471-24195-4}}.
* Gray, R. M. (2011), ''Entropy and Information Theory'', Springer.
* [[David J. C. MacKay|MacKay, David J. C.]]. ''[http://www.inference.phy.cam.ac.uk/mackay/itila/book.html Information Theory, Inference, and Learning Algorithms]'' Cambridge: Cambridge University Press, 2003. {{isbn|0-521-64298-1}}
* {{cite book|authors=Martin, Nathaniel F.G. &amp; England, James W.|title=Mathematical Theory of Entropy|publisher=Cambridge University Press|year=2011|isbn=978-0-521-17738-2|url=https://books.google.com/books?id=_77lvx7y8joC}}
* [[Claude Shannon|Shannon, C.E.]], [[Warren Weaver|Weaver, W.]] (1949) ''The Mathematical Theory of Communication'', Univ of Illinois Press. {{isbn|0-252-72548-4}}
* Stone, J. V. (2014), Chapter 1 of [http://jim-stone.staff.shef.ac.uk/BookInfoTheory/InfoTheoryBookMain.html ''Information Theory: A Tutorial Introduction''], University of Sheffield, England. {{isbn|978-0956372857}}.

==External links==
{{external cleanup|date=June 2015}}
{{Library resources box|onlinebooks=yes}}
* {{springer|title=Entropy|id=p/e035740}}
* [http://pespmc1.vub.ac.be/ENTRINFO.html Introduction to entropy and information] on [[Principia Cybernetica Web]]
* ''[http://www.mdpi.com/journal/entropy Entropy]'' an interdisciplinary journal on all aspects of the entropy concept. Open access.
* [http://www.rheingold.com/texts/tft/6.html Description of information entropy from "Tools for Thought" by Howard Rheingold]
* [http://math.ucsd.edu/~crypto/java/ENTROPY/ A java applet representing Shannon's Experiment to Calculate the Entropy of English]
* [http://www.autonlab.org/tutorials/infogain.html Slides on information gain and entropy]
*[[Wikibooks:An Intuitive Guide to the Concept of Entropy Arising in Various Sectors of Science|''An Intuitive Guide to the Concept of Entropy Arising in Various Sectors of Science'']] – a wikibook on the interpretation of the concept of entropy.
* [//researchspace.auckland.ac.nz/handle/2292/3427 Network Event Detection With Entropy Measures], Dr. Raimund Eimann, University of Auckland, PDF; 5993&amp;nbsp;kB – a PhD thesis demonstrating how entropy measures may be used in network anomaly detection.
* [http://rosettacode.org/wiki/Entropy [[Rosetta Code]] repository of implementations of Shannon entropy in different programming languages.]
* [http://tuvalu.santafe.edu/~simon/it.pdf "Information Theory for Intelligent People"]. Short introduction to the axioms of information theory, entropy, mutual information, Kullback–Liebler divergence, and Jensen–Shannon distance.
* [http://www.shannonentropy.netmark.pl Online tool for calculating entropy (plain text)]
* [//servertest.online/entropy Online tool for calculating entropy (binary)]

{{Compression Methods}}

{{Authority control}}

{{DEFAULTSORT:Entropy (Information Theory)}}
[[Category:Entropy and information| ]]
[[Category:Information theory]]
[[Category:Statistical randomness]]</text>
      <sha1>oj2eq0wc3hb6m8e87te6udmlvhwvdoh</sha1>
    </revision>
  </page>
  <page>
    <title>Enumerative combinatorics</title>
    <ns>0</ns>
    <id>3925533</id>
    <revision>
      <id>856452232</id>
      <parentid>856391194</parentid>
      <timestamp>2018-08-25T09:15:08Z</timestamp>
      <contributor>
        <username>Jonesey95</username>
        <id>9755426</id>
      </contributor>
      <minor/>
      <comment>Fix unsupported parameter in Template:Columns-list or Template:Div col using [[WP:AutoEd|AutoEd]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9307">'''Enumerative combinatorics''' is an area of [[combinatorics]] that deals with the number of ways that certain patterns can be formed. Two examples of this type of problem are counting [[combinations]] and counting [[permutations]].  More generally, given an infinite collection of finite sets ''S''&lt;sub&gt;''i''&lt;/sub&gt; indexed by the [[natural number]]s, enumerative combinatorics seeks to describe a ''counting function'' which counts the number of objects in ''S''&lt;sub&gt;''n''&lt;/sub&gt; for each ''n''. Although [[Counting#Counting_in_mathematics|counting]] the number of elements in a set is a rather broad [[mathematical problem]], many of the problems that arise in applications have a relatively simple [[combinatorial]] description. The [[twelvefold way]] provides a unified framework for counting [[permutations]], [[combinations]] and [[Partition of a set|partitions]].

The simplest such functions are ''[[closed formula]]s'', which can be expressed as a composition of elementary functions such as [[factorial]]s, powers, and so on.  For instance, as shown below, the number of different possible orderings of a deck of ''n'' cards is ''f''(''n'') = ''n''!.  The problem of finding a closed formula is known as [[algebraic enumeration]], and frequently involves deriving a [[recurrence relation]] or [[generating function]] and using this to arrive at the desired closed form.

Often, a complicated closed formula yields little insight into the behavior of the counting function as the number of counted objects grows. 
In these cases, a simple [[Asymptotic analysis|asymptotic]] approximation may be preferable.  A function &lt;math&gt;g(n)&lt;/math&gt; is an asymptotic approximation to &lt;math&gt;f(n)&lt;/math&gt; if &lt;math&gt;f(n)/g(n)\rightarrow 1&lt;/math&gt; as &lt;math&gt;n\rightarrow \infty&lt;/math&gt;. In this case, we write &lt;math&gt;f(n) \sim g(n).\,&lt;/math&gt;

==Generating functions==
Generating functions are used to describe families of combinatorial objects. Let &lt;math&gt;\mathcal{F}&lt;/math&gt; denote the family of objects and let ''F''(''x'') be its generating function. Then
:&lt;math&gt;F(x) = \sum^{\infty}_{n=0}f_nx^n&lt;/math&gt;
where &lt;math&gt;f_n&lt;/math&gt; denotes the number of combinatorial objects of size ''n''. The number of combinatorial objects of size ''n'' is therefore given by the coefficient of &lt;math&gt;x^n&lt;/math&gt;. Some common operation on families of combinatorial objects and its effect on the generating function will now be developed.
The exponential generating function is also sometimes used. In this case it would have the form
:&lt;math&gt;F(x) = \sum^{\infty}_{n=0}f_n\frac{x^n}{n!}&lt;/math&gt;

Once determined, the generating function yields the information given by the previous approaches. In addition, the various natural operations on generating functions such as addition, multiplication, differentiation, etc., have a combinatorial significance; this allows one to extend results from one combinatorial problem in order to solve others.

===Union===
Given two combinatorial families, &lt;math&gt;\mathcal{F}&lt;/math&gt; and &lt;math&gt;\mathcal{G}&lt;/math&gt; with generating functions ''F''(''x'') and ''G''(''x'') respectively, the disjoint union of the two families (&lt;math&gt;\mathcal{F} \cup \mathcal{G}&lt;/math&gt;) has generating function ''F''(''x'') + ''G''(''x'').

===Pairs===
For two combinatorial families as above the Cartesian product (pair) of the two families (&lt;math&gt;\mathcal{F} \times \mathcal{G}&lt;/math&gt;) has generating function ''F''(''x'')''G''(''x'').

===Sequences===
A sequence generalizes the idea of the pair as defined above. Sequences are arbitrary [[Cartesian product]]s of a combinatorial object with itself. Formally:

:&lt;math&gt;\mbox{Seq}(\mathcal{F}) = \epsilon\ \cup\ \mathcal{F} \ \cup\ \mathcal{F} \times \mathcal{F} \ \cup\ \mathcal{F} \times \mathcal{F} \times \mathcal{F}\  \cup \cdots&lt;/math&gt;

To put the above in words: An empty sequence or a sequence of one element or a sequence of two elements or a sequence of three elements, etc.
The generating function would be:

:&lt;math&gt;1+F(x) + [F(x)]^2 + [F(x)]^3 + \cdots = \frac{1}{1-F(x)}&lt;/math&gt;

==Combinatorial structures==
The above operations can now be used to enumerate common combinatorial objects including trees (binary and plane), [[Dyck path]]s and cycles. A combinatorial structure is composed of atoms. For example, with trees the atoms would be the nodes. The atoms which compose the object can either be labeled or unlabeled. Unlabeled atoms are indistinguishable from each other, while labelled atoms are distinct. Therefore, for a combinatorial object consisting of labeled atoms a new object can be formed by simply swapping two or more atoms.

===Binary and plane trees===
Binary and plane [[tree (mathematics)|tree]]s are examples of an unlabeled combinatorial structure. Trees consist of nodes linked by edges in such a way that there are no cycles. There is generally a node called the root, which has no parent node. In Plane trees each node can have an arbitrary number of children. In binary trees, a special case of plane trees, each node can have either two or no children. Let &lt;math&gt;\mathcal{P}&lt;/math&gt; denote the family of all plane trees. Then this family can be recursively defined as follows:
:&lt;math&gt;\mathcal{P} = \{\bullet\} \times \mbox{Seq}(\mathcal{P})&lt;/math&gt;
In this case &lt;math&gt;\{\bullet\}&lt;/math&gt; represents the family of objects consisting of one node. This has generating function ''x''. Let ''P''(''x'') denote the generating function &lt;math&gt;\mathcal{P}&lt;/math&gt;.
Putting the above description in words: A plane tree consists of a node to which is attached an arbitrary number of subtrees, each of which is also a plane tree. Using the operation on families of combinatorial structures developed earlier this translates to a recursive generating function:
:&lt;math&gt;P(x) = x\frac{1}{1-P(x)}&lt;/math&gt;
After solving for ''P''(''x''):

:&lt;math&gt;P(x) = \frac{1-\sqrt{1-4x}}{2}&lt;/math&gt;

An explicit formula for the number of plane trees of size ''n'' can now be determined by extracting the coefficient of ''x''&lt;sup&gt;''n''&lt;/sup&gt;.

: &lt;math&gt;
\begin{align}
p_n &amp; = [x^n] P(x) = [x^n] \frac{1-\sqrt{1-4x}}{2} \\[6pt]
&amp; = [x^n] \frac{1}{2} - [x^n] \frac{1}{2} \sqrt{1-4x} \\[6pt]
&amp; = -\frac{1}{2} [x^n] \sum^{\infty}_{k=0} {\frac{1}{2} \choose k} (-4x)^k \\[6pt]
&amp; = -\frac{1}{2} {\frac{1}{2} \choose n} (-4)^n \\[6pt]
&amp; = \frac{1}{n} {2n-2 \choose n-1}
\end{align}
&lt;/math&gt;

Note: The notation [''x''&lt;sup&gt;''n''&lt;/sup&gt;] ''f''(''x'') refers to the coefficient of ''x''&lt;sup&gt;''n''&lt;/sup&gt; in ''f''(''x'').
The series expansion of the square root is based on Newton's generalization of the [[Binomial theorem#Newton's generalised binomial theorem|binomial theorem]]. To get from the fourth to fifth line manipulations using the [[Binomial coefficient#Generalization and connection to the binomial series|generalized binomial coefficient]] is needed.

The expression on the last line is equal to the (''n''&amp;nbsp;&amp;minus;&amp;nbsp;1)&lt;sup&gt;th&lt;/sup&gt; [[Catalan number]]. Therefore ''p''&lt;sub&gt;''n''&lt;/sub&gt; = ''c''&lt;sub&gt;''n''−1&lt;/sub&gt;.

== See also ==
{{Div col|colwidth=20em}}
* [[Algebraic combinatorics]]
* [[Asymptotic combinatorics]]
* [[Burnside's lemma]]
* [[Combinatorial explosion]]
* [[Combinatorial game theory]]
* [[Combinatorial principles]]
* [[Combinatorial species]]
* [[Inclusion–exclusion principle]]
* [[Method of distinguished element]]
* [[Pólya enumeration theorem]]
* [[Sieve theory]]
{{Div col end}}

==References==
* [[Doron Zeilberger|Zeilberger, Doron]], [http://www.math.rutgers.edu/~zeilberg/mamarim/mamarimPDF/enuPCM.pdf Enumerative and Algebraic Combinatorics]
* [[Anders Björner|Björner, Anders]] and [[Richard P. Stanley|Stanley, Richard P.]], [http://www-math.mit.edu/~rstan/papers/comb.pdf ''A Combinatorial Miscellany'']
* [[Ronald Graham|Graham, Ronald L.]],  [[Martin Grötschel|Grötschel M.]], and [[László_Lovász|Lovász, László]], eds. (1996). ''Handbook of Combinatorics'', Volumes 1 and 2.  Elsevier (North-Holland), Amsterdam, and MIT Press, Cambridge, Mass. {{ISBN|0-262-07169-X}}.
* {{cite book |last=Joseph |first=George Gheverghese |title=The Crest of the Peacock: Non-European Roots of Mathematics |edition=2nd |publisher=[[Penguin Books]] |location=London |isbn=0-14-012529-9 |origyear=1991 |year=1994 }}
* Loehr, Nicholas A. (2011).  [http://www.math.vt.edu/people/nloehr/bijbook.html Bijective Combinatorics].  [http://www.crcpress.com CRC Press].  {{ISBN|143984884X}},  {{ISBN|978-1439848845}}.
* [[Richard P. Stanley|Stanley, Richard P.]] (1997, 1999).  [http://www-math.mit.edu/~rstan/ec/ ''Enumerative Combinatorics'', Volumes 1 and 2]. [[Cambridge University Press]].  {{ISBN|0-521-55309-1}}, {{ISBN|0-521-56069-1}}.
* [http://encyclopedia.jrank.org/CLI_COM/COMBINATORIAL_ANALYSIS.html Combinatorial Analysis] – an article in [[Encyclopædia Britannica Eleventh Edition]]
* [[John Riordan (mathematician)|Riordan, John]] (1958). ''An Introduction to Combinatorial Analysis'', Wiley &amp; Sons, New York (republished). 
* Riordan, John (1968). ''Combinatorial Identities'', Wiley &amp; Sons, New York (republished).
* {{cite book | last=Wilf | first=Herbert S. | authorlink=Herbert Wilf | title=Generatingfunctionology | edition=2nd | location=Boston, MA | publisher=Academic Press | year=1994 | isbn=0-12-751956-4 | zbl=0831.05001 | url=http://www.math.upenn.edu/%7Ewilf/DownldGF.html }}

[[Category:Enumerative combinatorics|*]]</text>
      <sha1>6rri7mfqzfsr3airnas4vtzrm1x4zna</sha1>
    </revision>
  </page>
  <page>
    <title>Euler integral</title>
    <ns>0</ns>
    <id>245658</id>
    <revision>
      <id>829979213</id>
      <parentid>811779714</parentid>
      <timestamp>2018-03-12T00:04:02Z</timestamp>
      <contributor>
        <username>Nbarth</username>
        <id>570614</id>
      </contributor>
      <comment>/* top */ dab: [[Euler–Poisson integral]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="963">{{for|the Euler–Poisson integral|Gaussian integral}}
In [[mathematics]], there are two types of '''Euler integral''':&lt;ref&gt;Jeffrey, Alan; and Dai, Hui-Hui (2008).  Handbook of Mathematical Formulas 4th Ed. Academic Press. {{ISBN|978-0-12-374288-9}}. pp. 234–235&lt;/ref&gt;

: 1. ''Euler [[integral]] of the first kind'': the [[beta function]]
:: &lt;math&gt;\mathrm{\Beta}(x,y)= \int_0^1t^{x-1}(1-t)^{y-1}\,dt =\frac{\Gamma(x)\Gamma(y)}{\Gamma(x+y)}&lt;/math&gt;

: 2. ''Euler integral of the second kind'': the [[gamma function]]
:: &lt;math&gt;\int_0^\infty  t^{z-1}\,\mathrm e^{-t}\,dt&lt;/math&gt;

For [[natural number|positive integer]]s ''m'' and ''n''
:&lt;math&gt;\Beta(n,m)= {(n-1)!(m-1)! \over (n+m-1)!}={n+m \over nm{n+m \choose n}}&lt;/math&gt;
:&lt;math&gt;\Gamma(n) = (n-1)! &lt;/math&gt;

==See also==
*[[Euler integral (thermodynamics)]]
*[[Leonhard Euler]]
*[[List of topics named after Leonhard Euler]]

==References==
{{Reflist}}

[[Category:Gamma and related functions]]


{{sia|mathematics}}</text>
      <sha1>004o8ldfhvyg9o5p7003rzqk6lqpjxb</sha1>
    </revision>
  </page>
  <page>
    <title>Existentially closed model</title>
    <ns>0</ns>
    <id>11168195</id>
    <revision>
      <id>766753832</id>
      <parentid>717066817</parentid>
      <timestamp>2017-02-21T23:19:25Z</timestamp>
      <contributor>
        <ip>73.102.59.169</ip>
      </contributor>
      <comment>Existential closedness must be checked for tuples, not just elements.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3903">In [[model theory]], a branch of [[mathematical logic]], the notion of an '''existentially closed model''' (or '''existentially complete model''') of a [[theory (mathematical logic)|theory]] generalizes the notions of [[algebraically closed field]]s (for the theory of [[field (mathematics)|field]]s), [[real closed field]]s (for the theory of ordered fields), [[divisible group|existentially closed group]]s (for the class of groups), and dense linear orders without endpoints (for the class of linear orders).

==Definition==
A substructure ''M'' of a [[structure (mathematical logic)|structure]] ''N'' is said to be '''existentially closed in''' (or '''existentially complete in''') &lt;math&gt;N&lt;/math&gt; if for every quantifier-free formula φ(''x''&lt;sub&gt;1&lt;/sub&gt;,…,''x''&lt;sub&gt;n&lt;/sub&gt;,''y''&lt;sub&gt;1&lt;/sub&gt;,…,''y''&lt;sub&gt;n&lt;/sub&gt;) and all elements ''b''&lt;sub&gt;1&lt;/sub&gt;,…,''b''&lt;sub&gt;n&lt;/sub&gt; of ''M'' such that φ(''x''&lt;sub&gt;1&lt;/sub&gt;,…,''x''&lt;sub&gt;n&lt;/sub&gt;,''b''&lt;sub&gt;1&lt;/sub&gt;,…,''b''&lt;sub&gt;n&lt;/sub&gt;) is realized in ''N'', then  φ(''x''&lt;sub&gt;1&lt;/sub&gt;,…,''x''&lt;sub&gt;n&lt;/sub&gt;,''b''&lt;sub&gt;1&lt;/sub&gt;,…,''b''&lt;sub&gt;n&lt;/sub&gt;) is also realized in ''M''. In other words: If there is a tuple ''a''&lt;sub&gt;1&lt;/sub&gt;,…,''a''&lt;sub&gt;n&lt;/sub&gt; in ''N'' such that φ(''a''&lt;sub&gt;1&lt;/sub&gt;,…,''a''&lt;sub&gt;n&lt;/sub&gt;,''b''&lt;sub&gt;1&lt;/sub&gt;,…,''b''&lt;sub&gt;n&lt;/sub&gt;) holds in ''N'', then such a tuple also exists in ''M''. This notion is often denoted &lt;math&gt;M\prec_1 N&lt;/math&gt;.

A model ''M'' of a theory ''T'' is called existentially closed in ''T'' if it is existentially closed in every superstructure ''N'' that is itself a model of ''T''. More generally, a structure ''M'' is called existentially closed in a class ''K'' of structures (in which it is contained as a member) if ''M'' is existentially closed in every superstructure ''N'' that is itself a member of ''K''.

The '''existential closure''' in ''K'' of a member ''M'' of ''K'', when it exists, is, up to isomorphism, the least existentially closed superstructure of ''M''.  More precisely, it is any extensionally closed superstructure ''M''&lt;sup&gt;&amp;lowast;&lt;/sup&gt; of ''M'' such that for every existentially closed superstructure ''N'' of ''M'', ''M''&lt;sup&gt;&amp;lowast;&lt;/sup&gt; is isomorphic to a substructure of ''N'' via an isomorphism that is the identity on ''M''.

==Examples==
Let σ = (+,&amp;times;,0,1) be the [[signature (logic)|signature]] of fields, i.e. +,&amp;times; are binary relation symbols and 0,1 are constant symbols. Let ''K'' be the class of structures of signature σ which are fields.
If ''A'' is a subfield of ''B'', then ''A'' is existentially closed in ''B'' if and only if every system of [[polynomial]]s over ''A'' which has a solution in ''B'' also has a solution in ''A''. It follows that the existentially closed members of ''K'' are exactly the algebraically closed fields.

Similarly in the class of [[ordered field]]s, the existentially closed structures are the [[real closed field]]s. In the class of [[total order|totally ordered structures]], the existentially closed structures are those that are [[dense order|dense]] without endpoints, while the existential closure of any countable (including empty) total order is, up to isomorphism, the countable dense total order without endpoints, namely the [[order type]] of the [[rationals]].

==References==
* {{Citation | last1=Chang | first1=Chen Chung | author1-link=Chen Chung Chang | last2=Keisler | first2=H. Jerome | author2-link=Howard Jerome Keisler | title=Model Theory | origyear=1973 | publisher=Elsevier | edition=3rd | series=Studies in Logic and the Foundations of Mathematics | isbn=978-0-444-88054-3 | year=1990}}
* {{Citation | last1=Hodges | first1=Wilfrid | author1-link=Wilfrid Hodges | title=A shorter model theory | publisher= [[Cambridge University Press]]| location=Cambridge | isbn=978-0-521-58713-6 | year=1997}}

==External links==
*[http://eom.springer.de/e/e110140.htm EoM article]

[[Category:Model theory]]</text>
      <sha1>1fnn3q14qks7ps4pf6ivumi69tdp55g</sha1>
    </revision>
  </page>
  <page>
    <title>Experience modifier</title>
    <ns>0</ns>
    <id>3962591</id>
    <revision>
      <id>826021469</id>
      <parentid>826021162</parentid>
      <timestamp>2018-02-16T19:17:19Z</timestamp>
      <contributor>
        <username>PaulTanenbaum</username>
        <id>3418254</id>
      </contributor>
      <minor/>
      <comment>/* Methods of calculation */ removed a broken link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8591">In the [[insurance]] industry in the [[United States]], an '''experience modifier''' or '''experience modification''' is an adjustment of an employer's [[insurance premium|premium]] for [[worker's compensation]] coverage based on the losses the insurer has experienced from that employer. An experience modifier of 1 would be applied for an employer that had demonstrated the [[actuarial science|actuarially]] expected performance. Poorer loss experience leads to a modifier greater than 1, and better experience to a modifier less than 1. The loss experience used in determining the modifier typically comprises three years but excluding the immediate past year. For instance, if a policy expired on January 1, 2018, the period reflected by the experience modifier would run from January 1, 2014 to January 1, 2017.

==Methods of calculation==
Experience modifiers are normally recalculated for an employer annually by using experience ratings. The rating is a method used by [[insurer]]s to determine pricing of [[insurance premium|premiums]] for different groups or individuals based on the group or individual's history of claims. The experience rating approach uses an individual's or group’s historic data as a proxy for [[risk|future risk]], and insurers adjust and set insurance premiums and plans accordingly.&lt;ref&gt;{{cite web|url=http://www.investorwords.com/1845/experience_rating.html|title=What is Experience Rating? definition and meaning|publisher=|accessdate=8 September 2016}}&lt;/ref&gt;  Each year, a newer year's data is added to the three year window of experience used in the calculation, and the oldest year from the prior calculation is dropped off.  The other two years worth of data in the rating window are also updated on an annual basis.

Experience modifiers are calculated by organizations known as "rating bureaus" and rely on information reported by insurance companies.  The rating bureau used by most states is the NCCI, the [[National Council on Compensation Insurance]].  But a number of states have independent rating bureaus: [[California]], [[Michigan]], [[Delaware]], and [[Pennsylvania]] have stand-alone rating bureaus that do not integrate data with NCCI.  Other states such as [[Wisconsin]], [[Texas]], [[New York (state)|New York]], [[New Jersey]], [[Indiana]], and [[North Carolina]], maintain their own rating bureaus but integrate with NCCI for multi-state employers.&lt;ref&gt;''Ultimate Guide to Workers' Compensation Insurance'', Edward J. Priz, 2005, Entrepreneur Press, P. 81&lt;/ref&gt;

The experience modifier adjusts workers compensation insurance premiums for a particular employer based on a comparison of past losses of that employer to what is calculated to be "average" losses of other employers in that state in the same business, adjusted for size.  To do this, experience modifier calculations use loss information reported in by an employer's past insurers.  This is compared to a calculation of expected losses for a company in that line of work, in that particular state, and adjusted for the size of the employer.  The calculation of expected losses utilizes past audited payroll information for a particular employer, by classification code and state.  These payrolls are multiplied by Expected Loss Rates, which are calculated by rating bureaus based on past reported claims costs per classification.

Errors in experience modifiers can occur if inaccurate information is reported to a rating bureau by a past insurer of an employer.  Some states (Illinois and Tennessee) prohibit increases in experience modifiers once a workers compensation policy begins, even if the higher modifier has been correctly calculated under the rules.  Most states allow increases in experience modifiers if done relatively early in the term of the workers compensation insurance policy, and most states prohibit increases in experience modifier late in the term of the policy.

The detailed rules governing calculation of experience modifiers are developed by the various rating bureaus.  Although all states use similar methodology, there can be differences in details in the formulas used by independent rating bureaus and the NCCI.

In many NCCI states, the Experience Rating Adjustment plan is in place, allowing for the 70% reduction in the reportable amount of medical-only claims.  That is, for claims where there has been no payment to the worker for lost time, but only for medical expenses. This gives employers an incentive to report all claims to their insurers, rather than trying to pay for medical-only claims out of pocket.  Discounting medical-only claims in the experience modifier calculation greatly reduces the impact of medical-only claims on the modifier.

==Formula and calculations==

The formula primarily used by the NCCI is the following.

&lt;math&gt;
\frac{I+(C*(1-A)+G)+(A*F)}{E+(C*(1-A)+G)+(A*C)}
&lt;/math&gt;
&lt;pre&gt;
A = Weight Factor
G = Ballast
I = Actual Primary Losses
H = Actual Incurred Losses
F = Actual Excess Losses  (H-I)
E = Expected Primary Losses
D = Expected Incurred Loses
C = Expected Excess Losses (D-E)
&lt;/pre&gt;

The formula is broken down into 3 main categories or subsections for understanding.
# Primary Losses
#* Primary losses show up as both I and E in the above formula, E is for "Expected" primary losses vs actual.  This expected value is determined based on a company's payroll cost with a little actuarial calculations.
#* &lt;math&gt;I&lt;/math&gt;
#* &lt;math&gt;E&lt;/math&gt;
# Stabilizing Value
#* This is a calculation based on expected excess losses, a weighting factor, and a Ballast factor.
#* The weighting factor and Ballast factor are determined from proprietary calculations that are not published publicly.
#* &lt;math&gt;(C*(1-A)+G)&lt;/math&gt;
# Ratable Excess
#* Using the weighting factor the Ratable excess is simply the excess losses times this factor.
#*&lt;math&gt;(A*F)&lt;/math&gt;
#*&lt;math&gt;(A*C)&lt;/math&gt;

These 3 categories are summed up, with Actual numbers divided by Expected numbers, notice that the Stabilizing value does not change between the numerator and denominator.

&lt;math&gt;
\frac{ActualPrimaryLosses + StabilizingValue + ActualRatableExcess}{ExpectedPrimaryLosses + StabilizingValue + ExpectedRatableExcess}
&lt;/math&gt;

=== A note about losses ===
In the EMR calculation there are 4 fundamental losses that are necessary for the calculation, they are:
# D = Expected Incurred Losses
# E = Expected Primary Losses
# H = Actual Incurred Losses
#* Claims under $2,000.
# I = Actual Primary Losses
#* All claims including Actual Incurred Losses

The losses that are not part of this fundamental 4 are,
# C = Expected Excess Losses
#* &lt;math&gt;(D-E)&lt;/math&gt;
# F = Actual Excess Losses
#* &lt;math&gt;(H-I)&lt;/math&gt;

==Examples==
[[Unemployment insurance]] is experience rated in the United States; companies that have more claims resulting from past workers face higher unemployment insurance rates.&lt;ref&gt;{{cite journal|url=https://link.springer.com/article/10.1007/BF02788524|title=Experience rating in unemployment insurance|first=Gladys W.|last=Gruenberg|publisher=|journal=FSSE|volume=20|issue=2|pages=33–41|accessdate=8 September 2016|via=link.springer.com|doi=10.1007/BF02788524}}&lt;/ref&gt; The logic of this approach is that these are the companies that are more likely to cause someone to be unemployed, so they should pay more into the pool from which [[unemployment compensation]] is paid.&lt;ref&gt;{{cite web |url=http://www.wcbsask.com/book_employers/page_employers_experience_rating.page?_nfpb=true&amp;_pageLabel=page_search |title=Archived copy |accessdate=2014-02-20 |deadurl=yes |archiveurl=https://web.archive.org/web/20080218065741/http://www.wcbsask.com/book_employers/page_employers_experience_rating.page?_nfpb=true&amp;_pageLabel=page_search |archivedate=2008-02-18 |df= }}&lt;/ref&gt;  Unemployment insurance is financed by a payroll tax paid by employers.  Experience rating in unemployment insurance is described as imperfect, due in large part to the fact that there are statutory maximum and minimum rates that an employer can receive without regard to its history of lay-off.&lt;ref name="auto"&gt;{{cite book |last=Rosen |first=Harvey S. |title= '''''Public Finance''''' |year= 2008 |publisher= [[McGraw-Hill/Irvin]] |location=[[New York, New York]] |isbn=978-0-07-351128-3 | pages=293}}&lt;/ref&gt;  If a worker is laid off, generally the increased costs to the employer due to the higher value of unemployment insurance tax rates are less than the UI benefits received by the worker.&lt;ref name="auto"/&gt;

==References==
{{Reflist|2}}

{{DEFAULTSORT:Experience Modifier}}
[[Category:Insurance in the United States]]
[[Category:Actuarial science]]</text>
      <sha1>4n28fzjfsos54skwz46xq0t24q1gkao</sha1>
    </revision>
  </page>
  <page>
    <title>Factory Physics</title>
    <ns>0</ns>
    <id>17644390</id>
    <revision>
      <id>825101935</id>
      <parentid>825101900</parentid>
      <timestamp>2018-02-11T14:11:31Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Manufacturing]]; added [[Category:Operations research]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2523">'''''Factory Physics'''''&lt;ref&gt;[http://www.factoryphysics.com/custom_page.cfm?category=5&amp;page=257&amp;active=257 Factory Physics]&lt;/ref&gt; is a book written by Wallace Hopp and Mark Spearman, which introduces a science of operations for [[manufacturing]] [[management]].  According to the book's preface, Factory Physics is "a systematic description of the underlying behavior of [[Operations_management#Production_systems|manufacturing systems]].  Understanding it enables managers and engineers to work with the natural tendencies of manufacturing systems to:

# Identify opportunities for improving existing systems.
# Design effective new systems.
# Make the [[trade-off]]s needed to coordinate policies from disparate areas.

The book is used both in industry and in academia for reference and teaching on [[operations management]]. It describes a new approach to manufacturing management based on the laws of Factory Physics science. The fundamental Factory Physics framework states that the essential components of all value streams or production processes or service processes are demand and transformation which are described by structural elements of flows and stocks. There are very specific practical, [[mathematical]] relationships that enable one to describe and control the performance of flows and stocks. The book states that, in the presence of [[Statistical variability|variability]], there are only three buffers available to synchronize [[demand]] and transformation with lowest [[cost]] and highest [[service level]]:

*[[Capacity utilization|capacity]]
*[[inventory]]
*response time.

The book states that its approach enables practical, predictive understanding of flows and stocks and how to best use the three levers to optimally synchronize demand and transformation.

This work won the 1996 [[Institute of Industrial Engineers]] IIE/Joint Publishers Book of the Year Award.&lt;ref&gt;http://www.iienet2.org/awards.aspx?id=10802&lt;/ref&gt;

==Editions==
*''Factory Physics: Foundations of Manufacturing Management'', first edition, 1996. 668pp. {{ISBN|0-256-15464-3}}.
*''Factory Physics: Foundations of Manufacturing Management'', second edition, 2000. 698pp. {{ISBN|0-256-24795-1}}.
*''Factory Physics: Foundations of Manufacturing Management'', third edition, 2008. 720pp. {{ISBN|978-0-07-282403-2}}.

==See also==
* [[CONWIP]]
* [[Supply chain management]]

==References==
{{reflist}}
{{refbegin}}
{{refend}}

{{italic title}}
[[Category:1996 books]]
[[Category:Business books]]
[[Category:Operations research]]</text>
      <sha1>ppk6a6a209o0pnfkwemivqzghfw7gwl</sha1>
    </revision>
  </page>
  <page>
    <title>Frederik Schuh</title>
    <ns>0</ns>
    <id>36617753</id>
    <revision>
      <id>857357904</id>
      <parentid>730946326</parentid>
      <timestamp>2018-08-31T05:13:55Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>/* External links */add authority control, test</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1094">'''Frederik Schuh''' (7 February 1875, [[Amsterdam]] – 6 January 1966, [[The Hague]]) was a Dutch mathematician.

==Career==

He completed his PhD in algebraic geometry from Amsterdam University in 1905, where his advisor was [[Diederik Johannes Korteweg]].&lt;ref&gt;{{MathGenealogy|id=74731}}&lt;/ref&gt; He taught at the Technische Hoogeschool at Delft (1907–1909 and 1916–1945) and at Groningen (1909–1916).&lt;ref&gt;[[The Universal Book of Mathematics]]&lt;/ref&gt;

==Works==

He was the inventor of the [[Chomp]] game and wrote ''The Master Book of Mathematical Recreations'' (1943).

==References==

{{reflist}}

==External links==
* [http://www.genealogy.math.ndsu.nodak.edu/id.php?id=74731 Mathematics Genealogy]


{{authority control}}

{{DEFAULTSORT:Schuh, Frederik}}
[[Category:1875 births]]
[[Category:1966 deaths]]
[[Category:20th-century Dutch mathematicians]]
[[Category:Game theorists]]
[[Category:Scientists from Amsterdam]]
[[Category:University of Amsterdam alumni]]
[[Category:Delft University of Technology faculty]]
[[Category:University of Groningen faculty]]


{{mathematician-stub}}</text>
      <sha1>arinaowiswqpqa234gldqbvqf2w1de5</sha1>
    </revision>
  </page>
  <page>
    <title>Generalized game</title>
    <ns>0</ns>
    <id>746550</id>
    <revision>
      <id>818365883</id>
      <parentid>816863491</parentid>
      <timestamp>2018-01-03T03:40:51Z</timestamp>
      <contributor>
        <username>MeixiangKazuki</username>
        <id>32586779</id>
      </contributor>
      <comment>Add citations needed tags, remove arbitrary entries in "See also", and remove "External links" (has no entries with close connection to topic of this article).</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1753">{{multiple image
 | width = 180
 | footer = Generalized [[Sudoku]] includes puzzles of different sizes
 | image1 = Minisudoku1.png
 | alt1 = Sudoku (4×4)
 | caption1 = Sudoku (4×4)
 | image2 = Sudoku_Puzzle_(Tourmaline)R2.png
 | alt2 = Sudoku (9×9)
 | caption2 = Sudoku (9×9)
 | image3 = 25by25sudoku.png
 | alt3 = Sudoku (25×25)
 | caption3 = Sudoku (25×25)
 }}
In [[computational complexity theory]], a '''generalized game''' is a game or puzzle that has been generalized so that it can be played on a board or grid of any size. For example, generalized [[chess]] is the game of [[chess]] played on an ''n×n'' board, with 2''n'' pieces on each side. Generalized [[Sudoku]] includes Sudokus constructed on an ''n×n'' grid.

Complexity theory studies the [[asymptotic]] difficulty of problems, so generalizations of games are needed, as games on a fixed size of board are finite problems.

For many generalized games which last for a number of moves polynomial in the size of the board, the problem of determining if there is a win for the first player in a given position is [[PSPACE-complete]]. Generalized [[hex (board game)|hex]] and [[reversi]] are PSPACE-complete.{{citation needed|date=January 2018}}

For many generalized games which may last for a number of moves exponential in the size of the board, the problem of determining if there is a win for the first player in a given position is [[EXPTIME-complete]]. Generalized [[chess]], [[go (board game)|go]] and [[checkers]] are EXPTIME-complete.{{citation needed|date=January 2018}}

==See also==
*[[Game complexity]]
*[[Combinatorial game theory]]
&lt;br /&gt;
[[Category:Computational complexity theory]]
[[Category:Combinatorial game theory]]

{{numtheory-stub}}
{{comp-sci-theory-stub}}</text>
      <sha1>rs4amipk0s4nbytqsegisljpdhntfft</sha1>
    </revision>
  </page>
  <page>
    <title>Glaeser's composition theorem</title>
    <ns>0</ns>
    <id>37595113</id>
    <revision>
      <id>675111751</id>
      <parentid>655578249</parentid>
      <timestamp>2015-08-08T08:39:24Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <comment>/* References */ 10.2307/1970204</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="889">In mathematics, '''Glaeser's theorem''', introduced by {{harvs|txt|last=Glaeser|first=Georges|authorlink=Georges Glaeser|year=1963}},  is a theorem giving conditions for a [[smooth function]] to be a [[Function composition|composition]] of ''F'' and θ for some given smooth function θ. One consequence is a generalization of [[Newton's theorem on symmetric polynomials|Newton's theorem]] that every [[symmetric polynomial]] is a polynomial in the [[elementary symmetric polynomial]]s, from polynomials to smooth functions.

==References==
*{{Citation | last1=Glaeser | first1=Georges | authorlink = Georges Glaeser | title=Fonctions composées différentiables | jstor = 1970204 | mr = 0143058 | year=1963 | journal=[[Annals of Mathematics]] | series = Second Series | volume=77 | pages=193–209 | doi = 10.2307/1970204 }}

[[Category:Theorems in real analysis]]


{{mathanalysis-stub}}</text>
      <sha1>exf5doeh2wm2tjc5eo5ck5gv0yjj5lz</sha1>
    </revision>
  </page>
  <page>
    <title>Identric mean</title>
    <ns>0</ns>
    <id>7423424</id>
    <revision>
      <id>608209579</id>
      <parentid>573834031</parentid>
      <timestamp>2014-05-12T11:45:33Z</timestamp>
      <contributor>
        <username>Monkbot</username>
        <id>20483999</id>
      </contributor>
      <minor/>
      <comment>Task 3: Fix [[Help:CS1_errors#deprecated_params|CS1 deprecated coauthor parameter errors]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1360">{{Refimprove|date=January 2010}}
The '''identric mean''' of two positive [[real number]]s ''x'',&amp;nbsp;''y'' is defined as:&lt;ref name=RICHARDS2006&gt;{{cite journal|last=RICHARDS|first=KENDALL C|author2=HILARI C. TIEDEMAN|title=A NOTE ON WEIGHTED IDENTRIC AND LOGARITHMIC MEANS|journal=Journal of Inequalities in Pure and Applied Mathematics|year=2006|volume=7|issue=5|url=http://www.kurims.kyoto-u.ac.jp/EMIS/journals/JIPAM/images/202_06_JIPAM/202_06_www.pdf|accessdate=20 September 2013}}&lt;/ref&gt; 
:&lt;math&gt;
\begin{align}
I(x,y)
&amp;=
\frac{1}{e}\cdot
\lim_{(\xi,\eta)\to(x,y)}
\sqrt[\xi-\eta]{\frac{\xi^\xi}{\eta^\eta}}
\\[8pt]
&amp;=
\lim_{(\xi,\eta)\to(x,y)}
\exp\left(\frac{\xi\cdot\ln\xi-\eta\cdot\ln\eta}{\xi-\eta}-1\right)
\\[8pt]
&amp;=
\begin{cases}
x &amp; \text{if }x=y \\[8pt]
\frac{1}{e} \sqrt[x-y]{\frac{x^x}{y^y}} &amp; \text{else}
\end{cases}
\end{align}
&lt;/math&gt;

It can be derived from the [[mean value theorem]] by considering the [[Secant line|secant]] of the graph of the function &lt;math&gt;x \mapsto x\cdot \ln x&lt;/math&gt;. It can be generalized to more variables according by the [[mean value theorem for divided differences]]. The identric mean is a special case of the [[Stolarsky mean]].

==See also==
* [[Mean]]
* [[Logarithmic mean]]

==References==
{{reflist}}
{{MathWorld|title=Identric Mean|urlname=IdentricMean}}

{{DEFAULTSORT:Identric Mean}}
[[Category:Means]]</text>
      <sha1>nqgmsdonsd562pnzzww32zgrmy7a21x</sha1>
    </revision>
  </page>
  <page>
    <title>Inscribed angle theorem</title>
    <ns>0</ns>
    <id>19358786</id>
    <redirect title="Inscribed angle" />
    <revision>
      <id>457009219</id>
      <parentid>419994834</parentid>
      <timestamp>2011-10-23T17:23:52Z</timestamp>
      <contributor>
        <username>Geometry guy</username>
        <id>3483166</id>
      </contributor>
      <comment>subcat</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="89">#REDIRECT [[Inscribed angle#Theorem]] {{R from merge}}

[[Category:Theorems in geometry]]</text>
      <sha1>7psj7nj7m4uaqc5ou043azp759rqvaa</sha1>
    </revision>
  </page>
  <page>
    <title>John Hudson (mathematician)</title>
    <ns>0</ns>
    <id>17920985</id>
    <revision>
      <id>857361639</id>
      <parentid>746967430</parentid>
      <timestamp>2018-08-31T05:38:59Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>/* References */add authority control, test</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2918">{{Other people|John Hudson}}
{{Infobox scientist
|name              = John Hudson
|image             =  &lt;!--(filename only)--&gt;
|caption           = John Hudson (1773-1843)  
|birth_date        = 1773
|birth_place       = 
|death_date        = 31 October 1843
|death_place       = [[Haverbrack]], [[England]]
|residence         = [[England]]
|citizenship       = 
|nationality       = [[English people|English]]
|ethnicity         = 
|fields            = [[Mathematician]]
|workplaces        = [[Trinity College, Cambridge]]
|alma_mater        = [[Trinity College, Cambridge]]
|doctoral_advisor  = 
|academic_advisors = [[Thomas Jones (mathematician)|Thomas Jones]]
|doctoral_students = 
|notable_students  = [[George Peacock]]&lt;br /&gt;[[John Martin Frederick Wright]]&lt;br /&gt;[[Charles James Blomfield]] 
|known_for         = 
|influences        = 
|influenced        = 
|awards            = [[Smith's prize]] (1797)
|signature         = &lt;!--(filename only)--&gt;
|footnotes         = 
}}

'''John Hudson'''  (1773 &amp;ndash; 31 October 1843) was an [[England|English]] [[mathematician]] and [[clergyman]]. He was notable for being a [[senior wrangler]] as well as the tutor of [[George Peacock]].

==Early life==
John Hudson was the son of John Hudson, a farmer at [[Haverbrack]] in the parish of [[Beetham]]. He attended [[Heversham Grammar School|Heversham School]] and entered [[Trinity College, Cambridge]] in 1793. He became [[senior wrangler]] in 1797, also winning the [[Smith's prize]] in that year, and obtained his MA in 1800.&lt;ref&gt;{{acad|id=HT793J|name=Hudson, John}}&lt;/ref&gt;

==Career==
He became a Fellow, in 1798, and tutor, in 1807, of  [[Trinity College, Cambridge]], where he notably tutored [[George Peacock]]: he also tutored [[John Martin Frederick Wright]]. In 1815, he became the vicar of [[Kendal]], [[Westmorland|Westmoreland]].  In 1815, he married the daughter of an army officer by the name of Culliford.

At Cambridge, Hudson also tutored [[Charles James Blomfield]] who became a prominent bishop. As a bishop, Blomfield visited Hudson's parish and at a dinner party declared "I remember well, Mr. Hudson, how much I stood in awe of you at College." To which Hudson retorted, "Perhaps so, but your Lordship has turned the tables on me now."

Hudson died at [[Haverbrack]], Tuesday, October 31, 1843 at the age of 71 and was buried in the interior of the parish church at [[Kendal]].

==References==
{{Reflist}}
* ''[[The Gentleman's Magazine]]'', Dec 1843, p.&amp;nbsp;662
* Cornelius Nicholson, ''The annals of Kendal'', 1861, pp.&amp;nbsp;374–375.


{{authority control}}

{{DEFAULTSORT:Hudson, John}}
[[Category:1773 births]]
[[Category:1843 deaths]]
[[Category:18th-century English mathematicians]]
[[Category:19th-century English mathematicians]]
[[Category:Alumni of Trinity College, Cambridge]]
[[Category:Fellows of Trinity College, Cambridge]]
[[Category:Senior Wranglers]]


{{UK-mathematician-stub}}</text>
      <sha1>717qlfixh2wz1deyfncftfkmcje3khx</sha1>
    </revision>
  </page>
  <page>
    <title>Kneser–Ney smoothing</title>
    <ns>0</ns>
    <id>45391945</id>
    <revision>
      <id>787451608</id>
      <parentid>779481236</parentid>
      <timestamp>2017-06-25T14:09:13Z</timestamp>
      <contributor>
        <ip>24.125.31.82</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4712">'''Kneser–Ney smoothing''' is a method primarily used to calculate the [[probability]] distribution of [[n-gram|''n''-gram]]s in a [[document]] based on their histories.&lt;ref&gt;[http://www.stats.ox.ac.uk/~teh/research/compling/hpylm.pdf 'A Bayesian Interpretation of Interpolated Kneser-Ney NUS School of Computing Technical Report TRA2/06']&lt;/ref&gt; It is widely considered the most effective method of [[smoothing]] due to its use of absolute discounting by subtracting a fixed value from the probability's lower order terms to omit ''n''-grams with lower frequencies. This approach has been considered equally effective for both higher and lower order ''n''-grams. The method is due to Reinhard Kneser and Hermann Ney.&lt;ref&gt;N
-gram Counts and Language Models from the Common Crawl&lt;/ref&gt;  

A common example that illustrates the concept behind this method is the frequency of the [[bigram]] "[[San Francisco]]". If it appears several times in a training [[Text corpus|corpus]], the frequency of the [[unigram]] "Francisco" will also be high. Relying on only the unigram frequency to predict the frequencies of ''n''-grams leads to skewed results;&lt;ref&gt;[http://cs.brown.edu/courses/cs146/assets/files/langmod.pdf 'Brown University: Introduction to Computational Linguistics ']&lt;/ref&gt; however, Kneser–Ney smoothing corrects this by considering the frequency of the unigram in relation to possible words preceding it.

==Method==
Let &lt;math&gt;c(w,w')&lt;/math&gt; be the number of occurrences of the word &lt;math&gt;w&lt;/math&gt; followed by the word &lt;math&gt;w'&lt;/math&gt; in the corpus.

The equation for bigram probabilities is as follows:

&lt;math&gt;
p_{KN}(w_i|w_{i-1}) = \frac{\max(c(w_{i-1},w_{i}) - \delta,0)}{\sum_{w'} c(w_{i-1},w')} + \lambda_{w_{i-1}} p_{KN}(w_i) 
&lt;/math&gt;&lt;ref&gt;[http://www.foldl.me/2014/kneser-ney-smoothing/ 'Kneser Ney Smoothing Explained']&lt;/ref&gt;

Where the unigram probability &lt;math&gt;p_{KN}(w_i)&lt;/math&gt; depends on how likely it is to see the word &lt;math&gt;w_i&lt;/math&gt; in an unfamiliar context, which is estimated as the number of times it appears after any other word divided by the number of distinct pairs of consecutive words in the corpus:

&lt;math&gt;
p_{KN}(w_i) = \frac { |\{ w' : 0 &lt; c(w',w_i)     \} | } 
                    { |\{ (w',w'') : 0 &lt; c(w',w'') \} | }
&lt;/math&gt;

Please note that &lt;math&gt;p_{KN}&lt;/math&gt; is a proper distribution, as the values defined in above way are non-negative and sum to one.

The parameter &lt;math&gt;\delta&lt;/math&gt; is a constant which denotes the discount value subtracted from the count of each n-gram, usually between 0 and 1.

The value of the normalizing constant &lt;math&gt;\lambda_{w_{i-1}}&lt;/math&gt; is calculated to make the sum of conditional probabilities &lt;math&gt;p_{KN}(w_i|w_{i-1})&lt;/math&gt; over all &lt;math&gt;w_i&lt;/math&gt; equal to one. 
Observe that (provided &lt;math&gt;\delta &lt; 1 &lt;/math&gt;) for each &lt;math&gt;w_i&lt;/math&gt; which occurs at least once in the context of &lt;math&gt;w_{i-1}&lt;/math&gt; in the corpus we discount the probability by exactly the same constant amount &lt;math&gt;{\delta} / \left(\sum_{w'} c(w_{i-1},w')\right)&lt;/math&gt;,
so the total discount depends linearly on the number of unique words &lt;math&gt;w_i&lt;/math&gt; that can occur after &lt;math&gt;w_{i-1}&lt;/math&gt;.
This total discount is a budget we can spread over all &lt;math&gt;p_{KN}(w_i|w_{i-1})&lt;/math&gt; proportionally to &lt;math&gt;p_{KN}(w_i)&lt;/math&gt;.
As the values of &lt;math&gt;p_{KN}(w_i)&lt;/math&gt; sum to one, we can simply define &lt;math&gt;\lambda_{w_{i-1}}&lt;/math&gt; to be equal to this total discount:

&lt;math&gt;
\lambda_{w_{i-1}} = \frac {\delta} {\sum_{w'} c(w_{i-1},w')} |\{ w' : 0 &lt; c(w_{i-1},w') \} |
&lt;/math&gt;

This equation can be extended to n-grams. Let &lt;math&gt;w_{i-n+1}^{i-1}&lt;/math&gt; be the &lt;math&gt;n-1&lt;/math&gt; words before &lt;math&gt;w_i&lt;/math&gt;:

&lt;math&gt;
p_{KN}(w_i|w_{i-n+1}^{i-1}) = \frac{\max(c(w_{i-n+1}^{i-1},w_{i}) - \delta,0)}{\sum_{w'} c(w_{i-n+1}^{i-1},w')} + \delta \frac{| \{ w' : 0 &lt; c(w_{i-n+1}^{i-1},w') \} |}{\sum_{w_i} c(w_{i-n+1}^{i})} p_{KN}(w_i|w_{i-n+2}^{i-1})
&lt;/math&gt;&lt;ref&gt;[http://nlp.stanford.edu/~wcmac/papers/20050421-smoothing-tutorial.pdf 'NLP Tutorial: Smoothing']&lt;/ref&gt;


This model uses the concept of absolute-discounting interpolation which incorporates information from higher and lower order language models. The addition of the term for lower order n-grams adds more weight to the overall probability when the count for the higher order n-grams is zero.&lt;ref&gt;[http://u.cs.biu.ac.il/~yogo/courses/mt2013/papers/chen-goodman-99.pdf 'An empirical study of smoothing techniques for language modeling']&lt;/ref&gt; Similarly, the weight of the lower order model decreases when the count of the n-gram is non zero.

==References==
{{Reflist}}

{{DEFAULTSORT:Kneser-Ney smoothing}}
[[Category:Statistical methods]]
[[Category:Language modeling]]


{{probability-stub}}</text>
      <sha1>r01jee1w3dtaaipvyd4ohy75dnd8wh6</sha1>
    </revision>
  </page>
  <page>
    <title>Le Chiffre</title>
    <ns>0</ns>
    <id>2058776</id>
    <revision>
      <id>866909789</id>
      <parentid>866909784</parentid>
      <timestamp>2018-11-02T10:43:00Z</timestamp>
      <contributor>
        <username>Wtmitchell</username>
        <id>136745</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/176.133.243.124|176.133.243.124]] ([[User talk:176.133.243.124|talk]]) ([[WP:HG|HG]]) (3.4.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14594">{{Use British English|date=January 2015}}
{{Use dmy dates|date=June 2011}}

{{Infobox character
|name        = Le Chiffre
|image       = Le Chiffre by Mads Mikkelsen.jpg
|caption     = Mads Mikkelsen as Le Chiffre in [[Casino Royale (2006 film)|''Casino Royale'' (2006)]]
|portrayer   = {{Plainlist|
*[[Peter Lorre]] (1954)
*[[Orson Welles]] (1967)
*[[Mads Mikkelsen]] (2006)
}}
|colour      = #000
|series      = [[James Bond]]
|first       = &lt;!-- First appearance --&gt;
|last        = &lt;!-- Last appearance to date --&gt;
|creator     = [[Ian Fleming]]
|gender      = Male
|occupation  = Paymaster for the Syndicat des Ouvriers d'Alsace (novel)&lt;br&gt;Terrorist banker (film)
|affiliation = {{Plainlist|
*[[SMERSH (James Bond)|SMERSH]] (novel)
*[[SPECTRE|Spectre]]
*[[Quantum (James Bond)|Quantum]]&lt;ref&gt;{{cite news |first=Anthony|last=Breznican |url=https://www.usatoday.com/life/movies/news/2008-04-03-bond-quantum_N.htm|title=James Bond series takes a 'Quantum' leap |work=[[USA Today]] |publisher=[[Gannett Company]]|location=McLean, Virginia|date=April 4, 2008 |accessdate=April 4, 2008}}&lt;/ref&gt; (2006 film)
}}
|nationality = Albanian
|lbl21       = Classification 
|data21      = [[List of James Bond villains|Villain]]
|lbl22       = Henchmen
|data22      = [[List of henchmen of James Bond villains#Casino Royale|Valenka]]
}}
'''Le Chiffre''' ({{IPA-fr|lə ʃifʁ|lang}}, "The Cypher" or "The Number") is a fictional character appearing in [[Ian Fleming]]'s 1953 first [[James Bond]] novel, ''[[Casino Royale (novel)|Casino Royale]]''. On screen Le Chiffre has been portrayed by [[Peter Lorre]] in the [[Casino Royale (Climax! episode)|1954 television adaptation]] of the novel for [[CBS]]'s ''[[Climax!]]'' television series, by [[Orson Welles]] in the [[Casino Royale (1967 film)|1967 spoof]] of the novel and Bond film series, and by [[Mads Mikkelsen]] in the [[Casino Royale (2006 film)|2006 film version]] of Fleming's novel.

Fleming based the character on occultist [[Aleister Crowley]].&lt;ref&gt;{{cite news | first=Ben|last=Macintyre | url = http://entertainment.timesonline.co.uk/tol/arts_and_entertainment/books/article3652410.ece |title = Was Ian Fleming the real 007? |work=[[The Times]] |publisher=[[News UK]]| location=London, England | date = 5 April 2008| accessdate=8 April 2008}}&lt;/ref&gt;

==Novel biography==
Le Chiffre, alias "Die Nummer", "Mr. Number", "Herr Ziffer" and other translations of "The Number" or "The Cipher" in various languages, is the paymaster of the "Syndicat des Ouvriers d'Alsace" (French for "Alsatian Workmen's Union"), a [[SMERSH (James Bond)|SMERSH]]-controlled trade union.

He is first encountered as an inmate of the [[Dachau]] [[displaced persons camp]] in the US zone of Germany in June 1945 and transferred to [[Alsace-Lorraine]] and [[Strasbourg]] three months later on a stateless passport. There he adopts the name Le Chiffre because as he claims, he is "only a number on a passport". Not much else is really known about Le Chiffre's background or where he comes from, except for educated guesses based on his description:

{{quote|Height 5 ft. 8 in. Weight 18 stone. 
Complexion very pale. Clean-shaven. Hair red- 
brown, 'en brosse.' Eyes very dark brown with 
whites showing all round iris. Small, rather 
feminine mouth. False teeth of expensive 
quality. Ears small, with large lobes, indicating 
some Jewish blood. Hands small, well-tended, 
hirsute. Feet small. Racially, subject is probably 
a mixture of Mediterranean with Prussian or 
Polish strains. Dresses well and meticulously, 
generally in dark double-breasted suits. Smokes 
incessantly Caporals, using a denicotinizing 
holder. At frequent intervals inhales from ben- 
zedrine inhaler. Voice soft and even. Bilingual 
in French and English. Good German. Traces 
of Marseillais accent. Smiles infrequently. Does 
not laugh. 

Habits: Mostly expensive, but discreet. Large sexual appetites. Flagellant. Expert driver of fast 
cars. Adept with small arms and other forms of personal combat, including knives. Carries 
three Eversharp razor blades, in hatband, heel 
of left shoe, and cigarette case. Knowledge of 
accountancy and mathematics. Fine gambler.}}

[[File:Peter_Lorre_as_Le_Chiffre.jpeg|thumb|[[Peter Lorre]] as Le Chiffre in the [[Casino Royale (Climax!)|1954 TV adaption of ''Casino Royale'']].]]

In the novel, he makes a major investment in a string of brothels with money belonging to SMERSH. The investment fails after a bill is signed into law banning prostitution. Le Chiffre then goes to the casino Royale-les-Eaux in an attempt to replace his lost funds. [[MI6]] sends Bond, an expert [[Baccarat (card game)|baccarat]] player, to the casino to bankrupt Le Chiffre and force him to take refuge with the British government and inform on SMERSH. Bond bests Le Chiffre in a game of [[Baccarat (card game)#Chemin de fer|Chemin de Fer]], taking all of his money. Le Chiffre kidnaps [[Bond girl|Bond's love interest]], [[Vesper Lynd]], to lure Bond into a trap and get back his money. The trap works, and Le Chiffre tortures Bond to get him to give up the money. He is interrupted by a SMERSH agent, however, who shoots him between the eyes with a [[Suppressor|silenced]] [[TT pistol]] as punishment for losing the money.

Le Chiffre's death is seen by the Soviet government as an embarrassment, which in addition to the death and defeat of [[Mr. Big (James Bond)|Mr. Big]] in ''[[Live and Let Die (novel)|Live and Let Die]]'', leads to the events of ''[[From Russia, with Love (novel)|From Russia, with Love]]''.

===Novel henchmen===
* Basil – bodyguard and martial arts expert who takes pleasure in roughing up Bond. He is later killed by a SMERSH agent.
* Kratt - Le Chiffre's Corsican bodyguard who wields a walking-stick gun with which he threatens to cripple Bond at the gaming table. He is later killed by a SMERSH agent.

==1967 film biography==
[[File:Orson Welles As Le Chiffre in Casino Royale (1967).png|thumb|right|[[Orson Welles]] as Le Chiffre in the 1967 film ''[[Casino Royale (1967 film)|Casino Royale]]''.]]

Le Chiffre is a secondary villain in the 1967 [[satire]] and appears in one of the few segments of the [[Casino Royale (1967 film)|film]] actually adapted from Fleming's book. As in the novel, Le Chiffre is charged with recovering a large sum of money for SMERSH after he loses it at the [[baccarat]] table. He first attempts to raise the funds by holding an auction of embarrassing photographs of military and political leaders from China, the US and the [[USSR]], but this is foiled by Sir James Bond's daughter, [[List of characters in Casino Royale (1967)#Mata Bond|Mata Bond]]. With no other option, he returns to the baccarat table to try to win back the money. Later, he encounters baccarat Master [[List of James Bond characters in Casino Royale#Evelyn Tremble|Evelyn Tremble]], who has been recruited by Bond to stop Le Chiffre from raising the money. Le Chiffre attempts to distract Tremble by performing elaborate magic tricks, but fails to prevent Tremble from winning. Afterwards, he arranges for Tremble to be kidnapped and subjects the agent to [[Psychedelia|psychedelic]] torture in order to get back the money. The torture session is interrupted when his SMERSH masters, led by the film's main villain, Dr. Noah, shoot him dead.

==2006 film biography==
Le Chiffre is the main villain of the official 2006 James Bond film, ''[[Casino Royale (2006 film)|Casino Royale]]'', portrayed by [[Danish people|Danish]] actor [[Mads Mikkelsen]]. Believed by [[Secret Intelligence Service|MI6]] to be [[Albanians|Albanian]] and officially [[stateless person|stateless]], Le Chiffre is a financier of international terrorism. [[M (James Bond)|M]] implies that Le Chiffre conspired with [[al-Qaeda]] in orchestrating [[September 11 attacks|9/11]], or at least deliberately [[war profiteering|profiteering]] from the attacks by [[short selling]] large quantities of airline stocks beforehand. In the video game version of ''[[Quantum of Solace]]'', it is said that his birth name is "Jean Duran", in the [[MI6]] mission briefings. A mathematical [[genius]] and a [[chess]] [[Child prodigy|prodigy]], his abilities enable him to earn large sums of money on games of chance and probabilities, and he likes to show off by playing [[poker]]. He suffers from [[haemolacria]], which causes him to weep blood out of a damaged vessel in his left eye. As in Fleming's novel, he dresses in immaculate black suits and uses a [[Salbutamol]] inhaler, here plated with platinum.

Le Chiffre is contacted by [[Mr. White (James Bond)|Mr. White]], a representative of an elite criminal organisation known as [[SPECTRE]]. White introduces [[Steven Obanno]], a leader of the [[Lord's Resistance Army]] in [[Uganda]], to Le Chiffre, and arranges to bank several briefcases full of cash for Obanno. Le Chiffre invests the money along with his other creditors' money in the aircraft manufacturer SkyFleet. Though SkyFleet's shares have been skyrocketing, he plans to short the company by purchasing [[put option]]s, and ordering the destruction of the company's new prototype airliner, set to make its first flight out of [[Miami International Airport]]. Bond intervenes and foils the plan by killing the person Le Chiffre hired to destroy the plane.  Le Chiffre finds out he lost millions and realises leaked information about his plans.

In order to win the money back, Le Chiffre sets up and enters a high-stakes [[Texas hold 'em]] tournament at Casino Royale in [[Montenegro]] in an attempt to recoup his losses before his clients find out that their money has been misappropriated. Bond is sent to make sure that Le Chiffre does not win back the money; if Le Chiffre is bankrupt, he will be forced to turn to MI6 for asylum, in exchange for information on his creditors and employers.

During the tournament, an angry Obanno and his lieutenant break into Le Chiffre's hotel room and threaten him and his girlfriend, [[List of James Bond henchmen in Casino Royale#Casino Royale|Valenka]]. Le Chiffre, who does not object to the threatened amputation of Valenka's arm, is granted one last chance to win their money back. As Obanno leaves the room, his bodyguard spots Bond and hears Valenka's cries coming from Bond's earpiece. Bond kills the bodyguard by throwing him over a railing, then chokes Obanno to death after relieving Obanno of his machete. [[Rene Mathis]] arranges the blame to be placed on Le Chiffre's bodyguard Leo.

During the tournament, Le Chiffre initially outwits and bankrupts Bond, who cannot get additional funding from [[HM Treasury]] accountant [[Vesper Lynd]], who has accompanied Bond to make sure the money is used properly. However, [[Felix Leiter]], a [[CIA]] agent sent to participate in the game, also in hopes of bankrupting Le Chiffre, agrees to bankroll Bond, on the condition that CIA is allowed to take Le Chiffre in afterwards. Desperate, Le Chiffre has Valenka spike Bond's drink. Bond almost dies, but, thanks to an [[antitoxin]] kit in his car, a defibrillator, and Vesper's timely interference, he is revived at the last moment and returns to the game. During the final round, Le Chiffre's [[List of poker hands#Full house|full house]] bests the hands of the two players preceding him, but loses to Bond's [[straight flush]].

Le Chiffre kidnaps Vesper, forcing Bond to give chase, and leads him straight into a trap. Le Chiffre leaves Vesper, bound at the feet and hands, in the middle of the road, and Bond is forced to swerve to avoid hitting her and crashes his car.

Semiconscious, Bond is stripped naked and bound to a chair with the seat removed. Le Chiffre proceeds to bludgeon Bond in the [[testicles]] repeatedly with the knotted end of a ship's lanyard, each time demanding the password for the account into which the tournament winnings will be transferred. Bond refuses to give in, taunting him with the knowledge that he knows Le Chiffre's clients will track and kill him. Le Chiffre gloats that, even after he kills Bond and Vesper, MI6 will still give him sanctuary in return for information. When Bond refuses to give in, Le Chiffre brandishes a knife and is about to [[castration|castrate]] him when he hears gunshots from outside. Seconds later, Mr. White bursts into the room brandishing a handgun. Le Chiffre pleads for his life, but Mr. White nevertheless shoots him above the left eye. To date, he is the only main Bond villain to die before the film's final act.

Le Chiffre is mentioned in the [[Quantum of Solace]] and is also seen in a background image inside MI6.  
Le Chiffre appears in several images in ''[[Spectre (2015 film)|Spectre]]'' as it is revealed that he, more or less conscious was an associate of Spectre and Ernst Stravo Blofeld.  Blofeld states to Bond Le Chiffre's affiliation with Spectre and how Bond's interference in Blofeld's world caused him to destroy Bond's, implying Bond's foiling of Le Chiffre's and Quantum's scheme led him to enlist Raoul Silva to destroy MI6 and also kill M.

==Appearances==
=== [[Eon Productions|Eon]] films ===
* ''[[Casino Royale (2006 film)|Casino Royale]]'' (2006)
* ''[[Quantum of Solace]]'' (2008) – mentioned/seen in a photograph only
* ''[[Spectre (2015 film)|Spectre]]'' (2015) – mentioned/seen in archive footage and a photograph only

=== Non-[[Eon Productions|Eon]] productions ===
* "[[Casino Royale (Climax!)|Casino Royale]]" (a [[CBS]] television adaptation for the TV series ''[[Climax!]]'', 1954)
* ''[[Casino Royale (1967 film)|Casino Royale]]'' (a [[Columbia Pictures]] release, 1967)

===2006 film henchmen===
{{main|List of henchmen of James Bond villains#Casino Royale}}
* Alex Dimitrios
* Carlos
* Mollaka
* Leo
* Bobbie
* Jochum
* Kratt
* Valenka
* General Grafin von Wallenstein
* Madame Wu
* Sheriff Tomelli
* Lionel
* Ison

==See also==
* [[SMERSH (James Bond)]]
* [[Casino Royale (novel)]]
* [[Casino Royale (Climax!)]]
* [[Casino Royale (1967 film)]]
* [[Casino Royale (2006 film)]]

==Notes==
{{reflist}}

{{James Bond characters}}
{{Casino Royale (2006 film)}}

{{DEFAULTSORT:Chiffre, Le}}
[[Category:Bond villains]]
[[Category:Fictional gamblers]]
[[Category:Fictional mathematicians]]
[[Category:Fictional murderers]]
[[Category:Fictional torturers]]
[[Category:Literary villains]]
[[Category:Fictional Albanian people]]
[[Category:Fictional French people]]
[[Category:Casino Royale (2006 film)]]
[[Category:Fictional characters introduced in 1953]]
[[Category:Fictional characters based on real people]]
[[Category:Characters in British novels of the 20th century]]
[[Category:Fictional bankers]]</text>
      <sha1>2k7sibzj6c75rn6auefhjgu6gfqyls4</sha1>
    </revision>
  </page>
  <page>
    <title>Limits of integration</title>
    <ns>0</ns>
    <id>12313191</id>
    <revision>
      <id>544869251</id>
      <parentid>492222615</parentid>
      <timestamp>2013-03-17T06:59:39Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Migrating 1 interwiki links, now provided by [[Wikipedia:Wikidata|Wikidata]] on [[d:q6549573]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1002">{{Unreferenced|date=July 2007}}

In [[calculus]] and [[mathematical analysis]] the '''limits of integration''' of the [[integral]]
:&lt;math&gt; \int_a^b f(x) \, dx &lt;/math&gt;
of a [[Riemann integral|Riemann integrable]] [[function (mathematics)|function]] ''f'' defined on a [[closed set|closed]] and [[bounded set|bounded]] [interval] are the [[real number]]s ''a'' and ''b''.

==Improper integrals==

'''Limits of integration''' can also be defined for [[improper integral]]s, with the limits of integration of both
:&lt;math&gt; \lim_{z \rightarrow a^+} \int_z^b f(x) \, dx&lt;/math&gt;
and
:&lt;math&gt; \lim_{z \rightarrow b^-} \int_a^z f(x) \, dx&lt;/math&gt;
again being ''a'' and ''b''. For an [[improper integral]]
:&lt;math&gt; \int_a^\infty f(x) \, dx &lt;/math&gt;
or
:&lt;math&gt; \int_{-\infty}^b f(x) \, dx &lt;/math&gt;
the limits of integration are ''a'' and ∞, or &amp;minus;∞ and ''b'', respectively.

==See also==

* [[Integral]]
* [[Riemann integration]]

[[Category:Integral calculus]]
[[Category:Real analysis]]


{{mathanalysis-stub}}</text>
      <sha1>moc77715zhb7e6boda7s5g9216r10vu</sha1>
    </revision>
  </page>
  <page>
    <title>Linear independence</title>
    <ns>0</ns>
    <id>101863</id>
    <revision>
      <id>867901462</id>
      <parentid>867857342</parentid>
      <timestamp>2018-11-08T18:17:39Z</timestamp>
      <contributor>
        <username>Wcherowi</username>
        <id>13428914</id>
      </contributor>
      <comment>Reverted 3 edits by [[Special:Contributions/2601:900:8200:C399:E150:4771:8C36:D581|2601:900:8200:C399:E150:4771:8C36:D581]]: Too pedantic, all definitions are if and only if statements, it is not common to explicitly say it. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17509">&lt;!--{{technical|date=April 2014}}--&gt;
{{For|linear dependence of random variables|Covariance}}
[[File:Vec-indep.png|thumb|right|Linearly independent vectors in &lt;math&gt;\R^3&lt;/math&gt;]]
[[File:Vec-dep.png|thumb|right|Linearly dependent vectors in a plane in &lt;math&gt;\R^3&lt;/math&gt;.]]

In the theory of [[vector space]]s,  a [[set (mathematics)|set]] of [[vector (mathematics)|vector]]s is said to be '''{{visible anchor|linearly dependent}}''' if one of the vectors in the set can be defined as a [[linear combination]] of the others;  if no vector in the set can be written in this way, then the vectors are said to be '''{{visible anchor|linearly independent}}'''. These concepts are central to the definition of [[Dimension (vector space)|dimension]].&lt;ref&gt;G. E. Shilov, ''Linear Algebra'' (Trans. R. A. Silverman), Dover Publications, New York, 1977.&lt;/ref&gt;
&lt;!-- these distinctions are not useful
* An [[indexed family]] of [[vector space|vector]]s is a '''linearly independent family''' if none of them can be written as a [[linear combination]] of finitely many other vectors in the family. A family of vectors which is not linearly independent is called '''linearly dependent'''.
* A [[set (mathematics)|set]] of vectors is a '''linearly independent set''' if the set (regarded as a family indexed by itself) is a linearly independent family.

These two notions are not equivalent: the difference being that in a family we allow repeated elements, while in a set we do not.  For example if &lt;math&gt;V&lt;/math&gt; is a vector space, then the family &lt;math&gt;F : \{1,2\}\to V&lt;/math&gt; such that &lt;math&gt;f(1)=v&lt;/math&gt; and &lt;math&gt;f(2)=v&lt;/math&gt; is a ''linearly dependent family'', but the singleton set of the images of that family is &lt;math&gt;\{v\}&lt;/math&gt; which is a ''linearly independent set''.

Both notions are important and used in common, and sometimes even confused in the literature.
--&gt;
&lt;!-- this too early
For instance, in the [[3 dimensional space|three-dimensional]] [[real vector space]] &lt;math&gt;\R^3&lt;/math&gt; we have the following example:

:&lt;math&gt;
\begin{matrix}
\mbox{independent}\qquad\\
\underbrace{
  \overbrace{
    \begin{bmatrix}0\\0\\1\end{bmatrix},
    \begin{bmatrix}0\\2\\-2\end{bmatrix},
    \begin{bmatrix}1\\-2\\1\end{bmatrix}
  },
  \begin{bmatrix}4\\2\\3\end{bmatrix}
}\\
\mbox{dependent}\\
\end{matrix}
&lt;/math&gt;--&gt;&lt;!-- weights 9, 5, 4 
Here the first three vectors are linearly independent; but the fourth vector equals 9 times the first plus 5 times the second plus 4 times the third, so the four vectors together are linearly dependent. Linear dependence is a property of the set of vectors, not of any particular vector.  For example in this case we could just as well write the first vector as a linear combination of the last three.
:&lt;math&gt;\bold{v}_1=\left(-\frac{5}{9}\right)\bold{v}_2+\left(-\frac{4}{9}\right)\bold{v}_3+\frac{1}{9}\bold{v}_4 .&lt;/math&gt;
--&gt;
&lt;!-- In [[probability theory]] and [[statistics]] there is an unrelated measure of linear dependence between [[random variable]]s. --&gt;

A vector space can be of [[finite-dimension]] or [[dimension (vector space)|infinite-dimension]] depending on the number of linearly independent [[basis vectors]].  The definition of linear dependence and the ability to determine whether a subset of vectors in a vector space is linearly dependent are central to determining a [[Basis (linear algebra)|basis]] for a vector space.

== Definition ==
The vectors in a subset &lt;math&gt;S=\{\vec v_1,\vec v_2,\dots,\vec v_k\}&lt;/math&gt; of a [[vector space]] ''V'' are said to be ''linearly dependent'', if there exist scalars &lt;math&gt;a_1,a_2,\dots,a_k&lt;/math&gt; , not all zero, such that
:&lt;math&gt;a_1\vec v_1+a_2\vec v_2+\cdots+a_k\vec v_k= \vec 0,&lt;/math&gt;
where &lt;math&gt;\vec 0&lt;/math&gt; denotes the zero vector.

Notice that if not all of the scalars are zero, then at least one is non-zero, say &lt;math&gt;a_1&lt;/math&gt;, in which case this equation can be written in the form
:&lt;math&gt;\vec v_1=\frac{-a_2}{a_1}\vec v_2+\cdots+\frac{-a_k}{a_1}\vec v_k.&lt;/math&gt;
Thus, &lt;math&gt;\vec v_1&lt;/math&gt; is shown to be a linear combination of the remaining vectors.
&lt;!-- this is not helpful
For any vectors &lt;math&gt;\vec u_1,\vec u_2,\dots,\vec u_n,&lt;/math&gt; we have that
:&lt;math&gt;0 u_1+0 u_2+\cdots+0 u_n=0,&lt;/math&gt;
This is called the [[Triviality (mathematics)|trivial]] representation of 0 as a linear combination of &lt;math&gt;\vec u_1,\vec u_2,\dots,\vec u_n,&lt;/math&gt; , this motivates a very simple definition of both linear independence and linear dependence, for a set to be linearly dependent, there must exist a non-trivial representation of 0 as a linear combination of vectors in the set.
--&gt;

The vectors in a set &lt;math&gt;T=\{\vec v_1,\vec v_2,\dots,\vec v_n\}&lt;/math&gt; are said to be ''linearly independent'' if the equation
:&lt;math&gt;a_1\vec v_1+a_2\vec v_2+\cdots+a_n\vec v_n= \vec 0,&lt;/math&gt;
can only be satisfied by &lt;math&gt;a_i=0&lt;/math&gt; for &lt;math&gt;i=1,\dots,n&lt;/math&gt;. This implies that no vector in the set can be represented as a linear combination of the remaining vectors in the set.  In other words, a set of vectors is linearly independent if the only representations of &lt;math&gt;\vec 0&lt;/math&gt; as a linear combination of its vectors is the trivial representation in which all the scalars &lt;math&gt;a_i&lt;/math&gt; are zero.&lt;ref&gt;{{cite book|last=Friedberg, Insel, Spence|first=Stephen, Arnold, Lawrence|title=Linear Algebra|publisher=Pearson, 4th Edition|isbn=0130084514|pages=48–49}}&lt;/ref&gt;

The alternate definition, that a set of vectors is linearly dependent if and only if some vector in that set can be written as a linear combination of the other vectors, is only useful when the set contains two or more vectors. When the set contains zero or one vector, the original definition is used.

===Infinite dimensions===
In order to allow the number of linearly independent vectors in a vector space to be [[countably infinite]], it is useful to define linear dependence as follows. More generally, let ''V'' be a vector space over a [[field (mathematics)|field]] ''K'', and let {''v''&lt;sub&gt;''i''&lt;/sub&gt; | ''i''∈''I''} be a [[indexed family|family]] of elements of ''V''. The family is ''linearly dependent'' over ''K'' if there exists a family {''a''&lt;sub&gt;''j''&lt;/sub&gt; | ''j''∈''J''} of elements of ''K'', not all zero, such that
:&lt;math&gt; \sum_{j \in J} a_j v_j = 0 &lt;/math&gt;
where the index set ''J'' is a nonempty, finite subset of ''I''.

A set ''X'' of elements of ''V'' is ''linearly independent'' if the corresponding family {''x''}&lt;sub&gt;'''x'''∈''X''&lt;/sub&gt; is linearly independent.  Equivalently, a family is dependent if a member is in the closure of the [[linear span]] of the rest of the family, i.e., a member is a [[linear combination]] of the rest of the family.  The trivial case of the empty family must be regarded as linearly independent for theorems to apply.

A set of vectors which is linearly independent and [[linear span|spans]] some vector space, forms a [[basis (linear algebra)|basis]] for that vector space. For example, the vector space of all polynomials in ''x'' over the reals has the (infinite) subset {1, ''x'', ''x''&lt;sup&gt;2&lt;/sup&gt;, ...} as a basis.

== Geometric meaning ==

A geographic example may help to clarify the concept of linear independence.  A person describing the location of a certain place might say, "It is 3 miles north and 4 miles east of here."  This is sufficient information to describe the location, because the geographic coordinate system may be considered as a 2-dimensional vector space (ignoring altitude and the curvature of the Earth's surface).  The person might add, "The place is 5 miles northeast of here."  Although this last statement is ''true'', it is not necessary.

In this example the "3 miles north" vector and the "4 miles east" vector are linearly independent.  That is to say, the north vector cannot be described in terms of the east vector, and vice versa.  The third "5 miles northeast" vector is a [[linear combination]] of the other two vectors, and it makes the set of vectors ''linearly dependent'', that is, one of the three vectors is unnecessary.

Also note that if altitude is not ignored, it becomes necessary to add a third vector to the linearly independent set.  In general, ''n'' linearly independent vectors are required to describe all locations in ''n''-dimensional space.

== Evaluating linear independence ==

===Vectors in R&lt;sup&gt;2&lt;/sup&gt;===
'''Three vectors:'''  Consider the set of vectors ''v''&lt;sub&gt;1&lt;/sub&gt; = (1, 1), ''v''&lt;sub&gt;2&lt;/sub&gt; = (−3, 2) and ''v''&lt;sub&gt;3&lt;/sub&gt; = (2, 4), then the condition for linear dependence seeks a set of non-zero scalars, such that
::&lt;math&gt; a_1 \begin{Bmatrix} 1\\1\end{Bmatrix} + a_2 \begin{Bmatrix} -3\\2\end{Bmatrix} + a_3 \begin{Bmatrix} 2\\4\end{Bmatrix} =\begin{Bmatrix} 0\\0\end{Bmatrix},&lt;/math&gt;
or
::&lt;math&gt; \begin{bmatrix} 1 &amp; -3 &amp; 2 \\ 1 &amp; 2 &amp; 4 \end{bmatrix}\begin{Bmatrix} a_1\\ a_2 \\ a_3 \end{Bmatrix}= \begin{Bmatrix} 0\\0\end{Bmatrix}.&lt;/math&gt;

[[Row reduction|Row reduce]] this matrix equation by subtracting the first row from the second to obtain,
::&lt;math&gt; \begin{bmatrix} 1 &amp; -3 &amp; 2 \\ 0 &amp; 5 &amp; 2 \end{bmatrix}\begin{Bmatrix} a_1\\ a_2 \\ a_3 \end{Bmatrix}= \begin{Bmatrix} 0\\0\end{Bmatrix}.&lt;/math&gt;
Continue the row reduction by (i) dividing the second row by 5, and then (ii) multiplying by 3 and adding to the first row, that is
::&lt;math&gt; \begin{bmatrix} 1 &amp; 0 &amp; 16/5 \\ 0 &amp; 1 &amp; 2/5 \end{bmatrix}\begin{Bmatrix} a_1\\ a_2 \\ a_3 \end{Bmatrix}= \begin{Bmatrix} 0\\0\end{Bmatrix}.&lt;/math&gt;

We can now rearrange this equation to obtain 
::&lt;math&gt; \begin{bmatrix} 1 &amp; 0  \\ 0 &amp; 1 \end{bmatrix}\begin{Bmatrix} a_1\\ a_2 \end{Bmatrix}= \begin{Bmatrix} a_1\\ a_2 \end{Bmatrix}=-a_3\begin{Bmatrix} 16/5\\2/5\end{Bmatrix}.&lt;/math&gt;
which shows that non-zero ''a''&lt;sub&gt;''i''&lt;/sub&gt; exist such that ''v''&lt;sub&gt;3&lt;/sub&gt; = (2, 4) can be defined in terms of ''v''&lt;sub&gt;1&lt;/sub&gt; = (1, 1), ''v''&lt;sub&gt;2&lt;/sub&gt; = (−3, 2).  Thus, the three vectors are linearly dependent.

'''Two vectors:'''  Now consider the linear dependence of the two vectors ''v''&lt;sub&gt;1&lt;/sub&gt; = (1, 1), ''v''&lt;sub&gt;2&lt;/sub&gt; = (−3, 2), and check, 
::&lt;math&gt; a_1 \begin{Bmatrix} 1\\1\end{Bmatrix} + a_2 \begin{Bmatrix} -3\\2\end{Bmatrix}  =\begin{Bmatrix} 0\\0\end{Bmatrix},&lt;/math&gt;
or
::&lt;math&gt; \begin{bmatrix} 1 &amp; -3  \\ 1 &amp; 2  \end{bmatrix}\begin{Bmatrix} a_1\\ a_2 \end{Bmatrix}= \begin{Bmatrix} 0\\0\end{Bmatrix}.&lt;/math&gt;

The same row reduction presented above yields,
::&lt;math&gt; \begin{bmatrix} 1 &amp; 0  \\ 0 &amp; 1 \end{bmatrix}\begin{Bmatrix} a_1\\ a_2 \end{Bmatrix}= \begin{Bmatrix} 0\\0\end{Bmatrix}.&lt;/math&gt;
This shows that ''a''&lt;sub&gt;i&lt;/sub&gt; = 0, which means that the vectors ''v''&lt;sub&gt;1&lt;/sub&gt; = (1, 1) and ''v''&lt;sub&gt;2&lt;/sub&gt; = (−3, 2) are linearly independent.

===Vectors in R&lt;sup&gt;4&lt;/sup&gt;===
In order to determine if the three vectors in '''R'''&lt;sup&gt;4&lt;/sup&gt;,
::&lt;math&gt; \mathbf{v}_1= \begin{Bmatrix}1\\4\\2\\-3\end{Bmatrix},  \mathbf{v}_2=\begin{Bmatrix}7\\10\\-4\\-1\end{Bmatrix}, \mathbf{v}_3=\begin{Bmatrix}-2\\1\\5\\-4\end{Bmatrix}. &lt;/math&gt;
are linearly dependent, form the matrix equation,

::&lt;math&gt;\begin{bmatrix}1&amp;7&amp;-2\\4&amp; 10&amp; 1\\2&amp;-4&amp;5\\-3&amp;-1&amp;-4\end{bmatrix}\begin{Bmatrix} a_1\\ a_2 \\ a_3 \end{Bmatrix} =  \begin{Bmatrix}0\\0\\0\\0\end{Bmatrix}.&lt;/math&gt;

Row reduce this equation to obtain,
::&lt;math&gt; \begin{bmatrix} 1&amp; 7 &amp; -2 \\ 0&amp; -18&amp; 9\\ 0 &amp; 0 &amp; 0\\ 0&amp; 0&amp; 0\end{bmatrix} \begin{Bmatrix} a_1\\ a_2 \\ a_3 \end{Bmatrix} =  \begin{Bmatrix}0\\0\\0\\0\end{Bmatrix}.&lt;/math&gt;
Rearrange to solve for v&lt;sub&gt;3&lt;/sub&gt; and obtain,
::&lt;math&gt; \begin{bmatrix} 1&amp; 7 \\ 0&amp; -18&amp; \end{bmatrix} \begin{Bmatrix} a_1\\ a_2  \end{Bmatrix} =  -a_3\begin{Bmatrix}-2\\9\end{Bmatrix}.&lt;/math&gt;
This equation is easily solved to define non-zero ''a''&lt;sub&gt;i&lt;/sub&gt;,
::&lt;math&gt; a_1 = -3 a_3 /2,  a_2 = a_3/2,
&lt;/math&gt;
where ''a''&lt;sub&gt;3&lt;/sub&gt; can be chosen arbitrarily.  Thus, the vectors ''v''&lt;sub&gt;1&lt;/sub&gt;, ''v''&lt;sub&gt;2&lt;/sub&gt; and ''v''&lt;sub&gt;3&lt;/sub&gt; are linearly dependent.

=== Alternative method using determinants ===

An alternative method relies on the fact that ''n'' vectors in &lt;math&gt;\mathbb{R}^n&lt;/math&gt; are linearly '''independent''' [[if and only if]] the [[determinant]] of the [[matrix (mathematics)|matrix]] formed by taking the vectors as its columns is non-zero.

In this case, the matrix formed by the vectors is
:&lt;math&gt;A = \begin{bmatrix}1&amp;-3\\1&amp;2\end{bmatrix} . &lt;/math&gt;
We may write a linear combination of the columns as
:&lt;math&gt; A \Lambda = \begin{bmatrix}1&amp;-3\\1&amp;2\end{bmatrix} \begin{bmatrix}\lambda_1 \\ \lambda_2 \end{bmatrix} . &lt;/math&gt;
We are interested in whether ''A''Λ&amp;nbsp;= '''0''' for some nonzero vector Λ. This depends on the determinant of ''A'', which is
:&lt;math&gt; \det A = 1\cdot2 - 1\cdot(-3) = 5 \ne 0 . &lt;/math&gt;
Since the [[determinant]] is non-zero, the vectors (1, 1) and (&amp;minus;3, 2) are linearly independent.

Otherwise, suppose we have ''m'' vectors of ''n'' coordinates, with ''m''&amp;nbsp;&amp;lt;&amp;nbsp;''n''. Then ''A'' is an ''n''×''m'' matrix and Λ is a column vector with ''m'' entries, and we are again interested in ''A''Λ&amp;nbsp;= '''0'''. As we saw previously, this is equivalent to a list of ''n'' equations. Consider the first ''m'' rows of ''A'', the first ''m'' equations; any solution of the full list of equations must also be true of the reduced list. In fact, if 〈''i''&lt;sub&gt;1&lt;/sub&gt;,...,''i''&lt;sub&gt;''m''&lt;/sub&gt;〉 is any list of ''m'' rows, then the equation must be true for those rows.
:&lt;math&gt; A_{{\lang i_1,\dots,i_m} \rang} \Lambda = \bold{0} . &lt;/math&gt;
Furthermore, the reverse is true. That is, we can test whether the ''m'' vectors are linearly dependent by testing whether
:&lt;math&gt; \det A_{{\lang i_1,\dots,i_m} \rang} = 0 &lt;/math&gt;
for all possible lists of ''m'' rows. (In case ''m''&amp;nbsp;= ''n'', this requires only one determinant, as above. If ''m''&amp;nbsp;&amp;gt;&amp;nbsp;''n'', then it is a theorem that the vectors must be linearly dependent.) This fact is valuable for theory; in practical calculations more efficient methods are available.

===More vectors than dimensions===
If there are more vectors than dimensions, the vectors are linearly dependent. This is illustrated in the example above of three vectors in '''R'''&lt;sup&gt;2&lt;/sup&gt;.

== Natural basis vectors ==

Let ''V''&amp;nbsp;=&amp;nbsp;'''R'''&lt;sup&gt;''n''&lt;/sup&gt; and consider the following elements in ''V'', known as the [[Standard basis|natural basis]] vectors:

:&lt;math&gt;\begin{matrix}
\mathbf{e}_1 &amp; = &amp; (1,0,0,\ldots,0) \\
\mathbf{e}_2 &amp; = &amp; (0,1,0,\ldots,0) \\
&amp; \vdots \\
\mathbf{e}_n &amp; = &amp; (0,0,0,\ldots,1).\end{matrix}&lt;/math&gt;

Then '''e'''&lt;sub&gt;1&lt;/sub&gt;, '''e'''&lt;sub&gt;2&lt;/sub&gt;, ..., '''e&lt;sub&gt;n&lt;/sub&gt;''' are linearly independent.

=== Proof ===

Suppose that ''a''&lt;sub&gt;1&lt;/sub&gt;, ''a''&lt;sub&gt;2&lt;/sub&gt;, ..., ''a''&lt;sub&gt;''n''&lt;/sub&gt; are elements of '''R''' such that

:&lt;math&gt; a_1 \mathbf{e}_1 + a_2 \mathbf{e}_2 + \cdots + a_n \mathbf{e}_n = \mathbf{0} . &lt;/math&gt;

Since
:&lt;math&gt; a_1 \mathbf{e}_1 + a_2 \mathbf{e}_2 + \cdots + a_n \mathbf{e}_n = (a_1 ,a_2 ,\ldots, a_n) , &lt;/math&gt;

then ''a''&lt;sub&gt;''i''&lt;/sub&gt; = 0 for all ''i'' in {1, ..., ''n''}.

== Linear independence of basis functions ==

Let &lt;math&gt;V&lt;/math&gt; be the [[vector space]] of all differentiable [[function (mathematics)|function]]s of a real variable &lt;math&gt;t&lt;/math&gt;. Then the functions &lt;math&gt;e^t&lt;/math&gt; and &lt;math&gt;e^{2t}&lt;/math&gt; in &lt;math&gt;V&lt;/math&gt; are linearly independent.

=== Proof ===
Suppose &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; are two real numbers such that

:&lt;math&gt; ae ^ t + be ^ {2t} = 0 &lt;/math&gt;

Take the first derivative of the above equation such that

:&lt;math&gt; ae ^ t + 2be ^ {2t} = 0 &lt;/math&gt;

for ''all'' values of ''t''. We need to show that &lt;math&gt;a = 0&lt;/math&gt; and &lt;math&gt;b = 0 &lt;/math&gt;. In order to do this, we subtract the first equation from the second, giving &lt;math&gt; be ^ {2t} = 0 &lt;/math&gt;. Since &lt;math&gt; e^{2t} &lt;/math&gt; is strictly greater than zero (thus not equal to zero) for all ''t'', &lt;math&gt; b=0 &lt;/math&gt; for all ''t''. It follows that &lt;math&gt;a=0&lt;/math&gt; too. Therefore, according to the definition of linear independence, &lt;math&gt; e^{t} &lt;/math&gt; and &lt;math&gt; e^{2t} &lt;/math&gt; are linearly independent.

== Projective space of linear dependencies ==

A '''linear dependence''' among vectors '''v'''&lt;sub&gt;1&lt;/sub&gt;, ..., '''v'''&lt;sub&gt;''n''&lt;/sub&gt; is a [[tuple]] (''a''&lt;sub&gt;1&lt;/sub&gt;, ..., ''a''&lt;sub&gt;''n''&lt;/sub&gt;) with ''n'' [[scalar (mathematics)|scalar]] components, not all zero, such that

:&lt;math&gt;a_1 \mathbf{v}_1 + \cdots + a_n \mathbf{v}_n= \mathbf{0}. &lt;/math&gt;

If such a linear dependence exists, then the ''n'' vectors are linearly dependent. It makes sense to identify two linear dependencies if one arises as a non-zero multiple of the other, because in this case the two describe the same linear relationship among the vectors. Under this identification, the set of all linear dependencies among '''v'''&lt;sub&gt;1&lt;/sub&gt;, ...., '''v'''&lt;sub&gt;''n''&lt;/sub&gt; is a [[projective space]].

== See also ==
* [[Gramian matrix]]
* [[Matroid]]
* [[Orthogonality]]
* [[Wronskian]]
* [[Multicollinearity]]

== References ==
{{reflist}}

== External links ==
* {{springer|title=Linear independence|id=p/l059290}}
* [http://mathworld.wolfram.com/LinearlyDependentFunctions.html Linearly Dependent Functions] at WolframMathWorld.
* [http://people.revoledu.com/kardi/tutorial/LinearAlgebra/LinearlyIndependent.html Tutorial and interactive program] on Linear Independence.
* [https://www.khanacademy.org/math/linear-algebra/vectors_and_spaces/linear_independence/v/linear-algebra-introduction-to-linear-independence Introduction to Linear Independence] at KhanAcademy.

{{linear algebra}}

{{DEFAULTSORT:Linear Independence}}
[[Category:Abstract algebra]]
[[Category:Linear algebra]]
[[Category:Articles containing proofs]]</text>
      <sha1>iljo2dwkza9fgn5ipdlr0vpp0ey1gef</sha1>
    </revision>
  </page>
  <page>
    <title>Maximum principle</title>
    <ns>0</ns>
    <id>1436104</id>
    <revision>
      <id>790733990</id>
      <parentid>750078789</parentid>
      <timestamp>2017-07-15T19:08:33Z</timestamp>
      <contributor>
        <username>Deacon Vorbis</username>
        <id>29330520</id>
      </contributor>
      <minor/>
      <comment>/* The classical example */LaTeX spacing clean up, replaced: \,&lt;/math&gt; → &lt;/math&gt; (2) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4536">:''This article describes the maximum principle in the theory of partial differential equations. For the maximum principle in optimal control theory, see [[Pontryagin's maximum principle]].''

In [[mathematics]], the '''maximum principle''' is a property of solutions to certain [[partial differential equations]], of the [[elliptic partial differential equation|elliptic]] and [[parabolic partial differential equation|parabolic]] types. Roughly speaking, it says that the [[maximum]] of a function in a [[Domain (mathematical analysis)|domain]] is to be found on the boundary of that domain. Specifically, the ''strong'' maximum principle says that if a function achieves its maximum in the interior of the domain, the function is uniformly a constant. The ''weak'' maximum principle says that the maximum of the function is to be found on the boundary, but may re-occur in the interior as well. Other, even weaker maximum principles exist which merely bound a function in terms of its maximum on the boundary. 

In [[convex optimization]], the maximum principle states that the maximum of a [[convex function]] on a [[compact set|compact]] [[convex set]] is attained on the [[boundary (topology)|boundary]].&lt;ref&gt;Chapter 32 of [[R. Tyrrell Rockafellar|Rockafellar]] (1970).&lt;/ref&gt;

==The classical example==

[[Harmonic functions]] are the classical example to which the strong maximum principle applies. Formally, if ''f'' is a [[harmonic function]], then ''f'' cannot exhibit a true [[local maximum]] within the domain of definition of ''f''. In other words, either ''f'' is a [[constant function]], or, for any point &lt;math&gt;x_0&lt;/math&gt; inside the domain of ''f'', there exist other points arbitrarily close to &lt;math&gt;x_0&lt;/math&gt; at which ''f'' takes larger values.&lt;ref&gt;Berenstein and Gay.&lt;/ref&gt;

Let ''f'' be a harmonic function defined on some [[connected space|connected]] [[open set|open]] [[subset]] ''D'' of the [[Euclidean space]] '''R'''&lt;sup&gt;''n''&lt;/sup&gt;. If &lt;math&gt;x_0&lt;/math&gt; is a point in ''D'' such that 

:&lt;math&gt;f(x_0)\ge f(x) &lt;/math&gt; 

for all ''x'' in a [[neighborhood (topology)|neighborhood]] of &lt;math&gt;x_0&lt;/math&gt;, then the function ''f'' is constant on ''D''.

By replacing "maximum" with "minimum" and "larger" with "smaller", one obtains the '''minimum principle''' for harmonic functions.

The maximum principle also holds for the more general [[subharmonic function]]s, while [[superharmonic function]]s satisfy the minimum principle.&lt;ref&gt;Evans.&lt;/ref&gt;

=== Heuristics for the proof ===

The ''weak maximum principle'' for harmonic functions is a simple consequence of facts from calculus. The key ingredient for the proof is the fact that, by the definition of a harmonic function, the [[Laplacian]] of ''f'' is zero. Then, if &lt;math&gt;x_0&lt;/math&gt; is a non-degenerate [[critical point (mathematics)|critical point]] of ''f''(''x''), we must be seeing a [[saddle point]], since otherwise there is no chance that the sum of the second derivatives of ''f'' is zero. This of course is not a complete proof, and we left out the case of &lt;math&gt;x_0&lt;/math&gt; being a degenerate point, but this is the essential idea. 

The ''strong maximum principle'' relies on the [[Hopf lemma]], and this is more complicated.

== See also ==

* [[Maximum modulus principle]]
* [[Hopf maximum principle]]

==References==
{{reflist}}
*{{cite book |title=Complex Variables: An Introduction|last=Berenstein |first=Carlos A. |author2=Roger Gay| year=1997|publisher=Springer (Graduate Texts in Mathematics)| isbn=0-387-97349-4}}
*{{cite book |title=Fully Nonlinear Elliptic Equations |last=Caffarelli |first=Luis A. |authorlink=Luis Caffarelli |author2=Xavier Cabre |year=1995 |publisher=American Mathematical Society |location=Providence, Rhode Island |pages=31–41 |isbn=0-8218-0437-5}}
*{{cite book |title=Partial Differential Equations |last=Evans |first=Lawrence C. |authorlink=Lawrence C. Evans|year=1998|publisher=American Mathematical Society|location=Providence, Rhode Island|isbn=0-8218-0772-2}}
* {{cite book
  | last = Rockafellar
  | first = R. T.|authorlink=R. Tyrrell Rockafellar
  | title = Convex analysis
  | publisher = Princeton University Press
  | date = 1970
  | location = Princeton
}}
*{{Cite book|first1=D.|last=Gilbarg|first2=Neil|last2=Trudinger|authorlink2=Neil Trudinger|title=Elliptic Partial Differential Equations of Second Order|publisher=Springer|publication-place=New York|year=1983|isbn=3-540-41160-7}}

[[Category:Harmonic functions]]
[[Category:Partial differential equations]]
[[Category:Mathematical principles]]</text>
      <sha1>a7fppj14uarhtw2t3wd5qw3ymqbvxlj</sha1>
    </revision>
  </page>
  <page>
    <title>N-monoid</title>
    <ns>0</ns>
    <id>3980101</id>
    <revision>
      <id>810390156</id>
      <parentid>810061428</parentid>
      <timestamp>2017-11-14T23:35:28Z</timestamp>
      <contributor>
        <username>Siddharthist</username>
        <id>28122572</id>
      </contributor>
      <comment>Add further reading, note that this is a definition of a strict n-monoid</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="711">{{DISPLAYTITLE:''n''-monoid}}
In [[category theory]], a '''(strict) ''n''-monoid''' is an [[n-category|''n''-category]] with only one 0-cell. In particular, a 1-monoid is a [[monoid]] and a 2-monoid is a [[monoidal category|strict monoidal category]].

==References==
*{{cite book
 | author = Albert Burroni
 | year = 1993
 | title = Higher dimensional word problems with applications to equational logic
 | publisher = [[Theoretical Computer Science (journal)|Theoretical Computer Science]]
 | url = http://www.pps.univ-paris-diderot.fr/~burroni/mapage/highwordpb.pdf
}}
==Further reading==
* {{nlab|id=n-monoid|title=n-monoid}}

{{Category theory}}
{{categorytheory-stub}}

[[Category:Higher category theory]]</text>
      <sha1>mxeuv5i1nqvcu5lojzuvyswvqoyxytc</sha1>
    </revision>
  </page>
  <page>
    <title>National Museum of Mathematics</title>
    <ns>0</ns>
    <id>35783995</id>
    <revision>
      <id>869353410</id>
      <parentid>869353368</parentid>
      <timestamp>2018-11-18T02:10:46Z</timestamp>
      <contributor>
        <username>Epicgenius</username>
        <id>17859592</id>
      </contributor>
      <comment>unnecessary</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12590">{{Infobox museum
| name             = National Museum of Mathematics&lt;br&gt;(MoMath)
| native_name      = 
| native_name_lang = 
|logo              = National Museum of Mathematics logo.svg
| image            = File:National Museum of Mathematics 11 East 26th Street entrance.jpg
| imagesize        = 300px
| caption          = Entrance
| alt              = 
| map_type         =
| map_caption      = 
| map_alt          = 
| map_size         =
| coordinates      = {{coord|40.74371|-73.9883|type:landmark_globe:earth_region:US-NY|display=title,inline|format=dms}}
| established      = {{Start date|2009|11|17}} ''(chartered)''
| dissolved        = 
| location         = 11 East 26th Street&lt;br&gt;[[Manhattan]], [[New York City]]
| type             = 
| collection       = 
| visitors         = 
| director         = Cindy Lawrence
| president        = 
| curator          = 
| owner            = 
| publictransit    = '''[[New York City Subway]]:''' 
*{{NYCS trains|Broadway center local day|time=bullets}} to [[23rd Street (BMT Broadway Line)|23rd Street]] or [[28th Street (BMT Broadway Line)|28th Street]] 
*{{NYCS trains|Lexington local day|time=bullets}} to [[23rd Street (IRT Lexington Avenue Line)|23rd Street]] or [[28th Street (IRT Lexington Avenue Line)|28th Street]]
*{{NYCS trains|Sixth local|time=bullets}} to [[23rd Street (IND Sixth Avenue Line)|23rd Street]]
'''[[Port Authority Trans-Hudson]]:''' [[HOB-33]], [[JSQ-33 (via HOB)]], or [[JSQ-33]] to [[23rd Street (PATH station)|23rd Street]]
'''[[MTA Regional Bus Operations|MTA New York City Bus]]:''' {{NYC bus link|M1|M2|M3|M55|M7|M20}}
| car_park         = 
| network          = 
| website          = {{url|http://momath.org/}}
}}
The '''National Museum of Mathematics'''  or '''MoMath'''&lt;ref&gt;{{citation |author=Ralph Gardner Jr. |date=2011-03-09 |title=Making Math Fun (Seriously) |publisher=Wall Street Journal}}&lt;/ref&gt; is a [[museum]] dedicated to [[mathematics]] in [[Manhattan]], [[New York City]].&lt;ref name="CBS News - Mo Rocca - MoMath"&gt;{{cite news|last=Rocca|first=Mo|title=A new museum devoted to math|url=http://www.cbsnews.com/8301-3445_162-57572198/a-new-museum-devoted-to-math/|accessdate=4 March 2013|newspaper=[[CBS News]]|date=2013-03-04}}&lt;/ref&gt;&lt;ref name="NYT - Opening the Doors"&gt;{{cite news|last=Rothstein|first=Edward|title=Opening the Doors to the Life of Pi|url=https://www.nytimes.com/2012/12/14/arts/design/museum-of-mathematics-at-madison-square-park.html?smid=pl-share&amp;_r=0|accessdate=4 March 2013|newspaper=[[The New York Times]]|date=2012-12-13}}&lt;/ref&gt;
It opened on December 15, 2012.  It is located at 11 [[26th Street (Manhattan)|East 26th Street]] between [[Fifth Avenue (Manhattan)|Fifth]] and [[Madison Avenue (Manhattan)|Madison]] Avenues, across from [[Madison Square Park]] in the [[NoMad]] neighborhood.  It is the only museum dedicated to mathematics in [[North America]],&lt;ref name="Bloomberg - MoMath - Heidi Klum"&gt;{{cite news|last=Keene|first=Tom|title=New York's Coolest New Museum, Starring Heidi Klum|url=https://www.bloomberg.com/video/new-york-s-coolest-new-museum-starring-heidi-klum-FeAnmwShQ82K_tkIzChNmg.html|accessdate=4 March 2013|newspaper=[[Bloomberg L.P.|Bloomberg]]|date=2013-02-22}}&lt;/ref&gt; and features over thirty [[interactive]] exhibits.&lt;ref&gt;{{cite news | url=http://www.dnainfo.com/new-york/20110418/midtown/new-math-museum-hopes-make-numbers-fun-for-kids | location=New York | publisher=Dnainfo.com | title=new-Math Museum Hopes To Make Numbers Fun For Kids | first=Jill | last=Colvin | accessdate=2009-04-11 | deadurl=yes | archiveurl=https://web.archive.org/web/20130926165146/http://www.dnainfo.com/new-york/20110418/midtown/new-math-museum-hopes-make-numbers-fun-for-kids | archivedate=2013-09-26 | df= }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://observer.com/2012/12/momath-no-problems-north-americas-only-math-museum-now-open-in-madison-square/ |title=MoMath No Problems: North America’s Only Math Museum Now Open in Madison Square |publisher=New York Observer |accessdate=December 17, 2012 |author=Kit Dillon}}&lt;/ref&gt; The mission of the museum is to "enhance public understanding and perception of mathematics".&lt;ref name=about&gt;{{cite web|url=https://momath.org/about/ |title=About – The Museum of Mathematics |publisher=Momath.org |date=2009-11-17 |accessdate=2013-11-17}}&lt;/ref&gt; The museum is known for a special [[tricycle]] with [[square wheel]]s, which operates smoothly on a [[catenary]] surface.&lt;ref&gt;{{cite web|url= https://mathenchant.wordpress.com/2015/07/15/the-lessons-of-a-square-wheeled-trike/|title= The Lessons of a Square-Wheeled Trike|date=2015-07-15|website=Mathematic Enchantments}}&lt;/ref&gt;

==History==
In 2006 the [[Goudreau Museum of Mathematics in Art and Science|Goudreau Museum]] on [[Long Island, New York|Long Island]], at the time the only museum in the United States dedicated to mathematics, closed its doors.&lt;ref&gt;{{cite web|url=http://www.mathmuseum.org/ |title=Goudreau Museum of Mathematics in Art and Science |publisher=The Math Museum |date= |accessdate=2013-11-17}}&lt;/ref&gt; In response, a group led by founder Glen Whitney met to explore the opening of a new museum. They received a charter from the [[New York State Department of Education]] in 2009,&lt;ref name=about /&gt; and raised over 22 million dollars in under four years.&lt;ref&gt;{{cite news|url=https://www.nytimes.com/2011/06/28/science/28math.html | location=New York | work=New York Times | title= One Math Museum, Many Variables| first=Kenneth | last=Chang | date=2011-06-27}}&lt;/ref&gt; 

With this funding, a {{convert|19,000|sqft|m2}} space was leased in the Goddard Building at 11-13 East 26th Street, located in the [[Madison Square North Historic District]]. Despite some opposition to the architectural plans within the local community,&lt;ref&gt;{{cite news | url=http://www.dnainfo.com/new-york/20120126/murray-hill-gramercy/landmarks-commission-gives-thumbs-up-momath-facade-plans | location=New York | publisher=Dnainfo.com | title=Landmarks Commission Gives Thumbs Up to MoMath Facade Plans | first=Mary | last=Johnson | date=2012-01-26 | deadurl=yes | archiveurl=https://web.archive.org/web/20130605104618/http://www.dnainfo.com/new-york/20120126/murray-hill-gramercy/landmarks-commission-gives-thumbs-up-momath-facade-plans | archivedate=2013-06-05 | df= }}&lt;/ref&gt; permission for construction was granted by the [[New York City Landmarks Preservation Commission]] and the [[New York City Department of Buildings|Department of Buildings]].

==Programs==
* '''Math Midway''' is a traveling exhibition of math-based interactive displays. The exhibits include a square-wheeled tricycle that travels smoothly over an undulating [[cycloidal]] track; the ''Ring of Fire'', which uses lasers to intersect three-dimensional objects with a two-dimensional plane to uncover interesting shapes; and an "organ function grinder" which allows users to create their own [[Function (mathematics)|mathematical function]]s and see the results.&lt;ref&gt;{{citation |author=Andrea Christie Elkin |date=February 2012 |publisher=Nctm.org |title=Math Carnival Time}}&lt;/ref&gt; After making its debut at the [[World Science Festival]] in 2009, Math Midway traveled the country, reaching more than a half million visitors.  The Midway's schedule included stops in New York, Pennsylvania, Texas, California, New Jersey, Ohio, Maryland, Florida, Indiana, and Oregon.   In 2016, the Math Midway exhibit was sold to the [[Science Centre Singapore]].{{cn|date=March 2017}}
* '''Math Midway 2 Go''' (MM2GO) is a spinoff of Math Midway. MM2GO includes six of the most popular Math Midway Exhibits. MM2GO began traveling to science festivals, schools, community centers, and libraries in the autumn of 2012.&lt;ref&gt;{{cite web|url=http://mathmidway.org/mm2go/ |title=Math Midway 2 Go - Interactive Mathematics Exhibition - Museum of Mathematics |publisher=Mathmidway.org |date= |accessdate=2013-11-17}}&lt;/ref&gt;
* '''Math Encounters''' is a monthly speaker series presented by the Museum of Math and the [[Simons Foundation]].&lt;ref&gt;{{cite web|url=http://mrhonner.com/2012/01/05/math-encounters-craig-kaplan-on-math-and-art/ |title=Math Encounters: Craig Kaplan on Math and Art « Mr Honner |publisher=Mrhonner.com |date=2012-01-05 |accessdate=2013-11-17}}&lt;/ref&gt; The lectures initially took place at [[Baruch College]] in Manhattan on the first Wednesday of each month, but moved to MoMath's visitor center at 11 East 26th Street in March, 2013. Every month a different mathematician is invited to deliver a lecture. Lecturers have included [[Google]]'s Director of Research [[Peter Norvig]], journalist [[Paul Hoffman (science writer)|Paul Hoffman]], and computer scientist Craig Kaplan. Examples of topics are "The Geometry of Origami", "The Patterns of Juggling", and "Mathematical Morsels from The Simpsons and Futurama".&lt;ref&gt;{{cite news | location=New York | work= Examiner | title= Museum of Mathematics prepares for future home| first=Jennifer | last=Eberhart | date=2012-03-20}}&lt;/ref&gt; The lectures are meant to be accessible and engaging for high school students and adults. The first lecture occurred on March 3, 2011. Twenty unique lectures had been delivered {{as of|2012|12|lc=y}}.&lt;ref&gt;{{cite web|url=http://momath.org/home/math-encounters/ |title=Math Encounters – The Museum of Mathematics |publisher=Momath.org |date=2013-07-31 |accessdate=2013-11-17}}&lt;/ref&gt;
* '''Family Fridays''' began in April 2014 and occur once a month.  MoMath and [[Time Warner Cable]] launched the initiative to provide free mathematical opportunities to low-income families in the form of an event series with new activities and presentations each month.&lt;ref&gt;{{cite news|title=National Museum of Mathematics and Time Warner Cable Launch Family Fridays Partnership to Provide Engaging Mathematical Programming to Low-Income Families|url=http://www.virtual-strategy.com/2014/04/08/national-museum-mathematics-and-time-warner-cable-launch-family-fridays-partnership-provi|accessdate=3 May 2014|newspaper=Virtual-Strategy Magazine|date=8 April 2014|deadurl=yes|archiveurl=https://web.archive.org/web/20140503184914/http://www.virtual-strategy.com/2014/04/08/national-museum-mathematics-and-time-warner-cable-launch-family-fridays-partnership-provi|archivedate=3 May 2014|df=}}&lt;/ref&gt;  In 2017, the sponsorship was taken over by Two Sigma.

==Exhibits==
In October 2016, the  exhibit ''The Insides of Things: The Art of Miguel Berrocal'' was opened, displaying a collection of [[mechanical puzzle|puzzle sculptures]] by Spanish artist [[Miguel Ortiz Berrocal]] (1933-2006), donated by the late Samuel Sensiper. Each sculpture can be disassembled into small interlocking pieces, eventually revealing a small piece of jewelry or other surprise.&lt;ref name="PRW"&gt;{{cite web|title=Museum's Newest Art/Math Exhibit Showcases Spanish Abstract Sculptor Miguel Berrocal|url=http://www.prweb.com/releases/2016/10/prweb13741605.htm|website=PRWeb|accessdate=2017-02-25}}&lt;/ref&gt;

==Visiting professorship==
On August 2, 2018 MoMath announced the creation of a Distinguished Chair for the Public Dissemination of Mathematics.  Princeton professor and [[Fields Medal]] winner [[Manjul Bhargava]] was named as the first recipient of this position.&lt;ref&gt;[https://www.prweb.com/releases/momath_announces_first_distinguished_chair_for_the_public_dissemination_of_mathematics_first_visiting_professorship_in_u_s_dedicated_to_raising_public_awareness_of_math/prweb15669001.htm 	 MoMath Announces First Distinguished Chair for the Public Dissemination of Mathematics] The National Museum of Mathematics, August 2, 2018&lt;/ref&gt;

[[File:Museum of Mathematics-New York-Square Wheeled Tricycles.jpg|thumb|400px|center|&lt;div align="center"&gt;[[Square wheel]]ed tricycles at MoMath&lt;/div&gt;]]

{{clear}}

==See also==
* ''[[Mathematica: A World of Numbers... and Beyond]]'' – a classic exhibit of mathematical concepts, organized by [[Ray and Charles Eames]]
* [[Science tourism]]
* [[Square wheel]]

==References==
{{Reflist|30em}}

== External links ==
{{Commons category|Museum of Mathematics}}
*{{official website|http://www.momath.org/}}
*{{Twitter}}
*[http://www.mathencounters.org/ Math Encounters]
*[http://www.MathMidway.org/ Math Midway]

{{Museums in Manhattan|state=collapsed}}
{{Areas of mathematics|state=collapsed}}
{{Mathematics and art}}

[[Category:Museums in Manhattan]]
[[Category:Children's museums in New York City]]
[[Category:Science museums in New York (state)]]
[[Category:2009 establishments in New York (state)]]
[[Category:Midtown Manhattan]]
[[Category:Mathematics education]]
[[Category:Association of Science-Technology Centers member institutions]]</text>
      <sha1>r7q9tfhaausggl82sme8bqfavxmhlhy</sha1>
    </revision>
  </page>
  <page>
    <title>Nullable type</title>
    <ns>0</ns>
    <id>6084375</id>
    <revision>
      <id>863401278</id>
      <parentid>863400405</parentid>
      <timestamp>2018-10-10T15:21:42Z</timestamp>
      <contributor>
        <ip>212.34.73.12</ip>
      </contributor>
      <comment>/* Language support */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6178">{{Distinguish|nullable symbol}}
{{Refimprove|date=March 2009}}
In [[programming language|programming]], '''nullable types''' are a feature of the [[type system]] of some [[programming languages]] which allow the value to be set to the special value '''NULL''' instead of the usual possible values of the [[data type]]. In statically-typed languages, a nullable type is an [[option type]] (in functional programming terms), while in dynamically-typed languages (where values have types, but variables do not), equivalent behavior is provided by having a single null value.

[[Primitive type]]s such as [[integer]]s and [[boolean datatype|boolean]]s cannot generally be null, but the corresponding nullable types (nullable integer and nullable boolean, respectively) can also assume the NULL value. NULL is frequently used to represent a missing value or invalid value, such as from a function that failed to return or a missing field in a database, as in [[Null (SQL)|NULL]] in [[SQL]].

==Example==
An integer variable may represent integers, but 0 (zero) is a special case because 0 in many programming languages can mean "false". Also this doesn't give us any notion of saying that the variable is empty, a need for which occurs in many circumstances. This need can be achieved with a nullable type. In programming languages like [[C Sharp (programming language)|C#]] 2.0, a nullable integer, for example, can be declared by a question mark (int? x).&lt;ref name="msdn"&gt;{{cite web|url=http://msdn.microsoft.com/en-us/library/1t3y8s4s(VS.80).aspx |title=Nullable Types (C#) |publisher=Msdn.microsoft.com |date= |accessdate=2013-08-19}}&lt;/ref&gt;  In programming languages like [[C Sharp (programming language)|C#]] 1.0, nullable types can be defined by an external library&lt;ref name="sourceforge"&gt;{{cite web|url=http://nullabletypes.sourceforge.net/ |title=(luKa) Developer Zone - NullableTypes |publisher=Nullabletypes.sourceforge.net |date= |accessdate=2013-08-19}}&lt;/ref&gt; as new types (e.g. NullableInteger, NullableBoolean).&lt;ref name="onlinedoc"&gt;{{cite web|url=http://nullabletypes.sourceforge.net/onlinedoc-v1.2/ |title=NullableTypes |publisher=Nullabletypes.sourceforge.net |date= |accessdate=2013-08-19}}&lt;/ref&gt;

A boolean variable makes the effect more clear. Its values can be either "true" or "false", while a nullable boolean may also contain a representation for "undecided". However, the interpretation or treatment of a logical operation involving such a variable depends on the language.

==Compared with null pointers==
In contrast, object [[Pointer (computer programming)|pointers]] can be set to [[Null pointer|NULL]] by default in most common languages, meaning that the pointer or reference points to nowhere, that no object is assigned (the variable does not point to any object).
Nullable references were invented by [[C. A. R. Hoare]] in 1965 as part of the [[Algol W]] language.  Hoare later described their invention as a "billion-dollar mistake".&lt;ref&gt;{{cite web
|url=http://www.infoq.com/presentations/Null-References-The-Billion-Dollar-Mistake-Tony-Hoare
|title=Null References: The Billion Dollar Mistake
|location=QCon London
|year=2009
|author=Tony Hoare}}&lt;/ref&gt;  This is because object pointers that can be NULL require the user to check the pointer before using it and require specific code to handle the case when the object pointer is NULL. &lt;!-- Not really true about Objective-C: In some languages, like [[Objective-C]],&lt;ref name="Objective-C messaging"&gt;{{cite web|url=http://developer.apple.com/library/ios/documentation/cocoa/conceptual/ProgrammingWithObjectiveC/WorkingwithObjects/WorkingwithObjects.html#//apple_ref/doc/uid/TP40011210-CH4-SW22 |title=Programming with Objective-C: Working with Objects |publisher=Developer.apple.com |date=2012-12-13 |accessdate=2013-08-19}}&lt;/ref&gt; however, NULL object pointers can be used without problems. --&gt;

[[Java (programming language)|Java]] has classes that correspond to scalar values, such as Integer, Boolean and Float.  Combined with [[autoboxing]] (automatic usage-driven conversion between object and value), this effectively allows nullable variables for scalar values.

==Compared with option types==
Nullable type implementations usually adhere to the [[null object pattern]].

There is a more general and formal concept that extend the nullable type concept, it comes from [[option type]]s, which enforce explicit handling of the exceptional case.
Option type implementations usually adhere to the Special Case pattern.&lt;ref&gt;{{cite web|url=http://martinfowler.com/eaaCatalog/specialCase.html |title=P of EAA: Special Case |publisher=Martinfowler.com |date= |accessdate=2013-08-19}}&lt;/ref&gt;

==Language support==
The following programming languages support nullable types.

Statically typed languages with native null support include:
* [[Kotlin (programming language)|Kotlin]]  &lt;ref&gt;{{cite web |url=https://kotlinlang.org/docs/reference/null-safety.html |title=Null Safety - Kotlin Programming Language }}&lt;/ref&gt;
* [[Ceylon_(programming_language)|Ceylon]]
* [[SQL]]
* [[SAS_(software)|SAS]] (Missing values)

Statically typed languages with library null support include:
* [[C_Sharp_(programming_language)|C#]] (since version 2)
* [[Visual_Basic_.NET | VB.NET]]
* [[Java (programming language)|Java]] (since version 8)
* [[Swift (programming language)|Swift]]
* [[Scala_(programming_language)|Scala]]
* [[Oxygene (programming language)|Oxygene]]
* [[F_Sharp_(programming_language)|F#]]
* Statically typed [[List of CLI languages|CLI languages]]
Dynamically-typed languages with null include:
* [[Perl]] scalar variables default to &lt;code&gt;undef&lt;/code&gt; and can be set to &lt;code&gt;undef&lt;/code&gt;.
* PHP with NULL type and is_null() method, native nullable type in version 7.1 &lt;ref&gt;https://wiki.php.net/rfc/nullable_types&lt;/ref&gt;
* Python has the &lt;code&gt;None&lt;/code&gt; value.&lt;ref&gt;https://docs.python.org/3/library/constants.html#None&lt;/ref&gt;
* Ruby with nil value and NilClass type.
* [[JavaScript]] has a &lt;code&gt;null&lt;/code&gt; value

==See also==
* [[Null coalescing operator]]
* [[Option type]]
* [[Semipredicate problem]]
* [[Union type]]
* [[Unit type]]

==References==
{{Reflist}}

{{nulls}}

[[Category:Type theory]]</text>
      <sha1>6lsdb9ff9uw479a4emd7v6ke8invekc</sha1>
    </revision>
  </page>
  <page>
    <title>Ostrowski Prize</title>
    <ns>0</ns>
    <id>3159411</id>
    <revision>
      <id>802329456</id>
      <parentid>802328225</parentid>
      <timestamp>2017-09-25T13:51:42Z</timestamp>
      <contributor>
        <username>Jarszick</username>
        <id>1107800</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2301">The '''Ostrowski Prize''' is a [[mathematics]] [[award]] given every odd year for outstanding mathematical achievement judged by an international [[jury]] from the universities of [[University of Basel|Basel]], [[University of Jerusalem|Jerusalem]], [[University of waterloo|Waterloo]] and the academies of [[Danish Academy of Science|Denmark]] and the [[Royal Netherlands Academy of Arts and Sciences|Netherlands]]. [[Alexander Ostrowski]], a longtime professor at the [[University of Basel]], left his estate to the foundation in order to establish a prize for outstanding achievements in pure [[mathematics]] and the foundations of numerical [[mathematics]]. It currently carries a monetary award of 100,000 [[Swiss franc|Swiss]] [[franc]]s.

Its recipients are:	
* '''1989:'''	[[Louis de Branges]]    (France / United States)
* '''1991:'''	[[Jean Bourgain]]    (Belgium)
* '''1993:'''	[[Miklós Laczkovich]] (Hungary) and [[Marina Ratner]]    (Russia / United States)
* '''1995:'''	[[Andrew J. Wiles]]    (UK)
* '''1997:'''	[[Yuri Valentinovich Nesterenko|Yuri V. Nesterenko]] (Russia) and [[Gilles I. Pisier]]    (France)
* '''1999:'''	[[Alexander A. Beilinson]]    (Russia / United States) and [[Helmut Hofer|Helmut H. Hofer]]    (Switzerland / United States)
* '''2001:'''	[[Henryk Iwaniec]]    ([[Poland]] / United States) and [[Peter Sarnak]] ([[South Africa]] / United States) and [[Richard Taylor (mathematician)|Richard L. Taylor]] (UK / United States)
* '''2003:'''	[[Paul Seymour (mathematician)|Paul Seymour]]    (UK)
* '''2005:'''	[[Ben Green (mathematician)|Ben Green]]    (UK) and [[Terence Tao]]    (Australia / United States)
* '''2007:'''   [[Oded Schramm]]   (Israel / United States)
* '''2009:'''   [[Sorin Popa]]   ([[Romania]] / United States) 
* '''2011:'''   [[Ib Madsen]]   (Denmark), [[David Preiss]] (UK) and [[Kannan Soundararajan]] (India / United States) 
* '''2013:'''   [[Yitang Zhang]] (United States) 
* '''2015:'''   [[Peter Scholze]] (Germany) &lt;ref&gt;[http://www.ostrowski.ch/index_e.php?ifile=home Ostrowski Prize - Home Page]&lt;/ref&gt;
* '''2017:'''   [[Akshay Venkatesh]] (India / Australia) &lt;ref&gt;[https://ostrowski.ch/pdf/preis2017.pdf Ostrowski Prize 2017]&lt;/ref&gt;

==References==
{{reflist}}

[[Category:Mathematics awards]]
[[Category:Awards established in 1989]]</text>
      <sha1>3f9k4b6srdufh8rtd4scy93mszrs5f1</sha1>
    </revision>
  </page>
  <page>
    <title>Outline of logic</title>
    <ns>0</ns>
    <id>6306271</id>
    <revision>
      <id>857235731</id>
      <parentid>846945338</parentid>
      <timestamp>2018-08-30T12:25:07Z</timestamp>
      <contributor>
        <username>RussBot</username>
        <id>279219</id>
      </contributor>
      <minor/>
      <comment>Robot: fix [[WP:DPL|links]] to [[WP:D|disambiguation]] page [[Validity]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="22703">
'''[[Logic]]'''  is the formal science of using [[reason]] and is considered a branch of both [[philosophy]] and [[mathematics]]. Logic investigates and classifies the structure of statements and arguments, both through the study of [[formal system]]s of inference and the study of arguments in [[natural language]]. The scope of logic can therefore be very large, ranging from core topics such as the study of [[fallacies]] and [[paradox]]es, to specialized analyses of reasoning such as [[probability]], correct reasoning, and arguments involving [[causality]]. One of the aims of logic is to identify the correct (or [[Validity (logic)|valid]]) and incorrect (or [[fallacy|fallacious]]) [[inference]]s. Logicians study the criteria for the evaluation of [[logical argument|arguments]].

== Foundations of logic ==

[[Philosophy of logic]]
* [[Analytic-synthetic distinction]]
* [[Antinomy]]
* [[A priori and a posteriori]]
* [[Definition]]
* [[Description]]
* [[Entailment]]
* [[Identity (philosophy)]]
* [[Inference]]
* [[Logical form]]
* [[Logical implication]]
* [[Logical truth]]
* [[Logical consequence]]
* [[Name]]
* [[Necessary and sufficient condition|Necessity]]
* [[Material conditional]]
* [[Meaning (linguistic)]]
* [[Meaning (non-linguistic)]]
* [[Paradox]] &amp;nbsp;([[List of paradoxes|list]])
* [[Possible world]]
* [[Presupposition]]
* [[Probability]]
* [[Quantification (logic)|Quantification]]
* [[Reason]]
* [[Reasoning]]
* [[Reference]]
* [[Semantics]]
* [[Strict conditional]]
* [[Syntax (logic)]]
* [[Truth]]
* [[Truth value]]
* [[Validity (logic)|Validity]]

== Philosophical logic ==
[[Philosophical logic]]

=== Informal logic and critical thinking ===
[[Informal logic]]
[[Critical thinking]]
[[Argumentation theory]]
* [[Argument]]
* [[Argument map]]
* [[Accuracy and precision]]
* [[Ad hoc hypothesis]]
* [[Ambiguity]]
* [[Analysis]]
* [[Attacking Faulty Reasoning]]
* [[Belief]]
* [[Belief bias]]
* [[Bias]]
* [[Cogency]]
* [[Cognitive bias]]
* [[Confirmation bias]]
* [[Credibility]]
* [[Critical pedagogy]]
* [[Critical reading]]
* [[Decidophobia]]
* [[Decision making]]
* [[Dispositional and occurrent belief]]
* [[Emotional reasoning]]
* [[Evidence]]
* [[Expert]]
* [[Explanation]]
* [[Explanatory power]]
* [[Fact]]
* [[Fallacy]]
* [[Higher-order thinking]]
* [[Inquiry]]
* [[Interpretive discussion]]
* [[Narrative logic]]
* [[Occam's razor]]
* [[Opinion]]
* [[Practical syllogism]]
* [[Precision questioning]]
* [[Propaganda]]
* [[Propaganda techniques]]
* [[Prudence]]
* [[Pseudophilosophy]]
* [[Reasoning]]
* [[Relevance]]
* [[Rhetoric]]
* [[Rigour]]
* [[Socratic questioning]]
* [[Source credibility]]
* [[Source criticism]]
* [[Theory of justification]]
* [[Topical logic]]
* [[Vagueness]]

=== Deductive reasoning ===

==== Theories of deduction ====
*[[Anti-psychologism]]
*[[Conceptualism]]
*[[Constructivist epistemology|Constructivism]]
*[[Conventionalism]]
*[[Counterpart theory]]
*[[Deflationary theory of truth]]
*[[Dialetheism]]
*[[Fictionalism]]
*[[Formalism (philosophy)]]
*[[Game theory]]
*[[Illuminationist philosophy]]
*[[Logical atomism]]
*[[Logical holism]]
*[[Logicism]]
*[[Modal fictionalism]]
*[[Nominalism]]
*[[Object theory]]
*[[Polylogism]]
*[[Pragmatism]]
*[[Preintuitionism]]
*[[Proof theory]]
*[[Psychologism]]
*[[Ramism]]
*[[Semantic theory of truth]]
*[[Sophism]]
*[[Trivialism]]
*[[Ultrafinitism]]

=== Fallacies ===

* [[Fallacy]] &amp;nbsp;([[List of fallacies|list]]) &amp;ndash; incorrect argumentation in reasoning resulting in a misconception or presumption. By accident or design, fallacies may exploit emotional triggers in the listener or interlocutor (appeal to emotion), or take advantage of social relationships between people (e.g. argument from authority). Fallacious arguments are often structured using rhetorical patterns that obscure any logical argument. Fallacies can be used to win arguments regardless of the merits.  There are dozens of types of fallacies.

== Formal logic ==

* [[Formal logic]] &amp;ndash; Mathematical logic, symbolic logic and formal logic are largely, if not completely synonymous. The essential feature of this field is the use of [[formal language]]s to express the ideas whose logical validity is being studied.
** [[List of mathematical logic topics]]

=== Symbols and strings of symbols ===

==== Logical symbols ====
{{Main|Table of logic symbols|Symbol (formal)}}
*[[Variable (mathematics)|Logical variables]]
**[[Propositional variable]]
**[[Predicate variable]]
**[[Literal (mathematical logic)|Literal]]
**[[Metavariable]]
*[[Logical constant]]s
**[[Logical connective]]
**[[Quantification (logic)|Quantifier]]
**[[Identity (mathematics)|Identity]]
**[[Bracket (mathematics)|Brackets]]

===== Logical connectives =====
[[Logical connective]]
* [[Converse implication]]
* [[Converse nonimplication]]
* [[Exclusive or]]
* [[Logical NOR]]
* [[Logical biconditional]]
* [[Logical conjunction]]
* [[Logical disjunction]]
* [[Material conditional|Material implication]]
* [[Material nonimplication]]
* [[Negation]]
* [[Sheffer stroke]]
{{Logical connectives}}

==== Strings of symbols ====
{{Main|Well-formed formula}}
*[[Atomic formula]]
*[[Open sentence]]

==== Types of propositions ====

[[Proposition]]
* [[Analytic proposition]]
* [[Axiom]]
* [[Atomic sentence]]
* [[Clause (logic)]]
* [[Contingency (philosophy)|Contingent proposition]]
* [[Contradiction]]
* [[Logical truth]]
* [[Propositional formula]]
* [[Rule of inference]]
* [[Sentence (mathematical logic)]]
* [[Sequent]]
* [[Statement (logic)]]
* [[Tautology (logic)|Tautology]]
* [[Theorem]]

===== Rules of inference =====

[[Rule of inference]] &amp;nbsp;([[List of rules of inference|list]])
* [[Biconditional elimination]]
* [[Biconditional introduction]]
* [[Proof by cases|Case analysis]]
* [[Commutativity of conjunction]]
* [[Conjunction introduction]]
* [[Constructive dilemma]]
* [[Contraposition (traditional logic)]]
* [[Conversion (logic)]]
* [[De Morgan's laws]]
* [[Destructive dilemma]]
* [[Disjunction elimination]]
* [[Disjunction introduction]]
* [[Disjunctive syllogism]]
* [[Double negative elimination]]
* [[Generalization (logic)]]
* [[Hypothetical syllogism]]
* [[Law of excluded middle]]
* [[Law of identity]]
* [[Modus ponendo tollens]]
* [[Modus ponens]]
* [[Modus tollens]]
* [[Obversion]]
* [[Principle of contradiction]]
* [[Resolution (logic)]]
* [[Conjunction elimination|Simplification]]
* [[Transposition (logic)]]

==== Formal theories ====
{{Main|Theory (mathematical logic)}}
*[[Formal proof]]
*[[List of first-order theories]]

==== Expressions in an object language ====

[[Object language]]
* [[Symbol (formal)|Symbol]]
* [[Well-formed formula|Formula]]
* [[Formal system]]
* [[Theorem]]
* [[Formal proof]]
* [[Theory]]

==== Expressions in a metalanguage ====

[[Metalanguage]]
* [[Metalinguistic variable]]
* [[Deductive system]]
* [[Metatheorem]]
* [[Metatheory]]
* [[Interpretation (logic)|Interpretation]]

=== Propositional and boolean logic ===

==== Propositional logic ====

[[Propositional logic]]
* [[Absorption law]]
* [[Clause (logic)]]
* [[Deductive closure]]
* [[Entailment]]
* [[Formation rule]]
* [[Functional completeness]]
* [[Intermediate logic]]
* [[Literal (mathematical logic)]]
* [[Logical connective]]
* [[Logical consequence]]
* [[Negation normal form]]
* [[Open sentence]]
* [[Propositional calculus]]
* [[Propositional formula]]
* [[Propositional variable]]
* [[Rule of inference]]
* [[Strict conditional]]
* [[Substitution instance]]
* [[Truth table]]
* [[Zeroth-order logic]]

==== Boolean logic ====
* [[Boolean algebra]] &amp;nbsp; ([[List of Boolean algebra topics|list]])
* [[Boolean algebra (logic)|Boolean logic]]
* [[Boolean algebra (structure)]]
* [[Boolean algebras canonically defined]]
* [[Introduction to Boolean algebra]]
* [[Complete Boolean algebra]]
* [[Free Boolean algebra]]
* [[Monadic Boolean algebra]]
* [[Residuated Boolean algebra]]
* [[Two-element Boolean algebra]]
* [[Modal algebra]]
* [[Derivative algebra (abstract algebra)]]
* [[Relation algebra]]
* [[Absorption law]]
* [[Laws of Form]]
* [[De Morgan's laws]]
* [[Algebraic normal form]]
* [[Canonical form (Boolean algebra)]]
* [[Boolean conjunctive query]]
* [[Boolean-valued model]]
* [[Boolean domain]]
* [[Boolean expression]]
* [[Boolean ring]]
* [[Boolean function]]
* [[Boolean-valued function]]
* [[Parity function]]
* [[Symmetric Boolean function]]
* [[Conditioned disjunction]]
* [[Field of sets]]
* [[Functional completeness]]
* [[Implicant]]
* [[Logic alphabet]]
* [[Logic redundancy]]
* [[Logical connective]]
* [[Logical matrix]]
* [[Minimal negation operator]]
* [[Product term]]
* [[True quantified Boolean formula]]
* [[Truth table]]

=== Predicate logic and relations ===

==== Predicate logic ====

[[Predicate logic]]

* [[Atomic formula]]
* [[Atomic sentence]]
* [[Domain of discourse]]
* [[Empty domain]]
* [[Extension (predicate logic)]]
* [[First-order logic]]
* [[First-order predicate]]
* [[Formation rule]]
* [[Free variables and bound variables]]
* [[Generalization (logic)]]
* [[Monadic predicate calculus]]
* [[Predicate (mathematical logic)]]
* [[Predicate logic]]
* [[Predicate variable]]
* [[Quantification (logic)|Quantification]]
* [[Second-order predicate]]
* [[Sentence (mathematical logic)]]
* [[Universal instantiation]]

==== Relations ====

[[Mathematical relation]]
* [[Finitary relation]]
* [[Antisymmetric relation]]
* [[Asymmetric relation]]
* [[Bijection]]
* [[Bijection, injection and surjection]]
* [[Binary relation]]
* [[Composition of relations]]
* [[Concurrent relation]]
* [[Congruence relation]]
* [[Converse relation]]
* [[Coreflexive relation]]
* [[Covering relation]]
* [[Cyclic order]]
* [[Dense relation]]
* [[Dependence relation]]
* [[Dependency relation]]
* [[Directed set]]
* [[Equivalence relation]]
* [[Euclidean relation]]
* [[Homogeneous relation]]
* [[Idempotence]]
* [[Intransitivity]]
* [[Involutive relation]]
* [[Partial equivalence relation]]
* [[Partial function]]
* [[Partially ordered set]]
* [[Preorder]]
* [[Prewellordering]]
* [[Propositional function]]
* [[Quasitransitive relation]]
* [[Reflexive relation]]
* [[Surjective function]]
* [[Symmetric relation]]
* [[Ternary relation]]
* [[Total relation]]
* [[Transitive relation]]
* [[Trichotomy (mathematics)]]
* [[Well-founded relation]]

== Mathematical logic ==
[[Mathematical logic]]

=== Set theory ===
[[Set theory]] &amp;nbsp;([[List of set theory topics|list]])
* [[Aleph null]]
* [[Bijection, injection and surjection]]
* [[Binary set]]
* [[Cantor's diagonal argument]]
* [[Cantor's first uncountability proof]]
* [[Cantor's theorem]]
* [[Cardinality of the continuum]]
* [[Cardinal number]]
* [[Codomain]]
* [[Complement (set theory)]]
* [[Constructible universe]]
* [[Continuum hypothesis]]
* [[Countable set]]
* [[Decidable set]]
* [[Denumerable set]]
* [[Disjoint sets]]
* [[Disjoint union]]
* [[Domain of a function]]
* [[Effective enumeration]]
* [[Element (mathematics)]]
* [[Empty function]]
* [[Empty set]]
* [[Enumeration]]
* [[Extensionality]]
* [[Finite set]] 
* [[Forcing (mathematics)]]
* [[Function (set theory)]]
* [[Function composition]]
* [[Generalized continuum hypothesis]]
* [[Index set]]
* [[Infinite set]]
* [[Intension]]
* [[Intersection (set theory)]]
* [[Inverse function]]
* [[Large cardinal]]
* [[Löwenheim–Skolem theorem]]
* [[Map (mathematics)]]
* [[Multiset]]
* [[Morse–Kelley set theory]]
* [[Naïve set theory]]
* [[Non-Cantorian set theory]]
* [[One-to-one correspondence]]
* [[Ordered pair]]
* [[Partition of a set]]
* [[Pointed set]]
* [[Power set]]
* [[Projection (set theory)]]
* [[Proper subset]]
* [[Proper superset]]
* [[Range (mathematics)]]
* [[Russell's paradox]]
* [[Sequence (mathematics)]]
* [[Set (mathematics)]]
* [[Set of all sets]]
* [[Simple theorems in the algebra of sets]]
* [[Singleton (mathematics)]]
* [[Skolem paradox]]
* [[Subset]]
* [[Superset]]
* [[Tuple]]
* [[Uncountable set]]
* [[Union (set theory)]]
* [[Von Neumann–Bernays–Gödel set theory]]
* [[Zermelo set theory]]
* [[Zermelo–Fraenkel set theory]]

=== Metalogic ===
[[Metalogic]] &amp;ndash; The study of the [[metatheory]] of logic.
* [[Completeness (logic)]]
* [[Syntax (logic)]]
* [[Consistency]]
* [[Decidability (logic)]]
* [[Deductive system]]
* [[Interpretation (logic)]]
* [[Cantor's theorem]]
* [[Church's theorem]]
* [[Church's thesis]]
* [[Effective method]]
* [[Formal system]]
* [[Gödel's completeness theorem]]
* [[Gödel's first incompleteness theorem]]
* [[Gödel's second incompleteness theorem]]
* [[Independence (mathematical logic)]]
* [[Logical consequence]]
* [[Löwenheim–Skolem theorem]]
* [[Metalanguage]]
* [[Metasyntactic variable]]
* [[Metatheorem]]
* [[Object language]]
* [[Symbol (formal)]]
* [[Type–token distinction]]
* [[Use–mention distinction]]
* [[Well-formed formula]]

==== Proof theory ====
[[Proof theory]] &amp;ndash; The study of [[deductive apparatus]].
* [[Axiom]]
* [[Deductive system]]
* [[Formal proof]]
* [[Formal system]]
* [[Formal theorem]]
* [[Syntactic consequence]]
* [[Syntax (logic)]]
* [[Transformation rules]]

==== Model theory ====
[[Model theory]] &amp;ndash; The study of interpretation of formal systems.
* [[Interpretation (logic)]]
* [[Logical validity]]
* [[Non-standard model]]
* [[Normal model]]
* [[Structure (mathematical logic)|Model]]
* [[Semantic consequence]]
* [[Truth value]]

=== Computability theory ===
[[Computability theory]] &amp;ndash; branch of mathematical logic that originated in the 1930s with the study of computable functions and [[Turing degree]]s. The field has grown to include the study of generalized computability and definability. The basic questions addressed by recursion theory are "What does it mean for a function from the natural numbers to themselves to be computable?" and "How can noncomputable functions be classified into a hierarchy based on their level of noncomputability?". The answers to these questions have led to a rich theory that is still being actively researched.
* [[Alpha recursion theory]]
* [[Arithmetical set]]
* [[Church–Turing thesis]]
* [[Computability logic]]
* [[Computable function]]
* [[Computation]]
* [[Decision problem]]
* [[Effective method]]
* [[Entscheidungsproblem]]
* [[Enumeration]]
* [[Forcing (recursion theory)]]
* [[Halting problem]]
* [[History of the Church–Turing thesis]]
* [[Lambda calculus]]
* [[List of undecidable problems]]
* [[Post correspondence problem]]
* [[Post's theorem]]
* [[Primitive recursive function]]
* [[Recursion (computer science)]]
* [[Recursive language]]
* [[Recursive languages and sets]]
* [[Recursive set]]
* [[Recursively enumerable language]]
* [[Recursively enumerable set]]
* [[Reduction (recursion theory)]]
* [[Turing machine]]

== Classical logic ==
[[Classical logic]]
* Properties of classical logics:
** [[Law of the excluded middle]]
** [[Double negative elimination]]
** [[Law of noncontradiction]]
** [[Principle of explosion]]
** [[Monotonicity of entailment]]
** [[Idempotency of entailment]]
** [[Commutativity of conjunction]]
** [[De Morgan duality]] &amp;ndash; every [[logical operator]] is dual to another
* [[Term logic]]
* General concepts in classical logic
** [[Baralipton]]
** [[Baroco]]
** [[Bivalence]]
** [[Boolean logic]]
** [[Boolean-valued function]]
** [[Categorical proposition]]
** [[Distribution of terms]]
** [[End term]]
** [[Enthymeme]]
** [[Immediate inference]]
** [[Law of contraries]]
** [[Logical connective]]
** [[Major term]]
** [[Middle term]]
** [[Minor term]]
** [[Organon]]
** [[Polysyllogism]]
** [[Port-Royal Logic]]
** [[Premise]]
** [[Prior Analytics]]
** [[Relative term]]
** [[Sorites paradox]]
** [[Square of opposition]]
** [[Sum of Logic]]
** [[Syllogism]]
** [[Tetralemma]]
** [[Truth function]]

== Non-classical logic ==
[[Non-classical logic]]

* [[Affine logic]]
* [[Bunched logic]]
* [[Computability logic]]
* [[Decision theory]]
* [[Description logic]]
* [[Deviant logic]]
* [[Free logic]]
* [[Fuzzy logic]]
* [[Game theory]]
* [[Intensional logic]]
* [[Intuitionistic logic]]
* [[Linear logic]]
* [[Many-valued logic]]
* [[Minimal logic]]
* [[Non-monotonic logic]]
* [[Noncommutative logic]]
* [[Paraconsistent logic]]
* [[Probability theory]]
* [[Quantum logic]]
* [[Relevance logic]]
* [[Strict logic]]
* [[Substructural logic]]

=== Modal logic ===
[[Modal logic]]
* [[Alethic logic]]
* [[Axiological logic]]
* [[Deontic logic]]
* [[Doxastic logic]]
* [[Epistemic logic]]
* [[Temporal logic]]

== Concepts of logic ==
* [[Deductive reasoning]] –
* [[Inductive reasoning]] –
* [[Abductive reasoning]] –
[[Mathematical logic]] –
* [[Proof theory]] –
* [[Set theory]] –
* [[Formal system]] –
** [[Predicate logic]] –
*** [[Predicate (logic)|Predicate]] –
*** [[Higher-order logic]] –
** [[Propositional calculus]] –
*** [[Proposition]] –
* [[Boolean algebra]] –
** [[Boolean algebra (logic)|Boolean logic]] –
** [[Truth value]] –
** [[Venn diagram]] –
** [[Pierce's law]] –
* [[Aristotelian logic]] –
* [[Non-Aristotelian logic]] –
* [[Informal logic]] –
* [[Fuzzy logic]] –
* [[Infinitary logic]] –
** [[Infinity]] –
* [[Categorical logic]] –
* [[College logic]] –
* [[Linear logic]] –
* [[Metalogic]] –
* [[order (logic)|order]] –
* [[ordered logic (linear logic)|Ordered logic]] –
* [[Temporal logic]] –
* [[Sequential logic]] –
* [[Provability logic]] –
** [[Interpretability logic]] –
*** [[Interpretability]] –
* [[Quantum logic]] –
* [[Relevant logic]] –
* [[Consequent]] –
* [[Affirming the consequent]] –
* [[Antecedent (logic)|Antecedent]] –
* [[Denying the antecedent]] –
* [[Theorem]] –
* [[Axiom]] –
* [[Axiomatic system]] –
* [[Axiomatization]] –
* [[Conditional proof]] –
* [[Invalid proof]] –
* [[Degree of truth]] –
* [[Truth]] –
* [[Truth condition]] –
* [[Truth function]] –
* [[Double negative]] –
** [[Double negative elimination]] –
* [[Fallacy]] –
** [[Existential fallacy]] –
** [[Informal fallacy|Logical fallacy]] –
** [[Syllogistic fallacy]] –
* [[Type theory]] –
* [[Game theory]] –
* [[Game semantics]] –
* [[Rule of inference]] –
* [[Inference procedure]] –
* [[Inference rule]] –Quantification
* [[Introduction rule]] –
* [[Law of excluded middle]] –
* [[Law of non-contradiction]] –
* [[Logical constant]] –
** [[Logical connective]] –
** [[Quantification (logic)|Quantifier]] –
* [[Logic gate]] –
** [[Boolean Function]] –
* [[Tautology (logic)|Tautology]] –
* [[Logical assertion]] –
* [[Logical conditional]] –
* [[Logical biconditional]] –
* [[Logical equivalence]] –
* [[Logical AND]] –
* [[Negation]] –
* [[Logical OR]] –
* [[Logical NAND]] –
* [[Logical NOR]] –
* [[Contradiction]] –
* [[Logicism]] –
* [[Polysyllogism]] –
* [[Syllogism]] –
* [[Hypothetical syllogism]] –
* [[Major premise]] –
* [[Minor premise]] –
* [[First-order logic#Formation rules|Term]] –
* [[Singular term]] –
* [[Major term]] –
* [[Middle term]] –
* [[Quantification (logic)|Quantification]] –
* [[Plural quantification]] –
* [[Logical argument]] –
** [[Validity (logic)|Validity]] –
** [[Soundness]] –
* [[Inverse (logic)]] –
* [[Non sequitur (logic)|Non sequitur]] –
* [[Tolerance (in logic)|Tolerance]] –
* [[Satisfiability]] –
* [[Engineered language#Logical languages|Logical language]] –
* [[Paradox]] –
* [[Polish notation]] –
* [[Principia Mathematica]] –
* [[Q.E.D.|Quod erat demonstrandum]] –
* [[Reductio ad absurdum]] –
* [[Rhetoric]] –
* [[Self-reference]] –
* [[Necessary and sufficient]] –
* [[Sufficient condition]] –
* [[Nonfirstorderizability]] –
* [[Occam's Razor]] –
* [[Socratic dialoge]] –
* [[Socratic method]] –
* [[Argument form]] –
* [[Logic programming]] –
* [[Unification (computing)|Unification]] –

== History of logic ==

[[History of logic]]

== Literature about logic ==
=== Journals ===
* ''[[Journal of Logic, Language and Information]]''
* ''[[Journal of Philosophical Logic]]''
* ''[[Linguistics and Philosophy]]''

=== Books ===
* ''[[A System of Logic]]''
* ''[[Attacking Faulty Reasoning]]''
* ''[[Begriffsschrift]]''
* ''[[Categories (Aristotle)]]''
* ''[[Charles Sanders Peirce bibliography]]''
* ''[[De Interpretatione]]''
* ''[[Gödel, Escher, Bach]]''
* ''[[Introduction to Mathematical Philosophy]]''
* ''[[Language, Truth, and Logic]]''
* ''[[Laws of Form]]''
* ''[[Novum Organum]]''
* ''[[On Formally Undecidable Propositions of Principia Mathematica and Related Systems]]''
* ''[[Organon]]''
* ''[[Philosophy of Arithmetic (book)|Philosophy of Arithmetic]]''
* ''[[Polish Logic]]''
* ''[[Port-Royal Logic]]''
* ''[[Posterior Analytics]]''
* ''[[Principia Mathematica]]''
* ''[[Principles of Mathematical Logic]]''
* ''[[Prior Analytics]]''
* ''[[Rhetoric (Aristotle)]]''
* ''[[Sophistical Refutations]]''
* ''[[Sum of Logic]]''
* ''[[The Art of Being Right]]''
* ''[[The Foundations of Arithmetic]]''
* ''[[Topics (Aristotle)|Topics]]'' (Aristotle)
* ''[[Tractatus Logico-Philosophicus]]''

== Logic organizations ==

* [[Association for Symbolic Logic]]

==Logicians==

* [[List of logicians]]
* [[List of philosophers of language]]

== See also ==
{{Portal|Logic}}
* [[Index of logic articles]]
* [[Mathematics]]
** [[List of basic mathematics topics]]
** [[List of mathematics articles]]
* [[Philosophy]]
** [[List of basic philosophy topics]]
** [[List of philosophy topics]]
* [[Outline of discrete mathematics]] &amp;ndash; for introductory set theory and other supporting material

==External links==
{{sisterlinks|Logic}}
* [http://www.fallacyfiles.org/taxonomy.html ''Taxonomy of Logical Fallacies'']
* ''[http://www.galilean-library.org/int4.html An Introduction to Philosophical Logic]'', by Paul Newall, aimed at beginners
* ''[http://www.fecundity.com/logic/ forall x: an introduction to formal logic]'', by [[P.D. Magnus]], covers sentential and quantified logic
* ''[http://www.earlham.edu/~peters/courses/log/transtip.htm Translation Tips]'', by Peter Suber, for translating from English into logical notation
* [http://etext.lib.virginia.edu/DicHist/analytic/anaVII.html Math &amp; Logic: The history of formal mathematical, logical, linguistic and methodological ideas.] In ''The Dictionary of the History of Ideas.''
* ''[http://www.think-logically.co.uk/lt.htm Logic test]'' Test your logic skills
* ''[http://kpaprzycka.swps.edu.pl/xLogicSelfTaught/LogicSelfTaught.html Logic Self-Taught: A Workbook]''  (originally prepared for on-line logic instruction)

{{logic}}

{{DEFAULTSORT:Logic}}

[[Category:Logic| ]]
[[Category:Wikipedia outlines|logic]]
[[Category:Philosophy-related outlines|Logic]]
[[Category:Mathematical logic]]
[[Category:Mathematics-related lists|Logic]]</text>
      <sha1>pglk7a3arc9ui6b34x3wfs5frmr3hun</sha1>
    </revision>
  </page>
  <page>
    <title>Post–Turing machine</title>
    <ns>0</ns>
    <id>3688147</id>
    <revision>
      <id>838310553</id>
      <parentid>838273907</parentid>
      <timestamp>2018-04-26T06:13:34Z</timestamp>
      <contributor>
        <username>Omnipaedista</username>
        <id>8524693</id>
      </contributor>
      <comment>see MOS:SECTIONORDER</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="30418">:''The article [[Turing machine]] gives a general introduction to Turing machines, while this article covers a specific class of Turing machines.''

A '''Post–Turing machine'''&lt;ref&gt;Rajendra Kumar, ''Theory of Automata'', Tata McGraw-Hill Education, 2010, p. 343.&lt;/ref&gt; is a "program formulation" of an especially simple type of [[Turing machine]], comprising a variant of [[Emil Post]]'s [[Turing completeness|Turing-equivalent]] [[model (abstract)|model]] of [[computation]] described below.  (Post's model and Turing's model, though very similar to one another, were developed independently. Turing's paper was received for publication in May 1936, followed by Post's in October.)  A Post–Turing machine uses a [[Binary numeral system|binary alphabet]], an [[Infinite set|infinite]] [[sequence]] of binary [[computer storage|storage]] locations, and a primitive [[programming language]] with instructions for bi-directional movement among the storage locations and alteration of their contents one at a time.  The names "Post–Turing program" and "Post–Turing machine" were used by [[Martin Davis]] in 1973–1974 (Davis 1973, p.&amp;nbsp;69ff). Later in 1980, Davis used the name "Turing–Post program" (Davis, in Steen p.&amp;nbsp;241).

== 1936: Post model ==
In his 1936 paper "Finite Combinatory Processes&amp;mdash;Formulation 1" (which can be found on page 289 of ''Undecidable''), [[Emil Post]] described a model of extreme simplicity which he conjectured is "[[logically equivalent]] to [[recursion (computer science)|recursiveness]]", and which was later proved to be so.  The quotes in the following are from this paper.

Post's model of a computation differs from the Turing-machine model in a further "atomization" of the acts a human "computer" would perform during a computation.{{cref|

Post's model employs a "[[mathematical logic|symbol]] space" consisting of a "two-way infinite sequence of spaces or boxes", each box capable of being in either of two possible conditions, namely "marked" (as by a single vertical stroke) and "unmarked" (empty).  Initially, [[finite set|finitely]]-many of the boxes are marked, the rest being unmarked. A "worker" is then to move among the boxes, being in and operating in only one box at a time, according to a fixed finite "set of directions" ([[instruction (computer science)|instructions]]), which are numbered in order (1,2,3,...,n).  Beginning at a box "singled out as the starting point", the worker is to follow the set of instructions one at a time, beginning with instruction 1.

The instructions may require the worker to perform the following "basic acts" or "[[operation (mathematics)|operations]]":

:(a)  ''Marking the box he is in (assumed empty),''
:(b)  ''Erasing the mark in the box he is in (assumed marked),''
:(c)  ''Moving to the box on his right,''
:(d)  ''Moving to the box on his left,''
:(e)  ''Determining whether the box he is in, is or is not marked.''

Specifically, the ''i'' &lt;sup&gt;th&lt;/sup&gt; "direction" (instruction) given to the worker is to be one of the following forms:

: (A) ''Perform operation O&lt;sub&gt;i&lt;/sub&gt;'' [''O&lt;sub&gt;i&lt;/sub&gt;'' = (a), (b), (c) ''or'' (d)] ''and then follow direction j&lt;sub&gt;i&lt;/sub&gt;'',{{clarify|date=September 2017}}
: (B) ''Perform operation'' (e) ''and according as the answer is yes or no correspondingly follow direction j&lt;sub&gt;i&lt;/sub&gt;' or j&lt;sub&gt;i&lt;/sub&gt;' ' '',{{clarify|date=September 2017}}
: (C) ''Stop''.

(The above indented text and italics are as in the original.) Post remarks that this formulation is "in its initial stages" of development, and mentions several possibilities for "greater flexibility" in its final "definitive form", including
: (1) replacing the infinity of boxes by a finite extensible symbol space, "extending the primitive operations to allow for the necessary extension of the given finite symbol space as the process proceeds",
: (2) using an alphabet of more than two symbols, "having more than one way to mark a box",
: (3) introducing finitely-many "physical objects to serve as pointers, which the worker can identify and move from box to box".

== 1947: Post's formal reduction of the Turing 5-tuples to 4-tuples  ==

As briefly mentioned in the article [[Turing machine]], Post, in his paper of 1947 (''Recursive Unsolvability of a Problem of Thue'') atomized the Turing 5-tuples to 4-tuples:
:"Our quadruplets are quintuplets in the Turing development. That is, where our standard instruction orders either a printing (overprinting) '''or''' motion, left or right, Turing's standard instruction always order a printing '''and''' a motion, right, left, or none" (footnote 12, ''Undecidable'', p. 300)

Like Turing he defined erasure as printing a symbol "S0". And so his model admitted  quadruplets of only three types (cf. ''Undecidable'', p.&amp;nbsp;294):
: ''q''&lt;sub&gt;''i''&lt;/sub&gt; ''S''&lt;sub&gt;''j''&lt;/sub&gt; ''L'' ''q''&lt;sub&gt;''l''&lt;/sub&gt;,
: ''q''&lt;sub&gt;''i''&lt;/sub&gt; ''S''&lt;sub&gt;''j''&lt;/sub&gt; ''R'' ''q''&lt;sub&gt;''l''&lt;/sub&gt;,
: ''q''&lt;sub&gt;''i''&lt;/sub&gt; ''S''&lt;sub&gt;''j''&lt;/sub&gt; ''S''&lt;sub&gt;''k''&lt;/sub&gt; ''q''&lt;sub&gt;''l''&lt;/sub&gt;

At this time he was still retaining the Turing state-machine convention – he had not formalized the notion of an assumed ''sequential'' execution of steps until a specific test of a symbol "branched" the execution elsewhere.

== 1954, 1957: Wang model ==
For an even further reduction – to only four instructions – of the Wang model presented here see [[Wang B-machine]].

Wang (1957, but presented to the ACM in 1954) is often cited (cf. Minsky (1967), p.&amp;nbsp;200) as the source of the "program formulation" of binary-tape Turing machines using numbered instructions from the set

: write 0
: write 1
: move left
: move right
: if scanning 0 then goto instruction ''i''
: if scanning 1 then goto instruction ''j''

where ''sequential [[execution (computers)|execution]]'' is assumed, and Post's single "[[Conditional (programming)|if ... then ... else]]" has been "atomised" into two "if ... then ..." statements.  (Here '1' and '0' are used where Wang used "marked" and "unmarked", respectively, and the initial tape is assumed to contain only '0's except for finitely-many '1's.)

Wang noted the following: 
* "Since there is no separate instruction for halt (stop), it is understood that the machine will stop when it has arrived at a stage that the program contains no instruction telling the machine what to do next." (p.&amp;nbsp;65)
* "In contrast with Turing who uses a one-way infinite tape that has a beginning, we are following Post in the use of a 2-way infinite tape." (p.&amp;nbsp;65)
* Unconditional gotos are easily derived from the above instructions, so "we can freely use them too". (p.&amp;nbsp;84)

Any binary-tape Turing machine is readily converted to an equivalent "Wang program" using the above instructions. 

== 1974: first Davis model ==
Martin Davis was an [[undergraduate]] student of Emil Post's. Along with [[Stephen Kleene]] he completed his PhD under [[Alonzo Church]] (Davis (2000) 1st and 2nd footnotes p.&amp;nbsp;188). 

The following model he presented in a series of lectures to the Courant Institute at NYU in 1973–1974. This is the model to which Davis formally applied the name "Post–Turing machine" with its "Post–Turing language".&lt;ref&gt;In his chapter XIII ''Computable Functions'', Kleene adopts the Post model; Kleene's model uses a blank and one symbol "tally mark &amp;#164;" (Kleene p. 358), a "treatment closer in some respects to Post 1936. Post 1936 considered computation with a 2-way infinite tape and only 1 symbol" (Kleene p. 361). Kleene observes that Post's treatment provided a further reduction to "atomic acts" (Kleene p. 357) of "the Turing act" (Kleene p. 379). As described by Kleene "The Turing act" is the combined 3 (time-sequential) actions specified on a line in a Turing table: (i) print-symbol/erase/do-nothing followed by (ii) move-tape-left/move-tape-right/do-nothing followed by (iii) test-tape-go-to-next-instruction: e.g. "s1Rq1" means "Print symbol "&amp;#164;", then move tape right, then if tape symbol is "&amp;#164;" then go to state q1". (See Kleene's example p. 358.)

Kleene observes that Post atomized these 3-actions further into two types of 2-actions. The first type is a "print/erase" action, the second is a "move tape left/right action": (1.i) print-symbol/erase/do-nothing followed by (1.ii) test-tape-go-to-next-instruction, OR (2.ii) move-tape-left/move-tape-right/do-nothing followed by (2.ii) test-tape-go-to-next-instruction. 

But Kleene observes that while
:"Indeed it could be argued that the Turing machine act is already compound, and consists psychologically in a printing and change in state of mind, followed by a motion and another state of mind [, and] Post 1947 does thus separate the Turing act into two; we have not here, primarily because it saves space in the machine tables not to do so."(Kleene p. 379)

In fact Post's treatment (1936) is ambiguous; both (1.1) and (2.1) could be followed by "(.ii) go to next instruction in numerical sequence". This represents a further atomization into three types of instructions: (1) print-symbol/erase/do-nothing then go-to-next-instruction-in-numerical-sequence, (2) move-tape-left/move-tape-right/do-nothing then go-to-next-instruction-in-numerical-sequence (3) test-tape then go-to-instruction-xxx-else-go-to-next-instruction-in-numerical-sequence.&lt;/ref&gt; The instructions are assumed to be executed sequentially (Davis 1974, p.&amp;nbsp;71):

: "Write 1
: "Write B
: "To A if read 1
: "To A if read B
: "RIGHT
: "LEFT

Note that there is no "halt" or "stop".

== 1978: second Davis model ==
The following model appears as an essay ''What is a computation?'' in Steen pages 241–267. For some reason Davis has renamed his model a "Turing–Post machine" (with one back-sliding on page 256.) 

In the following model Davis assigns the numbers "1" to Post's "mark/slash" and "0" to the blank square. To quote Davis: "We are now ready to introduce the Turing–Post Programming Language. In this language there are seven kinds of instructions:
:: "PRINT 1
:: "PRINT 0
:: "GO RIGHT
:: "GO LEFT
:: "GO TO STEP i IF '''1''' IS SCANNED
:: "GO TO STEP i IF '''0''' IS SCANNED
:: "STOP
"A Turing–Post program is then a list of instructions, each of which is of one of these seven kinds. Of course in an actual program the letter ''i'' in a step of either the fifth or sixth kind must replaced with a definite (positive whole) number." (Davis in Steen, p.&amp;nbsp;247). 

* Confusion arises if one does not realize that a "blank" tape is actually printed with all zeroes &amp;mdash; there is no "blank".
* Splits Post's "[[GOTO|GO TO]]" ("[[branch (computer science)|branch]]" or "jump") instruction into two, thus creating a larger (but easier-to-use) instruction set of seven rather than Post's six instructions.
* Does not mention that instructions [[input/output|PRINT]] 1, PRINT 0, GO RIGHT and GO LEFT imply that, after execution, the "computer" must go to the next step in numerical sequence.

== 1994 (2nd edition): Davis–Sigal–Weyuker's Post–Turing program model ==

"Although the formulation of Turing we have presented is closer in spirit to that originally given by Emil Post, it was Turing's analysis of the computation that has made this formulation seem so appropriate. This language has played a fundamental role in theoretical computer science." (Davis et al. (1994) p.&amp;nbsp;129)

This model allows for the printing of multiple symbols. The model allows for B (blank) instead of S&lt;sub&gt;0&lt;/sub&gt;. The tape is infinite in both directions. Either the head or the tape moves, but their definitions of RIGHT and LEFT always specify the same outcome in either case (Turing used the same convention).

:: PRINT σ          ;Replace scanned symbol with σ
:: IF σ GOTO L      ;IF scanned symbol is σ THEN goto "the first" instruction labelled L
:: RIGHT            ;Scan square immediately right of the square currently scanned
:: LEFT             ;Scan square immediately left of the square currently scanned

Note that only one type of "jump" – a conditional GOTO – is specified; for an unconditional jump a string of GOTO's must test each symbol.

This model reduces to the binary { 0, 1 } versions presented above, as shown here:
:: PRINT 0 = ERASE  ;Replace scanned symbol with 0 = B = BLANK
:: PRINT 1          ;Replace scanned symbol with 1
:: IF 0 GOTO L      ;IF scanned symbol is 0 THEN goto "the first" instruction labelled L
:: IF 1 GOTO L      ;IF scanned symbol is 1 THEN goto "the first" instruction labelled L
:: RIGHT            ;Scan square immediately right of the square currently scanned
:: LEFT             ;Scan square immediately left of the square currently scanned

==Examples of the Post–Turing machine==

=== Atomizing Turing quintuples into a sequence of Post–Turing instructions ===
The following "reduction" (decomposition, atomizing) method – from 2-symbol Turing 5-tuples to a sequence of 2-symbol Post–Turing instructions – can be found in Minsky (1961). He states that this reduction to "a ''program'' ... a sequence of ''Instructions''" is in the spirit of [[Hao Wang (academic)|Hao Wang's]] [[Wang B-machine|B-machine]] (italics in original, cf. Minsky (1961) p.&amp;nbsp;439). 

(Minsky's reduction to what he calls "a sub-routine" results in 5 rather than 7 Post–Turing instructions. He did not atomize Wi0: "Write symbol Si0; go to new state Mi0", and Wi1: "Write symbol Si1; go to new state Mi1". The following method further atomizes Wi0 and Wi1; in all other respects the methods are identical.) 

This reduction of Turing 5-tuples to Post–Turing instructions may not result in an "efficient" Post–Turing program, but it will be faithful to the original Turing-program. 

In the following example, each Turing 5-tuple of the 2-state [[busy beaver]] converts into 
:(i) an initial conditional "jump" (goto, branch), followed by
:(ii) 2 tape-action instructions for the "0" case – Print or Erase or None, followed by Left or Right or None, followed by
:(iii) an unconditional "jump" for the "0" case to its next instruction
:(iv) 2 tape-action instructions for the "1" case – Print or Erase or None, followed by Left or Right or None, followed by
:(v) an unconditional "jump" for the "1" case to its next instruction

for a total of {{nowrap|1=1&amp;nbsp;+&amp;nbsp;2&amp;nbsp;+&amp;nbsp;1&amp;nbsp;+&amp;nbsp;2&amp;nbsp;+&amp;nbsp;1&amp;nbsp;=&amp;nbsp;7}} instructions per Turing-state.

For example, the 2-state busy beaver's "A" Turing-state, written as two lines of 5-tuples, is:
{|class="wikitable"
|- style="font-size:9pt" align="center" valign="bottom"
! Initial m-configuration (Turing state)
! Tape symbol
! Print operation
! Tape motion
! Final m-configuration (Turing state)
|- style="font-size:9pt" align="center" valign="bottom"
| Height="11.4" | '''A'''
 | 0
 | P
 | R
 | '''B'''
|- style="font-size:9pt" align="center" valign="bottom"
| Height="11.4" | '''A'''
 | 1
 | P
 | L
 | '''B'''
|}

The table represents just a single Turing "instruction", but we see that it consists of two lines of 5-tuples, one for the case "tape symbol under head = 1", the other for the case "tape symbol under head = 0". Turing observed (''Undecidable'', p.&amp;nbsp;119) that the left-two columns – "m-configuration" and "symbol" – represent the machine's current "configuration" – its state including both Tape and Table at that instant – and the last three columns are its subsequent "behavior". As the machine cannot be in two "states" at once, the machine must "branch" to either one configuration or the other:    

{|class="wikitable"
|- style="font-size:9pt" align="center" valign="bottom"
! Initial m-configuration and symbol S
! Print operation
! Tape motion
! Final m-configuration
|- style="font-size:9pt"
| Height="12" align="right" valign="bottom" | S=0 --&gt;
| align="center" valign="bottom" | P --&gt;
| align="center" valign="bottom" | R --&gt;
|style="font-weight:bold" align="center" valign="bottom" | B
|- style="font-size:9pt"
| Height="12" valign="bottom" |  --&gt; '''A''' &lt;
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
|style="font-weight:bold" align="center" valign="bottom" | 
|- style="font-size:9pt"
| Height="12" align="right" valign="bottom" | S=1 --&gt;
| align="center" valign="bottom" | P --&gt;
| align="center" valign="bottom" | L --&gt;
|style="font-weight:bold" align="center" valign="bottom" | B
|}

After the "configuration branch" (J1 xxx) or (J0 xxx) the machine follows one of the two subsequent "behaviors". We list these two behaviors on one line, and number (or label) them sequentially (uniquely). Beneath each jump (branch, go to) we place its jump-to "number" (address, location):

{|class="wikitable"
|- style="font-size:9pt" align="center" valign="bottom"
! 
! Initial m-configuration &amp; symbol S
! Print operation
! Tape motion
! Final m-configuration case S=0
! Print operation
! Tape motion
! Final m-configuration case S=1
|- style="font-size:9pt"
| Height="11.4" align="center" valign="bottom" | 
| align="right" valign="bottom" | If S=0 then:
| align="center" valign="bottom" | P
| align="center" valign="bottom" | R
| align="center" valign="bottom" | '''B'''
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
|- style="font-size:9pt"
| Height="11.4" align="center" valign="bottom" | 
| valign="bottom" |  ---&gt; '''A''' &lt;
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
|- style="font-size:9pt"
| Height="11.4" align="center" valign="bottom" | 
| align="right" valign="bottom" | If S=1 then:
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | 
| align="center" valign="bottom" | P
| align="center" valign="bottom" | L
| align="center" valign="bottom" | '''B'''
|- style="font-size:9pt" align="center" valign="bottom"
| Height="3" | 
 | 
 | 
 | 
 | 
 | 
 | 
 | 
|- style="font-size:9pt" align="center" valign="bottom"
| Height="11.4" | instruction #
 | 1
 | 2
 | 3
 | 4
 | 5
 | 6
 | 7
|- style="font-size:9pt" align="center" valign="bottom"
| Height="12" | Post–Turing instruction
|style="font-weight:bold" | J1
|style="font-weight:bold" | P
|style="font-weight:bold" | R
|style="font-weight:bold" | J
|style="font-weight:bold" | P
|style="font-weight:bold" | L
|style="font-weight:bold" | J
|- style="font-size:9pt" align="center" valign="bottom"
| Height="12" | jump-to instruction #
|style="font-weight:bold" | 5
|style="font-weight:bold" | 
|style="font-weight:bold" | 
|style="font-weight:bold" | B
|style="font-weight:bold" | 
|style="font-weight:bold" | 
|style="font-weight:bold" | B
|}

Per the Post–Turing machine conventions each of the Print, Erase, Left, and Right instructions consist of two actions:
: (i) Tape action: { P, E, L, R}, then
: (ii) Table action: go to next instruction in sequence

And per the Post–Turing machine conventions the conditional "jumps" J0xxx, J1xxx consist of two actions:
: (i) Tape action: look at symbol on tape under the head
: (ii) Table action: If symbol is 0 (1) and J0 (J1) then go to xxx else go to next instruction in sequence

And per the Post–Turing machine conventions the unconditional "jump" Jxxx consists of a single action, or if we want to regularize the 2-action sequence:
: (i) Tape action: look at symbol on tape under the head
: (ii) Table action: If symbol is 0 then go to xxx else if symbol is 1 then go to xxx.

Which, and how many, jumps are necessary? The unconditional jump '''J'''xxx is simply '''J0''' followed immediately by '''J1''' (or vice versa). Wang (1957) also demonstrates that only one conditional jump is required, i.e. either '''J0'''xxx or '''J1'''xxx. However, with this restriction the machine becomes difficult to write instructions for. Often only two are used, i.e.
:(i) { '''J0'''xxx, '''J1'''xxx }
:(ii) { '''J1'''xxx, '''J'''xxx }
:(iii) { '''J0'''xxx, '''J'''xxx },

but the use of all three { '''J0'''xxx, '''J1'''xxx, '''J'''xxx } does eliminate extra instructions. In the 2-state Busy Beaver example that we use only { '''J1'''xxx, '''J'''xxx }.

=== 2-state busy beaver ===

The mission of the [[busy beaver]] is to print as many ones as possible before halting. The "Print" instruction writes a 1, the "Erase" instruction (not used in this example) writes a 0 (i.e. it is the same as P0). The tape moves "Left" or "Right" (i.e. the "head" is stationary). 

State table for a 2-state Turing-machine [[busy beaver]]: 
{|class="wikitable"
|- style="font-size:9pt"
! rowspan=2 | Tape symbol
|  colspan="3" | Current state '''A'''
|  colspan="3"| Current state '''B'''
|- style="font-size:9pt"
! Write symbol
! Move tape
! Next state
! Write symbol
! Move tape
! Next state
|- style="font-size:9pt" 
! 0
 | 1
 | R
|style="font-weight:bold" | '''B'''
 | 1
 | L
 | '''A'''
|- style="font-size:9pt" 
! 1
 | 1
 | L
|style="font-weight:bold" | '''B'''
 | 1
 | N
 | '''H'''
|}

Instructions for the Post–Turing version of a 2-state busy beaver: observe that all the instructions are on the same line and in sequence. This is a significant departure from the "Turing" version and is in the same format as what is called a "computer program":

{|class="wikitable"
|- style="font-size:9pt;font-weight:bold" 
! Instruction #
! 1
! 2
! 3
! 4
! 5
! 6
! 7
! 8
! 9
! 10
! 11
! 12
! 13
! 14
! 15
|- style="font-size:9pt" 
! Instruction
 | J1
 | P
 | R
 | J
 | P
 | L
 | J
 | J1
 | P
 | L
 | J
 | P
 | N
 | J
 | H
|- style="font-size:9pt" 
! Jump-to #
 | 5
 | 
 | 
 | 8
 | 
 | 
 | 8
 | 12
 | 
 | 
 | 1
 | 
 | 
 | 15
 | 
|- style="font-size:9pt" 
! Turing-state label
|style="font-weight:bold" | A
 | 
 | 
 | 
 | 
 | 
 | 
|style="font-weight:bold" | B
 | 
 | 
 | 
 | 
 | 
 | 
|style="font-weight:bold" | H
|}

Alternately, we might write the table as a string. The use of "parameter separators" ":" and instruction-separators "," are entirely our choice and do not appear in the model. There are no conventions (but see Booth (1967) p.&amp;nbsp;374, and Boolos and Jeffrey (1974, 1999) p.&amp;nbsp;23), for some useful ideas of how to combine state diagram conventions with the instructions – i.e. to use arrows to indicate the destination of the jumps). In the example immediately below, the instructions are ''sequential'' starting from "1", and the parameters/"operands" are considered part of their instructions/"opcodes":
: J1:5, P, R, J:8, P, L, J:8, J1:12, P, L, J1:1, P, N, J:15, H

The state diagram of a two-state busy beaver (little drawing, right-hand corner) converts to the equivalent Post–Turing machine with the substitution of 7 Post–Turing instructions per "Turing" state. The HALT instruction adds the 15th state:
[[File:State diagram 2 state busy beaver 2.JPG|location: left|900px|2-state Busy Beaver run on a P–T machine]]

A "run" of the 2-state busy beaver with all the intermediate steps of the Post–Turing machine shown:

[[Image:2 state busy beaver.JPG|location: left|900px|2-state Busy Beaver run on a P–T machine]]

=== Two state busy beaver followed by "tape cleanup" ===

The following is a two-state Turing busy beaver with additional instructions 15–20 to demonstrate the use of "Erase", J0, etc. These will erase the 1's written by the busy beaver: 

{|class="wikitable"
|- style="font-size:9pt;font-weight:bold" 
! Instruction #
! 1
! 2
! 3
! 4
! 5
! 6
! 7
! 8
! 9
! 10
! 11
! 12
! 13
! 14
! 15
! 16
! 17
! 18
! 19
! 20
|- style="font-size:9pt" 
! Instruction
 | J1
 | P
 | R
 | J
 | P
 | L
 | J
 | J1
 | P
 | L
 | J
 | P
 | N
 | J
 | L
 | J0
 | E
 | R
 | J1
 | H
|- style="font-size:9pt" 
! Jump-to #
 | 5
 | 
 | 
 | 8
 | 
 | 
 | 8
 | 12
 | 
 | 
 | 1
 | 
 | 
 | 15
 | 
 | 20
 | 
 | 
 | 17
 | 
|- style="font-size:9pt" 
! Turing-state label
|style="font-weight:bold" | A
 | 
 | 
 | 
 | 
 | 
 | 
|style="font-weight:bold" | B
 | 
 | 
 | 
 | 
 | 
 | 
|style="font-weight:bold" | *
 | 
 | 
 | 
 | 
 | 
|}

Additional Post–Turing instructions 15 through 20 erase the symbols created by the busy beaver. These "atomic" instructions are more "efficient" than their Turing-state equivalents (of 7 Post–Turing instructions). To accomplish the same task a Post–Turing machine will (usually) require fewer Post–Turing states than a Turing-machine, because (i) a jump (go-to) can occur to any Post–Turing instruction (e.g. P, E, L, R) within the Turing-state, (ii) a grouping of move-instructions such as L, L, L, P are possible, etc.:

{|class="wikitable"
|- style="font-size:9pt;font-weight:bold" 
! Instruction #
! 16
! 17
! 18
! 19
! 20
|- style="font-size:9pt" 
! Instruction
 | J0
 | E
 | R
 | J1
 | H
|- style="font-size:9pt" 
! Jump-to #
 | 20
 | 
 | 
 | 17
 | 
|}

[[File:State diagram 2 state busy beaver.JPG|location: left|900px|2-state Busy Beaver followed by tape-erase, as run on a P–T machine]]

[[Image:2 state busy beaver 2.JPG|location: left|900px|2-state Busy Beaver followed by tape-erase, as run on a P-T machine]]

== Example: Multiply 3 &amp;times; 4 with a Post–Turing machine ==

[[Image:Algorithm P-T multiply 2.JPG|thumbnail|500px|An example of "multiply" ''a''&amp;nbsp;&amp;times;&amp;nbsp;''b''&amp;nbsp;=&amp;nbsp;''c'' on a Post–Turing machine. At the start, the tape (shown on the left) has two numbers on it – '''a' ''' = 3' (4 marks), '''b' ''' = 4' (5 marks). (A single mark would represent "0".) At the end the tape will have the product '''c' ''' = 12' (13 marks) to the right of b. Note "top" and "bottom" are there just to clarify what the P–T machine is doing.]]

This example is a reference to show how a "multiply" computation would proceed on a single-tape, 2-symbol { blank, 1 } Post–Turing machine model. 

This particular "multiply" algorithm is recursive through two loops. The head moves. It starts to the far left (the top) of the string of unary marks representing '''a' ''':
:*Move head far right. Establish (i.e. "clear") register '''c''' by placing a single blank and then a mark to the right of '''b'''
:*'''a_loop''': Move head right once, test for the bottom of '''a' ''' (a blank). If blank then done else erase mark;
:*Move head right to '''b' '''. Move head right once past the top mark of '''b' ''';
:*'''b_loop''': If head is at the bottom of '''b' ''' (a blank) then move head to far left of '''a' ''', else:
::*Erase a mark to locate counter (a blank) in '''b' '''.
::*Increment '''c' ''': Move head right to top of '''c' ''' and increment '''c' '''.
::*Move head left to the counter inside '''b' ''',
::*Repair counter: print a mark in the blank counter.
::*Decrement '''b' '''&amp;minus;count: Move head right once.
::*Return to b_loop.

:Multiply '''a''' &amp;times; '''b''' = '''c''', for example: 3 &amp;times; 4 = 12. The scanned square is indicated by brackets around the mark i.e. [ '''|''' ]. An extra mark serves to indicate the symbol "0":
::At the start of the computation '''a' ''' is 4 unary marks, then a separator blank, '''b' ''' is 5 unary marks, then a separator mark. An unbounded number of empty spaces must be available for '''c''' to the right:
::: ....a'.b'.... = : ....'''[ | ]''' '''| | |''' . '''| | | | |''' ....

::During the computation the head shuttles back and forth from '''a' ''' to '''b' ''' to '''c' ''' back to '''b' ''' then to '''c' ''', then back to '''b' ''', then to '''c' ''' [[ad nauseam]] while the machine counts through '''b' ''' and increments '''c' '''. Multiplicand '''a' ''' is slowly counted down (its marks erased – shown for reference with x's below). A "counter" inside '''b' '''moves to the right through '''b''' (an erased mark shown being read by the head as '''[ . ]''' ) but is reconstructed after each pass when the head returns from incrementing '''c' ''':
::: ....'''a'''.'''b'''.... = : ....xxx '''|''' . '''| | [ . ] | |''' . '''| | | | | | |''' ...

::At end of computation: '''c' ''' is 13 marks = "successor of 12" appearing to the right of '''b' '''. '''a' ''' has vanished in process of the computation
:::....'''b'''.'''c''' = ......... '''| | | | |''' . '''| | | | | | | | | | | | | '''...

== Notes ==
{{reflist}}

== References ==
* Stephen C. [[Kleene]], ''Introduction to Meta-Mathematics, North-Holland Publishing Company'', New York, 10th edition 1991, first published 1952. Chapter XIII is an excellent description of Turing machines; Kleene uses a Post-like model in his description and admits the Turing model could be further atomized, see footnote 1.
* [[Martin Davis]], editor: ''The Undecidable, Basic Papers on Undecidable Propositions, Unsolvable Problems and Computable Functions'', Raven Press, New York, 1965. Papers include those by [[Gödel]], [[Alonzo Church|Church]], [[J. Barkley Rosser|Rosser]], [[Kleene]], and Post.
* [[Martin Davis]], "What is a computation", in ''Mathematics Today'', Lynn Arthur Steen, Vintage Books (Random House), 1980. A wonderful little paper, perhaps the best ever written about Turing Machines. Davis reduces the Turing Machine to a far-simpler model based on Post's model of a computation. Includes a little biography of Emil Post.
* [[Martin Davis]], ''Computability: with Notes by Barry Jacobs'', Courant Institute of Mathematical Sciences, New York University, 1974.
* [[Martin Davis]], [[Ron Sigal]], [[Elaine J. Weyuker]], (1994) ''Computability, Complexity, and Languages: Fundamentals of Theoretical Computer Science – 2nd edition'', Academic Press: Harcourt, Brace &amp; Company, San Diego, 1994 {{isbn|0-12-206382-1}} (First edition, 1983).
* [[Fred Hennie]], ''Introduction to Computability'', Addison–Wesley, 1977.
* [[Marvin Minsky]], (1961), ''Recursive Unsolvability of Post's problem of 'Tag' and other Topics in Theory of Turing Machines'', Annals of Mathematics, Vol. 74, No. 3, November, 1961.
* [[Roger Penrose]], ''The Emperor's New Mind: Concerning computers, Minds and the Laws of Physics'', Oxford University Press, Oxford England, 1990 (with corrections). Cf. Chapter 2, "Algorithms and Turing Machines". An overcomplicated presentation (see Davis's paper for a better model), but a thorough presentation of Turing machines and the [[halting problem]], and Church's [[lambda calculus]].
* [[Hao Wang (academic)|Hao Wang]] (1957): "A variant to Turing's theory of computing machines", ''Journal of the Association for Computing Machinery'' (JACM) 4, 63–92.

{{DEFAULTSORT:Post-Turing machine}}
[[Category:Turing machine]]
[[Category:Models of computation]]</text>
      <sha1>18dt9j4wp16gnjuxapw29acrsmk1g9y</sha1>
    </revision>
  </page>
  <page>
    <title>Quotient of an abelian category</title>
    <ns>0</ns>
    <id>48897477</id>
    <revision>
      <id>870191909</id>
      <parentid>870191873</parentid>
      <timestamp>2018-11-23T02:41:38Z</timestamp>
      <contributor>
        <username>AxelBoldt</username>
        <id>2</id>
      </contributor>
      <comment>2nd example false?</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4370">In [[mathematics]], the '''quotient''' (also called '''Serre quotient''' or '''Gabriel quotient''') of an [[abelian category]] &lt;math&gt;\mathcal A&lt;/math&gt; by a [[Serre subcategory]] ''&lt;math&gt;\mathcal B&lt;/math&gt;'' is the abelian category ''&lt;math&gt;\mathcal A/\mathcal B&lt;/math&gt;'' which, intuitively, is obtained from ''&lt;math&gt;\mathcal A&lt;/math&gt;'' by ignoring (i.e. treating as [[Zero-object|zero]]) all objects from ''&lt;math&gt;\mathcal B&lt;/math&gt;.''  There is a canonical [[Exact functor|exact]] [[functor]] &lt;math&gt;Q \colon \mathcal A \to \mathcal A/\mathcal B&lt;/math&gt; whose kernel is ''&lt;math&gt;\mathcal B&lt;/math&gt;''.

== Definition ==
Formally, ''&lt;math&gt;\mathcal A/\mathcal B&lt;/math&gt;'' is the category whose objects are those of ''&lt;math&gt;\mathcal A&lt;/math&gt;'' and whose morphisms from ''X'' to ''Y'' are given by the [[direct limit]] (of [[Abelian group|abelian groups]]) &lt;math&gt;\varinjlim \mathrm{Hom}_\mathcal A(X', Y/Y')&lt;/math&gt; over [[Subobject|subobjects]] &lt;math&gt;X' \subseteq X&lt;/math&gt; and &lt;math&gt;Y' \subseteq Y&lt;/math&gt; such that &lt;math&gt;X/X'\in \cal{B}&lt;/math&gt; and &lt;math&gt; Y' \in \cal{B}&lt;/math&gt;.  (Here, &lt;math&gt;X/X'&lt;/math&gt; and &lt;math&gt;Y/Y'&lt;/math&gt; denote [[Quotient object|quotient objects]] computed in ''&lt;math&gt;\mathcal A&lt;/math&gt;''.) Composition of morphisms in ''&lt;math&gt;\mathcal A/\mathcal B&lt;/math&gt;'' is induced by the universal property of the direct limit. 

The canonical functor &lt;math&gt;Q \colon \mathcal A \to \mathcal A/\mathcal B&lt;/math&gt; sends an object ''X'' to itself and a morphism &lt;math&gt;f \colon X \to Y&lt;/math&gt; to the corresponding element of the direct limit with X'=X and Y'=0.

== Examples ==
Let &lt;math&gt;k&lt;/math&gt; be a [[Field (mathematics)|field]] and consider the abelian category &lt;math&gt;{\rm Mod}(k)&lt;/math&gt; of all [[Vector space|vector spaces]] over &lt;math&gt;k&lt;/math&gt;. Then the full subcategory &lt;math&gt;{\rm mod}(k)&lt;/math&gt;of finite-[[Dimension|dimensional]] vector spaces is a Serre-subcategory of &lt;math&gt;{\rm Mod}(k)&lt;/math&gt;. The quotient &lt;math&gt;\cal{C}={\rm Mod}(k)/{\rm mod}(k)&lt;/math&gt; has as objects the &lt;math&gt;k&lt;/math&gt;-vector spaces, and the set of morphisms from &lt;math&gt;X&lt;/math&gt; to &lt;math&gt;Y&lt;/math&gt; in &lt;math&gt;\cal{C}&lt;/math&gt; is &lt;math display="block"&gt;\{k\text{-linear maps from } X\text{ to } Y\}/\{k\text{-linear maps from } X \text{ to } Y \text{ with finite-dimensional image}\}&lt;/math&gt;(which is a [[Quotient space (linear algebra)|quotient of vector spaces]]).  This has the effect of identifying all finite-dimensional vector spaces with 0, and of identifying two linear maps whenever their difference has finite-dimensional image.

== Properties ==
The quotient ''&lt;math&gt;\mathcal A/\mathcal B&lt;/math&gt;'' is an abelian category, and the canonical functor &lt;math&gt;Q \colon \mathcal A \to \mathcal A/\mathcal B&lt;/math&gt; is [[Exact functor|exact]].   The kernel of  &lt;math&gt;Q&lt;/math&gt; is ''&lt;math&gt;\mathcal B&lt;/math&gt;'', i.e., &lt;math&gt;Q(X)&lt;/math&gt;is a [[Zero-object|zero object]] of  ''&lt;math&gt;\mathcal A/\mathcal B&lt;/math&gt;'' if and only if &lt;math&gt;X&lt;/math&gt; belongs to ''&lt;math&gt;\mathcal B&lt;/math&gt;''. 

The quotient and canonical functor are characterized by the following [[universal property]]: if  ''&lt;math&gt;\mathcal C&lt;/math&gt;'' is any abelian category and &lt;math&gt;F \colon \mathcal A \to \mathcal C&lt;/math&gt; is an exact functor such that &lt;math&gt;F(X)&lt;/math&gt; is a zero object of ''&lt;math&gt;\mathcal C&lt;/math&gt;'' for each object &lt;math&gt;X \in \mathcal B&lt;/math&gt;, then there is a unique exact functor &lt;math&gt;\overline{F} \colon \mathcal A/\mathcal B \to \mathcal C&lt;/math&gt; such that &lt;math&gt;F = \overline{F} \circ Q&lt;/math&gt;.&lt;ref&gt;Gabriel, Pierre, ''[http://www.numdam.org/article/BSMF_1962__90__323_0.pdf Des categories abeliennes]'', Bull. Soc. Math. France '''90''' (1962), 323-448.&lt;/ref&gt;

== Gabriel–Popescu ==
The [[Gabriel–Popescu theorem]] states that any [[Grothendieck category]] &lt;math&gt;\mathcal{A}&lt;/math&gt; is equivalent to a quotient category &lt;math&gt;\operatorname{Mod}(R)/\cal{B}&lt;/math&gt;, where &lt;math&gt;\operatorname{Mod}(R)&lt;/math&gt;denotes the abelian category of right modules over some unital ring &lt;math&gt;R&lt;/math&gt;, and &lt;math&gt;\cal{B}&lt;/math&gt; is some [[localizing subcategory]] of &lt;math&gt;\operatorname{Mod}(R)&lt;/math&gt;.&lt;ref&gt;{{cite journal|authors=N. Popesco, P. Gabriel|year=1964|title=Caractérisation des catégories abéliennes avec générateurs et limites inductives exactes|journal=Comptes Rendus de l'Académie des Sciences|volume=258|pages=4188–4190}}&lt;/ref&gt;

==References==
{{reflist}}

[[Category:Category theory]]

{{categorytheory-stub}}</text>
      <sha1>e6la0hgrcxp1rq9g2sq3lr1kzp0mawz</sha1>
    </revision>
  </page>
  <page>
    <title>Reversible computing</title>
    <ns>0</ns>
    <id>1539548</id>
    <revision>
      <id>862842593</id>
      <parentid>857875163</parentid>
      <timestamp>2018-10-07T02:09:02Z</timestamp>
      <contributor>
        <ip>67.160.212.50</ip>
      </contributor>
      <comment>/* Logical reversibility */ Included the first name and initial when naming Bennett for the first time in the text.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14411">'''Reversible computing''' is a model of [[computing]] where the [[computational process]] to some extent is reversible, i.e., time-invertible. In a model of computation that uses [[deterministic]] [[State transition system|transitions]] from one state of the abstract machine to another, a necessary condition for reversibility is that the relation of the mapping from (nonzero-probability) states to their successors must be [[injective function|one-to-one]]. Reversible computing is a form of [[unconventional computing]].

==Reversibility==
There are two major, closely related types of reversibility that are of particular interest for this purpose: ''physical reversibility'' and ''logical reversibility''.&lt;ref&gt;http://www.cise.ufl.edu/research/revcomp/&lt;/ref&gt;

A process is said to be ''physically reversible'' if it results in no increase in physical [[entropy]]; it is ''[[isentropic]]''.  There is a style of circuit design ideally exhibiting this property that is referred to as '''charge recovery logic''', [[adiabatic circuit]]s, or '''[[adiabatic]] computing'''.  Although ''in practice'' no nonstationary physical process can be ''exactly'' physically reversible or isentropic, there is no known limit to the closeness with which we can approach perfect reversibility, in systems that are sufficiently well isolated from interactions with unknown external environments, when the laws of physics describing the system's evolution are precisely known.

Probably the largest motivation for the study of technologies aimed at actually implementing reversible computing is that they offer what is predicted to be the only potential way to improve the [[computational energy efficiency]] of computers beyond the fundamental [[von Neumann-Landauer limit]]&lt;ref name="neumann"&gt;J. von Neumann, ''Theory of Self-Reproducing Automata'', Univ. of Illinois Press, 1966.&lt;/ref&gt; of ''kT'' ln(2) energy dissipated per irreversible [[bit operation]].  Although the Landauer limit was millions of times below the energy consumption of computers in the 2000s and thousands of times less in the 2010s,&lt;ref&gt;Bérut, Antoine, et al. "[http://www.nature.com/nature/journal/v483/n7388/abs/nature10872.html Experimental verification of Landauer's principle linking information and thermodynamics.]" Nature 483.7388 (2012): 187-189: "''From a technological perspective, energy dissipation per logic operation in present-day silicon-based digital circuits is about a factor of 1,000 greater than the ultimate Landauer limit, but is predicted to quickly attain it within the next couple of decades''"&lt;/ref&gt; proponents of reversible computing argue that this can be attributed largely to architectural overheads which effectively magnify the impact of Landauer's limit in practical circuit designs, so that it may prove difficult for practical technology to progress very far beyond current levels of energy efficiency if reversible computing principles are not used.&lt;ref&gt;Michael P. Frank, "Foundations of Generalized Reversible Computing," to be published at the 9th Conference on Reversible Computation, Jul. 6-7, 2017, Kolkata, India.  Preprint available at https://cfwebprod.sandia.gov/cfdocs/CompResearch/docs/grc-rc17-preprint2.pdf.&lt;/ref&gt;

==Relation to thermodynamics==
As was first argued by [[Rolf Landauer]] of [[IBM]],&lt;ref&gt;R. Landauer, "Irreversibility and heat generation in the computing process," IBM Journal of Research and Development, vol. 5, pp. 183-191, 1961.&lt;/ref&gt; in order for a computational process to be physically reversible, it must also be ''logically reversible''. [[Landauer's principle]] is the rigorously valid observation that the oblivious erasure of ''n'' bits of known information must always incur a cost of ''nkT'' ln(2) in thermodynamic [[entropy]].  A discrete, deterministic computational process is said to be logically reversible if the transition function that maps old computational states to new ones is a [[one-to-one function]]; i.e. the output logical states uniquely determine the input logical states of the computational operation.

For computational processes that are nondeterministic (in the sense of being probabilistic or random), the relation between old and new states is not a [[single-valued function]], and the requirement needed to obtain physical reversibility becomes a slightly weaker condition, namely that the size of a given ensemble of possible initial computational states does not decrease, on average, as the computation proceeds forwards.

==Physical reversibility==
Landauer's principle (and indeed, the [[second law of thermodynamics]] itself) can also be understood to be a direct [[logical consequence]] of the underlying reversibility of [[physics]], as is reflected in the [[Hamiltonian mechanics|general Hamiltonian formulation of mechanics]], and in the [[time evolution|unitary time-evolution operator]] of [[quantum mechanics]] more specifically.

The implementation of reversible computing thus amounts to learning how to characterize and control the physical dynamics of mechanisms to carry out desired computational operations so precisely that we can accumulate a negligible total amount of uncertainty regarding the complete physical state of the mechanism, per each logic operation that is performed. In other words, we would need to precisely track the state of the active energy that is involved in carrying out computational operations within the machine, and design the machine in such a way that the majority of this energy is recovered in an organized form that can be reused for subsequent operations, rather than being permitted to dissipate into the form of heat.

Although achieving this goal presents a significant challenge for the design, manufacturing, and characterization of ultra-precise new physical mechanisms for [[computing]], there is at present no fundamental reason to think that this goal cannot eventually be accomplished, allowing us to someday build computers that generate much less than 1 bit's worth of physical entropy (and dissipate much less than ''kT'' ln 2 energy to heat) for each useful logical operation that they carry out internally.

Today, the field has a substantial body of academic literature behind it. A wide variety of reversible device concepts, [[logic gate]]s, [[electronic circuit]]s, processor architectures, [[programming language]]s, and application [[algorithm]]s have been designed and analyzed by [[physicist]]s, [[electrical engineer]]s, and [[computer scientist]]s.

This field of research awaits the detailed development of a high-quality, cost-effective, nearly reversible logic device technology, one that includes highly energy-efficient [[clocking]] and [[synchronization]] mechanisms, or avoids the need for these through asynchronous design. This sort of solid engineering progress will be needed before the large body of theoretical research on reversible computing can find practical application in enabling real computer technology to circumvent the various near-term barriers to its energy efficiency, including the von Neumann-Landauer bound.  This may only be circumvented by the use of logically reversible computing, due to the [[Second Law of Thermodynamics]].

==Logical reversibility==
To implement reversible computation, estimate its cost, and to judge its limits, it can be formalized in terms of gate-level circuits.  A simplified model of such circuits is one in which inputs are consumed (however, note that real logic gates as implemented e.g. in [[CMOS]] do not do this).  In this modeling framework, an [[inverter (logic gate)]] (NOT) gate is reversible because it can be ''undone''. The [[exclusive or]] (XOR) gate is irreversible because its two inputs cannot be unambiguously reconstructed from its single output. However, a reversible version of the XOR gate—the [[controlled NOT gate]] (CNOT)—can be defined by preserving one of the inputs. The three-input variant of the CNOT gate is called the [[Toffoli gate]]. It preserves two of its inputs ''a,b'' and replaces the third ''c'' by &lt;math&gt;c\oplus (a\cdot b)&lt;/math&gt;. With &lt;math&gt;c=0&lt;/math&gt;, this gives the AND function, and with &lt;math&gt;a\cdot b=1&lt;/math&gt; this gives the NOT function. Thus, the Toffoli gate is universal and can implement any reversible [[Boolean function]] (given enough zero-initialized ancillary bits). More generally, reversible gates that consume their input have no more inputs than outputs. A reversible circuit connects reversible gates without [[fanout]]s and loops. Therefore, such circuits contain equal numbers of input and output wires, each going through an entire circuit. Similarly, in the Turing machine model of computation, a reversible Turing machine is one whose transition function is invertible, so that each machine state has at most one predecessor.

[[:fr:Yves Lecerf|Yves Lecerf]] proposed a reversible Turing machine in a 1963 paper,&lt;ref&gt;Lecerf (Y.) : Logique Mathématique : Machines de Turing réversibles. Comptes rendus des séances de l'académie des sciences, 257:2597--2600, 1963.&lt;/ref&gt; but apparently unaware of Landauer's principle, did not pursue the subject further, devoting most of the rest of his career to ethnolinguistics.  In 1973 Charles H. Bennett, at IBM Research, showed that a universal Turing machine could be made both logically and thermodynamically reversible,&lt;ref&gt;C. H. Bennett, "[http://www.dna.caltech.edu/courses/cs191/paperscs191/bennett1973.pdf Logical reversibility of computation]", IBM Journal of Research and Development, vol. 17, no. 6, pp. 525-532, 1973&lt;/ref&gt; and therefore able in principle to perform arbitrarily much computation per unit of physical energy dissipated, in the limit of zero speed.  In 1982 [[Edward Fredkin]] and [[Tommaso Toffoli]] proposed the [[Billiard ball computer]], a mechanism using classical hard spheres to do reversible computations at finite speed with zero dissipation, but requiring perfect initial alignment of the balls' trajectories, and Bennett's review&lt;ref&gt;C. H. Bennett, "The Thermodynamics of Computation -- A Review," International Journal of Theoretical Physics, vol. 21, no. 12, pp. 905-940, 1982&lt;/ref&gt; compared these "Brownian" and "ballistic" paradigms for reversible computation. Aside from the motivation of energy-efficient computation, reversible logic gates offered practical improvements of [[bit manipulation|bit-manipulation]] transforms in cryptography and computer graphics. Since the 1980s, reversible circuits have attracted interest as components of [[quantum algorithm]]s, and more recently in photonic and nano-computing technologies where some switching devices offer no [[signal gain]].

Surveys of reversible circuits, their construction and optimization, as well as recent research challenges, are available.&lt;ref&gt;Rolf Drechsler, Robert Wille. From Truth Tables to Programming Languages: Progress in the Design of Reversible Circuits. International Symposium on Multiple-Valued Logic, 2011. http://www.informatik.uni-bremen.de/agra/doc/konf/11_ismvl_reversible_circuit_design_tutorial.pdf&lt;/ref&gt;&lt;ref&gt;Mehdi Saeedi, Igor L. Markov, Synthesis and Optimization of Reversible Circuits - A Survey, ACM Computing Surveys, 2012. https://arxiv.org/abs/1110.2574&lt;/ref&gt;&lt;ref&gt;Rolf Drechsler and Robert Wille. Reversible Circuits: Recent Accomplishments and Future Challenges for an Emerging Technology. International Symposium on VLSI Design and Test, 2012. http://www.informatik.uni-bremen.de/agra/doc/konf/2012_vdat_reversible_circuits_accompl_chall.pdf&lt;/ref&gt;&lt;ref&gt;Eyal Cohen, Shlomi Dolev and Michael Rosenblit, "All-optical design for inherently energy-conserving reversible gates and circuits", Nature Communications 7, Article number: 11424 (2016). https://www.nature.com/articles/ncomms11424&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last =Ang|first = Y. S.|last2 = Yang|first2 = S. A.|last3 = Zhang|first3 = C.|last4 = Ma|first4 = Z. S.|last5 = Ang|first5 = L. K.|date = 2017|title = Valleytronics in merging Dirac cones: All-electric-controlled valley filter, valve, and universal reversible logic gate|url = https://journals.aps.org/prb/abstract/10.1103/PhysRevB.96.245410|journal = Physical Review B|volume = 96|pages = 245410|doi = 10.1103/PhysRevB.96.245410}}&lt;/ref&gt;

==See also==
*[[Reverse computation]]
*[[Reversible dynamics]]
*[[Maximum entropy thermodynamics]], on the uncertainty interpretation of the second law of thermodynamics
*[[Reversible process (thermodynamics)]]
*[[Toffoli gate]]
*[[Fredkin gate]]
*[[Quantum computing]]
*[[Billiard-ball computer]]
*[[Bidirectional transformation]]
*[[Undo]]
*[[Reversible cellular automaton]]
*[[Janus (time-reversible computing programming language)]]
*[[Generalized lifting]]

==References==
{{reflist}}

==Further reading==
* {{citation| ref=none| author1-first= P. J. | author1-last=Denning | author1-link= Peter J. Denning | author2-first=T. G. | author2-last= Lewis | author2-link= Ted Lewis (computer scientist)| title= Computers that can run backwards | journal= [[American Scientist]] | volume= 105 | number= 5 | year= 2017 | doi= 10.1511/2017.105.5.270}}.
* Lange K.-J., McKenzie P., Tapp A. (2000), "Reversible space equals deterministic space", ''[[Journal of Computer and System Sciences]]'', 60: 354–367, {{doi|10.1006/jcss.1999.1672}}.
* Perumalla K. S. (2014), ''Introduction to Reversible Computing'', [[CRC Press]].
* Vitanyi P. M. B. (2005), "[https://arxiv.org/pdf/cs/0504088 Time, space, and energy in reversible computing]", ''Proceedings of the 2nd ACM conference on Computing frontiers'', 435–444. [Review of later theoretical work.]

==External links==
* [http://strangepaths.com/reversible-computation/2008/01/20/en/ Introductory article on reversible computing]
* [http://www.eng.fsu.edu/~mpf/CF05/RC05.htm First International Workshop on reversible computing]
* [http://www.eng.fsu.edu/~mpf/pubs.htm Recent publications of Michael P. Frank]
* [https://web.archive.org/web/20080626033214rn_1/revcomp.jot.com/WikiHome Internet Archive backup] of the "Reversible computing community Wiki" that was administered by Frank
* [http://www.reversible-computation.org/ Recent Workshops on Reversible Computation]
* [http://www.revkit.org/ Open-source toolkit for reversible circuit design]

{{DEFAULTSORT:Reversible Computing}}
[[Category:Digital electronics]]
[[Category:Models of computation]]
[[Category:Reversible computing| ]]
[[Category:Thermodynamics]] &lt;!-- replace with better variant --&gt;</text>
      <sha1>ln0311kk11dijyfen6ycgwcjncky9jp</sha1>
    </revision>
  </page>
  <page>
    <title>Selmer group</title>
    <ns>0</ns>
    <id>1970862</id>
    <revision>
      <id>864360334</id>
      <parentid>862710427</parentid>
      <timestamp>2018-10-16T18:13:59Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4414">In [[arithmetic geometry]], the  '''Selmer group''', named in honor of the work of {{harvs|txt|authorlink=Ernst Sejersted Selmer|last=Selmer|first=Ernst Sejersted|year=1951}} by {{harvs|txt|last=Cassels|first=John William Scott|authorlink=J. W. S. Cassels|year=1962}}, is a group constructed from an [[isogeny]] of [[abelian varieties]].

==The Selmer group of an isogeny==

The Selmer group of an abelian variety ''A'' with respect to an [[isogeny]] ''f''&amp;nbsp;:&amp;nbsp;''A''&amp;nbsp;→&amp;nbsp;''B'' of abelian varieties can be defined in terms of [[Galois cohomology]] as

:&lt;math&gt;\operatorname{Sel}^{(f)}(A/K)=\bigcap_v\ker(H^1(G_K,\ker(f))\rightarrow H^1(G_{K_v},A_v[f])/\operatorname{im}(\kappa_v))&lt;/math&gt;

where ''A''&lt;sub&gt;v&lt;/sub&gt;[''f''] denotes the ''f''-[[torsion (algebra)|torsion]] of ''A''&lt;sub&gt;v&lt;/sub&gt; and &lt;math&gt;\kappa_v&lt;/math&gt; is the local Kummer map &lt;math&gt;B_v(K_v)/f(A_v(K_v))\rightarrow H^1(G_{K_v},A_v[f])&lt;/math&gt;. Note that &lt;math&gt;H^1(G_{K_v},A_v[f])/\operatorname{im}(\kappa_v)&lt;/math&gt; is isomorphic to &lt;math&gt;H^1(G_{K_v},A_v)[f]&lt;/math&gt;. Geometrically, the principal homogeneous spaces coming from elements of the Selmer group have ''K''&lt;sub&gt;v&lt;/sub&gt;-rational points for all places ''v'' of ''K''. The Selmer group is finite. This implies that the part of the [[Tate–Shafarevich group]] killed by ''f'' is finite due to the following [[exact sequence]]

: 0 → ''B''(''K'')/''f''(''A''(''K'')) → Sel&lt;sup&gt;(f)&lt;/sup&gt;(''A''/''K'') → Ш(''A''/''K'')[''f''] → 0.

The Selmer group in the middle of this exact sequence is finite and effectively computable. This implies the weak [[Mordell–Weil theorem]] that its subgroup ''B''(''K'')/''f''(''A''(''K'')) is finite. There is a notorious problem about whether this subgroup  can be effectively computed: there is a procedure for  computing it that will terminate with the correct answer if there is some prime ''p'' such that the ''p''-component of  the Tate–Shafarevich group is finite. It is conjectured that the [[Tate–Shafarevich group]] is in fact finite, in which case any prime ''p'' would work. However, if (as seems unlikely) the [[Tate–Shafarevich group]] has an infinite ''p''-component for every prime ''p'', then the procedure may never terminate.

{{harvs|txt|authorlink=Ralph Greenberg|last=Greenberg|first=Ralph|year=1994}} has generalized the notion of Selmer group to more general ''p''-adic [[Galois representation]]s and to ''p''-adic variations of [[Motive (algebraic geometry)|motive]]s in the context of [[Iwasawa theory]].

==The Selmer group of a finite Galois module==

More generally one can define the Selmer group of a finite Galois module ''M'' (such as the kernel of an isogeny) as the elements of ''H''&lt;sup&gt;1&lt;/sup&gt;(''G''&lt;sub&gt;''K''&lt;/sub&gt;,''M'') that have images inside certain given subgroups of ''H''&lt;sup&gt;1&lt;/sup&gt;(''G''&lt;sub&gt;''K''&lt;sub&gt;''v''&lt;/sub&gt;&lt;/sub&gt;,''M'').

== References ==

*{{Citation | last1=Cassels | first1=John William Scott | authorlink=J. W. S. Cassels| title=Arithmetic on curves of genus 1. III. The Tate–Šafarevič and Selmer groups | doi=10.1112/plms/s3-12.1.259  |mr=0163913 | year=1962 | journal=Proceedings of the London Mathematical Society |series=Third Series | issn=0024-6115 | volume=12 | pages=259–296}}
*{{Citation | last1=Cassels | first1=John William Scott | authorlink=J. W. S. Cassels | title=Lectures on elliptic curves | url=https://books.google.com/books?id=zgqUAuEJNJ4C | publisher=[[Cambridge University Press]] | series=London Mathematical Society Student Texts | isbn=978-0-521-41517-0 |mr=1144763 | year=1991 | volume=24 | doi=10.1017/CBO9781139172530}}
*{{Citation | last1=Greenberg | first1=Ralph | author1-link=Ralph Greenberg | editor1-last=Serre | editor1-first=Jean-Pierre | editor1-link=Jean-Pierre Serre | editor2-last=Jannsen | editor2-first=Uwe | editor3-last=Kleiman | editor3-first=Steven L. | title=Motives | publisher=[[American Mathematical Society]] | location=Providence, R.I. | isbn=978-0-8218-1637-0 | year=1994 | chapter=Iwasawa Theory and p-adic Deformation of Motives | mr=1265554}}
*{{Citation | last1=Selmer | first1=Ernst S. | authorlink=Ernst Sejersted Selmer | title=The Diophantine equation ''ax''&lt;sup&gt;3&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''by''&lt;sup&gt;3&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''cz''&lt;sup&gt;3&lt;/sup&gt;&amp;nbsp;&amp;nbsp;=&amp;nbsp;0 | doi=10.1007/BF02395746 |mr=0041871 | year=1951 | journal=[[Acta Mathematica]] | issn=0001-5962 | volume=85 | pages=203–362 }}

[[Category:Number theory]]</text>
      <sha1>ol5wlqs7dc4skfenfwjvoxnbvbtfyqq</sha1>
    </revision>
  </page>
  <page>
    <title>Stirling number</title>
    <ns>0</ns>
    <id>95465</id>
    <revision>
      <id>868948029</id>
      <parentid>868943855</parentid>
      <timestamp>2018-11-15T12:39:37Z</timestamp>
      <contributor>
        <username>Joel B. Lewis</username>
        <id>13974845</id>
      </contributor>
      <comment>/* Stirling numbers with negative integral values */ various cleanup: minus signs, grammar, "note that", clarity</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="21443">In [[mathematics]], '''Stirling numbers''' arise in a variety of [[Analysis (mathematics)|analytic]] and [[combinatorics|combinatorial]] problems.  They are named after [[James Stirling (mathematician)|James Stirling]], who introduced them in the 18th century. Two different sets of numbers bear this name: the [[Stirling numbers of the first kind]] and the [[Stirling numbers of the second kind]]. Additionally, [[Lah numbers]] are sometimes referred to as Stirling numbers of the third kind.
Each kind is detailed in its respective article, this one serving as a description of relations between them.

A common property of all three kinds is that they describe coefficients relating three different sequences of polynomials that frequently arise in combinatorics.
Moreover, all three can be defined as the number of partitions of ''n'' elements into ''k'' non-empty subsets, with different ways of counting orderings within each subset.

==Notation==
{{main|Stirling numbers of the first kind}}
{{main|Stirling numbers of the second kind}}
Several different notations for Stirling numbers are in use. Common notations are:

: &lt;math&gt; \left[{n \atop k}\right]=c(n,k)=|s(n,k)|\,&lt;/math&gt;
for unsigned '''Stirling numbers of the first kind''',
which count the number of [[permutation]]s of ''n'' elements with ''k'' disjoint [[cyclic permutation|cycle]]s,

: &lt;math&gt; s(n,k)=(-1)^{n-k} \left[{n \atop k}\right]\,&lt;/math&gt; 
for ordinary (signed) Stirling numbers of the first kind, and

: &lt;math&gt; \left\{\begin{matrix} n \\ k \end{matrix}\right\} = S(n,k) = S_n^{(k)} \,&lt;/math&gt;
for '''Stirling numbers of the second kind''', which count the number of ways to partition a set of ''n'' elements into ''k'' nonempty subsets.&lt;ref&gt;Ronald L. Graham, Donald E. Knuth, Oren Patashnik (1988) ''[[Concrete Mathematics]]'', Addison-Wesley, Reading MA. {{isbn|0-201-14236-8}}, p.&amp;nbsp;244.&lt;/ref&gt;

For example, the sum &lt;math&gt;\sum_{k=0}^n \left[{n \atop k}\right] = n!&lt;/math&gt; is the number of all permutations,
while the sum &lt;math&gt;\sum_{k=0}^n \left\{\begin{matrix} n \\ k \end{matrix}\right\} = B_n&lt;/math&gt; is the ''n''th [[Bell numbers|Bell number]].

[[Abramowitz and Stegun]] use an uppercase S and a [[blackletter]] S, respectively, for the first and second kinds of Stirling number.  The notation of brackets and braces, in analogy to [[binomial coefficients]], was introduced in 1935 by [[Jovan Karamata]] and promoted later by [[Donald Knuth]].  (The bracket notation conflicts with a common notation for [[Gaussian coefficient]]s.&lt;ref&gt;[[Donald Knuth]]&lt;/ref&gt;)  The mathematical motivation for this type of notation, as well as additional Stirling number formulae, may be found on the page for [[Stirling numbers and exponential generating functions]].

==Expansions of falling and rising factorials==
Stirling numbers express coefficients in expansions of [[falling and rising factorials]] (also known as the Pochhammer symbol) as polynomials.

That is, the '''falling factorial''', defined as &lt;math&gt;(x)_{n} = x(x-1)\cdots(x-n+1)&lt;/math&gt;, is a polynomial in ''x'' of degree ''n'' whose expansion is

:&lt;math&gt;(x)_{n} = \sum_{k=0}^n s(n,k) x^k&lt;/math&gt;

with (signed) Stirling numbers of the first kind as coefficients.

Note that (''x'')&lt;sub&gt;0&lt;/sub&gt; = 1 because it is an [[empty product]]. [[Combinatorics|Combinatorialists]] also sometimes use the notation &lt;math style="vertical-align:baseline;"&gt;x^{\underline{n}}&lt;/math&gt; for the falling factorial, and &lt;math style="vertical-align:baseline;"&gt;x^{\overline{n}}&lt;/math&gt; for the rising factorial.&lt;ref&gt;{{cite book|last=Aigner|first=Martin|title=A Course In Enumeration|publisher=Springer|year=2007|pages=561|chapter=Section 1.2 - Subsets and Binomial Coefficients|isbn=3-540-39032-4}}&lt;/ref&gt; (Confusingly, the Pochhammer symbol that many use for ''falling'' factorials is used in [[special function]]s for ''rising'' factorials.)

Similarly, the '''rising factorial''', defined as &lt;math&gt;x^{(n)} = x(x+1)\cdots(x+n-1)&lt;/math&gt;, is a polynomial in ''x'' of degree ''n'' whose expansion is

:&lt;math&gt;x^{(n)} = \sum_{k=0}^n \left[{n \atop k}\right] x^k&lt;/math&gt;

with unsigned Stirling numbers of the first kind as coefficients.
One expansion can be derived from the other by observing that &lt;math&gt;x^{(n)}=(-1)^n (-x)_{n}&lt;/math&gt;.

Stirling numbers of the second kind express reverse relations:

:&lt;math&gt;x^n = \sum_{k=0}^n \left\{\begin{matrix} n \\ k \end{matrix}\right\}(x)_k&lt;/math&gt;

and

:&lt;math&gt;x^n = \sum_{k=0}^n (-1)^{n-k} \left\{\begin{matrix} n \\ k \end{matrix}\right\}x^{(k)}.&lt;/math&gt;

==As change of basis coefficients==
Considering the set of [[polynomial]]s in the (indeterminate) variable ''x'' as a vector space,
each of the three sequences
:&lt;math&gt;x^0,x^1,x^2,x^3,\cdots \quad (x)_0,(x)_1,(x)_2,\cdots \quad x^{(0)},x^{(1)},x^{(2)},\cdots&lt;/math&gt;
is a [[Basis (linear algebra)|basis]].
That is, every polynomial in ''x'' can be written as a sum &lt;math&gt;a_0 x^{(0)}+a_1 x^{(1)}+\dots+a_n x^{(n)}&lt;/math&gt; for some unique coefficients &lt;math&gt;a_i&lt;/math&gt; (similarly for the other two bases).
The above relations then express the [[change of basis]] between them, as summarized in the following [[commutative diagram]]:
: [[File:Stirling numbers as polynomial basis change.svg|400px]]
The coefficients for the two bottom changes are described by the [[#Lah numbers|Lah numbers]] below.
Since coefficients in any basis are unique, one can define Stirling numbers this way, as the coefficients expressing polynomials of one basis in terms of another, that is, the unique numbers relating &lt;math&gt;x^n&lt;/math&gt; with falling and rising factorials as above.

Falling factorials define, up to scaling, the same polynomials as [[Binomial coefficient#Binomial coefficients as polynomials|binomial coefficients]]: &lt;math&gt;\textstyle \binom{x}{k}=\frac{(x)_k}{k!}&lt;/math&gt;. The changes between the standard basis &lt;math&gt;\textstyle x^0, x^1, x^2, \dots&lt;/math&gt; and the basis &lt;math&gt;\textstyle\binom{x}{0}, \binom{x}{1}, \binom{x}{2}, \dots&lt;/math&gt; are thus described by similar formulas:
: &lt;math&gt;x^n=\sum_{k=0}^n \begin{Bmatrix}n\\k\end{Bmatrix} k! \binom{x}{k}&lt;/math&gt; and &lt;math&gt;\binom{x}{n}=\sum_{k=0}^n \frac{s(n,k)}{n!} x^k&lt;/math&gt;.

===As inverse matrices===
The Stirling numbers of the first and second kinds can be considered inverses of one another:

:&lt;math&gt;\sum_{j\geq 0} s(n,j) S(j,k) = \sum_{j\geq 0} (-1)^{n-j} \begin{bmatrix}n\\j\end{bmatrix} \begin{Bmatrix}j\\k\end{Bmatrix} = \delta_{nk}&lt;/math&gt;

and

:&lt;math&gt;\sum_{j\geq 0}  S(n,j) s(j,k) = \sum_{j\geq 0} (-1)^{j-k} \begin{Bmatrix}n\\j\end{Bmatrix} \begin{bmatrix}j\\k\end{bmatrix} = \delta_{nk},&lt;/math&gt;

where &lt;math&gt;\delta_{nk}&lt;/math&gt; is the [[Kronecker delta]]. These two relationships may be understood to be matrix inverse relationships. That is, let ''s'' be the [[lower triangular matrix]] of Stirling numbers of the first kind, whose matrix elements
&lt;math&gt;s_{nk}=s(n,k).\,&lt;/math&gt;
The [[matrix inverse|inverse]] of this matrix is ''S'', the [[lower triangular matrix]] of Stirling numbers of the second kind, whose entries are &lt;math&gt;S_{nk}=S(n,k).&lt;/math&gt;  Symbolically, this is written

:&lt;math&gt;s^{-1} = S\,&lt;/math&gt;

Although ''s'' and ''S'' are infinite, so calculating a product entry involves an infinite sum, the matrix multiplications work because these matrices are lower triangular, so only a finite number of terms in the sum are nonzero.

===Example===
Expressing a polynomial in the basis of falling factorials is useful for calculating sums of the polynomial evaluated at consecutive integers.
Indeed, the sum of a falling factorial is simply expressed as another falling factorial (for ''k≠-1'')

:&lt;math&gt;\sum_{0\leq i &lt; n} (i)_k = \frac{(n)_{k+1}}{k+1} &lt;/math&gt;

This is analogous to the integral &lt;math&gt;\textstyle\int_0^n x^k = \frac{n^{k+1}}{k+1}&lt;/math&gt;, though the sum should be over integers ''i'' strictly less than ''n''.

For example, the sum of fourth powers of integers up to ''n'' (this time with ''n'' included), is:

:&lt;math&gt;\begin{align}
&amp;\sum_{i=0}^{n} i^4 = \sum_{i=0}^{n} \sum_{k=0}^4 \begin{Bmatrix}4\\k\end{Bmatrix} (i)_4 =  \sum_{k=0}^4 \begin{Bmatrix}4\\k\end{Bmatrix} \frac{(n+1)_{k+1}}{k+1} =\\
&amp; =\frac{\{\begin{smallmatrix}4\\1\end{smallmatrix}\}}{2} (n+1)_{2} + \frac{\{\begin{smallmatrix}4\\2\end{smallmatrix}\}}{3} (n+1)_{3} + \frac{\{\begin{smallmatrix}4\\3\end{smallmatrix}\}}{4} (n+1)_{4} + \frac{\{\begin{smallmatrix}4\\4\end{smallmatrix}\}}{5} (n+1)_{5} =\\
&amp; = \frac{1}{2} (n+1)_{2} + \frac{7}{3} (n+1)_{3} + \frac{6}{4} (n+1)_{4} + \frac{1}{5} (n+1)_{5}
\end{align}&lt;/math&gt;

Here the Stirling numbers can be computed from their definition as the number of partitions of 4 elements into ''k'' non-empty unlabeled subsets.

In contrast, the sum &lt;math&gt;\sum_{i=0}^{n} i^k&lt;/math&gt; in the standard basis is given by [[Faulhaber's formula]], which in general is more complex.

==Lah numbers==
{{main|Lah numbers}}

The Lah numbers &lt;math&gt;L(n,k) = {n-1 \choose k-1} \frac{n!}{k!}&lt;/math&gt; are sometimes called Stirling numbers of the third kind.&lt;ref&gt;{{cite book
 | last1=Sándor | first1=Jozsef | last2=Crstici | first2=Borislav
 | title=Handbook of Number Theory II  | publisher=[[Kluwer Academic Publishers]] | year=2004
 | url=https://books.google.com/books?id=B2WZkvmFKk8C&amp;pg=PA464&amp;lpg=PA464&amp;dq=%22Stirling+numbers+of+the+third+kind%22&amp;source=bl&amp;ots=JhIJKIhaFH&amp;sig=_0-CWfixhUoAuhh7DAo4fJco6y4&amp;hl=en&amp;ei=BKh2TfnBJ_KH0QGn17XZBg&amp;sa=X&amp;oi=book_result&amp;ct=result&amp;resnum=2&amp;ved=0CCAQ6AEwAQ#v=onepage&amp;q=%22Stirling%20numbers%20of%20the%20third%20kind%22&amp;f=f | isbn=9781402025464 | page=464}}&lt;/ref&gt;
By convention, &lt;math&gt;L(0,0)=1&lt;/math&gt; and &lt;math&gt;L(n,k)=0&lt;/math&gt; if &lt;math&gt;n&gt;k&lt;/math&gt; or &lt;math&gt;k = 0 &lt; n&lt;/math&gt;.

These numbers are coefficients expressing falling factorials in terms of rising factorials and vice versa:
:&lt;math&gt;x^{(n)} = \sum_{k=0}^n L(n,k) (x)_k\quad&lt;/math&gt; and &lt;math&gt;\quad(x)_n = \sum_{k=0}^n (-1)^{n-k} L(n,k)x^{(k)}.&lt;/math&gt;

As above, this means they express the change of basis between the bases &lt;math&gt;(x)_0,(x)_1,(x)_2,\cdots&lt;/math&gt; and &lt;math&gt;x^{(0)},x^{(1)},x^{(2)},\cdots&lt;/math&gt;, completing the diagram.
In particular, one formula is the inverse of the other, thus:

: &lt;math&gt;\sum_{j} L(n,j) \cdot (-1)^{j-k} L(j,k) = \delta_{nk}.&lt;/math&gt;

Similarly, composing for example the change of basis from &lt;math&gt;x^{(n)}&lt;/math&gt; to &lt;math&gt;x^n&lt;/math&gt; with the change of basis from &lt;math&gt;x^n&lt;/math&gt; to &lt;math&gt;(x)_{n}&lt;/math&gt; gives the change of basis directly from &lt;math&gt;x^{(n)}&lt;/math&gt; to &lt;math&gt;(x)_{n}&lt;/math&gt;:

:&lt;math&gt; L(n,k) = \sum_{j} \begin{bmatrix}n\\j\end{bmatrix} \begin{Bmatrix}j\\k\end{Bmatrix},&lt;/math&gt;

In terms of matrices, if &lt;math&gt;L&lt;/math&gt; denotes the matrix with entries &lt;math&gt;L_{nk}=L(n,k)&lt;/math&gt; and &lt;math&gt;L^{-}&lt;/math&gt; denotes the matrix with entries  &lt;math&gt;L^{-}_{nk}=(-1)^{n-k}L(n,k)&lt;/math&gt;, then one is the inverse of the other: &lt;math&gt; L^{-} = L^{-1}&lt;/math&gt;.
Similarly, composing the matrix of unsigned Stirling numbers of the first kind with the matrix of Stirling numbers of the second kind gives the Lah numbers: &lt;math&gt;L = |s| \cdot S&lt;/math&gt;.

The numbers &lt;math&gt;\textstyle\begin{Bmatrix}n\\k\end{Bmatrix}, \begin{bmatrix}n\\k\end{bmatrix}, L(n,k)&lt;/math&gt; can be defined as the number of partitions of ''n'' elements into ''k'' non-empty unlabeled subsets, each of which is unordered, [[Cyclic order|cyclically ordered]], or linearly ordered, respectively. In particular, this implies the following inequalities:
: &lt;math&gt;\begin{Bmatrix}n\\k\end{Bmatrix} \leq \begin{bmatrix}n\\k\end{bmatrix} \leq L(n,k).&lt;/math&gt;

==Symmetric formulae==

Abramowitz and Stegun give the following symmetric formulae that relate the Stirling numbers of the first and second kind.&lt;ref&gt;{{Citation|editor-last1=Abramowitz|editor-first1=Milton|editor-last2=Stegun|editor-first2=Irene A.|last1=Goldberg|first1=K.|last2=Newman|first2=M|last3=Haynsworth|first3=E.|title=Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical Tables, 10th printing|contribution=Stirling Numbers of the First Kind, Stirling Numbers of the Second Kind|publisher=Dover|year=1972|pages=824–825|location=New York}}&lt;/ref&gt;

:&lt;math&gt;s(n,k) = \sum_{j=0}^{n-k} (-1)^j {n-1+j \choose n-k+j} {2n-k \choose n-k-j} S(n-k+j,j)&lt;/math&gt;

and

:&lt;math&gt;S(n,k) = \sum_{j=0}^{n-k} (-1)^j {n-1+j \choose n-k+j} {2n-k \choose n-k-j} s(n-k+j,j).&lt;/math&gt;

==Stirling numbers with negative integral values==
The Stirling numbers can be extended to negative integral values, but not all authors do so in the same way.&lt;ref name="Loeb"&gt;{{cite journal |last1=Loeb |first1=Daniel E.|orig-year=Received 3 Nov 1989|title=A generalization of the Stirling numbers |url=https://ac.els-cdn.com/0012365X9290318A/1-s2.0-0012365X9290318A-main.pdf?_tid=e9349704-665b-47c4-812a-f7fcae4daa54&amp;acdnat=1522093775_a67055edb2b0c77d5ff34bd3e07c7c8f |journal=Discrete Mathematics |volume=103 |issue= |pages=259–269 |doi= |year=1992 |access-date=26 Mar 2018}}&lt;/ref&gt;&lt;ref name=":0"&gt;{{cite web |url=http://www.fq.math.ca/Scanned/34-3/branson.pdf |title=An extension of Stirling numbers |last=Branson |first=David |date=August 1994 |website=The Fibonacci Quarterly |access-date=Dec 6, 2017 |quote=}}&lt;/ref&gt;&lt;ref name=":1"&gt;D.E. Knuth, 1992.&lt;/ref&gt; Regardless of the approach taken, it is worth noting that Stirling numbers of first and second kind are connected by the relations:

: &lt;math&gt; \left[{n \atop k}\right]=\left\{\begin{matrix} -k \\ -n \end{matrix}\right\}\, \qquad\text{and}\qquad \left\{\begin{matrix} n \\ k \end{matrix}\right\}=\left[{-k \atop -n}\right]&lt;/math&gt;
when ''n'' and ''k'' are nonnegative integers.  So we have following table for &lt;math&gt;\left[{-n \atop -k}\right]&lt;/math&gt;:
{| cellspacing="0" cellpadding="5" style="text-align:right;" class="wikitable"
|-
| {{diagonal split header|''n''|''k''}}
! {{rh|align=right}} | −1
! {{rh|align=right}} | −2
! {{rh|align=right}} | −3
! {{rh|align=right}} | −4
! {{rh|align=right}} | −5
|-
! {{rh|align=right}} | −1
| 1
| 1
| 1
| 1
| 1
|-
! {{rh|align=right}} | −2
| 0
| 1
| 3
| 7
| 15
|-
! {{rh|align=right}} | −3
| 0
| 0
| 1
| 6
| 25
|-
! {{rh|align=right}} | −4
| 0
| 0
| 0
| 1
| 10
|-
! {{rh|align=right}} | −5
| 0
| 0
| 0
| 0
| 1
|-
 |}

Donald Knuth&lt;ref name=":1" /&gt; defined the more general Stirling numbers by extending a [[Stirling numbers of the second kind#Recurrence relation|recurrence relation]] to all integers.  In this approach, &lt;math&gt; \left[{n \atop k}\right]&lt;/math&gt; and &lt;math&gt; \left\{\begin{matrix} n \\ k \end{matrix}\right\}&lt;/math&gt; are zero if ''n'' is negative and ''k'' is nonnegative, or if ''n'' is nonnegative and ''k'' is negative, and so we have, for ''any'' integers ''n'' and ''k'',
: &lt;math&gt; \left[{n \atop k}\right]=\left\{\begin{matrix} -k \\ -n \end{matrix}\right\}\, \qquad\text{and}\qquad \left\{\begin{matrix} n \\ k \end{matrix}\right\}=\left[{-k \atop -n}\right]&lt;/math&gt;.

On the other hand, for positive integers ''n'' and ''k'', David Branson&lt;ref name=":0" /&gt; defined &lt;math&gt; \left[{-n \atop -k}\right]&lt;/math&gt;, &lt;math&gt; \left\{\begin{matrix} -n \\ -k \end{matrix}\right\}&lt;/math&gt;, 
&lt;math&gt; \left[{{-n} \atop k}\right]&lt;/math&gt;, 
and &lt;math&gt; \left\{\begin{matrix} -n \\ k \end{matrix}\right\}&lt;/math&gt; (but not &lt;math&gt; \left[{n \atop -k}\right]&lt;/math&gt; or &lt;math&gt; \left\{\begin{matrix} n \\ -k \end{matrix}\right\}&lt;/math&gt;). In this approach, one has the following extension of the [[Stirling numbers of the second kind#Recurrence relation|recurrence relation]] of the Stirling numbers of the first kind:

: &lt;math&gt; \left[{-n \atop k}\right]=\frac{(-1)^{n+1}}{n!}\sum_{i=1}^{n}(-1)^{i+1}\frac{n \choose i}{i^k} &lt;/math&gt;,

For example, &lt;math&gt; \left[{-5 \atop k}\right]=\frac{5-\frac{10}{2^k}+\frac{10}{3^k}-\frac 5{4^k}+\frac 1{5^k}}{120} &lt;/math&gt;.  This leads to the following table of values of &lt;math&gt;\left[{-n \atop k}\right]&lt;/math&gt;.
{| cellspacing="0" cellpadding="5" style="text-align:center;" class="wikitable"
|-
| {{diagonal split header|''n''|''k''}}
! 0
! 1
! 2
! 3
! 4
|-
! −1
| 1
| 1
| 1
| 1
| 1
|-
! −2
| &lt;math&gt;\tfrac{-1}{2}&lt;/math&gt;
| &lt;math&gt;\tfrac{-3}{4}&lt;/math&gt;
| &lt;math&gt;\tfrac{-7}{8}&lt;/math&gt;
| &lt;math&gt;\tfrac{-15}{16}&lt;/math&gt;
| &lt;math&gt;\tfrac{-31}{32}&lt;/math&gt;
|-
! −3
| &lt;math&gt;\tfrac{1}{6}&lt;/math&gt;
| &lt;math&gt;\tfrac{11}{36}&lt;/math&gt;
| &lt;math&gt;\tfrac{85}{216}&lt;/math&gt;
| &lt;math&gt;\tfrac{575}{1296}&lt;/math&gt;
| &lt;math&gt;\tfrac{3661}{7776}&lt;/math&gt;
|-
! −4
| &lt;math&gt;\tfrac{-1}{24}&lt;/math&gt;
| &lt;math&gt;\tfrac{-25}{288}&lt;/math&gt;
| &lt;math&gt;\tfrac{-415}{3456}&lt;/math&gt;
| &lt;math&gt;\tfrac{-5845}{41472}&lt;/math&gt;
| &lt;math&gt;\tfrac{-76111}{497664}&lt;/math&gt;
|-
! −5
| &lt;math&gt;\tfrac{1}{120}&lt;/math&gt;
| &lt;math&gt;\tfrac{137}{7200}&lt;/math&gt;
| &lt;math&gt;\tfrac{12019}{432000}&lt;/math&gt;
| &lt;math&gt;\tfrac{874853}{25920000}&lt;/math&gt;
| &lt;math&gt;\tfrac{58067611}{1555200000}&lt;/math&gt;
|-
 |}

In this case &lt;math&gt;\sum_{n=-1}^{-\infty}\left[{-n \atop -k}\right]=B_{k} &lt;/math&gt; where &lt;math&gt;B_{k}&lt;/math&gt; is a [[Bell number]], and so one may define the negative Bell numbers by &lt;math&gt;\sum_{n=-1}^{-\infty}\left[{-n \atop k}\right]=B_{-k}&lt;/math&gt;.  For example, this produces &lt;math&gt;\sum_{n=-1}^{-\infty}\left[{-n \atop 2}\right]=B_{-2}=0.421773\ldots&lt;/math&gt;.

==See also==
* [[Bell polynomials]]
* [[Cycles and fixed points]]
* [[Lah number]]
* [[Pochhammer symbol]]
* [[Polynomial sequence]]
* [[Stirling transform]]
* [[Touchard polynomials]]

==References==
{{Reflist}}

==Further reading==
* {{cite journal|first=Victor|last=Adamchik|url=http://www-2.cs.cmu.edu/~adamchik/articles/stirling.pdf|title=On Stirling Numbers and Euler Sums|journal=Journal of Computational and Applied Mathematics|volume=79|year=1997|pages=119–130|doi=10.1016/s0377-0427(96)00167-7}}
* {{cite journal|first1=Arthur T.|last1=Benjamin|first2=Gregory O.|last2=Preston|first3=Jennifer J.|last3=Quinn|url=http://www.math.hmc.edu/~benjamin/papers/harmonic.pdf|title=A Stirling Encounter with Harmonic Numbers|date=2002|journal=Mathematics Magazine|volume=75|number=2|pages=95–103|doi=10.2307/3219141}}
* {{cite journal|url=http://www.maa.org/sites/default/files/pdf/upload_library/2/Boyadzhiev-2013.pdf|first=Khristo N.|last=Boyadzhiev|title=Close encounters with the Stirling numbers of the second kind|date=2012|journal=Mathematics Magazine|volume=85|number=4|pages=252–266|doi=10.4169/math.mag.85.4.252}}
* {{cite journal|first=Louis|last=Comtet|url=http://www.techniques-ingenieur.fr/page/af202niv10002/permutations.html#2.2|title=Valeur de ''s''(''n'',&amp;nbsp;''k'')''|journal=Analyse combinatoire, Tome second|page=51|publisher=Presses universitaires de France|year=1970|language=fr|url-access=subscription}}
* {{cite book|first=Louis|last=Comtet|title=Advanced Combinatorics: The Art of Finite and Infinite Expansions|publisher=Reidel Publishing Company|location=Dordrecht-Holland/Boston-U.S.A.|date=1974}}
* {{cite journal| author=Hsien-Kuei Hwang |title=Asymptotic Expansions for the Stirling Numbers of the First Kind |journal=Journal of Combinatorial Theory, Series A |volume=71 |issue=2 |pages=343–351 |year=1995 |url=http://citeseer.ist.psu.edu/577040.html |doi=10.1016/0097-3165(95)90010-1}}
* {{citation |author-link=Donald Knuth|first=D.E.|last=Knuth| journal = Amer. Math. Monthly | title=Two notes on notation| year=1992 | volume=99 | pages=403-422 | arxiv=math/9205211 | doi=10.2307/2325085 | jstor= 2325085}}
* {{cite journal|first=Francis L.|last=Miksa|jstor=2002617|title=Stirling numbers of the first kind: 27 leaves reproduced from typewritten manuscript on deposit in the UMT File|journal=Mathematical Tables and Other Aids to Computation: Reviews and Descriptions of Tables and Books|volume=10|number=53|date=January 1956|pages=37–38}}
* {{cite book|editor1-first=Milton|editor1-last=Abramowitz|editor2-first=Irene A.|editor2-last=Stegun|url=http://www.convertit.com/Go/ConvertIt/Reference/AMS55.ASP|title=Handbook of Mathematical Functions (with Formulas, Graphs and Mathematical Tables)|publisher=U.S. Dept. of Commerce, National Bureau of Standards, Applied Math.|series=55|orig-year=1964|year=1972|chapter=Combinatorial Analysis, Table 24.4, Stirling Numbers of the Second Kind|first=Francis L.|last=Miksa|page=835}}
* {{cite journal|first=Dragoslav S.|last=Mitrinović|url=http://pefmath2.etf.bg.ac.rs/files/23/23.pdf|title=Sur les nombres de Stirling de première espèce et les polynômes de Stirling|journal=Publications de la Faculté d'Electrotechnique de l'Université de Belgrade, Série Mathématiques et Physique|language=fr|issn=0522-8441|number=23|date=1959|pages=1–20}}
* {{cite web|first1=John J.|last1=O'Connor|first2=Edmund F.|last2=Robertson|url=http://www-history.mcs.st-andrews.ac.uk/history/Biographies/Stirling.html|title=James Stirling (1692&amp;ndash;1770)|date=September 1998}}
* {{cite journal| first1=J. M. |last1=Sixdeniers |first2= K. A. |last2=Penson
|first3=A. I. |last3= Solomon | url = http://www.cs.uwaterloo.ca/journals/JIS/VOL4/SIXDENIERS/bell.pdf
|title= Extended Bell and Stirling Numbers From Hypergeometric Exponentiation
|year=2001
|journal = Journal of Integer Sequences | volume= 4 | pages=01.1.4}}
* {{cite news| first1=Michael Z. | last1=Spivey | title=Combinatorial sums and finite differences
|doi=10.1016/j.disc.2007.03.052 | journal=Discrete Math.
|year=2007 | volume=307 | number=24 | pages=3130–3146}}
* {{Cite OEIS|sequencenumber=A008275|name=Stirling numbers of first kind}}
* {{Cite OEIS|sequencenumber=A008277|name=Stirling numbers of 2nd kind}}
* {{planetmath reference |id=2809|title=Stirling numbers of the first kind, s(n,k)}}
* {{planetmath reference |id=2805|title=Stirling numbers of the second kind, S(n,k)}}

[[Category:Permutations]]
[[Category:Q-analogs]]
[[Category:Factorial and binomial topics]]
[[Category:Integer sequences]]</text>
      <sha1>f344y4qeuznpenymtncj1zav7nvu8s0</sha1>
    </revision>
  </page>
  <page>
    <title>Ternary Golay code</title>
    <ns>0</ns>
    <id>3070794</id>
    <revision>
      <id>850878107</id>
      <parentid>800777793</parentid>
      <timestamp>2018-07-18T15:19:35Z</timestamp>
      <contributor>
        <username>Matthiaspaul</username>
        <id>13467261</id>
      </contributor>
      <comment>parked anchor for redirects</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4861">{{anchor|Perfect ternary Golay code}}{{infobox code
 | name           = Perfect ternary Golay code
 | image          =
 | image_caption  =
 | namesake       = [[Marcel J. E. Golay]]
 | type           = [[Linear block code]]
 | block_length   = 11
 | message_length = 6
 | rate           = 6/11 ~ 0.545
 | distance       = 5
 | alphabet_size  = 3
 | notation       = &lt;math&gt;[11,6,5]_3&lt;/math&gt;-code
}}
{{infobox code
 | name           = Extended ternary Golay code
 | image          =
 | image_caption  =
 | namesake       = [[Marcel J. E. Golay]]
 | type           = [[Linear block code]]
 | block_length   = 12
 | message_length = 6
 | rate           = 6/12 = 0.5 
 | distance       = 6
 | alphabet_size  = 3
 | notation       = &lt;math&gt;[12,6,6]_3&lt;/math&gt;-code
}}

In [[coding theory]], the '''ternary Golay codes''' are two closely related [[error-correcting code]]s.
The code generally known simply as the '''ternary Golay code''' is an &lt;math&gt;[11, 6, 5]_3&lt;/math&gt;-code, that is, it is a [[linear code]] over a [[ternary numeral system|ternary]] alphabet; the [[Block_code#The_distance_d|relative distance]] of the code is as large as it possibly can be for a ternary code, and hence, the ternary Golay code is a [[perfect code]].
The '''extended ternary Golay code''' is a [12, 6, 6] [[linear code]] obtained by adding a zero-sum [[check digit]] to the [11, 6, 5] code.
In finite [[group theory]], the extended ternary Golay code is sometimes referred to as the ternary Golay code.{{citation missing|date=February 2013}}

== Properties ==

=== Ternary Golay code ===
The ternary Golay code consists of 3&lt;sup&gt;6&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;729 codewords. 
Its [[parity check matrix]] is
:&lt;math&gt;

\left[
\begin{array}{ccccccccccc}

1 &amp; 1 &amp; 1 &amp; 2 &amp; 2 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0\\

1 &amp; 1 &amp; 2 &amp; 1 &amp; 0 &amp; 2 &amp; 0 &amp; 1 &amp; 0 &amp; 0 &amp; 0\\

1 &amp; 2 &amp; 1 &amp; 0 &amp; 1 &amp; 2 &amp; 0 &amp; 0 &amp; 1 &amp; 0 &amp; 0\\

1 &amp; 2 &amp; 0 &amp; 1 &amp; 2 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp; 0\\

1 &amp; 0 &amp; 2 &amp; 2 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 &amp; 0 &amp; 1

\end{array}
\right].&lt;/math&gt;
Any two different codewords differ in at least 5 positions.
Every ternary word of length 11 has a [[Hamming distance]] of at most 2 from exactly one codeword.
The code can also be constructed as the [[quadratic residue code]] of length 11 over the [[finite field]] '''F'''&lt;sub&gt;3&lt;/sub&gt;.  

Used in a [[football pool]] with 11 games, the ternary Golay code corresponds to 729 bets and guarantees exactly one bet with at most 2 wrong outcomes.

The set of codewords with Hamming weight 5 is a 3-(11,5,4) [[t-design|design]].

=== Extended ternary Golay code ===
The [[enumerator polynomial|complete weight enumerator]] of the extended ternary Golay code is
:&lt;math&gt;x^{12}+y^{12}+z^{12}+22\left(x^6y^6+y^6z^6+z^6x^6\right)+220\left(x^6y^3z^3+y^6z^3x^3+z^6x^3y^3\right).&lt;/math&gt;

The [[automorphism group]] of the extended ternary Golay code is 2.''M''&lt;sub&gt;12&lt;/sub&gt;, where ''M''&lt;sub&gt;12&lt;/sub&gt; is the [[Mathieu group M12]].  

The extended ternary Golay code can be constructed as the span of the rows of a [[Hadamard matrix]] of order 12 over the field '''F'''&lt;sub&gt;3&lt;/sub&gt;.  

Consider all codewords of the extended code which have just six nonzero digits.  The sets of positions at which these nonzero digits occur form the [[Steiner system]] S(5, 6, 12).

== History ==

The ternary Golay code was discovered by {{harvs|txt|authorlink=Marcel J. E. Golay|last=Golay|year=1949}}.  It was independently discovered two years earlier by the [[Finland|Finnish]] football pool enthusiast [[Juhani Virtakallio]], who published it in 1947 in issues 27, 28 and 33 of the football [[magazine]] ''[[Veikkaaja]]''. {{harv|Barg|1993|loc=p.25}}

== See also ==

* [[Binary Golay code]]

==References==

*{{Citation | last1=Barg | first1=Alexander | title=At the dawn of the theory of codes | url=https://dx.doi.org/10.1007/BF03025254 | doi=10.1007/BF03025254 |mr=1199273 | year=1993 | journal=[[The Mathematical Intelligencer]] | issn=0343-6993 | volume=15 | issue=1 | pages=20–26}}
* M.J.E. Golay, Notes on digital coding, ''Proceedings of the [[Institute of Radio Engineers|I.R.E.]]'' 37 (1949) 657
* I.F. Blake (ed.), ''Algebraic Coding Theory: History and Development'', Dowden, Hutchinson &amp; Ross, Stroudsburg 1973
* [[J. H. Conway]] and [[N. J. A. Sloane]], ''[[Sphere packing|Sphere Packings]], [[Lattice (group)|Lattices]] and [[Group (mathematics)|Groups]]'', [[Springer Science+Business Media|Springer]], New York, Berlin, Heidelberg, 1988. 
* [[Robert L. Griess]], ''Twelve [[Sporadic group|Sporadic Groups]]'', Springer, 1998.
* G. Cohen, I. Honkala, S. Litsyn, A. Lobstein, ''Covering Codes'', [[Elsevier]] (1997) {{ISBN|0-444-82511-8}}
* Th. M. Thompson, ''From Error Correcting Codes through Sphere Packings to [[Simple_group|Simple Groups]]'', [[The Mathematical Association of America]] 1983, {{ISBN|0-88385-037-0}}

[[Category:Coding theory]]
[[Category:Finite fields]]

[[ja:3元ゴレイ符号]]</text>
      <sha1>3v2mgaa89pvybezwj3qa274gh7webs1</sha1>
    </revision>
  </page>
  <page>
    <title>Ulam number</title>
    <ns>0</ns>
    <id>8711785</id>
    <revision>
      <id>850160191</id>
      <parentid>846800587</parentid>
      <timestamp>2018-07-14T01:28:07Z</timestamp>
      <contributor>
        <ip>2601:142:3:F83A:1CA3:73E8:43EA:48E</ip>
      </contributor>
      <comment>/* Examples */ just a random intersection of two categories, not obviously notable or interesting</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7996">An '''Ulam number''' is a member of an [[integer sequence]] devised by and named after [[Stanislaw Ulam]], who introduced it in 1964.&lt;ref&gt;{{harvs|last=Ulam|year=1964a|year2=1964b|txt|authorlink=Stanislaw Ulam}}.&lt;/ref&gt;  The standard Ulam sequence (the (1,&amp;nbsp;2)-Ulam  sequence) starts with ''U''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;1 and ''U''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;2.  Then for ''n''&amp;nbsp;&gt;&amp;nbsp;2, ''U''&lt;sub&gt;''n''&lt;/sub&gt; is defined to be the smallest [[integer]] that is the sum of two distinct earlier terms in exactly one way and larger than all earlier terms.

==Examples==
As a consequence of the definition, 3 is an Ulam number (1+2); and 4 is an Ulam number (1+3). (Here 2+2 is not a second representation of 4, because the previous terms must be distinct.)  The integer 5 is not an Ulam number, because 5&amp;nbsp;=&amp;nbsp;1&amp;nbsp;+&amp;nbsp;4&amp;nbsp;=&amp;nbsp;2&amp;nbsp;+&amp;nbsp;3.  The first few terms are
:1, 2, 3, 4, 6, 8, 11, 13, 16, 18, 26, 28, 36, 38, 47, 48, 53, 57, 62, 69, 72, 77, 82, 87, 97, 99, 102, 106, 114, 126, 131, 138, 145, 148, 155, 175, 177, 180, 182, 189, 197, 206, 209, 219, 221, 236, 238, 241, 243, 253, 258, 260, 273, 282, ... {{OEIS|id=A002858}}.

There are infinitely many Ulam numbers. For, after the first ''n'' numbers in the sequence have already been determined, it is always possible to extend the sequence by one more element: {{nowrap|''U''&lt;sub&gt;''n'' &amp;minus; 1&lt;/sub&gt; + ''U''&lt;sub&gt;''n''&lt;/sub&gt;}} is uniquely represented as a sum of two of the first ''n'' numbers, and there may be other smaller numbers that are also uniquely represented in this way, so the next element can be chosen as the smallest of these uniquely representable numbers.&lt;ref&gt;{{harvtxt|Recaman|1973}} gives a similar argument, phrased as a [[proof by contradiction]]. He states that, if there were finitely many Ulam numbers, then the sum of the last two would also be an Ulam number &amp;ndash; a contradiction. However, although the sum of the last two numbers would in this case have a unique representation as a sum of two Ulam numbers, it would not necessarily be the smallest number with a unique representation.&lt;/ref&gt;

Ulam is said to have conjectured that the numbers have zero [[Natural density|density]],&lt;ref&gt;The statement that Ulam made this conjecture is in OEIS {{OEIS2C|id=A002858}}, but Ulam does not address the density of this sequence in {{harvtxt|Ulam|1964a}}, and in {{harvtxt|Ulam|1964b}} he poses the question of determining its density without conjecturing a value for it. {{harvtxt|Recaman|1973}} repeats the question from {{harvtxt|Ulam|1964b}} of the density of this sequence, again without conjecturing a value for it.&lt;/ref&gt; but they seem to have a density of approximately 0.07398.&lt;ref&gt;OEIS {{OEIS2C|id=A002858}}&lt;/ref&gt;

==Hidden structure==
It has been observed&lt;ref&gt;{{harvtxt|Steinerberger|2015}}&lt;/ref&gt; that the first 10 million Ulam numbers satisfy &lt;math&gt; \cos{(2.5714474995 a_n)} &lt; 0 &lt;/math&gt; except for the four elements &lt;math&gt; \left\{2,3,47,69\right\} &lt;/math&gt; (this has now been verified up to &lt;math&gt;n = 10^9&lt;/math&gt;). Inequalities of this type are usually true for sequences exhibiting some form of periodicity but the Ulam sequence does not seem to be periodic and the phenomenon is not understood. It can be exploited to do a fast computation of the Ulam sequence (see external links).

==Generalizations==
The idea can be generalized as (''u'',&amp;nbsp;''v'')-Ulam numbers by selecting different starting values (''u'',&amp;nbsp;''v''). A sequence of (''u'',&amp;nbsp;''v'')-Ulam numbers is ''regular'' if the sequence of differences between consecutive numbers in the sequence is eventually periodic. When ''v'' is an odd number greater than three, the (2,&amp;nbsp;''v'')-Ulam numbers are regular. When ''v'' is congruent to 1 (mod 4) and at least five, the (4,&amp;nbsp;''v'')-Ulam numbers are again regular. However, the Ulam numbers themselves do not appear to be regular.&lt;ref&gt;{{harvtxt|Queneau|1972}} first observed the regularity of the sequences for ''u''&amp;nbsp;=&amp;nbsp;2 and ''v''&amp;nbsp;=&amp;nbsp;7 and ''v''&amp;nbsp;=&amp;nbsp;9. {{harvtxt|Finch|1992}} conjectured the extension of this result to all odd ''v'' greater than three, and this conjecture was proven by {{harvtxt|Schmerl|Spiegel|1994}}. The regularity of the (4,&amp;nbsp;''v'')-Ulam numbers was proven by {{harvtxt|Cassaigne|Finch|1995}}.&lt;/ref&gt;

A sequence of numbers is said to be ''s''-additive if each number in the sequence, after the initial 2''s'' terms of the sequence, has exactly ''s'' representations as a sum of two previous numbers. Thus, the Ulam numbers and the (''u'',&amp;nbsp;''v'')-Ulam numbers are 1-additive sequences.&lt;ref&gt;{{harvtxt|Queneau|1972}}.&lt;/ref&gt;

If a sequence is formed by appending the largest number with a unique representation as a sum of two earlier numbers, instead of appending the smallest uniquely representable number, then the resulting sequence is the sequence of [[Fibonacci number]]s.&lt;ref&gt;{{harvtxt|Finch|1992}}.&lt;/ref&gt;

==Notes==
{{reflist}}

==References==
*{{citation
 | last = Cassaigne | first = Julien
 | last2 = Finch | first2 = Steven R.
 | issue = 1
 | journal = Experimental Mathematics
 | pages = 49–60
 | title = A class of 1-additive sequences and quadratic recurrences
 | url = http://www.emis.ams.org/journals/EM/restricted/4/4.1/finch.ps
 | volume = 4
 | year = 1995
 | doi = 10.1080/10586458.1995.10504307
 | mr = 1359417}}
*{{citation
 | last = Finch | first = Steven R.
 | doi = 10.1016/0097-3165(92)90042-S
 | issue = 1
 | journal = Journal of Combinatorial Theory, Series A
 | pages = 123–130
 | title = On the regularity of certain 1-additive sequences
 | volume = 60
 | year = 1992
 | mr = 1156652}}
*{{Citation
|last=Guy|first=Richard|authorlink=Richard K. Guy
|year=2004
|title=Unsolved Problems in Number Theory
|edition=3rd
|publisher=[[Springer-Verlag]]
|isbn= 0-387-20860-7
|pages=166–167}}
*{{citation
 | last = Queneau | first = Raymond
 | doi = 10.1016/0097-3165(72)90083-0
 | issue = 1
 | journal = Journal of Combinatorial Theory, Series A
 | language = fr
 | pages = 31–71
 | title = Sur les suites ''s''-additives
 | volume = 12
 | year = 1972
 | mr = 0302597}}
*{{citation
 | last = Recaman | first = Bernardo
 | issue = 8
 | journal = American Mathematical Monthly
 | pages = 919–920
 | title = Questions on a sequence of Ulam
 | jstor = 2319404
 | volume = 80
 | year = 1973
 | mr = 1537172
 | doi = 10.2307/2319404}}
*{{citation
 | last = Schmerl | first = James
 | last2 = Spiegel | first2 = Eugene
 | year = 1994
 | doi = 10.1016/0097-3165(94)90058-2
 | issue = 1
 | journal = Journal of Combinatorial Theory, Series A
 | pages = 172–175
 | title = The regularity of some 1-additive sequences
 | volume = 66
 | mr = 1273299}}
*{{citation
 | last = Ulam | first = Stanislaw | authorlink = Stanislaw Ulam
 | journal = SIAM Review
 | pages = 343–355
 | title = Combinatorial analysis in infinite sets and some physical theories
 | volume = 6
 | jstor = 2027963
 | doi = 10.1137/1006090
 | year = 1964a
 | mr = 0170832}}
*{{citation | last = Ulam | first = Stanislaw | authorlink = Stanislaw Ulam
| title = Problems in Modern Mathematics
| publisher = John Wiley &amp; Sons, Inc
| location = New York
| year = 1964b
| page = xi
| mr = 0280310}}
*{{citation|arxiv=1507.00267|first=Stefan|
last=Steinerberger
| year = 2015
|title=A Hidden Signal in the Ulam sequence
| publisher =Experimental Mathematics|bibcode=2015arXiv150700267S}}



==External links==
* [http://mathworld.wolfram.com/UlamSequence.html Ulam Sequence from MathWorld]
* [http://vixra.org/abs/1508.0085 Fast computation of the Ulam sequence by Philip Gibbs]&lt;!-- viXra sources tend to be unreliable, but this one has been reviewed favourably by Knuth's paper below, so it can stay per [[WP:USEBYOTHERS]] --&gt;
* [http://www-cs-faculty.stanford.edu/~uno/ulam-gibbs.ps Description of Algorithm by [[Donald Knuth]]]
* [https://github.com/daniel3735928559/wip-ulam The github page of Daniel Ross]

{{Classes of natural numbers |state=collapsed}}
{{DEFAULTSORT:Ulam Number}}
[[Category:Integer sequences]]</text>
      <sha1>6fyqezucxqnb17fbrtyklcbf90ldaqi</sha1>
    </revision>
  </page>
  <page>
    <title>Unique sink orientation</title>
    <ns>0</ns>
    <id>18476346</id>
    <revision>
      <id>866029559</id>
      <parentid>866022705</parentid>
      <timestamp>2018-10-27T20:22:49Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>Undid revision 866022705 by [[Special:Contributions/Kirbanzo|Kirbanzo]] ([[User talk:Kirbanzo|talk]]) this article has plenty of inline citations in a recognized and valid referencing format. See [[WP:HARV]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5238">In [[mathematics]], a '''unique sink orientation''' is an orientation of the edges of a [[polytope]] such that, in every face of the polytope (including the whole polytope as one of the faces), there is exactly one [[vertex (graph theory)|vertex]] for which all adjoining edges are oriented inward (i.e. towards that vertex). If a polytope is given together with a linear objective function, and edges are oriented from vertices with smaller objective function values to vertices with larger objective values, the result is a unique sink orientation. Thus, unique sink orientations can be used to model [[linear program]]s as well as certain nonlinear programs such as the [[smallest circle problem]].

==In hypercubes==
The problem of finding the sink in a unique sink orientation of a hypercube was formulated as an abstraction of [[linear complementarity problem]]s by {{harvtxt|Stickney|Watson|1978}}.
It is possible for an [[algorithm]] to determine the unique sink of a {{mvar|d}}-dimensional hypercube in time {{math|''c''&lt;sup&gt;''d''&lt;/sup&gt;}} for {{math|''c'' &lt; 2}}, substantially smaller than the {{math|2&lt;sup&gt;''d''&lt;/sup&gt;}} time required to examine all vertices. When the orientation has the additional property that the orientation forms a [[directed acyclic graph]], which happens when unique sink orientations are used to model [[LP-type problem]]s, it is possible to find the sink using a randomized algorithm in expected time exponential in the square root of ''d'' {{harv|Szabó|Welzl|2001}}.

==In simple polytopes==
A [[Simple polytope|simple ''d''-dimensional polytope]] is a polytope in which every vertex has exactly ''d'' incident edges. In a unique-sink orientation of a simple polytope, every subset of ''k'' incoming edges at a vertex ''v'' determines a ''k''-dimensional face for which ''v'' is the unique sink. Therefore, the number of faces of all dimensions of the polytope (including the polytope itself, but not the empty set) can be computed by the sum of the number of subsets of incoming edges,
:&lt;math&gt;\sum_{v\in G(P)} 2^{d_{\operatorname{in}}(v)}&lt;/math&gt;
where ''G''(''P'') is the graph of the polytope, and ''d''&lt;sub&gt;in&lt;/sub&gt;(''v'') is the [[degree (graph theory)|in-degree]] (number of incoming edges) of a vertex ''v'' in the given orientation {{harv|Kalai|1988}}.

More generally, for any orientation of a simple polytope, the same sum counts the number of incident pairs of a face of the polytope and a sink of the face. And in an [[acyclic orientation]], every face must have at least one sink. Therefore, an acyclic orientation is a unique sink orientation if and only if there is no other acyclic orientation with a smaller sum. Additionally, a ''k''-regular subgraph of the given graph forms a face of the polytope if and only if its vertices form a [[lower set]] for at least one acyclic unique sink orientation. In this way, the [[face lattice]] of the polytope is uniquely determined from the graph {{harv|Kalai|1988}}. Based on this structure, the face lattices of simple polytopes can be reconstructed from their graphs in [[polynomial time]] using [[linear programming]] {{harv|Friedman|2009}}.

==References==
*{{citation
 | last = Friedman | first = Eric J.
 | doi = 10.1007/s00454-008-9121-7
 | issue = 2
 | journal = [[Discrete and Computational Geometry]]
 | mr = 2471873
 | pages = 249–256
 | title = Finding a simple polytope from its graph in polynomial time
 | volume = 41
 | year = 2009}}
*{{citation
 | last = Kalai | first = Gil | authorlink = Gil Kalai
 | doi = 10.1016/0097-3165(88)90064-7
 | issue = 2
 | journal = [[Journal of Combinatorial Theory]] | series = Series A
 | mr = 964396
 | pages = 381–383
 | title = A simple way to tell a simple polytope from its graph
 | volume = 49
 | year = 1988}}.
*{{citation
 | last = Matoušek | first = Jiří | authorlink = Jiří Matoušek (mathematician)
 | doi = 10.1007/s00493-006-0007-0
 | issue = 1
 | journal = Combinatorica
 | mr = 2201286
 | pages = 91–99
 | title = The number of unique-sink orientations of the hypercube
 | volume = 26
 | year = 2006}}.
*{{citation
 | last1 = Schurr | first1 = Ingo
 | last2 = Szabó | first2 = Tibor
 | doi = 10.1007/s00454-003-0813-8
 | issue = 4
 | journal = Discrete &amp; Computational Geometry
 | mr = 2053502
 | pages = 627–642
 | title = Finding the sink takes some time: an almost quadratic lower bound for finding the sink of unique sink oriented cubes
 | volume = 31
 | year = 2004}}.
*{{citation
 | last1 = Stickney | first1 = Alan
 | last2 = Watson | first2 = Layne
 | doi = 10.1287/moor.3.4.322
 | issue = 4
 | journal = Mathematics of Operations Research
 | mr = 509668
 | pages = 322–333
 | title = Digraph models of Bard-type algorithms for the linear complementarity problem
 | volume = 3
 | year = 1978}}.
*{{citation
 | last1 = Szabó | first1 = Tibor
 | last2 = Welzl | first2 = Emo | author2-link = Emo Welzl
 | contribution = Unique sink orientations of cubes
 | doi = 10.1109/SFCS.2001.959931
 | mr = 1948744
 | pages = 547–555
 | publisher = IEEE Computer Soc., Los Alamitos, CA
 | title = 42nd IEEE Symposium on Foundations of Computer Science (Las Vegas, NV, 2001)
 | year = 2001}}.

[[Category:Graph theory objects]]
[[Category:Polyhedral combinatorics]]</text>
      <sha1>rbs6oxt3z6izgr4ux0wjtls2oo0vb02</sha1>
    </revision>
  </page>
  <page>
    <title>Voorhoeve index</title>
    <ns>0</ns>
    <id>32394679</id>
    <revision>
      <id>823698430</id>
      <parentid>803607234</parentid>
      <timestamp>2018-02-02T21:14:24Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>/* References */Journal cites, Added 2 dois to journal cites using [[Project:AWB|AWB]] (12158)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2251">In mathematics, the '''Voorhoeve index''' is a non-negative [[real number]] associated with certain [[Function (mathematics)|functions]] on the [[complex number]]s, named after [[Marc Voorhoeve]]. It may be used to extend [[Rolle's theorem]] from real functions to complex functions, taking the role that for real functions is played by the number of zeros of the function in an [[interval (mathematics)|interval]].

==Definition==
The Voorhoeve index &lt;math&gt;V_I(f)&lt;/math&gt; of a complex-valued function ''f'' that is [[analytic function|analytic]] in a complex [[Neighbourhood (mathematics)|neighbourhood]] of the real interval &lt;math&gt;I&lt;/math&gt;&amp;nbsp;=&amp;nbsp;[''a'',&amp;nbsp;''b''] is given by

: &lt;math&gt;V_I(f) = \frac{1}{2\pi}\int_a^b \! \left| \frac{d}{dt} {\rm Arg} \, f(t) \right| \,\, dt \, = \frac{1}{2\pi} \int_a^b \! \left| {\rm Im}\left(\frac{f'}{f}\right) \right| \, dt. &lt;/math&gt;

(Different authors use different normalization factors.)

==Rolle's theorem==
[[Rolle's theorem]] states that if ''f'' is a [[continuously differentiable function|continuously differentiable]] real-valued function on the [[real line]], and ''f''(''a'') = ''f''(''b'') = 0, where ''a'' &amp;lt; ''b'', then its derivative ''f'' ' must have a zero strictly between ''a'' and ''b''. Or, more generally, if &lt;math&gt;N_I(f)&lt;/math&gt; denotes the number of zeros of the continuously differentiable function ''f'' on the interval &lt;math&gt;I&lt;/math&gt;, then &lt;math&gt;N_I(f)&lt;/math&gt; ≤ &lt;math&gt;N_I&lt;/math&gt;(''f'' ')&amp;nbsp;+&amp;nbsp;1.

Now one has the analogue of Rolle's theorem:

: &lt;math&gt;V_I(f) \le V_I (f') + \frac12.&lt;/math&gt;

This leads to bounds on the number of zeros of an analytic function in a complex region.

==References==
{{refbegin}}
* {{ Citation | authorlink = Marc Voorhoeve | first = Marc | last = Voorhoeve | journal = Math. Z. | volume = 151 | title = On the oscillation of exponential polynomials | pages = 277–294 | year = 1976 | doi=10.1007/bf01214940}}
* {{ Citation | first1 = A. | last1 = Khovanskii | first2 = S. | last2 = Yakovenko | journal = J. Dyn. Control Syst. | volume = 2 | title = Generalized Rolle theorem in &lt;math&gt;R^n&lt;/math&gt; and &lt;math&gt;C&lt;/math&gt; | pages = 103–123 | year = 1996 | doi=10.1007/bf02259625}}
{{refend}}

[[Category:Calculus]]
[[Category:Complex analysis]]</text>
      <sha1>s6fe9rwxb8woqtqph1240gav0o6r7bc</sha1>
    </revision>
  </page>
  <page>
    <title>Weighing matrix</title>
    <ns>0</ns>
    <id>3301463</id>
    <revision>
      <id>855212375</id>
      <parentid>822471384</parentid>
      <timestamp>2018-08-16T18:11:10Z</timestamp>
      <contributor>
        <ip>37.26.146.158</ip>
      </contributor>
      <comment>/* Open Questions */Fixed typo and added content.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3323">{{refimprove|date=December 2013}}
In [[mathematics]], a '''weighing matrix''' ''W'' of order ''n'' and weight ''w'' is an ''n'' × ''n'' (0,1,-1)-matrix such that &lt;math&gt;WW^{T}=wI_n&lt;/math&gt;, where &lt;math&gt;W^T&lt;/math&gt; is the [[transpose]] of &lt;math&gt;W&lt;/math&gt; and &lt;math&gt;I_n&lt;/math&gt; is the [[identity matrix]] of order &lt;math&gt;n&lt;/math&gt;.

For convenience, a weighing matrix of order ''n'' and weight ''w'' is often denoted by ''W''(''n'',''w''). A ''W''(''n'',''n'') is a [[Hadamard matrix]] and a ''W(n,n-1)'' is equivalent to a [[conference matrix]].

==Properties==

Some properties are immediate from the definition. If ''W'' is a ''W''(''n'',''w''), then:
* The rows of ''W'' are pairwise [[orthogonal]] (that is, every pair of rows you pick from ''W'' will be orthogonal). Similarly, the columns are pairwise orthogonal.
* Each row and each column of ''W'' has exactly ''w'' non-zero elements.
* &lt;math&gt;W^{T}W=wI&lt;/math&gt;, since the definition means that &lt;math&gt;W^{-1} = w^{-1}W^{T}&lt;/math&gt;, where &lt;math&gt;W^{-1}&lt;/math&gt; is the [[Inverse_matrix|inverse]] of &lt;math&gt;W&lt;/math&gt;.
* &lt;math&gt;\operatorname{det}(W)=\pm w^{n/2}&lt;/math&gt; where &lt;math&gt;\operatorname{det}(W)&lt;/math&gt; is the [[determinant]] of &lt;math&gt;W&lt;/math&gt;.

==Examples==

Note that when weighing matrices are displayed, the symbol &lt;math&gt;-&lt;/math&gt; is used to represent -1. Here are two examples:

This is a ''W''(''2'',''2''):
:&lt;math&gt;\begin{pmatrix}1 &amp; 1 \\ 1 &amp; -\end{pmatrix}&lt;/math&gt;

This is a ''W''(''7'',''4''):
:&lt;math&gt;\begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; 1 &amp; 0 &amp; 0 &amp; 0 \\
1 &amp; - &amp; 0 &amp; 0 &amp; 1 &amp; 1 &amp; 0 \\
1 &amp; 0 &amp; - &amp; 0 &amp; - &amp; 0 &amp; 1 \\
1 &amp; 0 &amp; 0 &amp; - &amp; 0 &amp; - &amp; - \\
0 &amp; 1 &amp; - &amp; 0 &amp; 0 &amp; 1 &amp; - \\
0 &amp; 1 &amp; 0 &amp; - &amp; 1 &amp; 0 &amp; 1 \\
0 &amp; 0 &amp; 1 &amp; - &amp; - &amp; 1 &amp; 0
\end{pmatrix}&lt;/math&gt;

==Equivalence==

Two weighing matrices are considered to be equivalent if one can be obtained from the other by a series of permutations and negations of the rows and columns of the matrix. The classification of weighing matrices is complete for cases where ''w'' ≤ 5 as well as all cases where ''n'' ≤ 15 are also completed.&lt;ref&gt;M. Harada, A. Munemasa, On the classification of weighing matrices and self-orthogonal codes, 2011, https://arxiv.org/abs/1011.5382.&lt;/ref&gt; However, very little has been done beyond this with exception to classifying circulant weighing matrices.&lt;ref&gt;Ang, Miin Huey, et al. "Study of proper circulant weighing matrices with weight 9." Discrete Mathematics 308.13 (2008): 2802-2809.&lt;/ref&gt;&lt;ref&gt;Arasu, K. T., et al. "Determination of all possible orders of weight 16 circulant weighing matrices." Finite Fields and Their Applications 12.4 (2006): 498-538.&lt;/ref&gt;

==Open Questions==

There are many open questions about weighing matrices. The main question about weighing matrices is their existence: for which values of ''n'' and ''w'' does there exist a ''W''(''n'',''w'')?  A great deal about this is unknown.  An equally important but often overlooked question about weighing matrices is their enumeration: for a given ''n'' and ''w'', how many ''W''(''n'',''w'')'s are there?

This question has two different meanings. Enumerating up to equivalence and enumerating different matrices with same n,k parametets. Some papers were published on the first question but none were published on the second important question.

==References==
{{Reflist}}

[[Category:Matrix theory]]
[[Category:Combinatorics]]</text>
      <sha1>3bsrpbrbe85h9ywjzpsp7z53t6oct6r</sha1>
    </revision>
  </page>
  <page>
    <title>Zonal and meridional</title>
    <ns>0</ns>
    <id>8810330</id>
    <revision>
      <id>851025972</id>
      <parentid>845562132</parentid>
      <timestamp>2018-07-19T15:32:59Z</timestamp>
      <contributor>
        <ip>62.155.223.60</ip>
      </contributor>
      <comment>deleted two self referential liks (link deviated onto same page)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4058">{{redirect|Meridional|the Norma Jean album|Meridional (album)}}
{{redirect|Mer.|other uses|Mer (disambiguation)}}
{{refimprove|date=May 2011}}
{{wiktionary|zonal|meridional}}
[[File:Zonal band.svg|thumb|A zonal region on the globe]]
The terms '''zonal''' and '''meridional''' are used to describe directions on a [[globe]].

'''Zonal''' means "along a [[Circle of latitude|latitudinal circle]]" or "in the west–east direction." &lt;ref&gt;{{cite web|title=Zonal|url=http://glossary.ametsoc.org/wiki/Zonal|website=Glossary of Meteorology|publisher=American Meteorological Society|accessdate=12 January 2018}}&lt;/ref&gt; '''Zonal flow''' is a [[Meteorology|meteorological]] term regarding [[atmospheric circulation]] following a general flow pattern along [[Latitude|latitudinal]] lines, as opposed to [[meridional flow]] along [[Longitude|longitudinal]] lines. Zonal, in the context of physics, connotes a tendency of [[flux]] to conform to a pattern parallel to the [[equator]] of a sphere. Generally, zonal flow of the atmosphere brings a temperature contrast along the Earth's longitude. [[Extratropical cyclone|Extratropical cyclones]] in this environment tend to be weaker, moving faster and producing relatively little impact on local weather.

'''Meridional''' means "along a [[Circle of longitude|longitudinal circle]]" (a.k.a. ''[[Meridian (geography)|meridian]]'') or "in the north–south direction" &lt;ref&gt;{{cite web|title=Meridional|url=http://glossary.ametsoc.org/wiki/Meridional|website=Glossary of Meteorology|publisher=American Meteorological Society|accessdate=12 January 2018}}&lt;/ref&gt;. '''Meridional flow''' is a general air flow pattern from north to south, or from south to north, along the Earth's [[longitude]] lines (perpendicular to a [[zonal flow]]).  [[Extratropical cyclone]]s in this environment tend to be stronger and move slower.  This pattern is responsible for most instances of [[extreme weather]], as not only are storms stronger in this type of flow regime, but temperatures can reach extremes as well, producing [[heat wave]]s and [[cold wave]]s depending on the [[equator]]-ward or poleward direction of the flow.

These terms are often used in the [[atmospheric sciences|atmospheric]] and [[earth science]]s to describe global phenomena, such as "meridional wind", or "zonal average temperature". (Strictly speaking, '''zonal''' means more than simply a direction as it also implies a degree of localization in the meridional direction, so that the phenomenon in question is localized to a zone of the planet.)

"Meridional" is also used to describe the axis close to the chain orientation in a polymer fiber, while the term "equatorial" is used to describe the direction normal to the fiber axis.

For [[vector field]]s (such as [[wind velocity]]), the zonal component (or ''x''-[[coordinate]]) is denoted as ''u'', while the meridional component (or ''y''-coordinate) is denoted as ''v''.

==Meridional meaning ''South''==
The word comes from [[Latin language|Latin]] ''meri dies'' ("midday"), meaning the position of the [[Sun]] at that time. As the original [[Lazio|Latin territory]] was in the [[Northern Hemisphere]], this is still used with that sense in some [[Romance languages]] such as [[Portuguese Language|Portuguese]] (Banco Meridional, in Brazil), [[Spanish language|Spanish]], [[French language|French]], [[Italian language|Italian]] (as in [[Meridione]]) or even [[:wikt:meridional|in English]] (as in the [[Norma Jean (band)|Norma Jean]] album ''[[Meridional (album)|Meridional]]'').

The term '''meridional''', sometimes abbreviated to "Mer.", was used in historical astronomy to indicate the southern direction on the celestial globe, together with [[septentrional]] ("Sep.") for northern, [[wikt:oriental|oriental]] ("Ori.") for eastern and [[wikt:occidental|occidental]] ("Occ.") for western.&lt;ref&gt;Hooke, Robert. 1666. Volume 1. ''[[Philosophical Transactions]]''&lt;/ref&gt;

==See also==
* [[Zonal and poloidal]]
* [[Zonal flow (plasma)]]
* [[Meridione]]

==Notes==
{{Reflist}}

[[Category:Orientation (geometry)]]</text>
      <sha1>b18jqwcr6zqaaok4l4sybiyt20l5c7a</sha1>
    </revision>
  </page>
  <page>
    <title>Édouard Goursat</title>
    <ns>0</ns>
    <id>1674255</id>
    <revision>
      <id>817849392</id>
      <parentid>792021816</parentid>
      <timestamp>2017-12-30T23:03:29Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 0 sources and tagging 7 as dead. #IABot (v1.6.1) ([[User:Balon Greyjoy|Balon Greyjoy]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8900">{{Use dmy dates|date=July 2013}}
{{Infobox scientist
|name              = Édouard Goursat
|image             = Goursat_Edouard.jpg
|image_size       = 160px
|caption           = Edouard Goursat
|birth_date        = {{Birth date|1858|05|21|df=y}}
|birth_place       = [[Lanzac]], [[Lot (département)|Lot]]
|death_date        = {{Death date and age|1936|11|25|1858|05|21|df=y}}
|death_place       = [[Paris]]
|nationality       = French
|field             = [[Mathematics]]
|work_institutions = [[University of Paris]]
|alma_mater        = [[École Normale Supérieure]]
|doctoral_advisor  = [[Jean Gaston Darboux]]
|doctoral_students = [[Georges Darmois]]&lt;br&gt;{{Interlanguage link multi|Dumitru Ionescu|ro}}
|known_for         = [[Goursat tetrahedron]], [[Cauchy–Goursat theorem]], [[Goursat's lemma]]
|prizes            =
}}
'''Édouard Jean-Baptiste Goursat''' (21 May 1858 – 25 November 1936) was a French [[mathematician]], now remembered principally as an expositor for his ''Cours d'analyse mathématique'', which appeared in the first decade of the twentieth century. It set a standard for the high-level teaching of [[mathematical analysis]], especially [[complex analysis]]. This text was reviewed by [[William Fogg Osgood]] for the Bulletin of the [[American Mathematical Society]].&lt;ref&gt;{{cite journal|author=Osgood, W. F.|authorlink=William Fogg Osgood|title=Review: ''Cours d'analyse mathématique''. Tome I.|journal=Bull. Amer. Math. Soc.|year=1903|volume=9|issue=10|pages=547–555|url=http://www.ams.org/journals/bull/1903-09-10/S0002-9904-1903-01028-3/|doi=10.1090/s0002-9904-1903-01028-3}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|author=Osgood, W. F.|title=Review: ''Cours d'analyse mathématique''. Tome II.|journal=Bull. Amer. Math. Soc.|year=1908|volume=15|issue=3|pages=120–126|url=http://www.ams.org/journals/bull/1908-15-03/S0002-9904-1908-01704-X/|doi=10.1090/s0002-9904-1908-01704-x}}&lt;/ref&gt; This led to its translation in English by [[Earle Raymond Hedrick]] published by Ginn and Company. Goursat also published texts on [[partial differential equation]]s and [[hypergeometric series]].

==Life==
Edouard Goursat was born in [[Lanzac]], [[Lot (département)|Lot]]. He was a graduate of the [[École Normale Supérieure]], where he later taught and developed his ''Cours''. At that time the [[topological]] foundations of complex analysis were still not clarified, with the [[Jordan curve theorem]] considered a challenge to [[mathematical rigour]] (as it would remain until [[L. E. J. Brouwer]] took in hand the approach from [[combinatorial topology]]). Goursat’s work was considered by his contemporaries, including [[G. H. Hardy]], to be exemplary in facing up to the difficulties inherent in stating the fundamental [[Cauchy integral theorem]] properly. For that reason it is sometimes called the [[Cauchy–Goursat theorem]].

==Work==
Goursat was the first to note that the generalized [[Stokes theorem]] can be written in the simple form

:&lt;math&gt;\int_S \omega = \int_T d \omega &lt;/math&gt;

where &lt;math&gt;\omega&lt;/math&gt; is a ''p''-form in ''n''-space and ''S'' is the ''p''-dimensional boundary of the (''p''&amp;nbsp;+&amp;nbsp;1)-dimensional region ''T''. Goursat also used differential forms to state the [[Poincaré lemma]] and its converse, namely, that if &lt;math&gt;\omega&lt;/math&gt; is a ''p''-form, then &lt;math&gt;d\omega=0&lt;/math&gt; if and only if there is a (''p''&amp;nbsp;−&amp;nbsp;1)-form &lt;math&gt;\eta&lt;/math&gt;  with
&lt;math&gt;d \eta=\omega&lt;/math&gt;. However Goursat did not notice that the "only if" part of the result depends on the domain of &lt;math&gt;\omega&lt;/math&gt; and is not true in general. [[E. Cartan]] himself in 1922 gave a counterexample, which provided one of the impulses in the next decade for the development of the [[De Rham cohomology]] of a differential manifold.

==Books by Edouard Goursat==
* [https://archive.org/details/coursemathanalys01gourrich A Course In Mathematical Analysis Vol I] Translated by O. Dunkel and E. R. Hedrick (Ginn and Company, 1904)
* [https://archive.org/details/coursemathema0102gourrich A Course In Mathematical Analysis Vol II, part I] Translated by O. Dunkel and E. R. Hedrick (Ginn and Company, 1916) (Complex analysis)
* [https://archive.org/details/differentalequat033197mbp  A Course In Mathematical Analysis Vol II Part II] Translated by O. Dunkel and E. R. Hedrick (Ginn and Company, 1917) (Differential Equations)
* [http://name.umdl.umich.edu/ACR1803.0001.001 Leçons sur l'intégration des équations aux dérivées partielles du premier ordre] (Hermann, Paris, 1891)&lt;ref name="LovettReview"&gt;{{cite journal|author=Lovett, Edgar Odell|authorlink=Edgar Odell Lovett|title=Review: Goursat's Partial Differential Equations|journal=Bull. Amer. Math. Soc.|year=1898|volume=4|issue=9|pages=452–487|url=http://www.ams.org/journals/bull/1898-04-09/S0002-9904-1898-00540-2/|doi=10.1090/S0002-9904-1898-00540-2}}&lt;/ref&gt;
* [http://gallica.bnf.fr/document?O=N084146 Leçons sur l'intégration des équations aux dérivées partielles du second ordre, à deux variables indépendantes Tome 1]{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }} (Hermann, Paris 1896–1898)&lt;ref name=LovettReview/&gt;
* [http://gallica.bnf.fr/document?O=N084147 Leçons sur l'intégration des équations aux dérivées partielles du second ordre, à deux variables indépendantes Tome 2]{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }} (Hermann, Paris 1896–1898)&lt;ref name=LovettReview/&gt;
* [http://gallica.bnf.fr/document?O=N038309 Leçons sur les séries hypergéométriques et sur quelques fonctions qui s'y rattachent]{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }} (Hermann, Paris, 1936–1939)&lt;ref&gt;{{cite journal|author=Szegő, G.|authorlink=Gábor Szegő|title=Review: ''Leçons sur les séries hypergéométriques et sur quelques fonctions qui s'y rattachent'' by É. Goursat|journal=Bull. Amer. Math. Soc.|year=1938|volume=44|issue=1, Part 1|pages=16–17|url=http://www.ams.org/journals/bull/1938-44-01/S0002-9904-1938-06654-2/S0002-9904-1938-06654-2.pdf|doi=10.1090/s0002-9904-1938-06652-9}}&lt;/ref&gt;
* [http://gallica.bnf.fr/document?O=N038954 Le problème de Bäcklund]{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }} (Gauthier-Villars, Paris, 1925)
*  [http://gallica.bnf.fr/document?O=N099552 Leçons sur le problème de Pfaff]{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }} (Hermann,Paris, 1922)&lt;ref&gt;{{cite journal|author=Dresden, Arnold|authorlink=Arnold Dresden|title=Review: ''Leçons sur le problème de Pfaff''|journal=Bull. Amer. Math. Soc.|year=1924|volume=30|issue=7|pages=359–362|url=http://www.ams.org/journals/bull/1924-30-07/S0002-9904-1924-03903-2/|doi=10.1090/s0002-9904-1924-03903-2}}&lt;/ref&gt;
* [http://gallica.bnf.fr/document?O=N099595 Théorie des fonctions algébriques et de leurs intégrales : étude des fonctions analytiques sur une surface de Riemann]{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }} with [[Paul Appell]] (Gauthier-Villars, Paris, 1895)&lt;ref&gt;{{cite journal|author=Osgood, W. F.|title=Review: ''Théorie des fonctions algébriques et de leurs intégrales'', by P. Appell and É. Goursat|journal=Bull. Amer. Math. Soc.|year=1896|volume=2|issue=10|pages=317–327|url=http://www.ams.org/journals/bull/1896-02-10/S0002-9904-1896-00353-0/|doi=10.1090/s0002-9904-1896-00353-0}}&lt;/ref&gt;
* [http://gallica.bnf.fr/document?O=N092706 Théorie des fonctions algébriques d'une variable et des transcendantes qui s'y rattachent Tome II, Fonctions automorphes]{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }} with Paul Appell (Gauthier-Villars, 1930)

==See also==
*[[Goursat problem]]
*[[Goursat tetrahedron]]
*[[Goursat's lemma]]
*[[Cauchy–Riemann equations#Goursat's theorem and its generalizations|Goursat's theorem (Complex analysis)]]

==References==
{{reflist}}
* {{Cite book|first=Victor |last=Katz |title=A History of Mathematics: An introduction |edition=3rd |publisher=Addison-Wesley |location=Boston |year=2009 |isbn=978-0-321-38700-4 }}

==External links==
* {{MacTutor Biography|id=Goursat}}
* William Fogg Osgood [http://projecteuclid.org/euclid.bams/1183417526 A modern French Calculus]  Bull. Amer. Math. Soc. '''9''', (1903), pp.&amp;nbsp;547–555.
* William Fogg Osgood [http://projecteuclid.org/euclid.bams/1183418774 Review: Edouard Goursat, A Course in Mathematical Analysis]  Bull. Amer. Math. Soc. '''12''', (1906), p.&amp;nbsp;263.
*{{MathGenealogy |id=96283}}

{{Authority control}}

{{DEFAULTSORT:Goursat, Edouard}}
[[Category:1858 births]]
[[Category:1936 deaths]]
[[Category:19th-century French mathematicians]]
[[Category:20th-century mathematicians]]
[[Category:Mathematical analysts]]
[[Category:Members of the French Academy of Sciences]]
[[Category:École Normale Supérieure alumni]]
[[Category:University of Paris faculty]]</text>
      <sha1>9g8scxxo0m49uq86lvkr61ayvkr5qpp</sha1>
    </revision>
  </page>
</mediawiki>
