<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>1701 (number)</title>
    <ns>0</ns>
    <id>5182124</id>
    <revision>
      <id>853157038</id>
      <parentid>838690856</parentid>
      <timestamp>2018-08-02T20:08:22Z</timestamp>
      <contributor>
        <username>Ifnord</username>
        <id>470876</id>
      </contributor>
      <comment>/* In mathematics */ Adding/improving reference(s)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2140">{{about|the number 1701|other uses of 1701|1701 (disambiguation)}}
{{Infobox number
| number = 1701
| divisor = 1, 3, 7, 9, 21, 27, 63, 81, 189, 243, 567, 1701
}}
'''1701''' is the [[natural number]] preceding [[1000 (number)#1700 to 1799|1702]] and following [[1000 (number)#1700 to 1799|1700]].

==In mathematics==
1701 is an [[odd number]] and a [[Stirling number]] of the second kind.

The number '''1701''' also has unusual properties as it:
* belongs to a set of numbers such that &lt;math&gt;n^2&lt;/math&gt; contains exactly seven different digits.
* is a [[Polygonal number|decagonal]] and a 13-gonal number.
* is divisible by the square of the sum of its digits.
* belongs to a set of numbers with only [[Palindrome|palindromic]] [[Prime number|prime]] factors whose sum is palindromic.
* is a First [[Beale ciphers|Beale cipher]].&lt;ref&gt;{{cite book|last1=Jameson|first1=W.C.|title=The Silver Madonna and Other Tales of America's Greatest Lost Treasures|date=2013|publisher=Taylor Trade Publishing, an imprint of The Rowman &amp; Littlefield Publishing Group, Inc.|location=Lanham, Maryland|isbn=1589798406|page=117}}&lt;/ref&gt;
* belongs to a set of numbers whose digits of prime factors are either [[3 (number)|3]] or [[7 (number)|7]].
* its reversal digit sequence (1071) is divisible by 7.
* is a [[Harshad number]].&lt;ref&gt;{{cite web |title=anarchy golf - Harshad numbers |url=http://golf.shinh.org/p.rb?Harshad+numbers |website=golf.shinh.org |accessdate=2 August 2018}}&lt;/ref&gt;

==In other fields==
* In the ''[[Star Trek]]'' [[science fiction]] franchise, '''NCC-1701''' is the designation for several [[starship]]s named [[Starship Enterprise|USS ''Enterprise'']].&lt;ref&gt;{{cite book|last1=Robinson|first1=Ben|last2=Okuda|first2=Marcus Riley ; technical consultant: Michael|title=U.S.S. Enterprise NX-01, NCC-1701, NCC-1701-A to NCC-1701-E : owners' workshop manual|date=2010|publisher=Gallery Books|location=New York|isbn=1451621299|edition=1st Gallery Books hardcover}}&lt;/ref&gt; Several of these vessels are focal points in the fictional universe created by [[Gene Roddenberry]].

==References==
{{reflist|30em}}

[[Category:Integers]]


{{number-stub}}</text>
      <sha1>oadtgvw3klpcx58x41z6zau2wtnjxiy</sha1>
    </revision>
  </page>
  <page>
    <title>Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg</title>
    <ns>0</ns>
    <id>30625393</id>
    <revision>
      <id>797621604</id>
      <parentid>747646739</parentid>
      <timestamp>2017-08-28T08:23:09Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[Wikipedia:Bots/Requests for approval/KolbertBot|HTTP→HTTPS]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1748">{{Infobox journal
 | title = Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg
 | cover =
 | abbreviation = Abh. Math. Semin. Univ. Hambg.
 | discipline = [[Pure mathematics]]
 | editor = {{nowrap|1=[[Vicente Cortés]],}} {{nowrap|1=[[Birgit Richter]]}}
 | publisher = [[Springer Science+Business Media]]
 | frequency = Biannual
 | history = 1922–present
 | impact = 0.667
 | impact-year = 2015
 | url = https://www.springer.com/mathematics/algebra/journal/12188
 | link1 = http://www.springerlink.com/content/0025-5858
 | link1-name = Online access
 | ISSN = 0025-5858
 | eISSN = 1865-8784
 | CODEN = AMHAAJ
 | LCCN = 32024459
 | OCLC = 01913576
}}
''''' Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg''''' (English: ''Papers from the Mathematical Seminar of the University of Hamburg'') is a [[peer review|peer-reviewed]] [[mathematics journal]] published by [[Springer Science+Business Media]]. It was established in 1922 and publishes articles on [[pure mathematics]]. The journal is indexed by ''[[Mathematical Reviews]]'' and [[Zentralblatt MATH]]. Its 2009 [[Mathematical Citation Quotient|MCQ]] was 0.29, and its 2015 [[impact factor]] was 0.667.

== External links ==

* {{Official website|https://www.springer.com/mathematics/algebra/journal/12188}}
* [http://abhandlungen.math.uni-hamburg.de/ Journal Webpage at the Universität Hamburg]

{{DEFAULTSORT:Abhandlungen aus dem Mathematischen Seminar der Universitat Hamburg}}
[[Category:Mathematics journals]]
[[Category:Publications established in 1922]]
[[Category:English-language journals]]
[[Category:Springer Science+Business Media academic journals]]
[[Category:Biannual journals]]
[[Category:University of Hamburg]]


{{math-journal-stub}}</text>
      <sha1>00q68tpsqrys89monqh12wgyalyqrbp</sha1>
    </revision>
  </page>
  <page>
    <title>Analysis of Boolean functions</title>
    <ns>0</ns>
    <id>54171755</id>
    <revision>
      <id>864838373</id>
      <parentid>842502460</parentid>
      <timestamp>2018-10-19T20:18:50Z</timestamp>
      <contributor>
        <username>JoeHebda</username>
        <id>21071050</id>
      </contributor>
      <comment>/* Category */ additional; rm cat.notice</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="27237">In [[mathematics]] and [[theoretical computer science]], '''analysis of Boolean functions'''&lt;ref&gt;{{cite book |last=O'Donnell|first=Ryan|date=2014|title=Analysis of Boolean functions|publisher=Cambridge University Press|isbn=978-1-107-03832-5}}&lt;/ref&gt; is the study of real-valued functions on &lt;math&gt;\{0,1\}^n&lt;/math&gt; or &lt;math&gt;\{-1,1\}^n&lt;/math&gt; from a spectral perspective (such functions are sometimes known as [[pseudo-Boolean function]]s). The functions studied are often, but not always, Boolean-valued, making them [[Boolean function]]s. The area has found many applications in [[combinatorics]], [[social choice theory]], [[random graph]]s, and theoretical computer science, especially in [[hardness of approximation]], [[property testing]] and [[probably approximately correct learning|PAC learning]].

==Basic concepts==

We will mostly consider functions defined on the domain &lt;math&gt;\{-1,1\}^n&lt;/math&gt;. Sometimes it is more convenient to work with the domain &lt;math&gt;\{0,1\}^n&lt;/math&gt; instead. If &lt;math&gt;f&lt;/math&gt; is defined on &lt;math&gt;\{-1,1\}^n&lt;/math&gt;, then the corresponding function defined on &lt;math&gt;\{0,1\}^n&lt;/math&gt; is

:&lt;math&gt;f_{01}(x_1,\ldots,x_n) = f((-1)^{x_1},\ldots,(-1)^{x_n}).&lt;/math&gt;

Similarly, for us a Boolean function is a &lt;math&gt;\{-1,1\}&lt;/math&gt;-valued function, though often it is more convenient to consider &lt;math&gt;\{0,1\}&lt;/math&gt;-valued functions instead.

===Fourier expansion===

Every real-valued function &lt;math&gt;f\colon \{-1,1\}^n \to \mathbb{R}&lt;/math&gt; has a unique expansion as a multilinear polynomial:

:&lt;math&gt; f(x) = \sum_{S \subseteq [n]} \hat{f}(S) \chi_S(x), \quad \chi_S(x) = \prod_{i \in S} x_i. &lt;/math&gt;

This is the [[Hadamard transform]] of the function &lt;math&gt;f&lt;/math&gt;, which is the [[Fourier transform]] in the [[group (mathematics)|group]] &lt;math&gt;\mathbb{Z}_2^n&lt;/math&gt;. The coefficients &lt;math&gt;\hat{f}(S)&lt;/math&gt; are known as ''Fourier coefficients'', and the entire sum is known as the ''Fourier expansion'' of &lt;math&gt;f&lt;/math&gt;. The functions &lt;math&gt;\chi_S&lt;/math&gt; are known as ''Fourier characters'', and they form an orthonormal basis for the space of all functions over &lt;math&gt;\{-1,1\}^n&lt;/math&gt;, with respect to the inner product &lt;math&gt;\langle f,g \rangle = 2^{-n} \sum_{x \in \{-1,1\}^n} f(x) g(x)&lt;/math&gt;.

The Fourier coefficients can be calculated using an inner product:

:&lt;math&gt; \hat{f}(S) = \langle f, \chi_S \rangle. &lt;/math&gt;

In particular, this shows that &lt;math&gt;\hat{f}(\emptyset) = \mathbb{E}[f].&lt;/math&gt; Parseval's identity states that

:&lt;math&gt; \|f\|^2 = \mathbb{E}[f^2] = \sum_S \hat{f}(S)^2. &lt;/math&gt;

If we skip &lt;math&gt;S = \emptyset&lt;/math&gt;, then we get the variance of &lt;math&gt;f&lt;/math&gt;:

:&lt;math&gt; \mathbb{V}[f] = \sum_{S \neq \emptyset} \hat{f}(S)^2. &lt;/math&gt;

=== Fourier degree and Fourier levels ===

The ''degree'' of a function &lt;math&gt;f\colon \{-1,1\}^n \to \mathbb{R}&lt;/math&gt; is the maximum &lt;math&gt;d&lt;/math&gt; such that &lt;math&gt;\hat{f}(S) \neq 0&lt;/math&gt; for some set &lt;math&gt;S&lt;/math&gt; of size &lt;math&gt;d&lt;/math&gt;. In other words, the degree of &lt;math&gt;f&lt;/math&gt; is its degree as a multilinear polynomial.

It is convenient to decompose the Fourier expansion into ''levels'': the Fourier coefficient &lt;math&gt;\hat{f}(S)&lt;/math&gt; is on level &lt;math&gt;|S|&lt;/math&gt;.

The ''degree &lt;math&gt;d&lt;/math&gt;'' part of &lt;math&gt;f&lt;/math&gt; is

:&lt;math&gt; f^{=d} = \sum_{|S| = d} \hat{f}(S) \chi_S. &lt;/math&gt;

It is obtained from &lt;math&gt;f&lt;/math&gt; by zeroing out all Fourier coefficients not on level &lt;math&gt;d&lt;/math&gt;.

We similarly define &lt;math&gt;f^{&gt;d},f^{&lt;d},f^{\geq d},f^{\leq d}&lt;/math&gt;.

===Influence===

The &lt;math&gt;i&lt;/math&gt;'th influence of a function &lt;math&gt;f\colon \{-1,1\}^n \to \mathbb{R}&lt;/math&gt; can be defined in two equivalent ways:

: &lt;math&gt;
\begin{align}
&amp; \operatorname{Inf}_i[f] = \mathbb{E}\left[ \left(\frac{f - f^{\oplus i}}{2} \right)^2 \right] = \sum_{S \ni i} \hat{f}(S)^2, \\[5pt]
&amp; f^{\oplus i}(x_1,\ldots,x_n) = f(x_1,\ldots,x_{i-1},-x_i,x_{i+1},\ldots,x_n).
\end{align}
&lt;/math&gt;

If &lt;math&gt;f&lt;/math&gt; is Boolean then &lt;math&gt;\operatorname{Inf}_i[f]&lt;/math&gt; is the probability that flipping the &lt;math&gt;i&lt;/math&gt;'th coordinate flips the value of the function:

:&lt;math&gt;\operatorname{Inf}_i[f] = \Pr[f(x) \neq f^{\oplus i}(x)]. &lt;/math&gt;

If &lt;math&gt;\operatorname{Inf}_i[f] = 0&lt;/math&gt; then &lt;math&gt;f&lt;/math&gt; doesn't depend on the &lt;math&gt;i&lt;/math&gt;'th coordinate.

The ''total influence'' of &lt;math&gt;f&lt;/math&gt; is the sum of all of its influences:

:&lt;math&gt;\operatorname{Inf}[f] = \sum_{i=1}^n \operatorname{Inf}_i[f] = \sum_S |S| \hat{f}(S)^2. &lt;/math&gt;

The total influence of a Boolean function is also the ''average sensitivity'' of the function. The ''sensitivity'' of a Boolean function &lt;math&gt;f&lt;/math&gt; at a given point is the number of coordinates &lt;math&gt;i&lt;/math&gt; such that if we flip the &lt;math&gt;i&lt;/math&gt;'th coordinate, the value of the function changes. The average value of this quantity is exactly the total influence.

The total influence can also be defined using the [[Discrete Laplace operator#Graph Laplacians|discrete Laplacian]] of the [[Hamming graph]], suitably normalized:  &lt;math&gt;\operatorname{Inf}[f] = \langle f,Lf \rangle&lt;/math&gt;.

===Noise stability===

Given &lt;math&gt;-1 \leq \rho \leq 1&lt;/math&gt;, we say that two random vectors &lt;math&gt;x,y \in \{-1,1\}^n&lt;/math&gt; are ''&lt;math&gt;\rho&lt;/math&gt;-correlated'' if the marginal distributions of &lt;math&gt;x,y&lt;/math&gt; are uniform, and &lt;math&gt;\mathbb{E}[x_iy_i] = \rho&lt;/math&gt;. Concretely, we can generate a pair of &lt;math&gt;\rho&lt;/math&gt;-correlated random variables by first choosing &lt;math&gt;x,z \in \{-1,1\}^n&lt;/math&gt; uniformly at random, and then choosing &lt;math&gt;y&lt;/math&gt; according to one of the following two equivalent rules, applied independently to each coordinate:

:&lt;math&gt; y_i = \begin{cases} x_i &amp; \text{w.p. } \rho, \\ z_i &amp; \text{w.p. } 1-\rho. \end{cases} \quad \text{or} \quad y_i = \begin{cases} x_i &amp; \text{w.p. } \frac{1+\rho}{2}, \\ -x_i &amp; \text{w.p. } \frac{1-\rho}{2}. \end{cases} &lt;/math&gt;

We denote this distribution by &lt;math&gt; y \sim N_\rho(x) &lt;/math&gt;.

The ''noise stability'' of a function &lt;math&gt;f\colon \{-1,1\}^n \to \mathbb{R}&lt;/math&gt; at &lt;math&gt;\rho&lt;/math&gt; can be defined in two equivalent ways:

:&lt;math&gt; \operatorname{Stab}_\rho[f] = \operatorname{\mathbb{E}}_{x; y \sim N_\rho(x)}[f(x) f(y)] = \sum_{S \subseteq [n]} \rho^{|S|} \hat{f}(S)^2. &lt;/math&gt;

For &lt;math&gt;0 \leq \delta \leq 1&lt;/math&gt;, the ''noise sensitivity'' of &lt;math&gt;f&lt;/math&gt; at &lt;math&gt;\delta&lt;/math&gt; is

:&lt;math&gt; \operatorname{NS}_\delta[f] = \frac{1}{2} - \frac{1}{2} \operatorname{Stab}_{1-2\delta}[f]. &lt;/math&gt;

If &lt;math&gt;f&lt;/math&gt; is Boolean, then this is the probability that the value of &lt;math&gt;f&lt;/math&gt; changes if we flip each coordinate with probability &lt;math&gt;\delta&lt;/math&gt;, independently.

===Noise operator===

The ''noise operator'' &lt;math&gt;T_\rho&lt;/math&gt; is an operator taking a function &lt;math&gt;f\colon \{-1,1\}^n \to \mathbb{R}&lt;/math&gt; and returning another function &lt;math&gt;T_\rho f\colon \{-1,1\}^n \to \mathbb{R}&lt;/math&gt; given by

:&lt;math&gt; (T_\rho f)(x) = \mathbb{E}_{y \sim N_\rho(x)}[f(y)] = \sum_{S \subseteq [n]} \rho^{|S|} \hat{f}(S) \chi_S. &lt;/math&gt;

When &lt;math&gt;\rho &gt; 0&lt;/math&gt;, the noise operator can also be defined using a [[continuous-time Markov chain]] in which each bit is flipped independently with rate 1. The operator &lt;math&gt;T_\rho&lt;/math&gt; corresponds to running this Markov chain for &lt;math&gt;\frac{1}{2}\log\frac{1}{\rho}&lt;/math&gt; steps starting at &lt;math&gt;x&lt;/math&gt;, and taking the average value of &lt;math&gt;f&lt;/math&gt; at the final state. This Markov chain is generated by the Laplacian of the Hamming graph, and this relates total influence to the noise operator.

Noise stability can be defined in terms of the noise operator: &lt;math&gt; \operatorname{Stab}_\rho[f] = \langle f, T_\rho f \rangle &lt;/math&gt;.

===Hypercontractivity===

For &lt;math&gt;1 \leq q &lt; \infty&lt;/math&gt;, the [[Lp space#Lp spaces|&lt;math&gt;L_q&lt;/math&gt;-norm]] of a function &lt;math&gt;f\colon \{-1,1\}^n \to \mathbb{R}&lt;/math&gt; is defined by

:&lt;math&gt; \|f\|_q = \sqrt[q]{\mathbb{E}[|f|^q]}. &lt;/math&gt;

We also define &lt;math&gt;\|f\|_\infty = \max_{x \in \{-1,1\}^n} |f(x)|.&lt;/math&gt;

The hypercontractivity theorem states that for any &lt;math&gt;q &gt; 2&lt;/math&gt; and &lt;math&gt;q' = 1/(1-1/q)&lt;/math&gt;,

:&lt;math&gt; \|T_\rho f\|_q \leq \|f\|_2 \quad \text{and} \quad \|T_\rho f\|_2 \leq \|f\|_{q'}. &lt;/math&gt;

Hypercontractivity is closely related to the [[logarithmic Sobolev inequalities]] of [[functional analysis]].&lt;ref&gt;{{cite journal|last1=Diaconis|first1=Persi|last2=Saloff-Coste|first2=Laurent|date=1996|title=Logarithmic Sobolev inequalities for finite Markov chains|journal=Ann. Appl. Probab.|volume=6|number=3|pages=695–750|doi=10.1214/aoap/1034968224}}&lt;/ref&gt;

A similar result for &lt;math&gt;q &lt; 2&lt;/math&gt; is known as ''reverse hypercontractivity''.&lt;ref&gt;{{cite journal|last1=Mossel|first1=Elchanan|last2=Oleszkiewicz|first2=Krzysztof|last3=Sen|first3=Arnab|date=2013|title=On reverse hypercontractivity|journal=GAFA|volume=23|number=3|pages=1062–1097|doi=10.1007/s00039-013-0229-4|arxiv=1108.1210}}&lt;/ref&gt;

===''p''-Biased analysis===

In many situations the input to the function is not uniformly distributed over &lt;math&gt;\{-1,1\}^n&lt;/math&gt;, but instead has a bias toward &lt;math&gt;-1&lt;/math&gt; or &lt;math&gt;1&lt;/math&gt;. In these situations it is customary to consider functions over the domain &lt;math&gt;\{0,1\}^n&lt;/math&gt;. For &lt;math&gt;0 &lt; p &lt; 1&lt;/math&gt;, the ''p''-biased measure &lt;math&gt;\mu_p&lt;/math&gt; is given by

:&lt;math&gt; \mu_p(x) = p^{\sum_i x_i} (1-p)^{\sum_i (1-x_i)}. &lt;/math&gt;

This measure can be generated by choosing each coordinate independently to be 1 with probability &lt;math&gt;p&lt;/math&gt; and 0 with probability &lt;math&gt;1-p&lt;/math&gt;.

The classical Fourier characters are no longer orthogonal with respect to this measure. Instead, we use the following characters:

:&lt;math&gt; \omega_S(x) = \left(\sqrt{\frac{p}{1-p}}\right)^{|\{i \in S : x_i = 0\}|} \left(-\sqrt{\frac{1-p}{p}}\right)^{|\{i \in S : x_i = 1\}|}. &lt;/math&gt;

The ''p''-biased Fourier expansion of &lt;math&gt;f&lt;/math&gt; is the expansion of &lt;math&gt;f&lt;/math&gt; as a linear combination of ''p''-biased characters:

:&lt;math&gt; f = \sum_{S \subseteq [n]} \hat{f}(S) \omega_S. &lt;/math&gt;

We can extend the definitions of influence and the noise operator to the ''p''-biased setting by using their spectral definitions.

====Influence====

The &lt;math&gt;i&lt;/math&gt;'s influence is given by

:&lt;math&gt; \operatorname{Inf}_i[f] = \sum_{S \ni i} \hat{f}(S)^2 = p(1-p) \mathbb{E}[(f-f^{\oplus i})^2]. &lt;/math&gt;

The total influence is the sum of the individual influences:

:&lt;math&gt;\operatorname{Inf}[f] = \sum_{i=1}^n \operatorname{Inf}_i[f].&lt;/math&gt;

====Noise operator====

A pair of &lt;math&gt;\rho&lt;/math&gt;-correlated random variables can be obtained by choosing &lt;math&gt;x,z \sim \mu_p&lt;/math&gt; independently and &lt;math&gt;y \sim N_\rho(x)&lt;/math&gt;, where &lt;math&gt;N_\rho&lt;/math&gt; is given by

:&lt;math&gt; y_i = \begin{cases} x_i &amp; \text{w.p. } \rho, \\ z_i &amp; \text{w.p. } 1-\rho. \end{cases} &lt;/math&gt;

The noise operator is then given by

:&lt;math&gt; (T_\rho f)(x) = \sum_{S \subseteq [n]} \rho^{|S|} \hat{f}(S) \omega_S(x) = \operatorname{\mathbb{E}}_{y \sim N_\rho(x)} [f(y)]. &lt;/math&gt;

Using this we can define the noise stability and the noise sensitivity, as before.

====Russo–Margulis formula====

The Russo–Margulis formula states that for monotone Boolean functions &lt;math&gt;f\colon \{0,1\}^n \to \{0,1\}&lt;/math&gt;,

:&lt;math&gt; \frac{d}{dp} \operatorname{\mathbb{E}}_{x \sim \mu_p} [f(x)] = \frac{\operatorname{Inf}[f]}{p(1-p)} = \sum_{i=1}^n \Pr[f \neq f^{\oplus i}]. &lt;/math&gt;

Both the influence and the probabilities are taken with respect to &lt;math&gt;\mu_p&lt;/math&gt;, and on the right-hand side we have the average sensitivity of &lt;math&gt;f&lt;/math&gt;. If we think of &lt;math&gt;f&lt;/math&gt; as a property, then the formula states that as &lt;math&gt;p&lt;/math&gt; varies, the derivative of the probability that &lt;math&gt;f&lt;/math&gt; occurs at &lt;math&gt;p&lt;/math&gt; equals the average sensitivity at &lt;math&gt;p&lt;/math&gt;.

The Russo–Margulis formula is key for proving sharp threshold theorems such as [[#Friedgut's sharp threshold theorem|Friedgut's]].

===Gaussian space===

One of the deepest results in the area, the [[#Invariance principle|invariance principle]], connects the distribution of functions on the Boolean cube &lt;math&gt;\{-1,1\}^n&lt;/math&gt; to their distribution on ''Gaussian space'', which is the space &lt;math&gt;\mathbb{R}^n&lt;/math&gt; endowed with the standard &lt;math&gt;n&lt;/math&gt;-dimensional [[Gaussian measure]].

Many of the basic concepts of Fourier analysis on the Boolean cube have counterparts in Gaussian space:

* The counterpart of the Fourier expansion in Gaussian space is the Hermite expansion, which is an expansion to an infinite sum (converging in &lt;math&gt;L^2&lt;/math&gt;) of multivariate [[Hermite polynomials]].
* The counterpart of total influence or average sensitivity for the indicator function of a set is Gaussian surface area, which is the Minkowski content of the boundary of the set.
* The counterpart of the noise operator is the [[Ornstein–Uhlenbeck process|Ornstein–Uhlenbeck operator]] (related to the [[Mehler kernel|Mehler transform]]), given by &lt;math&gt;(U_\rho f)(x) = \operatorname{\mathbb{E}}_{z \sim N(0,1)}[f(\rho x + \sqrt{1-\rho^2}z)]&lt;/math&gt;, or alternatively by &lt;math&gt;(U_\rho f)(x) = \operatorname{\mathbb{E}}[f(y)]&lt;/math&gt;, where &lt;math&gt;x,y&lt;/math&gt; is a pair of &lt;math&gt;\rho&lt;/math&gt;-correlated standard Gaussians.
* Hypercontractivity holds (with appropriate parameters) in Gaussian space as well.

Gaussian space is more symmetric than the Boolean cube (for example, it is rotation invariant), and supports continuous arguments which may be harder to get through in the discrete setting of the Boolean cube. The invariance principle links the two settings, and allows deducing results on the Boolean cube from results on Gaussian space.

==Basic results==

===Friedgut–Kalai–Naor theorem===

If &lt;math&gt;f\colon \{-1,1\}^n \to \{-1,1\}&lt;/math&gt; has degree at most 1, then &lt;math&gt;f&lt;/math&gt; is either constant, equal to a coordinate, or equal to the negation of a coordinate. In particular, &lt;math&gt;f&lt;/math&gt; is a ''dictatorship'': a function depending on at most one coordinate.

The Friedgut–Kalai–Naor theorem,&lt;ref&gt;{{cite journal |last1=Friedgut |first1=Ehud |last2=Kalai |first2=Gil |last3=Naor |first3=Assaf |date=2002 |title=Boolean functions whose Fourier transform is concentrated on the first two levels |journal=Adv. Appl. Math. |volume=29 |issue=3 |pages=427–437 |doi=10.1016/S0196-8858(02)00024-6}}&lt;/ref&gt; also known as the ''FKN theorem'', states that if &lt;math&gt;f&lt;/math&gt; ''almost'' has degree 1 then it is ''close'' to a dictatorship. Quantitatively, if &lt;math&gt;f\colon \{-1,1\}^n \to \{-1,1\}&lt;/math&gt; and &lt;math&gt;\|f^{&gt;1}\|^2 &lt; \varepsilon&lt;/math&gt;, then &lt;math&gt;f&lt;/math&gt; is &lt;math&gt;O(\varepsilon)&lt;/math&gt;-close to a dictatorship, that is, &lt;math&gt;\|f - g\|^2 = O(\varepsilon)&lt;/math&gt; for some Boolean dictatorship &lt;math&gt;g&lt;/math&gt;, or equivalently, &lt;math&gt;\Pr[f \neq g] = O(\varepsilon)&lt;/math&gt; for some Boolean dictatorship &lt;math&gt;g&lt;/math&gt;.

Similarly, a Boolean function of degree at most &lt;math&gt;d&lt;/math&gt; depends on at most &lt;math&gt;d2^{d-1}&lt;/math&gt; coordinates, making it a ''junta'' (a function depending on a constant number of coordinates). The Kindler–Safra theorem&lt;ref&gt;{{cite thesis |last=Kindler |first=Guy |date=2002 |title=Property testing, PCP, and juntas |chapter=16 |publisher=Tel Aviv University}}&lt;/ref&gt; generalizes the Friedgut–Kalai–Naor theorem to this setting. It states that if &lt;math&gt;f\colon \{-1,1\}^n \to \{-1,1\}&lt;/math&gt; satisfies &lt;math&gt;\|f^{&gt;d}\|^2 &lt; \varepsilon&lt;/math&gt; then &lt;math&gt;f&lt;/math&gt; is &lt;math&gt;O(\varepsilon)&lt;/math&gt;-close to a Boolean function of degree at most &lt;math&gt;d&lt;/math&gt;.

===Kahn–Kalai–Linial theorem===

The Poincaré inequality for the Boolean cube (which follows from formulas appearing above) states that for a function &lt;math&gt;f\colon \{-1,1\}^n \to \mathbb{R}&lt;/math&gt;,

:&lt;math&gt;\mathbb{V}[f] \leq \operatorname{Inf}[f] \leq \deg f \cdot \mathbb{V}[f]. &lt;/math&gt;

This implies that &lt;math&gt;\max_i \operatorname{Inf}_i[f] \geq \frac{\mathbb{V}[f]}{n}&lt;/math&gt;.

The Kahn–Kalai–Linial theorem,&lt;ref&gt;{{cite conference |title=The influence of variables on Boolean functions. |last1=Kahn |first1=Jeff |last2=Kalai |first2=Gil |last3=Linial |first3=Nati |date=1988 |publisher=IEEE |book-title=Proc. 29th Symp. on Foundations of Computer Science |pages=68–80 |location=White Plains |conference=SFCS'88 |doi=10.1109/SFCS.1988.2192 }}&lt;/ref&gt; also known as the ''KKL theorem'', states that if &lt;math&gt;f&lt;/math&gt; is Boolean then &lt;math&gt;\max_i \operatorname{Inf}_i[f] = \Omega\left(\frac{\log n}{n}\right)&lt;/math&gt;.

The bound given by the Kahn–Kalai–Linial theorem is tight, and is achieved by the ''Tribes'' function of Ben-Or and Linial:&lt;ref&gt;{{cite conference |title=Collective coin flipping, robust voting schemes and minima of Banzhaf values |last1=Ben-Or |first1=Michael |last2=Linial |first2=Nathan |date=1985 |publisher=IEEE |book-title=Proc. 26th Symp. on Foundations of Computer Science |pages=408–416 |location=Portland, Oregon |conference=SFCS'85 |doi=10.1109/SFCS.1985.15}}&lt;/ref&gt;

:&lt;math&gt; (x_{1,1} \land \cdots \land x_{1,w}) \lor \cdots \lor (x_{2^w,1} \land \cdots \land x_{2^w,w}). &lt;/math&gt;

The Kahn–Kalai–Linial theorem was one of the first results in the area, and was the one introducing hypercontractivity into the context of Boolean functions.

===Friedgut's junta theorem===

If &lt;math&gt;f\colon \{-1,1\}^n \to \{-1,1\}&lt;/math&gt; is an &lt;math&gt;M&lt;/math&gt;-junta (a function depending on at most &lt;math&gt;M&lt;/math&gt; coordinates) then &lt;math&gt;\operatorname{Inf}[f] \leq M&lt;/math&gt; according to the Poincaré inequality.

Friedgut's theorem&lt;ref&gt;{{cite journal |last=Friedgut |first=Ehud |date=1998 |title=Boolean functions with low average sensitivity depend on few coordinates |journal=Combinatorica |volume=18 |issue=1 |pages=474–483 |doi=10.1007/PL00009809}}&lt;/ref&gt; is a converse to this result. It states that for any &lt;math&gt;\varepsilon &gt; 0&lt;/math&gt;, the function &lt;math&gt;f&lt;/math&gt; is &lt;math&gt;\varepsilon&lt;/math&gt;-close to a Boolean junta depending on &lt;math&gt;\exp (\operatorname{Inf}[f]/\varepsilon)&lt;/math&gt; coordinates.

Combined with the Russo–Margulis lemma, Friedgut's junta theorem implies that for every &lt;math&gt;p&lt;/math&gt;, every monotone function is close to a junta with respect to &lt;math&gt;\mu_q&lt;/math&gt; for some &lt;math&gt;q \approx p&lt;/math&gt;.

===Invariance principle===

The invariance principle&lt;ref&gt;{{cite journal |last1=Mossel |first1=Elchanan |last2=O'Donnell |first2=Ryan |last3=Oleszkiewicz |first3=Krzysztof |date=2010 |title=Noise stability of functions with low influences: Invariance and optimality |journal=Ann. Math. |volume=171 |issue=1 |pages=295–341 |doi=10.4007/annals.2010.171.295|arxiv=math/0503503 }}&lt;/ref&gt; generalizes the [[Berry–Esseen theorem]] to non-linear functions.

The Berry–Esseen theorem states (among else) that if &lt;math&gt;f = \sum_{i=1}^n c_i x_i&lt;/math&gt; and no &lt;math&gt;c_i&lt;/math&gt; is too large compared to the rest, then the distribution of &lt;math&gt;f&lt;/math&gt; over &lt;math&gt;\{-1,1\}^n&lt;/math&gt; is close to a normal distribution with the same mean and variance.

The invariance principle (in a special case) informally states that if &lt;math&gt;f&lt;/math&gt; is a multilinear polynomial of bounded degree over &lt;math&gt;x_1,\ldots,x_n&lt;/math&gt; and all influences of &lt;math&gt;f&lt;/math&gt; are small, then the distribution of &lt;math&gt;f&lt;/math&gt; under the uniform measure over &lt;math&gt;\{-1,1\}^n&lt;/math&gt; is close to its distribution in Gaussian space.

More formally, let &lt;math&gt;\psi&lt;/math&gt; be a univariate [[Lipschitz continuity|Lipschitz function]], let &lt;math&gt;f = \sum_{S \subseteq [n]} \hat{f}(S) \chi_S&lt;/math&gt;, let &lt;math&gt;k=\deg f&lt;/math&gt;, and let
&lt;math&gt; \varepsilon = \max_i \sum_{S \ni i} \hat{f}(S)^2&lt;/math&gt;. Suppose that &lt;math&gt;\sum_{S \neq \emptyset} \hat{f}(S)^2 \leq 1&lt;/math&gt;. Then

:&lt;math&gt; \left| \mathbb{E}_{x \sim \{-1,1\}^n} [\psi(f(x))] - \mathbb{E}_{g \sim N(0,I)} [\psi(f(g))] \right| = O(k9^k \varepsilon). &lt;/math&gt;

By choosing appropriate &lt;math&gt;\psi&lt;/math&gt;, this implies that the distributions of &lt;math&gt;f&lt;/math&gt; under both measures are close in [[Cumulative distribution function|CDF distance]], which is given by &lt;math&gt;\sup_t |\Pr[f(x)&lt;t] - \Pr[f(g)&lt;t]|&lt;/math&gt;.

The invariance principle was the key ingredient in the original proof of the [[#Majority is Stablest|''Majority is Stablest'' theorem]].

==Some applications==

===Linearity testing===

A Boolean function &lt;math&gt;f\colon \{-1,1\}^n \to \{-1,1\}&lt;/math&gt; is ''linear'' if it satisfies &lt;math&gt;f(xy) = f(x)f(y)&lt;/math&gt;, where &lt;math&gt;xy = (x_1y_1,\ldots,x_ny_n)&lt;/math&gt;. It is not hard to show that the Boolean linear functions are exactly the characters &lt;math&gt;\chi_S&lt;/math&gt;.

In [[property testing]] we want to test whether a given function is linear. It is natural to try the following test: choose &lt;math&gt;x,y \in \{-1,1\}^n&lt;/math&gt; uniformly at random, and check that &lt;math&gt;f(xy) = f(x)f(y)&lt;/math&gt;. If &lt;math&gt;f&lt;/math&gt; is linear then it always passes the test. Blum, Luby and Rubinfeld&lt;ref&gt;{{cite journal |last1=Blum |first1=Manuel |last2=Luby |first2=Michael |last3=Rubinfeld |first3=Ronitt |date=1993 |title=Self-testing/correcting with applications to numerical problems |journal=J. Comput. Syst. Sci. |volume=47 |number=3 |pages=549–595 |doi=10.1016/0022-0000(93)90044-W}}&lt;/ref&gt; showed that if the test passes with probability &lt;math&gt;1-\varepsilon&lt;/math&gt; then &lt;math&gt;f&lt;/math&gt; is &lt;math&gt;O(\varepsilon)&lt;/math&gt;-close to a Fourier character. Their proof was combinatorial.

Bellare et al.&lt;ref&gt;{{cite conference |last1=Bellare |first1=Mihir |last2=Coppersmith |first2=Don |last3=Håstad |first3=Johan |last4=Kiwi |first4=Marcos |last5=Sudan |first5=Madhu |date=1995 |title= Linearity testing in characteristic two |booktitle = Proc. 36th Symp. on Foundations of Computer Science |conference=FOCS'95}}&lt;/ref&gt; gave an extremely simple Fourier-analytic proof, that also shows that if the test succeeds with probability &lt;math&gt;1/2 + \varepsilon&lt;/math&gt;, then &lt;math&gt;f&lt;/math&gt; is correlated with a Fourier character. Their proof relies on the following formula for the success probability of the test:

:&lt;math&gt; \frac{1}{2} + \frac{1}{2} \sum_{S \subseteq [n]} \hat{f}(S)^3. &lt;/math&gt;

===Arrow's theorem===

[[Arrow's impossibility theorem]] states that for three and more candidates, the only unanimous voting rule for which there is always a [[Condorcet criterion|Condorcet winner]] is a dictatorship.

The usual proof of Arrow's theorem is combinatorial. Kalai&lt;ref&gt;{{cite journal |last=Kalai |first=Gil |date=2002 |title=A Fourier-theoretic perspective on the Condorcet paradox and Arrow's theorem |journal=Adv. Appl. Math. |volume=29 |number=3 |pages=412–426 |doi=10.1016/S0196-8858(02)00023-4}}&lt;/ref&gt; gave an alternative proof of this result in the case of three candidates using Fourier analysis. If &lt;math&gt;f\colon \{-1,1\}^n \to \{-1,1\}&lt;/math&gt; is the rule that assigns a winner among two candidates given their relative orders in the votes, then the probability that there is a Condorcet winner given a uniformly random vote is &lt;math&gt;\frac{3}{4} - \frac{3}{4} \operatorname{Stab}_{-1/3}[f]&lt;/math&gt;, from which the theorem easily follows.

The [[#Friedgut–Kalai–Naor theorem|FKN theorem]] implies that if &lt;math&gt;f&lt;/math&gt; is a rule for which there is almost always a Condorcet winner, then &lt;math&gt;f&lt;/math&gt; is close to a dictatorship.

===Sharp thresholds===

A classical result in the theory of [[random graph]]s states that the probability that a &lt;math&gt;G(n,p)&lt;/math&gt; random graph is connected tends to &lt;math&gt;e^{-e^{-c}}&lt;/math&gt; if &lt;math&gt;p \sim \frac{\log n + c}{n}&lt;/math&gt;. This is an example of a ''sharp threshold'': the width of the "threshold window", which is &lt;math&gt;O(1/n)&lt;/math&gt;, is asymptotically smaller than the threshold itself, which is roughly &lt;math&gt;\frac{\log n}{n}&lt;/math&gt;. In contrast, the probability that a &lt;math&gt;G(n,p)&lt;/math&gt; graph contains a triangle tends to &lt;math&gt;e^{-c^3/6}&lt;/math&gt; when &lt;math&gt;p \sim \frac{c}{n}&lt;/math&gt;. Here both the threshold window and the threshold itself are &lt;math&gt;\Theta(1/n)&lt;/math&gt;, and so this is a ''coarse threshold''.

Friedgut's sharp threshold theorem&lt;ref&gt;{{cite journal |last=Friedgut |first=Ehud |date=1999 |title=Sharp thresholds of graph properties and the k-SAT problem |journal=J. Am. Math. Soc. |volume=12 |issue=4 |pages=1017–1054 |doi=10.1090/S0894-0347-99-00305-7}}&lt;/ref&gt; states, roughly speaking, that a monotone graph property (a graph property is a property which doesn't depend on the names of the vertices) has a sharp threshold unless it is correlated with the appearance of small subgraphs. This theorem has been widely applied to analyze random graphs and [[percolation]].

On a related note, the [[#Kahn–Kalai–Linial theorem|KKL theorem]] implies that the width of threshold window is always at most &lt;math&gt;O(1/\log n)&lt;/math&gt;.&lt;ref&gt;{{cite journal |last1=Friedgut |first1=Ehud |last2=Kalai |first2=Gil |date=1996 |title=Every monotone graph property has a sharp threshold |journal= Proc. Am. Math. Soc. |volume=124 |issue=10 |pages=2993–3002 |doi=10.1090/S0002-9939-96-03732-X}}&lt;/ref&gt;

===Majority is Stablest===

Let &lt;math&gt;\operatorname{Maj}_n\colon \{-1,1\}^n \to \{-1,1\}&lt;/math&gt; denote the majority function on &lt;math&gt;n&lt;/math&gt; coordinates. Sheppard's formula gives the asymptotic noise stability of majority:

:&lt;math&gt; \operatorname{Stab}_\rho[\operatorname{Maj}_n] \longrightarrow 1 - \frac{2}{\pi} \arccos \rho. &lt;/math&gt;

This is related to the probability that if we choose &lt;math&gt;x \in \{-1,1\}^n&lt;/math&gt; uniformly at random and form &lt;math&gt;y \in \{-1,1\}^n&lt;/math&gt; by flipping each bit of &lt;math&gt;x&lt;/math&gt; with probability &lt;math&gt;\frac{1-\rho}{2}&lt;/math&gt;, then the majority stays the same:
:&lt;math&gt; \operatorname{Stab}_\rho[\operatorname{Maj}_n] = 2\Pr[\operatorname{Maj}_n(x) = \operatorname{Maj}_n(y)]-1&lt;/math&gt;.

There are Boolean functions with larger noise stability. For example, a dictatorship &lt;math&gt;x_i&lt;/math&gt; has noise stability &lt;math&gt;\rho&lt;/math&gt;.

The Majority is Stablest theorem states, informally, then the only functions having noise stability larger than majority have influential coordinates. Formally, for every &lt;math&gt;\varepsilon &gt; 0&lt;/math&gt; there exists &lt;math&gt;\tau &gt; 0&lt;/math&gt; such that if &lt;math&gt;f\colon \{-1,1\}^n \to \{-1,1\}&lt;/math&gt; has expectation zero and &lt;math&gt;\max_i \operatorname{Inf}_i[f] \leq \tau&lt;/math&gt;, then &lt;math&gt;\operatorname{Stab}_\rho[f] \leq 1 - \frac{2}{\pi} \arccos \rho + \varepsilon&lt;/math&gt;.

The first proof of this theorem used the [[#Invariance principle|invariance principle]] in conjunction with an isoperimetric theorem of Borell in Gaussian space; since then more direct proofs were devised.

Majority is Stablest implies that the [[Semidefinite programming#Example 3 .28Goemans-Williamson MAX CUT approximation algorithm.29|Goemans–Williamson approximation algorithm]] for [[Maximum cut|MAX-CUT]] is optimal, assuming the [[unique games conjecture]]. This implication, due to Khot et al.,&lt;ref&gt;{{citation
 | author1-link = Subhash Khot
 | last1 = Khot | first1 = Subhash
 | last2 = Kindler | first2 = Guy
 | last3 = Mossel | first3 = Elchanan
 | last4 = O'Donnell | first4 = Ryan
 | doi = 10.1137/S0097539705447372
 | issue = 1
 | journal = [[SIAM Journal on Computing]]
 | pages = 319–357
 | title = Optimal inapproximability results for MAX-CUT and other two-variable CSPs?
 | url = http://www.cs.cornell.edu/~abrahao/tdg/papers/KKMO-maxcut.pdf
 | volume = 37
 | year = 2007
}}&lt;/ref&gt; was the impetus behind proving the theorem.

==References==
{{reflist}}

[[Category:Boolean algebra]]
[[Category:Mathematical optimization]]
[[Category:Mathematics]]
[[Category:Theoretical computer science]]</text>
      <sha1>8sr7va454z7zu2008i1dy1otkfvjiaf</sha1>
    </revision>
  </page>
  <page>
    <title>Antiunitary operator</title>
    <ns>0</ns>
    <id>3443011</id>
    <revision>
      <id>866275793</id>
      <parentid>840168234</parentid>
      <timestamp>2018-10-29T11:11:56Z</timestamp>
      <contributor>
        <username>Johnnyblade86</username>
        <id>12116852</id>
      </contributor>
      <minor/>
      <comment>changed three times C to \mathbb{C}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5061">In [[mathematics]], an '''antiunitary transformation''', is a bijective [[antilinear map]]

:&lt;math&gt;U:H_1\to H_2\,&lt;/math&gt;

between two [[complex number|complex]] Hilbert spaces such that 

:&lt;math&gt;\langle Ux, Uy \rangle = \overline{\langle x, y \rangle}&lt;/math&gt;

for all &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt; in &lt;math&gt;H_1&lt;/math&gt;, where the horizontal bar represents the [[complex conjugate]]. If additionally one has &lt;math&gt;H_1 = H_2&lt;/math&gt; then U is called an '''antiunitary operator'''.

Antiunitary operators are important in Quantum Theory because they are used to represent certain symmetries, such as [[T-symmetry#Time_reversal_in_quantum_mechanics|time-reversal]] symmetry.  Their fundamental importance in quantum physics is further demonstrated by [[Wigner's Theorem]]. 

==Invariance transformations==
In [[Quantum mechanics]], the invariance transformations of complex Hilbert space &lt;math&gt; H &lt;/math&gt; leave the absolute value of scalar product invariant:

:&lt;math&gt; |\langle Tx, Ty \rangle| =|\langle x, y \rangle|&lt;/math&gt; 

for all &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt; in &lt;math&gt;H&lt;/math&gt;.
Due to [[Wigner's Theorem]] these transformations fall into two categories, they can be [[Unitary_operator|unitary]] or antiunitary. 

===Geometric Interpretation===

[[Congruence_(geometry)|Congruences]] of the plane form two distinct classes. The first conserves the orientation and is generated by translations and rotations. The second does not conserve the orientation and is obtained from the first class by applying a reflection. On the complex plane these two classes corresponds (up to translation) to unitaries and antiunitaries, respectively.

==Properties==
*&lt;math&gt; \langle Ux, Uy \rangle = \overline{\langle x, y \rangle} = \langle y, x \rangle &lt;/math&gt; holds for all elements &lt;math&gt; x, y &lt;/math&gt; of the Hilbert space and an antiunitary &lt;math&gt; U &lt;/math&gt;.
*When &lt;math&gt; U &lt;/math&gt; is antiunitary then &lt;math&gt; U^2 &lt;/math&gt; is unitary. This follows from
:&lt;math&gt;\langle U^2x, U^2y \rangle = \overline{\langle Ux , Uy \rangle} = \langle x, y \rangle .&lt;/math&gt;
*For unitary operator &lt;math&gt; V &lt;/math&gt; the operator &lt;math&gt; VK &lt;/math&gt;, where &lt;math&gt; K &lt;/math&gt; is complex conjugate operator, is antiunitary. The reverse is also true, for antiunitary &lt;math&gt; U &lt;/math&gt; the operator &lt;math&gt; UK &lt;/math&gt; is unitary.
*For antiunitary &lt;math&gt; U &lt;/math&gt; the definition of the [[Hermitian_adjoint#Adjoints_of_antilinear_operators|adjoint]] operator &lt;math&gt; U^*&lt;/math&gt; is changed into{{or||date=May 2018}}
:&lt;math&gt;\langle U x,y\rangle=\overline{\langle x,U^*y\rangle}&lt;/math&gt;.
* The adjoint of an antiunitary &lt;math&gt;U&lt;/math&gt; is also antiunitary and 
:&lt;math&gt; U U^* = U^* U = 1. &lt;/math&gt; (This is not to be confused with the definition of [[Unitary_operator|unitary operators]], as the antiunitary operator &lt;math&gt; U &lt;/math&gt; is not complex linear.){{or|date=May 2018}}

==Examples==
*The complex conjugate operator &lt;math&gt; K, K z = \overline{z}, &lt;/math&gt; is an antiunitary operator on the complex plane.
*The operator
:&lt;math&gt;
U = i\sigma_y K =
\begin{pmatrix}
0&amp;1\\
-1&amp;0
\end{pmatrix} K,
&lt;/math&gt;
where &lt;math&gt; \sigma_y &lt;/math&gt; is the second [[Pauli_matrices|Pauli matrix]] and &lt;math&gt; K &lt;/math&gt; is the complex conjugate operator, is antiunitary. It satisfies &lt;math&gt; U^2 = -1 &lt;/math&gt;.

==Decomposition of an antiunitary operator into a direct sum of elementary Wigner antiunitaries==

An antiunitary operator on a finite-dimensional space may be decomposed as a direct sum of elementary Wigner antiunitaries &lt;math&gt;W_\theta&lt;/math&gt;, &lt;math&gt;0\le\theta\le\pi&lt;/math&gt;. The operator &lt;math&gt;W_0:\mathbb{C}\rightarrow \mathbb{C}&lt;/math&gt; is just simple complex conjugation on &lt;math&gt;\mathbb{C}&lt;/math&gt;

:&lt;math&gt;W_0(z)=\overline{z}\,&lt;/math&gt;

For &lt;math&gt;0&lt;\theta\le\pi&lt;/math&gt;, the operator &lt;math&gt;W_\theta&lt;/math&gt; acts on two-dimensional complex Hilbert space. It is defined by 

:&lt;math&gt;W_\theta((z_1,z_2)) = (e^{i\theta/2} \overline{z_2}, e^{-i\theta/2}\overline{z_1}). \, &lt;/math&gt;

Note that for &lt;math&gt;0&lt;\theta\le\pi&lt;/math&gt;

:&lt;math&gt;W_\theta(W_\theta((z_1,z_2)))=(e^{i\theta}z_1,e^{-i\theta}z_2),\,&lt;/math&gt;

so such &lt;math&gt;W_\theta&lt;/math&gt; may not be further decomposed into &lt;math&gt;W_0&lt;/math&gt;'s, which square to the identity map.

Note that the above decomposition of antiunitary operators contrasts with the spectral decomposition of unitary operators.  In particular, a unitary operator on a complex Hilbert space may be decomposed into a direct sum of unitaries acting on 1-dimensional complex spaces (eigenspaces), but an antiunitary operator may only be decomposed into a direct sum of elementary operators on 1- and 2-dimensional complex spaces.

==References==
&lt;references/&gt;
*Wigner, E. "Normal Form of Antiunitary Operators", Journal of Mathematical Physics Vol 1, no 5, 1960, pp. 409&amp;ndash;412
*Wigner, E. "Phenomenological Distinction between Unitary and Antiunitary Symmetry Operators", Journal of Mathematical Physics Vol1, no5, 1960, pp.414&amp;ndash;416

==See also==
*[[Unitary operator]]
*[[Wigner's Theorem]]
*[[Particle physics and representation theory]]

[[Category:Linear algebra]]
[[Category:Functional analysis]]</text>
      <sha1>i7mea5nn46eu33phkllh27t7ofc9caq</sha1>
    </revision>
  </page>
  <page>
    <title>Architectural rendering</title>
    <ns>0</ns>
    <id>2328909</id>
    <revision>
      <id>868506231</id>
      <parentid>865771548</parentid>
      <timestamp>2018-11-12T17:04:56Z</timestamp>
      <contributor>
        <username>Mtpanchal</username>
        <id>25337644</id>
      </contributor>
      <comment>/* Computer generated renderings */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4519">'''Architectural rendering''', or '''architectural illustration''', is the art of creating two-dimensional images or animations showing the attributes of a proposed [[architecture|architectural]] design.

[[File:Canada Permanent Building.jpg|thumb|Architectural rendering of the Canada Permanent Trust Building, Toronto, Canada]]


==Computer generated renderings==
[[File:Architectural render (Blender).jpg|thumb|right|Architectural render made in [[Blender (software)|Blender]]]]
[[File:Architectural render 02 (Blender).jpg|thumb|Same architectural render showing different rendering styles]]
Images that are generated by a computer using three-dimensional modeling software or other computer software for presentation purposes are commonly termed "Computer Generated Renderings".&lt;ref&gt;{{Cite web|title = What Is Architectural Rendering? (with pictures)|url = http://www.wisegeek.com/what-is-architectural-rendering.htm|accessdate = 2015-06-27}}&lt;/ref&gt; Rendering techniques vary. Some methods create simple flat images or images with basic shadows. A popular technique uses sophisticated software to approximate accurate lighting and materials. This technique is often referred to as a "Photo Real" rendering. Renderings are usually created for presentation, marketing and design analysis purposes.

*Still renderings
*3D Walk through and fly by animations (movie)
*Virtual Tours
*Floor Plans
*Photo realistic 3D Rendering
*Realtime 3D Renderings
*Panoramic Renderings
*Light and Shadow ([[sciography]]) study renderings
*Renovation Renderings (photomontage)
*and others

3D renderings play a major role in real estate marketing and sales. It also makes it possible to make design related decisions well before the building is actually built. Thus it helps experimenting with building design and its visual aspects.

==Common Types of Architectural Renderings==

Architectural renderings are often categorized into 3 sub-types: Exterior Renderings, Interior Renderings, and Aerial Renderings. 

Exterior renderings are defined as images where the vantage point or viewing angle is located outside of the building, while interior renderings are defined as images where the vantage point or viewing angle is located inside of the building. Aerial renderings are similar to exterior renderings however their viewing angle is located outside and above the building, looking down, usually at an angle.&lt;ref&gt;{{Cite web|title = What Are Architectural Renderings? |url = https://growthmedia.co/blog/what-are-architectural-renderings|accessdate = 2018-08-21}}&lt;/ref&gt;

==Hand-drawn renderings or architectural illustration==
Until 3D computer modeling became common, most architectural renderings were created by hand. There are still many architectural illustrators who create renderings entirely by hand. Some hand illustrators use a combination of hand and computer generated linework. Common mediums for hand architectural renderings include:

*Watercolor
*Markers
*Color Pencil
*Pen and Ink
*Acrylics

==Awards==
*The [[Hugh Ferriss Memorial Prize]] is awarded by the American Society of Architectural Illustrators in recognition of excellence in the graphic representation of architecture. It is the Society's highest award.
*The CGarchitect Architectural 3D Awards are awarded by CGarchitect.com in recognition of outstanding achievement in the field of [[Computer-generated imagery|computer-generated]] architectural rendering. The awards were started in 2004 and award in five main categories: Best Architectural Image, Best Architectural Film, Best Student Image, Best Student Film, Best Interactive Presentation/Emerging Technology.

==Education==
Traditionally rendering techniques were taught in a "master class" practice (such as the [[École des Beaux-Arts]]), where a student works creatively with a mentor in the study of fine arts. Contemporary architects use hand-drawn sketches, pen and ink drawings, and watercolor renderings to represent their design with the vision of an artist. Computer generated [[graphics]] is the newest medium to be utilized by architectural [[illustrator]]s.

==See also==
*[[3D rendering]]
*[[Architectural animation]]
*[[Architectural illustrator]]
*[[Concept art]]
*[[Museum for Architectural Drawing]], Berlin, Germany

==References==
{{refimprove|date=June 2015}}
{{reflist}}

[[Category:Architectural communication|Rendering]]
[[Category:Architecture occupations|Rendering]]
[[Category:Computer-aided design]]
[[Category:Perspective projection]]
[[Category:Technical drawing]]</text>
      <sha1>hmg5e8rgtw3ei4fe15r2c8gptsehst9</sha1>
    </revision>
  </page>
  <page>
    <title>Beez's theorem</title>
    <ns>0</ns>
    <id>34948394</id>
    <revision>
      <id>745459209</id>
      <parentid>637656779</parentid>
      <timestamp>2016-10-21T07:55:01Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor/>
      <comment>/* References */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="628">
In [[mathematics]], '''Beez's theorem''', introduced by [[Richard Beez]] in 1875, implies that if ''n''&amp;nbsp;&gt;&amp;nbsp;3 then an (''n''&amp;nbsp;–&amp;nbsp;1)-dimensional [[hypersurface]] in '''R'''&lt;sup&gt;''n''&lt;/sup&gt; cannot be deformed.

==References==
{{Refbegin}}
* {{Citation |last1=Laptev |first1=B. L. |last2=Rozenfeld |first2=Boris A. |last3=Markushevich |first3=A. I. |title=Mathematics of the 19th century |url=https://books.google.com/books?id=L6z2VmCOQtIC |publisher=Birkhäuser Verlag |isbn=978-3-7643-5048-2 |mr=1401111 |year=1996 }}
{{Refend}}


[[Category:Theorems in differential geometry]]


{{differential-geometry-stub}}</text>
      <sha1>l1uvnj5k6wpdap1yr16swf3k87kef6h</sha1>
    </revision>
  </page>
  <page>
    <title>Borel's theorem</title>
    <ns>0</ns>
    <id>43948116</id>
    <revision>
      <id>702117437</id>
      <parentid>702117372</parentid>
      <timestamp>2016-01-28T15:24:43Z</timestamp>
      <contributor>
        <username>RHaworth</username>
        <id>161142</id>
      </contributor>
      <minor/>
      <comment>RHaworth moved page [[Draft:Borel's theorem on classifying spaces]] to [[Borel's theorem]] without leaving a redirect</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="805">{{distinguish|Borel's theorem on power series}}
In [[topology]], a branch of [[mathematics]], '''Borel's theorem''', due to [[Armand Borel]], says the [[cohomology ring]] of a [[classifying space]] or a [[classifying stack]] is a [[polynomial ring]].&lt;ref&gt;{{harvnb|Behrend|loc=Theorem 6.1.6.}}&lt;/ref&gt;

== See also ==
*[[Atiyah–Bott formula]]

== References ==
{{reflist}}
*A. Borel. Sur la cohomologie des éspaces fibrés principaux et des éspaces  homogènes de groupes de Lie compacts. Annals of Mathematics, 57:115–207, 1953.
*Behrend, K. [http://www.math.ubc.ca/~behrend/ladic.pdf Derived l-adic categories for algebraic stacks.] Memoirs of the American Mathematical Society Vol. 163, 2003

{{topology-stub}}

[[Category:Theorems in algebraic topology]]
[[Category:Theorems in algebraic geometry]]</text>
      <sha1>7rteo5wsm1afn8in32thhjjdnqhfz84</sha1>
    </revision>
  </page>
  <page>
    <title>Chris Freiling</title>
    <ns>0</ns>
    <id>162265</id>
    <revision>
      <id>857348215</id>
      <parentid>783525581</parentid>
      <timestamp>2018-08-31T03:57:54Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>/* External links */add authority control, test</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3741">'''Christopher Francis Freiling''' is a mathematician responsible for [[Freiling's axiom of symmetry]] in [[set theory]].&lt;ref&gt;{{citation
 | last = Mumford | first = David | author-link = David Mumford
 | editor1-last = Arnold | editor1-first = V. | editor1-link = Vladimir Arnold
 | editor2-last = Atiyah | editor2-first = M. | editor2-link = Michael Atiyah
 | editor3-last = Lax | editor3-first = P. | editor3-link = Peter Lax
 | editor4-last = Mazur | editor4-first = B. | editor4-link = Barry Mazur
 
 | contribution = The dawning of the age of stochasticity
 | location = Providence, RI
 | mr = 1754778
 | pages = 197–218
 | publisher = American Mathematical Society
 | title = Mathematics: Frontiers and Perspectives
 | year = 2000}}. See in particular [https://books.google.com/books?id=qjVaD1OQbxEC&amp;pg=PA208 p. 208]: "This leads us to the stunning result of Christopher Freiling (1986): using the idea of throwing darts, we can disprove the continuum hypothesis."&lt;/ref&gt; He has also made significant contributions to [[coding theory]], in the process establishing connections between that field and [[matroid theory]].&lt;ref&gt;{{citation|title=Network Information Theory|first1=Abbas|last1=El Gamal|author1-link=Abbas El Gamal|first2=Young-Han|last2=Kim|publisher=Cambridge University Press|year=2011|isbn=9781139503143|page=171|url=https://books.google.com/books?id=Ack2AAAAQBAJ&amp;pg=RA1-PA171|quote=Dougherty, Freiling, and Zeger (2005) showed via an ingenious counterexample that unlike the multicast case, linear network coding fails to achieve the capacity region of a general graphical multimessage network error-free. This counterexample hinges on a deep connection between linear network coding and matroid theory.}}&lt;/ref&gt;

Freiling obtained his Ph.D. in 1981 from the [[University of California, Los Angeles]] under the supervision of [[Donald A. Martin]].&lt;ref&gt;{{mathgenealogy|id=46539}}&lt;/ref&gt; He is a member of the faculty of the Department of Mathematics at [[California State University, San Bernardino]].&lt;ref&gt;[http://www.math.csusb.edu/index.php?main=staffDirectory.html Faculty/staff directory], CSUSB Mathematics Department, retrieved 2015-04-11.&lt;/ref&gt;

==Selected publications==
*{{Citation | last1=Freiling | first1=Chris | title=Axioms of symmetry: throwing darts at the real number line | doi=10.2307/2273955 | mr=830085 | year=1986 | journal=The Journal of Symbolic Logic | issn=0022-4812 | volume=51 | issue=1 | pages=190–200}}
*{{citation
 | last1 = Dougherty | first1 = Randall | author1-link = Randall Dougherty
 | last2 = Freiling | first2 = Christopher
 | last3 = Zeger | first3 = Kenneth
 | doi = 10.1109/TIT.2005.851744
 | issue = 8
 | journal = IEEE Transactions on Information Theory
 | pages = 2745–2759
 | title = Insufficiency of linear coding in network information flow
 | volume = 51
 | year = 2005}}.
*{{citation
 | last1 = Dougherty | first1 = Randall | author1-link = Randall Dougherty
 | last2 = Freiling | first2 = Chris
 | last3 = Zeger | first3 = Kenneth
 | doi = 10.1109/TIT.2007.896862
 | issue = 6
 | journal = IEEE Transactions on Information Theory
 | mr = 2321860
 | pages = 1949–1969
 | title = Networks, matroids, and non-Shannon information inequalities
 | volume = 53
 | year = 2007}}.

==References==
{{reflist}}

==External links==
* [http://www.math.csusb.edu/faculty/freiling/index.php Home page]


{{authority control}}

{{DEFAULTSORT:Freiling, Chris}}
[[Category:20th-century American mathematicians]]
[[Category:21st-century mathematicians]]
[[Category:Set theorists]]
[[Category:Coding theorists]]
[[Category:University of California, Los Angeles alumni]]
[[Category:California State University, San Bernardino faculty]]
[[Category:Living people]]


{{Mathematician-stub}}</text>
      <sha1>s5nsuyhlxnjk7wvdvcqtn54h3zkaunq</sha1>
    </revision>
  </page>
  <page>
    <title>Convex analysis</title>
    <ns>0</ns>
    <id>3237201</id>
    <revision>
      <id>846758048</id>
      <parentid>846752603</parentid>
      <timestamp>2018-06-20T18:25:53Z</timestamp>
      <contributor>
        <username>Allforrous</username>
        <id>12120664</id>
      </contributor>
      <comment>removed [[Category:Convex optimization]]; added [[Category:Convex geometry]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8000">[[File:3dpoly.svg|thumb|right|A 3-dimensional convex polytope. Convex analysis includes not only the study of convex subsets of Euclidean spaces but also the study of convex functions on abstract spaces.]]
'''Convex analysis''' is the branch of [[mathematics]] devoted to the study of properties of [[convex function]]s and [[convex set]]s, often with applications in [[convex optimization|convex minimization]], a subdomain of [[optimization (mathematics)|optimization theory]].

== Convex sets ==
{{main|Convex set}}
A '''convex set''' is a set ''C'' ⊆ ''X'', for some [[vector space]] ''X'', such that for any ''x'', ''y'' ∈ ''C'' and λ ∈ [0, 1] then&lt;ref name="Rockafellar"&gt;{{cite book |author=[[Rockafellar, R. Tyrrell]]|title=Convex Analysis|publisher=Princeton University Press|location=Princeton, NJ|year=1997|origyear=1970|isbn=978-0-691-01586-6}}&lt;/ref&gt;
:&lt;math&gt;\lambda x + (1 - \lambda)y \in C&lt;/math&gt;.

== Convex functions ==
{{main|Convex function}}
A '''convex function''' is any [[extended reals|extended real-valued]] function ''f'' : ''X'' → '''R''' ∪ {±∞} which satisfies [[Jensen's inequality]], i.e. for any ''x'', ''y'' ∈ ''X'' and any λ ∈ [0, 1] then
:&lt;math&gt;f(\lambda x + (1 - \lambda)y) \leq \lambda f(x) + (1-\lambda) f(y)&lt;/math&gt;.&lt;ref name="Rockafellar" /&gt;

Equivalently, a convex function is any (extended) real valued function such that its [[epigraph (mathematics)|epigraph]]
:&lt;math&gt;\left\{(x,r) \in X \times \mathbf{R}: f(x) \leq r \right\}&lt;/math&gt;
is a convex set.&lt;ref name="Rockafellar" /&gt;

== Convex conjugate ==
{{main|Convex conjugate}}
The '''convex conjugate''' of an extended real-valued (not necessarily convex) function ''f'' : ''X'' → '''R''' ∪ {±∞} is ''f*'' : ''X*'' → '''R''' ∪ {±∞} where ''X*'' is the [[dual space]] of ''X'', and&lt;ref name="Zalinescu" /&gt;{{rp|pp.75–79}}

: &lt;math&gt;f^*(x^*) = \sup_{x \in X} \left \{\langle x^*,x \rangle - f(x) \right \}.&lt;/math&gt;

=== Biconjugate ===
The ''biconjugate'' of a function ''f'' : ''X'' → '''R''' ∪ {±∞} is the conjugate of the conjugate, typically written as ''f**'' : ''X'' → '''R''' ∪ {±∞}. The biconjugate is useful for showing when [[strong duality|strong]] or [[weak duality]] hold (via the [[perturbation function]]).

For any ''x'' ∈ ''X'' the inequality ''f**''(''x'') ≤ ''f''(''x'') follows from the ''Fenchel–Young inequality''.  For [[Proper convex function|proper functions]], ''f'' = ''f**'' [[if and only if]] ''f'' is convex and [[lower semi-continuous]] by [[Fenchel–Moreau theorem]].&lt;ref name="Zalinescu" /&gt;{{rp|pp.75–79}}&lt;ref name="BorweinLewis"&gt;{{cite book |last1=Borwein |first1=Jonathan |last2=Lewis |first2=Adrian |title=Convex Analysis and Nonlinear Optimization: Theory and Examples| edition=2 |year=2006 |publisher=Springer |isbn=978-0-387-29570-1|pages=76–77}}&lt;/ref&gt;

== Convex minimization ==
{{main|Convex optimization}}
A '''convex minimization''' (primal) problem is one of the form

:&lt;math&gt;\inf_{x \in M} f(x)&lt;/math&gt;

such that ''f'' : ''X'' → '''R''' ∪ {±∞} is a convex function and ''M'' ⊆ ''X'' is a convex set.

=== Dual problem ===
{{main|Duality (optimization)}}
&lt;!-- Copied from [[Duality (optimization)#Duality principle]] --&gt;
In optimization theory, the ''duality principle'' states that optimization problems may be viewed from either of two perspectives, the primal problem or the dual problem.

In general given two [[dual pair]]s [[separated space|separated]] [[locally convex space]]s (''X'', ''X*'') and (''Y'', ''Y*''). Then given the function ''f'' : ''X'' → '''R''' ∪ {+∞}, we can define the primal problem as finding ''x'' such that

:&lt;math&gt;\inf_{x \in X} f(x).&lt;/math&gt;

If there are constraint conditions, these can be built into the function ''f'' by letting &lt;math&gt;f = f + I_{\mathrm{constraints}}&lt;/math&gt; where ''I'' is the [[Characteristic function (convex analysis)|indicator function]].  Then let ''F'' : ''X'' × ''Y'' → '''R''' ∪ {±∞} be a [[perturbation function]] such that ''F''(''x'', 0) = ''f''(''x'').&lt;ref name="BWG"&gt;{{cite book |title=Duality in Vector Optimization |author1=Boţ, Radu Ioan |author2=Wanka, Gert|author3=Grad, Sorin-Mihai |year=2009 | publisher=Springer |isbn=978-3-642-02885-4 }}&lt;/ref&gt;

The ''dual problem'' with respect to the chosen perturbation function is given by

:&lt;math&gt;\sup_{y^* \in Y^*} -F^*(0,y^*)&lt;/math&gt;

where ''F*'' is the convex conjugate in both variables of ''F''.

The [[duality gap]] is the difference of the right and left hand sides of the inequality&lt;ref name="Zalinescu"&gt;{{cite book |last=Zălinescu |first=Constantin |title=Convex analysis in general vector spaces |publisher=World Scientific Publishing&amp;nbsp;Co.,&amp;nbsp;Inc. |isbn=981-238-067-1 |mr=1921556 |issue=J |year=2002 |location=River Edge, NJ }}&lt;/ref&gt;{{rp|pp. 106–113}}&lt;ref name="BWG" /&gt;&lt;ref&gt;{{cite book |title=Overcoming the failure of the classical generalized interior-point regularity conditions in convex optimization. Applications of the duality theory to enlargements of maximal monotone operators |author=Csetnek, Ernö Robert |year=2010 |publisher=Logos Verlag Berlin GmbH |isbn=978-3-8325-2503-3 }}&lt;/ref&gt;

:&lt;math&gt;\sup_{y^* \in Y^*} -F^*(0,y^*) \le \inf_{x \in X} F(x,0).&lt;/math&gt;

This principle is the same as [[weak duality]].  If the two sides are equal to each other, then the problem is said to satisfy [[strong duality]].

There are many conditions for strong duality to hold such as:
*''F'' = ''F**'' where ''F'' is the [[perturbation function]] relating the primal and dual problems and ''F**'' is the [[convex conjugate|biconjugate]] of ''F'';{{Citation needed|date=January 2012}}
* the primal problem is a [[linear optimization|linear optimization problem]];
* [[Slater's condition]] for a [[convex optimization|convex optimization problem]].&lt;ref name="borwein"&gt;{{cite book |last1=Borwein |first1=Jonathan |last2=Lewis |first2=Adrian |title=Convex Analysis and Nonlinear Optimization: Theory and Examples| edition=2 |year=2006 |publisher=Springer |isbn=978-0-387-29570-1}}&lt;/ref&gt;&lt;ref name="boyd"&gt;{{cite book |last1=Boyd |first1=Stephen |last2=Vandenberghe |first2=Lieven |title=Convex Optimization |publisher=Cambridge University Press |year=2004 |isbn=978-0-521-83378-3 |url=http://www.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf |format=pdf |accessdate=October 3, 2011}}&lt;/ref&gt;

==== Lagrange duality ====
For a convex minimization problem with inequality constraints,

::: min&lt;sub&gt;''x''&lt;/sub&gt; ''f''(''x'') subject to ''g&lt;sub&gt;i&lt;/sub&gt;''(''x'') ≤ 0 for ''i'' = 1, ..., ''m''.

the Lagrangian dual problem is

::: sup&lt;sub&gt;''u''&lt;/sub&gt; inf&lt;sub&gt;''x''&lt;/sub&gt; ''L''(''x'', ''u'') subject to ''u&lt;sub&gt;i&lt;/sub&gt;''(''x'') ≥ 0 for ''i'' = 1, ..., ''m''.

where the objective function ''L''(''x'', ''u'') is the Lagrange dual function defined as follows:

:&lt;math&gt;L(x,u) = f(x) + \sum_{j=1}^m u_j g_j(x)&lt;/math&gt;

== See also ==
* [[List of convexity topics]]
* [[Werner Fenchel]]

== Notes ==
{{reflist|30em}}

==References==
*{{cite book|author1=J.-B. Hiriart-Urruty|author2=[[C. Lemaréchal]]|title=Fundamentals of convex analysis|publisher=Springer-Verlag |location=Berlin |year=2001 |isbn=978-3-540-42205-1}}
*{{cite book|last=Singer|first=Ivan|title=Abstract convex analysis|series=Canadian Mathematical Society series of monographs and advanced texts|publisher=John Wiley&amp;nbsp;&amp;&amp;nbsp;Sons, Inc.|location=New&amp;nbsp;York|year= 1997|pages=xxii+491|isbn=0-471-16015-6|mr=1461544}}
*{{cite book|first1=J.|last1=Stoer|first2=C.|last2=Witzgall|title=Convexity and optimization in finite dimensions |volume=1 |publisher=Springer |location=Berlin | year=1970 |isbn=978-0-387-04835-2}}
*{{cite book|title=Subdifferentials: Theory and Applications|author1=A.G. Kusraev|first=|author2=[[Semen Samsonovich Kutateladze|S.S. Kutateladze]]|publisher=Kluwer Academic Publishers|year=1995|isbn=978-94-011-0265-0|location=Dordrecht|pages=}}

==External links==
*{{Commonscat-inline}}

[[Category:Convex analysis| ]]
[[Category:Convex geometry|Analysis]]
[[Category:Variational analysis]]</text>
      <sha1>0uzzyjvtt1q3efvf22zgc16jp8fmpmq</sha1>
    </revision>
  </page>
  <page>
    <title>Curvilinear coordinates</title>
    <ns>0</ns>
    <id>755300</id>
    <revision>
      <id>866987206</id>
      <parentid>866984998</parentid>
      <timestamp>2018-11-02T21:52:02Z</timestamp>
      <contributor>
        <ip>131.215.220.163</ip>
      </contributor>
      <comment>Removed nonsensical phrase at beginning of article</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="52735">{{hatnote|See [[orthogonal coordinates]] for the important special case, [[skew coordinates]] for a less common special case, and [[tensors in curvilinear coordinates]] for more generalized mathematical details.}}
{{Multiple issues|
{{confusing|date=January 2013}}
{{expert needed|Mathematics |date=January 2013}}
{{disputed|date=January 2013}}
}}

[[File:Curvilinear.svg|thumb|right|350px|&lt;span style="color:blue"&gt;'''Curvilinear'''&lt;/span&gt; (top), [[Affine coordinate system|&lt;span style="color:red"&gt;'''affine'''&lt;/span&gt;]] (right), and [[Cartesian coordinate system|&lt;span style="color:black"&gt;'''Cartesian'''&lt;/span&gt;]] (left) coordinates in two-dimensional space]]

In [[geometry]], '''curvilinear coordinates''' are a [[coordinate system]] for [[Euclidean space]] in which the [[coordinate line]]s may be curved. Commonly used curvilinear coordinate systems include: rectangular, spherical, and cylindrical coordinate systems.  These coordinates may be derived from a set of [[Cartesian coordinate]]s by using a transformation that is [[invertible|locally invertible]] (a one-to-one map) at each point. This means that one can convert a point given in a Cartesian coordinate system to its curvilinear coordinates and back. The name ''curvilinear coordinates'', coined by the French mathematician [[Gabriel Lamé|Lamé]], derives from the fact that the [[coordinate surfaces]] of the curvilinear systems are curved.

Well-known examples of curvilinear coordinate systems in three-dimensional Euclidean space ('''R'''&lt;sup&gt;3&lt;/sup&gt;) are [[Cartesian coordinate system|Cartesian]], [[Cylindrical coordinate system|cylindrical]] and [[spherical coordinates|spherical polar]] coordinates. A Cartesian coordinate surface in this space is a [[coordinate plane]]; for example ''z'' = 0 defines the ''x''-''y'' plane. In the same space, the coordinate surface ''r'' = 1 in spherical polar coordinates is the surface of a unit [[sphere]], which is curved. The formalism of curvilinear coordinates provides a unified and general description of the standard coordinate systems.

Curvilinear coordinates are often used to define the location or distribution of physical quantities which may be, for example, [[scalar (mathematics)|scalar]]s, [[vector (geometric)|vector]]s, or [[tensor]]s. Mathematical expressions involving these quantities in [[vector calculus]] and [[tensor analysis]] (such as the [[gradient]], [[divergence]], [[curl (mathematics)|curl]], and [[Laplacian]]) can be transformed from one coordinate system to another, according to transformation rules for scalars, vectors, and tensors. Such expressions then become valid for any curvilinear coordinate system.

Depending on the application, a curvilinear coordinate system may be simpler to use than the Cartesian coordinate system. For instance, a physical problem with [[Circular symmetry|spherical symmetry]] defined in '''R'''&lt;sup&gt;3&lt;/sup&gt; (for example, motion of particles under the influence of [[central force]]s) is usually easier to solve in [[spherical coordinates|spherical polar coordinates]] than in Cartesian coordinates. Equations with [[boundary conditions]] that follow coordinate surfaces for a particular curvilinear coordinate system may be easier to solve in that system. One would for instance describe the motion of a particle in a rectangular box in Cartesian coordinates, whereas one would prefer spherical coordinates for a particle in a sphere. Spherical coordinates are one of the most used curvilinear coordinate systems in such fields as [[Earth sciences]], [[cartography]], and [[physics]] (in particular [[quantum mechanics]], [[Theory of relativity|relativity]]), and [[engineering]].

==Orthogonal curvilinear coordinates in 3 Dimensions==

=== Coordinates, basis, and vectors ===
[[File:General curvilinear coordinates 1.svg|thumb|right|350px|Fig. 1 - Coordinate surfaces, coordinate lines, and coordinate axes of general curvilinear coordinates.]]
[[File:Spherical coordinate elements.svg|thumb|right|350px|Fig. 2 - Coordinate surfaces, coordinate lines, and coordinate axes of spherical coordinates. '''Surfaces:''' ''r'' - spheres, θ - cones, φ - half-planes; '''Lines:''' ''r'' - straight beams, θ - vertical semicircles, φ - horizontal circles;

'''Axes:''' ''r'' - straight beams, θ - tangents to vertical semicircles, φ - tangents to horizontal circles]]

For now, consider [[three-dimensional space|3d space]]. A point ''P'' in 3d space (or its [[position vector]] '''r''') can be defined using Cartesian coordinates (''x'', ''y'', ''z'') [equivalently written (''x''&lt;sup&gt;1&lt;/sup&gt;, ''x''&lt;sup&gt;2&lt;/sup&gt;, ''x''&lt;sup&gt;3&lt;/sup&gt;)], by &lt;math&gt;\mathbf{r} = x \mathbf{e}_x + y\mathbf{e}_y + z\mathbf{e}_z&lt;/math&gt;, where '''e'''&lt;sub&gt;''x''&lt;/sub&gt;, '''e'''&lt;sub&gt;''y''&lt;/sub&gt;, '''e'''&lt;sub&gt;''z''&lt;/sub&gt; are the ''[[standard basis|standard]] [[Basis (linear algebra)|basis]] vectors''.

It can also be defined by its '''curvilinear coordinates''' (''q''&lt;sup&gt;1&lt;/sup&gt;, ''q''&lt;sup&gt;2&lt;/sup&gt;, ''q''&lt;sup&gt;3&lt;/sup&gt;) if this triplet of numbers defines a single point in an unambiguous way. The relation between the coordinates is then given by the invertible transformation functions:

:&lt;math&gt; x = f^1(q^1, q^2, q^3),\, y = f^2(q^1, q^2, q^3),\, z = f^3(q^1, q^2, q^3)&lt;/math&gt;
:&lt;math&gt; q^1 = g^1(x,y,z),\, q^2 = g^2(x,y,z),\, q^3 = g^3(x,y,z)&lt;/math&gt;

The surfaces ''q''&lt;sup&gt;1&lt;/sup&gt; = constant, ''q''&lt;sup&gt;2&lt;/sup&gt; = constant, ''q''&lt;sup&gt;3&lt;/sup&gt; = constant are called the '''coordinate surfaces'''; and the space curves formed by their intersection in pairs are called the '''[[coordinate curves]]'''. The '''coordinate axes''' are determined by the [[tangent]]s to the coordinate curves at the intersection of three surfaces. They are not in general fixed directions in space, which happens to be the case for simple Cartesian coordinates, and thus there is generally no natural global basis for curvilinear coordinates.

In the Cartesian system, the standard basis vectors can be derived from the derivative of the location of point ''P'' with respect to the local coordinate

:&lt;math&gt;\mathbf{e}_x = \dfrac{\partial\mathbf{r}}{\partial x}; \;
\mathbf{e}_y = \dfrac{\partial\mathbf{r}}{\partial y}; \;
\mathbf{e}_z = \dfrac{\partial\mathbf{r}}{\partial z}.&lt;/math&gt;

Applying the same derivatives to the curvilinear system locally at point ''P'' defines the natural basis vectors:

:&lt;math&gt;\mathbf{h}_1 = \dfrac{\partial\mathbf{r}}{\partial q_1}; \;
\mathbf{h}_2 = \dfrac{\partial\mathbf{r}}{\partial q_2}; \;
\mathbf{h}_3 = \dfrac{\partial\mathbf{r}}{\partial q_3}.&lt;/math&gt;

Such a basis, whose vectors change their direction and/or magnitude from point to point is called a '''local basis'''. All bases associated with curvilinear coordinates are necessarily local. Basis vectors that are the same at all points are '''global bases''', and can be associated only with linear or [[affine coordinate system]]s.

Note: for this article '''e''' is reserved for the [[standard basis]] (Cartesian) and '''h''' or '''b''' is for the curvilinear basis.

These may not have unit length, and may also not be orthogonal.  In the case that they ''are'' orthogonal at all points where the derivatives are well-defined, we define the [[#Relation to Lamé coefficients|Lamé coefficients]] (after [[Gabriel Lamé]]) by

:&lt;math&gt;h_1 = |\mathbf{h}_1|; \; h_2 = |\mathbf{h}_2|; \; h_3 = |\mathbf{h}_3|&lt;/math&gt;

and the curvilinear orthonormal basis vectors by

:&lt;math&gt;\mathbf{b}_1 = \dfrac{\mathbf{h}_1}{h_1}; \;
\mathbf{b}_2 = \dfrac{\mathbf{h}_2}{h_2}; \;
\mathbf{b}_3 = \dfrac{\mathbf{h}_3}{h_3}.&lt;/math&gt;

It is important to note that these basis vectors may well depend upon the position of ''P''; it is therefore necessary that they are not assumed to be constant over a region.  (They technically form a basis for the [[tangent bundle]] of &lt;math&gt;\mathbb{R}^3&lt;/math&gt; at ''P'', and so are local to ''P''.)

In general, curvilinear coordinates allow the natural basis vectors '''h'''&lt;sub&gt;i&lt;/sub&gt; not all mutually perpendicular to each other, and not required to be of unit length: they can be of arbitrary magnitude and direction. The use of an orthogonal basis makes vector manipulations simpler than for non-orthogonal. However, some areas of [[physics]] and [[engineering]], particularly [[fluid mechanics]] and [[continuum mechanics]], require non-orthogonal bases to describe deformations and fluid transport to account for complicated directional dependences of physical quantities.  A discussion of the general case appears later on this page.

==Vector calculus==
{{see also|Differential geometry}}

===Differential elements===


In orthogonal curvilinear coordinates, since the [[total differential]] change in '''r''' is

:&lt;math&gt;d\mathbf{r}=\dfrac{\partial\mathbf{r}}{\partial q^1}dq^1 + \dfrac{\partial\mathbf{r}}{\partial q^2}dq^2 + \dfrac{\partial\mathbf{r}}{\partial q^3}dq^3 = h_1 dq^1 \mathbf{b}_1 + h_2 dq^2 \mathbf{b}_2 + h_3 dq^3 \mathbf{b}_3 &lt;/math&gt;

so scale factors are &lt;math&gt;h_i = \left|\frac{\partial\mathbf{r}}{\partial q^i}\right|&lt;/math&gt;

In non-orthogonal coordinates the length of  &lt;math&gt;d\mathbf{r}= dq^1 \mathbf{h}_1 + dq^2 \mathbf{h}_2 + dq^3 \mathbf{h}_3 &lt;/math&gt; is the positive square root of &lt;math&gt;d\mathbf{r} \cdot d\mathbf{r} = dq^i dq^j \mathbf{h}_i \cdot \mathbf{h}_j &lt;/math&gt; (with Einstein summation convention). The six independent scalar products ''g&lt;sub&gt;ij&lt;/sub&gt;''='''h'''&lt;sub&gt;i&lt;/sub&gt;.'''h'''&lt;sub&gt;j&lt;/sub&gt; of the natural basis vectors generalize the three scale factors defined above for orthogonal coordinates. The nine ''g&lt;sub&gt;ij&lt;/sub&gt;'' are the components of the [[metric tensor]], which has only three non zero components in orthogonal coordinates: ''g&lt;sub&gt;11&lt;/sub&gt;''=''h&lt;sub&gt;1&lt;/sub&gt;h&lt;sub&gt;1&lt;/sub&gt;'', ''g&lt;sub&gt;22&lt;/sub&gt;''=''h&lt;sub&gt;2&lt;/sub&gt;h&lt;sub&gt;2&lt;/sub&gt;'', ''g&lt;sub&gt;33&lt;/sub&gt;''=''h&lt;sub&gt;3&lt;/sub&gt;h&lt;sub&gt;3&lt;/sub&gt;''.

==Covariant and contravariant bases==

{{Main|Covariance and contravariance of vectors|Raising and lowering indices}}

[[File:Vector 1-form.svg|400px|thumb| A vector '''v''' ('''&lt;span style="color:#CC0000;"&gt;red&lt;/span&gt;''') represented by

• a vector basis ('''&lt;span style="color:orange;"&gt;yellow&lt;/span&gt;, left:''' '''e'''&lt;sub&gt;1&lt;/sub&gt;, '''e'''&lt;sub&gt;2&lt;/sub&gt;, '''e'''&lt;sub&gt;3&lt;/sub&gt;), tangent vectors to coordinate curves ('''black''') and

• a covector basis or cobasis ('''&lt;span style="color:blue;"&gt;blue&lt;/span&gt;, right:''' '''e'''&lt;sup&gt;1&lt;/sup&gt;, '''e'''&lt;sup&gt;2&lt;/sup&gt;, '''e'''&lt;sup&gt;3&lt;/sup&gt;), normal vectors to coordinate surfaces ('''&lt;span style="color:#3B444B;"&gt;grey&lt;/span&gt;''')

 in ''general'' (not necessarily [[orthogonal coordinates|orthogonal]]) curvilinear coordinates (''q''&lt;sup&gt;1&lt;/sup&gt;, ''q''&lt;sup&gt;2&lt;/sup&gt;, ''q''&lt;sup&gt;3&lt;/sup&gt;). Note the basis and cobasis do not coincide unless the coordinate system is orthogonal.&lt;ref&gt;{{cite book|title=Gravitation|author1=J.A. Wheeler |author2=C. Misner |author3=K.S. Thorne |publisher=W.H. Freeman &amp; Co|year=1973|isbn=0-7167-0344-0}}&lt;/ref&gt;]]

Spatial gradients, distances, time derivatives and scale factors are interrelated within a coordinate system by two groups of basis vectors:

{{ordered list
|1 = basis vectors that are locally tangent to their associated coordinate pathline:

:&lt;math&gt;\mathbf{b}_i=\dfrac{\partial\mathbf{r}}{\partial q^i}&lt;/math&gt;

which transforms like [[covariance and contravariance of vectors|covariant vectors]] (denoted by lowered indices), or

|2 = basis vectors that are locally normal to the isosurface created by the other coordinates:

:&lt;math&gt;\mathbf{b}^i=\nabla q^i &lt;/math&gt;

which transforms like [[covariance and contravariance of vectors|contravariant vectors]] (denoted by raised indices), ∇ is the [[del]] [[linear operator|operator]].
}}

Consequently, a general curvilinear coordinate system has two sets of basis vectors for every point: {'''b'''&lt;sub&gt;1&lt;/sub&gt;, '''b'''&lt;sub&gt;2&lt;/sub&gt;, '''b'''&lt;sub&gt;3&lt;/sub&gt;} is the covariant basis, and {'''b'''&lt;sup&gt;1&lt;/sup&gt;, '''b'''&lt;sup&gt;2&lt;/sup&gt;, '''b'''&lt;sup&gt;3&lt;/sup&gt;} is the contravariant (a.k.a. reciprocal) basis. The covariant and contravariant basis vectors types have identical direction for orthogonal curvilinear coordinate systems, but as usual have inverted units with respect to each other.

Note the following important equality:
:&lt;math&gt; \mathbf{b}_i\cdot\mathbf{b}^j = \delta^i_j &lt;/math&gt;
wherein &lt;math&gt; \delta^i_j &lt;/math&gt; denotes the [[Kronecker delta#Generalizations of the Kronecker delta|generalized Kronecker delta]].

:{| class="toccolours collapsible collapsed" width="80%" style="text-align:left"
!Proof
|-
|
In the Cartesian coordinate system &lt;math&gt; ( \mathbf{e}_x ,  \mathbf{e}_y, \mathbf{e}_z ) &lt;/math&gt;, we can write the dot product as:

:&lt;math&gt; \mathbf{b}_i\cdot\mathbf{b}^j = ( \dfrac {\partial x} {\partial q_i} , \dfrac {\partial y} {\partial q_i} , \dfrac {\partial z} {\partial q_i} ) \cdot ( \dfrac {\partial q_j} {\partial x} , \dfrac {\partial q_j} {\partial y} , \dfrac {\partial q_j} {\partial z} ) = \dfrac {\partial x} {\partial q_i} \dfrac {\partial q_j} {\partial x} + \dfrac {\partial y} {\partial q_i} \dfrac {\partial q_j} {\partial y} + \dfrac {\partial z} {\partial q_i} \dfrac {\partial q_j} {\partial z} &lt;/math&gt;

Let us consider an infinitesimal displacement &lt;math&gt; d \mathbf{r} = dx \cdot \mathbf{e}_x + dy \cdot \mathbf{e}_y + dz \cdot \mathbf{e}_z &lt;/math&gt; . Let dq&lt;sub&gt;1&lt;/sub&gt;, dq&lt;sub&gt;2&lt;/sub&gt; and dq&lt;sub&gt;3&lt;/sub&gt; denote the corresponding infinitesimal changes in curvilinear coordinates q&lt;sub&gt;1&lt;/sub&gt;, q&lt;sub&gt;2&lt;/sub&gt; and q&lt;sub&gt;3&lt;/sub&gt; respectively.

By the chain rule, dq&lt;sub&gt;1&lt;/sub&gt; can be expressed as:
:&lt;math&gt; dq_1 = \dfrac {\partial q_1} {\partial x} dx + \dfrac {\partial q_1} {\partial y} dy + \dfrac {\partial q_1} {\partial z} dz
= \dfrac {\partial q_1} {\partial x} dx + \dfrac {\partial q_1} {\partial y} (\dfrac {\partial y} {\partial q_1} dq_1 + \dfrac {\partial y} {\partial q_2} dq_2 + \dfrac {\partial y} {\partial q_3} dq_3) + \dfrac {\partial q_1} {\partial z} (\dfrac {\partial z} {\partial q_1} dq_1 + \dfrac {\partial z} {\partial q_2} dq_2 + \dfrac {\partial z} {\partial q_3} dq_3) &lt;/math&gt;

If the displacement d'''r''' is such that dq&lt;sub&gt;2&lt;/sub&gt; = dq&lt;sub&gt;3&lt;/sub&gt; = 0, i.e. the position vector '''r'''  moves by an infinitesimal amount along the coordinate axis q&lt;sub&gt;2&lt;/sub&gt;=const and q&lt;sub&gt;3&lt;/sub&gt;=const, then:
:&lt;math&gt; dq_1 = \dfrac {\partial q_1} {\partial x} dx + \dfrac {\partial q_1} {\partial y} \dfrac {\partial y} {\partial q_1} dq_1 + \dfrac {\partial q_1} {\partial z} \dfrac {\partial z} {\partial q_1} dq_1 &lt;/math&gt;
Dividing by dq&lt;sub&gt;1&lt;/sub&gt;, and taking the limit dq&lt;sub&gt;1&lt;/sub&gt; → 0:
:&lt;math&gt;  1 =  \dfrac {\partial q_1} {\partial x} \dfrac {\partial x} {\partial q_1}   + \dfrac {\partial q_1} {\partial y} \dfrac {\partial y} {\partial q_1} + \dfrac {\partial q_1} {\partial z} \dfrac {\partial z} {\partial q_1} = \dfrac {\partial x} {\partial q_1} \dfrac {\partial q_1} {\partial x} + \dfrac {\partial y} {\partial q_1} \dfrac {\partial q_1} {\partial y} + \dfrac {\partial z} {\partial q_1} \dfrac {\partial q_1} {\partial z} &lt;/math&gt;
or equivalently:
:&lt;math&gt; \mathbf{b}_1\cdot\mathbf{b}^1 = 1 &lt;/math&gt;

Now if the displacement d'''r''' is such that dq&lt;sub&gt;1&lt;/sub&gt;=dq&lt;sub&gt;3&lt;/sub&gt;=0, i.e. the position vector '''r'''  moves by an infinitesimal amount along the coordinate axis q&lt;sub&gt;1&lt;/sub&gt;=const and q&lt;sub&gt;3&lt;/sub&gt;=const, then:
:&lt;math&gt; 0 = \dfrac {\partial q_1} {\partial x} dx + \dfrac {\partial q_1} {\partial y} \dfrac {\partial y} {\partial q_2} dq_2 + \dfrac {\partial q_1} {\partial z} \dfrac {\partial z} {\partial q_2} dq_2 &lt;/math&gt;
Dividing by dq&lt;sub&gt;2&lt;/sub&gt;, and taking the limit dq&lt;sub&gt;2&lt;/sub&gt; → 0:
:&lt;math&gt; 0 = \dfrac {\partial q_1} {\partial x} \dfrac {\partial x} {\partial q_2}  + \dfrac {\partial q_1} {\partial y} \dfrac {\partial y} {\partial q_2} + \dfrac {\partial q_1} {\partial z} \dfrac {\partial z} {\partial q_2}  = \dfrac {\partial x} {\partial q_2} \dfrac {\partial q_1} {\partial x} + \dfrac {\partial y} {\partial q_2} \dfrac {\partial q_1} {\partial y}  + \dfrac {\partial z} {\partial q_2} \dfrac {\partial q_1} {\partial z}  &lt;/math&gt; 
or equivalently:
:&lt;math&gt; \mathbf{b}_2\cdot\mathbf{b}^1 = 0 &lt;/math&gt;

And so forth for the other dot products.

|}

A vector '''v''' can be specified in terms either basis, i.e.,

:&lt;math&gt; \mathbf{v} = v^1\mathbf{b}_1 + v^2\mathbf{b}_2 + v^3\mathbf{b}_3 = v_1\mathbf{b}^1 + v_2\mathbf{b}^2 + v_3\mathbf{b}^3 &lt;/math&gt;

Using the Einstein summation convention, the basis vectors relate to the components by&lt;ref name=Simmonds/&gt;{{rp|pages=30–32}}
:&lt;math&gt; \mathbf{v}\cdot\mathbf{b}^i = v^k\mathbf{b}_k\cdot\mathbf{b}^i = v^k\delta^i_k = v^i &lt;/math&gt;
:&lt;math&gt; \mathbf{v}\cdot\mathbf{b}_i = v_k\mathbf{b}^k\cdot\mathbf{b}_i = v_k\delta_i^k = v_i &lt;/math&gt;
and
:&lt;math&gt; \mathbf{v}\cdot\mathbf{b}_i = v^k\mathbf{b}_k\cdot\mathbf{b}_i = g_{ki}v^k &lt;/math&gt;
:&lt;math&gt; \mathbf{v}\cdot\mathbf{b}^i = v_k\mathbf{b}^k\cdot\mathbf{b}^i = g^{ki}v_k &lt;/math&gt;
where ''g'' is the metric tensor (see below).

A vector can be specified with covariant coordinates (lowered indices, written ''v&lt;sub&gt;k&lt;/sub&gt;'') or contravariant coordinates (raised indices, written ''v&lt;sup&gt;k&lt;/sup&gt;''). From the above vector sums, it can be seen that contravariant coordinates are associated with covariant basis vectors, and covariant coordinates are associated with contravariant basis vectors.

A key feature of the representation of vectors and tensors in terms of indexed components and basis vectors is ''invariance'' in the sense that vector components which transform in a covariant manner (or contravariant manner) are paired with basis vectors that transform in a contravariant manner (or covariant manner).

==Covariant basis==
{{Main|Covariant transformation}}

===Constructing a covariant basis in one dimension===

[[File:Local basis transformation.svg|thumb|right|350px|Fig. 3 – Transformation of local covariant basis in the case of general curvilinear coordinates]]

Consider the one-dimensional curve shown in Fig. 3.  At point ''P'', taken as an [[Origin (mathematics)|origin]], ''x'' is one of the Cartesian coordinates, and ''q''&lt;sup&gt;1&lt;/sup&gt; is one of the curvilinear coordinates. The local (non-unit) basis vector is '''b'''&lt;sub&gt;1&lt;/sub&gt; (notated '''h'''&lt;sub&gt;1&lt;/sub&gt; above, with '''b''' reserved for unit vectors) and it is built on the ''q''&lt;sup&gt;1&lt;/sup&gt; axis which is a tangent to that coordinate line at the point ''P''. The axis ''q''&lt;sup&gt;1&lt;/sup&gt; and thus the vector '''b'''&lt;sub&gt;1&lt;/sub&gt; form an angle &lt;math&gt;\alpha&lt;/math&gt; with the Cartesian ''x'' axis and the Cartesian basis vector '''e'''&lt;sub&gt;1&lt;/sub&gt;.

It can be seen from triangle ''PAB'' that
:&lt;math&gt; \cos \alpha = \cfrac{|\mathbf{e}_1|}{|\mathbf{b}_1|} \quad \Rightarrow \quad |\mathbf{e}_1| = |\mathbf{b}_1|\cos \alpha&lt;/math&gt;
where |'''e'''&lt;sub&gt;1&lt;/sub&gt;|, |'''b'''&lt;sub&gt;1&lt;/sub&gt;| are the magnitudes of the two basis vectors, i.e., the scalar intercepts ''PB'' and ''PA''.  Note that ''PA'' is also the projection of '''b'''&lt;sub&gt;1&lt;/sub&gt; on the ''x'' axis.

However, this method for basis vector transformations using ''directional cosines'' is inapplicable to curvilinear coordinates for the following reasons:
#By increasing the distance from ''P'', the angle between the curved line ''q''&lt;sup&gt;1&lt;/sup&gt; and Cartesian axis ''x'' increasingly deviates from &lt;math&gt;\alpha&lt;/math&gt;.
#At the distance ''PB'' the true angle is that which the tangent '''at point C''' forms with the ''x'' axis and the latter angle is clearly different from &lt;math&gt;\alpha&lt;/math&gt;.

The angles that the ''q''&lt;sup&gt;1&lt;/sup&gt; line and that axis form with the ''x'' axis become closer in value the closer one moves towards point ''P'' and become exactly equal at ''P''.

Let point ''E'' be located very close to ''P'', so close that the distance ''PE'' is infinitesimally small. Then ''PE'' measured on the ''q''&lt;sup&gt;1&lt;/sup&gt; axis almost coincides with ''PE'' measured on the ''q''&lt;sup&gt;1&lt;/sup&gt; line. At the same time, the ratio ''PD/PE'' (''PD'' being the projection of ''PE'' on the ''x'' axis) becomes almost exactly equal to &lt;math&gt;\cos\alpha&lt;/math&gt;.

Let the infinitesimally small intercepts ''PD'' and ''PE'' be labelled, respectively, as ''dx'' and d''q''&lt;sup&gt;1&lt;/sup&gt;. Then
:&lt;math&gt;\cos \alpha = \cfrac{dx}{dq^1} = \frac{|\mathbf{e}_1|}{|\mathbf{b}_1|}&lt;/math&gt;.

Thus, the directional cosines can be substituted in transformations with the more exact ratios between infinitesimally small coordinate intercepts.  It follows that the component (projection) of '''b'''&lt;sub&gt;1&lt;/sub&gt; on the ''x'' axis is

:&lt;math&gt;p^1 = \mathbf{b}_1\cdot\cfrac{\mathbf{e}_1}{|\mathbf{e}_1|} = |\mathbf{b}_1|\cfrac{|\mathbf{e}_1|}{|\mathbf{e}_1|}\cos\alpha = |\mathbf{b}_1|\cfrac{dx}{dq^1} \quad \Rightarrow \quad \cfrac{p^1}{|\mathbf{b}_1|} = \cfrac{dx}{dq^1}&lt;/math&gt;.

If ''q&lt;sup&gt;i&lt;/sup&gt;'' = ''q&lt;sup&gt;i&lt;/sup&gt;''(''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ''x''&lt;sub&gt;3&lt;/sub&gt;) and ''x&lt;sub&gt;i&lt;/sub&gt;'' = ''x&lt;sub&gt;i&lt;/sub&gt;''(''q''&lt;sup&gt;1&lt;/sup&gt;, ''q''&lt;sup&gt;2&lt;/sup&gt;, ''q''&lt;sup&gt;3&lt;/sup&gt;) are [[Smooth function|smooth]] (continuously differentiable) functions the transformation ratios can be written as &lt;math&gt;\cfrac{\partial q^i}{\partial x_j}&lt;/math&gt; and &lt;math&gt;\cfrac{\partial x_i}{\partial q^j}&lt;/math&gt;. That is, those ratios are [[partial derivative]]s of coordinates belonging to one system with respect to coordinates belonging to the other system.

===Constructing a covariant basis in three dimensions===


Doing the same for the coordinates in the other 2 dimensions, '''b'''&lt;sub&gt;1&lt;/sub&gt; can be expressed as:
:&lt;math&gt;
\mathbf{b}_1 = p^1\mathbf{e}_1 + p^2\mathbf{e}_2 + p^3\mathbf{e}_3 = \cfrac{\partial x_1}{\partial q^1} \mathbf{e}_1 + \cfrac{\partial x_2}{\partial q^1} \mathbf{e}_2 + \cfrac{\partial x_3}{\partial q^1} \mathbf{e}_3
&lt;/math&gt;
Similar equations hold for '''b'''&lt;sub&gt;2&lt;/sub&gt; and '''b'''&lt;sub&gt;3&lt;/sub&gt; so that the standard basis {'''e'''&lt;sub&gt;1&lt;/sub&gt;, '''e'''&lt;sub&gt;2&lt;/sub&gt;, '''e'''&lt;sub&gt;3&lt;/sub&gt;} is transformed to a local (ordered and '''''normalised''''') basis {'''b'''&lt;sub&gt;1&lt;/sub&gt;, '''b'''&lt;sub&gt;2&lt;/sub&gt;, '''b'''&lt;sub&gt;3&lt;/sub&gt;} by the following system of equations:

:&lt;math&gt;\begin{align}
   \mathbf{b}_1 &amp; = \cfrac{\partial x_1}{\partial q^1} \mathbf{e}_1 + \cfrac{\partial x_2}{\partial q^1} \mathbf{e}_2 + \cfrac{\partial x_3}{\partial q^1} \mathbf{e}_3 \\
   \mathbf{b}_2 &amp; = \cfrac{\partial x_1}{\partial q^2} \mathbf{e}_1 + \cfrac{\partial x_2}{\partial q^2} \mathbf{e}_2 + \cfrac{\partial x_3}{\partial q^2} \mathbf{e}_3 \\
   \mathbf{b}_3 &amp; = \cfrac{\partial x_1}{\partial q^3} \mathbf{e}_1 + \cfrac{\partial x_2}{\partial q^3} \mathbf{e}_2 + \cfrac{\partial x_3}{\partial q^3} \mathbf{e}_3
\end{align}&lt;/math&gt;

By analogous reasoning, one can obtain the inverse transformation from local basis to standard basis:
:&lt;math&gt;\begin{align}
   \mathbf{e}_1 &amp; = \cfrac{\partial q^1}{\partial x_1} \mathbf{b}_1 + \cfrac{\partial q^2}{\partial x_1} \mathbf{b}_2 + \cfrac{\partial q^3}{\partial x_1} \mathbf{b}_3 \\
   \mathbf{e}_2 &amp; = \cfrac{\partial q^1}{\partial x_2} \mathbf{b}_1 + \cfrac{\partial q^2}{\partial x_2} \mathbf{b}_2 + \cfrac{\partial q^3}{\partial x_2} \mathbf{b}_3 \\
   \mathbf{e}_3 &amp; = \cfrac{\partial q^1}{\partial x_3} \mathbf{b}_1 + \cfrac{\partial q^2}{\partial x_3} \mathbf{b}_2 + \cfrac{\partial q^3}{\partial x_3} \mathbf{b}_3
\end{align}&lt;/math&gt;

===Jacobian of the transformation===


The above [[systems of linear equations]] can be written in matrix form using the Einstein summation convention as
:&lt;math&gt;\cfrac{\partial x_i}{\partial q^k} \mathbf{e}_i = \mathbf{b}_k, \quad \cfrac{\partial q^i}{\partial x_k} \mathbf{b}_i = \mathbf{e}_k&lt;/math&gt;.

This [[coefficient matrix]] of the linear system is the [[Jacobian matrix]] (and its inverse) of the transformation. These are the equations that can be used to transform a Cartesian basis into a curvilinear basis, and vice versa.

In three dimensions, the expanded forms of these matrices are
:&lt;math&gt;
\mathbf{J} = \begin{bmatrix}
       \cfrac{\partial x_1}{\partial q^1} &amp; \cfrac{\partial x_1}{\partial q^2} &amp; \cfrac{\partial x_1}{\partial q^3} \\
       \cfrac{\partial x_2}{\partial q^1} &amp; \cfrac{\partial x_2}{\partial q^2} &amp; \cfrac{\partial x_2}{\partial q^3} \\
       \cfrac{\partial x_3}{\partial q^1} &amp; \cfrac{\partial x_3}{\partial q^2} &amp; \cfrac{\partial x_3}{\partial q^3} \\
     \end{bmatrix},\quad
\mathbf{J}^{-1} = \begin{bmatrix}
       \cfrac{\partial q^1}{\partial x_1} &amp; \cfrac{\partial q^1}{\partial x_2} &amp; \cfrac{\partial q^1}{\partial x_3} \\
       \cfrac{\partial q^2}{\partial x_1} &amp; \cfrac{\partial q^2}{\partial x_2} &amp; \cfrac{\partial q^2}{\partial x_3} \\
       \cfrac{\partial q^3}{\partial x_1} &amp; \cfrac{\partial q^3}{\partial x_2} &amp; \cfrac{\partial q^3}{\partial x_3} \\
     \end{bmatrix}
 &lt;/math&gt;

In the inverse transformation (second equation system), the unknowns are the curvilinear basis vectors. For any specific location there can only exist one and only one set of basis vectors (else the basis is not well defined at that point). This condition is satisfied if and only if the equation system has a single solution, from [[linear algebra]], a linear equation system has a single solution (non-trivial) only if the determinant of its system matrix is non-zero:
:&lt;math&gt; \det(\mathbf{J}^{-1})  \neq 0&lt;/math&gt;
which shows the rationale behind the above requirement concerning the inverse Jacobian determinant.

==Generalization to ''n'' dimensions==

The formalism extends to any finite dimension as follows.

Consider the [[real number|real]] [[Euclidean space|Euclidean]] ''n''-dimensional space, that is '''R'''&lt;sup&gt;''n''&lt;/sup&gt; = '''R''' × '''R''' × ... × '''R''' (''n'' times) where '''R''' is the [[set (mathematics)|set]] of [[real numbers]] and × denotes the [[Cartesian product]], which is a [[vector space]].

The [[coordinates]] of this space can be denoted by: '''x''' = (''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;,...,''x&lt;sub&gt;n&lt;/sub&gt;''). Since this is a vector (an element of the vector space), it can be written as:

:&lt;math&gt; \mathbf{x} = \sum_{i=1}^n x_i\mathbf{e}^i &lt;/math&gt;

where '''e'''&lt;sup&gt;1&lt;/sup&gt; = (1,0,0...,0), '''e'''&lt;sup&gt;2&lt;/sup&gt; = (0,1,0...,0), '''e'''&lt;sup&gt;3&lt;/sup&gt; = (0,0,1...,0),...,'''e'''&lt;sup&gt;''n''&lt;/sup&gt; = (0,0,0...,1) is the ''[[standard basis|standard]] [[Basis (linear algebra)|basis]] set of vectors'' for the space '''R'''&lt;sup&gt;''n''&lt;/sup&gt;, and ''i'' = 1, 2,...''n'' is an index labelling components. Each vector has exactly one component in each dimension (or "axis") and they are mutually [[orthogonal vector|orthogonal]] ([[perpendicular]]) and normalized (has [[unit vector|unit magnitude]]).

More generally, we can define basis vectors '''b'''&lt;sub&gt;''i''&lt;/sub&gt; so that they depend on '''q''' = (''q''&lt;sub&gt;1&lt;/sub&gt;, ''q''&lt;sub&gt;2&lt;/sub&gt;,...,''q&lt;sub&gt;n&lt;/sub&gt;''), i.e. they change from point to point: '''b'''&lt;sub&gt;''i''&lt;/sub&gt; = '''b'''&lt;sub&gt;''i''&lt;/sub&gt;('''q'''). In which case to define the same point '''x''' in terms of this alternative basis: the ''[[coordinate vector|coordinates]]'' with respect to this basis ''v&lt;sub&gt;i&lt;/sub&gt;'' also necessarily depend on '''x''' also, that is ''v&lt;sub&gt;i&lt;/sub&gt;'' = ''v&lt;sub&gt;i&lt;/sub&gt;''('''x'''). Then a vector '''v''' in this space, with respect to these alternative coordinates and basis vectors, can be expanded as a [[linear combination]] in this basis (which simply means to multiply each basis [[Coordinate vector|vector]] '''e'''&lt;sub&gt;''i''&lt;/sub&gt; by a number ''v''&lt;sub&gt;''i''&lt;/sub&gt; – [[scalar multiplication]]):

:&lt;math&gt; \mathbf{v} = \sum_{j=1}^n \bar{v}^j\mathbf{b}_j = \sum_{j=1}^n \bar{v}^j(\mathbf{q})\mathbf{b}_j(\mathbf{q}) &lt;/math&gt;

The vector sum that describes '''v''' in the new basis is composed of different vectors, although the sum itself remains the same.

==Transformation of coordinates==

From a more general and abstract perspective, a curvilinear coordinate system is simply a [[Atlas (topology)|coordinate patch]] on the [[differentiable manifold]] '''E'''&lt;sup&gt;n&lt;/sup&gt; (n-dimensional [[Euclidean space]]) that is [[Diffeomorphism|diffeomorphic]] to the [[Cartesian coordinate system|Cartesian]] coordinate patch on the manifold.&lt;ref&gt;{{cite book | last=Boothby | first=W. M. | year=2002 | title=An Introduction to Differential Manifolds and Riemannian Geometry | edition=revised | publisher=Academic Press | location=New York, NY }}&lt;/ref&gt; Note that two diffeomorphic coordinate patches on a differential manifold need not overlap differentiably. With this simple definition of a curvilinear coordinate system, all the results that follow below are simply applications of standard theorems in [[differential topology]].

The transformation functions are such that there's a one-to-one relationship between points in the "old" and "new" coordinates, that is, those functions are [[bijection]]s, and fulfil the following requirements within their [[domain of a function|domain]]s:
{{ordered list
|1= They are [[smooth function]]s: q&lt;sup&gt;''i''&lt;/sup&gt; = q&lt;sup&gt;''i''&lt;/sup&gt;('''x''')

|2= The inverse [[Jacobian matrix and determinant|Jacobian]] determinant
:&lt;math&gt; J^{-1}=\begin{vmatrix}
\dfrac{\partial q^1}{\partial x_1} &amp; \dfrac{\partial q^1}{\partial x_2} &amp; \cdots &amp; \dfrac{\partial q^1}{\partial x_n} \\
\dfrac{\partial q^2}{\partial x_1} &amp; \dfrac{\partial q^2}{\partial x_2} &amp; \cdots &amp; \dfrac{\partial q^2}{\partial x_n} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
\dfrac{\partial q^n}{\partial x_1} &amp; \dfrac{\partial q^n}{\partial x_2} &amp; \cdots &amp; \dfrac{\partial q^n}{\partial x_n}
\end{vmatrix} \neq 0 &lt;/math&gt;

is not zero; meaning the transformation is [[invertible]]: ''x&lt;sub&gt;i''&lt;/sub&gt;('''q''').

according to the [[inverse function theorem]]. The condition that the Jacobian determinant is not zero reflects the fact that three surfaces from different families intersect in one and only one point and thus determine the position of this point in a unique way.&lt;ref&gt;{{cite book | last=McConnell | first=A. J. | year=1957 | publisher=Dover Publications, Inc. | location=New York, NY | at=Ch. 9, sec. 1 | title=Application of Tensor Analysis | isbn=0-486-60373-3 }}&lt;/ref&gt;
}}

==Vector and tensor algebra in three-dimensional curvilinear coordinates==
{{Einstein_summation_convention}}
Elementary vector and tensor algebra in curvilinear coordinates is used in some of the older scientific literature in [[mechanics]] and [[physics]] and can be indispensable to understanding work from the early and mid-1900s, for example the text by Green and Zerna.&lt;ref name=Green&gt;{{cite book | last1=Green | first1=A. E. | last2=Zerna | first2=W. | year=1968 | title=Theoretical Elasticity | publisher=Oxford University Press | isbn=0-19-853486-8 }}&lt;/ref&gt;  Some useful relations in the algebra of vectors and second-order tensors in curvilinear coordinates are given in this section.  The notation and contents are primarily from Ogden,&lt;ref name=Ogden00&gt;{{cite book | last=Ogden | first=R. W. | year=2000 | title=Nonlinear elastic deformations | publisher=Dover}}&lt;/ref&gt; Naghdi,&lt;ref name=Naghdi&gt;{{cite book | first1=P. M. | last1=Naghdi | year=1972 | contribution=Theory of shells and plates | editor=S. Flügge | title=Handbook of Physics | volume=VIa/2 | pages=425–640}}&lt;/ref&gt; Simmonds,&lt;ref name=Simmonds&gt;{{cite book | last=Simmonds | first=J. G. | year=1994 | title=A brief on tensor analysis | publisher=Springer | isbn=0-387-90639-8}}&lt;/ref&gt; Green and Zerna,&lt;ref name=Green/&gt;  Basar and Weichert,&lt;ref name=Basar&gt;{{cite book | last1=Basar | first1=Y. | last2=Weichert | first2=D. | year=2000 | title=Numerical continuum mechanics of solids: fundamental concepts and perspectives | publisher=Springer}}&lt;/ref&gt; and Ciarlet.&lt;ref name=Ciarlet&gt;{{cite book | last=Ciarlet | first=P. G. | year=2000 | title=Theory of Shells | volume=1 | publisher=Elsevier Science }}&lt;/ref&gt;

==Tensors in curvilinear coordinates==
{{main|Tensors in curvilinear coordinates}}
A second-order tensor can be expressed as
:&lt;math&gt;
   \boldsymbol{S} = S^{ij}\mathbf{b}_i\otimes\mathbf{b}_j = S^i{}_j\mathbf{b}_i\otimes\mathbf{b}^j = S_i{}^j\mathbf{b}^i\otimes\mathbf{b}_j = S_{ij}\mathbf{b}^i\otimes\mathbf{b}^j
 &lt;/math&gt;
where &lt;math&gt;\scriptstyle\otimes&lt;/math&gt; denotes the [[tensor product]]. The components ''S&lt;sup&gt;ij&lt;/sup&gt;'' are called the '''contravariant''' components, ''S&lt;sup&gt;i&lt;/sup&gt; &lt;sub&gt;j&lt;/sub&gt;'' the '''mixed right-covariant''' components, ''S&lt;sub&gt;i&lt;/sub&gt; &lt;sup&gt;j&lt;/sup&gt;'' the '''mixed left-covariant''' components, and ''S&lt;sub&gt;ij&lt;/sub&gt;'' the '''covariant''' components of the second-order tensor. The components of the second-order tensor are related by

:&lt;math&gt; S^{ij} = g^{ik}S_k{}^j = g^{jk}S^i{}_k = g^{ik}g^{j\ell}S_{k\ell} &lt;/math&gt;

===The metric tensor in orthogonal curvilinear coordinates===
{{Main|Metric tensor}}

At each point, one can construct a small line element {{math|d'''x'''}}, so the square of the length of the line element is the scalar product d'''x''' • d'''x''' and is called the [[Metric (mathematics)|metric]] of the [[space]], given by:

:&lt;math&gt;d\mathbf{x}\cdot d\mathbf{x} = \cfrac{\partial x_i}{\partial q^j}\cfrac{\partial x_i}{\partial q^k}dq^jdq^k
 &lt;/math&gt;.

The following portion of the above equation
:&lt;math&gt;  \cfrac{\partial x_k}{\partial q^i}\cfrac{\partial x_k}{\partial q^j} = g_{ij}(q^i,q^j) = \mathbf{b}_i\cdot\mathbf{b}_j &lt;/math&gt;
is a ''symmetric'' tensor called the '''[[metric tensor|fundamental (or metric) tensor]]''' of the [[Euclidean space]] in curvilinear coordinates.

Indices can be [[raising and lowering indices|raised and lowered]] by the metric:
:&lt;math&gt; v^i = g^{ik}v_k &lt;/math&gt;

====Relation to Lamé coefficients====

Defining the scale factors ''h&lt;sub&gt;i&lt;/sub&gt;'' by

:&lt;math&gt; h_ih_j = g_{ij} = \mathbf{b}_i\cdot\mathbf{b}_j \quad \Rightarrow \quad h_i =\sqrt{g_{ii}}= \left|\mathbf{b}_i\right|=\left|\cfrac{\partial\mathbf{x}}{\partial q^i}\right| &lt;/math&gt;

gives a relation between the metric tensor and the Lamé coefficients. Note also that

:&lt;math&gt; g_{ij} = \cfrac{\partial\mathbf{x}}{\partial q^i}\cdot\cfrac{\partial\mathbf{x}}{\partial q^j}
= \left( h_{ki}\mathbf{e}_k\right)\cdot\left( h_{mj}\mathbf{e}_m\right)
= h_{ki}h_{kj} &lt;/math&gt;

where ''h&lt;sub&gt;ij&lt;/sub&gt;'' are the Lamé coefficients. For an orthogonal basis we also have:

:&lt;math&gt; g = g_{11}g_{22}g_{33} = h_1^2h_2^2h_3^2 \quad \Rightarrow \quad \sqrt{g} = h_1h_2h_3 = J &lt;/math&gt;

====Example: Polar coordinates====

If we consider polar coordinates for '''R'''&lt;sup&gt;2&lt;/sup&gt;, note that
:&lt;math&gt; (x, y)=(r \cos \theta, r \sin \theta) &lt;/math&gt;
(r, θ) are the curvilinear coordinates, and the Jacobian determinant of the transformation (''r'',θ) → (''r'' cos θ, ''r'' sin θ) is ''r''.

The [[orthogonal]] basis vectors are '''b'''&lt;sub&gt;''r''&lt;/sub&gt; = (cos θ, sin θ), '''b'''&lt;sub&gt;θ&lt;/sub&gt; = (−sin θ, cos θ).  The scale factors are ''h''&lt;sub&gt;''r''&lt;/sub&gt; = 1 and ''h''&lt;sub&gt;θ&lt;/sub&gt;= ''r''. The fundamental tensor is ''g''&lt;sub&gt;11&lt;/sub&gt; =1, ''g''&lt;sub&gt;22&lt;/sub&gt; =''r''&lt;sup&gt;2&lt;/sup&gt;, ''g''&lt;sub&gt;12&lt;/sub&gt; = ''g''&lt;sub&gt;21&lt;/sub&gt; =0.

===The alternating tensor===

In an orthonormal right-handed basis, the third-order [[Levi-Civita symbol|alternating tensor]] is defined as

:&lt;math&gt; \boldsymbol{\mathcal{E}} = \varepsilon_{ijk}\mathbf{e}^i\otimes\mathbf{e}^j\otimes\mathbf{e}^k &lt;/math&gt;

In a general curvilinear basis the same tensor may be expressed as

:&lt;math&gt;
  \boldsymbol{\mathcal{E}} = \mathcal{E}_{ijk}\mathbf{b}^i\otimes\mathbf{b}^j\otimes\mathbf{b}^k
   = \mathcal{E}^{ijk}\mathbf{b}_i\otimes\mathbf{b}_j\otimes\mathbf{b}_k
&lt;/math&gt;

It can also be shown that
:&lt;math&gt;
  \mathcal{E}^{ijk} = \cfrac{1}{J}\varepsilon_{ijk} = \cfrac{1}{+\sqrt{g}}\varepsilon_{ijk}
&lt;/math&gt;

===Christoffel symbols===

;[[Christoffel symbols]] of the first kind:

:&lt;math&gt;
\mathbf{b}_{i,j} = \frac{\partial \mathbf{b}_i}{\partial q^j} = \Gamma_{ijk}\mathbf{b}^k \quad \Rightarrow \quad
\mathbf{b}_{i,j} \cdot \mathbf{b}_k = \Gamma_{ijk}
&lt;/math&gt;

where the comma denotes a [[partial derivative]] (see [[Ricci calculus]]). To express Γ&lt;sub&gt;''ijk''&lt;/sub&gt; in terms of ''g&lt;sub&gt;ij&lt;/sub&gt;'' we note that

:&lt;math&gt;
\begin{align}
g_{ij,k} &amp; = (\mathbf{b}_i\cdot\mathbf{b}_j)_{,k} = \mathbf{b}_{i,k}\cdot\mathbf{b}_j + \mathbf{b}_i\cdot\mathbf{b}_{j,k}
= \Gamma_{ikj} + \Gamma_{jki}\\
g_{ik,j} &amp; = (\mathbf{b}_i\cdot\mathbf{b}_k)_{,j} = \mathbf{b}_{i,j}\cdot\mathbf{b}_k + \mathbf{b}_i\cdot\mathbf{b}_{k,j}
= \Gamma_{ijk} + \Gamma_{kji}\\
g_{jk,i} &amp; = (\mathbf{b}_j\cdot\mathbf{b}_k)_{,i} = \mathbf{b}_{j,i}\cdot\mathbf{b}_k + \mathbf{b}_j\cdot\mathbf{b}_{k,i}
= \Gamma_{jik} + \Gamma_{kij}
\end{align}
&lt;/math&gt;

Since
:&lt;math&gt;\mathbf{b}_{i,j} = \mathbf{b}_{j,i}\quad\Rightarrow\quad\Gamma_{ijk} = \Gamma_{jik}&lt;/math&gt;
using these to rearrange the above relations gives

:&lt;math&gt;\Gamma_{ijk} = \frac{1}{2}(g_{ik,j} + g_{jk,i} - g_{ij,k}) = \frac{1}{2}[(\mathbf{b}_i\cdot\mathbf{b}_k)_{,j} + (\mathbf{b}_j\cdot\mathbf{b}_k)_{,i} - (\mathbf{b}_i\cdot\mathbf{b}_j)_{,k}]
&lt;/math&gt;

;[[Christoffel symbols]] of the second kind:

:&lt;math&gt;\Gamma_{ij}{}^k = \Gamma_{ji}{}^k,\quad \cfrac{\partial \mathbf{b}_i}{\partial q^j} = \Gamma_{ij}{}^k\mathbf{b}_k &lt;/math&gt;

This implies that
:&lt;math&gt; \Gamma_{ij}{}^k = \cfrac{\partial \mathbf{b}_i}{\partial q^j}\cdot\mathbf{b}^k = -\mathbf{b}_i\cdot\cfrac{\partial \mathbf{b}^k}{\partial q^j} &lt;/math&gt;

Other relations that follow are
:&lt;math&gt;
\cfrac{\partial \mathbf{b}^i}{\partial q^j} = -\Gamma^i{}_{jk}\mathbf{b}^k,\quad
\boldsymbol{\nabla}\mathbf{b}_i = \Gamma_{ij}{}^k\mathbf{b}_k\otimes\mathbf{b}^j,\quad
\boldsymbol{\nabla}\mathbf{b}^i = -\Gamma_{jk}{}^i\mathbf{b}^k\otimes\mathbf{b}^j
&lt;/math&gt;

===Vector operations===
{{ordered list
|1= '''[[Dot product]]:'''
The scalar product of two vectors in curvilinear coordinates is&lt;ref name=Simmonds/&gt;{{rp|page=32}}
:&lt;math&gt;
   \mathbf{u}\cdot\mathbf{v} = u^iv_i = u_iv^i = g_{ij}u^iv^j = g^{ij}u_iv_j
 &lt;/math&gt;

|2= '''[[Cross product]]:'''
The [[cross product]] of two vectors is given by&lt;ref name=Simmonds/&gt;{{rp|pages=32–34}}
:&lt;math&gt;
  \mathbf{u}\times\mathbf{v} = \epsilon_{ijk}{u}_j{v}_k\mathbf{e}_i
&lt;/math&gt;
where &lt;math&gt;\epsilon_{ijk}&lt;/math&gt; is the [[permutation symbol]] and &lt;math&gt;\mathbf{e}_i&lt;/math&gt; is a Cartesian basis vector.  In curvilinear coordinates, the equivalent expression is
:&lt;math&gt;
  \mathbf{u}\times\mathbf{v} = [(\mathbf{b}_m\times\mathbf{b}_n)\cdot\mathbf{b}_s]u^mv^n\mathbf{b}^s
    = \mathcal{E}_{smn}u^mv^n\mathbf{b}^s
 &lt;/math&gt;
where &lt;math&gt;\mathcal{E}_{ijk}&lt;/math&gt; is the [[Curvilinear coordinates#The alternating tensor|third-order alternating tensor]].
}}

==Vector and tensor calculus in three-dimensional curvilinear coordinates==
{{Einstein_summation_convention}}

Adjustments need to be made in the calculation of [[line integral|line]], [[surface integral|surface]] and [[volume integral|volume]] [[integration (mathematics)|integrals]].  For simplicity, the following restricts to three dimensions and orthogonal curvilinear coordinates.  However, the same arguments apply for ''n''-dimensional spaces. When the coordinate system is not orthogonal, there are some additional terms in the expressions.

Simmonds,&lt;ref name=Simmonds/&gt; in his book on [[tensor analysis]], quotes [[Albert Einstein]] saying&lt;ref name=Lanczos&gt;{{cite book | last=Einstein | first=A. | year=1915 | contribution=Contribution to the Theory of General Relativity | editor=Laczos, C. | title=The Einstein Decade | page=213 | isbn=0-521-38105-3 }}&lt;/ref&gt;
&lt;blockquote&gt;
The magic of this theory will hardly fail to impose itself on anybody who has truly understood it; it represents a genuine triumph of the method of absolute differential calculus, founded by Gauss, Riemann, Ricci, and Levi-Civita.
&lt;/blockquote&gt;
Vector and tensor calculus in general curvilinear coordinates is used in tensor analysis on four-dimensional curvilinear [[manifold]]s in [[general relativity]],&lt;ref name=Misner&gt;{{cite book | last1=Misner | first1=C. W. | last2=Thorne | first2=K. S. | last3=Wheeler | first3=J. A. | year=1973 | title=Gravitation | publisher=W. H. Freeman and Co. | isbn=0-7167-0344-0}}&lt;/ref&gt; in the [[solid mechanics|mechanics]] of curved [[Plate theory|shells]],&lt;ref name=Ciarlet/&gt; in examining the [[invariant (mathematics)|invariance]] properties of [[Maxwell's equations]] which has been of interest in [[metamaterials]]&lt;ref name=Greenleaf&gt;{{cite journal | doi=10.1088/0967-3334/24/2/353 | last1=Greenleaf | first1=A. | last2=Lassas | first2=M. | last3=Uhlmann | first3=G. | year=2003 | title=Anisotropic conductivities that cannot be detected by EIT | journal=Physiological measurement | volume=24 | issue=2 | pages=413–419 | pmid=12812426}}&lt;/ref&gt;&lt;ref name=Leonhardt&gt;{{cite journal | last1=Leonhardt | first1=U. | last2=Philbin | first2=T.G. | year=2006 | title=General relativity in electrical engineering | journal=New Journal of Physics | volume=8 | page=247 | doi=10.1088/1367-2630/8/10/247 | issue=10 }}&lt;/ref&gt; and in many other fields.

Some useful relations in the calculus of vectors and second-order tensors in curvilinear coordinates are given in this section.  The notation and contents are primarily from Ogden,&lt;ref&gt;Ogden&lt;/ref&gt; Simmonds,&lt;ref name=Simmonds /&gt; Green and Zerna,&lt;ref name=Green/&gt;  Basar and Weichert,&lt;ref name=Basar/&gt; and Ciarlet.&lt;ref name=Ciarlet/&gt;

Let φ = φ('''x''') be a well defined scalar field and '''v''' = '''v'''('''x''') a well-defined vector field, and ''λ''&lt;sub&gt;1&lt;/sub&gt;, ''λ''&lt;sub&gt;2&lt;/sub&gt;... be parameters of the coordinates

===Geometric elements===

{{ordered list
|1= '''[[Tangent vector]]:''' If '''x'''(''λ'') parametrizes a curve ''C'' in Cartesian coordinates, then

:&lt;math&gt; {\partial \mathbf{x} \over \partial \lambda} = {\partial \mathbf{x} \over \partial q^i}{\partial q^i \over \partial \lambda} = \left( h_{ki}\cfrac{\partial q^i}{\partial \lambda}\right)\mathbf{b}_k &lt;/math&gt;

is a tangent vector to ''C'' in curvilinear coordinates (using the [[chain rule]]). Using the definition of the Lamé coefficients, and that for the metric ''g&lt;sub&gt;ij&lt;/sub&gt;'' = 0 when ''i'' ≠ ''j'', the magnitude is:

:&lt;math&gt; \left|{\partial \mathbf{x} \over \partial \lambda} \right| = \sqrt{h_{ki}h_{kj}\cfrac{\partial q^i}{\partial \lambda}\cfrac{\partial q^j}{\partial \lambda}} = \sqrt{ g_{ij}\cfrac{\partial q^i}{\partial \lambda}\cfrac{\partial q^j}{\partial \lambda}} = \sqrt{h_{i}^2\left(\cfrac{\partial q^i}{\partial \lambda}\right)^2} &lt;/math&gt;
|2= '''[[Tangent plane]] element:''' If '''x'''(''λ''&lt;sub&gt;1&lt;/sub&gt;, ''λ''&lt;sub&gt;2&lt;/sub&gt;) parametrizes a surface ''S'' in Cartesian coordinates, then the following cross product of tangent vectors is a normal vector to ''S'' with the magnitude of infinitesimal plane element, in curvilinear coordinates. Using the above result,

:&lt;math&gt; {\partial \mathbf{x} \over \partial \lambda_1}\times {\partial \mathbf{x} \over \partial \lambda_2} =\left({\partial \mathbf{x} \over \partial q^i}{\partial q^i \over \partial \lambda_1}\right) \times \left({\partial \mathbf{x} \over \partial q^j}{\partial q^j \over \partial \lambda_2}\right) = \mathcal{E}_{kmp}\left( h_{ki}{\partial q^i \over \partial \lambda_1}\right)\left(h_{mj}{\partial q^j \over \partial \lambda_2}\right) \mathbf{b}_p &lt;/math&gt;

where &lt;math&gt;\mathcal{E}&lt;/math&gt; is the [[permutation symbol]]. In determinant form:

:&lt;math&gt;{\partial \mathbf{x} \over \partial \lambda_1}\times {\partial \mathbf{x} \over \partial \lambda_2}
=\begin{vmatrix}
\mathbf{e}_1 &amp; \mathbf{e}_2 &amp; \mathbf{e}_3 \\
h_{1i} \dfrac{\partial q^i}{\partial \lambda_1} &amp; h_{2i} \dfrac{\partial q^i}{\partial \lambda_1} &amp; h_{3i} \dfrac{\partial q^i }{\partial \lambda_1} \\
h_{1j} \dfrac{\partial q^j}{\partial \lambda_2} &amp; h_{2j} \dfrac{\partial q^j}{\partial \lambda_2} &amp; h_{3j} \dfrac{\partial q^j }{\partial \lambda_2}
\end{vmatrix}&lt;/math&gt;
}}

===Integration===

:{| class="wikitable"
|-
!scope=col width="10px"| Operator
!scope=col width="200px"| Scalar field
!scope=col width="200px"| Vector field
|-
|[[Line integral]]
||&lt;math&gt; \int_C \varphi(\mathbf{x}) ds = \int_a^b \varphi(\mathbf{x}(\lambda))\left|{\partial \mathbf{x} \over \partial \lambda}\right| d\lambda&lt;/math&gt;
||&lt;math&gt; \int_C \mathbf{v}(\mathbf{x}) \cdot d\mathbf{s} = \int_a^b \mathbf{v}(\mathbf{x}(\lambda))\cdot\left({\partial \mathbf{x} \over \partial \lambda}\right) d\lambda&lt;/math&gt;
|-
| [[Surface integral]]
|| &lt;math&gt;\int_S \varphi(\mathbf{x}) dS = \iint_T \varphi(\mathbf{x}(\lambda_1, \lambda_2)) \left|{\partial \mathbf{x} \over \partial \lambda_1}\times {\partial \mathbf{x} \over \partial \lambda_2}\right| d\lambda_1 d\lambda_2&lt;/math&gt;
||&lt;math&gt;\int_S \mathbf{v}(\mathbf{x}) \cdot dS = \iint_T \mathbf{v}(\mathbf{x}(\lambda_1, \lambda_2)) \cdot\left({\partial \mathbf{x} \over \partial \lambda_1}\times {\partial \mathbf{x} \over \partial \lambda_2}\right) d\lambda_1 d\lambda_2&lt;/math&gt;
|-
| [[Volume integral]]
|| &lt;math&gt;\iiint_V \varphi(x,y,z) dV = \iiint_V \chi(q_1,q_2,q_3) Jdq_1dq_2dq_3 &lt;/math&gt;
|| &lt;math&gt;\iiint_V \mathbf{u}(x,y,z) dV = \iiint_V \mathbf{v}(q_1,q_2,q_3) Jdq_1dq_2dq_3 &lt;/math&gt;
|-
|}

===Differentiation===

The expressions for the gradient, divergence, and Laplacian can be directly extended to ''n''-dimensions, however the curl is only defined in 3d.

The vector field '''b'''&lt;sub&gt;''i''&lt;/sub&gt; is tangent to the ''q&lt;sup&gt;i&lt;/sup&gt;'' coordinate curve and forms a '''natural basis''' at each point on the curve.  This basis, as discussed at the beginning of this article, is also called the '''covariant''' curvilinear basis.  We can also define a '''reciprocal basis''', or '''contravariant''' curvilinear basis,  '''b'''&lt;sup&gt;''i''&lt;/sup&gt;.  All the algebraic relations between the basis vectors, as discussed in the section on tensor algebra, apply for the natural basis and its reciprocal at each point '''x'''.

:{| class="wikitable"
|-
!scope=col width="10px"| Operator
!scope=col width="200px"| Scalar field
!scope=col width="200px"| Vector field
!scope=col width="200px"| 2nd order tensor field
|-
| [[Gradient]]
|| &lt;math&gt; \nabla\varphi = \cfrac{1}{h_i}{\partial\varphi \over \partial q^i} \mathbf{b}^i &lt;/math&gt;
|| &lt;math&gt;\nabla\mathbf{v} = \cfrac{1}{h_i^2}{\partial \mathbf{v} \over \partial q^i}\otimes\mathbf{b}_i &lt;/math&gt;
|| &lt;math&gt;\boldsymbol{\nabla}\boldsymbol{S} = \cfrac{\partial \boldsymbol{S}}{\partial q^i}\otimes\mathbf{b}^i&lt;/math&gt;
|-
| [[Divergence]]
|| N/A
|| &lt;math&gt; \nabla \cdot \mathbf{v} = \cfrac{1}{\prod_j h_j} \frac{\partial }{\partial q^i}(v^i\prod_{j\ne i} h_j) &lt;/math&gt;
|| &lt;math&gt;
   (\boldsymbol{\nabla}\cdot\boldsymbol{S})\cdot\mathbf{a} = \boldsymbol{\nabla}\cdot(\boldsymbol{S}\cdot\mathbf{a})
 &lt;/math&gt;
where '''a''' is an arbitrary constant vector.
In curvilinear coordinates,

&lt;math&gt;\boldsymbol{\nabla}\cdot\boldsymbol{S} = \left[\cfrac{\partial S_{ij}}{\partial q^k} - \Gamma^l_{ki}S_{lj} - \Gamma^l_{kj}S_{il}\right]g^{ik}\mathbf{b}^j &lt;/math&gt;
|-
| [[Laplacian]]
||&lt;math&gt;
\nabla^2 \varphi =  \cfrac{1}{\prod _j h_j}\frac{\partial }{\partial q^i}\left(\cfrac{\prod _j h_j}{h_i^2}\frac{\partial \varphi}{\partial q^i}\right)
&lt;/math&gt;
||

||
|-
| [[Curl (mathematics)|Curl]]
|| N/A
|| For vector fields in 3d only,
&lt;math&gt; \nabla\times\mathbf{v} = \frac{1}{h_1h_2h_3} \mathbf{e}_i \epsilon_{ijk} h_i \frac{\partial (h_k v_k)}{\partial q^j} &lt;/math&gt;

where &lt;math&gt;\epsilon_{ijk}&lt;/math&gt; is the [[Levi-Civita symbol]].
|| [[Tensor_derivative_(continuum_mechanics)#Curl_of_a_tensor_field|See Curl of a tensor field]]
|}

==Fictitious forces in general curvilinear coordinates==
By definition, if a particle with no forces acting on it has its position expressed in an inertial coordinate system, (''x''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''x''&lt;sub&gt;2&lt;/sub&gt;,&amp;nbsp;''x''&lt;sub&gt;3&lt;/sub&gt;,&amp;nbsp;''t''), then there it will have no acceleration (d&lt;sup&gt;2&lt;/sup&gt;''x''&lt;sub&gt;''j''&lt;/sub&gt;/d''t''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;0).&lt;ref&gt;{{cite book | first1=Michael | last1=Friedman | title=The Foundations of Space–Time Theories | publisher=Princeton University Press | year=1989 | isbn=0-691-07239-6 }}&lt;/ref&gt; In this context, a coordinate system can fail to be “inertial” either due to non-straight time axis or non-straight space axes (or both). In other words, the basis vectors of the coordinates may vary in time at fixed positions, or they may vary with position at fixed times, or both. When equations of motion are expressed in terms of any non-inertial coordinate system (in this sense), extra terms appear, called Christoffel symbols. Strictly speaking, these terms represent components of the absolute acceleration (in classical mechanics), but we may also choose to continue to regard d&lt;sup&gt;2&lt;/sup&gt;''x''&lt;sub&gt;''j''&lt;/sub&gt;/d''t''&lt;sup&gt;2&lt;/sup&gt; as the acceleration (as if the coordinates were inertial) and treat the extra terms as if they were forces, in which case they are called fictitious forces.&lt;ref&gt;{{cite book | title=An Introduction to the Coriolis Force | first1=Henry M. | last1=Stommel | first2=Dennis W. | last2=Moore | year=1989 | publisher=Columbia University Press | isbn=0-231-06636-8}}&lt;/ref&gt; The component of any such fictitious force normal to the path of the particle and in the plane of the path’s curvature is then called [[centrifugal force]].&lt;ref&gt;{{cite book | title=Statics and Dynamics | last1=Beer | last2=Johnston | publisher=McGraw–Hill | edition=2nd | page=485 | year=1972 | isbn=0-07-736650-6 }}&lt;/ref&gt;

This more general context makes clear the correspondence between the concepts of centrifugal force in [[rotating reference frame|rotating coordinate system]]s and in stationary curvilinear coordinate systems. (Both of these concepts appear frequently in the literature.&lt;ref&gt;{{cite book | title=Methods of Applied Mathematics | authorlink=Francis B. Hildebrand | first=Francis B. |last=Hildebrand | year=1992 | publisher=Dover | page=156 | isbn=0-13-579201-0 }}&lt;/ref&gt;&lt;ref&gt;{{cite book | title=Statistical Mechanics | first=Donald Allan | last=McQuarrie | year=2000 | publisher=University Science Books | isbn=0-06-044366-9}}&lt;/ref&gt;&lt;ref&gt;{{cite book | title=Essential Mathematical Methods for Physicists | first1=Hans-Jurgen | last1=Weber | first2=George Brown | last2=Arfken | authorlink2 = George B. Arfken | publisher=Academic Press | year=2004 | page=843 | isbn=0-12-059877-9}}&lt;/ref&gt;) For a simple example, consider a particle of mass ''m'' moving in a circle of radius ''r'' with angular speed ''w'' relative to a system of polar coordinates rotating with angular speed ''W''. The radial equation of motion is ''mr''”&amp;nbsp;=&amp;nbsp;''F''&lt;sub&gt;''r''&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;''mr''(''w''&amp;nbsp;+&amp;nbsp;''W'')&lt;sup&gt;2&lt;/sup&gt;. Thus the centrifugal force is ''mr'' times the square of the absolute rotational speed ''A''&amp;nbsp;=&amp;nbsp;''w''&amp;nbsp;+&amp;nbsp;''W'' of the particle. If we choose a coordinate system rotating at the speed of the particle, then ''W''&amp;nbsp;=&amp;nbsp;''A'' and ''w''&amp;nbsp;=&amp;nbsp;0, in which case the centrifugal force is ''mrA''&lt;sup&gt;2&lt;/sup&gt;, whereas if we choose a stationary coordinate system we have ''W''&amp;nbsp;=&amp;nbsp;0 and ''w''&amp;nbsp;=&amp;nbsp;''A'', in which case the centrifugal force is again ''mrA''&lt;sup&gt;2&lt;/sup&gt;. The reason for this equality of results is that in both cases the basis vectors at the particle’s location are changing in time in exactly the same way. Hence these are really just two different ways of describing exactly the same thing, one description being in terms of rotating coordinates and the other being in terms of stationary curvilinear coordinates, both of which are non-inertial according to the more abstract meaning of that term.

When describing general motion, the actual forces acting on a particle are often referred to the instantaneous osculating circle tangent to the path of motion, and this circle in the general case is not centered at a fixed location, and so the decomposition into centrifugal and Coriolis components is constantly changing. This is true regardless of whether the motion is described in terms of stationary or rotating coordinates.

==See also==

* [[Covariance and contravariance of vectors|Covariance and contravariance]]
* [[Introduction to the mathematics of general relativity]]
* [[Orthogonal coordinates]]
* [[Frenet–Serret formulas]]
* [[Covariant derivative]]
* [[Tensor derivative (continuum mechanics)]]
* [[Curvilinear perspective]]
* [[Del in cylindrical and spherical coordinates]]

==References==
{{reflist|30em}}

==Further reading==
{{refbegin}}
*{{Cite book| first=M. R. | last=Spiegel | title=Vector Analysis | publisher=Schaum's Outline Series | location=New York | year=1959| isbn=0-07-084378-3 }}
*{{Cite book| last=Arfken | first=George | title=Mathematical Methods for Physicists | publisher=Academic Press | year=1995| isbn=0-12-059877-9}}
{{refend}}

==External links==
* [http://planetmath.org/derivationofunitvectorsincurvilinearcoordinates Planetmath.org Derivation of Unit vectors in curvilinear coordinates]
* [http://mathworld.wolfram.com/CurvilinearCoordinates.html MathWorld's page on Curvilinear Coordinates]
* [http://www.mech.utah.edu/~brannon/public/curvilinear.pdf Prof. R. Brannon's E-Book on Curvilinear Coordinates]
* [[Wikiversity:Introduction to Elasticity/Tensors#The divergence of a tensor field]] – [[Wikiversity]], Introduction to Elasticity/Tensors.

{{Orthogonal coordinate systems}}

{{DEFAULTSORT:Curvilinear Coordinates}}
[[Category:Coordinate systems]]
[[Category:Metric tensors|*3]]</text>
      <sha1>8b135arz566zuem6ega9igo7qk2q3ye</sha1>
    </revision>
  </page>
  <page>
    <title>Decimal data type</title>
    <ns>0</ns>
    <id>32940598</id>
    <revision>
      <id>869084686</id>
      <parentid>851602622</parentid>
      <timestamp>2018-11-16T09:08:15Z</timestamp>
      <contributor>
        <ip>213.105.59.2</ip>
      </contributor>
      <comment>/* Rationale */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4714">== Rationale ==
Fractional numbers are supported on most programming languages as [[floating-point number]]s or [[fixed-point number]]s. However, such representations typically restrict the denominator to a power of two. Most decimal fractions (or most fractions in general) cannot be represented exactly as a fraction with a denominator that is a power of two. For example, the simple decimal fraction 0.3 (3/10) might be represented as 5404319552844595/18014398509481984 (0.299999999999999988897769...). This inexactness causes many problems that are familiar to experienced programmers. For example, the expression &lt;code&gt;0.1 * 7 == 0.7&lt;/code&gt; might counterintuitively evaluate to false in some systems, due to the inexactness of the representation of decimals.

Although all decimal fractions are [[fraction (mathematics)|fractions]], and thus it is possible to use a [[rational data type]] to represent it exactly, it may be more convenient in many situations to consider only non-repeating decimal fractions (fractions whose denominator is a power of ten). For example, fractional units of currency worldwide are mostly based on a denominator that is a power of ten. Also, most fractional measurements in science are reported as decimal fractions, as opposed to fractions with any other system of denominators.

A decimal data type could be implemented as either a [[floating-point number]] or as a [[fixed-point number]]. In the fixed-point case, the denominator would be set to a fixed power of ten. In the floating-point case, a variable exponent would represent the power of ten to which the mantissa of the number is multiplied.

Languages that support a rational data type usually allow the construction of such a value from a string, instead of a base-2 floating-point number, due to the loss of exactness the latter would cause. Usually the basic arithmetic operations ('+', '&amp;minus;', '×', '/', integer [[power (mathematics)|powers]]) and comparisons ('=', '&amp;lt;', '&amp;gt;', '≤') would be extended to act on them — either natively or through [[operator overloading]] facilities provided by the language.  These operations may be translated by the [[compiler]] into a sequence of integer [[machine instruction]]s, or into [[library (computer science)|library]] calls.  Support may also extend to other operations, such as formatting, rounding to an integer or [[floating point]] value, etc..
An example of this is 123.456

== Standard formats ==
[[IEEE 754]] specifies three standard floating-point decimal data types of different precision:
* [[Decimal32 floating-point format]]
* [[Decimal64 floating-point format]]
* [[Decimal128 floating-point format]]

== Language support ==
* C# has a built-in data type 'decimal', consisting of 128-bit resulting in 28-29 significant digits. It has an approximate Range of (-7.9 x 10^28 to 7.9 x 10^28) / (10^(0 to 28)). &lt;ref&gt; http://msdn.microsoft.com/en-us/library/364x0z75.aspx&lt;/ref&gt;
* Starting with Python 2.4, [[Python (programming language)|Python]]'s standard library includes a &lt;tt&gt;Decimal&lt;/tt&gt; class in the module &lt;tt&gt;decimal&lt;/tt&gt;.&lt;ref&gt;https://docs.python.org/library/decimal.html&lt;/ref&gt;
* [[Ruby (programming language)|Ruby]]'s standard library includes a &lt;tt&gt;BigDecimal&lt;/tt&gt; class in the module &lt;tt&gt;bigdecimal&lt;/tt&gt;
* [[Java (programming language)|Java]]'s standard library includes a [http://docs.oracle.com/javase/6/docs/api/java/math/BigDecimal.html java.math.BigDecimal] class
* In [[Objective-C]], the [[Cocoa (API)|Cocoa]] and [[GNUstep]] APIs provide an [https://developer.apple.com/library/mac/#documentation/Cocoa/Reference/Foundation/Classes/NSDecimalNumber_Class/Reference/Reference.html NSDecimalNumber] class and an [https://developer.apple.com/library/mac/#documentation/Cocoa/Reference/Foundation/Classes/NSDecimalNumber_Class/Reference/Reference.html NSDecimal] C datatype for representing decimals whose mantissa is up to 38 digits long, and exponent is from -128 to 127.
* Some IBM systems and SQL systems support DECFLOAT format with at least the two larger formats&lt;ref&gt;http://www.ibm.com/developerworks/data/library/techarticle/dm-0801chainani/&lt;/ref&gt;
* ABAP's new DECFLOAT data type includes decimal64 (as DECFLOAT16) and decimal128 (as DECFLOAT34) formats&lt;ref&gt;http://sapignite.com/decfloat-in-abap/&lt;/ref&gt;
* [[PL/I]] natively supports both fixed-point and floating-point decimal data.
* [[GNU Compiler Collection]] (aka gcc) provides support for decimal floats as an extension&lt;ref&gt;{{cite web |title=GCC Manual |at=6.13 Decimal Floating Types |url=https://gcc.gnu.org/onlinedocs/gcc/Decimal-Float.html#Decimal-Float}}&lt;/ref&gt;

== References ==
&lt;references/&gt;

{{Data types}}

[[Category:Data types]]
[[Category:Computer arithmetic]]</text>
      <sha1>tmzvpdruczzgrovp100l9u0zgc15k15</sha1>
    </revision>
  </page>
  <page>
    <title>Desmond Paul Henry</title>
    <ns>0</ns>
    <id>11814754</id>
    <revision>
      <id>851206366</id>
      <parentid>839319284</parentid>
      <timestamp>2018-07-20T19:42:24Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v485)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="24259">{{EngvarB|date=September 2014}}
{{Use dmy dates|date=September 2014}}
{{citation style|date=March 2017}}
{{Infobox artist
 | name          = Desmond Paul Henry
 | image         = File:Desmond Paul Henry with Drawing Machine 1.jpg  
 | image_size    =
 | alt           =
 | caption       = Henry with one of his [[analogue computer]]-based drawing machines, 1962 
 | birth_name    =  
 | birth_date    = 1921
 | birth_place   = 
 | death_date    = 2004 
 | death_place   =
 | nationality   = British
 | spouse        =
 | field         = 
 | training      =
 | movement      = [[Computer art]]
 | works         = Outputs of Drawing Machine 1
 | patrons       = 
 | influenced by =
 | influenced    =
 | awards        =
 | elected       =
 | website       = http://www.desmondhenry.com/  
}}
'''Desmond Paul Henry''' (1921–2004) was a [[Victoria University of Manchester|Manchester University]] Lecturer and Reader in [[Philosophy]] (1949–82). He was one of the first British artists to experiment with machine-generated visual effects at the time of the emerging global [[computer art]] movement of the 1960s (The Cambridge Encyclopaedia 1990 p.&amp;nbsp;289; Levy 2006 pp.&amp;nbsp;178–180). During this period, Henry constructed a succession of three drawing machines from modified [[bombsight]] [[analogue computer]]s which were employed in [[World War II]] bombers to calculate the accurate release of bombs onto their targets (O'Hanrahan 2005). Henry's machine-generated effects resemble complex versions of the abstract, curvilinear graphics which accompany [[Microsoft]]'s [[Windows Media Player]]. Henry's machine-generated effects may therefore also be said to represent early examples of [[computer graphics]]: "the making of [[line art|line drawing]]s with the aid of computers and drawing machines". (Franke 1971, p.&amp;nbsp;41)

During the 1970s Henry focussed on further developing his own unique [[photochemistry|photo-chemical]] techniques for the production of original [[visual effects]]. He went on to make a fourth and a fifth drawing machine in 1984 and 2002 respectively. These later machines however, were based on a mechanical [[pendulum]] design and not bombsight computers. (O'Hanrahan 2005)

==Henry's artistic career==
It was thanks to artist [[L. S. Lowry]], working in collaboration with the then director of [[Salford Art Gallery]], A. Frape, that Henry's artistic career was launched in 1961 when he won a local competition at Salford Art gallery, entitled London Opportunity. The prize for winning this competition was a one-man exhibition show in London at the [[Reid Gallery]]. Lowry knew how crucial such a London show could be in bringing an artist to public attention. As one of the competition judges, Lowry visited Henry's home in Burford Drive, Manchester, to view his range of artistic work. (O'Hanrahan 2005)

It was at this London show of 1962, entitled Ideographs, that Henry's machine-generated effects were exhibited for the first time, along with pictures based upon Henry's photo-chemical techniques which had originally won him the competition prize. (O'Hanrahan 2005) It was this first exhibition of machine-produced effects which led to Henry and his first drawing machine being included in the first ever programme in the [[BBC]]'s ''North at Six'' series and to his being approached by the American magazine [[Life (magazine)|Life]]. (O'Hanrahan 2005) Henry and his first drawing machine were to be featured in this magazine, but the article was scrapped following the assassination of US President [[John F. Kennedy]]. The generally positive response his pictures received reflects the [[zeitgeist]] of technological optimism of the 1960s. (O'Hanrahan 2005)  [[The Guardian]] of 17/9/62 described the images produced by this first machine as being "quite out of this world" and "almost impossible to produce by human hands".

Henry's machine-generated effects went on to be exhibited at various venues during the 1960s, the most major being [[Cybernetic Serendipity]] (1968) held at the [[Institute of Contemporary Arts]] (I.C.A) in London. This represented one of the most significant art and technology exhibitions of the decade. (Goodman 1987) In this exhibition not only the effects but also the drawing machine itself was included as an interactive exhibit. Cybernetic Serendipity then went on to tour the United States, where exhibition venues included the [[Corcoran Gallery]] in [[Washington, D.C.|Washington]] and San Francisco's [[Palace of Fine Arts]]. (O'Hanrahan 2005)

This second machine returned from its tour of the United States in 1972 in a complete state of disrepair. (O'Hanrahan 2005) Such technical failures were not unusual in electric and motor-driven exhibition items. (Rosenburg 1972) More recently, frequent mechanical and/or electronic computer breakdowns contributed to the decision to close Artworks, ([[The Lowry]], Salford Quays, Manchester, U.K) in March 2003 after only three years in operation as a permanent, technology-based, interactive exhibition. (O'Hanrahan 2005)

==Henry's inspiration: the bombsight computer==
The main component of each Henry drawing machine was the bombsight computer. These mechanical analogue computers represented some of the most important technological advancements of World War Two. However, by the 1960s they already represented "old" technology when compared to the more modern [[digital computer]]s then available. (O'Hanrahan 2005)

The mechanical analogue bombsight computer was employed in [[World War Two]] bomber aircraft to determine the exact moment bombs were to be released to hit their target. The [[Bombardier (air force)|bombardier]] entered information on air and wind speed, wind direction, altitude, angle of drift and bomb weight into the computer which then calculated the bomb release point, using a complex arrangement of [[gyroscope|gyros]], [[engine|motors]], [[gear]]s and a [[telescope]]. (Jacobs 1996)
 
It was in the early 1950s that Henry purchased his very first [[Sperry Corporation|Sperry]] bombsight computer, in mint condition, from an [[army surplus]] warehouse in Shude Hill, Manchester. This purchase was inspired by Henry's lifelong passion for all things mechanical, which had been further fuelled by seven years serving as a technical clerk with the [[Royal Electrical and Mechanical Engineers]] during World War Two. (O'Hanrahan 2005) Henry so marvelled at the mechanical inner workings of this bombsight computer in motion, that he eventually decided to capture its "peerless [[parabola]]s" (as Henry termed its inner workings), on paper. He then modified the bombsight to create the first drawing machine of 1960. A second was constructed in 1963 and a third in 1967. (O'Hanrahan 2005) These machines created complex, abstract, asymmetrical, curvilinear, repetitive [[line drawing algorithm|line drawings]] which were either left untouched as completed drawings or embellished by the artist's hand in response to the suggestive machine-generated effects. None of Henry's machines now remains in operational order. (O'Hanrahan 2005)

==The drawing machines==
[[File:Wiki.Henry Drawing Machine 1.jpg|thumb|Henry's first drawing machine]]
Each Henry drawing machine was based around an analogue bombsight computer in combination with other components which Henry happened to have acquired for his home-based [[workshop]] in [[Whalley Range, Greater Manchester|Whalley Range]], Manchester. (O'Hanrahan 2005) Each machine took up to six weeks to construct and each drawing from between two hours to two days to complete. The drawing machines relied upon an external [[electric power]] source to operate either one or two [[servo motor]]s which powered the synchronisation of suspended drawing implement(s) acting upon either a stationary or moving drawing table. (O'Hanrahan 2005) With the first drawing machine Henry employed [[Ballpoint pen|biros]] as the mark-making implement; however with the machines that followed he preferred to use [[Indian ink]] in technical tube pens, since these effects, in contrast to biro ink, do not risk fading upon prolonged exposure to sunlight. (O'Hanrahan 2005)

==How the drawing machines operated==
Henry's drawing machines were quite unlike the conventional computers of the 1960s since they could not be pre-programmed nor store information. (O'Hanrahan 2005) His machines relied instead, as did those of artist [[Jean Tinguely]], upon a "mechanics of chance". (Pontus Hulten in Peiry 1997, p.&amp;nbsp;237) That is to say, they relied upon the chance relationship in the arrangement of each machine's mechanical components, the slightest alteration to which, (for example, a loosened [[screw]]), could dramatically impinge on the final result. In the words of Henry, he let each machine "do its own thing" in accordance with its ''[[sui generis]]'' mechanical features, with often surprising and unpredictable results. The imprecise way Henry's machines were both constructed and operated ensured that their effects could not be [[mass-produced]] and would be infinitely varied. (O'Hanrahan 2005)

Such imprecise tools as Henry's machines, have been judged by some to enhance artistic creativity as opposed to modern [[computer imaging]] software which leaves no scope for artistic intuition. (Reffin-Smith 1997) Nor could Henry's machines have been accused of preventing the artist from exercising [[aesthetics|aesthetic]] choice. They were truly interactive, like modern computer graphic manipulation software. With a Henry drawing machine, the artist had general overall control and was free to exercise personal and artistic intuition at any given moment of his choosing during the drawing production process. (O'Hanrahan 2005)

Both these elements of chance and interaction were in contrast to most other computer artists or [[graphic design]]ers of the period, for whom the first stage in producing a digital computer graphic was to conceive the end product. The next stage was one where, "mathematical [[formula]]e or geometric pattern manipulations (were) found to represent the desired lines. These were then programmed into a [[computer language]], punched onto [[Punched card|cards]], and read into the computer". (Sumner 1968 p.&amp;nbsp;11)

==Henry's machine-generated effects==
[[File:Wiki.picture by drawing machine 1.jpg|thumb|left|Picture produced by Drawing Machine 1]]
In 2001 Henry's machine-generated work was discussed in terms of the use made, since earliest times, of a range of tools for producing similar abstract, visual effects. (O'Hanrahan 2001) Once Henry himself had beheld the visual effects produced by his first machine, he then strove to find possible precursors such as the organic forms described in natural form mathematics. (D'Arcy-Thompson 1917; Cook 1914). Henry also compared his machine-generated effects to those produced using earlier scientific and mathematical instruments such as: Suardi's Geometric Pen of 1750 (Adams 1813), Pendulum Harmonographs (Goold et al., 1909) and the Geometric Lathe as used in ornamental and bank-note [[engraving]]. (Holtzapffel 1973 [1894])

His inclusion in 1968 in Cybernetic Serendipity enabled him to further contrast his machine-generated effects with similar though less complex and varied ones produced using a variety of tools. These included effects displayed on a visual display screen using a [[cathode-ray]] [[oscilloscope]] ([[Ben F. Laposky]] in Cybernetic Serendipity 1968) and those produced using a mechanical [[plotter]] linked to either a digital (Lloyd Sumner in Cybernetic Serendipity 1968) or analogue computer (Maughan S. Mason in Cybernetic Serendipity 1968). However Henry's drawing machines, in contrast to other precision mark-making instruments like the [[lathe]] and mechanical plotter, relied heavily upon the element of [[randomness|chance]] both in their construction and function. (O'Hanrahan 2005)

==Henry and fractal mathematics==
Henry's introduction in 2001 to the aesthetic application of [[fractal]] mathematics (Briggs 1994[1992]) provided Henry with the necessary terms of reference for describing the chance-based operational aspects of his machines. Fractal mathematics could also help describe the aesthetic appreciation of his machine-generated effects or "mechanical fractals" (Henry 2002) as he came to term them. (O'Hanrahan 2005)

Fractal systems are produced by a dynamic, non-linear system of interdependent and interacting elements; in Henry's case, this is represented by the mechanisms and motions of the drawing machine itself. (O'Hanrahan 2005) In a fractal system, as in each Henry drawing machine, very small changes or adjustments to initial influences can have far-reaching effects.

Fractal images appeal to our intuitive aesthetic appreciation of order and [[wikt:chaos|chaos]] combined. Each Henry machine-produced drawing bears all the hallmarks of a fractal image since they embody regularity and repetition coupled with abrupt changes and discontinuities. (Briggs 1994[1992]) In other words, they exhibit self-similarity (similar details on different scales) and simultaneous order and chaos. These images also resemble fractal "[[strange attractor]]s", since groups of curves present in the machine-generated effects tend to form clusters creating suggestive patterns. (Briggs 1994[1992])

Fractal patterns, similar to Henry's machine-generated effects, have been found to exist when plotting [[volcanic tremor]]s, weather systems, the [[ECG]] of heart beats and the [[electroencephalograph]]ic data of brain activity. (Briggs 1994[1992])

Henry found in fractals a means of both classifying his artistic activity and describing the aesthetic appreciation of his visual effects. Among the many artists who have previously employed what are now recognised as fractal images, are: "[[Vincent van Gogh]]'s dense swirls of energy around objects; the recursive geometries of [[M. C. Escher|Maritus Escher]]; the drip-paint, tangled abstractions of [[Jackson Pollock]]". (Briggs 1994[1992] p.&amp;nbsp;166)

==Art and technology==
{{further|Mathematics and art}}

Some would argue that scientific and technological advances have always influenced art in terms of its inspiration, tools and visual effects. In the words of Douglas Davis: "Art can no more reject either technology or science than it can the world itself". (Davis 1973, introduction) In his writings Henry himself often expressed his lifelong enthusiasm for fruitful collaborations between art and technology.(Henry: 1962, 1964, 1969, 1972)

During the First [[Machine Age]], prior to World War Two, enthusiasm for technological advances was expressed by the Machine Aesthetic which heralded the [[Modern Movement]]. (Banham 1960) Affiliated art movements of this time which shared aspects of the Machine Aesthetic included: [[Purism (arts)|Purism]] in France, [[Futurism]] in Italy (both of which celebrated the glories of modern machines and the excitement of speed), [[Suprematism]], [[Productivism]] in Russia, [[Constructivism (art)|Constructivism]], [[Precisionism]] in North America and [[kinetic sculpture]]. (Meecham and Sheldon 2000)

By the 1960s, in the Second Machine Age, technology provided not only the inspiration for art production but above all its tools (Popper 1993), as reflected by the Art and Technology movement in the United States. Adherents to this movement employed only the very latest available computer equipment. In this early phase of computer art, programmers became artists and artists became programmers to experiment with the computer's creative possibilities (Darley, 1990). Since Henry worked in comparative artistic and scientific isolation, he did not have access to the latest technological innovations, in contrast to those working, for example, at the [[Massachusetts Institute of Technology]]. (O'Hanrahan 2005)

By the 1970s, the earlier enthusiasm for technology witnessed in the 60s gave way to the [[post-modern]] loss of faith in technology as its destructive effects, both in war and on the [[Environment (biophysical)|environment]], became more apparent. (Lucie-Smith 1980) Goodman (1987) suggests that it is since 1978 that a second generation of computer artists may be recognised; a generation which no longer needs to be electronically knowledgeable or adept because the "software does it for them". (Goodman 1987, p.&amp;nbsp;47) This is in contrast to Henry who had to acquire the necessary knowledge and skills to manipulate and modify the components of the bombsight computers to construct the drawing machines. (O'Hanrahan 2005)

During the 1980s, the application in computers of the [[Integrated circuit|microchip]] (developed by 1972) increased the affordability of a [[home computer]] and led to the development of interactive computer graphics programmes like [[Sketchpad]] and various [[Paintbox (software)|Paintbox]] systems. (Darley 1991) During this period, computer art gave way almost completely to computer graphics as the computer's imaging capabilities became exploited both industrially and commercially and moved into entertainment related spheres, e.g.: [[Pixar]], [[Lucas Films]]. (Goodman 1987) The computer once again became, for some, an undisputed artistic tool in its own right. (Goodman 1987)

This renewed enthusiasm in the computer's artistic possibilities has been further reflected by the emergence towards the end of the twentieth century of various forms of cyber, virtual, or digital art, examples of which include algorithmic art and fractal art. By the twenty-first century, digitally produced and/or manipulated images were exhibited in galleries as veritable works of art in their own right (O'Hanrahan 2005).

==Legacy==
Henry's drawing machines of the 1960s represented a remarkable innovation in the field of art and technology for a variety of reasons. Firstly, the bombsight analogue computer provided not only the inspiration but also the main tool for producing highly original visual effects. (O'Hanrahan 2005) Secondly, his machines' reliance on a mechanics of chance, as opposed to pre-determined computer programmes, ensured the unrepeatable and unique quality of his infinitely varied machine-generated effects or "mechanical fractals". (O'Hanrahan 2005) Thirdly, the spontaneous, interactive potential of his drawing machines' modus operandi pre-empted by some twenty years this particular aspect of later computer graphic manipulation software. (O'Hanrahan 2005)

Finally, Henry was never artistically inspired by the graphic potential of the modern digital computer. (O'Hanrahan 2005) He much preferred the direct interaction afforded by the clearly visible interconnecting mechanical components of the earlier analogue computer and as a consequence of his drawing machines also. This was in stark contrast to the invisible and indirect workings of the later digital computer: "the mechanical analogue computer, was a work of art in itself, involving a most beautiful arrangement of gears, belts, cams differentials and so on- it still retained in its working a visual attractiveness which has now vanished in the modern electronic counterpart; … I enjoyed seeing the machine work…". (Henry, 1972)

In view of these considerations, Henry’s drawing machines may be said to not only reflect the early experimental phase of Computer Art and computer graphics but to also provide an important artistic and technological link between two distinct ages of the twentieth century: the earlier Mechanical/Industrial Age and the later Electronic/Digital Age. (O'Hanrahan 2005)

==See also==
*[[Computer art]]
*[[Interactive art]]
*[[L.S. Lowry]]
*[[Fractal art]]
*[[Computer graphics]]
*[[Jean Tinguely]]
*[[Analog computer]]

==References==
Adams, George (1813), ''Geometrical and Graphical Essays'', W &amp; S. Jones, London. (Courtesy of the Science Museum Library, London).

Banham, Reyner (1996 [1960]), ''Theory and Design in the First Machine Age'', Architectural Press, Oxford.

Briggs, John (1994[1992]), ''Fractals: the Patterns of Chaos'', Thames and Hudson, London.

''Cambridge Encyclopaedia'' (1990), Crystal, D. (ed.), "Computer Art" by David Manning, Cambridge University Press, Cambridge, p.&amp;nbsp;289.

Cook, Theodore (1979[1914]), ''The Curves of Life: An account of spiral formations and their application to the growth in nature, science and art'', Dover, New York.

''Cybernetic Serendipity'',[exh.cat] (1968). Reichardt, Jasia (ed.), Studio International, Special Issue, London.

Darley, Andy (1990), "From Abstraction to Simulation" in Philip Hayward (ed.)(1994[1990]) ''Culture, Technology and Creativity in the Late Twentieth Century'', John Libbey &amp; Company. London, pp.&amp;nbsp;39–64.

Darley, Andy (1991), 'Big Screen, Little Screen' in Ten-8:vol.2, no.2: Digital Dialogues, (ed. Bishton), pp.&amp;nbsp;80–84.

Davis, Douglas (1973) ''Art and The Future'', Praeger, New York.

Franke, H.W (1971), ''Computer graphics, Computer Art'', Phaedon, Oxford, p.&amp;nbsp;41.

Goodman, Cynthia (1987), Digital Visions: Computers and Art, Abrams, New York. 
 
Goold, J., Benham, C.E., Kerr, R., Wilberforce, L.R., (1909), ''Harmonic Vibrations'', Newton &amp; Co., London.

Henry, D.P. (1962), ''A New Project for Art''. Unpublished article submitted to Today 04/03/62.

Henry, D.P. (1964), "Art and Technology", in Bulletin of the Philosophy of Science Group, Newman Association, No. 53.

Henry, D.P (1969), "The End or the Beginning?" in Solem (Manchester Students' Union Magazine) pp.&amp;nbsp;25–27.

Henry, D.P (1972), ''Computer graphics: a case study''. (lecture given to Aberdeen University art students).

Holtzapffel, John Jacob (1973[1894]), ''The Principles and Practice of Ornamental or Complex Turning'', Dover, New York.

Jacobs, Peter (1996), ''The Lancaster Story'', Silverdale Books, Leicester.

Levy, David (2006) ''Robots Unlimited-Life in a Virtual Age'', A.K.Peters, Wellesley, USA, pp.&amp;nbsp;178–180.

Lucie-Smith, Edward (1980), ''Art in the Seventies'', Phaedon, Oxford.

Meecham, Pam and Sheldon, Julie (2000), ''Modern Art: A Critical Introduction'', Routledge, London.

O’Hanrahan, Elaine (2001)(interview) ''Intercultural Drawing Practice: the Art School Response'' In Jagjit Chuhan, (ed.) (2001), ''Responses: Intercultural Drawing Practice'', Cornerhouse Publications, Manchester, pp.: 40–47.

O’Hanrahan, Elaine (2005), ''Drawing Machines: The machine produced drawings of Dr. D. P. Henry in relation to conceptual and technological developments in machine-generated art (UK 1960–1968)''. Unpublished MPhil. Thesis. John Moores University, Liverpool.

Peiry, Lucienne (1997), ''Art Brut- The Origins of Outsider Art'', Flammarion, Paris.

Popper, Frank (1993), ''Art of the Electronic Age'', Thames and Hudson, London.

Reffin-Smith, Brian (1997), ''Post-modem Art, or: Virtual Reality as Trojan Donkey, or: horsetail tartan literature groin art'' in Stuart Mealing (ed.) (1997) Computers and Art, Intellect Books, Bristol, pp.&amp;nbsp;97–117.

Rosenberg, Harold (1972), ''The De-definition of Art'', University of Chicago Press, Chicago.

Sumner, Lloyd (1968), ''Computer Art and Human Response'', Paul B. Victorius, Charlottesville, Virginia.

[[D'Arcy Wentworth Thompson|Thompson, D'Arcy Wentworth]] (1961[1917]), ''[[On Growth and Form]]'', Cambridge University Press, Cambridge .

==External links==
*Artist's Website  http://www.desmondhenry.com/
*[http://www.bbc.co.uk/ww2peopleswar/stories/74/a2701874.shtml#msg5466032 Desmond Paul Henry: How World War Two Changed One Man's Life for the Better]
*[https://collections.vam.ac.uk/search/?listing_type=&amp;offset=0&amp;limit=15&amp;narrow=&amp;q=desmond+paul+henry Works held by the Victoria and Albert Museum]
* {{cite web |publisher= [[Victoria and Albert Museum]]
 |url= http://www.vam.ac.uk/content/articles/c/computer-art-artworks-in-detail/
 |title= Desmond Paul Henry, 'Serpent', 1962 – in detail
 |work= Prints &amp; Books
 |accessdate= 24 March 2011}}

{{Mathematical art}}
{{Authority control}}

{{DEFAULTSORT:Henry, Desmond}}
[[Category:1921 births]]
[[Category:2004 deaths]]
[[Category:British philosophers]]
[[Category:British digital artists]]
[[Category:Analog computers]]
[[Category:Academics of the Victoria University of Manchester]]
[[Category:Mathematical artists]]</text>
      <sha1>6rsnvrs5cwkr3ihednila44c2cgz9tm</sha1>
    </revision>
  </page>
  <page>
    <title>Dual impedance</title>
    <ns>0</ns>
    <id>16627059</id>
    <revision>
      <id>819506460</id>
      <parentid>819506340</parentid>
      <timestamp>2018-01-09T19:05:35Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <comment>/* Scaled and normalised duals */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7504">Dual [[Electrical impedance|impedance]] and dual network are terms used in [[Network analysis (electronics)|electronic network analysis]].  The dual of an impedance &lt;math&gt;Z&lt;/math&gt; is its reciprocal, or algebraic inverse &lt;math&gt;Z'=\frac{1}{Z}&lt;/math&gt;.   For this reason the '''dual impedance''' is also called the inverse impedance. Another way of stating this is that the dual of &lt;math&gt;Z&lt;/math&gt; is the admittance &lt;math&gt;Y'=Z&lt;/math&gt;.

The dual of a network is the network whose impedances are the duals of the original impedances.  In the case of a black-box network with multiple [[Port (circuit theory)|ports]], the impedance looking into each port must be the dual of the impedance of the corresponding port of the dual network.

This is consistent with the general notion [[Duality (electrical circuits)|duality]] of electric circuits, where the voltage and current are interchanged, etc., since &lt;math&gt;Z=\frac{V}{I}&lt;/math&gt; yields &lt;math&gt;Z'=\frac{I}{V}&lt;/math&gt;&lt;ref&gt;Ghosh, pp. 50–51&lt;/ref&gt;

__TOC__
{{Complex Z}}

==Scaled and normalised duals==

In physical units, the dual is taken with respect to some nominal or [[characteristic impedance]].  To do this, Z and Z' are scaled to the nominal impedance Z&lt;sub&gt;0&lt;/sub&gt; so that

: &lt;math&gt;\frac{Z'}{Z_0}=\frac{Z_0}{Z}&lt;/math&gt;

Z&lt;sub&gt;0&lt;/sub&gt; is usually taken to be a purely real number R&lt;sub&gt;0&lt;/sub&gt;, so Z' is changed by a real factor of R&lt;sub&gt;0&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;.  In other words, the dual circuit is qualitatively the same circuit but all the component values are scaled by R&lt;sub&gt;0&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;.&lt;ref&gt;Redifon, p.44&lt;/ref&gt;  The scaling factor R&lt;sub&gt;0&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; has the dimensions of Ω&lt;sup&gt;2&lt;/sup&gt;, so the constant 1 in the unitless expression would actually be assigned the dimensions Ω&lt;sup&gt;2&lt;/sup&gt; in a [[dimensional analysis]].

==Duals of basic circuit elements==

{| class="wikitable" style="text-align:center;"
|+&lt;ref&gt;Guillemin, pp. 535–539&lt;/ref&gt;
|-
! Element
! Z
! Dual
! Z'
|-
| [[File:Dual Z 1.PNG|thumb|left|Resistor R]]
| &lt;math&gt; R\,\!&lt;/math&gt;
| [[File:Dual Z 2.PNG|thumb|left|Conductor G = R]]
| &lt;math&gt;\frac{1}{R}&lt;/math&gt;
|-
| [[File:Dual Z 2.PNG|thumb|left|Conductor G]]
| &lt;math&gt;\frac{1}{G}&lt;/math&gt;
| [[File:Dual Z 1.PNG|thumb|left|Resistor R = G]]
| &lt;math&gt; G\,\!&lt;/math&gt;
|-
| [[File:Dual Z 3.PNG|thumb|left|Inductor L]]
| &lt;math&gt; i\omega L\,\!&lt;/math&gt;
| [[File:Dual Z 4.PNG|thumb|left|Capacitor C = L]]
| &lt;math&gt; \frac{1}{i\omega L}&lt;/math&gt;
|-
| [[File:Dual Z 4.PNG|thumb|left|Capacitor C]]
| &lt;math&gt; \frac {1}{i\omega C}&lt;/math&gt;
| [[File:Dual Z 3.PNG|thumb|left|Inductor L = C]]
| &lt;math&gt; i\omega C\,\!&lt;/math&gt;
|-
| [[File:Dual Z 5.PNG|thumb|left|Series impedances Z = Z&lt;sub&gt;1&lt;/sub&gt; + Z&lt;sub&gt;2&lt;/sub&gt;]]
| &lt;math&gt; Z_1 + Z_2\,\!&lt;/math&gt;
| [[File:Dual Z 6.PNG|thumb|left|Parallel admittances Y = Z&lt;sub&gt;1&lt;/sub&gt; + Z&lt;sub&gt;2&lt;/sub&gt;]]
| &lt;math&gt; \frac {1}{Z_1 + Z_2}&lt;/math&gt;
|-
| [[File:Dual Z 6.PNG|thumb|left|Parallel impedances 1/Z = 1/Z&lt;sub&gt;1&lt;/sub&gt; + 1/Z&lt;sub&gt;2&lt;/sub&gt;]]
| &lt;math&gt; Z = \frac{Z_1 Z_2}{Z_1 + Z_2}&lt;/math&gt;
| [[File:Dual Z 5.PNG|thumb|left|Series admittances 1/Y = 1/Z&lt;sub&gt;1&lt;/sub&gt; + 1/Z&lt;sub&gt;2&lt;/sub&gt;]]
| &lt;math&gt; \frac {1}{Z_1} + \frac{1}{Z_2}&lt;/math&gt;
|-
| [[File:Dual Z 7.PNG|thumb|left|Voltage generator V]]
| 
| [[File:Dual Z 8.PNG|thumb|left|Current generator I = V]]
|
|-
| [[File:Dual Z 8.PNG|thumb|left|Current generator I]]
| 
| [[File:Dual Z 7.PNG|thumb|left|Voltage generator V = I]]
| 
|}

==Graphical method==
There is a graphical method of obtaining the dual of a network which is often easier to use than the mathematical expression for the impedance.  Starting with a circuit diagram of the network in question, Z, the following steps are drawn on the diagram to produce Z' superimposed on top of Z.  Typically, Z' will be drawn in a different colour to help distinguish it from the original, or, if using [[computer-aided design]], Z' can be drawn on a different layer.

#A generator is connected to each [[Port (circuit theory)|port]] of the original network.  The purpose of this step is to prevent the ports from being "lost" in the inversion process.  This happens because a port left open circuit will transform into a short circuit and disappear.
#A dot is drawn at the centre of each [[Mesh analysis|mesh]] of the network Z.  These dots will become the circuit [[Node (circuits)|nodes]] of Z'.
#A conductor is drawn which entirely encloses the network Z.  This conductor also becomes a node of Z'.
#For each circuit element of Z, its dual is drawn between the nodes in the centre of the meshes either side of Z.  Where Z is on the edge of the network, one of these nodes will be the enclosing conductor from the previous step.&lt;ref&gt;Guillemin, pp. 49–52&lt;br/&gt;Suresh, pp. 516–517&lt;/ref&gt;

This completes the drawing of Z'.  This method also serves to demonstrate that the dual of a mesh transforms into a node and the dual of a node transforms into a mesh.  Two examples are given below.

===Example: star network===
{|
|-
|[[File:Graphic method 1.svg|thumb|left|200px|A star network of [[inductor]]s, such as might be found on a [[three-phase]] [[transformer]]]]
|[[File:Graphic method 2.svg|thumb|none|200px|Attaching generators to the three ports]]
|[[File:Graphic method 3.svg|thumb|left|200px|Nodes of the dual network]]
|-
|[[File:Graphic method 4.svg|thumb|none|200px|Components of the dual network]]
|[[File:Graphic method 5.svg|thumb|left|200px|The dual network with the original removed and slightly redrawn to make the topology clearer]]
|[[File:Graphic method 6.svg|thumb|none|200px|The dual network with the notional generators removed]]
|}

It is now clear that the dual of a star network of inductors is a delta network of [[capacitor]]s.  This dual circuit is not the same thing as a star-delta (Y-Δ) transformation.  A [[Y-Δ transform]] results in an [[equivalent circuit|''equivalent'' circuit]], not a dual circuit.

===Example: Cauer network===

Filters designed using [[Cauer topology (electronics)|Cauer's topology]] of the first form are [[low-pass]] filters consisting of a [[ladder network]] of series inductors and [[Shunt (electrical)#Use in electronic filter circuits|shunt]] capacitors.

[[File:Graphic method 7.svg|thumb|left|350px|A low-pass filter implemented in Cauer topology]]
[[File:Graphic method 8.svg|thumb|none|350px|Attaching generators to the input and output ports]]
[[File:Graphic method 9.svg|thumb|left|350px|Nodes of the dual network]]
[[File:Graphic method 10.svg|thumb|none|350px|Components of the dual network]]
[[File:Graphic method 11.svg|thumb|left|350px|The dual network with the original removed and slightly redrawn to make the topology clearer]]
{{clear}}

It can now be seen that the dual of a Cauer low-pass filter is still a Cauer low-pass filter.  It does not transform into a [[high-pass]] filter as might have been expected.  Note, however, that the first element is now a shunt component instead of a series component.

==See also==

* [[Topology (electrical circuits)]]

==References==
{{reflist}}

==Bibliography==
*''Redifon Radio Diary, 1970'', pp.&amp;nbsp;45–48, William Collins Sons &amp; Co, 1969.
*Ghosh, Smarajit, ''Network Theory: Analysis and Synthesis'', Prentice Hall of India
*Guillemin, Ernst A., ''Introductory Circuit Theory'', New York: John Wiley &amp; Sons, 1953 {{OCLC|535111}}
*Suresh, Kumar K. S., "Introduction to network topology" chapter 11 in ''Electric Circuits And Networks'', Pearson Education India, 2010 {{ISBN|81-317-5511-8}}.

[[Category:Analog circuits]]
[[Category:Filter theory]]
[[Category:Electronic design]]
[[Category:Duality theories]]</text>
      <sha1>9wel5itvtp0k3lk0nrlo9jcczwp07j3</sha1>
    </revision>
  </page>
  <page>
    <title>Ellen Kirkman</title>
    <ns>0</ns>
    <id>44807292</id>
    <revision>
      <id>868100788</id>
      <parentid>862961415</parentid>
      <timestamp>2018-11-10T00:39:53Z</timestamp>
      <contributor>
        <username>Smasongarrison</username>
        <id>16185737</id>
      </contributor>
      <minor/>
      <comment>copy edit with [[Wikipedia:AutoWikiBrowser/General_fixes|General fixes]]; url trimming of identifying info per[[WP:LINKSTOAVOID]] and [[Template:Citation_Style_documentation#url|Citation Style]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4691">{{Infobox scientist
| name        = Ellen Kirkman
| native_name = 
| native_name_lang = 
| image       =        
| image_size  = 
| alt         = 
| caption     = 
| birth_date  =         &lt;!--{{birth date |YYYY|MM|DD}}--&gt;
| birth_place = 
| death_date  =         &lt;!--{{death date and age |YYYY|MM|DD |YYYY|MM|DD}} (death date then birth date)--&gt;
| death_place = 
| death_cause = 
| resting_place = 
| resting_place_coordinates =  &lt;!--{{coord|LAT|LONG|type:landmark|display=inline,title}}--&gt;
| other_names = 
| residence   =
| citizenship =
| nationality = 
| fields      = [[Mathematics]]
| workplaces  = [[Wake Forest University]]
| patrons     = 
| education   = 
| alma_mater  = [[Michigan State University]]
| thesis_title =        ''On the Characterization of Inertial Coefficient Rings''
| thesis_url  =         
| thesis_year =         1975
| doctoral_advisor =   Edward E. Ingraham
| academic_advisors = 
| doctoral_students = 
| notable_students = 
| known_for   = [[Noncommutative algebra]], [[representation theory]]
| influences  = 
| influenced  = 
| awards      = {{ublist |Treasurer of [[Association for Women in Mathematics]]
| Fellow of the [[American Mathematical Society]]}}
| author_abbrev_bot = 
| author_abbrev_zoo = 
| spouse      =         &lt;!--(or | spouses = )--&gt;
| partner     =         &lt;!--(or | partners = )--&gt;
| children    = 
| signature   =         &lt;!--(filename only)--&gt;
| signature_alt = 
| website     =         &lt;!--{{URL|www.example.com}}--&gt;
| footnotes   = 
}}
'''Ellen Elizabeth Kirkman''' is professor of mathematics at [[Wake Forest University]]. Her research interests include [[noncommutative algebra]], [[representation theory]], and [[homological algebra]].&lt;ref&gt;[http://college.wfu.edu/math/people/faculty/kirkman Dr. Ellen Kirkman]&lt;/ref&gt;

==Education==
She received her Ph.D. in Mathematics and M.A. in Statistics from [[Michigan State University]] in 1975.
Her doctoral dissertation, ''On the Characterization of Inertial Coefficient Rings'', was supervised by Edward C. Ingraham.&lt;ref&gt;{{MathGenealogy|id=15430|title=Ellen Kirkman}}&lt;/ref&gt;

==Professional activities==
Since 2012 she has been treasurer of the [[Association for Women in Mathematics]].&lt;ref&gt;[https://sites.google.com/site/awmmath/awm/history AWM History], retrieved 2014-12-31.&lt;/ref&gt;

Kirkman's professional activities include serving on the [[American Mathematical Society]] (AMS) Nominating Committee 2009–11,&lt;ref&gt;{{cite web|title=AMS Committees|url=http://www.ams.org/about-us/governance/committees/nomcom-past.html|website=www.ams.org|publisher=American Mathematical Society|accessdate=19 August 2015}}&lt;/ref&gt; as an [[Mathematical Association of America]] (MAA) Governor 2006–8, on the Joint Data Committee of AMS-ASA-MAA-IMS-SIAM (2000– 2007 and 2009–present),&lt;ref&gt;{{cite web|title=AMS Committees|url=http://www.ams.org/about-us/governance/committees/comm-all.html|website=www.ams.org|publisher=American Mathematical Society|accessdate=19 August 2015}}&lt;/ref&gt; directing the CBMS 2010 survey of undergraduate mathematical sciences programs, and involvement in several [[EDGE]] programs.&lt;ref&gt;{{cite web|title=The EDGE Program|url=http://www.edgeforwomen.org/edge-1999/|website=www.edgeforwomen.org|publisher=Sylvia Bozeman and Rhonda Hughes EDGE Founcation|accessdate=19 August 2015}}&lt;/ref&gt; She is an associate editor of Communications in Algebra.&lt;ref&gt;{{cite web|title=Editorial Board|url=http://www.tandfonline.com/action/journalInformation?show=editorialBoard&amp;journalCode=lagb20|website=Communications in Algebra|publisher=Taylor and Francis|accessdate=19 August 2015}}&lt;/ref&gt;

==Recognition==
In 2012, Kirkman became a fellow of the [[American Mathematical Society]].&lt;ref&gt;[http://www.ams.org/profession/fellows-list List of Fellows of the American Mathematical Society]&lt;/ref&gt;
She is part of the 2019 class of fellows of the [[Association for Women in Mathematics]].&lt;ref&gt;{{citation|url=https://sites.google.com/site/awmmath/awm-fellows|title=2019 Class of AWM Fellows|publisher=[[Association for Women in Mathematics]]|accessdate=2018-10-07}}&lt;/ref&gt;
She has also received service awards from Wake Forest University and the Southeastern Section of the MAA.

==References==
{{Reflist}}

{{Authority control}}

{{DEFAULTSORT:Kirkman, Ellen}}
[[Category:Living people]]
[[Category:20th-century American mathematicians]]
[[Category:21st-century American mathematicians]]
[[Category:American women mathematicians]]
[[Category:Michigan State University alumni]]
[[Category:Wake Forest University faculty]]
[[Category:Fellows of the American Mathematical Society]]
[[Category:Fellows of the Association for Women in Mathematics]]
[[Category:Algebraists]]


{{US-mathematician-stub}}</text>
      <sha1>oobf4chzstbkl9r1fn13n1x7u474r60</sha1>
    </revision>
  </page>
  <page>
    <title>Euclid and his Modern Rivals</title>
    <ns>0</ns>
    <id>715365</id>
    <revision>
      <id>866022287</id>
      <parentid>866022129</parentid>
      <timestamp>2018-10-27T19:21:23Z</timestamp>
      <contributor>
        <username>Zaslav</username>
        <id>88809</id>
      </contributor>
      <comment>Clarify "Lewis Carroll".</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3971">{{italic title}}
'''''Euclid and his Modern Rivals''''' is a mathematical book published in 1879 by the English mathematician Charles Lutwidge Dodgson (1832–1898), better known under his literary pseudnym "[[Lewis Carroll]]". It considers the pedagogic merit of thirteen contemporary geometry textbooks, demonstrating how each in turn is either inferior to or functionally identical to [[Euclid]]'s ''[[Euclid's Elements|Elements]]''.

In it Dodgson supports using Euclid's geometry textbook ''The Elements'' as the geometry textbook in schools against more modern geometry textbooks that were replacing it, advocated by the [[Association for the Improvement of Geometrical Teaching]], satirized in the book as the "Association for the Improvement of Things in General".&lt;ref&gt;{{cite book | title=Mathematics in Victorian Britain | editor1-first=Raymond | editor1-last=Flood | editor2-first=Adrian | editor2-last=Rice | editor3-first=Robin | editor3-last=Wilson | editor3-link=Robin Wilson (mathematician) | publisher=[[Oxford University Press]] | year=2011 | isbn=0-19-162794-1 | page=170 }}&lt;/ref&gt;  Euclid's ghost returns in the play to defend his book against its modern rivals and tries to demonstrate how all of them are inferior to his book.

Despite its scholarly subject and content, the work takes the form of a whimsical dialogue, principally between a mathematician named Minos (taken from [[Minos]], judge of [[Hades|the underworld]] in [[Greek mythology]]) and a "[[devil's advocate]]" named Professor Niemand (German for 'nobody') who represents the "Modern Rivals" of the title.

A quote from the preface of this book was used in the [[Wikipedia:Wikipedia logos#The first official logo|first official Wikipedia logo]], which was kept in use for eight months, during the course of 2001.{{Citation needed|date=November 2011}} Due to the fisheye effect, only part of the text can be read:

{{quote|&lt;span style{{=}}"color:gray"&gt;In one respect this book is an experiment, and may chance to&lt;/span&gt; prove a failure&lt;span style{{=}}"color:gray"&gt;: I mean that I have not thought it neces&lt;/span&gt;sary to maintain throu&lt;span style{{=}}"color:gray"&gt;ghout the gravity of style which&lt;/span&gt; scientific writers usually &lt;span style{{=}}"color:gray"&gt;affect, and which has somehow&lt;/span&gt; come to be regarded as an &lt;span style{{=}}"color:gray"&gt;‘inseparable accident’ of scie&lt;/span&gt;ntific teaching. I never co&lt;span style{{=}}"color:gray"&gt;uld quite see the reasonab&lt;/span&gt;leness of this immemorial law: &lt;span style{{=}}"color:gray"&gt;subjects there are, no do&lt;/span&gt;ubt, which are in their essence &lt;span style{{=}}"color:gray"&gt;too serious to admit of any&lt;/span&gt; lightness of treatment&amp;nbsp;– but &lt;span style{{=}}"color:gray"&gt;I cannot recognise Geome&lt;/span&gt;try as one of them. Neverthe&lt;span style{{=}}"color:gray"&gt;less it will, I trust, be fou&lt;/span&gt;nd that I have permitted my&lt;span style{{=}}"color:gray"&gt;self a glimpse of the comic side&lt;/span&gt; of things only at fitting sea&lt;span style{{=}}"color:gray"&gt;sons, when the tired reader might well&lt;/span&gt; crave a moment’s&lt;span style{{=}}"color:gray"&gt; breathing-space, and not on any occasion where it could endanger the continuity of the line of argument.&lt;/span&gt;}}

{{clear}}

==References==
{{reflist}}
* {{cite book | title=Lewis Carroll in Numberland | author=Robin Wilson | authorlink=Robin Wilson (mathematician) | publisher=Allen Lane | year=2008 | isbn=978-0-7139-9757-6 | pages=91–95 }}

== External links ==
* [https://archive.org/stream/euclidandhismode000469mbp ''Euclid and his Modern Rivals'']: Scanned book at the Internet Archive
* [http://digital.library.cornell.edu/cgi/t/text/text-idx?c=math;cc=math;view=toc;subview=short;idno=03190001 ''Euclid and his Modern Rivals'' in Historical Math Monographs, Cornell University], 2nd edition from 1885

{{Lewis Carroll}}

[[Category:1879 books]]
[[Category:Mathematics books]]
[[Category:Works by Lewis Carroll]]
[[Category:1879 in science]]


{{mathematics-lit-stub}}</text>
      <sha1>eko7pssgu4x5oj9k91fb8ax3zufkhl9</sha1>
    </revision>
  </page>
  <page>
    <title>Exploded-view drawing</title>
    <ns>0</ns>
    <id>2795027</id>
    <revision>
      <id>859348597</id>
      <parentid>859348583</parentid>
      <timestamp>2018-09-13T13:06:55Z</timestamp>
      <contributor>
        <username>Zachary Schr</username>
        <id>4972794</id>
      </contributor>
      <minor/>
      <comment>Reverted 2 edits by [[Special:Contributions/223.187.229.184|223.187.229.184]] ([[User talk:223.187.229.184|talk]]) to last revision by 81.97.245.234. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5002">{{for|the studio album by Polvo|Exploded Drawing}}
{{Views}}

An '''exploded view drawing''' is a [[diagram]], [[picture]], schematic or [[technical drawing]] of an object, that shows the relationship or order of [[manufacturing|assembly]] of various parts.&lt;ref name="US PTO05"&gt;[[United States Patent and Trademark Office]] (2005), [http://www.uspto.gov/web/offices/pac/doc/general/#drawing General Information Concerning Patents § 1.84 Standards for drawings] (Revised January 2005). Accessed 13 Feb 2009.&lt;/ref&gt;

It shows the components of an object slightly separated by distance, or suspended in surrounding space in the case of a three-[[dimension]]al exploded diagram. An object is represented as if there had been a small controlled [[explosion]] emanating from the middle of the object, causing the object's parts to be separated an equal distance away from their original locations.

The exploded view drawing is used in [[parts catalog]]s, assembly and maintenance manuals and other instructional material.

The projection of an exploded view is usually shown from above and slightly in diagonal from the left or right side of the drawing. (See exploded view drawing of a gear pump to the right: it is slightly from above and shown from the left side of the drawing in diagonal.)

== Overview ==
[[File:Fully assembled view and exploded view.jpg|thumb|left|Fully assembled and exploded view in a  [[patent drawing]]]] 
[[Image:Gear pump exploded.png|thumb|320px|Exploded view drawing of a [[gear pump]]]]

An exploded view drawing is a type of drawing, that shows the intended assembly of mechanical or other parts. It shows all parts of the assembly and how they fit together. In mechanical systems usually the component closest to the center are assembled first, or is the main part in which the other parts get assembled. This drawing can also help to represent the disassembly of parts, where the parts on the outside normally get removed first.&lt;ref&gt;Michael E. Brumbach, Jeffrey A. Clade (2003). ''Industrial Maintenance''. p.65&lt;/ref&gt;

Exploded diagrams are common in descriptive [[user manual|manual]]s showing parts placement, or parts contained in an assembly or sub-assembly.  Usually such diagrams have the part identification number and a label indicating which part fills the particular position in the diagram.  Many [[spreadsheet]] applications can automatically create exploded diagrams, such as [[Pie chart#Exploded pie chart|exploded pie chart]]s.

In [[patent drawing]]s in an exploded views the separated parts should be embraced by a bracket, to show the relationship or order of assembly of various parts are permissible, see image. When an exploded view is shown in a figure that is on the same sheet as another figure, the exploded view should be placed in brackets.&lt;ref name="US PTO05"/&gt;

Exploded views can also be used in [[architectural drawing]], for example in the presentation of landscape design. An exploded view can create an image in which the elements are flying through the air above the [[architectural plan]], almost like a [[cubism|cubist painting]]. The locations can be shadowed or dotted in the siteplan of the elements.&lt;ref&gt;Chip Sullivan (2004) ''Drawing the Landscape''. p.245.&lt;/ref&gt;
{{clear|right}}

== History ==
[[File:Трансформация переменного движения в непрерывное.jpg|thumb|Exploded view by [[Leonardo da Vinci]]]]

Together with the [[Cutaway drawing|cutaway view]] the exploded view  was among the many graphic inventions of the [[Renaissance]], which were developed to clarify pictorial representation in a renewed naturalistic way. The exploded view can be traced back to the early fifteenth century notebooks of [[Taccola|Marino Taccola]] (1382–1453), and were perfected by [[Francesco di Giorgio]] (1439–1502) and [[Leonardo da Vinci]] (1452–1519).&lt;ref&gt;Eugene S. Ferguson (1999). ''Engineering and the Mind's Eye''. p.82.&lt;/ref&gt;

One of the first clearer examples of an exploded view was created by Leonardo in his design drawing of a [[reciprocating motion]] machine. Leonardo applied this method of presentation in several other studies, including those on human anatomy.&lt;ref&gt;Domenico Laurenza, Mario Taddei, Edoardo Zanon (2006). ''Leonardo's Machines''. p.165&lt;/ref&gt;

The term "Exploded View Drawing" emerged in the 1940s, and is one of the first times defined in 1965 as "Three-dimensional (isometric) illustration that shows the mating relationships of parts, subassemblies, and higher assemblies. May also show the sequence of assembling or disassembling the detail parts."&lt;ref&gt;Thomas F. Walton (1965). ''Technical Data Requirements for Systems Engineering and Support''. Prentice-Hall. p.170&lt;/ref&gt;
{{clear|right}}

== See also ==
* [[Cross section (geometry)|Cross-section]]
* [[Cutaway drawing]]
* [[Perspective (graphical)|Perspective]]

== References ==
{{reflist|30em}}
{{Commons category|Exploded views}}

[[Category:Technical drawing]]
[[Category:Methods of representation]]</text>
      <sha1>htsj9uhzjw9en6pqlewbt3cxvov65g9</sha1>
    </revision>
  </page>
  <page>
    <title>Feistel cipher</title>
    <ns>0</ns>
    <id>352709</id>
    <revision>
      <id>853833360</id>
      <parentid>811387732</parentid>
      <timestamp>2018-08-07T07:01:35Z</timestamp>
      <contributor>
        <username>Citation bot</username>
        <id>7903804</id>
      </contributor>
      <minor/>
      <comment>Alter: isbn, journal. Add: series, volume, isbn. Removed parameters. You can [[WP:UCB|use this bot]] yourself. [[WP:DBUG|Report bugs here]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9116">In [[cryptography]], a '''Feistel cipher''' is a symmetric structure used in the construction of [[block cipher]]s, named after the [[Germany|German]]-born [[physicist]] and cryptographer [[Horst Feistel]] who did pioneering research while working for [[IBM]] (USA); it is also commonly known as a '''Feistel network'''. A large proportion of block [[cipher]]s use the scheme, including the [[Data Encryption Standard]] (DES). The Feistel structure has the advantage that [[encryption]] and [[decryption]] operations are very similar, even identical in some cases, requiring only a reversal of the [[key schedule]]. Therefore, the size of the code or circuitry required to implement such a cipher is nearly halved.

A Feistel network is an iterated cipher with an internal function called a round function.&lt;ref&gt;{{cite book |title=Handbook of Applied Cryptography |first=Alfred J. |last=Menezes |first2=Paul C. van |last2=Oorschot |first3=Scott A. |last3=Vanstone |edition=Fifth |year=2001 |page=251 |isbn=978-0849385230 }}&lt;/ref&gt;

==Historical==
Feistel networks were first seen commercially in IBM's [[Lucifer (cipher)|Lucifer]] cipher, designed by [[Horst Feistel]] and [[Don Coppersmith]] in 1973. Feistel networks gained respectability when the U.S. Federal Government adopted the [[Data Encryption Standard|DES]] (a cipher based on Lucifer, with changes made by the [[National Security Agency|NSA]]). Like other components of the DES, the iterative nature of the Feistel construction makes implementing the cryptosystem in hardware easier (particularly on the hardware available at the time of DES's design).

==Theoretical work==
Many modern and also some old symmetric block ciphers are based on Feistel networks (e.g. [[GOST 28147-89]] block cipher), and the structure and properties of Feistel ciphers have been extensively explored by [[cryptographer]]s. Specifically, [[Michael Luby]] and [[Charles Rackoff]] analyzed the Feistel cipher construction, and proved that if the round function is a cryptographically secure [[pseudorandom function]], with K&lt;sub&gt;i&lt;/sub&gt; used as the seed, then 3 rounds are sufficient to make the block cipher a [[pseudorandom permutation]], while 4 rounds are sufficient to make it a "strong" pseudorandom permutation (which means that it remains pseudorandom even to an adversary who gets [[oracle machine|oracle]] access to its inverse permutation).&lt;ref name=pseudorandom&gt;{{Citation |first1=Michael |last1=Luby |first2=Charles |last2=Rackoff |title=How to Construct Pseudorandom Permutations from Pseudorandom Functions |journal=SIAM Journal on Computing |volume=17 |issue=2 |date=April 1988 |doi=10.1137/0217022 |url=http://epubs.siam.org/doi/abs/10.1137/0217022|pages=373–386 |issn=0097-5397}}&lt;/ref&gt;

Because of this very important result of Luby and Rackoff, Feistel ciphers are sometimes called Luby–Rackoff block ciphers. Further theoretical work has generalized the construction somewhat, and given more precise bounds for security.&lt;ref name=7-rounds&gt;{{Citation |first=Jacques |editor1-last=Boneh |last=Patarin |editor1-first=Dan |title=Luby–Rackoff: 7 Rounds Are Enough for 2&lt;sup&gt;''n''(1−ε)&lt;/sup&gt; Security |url=https://www.iacr.org/archive/crypto2003/27290510/27290510.pdf |doi=10.1007/b11817 |journal=Advances in Cryptology—CRYPTO 2003 |series=Lecture Notes in Computer Science |volume=2729 |date=October 2003 |pages=513–529 |accessdate=2009-07-27|isbn=978-3-540-40674-7 }}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last1=Zheng|first1=Yuliang|last2=Matsumoto|first2=Tsutomu|last3=Imai|first3=Hideki|title=On the Construction of Block Ciphers Provably Secure and Not Relying on Any Unproved Hypotheses|journal=Advances in Cryptology — CRYPTO' 89 Proceedings|volume=435|date=1989-08-20|pages=461–480|doi=10.1007/0-387-34805-0_42|url=https://link.springer.com/chapter/10.1007/0-387-34805-0_42|accessdate=2017-11-21|language=en|series=Lecture Notes in Computer Science|isbn=978-0-387-97317-3}}&lt;/ref&gt;

==Construction details==
[[File:Feistel cipher diagram en.svg|right]]

Let &lt;math&gt;{\rm F}&lt;/math&gt; be the round function and let
&lt;math&gt;K_0,K_1,\ldots,K_{n}&lt;/math&gt; be the sub-keys for the rounds &lt;math&gt;0,1,\ldots,n&lt;/math&gt; respectively.

Then the basic operation is as follows:

Split the plaintext block into two equal pieces, (&lt;math&gt;L_0&lt;/math&gt;, &lt;math&gt;R_0&lt;/math&gt;)

For each round &lt;math&gt;i =0,1,\dots,n&lt;/math&gt;, compute

:&lt;math&gt;L_{i+1} = R_i\,&lt;/math&gt;
:&lt;math&gt;R_{i+1}= L_i \oplus {\rm F}(R_i, K_i).&lt;/math&gt;

Then the ciphertext is &lt;math&gt;(R_{n+1}, L_{n+1})&lt;/math&gt;.

Decryption of a ciphertext &lt;math&gt;(R_{n+1}, L_{n+1})&lt;/math&gt; is accomplished by computing for &lt;math&gt;i=n,n-1,\ldots,0&lt;/math&gt;

:&lt;math&gt;R_{i} = L_{i+1}\,&lt;/math&gt;
:&lt;math&gt;L_{i} = R_{i+1} \oplus \operatorname{F}(L_{i+1}, K_i).&lt;/math&gt;

Then &lt;math&gt;(L_0,R_0)&lt;/math&gt; is the plaintext again.

One advantage of the Feistel model compared to a [[substitution–permutation network]] is that the round function &lt;math&gt;\operatorname{F}&lt;/math&gt; does not have to be invertible.

The diagram illustrates both encryption and decryption. Note the reversal of the subkey order for decryption; this is the only difference between encryption and decryption.

===Unbalanced Feistel cipher===
Unbalanced Feistel ciphers use a modified structure where &lt;math&gt;L_0&lt;/math&gt; and &lt;math&gt;R_0&lt;/math&gt; are not of equal lengths.&lt;ref&gt;{{cite journal|last1=Schneier|first1=Bruce|last2=Kelsey|first2=John|title=Unbalanced Feistel networks and block cipher design|journal=Fast Software Encryption|volume=1039|date=1996-02-21|pages=121–144|doi=10.1007/3-540-60865-6_49|url=https://www.schneier.com/academic/paperfiles/paper-unbalanced-feistel.ps.gz|accessdate=2017-11-21|language=en|series=Lecture Notes in Computer Science|isbn=978-3-540-60865-3}}&lt;/ref&gt; The [[Skipjack (cipher)|Skipjack]] cipher is an example of such a cipher. The [[Texas Instruments]] [[digital signature transponder]] uses a proprietary unbalanced Feistel cipher to perform [[challenge–response authentication]].&lt;ref name="crypto-rfid"&gt;{{cite journal|last1=Bono|first1=Stephen|last2=Green|first2=Matthew|last3=Stubblefield|first3=Adam|last4=Juels|first4=Ari|last5=Rubin|first5=Aviel|last6=Szydlo|first6=Michael|title=Security Analysis of a Cryptographically-Enabled RFID Device|journal=Proceedings of the USENIX Security Symposium|date=2005-08-05|url=https://www.usenix.org/event/sec05/tech/bono/bono.pdf|accessdate=2017-11-21}}&lt;/ref&gt;

The [[Thorp shuffle]] is an extreme case of an unbalanced Feistel cipher in which one side is a single bit. This has better provable security than a balanced Feistel cipher but requires more rounds.&lt;ref name="thorp"&gt;{{cite journal|last1=Morris|first1=Ben|last2=Rogaway|first2=Phillip|last3=Stegers|first3=Till|title=How to Encipher Messages on a Small Domain|journal=Advances in Cryptology - CRYPTO 2009|volume=5677|date=2009|pages=286–302|doi=10.1007/978-3-642-03356-8_17|url=http://www.cs.ucdavis.edu/~rogaway/papers/thorp.pdf|accessdate=2017-11-21|language=en|series=Lecture Notes in Computer Science|isbn=978-3-642-03355-1}}&lt;/ref&gt;

===Other uses===
The Feistel construction is also used in cryptographic algorithms other than block ciphers. For example, the [[optimal asymmetric encryption padding]] (OAEP) scheme uses a simple Feistel network to randomize ciphertexts in certain [[asymmetric key encryption]] schemes.

A generalized Feistel algorithm can be used to create strong permutations on small domains of size not a power of two (see [[format-preserving encryption]]).&lt;ref name=thorp /&gt;

===Feistel networks as a design component===
Whether the entire cipher is a Feistel cipher or not, Feistel-like networks can be used as a component of a cipher's design. For example, [[MISTY1]] is a Feistel cipher using a three-round Feistel network in its round function, [[Skipjack (cipher)|Skipjack]] is a modified Feistel cipher using a Feistel network in its G permutation, and [[Threefish]] (part of [[Skein (hash function)|Skein]]) is a non-Feistel block cipher that uses a Feistel-like MIX function.

==List of Feistel ciphers==
Feistel or modified Feistel:

{{col-begin}}
{{col-break}}
* [[Blowfish (cipher)|Blowfish]]
* [[Camellia (cipher)|Camellia]]
* [[CAST-128]]
* [[Data Encryption Standard|DES]]
* [[FEAL]]
* [[GOST 28147-89]]
* [[Information Concealment Engine|ICE]]
{{col-break}}
* [[KASUMI (block cipher)|KASUMI]]
* [[LOKI97]]
* [[Lucifer (cipher)|Lucifer]]
* [[MARS (cryptography)|MARS]]
* [[MAGENTA (cipher)|MAGENTA]]
* [[MISTY1]]
{{col-break}}
* [[RC5]]
* [[Simon (cipher)|Simon]]
* [[Tiny Encryption Algorithm|TEA]]
* [[Triple DES]]
* [[Twofish]]
* [[XTEA]]
{{col-end}}

Generalised Feistel:

{{col-begin}}
{{col-break}}
* [[CAST-256]]
* [[CLEFIA]]
* [[MacGuffin (cipher)|MacGuffin]]
* [[RC2]]
{{col-break}}
* [[RC6]]
* [[Skipjack (cipher)|Skipjack]]
* [[SMS4]]
{{col-end}}

==See also==
* [[Cryptography]]
* [[Stream cipher]]
* [[Substitution–permutation network]]
* [[Lifting scheme]] for discrete wavelet transform has pretty much the same structure
* [[Format-preserving encryption]]
* [[Lai–Massey scheme]]

==References==
{{Reflist}}

{{Cryptography navbox | block}}

[[Category:Cryptography]]
[[Category:Feistel ciphers]]</text>
      <sha1>9v01sl3nehvx7g43iipkanutsb5kpgt</sha1>
    </revision>
  </page>
  <page>
    <title>Fundamental normality test</title>
    <ns>0</ns>
    <id>28268647</id>
    <revision>
      <id>830789754</id>
      <parentid>669418240</parentid>
      <timestamp>2018-03-16T23:21:39Z</timestamp>
      <contributor>
        <username>Certes</username>
        <id>5984052</id>
      </contributor>
      <minor/>
      <comment>Disambiguating links to [[Family (disambiguation)]] (link changed to [[Normal family]]) using [[User:Qwertyytrewqqwerty/DisamAssist|DisamAssist]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="967">In [[complex analysis]], a mathematical discipline, the '''fundamental normality test''' gives sufficient conditions to test the normality of a [[Normal family|family]] of [[analytic functions]]. It is another name for the stronger version of [[Montel%27s theorem#Functions omitting two values|Montel's Theorem]].

==Statement of theorem==
Let &lt;math&gt;\mathcal{F} &lt;/math&gt; be a family of analytic functions defined on a domain &lt;math&gt; \Omega &lt;/math&gt;. If there are two fixed complex numbers ''a'' and ''b'' that are omitted from the range of every ''&amp;fnof;''&amp;nbsp;&amp;isin;&amp;nbsp;&lt;math&gt;\mathcal{F}&lt;/math&gt;, then &lt;math&gt; \mathcal{F} &lt;/math&gt; is a [[normal family]] on &lt;math&gt; \Omega &lt;/math&gt;.

The proof relies on properties of the [[elliptic modular function]] and can be found here: 
{{cite book | author = J. L. Schiff | title = Normal Families | publisher = Springer-Verlag | year = 1993 | isbn=0-387-97967-0 }}

==See also==
*[[Montel's theorem]]

[[Category:Complex analysis]]</text>
      <sha1>9d6rk39vdmfwfwxodi82jjqrh36bji5</sha1>
    </revision>
  </page>
  <page>
    <title>German Statutory Accident Insurance</title>
    <ns>0</ns>
    <id>17604036</id>
    <revision>
      <id>839360269</id>
      <parentid>819707617</parentid>
      <timestamp>2018-05-02T22:47:11Z</timestamp>
      <contributor>
        <username>Premeditated Chaos</username>
        <id>31530</id>
      </contributor>
      <comment>not an orphan</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6708">{{Merge|Worker's compensation (Germany)|date=December 2017}}{{Multiple issues|
{{refimprove|date=May 2008}}
{{original research|date=May 2008}}
}}

'''German Statutory Accident Insurance''' is among the oldest branches of German [[social insurance]].  [[Occupational accident]] insurance was established in Germany by statute in 1884. 

==History==
In 1871, the [[German Empire]] was founded again, at the end of the [[Franco-Prussian War]]. Formerly Chancellor of [[Prussia]], [[Otto von Bismarck]], now Chancellor of the new German Empire, introduced highly progressive [[welfare]] [[legislation]] by the standards of [[Europe]] at the time. (5, 6)

[[Occupational accident]] insurance was established in Germany by statute in 1884. The agencies in charge of providing this form of [[insurance]] are the industrial and agricultural [[employers' liability]] funds as well as [[public sector]] [[accident insurance]] funds, which include both municipal accident insurance association and other accident funds. While employers'liability funds are organized according to industry, the public sector accident insurance funds are the most part organized regionally. (1, 2, 3)

The accident insurance funds govern themselves ([[self-administration]]) with equal representation divided between [[employer]]s, [[entrepreneur]]s and [[employee]]s. The organs of self-administration are the member’s assembly and executive board. This arrangement ensures that the interests of all participants are represented.

The legal basis for occupational accident insurance is formed by the German Social Code, in particular Book VII (SGB VII). (1, 2, 3)

==Financing==
Statutory occupational accident insurance is among the oldest branches of German [[social insurance]]. By contrast with health, [[long-term care]], [[pension]] and [[unemployment]] insurance, statutory occupational accident insurance is contribution-free for those insured. The costs for comprehensive insurance coverage for prevention, rehabilitation are borne by employers. For public-sector jobs, the federal, state and municipal governments carry the costs.

The contribution rates are determinates according to the pay-as-you-go principle, based on [[expenditure]]s in prior years. This means that at the end of each [[fiscal year]] the statutory accident insurance funds allocate their expenditures among the member companies. The calculation basis is thus formed by actual financing needs, i.e. the allocation amount to be put aside, the wages and salaries of the insured and the [[hazard class]] of the particular industry concerned. For the municipal accident insurance associations and accidents funds, the contributions are based on the population, the number of insured persons, or wages and salaries. (1, 2, 3)

==Who is insured==
Every year some 1,400,000 accidents in the [[Federal Republic of Germany]] involving employees who are either working or on their way to or from work. These are joined by around 18,000 cases of recognized [[occupational illness]]es and some 1, 5 million school accidents. For those affected, the consequences often entail wide-ranging changes in their way of life. Restoring these people’s health and, as far as possible, their ability to work is the task of statutory accident insurance.

Every employee and trainee is covered by statutory occupational accident insurance. In [[industry]] and [[agriculture]] the employer’s liability insurance fund (Berufsgenossenschaften) are responsible for accident insurance. Providing coverage in the public sector are the municipal accident insurance associations (Gemeindeunfallversicherungsverbände) and other public-sector accident funds.

Coverage is provided for accidents at work or school or on the way to or from work or school- as well as for occupational illnesses. (1, 2, 3)

==Benefits==
Statutory occupational accident insurance has the task of undertaking measures to prevent job-related [[accident]]s and [[illness]]es, as well as to protect workers from on-the-job hazards. In cases where occupational accidents or illnesses do occur, [[accident insurance]] provides assistance toward restoring the health and working ability of the persons involved and compensation to the insured persons or their persons or their survivors through the provision of cash benefits.

The primary mission of statutory occupational accident insurance according to the legislation is the use all means at its disposal to prevent occupational accidents and illness from occurring in the first place and to minimize potential job-related hazards. The focus is placed on advising companies in all matters having to do with [[industrial safety]] and health. This includes providing employers and employees with comprehensive instructions and guideline, as well as international media. The accident insurance agencies also hold free informational, motivational events on the subject of safety at work. (1, 3)

If an insured person has an accident at work or suffers from an occupational illness, statutory occupational accident insurance covers the resulting costs. This means that the insurance fund provides the best possible medical, occupational and social rehabilitation, as well as [[Damages|financial compensation]] if applicable.

In the event of an occupational accident or illness, statutory occupational accident insurance provides:
payment for full [[medical treatment]],
occupational integration assistance (for example, retraining),
social integration assistance and supplementary assistance, and
cash benefits to the insured and their surviving independents.

The top priority of the accident insurance fund is to restore the health and ability to work of the insured person. Pensions are paid to fund members only when it is not possible to fully restore their ability to work, i.e. for those whose earning capacity is reduced by at least 20 percent. (1, 3)

==References==
1. [http://www.dguv.de German Accident Insurance Organisation] &lt;br /&gt;
2. [http://www.lsv.de/verbaende/02blb/index.html Federal association of agricultural Accident Insurance Funds (Bundesverband der landwirtschaftlichen Berufsgenossenschaften)]&lt;br /&gt;
3. [http://www.deutsche-sozialversicherung.de German Social Security]&lt;br /&gt;
5. [http://www.alemania-turismo.com German Travel Agency]&lt;br /&gt;
6. [http://www.facts-about-germany.de Fact about Germany] &lt;br /&gt;
7. [http://www.deutsche-kultur-international.de German International Culture] &lt;br /&gt;
8. [http://www.deutschland.de Germany]&lt;br /&gt;
&lt;br /&gt;

[[Category:Actuarial science]]
[[Category:Employee benefits]]
[[Category:German labour law]]
[[Category:Social programs]]
[[Category:Trade unions in Germany]]
[[Category:Types of insurance]]</text>
      <sha1>qigu0s3tx12jwvr22yhm1nx49pn7y57</sha1>
    </revision>
  </page>
  <page>
    <title>Herbert Enderton</title>
    <ns>0</ns>
    <id>29407693</id>
    <revision>
      <id>869209372</id>
      <parentid>812149370</parentid>
      <timestamp>2018-11-17T03:06:07Z</timestamp>
      <contributor>
        <username>DferDaisy</username>
        <id>28778608</id>
      </contributor>
      <comment>/* Selected publications */ citation clean up. 2001 edition was a reprint, not a new edition.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4224">{{Infobox scientist
| name              = Herbert B. Enderton
| image             = &lt;!--(filename only)--&gt;
| image_size        = 
| alt               = 
| caption           = 
| birth_name        = Herbert Bruce Enderton
| birth_date        = {{Birth date|1936|4|15}}
| birth_place       = 
| death_date        = {{Death date and age|2010|10|20|1936|4|15}} 
| death_place       = [[Santa Monica, California]], U.S.
| nationality       = [[United States|American]]
| fields            = [[Mathematical Logic]]
| workplaces        = [[University of California, Los Angeles|UCLA]]
| alma_mater        = [[Harvard University]]}}

'''Herbert Bruce Enderton''' (April 15, 1936 – October 20, 2010)&lt;ref name="NAMS"&gt;{{cite journal|url=http://www.ams.org/notices/201101/rtx110100070p.pdf|title=Deaths of AMS Members|format=PDF|publisher=[[American Mathematical Society|AMS]]|journal=[[Notices of the American Mathematical Society]]|volume=58|issue=1|date=January 2011}}&lt;/ref&gt; was a [[Professor Emeritus]] of [[Mathematics]] at [[University of California, Los Angeles|UCLA]] and a former member of the faculties of Mathematics and of Logic and the Methodology of Science at the [[University of California, Berkeley]].

Enderton also contributed to [[recursion theory]], the theory of definability, models of analysis, [[Computational complexity theory|computational complexity]], and the [[history of logic]].&lt;ref&gt;{{cite web|url=https://www.ucalgary.ca/rzach/blog/2010/10/herbert-b-enderton-1936-2010.html|title=Herbert B. Enderton, 1936-2010|author=Richard Zach|publisher=[[University of Calgary]]|date=October 28, 2010|accessdate=February 9, 2011}}&lt;/ref&gt;

He earned his Ph.D. at [[Harvard University|Harvard]] in 1962.&lt;ref&gt;{{cite web |url=http://www.math.ucla.edu/people/pages/hbe.shtml|title=UCLA Department of Mathematics|publisher=UCLA|accessdate=February 9, 2011}}&lt;/ref&gt; He was a member of the [[American Mathematical Society]] from 1961 until his death.&lt;ref name="NAMS"/&gt;

==Personal life== 
He lived in [[Santa Monica]]. He married his wife, Cathy, in 1961 and they had two sons; Eric and Bert.&lt;ref name="LAT"&gt;{{cite news|url=http://www.legacy.com/obituaries/latimesobituary.aspx?pid=146308249&amp;ua=ixxkrea3b00vhy80sbujtq%3d%3d|title=Obituary|newspaper=[[Los Angeles Times]]|date=October 31, 2010|accessdate=February 9, 2011}}&lt;/ref&gt;

==Later years==
From 1980 to 2002 he was coordinating editor of the reviews section of the [[Association for Symbolic Logic]]'s [[Journal of Symbolic Logic]].&lt;ref&gt;{{cite web|url=http://www.aslonline.org/journals_reviews.htm|title=Journals - Reviews|publisher=[[Association for Symbolic Logic]]|accessdate=February 9, 2011}}&lt;/ref&gt;

==Death==
He died from [[leukemia]] in 2010.&lt;ref name="LAT"/&gt;

==Selected publications==
* {{cite book|first=Herbert B.|last=Enderton|display-authors=0|title=Elements of Set Theory|publisher=[[Academic Press]]|isbn= 978-0-12-238440-0|year=1977}}
* {{cite book|first=Herbert B.|last=Enderton|display-authors=0|title=A Mathematical Introduction to Logic|publisher=[[Academic Press]]|year=1972|isbn=978-0-12-238452-3}}
* {{cite book|first=Herbert B.|last=Enderton|display-authors=0|title=Computability Theory: An Introduction to Recursion Theory|publisher=[[Academic Press]]|date=2011|isbn=978-0-12-384958-8}}

==References==
{{reflist}}

==External links==
* [https://web.archive.org/web/20110320065537/http://www.math.ucla.edu/~hbe/ Herbert B. Enderton home page]
* [https://web.archive.org/web/20101104003152/http://www.math.ucla.edu/~hbe/pub.html Enderton publications]
* [https://www.youtube.com/results?search_query=herbert+enderton&amp;aq=f Herbert Enderton UCLA lectures] on [[YouTube]]

{{Authority control}}

{{DEFAULTSORT:Enderton, Herbert}}
[[Category:1936 births]]
[[Category:2010 deaths]]
[[Category:American educators]]
[[Category:20th-century American mathematicians]]
[[Category:21st-century American mathematicians]]
[[Category:American logicians]]
[[Category:Mathematical logicians]]
[[Category:Set theorists]]
[[Category:Harvard University alumni]]
[[Category:Disease-related deaths in California]]
[[Category:University of California, Berkeley faculty]]
[[Category:University of California, Los Angeles faculty]]
[[Category:Place of birth missing]]</text>
      <sha1>1717t2tfxo0g8c53gkbedvrce2otiwl</sha1>
    </revision>
  </page>
  <page>
    <title>Homersham Cox (mathematician)</title>
    <ns>0</ns>
    <id>55620821</id>
    <revision>
      <id>870783882</id>
      <parentid>867189787</parentid>
      <timestamp>2018-11-26T23:50:58Z</timestamp>
      <contributor>
        <username>Rgdboer</username>
        <id>92899</id>
      </contributor>
      <comment>paraphrase, not quote, wrong citation Undid revision 867117608 by [[Special:Contributions/Diannaa|Diannaa]] ([[User talk:Diannaa|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8700">'''Homersham Cox''' (1857–1918) was an English mathematician.&lt;ref&gt;{{cite book|editor=Steed, H. E.|title=The register of Tonbridge School from 1826 to 1910|year=1911|publisher=Rivingtons|url=https://archive.org/details/b29012715|pages=150}}&lt;/ref&gt;&lt;ref&gt;{{acad|CS875H|Cox, Homersham}}&lt;/ref&gt;

==Life==

He was the son of [[Homersham Cox]] (1821–1897) and brother of [[Harold Cox]] and was educated at [[Tonbridge School]] (1870-75). At [[Trinity College, Cambridge]], he graduated B.A. as 4th [[Wrangler (University of Cambridge)|wrangler]] in 1880, and M.A. in 1883. He became a [[Research fellow|fellow]] in 1881. 

Cox wrote four papers applying algebra to physics, and then turned to [[mathematics education]] with a book on [[arithmetic]] in 1885. His ''Principles of Arithmetic'' included [[binary number]]s, [[prime number]]s, and [[permutation]]s.&lt;ref group=c&gt;{{cite book|author=Cox, H.|title=Principles of Arithmetic|year=1885|publisher=Deighton|url=https://archive.org/details/principlesarith00coxgoog}}&lt;/ref&gt;

Contracted to teach mathematics at [[Muir Central College]], Cox became a resident of [[Allahabad]], [[Uttar Pradesh]] from 1891 to 1918.

==Work on non-Euclidean geometry==
1881–1883 he published papers on [[non-Euclidean geometry]].&lt;ref group=c name=cox1&gt;{{Cite journal|author=Cox, H.|year=1881|title=Homogeneous coordinates in imaginary geometry and their application to systems of forces|journal=[[The Quarterly Journal of Pure and Applied Mathematics]]|volume=18|issue=70|pages=178-192|url=http://resolver.sub.uni-goettingen.de/purl?PPN600494829_0018}}&lt;/ref&gt;&lt;ref group=c name=cox2&gt;{{Cite journal|author=Cox, H.|year=1882|orig-year=1881|title=Homogeneous coordinates in imaginary geometry and their application to systems of forces (continued)|journal=The Quarterly Journal of Pure and Applied Mathematics|volume=18|issue=71|pages=193-215|url=http://resolver.sub.uni-goettingen.de/purl?PPN600494829_0018}}&lt;/ref&gt;&lt;ref group=c name=cox3&gt;{{Cite journal|author=Cox, H.|year=1883|orig-year=1882|title=On the Application of Quaternions and Grassmann's Ausdehnungslehre to different kinds of Uniform Space|journal=[[Transactions of the Cambridge Philosophical Society]]|volume=13|pages=69-143|url=https://archive.org/details/transactions13camb}}&lt;/ref&gt;&lt;ref group=c name=cox4&gt;{{Cite journal|author=Cox, H.|year=1883|orig-year=1882|title=On the Application of Quaternions and Grassmann's Ausdehnungslehre to different kinds of Uniform Space|journal=[[Proceedings of the Cambridge Philosophical Society]]|volume=4|pages=194-196|url=https://archive.org/details/proceedingsofcam4188083camb}}&lt;/ref&gt;

For instance, in his 1881 paper (which was published in two parts in 1881 and 1882)&lt;ref group=c name=cox1 /&gt;&lt;ref group=c name=cox2 /&gt; he described homogeneous coordinates for hyperbolic geometry, now called Weierstrass coordinates of the [[hyperboloid model]] introduced by [[Wilhelm Killing]] (1879) and [[Henri Poincaré]] (1881)). Like Poincaré in 1881, Cox wrote the general [[Lorentz transformation]]s leaving invariant the quadratic form &lt;math&gt;z^2-x^2-y^2=1&lt;/math&gt;, and in addition also for &lt;math&gt;w^2-x^2-y^2-z^2=1&lt;/math&gt; (see [[History of Lorentz transformations#Cox]]). He also formulated the [[Lorentz boost]] which he described as a transfer of the origin in the hyperbolic plane, on page 194:

:&lt;math&gt;\begin{align}X &amp; =x\cosh p-z\sinh p\\
Z &amp; =-x\sinh p+z\cosh p
\end{align} \quad \text{and} \quad \begin{align}x &amp; =X\cosh p+Z\sinh p\\
z &amp; =X\sinh p+Z\cosh p
\end{align}&lt;/math&gt;

Similar formulas have been used by [[Gustav von Escherich]] in 1874, whom Cox mentions on page 186. In his 1882/1883 paper&lt;ref group=c name=cox3 /&gt;&lt;ref group=c name=cox4 /&gt;, which deals with Non-Euclidean geometry, [[quaternion]]s and [[exterior algebra]], he provided the following formula describing a transfer of point P to point Q in the hyperbolic plane, on page 86

: &lt;math&gt;
\begin{align}
QP^{-1} &amp; =\cosh\theta+\iota\sinh\theta\\
QP^{-1} &amp; =e^{\iota\theta}
\end{align} \quad (\iota^2=1)
&lt;/math&gt;

together with &lt;math&gt;\cos\theta+\iota\sin\theta&lt;/math&gt; with &lt;math&gt;\iota^2=-1&lt;/math&gt; for elliptic space, and &lt;math&gt;1-\iota\theta&lt;/math&gt; with &lt;math&gt;\iota^2=0&lt;/math&gt; for parabolic space. On page 88, he identified all these cases as [[quaternion]] multiplications. The variant &lt;math&gt;\iota^2=1&lt;/math&gt; is now called a [[hyperbolic number]], the whole expression on the left can be used as a hyperbolic [[versor]]. Subsequently, that paper was described by [[Alfred North Whitehead]] (1898) as follows:&lt;ref&gt;{{cite book|author=Whitehead, A.|title=A Treatise on Universal Algebra|year=1898|pages=370|publisher=Cambridge University Press|url=https://archive.org/details/principlesarith00coxgoog}}&lt;/ref&gt; 

{{Quote|''Homersham Cox constructs a linear algebra [cf. 22] analogous to Clifford's [[Biquaternion]]s which applies to Hyperbolic Geometry of two and three and higher dimensions. He also points out the applicability of Grassmann's Inner Multiplication for the expression of the distance formulae both in Elliptic and Hyperbolic Space; and applies it to the metrical theory of systems of forces. His whole paper is most suggestive.}}

==Cox's chain==
In 1891 Cox published a chain of theorems in Euclidean geometry of three dimensions:

(i) In space of three dimensions take a point 0 through which pass sundry planes ''a, b, c, d, e'',....

(ii) Each two planes intersect in a line through 0. On each such line a point is taken at random. The point on the line of intersection of the planes ''a'' and ''b'' will be called the point ''ab''.

(iii) Three planes ''a, b, c'', give three points ''be, ac, ab''. These determine a plane. It will be called the plane ''abc''. Thus the planes ''a, b, c, abc'', form a tetrahedron with vertices ''be, ac, ab'', 0.

(iv) Four planes ''a, b, c, d'', give four planes ''abc, abd, acd, bed''. It can be proved that these meet in a point. Call it the point ''abed''.

(v) Five planes ''a, b, c, d, e'', give five points such as ''abed''. It can be proved that these lie in a plane. Call it the plane ''abede''.

(vi) Six planes ''a, b, c, d, e, f'', give six planes such as ''abcde''. It can be proved that these lie in a plane. Call it the plane ''abcdef''.
And so on indefinitely.&lt;ref group=c&gt;{{Cite journal|author=Cox, H.|year=1891|title=Application of Grassmann's Ausdehnungslehre to properties of circles|journal=[[The Quarterly Journal of Pure and Applied Mathematics]]|volume=25|pages=1–70|url=http://resolver.sub.uni-goettingen.de/purl?PPN600494829_0025}}&lt;/ref&gt;

The theorem has been compared to [[Clifford's circle theorems]] since they both are an infinite chain of theorems. In 1941 Richmond argued that Cox's chain was superior:
:Cox's interest lay in the discovery of applications of Grassmann's Ausdehnungslehre and he uses the chain to that end. Any present-day geometer (to whom many of Cox's properties of circles in a plane must appear not a little artificial) would agree that his figure of points and planes in space is simpler and more fundamental than that of circles in a plane which he derives from it. Yet this figure of 2&lt;sup&gt;n&lt;/sup&gt; circles shows beyond a doubt the superiority of Cox's chain over Clifford's; for the latter is included as a special case when half the circles in the former shrink into points. Cox's plane figure of 2&lt;sup&gt;n&lt;/sup&gt; circles can be derived by elementary methods.&lt;ref&gt;Herbert W. Richmond (1941) "On a chain of theorems due to Homersham Cox", [[Journal of the London Mathematical Society]] 16: 105–7, {{mr|id=0004964}}&lt;/ref&gt;

[[H. S. M. Coxeter]] derived Clifford's theorem by exchanging the arbitrary point on a line ''ab'' with an arbitrary sphere about 0 which then intersects ''ab''. The planes ''a, b, c'', ... intersect this sphere in circles which can be projected stereographically into a plane. The planar language of Cox then translates to the circles of Clifford.&lt;ref&gt;H. S. M. Coxeter (1950) [https://projecteuclid.org/download/pdf_1/euclid.bams/1183514923 Self-dual configurations and regular graphs], [[Bulletin of the American Mathematical Society]] 56: 413–55,  especially 447, via [[Project Euclid]]&lt;/ref&gt;

In 1965 Cox's first three theorems were proven in Coxeter's [[textbook]] ''Introduction to Geometry''.&lt;ref&gt;H. S. M Coxeter (1965) ''Introduction to Geometry'', page 258, [[John Wiley &amp; Sons]]&lt;/ref&gt;

==Works==
&lt;references group=c /&gt;

==References==

{{reflist}}


{{Authority control}}

{{DEFAULTSORT:Cox, Homersham}}
[[Category:1857 births]]
[[Category:1918 deaths]]
[[Category:19th-century English mathematicians]]
[[Category:20th-century English mathematicians]]
[[Category:Geometers]]
[[Category:Fellows of Trinity College, Cambridge]]
[[Category:English expatriates in India]]</text>
      <sha1>hjz5znfk4fi7in16nbzzyv2gkd0dsdr</sha1>
    </revision>
  </page>
  <page>
    <title>Jayme Luiz Szwarcfiter</title>
    <ns>0</ns>
    <id>27497399</id>
    <revision>
      <id>841847066</id>
      <parentid>841771528</parentid>
      <timestamp>2018-05-18T13:18:30Z</timestamp>
      <contributor>
        <ip>37.26.147.143</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5228">{{Infobox scientist|name = Jayme Luiz Szwarcfiter
|image = 
|caption = 
|birth_date = {{Birth date|1942|7|5|df=y}}
|birth_place = [[Rio de Janeiro]], [[Brazil]]
|death_date = 
|death_place = 
|residence = [[Rio de Janeiro]], [[Brazil]]
|citizenship =
|nationality = [[Brazil]]ian
|ethnicity = [[Jewish]]
|fields = [[Computer science]]
|workplaces = [[Federal University of Rio de Janeiro]]
|alma_mater = [[Federal University of Rio de Janeiro]]
|doctoral_advisor = [[Leslie Blackett Wilson]]
|academic_advisors =
|doctoral_students = 
|notable_students =
|known_for = [[Combinatorics]]&lt;br /&gt;[[Graph theory]]
|author_abbrev_bot =
|author_abbrev_zoo =
|influences =
|influenced =
|awards = [[Ordem Nacional do Mérito Científico]]
|religion = |signature = &lt;!--(filename only)--&gt;
|footnotes = 
}}

'''Jayme Luiz Szwarcfiter''' (born July 5, 1942 in [[Rio de Janeiro]]) is a [[computer science|computer scientist]] in [[Brazil]].

==Biography==
Szwarcfiter graduated in 1967 in electronic engineering from the [[Federal University of Rio de Janeiro]] (UFRJ). He received his MA in 1971 from [[COPPE]]. In 1975 he obtained his PhD in computer science from the [[University of Newcastle Upon Tyne]], England, under supervision of [[Leslie Blackett Wilson]].&lt;ref&gt;{{cite web|title=Jayme Luiz Szwarcfiter|url=http://buscatextual.cnpq.br/buscatextual/visualizacv.do?id=E69571|work=[[Lattes Platform|Currículo do Sistema de Currículos Lattes]]|publisher=[[Conselho Nacional de Desenvolvimento Científico e Tecnológico]]|accessdate=11 February 2012}}&lt;/ref&gt; He is currently a professor emeritus at UFRJ. The [[Journal of the Brazilian Computer Society]] dedicated a special edition in 2001 to Szwarcfiter's major publications.&lt;ref name="scielo"&gt;{{cite web |url=http://www.scielo.br/scielo.php?pid=S0104-65002001000200002&amp;script=sci_arttext |title=scielo |author= |date= |work= |publisher= |accessdate=2010-05-27}}&lt;/ref&gt; Among others, he has written joint articles with [[Donald E. Knuth]] and [[Christos Papadimitriou]].&lt;ref name="scielo" /&gt;&lt;ref name="dblp"&gt;{{cite web |url=http://www.informatik.uni-trier.de/~ley/db/indices/a-tree/s/Szwarcfiter:Jayme_Luiz.html|title=List of publications from the DBLP Bibliography Server|author= |date= |work= |publisher= |accessdate=2010-05-27}}
&lt;/ref&gt;

==Awards==
He received the Award of Scientific Merit from the Brazilian Computer Society in 2005.&lt;ref&gt;{{cite web |url=http://www.sbc.org.br/index.php?language=1&amp;subject=468  |title=Gallery of winners |author= |date= |work= |publisher= |accessdate=2010-05-27}}&lt;/ref&gt; In April 2006 he won the Almirante Álvaro Alberto prize in computer science,&lt;ref&gt;http://www.planeta.coppe.ufrj.br/artigo.php?artigo=726&lt;/ref&gt; one of the most important academic recognitions in Brazil. Szwarcfiter is also one of the recipients of the [[Ordem Nacional do Mérito Científico]] (National Order of Scientific Merit).&lt;ref&gt;{{cite web |url=http://www.mct.gov.br/index.php/content/view/11199.html |title=General List of the National Order of Scientific Merit |author= |date= |work= |publisher=Ministry of Science and Technology |accessdate=2010-05-27}}&lt;/ref&gt;
In 2011, Prof. Szwarcfiter was elected a Member of the Brazilian Academy of Sciences.

==Books==
* {{cite book |last1=Markenzon |first1=Lilian |authorlink1= |last2=Szwarcfiter |first2=Jayme Luiz |editor1-first= |editor1-last= |editor1-link= |others= |title=Estruturas de Dados e seus Algoritmos |trans-title=Data Structures and their Algorithms |url= |format= |accessdate= |type= |edition= |series= |volume= |date= |year=1997 |month= |origyear= |publisher=LTC |location=Rio de Janeiro |language=portuguese |oclc= |doi= |isbn=85-2161-014-9 |page= |pages= |at= |trans-chapter=|chapter= |chapterurl= |quote= |ref= |bibcode= |laysummary= |laydate= |separator= |postscript= |lastauthoramp=}}
* {{cite book |last1=Szwarcfiter |first1=Jayme Luiz |authorlink1= |last2=|first2=|editor1-first= |editor1-last= |editor1-link= |others= |title=Grafos e algoritmos computacionais|trans-title=Graphs and computational algorithms|url= |format= |accessdate= |type= |edition= |series= |volume= |date= |year=1988|month= |origyear= |publisher=Campus|location=Rio de Janeiro |language=portuguese |oclc= |doi= |isbn=85-7001-341-8 |page= |pages= |at= |trans-chapter=|chapter= |chapterurl= |quote= |ref= |bibcode= |laysummary= |laydate= |separator= |postscript= |lastauthoramp=}}
* {{cite book |last1=Szwarcfiter |first1=Jayme Luiz |authorlink1= |last2=|first2=|editor1-first= |editor1-last= |editor1-link= |others= |title=Teoria Computacional de Grafos |trans-title=Computational theory of Graphs|url= |format= |accessdate= |type= |edition= |series= |volume= |date= |year=2018|month= |origyear= |publisher=Elsevier|location=Rio de Janeiro |language=portuguese |oclc= |doi= |isbn=978-85-3528-884-1 |page= |pages= |at= |trans-chapter=|chapter= |chapterurl= |quote= |ref= |bibcode= |laysummary= |laydate= |separator= |postscript= |lastauthoramp=}}

==References==
{{Reflist}}

{{Authority control}}

{{DEFAULTSORT:Szwarcfiter, Jayme Luiz}}
[[Category:Brazilian computer scientists]]
[[Category:Brazilian Jews]]
[[Category:Living people]]
[[Category:Graph theorists]]
[[Category:People from Rio de Janeiro (city)]]
[[Category:1942 births]]</text>
      <sha1>6g28y0jphwcflh82iwrsfp7nzp3oipq</sha1>
    </revision>
  </page>
  <page>
    <title>Kosnita's theorem</title>
    <ns>0</ns>
    <id>44012730</id>
    <revision>
      <id>779285052</id>
      <parentid>776217906</parentid>
      <timestamp>2017-05-08T01:50:35Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.3)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4263">[[File:Kosnita points.svg|300px|right|thumb|X(54) is Kosnita point of the triangle ABC]]

In [[Euclidean geometry]], '''Kosnita's theorem''' is a property of certain [[circle]]s associated with an arbitrary [[triangle]].

Let &lt;math&gt;ABC&lt;/math&gt; be an arbitrary triangle, &lt;math&gt;O&lt;/math&gt; its [[circumcenter]] and &lt;math&gt;O_a,O_b,O_c&lt;/math&gt; are the circumcenters of three triangles &lt;math&gt;OBC&lt;/math&gt;, &lt;math&gt;OCA&lt;/math&gt;, and &lt;math&gt;OAB&lt;/math&gt; respectively.  The theorem claims that the three [[straight line]]s &lt;math&gt;AO_a&lt;/math&gt;, &lt;math&gt;BO_b&lt;/math&gt;, and &lt;math&gt;CO_c&lt;/math&gt; are concurrent.&lt;ref name=wolframKosnita/&gt; This result was established by the Romanian mathematician [[Cezar Co&amp;#351;ni&amp;#355;&amp;#259;]] (1910-1962).&lt;ref name=patrascu/&gt;

Their point of concurrence is known as the triangle's '''Kosnita point''' (named by Rigby in 1997).  It is the [[isogonal conjugate]] of the [[nine-point center]].&lt;ref name=grinberg2003/&gt;&lt;ref name=rigby1997/&gt; It is [[triangle center]] &lt;math&gt;X(54)&lt;/math&gt; in [[Kimberling center|Clark Kimberling's list]].&lt;ref name=kimberlingX54/&gt; This theorem is special case of [[Dao's theorem on six circumcenters]] associated with a cyclic hexagon in.&lt;ref name=dergiadesDao6CCCH/&gt;&lt;ref name=cohlDao6CCCH/&gt;&lt;ref name=NgoQuangDuong /&gt;&lt;ref name=C.Kimberling/&gt;&lt;ref&gt;Nguyễn Minh Hà, ''[http://geometry-math-journal.ro/pdf/Volume6-Issue1/4.pdf Another Purely Synthetic Proof of Dao's Theorem on Sixcircumcenters]''.  Journal of Advanced Research on Classical and Modern Geometries,  {{ISSN|2284-5569}}, volume 6, pages 37–44. {{MR|....}}&lt;/ref&gt;&lt;ref&gt;Nguyễn Tiến Dũng, ''[http://geometry-math-journal.ro/pdf/Volume6-Issue1/6.pdf A Simple proof of Dao's Theorem on Sixcircumcenters]''.  Journal of Advanced Research on Classical and Modern Geometries, {{ISSN|2284-5569}}, volume 6, pages 58–61. {{MR|....}}&lt;/ref&gt;&lt;ref&gt;[http://www.journal-1.eu/2016-3/Nguyen-Ngoc-Giang-The-extension-pp.21-32.pdf The extension from a circle to a conic having center: The creative method of new theorems], International Journal of Computer Discovered Mathematics, pp.21-32.&lt;/ref&gt;

== References ==
&lt;references&gt;
&lt;ref name=patrascu&gt;Ion P&amp;#259;tra&amp;#351;cu (2010), ''[http://recreatiimatematice.ro/arhiva/processed/22010/14_22010_RM22010.pdf A generalization of Kosnita's theorem]'' (in Romanian)&lt;/ref&gt;

&lt;ref name=rigby1997&gt;John Rigby (1997), ''Brief notes on some forgotten geometrical theorems.'' Mathematics and Informatics Quarterly, volume 7, pages 156-158 (as cited by Kimberling).&lt;/ref&gt;

&lt;ref name=grinberg2003&gt;Darij Grinberg (2003), ''[http://forumgeom.fau.edu/FG2003volume3/FG200311.pdf On the Kosnita Point and the Reflection Triangle].'' [[Forum Geometricorum]], volume 3, pages 105–111. {{ISSN|1534-1178}}&lt;/ref&gt;

&lt;ref name=kimberlingX54&gt;Clark Kimberling (2014), ''[http://faculty.evansville.edu/ck6/encyclopedia/ETC.html#X54 Encyclopedia of Triangle Centers] {{webarchive|url=https://web.archive.org/web/20120419171900/http://faculty.evansville.edu/ck6/encyclopedia/ETC.html |date=2012-04-19 }}'', section ''X(54) = Kosnita Point''.  Accessed on 2014-10-08&lt;/ref&gt;

&lt;ref name=dergiadesDao6CCCH&gt;Nikolaos Dergiades (2014), ''[http://forumgeom.fau.edu/FG2014volume14/FG201424.pdf Dao’s Theorem on Six Circumcenters associated with a Cyclic Hexagon].'' [[Forum Geometricorum]], volume 14, pages=243–246. {{ISSN|1534-1178}}.&lt;/ref&gt;

&lt;ref name=cohlDao6CCCH&gt;Telv Cohl (2014), ''[http://forumgeom.fau.edu/FG2014volume14/FG201429index.html A purely synthetic proof of Dao's theorem on six circumcenters associated with a cyclic hexagon].'' [[Forum Geometricorum]], volume 14, pages 261–264. {{ISSN|1534-1178}}.&lt;/ref&gt;

&lt;ref name=NgoQuangDuong&gt;[http://www.journal-1.eu/2016-2/Ngo-Quang-Duong-Dao-theorem-pp.40-47.pdf Ngo Quang Duong, International Journal of Computer Discovered Mathematics, Some problems around the Dao's theorem on six circumcenters associated with a cyclic hexagon configuration], volume 1, pages=25-39. {{ISSN|2367-7775}}&lt;/ref&gt;

&lt;ref name=C.Kimberling&gt;Clark Kimberling (2014), [http://faculty.evansville.edu/ck6/encyclopedia/ETCPart3.html#X3649 X(3649) = KS(INTOUCH TRIANGLE)]&lt;/ref&gt;

&lt;ref name=wolframKosnita&gt;{{mathworld|id=KosnitaTheorem|title=Kosnita Theorem}}&lt;/ref&gt;
&lt;/references&gt;

[[Category:Euclidean geometry]]
[[Category:Theorems in geometry]]</text>
      <sha1>ki3ilmuzejd7tr1dmtihoglh9gfovmt</sha1>
    </revision>
  </page>
  <page>
    <title>Lane (hash function)</title>
    <ns>0</ns>
    <id>20395096</id>
    <revision>
      <id>482744821</id>
      <parentid>277595904</parentid>
      <timestamp>2012-03-19T16:27:19Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>/* External links */[[WP:CHECKWIKI]] error  fixes + [[WP:GENFIXES|general fixes]] using [[Project:AWB|AWB]] (8024)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="690">{{refimprove|date=December 2008}}
'''Lane''' is a  [[cryptographic hash function]] submitted to the [[NIST hash function competition]]; it was designed by Sebastiaan Indesteege with contributions by Elena Andreeva, Christophe De Cannière, Orr Dunkelman, Emilia Käsper, Svetla Nikova, [[Bart Preneel]] and Elmar Tischhauser.  It re-uses many components from [[Advanced Encryption Standard|AES]] in a custom construction.  The authors claim performance of up to 25.66 [[cycles per byte]] on an Intel Core 2 Duo.

== External links ==
* [http://www.cosic.esat.kuleuven.be/lane/ The Lane web site]

{{Cryptography navbox | hash}}

[[Category:NIST hash function competition]]


{{crypto-stub}}</text>
      <sha1>kxup20kzm1yn97en7hfgadrwk2df011</sha1>
    </revision>
  </page>
  <page>
    <title>Linear-feedback shift register</title>
    <ns>0</ns>
    <id>58992</id>
    <revision>
      <id>863159640</id>
      <parentid>863159373</parentid>
      <timestamp>2018-10-09T02:21:17Z</timestamp>
      <contributor>
        <username>Zephyrtronium</username>
        <id>7658157</id>
      </contributor>
      <minor/>
      <comment>/* Matrix forms */ reworded section intro paragraph since we aren't using all of the same definitions anymore</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="28950">{{Redirect|LFSR|the airport using that ICAO code|Reims – Champagne Air Base}}
{{Multiple issues|
{{Refimprove|date=March 2009}}
{{More footnotes|date=April 2009}}
}}

In [[computing]], a '''linear-feedback shift register''' ('''LFSR''') is a [[shift register]] whose input bit is a [[Linear#Boolean functions|linear function]] of its previous state.

The most commonly used linear function of single bits is [[exclusive-or]] (XOR). Thus, an LFSR is most often a shift register whose input bit is driven by the XOR of some bits of the overall shift register value.

The initial value of the LFSR is called the seed, and because the operation of the register is deterministic, the stream of values produced by the register is completely determined by its current (or previous) state. Likewise, because the register has a finite number of possible states, it must eventually enter a repeating cycle. However, an LFSR with a [[Primitive polynomial (field theory)|well-chosen feedback function]] can produce a sequence of bits that appears random and has a [[Maximal length sequence|very long cycle]].

Applications of LFSRs include generating [[Pseudorandomness|pseudo-random numbers]], [[Pseudorandom noise|pseudo-noise sequences]], fast digital counters, and [[whitening sequences]]. Both hardware and software implementations of LFSRs are common.

The mathematics of a [[cyclic redundancy check]], used to provide a quick check against transmission errors, are closely related to those of an LFSR.&lt;ref&gt;{{Cite web|url=https://www.ti.com/lit/an/spra530/spra530.pdf|title=Cyclic Redundancy Check Computation: An Implementation Using the TMS320C54x|last=Geremia|first=Patrick|date=|website=|publisher=Texas Instruments|page=6|access-date=October 16, 2016}}&lt;/ref&gt;

== Fibonacci LFSRs ==
[[File:LFSR-F16.svg|thumb|right|351 px|A 16-bit [[Fibonacci]] LFSR. The feedback tap numbers shown correspond to a primitive polynomial in the table, so the register cycles through the maximum number of 65535 states excluding the all-zeroes state. The state shown, 0xACE1 ([[hexadecimal]]) will be followed by 0x5670. ]]
The bit positions that affect the next state are called the taps. In the diagram the taps are [16,14,13,11]. The rightmost bit of the LFSR is called the output bit. The taps are XOR'd sequentially with the output bit and then fed back into the leftmost bit. The sequence of bits in the rightmost position is called the output stream.
* The bits in the LFSR state that influence the input are called ''taps''.
* A maximum-length LFSR produces an [[maximum length sequence|m-sequence]] (i.e., it cycles through all possible 2&lt;sup&gt;''m''&lt;/sup&gt;&amp;nbsp;−&amp;nbsp;1 states within the shift register except the state where all bits are zero), unless it contains all zeros, in which case it will never change.
* As an alternative to the XOR-based feedback in an LFSR, one can also use [[XNOR]].&lt;ref&gt;[http://www.xilinx.com/support/documentation/application_notes/xapp210.pdf Linear Feedback Shift Registers in Virtex Devices]&lt;/ref&gt; This function is an [[affine transformation|affine map]], not strictly a [[linear map]], but it results in an equivalent polynomial counter whose state is the complement of the state of an LFSR. A state with all ones is illegal when using an XNOR feedback, in the same way as a state with all zeroes is illegal when using XOR. This state is considered illegal because the counter would remain "locked-up" in this state.

The sequence of numbers generated by an LFSR or its XNOR counterpart can be considered a [[binary numeral system]] just as valid as [[Gray code]] or the natural binary code.
&lt;!-- perhaps this statement should be moved to the [[binary numeral system]] article ? --&gt;

The arrangement of taps for feedback in an LFSR can be expressed in [[finite field arithmetic]] as a [[polynomial]] [[modular arithmetic|mod]] 2. This means that the coefficients of the polynomial must be 1s or 0s. This is called the feedback polynomial or reciprocal characteristic polynomial. For example, if the taps are at the 16th, 14th, 13th and 11th bits (as shown), the feedback polynomial is

:&lt;math&gt;x^{16} + x^{14} + x^{13} + x^{11} + 1.&lt;/math&gt;

The "one" in the polynomial does not correspond to a tap – it corresponds to the input to the first bit (i.e. ''x''&lt;sup&gt;0&lt;/sup&gt;, which is equivalent to 1). The powers of the terms represent the tapped bits, counting from the left. The first and last bits are always connected as an input and output tap respectively.

The LFSR is maximal-length if and only if the corresponding feedback polynomial is [[primitive polynomial (field theory)|primitive]]. This means that the following conditions are necessary (but not sufficient):
* The number of taps is [[Even and odd numbers|even]].
* The set of taps is [[coprime integers#Coprimality in sets|setwise co-prime]]; i.e., there must be no divisor other than 1 common to all taps.

Tables of primitive polynomials from which maximum-length LFSRs can be constructed are given below and in the references.

There can be more than one maximum-length tap sequence for a given LFSR length. Also, once one maximum-length tap sequence has been found, another automatically follows. If the tap sequence in an ''n''-bit LFSR is {{nobr|[''n'', ''A'', ''B'', ''C'', 0]}}, where the 0 corresponds to the ''x''&lt;sup&gt;0&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;1 term, then the corresponding "mirror" sequence is {{nobr|[''n'', ''n'' − ''C'', ''n'' − ''B'', ''n'' − ''A'', 0]}}. So the tap sequence {{nobr|[32, 7, 3, 2, 0]}} has as its counterpart {{nobr|[32, 30, 29, 25, 0]}}. Both give a maximum-length sequence.

An example in [[C_(programming_language)|C]] is below:

&lt;source lang="c"&gt;
# include &lt;stdint.h&gt;

int main(void)
{
    uint16_t start_state = 0xACE1u;  /* Any nonzero start state will work. */
    uint16_t lfsr = start_state;
    uint16_t bit;                    /* Must be 16bit to allow bit&lt;&lt;15 later in the code */
    unsigned period = 0;

    do
    {
        /* taps: 16 14 13 11; feedback polynomial: x^16 + x^14 + x^13 + x^11 + 1 */
        bit  = ((lfsr &gt;&gt; 0) ^ (lfsr &gt;&gt; 2) ^ (lfsr &gt;&gt; 3) ^ (lfsr &gt;&gt; 5) ) &amp; 1;
        lfsr =  (lfsr &gt;&gt; 1) | (bit &lt;&lt; 15);
        ++period;
    } while (lfsr != start_state);

    return 0;
}
&lt;/source&gt;

If a fast [[parity function|parity]] or [[popcount]] operation is available, the feedback bit can be computed more efficiently as &lt;code&gt;bit = parity(lfsr &amp; 0x002Du)&lt;/code&gt; or &lt;code&gt;bit = popcnt(lfsr &amp; 0x002Du) &amp; 1&lt;/code&gt;, effectively computing the [[dot product]] of the register with the characteristic polynomial.

This LFSR configuration is also known as '''standard''', '''many-to-one''' or '''external XOR gates'''. The alternative Galois configuration is described in the next section.

== Galois LFSRs ==
[[File:LFSR-G16.svg|thumb|right|393 px|A 16-bit Galois LFSR. The register numbers above correspond to the same primitive polynomial as the Fibonacci example but are counted in reverse to the shifting direction. This register also cycles through the maximal number of 65535 states excluding the all-zeroes state. The state ACE1 hex shown will be followed by E270 hex.]]

Named after the French mathematician [[Évariste Galois]], an LFSR in Galois configuration, which is also known as '''modular''', '''internal XORs''', or '''one-to-many LFSR''', is an alternate structure that can generate the same output stream as a conventional LFSR (but offset in time).&lt;ref&gt;
{{cite book
  |last1 = Press
  |first1 = William
  |last2 = Teukolsky
  |first2 = Saul
  |last3 = Vetterling
  |first3 = William
  |last4 = Flannery
  |first4 = Brian
  |title = Numerical Recipes: The Art of Scientific Computing, Third Edition
  |publisher = [[Cambridge University Press]]
  |year = 2007
  |page = 386
  |isbn = 978-0-521-88407-5
}}
&lt;/ref&gt; In the Galois configuration, when the system is clocked, bits that are not taps are shifted one position to the right unchanged. The taps, on the other hand, are XORed with the output bit before they are stored in the next position. The new output bit is the next input bit. The effect of this is that when the output bit is zero, all the bits in the register shift to the right unchanged, and the input bit becomes zero. When the output bit is one, the bits in the tap positions all flip (if they are 0, they become 1, and if they are 1, they become 0), and then the entire register is shifted to the right and the input bit becomes 1.

To generate the same output stream, the order of the taps is the ''counterpart'' (see above) of the order for the conventional LFSR, otherwise the stream will be in reverse. Note that the internal state of the LFSR is not necessarily the same. The Galois register shown has the same output stream as the Fibonacci register in the first section. A time offset exists between the streams, so a different startpoint will be needed to get the same output each cycle.
* Galois LFSRs do not concatenate every tap to produce the new input (the XORing is done within the LFSR, and no XOR gates are run in serial, therefore the propagation times are reduced to that of one XOR rather than a whole chain), thus it is possible for each tap to be computed in parallel, increasing the speed of execution.
* In a software implementation of an LFSR, the Galois form is more efficient, as the XOR operations can be implemented a word at a time: only the output bit must be examined individually.

Below is a [[C (programming language)|C]] code example for the 16-bit maximal-period Galois LFSR example in the figure:
&lt;source lang="c"&gt;
# include &lt;stdint.h&gt;
int main(void) {
    uint16_t start_state = 0xACE1u;  /* Any nonzero start state will work. */
    uint16_t lfsr = start_state;
    unsigned period = 0;

    do {
        unsigned lsb = lfsr &amp; 1;   /* Get LSB (i.e., the output bit). */
        lfsr &gt;&gt;= 1;                /* Shift register */
        if (lsb)                   /* If the output bit is 1, apply toggle mask. */
            lfsr ^= 0xB400u;
        ++period;
    } while (lfsr != start_state);

    return 0;
}
&lt;/source&gt;

Note that
&lt;source lang="c"&gt;
        if (lsb)
            lfsr ^= 0xB400u;
&lt;/source&gt;
can also be written as
&lt;source lang="c"&gt;
        lfsr ^= (-lsb) &amp; 0xB400u;
&lt;/source&gt;
which may produce more efficient code on some compilers.
&lt;!-- NOTE: The C standard guarantees that arithmetic operations on unsigned types are computed modulo 2^bitsize (i.e., as if in two's complement arithmetic). Thus, the "-lsb" is fully portable and gives the intended result even if the target environment uses natively a different integer representation. --&gt;

=== Non-binary Galois LFSR ===
Binary Galois LFSRs like the ones shown above can be generalized to any ''q''-ary alphabet {0, 1, ..., ''q''&amp;nbsp;−&amp;nbsp;1} (e.g., for binary, ''q'' = 2, and the alphabet is simply {0, 1}). In this case, the exclusive-or component is generalized to addition [[Modular arithmetic|modulo]]-''q'' (note that XOR is addition modulo 2), and the feedback bit (output bit) is multiplied (modulo-''q'') by a ''q''-ary value, which is constant for each specific tap point. Note that this is also a generalization of the binary case, where the feedback is multiplied by either 0 (no feedback, i.e., no tap) or 1 (feedback is present). Given an appropriate tap configuration, such LFSRs can be used to generate [[Finite field|Galois fields]] for arbitrary prime values of ''q''.

== Matrix forms ==
Binary LFSRs of both Fibonacci and Galois configurations can be expressed as linear functions using matrices in &lt;math&gt;\mathbb{F}_2&lt;/math&gt;.&lt;ref&gt;{{Cite book|title=Stream Ciphers|chapter=Linear Feedback Shift Registers|last=Klein|first=A.|year=2013|pages=17-18|publisher=Springer|location=London|doi=10.1007/978-1-4471-5079-4_2|isbn=978-1-4471-5079-4|chapter-url=https://pdfs.semanticscholar.org/0488/8883afd08ffbc4b6e8eb4d3c2f9d2182adae.pdf}}&lt;/ref&gt; Using the [[companion matrix]] of the characteristic polynomial of the LFSR and denoting the seed as a column vector &lt;math&gt;(a_0, a_1, \dots, a_{n-1})^\mathrm{T}&lt;/math&gt;, the state of the register in Fibonacci configuration after &lt;math&gt;k&lt;/math&gt; steps is given by

:&lt;math&gt;\begin{pmatrix} a_{k} \\ a_{k+1} \\ a_{k+2} \\ \vdots \\ a_{k+n-1} \end{pmatrix} =
\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ c_{0} &amp; c_{1} &amp; c_{2} &amp; \cdots &amp; c_{n-1} \end{pmatrix}
\begin{pmatrix} a_{k-1} \\ a_{k} \\ a_{k+1} \\ \vdots \\ a_{k+n-2} \end{pmatrix} =
\begin{pmatrix} 0 &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\ 0 &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ c_{0} &amp; c_{1} &amp; c_{2} &amp; \cdots &amp; c_{n-1} \end{pmatrix}^k
\begin{pmatrix} a_0 \\ a_1 \\ a_2 \\ \vdots \\ a_{n-1} \end{pmatrix}&lt;/math&gt;

For the Galois form, we have

:&lt;math&gt;\begin{pmatrix} a_{k} \\ a_{k+1} \\ a_{k+2} \\ \vdots \\ a_{k+n-1} \end{pmatrix} =
\begin{pmatrix} c_{0} &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\ c_{1} &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ c_{n-1} &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \end{pmatrix}
\begin{pmatrix} a_{k-1} \\ a_{k} \\ a_{k+1} \\ \vdots \\ a_{k+n-2} \end{pmatrix} =
\begin{pmatrix} c_{0} &amp; 1 &amp; 0 &amp; \cdots &amp; 0 \\ c_{1} &amp; 0 &amp; 1 &amp; \cdots &amp; 0 \\ \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\ c_{n-1} &amp; 0 &amp; 0 &amp; \cdots &amp; 0 \end{pmatrix}^k
\begin{pmatrix} a_0 \\ a_1 \\ a_2 \\ \vdots \\ a_{n-1} \end{pmatrix}&lt;/math&gt;

These forms generalize naturally to arbitrary fields.

== Some polynomials for maximal LFSRs ==
The following table lists maximal-length polynomials for shift-register lengths up to 24.  Note that more than one maximal-length polynomial may exist for any given shift-register length. A list of alternative maximal-length polynomials for shift-register lengths 4–32 (beyond which it becomes unfeasible to store or transfer them) can be found here: http://www.ece.cmu.edu/~koopman/lfsr/index.html.
&lt;!-- Since alternative polynomials are possible I have verified independently those tabulated here. ~~~~ --&gt;
{|class="wikitable" style="text-align:right"
|-
!Bits (n)
!Feedback polynomial
!Period (&lt;math&gt;2^n - 1&lt;/math&gt;)
|-
! 2
|&lt;math&gt;x^2 + x + 1&lt;/math&gt;
| 3
|-
! 3
|&lt;math&gt;x^3 + x^2 + 1&lt;/math&gt;
| 7
|-
! 4
|&lt;math&gt;x^4 + x^3 + 1&lt;/math&gt;
| 15
|-
! 5
|&lt;math&gt;x^{ 5 }+x^{ 3 }+1&lt;/math&gt;
| 31
|-
! 6
|&lt;math&gt;x^{ 6 }+x^{ 5 }+1&lt;/math&gt;
| 63
|-
! 7
|&lt;math&gt;x^{ 7 }+x^{ 6 }+1&lt;/math&gt;
| 127
|-
! 8
|&lt;math&gt;x^{ 8 }+x^{ 6 }+x^{ 5 }+x^{ 4 }+1&lt;/math&gt;
| 255
|-
! 9
|&lt;math&gt;x^{ 9 }+x^{ 5 }+1&lt;/math&gt;
| 511
|-
! 10
|&lt;math&gt;x^{ 10 }+x^{ 7 }+1&lt;/math&gt;
| 1,023
|-
! 11
|&lt;math&gt;x^{ 11 }+x^{ 9 }+1&lt;/math&gt;
| 2,047
|-
! 12
|&lt;math&gt;x^{ 12 }+x^{ 11 }+x^{ 10 }+x^{ 4 }+1&lt;/math&gt;
| 4,095
|-
! 13
|&lt;math&gt;x^{ 13 }+x^{ 12 }+x^{ 11 }+x^{ 8 }+1&lt;/math&gt;
| 8,191
|-
! 14
|&lt;math&gt;x^{ 14 }+x^{ 13 }+x^{ 12 }+x^{ 2 }+1&lt;/math&gt;
| 16,383
|-
! 15
|&lt;math&gt;x^{ 15 }+x^{ 14 }+1&lt;/math&gt;
| 32,767
|-
! 16
|&lt;math&gt;x^{ 16 }+x^{ 15 }+x^{ 13 }+x^{ 4 }+1&lt;/math&gt;
| 65,535
|-
! 17
|&lt;math&gt;x^{ 17 }+x^{ 14 }+1&lt;/math&gt;
| 131,071
|-
! 18
|&lt;math&gt;x^{ 18 }+x^{ 11 }+1&lt;/math&gt;
| 262,143

|-
! 19
|&lt;math&gt;x^{ 19 }+x^{ 18 }+x^{ 17 }+x^{ 14 }+1&lt;/math&gt;
| 524,287
|-
! 20
|&lt;math&gt;x^{ 20 }+x^{ 17 }+1&lt;/math&gt;
| 1,048,575
|-
! 21
|&lt;math&gt;x^{ 21 }+x^{ 19 }+1&lt;/math&gt;
| 2,097,151
|-
! 22
|&lt;math&gt;x^{ 22 }+x^{ 21 }+1&lt;/math&gt;
| 4,194,303
|-
! 23
|&lt;math&gt;x^{ 23 }+x^{ 18 }+1&lt;/math&gt;
| 8,388,607
|-
! 24
|&lt;math&gt;x^{ 24 }+x^{ 23 }+x^{ 22 }+x^{ 17 }+1&lt;/math&gt;
| 16,777,215
|-
!3–168
|[http://www.xilinx.com/support/documentation/application_notes/xapp052.pdf]
|
|-
!2–786,&lt;br&gt;1024,&lt;br&gt;2048,&lt;br&gt;4096
|[https://web.archive.org/web/20161007061934/http://courses.cse.tamu.edu/csce680/walker/lfsr_table.pdf]
|
|}

== Output-stream properties ==
* Ones and zeroes occur in "runs". The output stream 1110010, for example, consists of four runs of lengths 3, 2, 1, 1, in order. In one period of a maximal LFSR, 2&lt;sup&gt;''n''−1&lt;/sup&gt; runs occur (in the example above, the 3-bit LFSR has 4 runs). Exactly half of these runs are one bit long, a quarter are two bits long, up to a single run of zeroes ''n''&amp;nbsp;−&amp;nbsp;1 bits long, and a single run of ones ''n'' bits long. This distribution almost equals the statistical [[Expected value|expectation value]] for a truly random sequence. However, the probability of finding exactly this distribution in a sample of a truly random sequence is rather low{{vague|date=April 2013}}.
* LFSR output streams are [[deterministic]]. If the present state and the positions of the XOR gates in the LFSR are known, the next state can be predicted.&lt;ref name="xilinx.com"&gt;http://www.xilinx.com/support/documentation/application_notes/xapp052.pdf&lt;/ref&gt; This is not possible with truly random events. With maximal-length LFSRs, it is much easier to compute the next state, as there are only an easily limited number of them for each length.
* The output stream is reversible; an LFSR with mirrored taps will cycle through the output sequence in reverse order.
* The value consisting of all zeros cannot appear. Thus an LFSR of length ''n'' cannot be used to generate all 2&lt;sup&gt;''n''&lt;/sup&gt; values.

== Applications ==
LFSRs can be implemented in hardware, and this makes them useful in applications that require very fast generation of a pseudo-random sequence, such as [[direct-sequence spread spectrum]] radio. LFSRs have also been used for generating an approximation of [[white noise]] in various [[programmable sound generator]]s.

=== Uses as counters ===
The repeating sequence of states of an LFSR allows it to be used as a [[clock divider]] or as a counter when a non-binary sequence is acceptable, as is often the case where computer index or framing locations need to be machine-readable.&lt;ref name="xilinx.com"/&gt; LFSR [[Counter (digital)|counter]]s have simpler feedback logic than natural binary counters or [[Gray-code counter]]s, and therefore can operate at higher clock rates. However, it is necessary to ensure that the LFSR never enters an all-zeros state, for example by presetting it at start-up to any other state in the sequence.
The table of primitive polynomials shows how LFSRs can be arranged in Fibonacci or Galois form to give maximal periods. One can obtain any other period by adding to an LFSR that has a longer period some logic that shortens the sequence by skipping some states.

=== Uses in cryptography ===
LFSRs have long been used as [[pseudo-random number generator]]s for use in [[stream cipher]]s (especially in [[military]] [[cryptography]]), due to the ease of construction from simple [[electromechanical]] or [[electronic circuits]], long [[periodic function|periods]], and very uniformly [[probability distribution|distributed]] output streams. However, an LFSR is a linear system, leading to fairly easy [[cryptanalysis]]. For example, given a stretch of [[known-plaintext attack|known plaintext and corresponding ciphertext]], an attacker can intercept and recover a stretch of LFSR output stream used in the system described, and from that stretch of the output stream can construct an LFSR of minimal size that simulates the intended receiver by using the [[Berlekamp-Massey algorithm]]. This LFSR can then be fed the intercepted stretch of output stream to recover the remaining plaintext.

Three general methods are employed to reduce this problem in LFSR-based stream ciphers:
* [[Non-linear]] combination of several [[bit]]s from the LFSR [[state (computer science)|state]];
* Non-linear combination of the output bits of two or more LFSRs (see also: [[shrinking generator]]); or using [[Evolutionary algorithm]] to introduce non-linearity.&lt;ref&gt;A. Poorghanad, A. Sadr, A. Kashanipour" Generating High Quality Pseudo Random Number Using Evolutionary Methods", IEEE Congress on Computational Intelligence and Security, vol. 9, pp. 331-335 , May,2008 [http://www.computer.org/csdl/proceedings/cis/2008/3508/01/3508a331.pdf]&lt;/ref&gt;
* Irregular clocking of the LFSR, as in the [[alternating step generator]].

Important LFSR-based stream ciphers include [[A5/1]] and [[A5/2]], used in [[GSM]] cell phones, [[E0 (cipher)|E0]], used in [[Bluetooth]], and the [[shrinking generator]]. The A5/2 cipher has been broken and both A5/1 and E0 have serious weaknesses.&lt;ref&gt;{{Citation
 | last = Barkam
 | first = Elad
 | last2 = Biham
 | first2 = Eli
 | last3 = Keller
 | first3 = Nathan
 | title= Instant Ciphertext-Only Cryptanalysis of GSM Encrypted Communication
 | journal=Journal of Cryptology
 | volume=21
 | issue=3
 | year=2008
 | pages=392–429
 | url=http://www.cs.technion.ac.il/users/wwwb/cgi-bin/tr-get.cgi/2006/CS/CS-2006-07.pdf
 | doi=10.1007/s00145-007-9001-y
 }}&lt;/ref&gt;&lt;ref&gt;{{cite journal
 | first = Yi | last = Lu |author2=Willi Meier |author3=Serge Vaudenay
 | title =  The Conditional Correlation Attack: A Practical Attack on Bluetooth Encryption
 | journal =  [[Crypto (journal)|Crypto]] 2005 | year = 2005 | location = Santa Barbara, California, USA
 | url = http://www.terminodes.org/micsPublicationsDetail.php?pubno=1216 | volume = 3621 | pages = 97–117 | doi=10.1007/11535218_7
}}&lt;/ref&gt;

The linear feedback shift register has a strong relationship to [[linear congruential generator]]s.&lt;ref&gt;
RFC 4086
section 6.1.3 "Traditional Pseudo-random Sequences"
&lt;/ref&gt;

=== Uses in circuit testing ===
{{unsourced section|date=May 2016}}
LFSRs are used in circuit testing for test-pattern generation (for exhaustive testing, pseudo-random testing or pseudo-exhaustive testing) and for signature analysis.

==== Test-pattern generation ====
Complete LFSR are commonly used as pattern generators for exhaustive testing, since they cover all possible inputs for an ''n''-input circuit. Maximal-length LFSRs and weighted LFSRs are widely used as pseudo-random test-pattern generators for pseudo-random test applications.

==== Signature analysis ====
In [[built-in self-test]] (BIST) techniques, storing all the circuit outputs on chip is not possible, but the circuit output can be compressed to form a signature that will later be compared to the golden signature (of the good circuit) to detect faults. Since this compression is lossy, there is always a possibility that a faulty output also generates the same signature as the golden signature and the faults cannot be detected. This condition is called error masking or aliasing. BIST is accomplished with a multiple-input signature register (MISR or MSR), which is a type of LFSR. A standard LFSR has a single XOR or XNOR gate, where the input of the gate is connected to several "taps" and the output is connected to the input of the first flip-flop. A MISR has the same structure, but the input to every flip-flop is fed through an XOR/XNOR gate. For example, a 4-bit MISR has a 4-bit parallel output and a 4-bit parallel input. The input of the first flip-flop is XOR/XNORd with parallel input bit zero and the "taps". Every other flip-flop input is XOR/XNORd with the preceding flip-flop output and the corresponding parallel input bit. Consequently, the next state of the MISR depends on the last several states opposed to just the current state. Therefore, a MISR will always generate the same golden signature given that the input sequence is the same every time.

=== Uses in digital broadcasting and communications ===

==== Scrambling ====
{{ main | Scrambler }}
To prevent short repeating sequences (e.g., runs of 0s or 1s) from forming spectral lines that may complicate symbol tracking at the
receiver or interfere with other transmissions, the data bit sequence is combined with the output of a linear-feedback register before modulation and transmission. This scrambling is removed at the receiver after demodulation. When the LFSR runs at the same [[bit rate]] as the transmitted symbol stream, this technique is referred to as [[scrambler|scrambling]]. When the LFSR runs considerably faster than the symbol stream, the LFSR-generated bit sequence is called ''chipping code''. The chipping code is combined with the data using [[exclusive or]] before transmitting using [[binary phase-shift keying]] or a similar modulation method. The resulting signal has a higher bandwidth than the data, and therefore this is a method of [[spread-spectrum]] communication. When used only for the spread-spectrum property, this technique is called [[direct-sequence spread spectrum]]; when used to distinguish several signals transmitted in the same channel at the same time and frequency, it is called [[code division multiple access]].

Neither scheme should be confused with [[encryption]] or [[encipherment]]; scrambling and spreading with LFSRs do ''not'' protect the information from eavesdropping. They are instead used to produce equivalent streams that possess convenient engineering properties to allow robust and efficient modulation and demodulation.

Digital broadcasting systems that use linear-feedback registers:
* [[ATSC Standards]] (digital TV transmission system – North America)
* [[Digital Audio Broadcasting|DAB]] ([[Digital Audio Broadcasting]] system – for radio)
* [[DVB-T]] (digital TV transmission system – Europe, Australia, parts of Asia)
* [[NICAM]] (digital audio system for television)

Other digital communications systems using LFSRs:
* INTELSAT business service (IBS)
* Intermediate data rate (IDR)
* [[Serial digital interface|SDI]] (Serial Digital Interface transmission)
* Data transfer over [[PSTN]] (according to the [[ITU-T]] V-series recommendations)
* [[CDMA]] (Code Division Multiple Access) cellular telephony
* [[Fast Ethernet#100BASE-T2|100BASE-T2 "fast" Ethernet]] scrambles bits using an LFSR
* [[Gigabit Ethernet#1000BASE-T|1000BASE-T Ethernet]], the most common form of Gigabit Ethernet, scrambles bits using an LFSR
* [[PCI Express]] 3.0
* [[SATA]]&lt;ref&gt;Section 9.5 of the SATA Specification, revision 2.6&lt;/ref&gt;
* [[Serial attached SCSI]] (SAS/SPL)
* [[USB 3.0]]
* [[IEEE 802.11a]] scrambles bits using an LFSR
* [[Bluetooth Low Energy]] Link Layer is making use of LFSR (referred to as whitening)
* [[Satellite navigation system]]s such as [[GPS]] and [[GLONASS]]. All current systems use LFSR outputs to generate some or all of their ranging codes (as the chipping code for CDMA or DSSS) or to modulate the carrier without data (like GPS&amp;nbsp;L2&amp;nbsp;CL ranging code). GLONASS also uses [[frequency-division multiple access]] combined with DSSS.

==== Other uses ====
LFSRs are also used in [[radio jamming]] systems to generate pseudo-random noise to raise the noise floor of a target communication system.

The German time signal [[DCF77]], in addition to amplitude keying, employs [[phase-shift keying]] driven by a 9-stage LFSR to increase the accuracy of received time and the robustness of the data stream in the presence of noise.&lt;ref name="phasemod"&gt;{{cite conference |url=https://www.ptb.de/cms/fileadmin/internet/fachabteilungen/abteilung_4/4.4_zeit_und_frequenz/pdf/5_1988_Hetzel_-_Proc_EFTF_88.pdf |title=Time dissemination via the LF transmitter DCF77 using a pseudo-random phase-shift keying of the carrier |first=P. |last=Hetzel |date=16 March 1988 |conference=2nd European Frequency and Time Forum |location=Neuchâtel |pages=351–364 |accessdate=11 October 2011}}&lt;/ref&gt;

== See also ==
* [[Pinwheel (cryptography)|Pinwheel]]
* [[Mersenne twister]]
* [[Maximum length sequence]]
* [[Analog feedback shift register]]
* [[NLFSR]], Non-Linear Feedback Shift Register
* [[Ring counter]]
* [[Pseudo-random binary sequence]]

== References ==
{{Reflist|30em}}

== External links ==
{{Div col|colwidth=30em}}
* [http://www.newwaveinstruments.com/resources/articles/m_sequence_linear_feedback_shift_register_lfsr.htm LFSR Reference] LFSR theory and implementation, maximal length sequences, and comprehensive feedback tables for lengths from 7 to 16,777,215 (3 to 24 stages), and partial tables for lengths up to 4,294,967,295 (25 to 32 stages).
* [http://www.itu.int/rec/T-REC-O.151-199210-I/en International Telecommunications Union Recommendation O.151] (August 1992)
* [http://spreadsheets.google.com/ccc?key=0AvYtZsho-JTldFRYZnJLRFFaSWtUcVNXc1Y3M2VWd1E&amp;hl=en Maximal Length LFSR table] with length from 2 to 67.
* [http://www.maxim-ic.com/appnotes.cfm?appnote_number=1743&amp;CMP=WP-9 Pseudo-Random Number Generation Routine]
* http://www.ece.ualberta.ca/~elliott/ee552/studentAppNotes/1999f/Drivers_Ed/lfsr.html
* http://www.quadibloc.com/crypto/co040801.htm
* [https://web.archive.org/web/20060315203220/http://www.yikes.com/~ptolemy/lfsr_web/index.htm Simple explanation of LFSRs for Engineers]
* [http://www.ece.cmu.edu/~koopman/lfsr/index.html Feedback terms]
* [https://web.archive.org/web/20060111183721/http://homepage.mac.com/afj/lfsr.html General LFSR Theory]
* [http://opencores.org/project,lfsr_randgen An implementation of LFSR in VHDL.]
* [http://emmanuel.pouly.free.fr Simple VHDL coding for Galois and Fibonacci LFSR.]
* [https://bitbucket.org/gallen/mlpolygen mlpolygen: A Maximal Length polynomial generator]
{{div col end}}

{{Cryptography stream}}

[[Category:Binary arithmetic]]
[[Category:Digital registers]]
[[Category:Cryptographic algorithms]]
[[Category:Pseudorandom number generators]]
[[Category:Articles with example C code]]</text>
      <sha1>kqhgsux7qxta94tauzcoexvtqencll4</sha1>
    </revision>
  </page>
  <page>
    <title>Locally regular space</title>
    <ns>0</ns>
    <id>18263842</id>
    <revision>
      <id>588224469</id>
      <parentid>487532312</parentid>
      <timestamp>2013-12-29T16:14:36Z</timestamp>
      <contributor>
        <username>Forgetfulfunctor00</username>
        <id>20019897</id>
      </contributor>
      <comment>/* Examples and properties */ Fixed an awkward wording/ordering of the examples</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1434">{{Unreferenced|date=July 2008}}
In [[mathematics]], particularly [[topology]], a [[topological space]] ''X'' is '''locally regular''' if intuitively it looks locally like a [[regular space]]. More precisely, a locally regular space satisfies the property that each point of the space belongs to an open subset of the space that is regular under the [[subspace topology]]. 

==Formal definition==

A [[topological space]] ''X'' is said to be '''locally regular''' [[if and only if]] each point, ''x'', of ''X'' has a [[neighbourhood]] that is [[regular space|regular]] under the [[subspace topology]]. Equivalently, a space ''X'' is locally regular if and only if the collection of all open sets that are regular under the subspace topology forms a base for the topology on ''X''.

==Examples and properties==

* Every locally regular [[T0 space]] is [[Locally Hausdorff space|locally Hausdorff]].
* A regular space is always locally regular.
* A [[Locally compact space|locally compact]] [[Hausdorff space]] is regular, hence locally regular.
* A [[T1 space]] need not be locally regular as the set of all real numbers endowed with the [[cofinite topology]] shows.

==See also==

*[[Locally Hausdorff space]]
*[[Locally compact space]]
*[[Locally metrizable space]]
*[[Normal space]]
*[[Homeomorphism]]
*[[Locally normal space]]

== References ==

[[Category:Topology]]
[[Category:Properties of topological spaces]]

{{topology-stub}}</text>
      <sha1>3k9qbv25xwsmnzhpoet2kqlexpblh20</sha1>
    </revision>
  </page>
  <page>
    <title>Malliavin's absolute continuity lemma</title>
    <ns>0</ns>
    <id>13650583</id>
    <revision>
      <id>384957253</id>
      <parentid>259902912</parentid>
      <timestamp>2010-09-15T10:32:24Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>page range fixes, replaced: | pages= 195--263 → | pages= 195–263 using [[Project:AWB|AWB]] (7126)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2059">In [[mathematics]] &amp;mdash; specifically, in [[measure theory]] &amp;mdash; '''Malliavin's absolute continuity lemma''' is a result due to the [[France|French]] [[mathematician]] [[Paul Malliavin]] that plays a foundational rôle in the regularity ([[smooth function|smoothness]]) [[theorem]]s of the [[Malliavin calculus]].  Malliavin's lemma gives a sufficient condition for a [[finite measure|finite]] [[Borel measure]] to be [[absolute continuity|absolutely continuous]] with respect to [[Lebesgue measure]].

==Statement of the lemma==

Let ''&amp;mu;'' be a finite Borel measure on ''n''-[[dimension]]al [[Euclidean space]] '''R'''&lt;sup&gt;''n''&lt;/sup&gt;.  Suppose that, for every ''x''&amp;nbsp;&amp;isin;&amp;nbsp;'''R'''&lt;sup&gt;''n''&lt;/sup&gt;, there exists a constant ''C''&amp;nbsp;=&amp;nbsp;''C''(''x'') such that

:&lt;math&gt;\left| \int_{\mathbf{R}^{n}} \mathrm{D} \varphi (y) (x) \, \mathrm{d} \mu(y) \right| \leq C(x) \| \varphi \|_{\infty}&lt;/math&gt;

for every ''C''&lt;sup&gt;&amp;infin;&lt;/sup&gt; function ''φ''&amp;nbsp;:&amp;nbsp;'''R'''&lt;sup&gt;''n''&lt;/sup&gt;&amp;nbsp;&amp;rarr;&amp;nbsp;'''R''' with [[compact support]].  Then ''μ'' is absolutely continuous with respect to ''n''-dimensional Lebesgue measure ''λ''&lt;sup&gt;''n''&lt;/sup&gt; on '''R'''&lt;sup&gt;''n''&lt;/sup&gt;.  In the above, D''φ''(''y'') denotes the [[Fréchet derivative]] of ''&amp;phi;'' at ''y'' and ||''φ''||&lt;sub&gt;&amp;infin;&lt;/sub&gt; denotes the [[supremum norm]] of ''φ''.

==References==

* {{cite book
| last = Bell
| first = Denis R.
| title = The Malliavin calculus
| publisher = Dover Publications Inc.
| location = Mineola, NY
| year = 2006
| pages = x+113
| isbn = 0-486-44994-7
}} {{MathSciNet|id=2250060}} (See section 1.3)
* {{cite book
| last = Malliavin
| first = Paul
| authorlink = Paul Malliavin
| chapter = Stochastic calculus of variations and hypoelliptic operators
| title = Proceedings of the International Symposium on Stochastic Differential Equations (Res. Inst. Math. Sci., Kyoto Univ., Kyoto, 1976)
| pages= 195–263
| publisher = Wiley
| location = New York
| year = 1978
}} {{MathSciNet|id=536013}}

[[Category:Lemmas]]
[[Category:Measure theory]]</text>
      <sha1>bygbvfrysx8eo6hyngz0md2e9rk732c</sha1>
    </revision>
  </page>
  <page>
    <title>Material inference</title>
    <ns>0</ns>
    <id>40203396</id>
    <revision>
      <id>867971587</id>
      <parentid>828780019</parentid>
      <timestamp>2018-11-09T04:21:36Z</timestamp>
      <contributor>
        <username>Headbomb</username>
        <id>1461430</id>
      </contributor>
      <comment>ce</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5685">{{more footnotes|date=January 2014}}

In [[logic]], [[inference]] is the process of deriving logical conclusions from premises known or assumed to be true. In checking a logical inference for '''formal'''&lt;!--'Formal inference' redirects here; boldface per WP:R#PLA--&gt; and '''material''' validity, the meaning of only its logical vocabulary and of both its logical and extra-logical vocabulary{{clarify|reason=The distinction between logical and extra-logical vocabulary should be explained. In a first approach, a notion is called 'logical' if it applies to sentences only. E.g. the connective 'x and y' joins two sentences x, y and hence is logical vocabulary, while 'x is human' applies to a real-world object x and hence is extra-logical. However, a quantifier like 'each x satisfies y' is usually considered as logical vocabulary although it applies to a real-world object x and a sentence y.|date=August 2013}}
is considered, respectively.

==Examples==
For example, the inference "''Socrates is a human, and each human must eventually die, therefore Socrates must eventually die''" is a formally valid inference; it remains valid if the nonlogical vocabulary "''Socrates''", "''is human''", and "''must eventually die''" is arbitrarily, but consistently replaced.&lt;ref group="note"&gt;A completely fictitious, but formally valid inference obtained by consistent replacement is e.g. "''Buckbeak is a unicorn, and each unicorn has gills, therefore Buckbeak has gills''".&lt;/ref&gt;

In contrast, the inference "''Montreal is north of New York, therefore New York is south of Montreal''" is materially valid only; its validity relies on the extra-logical relations "''is north of''" and "''is south of''" being converse to each other.&lt;ref group="note"&gt;A completely fictitious, but materially (and formally) '''in'''valid inference obtained by consistent replacement is e.g. "''Hagrid is younger than Albus, therefore Albus is larger than Hagrid''". Consistent replacement doesn't respect conversity.&lt;/ref&gt;

==Material inferences vs. enthymemes==
Classical formal logic considers the above "north/south" inference as an [[enthymeme]], that is, as an incomplete inference; it can be made formally valid by supplementing the tacitly used conversity relationship explicitly: "''Montreal is north of New York, and whenever a location x is north of a location y, then y is south of x; therefore New York is south of Montreal''".

In contrast, the notion of a '''material inference''' has been developed by [[Wilfrid Sellars]]&lt;ref&gt;{{cite book| author=[[Wilfrid Sellars]]| title=Inference and Meaning| year=1980| pages=261f| publisher=| editor=J. Sicha}}&lt;/ref&gt; in order to emphasize his view that such supplements are not necessary to obtain a correct argument.

==Brandom on material inference==
===Non-monotonic inference===
[[Robert Brandom]] adopted Sellars' view,&lt;ref&gt;{{cite book| author=[[Robert Brandom]]| title=Articulating Reasons: An Introduction to Inferentialism| year=2000| publisher=Harvard University Press| isbn=0-674-00158-3}}; Sect. 2.III-IV&lt;/ref&gt; arguing that everyday (practical) reasoning is usually [[nonmonotonic logic|non-monotonic]], i.e. additional premises can turn a practically valid inference into an invalid one, e.g.
# "If I rub this [[match]] along the striking surface, then it will ignite." (''p''→''q'')
# "If ''p'', but the match is inside a strong [[electromagnetic field]], then it will not ignite." (''p''∧''r''→¬''q'')
# "If ''p'' and ''r'', but the match is in a [[Faraday cage]], then it will ignite." (''p''∧''r''∧''s''→''q'')
# "If ''p'' and ''r'' and ''s'', but there is no [[oxygen]] in the room, then the match will not ignite." (''p''∧''r''∧''s''∧''t''→¬''q'')
# ...
Therefore, practically valid inference is different from formally valid inference (which is monotonic - the above argument that ''Socrates must eventually die'' cannot be challenged by whatever additional information), and should better be modelled by materially valid inference. While a classical logician could add a [[ceteris paribus]] clause to 1. to make it usable in formally valid inferences:
# "If I rub this match along the striking surface, then, ceteris paribus,&lt;ref group="note"&gt;[[ceteris paribus|literally]]: "''all other things being equal''"; here: "''assuming a typical situation''"&lt;/ref&gt; it will inflame."
However, Brandom doubts that the meaning of such a clause can be made explicit, and prefers to consider it as a hint to non-monotony rather than a miracle drug to establish monotony.

Moreover, the "match" example shows that a typical everyday inference can hardly be ever made formally complete. In a similar way, [[Lewis Carroll]]'s dialogue "''[[What the Tortoise Said to Achilles]]''" demonstrates that the attempt to make every inference fully complete can lead to an infinite regression.&lt;ref&gt;{{cite journal |author=Carroll, Lewis |title=What the Tortoise Said to Achilles |journal=Mind |series=New Series |volume=4 |issue=14 |date=Apr 1895 |pages=278–280 |url=http://courseweb.stthomas.edu/kwkemp/logic/R/Tortoise.pdf}}&lt;/ref&gt;

== See also==
Material inference should not be confused with the following concepts, which refer to ''formal'', not '''material''' validity:
* [[Material conditional]] &amp;mdash; the logical connective "→" (i.e. "formally implies") 
* [[Material implication (rule of inference)]] &amp;mdash; a rule for formally replacing "→" by "¬" (negation) and "∨" (disjunction)

==Notes==
{{reflist|group="note"}}

==Citations==
{{reflist}}

==References==
* [http://plato.stanford.edu/entries/sellars#3.1 Stanford Encyclopedia of Philosophy on Sellars view]

[[Category:Non-classical logic]]
[[Category:Inference]]</text>
      <sha1>a2u9s18usen5knr9878t7cy2cjt65l3</sha1>
    </revision>
  </page>
  <page>
    <title>Maze solving algorithm</title>
    <ns>0</ns>
    <id>22074859</id>
    <revision>
      <id>837302427</id>
      <parentid>837302368</parentid>
      <timestamp>2018-04-20T00:02:46Z</timestamp>
      <contributor>
        <ip>147.174.133.95</ip>
      </contributor>
      <comment>/* Dead-end filling */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17461">[[File:Cyclope robot.jpg|thumb|right|Robot in a wooden maze]]
There are a number of different '''maze solving [[algorithm]]s''', that is, automated methods for the solving of [[maze]]s. The random mouse, wall follower, Pledge, and Trémaux's [[algorithms]] are designed to be used inside the maze by a traveler with no prior knowledge of the maze, whereas the [[Cul-de-sac|dead-end]] filling and [[shortest path algorithm]]s are designed to be used by a person or computer program that can see the whole maze at once.

Mazes containing no loops are known as "simply connected", or "perfect" mazes, and are equivalent to a [[Tree (graph theory)|''tree'']] in graph theory. Thus many maze solving algorithms are closely related to [[graph theory]]. Intuitively, if one pulled and stretched out the paths in the maze in the proper way, the result could be made to resemble a tree.&lt;ref&gt;{{youtube|k1tSK5V1pds|Maze to Tree}}&lt;/ref&gt;

== Random mouse algorithm  ==
This is a trivial method that can be implemented by a very unintelligent [[robot]] or perhaps a mouse. It is simply to proceed following the current passage until a junction is reached, and then to make a random decision about the next direction to follow. Although such a method would always [[Las Vegas algorithm|eventually find the right solution]], this algorithm can be extremely slow.

== Wall follower ==
[[File:maze01-02.png|left|frame|Traversal using ''right-hand rule'']]
[[File:MAZE.png|right|thumb|upright=1.5|Maze with two solutions]]
[[File:MAZE solution.png|right|thumb|upright=1.5|Solution to above maze. The solution is the boundary between the connected components of the wall of the maze, each represented by a different color.]]
The wall follower, the best-known rule for traversing mazes, is also known as either the ''left-hand rule'' or the ''right-hand rule''. If the maze is [[Simply connected space|''simply connected'']], that is, all its walls are connected together or to the maze's outer boundary, then by keeping one hand in contact with one wall of the maze the solver is guaranteed not to get lost and will reach a different exit if there is one; otherwise, he or she will return to the entrance having traversed every corridor next to that connected section of walls at least once.

Another perspective into why wall following works is topological. If the walls are connected, then they may be deformed into a loop or circle.&lt;ref&gt;{{youtube|IIBwiGrUgzc|Maze Transformed}}&lt;/ref&gt; Then wall following reduces to walking around a circle from start to finish. To further this idea, notice that by grouping together connected components of the maze walls, the boundaries between these are precisely the solutions, even if there is more than one solution (see figures on the right).

If the maze is not simply-connected (i.e. if the start or endpoints are in the center of the structure surrounded by passage loops, or the pathways cross over and under each other and such parts of the solution path are surrounded by passage loops), this method will not reach the goal.

Another concern is that care should be taken to begin wall-following at the entrance to the maze. If the maze is not simply-connected and one begins wall-following at an arbitrary point inside the maze, one could find themselves trapped along a separate wall that loops around on itself and containing no entrances or exits.  Should it be the case that wall-following begins late, attempt to mark the position in which wall-following began. Because wall-following will always lead you back to where you started, if you come across your starting point a second time, you can conclude the maze is not simply-connected, and you should switch to an alternative wall not yet followed. See the ''Pledge Algorithm'', below, for an alternative methodology.

Wall-following can be done in 3D or higher-dimensional mazes if its higher-dimensional passages can be projected onto the 2D plane in a deterministic manner. For example, if in a 3D maze "up" passages can be assumed to lead northwest, and "down" passages can be assumed to lead southeast, then standard wall following rules can apply. However, unlike in 2D, this requires that the current orientation be known, to determine which direction is the first on the left or right.

== Pledge algorithm ==
[[File:Pledge Algorithm.png|left|thumb| Left: Left-turn solver trapped &lt;br /&gt; Right: Pledge algorithm solution]]
Disjoint{{huh?|date=March 2017}} mazes can be solved with the wall follower method, so long as the entrance and exit to the maze are on the outer walls of the maze. If however, the solver starts inside the maze, it might be on a section disjoint from the exit, and wall followers will continually go around their ring. The Pledge algorithm (named after [[Jon Pledge]] of [[Exeter]]) can solve this problem.&lt;ref&gt;{{citation|title=Turtle Geometry: the computer as a medium for exploring mathematics|last1=Abelson|last2=diSessa|year=1980}}&lt;/ref&gt;&lt;ref&gt;Seymour Papert, [ftp://publications.ai.mit.edu/ai-publications/pdf/AIM-298.pdf "Uses of Technology to Enhance Education"], ''MIT Artificial Intelligence Memo No. 298'', June 1973&lt;/ref&gt;

The Pledge algorithm, designed to circumvent obstacles, requires an arbitrarily chosen direction to go toward, which will be preferential. When an obstacle is met, one hand (say the right hand) is kept along the obstacle while the angles turned are counted (clockwise  turn is positive, counter-clockwise turn is negative). When the solver is facing the original preferential direction again, and the angular sum of the turns made is 0, the solver leaves the obstacle and continues moving in its original direction.

The hand is removed from the wall only when both "sum of turns made" and "current heading" are at zero. This allows the algorithm to avoid traps shaped like an upper case letter "G". Assuming the algorithm turns left at the first wall, one gets turned around a full 360 [[degree (angle)|degree]]s by the walls. An algorithm that only keeps track of "current heading" leads into an infinite loop as it leaves the lower rightmost wall heading left and runs into the curved section on the left hand side again. The Pledge algorithm does not leave the rightmost wall due to the "sum of turns made" not being zero at that point (note 360 [[degree (angle)|degree]]s is not equal to 0 [[degree (angle)|degree]]s). It follows the wall all the way around, finally leaving it heading left outside and just underneath the letter shape.

This algorithm allows a person with a compass to find their way from any point inside to an outer exit of any finite two-dimensional maze, regardless of the initial position of the solver. However, this algorithm will not work in doing the reverse, namely finding the way from an entrance on the outside of a maze to some end goal within it.

== Trémaux's algorithm ==
[[File:Tremaux Maze Solving Algorithm.gif|thumb| Trémaux's algorithm. The large green dot shows the current position, the small blue dots show single marks on paths, and the red crosses show double marks. Once the exit is found, the route is traced through the singly-marked paths.]]
Trémaux's algorithm, invented by [[Charles Pierre Trémaux]],&lt;ref&gt;Public conference, December 2, 2010 – by professor Jean Pelletier-Thibert in Academie de Macon (Burgundy – France) – (Abstract published in the Annals academic, March 2011 – {{ISSN|0980-6032}}) &lt;br/&gt;Charles Tremaux (° 1859 – † 1882) Ecole Polytechnique of Paris (X:1876), French engineer of the telegraph&lt;/ref&gt; is an efficient method to find the way out of a maze that requires drawing lines on the floor to mark a path, and is guaranteed to work for all mazes that have well-defined passages.&lt;ref name="Récréations Mathématiques"&gt;Édouard Lucas: ''Récréations Mathématiques'' Volume I, 1882.&lt;/ref&gt;
A path from a junction is either unvisited, marked once or marked twice. The algorithm works according to the following rules:
* Mark each path once, when you follow it. The marks need to be visible at both ends of the path. Therefore, if they are being made as physical marks, rather than stored as part of a computer algorithm, the same mark should be made at both ends of the path.
* Never enter a path which has two marks on it.
* If you arrive at a junction that has no marks (except possibly for the one on the path by which you entered), choose an arbitrary unmarked path, follow it, and mark it.
* Otherwise:
** If the path you came in on has only one mark, turn around and return along that path, marking it again. In particular this case should occur whenever you reach a dead end.
** If not, choose arbitrarily one of the remaining paths with the fewest marks (zero if possible, else one), follow that path, and mark it.
When you finally reach the solution, paths marked exactly once will indicate a way back to the start. If there is no exit, this method will take you back to the start where all paths are marked twice.
In this case each path is walked down exactly twice, once in each direction. The resulting [[Glossary of graph theory#Walks|walk]] is called a bidirectional double-tracing.&lt;ref name="Eulerian Graphs and related Topics"&gt;H. Fleischner: ''Eulerian Graphs and related Topics.'' In: ''Annals of Discrete Mathematics'' No. 50 Part 1 Volume 2, 1991, page X20.&lt;/ref&gt;

Essentially, this algorithm, which was discovered in the 19th century, has been used about a hundred years later as [[depth-first search]].&lt;ref&gt;{{citation|title=Graph Algorithms|first=Shimon|last=Even|authorlink=Shimon Even|edition=2nd|publisher=Cambridge University Press|year=2011|isbn=978-0-521-73653-4|pages=46–48|url=https://books.google.com/books?id=m3QTSMYm5rkC&amp;pg=PA46}}.&lt;/ref&gt;&lt;ref&gt;{{citation|title=Algorithms in C++: Graph Algorithms|first=Robert|last=Sedgewick|edition=3rd|publisher=Pearson Education|year=2002|isbn=978-0-201-36118-6}}.&lt;/ref&gt;

== Dead-end filling ==
Dead-end filling is an algorithm for solving mazes that fills all dead ends, leaving only the correct ways unfilled. It can be used for solving mazes on paper or with a computer program, but it is not useful to a person inside an unknown maze since this method looks at the entire maze at once. The method is to 1) find all of the dead-ends in the maze, and then 2) "fill in" the path from each dead-end until the first junction is met. Note that some passages won't become parts of dead end passages until other dead ends are removed first. A video of dead-end filling in action can be seen here: [https://www.youtube.com/watch?v=yqZDYcpCGAI][https://www.youtube.com/watch?v=FkueaIT6RSU].

Dead-end filling cannot accidentally "cut off" the start from the finish since each step of the process preserves the topology of the maze. Furthermore, the process won't stop "too soon" since the end result cannot contain any dead-ends. Thus if dead-end filling is done on a perfect maze (maze with no loops), then only the solution will remain. If it is done on a partially braid maze (maze with some loops), then every possible solution will remain but nothing more. [http://www.astrolog.org/labyrnth/algrithm.htm]

== Recursive algorithm ==
If given an omniscient view of the maze, a simple recursive algorithm can tell one how to get to the end. The algorithm will be given a starting X and Y value. If the X and Y values are not on a wall, the method will call itself with all adjacent X and Y values, making sure that it did not already use those X and Y values before. If the X and Y values are those of the end location, it will save all the previous instances of the method as the correct path. Here is a sample code in [[Java (programming language)|Java]]:
&lt;source lang="java"&gt;
int[][] maze = new int[width][height]; // The maze
boolean[][] wasHere = new boolean[width][height];
boolean[][] correctPath = new boolean[width][height]; // The solution to the maze
int startX, startY; // Starting X and Y values of maze
int endX, endY;     // Ending X and Y values of maze

public void solveMaze() {
    maze = generateMaze(); // Create Maze (1 = path, 2 = wall)
    for (int row = 0; row &lt; maze.length; row++)  
        // Sets boolean Arrays to default values
        for (int col = 0; col &lt; maze[row].length; col++){
            wasHere[row][col] = false;
            correctPath[row][col] = false;
        }
    boolean b = recursiveSolve(startX, startY);
    // Will leave you with a boolean array (correctPath) 
    // with the path indicated by true values.
    // If b is false, there is no solution to the maze
}
public boolean recursiveSolve(int x, int y) {
    if (x == endX &amp;&amp; y == endY) return true; // If you reached the end
    if (maze[x][y] == 2 || wasHere[x][y]) return false;  
    // If you are on a wall or already were here
    wasHere[x][y] = true;
    if (x != 0) // Checks if not on left edge
        if (recursiveSolve(x-1, y)) { // Recalls method one to the left
            correctPath[x][y] = true; // Sets that path value to true;
            return true;
        }
    if (x != width - 1) // Checks if not on right edge
        if (recursiveSolve(x+1, y)) { // Recalls method one to the right
            correctPath[x][y] = true;
            return true;
        }
    if (y != 0)  // Checks if not on top edge
        if (recursiveSolve(x, y-1)) { // Recalls method one up
            correctPath[x][y] = true;
            return true;
        }
    if (y != height - 1) // Checks if not on bottom edge
        if (recursiveSolve(x, y+1)) { // Recalls method one down
            correctPath[x][y] = true;
            return true;
        }
    return false;
}
&lt;/source&gt;

== Maze-routing algorithm ==
The maze-routing algorithm &lt;ref&gt;{{cite journal|last1=Fattah|first1=Mohammad|last2=et|first2=al.|title=A Low-Overhead, Fully-Distributed, Guaranteed-Delivery Routing Algorithm for Faulty Network-on-Chips|journal=NOCS '15 Proceedings of the 9th International Symposium on Networks-on-Chip|date=2015-09-28|doi=10.1145/2786572.2786591|url=http://dl.acm.org/citation.cfm?id=2786591}}&lt;/ref&gt; is a low overhead method to find the way between any two locations of the maze. The algorithm is initially proposed for [[Chip multiprocessor|chip multiprocessors]] (CMPs) domain and guarantees to work for any grid-based maze. In addition to finding paths between two location of the grid (maze), the algorithm can detect when there is no path between the source and destination. Also, the algorithm is to be used by an inside traveler with no prior knowledge of the maze with fixed memory complexity regardless of the maze size; requiring 4 variables in total for finding the path and detecting the unreachable locations. Nevertheless, the algorithm is not to find the shortest path.

Maze-routing algorithm uses the notion of [[Manhattan distance]] (MD) and relies on the property of grids that the MD increments/decrements ''exactly'' by 1 when moving from one location to any 4 neighboring locations. Here is the pseudocode without the capability to detect unreachable locations.
&lt;source lang="C++"&gt;
Point src, dst;// Source and destination coordinates
// cur also indicates the coordinates of the current location
int MD_best = MD(src, dst);// It stores the closest MD we ever had to dst
// A productive path is the one that makes our MD to dst smaller
while(cur != dst){
    if(there exists a productive path){
        Take the productive path;
    }else{
        MD_best = MD(cur, dst);
        Imagine a line between cur and dst;
        Take the first path in the left/right of the line;// The left/right selection affects the following hand rule
        while(MD(cur, dst) != MD_best || there does not exist a productive path){
            Follow the right-hand/left-hand rule;// The opposite of the selected side of the line
    }
}
&lt;/source&gt;

== Shortest path algorithm ==
[[File:MAZE 40x20 DFS no deadends.png|thumb|A maze with many solutions and no dead-ends, where it may be useful to find the shortest path]]
When a maze has multiple solutions, the solver may want to find the shortest path from start to finish. There are several algorithms to find shortest paths, most of them coming from [[graph theory]]. One possible algorithm finds the shortest path by implementing a [[breadth-first search]], while another, the [[A* algorithm]], uses a [[heuristic]] technique. The breadth-first search algorithm uses a [[queue (data structure)|queue]] to visit cells in increasing distance order from the start until the finish is reached. Each visited cell needs to keep track of its distance from the start or which adjacent cell nearer to the start caused it to be added to the queue. When the finish location is found, follow the path of cells backwards to the start, which is the shortest path. The breadth-first search in its simplest form has its limitations, like finding the shortest path in weighted graphs.

==See also==
* [[Mazes]]
* [[Maze generation algorithm]]

== References ==
{{reflist}}

==External links==
* [http://www.astrolog.org/labyrnth/algrithm.htm#solve Think Labyrinth: Maze algorithms] (details on these and other maze solving algorithms)
* [http://www.cb.uu.se/~cris/blog/index.php/archives/277 MazeBlog: Solving mazes using image analysis]
* [https://www.youtube.com/watch?v=jhL8uELbVIM Video: Maze solving simulation]
* Simon Ayrinhac, [http://iopscience.iop.org/article/10.1088/0031-9120/49/4/443 Electric current solves mazes], © 2014 IOP Publishing Ltd.

[[Category:Mazes]]
[[Category:Algorithms]]</text>
      <sha1>jnzvbou8b7abi9xi8lrt1zmbof1w8tu</sha1>
    </revision>
  </page>
  <page>
    <title>Menger's theorem</title>
    <ns>0</ns>
    <id>1155738</id>
    <revision>
      <id>852598729</id>
      <parentid>852513549</parentid>
      <timestamp>2018-07-30T02:57:04Z</timestamp>
      <contributor>
        <username>Holdoffhunger</username>
        <id>27431995</id>
      </contributor>
      <comment>/* Other proofs */ Remove double "the."</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10982">In the [[mathematical]] discipline of [[graph theory]], '''Menger's theorem''' says that in a [[finite graph]], the size of a minimum [[cut set]] is equal to the maximum number of disjoint paths that can be found between any pair of vertices.
Proved by [[Karl Menger]] in 1927, it [[Characterization (mathematics)|characterizes]] the [[connectivity (graph theory)|connectivity]] of a graph.
It is generalized by the [[max-flow min-cut theorem]], which is a weighted, edge version, and which in turn is a special case of the [[Linear programming#Duality|strong duality theorem]] for linear programs.

==Edge connectivity==
The '''edge-connectivity''' version of Menger's theorem is as follows:

:Let ''G'' be a finite undirected graph and ''x'' and ''y'' two distinct vertices. Then the size of the minimum [[Edge cut#Connectivity|edge cut]] for ''x'' and ''y'' (the minimum number of edges whose removal disconnects ''x'' and ''y'') is equal to the maximum number of pairwise [[path (graph theory)|edge-independent path]]s from ''x'' to ''y''.

:Extended to all pairs: a graph is [[K-edge-connected graph|''k''-edge-connected]] (it remains connected after removing fewer than ''k'' edges) if and only if every pair of vertices has ''k'' edge-disjoint paths in between.

==Vertex connectivity==
The '''vertex-connectivity''' statement of Menger's theorem is as follows:

:Let ''G'' be a finite undirected graph and ''x'' and ''y'' two [[nonadjacent]] vertices. Then the size of the minimum [[vertex cut]] for ''x'' and ''y'' (the minimum number of vertices, distinct from ''x'' and ''y'', whose removal disconnects ''x'' and ''y'') is equal to the maximum number of pairwise [[path (graph theory)|internally vertex-disjoint paths]] from ''x'' to ''y''.

:Extended to all pairs: a graph is [[K-vertex-connected graph|''k''-vertex-connected]] (it has more than ''k'' vertices and it remains connected after removing fewer than ''k'' vertices) if and only if every pair of vertices has ''k'' internally vertex-disjoint paths in between.

All these statements (in both edge and vertex versions) remain true in directed graphs (when considering directed paths).

==Short Proof==
Most direct proofs consider a more general statement to allow proving it by induction. It is also convenient to use definitions that include some degenerate cases.
The following proof for undirected graphs works without change for directed graphs or multi-graphs, provided we take ''path'' to mean directed path.

For sets of vertices ''A,B ⊂ G'' (not necessarily disjoint), an ''AB-path'' is a path in ''G'' with a starting vertex in ''A'', a final vertex in ''B'', and no internal vertices in ''A'' or ''B''. We allow a path with a single vertex in ''A ∩ B'' and zero edges.
An ''AB-separator'' of size ''k'' is a set ''S'' of ''k'' vertices (which may intersect ''A'' and ''B'') such that ''G−S'' contains no ''AB''-path.
An ''AB-connector'' of size ''k'' is a union of ''k'' vertex-disjoint ''AB''-paths.

: '''Theorem:''' The minimum size of an ''AB''-separator is equal to the maximum size of an ''AB''-connector.

In other words, if no ''k''−1 vertices disconnect ''A'' from ''B'', then there exist ''k'' disjoint paths from ''A'' to ''B''.
This variant implies the above vertex-connectivity statement: for ''x,y &amp;isin; G'' in the previous section, apply the current theorem to ''G''−{''x,y''} with ''A = N(x)'', ''B = N(y)'', the neighboring vertices of ''x,y''.  Then a set of vertices disconnecting ''x'' and ''y'' is the same thing as an
''AB''-separator, and removing the end vertices in a set of independent ''xy''-paths gives an ''AB''-connector.

''Proof of the Theorem:''&lt;ref&gt;F. Göring, ''Short Proof of Menger's Theorem'', Discrete Mathematics '''219''' (2000) 295-296.)&lt;/ref&gt;
Induction on the number of edges in ''G''.
For ''G'' with no edges, the minimum ''AB''-separator is ''A ∩ B'',
which is itself an ''AB''-connector consisting of single-vertex paths.

For ''G'' having an edge ''e'', we may assume by induction that the Theorem holds for ''G−e''. If ''G−e'' has a minimal ''AB''-separator of size ''k'', then there is an ''AB''-connector of size ''k'' in ''G−e'', and hence in ''G''.

[[File:Proof of Menger's Theorem.svg|thumb|An illustration for the proof.]]

Otherwise, let ''S'' be a ''AB''-separator of ''G−e'' of size less than ''k'',
so that every ''AB''-path in ''G'' contains a vertex of ''S'' or the edge ''e''.
The size of ''S'' must be ''k-1'', since if it was less, ''S'' together with either endpoint of ''e'' would be a better ''AB''-separator of ''G''.
In ''G−S'' there is an ''AB''-path through ''e'', since ''S'' alone is too small to be an ''AB''-separator of ''G''.
Let ''v&lt;sub&gt;1&lt;/sub&gt;'' be the earlier and ''v&lt;sub&gt;2&lt;/sub&gt;'' be the later vertex of ''e'' on such a path.
Then ''v&lt;sub&gt;1&lt;/sub&gt;'' is reachable from ''A'' but not from ''B'' in ''G−S−e'', while ''v&lt;sub&gt;2&lt;/sub&gt;'' is reachable from ''B'' but not from ''A''.

Now, let ''S&lt;sub&gt;1&lt;/sub&gt; = S ∪ {v&lt;sub&gt;1&lt;/sub&gt;}'', and consider a minimum ''AS&lt;sub&gt;1&lt;/sub&gt;''-separator ''T'' in ''G−e''.
Since ''v&lt;sub&gt;2&lt;/sub&gt;'' is not reachable from ''A'' in ''G−S&lt;sub&gt;1&lt;/sub&gt;'', ''T'' is also an ''AS&lt;sub&gt;1&lt;/sub&gt;''-separator in ''G''.
Then ''T'' is also an ''AB''-separator in ''G'' (because every ''AB''-path intersects ''S&lt;sub&gt;1&lt;/sub&gt;'').
Hence it has size at least ''k''.
By induction, ''G−e'' contains an ''AS&lt;sub&gt;1&lt;/sub&gt;''-connector ''C&lt;sub&gt;1&lt;/sub&gt;'' of size ''k''.
Because of its size, the endpoints of the paths in it must be exactly ''S&lt;sub&gt;1&lt;/sub&gt;''.

Similarly, letting ''S&lt;sub&gt;2&lt;/sub&gt; = S  ∪ {v&lt;sub&gt;2&lt;/sub&gt;}'', a minimum ''S&lt;sub&gt;2&lt;/sub&gt;B''-separator has size ''k'', and there is 
an ''S&lt;sub&gt;2&lt;/sub&gt;B''-connector ''C&lt;sub&gt;2&lt;/sub&gt;'' of size ''k'', with paths whose starting points are exactly ''S&lt;sub&gt;2&lt;/sub&gt;''.

Furthermore, since ''S&lt;sub&gt;1&lt;/sub&gt;'' disconnects ''G'', every path in ''C&lt;sub&gt;1&lt;/sub&gt;'' is internally disjoint from 
every path in ''C&lt;sub&gt;2&lt;/sub&gt;'', and we can define an ''AB''-connector of size ''k'' in ''G'' by concatenating paths (''k−1'' paths through ''S'' and one path going through ''e=v&lt;sub&gt;1&lt;/sub&gt;v&lt;sub&gt;2&lt;/sub&gt;''). Q.E.D.

== Other proofs ==
The directed edge version of the theorem easily implies the other versions.
To infer the directed graph vertex version, it suffices to split each vertex ''v'' into two vertices ''v&lt;sub&gt;1&lt;/sub&gt;'', ''v&lt;sub&gt;2&lt;/sub&gt;'', with all ingoing edges going to ''v&lt;sub&gt;1&lt;/sub&gt;'', all outgoing edges going from ''v&lt;sub&gt;2&lt;/sub&gt;'', and an additional edge from ''v&lt;sub&gt;1&lt;/sub&gt;'' to ''v&lt;sub&gt;2&lt;/sub&gt;''.
The directed versions of the theorem immediately imply undirected versions: it suffices to replace each edge of an undirected graph with a pair of directed edges (a digon).

The directed edge version in turn follows from its weighted variant, the [[max-flow min-cut theorem]].
Its [[Max-flow min-cut theorem#Proof|proof]]s are often correctness proofs for max flow algorithms.
It is also a special case of the still more general (strong) [[Linear programming#Duality|duality theorem]] for [[linear program]]s.

A formulation that for finite digraphs is equivalent to the above formulation is:
: Let ''A'' and ''B'' be sets of vertices in a finite [[directed graph|digraph]] ''G''. Then there exists a family ''P'' of disjoint ''AB''-paths and an ''AB''-separating set that consists of exactly one vertex from each path in ''P''.

In this version the theorem follows in fairly easily from [[Kőnig's theorem (graph theory)|König's theorem]]: in a bipartite graph, the minimal size of a cover is equal to the maximal size of a matching.

This is done as follows: replace every vertex ''v'' in the original digraph ''D'' by two vertices ''v' '', ''v&lt;nowiki&gt;''&lt;/nowiki&gt;'', and every edge ''uv'' by the edge ''u'v&lt;nowiki&gt;''&lt;/nowiki&gt;''. This results in a bipartite graph, whose one side consists of the vertices ''v' '', and the other of the vertices ''v&lt;nowiki&gt;''&lt;/nowiki&gt;''.

Applying König's theorem we obtain a matching ''M'' and a cover ''C'' of the same size. In particular, exactly one endpoint of each edge of ''M'' is in ''C''. Add to ''C'' all vertices ''a&lt;nowiki&gt;''&lt;/nowiki&gt;'', for ''a'' in ''A,'' and all vertices ''b' '', for ''b'' in ''B''. Let ''P'' be the set of all ''AB''-paths composed of edges ''uv'' in ''D'' such that ''u'v&lt;nowiki&gt;''&lt;/nowiki&gt;'' belongs to M. Let ''Q'' in the original graph consist of all vertices ''v'' such that both ''v' '' and ''v&lt;nowiki&gt;''&lt;/nowiki&gt;'' belong to ''C''. It is straightforward to check that ''Q'' is an ''AB''-separating set, and that every path in the family ''P'' contains precisely one vertex from ''Q'', as desired.&lt;ref&gt;{{cite journal |doi=10.1016/S0195-6698(83)80012-2 |title=Menger's Theorem for Graphs Containing no Infinite Paths |journal=European Journal of Combinatorics |volume=4 |issue=3 |pages=201–4 |year=1983 |last1=Aharoni |first1=Ron }}&lt;/ref&gt;

==Infinite graphs==
Menger's theorem holds for infinite graphs, and in that context it applies to the minimum cut between any two elements that are either vertices or [[end (graph theory)|ends]] of the graph {{harv|Halin|1974}}. The following result of [[Ron Aharoni]] and [[Eli Berger]] was originally a conjecture proposed by [[Paul Erdős]], and before being proved was known as the '''Erdős–Menger conjecture'''.
It is equivalent to Menger's theorem when the graph is finite.

:Let ''A'' and ''B'' be sets of vertices in a (possibly infinite) [[directed graph|digraph]] ''G''. Then there exists a family ''P'' of disjoint ''A''-''B''-paths and a separating set which consists of exactly one vertex from each path in ''P''.

==See also==
* [[Gammoid]]
* [[k-vertex-connected graph]]
* [[k-edge-connected graph]]
* [[Vertex separator]]

==References==
{{Reflist}}

==Further reading==
* {{cite journal
  | author = Menger, Karl
  | title = Zur allgemeinen Kurventheorie
  | journal = Fund. Math.
  | volume = 10
  | pages = 96–115
  | year = 1927}}
* {{cite journal |doi=10.1007/s00222-008-0157-3 |title=Menger's theorem for infinite graphs |journal=Inventiones mathematicae |volume=176 |pages=1 |year=2008 |last1=Aharoni |first1=Ron |last2=Berger |first2=Eli |arxiv=math/0509397 |bibcode=2009InMat.176....1A }}
*{{cite journal |doi=10.1007/BF02993589 |title=A note on Menger's theorem for infinite locally finite graphs |journal=Abhandlungen aus dem Mathematischen Seminar der Universität Hamburg |volume=40 |pages=111 |year=1974 |last1=Halin |first1=R. }}

== External links ==
*[http://www.math.unm.edu/~loring/links/graph_s05/Menger.pdf A Proof of Menger's Theorem]
*[http://brain.math.fau.edu/locke/Menger.htm Menger's Theorems and Max-Flow-Min-Cut]
*[http://gepard.bioinformatik.uni-saarland.de/teaching/ws-2008-09/bioinformatik-3/lectures/V12-NetworkFlow.pdf Network flow]
*[http://gepard.bioinformatik.uni-saarland.de/teaching/ws-2008-09/bioinformatik-3/lectures/V13-MaxFlowMinCut.pdf Max-Flow-Min-Cut]

[[Category:Graph connectivity]]
[[Category:Network theory]]
[[Category:Theorems in graph theory]]</text>
      <sha1>coa9bsh79hi7edxwhxuq0s2pvp5kiif</sha1>
    </revision>
  </page>
  <page>
    <title>Method of distinguished element</title>
    <ns>0</ns>
    <id>1590804</id>
    <revision>
      <id>790734125</id>
      <parentid>729656315</parentid>
      <timestamp>2017-07-15T19:09:41Z</timestamp>
      <contributor>
        <username>Deacon Vorbis</username>
        <id>29330520</id>
      </contributor>
      <minor/>
      <comment>/* Examples */LaTeX spacing clean up, replaced: \,&lt;/math&gt; → &lt;/math&gt; using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5628">{{Redirect|Distinguished element|sets with pre-defined distinguished elements|Pointed set}}

In the [[mathematics|mathematical]] field of [[enumerative combinatorics]], [[identity (mathematics)|identities]] are sometimes established by arguments that rely on singling out one '''"distinguished element"''' of a set.

==Definition==

Let &lt;math&gt;\mathcal{A}&lt;/math&gt; be a family of subsets of the set &lt;math&gt;A&lt;/math&gt; and let &lt;math&gt;x \in A&lt;/math&gt; be a distinguished element of set &lt;math&gt;A&lt;/math&gt;. Then suppose there is a predicate &lt;math&gt;P(X,x)&lt;/math&gt; that relates a subset &lt;math&gt;X\subseteq A&lt;/math&gt; to &lt;math&gt;x&lt;/math&gt;. Denote &lt;math&gt;\mathcal{A}(x)&lt;/math&gt; to be the set of subsets &lt;math&gt;X&lt;/math&gt; from &lt;math&gt;\mathcal{A}&lt;/math&gt; for which &lt;math&gt;P(X,x)&lt;/math&gt; is true and &lt;math&gt;\mathcal{A}-x&lt;/math&gt; to be the set of subsets &lt;math&gt;X&lt;/math&gt; from &lt;math&gt;\mathcal{A}&lt;/math&gt; for which &lt;math&gt;P(X,x)&lt;/math&gt; is false, Then &lt;math&gt;\mathcal{A}(x)&lt;/math&gt; and &lt;math&gt;\mathcal{A}-x&lt;/math&gt; are disjoint sets, so by the method of summation, the cardinalities are additive&lt;ref name=Petkovsek2002&gt;{{cite journal|last=Petkovšek|first=Marko|author2=Tomaž Pisanski|title=Combinatorial Interpretation of Unsigned Stirling and Lah Numbers|journal=University of Ljubljana preprint series|date=November 2002|volume=40|issue=837|pages=1-6|url=http://www.imfm.si/preprinti/PDF/00837.pdf|accessdate=12 July 2013}}&lt;/ref&gt;

:&lt;math&gt;|\mathcal{A}| = |\mathcal{A}(x)| + |\mathcal{A}-x|&lt;/math&gt;

Thus the distinguished element allows for a decomposition according to a predicate that is a simple form of a [[divide and conquer algorithm]]. In combinatorics, this allows for the construction of [[recurrence relation]]s. Examples are in the next section.

==Examples==
* The [[binomial coefficient]] &lt;math&gt;{n \choose k}&lt;/math&gt; is the number of size-''k'' subsets of a size-''n'' set.  A basic identity—one of whose consequences is that the binomial coefficients are precisely the numbers appearing in [[Pascal's triangle]]—states that:

::&lt;math&gt;{n \choose k-1}+{n \choose k}={n+1 \choose k}.&lt;/math&gt;

:'''Proof:''' In a size-(''n''&amp;nbsp;+&amp;nbsp;1) set, choose one distinguished element.  The set of all size-''k'' subsets contains: (1) all size-''k'' subsets that ''do'' contain the distinguished element, and (2) all size-''k'' subsets that ''do not'' contain the distinguished element.  If a size-''k'' subset of a size-(''n''&amp;nbsp;+&amp;nbsp;1) set ''does'' contain the distinguished element, then its other ''k''&amp;nbsp;&amp;minus;&amp;nbsp;1 elements are chosen from among the other ''n'' elements of our size-(''n''&amp;nbsp;+&amp;nbsp;1) set.  The number of ways to choose those is therefore &lt;math&gt;{n \choose k-1}&lt;/math&gt;.  If a size-''k'' subset ''does not'' contain the distinguished element, then all of its ''k'' members are chosen from among the other ''n'' "non-distinguished" elements.  The number of ways to choose those is therefore &lt;math&gt;{n \choose k}&lt;/math&gt;.

*The number of subsets of any size-''n'' set is 2&lt;sup&gt;''n''&lt;/sup&gt;.

:'''Proof:'''  We use [[mathematical induction]].  The basis for induction is the truth of this proposition in case ''n''&amp;nbsp;=&amp;nbsp;0.  The [[empty set]] has 0 members and 1 subset, and 2&lt;sup&gt;0&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;1.  The induction hypothesis is the proposition in case ''n''; we use it to prove case ''n''&amp;nbsp;+&amp;nbsp;1.  In a size-(''n''&amp;nbsp;+&amp;nbsp;1) set, choose a distinguished element.  Each subset either contains the distinguished element or does not.  If a subset contains the distinguished element, then its remaining elements are chosen from among the other ''n'' elements.  By the induction hypothesis, the number of ways to do that is 2&lt;sup&gt;''n''&lt;/sup&gt;.  If a subset does not contain the distinguished element, then it is a subset of the set of all non-distinguished elements.  By the induction hypothesis, the number of such subsets is 2&lt;sup&gt;''n''&lt;/sup&gt;.  Finally, the whole list of subsets of our size-(''n''&amp;nbsp;+&amp;nbsp;1) set contains 2&lt;sup&gt;''n''&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;2&lt;sup&gt;''n''&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;2&lt;sup&gt;''n''+1&lt;/sup&gt; elements.

* Let ''B''&lt;sub&gt;''n''&lt;/sub&gt; be the ''n''th [[Bell number]], i.e., the number of [[partition of a set|partitions of a set]] of ''n'' members.  Let ''C''&lt;sub&gt;''n''&lt;/sub&gt; be the total number of "parts" (or "blocks", as combinatorialists often call them) among all partitions of that set.  For example, the partitions of the size-3 set {''a'',&amp;nbsp;''b'',&amp;nbsp;''c''} may be written thus:

::&lt;math&gt;\begin{matrix}abc \\  a/bc \\  b/ac \\  c/ab \\  a/b/c \end{matrix}&lt;/math&gt;

:We see 5 partitions, containing 10 blocks, so ''B''&lt;sub&gt;3&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;5 and ''C''&lt;sub&gt;3&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;10.  An identity states:

::&lt;math&gt;B_n+C_n=B_{n+1}.&lt;/math&gt;

:'''Proof:''' In a size-(''n''&amp;nbsp;+&amp;nbsp;1) set, choose a distinguished element.  In each partition of our size-(''n''&amp;nbsp;+&amp;nbsp;1) set, either the distinguished element is a "singleton", i.e., the set containing ''only'' the distinguished element is one of the blocks, or the distinguished element belongs to a larger block.  If the distinguished element is a singleton, then deletion of the distinguished element leaves a partition of the set containing the ''n'' non-distinguished elements.  There are ''B''&lt;sub&gt;''n''&lt;/sub&gt; ways to do that.  If the distinguished element belongs to a larger block, then its deletion leaves a block in a partition of the set containing the ''n'' non-distinguished elements.  There are ''C''&lt;sub&gt;''n''&lt;/sub&gt; such blocks.

==See also==
* [[Combinatorial principles]]
* [[Combinatorial proof]]

==References==
{{reflist}}

{{DEFAULTSORT:Method Of Distinguished Element}}
[[Category:Combinatorics]]
[[Category:Mathematical principles]]</text>
      <sha1>rgq26of1fwei3dz716yuefk5zutfhay</sha1>
    </revision>
  </page>
  <page>
    <title>Michael Brin Prize in Dynamical Systems</title>
    <ns>0</ns>
    <id>47522601</id>
    <revision>
      <id>842552837</id>
      <parentid>841533918</parentid>
      <timestamp>2018-05-23T05:30:03Z</timestamp>
      <contributor>
        <username>OAbot</username>
        <id>28481209</id>
      </contributor>
      <minor/>
      <comment>[[Wikipedia:OABOT|Open access bot]]: add arxiv identifier to citation with #oabot.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6901">The '''Michael Brin Prize in Dynamical Systems''', abbreviated as the '''Brin Prize''', is awarded to mathematicians who have made outstanding advances in the field of [[dynamical system]]s and are within 14 years of their PhD.&lt;ref name="mbpds"&gt;{{citation|title=Michael Brin Prize in Dynamical Systems|work=Center for Dynamical Systems and Geometry|publisher=Pennsylvania State University Mathematics Department|url=http://www.math.psu.edu/dynsys/Brinprize/|accessdate=2015-08-25}}.&lt;/ref&gt; The prize is endowed by and named after Michael Brin,&lt;ref name="mbpds"/&gt;  whose son  [[Sergey Brin]],&lt;ref&gt;{{citation|url=http://www.cnn.com/2014/07/03/us/sergey-brin-fast-facts/|title=Sergey Brin Fast Facts|date=August 11, 2015|publisher=[[CNN]]|accessdate=2015-08-24}}.&lt;/ref&gt; is a  co-founder of [[Google]]. Michael Brin is a retired mathematician at the [[University of Maryland]] and a specialist in dynamical systems.&lt;ref&gt;Author biography from [http://www.cambridge.org/us/academic/subjects/mathematics/differential-and-integral-equations-dynamical-systems-and-co/introduction-dynamical-systems-1?format=PB publisher's web site] for {{citation|title=Introduction to Dynamical Systems|first1=Michael|last1=Brin|first2=Garrett|last2=Stuck|publisher=Cambridge University Press|year=2015}}.&lt;/ref&gt;

The first prize was awarded in 2008, and since 2009, it has been awarded bi-annually. [[Artur Avila]], the 2011 awardee, went on to win the [[Fields Medal]] in 2014.&lt;ref&gt;{{citation|url=https://www.theguardian.com/science/alexs-adventures-in-numberland/2014/aug/13/fields-medals-2014-maths-avila-bhargava-hairer-mirzakhani|title=Fields Medals 2014: the maths of Avila, Bhargava, Hairer and Mirzakhani explained|newspaper=[[The Guardian]]|date=August 13, 2014|first=Alex|last=Bellos|quote=Among his [Avila's] previous honors are ... the Michael Brin Prize (2011)}}.&lt;/ref&gt;

==Past winners==
* 2008 [[Giovanni Forni (mathematician)|Giovanni Forni]] for his work on area-preserving flows.&lt;ref&gt;{{citation
 | issue = 3
 | journal = Journal of Modern Dynamics
 | mr = 2453628
 | page = ii
 | title = Giovanni Forni—Brin Prize recipient 2008
 | volume = 2
 | year = 2008}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last =  Veech | first = William A.
 | doi = 10.3934/jmd.2008.2.375
 | issue = 3
 | journal = Journal of Modern Dynamics
 | mr = 2417477
 | pages = 375-395
 | title = The Forni Cocycle
 | volume = 2
 | year = 2008}}.&lt;/ref&gt;&lt;ref&gt;{{citation|department=Mathematics People|title=Forni Awarded First Brin Prize in Dynamical Systems|journal=Notices of the AMS|date=June–July 2008|pages=715–716|first=Anatole|last=Katok|volume=55|issue=6|url=http://www.ams.org/notices/200806/tx080600715p.pdf}}.&lt;/ref&gt;
* 2009 [[Dmitry Dolgopyat]] for his work on rapid mixing of flows.&lt;ref&gt;{{citation
 | issue = 2
 | journal = Journal of Modern Dynamics
 | mr = 2672293
 | pages = i–ii
 | title = The 2009 Michael Brin Prize in dynamical systems
 | volume = 4
 | year = 2010}}&lt;/ref&gt;&lt;ref&gt;{{citation
 | last = Liverani | first = Carlangelo
 | doi = 10.3934/jmd.2010.4.211
 | issue = 2
 | journal = Journal of Modern Dynamics
 | mr = 2672294
 | pages = 211-225
 | title = On the work and vision of Dmitry Dolgopyat
 | volume = 4
 | year = 2010}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last = Pesin | first = Yakov
 | doi = 10.3934/jmd.2010.4.211
 | issue = 2
 | journal = Journal of Modern Dynamics
 | mr = 2672295
 | pages = 227-241
 | title = On the work of Dolgopyat on partial and nonuniform hyperbolicity
 | volume = 4
 | year = 2010}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last = Chernov | first = Nikolai
 | doi = 10.3934/jmd.2010.4.243
 | issue = 2
 | journal = Journal of Modern Dynamics
 | mr = 2672296
 | pages = 243-255
 | title = On the work of Dmitry Dolgopyat on physical models with moving particles
 | volume = 4
 | year = 2010}}.&lt;/ref&gt;
* 2011 [[Artur Avila]] for his work on Teichmüller dynamics and interval-exchange transformations.&lt;ref&gt;{{citation
 | issue = 2
 | journal = Journal of Modern Dynamics
 | mr = 2968952
 | pages = i–ii
 | title = The 2011 Michael Brin Prize in dynamical systems
 | volume = 6
 | year = 2012}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last = Forni | first = Giovanni
 | doi = 10.3934/jmd.2012.6.139
 | issue = 2
 | journal = Journal of Modern Dynamics
 | mr = 2968953
 | pages = 139–182
 | title = On the Brin Prize work of Artur Avila in Teichmüller dynamics and interval-exchange transformations
 | volume = 6
 | year = 2012}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last = Lyubich | first = Mikhail
 | doi = 10.3934/jmd.2012.6.183
 | issue = 2
 | journal = Journal of Modern Dynamics
 | mr = 2968954
 | pages = 183–203
 | title = Forty years of unimodal dynamics: on the occasion of Artur Avila winning the Brin Prize
 | volume = 6
 | year = 2012}}.&lt;/ref&gt;
* 2013 [[Omri Sarig]] for his work on the thermodynamics of countable Markov shifts.&lt;ref&gt;{{citation
 | issue = 1
 | journal = Journal of Modern Dynamics
 | mr = 3296938
 | pages = i–ii
 | title = The 2013 Michael Brin prize in dynamical systems
 | volume = 8
 | year = 2014}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last = Pesin | first = Yakov
 | doi = 10.3934/jmd.2014.8.1
 | issue = 1
 | journal = Journal of Modern Dynamics
 | mr = 3296563
 | pages = 1–14
 | title = On the work of Sarig on countable Markov Chains and Thermodynamics
 | volume = 8
 | year = 2014| arxiv = 1301.3917
 }}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last = Ledrappier | first = Francois
 | doi = 10.3934/jmd.2014.8.15
 | issue = 1
 | journal = Journal of Modern Dynamics
 | mr = 3296564
 | pages = 15–24
 | title = On Omri Sarig’s work on the dynamics on surfaces
 | volume = 8
 | year = 2014}}.&lt;/ref&gt;
*2015 [[Federico Rodriguez Hertz]] for his work on geometric and measure rigidity and on stable ergodicity of partially hyperbolic systems.&lt;ref&gt;{{citation
 | journal = Journal of Modern Dynamics
 | mr = 3510293
 | pages = 173–174
 | title = The 2015 Michael Brin prize in dynamical systems
 | volume = 10
 | year = 2016}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last = Dolgopyat | first = Dmitry
 | doi = 10.3934/jmd.2016.10.175
 | journal = Journal of Modern Dynamics
 | mr = 3510294
 | pages = 175–189
 | title = The work of Federico Rodriguez Hertz on ergodicity of dynamical systems
 | volume = 10
 | year = 2016}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last = Spatzier | first = Ralf
 | doi = 10.3934/jmd.2016.10.191
 | journal = Journal of Modern Dynamics
 | mr = 3510295
 | pages = 191–207
 | title = On the work of Rodriguez Hertz on rigidity in dynamics
 | volume = 10
 | year = 2016| arxiv = 1606.00527
 }}.&lt;/ref&gt;
*2017 : [[Lewis Bowen]] for creation of entropy theory for a broad class of non-amenable groups and solution of the long standing isomorphism problem for Bernoulli actions of such groups.
*2018 : [[Mike Hochman]] for his work in ergodic theory and fractal geometry.

==References==
{{Reflist}}

[[Category:Awards established in 2008]]
[[Category:Academic awards]]
[[Category:Mathematics awards]]
[[Category:Dynamical systems]]</text>
      <sha1>goplnlhang6cttg43gykun5p5d6a2kt</sha1>
    </revision>
  </page>
  <page>
    <title>Miller index</title>
    <ns>0</ns>
    <id>1089079</id>
    <revision>
      <id>866623718</id>
      <parentid>866623627</parentid>
      <timestamp>2018-10-31T14:00:41Z</timestamp>
      <contributor>
        <ip>2001:62A:4:42D:9137:DBCE:8D5:DE0D</ip>
      </contributor>
      <comment>fixed link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13839">[[Image:Miller Indices Felix Kling.svg|thumb|300px|Planes with different Miller indices in cubic crystals]]
[[Image:Indices miller direction exemples.png|thumb|300px|Examples of directions]]
'''Miller indices''' form a notation system in [[crystallography]] for planes in [[Bravais lattice|crystal (Bravais) lattices]].

In particular, a family of [[lattice plane]]s is determined by three [[integer]]s ''h'', ''k'', and&amp;nbsp;''ℓ'', the ''Miller indices''.  They are written (hkℓ), and denote the family of planes orthogonal to &lt;math&gt;h\mathbf{b_1} + k\mathbf{b_2} + \ell\mathbf{b_3}&lt;/math&gt;, where &lt;math&gt;\mathbf{b_i}&lt;/math&gt; are the [[Basis (linear algebra)|basis]] of the [[reciprocal lattice]] vectors. (Note that the plane is not always orthogonal to the linear combination of direct lattice vectors &lt;math&gt;h\mathbf{a_1} + k\mathbf{a_2} + \ell\mathbf{a_3}&lt;/math&gt; because the reciprocal lattice vectors need not be mutually orthogonal.) By convention, [[negative integer]]s are written with a bar, as in {{overline|3}} for&amp;nbsp;−3.  The integers are usually written in lowest terms, i.e. their [[greatest common divisor]] should be&amp;nbsp;1.
m 
There are also several related notations:&lt;ref name="Ash"&gt;Neil W. Ashcroft and N. David Mermin, Solid State Physics (Harcourt: New York, 1976)&lt;/ref&gt;
*the notation {hkℓ} denotes the set of all planes that are equivalent to (hkℓ) by the symmetry of the lattice.
In the context of crystal ''directions'' (not planes), the corresponding notations are:
*[hkℓ], with square instead of round brackets, denotes a direction in the basis of the ''direct'' lattice vectors instead of the reciprocal lattice; and
*similarly, the notation &lt;hkℓ&gt; denotes the set of all directions that are equivalent to [hkℓ] by symmetry.

Miller indices were introduced in 1839 by the British mineralogist [[William Hallowes Miller]], although an almost identical system (''Weiss parameters'') had already been used by German mineralogist [[Christian Samuel Weiss]] since 1817 &lt;ref&gt;{{cite journal |last1=Weiss |first1=Christian Samuel |title=Ueber eine verbesserte Methode für die Bezeichnung der verschiedenen Flächen eines Krystallisationssystems, nebst Bemerkungen über den Zustand der Polarisierung der Seiten in den Linien der krystallinischen Structur |journal=Abhandlungen der physikalischen Klasse der Königlich-Preussischen Akademie der Wissenschaften |date=1817 |pages=286–336 |url=http://www.archive.org/stream/abhandlungenderp16akad#page/286/mode/2up}}&lt;/ref&gt;. The method was also historically known as the Millerian system, and the indices as Millerian,&lt;ref&gt;[http://dictionary.oed.com Oxford English Dictionary Online] (Consulted May 2007)&lt;/ref&gt; although this is now rare.

The Miller indices are defined with respect to any choice of unit cell and not only with respect to primitive basis vectors, as is sometimes stated.

==Definition==
[[Image:Indices miller plan definition.png|thumb|300px|Examples of determining indices for a plane using intercepts with axes; left (111), right (221)]]
There are two equivalent ways to define the meaning of the Miller indices:&lt;ref name="Ash"/&gt; via a point in the [[reciprocal lattice]], or as the inverse intercepts along the lattice vectors.  Both definitions are given below.  In either case, one needs to choose the three lattice vectors '''a&lt;sub&gt;1&lt;/sub&gt;''', '''a&lt;sub&gt;2&lt;/sub&gt;''', and '''a&lt;sub&gt;3&lt;/sub&gt;''' that define the unit cell (note that the conventional unit cell may be larger than the primitive cell of the [[Bravais lattice]], as the [[Miller_index#Case_of_cubic_structures|examples below]] illustrate). Given these, the three primitive reciprocal lattice vectors are also determined (denoted '''b&lt;sub&gt;1&lt;/sub&gt;''', '''b&lt;sub&gt;2&lt;/sub&gt;''', and '''b&lt;sub&gt;3&lt;/sub&gt;''').

Then, given the three Miller indices h, k, ℓ, (hkℓ) denotes planes orthogonal to the reciprocal lattice vector:
:&lt;math&gt; \mathbf{g}_{hk\ell} = h \mathbf{b}_1 + k \mathbf{b}_2 + \ell \mathbf{b}_3 .&lt;/math&gt;
That is, (hkℓ) simply indicates a normal to the planes in the [[Basis (linear algebra)|basis]] of the primitive reciprocal lattice vectors.  Because the coordinates  are integers, this normal is itself always a reciprocal lattice vector.  The requirement of lowest terms means that it is the ''shortest'' reciprocal lattice vector in the given direction.

Equivalently, (hkℓ) denotes a plane that intercepts the three points '''a&lt;sub&gt;1&lt;/sub&gt;'''/''h'', '''a&lt;sub&gt;2&lt;/sub&gt;'''/''k'', and '''a&lt;sub&gt;3&lt;/sub&gt;'''/''ℓ'', or some multiple thereof.  That is, the Miller indices are proportional to the ''inverses'' of the intercepts of the plane, in the basis of the lattice vectors.  If one of the indices is zero, it means that the planes do not intersect that axis (the intercept is "at infinity").

Considering only (hkℓ) planes intersecting one or more lattice points (the ''lattice planes''), the perpendicular distance ''d'' between adjacent lattice planes is related to the (shortest) reciprocal lattice vector orthogonal to the planes by the formula: &lt;math&gt;d = 2\pi / |\mathbf{g}_{h k \ell}|&lt;/math&gt;.&lt;ref name="Ash"/&gt;

The related notation [hkℓ] denotes the ''direction'':
:&lt;math&gt;h \mathbf{a}_1 + k \mathbf{a}_2 + \ell \mathbf{a}_3 .&lt;/math&gt;
That is, it uses the direct lattice basis instead of the reciprocal lattice.  Note that [hkℓ] is ''not'' generally normal to the (hkℓ) planes, except in a cubic lattice as described below.

==Case of cubic structures==
For the special case of simple cubic crystals, the lattice vectors are orthogonal and of equal length (usually denoted ''a''), as are those of the reciprocal lattice.  Thus, in this common case, the Miller indices (hkℓ) and [hkℓ] both simply denote normals/directions in [[Cartesian coordinates]].

For cubic crystals with [[lattice constant]] ''a'', the spacing ''d'' between adjacent (hkℓ) lattice planes is (from above)
: &lt;math&gt;d_{hk \ell}= \frac {a} { \sqrt{h^2 + k^2 + \ell ^2} }&lt;/math&gt;.

Because of the symmetry of cubic crystals, it is possible to change the place and sign of the integers and have equivalent directions and planes:
*Indices in ''angle brackets'' such as ⟨100⟩ denote a ''family'' of directions which are equivalent due to symmetry operations, such as [100], [010], [001] or the negative of any of those directions.
*Indices in ''curly brackets'' or ''braces'' such as {100} denote a family of plane normals which are equivalent due to symmetry operations, much the way angle brackets denote a family of directions.

For [[face-centered cubic]] and [[body-centered cubic]] lattices, the primitive lattice vectors are not orthogonal.  However, in these cases the Miller indices are conventionally defined relative to the lattice vectors of the cubic [[supercell (crystal)|supercell]] and hence are again simply the Cartesian directions.

==Case of hexagonal and rhombohedral structures==

[[Image:Miller-bravais.svg|thumb|Miller-Bravais indices]]

With [[hexagonal lattice system|hexagonal]] and [[rhombohedral lattice system|rhombohedral]] [[lattice system]]s, it is possible to use the Bravais-Miller system, which uses four indices (''h'' ''k'' ''i'' ''ℓ'') that obey the constraint
: ''h'' + ''k'' + ''i'' = 0.
Here ''h'', ''k'' and ''ℓ'' are identical to the corresponding Miller indices, and ''i'' is a redundant index.

This four-index scheme for labeling planes in a hexagonal lattice makes permutation symmetries apparent.  For example, the similarity between (110) ≡ (11{{overline|2}}0) and (1{{overline|2}}0) ≡ (1{{overline|2}}10) is more obvious when the redundant index is shown.

In the figure at right, the (001) plane has a 3-fold symmetry: it remains unchanged by a rotation of 1/3 (2π/3 rad, 120°). The [100], [010] and the [{{overline|1}}{{overline|1}}0] directions are really similar. If ''S'' is the intercept of the plane with the [{{overline|1}}{{overline|1}}0] axis, then
: ''i'' = 1/''S''.

There are also ''[[ad hoc]]'' schemes (e.g. in the [[transmission electron microscopy]] literature) for indexing hexagonal ''lattice vectors'' (rather than reciprocal lattice vectors or planes) with four indices.  However they don't operate by similarly adding a redundant index to the regular three-index set.

For example, the reciprocal lattice vector (hkℓ) as suggested above can be written in terms of reciprocal lattice vectors as &lt;math&gt;h\mathbf{b_1} + k\mathbf{b_2} + \ell\mathbf{b_3}&lt;/math&gt;. For hexagonal crystals this may be expressed in terms of direct-lattice basis-vectors '''a&lt;sub&gt;1&lt;/sub&gt;''', '''a&lt;sub&gt;2&lt;/sub&gt;''' and '''a&lt;sub&gt;3&lt;/sub&gt;''' as

:&lt;math&gt;h\mathbf{b_1} + k\mathbf{b_2} + \ell \mathbf{b_3}= \frac{2}{3 a^2}(2 h + k)\mathbf{a_1} + \frac{2}{3 a^2}(h+2k)\mathbf{a_2} + \frac{1}{c^2} (\ell) \mathbf{a_3}.&lt;/math&gt;

Hence zone indices of the direction perpendicular to plane (hkℓ) are, in suitably normalized triplet form, simply &lt;math&gt;[2h+k,h+2k,\ell(3/2)(a/c)^2]&lt;/math&gt;.  When ''four indices'' are used for the zone normal to plane (hkℓ), however, the literature often uses &lt;math&gt;[h,k,-h-k,\ell(3/2)(a/c)^2]&lt;/math&gt; instead.&lt;ref&gt;J. W. Edington (1976) ''Practical electron microscopy in materials science'' (N. V. Philips' Gloeilampenfabrieken, Eindhoven) {{ISBN|1-878907-35-2}}, Appendix 2&lt;/ref&gt;  Thus as you can see, four-index zone indices in square or angle brackets sometimes mix a single direct-lattice index on the right with reciprocal-lattice indices (normally in round or curly brackets) on the left.

==Crystallographic planes and directions==

[[Image:Cristal densite surface.svg|thumb|Dense crystallographic planes]]

The crystallographic directions are fictitious [[line (mathematics)|lines]] linking nodes ([[atom]]s, [[ion]]s or [[molecule]]s) of a crystal. Similarly, the crystallographic [[plane (mathematics)|planes]] are fictitious ''planes'' linking nodes. Some directions and planes have a higher density of nodes; these dense planes have an influence on the behaviour of the crystal:
*[[optics|optical properties]]: in condensed matter, the [[light]] "jumps" from one atom to the other with the [[Rayleigh scattering]]; the [[velocity of light]] thus varies according to the directions, whether the atoms are close or far; this gives the [[birefringence]]
*[[adsorption]] and [[reactivity (chemistry)|reactivity]]: the adsorption and the chemical reactions occur on atoms or molecules, these phenomena are thus sensitive to the density of nodes;
*[[surface tension]]: the condensation of a material means that the atoms, ions or molecules are more stable if they are surrounded by other similar species; the surface tension of an interface thus varies according to the density on the surface
**the [[wiktionary:pore|pore]]s and [[crystallite]]s tend to have straight grain boundaries following dense planes
**[[cleavage (crystal)|cleavage]]
*[[dislocation]]s ([[plastic deformation]])
**the dislocation core tends to spread on dense planes (the elastic perturbation is "diluted"); this reduces the [[friction]] ([[Peierls–Nabarro force]]), the sliding occurs more frequently on dense planes;
**the perturbation carried by the dislocation ([[Burgers vector]]) is along a dense direction: the shift of one node in a dense direction is a lesser distortion;
**the dislocation line tends to follow a dense direction, the dislocation line is often a straight line, a dislocation loop is often a [[polygon]].
For all these reasons, it is important to determine the planes and thus to have a notation system.

==Integer vs. irrational Miller indices: Lattice planes and quasicrystals==

Ordinarily, Miller indices are always integers by definition, and this constraint is physically significant.  To understand this, suppose that we allow a plane (abc) where the Miller "indices" ''a'', ''b'' and ''c'' (defined as above) are not necessarily integers.

If ''a'', ''b'' and ''c'' have [[rational number|rational]] ratios, then the same family of planes can be written in terms of integer indices (hkℓ) by scaling ''a'', ''b'' and ''c'' appropriately: divide by the largest of the three numbers, and then multiply by the [[least common denominator]].  Thus, integer Miller indices implicitly include indices with all rational ratios.  The reason why planes where the components (in the reciprocal-lattice basis) have rational ratios are of special interest is that these are the [[lattice plane]]s: they are the only planes whose intersections with the crystal are 2d-periodic.

For a plane (abc) where ''a'', ''b'' and ''c'' have [[irrational number|irrational]] ratios, on the other hand, the intersection of the plane with the crystal is ''not'' periodic.  It forms an aperiodic pattern known as a [[quasicrystal]]. This construction corresponds precisely to the standard "cut-and-project" method of defining a quasicrystal, using a plane with irrational-ratio Miller indices. (Although many quasicrystals, such as the [[Penrose tiling]], are formed by "cuts" of periodic lattices in more than three dimensions, involving the intersection of more than one such [[hyperplane]].)

==See also==

*[[Crystal structure]]
*[[Crystal habit]]
*[[Kikuchi line]]
*[[Zone axis]]

==References==
{{reflist}}

==External links==
*[http://reference.iucr.org/dictionary/Miller_indices IUCr Online Dictionary of Crystallography]
*[https://web.archive.org/web/20060208140059/http://www.ece.byu.edu/cleanroom/EW_orientation.phtml Miller index description with diagrams]
*[http://www.doitpoms.ac.uk/tlplib/miller_indices/index.php Online tutorial about lattice planes and Miller indices].
*[https://mtex-toolbox.github.io/ MTEX&amp;nbsp;– Free MATLAB toolbox for Texture Analysis]
*http://sourceforge.net/projects/orilib &amp;nbsp;– A collection of routines for rotation / orientation manipulation, including special tools for crystal orientations.

{{DEFAULTSORT:Miller Index}}
[[Category:Geometry]]
[[Category:Crystallography]]</text>
      <sha1>8woamtfsifgao37x3kd1bt8ldljh32p</sha1>
    </revision>
  </page>
  <page>
    <title>Mixed linear complementarity problem</title>
    <ns>0</ns>
    <id>23629444</id>
    <revision>
      <id>468936881</id>
      <parentid>302452612</parentid>
      <timestamp>2012-01-01T14:22:33Z</timestamp>
      <contributor>
        <username>Qetuth</username>
        <id>15931398</id>
      </contributor>
      <minor/>
      <comment>more specific stub types</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="791">In mathematical [[optimization (mathematics)|optimization theory]], the '''mixed linear complementarity problem''', often abbreviated as '''MLCP''' or '''LMCP''', is a generalization of the [[linear complementarity problem]] to include [[free variables]].

== References ==
* [http://www.uclm.es/area/gsee/Web/Raquel/Complementarity_Problems.pdf Complementarity problems]
* [ftp://ftp.cs.wisc.edu/math-prog/tech-reports/95-14.ps Algorithms for complementarity problems and generalized equations]
* [http://www.mcs.anl.gov/~leyffer/listn/slides-07/morales.pdf An Algorithm for the Approximate and Fast Solution of Linear Complementarity Problems]

{{Mathematical programming}}

[[Category:Linear algebra]]
[[Category:Mathematical optimization]]


{{Linear-algebra-stub}}
{{mathanalysis-stub}}</text>
      <sha1>du8jb1rji0z27990wqbw42l1eg8udnz</sha1>
    </revision>
  </page>
  <page>
    <title>Mutual exclusivity</title>
    <ns>0</ns>
    <id>312648</id>
    <revision>
      <id>865696397</id>
      <parentid>859730600</parentid>
      <timestamp>2018-10-25T15:35:41Z</timestamp>
      <contributor>
        <ip>2603:3024:204:B00:209F:C8D8:9D15:B856</ip>
      </contributor>
      <comment>The statement incorrectly said that neither one could occur at all.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9143">{{About|logical exclusivity of events and propositions|the concept in concurrent computing|Mutual exclusion|the concept in developmental psychology|Mutual exclusivity (psychology)}}
{{More footnotes|date=October 2009}}

In [[logic]] and [[probability theory]], two events (or propositions) are '''mutually exclusive''' or '''disjoint''' if they cannot both occur at the same time(be true). A clear example is the set of outcomes of a single coin toss, which can result in either heads or tails, but not both.

In the coin-tossing example, both outcomes are, in theory, [[Collectively exhaustive events|collectively exhaustive]], which means that at least one of the outcomes must happen, so these two possibilities together exhaust all the possibilities.&lt;ref&gt;{{cite book |last=Miller |first=Scott |first2=Donald |last2=Childers |title=Probability and Random Processes |location= |publisher=Academic Press |edition=Second |year=2012 |page=8 |isbn=978-0-12-386981-4 |quote=The sample space is the collection or set of 'all possible' distinct (collectively exhaustive and mutually exclusive) outcomes of an experiment. }}&lt;/ref&gt; However, not all mutually exclusive events are collectively exhaustive. For example, the outcomes 1 and 4 of a single roll of a [[six-sided die]] are mutually exclusive (both cannot happen at the same time) but not collectively exhaustive (there are other possible outcomes; 2,3,5,6).

==Logic==
In [[logic]], two mutually exclusive propositions are propositions that [[logical possibility|logically cannot]] be true in the same sense at the same time. To say that more than two propositions are mutually exclusive, depending on context, means that one cannot be true if the other one is true, or at least one of them cannot be true. The term ''pairwise mutually exclusive'' always means that two of them cannot be true simultaneously.

==Probability==
In [[probability theory]], events ''E''&lt;sub&gt;1&lt;/sub&gt;, ''E''&lt;sub&gt;2&lt;/sub&gt;, ..., ''E''&lt;sub&gt;''n''&lt;/sub&gt; are said to be '''mutually exclusive''' if the occurrence of any one of them implies the non-occurrence of the remaining ''n''&amp;nbsp;−&amp;nbsp;1 events. Therefore, two mutually exclusive events cannot both occur. Formally said, the intersection of each two of them is empty (the null event): ''A''&amp;nbsp;∩&amp;nbsp;''B''&amp;nbsp;=&amp;nbsp;∅. In consequence, mutually exclusive events have the property: P(''A'' ∩ ''B'') = 0.&lt;ref&gt;[http://www.intmath.com/Counting-probability/9_Mutually-exclusive-events.php intmath.com]; Mutually Exclusive Events. Interactive Mathematics. December 28, 2008.&lt;/ref&gt;

For example, it is impossible to draw a card that is both red and a club because clubs are always black. If just one card is drawn from the deck, either a red card (heart or diamond) or a black card (club or spade) will be drawn. When ''A'' and ''B'' are mutually exclusive, P(''A'' ∪ ''B'') = P(''A'') + P(''B'').&lt;ref name="rules"&gt;[http://people.richland.edu/james/lecture/m170/ch05-rul.html Stats: Probability Rules.]&lt;/ref&gt; To find the probability of drawing a red card or a club, for example, add together the probability of drawing a red card and the probability of drawing a club. In a standard 52-card deck, there are twenty-six red cards and thirteen clubs: 26/52 + 13/52 = 39/52 or 3/4.

One would have to draw at least two cards in order to draw both a red card and a club. The probability of doing so in two draws depends on whether the first card drawn were replaced before the second drawing, since without replacement there is one fewer card after the first card was drawn. The probabilities of the individual events (red, and club) are multiplied rather than added. The probability of drawing a red and a club in two drawings without replacement is then 26/52&amp;nbsp;×&amp;nbsp;13/51&amp;nbsp;×&amp;nbsp;2 = 676/2652, or 13/51. With replacement, the probability would be 26/52&amp;nbsp;×&amp;nbsp;13/52&amp;nbsp;×&amp;nbsp;2 = 676/2704, or 13/52.

In probability theory, the word ''or'' allows for the possibility of both events happening. The probability of one or both events occurring is denoted P(''A'' ∪ ''B'') and in general it equals P(''A'') + P(''B'') – P(''A'' ∩ ''B'').&lt;ref name="rules" /&gt; Therefore, in the case of drawing a red card or a king, drawing any of a red king, a red non-king, or a black king is considered a success. In a standard 52-card deck, there are twenty-six red cards and four kings, two of which are red, so the probability of drawing a red or a king is 26/52 + 4/52 – 2/52 = 28/52.

Events are [[collectively exhaustive]] if all the possibilities for outcomes are exhausted by those possible events, so at least one of those outcomes must occur. The probability that at least one of the events will occur is equal to one.&lt;ref name="events"&gt;[https://web.archive.org/web/20110720051423/http://www.cs.stedwards.edu/chem/Chemistry/CHEM4341/BayesPrimer2.pdf Scott Bierman. A Probability Primer. Carleton College. Pages 3-4.]&lt;/ref&gt; For example, there are theoretically only two possibilities for flipping a coin. Flipping a head and flipping a tail are collectively exhaustive events, and there is a probability of one of flipping either a head or a tail. Events can be both mutually exclusive and collectively exhaustive.&lt;ref name="events" /&gt; In the case of flipping a coin, flipping a head and flipping a tail are also mutually exclusive events. Both outcomes cannot occur for a single trial (i.e., when a coin is flipped only once). The probability of flipping a head and the probability of flipping a tail can be added to yield a probability of 1: 1/2 + 1/2 =1.&lt;ref&gt;[http://www.cliffsnotes.com/WileyCDA/CliffsReviewTopic/NonMutually-Exclusive-Outcomes.topicArticleId-25951,articleId-25914.html Non-Mutually Exclusive Outcomes. CliffsNotes.]&lt;/ref&gt;

==Statistics==
In [[statistics]] and [[regression analysis]], an [[dependent and independent variables|independent variable]] that can take on only two possible values is called a [[Dummy variable (statistics)|dummy variable]]. For example, it may take on the value 0 if an observation is of a male subject or 1 if the observation is of a female subject. The two possible categories associated with the two possible values are mutually exclusive, so that no observation falls into more than one category, and the categories are exhaustive, so that every observation falls into some category. Sometimes there are three or more possible categories, which are pairwise mutually exclusive and are collectively exhaustive — for example, under 18 years of age, 18 to 64 years of age, and age 65 or above. In this case a set of dummy variables is constructed, each dummy variable having two mutually exclusive and jointly exhaustive categories — in this example, one dummy variable (called D&lt;sub&gt;1&lt;/sub&gt;) would equal 1 if age is less than 18, and would equal 0 ''otherwise''; a second dummy variable (called D&lt;sub&gt;2&lt;/sub&gt;) would equal 1 if age is in the range 18-64, and 0 otherwise. In this set-up, the dummy variable pairs (D&lt;sub&gt;1&lt;/sub&gt;, D&lt;sub&gt;2&lt;/sub&gt;) can have the values (1,0) (under 18), (0,1) (between 18 and 64), or (0,0) (65 or older) (but not (1,1), which would nonsensically imply that an observed subject is both under 18 and between 18 and 64). Then the dummy variables can be included as independent (explanatory) variables in a regression. Note that the number of dummy variables is always one less than the number of categories: with the two categories male and female there is a single dummy variable to distinguish them, while with the three age categories two dummy variables are needed to distinguish them.

Such [[qualitative data]] can also be used for [[dependent variable]]s. For example, a researcher might want to predict whether someone goes to college or not, using family income, a gender dummy variable, and so forth as explanatory variables. Here the variable to be explained is a dummy variable that equals 0 if the observed subject does not go to college and equals 1 if the subject does go to college. In such a situation, [[ordinary least squares]] (the basic regression technique) is widely seen as inadequate; instead [[probit regression]] or [[logistic regression]] is used. Further, sometimes there are three or more categories for the dependent variable — for example, no college, community college, and four-year college. In this case, the [[multinomial probit]] or [[multinomial logit]] technique is used.

==See also==
* [[Contrariety]]
* [[Dichotomy]]
* [[Disjoint sets]]
* [[Double bind]]
* [[Event structure]]
* [[Oxymoron]]
* [[Synchronicity]]

==Notes==
{{Reflist}}

==References==
* {{cite book |title=The Analysis of Biological Data |first=Michael C. |last=Whitlock |first2=Dolph |last2=Schluter |location= |publisher=Roberts and Co. |year=2008 |isbn=978-0-9815194-0-1 }}
* {{cite book |title=Basic Statistics for Business &amp; Economics |edition=4th |first=Douglas A. |last=Lind |first2=William G. |last2=Marchal |first3=Samuel A. |last3=Wathen |location=Boston |publisher=McGraw-Hill |year=2003 |isbn=0-07-247104-2 }}

{{Philosophy topics}}

[[Category:Philosophy of mathematics]]
[[Category:Logic]]
[[Category:Abstraction]]
[[Category:Dichotomies]]</text>
      <sha1>dr1mn4n4f57alrtmm7g6icljbnt679g</sha1>
    </revision>
  </page>
  <page>
    <title>Planar cover</title>
    <ns>0</ns>
    <id>42928696</id>
    <revision>
      <id>842554219</id>
      <parentid>838703821</parentid>
      <timestamp>2018-05-23T05:46:57Z</timestamp>
      <contributor>
        <username>OAbot</username>
        <id>28481209</id>
      </contributor>
      <minor/>
      <comment>[[Wikipedia:OABOT|Open access bot]]: add arxiv identifier to citation with #oabot.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="18337">[[File:Covering-graph-4.svg|thumb|upright=1.35|The graph ''C'' is a planar cover of the graph ''H''. The covering map is indicated by the vertex colors.]]
In [[graph theory]], a '''planar cover''' of a finite [[undirected graph|graph]] ''G'' is a finite [[covering graph]] of ''G'' that is itself a [[planar graph]]. Every graph that can be [[graph embedding|embedded]] into the [[projective plane]] has a planar cover; an unsolved conjecture of  Seiya Negami states that these are the only graphs with planar covers.&lt;ref name="20y-p1"&gt;{{harvtxt|Hliněný|2010}}, p.&amp;nbsp;1&lt;/ref&gt;

The existence of a planar cover is a [[graph minor|minor-closed graph property]],&lt;ref name="20y-prop1"&gt;{{harvtxt|Hliněný|2010}}, Proposition 1, p.&amp;nbsp;2&lt;/ref&gt; and so can be characterized by finitely many [[forbidden graph characterization|forbidden minors]], but the exact set of forbidden minors is not known. For the same reason, there exists a [[polynomial time]] algorithm for testing whether a given graph has a planar cover, but an explicit description of this algorithm is not known.

==Definition==
A ''covering map'' from one graph ''C'' to another graph ''H'' may be described by a function ''f'' from the vertices of ''C'' onto the vertices of ''H'' that, for each vertex ''v'' of ''C'', gives a [[bijection]] between the [[Neighborhood (graph theory)|neighbors]] of ''v'' and the neighbors of ''f''(''v'').&lt;ref&gt;{{harvtxt|Hliněný|2010}}, Definition, p.&amp;nbsp;2&lt;/ref&gt; If ''H'' is a [[connected graph]], each vertex of ''H'' must have the same number of pre-images in ''C'';&lt;ref name="20y-prop1"/&gt; this number is called the ''ply'' of the map, and ''C'' is called a [[covering graph]] of ''G''. If ''C'' and ''H'' are both finite, and ''C'' is a [[planar graph]], then ''C'' is called a planar cover of ''H''.

==Examples==
[[File:Dodecahedron.png|thumb|Identifying pairs of opposite vertices of the [[dodecahedron]] gives a covering map to the [[Petersen graph]]]]
The graph of the [[dodecahedron]] has a [[graph automorphism|symmetry]] that maps each vertex to the antipodal vertex. The set of antipodal pairs of vertices and their adjacencies can itself be viewed as a graph, the [[Petersen graph]]. The dodecahedron forms a planar cover of this nonplanar graph.&lt;ref&gt;{{harvtxt|Inkmann|Thomas|2011}}: "This construction is illustrated in Figure 1, where the dodecahedron is shown to be a planar double cover of the Petersen graph."&lt;/ref&gt; As this example shows, not every graph with a planar cover is itself planar. However, when a planar graph covers a non-planar one, the ply must be an [[even number]].&lt;ref&gt;{{harvtxt|Archdeacon|Richter|1990}}; {{harvtxt|Negami|2003}}.&lt;/ref&gt;

[[File:Dodecagonal prism.png|thumb|The [[dodecagonal prism]] can form a 2-ply cover of the [[hexagonal prism]], a 3-ply cover of the [[cube]], or a 4-ply cover of the [[triangular prism]].]]
The graph of a ''k''-gonal [[Prism (geometry)|prism]] has 2''k'' vertices, and is planar with two ''k''-gon faces and ''k'' quadrilateral faces. If ''k''&amp;nbsp;=&amp;nbsp;''ab'', with ''a''&amp;nbsp;≥&amp;nbsp;2 and ''b''&amp;nbsp;≥&amp;nbsp;3, then it has an ''a''-ply covering map to a ''b''-fonal prism, in which two vertices of the ''k''-prism are mapped to the same vertex of the ''b''-prism if they both belong to the same ''k''-gonal face and the  distance from one to the other is a multiple of&amp;nbsp;''b''. For instance, the [[dodecagonal prism]] can form a 2-ply cover of the [[hexagonal prism]], a 3-ply cover of the [[cube]], or a 4-ply cover of the [[triangular prism]]. These examples show that a graph may have many different planar covers, and may be the planar cover for many other graphs. Additionally they show that the ply of a planar cover may be arbitrarily large.
They are not the only covers involving prisms: for instance, the hexagonal prism can also cover a non-planar graph, the [[utility graph]] ''K''&lt;sub&gt;3,3&lt;/sub&gt;, by identifying antipodal pairs of vertices.&lt;ref&gt;{{harvtxt|Zelinka|1982}}&lt;/ref&gt;

==Cover-preserving operations==
If a graph ''H'' has a planar cover, so does every [[graph minor]] of ''H''.&lt;ref name="20y-prop1"/&gt; A minor ''G'' of ''H'' may be formed by deleting edges and vertices from ''H'', and by contracting edges of ''H''. The covering graph ''C'' can be transformed in the same way: for each deleted edge or vertex in ''H'', delete its preimage in ''C'', and for each contracted edge or vertex in ''H'', contract its preimage in ''C''. The result of applying these operations to ''C'' is a minor of ''C'' that covers ''G''. Since every minor of a planar graph is itself planar, this gives a planar cover of the minor ''G''.

Because the graphs with planar covers are closed under the operation of taking minors, it follows from the [[Robertson–Seymour theorem]] that they may be characterized by a finite set of [[forbidden graph characterization|forbidden minors]].&lt;ref&gt;{{harvtxt|Robertson|Seymour|2004}}&lt;/ref&gt; A graph is a forbidden minor for this property if it has no planar cover, but all of its minors do have planar covers. This characterization can be used to prove the existence of a [[polynomial time]] algorithm that tests for the existence of a planar cover, by searching for each of the forbidden minors and returning that a planar cover exists only if this search fails to find any of them.&lt;ref&gt;{{harvtxt|Robertson|Seymour|1995}}&lt;/ref&gt; However, because the exact set of forbidden minors for this property is not known, this proof of existence is [[non-constructive]], and does not lead to an explicit description of the set of forbidden minors or of the algorithm based on them.&lt;ref&gt;{{harvtxt|Fellows|Langston|1988}}; {{harvtxt|Fellows|Koblitz|1992}}. The non-constructivity of algorithmically testing the existence of ''k''-fold planar covers is given explicitly as an example by Fellows and Koblitz.&lt;/ref&gt;

Another [[Graph operations|graph operation]] that preserves the existence of a planar cover is the [[Y-Δ transform]], which replaces any degree-three vertex of a graph ''H'' by a triangle connecting its three neighbors.&lt;ref name="20y-prop1"/&gt; However, the reverse of this transformation, a Δ-Y transform, does not necessarily preserve planar covers.

Additionally, a [[disjoint union]] of two graphs that have covers will also have a cover, formed as the disjoint union of the covering graphs. If the two covers have the same ply as each other, then this will also be the ply of their union.

==Negami's conjecture==
{{unsolved|mathematics|Does every connected graph with a planar cover have an embedding into the projective plane?}}
If a graph ''H'' has an [[graph embedding|embedding]] into the [[projective plane]], then it necessarily has a planar cover, given by the preimage of ''H'' in the [[orientable double cover]] of the projective plane, which is a sphere.
{{harvtxt|Negami|1986}} proved, conversely, that if a [[connected graph]] ''H'' has a two-ply planar cover then ''H'' must have an embedding into the projective plane.&lt;ref&gt;{{harvtxt|Negami|1986}}; {{harvtxt|Hliněný|2010}}, Theorem&amp;nbsp;2, p.&amp;nbsp;2&lt;/ref&gt; The assumption that ''H'' is connected is necessary here, because a disjoint union of projective-planar graphs may not itself be projective-planar&lt;ref&gt;For instance, the two [[Kuratowski's theorem|Kuratowski graphs]] are projective-planar but any union of two of them is not {{harv|Archdeacon |1981}}.&lt;/ref&gt; but will still have a planar cover, the disjoint union of the orientable double covers.

A ''regular cover'' of a graph ''H'' is one that comes from a [[Automorphism group|group of symmetries]] of its covering graph: the preimages of each vertex in ''H'' are an [[Orbit (group theory)|orbit]] of the group. {{harvtxt|Negami|1988}} proved that every connected graph with a planar regular cover can be embedded into the projective plane.&lt;ref&gt;{{harvtxt|Negami|1988}}; {{harvtxt|Hliněný|2010}}, Theorem&amp;nbsp;3, p.&amp;nbsp;3&lt;/ref&gt; Based on these two results, he conjectured that in fact every connected graph with a planar cover is projective.&lt;ref&gt;{{harvtxt|Negami|1988}}; {{harvtxt|Hliněný|2010}}, Conjecture&amp;nbsp;4, p.&amp;nbsp;4&lt;/ref&gt;
As of 2013, this conjecture remains unsolved.&lt;ref&gt;{{harvtxt|Chimani|Derka|Hliněný|Klusáček|2013}}&lt;/ref&gt; It is also known as Negami's "1-2-∞ conjecture", because it can be reformulated as stating that the minimum ply of a planar cover, if it exists, must be either 1 or&amp;nbsp;2.&lt;ref&gt;{{harvtxt|Huneke|1993}}&lt;/ref&gt;

[[File:Octahedral pyramid.png|thumb|''K''&lt;sub&gt;1,2,2,2&lt;/sub&gt;, the only possible minimal counterexample to Negami's conjecture]]
Like the graphs with planar covers, the graphs with projective plane embeddings can be characterized by forbidden minors. In this case the exact set of forbidden minors is known: there are 35 of them. 32 of these are connected, and one of these 32 graphs necessarily appears as a minor in any connected non-projective-planar graph.&lt;ref&gt;{{harvtxt|Hliněný|2010}}, p.&amp;nbsp;4; the list of forbidden projective-planar minors is from {{harvtxt|Archdeacon|1981}}. {{harvtxt|Negami|1988}} instead stated the corresponding observation for the 103 irreducible non-projective-planar graphs given by {{harvtxt|Glover|Huneke|Wang|1979}}.&lt;/ref&gt; Since Negami made his conjecture, it has been proven that 31 of these 32 forbidden minors either do not have planar covers, or can be reduced by Y-Δ transforms to a simpler graph on this list.&lt;ref&gt;{{harvtxt|Negami|1988}}; {{harvtxt|Huneke|1993}}; {{harvtxt|Hliněný|1998}}; {{harvtxt|Archdeacon|2002}}; {{harvtxt|Hliněný|2010}}, pp.&amp;nbsp;4–6&lt;/ref&gt; The one remaining graph for which this has not yet been done is ''K''&lt;sub&gt;1,2,2,2&lt;/sub&gt;, a seven-vertex [[apex graph]] that forms the skeleton of a four-dimensional [[octahedral pyramid]]. If ''K''&lt;sub&gt;1,2,2,2&lt;/sub&gt; could be shown not to have any planar covers, this would complete a proof of the conjecture. On the other hand, if the conjecture is false, ''K''&lt;sub&gt;1,2,2,2&lt;/sub&gt; would necessarily be its smallest counterexample.&lt;ref&gt;{{harvtxt|Hliněný|2010}}, pp.&amp;nbsp;6–9&lt;/ref&gt;

A related conjecture by [[Michael Fellows]], now solved, concerns planar ''emulators'', a generalization of planar covers that maps graph neighborhoods surjectively rather than bijectively.&lt;ref&gt;{{harvtxt|Fellows|1985}}; {{harvtxt|Kitakubo|1991}}; {{harvtxt|Hliněný|2010}}, Definition, p.&amp;nbsp;9&lt;/ref&gt; The graphs with planar emulators, like those with planar covers, are closed under minors and Y-Δ transforms.&lt;ref&gt;{{harvtxt|Hliněný|2010}}, Proposition&amp;nbsp;13, p.&amp;nbsp;9. Hliněný credits this to Fellows and writes that its proof is nontrivial.&lt;/ref&gt; In 1988, independently of Negami, Fellows conjectured that the graphs with planar emulators are exactly the graphs that can be embedded into the projective plane.&lt;ref&gt;{{harvtxt|Hliněný|2010}}, Conjecture&amp;nbsp;14, p.&amp;nbsp;9&lt;/ref&gt; The conjecture is true for ''regular'' emulators, coming from automomorphisms of the underlying covering graph, by a result analogous to the result of {{harvtxt|Negami|1988}} for regular planar covers.&lt;ref&gt;{{harvtxt|Kitakubo|1991}}.&lt;/ref&gt;
However, several of the 32 connected forbidden minors for projective-planar graphs turn out to have planar emulators.&lt;ref&gt;{{harvtxt|Hliněný|2010}}, pp.&amp;nbsp;9–10; {{harvtxt|Rieck|Yamashita|2010}}; {{harvtxt|Chimani|Derka|Hliněný|Klusáček|2013}}&lt;/ref&gt; Therefore, Fellows' conjecture is false. Finding a full set of forbidden minors for the existence of planar emulators remains an open problem.&lt;ref&gt;{{harvtxt|Hliněný|2010}}, p.&amp;nbsp;10&lt;/ref&gt;

==Notes==
{{reflist|30em}}

==References==
{{refbegin|30em}}
===Secondary sources about Negami's conjecture===
*{{citation
 | last = Hliněný | first = Petr
 | doi = 10.1007/s00373-010-0934-9
 | issue = 4
 | journal = [[Graphs and Combinatorics]]
 | mr = 2669457
 | pages = 525–536
 | title = 20 years of Negami's planar cover conjecture
 | url = http://www.fi.muni.cz/~hlineny/papers/plcover20-gc.pdf
 | volume = 26
 | year = 2010}}. Page numbers in notes refer to the preprint version.
*{{citation
 | last = Huneke | first = John Philip
 | contribution = A conjecture in topological graph theory
 | doi = 10.1090/conm/147/01186
 | mr = 1224718
 | pages = 387–389
 | publisher = American Mathematical Society | location = Providence, RI
 | series = Contemporary Mathematics
 | title = Graph Structure Theory (Seattle, WA, 1991)
 | volume = 147
 | year = 1993}}.

===Primary sources about planar covers===
*{{citation
 | last = Archdeacon | first = Dan | authorlink = Dan Archdeacon
 | doi = 10.1002/jgt.10075
 | issue = 4
 | journal = [[Journal of Graph Theory]]
 | mr = 1936947
 | pages = 318–326
 | title = Two graphs without planar covers
 | volume = 41
 | year = 2002}}.
*{{citation
 | last1 = Archdeacon | first1 = Dan | author1-link = Dan Archdeacon
 | last2 = Richter | first2 = R. Bruce
 | doi = 10.1002/jgt.3190140208
 | issue = 2
 | journal = [[Journal of Graph Theory]]
 | mr = 1053603
 | pages = 199–204
 | title = On the parity of planar covers
 | volume = 14
 | year = 1990}}.
*{{citation
 | last1 = Chimani | first1 = Markus
 | last2 = Derka | first2 = Martin
 | last3 = Hliněný | first3 = Petr
 | last4 = Klusáček | first4 = Matěj
 | doi = 10.1016/j.aam.2012.06.004
 | issue = 1
 | journal = [[Advances in Applied Mathematics]]
 | mr = 2996383
 | pages = 46–68
 | title = How not to characterize planar-emulable graphs
 | volume = 50
 | year = 2013| arxiv = 1107.0176}}.
*{{citation
 | last = Hliněný | first = Petr
 | doi = 10.1002/(SICI)1097-0118(199801)27:1&lt;51::AID-JGT8&gt;3.3.CO;2-S
 | issue = 1
 | journal = [[Journal of Graph Theory]]
 | mr = 1487786
 | pages = 51–60
 | title = ''K''&lt;sub&gt;4,4&lt;/sub&gt;&amp;nbsp;&amp;minus;&amp;nbsp;''e'' has no finite planar cover
 | volume = 27
 | year = 1998}}.
*{{citation
 | last1 = Inkmann | first1 = Torsten
 | last2 = Thomas | first2 = Robin | author2-link = Robin Thomas (mathematician)
 | doi = 10.1017/S0963548310000283
 | issue = 1
 | journal = [[Combinatorics, Probability and Computing]]
 | mr = 2745678
 | pages = 73–82
 | title = Minor-minimal planar graphs of even branch-width
 | volume = 20
 | year = 2011| arxiv = 1007.0373}}.
*{{citation
 | last = Kitakubo | first = Shigeru
 | issue = 2
 | journal = Yokohama Mathematical Journal
 | mr = 1105068
 | pages = 113–120
 | title = Planar branched coverings of graphs
 | volume = 38
 | year = 1991}}.
*{{citation
 | last = Negami | first = Seiya
 | doi = 10.1016/0012-365X(86)90217-7
 | issue = 3
 | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]
 | mr = 866945
 | pages = 299–306
 | title = Enumeration of projective-planar embeddings of graphs
 | volume = 62
 | year = 1986}}.
*{{citation
 | last = Negami | first = Seiya
 | doi = 10.1016/0012-365X(88)90090-8
 | issue = 2
 | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]
 | mr = 949775
 | pages = 159–168
 | title = The spherical genus and virtually planar graphs
 | volume = 70
 | year = 1988}}.
*{{citation
 | last = Negami | first = Seiya
 | doi = 10.1016/S0012-365X(02)00689-1
 | issue = 1-3
 | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]
 | mr = 1983279
 | pages = 207–216
 | title = Composite planar coverings of graphs
 | volume = 268
 | year = 2003}}.
*{{citation
 | last1 = Rieck | first1 = Yo'av
 | last2 = Yamashita | first2 = Yasushi
 | doi = 10.1016/j.ejc.2009.06.003
 | issue = 3
 | journal = [[European Journal of Combinatorics]]
 | mr = 2587038
 | pages = 903–907
 | title = Finite planar emulators for ''K''&lt;sub&gt;4,5&lt;/sub&gt;&amp;nbsp;&amp;minus;&amp;nbsp;4''K''&lt;sub&gt;2&lt;/sub&gt; and ''K''&lt;sub&gt;1,2,2,2&lt;/sub&gt; and Fellows' conjecture
 | volume = 31
 | year = 2010}}.

===Supporting literature===
*{{citation
 | last = Archdeacon | first = Dan
 | doi = 10.1002/jgt.3190050305
 | issue = 3
 | journal = [[Journal of Graph Theory]]
 | mr = 625065
 | pages = 243–246
 | title = A Kuratowski theorem for the projective plane
 | volume = 5
 | year = 1981}}
*{{citation
 | last = Fellows | first = Michael R. | author-link = Michael Fellows
 | publisher = Univ. of California, San Diego
 | series = Ph.D. thesis
 | title = Encoding Graphs in Graphs
 | year = 1985}}.
*{{citation
 | last1 = Fellows | first1 = Michael R. | author1-link = Michael Fellows
 | last2 = Koblitz | first2 = Neal | author2-link = Neal Koblitz
 | doi = 10.1007/BF00141967
 | issue = 3
 | journal = Designs, Codes and Cryptography
 | mr = 1181730
 | pages = 231–235
 | title = Self-witnessing polynomial-time complexity and prime factorization
 | volume = 2
 | year = 1992}}.
*{{citation
 | last1 = Fellows | first1 = Michael R. | author1-link = Michael Fellows
 | last2 = Langston | first2 = Michael A. | author2-link = Michael Langston
 | doi = 10.1145/44483.44491
 | issue = 3
 | journal = [[Journal of the ACM]]
 | pages = 727–739
 | title = Nonconstructive tools for proving polynomial-time decidability
 | volume = 35
 | year = 1988}}.
*{{citation
 | last1 = Glover | first1 = Henry H.
 | last2 = Huneke | first2 = John P.
 | last3 = Wang | first3 = Chin San
 | doi = 10.1016/0095-8956(79)90022-4
 | issue = 3
 | journal = [[Journal of Combinatorial Theory]]
 | mr = 554298
 | pages = 332–370
 | series = Series B
 | title = 103 graphs that are irreducible for the projective plane
 | volume = 27
 | year = 1979}}.
*{{citation
 | last1 = Robertson | first1 = Neil | author1-link = Neil Robertson (mathematician)
 | last2 = Seymour | first2 = Paul | author2-link = Paul Seymour (mathematician)
 | doi = 10.1006/jctb.1995.1006
 | issue = 1
 | journal = [[Journal of Combinatorial Theory]] | series = Series B
 | pages = 65–110
 | title = Graph Minors. XIII. The disjoint paths problem
 | volume = 63
 | year = 1995}}.
*{{citation
 | last1 = Robertson | first1 = Neil | author1-link = Neil Robertson (mathematician)
 | last2 = Seymour | first2 = Paul | author2-link = Paul Seymour (mathematician)
 | doi = 10.1016/j.jctb.2004.08.001
 | issue = 2
 | journal = [[Journal of Combinatorial Theory]] | series = Series B
 | pages = 325–357
 | title = Graph Minors. XX. Wagner's conjecture
 | volume = 92
 | year = 2004}}.
*{{citation
 | last = Zelinka | first = Bohdan
 | issue = 1
 | journal = Mathematica Slovaca
 | mr = 648219
 | pages = 49–54
 | title = On double covers of graphs
 | url = http://dml.cz/dmlcz/129125
 | volume = 32
 | year = 1982}}.
{{refend}}

[[Category:Graph theory objects]]
[[Category:Graph minor theory]]</text>
      <sha1>dv7vqjwcsme895l28k1x30gg03xhmce</sha1>
    </revision>
  </page>
  <page>
    <title>Quantization (physics)</title>
    <ns>0</ns>
    <id>25182</id>
    <revision>
      <id>869028920</id>
      <parentid>856023765</parentid>
      <timestamp>2018-11-15T23:34:29Z</timestamp>
      <contributor>
        <username>Hassanjolany1984</username>
        <id>35057713</id>
      </contributor>
      <comment>/* References */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10793">{{for|quantization of general classical theories|Canonical quantization}}
{{About|quantization in quantum field theory||Quantization (disambiguation){{!}}Quantization}}
{{Quantum field theory}}
In [[physics]], '''quantization''' is the process of transition from a classical understanding of physical phenomena to a newer understanding known as [[quantum mechanics]].  (It is a procedure for constructing a [[quantum field theory]] starting from a classical [[field (physics)|field theory]].) This is a generalization of the procedure for building [[quantum mechanics]] from [[classical mechanics]]. One also speaks of '''field quantization''', as in the "quantization of the [[electromagnetic field]]", where one refers to [[photons]] as field "[[Quantum|quanta]]" (for instance as [[light quantum|light quanta]]). This procedure is basic to theories of [[particle physics]], [[nuclear physics]], [[condensed matter physics]], and [[quantum optics]].

== Quantization methods ==
Quantization converts classical [[field (physics)|field]]s into operators acting on [[quantum state]]s of the field theory. The lowest energy state is called the [[vacuum state]]. The reason for quantizing a theory is to deduce properties of materials, objects or particles through the computation of [[probability amplitude|quantum amplitudes]], which may be very complicated. Such computations have to deal with certain subtleties called [[renormalization]], which, if neglected, can often lead to nonsense results, such as the appearance of infinities in various amplitudes. The full specification of a quantization procedure requires methods of performing renormalization.

The first method to be developed for quantization of field theories was [[canonical quantization]]. While this is extremely easy to implement on sufficiently simple theories, there are many situations where other methods of quantization yield more efficient procedures for computing quantum amplitudes. However, the use of [[canonical quantization]] has left its mark on the language and interpretation of quantum field theory.

===Canonical quantization ===
{{main|canonical quantization}}

Canonical quantization of a field theory is analogous to the construction of [[quantum mechanics]] from [[classical mechanics]]. The classical [[field (physics)|field]] is treated as a dynamical variable called the [[canonical coordinate]], and its time-derivative is the [[canonical momentum]]. One introduces a [[commutation relation]] between these which is exactly the same as the commutation relation between a particle's position and momentum in [[quantum mechanics]]. Technically, one converts the field to an operator, through combinations of [[creation and annihilation operators]]. The [[field operator]] acts on [[quantum state]]s of the theory. The lowest energy state is called the [[vacuum state]]. The procedure is also called [[second quantization]].

This procedure can be applied to the quantization of any [[field (physics)|field]] theory: whether of [[fermion]]s or [[boson]]s, and with any [[internal symmetry]]. However, it leads to a fairly simple picture of the [[vacuum state]] and is not easily amenable to use in some [[quantum field theory|quantum field theories]], such as [[quantum chromodynamics]] which is known to have a [[QCD vacuum|complicated vacuum]] characterized by many different [[vacuum expectation value|condensates]].

===Quantization schemes===
Even within the setting of canonical quantization, there is difficulty associated to quantizing arbitrary observables on the classical phase space. This is the ''ordering ambiguity'': Classically the position and momentum variables ''x'' and ''p'' commute, but their quantum mechanical counterparts do not. Various ''quantization schemes'' have been proposed to resolve this ambiguity,&lt;ref&gt;{{harvnb|Hall|2013}} Chapter 13&lt;/ref&gt; of which the most popular is the [[Wigner–Weyl transform|Weyl quantization scheme]]. Nevertheless, the ''Groenewold–van Hove theorem'' says that no perfect quantization scheme exists. Specifically, if the quantizations of ''x'' and ''p'' are taken to be the usual position and momentum operators, then no quantization scheme can perfectly reproduce the Poisson bracket relations among the classical observables.&lt;ref&gt;{{harvnb|Hall|2013}} Theorem 13.13&lt;/ref&gt; See [[Canonical quantization#Groenewold.27s theorem|Groenewold's theorem]] for one version of this result.

=== Covariant canonical quantization ===

There is a way to perform a canonical quantization without having to resort to the non covariant approach of foliating spacetime and choosing a [[Hamiltonian (quantum mechanics)|Hamiltonian]]. This method is based upon a classical action, but is different from the functional integral approach.

The method does not apply to all possible actions (for instance, actions with a noncausal structure or actions with [[analysis of flows|gauge "flows"]]). It starts with the classical algebra of all (smooth) functionals over the configuration space. This algebra is quotiented over by the ideal generated by the [[Euler–Lagrange equation]]s. Then, this quotient algebra is converted into a Poisson algebra by introducing a Poisson bracket derivable from the action, called the [[Peierls bracket]]. This Poisson algebra is then &lt;math&gt;\hbar&lt;/math&gt;-deformed in the same way as in canonical quantization.

There is also a way to quantize actions with [[analysis of flows|gauge "flows"]]. It involves the [[Batalin–Vilkovisky formalism]], an extension of the [[BRST formalism]].

=== Deformation quantization ===
{{main| phase space formulation}}
{{see also|Weyl quantization|Moyal product|Wigner quasi-probability distribution}}

=== Geometric quantization ===
{{main|Geometric quantization}}

In mathematical physics, geometric quantization is a mathematical approach to defining a quantum theory corresponding to a given classical theory. It attempts to carry out quantization, for which there is in general no exact recipe, in such a way that certain analogies between the classical theory and the quantum theory remain manifest. For example, the similarity between the Heisenberg equation in the Heisenberg picture of quantum mechanics and the Hamilton equation in classical physics should be built in.

One of the earliest attempts at a natural quantization was Weyl quantization, proposed by Hermann Weyl in 1927. Here, an attempt is made to associate a quantum-mechanical observable (a self-adjoint operator on a Hilbert space) with a real-valued function on classical phase space. The position and momentum in this phase space are mapped to the generators of the Heisenberg group, and the Hilbert space appears as a group representation of the Heisenberg group. In 1946, H. J. Groenewold &lt;ref&gt;H.J. Groenewold, "On the Principles of elementary quantum mechanics", Physica,12 (1946) pp. 405–460&lt;/ref&gt; considered the product of a pair of such observables and asked what the corresponding function would be on the classical phase space. This led him to discover the phase-space star-product of a pair of functions.
More generally, this technique leads to deformation quantization, where the ★-product is taken to be a deformation of the algebra of functions on a symplectic manifold or Poisson manifold. However, as a natural quantization scheme (a functor), Weyl's map is not satisfactory. For example, the Weyl map of the classical angular-momentum-squared is not just the quantum angular momentum squared operator, but it further contains a constant term 3ħ2/2. (This extra term is actually physically significant, since it accounts for the nonvanishing angular momentum of the ground-state Bohr orbit in the hydrogen atom.&lt;ref&gt;Dahl, J.; Schleich, W. (2002). "Concepts of radial and angular kinetic energies". Physical Review A 65 (2). doi:10.1103/PhysRevA.65.022109.&lt;/ref&gt; As a mere representation change, however, Weyl's map underlies the alternate Phase space formulation of conventional quantum mechanics.

A more geometric approach to quantization, in which the classical phase space can be a general symplectic manifold, was developed in the 1970s by [[Bertram Kostant]] and [[Jean-Marie Souriau]]. The method proceeds in two stages. First, once constructs a "prequantum Hilbert space" consisting of square-integrable functions (or, more properly, sections of a line bundle) over the phase space. Here one can construct operators satisfying commutation relations corresponding exactly to the classical Poisson-bracket relations. On the other hand, this prequantum Hilbert space is too big to be physically meaningful. One then restricts to functions (or sections) depending on half the variables on the phase space, yielding the quantum Hilbert space.

=== Loop quantization ===

See [[Loop quantum gravity]].

=== Path integral quantization ===
{{further|Path integral formulation}}
A classical mechanical theory is given by an [[action (physics)|action]] with the permissible configurations being the ones which are extremal with respect to functional [[Calculus of variations|variations]] of the action. A quantum-mechanical description of the classical system can also be constructed from the action of the system by means of the [[path integral formulation]].

===Quantum statistical mechanics approach ===

See [[Uncertainty principle]]

=== Schwinger's variational approach ===

See [[Schwinger's quantum action principle]]

== See also ==
* [[First quantization]]
* [[Feynman path integral]]
* [[Light front quantization]]
* [[Photon polarization]]
* [[Quantum Hall effect]]
* [[Quantum number]]

== References ==
* Abraham, R. &amp; Marsden (1985): ''Foundations of Mechanics'', ed. Addison–Wesley, {{ISBN|0-8053-0102-X}}.
* G. Giachetta, L. Mangiarotti, [[Gennadi Sardanashvily|G. Sardanashvily]], ''Geometric and Algebraic Topological Methods in Quantum Mechanics'' (World Scientific, 2005) {{ISBN|981-256-129-3}}.
* {{citation|first=Brian C.|last=Hall|title=Quantum Theory for Mathematicians|series=Graduate Texts in Mathematics|volume=267 |publisher=Springer|year=2013}}
* M. Peskin, D. Schroeder, ''An Introduction to Quantum Field Theory'' (Westview Press, 1995) {{ISBN|0-201-50397-2}}
* Weinberg, Steven, ''The Quantum Theory of Fields'' (3 volumes)
*Ali, S. T., &amp; Engliš, M. (2005). "Quantization methods: a guide for physicists and analysts". ''Reviews in Mathematical Physics''  '''17''' (04), 391-490. [https://arxiv.org/pdf/math-ph/0405065.pdf arXiv preprint]
* Todorov, Ivan (2012). "Quantization is a mystery." arXiv preprint arXiv:1206.3116 (2012).

==Notes==
&lt;references/&gt;

[[Category:Concepts in physics]]
[[Category:Theoretical physics]]
[[Category:Quantum mechanics]]
[[Category:Quantum field theory]]
[[Category:Mathematical quantization]]
[[Category:Mathematical physics]]</text>
      <sha1>6s9xayvmkv46in9cvk389jhjfpcakhq</sha1>
    </revision>
  </page>
  <page>
    <title>Rajeev Motwani</title>
    <ns>0</ns>
    <id>16989288</id>
    <revision>
      <id>857759079</id>
      <parentid>833750389</parentid>
      <timestamp>2018-09-02T20:49:37Z</timestamp>
      <contributor>
        <ip>2601:647:5880:323C:C1A4:BC8C:821:52C</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11724">{{Infobox scientist
| name = 
| native_name = 
| native_name_lang = 
| image = Rajeev Motwani in 2006.jpg
| image_size = 
| alt = 
| caption = Rajeev  Motwani in 2006
| birth_date = {{Birth date|1962|03|24}}
| birth_place = [[Jammu (city)|Jammu]], [[India]]
| death_date = {{Death date and age|2009|06|05|1962|03|26}}
| death_place = [[Atherton, California]], [[United States]]
| other_names = 
| residence = 
| citizenship = 
| nationality = 
| fields = [[theoretical computer science]]&lt;br&gt;[[data privacy]]&lt;br&gt;[[web search]]&lt;br&gt;[[robotics]]&lt;br&gt;[[drug design|computational drug design]]
| workplaces = 
| alma_mater = 
| thesis_title = Probabilistic Analysis of Matching and network flow Algorithms
| thesis_url = http://dl.acm.org/citation.cfm?id=915301
| thesis_year = 1988
| doctoral_advisor = [[Richard M. Karp]]&lt;ref name="mathgene"&gt;{{MathGenealogy|id=41471}}&lt;/ref&gt;
| doctoral_students = &lt;!-- Bluelinks only here, please --&gt;{{plainlist|1=
*[[Moses Charikar]]
*[[Piotr Indyk]]
*[[David Karger]]
*[[Sanjeev Khanna]]
*[[Suresh Venkatasubramanian]]
}}
| notable_students = 
| known_for = 
| awards = [[Gödel Prize]]
| website = {{URL|http://theory.stanford.edu/~rajeev}}
| footnotes = 
| spouse = [[Asha Jadeja Motwani]]
}}

'''Rajeev Motwani''' ({{lang-hi|राजीव मोटवानी}}; March 26, 1962 – June 5, 2009) was a professor of Computer Science at [[Stanford University]] whose research focused on [[theoretical computer science]]. He was an early advisor and supporter of companies including [[Google]] and [[PayPal]], and a special advisor to [[Sequoia Capital]]. He was a winner of the [[Gödel Prize]] in 2001.&lt;ref name="dblp"&gt;{{DBLP|name=Rajeev Motwani}}&lt;/ref&gt;&lt;ref name="acm"&gt;{{ACMPortal|id=81100047334}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal | last1 = Raghavan | first1 = Prabhakar| authorlink1 = Prabhakar Raghavan| title = Rajeev Motwani (1962-2009)| journal = Theory of Computing | volume = 8 | pages = 55–57 | doi = 10.4086/toc.2012.v008a003 | url = http://www.theoryofcomputing.org/articles/v008a003/v008a003.pdf| year = 2012 | pmid =  | pmc = }}&lt;/ref&gt;

==Education==
Rajeev Motwani was born in [[Jammu (city)|Jammu]] and grew up in [[New Delhi]].&lt;ref&gt;[http://news.stanford.edu/news/2009/june10/rajeev_motwani-061009.html Rajeev Motwani, computer scientist at Stanford; adviser, investor in Silicon Valley, dead at 47]&lt;/ref&gt; His father was in the [[Indian Army]]. He had two brothers. As a child, inspired by luminaries like [[Gauss]], he wanted to become a [[mathematician]].
Motwani went to [[St Columba's School (Delhi)|St Columba's School, New Delhi]]. He completed his B.Tech in [[Computer Science]] from the [[Indian Institute of Technology Kanpur]] in 1983 and got his [[Ph.D.]] in Computer Science from the [[University of California, Berkeley]] in 1988 under the supervision of [[Richard M. Karp]].&lt;ref name="mathgene"/&gt;

==Career==
Motwani joined Stanford soon after U.C. Berkeley. 
He founded the Mining Data at Stanford project (MIDAS), an umbrella organization for several groups looking into new and innovative data management concepts. His research included [[data privacy]], [[web search]], [[robotics]], and [[Drug design#Computer-assisted drug design|computational drug design]].  He is also one of the originators of the [[Locality-sensitive hashing]] algorithm.

Motwani was one of the co-authors (with [[Larry Page]] and [[Sergey Brin]], and [[Terry Winograd]]) of an influential early paper on the [[PageRank]] [[algorithm]].  He also co-authored another seminal search paper ''What Can You Do With A Web In Your Pocket'' with those same authors.&lt;ref&gt;{{Cite journal
 |first1      = Sergey
 |last1       = Brin
 |first2      = Rajeev
 |last2       = Motwani
 |first3      = Lawrence
 |last3       = Page
 |first4      = Terry
 |last4       = Winograd
 |title       = What can you do with a Web in your Pocket?
 |journal     = IEEE Data Engineering Bulletin
 |volume      = 21
 |issue       = 2
 |year        = 1998
 |pages       = 37–47
 |url         = http://www.informatik.uni-trier.de/~ley/db/journals/debu/BrinMPW98.html
 |postscript  = &lt;!--None--&gt;
 |deadurl     = yes
 |archiveurl  = https://web.archive.org/web/20090610212202/http://www.informatik.uni-trier.de/~ley/db/journals/debu/BrinMPW98.html
 |archivedate = 2009-06-10
 |df          = 
}}&lt;/ref&gt;
PageRank was the basis for search techniques of Google (founded by Page and Brin), and Motwani advised or taught many of Google's developers and researchers,&lt;ref&gt;{{Cite web |title= Remembering Rajeev Motwani |author= [[Alfred Spector]], VP of Research |publisher= Google |date= June 8, 2009  |url= http://googleresearch.blogspot.com/2009/06/remembering-rajeev-motwani.html |accessdate= September 11, 2013 }}&lt;/ref&gt; including the first employee, [[Craig Silverstein]].&lt;ref name="csilvers"&gt;{{cite web|url=http://www-cs-students.stanford.edu/~csilvers/ |title=Craig Silverstein's website |archiveurl=https://web.archive.org/web/19991002122809/http://www-cs-students.stanford.edu/~csilvers/ |archivedate=October 2, 1999 |publisher=Stanford University |accessdate=October 12, 2010 |deadurl=yes |df= }}&lt;/ref&gt;

He was an author of two widely used theoretical computer science textbooks: ''Randomized Algorithms'' with [[Prabhakar Raghavan]]&lt;ref name="random"&gt;{{cite book |author1=Raghavan, Prabhakar |author2=Motwani, Rajeev |title=Randomized algorithms |publisher=Cambridge University Press |location=Cambridge, UK |year=1995 |isbn=0-521-47465-5 }}&lt;/ref&gt; and ''[[Introduction to Automata Theory, Languages, and Computation]]'' with [[John Hopcroft]] and [[Jeffrey Ullman]].&lt;ref name="automata"&gt;{{cite book |author1=Ullman, Jeffrey D. |author2=Hopcroft, John E. |author3=Motwani, Rajeev |title=Introduction to automata theory, languages, and computation |publisher=Pearson/Addison Wesley |location=Boston |year=2007 |isbn=0-321-45536-3 }}&lt;/ref&gt;

He was an avid [[angel investor]] and helped fund a number of startups to emerge from Stanford. He sat on boards including Google, Kaboodle,  Mimosa Systems (acquired by [[Iron Mountain Incorporated]]), Adchemy, [[Baynote]], [[Vuclip]], NeoPath Networks (acquired by [[Cisco Systems]] in 2007), [[Tapulous]] and [[StartX|Stanford Student Enterprises]]. He was active in the [[Business Association of Stanford Entrepreneurial Students]] (BASES).&lt;ref&gt;[http://findarticles.com/p/articles/mi_m0EIN/is_2004_March_8/ai_113997405/ NeoPath Networks Locks Up $6M Equity Financing; August Capital and DCM-Doll Capital Management Lead the Investment] 2004-03-08&lt;/ref&gt;&lt;ref&gt;[http://www.networkworld.com/news/2007/040407-cisco-neopath.html "Cisco kisses NeoPath products goodbye"] {{webarchive|url=https://web.archive.org/web/20090610210210/http://www.networkworld.com/news/2007/040407-cisco-neopath.html |date=2009-06-10 }} by Deni Connor, ''Network World'', 2007-04-04. Retrieved 2009-06-06.&lt;/ref&gt;&lt;ref&gt;[https://www.nytimes.com/external/venturebeat/2009/06/05/05venturebeat-rajeev-motwani-google-founders-professor-and-21359.html Rajeev Motwani, Google founders’ professor and early investor, dies] 2009-06-05&lt;/ref&gt;

He was a winner of the [[Gödel Prize]] in 2001 for his work on the [[PCP theorem]] and its applications to [[hardness of approximation]].&lt;ref&gt;[http://sigact.acm.org/prizes/godel/2001.html 2001 Gödel Prize citation]&lt;/ref&gt;&lt;ref&gt;{{Cite journal | last1 = Arora | first1 = S. |authorlink1=Sanjeev Arora| last2 = Lund | first2 = C. |authorlink2=Carsten Lund| last3 = Motwani | first3 = R. |authorlink3=Rajeev Motwani| last4 = Sudan | first4 = M. |authorlink4=Madhu Sudan| last5 = Szegedy | first5 = M. |authorlink5=Mario Szegedy| title = Proof verification and the hardness of approximation problems | doi = 10.1145/278298.278306 | journal = [[Journal of the ACM]]| volume = 45 | issue = 3 | pages = 501–555 | year = 1998 }}&lt;/ref&gt;

He served on the editorial boards of [[SIAM Journal on Computing]], Journal of Computer and System Sciences, ACM Transactions on Knowledge Discovery from Data, and IEEE Transactions on Knowledge and Data Engineering.

==Death==
Motwani was found dead in his pool in the backyard of his [[Atherton, California|Atherton]] home on June 5, 2009. The [[San Mateo County, California|San Mateo County]] coroner, [[Robert Foucrault]], ruled the death an accidental drowning. Toxicology tests showed that Motwani's [[blood alcohol content]] was 0.26 percent.&lt;ref name="Chron-Death"&gt;{{cite web|url=http://www.sfgate.com/cgi-bin/article.cgi?f=/c/a/2009/07/16/BAOO18Q2HH.DTL|title=Stanford tech mentor was drunk when he drowned|last=Lee|first=Henry K.|date=July 16, 2009|work=San Francisco Chronicle|publisher=Hearst Communications, Inc.|pages=D–4|accessdate=2009-07-17}}&lt;/ref&gt;
He could not swim, but was planning on taking lessons, according to his friends.&lt;ref&gt;{{cite web |last=Weaver |first=Matthew |title=Google founders' mentor found dead in swimming pool |url=https://www.theguardian.com/technology/2009/jun/07/rajeev-motwani-dead-google-swimming-pool |work=guardian.co.uk |publisher=Guardian News and Media Limited | date = 2009-06-07}}&lt;/ref&gt;

==Personal life==
Motwani, and his wife [[Asha Jadeja Motwani]], had two daughters named Naitri and Anya.&lt;ref&gt;[http://www.hindu.com/thehindu/holnus/001200906071814.htm Google mentor Rajeev Motwani dies in freak accident] {{webarchive|url=https://web.archive.org/web/20090610051722/http://www.hindu.com/thehindu/holnus/001200906071814.htm |date=2009-06-10 }} 2009-06-07&lt;/ref&gt;
After his death his family donated US$1.5 million in 2011, a building was named in his honor at IIT Kanpur.&lt;ref name="Kanpur"&gt;{{Cite web |title= The Rajeev Motwani Building: Department of Computer Science and Engineering |url= http://www.cse.iitk.ac.in/building/rajeev_motwani_building.html |accessdate= September 11, 2013 |deadurl= yes |archiveurl= https://web.archive.org/web/20130522004719/http://www.cse.iitk.ac.in/building/rajeev_motwani_building.html |archivedate= May 22, 2013 |df=  }}&lt;/ref&gt;

==Awards==
* [[Gödel Prize]] in 2001
* [[Isao Okawa|Okawa]] Foundation Research Award&lt;ref name="Tha"&gt;{{Cite news |title= Rajeev Motwani passes away |date= June 6, 2009 |work= Thaindian |url=  http://www.thaindian.com/newsportal/world/rajeev-motwani-passes-away_100201732.html |accessdate= September 11, 2013 }}&lt;/ref&gt;
* [[Arthur Sloan Research Fellowship]]&lt;ref name="Tha" /&gt;
* National Young Investigator Award from the National Science Foundation
* Distinguished Alumnus Award from IIT Kanpur in 2006&lt;ref name="Kanpur" /&gt;
* Bergmann Memorial Award from the US-Israel Bi-National Science Foundation
* IBM Faculty Award

==Notes==
{{reflist|2}}

==External links==
* [http://reflections-shivanand.blogspot.com/2007/08/rajiv-motwani.html Mathematician at heart]
* [https://www.telegraph.co.uk/news/obituaries/technology-obituaries/5487846/Professor-Rajeev-Motwani.html Professor Rajeev Motwani] at [[The Daily Telegraph|The Telegraph]]
{{Gödel winners}}

{{Authority control}}

{{DEFAULTSORT:Motwani, Rajeev}}
[[Category:Indian emigrants to the United States]]
[[Category:Stanford University School of Engineering faculty]]
[[Category:Theoretical computer scientists]]
[[Category:American computer scientists]]
[[Category:Gödel Prize laureates]]
[[Category:Indian Institute of Technology Kanpur alumni]]
[[Category:University of California, Berkeley alumni]]
[[Category:Google people]]
[[Category:1962 births]]
[[Category:2009 deaths]]
[[Category:American people of Sindhi descent]]
[[Category:St. Columba's School, Delhi alumni]]
[[Category:Sindhi people]]
[[Category:Sindhi computer scientists]]
[[Category:American male scientists of Indian descent]]
[[Category:Scientists from Jammu and Kashmir]]
[[Category:People from Jammu (city)]]
[[Category:20th-century Indian mathematicians]]
[[Category:People from Atherton, California]]</text>
      <sha1>43n46ttqa0u0a3inl62eo1aa416zenn</sha1>
    </revision>
  </page>
  <page>
    <title>Reduced derivative</title>
    <ns>0</ns>
    <id>16993965</id>
    <revision>
      <id>854514329</id>
      <parentid>618205715</parentid>
      <timestamp>2018-08-11T22:40:33Z</timestamp>
      <contributor>
        <username>Marco Ciaramella</username>
        <id>19222521</id>
      </contributor>
      <comment>+ Link: Alexander Mielke</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5565">In [[mathematics]], the '''reduced derivative''' is a generalization of the notion of [[derivative]] that is well-suited to the study of functions of [[bounded variation]].  Although functions of bounded variation have derivatives in the sense of [[Radon measure]]s, it is desirable to have a derivative that takes values in the same space as the functions themselves.  Although the precise definition of the reduced derivative is quite involved, its key properties are quite easy to remember:

* it is a multiple of the usual derivative wherever it exists;
* at jump points, it is a multiple of the jump vector.

The notion of reduced derivative appears to have been introduced by [[Alexander Mielke]] and Florian Theil in 2004.

==Definition==

Let ''X'' be a [[separable space|separable]], [[reflexive space|reflexive]] [[Banach space]] with [[norm (mathematics)|norm]] ||&amp;nbsp;|| and fix ''T''&amp;nbsp;&amp;gt;&amp;nbsp;0.  Let BV&lt;sub&gt;&amp;minus;&lt;/sub&gt;([0,&amp;nbsp;''T''];&amp;nbsp;''X'') denote the space of all [[continuous function|left-continuous]] functions ''z''&amp;nbsp;:&amp;nbsp;[0,&amp;nbsp;''T'']&amp;nbsp;&amp;rarr;&amp;nbsp;''X'' with bounded variation on [0,&amp;nbsp;''T''].

For any function of time ''f'', use subscripts +/&amp;minus; to denote the right/left continuous versions of ''f'', i.e.

:&lt;math&gt;f_{+} (t) = \lim_{s \downarrow t} f(s);&lt;/math&gt;
:&lt;math&gt;f_{-} (t) = \lim_{s \uparrow t} f(s).&lt;/math&gt;

For any sub-interval [''a'',&amp;nbsp;''b''] of [0,&amp;nbsp;''T''], let Var(''z'',&amp;nbsp;[''a'',&amp;nbsp;''b'']) denote the variation of ''z'' over [''a'',&amp;nbsp;''b''], i.e., the [[supremum]]

:&lt;math&gt;\mathrm{Var}(z, [a, b]) = \sup \left\{ \left. \sum_{i = 1}^{k} \| z(t_{i}) - z(t_{i - 1}) \| \right| a = t_{0} &lt; t_{1} &lt; \cdots &lt; t_{k} = b, k \in \mathbb{N} \right\}.&lt;/math&gt;

The first step in the construction of the reduced derivative is the &amp;ldquo;stretch&amp;rdquo; time so that ''z'' can be linearly interpolated at its jump points.  To this end, define

:&lt;math&gt;\hat{\tau} \colon [0, T] \to [0, + \infty);&lt;/math&gt;
:&lt;math&gt;\hat{\tau}(t) = t + \int_{[0, t]} \| \mathrm{d} z \| = t + \mathrm{Var}(z, [0, t]).&lt;/math&gt;

The &amp;ldquo;stretched time&amp;rdquo; function ''&amp;tau;&amp;#x302;'' is left-continuous (i.e. ''&amp;tau;&amp;#x302;''&amp;nbsp;=&amp;nbsp;''&amp;tau;&amp;#x302;''&lt;sub&gt;&amp;minus;&lt;/sub&gt;);  moreover, ''&amp;tau;&amp;#x302;''&lt;sub&gt;&amp;minus;&lt;/sub&gt; and ''&amp;tau;&amp;#x302;''&lt;sub&gt;+&lt;/sub&gt; are [[strictly increasing]] and agree except at the (at most countable) jump points of ''z''.  Setting ''T&amp;#x302;''&amp;nbsp;=&amp;nbsp;''&amp;tau;&amp;#x302;''(''T''), this &amp;ldquo;stretch&amp;rdquo; can be inverted by

:&lt;math&gt;\hat{t} \colon [0, \hat{T}] \to [0, T];&lt;/math&gt;
:&lt;math&gt;\hat{t}(\tau) = \max \{ t \in [0, T] | \hat{\tau}(t) \leq \tau \}.&lt;/math&gt;

Using this, the stretched version of ''z'' is defined by

:&lt;math&gt;\hat{z} \in C^{0} ([0, \hat{T}]; X);&lt;/math&gt;
:&lt;math&gt;\hat{z}(\tau) = (1 - \theta) z_{-}(t) + \theta z_{+}(t)&lt;/math&gt;

where ''&amp;theta;''&amp;nbsp;&amp;isin;&amp;nbsp;[0,&amp;nbsp;1] and

:&lt;math&gt;\tau = (1 - \theta) \hat{\tau}_{-} (t) + \theta \hat{\tau}_{+} (t).&lt;/math&gt;

The effect of this definition is to create a new function ''z&amp;#x302;'' which &amp;ldquo;stretches out&amp;rdquo; the jumps of ''z'' by linear interpolation.  A quick calculation shows that ''z&amp;#x302;'' is not just continuous, but also lies in a [[Sobolev space]]:

:&lt;math&gt;\hat{z} \in W^{1, \infty} ([0, \hat{T}]; X);&lt;/math&gt;
:&lt;math&gt;\left\| \frac{\mathrm{d} \hat{z}}{\mathrm{d} \tau} \right\|_{L^{\infty} ([0, \hat{T}]; X)} \leq 1.&lt;/math&gt;

The derivative of ''z&amp;#x302;''(''&amp;tau;'') with respect to ''&amp;tau;'' is defined [[almost everywhere]] with respect to [[Lebesgue measure]].  The '''reduced derivative''' of ''z'' is the [[pull-back]] of this derivative by the stretching function ''&amp;tau;&amp;#x302;''&amp;nbsp;:&amp;nbsp;[0,&amp;nbsp;''T'']&amp;nbsp;&amp;rarr;&amp;nbsp;[0,&amp;nbsp;''T&amp;#x302;''].  In other words,

:&lt;math&gt;\mathrm{rd}(z) \colon [0, T] \to \{ x \in X | \| x \| \leq 1 \};&lt;/math&gt;
:&lt;math&gt;\mathrm{rd}(z)(t) = \frac{\mathrm{d} \hat{z}}{\mathrm{d} \tau} \left( \frac{\hat{\tau}_{-} (t) + \hat{\tau}_{+}(t)}{2} \right).&lt;/math&gt;

Associated with this pull-back of the derivative is the pull-back of Lebesgue measure on [0,&amp;nbsp;''T&amp;#x302;''], which defines the '''differential measure''' ''&amp;mu;''&lt;sub&gt;''z''&lt;/sub&gt;:

:&lt;math&gt;\mu_{z} ([t_{1}, t_{2})) = \lambda ([\hat{\tau}(t_{1}), \hat{\tau}(t_{2})) = \hat{\tau} (t_{2}) - \hat{\tau}(t_{1}) = t_{2} - t_{1} + \int_{[t_{1}, t_{2}]} \| \mathrm{d} z \|.&lt;/math&gt;

==Properties==

* The reduced derivative rd(''z'') is defined only ''&amp;mu;''&lt;sub&gt;''z''&lt;/sub&gt;-almost everywhere on [0,&amp;nbsp;''T''].
* If ''t'' is a jump point of ''z'', then

::&lt;math&gt;\mu_{z} (\{ t \}) = \| z_{+}(t) - z_{-}(t) \| \mbox{ and } \mathrm{rd}(z)(t) = \frac{z_{+}(t) - z_{-}(t)}{\| z_{+}(t) - z_{-}(t) \|}.&lt;/math&gt;

* If ''z'' is differentiable on (''t''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''t''&lt;sub&gt;2&lt;/sub&gt;), then

::&lt;math&gt;\mu_{z} ((t_{1}, t_{2})) = \int_{t_{1}}^{t_{2}} 1 + \| \dot{z}(t) \| \, \mathrm{d} t&lt;/math&gt;

:and, for ''t''&amp;nbsp;&amp;isin;&amp;nbsp;(''t''&lt;sub&gt;1&lt;/sub&gt;,&amp;nbsp;''t''&lt;sub&gt;2&lt;/sub&gt;),

::&lt;math&gt;\mathrm{rd}(z)(t) = \frac{\dot{z}(t)}{1 + \| \dot{z}(t) \|}&lt;/math&gt;,

* For 0&amp;nbsp;&amp;le;&amp;nbsp;''s''&amp;nbsp;&amp;lt;&amp;nbsp;''t''&amp;nbsp;&amp;le;&amp;nbsp;''T'',

::&lt;math&gt;\int_{[s, t)} \mathrm{rd}(z)(r) \, \mathrm{d} \mu_{z} (r) = \int_{[s, t)} \mathrm{d} z = z(t) - z(s).&lt;/math&gt;

==References==

* {{cite journal
| last = Mielke
| first = Alexander
|author2=Theil, Florian
 | title = On rate-independent hysteresis models
| journal = NoDEA Nonlinear Differential Equations Appl.
| volume = 11
| year = 2004
| issue = 2
| pages = 151&amp;ndash;189
| issn = 1021-9722
}} {{MathSciNet|id=2210284}}

[[Category:Differential calculus]]
[[Category:Mathematical analysis]]</text>
      <sha1>831htelqynz9lcdd0t0aqpm1vlv05qz</sha1>
    </revision>
  </page>
  <page>
    <title>Rocky Mountain Journal of Mathematics</title>
    <ns>0</ns>
    <id>30640685</id>
    <revision>
      <id>841464908</id>
      <parentid>802267907</parentid>
      <timestamp>2018-05-16T01:11:15Z</timestamp>
      <contributor>
        <username>Headbomb</username>
        <id>1461430</id>
      </contributor>
      <comment>| mathscinet   = Rocky Mountain J. Math.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1869">{{Infobox journal
 | title        = The Rocky Mountain Journal of Mathematics
 | cover        = 
 | abbreviation = Rocky Mt. J. Math.
 | mathscinet   = Rocky Mountain J. Math.
 | discipline   = [[Mathematics]]
 | editor       = {{nowrap|1=John Quigg}}
 | publisher    = Rocky Mountain Mathematics Consortium
 | frequency    = Bimonthly
 | history      = 1971–present
 | impact       = 0.250
 | impact-year  = 2016
 | url          = http://rmmc.eas.asu.edu/rmj/rmj.html
 | ISSN         = 0035-7596
 | eISSN        = 
 | CODEN        = RMJMAE
 | LCCN         = 77612778
 | OCLC         = 01764461
 | link1        = http://projecteuclid.org/DPubS?service=UI&amp;version=1.0&amp;verb=Display&amp;page=past&amp;handle=euclid.rmjm&amp;collection= 
 | link1-name   = Online access
}}

''''' The Rocky Mountain Journal of Mathematics''''' is a [[peer review|peer-reviewed]] [[mathematics journal]] published by the Rocky Mountain Mathematics Consortium.
Founded in 1971, the journal publishes both research and expository articles on mathematics, with an emphasis on survey articles. 
The journal is indexed by ''[[Mathematical Reviews]]'' and [[Zentralblatt MATH]].
Its 2009 [[Mathematical Citation Quotient|MCQ]] was 0.25. According to the ''[[Journal Citation Reports]]'', the journal has a 2016 [[impact factor]] of 0.250.&lt;ref&gt;{{cite book |year=2017 |chapter=Rocky Mountain Journal of Mathematics |title=2016 [[Journal Citation Reports]] |publisher=[[Thomson Reuters]] |edition=Social Sciences |series=[[Web of Science]]}}&lt;/ref&gt;

==References==
{{reflist}}

==External links==
* {{Official website|http://rmmc.eas.asu.edu/rmj/rmj.html}}

[[Category:Mathematics journals]]
[[Category:Publications established in 1971]]
[[Category:English-language journals]]
[[Category:Bimonthly journals]]
[[Category:Academic journals published by learned and professional societies]]


{{math-journal-stub}}</text>
      <sha1>mmgibsvmcjacs9uizxkhuqcvuvgay6s</sha1>
    </revision>
  </page>
  <page>
    <title>Schröder–Hipparchus number</title>
    <ns>0</ns>
    <id>33490570</id>
    <revision>
      <id>862342610</id>
      <parentid>862342358</parentid>
      <timestamp>2018-10-03T19:36:34Z</timestamp>
      <contributor>
        <username>PrzA</username>
        <id>10301681</id>
      </contributor>
      <minor/>
      <comment>corrected journal name in Schröder 1870</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10619">[[File:Pentagon subdivisions.svg|thumb|280px|Eleven subdivisions of a pentagon]]
In [[number theory]], the '''Schröder–Hipparchus numbers''' form an [[integer sequence]] that can be used to count the number of [[Tree (graph theory)|plane tree]]s with a given set of leaves, the number of ways of inserting parentheses into a sequence, and the number of ways of dissecting a convex polygon into smaller polygons by inserting diagonals. These numbers begin
:1, 1, 3, 11, 45, 197, 903, 4279, 20793, 103049, ... {{OEIS|A001003}}.
They are also called the '''super-Catalan numbers''', the '''little Schröder numbers''', or the '''Hipparchus numbers''', after [[Eugène Charles Catalan]] and his [[Catalan number]]s, [[Ernst Schröder]] and the closely related [[Schröder number]]s, and the ancient Greek mathematician [[Hipparchus]] who appears from evidence in [[Plutarch]] to have known of these numbers.

==Combinatorial enumeration applications==
[[File:Tree-polygon-paren equivalence.svg|thumb|240px|Combinatorial equivalence between subdivisions of a polygon, plane trees, and parenthesizations]]
The Schröder–Hipparchus numbers may be used to count several closely related combinatorial objects:&lt;ref name="stan-ec"/&gt;&lt;ref name="ss"&gt;{{citation
 | last1 = Shapiro | first1 = Louis W.
 | last2 = Sulanke | first2 = Robert A.
 | doi = 10.2307/2690814
 | issue = 5
 | journal = [[Mathematics Magazine]]
 | mr = 1805263
 | pages = 369–376
 | title = Bijections for the Schröder numbers
 | volume = 73
 | year = 2000}}.&lt;/ref&gt;&lt;ref name="eth"&gt;{{citation
 | last = Etherington | first = I. M. H. | author-link = Ivor Malcolm Haddon Etherington
 | doi = 10.1017/S0950184300002639
 | journal = [[Edinburgh Mathematical Notes]]
 | pages = 1–6
 | title = Some problems of non-associative combinations (I)
 | volume = 32
 | year = 1940}}.&lt;/ref&gt;&lt;ref&gt;{{citation
 | last = Holtkamp | first = Ralf
 | arxiv = math/0407074
 | doi = 10.1016/j.aim.2005.12.004
 | issue = 2
 | journal = [[Advances in Mathematics]]
 | mr = 2271016
 | pages = 544–565
 | title = On Hopf algebra structures over free operads
 | volume = 207
 | year = 2006}}.&lt;/ref&gt;
*The ''n''th number in the sequence counts the number of different ways of subdividing of a polygon with ''n''&amp;nbsp;+&amp;nbsp;1 sides into smaller polygons by adding diagonals of the original polygon.
*The ''n''th number counts the number of different [[Tree (graph theory)|plane tree]]s with ''n'' leaves and with all internal vertices having two or more children.
*The ''n''th number counts the number of different ways of inserting parentheses into a sequence of ''n'' symbols, with each pair of parentheses surrounding two or more symbols or parenthesized groups, and without any parentheses surrounding the entire sequence.
*The ''n''th number counts the number of faces of all dimensions of an [[associahedron]] ''K''&lt;sub&gt;''n''&amp;nbsp;+&amp;nbsp;1&lt;/sub&gt; of dimension ''n''&amp;nbsp;&amp;minus;&amp;nbsp;1, including the associahedron itself as a face, but not including the empty set. For instance, the two-dimensional associahedron ''K''&lt;sub&gt;4&lt;/sub&gt; is a [[pentagon]]; it has five vertices, five faces, and one whole associahedron, for a total of 11 faces.
As the figure shows, there is a simple combinatorial equivalence between these objects: a polygon subdivision has a plane tree as a form of its [[dual graph]], the leaves of the tree correspond to the symbols in a parenthesized sequence, and the internal nodes of the tree other than the root correspond to parenthesized groups. The parenthesized sequence itself may be written around the perimeter of the polygon with its symbols on the sides of the polygon and with parentheses at the endpoints of the selected diagonals. This equivalence provides a [[bijective proof]] that all of these kinds of objects are counted by a single integer sequence.&lt;ref name="ss"/&gt;

The same numbers also count the number of [[double permutation]]s (sequences of the numbers from 1 to ''n'', each number appearing twice, with the first occurrences of each number in sorted order) that avoid the [[permutation pattern]]s 12312 and 121323.&lt;ref&gt;{{citation
 | last1 = Chen | first1 = William Y. C.
 | last2 = Mansour | first2 = Toufik
 | last3 = Yan | first3 = Sherry H. F.
 | issue = 1
 | journal = [[Electronic Journal of Combinatorics]]
 | mr = 2274327
 | page = Research Paper 112, 17 pp. (electronic)
 | title = Matchings avoiding partial patterns
 | url = http://www.combinatorics.org/Volume_13/Abstracts/v13i1r112.html
 | volume = 13
 | year = 2006}}.&lt;/ref&gt;

==Related sequences==
The closely related [[Schröder number|large Schröder numbers]] are equal to twice the Schröder–Hipparchus numbers, and may also be used to count several types of combinatorial objects including certain kinds of lattice paths, partitions of a rectangle into smaller rectangles by recursive slicing, and parenthesizations in which a pair of parentheses surrounding the whole sequence of elements is also allowed. The [[Catalan number]]s also count closely related sets of objects including subdivisions of a polygon into triangles, plane trees in which all internal nodes have exactly two children, and parenthesizations in which each pair of parentheses surrounds exactly two symbols or parenthesized groups.&lt;ref name="eth"/&gt;

The sequence of Catalan numbers and the sequence of Schröder–Hipparchus numbers, viewed as infinite-dimensional [[Row vector|vector]]s, are the unique [[eigenvector]]s for the first two in a sequence of naturally defined linear operators on number sequences.&lt;ref name="bs"&gt;{{citation
 | last1 = Bernstein | first1 = M.
 | last2 = Sloane | first2 = N. J. A. | author2-link = Neil Sloane
 | doi = 10.1016/0024-3795(94)00245-9
 | journal = Linear Algebra and its Applications
 | mr = 1344554
 | pages = 57–72
 | title = Some canonical sequences of integers
 | volume = 226/228
 | year = 1995}}.&lt;/ref&gt;&lt;ref name="coker"&gt;{{citation
 | last = Coker | first = Curtis
 | doi = 10.1016/j.disc.2003.12.008
 | issue = 1-3
 | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]
 | mr = 2059525
 | pages = 249–250
 | title = A family of eigensequences
 | volume = 282
 | year = 2004}}.&lt;/ref&gt; More generally, the ''k''th sequence in this sequence of integer sequences is  (''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ''x''&lt;sub&gt;3&lt;/sub&gt;, ...) where the numbers ''x&lt;sub&gt;n&lt;/sub&gt;'' are calculated as the sums of [[Narayana number]]s multiplied by powers of&amp;nbsp;''k'':
:&lt;math&gt;x_n = \sum_{i=1}^n N(n,i)\, k^{i-1} = \sum_{i=1}^n \frac{1}{n}{n\choose i}{n\choose i-1} k^{i-1}.&lt;/math&gt;
Substituting ''k''&amp;nbsp;=&amp;nbsp;1 into this formula gives the Catalan numbers and substituting ''k''&amp;nbsp;=&amp;nbsp;2 into this formula gives the Schröder–Hipparchus numbers.&lt;ref name="coker"/&gt;

In connection with the property of Schröder–Hipparchus numbers of counting faces of an associahedron, the number of vertices of the associahedron is given by the Catalan numbers. The corresponding numbers for the [[permutohedron]] are respectively the [[ordered Bell number]]s and the [[factorial]]s.

==Recurrence==
As well as the summation formula above, the Schröder–Hipparchus numbers may be defined by a [[recurrence relation]]:
:&lt;math&gt;S(n)=\frac{1}{n}\left((6n-9)S(n-1)-(n-3)S(n-2)\right).&lt;/math&gt;
Stanley proves this fact using [[generating function]]s&lt;ref name="stan-amm"&gt;{{citation
 | last = Stanley | first = Richard P. | authorlink = Richard P. Stanley
 | doi = 10.2307/2974582
 | issue = 4
 | journal = [[American Mathematical Monthly]]
 | mr = 1450667
 | pages = 344–350
 | title = Hipparchus, Plutarch, Schröder, and Hough
 | url = http://www-math.mit.edu/~rstan/papers/hip.pdf
 | volume = 104
 | year = 1997}}.&lt;/ref&gt; while Foata and Zeilberger provide a direct combinatorial proof.&lt;ref&gt;{{citation
 | last1 = Foata | first1 = Dominique | author1-link = Dominique Foata
 | last2 = Zeilberger | first2 = Doron | author2-link = Doron Zeilberger
 | arxiv = math/9805015
 | doi = 10.1006/jcta.1997.2814
 | issue = 2
 | journal = [[Journal of Combinatorial Theory]] | series = Series A
 | mr = 1485153
 | pages = 380–384
 | title = A classic proof of a recurrence for a very classical sequence
 | volume = 80
 | year = 1997}}.&lt;/ref&gt;

==History==
According to a line in [[Plutarch]]'s ''Table Talk'', Hipparchus showed that the number of "affirmative compound propositions" that can be made from ten simple propositions is 103049 and that the number of negative compound propositions that can be made from ten simple propositions is 310952. This statement went unexplained until 1994, when David Hough, a graduate student at [[George Washington University]], observed that there are 103049 ways of inserting parentheses into a sequence of ten items.&lt;ref name="stan-ec"&gt;{{citation|title=Enumerative Combinatorics|first=Richard P.|last=Stanley|authorlink=Richard P. Stanley|publisher=Cambridge University Press|year=1997, 1999}}. Exercise 1.45, vol. I, p. 51; vol. II, pp. 176–178 and p. 213.&lt;/ref&gt;&lt;ref name="stan-amm"/&gt;&lt;ref name="acerbi"&gt;{{citation
 |last        = Acerbi
 |first       = F.
 |journal     = [[Archive for History of Exact Sciences]]
 |pages       = 465–502
 |title       = On the shoulders of Hipparchus: A reappraisal of ancient Greek combinatorics
 |url         = http://stl.recherche.univ-lille3.fr/sitespersonnels/acerbi/acerbipub5.pdf
 |volume      = 57
 |year        = 2003
 |doi         = 10.1007/s00407-003-0067-0
 |deadurl     = yes
 |archiveurl  = https://web.archive.org/web/20110721023220/http://stl.recherche.univ-lille3.fr/sitespersonnels/acerbi/acerbipub5.pdf
 |archivedate = 2011-07-21
 |df          = 
}}.&lt;/ref&gt; A similar explanation can be provided for the other number: it is very close to the average of the tenth and eleventh Schröder–Hipparchus numbers, 310954, and counts bracketings of ten terms together with a negative particle.&lt;ref name="acerbi"/&gt;

The problem of counting parenthesizations was introduced to modern mathematics by {{harvtxt|Schröder|1870}}.&lt;ref&gt;{{citation
 | last = Schröder | first = Ernst | author-link = Ernst Schröder
 | journal = [[Zeitschrift für Mathematik und Physik]]
 | pages = 361–376
 | title = Vier kombinatorische Probleme
 | volume = 15
 | year = 1870}}.&lt;/ref&gt;

==References==
{{reflist}}

==External links==
*{{mathworld|title=Super Catalan Number|urlname=SuperCatalanNumber}}
*[http://golem.ph.utexas.edu/category/2013/04/permutations_polynomials_and_p.html The Hipparchus Operad], The n-Category Café, April 1, 2013

{{Classes of natural numbers}}

{{DEFAULTSORT:Schroder-Hipparchus number}}
[[Category:Integer sequences]]
[[Category:Enumerative combinatorics]]</text>
      <sha1>ic15nuq8w52a401aeur90hhvzscg25o</sha1>
    </revision>
  </page>
  <page>
    <title>Singular distribution</title>
    <ns>0</ns>
    <id>7820200</id>
    <revision>
      <id>838314105</id>
      <parentid>825471708</parentid>
      <timestamp>2018-04-26T06:56:54Z</timestamp>
      <contributor>
        <ip>205.175.107.238</ip>
      </contributor>
      <comment>/* Properties */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1526">In [[probability]], a '''singular distribution''' is a [[probability distribution]] concentrated on a [[Null set|set of Lebesgue measure zero]], where the probability of each point in that set is zero. 

==Other names==
These distributions are sometimes called '''singular continuous distributions''', since their [[cumulative distribution function]]s are [[singular function|singular]] and [[continuous function|continuous]]. 

==Properties==
Such distributions are not [[absolutely continuous]] with respect to [[Lebesgue measure]].

A singular distribution is not a [[discrete probability distribution]] because each discrete point has a zero probability.  On the other hand, neither does it have a [[probability density function]], since the [[Lebesgue integral]] of any such function would be zero.

In general, distributions can be described as a discrete distribution (with a probability mass function), an absolutely continuous distribution (with a probability density), a singular distribution (with neither), or can be decomposed into a mixture of these.

==Example==
An example is the [[Cantor distribution]]; its cumulative distribution function is a [[Cantor function|devil's staircase]].

==See also==
*[[Singular measure]]
*[[Lebesgue's decomposition theorem]]

==External links==
*[http://eom.springer.de/S/s085530.htm Springer Encyclopaedia of Mathematics]

{{ProbDistributions}}

{{DEFAULTSORT:Singular Distribution}}
[[Category:Types of probability distributions]]


{{Statistics-stub}}
{{probability-stub}}</text>
      <sha1>kceb05gnw72b21sg8x0uasp0z15u1d6</sha1>
    </revision>
  </page>
  <page>
    <title>Situation calculus</title>
    <ns>0</ns>
    <id>2256109</id>
    <revision>
      <id>868862625</id>
      <parentid>837697612</parentid>
      <timestamp>2018-11-14T22:49:23Z</timestamp>
      <contributor>
        <username>Texvc2LaTeXBot</username>
        <id>33995001</id>
      </contributor>
      <minor/>
      <comment>Replacing deprecated latex syntax [[mw:Extension:Math/Roadmap]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19757">The '''situation calculus''' is a [[logic]] formalism designed for representing and reasoning about dynamical domains. It was first introduced by [[John McCarthy (computer scientist)|John McCarthy]] in 1963. The main version of the situational calculus that is presented in this article is based on that introduced by [[Ray Reiter]] in 1991. It is followed by sections about McCarthy's 1986 version and a [[logic programming]] formulation.

== Overview ==

The situation calculus represents changing scenarios as a set of [[first-order logic]] formulae. The basic elements of the calculus are:

*The actions that can be performed in the world
*The [[Fluent (artificial intelligence)|fluents]] that describe the state of the world
*The situations

A domain is formalized by a number of formulae, namely:

*Action precondition axioms, one for each action
*Successor state axioms, one for each fluent
*Axioms describing the world in various situations
*The foundational axioms of the situation calculus

A simple robot world will be modeled as a running example. In this world there is a single robot and several inanimate objects. The world is laid out according to a grid so that locations can be specified in terms of &lt;math&gt;(x,y)&lt;/math&gt; coordinate points. It is possible for the robot to move around the world, and to pick up and drop items. Some items may be too heavy for the robot to pick up, or fragile so that they break when they are dropped. The robot also has the ability to repair any broken items that it is holding.

== Elements ==

The main elements of the situation calculus are the actions, fluents and the situations. A number of objects are also typically involved in the description of the world. The situation calculus is based on a sorted domain with three sorts: actions, situations, and objects, where the objects include everything that is not an action or a situation. Variables of each sort can be used. While actions, situations, and objects are elements of the domain, the fluents are modeled as either predicates or functions.

===Actions===

The actions form a sort of the domain. Variables of sort action can be used. Actions can be quantified. A special predicate &lt;math&gt;Poss&lt;/math&gt; is used to indicate when an action is executable.

In the example robot world, possible action terms would be &lt;math&gt;move(x,y)&lt;/math&gt; to model the robot moving to a new location &lt;math&gt;(x,y)&lt;/math&gt;, and &lt;math&gt;pickup(o)&lt;/math&gt; to model the robot picking up an object &lt;math&gt;o&lt;/math&gt;.

===Situations===

In the situation calculus, a dynamic world is modeled as progressing through a series of situations as a result of various actions being performed within the world. A situation represents a history of action occurrences. In the Reiter version of the situation calculus described here,  a situation does not represent a state, contrarily to the literal meaning of the term and contrarily to the original definition by McCarthy and Hayes. This point has been summarized by Reiter as follows:

: A situation is a finite sequence of actions. Period. It's not a state, it's not a snapshot, it's a ''history'' [http://www.ida.liu.se/ext/etai/rac/notes/1997/09/note.html].

The situation before any actions have been performed is typically denoted &lt;math&gt;S_0&lt;/math&gt; and called the initial situation. The new situation resulting from the performance of an action is denoted using the function symbol &lt;math&gt;do&lt;/math&gt; (Some other references also use &lt;math&gt;result&lt;/math&gt;). This function symbol has a situation and an action as arguments, and a situation as a result, the latter being the situation that results from performing the given action in the given situation.

The fact that situations are sequences of actions and not states is enforced by an axiom stating that &lt;math&gt;do(a,s)&lt;/math&gt; is equal to &lt;math&gt;do(a',s')&lt;/math&gt; if and only if &lt;math&gt;a=a'&lt;/math&gt; and &lt;math&gt;s=s'&lt;/math&gt;. This condition makes no sense if situations were states, as two different actions executed in two different states can result in the same state.

In the example robot world, if the robot's first action is to move to location &lt;math&gt;(2,3)&lt;/math&gt;, the first action is &lt;math&gt;move(2,3)&lt;/math&gt; and the resulting situation is &lt;math&gt;do(move(2,3),S_{0})&lt;/math&gt;. If its next action is to pick up the ball, the resulting situation is &lt;math&gt;do(pickup(Ball),do(move(2,3),S_{0}))&lt;/math&gt;. Situations terms like &lt;math&gt;do(move(2,3),S_{0})&lt;/math&gt; and &lt;math&gt;do(pickup(Ball),do(move(2,3),S_{0}))&lt;/math&gt; denote the sequences of executed actions, and not the description of the state that result from execution.

=== Fluents ===
{{main| Fluent (artificial intelligence)}}
Statements whose [[truth value]] may change are modeled by ''relational fluents'', predicates which take a situation as their final argument. Also possible are ''functional fluents'', functions which take a situation as their final argument and return a situation-dependent value. Fluents may be thought of as "properties of the world"'.
 
In the example, the fluent &lt;math&gt;is\_ carrying(o,s)&lt;/math&gt; can be used to indicate that the robot is carrying a particular object in a particular situation. If the robot initially carries nothing, &lt;math&gt;is\_ carrying(Ball,S_{0})&lt;/math&gt; is false while &lt;math&gt;is\_ carrying(Ball,do(pickup(Ball),S_{0}))&lt;/math&gt; is true. The location of the robot can be modeled using a functional fluent &lt;math&gt;location(s)&lt;/math&gt; which returns the location &lt;math&gt;(x,y)&lt;/math&gt; of the robot in a particular situation.

==Formulae==

The description of a dynamic world is encoded in [[Second-order_logic|second order logics]] using three kinds of formulae: formulae about actions (preconditions and effects), formulae about the state of the world, and foundational axioms.

===Action preconditions===

Some actions may not be executable in a given situation. For example, it is impossible to put down an object unless one is in fact carrying it. The restrictions on the performance of actions are modeled by literals of the form &lt;math&gt;Poss(a,s)&lt;/math&gt;, where &lt;math&gt;a&lt;/math&gt; is an action, &lt;math&gt;s&lt;/math&gt; a situation, and &lt;math&gt;Poss&lt;/math&gt; is a special binary predicate denoting executability of actions. In the example, the condition that dropping an object is only possible when one is carrying it is modeled by:

&lt;math&gt;
Poss(drop(o),s)\leftrightarrow is\_ carrying(o,s)
&lt;/math&gt;

As a more complex example, the following models that the robot can carry only one object at a time, and that some objects are too heavy for the robot to lift (indicated by the predicate &lt;math&gt;heavy&lt;/math&gt;):

&lt;math&gt;
Poss(pickup(o),s)\leftrightarrow(\forall z\neg is\_ carrying(z,s))\wedge\neg heavy(o)
&lt;/math&gt;

===Action effects===

Given that an action is possible in a situation, one must specify the effects of that action on the fluents. This is done by the effect axioms. For example, the fact that picking up an object causes the robot to be carrying it can be modeled as:

&lt;math&gt;
Poss(pickup(o),s)\rightarrow is\_ carrying(o,do(pickup(o),s))
&lt;/math&gt;

It is also possible to specify conditional effects, which are effects that depend on the current state. The following models that some objects are fragile (indicated by the predicate &lt;math&gt;fragile&lt;/math&gt;) and dropping them causes them to be broken (indicated by the fluent &lt;math&gt;broken&lt;/math&gt;):

&lt;math&gt;
Poss(drop(o),s)\wedge fragile(o)\rightarrow broken(o,do(drop(o),s))
&lt;/math&gt;

While this formula correctly describes the effect of the actions, it is not sufficient to correctly describe the action in logic, because of the [[frame problem]].

===The frame problem===

While the above formulae seem suitable for reasoning about the effects of actions, they have a critical weakness - they cannot be used to derive the ''non-effects'' of actions. For example, it is not possible to deduce that after picking up an object, the robot's location remains unchanged. This requires a so-called frame axiom, a formula like:

&lt;math&gt;
Poss(pickup(o),s)\wedge location(s)=(x,y)\rightarrow location(do(pickup(o),s))=(x,y)
&lt;/math&gt;

The need to specify frame axioms has long been recognised as a problem in axiomatizing dynamic worlds, and is known as the [[frame problem]]. As there are generally a very large number of such axioms, it is very easy for the designer to leave out a necessary frame axiom, or to forget to modify all appropriate axioms when a change to the world description is made.

===The successor state axioms===

The successor state axioms "solve" the frame problem in the situation calculus. According to this solution, the designer must enumerate as effect axioms all the ways in which the value of a particular fluent can be changed. The effect axioms affecting the value of fluent &lt;math&gt;F(\overrightarrow{x},s)&lt;/math&gt; can be written in generalised form as a positive and a negative effect axiom:

&lt;math&gt;
Poss(a,s)\wedge\gamma_{F}^{+}(\overrightarrow{x},a,s)\rightarrow F(\overrightarrow{x},do(a,s))
&lt;/math&gt;

&lt;math&gt;
Poss(a,s)\wedge\gamma_{F}^{-}(\overrightarrow{x},a,s)\rightarrow\neg F(\overrightarrow{x},do(a,s))
&lt;/math&gt;

The formula &lt;math&gt;\gamma_{F}^{+}&lt;/math&gt; describes the conditions under which action &lt;math&gt;a&lt;/math&gt; in situation &lt;math&gt;s&lt;/math&gt; makes the fluent &lt;math&gt;F&lt;/math&gt; become true in the successor situation &lt;math&gt;do(a,s)&lt;/math&gt;. Likewise, &lt;math&gt;\gamma_{F}^{-}&lt;/math&gt; describes the conditions under which performing action &lt;math&gt;a&lt;/math&gt; in situation &lt;math&gt;s&lt;/math&gt; makes fluent &lt;math&gt;F&lt;/math&gt; false in the successor situation.

If this pair of axioms describe all the ways in which fluent &lt;math&gt;F&lt;/math&gt; can change value, they can be rewritten as a single axiom:

&lt;math&gt;
Poss(a,s)\rightarrow\left[F(\overrightarrow{x},do(a,s))\leftrightarrow\gamma_{F}^{+}(\overrightarrow{x},a,s)\vee\left(F(\overrightarrow{x},s)\wedge\neg\gamma_{F}^{-}(\overrightarrow{x},a,s)\right)\right]
&lt;/math&gt;

In words, this formula states: "given that it is possible to perform action &lt;math&gt;a&lt;/math&gt; in situation &lt;math&gt;s&lt;/math&gt;, the fluent &lt;math&gt;F&lt;/math&gt; would be true in the resulting situation &lt;math&gt;do(a,s)&lt;/math&gt; if and only if performing &lt;math&gt;a&lt;/math&gt; in &lt;math&gt;s&lt;/math&gt; would make it true, or it is true in situation &lt;math&gt;s&lt;/math&gt; and performing &lt;math&gt;a&lt;/math&gt; in &lt;math&gt;s&lt;/math&gt; would not make it false."

By way of example, the value of the fluent &lt;math&gt;broken&lt;/math&gt; introduced above is given by the following successor state axiom:

&lt;math&gt;
Poss(a,s) \rightarrow \left[ broken(o,do(a,s)) \leftrightarrow a=drop(o)\wedge fragile(o)
\vee broken(o,s) \wedge a \neq repair(o) \right]
&lt;/math&gt;

===States===

The properties of the initial or any other situation can be specified by simply stating them as formulae. For example, a fact about the initial state is formalized by making assertions about &lt;math&gt;S_{0}&lt;/math&gt; (which is not a state, but a ''situation''). The following statements model that initially, the robot carries nothing, is at
location &lt;math&gt;(0,0)&lt;/math&gt;, and there are no broken objects:

&lt;math&gt;
\forall z\,\neg is\_ carrying(z,S_{0})
&lt;/math&gt;

&lt;math&gt;
location(S_{0})=(0,0)\,
&lt;/math&gt;

&lt;math&gt;
\forall o\,\neg broken(o,S_{0})
&lt;/math&gt;

===Foundational axioms===

The foundational axioms of the situation calculus formalize the idea that situations are histories by having &lt;math&gt;do(a,s)=do(a',s') \iff a=a' \land s=s'&lt;/math&gt;. They also include other properties such as the second order induction on situations.

==Regression==

Regression is a mechanism for proving consequences in the situation calculus. It is based on expressing a formula containing the situation &lt;math&gt;do(a,s)&lt;/math&gt; in terms of a formula containing the action &lt;math&gt;a&lt;/math&gt; and the situation &lt;math&gt;s&lt;/math&gt;, but not the situation &lt;math&gt;do(a,s)&lt;/math&gt;. By iterating this procedure, one can end up with an equivalent formula containing only the initial situation &lt;math&gt;S_0&lt;/math&gt;. Proving consequences is supposedly simpler from this formula than from the original one.

==GOLOG==

GOLOG is a logic programming language based on the situation calculus.&lt;ref name=Lakemeyer2013&gt;{{cite web|last1=Lakemeyer|first1=Gerhard|title=The Situation Calculus and Golog: A Tutorial|url=https://www.hybrid-reasoning.org/media/filer/2013/05/24/hybris-2013-05-sitcalc-slides.pdf|website=www.hybrid-reasoning.org|accessdate=16 July 2014}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Publications about GOLOG|url=http://bibbase.org/network/keyword/golog|accessdate=16 July 2014}}&lt;/ref&gt;

==The original version of the situation calculus==

The main difference between the original situation calculus by McCarthy and Hayes and the one in use today is the interpretation of situations. In the modern version of the situational calculus, a situation is a sequence of actions. Originally, situations were defined as “the complete state of the universe at an instant of time”. It was clear from the beginning that such situations could not be completely described; the idea was simply to give some statements about situations, and derive consequences from them. This is also different from the approach that is taken by the [[fluent calculus]], where a state can be a collection of known facts, that is, a possibly ''incomplete'' description of the universe.

In the original version of the situation calculus, fluents are not reified. In other words, conditions that can change are represented by predicates and not by functions. Actually, McCarthy and Hayes defined a fluent as a function that depends on the situation, but they then proceeded always using predicates to represent fluents. For example, the fact that it is raining at place &lt;math&gt;x&lt;/math&gt; in the situation &lt;math&gt;s&lt;/math&gt; is represented by the literal &lt;math&gt;raining(x,s)&lt;/math&gt;. In the 1986 version of the situation calculus by McCarthy, functional fluents are used. For example, the position of an object &lt;math&gt;x&lt;/math&gt; in the situation &lt;math&gt;s&lt;/math&gt; is represented by the value of &lt;math&gt;location(x,s)&lt;/math&gt;, where &lt;math&gt;location&lt;/math&gt; is a function. Statements about such functions can be given using equality: &lt;math&gt;location(x,s)=location(x,s')&lt;/math&gt; means that the location of the object &lt;math&gt;x&lt;/math&gt; is the same in the two situations &lt;math&gt;s&lt;/math&gt; and &lt;math&gt;s'&lt;/math&gt;.

The execution of actions is represented by the function &lt;math&gt;result&lt;/math&gt;: the execution of the action &lt;math&gt;a&lt;/math&gt; in the situation &lt;math&gt;s&lt;/math&gt; is the situation &lt;math&gt;result(a,s)&lt;/math&gt;. The effects of actions are expressed by formulae relating fluents in situation &lt;math&gt;s&lt;/math&gt; and fluents in situations &lt;math&gt;result(a,s)&lt;/math&gt;. For example, that the action of opening the door results in the door being open if not locked is represented by: 

:&lt;math&gt;\neg locked(door,s) \rightarrow open(door, result(opens,s))&lt;/math&gt;

The predicates &lt;math&gt;locked&lt;/math&gt; and &lt;math&gt;open&lt;/math&gt; represent the conditions of a door being locked and open, respectively. Since these condition may vary, they are represented by predicates with a situation argument. The formula says that if the door is not locked in a situation, then the door is open after executing the action of opening, this action being represented by the constant &lt;math&gt;opens&lt;/math&gt;.

These formulae are not sufficient to derive everything that is considered plausible. Indeed, fluents at different situations are only related if they are preconditions and effects of actions; if a fluent is not affected by an action, there is no way to deduce it did not change. For example, the formula above does not imply that &lt;math&gt;\neg locked(door,result(opens,s))&lt;/math&gt; follows from &lt;math&gt;\neg locked(door,s)&lt;/math&gt;, which is what one would expect (the door is not made locked by opening it). In order for inertia to hold, formulae called ''frame axioms'' are needed. These formulae specify all non-effects of actions:

:&lt;math&gt;\neg locked(door,s) \rightarrow \neg locked(door, result(opens,s))&lt;/math&gt;

In the original formulation of the situation calculus, the initial situation, later denoted by &lt;math&gt;S_0&lt;/math&gt;, is not explicitly identified. The initial situation is not needed if situations are taken to be descriptions of the world. For example, to represent the scenario in which the door was closed but not locked and the action of opening it is performed is formalized by taking a constant &lt;math&gt;s&lt;/math&gt; to mean the initial situation and making statements about it (e.g., &lt;math&gt;\neg locked(door,s)&lt;/math&gt;). That the door is open after the change is reflected by formula &lt;math&gt;open(door,result(opens,s))&lt;/math&gt; being entailed. The initial situation is instead necessary if, like in the modern situation calculus, a situation is taken to be a history of actions, as the initial situation represents the empty sequence of actions.
 
The version of the situation calculus introduced by McCarthy in 1986 differs to the original one for the use of functional fluents (e.g., &lt;math&gt;location(x,s)&lt;/math&gt; is a term representing the position of &lt;math&gt;x&lt;/math&gt; in the situation &lt;math&gt;s&lt;/math&gt;) and for an attempt to use [[Circumscription (logic)|circumscription]] to replace the frame axioms.

==The situation calculus as a logic program==

It is also possible (e.g. Kowalski 1979, Apt and Bezem 1990, Shanahan 1997) to write the situation calculus as a logic program:

&lt;math&gt;Holds(f, do(a, s)) \leftarrow Poss(a, s) \wedge Initiates(a, f, s)&lt;/math&gt;

&lt;math&gt;Holds(f, do(a, s)) \leftarrow Poss(a, s) \wedge Holds(f, s) \wedge \neg Terminates(a, f, s)&lt;/math&gt;

Here &lt;math&gt;Holds&lt;/math&gt; is a meta-predicate and the variable &lt;math&gt;f&lt;/math&gt; ranges over fluents. The predicates &lt;math&gt;Poss&lt;/math&gt;, &lt;math&gt;Initiates&lt;/math&gt; and &lt;math&gt;Terminates&lt;/math&gt; correspond to the predicates &lt;math&gt;Poss&lt;/math&gt;, &lt;math&gt;\gamma_{F}^{+}(\overrightarrow{x},a,s)&lt;/math&gt;, and &lt;math&gt;\gamma_{F}^{-}(\overrightarrow{x},a,s)&lt;/math&gt; respectively. The left arrow &lt;math&gt;\leftarrow&lt;/math&gt; is half of the equivalence &lt;math&gt;\leftrightarrow&lt;/math&gt;. The other half is implicit in the completion of the program, in which negation is interpreted as [[negation as failure]]. Induction axioms are also implicit, and are needed only to prove program properties. Backward reasoning as in [[SLD resolution]], which is the usual mechanism used to execute logic programs, implements regression implicitly.

==See also==

* [[Frame problem]]
* [[Event calculus]]

== References ==
{{reflist}}

* J. McCarthy and P. Hayes (1969). [http://www-formal.stanford.edu/jmc/mcchay69.html Some philosophical problems from the standpoint of artificial intelligence]. In B. Meltzer and D. Michie, editors, ''Machine Intelligence'', 4:463–502.  Edinburgh University Press, 1969.
* R. Kowalski (1979). Logic for Problem Solving  - Elsevier North Holland.
* K.R. Apt and M. Bezem (1990). Acyclic Programs. In: 7th International Conference on Logic Programming. MIT Press. Jerusalem, Israel.
* R. Reiter (1991). The frame problem in the situation calculus: a simple solution (sometimes) and a completeness result for goal regression. In Vladimir Lifshitz, editor, ''Artificial intelligence and mathematical theory of computation: papers in honour of John McCarthy'', pages 359–380, San Diego, CA, USA.  Academic Press Professional, Inc. 1991.
* M. Shanahan (1997). Solving the Frame Problem: a Mathematical Investigation of the Common Sense Law of Inertia. MIT Press. 
* H. Levesque, F. Pirri, and R. Reiter (1998). [http://www.ep.liu.se/ej/etai/1998/005 Foundations for the situation calculus]. ''[[Electronic Transactions on Artificial Intelligence]]'', 2(3–4):159-178.
* F. Pirri and R. Reiter (1999). Some contributions to the metatheory of the Situation Calculus. ''[[Journal of the ACM]]'', 46(3):325–361. {{doi|10.1145/316542.316545}}
* R. Reiter (2001). Knowledge in Action: Logical Foundations for Specifying and Implementing Dynamical Systems. The MIT Press.

{{John McCarthy navbox}}
[[Category:1963 introductions]]
[[Category:Logic programming]]
[[Category:Logical calculi]]</text>
      <sha1>edca3jk5xgtuh91kotj8dxpglbqknz6</sha1>
    </revision>
  </page>
  <page>
    <title>Software verification and validation</title>
    <ns>0</ns>
    <id>699718</id>
    <revision>
      <id>870007870</id>
      <parentid>870003408</parentid>
      <timestamp>2018-11-21T20:25:02Z</timestamp>
      <contributor>
        <username>CrazyBrit</username>
        <id>29816118</id>
      </contributor>
      <minor/>
      <comment>/* Definitions */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19316">{{IEEE software documents}}
In [[software project management]], [[software testing]], and [[software engineering]], '''verification and validation''' ('''V&amp;V''') is the process of checking that a software system meets specifications and that it fulfills its intended purpose. It may also be referred to as [[software quality control]].  It is normally the responsibility of [[software testing|software testers]] as part of the [[software development process|software development lifecycle]]. In simple terms, software verification is: "Assuming we should build X, does our software achieve its goals without any bugs or gaps?" On the other hand, software validation is: "Was X what we should have built? Does X meet the high level requirements?"

==Definitions==
Verification and validation are not the same things, although they are often confused. [[Barry Boehm|Boehm]] succinctly expressed the difference as&lt;ref name="PhamSoft99"&gt;{{cite book |title=Software Reliability |author=Pham, H. |publisher=John Wiley &amp; Sons, Inc |page=567 |year=1999 |isbn=9813083840 |quote="Software Validation. The process of ensuring that the software is performing the right process. Software Verification. The process of ensuring that the software is performing the process right." Likewise and also there: "In short, Boehm (3) expressed the difference between the software verification and software validation as follows: Verification: ‘‘Are we building the product right?’’ Validation: ‘‘Are we building the right product?’’."}}&lt;/ref&gt;
* Verification: Are we building the product right?&lt;!--This definition is correct and sourced to Boehm. Do not change to the competing definition--&gt;
* Validation: Are we building the right product?&lt;!--This definition is correct and sourced to Boehm. Do not change to the competing definition--&gt;

Building the product right implies the use of the Requirements Specification as input for the next phase of the development process, the design process, the output of which is the Design Specification. Then, it also implies the use of the Design Specification to feed the construction process. Every time the output of a process correctly implements its input specification, the software product is one step closer to final verification. If the output of a process is incorrect, the developers are not building the product the stakeholders want correctly. This kind of verification is called "artifact or specification verification".

Building the right product implies creating a Requirements Specification that contains the needs and goals of the stakeholders of the software product. If such artifact is incomplete or wrong, the developers will not be able to build the product the stakeholders want. This is a form of "artifact or specification validation".

Note:  Verification begins before Validation and then they run in parallel until the software product is released. 

=== Software verification ===
It would imply to verify if the specifications are met by running the software but this is not possible (e. g., how can anyone know if the architecture/design/etc. are correctly implemented by running the software?). Only by reviewing its associated artifacts, someone can conclude if the specifications are met.

=== Artifact or specification verification===
The output of each software development process stage can also be subject to verification when checked against its input specification (see the definition by CMMI below).

Examples of artifact verification:
* Of the design specification against the requirement specification: Do the architectural design, detailed design and database logical model specifications correctly implement the functional and non-functional requirement specifications?
* Of the construction artifacts against the design specification: Do the source code, user interfaces and database physical model correctly implement the design specification?

=== Software validation===
Software validation checks that the software product satisfies or fits the intended use (high-level checking), i.e., the software meets the user requirements, not as specification artifacts or as needs of those who will operate the software only; but, as the needs of all the stakeholders (such as users, operators, administrators, managers, investors, etc.). There are two ways to perform software validation: internal and external. During internal software validation, it is assumed that the goals of the stakeholders were correctly understood and that they were expressed in the requirement artifacts precise and comprehensively. If the software meets the requirement specification, it has been internally validated. External validation happens when it is performed by asking the stakeholders if the software meets their needs. Different software development methodologies call for different levels of user and stakeholder involvement and feedback; so, external validation can be a discrete or a continuous event. Successful final external validation occurs when all the stakeholders accept the software product and express that it satisfies their needs. Such final external validation requires the use of an [[acceptance testing|acceptance test]] which is a [[dynamic testing|dynamic test]].

However, it is also possible to perform internal static tests to find out if it meets the requirements specification but that falls into the scope of static verification because the software is not running.

=== Artifact or specification validation ===
Requirements should be validated before the software product as a whole is ready (the waterfall development process requires them to be perfectly defined before design starts; but, iterative development processes do not require this to be so and allow their continual improvement).

Examples of artifact validation:

* User Requirements Specification validation: User requirements as stated in a document called User Requirements Specification are validated by checking if they indeed represent the will and goals of the stakeholders. This can be done by interviewing them and asking them directly (static testing) or even by releasing prototypes and having the users and stakeholders to assess them (dynamic testing).
* User input validation: User input (gathered by any peripheral such as keyboard, bio-metric sensor, etc.) is [[data validation|validated]] by checking if the input provided by the software operators or users meet the domain rules and constraints (such as data type, range, and format).

=== Validation vs. verification===
According to the [[Capability Maturity Model]] (CMMI-SW v1.1),
* Software Validation: The process of evaluating software during or at the end of the development process to determine whether it satisfies specified requirements. [IEEE-STD-610]
* Software Verification: The process of evaluating software to determine whether the products of a given development phase satisfy the conditions imposed at the start of that phase. [IEEE-STD-610]

Validation during the software development process can be seen as a form of User Requirements Specification validation; and, that at the end of the development process is equivalent to Internal and/or External Software validation. Verification, from CMMI's point of view, is evidently of the artifact kind.

In other words, software verification ensures that the output of each phase of the software development process effectively carry out what its corresponding input artifact specifies (requirement -&gt; design -&gt; software product), while software validation ensures that the software product meets the needs of all the stakeholders (therefore, the requirement specification was correctly and accurately expressed in the first place). Software verification ensures that "you built it right" and confirms that the product, as provided, fulfills the plans of the developers. Software validation ensures that "you built the right thing" and confirms that the product, as provided, fulfills the intended use and goals of the stakeholders.

This article has used the strict or [[Software verification#Narrow_scope|narrow]] definition of verification.

From a testing perspective:
* Fault – wrong or missing function in the code.
* Failure – the manifestation of a fault during execution. The software was not effective. It does not do "what" it is supposed to do.
* Malfunction – according to its specification the system does not meet its specified functionality. The software was not efficient (it took too many resources such as CPU cycles, it used too much memory, performed too many I/O operations, etc.), it was not usable, it was not reliable, etc. It does not do something "how" it is supposed to do it.

==Related concepts==
Both verification and validation are related to the concepts of [[Quality (business)|quality]] and of [[software quality assurance]]. By themselves, verification and validation do not guarantee software quality; planning, [[traceability]], configuration management and other aspects of software engineering are required.

Within the [[modeling and simulation]] (M&amp;S) community, the definitions of verification, validation and accreditation are similar:

* M&amp;S Verification is the process of determining that a [[computer model]], simulation, or federation of models and simulations implementations and their associated data accurately represent the developer's conceptual description and specifications.&lt;ref name="Missile Defense Agency"/&gt;
* M&amp;S Validation is the process of determining the degree to which a model, simulation, or federation of models and simulations, and their associated data are accurate representations of the real world from the perspective of the intended use(s).&lt;ref name="Missile Defense Agency"&gt;{{Cite document
  | year = 2008
  | title = Department of Defense Documentation of Verification, Validation &amp; Accreditation (VV&amp;A) for Models and Simulations
  | publisher = Missile Defense Agency
  | postscript = &lt;!--None--&gt;}}&lt;/ref&gt;
* [[Accreditation]] is the formal certification that a model or simulation is acceptable to be used for a specific purpose.&lt;ref name="Missile Defense Agency"/&gt;
The definition of M&amp;S validation focuses on the accuracy with which the M&amp;S represents the real-world intended use(s).  Determining the degree of M&amp;S accuracy is required because all M&amp;S are approximations of reality, and it is usually critical to determine if the degree of approximation is acceptable for the intended use(s).  This stands in contrast to software validation.

==Classification of methods==
In [[mission-critical]] software systems, where flawless performance is absolutely necessary, [[formal methods]] may be used to ensure the correct operation of a system.&lt;ref name="WangPrecise14"&gt;{{cite book |url=https://books.google.com/books?id=0Bi5BQAAQBAJ&amp;pg=PA262 |chapter=Precise Documentation and Validation of Requirements |title=Formal Techniques for Safety-Critical Systems |author=Wang, C.-W.; Ostroff, J.S.; Hudon, S. |editor=Artho, C.; Ölveczky, P.C. |publisher=Springer |pages=262–279 |year=2014 |isbn=9783319054162 |accessdate=18 May 2018}}&lt;/ref&gt; These formal methods can prove costly, however, representing as much as 80 percent of total software design cost.&lt;ref name="KoopmanReli07"&gt;{{cite book |chapter=Reliability, Safety, and Security in Everyday Embedded Systems |title=Dependable Computing |author=Koopman, P. |editor=Bondavelli, A.; Brasileiro, F.; Rajsbaum, S. |publisher=Springer |doi=10.1007/978-3-540-75294-3_1 |isbn=978-3-540-75294-3}}&lt;/ref&gt;

===Test cases===
{{main|Test case}}
A test case is a tool used in the process. Test cases may be prepared for software verification and software validation to determine if the product was built according to the requirements of the user. Other methods, such as reviews, may be used early in the life cycle to provide for software validation.

==Independent Verification and Validation==
ISVV stands for '''Independent Software Verification and Validation'''. ISVV is targeted at safety-critical [[software]] systems and aims to increase the quality of software products, thereby reducing risks and costs through the operational life of the software. ISVV provides assurance that software performs to the specified level of confidence and within its designed parameters and defined requirements.

ISVV activities are performed by independent engineering teams, not involved in the software development process, to assess the processes and the resulting products. The ISVV team independency is performed at three different levels: financial, managerial and technical.

ISVV goes far beyond “traditional” verification and validation techniques, applied by development teams. While the latter aim to ensure that the software performs well against the nominal requirements, ISVV is focused on non-functional requirements such as robustness and reliability, and on conditions that can lead the software to fail. ISVV results and findings are fed back to the development teams for correction and improvement.

===ISVV History===
ISVV derives from the application of IV&amp;V (Independent Verification and Validation) to the software. Early ISVV application (as known today) dates back to the early 1970s when the [[U.S. Army]] sponsored the first significant program related to IV&amp;V for the Safeguard [[Anti-Ballistic Missile]] System.

By the end of the 1970s IV&amp;V was rapidly becoming popular. The constant increase in complexity, size and importance of the software lead to an increasing demand on IV&amp;V applied to software (ISVV).

Meanwhile, IV&amp;V (and ISVV for software systems) gets consolidated and is now widely used by organisations such as the DoD, FAA, NASA&lt;ref&gt;[http://www.ivv.nasa.gov NASA IV&amp;V Facility]&lt;/ref&gt; and ESA.&lt;ref&gt;[http://www.esa.int ESA Web site]&lt;/ref&gt; IV&amp;V is mentioned in [DO-178B], [ISO/IEC 12207] and formalised in [IEEE 1012].

Initially in 2004-2005, a European consortium led by the [[European Space Agency]], and composed by DNV(N),&lt;ref&gt;[http://www.dnv.com DNV Web site]&lt;/ref&gt; Critical Software SA(P),&lt;ref&gt;[http://www.criticalsoftware.com Critical Software SA Web site]&lt;/ref&gt; Terma(DK)&lt;ref&gt;[http://www.terma.com Terma Web site]&lt;/ref&gt; and CODA Scisys(UK)&lt;ref&gt;[http://www.scisys.co.uk Scisys Web site]&lt;/ref&gt; created the first version of a guide devoted to ISVV, called "ESA Guide for Independent Verification and Validation" with support from other organizations, e.g. SoftWcare SL (E) (&lt;ref&gt;[http://www.softwcare.com SoftWcare SL Web site]&lt;/ref&gt;), etc.

In 2008 the European Space Agency released a second version, being SoftWcare SL was the supporting editor having received inputs from many different European Space ISVV stakeholders. This guide covers the methodologies applicable to all the software engineering phases in what concerns ISVV.

===ISVV Methodology===
ISVV is usually composed by five principal phases, these phases can be executed sequentially or as results of a tailoring process.

&lt;blockquote&gt;''ISVV Planning
* Planning of ISVV Activities
* System Criticality Analysis: Identification of Critical Components through a set of RAMS activities (Value for Money)
* Selection of the appropriate Methods and Tools''&lt;/blockquote&gt;

&lt;blockquote&gt;''Requirements Verification
* Verification for: Completeness, Correctness, Testability''&lt;/blockquote&gt;

&lt;blockquote&gt;''Design Verification
* Design adequacy and conformance to Software Requirements and Interfaces
* Internal and External Consistency
* Verification of Feasibility and Maintenance''&lt;/blockquote&gt;

&lt;blockquote&gt;''Code Verification
* Verification for: Completeness, Correctness, Consistency
* Code Metrics Analysis
* Coding Standards Compliance Verification''&lt;/blockquote&gt;

&lt;blockquote&gt;''Validation
* Identification of unstable components/functionalities
* Validation focused on Error-Handling: complementary (not concurrent!) validation regarding the one performed by the Development team (More for the Money, More for the Time)
* Compliance with Software and System Requirements
* [[Black box testing]] and [[White box testing]] techniques
* Experience based techniques''&lt;/blockquote&gt;

==Regulatory environment==
Verification and validation must meet the compliance requirements of law regulated industries, which is often guided by government agencies&lt;ref name=gpsv&gt;{{cite web
  | title = General Principles of Software validation; Final Guidance for Industry and FDA Staff
  | publisher = [[Food and Drug Administration]]
  | date = 11 January 2002
  | url = http://www.fda.gov/downloads/MedicalDevices/DeviceRegulationandGuidance/GuidanceDocuments/ucm085371.pdf
  | format = PDF
  | accessdate = 12 July 2009
  }}&lt;/ref&gt;&lt;ref name=dgi21&gt;{{cite web
  | title = Guidance for Industry: Part 11, Electronic Records; Electronic Signatures — Scope and Application
  | publisher = [[Food and Drug Administration]]
  | date = August 2003
  | url = http://www.fda.gov/downloads/Drugs/GuidanceComplianceRegulatoryInformation/Guidances/UCM072322.pdf
  | format = PDF
  | accessdate = 12 July 2009
  }}&lt;/ref&gt; or industrial administrative authorities. For instance, the [[Food and Drug Administration|FDA]] requires software versions and [[Patch (computing)|patches]] to be validated.&lt;ref name=gicn&gt;{{cite web
  | title = Guidance for Industry: Cybersecurity for Networked Medical Devices Containing Off-the Shelf (OTS) Software
  | publisher = [[Food and Drug Administration]]
  | date = 14 January 2005
  | url = http://www.fda.gov/downloads/MedicalDevices/DeviceRegulationandGuidance/GuidanceDocuments/ucm077823.pdf
  | accessdate = 12 July 2009
  }}&lt;/ref&gt;

==See also==
{{Portal|Software Testing}}
* [[Compiler correctness]]
* [[Cross-validation (statistics)|Cross-validation]]
* [[Formal verification]]
* [[Functional specification]]
* [[Independent Verification and Validation Facility]]
* [[International Software Testing Qualifications Board]]
* [[Software verification]]
* [[Software requirements specification]]
* [[Validation (drug manufacture)]]
* [[Verification and validation]] – General
* [[Verification and Validation of Computer Simulation Models]]
* [[Independent verification systems]]
* [[Software testing]]
* [[Software engineering]]
* [[Software quality]]
* [[Static code analysis]]


==Notes and references==
{{Reflist}}
{{Refbegin}}
* {{Cite book | doi = 10.1109/IEEESTD.2012.6204026| title = 1012-2012 IEEE Standard for System and Software Verification and Validation| year = 2012| isbn = 978-0-7381-7268-2}}
* {{cite book
|last=Tran
|first=E.
|editor=Koopman, P.
|title=Topics in Dependable Embedded Systems
|publisher=Carnegie Mellon University
|year=1999
|chapter=Verification/Validation/Certification
|chapterurl=http://www.ece.cmu.edu/~koopman/des_s99/verification/index.html
|accessdate=2007-05-18
}}
* {{cite journal
|last=Menzies
|first=T.
|author2=Y. Hu
 |title=Data mining for very busy people
|journal=[[Computer (magazine)|Computer]]
|volume=36
|issue=1
|year=2003
|pages=22–29
|doi=10.1109/MC.2003.1244531
}}
{{Refend}}

==External links==
* [http://www.computer.org/portal/web/swebok/html/ch11 Chapter on Software quality (including VnV)] in [[SWEBOK]]

{{Software engineering}}

{{DEFAULTSORT:Verification And Validation (Software)}}
[[Category:Software testing]]
[[Category:Formal methods]]
[[Category:Software quality]]

[[pt:Qualidade de software]]
[[pt:Teste de software]]</text>
      <sha1>nvrnfr203ndxbn3r4jqytbscww050lv</sha1>
    </revision>
  </page>
  <page>
    <title>Somos sequence</title>
    <ns>0</ns>
    <id>24612539</id>
    <revision>
      <id>857420098</id>
      <parentid>846818307</parentid>
      <timestamp>2018-08-31T15:43:05Z</timestamp>
      <contributor>
        <username>Joel B. Lewis</username>
        <id>13974845</id>
      </contributor>
      <comment>rephrase lead as per talk page comment; also remove external link to apparently unpublished research notes (?)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4354">In [[mathematics]], a '''Somos sequence''' is a sequence of numbers defined by a certain [[recurrence relation]], described below.  They were discovered by mathematician [[Michael Somos]]. From the form of their defining recurrence (which involves division), one would expect the terms of the sequence to be fractions, but nevertheless many Somos sequences have the property that all of their members are integers.

==Recurrence equations==
For an integer number ''k'' larger than 1, the Somos-''k'' sequence &lt;math&gt;(a_0, a_1, a_2, \ldots )&lt;/math&gt; is defined by the equation
:&lt;math&gt;a_n a_{n-k} = a_{n-1} a_{n-k+1} + a_{n-2} a_{n-k+2} + \cdots + a_{n-(k-1)/2} a_{n-(k+1)/2}&lt;/math&gt;
when ''k'' is odd, or by the analogous equation
:&lt;math&gt;a_n a_{n-k} = a_{n-1} a_{n-k+1} + a_{n-2} a_{n-k+2} + \cdots + (a_{n-k/2})^2&lt;/math&gt;
when ''k'' is even, together with the initial values
: ''a''&lt;sub&gt;''i''&lt;/sub&gt; = 1 for ''i''&amp;nbsp;&amp;lt;&amp;nbsp;''k''.

For ''k'' = 2 or 3, these recursions are very simple (there is no addition on the right-hand side) and they define the all-ones sequence (1, 1, 1, 1, 1, 1, ...).  In the first nontrivial case, ''k''&amp;nbsp;=&amp;nbsp;4, the defining equation is
:&lt;math&gt;a_n a_{n-4} = a_{n-1} a_{n-3} + a_{n-2}^2&lt;/math&gt;
while for ''k''&amp;nbsp;=&amp;nbsp;5 the equation is
:&lt;math&gt;a_n a_{n-5} = a_{n-1} a_{n-4} + a_{n-2} a_{n-3}\,.&lt;/math&gt;

These equations can be rearranged into the form of a [[recurrence relation]], in which the value ''a''&lt;sub&gt;''n''&lt;/sub&gt; on the left hand side of the recurrence is defined by a formula on the right hand side, by dividing the formula by ''a''&lt;sub&gt;''n''&amp;nbsp;&amp;minus;&amp;nbsp;''k''&lt;/sub&gt;. For ''k''&amp;nbsp;=&amp;nbsp;4, this yields the recurrence
:&lt;math&gt;a_n = \frac{a_{n-1} a_{n-3} + a_{n-2}^2}{a_{n-4}}&lt;/math&gt;
while for ''k''&amp;nbsp;=&amp;nbsp;5 it gives the recurrence
:&lt;math&gt;a_n = \frac{a_{n-1} a_{n-4} + a_{n-2} a_{n-3}}{a_{n-5}}.&lt;/math&gt;

While in the usual definition of the Somos sequences, the values of ''a''&lt;sub&gt;''i''&lt;/sub&gt; for ''i''&amp;nbsp;&amp;lt;&amp;nbsp;''k'' are all set equal to 1, it is also possible to define other sequences by using the same recurrences with different initial values.

==Sequence values==
The values in the Somos-4 sequence are
:1, 1, 1, 1, 2, 3, 7, 23, 59, 314, 1529, 8209, 83313, 620297, 7869898, ... {{OEIS|A006720}}.
The values in the Somos-5 sequence are
:1, 1, 1, 1, 1, 2, 3, 5, 11, 37, 83, 274, 1217, 6161, 22833, 165713, ... {{OEIS|A006721}}.
The values in the Somos-6 sequence are
:1, 1, 1, 1, 1, 1, 3, 5, 9, 23, 75, 421, 1103, 5047, 41783, 281527, ... {{OEIS|A006722}}.
The values in the Somos-7 sequence are
:1, 1, 1, 1, 1, 1, 1, 3, 5, 9, 17, 41, 137, 769, 1925, 7203, 34081, ... {{OEIS|A006723}}.

==Integrality==
The form of the recurrences describing the Somos sequences involves divisions, making it appear likely that the sequences defined by these recurrence will contain fractional values. Nevertheless, for ''k''&amp;nbsp;≤&amp;nbsp;7 the Somos sequences contain only integer values. Several mathematicians have studied the problem of proving and explaining this integer property of the Somos sequences; it is closely related to the combinatorics of [[cluster algebra]]s.&lt;ref&gt;{{citation | first=Janice L. | last=Malouf | title=An integer sequence from a rational recursion | journal=[[Discrete Mathematics (journal)|Discrete Mathematics]] | year=1992 | volume=110 | number=1–3 | pages=257–261 | doi=10.1016/0012-365X(92)90714-Q}}.&lt;/ref&gt;&lt;ref&gt;{{citation | first1=Sergey | last1=Fomin | first2=Andrei | last2=Zelevinsky | title=The Laurent phenomenon | journal=[[Advances in Applied Mathematics]]| volume=28 | year=2002 | pages=119&amp;ndash;144 | arxiv=math.CO/0104241 }}.&lt;/ref&gt;&lt;ref&gt;{{citation | first1=Gabriel D. | last1=Carroll | first2=David E. | last2=Speyer | journal=[[Electronic Journal of Combinatorics]] | volume=11 | year=2004 | pages= R73 | title=The Cube Recurrence| arxiv=math.CO/0403417 }}.&lt;/ref&gt;

For ''k''&amp;nbsp;≥&amp;nbsp;8 the analogously defined sequences eventually contain fractional values.  For ''k''&amp;nbsp;&lt;&amp;nbsp;7, changing the initial values (but using the same recurrence relation) also typically results in fractional values.

==References==
{{reflist}}

==External links==
*[http://faculty.uml.edu/jpropp/somos.html Jim Propp's Somos Sequence Site]
*{{mathworld|title=Somos Sequence|urlname=SomosSequence}}

[[Category:Integer sequences]]
[[Category:Recurrence relations]]</text>
      <sha1>sk63vjf91w5tvdf315il0osd8rdw66r</sha1>
    </revision>
  </page>
  <page>
    <title>Starlike tree</title>
    <ns>0</ns>
    <id>53828785</id>
    <revision>
      <id>791882487</id>
      <parentid>776586262</parentid>
      <timestamp>2017-07-23T01:54:32Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>{{combin-stub}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="668">{{unreferenced|date=April 2017}}
In the area of mathematics known as [[graph theory]], a [[tree (graph theory)|tree]] is said to be '''starlike''' if it has exactly one vertex of [[degree (graph theory)|degree]] greater than&amp;nbsp;2. This high-degree vertex is the '''root''' and a starlike tree is obtained by attaching at least three [[path graph|linear graphs]] to this central vertex.

== Properties ==

Two finite starlike trees are [[isospectral]], i.e. their [[discrete Laplace operator|graph Laplacians]] have the same spectra, if and only if they are [[graph isomorphism|isomorphic]].  

[[Category:Graph theory]]
[[Category:Spectral theory]]


{{combin-stub}}</text>
      <sha1>0olmvsczba3d791fhwbbf7jem8t1a4m</sha1>
    </revision>
  </page>
  <page>
    <title>Trachtenberg system</title>
    <ns>0</ns>
    <id>567391</id>
    <revision>
      <id>868396071</id>
      <parentid>868294429</parentid>
      <timestamp>2018-11-11T23:00:22Z</timestamp>
      <contributor>
        <username>DBlomgren</username>
        <id>2335596</id>
      </contributor>
      <comment>Copy edit.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19733">The '''Trachtenberg system''' is a system of rapid [[mental calculation]]. The system consists of a number of readily memorized operations that allow one to perform arithmetic computations very quickly. It was developed by the Russian Jewish engineer [[Jakow Trachtenberg]] in order to keep his mind occupied while being in a [[Nazi concentration camp]].

The rest of this article presents some methods devised by Trachtenberg.&lt;!-- These are for illustration only. To actually learn the method requires practice and a more complete treatment.--&gt; Some of the algorithms Trachtenberg developed are ones for general multiplication, division and addition. Also, the Trachtenberg system includes some specialised methods for multiplying small numbers between 5 and 13.

The section on addition demonstrates an effective method of checking calculations that can also be applied to multiplication.

==General multiplication==
The method for general multiplication is a method to achieve multiplications &lt;math&gt; a\times b &lt;/math&gt; with low space complexity, i.e. as few temporary results as possible to be kept in memory. This is achieved by noting that the final digit is completely determined by multiplying the last digit of the [[multiplicand]]s. This is held as a temporary result. To find the next to last digit, we need everything that influences this digit: The temporary result, the last digit of &lt;math&gt;a&lt;/math&gt; times the next-to-last digit of &lt;math&gt;b&lt;/math&gt;, as well as the next-to-last digit of &lt;math&gt;a&lt;/math&gt; times the last digit of &lt;math&gt;b&lt;/math&gt;. This calculation is performed, and we have a temporary result that is correct in the final two digits.

In general, for each position &lt;math&gt;n&lt;/math&gt; in the final result, we sum for all &lt;math&gt;i&lt;/math&gt;:

: &lt;math&gt;a \text{ (digit at } i\text{ )} \times b \text{ (digit at } (n-i)\text{)}.&lt;/math&gt;

People can learn this algorithm and thus multiply four digit numbers in their head – writing down only the final result. They would write it out starting with the rightmost digit and finishing with the leftmost.

Trachtenberg defined this algorithm with a kind of pairwise multiplication where two digits are multiplied by one digit, essentially only keeping the middle digit of the result. By performing the above algorithm with this pairwise multiplication, even fewer temporary results need to be held.

'''Example:''' &lt;math&gt;123456 \times 789&lt;/math&gt;

To find the first digit of the answer, start at the first digit of the multiplicand:
:The units digit of &lt;math&gt;9 \times 6&lt;/math&gt;
:&lt;math&gt;54 \rightarrow 4&lt;/math&gt;
:The first digit of the answer is &lt;math&gt;4&lt;/math&gt;

To find the second digit of the answer, start at the second digit of the multiplicand:
:The units digit of &lt;math&gt;9 \times 5&lt;/math&gt; plus the tens digit of &lt;math&gt;9 \times 6&lt;/math&gt; plus
:The units digit of &lt;math&gt;8 \times 6&lt;/math&gt;.
:&lt;math&gt;5 + 5 + 8 = 18&lt;/math&gt;.
:The second digit of the answer is &lt;math&gt;8&lt;/math&gt; and carry &lt;math&gt;1&lt;/math&gt; to the third digit.

To find the third digit of the answer, start at the third digit of the multiplicand:
:The units digit of &lt;math&gt;9 \times 4&lt;/math&gt; plus the tens digit of &lt;math&gt;9 \times 5&lt;/math&gt; plus
:The units digit of &lt;math&gt;8 \times 5&lt;/math&gt; plus the tens digit of &lt;math&gt;8 \times 6&lt;/math&gt; plus
:The units digit of &lt;math&gt;7 \times 6&lt;/math&gt;
:&lt;math&gt;1 + 6 + 4 + 0 + 4 + 2 = 17&lt;/math&gt;
:The third digit of the answer is &lt;math&gt;7&lt;/math&gt; and carry &lt;math&gt;1&lt;/math&gt; to the next digit.

To find the fourth digit of the answer, start at the fourth digit of the multiplicand:
:The units digit of &lt;math&gt;9 \times 3&lt;/math&gt; plus the tens digit of &lt;math&gt;9 \times 4&lt;/math&gt; plus
:The units digit of &lt;math&gt;8 \times 4&lt;/math&gt; plus the tens digit of &lt;math&gt;8 \times 5&lt;/math&gt; plus
:The units digit of &lt;math&gt;7 \times 5&lt;/math&gt; plus the tens digit of &lt;math&gt;7 \times 6&lt;/math&gt;.
:&lt;math&gt;1 + 7 + 3 + 2 + 4 + 5 + 4 = 26&lt;/math&gt; carried from the third digit.
:The fourth digit of the answer is &lt;math&gt;6&lt;/math&gt; and carry &lt;math&gt;2&lt;/math&gt; to the next digit.

Continue with the same method to obtain the remaining digits.

[[File:2FingerMethod.png|bottom|thumb|alt=Two headed arrows drawn from each digit of the multiplier to two digits of the multiplicand|2 Finger method]] 
Trachtenberg called this the 2 Finger Method.  The calculations for finding the fourth digit from the example above are illustrated at right.  The arrow from the nine will always point to the digit of the multiplicand directly above the digit of the answer you wish to find, with the other arrows each pointing one digit to the right.  Each arrow head points to a UT Pair, or Product Pair.  The vertical arrow points to the product where we will get the Units digit, and the sloping arrow points to the product where we will get the Tens digits of the Product Pair.  If an arrow points to a space with no digit there is no calculation for that arrow.  As you solve for each digit you will move each of the arrows over the multiplicand one digit to the left until all of the arrows point to prefixed zeros.

An example of the algorithm multiplying numbers represented as variable length ASCII strings is shown below in C++.&lt;syntaxhighlight lang="c++" line="1"&gt;
class Trachtenberg
{
public:
  void multiply(const char* a, const char* b)
  {
    size_t large = strlen(a);
    size_t small = strlen(b);
    if (large == 0 || small == 0) {
      answer = solution = new char[2]{ '0', '\0' };
      return;
    }
    if (large &lt; small) {
      std::swap(a, b);
      std::swap(large, small);
    }
    solution = new char[large + small];
    answer = solution + large + small - 1;
    *answer = '\0'; // zero terminate
    size_t digits = 0;
    const char* starta = a + large;
    const char* startb = b + small;
    intermediate = 0;
    while (digits != small) {
      multiplystep(++digits, --starta, startb);
    }
    for (size_t d = digits; d != large; ++d) {
      multiplystep(digits, --starta, startb);
    }
    while (digits != 1) {
      multiplystep(--digits, starta, --startb);
    }
    while (intermediate) {
      *--answer = '0' + intermediate % 10;
      intermediate /= 10;
    }
  }
  const char* result() const { return answer;  }

private:
  size_t intermediate;
  char* solution = nullptr;
  char* answer = nullptr;

  void multiplystep(size_t digits, const char* starta, const char* startb)
  {
    const char* digita = starta;
    const char* digitb = startb;
    for (size_t n = 0; n != digits; ++n) {
      --digitb;
      intermediate += (*digita - '0') * (*digitb - '0'); // convert from ASCII string
      ++digita;
    }
    *--answer = '0' + intermediate % 10; // convert back to string
    intermediate /= 10;
  }
};
&lt;/syntaxhighlight&gt;
{{Clear}}

[[File:Trachtenberg Division.png|bottom|thumb|alt=Setup for division using Trachtenberg Method|Setting up for Division]]
Division in the Trachtenberg System is done much the same as in multiplication but with subtraction instead of addition.  Splitting the dividend into smaller Partial Dividends,  then dividing this Partial Dividend by only the left-most digit of the divisor will provide the answer one digit at a time.  As you solve each digit of the answer you then subtract Product Pairs (UT pairs) and also NT pairs (Number-Tens) from the Partial Dividend to find the next Partial Dividend.  The Product Pairs are found between the digits of the answer so far and the divisor. If a subtraction results in a negative number you have to back up one digit and reduce that digit of the answer by one. With enough practice this method can be done in your head.
{{Clear}}

==General addition==
A method of adding columns of numbers and accurately checking the result without repeating the first operation. An intermediate sum, in the form of two rows of digits, is produced. The answer is obtained by taking the sum of the intermediate results with an L-shaped algorithm. As a final step, the checking method that is advocated removes both the risk of repeating any original errors and allows the precise column in which an error occurs to be identified at once. It is based on a check (or digit) sums, such as the nines-remainder method.

For the procedure to be effective, the different operations used in each stage must be kept distinct, otherwise there is a risk of interference.

==Other multiplication algorithms==

When performing any of these multiplication algorithms the following "steps" should be applied.

The answer must be found one digit at a time starting at the least significant digit and moving left.  The last calculation is on the leading zero of the multiplicand.

Each digit has a ''neighbor'', i.e., the digit on its right.  The rightmost digit's neighbor is the trailing zero.

The 'halve' operation has a particular meaning to the Trachtenberg system.  It is intended to mean "half the digit, rounded down" but for speed reasons people following the Trachtenberg system are encouraged to make this halving process instantaneous.  So instead of thinking "half of seven is three and a half, so three" it's suggested that one thinks "seven, three".  This speeds up calculation considerably.  In this same way the tables for subtracting digits from 10 or 9 are to be memorized.

And whenever the rule calls for adding half of the neighbor, always add 5 if the current digit is odd.  This makes up for dropping 0.5 in the next digit's calculation.

===Multiplying by 2===
*'''Rule''': to multiply by 2, double each digit.From right hand side.

===Multiplying by 3===
'''Rule:'''
#Subtract the rightmost digit from 10.
##Subtract the remaining digits from 9.
#Double the result.
#Add half of the neighbor to the right, ''plus'' 5 if the digit is odd.
#For the leading zero, subtract 2 from half of the neighbor.

'''Example:'''  492 × 3 = 1476

Working from right to left:
*(10 − 2) × 2 + Half of 0 (0) = 16. Write 6, carry 1.
*(9 − 9) × 2 + Half of 2 (1) + 5 (since 9 is odd) + 1 (carried) = 7. Write 7.
*(9 − 4) × 2 + Half of 9 (4) = 14. Write 4, carry 1.
*Half of 4 (2) − 2 + 1 (carried) = 1. Write 1.

===Multiplying by 4===
'''Rule:'''
#Subtract the right-most digit from 10.
##Subtract the remaining digits from 9.
#Add half of the neighbor, ''plus'' 5 if the digit is odd.
#For the leading 0, subtract 1 from half of the neighbor.

'''Example:'''  346×4 = 1384

Working from right to left:
*(10 − 6) + Half of 0 (0) = 4. Write 4.
*(9 − 4) + Half of 6 (3) = 8. Write 8.
*(9 − 3) + Half of 4 (2) + 5 (since 3 is odd) = 13. Write 3, carry 1.
*Half of 3 (1) − 1 + 1 (carried) = 1. Write 1.

===Multiplying by 5===
*'''Rule''': to multiply by 5: Take half of the neighbor, then, if the current digit is odd, add 5.
Example:
'''42×5=210'''
: Half of 2's neighbor, the trailing zero, is 0.
: Half of 4's neighbor is 1.
: Half of the leading zero's neighbor is 2.
: '''43×5 = 215'''
: Half of 3's neighbor is 0, plus 5 because 3 is odd, is 5.
: Half of 4's neighbor is 1.
: Half of the leading zero's neighbor is 2.
: '''93×5=465'''
: Half of 3's neighbor is 0, plus 5 because 3 is odd, is 5.
: Half of 9's neighbor is 1, plus 5 because 9 is odd, is 6.
: Half of the leading zero's neighbor is 4.

===Multiplying by 6===
*'''Rule:'''  Add half of the neighbor to each digit. If the current digit is odd, add 5.

Example: 357 × 6 = 2142

Working right to left,

: 7 has no neighbor, add 5 (since 7 is odd) = 12. Write 2, carry the 1.
: 5 + half of 7 (3) + 5 (since the starting digit 5 is odd) + 1 (carried) = 14. Write 4, carry the 1.
: 3 + half of 5 (2) + 5 (since 3 is odd) + 1 (carried) = 11.  Write 1, carry 1.
: 0 + half of 3 (1) + 1 (carried) = 2.  Write 2.

===Multiplying by 7===
'''Rule:'''
#Double each digit.
#Add half of its neighbor.
#If the digit is odd, add 5.

Example: '''523 × 7 = 3,661.'''

: 3×2 + 0 + 5 = 11, 1.
: 2×2 + 1 + 1 = 6.
: 5×2 + 1 + 5 = 16, 6.
: 0×2 + 2 + 1 = 3.
3661.

===Multiplying by 8===
'''Rule:'''
#Subtract right-most digit from 10.
##Subtract the remaining digits from 9.
#Double the result.
#Add the neighbor.
#For the leading zero, subtract 2 from the neighbor.

'''Example:''' 456 × 8 = 3648

Working from right to left:
*(10 − 6) × 2 + 0 = 8. Write 8.
*(9 − 5) × 2 + 6 = 14, Write 4, carry 1.
*(9 − 4) × 2 + 5 + 1 (carried) = 16. Write 6, carry 1.
*4 − 2 + 1 (carried) = 3. Write 3.


===Multiplying by 9===
'''Rules:'''
#Subtract the right-most digit from 10.
##Subtract the remaining digits from 9.
#Add the neighbor.
#For the leading zero, subtract 1 from the neighbor.


For rules 9, 8, 4, and 3 only the first digit is subtracted from 10.  After that each digit is subtracted from nine instead.

'''Example:''' 2,130 × 9 = 19,170

Working from right to left:
*(10 − 0) + 0 = 10. Write 0, carry 1.
*(9 − 3) + 0 + 1 (carried) = 7. Write 7.
*(9 − 1) + 3 = 11.  Write 1, carry 1.
*(9 − 2) + 1 + 1 (carried) = 9.  Write 9.
*2 − 1 = 1. Write 1.

===Multiplying by 11===
'''Rule:''' Add the digit to its neighbor. (By "neighbor" we mean the digit on the right.)

'''Example:''' &lt;math&gt;3,425 \times 11 = 37,675&lt;/math&gt;
: (0 + 3) (3 + 4) (4 + 2) (2 + 5) (5 + 0)
: 3      7      6      7      5

To illustrate:
: &lt;math&gt;11=10+1&lt;/math&gt;

Thus,
: &lt;math&gt;3425 \times 11 = 3425 \times (10+1)&lt;/math&gt;
: &lt;math&gt; \rightarrow 37675 = 34250 + 3425 &lt;/math&gt;

===Multiplying by 12===

'''Rule:''' to multiply by [[12 (number)|12]]:&lt;br /&gt;
Starting from the rightmost digit,
double each digit and add the neighbor. (The "neighbor" is the digit on the right.)

If the answer is greater than a single digit, simply carry over the extra digit (which will be a 1 or 2) to the next operation.
The remaining digit is one digit of the final result.

'''Example:'''  &lt;math&gt;316 \times 12&lt;/math&gt;

Determine neighbors in the multiplicand 0316:
* digit 6 has no right neighbor
* digit 1 has neighbor 6
* digit 3 has neighbor 1
* digit 0 (the prefixed zero) has neighbor 3
: &lt;math&gt;
\begin{align}
6 \times 2         &amp; = 12 \text{ (2 carry 1) } \\
1 \times 2 + 6 + 1 &amp; = 9 \\
3 \times 2 + 1     &amp; = 7 \\
0 \times 2 + 3     &amp; = 3 \\
0 \times 2 + 0     &amp; = 0 \\[10pt]
316 \times 12 &amp; = 3,792
\end{align}
&lt;/math&gt;

==Publications==
* Rushan Ziatdinov, Sajid Musa. ''Rapid mental computation system as a tool for algorithmic thinking of elementary school students development''. [http://erjournal.ru/en/index.html European Researcher] 25(7): 1105-1110, 2012 [http://erjournal.ru/journals_n/1342467174.pdf].
* ''The Trachtenberg Speed System of Basic Mathematics'' by Jakow Trachtenberg, A. Cutler (Translator), R. McShane (Translator), was published by Doubleday and Company, Inc. Garden City, New York in 1960.&lt;ref&gt;{{cite book
|title= The Trachtenberg Speed System of Basic Mathematics
|last=Trachtenberg
|first=Jakow
|year=1960
|pages=270
|publisher=Doubleday and Company, Inc.
|others=Translated by A. Cutler, R. McShane
}}&lt;/ref&gt;

The book contains specific algebraic explanations for each of the above operations.

Most of the information in this article is from the original book.

The algorithms/operations for multiplication, etc., can be expressed in other more compact ways that the book does not specify, despite the chapter on algebraic description.&lt;ref&gt;All of this information is from an original book published and printed in 1960. The original book has seven full Chapters and is 270 pages long.
The chapter titles are as follows. The numerous sub-categories in each chapter are not listed.
&lt;/ref&gt;
&lt;ref&gt;
The Trachtenberg speed system of basic mathematics&lt;br /&gt;
*Chapter 1  Tables or no tables
*Chapter 2  Rapid multiplication by the direct method
*Chapter 3  Speed multiplication-"two-finger" method
*Chapter 4  Addition and the right answer
*Chapter 5  Division – Speed and accuracy
*Chapter 6  Squares and square roots
*Chapter 7  Algebraic description of the method
&lt;br /&gt;
"A revolutionary new method for high-speed multiplication, division, addition, subtraction and square root." (1960)
"The best selling method for high-speed multiplication, division, addition, subtraction and square root – without a calculator." (Reprinted 2009)
&lt;br /&gt;
Multiplication is done without multiplication tables
"Can you multiply 5132437201 times 4522736502785 in seventy seconds?"
"One young boy (grammar school-no calculator) did--successfully--by
using The Trachtenberg Speed System of Basic Mathematics"
&lt;br /&gt;
Jakow Trachtenberg (its founder) escaped from Hitler's Germany from an active institution toward the close of WWII. Professor Trachtenberg fled to Germany when the czarist regime was overthrown in his homeland, Russia, and lived there peacefully until his mid-thirties when
his anti-Hitler attitudes forced him to flee again. He was a fugitive and when captured spent a total of seven years in various concentration camps. It was during these years that Professor Trachtenberg devised the system of speed mathematics. Most of his work was done without pen or paper. Therefore most of the techniques can be performed mentally.
&lt;/ref&gt;

== In popular culture ==
The 2017 American film ''[[Gifted (film)|Gifted]]'' revolves around a child prodigy who at the age of 7 impresses her teacher by doing calculations in her head using the Trachtenberg system.&lt;ref&gt;{{cite tweet |user=GiftedtheMovie |number=839928490904297473 |date=9 March 2017 |script-title=&lt;nowiki&gt;Hobbies include playing with legos and learning the Trachtenberg system 👷‍♀️📚✏️ @McKennaGraceful is Mary // #GiftedMovie https://t.co/dBrnSzDvpE&lt;/nowiki&gt; }}&lt;/ref&gt;

== Other systems ==
There are many other methods of calculation in mental mathematics. The list below shows a few other methods of calculating, though they may not be entirely mental.

*[[Bharati Krishna Tirtha's Vedic mathematics|Bharati Krishna Tirtha's book "Vedic mathematics]]"
*[[Mental abacus]] – As students become used to manipulating the [[abacus]] with their fingers, they are typically asked to do calculation by visualizing abacus in their head. Almost all proficient abacus users are adept at doing arithmetic mentally.{{Citation needed|date=July 2010}}
*[[Chisanbop]]

== Software ==
{{External links|section|date=May 2016}}
Following are known programs and sources available as teaching tools

'''Web'''
*Vedic Mathematics Academy  [https://web.archive.org/web/20130106190645/http://vedicmaths.org/Other%20Material/Trachtenberg.asp]
*Trachtenberg Speed Math  [https://trachtenbergspeedmath.com]

'''iPhone'''
*Mercury Math [https://itunes.apple.com/us/app/mercury-math-fast-mathematics/id318133094?mt=8]
*Multiply Without Times Table (both iPhone and iPad) [https://itunes.apple.com/us/app/trachtenberg-speed-system/id708337019]

'''Android'''
*Multiply Without Times Table - Google Play [https://play.google.com/store/apps/details?id=com.ammobile.mentalcalc], Amazon [https://www.amazon.com/Trachtenberg-Speed-System-Multiplication-Without/dp/B00FBBLMJQ], Barnes and Noble [http://www.barnesandnoble.com/w/trachtenberg-speed-system-basic-multiplication-without-times-table-am-mobile/1116970588]

'''BlackBerry'''
*Multiply Without Times Table [http://appworld.blackberry.com/webstore/content/36799894/]

==References==

* Trachtenberg, J. (1960). The Trachtenberg Speed System of Basic Mathematics. Doubleday and Company, Inc., Garden City, NY, USA.
* Катлер Э., Мак-Шейн Р.''Система быстрого счёта по Трахтенбергу'', 1967.
* Rushan Ziatdinov, Sajid Musa. ''Rapid mental computation system as a tool for algorithmic thinking of elementary school students development''. European Researcher 25(7): 1105-1110, 2012 [http://erjournal.ru/journals_n/1342467174.pdf].

{{reflist}}

==External links==
*[http://www.sapnaedu.in/category/mathematical-shortcuts Learn All about Mathematical Shortcuts]

{{DEFAULTSORT:Trachtenberg System}}
[[Category:Arithmetic]]
[[Category:Mental calculation]]</text>
      <sha1>5f4xit4o3a798sh2z6oadj4bx0wvosi</sha1>
    </revision>
  </page>
  <page>
    <title>Triangle inequality</title>
    <ns>0</ns>
    <id>53941</id>
    <revision>
      <id>868976985</id>
      <parentid>868052732</parentid>
      <timestamp>2018-11-15T16:49:13Z</timestamp>
      <contributor>
        <ip>82.32.47.175</ip>
      </contributor>
      <comment>/* Reverse triangle inequality */ fix minus sign inside || and use \cdot as place holder</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="29815">{{about|the basic inequality &lt;math&gt;z\le x+y&lt;/math&gt;&lt;!-- DO NOT USE {{math}}. IT DOES NOT WORK INSIDE HATNOTES. --&gt;|other inequalities associated with triangles|List of triangle inequalities}}

[[File:TriangleInequality.svg|thumb|Three examples of the triangle inequality for triangles with sides of lengths {{math|''x''}}, {{math|''y''}}, {{math|''z''}}. The top example shows a case where {{math|''z''}} is much less than the sum {{math|''x'' + ''y''}} of the other two sides,  and the bottom example shows a case where the side {{math|''z''}} is only slightly less than {{math|''x'' + ''y''}}.]]

In [[mathematics]], the '''triangle inequality''' states that for any [[triangle]], the sum of the lengths of any two sides must be greater than or equal to the length of the remaining side.&lt;ref&gt;Wolfram MathWorld – http://mathworld.wolfram.com/TriangleInequality.html&lt;/ref&gt;&lt;ref name=Khamsi&gt;
{{cite book |title=An introduction to metric spaces and fixed point theory |author1=Mohamed A. Khamsi |author2=William A. Kirk |url=https://books.google.com/?id=4qXbEpAK5eUC&amp;pg=PA8 |chapter=§1.4 The triangle inequality in {{math|ℝ&lt;sup&gt;n&lt;/sup&gt;}} |isbn=0-471-41825-0 |year=2001 |publisher=Wiley-IEEE}}&lt;/ref&gt; This statement permits the inclusion of [[Degeneracy (mathematics)#Triangle|degenerate triangles]], but some authors, especially those writing about elementary geometry, will exclude this possibility, thus leaving out the possibility of equality.&lt;ref&gt;for instance, {{citation|first=Harold R.|last=Jacobs|title=Geometry|year=1974|publisher=W. H. Freeman &amp; Co.|isbn=0-7167-0456-0|page=246}}&lt;/ref&gt; If {{math|''x''}}, {{math|''y''}}, and {{math|''z''}} are the lengths of the sides of the triangle, with no side being greater than {{math|''z''}}, then the triangle inequality states that
:&lt;math&gt;z \leq x + y ,&lt;/math&gt;
with equality only in the degenerate case of a triangle with zero area.
In [[Euclidean geometry]] and some other geometries, the triangle inequality is a theorem about distances, and it is written using vectors and vector lengths ([[Norm (mathematics)|norms]]):
:&lt;math&gt;\|\mathbf x + \mathbf y\| \leq \|\mathbf x\| + \|\mathbf y\| ,&lt;/math&gt;
where the length {{math|''z''}} of the third side has been replaced by the vector sum {{math|'''x''' + '''y'''}}. When {{math|'''x'''}} and {{math|'''y'''}} are [[real number]]s, they can be viewed as vectors in {{math|ℝ&lt;sup&gt;1&lt;/sup&gt;}}, and the triangle inequality expresses a relationship between [[absolute value]]s.

In Euclidean geometry, for [[right triangle]]s the triangle inequality is a consequence of the [[Pythagorean theorem]], and for general triangles a consequence of the [[law of cosines]], although it may be proven without these theorems. The inequality can be viewed intuitively in either {{math|ℝ&lt;sup&gt;2&lt;/sup&gt;}} or {{math|ℝ&lt;sup&gt;3&lt;/sup&gt;}}. The figure at the right shows three examples beginning with clear inequality (top) and approaching equality (bottom). In the Euclidean case, equality occurs only if the triangle has a {{math|180°}} angle and two {{math|0°}} angles, making the three [[Vertex (geometry)|vertices]] [[Straight line|collinear]], as shown in the bottom example. Thus, in Euclidean geometry, the shortest distance between two points is a straight line.

In [[spherical geometry]], the shortest distance between two points is an arc of a [[great circle]], but the triangle inequality holds provided the restriction is made that the distance between two points on a sphere is the length of a minor spherical line segment (that is, one with central angle in {{math|[0, {{pi}}]}}) with those endpoints.&lt;ref name= Ramos&gt;

{{cite book |title=Robotics: Science and Systems IV |author1=Oliver Brock |author2=Jeff Trinkle |author3=Fabio Ramos |url=https://books.google.com/?id=fvCaQfBQ7qEC&amp;pg=PA195 |page=195 |isbn=0-262-51309-9 |publisher=MIT Press |year=2009}}

&lt;/ref&gt;&lt;ref name=Ramsay&gt;

{{cite book |title=Introduction to hyperbolic geometry |author1=Arlan Ramsay |author2=Robert D. Richtmyer |url=https://books.google.com/?id=0QA_1lKC0dwC&amp;pg=PA17 |page=17 |isbn=0-387-94339-0 |year=1995 |publisher=Springer}}

&lt;/ref&gt;

The triangle inequality is a ''defining property'' of [[norm (mathematics)|norms]] and measures of [[Metric (mathematics)#Definition|distance]]. This property must be established as a theorem for any function proposed for such purposes for each particular space: for example, spaces such as the [[real number]]s, [[Euclidean space]]s, the [[Lp space|L&lt;sup&gt;p&lt;/sup&gt; space]]s ({{math|''p'' ≥ 1}}), and [[inner product space]]s.

==Euclidean geometry==

[[File:Euclid triangle inequality.svg|thumb|Euclid's construction for proof of the triangle inequality for plane geometry.]]

Euclid proved the triangle inequality for distances in [[Euclidean geometry|plane geometry]] using the construction in the figure.&lt;ref name=Jacobs&gt;

{{cite book |page=201 |author=Harold R. Jacobs |title=Geometry: seeing, doing, understanding |url=https://books.google.com/?id=XhQRgZRDDq0C&amp;pg=PA201 |isbn=0-7167-4361-2 |edition=3rd |publisher=Macmillan |year=2003}}

&lt;/ref&gt; Beginning with triangle {{math|ABC}}, an isosceles triangle is constructed with one side taken as {{math|BC}} and the other equal leg {{math|BD}} along the extension of side {{math|AB}}. It then is argued that angle {{math|''β'' &gt; ''α''}}, so side {{math|{{overline|AD}} &gt; {{overline|AC}}}}. But {{math|{{overline|AD}} {{=}} {{overline|AB}} + {{overline|BD}} {{=}} {{overline|AB}} + {{overline|BC}}}} so the sum of sides {{math|{{overline|AB}} + {{overline|BC}} &gt; {{overline|AC}}}}. This proof appears in [[Euclid's Elements]], Book 1, Proposition 20.&lt;ref name=Joyce&gt;

{{cite web
| url         = http://aleph0.clarku.edu/~djoyce/java/elements/bookI/propI20.html
| title       = Euclid's elements, Book 1, Proposition 20
| author      = David E. Joyce
| date        = 
| month       = 
| year        = 1997
| work        = Euclid's elements
| publisher   = Dept. Math and Computer Science, Clark University
| location    = 
| page        = 
| pages       = 
| at          = 
| language    = 
|trans-title=| doi         = 
| archiveurl  = 
| archivedate = 
| accessdate  = 2010-06-25
| quote       = 
| ref         = 
| separator   = 
| postscript  = 
}}

&lt;/ref&gt;

===Mathematical expression of the constraint on the sides of a triangle===

For a proper triangle, the triangle inequality, as stated in words, literally translates into three inequalities (given that a proper triangle has side lengths {{math|''a''}}, {{math|''b''}}, {{math|''c''}} that are all positive and excludes the degenerate case of zero area):
:&lt;math&gt;a + b &gt; c ,\quad b + c &gt; a ,\quad c + a &gt; b .&lt;/math&gt;
A more succinct form of this inequality system can be shown to be
:&lt;math&gt;|a - b| &lt; c &lt; a + b .&lt;/math&gt;
Another way to state it is
:&lt;math&gt;\max(a,\text{ }b,\text{ }c) &lt; a + b + c - \max(a,\text{ }b,\text{ } c)&lt;/math&gt;
implying
:&lt;math&gt;2 \max(a,\text{ }b,\text{ } c) &lt; a + b + c&lt;/math&gt;
and thus that the longest side length is less than the [[semiperimeter]].

A mathematically equivalent formulation is that the area of a triangle with sides ''a'', ''b'', ''c'' must be a real number greater than zero. [[Heron's formula]] for the area is

:&lt;math&gt;
\begin{align}
4\cdot \text{area} &amp; =\sqrt{(a+b+c)(-a+b+c)(a-b+c)(a+b-c)} \\
&amp; = \sqrt{-a^4-b^4-c^4+2a^2b^2+2a^2c^2+2b^2c^2}.
\end{align}
&lt;/math&gt;

In terms of either area expression, the triangle inequality imposed on all sides is equivalent to the condition that the expression under the square root sign be real and greater than zero (so the area expression is real and greater than zero).

The triangle inequality provides two more interesting constraints for triangles whose sides are ''a, b, c'', where ''a &amp;ge; b &amp;ge; c'' and ''&lt;math&gt;\phi&lt;/math&gt;'' is the [[golden ratio]], as
:&lt;math&gt;1&lt;\frac{a+c}{b}&lt;3&lt;/math&gt;

:&lt;math&gt;1\le\min(\frac{a}{b},\text{ }
 \frac{b}{c})&lt;\phi.&lt;/math&gt;&lt;ref&gt;''[[American Mathematical Monthly]]'', pp. 49-50, 1954.&lt;/ref&gt;

===Right triangle===

[[File:Isosceles triangle made of right triangles.svg|thumb|Isosceles triangle with equal sides {{math|{{overline|AB}} {{=}} {{overline|AC}}}} divided into two right triangles by an altitude drawn from one of the two base angles.]]

In the case of right triangles, the triangle inequality specializes to the statement that the hypotenuse is greater than either of the two sides, and less than their sum.&lt;ref name=Palmer&gt;
{{cite book |title=Practical mathematics for home study: being the essentials of arithmetic, geometry, algebra and trigonometry |author=Claude Irwin Palmer |url=https://books.google.com/?id=EAmgAAAAMAAJ&amp;pg=PA422 |page=422 |publisher=McGraw-Hill |year=1919}}
&lt;/ref&gt;

The second part of this theorem is already established above for any side of any triangle. The first part is established using the lower figure. In the figure, consider the right triangle {{math|ADC}}. An isosceles triangle {{math|ABC}} is constructed with equal sides {{math|{{overline|AB}} {{=}} {{overline|AC}}}}. From the [[triangle postulate]], the angles in the right triangle {{math|ADC}} satisfy:
:&lt;math&gt; \alpha + \gamma = \pi /2 \ . &lt;/math&gt;
Likewise, in the isosceles triangle {{math|ABC}}, the angles satisfy:
:&lt;math&gt;2\beta + \gamma = \pi \ . &lt;/math&gt;
Therefore,
:&lt;math&gt; \alpha = \pi/2 - \gamma ,\ \mathrm{while} \ \beta= \pi/2 - \gamma /2  \ ,&lt;/math&gt;
and so, in particular,
:&lt;math&gt;\alpha &lt; \beta \ . &lt;/math&gt;
That means side {{math|AD}} opposite angle {{math|''α''}} is shorter than side {{math|AB}} opposite the larger angle {{math|''β''}}. But {{math|{{overline|AB}} {{=}} {{overline|AC}}}}. Hence:
:&lt;math&gt;\overline{\mathrm{AC}} &gt; \overline{\mathrm{AD}} \ . &lt;/math&gt;
A similar construction shows {{math|{{overline|AC}} &gt; {{overline|DC}}}}, establishing the theorem.

An alternative proof (also based upon the triangle postulate) proceeds by considering three positions for point {{math|B}}:&lt;ref name=Zawaira&gt;

{{cite book |title=A primer for mathematics competitions |url=https://books.google.com/?id=A21T73sqZ3AC&amp;pg=PA30 |chapter=Lemma 1: In a right-angled triangle the hypotenuse is greater than either of the other two sides |author1=Alexander Zawaira |author2=Gavin Hitchcock |isbn=0-19-953988-X |year=2009 |publisher=Oxford University Press}}

&lt;/ref&gt; (i) as depicted (which is to be proven), or (ii) {{math|B}} coincident with {{math|D}} (which would mean the isosceles triangle had two right angles as base angles plus the vertex angle {{math|''γ''}}, which would violate the [[triangle postulate]]), or lastly, (iii) {{math|B}} interior to the right triangle between points {{math|A}} and {{math|D}} (in which case angle {{math|ABC}} is an exterior angle of a right triangle {{math|BDC}} and therefore larger than {{math|''π''/2}}, meaning the other base angle of the isosceles triangle also is greater than {{math|''π''/2}} and their sum exceeds {{math|''π''}} in violation of the triangle postulate).

This theorem establishing inequalities is sharpened by [[Pythagoras' theorem]] to the equality that the square of the length of the hypotenuse equals  the sum of the squares of the other two sides.

===Examples of use===
Consider a triangle whose sides are in an [[arithmetic progression]] and let the sides be {{math|''a''}}, {{math|''a'' + ''d''}}, {{math|''a'' + 2''d''}}. Then the triangle inequality requires that

:&lt;math&gt;
0&lt;a&lt;2a+3d &lt;/math&gt;
:&lt;math&gt;
0&lt;a+d&lt;2a+2d &lt;/math&gt;
:&lt;math&gt;
0&lt;a+2d&lt;2a+d. &lt;/math&gt;

To satisfy all these inequalities requires

:&lt;math&gt; a&gt;0 \text{ and } -\frac{a}{3}&lt;d&lt;a. &lt;/math&gt;&lt;ref&gt;{{cite journal|title=input: ''solve 0&lt;a&lt;2a+3d, 0&lt;a+d&lt;2a+2d, 0&lt;a+2d&lt;2a+d,'' |last=Wolfram{{!}}Alpha|journal=Wolfram Research|url=http://www.wolframalpha.com/input/?i=solve%200%3Ca%3C2a%2B3d%2C%200%3Ca%2Bd%3C2a%2B2d%2C%200%3Ca%2B2d%3C2a%2Bd&amp;t=ff3tb01|accessdate=2010-09-07}}&lt;/ref&gt;

When {{math|''d''}} is chosen such that {{math|''d'' {{=}} ''a''/3}}, it generates a right triangle that is always similar to the [[Pythagorean triple]] with sides {{math|3}}, {{math|4}}, {{math|5}}.

Now consider a triangle whose sides are in a [[geometric progression]] and let the sides be {{math|''a''}}, {{math|''ar''}}, {{math|''ar''&lt;sup&gt;2&lt;/sup&gt;}}. Then the triangle inequality requires that

:&lt;math&gt; 0&lt;a&lt;ar+ar^2 &lt;/math&gt;
:&lt;math&gt; 0&lt;ar&lt;a+ar^2 &lt;/math&gt;
:&lt;math&gt; 0&lt;ar^2&lt;a+ar. &lt;/math&gt;

The first inequality requires {{math|''a'' &gt; 0}}; consequently it can be divided through and eliminated. With {{math|''a'' &gt; 0}}, the middle inequality only requires {{math|''r'' &gt; 0}}. This now leaves the first and third inequalities needing to satisfy

:&lt;math&gt;
\begin{align}
r^2+r-1 &amp; {} &gt;0 \\
r^2-r-1 &amp; {} &lt;0.
\end{align}
&lt;/math&gt;

The first of these quadratic inequalities requires {{math|''r''}} to range in the region beyond the value of the positive root of the quadratic equation {{math|''r''&lt;sup&gt;2&lt;/sup&gt; + ''r'' − 1 {{=}} 0}}, i.e. {{math|''r'' &gt; ''φ'' − 1}}  where {{math|''φ''}} is the [[golden ratio]]. The second quadratic inequality requires {{math|''r''}} to range between 0 and the positive root of the quadratic equation {{math|''r''&lt;sup&gt;2&lt;/sup&gt; − ''r'' − 1 {{=}} 0}}, i.e. {{math|0 &lt; ''r'' &lt; ''φ''}}. The combined requirements result in {{math|''r''}} being confined to the range
:&lt;math&gt;\varphi - 1 &lt; r &lt;\varphi\, \text{ and } a &gt;0.&lt;/math&gt;&lt;ref&gt;{{cite journal|title=input: ''solve 0&lt;a&lt;ar+ar&lt;sup&gt;2&lt;/sup&gt;, 0&lt;ar&lt;a+ar&lt;sup&gt;2&lt;/sup&gt;, 0&lt;ar&lt;sup&gt;2&lt;/sup&gt;&lt;a+ar'' |last=Wolfram{{!}}Alpha|journal=Wolfram Research|url=http://wolframalpha.com/input?i=solve+0%3Ca%3Car%2Bar^2%2C+0%3Car%3Ca%2Bar^2%2C+0%3Car^2%3Ca%2Bar|accessdate=2010-09-07}}&lt;/ref&gt;

When {{math|''r''}} the common ratio is chosen such that {{math|''r'' {{=}} {{sqrt|''φ''}}}} it generates a right triangle that is always similar to the [[Kepler triangle]].

===Generalization to any polygon===
The triangle inequality can be extended by [[mathematical induction]] to arbitrary polygonal paths, showing that the total length of such a path is no less than the length of the straight line between its endpoints. Consequently, the length of any polygon side is always less than the sum of the other polygon side lengths.

====Example of the generalized polygon inequality for a quadrilateral====
Consider a quadrilateral whose sides are in a [[geometric progression]] and let the sides be {{math|''a''}}, {{math|''ar''}}, {{math|''ar''&lt;sup&gt;2&lt;/sup&gt;}}, {{math|''ar''&lt;sup&gt;3&lt;/sup&gt;}}. Then the generalized polygon inequality requires that

:&lt;math&gt; 0&lt;a&lt;ar+ar^2+ar^3 &lt;/math&gt;
:&lt;math&gt; 0&lt;ar&lt;a+ar^2+ar^3 &lt;/math&gt;
:&lt;math&gt; 0&lt;ar^2&lt;a+ar+ar^3 &lt;/math&gt;
:&lt;math&gt; 0&lt;ar^3&lt;a+ar+ar^2. &lt;/math&gt;

These inequalities for {{math|''a'' &gt; 0}} reduce to the following

:&lt;math&gt; r^3+r^2+r-1&gt;0 &lt;/math&gt;
:&lt;math&gt; r^3-r^2-r-1&lt;0. &lt;/math&gt;&lt;ref&gt;{{cite journal|title=input: ''solve 0&lt;a&lt;ar+ar&lt;sup&gt;2&lt;/sup&gt;+ar&lt;sup&gt;3&lt;/sup&gt;,  0&lt;ar&lt;sup&gt;3&lt;/sup&gt;&lt;a+ar+ar&lt;sup&gt;2&lt;/sup&gt;'' |last=Wolfram{{!}}Alpha|journal=Wolfram Research|url=http://www.wolframalpha.com/input/?i=solve%20{0%3Ca%3Ca*r%2Ba*r^2%2Ba*r^3%2C%200%3Ca*r^3%3Ca%2Ba*r%2Ba*r^2}&amp;t=ff3tb01|accessdate=2012-07-29}}&lt;/ref&gt;
The left-hand side polynomials of these two inequalities have roots that are the [[Generalizations of Fibonacci numbers#Tribonacci numbers|tribonacci constant]] and its reciprocal. Consequently, {{math|''r''}} is limited to the range {{math|1/''t'' &lt; ''r'' &lt; ''t''}} where {{math|''t''}} is the tribonacci constant.

====Relationship with shortest paths====
[[File:Arclength.svg|300px|thumb|The arc length of a curve is defined as the least upper bound of the lengths of polygonal approximations.]]
This generalization can be used to prove that the shortest curve between two points in Euclidean geometry is a straight line.

No polygonal path between two points is shorter than the line between them. This implies that no curve can have an [[arc length]] less than the distance between its endpoints. By definition, the arc length of a curve is the [[least upper bound]] of the lengths of all polygonal approximations of the curve. The result for polygonal paths shows that the straight line between the endpoints is shortest of all the polygonal approximations. Because the arc length of the curve is greater than or equal to the length of every polygonal approximation, the curve itself cannot be shorter than the straight line path.&lt;ref&gt;{{cite book|title=Numbers and Geometry|author=John Stillwell|authorlink=John Stillwell|year=1997|publisher=Springer|isbn=978-0-387-98289-2|url=https://books.google.com/?id=4elkHwVS0eUC&amp;pg=PA95}} p. 95.&lt;/ref&gt;

===Converse===

The converse of the triangle inequality theorem is also true: if three real numbers are such that each is less than the sum of the others, then there exists a triangle with these numbers as its side lengths and with positive area; and if one number equals the sum of the other two, there exists a degenerate triangle (i.e., with zero area) with these numbers as its side lengths.

In either case, if the side lengths are ''a, b, c'' we can attempt to place a triangle in the [[Euclidean plane]] as shown in the diagram. We need to prove that there exists a real number ''h'' consistent with the values ''a, b,'' and ''c'', in which case this triangle exists.

[[Image:Triangle with notations 3.svg|thumb|270px|Triangle with altitude {{math|''h''}} cutting base {{math|''c''}} into {{math|''d'' + (''c'' − ''d'')}}.]]

By the [[Pythagorean theorem]] we have {{math|''b''{{sup|2}} {{=}} ''h''{{sup|2}} + ''d''{{sup|2}}}} and {{math|''a''{{sup|2}} {{=}} ''h''{{sup|2}} + (''c'' − ''d''){{sup|2}}}} according to the figure at the right. Subtracting these yields {{math|''a''{{sup|2}} − ''b''{{sup|2}} {{=}} ''c''{{sup|2}} − 2''cd''}}. This equation allows us to express {{math|''d''}} in terms of the sides of the triangle:
:&lt;math&gt;d=\frac{-a^2+b^2+c^2}{2c}.&lt;/math&gt;
For the height of the triangle we have that {{math|''h''{{sup|2}} {{=}} ''b''{{sup|2}} − ''d''{{sup|2}}}}. By replacing {{math|''d''}} with the formula given above, we have

:&lt;math&gt;h^2 = b^2-\left(\frac{-a^2+b^2+c^2}{2c}\right)^2.&lt;/math&gt;

For a real number ''h'' to satisfy this, &lt;math&gt;h^2&lt;/math&gt; must be non-negative:
:&lt;math&gt;b^2-\left (\frac{-a^2+b^2+c^2}{2c}\right) ^2 \ge 0,&lt;/math&gt;
:&lt;math&gt;\left( b- \frac{-a^2+b^2+c^2}{2c}\right) \left( b+ \frac{-a^2+b^2+c^2}{2c}\right) \ge 0,&lt;/math&gt;
:&lt;math&gt;\left(a^2-(b-c)^2)((b+c)^2-a^2 \right) \ge 0,&lt;/math&gt;
:&lt;math&gt;(a+b-c)(a-b+c)(b+c+a)(b+c-a) \ge 0,&lt;/math&gt;
:&lt;math&gt;(a+b-c)(a+c-b)(b+c-a) \ge 0,&lt;/math&gt;
which holds if the triangle inequality is satisfied for all sides. Therefore there does exist a real number ''h'' consistent with the sides ''a, b, c'', and the triangle exists. If each triangle inequality holds [[strict inequality|strictly]], ''h'' &gt; 0 and the triangle is non-degenerate (has positive area); but if one of the inequalities holds with equality, so ''h'' = 0, the triangle is degenerate.

===Generalization to higher dimensions===
In Euclidean space, the hypervolume of an {{math|(''n'' − 1)}}-[[Facet (mathematics)|facet]] of an {{math|''n''}}-[[simplex]] is less than or equal to the sum of the hypervolumes of the other {{math|''n''}} facets.  In particular, the area of a triangular face of a [[tetrahedron]] is less than or equal to the sum of the areas of the other three sides.

==Normed vector space==
[[File:Vector-triangle-inequality.svg|thumb|300px|Triangle inequality for norms of vectors.]]
In a [[normed vector space]] {{math|''V''}}, one of the defining properties of the [[norm (mathematics)|norm]] is the triangle inequality:

:&lt;math&gt;\displaystyle \|x + y\| \leq \|x\| + \|y\| \quad \forall \, x, y \in V&lt;/math&gt;

that is, the norm of the [[Vector sum#Addition and subtraction|sum of two vectors]] is at most as large as the sum of the norms of the two vectors.  This is also referred to as [[subadditivity]]. For any proposed function to behave as a norm, it must satisfy this requirement.&lt;ref name=Kress&gt;

{{cite book |title=Numerical analysis |author=Rainer Kress |chapter=§3.1: Normed spaces |url=https://books.google.com/?id=e7ZmHRIxum0C&amp;pg=PA26 |page=26 |isbn=0-387-98408-9 |year=1988 |publisher=Springer}}&lt;/ref&gt;

If the normed space is [[euclidean space|euclidean]], or, more generally, [[strictly convex space|strictly convex]], then &lt;math&gt;\|x+y\|=\|x\|+\|y\|&lt;/math&gt; if and
only if the triangle formed by {{math|''x''}}, {{math|''y''}}, and {{math|''x'' + ''y''}}, is degenerate, that is,
{{math|''x''}} and {{math|''y''}} are on the same ray, i.e., {{math|''x'' {{=}} 0}} or {{math|''y'' {{=}} 0}}, or
{{math|''x'' {{=}} ''α y''}} for some {{math|''α'' &gt; 0}}. This property characterizes strictly convex normed spaces such as
the {{math|''ℓ&lt;sub&gt;p&lt;/sub&gt;''}} spaces with {{math|1 &lt; ''p'' &lt; ∞}}. However, there are normed spaces in which this is
not true. For instance, consider the plane with the {{math|''ℓ''&lt;sub&gt;1&lt;/sub&gt;}} norm (the [[Manhattan distance]]) and
denote {{math|''x'' {{=}} (1, 0)}} and {{math|''y'' {{=}} (0, 1)}}. Then the triangle formed by
{{math|''x''}}, {{math|''y''}}, and {{math|''x'' + ''y''}}, is non-degenerate but

:&lt;math&gt;\|x+y\|=\|(1,1)\|=|1|+|1|=2=\|x\|+\|y\|.&lt;/math&gt;

===Example norms===
*''Absolute value as norm for the [[real line]].'' To be a norm, the triangle inequality requires that the [[absolute value]] satisfy for any real numbers {{math|''x''}} and {{math|''y''}}:
::&lt;math&gt;|x + y| \leq |x|+|y|,&lt;/math&gt;

:which it does.

Proof:&lt;ref name=Stewart&gt;

{{cite book |page=A10 |author=James Stewart |title=Essential Calculus |isbn=978-0-495-10860-3 |publisher=Thomson Brooks/Cole, |year=2008}}

&lt;/ref&gt;

:&lt;math&gt;-\left\vert x \right\vert \leq x \leq \left\vert x \right\vert&lt;/math&gt;
:&lt;math&gt;-\left\vert y \right\vert \leq y \leq \left\vert y \right\vert&lt;/math&gt;
After adding, 
:&lt;math&gt;-( \left\vert x \right\vert + \left\vert y \right\vert ) \leq x+y \leq \left\vert x \right\vert + \left\vert y \right\vert&lt;/math&gt;
Use the fact that &lt;math&gt;\left\vert b \right\vert \leq a \Leftrightarrow -a \leq b \leq a&lt;/math&gt;
(with ''b'' replaced by ''x''+''y'' and ''a'' by &lt;math&gt;\left\vert x \right\vert + \left\vert y \right\vert&lt;/math&gt;), we have

:&lt;math&gt;|x + y| \leq |x|+|y|&lt;/math&gt;

The triangle inequality is useful in [[mathematical analysis]] for determining the best upper estimate on the size of the sum of two numbers, in terms of the sizes of the individual numbers.

There is also a lower estimate, which can be found using the ''reverse triangle inequality'' which states that for any real numbers {{math|''x''}} and {{math|''y''}}:

:&lt;math&gt;|x-y| \geq \bigg||x|-|y|\bigg|.&lt;/math&gt;

*''Inner product as norm in an [[inner product space]].'' If the norm arises from an inner product (as is the case for Euclidean spaces), then the triangle inequality follows from the [[Cauchy–Schwarz inequality]] as follows: Given vectors &lt;math&gt;x&lt;/math&gt; and &lt;math&gt;y&lt;/math&gt;, and denoting the inner product as &lt;math&gt;\langle x , y\rangle &lt;/math&gt;:&lt;ref name= Stillwell&gt;

{{cite book |title=The four pillars of geometry |author=John Stillwell |page=80 |url=https://books.google.com/?id=fpAjJ6VJ3y8C&amp;pg=PA80 |isbn=0-387-25530-3 |year=2005 |publisher=Springer}}

&lt;/ref&gt;
:{|
|&lt;math&gt;\|x + y\|^2&lt;/math&gt; || &lt;math&gt;= \langle x + y, x + y \rangle&lt;/math&gt;
|-
| || &lt;math&gt;= \|x\|^2 + \langle x, y \rangle + \langle y, x \rangle + \|y\|^2&lt;/math&gt;
|-
| || &lt;math&gt;\le \|x\|^2 + 2|\langle x, y \rangle| + \|y\|^2&lt;/math&gt;
|-
| || &lt;math&gt;\le \|x\|^2 + 2\|x\|\|y\| + \|y\|^2&lt;/math&gt; (by the Cauchy–Schwarz inequality)
|-
| || &lt;math&gt;=  \left(\|x\| + \|y\|\right)^2&lt;/math&gt;
|}
:where the last form is a consequence of:

::&lt;math&gt;\|x\|^2 + 2\|x\|\|y\| + \|y\|^2 = \left(\|x\| + \|y\|\right)^2 \ .&lt;/math&gt;

The Cauchy–Schwarz inequality turns into an equality if and only if {{math|''x''}} and {{math|''y''}}
are linearly dependent. The inequality
&lt;math&gt;\langle x, y \rangle + \langle y, x \rangle \le 2|\langle x, y \rangle| &lt;/math&gt;
turns into an equality for linearly dependent &lt;math&gt;x&lt;/math&gt; and  &lt;math&gt;y&lt;/math&gt;
if and only if one of the vectors {{math|''x''}} or {{math|''y''}} is a ''nonnegative'' scalar of the other.

:Taking the square root of the final result gives the triangle inequality.
*[[p-norm|{{math|''p''}}-norm]]: a commonly used norm is the ''p''-norm:

::&lt;math&gt;\|x\|_p = \left( \sum_{i=1}^n |x_i|^p \right) ^{1/p} \ , &lt;/math&gt;

:where the {{math|''x&lt;sub&gt;i&lt;/sub&gt;''}} are the components of vector {{math|''x''}}. For {{math|''p'' {{=}} 2}} the {{math|''p''}}-norm becomes the ''Euclidean norm'':
::&lt;math&gt;\|x\|_2 = \left( \sum_{i=1}^n |x_i|^2 \right) ^{1/2} = \left( \sum_{i=1}^n x_{i}^2 \right) ^{1/2} \ , &lt;/math&gt;
:which is [[Pythagoras' theorem]] in {{math|''n''}}-dimensions, a very special case corresponding to an inner product norm. Except for the case {{math|''p'' {{=}} 2}}, the {{math|''p''}}-norm is ''not'' an inner product norm, because it does not satisfy the [[parallelogram law]]. The triangle inequality for general values of {{math|''p''}} is called [[Minkowski's inequality]].&lt;ref name=Saxe&gt;

{{cite book |title=Beginning functional analysis |author= Karen Saxe|authorlink= Karen Saxe |url=https://books.google.com/?id=0LeWJ74j8GQC&amp;pg=PA61 |page=61 |isbn=0-387-95224-1 |publisher=Springer |year=2002}}

&lt;/ref&gt; It takes the form:
::&lt;math&gt;\|x+y\|_p \le \|x\|_p + \|y\|_p \ .&lt;/math&gt;

==Metric space==
In a [[metric space]] {{math|''M''}} with metric {{math|''d''}}, the triangle inequality is a requirement upon [[Metric (mathematics)#Definition|distance]]:
:&lt;math&gt;d(x,\ z) \le d(x,\ y) + d(y,\ z) \ , &lt;/math&gt;

for all {{math|''x''}}, {{math|''y''}}, {{math|''z''}} in {{math|''M''}}. That is, the distance from {{math|''x''}} to {{math|''z''}} is at most as large as the sum of the distance from {{math|''x''}} to {{math|''y''}} and the distance from {{math|''y''}} to {{math|''z''}}.

The triangle inequality is responsible for most of the interesting structure on a metric space, namely, convergence.  This is because the remaining requirements for a metric are rather simplistic in comparison.  For example, the fact that any [[limit of a sequence|convergent sequence]] in a metric space is a [[Cauchy sequence]] is a direct consequence of the triangle inequality, because if we choose any {{math|''x&lt;sub&gt;n&lt;/sub&gt;''}} and {{math|''x&lt;sub&gt;m&lt;/sub&gt;''}} such that {{math|''d''(''x&lt;sub&gt;n&lt;/sub&gt;'', ''x'') &lt; ''ε''/2}} and {{math|''d''(''x&lt;sub&gt;m&lt;/sub&gt;'', ''x'') &lt; ''ε''/2}}, where {{math|''ε'' &gt; 0}} is given and arbitrary (as in the definition of a limit in a metric space), then by the triangle inequality, {{math|''d''(''x&lt;sub&gt;n&lt;/sub&gt;'', ''x&lt;sub&gt;m&lt;/sub&gt;'') ≤ ''d''(''x&lt;sub&gt;n&lt;/sub&gt;'', ''x'') + ''d''(''x&lt;sub&gt;m&lt;/sub&gt;'', ''x'') &lt; ''ε''/2 + ''ε''/2 {{=}} ''ε''}}, so that the sequence {{math|{{mset|''x&lt;sub&gt;n&lt;/sub&gt;''}}}} is a Cauchy sequence, by definition.

This version of the triangle inequality reduces to the one stated above in case of normed vector spaces where a metric is induced via {{math|''d''(''x'', ''y'') ≔ ‖''x'' − ''y''‖}}, with {{math|''x'' − ''y''}} being the vector pointing from point {{math|''y''}} to {{math|''x''}}.

==Reverse triangle inequality==
The '''reverse triangle inequality''' is an elementary consequence of the triangle inequality that gives lower bounds instead of upper bounds. For plane geometry the statement is:&lt;ref name=inequality&gt;

{{cite book |title=The popular educator; fourth volume |url=https://books.google.com/?id=lTACAAAAQAAJ&amp;pg=PA196 |page=196 |chapter=Exercise I. to proposition XIX |year=1854 |publisher=John Cassell |location=Ludgate Hill, London |author=Anonymous}}

&lt;/ref&gt;

:''Any side of a triangle is greater than the difference between the other two sides''.

In the case of a normed vector space, the statement is:
: &lt;math&gt;\bigg|\|x\|-\|y\|\bigg| \leq \|x-y\|,&lt;/math&gt;
or for metric spaces, {{math|{{!}}''d''(''y'', ''x'') − ''d''(''x'', ''z''){{!}} ≤ ''d''(''y'', ''z'')}}.
This implies that the norm &lt;math&gt;\|\cdot\|&lt;/math&gt; as well as the distance function &lt;math&gt;d(x,\cdot)&lt;/math&gt; are [[Lipschitz continuity|Lipschitz continuous]] with Lipschitz constant {{math|1}}, and therefore are in particular [[uniform continuity|uniformly continuous]].

The proof for the reverse triangle uses the regular triangle inequality, and &lt;math&gt; \|y-x\| = \|{-}1(x-y)\| = |{-}1|\cdot\|x-y\| = \|x-y\| &lt;/math&gt;:
: &lt;math&gt; \|x\| = \|(x-y) + y\| \leq \|x-y\| + \|y\| \Rightarrow \|x\| - \|y\| \leq \|x-y\|, &lt;/math&gt;
: &lt;math&gt; \|y\| = \|(y-x) + x\| \leq \|y-x\| + \|x\| \Rightarrow \|x\| - \|y\| \geq -\|x-y\|, &lt;/math&gt;

Combining these two statements gives:
: &lt;math&gt; -\|x-y\| \leq \|x\|-\|y\| \leq \|x-y\| \Rightarrow \bigg|\|x\|-\|y\|\bigg| \leq \|x-y\|.&lt;/math&gt;

==Reversal in Minkowski space==

In [[Minkowski space]], if ''x'' and ''y'' are both timelike vectors lying in the future light cone, the triangle inequality is reversed:

: &lt;math&gt; \|x+y\| \geq \|x\| + \|y\| &lt;/math&gt;

A physical example of this inequality is the [[twin paradox]] in [[special relativity]]. The same reversed form of the inequality holds if both vectors lie in the past light cone, and if one or both are null vectors. The result holds in ''n''+1 dimensions for any ''n''≥1.  If the plane defined by ''x'' and ''y'' is spacelike (and therefore a euclidean subspace) then the usual triangle inequality holds.

==See also==
* [[Subadditivity]]
* [[Minkowski inequality]]
* [[Ptolemy's inequality]]

==Notes==
{{reflist|30em}}

==References==
* {{Cite book|authorlink = Daniel Pedoe|last=Pedoe|first=Daniel|title = Geometry: A comprehensive course|publisher=Dover|year=1988|isbn = 0-486-65812-0|postscript = &lt;!--None--&gt;}}.
* {{Cite book | last1=Rudin | first1=Walter | author1-link=Walter Rudin | title=Principles of Mathematical Analysis | publisher=[[McGraw-Hill]]| location=New York | isbn=0-07-054235-X | year=1976 | postscript=&lt;!--None--&gt;}}.

==External links==
{{ProofWiki|id=Triangle_Inequality|title=Triangle inequality}}

{{DEFAULTSORT:Triangle Inequality}}
[[Category:Geometric inequalities]]
[[Category:Linear algebra]]
[[Category:Metric geometry]]
[[Category:Articles containing proofs]]
[[Category:Theorems in geometry]]</text>
      <sha1>tlttkrbgd61fx0mibubfy7sawocniz1</sha1>
    </revision>
  </page>
  <page>
    <title>Vantieghems theorem</title>
    <ns>0</ns>
    <id>13048500</id>
    <revision>
      <id>846806665</id>
      <parentid>787011121</parentid>
      <timestamp>2018-06-21T00:55:03Z</timestamp>
      <contributor>
        <username>Bibcode Bot</username>
        <id>14394459</id>
      </contributor>
      <minor/>
      <comment>Adding 0 [[arXiv|arxiv eprint(s)]], 1 [[bibcode|bibcode(s)]] and 0 [[digital object identifier|doi(s)]]. Did it miss something? Report bugs, errors, and suggestions at [[User talk:Bibcode Bot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1575">In [[number theory]], '''Vantieghems theorem''' is a [[primality]] criterion. It states that a [[natural number]] ''n'' is [[prime number|prime]] if and only if
:&lt;math&gt;  \prod_{1 \leq k \leq n-1} \left( 2^k - 1  \right) \equiv n \mod \left(  2^n - 1 \right). &lt;/math&gt;

Similarly, ''n'' is prime, if and only if the following [[Modular arithmetic|congruence]] for [[polynomial]]s in ''X'' holds:
:&lt;math&gt;  \prod_{1 \leq k \leq n-1} \left( X^k - 1  \right) \equiv  n- \left( X^n - 1 \right)/\left( X - 1 \right) \mod \left(  X^n - 1 \right) &lt;/math&gt;

or:
:&lt;math&gt;  \prod_{1 \leq k \leq n-1} \left( X^k - 1  \right) \equiv n \mod \left( X^n - 1 \right)/\left( X - 1 \right). &lt;/math&gt;

== Example ==
Let n=7 forming the product 1*3*7*15*31*63 = 615195.  615195 = 7 mod 127 and so 7 is prime&lt;br&gt;
Let n=9 forming the product 1*3*7*15*31*63*127*255 = 19923090075.  19923090075 = 301 mod 511  and so 9 is composite

==References==
* {{cite journal | last=Kilford | first=L.J.P. | title=A generalization of a necessary and sufficient condition for primality due to Vantieghem | zbl=1126.11307 | journal=Int. J. Math. Math. Sci. | number=69-72 | pages=3889–3892 | year=2004 | arxiv=math/0402128 | bibcode=2004math......2128K }}. An article with proof and generalizations.
* {{cite journal | last=Vantieghem | first=E.| title=On a congruence only holding for primes | zbl=0734.11003 | journal=Indag. Math., New Ser.  | volume=2 | number=2 | pages=253–255 | year=1991 }}

[[Category:Factorial and binomial topics]]
[[Category:Modular arithmetic]]
[[Category:Theorems about prime numbers]]</text>
      <sha1>gq0473m2hiz31vbuziazi4zzmopdudb</sha1>
    </revision>
  </page>
  <page>
    <title>Zemor's decoding algorithm</title>
    <ns>0</ns>
    <id>41918550</id>
    <revision>
      <id>863703444</id>
      <parentid>746178421</parentid>
      <timestamp>2018-10-12T13:24:31Z</timestamp>
      <contributor>
        <username>WereSpielChequers</username>
        <id>4071608</id>
      </contributor>
      <minor/>
      <comment>/* Proof */[[WP:AWB/T|Typo fixing]], replaced: ,i → , i</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="16270">In [[coding theory]], '''Zemor's algorithm''', designed and developed by Gilles Zemor,&lt;ref name="vg1"&gt;[http://www.math.u-bordeaux1.fr/~zemor/ Gilles Zemor]&lt;/ref&gt; is a recursive low-complexity approach to code construction.  It is an improvement over the algorithm of [[Michael Sipser|Sipser]] and [[Daniel Spielman|Spielman]].

Zemor considered a typical class of Sipser–Spielman construction of [[expander code]]s, where the underlying graph is [[bipartite graph]]. Sipser and Spielman introduced a constructive family of asymptotically good linear-error codes together with a simple parallel algorithm that will always remove a constant fraction of errors.  The article is based on Dr. Venkatesan Guruswami's course notes &lt;ref name="vg2"&gt;http://www.cs.washington.edu/education/courses/cse590vg/03wi/scribes/1-27.ps&lt;/ref&gt;

== Code construction ==

Zemor's algorithm is based on a type of [[expander graphs]] called [[Tanner graph]]. The construction of code was first proposed by Tanner.&lt;ref name="vg3"&gt;http://www.cs.washington.edu/education/courses/cse533/06au/lecnotes/lecture14.pdf&lt;/ref&gt; The codes are based on [[double cover (topology)|double cover]] &lt;math&gt;d&lt;/math&gt;, regular expander &lt;math&gt;G&lt;/math&gt;, which is a bipartite graph. &lt;math&gt;G&lt;/math&gt; =&lt;math&gt; \left(V,E\right)&lt;/math&gt;, where &lt;math&gt;V&lt;/math&gt; is the set of vertices and &lt;math&gt;E&lt;/math&gt; is the set of edges and &lt;math&gt;V&lt;/math&gt; = &lt;math&gt;A&lt;/math&gt; &lt;math&gt;\cup&lt;/math&gt; &lt;math&gt;B&lt;/math&gt; and &lt;math&gt;A&lt;/math&gt; &lt;math&gt;\cap&lt;/math&gt; &lt;math&gt;B&lt;/math&gt; = &lt;math&gt;\emptyset&lt;/math&gt;, where &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; denotes the set of 2 vertices. Let &lt;math&gt;n&lt;/math&gt; be the number of vertices in each group, ''i.e'', &lt;math&gt;|A| =|B| =n&lt;/math&gt;. The edge set &lt;math&gt;E&lt;/math&gt; be of size &lt;math&gt;N&lt;/math&gt; =&lt;math&gt;nd&lt;/math&gt; and every edge in &lt;math&gt;E&lt;/math&gt; has one endpoint in both &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt;. &lt;math&gt;E(v)&lt;/math&gt; denotes the set of edges containing &lt;math&gt;v&lt;/math&gt;.

Assume an ordering on &lt;math&gt;V&lt;/math&gt;, therefore ordering will be done on every edges of &lt;math&gt;E(v)&lt;/math&gt; for every &lt;math&gt;v \in V&lt;/math&gt;. Let [[finite field]] &lt;math&gt;\mathbb{F}=GF(2)&lt;/math&gt;, and for a word  &lt;math&gt;x=(x_e), e\in E&lt;/math&gt; in &lt;math&gt;\mathbb{F}^N&lt;/math&gt;, let the subword of the  word will be indexed by &lt;math&gt;E(v)&lt;/math&gt;. Let that word be denoted by &lt;math&gt;(x)_v&lt;/math&gt;. The subset of vertices &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; induces every word &lt;math&gt;x\in \mathbb{F}^N&lt;/math&gt; a partition into &lt;math&gt;n&lt;/math&gt; non-overlapping sub-words &lt;math&gt; \left(x\right)_v\in \mathbb{F}^d&lt;/math&gt;, where &lt;math&gt;v&lt;/math&gt; ranges over the elements of &lt;math&gt;A&lt;/math&gt;. 
For constructing a code &lt;math&gt;C&lt;/math&gt;, consider a linear subcode &lt;math&gt;C_o&lt;/math&gt;, which is a &lt;math&gt;[d,r_od,\delta] &lt;/math&gt; code, where &lt;math&gt;q&lt;/math&gt;, the size of the alphabet is &lt;math&gt;2&lt;/math&gt;. For any vertex &lt;math&gt;v \in V&lt;/math&gt;, let &lt;math&gt; v(1), v(2),\ldots,v(d)&lt;/math&gt; be some ordering of the &lt;math&gt;d&lt;/math&gt; vertices of &lt;math&gt;E&lt;/math&gt; adjacent to &lt;math&gt;v&lt;/math&gt;. In this code, each bit &lt;math&gt;x_e&lt;/math&gt; is linked with an edge &lt;math&gt;e&lt;/math&gt; of &lt;math&gt;E&lt;/math&gt;. 

We can define the code &lt;math&gt;C&lt;/math&gt; to be the set of binary vectors &lt;math&gt;x = \left( x_1,x_2,\ldots,x_N \right)&lt;/math&gt; of &lt;math&gt;\{0,1\}^N &lt;/math&gt; such that, for every vertex &lt;math&gt;v&lt;/math&gt; of &lt;math&gt;V&lt;/math&gt;, &lt;math&gt; \left(x_{v(1)}, x_{v(2)},\ldots, x_{v(d)}\right) &lt;/math&gt; is a code word of &lt;math&gt;C_o&lt;/math&gt;. In this case, we can consider a special case when every vertex of &lt;math&gt;E&lt;/math&gt; is adjacent to exactly &lt;math&gt;2&lt;/math&gt; vertices of &lt;math&gt;V&lt;/math&gt;. It means that &lt;math&gt;V&lt;/math&gt; and &lt;math&gt;E&lt;/math&gt; make up, respectively, the vertex set and edge set of &lt;math&gt;d&lt;/math&gt; regular graph &lt;math&gt;G&lt;/math&gt;.

Let us call the code &lt;math&gt;C&lt;/math&gt; constructed in this way as &lt;math&gt;\left(G,C_o\right) &lt;/math&gt; code. For a given graph &lt;math&gt;G&lt;/math&gt; and a given code &lt;math&gt;C_o&lt;/math&gt;, there are several &lt;math&gt;\left(G,C_o\right) &lt;/math&gt; codes as there are different ways of ordering edges incident to a given vertex &lt;math&gt;v&lt;/math&gt;, i.e., &lt;math&gt; v(1), v(2),\ldots,v(d) &lt;/math&gt;. In fact our code &lt;math&gt;C&lt;/math&gt; consist of all codewords such that &lt;math&gt;x_v\in C_o&lt;/math&gt; for all &lt;math&gt;v \in A, B&lt;/math&gt;. The code &lt;math&gt; C&lt;/math&gt; is linear &lt;math&gt;[N,K,D] &lt;/math&gt; in &lt;math&gt;\mathbb{F}&lt;/math&gt; as it is generated from a subcode &lt;math&gt;C_o&lt;/math&gt;, which is linear. The code &lt;math&gt;C&lt;/math&gt; is defined as &lt;math&gt;C=\{c\in \mathbb{F}^N :(c)_v \in C_o\}&lt;/math&gt; for every &lt;math&gt;v\in V&lt;/math&gt;.

[[File:Zemor Decoding.jpg|thumb|upright=2.0|alt=A |Graph G and code C]]

In this figure, &lt;math&gt;(x)_v =\left(x_{e1}, x_{e2}, x_{e3}, x_{e4}\right)\in C_o&lt;/math&gt;. It shows the graph &lt;math&gt;G&lt;/math&gt; and code &lt;math&gt;C&lt;/math&gt;.

In matrix &lt;math&gt;G&lt;/math&gt;, let &lt;math&gt;\lambda&lt;/math&gt; is equal to the second largest [[eigen value]] of [[adjacency matrix]] of &lt;math&gt;G&lt;/math&gt;. Here the largest eigen value is &lt;math&gt; d&lt;/math&gt;. 
Two important claims are made:

=== Claim 1 ===
'''&lt;math&gt;\left(\dfrac{K}{N}\right)\geq 2r_o-1&lt;/math&gt;'''&lt;br&gt;
''. Let &lt;math&gt;R&lt;/math&gt; be the rate of a linear code constructed from a bipartite graph whose digit nodes have degree &lt;math&gt;m&lt;/math&gt; and whose subcode nodes have degree &lt;math&gt;n&lt;/math&gt;. If a single linear code with parameters &lt;math&gt;\left(n,k\right)&lt;/math&gt; and rate &lt;math&gt;r = \left(\dfrac{k}{n}\right)&lt;/math&gt; is associated with each of the subcode nodes, then &lt;math&gt;k\geq 1-\left(1-r\right)m&lt;/math&gt;''.

==== Proof ====
Let &lt;math&gt;R&lt;/math&gt; be the rate of the linear code, which is equal to &lt;math&gt;K/N&lt;/math&gt;
Let there are &lt;math&gt;S&lt;/math&gt; subcode nodes in the graph. If the degree of the subcode is &lt;math&gt;n&lt;/math&gt;, then the code must have &lt;math&gt;\left(\dfrac{n}{m}\right) S&lt;/math&gt; digits, as each digit node is connected to &lt;math&gt;m&lt;/math&gt; of the &lt;math&gt;\left(n\right)S&lt;/math&gt; edges in the graph. Each subcode node contributes &lt;math&gt;(n-k)&lt;/math&gt; equations to parity check matrix for a total of &lt;math&gt;\left(n-k\right) S&lt;/math&gt;. These equations may not be linearly independent. 
Therefore, &lt;math&gt;\left(\dfrac{K}{N}\right)\geq \left(\dfrac{(\dfrac{n}{m})S - (n-k)S}{(\dfrac{n}{m}) S}\right)&lt;/math&gt;&lt;br&gt;
&lt;math&gt;\geq 1-m\left(\dfrac{n-k}{n}\right)&lt;/math&gt;&lt;br&gt;
&lt;math&gt;\geq 1-m \left(1-r\right) &lt;/math&gt;, Since the value of &lt;math&gt;m&lt;/math&gt;, i.e., the digit node of this bipartite graph is &lt;math&gt;2&lt;/math&gt; and here &lt;math&gt;r = r_o&lt;/math&gt;, we can write as:&lt;br&gt;
&lt;math&gt;\left(\dfrac{K}{N}\right)\geq 2r_o -1&lt;/math&gt;

=== Claim 2 ===

: &lt;math&gt;D\geq N\left(\dfrac{(\delta-(\dfrac{\lambda}{d}))}{(1-(\dfrac{\lambda}{d})})\right)^2&lt;/math&gt;
: &lt;math&gt;=N\left(\delta^2- O\left(\dfrac{\lambda}{d}\right)\right)&lt;/math&gt; &lt;math&gt;\rightarrow (1)&lt;/math&gt;

''If &lt;math&gt; S &lt;/math&gt; is linear code of rate &lt;math&gt;r&lt;/math&gt;, block code length &lt;math&gt; d&lt;/math&gt;, and minimum relative distance &lt;math&gt;\delta&lt;/math&gt;, and if &lt;math&gt;B&lt;/math&gt; is the edge vertex incidence graph of a &lt;math&gt;d&lt;/math&gt; – regular graph with second largest eigen value &lt;math&gt;\lambda&lt;/math&gt;, then the code &lt;math&gt;C(B,S)&lt;/math&gt; has rate at least &lt;math&gt;2r_o -1&lt;/math&gt; and minimum relative distance at least &lt;math&gt;\left(\left(\dfrac{\delta- \left(\dfrac{\lambda}{d}\right)}{1-\left(\dfrac{\lambda}{d}\right)}\right)\right)^2&lt;/math&gt;.

==== Proof ====
Let &lt;math&gt; B&lt;/math&gt; be derived from the &lt;math&gt;d&lt;/math&gt; regular graph &lt;math&gt;G&lt;/math&gt;. So, the number of variables of &lt;math&gt;C(B,S)&lt;/math&gt; is &lt;math&gt; \left(\dfrac{dn}{2}\right)&lt;/math&gt; and the number of constraints is &lt;math&gt;n&lt;/math&gt;. According to Alon - Chung,&lt;ref name="vg4"&gt;http://math.ucsd.edu/~fan/mypaps/fanpap/93tolerant.pdf&lt;/ref&gt; if &lt;math&gt;X&lt;/math&gt; is a subset of vertices of &lt;math&gt;G&lt;/math&gt; of size &lt;math&gt;\gamma n&lt;/math&gt;, then the number of edges contained in the subgraph is induced by &lt;math&gt;X&lt;/math&gt; in &lt;math&gt;G&lt;/math&gt; is at most &lt;math&gt;\left(\dfrac{dn}{2}\right) \left(\gamma^2 + (\dfrac{\lambda}{d})\gamma \left(1-\gamma\right)\right)&lt;/math&gt;. 

As a result, any set of &lt;math&gt;\left(\dfrac{dn}{2}\right) \left(\gamma^2 + \left(\dfrac{\lambda}{d}\right)\gamma \left(1-\gamma\right)\right)&lt;/math&gt; variables will be having at least &lt;math&gt;\gamma n&lt;/math&gt; constraints as neighbours. So the average number of variables per constraint is : &lt;math&gt;\left(\dfrac{(\dfrac{2nd}{2}) \left(\gamma^2 + (\dfrac{\lambda}{d})\gamma \left(1-\gamma\right)\right)}{\gamma n}\right)&lt;/math&gt;
&lt;math&gt;= d\left( \gamma + (\dfrac{\lambda}{d}) \left( 1-\gamma\right)\right)&lt;/math&gt;           &lt;math&gt;\rightarrow (2)&lt;/math&gt;

So if &lt;math&gt; d\left( \gamma + (\dfrac{\lambda}{d}) \left( 1-\gamma\right)\right) &lt; \gamma d&lt;/math&gt;, then a word of relative weight  &lt;math&gt;  \left(\gamma^2 + (\dfrac{\lambda}{d})\gamma \left(1-\gamma\right)\right)&lt;/math&gt;, cannot be a codeword of &lt;math&gt;C(B,S)&lt;/math&gt;. The inequality &lt;math&gt;(2)&lt;/math&gt; is satisfied for &lt;math&gt;\gamma &lt; \left(\dfrac{1-(\dfrac{\lambda}{d})}{\delta-(\dfrac{\lambda}{d})}\right)&lt;/math&gt;. Therefore, &lt;math&gt;C(B,S)&lt;/math&gt; cannot have a non zero codeword of relative weight   &lt;math&gt;\left(\dfrac{\delta-(\dfrac{\lambda}{d})}{1-(\dfrac{\lambda}{d})}\right)^2&lt;/math&gt; or less.

In matrix &lt;math&gt;G&lt;/math&gt;, we can assume that &lt;math&gt;\lambda/d&lt;/math&gt; is bounded away from &lt;math&gt;1&lt;/math&gt;. For those values of &lt;math&gt; d&lt;/math&gt; in which &lt;math&gt;d-1&lt;/math&gt; is odd prime, there are explicit constructions of sequences of &lt;math&gt; d&lt;/math&gt; - regular bipartite graphs with arbitrarily large number of vertices such that each graph &lt;math&gt;G&lt;/math&gt; in the sequence is a  [[Ramanujan graph]]. It is called Ramanujan graph as it satisfies the inequality &lt;math&gt;\lambda(G) \leq 2\sqrt{d-1}&lt;/math&gt;. Certain expansion properties are visible in graph &lt;math&gt;G&lt;/math&gt; as the separation between the eigen values &lt;math&gt;d&lt;/math&gt; and &lt;math&gt; \lambda &lt;/math&gt;. If the graph &lt;math&gt; G &lt;/math&gt; is Ramanujan graph, then that expression  &lt;math&gt;(1)&lt;/math&gt; will become &lt;math&gt;0&lt;/math&gt; eventually as &lt;math&gt; d &lt;/math&gt; becomes large.

== Zemor's algorithm ==

The iterative decoding algorithm written below alternates between the vertices &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; in &lt;math&gt;G&lt;/math&gt; and corrects the codeword of &lt;math&gt;C_o&lt;/math&gt; in &lt;math&gt;A&lt;/math&gt; and then it switches to correct the codeword &lt;math&gt;C_o&lt;/math&gt; in &lt;math&gt;B&lt;/math&gt;. Here edges associated with a vertex on one side of a graph are not incident to other vertex on that side. In fact, it doesn't matter in which order, the set of nodes &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; are processed. The vertex processing can also be done in parallel. 

The decoder &lt;math&gt;\mathbb{D}:\mathbb{F}^d \rightarrow C_o&lt;/math&gt;stands for a decoder for &lt;math&gt;C_o&lt;/math&gt; that recovers correctly with any codewords with less than &lt;math&gt;\left(\dfrac{d}{2}\right)&lt;/math&gt; errors.

=== Decoder algorithm ===

Received word : &lt;math&gt;w=(w_e), e\in E&lt;/math&gt;&lt;br&gt; 
&lt;code&gt;
&lt;math&gt;z \leftarrow w&lt;/math&gt;&lt;br&gt;
For  &lt;math&gt;t \leftarrow 1&lt;/math&gt; to &lt;math&gt;m&lt;/math&gt; do         //&lt;math&gt;m&lt;/math&gt; is the number of iterations&lt;br&gt;
{ if (&lt;math&gt;t&lt;/math&gt; is odd)                                       // Here the algorithm will alternate between its two vertex sets.&lt;br&gt;
&lt;math&gt;X \leftarrow A&lt;/math&gt;&lt;br&gt;
else  &lt;math&gt;X \leftarrow B&lt;/math&gt;  &lt;br&gt;  
Iteration &lt;math&gt;t&lt;/math&gt;: For every &lt;math&gt;v \in X&lt;/math&gt;, let &lt;math&gt;(z)_v \leftarrow \mathbb{D}((z)_v)&lt;/math&gt; // Decoding &lt;math&gt;z_v&lt;/math&gt; to its nearest codeword.&lt;br&gt;
}&lt;br&gt;
&lt;/code&gt;
Output: &lt;math&gt;z&lt;/math&gt;

=== Explanation of the algorithm ===

Since &lt;math&gt;G&lt;/math&gt; is bipartite, the set &lt;math&gt;A&lt;/math&gt; of vertices induces the partition of the edge set &lt;math&gt;E&lt;/math&gt; = &lt;math&gt;\cup_{v\in A}  E_v&lt;/math&gt; . The set &lt;math&gt;B&lt;/math&gt; induces another partition, &lt;math&gt;E&lt;/math&gt; = &lt;math&gt;\cup_{v\in B}  E_v&lt;/math&gt; .

Let &lt;math&gt;w \in \{0,1\}^N&lt;/math&gt; be the received vector, and recall that &lt;math&gt;N=dn&lt;/math&gt;. The first iteration of the algorithm consists of applying the complete decoding for the code induced by &lt;math&gt;E_v&lt;/math&gt; for every &lt;math&gt;v \in A&lt;/math&gt; . This means that for replacing, for every &lt;math&gt;v \in A&lt;/math&gt;, the vector &lt;math&gt;\left( w_{v(1)}, w_{v(2)}, \ldots, w_{v(d)}\right)&lt;/math&gt; by one of the closest codewords of &lt;math&gt;C_o&lt;/math&gt;. Since the subsets of edges &lt;math&gt;E_v&lt;/math&gt; are disjoint for &lt;math&gt;v \in A&lt;/math&gt;, the decoding of these &lt;math&gt;n&lt;/math&gt;  subvectors of &lt;math&gt;w&lt;/math&gt; may be done in parallel.

The iteration will yield a new vector &lt;math&gt;z&lt;/math&gt;. The next iteration consists of applying the preceding procedure to &lt;math&gt;z&lt;/math&gt; but with &lt;math&gt;A&lt;/math&gt; replaced by &lt;math&gt;B&lt;/math&gt;. In other words, it consists of decoding all the subvectors induced by the vertices of &lt;math&gt;B&lt;/math&gt;. The coming iterations repeat those two steps alternately applying parallel decoding to the subvectors induced by the vertices of &lt;math&gt;A&lt;/math&gt; and to the subvectors induced by the vertices of &lt;math&gt;B&lt;/math&gt;. &lt;br&gt;
'''Note:''' [If &lt;math&gt;d=n&lt;/math&gt; and &lt;math&gt;G&lt;/math&gt; is the complete bipartite graph, then &lt;math&gt;C&lt;/math&gt; is a product code of &lt;math&gt;C_o&lt;/math&gt; with itself and the above algorithm reduces to the natural hard iterative decoding of product codes].

Here, the number of iterations, &lt;math&gt;m&lt;/math&gt; is &lt;math&gt;\left(\dfrac{(\log{n})}{\log(2-\alpha)}\right)&lt;/math&gt;. 
In general, the above algorithm can correct a code word whose Hamming weight is no more than &lt;math&gt;(\dfrac{1}{2}).\alpha N \delta \left((\dfrac{\delta}{2})-(\dfrac{\lambda}{d})\right) =\left((\dfrac{1}{4}).\alpha N (\delta^2- O(\dfrac{\lambda}{d})\right)&lt;/math&gt; for values of &lt;math&gt;\alpha &lt; 1&lt;/math&gt;. Here, the decoding algorithm is implemented as a circuit of size &lt;math&gt;O(N \log{N} )&lt;/math&gt; and depth &lt;math&gt;O(\log{N})&lt;/math&gt; that returns the codeword given that error vector has weight less than &lt;math&gt;\alpha N \delta^2 (1-\epsilon)/4&lt;/math&gt; .

=== Theorem ===

''If &lt;math&gt;G&lt;/math&gt; is a Ramanujan graph of sufficiently high degree, for any &lt;math&gt;\alpha &lt; 1&lt;/math&gt;, the decoding algorithm can correct &lt;math&gt;(\dfrac{\alpha \delta_o^2}{4})(1-\in) N &lt;/math&gt; errors, in &lt;math&gt; O(\log {n}) &lt;/math&gt; rounds ( where the big- &lt;math&gt;O&lt;/math&gt; notation hides a dependence on &lt;math&gt;\alpha&lt;/math&gt;). This can be implemented in linear time on a single processor; on &lt;math&gt;n&lt;/math&gt; processors each round can be implemented in constant time.''

==== Proof ====

Since the  decoding algorithm is insensitive to the value of the edges and by linearity, we can assume that the transmitted codeword is the all zeros - vector. Let the received codeword be &lt;math&gt;w&lt;/math&gt;. The set of edges which has an incorrect value while decoding is considered. Here by incorrect value, we mean &lt;math&gt;1&lt;/math&gt; in any of the bits. Let &lt;math&gt;w=w^0&lt;/math&gt; be the initial value of the codeword, &lt;math&gt;w^1, w^2,\ldots, w^t&lt;/math&gt; be the values after first, second&amp;nbsp;.&amp;nbsp;.&amp;nbsp;. &lt;math&gt;t&lt;/math&gt; stages of decoding. 
Here, &lt;math&gt;X^i={e\in E|x_e^i =1}&lt;/math&gt;, and  &lt;math&gt;S^i ={v\in V^i | E_v \cap X^{i+1} !=\emptyset}&lt;/math&gt;.  Here &lt;math&gt;S^i&lt;/math&gt; corresponds to those set of vertices that was not able to successfully decode their codeword in the &lt;math&gt;i^{th}&lt;/math&gt; round. From the above algorithm &lt;math&gt;S^1 &lt;S^0 &lt;/math&gt;  as number of unsuccessful vertices will be corrected in every iteration. We can prove that &lt;math&gt;S^0&gt;S^1&gt;S^2&gt;\cdots&lt;/math&gt;is a decreasing sequence.
In fact, &lt;math&gt;|S_{i+1}|&lt;=(\dfrac{1}{2-\alpha})|S_i|&lt;/math&gt;. As we are assuming, &lt;math&gt;\alpha&lt;1&lt;/math&gt;, the above equation is in a [[geometric series|geometric decreasing sequence]]. 
So, when &lt;math&gt;|S_i|&lt;n&lt;/math&gt;, more than &lt;math&gt;log_{2-\alpha} n &lt;/math&gt; rounds are necessary. Furthermore, &lt;math&gt;\sum|S_i|=n\sum(\dfrac{1}{(2-\alpha)^i})=O(n)&lt;/math&gt;, and if we implement the &lt;math&gt;i^{th}&lt;/math&gt; round in &lt;math&gt;O(|S_i|)&lt;/math&gt; time, then the total sequential running time will be linear.

== Drawbacks of Zemor's algorithm ==
# It is lengthy process as the number of iterations &lt;math&gt;m&lt;/math&gt; in decoder algorithm takes is &lt;math&gt;[(\log{ n})/(\log(2-\alpha))]&lt;/math&gt;
# Zemor's decoding algorithm finds it difficult to decode erasures. A detailed way of how we can improve the algorithm is
given in.&lt;ref name="vg5"&gt;{{cite web|url=http://www.cs.technion.ac.il/~vitalys/Papers/GMD-expander/GMD-decode-expander.ps |title=Archived copy |accessdate=May 1, 2012 |deadurl=yes |archiveurl=https://web.archive.org/web/20040914064028/http://www.cs.technion.ac.il/~vitalys/Papers/GMD-expander/GMD-decode-expander.ps |archivedate=September 14, 2004 }}&lt;/ref&gt;

==See also==

*[[Expander code]]s
*[[Tanner graph]]
*[[Linear time encoding and decoding of error-correcting codes]]

==References==
{{reflist}}

[[Category:Coding theory]]
[[Category:Error detection and correction]]</text>
      <sha1>rk9mr2wuchaol82a1g1vakul02593hz</sha1>
    </revision>
  </page>
</mediawiki>
