<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>186 (number)</title>
    <ns>0</ns>
    <id>3334535</id>
    <revision>
      <id>870240704</id>
      <parentid>870183090</parentid>
      <timestamp>2018-11-23T12:39:55Z</timestamp>
      <contributor>
        <username>Arthur Rubin</username>
        <id>374195</id>
      </contributor>
      <comment>Reverted [[WP:AGF|good faith]] edits by [[Special:Contributions/208.98.222.39|208.98.222.39]] ([[User talk:208.98.222.39|talk]]): No Wikipedia article, indication of importance, or correct grammar. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4040">{{Example farm|date=April 2010}}
{{Infobox number
| number = 186
}}
'''186''' ('''one hundred [and] eighty-six''') is the [[natural number]] following [[185 (number)|185]] and preceding [[187 (number)|187]].

==In mathematics==
* 186 is an [[Parity (mathematics)|even number]]
* 186 is an [[abundant number]], as [[198 (number)|198]] is greater than 186
* 186 is a [[composite number]]
* There is no integer with exactly 186 [[coprime]]s less than it. 186 is a [[nontotient]], nor is it ever the difference between an integer and the total of coprimes below it, thus also a [[noncototient]]
* 186 is an [[Thue–Morse sequence|odious number]]
* 186 is a [[sphenic number]], so the [[Möbius function]] returns -1 (and the [[Mertens function]] returns -4)
* 186 is a [[square-free]] number
* 186 is a 14-[[gonal number]]&lt;ref&gt;{{Cite OEIS|1=A051866|2=14-gonal numbers|accessdate=2016-05-28}}&lt;/ref&gt; and a 63-gonal number&lt;ref&gt;{{Cite OEIS|1=A098140|2=63-gonal numbers|accessdate=2016-05-28}}&lt;/ref&gt;
* 186 factors: [[2 (number)|2]] × [[3 (number)|3]] × [[31 (number)|31]]
* 186 divisors: [[1 (number)|1]], [[2 (number)|2]], [[3 (number)|3]], [[6 (number)|6]], [[31 (number)|31]], [[62 (number)|62]], [[93 (number)|93]], 186
* There are 186 degree-11 [[irreducible polynomial]]s over a [[Galois field]] with 2 elements {{OEIS|id=A001037}}

==In astronomy==
* [[186 Celuta]] is a large [[S-type asteroid]] [[Asteroid belt|Main belt]] [[asteroid]]
* [[POX 186]] is a small [[galaxy]] still forming

==In geography==
* [[Abernethy No. 186, Saskatchewan]] is a [[rural municipality]] in [[Saskatchewan]], [[Canada]]

==In the military==
* {{ship|RFA|Fort Rosalie|A186}} was a [[Royal Fleet Auxiliary]] [[armament]] stores carrier following [[World War II]]
* {{USNS|Bessemer|T-AG-186}} was a [[United States Navy]] [[Victory ship]] during World War II
* {{USS|Clemson|DD-186}} was a United States Navy [[destroyer]] during World War II
* {{USS|Dawn|IX-186}} was a United States Navy [[Tank ship|tanker ship]] during World War II
* {{USS|Habersham|AK-186}} was a United States Navy ''Alamosa''-class [[cargo ship]] during World War II
* {{USS|Platte|AO-186}} was a United States Navy ''Cimarron''-class ship from 1981–1999
* {{USS|Stingray|SS-186}} was a United States Navy {{sclass-|Salmon|submarine|1}} during World War II
* {{USS|Swearer|DE-186}} was a United States Navy {{sclass-|Cannon|destroyer escort|1}} during World War II

==In sports==
* The [[Light heavyweight (MMA)|light heavyweight]] division in [[mixed martial arts]] refers to competitors weighing between 186 and [[205 (number)|205]] [[pound (mass)|pounds]]


==In transportation==
* The 186 is a legendary 6-cylinder [[Holden]] engine (L6), that was first used in the [[Holden HR]] series. It has a piston displacement of 186 cubic inches (3048cc).
* [[London Buses route 186]]
* [[Cosmos 186 and Cosmos 188]] incorporated a [[Soyuz programme]] descent module for landing scientific instruments and test objects

==In other fields==
'''186''' is also:
* The year [[186|AD 186]] or [[186 BC]]
* The [[atomic number]] of an element temporarily called [http://www.flw.com/datatools/periodic/001.php?id=186 Unocthexium]
* [[Minuscule 186]] is a [[Greek language|Greek]] [[Lower case|minuscule]] [[manuscript]] of the [[New Testament]], on [[parchment]]
==See also==
* [[List of highways numbered 186]]
* [[United Nations Security Council Resolution 186]]
* [[Pennsylvania House of Representatives, District 186]]
* [[List of United States Supreme Court cases, volume 186|United States Supreme Court cases, Volume 186]]

==References==
{{reflist}}

==External links==
{{Commons category|186 (number)}}
* [http://athensohio.net/reference/number/186/ Number Facts and Trivia: 186]
* [http://www.numdic.com/186 The Number 186]
* [http://www.positiveintegers.org/186 The Positive Integer 186]
* [http://primes.utm.edu/curios/page.php/186.html Prime curiosities: 186]
* [http://www.numbergossip.com/186 Number Gossip: 186]

{{Integers|1}}

{{DEFAULTSORT:186 (Number)}}
[[Category:Integers]]</text>
      <sha1>26b4qvldyjqzum5ry26ctp2yeez91dj</sha1>
    </revision>
  </page>
  <page>
    <title>A Course of Modern Analysis</title>
    <ns>0</ns>
    <id>2139612</id>
    <revision>
      <id>817551472</id>
      <parentid>794309600</parentid>
      <timestamp>2017-12-29T03:27:54Z</timestamp>
      <contributor>
        <username>Just a guy from the KP</username>
        <id>9994896</id>
      </contributor>
      <minor/>
      <comment>/* top */ Typos</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5689">{{Infobox book
| italic title   = &lt;!--(see above)--&gt;
| name           = A Course of Modern Analysis
| image          = 
| image_size     = 
| alt            = 
| caption        = 
| author         = [[E. T. Whittaker]] and [[G. N. Watson]]
| audio_read_by  = 
| title_orig     = 
| orig_lang_code = 
| title_working  = 
| translator     = 
| illustrator    = 
| cover_artist   = 
| country        = 
| language       = English
| series         = 
| release_number = 
| subject        = [[Mathematics]]
| genre          = 
| set_in         = 
| published      = 
| publisher      = [[Cambridge University Press]]
| publisher2     = 
| pub_date       = 1902
| english_pub_date = 
| media_type     = 
| pages          = 
| awards         = 
| isbn           = 
| isbn_note      = 
| oclc           = 
| dewey          = 
| congress       = 
| preceded_by    = 
| followed_by    = 
| native_wikisource = 
| wikisource     = 
| notes          =
| exclude_cover  = 
| website        =
}}
'''''A Course of Modern Analysis''': an introduction to the general theory of infinite processes and of analytic functions; with an account of the principal transcendental functions'' (colloquially known as ''Whittaker and Watson'') is a landmark textbook on [[mathematical analysis]] written by [[E. T. Whittaker]] and [[G. N. Watson]], first published by [[Cambridge University Press]] in 1902.&lt;ref&gt;{{cite journal|author=Bôcher, Maxime|authorlink=Maxime Bôcher|title=Review: ''A Course of Modern Analysis'', by E. T. Whittaker|journal=Bull. Amer. Math. Soc.|year=1904|volume=10|issue=7|pages=351–354|url=http://www.ams.org/journals/bull/1904-10-07/S0002-9904-1904-01123-4/|doi=10.1090/s0002-9904-1904-01123-4}}&lt;/ref&gt; (The first edition was Whittaker's alone; it was in later editions with Watson that this book is best known.)

Its first, second, third, and the fourth, last edition were published in 1902, 1915, 1920, and 1927, respectively. Since then, it has continuously been reprinted and still in print today.

The book is notable for being the standard reference and textbook for a generation of Cambridge mathematicians including [[John Edensor Littlewood|Littlewood]] and [[G. H. Hardy]]. [[Mary Cartwright]] studied it as preparation for her final honours on the advice of fellow student [[V.C. Morton]], later Professor of Mathematics at [[Aberystwyth University]].&lt;ref&gt;{{cite web|title=Dame Mary Lucy Cartwright|url=http://www-history.mcs.st-and.ac.uk/Biographies/Cartwright.html|website=www-history.mcs.st-and.ac.uk|publisher=[[St. Andrews University]]}}&lt;/ref&gt; But its reach was much further than just the Cambridge school; [[André Weil]] in his obituary of the French mathematician [[Jean Delsarte]] noted that Delsarte always had a copy on his desk.&lt;ref&gt;{{cite web|title=Jean Frédéric Auguste Delsarte|url=http://www-history.mcs.st-and.ac.uk/Biographies/Delsarte.html|website=www-history.mcs.st-and.ac.uk|publisher=[[St. Andrews University]]}}&lt;/ref&gt;

Today, the book retains much of its original appeal.&lt;ref name= CUP&gt;{{cite web|title=A Course of Modern Analysis|url=http://www.cambridge.org/us/academic/subjects/mathematics/real-and-complex-analysis/course-modern-analysis-4th-edition-1|website=cambridge.org|publisher=[[Cambridge University Press]]}}&lt;/ref&gt; Some idiosyncratic but interesting problems from the salad days of the [[Cambridge Mathematical Tripos]] are to be found in the exercises. It is terse, yet readable by the motivated student. It conforms to high standards of [[mathematical rigour]], while compressing much actual formulaic information also.&lt;ref name= CUP/&gt;

The book was one of the earliest to use [[Paragraph#Numbering|decimal numbering for its sections]], an innovation the authors attribute to [[Giuseppe Peano]].&lt;ref&gt;{{cite web|last1=Kowalski|first1=E.|title=Peano paragraphing|url=http://blogs.ethz.ch/kowalski/2008/06/03/peano-paragraphing/|website=blogs.ethz.ch}}&lt;/ref&gt;

== Contents ==
Below is the contens of the fourth edition:

;Part I. The Process of Analysis
{{ordered list|type=upper-roman
  | Complex Numbers
  | The Theory of Convergence
  | Continuous Functions and Uniform Convergence
  | The Theory of Riemann Integration
  | The fundamental properties of Analytic Functions; Taylor's, Laurent's, and Liouville's Theorems &lt;!-- "fundamental properties" is lowercase  --&gt;
  | The Theory of Residues; application to the evaluation of Definite Integrals
  | The expansion of functions in Infinite Series
  | Asymptotic Expansions and Summable Series
  | Linear Differential Equations
  | Integral Equations
}}

;Part II. The Transcendental Functions
{{ordered list|type=upper-roman|start=12
  | The Gamma Function
  | The Zeta Function of Riemann
  | The Hypergeometric Function
  | Legendre Functions
  | The Confluent Hypergeometric Function
  | Bessel Functions
  | The Equations of Mathematical Physics
  | Mathieu Functions
  | Elliptic Functions. General theorems and the Weierstrassian Functions
  | The Theta Functions
  | The Jacobian Elliptic Functions
  | Ellipsoidal Harmonics and Lamé's Equation
}}

==See also==
*[[Bateman Manuscript Project]]

==References==
{{reflist}}
*E. T. Whittaker and G. N. Watson. ''A Course of Modern Analysis''. Cambridge University Press; 4th edition (January 2, 1927). {{ISBN|0-521-09189-6}}

==External links==
* [http://www.cambridge.org/catalogue/catalogue.asp?isbn=9780521588072 ''A Course of Modern Analysis'' at Cambridge University Press] (4 e. 1927, reissued 1996)
*[https://books.google.com/books?id=_hoPAAAAIAAJ First edition (1902) at Google Books]

{{DEFAULTSORT:Course of Modern Analysis, A}}
[[Category:1902 books]]
[[Category:Mathematics textbooks]]
[[Category:Mathematical analysis]]</text>
      <sha1>hmq99x6dofvvphnfe08hvwa4gqcvm26</sha1>
    </revision>
  </page>
  <page>
    <title>Acylindrically hyperbolic group</title>
    <ns>0</ns>
    <id>55866076</id>
    <revision>
      <id>860247057</id>
      <parentid>825282198</parentid>
      <timestamp>2018-09-19T10:40:31Z</timestamp>
      <contributor>
        <username>Jean Raimbault</username>
        <id>27087074</id>
      </contributor>
      <comment>/* Acylindrical action */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8837">In the mathematical subject of [[geometric group theory]], an '''acylindrically hyperbolic group''' is a [[group (mathematics)|group]] admitting a non-elementary 'acylindrical' isometric [[group action|action]] on some geodesic [[hyperbolic metric space]].&lt;ref name=Osin&gt;D. Osin, [http://www.ams.org/journals/tran/2016-368-02/S0002-9947-2015-06343-0/ ''Acylindrically hyperbolic groups''.] [[Transactions of the American Mathematical Society]] '''368''' (2016), no. 2, pp. 851–888  &lt;/ref&gt; This notion generalizes the notions of a [[hyperbolic group]] and of a [[relatively hyperbolic group]] and includes a significantly wider class of examples, such as [[mapping class group]]s and [[Out(Fn)|Out(''F&lt;sub&gt;n&lt;/sub&gt;'')]].

==Formal definition==

===Acylindrical action===

Let ''G'' be a group with an isometric action on some geodesic [[hyperbolic metric space]] ''X''. This action is called '''acylindrical'''&lt;ref name=Osin/&gt; if for every &lt;math&gt;R\ge 0&lt;/math&gt; there exist &lt;math&gt; N&gt;0, L&gt;0&lt;/math&gt; such that for every &lt;math&gt;x,y\in X&lt;/math&gt; with &lt;math&gt;d(x,y)\ge L&lt;/math&gt; one has
:&lt;math&gt;\# \{g\in G| d(x,gx)\le R, d(y,gy)\le R)\} \le N.   &lt;/math&gt;

If the above property holds for a specific &lt;math&gt;R\ge 0&lt;/math&gt;, the action of ''G'' on ''X'' is called ''R''-'''acylindrical'''.  The notion of acylindricity provides a suitable substitute for being a [[proper action]] in the more general context where non-proper actions are allowed. 

An acylindrical isometric action of a group ''G'' on a geodesic [[hyperbolic metric space]] ''X'' is '''non-elementary''' if '''G''' admits two independent [[Hyperbolic_metric_space#The_action_of_isometries_on_the_boundary_and_their_classification|hyperbolic]] isometries of ''X'', that is, two loxodromic elements &lt;math&gt;g,h\in G&lt;/math&gt; such that their fixed point sets &lt;math&gt;\{g^+,g^-\}\subseteq \partial X&lt;/math&gt; and &lt;math&gt;\{h^+,h^-\}\subseteq \partial X&lt;/math&gt; are disjoint. 

It is known (Theorem 1.1 in &lt;ref name=Osin/&gt;) that an acylindrical action of a group ''G'' on a geodesic hyperbolic metric space ''X'' is non-elementary if and only if this action has unbounded orbits in ''X'' and the group ''G'' is not a finite extension of a cyclic group generated by loxodromic isometry of ''X''.

===Acylindrically hyperbolic group===

A group ''G'' is called '''acylindrically hyperbolic''' if ''G'' admits a non-elementary acylindrical isometric action on some geodesic hyperbolic metric space ''X''.


==Equivalent characterizations== 

It is known (Theorem 1.2 in &lt;ref name=Osin/&gt;) that for a group ''G'' the following conditions are equivalent:

*The group ''G'' is acylindrically hyperbolic.
*There exists a (possibly infinite) generating set ''S'' for ''G'', such that the [[Cayley graph]] &lt;math&gt;\Gamma(G,S)&lt;/math&gt; is hyperbolic, and the natural translation action of ''G'' on &lt;math&gt;\Gamma(G,S)&lt;/math&gt; is a non-elementary acylindrical action.
*The group ''G'' is not [[virtually cyclic group|virtually cyclic]], and there exists an isometric action of ''G'' on a geodesic hyperbolic metric space ''X'' such that at least one element of ''G'' acts on ''X'' with the WPD ('Weakly Properly Discontinuous') property.
*The group ''G'' contains a proper infinite 'hyperbolically embedded' [[subgroup]].&lt;ref name=DGO&gt; F. Dahmani, V. Guirardel, D. Osin, ''Hyperbolically embedded subgroups and rotating families in groups acting on hyperbolic spaces''. [[Memoirs of the American Mathematical Society]] '''245''' (2017), no. 1156,  {{ISBN|978-1-4704-2194-6}}&lt;/ref&gt;


==History==

==Properties==

*Every acylindrically hyperbolic group ''G'' is [[SQ-universal group|SQ-universal]], that is, every countable group embeds as a subgroup in some [[quotient group]] of ''G''. 
*The class of acylindrically hyperbolic groups is closed under taking infinite [[normal subgroup]]s, and, more generally, under taking 's-normal' subgroups.&lt;ref name=Osin/&gt; Here a subgroup &lt;math&gt;H\le G&lt;/math&gt; is called ''s-normal'' in &lt;math&gt;G&lt;/math&gt; if for every &lt;math&gt;g\in G&lt;/math&gt; one has &lt;math&gt;|H\cap g^{-1}Hg|=\infty&lt;/math&gt;. 
*If ''G'' is an acylindrically hyperbolic group and &lt;math&gt;V=\mathbb R&lt;/math&gt; or &lt;math&gt;V=\ell^p(G)&lt;/math&gt; with &lt;math&gt;p\in [1,\infty)&lt;/math&gt; then the bounded cohomology &lt;math&gt;H_b(G,V)&lt;/math&gt; is infinite-dimensional.&lt;ref&gt;M. Bestvina and K.Fujiwara, ''Bounded cohomology of subgroups of mapping class groups'', [[Geometry &amp; Topology]] '''6''' (2002), pp. 69–89 &lt;/ref&gt;&lt;ref&gt;U. Hamenstädt, ''Bounded cohomology and isometry groups of hyperbolic spaces'', [[Journal of the European Mathematical Society]] (JEMS) '''10''' (2008), no. 2, pp. 315–349&lt;/ref&gt;&lt;ref name=Osin/&gt;
*Every acylindrically hyperbolic group ''G'' admits a unique maximal normal finite subgroup denoted ''K(G)''.&lt;ref name=DGO/&gt;
*If ''G'' is an acylindrically hyperbolic group with ''K(G)={1}'' then ''G'' has infinite conjugacy classes of nontrivial elements, ''G'' is not inner amenable, and the reduced [[C*-algebra]] of ''G'' is simple with unique trace.&lt;ref name=DGO/&gt;
*There is a version of [[small cancellation theory]] over acylindrically hyperbolic groups, allowing one to produce many quotients of such groups with prescribed properties.&lt;ref&gt;M. Hull, ''Small cancellation in acylindrically hyperbolic groups''. [[Groups, Geometry, and Dynamics]] '''10''' (2016), no. 4, pp. 1077–1119 &lt;/ref&gt;
*Every finitely generated acylindrically hyperbolic group has cut points in all of its [[asymptotic cone]]s.&lt;ref&gt; A. Sisto, ''Quasi-convexity of hyperbolically embedded subgroups''. [[Mathematische Zeitschrift]] '''283''' (2016), no. 3-4, pp. 649–658 &lt;/ref&gt;
*For a finitely generated acylindrically hyperbolic group ''G'', the probability that the [[simple random walk]] on ''G'' of length ''n'' produces a 'generalized loxodromic element' in ''G'' converges to 1 exponentially fast as &lt;math&gt;n\to\infty&lt;/math&gt;. &lt;ref&gt;A. Sisto, [https://www.degruyter.com/view/j/crll.ahead-of-print/crelle-2015-0093/crelle-2015-0093.xml?format=INT ''Contracting elements and random walks''], [[Journal für die reine und angewandte Mathematik]], ahead of print, DOI 10.1515/crelle-2015-0093 &lt;/ref&gt;
*Every finitely generated acylindrically hyperbolic group ''G'' has exponential conjugacy growth, meaning that the number of distinct [[conjugacy classes]] of elements of ''G'' coming from the ball of radius ''n'' in the [[Cayley graph]] of ''G'' grows exponentially in ''n''. &lt;ref&gt; M. Hull, and D. Osin, ''Conjugacy growth of finitely generated groups''. [[Advances in Mathematics]] '''235''' (2013), pp. 361–389&lt;/ref&gt;

==Examples and non-examples==

*Finite groups, virtually nilpotent groups and virtually solvable groups are not acylindrically hyperbolic.
*Every non-elementary subgroup of a [[word-hyperbolic group]] is acylindrically hyperbolic.
*Every non-elementary [[relatively hyperbolic group]] is acylindrically hyperbolic. 
*The [[mapping class group]] &lt;math&gt;MCG(S_{g,p})&lt;/math&gt; of a connected oriented surface of genus &lt;math&gt;g\ge 0&lt;/math&gt; with &lt;math&gt;p\ge 0&lt;/math&gt; punctures is acylindrically hyperbolic, except for the cases where &lt;math&gt;g=0, p\le 3&lt;/math&gt; (in those exceptional cases the mapping class group is finite).&lt;ref name=Osin/&gt;  
*For &lt;math&gt;n\ge 2&lt;/math&gt; the group [[Out(Fn)|Out(''F&lt;sub&gt;n&lt;/sub&gt;'')]] is acylindrically hyperbolic.&lt;ref name=Osin/&gt; 
*Every non virtually cyclic group ''G'', that admits a proper isometric action on a proper [[CAT(0) space]] with ''G'' having at least one rank-1 element, is acylindrically hyperbolic.&lt;ref name=Osin/&gt; 
*Every right-angled Artin group ''G'', which is not cyclic and which is directly indecomposable, is acylindrically hyperbolic. 
*For &lt;math&gt;n\ge 3&lt;/math&gt; the [[special linear group]] &lt;math&gt;SL(n,\mathbb Z)&lt;/math&gt; is not acylindrically hyperbolic (Example 7.5 in &lt;ref name=Osin/&gt;).
*For &lt;math&gt;m\ne 0, n\ne 0&lt;/math&gt; the [[Baumslag-Solitar group]] &lt;math&gt;BS(m,n)=\langle a,t| t^{-1}a^mt=a^n\rangle &lt;/math&gt; is not acylindrically hyperbolic. (Example 7.4 in &lt;ref name=Osin/&gt;)
*Many groups admitting nontrivial actions on simplicity trees (that is, admitting nontrivial splittings as fundamental groups of graphs of groups in the sense of [[Bass-Serre theory]]) are acylindrically hyperbolic. For example, all one-relator groups on at least three generators are acylindrically hyperbolic.&lt;ref name=MO&gt;A. Minasyan, D. Osin,
''Acylindrical hyperbolicity of groups acting on trees''. 
[[Mathematische Annalen]] '''362''' (2015), no. 3-4, pp. 1055–1105  &lt;/ref&gt;
*Most 3-manifold groups are acylindrically hyperbolic.&lt;ref name=MO/&gt;
==References==
{{Reflist}}

==Further reading==

*Thomas Koberda, [http://www.ams.org/journals/notices/201801/rnoti-p31.pdf WHAT IS...an Acylindrical Group Action?], [[Notices of the American Mathematical Society]] '''65''' (2018), no. 1, pp. 31–34


[[Category:Group theory]]
[[Category:Geometric group theory]]
[[Category:Geometric topology]]
[[Category:Geometry]]</text>
      <sha1>mtlqgdt2zoy2wgo0543xbso44v38itc</sha1>
    </revision>
  </page>
  <page>
    <title>An Introduction to the Theory of Numbers</title>
    <ns>0</ns>
    <id>31962097</id>
    <revision>
      <id>857515790</id>
      <parentid>808515537</parentid>
      <timestamp>2018-09-01T04:30:58Z</timestamp>
      <contributor>
        <username>Smasongarrison</username>
        <id>16185737</id>
      </contributor>
      <minor/>
      <comment>/* References */copy editing</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2171">{{DISPLAYTITLE:''An Introduction to the Theory of Numbers''}}

'''''An Introduction to the Theory of Numbers''''' is a classic book in the field of [[number theory]], by [[G. H. Hardy]] and [[E. M. Wright]].

The book grew out of a series of lectures by Hardy and Wright and was first published in 1938.

The third edition added an elementary proof of the [[prime number theorem]], and the sixth edition added a chapter on elliptic curves.

==See also==

* [[List of important publications in mathematics]]

==References==

*{{Citation | last1=Bell | first1=E. T. | title=Book Review: An Introduction to the Theory of Numbers | doi=10.1090/S0002-9904-1939-07025-0 | year=1939 | journal=[[Bulletin of the American Mathematical Society]] | issn=0002-9904 | volume=45 | issue=7 | pages=507–509}}
*{{Citation | last1=Hardy | first1=Godfrey Harold | author1-link=G. H. Hardy | last2=Wright | first2=E. M. | title=An introduction to the theory of numbers. | publisher=Oxford: Clarendon Press | edition=First | year=1938 |jfm = 64.0093.03 | zbl = 0020.29201}}
*{{Citation | last1=Hardy | first1=Godfrey Harold | author1-link=G. H. Hardy | last2=Wright | first2=E. M. | title=An introduction to the theory of numbers | origyear=1938 | publisher=Oxford, at the Clarendon Press | edition=Third | mr=0067125 | year=1954}}
*{{Citation | last1=Hardy | first1=Godfrey Harold | author1-link=G. H. Hardy | last2=Wright | first2=E. M. | title=An introduction to the theory of numbers | origyear=1938 | publisher=The Clarendon Press Oxford University Press | edition=Fifth | isbn=978-0-19-853171-5 | mr=568909 | year=1979}}
*{{Citation | last1=Hardy | first1=Godfrey Harold | author1-link=G. H. Hardy | last2=Wright | first2=E. M. |editor1-first= D. R.|editor1-last= Heath-Brown|editor2-first= J. H. |editor2-last=Silverman|title=An introduction to the theory of numbers | origyear=1938 | url=https://books.google.com/books?id=d3wpAQAAMAAJ | publisher=[[Oxford University Press]] | edition=Sixth | isbn=978-0-19-921986-5 | mr=2445243 | year=2008}}

{{DEFAULTSORT:Introduction to the Theory of Numbers}}
[[Category:Mathematics books]]
[[Category:Number theory]]


{{mathematics-lit-stub}}</text>
      <sha1>e7c2lj9q2jpiolytxl8rgtrbigsb3s7</sha1>
    </revision>
  </page>
  <page>
    <title>Arrival theorem</title>
    <ns>0</ns>
    <id>21257247</id>
    <revision>
      <id>790659564</id>
      <parentid>787010071</parentid>
      <timestamp>2017-07-15T06:00:20Z</timestamp>
      <contributor>
        <username>Deacon Vorbis</username>
        <id>29330520</id>
      </contributor>
      <minor/>
      <comment>/* Theorem for Gordon–Newell networks */LaTeX spacing clean up, replaced: \,&lt;/math&gt; → &lt;/math&gt; using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6909">In [[queueing theory]], a discipline within the mathematical [[probability theory|theory of probability]], the '''arrival theorem'''&lt;ref&gt;{{Cite book | first1 = Søren | last1 = Asmussen| doi = 10.1007/0-387-21525-5_4 | chapter = Queueing Networks and Insensitivity | title = Applied Probability and Queues | series = Stochastic Modelling and Applied Probability | volume = 51 | pages = 114–136 | year = 2003 | isbn = 978-0-387-00211-8 | pmid =  | pmc = }}&lt;/ref&gt; (also referred to as the '''random observer property''', '''ROP''' or '''job observer property'''&lt;ref&gt;{{cite book|title=Sample-path Analysis of Queueing Systems|first=Muhammad|last=El-Taha|publisher=Springer|year=1999|isbn=0-7923-8210-2|page=94}}&lt;/ref&gt;) states that "upon arrival at a station, a job observes the system as if in steady state at an arbitrary instant for the system without that job."&lt;ref name="dijk"&gt;{{Cite journal | last1 = Van Dijk | first1 = N. M. | title = On the arrival theorem for communication networks | doi = 10.1016/0169-7552(93)90073-D | journal = Computer Networks and ISDN Systems | volume = 25 | issue = 10 | pages = 1135–2013 | year = 1993 | pmid =  | pmc = }}&lt;/ref&gt;

The arrival theorem always holds in open [[product-form solution|product-form networks]] with unbounded queues at each node, but it also holds in more general networks. A necessary and sufficient condition for the arrival theorem to be satisfied in product-form networks is given in terms of [[Palm probabilities]] in Boucherie &amp; Dijk, 1997.&lt;ref name=boucherie-dijk /&gt; A similar result also holds in some closed networks. Examples of product-form networks where the arrival theorem does not hold include reversible Kingman networks&lt;ref name=boucherie-dijk&gt;{{Cite journal | last1 = Boucherie | first1 = R. J. | last2 = Van Dijk | first2 = N. M. | doi = 10.1016/S0166-5316(96)00045-4 | title = On the arrivai theorem for product form queueing networks with blocking | journal = Performance Evaluation | volume = 29 | issue = 3 | pages = 155 | year = 1997 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref&gt;{{cite journal | last1 = Kingman | first1 = J. F. C. | authorlink1 = John Kingman | year = 1969 | title = Markov Population Processes | journal = Journal of Applied Probability | volume = 6 | issue = 1 | pages = 1–18 | publisher = Applied Probability Trust | jstor = 3212273}}&lt;/ref&gt; and networks with a delay protocol.&lt;ref name="dijk" /&gt;

Mitrani offers the intuition that "The state of node ''i'' as seen by an incoming job has a different distribution from the state seen by a random observer. For instance, an incoming job can never see all '''k'' jobs present at node ''i'', because it itself cannot be among the jobs already present."&lt;ref&gt;{{cite book | page = 114 | title = Modelling of Computer and Communication Systems | first = Isi | last = Mitrani | publisher = CUP | year = 1987 | isbn = 0521314224 }}&lt;/ref&gt;

==Theorem for arrivals governed by a Poisson process==

For [[Poisson processes]] the property is often referred to as the '''PASTA property''' (Poisson Arrivals See Time Averages) and states that the probability of the state as seen by an outside random observer is the same as the probability of the state seen by an arriving customer.&lt;ref&gt;{{Cite journal | last1 = Wolff | first1 = R. W. | title = Poisson Arrivals See Time Averages | doi = 10.1287/opre.30.2.223 | journal = Operations Research | volume = 30 | issue = 2 | pages = 223–231 | year = 1982 | pmid =  | pmc = }}&lt;/ref&gt; The property also holds for the case of a [[doubly stochastic Poisson process]] where the rate parameter is allowed to vary depending on the state.&lt;ref&gt;{{Cite journal | last1 = Van Doorn | first1 = E. A. | last2 = Regterschot | first2 = G. J. K. | doi = 10.1016/0167-6377(88)90036-3 | title = Conditional PASTA | journal = Operations Research Letters | url = http://doc.utwente.nl/70043/1/Doorn88conditional.pdf| volume = 7 | issue = 5 | pages = 229 | year = 1988 | pmid =  | pmc = }}&lt;/ref&gt;

==Theorem for Jackson networks==

In an open [[Jackson network]] with ''m'' queues, write &lt;math&gt;\scriptstyle{\mathbf{n} = (n_1, n_2, \ldots, n_m)}&lt;/math&gt; for the state of the network. Suppose &lt;math&gt;\scriptstyle{\pi(\mathbf{n})}&lt;/math&gt; is the equilibrium probability that the network is in state &lt;math&gt;\scriptstyle{\mathbf{n}}&lt;/math&gt;. Then the probability that the network is in state &lt;math&gt;\scriptstyle{\mathbf{n}}&lt;/math&gt; immediately before an arrival to any node is also &lt;math&gt;\scriptstyle{\pi(\mathbf{n})}&lt;/math&gt;.

Note that this theorem does not follow from [[Jackson's theorem (queueing theory)|Jackson's theorem]], where the steady state in continuous time is considered. Here we are concerned with particular points in time, namely arrival times.&lt;ref&gt;{{cite book|first=Peter G.|last=Harrison|authorlink=Peter G. Harrison|first2=Naresh M.|last2=Patel|title=Performance Modelling of Communication Networks and Computer Architectures|publisher=Addison-Wesley|year=1992|page=228|isbn=0-201-54419-9}}&lt;/ref&gt; This theorem first published by Sevcik and Mitrani in 1981.&lt;ref name="sevcik" /&gt;

==Theorem for Gordon–Newell networks==

In a closed [[Gordon–Newell network]] with ''m'' queues, write &lt;math&gt;{\mathbf{n} = (n_1, n_2, \ldots, n_m)}&lt;/math&gt; for the state of the network. For a customer in transit to state &lt;math&gt;i&lt;/math&gt;, let &lt;math&gt;{\alpha_i(\mathbf{n}-\mathbf{e}_i)}&lt;/math&gt; denote the probability that immediately before arrival the customer 'sees' the state of the system to be

:&lt;math&gt;\mathbf{n}-\mathbf{e}_i = (n_1,n_2,\ldots,n_i - 1,\ldots,n_m).&lt;/math&gt;

This probability, &lt;math&gt;{\alpha_i(\mathbf{n}-\mathbf{e}_i)}&lt;/math&gt;, is the same as the steady state probability for state &lt;math&gt;{\mathbf{n}-\mathbf{e}_i}&lt;/math&gt; for a network of the same type with ''one customer less''.&lt;ref&gt;{{Cite book | first1 = L. | last1 = Breuer| first2 = Dave | last2 = Baum| doi = 10.1007/1-4020-3631-0_5 | chapter = Markovian Queueing Networks | title = An Introduction to Queueing Theory and Matrix-Analytic Methods | pages = 63–61 | year = 2005 | isbn = 1-4020-3630-2 | pmid =  | pmc = }}&lt;/ref&gt; It was published independently by Sevcik and Mitrani,&lt;ref name="sevcik"&gt;{{Cite journal | last1 = Sevcik | first1 = K. C. | last2 = Mitrani | first2 = I. | doi = 10.1145/322248.322257 | title = The Distribution of Queuing Network States at Input and Output Instants | journal = Journal of the ACM | volume = 28 | issue = 2 | pages = 358 | year = 1981 | pmid =  | pmc = }}&lt;/ref&gt; and Reiser and Lavenberg,&lt;ref&gt;{{Cite journal | last1 = Reiser | first1 = M.| last2 = Lavenberg | first2 = S. S. | doi = 10.1145/322186.322195 | title = Mean-Value Analysis of Closed Multichain Queuing Networks | journal = [[Journal of the ACM]]| volume = 27 | issue = 2 | pages = 313 | year = 1980 | pmid =  | pmc = }}&lt;/ref&gt; where the result was used to develop [[mean value analysis]].

==Notes==
{{Reflist}}

{{Queueing theory}}

[[Category:Queueing theory]]
[[Category:Probability theorems]]</text>
      <sha1>moiealh5ziykrmzzl5c94iu5cn9vke2</sha1>
    </revision>
  </page>
  <page>
    <title>Basis (universal algebra)</title>
    <ns>0</ns>
    <id>16728132</id>
    <revision>
      <id>788969360</id>
      <parentid>596976481</parentid>
      <timestamp>2017-07-04T15:07:25Z</timestamp>
      <contributor>
        <username>PrimeBOT</username>
        <id>29463730</id>
      </contributor>
      <minor/>
      <comment>Replace [[Help:Magic_links|magic links]] with templates per [[Special:PermaLink/772743896#Future_of_magic_links|local RfC]] - [[User:PrimeBOT/13|BRFA]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11849">In [[universal algebra]] a '''basis''' is a structure inside of some (universal) algebras, which are called [[free algebra]]s. It generates all algebra elements from its own elements by the algebra operations in an independent manner. It also represents the [[endomorphisms]] of an algebra by certain indexings of algebra elements, which can correspond to the usual [[Matrix (mathematics)|matrices]] when the free algebra is a [[vector space]].

== Definitions ==

The '''basis''' (or '''reference frame''') ''' of a [[Universal algebra|(universal) algebra]]''' is a [[Function (set theory)|function]] ''b'' that takes some algebra elements as values &lt;math&gt;b(i)&lt;/math&gt; and satisfies either one of the following two equivalent conditions. Here, the set of all &lt;math&gt;b(i)&lt;/math&gt; is called '''basis set''', whereas several authors call it the "basis".&lt;ref&gt;Gould.&lt;/ref&gt;&lt;ref&gt;Grätzer  1968, p.198.&lt;/ref&gt; The set &lt;math&gt;I&lt;/math&gt; of its arguments ''i'' is called '''dimension set'''. Any function, with all its arguments in the whole &lt;math&gt;I&lt;/math&gt;, that takes algebra elements as values (even outside the basis set) will be denoted by ''m''. Then, ''b'' will be an ''m''.

=== Outer condition ===
This condition will define bases by the set ''L'' of the &lt;math&gt;I&lt;/math&gt;-'''ary elementary functions of the algebra''', which are certain functions &lt;math&gt;\ell&lt;/math&gt;  that take every ''m'' as argument to get some algebra element as value &lt;math&gt;\ell(m)&lt;/math&gt;. In fact, they consist of all the '''projections''' &lt;math&gt;p_i&lt;/math&gt; with ''i'' in &lt;math&gt;I&lt;/math&gt;, which are the functions such that &lt;math&gt;p_i(m)=m(i)&lt;/math&gt; for each ''m'', and of all functions that rise from them by repeated "multiple compositions" with operations of the algebra. 

(When an algebra operation has a single algebra element as argument, the value of such a composed function is the one that the operation takes from the value of a single previously computed &lt;math&gt;I&lt;/math&gt;-ary function as in [[Function composition|composition]]. When it does not, such compositions require that many (or none for a nullary operation) &lt;math&gt;I&lt;/math&gt;-ary functions are evaluated before the algebra operation: one for each possible algebra element in that argument. In case &lt;math&gt;I&lt;/math&gt; and the numbers of  elements in the arguments, or “arity”, of the operations are finite, this is the [[clone (algebra)|finitary multiple composition]] .) 

Then, according to the ''outer condition'' a basis has to ''generate'' the algebra (namely when &lt;math&gt;\ell&lt;/math&gt; ranges over the whole ''L'', &lt;math&gt;\ell(b)&lt;/math&gt; gets every algebra element) and must be ''independent'' (namely whenever any two &lt;math&gt;I&lt;/math&gt;-ary elementary functions coincide at ''b'', they will do everywhere: &lt;math&gt;\ell'(b)=\ell''(b)&lt;/math&gt; implies &lt;math&gt;\ell'=\ell''&lt;/math&gt;).&lt;ref&gt;For instance, see (Grätzer  1968, p.198).&lt;/ref&gt; This is the same as to require that there exists a ''single'' function &lt;math&gt;\chi&lt;/math&gt; that takes every algebra element as argument to get an  &lt;math&gt;I&lt;/math&gt;-ary elementary function as value and satisfies &lt;math&gt;\chi({\ell(b)})=\ell&lt;/math&gt; for all &lt;math&gt;\ell&lt;/math&gt; in ''L''.

=== Inner condition ===
This other condition will define bases by the set ''E'' of the '''endomorphisms''' of the algebra, which are the  [[Universal algebra|homomorphisms]] from the algebra into itself, through its '''analytic representation''' &lt;math&gt;\varrho&lt;/math&gt; by a basis. The latter is a function that takes every endomorphism ''e'' as argument to get a function ''m'' as value: &lt;math&gt;\varrho(e)=m&lt;/math&gt;, where this ''m'' is the "sample" of the values of ''e'' at ''b'', namely &lt;math&gt;m(i)=[\varrho(e)]_i=e(b(i))&lt;/math&gt; for all ''i'' in the dimension set.

Then, according to the ''inner condition'' ''b'' is a basis, when &lt;math&gt;\varrho&lt;/math&gt; is a '''bijection''' from ''E'' onto the set of all ''m'', namely for each ''m'' there is one and only one endomorphism ''e'' such that &lt;math&gt;m=\varrho(e)&lt;/math&gt;. This is the same as to require that there exists an '''extension function''', namely a function &lt;math&gt;\eta&lt;/math&gt; that takes   every (sample) ''m'' as argument to extend it onto an endomorphism  &lt;math&gt;\eta(m)&lt;/math&gt;  such that &lt;math&gt;\varrho(\eta(m))=m&lt;/math&gt;.&lt;ref&gt;For instance, see '''0.4''' and '''0.5''' of (Ricci 2007)&lt;/ref&gt;

The link between these two conditions is given by the identity  &lt;math&gt;[\chi(a)]_m=[\eta(m)]_a&lt;/math&gt;, which holds for all ''m'' and all algebra elements ''a''.&lt;ref&gt;For instance, see '''0.4''' (E) of (Ricci 2007)&lt;/ref&gt; Several other conditions that characterize bases for universal algebras are omitted.

As the next example will show, present bases are a generalization of the [[Basis (linear algebra)|bases]] of vector spaces. Then, the name "reference frame" can well replace "basis". Yet, contrary to the vector space case, a universal algebra might lack bases and, when it has them, their dimension sets might have different finite positive cardinalities.&lt;ref&gt;Grätzer  1979.&lt;/ref&gt;

== Examples ==

=== Vector space algebras ===
In the universal algebra corresponding to a vector space with positive dimension the bases essentially are the [[ordered basis|ordered bases]] of this vector space. Yet, this will come after several details.

When the vector space is finite-dimensional, for instance &lt;math&gt;I=\{0,1,\ldots n-1\}&lt;/math&gt; with &lt;math&gt;n &gt; 0&lt;/math&gt;, the functions &lt;math&gt;\ell&lt;/math&gt; in the set ''L'' of the ''outer condition'' exactly are the ones that provide the [[Basis (linear algebra)|spanning and linear independence properties]] with linear combinations &lt;math&gt;\ell(b)=c_0 b_0+c_1b_1+\ldots c_{n-1}b_{n-1}&lt;/math&gt; and present generator property becomes the spanning one. On the contrary, linear independence is a mere instance of present independence, which becomes equivalent to it in such vector spaces. (Also, several other generalizations of linear independence for universal algebras do not imply present independence.)

The functions ''m'' for the ''inner condition'' correspond to the square arrays of field numbers (namely, usual vector-space square  matrices) that serve to build the endomorphisms of vector spaces (namely, [[linear maps]] into themselves). Then, the ''inner condition'' requires a bijection property from endomorphisms also to arrays. In fact, each column of such an array represents a vector &lt;math&gt;m(i)&lt;/math&gt; as its ''n''-tuple of [[coordinate]]s with respect to the basis ''b''. For instance, when the vectors are ''n''-tuples of numbers from the underlying field and ''b'' is the [[standard basis|Kronecker basis]], ''m'' is such an array ''seen by columns'', &lt;math&gt;\varrho&lt;/math&gt; is the sample of such a linear map at the reference vectors and &lt;math&gt;\eta&lt;/math&gt; extends this sample to this map as below.

&lt;math&gt;{}\qquad
\left(\begin{array}{rrc}
0  &amp; -1 &amp; 2 \\
-2 &amp; 3  &amp; 1 \\
1  &amp; 0  &amp; 2
\end{array}\right)
\quad
\begin{array}{c}
\stackrel{\eta}{\longmapsto}\\
\stackrel{\varrho}{\longleftarrow\!\!{}^{{}_{\!{}_\mathsf{l}}}}
\end{array}
\quad
\left\{
\begin{array}{rcrccr}
x'_0  &amp; = &amp; &amp; -x_1 &amp;+&amp; 2x_2 \\
x'_1  &amp; = &amp;-2x_0&amp;+3x_1&amp;+&amp; x_2\\
x'_2  &amp; = &amp;  x_0 &amp; &amp; +&amp;2x_2
\end{array}\right.&lt;/math&gt; 

When the vector space is not finite-dimensional, further distinctions are needed. In fact, though the functions &lt;math&gt;\ell&lt;/math&gt; formally have an infinity of vectors in every argument, the linear combinations they evaluate never require infinitely many addenda &lt;math&gt;c_i m(i)&lt;/math&gt; and each &lt;math&gt;\ell&lt;/math&gt; determines a finite subset ''J'' of &lt;math&gt;I&lt;/math&gt; that contains all required ''i''. Then, every value &lt;math&gt;\ell(m)&lt;/math&gt; equals an &lt;math&gt;\ell'(m')&lt;/math&gt;, where &lt;math&gt;m'&lt;/math&gt; is the restriction of ''m'' to ''J'' and &lt;math&gt;\ell'&lt;/math&gt; is the ''J''-ary  elementary function corresponding to &lt;math&gt;\ell&lt;/math&gt;. When the &lt;math&gt;\ell'&lt;/math&gt; replace the  &lt;math&gt;\ell&lt;/math&gt;, both the linear independence and spanning properties for infinite basis sets follow from present ''outer condition'' and conversely. 

Therefore, as far as vector spaces of a positive dimension are concerned, the only difference between present bases for universal algebras and the [[ordered basis|ordered bases]] of vector spaces is that here no order on &lt;math&gt;I&lt;/math&gt; is required. Still it is allowed, in case it serves some purpose. 

When the space is zero-dimensional, its ordered basis is empty. Then, being the [[empty function]], it is a present basis. Yet, since this space only contains the null vector and its only endomorphism is the identity, any function ''b'' from any set &lt;math&gt;I&lt;/math&gt; (even a nonempty one) to this singleton space work as a present basis. This is not so strange from the point of view of Universal Algebra, where singleton algebras, which are called "trivial", enjoy a lot of other seeming strange properties.

=== Word monoid ===
Let &lt;math&gt;I=\{ \mathsf{a, b, c,} \ldots\}&lt;/math&gt; be an "alphabet", namely a (usually finite) set of objects called "letters". Let ''W'' denote the corresponding set of '''words''' or "strings", which will be denoted as in [[String (computer science)|strings]], namely either by writing their letters in sequence or by &lt;math&gt;\epsilon&lt;/math&gt; in case of the empty word ([[formal language|Formal Language]] notation).&lt;ref name=warning&gt;Formal Language notation is used in Computer Science and sometimes collides with the set-theoretical definitions of words. See G. Ricci,  ''An observation on a Formal Language notation,'' SIGACT News, '''17''' (1972), 18&amp;ndash;23.&lt;/ref&gt; Accordingly, the juxtaposition ''&lt;math&gt;vw&lt;/math&gt;'' will denote the [[concatenation]] of two words ''v'' and ''w'', namely the word that begins with ''v'' and is followed by ''w''.

Concatenation is a binary operation on ''W'' that together with the empty word &lt;math&gt;\epsilon&lt;/math&gt; defines a [[free monoid]], the monoid of the words on &lt;math&gt;I&lt;/math&gt;, which is one of the simplest universal algebras. Then, the ''inner condition'' will immediately prove that one of its bases is the function ''b'' that makes a single-letter word &lt;math&gt;{i}&lt;/math&gt; of each letter &lt;math&gt;\mathsf{i}&lt;/math&gt;, &lt;math&gt;b(\mathsf{i})=i&lt;/math&gt;.

(Depending on the set-theoretical implementation of sequences, ''b'' may not be an identity function, namely &lt;math&gt;i&lt;/math&gt; may not be &lt;math&gt;\mathsf{i}&lt;/math&gt;, rather an object like &lt;math&gt;\{ (\emptyset,\mathsf{i})\}&lt;/math&gt;, namely a singleton function, or a pair like &lt;math&gt;(\emptyset,\mathsf{i})&lt;/math&gt; or &lt;math&gt;(\mathsf{i},\emptyset)&lt;/math&gt;.&lt;ref name=warning/&gt;) 

In fact, in the theory of [[D0L system]]s (Rozemberg &amp; Salomaa 1980) such &lt;math&gt;m=\varrho(e)&lt;/math&gt; are the tables of  [[L-system|"productions"]], which such systems use to define the simultaneous substitutions of every &lt;math&gt;i&lt;/math&gt; by a single word &lt;math&gt;w=m(\mathsf{i})&lt;/math&gt; in any word ''u'' in ''W'': if &lt;math&gt;u={i}_0{i}_1\cdots {i}_k&lt;/math&gt;, then &lt;math&gt;e(u)=m(\mathsf{i}_0)m(\mathsf{i}_1)\cdots m(\mathsf{i}_k)&lt;/math&gt;. Then, ''b'' satisfies the ''inner condition'', since the function &lt;math&gt;\varrho&lt;/math&gt; is the well-known bijection that identifies every word endomorphism with any such table. (The repeated applications of such an endomorphism starting from a given "seed" word are able to model many growth processes, where words and concatenation serve to build fairly heterogeneous structures as in [[L-system]], not just "sequences".)

== Notes ==
&lt;references/&gt;

== References ==

# Gould, V. ''Independence algebras,''  Algebra Universalis '''33''' (1995), 294&amp;ndash;318.
# Grätzer, G. (1968). ''Universal Algebra'', D. Van Nostrand Company Inc.. 	
# Grätzer, G. (1979). ''Universal Algebra'' 2-nd 2ed., Springer Verlag. {{ISBN|0-387-90355-0}}.
# Ricci, G. (2007). ''Dilatations kill fields'', Int. J. Math. Game Theory Algebra, '''16''' 5/6, pp.&amp;nbsp;13&amp;ndash;34.
# Rozenberg G. and Salomaa A. (1980). ''The mathematical theory of L systems'', Academic Press, New York. {{ISBN|0-12-597140-0}}

[[Category:Universal algebra]]</text>
      <sha1>l7yz6mn4ox5hmpw5fai265ympwl7sca</sha1>
    </revision>
  </page>
  <page>
    <title>Binary Goppa code</title>
    <ns>0</ns>
    <id>37834267</id>
    <revision>
      <id>868847774</id>
      <parentid>825837252</parentid>
      <timestamp>2018-11-14T20:55:40Z</timestamp>
      <contributor>
        <username>Texvc2LaTeXBot</username>
        <id>33995001</id>
      </contributor>
      <minor/>
      <comment>Replacing deprecated latex syntax [[mw:Extension:Math/Roadmap]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5781">In [[mathematics]] and [[computer science]], the '''binary Goppa code''' is an [[error-correcting code]] that belongs to the class of general [[Goppa code]]s originally described by [[Valerii Denisovich Goppa]], but the binary structure gives it several mathematical advantages over non-binary variants, also providing a better fit for common usage in computers and telecommunication. Binary Goppa codes have interesting properties suitable for [[cryptography]] in [[McEliece cryptosystem|McEliece-like cryptosystems]] and similar setups.

==Construction and properties==

A binary Goppa code is defined by a [[polynomial]] &lt;math&gt;g(x)&lt;/math&gt; of degree &lt;math&gt;t&lt;/math&gt; over a [[finite field]] &lt;math&gt;GF(2^m)&lt;/math&gt; without multiple zeros, and a sequence &lt;math&gt;L&lt;/math&gt; of &lt;math&gt;n&lt;/math&gt; distinct elements from &lt;math&gt;GF(2^m)&lt;/math&gt; that aren't roots of the polynomial:

: &lt;math&gt;\forall i,j \in \{0,\ldots,n-1\}: L_i \in GF(2^m) \land L_i \neq L_j \land g(L_i) \neq 0&lt;/math&gt;

Codewords belong to the kernel of syndrome function, forming a subspace of &lt;math&gt;\{0,1\}^n&lt;/math&gt;:

: &lt;math&gt;\Gamma(g,L)=\left\{ c \in \{0,1\}^n \left| \sum_{i=0}^{n-1} \frac{c_i}{x-L_i} \equiv 0 \mod g(x) \right. \right\}&lt;/math&gt;

Code defined by a tuple &lt;math&gt;(g,L)&lt;/math&gt; has minimum distance &lt;math&gt;2t+1&lt;/math&gt;, thus it can correct &lt;math&gt;t=\left\lfloor \frac{(2t+1)-1}{2} \right\rfloor&lt;/math&gt; errors in a word of size &lt;math&gt;n-mt&lt;/math&gt; using codewords of size &lt;math&gt;n&lt;/math&gt;. It also possesses a convenient [[parity-check matrix]] &lt;math&gt;H&lt;/math&gt; in form

: &lt;math&gt;
H=VD=\begin{pmatrix}
1 &amp; 1 &amp; 1 &amp; \cdots &amp; 1\\
L_0^1 &amp; L_1^1 &amp; L_2^1 &amp; \cdots &amp; L_{n-1}^1\\
L_0^2 &amp; L_1^2 &amp; L_2^2 &amp; \cdots &amp; L_{n-1}^2\\
\vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
L_0^{t-1} &amp; L_1^{t-1} &amp; L_2^{t-1} &amp; \cdots &amp; L_{n-1}^{t-1}
\end{pmatrix}
\begin{pmatrix}
\frac{1}{g(L_0)} &amp; &amp; &amp; &amp; \\
 &amp; \frac{1}{g(L_1)} &amp; &amp; &amp; \\
 &amp; &amp; \frac{1}{g(L_2)} &amp; &amp; \\
 &amp; &amp; &amp; \ddots &amp; \\
 &amp; &amp; &amp; &amp; \frac{1}{g(L_{n-1})}
\end{pmatrix}
&lt;/math&gt;

Note that this form of the parity-check matrix, being composed of a [[Vandermonde matrix]] &lt;math&gt;V&lt;/math&gt; and [[diagonal matrix]] &lt;math&gt;D&lt;/math&gt;, shares the form with check matrices of [[alternant code]]s, thus alternant decoders can be used on this form. Such decoders usually provide only limited error-correcting capability (in most cases &lt;math&gt;t/2&lt;/math&gt;).

For practical purposes, parity-check matrix of a binary Goppa code is usually converted to a more computer-friendly binary form by a trace construction, that converts the &lt;math&gt;t&lt;/math&gt;-by-&lt;math&gt;n&lt;/math&gt; matrix over &lt;math&gt;GF(2^m)&lt;/math&gt; to a &lt;math&gt;mt&lt;/math&gt;-by-&lt;math&gt;n&lt;/math&gt; binary matrix by writing polynomial coefficients of &lt;math&gt;GF(2^m)&lt;/math&gt; elements on &lt;math&gt;m&lt;/math&gt; successive rows.

==Decoding==

Decoding of binary Goppa codes is traditionally done by Patterson algorithm, which gives good error-correcting capability (it corrects all &lt;math&gt;t&lt;/math&gt; design errors), and is also fairly simple to implement.

Patterson algorithm converts a syndrome to a vector of errors. The syndrome of a word &lt;math&gt;c=(c_0,\dots,c_{n-1})&lt;/math&gt; is expected to take a form of

: &lt;math&gt;s(x) \equiv \sum_{i=0}^{n-1} \frac{1}{x - L_i} \mod g(x)&lt;/math&gt;

Alternative form of a parity-check matrix based on formula for &lt;math&gt;s(x)&lt;/math&gt; can be used to produce such syndrome with a simple matrix multiplication.

The algorithm then computes &lt;math&gt;v(x) \equiv \sqrt{s(x)^{-1}-x} \mod g(x)&lt;/math&gt;. That fails when &lt;math&gt;s(x) \equiv 0&lt;/math&gt;, but that is the case when the input word is a codeword, so no error correction is necessary.

&lt;math&gt;v(x)&lt;/math&gt; is reduced to polynomials &lt;math&gt;a(x)&lt;/math&gt; and &lt;math&gt;b(x)&lt;/math&gt; using the [[extended euclidean algorithm]], so that &lt;math&gt;a(x) \equiv b(x)\cdot v(x) \mod g(x)&lt;/math&gt;, while &lt;math&gt;\deg(a)\leq\lfloor t/2 \rfloor&lt;/math&gt; and &lt;math&gt;\deg(b)\leq\lfloor (t-1)/2 \rfloor&lt;/math&gt;.

Finally, the ''error locator polynomial'' is computed as &lt;math&gt;\sigma(x) = a(x)^2 + x\cdot b(x)^2&lt;/math&gt;. Note that in binary case, locating the errors is sufficient to correct them, as there's only one other value possible. In non-binary cases a separate error correction polynomial has to be computed as well.

If the original codeword was decodable and the &lt;math&gt;e=(e_0,e_1,\dots,e_{n-1})&lt;/math&gt; was the error vector, then

: &lt;math&gt;\sigma(x) = \prod_{i=0}^{n-1} (x-L_i) &lt;/math&gt;

Factoring or evaluating all roots of &lt;math&gt;\sigma(x)&lt;/math&gt; therefore gives enough information to recover the error vector and fix the errors.

==Properties and usage==

Binary Goppa codes viewed as a special case of Goppa codes have the interesting property that they correct full &lt;math&gt;\deg(g)&lt;/math&gt; errors, while only &lt;math&gt;\deg(g)/2&lt;/math&gt; errors in ternary and all other cases. Asymptotically, this error correcting capability meets the famous [[Gilbert–Varshamov bound]].

Because of the high error correction capacity compared to code rate and form of parity-check matrix (which is usually hardly distinguishable from a random binary matrix of full rank), the binary Goppa codes are used in several [[post-quantum]] [[cryptosystem]]s, notably [[McEliece cryptosystem]] and [[Niederreiter cryptosystem]].

==References==

* Elwyn R. Berlekamp, Goppa Codes, IEEE Transactions on information theory, Vol. IT-19, No. 5, September 1973, http://infosec.seu.edu.cn/space/kangwei/senior_thesis/Goppa.pdf
* Daniela Engelbert, Raphael Overbeck, Arthur Schmidt. "A summary of McEliece-type cryptosystems and their security." Journal of Mathematical Cryptology 1, 151–199. {{MR|2345114}}. Previous version: http://eprint.iacr.org/2006/162/
* Daniel J. Bernstein. "List decoding for binary Goppa codes." http://cr.yp.to/codes/goppalist-20110303.pdf

==See also==

* [[BCH codes]]
* [[Code rate]]
* [[Reed–Solomon error correction]]

[[Category:Coding theory]]</text>
      <sha1>0wxcbbirqg6f3x3967hkhrx96127rh9</sha1>
    </revision>
  </page>
  <page>
    <title>Bogdan Suceavă</title>
    <ns>0</ns>
    <id>41819221</id>
    <revision>
      <id>867798619</id>
      <parentid>864387651</parentid>
      <timestamp>2018-11-08T02:26:37Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>recategorize</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12497">{{Infobox scientist
| name              = Bogdan Suceavă
| native_name       = 
| native_name_lang  = 
| image             = 
| image_size        =
| alt               = 
| caption           = 
| birth_date        = {{Birth date|1969|09|27}} 
| birth_place       = Curtea de Argeș, Romania
| death_date        = &lt;!-- {{Death date and age|YYYY|MM|DD|YYYY|MM|DD}} 
                           (death date then birth date) --&gt;
| death_place       = 
| resting_place     = 
| resting_place_coordinates = &lt;!-- 
                      {{Coord|LAT|LONG|type:landmark|display=inline,title}} --&gt;
| other_names       = 
| residence         = [[Fullerton, California]]
| citizenship       = United States
| nationality       = Romanian
| fields            = [[Romanian literature]], [[Differential geometry]], [[History of mathematics]]
| workplaces        = [[California State University, Fullerton]]
| patrons           = 
| alma_mater        = [[University of Bucharest]], [[Michigan State University]]
| thesis_title      = New Riemannian and Kählerian Curvature Invariants and Strongly Minimal Submanifolds
| thesis_url        = 
| thesis_year       = 
| doctoral_advisor  = [[Bang-Yen Chen]]
| academic_advisors = 
| doctoral_students = 
| notable_students  = 
| known_for         = "Coming from An Off-Key Time", "Miruna, A Tale"
| author_abbrev_bot = 
| author_abbrev_zoo = 
| influences        = 
| influenced        = 
| awards            = Bucharest Writers Association Fiction Award (2007); Honorary citizen of Târgoviște (2016)
| signature         = &lt;!--(filename only)--&gt;
| signature_alt     = 
| website           =  {{URL|mathfaculty.fullerton.edu/bsuceava/|Bogdan D. Suceavă}}
| footnotes         = 
| spouse            = 
| children          = 
}}
'''Bogdan Suceavă''' (born September 27, 1969 in [[Curtea de Argeș]]) is a Romanian-born U.S. mathematician and writer.

==Biography==
Bogdan Suceavă was born in Curtea de Argeș, Romania, on September 27, 1969. Growing up, Suceavă spent his holidays with his maternal grandparents at [[Nucșoara]], a remote community that maintained its traditions, unbroken by the collectivisation elsewhere of [[Nicolae Ceaușescu|Ceaușescu]] regime. There he absorbed Balkan folk-tales and myths, which would inform some of his literary works.&lt;ref&gt;"Author's Note" in {{cite book|author=Bogdan Suceavă|title=Miruna, A Tale|publisher=Twisted Spoon|location=Prague|year=2013}}&lt;/ref&gt;

Suceavă attended the [[University of Bucharest]], where he obtained his undergraduate and master's degree in mathematics. He then moved to the United States to study at the [[Michigan State University]] for his doctorate. His thesis, titled ''New Riemannian and Kählerian Curvature Invariants and Strongly Minimal Submanifolds'', was written under the supervision of [[Bang-Yen Chen]].&lt;ref name=evz&gt;{{cite web|url=http://www.evz.ro/detalii/stiri/bogdan-suceava-cel-mai-important-ar-fi-sa-taca-din-gura-cei-care-nu-se-pricep-899524.html|title=Bogdan Suceava: "Cel mai important ar fi să tacă din gură cei care nu se pricep"|date=July 4, 2010|publisher=Voci pentru România|accessdate=February 2, 2014|language=Romanian}}&lt;/ref&gt;

Following his doctorate in 2002, Suceavă was hired by [[California State University, Fullerton]].&lt;ref name=evz/&gt;

==Career==

===Mathematics===
At the age of 13, Suceavă won a prize at the Romanian National Mathematical Olympiad, following which he was encouraged to pursue mathematics as a viable career.&lt;ref name=bookaholic&gt;{{cite web|url=http://bookaholic.ro/am-ales-sa-traiesc-in-statele-unite-pentru-ca-imi-place-sa-fiu-liber-intr-o-lume-stabila-interviu-cu-scriitorul-bogdan-suceava-2.html|title=Am ales să trăiesc în Statele Unite pentru că îmi place să fiu liber într-o lume stabilă|author=Adina Diniţoiu|language=Romanian|date=June 26, 2013|accessdate=February 2, 2014}}&lt;/ref&gt;

Suceavă is a [[Professor]] of Mathematics at the [[California State University, Fullerton]]. He specialises in [[Differential geometry]], the foundations of geometry, and the history of mathematics.

Suceavă is active in the encouragement of mathematical research among young students in California. He has established a mathematics circle involving undergraduates, and extensively published in gazettes of mathematical problems aimed at high school students.&lt;ref name=suplimentul&gt;{{cite web|url=http://www.suplimentuldecultura.ro/index/continutArticolNrIdent/Interviu/8521|language=Romanian|date=August 6, 2013|author=George Onofrei|publisher=Suplimentul del Cultură|title=Bogdan Suceava: "Cum ar fi fost sa ai o diplomatie extraordinara si sa nu fi existat scriitorii?"|accessdate=February 2, 2013}}&lt;/ref&gt;

His mathematical works appeared in ''Houston Journal of Mathematics, Taiwanese Journal of Mathematics, American Mathematical Monthly, Mathematical Intelligencer, Beiträge zur Algebra und Geometrie, Differential Geometry and Its Applications, Czechoslovak Mathematical Journal, Publicationes Mathematicae, Results in Mathematics, Tsukuba Journal of Mathematics, Notices of the American Mathematical Society, Contemporary Mathematics, Historia Mathematica'', and other mathematical journals.

Suceavă served as editor, together with Alfonso Carriazo, Yun Myung Oh and Joeri Van Der Veken, of the volume  ''Recent Advances in the Geometry of Submanifolds. Dedicated to the Memory of Franki Dillen (1963-2013),'' American Mathematical Society, 2016.

===Literary===
Suceavă began his writing career in 1990 with a volume of prose and essays published by Topaz, ''Teama de amurg'' ("Fear of twilight"). He has also published various volumes of novels and short stories.

While Suceavă writes predominantly in Romanian, his short fiction in English has appeared in ''Review of Contemporary Fiction'', ''Absinthe: New European Writing'', and ''Red Mountain Review''.

In 1989, Suceavă was a student in Bucharest during the downfall of the [[Nicolae Ceaușescu|Ceaușescu]] dictatorship. Its impact on his country's social and cultural life motivated him to write his novel ''Venea din timpul diez'' in 2004.&lt;ref name=ramos&gt;{{cite journal|title=Romanian Satire: Professor’s Novel Addresses Post-Communist Life|author=Debra Cano Ramos|journal=Spotlight|date=March 23, 2011|url=http://calstate.fullerton.edu/spotlight/2011sp/Bogdan-Suceava.asp|accessdate=February 3, 2014}}&lt;/ref&gt;
&lt;ref name=kelleher&gt;{{cite journal|title=The Bogdan Suceava Interview|author=Damian Kelleher|journal=Quarterly Conversation|date=June 6, 2011|url=http://quarterlyconversation.com/the-bogdan-suceava-interview|accessdate=February 10, 2014}}&lt;/ref&gt;

In 2007, Suceavă received the Fiction Award of the Association of Bucharest Writers for his novel, ''Miruna, A Tale.''&lt;ref name=ramos/&gt;

Two of his books (''Coming from an Off-Key Time'', and ''Miruna, A Tale'') have been translated into English, and received positive reviews.

In 2015, the Czech version of the novel ''Coming from an Off-Key Time'', in Jiří Našinec's translation, was presented with the Josef Jungmann Award.&lt;ref&gt;{{cite web|url=https://www.ceatl.eu/several-czech-translation-prizes-awarded|title=Several Czech translation prizes awarded|publisher=CEATL|date=19 October 2015|accessdate=18 March 2018}}&lt;/ref&gt;

==Bibliography==

===Literature===
# ''Teama de amurg'', Editura Topaz, Bucharest (1990)
# ''Sub semnul Orionului'', Editura Artprint, Bucharest (1992) – novel
# ''Legende și eresuri'', Magic Art Design, Bucharest (1995) – poetry
# ''Imperiul generalilor târzii și alte istorii'', Editura Dacia (2002) – short stories
# ''Bunicul s-a întors la franceză, istorii'', Editura T/Fundația Timpul, Iași (2003) - short stories
# ''Venea din timpul diez'', Editura Polirom, Bucharest (2004) – novel (''Coming from an Off-Key Time'', translated by Alistair Ian Blyth, [[Northwestern University Press]], 2011)
# ''Bătălii și mesagii'', Editura LiterNet, Bucharest (2005) - poetry
# ''Miruna, o poveste'', Editura Curtea Veche, Bucharest (2007) – novel (''Miruna, A Tale'', translated by Alistair Ian Blyth, Twisted Spoon Press, Prague, 2014.)
# ''Distanțe, demoni, aventuri'', Editura Tritonic, Bucharest (2007) - essays
# ''Vincent nemuritorul'', Editura Curtea Veche, Bucharest (2008) – novel
# ''Noaptea când cineva a murit pentru tine'', Editura Polirom, Bucharest (2010) – novel
# ''Memorii din biblioteca ideală'', Editura Polirom, Bucharest (2013) – essays
# ''Să auzi forma unei tobe'', Millennium Books, Satu Mare (2013) - collected short stories
#''Scrisori de la Polul Est'', Editura Agol, Bucharest (2015) - essays
#''Republica'', Editura Polirom, Iași (2016) - novel
# ''Istoria lacunelor. Despre manuscrise pierdute'', Editura Polirom, Iași (2017) - essay
#''Avalon. Istoria emigranților fericiți'', Editura Polirom, Iași (2018) - novel

===Mathematics===
# {{cite journal|last=Suceavă|first=Bogdan|title=The Chen invariants of warped products of hyperbolic planes and their applications to immersibility problems|url=http://www.tulips.tsukuba.ac.jp/limedio/dlam/M54/M549172/8.pdf|archive-url=https://web.archive.org/web/20140222043648/http://www.tulips.tsukuba.ac.jp/limedio/dlam/M54/M549172/8.pdf|dead-url=yes|archive-date=2014-02-22|journal=Tsukuba Journal of Mathematics|volume=25|number=2|pages=311–320|year=2001}}
# {{cite book|last = Suceavă|first=Bogdan|title=New Riemannian and Kählerian Curvature Invariants and Strongly Minimal Submanifolds|url=https://books.google.com/books?id=rVAlnQEACAAJ|year=2002|publisher=Michigan State University. Department of Mathematics}}
# {{cite journal|url=http://forumgeom.fau.edu/FG2006volume6/FG200621index.html|title=The Feuerbach point and Euler lines|last1 = Yiu|first1 = Paul|last2 = Suceavă|first2=Bogdan|journal=Forum Geometricorum|volume=6|pages=191–197|year=2006}}
# {{cite journal|url=https://eudml.org/doc/227783|last1 = Boskoff|first1 = Wladimir|last2 = Suceavă|first2=Bogdan|title=A projective characterization of cyclicity|journal=Beiträge zur Algebra und Geometrie|volume = 49|number = 1|pages=195–203|year=2008}}
# {{cite journal|url=http://www.mathematica-journal.com/2010/09/tzitzeica-curves-and-surfaces/|journal=Mathematica Journal|volume=12|title=Tzitzeica Curves and Surfaces|year=2010|last = Suceavă|first=Bogdan}}, with A.F. Agnew, A. Bobe, W.G. Boskoff.
# {{cite journal|last = Suceavă|first=Bogdan|title=Distances generated by Barbilian's metrization procedure by oscillation of sub logarithmic functions|url=http://math.uh.edu/~hjm/Vol37-1.html | journal=Houston Journal of Mathematics|volume=37|pages=147–159|year=2011}}
# {{cite journal|last = Suceavă|first=Bogdan|title = New Curvature Inequalities for Hypersurfaces in the Euclidean Ambient Space|journal=Taiwanese Journal of Mathematics|volume = 17|number=3|pages=885–895|year=2013|url=http://journal.taiwanmathsoc.org.tw/index.php/TJM/article/view/2504}}, with C.T.R. Conley, R. Etnyre, B. Gardener, L. H. Odom
# {{cite journal|last = Suceavă|first=Bogdan|title = A Medieval Mystery: Nicole Oresme’s Concept of ''Curvitas''=Notices of the American Mathematical Society|volume = 62|number=9|pages=1030–1034|year=2015|url=http://www.ams.org/notices/201509/rnoti-p1030.pdf}}, with Isabel M. Serrano

==References==
{{Reflist|30em}}

{{Authority control}}

{{DEFAULTSORT:Suceava, Bogdan}}
[[Category:People from Curtea de Argeș]]
[[Category:People from Bucharest]]
[[Category:People from Târgoviște]]
[[Category:People from Fullerton, California]]
[[Category:People from Michigan]]
[[Category:Romanian mathematicians]]
[[Category:Romanian novelists]]
[[Category:Romanian male novelists]]
[[Category:Romanian essayists]]
[[Category:Romanian male short story writers]]
[[Category:Romanian short story writers]]
[[Category:Romanian science fiction writers]]
[[Category:Magic realism writers]]
[[Category:20th-century mathematicians]]
[[Category:21st-century mathematicians]]
[[Category:Geometers]]
[[Category:University of Bucharest alumni]]
[[Category:University of Bucharest faculty]]
[[Category:California State University, Fullerton people]]
[[Category:California State University, Fullerton faculty]]
[[Category:Michigan State University alumni]]
[[Category:1969 births]]
[[Category:Living people]]
[[Category:Male essayists]]
[[Category:20th-century short story writers]]
[[Category:21st-century short story writers]]
[[Category:20th-century essayists]]
[[Category:21st-century essayists]]
[[Category:20th-century Romanian male writers]]
[[Category:21st-century male writers]]</text>
      <sha1>6aim2gaj0cm1mb3ho9kxjqef2oma7jv</sha1>
    </revision>
  </page>
  <page>
    <title>Bohr compactification</title>
    <ns>0</ns>
    <id>634780</id>
    <revision>
      <id>862708363</id>
      <parentid>855112471</parentid>
      <timestamp>2018-10-06T05:12:09Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Removing category Eponymous scientific concepts per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2018 September 22]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3156">In [[mathematics]], the  '''Bohr compactification''' of a [[topological group]] ''G'' is a [[compact Hausdorff space|compact Hausdorff]] topological group ''H'' that may be [[canonical form|canonically]] associated to ''G''. Its importance lies in the reduction of the theory of [[uniformly almost periodic function]]s on ''G'' to the theory of [[continuous function]]s on ''H''. The concept is named after [[Harald Bohr]] who pioneered the study of [[almost periodic function]]s, on the [[real line]].

==Definitions and basic properties==
Given a [[topological group]] ''G'', the '''Bohr compactification''' of ''G''  is a compact ''Hausdorff'' topological group '''Bohr'''(''G'') and a continuous homomorphism 

:'''b''': ''G'' → '''Bohr'''(''G'')

which is [[universal property|universal]] with respect to homomorphisms into compact Hausdorff groups;  this means that if ''K'' is another compact Hausdorff topological group and

:''f'': ''G'' → ''K''

is a continuous homomorphism, then there is a unique continuous homomorphism

:'''Bohr'''(''f''): '''Bohr'''(''G'') → ''K''

such that ''f'' = '''Bohr'''(''f'') ∘ '''b'''.

'''Theorem'''.  The Bohr compactification exists{{cn|date=March 2015}} and is unique up to isomorphism.

We will denote the Bohr compactification of ''G'' by '''Bohr'''(''G'') and the canonical map by

:&lt;math&gt; \mathbf{b}: G \rightarrow \mathbf{Bohr}(G). &lt;/math&gt;

The correspondence ''G'' ↦ '''Bohr'''(''G'') defines a covariant functor on the category of topological groups and continuous homomorphisms.

The Bohr compactification is intimately connected to the finite-dimensional [[unitary representation]] theory of a topological group.  The [[kernel (algebra)|kernel]] of '''b''' consists exactly of those elements of ''G'' which cannot be separated from the identity of ''G'' by finite-dimensional ''unitary'' representations.

The Bohr compactification also reduces many problems in the theory of [[almost periodic function]]s on topological groups to that of functions on compact groups.

A bounded continuous  complex-valued function ''f'' on a topological group ''G'' is '''uniformly almost periodic''' if and only if the set of right translates &lt;sub&gt;''g''&lt;/sub&gt;''f'' where

:&lt;math&gt; [{}_g f ] (x)  = f(g^{-1} \cdot x) &lt;/math&gt;

is relatively compact in the uniform topology as ''g'' varies through ''G''.

'''Theorem'''. A bounded continuous  complex-valued function ''f'' on ''G'' is uniformly almost periodic if and only if there is a continuous function ''f''&lt;sub&gt;1&lt;/sub&gt; on '''Bohr'''(''G'') (which is uniquely determined) such that

:&lt;math&gt; f = f_1 \circ \mathbf{b}. &lt;/math&gt;

==Maximally almost periodic groups==
Topological groups for which the Bohr compactification mapping is injective are called ''maximally almost periodic'' (or MAP groups).  In the case ''G'' is a locally compact connected group, MAP groups are completely characterized: They are precisely products of compact groups with vector groups
of finite dimension.

==References==
*{{Springer|id=B/b016780}}

{{DEFAULTSORT:Bohr Compactification}}
[[Category:Topological groups]]
[[Category:Harmonic analysis]]
[[Category:Compactification]]</text>
      <sha1>sbrjhr0yyu6sqvcn0sfyyy1bgwqhmi7</sha1>
    </revision>
  </page>
  <page>
    <title>Branch-decomposition</title>
    <ns>0</ns>
    <id>16823137</id>
    <revision>
      <id>841572927</id>
      <parentid>826695221</parentid>
      <timestamp>2018-05-16T17:24:24Z</timestamp>
      <contributor>
        <username>Nemo bis</username>
        <id>2584239</id>
      </contributor>
      <comment>Added free to read link in citations with [[WP:OABOT|OAbot]] #oabot</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="20511">[[File:Branch-decomposition.svg|thumb|upright=1.35|Branch decomposition of a [[grid graph]], showing an e-separation. The separation, the decomposition, and the graph all have width three.]]
In [[graph theory]], a '''branch-decomposition''' of an [[undirected graph]] ''G'' is a [[hierarchical clustering]] of the edges of ''G'', represented by an [[unrooted binary tree]] ''T'' with the edges of ''G'' as its leaves. Removing any edge from ''T'' partitions the edges of ''G'' into two subgraphs, and the width of the decomposition is the maximum number of shared vertices of any pair of subgraphs formed in this way. 
The '''branchwidth''' of ''G'' is the minimum width of any branch-decomposition of ''G''.

Branchwidth is closely related to [[tree decomposition|tree-width]]: for all graphs, both of these numbers are within a constant factor of each other, and both quantities may be characterized by [[forbidden graph characterization|forbidden minors]]. And as with treewidth, many graph optimization problems may be solved efficiently for graphs of small branchwidth. However, unlike treewidth, the branchwidth of [[planar graphs]] may be computed exactly, in [[polynomial time]]. Branch-decompositions and branchwidth may also be generalized from graphs to [[matroid]]s.

==Definitions==
An [[unrooted binary tree]] is a connected undirected graph with no cycles in which each non-leaf node has exactly three neighbors. A branch-decomposition may be represented by an unrooted binary tree ''T'', together with a bijection between the leaves of ''T'' and the edges of the given graph ''G''&amp;nbsp;=&amp;nbsp;(''V'',''E'').
If ''e'' is any edge of the tree ''T'', then removing ''e'' from ''T'' partitions it into two subtrees ''T''&lt;sub&gt;1&lt;/sub&gt; and ''T''&lt;sub&gt;2&lt;/sub&gt;. This partition of ''T'' into subtrees induces a partition of the edges associated with the leaves of ''T'' into two subgraphs ''G''&lt;sub&gt;1&lt;/sub&gt; and ''G''&lt;sub&gt;2&lt;/sub&gt; of ''G''. This partition of ''G'' into two subgraphs is called an '''e-separation'''.

The width of an e-separation is the number of vertices of ''G'' that are incident both to an edge of ''E''&lt;sub&gt;1&lt;/sub&gt; and to an edge of ''E''&lt;sub&gt;2&lt;/sub&gt;; that is, it is the number of vertices that are shared by the two subgraphs ''G''&lt;sub&gt;1&lt;/sub&gt; and ''G''&lt;sub&gt;2&lt;/sub&gt;. The width of the branch-decomposition is the maximum width of any of its e-separations. The branchwidth of ''G'' is the minimum width of a branch-decomposition of ''G''.

==Relation to treewidth==
Branch-decompositions of graphs are closely related to [[tree decomposition]]s, and branch-width is closely related to [[tree decomposition|tree-width]]: the two quantities are always within a constant factor of each other. In particular, in the paper in which they introduced branch-width, [[Neil Robertson (mathematician)|Neil Robertson]] and [[Paul Seymour (mathematician)|Paul Seymour]]&lt;ref&gt;{{harvnb|Robertson|Seymour|1991}}, Theorem 5.1, p. 168.&lt;/ref&gt; showed that for a graph ''G''
with tree-width ''k'' and branchwidth {{nowrap|''b'' &gt; 1,}} 
:&lt;math&gt;b -1 \le k \le \left\lfloor\frac{3}{2}b\right\rfloor -1.&lt;/math&gt;

==Carving width==
Carving width is a concept defined similarly to branch width, except with edges replaced by vertices and vice versa. A carving decomposition is an unrooted binary tree with each leaf representing a vertex in the original graph, and the width of a cut is the number (or total weight in a weighted graph) of edges that are incident to a vertex in both subtrees.

Branch width algorithms typically work by reducing to an equivalent carving width problem. In particular, the carving width of the [[medial graph]] of a planar graph is exactly twice the branch width of the original graph.&lt;ref name="st94"&gt;{{harvtxt|Seymour|Thomas|1994}}.&lt;/ref&gt;

==Algorithms and complexity==
It is [[NP-complete]] to determine whether a graph ''G'' has a branch-decomposition of width at most ''k'', when ''G'' and ''k'' are both considered as inputs to the problem.&lt;ref name="st94"/&gt; However, the graphs with branchwidth at most ''k'' form a [[Minor (graph theory)|minor-closed family of graphs]],&lt;ref&gt;{{harvtxt|Robertson|Seymour|1991}}, Theorem 4.1, p. 164.&lt;/ref&gt; from which it follows that computing the branchwidth is [[Parameterized complexity|fixed-parameter tractable]]: there is an algorithm for computing optimal branch-decompositions whose running time, on graphs of branchwidth ''k'' for any fixed constant ''k'', is linear in the size of the input graph.&lt;ref&gt;{{harvtxt|Bodlaender|Thilikos|1997}}. {{harvtxt|Fomin|Mazoit|Todinca|2009}} describe an algorithm with improved dependence on ''k'', (2{{radic|3}})&lt;sup&gt;''k''&lt;/sup&gt;, at the expense of an increase in the dependence on the number of vertices from linear to quadratic.&lt;/ref&gt;

For [[planar graph]]s, the branchwidth can be computed exactly in polynomial time. This in contrast to treewidth for which the complexity on planar graphs is a well known open problem.&lt;ref&gt;{{citation|title=	Encyclopedia of Algorithms|editor-first=Ming-Yang|editor-last=Kao|publisher=Springer|year=2008|isbn=9780387307701|contribution=Treewidth of graphs|page=969|url=https://books.google.com/books?id=i3S9_GnHZwYC&amp;pg=PA969|quote=Another long-standing open problem is whether there is a polynomial-time algorithm to compute the treewidth of planar graphs.}}&lt;/ref&gt; The original algorithm for planar branchwidth, by [[Paul Seymour (mathematician)|Paul Seymour]] and [[Robin Thomas (mathematician)|Robin Thomas]], took time O(''n''&lt;sup&gt;2&lt;/sup&gt;) on graphs with ''n'' vertices, and their algorithm for constructing a branch decomposition of this width took time O(''n''&lt;sup&gt;4&lt;/sup&gt;).&lt;ref name="st94"/&gt; This was later sped up to O(''n''&lt;sup&gt;3&lt;/sup&gt;).{{sfnp|Gu|Tamaki|2008}}

As with treewidth, branchwidth can be used as the basis of [[dynamic programming]] algorithms for many NP-hard optimization problems, using an amount of time that is exponential in the width of the input graph or matroid.&lt;ref&gt;{{harvtxt|Hicks|2000}}; {{harvtxt|Hliněný|2003}}.&lt;/ref&gt; For instance, {{harvtxt|Cook|Seymour|2003}} apply branchwidth-based dynamic programming to a problem of merging multiple partial solutions to the [[travelling salesman problem]] into a single global solution, by forming a sparse graph from the union of the partial solutions, using a [[spectral clustering]] heuristic to find a good branch-decomposition of this graph, and applying dynamic programming to the decomposition. {{harvtxt|Fomin|Thilikos|2006}} argue that branchwidth works better than treewidth in the development of fixed-parameter-tractable algorithms on planar graphs, for multiple reasons: branchwidth may be more tightly bounded by a function of the parameter of interest than the bounds on treewidth, it can be computed exactly in polynomial time rather than merely approximated, and the algorithm for computing it has no large hidden constants.

==Generalization to matroids==
It is also possible to define a notion of branch-decomposition for [[matroid]]s that generalizes branch-decompositions of graphs.&lt;ref&gt;{{harvnb|Robertson|Seymour|1991}}. Section 12, "Tangles and Matroids", pp. 188–190.&lt;/ref&gt; A branch-decomposition of a matroid is a hierarchical clustering of the matroid elements, represented as an unrooted binary tree with the elements of the matroid at its leaves. An e-separation may be defined in the same way as for graphs, and results in a partition of the set ''M'' of matroid elements into two subsets ''A'' and ''B''. If ρ denotes the [[Matroid rank|rank function]] of the matroid, then the width of an e-separation is defined as {{nowrap|ρ(''A'') + ρ(''B'') &amp;minus; ρ(''M'') + 1}}, and the width of the decomposition and the branchwidth of the matroid are defined analogously. The branchwidth of a graph and the branchwidth of the corresponding [[graphic matroid]] may differ: for instance, the three-edge [[path graph]] and the three-edge [[star (graph theory)|star]] have different branchwidths, 2 and 1 respectively, but they both induce the same graphic matroid with branchwidth 1.&lt;ref name="mt07"/&gt; However, for graphs that are not trees, the branchwidth of the graph is equal to the branchwidth of its associated graphic matroid.&lt;ref&gt;{{harvtxt|Mazoit|Thomassé|2007}}; {{harvtxt|Hicks|McMurray|2007}}.&lt;/ref&gt; The branchwidth of a matroid is equal to the branchwidth of its [[dual matroid]], and in particular this implies that the branchwidth of any planar graph that is not a tree is equal to that of its dual.&lt;ref name="mt07"&gt;{{harvtxt|Mazoit|Thomassé|2007}}.&lt;/ref&gt;

Branchwidth is an important component of attempts to extend the theory of [[graph minor]]s to [[matroid minor]]s: although [[treewidth]] can also be generalized to matroids,&lt;ref&gt;{{harvtxt|Hliněný|Whittle|2006}}.&lt;/ref&gt; and plays a bigger role than branchwidth in the theory of graph minors, branchwidth has more convenient properties in the matroid setting.&lt;ref&gt;{{harvtxt|Geelen|Gerards|Whittle|2006}}.&lt;/ref&gt; Robertson and Seymour conjectured that the matroids representable over any particular [[finite field]] are [[well-quasi-ordering|well-quasi-ordered]], analogously to the [[Robertson–Seymour theorem]] for graphs, but so far this has been proven only for the matroids of bounded branchwidth.&lt;ref&gt;{{harvtxt|Geelen|Gerards|Whittle|2002}}; {{harvtxt|Geelen|Gerards|Whittle|2006}}.&lt;/ref&gt; Additionally, if a minor-closed family of matroids representable over a finite field does not include the graphic matroids of all planar graphs, then there is a constant bound on the branchwidth of the matroids in the family, generalizing similar results for minor-closed graph families.&lt;ref&gt;{{harvtxt|Geelen|Gerards|Whittle|2006}}; {{harvtxt|Geelen|Gerards|Whittle|2007}}.&lt;/ref&gt;

For any fixed constant ''k'', the matroids with branchwidth at most ''k'' can be recognized in [[polynomial time]] by an algorithm that has access to the matroid via an [[matroid oracle|independence oracle]].&lt;ref&gt;{{harvtxt|Oum|Seymour|2007}}.&lt;/ref&gt;

==Forbidden minors==
[[File:Branchwidth 3-forbidden minors.svg|thumb|The four [[forbidden minor]]s for graphs of branchwidth three.]]
By the [[Robertson–Seymour theorem]], the graphs of branchwidth ''k'' can be characterized by a finite set of [[forbidden minor]]s. The graphs of branchwidth 0 are the [[matching (graph theory)|matchings]]; the minimal forbidden minors are a two-edge [[path graph]] and a triangle graph  (or the two-edge cycle, if multigraphs rather than simple graphs are considered).&lt;ref name="rs91-4.2"/&gt; The graphs of branchwidth 1 are the graphs in which each [[connected component (graph theory)|connected component]] is a [[star (graph theory)|star]]; the minimal forbidden minors for branchwidth 1 are the triangle graph (or the two-edge cycle, if multigraphs rather than simple graphs are considered) and the three-edge path graph.&lt;ref name="rs91-4.2"/&gt; The graphs of branchwidth 2 are the graphs in which each [[biconnected component]] is a [[series-parallel graph]]; the only minimal forbidden minor is the [[complete graph]] ''K''&lt;sub&gt;4&lt;/sub&gt; on four vertices.&lt;ref name="rs91-4.2"&gt;{{harvtxt|Robertson|Seymour|1991}}, Theorem 4.2, p. 165.&lt;/ref&gt; A graph has branchwidth three if and only if it has treewidth three and does not have the [[hypercube graph|cube graph]] as a minor; therefore, the four minimal forbidden minors are three of the four forbidden minors for treewidth three (the graph of the [[octahedron]], the complete graph ''K''&lt;sub&gt;5&lt;/sub&gt;, and the [[Wagner graph]]) together with the cube graph.&lt;ref&gt;{{harvtxt|Bodlaender|Thilikos|1999}}. The fourth forbidden minor for treewidth three, the pentagonal prism, has the cube graph as a minor, so it is not minimal for branchwidth three.&lt;/ref&gt;

Forbidden minors have also been studied for matroid branchwidth, despite the lack of a full analogue to the Robertson–Seymour theorem in this case. A matroid has branchwidth one if and only if every element is either a loop or a coloop, so the unique minimal forbidden minor is the [[uniform matroid]] U(2,3), the graphic matroid of the triangle graph. A matroid has branchwidth two if and only if it is the graphic matroid of a graph of branchwidth two, so its minimal forbidden minors are the graphic matroid of ''K''&lt;sub&gt;4&lt;/sub&gt; and the non-graphic matroid U(2,4). The matroids of branchwidth three are not well-quasi-ordered without the additional assumption of representability over a finite field, but nevertheless the matroids with any finite bound on their branchwidth have finitely many minimal forbidden minors, all of which have a number of elements that is at most exponential in the branchwidth.&lt;ref&gt;{{harvtxt|Hall|Oxley|Semple|Whittle|2002}}; {{harvtxt|Geelen|Gerards|Robertson|Whittle|2003}}.&lt;/ref&gt;

==Notes==
{{Commons category|Tree decomposition}}
{{reflist|2}}

==References==
{{refbegin|2}}
*{{citation
 | last1 = Bodlaender | first1 = Hans L. | author1-link = Hans L. Bodlaender
 | last2 = Thilikos | first2 = Dimitrios M.
 | contribution = Constructive linear time algorithms for branchwidth
 | doi = 10.1007/3-540-63165-8_217
 | pages = 627–637
 | publisher = Springer-Verlag
 | series = Lecture Notes in Computer Science
 | title = Proc. 24th International Colloquium on Automata, Languages and Programming (ICALP '97)
 | volume = 1256
 | year = 1997| hdl = 2117/96447}}.
*{{citation
 | last1 = Bodlaender | first1 = Hans L. | author1-link = Hans L. Bodlaender
 | last2 = Thilikos | first2 = Dimitrios M.
 | doi = 10.1006/jagm.1999.1011
 | issue = 2
 | journal = Journal of Algorithms
 | pages = 167–194
 | title = Graphs with branchwidth at most three
 | volume = 32
 | year = 1999}}.
*{{citation
 | doi = 10.1287/ijoc.15.3.233.16078
 | last1 = Cook | first1 = William
 | last2 = Seymour | first2 = Paul D. | author2-link = Paul Seymour (mathematician)
 | issue = 3
 | journal = INFORMS Journal on Computing
 | pages = 233–248
 | title = Tour merging via branch-decomposition
 | url = http://www.cs.utk.edu/~langston/projects/papers/tmerge.pdf
 | volume = 15
 | year = 2003}}.
*{{citation
 | last1 = Fomin | first1 = Fedor V.
 | last2 = Thilikos | first2 = Dimitrios M.
 | doi = 10.1137/S0097539702419649
 | journal = SIAM Journal on Computing
 | page = 281
 | title = Dominating sets in planar graphs: branch-width and exponential speed-up
 | volume = 36
 | year = 2006
 | issue = 2}}.
*{{citation
 | last1 = Fomin | first1 = Fedor V.
 | last2 = Mazoit | first2 = Frédéric
 | last3 = Todinca | first3 = Ioan
 | doi = 10.1016/j.dam.2008.08.009
 | issue = 12
 | journal = Discrete Applied Mathematics
 | pages = 2726–2736
 | title = Computing branchwidth via efficient triangulations and blocks
 | url = http://hal.archives-ouvertes.fr/hal-00390623/
 | volume = 157
 | year = 2009}}.
*{{citation
 | last1 = Geelen | first1 = Jim | author1-link = Jim Geelen
 | last2 = Gerards | first2 = Bert
 | last3 = Robertson | first3 = Neil | author3-link = Neil Robertson (mathematician)
 | last4 = Whittle | first4 = Geoff
 | doi = 10.1016/S0095-8956(02)00046-1
 | issue = 2
 | journal = Journal of Combinatorial Theory, Series B
 | pages = 261–265
 | title = On the excluded minors for the matroids of branch-width ''k''
 | volume = 88
 | year = 2003}}.
*{{citation
 | last1 = Geelen | first1 = Jim | author1-link = Jim Geelen
 | last2 = Gerards | first2 = Bert
 | last3 = Whittle | first3 = Geoff
 | doi = 10.1006/jctb.2001.2082
 | issue = 2
 | journal = Journal of Combinatorial Theory, Series B
 | pages = 270–290
 | title = Branch-width and well-quasi-ordering in matroids and graphs
 | volume = 84
 | year = 2002}}.
*{{citation
 | last1 = Geelen | first1 = Jim | author1-link = Jim Geelen
 | last2 = Gerards | first2 = Bert
 | last3 = Whittle | first3 = Geoff
 | contribution = Towards a structure theory for matrices and matroids
 | pages = 827–842
 | title = Proc. [[International Congress of Mathematicians]]
 | contribution-url = http://www.icm2006.org/proceedings/Vol_III/contents/ICM_Vol_3_41.pdf
 | volume = III
 | year = 2006}}.
*{{citation
 |last1=Geelen 
 |first1=Jim 
 |author1-link=Jim Geelen 
 |last2=Gerards 
 |first2=Bert 
 |last3=Whittle 
 |first3=Geoff 
 |doi=10.1016/j.jctb.2007.02.005 
 |issue=6 
 |journal=Journal of Combinatorial Theory, Series B 
 |pages=971–998 
 |title=Excluding a planar graph from GF(''q'')-representable matroids 
 |url=http://www.math.uwaterloo.ca/~jfgeelen/publications/grid.pdf 
 |volume=97 
 |year=2007 
 |deadurl=yes 
 |archiveurl=https://web.archive.org/web/20100924110915/http://www.math.uwaterloo.ca/~jfgeelen/publications/grid.pdf 
 |archivedate=2010-09-24 
 |df= 
}}.
*{{citation
 | last1 = Gu | first1 = Qian-Ping
 | last2 = Tamaki | first2 = Hisao
 | date = July 2008
 | doi = 10.1145/1367064.1367070
 | issue = 3
 | journal = ACM Transactions on Algorithms
 | pages = 30:1–30:13
 | title = Optimal branch-decomposition of planar graphs in O(''n''&lt;sup&gt;3&lt;/sup&gt;) time
 | volume = 4}}.
*{{citation
 | last1 = Hall | first1 = Rhiannon
 | last2 = Oxley | first2 = James | author2-link = James Oxley
 | last3 = Semple | first3 = Charles
 | last4 = Whittle | first4 = Geoff
 | doi = 10.1006/jctb.2002.2120
 | issue = 1
 | journal = Journal of Combinatorial Theory, Series B
 | pages = 148–171
 | title = On matroids of branch-width three
 | volume = 86
 | year = 2002}}.
*{{citation
 | last = Hicks | first = Illya V.
 | publisher = Rice University
 | series = Ph.D. thesis
 | title = Branch Decompositions and their Applications
 | url = http://www.caam.rice.edu/caam/trs/2000/TR00-17.ps
 | year = 2000}}.
*{{citation
 | last1 = Hicks | first1 = Illya V.
 | last2 = McMurray | first2 = Nolan B., Jr.
 | doi = 10.1016/j.jctb.2006.12.007
 | issue = 5
 | journal = Journal of Combinatorial Theory, Series B
 | pages = 681–692
 | title = The branchwidth of graphs and their cycle matroids
 | volume = 97
 | year = 2007}}.
*{{citation
 | last = Hliněný | first = Petr
 | contribution = On matroid properties definable in the MSO logic
 | doi = 10.1007/978-3-540-45138-9\_41
 | pages = 470–479
 | publisher = Springer-Verlag
 | series = Lecture Notes in Computer Science
 | title = Proc. 28th International Symposium on Mathematical Foundations of Computer Science (MFCS '03)
 | volume = 2747
 | year = 2003}}
*{{citation
 | last1 = Hliněný | first1 = Petr
 | last2 = Whittle | first2 = Geoff
 | doi = 10.1016/j.ejc.2006.06.005
 | issue = 7
 | journal = European Journal of Combinatorics
 | pages = 1117–1128
 | title = Matroid tree-width
 | url = http://www.fi.muni.cz/~hlineny/Research/papers/matr-tw-final.pdf
 | volume = 27
 | year = 2006}}. 
**Addendum and corrigendum: {{citation 
 | last1 = Hliněný | first1 = Petr
 | last2 = Whittle | first2 = Geoff
 | title = Addendum to matroid tree-width 
 | journal = European Journal of Combinatorics
 | volume=30
 | issue=4
 | pages=1036–1044 
 | year = 2009 
 | doi=10.1016/j.ejc.2008.09.028}}.
*{{citation
 | last1 = Mazoit | first1 = Frédéric
 | last2 = Thomassé | first2 = Stéphan
 | contribution = Branchwidth of graphic matroids
 | editor1-last = Hilton | editor1-first = Anthony
 | editor2-last = Talbot | editor2-first = John
 | page = 275
 | publisher = Cambridge University Press
 | series = London Mathematical Society Lecture Note Series
 | title = Surveys in Combinatorics 2007
 | url = http://hal.archives-ouvertes.fr/docs/00/04/09/28/PDF/Branchwidth.pdf
 | volume = 346
 | year = 2007}}.
*{{citation
 | last1 = Oum | first1 = Sang-il
 | last2 = Seymour | first2 = Paul | author2-link = Paul Seymour (mathematician)
 | doi = 10.1016/j.jctb.2006.06.006
 | issue = 3
 | journal = [[Journal of Combinatorial Theory]]
 | mr = 2305892
 | pages = 385–393
 | series = Series B
 | title = Testing branch-width
 | volume = 97
 | year = 2007}}.
*{{citation
 | last1 = Robertson | first1 = Neil | author1-link = Neil Robertson (mathematician)
 | last2 = Seymour | first2 = Paul D. | author2-link = Paul Seymour (mathematician)
 | doi = 10.1016/0095-8956(91)90061-N
 | issue = 2
 | journal = Journal of Combinatorial Theory
 | pages = 153–190
 | title = Graph minors. X. Obstructions to tree-decomposition
 | volume = 52
 | year = 1991}}.
*{{citation
 | last1 = Seymour | first1 = Paul D. | author1-link = Paul Seymour (mathematician)
 | last2 = Thomas | first2 = Robin
 | doi = 10.1007/BF01215352
 | issue = 2
 | journal = Combinatorica
 | pages = 217–241
 | title = Call routing and the ratcatcher
 | volume = 14
 | year = 1994}}.
{{refend}}

[[Category:Trees (graph theory)]]
[[Category:Graph minor theory]]
[[Category:Graph invariants]]
[[Category:Matroid theory]]</text>
      <sha1>4hjdjza58yqzgywtimh50tgyo1jsqf6</sha1>
    </revision>
  </page>
  <page>
    <title>Consumer network</title>
    <ns>0</ns>
    <id>39474050</id>
    <revision>
      <id>718930733</id>
      <parentid>683669891</parentid>
      <timestamp>2016-05-06T14:22:49Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Economics]]; added [[Category:Consumer]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3830">{{Orphan|date=June 2013}}

The notion of consumer networks expresses the idea that people’s embeddedness in [[social network]]s affects their behavior as [[consumer]]s. Interactions within consumer networks such as [[Communication|information exchange]] and [[imitation]] can affect demand and market outcomes in ways not considered in the neoclassical theory of [[consumer choice]].

==Economics==
Economic research on the topic is not ample. In attempts to incorporate consumer networks into standard microeconomic models, some interesting implications have been found concerning market structure, market dynamics and the firm’s profit maximizating decision.

It has been shown that under certain assumptions the structure of the consumer network can affect market structure.&lt;ref name="itoh et al"&gt;{{citation|first1=Satoshi|last1=Itoh|first2=Yasuyuki|last2=Murakami|first3=Takashi|last3=Iba|title=Consumer Network and Market Dynamics|year=2006|url=http://web.sfc.keio.ac.jp/~iba/papers/2006CIEF06-Itoh.pdf}}.&lt;/ref&gt; In certain scenarios, where consumers have a higher inclination to compare their habitually consumed product to that of their acquaintances, the equilibrium market structure can switch from [[oligopoly]] to [[monopoly]].

In an other model, which incorporates [[Network science#Watts-Strogatz Small World model|small world]] consumer networks into the [[Profit maximization|profit function of the firm]],&lt;ref name="jun et al"&gt;{{citation|first1=Tackseung|last1=Jun|first2=Jeong-Yoo|last2=Kim|first3=Beom Jun|last3=Kim|first4=M.Y.|last4=Choi|title=Consumer referral in a small world network|journal=[[Social Networks (journal)|Social Networks]]|volume=28|year=2006|pages=232–246|url=http://web.khu.ac.kr/~jyookim/paper/swn.pdf}}.&lt;/ref&gt; it has been demonstrated that the [[Dense graph|density]] of the network significantly affects the optimal price the firm should charge and the optimal referral fee (paid to consumers who can convince an other one to buy). On the other hand, the size of the network does not have an important effect on these.

A 2007 laboratory [[Experimental economics|experiment]]&lt;ref name="huck et al"&gt;{{citation|first1=Steffen|last1=Huck|first2=Gabriele K.|last2=Lünser|first3=Jean-Robert|last3=Tyran|title=Consumer networks and firm reputation: A first experimental investigation|year=2007|url=http://discovery.ucl.ac.uk/14364/1/14364.pdf}}.&lt;/ref&gt; found that increased density of consumer networks can reduce market inefficiencies caused by moral hazard. The ability of consumers to exchange information with more neighbors increases firms’ incentives to build a reputation through selling high quality products. Even a low level of density was found to isolated consumers who can rely only on their own experience.

==Marketing==
Exploiting consumer networks for marketing purposes, through techniques such as [[viral marketing]], [[word-of-mouth marketing]], or network marketing, is increasingly experimented with by marketers, to the extent that "some developments in customer networking are ahead of empirical research, and a few seem ahead even of accepted theory".&lt;ref&gt;Van den Bulte, Christophe (2010). "Opportunities and Challenges in Studying Customer Networks" P. 7 in Stefan H.K. Wuyts et al. (eds.) ''The Connected Customer: The Changing Nature of Consumer and Business Markets'', Routledge.&lt;/ref&gt; These might often be more effective than more traditional forms of [[advertising]]. A key task of such forms of marketing is to target the people who are opinion leaders regarding consumption, having many contacts and positive reputation. They are, in network science language, the hubs of consumer networks.

==See also==
* [[Viral marketing]]
* [[Word-of-mouth marketing]]

==Notes and references==
{{Reflist}}

[[Category:Consumer|Network]]
[[Category:Network theory]]</text>
      <sha1>qievnv3wt1cv11ma6z2ssuats0xneq8</sha1>
    </revision>
  </page>
  <page>
    <title>Cypher Query Language</title>
    <ns>0</ns>
    <id>41583056</id>
    <revision>
      <id>865644466</id>
      <parentid>865644454</parentid>
      <timestamp>2018-10-25T07:26:35Z</timestamp>
      <contributor>
        <username>RHcosm</username>
        <id>34625434</id>
      </contributor>
      <comment>Reverted 1 edit by [[Special:Contributions/49.204.217.188|49.204.217.188]] ([[User talk:49.204.217.188|talk]]) to last revision by The RedBurn. ([[WP:TW|Twinkle]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5388">'''Cypher''' is a [[Declarative programming|declarative]] graph query language that allows for expressive and efficient querying and updating of a property graph. Cypher is a relatively simple but still very powerful language. Very complicated database queries can easily be expressed through Cypher. This allows users to focus on their domain instead of getting lost in database access.&lt;ref name="cypher-introduction"&gt;{{cite web|url=http://neo4j.com/docs/developer-manual/current/cypher/#cypher-intro|title=Cypher Introduction|last=|first=|date=|website=|publisher=Neo Technology|accessdate=January 31, 2017}}&lt;/ref&gt;

Cypher was largely an invention of Andrés Taylor while working for Neo4j, Inc.(formerly Neo Technology) in 2011.&lt;ref name="cypher-paper"&gt;{{cite web|url=https://hal.archives-ouvertes.fr/hal-01803524/file/paper.pdf|title=Cypher: An Evolving Query Language for Property Graphs|last=|first=|date=|website=|publisher=Proceedings of the 2018 International Conference on Management of Data. ACM|accessdate=June 27, 2018}}&lt;/ref&gt; Cypher was originally intended to be used with the graph database [[Neo4j]], but was opened up through the openCypher project in October 2015.&lt;ref&gt;{{Cite news|url=https://neo4j.com/blog/open-cypher-sql-for-graphs/|title=Meet openCypher: The SQL for Graphs - Neo4j Graph Database|date=2015-10-21|newspaper=Neo4j Graph Database|language=en-US|access-date=2017-01-31}}&lt;/ref&gt;

== Graph model ==
Cypher is based on the Property Graph Model, which in addition to the standard [[Graph (discrete mathematics)|graph]] elements of nodes and edges (which are called ''relationships'' in Cypher) adds labels and properties as concepts. Nodes may have zero or more labels, while each relationship has exactly one relationship type.&lt;ref&gt;{{Cite web|url=https://github.com/opencypher/openCypher/blob/master/docs/property-graph-model.adoc|title=Property Graph Model|last=|first=|date=|website=GitHub|publisher=|language=en|access-date=2017-01-31}}&lt;/ref&gt; Nodes and relationships also have zero or more properties, where a property is a key-value binding of a string key and some value from the Cypher type system.

=== Type system ===
The Cypher type system includes nodes, relationships, paths, maps, lists, integers, floating-point numbers, booleans, and strings.&lt;ref&gt;{{Cite web|url=https://github.com/opencypher/openCypher/blob/master/cip/1.accepted/CIP2015-09-16-public-type-system-type-annotation.adoc|title=Cypher Type System|last=|first=|date=|website=GitHub|publisher=|language=en|access-date=2017-01-31}}&lt;/ref&gt;

== Syntax ==
Cypher contains a variety of clauses. Among the most common are: MATCH and WHERE. These functions are slightly different than in [[SQL]]. MATCH is used for describing the structure of the pattern searched for, primarily based on relationships. WHERE is used to add additional constraints to patterns.&lt;ref name="cypher-queries"&gt;{{cite web|url=http://neo4j.com/docs/developer-manual/current/cypher/clauses/match/|title=Neo4j 3.1.1 manual - MATCH clause|last=|first=|date=|website=|publisher=Neo Technology|accessdate=January 31, 2017}}&lt;/ref&gt; For example, the below query will return all movies where an actor named 'Nicole Kidman' has acted, and that were produced before a certain year (sent by parameter):

&lt;source lang="cypher"&gt;
MATCH (nicole:Actor {name: 'Nicole Kidman'})-[:ACTED_IN]-&gt;(movie:Movie)
WHERE movie.year &lt; $yearParameter
RETURN movie&lt;/source&gt;

Cypher additionally contains clauses for writing, updating, and deleting data. CREATE and DELETE are used to create and delete nodes and relationships. SET and REMOVE are used to set values to properties and add labels on nodes. Nodes can only be deleted when they have no other relationships still existing. For example:&lt;ref name='cypher-queries' /&gt;

&lt;source lang="cypher"&gt;
MATCH (start:Content)-[:RELATED_CONTENT]-&gt;(content:Content)
WHERE content.source = 'user'
OPTIONAL MATCH (content)-[r]-()
DELETE r, content&lt;/source&gt;

== Standardization ==
With the openCypher project, an effort was started to standardize Cypher as the query language for graph processing. One part of this process is the First openCypher Implementers Meeting (oCIM), which was first announced in December 2016.&lt;ref&gt;{{Cite web|url=http://www.opencypher.org/2017/01/17/ocim-announcement/|title=openCypher Implementers Meeting · openCypher.org|last=|first=|date=|website=www.opencypher.org|publisher=|language=en|access-date=2017-01-31}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=https://groups.google.com/forum/#!topic/opencypher/vCNM4UOXcTY|title=oCIM announcement on openCypher Google Groups|last=|first=|date=|website=groups.google.com|publisher=|access-date=2017-01-31}}&lt;/ref&gt;

== See also ==
* [[SPARQL]], another declarative query language for querying graph data

== References ==
{{reflist}}

== External links ==
* [https://www.opencypher.org/ OpenCypher], an initiative by Neo4j, Inc. and others to extend the use of Cypher to other graph databases
* [https://neo4j.com/blog/open-cypher-sql-for-graphs/ Meet openCypher: The SQL for Graphs]

{{Query languages}}

[[Category:Programming languages]]
[[Category:Declarative programming]]
[[Category:Graph databases]]
[[Category:Structured storage]]
[[Category:Free database management systems]]
[[Category:Free software programmed in Java (programming language)]]
[[Category:2007 software]]
[[Category:NoSQL]]
[[Category:Domain-specific programming languages]]</text>
      <sha1>tukuh5owrjmh2kdev2nfftwmzyeo6m6</sha1>
    </revision>
  </page>
  <page>
    <title>DEAP (software)</title>
    <ns>0</ns>
    <id>39447416</id>
    <revision>
      <id>868541925</id>
      <parentid>864128205</parentid>
      <timestamp>2018-11-12T21:26:29Z</timestamp>
      <contributor>
        <username>Haxxorz596</username>
        <id>20937393</id>
      </contributor>
      <comment>typos</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4468">{{Refimprove|date=September 2015}}
{{Infobox software
| name                   = Distributed Evolutionary Algorithms in Python
| title                  = DEAP
| logo                   = &lt;!-- [[File: ]] --&gt;
| author                 = François-Michel De Rainville, Félix-Antoine Fortin, Marc-André Gardner, Marc Parizeau, Christian Gagné
| developer              = François-Michel De Rainville, Félix-Antoine Fortin, Marc-André Gardner
| released               = {{Start date|2009}}
| discontinued           =
| latest release version = 1.2.2
| latest release date    = {{Start date and age|2017|11|12}}
| programming language   = [[Python (programming language)|Python]]
| operating system       = [[Cross-platform]]
| language               =
| status                 = Active
| genre                  = [[Evolutionary computation]] framework
| license                = [[GNU Lesser General Public License|LGPL]]
| website                = {{URL|https://github.com/deap}}
}}

'''Distributed Evolutionary Algorithms in Python''' (DEAP) is an [[evolutionary computation]] [[Software framework|framework]] for rapid [[Software prototyping|prototyping]] and testing of ideas.&lt;ref&gt;{{cite journal|last=Fortin|first=Félix-Antoine|author2=F.-M. De Rainville |author3=M-A. Gardner |author4=C. Gagné|author5=M. Parizeau|title=DEAP: Evolutionary Algorithms Made Easy|journal=Journal of Machine Learning Research|year=2012|volume=13|pages=2171–2175|url=http://jmlr.org/papers/v13/fortin12a.html}}&lt;/ref&gt;&lt;ref&gt;
{{cite journal|last=De Rainville|first=François-Michel|author2=F.-A Fortin |author3=M-A. Gardner |author4=C. Gagné |author5=M. Parizeau|title=DEAP: Enabling Nimber Evolutionss|journal=SIGEvolution|year=2014|volume=6|issue=2|pages=17–26|url=http://www.sigevolution.org/issues/pdf/SIGEVOlution0602.pdf}}&lt;/ref&gt;&lt;ref&gt;
{{cite journal|last=De Rainville|first=François-Michel|author2=F.-A Fortin |author3=M-A. Gardner |author4=C. Gagné |author5=M. Parizeau|title=DEAP: A Python Framework for Evolutionary Algorithms|journal=In Companion Proceedings of the Genetic and Evolutionary Computation Conference|year=2012|url=http://vision.gel.ulaval.ca/~cgagne/pubs/deap-gecco-2012.pdf}}
&lt;/ref&gt; It incorporates the data structures and tools required to implement most common evolutionary computation techniques such as [[genetic algorithm]], [[genetic programming]], [[evolution strategies]], [[particle swarm optimization]], [[differential evolution]], traffic flow&lt;ref&gt;{{Cite web|url=http://sior.ub.edu/jspui/cris/socialimpact/socialimpact00441|title=Creation of one algorithm to manage traffic systems|last=|first=|date=|website=Social Impact Open Repository|archive-url=http://sior.ub.edu/jspui/cris/socialimpact/socialimpact00441|archive-date=2017|dead-url=|access-date=2017-09-05}}&lt;/ref&gt; and [[estimation of distribution algorithm]]. It is developed at [[Université Laval]] since 2009.

== Example ==
The following code gives a quick overview how the Onemax problem optimization with genetic algorithm can be implemented with DEAP.
&lt;source lang="python" enclose="div"&gt;
import array, random
from deap import creator, base, tools, algorithms

creator.create("FitnessMax", base.Fitness, weights=(1.0,))
creator.create("Individual", array.array, typecode='b', fitness=creator.FitnessMax)

toolbox = base.Toolbox()

toolbox.register("attr_bool", random.randint, 0, 1)
toolbox.register("individual", tools.initRepeat, creator.Individual, toolbox.attr_bool, 100)
toolbox.register("population", tools.initRepeat, list, toolbox.individual)

evalOneMax = lambda individual: (sum(individual),)

toolbox.register("evaluate", evalOneMax)
toolbox.register("mate", tools.cxTwoPoint)
toolbox.register("mutate", tools.mutFlipBit, indpb=0.05)
toolbox.register("select", tools.selTournament, tournsize=3)

population = toolbox.population(n=300)

NGEN=40
for gen in range(NGEN):
    offspring = algorithms.varAnd(population, toolbox, cxpb=0.5, mutpb=0.1)
    fits = toolbox.map(toolbox.evaluate, offspring)
    for fit, ind in zip(fits, offspring):
        ind.fitness.values = fit
    population = offspring
&lt;/source&gt;

== See also ==
* [[Python SCOOP (software)]]
* {{Portal-inline|Free software}}

== References == 
{{Reflist}}

== External links ==
* {{Official website|https://github.com/deap}}
* {{GitHub|deap/deap}}

[[Category:Articles with example Python code]]
[[Category:Evolutionary computation]]
[[Category:Free science software]]
[[Category:Python scientific libraries]]</text>
      <sha1>rmavgia8tv8uot9au8y38fzz9ux9n0g</sha1>
    </revision>
  </page>
  <page>
    <title>DataStax</title>
    <ns>0</ns>
    <id>37892981</id>
    <revision>
      <id>865619528</id>
      <parentid>865617234</parentid>
      <timestamp>2018-10-25T02:20:43Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor/>
      <comment>Dating maintenance tags: {{Clarify}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7400">{{Infobox company
 | name             = DataStax
 | logo             = DataStax Logo.png
 | type             = [[Privately held company|Private]]
 | location_city    = [[Santa Clara, CA]], USA
 | location_country = United States
 | foundation       = April 2010 &lt;br/&gt; [[Austin, TX]], USA
 | founder          = {{plainlist|
* Jonathan Ellis (CTO)
* Matt Pfeil
}}
 | key_people       = Billy Bosworth (CEO)&lt;br /&gt;Jonathan Ellis (Co-Founder &amp; CTO)&lt;br /&gt;Steve Rowland (President)&lt;br /&gt;Robert O'Donovan (CFO)
 | industry         = [[Database Technologies]]
 | genre            = [[Multi-model database|Multi-Model DBMS]]
 | num_employees    = 450+ (Nov 2017)&lt;ref name="forbes"&gt;{{cite news |last1=Cohan |first1=Peter |title=DataStax Partners With Oracle In $46B Database Market |url=https://www.forbes.com/sites/petercohan/2017/11/24/datastax-partners-with-oracle-in-46-billion-database-market/#4834af397f44 |publisher=Forbes.com |date=24 Nov 2017}}&lt;/ref&gt;
 | homepage         = [http://www.DataStax.com DataStax.com]
}}

'''DataStax, Inc.''' is a data management company based in [[Santa Clara, California]].&lt;ref name="wsj"&gt;{{cite news |last1=Gage |first1=Deborah |title=DataStax Raises $106 Million in New Pre-IPO Round, Chips Away at Oracle |url=https://blogs.wsj.com/venturecapital/2014/09/04/datastax-raises-106-million-in-new-pre-ipo-round-chips-away-at-oracle/ |publisher=Wall Street Journal |date=4 September 2014}}&lt;/ref&gt; The company was built on{{clarify|date=October 2018}} [[Apache Cassandra]]. As of October 2017, the company has roughly 400 customers distributed in over 50 countries.&lt;ref name="diginomica"&gt;{{cite news |last1=Banks |first1=Martin |title=DataStax adds Oracle to provide practical collaboration |url=https://diginomica.com/2017/10/06/datastax-adds-oracle-provide-practical-collaboration/ |publisher=Diginomica.com |date=6 October 2017}}&lt;/ref&gt;&lt;ref name="fortune"&gt;{{cite news |last1=Clancy |first1=Heather |title=DataStax just scored a big partnership with HP. Here’s why. |url=http://fortune.com/2015/04/14/datastax-hp-sales-partnership/ |publisher=Fortune |date=14 April 2015}}&lt;/ref&gt;

==History==
DataStax was built on the open source [[NoSQL]] database [[Apache Cassandra]]. Cassandra was initially developed internally at [[Facebook]] to handle [[Big data|large data sets]] across multiple servers,&lt;ref name="wired"/&gt; and was released as an Apache open source project in 2008.&lt;ref&gt;{{cite news |last1=Jackson |first1=Joab |title=Apache Cassandra Ready for the Enterprise |url=https://www.cio.com/article/2403250/data-management/apache-cassandra-ready-for-the-enterprise.html |publisher=CIO |date=18 October 2011}}&lt;/ref&gt; In 2010, Jonathan Ellis and Matt Pfeil left [[Rackspace]], where they had worked with Cassandra, to launch Riptano in Austin, Texas.&lt;ref name="wired"&gt;{{cite web|url=https://www.wired.com/2014/08/datastax/|title=OUT IN THE OPEN: THE ABANDONED FACEBOOK TECH THAT NOW HELPS POWER APPLE|date=4 August 2014|publisher=Wired|accessdate=18 September 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite news |last1=Clark |first1=Don |title=Start-Up Riptano Predicts Success With Cassandra Database |url=https://blogs.wsj.com/venturecapital/2010/10/26/start-up-predicts-success-with-cassandra-database/ |publisher=Wall Street Journal |date=26 October 2010}}&lt;/ref&gt; Ellis and Pfeil later renamed the company DataStax, and moved its headquarters to Santa Clara, California.&lt;ref name="wsj"/&gt;&lt;ref&gt;{{cite news |last1=Harris |first1=Derrick |title=NoSQL is growing up, and DataStax just raised $106M to prove it |url=https://gigaom.com/2014/09/04/nosql-startup-datastax-raises-106m-as-it-grows-into-being-an-enterprise-software-company/ |publisher=gigaom.com |date=4 September 2014}}&lt;/ref&gt;

The company went on to create its own proprietary version of Cassandra, a NoSQL database called DataStax Enterprise (DSE).&lt;ref name="wired"/&gt; Version 1.0, released in October 2011, was the first commercial distribution of the Cassandra database, designed to provide real-time application performance and heavy analytics on the same physical infrastructure.&lt;ref name="forbes"/&gt;&lt;ref&gt;{{cite news |last1=Harris |first1=Derrick |title=DataStax gets $11M, fuses NoSQL and Hadoop |url=https://gigaom.com/2011/09/20/datastax-gets-11m-fuses-nosql-and-hadoop/ |publisher=gigaom.com |date=20 September 2011}}&lt;/ref&gt; It grew to include advanced security controls, graph database models, operational analytics and advanced search capabilities.&lt;ref name="computerworld"&gt;{{cite news |last1=Carey |first1=Scott |title=How DataStax wants its NoSQL platform to drive the 'right now economy' |url=https://www.computerworlduk.com/data/how-datastax-wants-its-nosql-platform-drive-right-now-economy-3664812/ |publisher=Computerworld UK |date=4 October 2017}}&lt;/ref&gt;

In September 2014, DataStax raised $106 million in a Series E funding round, raising the total investment in the company to $190 million.&lt;ref name="wsj"/&gt;

In April 2016, the company announced the release of DataStax Enterprise Graph, adding graph data model functionality to DSE.&lt;ref name="techcrunch"&gt;{{cite news |last1=Miller |first1=Ron |title=DataStax adds graph databases to enterprise Cassandra product set |url=https://techcrunch.com/2016/04/12/datastax-adds-graph-databases-to-enterprise-cassandra-product-set/ |publisher=techcrunch.com |date=12 April 2016}}&lt;/ref&gt;

In March 2017, DataStax announced the release of its DSE platform 5.1, which included improved search capabilities, improved security control, improvements to its Graph data management and improvements to operational analytics performance. DataStax also announced a shift in strategy, with an added focus on customer experience applications. Rather than a new set of technologies, the company started to offer advice on best practice to users of its core DSE platform.&lt;ref&gt;{{cite web|url=http://diginomica.com/2017/03/15/datastax-ceo-launches-new-cx-strategy-focusing-shifting-tech-business/|title=DataStax CEO launches new CX strategy – focus shifting from tech to business|date=15 March 2017|publisher=diginomica|accessdate=12 September 2017}}&lt;/ref&gt;&lt;ref name="computerworld"/&gt; 

In April 2018, DataStax released DSE 6, with the new version focused on businesses using a hybrid cloud computing model, with all the benefits of a distributed cloud database on any public cloud or on-premise, twice the responsiveness and ability to handle twice the throughput.&lt;ref name="sdtimes"&gt;{{cite news |last1=Sargent |first1=Jenna |title=DataStax Enterprise 6 released with double the Apache Cassandra performance |url=https://sdtimes.com/data/datastax-enterprise-6-released-with-double-the-apache-cassandra-performance/ |publisher=San Diego Times |date=19 April 2018}}&lt;/ref&gt;&lt;ref&gt;{{cite news |last1=Whiting |first1=Rick |title=DataStax Pushes The Cloud Database Performance Boundary With New Release |url=https://www.crn.com/news/applications-os/300102226/datastax-pushes-the-cloud-database-performance-boundary-with-new-release.htm?itc=refresh |publisher=crn.com |date=17 April 2018}}&lt;/ref&gt;

==See also==
* [[Apache Cassandra]]
* [[Oracle Corporation]]
* [[MongoDB]]
* [[Couchbase]]

==References==
{{Reflist}}

==External links==
*{{Official website}}

[[Category:Companies based in Santa Clara, California]]
[[Category:Column-oriented DBMS software for Linux]]
[[Category:Graph databases]]
[[Category:NoSQL]]
[[Category:American companies established in 2010]]
[[Category:Cloud computing providers]]</text>
      <sha1>15rrrrr0ocdtk4r3lssbzswzw3lvmaa</sha1>
    </revision>
  </page>
  <page>
    <title>Depolarization ratio</title>
    <ns>0</ns>
    <id>1157370</id>
    <revision>
      <id>834497817</id>
      <parentid>834320873</parentid>
      <timestamp>2018-04-06T01:27:16Z</timestamp>
      <contributor>
        <username>Dirac66</username>
        <id>2038644</id>
      </contributor>
      <comment>Added molecules of lower symmetry than spherical tops</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4486">{{Other uses|Depolarization (disambiguation)}}

[[File:Cyclohexane Polarized Raman.JPG|right]]
In [[Raman spectroscopy]], the '''depolarization ratio''' is the [[Intensity (physics)|intensity]] ratio between the perpendicular component and the parallel component of Raman scattered light.&lt;ref name=Allemand&gt;Charly D. Allemand, "Depolarization Ratio Measurements in Raman Spectrometry", ''Applied Spectroscopy'' '''24'''(3), 1970, pp.&amp;nbsp;348–353&lt;/ref&gt;

Early work in this field was carried out by [[George Placzek]], who developed the theoretical treatment of bond polarizability&lt;ref name=Long&gt;{{cite journal|author=D. A. Long|title=Intensities in Raman Spectra. I. A Bond Polarizability Theory|journal=Proceedings of the Royal Society of London. Series A, Mathematical and Physical Sciences|volume=217|issue=1129|date=Apr 8, 1953|pages=203–221|jstor=99022|doi=10.1098/rspa.1953.0057|bibcode = 1953RSPSA.217..203L }}&lt;/ref&gt;

The Raman scattered light is emitted by the stimulation of the [[electric field]] of the incident light. Therefore, the direction of the vibration of the electric field, or [[Polarization (waves)|polarization]] direction, of the scattered light might be expected to be the same as that of the incident light. In reality, however, some fraction of the Raman scattered light has a polarization direction that is perpendicular to that of the incident light. This component is called the '''perpendicular component'''. Naturally, the component of the Raman scattered light whose polarization direction is parallel to that of the incident light is called the '''parallel component''', and the Raman scattered light consists of the parallel component and the perpendicular component. 

The ratio of the peak intensity of the parallel and perpendicular component is known as the depolarization ratio (ρ), defined in [[#equation-1|equation 1]].&lt;ref name=PE&gt;PerkinElmer Technical Note, "Raman Polarization Accessory for the RamanStation 400", http://las.perkinelmer.com/content/relatedmaterials/productnotes/PRD_RamanPolarizationAccessory4RamanStation400.pdf&lt;/ref&gt;

{{Equation|1=\rho = \frac{ I_{perpendicular} }{ I_{parallel} } \quad\textrm{or}\quad \frac{ I_{depolarized} }{ I_{polarized} } |id=1}}

For example, a spectral band with a peak of intensity 10 units when the polarizers are parallel, and intensity 1 unit when the polarizers are perpendicular, would have a depolarization ratio of 1/10 = 0.1, which corresponds to a highly polarized band. 

The value of the depolarization ratio of a Raman band depends on the [[symmetry]] of the molecule and the normal vibrational mode, in other words, the [[point group]] of the molecule and its [[irreducible representation]] to which the [[normal mode]] belongs. Under Placzek’s polarizability approximation, it is known that the depolarization ratio of a totally symmetric vibrational mode is less than 0.75, and that of the other modes equals 0.75. A Raman band whose depolarization ratio is less than 0.75 is called a '''polarized band''', and a band with a depolarization ratio equal to or greater than 0.75 is called a '''depolarized band'''.&lt;ref name=Banwell&gt;{{cite book |last1=Banwell |first1=Colin N. |last2=McCash |first2=Elaine M. |date=1994 |title=Fundamentals of Molecular Spectroscopy |edition=4th |publisher=McGraw–Hill |page=117–8 |isbn=0-07-707976-0 }}&lt;/ref&gt;&lt;ref&gt;{{cite book |last1=Atkins |first1=Peter |last2=de Paula |first2=Julio |date=2006 |title=Physical Chemistry |edition=8th |publisher=W.H.Freeman |page=464 |isbn=0-7167-8759-8 |author-link=Peter Atkins }}&lt;/ref&gt;

For a [[spherical top]] molecule in which all three axes are equivalent, symmetric vibrations have Raman spectral bands which are completely polarized (ρ = 0). An example is the symmetric stretching or "breathing" mode of [[methane]] (CH&lt;sub&gt;4&lt;/sub&gt;) in which all 4 C–H bonds vibrate in phase. However for the asymmetric mode in which one C–H bond stretches while the other three contract, the Raman scattered radiation is depolarized.&lt;ref name=Banwell/&gt;

For molecules of lower symmetry ([[Rotational_spectroscopy#Classification_of_molecular_rotors|symmetric tops or asymmetric tops]]), a vibration with the full [[molecular symmetry|symmetry of the molecule]] leads to a polarized or partially polarized Raman band (ρ &lt; 0.75), while a less symmetric vibration yields a depolarized band (ρ ≥ 0.75).&lt;ref name=Banwell/&gt;

==References==
&lt;references/&gt;

[[Category:Spectroscopy]]
[[Category:Ratios]]

{{CMP-stub}}</text>
      <sha1>1ljcqfonrtp45h9ntfkp7rj0gluxq1j</sha1>
    </revision>
  </page>
  <page>
    <title>Distribution function (measure theory)</title>
    <ns>0</ns>
    <id>58117270</id>
    <revision>
      <id>855217091</id>
      <parentid>855217015</parentid>
      <timestamp>2018-08-16T18:50:39Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2772">In mathematics, a '''distribution function''' is a [[real function]] in [[measure theory]]. From every [[measure (mathematics)|measure]] on the algebra of [[Borel sets]] of real numbers, a distribution function can be constructed, which reflects some of the properties of this measure. Distribution functions (in the sense of measure theory) are a generalization of [[Cumulative distribution function|distribution functions (in the sense of probability theory)]].

== Definition ==
Let &lt;math&gt; \mu &lt;/math&gt; be a [[Measure (mathematics)|measure]] on the [[real number]]s, equipped with the [[Borel sigma algebra|Borel &lt;math&gt;\sigma&lt;/math&gt;-algebra]]. Then the function
:&lt;math&gt; F_\mu \colon \R \to \R \cup \{ +\infty, - \infty \} &lt;/math&gt;

defined by
:&lt;math&gt; F_\mu(t)= \begin{cases} \mu((0,t]) &amp; \text{if } t\geq 0 \\ -\mu((t,0]) &amp; \text{if } t &lt; 0\end{cases}&lt;/math&gt;

is called the ([[right continuous]]) distribution function of the measure &lt;math&gt; \mu &lt;/math&gt;.&lt;ref name="Kallenberg164" /&gt;

== Example ==
As the measure, choose the [[Lebesgue measure]] &lt;math&gt; \lambda &lt;/math&gt;. Then by Definition of &lt;math&gt; \lambda &lt;/math&gt;
:&lt;math&gt; \lambda((0,t])=t-0=t \text{ and } -\lambda((t,0])=-(0-t)=t&lt;/math&gt;

Therefore, the distribution function of the Lebesgue measure is
:&lt;math&gt; F_\lambda(t)=t&lt;/math&gt;

for all &lt;math&gt; t \in \R &lt;/math&gt;

== Comments ==
The definition of the distribution function (in the sense of measure theory) differs slightly from the definition of the [[Cumulative distribution function|distribution function (in the sense of probability theory)]]. The latter has the boundary conditions
:&lt;math&gt; \lim_{t \to - \infty} F(t)=0 \text{ and } \lim_{t \to  \infty} F(t)=1. &lt;/math&gt;

This makes this distribution function well defined for all probability measures.
However, in the case of an unbounded measure &lt;math&gt; \mu &lt;/math&gt;, defining the distribution function as in probability theory by
:&lt;math&gt;F_\mu(t)= \mu((- \infty, t]) &lt;/math&gt;
can be without meaning. This is since many measures take on the value &lt;math&gt;  +\infty &lt;/math&gt; on all intervals &lt;math&gt; (- \infty,t] &lt;/math&gt;, making their distribution function a constant function with value infinity. This is for example the case for the Lebesgue measure. To avoid this pathological case, the distribution function is defined to be zero at the origin. This makes sure that even for unbounded measures, the distribution function is well defined and finite close to the origin.

== References == 
&lt;references&gt;
&lt;ref name="Kallenberg164" &gt;{{cite book |last1=Kallenberg |first1=Olav |author-link1=Olav Kallenberg |year=2017  |title=Random Measures, Theory and Applications|location= Switzerland |publisher=Springer |doi= 10.1007/978-3-319-41598-7|isbn=978-3-319-41596-3|pages=164}} &lt;/ref&gt;
&lt;/references&gt;

[[Category:Measure theory]]</text>
      <sha1>3y3mlq4md1y7ltvcljkho0l0rqeiawy</sha1>
    </revision>
  </page>
  <page>
    <title>EFF DES cracker</title>
    <ns>0</ns>
    <id>839358</id>
    <revision>
      <id>865172166</id>
      <parentid>843139581</parentid>
      <timestamp>2018-10-22T07:03:18Z</timestamp>
      <contributor>
        <ip>2A0B:7080:10:0:0:0:1:E187</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8365">{{Use mdy dates|date=June 2015}}
[[Image:Board300.jpg|thumbnail|right|260px|The [[Electronic Frontier Foundation|EFF]]'s US$250,000 DES cracking machine contained 1,856 custom chips and could [[brute force attack|brute force]] a DES [[key (cryptography)|key]] in a matter of days — the photo shows a two-sided DES Cracker circuit board fitted with 64 Deep Crack chips]][[Image:Chip300.jpg|thumbnail|right|The EFF's DES cracker "Deep Crack" custom microchip]]
In [[cryptography]], the '''EFF DES cracker''' (nicknamed "'''Deep Crack'''") is a machine built by the [[Electronic Frontier Foundation]] (EFF) in 1998, to perform a [[brute force attack|brute force]] search of the [[Data Encryption Standard]] (DES) cipher's [[Key space (cryptography)|key space]] – that is, to decrypt an encrypted message by trying every possible key. The aim in doing this was to prove that the [[key size]] of DES was not sufficient to be secure.

==Background==
DES uses a 56-bit [[Key size|key]], meaning that there are 2&lt;sup&gt;56&lt;/sup&gt; possible keys under which a message can be encrypted. This is exactly 72,057,594,037,927,936, or approximately 72 [[Orders of magnitude (numbers)#1015|quadrillion]] possible keys.  One of the major criticisms of DES, when proposed in 1975, was that the key size was too short.  [[Martin Hellman]] and [[Whitfield Diffie]] of [[Stanford University]] estimated that a machine fast enough to test that many keys in a day would have cost about $20 million in 1976, an affordable sum to national intelligence agencies such as the US [[National Security Agency]].&lt;ref&gt;{{cite web|url=http://www.toad.com/des-stanford-meeting.html|title=DES (Data Encryption Standard) Review at Stanford University - Recording and Transcript|year=1976|deadurl=no|archiveurl=https://web.archive.org/web/20120503083539/http://www.toad.com/des-stanford-meeting.html|archivedate=May 3, 2012|df=mdy-all}}&lt;/ref&gt; Subsequent advances in the price/performance of chips kept reducing that cost until twenty years later it became affordable to even a small nonprofit organization such as the EFF.&lt;ref&gt;{{cite web |url=https://w2.eff.org/Privacy/Crypto/Crypto_misc/DESCracker |title=Archived copy |accessdate=2013-10-09 |deadurl=yes |archiveurl=https://web.archive.org/web/20130622022127/https://w2.eff.org/Privacy/Crypto/Crypto_misc/DESCracker/ |archivedate=June 22, 2013 |df=mdy-all }}&lt;/ref&gt;

==The DES challenges==
DES was a federal standard, and the [[US government]] encouraged the use of DES for all non-classified data. [[RSA Security]] wished to demonstrate that DES's key length was not enough to ensure security, so they set up the [[DES Challenges]] in 1997, offering a monetary prize. The first DES Challenge was solved in 96 days by the [[DESCHALL Project]] led by Rocke Verser in [[Loveland, Colorado]]. RSA Security set up DES Challenge II-1, which was solved by [[distributed.net]] in 39 days in January and February 1998.&lt;ref&gt;{{cite web|url=http://lists.distributed.net/pipermail/announce/1998/000037.html|title=The secret message is...|author=David C. McNett|date=February 24, 1998|publisher=distributed.net|accessdate=February 27, 2014|deadurl=no|archiveurl=https://web.archive.org/web/20160304000105/http://lists.distributed.net/pipermail/announce/1998/000037.html|archivedate=March 4, 2016|df=mdy-all}}&lt;/ref&gt;

In 1998, the EFF built Deep Crack (named in reference to IBM's [[Deep Blue (chess computer)|Deep Blue]] chess computer) for less than $250,000.&lt;ref&gt;{{cite web|url=http://w2.eff.org/Privacy/Crypto/Crypto_misc/DESCracker/HTML/19980716_eff_des_faq.html|quote=On Wednesday, July 17, 1998 the EFF DES Cracker, which was built for less than $250,000, easily won RSA Laboratory's "DES Challenge II" contest and a $10,000 cash prize.|title=DES Cracker Project|publisher=EFF|accessdate=July 8, 2007|deadurl=yes|archiveurl=https://web.archive.org/web/20170507231657/https://w2.eff.org/Privacy/Crypto/Crypto_misc/DESCracker/HTML/19980716_eff_des_faq.html|archivedate=May 7, 2017|df=mdy-all}}&lt;/ref&gt; In response to DES Challenge II-2, on July 15, 1998, Deep Crack decrypted a DES-encrypted message after only 56 hours of work, winning $10,000. The brute force attack showed that cracking DES was actually a very practical proposition. Most governments and large corporations could reasonably build a machine like Deep Crack.

Six months later, in response to RSA Security's DES Challenge III, and in collaboration with distributed.net, the EFF used Deep Crack to decrypt another DES-encrypted message, winning another $10,000. This time, the operation took less than a day – 22 hours and 15 minutes. The decryption was completed on January 19, 1999. In October of that year, DES was reaffirmed as a federal standard, but this time the standard recommended [[Triple DES]].

The small key-space of DES, and relatively high computational costs of Triple DES resulted in its replacement by [[Advanced Encryption Standard|AES]] as a Federal standard, effective May 26, 2002.

==Technology==
Deep Crack was designed by [[Cryptography Research|Cryptography Research, Inc.]], Advanced Wireless Technologies, and the [[Electronic Frontier Foundation|EFF]]. The principal designer was [[Paul Kocher]], president of Cryptography Research. Advanced Wireless Technologies built 1856 custom [[Application-specific integrated circuit|ASIC]] DES chips (called ''Deep Crack'' or ''AWT-4500''), housed on 29 circuit boards of 64 chips each. The boards were then fitted in six cabinets and mounted in a [[Sun-4|Sun-4/470]] chassis.&lt;ref&gt;{{cite book|title=Cracking DES - Secrets of Encryption Research, Wiretap Politics &amp; Chip Design|author=Electronic Frontier Foundation|isbn=1-56592-520-3|publisher=Oreilly &amp; Associates Inc|year=1998|url=http://cryptome.org/jya/cracking-des/cracking-des.htm|deadurl=no|archiveurl=https://web.archive.org/web/20131017055750/http://cryptome.org/jya/cracking-des/cracking-des.htm|archivedate=October 17, 2013|df=mdy-all}}&lt;/ref&gt;
[[File:Paul_kocher_deepcrack.jpg|thumb|334x334px|Paul Kocher of the EFF posing in front of Deepcrack]]
The search was coordinated by a single PC which assigned ranges of keys to the chips. The entire machine was capable of testing over 90 billion keys per second. It would take about 9 days to test every possible key at that rate. On average, the correct key would be found in half that time.

In 2006, another [[custom hardware attack]] machine was designed based on [[FPGA]]s. COPACOBANA (COst-optimized PArallel COdeBreaker) is able to crack DES at considerably lower cost.&lt;ref&gt;{{cite web|url=http://www.sciengines.com/copacobana/faq.html|title=COPACOBANA - Special-Purpose Hardware for Code-Breaking|author=|date=|website=www.sciengines.com|accessdate=April 26, 2018|deadurl=no|archiveurl=https://web.archive.org/web/20160724092435/http://www.sciengines.com/copacobana/faq.html|archivedate=July 24, 2016|df=mdy-all}}&lt;/ref&gt; This advantage is mainly due to progress in [[integrated circuit]] technology.  

In July 2012, security researchers David Hulton and [[Moxie Marlinspike]] unveiled a cloud computing tool for breaking the [[MS-CHAPv2]] protocol by recovering the protocol's DES encryption keys by brute force.  This tool effectively allows members of the general public to recover a DES key from a known plaintext-ciphertext pair in about 24 hours.&lt;ref&gt;{{cite web |url=https://www.cloudcracker.com/blog/2012/07/29/cracking-ms-chap-v2/ |title=Archived copy |accessdate=2016-03-16 |deadurl=yes |archiveurl=https://web.archive.org/web/20160316174007/https://www.cloudcracker.com/blog/2012/07/29/cracking-ms-chap-v2/ |archivedate=March 16, 2016 |df=mdy-all }}&lt;/ref&gt;

==References==
{{reflist|30em}}

==External links==
{{Commons category|EFF DES cracker}}
* [https://web.archive.org/web/20170507231657/https://w2.eff.org/Privacy/Crypto/Crypto_misc/DESCracker/HTML/19980716_eff_des_faq.html The DES Cracker page on EFF's site]
* [http://www.cryptography.com/resources/whitepapers/DES-photos.html Photos of the machine on Cryptography Research's site]
* [http://www.sciengines.com/copacobana COPACOBANA, an off-the-shelf DES cracking device]
* [http://crack.sh/ A FPGA implementation using 48 Virtex-6 LX240Ts]

{{Cryptography navbox | block}}

[[Category:Cryptography]]
[[Category:Cryptographic hardware]]
[[Category:Cryptanalytic devices]]
[[Category:Data Encryption Standard]]
[[Category:One-of-a-kind computers]]</text>
      <sha1>iwb31ep1zschmzptxwmo9whsw12w1ro</sha1>
    </revision>
  </page>
  <page>
    <title>Exponential dichotomy</title>
    <ns>0</ns>
    <id>1844527</id>
    <revision>
      <id>786513781</id>
      <parentid>647612462</parentid>
      <timestamp>2017-06-19T23:11:35Z</timestamp>
      <contributor>
        <username>Magic links bot</username>
        <id>30707369</id>
      </contributor>
      <minor/>
      <comment>Replace [[Help:Magic links|magic links]] with templates per [[Special:Permalink/772743896#Future of magic links|local RfC]] and [[:mw:Requests for comment/Future of magic links|MediaWiki RfC]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2290">In the [[mathematics|mathematical]] theory of [[dynamical systems]], an '''exponential dichotomy''' is a property of an [[equilibrium point]] that extends the idea of [[hyperbolic equilibrium point|hyperbolicity]] to non-[[autonomous system (mathematics)|autonomous system]]s.

==Definition==
If

:&lt;math&gt;\dot{\mathbf{x}} = A(t)\mathbf{x}&lt;/math&gt;

is a [[linear system|linear]] non-autonomous dynamical system in '''R'''&lt;sup&gt;''n''&lt;/sup&gt; with [[fundamental solution matrix]] Φ(''t''), Φ(0) = ''I'', then the equilibrium point '''0''' is said to have an ''exponential dichotomy'' if there exists a (constant) [[matrix (mathematics)|matrix]] ''P'' such that ''P''&lt;sup&gt;2&lt;/sup&gt; = ''P'' and positive constants ''K'', ''L'', α, and β such that

:&lt;math&gt;|| \Phi(t) P \Phi^{-1}(s) || \le Ke^{-\alpha(t - s)}\mbox{ for }s \le t &lt; \infty&lt;/math&gt;

and

:&lt;math&gt;|| \Phi(t) (I - P) \Phi^{-1}(s) || \le Le^{-\beta(s - t)}\mbox{ for }s \ge t &gt; -\infty.&lt;/math&gt;

If furthermore, ''L'' = 1/''K'' and β = α, then '''0''' is said to have a ''uniform exponential dichotomy''.

The constants α and β allow us to define the ''spectral window'' of the equilibrium point, (&amp;minus;α,&amp;nbsp;β).

==Explanation==
The matrix ''P'' is a projection onto the stable subspace and ''I''&amp;nbsp;&amp;minus;&amp;nbsp;''P'' is a projection onto the unstable subspace.  What the exponential dichotomy says is that the norm of the projection onto the stable subspace of any orbit in the system [[exponential decay|decays exponentially]] as ''t''&amp;nbsp;→&amp;nbsp;∞ and the norm of the projection onto the unstable subspace of any orbit decays exponentially as ''t''&amp;nbsp;→&amp;nbsp;&amp;minus;∞, and furthermore that the stable and unstable subspaces are conjugate (because &lt;math&gt;\scriptstyle P \oplus (I - P) = \mathbb{R}^n&lt;/math&gt;).

An equilibrium point with an exponential dichotomy has many of the properties of a hyperbolic equilibrium point in [[autonomous system (mathematics)|autonomous system]]s.  In fact, it can be shown that a hyperbolic point has an exponential dichotomy.

==References==
&lt;references/&gt;
* Coppel, W. A. ''Dichotomies in stability theory'', Springer-Verlag (1978), {{ISBN|978-3-540-08536-2}} {{doi|10.1007/BFb0067780}}

{{DEFAULTSORT:Exponential Dichotomy}}
[[Category:Dynamical systems]]
[[Category:Dichotomies]]</text>
      <sha1>71fnv39xmqhvv372s98xh1osteu2glx</sha1>
    </revision>
  </page>
  <page>
    <title>Fourier optics</title>
    <ns>0</ns>
    <id>312008</id>
    <revision>
      <id>868856214</id>
      <parentid>862251762</parentid>
      <timestamp>2018-11-14T22:00:39Z</timestamp>
      <contributor>
        <username>Texvc2LaTeXBot</username>
        <id>33995001</id>
      </contributor>
      <minor/>
      <comment>Replacing deprecated latex syntax [[mw:Extension:Math/Roadmap]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="58545">{{See also|Huygens–Fresnel principle|geometrical optics}}

'''Fourier optics''' is the study of classical [[optics]] using [[Fourier transform]]s (FTs), in which the waveform being considered is regarded as made up of a combination, or ''[[Superposition principle|superposition]]'', of plane waves. It has some parallels to the [[Huygens–Fresnel principle]], in which the wavefront is regarded as being made up of a combination of spherical wavefronts whose sum is the wavefront being studied. A key difference is that Fourier optics considers the plane waves to be natural modes of the propagation medium, as opposed to Huygens–Fresnel, where the spherical waves originate in the physical medium. 

A curved phasefront may be synthesized from an infinite number of these "natural modes" i.e., from plane wave phasefronts oriented in different directions in space.  Far from its sources, an expanding spherical wave is locally tangent to a planar phase front (a single plane wave out of the infinite spectrum), which is transverse to the radial direction of propagation.  In this case, a [[Fraunhofer diffraction]] pattern is created, which emanates from a single spherical wave phase center.  In the near field, no single well-defined spherical wave phase center exists, so the wavefront isn't locally tangent to a spherical ball.  In this case, a [[Fresnel diffraction]] pattern would be created, which emanates from an ''extended'' source, consisting of a distribution of (physically identifiable) spherical wave sources in space.  In the near field, a full spectrum of plane waves is necessary to represent the Fresnel near-field wave, ''even locally''.  A "wide" [[wave]] moving forward (like an expanding ocean wave coming toward the shore) can be regarded as an infinite number of "[[Wave function|plane wave modes]]", all of which could (when they collide with something in the way) scatter independently of one other. These mathematical simplifications and calculations are the realm of [[Fourier analysis|Fourier analysis and synthesis]] &amp;ndash; together, they can describe what happens when light passes through various slits, lenses or mirrors curved one way or the other, or is fully or partially reflected.

Fourier optics forms much of the theory behind [[image processing|image processing techniques]], as well as finding applications where information needs to be extracted from optical sources such as in [[quantum optics]]. To put it in a slightly more complex way, similar to the concept of ''[[frequency]]'' and ''[[Time in physics|time]]'' used in traditional [[Fourier transform|Fourier transform theory]], Fourier optics makes use of the [[spatial frequency]] domain (''k&lt;sub&gt;x&lt;/sub&gt;'', ''k&lt;sub&gt;y&lt;/sub&gt;'') as the conjugate of the spatial (''x'', ''y'') domain. Terms and concepts such as transform theory, spectrum, bandwidth, window functions and sampling from one-dimensional [[signal processing]] are commonly used.

== Propagation of light in homogeneous, source-free media ==

Light can be described as a waveform propagating through free space (vacuum) or a material medium (such as air or glass). Mathematically, the (real valued) amplitude of one wave component is represented by a scalar wave function ''u'' that depends on both space and time:

:&lt;math&gt; u = u(\mathbf{r},t)&lt;/math&gt;

where

:&lt;math&gt; \mathbf{r} = (x,y,z) &lt;/math&gt;

represents position in three dimensional space, and ''t'' represents time.

===The wave equation===

Fourier optics begins with the homogeneous, scalar [[wave equation]] (valid in source-free regions):

:&lt;math&gt;
\left(\nabla^2-\frac{1}{c^2}\frac{\partial^2}{\partial{t}^2}\right)u(\mathbf{r},t)=0.
&lt;/math&gt;

where ''u''('''r''',''t'') is a [[real number|real valued]] Cartesian component of an electromagnetic wave propagating through free space.

=== Sinusoidal steady state ===

If light of a fixed [[frequency]]/[[wavelength]]/[[color]] (as from a laser) is assumed, then the time-[[harmonic]] form of the optical field is given as:

:&lt;math&gt;u(\mathbf{r},t) = \mathrm{Re} \left\{  \psi(\mathbf{r}) e^{j\omega t} \right\} &lt;/math&gt;.
where &lt;math&gt;j&lt;/math&gt; is the [[imaginary unit]],
:&lt;math&gt;\omega = 2\pi f &lt;/math&gt;
is the angular frequency (in radians per unit time) of the light waves, and
:&lt;math&gt;\psi(\mathbf{r}) = a(\mathbf{r}) e^{j \phi (\mathbf{r}) }  &lt;/math&gt;
is, in general, a [[complex number|complex]] quantity, with separate amplitude &lt;math&gt;a&lt;/math&gt; and phase &lt;math&gt;\phi&lt;/math&gt;.

=== The Helmholtz equation ===

Substituting this expression into the wave equation yields the time-independent form of the wave equation, also known as the [[Helmholtz equation]]:

:&lt;math&gt;\left(\nabla^2+ k^2 \right) \psi (\mathbf{r})=0&lt;/math&gt;

where 
:&lt;math&gt;k = { \omega \over c} = { 2 \pi \over \lambda }&lt;/math&gt;

is the wave number, ψ('''r''') is the time-independent, [[complex number|complex-valued]] component of the propagating wave. Note that the propagation constant, k, and the frequency, &lt;math&gt; \omega &lt;/math&gt;, are linearly related to one another, a typical characteristic of transverse electromagnetic (TEM) waves in homogeneous media.

===Solving the Helmholtz equation===

Solutions to the Helmholtz equation may readily be found in [[rectangular coordinates]] via the principle of [[separation of variables]] for [[partial differential equation]]s. This principle says that in separable [[orthogonal coordinates]], an ''elementary product solution'' to this wave equation may be constructed of the following form:

: &lt;math&gt;  \psi(x, y, z) = f_x(x) \times f_y(y) \times f_z(z)&lt;/math&gt;

i.e., as the product of a function of ''x'', times a function of ''y'', times a function of ''z''. If this ''elementary product solution'' is substituted into the wave equation (2.0), using the [[Laplace operator|scalar Laplacian]] in rectangular coordinates:

: &lt;math&gt; \nabla^2 \psi = \frac{\partial^2 \psi}{\partial x^2} + \frac{\partial^2 \psi}{\partial y^2} + \frac{\partial^2 \psi}{\partial z^2}  &lt;/math&gt;

then the following equation for the 3 individual functions is obtained

: &lt;math&gt;f''_x(x)f_y(y)f_z(z) + f_x(x)f''_y(y)f_z(z) + f_x(x)f_y(y)f''_z(z) + k^2f_x(x)f_y(y)f_z(z)=0  \,&lt;/math&gt;

which is readliy rearranged into the form:

: &lt;math&gt;   \frac{f''_x(x)}{f_x(x)}+ \frac{f''_y(y)}{f_y(y)} + \frac{f''_z(z)}{f_z(z)} + k^2=0 &lt;/math&gt;

It may now be argued that each of the quotients in the equation above must, of necessity, be constant. For, say the first quotient is not constant, and is a function of ''x''. None of the other terms in the equation has any dependence on the variable x. Therefore, the first term may not have any ''x''-dependence either; it must be constant. The constant is denoted as -''k''&lt;sub&gt;x&lt;/sub&gt;². Reasoning in a similar way for the ''y'' and ''z'' quotients, three ordinary differential equations are obtained for the ''f''&lt;sub&gt;x&lt;/sub&gt;, ''f''&lt;sub&gt;y&lt;/sub&gt; and ''f''&lt;sub&gt;z&lt;/sub&gt;, along with one ''separation condition'':

: &lt;math&gt;\frac{d^2}{dx^2}f_x(x) + k_x^2 f_x(x)=0&lt;/math&gt;

: &lt;math&gt;\frac{d^2}{dy^2}f_y(y) + k_y^2 f_y(y)=0&lt;/math&gt;

: &lt;math&gt;\frac{d^2}{dz^2}f_z(z) + k_z^2 f_z(z)=0&lt;/math&gt;

: &lt;math&gt;k_x^2+k_y^2+k_z^2= k^2&lt;/math&gt;

Each of these 3 differential equations has the same solution: sines, cosines or complex exponentials.  We'll go with the complex exponential for notational simplicity, compatibility with usual FT notation, and the fact that a two-sided integral of complex exponentials picks up both the sine and cosine contributions.  As a result, the elementary product solution for ''E''&lt;sub&gt;u&lt;/sub&gt; is:

: &lt;math&gt;\psi(x,y,z)=e^{j(k_x x + k_y y + k_z z)} &lt;/math&gt;
::::: &lt;math&gt; =e^{j(k_x x + k_y y)} e^{j k_z z}&lt;/math&gt;
::::: &lt;math&gt; =e^{j(k_x x + k_y y)} e^{\pm j z \sqrt{k^2-k_x^2-k_y^2} }&lt;/math&gt;

which represents a propagating or exponentially decaying uniform plane wave solution to the homogeneous wave equation. The - sign is used for a wave propagating/decaying in the +z direction and the + sign is used for a wave propagating/decaying in the -z direction (this follows the engineering time convention, which assumes an e&lt;sup&gt;jωt&lt;/sup&gt; time dependence). This field represents a propagating plane wave when the quantity under the radical is positive, and an exponentially decaying wave when it is negative (in passive media, the root with a non-positive imaginary part must always be chosen, to represent uniform propagation or decay, but not amplification).

Product solutions to the Helmholtz equation are also readily obtained in [[Cylindrical coordinate system|cylindrical]] and [[Spherical coordinate system|spherical coordinates]], yielding cylindrical and spherical harmonics (with the remaining separable coordinate systems being used much less frequently).

=== The complete solution: the superposition integral ===

A general solution to the homogeneous electromagnetic wave equation in rectangular coordinates may be formed as a weighted superposition of all possible elementary plane wave solutions as:

: &lt;math&gt;\psi(x,y,z)=\int_{-\infty}^{+\infty}    \int_{-\infty}^{+\infty}      \Psi_0(k_x,k_y) ~ e^{j(k_x x + k_y y)} ~ e^{\pm j z \sqrt{k^2-k_x^2-k_y^2} } ~ dk_x dk_y ~~~~~~~~~~~~~~~~~~(2.1) &lt;/math&gt;

Next, let

:&lt;math&gt;\psi_0(x,y) = \psi(x,y,z)|_{z=0}&lt;/math&gt;.

Then:

: &lt;math&gt;\psi_0(x,y)=\int_{-\infty}^{+\infty}      \int_{-\infty}^{+\infty}  \Psi_0(k_x,k_y) ~ e^{j(k_x x + k_y y)}  ~ dk_x dk_y &lt;/math&gt;

'''This plane wave spectrum representation of the electromagnetic field is the basic foundation of Fourier optics''' (this point cannot be emphasized strongly enough), because when ''z''=0, the equation above simply becomes a '''Fourier transform (FT) relationship between the field and its plane wave content'''  (hence the name, "Fourier optics").

Thus:

:&lt;math&gt;\Psi_0(k_x,k_y) = \mathcal{F} \{ \psi_0(x,y) \}&lt;/math&gt;

and

:&lt;math&gt;\psi_0(x,y) = \mathcal{F}^{-1} \{  \Psi_0(k_x,k_y)  \}&lt;/math&gt;

All spatial dependence of the individual plane wave components is described explicitly via the exponential functions. The coefficients of the exponentials are only functions of spatial wavenumber ''k&lt;sub&gt;x&lt;/sub&gt;'', ''k&lt;sub&gt;y&lt;/sub&gt;'', just as in ordinary [[Fourier analysis]] and [[Fourier transform]]s.

=== The diffraction limit ===
When
: &lt;math&gt;   k_x^2+k_y^2 &gt; k^2   &lt;/math&gt;
the plane waves are [[Evanescent wave|evanescent]] (decaying), so that any spatial frequency content in an object plane transparency which is finer than one wavelength will not be transferred over to the image plane, simply because the plane waves corresponding to that content cannot propagate. In connection with lithography of electronic components, this phenomenon is known as the [[diffraction limit]] and is the reason why light of progressively higher frequency (smaller wavelength, thus larger ''k'') is required for etching progressively finer features in integrated circuits.

== The paraxial approximation ==

===Paraxial plane waves (Optic axis is assumed z-directed)===

As shown above, an elementary product solution to the Helmholtz equation takes the form:

:&lt;math&gt;\psi(\mathbf{r}) = A(\mathbf{r}) e^{-j \mathbf{k} \cdot \mathbf{r}}&lt;/math&gt;

where

:&lt;math&gt;\mathbf{k} = k_x \mathbf{x} + k_y \mathbf{y} + k_z\mathbf{z} &lt;/math&gt;

is the [[wave vector]], and

:&lt;math&gt; k = \|\mathbf{k}\| = \sqrt{k_x^2 + k_y^2  + k_z^2} = {\omega \over c}&lt;/math&gt;

is the wave number. Next, using the [[paraxial approximation]], it is assumed that

:&lt;math&gt;k_x^2 + k_y^2 \ll k_z^2 &lt;/math&gt;

or equivalently,

:&lt;math&gt;\sin \theta \approx \theta &lt;/math&gt;

where θ is the angle between the wave vector '''k''' and the z-axis.

As a result,
:&lt;math&gt; k_z = k \cos \theta \approx k ( 1 - \theta^2 / 2)&lt;/math&gt;
and
:&lt;math&gt;\psi(\mathbf{r}) \approx A(\mathbf{r}) e^{-j(k_x x+k_y y)} e^{jkz \theta^2/2 }  e^{-jk z}&lt;/math&gt;

=== The paraxial wave equation ===

Substituting this expression into the Helmholtz equation, the paraxial wave equation is derived:
:&lt;math&gt;\nabla_T^2 A - 2jk { \partial A \over \partial z} = 0&lt;/math&gt;
where
:&lt;math&gt;\nabla_T^2 = \nabla^2 - {\partial^2 \over \partial z^2} = {\partial^2 \over \partial x^2} + {\partial^2 \over \partial y^2} &lt;/math&gt;
is the transverse [[Laplace operator]], shown here in Cartesian coordinates.

== The far field approximation ==

{{Main|Fraunhofer diffraction}}

The equation above may be evaluated asymptotically in the far field (using the [[stationary phase approximation|stationary phase method]]) to show that the field at the distant point (''x'',''y'',''z'') is indeed due solely to the plane wave component (''k&lt;sub&gt;x&lt;/sub&gt;'', ''k&lt;sub&gt;y&lt;/sub&gt;'', ''k&lt;sub&gt;z&lt;/sub&gt;'') which propagates parallel to the vector (''x'',''y'',''z''), and whose plane is tangent to the phasefront at (''x'',''y'',''z''). The mathematical details of this process may be found in Scott [1998] or Scott [1990]. The result of performing a stationary phase integration on the expression above is the following expression,

: &lt;math&gt;E_u(r,\theta,\phi)~=~2 \pi j~ (k~\cos\theta)~ \frac{e^{-jkr}}{r}~ E_u(k~\sin\theta~\cos\phi,k~\sin\theta~\sin\phi) ~~~~~~~~~~~~(2.2)&lt;/math&gt;

which clearly indicates that the field at (x,y,z) is directly proportional to the spectral component in the direction of (x,y,z), where,

: &lt;math&gt; x = r ~ \sin \theta ~ \cos \phi &lt;/math&gt;

: &lt;math&gt; y = r ~ \sin \theta ~ \sin \phi &lt;/math&gt;

: &lt;math&gt; z = r ~ \cos \theta ~ &lt;/math&gt;

and

: &lt;math&gt; k_x = k ~ \sin \theta ~ \cos \phi &lt;/math&gt;

: &lt;math&gt; k_y = k ~ \sin \theta ~ \sin \phi &lt;/math&gt;

: &lt;math&gt; k_z = k ~ \cos \theta ~ &lt;/math&gt;

Stated another way, the radiation pattern of any planar field distribution is the FT of that source distribution (see [[Huygens–Fresnel principle]], wherein the same equation is developed using a [[Green's function]] approach). Note that this is NOT a plane wave. The &lt;math&gt; \frac{e^{-jkr}}{r}&lt;/math&gt; radial dependence is a spherical wave - both in magnitude and phase - whose local amplitude is the FT of the source plane distribution at that far field angle. The plane wave spectrum has nothing to do with saying that the field behaves something like a plane wave for far distances.

=== Spatial versus angular bandwidth ===

Equation (2.2) above is '''critical''' to making the connection between ''spatial bandwidth'' (on the one hand) and ''angular bandwidth'' (on the other), in the far field. Note that the term "far field" usually means we're talking about a converging or diverging spherical wave with a pretty well defined phase center. The connection between spatial and angular bandwidth in the far field is essential in understanding the low pass filtering property of thin lenses. See section 5.1.3 for the condition defining the far field region.

Once the concept of angular bandwidth is understood, the optical scientist can "jump back and forth" between the spatial and spectral domains to quickly gain insights which would ordinarily not be so readily available just through spatial domain or ray optics considerations alone. For example, any source bandwidth which lies past the edge angle to the first lens (this edge angle sets the bandwidth of the optical system) will not be captured by the system to be processed.

As a side note, electromagnetics scientists have devised an alternative means for calculating the far zone electric field which does not involve stationary phase integration.  They have devised a concept known as "fictitious magnetic currents" usually denoted by '''M''', and defined as
: &lt;math&gt; ~ ~ \mathbf{M} ~ = ~ 2 \mathbf{E}^{aper} ~ \times ~ \mathbf{\hat{z}} &lt;/math&gt;.
In this equation, it is assumed that the unit vector in the z-direction points into the half-space where the far field calculations will be made.  These equivalent magnetic currents are obtained using equivalence principles which, in the case of an infinite planar interface, allow any electric currents, '''J''' to be "imaged away" while the fictitious magnetic currents are obtained from twice the aperture electric field (see Scott [1998]).  Then the radiated electric field is calculated from the magnetic currents using an equation similar to the equation for the magnetic field radiated by an electric current.  In this way, a vector equation is obtained for the radiated electric field in terms of the aperture electric field and the derivation requires no use of stationary phase ideas.

==The plane wave spectrum: the foundation of Fourier optics==

Fourier optics is somewhat different from ordinary ray optics typically used in the analysis and design of focused imaging systems such as cameras, telescopes and microscopes.  Ray optics is the very first type of optics most of us encounter in our lives; it's simple to conceptualize and understand, and works very well in gaining a baseline understanding of common optical devices.  Unfortunately, ray optics does not explain the operation of Fourier optical systems, which are in general not focused systems.  Ray optics is a subset of wave optics (in the jargon, it is "the asymptotic zero-wavelength limit" of wave optics) and therefore has limited applicability.  We have to know when it is valid and when it is not - and this is one of those times when it is not.  For our current task, we must expand our understanding of optical phenomena to encompass wave optics, in which the optical field is seen as a solution to Maxwell's equations.  This more general ''wave optics'' accurately explains the operation of Fourier optics devices.

In this section, we won't go all the way back to Maxwell's equations, but will start instead with the homogeneous Helmholtz equation (valid in source-free media), which is one level of refinement up from Maxwell's equations (Scott [1998]).  From this equation, we'll show how infinite uniform plane waves comprise one field solution (out of many possible) in free space.  These uniform plane waves form the basis for understanding Fourier optics.

The [[plane wave]] spectrum concept is the basic foundation of Fourier Optics. The plane wave spectrum is a continuous spectrum of ''uniform'' plane waves, and there is one plane wave component in the spectrum for every tangent point on the far-field phase front. The amplitude of that plane wave component would be the amplitude of the optical field at that tangent point. Again, this is true only in the far field, defined as: Range = 2 D&lt;sup&gt;2&lt;/sup&gt; / λ where D is the maximum linear extent of the optical sources and λ is the wavelength (Scott [1998]). The plane wave spectrum is often regarded as being discrete for certain types of periodic gratings, though in reality, the spectra from gratings are continuous as well, since no physical device can have the infinite extent required to produce a true line spectrum.

As in the case of electrical signals, bandwidth is a measure of how finely detailed an image is; the finer the detail, the greater the bandwidth required to represent it.  A DC electrical signal is constant and has no oscillations; a plane wave propagating parallel to the optic (&lt;math&gt;z&lt;/math&gt;) axis has constant value in any ''x''-''y'' plane, and therefore is analogous to the (constant) DC component of an electrical signal.  Bandwidth in electrical signals relates to the difference between the highest and lowest frequencies present in the spectrum of the signal.  For ''optical'' systems, bandwidth also relates to spatial frequency content (spatial bandwidth), but it also has a secondary meaning.  It also measures how far from the optic axis the corresponding plane waves are tilted, and so this type of bandwidth is often referred to also as angular bandwidth. It takes more frequency bandwidth to produce a short pulse in an electrical circuit, and more angular (or, spatial frequency) bandwidth to produce a sharp spot in an optical system (see discussion related to [[Point spread function]]).

The plane wave spectrum arises naturally as the [[eigenfunction]] or "natural mode" solution to the homogeneous [[electromagnetic wave equation]] in rectangular coordinates (see also [[Electromagnetic radiation]], which derives the wave equation from Maxwell's equations in source-free media, or Scott [1998]). In the [[frequency domain]], with an assumed (engineering) time convention of &lt;math&gt;  e^{j \omega t} &lt;/math&gt;, the homogeneous electromagnetic wave equation is known as the [[Helmholtz equation]] and takes the form:

: &lt;math&gt;  \nabla^2 E_u + k^2E_u = 0  ~~~~~~~~~~~~~~(2.0) &lt;/math&gt;

where ''u'' = ''x'', ''y'', ''z'' and ''k'' = 2π/λ is the [[wavenumber]] of the medium.

===Eigenfunction (natural mode) solutions: background and overview===

In the case of differential equations, as in the case of matrix equations, whenever the right-hand side of an equation is zero (i.e., the forcing function / forcing vector is zero), the equation may still admit a non-trivial solution, known in applied mathematics as an [[eigenfunction]] solution, in physics as a "natural mode" solution and in electrical circuit theory as the "zero-input response."  This is a concept that spans a wide range of physical disciplines.  Common physical examples of ''resonant'' natural modes would include the resonant vibrational modes of stringed instruments (1D), percussion instruments (2D) or the former [[Tacoma Narrows Bridge (1940)|Tacoma Narrows Bridge]] (3D).  Examples of ''propagating'' natural modes would include [[Waveguide (electromagnetism)|waveguide]] modes, [[optical fiber]] modes, [[Soliton (optics)|solitons]] and [[Bloch wave]]s.  Infinite homogeneous media admit the rectangular, circular and spherical harmonic solutions to the Helmholtz equation, depending on the coordinate system under consideration.  The propagating plane waves we'll study in this article are perhaps the simplest type of propagating waves found in any type of media.

There is a striking similarity between the Helmholtz equation (2.0) above, which may be written

: &lt;math&gt;  (\nabla^2 + k^2)~f = 0 , &lt;/math&gt;

and the usual equation for the [[Eigenvalues and eigenvectors|eigenvalues/eigenvectors]] of a square matrix, '''A''',

: &lt;math&gt;  (\mathbf A - \lambda \mathbf I) ~ \mathbf x = 0   &lt;/math&gt;  ,

particularly since both the scalar Laplacian,  &lt;math&gt;  \nabla^2  &lt;/math&gt; and the matrix, '''A''' are linear operators on their respective function/vector spaces (the minus sign in the second equation is, for all intents and purposes, immaterial; the plus sign in the first equation however is significant).  It is perhaps worthwhile to note that both the eigenfunction and eigenvector solutions to these two equations respectively, often yield an orthogonal set of functions/vectors which span (i.e., form a basis set for) the function/vector spaces under consideration. The interested reader may investigate other functional linear operators which give rise to different kinds of orthogonal eigenfunctions such as [[Legendre polynomials]], [[Chebyshev polynomials]] and [[Hermite polynomials]].

In the matrix case, eigenvalues &lt;math&gt;\lambda&lt;/math&gt; may be found by setting the determinant of the matrix equal to zero, i.e. finding where the matrix has no inverse.  Finite matrices have only a finite number of eigenvalues/eigenvectors, whereas linear operators can have a countably infinite number of eigenvalues/eigenfunctions (in confined regions) or uncountably infinite (continuous) spectra of solutions, as in unbounded regions.

In certain physics applications such as in the [[Bloch wave – MoM method|computation of bands in a periodic volume]], it is often the case that the elements of a matrix will be very complicated functions of frequency and wavenumber, and the matrix will be non-singular for most combinations of frequency and wavenumber, but will also be singular for certain specific combinations. By finding which combinations of frequency and wavenumber drive the determinant of the matrix to zero, the propagation characteristics of the medium may be determined. Relations of this type, between frequency and wavenumber, are known as dispersion relations and some physical systems may admit many different kinds of dispersion relations. An example from electromagnetics is the ordinary waveguide, which may admit numerous dispersion relations, each associated with a unique mode of the waveguide. Each propagation mode of the waveguide is known as an [[eigenfunction]] solution (or eigenmode solution) to Maxwell's equations in the waveguide. Free space also admits eigenmode (natural mode) solutions (known more commonly as plane waves), but with the distinction that for any given frequency, free space admits a continuous modal spectrum, whereas waveguides have a discrete mode spectrum.  In this case the dispersion relation is linear, as in section 1.2.

=== K-space ===

The separation condition,

: &lt;math&gt; k_x^2+k_y^2+k_z^2=k^2 &lt;/math&gt;

which is identical to the equation for the [[Euclidean metric]] in three-dimensional configuration space, suggests the notion of a [[wave vector|k-vector]] in three-dimensional "k-space", defined (for propagating plane waves) in rectangular coordinates as:

: &lt;math&gt; \mathbf{k} ~ = ~ k_x  \mathbf{\hat{x}} + k_y \mathbf{\hat{y}} + k_z  \mathbf{\hat{z}} &lt;/math&gt;

and in the [[spherical coordinate system]] as

: &lt;math&gt; k_x = k ~ \sin \theta ~ \cos \phi &lt;/math&gt;

: &lt;math&gt; k_y = k ~ \sin \theta ~ \sin \phi &lt;/math&gt;

: &lt;math&gt; k_z = k ~ \cos \theta ~ &lt;/math&gt;

Use will be made of these spherical coordinate system relations in the next section.

The notion of k-space is central to many disciplines in engineering and physics, especially in the study of periodic volumes, such as in crystallography and the band theory of semiconductor materials.

=== The two-dimensional Fourier transform ===

Analysis Equation (calculating the spectrum of the function):
:&lt;math&gt;U(k_x,k_y) = \int_{-\infty}^{\infty}  \int_{-\infty}^{\infty} u(x,y) e^{-j(k_x x + k_y y)} dx dy &lt;/math&gt;

Synthesis Equation (reconstructing the function from its spectrum):
:&lt;math&gt; u(x,y) = \frac{1}{(2\pi)^2}\int_{-\infty}^{\infty}  \int_{-\infty}^{\infty} U(k_x,k_y) e^{j(k_x x + k_y y)} dk_x dk_y &lt;/math&gt;

''Note'': the normalizing factor of:&lt;math&gt;\frac{1}{(2\pi)^2} &lt;/math&gt; is present whenever angular frequency (radians) is used, but not when ordinary frequency (cycles) is used.

==Optical systems: General overview and analogy with electrical signal processing systems==

An optical system consists of an input plane, and output plane, and a set of components that transforms the image ''f'' formed at the input into a different image ''g'' formed at the output. The output image is related to the input image by convolving the input image with the optical impulse response, ''h'' (known as the ''point-spread function'', for focused optical systems). The impulse response uniquely defines the input-output behavior of the optical system. By convention, the optical axis of the system is taken as the ''z''-axis. As a result, the two images and the impulse response are all functions of the transverse coordinates, ''x'' and ''y''.

:&lt;math&gt;g(x,y) = h(x,y) * f(x,y)&lt;/math&gt;

The impulse response of an optical imaging system is the output plane field which is produced when an ideal mathematical point source of light is placed in the input plane (usually on-axis). In practice, it is not necessary to have an ideal point source in order to determine an exact impulse response. This is because any source bandwidth which lies outside the bandwidth of the system won't matter anyway (since it cannot even be captured by the optical system), so therefore it's not necessary in determining the impulse response. The source only needs to have at least as much (angular) bandwidth as the optical system.

Optical systems typically fall into one of two different categories. The first is the ordinary focused optical imaging system, wherein the input plane is called the object plane and the output plane is called the image plane. The field in the image plane is desired to be a high-quality reproduction of the field in the object plane. In this case, the impulse response of the optical system is desired to approximate a 2D delta function, at the same location (or a linearly scaled location) in the output plane corresponding to the location of the impulse in the input plane. The ''actual'' impulse response typically resembles an [[Airy disk|Airy function]], whose radius is on the order of the wavelength of the light used. In this case, the impulse response is typically referred to as a [[point spread function]], since the mathematical point of light in the object plane has been spread out into an Airy function in the image plane.

The second type is the optical image processing system, in which a significant feature in the input plane field is to be located and isolated. In this case, the impulse response of the system is desired to be a close replica (picture) of that feature which is being searched for in the input plane field, so that a convolution of the impulse response (an image of the desired feature) against the input plane field will produce a bright spot at the feature location in the output plane.  It is this latter type of optical ''image processing'' system that is the subject of this section.  Section 5.2 presents one hardware implementation of the optical image processing operations described in this section.

=== Input plane ===

The input plane is defined as the locus of all points such that ''z'' = 0. The input image ''f'' is therefore

:&lt;math&gt;f(x,y) = U(x,y,z)\big|_{z=0} &lt;/math&gt;

=== Output plane ===

The output plane is defined as the locus of all points such that ''z'' = ''d''. The output image ''g'' is therefore

:&lt;math&gt;g(x,y) = U(x,y,z)\big|_{z=d} &lt;/math&gt;

=== The 2D convolution of input function against the impulse response function ===

:&lt;math&gt; g(x,y) ~ = ~ h(x,y) * f(x,y) &lt;/math&gt;

i.e.,

:&lt;math&gt; g(x,y)= \int_{-\infty}^{\infty}  \int_{-\infty}^{\infty}  h(x-x', y-y') ~ f(x',y') ~ dx' dy'   ~~~~~~(4.1)  ~&lt;/math&gt;

The alert reader will note that the integral above tacitly assumes that the impulse response is NOT a function of the position (x',y') of the impulse of light in the input plane (if this were not the case, this type of convolution would not be possible). This property is known as ''shift invariance'' (Scott [1998]). No optical system is perfectly shift invariant: as the ideal, mathematical point of light is scanned away from the optic axis, aberrations will eventually degrade the impulse response (known as a [[coma (optics)|coma]] in focused imaging systems). However, high quality optical systems are often "shift invariant enough" over certain regions of the input plane that we may regard the impulse response as being a function of only the difference between input and output plane coordinates, and thereby use the equation above with impunity.

Also, this equation assumes unit magnification. If magnification is present, then eqn. (4.1) becomes

:&lt;math&gt; g(x,y)= \int_{-\infty}^{\infty}  \int_{-\infty}^{\infty}  h_M(x-Mx', y-My') ~ f(x',y') ~ dx' dy'   ~~~~~~(4.2)  ~&lt;/math&gt;

which basically translates the impulse response function, h&lt;sub&gt;M&lt;/sub&gt;(), from x' to x=Mx'. In (4.2), h&lt;sub&gt;M&lt;/sub&gt;() will be a magnified version of the impulse response function h() of a similar, unmagnified system, so that h&lt;sub&gt;M&lt;/sub&gt;(x,y) =h(x/M,y/M).

=== Derivation of the convolution equation ===
The extension to two dimensions is trivial, except for the difference that [[causality]] exists in the time domain, but not in the spatial domain. Causality means that the impulse response ''h''(''t'' - t') of an electrical system, due to an impulse applied at time t', must of necessity be zero for all times t such that t - t' &lt; 0.

Obtaining the convolution representation of the system response requires representing the input signal as a weighted superposition over a train of impulse functions by using the ''shifting property'' of [[Dirac delta function]]s.

:&lt;math&gt;  f(t) = \int_{-\infty}^{\infty}    \delta(t-t') f(t') dt'&lt;/math&gt;

It is then presumed that the system under consideration is ''linear'', that is to say that the output of the system due to two different inputs (possibly at two different times) is the sum of the individual outputs of the system to the two inputs, when introduced individually. Thus the optical system may contain no nonlinear materials nor active devices (except possibly, extremely linear active devices). The output of the system, for a single delta function input is defined as the ''impulse response'' of the system, h(t - t').  And, by our linearity assumption (i.e., that the output of system to a pulse train input is the sum of the outputs due to each individual pulse), we can now say that the general input function ''f''(''t'') produces the output:

:&lt;math&gt;  g(t) = \int_{-\infty}^{\infty}    h(t-t') f(t') dt'&lt;/math&gt;

where ''h''(t - t') is the (impulse) response of the linear system to the delta function input δ(t - t'), applied at time t'. This is where the convolution equation above comes from. The convolution equation is useful because it is often much easier to find the response of a system to a delta function input - and then perform the convolution above to find the response to an arbitrary input - than it is to try to find the response to the arbitrary input directly. Also, the impulse response (in either time or frequency domains) usually yields insight to relevant figures of merit of the system. In the case of most lenses, the point spread function (PSF) is a pretty common figure of merit for evaluation purposes.

The same logic is used in connection with the [[Huygens–Fresnel principle]], or Stratton-Chu formulation, wherein the "impulse response" is referred to as the [[Green's function]] of the system. So the spatial domain operation of a linear optical system is analogous in this way to the Huygens–Fresnel principle.

=== System transfer function ===
If the last equation above is Fourier transformed, it becomes:

:&lt;math&gt;  G(\omega) ~ = ~  H(\omega) \cdot  F(\omega) &lt;/math&gt;

where

:&lt;math&gt; G(\omega) ~ &lt;/math&gt; is the spectrum of the output signal

:&lt;math&gt; H(\omega) ~ &lt;/math&gt; is the system transfer function

:&lt;math&gt; F(\omega) ~ &lt;/math&gt; is the spectrum of the input signal

In like fashion, (4.1) may be Fourier transformed to yield:

:&lt;math&gt; G(k_x,k_y) ~ = ~ H(k_x,k_y) \cdot F(k_x,k_y)&lt;/math&gt;

The system transfer function, &lt;math&gt;H(\omega)&lt;/math&gt;. In optical imaging this function is better known as the [[optical transfer function]] ''(Goodman)''.

Once again it may be noted from the discussion on the [[Abbe sine condition]], that this equation assumes unit magnification.

This equation takes on its real meaning when the Fourier transform, &lt;math&gt; ~G(k_x,k_y)&lt;/math&gt; is associated with the coefficient of the plane wave whose transverse wavenumbers are &lt;math&gt;~ (k_x,k_y)&lt;/math&gt;. Thus, the input-plane plane wave spectrum is transformed into the output-plane plane wave spectrum through the multiplicative action of the system transfer function. It is at this stage of understanding that the previous background on the plane wave spectrum becomes invaluable to the conceptualization of Fourier optical systems.

==Applications of  Fourier optics principles==
Fourier optics is used in the field of optical information processing, the staple of which is the classical 4F processor.

The [[Fourier transform]] properties of a [[lens (optics)|lens]] provide numerous applications in [[optical signal processing]] such as [[spatial filtering]], [[optical correlation]] and [[computer generated holograms]].

Fourier optical theory is used in [[interferometry]], [[optical tweezers]], [[Magnetic trap (atoms)|atom traps]], and [[quantum computing]]. Concepts of Fourier optics are used to reconstruct the [[Phase (waves)|phase]] of light intensity in the spatial frequency plane (see [[adaptive-additive algorithm]]).

===Fourier transforming property of lenses===

If a transmissive object is placed one focal length in front of a [[lens (optics)|lens]], then its [[Fourier transform]] will be formed one focal length behind the lens. Consider the figure to the right (click to enlarge)

[[File:Lens FT.jpg|On the Fourier transforming property of lenses|right|thumb|500px]]

In this figure, a plane wave incident from the left is assumed. The transmittance function in the front focal plane (i.e., Plane 1) ''spatially modulates the incident plane wave'' in magnitude and phase, ''like on the left-hand side of eqn. (2.1)'' (specified to ''z''=0), and ''in so doing, produces a spectrum of plane waves'' corresponding to the FT of the transmittance function, ''like on the right-hand side of eqn. (2.1)'' (for ''z''&gt;0). The various plane wave components propagate at different tilt angles with respect to the optic axis of the lens (i.e., the horizontal axis). The finer the features in the transparency, the broader the angular bandwidth of the plane wave spectrum. We'll consider one such plane wave component, propagating at angle θ with respect to the optic axis. It is assumed that θ is small ([[paraxial approximation]]), so that

: &lt;math&gt;\frac{k_x}{k} = \sin \theta \simeq \theta&lt;/math&gt;

and

: &lt;math&gt;\frac{k_z}{k} = \cos \theta \simeq 1 - \frac{\theta^2}{2} &lt;/math&gt;

and

: &lt;math&gt; \frac{1}{\cos \theta} \simeq \frac{1}{1 - \frac{\theta^2}{2}} \simeq 1 + \frac{\theta^2}{2} &lt;/math&gt;

In the figure, the ''plane wave'' phase, moving horizontally from the front focal plane to the lens plane, is

: &lt;math&gt; e^{j k f \cos \theta} \,&lt;/math&gt;

and the ''spherical wave'' phase from the lens to the spot in the back focal plane is:

: &lt;math&gt; e^{j k f / \cos \theta} \,&lt;/math&gt;

and the sum of the two path lengths is ''f'' (1 + θ&lt;sup&gt;2&lt;/sup&gt;/2 + 1 - θ&lt;sup&gt;2&lt;/sup&gt;/2) = 2''f''   i.e., it is a constant value, independent of tilt angle, θ, for paraxial plane waves. Each paraxial plane wave component of the field in the front focal plane appears as a [[point spread function]] spot in the back focal plane, with an intensity and phase equal to the intensity and phase of the original plane wave component in the front focal plane. In other words, the field in the back focal plane is the [[Fourier transform]] of the field in the front focal plane.

All FT components are computed simultaneously - in parallel - at the speed of light. As an example, light travels at a speed of roughly {{convert|1|ft|m|abbr=on}}. / ns, so if a lens has a {{convert|1|ft|m|abbr=on}}. focal length, an entire 2D FT can be computed in about 2 ns (2 x 10&lt;sup&gt;−9&lt;/sup&gt; seconds). If the focal length is 1 in., then the time is under 200 ps. No electronic computer can compete with these kinds of numbers or perhaps ever hope to, although new supercomputers such as the petaflop [[IBM Roadrunner]] may actually prove faster than optics, as improbable as that may seem. However, their speed is obtained by combining numerous computers which, individually, are still slower than optics. The disadvantage of the optical FT is that, as the derivation shows, the FT relationship only holds for paraxial plane waves, so this FT "computer" is inherently bandlimited. On the other hand, since the wavelength of visible light is so minute in relation to even the smallest visible feature dimensions in the image i.e.,

: &lt;math&gt; k^2 \gg  k_x ^2 + k_y ^2 &lt;/math&gt;

(for all ''k&lt;sub&gt;x&lt;/sub&gt;'', ''k&lt;sub&gt;y&lt;/sub&gt;'' within the spatial bandwidth of the image, so that ''k&lt;sub&gt;z&lt;/sub&gt;'' is nearly equal to ''k''), the paraxial approximation is not terribly limiting in practice. And, of course, this is an analog - not a digital - computer, so precision is limited. Also, phase can be challenging to extract; often it is inferred interferometrically.

Optical processing is especially useful in real time applications where rapid processing of massive amounts of 2D data is required, particularly in relation to pattern recognition.

====Object truncation and Gibbs phenomenon====

The spatially modulated electric field, shown on the left-hand side of eqn. (2.1), typically only occupies a finite (usually rectangular) aperture in the x,y plane. The rectangular aperture function acts like a 2D square-top filter, where the field is assumed to be zero outside this 2D rectangle. The spatial domain integrals for calculating the FT coefficients on the right-hand side of eqn. (2.1) are truncated at the boundary of this aperture. This step truncation can introduce inaccuracies in both theoretical calculations and measured values of the plane wave coefficients on the RHS of eqn. (2.1).

Whenever a function is discontinuously truncated in one FT domain, broadening and rippling are introduced in the other FT domain. A perfect example from optics is in connection with the point spread function, which for on-axis plane wave illumination of a quadratic lens (with circular aperture), is an Airy function,  ''J''&lt;sub&gt;1&lt;/sub&gt;(''x'')/''x''. Literally, the point source has been "spread out" (with ripples added), to form the Airy point spread function (as the result of truncation of the plane wave spectrum by the finite aperture of the lens). This source of error is known as [[Gibbs phenomenon]] and it may be mitigated by simply ensuring that all significant content lies near the center of the transparency, or through the use of [[window function]]s which smoothly taper the field to zero at the frame boundaries. By the convolution theorem, the FT of an arbitrary transparency function - multiplied (or truncated) by an aperture function - is equal to the FT of the non-truncated transparency function convolved against the FT of the aperture function, which in this case becomes a type of "Greens function" or "impulse response function" in the spectral domain. Therefore, the image of a circular lens is equal to the object plane function convolved against the Airy function (the FT of a circular aperture function is ''J''&lt;sub&gt;1&lt;/sub&gt;(''x'')/''x'' and the FT of a rectangular aperture function is a product of sinc functions, sin ''x''/''x'').

==== Fourier analysis and functional decomposition ====

Even though the input transparency only occupies a finite portion of the ''x''-''y'' plane (Plane 1), the uniform plane waves comprising the plane wave spectrum occupy the '''entire''' ''x''-''y'' plane, which is why (for this purpose) only the longitudinal plane wave phase (in the ''z''-direction, from Plane 1 to Plane 2) must be considered, and not the phase transverse to the ''z''-direction. It is of course, very tempting to think that if a plane wave emanating from the finite aperture of the transparency is tilted too far from horizontal, it will somehow "miss" the lens altogether but again, since the uniform plane wave extends infinitely far in all directions in the transverse (''x''-''y'') plane, the planar wave components cannot miss the lens.

This issue brings up perhaps the predominant difficulty with Fourier analysis, namely that the input-plane function, defined over a finite support (i.e., over its own finite aperture), is being approximated with other functions (sinusiods) which have infinite support (''i''.''e''., they are defined over the entire infinite ''x''-''y'' plane). This is unbelievably inefficient computationally, and is the principal reason why [[wavelet]]s were conceived, that is to represent a function (defined on a finite interval or area) in terms of oscillatory functions which are also defined over finite intervals or areas. Thus, instead of getting the frequency content of the entire image all at once (along with the frequency content of the entire rest of the ''x''-''y'' plane, over which the image has zero value), the result is instead the frequency content of different parts of the image, which is usually much simpler. Unfortunately, wavelets in the ''x''-''y'' plane don't correspond to any known type of propagating wave function, in the same way that Fourier's sinusoids (in the ''x''-''y'' plane) correspond to plane wave functions in three dimensions. However, the FTs of most wavelets are well known and could possibly be shown to be equivalent to some useful type of propagating field.

On the other hand, [[Sinc function]]s and [[Airy function]]s - which are not only the point spread functions of rectangular and circular apertures, respectively, but are also cardinal functions commonly used for functional decomposition in [[Whittaker–Shannon interpolation formula|interpolation/sampling theory]] [Scott 1990] - '''do''' correspond to converging or diverging spherical waves, and therefore could potentially be implemented as a whole new functional decomposition of the object plane function, thereby leading to another point of view similar in nature to Fourier optics. This would basically be the same as conventional ray optics, but with diffraction effects included. In this case, each point spread function would be a type of "smooth pixel," in much the same way that a soliton on a fiber is a "smooth pulse."

Perhaps a lens figure-of-merit in this "point spread function" viewpoint would be to ask how well a lens transforms an Airy function in the object plane into an Airy function in the image plane, as a function of radial distance from the optic axis, or as a function of the size of the object plane Airy function. This is somewhat like the point spread function, except now we're really looking at it as a kind of input-to-output plane transfer function (like MTF), and not so much in absolute terms, relative to a perfect point. Similarly, Gaussian wavelets, which would correspond to the waist of a propagating Gaussian beam, could also potentially be used in still another functional decomposition of the object plane field.

==== Far-field range and the 2D&lt;sup&gt;2&lt;/sup&gt; / λ criterion ====

In the figure above, illustrating the Fourier transforming property of lenses, the lens is in the near field of the object plane transparency, therefore the object plane field at the lens may be regarded as a superposition of plane waves, each one of which propagates at some angle with respect to the z-axis. In this regard, the far-field criterion is loosely defined as: Range = 2 ''D''&lt;sup&gt;2&lt;/sup&gt; / λ where ''D'' is the maximum linear extent of the optical sources and λ is the wavelength (Scott [1998]).  The ''D'' of the transparency is on the order of cm (10&lt;sup&gt;−2&lt;/sup&gt; m) and the wavelength of light is on the order of 10&lt;sup&gt;−6&lt;/sup&gt; m, therefore ''D''/λ  for the whole transparency is on the order of 10&lt;sup&gt;4&lt;/sup&gt;. This times ''D'' is on the order of 10&lt;sup&gt;2&lt;/sup&gt; m, or hundreds of meters. On the other hand, the far field distance from a PSF spot is on the order of λ. This is because D for the spot is on the order of λ, so that ''D''/λ is on the order of unity; this times ''D'' (i.e., λ) is on the order of λ (10&lt;sup&gt;−6&lt;/sup&gt; m).

Since the lens is in the far field of any PSF spot, the field incident on the lens from the spot may be regarded as being a spherical wave, as in eqn. (2.2), not as a plane wave spectrum, as in eqn. (2.1). On the other hand, the lens is in the near field of the entire input plane transparency, therefore eqn. (2.1) - the full plane wave spectrum - accurately represents the field incident on the lens from that larger, extended source.

==== Lens as a low-pass filter ====

A lens is basically a low-pass plane wave filter (see [[Low-pass filter]]). Consider a "small" light source located on-axis in the object plane of the lens. It is assumed that the source is small enough that, by the far-field criterion, the lens is in the far field of the "small" source. Then, the field radiated by the small source is a spherical wave which is modulated by the FT of the source distribution, as in eqn. (2.2), Then, the lens passes - from the object plane over onto the image plane - only that portion of the radiated spherical wave which lies inside the edge angle of the lens. In this far-field case, truncation of the radiated spherical wave is equivalent to truncation of the plane wave spectrum of the small source. So, the plane wave components in this far-field spherical wave, which lie beyond the edge angle of the lens, are not captured by the lens and are not transferred over to the image plane. Note: this logic is valid only for small sources, such that the lens is in the far field region of the source, according to the 2 ''D''&lt;sup&gt;2&lt;/sup&gt; / λ criterion mentioned previously. If an object plane transparency is imagined as a summation over small sources (as in the [[Whittaker–Shannon interpolation formula]], Scott [1990]), each of which has its spectrum truncated in this fashion, then every point of the entire object plane transparency suffers the same effects of this low pass filtering.

Loss of the high (spatial) frequency content causes blurring and loss of sharpness (see discussion related to [[point spread function]]). Bandwidth truncation causes a (fictitious, mathematical, ideal) point source in the object plane to be blurred (or, spread out) in the image plane, giving rise to the term, "point spread function."  Whenever bandwidth is expanded or contracted, image size is typically contracted or expanded accordingly, in such a way that the space-bandwidth product remains constant, by Heisenberg's principle (Scott [1998] and [[Abbe sine condition]]).

==== Coherence and Fourier transforming ====

While working in the frequency domain, with an assumed e&lt;sup&gt;jωt&lt;/sup&gt; (engineering) time dependence, coherent (laser) light is implicitly assumed, which has a delta function dependence in the frequency domain. Light at different (delta function) frequencies will "spray" the plane wave spectrum out at different angles, and as a result these plane wave components will be focused at different places in the output plane. The Fourier transforming property of lenses works best with coherent light, unless there is some special reason to combine light of different frequencies, to achieve some special purpose.

===Hardware implementation of the system transfer function: The 4F correlator {{anchor|4F Correlator}}===
{{Main|Optical correlator}}
The theory on optical transfer functions presented in section 4 is somewhat abstract.  However, there is one very well known device which implements the system transfer function H in hardware using only 2 identical lenses and a transparency plate - the 4F correlator.  Although one important application of this device would certainly be to implement the mathematical operations of [[cross-correlation]] and [[convolution]], this device - 4 focal lengths long - actually serves a wide variety of image processing operations that go well beyond what its name implies.  A diagram of a typical 4F correlator is shown in the figure below (click to enlarge).  This device may be readily understood by combining the plane wave spectrum representation of the electric field (''section 2'') with the Fourier transforming property of quadratic lenses (''section 5.1'') to yield the optical image processing operations described in section 4.

[[File:4F Correlator.svg|4F Correlator|right|thumb|430px]]

The 4F correlator is based on the [[convolution theorem]] from [[Fourier transform]] theory, which states that [[convolution]] in the spatial (''x'',''y'') domain is equivalent to direct multiplication in the spatial frequency (''k''&lt;sub&gt;x&lt;/sub&gt;, ''k''&lt;sub&gt;y&lt;/sub&gt;) domain (aka: ''spectral domain''). Once again, a plane wave is assumed incident from the left and a transparency containing one 2D function, ''f''(''x'',''y''), is placed in the input plane of the correlator, located one focal length in front of the first lens. The transparency spatially modulates the incident plane wave in magnitude and phase, like on the left-hand side of eqn. (2.1), and in so doing, produces a spectrum of plane waves corresponding to the FT of the transmittance function, like on the right-hand side of eqn. (2.1). That spectrum is then formed as an "image" one focal length behind the first lens, as shown. A transmission mask containing the FT of the second function, ''g''(''x'',''y''), is placed in this same plane, one focal length behind the first lens, causing the transmission through the mask to be equal to the product, ''F''(''k''&lt;sub&gt;x&lt;/sub&gt;,''k''&lt;sub&gt;y&lt;/sub&gt;) x ''G''(''k''&lt;sub&gt;x&lt;/sub&gt;,''k''&lt;sub&gt;y&lt;/sub&gt;). This product now lies in the "input plane" of the second lens (one focal length in front), so that the FT of this product (i.e., the [[convolution]] of ''f''(''x'',''y'') and ''g''(''x'',''y'')), is formed in the back focal plane of the second lens.

If an ideal, mathematical point source of light is placed on-axis in the input plane of the first lens, then there will be a uniform, collimated field produced in the output plane of the first lens. When this uniform, collimated field is multiplied by the FT plane mask, and then Fourier transformed by the second lens, the output plane field (which in this case is the ''impulse response'' of the correlator) is just our correlating function, ''g''(''x'',''y''). In practical applications, ''g''(''x'',''y'') will be some type of feature which must be identified and located within the input plane field (see Scott [1998]). In military applications, this feature may be a tank, ship or airplane which must be quickly identified within some more complex scene.

The 4F correlator is an excellent device for illustrating the "systems" aspects of optical instruments, alluded to in ''section 4'' above. The FT plane mask function, ''G''(''k''&lt;sub&gt;x&lt;/sub&gt;,''k''&lt;sub&gt;y&lt;/sub&gt;) is the system transfer function of the correlator, which we'd in general denote as ''H''(''k''&lt;sub&gt;x&lt;/sub&gt;,''k''&lt;sub&gt;y&lt;/sub&gt;), and it is the FT of the impulse response function of the correlator, ''h''(''x'',''y'') which is just our correlating function ''g''(''x'',''y''). And, as mentioned above, the impulse response of the correlator is just a picture of the feature we're trying to find in the input image. In the 4F correlator, the system transfer function ''H''(''k''&lt;sub&gt;x&lt;/sub&gt;,''k''&lt;sub&gt;y&lt;/sub&gt;) is directly multiplied against the spectrum ''F''(''k''&lt;sub&gt;x&lt;/sub&gt;,''k''&lt;sub&gt;y&lt;/sub&gt;) of the input function, to produce the spectrum of the output function. This is how electrical signal processing systems operate on 1D temporal signals.

==Afterword: Plane wave spectrum within the broader context of functional decomposition==

Electrical fields can be represented mathematically in many different ways. In the [[Huygens–Fresnel principle|Huygens–Fresnel]] or [[Julius Adams Stratton|Stratton]]-Chu viewpoints, the electric field is represented as a superposition of point sources, each one of which gives rise to a [[Green's function]] field. The total field is then the weighted sum of all of the individual Green's function fields. That seems to be the most natural way of viewing the electric field for most people - no doubt because most of us have, at one time or another, drawn out the circles with protractor and paper, much the same way Thomas Young did in his classic paper on the [[double-slit experiment]]. However, it is by no means the only way to represent the electric field, which may also be represented as a spectrum of sinusoidally varying plane waves. In addition, [[Frits Zernike]] proposed still another [[functional decomposition]] based on his [[Zernike polynomials]], defined on the unit disc. The third-order (and lower) Zernike polynomials correspond to the normal lens aberrations. And still another functional decomposition could be made in terms of [[Sinc function]]s and Airy functions, as in the [[Whittaker–Shannon interpolation formula]] and the [[Nyquist–Shannon sampling theorem]]. All of these functional decompositions have utility in different circumstances. The optical scientist having access to these various representational forms has available a richer insight to the nature of these marvelous fields and their properties. These different ways of looking at the field are not conflicting or contradictory, rather, by exploring their connections, one can often gain deeper insight into the nature of wave fields.

=== Functional decomposition and eigenfunctions ===

The twin subjects of [[eigenfunction]] expansions and [[functional decomposition]], both briefly alluded to here, are not completely independent. The eigenfunction expansions to certain linear operators defined over a given domain, will often yield a countably infinite set of [[orthogonal functions]] which will span that domain. Depending on the operator and the dimensionality (and shape, and boundary conditions) of its domain, many different types of functional decompositions are, in principle, possible.

==See also==
* [[Abbe sine condition]]
* [[Huygens–Fresnel principle]]
* [[Point spread function]]
* [[Phase contrast microscopy]]
* [[Fraunhofer diffraction]]
* [[Fresnel diffraction]]
* [[Adaptive-additive algorithm]]
* [[Hilbert space]]
* [[Optical correlator]]
* [[Optical Hartley transform]]

==References==
{{Reflist}}
* {{cite book |author-first=Pierre-Michel |author-last=Duffieux |author-link=Pierre-Michel Duffieux |date=1983 |title=The Fourier Transform and its Applications to Optics |publisher=[[John Wiley &amp; Sons]] |location=New York, USA}}
* {{cite book |author-first=Joseph |author-last=Goodman |date=2005 |title=Introduction to Fourier Optics |edition=3 |publisher=Roberts &amp; Company Publishers |isbn=0-9747077-2-4 |url=https://books.google.com/books?id=ow5xs_Rtt9AC |access-date=2017-10-28}}
* {{cite book |author-first=Eugene |author-last=Hecht |date=1987 |title=Optics |edition=2 |publisher=[[Addison Wesley]] |isbn=0-201-11609-X}}
* {{cite book |author-first=Raymond |author-last=Wilson |date=1995 |title=Fourier Series and Optical Transform Techniques in Contemporary Optics |publisher=[[John Wiley &amp; Sons]] |isbn=0-471-30357-7}}
* {{cite book |author-first=Craig |author-last=Scott |date=1998 |title=Introduction to Optics and Optical Imaging |publisher=[[John Wiley &amp; Sons]] |isbn=0-7803-3440-X}}
* {{cite book |author-first=Craig |author-last=Scott |date=1990 |title=Modern Methods of Reflector Antenna Analysis and Design |publisher=[[Artech House]] |isbn=0-89006-419-9}}
* {{cite book |author-first=Craig |author-last=Scott |date=1989 |title=The Spectral Domain Method in Electromagnetics |publisher=[[Artech House]] |isbn=0-89006-349-4}}
* [https://www.youtube.com/watch?v=wcRB3TWIAXE Intro to Fourier Optics and the 4F correlator]

== External links ==

* [http://www.hindawi.com/journals/aot/2010/372652.html Optical Computing: A 60 Year Adventure]
* [http://server.physics.miami.edu/~curtright/Diffraction/StrattonChu1939.pdf Diffraction Theory of Electromagnetic Waves], ''Phys Rev''

[[Category:Optics]]
[[Category:Physical optics]]
[[Category:Fourier analysis]]</text>
      <sha1>f1c6lf3ikrb3e09ym4vfnpmj6ukdr92</sha1>
    </revision>
  </page>
  <page>
    <title>G. W. Peck</title>
    <ns>0</ns>
    <id>11168505</id>
    <revision>
      <id>861374212</id>
      <parentid>668561979</parentid>
      <timestamp>2018-09-26T23:44:22Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>/* External links */add authority control, test</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2883">{{dablink|This article is about a pseudonymous attribution. For people named G. W. Peck, see [[G. W. Peck (disambiguation)]].}}

'''G. W. Peck''' is a [[pseudonym]]ous attribution used as the author or co-author of a number of published [[mathematics]] [[academic paper]]s. Peck is sometimes humorously identified with [[George Wilbur Peck]], a former governor of the [[United States|US]] state of [[Wisconsin]].&lt;ref name="kcc"&gt;{{citation
 | last = Peck | first = G. W.
 | doi = 10.1016/S0012-365X(02)00595-2
 | mr = 1935723
 | issue = 2–3
 | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]
 | pages = 193–224
 | title = Kleitman and combinatorics: a celebration
 | volume = 257
 | year = 2002}}.&lt;/ref&gt;

Peck first appeared as the official author of a 1979 paper entitled "Maximum [[antichain]]s of rectangular arrays".&lt;ref&gt;{{citation
 | last = Peck | first = G. W.
 | doi = 10.1016/0097-3165(79)90035-9
 | mr = 0555816
 | issue = 3
 | journal = [[Journal of Combinatorial Theory|Journal of Combinatorial Theory, Series A]]
 | pages = 397–400
 | title = Maximum antichains of rectangular arrays
 | volume = 27
 | year = 1979}}.&lt;/ref&gt; The name "G. W. Peck" is derived from the initials of the actual writers of this paper: [[Ronald Graham]], [[Douglas West (mathematician)|Douglas West]], [[George B. Purdy]], [[Paul Erdős]], [[Fan Chung]], and [[Daniel Kleitman]]. The paper initially listed Peck's affiliation as [[Shangdu|Xanadu]], but the editor of the journal objected, so Ron Graham gave him a job at [[Bell Labs]]. Since then, Peck's name has appeared on some sixteen publications,&lt;ref&gt;[http://www.ams.org/mathscinet/search/publications.html?pg1=IID&amp;s1=202591 Listing of Peck's publications] in [[MathSciNet]] (subscription required), retrieved 2010-03-11.&lt;/ref&gt; primarily as a pseudonym of Daniel Kleitman.&lt;ref name="kcc"/&gt;

In reference to "G. W. Peck", [[Richard P. Stanley]] defined a '''Peck poset''' to be a [[graded poset|graded]] [[partially ordered set]] that is [[rank symmetric]], [[rank unimodal]], and [[strongly Sperner]].&lt;ref&gt;{{citation
 | last = Stanley | first = Richard
 | doi = 10.1007/BF00396271
 | mr = 0745587
 | issue = 1
 | journal = Order
 | pages = 29–34
 | title = Quotients of Peck posets
 | volume = 1
 | year = 1984}}.&lt;/ref&gt; The [[poset]]s in the original paper by G. W. Peck are not quite Peck posets, as they lack the property of being rank symmetric.

==See also==
*[[Nicolas Bourbaki]]
*[[Arthur Besse]]
*[[John Rainwater]]

==References==
{{reflist}}

==External links==
*[https://www.youtube.com/watch?v=izdZPx89ph4 Imaginary Erdős numbers], Numberphile, Nov 26, 2014. Video interview with Ron Graham in which he tells the story of G. W. Peck.

{{authority control}}

{{DEFAULTSORT:Peck, G. W.}}
[[Category:Academic shared pseudonyms]]
[[Category:American mathematicians]]
[[Category:Pseudonymous mathematicians]]</text>
      <sha1>icf3zlfw01xg9bm393kxhwrlwxu6xs6</sha1>
    </revision>
  </page>
  <page>
    <title>Gelfand–Naimark–Segal construction</title>
    <ns>0</ns>
    <id>297446</id>
    <revision>
      <id>862709169</id>
      <parentid>856548597</parentid>
      <timestamp>2018-10-06T05:18:58Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Removing category Eponymous scientific concepts per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2018 September 22]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13596">In [[functional analysis]], a discipline within [[mathematics]], given a [[C*-algebra]] ''A'',  the '''Gelfand–Naimark–Segal construction''' establishes a correspondence between cyclic *-representations of ''A'' and certain [[linear functional]]s on ''A'' (called ''states'').  The correspondence is shown by an explicit construction of the *-representation from the state. It is named for [[Israel Gelfand]], [[Mark Naimark]], and [[Irving Segal]].

== States and representations ==

A '''*-representation''' of a [[C*-algebra]] ''A'' on a [[Hilbert space]] ''H'' is a [[map (mathematics)|map]]ping
π from ''A'' into the algebra of [[bounded operator]]s on ''H'' such that
* π is a [[ring homomorphism]] which carries [[Involution (mathematics)|involution]] on ''A'' into involution on operators
* π is [[nondegenerate]], that is the space of vectors π(''x'') ξ is dense as ''x'' ranges through ''A'' and ξ ranges through ''H''. Note that if ''A'' has an identity, nondegeneracy means exactly π is unit-preserving, i.e. π maps the identity of ''A'' to the identity operator on ''H''.

A [[state (functional analysis)|state]] on  C*-algebra ''A'' is a [[positive linear functional]] ''f'' of norm 1. If ''A'' has a multiplicative unit element this condition is equivalent to ''f''(1) = 1.

For a representation π of a C*-algebra ''A'' on a Hilbert space ''H'', an element ξ is called a '''cyclic vector'''  if the set of vectors
:&lt;math&gt;\{\pi(x)\xi:x\in A\}&lt;/math&gt;
is norm dense in ''H'', in which case π is called a '''cyclic representation'''. Any non-zero vector of an [[irreducible representation]] is cyclic. However, non-zero vectors in a cyclic representation may fail to be cyclic.

===The GNS construction===
Let π be a *-representation of a C*-algebra ''A''  on the Hilbert space ''H'' and ξ be a unit norm cyclic vector for π. Then
:&lt;math&gt; a \mapsto \langle \pi(a) \xi, \xi\rangle &lt;/math&gt;
is a state of ''A''.

In fact, every state of ''A'' may be viewed as a [[State (functional analysis)#Vector states|vector state]] as above, under a suitable canonical representation.

:'''Theorem.'''&lt;ref&gt;[[Kadison, R. V.]], Theorem 4.5.2, Fundamentals of the Theory of Operator Algebras, Vol. I : Elementary Theory, American Mathematical Society. {{ISBN|978-0821808191}}&lt;/ref&gt; Given a state ρ of ''A'', there is a *-representation π of ''A'' acting on a Hilbert space ''H'' with distinguished unit cyclic vector ξ such that &lt;math&gt;\rho(a)=\langle \pi(a) \xi, \xi \rangle&lt;/math&gt; for every ''a'' in ''A''.

:'''Proof.'''
:1) '''Construction of the Hilbert space ''H'' '''

:Define on ''A'' a semi-definite [[sesquilinear form]]
::&lt;math&gt; \langle a, b \rangle =\rho(b^*a), \; a, b \in A.&lt;/math&gt;
:By the [[Positive linear functional#Cauchy-Schwarz inequality|Cauchy–Schwarz inequality]], the degenerate elements, ''a'' in ''A'' satisfying ρ(''a* a'')= 0, form a vector subspace ''I'' of ''A''. By a C*-algebraic argument, one can show that ''I'' is a [[left ideal]] of ''A'' (known as the left kernel of ρ). In fact, it is the largest left ideal in the null space of ρ. The [[quotient space (linear algebra)|quotient space]] of ''A'' by the vector subspace ''I'' is an inner product space with the inner product defined by&lt;math&gt;\langle a+I,b+I\rangle :=\rho(b^*a),\; a,b\in A&lt;/math&gt;. The [[Cauchy completion]] of ''A''/''I'' in the norm induced by this inner product is a Hilbert space, which we denote by ''H''.

:2) '''Construction of the representation π '''

:Define the action π of ''A'' on ''A''/''I'' by π(''a'')(''b''+''I'') = ''ab''+''I'' of ''A'' on ''A''/''I''. The same argument showing ''I'' is a left ideal also implies that π(''a'') is a bounded operator on ''A''/''I'' and therefore can be extended uniquely to the completion. Unravelling the definition of the [[adjoint]] of an operator on a Hilbert space, π turns out to be *-preserving. This proves the existence of a  *-representation π.

:3) '''Identifying the unit norm cyclic vector ξ '''

:If ''A'' has a multiplicative identity 1, then it is immediate that the equivalence class ξ in the GNS Hilbert space ''H'' containing 1 is a cyclic vector for the above representation. If ''A'' is non-unital, take  an [[approximate identity]] {''e&lt;sub&gt;&amp;lambda;&lt;/sub&gt;''} for ''A''. Since positive linear functionals are bounded, the equivalence classes of the net {''e&lt;sub&gt;&amp;lambda;&lt;/sub&gt;''} converges to some vector ξ in ''H'', which is a cyclic vector for π.

:It is clear from the definition of the inner product on the GNS Hilbert space ''H'' that the state ρ can be recovered as a vector state on ''H''. This proves the theorem.

The method used to produce a *-representation from a state of ''A'' in the proof of the above theorem is called the '''GNS construction'''.
For a state of a C*-algebra ''A'', the corresponding GNS representation is essentially uniquely determined by the condition, &lt;math&gt;\rho(a) = \langle \pi(a) \xi, \xi \rangle&lt;/math&gt; as seen in the theorem below.
:'''Theorem.'''&lt;ref&gt;[[Kadison, R. V.]], Proposition 4.5.3, Fundamentals of the Theory of Operator Algebras, Vol. I : Elementary Theory, American Mathematical Society. {{ISBN|978-0821808191}}&lt;/ref&gt; Given a state ρ of ''A'', let π, π' be *-representations of ''A'' on Hilbert spaces ''H'', ''K'' respectively each with unit norm cyclic vectors ξ ∈ ''H'', ξ' ∈ ''K'' such that &lt;math&gt;\rho(a) = \langle \pi(a) \xi, \xi \rangle = \langle \pi'(a) \xi ', \xi ' \rangle&lt;/math&gt; for all &lt;math&gt;a \in A&lt;/math&gt;. Then π, π' are unitarily equivalent *-representations i.e. there is a unitary operator ''U'' from ''H'' to ''K'' such that π'(''a'') = Uπ(''a'')U* for all ''a'' in ''A''. The operator ''U'' that  implements the unitary equivalence maps π(''a'')ξ to π'(''a'')ξ' for all ''a'' in ''A''.

===Significance of the GNS construction===
The GNS construction is at the heart of the proof of the [[Gelfand–Naimark theorem]] characterizing C*-algebras as algebras of operators. A C*-algebra has sufficiently many pure states (see below) so that the direct sum of corresponding irreducible GNS representations is [[Faithful group action|faithful]].

The direct sum of the corresponding GNS representations of all states is called the '''[[universal representation (C*-algebra)|universal representation]]''' of ''A''. The universal representation of ''A'' contains every cyclic representation. As every *-representation is a direct sum of cyclic representations, it follows that every *-representation of ''A'' is a direct summand of some sum of copies of the universal representation.

If Φ is the universal representation of a C*-algebra ''A'', the closure of Φ(''A'') in the weak operator topology is called the '''[[enveloping von Neumann algebra]]''' of ''A''. It can be identified with the double dual ''A**''.

== Irreducibility ==

Also of significance is the relation between [[irreducible representation|irreducible]] *-representations and extreme points of the convex set of states.  A representation π on ''H'' is irreducible if and only if there are no closed subspaces of ''H'' which are invariant under all the operators π(''x'') other than ''H'' itself and the trivial subspace {0}.

:'''Theorem'''.  The set of states of a C*-algebra ''A'' with a unit element is a compact [[convex set]] under the weak-* topology.  In general, (regardless of whether or not ''A'' has a unit element) the set of positive functionals of norm ≤ 1 is a compact convex set.

Both of these results follow immediately from the [[Banach–Alaoglu theorem]].

In the unital commutative case, for the C*-algebra ''C''(''X'') of continuous functions on some compact ''X'', [[Riesz–Markov–Kakutani representation theorem]] says that the positive functionals of norm ≤ 1 are precisely the Borel positive measures on ''X'' with total mass ≤ 1. It follows from [[Krein–Milman theorem]] that the extremal states are the Dirac point-mass measures.

On the other hand, a representation of ''C''(''X'') is irreducible if and only if it is one-dimensional. Therefore the GNS representation of ''C''(''X'') corresponding to a measure μ is irreducible if and only if μ is an extremal state. This is in fact true for C*-algebras in general.

:'''Theorem'''. Let ''A'' be a C*-algebra.  If π is a *-representation of ''A''  on the Hilbert space ''H'' with unit norm cyclic vector ξ, then π is irreducible if and only if the corresponding state ''f'' is an [[extreme point]] of the convex set of positive linear functionals on ''A'' of norm ≤ 1.

To prove this result one notes first that a representation is irreducible if and only if the [[commutant]] of π(''A''), denoted by π(''A'')', consists of scalar multiples of the identity.

Any positive linear functionals ''g'' on ''A'' dominated by ''f'' is of the form

:&lt;math&gt; g(x^*x) = \langle \pi(x) \xi,  \pi(x) T_g \, \xi \rangle &lt;/math&gt;

for some positive operator ''T&lt;sub&gt;g&lt;/sub&gt;'' in π(''A'')' with 0 ≤ ''T'' ≤ 1 in the operator order. This is a version of the [[Radon–Nikodym theorem]].

For such ''g'', one can write ''f'' as a sum of positive linear functionals: ''f'' = ''g'' + ''g' ''. So π is unitarily equivalent to a subrepresentation of π&lt;sub&gt;''g''&lt;/sub&gt; ⊕ π&lt;sub&gt;''g' ''&lt;/sub&gt;. This shows that π is irreducible if and only if any such π&lt;sub&gt;''g''&lt;/sub&gt; is unitarily equivalent to π, i.e. ''g'' is a scalar multiple of ''f'', which proves the theorem.

Extremal states are usually called [[State (functional analysis)#Pure states|pure states]].  Note that a state is a pure state if and only if it is extremal in the convex set of states.

The theorems above for C*-algebras are valid more generally in the context of  [[B-star algebra|B*-algebra]]s with approximate identity.

== Generalizations ==

The [[Stinespring factorization theorem]] characterizing [[completely positive map]]s is an important generalization of the GNS construction.

== History ==
Gelfand and Naimark's paper on the Gelfand–Naimark theorem was published in 1943.&lt;ref&gt;{{cite journal |author=[[I. M. Gelfand]], [[M. A. Naimark]] |title=On the imbedding of normed rings into the ring of operators on a Hilbert space |journal=[[Matematicheskii Sbornik]] |volume=12 |issue=2 |year=1943 |pages=197–217 |url=http://mi.mathnet.ru/eng/msb6155}} (also [https://www.google.com/books?id=DYCUp0JYU6sC&amp;printsec=frontcover#PPA3,M1 Google Books], see pp.&amp;nbsp;3–20)&lt;/ref&gt; Segal recognized the construction that was implicit in this work and presented it in sharpened form.&lt;ref&gt;[[Richard V. Kadison]]: ''Notes on the Gelfand–Neimark theorem''. In: Robert C. Doran (ed.): ''C*-Algebras: 1943–1993. A Fifty Year Celebration'', AMS special session commemorating the first fifty years of C*-algebra theory, January 13–14, 1993, San Antonio, Texas, American Mathematical Society, pp.&amp;nbsp;21–54, {{ISBN|0-8218-5175-6}} ([https://www.google.com/books?id=DYCUp0JYU6sC&amp;printsec=frontcover#PPA3,M1 available from Google Books], see pp.&amp;nbsp;21 ff.)&lt;/ref&gt;

In his paper of 1947 Segal showed that it is sufficient, for any physical system that can be described by an algebra of operators on a Hilbert space, to consider the ''irreducible'' representations of a C*-algebra. In quantum theory this means that the C*-algebra is generated by the observables. This, as Segal pointed out, had been shown earlier by [[John von Neumann]] only for the specific case of the non-relativistic Schrödinger-Heisenberg theory.&lt;ref&gt;{{cite journal |author=[[I. E. Segal]]|title=Irreducible representations of operator algebras |journal=Bull. Am. Math. Soc. |volume=53 |issue= |year=1947 |pages=73–88 |url=http://www.ams.org/journals/bull/1947-53-02/S0002-9904-1947-08742-5/S0002-9904-1947-08742-5.pdf |doi=10.1090/s0002-9904-1947-08742-5}}&lt;/ref&gt;

==References==

* [[William Arveson]], ''An Invitation to C*-Algebra'', Springer-Verlag, 1981
*[[Kadison, Richard]], ''Fundamentals of the Theory of Operator Algebras, Vol. I : Elementary Theory'', American Mathematical Society. {{ISBN|978-0821808191}}.
* [[Jacques Dixmier]], ''Les C*-algèbres et leurs Représentations'', Gauthier-Villars, 1969.&lt;br/&gt;English translation: {{cite book
  | last =Dixmier
  | first =Jacques
  | authorlink =
  | coauthors =
  | title = C*-algebras
  | publisher =North-Holland
  | year = 1982
  | location =
  | pages =
  | url =
  | doi =
  | id =  
  | isbn = 0-444-86391-5}}
* Thomas Timmermann, ''An invitation to quantum groups and duality: from Hopf algebras to multiplicative unitaries and beyond'', European Mathematical Society, 2008, {{ISBN|978-3-03719-043-2}} – [https://books.google.com/books?id=S8sZiieo-04C&amp;pg=PA371 Appendix 12.1, section: GNS construction (p. 371)]
* Stefan Waldmann: ''On the representation theory of [[deformation quantization]]'', In: ''Deformation Quantization: Proceedings of the Meeting of Theoretical Physicists and Mathematicians, Strasbourg, May 31-June 2, 2001 (Studies in Generative Grammar) '', Gruyter, 2002, {{ISBN|978-3-11-017247-8}}, p.&amp;nbsp;107–134 – [https://books.google.com/books?id=xuq8CHNEFKoC&amp;pg=PA113 section 4. The GNS construction (p. 113)]
*{{cite book
 | author = G. Giachetta, L. Mangiarotti, [[Gennadi Sardanashvily|G. Sardanashvily]]
 | year = 2005
 | title = Geometric and Algebraic Topological Methods in Quantum Mechanics
 | publisher = World Scientific
 | isbn = 981-256-129-3
 | url =
}}

;Inline references:
{{reflist}}

{{DEFAULTSORT:Gelfand-Naimark-Segal construction}}
[[Category:Functional analysis]]
[[Category:C*-algebras]]
[[Category:Quantum field theory]]

[[ru:Алгебраическая квантовая теория]]</text>
      <sha1>fsywubutanf6z0tzn0spo6ruzdjhb3g</sha1>
    </revision>
  </page>
  <page>
    <title>Goudreau Museum of Mathematics in Art and Science</title>
    <ns>0</ns>
    <id>35841633</id>
    <revision>
      <id>792371753</id>
      <parentid>787792344</parentid>
      <timestamp>2017-07-26T03:40:52Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Speedily moving category Defunct museums in New York to [[:Category:Defunct museums in New York (state)]] per [[WP:CFDS|CFDS]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2197">{{Infobox museum
| name             = Goudreau Museum of Mathematics in Art and Science
| native_name      = 
| native_name_lang = 
| image            = 
| imagesize        = 
| caption          = 
| alt              = 
| map_type         = 
| map_caption      = 
| map_alt          = 
| coordinates      = 
| established      = 1980
| dissolved        = 2006
| location         = Herricks Community Center &lt;br/&gt;999 Herricks Road&lt;br/&gt;[[New Hyde Park]], [[New York (state)|New York]], [[United States]]
| type             = 
| collection       = 
| visitors         = 
| director         = Glen Whitney
| president        = 
| curator          = 
| publictransit    = 
| car_park         = 
| network          = 
| website          = http://www.mathmuseum.org/
}}

The '''Goudreau Museum of Mathematics in Art and Science''' was a museum of math that was open from 1980–2006 in [[Long Island]], [[New York (state)|New York]].&lt;ref&gt;{{citation |url=http://www.mathfactory.org/Goudreau+Museum |title=Goudreau Museum |publisher=www.mathfactory.org}}&lt;/ref&gt; The museum was named after mathematics teacher [[Bernhard Goudreau]], who had died in 1985, and featured many of the 3-dimensional solid models, oversized wooden math games, and puzzles built by Goudreau and his former students.&lt;ref&gt;{{citation|url=http://www.mathmuseum.org/20thanniversary.htm |title=Goudreau Museum of Mathematics in Art and Science: 20th Anniversary |publisher=www.mathmuseum.org |deadurl=yes |archiveurl=https://web.archive.org/web/20120214122940/http://www.mathmuseum.org/20thanniversary.htm |archivedate=2012-02-14 |df= }}&lt;/ref&gt; After the museum closed, [[Glen Whitney]], a former math professor, decided to open the [[Museum of Mathematics]] in Manhattan (New York City), which opened in December 2012.&lt;ref&gt;{{citation |url=https://www.nytimes.com/2011/06/28/science/28math.html |title=One Math Museum, Many Variables |publisher=[[New York Times]]}}&lt;/ref&gt;

==References==
{{reflist}}

{{Mathematical art}}

[[Category:Defunct museums in New York (state)]]
[[Category:Museums established in 1980]]
[[Category:Museums disestablished in 2006]]
[[Category:Museums in Nassau County, New York]]
[[Category:Mathematics education]]</text>
      <sha1>m3m1h3z7tdkzb2xjslhaatbn0bohf1p</sha1>
    </revision>
  </page>
  <page>
    <title>HAS-160</title>
    <ns>0</ns>
    <id>1063408</id>
    <revision>
      <id>562995525</id>
      <parentid>541169499</parentid>
      <timestamp>2013-07-05T16:51:17Z</timestamp>
      <contributor>
        <username>Htexmexh</username>
        <id>17044386</id>
      </contributor>
      <comment>/* External links */ fixed broken link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="826">'''HAS-160''' is a [[cryptographic hash function]] designed for use with the [[Korea]]n [[KCDSA]] [[digital signature]] algorithm.  It is derived from [[SHA-1]], with assorted changes intended to increase its security. It produces a 160-bit output.

HAS-160 is used in the same way as SHA-1. First it divides input in blocks of 512 bits each and pads the final block. A digest function updates the intermediate hash value by processing the input blocks in turn.

The message digest algorithm consists of 80 rounds.

==External links==
* [http://www.randombit.net/has160.html A description of HAS-160, and some test vectors.]
* [http://rhash.sourceforge.net/ RHash], an [[open source]] command-line tool capable of calculating HAS-160.

{{Cryptography navbox | hash}}

[[Category:Cryptographic hash functions]]

{{crypto-stub}}</text>
      <sha1>svr0ium2njq0p8z376a6hkste7bwdso</sha1>
    </revision>
  </page>
  <page>
    <title>Hilbert's paradox of the Grand Hotel</title>
    <ns>0</ns>
    <id>37797</id>
    <revision>
      <id>870830992</id>
      <parentid>869150534</parentid>
      <timestamp>2018-11-27T07:55:31Z</timestamp>
      <contributor>
        <username>Caps Loch</username>
        <id>30035926</id>
      </contributor>
      <minor/>
      <comment>/* References in fiction */ remove dangling links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17037">{{more footnotes|date=February 2016}}
'''Hilbert's paradox of the Grand Hotel'''  ([[colloquial]]: '''Infinite Hotel Paradox''' or '''Hilbert's Hotel''') is a [[thought experiment]] which illustrates a [[counterintuitive]] property of infinite sets. It is demonstrated that a fully occupied hotel with infinitely many rooms may still accommodate additional guests, even infinitely many of them, and this process may be repeated infinitely often. The idea was introduced by [[David Hilbert]] in a 1924 lecture "Über das Unendliche", reprinted in {{harv|Hilbert|2013|loc=p.730}}, and was popularized through [[George Gamow]]'s 1947 book ''[[One Two Three... Infinity]]''.&lt;ref&gt;{{cite arxiv |last=Kragh |first=Helge |title=The True (?) Story of Hilbert's Infinite Hotel |year=2014 |eprint=1403.0059 }}&lt;/ref&gt;&lt;ref&gt;{{cite book |last=Gamow |first=George |year=1947 |title=One Two Three... Infinity: Facts and Speculations of Science |location=New York |publisher=Viking Press |page=17 }}&lt;/ref&gt;

==The paradox==
Consider a hypothetical hotel with a [[countable set|countably infinite]] number of rooms, all of which are occupied. One might be tempted to think that the hotel would not be able to accommodate any newly arriving guests, as would be the case with a finite number of rooms, where the [[pigeonhole principle]] would apply.

===Finitely many new guests===
Suppose a new guest arrives and wishes to be accommodated in the hotel. We can (simultaneously) move the guest currently in room 1 to room 2, the guest currently in room 2 to room 3, and so on, moving every guest from his current room ''n'' to room ''n''+1.  After this, room 1 is empty and the new guest can be moved into that room. By repeating this procedure, it is possible to make room for any finite number of new guests.

===Infinitely many new guests===
It is also possible to accommodate a ''countably infinite'' number of new guests: just move the person occupying room 1 to room 2, the guest occupying room 2 to room 4, and, in general, the guest occupying room ''n'' to room 2''n'' (2 times ''n''),  and all the odd-numbered rooms (which are countably infinite) will be free for the new guests.

===Infinitely many coaches with infinitely many guests each===
{{Details|Pairing function}}
It is possible to accommodate countably infinitely many [[coach (bus)|coach]]loads of countably infinite passengers each, by several different methods. Most methods depend on the seats in the coaches being already numbered (or use the [[axiom of countable choice]]). In general any [[pairing function]] can be used to solve this problem. For each of these methods, consider a passenger's seat number on a coach to be &lt;math&gt;n&lt;/math&gt;, and their coach number to be &lt;math&gt;c&lt;/math&gt;, and the numbers &lt;math&gt;n&lt;/math&gt; and &lt;math&gt;c&lt;/math&gt; are then fed into the two arguments of the [[pairing function]].

====Prime powers method====

Empty the odd numbered rooms by sending the guest in room &lt;math&gt;i&lt;/math&gt; to room &lt;math&gt;2^i&lt;/math&gt;, then put the first coach's load in rooms &lt;math&gt;3^n&lt;/math&gt;, the second coach's load in rooms &lt;math&gt;5^n&lt;/math&gt;; for coach number &lt;math&gt;c&lt;/math&gt; we use the rooms &lt;math&gt;p^n&lt;/math&gt; where &lt;math&gt;p&lt;/math&gt; is the &lt;math&gt;c&lt;/math&gt;th odd [[prime number]]. This solution leaves certain rooms empty (which may or may not be useful to the hotel); specifically, all odd numbers that are not [[prime powers]], such as 15 or 847, will no longer be occupied. (So, strictly speaking, this shows that the number of arrivals is ''less than or equal to'' the number of vacancies created. It is easier to show, by an independent means, that the number of arrivals is also ''greater than or equal to'' the number of vacancies, and thus that they are ''equal'', than to modify the algorithm to an exact fit.) (The algorithm works equally well if one interchanges &lt;math&gt;n&lt;/math&gt; and &lt;math&gt;c&lt;/math&gt;, but whichever choice is made, it must be applied uniformly throughout.)

====Prime factorization method====

You can put each person of a certain seat &lt;math&gt;s&lt;/math&gt; and coach &lt;math&gt;c&lt;/math&gt; into room &lt;math&gt;2^s 3^c&lt;/math&gt; (presuming ''c''=0 for the people already in the hotel, 1 for the first coach, etc. ...). Because every number has a unique [[prime factorization]], it's easy to see all people will have a room, while no two people will end up in the same room. For example, the person in room 2592 (&lt;math&gt;2^5  3^4&lt;/math&gt;) was sitting in on the 4th coach, on the 5th seat. Like the prime powers method, this solution leaves certain rooms empty.

This method can also easily be expanded for infinite nights, infinite entrances, etc. ... ( &lt;math&gt;2^s 3^c 5^n 7^e&lt;/math&gt; )

==== Interleaving method ====

For each passenger, compare the lengths of &lt;math&gt;n&lt;/math&gt; and &lt;math&gt;c&lt;/math&gt; as written in any positional [[numeral system]], such as [[decimal]]. (Treat each hotel resident as being in coach #0.) If either number is shorter, add [[leading zero]]es to it until both values have the same number of digits. [[Interleave sequence|Interleave]] the digits to produce a room number: its digits will be [first digit of coach number]-[first digit of seat number]-[second digit of coach number]-[second digit of seat number]-etc. The hotel (coach #0) guest in room number 1729 moves to room 01070209 (i.e., room 1,070,209.) The passenger on seat 1234 of coach 789 goes to room 01728394 (or just 1728394).

Unlike the prime powers solution, this one fills the hotel completely, and we can reconstruct a guest's original coach and seat by reversing the interleaving process. First add a leading zero if the room has an odd number of digits. Then de-interleave the number into two numbers: the seat number consists of the odd-numbered digits and the coach number is the even-numbered ones. Of course, the original encoding is arbitrary, and the roles of the two numbers can be reversed (seat-odd and coach-even), so long as it is applied consistently.

====Triangular number method====

Those already in the hotel will be moved to room &lt;math&gt;(n^2+n)/2&lt;/math&gt;, or the &lt;math&gt;n&lt;/math&gt;th [[triangular number]].  Those in a coach will be in room &lt;math&gt; ((c+n-1)^2+c+n-1)/2+n&lt;/math&gt;, or the &lt;math&gt;(c+n-1)&lt;/math&gt; triangular number plus &lt;math&gt;n&lt;/math&gt;. In this way all the rooms will be filled by one, and only one, guest.

This pairing function can be demonstrated visually by structuring the hotel as a one-room-deep, infinitely tall [[pyramid]]. The pyramid's topmost row is a single room: room 1; its second row is rooms 2 and 3; and so on. The column formed by the set of rightmost rooms will correspond to the triangular numbers. Once they are filled (by the hotel's redistributed occupants), the remaining empty rooms form the shape of a pyramid exactly identical to the original shape. Thus, the process can be repeated for each infinite set. Doing this one at a time for each coach would require an infinite number of steps, but by using the prior formulas, a guest can determine what his room "will be" once his coach has been reached in the process, and can simply go there immediately.

====Arbitrary enumeration method====

Let &lt;math&gt; S := \{(a, b) \mid a, b \in \mathbb{N}\}&lt;/math&gt;. &lt;math&gt; S &lt;/math&gt; is countable since &lt;math&gt;\mathbb{N}&lt;/math&gt; is countable, hence we may enumerate its elements &lt;math&gt;s_1, s_2, \dots&lt;/math&gt;. Now if &lt;math&gt;s_n = (a, b)&lt;/math&gt;, assign the &lt;math&gt;b&lt;/math&gt;th guest of the &lt;math&gt;a&lt;/math&gt;th coach to the &lt;math&gt;n&lt;/math&gt;th room (consider the guests already in the hotel as guests of the &lt;math&gt;0&lt;/math&gt;th coach). Thus we have a function assigning each person to a room; furthermore, this assignment does not skip over any rooms.

===Further layers of infinity===

Suppose the hotel is next to an ocean, and an infinite number of [[Roll-on/roll-off|car ferries]] arrive, each bearing an infinite number of coaches, each with an infinite number of passengers. This is a situation involving three "levels" of [[infinity]], and it can be solved by extensions of any of the previous solutions.

The prime factorization method can be applied by adding a new prime number for every additional layer of infinity ( &lt;math&gt;2^s 3^c 5^f&lt;/math&gt;, with &lt;math&gt;f&lt;/math&gt; the ferry).

The prime power solution can be applied with further [[exponentiation]] of prime numbers, resulting in very large room numbers even given small inputs. For example, the passenger in the second seat of the third bus on the second ferry (address 2-3-2) would raise the 2nd odd prime (5) to 49, which is the result of the 3rd odd prime (7) being raised to the power of his seat number (2). This room number would have over thirty decimal digits.

The interleaving method can be used with three interleaved "strands" instead of two. The passenger with the address 2-3-2 would go to room 232, while the one with the address 4935-198-82217 would go to room #008,402,912,391,587 (the leading zeroes can be removed).

Anticipating the possibility of any number of layers of infinite guests, the hotel may wish to assign rooms such that no guest will need to move, no matter how many guests arrive afterward. One solution is to convert each arrival's address into a [[binary number]] in which ones are used as separators at the start of each layer, while a number within a given layer (such as a guests' coach number) is represented with that many zeroes. Thus, a guest with the prior address 2-5-1-3-1 (five infinite layers) would go to room 10010000010100010 (decimal 295458).

As an added step in this process, one zero can be removed from each section of the number; in this example, the guest's new room is 101000011001 (decimal 2585). This ensures that every room could be filled by a hypothetical guest. If no infinite sets of guests arrive, then only rooms that are a power of two will be occupied.

====Infinite layers of nesting====
Although a room can be found for any finite number of nested infinities of people, the same is not always true for an infinite number of layers, even if a finite number of elements exists at each layer.

The set of real numbers, and the set of guests in this example, is [[uncountably infinite]]. Because no one-to-one pairing can be made between countable and uncountable sets, rooms at the hotel cannot be made for all of these guests, although any countably infinite subset of them can still be accommodated.

If this variant is modified in certain ways, then the set of people is countable again. For example, suppose there ''were'' a largest ship, directly containing a finite (or countably infinite) number of both ships and people, and each of these ships in turn contained both ships and people, and so forth. This time, any given person is a finite number of levels "down" from the top, and thus can be identified with a unique finite address. The set of people is countable again, even if the total number of layers is infinite, because we do not have to consider an "infinitieth layer" in either direction.

== Analysis ==
Hilbert's paradox is a [[veridical paradox]]: it leads to a counter-intuitive result that is provably true. The statements "there is a guest to every room" and "no more guests can be accommodated" are not [[Logical equivalence|equivalent]] when there are infinitely many rooms.

Initially, this state of affairs might seem to be counter-intuitive. The properties of "infinite collections of things" are quite different from those of "finite collections of things". The paradox of Hilbert's Grand Hotel can be understood by using Cantor's theory of [[transfinite number]]s. Thus, while in an ordinary (finite) hotel with more than one room, the number of odd-numbered rooms is obviously smaller than the total number of rooms. However, in Hilbert's aptly named Grand Hotel, the quantity of odd-numbered rooms is not smaller than the total "number" of rooms. In mathematical terms, the [[cardinality]] of the [[subset]] containing the odd-numbered rooms is the same as the cardinality of the [[Set (mathematics)|set]] of all rooms. Indeed, infinite sets are characterized as sets that have proper subsets of the same cardinality. For countable sets (sets with the same cardinality as the [[natural number]]s) this cardinality is [[aleph-null|&lt;math&gt;\aleph_0&lt;/math&gt;]].&lt;ref name=Rucker&gt;{{cite book |last1=Rucker |first1=Rudy |authorlink=Rudy Rucker |title=Infinity and the Mind. The Science and Philosophy of the Infinite |date=1984 |origyear=1982&lt;!--Harvester Press--&gt; |publisher=Paladin |isbn=0-586-08465-7 |pages=73–78}}&lt;/ref&gt;

Rephrased, for any countably infinite set, there exists a [[bijective]] function which maps the countably infinite set to the set of natural numbers, even if the countably infinite set contains the natural numbers. For example, the set of rational numbers—those numbers which can be written as a quotient of integers—contains the natural numbers as a subset, but is no bigger than the set of natural numbers since the rationals are countable: there is a bijection from the naturals to the rationals.

==References in fiction==
* [[BBC Learning Zone]] repeatedly screened a 1996 one-off educational [[docu drama]] ''Hotel Hilbert'' set in the Hotel as seen through the eyes of a young female guest Fiona Knight, her name a pun on finite.  The programme was designed to educate viewers about the concept of infinity.&lt;ref&gt;https://www.imdb.com/title/tt0443537/&lt;/ref&gt;
* The novel ''[[White Light (novel)|White Light]]'' by [[mathematician]]/[[science fiction]] writer [[Rudy Rucker]] includes a hotel based on Hilbert's paradox, and where the protagonist of the story meets [[Georg Cantor]].
* [[Stephen Baxter (author)|Stephen Baxter]]'s science fiction novel ''[[Transcendent (novel)|Transcendent]]'' has a brief discussion on the nature of infinity, with an explanation based on the paradox, modified to use starship troopers rather than hotels.
* [[Geoffrey A. Landis]]' [[Nebula Award]]-winning short story "Ripples in the Dirac Sea" uses the Hilbert hotel as an explanation of why an infinitely-full [[Dirac sea]] can nevertheless still accept particles.
* In [[Peter Høeg]]'s novel ''[[Miss Smilla's Feeling for Snow]]'', the titular heroine reflects that it is admirable for the hotel's manager and guests to go to all that trouble so that the latecomer can have his own room and some privacy.
* In [[Ivar Ekeland]]'s novel for children, ''[[The Cat in Numberland]]'', a "Mr. Hilbert" and his wife run an infinite hotel for all the integers. The story progresses through the triangular method for the rationals.
* In Will Wiles's novel ''The Way Inn'', about an infinitely large motel, the villain's name is Hilbert.
* In Reginald Hill's novel "The Stranger House" the character Sam refers to the Hilbert Hotel paradox.
* The short story by [[Naum Ya. Vilenkin]] ''The Extraordinary Hotel'' (often erroneously attributed to [[Stanislaw Lem]]) shows the way in which Hilbert's Grand Hotel may be reshuffled when infinite new hosts arrive.
*[[John Roderick (musician)|John Roderick]] and [[Ken Jennings]] discussed the Hotel on their Omnibus podcast in the episode [https://www.omnibusproject.com/podcasts/the-hilbert-hotel-entry-587lk0207.htm ''The Hilbert Hotel Entry''].

==See also==
* [[List of paradoxes]]
* [[Banach–Tarski paradox]]
* [[Galileo's paradox]]
* [[Paradoxes of set theory]]
* [[Pigeonhole principle]]

==References==
{{Reflist|30em}}
*{{citation|last=Hilbert|first= David|year= 2013|title= David Hilbert’s Lectures on the Foundations of Arithmetics and Logic 1917-1933|editor-first= William |editor-last=Ewald|editor2-first= Wilfried |editor2-last=Sieg|place= Heidelberg|publisher= Springer-Verlag|isbn=978-3-540-20578-4 |doi=10.1007/978-3-540-69444-1}}

==External links==
* [http://eom.springer.de/h/h130080.htm Hilbert infinite hotel]. M. Hazewinkel. ''Encyclopedia of Mathematics'', Springer. Accessed May 25, 2007.
* Nancy Casey, [https://www.math.stonybrook.edu/~scott/mat200.fall06/hotel-infinity.pdf Welcome to the Hotel Infinity!]&amp;nbsp;— The paradox told as a humorous narrative, featuring a hotel owner and a building contractor based on the feuding 19th-century mathematicians [[Georg Cantor]] and [[Leopold Kronecker]]
* Steven Strogatz, [http://opinionator.blogs.nytimes.com/2010/05/09/the-hilbert-hotel/ The Hilbert Hotel], NY Times, May 9, 2010
* [http://www.bbc.co.uk/dna/h2g2/A4080467 Hilbert's Infinite Hotel], h2g2
* [https://www.youtube.com/user/Davidson1956#p/u/5/BRn_GNcglKo The Hilbert Hotel] - YouTube presentation
* [http://web.science.mq.edu.au/~chris/beyond/infinity.pdf "Beyond the Finite"]{{Dead link|date=April 2016}}
* see song on p.&amp;nbsp;704 of the October 2006 American Mathematical Monthly or p.&amp;nbsp;177 of the Dec. 2011 Journal of Mathematics and the Arts
* [http://ed.ted.com/lessons/the-infinite-hotel-paradox-jeff-dekofsky/ The Infinite Hotel Paradox - Jeff Dekofsky] - TED-Ed Lessons


[[Category:Paradoxes of set theory]]
[[Category:Supertasks]]
[[Category:Mathematics paradoxes]]
[[Category:Infinity]]
[[Category:Fictional hotels]]
[[Category:Paradoxes of infinity]]
[[Category:1920 introductions]]
[[Category:Logical paradoxes]]</text>
      <sha1>2sf8hk5a4wqpdmnnlengk12uvp3zgdb</sha1>
    </revision>
  </page>
  <page>
    <title>Hindley–Milner type system</title>
    <ns>0</ns>
    <id>32612385</id>
    <revision>
      <id>870519261</id>
      <parentid>870517345</parentid>
      <timestamp>2018-11-25T10:02:34Z</timestamp>
      <contributor>
        <username>AnomieBOT</username>
        <id>7611264</id>
      </contributor>
      <minor/>
      <comment>Dating maintenance tags: {{Citation needed}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="69203">A '''Hindley–Milner''' ('''HM''') '''type system''' is a classical [[type system]] for the [[lambda calculus]] with [[parametric polymorphism]]. It is also known as '''Damas–Milner''' or '''Damas–Hindley–Milner'''. It was first described by [[J. Roger Hindley]]&lt;ref&gt;{{cite journal | authorlink = J. Roger Hindley | first = J. Roger | last = Hindley | date = 1969 | title = The Principal Type-Scheme of an Object in Combinatory Logic | journal = Transactions of the American Mathematical Society | volume = 146 | pages = 29–60 | jstor = 1995158 | doi=10.2307/1995158}}&lt;/ref&gt; and later rediscovered by [[Robin Milner]].&lt;ref name="Milner"&gt;{{cite journal | authorlink = Robin Milner | last = Milner | first = Robin | date = 1978 | title = A Theory of Type Polymorphism in Programming | journal = Journal of Computer and System Sciences | volume = 17 | pages = 348–374 | citeseerx = 10.1.1.67.5276 | doi=10.1016/0022-0000(78)90014-4}}&lt;/ref&gt; Luis Damas contributed a close formal analysis and proof of the method in his PhD thesis.&lt;ref&gt;{{cite thesis | first = Luis | last = Damas | date = 1985 | title = Type Assignment in Programming Languages | degree = PhD  | publisher = University of Edinburgh (CST-33-85) }}&lt;/ref&gt;&lt;ref name="Damas"&gt;{{cite conference | last1 = Damas | first1 = Luis | authorlink2 = Robin Milner | last2 = Milner | first2 = Robin | date = 1982 | title = Principal type-schemes for functional programs | conference = 9th Symposium on Principles of programming languages (POPL'82) | pages = 207–212 | publisher = ACM | url = http://web.cs.wpi.edu/~cs4536/c12/milner-damas_principal_types.pdf }}&lt;/ref&gt;

Among HM's more notable properties are its completeness and its ability to infer the [[principal type|most general type]] of a given program without programmer-supplied [[type annotation]]s or other hints. [[#Algorithm W|Algorithm W]] is an efficient [[type inference]] method that performs in almost [[linear time]] with respect to the size of the source, making it practically useful to type large programs.&lt;ref group="note"&gt;Hindley–Milner is [[DEXPTIME]]-complete. Non-linear behaviour does manifest itself, yet only on [[Pathological (mathematics)|pathological]] inputs. Thus the complexity theoretic proofs by {{harvtxt|Mairson|1990}} and {{harvtxt|Kfoury|Tiuryn|Urzyczyn|1990}} came as a surprise to the research community. When the depth of nested let-bindings is bounded—which is the case in realistic programs—Hindley–Milner type inference becomes polynomial.&lt;/ref&gt; HM is preferably used for [[functional language]]s. It was first implemented as part of the type system of the programming language [[ML (programming language)|ML]]. Since then, HM has been extended in various ways, most notably with [[type class]] constraints like those in [[Haskell (programming language)|Haskell]].

==Introduction==

{{main|Type system}}

One and the same thing can be used for many purposes.  A [[chair]] might be used to support a sitting person but also as a [[ladder]] to stand on while changing a light bulb or as a [[clothes valet]]. Beside having particular material qualities, which make a chair usable as such, it also has the particular designation for its use. When no chair is at hand, other things might be used as a seat, and so the designation of a thing can be changed as fast as one can turn an empty bottle crate upside down to change its purpose from a container to that of a support.

Different uses of physically near-identical things are usually accompanied by giving those things different names to emphasize the intended purpose. Depending on the use, seamen have a dozen or more words for a [[rope]] though it might materially be the same thing. The same in everyday language, where a [[leash]] indicates a use different to a [[clothes line|line]].

In computer science, this practice of naming things by its intended use is put to an extreme called ''typing'' and the names or expressions called ''types'':

* Contrary to the richly varying raw materials in the physical world, computing has only one raw material, bytes, and much like letters in a book they are used to build up and express everything in a computer's memory.
* Programmers not only have many words indicating the different uses for bytes, but rather a plethora of formal ''type'' ''languages'', each having their own grammar.
* There are branches of research in computer science and mathematics dedicated to developing and enhancing such languages. Their theoretical foundations have [[type systems]] and [[type theory]] as their subjects.
* Types are not only used in specifications and documentations, but are also an integral part of programming languages. There, they are mechanised to strongly support the programmers' tasks.

Beside structuring objects, (data) types serve as means to validate that these objects are used as intended. Much like a crate that could only be used as a support or a container at a time, a particular arrangement of bytes designated for one purpose might exclude other possible uses.

In programming, these uses are expressed as ''functions'' or ''procedures'' which serve the role of verbs in [[natural language]]. As an example for typing verbs, an English dictionary might define ''gift'' as "to give so. sth.", indicating that the object must be a person and the indirect object a physical thing. In programming, "so." (someone) and "sth." (something) would be called types and using a thing into the place of "so." would be indicated as a programming error by a [[type checker]].

Beside checking, one can use the types in this example to gain knowledge about an unknown word. Reading the sentence "Mary gifts John a bilber" the types could be used to conclude that a "bilber" is likely a physical thing. This activity and conclusion is called ''type inference''. As the story unfolds, more and more information about the unknown "bilber" may be gained, and eventually enough details become known to form a complete image of that kind of thing.

The type inference method designed by [[J. Roger Hindley|Hindley]] and [[Robin Milner|Milner]] does just this for programming languages. The advantage of type inference over type checking is that it allows a more natural and dense style of programming. Instead of starting a program text with a glossary defining what a bilber and everything else is, one can distribute this information over the text simply by using the yet undefined words and let a program collect all the details about them. The method works for both nouns (data types) and for verbs (functions types).  As a consequence, a programmer can proceed without ever mentioning types at all, while still having the full support of a type checker that validates their writing. When reading a program, the programmer can use type inference to query the full definition of anything named in the program whenever needed.

=== History of type inference ===
Historically, type inference to this extent was developed for a particular group of programming languages, called [[functional languages]]. These started in 1958 with [[Lisp]], a programming language based on the [[lambda calculus]] and that compares well with modern [[scripting languages]] like [[Python (programming language)|Python]] or [[Lua (programming language)|Lua]]. Lisp was mainly used for computer science research, often for symbol manipulation purposes where large, tree-like data structures were common.

Data in Lisp is [[dynamic type|dynamically typed]] and the types are only available to some degree while running a program. Debugging type errors was no less of a concern than it is with modern script languages. But, being completely untyped, i.e. written without any explicit type information, maintaining large programs written in Lisp soon became a problem because the many complicated types of the data were mentioned only in the program documentation and comments at best.

Thus, the need to have a Lisp-like language with machine-checkable types became more and more pressing. At some point, programming language development faced two challenges:

# [[Polymorphism (computer science)|Polymorphism]]. Some kinds of data are very generic. In particular, Lisp is a "list programming language" where lists are data whose type can be a "list of something", e.g. a list of numbers.  Functions for such generic data types are often themselves generic. For instance, counting the number of items on a list is independent of the type of its items. However, a generic function that adds another item to a given list, needs type checking to ensure that the list will remain consistent with respect to the type of its items. For example, that only numbers may be added to a list of numbers.  Types for such "generic" data and functions are called ''polymorphic'', meaning that they can be used for more than one type. The polymorphic function for adding an item can be used for a list of numbers as well as for a list of words or even a list of anything. More precisely, this kind of polymorphism is called [[parametric polymorphism]], where "something" is the parameter in "list of something".  More formally, "list of something" may be written &lt;code&gt;List T&lt;/code&gt; with &lt;code&gt;T&lt;/code&gt; being the type parameter. The type of a function that adds a new item to a list is &lt;code&gt;forall T . T -&gt; List T -&gt; List T&lt;/code&gt;, meaning that for any and all types T the adding function needs an item of type T and a list-of-Ts, to produce a resulting (new) list-of-Ts.  Thus, the first challenge was, to design a type system that properly expressed parametric polymorphism.
# [[Type inference]]. Unfortunately, polymorphic functions requiring type checking must be continuously informed of the types. The above function would need the type T as an additional first parameter, resulting in program text so cluttered with type information that it becomes unreadable. Additionally, when keying in a program, a programmer would spend a significant portion of time keying in types.

As an example, polymorphically constructing the list "(1 2)" of two numbers would mean writing:

&lt;source lang=haskell&gt;
  (addItem Number 1 (addItem Number 2 (emptyList Number)))
&lt;/source&gt;

This example was quite typical. Every third word a type, monotonously serving the type checker in every step. This worsens when the types become more complex. Then, the methods to be expressed in code become buried in types.

To handle this issue, effective methods for type inference were the subject of research, and Hindley-Milner's method was one of them. Their method was first used in [[ML (programming language)|ML]] (1973) and is also used in an extended form in [[Haskell (programming language)|Haskell]] (1990). The HM type inference method is strong enough to infer types not only for expressions, but for whole programs including the procedures and local definitions, providing a type-less style of programming.

The following text gives an impression of the resulting programming style for the [[quicksort]] procedure in Haskell:

&lt;source lang="haskell"&gt;
quickSort []     = []                           -- Sort an empty list
quickSort (x:xs) = quickSort (filter (&lt;x) xs)   -- Sort the left part of the list
                   ++ [x] ++                    -- Insert pivot between two sorted parts
                   quickSort (filter (&gt;=x) xs)  -- Sort the right part of the list
&lt;/source&gt;

Though all of the functions in the above example need type parameters, types are nowhere mentioned. The code is statically type-checked even though the type of the function defined is unknown and must be inferred to type-check the applications in the body.

Over the years, other programming languages added their own version of parametric types. [[C++]] templates were introduced in 1998 and [[Java (programming language)|Java]] introduced [[Generics in Java|generics]] in 2004. As programming with type parameters became more common, problems similar to the ones sketched for Lisp surfaced in [[imperative programming|imperative languages]] too, perhaps not as pressing as it was for the functional languages. As a consequence, these languages obtained support for some type inference techniques, for instance "auto" in [[C++11#Type inference|C++11]] (2014). Unfortunately, the stronger type inference methods developed for functional programming cannot simply be integrated in the imperative languages, as their type systems' features are in part incompatible, and a programming language must be designed from the ground up when continuous type inference is wanted.

== Features of the Hindley–Milner method ==

Before presenting the HM type system and related algorithms, the following sections make some features of HM more formal and precise.

===Type-checking vs. type-inference===

In a typing, an expression E is opposed to a type T, formally written as E : T. Usually a typing only makes sense within some context, which is omitted here.

In this setting, the following questions are of particular interest:

# E : T? In this case, both an expression E and a type T are given. Now, is E really a T? This scenario is known as [[type-checking]].
# E : _? Here, only the expression is known. So, what type is E? If there is a way to derive a type for E, then we have accomplished [[type inference]].
# _ : T? The other way round. Given only a type, is there any expression for it or does the type have no values? Is there any example of a T? And in light of the [[Curry–Howard isomorphism]], is there a proof for T?

For the [[simply typed lambda calculus]], all three questions are decidable. The situation is not as comfortable when more expressive types are allowed. Additionally, the simply typed lambda calculus makes the types of the parameters of each function explicit, while they are not needed in HM. While HM is a method for type inference, it can be used also for type checking and answer the first question. To do that, a type is first inferred from E and then compared with the type wanted. The third question becomes of interest when looking at recursively-defined functions at the end of this article.

===Monomorphism vs. polymorphism===

{{main|parametric polymorphism}}

In the [[simply typed lambda calculus]], types &lt;math&gt;T&lt;/math&gt; are either atomic type constants or function types of form &lt;math&gt;T \rightarrow T&lt;/math&gt;.  Such types are ''monomorphic''. Typical examples are the types used in arithmetic values:

  3       : Number
  add 3 4 : Number
  add     : Number -&gt; Number -&gt; Number

Contrary to this, the untyped lambda calculus is neutral to typing at all, and many of its functions can be meaningfully applied to all type of arguments. The trivial example is the identity function

:id &lt;math&gt;\equiv \lambda&lt;/math&gt; x . x

which simply returns whatever value it is applied to. Less trivial examples include parametric types like lists.

While polymorphism in general means that operations accept values of more than one type, the polymorphism used here is parametric. One finds the notation of ''type schemes'' in the literature, too, emphasizing the parametric nature of the polymorphism. Additionally, constants may be typed with (quantified) type variables. E.g.:

  cons : forall a . a -&gt; List a -&gt; List a
  nil  : forall a . List a.
  id   : forall a . a -&gt; a

Polymorphic types can become monomorphic by consistent substitution of their variables. Examples of monomorphic ''instances'' are:

 id'  : String -&gt; String
 nil' : List Number

More generally, types are polymorphic when they contain type variables, while types without them are monomorphic.

Contrary to the type systems used for example in [[Pascal (programming language)|Pascal]] (1970) or [[C (programming language)|C]] (1972), which only support monomorphic types, HM is designed with emphasis on parametric polymorphism. The successors of the languages mentioned, like [[C++]] (1985), focused on different types of polymorphism, namely [[Polymorphism (computer science)#Subtyping|subtyping]] in connection with object-oriented programming and [[Polymorphism (computer science)#Ad hoc polymorphism|overloading]]. While subtyping is incompatible with HM, a variant of systematic overloading is available in the HM-based type system of Haskell.

===Let-polymorphism===

When extending the simply-typed lambda calculus towards polymorphism, one has to define when deriving an instance of a value is admissible. Ideally, this would be allowed with any use of a bound variable, as in:

  (λ id .  ... (id 3) ... (id "text") ... ) (λ x . x)

Unfortunately, type inference in such a system is not decidable.{{Citation needed|reasoning=Not obvious, under what rules.|date=November 2018}} Instead, HM provides ''let-polymorphism'' of the form

  '''let''' id = λ x . x
   '''in''' ... (id 3) ... (id "text") ...

restricting the binding mechanism in an extension of the expression syntax. Only values bound in a let construct are subject to instantiation, i.e. are polymorphic, while the parameters in lambda-abstractions are treated as being monomorphic.

==Overview==

The remainder of the article is more technical as it has to present the HM method as it is handled in the literature. It proceeds as follows:

* The HM type system is defined. This is done by describing a deduction system that makes precise what expressions have what type, if any.
* From there, it works towards an implementation of the type inference method. After introducing a syntax driven variant of the above deductive system, it sketches an efficient implementation (algorithm J), appealing mostly to the reader's metalogical intuition.
* Because it remains open whether algorithm J indeed realises the initial deduction system, a less efficient implementation (algorithm W), is introduced and its use in a proof is hinted.
* Finally, further topics related to the algorithm are discussed.

The same description of the deduction system is used throughout, even for the two algorithms, to make the various forms in which the HM method is presented directly comparable.

== The Hindley–Milner type system ==

The type system can be formally described by [[Formal grammar|syntax rules]] that fix a language for the expressions, types, etc. The presentation here of such a syntax is not too formal, in that it is written down not to study the [[Parse tree|surface grammar]], but rather the [[Abstract syntax|depth grammar]], and leaves some syntactical details open. This form of presentation is usual. Building on this, [[type rule]]s are used to define how expressions and types are related. As before, the form used is a bit liberal.

=== Syntax ===
{| class=infobox
|align=center style="background:#e0e0ff"|'''Expressions'''
|-
| &lt;math&gt;
  \begin{array}{lrll}
\\
  e &amp; =     &amp; x                                   &amp; \textrm{variable}\\
    &amp; \vert &amp; e_1\ e_2                            &amp; \textrm{application}\\
    &amp; \vert &amp; \lambda\ x\ .\ e                    &amp; \textrm{abstraction} \\
    &amp; \vert &amp; \mathtt{let}\ x = e_1\ \mathtt{in}\ e_2 &amp;\\
\\
  \end{array}
  &lt;/math&gt;
|-
|align=center style="background:#e0e0ff"|'''Types'''
|-
| &lt;math&gt;
  \begin{array}{llrll}
\\
  \textrm{mono} &amp; \tau   &amp;=     &amp; \alpha                    &amp; \ \textrm{variable} \\
                    &amp;        &amp;\vert &amp;  C\ \tau\dots\tau         &amp; \ \textrm{application} \\
  \textrm{poly} &amp; \sigma &amp;=    &amp; \tau                                           \\
                    &amp;        &amp;\vert&amp; \forall\ \alpha\ .\ \sigma &amp; \ \textrm{quantifier}\\
\\
  \end{array}
  &lt;/math&gt;
|-
|align=center style="background:#e0e0ff"|'''Context and Typing'''
|-
| &lt;math&gt;
\begin{array}{llrl}
\\
  \text{Context}     &amp; \Gamma &amp; = &amp; \epsilon\ \mathtt{(empty)}\\
                     &amp;        &amp; \vert&amp; \Gamma,\ x : \sigma\\
  \text{Typing}      &amp;        &amp; = &amp; \Gamma \vdash e : \sigma\\
\\
\end{array}
&lt;/math&gt;
|-
|align=center style="background:#e0e0ff"|'''Free Type Variables'''
|-
| &lt;math&gt;
\begin{array}{ll}
\\
\text{free}(\ \alpha\ ) &amp;=\ \left\{\alpha\right\}\\
\text{free}(\ C\ \tau_1\dots\tau_n\ ) &amp;=\ \bigcup\limits_{i=1}^n{\text{free}(\ \tau_i\ )} \\
\text{free}(\ \Gamma\ ) &amp;=\ \bigcup\limits_{x:\sigma \in \Gamma}\text{free}(\ \sigma\ )\\
\\
\text{free}(\ \forall\ \alpha\ .\ \sigma\ ) &amp;=\ \text{free}(\ \sigma\ )\  -\  \left\{\alpha\right\}\\
\text{free}(\ \Gamma \vdash e : \sigma\ )&amp;=\ \text{free}(\ \sigma\ )\ -\ \text{free}(\ \Gamma \ )\\
\\
\end{array}
&lt;/math&gt;
|}
The expressions to be typed are exactly those of the [[lambda calculus]] extended with a let-expression as shown in the adjacent table. Parentheses can be used to disambiguate an expression. The application is left-binding and binds stronger than abstraction or the let-in construct.

Types are syntactically split into two groups, monotypes and polytypes.&lt;ref group="note"&gt;Polytypes are called "type schemes" in the original article.&lt;/ref&gt;

==== Monotypes ====
Monotypes always designate a particular type. Monotypes &lt;math&gt;\tau&lt;/math&gt; are syntactically represented as [[Term (logic)|terms]].

Examples of monotypes include type constants like &lt;math&gt;\mathtt{int}&lt;/math&gt; or &lt;math&gt;\mathtt{string}&lt;/math&gt;, and parametric types like &lt;math&gt;\mathtt{Map\ (Set\ string)\ int}&lt;/math&gt;.   The later types are examples of ''applications'' of type functions, for example, from the set
&lt;math&gt; \{ \mathtt{Map^2,\ Set^1,\ string^0,\ int^0},\ \rightarrow^2 \} &lt;/math&gt;, 
where the superscript indicates the number of type parameters.  The complete set of type functions &lt;math&gt;C&lt;/math&gt; is arbitrary in HM,&lt;ref group="note"&gt;The parametric types &lt;math&gt;C\ \tau\dots\tau&lt;/math&gt; were not present in the original paper on HM and are not needed to present the method. None of the inference rules below will take care or even note them. The same holds for the non-parametric "primitive types" in said paper. All the machinery for polymorphic type inference can be defined without them. They have been included here for sake of examples but also because the nature of HM is all about parametric types. This comes from the function type &lt;math&gt;\tau\rightarrow\tau&lt;/math&gt;, hard-wired in the inference rules, below, which already has two parameters and has been presented here as only a special case.&lt;/ref&gt; except that it ''must'' contain at least &lt;math&gt;\rightarrow^2&lt;/math&gt;, the type of functions.  It is often written in infix notation for convenience.  For example, a function mapping integers to strings has type &lt;math&gt;\mathtt{int}\rightarrow \mathtt{string}&lt;/math&gt;. Again, parentheses can be used to disambiguate a type expression. The application binds stronger than the infix arrow, which is right-binding.

Type variables are admitted as monotypes. Monotypes are not to be confused with monomorphic types, which exclude variables and allow only ground terms.

Two monotypes are equal if they have identical terms.

==== Polytypes ====
''Polytypes'' (or ''type schemes'') are types containing variables bound by one or more for-all quantifiers, e.g. &lt;math&gt;\forall\alpha.\alpha\rightarrow\alpha&lt;/math&gt;.

A function with polytype &lt;math&gt;\forall\alpha.\alpha\rightarrow\alpha&lt;/math&gt; can map ''any'' value of the same type to itself,
and the [[identity function]] is a value for this type.

As another example &lt;math&gt;\forall\alpha.(\mathtt{Set}\ \alpha)\rightarrow \mathtt{int}&lt;/math&gt; is the type of a function mapping all finite sets to integers. A function which returns the [[cardinality]] of a set would be a value of this type.

Note that quantifiers can only appear top level, i.e. a type &lt;math&gt;\forall\alpha.\alpha\rightarrow\forall\alpha.\alpha&lt;/math&gt; for instance, is excluded by the syntax of types. Note also that monotypes are included in the polytypes, thus a type has the general form &lt;math&gt;\forall\alpha_1\dots\forall\alpha_n.\tau&lt;/math&gt;, where &lt;math&gt;\tau&lt;/math&gt; is a monotype.

Equality of polytypes is up to reordering the quantification and renaming the quantified variables (&lt;math&gt;\alpha&lt;/math&gt;-conversion). Further, quantified variables not occurring in the monotype can be dropped.

==== Context and Typing ====

To meaningfully bring together the still disjoint parts (syntax expressions and types) a third part is needed: context. Syntactically, a context is a list of pairs &lt;math&gt;x:\sigma&lt;/math&gt;, called [[Assignment (mathematical logic)|assignments]], [[:wikt:assumption|assumptions]] or [[Name binding|bindings]], each pair stating that value variable &lt;math&gt;x_i&lt;/math&gt;has type &lt;math&gt;\sigma_i&lt;/math&gt;. All three parts combined give a ''typing judgment'' of the form &lt;math&gt;\Gamma\ \vdash\ e:\sigma&lt;/math&gt;,
stating that under assumptions &lt;math&gt;\Gamma&lt;/math&gt;, the expression &lt;math&gt;e&lt;/math&gt; has type &lt;math&gt;\sigma&lt;/math&gt;.

==== Free type variables ====

In a type &lt;math&gt;\forall\alpha_1\dots\forall\alpha_n.\tau&lt;/math&gt;, the symbol &lt;math&gt;\forall&lt;/math&gt; is the quantifier binding the type variables &lt;math&gt;\alpha_i&lt;/math&gt; in the monotype &lt;math&gt;\tau&lt;/math&gt;. The variables &lt;math&gt;\alpha_i&lt;/math&gt; are called ''quantified'' and any occurrence of a quantified type variable in &lt;math&gt;\tau&lt;/math&gt; is called ''bound'' and all unbound type variables in &lt;math&gt;\tau&lt;/math&gt; are called ''free''. Additionally to the quantification &lt;math&gt;\forall&lt;/math&gt; in polytypes, type variables can also be bound by occurring in the context, but with the inverse effect on the right hand side of the &lt;math&gt;\vdash&lt;/math&gt;. Such variables then behave like type constants there. Finally, a type variable may legally occur unbound in a typing, in which case they are implicitly all-quantified.

The presence of both bound and unbound type variables is a bit uncommon in programming languages. Often, all type variables are implicitly treated all-quantified. For instance, one does not have clauses with free variables in [[Prolog]]. Likely in Haskell, in the absence of the ScopedTypeVariables language extension, all type variables implicitly occur quantified, i.e. a Haskell type &lt;code&gt;a -&gt; a&lt;/code&gt; means &lt;math&gt;\forall\alpha.\alpha\rightarrow\alpha&lt;/math&gt; here.

=== Type order ===
{{main|Unification (computer science)#Substitution}}

Polymorphism means that one and the same expression can have (perhaps
infinitely) many types. But in this type system, these types are not completely
unrelated, but rather orchestrated by the parametric polymorphism.

As an example, the identity &lt;math&gt;\lambda x . x&lt;/math&gt; can have &lt;math&gt;\forall
\alpha . \alpha \rightarrow \alpha&lt;/math&gt; as its type as well as
&lt;math&gt;\texttt{string} \rightarrow \texttt{string}&lt;/math&gt; or &lt;math&gt;\texttt{int}
\rightarrow \texttt{int}&lt;/math&gt; and many others, but not &lt;math&gt;\texttt{int}
\rightarrow \texttt{string}&lt;/math&gt;. The most general type for this function is
&lt;math&gt;\forall \alpha . \alpha\rightarrow \alpha&lt;/math&gt;, while the
others are more specific and can be derived from the general one by consistently
replacing another type for the ''type parameter'', i.e. the quantified
variable &lt;math&gt;\alpha&lt;/math&gt;.  The counter-example fails because the
replacement is not consistent.

The consistent replacement can be made formal by applying a [[Unification (computer science)#Substitution|substitution]] &lt;math&gt;S = \left\{\ a_i \mapsto \tau_i,\ \dots\ \right\}&lt;/math&gt; to the term of a type &lt;math&gt;\tau&lt;/math&gt;, written &lt;math&gt;S\tau&lt;/math&gt;. As the example suggests, substitution is not only strongly related to an order, that expresses that a type is more or less special, but also with the all-quantification which allows the substitution to be applied.

{| class=infobox
|align=center style="background:#e0e0ff"|'''Specialization Rule'''
|-
| &lt;math&gt;\displaystyle\frac{\tau' = \left\{\alpha_i \mapsto \tau_i\right\} \tau \quad \beta_i \not\in \textrm{free}(\forall \alpha_1...\forall\alpha_n . \tau)}{\forall \alpha_1...\forall\alpha_n . \tau \sqsubseteq \forall \beta_1...\forall\beta_m . \tau'}&lt;/math&gt;
|}
Formally, in HM, a type &lt;math&gt;\sigma&lt;/math&gt; is 
more general than &lt;math&gt;\sigma'&lt;/math&gt;, formally &lt;math&gt;\sigma \sqsubseteq
\sigma'&lt;/math&gt; if some quantified variable in &lt;math&gt;\sigma&lt;/math&gt; is
consistently substituted such that one gains &lt;math&gt;\sigma'&lt;/math&gt; as shown in the side bar.
This order is part of the type definition of the type system.

While substituting a monomorphic (ground) type for a quantified variable is
straight forward, substituting a polytype has some pitfalls caused by the
presence of free variables. Most particularly, unbound variables must not be
replaced. They are treated as constants here. Additionally, note
that quantifications can only occur top-level. Substituting a parametric type,
one has to lift its quantors. The table on the right makes the rule precise.

Alternatively, consider an equivalent notation for the polytypes without
quantors in which quantified variables are represented by a different set of
symbols. In such a notation, the specialization reduces to plain consistent
replacement of such variables.

The relation &lt;math&gt;\sqsubseteq&lt;/math&gt; is a [[partial order]]
and  &lt;math&gt;\forall \alpha . \alpha&lt;/math&gt; is its smallest element.

==== Principal type ====
While specialization of a type scheme is one use of the order, it plays a
crucial second role in the type system. Type inference with polymorphism
faces the challenge of summarizing all possible types an expression may have.
The order guarantees that such a summary exists as the most general type
of the expression.

==== Substitution in typings ====

The type order defined above can be extended to typings because the implied all-quantification of typings enables consistent replacement:
:&lt;math&gt;
\Gamma \vdash e : \sigma \quad\Longrightarrow\quad S\Gamma \vdash e : S\sigma
&lt;/math&gt;
Contrary to the specialisation rule, this is not part of the definition, but like the implicit all-quantification rather a consequence of the type rules defined next.
Free type variables in a typing serve as placeholders for possible refinement. The binding effect of the environment to free type
variables on the right hand side of &lt;math&gt;\vdash&lt;/math&gt; that prohibits their substitution in the specialisation rule is again
that a replacement has to be consistent and would need to include the whole typing.

=== Deductive system ===
{| class=infobox
|align=center style="background:#e0e0ff"|'''The Syntax of Rules'''
|-
| &lt;math&gt;
\begin{array}{lrl}
  \text{Predicate}  &amp; =      &amp;\sigma\sqsubseteq\sigma'\\
                    &amp; \vert\ &amp;\alpha\not\in free(\Gamma)\\
                    &amp; \vert\ &amp;x:\alpha\in \Gamma\\
\\
  \text{Judgment}   &amp; =      &amp;\text{Typing}\\
  \text{Premise}    &amp; =      &amp;\text{Judgment}\ \vert\ \text{Predicate}\\
  \text{Conclusion} &amp; =      &amp;\text{Judgment}\\
\\
  \text{Rule}       &amp; =      &amp;\displaystyle\frac{\textrm{Premise}\ \dots}{\textrm{Conclusion}}\quad [\mathtt{Name}]
\end{array}
&lt;/math&gt;
|}

The syntax of HM is carried forward to the syntax of the [[Rule of inference|inference rules]] that form the body of the [[formal system]], by using the typings as [[Judgment (mathematical logic)|judgments]]. Each of the rules define what conclusion could be drawn from what premises. Additionally to the judgments, some extra conditions introduced above might be used as premises, too.

A proof using the rules is a sequence of judgments such that all premises are listed before a conclusion. The examples below show a possible format of proofs. From left to right, each line shows the conclusion, the &lt;math&gt;[\mathtt{Name}]&lt;/math&gt; of the rule applied and the premises, either by referring to an earlier line (number) if the premise is a judgment or by making the predicate explicit.

==== Typing rules ====
{{see also|Type rules}}
{| class=infobox
|align=center style="background:#e0e0ff"|'''Declarative Rule System'''
|-
| &lt;math&gt;
\begin{array}{cl}
 \displaystyle\frac{x:\sigma \in \Gamma}{\Gamma \vdash_D x:\sigma}&amp;[\mathtt{Var}]\\ \\
 \displaystyle\frac{\Gamma \vdash_D e_0:\tau \rightarrow \tau' \quad\quad \Gamma \vdash_D e_1 : \tau }{\Gamma \vdash_D e_0\ e_1 : \tau'}&amp;[\mathtt{App}]\\ \\
 \displaystyle\frac{\Gamma,\;x:\tau\vdash_D e:\tau'}{\Gamma \vdash_D \lambda\ x\ .\ e : \tau \rightarrow \tau'}&amp;[\mathtt{Abs}]\\ \\
 \displaystyle\frac{\Gamma \vdash_D e_0:\sigma \quad\quad \Gamma,\,x:\sigma \vdash_D e_1:\tau}{\Gamma \vdash_D \mathtt{let}\ x = e_0\ \mathtt{in}\ e_1 : \tau} &amp;[\mathtt{Let}]\\ \\ \\
 \displaystyle\frac{\Gamma \vdash_D e:\sigma' \quad \sigma' \sqsubseteq \sigma}{\Gamma \vdash_D e:\sigma}&amp;[\mathtt{Inst}]\\ \\
 \displaystyle\frac{\Gamma \vdash_D e:\sigma \quad \alpha \notin \text{free}(\Gamma)}{\Gamma \vdash_D e:\forall\ \alpha\ .\ \sigma}&amp;[\mathtt{Gen}]\\ \\
 \end{array}&lt;/math&gt;
|}

The side box shows the deduction rules of the HM type system. One can roughly divide the rules into two groups:

The first four rules &lt;math&gt;[\mathtt{Var}]&lt;/math&gt; (variable or function access), &lt;math&gt;[\mathtt{App}]&lt;/math&gt; (''application'', i.e. function call with one parameter), &lt;math&gt;[\mathtt{Abs}]&lt;/math&gt; (''abstraction'', i.e. function declaration) and &lt;math&gt;[\mathtt{Let}]&lt;/math&gt; (variable declaration) are centered around the syntax, presenting one rule for each of the expression forms. Their meaning is obvious at the first glance, as they decompose each expression, prove their sub-expressions and finally combine the individual types found in the premises to the type in the conclusion.

The second group is formed by the  remaining two rules &lt;math&gt;[\mathtt{Inst}]&lt;/math&gt; and &lt;math&gt;[\mathtt{Gen}]&lt;/math&gt;.
They handle specialization and generalization of types. While the rule &lt;math&gt;[\mathtt{Inst}]&lt;/math&gt; should be clear from the section on specialization above, &lt;math&gt;[\mathtt{Gen}]&lt;/math&gt; complements the former, working in the opposite direction. It allows generalization, i.e. to quantify monotype variables not bound in the context.
 
The following two examples exercise the rule system in action. Since both the expression and the type are given, they are a type-checking use of the rules.

'''Example''': A proof for &lt;math&gt;\Gamma \vdash_D id(n):int&lt;/math&gt; where &lt;math&gt;\Gamma = id:\forall \alpha . \alpha\rightarrow\alpha,\ n:int&lt;/math&gt;,
could be written

: &lt;math&gt;\begin{array}{llll}
1:&amp;\Gamma \vdash_D id : \forall\alpha.\alpha \rightarrow \alpha  &amp;[\mathtt{Var}]&amp; (id : \forall\alpha.\alpha \rightarrow \alpha \in \Gamma) \\
2:&amp;\Gamma \vdash_D id : int \rightarrow int &amp; [\mathtt{Inst}]&amp;(1),\ (\forall\alpha.\alpha \rightarrow \alpha \sqsubseteq int\rightarrow int)\\
3:&amp;\Gamma \vdash_D n : int&amp;[\mathtt{Var}]&amp;(n : int \in \Gamma)\\
4:&amp;\Gamma \vdash_D id(n) : int&amp;[\mathtt{App}]&amp; (2),\ (3)\\
\end{array}
&lt;/math&gt;

'''Example''': To demonstrate generalization,
&lt;math&gt;\vdash_D\ \textbf{let}\, id = \lambda x . x\ \textbf{in}\ id\, :\, \forall\alpha.\alpha\rightarrow\alpha&lt;/math&gt;
is shown below:

: &lt;math&gt;
\begin{array}{llll}
1: &amp; x:\alpha \vdash_D x : \alpha &amp; [\mathtt{Var}] &amp; (x:\alpha \in \left\{x:\alpha\right\})\\
2: &amp; \vdash_D \lambda x.x : \alpha\rightarrow\alpha &amp; [\mathtt{Abs}] &amp; (1)\\
3: &amp; \vdash_D \lambda x.x : \forall \alpha.\alpha\rightarrow\alpha &amp; [\mathtt{Gen}] &amp; (2),\ (\alpha \not\in free(\epsilon))\\
4: &amp; id:\forall \alpha.\alpha\rightarrow\alpha \vdash_D id : \forall \alpha.\alpha\rightarrow\alpha &amp; [\mathtt{Var}] &amp; (id:\forall \alpha.\alpha\rightarrow\alpha \in \left\{id : \forall \alpha.\alpha\rightarrow\alpha\right\})\\
5: &amp; \vdash_D \textbf{let}\, id = \lambda x . x\ \textbf{in}\  id\, :\,\forall\alpha.\alpha\rightarrow\alpha  &amp; [\mathtt{Let}] &amp; (3),\ (4)\\
\end{array}
&lt;/math&gt;

=== Let-polymorphism ===
Not visible immediately, the rule set encodes a regulation under which circumstances a type might be generalized or not by a slightly varying use of mono- and polytypes in the rules &lt;math&gt;[\mathtt{Abs}]&lt;/math&gt; and &lt;math&gt;[\mathtt{Let}]&lt;/math&gt;. Remember that &lt;math&gt;\sigma&lt;/math&gt; and &lt;math&gt;\tau&lt;/math&gt; denote poly- and monotypes respectively.

In rule &lt;math&gt;[\mathtt{Abs}]&lt;/math&gt;, the value variable of the parameter of the function &lt;math&gt;\lambda x.e&lt;/math&gt; is added to the context with a monomorphic type through the premise &lt;math&gt;\Gamma,\ x:\tau \vdash_D e:\tau'&lt;/math&gt;, while in the rule  &lt;math&gt;[\mathtt{Let}]&lt;/math&gt;, the variable enters the environment in polymorphic form &lt;math&gt;\Gamma,\ x:\sigma \vdash_D e_1:\tau&lt;/math&gt;. Though in both cases the presence of &lt;math&gt;x&lt;/math&gt; in the context prevents the use of the generalisation rule for any free variable in the assignment, this regulation forces the type of parameter &lt;math&gt;x&lt;/math&gt; in a &lt;math&gt;\lambda&lt;/math&gt;-expression to remain monomorphic, while in a let-expression, the variable could be introduced polymorphic, making specializations possible.

As a consequence of this regulation, &lt;math&gt;\lambda f.(f\, \textrm{true}, f\, \textrm{0})&lt;/math&gt; cannot be typed,
since the parameter &lt;math&gt;f&lt;/math&gt; is in a monomorphic position, while &lt;math&gt;\textbf{let}\ f = \lambda x . x\, \textbf{in}\, (f\, \textrm{true}, f\, \textrm{0})&lt;/math&gt; has type &lt;math&gt;(bool, int)&lt;/math&gt;, because &lt;math&gt;f&lt;/math&gt; has been introduced in a let-expression and is treated polymorphic therefore.

===Generalization rule===

The generalisation rule is also worth for closer look. Here, the all-quantification implicit in the premise &lt;math&gt;\Gamma \vdash e : \sigma&lt;/math&gt; is simply moved to the right hand side of &lt;math&gt;\vdash_D&lt;/math&gt; in the conclusion. This is possible, since &lt;math&gt;\alpha&lt;/math&gt; does not occur free in the context. Again, while this makes the generalisation rule plausible, it is not really a consequence. Vis versa, the generalisation rule is part of the definition of HM's type system and the implicit all-quantification a consequence.

== An inference algorithm ==
Now that the deduction system of HM is at hand, one could present an algorithm and validate it with respect to the rules.
Alternatively, it might be possible to derive it by taking a closer look on how the rules interact and proof are
formed. This is done in the remainder of this article focusing on the possible decisions one can make while proving a typing.

=== Degrees of freedom choosing the rules ===
Isolating the points in a proof, where no decision is possible at all,
the first group of rules centered around the syntax leaves no choice since
to each syntactical rule corresponds a unique typing rule, which determines
a part of the proof, while between the conclusion and the premises of these
fixed parts chains of &lt;math&gt;[\mathtt{Inst}]&lt;/math&gt; and &lt;math&gt;[\mathtt{Gen}]&lt;/math&gt;
could occur. Such a chain could also exist between the conclusion of the
proof and the rule for topmost expression. All proofs must have
the so sketched shape.

Because the only choice in a proof with respect of rule selection are the
&lt;math&gt;[\mathtt{Inst}]&lt;/math&gt; and &lt;math&gt;[\mathtt{Gen}]&lt;/math&gt; chains, the
form of the proof suggests the question whether it can be made more precise,
where these chains might be needed. This is in fact possible and leads to a
variant of the rules system with no such rules.

=== Syntax-directed rule system ===
{| class=infobox
|align=center style="background:#e0e0ff"|'''Syntactical Rule System'''
|-
| &lt;math&gt;
\begin{array}{cl}
\displaystyle\frac{x:\sigma \in \Gamma \quad \sigma \sqsubseteq \tau}{\Gamma \vdash_S x:\tau}&amp;[\mathtt{Var}]\\ \\
\displaystyle\frac{\Gamma \vdash_S e_0:\tau \rightarrow \tau' \quad\quad \Gamma \vdash_S e_1 : \tau }{\Gamma \vdash_S e_0\ e_1 : \tau'}&amp;[\mathtt{App}]\\ \\
\displaystyle\frac{\Gamma,\;x:\tau\vdash_S e:\tau'}{\Gamma \vdash_S \lambda\ x\ .\ e : \tau \rightarrow \tau'}&amp;[\mathtt{Abs}]\\ \\
\displaystyle\frac{\Gamma \vdash_S e_0:\tau \quad\quad \Gamma,\,x:\bar{\Gamma}(\tau) \vdash_S e_1:\tau'}{\Gamma \vdash_S \mathtt{let}\ x = e_0\ \mathtt{in}\ e_1 :  \tau'}&amp;[\mathtt{Let}]
\end{array}
&lt;/math&gt;
|-
|align=center style="background:#e0e0ff"|'''Generalization'''
|-
| &lt;math&gt;
\bar{\Gamma}(\tau) = \forall\ \hat{\alpha}\ .\ \tau \quad\quad \hat{\alpha} = \textrm{free}(\tau) - \textrm{free}(\Gamma)
&lt;/math&gt;
|}

A contemporary treatment of HM uses a purely [[syntax-directed]] rule system due to
Clement&lt;ref&gt;{{cite conference | last = Clement | date = 1986 | title = A Simple Applicative Language: Mini-ML | conference = LFP'86 | publisher = ACM}}&lt;/ref&gt;
as an intermediate step. In this system, the specialization is located directly after the original &lt;math&gt;[\mathtt{Var}]&lt;/math&gt; rule
and merged into it, while the generalization becomes part of the &lt;math&gt;[\mathtt{Let}]&lt;/math&gt; rule. There the generalization is
also determined to always produce the most general type by introducing the function &lt;math&gt;\bar{\Gamma}(\tau)&lt;/math&gt;, which quantifies
all monotype variables not bound in &lt;math&gt;\Gamma&lt;/math&gt;.

Formally, to validate, that this new rule system &lt;math&gt;\vdash_S&lt;/math&gt; is equivalent to the original &lt;math&gt;\vdash_D&lt;/math&gt;, one has
to show that &lt;math&gt;\Gamma \vdash_D\ e:\sigma \Leftrightarrow \Gamma \vdash_S\ e:\sigma&lt;/math&gt;, which falls apart into two sub-proofs:

* &lt;math&gt;\Gamma \vdash_D\ e:\sigma \Leftarrow \Gamma \vdash_S\ e:\sigma&lt;/math&gt; ([[Consistency]])
* &lt;math&gt;\Gamma \vdash_D\ e:\sigma \Rightarrow \Gamma \vdash_S\ e:\sigma&lt;/math&gt; ([[Completeness (logic)|Completeness]])

While consistency can be seen by decomposing the rules &lt;math&gt;[\mathtt{Let}]&lt;/math&gt; and &lt;math&gt;[\mathtt{Var}]&lt;/math&gt;
of &lt;math&gt;\vdash_S&lt;/math&gt; into proofs in &lt;math&gt;\vdash_D&lt;/math&gt;, it is likely visible that &lt;math&gt;\vdash_S&lt;/math&gt; is incomplete, as
one cannot show &lt;math&gt;\lambda\ x.x:\forall\alpha.\alpha\rightarrow\alpha&lt;/math&gt; in &lt;math&gt;\vdash_S&lt;/math&gt;, for instance, but only
&lt;math&gt;\lambda\ x.x:\alpha\rightarrow\alpha&lt;/math&gt;.  An only slightly weaker version of completeness is provable
&lt;ref name=x&gt;{{cite journal | first = Jeff | last = Vaughan | archiveurl = https://web.archive.org/web/20120324105848/http://www.cs.ucla.edu/~jeff/docs/hmproof.pdf | archivedate = 2012-03-24 | url = http://www.cs.ucla.edu/~jeff/docs/hmproof.pdf | title = A proof of correctness for the Hindley–Milner type inference algorithm | origyear = May 5, 2005 | date = July 23, 2008 }}&lt;/ref&gt; though, namely

* &lt;math&gt;\Gamma \vdash_D\ e:\sigma \Rightarrow \Gamma \vdash_S\ e:\tau \wedge \bar{\Gamma}(\tau)\sqsubseteq\sigma&lt;/math&gt;

implying, one can derive the principal type for an expression in &lt;math&gt;\vdash_S&lt;/math&gt; allowing us to generalize the proof in the end.

Comparing &lt;math&gt;\vdash_D&lt;/math&gt; and &lt;math&gt;\vdash_S&lt;/math&gt;, note that now only monotypes appear in the judgments of all rules. Additionally, the shape of any possible proof with the deduction system is now identical to the shape of the expression (both seen as [[Term (logic)#Formal definition|trees]]). Thus the expression fully determines the shape of the proof. In &lt;math&gt;\vdash_D&lt;/math&gt; the shape would likely be determined with respect to all rules except &lt;math&gt;[\mathtt{Inst}]&lt;/math&gt; and &lt;math&gt;[\mathtt{Gen}]&lt;/math&gt;, which allow building arbitrarily long branches (chains) between the other nodes.

=== Degrees of freedom instantiating the rules ===

Now that the shape of the proof is known, one is already close to formulating a type inference algorithm.
Because any proof for a given expression must have the same shape, one can assume the monotypes in the
proof's judgements to be undetermined and consider how to determine them.

Here, the substitution (specialisation) order comes into play. Although at the first glance one cannot determine the types locally, the hope is that it is possible to refine them with the help of the order while traversing the proof tree, additionally assuming, because the resulting algorithm is to become an inference method, that the type in any premise will be determined as the best possible. And in fact, one can, as looking at the rules of &lt;math&gt;\vdash_S&lt;/math&gt; suggests:

* &lt;math&gt;[Abs]&lt;/math&gt;: The critical choice is &lt;math&gt;\tau&lt;/math&gt;. At this point, nothing is known about &lt;math&gt;\tau&lt;/math&gt;, so one can only assume the most general type, which is &lt;math&gt;\forall \alpha . \alpha&lt;/math&gt;. The plan is to specialize the type if it should become necessary. Unfortunately, a polytype is not permitted in this place, so some &lt;math&gt;\alpha&lt;/math&gt; has to do for the moment. To avoid unwanted captures, a type variable not yet in the proof is a safe choice. Additionally, one has to keep in mind that this monotype is not yet fixed, but might be further refined.
* &lt;math&gt;[Var]&lt;/math&gt;: The choice is how to refine &lt;math&gt;\sigma&lt;/math&gt;. Because any choice of a type &lt;math&gt;\tau&lt;/math&gt; here depends on the usage of the variable, which is not locally known, the safest bet is the most general one. Using the same method as above one can instantiate all quantified variables in &lt;math&gt;\sigma&lt;/math&gt; with fresh monotype variables, again keeping them open to further refinement.
* &lt;math&gt;[Let]&lt;/math&gt;: The rule does not leave any choice. Done.
* &lt;math&gt;[App]&lt;/math&gt;: Only the application rule might force a refinement to the variables "opened" so far.

The first premise forces the outcome of the inference to be of the form &lt;math&gt;\tau \rightarrow \tau'&lt;/math&gt;.
* If it is, then fine. One can later pick its &lt;math&gt;\tau'&lt;/math&gt; for the result.
* If not, it might be an open variable. Then this can be refined to the required form with two new variables as before.
* Otherwise, the type checking fails because the first premise inferred a type which is not and cannot be made into a function type.

The second premise requires that the inferred type is equal to &lt;math&gt;\tau&lt;/math&gt; of the first premise. Now there are two possibly different types, perhaps with open type variables, at hand to compare and to make equal if it is possible. If it is, a refinement is found, and if not, a type error is detected again. An effective method is known to "make two terms equal" by substitution, [[John Alan Robinson|Robinson's]] [[Unification (computing)|Unification]] in combination with the so-called [[Disjoint-set data structure|Union-Find]] algorithm.

To briefly summarize the union-find algorithm, given the set of all types in a proof, it allows one to group them together into [[equivalence class]]es by means of a &lt;math&gt;\mathtt{union}&lt;/math&gt;
procedure and to pick a representative for each such class using a &lt;math&gt;\mathtt{find}&lt;/math&gt; procedure. Emphasizing the word [[Procedure (computer science)|procedure]] in the sense of [[Side effect (computer science)|side effect]], we're clearly leaving the realm of logic in order to prepare an effective algorithm. The representative of a &lt;math&gt;\mathtt{union}(a,b)&lt;/math&gt; is determined such that, if both &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; are type variables then the representative is arbitrarily one of them, but while uniting a variable and a term, the term becomes the representative. Assuming an implementation of union-find at hand, one can formulate the unification of two monotypes as follows:

 unify(ta,tb):
   ta = find(ta)
   tb = find(tb)
   '''if''' both ta,tb are terms of the form D p1..pn with identical D,n '''then'''
     unify(ta[i],tb[i]) for each corresponding ''i''th parameter
   '''else'''
   '''if''' at least one of ta,tb is a type variable '''then'''
     union(ta,tb)
   '''else'''
     error 'types do not match'

Now having a sketch of an inference algorithm at hand, a more formal presentation is given in the next section. It is described in Milner&lt;ref name="Milner"/&gt; P. 370 ff. as algorithm J.

=== Algorithm J ===
{| class=infobox
|align=center style="background:#e0e0ff"|'''Algorithm J'''
|-
| &lt;math&gt;
\begin{array}{cl}
\displaystyle\frac{x:\sigma \in \Gamma \quad \tau = \mathit{inst}(\sigma)}{\Gamma \vdash_J x:\tau}&amp;[\mathtt{Var}]\\ \\
\displaystyle\frac{\Gamma \vdash_J e_0:\tau_0 \quad \Gamma \vdash_J e_1 : \tau_1 \quad \tau'=\mathit{newvar} \quad \mathit{unify}(\tau_0,\ \tau_1 \rightarrow \tau') }{\Gamma \vdash_J e_0\ e_1 : \tau'}&amp;[\mathtt{App}]\\ \\
\displaystyle\frac{\tau = \mathit{newvar} \quad \Gamma,\;x:\tau\vdash_J e:\tau'}{\Gamma \vdash_J \lambda\ x\ .\ e : \tau \rightarrow \tau'}&amp;[\mathtt{Abs}]\\ \\
\displaystyle\frac{\Gamma \vdash_J e_0:\tau \quad\quad \Gamma,\,x:\bar{\Gamma}(\tau) \vdash_J e_1:\tau'}{\Gamma \vdash_J \mathtt{let}\ x = e_0\ \mathtt{in}\ e_1 :  \tau'}&amp;[\mathtt{Let}]
\end{array}
&lt;/math&gt;
|}

The presentation of Algorithm J is a misuse of the notation of logical rules, since it includes side effects but allows a direct comparison with &lt;math&gt;\vdash_S&lt;/math&gt; while expressing an efficient implementation at the same time. The rules now specify a procedure with parameters &lt;math&gt;\Gamma, e&lt;/math&gt; yielding &lt;math&gt;\tau&lt;/math&gt; in the conclusion where the execution of the premises proceeds from left to right.

The procedure &lt;math&gt;inst(\sigma)&lt;/math&gt; specializes the polytype &lt;math&gt;\sigma&lt;/math&gt; by copying the term and replacing the bound type variables consistently by new monotype variables. '&lt;math&gt;newvar&lt;/math&gt;' produces a new monotype variable. Likely, &lt;math&gt;\bar{\Gamma}(\tau)&lt;/math&gt; has to copy the type introducing new variables for the quantification to avoid unwanted captures. Overall, the algorithm now proceeds by always making the most general choice leaving the specialization to the unification, which by itself produces the most general result. As noted [[#Syntax driven Rule System|above]], the final result &lt;math&gt;\tau&lt;/math&gt; has to be generalized to &lt;math&gt;\bar{\Gamma}(\tau)&lt;/math&gt; in the end, to gain the most general type for a given expression.

Because the procedures used in the algorithm have nearly O(1) cost, the overall cost of the algorithm is close to linear in the size of the expression for which a type is to be inferred. This is in strong contrast to many other attempts to derive type inference algorithms, which often came out to be [[NP-hard]], if not [[Undecidable problem|undecidable]] with respect to termination. Thus the HM performs as well as the best fully informed type-checking algorithms can. Type-checking here means that an algorithm does not have to find a proof, but only to validate a given one.

Efficiency is slightly reduced because the binding of type variables in the context has to be maintained to allow computation of &lt;math&gt;\bar{\Gamma}(\tau)&lt;/math&gt; and enable an [[occurs check]] to prevent the building of recursive types during &lt;math&gt;union(\alpha,\tau)&lt;/math&gt;.
An example of such a case is &lt;math&gt;\lambda\ x.(x\ x)&lt;/math&gt;, for which no type can be derived using HM.  Practically, types are only small terms and do not build up expanding structures.  Thus, in complexity analysis, one can treat comparing them as a constant, retaining O(1) costs.

==Proving the algorithm==

In the previous section, while sketching the algorithm its proof was hinted at with metalogical argumentation.  While this leads to an efficient algorithm J, it is
not clear whether the algorithm properly reflects the deduction systems D or S
which serve as a semantic base line.

The most critical point in the above argumentation is the refinement of monotype
variables bound by the context. For instance, the algorithm boldly changes the
context while inferring e.g. &lt;math&gt;\lambda f . (f\ 1)&lt;/math&gt;,
because the monotype variable added to the context for the parameter &lt;math&gt;f&lt;/math&gt; later needs to be refined
to &lt;math&gt;int \rightarrow \beta&lt;/math&gt; when handling application.
The problem is that the deduction rules do not allow such a refinement.
Arguing that the refined type could have been added earlier instead of the
monotype variable is an expedient at best.

The key to reaching a formally satisfying argument is to properly include
the context within the refinement. Formally,
typing is compatible with substitution of free type variables.

:&lt;math&gt;\Gamma \vdash_S e : \tau \quad\Longrightarrow\quad S \Gamma \vdash_S e : S \tau&lt;/math&gt;

To refine the free variables thus means to refine the whole typing.

=== Algorithm W ===
{| class=infobox
|align=center style="background:#e0e0ff"|'''Algorithm W'''
|-
| &lt;math&gt;
\begin{array}{cl}
\displaystyle\frac{x:\sigma \in \Gamma \quad \tau = \mathit{inst}(\sigma)}{\Gamma \vdash_W x:\tau,\emptyset}&amp;[\mathtt{Var}]\\ \\
\displaystyle\frac{\begin{array}{ll}\Gamma \vdash_W e_0 : \tau_0, S_0 &amp; S_0 \Gamma \vdash_W e_1 : \tau_1, S_1\\ \tau'=\mathit{newvar} &amp; S_2 = \mathsf{mgu}(S_1 \tau_0,\ \tau_1 \rightarrow \tau')\end{array} }{\Gamma \vdash_W e_0\ e_1 : S_2 \tau', S_2 S_1 S_0}&amp;[\mathtt{App}]\\ \\
\displaystyle\frac{\tau = \mathit{newvar} \quad \Gamma,\;x:\tau\vdash_W e:\tau',S}{\Gamma \vdash_W \lambda\ x\ .\ e : S\tau \rightarrow \tau',S}&amp;[\mathtt{Abs}]\\ \\
\displaystyle\frac{\Gamma \vdash_W e_0:\tau,S_0 \quad S_0\Gamma,\,x:\overline{S_0\Gamma}(\tau) \vdash_W e_1:\tau',S_1}{\Gamma \vdash_W \mathtt{let}\ x = e_0\ \mathtt{in}\ e_1 : \tau', S_1 S_0 }&amp;[\mathtt{Let}]
\end{array}

&lt;/math&gt;
|}
From there, a proof of algorithm J leads to algorithm W, which only makes the
side effects imposed by the procedure &lt;math&gt;\textit{union}&lt;/math&gt; explicit by
expressing its serial composition by means of the substitutions
&lt;math&gt;S_i&lt;/math&gt;. The presentation of algorithm W in the sidebar still makes use of side effects
in the operations set in italic, but these are now limited to generating
fresh symbols. The form of judgement is &lt;math&gt;\Gamma \vdash e : \tau, S&lt;/math&gt;,
denoting a function with a context and expression as parameter producing a monotype together with
a substitution. &lt;math&gt;\textsf{mgu}&lt;/math&gt; is a side-effect free version
of &lt;math&gt;\textit{union}&lt;/math&gt; producing a substitution which is the [[Unification (computer science)#Syntactic unification of first-order terms|most general unifier]].

While algorithm W is normally considered to be ''the'' HM algorithm and is
often directly presented after the rule system in literature, its purpose is
described by Milner&lt;ref name="Milner"/&gt; on P. 369 as follows:

: ''As it stands, W is hardly an efficient algorithm; substitutions are applied too often. It was formulated to aid the proof of soundness. We now present a simpler algorithm J which simulates W in a precise sense.''

While he considered W more complicated and less efficient, he presented it 
in his publication before J. It has its merits when side effects are unavailable or unwanted.
By the way, W is also needed to prove completeness, which is factored by him into the soundness proof.

=== Proof obligations ===

Before formulating the proof obligations, a deviation between the rules systems
D and S and the algorithms presented needs to be emphasized.

While the development above sort of misused the monotypes as "open" proof variables, the possibility that proper monotype variables might be harmed was sidestepped by introducing fresh variables and hoping for the best. But there's a catch: One of the promises made was that these fresh variables would be "kept in mind" as such. This promise is not fulfilled by the algorithm.

Having a context &lt;math&gt;1 : int,\ f : \alpha&lt;/math&gt;, the expression &lt;math&gt;f\ 1&lt;/math&gt;
cannot be typed in either &lt;math&gt;\vdash_D&lt;/math&gt; or &lt;math&gt;\vdash_S&lt;/math&gt;, but the algorithms come up with
the type &lt;math&gt;\beta&lt;/math&gt;, where W additionally delivers the substitution &lt;math&gt;\left\{\alpha \mapsto int \rightarrow \beta\right\}&lt;/math&gt;,
meaning that the algorithm fails to detect all type errors. This omission can easily be fixed by more carefully distinguishing proof
variables and monotype variables.

The authors were well aware of the problem but decided not to fix it. One might assume a pragmatic reason behind this.
While more properly implementing the type inference would have enabled the algorithm to deal with abstract monotypes,
they were not needed for the intended application where none of the items in a preexisting context have free
variables. In this light, the unneeded complication was dropped in favor of a simpler algorithm.
The remaining downside is that the proof of the algorithm with respect to the rule system is less general and can only be made
for contexts with &lt;math&gt;free(\Gamma) = \emptyset&lt;/math&gt; as a side condition.

&lt;math&gt;
\begin{array}{lll}
\text{(Correctness)}  &amp;\Gamma |-_W e : \tau, S &amp;\quad\Longrightarrow\quad \Gamma \vdash_S e : \tau \\
\text{(Completeness)} &amp;\Gamma |-_S e : \tau   &amp;\quad\Longrightarrow\quad  \Gamma \vdash_W e : \tau', S \quad\quad \text{forall}\ \tau\ \text{where}\ \overline\emptyset(\tau') \sqsubseteq \tau
\end{array}
&lt;/math&gt;

The side condition in the completeness obligation addresses how the deduction may give many types, while the algorithm always produces one. At the same time, the side condition demands that the type inferred is actually the most general.

To properly prove the obligations one needs to strengthen them first to allow activating the substitution lemma threading the substitution &lt;math&gt;S&lt;/math&gt; through &lt;math&gt;\vdash_S&lt;/math&gt; and &lt;math&gt;\vdash_W&lt;/math&gt;. From there, the proofs are by induction over the expression.

Another proof obligation is the substitution lemma itself, i.e. the substitution of the typing, which finally establishes the all-quantification. The later cannot formally be proven, since no such syntax is at hand.

== Extensions ==

=== Recursive definitions ===

To make [[Turing completeness|programming practical]] recursive functions are needed.
A central property of the lambda calculus is that recursive definitions
are not directly available, but can instead be expressed with a [[fixed point combinator]].
But unfortunately, the fixpoint combinator cannot be formulated in a typed version
of the lambda calculus without having a disastrous effect on the system as outlined
below.

==== Typing rule ====

The original paper&lt;ref name="Damas"/&gt; notes that recursion can be realized by a combinator
&lt;math&gt;\mathit{fix}:\forall\alpha.(\alpha\rightarrow\alpha)\rightarrow\alpha&lt;/math&gt;. A possible recursive definition could thus be formulated as
&lt;math&gt;\mathtt{rec}\ v = e_1\ \mathtt{in}\ e_2\ ::=\mathtt{let}\ v = \mathit{fix}(\lambda v.e_1)\ \mathtt{in}\ e_2&lt;/math&gt;.

Alternatively an extension of the expression syntax and an extra typing rule is possible:

: &lt;math&gt;\displaystyle\frac{
\Gamma, \Gamma' \vdash e_1:\tau_1\quad\dots\quad\Gamma, \Gamma' \vdash e_n:\tau_n\quad\Gamma, \Gamma'' \vdash e:\tau
}{
\Gamma\ \vdash\ \mathtt{rec}\ v_1 = e_1\ \mathtt{and}\ \dots\ \mathtt{and}\ v_n = e_n\ \mathtt{in}\ e:\tau
}\quad[\mathtt{Rec}]&lt;/math&gt;

where
* &lt;math&gt;\Gamma' = v_1:\tau_1,\ \dots,\ v_n:\tau_n&lt;/math&gt;
* &lt;math&gt;\Gamma'' = v_1:\bar\Gamma(\ \tau_1\ ),\ \dots,\ v_n:\bar\Gamma(\ \tau_n\ )&lt;/math&gt;
basically merging &lt;math&gt;[\mathtt{Abs}]&lt;/math&gt; and &lt;math&gt;[\mathtt{Let}]&lt;/math&gt; while including the recursively defined
variables in monotype positions where they occur to the left of the &lt;math&gt;\mathtt{in}&lt;/math&gt; but as polytypes to the right of it. This
formulation perhaps best summarizes the essence of [[#Let-polymorphism|let-polymorphism]].

==== Consequences ====

While the above is straightforward it does come at a price.

[[Type theory]] connects lambda calculus computation and logic.
The easy modification above has effects on both:

* The [[Normalization property (abstract rewriting)|strong normalisation property]] is spoiled.
* The logic [[Consistency|collapses]].

Programs in simply typed lambda calculus are guaranteed to always terminate. Moreover, they
are even guaranteed to terminate under any evaluation strategy, be it top down, bottom up, breadth first, whatever. The same is true for expressions that have types in HM. It is well-known that separating
terminating from non-terminating programs is most difficult, and especially in lambda calculus,
which is so expressive that it can formulate recursion with just a few symbols. Thus the initial
inability of HM to provide recursive functions was not an omission, but a feature. Adding
recursion enables normal programming but the guarantee is not longer valid.

Another reading of the typing is given by the [[Curry–Howard isomorphism]]. Here
the types are interpreted as logical expressions. Let's look at the type of the fixpoint combinator from this
perspective, assuming the variables to have logical value:
: &lt;math&gt;
(\alpha \rightarrow \alpha) \rightarrow \alpha  \quad\Leftrightarrow\quad 
\textsf{true} \rightarrow \alpha  \quad\Leftrightarrow\quad  \alpha
&lt;/math&gt;
But this is [[Validity (logic)|invalid]].
Adding an invalid axiom will break the logic in the sense that
every formula can then be shown to be true in it, e.g. &lt;math&gt;\text{true} \Leftrightarrow \text{false}&lt;/math&gt;.
Thus the ability to distinguish even two simple things is no longer given. Everything is the same and
collapses into [[Answer to the Ultimate Question of Life, the Universe, and Everything|42]]. The fixpoint
combinator that came in so handy above also plays a role in [[Curry's paradox#Lambda calculus|Curry's paradox]].

Logic aside, does this matter for typing programs? It does. Since one is now able to formulate
non-terminating functions, one can make a function that would return whatever one wants&lt;ref group="note"&gt;
To make a function that works just like [[Aladdin]]'s wonderful lamp, one can define: &lt;math&gt;\text{iwisha} \equiv \text{fix id}&lt;/math&gt;
and formulate the wish in a slightly more expressive type language like: "(iwisha ([[flying carpet]]))" passing the type explicitly as
in the introduction. When the function returns, it delivers the wish, but no longer as a type (i.e. specification), but as something
real, a value of that type. This implementation at least does not return. Many have dreamed of such a useful device, and if it exists it could do wonders even if only in computing. The type &lt;math&gt;\forall \alpha.\alpha&lt;/math&gt; is so general, that its values belong to the realm of imagination.&lt;/ref&gt; but never really returns:
: &lt;math&gt;\text{fix}\ \text{id} : \forall a . a&lt;/math&gt;.
In practical programming such a function can come in handy when breaking out of a computation,
like with &lt;code&gt;exit(1)&lt;/code&gt; in [[C (programming language)|C]], while silencing the type checker in
the current branch by returning essentially nothing but with a suitable type.

Less desirable is that the type checker (type inferencer) now succeeds with a type for a function that in fact never returns any value, like &lt;math&gt;\text{fix}\ \text{id}&lt;/math&gt;. The function ''would'' return a value of this type, but it ''cannot'' because no terminating function with this type exists. The type checker's claim that everything is ok thus has to be taken with a grain of salt. The types might only be ''claimed'' to be checked, but the program can still be typed wrong. Only if all functions are terminating does &lt;math&gt;\alpha&lt;/math&gt; in the logic above have a ''true'' value, and the assertions of the type checker become strong again.

=== Overloading ===

{{main|Type class}}

Overloading means, that different functions still can be defined and used with the same name. Most programming languages at least provide overloading with the built-in arithmetic operations (+,&lt;,etc.), to allow the programmer to write arithmetic expressions in the same form, even for different numerical types like &lt;code&gt;int&lt;/code&gt; or &lt;code&gt;real&lt;/code&gt;. Because a mixture of these different types within the same expression also demands for implicit conversion, overloading especially for these operations is often built into the programming language itself. In some languages, this feature is generalized and made available to the user, e.g. in C++.

While [[Ad hoc polymorphism|ad-hoc overloading]] has been avoided in functional programming for the computation costs both in type checking and inference{{Citation needed|reasoning=I remember Huet wrote an article on this topic|date=June 2018}}, a means to systematise overloading has been introduced that resembles both in form and naming to object oriented programming, but works one level upwards. "Instances" in this systematic are not objects (i.e. on value level), but rather types.
The quicksort example mentioned in the introduction uses the overloading in the orders, having the following type annotation in Haskell:
&lt;source lang="haskell"&gt;
quickSort :: Ord a =&gt; [a] -&gt; [a]
&lt;/source&gt;
Herein, the type &lt;code&gt;a&lt;/code&gt; is not only polymorphic, but also restricted to be an instance of some type class &lt;code&gt;Ord&lt;/code&gt;, that provides the order predicates &lt;code&gt;&lt;&lt;/code&gt; and &lt;code&gt;&gt;=&lt;/code&gt; used in the functions body. The proper implementations of these predicates are then passed to quicksorts as additional parameters, as soon as quicksort is used on more concrete types providing a single implementation of the overloaded function quickSort.

Because the "classes" only allow a single type as their argument, the resulting type system can still provide inference. Additionally, the type classes can then be equipped with some kind of overloading order allowing one to arrange the classes as a [[Lattice (order)|lattice]].

=== Meta types ===

{{main|Type_class#Higher-kinded_polymorphism}}

Parametric polymorphism implies that types themselves are passed as parameters as if they were proper values. Passed as arguments into a proper functions as in the introduction, but also into "type functions" as in the "parametric" type constants, leads to the question how to more properly type types themselves. A meta type,&lt;ref group="note"&gt;
The term "higher-order" has been so overused, that "meta type" is preferred here.
&lt;/ref&gt; the "type of types" would be useful to create an even more expressive type system.

Though this would be a straight forward extension, unfortunately, only [[Unification (computer science)#Higher-order unification|unification]] is not longer decidable in the presence of meta types, rendering type inference impossible in this extend of generality.
Additionally, assuming a type of all types that includes itself as type leads into a paradox, as in the set of all sets, so one must proceed in steps of levels of abstraction.
Research in [[second order lambda calculus]], one step upwards, showed, that type inference is undecidable in this generality.

Parts of one extra level has been introduced into Haskell named [[Kind (type theory)|kind]], where it is used helping to type [[Monad (functional programming)|monads]]. Kinds are left implicit, working behind scene in the inner mechanics of the extended type system.

=== Subtyping ===

{{main|Subtyping}}

Attempts to combine subtyping and type inference have caused quite some frustration. While type inference is needed in object-oriented programming for the same reason as in functional languages, methods like HM cannot be made going for this purpose.{{citation needed|date=June 2018}} It is not difficult to setup a type system with subtyping enabling object-oriented style, as
e.g. [[Cardelli]]
&lt;ref&gt;{{cite conference
  | first = Luca
  | last = Cardelli |author2=Martini, Simone |author3=Mitchell, John C. |author4=Scedrov, Andre
  | title = An extension of  system F with subtyping
  | booktitle = Information and Computation, vol. 9
  | pages = 4–56
  | year = 1994
  | location = North Holland, Amsterdam
  | url = http://www.sciencedirect.com/science/article/pii/S0890540184710133
  | doi = 10.1006/inco.1994.1013
}}
&lt;/ref&gt; shows in [[System F-sub|his system]] &lt;math&gt;F_{&lt;:}&lt;/math&gt;.

* The type equivalence can be broken up into a subtyping relation "&lt;:".
* Extra type rules define this relation e.g. for the [[Subtyping#Function types|functions]].
* A suiting [[Subtyping#Record types|record type]] is then added whose values represent the objects.

Such objects would be immutual in a functional language context, but the type system would enable object-oriented programming style and the type inference method could be reused in imperative languages.

The subtyping rule for the record types is:
:&lt;math&gt;
\displaystyle\frac{
n \le m\quad\quad \tau_1 &lt;: \tau_1' \quad \dots \quad \tau_n &lt;: \tau_n'
}{
\Gamma\ \vdash\ \mathbf{Record}\ v_1 : \tau_1\ \dots\  v_n : \tau_n\ \mathbf{end} &lt;: \mathbf{Record}\ v_1 : \tau_1'\ \dots\  v_m : \tau_m'\ \mathbf{end}
}\quad[\mathtt{Record}]
&lt;/math&gt;
Syntatically, record expressions would have form
:&lt;math&gt;\mathbf{record}\ v_1 = e_1,\ \dots\ \mathbf{end}&lt;/math&gt;
and have a type rule leading to the above type.
Such record values could then be used the same way as objects in object oriented programming.

== Notes ==
{{Reflist|group="note"}}

== References ==
{{Reflist}}
{{refbegin}}
* {{cite journal|last=Mairson|first=Harry G. | authorlink = Harry Mairson | title = Deciding ML typability is complete for deterministic exponential time | journal = Proceedings of the 17th ACM SIGPLAN-SIGACT symposium on Principles of programming languages | series = POPL '90 | year = 1990 | isbn = 0-89791-343-4 | pages = 382–401 | doi = 10.1145/96709.96748 | publisher = ACM | ref = harv }}
* {{cite journal|first1=A. J.|last1=Kfoury|first2=J.|last2=Tiuryn|first3=P.|last3=Urzyczyn|year=1990|title=ML typability is dexptime-complete|series=CAAP '90|journal=Lecture Notes in Computer Science|volume=431|pages=206–220|ref=harv|doi=10.1007/3-540-52590-4_50}}
{{refend}}

== External links ==
* [https://github.com/wh5a/Algorithm-W-Step-By-Step/blob/master/AlgorithmW.pdf A literate Haskell implementation of Algorithm W] along with its [https://github.com/wh5a/Algorithm-W-Step-By-Step source code on GitHub].
* [https://eli.thegreenplace.net/2018/type-inference/ A simple implementation of Hindley-Milner algorithm in Python].

{{DEFAULTSORT:Hindley-Milner type system}}
[[Category:Type systems]]
[[Category:Type theory]]
[[Category:Type inference]]
[[Category:Lambda calculus]]
[[Category:Theoretical computer science]]
[[Category:Formal methods]]
[[Category:1969 in computer science]]
[[Category:1978 in computer science]]
[[Category:1985 in computer science]]
[[Category:Algorithms]]</text>
      <sha1>kwn1bts6b5622db0r44bluzblrow1tz</sha1>
    </revision>
  </page>
  <page>
    <title>Homological dimension</title>
    <ns>0</ns>
    <id>15682063</id>
    <revision>
      <id>747490233</id>
      <parentid>747478727</parentid>
      <timestamp>2016-11-02T17:47:45Z</timestamp>
      <contributor>
        <username>D.Lazard</username>
        <id>12336988</id>
      </contributor>
      <comment>Without context, "homological dimension" may only refer to "global dimension"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="596">'''Homological dimension''' may refer to the [[global dimension]] of a ring. It may also refer to any other concept of dimension that is defined in terms of [[homological algebra]], which includes:
* [[Projective dimension]] of a module, based on projective resolutions
* [[Injective dimension]] of a module, based on injective resolutions
* [[Weak dimension]] of a module, or flat dimension, based on flat resolutions
* [[Weak global dimension]] of a ring, based on the weak dimension of its modules 
* [[Cohomological dimension]] of a group

{{SIA|mathematics}}
[[Category:Homological algebra]]</text>
      <sha1>atkomjzk7g5gh2zre49tz9edp0x1vcz</sha1>
    </revision>
  </page>
  <page>
    <title>Infinity Laplacian</title>
    <ns>0</ns>
    <id>32160914</id>
    <revision>
      <id>846639418</id>
      <parentid>720594385</parentid>
      <timestamp>2018-06-20T01:00:54Z</timestamp>
      <contributor>
        <username>Bibcode Bot</username>
        <id>14394459</id>
      </contributor>
      <minor/>
      <comment>Adding 0 [[arXiv|arxiv eprint(s)]], 1 [[bibcode|bibcode(s)]] and 0 [[digital object identifier|doi(s)]]. Did it miss something? Report bugs, errors, and suggestions at [[User talk:Bibcode Bot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6451">In [[mathematics]], the '''infinity Laplace''' (or  &lt;math&gt;L^\infty&lt;/math&gt;-Laplace) operator is a 2nd-order [[partial differential operator]], commonly abbreviated &lt;math&gt;\Delta_\infty&lt;/math&gt;. It is alternately defined by

:&lt;math&gt;\Delta_\infty u(x) = \langle Du, D^2 u \, Du \rangle = \sum_{i,j} \frac{\partial^2 u}{\partial x_i \, \partial x_j} \frac{\partial u}{\partial x_i} \frac{\partial u}{\partial x_j}&lt;/math&gt;

or

:&lt;math&gt;\Delta_\infty u(x) = \frac{\langle Du, D^2 u \, Du \rangle}{|Du|^2} = \frac{1}{|Du|^2} \sum_{i,j} \frac{\partial^2 u}{\partial x_i \, \partial x_j} \frac{\partial u}{\partial x_i} \frac{\partial u}{\partial x_j}.&lt;/math&gt;

The first version avoids the singularity which occurs when the gradient vanishes, while the second version is homogeneous of order zero in the gradient. Verbally, the second version is the '''second derivative in the direction of the gradient'''. In the case of the infinity Laplace equation &lt;math&gt;\Delta_\infty u = 0&lt;/math&gt;, the two definitions are equivalent.

While the equation involves second derivatives, usually (generalized) solutions are not twice differentiable, as evidenced by the well-known Aronsson solution &lt;math&gt;u(x,y) = |x|^{4/3} - |y|^{4/3}&lt;/math&gt;.  For this reason the correct notion of solutions is that given by the [[viscosity solutions]].

Viscosity solutions to the equation &lt;math&gt;\Delta_\infty u = 0&lt;/math&gt; are also known as '''infinity harmonic functions'''.   This terminology arises from the fact that the infinity Laplace operator first arose in the study of absolute minimizers for &lt;math&gt;\| Du \|_{L^\infty}&lt;/math&gt;, and it can be viewed in a certain sense as the limit of the [[p-Laplacian]] as &lt;math&gt;p \rightarrow \infty&lt;/math&gt;. More recently, viscosity solutions to the infinity Laplace equation have been identified with the payoff functions from '''randomized tug-of-war''' [[Differential game|games]]. The game theory point of view has significantly improved the understanding of the [[partial differential equation]] itself.

==Discrete version and game theory==

A defining property of the usual &lt;math&gt;L^2&lt;/math&gt;-[[harmonic function]]s is the [[Harmonic function#The mean value property|mean value property]]. That has a natural and important discrete version: a real-valued function &lt;math&gt;f&lt;/math&gt; on a finite or infinite [[graph theory|graph]] &lt;math&gt;G(V,E)&lt;/math&gt; is '''discrete harmonic''' on a subset &lt;math&gt;U\subseteq V&lt;/math&gt; if 
:&lt;math&gt;f(x)=\sum_{y\sim x} f(y)&lt;/math&gt; 
for all &lt;math&gt;x\in U&lt;/math&gt;. Similarly, the vanishing second derivative in the direction of the gradient has a natural discrete version:

:&lt;math&gt;f(x)=\frac{\sup \{f(y): y\sim x\} + \inf \{f(y): y\sim x\}}{2}&lt;/math&gt;.

In this equation, we used sup and inf instead of max and min because the graph &lt;math&gt;G(V,E)&lt;/math&gt; does not have to be locally finite (i.e., to have finite degrees): a key example is when &lt;math&gt;V(G)&lt;/math&gt; is the set of points in a domain in &lt;math&gt;\R^d&lt;/math&gt;, and &lt;math&gt;(x,y)\in E(G)&lt;/math&gt; if their Euclidean distance is at most &lt;math&gt;\epsilon&lt;/math&gt;. The importance of this example lies in the following.

Consider a bounded open set &lt;math&gt;D\subseteq \R^d&lt;/math&gt; with smooth boundary &lt;math&gt;\partial D&lt;/math&gt;, and a continuous function &lt;math&gt;f:\partial D\longrightarrow\R&lt;/math&gt;. In the &lt;math&gt;L^2&lt;/math&gt;-case, an approximation of the harmonic extension of ''f'' to ''D'' is given by taking a lattice &lt;math&gt;\Z_\epsilon^d&lt;/math&gt; with small mesh size &lt;math&gt;\epsilon&lt;/math&gt;, letting &lt;math&gt;G_\epsilon(V,E)=D\cap\Z_\epsilon^d&lt;/math&gt; and &lt;math&gt;\partial V\subset V&lt;/math&gt; be the set of vertices with degree less than ''2d'', taking a natural approximation &lt;math&gt;f_\epsilon:\partial V\longrightarrow\R&lt;/math&gt;, and then taking the unique discrete harmonic extension of &lt;math&gt;f_\epsilon&lt;/math&gt; to ''V''. However, it is easy to see by examples that this does not work for the &lt;math&gt;L^\infty&lt;/math&gt;-case. Instead, as it turns out, one should take the ''continuum graph'' with all edges of length at most &lt;math&gt;\epsilon&lt;/math&gt;, mentioned above.

Now, a '''probabilistic way''' of looking at the &lt;math&gt;L^2&lt;/math&gt;-harmonic extension of &lt;math&gt;f_\epsilon&lt;/math&gt; from &lt;math&gt;\partial V&lt;/math&gt; to &lt;math&gt;V&lt;/math&gt; is that 
:&lt;math&gt;f_\epsilon(x)=\mathbb{E}\big[f_\epsilon(X_\tau)\,|\, X_0=x\big]&lt;/math&gt;,
where &lt;math&gt;X_0,X_1,\dots&lt;/math&gt; is the [[Random walk#Random walk on graphs|simple random walk]] on &lt;math&gt;G_\epsilon(V,E)&lt;/math&gt; started at &lt;math&gt;X_0=x&lt;/math&gt;, and &lt;math&gt;\tau&lt;/math&gt; is the [[hitting time]] of &lt;math&gt;\partial V&lt;/math&gt;.

For the &lt;math&gt;L^\infty&lt;/math&gt;-case, we need [[game theory]]. A token is started at location &lt;math&gt;x\in V(G)&lt;/math&gt;, and &lt;math&gt;f:\partial V\longrightarrow \R&lt;/math&gt; is given. There are two players, in each turn they flip a fair coin, and the winner can move the token to any neighbour of the current location. The game ends when the token reaches &lt;math&gt;\partial V&lt;/math&gt; at some time &lt;math&gt;\tau&lt;/math&gt; and location &lt;math&gt;X_\tau&lt;/math&gt;, at which point the first player gets the amount &lt;math&gt;f(X_\tau)&lt;/math&gt; from the second player. Therefore, the first player wants to maximize &lt;math&gt;f(X_\tau)&lt;/math&gt;, while the second player wants to minimize it. If both players play optimally (which has a well-defined meaning in game theory), the expected payoff &lt;math&gt;\mathbb{E}[f(X_\tau)\,|\, X_0=x]&lt;/math&gt; to the first player is a discrete infinity harmonic function, as defined above.

There is a game theory approach to the [[p-Laplacian]], too, interpolating between simple random walk and the above random tug-of-war game.

==Sources==

*{{Citation | last1=Barron | first1=Emmanuel Nicholas | last2=[[Lawrence C. Evans|Evans]] | first2=Lawrence C. | last3=Jensen | first3=Robert | title=The infinity Laplacian, Aronsson's equation and their generalizations | url=http://math.berkeley.edu/~evans/BEJ.pdf | doi=10.1090/S0002-9947-07-04338-3 | year=2008 | journal=[[Transactions of the American Mathematical Society]] | issn=0002-9947 | volume=360 | issue=1 | pages=77–101}}
*{{Citation | last1=Peres | first1=Yuval| last2=[[Oded Schramm|Schramm]] | first2=Oded| last3=Sheffield | first3=Scott| last4=Wilson | first4=David B.| title=
Tug-of-war and the infinity Laplacian.| arxiv=math/0605002v2 | mr=2449057  | year=2009 | journal=[[Journal of the American Mathematical Society]] | volume=22 | pages=167–210 | doi=10.1090/s0894-0347-08-00606-1| bibcode=2009JAMS...22..167P}}.

[[Category:Differential operators]]
[[Category:Elliptic partial differential equations]]</text>
      <sha1>iztz645gdjin8vjcffy4dzejqwjtkpn</sha1>
    </revision>
  </page>
  <page>
    <title>International Journal of Computational Geometry and Applications</title>
    <ns>0</ns>
    <id>25606481</id>
    <revision>
      <id>824121916</id>
      <parentid>799664385</parentid>
      <timestamp>2018-02-05T13:11:21Z</timestamp>
      <contributor>
        <username>Julia W</username>
        <id>7377492</id>
      </contributor>
      <comment>update IF</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2102">{{Infobox journal
| title	=	International Journal of Computational Geometry and Applications
| cover	=	
| discipline	=	[[Computer Science]] [[Mathematics]]
| abbreviation	=	Int. J. Comput. Geom. Appl.
| publisher	=	[[World Scientific]]
| editor = {{plainlist|1=
*[[Der-Tsai Lee|D.-T. Lee]]
*[[Joseph S. B. Mitchell]]
}}
| impact        =       0.082
| impact-year   =       2013
| country	=	
| website	=	http://www.worldscinet.com/ijcga/ijcga.shtml
| ISSN	= 0218-1959
| eISSN = 1793-6357
}}
The '''''International Journal of Computational Geometry and Applications'' (IJCGA)''' is a bimonthly journal published since 1991, by [[World Scientific]]. It covers the application of [[computational geometry]] in design and [[analysis of algorithms]], focusing on problems arising in various fields of science and engineering such as [[Computer-aided design|computer-aided geometry design]] (CAGD), [[operations research]], and others.

The current editors-in-chief are [[Der-Tsai Lee|D.-T. Lee]] of the Institute of Information Science in Taiwan, and [[Joseph S. B. Mitchell]] from the Department of Applied Mathematics and Statistics
in the [[State University of New York]] at Stony Brook.

== Abstracting and indexing ==
* Current Contents/Engineering, Computing &amp; Technology
* ISI Alerting Services
* Science Citation Index Expanded (also known as SciSearch)
* CompuMath Citation Index
* [[Mathematical Reviews]]
* [[INSPEC]]
* DBLP Bibliography Server
* [[Zentralblatt MATH]]
* Computer Abstracts

== References ==
&lt;!--- See [[Wikipedia:Footnotes]] on how to create references using &lt;ref&gt;&lt;/ref&gt; tags which will then appear here automatically --&gt;
{{Reflist}}

== External links ==
* [http://www.worldscinet.com/ijcga/ijcga.shtml IJCGA Journal Website]

{{Publications in computational geometry}}
{{DEFAULTSORT:International Journal Of Computational Geometry And Applications}}
[[Category:Computer science journals]]
[[Category:World Scientific academic journals]]
[[Category:Bimonthly journals]]
[[Category:Mathematics journals]]
[[Category:English-language journals]]
[[Category:Computational geometry]]</text>
      <sha1>pj4d7lny4pqpr0rowvy3d6mq2w2tiyo</sha1>
    </revision>
  </page>
  <page>
    <title>Jacqueline Ferrand</title>
    <ns>0</ns>
    <id>35256327</id>
    <revision>
      <id>729820885</id>
      <parentid>721460230</parentid>
      <timestamp>2016-07-14T19:47:26Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>sectionize and unstub</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3503">'''Jacqueline Lelong-Ferrand''' (17 February 1918, [[Alès]], France &amp;ndash; 26 April 2014, [[Sceaux, Hauts-de-Seine|Sceaux]], France) was a French mathematician who worked on [[conformal map|conformal]] [[representation theory]], [[potential theory]], and [[Riemannian manifold]]s. She taught at universities in  Caen, Lille, and Paris.&lt;ref&gt;[http://www.math.u-psud.fr/~pansu/ferrand.html Curriculum vitae]; accessed 5 May 2014.&lt;/ref&gt;&lt;ref&gt;[http://smf.emath.fr/content/décès-de-jacqueline-ferrand Jacqueline Ferrand profile], smf.emath.fr; accessed 5 May 2014 {{fr}}&lt;/ref&gt;&lt;ref&gt;[http://www.math.u-psud.fr/~pansu/ferrand.html Lelong-Ferrand profile]; accessed 5 May 2014. {{fr}}&lt;/ref&gt;

==Education and career==
Ferrand was born in [[Alès]], the daughter of a classics teacher, and went to secondary school in [[Nîmes]].&lt;ref name="mactutor"&gt;{{MacTutor|id=Ferrand}}&lt;/ref&gt;
In 1936 the [[École Normale Supérieure]] began admitting women, and she was one of the first to apply and be admitted. In 1939 she and [[Roger Apéry]] placed first in the mathematics [[agrégation]]; she began teaching at a girls' school in [[Sèvres]], while continuing to do mathematics research under the supervision of [[Arnaud Denjoy]], publishing three papers in 1941 and defending a doctoral thesis in 1942.&lt;ref name="mactutor"/&gt;&lt;ref name="riddle"/&gt;&lt;ref&gt;{{mathgenealogy|id=188260}}&lt;/ref&gt; In 1943 she won the Girbal-Baral Prize of the [[French Academy of Sciences]], and obtained a faculty position at the [[University of Bordeaux]]. She moved to the [[University of Caen]] in 1945, was given a chair at the [[University of Lille]] in 1948, and in 1956 moved to the [[University of Paris]] as a full professor. She retired in 1984.&lt;ref name="mactutor"/&gt;&lt;ref name="riddle"&gt;[http://www.agnesscott.edu/lriddle/women/ferrand.htm Biographies of Women Mathematicians], [[Agnes Scott College]]; accessed 5 May 2014.&lt;/ref&gt;&lt;ref name="yks"&gt;{{citation|title=Women mathematicians in France in the mid-twentieth century|first=Yvette|last=Kosmann-Schwarzbach|doi=10.1080/17498430.2014.976804|journal=BSHM Bulletin: Journal of the British Society for the History of Mathematics|year=2015|arxiv=1502.07597}}.&lt;/ref&gt;

==Contributions==
Ferrand had nearly 100 mathematical publications, including ten books,&lt;ref name="riddle"/&gt; and was active in mathematical research into her late 70s.&lt;ref name="mactutor"/&gt; 
One of her accomplishments, in 1971, was to prove the [[compact group|compactness]] of the group of conformal mappings of a non-spherical compact [[Riemannian manifold]], resolving a conjecture of [[André Lichnerowicz]], and on the basis of this work she became an invited speaker at the 1974 [[International Congress of Mathematicians]] in [[Vancouver]].&lt;ref name="mactutor"/&gt;&lt;ref name="yks"/&gt;

==Personal life==
She married mathematician [[Pierre Lelong]] in 1947, taking his surname alongside hers in her subsequent publications&lt;ref name="riddle"/&gt; until their separation in 1977.&lt;ref name="mactutor"/&gt;&lt;ref name="yks"/&gt;

==References==
{{reflist}}

==Links==
*[http://serge.mehl.free.fr/chrono/Ferrand.html ChronoMath, une chronologie des MATHÉMATIQUES] {{fr}}; accessed 5 May 2014

{{Authority control}}

{{DEFAULTSORT:Ferrand, Jacqueline}}
[[Category:1918 births]]
[[Category:2014 deaths]]
[[Category:École Normale Supérieure alumni]]
[[Category:French mathematicians]]
[[Category:Women mathematicians]]
[[Category:Disease-related deaths in France]]
[[Category:People from Alès]]
[[Category:20th-century women scientists]]</text>
      <sha1>fapdcbeln3jj5f5oe0ndk2yoka3owcn</sha1>
    </revision>
  </page>
  <page>
    <title>Jedediah Buxton</title>
    <ns>0</ns>
    <id>8102238</id>
    <revision>
      <id>858838937</id>
      <parentid>850280672</parentid>
      <timestamp>2018-09-09T23:41:08Z</timestamp>
      <contributor>
        <ip>2602:306:CD3B:E3F0:7DC3:B1F9:2B04:7A71</ip>
      </contributor>
      <comment>/* Journey to London */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6167">{{Infobox person
| name   = Jedediah Buxton
| image     = Jedediah buxton images.nypl.org.jpg
| image_size     = 250px
| caption  = 
| birth_date  =1707&lt;ref name=dnb&gt;[[Dictionary of National Biography]], now in the public domain&lt;/ref&gt;
| birth_place = [[Elmton]], near [[Creswell, Derbyshire|Creswell]]
| death_date  = 1772
| death_place = [[Elmton]]
| education      = 
| occupation     = Farm labourer
| spouse         = 
| parents        = 
| children       = 
}}
[[File:Jedediah Buxton.jpg|thumb|255px|The caption reads: ''Jedediah Buxton, A poor Day Labourer: born at Elmton in Derbyshire: who without being able to write or cast Accounts in the Ordinary method: perfor'md the longest Calculations and solv'd the most difficult Problems in Arithmetics, by the strength of his Memory; – neither Noise, nor Conversation cou'd interrupt him: he would either go on with his Calculations all the time or leave off in the midst and resume them again eventhough it should be Years afterwards.'']]

'''Jedediah Buxton''' (1707–1772) was a noted [[England|English]] [[mental calculator]], born at [[Elmton]], near [[Creswell, Derbyshire|Creswell]], in [[Derbyshire]].&lt;ref name="rouse_ball_1960"&gt;[[W. W. Rouse Ball]] (1960) ''Calculating Prodigies'', in Mathematical Recreations and Essays, Macmillan, New York, chapter 13.&lt;/ref&gt; He was one of the earliest people referred to as an [[autistic savant]].&lt;ref name="ReynoldsFletcher-Janzen2007"&gt;{{cite book|author1=Cecil R. Reynolds|author2=Elaine Fletcher-Janzen|title=Encyclopedia of Special Education: A Reference for the Education of Children, Adolescents, and Adults with Disabilities and Other Exceptional Individuals|url=https://books.google.com/books?id=7rvZT5J3osYC&amp;pg=PA1783|date=2 January 2007|publisher=John Wiley &amp; Sons|isbn=978-0-471-67801-4|pages=1783–}}&lt;/ref&gt;

==Life==
Buxton was born in 1707 and although his father was [[schoolmaster]] of Elmton, and his grandfather had been the [[vicar]], he could not write; and his knowledge, except of numbers, was extremely limited. How he came to understand the relative proportions of numbers, and their progressive denominations, he did not remember. However this was his interest. He frequently took no notice of objects, and when he did, it was only with reference to their numbers. He measured the lands of [[Elmton]], consisting of some thousand [[acre]]s (4&amp;nbsp;km²), simply by striding over it. He gave the area not only in acres, [[rood]]s and [[rod (unit)|perches]], but even in square inches. After this, he reduced them into square hairs'-breadths, reckoning forty-eight to each side of the inch. His memory was so great, that in resolving a question he could leave off and resume the operation again at the same point after the lapse of several months. His perpetual application to figures prevented the acquisition of other knowledge. Among the examples of Buxton's arithmetical feats which are given are his calculation of the product of a [[Farthing (British coin)|farthing]] doubled 139 times. The result, expressed in pounds, extends to thirty-nine figures, and is correct so far as it can be readily verified by the use of [[logarithms]].&lt;ref name=dnb/&gt; Buxton afterwards multiplied this enormous number by itself. It appears that he had invented an original nomenclature for large numbers, a 'tribe' being the cube of a million, and a 'cramp' (if Mr. Holliday's statement can be trusted) a thousand 'tribes of tribes'.&lt;ref name=dnb/&gt;

==Journey to London==
His mental acuity was tested in 1754 by the [[Royal Society]] when he walked to London,&lt;ref name=dnb/&gt; who acknowledged their satisfaction by presenting him with a handsome gratuity. During his visit to the metropolis he was taken to see the tragedy of [[Richard III (play)|''Richard III.'']] performed at [[Theatre Royal, Drury Lane|Drury Lane]] theatre, but his whole mind was given to the counting of the words uttered by [[David Garrick]].&lt;ref name=dnb/&gt; Similarly, he set himself to count the steps of the dancers; and he declared that the innumerable sounds produced by the musical instruments had perplexed him beyond measure.

A memoir appeared in the [[Gentleman's Magazine]] for June 1754, to which (probably through the medium of a Mr Holliday, of [[Haughton Hall]], Nottinghamshire), Buxton had contributed several letters. In this memoir, his age is given as forty-nine, which points to his birth in 1705; the date adopted above is on the authority of [[Daniel Lysons (antiquarian)|Daniel]] and [[Samuel Lysons]]' ''[[Magna Britannia]]'' (Derbyshire).

His image can be seen online in the New York Library.&lt;ref&gt;[http://digitalgallery.nypl.org/nypldigital/dgkeysearchresult.cfm?parent_id=461863&amp;word Gallery collection]&lt;/ref&gt; A portrait by Miss Maria Hartley in 1764 hangs in [[Elmton]] Church.

Jedediah Buxton was the son of William Buxton, a farmer and also the schoolmaster at Elmton. However, the Vicar of Elmton was not Jedediah's biological grandfather. John Davenport, the Vicar of Elmton, 1689–1709, was the second husband of Ann (William Buxton's mother). She had been previously married to Jedidiah's paternal grandfather, Edward Buxton of Chelmorton.&lt;ref&gt;See John Davenport's Will dated 1709 at Lichfield Record Office and also his Wife's administration papers of 1716. Gary Woodhouse (5X grt grandchild of Jedidiah Buxton)&lt;/ref&gt;

A blue plaque was erected in Jedediah's honour in [[Elmton]] in 2011 after a public poll.&lt;ref&gt;[http://www.derbyshire.gov.uk/leisure/blue_plaques/first_six/jedidiah_buxton/default.asp Jedidiah Buxton – untaught mathematical genius]. Derbyshire County Council&lt;/ref&gt;

[[File:Jedediah Buxton blue plaque.jpg|300px]]
d

==References==
{{Reflist}}
{{EB1911|wstitle=Buxton, Jedediah}}

==External links==
{{Commons category}}
* [http://thethund.ipower.com//Sourcebooks/Asimov/AisForAsimov/AsimovOnEveryone.html Asimov on Everyone] – Biographical Index of Isaac Asimov's essays

{{Authority control}}

{{DEFAULTSORT:Buxton, Jedediah}}
[[Category:1707 births]]
[[Category:1772 deaths]]
[[Category:Mental calculators]]
[[Category:Autistic savants]]
[[Category:People from Bolsover]]
[[Category:18th-century English mathematicians]]</text>
      <sha1>bm2tjnqzi9cr4r4vfbv6gm0lg874yeg</sha1>
    </revision>
  </page>
  <page>
    <title>Juhani Karhumäki</title>
    <ns>0</ns>
    <id>53552649</id>
    <revision>
      <id>774216297</id>
      <parentid>772480712</parentid>
      <timestamp>2017-04-07T00:19:23Z</timestamp>
      <contributor>
        <username>GoodDay</username>
        <id>589223</id>
      </contributor>
      <comment>Intro</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4176">{{Infobox scientist 
|name = Juhani Karhumäki
|image = Juhani Karhumäki.jpg
|birth_date = {{Birth date and age|1949|8|20|mf=y}} 
|birth_place = [[Jyväskylän maalaiskunta]], [[Finland]]
|residence = [[Finland]] 
|death_date = 
|death_place = 
|field = [[Theoretical computer science]] &lt;br&gt;[[Automata theory]] &lt;br&gt; [[Combinatorics on words]]
|work_institution = [[University of Turku]]
|alma_mater = [[University of Turku]]
|doctoral_advisor = [[Arto Salomaa]]
|known_for = [[combinatorics on words]], equivalence problem for multitape finite automata
|prizes =
|religion =
|footnotes = 
}}
'''Eero Urho Juhani Karhumäki'''&lt;ref&gt;https://www.utu.fi/fi/yksikot/sci/yksikot/mattil/hallinto/Sivut/karhumak.aspx&lt;/ref&gt; (born 1949) is a [[Finland|Finnish]] [[mathematician]]
and [[Theoretical computer science|theoretical computer scientist]]
known for his contributions to [[automata theory]].
He is a professor at the [[University of Turku]].

==Biography==

Karhumäki earned his doctorate from the [[University of Turku]] in 1976.
In 1980–1985, he was a junior researcher of [[Academy of Finland]].
Since 1986, he has held teaching positions at the [[University of Turku]],
attaining full professorship in 1998.
In 1998–2015,
Karhumäki was the head of the mathematics department at the [[University of Turku]].
He has authored altogether around 200 research papers.

Karhumäki is a member of the [[Finnish Academy of Science and Letters]] since 2000
and of [[Academia Europaea]] since 2006.
A festschrift in his honour was published in 2009
as a special issue of [[Theoretical Computer Science (journal)|Theoretical Computer Science]].&lt;ref&gt;{{cite journal|title=Preface|journal=Theoretical Computer Science|volume=410|issue=30-32|year=2009|pages=2785–2794|issn=0304-3975|doi=10.1016/j.tcs.2009.04.014}}&lt;/ref&gt;

==Research contributions==

Karhumäki has been a member of the [[M. Lothaire|Lothaire]] group of mathematicians
that developed the foundations of combinatorics of words. In 1991, jointly with [[Tero Harju]], he solved the long-standing equivalence problem for multitape [[finite automata]] in automata theory.&lt;ref name="HarjuKarhumäki1991"&gt;{{cite journal|last1=Harju|first1=T.|last2=Karhumäki|first2=J.|title=The equivalence problem of multitape finite automata|journal=Theoretical Computer Science|volume=78|issue=2|year=1991|pages=347–355|issn=0304-3975|doi=10.1016/0304-3975(91)90356-7}}&lt;/ref&gt;
Karhumäki contributed to different areas of [[formal language theory]], such as [[word equation]]s,&lt;ref name="KarhumäkiMignosi2000"&gt;{{cite journal|last1=Karhumäki|first1=Juhani|last2=Mignosi|first2=Filippo|last3=Plandowski|first3=Wojciech|title=The expressibility of languages and relations by word equations|journal=Journal of the ACM|volume=47|issue=3|year=2000|pages=483–505|issn=0004-5411|doi=10.1145/337244.337255}}&lt;/ref&gt;
[[Language equation|language equations]]&lt;ref name="KarhumäkiPetre2002"&gt;{{cite journal|last1=Karhumäki|first1=Juhani|last2=Petre|first2=Ion|title=Conway's problem for three-word sets|journal=Theoretical Computer Science|volume=289|issue=1|year=2002|pages=705–725|issn=0304-3975|doi=10.1016/S0304-3975(01)00389-9}}&lt;/ref&gt;
and [[descriptional complexity]] of finite automata.&lt;ref name="HromkovičSeibert2002"&gt;{{cite journal|last1=Hromkovič|first1=Juraj|last2=Seibert|first2=Sebastian|last3=Karhumäki|first3=Juhani|last4=Klauck|first4=Hartmut|last5=Schnitger|first5=Georg|title=Communication Complexity Method for Measuring Nondeterminism in Finite Automata|journal=Information and Computation|volume=172|issue=2|year=2002|pages=202–217|issn=0890-5401|doi=10.1006/inco.2001.3069}}&lt;/ref&gt;

==References==
{{Reflist}}

==External links==
* {{DBLP|name=Juhani Karhumäki}}
* {{MathGenealogy|name=Juhani Karhumäki}}

{{Authority control}}

{{DEFAULTSORT:Karhumaki, Juhani}}
[[Category:Finnish mathematicians]]
[[Category:Finnish computer scientists]]
[[Category:University of Turku alumni]]
[[Category:Members of Academia Europaea]]
[[Category:Members of the Finnish Academy of Science and Letters]]
[[Category:People from Turku]]
[[Category:Living people]]
[[Category:1949 births]]
[[Category:Theoretical computer scientists]]</text>
      <sha1>115bkko6n0okzahslvtatbe03g7xovg</sha1>
    </revision>
  </page>
  <page>
    <title>Kirby–Siebenmann class</title>
    <ns>0</ns>
    <id>2180593</id>
    <revision>
      <id>867153613</id>
      <parentid>862709568</parentid>
      <timestamp>2018-11-04T00:46:34Z</timestamp>
      <contributor>
        <username>Gehenna1510</username>
        <id>34982813</id>
      </contributor>
      <minor/>
      <comment>CS1 error fixed</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1072">In [[mathematics]], the '''Kirby–Siebenmann class''' is an element of the fourth [[cohomology group]]
:&lt;math&gt;\operatorname{ks}(M) \in H^4(M;\mathbb{Z}/2)&lt;/math&gt;
which must vanish if a [[topological manifold]] ''M'' is to have a [[piecewise linear structure]]. It is named for [[Robion Kirby]] and [[Larry Siebenmann]].

==See also==
*[[Hauptvermutung]]

==References==
*{{cite arXiv|title=Piecewise linear structures on topological manifolds|year=2001|version=|eprint=math.AT/0105047|last1=Rudyak | first1=Yuli B.}}
*{{Cite book|url=http://www.maths.ed.ac.uk/~aar/papers/ks.pdf |title=Foundational Essays on Topological Manifolds, Smoothings, and Triangulations|first= Robion C. |last=Kirby|first2= Laurence C.|last2=Siebenmann |year=1977|isbn= 0-691-08191-3|publisher=Princeton Univ. Pr.|location=Princeton, NJ}}
*''Topology of 4-Manifolds'' by Robion C. Kirby {{isbn|0-387-51148-2}}

{{DEFAULTSORT:Kirby-Siebenmann class}}
[[Category:Homology theory]]
[[Category:Geometric topology]]
[[Category:Structures on manifolds]]
[[Category:Surgery theory]]


{{topology-stub}}</text>
      <sha1>obyv1i3e8g5ksujarpkeeoun9ari462</sha1>
    </revision>
  </page>
  <page>
    <title>Knuth Prize</title>
    <ns>0</ns>
    <id>4827706</id>
    <revision>
      <id>855759443</id>
      <parentid>855128519</parentid>
      <timestamp>2018-08-20T16:59:09Z</timestamp>
      <contributor>
        <username>Horacelamb</username>
        <id>2098168</id>
      </contributor>
      <minor/>
      <comment>Fixed dash type.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2875">[[File:Strassen Knuth Prize presentation.jpg|thumb|[[Gary Miller (professor)|Gary Miller]] presents [[Volker Strassen]] with the 2008 Knuth Prize at SODA 2009.]]
The '''Donald E. Knuth Prize''' is a prize for outstanding contributions to the foundations of [[computer science]], named after [[Donald Knuth|Donald E. Knuth]].

==History==
The Knuth Prize has been awarded since 1996 and includes an award of $5000. The prize is awarded by [[ACM SIGACT]] and by [[IEEE Computer Society]]'s Technical Committee on the Mathematical Foundations of Computing. Prizes are awarded in alternation at the ACM [[Symposium on Theory of Computing]] and at the IEEE [[Symposium on Foundations of Computer Science]], which are among the most prestigious conferences in [[theoretical computer science]].

In contrast with the [[Gödel Prize]], which recognizes outstanding papers, the Knuth Prize is awarded to individuals for their overall impact in the field.

==Winners==
Since the prize was instituted in 1996, it has been awarded to:&lt;ref&gt;{{cite web | url = http://www.sigact.org/Prizes/Knuth/ | title = Knuth Prize | publisher = ACM SIGACT | date = February 17, 2014 | accessdate = September 17, 2014}}&lt;/ref&gt;

* 1996 – [[Andrew Yao]]
* 1997 – [[Leslie Valiant]]
* 1999 – [[László Lovász]]
* 2000 – [[Jeffrey Ullman]]
* 2002 – [[Christos Papadimitriou]]
* 2003 – [[Miklós Ajtai]]
* 2005 – [[Mihalis Yannakakis]]
* 2007 – [[Nancy Lynch]]
* 2008 – [[Volker Strassen]]
* 2010 – [[David S. Johnson]]
* 2011 – [[Ravi Kannan]]
* 2012 – [[Leonid Levin]]
* 2013 – [[Gary Miller (computer scientist)|Gary Miller]]
* 2014 – [[Richard J. Lipton]]&lt;ref&gt;{{cite web | url = http://www.acm.org/press-room/news-releases/2014/knuth-prize-2014 | archive-url = https://web.archive.org/web/20140920220414/http://www.acm.org/press-room/news-releases/2014/knuth-prize-2014 | dead-url = yes | archive-date = September 20, 2014 | title = ACM Awards Knuth Prize to Pioneer for Advances in Algorithms and Complexity Theory | publisher = Association for Computing Machinery | date = September 15, 2014 }}&lt;/ref&gt;
* 2015 – [[László Babai]]
* 2016 – [[Noam Nisan]]&lt;ref&gt;{{citation|url=http://www.acm.org/media-center/2016/september/knuth-prize-2016|title=ACM Awards Knuth Prize to Pioneer of Algorithmic Game Theory|publisher=ACM|date=September 8, 2016}}&lt;/ref&gt;
* 2017 – [[Oded Goldreich]]
* 2018 – [[Johan Håstad]]

== References ==
{{reflist}}

== External links ==
*[http://www.sigact.org/Prizes/Knuth/ Knuth Prize website]

{{Knuth Prize laureates|state=expanded}}
{{Donald Knuth navbox}}
{{Association for Computing Machinery}}
{{Institute of Electrical and Electronics Engineers}}
[[Category:Theoretical computer science]]
[[Category:Computer science awards]]
[[Category:Donald Knuth]]
[[Category:IEEE society and council awards]]

{{comp-sci-theory-stub}}

{{Award-stub}}</text>
      <sha1>1n8fxdyuixqr5z9ng242g2c4bwaluqw</sha1>
    </revision>
  </page>
  <page>
    <title>Lagrange's identity</title>
    <ns>0</ns>
    <id>2557590</id>
    <revision>
      <id>805031900</id>
      <parentid>805031877</parentid>
      <timestamp>2017-10-12T16:46:39Z</timestamp>
      <contributor>
        <ip>201.200.126.239</ip>
      </contributor>
      <comment>/* Lagrange's identity and exterior algebra */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19068">{{other uses|Lagrange's identity (disambiguation)}}

In [[algebra]], '''Lagrange's identity''', named after [[Joseph Louis Lagrange]], is:&lt;ref name=Weisstein&gt;
{{cite book |title=CRC concise encyclopedia of mathematics |author=Eric W. Weisstein |year= 2003|url=https://books.google.com/books?id=8LmCzWQYh_UC&amp;pg=PA228 |isbn=1-58488-347-2 |edition=2nd |publisher=CRC Press }}

&lt;/ref&gt;&lt;ref name= Greene&gt;

{{cite book |title=Function theory of one complex variable |author1=Robert E Greene|author1-link= Robert Everist Greene |author2=Steven G Krantz |url=https://www.amazon.com/Function-Complex-Variable-Graduate-Mathematics/dp/082182905X/ref=sr_1_1?ie=UTF8&amp;s=books&amp;qid=1271907834&amp;sr=1-1#reader_082182905X |page=22 |chapter=Exercise 16   |isbn=0-8218-3962-4 |year=2006 |edition=3rd |publisher=American Mathematical Society}}

&lt;/ref&gt;

:&lt;math&gt;
\begin{align}
\biggl( \sum_{k=1}^n a_k^2\biggr) \biggl(\sum_{k=1}^n b_k^2\biggr) - \biggl(\sum_{k=1}^n a_k b_k\biggr)^2 &amp; = \sum_{i=1}^{n-1} \sum_{j=i+1}^n (a_i b_j - a_j b_i)^2 \\
&amp; \biggl(= \frac{1}{2} \sum_{i=1}^n \sum_{j=1,j\neq i}^n (a_i b_j - a_j b_i)^2\biggr),
\end{align}
&lt;/math&gt;

which applies to any two sets {''a''&lt;sub&gt;1&lt;/sub&gt;, ''a''&lt;sub&gt;2&lt;/sub&gt;, .&amp;nbsp;.&amp;nbsp;., ''a&lt;sub&gt;n&lt;/sub&gt;''} and {''b''&lt;sub&gt;1&lt;/sub&gt;, ''b''&lt;sub&gt;2&lt;/sub&gt;, .&amp;nbsp;.&amp;nbsp;., ''b&lt;sub&gt;n&lt;/sub&gt;''} of [[real number|real]] or [[complex number]]s (or more generally, elements of a [[commutative ring]]). This identity is a generalisation of the [[Brahmagupta-Fibonacci identity]] and a special form of the [[Binet–Cauchy identity]].

In a more compact vector notation, Lagrange's identity is expressed as:&lt;ref name=Boichenko&gt;

{{cite book |title=Dimension theory for ordinary differential equations |author1=Vladimir A. Boichenko |author2=Gennadiĭ Alekseevich Leonov |author3=Volker Reitmann |url=https://books.google.com/books?id=9bN1-b_dSYsC&amp;pg=PA26 |page=26 |isbn=3-519-00437-2 |year=2005 |publisher=Vieweg+Teubner Verlag}}

&lt;/ref&gt;

:&lt;math&gt;\| \mathbf a \|^2 \ \| \mathbf b \|^2 - (\mathbf {a \cdot b } )^2 = \sum_{1 \le i &lt; j \le n} \left(a_ib_j-a_jb_i \right)^2 \ , &lt;/math&gt;

where '''a''' and '''b''' are ''n''-dimensional vectors with components that are real numbers. The extension to complex numbers requires the interpretation of the [[dot product]] as an [[inner product]] or Hermitian dot product. Explicitly, for complex numbers, Lagrange's identity can be written in the form:&lt;ref name=Steele&gt;

{{cite book |title=The Cauchy-Schwarz master class: an introduction to the art of mathematical inequalities |author=J. Michael Steele |pages=68–69 |chapter=Exercise 4.4: Lagrange's identity for complex numbers |url=https://books.google.com/books?id=bvgBdZKEYAEC&amp;pg=PA68  |year=2004 |isbn=0-521-54677-X |publisher=Cambridge University Press}}

&lt;/ref&gt;

:&lt;math&gt;\biggl( \sum_{k=1}^n |a_k|^2\biggr) \biggl(\sum_{k=1}^n |b_k|^2\biggr) - \biggl|\sum_{k=1}^n a_k b_k\biggr|^2 = \sum_{i=1}^{n-1} \sum_{j=i+1}^n |a_i \overline{b}_j - a_j \overline{b}_i|^2&lt;/math&gt;

involving the [[Absolute value#Complex numbers|absolute value]].&lt;ref&gt;
{{Cite book | last1=Greene | first1=Robert E. | last2=Krantz | first2=Steven G. | title=Function Theory of One Complex Variable | publisher=[[American Mathematical Society]] | location=Providence, R.I. | isbn=978-0-8218-2905-9 | year=2002 | page=22, Exercise 16 | postscript=&lt;!--None--&gt;}};&lt;br&gt;
{{Cite book | last1=Palka | first1=Bruce P. | title=An Introduction to Complex Function Theory | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-0-387-97427-9 | year=1991 | page=27, Exercise 4.22 | postscript=&lt;!--None--&gt;}}.
&lt;/ref&gt;

Since the right-hand side of the identity is clearly non-negative, it implies [[Cauchy–Schwarz inequality|Cauchy's inequality]] in the [[Dimension (vector space)|finite-dimensional]] [[real coordinate space]] ℝ&lt;sup&gt;''n''&lt;/sup&gt; and its complex counterpart ℂ&lt;sup&gt;''n''&lt;/sup&gt;.

Geometrically, the identity asserts that the square of the volume of the parallelepiped spanned by a set of vectors is the [[Gramian|Gram determinant]] of the vectors.

==Lagrange's identity and exterior algebra==

In terms of the [[wedge product]], Lagrange's identity can be written

:&lt;math&gt;(a \cdot a)(b \cdot b) - (a \cdot b)^2 = (a \wedge b) \cdot (a \wedge b).&lt;/math&gt;

Hence, it can be seen as a formula which gives the length of the wedge product of two vectors, which is the area of the parallelogram they define, in terms of the dot products of the two vectors, as

:&lt;math&gt;\|a \wedge b\| = \sqrt{(\|a\|\ \|b\|)^2 - \|a \cdot b\|^2}.&lt;/math&gt;

==Lagrange's identity and vector calculus==

In three dimensions, Lagrange's identity asserts that if '''a''' and '''b''' are vectors in ℝ&lt;sup&gt;3&lt;/sup&gt; with lengths |'''a'''| and |'''b'''|, then Lagrange's identity can be written in terms of the [[cross product]] and [[dot product]]:&lt;ref name=Anton&gt;

{{cite book |title=Elementary Linear Algebra: Applications Version |author1=Howard Anton |author2=Chris Rorres |url=https://books.google.com/books?id=1PJ-WHepeBsC&amp;pg=PA162&amp;dq=%22cross+product%22+%22Lagrange%27s+identity%22&amp;cd=6#v=onepage&amp;q=%22cross%20product%22%20%22Lagrange%27s%20identity%22 |page=162 |chapter=Relationships between dot and cross products
|isbn=0-470-43205-5 |year=2010 |publisher=John Wiley and Sons |edition=10th}}

&lt;/ref&gt;&lt;ref name=Lounesto1&gt;

{{cite book |title=Clifford algebras and spinors |author=Pertti Lounesto |url=https://books.google.com/books?id=kOsybQWDK4oC&amp;pg=PA94&amp;dq=%22which+in+coordinate+form+means+Lagrange%27s+identity%22&amp;cd=1#v=onepage&amp;q=%22which%20in%20coordinate%20form%20means%20Lagrange%27s%20identity%22 |isbn=0-521-00551-5 |year=2001 |page=94 |edition=2nd |publisher=Cambridge University Press}}

&lt;/ref&gt;

:&lt;math&gt; |\mathbf{a}|^2 |\mathbf{b}|^2 - (\mathbf {a \cdot b})^2 = |\mathbf {a \times b}|^2&lt;/math&gt;

Using the definition of angle based upon the [[Dot product#Geometric interpretation|dot product]] (see also [[Cauchy–Schwarz inequality#Use|Cauchy–Schwarz inequality]]), the left-hand side is
:&lt;math&gt;|\mathbf{a}|^2|\mathbf{b}|^2(1-\cos^2\theta) = |\mathbf{a}|^2|\mathbf{b}|^2\sin^2\theta&lt;/math&gt;
where θ is the angle formed by the vectors '''a''' and '''b'''.  The area of a parallelogram with sides |'''a'''| and |'''b'''| and angle θ is known in elementary geometry to be
:&lt;math&gt;|\mathbf{a}|\,|\mathbf{b}|\,|\sin\theta|,&lt;/math&gt;
so the left-hand side of Lagrange's identity is the squared area of the parallelogram.  The cross product appearing on the right-hand side is defined by
:&lt;math&gt;\mathbf{a}\times\mathbf{b} = (a_2b_3-a_3b_2)\mathbf{i} + (a_3b_1-a_1b_3)\mathbf{j} + (a_1b_2-a_2b_1)\mathbf{k}&lt;/math&gt;
which is a vector whose components are equal in magnitude to the areas of the projections of the parallelogram onto the ''yz'', ''zx'', and ''xy'' planes, respectively.

===Seven dimensions===
{{main|Seven-dimensional cross product}} 
For '''a''' and '''b''' as vectors in ℝ&lt;sup&gt;7&lt;/sup&gt;, Lagrange's identity takes on the same form as in the case of ℝ&lt;sup&gt;3&lt;/sup&gt; &lt;ref name=Lounesto&gt;{{cite book |title=Clifford algebras and spinors |author=Door Pertti Lounesto |isbn=0-521-00551-5 |year=2001 |edition =2nd  |url=https://books.google.com/books?id=kOsybQWDK4oC&amp;printsec=frontcover&amp;q=Pythagorean |publisher=Cambridge University Press}} See particularly [https://books.google.com/books?id=kOsybQWDK4oC&amp;pg=PA96#v=onepage&amp;q&amp;f=false § 7.4 Cross products in ℝ&lt;sup&gt;7&lt;/sup&gt;], p. 96.

&lt;/ref&gt;

:&lt;math&gt;|\mathbf{a}|^2 |\mathbf{b}|^2 -|\mathbf{a} \cdot \mathbf{b}|^2 = |\mathbf{a} \times \mathbf{b}|^2 \ ,&lt;/math&gt;

However, the cross product in 7 dimensions does not share all the properties of the cross product in 3 dimensions. For example, the direction of '''a × b''' in 7-dimensions may be the same as '''c × d''' even though '''c''' and '''d''' are linearly independent of '''a''' and '''b'''. Also the [[seven-dimensional cross product]] is not compatible with the [[Jacobi identity]].&lt;ref name=Lounesto/&gt;

===Quaternions===

A [[quaternion]] ''p'' is defined as the sum of a scalar ''t'' and a vector '''v''':

:&lt;math&gt;p = t + \mathbf v = t + x \ \mathbf  i +y \ \mathbf j + z\  \mathbf k.&lt;/math&gt;

The product of two quaternions {{nowrap|1=''p'' = ''t'' + '''v'''}} and {{nowrap|1=''q'' = ''s'' + '''w'''}} is defined by

:&lt;math&gt;pq = (st - \mathbf{v}\cdot\mathbf{w}) + s \ \mathbf{v} + t \ \mathbf{w}  + \mathbf{v}\times\mathbf{w}.&lt;/math&gt;

The quaternionic conjugate of ''q'' is defined by

:&lt;math&gt;\overline{q} = t - \mathbf{v},&lt;/math&gt;

and the norm squared is

:&lt;math&gt;|q|^2 = q\overline{q} = t^2 \ + \ x ^2   + \ y^2 \ +\  z^2.&lt;/math&gt;

The multiplicativity of the norm in the quaternion algebra provides, for quaternions ''p'' and ''q'':&lt;ref name=Kuipers&gt;

{{cite book |title=Quaternions and rotation sequences: a primer with applications to orbits |author=Jack B. Kuipers |url=https://books.google.com/books?id=_2sS4mC0p-EC&amp;pg=PA111 |page=111 |chapter=§5.6 The norm |isbn=0-691-10298-8 |year=2002 |publisher=Princeton University Press}}

&lt;/ref&gt;

:&lt;math&gt;|pq| = |p| |q|.&lt;/math&gt;

The quaternions ''p'' and ''q'' are called imaginary if their scalar part is zero; equivalently, if

:&lt;math&gt;p = \mathbf{v},\quad q=\mathbf{w}.&lt;/math&gt;

Lagrange's identity is just the multiplicativity of the norm of imaginary quaternions,

:&lt;math&gt;|\mathbf{v}\mathbf{w}|^2 = |\mathbf{v}|^2|\mathbf{w}|^2,&lt;/math&gt;

since, by definition,
:&lt;math&gt;
|\mathbf{v}\mathbf{w}|^2 = (\mathbf{v}\cdot\mathbf{w})^2 + |\mathbf{v}\times\mathbf{w}|^2.
&lt;/math&gt;

== Proof of algebraic form ==

The vector form follows from the Binet-Cauchy identity by setting ''c&lt;sub&gt;i&lt;/sub&gt;''&amp;nbsp;=&amp;nbsp;''a&lt;sub&gt;i&lt;/sub&gt;'' and ''d&lt;sub&gt;i&lt;/sub&gt;''&amp;nbsp;=&amp;nbsp;''b&lt;sub&gt;i&lt;/sub&gt;''. The second version follows by letting ''c&lt;sub&gt;i&lt;/sub&gt;'' and ''d&lt;sub&gt;i&lt;/sub&gt;'' denote the [[complex conjugate]]s of ''a&lt;sub&gt;i&lt;/sub&gt;'' and ''b&lt;sub&gt;i&lt;/sub&gt;'', respectively,

Here is also a direct proof.&lt;ref name=Jones&gt;

See, for example, [https://docs.google.com/viewer?a=v&amp;q=cache:rDnOA-ZKljkJ:www.owlnet.rice.edu/~fjones/chap7.pdf+lagrange%27s+identity+in+the+seven+dimensional+cross+product&amp;hl=en&amp;gl=ph&amp;sig=AHIEtbQQtdVGhgbYhz78SQQb2biLxRi4kA Frank Jones, Rice University], page 4 in Chapter 7 of a [http://www.owlnet.rice.edu/~fjones/ book still to be published].

&lt;/ref&gt; The expansion of the first term on the left side is:

:({{EquationRef|1}}) &amp;ensp;&amp;ensp;&lt;math&gt; \left( \sum_{k=1}^n a_k^2\right) \left(\sum_{k=1}^n b_k^2\right) = 
\sum_{i=1}^n \sum_{j=1}^n a_i^2 b_j^2 
= \sum_{k=1}^n a_k^2 b_k^2 
+ \sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i^2 b_j^2 
+ \sum_{j=1}^{n-1} \sum_{i=j+1}^n a_i^2 b_j^2 \ ,&lt;/math&gt;

which means that the product of a column of ''a''&lt;small&gt;s&lt;/small&gt; and a row of ''b''&lt;small&gt;s&lt;/small&gt; yields (a sum of elements of) a square of ''ab''&lt;small&gt;s&lt;/small&gt;, which can be broken up into a diagonal and a pair of triangles on either side of the diagonal.

The second term on the left side of Lagrange's identity can be expanded as:

:({{EquationRef|2}}) &amp;ensp;&amp;ensp;&lt;math&gt; \left(\sum_{k=1}^n a_k b_k\right)^2 = 
\sum_{k=1}^n a_k^2 b_k^2 + 2\sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i b_i a_j b_j \ ,&lt;/math&gt;

which means that a symmetric square can be broken up into its diagonal and a pair of equal triangles on either side of the diagonal.

To expand the summation on the right side of Lagrange's identity, first expand the square within the summation:

:&lt;math&gt; \sum_{i=1}^{n-1} \sum_{j=i+1}^n (a_i b_j - a_j b_i)^2 = \sum_{i=1}^{n-1} \sum_{j=i+1}^n (a_i^2 b_j^2 + a_j^2 b_i^2 - 2 a_i b_j a_j b_i). &lt;/math&gt;

Distribute the summation on the right side,

:&lt;math&gt; \sum_{i=1}^{n-1} \sum_{j=i+1}^n (a_i b_j - a_j b_i)^2 = \sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i^2 b_j^2 + \sum_{i=1}^{n-1} \sum_{j=i+1}^n a_j^2 b_i^2 - 2 \sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i b_j a_j b_i .&lt;/math&gt;

Now exchange the indices ''i'' and ''j'' of the second term on the right side, and permute the ''b'' factors of the third term, yielding:

:({{EquationRef|3}}) &amp;ensp;&amp;ensp;&lt;math&gt; \sum_{i=1}^{n-1} \sum_{j=i+1}^n (a_i b_j - a_j b_i)^2 = \sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i^2 b_j^2 + \sum_{j=1}^{n-1} \sum_{i=j+1}^n a_i^2 b_j^2 - 2 \sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i b_i a_j b_j \ .&lt;/math&gt;

Back to the left side of Lagrange's identity: it has two terms, given in expanded form by Equations '''('''{{EquationNote|1}}''')''' and '''('''{{EquationNote|2}}''')'''.  The first term on the right side of Equation '''('''{{EquationNote|2}}''')''' ends up canceling out the first term on the right side of Equation '''('''{{EquationNote|1}}''')''', yielding
:'''('''{{EquationNote|1}}''')''' - '''('''{{EquationNote|2}}''')''' = &lt;math&gt;\sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i^2 b_j^2 
+ \sum_{j=1}^{n-1} \sum_{i=j+1}^n a_i^2 b_j^2 - 2\sum_{i=1}^{n-1} \sum_{j=i+1}^n a_i b_i a_j b_j &lt;/math&gt;
which is the same as Equation '''('''{{EquationNote|3}}''')''', so Lagrange's identity is indeed an identity, ''[[Q.E.D.]]''.

== Proof of Lagrange's identity for complex numbers ==
Normed division algebras require that the norm of the product is equal
to the product of the norms. Lagrange's identity exhibits this equality.
The product identity used as a starting point here, is a consequence of the norm of the product equality with the product of the norm for scator algebras. This proposal, originally presented in the context of a deformed Lorentz metric, is based on a transformation stemming from the product operation and magnitude definition in hyperbolic scator algebra.&lt;ref&gt;M. Fernández-Guasti, ''Alternative realization for the composition of relativistic velocities'', Optics and Photonics 2011, vol. 8121 of The nature of light: What are photons? IV, pp. 812108–1–11. SPIE, 2011.&lt;/ref&gt;
Lagrange's identity can be proved in a variety of ways.&lt;ref name="Steele"/&gt;
Most derivations use the identity as a starting point and prove in one way or another that the equality is true. In the present approach, Lagrange's identity is actually derived without assuming it ''a priori''. An extended version of these results are available in an open source journal.&lt;ref&gt;M. Fernández-Guasti. Lagrange's identity obtained from product identity, Int. Math. Forum, 70(52):2555-2559, 2012. [http://www.m-hikari.com/imf/imf-2012/49-52-2012/fernandezguastiIMF49-52-2012.pdf]&lt;/ref&gt;

Let &lt;math&gt;a_{i},b_{i}\in\mathbb{C}&lt;/math&gt; be complex numbers and the overbar
represents complex conjugate.

The product identity &lt;math&gt;\prod_{i=1}^{n}\left(1-a_{i}\bar{a}_{i}-b_{i}\bar{b}_{i}+a_{i}\bar{a}_{i}b_{i}\bar{b}_{i}\right)=\prod_{i=1}^{n}\left(1-a_{i}\bar{a}_{i}\right)\prod_{i=1}^{n}\left(1-b_{i}\bar{b}_{i}\right)&lt;/math&gt;
reduces to the complex Lagrange's identity when fourth order terms, in a series expansion, are considered.

In order to prove it, expand the product on the LHS of the product identity in terms of
series up to fourth order. To this end, recall that products of the form &lt;math&gt;\left(1+x_{i}\right)&lt;/math&gt; can be expanded
in terms of sums as
&lt;math&gt;
\prod_{i=1}^{n}\left(1+x_{i}\right)=1+\sum_{i=1}^{n}x_{i}+\sum_{i&lt;j}^{n}x_{i}x_{j}+\mathcal{O}^{3+}(x),
&lt;/math&gt;
where &lt;math&gt;\mathcal{O}^{3+}(x)&lt;/math&gt; means terms with order three or higher
in &lt;math&gt;x&lt;/math&gt;.

&lt;center&gt;&lt;math&gt;
\prod_{i=1}^{n}\left(1-a_{i}\bar{a}_{i}-b_{i}\bar{b}_{i}+a_{i}\bar{a}_{i}b_{i}\bar{b}_{i}\right)=1-\sum_{i=1}^{n}\left(a_{i}\bar{a}_{i}+b_{i}\bar{b}_{i}\right)+\sum_{i=1}^{n}a_{i}\bar{a}_{i}b_{i}\bar{b}_{i}
+\sum_{i&lt;j}^{n}\left(a_{i}\bar{a}_{i}a_{j}\bar{a}_{j}+b_{i}\bar{b}_{i}b_{j}\bar{b}_{j}\right)+\sum_{i&lt;j}^{n}\left(a_{i}\bar{a}_{i}b_{j}\bar{b}_{j}+a_{j}\bar{a}_{j}b_{i}\bar{b}_{i}\right)+\mathcal{O}^{5+}.
&lt;/math&gt;&lt;/center&gt;

The two factors on the RHS are also written in terms of series
&lt;math&gt;
\prod_{i=1}^{n}\left(1-a_{i}\bar{a}_{i}\right)\prod_{i=1}^{n}\left(1-b_{i}\bar{b}_{i}\right)=\left(1-\sum_{i=1}^{n}a_{i}\bar{a}_{i}+\sum_{i&lt;j}^{n}a_{i}\bar{a}_{i}a_{j}\bar{a}_{j}+\mathcal{O}^{5+}\right)
\left(1-\sum_{i=1}^{n}b_{i}\bar{b}_{i}+\sum_{i&lt;j}^{n}b_{i}\bar{b}_{i}b_{j}\bar{b}_{j}+\mathcal{O}^{5+}\right).
&lt;/math&gt;

The product of this expression up to fourth order is 
&lt;center&gt;&lt;math&gt;
\prod_{i=1}^{n}\left(1-a_{i}\bar{a}_{i}\right)\prod_{i=1}^{n}\left(1-b_{i}\bar{b}_{i}\right)=1-\sum_{i=1}^{n}\left(a_{i}\bar{a}_{i}+b_{i}\bar{b}_{i}\right)
+\left(\sum_{i=1}^{n}a_{i}\bar{a}_{i}\right)\left(\sum_{i=1}^{n}b_{i}\bar{b}_{i}\right)+\sum_{i&lt;j}^{n}\left(a_{i}\bar{a}_{i}a_{j}\bar{a}_{j}+b_{i}\bar{b}_{i}b_{j}\bar{b}_{j}\right)+\mathcal{O}^{5+}.
&lt;/math&gt;&lt;/center&gt;
Substitution of these two results in the product identity give 
&lt;center&gt;&lt;math&gt;
\sum_{i=1}^{n}a_{i}\bar{a}_{i}b_{i}\bar{b}_{i}+\sum_{i&lt;j}^{n}\left(a_{i}\bar{a}_{i}b_{j}\bar{b}_{j}+a_{j}\bar{a}_{j}b_{i}\bar{b}_{i}\right)=\left(\sum_{i=1}^{n}a_{i}\bar{a}_{i}\right)\left(\sum_{i=1}^{n}b_{i}\bar{b}_{i}\right).
&lt;/math&gt;&lt;/center&gt;

The product of two conjugates series can be expressed as series involving the product of conjugate terms. The conjugate series product is &lt;math&gt;\left(\sum_{i=1}^{n}x_{i}\right)\left(\sum_{i=1}^{n}\bar{x}_{i}\right)=\sum_{i=1}^{n}x_{i}\bar{x}_{i}+\sum_{i&lt;j}^{n}\left(x_{i}\bar{x}_{j}+\bar{x}_{i}x_{j}\right)&lt;/math&gt;, thus

&lt;math&gt;
\left(\sum_{i=1}^{n}a_{i}b_{i}\right)\left(\sum_{i=1}^{n}\overline{a_{i}b_{i}}\right)-\sum_{i&lt;j}^{n}\left(a_{i}b_{i}\bar{a}_{j}\bar{b}_{j}+\bar{a}_{i}\bar{b}_{i}a_{j}b_{j}\right)+\sum_{i&lt;j}^{n}\left(a_{i}\bar{a}_{i}b_{j}\bar{b}_{j}+a_{j}\bar{a}_{j}b_{i}\bar{b}_{i}\right)
=\left(\sum_{i=1}^{n}a_{i}\bar{a}_{i}\right)\left(\sum_{i=1}^{n}b_{i}\bar{b}_{i}\right).
&lt;/math&gt;

The terms of the last two series on the LHS are grouped as 
&lt;math&gt;
a_{i}\bar{a}_{i}b_{j}\bar{b}_{j}+a_{j}\bar{a}_{j}b_{i}\bar{b}_{i}-a_{i}b_{i}\bar{a}_{j}\bar{b}_{j}-\bar{a}_{i}\bar{b}_{i}a_{j}b_{j}=\left(a_{i}\bar{b}_{j}-a_{j}\bar{b}_{i}\right)\left(\bar{a}_{i}b_{j}-\bar{a}_{j}b_{i}\right),
&lt;/math&gt;
in order to obtain the complex Lagrange's identity:

&lt;center&gt;&lt;math&gt;
\left(\sum_{i=1}^{n}a_{i}b_{i}\right)\left(\sum_{i=1}^{n}\overline{a_{i}b_{i}}\right)+\sum_{i&lt;j}^{n}\left(a_{i}\bar{b}_{j}-a_{j}\bar{b}_{i}\right)\left(\overline{a_{i}\bar{b}_{j}-a_{j}\bar{b}_{i}}\right)=\left(\sum_{i=1}^{n}a_{i}\bar{a}_{i}\right)\left(\sum_{i=1}^{n}b_{i}\bar{b}_{i}\right).
&lt;/math&gt;&lt;/center&gt;

In terms of the modulii,
&lt;center&gt;&lt;math&gt;
\left|\sum_{i=1}^{n}a_{i}b_{i}\right|^{2}+\sum_{i&lt;j}^{n}\left|a_{i}\bar{b}_{j}-a_{j}\bar{b}_{i}\right|^{2}=\left(\sum_{i=1}^{n}\left|a_{i}\right|^{2}\right)\left(\sum_{i=1}^{n}\left|b_{i}\right|^{2}\right).
&lt;/math&gt;&lt;/center&gt;

Lagrange's identity for complex numbers has been obtained from a straightforward
product identity. A derivation for the reals is obviously even more succinct. Since the Cauchy–Schwarz inequality is a particular case of Lagrange's identity,&lt;ref name="Steele"/&gt; this
proof is yet another way to obtain the CS inequality. [http://luz.izt.uam.mx/mediawiki/index.php/Norm_of_product Higher order terms] in the series produce novel identities.

== See also ==
*[[Brahmagupta&amp;ndash;Fibonacci identity]]
*[[Lagrange's identity (boundary value problem)]]
*[[Binet–Cauchy identity]]

== References ==

&lt;references/&gt;

== External links ==
*{{MathWorld|LagrangesIdentity|Lagrange's Identity}}

[[Category:Mathematical identities]]
[[Category:Multilinear algebra]]
[[Category:Articles containing proofs]]</text>
      <sha1>frlpxrmjqssk3me5los3vmtl2zpbbsx</sha1>
    </revision>
  </page>
  <page>
    <title>Lebesgue point</title>
    <ns>0</ns>
    <id>1519594</id>
    <revision>
      <id>716819417</id>
      <parentid>663242447</parentid>
      <timestamp>2016-04-24T02:26:08Z</timestamp>
      <contributor>
        <ip>129.82.198.10</ip>
      </contributor>
      <comment>Added "of f" for clarity.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1670">In [[mathematics]], given a locally [[Lebesgue integrable]] function &lt;math&gt;f&lt;/math&gt; on &lt;math&gt;\mathbb{R}^k&lt;/math&gt;, a point &lt;math&gt;x&lt;/math&gt; in the domain of &lt;math&gt;f&lt;/math&gt; is a '''Lebesgue point''' if&lt;ref&gt;{{citation|title=Measure Theory, Volume 1|first=Vladimir I.|last=Bogachev|publisher=Springer|year=2007|isbn=9783540345145|page=351|url=https://books.google.com/books?id=CoSIe7h5mTsC&amp;pg=PA351}}.&lt;/ref&gt;
:&lt;math&gt;\lim_{r\rightarrow 0^+}\frac{1}{|B(x,r)|}\int_{B(x,r)} \!|f(y)-f(x)|\,\mathrm{d}y=0.&lt;/math&gt;

Here, &lt;math&gt;B(x,r)&lt;/math&gt; is a ball centered at &lt;math&gt;x&lt;/math&gt; with radius &lt;math&gt;r &gt; 0&lt;/math&gt;, and &lt;math&gt;|B(x,r)|&lt;/math&gt; is its [[Lebesgue measure]].  The Lebesgue points of &lt;math&gt;f&lt;/math&gt; are thus points where &lt;math&gt;f&lt;/math&gt; does not oscillate too much, in an average sense.&lt;ref&gt;{{citation|title=Moduli in Modern Mapping Theory|series=Springer Monographs in Mathematics|first1=Olli|last1=Martio|first2=Vladimir|last2=Ryazanov|first3=Uri|last3=Srebro|first4=Eduard|last4=Yakubov|publisher=Springer|year=2008|isbn=9780387855882|page=105|url=https://books.google.com/books?id=y3oGDTHi-6oC&amp;pg=PA105}}.&lt;/ref&gt;

The [[Lebesgue differentiation theorem]] states that, given any &lt;math&gt;f\in L^1(\mathbb{R}^k)&lt;/math&gt;, [[almost everywhere|almost every]] &lt;math&gt;x&lt;/math&gt; is a Lebesgue point of &lt;math&gt;f&lt;/math&gt;.&lt;ref&gt;{{citation|title=Mathematical Analysis: An Introduction to Functions of Several Variables|first1=Mariano|last1=Giaquinta|first2=Giuseppe|last2=Modica|publisher=Springer|year=2010|isbn=9780817646127|page=80|url=https://books.google.com/books?id=0YE_AAAAQBAJ&amp;pg=PA80}}.&lt;/ref&gt;

==References==
{{reflist}}

{{DEFAULTSORT:Lebesgue Point}}
[[Category:Mathematical analysis]]</text>
      <sha1>q5civod34z0gnkfgpl7agptp64whs6s</sha1>
    </revision>
  </page>
  <page>
    <title>Linear network coding</title>
    <ns>0</ns>
    <id>3084295</id>
    <revision>
      <id>868124123</id>
      <parentid>865947550</parentid>
      <timestamp>2018-11-10T04:25:41Z</timestamp>
      <contributor>
        <username>MBK84</username>
        <id>31063112</id>
      </contributor>
      <comment>/* Applications */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="18594">Network coding is a field of research founded in a series of papers from the late 1990s to the early 2000s. However, the concept of network coding, in particular '''linear network coding''', appeared much earlier. In a 1978 paper,&lt;ref name="Celebiler&amp;S1978"&gt;{{cite journal| first=M.| last= Celebiler|author2=G. Stette | title=On Increasing the Down-Link Capacity of a Regenerative Satellite Repeater in Point-to-Point Communications| journal=Proceedings of the IEEE| year= 1978| volume=66| issue=1}}&lt;/ref&gt; a scheme for improving the throughput of a two-way communication through a satellite was proposed. In this scheme, two users trying to communicate with each other transmit their data streams to a satellite, which combines the two streams by summing them modulo 2 and then broadcasts the combined stream. Each of the two users, upon receiving the broadcast stream, can decode the other stream by using the information of their own stream.

The 2000 paper &lt;ref name="Ahlswede2000" /&gt; gave the butterfly network example (discussed below) that illustrates how linear network coding can outperform routing. This example is equivalent to the scheme for satellite communication described above. The same paper gave an optimal coding scheme for a network with one source node and three destination nodes. This is the first example illustrating the optimality of convolutional network coding (a more general form of linear network coding) over a cyclic network.

Linear network coding may be used to improve a network's throughput, efficiency and [[scalability]], as well as resilience to attacks and eavesdropping. Instead of simply relaying the [[Packet (information technology)|packets]] of information they receive, the [[Node (networking)|nodes]] of a network take ''several'' packets and combine them together for transmission. This may be used to attain the maximum possible [[information]] [[flow network|flow]] in a [[Network theory|network]].

It has been mathematically proven in theory that [[linear code|linear coding]] is enough to achieve the upper bound in multicast problems with one source.&lt;ref&gt;S. Li, R. Yeung, and N. Cai, "Linear Network Coding"([http://pdos.lcs.mit.edu/decouto/papers/li03.pdf PDF]), in IEEE Transactions on Information Theory, Vol 49, No. 2, pp. 371–381, 2003&lt;/ref&gt; However linear coding is not sufficient in general (e.g. multisource, multisink with arbitrary demands), even for more general versions of linearity such as [[convolutional coding]] and [[filter-bank coding]].&lt;ref&gt;R. Dougherty, [[Chris Freiling|C. Freiling]], and K. Zeger, "Insufficiency of Linear Coding in Network Information Flow" ([http://code.ucsd.edu/~zeger/publications/journals/DoFrZe05-IT-Insufficiency/DoFrZe05-IT-Insufficiency.pdf PDF]), in IEEE Transactions on Information Theory,  Vol. 51, No. 8, pp. 2745-2759, August 2005 ( [http://code.ucsd.edu/~zeger/publications/journals/DoFrZe05-IT-Insufficiency/DoFrZe05-IT-Insufficiency-erratum.pdf erratum])&lt;/ref&gt; Finding optimal coding solutions for general network problems with arbitrary demands remains an open problem.

== Encoding and decoding ==
In a linear network coding problem, a group of nodes &lt;math&gt;P&lt;/math&gt; are involved in moving the data from &lt;math&gt;S&lt;/math&gt; source nodes to &lt;math&gt;K&lt;/math&gt; sink nodes. Each node generates new packets which are linear combinations of earlier received packets, multiplying them by [[coefficient]]s chosen from a [[Galois field|finite field]], typically of size &lt;math&gt;GF(2^s)&lt;/math&gt;.

Each node, &lt;math&gt;p_k&lt;/math&gt; with [[Indegree#Indegree and outdegree|indegree]], &lt;math&gt;InDeg(p_k) = S&lt;/math&gt;, generates a message &lt;math&gt;X_k&lt;/math&gt; from the linear combination of received messages &lt;math&gt;\{M_i\}_{i = 1}^S&lt;/math&gt; by the relation:
:&lt;math&gt;X_k = \sum_{i=1}^S g_k^i\cdot M_i&lt;/math&gt;
where the values &lt;math&gt;g_k^i&lt;/math&gt; are the coefficients selected from &lt;math&gt;GF(2^s)&lt;/math&gt;. Note that, since operations are computed in a finite field, the generated message is of the same length as the original messages. Each node forwards the computed value &lt;math&gt;X_k&lt;/math&gt; along with the coefficients, &lt;math&gt;g_k^i&lt;/math&gt;, used in the &lt;math&gt;k^\text{th}&lt;/math&gt; level, &lt;math&gt;g_k^i&lt;/math&gt;.

Sink nodes receive these network coded messages, and collect them in a matrix. The original messages can be recovered by performing [[Gaussian elimination]] on the matrix.&lt;ref&gt;{{citation
 | last1 = Chou | first1 = Philip A.
 | last2 = Wu | first2 = Yunnan
 | last3 = Jain | first3 = Kamal
 | contribution = Practical network coding
 | date = October 2003
 | quote = Any receiver can then recover the source vectors using Gaussian elimination on the vectors in its ''h'' (or more) received packets
 | title = Allerton Conference on Communication, Control, and Computing}}.&lt;/ref&gt; In reduced row echelon form, decoded packets correspond to the rows of the form &lt;math&gt;e_i=[0 ... 0 1 0 ... 0]&lt;/math&gt;.

== A brief history ==
A network is represented by a [[directed graph]] &lt;math&gt;\mathcal{G}=(V, E, C)&lt;/math&gt;. &lt;math&gt;V&lt;/math&gt; is the set of nodes or vertices, &lt;math&gt;E&lt;/math&gt; is the set of directed links (or edges), and &lt;math&gt;C&lt;/math&gt; gives the capacity of each link of &lt;math&gt;E&lt;/math&gt;. Let &lt;math&gt;T(s, t)&lt;/math&gt; be the maximum possible throughput from node &lt;math&gt;s&lt;/math&gt; to node &lt;math&gt;t&lt;/math&gt;. By the [[max-flow min-cut theorem]], &lt;math&gt;T(s, t)&lt;/math&gt; is upper bounded by the minimum capacity of all [[Cut (graph theory)|cuts]], which is the sum of the capacities of the edges on a cut, between these two nodes.

[[Karl Menger]] proved that there is always a set of edge-disjoint paths achieving the upper bound in a [[unicast]] scenario, known as the [[max-flow min-cut theorem]]. Later, the [[Ford–Fulkerson algorithm]] was proposed to find such paths in polynomial time. Then, Edmonds proved in the paper "Edge-Disjoint Branchings" the upper bound in the broadcast scenario is also achievable, and proposed a polynomial time algorithm.

However, the situation in the [[multicast]] scenario is more complicated, and in fact, such an upper bound can't be reached using traditional [[routing]] ideas. Ahlswede, et al. proved that it can be achieved if additional computing tasks (incoming packets are combined into one or several outgoing packets) can be done in the intermediate nodes.&lt;ref name="Ahlswede2000"&gt;{{cite journal| first=Rudolf| last= Ahlswede| author-link= Rudolf Ahlswede|author2=N. Cai |author3=S.-Y. R. Li |author4=R. W. Yeung | title=Network Information Flow| journal=IEEE Transactions on Information Theory| pages= 1204–1216| year= 2000| doi=10.1109/18.850663| volume=46| issue=4}}&lt;/ref&gt;

== The butterfly network example ==
[[Image:Butterfly network.svg|thumb|Butterfly Network.]]
The butterfly network &lt;ref name="Ahlswede2000"/&gt; is often used to illustrate how linear network coding can outperform [[routing]]. Two source nodes (at the top of the picture) have information A and B that must be transmitted to the two destination nodes (at the bottom), which each want to know both A and B. Each edge can carry only a single value (we can think of an edge transmitting a bit in each time slot).

If only routing were allowed, then the central link would be only able to carry A or B, but not both. Suppose we send A through the center; then the left destination would receive A twice and not know B at all. Sending B poses a similar problem for the right destination. We say that routing is insufficient because no routing scheme can transmit both A and B simultaneously to both destinations.

Using a simple code, as shown, A and B can be transmitted to both destinations simultaneously by sending the sum of the symbols through the center&amp;nbsp;– in other words, we encode A and B using the formula "A+B". The left destination receives A and A&amp;nbsp;+&amp;nbsp;B, and can calculate B by subtracting the two values. Similarly, the right destination will receive B and A&amp;nbsp;+&amp;nbsp;B, and will also be able to determine both A and B.

== Random Linear Network Coding ==

Random linear network coding &lt;ref name="Ho2003"&gt;T. Ho, R. Koetter, [[Muriel Médard|M. Médard]], D. R. Karger and M. Effros, [http://www.its.caltech.edu/~tho/i1.pdf "The Benefits of Coding over Routing in a Randomized Setting"] in 2003 IEEE International Symposium on Information Theory. {{DOI|10.1109/ISIT.2003.1228459}}&lt;/ref&gt; is a simple yet powerful encoding scheme, which in broadcast transmission schemes allows close to optimal throughput using a decentralized algorithm. Nodes transmit random linear combinations of the packets they receive, with coefficients chosen from a Galois field. If the field size is sufficiently large, the probability that the receiver(s) will obtain linearly independent combinations (and therefore obtain innovative information) approaches 1. It should however be noted that, although random linear network coding has excellent throughput performance, if a receiver obtains an insufficient number of packets, it is extremely unlikely that they can recover any of the original packets. This can be addressed by sending additional random linear combinations until the receiver obtains the appropriate number of packets.

=== Open issues ===
Linear network coding is still a relatively new subject. Based on previous studies, there are three important open issues in RLNC:
# High decoding computational complexity due to using the Gauss-Jordan elimination method
# High transmission overhead due to attaching large coefficients vectors to encoded blocks
# Linear dependency among coefficients vectors which can reduce the number of innovative encoded blocks

== Wireless Network Coding ==
The broadcast nature of wireless (coupled with network topology) determines the nature of [[Interference (communication)|interference]]. Simultaneous transmissions in a wireless network typically result in all of the packets being lost (i.e., collision, see [[Multiple Access with Collision Avoidance for Wireless]]). A wireless network therefore requires a scheduler (as part of the [[Media access control|MAC]] functionality) to minimize such interference. Hence any gains from network coding are strongly impacted by the underlying scheduler and will deviate from the gains seen in wired networks. Further, wireless links are typically half-duplex due to hardware constraints; i.e., a node can not simultaneously transmit and receive due to the lack of sufficient isolation between the two paths.

Although, originally network coding was proposed to be used at Network layer (see [[OSI model]]), in wireless networks, network coding has been widely used in either MAC layer or [[Physical layer|PHY]] layer.&lt;ref name="firooz2013"&gt;M.H.Firooz, Z. Chen, S. Roy and H. Liu, ([https://arxiv.org/pdf/1210.1326.pdf Wireless Network Coding via Modified 802.11 MAC/PHY: Design and Implementation on SDR]) in IEEE Journal on Selected Areas in Communications, 2013.&lt;/ref&gt; It has been shown that network coding when used in wireless mesh networks need attentive design and thoughts to exploit the advantages of packet mixing, else advantages cannot be realized. There are also a variety of factors influencing throughput performance, such as media access layer protocol, congestion control algorithms, etc. It is not evident how network coding can co-exist and not jeopardize what existing congestion and flow control algorithms are doing for our Internet.&lt;ref&gt;[http://nms.csail.mit.edu/~sachin/papers/copesc.pdf XORs in The Air: Practical Wireless Network Coding]&lt;/ref&gt;

== Applications ==
Since linear network coding is a relatively new subject, its adoption in industries 
is still pending. Unlike other coding, linear network coding is not entirely applicable
in a system due to its narrow specific usage scenario. Theorists are trying to connect
to real world applications.&lt;ref&gt;{{cite web|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.77.6402&amp;rep=rep1&amp;type=pdf |title=How Practical is Network Coding? by Mea Wang, Baochun Li}}&lt;/ref&gt; In fact, it was found that BitTorrent approach is far superior than network coding.

It is envisaged that network coding is useful in the following areas:
* Owing to multi-source, multicast content-delivery nature of [[Information-centric networking]], the linear coding can improve over all network efficiency. &lt;ref&gt;{{cite journal|author=Bilal, Muhammad|displayauthors=etal|title=Network-Coding Approach for Information-Centric Networking|journal=IEEE Systems Journal |url=https://arxiv.org/abs/1808.00348 |arxiv=1808.00348}}&lt;/ref&gt;
* Alternative to [[forward error correction]] and [[Automatic repeat request|ARQ]] in traditional and wireless networks with packet loss. e.g.: [[Coded TCP]],&lt;ref&gt;https://arxiv.org/abs/1212.2291&lt;/ref&gt; [[Multi-user ARQ]]&lt;ref&gt;{{cite web|url=http://www.ericsson.com/technology/research_papers/wireless_access/doc/Multi-User%20ARQ.pdf |title=Archived copy |accessdate=2007-06-16 |deadurl=yes |archiveurl=https://web.archive.org/web/20071108173654/http://www.ericsson.com/technology/research_papers/wireless_access/doc/Multi-User%20ARQ.pdf |archivedate=2007-11-08 |df= }}&lt;/ref&gt;
* Robust and resilient to network attacks like snooping, eavesdropping, replay or data corruption attacks.&lt;ref&gt;http://securenetworkcoding.wikidot.com/&lt;/ref&gt;&lt;ref&gt;http://home.eng.iastate.edu/~yuzhen/publications/ZhenYu_INFOCOM_2008.pdf{{dead link|date=December 2017 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt;
* Digital file distribution and P2P file sharing. e.g.: [[Avalanche filesystem|Avalanche]] from Microsoft
* Distributed storage.&lt;ref&gt;{{Cite web |url=http://netcod.org/papers/11AcedanskiDMK-final.pdf |title=Archived copy |access-date=2013-04-18 |archive-url=https://web.archive.org/web/20130919032950/http://netcod.org/papers/11AcedanskiDMK-final.pdf |archive-date=2013-09-19 |dead-url=yes |df= }}&lt;/ref&gt;&lt;ref&gt;https://arxiv.org/pdf/cs/0702015.pdf&lt;/ref&gt;
* Throughput increase in wireless mesh networks. e.g. : [[COPE (network coding)|COPE]],&lt;ref&gt;http://people.csail.mit.edu/rahul/papers/cope-ton2008.pdf&lt;/ref&gt; [[CORE (network coding)|CORE]],&lt;ref&gt;{{cite journal | doi = 10.1109/VTCSpring.2013.6692495 | title=CORE: COPE with MORE in Wireless Meshed Networks | journal=2013 IEEE 77th Vehicular Technology Conference (VTC Spring)}}&lt;/ref&gt; [[Coding-aware routing]],&lt;ref&gt;{{cite web |url=http://arena.cse.sc.edu/papers/rocx.secon06.pdf |title=Archived copy |accessdate=2007-05-10 |deadurl=yes |archiveurl=https://web.archive.org/web/20081011124616/http://arena.cse.sc.edu/papers/rocx.secon06.pdf |archivedate=2008-10-11 |df= }}&lt;/ref&gt;&lt;ref&gt;http://www.cs.wisc.edu/~shravan/infocom-07-2.pdf&lt;/ref&gt; [[B.A.T.M.A.N.]]&lt;ref&gt;{{Cite web|title = NetworkCoding - batman-adv - Open Mesh|url = http://www.open-mesh.org/projects/batman-adv/wiki/NetworkCoding|website = www.open-mesh.org|accessdate = 2015-10-28}}&lt;/ref&gt;
* Buffer and Delay reduction in spatial sensor networks: [[Spatial buffer multiplexing]] &lt;ref&gt;[http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=4146919 Welcome to IEEE Xplore 2.0: Looking at Large Networks: Coding vs. Queueing&lt;!-- Bot generated title --&gt;]&lt;/ref&gt;
* Reduce the number of packet retransmission for a single-hop wireless multicast transmission, and hence improve network bandwidth.&lt;ref&gt;http://ieeexplore.ieee.org/xpl/articleDetails.jsp?arnumber=4549741&lt;/ref&gt;
* Distributed file sharing &lt;ref&gt;[https://arxiv.org/pdf/1203.5395.pdf Data dissemination in wireless networks with network coding]&lt;/ref&gt;
* Low-complexity video streaming to mobile devices &lt;ref&gt;[http://ieeexplore.ieee.org/document/6630050/?tp=&amp;arnumber=6630050 Band Codes for Energy-Efficient Network Coding With Application to P2P Mobile Streaming]&lt;/ref&gt;

== Maturity &amp; Issues ==
Since this area is relatively new and the mathematical treatment of this subject is
currently limited to a handful of people, network coding has yet found its way to
commercialization in products and services. It is unclear at this stage if this 
subject will prevail, or cease as a good mathematical exercise.&lt;ref&gt;{{cite web|url=http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.77.6402&amp;rep=rep1&amp;type=pdf |title=How Practical is Network Coding?}}&lt;/ref&gt;

Researchers have clearly pointed out that special care is needed to explore how network coding can co-exist with existing routing, media access, congestion, flow control algorithms and TCP protocol. If not, network coding may not offer much advantages and can increase computation complexity and memory requirements.&lt;ref&gt;{{cite web|url=http://nms.csail.mit.edu/~sachin/papers/copesc.pdf |title=XORs in The Air}}&lt;/ref&gt;

== See also ==
* [[Secret sharing protocol]]
* [[Homomorphic signatures for network coding]]
* [[Triangular network coding]]

== References ==
{{Reflist}}
* Fragouli, C.; Le Boudec, J. &amp; Widmer, J. "Network coding: An instant primer" in ''Computer Communication Review'', 2006.

Ali Farzamnia, Sharifah K. Syed-Yusof, Norsheila Fisa "Multicasting Multiple Description Coding Using p-Cycle Network Coding",  KSII Transactions on Internet and Information Systems, Vol 7, No 12, 2013.

== External links ==
* [https://web.archive.org/web/20070524100030/http://www.ifp.uiuc.edu/~koetter/NWC/index.html Network Coding Homepage]
* [https://web.archive.org/web/20110719100201/https://wiki.lnt.ei.tum.de/doku.php?id=network_coding:bibliography_for_network_coding A network coding bibliography]
* Raymond W. Yeung, Information Theory and Network Coding, Springer 2008, http://iest2.ie.cuhk.edu.hk/~whyeung/book2/
* Raymond W. Yeung et al., Network Coding Theory, now Publishers, 2005, http://iest2.ie.cuhk.edu.hk/~whyeung/netcode/monograph.html
* Christina Fragouli et al., Network Coding: An Instant Primer, ACM SIGCOMM 2006, http://infoscience.epfl.ch/getfile.py?mode=best&amp;recid=58339.
* Avalanche Filesystem, http://research.microsoft.com/en-us/projects/avalanche/default.aspx
* Random Network Coding, https://web.archive.org/web/20060618083034/http://www.mit.edu/~medard/coding1.htm
* Digital Fountain Codes, http://www.icsi.berkeley.edu/~luby/
* Coding-Aware Routing, https://web.archive.org/web/20081011124616/http://arena.cse.sc.edu/papers/rocx.secon06.pdf
* MIT offers a course: [http://web.mit.edu/professional/short-programs/courses/network_coding.html Introduction to Network Coding]
* [https://web.archive.org/web/20090331225831/http://www.networkworld.com/news/2007/121007-network-coding.html Network coding: Networking's next revolution?]
* Coding-aware protocol design for wireless networks: http://scholarcommons.sc.edu/etd/230/

{{DEFAULTSORT:Network Coding}}
[[Category:Coding theory]]
[[Category:Information theory]]
[[Category:Finite fields]]
[[Category:Network performance]]
[[Category:Wireless sensor network]]</text>
      <sha1>5mwd7iqvxtpykb9v3ir5u9z97at2b39</sha1>
    </revision>
  </page>
  <page>
    <title>List of mathematicians (K)</title>
    <ns>0</ns>
    <id>5971815</id>
    <revision>
      <id>870969574</id>
      <parentid>870805668</parentid>
      <timestamp>2018-11-28T03:13:38Z</timestamp>
      <contributor>
        <username>Mathbot</username>
        <id>234358</id>
      </contributor>
      <comment>Daily update. See [[User:Mathbot/Changes to mathlists]] for changes.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="32533">__NOTOC__
{{MathTopicTOC}}

== Kaa ==

* [[Mikko Kaasalainen|Kaasalainen, Mikko]] (Finland, ?–?)
* [[Rien Kaashoek|Kaashoek, Rien]] (Netherlands, born 1937)
* [[Mark Kac|Kac, Mark]] (Poland, 1914–1984)
* [[Victor Kac|Kac, Victor]] (USA/Russia/Soviet Union, born 1943)
* [[Stefan Kaczmarz|Kaczmarz, Stefan]] (Poland, 1895–1939)
* [[Ted Kaczynski|Kaczynski, Ted]] (USA, born 1942)
* [[Mikhail Kadets|Kadets, Mikhail]] (?, 1923–2011)
* [[Richard Kadison|Kadison, Richard]] (USA, born 1925)
* [[Veniamin Kagan|Kagan, Veniamin]] (Soviet Union/Lithuania/Russia, 1869–1953)
* [[William Kahan|Kahan, William]] (?, born 1933)
* [[Jean-Pierre Kahane|Kahane, Jean-Pierre]] (France, 1926–2017)
* [[Erich Kähler|Kähler, Erich]] (Germany, 1906–2000)
* [[Jeff Kahn|Kahn, Jeff]] (?, ?–?)
* [[Margarete Kahn|Kahn, Margarete]] (Germany, 1880–1942)
* [[Philippe Kahn|Kahn, Philippe]] (?, born 1952)
* [[Suzan Kahramaner|Kahramaner, Suzan]] (Turkey, 1913–2006)
* [[Chung Kai-lai|Kai-lai, Chung]] (USA, 1917–2009)
* [[Paul Chester Kainen|Kainen, Paul Chester]] (USA, born 1943)
* [[Gabriele Kaiser|Kaiser, Gabriele]] (Germany, ?–?)
* [[Fang Kaitai|Kaitai, Fang]] (China, born 1940)
* [[Avinash Kak|Kak, Avinash]] (USA, born 1944)
* [[Subhash Kak|Kak, Subhash]] (India, born 1947)
* [[Sōichi Kakeya|Kakeya, Sōichi]] (Japan, 1886–1947)
* [[Shizuo Kakutani|Kakutani, Shizuo]] (Japan, 1911–2004)
* [[Gil Kalai|Kalai, Gil]] (Israel, born 1955)
* [[Efstratia Kalfagianni|Kalfagianni, Efstratia]] (Greece, ?–?)
* [[Burt Kaliski|Kaliski, Burt]] (?, ?–?)
* [[Olav Kallenberg|Kallenberg, Olav]] (Sweden, born 1939)
* [[Gopinath Kallianpur|Kallianpur, Gopinath]] (India, 1925–2015)
* [[Eva Kallin|Kallin, Eva]] (?, ?–?)
* [[Rudolf E. Kálmán|Kálmán, Rudolf E.]] (Hungary, 1930–2016)
* [[László Kalmár|Kalmár, László]] (Hungary, 1905–1976)
* [[Charles Kalme|Kalme, Charles]] (USA, 1939–2002)
* [[Vadim Kaloshin|Kaloshin, Vadim]] (Russia, ?–?)
* [[Andrew Kalotay|Kalotay, Andrew]] (Hungary, born 1940)
* [[Nigel Kalton|Kalton, Nigel]] (Britain/USA, 1946–2010)
* [[Hermine Agavni Kalustyan|Kalustyan, Hermine Agavni]] (Armenia/Turkey, 1914–1989)
* [[Theodor Kaluza|Kaluza, Theodor]] (Germany, 1885–1954)

== Kam ==

* [[Kamalakara]] (India, 1616–1700)
* [[Friedrich Kambartel|Kambartel, Friedrich]] (Germany, born 1935)
* [[Thamsanqa Kambule|Kambule, Thamsanqa]] (?, 1921–2009)
* [[Shoshana Kamin|Kamin, Shoshana]] (Israel/Soviet Union, born 1930)
* [[Erich Kamke|Kamke, Erich]] (Germany, 1890–1961)
* [[Egbert van Kampen|van Kampen, Egbert]] (Belgium/Netherlands, 1908–1942)
* [[Niky Kamran|Kamran, Niky]] (Belgium/Canada, born 1959)
* [[Alexander Rinnooy Kan|Kan, Alexander Rinnooy]] (Netherlands, born 1949)
* [[Daniel Kan|Kan, Daniel]] (Israel, 1927–2013)
* [[Yasumasa Kanada|Kanada, Yasumasa]] (Japan, born 1949)
* [[P. Kanagasabapathy|Kanagasabapathy, P.]] (?, 1923–1977)
* [[Akihiro Kanamori|Kanamori, Akihiro]] (USA, born 1948)
* [[Daniel Kane (mathematician)|Kane, Daniel]] (USA, born 1986)
* [[Feng Kang|Kang, Feng]] (China, 1920–1993)
* [[Gunnar Kangro|Kangro, Gunnar]] (Estonia, 1913–1975)
* [[Ravindran Kannan|Kannan, Ravindran]] (India, born 1953)
* [[Timothy Kanold|Kanold, Timothy]] (USA, ?–?)
* [[Vladimir Kanovei|Kanovei, Vladimir]] (?, born 1951)
* [[Isaiah Kantor|Kantor, Isaiah]] (Russia, 1936–2006)
* [[Leonid Kantorovich|Kantorovich, Leonid]] (Soviet Union/Russia, 1912–1986)
* [[Hans G. Kaper|Kaper, Hans G.]] (Netherlands, born 1936)
* [[Edward L. Kaplan|Kaplan, Edward L.]] (?, 1920–2006)
* [[Wilfred Kaplan|Kaplan, Wilfred]] (USA, 1915–2007)
* [[Irving Kaplansky|Kaplansky, Irving]] (Canada, 1917–2006)
* [[Jay Kappraff|Kappraff, Jay]] (USA, ?–?)
* [[D. R. Kaprekar|Kaprekar, D. R.]] (India, 1905–1986)
* [[Nikolai Kapustin (mathematician)|Kapustin, Nikolai (mathematician)]] (Russia, born 1957)
* [[Al-Karaji|Al-Karaji Abu Bakr ibn Muhammad ibn al-Husayn]] (Medieval Persia, 953–1029)
* [[Jovan Karamata|Karamata, Jovan]] (Serbia/Yugoslavia, 1902–1967)
* [[Rajeeva Laxman Karandikar|Karandikar, Rajeeva Laxman]] (India, born 1956)
* [[Garnik A. Karapetyan|Karapetyan, Garnik A.]] (Armenia, born 1958)
* [[Anatoly Karatsuba|Karatsuba, Anatoly]] (Soviet Union/Russia, 1937–2008)
* [[Juhani Karhumäki|Karhumäki, Juhani]] (Finland, born 1949)
* [[Kari Karhunen|Karhunen, Kari]] (Finland, 1915–1992)
* [[Jarkko Kari|Kari, Jarkko]] (Finland, ?–?)
* [[Mary Cordia Karl|Karl, Mary Cordia]] (?, 1893–1984)
* [[Samuel Karlin|Karlin, Samuel]] (USA, 1924–2007)
* [[Narendra Karmarkar|Karmarkar, Narendra]] (India, born 1957)
* [[Maurice Karnaugh|Karnaugh, Maurice]] (?, born 1924)
* [[George Karniadakis|Karniadakis, George]] (USA, ?–?)
* [[Max Karoubi|Karoubi, Max]] (France, ?–?)
* [[Nicole El Karoui|Karoui, Nicole El]] (France, born 1944)
* [[Carol Karp|Karp, Carol]] (?, 1926–1972)
* [[Richard M. Karp|Karp, Richard M.]] (USA, born 1935)
* [[Fridrikh Karpelevich|Karpelevich, Fridrikh]] (Russia, 1927–2000)
* [[Louis Charles Karpinski|Karpinski, Louis Charles]] (USA, 1878–1956)
* [[Marek Karpinski|Karpinski, Marek]] (Poland, born 1948)
* [[Yael Karshon|Karshon, Yael]] (Canada/Israel, ?–?)
* [[Wenzeslaus Johann Gustav Karsten|Karsten, Wenzeslaus Johann Gustav]] (Germany, 1732–1787)
* [[William Karush|Karush, William]] (USA, 1917–1997)

== Kas ==

* [[Jamshīd al-Kāshī|al-Kāshī, Jamshīd]] (Medieval Persia, 1380–1429)
* [[Boris Kashin|Kashin, Boris]] (Soviet Union/Russia, born 1951)
* [[Masaki Kashiwara|Kashiwara, Masaki]] (Japan, born 1947)
* [[Edward Kasner|Kasner, Edward]] (USA, 1878–1955)
* [[Rob Kass|Kass, Rob]] (USA, ?–?)
* [[Martin Kassabov|Kassabov, Martin]] (?, born 1977)
* [[Fanny Kassel|Kassel, Fanny]] (France, born 1984)
* [[Abraham Gotthelf Kästner|Kästner, Abraham Gotthelf]] (Germany, 1719–1800)
* [[Rajesh Kasturirangan|Kasturirangan, Rajesh]] (India, ?–?)
* [[Michael Katehakis|Katehakis, Michael]] (Greece/USA, born 1952)
* [[Miroslav Katětov|Katětov, Miroslav]] (Czech Republic, 1918–1995)
* [[Najm al-Dīn al-Qazwīnī al-Kātibī|al-Kātibī, Najm al-Dīn al-Qazwīnī]] (Medieval Persia, ?–1276)
* [[Kazuya Kato|Kato, Kazuya]] (Japan, born 1952)
* [[Tosio Kato|Kato, Tosio]] (Japan, 1917–1999)
* [[Anatole Katok|Katok, Anatole]] (USA, 1944–2018)
* [[Svetlana Katok|Katok, Svetlana]] (Russia, born 1947)
* [[Gyula O. H. Katona|Katona, Gyula O. H.]] (Hungary, born 1941)
* [[Gyula Y. Katona|Katona, Gyula Y.]] (Hungary, born 1965)
* [[Moshe Katsav|Katsav, Moshe]] (?, born 1945)
* [[Kātyāyana]] (Ancient India, ?–?)
* [[Eric Katz|Katz, Eric]] (USA, born 1970s)
* [[Leo Katz (statistician)|Katz, Leo]] (USA, 1914–1976)
* [[Mikhail Katz|Katz, Mikhail]] (Israel, born 1958)
* [[Nick Katz|Katz, Nick]] (USA, born 1943)
* [[Sheldon Katz|Katz, Sheldon]] (?, born 1956)
* [[Victor J. Katz|Katz, Victor J.]] (?, born 1942)
* [[Yitzhak Katznelson|Katznelson, Yitzhak]] (Israel, born 1934)
* [[Manuel Kauers|Kauers, Manuel]] (Germany, born 1979)
* [[Louis Kauffman|Kauffman, Louis]] (?, born 1945)
* [[Bruria Kaufman|Kaufman, Bruria]] (Israel, 1918–2010)
* [[George Adams Kaufmann|Kaufmann, George Adams]] (Britain, 1894–1963)
* [[Ralph Kaufmann|Kaufmann, Ralph]] (Germany, born 1969)

== Kaw ==

* [[Takahiro Kawai|Kawai, Takahiro]] (?, born 1945)
* [[Yujiro Kawamata|Kawamata, Yujiro]] (?, born 1952)
* [[Ken-ichi Kawarabayashi|Kawarabayashi, Ken-ichi]] (Japan, born 1975)
* [[Bakul Kayastha|Kayastha, Bakul]] (India, 1400s–1450s)
* [[Jerry Kazdan|Kazdan, Jerry]] (?, born 1937)
* [[Elham Kazemi|Kazemi, Elham]] (Iran, ?–?)
* [[David Kazhdan|Kazhdan, David]] (Israel/Russia/Soviet Union, born 1946)
* [[Jonathan Keating|Keating, Jonathan]] (?, ?–?)
* [[Alexander S. Kechris|Kechris, Alexander S.]] (Greece/USA, born 1946)
* [[Kiran Kedlaya|Kedlaya, Kiran]] (?, born 1974)
* [[Ken Keeler|Keeler, Ken]] (USA, born 1961)
* [[Linda Keen|Keen, Linda]] (USA, born 1940)
* [[James Keener|Keener, James]] (USA, ?–?)
* [[Peter Keevash|Keevash, Peter]] (England, born 1978)
* [[John Keill|Keill, John]] (Scotland, 1671–1721)
* [[Julian Keilson|Keilson, Julian]] (USA, 1924–1999)
* [[Howard Jerome Keisler|Keisler, Howard Jerome]] (USA, born 1936)
* [[Mike Keith (mathematician)|Keith, Mike]] (USA, born 1955)
* [[Lyudmila Keldysh|Keldysh, Lyudmila]] (Russia, 1904–1976)
* [[Mstislav Keldysh|Keldysh, Mstislav]] (Soviet Union, 1911–1978)
* [[Philip Kelland|Kelland, Philip]] (England, 1808–1879)
* [[Bernhard Keller|Keller, Bernhard]] (Switzerland, born 1964)
* [[Hannes Keller|Keller, Hannes]] (Switzerland, born 1934)
* [[Herbert Keller|Keller, Herbert]] (USA, 1925–2008)
* [[Joseph Keller|Keller, Joseph]] (USA, 1923–2016)
* [[Ott-Heinrich Keller|Keller, Ott-Heinrich]] (Germany, 1906–1990)
* [[Ruth Kellerhals|Kellerhals, Ruth]] (Switzerland, born 1957)
* [[Henry J. Kelley|Kelley, Henry J.]] (USA, 1926–1988)
* [[John L. Kelley|Kelley, John L.]] (USA, 1916–1999)
* [[Oliver Dimon Kellogg|Kellogg, Oliver Dimon]] (USA, 1878–1932)
* [[David Kelly (mathematician)|Kelly, David]] (?, ?–?)
* [[Frank Kelly (mathematician)|Kelly, Frank]] (England, born 1950)
* [[Leroy Milton Kelly|Kelly, Leroy Milton]] (USA, 1914–2002)
* [[Max Kelly|Kelly, Max]] (Australia, 1930–2007)
* [[Patrick Kelly (metrologist)|Kelly, Patrick]] (Britain, 1756–1842)
* [[Paul Kelly (mathematician)|Kelly, Paul]] (?, 1915–1995)
* [[John Kelsey (cryptanalyst)|Kelsey, John]] (?, ?–?)
* [[William Thomson, 1st Baron Kelvin|Kelvin, William Thomson, 1st Baron]] (Britain, 1824–1907)

== Kem ==

* [[John G. Kemeny|Kemeny, John G.]] (Hungary, 1926–1992)
* [[John Kemp (mathematician)|Kemp, John]] (Scotland/USA, 1763–1812)
* [[Alfred Kempe|Kempe, Alfred]] (England/Britain, 1849–1922)
* [[Julia Kempe|Kempe, Julia]] (Germany/France/Israel, ?–?)
* [[Johannes Kemperman|Kemperman, Johannes]] (Netherlands, 1924–2011)
* [[George Kempf|Kempf, George]] (USA, 1944–2002)
* [[Aubrey J. Kempner|Kempner, Aubrey J.]] (USA, 1880–1973)
* [[Oscar Kempthorne|Kempthorne, Oscar]] (USA/Britain, 1911–2000)
* [[Claribel Kendall|Kendall, Claribel]] (USA, 1889–1965)
* [[David George Kendall|Kendall, David George]] (England, 1918–2007)
* [[Maurice Kendall|Kendall, Maurice]] (England, 1907–1983)
* [[Wilfrid Kendall|Kendall, Wilfrid]] (Britain, born 1954)
* [[Carlos Kenig|Kenig, Carlos]] (?, born 1953)
* [[Takebe Kenko|Kenko, Takebe]] (Japan, 1664–1739)
* [[Ralph Kenna|Kenna, Ralph]] (Ireland, born 1964)
* [[Hubert Kennedy|Kennedy, Hubert]] (USA, born 1931)
* [[Joseph C. G. Kennedy|Kennedy, Joseph C. G.]] (USA, ?–?)
* [[Richard Kenyon|Kenyon, Richard]] (USA, born 1964)
* [[Johannes Kepler|Kepler, Johannes]] (Germany, 1571–1630)
* [[Steven Kerckhoff|Kerckhoff, Steven]] (USA, born 1952)
* [[Béla Kerékjártó|Kerékjártó, Béla]] (Hungary, 1898–1946)
* [[Roy Kerr|Kerr, Roy]] (New Zealand, born 1934)
* [[John Edmund Kerrich|Kerrich, John Edmund]] (Britain, 1903–1985)
* [[Michel Kervaire|Kervaire, Michel]] (Switzerland, 1927–2007)
* [[Leah Keshet|Keshet, Leah]] (Canada/Israel, ?–?)
* [[Radha Kessar|Kessar, Radha]] (Britain/India, ?–?)
* [[Thomas Kessner|Kessner, Thomas]] (USA, ?–?)
* [[Harry Kesten|Kesten, Harry]] (USA, born 1931)
* [[Panayotis G. Kevrekidis|Kevrekidis, Panayotis G.]] (?, ?–?)
* [[Yannís G. Kevrekidis|Kevrekidis, Yannís G.]] (?, born 1959)

== Key ==

* [[Barbara Keyfitz|Keyfitz, Barbara]] (Canada, ?–?)
* [[John Maynard Keynes|Keynes, John Maynard]] (?, 1883–1946)
* [[Cassius Jackson Keyser|Keyser, Cassius Jackson]] (USA, 1862–1947)
* [[Leonid Khachiyan|Khachiyan, Leonid]] (Ethnic Armenia/Armenia/Soviet Union, 1952–2005)
* [[Zahid Khalilov|Khalilov, Zahid]] (Azerbaijan, 1911–1974)
* [[Salem Hanna Khamis|Khamis, Salem Hanna]] (Palestinian, 1919–2005)
* [[Mohamed Amine Khamsi|Khamsi, Mohamed Amine]] (Morocco, born 1959)
* [[Konstantin Khanin|Khanin, Konstantin]] (Russia, ?–?)
* [[Chandrashekhar Khare|Khare, Chandrashekhar]] (India, born 1968)
* [[Olga Kharlampovich|Kharlampovich, Olga]] (Canada, born 1958)
* [[Ravindra Khattree|Khattree, Ravindra]] (USA, born 1958)
* [[Omar Khayyam|Khayyam, Omar]] (Medieval Persia, 1048–1131)
* [[Aleksandr Khazanov|Khazanov, Aleksandr]] (?, 1979–2001)
* [[Abū Ja'far al-Khāzin|al-Khāzin, Abū Ja'far]] (Medieval Persia/Medieval Iraq, 900–971)
* [[Al-Khazini]] (Persia, ?–?)
* [[Boris Khesin|Khesin, Boris]] (Russia/Soviet Union/Canada, born 1964)
* [[Aleksandr Khinchin|Khinchin, Aleksandr]] (Russia/Soviet Union, 1894–1959)
* [[Estate Khmaladze|Khmaladze, Estate]] (Soviet Union, born 1944)
* [[Hà Huy Khoái|Khoái, Hà Huy]] (Vietnam, born 1946)
* [[David Khorol|Khorol, David]] (Soviet Union, 1920–1990)
* [[Subhash Khot|Khot, Subhash]] (USA, born 1978)
* [[Mikhail Khovanov|Khovanov, Mikhail]] (USA, born 1972)
* [[Askold Khovanskii|Khovanskii, Askold]] (Russia/Canada/Soviet Union, born 1947)
* [[Yusuf Al-Khuri|Al-Khuri, Yusuf]] (?, ?–?)
* [[Anders Nicolai Kiær|Kiær, Anders Nicolai]] (Norway, 1838–1919)
* [[Jack Kiefer (statistician)|Kiefer, Jack]] (USA, 1924–1981)
* [[Reinhardt Kiehl|Kiehl, Reinhardt]] (Germany, born 1935)
* [[Friedrich Wilhelm August Ludwig Kiepert|Kiepert, Friedrich Wilhelm August Ludwig]] (Germany, 1846–1934)
* [[Johann Kies|Kies, Johann]] (Germany, 1713–1781)
* [[Karl Johann Kiessling|Kiessling, Karl Johann]] (Germany, 1839–1905)

== Kil ==

* [[Wilhelm Killing|Killing, Wilhelm]] (Germany, 1847–1923)
* [[Peter Killworth|Killworth, Peter]] (England, 1946–2008)
* [[Clive W. Kilmister|Kilmister, Clive W.]] (Britain, 1924–2010)
* [[Jeong Han Kim|Kim, Jeong Han]] (South Korea, born 1962)
* [[Ju-Lee Kim|Kim, Ju-Lee]] (South Korea, ?–?)
* [[Kang-Tae Kim|Kim, Kang-Tae]] (South Korea, born 1957)
* [[Minhyong Kim|Kim, Minhyong]] (South Korea, ?–?)
* [[Chawne Kimber|Kimber, Chawne]] (?, ?–?)
* [[Clark Kimberling|Kimberling, Clark]] (USA, born 1942)
* [[Kurushima Kinai|Kinai, Kurushima]] (Japan, ?–1757)
* [[Al-Kindi]] (Medieval Arabia/Medieval Iraq, 801–873)
* [[Carl King-Millward|King-Millward, Carl]] (England, 1935–2000)
* [[Gregory King|King, Gregory]] (England, 1648–1712)
* [[Joshua King|King, Joshua]] (England, 1798–1857)
* [[Willford I. King|King, Willford I.]] (USA, 1880–1962)
* [[John Kingman|Kingman, John]] (England, born 1939)
* [[Donald Kingsbury|Kingsbury, Donald]] (Canada, born 1929)
* [[Calvin Kingsley|Kingsley, Calvin]] (?, 1812–1870)
* [[Hermann Kinkelin|Kinkelin, Hermann]] (Switzerland, 1832–1913)
* [[Charlotte Kipling|Kipling, Charlotte]] (Britain, 1919–1992)
* [[Robion Kirby|Kirby, Robion]] (USA, born 1938)
* [[Alexander Kirillov Jr.|Kirillov Jr., Alexander]] (Russia, ?–?)
* [[Alexandre Kirillov|Kirillov, Alexandre]] (Russia/Soviet Union, born 1936)
* [[Yelpidifor Anempodistovich Kirillov|Kirillov, Yelpidifor Anempodistovich]] (Russia, 1883–1964)
* [[Faina Mihajlovna Kirillova|Kirillova, Faina Mihajlovna]] (Belarus, born 1931)
* [[Vivien Kirk|Kirk, Vivien]] (New Zealand, ?–?)
* [[William Arthur Kirk|Kirk, William Arthur]] (USA, ?–?)
* [[Ellen Kirkman|Kirkman, Ellen]] (?, ?–?)
* [[Thomas Kirkman|Kirkman, Thomas]] (Britain, 1806–1895)
* [[Mojżesz David Kirszbraun|Kirszbraun, Mojżesz David]] (Poland, 1903–1942)
* [[Frances Kirwan|Kirwan, Frances]] (Britain, born 1959)

== Kis ==

* [[Andrei Petrovich Kiselyov|Kiselyov, Andrei]] (Russia/Soviet Union, 1852–1940)
* [[Laszlo B. Kish|Kish, Laszlo B.]] (?, born 1955)
* [[Leslie Kish|Kish, Leslie]] (USA, 1910–2000)
* [[Mark Kisin|Kisin, Mark]] (Australia, ?–?)
* [[Péter Kiss (mathematician)|Kiss, Péter]] (Hungary, 1937–2002)
* [[Tinne Hoff Kjeldsen|Kjeldsen, Tinne Hoff]] (Denmark, ?–?)
* [[Sergiu Klainerman|Klainerman, Sergiu]] (Romania, born 1950)
* [[Murray S. Klamkin|Klamkin, Murray S.]] (Canada, 1921–2004)
* [[David A. Klarner|Klarner, David A.]] (?, 1940–1999)
* [[Erica Klarreich|Klarreich, Erica]] (?, born 1972)
* [[Boáz Klartag|Klartag, Boáz]] (Israel, born 1978)
* [[Sandi Klavžar|Klavžar, Sandi]] (Slovenia, born 1962)
* [[Maria Klawe|Klawe, Maria]] (Canada, born 1951)
* [[Victor Klee|Klee, Victor]] (USA, 1925–2007)
* [[Stephen Cole Kleene|Kleene, Stephen Cole]] (USA, 1909–1994)
* [[Steven Kleiman|Kleiman, Steven]] (USA, born 1942)
* [[David Klein (mathematician)|Klein, David]] (USA, born 1953)
* [[Elisabeth Klein|Klein, Elisabeth]] (France, born 1953)
* [[Felix Klein|Klein, Felix Christian]] (Germany, 1849–1925)
* [[Jacob Klein (philosopher)|Klein, Jacob]] (?, 1899–1978)
* [[Jacob Theodor Klein|Klein, Jacob Theodor]] (Germany/Poland, 1685–1759)
* [[Willem Klein|Klein, Willem]] (?, 1912–1986)
* [[Bruce Kleiner|Kleiner, Bruce]] (USA, ?–?)
* [[Israel Kleiner (mathematician)|Kleiner, Israel]] (Canada, ?–?)
* [[Heinrich Kleisli|Kleisli, Heinrich]] (Switzerland, 1930–2011)
* [[Daniel Kleitman|Kleitman, Daniel]] (USA, born 1934)
* [[Valentin Klimov|Klimov, Valentin]] (Soviet Union/Russia, ?–?)
* [[John Robert Kline|Kline, John Robert]] (USA, 1891–1955)
* [[Morris Kline|Kline, Morris]] (USA, 1908–1992)
* [[Wilhelm Klingenberg|Klingenberg, Wilhelm]] (Germany, 1924–2010)
* [[Samuel Klingenstierna|Klingenstierna, Samuel]] (Sweden, 1698–1765)

== Klo ==

* [[Hendrik Kloosterman|Kloosterman, Hendrik]] (Netherlands, 1900–1968)
* [[Jan Willem Klop|Klop, Jan Willem]] (Netherlands, born 1945)
* [[Lipót Klug|Klug, Lipót]] (?, 1854–1945)
* [[Georg Simon Klügel|Klügel, Georg Simon]] (Germany, 1739–1812)
* [[Igor Kluvánek|Kluvánek, Igor]] (Slovakia/Australia, 1931–1993)
* [[Anthony W. Knapp|Knapp, Anthony W.]] (USA, born 1941)
* [[Harold A. Knapp|Knapp, Harold A.]] (USA, ?–1989)
* [[Bronisław Knaster|Knaster, Bronisław]] (Poland, 1893–1990)
* [[Alfred Kneschke|Kneschke, Alfred]] (Germany, 1902–1979)
* [[Adolf Kneser|Kneser, Adolf]] (Germany, 1862–1930)
* [[Hellmuth Kneser|Kneser, Hellmuth]] (Germany, 1898–1973)
* [[Martin Kneser|Kneser, Martin]] (Germany, 1928–2004)
* [[George Handley Knibbs|Knibbs, George Handley]] (Australia, 1858–1929)
* [[Genevieve M. Knight|Knight, Genevieve M.]] (?, born 1939)
* [[Julia F. Knight|Knight, Julia F.]] (USA, ?–?)
* [[Eberhard Knobloch|Knobloch, Eberhard]] (?, born 1943)
* [[Walter Knödel|Knödel, Walter]] (Austria, born 1926)
* [[Konrad Knopp|Knopp, Konrad]] (Germany, 1882–1957)
* [[Marvin Knopp|Knopp, Marvin]] (USA, 1933–2011)
* [[Wilbur Knorr|Knorr, Wilbur]] (?, 1945–1997)
* [[Horst Knörrer|Knörrer, Horst]] (Germany, born 1953)
* [[Cargill Gilston Knott|Knott, Cargill Gilston]] (Scotland, 1856–1922)
* [[Dilly Knox|Knox, Dilly]] (?, 1884–1943)
* [[Jens Martin Knudsen|Knudsen, Jens Martin]] (?, 1930–2005)
* [[Donald Knuth|Knuth, Donald]] (USA, born 1938)
* [[Allen Knutson|Knutson, Allen]] (USA, ?–?)
* [[Andrei Knyazev (mathematician)|Knyazev, Andrei]] (Russia/Soviet Union, born 1959)
* [[Shoshichi Kobayashi|Kobayashi, Shoshichi]] (Japan, 1932–2012)
* [[Toshiyuki Kobayashi|Kobayashi, Toshiyuki]] (Japan, born 1962)
* [[Hermann Kober|Kober, Hermann]] (Germany, 1888–1973)
* [[Neal Koblitz|Koblitz, Neal]] (USA, born 1948)

== Koc ==

* [[William Lawrence Kocay|Kocay, William Lawrence]] (?, ?–?)
* [[Helge von Koch|von Koch, Helge]] (Sweden, 1870–1924)
* [[Herbert Koch|Koch, Herbert]] (Germany, born 1962)
* [[Karl-Rudolf Koch|Koch, Karl-Rudolf]] (Germany, born 1935)
* [[Adam Adamandy Kochański|Kochański, Adam Adamandy]] (Poland, 1631–1700)
* [[Simon B. Kochen|Kochen, Simon B.]] (?, born 1934)
* [[Rudolf Kochendörffer|Kochendörffer, Rudolf]] (Germany, 1911–1980)
* [[Paul Kocher|Kocher, Paul]] (?, born 1973)
* [[Nikolai Kochin|Kochin, Nikolai]] (Russia/Soviet Union, 1901–1944)
* [[Ljubisa D.R. Kocinac|Kocinac, Ljubisa D.R.]] (Serbia, ?–?)
* [[Bogoljub Kočović|Kočović, Bogoljub]] (?, born 1920)
* [[Kunihiko Kodaira|Kodaira, Kunihiko]] (Japan, 1915–1997)
* [[Paul Koebe|Koebe, Paul]] (Germany, 1882–1945)
* [[Max Koecher|Koecher, Max]] (Germany, 1924–1990)
* [[Jan Koenderink|Koenderink, Jan]] (Netherlands, born 1943)
* [[Juan Ramón Koenig|Koenig, Juan Ramón]] (USA, 1623–1709)
* [[Gabriel Xavier Paul Koenigs|Koenigs, Gabriel Xavier Paul]] (France, 1858–1931)
* [[Elliot Koffman|Koffman, Elliot]] (USA, born 1942)
* [[Edward Kofler|Kofler, Edward]] (Switzerland, 1911–2007)
* [[Ervand Kogbetliantz|Kogbetliantz, Ervand G.]] (Armenia, 1888–1974)
* [[Sakabe Kōhan|Kōhan, Sakabe]] (Japan, 1759–1824)
* [[Yoshiharu Kohayakawa|Kohayakawa, Yoshiharu]] (Brazil/Japan, born 1963)
* [[Ulrich Kohlenbach|Kohlenbach, Ulrich]] (Germany, born 1962)
* [[Joseph J. Kohn|Kohn, Joseph J.]] (Czech Republic, born 1932)
* [[Robert V. Kohn|Kohn, Robert V.]] (USA, born 1953)
* [[Jurjen Ferdinand Koksma|Koksma, Jurjen Ferdinand]] (Netherlands, 1904–1964)
* [[Ellis Kolchin|Kolchin, Ellis]] (USA, 1916–1991)
* [[Tamara G. Kolda|Kolda, Tamara G.]] (?, ?–?)
* [[Antoon Kolen|Kolen, Antoon]] (Netherlands, 1953–2004)
* [[Milan Kolibiar|Kolibiar, Milan]] (Slovakia, 1922–1994)
* [[János Kollár|Kollár, János]] (USA/Hungary, born 1956)
* [[Louis Kollros|Kollros, Louis]] (?, 1878–1959)
* [[Ernst Kolman|Kolman, Ernst]] (Czech Republic, 1892–1979)
* [[Andrey Kolmogorov|Kolmogorov, Andrey]] (Russia/Soviet Union, 1903–1987)
* [[Gury Kolosov|Kolosov, Gury]] (Russia/Soviet Union, 1867–1936)
* [[Victor Kolyvagin|Kolyvagin, Victor]] (Russia, ?–?)

== Kom ==

* [[Péter Komjáth|Komjáth, Péter]] (Hungary, born 1953)
* [[János Komlós (mathematician)|Komlós, János]] (Hungary, born 1942)
* [[Boris Komrakov|Komrakov, Boris]] (Russia/Soviet Union, born 1948)
* [[Yuri Kondratiev|Kondratiev, Yuri]] (Ukraine, born 1953)
* [[Fatos Kongoli|Kongoli, Fatos]] (Albania, born 1944)
* [[Joseph Konhauser|Konhauser, Joseph]] (USA, 1924–1992)
* [[Dénes Kőnig|Kőnig, Dénes]] (Hungary, 1884–1944)
* [[Gyula Kőnig|Kőnig, Gyula]] (Hungary, 1849–1913)
* [[Johann Samuel König|König, Johann Samuel]] (?, 1712–1757)
* [[Julius König|König, Julius]] (Hungary, 1849–1913)
* [[Robert König|König, Robert]] (Austria, 1885–1979)
* [[Leo Königsberger|Königsberger, Leo]] (Germany, 1837–1921)
* [[Nikolay Konstantinov|Konstantinov, Nikolay]] (Russia/Soviet Union, born 1932)
* [[Maxim Kontsevich|Kontsevich, Maxim]] (Russia, born 1964)
* [[Sergei Konyagin|Konyagin, Sergei]] (Russia, born 1957)
* [[Bernard Koopman|Koopman, Bernard]] (USA, 1900–1981)
* [[Tjalling Koopmans|Koopmans, Tjalling]] (?, 1910–1985)
* [[Tom H. Koornwinder|Koornwinder, Tom H.]] (Netherlands, born 1943)
* [[Nancy Kopell|Kopell, Nancy]] (USA, born 1942)
* [[Ádám Korányi|Korányi, Ádám]] (USA/Hungary, born 1932)
* [[Gábor Korchmáros|Korchmáros, Gábor]] (Hungary, born 1948)
* [[Boris Kordemsky|Kordemsky, Boris]] (?, 1907–1999)
* [[Boris Korenblum|Korenblum, Boris]] (Soviet Union/Israel, 1923–2011)
* [[Vladimir Korepin|Korepin, Vladimir]] (Russia, born 1951)
* [[Jacob Korevaar|Korevaar, Jacob]] (Netherlands, born 1923)
* [[Aleksandr Korkin|Korkin, Aleksandr]] (Russia, 1837–1908)
* [[András Kornai|Kornai, András]] (Hungary, born 1957)
* [[Thomas Körner|Körner, Thomas]] (Britain, born 1946)
* [[Thomas William Körner|Körner, Thomas William]] (Britain, born 1946)
* [[Viktor Korolev|Korolev, Viktor]] (Russia, born 1954)
* [[Volodymyr Korolyuk|Korolyuk, Volodymyr]] (Ukraine, born 1925)
* [[Andrey Korotayev|Korotayev, Andrey]] (Russia, born 1961)
* [[Maria Korovina|Korovina, Maria]] (Russia, born 1962)
* [[Pavel Korovkin|Korovkin, Pavel]] (Russia/Soviet Union, 1913–1985)
* [[Alwin Korselt|Korselt, Alwin Reinhold]] (Germany, 1864–1947)
* [[Bernhard Korte|Korte, Bernhard]] (Germany, born 1938)
* [[Diederik Korteweg|Korteweg, Diederik]] (Netherlands, 1848–1941)
* [[Alfred Korzybski|Korzybski, Alfred]] (Poland, 1879–1950)

== Kos ==

* [[Damodar Dharmananda Kosambi|Kosambi, Damodar Dharmananda]] (India, 1907–1966)
* [[Volodymyr Koshmanenko|Koshmanenko, Volodymyr]] (Ukraine, born 1943)
* [[Bart Kosko|Kosko, Bart]] (?, born 1960)
* [[Yvette Kosmann-Schwarzbach|Kosmann-Schwarzbach, Yvette]] (France, born 1941)
* [[Bertram Kostant|Kostant, Bertram]] (USA, born 1928)
* [[Carl Kostka|Kostka, Carl]] (Germany, 1846–1921)
* [[Alexei Kostrikin|Kostrikin, Alexei]] (Soviet Union/Russia, 1929–2000)
* [[Jean-Louis Koszul|Koszul, Jean-Louis]] (France, 1921–2018)
* [[Tadeusz Kotarbiński|Kotarbiński, Tadeusz]] (Poland, 1886–1981)
* [[Aleksandr Kotelnikov|Kotelnikov, Aleksandr]] (Russia, 1865–1944)
* [[Gottfried Köthe|Köthe, Gottfried]] (Germany/Austria, 1905–1989)
* [[Dieter Kotschick|Kotschick, Dieter]] (?, born 1963)
* [[Ernst Kötter|Kötter, Ernst]] (Germany, 1859–1922)
* [[Robert Kottwitz|Kottwitz, Robert]] (USA, born 1950)
* [[Samuel Kotz|Kotz, Samuel]] (?, 1930–2010)
* [[Anton Kotzig|Kotzig, Anton]] (Slovakia/Canada, 1919–1991)
* [[Joseph Kouneiher|Kouneiher, Joseph]] (France, born 1963)
* [[Sofia Kovalevskaya|Kovalevskaya, Sofia]] (Russia/Sweden, 1850–1891)
* [[Gerhard Kowalewski|Kowalewski, Gerhard]] (Germany, 1876–1950)
* [[Boris Koyalovich|Koyalovich, Boris]] (Russia, 1867–1941)
* [[Shin-ya Koyama|Koyama, Shin-ya]] (?, ?–?)
* [[Gady Kozma|Kozma, Gady]] (Israel, ?–?)
* [[Robert Kozma|Kozma, Robert]] (USA, ?–?)
* [[Bryna Kra|Kra, Bryna]] (USA, born 1966)
* [[Irwin Kra|Kra, Irwin]] (USA, born 1937)
* [[Jens Kraft|Kraft, Jens]] (Denmark, 1720–1756)
* [[Edgar Krahn|Krahn, Edgar]] (Estonia, 1894–1961)
* [[Maurice Kraitchik|Kraitchik, Maurice]] (Belgium, 1882–1957)
* [[Stanisław Krajewski|Krajewski, Stanisław]] (Poland, born 1950)
* [[Daniel Kráľ|Kráľ, Daniel]] (Czech Republic, born 1978)
* [[Hrvoje Kraljević|Kraljević, Hrvoje]] (Croatia, born 1944)
* [[Edna Kramer|Kramer, Edna]] (USA, 1902–1984)
* [[Hans Kramers|Kramers, Hans]] (?, 1894–1952)
* [[Dmitry Kramkov|Kramkov, Dmitry]] (Russia/Soviet Union, ?–?)
* [[Christian Kramp|Kramp, Christian]] (France, 1760–1826)
* [[Steven G. Krantz|Krantz, Steven G.]] (USA, born 1951)
* [[Marc Krasner|Krasner, Marc]] (France, 1912–1985)
* [[Naum Krasner|Krasner, Naum]] (Soviet Union/Russia, 1924–1999)
* [[Mark Krasnosel'skii|Krasnosel'skii, Mark]] (Russia/Soviet Union/Ukraine, 1920–1997)
* [[Nikolay Krasovsky|Krasovsky, Nikolay]] (Russia, 1924–2012)
* [[Jan Kratochvíl|Kratochvíl, Jan]] (Czech Republic, born 1959)
* [[Christian Krattenthaler|Krattenthaler, Christian]] (Austria, born 1958)
* [[Nicholas Kratzer|Kratzer, Nicholas]] (?, 1480s–1550)
* [[Mikhail Kravchuk|Kravchuk, Mikhail Philippovich]] (Ukraine/Soviet Union, 1892–1942)
* [[Asher Kravitz|Kravitz, Asher]] (Israel, born 1969)
* [[Adolf Krazer|Krazer, Adolf]] (Germany, 1858–1926)

== Kre ==

* [[Matthias Kreck|Kreck, Matthias]] (Germany, born 1947)
* [[Mark Krein|Krein, Mark]] (Ukraine/Soviet Union, 1907–1989)
* [[Georg Kreisel|Kreisel, Georg]] (Austria, 1923–2015)
* [[Heinz-Otto Kreiss|Kreiss, Heinz-Otto]] (?, 1930–2015)
* [[Jakub Kresa|Kresa, Jakub]] (Czech Republic, 1648–1715)
* [[Erich Kretschmann|Kretschmann, Erich]] (Germany, 1887–1973)
* [[Martin Kreuzer|Kreuzer, Martin]] (Germany, born 1962)
* [[Erwin Kreyszig|Kreyszig, Erwin]] (Canada, 1922–2008)
* [[Cecilia Krieger|Krieger, Cecilia]] (Poland/Canada, 1894–1974)
* [[Gregory Kriegsmann|Kriegsmann, Gregory]] (USA, ?–?)
* [[Saul Kripke|Kripke, Saul]] (USA, born 1940)
* [[E. M. V. Krishnamurthy|Krishnamurthy, E. M. V.]] (?, 1934–2012)
* [[Michael Krivelevich|Krivelevich, Michael]] (Israel, born 1966)
* [[Josip Križan|Križan, Josip]] (Slovenia, 1841–1921)
* [[France Križanič|Križanič, France]] (Slovenia, 1928–2002)
* [[Leopold Kronecker|Kronecker, Leopold]] (Germany, 1823–1891)
* [[Peter B. Kronheimer|Kronheimer, Peter B.]] (Britain, born 1963)
* [[Alexander Kronrod|Kronrod, Alexander]] (Russia/Soviet Union, 1921–1986)
* [[André Krüger|Krüger, André]] (Germany, born 1964)
* [[Johann Heinrich Louis Krüger|Krüger, Johann Heinrich Louis]] (Germany, 1857–1923)
* [[Wolfgang Krull|Krull, Wolfgang]] (Germany, 1899–1971)
* [[Joseph Kruskal|Kruskal, Joseph]] (USA, 1928–2010)
* [[Martin David Kruskal|Kruskal, Martin David]] (USA, 1925–2006)
* [[William Kruskal|Kruskal, William]] (USA, 1919–2005)
* [[Anna Zofia Krygowska|Krygowska, Anna Zofia]] (?, 1904–1988)
* [[Zdzisław Krygowski|Krygowski, Zdzisław]] (Poland, 1872–1955)
* [[Aleksey Krylov|Krylov, Aleksey]] (Russia, 1863–1945)
* [[Andrei Krylov (mathematician)|Krylov, Andrei (mathematician)]] (Russia, born 1956)
* [[Nicolai V. Krylov|Krylov, Nicolai V.]] (Russia/Soviet Union, born 1941)
* [[Nikolay Mitrofanovich Krylov|Krylov, Nikolay Mitrofanovich]] (Russia/Ukraine/Soviet Union, 1879–1955)
* [[Nikolay Sergeyevich Krylov|Krylov, Nikolay Sergeyevich]] (?, 1917–1947)

== Krz ==

* [[Adrian Krzyżanowski|Krzyżanowski, Adrian]] (Poland, 1788–1852)
* [[Daniel Kubert|Kubert, Daniel]] (USA, 1947–2010)
* [[Ewa Kubicka|Kubicka, Ewa]] (Poland, ?–?)
* [[Jonas Kubilius|Kubilius, Jonas]] (Lithuania, 1921–2011)
* [[Vera Kublanovskaya|Kublanovskaya, Vera]] (Russia, 1921–2012)
* [[Tomio Kubota|Kubota, Tomio]] (Japan, born 1930)
* [[Marek Kuczma|Kuczma, Marek]] (Poland, 1935–1991)
* [[Stephen S. Kudla|Kudla, Stephen S.]] (USA, born 1950)
* [[Michio Kuga|Kuga, Michio]] (Japan, 1928–1990)
* [[Franz Xaver Kugler|Kugler, Franz Xaver]] (Germany, 1862–1929)
* [[Daniela Kühn|Kühn, Daniela]] (Germany, born 1973)
* [[Harold W. Kuhn|Kuhn, Harold W.]] (?, 1925–2014)
* [[Nicolaas Kuiper|Kuiper, Nicolaas]] (Netherlands, 1920–1994)
* [[Sergei B. Kuksin|Kuksin, Sergei B.]] (Soviet Union, born 1955)
* [[Jakob Philipp Kulik|Kulik, Jakob Philipp]] (Austria, 1793–1863)
* [[Ulrich Kulisch|Kulisch, Ulrich]] (Germany, born 1933)
* [[Ravindra Shripad Kulkarni|Kulkarni, Ravindra Shripad]] (India, born 1942)
* [[Solomon Kullback|Kullback, Solomon]] (USA, 1903–1994)
* [[Hitoshi Kumano-Go|Kumano-Go, Hitoshi]] (Japan, 1935–1982)
* [[Anand Kumar|Kumar, Anand]] (India, born 1973)
* [[Neithalath Mohan Kumar|Kumar, Neithalath Mohan]] (?, born 1951)
* [[Palash Kumar|Kumar, Palash]] (India, ?–?)
* [[Shrawan Kumar|Kumar, Shrawan]] (?, born 1953)
* [[Poondi Kumaraswamy|Kumaraswamy, Poondi]] (India, 1930–1988)
* [[Ernst Kummer|Kummer, Ernst]] (Germany, 1810–1893)
* [[Toru Kumon|Kumon, Toru]] (Japan, 1914–1995)
* [[Kenneth Kunen|Kunen, Kenneth]] (USA, born 1943)
* [[Hermann Künneth|Künneth, Hermann]] (Germany, 1892–1975)
* [[Hans-Rudolf Künsch|Künsch, Hans-Rudolf]] (Switzerland, born 1951)
* [[Jean Kuntzmann|Kuntzmann, Jean]] (France, 1912–1992)
* [[Ray Kunze|Kunze, Ray]] (?, 1928–2014)

== Kuo ==

* [[Shen Kuo|Kuo, Shen]] (Medieval China, 1031–1095)
* [[Greg Kuperberg|Kuperberg, Greg]] (Poland, born 1967)
* [[Krystyna Kuperberg|Kuperberg, Krystyna]] (Poland, born 1944)
* [[Włodzimierz Kuperberg|Kuperberg, Włodzimierz]] (Poland/USA, born 1941)
* [[Antti Kupiainen|Kupiainen, Antti]] (Finland, born 1954)
* [[Robert Kupperman|Kupperman, Robert]] (USA, 1935–2006)
* [[Masatake Kuranishi|Kuranishi, Masatake]] (?, born 1924)
* [[Kazimierz Kuratowski|Kuratowski, Kazimierz]] (Poland, 1896–1980)
* [[Đuro Kurepa|Kurepa, Đuro]] (Serbia/Yugoslavia, 1907–1993)
* [[Svetozar Kurepa|Kurepa, Svetozar]] (Serbia/Yugoslavia, 1929–2010)
* [[Věra Kůrková|Kůrková, Věra]] (Czech Republic, born 1948)
* [[Sigekatu Kuroda|Kuroda, Sigekatu]] (?, ?–?)
* [[Nobushige Kurokawa|Kurokawa, Nobushige]] (Japan, born 1952)
* [[Aleksandr Gennadievich Kurosh|Kurosh, Aleksandr Gennadievich]] (Russia/Soviet Union, 1908–1971)
* [[József Kürschák|Kürschák, József]] (Hungary, 1864–1933)
* [[Thomas G. Kurtz|Kurtz, Thomas G.]] (USA, born 1941)
* [[Jaroslav Kurzweil|Kurzweil, Jaroslav]] (Czech Republic, born 1926)
* [[Boris Kushner|Kushner, Boris]] (Russia, born 1941)
* [[Rachel Kuske|Kuske, Rachel]] (Canada/USA, ?–?)
* [[Semën Samsonovich Kutateladze|Kutateladze, Semën Samsonovich]] (Russia, born 1945)
* [[Mers Kutt|Kutt, Mers]] (Canada, born 1933)
* [[Martin Kutta|Kutta, Martin]] (?, 1867–1944)
* [[Brian Kuttner|Kuttner, Brian]] (England, 1908–1992)
* [[J. Nathan Kutz|Kutz, J. Nathan]] (?, ?–?)
* [[Philip Kutzko|Kutzko, Philip]] (USA, born 1946)
* [[Steve Kuzmicich|Kuzmicich, Steve]] (New Zealand, born 1931)
* [[Rodion Kuzmin|Kuzmin, Rodion]] (Soviet Union/Russia, 1891–1949)
* [[Simon Kuznets|Kuznets, Simon]] (USA, 1901–1985)
* [[Alexander Kuznetsov (mathematician)|Kuznetsov, Alexander]] (Russia, born 1973)
* [[Yuri A. Kuznetsov|Kuznetsov, Yuri A.]] (Russia/USA, ?–?)

[[Category:Mathematics-related lists]]</text>
      <sha1>dya4hgq4grf9lybbhuhdnceukps08oc</sha1>
    </revision>
  </page>
  <page>
    <title>Mivar-based approach</title>
    <ns>0</ns>
    <id>51291103</id>
    <revision>
      <id>840982798</id>
      <parentid>832930387</parentid>
      <timestamp>2018-05-13T09:07:51Z</timestamp>
      <contributor>
        <username>Clinamental</username>
        <id>26093074</id>
      </contributor>
      <minor/>
      <comment>martix ---&gt; matrix</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="20939">{{Multiple issues|
{{More footnotes|date=January 2018}}
{{Orphan|date=August 2016}}
}}

The '''Mivar-based approach''' is a mathematical tool for designing [[artificial intelligence]] (AI) systems. Mivar (''Multidimensional Informational Variable Adaptive Reality'') was developed by combining production  and [[Petri nets]]. The Mivar-based approach was developed for semantic analysis and adequate representation of humanitarian [[Epistemology|epistemological]] and [[Axiology|axiological]] principles in the process of developing artificial intelligence. The Mivar-based approach incorporates [[computer sciences|computer science]], [[informatics]] and [[discrete mathematics]], [[databases]], [[expert systems]],&lt;ref&gt;{{Cite journal|last=Shadrin|first=S.S.|last2=Varlamov|first2=O.O.|last3=Ivanov|first3=A.M.|date=2017|title=Experimental Autonomous Road Vehicle with Logical Artificial Intelligence|url=https://www.hindawi.com/journals/jat/2017/2492765/|journal=Journal of Advanced Transportation|language=en|volume=2017|pages=1–10|doi=10.1155/2017/2492765|issn=0197-6729|via=}}&lt;/ref&gt; [[graph theory]], matrices and inference systems. The Mivar-based approach involves two technologies:&lt;ref name=":0"&gt;{{Cite journal|last=Varlamov|first=Oleg|date=5 Nov 2011|title=MIVAR: Transition from Productions to Bipartite Graphs MIVAR Nets and Practical Realization of Automated Constructor of Algorithms Handling More than Three Million Production Rules|url=https://arxiv.org/ftp/arxiv/papers/1111/1111.1321.pdf|journal=Cornell University|volume=|issue=|doi=|pmid=|access-date=|via=}}&lt;/ref&gt;
* '''''Information accumulation''''' is a method of creating global evolutionary data-and-rules bases with variable structure. It works on the basis of adaptive, discrete, mivar-oriented information space, unified data and rules representation, based on three main concepts: “object, property, relation”. Information accumulation is designed to store any information with possible evolutionary structure and without limitations concerning the amount of information and forms of its presentation.&lt;ref&gt;{{Cite journal|last=Chuvikov|first=D.A.|last2=Kazakova|first2=N.A.|last3=Varlamov|first3=O.O.|last4=Goloviznin|first4=A.V.|date=2015-01-14|title=3D Modeling and 3D Objects Creation Technology Analysis for Various Intelligent Systems|url=http://journal-s.org/index.php/ijas/article/view/4065|journal=International Journal of Advanced Studies|language=en|volume=4|issue=4|pages=|doi=10.12731/2227-930x-2014-4-3|issn=2227-930X|via=}}&lt;/ref&gt;
* '''''Data processing'''''&lt;ref&gt;{{Cite journal|last=Adamova|first=L.E.|last2=Protopopova|first2=D.A.|date=2017-05-25|title=INTELLIGENT QUESTION-ANSWERING SYSTEM "MIVAR VIRTUAL CONSULTANT"|url=http://journal-s.org/index.php/ijas/article/view/9898|journal=International Journal of Advanced Studies|language=en|volume=6|issue=3|pages=9–19|doi=10.12731/2227-930x-2016-3-9-19|issn=2227-930X|via=}}&lt;/ref&gt; is a method of creating a logical inference system or automated algorithm construction from modules, services or procedures on the basis of a trained mivar network of rules with linear computational complexity. Mivar data processing includes logical inference, computational procedures and services.
Mivar networks allow us to develop cause-effect dependencies (“If-then”) and create an automated, trained, logical reasoning system.

Representatives of Russian association for artificial intelligence (RAAI) – for example, [[Vladimir Gorodetski|V. I. Gorodecki]], doctor of technical science, [[professor]] at SPIIRAS and V. N. Vagin, doctor of technical science, professor at [[Moscow Power Engineering Institute|MPEI]] declared that the term is incorrect and suggested that the author should use standard terminology.

== History ==
While working in the [[Ministry of Defence (Russia)|Russian Ministry of Defense]], O. O. Varlamov started developing the theory of “rapid logical inference” in 1985.&lt;ref name=":1"&gt;{{Cite web|url=http://rbth.com/science_and_tech/2015/05/18/startup_creates_brain_for_robots_capable_of_making_independe_46129.html|title=Startup creates 'brain' for robots capable of making independent decisions|last=Zavyalova|first=Victoria|date=2015-05-18|language=en-US|access-date=2016-08-10}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Panferov|first=A.A.|last2=Zhdanovich|first2=E.A.|last3=Yufimychev|first3=K.A.|last4=Chuvikov|first4=D.A.|date=2017-05-25|title=DESIGNING ALGORITHMS FOR SERVICE ROBOTS ON THE BASIS OF MIVAR APPROACH|url=http://journal-s.org/index.php/ijas/article/view/9903|journal=International Journal of Advanced Studies|language=en|volume=6|issue=3|pages=72–86|doi=10.12731/2227-930X-2016-3-72-86|issn=2227-930X|via=}}&lt;/ref&gt; He was analyzing Petri nets and productions to construct algorithms. Generally, mivar-based theory represents an attempt to combine [[Entity–relationship model|entity-relationship models]] and their problem instance – semantic networks and Petri networks.

The abbreviation MIVAR was introduced as a technical term by O. O. Varlamov, Doctor of Technical Science, professor at [[Bauman Moscow State Technical University|Bauman MSTU]] in 1993 to designate a “semantic unit” in the process of mathematical modeling.&lt;ref name=":1" /&gt;&lt;ref&gt;{{Cite journal|last=Varlamov|first=O.O.|last2=Adamova|first2=L.E.|last3=Eliseev|first3=D.V.|last4=Mayboroda|first4=Yu.I.|last5=Antonov|first5=P.D.|last6=Sergushin|first6=G.S.|last7=Chibirova|first7=M.O.|date=2014-04-17|title=MIVAR THECHNOLOGIES IN MATHEMATICAL MODELING OF NATURAL LANGUAGE, IMAGES AND HUMAN SPEECH UNDERSTANDING|url=https://dx.doi.org/10.12731/2227-930X-2013-3-3|journal=International Journal of Advanced Studies|language=en|volume=3|issue=3|pages=17–23|doi=10.12731/2227-930x-2013-3-3|issn=2227-930X|via=}}&lt;/ref&gt; The term has been established and used in all of his further works.

The first experimental systems operating according to mivar-based principles were developed in 2000. Applied mivar systems were introduced in 2015.

== Mivar ==
Mivar is the smallest structural element of discrete information space.

== Object-property-relation ==
Object-Property-Relation (VSO) is a graph, the nodes of which are concepts and arcs are connections between concepts.

Mivar space represents a set of axes, a set of elements, a set of points of space and a set of values of points.

&lt;math&gt;A = \{a_n\}, n = 1,\ldots,N,&lt;/math&gt;

where:

* &lt;math&gt;A&lt;/math&gt; is a set of mivar space axis names; 
* &lt;math&gt;N&lt;/math&gt; is a number of mivar space axes.

Then: &lt;math&gt;\forall a_n \exists F_n =\{ f_{{ni}_n}\}, n=1,\ldots,N, i_n = 1, \ldots, I_n, &lt;/math&gt;

where:

* &lt;math&gt;F_n&lt;/math&gt; is a set of axis &lt;math&gt;a_n&lt;/math&gt; elements;
* &lt;math&gt;i_n&lt;/math&gt; is a set &lt;math&gt;F_n&lt;/math&gt; element identifier;
* &lt;math&gt;I_n = |F_n|.&lt;/math&gt;

&lt;math&gt;F_n&lt;/math&gt; sets form multidimensional space: &lt;math&gt;M=F_1\times F_2\times \cdots \times F_n.&lt;/math&gt;
&lt;math&gt;m = (i_1, i_2,\ldots ,i_N),&lt;/math&gt;

where:

* &lt;math&gt;m \in M&lt;/math&gt;;
* &lt;math&gt;m&lt;/math&gt; is a point of multidimensional space;
* &lt;math&gt;(i_1, i_2,\ldots, i_N)&lt;/math&gt; are coordinates of point &lt;math&gt;m&lt;/math&gt;.

There is a set of values of multidimensional space points of &lt;math&gt;M&lt;/math&gt;:

: &lt;math&gt;C_M=\{c_{i_1,i_2,\ldots,i_N} \mid i_1 = 1,\ldots,I_1, i_2 = 1,\ldots,I_2, \ldots , i_n = 1,\ldots,I_N\},&lt;/math&gt;

where:

* &lt;math&gt;c_{i_1,i_2,\ldots,i_N}&lt;/math&gt; is a value of the point of multidimensional space &lt;math&gt;M&lt;/math&gt; is a value of the point of multidimensional space &lt;math&gt;(i_1,i_2,\ldots,i_N)&lt;/math&gt;.
For every point of space &lt;math&gt;M&lt;/math&gt; there is a single value from &lt;math&gt;C_M&lt;/math&gt; set or there is no such value. Thus,&lt;math&gt;C_M&lt;/math&gt; is a set of data model state changes represented in multidimensional space. To implement a transition between multidimensional space and set of points values the relation &lt;math&gt;\mu&lt;/math&gt; has been introduced: &lt;math&gt;C_x=\mu(M_x),&lt;/math&gt;

[[File:VSO.png|thumb|Mivar information space]]

where: 
* &lt;math&gt;M_x\subseteq M; &lt;/math&gt;
* &lt;math&gt;M_x = F_{1x}\times   F_{2x} \times \cdots \times F_{Nx}. &lt;/math&gt;

To describe a data model in mivar information space it is necessary to identify three axes:

* The axis of relations «&lt;math&gt;O&lt;/math&gt;»;
* The axis of attributes (properties) «&lt;math&gt;S&lt;/math&gt;»;  
* The axis of elements (objects) of subject domain «&lt;math&gt;V&lt;/math&gt;».

These sets are independent. The mivar space can be represented by the following tuple:

: &lt;math&gt;\langle V, S, O\rangle&lt;/math&gt;

Thus, mivar is described by «&lt;math&gt;VSO&lt;/math&gt;» formula, in which «&lt;math&gt;V&lt;/math&gt;» denotes an object or a thing, «&lt;math&gt;S&lt;/math&gt;» denotes properties, «&lt;math&gt;O&lt;/math&gt;» variety of relations between other objects of a particular subject domain.&lt;ref&gt;{{Cite journal|last=Chuvikov|first=D.A.|last2=Nazarov|first2=K.V.|date=2017-05-25|title=DESIGNING ALGORITHMS FOR SOLVING PHYSICS PROBLEMS ON THE BASIS OF MIVAR APPROACH|url=http://journal-s.org/index.php/ijas/article/view/9900|journal=International Journal of Advanced Studies|language=en|volume=6|issue=3|pages=31–50|doi=10.12731/2227-930x-2016-3-31-50|issn=2227-930X|via=}}&lt;/ref&gt; The category “Relations” can describe dependencies of any complexity level: formulae, logical transitions, text expressions, functions, services, computational procedures and even [[Artificial neural network|neural networks]]. A wide range of capabilities complicates description of modeling interconnections, but can take into consideration all the factors. Mivar computations use mathematical logic. In a simplified form they can be represented as implication in the form of an "if…, then …”&lt;ref&gt;{{Cite journal|last=Eugene|first=Kovshov|date=|title=MIVAR technology as a new generation in Artificial Intelligence (AI)|url=http://www.oulu.fi/sites/default/files/seminars/MIVAR%20technology%20%28Abstract%29.pdf|journal=|volume=|issue=|doi=|pmid=|access-date=|via=}}&lt;/ref&gt; formula. The result of mivar modeling can be represented in the form of a bipartite graph binding two sets of objects: source objects and resultant objects.

== Mivar network ==
[[File:Формирование двудольной миварной сети.png|thumb|Mivar network representation in the form of a bipartite directed graph]]
'''''Mivar network''''' is a method for representing objects of the subject domain and their processing rules in the form of a bipartite directed graph consisting of objects and rules.&lt;ref&gt;{{Cite journal|last=Varlamov|first=O.O.|last2=Danilkin|first2=A.I.|last3=Shoshev|first3=A.I.|date=2016-10-03|title=Mivar technologies in knowledge representation and reasoning|url=http://elib.bsu.by/bitstream/123456789/158772/1/Varlamov_Danilkin_Shoshev.pdf|journal=PRIP’2016|language=en|volume=|pages=30–32|via=}}&lt;/ref&gt;

A Mivar network is a bipartite graph that can be described in the form of a two-dimensional matrix, in that records information about the subject domain of the current task.&lt;ref&gt;{{Cite journal|last=Ivanchenko|first=N.O.|date=2014-01-01|title=Mivar technologies modelling of enterprise's technical and technological potential|url=http://nbuv.gov.ua/UJRN/ape_2014_1_61|journal=Актуальні проблеми економіки|volume=|issue=1|doi=|issn=1993-6788|pmid=|access-date=|via=}}&lt;/ref&gt;&lt;ref name=":2"&gt;{{Cite journal|last=Sandu|first=R. A.|date=2010-10-01|title=A method of processing experimental data on the parameters of physical processes in information-measurement systems based on Mivar logical nets|url=https://link.springer.com/article/10.1007/s11018-010-9548-0|journal=Measurement Techniques|language=en|volume=53|issue=6|pages=600–604|doi=10.1007/s11018-010-9548-0|issn=0543-1972}}&lt;/ref&gt;

Generally, mivar networks provide formalization and representation of human knowledge in the form of a connected multidimensional space. That is, a mivar network is a method of representing a piece of mivar space information in the form of a bipartite, [[directed graph]]. The mivar space information is formed by objects and connections, which in total represent the data model of the subject domain.Connections include rules for objects processing. Thus, a mivar network of a subject domain is a part of the mivar space knowledge for that domain.

The graph can consist of objects-variables and rules-procedures. First, two lists are made that form two nonintersecting partitions: the list of objects and the list of rules. Objects are denoted by circles. Each rule in a mivar network is an extension of productions, hyper-rules with multi-activators or computational procedures. It is proved that from the perspective of further processing, these formalisms are identical and in fact are nodes of the bipartite graph, denoted by rectangles.&lt;ref name=":2" /&gt;

=== Multi-dimensional binary matrices ===
Mivar networks can be implemented on single computing systems or [[service-oriented architecture]]s. Certain constraints restrict their application, in particular, the dimension of matrix of linear matrix method for determining logical inference path on the adaptive rule networks. The matrix dimension constraint is due to the fact that implementation requires sending a general matrix to multiple processors. Since every matrix value is initially represented in symbol form, the amount of sent data is crucial when obtaining, for example, 10000 rules/variables. Classical mivar-based method requires storing three values in each matrix cell:

* 0 – no value;
* x – input variable for the rule;
* y – output variable for the rule.

The analysis of possibility of firing a rule is separated from determining output variables according to stages after firing the rule. Consequently, it is possible to use different matrices for “search for fired rules” and “setting values for output variables”. This allowsthe use of multidimensional binary matrices. Binary matrix fragments occupy much less space and improve possibilities of applying mivar networks.

== Logical and computational data processing ==
To implement logical-and-computational data processing the following should be done. First, a formalized subject domain description is developed. The main objects-variables and rules-procedures are specified on the basis of mivar-based approach and then corresponding lists of “objects” and “rules” are formed. This formalized representation is analogous to the bipartite logical network graph.

The main stages of mivar-based information processing are:
* Forming a subject domain matrix;
* Working with the matrix and designing the solution algorithm for the task; 
* Executing the computations and finding the solution.

The first stage is the stage of synthesis of conceptual subject domain model and its formalization in the form of production rules with a transition to mivar rules. “Input objects – rules/procedures – output objects”. Currently, this stage is the most complex and requires involvement of a human expert to develop a mivar model of the subject domain.

Automated solution algorithm construction or logical inference is implemented at the second stage. Input data for algorithm construction are: mivar matrix of subject domain description and a set input of object-variables and required object-variables.

The solution is implemented at the third stage.&lt;ref&gt;{{Cite journal|last=A.Viattchenin|first=Dmitri|last2=Shyrai|first2=Stanislau|title=Intuitionistic Heuristic Prototype-based Algorithm of Possibilistic Clustering|url=http://caeaccess.org/research/volume1/number8/cae-1629.pdf|journal=Communications on Applied Electronics|volume=1|issue=8|pages=30–40|doi=10.5120/cae-1629}}&lt;/ref&gt;

=== Data processing method ===
Firstly, the matrix is constructed. Matrix analysis determines whether a successful inference path exists. Then possible logical inference paths are defined and at the last stage the shortest path is selected according to the set optimality criteria.
[[File:Formation of mivar network matrix.png|thumb|Formation of mivar network matrix|230x230px]]
Let &lt;math&gt;m&lt;/math&gt; rules and &lt;math&gt;n&lt;/math&gt; variables be included in the rules as input variables activating them or as  output variables. Then, matrix &lt;math&gt;V(m\cdot n)&lt;/math&gt;, each row of which corresponds to one of the rules and contains the information about variables used in the rule, can represent all the interconnections between rules and [[Variable (mathematics)|variables]].

* In each row all the input variables are denoted by &lt;math&gt;x&lt;/math&gt; in the corresponding positions of the matrix, all the output variables are denoted by &lt;math&gt;y&lt;/math&gt;. 
* All the variables that have already obtained certain value in the process of inference or setting input data – &lt;math&gt;z&lt;/math&gt;. 
* All the required (output) variables, that is, the variables that should be obtained on the basis of input data – &lt;math&gt;w&lt;/math&gt;.

One row and one column are added in the matrix &lt;math&gt;V&lt;/math&gt; to store service information. [[File:An example of matrix processing for solving the problem.png|thumb|An example of matrix processing for solving the problem|564x564px]] So, the matrix &lt;math&gt;V&lt;/math&gt; of dimension &lt;math&gt;(m+n)\times(n+1)&lt;/math&gt;, is obtained, which shows the whole structure of the source rule network. The structure of this logical network can change, that is, this is a network of rules with evolutionary dynamics.

=== Example ===
To search for a logical inference path the following actions are implemented:

# Known variables are denoted by &lt;math&gt;z&lt;/math&gt; and required variables are denoted by w in the row &lt;math&gt;(m+1)&lt;/math&gt;. For example, &lt;math&gt;z&lt;/math&gt; denotes positions: 1,2,3 in the row &lt;math&gt;(m+1)&lt;/math&gt;), the variable &lt;math&gt;w&lt;/math&gt; denotes the position &lt;math&gt;(n-2)&lt;/math&gt;. 
# The search of such rules that can be fired, that is, all the input variables of which are known, is implemented successively, for example, top-down. Absent such rules, no logical inference path exists and input data refinement (addition) is requested. Rules that can be fired, are labeled in the corresponding place of service row. For example, we can write 1 in the matrix cell, which is illustrated in the cell &lt;math&gt;(1, n+1)&lt;/math&gt;. 
# Given several such rules, the choice of rules to fire first is implemented according to previously determined criteria. Several rules can be fired simultaneously if sufficient resources are available. 
# Rule (procedure) firing simulation is implemented by assigning the values “known” to the variables inferred in this rule, that is, &lt;math&gt;z&lt;/math&gt; in this example. A fired rule can be marked additionally, for example by number 2 for convenience of further work. For example, the corresponding changes are made in the cells &lt;math&gt;(m+1, n-1), (m+1, n)&lt;/math&gt; and &lt;math&gt;(1, n+1)&lt;/math&gt;.
# After rule firing simulation, goal achievement analysis is carried out, that is, required value acquisition is analyzed by comparing special characters in the service row. Given at least one “unknown” (&lt;math&gt;w&lt;/math&gt;) value in the service row &lt;math&gt;(m+1)&lt;/math&gt;, logical inference path search is carried out. Otherwise, the task is considered to be solved successfully and the rules fired in a corresponding order form the logical inference path searched.
# The availability of the rules that can be fired after defining new values at the previous stage is assessed. Absent firearable rules, no inference path exists and actions are taken analogous to step 2. Given fireable rules the inference path search continues. In this example such rules exist. In cell &lt;math&gt;(2, n+1)&lt;/math&gt; the number 1 is obtained as an indication that this rule can be fired. 
# At the next stage, analogous to stage 4, the rules are fired (rule firing simulation), analogous to stages 5 and 6 necessary actions are performed to obtain the result. Stages 2–7 are implemented until the result is achieved. A path may or may not be found. 
# Deducibility of the variables 4 and 5 in the cells &lt;math&gt;(m+1, 4)&lt;/math&gt; and &lt;math&gt;(m+1, 5)&lt;/math&gt; is obtained, and indication that the rule has already been fired in the cell &lt;math&gt;(2, n+1)&lt;/math&gt; is formed, that is, the number 2 is set. After that the analysis of the service row is carried out, which shows that not all the required variables are known. Thus, it is necessary to continue processing matrix &lt;math&gt;V&lt;/math&gt; of dimension &lt;math&gt;(m+n)\times(n+1)&lt;/math&gt;. The analysis of this matrix demonstrates the possibility of rule &lt;math&gt;m&lt;/math&gt; firing. 
# When rule m is fired, new values are obtained for required variables as well.
# Thus, no required rules are in the service row and new values are obtained in the cells of the matrix: 2 appears in the cell &lt;math&gt;(m, n+1)-2,&lt;/math&gt; and we got the value &lt;math&gt;z&lt;/math&gt; instead of &lt;math&gt;w&lt;/math&gt; in the cell &lt;math&gt;(m+1, n-2)&lt;/math&gt;. So, positive result is obtained, consequently, a logical inference path exists with given input values.&lt;ref name=":0" /&gt;

== References ==
{{Reflist|}}

== External links ==
* [http://mivar.ru/en «Mivar»] official website.

[[Category:Artificial intelligence]]
[[Category:Mathematical logic]]</text>
      <sha1>4m2zf0akezdjslnmgy28aht8e6to6r1</sha1>
    </revision>
  </page>
  <page>
    <title>Nicolas Bourbaki</title>
    <ns>0</ns>
    <id>167394</id>
    <revision>
      <id>871565672</id>
      <parentid>871564369</parentid>
      <timestamp>2018-12-01T23:41:30Z</timestamp>
      <contributor>
        <username>Oshwah</username>
        <id>3174456</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/173.241.187.139|173.241.187.139]] ([[User talk:173.241.187.139|talk]]) ([[WP:HG|HG]]) (3.4.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="27768">{{Partisan sources|date=October 2018}}{{About|the group of mathematicians named Nicolas Bourbaki|the family of French officers named Bourbaki|Bourbaki family (disambiguation)|the computer scientist|Nikolaos Bourbakis }}

{{Infobox group
|name=          Association of Collaborators of Nicolas Bourbaki
|formation=     {{start date and age|1952|08|30}}
|headquarters=  [[École normale supérieure (Paris)|École Normale Supérieure]], [[Paris]]
|type=          [[Voluntary association]]
|language=      [[French language|French]]
|founders=       {{unbulleted list| [[Henri Cartan]] | [[Claude Chevalley]] | [[Jean Coulomb]] | [[Jean Delsarte]] | [[Jean Dieudonné]] | [[Charles Ehresmann]] | [[René de Possel]] | [[Szolem Mandelbrojt]] | [[André Weil]] }}
|membership=    Confidential
|website=       http://www.bourbaki.ens.fr/
|predecessor=
|native_name=   Association des collaborateurs de Nicolas Bourbaki
|formerly=      ''Groupe Bourbaki''
|image=         Bourbaki congress1938.png
|caption=      Bourbaki congress at [[Dieulefit]] on 1938. From left, [[Simone Weil]], [[Charles Pisot]], [[André Weil]], [[Jean Dieudonné]] (sitting), [[Claude Chabauty]], [[Charles Ehresmann]], and [[Jean Delsarte]].
}}

'''Nicolas Bourbaki''' was the collective [[pseudonym]] of a group of (mainly [[France|French]]) [[mathematician]]s. Their aim was to reformulate [[mathematics]] on an extremely abstract and formal but self-contained basis in a series of books beginning in 1935. With the goal of grounding all of mathematics on [[set theory]], the group strove for [[rigour]] and generality. Their work led to the discovery of several concepts and terminologies still used, and influenced modern branches of mathematics.

While there is no one person named Nicolas Bourbaki, the '''Bourbaki group,''' officially known as the ''Association des collaborateurs de Nicolas Bourbaki'' (Association of Collaborators of Nicolas Bourbaki), has an office at the [[École Normale Supérieure]] in [[Paris]].

==The group==
[[image:Weil.jpg|thumb|170px|[[André Weil]], de facto early leader of the group]]
In 1934, young French mathematicians from various French universities felt the need to form a group to jointly produce textbooks that they could all use for teaching. [[André Weil]] organized the first meeting on 10 December 1934 in the basement of a Parisian grill room, while all participants were attending a conference in Paris.

Accounts of the early days vary, but original documents have now come to light. The founding members were all connected to the [[École Normale Supérieure]] in [[Paris]] and included [[Henri Cartan]], [[Claude Chevalley]], [[Jean Coulomb]], [[Jean Delsarte]], [[Jean Dieudonné]], [[Charles Ehresmann]], [[René de Possel]], [[Szolem Mandelbrojt]] and [[André Weil]]. There was a preliminary meeting, towards the end of 1934.&lt;ref&gt;The minutes are in the Bourbaki archives — for a full description of the initial meeting consult Liliane Beaulieu in the ''[[Mathematical Intelligencer]]''.&lt;/ref&gt; [[Jean Leray]] and [[Paul Dubreil]] were present at the preliminary meeting but dropped out before the group actually formed. Other notable participants in later days were [[Hyman Bass]], [[Laurent Schwartz]], [[Jean-Pierre Serre]], [[Alexander Grothendieck]], [[Jean-Louis Koszul]], [[Samuel Eilenberg]], [[Serge Lang]] and [[Roger Godement]].

The original goal of the group had been to compile an improved [[mathematical analysis]] text; it was soon decided that a more comprehensive treatment of all of mathematics was necessary. There was no official status of membership, and at the time the group was quite secretive and also fond of supplying disinformation. Regular meetings were scheduled (totalling about 4 weeks a year), during which the  group would discuss vigorously every proposed line of every book. Members had to resign by age 50, which allegedly resulted in a complete change of personnel by 1958.&lt;ref name=":1"&gt;{{Cite web|url=http://afb.31.free.fr/mathematiciens%20celebres/Bourbaki%20-%20Mainard.pdf|title=Le Mouvement Bourbaki|last=Mainard|first=Robert|date=October 21, 2001|website=afb.31.free.fr|archive-url=|archive-date=|dead-url=|access-date=October 29, 2018}}&lt;/ref&gt; However, historian Liliane Beaulieu was quoted as never having found written affirmation of this rule.&lt;ref&gt;{{Cite journal|last=Aubin|first=David|date=1997|title=The Withering Immortality of Nicolas Bourbaki: A Cultural Connector at the Confluence of Mathematics, Structuralism, and the Oulipo in France|url=https://www.cambridge.org/core/journals/science-in-context/article/withering-immortality-of-nicolas-bourbaki-a-cultural-connector-at-the-confluence-of-mathematics-structuralism-and-the-oulipo-in-france/BDEFB5A7E4908417C27696871651A7C2|journal=Science in Context|publisher=Cambridge University Press|volume=10|issue=2|pages=297-342|doi=10.1017/S0269889700002660|via=}}&lt;/ref&gt;

The atmosphere in the group can be illustrated by an anecdote told by Laurent Schwartz. Dieudonné regularly and spectacularly threatened to resign unless topics were treated in their logical order, and after a while others played on this for a joke. Godement's wife wanted to see Dieudonné announcing his resignation, and so on one occasion while she was there Schwartz deliberately brought up again the question of permuting the order in which [[measure theory]] and [[topological vector space]]s were to be handled, to precipitate a guaranteed crisis.{{Citation needed|date=October 2018}}

The name "Bourbaki" refers to a French general, [[Charles Denis Bourbaki]];&lt;ref&gt;{{Cite book|title=The Apprenticeship of a Mathematician|last=Weil|first=André|publisher=Birkhäuser Verlag|year=1992|isbn=978-3764326500|location=|pages=93-122|author-link=André Weil}}&lt;/ref&gt; it was adopted by the group as a reference to a student anecdote about a hoax mathematical lecture, and also possibly to a statue. It is said that Weil's wife Evelyne supplied ''Nicolas''.&lt;ref&gt;{{Cite web|url=http://www.math.vassar.edu/faculty/mccleary/Bourbaki.pdf|title=Bourbaki and Algebraic Topology|last=McCleary|first=John|date=December 10, 2004|website=math.vassar.edu|archive-url=https://web.archive.org/web/20061030215012/http://math.vassar.edu/faculty/McCleary/Bourbaki.pdf|archive-date=October 30, 2006|dead-url=|access-date=}}&lt;/ref&gt; This is more or less confirmed by Robert Mainard.&lt;ref name=":1" /&gt;

===Pranks===

The Bourbaki group released a few humorous hoaxes related to the fake life of Nicolas Bourbaki. For example, the group released a wedding announcement, relating the marriage of [[Enrico Betti|Betti]] Bourbaki (daughter of Nicolas) with a certain ''Hector Pétard'' (Hector Firecrackers in English). In November 1968, a mock obituary of Nicolas Bourbaki was released during one of the seminars, containing a few mathematical puns.&lt;ref&gt;{{citeweb|url=http://www.neverendingbooks.org/according-to-groth-iv-22|title=according to Groth. IV.22|access-date=2018-10-24}}&lt;/ref&gt; The group is however still active as of 2018, organizing seminars&lt;ref&gt;{{Cite web|url=http://www.bourbaki.ens.fr/|title=Association des collaborateurs de Nicolas Bourbaki|website=www.bourbaki.ens.fr|language=fr|access-date=2018-10-29}}&lt;/ref&gt; and having released a book in 2016.

==Books by Bourbaki==
[[File:Bourbaki, Theorie des ensembles maitrier.jpg|thumb|First volume of ''[[Éléments de mathématique]]'', 1970 edition]]

Bourbaki's main work is the ''[[Éléments de mathématique|Elements of Mathematics (Éléments de mathématique)]]'' series. This series aims to be a completely self-contained treatment of the core areas of modern mathematics. Assuming no special knowledge of mathematics, it takes up mathematics from the very beginning, proceeds axiomatically and gives complete proofs.

The dates indicated below are for the first edition of the first chapter of each book. Most of the books were reedited several times (with significant changes between editions), and the books were released in several parts containing different chapters (e.g. Book II, ''Algebra'', was released in five parts, the first in 1942 with chapters 1-2-3 and the last in 1980 containing chapter 10).

*{{Cite book|title=Livre I: Théorie des ensembles|last=Bourbaki|first=Nicolas|publisher=|year=1939|isbn=|location=|pages=|language=French|trans-title=Book I: [[Set theory]]}}&lt;ref&gt;{{cite journal|author=Bagemihl, F.|authorlink=Frederick Bagemihl|title=Review: ''Théorie des ensembles'' (Chapter III)|journal=Bull. Amer. Math. Soc.|year=1958|volume=64|issue=6|pages=390–391|url=http://www.ams.org/journals/bull/1958-64-06/S0002-9904-1958-10248-7/S0002-9904-1958-10248-7.pdf|doi=10.1090/s0002-9904-1958-10248-7}}&lt;/ref&gt;
*{{Cite book|title=Livre II: Algèbre|last=Bourbaki|first=Nicolas|publisher=|year=1942|isbn=|location=|pages=|language=French|trans-title=Book II: [[Algebra]]}}&lt;ref&gt;{{cite journal|author=Artin, E.|authorlink=Emil Artin|title=Review: ''Éléments de mathématique'', by N. Bourbaki, Book II, Algebra, Chaps. I–VII|journal=Bull. Amer. Math. Soc.|year=1953|volume=59|issue=5|pages=474–479|url=http://www.ams.org/journals/bull/1953-59-05/S0002-9904-1953-09725-7/S0002-9904-1953-09725-7.pdf|doi=10.1090/s0002-9904-1953-09725-7}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last=Rosenberg|first= Alex|authorlink=Alex F. T. W. Rosenberg|title=Review: ''Éléments de mathématiques'' by N. Bourbaki. Book II, Algèbre. Chapter VIII, ''Modules et anneaux semi-simples''|journal=Bull. Amer. Math. Soc.|year=1960|volume=66|issue=1|pages=16–19|url=http://www.ams.org/journals/bull/1960-66-01/S0002-9904-1960-10371-0/S0002-9904-1960-10371-0.pdf|doi=10.1090/S0002-9904-1960-10371-0}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|author=Kaplansky, Irving|authorlink=Irving Kaplansky|title=Review: ''Formes sesquilinéairies et formes quadratiques'' by N. Bourbaki, ''Éléments de mathématique'' I, Livre II|journal=Bull. Amer. Math. Soc.|year=1960|volume=66|issue=4|pages=266–267|url=http://www.ams.org/journals/bull/1960-66-04/S0002-9904-1960-10461-2/S0002-9904-1960-10461-2.pdf|doi=10.1090/s0002-9904-1960-10461-2}}&lt;/ref&gt;
*{{Cite book|title=Livre III: Topologie|last=Bourbaki|first=Nicolas|publisher=|year=1940|isbn=|location=|pages=|language=French|trans-title=Book III: [[Topology]]}}
*{{Cite book|title=Livre IV: Fonctions d'une variable réelle|last=Bourbaki|first=Nicolas|publisher=|year=1949|isbn=|location=|pages=|language=French|trans-title=Book IV: [[real analysis|Functions of one real variable]]}}
*{{Cite book|title=Livre V: Espaces vectoriels topologiques|last=Bourbaki|first=Nicolas|publisher=|year=1953|isbn=|location=|pages=|language=French|trans-title=Book V: [[Topological vector space|Topological vector spaces]]}}
*{{Cite book|title=Livre VI: Intégration|last=Bourbaki|first=Nicolas|publisher=|year=1952|isbn=|location=|pages=|language=French|trans-title=Book VI: [[Integral|Integration]]}}&lt;ref&gt;{{cite journal|author=Halmos, Paul|authorlink=Paul Halmos|title=Review: ''Intégration'' (Chap. I-IV) by N. Bourbaki|journal=Bull. Amer. Math. Soc.|year=1953|volume=59|issue=3|pages=249–255|url=http://www.ams.org/journals/bull/1953-59-03/S0002-9904-1953-09698-7/S0002-9904-1953-09698-7.pdf|doi=10.1090/S0002-9904-1953-09698-7}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|author=Munroe, M. E.|title=Review: ''Intégration'' (Chapter V) by N. Bourbaki|journal=Bull. Amer. Math. Soc.|year=1958|volume=64|issue=3|pages=105–106|url=http://www.ams.org/journals/bull/1958-64-03/S0002-9904-1958-10176-7/S0002-9904-1958-10176-7.pdf|doi=10.1090/s0002-9904-1958-10176-7}}&lt;/ref&gt;
*{{Cite book|title=Livre VII: Algèbre commutative|last=Bourbaki|first=Nicolas|publisher=|year=1961|isbn=|location=|pages=|language=French|trans-title=Book VII: [[Commutative algebra]]}}&lt;ref&gt;{{cite journal|author=Nagata, M.|authorlink=Masayoshi Nagata|title=''Éléments de mathématique. Algèbre commutative'', by N. Bourbaki, Chapitres 8 et 9|journal=Bull. Amer. Math. Soc. (N.S.)|year=1985|volume=12|issue=1|pages=175–177|url=http://www.ams.org/journals/bull/1985-12-01/S0273-0979-1985-15338-8/S0273-0979-1985-15338-8.pdf|doi=10.1090/s0273-0979-1985-15338-8}}&lt;/ref&gt;
*{{Cite book|title=Livre VIII: Groupes et algèbres de Lie|last=Bourbaki|first=Nicolas|publisher=|year=1960|isbn=|location=|pages=|language=French|trans-title=Book VIII: [[Lie group|Lie groups]] and [[Lie algebra|algebras]]}}
*{{Cite book|title=Livre IX: Théories spectrales|last=Bourbaki|first=Nicolas|publisher=|year=1967|isbn=|location=|pages=|language=French|trans-title=Book IX: [[Spectral theory]]}}
*{{Cite book|title=Livre X: Variétés différentielles et analytiques|last=Bourbaki|first=Nicolas|publisher=|year=1967|isbn=|location=|pages=|language=French|trans-title=Book X: [[Differentiable manifold|Differentiable]] and [[Analytic manifold|analytic manifolds]]}}
*{{Cite book|title=Livre XI: Topologie algébrique|last=Bourbaki|first=Nicolas|publisher=|year=2016|isbn=|location=|pages=|language=French|trans-title=Book XI: [[Algebraic topology]]}}&lt;ref&gt;{{Cite web|url=https://www.springer.com/us/book/9783662493601|title=Topologie Algébrique, Chapitres 1 à 4|last=Bourbaki|first=Nicolas|date=|website=springer.com|publisher=Springer|archive-url=|archive-date=|dead-url=|access-date=2016-02-08}}&lt;/ref&gt;

The book ''Variétés différentielles et analytiques'' was a ''fascicule de résultats'', that is, a summary of results, on the theory of [[manifold]]s, rather than a worked-out exposition. The (still incomplete) volume on [[spectral theory]] (''Théories spectrales'') from 1967 was for almost four decades the last new book to be added to the series. After that several new chapters to existing books as well as revised editions of existing chapters appeared until the publication of chapters 8-9 of Commutative Algebra in 1983.  A long break in publishing activity followed, leading many to suspect the end of the publishing project. However, chapter 10 of Commutative Algebra  appeared in 1998, and after another long break a completely re-written and expanded chapter 8 of Algèbre was published in 2012. More importantly,  the first four chapters of a completely new book on algebraic topology were published in 2016. The new material from 2012 and 2014 address some references to forthcoming books in the book on Lie Groups and Algebras; there remain other such references (some very precise) to expected additional chapters of the book spectral theory.

Besides the ''[[Éléments de mathématique]]'' series, lectures from the [[Séminaire Nicolas Bourbaki|Séminaire Bourbaki]] also have been periodically published in [[monograph]] form since 1948.

==Influence on mathematics in general==
{{also|Structuralism (philosophy of mathematics)}}
Notations introduced by Bourbaki include the symbol &lt;math&gt; \varnothing &lt;/math&gt; for the [[empty set]] and a [[Bourbaki dangerous bend symbol|dangerous bend symbol]] ☡, and the terms ''[[injective]]'', ''[[surjective]]'', and ''[[bijective]]''.{{citation needed|date=December 2011}}

The emphasis on [[rigour]] may be seen as a reaction to the work of [[Henri Poincaré]],&lt;ref&gt;''Bourbaki came to terms with Poincaré only after a long struggle. When I joined the group in the fifties it was not the fashion to value Poincaré at all. He was old-fashioned.'' Pierre Cartier interviewed by Marjorie Senechall. {{Cite journal| title=The Continuing Silence of Bourbaki | journal=[[Mathematical Intelligencer]] | year=1998 | volume=19 | pages=22–28}} [http://www.ega-math.narod.ru/Bbaki/Cartier.htm]&lt;/ref&gt; who stressed the importance of free-flowing mathematical intuition, at a cost of completeness in presentation. The impact of Bourbaki's work initially was great on many active research mathematicians worldwide. For example:
{{quote|Our time is witnessing the creation of a monumental work: an exposition of the whole of present day mathematics. Moreover this exposition is done in such a way that the common bond between the various branches of mathematics become clearly visible, that the framework which supports the whole structure is not apt to become obsolete in a very short time, and that it can easily absorb new ideas.|[[Emil Artin]] (Bull.AMS 59 (1953), 474–479)}}

It provoked some hostility, too, mostly on the side of [[classical analysis|classical analysts]]; they approved of rigour but not of high [[Abstraction (mathematics)|abstraction]]. Around 1950, also, some parts of [[geometry]] were still not fully axiomatic — in less prominent developments, one way or another, these were brought into line with the new foundational standards, or quietly dropped. This led to a gulf with the way [[theoretical physics]] was practiced.&lt;ref name="ByeByeBourbaki"&gt;{{Cite journal |last=Stewart |first=Ian |title=Bye-Bye Bourbaki: Paradigm Shifts in Mathematics |journal=The Mathematical Gazette |date=November 1995 |volume=79 |pages=496–498 |publisher=[[The Mathematical Association]] |doi=10.2307/3618076 |issue=486 |jstor=3618076}}&lt;/ref&gt;

Bourbaki's direct influence has decreased over time.&lt;ref name="ByeByeBourbaki" /&gt; This is partly because certain concepts which are now important, such as the machinery of [[category theory]], are not covered in the treatise. {{Citation needed|date=January 2014}} The completely uniform and essentially linear referential structure of the books became difficult to apply to areas closer to current research than the already mature ones treated in the published books, and thus publishing activity diminished significantly from the 1970s.&lt;ref&gt;{{Cite journal|last=Borel|first=Armand|date=March 1998|title=Twenty-Five Years with Nicolas Bourbaki, (1949-1973)|url=http://www.ams.org/notices/199803/borel.pdf|journal=Notices Amer. Math. Soc.|volume=45|issue=3|pages=373-380}}&lt;/ref&gt; It also mattered that, while especially [[algebraic structure]]s can be naturally defined in Bourbaki's terms, there are areas where the Bourbaki approach was less straightforward to apply.{{Citation needed|date=March 2009}}

On the other hand, the approach and rigour advocated by Bourbaki have permeated the current mathematical practices to such extent that the task undertaken was completed.&lt;ref&gt;{{Cite journal|last=Guedj|first=Denis|date=1985|title=Nicholas Bourbaki, collective mathematician : An interview with Claude Chevalley|url=|journal=Math. Intelligencer|publisher=|volume=7|issue=2|pages=18-22|doi=10.1007/BF03024169|via=}}&lt;/ref&gt; This is particularly true for the less applied parts of mathematics.

The [[Bourbaki seminar]] series founded in post-WWII Paris continues; it has been going on since 1948, and contains more than 1000 items. It is an important source of [[survey article]]s, with sketches (or sometimes improvements) of proofs. The topics range through all branches of mathematics, including sometimes theoretical physics. The idea is that the presentation should be on the level of specialists, but should be tailored to an audience which is ''not'' specialized in the particular field.

==Appraisal of the Bourbaki perspective==
The underlying drive, in Weil and Chevalley at least, was the perceived need for French mathematics to absorb the best ideas of the [[University of Göttingen|Göttingen]] school, particularly [[David Hilbert|Hilbert]] and the modern algebra school of [[Emmy Noether|Noether]], [[Emil Artin|Artin]] and [[Bartel van der Waerden|van der Waerden]]. It is fairly clear that the Bourbaki point of view, while ''encyclopedic'', was never intended as ''neutral''. Quite the opposite: it was more a question of trying to make a consistent whole out of some enthusiasms, for example for Hilbert's legacy, with emphasis on formalism and axiomatics. But always through a transforming process of reception and selection—their ability to sustain this collective, critical approach has been described as "something unusual".&lt;ref&gt;Hector C. Sabelli, [[Louis H. Kauffman]], BIOS (2005), p. 423.&lt;/ref&gt;

The following is a list of some of the criticisms commonly made of the Bourbaki approach. [[Pierre Cartier (mathematician)|Pierre Cartier]], a Bourbaki member between 1955 and 1983, said that:&lt;ref name=":0"&gt;{{Cite web|url=http://ega-math.narod.ru/Bbaki/Cartier.htm|title=The Continuing Silence of Bourbaki|website=ega-math.narod.ru|access-date=2018-10-29}}&lt;/ref&gt;

{{quote|essentially no analysis beyond the foundations: nothing about partial differential equations, nothing about probability. There is also nothing about combinatorics, nothing about algebraic topology, nothing about concrete geometry. And Bourbaki never seriously considered logic. Dieudonné himself was very vocal against logic. Anything connected with mathematical physics is totally absent from Bourbaki's text.|sign=|source=}}

In addition, [[Algorithm|algorithms]] are considered off-topic and almost completely omitted.&lt;ref&gt;{{Cite web|url=http://publimath.irem.univ-mrs.fr/glossaire/BO025.htm|title=Bourbaki Nicolas|website=publimath.irem.univ-mrs.fr|access-date=2018-10-29}}&lt;/ref&gt; [[Mathematical analysis|Analysis]] is treated 'softly', without 'hard' estimates.{{Clarify|reason=|date=October 2018}}&lt;ref&gt;{{Cite web|url=http://www.matematikkforeningen.no/INFOMAT/06/0608.pdf|title=Interview with Lennart Carleson|last=Carleson|first=Lennart|authorlink=Lennart Carleson|date=August 2006|website=matematikkforeningen.no|archive-url=https://web.archive.org/web/20060928042809/http://www.matematikkforeningen.no/INFOMAT/06/0608.pdf|archive-date=2007-09-28|dead-url=|access-date=}}&lt;/ref&gt; [[Measure theory]] is developed from a [[functional analysis|functional analytic]] perspective. Taking the case of [[locally compact space|locally compact]] measure spaces as fundamental focuses the presentation on [[Radon measure]]s and leads to an approach to measurable functions that is cumbersome, especially from the viewpoint of probability theory.&lt;ref&gt;{{Cite web|url=http://www.math.tu-dresden.de/~pos_iv/Abstracts/koenig_abstract/index.html|title=Stochastic Processes on the Basis of New Measure Theory|last=König|first=Heinz|date=|website=math.tu-dresden.de|archive-url=https://web.archive.org/web/20070304000104/http://www.math.tu-dresden.de/~pos_iv/Abstracts/koenig_abstract/index.html|archive-date=2007-03-04|dead-url=|access-date=}}&lt;/ref&gt; However, the last chapter of the book addresses limitations, especially for use in [[probability theory]], of the restriction to locally compact spaces. [[Mathematical logic|Logic]] is treated minimally.&lt;ref&gt;{{Cite web|url=https://www.dpmms.cam.ac.uk/~ardm/bourbaki.pdf|title=The Ignorance of Bourbaki|last=Mathias|first=Adrien|date=August 22, 1990|website=dpmms.cam.ac.uk|archive-url=|archive-date=|dead-url=|access-date=}}&lt;/ref&gt;&lt;ref&gt;See also Mashaal (2006), p.120, "Lack of interest in foundations".&lt;/ref&gt;

Furthermore, Bourbaki makes only limited use of pictures in their presentation. Pierre Cartier is quoted as later saying: "''The Bourbaki were Puritans, and Puritans are strongly opposed to pictorial representations of truths of their faith.''"&lt;ref name=":0" /&gt; In general, Bourbaki has been criticized for reducing [[geometry]] as a whole to [[abstract algebra]] and [[soft analysis]].&lt;ref&gt;{{Cite web|url=https://www.apmep.fr/IMG/pdf/AAA02005.pdf|title=Pourquoi, pour qui enseigner les mathématiques ?|last=Gispert|first=Hélène|date=2000|website=apmep.fr|language=French|trans-title=Why, for whom, teach mathematics?|archive-url=|archive-date=|dead-url=|access-date=2018-10-29}}&lt;/ref&gt;

While several of Bourbaki's books have become standard references in their fields, some have felt that the austere presentation makes them unsuitable as textbooks.&lt;ref&gt;{{Cite journal|last=Hewitt|first=Edwin|year=1956|title=Review: Espaces vectoriels topologiques|journal=[[Bulletin of the American Mathematical Society]]|volume=62|issue=5|pages=507–508|doi=10.1090/S0002-9904-1956-10042-6}} [http://www.ams.org/bull/1956-62-05/S0002-9904-1956-10042-6/home.html]&lt;/ref&gt; The books' influence may have been at its strongest when few other graduate-level texts in current [[pure mathematics]] were available, between 1950 and 1960.&lt;ref&gt;http://turnbull.mcs.st-and.ac.uk/~history/PrintHT/Bourbaki_2.html&lt;/ref&gt;

In the longer term, the manifesto of Bourbaki has had a definite and deep influence. In secondary education the [[new math]] movement corresponded to teachers influenced by Bourbaki. In France the change was secured by the [[Andre Lichnerowicz|Lichnerowicz Commission]].&lt;ref&gt;Mashaal (2006) Ch.10: New Math in the Classroom&lt;/ref&gt;

==Dieudonné as speaker for Bourbaki==
{{Refimprove section|date=October 2018}}
Public discussion of, and justification for, Bourbaki's thoughts has in general been through [[Jean Dieudonné]] (who initially was the 'scribe' of the group) writing under his own name. In a survey of ''le choix bourbachique'' written in 1977, he did not shy away from a hierarchical development of the 'important' mathematics of the time.

He also wrote extensively under his own name: nine volumes on [[mathematical analysis|analysis]], perhaps in belated fulfillment of the original project or pretext; and also on other topics mostly connected with [[algebraic geometry]]. While Dieudonné could reasonably speak on Bourbaki's encyclopedic tendency and tradition, it may be doubted—after innumerable frank ''tais-toi, Dieudonné!'' ("Hush, Dieudonné!") remarks at the meetings—whether all others agreed with him about mathematical writing and research.{{Citation needed|date=September 2018}} In particular Serre has often championed greater attention to problem-solving, within [[number theory]] especially, not an area treated in the main Bourbaki texts.

Dieudonné stated the view that most workers in mathematics were doing ground-clearing work, in order that a future [[Bernhard Riemann|Riemann]] could find the way ahead intuitively open. He pointed to the way the axiomatic method can be used as a tool for problem-solving, for example by [[Alexander Grothendieck]]. Others found him too close to Grothendieck to be an unbiased observer. Comments in [[Pál Turán]]'s 1970 speech on the award of a [[Fields Medal]] to [[Alan Baker (mathematician)|Alan Baker]] about theory-building and problem-solving were a reply from the traditionalist camp at the next opportunity,&lt;ref&gt;{{cite journal|jstor=2272765|title=Review of On the Work of ., Paul Tur&amp;#xe1;n; Effective Methods in the Theory of Numbers., Alan Baker|first=Alonzo|last=Church|date=1 January 1972|publisher=|volume=37|issue=3|pages=606–606|doi=10.2307/2272765}}&lt;/ref&gt; Grothendieck having received the previous Fields Medal ''in absentia'' in 1966.

==See also==
*[[Bourbaki dangerous bend symbol]]
*[[Bourbaki–Witt theorem]]
*[[Jacobson–Bourbaki theorem]]

;People
*[[Arthur Besse]]
*[[G. W. Peck]]
*[[John Rainwater]]

==Notes==
{{Reflist|colwidth=30em}}

==References==

* Luca Vercelloni, Filosofia delle strutture, La Nuova Italia, Firenze, 1989
* Maurice Mashaal (2006). ''Bourbaki: A Secret Society of Mathematicians''. [[American Mathematical Society]]. {{ISBN|0-8218-3967-5}}.
* [[Amir Aczel]] (2007). ''The Artist and the Mathematician: The Story of Nicolas Bourbaki, the Genius Mathematician Who Never Existed''. [[High Stakes Publishing]], [[London]]. {{ISBN|1-84344-034-2}}.

==External links==
* [http://www.bourbaki.ens.fr/ Official Website of ''L'Association des Collaborateurs de Nicolas Bourbaki''] {{fr icon}}
*[http://sites.mathdoc.fr/archives-bourbaki/ Archives of the association] {{fr icon}}
* [http://www.math.nsc.ru/LBRT/g2/english/ssk/euclid.html Apology of Euclid], by [[Semën Samsonovich Kutateladze]]

{{Authority control}}

{{DEFAULTSORT:Bourbaki, Nicolas}}
[[Category:Nicolas Bourbaki]]
[[Category:Academic shared pseudonyms]]
[[Category:French mathematicians]]
[[Category:Secret societies in France]]
[[Category:Pseudonymous mathematicians]]
[[Category:Large-scale mathematical formalization projects]]</text>
      <sha1>79st4q8ffoy8naq8de5p6dxy6camcyo</sha1>
    </revision>
  </page>
  <page>
    <title>Out-of-bag error</title>
    <ns>0</ns>
    <id>48643701</id>
    <revision>
      <id>857006104</id>
      <parentid>826778637</parentid>
      <timestamp>2018-08-28T23:26:15Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1734">{{Machine learning bar}}

'''Out-of-bag (OOB) error''', also called '''out-of-bag estimate''', is a method of measuring the prediction error of [[random forest]]s, [[gradient boosting|boosted decision trees]], and other [[machine learning]] models utilizing [[bootstrap aggregating]] (bagging) to sub-sample data samples used for training. OOB is the mean prediction error on each training sample {{mvar|xᵢ}}, using only the trees that did not have {{mvar|xᵢ}} in their bootstrap sample.&lt;ref name="islr"&gt;{{cite book |first1=Gareth |last1=James |first2=Daniela |last2=Witten |first3=Trevor |last3=Hastie |first4=Robert |last4=Tibshirani |title=An Introduction to Statistical Learning |publisher=Springer |year=2013 |url=http://www-bcf.usc.edu/~gareth/ISL/ |pages=316–321}}&lt;/ref&gt;

Subsampling allows one to define an out-of-bag estimate of the prediction performance improvement by evaluating predictions on those observations which were not used in the building of the next base learner. Out-of-bag estimates help avoid the need for an independent validation dataset, but often underestimates actual performance improvement and the optimal number of iterations.&lt;ref name="gbm-vignette"&gt;{{cite web |last=Ridgeway |first=Greg |authorlink=Greg Ridgeway |year=2017 |url=https://cran.r-project.org/web/packages/gbm/gbm.pdf |title=Generalized Boosted Models: A guide to the gbm package }}&lt;/ref&gt;

== See also ==
*[[Boosting (meta-algorithm)]]
*[[Bootstrapping (statistics)]]
*[[Cross-validation (statistics)]]
*[[Random forest]]
*[[Random subspace method]] (attribute bagging)

== References ==
{{Reflist}}

[[Category:Ensemble learning]]
[[Category:Machine learning algorithms]]
[[Category:Computational statistics]]


{{compsci-stub}}</text>
      <sha1>130blcm9eiuarlv4ioffbgg1enssy6j</sha1>
    </revision>
  </page>
  <page>
    <title>Pattern language</title>
    <ns>0</ns>
    <id>182837</id>
    <revision>
      <id>866694501</id>
      <parentid>855917355</parentid>
      <timestamp>2018-10-31T23:05:05Z</timestamp>
      <contributor>
        <username>Thinkulum</username>
        <id>20426769</id>
      </contributor>
      <comment>Added "A Pattern Language for Pattern Writing" to the external links.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="26708">{{About|the structured design approach by architect Christopher Alexander}}
A '''pattern language''' is a method of describing good design practices or patterns of useful organization within a field of expertise. The term was coined by architect [[Christopher Alexander]] and popularized by his 1977 book ''[[A Pattern Language]]''.

A pattern language can also be an attempt to express the deeper wisdom of what brings aliveness within a particular field of human endeavor, through a set of interconnected patterns. Aliveness is one placeholder term for "the quality that has no name": a sense of wholeness, spirit, or grace, that while of varying form, is precise and empirically verifiable.&lt;ref&gt;{{cite book |last1=Alexander |first1=Christopher |title=The Timeless Way of Building |date=1979 |publisher=Oxford University Press |isbn=0-19-502402-8}}&lt;/ref&gt; Some advocates{{who|date=March 2016}} of this design approach claim that ordinary people can use it to successfully solve very large, complex design problems.

==What is a pattern?==
{{See also|Design pattern}}
When a designer designs something – whether a house, computer program, or lamp – they must make many decisions about how to solve problems. A single problem is documented with its typical place (the [[syntax]]), and use (the [[grammar]]) with the most common and recognized good solution seen in the wild, like the examples seen in [[dictionary|dictionaries]]. Each such entry is a single [[design pattern]]. Each pattern has a name, a descriptive entry, and some cross-references, much like a dictionary entry. A documented pattern should explain why that solution is good in the pattern's contexts.

Elemental or universal ''patterns'' such as "door" or "partnership" are versatile ideals of design, either as found in experience or for use as components in practice, explicitly described as holistic resolutions of the forces in recurrent contexts and circumstances, whether in architecture, medicine, software development or governance, etc. Patterns might be invented or found and studied, such as the naturally occurring patterns of design that characterize human environments.&lt;ref&gt;Henshaw, J. [http://www.synapse9.com/pub/2015_PURPLSOC-JLHfinalpub.pdf Guiding Patterns of Naturally Occurring Design: Elements. PURPLSOC 2015 proceedings, July 3-5 2015 Krems, Austria] PURPLSOC meeting on the many open scientific questions, e.g. regarding the theoretical background of patterns and the practical implementation of pattern methods in research and teaching.&lt;/ref&gt;

Like all languages, a pattern language has [[vocabulary]], [[syntax]], and [[grammar]] – but a pattern language applies to some complex activity other than communication. In pattern languages for design, the parts break down in this way:
* The language description – the ''vocabulary'' – is a collection of named, described solutions to problems in a field of interest. These are called ''design patterns''. So, for example, the language for architecture describes items like: settlements, buildings, rooms, windows, latches, etc.
* Each solution includes ''syntax'', a description that shows where the solution fits in a larger, more comprehensive or more abstract design. This automatically links the solution into a web of other needed solutions. For example, rooms have ways to get light, and ways to get people in and out.
* The solution includes ''grammar'' that describes how the solution solves a problem or produces a benefit. So, if the benefit is unneeded, the solution is not used. Perhaps that part of the design can be left empty to save money or other resources; if people do not need to wait to enter a room, a simple doorway can replace a waiting room.
* In the language description, grammar and syntax cross index (often with a literal alphabetic index of pattern names) to other named solutions, so the designer can quickly think from one solution to related, needed solutions, and document them in a logical way. In Christopher Alexander's book ''A Pattern Language'', the patterns are in decreasing order by size, with a separate alphabetic index.
* The web of relationships in the index of the language provides many paths through the design process.

This simplifies the design work because designers can start the process from any part of the problem they understand and work toward the unknown parts. At the same time, if the pattern language has worked well for many projects, there is reason to believe that even a designer who does not completely understand the design problem at first will complete the design process, and the result will be usable. For example, skiers coming inside must shed snow and store equipment. The messy snow and boot cleaners should stay outside. The equipment needs care, so the racks should be inside.

==Many patterns form a language==
Just as [[words]] must have [[Grammar|grammatical]] and [[Semantics|semantic]] relationships to each other in order to make a spoken [[language]] useful, design patterns must be related to each other in position and utility order to form a pattern language. Christopher Alexander's work describes a process of decomposition, in which the designer has a problem (perhaps a commercial assignment), selects a solution, then discovers new, smaller problems resulting from the larger solution. Occasionally, the smaller problems have no solution, and a different larger solution must be selected. Eventually all of the remaining design problems are small enough or routine enough to be solved by improvisation by the builders, and the "design" is done.

The actual organizational structure ([[Hierarchy|hierarchical]], [[Iterative method|iterative]], etc.) is left to the discretion of the designer, depending on the problem. This explicitly lets a designer explore a design, starting from some small part. When this happens, it's common for a designer to realize that the problem is actually part of a larger solution. At this point, the design almost always becomes a better design.

In the language, therefore, each pattern has to indicate its relationships to other patterns and to the language as a whole. This gives the designer using the language a great deal of guidance about the related problems that must be solved.

The most difficult part of having an outside expert apply a pattern language is in fact to get a reliable, complete list of the problems to be solved. Of course, the people most familiar with the problems are the people that need a design. So, Alexander famously advocated on-site improvisation by concerned, empowered users,&lt;ref name="Alexander_Pattern_1977"/&gt;&lt;ref&gt;Alexander, Christopher, The Oregon Project&lt;/ref&gt; as a powerful way to form very workable large-scale initial solutions, maximizing the utility of a design, and minimizing the design rework. The desire to empower users of architecture was, in fact, what led Alexander to undertake a pattern language project for architecture in the first place.

==Design problems in a context==
An important aspect of design patterns is to identify and document the key ideas that make a good system different from a poor system (that may be a house, a computer program or an object of daily use), and to assist in the design of future systems. The idea expressed in a pattern should be general enough to be applied in very different systems within its context, but still specific enough to give constructive guidance.

The range of situations in which the problems and solutions addressed in a pattern apply is called its context. An important part in each pattern is to describe this context. Examples can further illustrate how the pattern applies to very different situation.

For instance, Alexander's pattern "A PLACE TO WAIT" addresses bus stops in the same way as waiting rooms in a surgery, while still proposing helpful and constructive solutions. The [[Design Patterns|"Gang-of-Four" book ''Design Patterns'']] by Gamma et al. proposes solutions that are independent of the programming language, and the program's application domain.

Still, the problems and solutions described in a pattern can vary in their level of abstraction and generality on the one side, and specificity on the other side. In the end this depends on the author's preferences. However, even a very abstract pattern will usually contain examples that are, by nature, absolutely concrete and specific.

Patterns can also vary in how far they are proven in the real world. Alexander gives each pattern a rating by zero, one or two stars, indicating how well they are proven in real-world examples. It is generally claimed that all patterns need at least some existing real-world examples. It is, however, conceivable to document yet unimplemented ideas in a pattern-like format.

The patterns in Alexander's book also vary in their level of scale – some describing how to build a town or neighbourhood, others dealing with individual buildings and the interior of rooms. Alexander sees the low-scale artifacts as constructive elements of the large-scale world, so they can be connected to a [[#Aggregation in an associative network (pattern language)|hierarchic network]].

===Balancing of forces===
A pattern must characterize the problems that it is meant to solve, the context or situation where these problems arise, and the conditions under which the proposed solutions can be recommended.

Often these problems arise from a conflict of different interests or "forces". A pattern emerges as a dialogue that will then help to balance the forces and finally make a decision.

For instance, there could be a pattern suggesting a wireless telephone. The forces would be the need to communicate, and the need to get other things done at the same time (cooking, inspecting the bookshelf). A very specific pattern would be just "WIRELESS TELEPHONE". More general patterns would be "WIRELESS DEVICE" or "SECONDARY ACTIVITY", suggesting that a secondary activity (such as talking on the phone, or inspecting the pockets of your jeans) should not interfere with other activities.

Though quite unspecific in its context, the forces in the "SECONDARY ACTIVITY" pattern are very similar to those in "WIRELESS TELEPHONE". Thus, the competing forces can be seen as part of the essence of a design concept expressed in a pattern.

===Patterns contain their own rationale===
Usually a pattern contains a rationale referring to some given values. For Christopher Alexander, it is most important to think about the people who will come in contact with a piece of architecture. One of his key values is making these people feel more alive. He talks about the "quality without a name" (QWAN).

More generally, we could say that a good system should be accepted, welcomed and happily embraced as an enrichment of daily life by those who are meant to use it, or – even better – by all people it affects. For instance, when discussing a street café, Alexander discusses the possible desires of a guest, but also mentions people who just walk by.

The same thinking can be applied to technical devices such as telephones and cars, to social structures like a team working on a project, or to the user interface of a computer program. The qualities of a software system, for instance, could be rated by observing whether users spend their time enjoying or struggling with the system.

By focusing on the impacts on human life, we can identify patterns that are independent from changing technology, and thus find "timeless quality" (Alexander).

==Generic structure and layout==
Usually the author of a pattern language or collection chooses a generic structure for all the patterns it contains, breaking each into generic sections like context, problem statement, solution etc.

Christopher Alexander's patterns, for instance, each consist of a short name, a rating (up to two '*' symbols), a sensitizing picture, the context description, the problem statement, a longer part of text with examples and explanations, a solution statement, a sketch and further references. This structure and layout is sometimes referred to as the "Alexandrian form".

Alexander uses a special text layout to mark the different sections of his patterns. For instance, the problem statement and the solution statement are printed in bold font, the latter is always preceded by the "Therefore:" keyword. Some authors instead use explicit labels, which creates some degree of redundancy.

===Meaningful names===
When design is done by a team, pattern names will form a vocabulary they can share. This makes it necessary for pattern names to be easy to remember and highly descriptive. Some examples from Alexander's works are WINDOW PLACE (helps define where windows should go in a room) and A PLACE TO WAIT (helps define the characteristics of bus stops and hospital waiting rooms, for example).

==Aggregation in an associative network (pattern language)==
A pattern language, as conceived by Alexander, contains links from one pattern to another, so when trying to apply one pattern in a project, a designer is pushed to other patterns that are considered helpful in its context.

In Alexander's book, such links are collected in the "references" part, and echoed in the linked pattern's "context" part – thus the overall structure is a directed graph. A pattern that is linked to in the "references" usually addresses a problem of lower scale, that is suggested as a part of the higher-scale problem. For instance, the "PUBLIC OUTDOOR ROOM" pattern has a reference to "STAIR SEATS".

Even without the pattern description, these links, along with meaningful names, carry a message: When building a place outside where people can spend time ("PUBLIC OUTDOOR ROOM"), consider to surround it by stairs where people can sit ("STAIR SEATS"). If you are planning an office ("WORKSHOPS AND OFFICES"), consider to arrange workspaces in small groups ("SMALL WORKING GROUPS"). Alexander argues that the connections in the network can be considered even more meaningful than the text of the patterns themselves.

The links in Alexander's book clearly result in a hierarchic network. Alexander draws a parallel to the hierarchy of a grammar – that is one argument for him to speak of a pattern ''language''.

The idea of linking is generally accepted among pattern authors, though the semantic rationale behind the links may vary. Some authors, however, like Gamma et al. in ''[[Design Patterns]]'', make only little use of pattern linking – possibly because it did not make that much sense for their collection of patterns. In such a case we would speak of a ''pattern catalogue'' rather than a ''pattern language''.&lt;ref name="dearden"&gt;{{cite journal | author = Andy Dearden, Janet Finlay | title = Pattern Languages in HCI: A critical review | date = January 2006 | journal = Human Computer Interaction | volume = 21 | issue = 1 }}&lt;/ref&gt;

===Usage===
Alexander encouraged people who used his system to expand his language with patterns of their own. In order to enable this, his books do not focus strictly on architecture or civil engineering; he also explains the general method of pattern languages. The original concept for the book ''A Pattern Language'' was that it would be published in the form of a 3-ring binder, so that pages could easily be added later; this proved impractical in publishing.&lt;ref&gt;Portland Urban Architecture Research Laboratory
Symposium 2009, presentation by 4 of 6 original authors of ''A Pattern Language''.&lt;/ref&gt;  The pattern language approach has been used to document expertise in diverse fields. Some examples are [[Design pattern (architecture)|architectural patterns]], [[Design pattern (computer science)|computer science patterns]], [[interaction design pattern]]s, [[pedagogical patterns]],  social action patterns, and group facilitation patterns. The pattern language approach has also been recommended as a way to promote [[civic intelligence]] by helping to coordinate actions for diverse people and communities who are working together on significant shared problems.&lt;ref&gt;For additional discussion of motivation and rationale as well as examples and experiments, see: {{cite web |last=Schuler |first=Douglas |url=http://publicsphereproject.org/sites/default/files/Critical%20Enablers%20of%20Civic%20Intelligence.reduced.pdf |title=Choosing success: pattern languages as critical enablers of civic intelligence |publisher=Portland Urban Architecture Research Laboratory Conference, Portland, OR, 2009 |website=publicsphereproject.org |accessdate=6 March 2017}}&lt;/ref&gt; Alexander's specifications for using pattern languages as well as creating new ones remain influential, and his books are referenced for style by experts in unrelated fields.

It is important to note that notations such as [[Unified Modeling Language|UML]] or the [[flowchart]] symbol collection are not pattern languages. They could more closely be compared to an alphabet: their symbols could be used to document a pattern language, but they are not a language by themselves. A [[recipe]] or other sequential set of steps to be followed, with only one correct path from start to finish, is also not a pattern language. However, the process of designing a new recipe might benefit from the use of a pattern language.

===Simple example of a pattern===
*''Name'': ChocolateChipRatio
*''Context'': You are baking chocolate chip cookies in small batches for family and friends
*''Consider these patterns first'': SugarRatio, FlourRatio, EggRatio
*''Problem'': Determine the optimum ratio of chocolate chips to cookie dough
*''Solution'': Observe that most people consider chocolate to be the best part of the chocolate chip cookie. Also observe that too much chocolate may prevent the cookie from holding together, decreasing its appeal. Since you are cooking in small batches, cost is not a consideration. Therefore, use the maximum amount of chocolate chips that results in a really sturdy cookie.
*''Consider next'': NutRatio or CookingTime or FreezingMethod

==Origin==
[[Christopher Alexander]], an architect and author, coined the term pattern language.&lt;ref name="Alexander_Pattern_1977"&gt;{{Cite book | publisher = [[Oxford University Press]], USA | isbn = 0-19-501919-9 | last = Alexander | first = Christopher | title = A Pattern Language: Towns, Buildings, Construction | year = 1977 | page = 1216}}&lt;/ref&gt; He used it to refer to common problems of the [[design]] and [[construction]] of buildings and towns and how they should be solved. The solutions proposed in the book include suggestions ranging from how cities and towns should be structured to where windows should be placed in a room.

The framework and philosophy of the "pattern language" approach was initially popularized in the book ''[[A Pattern Language]]'' that was written by Christopher Alexander and five colleagues at the Center for Environmental Structure in Berkeley, California in the late 1970s. While ''A Pattern Language'' contains 253 "patterns" from the first pattern, "Independent Regions" (the most general) to the last, "Things from Your Life", Alexander's book ''[[The Timeless Way of Building]]'' goes into more depth about the motivation and purpose of the work. The following definitions of "pattern" and "pattern language" are paraphrased from ''A Pattern Language''&lt;ref name="Alexander_Pattern_1977"/&gt;:

"A ''pattern'' is a careful description of a perennial solution to a recurring problem within a building context, describing one of the configurations that brings life to a building. Each pattern describes a problem that occurs over and over again in our environment, and then describes the core solution to that problem, in such a way that you can use the solution a million times over, without ever doing it the same way twice."&lt;ref name="Alexander_Pattern_1977"/&gt;

A ''pattern language'' is a network of patterns that call upon one another. Patterns help us remember insights and knowledge about design and can be used in combination to create solutions.

== Application domains ==
Christopher Alexander's idea has been adopted in other disciplines, often much more heavily than the original [[Pattern (architecture)|application of patterns to architecture]] as depicted in the book ''[[A Pattern Language]]''.&lt;ref name="Alexander_Pattern_1977"/&gt; Recent examples include [[software design pattern]]s in software engineering and, more generally, [[Architectural pattern (computer science)|architectural patterns in computer science]], as well as [[interaction design pattern]]s. [[Pedagogical patterns]] are used to document good practices in teaching. The book ''Liberating Voices: A Pattern Language for Communication Revolution'', containing 136 patterns for using information and communication to promote sustainability, democracy and positive social change, was published in 2008 along with a website containing even more patterns.&lt;ref&gt;{{cite web |url=http://www.publicsphereproject.org/patterns/ |title=Liberating Voices Pattern Language |website=publicsphereproject.org |accessdate=6 March 2017}}&lt;/ref&gt; The deck "Group Works: A Pattern Language for Bringing Life to Meetings and Other Gatherings" was published in 2011.&lt;ref&gt;{{cite web |url=http://groupworksdeck.org/ |title=Group Pattern Language Project |website=groupworksdeck.org |accessdate=6 March 2017}}&lt;/ref&gt; Recently, patterns were also introduced into [[systems architecture]] design.&lt;ref&gt;{{cite web|last=Hein|first=Andreas|title=Adopting Patterns for Space Mission and Space Systems Architecting|url=https://www.academia.edu/2110976/A.M._Hein_Adopting_Patterns_for_Space_Mission_and._Space_Systems_Architecting_|work=5 th International Workshop on System &amp; Concurrent Engineering for Space ApplicationsSECESA 2012|accessdate=2 March 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Hein|first=Andreas|title=Project Icarus: Stakeholder Scenarios for an Interstellar Exploration Program|url=https://www.academia.edu/1354848/PROJECT_ICARUS_STAKEHOLDER_SCENARIOS_FOR_AN_INTERSTELLAR_EXPLORATION_PROGRAM|work=Journal of the British Interplanetary Society, 64, 224-233, 2011|accessdate=2 March 2013}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Cloutier|first=Robert|title=The Concept of Reference Architectures|url=http://www.calimar.com/TheConceptOfReferenceArchitectures.pdf|work=Systems Engineering Vol. 13, No. 1, 2010|accessdate=2 March 2013}}&lt;/ref&gt;  [[Chess]] [[Chess strategy|strategy]] and [[Chess tactics|tactics]] involve many patterns from [[Chess opening|opening]] to [[checkmate]].

[[Ward Cunningham]], the inventor of [[wiki]], coauthored a paper with [[Michael Mehaffy]] arguing that there are deep relationships between wikis and pattern languages, and that wikis "were in fact developed as tools to facilitate efficient sharing and modifying of patterns".&lt;ref&gt;{{cite book |last1=Cunningham |first1=Ward |authorlink1=Ward Cunningham |last2=Mehaffy |first2=Michael W. |authorlink2=Michael Mehaffy |date=2013 |chapter=Wiki as pattern language |title=Proceedings of the 20th Conference on [[Pattern Languages of Programs]], October 23–26, 2013, Monticello, Illinois |series=PLoP '13 |location=Corryton, TN |publisher=[[The Hillside Group]] |pages=32:1–32:14 |isbn=9781941652008 |chapterurl=http://dl.acm.org/citation.cfm?id=2725669.2725707}}&lt;/ref&gt;

==See also==
* [[Feng shui]]
* [[Method engineering]]
* [[Rule of thumb]]
* [[Shearing layers]]
* [[Systems theory]]
* [[Typology (urban planning and architecture)]]

==References==
{{Reflist}}

==Further reading==
* Christopher Alexander, Sara Ishikawa, Murray Silverstein (1974). 'A Collection of Patterns which Generate Multi-Service Centres' in Declan and Margrit Kennedy (eds.): ''The Inner City.'' Architects Year Book 14, Elek, London. {{ISBN|0 236 15431 1}}.
* Alexander, C. (1977). ''[[A Pattern Language: Towns, Buildings, Construction]]''. USA: [[Oxford University Press]]. {{ISBN|978-0-19-501919-3}}.
* Alexander, C. (1979). ''The Timeless Way of Building''. USA: Oxford University Press. {{ISBN|978-0-19-502402-9}}.
* Schuler, D. (2008). ''Liberating Voices: A Pattern Language for Communication Revolution''. USA: [[MIT Press]]. {{ISBN|978-0-262-69366-0}}.
* Leitner, Helmut (2015): ''Pattern Theory: Introduction and Perspectives on the Tracks of Christopher Alexander''. {{ISBN|1505637430}}.

==External links==

===About patterns in general===
* [http://www.c2.com/cgi/wiki?TipsForWritingPatternLanguages Tips For Writing Pattern Languages], by [[Ward Cunningham]]
* [https://hillside.net/index.php/a-pattern-language-for-pattern-writing A Pattern Language for Pattern Writing] by Gerard Meszaros and Jim Doble
* [http://www.gardenvisit.com/landscape/architecture/3.1-patternlanguage.htm Essay on the pattern language as it relates to urban design]
* [https://www.academia.edu/1354848/PROJECT_ICARUS_STAKEHOLDER_SCENARIOS_FOR_AN_INTERSTELLAR_EXPLORATION_PROGRAM Use of patterns for scenario development for large scale aerospace projects]
* [http://torgronsund.wordpress.com/2010/01/06/lean-startup-business-model-pattern/ Lean Startup Business Model Pattern]
* [http://www.informit.com/articles/printerfriendly.aspx?p=30084 What Is a Quality Use Case?] from the book ''Patterns for Effective Use Cases''
* [http://groupworksdeck.org/what-we-mean-by-pattern Characteristics of group facilitation patterns]

===Online pattern collections===
* [http://www.patternlanguage.com/ patternlanguage.com], by the Center for Environmental Structure
* [http://www.fusedgrid.ca/ Fused Grid] – A Contemporary Urban Pattern "a collection and synthesis of neighbourhood patterns"
* [http://www.reliableprosperity.net ReliableProsperity.net] – Patterns for building a "restorative, socially just, and reliably prosperous society"
* [http://www.hcipatterns.org/ hcipatterns.org] – Patterns for HCI
* [http://www.c2.com/cgi/wiki?PatternIndex The Portland Pattern Repository]
* [http://developer.yahoo.com/ypatterns Yahoo! Design Pattern Library]
* [http://groupworksdeck.org Group Works: A Pattern Language for Bringing Life to Meetings and Other Gatherings] – A pattern language of group process
* [http://liveingreatness.com/core-protocols/ The Core Protocols] – A set of team communication patterns
* [http://www.publicsphereproject.org/patterns/lv Liberating Voices! Pattern Language Project] — Short versions of patterns available in [http://www.publicsphereproject.org/patterns_arabic Arabic], [http://www.publicsphereproject.org/patterns_chinese Chinese], and [http://www.publicsphereproject.org/patterns_spanish Spanish]

{{DEFAULTSORT:Pattern Language}}

[[Category:Architectural theory]]
[[Category:Cybernetics]]
[[Category:Design]]
[[Category:Knowledge representation]]
[[Category:Linguistics]]

[[fi:Suunnittelumalli]]</text>
      <sha1>ejqdw6mqny96974hgjzxajmkej7wi6g</sha1>
    </revision>
  </page>
  <page>
    <title>Poincaré–Hopf theorem</title>
    <ns>0</ns>
    <id>1267288</id>
    <revision>
      <id>868844254</id>
      <parentid>845001626</parentid>
      <timestamp>2018-11-14T20:28:20Z</timestamp>
      <contributor>
        <username>RadostW</username>
        <id>35058827</id>
      </contributor>
      <comment>Added graphic illustrating 2D case</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5484">In [[mathematics]], the '''Poincaré–Hopf theorem''' (also known as the '''Poincaré–Hopf index formula''', '''Poincaré–Hopf index theorem''', or '''Hopf index theorem''') is an important theorem that is used in [[differential topology]].  It is named after [[Henri Poincaré]] and [[Heinz Hopf]].

The '''Poincaré–Hopf''' theorem is often 
illustrated by the special case of the [[hairy ball theorem]], which simply states that there is no smooth [[vector field]] on a sphere having no sources or sinks.

[[File:Poincare-Hopf.svg|thumb|According to Poincare-Hopf theorem closed trajectory can encircle two centres and one saddle or one centre but never just the saddle. (Here for in case of a Hamiltonian system)]]

==Formal statement==

Let ''M'' be a differentiable manifold, of dimension ''n'', and ''v'' a vector field on ''M''. Suppose that ''x'' is an isolated zero of ''v'', and fix some local coordinates near ''x''. Pick a closed ball ''D'' centered at ''x'', so that ''x'' is the only zero of ''v'' in ''D''. Then we define the index of ''v'' at ''x'', index&lt;sub&gt;x&lt;/sub&gt;(''v''), to be the [[Degree of a continuous mapping#Differential topology|degree]] of the map ''u'':∂''D''&amp;rarr;''S''&lt;sup&gt;n-1&lt;/sup&gt; from the [[Manifold#Boundary and interior|boundary]] of ''D'' to the (''n''-1)-sphere given by  ''u''(''z'')=''v''(''z'')/|&amp;thinsp;''v''(''z'')&amp;thinsp;|.

'''Theorem.''' Let ''M'' be a [[compact space|compact]] [[differentiable manifold]].  Let ''v'' be a [[vector field]] on ''M'' with isolated zeroes. If ''M'' has [[Manifold with boundary|boundary]], then we insist that ''v'' be pointing in the outward normal direction along the boundary.  Then we have the formula

:&lt;math&gt;\sum_i \operatorname{index}_{x_i}(v) = \chi(M)\,&lt;/math&gt;

where the sum of the indices is over all the isolated zeroes of ''v'' and &lt;math&gt;\chi(M)&lt;/math&gt; is the [[Euler characteristic]] of ''M''. A particularly useful corollary is when there is a non-vanishing vector field implying Euler characteristic 0.

The theorem was proven for two dimensions by [[Henri Poincaré]] and later generalized to higher dimensions by [[Heinz Hopf]].

==Significance==
The Euler characteristic of a closed surface is a purely [[topology|topological]] concept, whereas the index of a vector field is purely [[analysis (mathematics)|analytic]]. Thus, this theorem establishes a deep link between two seemingly unrelated areas of mathematics. It is perhaps as interesting that the proof of this theorem relies heavily on [[Integral|integration]], and, in particular, [[Stokes' theorem]], which states that the integral of the [[exterior derivative]] of a [[differential form]] is equal to the integral of that form over the boundary. In the special case of a [[manifold (mathematics)|manifold]] without boundary, this amounts to saying that the integral is 0. But by examining vector fields in a sufficiently small neighborhood of a source or sink, we see that sources and sinks contribute [[integer]] amounts (known as the index) to the total, and they must all sum to 0. This result may be considered{{bywhom|date=March 2015}} one of the earliest of a whole series of theorems{{which|date=March 2015}} establishing deep relationships between [[geometry|geometric]] and [[analysis (mathematics)|analytical]] or [[physics|physical]] concepts. They play an important role in the modern study of both fields.

==Sketch of proof==
1. Embed ''M'' in some high-dimensional Euclidean space. (Use the [[Whitney embedding theorem]].)

2. Take a small neighborhood of ''M'' in that Euclidean space, ''N''&lt;sub&gt;ε&lt;/sub&gt;. Extend the vector field to this neighborhood so that it still has the same zeroes and the zeroes have the same indices. In addition, make sure that the extended vector field at the boundary of ''N''&lt;sub&gt;ε&lt;/sub&gt; is directed outwards.

3. The sum of indices of the zeroes of the old (and new) vector field is equal to the degree of the [[Gauss map]] from the boundary of ''N''&lt;sub&gt;ε&lt;/sub&gt; to the {{nowrap|1=(''n''–1)-dimensional}} sphere. Thus, the sum of the indices is independent of the actual vector field, and depends only on the manifold ''M''.
Technique: cut away all zeroes of the vector field with small neighborhoods. Then use the fact that the degree of a map from the boundary of an n-dimensional manifold to an {{nowrap|1=(''n''–1)-dimensional}} sphere, that can be extended to the whole n-dimensional manifold, is zero.

4. Finally, identify this sum of indices as the Euler characteristic of ''M''. To do that, construct a very specific vector field on ''M'' using a [[triangulation (topology)|triangulation]] of ''M'' for which it is clear that the sum of indices is equal to the Euler characteristic.

== Generalization ==
It is still possible to define the index for a vector field with nonisolated zeroes. A construction of this index and the extension of Poincaré–Hopf theorem for vector fields with nonisolated zeroes is outlined in Section 1.1.2 of {{harv|Brasselet|Seade|Suwa|2009}}.

== See also ==
* [[Eisenbud–Levine–Khimshiashvili signature formula]]
* [[Hopf theorem]]

==References==
*{{Springer|id=p/p110160|title=Poincaré–Hopf theorem}}
*{{cite book|ref=singular|last1=Brasselet|first1=Jean-Paul|last2=Seade|first2=José|last3=Suwa|first3=Tatsuo|title=Vector fields on singular varieties|date=2009|publisher=Springer|location=Heidelberg|isbn=978-3-642-05205-7}}

{{DEFAULTSORT:Poincare-Hopf theorem}}
[[Category:Theorems in differential topology]]</text>
      <sha1>9ua4vxc3ai8wy4z5md0m4oggsn00ng7</sha1>
    </revision>
  </page>
  <page>
    <title>Relative dimension</title>
    <ns>0</ns>
    <id>14076693</id>
    <revision>
      <id>850650807</id>
      <parentid>850502176</parentid>
      <timestamp>2018-07-17T05:46:54Z</timestamp>
      <contributor>
        <username>Jimfbleak</username>
        <id>7872</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contribs/Shane O Perry|Shane O Perry]] ([[User talk:Shane O Perry|talk]]) to last version by Lucis Aeternae</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1240">{{unreferenced|date=September 2012}}
In [[mathematics]], specifically [[linear algebra]] and [[geometry]], '''relative dimension''' is the dual notion to [[codimension]].

In linear algebra, given a [[quotient space (linear algebra)|quotient map]] &lt;math&gt;V \to Q&lt;/math&gt;, the difference dim ''V'' − dim ''Q'' is the relative dimension; this equals the dimension of the [[Kernel (linear algebra)|kernel]].

In [[fiber bundle]]s, the relative dimension of the map is the dimension of the fiber.

More abstractly, the codimension of a map is the dimension of the [[cokernel]], while the relative dimension of a map is the dimension of the [[Kernel (algebra)|kernel]].

These are dual in that the inclusion of a subspace &lt;math&gt;V \to W&lt;/math&gt; of codimension ''k'' dualizes to yield a quotient map &lt;math&gt;W^* \to V^*&lt;/math&gt; of relative dimension ''k'', and conversely.

The additivity of codimension under intersection corresponds to the additivity of relative dimension in a [[fiber product]].

Just as codimension is mostly used for [[injective]] maps, relative dimension is mostly used for [[surjective]] maps.

[[Category:Algebraic geometry]]
[[Category:Geometric topology]]
[[Category:Linear algebra]]
[[Category:Dimension]]
{{geometry-stub}}</text>
      <sha1>k00u02hvmitj39rpuwcbosmsvbgi5in</sha1>
    </revision>
  </page>
  <page>
    <title>Routh–Hurwitz theorem</title>
    <ns>0</ns>
    <id>2067581</id>
    <revision>
      <id>790221824</id>
      <parentid>770559434</parentid>
      <timestamp>2017-07-12T11:09:55Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <comment>/* References */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4485">{{no footnotes|date=March 2012}}
In [[mathematics]], the '''Routh–Hurwitz theorem''' gives a test to determine whether all [[root of a function|roots]] of a given [[polynomial]] lie in the left half-plane. [[Polynomial]]s with this property are called Hurwitz-[[stable polynomial|stable]]. [[Derivation of the Routh array|The Routh–Hurwitz theorem was proved]] in 1895, and it was named after [[Edward John Routh]] and [[Adolf Hurwitz]].

==Notations==
Let ''f''(''z'') be a polynomial (with [[complex number|complex]] coefficients) of degree ''n'' with no roots on the [[complex plane|imaginary line]] (i.e. the line ''Z''&amp;nbsp;=&amp;nbsp;''ic'' where ''i'' is the [[imaginary unit]] and ''c'' is a [[real number]]).  Let us define &lt;math&gt;P_0(y)&lt;/math&gt; (a polynomial of degree ''n'') and &lt;math&gt;P_1(y)&lt;/math&gt; (a nonzero polynomial of degree strictly less than ''n'') by &lt;math&gt;f(iy)=P_0(y)+iP_1(y)&lt;/math&gt;, respectively the [[real part|real]] and [[imaginary part]]s of ''f'' on the imaginary line.

Furthermore, let us denote by:
* ''p'' the number of roots of ''f'' in the left [[half-plane]] (taking into account multiplicities);
* ''q'' the number of roots of ''f'' in the right half-plane (taking into account multiplicities);
* &lt;math&gt;\Delta\arg f(iy)&lt;/math&gt; the variation of the argument of ''f''(''iy'') when ''y'' runs from −&amp;infin; to +&amp;infin;;
* ''w''(''x'') is the number of variations of the [[Sturm's theorem#Generalized Sturm chains|generalized Sturm chain]] obtained from &lt;math&gt;P_0(y)&lt;/math&gt; and &lt;math&gt;P_1(y)&lt;/math&gt; by applying the [[Euclid's algorithm|Euclidean algorithm]];
* &lt;math&gt;I_{-\infty}^{+\infty}r&lt;/math&gt; is the [[Cauchy index]] of the [[rational function]] ''r'' over the real line.

==Statement==
With the notations introduced above, the '''Routh–Hurwitz theorem''' states that:

:&lt;math&gt;p-q=\frac{1}{\pi}\Delta\arg f(iy)= \left.\begin{cases} +I_{-\infty}^{+\infty}\frac{P_0(y)}{P_1(y)} &amp; \text{for odd degree} \\[10pt] -I_{-\infty}^{+\infty}\frac{P_1(y)}{P_0(y)} &amp; \text{for even degree} \end{cases}\right\} = w(+\infty)-w(-\infty).&lt;/math&gt;

From the first equality we can for instance conclude that when the variation of the argument of ''f''(''iy'') is positive, then ''f''(''z'') will have more roots to the left of the imaginary axis than to its right.
The equality ''p''&amp;nbsp;&amp;minus;&amp;nbsp;''q'' =&amp;nbsp;''w''(+&amp;infin;)&amp;nbsp;&amp;minus;&amp;nbsp;''w''(&amp;minus;&amp;infin;) can be viewed as the complex counterpart of [[Sturm's theorem]].  Note the differences:  in Sturm's theorem, the left member is ''p''&amp;nbsp;+&amp;nbsp;''q'' and the ''w'' from the right member is the number of variations of a Sturm chain (while ''w'' refers to a generalized Sturm chain in the present theorem).

==Routh–Hurwitz stability criterion==
{{main|Routh–Hurwitz stability criterion}}
We can easily determine a stability criterion using this theorem as it is trivial that ''f''(''z'') is [[Stable polynomial|Hurwitz-stable]] [[iff]] ''p''&amp;nbsp;&amp;minus;&amp;nbsp;''q'' = ''n''.  We thus obtain conditions on the coefficients of ''f''(''z'') by imposing ''w''(+&amp;infin;) = ''n'' and ''w''(&amp;minus;&amp;infin;) = 0.

==References==
* {{cite book
 | author = Routh, E.J.
 | year = 1877
 | title = A Treatise on the Stability of a Given State of Motion, Particularly Steady Motion.
 | publisher = Macmillan and co.
 | isbn = 
}}
* {{cite book
 | last = Hurwitz |first=A.
 | year = 1964
 | chapter = On The Conditions Under Which An Equation Has Only Roots With Negative Real Parts
 | title = Selected Papers on Mathematical Trends in Control Theory
 | location = New York |publisher=Dover
 | editor-first = Richard |editor-last= Bellman |editorlink=Richard E. Bellman
 | editor2-first = Robert E. |editor2-last=Kalaba
}}
*{{cite book
 | last = Gantmacher |first=F. R. |authorlink=Felix Gantmacher
 | year = 2005 |origyear = 1959
 | title = Applications of the Theory of Matrices
 | publisher = Dover |location= New York
 | pages = 226–233
 | isbn = 0-486-44554-2
}}
* {{cite book | last1=Rahman | first1=Q. I. | last2=Schmeisser | first2=G. | title=Analytic theory of polynomials | series=London Mathematical Society Monographs. New Series | volume=26 | location=Oxford | publisher=[[Oxford University Press]] | year=2002 | isbn=0-19-853493-0 | zbl=1072.30006 }}

==External links==
* [http://mathworld.wolfram.com/Routh-HurwitzTheorem.html Mathworld entry]

{{DEFAULTSORT:Routh-Hurwitz theorem}}
[[Category:Polynomials]]
[[Category:Theorems in complex analysis]]
[[Category:Theorems in real analysis]]</text>
      <sha1>7ag5vk6uajqjfs9d5dfmrlj4o4508h8</sha1>
    </revision>
  </page>
  <page>
    <title>Simple set</title>
    <ns>0</ns>
    <id>2451294</id>
    <revision>
      <id>604649618</id>
      <parentid>550350435</parentid>
      <timestamp>2014-04-17T21:31:39Z</timestamp>
      <contributor>
        <ip>159.83.54.2</ip>
      </contributor>
      <comment>/* Formal definitions and some properties */ +iff -&gt; if</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3501">In [[recursion theory]] a subset of the natural numbers is called a '''simple set''' if it is co-infinite and [[recursively enumerable]], but every infinite subset of its complement fails to be enumerated recursively.  Simple sets are examples of recursively enumerable sets that are not [[computable set|recursive]].

== Relation to Post's problem ==
Simple sets were devised by [[Emil Leon Post]] in the search for a non-Turing-complete recursively enumerable set.  Whether such sets exist is known as [[Post's problem]].  Post had to prove two things in order to obtain his result, one is that the simple set, say ''A'', does not Turing-reduce to the empty set, and that the ''K'', the [[halting problem]], does not Turing-reduce to ''A''.  He succeeded in the first part (which is obvious by definition), but for the other part, he managed only to prove a [[many-one reduction]].

It was affirmed by Friedberg and Muchnik in the 1950s using a novel technique called the [[priority method]].  They give a construction for a set that is simple (and thus non-recursive), but fails to compute the halting problem.&lt;ref name=Nies35&gt;Nies (2009) p.35&lt;/ref&gt;

== Formal definitions and some properties ==
*A set &lt;math&gt;I \subseteq \mathbb{N}&lt;/math&gt; is called '''immune''' if &lt;math&gt;I&lt;/math&gt; is infinite, but for every index &lt;math&gt;e&lt;/math&gt;, we have &lt;math&gt;W_e \text{ infinite} \implies W_e \not\subseteq I&lt;/math&gt;.  Or equivalently: there is no infinite subset of &lt;math&gt;I&lt;/math&gt; that is recursively enumerable.
*A set &lt;math&gt;S \subseteq \mathbb{N}&lt;/math&gt; is called ''' simple ''' if it is recursively enumerable and its complement is immune.
*A set &lt;math&gt;I \subseteq \mathbb{N}&lt;/math&gt; is called '''effectively immune''' if &lt;math&gt;I&lt;/math&gt; is infinite, but there exists a recursive function &lt;math&gt;f&lt;/math&gt; such that for every index &lt;math&gt;e&lt;/math&gt;, we have that &lt;math&gt; W_e \subseteq I \implies \#(W_e) &lt; f(e)&lt;/math&gt;. 
*A set &lt;math&gt;S \subseteq \mathbb{N}&lt;/math&gt; is called '''effectively simple''' if it is recursively enumerable and its complement is effectively immune.  Every effectively simple set, is simple and Turing-complete.
*A set &lt;math&gt;I \subseteq \mathbb{N}&lt;/math&gt; is called '''hyperimmune''' if &lt;math&gt;I&lt;/math&gt; is infinite, but &lt;math&gt;p_I&lt;/math&gt; is not computably dominated, where &lt;math&gt;p_I&lt;/math&gt; is the list of members of &lt;math&gt;I&lt;/math&gt; in order.&lt;ref name=Nies27&gt;Nies (2009) p.27&lt;/ref&gt;
*A set &lt;math&gt;S \subseteq \mathbb{N}&lt;/math&gt; is called '''hypersimple''' if it is simple and its complement is hyperimmune.&lt;ref name=Nies37&gt;Nies (2009) p.37&lt;/ref&gt;

== Notes ==
{{reflist}}

== References ==
* {{cite book | first=Robert I. | last=Soare | title=Recursively enumerable sets and degrees. A study of computable functions and computably generated sets | series=Perspectives in Mathematical Logic | publisher=[[Springer-Verlag]] | location=Berlin | year=1987 | isbn=3-540-15299-7 | zbl=0667.03030 }}
* {{cite book | first=Piergiorgio | last=Odifreddi | authorlink=Piergiorgio Odifreddi | title=Classical recursion theory. The theory of functions and sets of natural numbers | publisher=North Holland | year=1988 | zbl=0661.03029 | series=Studies in Logic and the Foundations of Mathematics | volume=125 | location=Amsterdam | isbn=0-444-87295-7 }}
* {{cite book | last=Nies | first=André | title=Computability and randomness | series=Oxford Logic Guides | volume=51 | location=Oxford | publisher=Oxford University Press | year=2009 | isbn=978-0-19-923076-1 | zbl=1169.03034 }}

[[Category:Computability theory]]</text>
      <sha1>tlic3n712u62mqybo31ihm2y2z4lvij</sha1>
    </revision>
  </page>
  <page>
    <title>Stiffness matrix</title>
    <ns>0</ns>
    <id>21878925</id>
    <revision>
      <id>852420284</id>
      <parentid>852418341</parentid>
      <timestamp>2018-07-28T21:45:31Z</timestamp>
      <contributor>
        <username>Deacon Vorbis</username>
        <id>29330520</id>
      </contributor>
      <minor/>
      <comment>/* Practical assembly of the stiffness matrix */ also adjust transpose per [[MOS:MATH]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7406">:''For the stiffness tensor in solid mechanics, see [[Hooke's law#Matrix representation (stiffness tensor)]].''

In the [[finite element method]] for the numerical solution of elliptic [[partial differential equations]], the '''stiffness matrix''' represents the system of linear equations that must be solved in order to ascertain an approximate solution to the differential equation.

==The stiffness matrix for the Poisson problem==
For simplicity, we will first consider the [[Poisson problem]]

:&lt;math&gt; -\nabla^2 u = f&lt;/math&gt;

on some domain Ω, subject to the boundary condition ''u'' = 0 on the boundary of Ω. To discretize this equation by the finite element method, one chooses a set of ''basis functions'' {''φ''&lt;sub&gt;1&lt;/sub&gt;, ..., ''φ''&lt;sub&gt;''n''&lt;/sub&gt;} defined on Ω which also vanish on the boundary. One then approximates

:&lt;math&gt; u \approx u^h = u_1\varphi_1+\cdots+u_n\varphi_n.&lt;/math&gt;

The coefficients ''u''&lt;sub&gt;1&lt;/sub&gt;, ..., ''u''&lt;sub&gt;''n''&lt;/sub&gt; are determined so that the error in the approximation is orthogonal to each basis function ''φ''&lt;sub&gt;''i''&lt;/sub&gt;:

:&lt;math&gt; \int_\Omega \varphi_i\cdot f \, dx = -\int_\Omega \varphi_i\nabla^2u^h \, dx = -\sum_j\left(\int_\Omega \varphi_i\nabla^2\varphi_j\,dx\right)\, u_j = \sum_j\left(\int_\Omega \nabla\varphi_i\cdot\nabla\varphi_j\, dx\right)u_j.&lt;/math&gt;

The '''stiffness matrix''' is the n-element square matrix A defined by

:&lt;math&gt; A_{ij} = \int_\Omega\nabla\varphi_i\cdot\nabla\varphi_j\, dx.&lt;/math&gt;

By defining the vector ''F'' with components ''F''&lt;sub&gt;''i''&lt;/sub&gt; = &lt;math&gt;\int_\Omega\varphi_i f\,dx&lt;/math&gt;, the coefficients ''u''&lt;sub&gt;''i''&lt;/sub&gt; are determined by the linear system ''AU'' = ''F''. The stiffness matrix is symmetric, i.e. ''A''&lt;sub&gt;''ij''&lt;/sub&gt; = ''A''&lt;sub&gt;''ji''&lt;/sub&gt;, so all its eigenvalues are real. Moreover, it is a strictly [[positive-definite matrix]], so that the system ''AU'' = ''F'' always has a unique solution. (For other problems, these nice properties will be lost.)

Note that the stiffness matrix will be different depending on the computational grid used for the domain and what type of finite element is used. For example, the stiffness matrix when piecewise quadratic finite elements are used will have more degrees of freedom than piecewise linear elements.

==The stiffness matrix for other problems==
Determining the stiffness matrix for other PDE follows essentially the same procedure, but it can be complicated by the choice of boundary conditions. As a more complex example, consider the elliptic equation

:&lt;math&gt; -\sum_{k,l}\frac{\partial}{\partial x_k}\left(a^{kl}\frac{\partial u}{\partial x_l}\right) = f&lt;/math&gt;

where ''A''(''x'') = ''a''&lt;sup&gt;''kl''&lt;/sup&gt;(''x'') is a positive-definite matrix defined for each point ''x'' in the domain. We impose  the [[Robin boundary condition]]

:&lt;math&gt; -\sum_{k,l}\nu_k a^{kl}\frac{\partial u}{\partial x_l} = c(u-g),&lt;/math&gt;

where ''ν''&lt;sub&gt;''k''&lt;/sub&gt; is the component of the unit outward normal vector ''ν'' in the ''k''-th direction. The system to be solved is

:&lt;math&gt; \sum_j\left(\sum_{k,l}\int_\Omega a^{kl}\frac{\partial\varphi_i}{\partial x_k}\frac{\partial\varphi_j}{\partial x_l}dx+\int_{\partial\Omega}c\varphi_i\varphi_j\, ds\right)u_j = \int_\Omega\varphi_i f\, dx+\int_{\partial\Omega}c\varphi_i g\, ds,&lt;/math&gt;

as can be shown using an analogue of Green's identity. The coefficients ''u''&lt;sub&gt;''i''&lt;/sub&gt; are still found by solving a system of linear equations, but the matrix representing the system is markedly different from that for the ordinary Poisson problem.

In general, to each scalar elliptic operator ''L'' of order 2''k'', there is associated a bilinear form ''B'' on the [[Sobolev space]] ''H''&lt;sup&gt;''k''&lt;/sup&gt;, so that the [[weak formulation]] of the equation ''Lu'' = ''f'' is

:&lt;math&gt; B[u,v] = (f,v)&lt;/math&gt;

for all functions ''v'' in ''H''&lt;sup&gt;''k''&lt;/sup&gt;. Then the stiffness matrix for this problem is

:&lt;math&gt; A_{ij} = B[\varphi_j,\varphi_i].&lt;/math&gt;

==Practical assembly of the stiffness matrix==
In order to implement the finite element method on a computer, one must first choose a set of basis functions and then compute the integrals defining the stiffness matrix. Usually, the domain Ω is discretized by some form of [[mesh generation]], wherein it is divided into non-overlapping triangles or quadrilaterals, which are generally referred to as elements. The basis functions are then chosen to be polynomials of some order within each element, and continuous across element boundaries. The simplest choices are piecewise linear for triangular elements and piecewise bilinear for rectangular elements.

The '''element stiffness matrix''' ''A''&lt;sup&gt;[''k'']&lt;/sup&gt; for element ''T''&lt;sub&gt;''k''&lt;/sub&gt; is the matrix

:&lt;math&gt; A^{[k]}_{ij} = \int_{T_k}\nabla\varphi_i\cdot\nabla\varphi_j\, dx.&lt;/math&gt;

The element stiffness matrix is zero for most values of i and j, for which the corresponding basis functions are zero within ''T''&lt;sub&gt;''k''&lt;/sub&gt;. The full stiffness matrix ''A'' is the sum of the element stiffness matrices. In particular, for basis functions that are only supported locally, the stiffness matrix is [[sparse matrix|sparse]].

For many standard choices of basis functions, i.e. piecewise linear basis functions on triangles, there are simple formulas for the element stiffness matrices. For example, for piecewise linear elements, consider a triangle with vertices (''x''&lt;sub&gt;1&lt;/sub&gt;,''y''&lt;sub&gt;1&lt;/sub&gt;), (''x''&lt;sub&gt;2&lt;/sub&gt;,''y''&lt;sub&gt;2&lt;/sub&gt;), (''x''&lt;sub&gt;3&lt;/sub&gt;,''y''&lt;sub&gt;3&lt;/sub&gt;), and define the 2×3 matrix

:&lt;math&gt; D = \left[\begin{matrix}x_3 - x_2 &amp; x_1 - x_3 &amp; x_2 - x_1 \\ y_3 - y_2 &amp; y_1 - y_3 &amp; y_2 - y_1\end{matrix}\right].&lt;/math&gt;

Then the element stiffness matrix is

:&lt;math&gt; A^{[k]} = D^\mathsf{T} D/(4 \operatorname{area}(T)).&lt;/math&gt;

When the differential equation is more complicated, say by having an inhomogeneous diffusion coefficient, the integral defining the element stiffness matrix can be evaluated by [[Gaussian quadrature]].

The [[condition number]] of the stiffness matrix depends strongly on the quality of the numerical grid. In particular, triangles with small angles in the finite element mesh induce large eigenvalues of the stiffness matrix, degrading the solution quality.

==References==
* {{citation |first1=A. |last1=Ern |first2=J.-L. |last2=Guermond |year=2004 |title=Theory and Practice of Finite Elements |publisher=Springer-Verlag |location=New York, NY |isbn=0387205748 }}
* {{citation |first=M.S. |last=Gockenbach |year=2006 |title=Understanding and Implementing the Finite Element Method |publisher=SIAM |location=Philadelphia, PA |isbn=0898716144 }}
* {{citation |first1=C. |last1=Grossmann |first2=H.-G. |last2=Roos |first3=M. |last3=Stynes |year=2007 |title=Numerical Treatment of Partial Differential Equations |publisher=Springer-Verlag |location=Berlin, Germany |isbn=978-3-540-71584-9 }}
* {{citation |first=C. |last=Johnson |year=2009 |title=Numerical Solution of Partial Differential Equations by the Finite Element Method |publisher=Dover |isbn=978-0486469003 }}
* {{citation |first1=O.C. |last1=Zienkiewicz |author1-link=Olgierd Zienkiewicz |first2=R.L. |last2=Taylor |first3=J.Z. |last3=Zhu |year=2005 |title=The Finite Element Method: Its Basis and Fundamentals |publisher=Elsevier Butterworth-Heinemann |edition=6th |location=Oxford, UK |isbn=978-0750663205 }}

[[Category:Applied mathematics]]
[[Category:Numerical analysis]]</text>
      <sha1>nqq242za12qx4i947nqrevy2jnz06f8</sha1>
    </revision>
  </page>
  <page>
    <title>Supercombinator</title>
    <ns>0</ns>
    <id>1479058</id>
    <revision>
      <id>841092359</id>
      <parentid>711794038</parentid>
      <timestamp>2018-05-14T01:23:51Z</timestamp>
      <contributor>
        <username>LaundryPizza03</username>
        <id>32694456</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1194">{{disputed|date=November 2015}}
{{cleanup-rewrite|date=November 2015}}

A '''supercombinator''' is a [[mathematical expression]] which is [[Free variables and bound variables|fully bound]] and self-contained. It may be either a [[constant (mathematics)|constant]] or a [[combinator]] where all the subexpressions are supercombinators. Supercombinators are used in the implementation of functional languages.

In mathematical terms, a [[Lambda calculus|lambda expression]] ''S'' is a supercombinator of [[arity]] ''n'' if it has no free variables and is of the form λx&lt;sub&gt;1&lt;/sub&gt;.λx&lt;sub&gt;2&lt;/sub&gt;...λx&lt;sub&gt;n&lt;/sub&gt;.''E'' (with ''n''&amp;nbsp;≥&amp;nbsp;0, so that lambdas are not required) such that ''E'' itself is not a [[lambda abstraction]] and any lambda abstraction in ''E'' is again a supercombinator.

== See also ==
* [[Lambda lifting]]

==References==
*S. L. Peyton Jones, ''[https://research.microsoft.com/en-us/um/people/simonpj/papers/slpj-book-1987/ The Implementation of Functional Programming Languages]''. Prentice Hall, 1987.

[[Category:Functional programming]]
[[Category:Implementation of functional programming languages]]
[[Category:Lambda calculus]]


{{comp-sci-theory-stub}}</text>
      <sha1>7r8ow8e0ygh36escynogmhodm2jjggx</sha1>
    </revision>
  </page>
  <page>
    <title>Truncated icosidodecahedron</title>
    <ns>0</ns>
    <id>286534</id>
    <revision>
      <id>856435593</id>
      <parentid>833858823</parentid>
      <timestamp>2018-08-25T05:15:20Z</timestamp>
      <contributor>
        <username>Double sharp</username>
        <id>10274643</id>
      </contributor>
      <minor/>
      <comment>/* Names */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12352">{{Semireg polyhedra db|Semireg polyhedron stat table|grID}}
In [[geometry]], the '''truncated icosidodecahedron''' is an [[Archimedean solid]], one of thirteen convex [[Isogonal figure|isogonal]] nonprismatic solids constructed by two or more types of [[regular polygon]] [[Face (geometry)|face]]s.

It has 62 faces: 30 [[square (geometry)|squares]], 20 regular [[hexagon]]s, and 12 regular [[decagon]]s. It has more vertices (120) and edges (180) than any other convex nonprismatic [[uniform polyhedron]]. Since each of its faces has point symmetry (equivalently, 180° [[rotation]]al symmetry), the truncated icosidodecahedron is a [[zonohedron]].

==Names==
{|
| &lt;!-- Does someone know a better way to make the image float right, but not below the infobox? --&gt;
The name ''truncated icosidodecahedron'', given originally by [[Johannes Kepler]], is misleading. An actual [[truncation (geometry)|truncation]] of a [[icosidodecahedron]] has [[rectangle]]s instead of [[square]]s. This nonuniform polyhedron is [[topologically]] equivalent to the Archimedean solid.

Alternate interchangeable names are:
*''Truncated icosidodecahedron'' ([[Johannes Kepler]])
*''Rhombitruncated icosidodecahedron'' ([[Magnus Wenninger]]&lt;ref&gt;Wenninger, (Model 16, p. 30)&lt;/ref&gt;)
*''Great rhombicosidodecahedron'' ([[Robert Williams (geometer)|Robert Williams]],&lt;ref&gt;Williamson (Section 3-9, p. 94)&lt;/ref&gt; Peter Cromwell&lt;ref&gt;Cromwell (p. 82)&lt;/ref&gt;)
*''[[Omnitruncation (geometry)|Omnitruncated]] dodecahedron'' or ''icosahedron'' ([[Norman Johnson (mathematician)|Norman Johnson]])
|style="padding-left:50px;"| &lt;!-- right table cell --&gt;
{{multiple image
 | align = left  | width = 150
 | image1 = Polyhedron 12-20 big.png
 | image2 = Polyhedron nonuniform truncated 12-20 big.png
 | footer = Icosidodecahedron and its truncation
}}
|}

The name ''great rhombicosidodecahedron'' refers to the relationship with the (small) [[rhombicosidodecahedron]] (compare section [[#Dissection|Dissection]]).&lt;br&gt;
There is a [[nonconvex uniform polyhedron]] with a similar name, the [[nonconvex great rhombicosidodecahedron]].

==Area and volume==
The surface area ''A'' and the volume ''V'' of the truncated icosidodecahedron of edge length ''a'' are:{{citation needed|date=January 2017}}
&lt;!-- wrong value, given on MathWorld, need a source for correct value above
&lt;math&gt;\begin{align} A &amp; = 30 \left [ 1 + \sqrt{ 2 \left ( 4 + \sqrt{5} + \sqrt{15+6\sqrt{6}} \right ) } \right ] a^2 \\
&amp; \approx 175.031\,045a^2 \\--&gt;
: &lt;math&gt;\begin{align} A &amp;= 30 \left (1 + \sqrt{3} + \sqrt{5 + 2\sqrt{5}} \right)a^2 &amp;&amp;\approx 174.292\,0303a^2. \\ V &amp;= \left( 95 + 50\sqrt{5} \right) a^3 &amp;&amp;\approx 206.803\,399a^3. \end{align}&lt;/math&gt;

If a set of all 13 [[Archimedean solid]]s were constructed with all edge lengths equal, the truncated icosidodecahedron would be the largest.

==Cartesian coordinates==
[[Cartesian coordinates]] for the vertices of a truncated icosidodecahedron with edge length 2''φ''&amp;nbsp;−&amp;nbsp;2, centered at the origin, are all the [[even permutation]]s of:&lt;ref&gt;{{mathworld|title=Icosahedral group|urlname=IcosahedralGroup}}&lt;/ref&gt;
:(±{{sfrac|1|''φ''}}, ±{{sfrac|1|''φ''}}, ±(3&amp;nbsp;+&amp;nbsp;''φ'')),
:(±{{sfrac|2|''φ''}}, ±''φ'', ±(1&amp;nbsp;+&amp;nbsp;2''φ'')),
:(±{{sfrac|1|''φ''}}, ±''φ''&lt;sup&gt;2&lt;/sup&gt;, ±(−1&amp;nbsp;+&amp;nbsp;3''φ'')),
:(±(2''φ''&amp;nbsp;−&amp;nbsp;1), ±2, ±(2&amp;nbsp;+&amp;nbsp;''φ'')) and
:(±''φ'', ±3, ±2''φ''),
where ''φ''&amp;nbsp;=&amp;nbsp;{{sfrac|1 + {{sqrt|5}}|2}} is the [[golden ratio]].

== Dissection ==
{{multiple image
 | align = left  | total_width = 300
 | image1 = Small in great rhombi 12-20, davinci small with cuboids.png |width1=1|height1=1 
 | image2 = Small in great rhombi 12-20, davinci.png
}}
The truncated icosidodecahedron is the [[convex hull]] of a [[rhombicosidodecahedron]] with [[cuboid]]s above its 30 squares whose height to base ratio is the [[golden ratio]]. The rest of its space can be dissected into 12 nonuniform [[pentagonal cupola]]s below the decagons and 20 nonuniform [[triangular cupola]]s below the hexagons.
{{-}}
==Orthogonal projections==
The truncated icosidodecahedron has seven special [[orthogonal projection]]s, centered on a vertex, on three types of edges, and three types of faces: square, hexagonal and decagonal. The last two correspond to the A&lt;sub&gt;2&lt;/sub&gt; and H&lt;sub&gt;2&lt;/sub&gt; [[Coxeter plane]]s.
{|class=wikitable
|+ Orthogonal projections
|-
!Centered by
!Vertex
!Edge&lt;br&gt;4-6
!Edge&lt;br&gt;4-10
!Edge&lt;br&gt;6-10
!Face&lt;br&gt;square
!Face&lt;br&gt;hexagon
!Face&lt;br&gt;decagon
|-
!Solid
|
|
|
|
|[[File:Polyhedron great rhombi 12-20 from blue max.png|100px]]
|[[File:Polyhedron great rhombi 12-20 from yellow max.png|100px]]
|[[File:Polyhedron great rhombi 12-20 from red max.png|100px]]
|-
!Wireframe
|[[File:Dodecahedron_t012_v.png|100px]]
|[[File:Dodecahedron_t012_e46.png|100px]]
|[[File:Dodecahedron_t012_e4x.png|100px]]
|[[File:Dodecahedron_t012_e6x.png|100px]]
|[[File:Dodecahedron_t012_f4.png|100px]]
|[[File:Dodecahedron_t012_A2.png|100px]]
|[[File:Dodecahedron_t012_H3.png|100px]]
|- align=center
!Projective&lt;br&gt;symmetry
|[2]&lt;sup&gt;+&lt;/sup&gt;
|[2]
|[2]
|[2]
|[2]
|[6]
|[10]
|-
!Dual&lt;BR&gt;image
|[[File:Dual dodecahedron_t012_v.png|100px]]
|[[File:Dual dodecahedron_t012_e46.png|100px]]
|[[File:Dual dodecahedron_t012_e4x.png|100px]]
|[[File:Dual dodecahedron_t012_e6x.png|100px]]
|[[File:Dual dodecahedron_t012_f4.png|100px]]
|[[File:Dual dodecahedron_t012_A2.png|100px]]
|[[File:Dual dodecahedron_t012_H3.png|100px]]
|}

==Spherical tilings and Schlegel diagrams ==
The truncated icosidodecahedron can also be represented as a [[spherical tiling]], and projected onto the plane via a [[stereographic projection]]. This projection is [[Conformal map|conformal]], preserving angles but not areas or lengths. Straight lines on the sphere are projected as circular arcs on the plane.

[[Schlegel diagram]]s are similar, with a [[perspective projection]] and straight edges.
{|class=wikitable
|-
![[Orthographic projection]]
!colspan=3|[[Stereographic projection]]s
|-
!
![[Decagon]]-centered
![[Hexagon]]-centered
![[Square]]-centered
|- align=center valign=top
|[[Image:Uniform tiling 532-t012.png|160px]]
|[[Image:Truncated icosidodecahedron stereographic projection decagon.png|160px]]
|[[Image:Truncated icosidodecahedron stereographic projection hexagon.png|168px]]
|[[Image:Truncated icosidodecahedron stereographic projection square.png|172px]]
|}

== Geometric variations ==
Within [[Icosahedral symmetry]] there are unlimited geometric variations of the ''truncated icosidodecahedron'' with [[Isogonal figure|isogonal]] faces. The [[truncated dodecahedron]], [[rhombicosidodecahedron]], and [[truncated icosahedron]] as degenerate limiting cases.
{| class=wikitable
|[[File:Truncated dodecahedron.png|100px]]
|[[File:Great truncated icosidodecahedron convex hull.png|100px]]
|[[File:Nonuniform_truncated_icosidodecahedron.png|100px]]
|[[File:Uniform_polyhedron-53-t012.png|100px]]
|[[File:Truncated_dodecadodecahedron_convex_hull.png|100px]]
|[[File:Icositruncated_dodecadodecahedron_convex_hull.png|100px]]
|[[File:Truncated icosahedron.png|100px]]
|[[File:Small rhombicosidodecahedron.png|100px]]
|- align=center
|{{CDD|node_1|5|node_1|3|node}}
|colspan=5|{{CDD|node_1|5|node_1|3|node_1}}
|{{CDD|node|5|node_1|3|node_1}}
|{{CDD|node_1|5|node|3|node_1}}
|}

== Truncated icosidodecahedral graph ==
{{Infobox graph
 | name = Truncated icosidodecahedral graph
 | image = [[File:Truncated icosidodecahedral graph.png|240px]]
 | image_caption = 5-fold symmetry
 | namesake = 
 | vertices = 120 
 | edges = 180 
 | automorphisms = 120 (A&lt;sub&gt;5&lt;/sub&gt;×2)
 | radius = 15
 | diameter = 15
 | girth = 4
 | chromatic_number = 2
 | chromatic_index = 
 | fractional_chromatic_index = 
 | properties = [[Cubic graph|Cubic]], [[hamiltonian graph|Hamiltonian]], [[regular graph|regular]], [[Zero-symmetric graph|zero-symmetric]]
}}
In the [[mathematics|mathematical]] field of [[graph theory]], a '''truncated icosidodecahedral graph''' (or '''great rhombicosidodecahedral graph''') is the [[1-skeleton|graph of vertices and edges]] of the truncated icosidodecahedron, one of the [[Archimedean solid]]s. It has 120 [[Vertex (graph theory)|vertices]] and 180 edges, and is a [[zero-symmetric graph|zero-symmetric]] and [[cubic graph|cubic]] [[Archimedean graph]].&lt;ref&gt;{{citation|last1=Read|first1=R. C.|last2=Wilson|first2=R. J.|title=An Atlas of Graphs|publisher=[[Oxford University Press]]|year= 1998|page=269}}&lt;/ref&gt;

{| class=wikitable
|+ [[Schlegel diagram]] graphs
|- align=center
|[[File:Truncated icosidodecahedral graph-hexcenter.png|160px]]&lt;BR&gt;3-fold symmetry
|[[File:Truncated icosidodecahedral graph-squarecenter.png|200px]]&lt;BR&gt;2-fold symmetry
|}
{{Clear}}

== Related polyhedra and tilings==
{| class=wikitable align=right width=320
|[[File:Conway_polyhedron_b3I.png|160px]] 
|[[File:Conway_polyhedron_b3D.png|160px]]
|-
|colspan=2|Bowtie icosahedron and dodecahedron contain two trapezoidal faces in place of the square.&lt;ref&gt;[http://www.cgl.uwaterloo.ca/csk/papers/bridges2001.html Symmetrohedra: Polyhedra from Symmetric Placement of Regular Polygons] Craig S. Kaplan&lt;/ref&gt;
|}
{{Icosahedral truncations}}

This polyhedron can be considered a member of a sequence of uniform patterns with vertex figure (4.6.2''p'') and [[Coxeter-Dynkin diagram]] {{CDD|node_1|p|node_1|3|node_1}}.  For ''p''&amp;nbsp;&lt;&amp;nbsp;6, the members of the sequence are [[Omnitruncation (geometry)|omnitruncated]] polyhedra ([[zonohedron]]s), shown below as spherical tilings. For ''p''&amp;nbsp;&gt;&amp;nbsp;6, they are tilings of the hyperbolic plane, starting with the [[truncated triheptagonal tiling]].

{{Omnitruncated table}}

==Notes==
{{reflist}}

==References==
*{{Citation |last1=Wenninger |first1=Magnus |author1-link=Magnus Wenninger |title=Polyhedron Models |publisher=[[Cambridge University Press]] |isbn=978-0-521-09859-5 |mr=0467493 |year=1974}}
*{{cite book|author=Cromwell, P.|year=1997|title=Polyhedra|location=United Kingdom|publisher=Cambridge|pages=79–86 ''Archimedean solids''|isbn=0-521-55432-2}}
*{{The Geometrical Foundation of Natural Structure (book)}}
*Cromwell, P.; [https://books.google.com/books?id=OJowej1QWpoC&amp;lpg=PP1&amp;pg=PA82#v=onepage&amp;q=&amp;f=false ''Polyhedra''], CUP hbk (1997), pbk. (1999).
*{{mathworld2 |urlname=GreatRhombicosidodecahedron |title=GreatRhombicosidodecahedron |urlname2=ArchimedeanSolid |title2=Archimedean solid}}
*{{KlitzingPolytopes|polyhedra.htm|3D convex uniform polyhedra|x3x5x - grid}}

==External links==
* {{mathworld | urlname = GreatRhombicosidodecahedron | title = Great rhombicosidodecahedron}}
* * {{mathworld | urlname = GreatRhombicosidodecahedralGraph | title = Great rhombicosidodecahedral graph}}
*[http://www.dr-mikes-math-games-for-kids.com/polyhedral-nets.html?net=jyCXwNVGKYhjPzSzHvH6BpWB1nxUllNUTutfUmnrdPCqgG9BfDHVz13yDxyy8ZbPXPYfuhjBgLDKLplZooNgdJytRWTgG9MGZ8jxB7DiK3wuGdDOyavgu12Y4kH9OibG0rtX2DxQy5UK9aRX0DuHn6C1VbRigFvqYXIbanO5FNXQHLTDE6mNNCh9aISfiW559Lk750AhuxmgOLuCn6UNixmnpE2QfOkJbL4PCYRmRpuIUS0QvLwkNqJ9q1F0I9vC5Fnjymc5wvWKYexxMjFeaqDG6Yg8zVPDADeEwOZJJqDpNXsl8m0ggt16OUXDFGtYHoyt5xQaq4zUpO9kofXCerjc8dVJiwH5uspTLVvp43GjR5TmfrHHfP8FxW7BmzeyeRYmAMMbpgYCcjc69XJG9F4owdKUZw7JkSSc1C3n2PAnZdeniInNLUEzmrLH4g4t9zH1R47Ebu57xji7f1sagyoPR6NiY8YZcN5a7oosgqpJSLouSkkJrPMnr5OOaB5NrI5tExTmzZzhIhCbZu8ywPdIW16i3nqv8rLgHaARBptR1nFHzKbepGzxiXCHlID2zJxCCHyYtiQtuI1oAUNNvoqb3oYNogIfxUD5JBb0noHc4n0O4wT3FGqfahAH6P7hPzmpEzTJvXL2klxvqZDFbNIAnMGj5nBvVbgNEjKyaBKcjNTGGbGRROZBn0bdEsPaDdd1WNmT1GAMXZMSw8TnBvk3UuRIcKGut2lijq3I2pkBIwjSMPsb2PMoWriRjzGLMW3G6uQaSCFV66d9LJSg5jOwaQEAsXsOO0fQLurlEfcRb4fK7zfYx01CcGc2cjaYvQvQtKvFQ4eLB9TMs05EaHtbXXJ4NNHXD27zh0jd9TO16gSBgSwwO1bDBp3bVZerWTc9p4wecUpEOW0jbQRQvEXdDmxZ8ABN7rVNJ6lkiRDQlIzpnL3Isus0VzwN8FHLEkdhV7gzFbQr7vrstMacKS5vN26ggaSgJ4PwWjQPOVQBsqY0H8V4DTUuUZ3vojV5jD8cehrj8KSjr71Zqxu&amp;name=Truncated+Icosidodecahedron#applet Editable printable net of a truncated icosidodecahedron with interactive 3D view]
*[http://www.mathconsult.ch/showroom/unipoly/ The Uniform Polyhedra]
*[http://www.georgehart.com/virtual-polyhedra/vp.html Virtual Reality Polyhedra] The Encyclopedia of Polyhedra

{{Archimedean solids}}
{{Polyhedron navigator}}

{{DEFAULTSORT:Truncated Icosidodecahedron}}
[[Category:Uniform polyhedra]]
[[Category:Archimedean solids]]
[[Category:Truncated tilings]]
[[Category:Zonohedra]]
[[Category:Individual graphs]]
[[Category:Planar graphs]]</text>
      <sha1>hekxmm6zai8nxtc5ptzcyjmhxvewmeb</sha1>
    </revision>
  </page>
  <page>
    <title>Turán's inequalities</title>
    <ns>0</ns>
    <id>14989249</id>
    <revision>
      <id>696558398</id>
      <parentid>691898329</parentid>
      <timestamp>2015-12-23T23:45:35Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>stub sort</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1976">In mathematics, '''Turán's inequalities''' are some inequalities for [[Legendre polynomials]] found by {{harvs|txt=yes|first=Paul |last=Turán|authorlink=Pál Turán|year=1950}} (and first published by {{harvtxt|Szegö|1948}}). There are many generalizations to other polynomials, often called Turán's inequalities, given by {{harvs|mr=0040487
|last=Beckenbach|first= E. F.|last2= Seidel|first2= W.|last3= Szász|first3= Otto
|title=Recurrent determinants of Legendre and of ultraspherical polynomials
|journal=Duke Math. J.|volume= 18|year=1951|pages= 1–10}}  and other authors.

If {{math|''P''&lt;sub&gt;''n''&lt;/sub&gt;}} is the {{mvar|n}}th [[Legendre polynomial]], Turán's inequalities state that
:&lt;math&gt;\,\! P_n(x)^2 &gt; P_{n-1}(x)P_{n+1}(x)\text{ for }-1&lt;x&lt;1.&lt;/math&gt;


For ''H''&lt;sub&gt;''n''&lt;/sub&gt;, the ''n''th [[Hermite polynomial]], Turán's inequalities are
:&lt;math&gt;H_n(x)^2 - H_{n-1}(x)H_{n+1}(x)= (n-1)!\cdot \sum_{i=0}^{n-1}\frac{2^{n-i}}{i!}H_i(x)^2&gt;0 ~,&lt;/math&gt;

whilst for [[Chebyshev polynomials]] they are
:&lt;math&gt;\! T_n(x)^2 - T_{n-1}(x)T_{n+1}(x)= 1-x^2&gt;0 \text{ for }-1&lt;x&lt;1~.&lt;/math&gt;

==See also==
*[[Askey–Gasper inequality]]
*[[Sturm Chain]]

==References==
*{{citation|mr=0040487
|last=Beckenbach|first= E. F.|last2= Seidel|first2= W.|last3= Szász|first3= Otto
|title=Recurrent determinants of Legendre and of ultraspherical polynomials
|journal=Duke Math. J.|volume= 18|year=1951|pages= 1–10|doi=10.1215/S0012-7094-51-01801-7}} 
*{{citation|mr=0023954
|last=Szegö|first= G.
|title=On an inequality of P. Turán concerning Legendre polynomials
|journal=Bull. Amer. Math. Soc. |year=1948|pages= 401–405
|doi=10.1090/S0002-9904-1948-09017-6|volume=54|issue=4 }} 
*{{citation|mr=0041284
|last=Turán|first= Paul
|title=On the zeros of the polynomials of Legendre 
|journal=Časopis Pěst. Mat. Fys. |volume=75|year=1950|pages= 113–122}}

{{DEFAULTSORT:Turan's inequalities}}
[[Category:Orthogonal polynomials]]
[[Category:Inequalities]]

{{mathanalysis-stub}}</text>
      <sha1>bpvkaskix3qhv6mx98gx3u89gvya51u</sha1>
    </revision>
  </page>
  <page>
    <title>Uncomputation</title>
    <ns>0</ns>
    <id>46879546</id>
    <revision>
      <id>846801308</id>
      <parentid>746209384</parentid>
      <timestamp>2018-06-21T00:08:37Z</timestamp>
      <contributor>
        <username>Bibcode Bot</username>
        <id>14394459</id>
      </contributor>
      <minor/>
      <comment>Adding 0 [[arXiv|arxiv eprint(s)]], 1 [[bibcode|bibcode(s)]] and 0 [[digital object identifier|doi(s)]]. Did it miss something? Report bugs, errors, and suggestions at [[User talk:Bibcode Bot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1187">[[File:Using Toffoli Gates and Ancilla Bits to make a Not Gate with many controls.png|thumb|400px|Creating a [[NOT gate]] with five controls out of [[Toffoli gate]]s and ancilla bits. Uncomputation is used to restore the ancilla bits to the OFF state before finishing.]]

'''Uncomputation''' is a technique, used in [[Reversible computing|reversible]] circuits, for cleaning up temporary side effects on [[Ancilla Bit|ancilla bits]] so they can be re-used.&lt;ref&gt;{{cite arXiv |eprint=1504.05155|last1=Aaronson|first1=Scott|title=The Classification of Reversible Bit Operations|last2=Grier|first2=Daniel|last3=Schaeffer|first3=Luke|class=quant-ph|year=2015}}&lt;/ref&gt;

Uncomputation is important to [[quantum computing]]. Whether or not intermediate effects have been uncomputed affects how states interfere with each other when measuring results.&lt;ref&gt;{{Cite journal|arxiv=quant-ph/0209060|last1=Aaronson|first1=Scott|title=Quantum Lower Bound for Recursive Fourier Sampling|journal=Quantum Information and Computation ():, 00|volume=3|issue=2|pages=165–174|year=2002|bibcode=2002quant.ph..9060A}}&lt;/ref&gt;

==References==
{{Reflist}}


{{Quantum-stub}}
[[Category:Quantum information science]]</text>
      <sha1>p3xu4iyktbs5gd86ndsctq5rg53kjwh</sha1>
    </revision>
  </page>
</mediawiki>
