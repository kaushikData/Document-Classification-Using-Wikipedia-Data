<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Bayesian econometrics</title>
    <ns>0</ns>
    <id>20484367</id>
    <revision>
      <id>845369430</id>
      <parentid>841567666</parentid>
      <timestamp>2018-06-11T10:03:06Z</timestamp>
      <contributor>
        <ip>91.198.67.77</ip>
      </contributor>
      <comment>/* Basics */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10432">{{More footnotes|date=July 2012}}
'''Bayesian econometrics''' is a branch of [[econometrics]] which applies [[Bayesianism|Bayesian]] principles to economic modelling.  Bayesianism is based on a degree-of-belief [[probability interpretations|interpretation of probability]], as opposed to a relative-frequency interpretation.

The Bayesian principle relies on [[Bayes' theorem]] which states that the [[Bayesian probability|probability]] of B conditional on A is the ratio of joint probability of A and B divided by probability of B. Bayesian econometricians assume that coefficients in the model have [[prior distribution]]s.

This approach was first propagated by [[Arnold Zellner]].&lt;ref&gt;{{cite book |last=Greenberg |first=Edward |title=Introduction to Bayesian Econometrics |location= |publisher=Cambridge University Press |edition=Second |year=2012 |isbn=978-1-107-01531-9 |url=https://books.google.com/books?id=ErUAOdGWFRAC }}&lt;/ref&gt;

== Basics ==

{{main|Bayesian inference}}

Subjective probabilities have to satisfy the [[Probability axioms|standard axioms of probability theory]] if one wishes to avoid losing a bet regardless of the outcome.&lt;ref&gt;Chapter 3 in de Finetti, B. (1990). ''Theory of Probability''. Chichester: John Wiley &amp; Sons.&lt;/ref&gt; Before the data is observed, the parameter &lt;math&gt;\theta&lt;/math&gt; is regarded as an unknown quantity and thus random variable, which is assigned a [[prior distribution]] &lt;math&gt;\pi(\theta)&lt;/math&gt; with &lt;math&gt;0 \leq \theta \leq 1&lt;/math&gt;. Bayesian analysis concentrates on the inference of the [[posterior distribution]] &lt;math&gt;\pi(\theta|y)&lt;/math&gt;, i.e. the distribution of the random variable &lt;math&gt;\theta&lt;/math&gt; conditional on the observation of the discrete data &lt;math&gt;y&lt;/math&gt;. The posterior density function &lt;math&gt;\pi(\theta|y)&lt;/math&gt; can be computed based on [[Bayes' Theorem]]:
:&lt;math&gt;\pi(\theta|y)=\frac{p(y|\theta)\pi(\theta)}{p(y)}&lt;/math&gt;
where &lt;math&gt;p(y)=\int p(y|\theta)\pi(\theta)d\theta&lt;/math&gt;, yielding a normalized probability function. For continuous data &lt;math&gt;y&lt;/math&gt;, this corresponds to:
:&lt;math&gt;\pi(\theta|y)=\frac{f(y|\theta)\pi(\theta)}{f(y)}&lt;/math&gt;
where &lt;math&gt;f(y)=\int f(y|\theta)\pi(\theta)d\theta&lt;/math&gt; and which is the centerpiece of Bayesian statistics and econometrics. It has the following components:
* &lt;math&gt;\pi(\theta|y)&lt;/math&gt;: the posterior density function of &lt;math&gt;\theta|y&lt;/math&gt;;
* &lt;math&gt;f(y|\theta)&lt;/math&gt;: the [[likelihood function]], i.e. the density function for the observed data &lt;math&gt;y&lt;/math&gt; when the parameter value is &lt;math&gt;\theta&lt;/math&gt;;
* &lt;math&gt;\pi(\theta)&lt;/math&gt;: the [[prior distribution]] of &lt;math&gt;\theta&lt;/math&gt;;
* &lt;math&gt;f(y)&lt;/math&gt;: the probability density function of &lt;math&gt;y&lt;/math&gt;.
The posterior function is given by &lt;math&gt;\pi(\theta|y)\propto f(y|\theta)\pi(\theta)&lt;/math&gt;, i.e., the posterior function is proportional to the product of the likelihood function and the prior distribution, and can be understood as a method of updating information, with the difference between &lt;math&gt;\pi(\theta)&lt;/math&gt; and &lt;math&gt;\pi(\theta|y)&lt;/math&gt; being the information gain concerning &lt;math&gt;\theta&lt;/math&gt; after observing new data. The choice of the prior distribution is used to impose restrictions on &lt;math&gt;\theta&lt;/math&gt;, e.g. &lt;math&gt;0\leq\theta\leq 1&lt;/math&gt;, with the [[beta distribution]] as a common choice due to (i) being defined between 0 and 1, (ii) being able to produce a variety of shapes, and (iii) yielding a posterior distribution of the standard form if combined with the likelihood function &lt;math&gt;\theta^{\Sigma y_i} (1-\theta)^{n-\Sigma y_i}&lt;/math&gt;. Based on the properties of the beta distribution, an ever-larger sample size implies that the mean of the posterior distribution approximates the maximum likelihood estimator &lt;math&gt;\bar{y}.&lt;/math&gt;
The assumed form of the likelihood function is part of the prior information and has to be justified. Different distributional assumptions can be compared using posterior odds ratios if a priori grounds fail to provide a clear choice. Commonly assumed forms include the beta distribution, the [[gamma distribution]], and the [[uniform distribution (continuous)|uniform distribution]], among others. If the model contains multiple parameters, the parameter can be redefined as a vector. Applying probability theory to that vector of parameters yields the marginal and conditional distributions of individual parameters or parameter groups. If data generation is sequential, Bayesian principles imply that the posterior distribution for the parameter based on new evidence will be proportional to the product of the likelihood for the new data, given previous data and the parameter, and the posterior distribution for the parameter, given the old data, which provides an intuitive way of allowing new information to influence beliefs about a parameter through [[Bayesian updating]]. If the sample size is large, (i) the prior distribution plays a relatively small role in determining the posterior distribution, (ii) the posterior distribution converges to a degenerate distribution at the true value of the parameter, and (iii) the posterior distribution is approximately normally distributed with mean &lt;math&gt;\hat{\theta}&lt;/math&gt;.

== History ==

The ideas underlying [[Bayesian statistics]] were developed by Rev. [[Thomas Bayes]] during the 18th century and later expanded by [[Pierre-Simon Laplace]]. As early as 1950, the potential of the Bayesian inference in econometrics was recognized by [[Jacob Marschak]].&lt;ref&gt;Marschak made this acknowledgment in a lecture, which was formalized in Marschak (1954); cf. Marschak, J. (1954). Probability in the Social Sciences. In Marschak, J. (1974). ''Economic Information, Decision, and Prediction. Selected Essays: Volume I Part I - Economics of Decision''. Amsterdam: Springer Netherlands.&lt;/ref&gt; The Bayesian approach was first applied to [[econometrics]] in the early 1960s by W. D. Fisher, [[Jacques Drèze]], [[Clifford Hildreth]], [[Thomas J. Rothenberg]], [[George Tiao]], and [[Arnold Zellner]]. The central motivation behind these early endeavors in Bayesian econometrics was the combination of the parameter estimators with available uncertain information on the model parameters that was not included in a given model formulation.&lt;ref&gt;{{cite journal |last=Qin |first=D. |year=1996 |title=Bayesian Econometrics: The First Twenty Years |journal=[[Econometric Theory]] |volume=12 |issue=3 |pages=500–516 |doi=10.1017/S0266466600006836 }}&lt;/ref&gt; From the mid-1960s to the mid-1970s, the reformulation of econometric techniques along Bayesian principles under the traditional structural approach dominated the research agenda, with Zellner's ''An Introduction to Bayesian Inference in Econometrics'' in 1971 as one of its highlights, and thus closely followed the work of frequentist econometrics. Therein, the main technical issues were the difficulty of specifying prior densities without losing either economic interpretation or mathematical tractability and the difficulty of integral calculation in the context of density functions. The result of the Bayesian reformulation program was to highlight the fragility of structural models to uncertain specification. This fragility came to motivate the work of [[Edward Leamer]], who emphatically criticized modelers' tendency to indulge in "post-data model construction" and consequently developed a method of economic modelling based on the selection of regression models according to the types of prior density specification in order to identify the prior structures underlying modelers' working rules in model selection explicitly.&lt;ref&gt;{{cite journal |first=Edward E. |last=Leamer |title=False Models and Post-Data Model Construction |journal=[[Journal of the American Statistical Association]] |volume=69 |year=1974 |issue=345 |pages=122–131 |doi=10.1080/01621459.1974.10480138 }}&lt;/ref&gt; Bayesian econometrics also became attractive to [[Christopher Sims]]' attempt to move from structural modeling to [[Vector autoregression|VAR]] modeling due to its explicit probability specification of parameter restrictions. Driven by the rapid growth of computing capacities from the mid-1980s on, the application of [[Markov chain Monte Carlo]] simulation to statistical and econometric models, first performed in the early 1990s, enabled Bayesian analysis to drastically increase its influence in economics and econometrics.&lt;ref&gt;{{cite journal |first=Gary |last=Koop |first2=Dimitris |last2=Korobilis |year=2010 |title=Bayesian Multivariate Time Series Methods for Empirical Macroeconomics |journal=Foundations and Trends in Econometrics |volume=3 |issue=4 |pages=267–358 |doi=10.1561/0800000013 |citeseerx=10.1.1.164.7962 }}&lt;/ref&gt;

== Current research topics ==

Since the beginning of the 21st century, research in Bayesian econometrics has concentrated on:&lt;ref&gt;[http://papers.tinbergen.nl/13191.pdf Basturk, N. (2013). Historical Developments in Bayesian Econometrics after Cowles Foundation Monographs 10, 14. Tinbergen Institute Discussion Paper 191/III.]&lt;/ref&gt;
* sampling methods suitable for [[parallelization]] and [[Graphics processing unit|GPU]] calculations;
* complex economic models accounting for nonlinear effects and complete predictive densities;
* analysis of implied model features and decision analysis;
* incorporation of model incompleteness in econometric analysis.

== References ==
{{reflist}}

* {{cite book |first=Gary |last=Koop |first2=Dale J. |last2=Poirier |first3=Justin L. |last3=Tobias |year=2007 |title=Bayesian Econometric Methods |publisher=Cambridge University Press |isbn=0-521-85571-3 |url=https://books.google.com/books?id=1EmjUH7fjJcC }}
* {{cite book |first=Tony |last=Lancaster |year=2004 |title=An Introduction to Modern Bayesian Econometrics |publisher=Blackwell |isbn=1-4051-1720-6 |url=https://books.google.com/books?id=PpmTkB5ns-UC }}
* {{cite book |last=Zellner |first=A. |year=1996 |title=An Introduction to Bayesian Inference in Econometrics |publisher=Wiley |edition=Reprint of 1971 |isbn=0-471-16937-4 |url=https://books.google.com/books?id=9tSrQgAACAAJ }}
* {{cite book |last=Greenberg |first=Edward |title=Introduction to Bayesian Econometrics |location= |publisher=Cambridge University Press |edition=Second |year=2012 |isbn=978-1-107-01531-9 |url=https://books.google.com/books?id=ErUAOdGWFRAC }}

{{DEFAULTSORT:Bayesian Econometrics}}
[[Category:Econometric modeling]]
[[Category:Bayesian statistics|Econometrics]]</text>
      <sha1>owjl47kwoerkw12kggp54nol5jx99qs</sha1>
    </revision>
  </page>
  <page>
    <title>Beltrami equation</title>
    <ns>0</ns>
    <id>31356616</id>
    <revision>
      <id>869403292</id>
      <parentid>869402654</parentid>
      <timestamp>2018-11-18T10:53:25Z</timestamp>
      <contributor>
        <username>LyleRamshaw</username>
        <id>21401134</id>
      </contributor>
      <minor/>
      <comment>Clarified that h(x,y) = u(x,y) + i v(x,y).</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="62321">{{distinguish|Laplace–Beltrami equation}}
In [[mathematics]], the '''Beltrami equation''', named after [[Eugenio Beltrami]], is the [[partial differential equation]]

:&lt;math&gt; {\partial w\over \partial \overline{z}} = \mu {\partial w\over \partial z}.&lt;/math&gt;

for ''w'' a complex distribution of the [[complex variable]] ''z'' in some open set ''U'', with derivatives that are locally ''L''&lt;sup&gt;2&lt;/sup&gt;, and where ''μ'' is a given complex function in ''L''&lt;sup&gt;∞&lt;/sup&gt;(''U'') of norm less than 1, called the '''Beltrami coefficient'''. Classically this differential equation was used by [[Gauss]] to prove the existence locally of [[isothermal coordinates]] on a surface with analytic Riemannian metric. Various techniques have been developed for solving the equation. The most powerful, developed in the 1950s, provides global solutions of the equation on '''C''' and relies on the L&lt;sup&gt;''p''&lt;/sup&gt; theory of the [[Beurling transform]], a [[singular integral operator]] defined on L&lt;sup&gt;''P''&lt;/sup&gt;('''C''') for all 1 &lt; ''p'' &lt; ∞. The same method applies equally well on the [[unit disk]] and [[upper half plane]] and plays a fundamental role in [[Teichmüller theory]] and the theory of [[quasiconformal mapping]]s. Various [[uniformization theorem]]s can be proved using the equation, including the [[measurable Riemann mapping theorem]] and the [[simultaneous uniformization theorem]]. The existence of [[conformal welding]]s can also be derived using the Beltrami equation. One of the simplest applications is to the [[Riemann mapping theorem]] for simply connected bounded open domains  in the complex plane. When the domain has smooth boundary, [[elliptic regularity]] for the equation can be used to show that the uniformizing map from the unit disk to the domain extends to a C&lt;sup&gt;∞&lt;/sup&gt; function from the closed disk to the closure of the domain.

==Metrics on planar domains==

Consider a 2-dimensional [[Riemannian manifold]], say with an (''x'', ''y'') coordinate system on it.  The curves of constant ''x'' on that surface typically don't intersect the curves of constant ''y'' orthogonally.  A new coordinate system (''u'', ''v'') is called [[Isothermal coordinates|isothermal]] when the curves of constant ''u'' do intersect the curves of constant ''v'' orthogonally and, in addition, the parameter spacing is the same — that is, for small enough ''h'', the little region with &lt;math&gt;a\le u\le a+h&lt;/math&gt; and &lt;math&gt;b\le v\le b+h&lt;/math&gt; is nearly square, not just nearly rectangular.  The Beltrami equation is the equation that has to be solved in order to construct isothermal coordinate systems.

To see how this works, let ''S'' be an open set in '''C''' and let

:&lt;math&gt; \displaystyle ds^2=E\,dx^2 + 2F\, dx\,dy + G\, dy^2 &lt;/math&gt;

be a smooth metric ''g'' on ''S''.  The [[first fundamental form]] of ''g''

:&lt;math&gt;\displaystyle g(x,y)=\begin{pmatrix} E &amp; F \\ F &amp; G\end{pmatrix} &lt;/math&gt;

is a positive real matrix (''E'' &gt; 0, ''G'' &gt; 0, ''EG'' − ''F''&lt;sup&gt;2&lt;/sup&gt; &gt; 0) that varies smoothly with ''x'' and ''y''.

The '''Beltrami coefficient''' of the metric ''g'' is defined to be

:&lt;math&gt;\displaystyle \mu(x,y)={E-G +2iF\over E+G +2\sqrt{EG-F^2}} &lt;/math&gt;

This coefficient has modulus strictly less than one since the identity

:&lt;math&gt;\displaystyle (E-G+2iF)(E-G-2iF)=(E+G+2\sqrt{EG-F^2})(E+G-2\sqrt{EG-F^2})&lt;/math&gt;

implies that 

:&lt;math&gt;\displaystyle |\mu|^2={E+G - 2\sqrt{EG -F^2}\over E+G + 2\sqrt{EG-F^2}}&lt;1. &lt;/math&gt;

Let ''f''(''x'',''y'') =(''u''(''x'',''y''),''v''(''x'',''y'')) be a smooth diffeomorphism of ''S'' onto another open set ''T'' in '''C'''.  The map ''f'' preserves orientation just when its [[Jacobian matrix and determinant|Jacobian]] is positive:

:&lt;math&gt;\displaystyle u_x v_y-v_x u_y&gt;0.&lt;/math&gt;

And using ''f'' to pull back to ''S'' the standard Euclidean metric ''ds''&lt;sup&gt;2&lt;/sup&gt; = ''du''&lt;sup&gt;2&lt;/sup&gt; + ''dv''&lt;sup&gt;2&lt;/sup&gt; on ''T'' induces a metric on ''S'' given by

:&lt;math&gt;\displaystyle ds^2=du^2 + dv^2=(u_x^2+v_x^2)\, dx^2 + 2 (u_xu_y + v_xv_y)\, dx \, dy + (u_y^2 + v_y^2)\, dy^2, &lt;/math&gt;

a metric whose first fundamental form is

:&lt;math&gt;\displaystyle \begin{pmatrix} u_x^2+v_x^2 &amp; u_x u_y+v_x v_y \\ u_x u_y+v_x v_y &amp; u_y^2+v_y^2\end{pmatrix}.&lt;/math&gt;

When ''f'' both preserves orientation and induces a metric that differs from the original metric ''g'' only by a positive, smoothly varying scale factor ''r''(''x'', ''y''), the new coordinates ''u'' and ''v'' defined on ''S'' by ''f'' are called '''[[isothermal coordinates]]'''. 

To determine when this happens, we reinterpret ''f'' as a complex-valued function of a complex variable ''f''(''x''+i''y'') = ''u''(''x''+i''y'') + i''v''(''x''+i''y'') so that we can apply the [[Wirtinger derivatives]]:

:&lt;math&gt;\displaystyle \partial_z= {1\over 2} (\partial_x-i\partial_y),\,\, \partial_{\overline{z}}={1\over 2} (\partial_x +i\partial_y);\,\,\,\,dz=dx+i\,dy,\,\,d\overline{z}=dx-i\,dy. &lt;/math&gt;

Since

:&lt;math&gt;\displaystyle f_z=((u_x+v_y)+i(v_x-u_y))/2&lt;/math&gt;
:&lt;math&gt;\displaystyle f_{\overline{z}}=((u_x-v_y)+i(v_x+u_y))/2,&lt;/math&gt;

the metric induced by ''f'' is given by

:&lt;math&gt;\displaystyle ds^2=|f_z \, dz + f_{\overline{z}}d\overline{z}|^2= |f_z|^2\, \left|dz + {f_{\overline{z}}\over f_z} \, d\overline{z}\right|^2. &lt;/math&gt;

The '''Beltrami quotient''' of this induced metric is defined to be &lt;math&gt;f_{\overline{z}}/f_z&lt;/math&gt;.  

The Beltrami quotient &lt;math&gt;f_{\overline{z}}/f_z&lt;/math&gt; of &lt;math&gt;f&lt;/math&gt; equals the Beltrami coefficient &lt;math&gt;\mu(z)&lt;/math&gt; of the original metric ''g'' just when

:&lt;math&gt;\displaystyle ((u_x+v_y)+i(v_x-u_y))\bigl(E-G+2iF\bigr)&lt;/math&gt;
:::&lt;math&gt;=((u_x-v_y)+i(v_x+u_y))\bigl(E+G+2\sqrt{EG-F^2}\bigr).&lt;/math&gt;

The real and imaginary parts of this identity linearly relate &lt;math&gt;u_x,&lt;/math&gt; &lt;math&gt;u_y,&lt;/math&gt; &lt;math&gt;v_x,&lt;/math&gt; and &lt;math&gt;v_y,&lt;/math&gt; and solving for &lt;math&gt;u_y&lt;/math&gt; and &lt;math&gt;v_y&lt;/math&gt; gives

:&lt;math&gt;\displaystyle u_y=\frac{F u_x-\sqrt{EG-F^2}\,v_x}{E}\quad\text{and}
\quad v_y=\frac{\sqrt{EG-F^2}\,u_x+F v_x}{E}.&lt;/math&gt;

It follows that the metric induced by ''f'' is then ''r''(''x'', ''y'') ''g''(''x'',''y''), where &lt;math&gt;r=(u_x^2+v_x^2)/E,&lt;/math&gt; which is positive, while the Jacobian of ''f'' is then &lt;math&gt;r\sqrt{EG-F^2},&lt;/math&gt; which is also positive.  So, when &lt;math&gt;f_{\overline{z}}/f_z=\mu(z),&lt;/math&gt; the new coordinate system given by ''f'' is isothermal.

Conversely, consider a diffeomorphiam ''f'' that does give us isothermal coordinates.  We then have

:&lt;math&gt;\displaystyle \mu(z)=
\frac{(u_x^2+v_x^2)-(u_y+v_y)^2+2i(u_xu_y+v_xv_y)}
{(u_x^2+v_x^2)+(u_y^2+v_y)^2+2\sqrt{(u_x^2+v_x^2)(u_y^2+v_y^2)-(u_xu_y+v_xv_y)^2}},&lt;/math&gt;

where the scale factor ''r''(''x'', ''y'') has dropped out and the expression inside the square root is the perfect square &lt;math&gt;u_x^2v_y^2-2u_xv_xu_yv_y+v_x^2u_y^2.&lt;/math&gt;  Since ''f'' must preserve orientation to give isothermal coordinates, the Jacobian &lt;math&gt;u_xv_y-v_xu_y&lt;/math&gt; is the positive square root; so we have

:&lt;math&gt;\displaystyle \mu(z)=
\frac{\bigl((u_x+iu_y)+i(v_x+iv_y)\bigr)\bigl((u_x+iu_y)-i(v_x+iv_y)\bigr)}
{\bigl((u_x+v_y)+i(v_x-u_y)\bigr)\bigl((u_x+v_y)-i(v_x-u_y)\bigr)}.&lt;/math&gt;

The right-hand factors in the numerator and denominator are equal and, since the Jacobian is positive, their common value can't be zero; so &lt;math&gt;\mu(z)=f_{\overline{z}}/f_x.&lt;/math&gt;  

Thus, the local coordinate system given by a diffeomorphism ''f'' is isothermal just when ''f'' solves the Beltrami equation for &lt;math&gt;\mu(z).&lt;/math&gt;

== Isothermal coordinates for analytic metrics ==

[[Gauss]] proved the existence of isothermal coordinates locally in the analytic case by reducing the Beltrami to an ordinary differential equation in the complex domain.&lt;ref&gt;{{harvnb|Spivak|1999|pp=314-317, which is pp. 455-460 in the first or second edition; but note that there is a typo in equation (**) on page 315 or 457.  The right-hand side, given as −β/α, should be −α/β.}}&lt;/ref&gt;  Here is a cookbook presentation of Gauss's technique.

An isothermal coordinate system, say in a neighborhood of the origin (''x'', ''y'') = (0, 0), is given by the real and imaginary parts of a complex-valued function ''f''(''x'', ''y'') that satisfies 

:&lt;math&gt;\displaystyle {f_{\overline{z}}\over f_z}=\mu(z)=\frac{E-G+2iF}{E+G+2\sqrt{EG-F^2}}. &lt;/math&gt;

Let &lt;math&gt;f&lt;/math&gt; be such a function, and let &lt;math&gt;\psi&lt;/math&gt; be a complex-valued function of a complex variable that is [[holomorphic function|holomorphic]] and whose derivative is nowhere zero.  Since any holomorphic function &lt;math&gt;\psi&lt;/math&gt; has &lt;math&gt;\psi_{\overline{z}}&lt;/math&gt; identically zero, we have

:&lt;math&gt;
\begin{align}
\frac{(\psi\circ f)_{\overline{z}}}{(\psi\circ f)_z} &amp; = \frac{(\psi_z\circ f)f_{\overline{z}}+(\psi_{\overline{z}}\circ f)\overline{f_{\overline{z}}}}{(\psi_z\circ f)f_z+(\psi_{\overline{z}}\circ f)\overline{f_z}}
=\frac{f_{\overline{z}}}{f_z}=\mu(z).
\end{align}
&lt;/math&gt;

Thus, the coordinate system given by the real and imaginary parts of &lt;math&gt;\psi\circ f&lt;/math&gt; is also isothermal.  Indeed, if we fix &lt;math&gt;f&lt;/math&gt; to give one isothermal coordinate system, then all of the possible isothermal coordinate systems are given by &lt;math&gt;\psi\circ f&lt;/math&gt; for the various holomorphic &lt;math&gt;\psi&lt;/math&gt; with nonzero derivative.

When ''E'', ''F'', and ''G'' are real analytic, Gauss constructed a particular isothermal coordinate system &lt;math&gt;h(x,y)=u(x,y)+iv(x,y),&lt;/math&gt; the one that he chose being the one with  &lt;math&gt;h(0,y)=iy&lt;/math&gt; for all ''y''.  So the ''v'' (imaginary) axis of his isothermal coordinate system coincides with the ''y'' axis of the original coordinates and is parameterized in the same way.  All other isothermal coordinate systems are then of the form &lt;math&gt;\psi\circ h&lt;/math&gt; for a holomorphic &lt;math&gt;\psi&lt;/math&gt; with nonzero derivative.  (If you would rather have the isothermal coordinate system &lt;math&gt;k(x,y)=u(x,y)+iv(x,y)&lt;/math&gt; with &lt;math&gt;k(x,0)=x&lt;/math&gt; for all ''x'', so that your ''u'' (real) axis coincides with the original ''x'' axis, you can swap ''x'' with ''y'', apply the following, and then swap ''u'' with ''v''.)

Gauss lets ''q''(''t'') be some complex-valued function of a real variable ''t'' that satisfies the following ordinary differential equation:

:&lt;math&gt;\displaystyle q'(t)=\frac{-F+i\sqrt{EG-F^2}}{G}(t,q(t)),&lt;/math&gt;

where ''E'', ''F'', and ''G'' are here evaluated at ''x'' = ''t'' and ''y'' = ''q''(''t'').  If we specify the value of ''q''(''s'') for some start value ''s'', this differential equation determines the values of ''q''(''t'') for ''t'' either less than or greater than ''s''.  Gauss then defines his isothermal coordinate system ''h'' by setting ''h''(''x'', ''y'') to be &lt;math&gt;iq(0)&lt;/math&gt; along the solution path of that differential equation that passes through the point (''x'', ''y''), and thus has ''q''(''x'') = ''y''.

This rule sets ''h''(0, ''y'') to be &lt;math&gt;iy&lt;/math&gt;, since the starting condition is then ''q''(0)=''y''.  More generally, suppose that we move by an infinitesimal vector (''dx'', ''dy'') away from some point (''x'', ''y''), where ''dx'' and ''dy'' satisfy

:&lt;math&gt;\displaystyle (F-i\sqrt{EG-F^2})\,dx+G\,dy=0.&lt;/math&gt;

Since &lt;math&gt;q'(t)=dy/dx&lt;/math&gt;, the vector (''dx'', ''dy'') is then tangent to the solution curve of the differential equation that passes through the point (''x'', ''y'').  Because we are assuming the metric to be analytic, it follows that 

:&lt;math&gt;\displaystyle dh =\lambda\bigl((F-i\sqrt{EG-F^2})\,dx+G\,dy\bigr)&lt;/math&gt;

for some smooth, complex-valued function &lt;math&gt;\lambda(x,y).&lt;/math&gt;  We thus have

:&lt;math&gt;\displaystyle h_z=\lambda(F-i\sqrt{EG-F^2}-iG)/2&lt;/math&gt;
:&lt;math&gt;\displaystyle h_{\overline{z}}=\lambda(F-i\sqrt{EG-F^2}+iG)/2,&lt;/math&gt;

from which it follows that 

:&lt;math&gt;
\begin{align}
{h_{\overline{z}}\over h_z} &amp; = \frac{(F-i\sqrt{EG-F^2}+iG)(F+i\sqrt{EG-F^2}+iG)}
{(F-i\sqrt{EG-F^2}-iG)(F+i\sqrt{EG-F^2}+iG)} \\[6pt]
&amp; =\frac{E-G+2iF}{E+G+2\sqrt{EG-F^2}} =\mu(z).
\end{align}
&lt;/math&gt;

Gauss's function ''h'' thus gives the desired isothermal coordinates.

==Solution in ''L''&lt;sup&gt;2&lt;/sup&gt; for smooth Beltrami coefficients==
In the simplest cases the Beltrami equation can be solved only Hilbert space techniques and the Fourier transform. The method of proof is the prototype for the general solution using L&lt;sup&gt;''p''&lt;/sup&gt; spaces, although [[Adrien Douady]] has indicated a method for handling the general case using only Hilbert spaces: the method relies on the classical theory of [[quasiconformal mapping]]s to establish Hölder estimates that are automatic in the L&lt;sup&gt;''p''&lt;/sup&gt; theory for ''p'' &gt; 2.&lt;ref&gt;See:
*{{harvnb|Astala|Iwaniec|Martin|2009}}
*{{harvnb|Bers|John|Schechter|1979}}
*{{harvnb|Ahlfors|1966}}
*{{harvnb|Glutsyuk|2008}}
*{{harvnb|Douady|Buff|2000}}&lt;/ref&gt;
Let ''T'' be the [[Beurling transform]] on L&lt;sup&gt;2&lt;/sup&gt;('''C''') defined on the Fourier transform of an L&lt;sup&gt;2&lt;/sup&gt; function ''f'' as a multiplication operator:

:&lt;math&gt;\displaystyle \widehat{Tf}(z)= {\overline{z}\over z} \widehat{f}(z). &lt;/math&gt;

It is a unitary operator and if ''h'' is a tempered distribution on '''C''' with partial derivatives in
L&lt;sup&gt;2&lt;/sup&gt; then

:&lt;math&gt;\displaystyle T(f_{\overline{z}})=f_z, &lt;/math&gt;

where the subscripts denote complex partial derivatives.

The [[fundamental solution]] of the operator

:&lt;math&gt;D=\partial_{\overline{z}}&lt;/math&gt;

is given by the distribution

:&lt;math&gt;\displaystyle{E(z) ={1\over \pi z},}&lt;/math&gt;

a locally integrable function on '''C'''. Thus on [[Schwartz function]]s ''f''

:&lt;math&gt;\displaystyle{\partial_{\overline{z}}(E\star f) = f.}&lt;/math&gt;

The same holds for distributions of compact support on '''C'''. In particular if ''f'' is an L&lt;sup&gt;2&lt;/sup&gt; function with compact support, then its '''Cauchy transform''', defined as

:&lt;math&gt;\displaystyle{Cf=E\star f,}&lt;/math&gt;

is locally square integrable. The above equation can be written

:&lt;math&gt;\displaystyle{(Cf)_{\overline{z}}=f.}&lt;/math&gt;

Moreover, still regarding ''f'' and ''Cf'' as distributions,

:&lt;math&gt;\displaystyle (Cf)_z= Tf. &lt;/math&gt;

Indeed, the operator ''D'' is given on Fourier transforms as multiplication by ''iz''/2 and ''C'' as multiplication by its inverse.

Now in the Beltrami equation

:&lt;math&gt;\displaystyle f_{\overline{z}} = \mu f_z, &lt;/math&gt;

with ''μ'' a smooth function of compact support, set

:&lt;math&gt;\displaystyle g(z) = f(z) -z &lt;/math&gt;

and assume that the first derivatives of ''g'' are L&lt;sup&gt;2&lt;/sup&gt;. Let ''h'' = ''g''&lt;sub&gt;''z''&lt;/sub&gt; = ''f''&lt;sub&gt;''z''&lt;/sub&gt; – 1. Then

:&lt;math&gt;\displaystyle h=g_z=T(g_{\overline{z}})= T(f_{\overline{z}})=T(\mu f_z)=T(\mu h) + T\mu &lt;/math&gt;

If ''A'' and ''B'' are the operators defined by

:&lt;math&gt;\displaystyle{AF=T\mu F,\,\,\,\, BF =\mu TF}&lt;/math&gt;

then their operator norms are strictly less that 1 and

:&lt;math&gt;\displaystyle{(I-A)h = T\mu.}&lt;/math&gt;

Hence

:&lt;math&gt;\displaystyle{h=(I-A)^{-1}T\mu,\,\,\, T^*h=(I-B)^{-1}\mu}&lt;/math&gt;

where the right hand sides can be expanded as [[Neumann series]]. It follows that

:&lt;math&gt;\displaystyle{g_{\overline{z}}=T^*h,}&lt;/math&gt;

has the same support as ''μ'' and ''g''. Hence ''f'' is given by

:&lt;math&gt;\displaystyle{f(z) = CT^*h(z) + z.}&lt;/math&gt;

[[Elliptic regularity]] can now be used to deduce that ''f'' is smooth.

In fact, off the support of ''μ'',

:&lt;math&gt;\displaystyle \partial_{\overline{z}} f =0, &lt;/math&gt;

so by [[Weyl's lemma (Laplace equation)|Weyl's lemma]] ''f'' is even holomorphic for |''z''| &gt; ''R''. Since ''f'' = ''CT*h'' + ''z'', it follows that
''f'' tends to 0 uniformly as |''z''| tends to ∞.

The elliptic regularity argument to prove smoothness, however, is the same everywhere and uses the theory of L&lt;sup&gt;2&lt;/sup&gt; Sobolev spaces on the torus.&lt;ref&gt;See:
*{{harvnb|Bers|John|Schechter|1979}}
*{{harvnb|Glutsyuk|2008}}&lt;/ref&gt; Let ψ be a smooth function of compact support on '''C''', identically equal to 1 on a neighbourhood of the support of ''μ'' and set ''F'' = ''ψ'' ''f''. The support of ''F'' lies in a large square |''x''|, |''y''| ≤ ''R'', so, identifying opposite sides of the square, ''F'' and ''μ'' can be regarded as a distribution and smooth function on a torus '''T'''&lt;sup&gt;2&lt;/sup&gt;. By construction ''F'' is in ''L''&lt;sup&gt;2&lt;/sup&gt;('''T'''&lt;sup&gt;2&lt;/sup&gt;). As a distribution on '''T'''&lt;sup&gt;2&lt;/sup&gt; it satisfies

:&lt;math&gt;\displaystyle F_{\overline{z}}= \mu F_{z} + GF, &lt;/math&gt;

where ''G'' is smooth. On the canonical basis ''e''&lt;sub&gt;''m''&lt;/sub&gt; of L&lt;sup&gt;2&lt;/sup&gt;('''T'''&lt;sup&gt;2&lt;/sup&gt;) with ''m'' in '''Z''' + ''i'' '''Z''', define

:&lt;math&gt;\displaystyle{Ue_m={\overline{m}\over m}e_m \,\, (m\ne 0), Ue_0=e_0.}&lt;/math&gt;

Thus ''U'' is a unitary and on trigonometric polynomials or smooth functions ''P''

:&lt;math&gt;\displaystyle{U(P_{\overline{z}})=P_z.}&lt;/math&gt;

Similarly it extends to a unitary on each [[Sobolev space]] H&lt;sub&gt;''k''&lt;/sub&gt;('''T'''&lt;sup&gt;2&lt;/sup&gt;) with the same property. It is the counterpart on the torus of the Beurling transform. The standard theory of [[Fredholm operator]]s shows that the operators corresponding to ''I'' – ''μ'' ''U'' and ''I'' – ''U'' ''μ'' are invertible on each Sobolev space. On the other hand,

:&lt;math&gt;\displaystyle{\partial_{z}(I-U\mu)F= UG.}&lt;/math&gt;

Since ''UG'' is smooth, so too is (''I'' – ''μU'')''F'' and hence also ''F''.

Thus the original function ''f'' is smooth. Regarded as a map of '''C''' = '''R'''&lt;sup&gt;2&lt;/sup&gt; into itself, the Jacobian is given by

:&lt;math&gt;\displaystyle{J(f)=|f_z|^2-|f_{\overline{z}}|^2=|f_z|^2(1-|\mu|^2).}&lt;/math&gt;

This Jacobian is nowhere vanishing by a classical argument of {{harvtxt|Ahlfors|1966}}. In fact formally writing
''f''&lt;sub&gt;''z''&lt;/sub&gt; = ''e''&lt;sup&gt;''k''&lt;/sup&gt;, it follows that

:&lt;math&gt;\displaystyle{k_{\overline{z}} = \mu k_{z} + \mu_{z}.}&lt;/math&gt;

This equation for ''k'' can be solved by the same methods as above giving a solution tending to 0 at ∞.
By uniqueness ''h'' + 1 = ''e''&lt;sup&gt;''k''&lt;/sup&gt; so that

:&lt;math&gt;\displaystyle{J(f)=(1-|\mu|^2)|e^{2 k}|}&lt;/math&gt;

is nowhere vanishing. Since ''f'' induces a smooth map of the Riemann sphere '''C''' ∪ ∞ into itself which is locally a diffeomorphism, ''f'' must be a diffeomorphism. In fact ''f'' must be onto by connectedness of the sphere, since its image is an open and closed subset; but then, as a [[covering map]], ''f'' must cover each point of the sphere the same number of times. Since only ∞ is sent to ∞, it follows that ''f'' is one-to-one.

The solution ''f'' is a quasiconformal conformal diffeomorphism. These form a group and their Beltrami coefficients can be computed according to the following rule:&lt;ref&gt;See:
*{{harvnb|Ahlfors|1966|p=9}}
*{{harvnb|Imayoshi|Taniguchi|1992|p=88}}&lt;/ref&gt;

:&lt;math&gt;\displaystyle{\mu_{g\circ f^{-1}}\circ f={f_z\over \overline{f_z}} {\mu_g-\mu_f\over 1 -\overline{\mu_f}\mu_g}.}&lt;/math&gt;

Moreover, if ''f''(0) = 0 and

:&lt;math&gt;\displaystyle{g(z)=f(z^{-1})^{-1},}&lt;/math&gt;

then&lt;ref&gt;{{harvnb|Ahlfors|1966|p=98}}&lt;/ref&gt;

:&lt;math&gt;\displaystyle{\mu_g(z)={z^2\over \overline{z}^2} \mu_f(z^{-1}).}&lt;/math&gt;

This formula reflects the fact that on a [[Riemann surface]], a Beltrami coefficient is not a function.
Under a holomorphic change of coordinate ''w'' = ''w''(''z''), the coefficient is transformed to

:&lt;math&gt;\displaystyle{\widetilde{\mu}(w)= (w^\prime/ \overline{w^\prime})\cdot\mu(z).}&lt;/math&gt;

Defining a smooth Beltrami coefficient on the sphere in this way, if ''μ'' is such a coefficient then, taking a smooth [[bump function]] ψ equal to 0 near 0, equal 1 for |''z''| &gt; 1 and satisfying 0 ≤ ''ψ'' ≤ 1, ''μ'' can be written as a sum of two Beltrami coefficients:

:&lt;math&gt;\displaystyle{\mu=\psi\mu + (1-\psi) \mu=\mu_0 + \mu_\infty.}&lt;/math&gt;

Let ''g'' be the quasiconformal diffeomorphism of the sphere fixing 0 and ∞ with coefficient 
''μ''&lt;sub&gt;∞&lt;/sub&gt;. Let λ be the Beltrami coefficient of compact support on '''C''' defined by

:&lt;math&gt;\displaystyle \lambda(z)= \left[{\mu_0 \over 1 -\mu\overline{\mu_\infty}}     {g_z\over\overline{g_z}}\right] \circ g^{-1}(z). &lt;/math&gt;

If ''f'' is the quasiconformal diffeomorphism of the sphere fixing 0 and ∞ with coefficient λ, then
the transformation formulas above show that ''f'' ∘ ''g''&lt;sup&gt;−1&lt;/sup&gt; is a quasiconformal diffeomorphism of the sphere fixing 0 and ∞ with coefficient ''μ''.

The solutions of Beltrami's equation restrict to diffeomorphisms of the upper halfplane or unit disk if the coefficient ''μ'' has extra symmetry properties;&lt;ref&gt;See
*{{harvnb|Ahlfors|1966|p=99}}
*{{harvnb|Bers|John|Schechter|1979|p=277}}
&lt;/ref&gt; since the two regions are related by a Möbius transformation (the Cayley transform), the two cases are essentially the same.

For the upper halfplane Im ''z'' &gt; 0, if ''μ'' satisfies

:&lt;math&gt; \displaystyle \mu(z)=\overline{\mu(\overline{z})}, &lt;/math&gt;

then by uniqueness the solution ''f'' of the Beltrami equation satisfies

:&lt;math&gt;\displaystyle f(z)=\overline{f(\overline{z})}, &lt;/math&gt;

so leaves the real axis and hence the upper halfplane invariant.

Similarly for the unit disc |''z''| &lt; 1, if ''μ'' satisfies

:&lt;math&gt;\displaystyle mu(z)={z^2\over \overline{z}^2}\overline{\mu(\overline{z}^{-1})}, &lt;/math&gt;

then by uniqueness the solution ''f'' of the Beltrami equation satisfies

:&lt;math&gt;\displaystyle{f(z)=\overline{f(\overline{z}^{-1})}^{-1},}&lt;/math&gt;

so leaves the unit circle and hence the unit disk invariant.

Conversely Beltrami coefficients defined on the closures of the upper halfplane or unit disk which satisfy these conditions on the boundary can be "reflected" using the formulas above. If the extended functions are smooth the preceding theory can be applied. Otherwise the extensions will be continuous but with a jump in the derivatives at the boundary. In that case the more general theory for measurable coefficients ''μ'' is required and is most directly handled within the L&lt;sup&gt;''p''&lt;/sup&gt; theory.

==Smooth Riemann mapping theorem==
{{See also|Riemann mapping theorem}}
Let ''U'' be an open simply connected domain in the complex plane with smooth boundary containing 0 in its interior and let ''F'' be a diffeomorphism of the unit disk ''D'' onto ''U'' extending smoothly to the boundary and the identity on a neighbourhood of 0. Suppose that in addition the induced metric on the closure of the unit disk can be reflected in the unit circle to define a smooth metric on '''C'''. The corresponding Beltrami coefficient is then a smooth function on '''C''' vanishing near 0 and ∞ and satisfying

:&lt;math&gt;\displaystyle \mu(z)={z^2\over \overline{z}^2}\overline{\mu(\overline{z}^{-1})}^{-1}. &lt;/math&gt;

The quasiconformal diffeomorphism ''h'' of '''C''' satisfying

:&lt;math&gt;\displaystyle{h_{\overline{z}}=\mu(z) h_z}&lt;/math&gt;

preserves the unit circle together with its interior and exterior. From the composition formulas for Beltrami coefficients

:&lt;math&gt;\displaystyle{\mu_{F\circ h^{-1}}=0,}&lt;/math&gt;

so that ''f'' = ''F''∘ ''h''&lt;sup&gt;−1&lt;/sup&gt; is a smooth diffeomorphism between the closures of ''D'' and ''U'' which is holomorphic on the interior. Thus, if a suitable diffeomorphism ''F'' can be constructed, the mapping ''f'' proves the smooth [[Riemann mapping theorem]] for the domain ''U''.

To produce a diffeomorphism ''F'' with the properties above, it can be assumed after an affine transformation that the boundary of ''U'' has length 2π and that 0 lies in ''U''. The smooth version of the [[Schoenflies theorem]] produces a smooth diffeomorphism ''G'' from the closure of ''D'' onto the closure of ''u'' equal to the identity on a neighbourhood of 0 and with an explicit form on a tubular neighbourhood of the unit circle. In fact taking polar coordinates (''r'',''θ'') in ''R''&lt;sup&gt;2&lt;/sup&gt; and letting (''x''(''θ''),''y''(''θ'')) (''θ'' in [0,2{{pi}}]) be a parametrization of ∂''U'' by arclength, ''G'' has the form

:&lt;math&gt;\displaystyle G(r,\theta)=(x(\theta)-(1-r)y^\prime(\theta),y(\theta) +(1-r)x^\prime(\theta)). &lt;/math&gt;

Taking ''t'' = 1 − ''r'' as parameter, the induced metric near the unit circle is given by

:&lt;math&gt; \displaystyle ds^2=(1+t\kappa(\theta))^2 d\theta^2 + dt^2, &lt;/math&gt;

where

:&lt;math&gt;\displaystyle \kappa(\theta)=y^{\prime\prime}(\theta)x^\prime(\theta) - x^{\prime\prime}(\theta) y^\prime(\theta) &lt;/math&gt;

is the [[radius of curvature (mathematics)|curvature]] of the [[plane curve]] (''x''(''θ''),''y''(''θ'')).

Let

: &lt;math&gt;\displaystyle \alpha={1\over 2\pi}\int_0^{2\pi} \kappa(\theta)\, d\theta,\,\,\, h(\theta)=\kappa(\theta) -\alpha. &lt;/math&gt;

After a change of variable in the ''t'' coordinate and a conformal change in the metric, the metric takes the form

:&lt;math&gt;\displaystyle ds^2 =(1 + t\psi(t)h(\theta))^2 d\theta^2 + dt^2, &lt;/math&gt;

where ψ is an analytic real-valued function of ''t'':

:&lt;math&gt;\displaystyle \psi(t)=1 +a_1t +a_2 t^2 + \cdots &lt;/math&gt;

A formal diffeomorphism sending (''θ'',''t'') to (''f''(''θ'',''t''),''t'') can be defined as a formal power series in ''t'':

:&lt;math&gt;\displaystyle f(\theta,t)=\theta + f_1(\theta)t + f_2(\theta)t^2 + \cdots=\theta + g(\theta,t) &lt;/math&gt;

where the coefficients ''f''&lt;sub&gt;''n''&lt;/sub&gt; are smooth functions on the circle. These coefficients can be defined by recurrence so that the transformed metric only has even powers of ''t'' in the coefficients. This condition is imposed by demanding that no odd powers of ''t'' appear in the formal power series expansion:

:&lt;math&gt; \left[1+t\psi(t)\left(\sum_{n\ge 0} h^{(n)} g^n/n!\right)\right] \cdot \left[(1+g^\prime) \, d\theta + \left(\sum_{n\ge 1} n f_n t^{n-1}\right) \, dt\right]. &lt;/math&gt;

By [[Borel's lemma]], there is a diffeomorphism defined in a neighbourhood of the unit circle, ''t'' = 0, for which the formal expression ''f''(''θ'',''t'') is the Taylor series expansion in the ''t'' variable. It follows that, after composing with this diffeomorphism, the extension of the metric obtained by reflecting in the line ''t'' = 0 is smooth.

==Hölder continuity of solutions==
Douady and others have indicated ways to extend the ''L''&lt;sup&gt;2&lt;/sup&gt; theory to prove the existence and uniqueness of solutions when the Beltrami coefficient ''μ'' is bounded and measurable with ''L''&lt;sup&gt;∞&lt;/sup&gt; norm ''k'' strictly less than one. Their approach involved the theory of quasiconformal mappings to establish directly the solutions of Beltrami's equation when ''μ'' is smooth with fixed compact support are uniformly [[Hölder continuous]].&lt;ref&gt;See:
*{{harvnb|Douady|Buff|2000}}
*{{harvnb|Glutsyuk|2008}}
*{{harvnb|Ahlfors|1966}}&lt;/ref&gt; In the L&lt;sup&gt;''p''&lt;/sup&gt; approach Hölder continuity follows automatically from operator theory.

The ''L''&lt;sup&gt;''p''&lt;/sup&gt; theory when ''μ'' is smooth of compact support proceeds as in the L&lt;sup&gt;2&lt;/sup&gt; case. By the [[Singular integral operators of convolution type|Calderón–Zygmund theory]] the Beurling transform and its inverse are known to be continuous for the L&lt;sup&gt;''p''&lt;/sup&gt; norm. The [[Riesz–Thorin theorem|Riesz–Thorin convexity theorem]] implies that the norms ''C''&lt;sup&gt;''p''&lt;/sup&gt; are continuous functions of ''p''. In particular ''C''&lt;sub&gt;''p''&lt;/sub&gt; tends to 1 when ''p'' tends to 2.

In the Beltrami equation

:&lt;math&gt;\displaystyle{f_{\overline{z}} = \mu f_z,}&lt;/math&gt;

with ''μ'' a smooth function of compact support, set

:&lt;math&gt;\displaystyle g(z) = f(z) -z &lt;/math&gt;

and assume that the first derivatives of ''g'' are L&lt;sup&gt;''p''&lt;/sup&gt;. Let ''h'' = ''g''&lt;sub&gt;''z''&lt;/sub&gt; = ''f''&lt;sub&gt;''z''&lt;/sub&gt; – 1. Then

:&lt;math&gt;\displaystyle{h=g_z=T(g_{\overline{z}})= T(f_{\overline{z}})=T(\mu f_z)=T(\mu h) + T\mu.}&lt;/math&gt;

If ''A'' and ''B'' are the operators defined by ''AF'' = ''TμF'' and ''BF'' = ''μTF'', then their operator norms are strictly less that 1 and  (''I'' − ''A'')''h'' = ''T''μ. Hence

:&lt;math&gt;\displaystyle h=(I-A)^{-1}T\mu,\,\,\, T^{-1}=(I-B)^{-1}\mu &lt;/math&gt;

where the right hand sides can be expanded as [[Neumann series]]. It follows that

:&lt;math&gt;\displaystyle g_{\overline{z}}=T^{-1}h, &lt;/math&gt;

has the same support as ''μ'' and ''g''. Hence, up to the addition of a constant, ''f'' is given by

:&lt;math&gt;\displaystyle f(z) = CT^{-1}h(z) + z. &lt;/math&gt;

Convergence of functions with fixed compact support in the L&lt;sup&gt;''p''&lt;/sup&gt; norm for ''p'' &gt; 2 implies convergence in
L&lt;sup&gt;2&lt;/sup&gt;, so these formulas are compatible with the L&lt;sup&gt;2&lt;/sup&gt; theory if ''p'' &gt; 2.

The Cauchy transform ''C'' is not continuous on L&lt;sup&gt;2&lt;/sup&gt; except as a map into functions of [[vanishing mean oscillation]].
&lt;ref&gt;{{harvnb|Astala|Iwaniec|Martin|2009}}&lt;/ref&gt; On L&lt;sup&gt;''p''&lt;/sup&gt; its image is contained in Hölder continuous functions with Hölder exponent 1 − 2''p''&lt;sup&gt;−1&lt;/sup&gt; once a suitable constant is added. In fact for a function ''f'' of compact support define

:&lt;math&gt;
\begin{align}
Pf(w) &amp; =Cf(w) +\pi ^{-1} \iint_{\mathbf{C}} {f(z)\over z} \, dx \, dy \\[4pt]
&amp; = -{1\over \pi} \iint_{\mathbf{C}} f(z)\left({1\over z-w} -{1\over z}\right)\, dx \, dy= -{1\over \pi}\iint_{\mathbf{C}} {f(z)w\over z(z-w)}\, dx \, dy.
\end{align}
&lt;/math&gt;

Note that the constant is added so that ''Pf''(0)&amp;nbsp;=&amp;nbsp;0. Since ''Pf'' only differs from ''Cf'' by a constant, it follows exactly as in the ''L''&lt;sup&gt;2&lt;/sup&gt; theory that

:&lt;math&gt;\displaystyle (Pf)_{\overline{z}} = f,\,\,\, (Pf)_z = Tf.&lt;/math&gt;

Moreover, ''P'' can be used instead of ''C'' to produce a solution:

:&lt;math&gt;\displaystyle f(z)=PT^{-1}h(z) +z.&lt;/math&gt;

On the other hand, the integrand defining ''Pf'' is in L&lt;sup&gt;''q''&lt;/sup&gt; if ''q''&lt;sup&gt;−1&lt;/sup&gt; = 1 − ''p''&lt;sup&gt;−1&lt;/sup&gt;. The [[Hölder inequality]] implies that ''Pf'' is [[Hölder continuous]] with an explicit estimate:

:&lt;math&gt;\displaystyle |Pf(w_1) - Pf(w_2)| \le K_p \|f\|_p|w_1-w_2|^{1-2/p},&lt;/math&gt;

where

:&lt;math&gt;\displaystyle K_p= \|z^{-1}(z-1)^{-1}\|_q/\pi.&lt;/math&gt;

For any ''p'' &gt; 2 sufficiently close to 2, ''C''&lt;sub&gt;''p''&lt;/sub&gt;''k'' &lt;1. Hence the Neumann series for (''I'' − ''A'')&lt;sup&gt;−1&lt;/sup&gt; and (''I'' − ''B'')&lt;sup&gt;−1&lt;/sup&gt; converge. The Hölder estimates for ''P'' yield the following uniform estimates for the normalized solution of the Beltrami equation:

:&lt;math&gt;\displaystyle |f(w_1)-f(w_2)|\le {K_p\over 1 -\|\mu\|_\infty C_p} \|\mu\|_p|w_1-w_2|^{1-2/p} + |w_1 -w_2|. &lt;/math&gt;

If ''μ'' is supported in |''z''| ≤ ''R'', then

:&lt;math&gt;\displaystyle \|\mu\|_p\le (\pi R^2)^{1/p} \|\mu\|_\infty. &lt;/math&gt;

Setting ''w''&lt;sub&gt;1&lt;/sub&gt; = ''z'' and ''w''&lt;sub&gt;2&lt;/sub&gt; = 0, it follows that for |''z''|  ≤ ''R''

:&lt;math&gt;\displaystyle |f(z)| \le   \left[ 1+ {K_p\pi^{1/p} \|\mu\|_\infty\over 1 -\|\mu\|_\infty C_p}\right]\cdot R = CR, &lt;/math&gt;

where the constant ''C'' &gt; 0 depends only on the L&lt;sup&gt;∞&lt;/sup&gt; norm of ''μ''. So the Beltrami coefficient of ''f''&lt;sup&gt;−1&lt;/sup&gt; is smooth and supported in
''z''| ≤ ''CR''. It has the same L&lt;sup&gt;∞&lt;/sup&gt; norm as that of ''f''. So the inverse diffeomorphisms also satisfy uniform Hölder estimates.

==Solution for measurable Beltrami coefficients==

===Existence===
The theory of the Beltrami equation can be extended to measurable Beltrami coefficients ''μ''. For simplicity only a special class of ''μ'' will be considered—adequate for most applications—namely those functions which are smooth an open set Ω (the regular set) with complement Λ a closed set of measure zero (the singular set). Thus Λ is a closed set that is contained in open sets of arbitrarily small area. For measurable Beltrami coefficients ''μ'' with compact support in |''z''| &lt; ''R'', the solution of the Beltrami equation can be obtained as a limit of solutions for smooth Beltrami coefficients.&lt;ref&gt;See:
*{{harvnb|Ahlfors|1966}}
*{{harvnb|Douady|Buff|2000}}&lt;/ref&gt;

In fact in this case the singular set Λ is compact. Take smooth functions φ&lt;sub&gt;''n''&lt;/sub&gt; of compact support with 0 ≤ φ&lt;sub&gt;''n''&lt;/sub&gt; ≤ 1, equal to 1 on a neighborhood of Λ and 0 off a slightly larger neighbourhood, shrinking to Λ as ''n'' increases. Set

:&lt;math&gt;\displaystyle{\mu_n=(1-\varphi_n)\cdot\mu.}&lt;/math&gt;

The ''μ''&lt;sub&gt;''n''&lt;/sub&gt; are smooth with compact support in |''z''| &lt; ''R'' and

:&lt;math&gt;\displaystyle{\|\mu_n\|_\infty \le \|\mu\|_\infty.}&lt;/math&gt;

The ''μ''&lt;sub&gt;''n''&lt;/sub&gt; tend to ''μ'' in any ''L''&lt;sup&gt;''p''&lt;/sup&gt; norm with ''p'' &lt; ∞.

The corresponding normalised solutions ''f''&lt;sub&gt;''n''&lt;/sub&gt; of the Beltrami equations and their inverses ''g''&lt;sub&gt;''n''&lt;/sub&gt; satisfy uniform Hölder estimates. They are therefore [[equicontinuous]] on any compact subset of '''C'''; they are even holomorphic for |''z''| &gt; ''R''. So by the [[Arzelà–Ascoli theorem]], passing to a subsequence if necessary, it can be assumed that both ''f''&lt;sub&gt;''n''&lt;/sub&gt; and ''g''&lt;sub&gt;''n''&lt;/sub&gt; converge uniformly on compacta to ''f'' and ''g''. The limits will satisfy the same Hölder estimates and be holomorphic for |''z''| &gt; ''R''. The relations ''f''&lt;sub&gt;''n''&lt;/sub&gt;∘''g''&lt;sub&gt;''n''&lt;/sub&gt; = id = ''g''&lt;sub&gt;''n''&lt;/sub&gt;∘''f''&lt;sub&gt;''n''&lt;/sub&gt; imply that in the limit ''f''∘''g'' = id = ''g''∘''f'', so that ''f'' and ''g'' are homeomorphisms.

*The limits ''f'' and ''g'' are weakly differentiable.&lt;ref&gt;{{harvnb|Douady|Buff|2000|pp=319–320}}&lt;/ref&gt; In fact let

::&lt;math&gt;\displaystyle{u_n=\partial_{\overline{z}} f_n,\,\, v_n =\partial_z f_n-1.}&lt;/math&gt;

:These lie in L&lt;sup&gt;p&lt;/sup&gt; and are uniformly bounded:

::&lt;math&gt;\displaystyle{\|u_n\|_p,\,\, \|v_n\|_p \le {\|\mu_n\|_p\over 1 - \|\mu_n\|_\infty}.}&lt;/math&gt;

:Passing to a subsequence if necessary, it can be assumed that the sequences have weak limits ''u'' and ''v'' in L&lt;sup&gt;p&lt;/sup&gt;. These are the distributional derivatives of ''f''(''z'') – ''z'', since if ψ is smooth of compact support

::&lt;math&gt;\displaystyle{\iint u \cdot \psi =\lim\iint u_n\cdot \psi =-\lim \iint f_n \cdot \partial_{\overline {z}} \psi = -\iint f\cdot   \partial_{\overline {z}} \psi,}&lt;/math&gt;

:and similarly for ''v''. A similar argument applies for the ''g'' using the fact that Beltrami coefficients of the ''g''&lt;sub&gt;''n''&lt;/sub&gt; are supported in a fixed closed disk.

*''f'' satisfies the Beltrami equation with Beltrami coefficient ''μ''.&lt;ref&gt;{{harvnb|Douady|Buff|2000|pp=319–320}}&lt;/ref&gt; In fact the relation ''u'' = ''μ'' ⋅ ''v'' + ''μ'' follows by continuity from the relation ''u''&lt;sub&gt;''n''&lt;/sub&gt; = ''μ''&lt;sub&gt;''n''&lt;/sub&gt; ⋅ ''v''&lt;sub&gt;''n''&lt;/sub&gt; + ''μ''&lt;sub&gt;''n''&lt;/sub&gt;. It suffices to show that ''μ''&lt;sub&gt;''n''&lt;/sub&gt; ⋅ ''v''&lt;sub&gt;''n''&lt;/sub&gt; tends weakly to ''μ'' ⋅ ''v''. The difference can be written

::&lt;math&gt;\displaystyle{\mu v -\mu_n v_n= \mu(v-v_n) +(\mu-\mu_n)v_n.}&lt;/math&gt;

:The first term tends weakly to 0, while the second term equals ''μ φ''&lt;sub&gt;''n''&lt;/sub&gt; ''v''&lt;sub&gt;''n''&lt;/sub&gt;. The terms are uniformly bounded in ''L''&lt;sup&gt;p&lt;/sup&gt;, so to check weak convergence to 0 it enough to check inner products with a dense subset of ''L''&lt;sup&gt;2&lt;/sup&gt;. The inner products with functions of  compact support in Ω are zero for ''n'' sufficiently large.

*''f'' carries closed sets of measure zero onto closed sets of measure zero.&lt;ref&gt;{{harvnb|Ahlfors|1966|pp=97–98}}&lt;/ref&gt; It suffices to check this for a compact set ''K'' of measure zero. If ''U'' is a bounded open set containing ''K'' and ''J'' denotes the Jacobian of a function, then

:: &lt;math&gt;
\begin{align}
A(f_n(U)) &amp; = \iint_U J(f_n)\, dx\, dy =\iint_U |\partial_z f_n|^2 -|\partial_{\overline{z}}f_n|^2\, dx\,dy \\[4pt]
&amp; \le \iint_U  |\partial_z f_n|^2\, dx\, dy\le \|\partial_z f_n|_U\|_p^2 \,A(U)^{1-2/p}.
\end{align}
&lt;/math&gt;

:Thus if ''A''(''U'') is small, so is ''A''(''f''&lt;sub&gt;''n''&lt;/sub&gt;(''U'')). On the other hand  ''f''&lt;sub&gt;''n''&lt;/sub&gt;(''U'') eventually contains ''f''(''K''), for applying the inverse ''g''&lt;sub&gt;''n''&lt;/sub&gt;, ''U'' eventually contains ''g''&lt;sub&gt;''n''&lt;/sub&gt; ∘''f'' (''K'') since ''g''&lt;sub&gt;''n''&lt;/sub&gt; ∘''f'' tends uniformly to the identity on compacta. Hence ''f''(''K'') has measure zero.
 
* ''f'' is smooth on the regular set Ω of ''μ''. This follows from the elliptic regularity results in the ''L''&lt;sup&gt;2&lt;/sup&gt; theory.
* ''f'' has non-vanishing Jacobian there. In particular ''f''&lt;sub&gt;''z''&lt;/sub&gt; ≠ 0 on Ω.&lt;ref&gt;{{harvnb|Douady|Buff|p=321}}&lt;/ref&gt; In fact for ''z''&lt;sub&gt;0&lt;/sub&gt; in Ω, if ''n'' is large enough

::&lt;math&gt;\displaystyle{\partial_{\overline{z}}  (f\circ g_n) = 0}&lt;/math&gt;

:near ''z''&lt;sub&gt;1&lt;/sub&gt; = ''f''&lt;sub&gt;''n''&lt;/sub&gt;(''z''&lt;sub&gt;0&lt;/sub&gt;). So ''h'' = ''f'' ∘ ''g''&lt;sub&gt;''n''&lt;/sub&gt; is holomorphic near ''z''&lt;sub&gt;1&lt;/sub&gt;. Since it is locally a homeomorphism, ''h'' ' (''z''&lt;sub&gt;1&lt;/sub&gt;) ≠ 0. Since ''f'' =''h'' ∘ ''f''&lt;sub&gt;''n''&lt;/sub&gt;. it follows that the Jacobian of ''f'' is non-zero at ''z''&lt;sub&gt;0&lt;/sub&gt;. On the other hand ''J''(''f'') = |''f''&lt;sub&gt;''z''&lt;/sub&gt;|&lt;sup&gt;2&lt;/sup&gt; (1 − |μ|&lt;sup&gt;2&lt;/sup&gt;), so ''f''&lt;sub&gt;''z''&lt;/sub&gt; ≠ 0 at ''z''&lt;sub&gt;0&lt;/sub&gt;.

* ''g'' satisfies the Beltrami equation with Beltrami coefficient

::&lt;math&gt;\displaystyle{\mu^\prime(f(z))=-{f_z\over \overline{f_z}}\cdot \mu(z)}&lt;/math&gt;

:or equivalently

::&lt;math&gt;\displaystyle{\mu^\prime(w) =-{\overline{g_w}\over g_w} \cdot \mu(g(w))}&lt;/math&gt;

:on the regular set Ω ' = ''f''(Ω), with corresponding singular set Λ ' = ''f''(Λ).

*''g'' satisfies the Beltrami equation for ''μ''&amp;prime;. In fact ''g'' has weak distributional derivatives in 1 + L&lt;sup&gt;''p''&lt;/sup&gt; and L&lt;sup&gt;''p''&lt;/sup&gt;. Pairing with smooth functions of compact support in Ω, these derivatives coincide with the actual derivatives at points of Ω. Since Λ has measure zero, the distributional derivatives equal the actual derivatives in ''L''&lt;sup&gt;''p''&lt;/sup&gt;. Thus ''g'' satisfies Beltrami's equation since the actual derivatives do.
*If ''f''* and ''f'' are solutions constructed as above for ''μ''* and ''μ'' then ''f''* ∘ ''f''&lt;sup&gt;−1&lt;/sup&gt; satisfies the Beltrami equation for

::&lt;math&gt;\displaystyle \nu(f(z))={f_z\over \overline{f_z}} \, {\mu^* - \mu\over 1-\overline{\mu} \mu^*}, &lt;/math&gt;

:defined on Ω ∩ Ω*. The weak derivatives of ''f''* ∘ ''f''&lt;sup&gt;−1&lt;/sup&gt; are given by the actual derivatives on Ω ∩ Ω*. In fact this follows by approximating ''f''* and ''g'' = ''f''&lt;sup&gt;−1&lt;/sup&gt; by ''f''*&lt;sub&gt;''n''&lt;/sub&gt; and ''g''&lt;sub&gt;''n''&lt;/sub&gt;. The derivatives are uniformly bounded in 1 + L&lt;sup&gt;''p''&lt;/sup&gt; and L&lt;sup&gt;''p''&lt;/sup&gt;, so as before weak limits give the distributional derivatives of ''f''* ∘ ''f''&lt;sup&gt;−1&lt;/sup&gt;. Pairing with smooth functions of compact support in  Ω ∩ Ω*, these agree with the usual derivatives. So the distributional derivatives are given by the usual derivatives off Λ ∪ Λ*, a set of measure zero.

This establishes the '''existence''' of homeomorphic solutions of Beltrami's equation in the case of Beltrami coefficients of compact support. It also shows that the inverse homeomorphisms and composed homeomorphisms satisfy Beltrami equations and that all computations can be performed by restricting to regular sets.

If the support is not compact the same trick used in the smooth case can be used to construct a solution in terms of two homeomorphisms associated to compactly supported Beltrami coefficients. Note that, because of the assumptions on the Beltrami coefficient, a Möbius transformation of the extended complex plane can be applied to make the singular set of the Beltrami coefficient compact. In that case one of the homeomorphisms can be chosen to be a diffeomorphism.

===Uniqueness===
There are several proofs of the uniqueness of solutions of the Beltrami equation with a given Beltrami coefficient.&lt;ref&gt;See:
*{{harvnb|Ahlfors|1966}}
*{{harvnb|Imayoshi|Taniguchi|1992}}
*{{harvnb|Lehto|1987}}
*{{harvnb|Lehto|Virtanen|1973}}
*{{harvnb|Buff|Xavier|2000|pp=321–322}}&lt;/ref&gt; Since applying a Möbius transformation of the complex plane to any solution gives another solution, solutions can be normalised so that they fix 0, 1 and ∞. The method of solution of the Beltrami equation using the Beurling transform also provides a proof of uniqueness for coefficients of compact support ''μ'' and for which the distributional derivatives are in 1 + L&lt;sup&gt;''p''&lt;/sup&gt; and L&lt;sup&gt;''p''&lt;/sup&gt;. The relations

:&lt;math&gt;\displaystyle P\psi)_{\overline{z}} = \psi, \,\, \, (P\psi)_{z} = T\psi &lt;/math&gt;

for smooth functions ψ of compact support are also valid in the distributional sense for L&lt;sup&gt;''p''&lt;/sup&gt; functions ''h'' since they can be written as L&lt;sup&gt;''p''&lt;/sup&gt; of  ψ&lt;sub&gt;''n''&lt;/sub&gt;'s. If ''f'' is a solution of the Beltrami equation with ''f''(0) = 0 and ''f''&lt;sub&gt;''z''&lt;/sub&gt; - 1 in L&lt;sup&gt;''p''&lt;/sup&gt; then

:&lt;math&gt;\displaystyle{F=f-P(f_{\overline{z}})}&lt;/math&gt;

satisfies

:&lt;math&gt;\displaystyle{\partial_{\overline{z}}F =0.}&lt;/math&gt;

So ''F'' is weakly holomorphic. Applying Weyl's lemma &lt;ref&gt;*{{harvnb|Astala|Iwaniec|Martin|2009}}&lt;/ref&gt; it is possible to conclude that there exists a holomorphic function ''G'' that is equal to ''F'' almost everywhere. Abusing notation redefine ''F:=G''. The conditions ''F'' '(z) − 1 lies in L&lt;sup&gt;''p''&lt;/sup&gt; and ''F''(0) = 0 force ''F''(''z'') = ''z''. Hence

:&lt;math&gt;\displaystyle{P(f_{\overline{z}}) = f(z) + z}&lt;/math&gt;

and so differentiating

:&lt;math&gt;\displaystyle{f_z=T(\mu f_z) + 1.}&lt;/math&gt;

If ''g'' is another solution then

:&lt;math&gt;\displaystyle{f_z-g_z=T(\mu(f_z-g_z)).}&lt;/math&gt;

Since ''T''μ has operator norm on L&lt;sup&gt;''p''&lt;/sup&gt; less than 1, this forces

:&lt;math&gt;\displaystyle{f_z=g_z.}&lt;/math&gt;

But then from the Beltrami equation

:&lt;math&gt;\displaystyle{f_{\overline{z}}=g_{\overline{z}}.}&lt;/math&gt;

Hence ''f'' − ''g'' is both holomorphic and antiholomorphic, so a constant. Since ''f''(0) = 0 = ''g''(0), it follows that ''f'' = ''g''. Note that since ''f'' is holomorphic off the support of ''μ'' and ''f''(∞) = ∞, the conditions that the derivatives are locally in L&lt;sup&gt;''p''&lt;/sup&gt; force

:&lt;math&gt;\displaystyle{f_z -1,\,\,f_{\overline{z}}\in L^p(\mathbf{C}).}&lt;/math&gt;

For a general ''f'' satisfying Beltrami's equation and with distributional derivatives locally in L&lt;sup&gt;''p''&lt;/sup&gt;, it can be assumed after applying a Möbius transformation that 0 is not in the singular set of the Beltrami coefficient ''μ''. If ''g'' is a smooth diffeomorphism ''g'' with Beltrami coefficient λ supported near 0, the Beltrami coefficient ''ν'' for ''f'' ∘ ''g''&lt;sup&gt;−1&lt;/sup&gt; can be calculated directly using the change of variables formula for distributional derivatives:

:&lt;math&gt;\displaystyle \nu(g(z))= {g_z\over \overline{g_z}} \,{\mu -\lambda\over 1-\overline{\lambda}\mu}. &lt;/math&gt;

''λ'' can be chosen so that ν vanishes near zero. Applying the map ''z''&lt;sup&gt;−1&lt;/sup&gt; results in a solution of Beltrami's equation with a Beltrami coefficient of compact support. The directional derivatives are still locally in L&lt;sup&gt;''p''&lt;/sup&gt;. The coefficient ν depends only on ''μ'', ''λ'' and ''g'', so any two solutions of the original equation will produce solutions near 0 with distributional derivatives locally in ''L''&lt;sup&gt;''p''&lt;/sup&gt; and the same Beltrami coefficient. They are therefore equal. Hence the solutions of the original equation are equal.

==Uniformization of multiply connected planar domains==

The method used to prove the smooth Riemann mapping theorem can be generalized to multiply connected planar regions with smooth boundary. The Beltrami coefficient in these cases is smooth on an open set, the complement of which has measure zero. The theory of the Beltrami equation with measurable coefficients is therefore required.&lt;ref&gt;{{harvnb|Bers|1961}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Sibner|1965}}&lt;/ref&gt;

'''Doubly connected domains.''' If Ω is a doubly connected planar region, then there is a diffeomorphism ''F'' of an annulus ''r'' ≤ |z| ≤ 1 onto the closure of Ω, such that after a conformal change the induced metric on the annulus can be continued smoothly by reflection in both boundaries. The annulus is a fundamental domain for the group generated by the two reflections, which reverse orientation. The images of the fundamental domain under the group fill out '''C''' with 0 removed and the Beltrami coefficient is smooth there. The canonical solution ''h'' of the Beltrami equation on '''C''', by the L&lt;sup&gt;''p''&lt;/sup&gt; theory is a homeomorphism. It is smooth on away from 0 by elliptic regularity. By uniqueness it preserves the unit circle, together with its interior and exterior. Uniqueness of the solution also implies that reflection there is a conjugate  Möbius transformation ''g'' such that ''h'' ∘ ''R'' = ''g'' ∘ ''h'' where ''R'' denotes reflection in |''z''| = ''r''.  Composing with a Möbius transformation that fixes the unit circle it can be assumed that ''g'' is a reflection in a circle |''z''| = ''s'' with ''s'' &lt; 1.  It follows that ''F'' ∘ ''h''&lt;sub&gt;−1&lt;/sub&gt; is a smooth diffeomorphism of the annulus ''s'' ≤ |''z''| ≤ 1 onto the closure of Ω, holomorphic in the interior.&lt;ref&gt;{{harvnb|Sibner|1965}}&lt;/ref&gt;

'''Multiply connected domains.''' For regions with a higher degree of connectivity ''k'' + 1, the result is essentially [[Lipman Bers|Bers']] generalization of the '''retrosection theorem'''.&lt;ref&gt;See: 
*{{harvnb|Bers|1961}}
*{{harvnb|Sibner|1965}}&lt;/ref&gt;  There is a smooth diffeomorphism ''F'' of the  region  Ω&lt;sub&gt;1&lt;/sub&gt;, given by the unit disk with ''k'' open disks removed, onto the closure of Ω.  It can be assumed that 0 lies in the interior of the domain. Again after a modification of the diffeomorphism and conformal change near the boundary, the metric can be assumed to be compatible with reflection. Let ''G'' be the group generated by reflections in the boundary circles of Ω&lt;sub&gt;1&lt;/sub&gt;. The interior of Ω&lt;sub&gt;1&lt;/sub&gt; iz a fundamental domain for ''G''. Moreover, the index two normal subgroup ''G''&lt;sub&gt;0&lt;/sub&gt; consisting of orientation-preserving mappings is a classical [[Schottky group]]. Its fundamental domain consists of the original fundamental domain with its reflection in the unit circle added. If the reflection is ''R''&lt;sub&gt;1&lt;/sub&gt;, it is a [[free group]] with generators ''R''&lt;sub&gt;''i''&lt;/sub&gt;∘''R''&lt;sub&gt;0&lt;/sub&gt; where ''R''&lt;sub&gt;''i''&lt;/sub&gt; are the reflections in the interior circles in the original domain. The images of the original domain by the ''G'', or equivalently the reflected domain by the Schottky group, fill out the regular set for the Schottky group. It acts properly discontinuously there. The complement is the [[limit set]] of ''G''&lt;sub&gt;0&lt;/sub&gt;. It has measure zero. The induced metric on Ω&lt;sub&gt;1&lt;/sub&gt; extends by reflection to the regular set. The corresponding Beltrami coefficient is invariant for the reflection group generated by the reflections ''R''&lt;sub&gt;''i''&lt;/sub&gt; for ''i'' ≥ 0. Since the limit set has measure zero, the Beltrami coefficient extends uniquely to a bounded measurable function on '''C'''. smooth  on the regular set. The normalised solution of the Beltrami equation ''h'' is a smooth diffeomorphism of the closure of  Ω&lt;sub&gt;1&lt;/sub&gt; onto itself preserving the unit circle, its exterior and interior. Necessarily ''h'' ∘ ''R''&lt;sub&gt;''i''&lt;/sub&gt; = ''S''&lt;sub&gt;''i''&lt;/sub&gt; ∘ ''h''. where ''S''&lt;sub&gt;''i''&lt;/sub&gt; is the reflection in another circle in the unit disk. Looking at fixed points, the circles arising this way for different ''i'' must be disjoint. It follows that ''F'' ∘ ''h''&lt;sup&gt;−1&lt;/sup&gt; defines a smooth diffeomorphism of the unit disc with the interior of these circles removed onto the closure of Ω, which is holomorphic in the interior.

==Simultaneous uniformization==
{{See also|Simultaneous uniformization theorem}}
{{harvtxt|Bers|1961}} showed that two compact Riemannian 2-manifolds ''M''&lt;sub&gt;1&lt;/sub&gt;, ''M''&lt;sub&gt;2&lt;/sub&gt; of genus ''g'' &gt; 1 can be simultaneously uniformized.

As topological spaces ''M''&lt;sub&gt;1&lt;/sub&gt; and ''M''&lt;sub&gt;2&lt;/sub&gt; are homeomorphic to a fixed quotient of the upper half plane '''H''' by a discrete cocompact subgroup Γ of PSL(2,'''R'''). Γ can be identified with the [[fundamental group]] of the manifolds and '''H''' is a [[universal covering space]]. The homeomorphisms can be chosen to be piecewise linear on corresponding  triangulations. A result of {{harvtxt|Munkres|1961}} implies that the homeomorphisms can be adjusted near the edges and the vertices of the triangulation to produce diffeomorphisms.  The metric on ''M''&lt;sub&gt;1&lt;/sub&gt; induces a metric on '''H''' which is  Γ-invariant. Let ''μ'' be the corresponding Beltrami coefficient on '''H'''. It can be extended to '''C''' by reflection

:&lt;math&gt;\displaystyle \widehat{\mu}(z) = \mu(z)\,\, (z\in \mathbf{H}),\,\, \widehat{\mu}(z)=0 \,\, (z\in\mathbf{R}),\,\, \widehat{\mu}(z)
=\overline{\mu(\overline{z})}\,\, (\overline{z} \in \mathbf{H}). &lt;/math&gt;

It satisfies the invariance property

:&lt;math&gt;\displaystyle \widehat{\mu}(g(z))=  {\overline{g_z}\over g_z}\, \widehat{\mu}(z), &lt;/math&gt;

for ''g'' in Γ. The solution ''f'' of the corresponding Beltrami equation defines a homeomorphism of '''C''', preserving the real axis and the upper and lower half planes. Conjugation of the group elements by ''f''&lt;sup&gt;−1&lt;/sup&gt; gives a new cocompact subgroup Γ&lt;sub&gt;1&lt;/sub&gt; of PSL(2,'''R'''). Composing the original diffeomorphism with the inverse of ''f'' then yield zero as the Beltrami coefficient. Thus the metric induced on '''H''' is invariant under Γ&lt;sub&gt;1&lt;/sub&gt; and conformal to the [[Poincaré metric]] on '''H'''. It must therefore be given by multiplying by a positive smooth function that is Γ&lt;sub&gt;1&lt;/sub&gt;-invariant. Any such function corresponds to a smooth function on ''M''&lt;sub&gt;1&lt;/sub&gt;. Dividing the metric on ''M''&lt;sub&gt;1&lt;/sub&gt; by this function results in a conformally equivalent metric on ''M''&lt;sub&gt;1&lt;/sub&gt; which agrees with the Poincaré metric on '''H''' /  Γ&lt;sub&gt;1&lt;/sub&gt;. In this way ''M''&lt;sub&gt;1&lt;/sub&gt; becomes a [[compact Riemann surface]], i.e. is uniformized and inherits a natural complex structure.

With this conformal change in metric ''M''&lt;sub&gt;1&lt;/sub&gt; can be identified with '''H''' /  Γ&lt;sub&gt;1&lt;/sub&gt;. The diffeomorphism between onto ''M''&lt;sub&gt;2&lt;/sub&gt; induces another metric on '''H''' which is invariant under Γ&lt;sub&gt;1&lt;/sub&gt;. It defines a Beltrami coefficient λ
omn '''H''' which this time is extended to '''C''' by defining λ to be 0 off '''H'''. The solution ''h'' of the Beltrami equation is a homeomorphism of '''C''' which is holomorphic on the lower half plane and smooth on the upper half plane. The image of the real axis is a [[Jordan curve]] dividing '''C''' into two components. Conjugation of Γ&lt;sub&gt;1&lt;/sub&gt; by ''h''&lt;sup&gt;−1&lt;/sup&gt; gives a [[Quasi-Fuchsian group|quasi-Fuchsian subgroup]] Γ&lt;sub&gt;2&lt;/sub&gt; of PSL(2,'''C'''). It leaves invariant the Jordan curve and acts properly discontinuously on each of the two components. The quotients of the two components by Γ&lt;sub&gt;2&lt;/sub&gt; are naturally identified with ''M''&lt;sub&gt;1&lt;/sub&gt; and ''M''&lt;sub&gt;2&lt;/sub&gt;. This identification is compatible with the natural complex structures on both ''M''&lt;sub&gt;1&lt;/sub&gt; and ''M''&lt;sub&gt;2&lt;/sub&gt;.

==Conformal welding==
{{See also|Conformal welding|Douady–Earle extension}}
An orientation-preserving homeomorphism ''f'' of the circle is said to be [[quasisymmetric map|quasisymmetric]] if there are positive constants ''a'' and ''b'' such that

:&lt;math&gt;\displaystyle{{{|f(z_1)-f(z_2)|\over |f(z_1)-f(z_3)|} \le a\cdot {|z_1-z_2|^b\over|z_1-z_3|^b}}.}&lt;/math&gt;

If

:&lt;math&gt;\displaystyle{|z_1-z_2|=|z_1-z_3|,}&lt;/math&gt;

then the condition becomes

:&lt;math&gt;\displaystyle{a^{-1}\le {|f(z_1)-f(z_2)|\over |f(z_1)-f(z_3)|} \le a.}&lt;/math&gt;

Conversely if this condition is satisfied for all such triples of points, then ''f'' is quasisymmetric.&lt;ref&gt;{{harvnb|Tukia|Väisälä|1980}}&lt;/ref&gt;

An apparently weaker condition on a homeomorphism ''f'' of the circle is that it be ''quasi-Möbius'', that is there are constants ''c'', ''d'' &gt; 0 such that

:&lt;math&gt;\displaystyle{|(f(z_1),f(z_2);f(z_3),f(z_4))| \le c |(z_1,z_2;z_3,z_4)|^d,}&lt;/math&gt;

where

:&lt;math&gt;\displaystyle{ (z_1,z_2;z_3,z_4)={(z_1-z_3)(z_2-z_4)\over(z_2-z_3)(z_1-z_4)}}&lt;/math&gt;

denotes the [[cross-ratio]]. In fact if ''f'' is quasisymmetric then it is also quasi-Möbius, with ''c'' = ''a''&lt;sup&gt;2&lt;/sup&gt; and ''d'' = ''b'': this follows by multiplying the first inequality above for (''z''&lt;sub&gt;1&lt;/sub&gt;,''z''&lt;sub&gt;3&lt;/sub&gt;,''z''&lt;sub&gt;4&lt;/sub&gt;) and (''z''&lt;sub&gt;2&lt;/sub&gt;,''z''&lt;sub&gt;4&lt;/sub&gt;,''z''&lt;sub&gt;3&lt;/sub&gt;).

Conversely if ''f'' is a quasi-Möbius homeomorphism then it is also quasisymmetric.&lt;ref&gt;{{harvnb|Väisälä|1984}}&lt;/ref&gt; Indeed, it is immediate that if ''f'' is quasi-Möbius so is its inverse. It then follows that ''f''  (and hence ''f''&lt;sup&gt;−1&lt;/sup&gt;) is [[Hölder continuous]]. To see this let ''S'' be the set of cube roots of unity, so that if ''a'' ≠ ''b'' in ''S'', then |''a'' − ''b''| = 2 sin {{pi}}/3 = {{radic|3}}. To prove a Hölder estimate, it can be assumed that ''x'' – ''y'' is uniformly small. Then both ''x'' and ''y'' are greater than a fixed distance away from ''a'', ''b'' in ''S'' with ''a'' ≠ ''b'', so the estimate follows by applying the quasi-Möbius inequality to ''x'', ''a'', ''y'', ''b''. To check that ''f'' is quasisymmetric, it suffices to find a uniform upper bound for |''f''(''x'') − ''f''(''y'')| / |''f''(''x'') − ''f''(''z'')| in the case of a triple with |''x'' − ''z''| = |''x'' − ''y''|, uniformly small. In this case there is a point ''w'' at a distance greater than 1 from ''x'', ''y'' and ''z''. Applying the quasi-Möbius inequality to ''x'', ''w'', ''y'' and ''z'' yields the required upper bound.

A homeomorphism ''f'' of the unit circle can be extended to a homeomorphism ''F'' of the closed unit disk which is diffeomorphism on its interior. {{harvtxt|Douady|Earle|1986}}, generalizing earlier results of Ahlfors and Beurling, produced such [[Douady–Earle extension|an extension]] with the additional properties that it commutes with the action of SU(1,1) by Möbius transformations and is quasiconformal if ''f'' is quasisymmetric. (A less elementary method was also found independently by {{harvtxt|Tukia|1985}}: Tukia's approach has the advantage of also applying in higher dimensions.) When ''f'' is a diffeomorphism of the circle, the [[Alexander trick|Alexander extension]] provides another way of extending ''f'':

:&lt;math&gt;\displaystyle{F(r,\theta)=r \exp [i\psi(r)g(\theta) + i(1-\psi(r))\theta] ,}&lt;/math&gt;

where ψ is a smooth function with values in [0,1], equal to 0 near 0 and 1 near 1, and

:&lt;math&gt;\displaystyle{f(e^{i\theta})=e^{ig(\theta)},}&lt;/math&gt;

with ''g''(''θ'' + 2{{pi}}) = ''g''(''θ'') + 2{{pi}}. {{harvtxt|Partyka|Sakan|Zając|1999}} give a survey of various methods of extension, including  variants of the Ahlfors-Beurling extension which are smooth or analytic in the open unit disk.

In the case of a diffeomorphism, the Alexander extension ''F'' can be continued to any larger disk |''z''| &lt; ''R'' with ''R'' &gt; 1. Accordingly, in the unit disc

:&lt;math&gt;\displaystyle{\|\mu\|_\infty &lt; 1, \,\,\, \mu(z)=F_{\overline{z}}/F_z.}&lt;/math&gt;

This is also true for the other extensions when ''f'' is only quasisymmetric.

Now extend ''μ'' to a Beltrami coefficient on the whole of '''C''' by setting it equal to 0 for |''z''| ≥ 1. Let ''G'' be the corresponding solution of the Beltrami equation. Let ''F''&lt;sub&gt;1&lt;/sub&gt;(''z'') = ''G'' ∘ ''F''&lt;sup&gt;−1&lt;/sup&gt;(''z'') for |''z''| ≤ 1 and
''F''&lt;sub&gt;2&lt;/sub&gt;(''z'') = ''G'' (''z'') for |''z''| ≥ 1. Thus ''F''&lt;sub&gt;1&lt;/sub&gt; and ''F''&lt;sub&gt;2&lt;/sub&gt; are univalent holomorphic maps of |''z''| &lt; 1 and |''z''| &gt; 1 onto the inside and outside of a Jordan curve. They extend continuously to homeomorphisms ''f''&lt;sub&gt;''i''&lt;/sub&gt; of the unit circle onto the Jordan curve on the boundary. By construction they satisfy the
'''conformal welding''' condition:

:&lt;math&gt;\displaystyle{f=f_1^{-1}\circ f_2.}&lt;/math&gt;

==See also==
*[[Quasiconformal mapping]]
*[[Measurable Riemann mapping theorem]]
*[[Isothermal coordinates]]

==Notes==
{{reflist|3}}

==References==
*{{citation|last=Ahlfors|first= Lars V.|title=Conformality with respect to Riemannian metrics|
series=Ann. Acad. Sci. Fenn. Ser. A. I.|year= 1955|volume= 206}}
*{{citation|last=Ahlfors|first=Lars V.|authorlink=Lars Ahlfors|title=Lectures on quasiconformal mappings|publisher=Van Nostrand|year=1966}}
*{{citation|last=Astala|first=Kari|last2= Iwaniec|first2= Tadeusz|author2-link=Tadeusz Iwaniec|last3=Martin|first3= Gaven|author3-link=Gaven Martin|title=Elliptic partial differential equations and quasiconformal mappings in the plane|series= 
Princeton Mathematical Series|volume= 48|publisher= Princeton University Press|year= 2009|isbn= 978-0-691-13777-3}}
*{{Citation | last1=Beltrami | first1=Eugenio | title=Saggio di interpretazione della geometria non euclidea (Essay on the interpretation of noneuclidean geometry) | url=http://www.caressa.it/pdf/beltrami01.pdf | language=Italian | jfm=01.0275.02 | year=1867 | journal=Giornale di Mathematica | volume=6}} English translation in {{harvtxt|Stillwell|1996}}
*{{citation|last=Bers|first=Lipman|title= Riemann surfaces|year=1958|publisher=Courant Institute}}
*{{citation|last=Bers|first=Lipman|last2=John|first2=Fritz|last3= Schechter|first3= Martin|title=Partial differential equations, with supplements by Lars Gȧrding and A. N. Milgram|series= Lectures in Applied Mathematics|volume= 3A|publisher= American Mathematical Society|year=1979|isbn=0-8218-0049-3}}, Chapter VI.
*{{citation|last=Bers|first= Lipman|title= Uniformization by Beltrami equations|journal= Comm. Pure Appl. Math. |volume=14|year=1961|pages= 215–228|doi=10.1002/cpa.3160140304}}
*{{citation|last=Douady|first= Adrien|last2= Earle|first2 =Clifford J.|title=Conformally natural extension of homeomorphisms of the circle|journal=Acta Math.|volume= 157|year=1986|pages=23–48|doi=10.1007/bf02392590}}
*{{citation|last=Douady|first= Adrien|authorlink=Adrien Douady|last2= Buff|first2= X.|title=Le théorème d'intégrabilité des structures presque complexes. [Integrability theorem for almost complex structures]|pages= 307–324|
series=London Math. Soc. Lecture Note Ser.|volume= 274|year= 2000|publisher =Cambridge Univ. Press}} 
*{{citation|last=Glutsyuk|first=Alexey A.|title=Simple proofs of uniformization theorems|pages= 125–143|journal=Fields Inst. Commun.|volume= 53|year=2008}}
*{{Citation | last1=Hubbard | first1=John Hamal | title=Teichmüller theory and applications to geometry, topology, and dynamics. Vol. 1 | url=http://matrixeditions.com/TeichmullerVol1.html | publisher=Matrix Editions, Ithaca, NY | isbn=978-0-9715766-2-9 | mr=2245223 | year=2006}}
*{{citation|first=Y. |last=Imayoshi|first2=M.|last2=Taniguchi|title=An Introduction to Teichmüller spaces|publisher=Springer-Verlag|year=1992|isbn=0-387-70088-9}}
*{{Citation | last1=Iwaniec | first1=Tadeusz | last2=Martin | first2=Gaven | title=The Beltrami equation | url=https://books.google.com/books?id=8iZczAQc59cC | series=Memoirs of the American Mathematical Society | isbn=978-0-8218-4045-0 | mr=2377904 | year=2008 | volume=191}}
*{{citation|first=Erwin|last=Kreyszig|authorlink=Erwin Kreyszig|year=1991|title=Differential Geometry|isbn=0-486-66721-9|publisher=Dover}}
*{{citation|last1=Lehto|first1= Olli|last2= Virtanen|first2=  K.I. |title=Quasiconformal mappings in the plane|edition=2nd|series= Die Grundlehren der mathematischen Wissenschaften|volume= 126|publisher= Springer-Verlag|year= 1973}}
*{{citation|last=Lehto|first= Olli|title=Univalent functions and Teichmüller spaces|series=Graduate Texts in Mathematics|volume= 109|publisher= Springer-Verlag|year= 1987|isbn=0-387-96310-3}}
*{{Citation | last1=Morrey | first1=Charles B. | title=On the solutions of quasi-linear elliptic partial differential equations. | doi=10.1090/S0002-9904-1936-06297-X | year=1936 | journal=[[Bulletin of the American Mathematical Society]] | issn=0002-9904 | volume=42 | issue=5 | pages=316 | jfm=62.0565.02}}
*{{citation|title=On the Solutions of Quasi-Linear Elliptic Partial Differential Equations|first=Charles B. Jr.|last=Morrey|authorlink=Charles B. Morrey, Jr.|journal=Transactions of the American Mathematical Society|volume=43|year=1938|pages=126–166|doi=10.2307/1989904|issue=1|jstor=1989904|publisher=American Mathematical Society|zbl=0018.40501}}
*{{citation|last=Munkres|first= James|title=Obstructions to the smoothing of piecewise-differentiable homeomorphisms|
journal=Ann. of Math. |volume= 72|year= 1960 |pages=521–554|doi=10.2307/1970228}}
*   Papadopoulos, Athanase, ed. (2007), Handbook of Teichmüller theory. Vol. I, IRMA Lectures in Mathematics and Theoretical Physics, 11, European Mathematical Society (EMS), Zürich, {{doi|10.4171/029}}, {{ISBN|978-3-03719-029-6}}, {{MR|2284826}}
*   Papadopoulos, Athanase, ed. (2009), Handbook of Teichmüller theory. Vol. II, IRMA Lectures in Mathematics and Theoretical Physics, 13, European Mathematical Society (EMS), Zürich, {{doi|10.4171/055}}, {{ISBN|978-3-03719-055-5}}, {{MR|2524085}}
*   Papadopoulos, Athanase, ed. (2012), Handbook of Teichmüller theory. Vol. III, IRMA Lectures in Mathematics and Theoretical Physics, 19, European Mathematical Society (EMS), Zürich, {{doi|10.4171/103}}, {{ISBN|978-3-03719-103-3}}
*{{citation|last=Partyka|first=Dariusz|last2= Sakan|first2= Ken-Ichi|last3= Zając|first3= Józef|title=The harmonic and quasiconformal extension operators|pages= 141–177|journal=Banach Center Publ.|volume= 48|year= 1999}}
*{{citation|last=Sibner|first= Robert J.|title=Uniformization of symmetric Riemann surfaces by Schottky groups|journal=Trans. Amer. Math. Soc.|volume= 116|year= 1965|pages= 79–85|doi=10.1090/s0002-9947-1965-0188431-2}}
*{{citation|last=Spivak|first= Michael|title= A comprehensive introduction to differential geometry. Vol. IV|edition=3nd|publisher= Publish or Perish|year= 1999|isbn=0-914098-70-5}}
*{{Citation | last1=Stillwell | first1=John | author1-link=John Stillwell | title=Sources of hyperbolic geometry | url=https://books.google.com/books?id=ZQjBXxxQsucC | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=History of Mathematics | isbn=978-0-8218-0529-9 | mr=1402697 | year=1996 | volume=10}}
*{{citation|last=Tukia|first= P.|last2= Väisälä|first2= J.|title=Quasisymmetric embeddings of metric spaces|
journal=Ann. Acad. Sci. Fenn. Ser. A I Math.|volume= 5 |year=1980|pages= 97–114}}
*{{citation|last=Tukia|first= Pekka|title=
Quasiconformal extension of quasisymmetric mappings compatible with a Möbius group|journal=Acta Math. |volume=154|year=1985|pages= 153–193|doi=10.1007/bf02392471}}
*{{citation|last=Väisälä|first=Jussi|title=Quasi-Möbius maps|journal=J. Analyse Math.|volume=44 |year=1984|pages= 218–234|doi=10.1007/bf02790198}}
*{{citation|last=Vekua|first= I. N.|title= Generalized analytic functions|publisher= Pergamon Press|year= 1962}}

[[Category:Partial differential equations]]
[[Category:Complex analysis]]
[[Category:Operator theory]]
[[Category:Moduli theory]]</text>
      <sha1>61tf4tlkmb17lcod5km9tb5mb9c1qzg</sha1>
    </revision>
  </page>
  <page>
    <title>Bolyai Prize</title>
    <ns>0</ns>
    <id>18706230</id>
    <revision>
      <id>834525637</id>
      <parentid>791900542</parentid>
      <timestamp>2018-04-06T04:42:12Z</timestamp>
      <contributor>
        <username>Bjankuloski06</username>
        <id>810946</id>
      </contributor>
      <comment>added [[Category:Hungarian awards]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1829">The '''International [[János Bolyai]] Prize of Mathematics''' is an international prize for mathematicians founded by the [[Hungarian Academy of Sciences]]. The prize is awarded in every five years&lt;ref&gt;Szénássy B.: Adalékok a Bolyai-díj történetéhez, ''Természet Világa'', '''124'''(1993),7, 291&amp;ndash;294. &lt;/ref&gt; to [[mathematician]]s having published their [[monograph]] describing their own highly important new results in the past 10 years.

== Medalists ==
* 1905 – {{flagicon|France}} [[Henri Poincaré]]
* 1910 – {{flagicon|Germany|empire}} [[David Hilbert]]
* 2000 – {{flagicon|Israel}} [[Saharon Shelah]]&lt;ref&gt;{{Cite web| title = Laudation of Shelah on the occasion of winning the Bolyai Prize (in Hungarian)| url = http://www.renyi.hu/~csirmaz/shelah/sh.pdf}}&lt;/ref&gt;
* 2005 – {{flagicon|Russia}} {{flagicon|France}} [[Mikhail Gromov (mathematician)|Mikhail Gromov]]
* 2010 – {{flagicon|Russia}} {{flagicon|Germany}} [[Yuri I. Manin]]&lt;ref&gt;{{Citeweb | title = Hungarian Academy of Sciences presents the János Bolyai International Mathematical Prize | url = http://www.math.bme.hu/akademia/jbimp.html | accessdate = 1 April 2010 | deadurl = yes | archiveurl = https://web.archive.org/web/20110718044545/http://www.math.bme.hu/akademia/jbimp.html | archivedate = 18 July 2011 | df =  }}&lt;/ref&gt;
* 2015 – {{flagicon|USA}} [[Barry Simon]]&lt;ref&gt;[http://mta.hu/news_and_views/mta-international-bolyai-prize-goes-to-barry-simon-135978/''Bolyai Prize goes to Barry Simon''] Communication by [[Hungarian Academy of Sciences]] (mta.hu). Retrieved 12 April 2015&lt;/ref&gt;

== References ==
{{reflist}}

[[Category:Hungarian Academy of Sciences]]
[[Category:Mathematics awards]]
[[Category:Lists of award winners]]
[[Category:Hungary-related lists]]
[[Category:Awards established in 1905]]
[[Category:Hungarian awards]]</text>
      <sha1>jd2rztlhwxrrjhjdx7gefd15yuplrhe</sha1>
    </revision>
  </page>
  <page>
    <title>Cheetah Math</title>
    <ns>0</ns>
    <id>19475686</id>
    <revision>
      <id>863366228</id>
      <parentid>860827916</parentid>
      <timestamp>2018-10-10T09:57:14Z</timestamp>
      <contributor>
        <ip>216.137.244.137</ip>
      </contributor>
      <comment>/* References */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1580">{{italic title}}
{{refimprove|date=July 2015}}

'''''Cheetah Math: Learning about Division from Baby Cheetahs''''' is a [[math]] book published by Henry Holt and Co. in 2007. ''Cheetah Math'' was designed to help students understand division. Ann Whitehead Nagda wrote the book with the cooperation of the [[San Diego Zoo]]. The book follows the lives of two baby [[cheetah]]s, Majani and Kubali, and relates their story to the principles of division. Sally Woolsey called the book "well done" and it is a popular item in many elementary school libraries. ''[[Kirkus Reviews]]'' called the book "A great addition to both the math and wild-animal conservation bookshelves".&lt;ref name=Author&gt;[http://www.annnagda.com/CheetahMath.htm Author's website]&lt;/ref&gt; The ''School Library Journal'' also gave a favorable review, saying ''Cheetah Math'' "is a wonderful cross-curricular book and an appealing way to introduce math".&lt;ref name=Author /&gt;

== Other books in the series ==
* ''Panda Math: Learning about Subtraction from Hua Mei and Mei Sheng''
* ''Tiger Math: Learning to Graph from a Baby Tiger''
* ''Chimp Math: Learning about Time from a Baby Chimpanzee''
* ''Polar Bear Math: Learning about Fractions from Klondike and Snow''

== References==
&lt;references/&gt;

{{Portal |Children's literature}}

[[Category:American children's books]]
[[Category:2007 children's books]]
[[Category:Mathematics books]]
[[Category:Zoology books]]
[[Category:Henry Holt and Company books]]
[[Category:Children's non-fiction books]]
[[Category:Books about cats]]
[[Category:American non-fiction books]]</text>
      <sha1>22co242swlokdfglpf2w49mbkdtvvdo</sha1>
    </revision>
  </page>
  <page>
    <title>Compact space</title>
    <ns>0</ns>
    <id>6042</id>
    <revision>
      <id>862037455</id>
      <parentid>858560844</parentid>
      <timestamp>2018-10-01T19:19:27Z</timestamp>
      <contributor>
        <ip>50.252.247.245</ip>
      </contributor>
      <comment>/* Examples */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="35094">{{short description|mathematical concept}}
{{redirect|Compactness}}
[[File:Compact.svg|thumb|upright=1.6|The interval {{math|''A'' {{=}} (−∞, −2]}} is not compact because it is not bounded. The interval {{math|''C'' {{=}} (2, 4)}} is not compact because it is not closed. The interval {{math|''B'' {{=}} [0, 1]}} is compact because it is both closed and bounded.]]
In [[mathematics]], and more specifically in [[general topology]], '''compactness''' is a property that generalizes the notion of a subset of [[Euclidean space]] being [[closed set|closed]] (that is, containing all its [[limit point]]s) and [[bounded set|bounded]] (that is, having all its points lie within some fixed distance of each other). Examples include a [[closed interval]], a [[rectangle]], or a finite set of points. This notion is defined for more general [[topological space]]s than Euclidean space in various ways.

One such generalization is that a topological space is [[sequentially compact|''sequentially'' compact]] if every [[infinite sequence]] of points sampled from the space has an infinite [[subsequence]] that converges to some point of the space. The [[Bolzano–Weierstrass theorem]] states that a subset of Euclidean space is compact in this sequential sense if and only if it is closed and bounded. Thus, if one chooses an infinite number of points in the ''closed'' [[unit interval]] {{math|[0, 1]}} some of those points will get arbitrarily close to some real number in that space. For instance, some of the numbers {{nowrap|1/2, 4/5, 1/3, 5/6, 1/4, 6/7, &amp;hellip;}} accumulate to 0 (others accumulate to 1). The same set of points would not accumulate to any point of the ''open'' unit interval {{math|(0, 1)}}; so the open unit interval is not compact. Euclidean space itself is not compact since it is not bounded. In particular, the sequence of points {{nowrap|0, 1, 2, 3, …}} has no subsequence that converges to any real number.

Apart from closed and bounded subsets of Euclidean space, typical examples of compact spaces include spaces consisting not of geometrical points but of [[function space|functions]]. The term ''compact'' was introduced into mathematics by [[Maurice Fréchet]] in 1904 as a distillation of this concept.  Compactness in this more general situation plays an extremely important role in [[mathematical analysis]], because many classical and important theorems of 19th-century analysis, such as the [[extreme value theorem]], are easily generalized to this situation.  A typical application is furnished by the [[Arzelà–Ascoli theorem]] or the [[Peano existence theorem]], in which one is able to conclude the existence of a function with some required properties as a limiting case of some more elementary construction.

Various equivalent notions of compactness, including [[Sequentially compact space|sequential compactness]] and [[limit point compact]]ness, can be developed in general [[metric space]]s. In general topological spaces, however, different notions of compactness are not necessarily equivalent. The most useful notion, which is the standard definition of the unqualified term ''compactness'', is phrased in terms of the existence of finite families of [[open set]]s that "[[Cover (topology)|cover]]" the space in the sense that each point of the space lies in some set contained in the family. This more subtle notion, introduced by [[Pavel Alexandrov]] and [[Pavel Urysohn]] in 1929, exhibits compact spaces as generalizations of [[finite set]]s.  In spaces that are compact in this sense, it is often possible to patch together information that holds [[local property|locally]]&amp;mdash;that is, in a neighborhood of each point&amp;mdash;into corresponding statements that hold throughout the space, and many [[#Theorems|theorems]] are of this character.

The term '''compact set''' is sometimes a synonym for compact space, but usually refers to a [[#Compactness of subsets|compact subspace]] of a topological space.

== Historical development ==
In the 19th century, several disparate mathematical properties were understood that would later be seen as consequences of compactness.  On the one hand, [[Bernard Bolzano]] ([[#CITEREFBolzano1817|1817]]) had been aware that any bounded sequence of points (in the line or plane, for instance) has a subsequence that must eventually get arbitrarily close to some other point, called a [[limit point]].  Bolzano's proof relied on the [[method of bisection]]: the sequence was placed into an interval that was then divided into two equal parts, and a part containing infinitely many terms of the sequence was selected.  The process could then be repeated by dividing the resulting smaller interval into smaller and smaller parts until it closes down on the desired limit point.  The full significance of [[Bolzano–Weierstrass theorem|Bolzano's theorem]], and its method of proof, would not emerge until almost 50 years later when it was rediscovered by [[Karl Weierstrass]].&lt;ref&gt;{{harvnb|Kline|1972|pp=952–953}}; {{harvnb|Boyer|Merzbach|1991|p=561}}&lt;/ref&gt;

In the 1880s, it became clear that results similar to the Bolzano–Weierstrass theorem could be formulated for [[function spaces|spaces of functions]] rather than just numbers or geometrical points. The idea of regarding functions as themselves points of a generalized space dates back to the investigations of [[Giulio Ascoli]] and [[Cesare Arzelà]].&lt;ref&gt;{{harvnb|Kline|1972|loc=Chapter 46, §2}}&lt;/ref&gt;  The culmination of their investigations, the [[Arzelà–Ascoli theorem]], was a generalization of the Bolzano–Weierstrass theorem to families of [[continuous function]]s, the precise conclusion of which was that it was possible to extract a [[uniform convergence|uniformly convergent]] sequence of functions from a suitable family of functions.  The uniform limit of this sequence then played precisely the same role as Bolzano's "limit point".  Towards the beginning of the twentieth century, results similar to that of Arzelà and Ascoli began to accumulate in the area of [[integral equation]]s, as investigated by [[David Hilbert]] and [[Erhard Schmidt]].  For a certain class of [[Green's function|Green functions]] coming from solutions of integral equations, Schmidt had shown that a property analogous to the Arzelà–Ascoli theorem held in the sense of [[mean convergence]]&amp;mdash;or convergence in what would later be dubbed a [[Hilbert space]].  This ultimately led to the notion of a [[compact operator]] as an offshoot of the general notion of a compact space. It was [[Maurice René Fréchet|Maurice Fréchet]] who, in [[#CITEREFFréchet1906|1906]], had distilled the essence of the Bolzano–Weierstrass property and coined the term ''compactness'' to refer to this general phenomenon (he used the term already in his 1904 paper&lt;ref&gt;Frechet, M. 1904. Generalisation d'un theorem de Weierstrass. Analyse Mathematique.&lt;/ref&gt; which led to the famous 1906 thesis).

However, a different notion of compactness altogether had also slowly emerged at the end of the 19th century from the study of the [[linear continuum|continuum]], which was seen as fundamental for the rigorous formulation of analysis.  In 1870, [[Eduard Heine]] showed that a [[continuous function]] defined on a closed and bounded interval was in fact [[uniformly continuous]].  In the course of the proof, he made use of a lemma that from any countable cover of the interval by smaller open intervals, it was possible to select a finite number of these that also covered it. The significance of this lemma was recognized by [[Émile Borel]] ([[#CITEREFBorel1895|1895]]), and it was generalized to arbitrary collections of intervals by [[Pierre Cousin]] (1895) and [[Henri Lebesgue]] ([[#CITEREFLebesgue1904|1904]]).  The [[Heine–Borel theorem]], as the result is now known, is another special property possessed by closed and bounded sets of real numbers.

This property was significant because it allowed for the passage from [[local property|local information]] about a set (such as the continuity of a function) to global information about the set (such as the uniform continuity of a function).  This sentiment was expressed by {{harvtxt|Lebesgue|1904}}, who also exploited it in the development of the [[Lebesgue integral|integral now bearing his name]].  Ultimately the Russian school of [[point-set topology]], under the direction of [[Pavel Alexandrov]] and [[Pavel Urysohn]], formulated Heine–Borel compactness in a way that could be applied to the modern notion of a [[topological space]].  {{harvtxt|Alexandrov|Urysohn|1929}} showed that the earlier version of compactness due to Fréchet, now called (relative) [[sequential compactness]], under appropriate conditions followed from the version of compactness that was formulated in terms of the existence of finite subcovers.  It was this notion of compactness that became the dominant one, because it was not only a stronger property, but it could be formulated in a more general setting with a minimum of additional technical machinery, as it relied only on the structure of the open sets in a space.
&lt;!--
One of the main reasons for studying compact spaces is because they are in some ways very similar to [[finite set]]s: there are many results which are easy to show for finite sets, whose proofs carry over with minimal change to compact spaces. Here is an example:

* Suppose ''X'' is a [[Hausdorff space]], and we have a point ''x'' in ''X'' and a finite subset ''A'' of ''X'' not containing ''x''. Then we can [[separated sets|separate]] ''x'' and ''A'' by [[neighborhood (topology)|neighborhoods]]: for each ''a'' in ''A'', let ''U''(''x'') and ''V''(''a'') be disjoint neighborhoods containing ''x'' and ''a'', respectively. Then the intersection of all the ''U''(''x'') and the union of all the ''V''(''a'') are the required neighborhoods of ''x'' and ''A''.

Note that if ''A'' is [[Infinity|infinite]], the proof fails, because the intersection of arbitrarily many neighborhoods of ''x'' might not be a neighborhood of ''x''. The proof can be "rescued", however, if ''A'' is compact: we simply take a finite subcover of the cover {{math|{''V''(''a'') : ''a'' ∈ A}}} of ''A'', then intersect the corresponding finitely many ''U''(''x''). In this way, we see that in a Hausdorff space, any point can be separated by neighborhoods from any compact set not containing it. In fact, repeating the argument shows that any two disjoint compact sets in a Hausdorff space can be separated by neighborhoods&amp;nbsp;– note that this is precisely what we get if we replace "point" (i.e. [[singleton set]]) with "compact set" in the Hausdorff [[separation axiom]]. Many of the arguments and results involving compact spaces follow such a pattern.
--&gt;

==Basic examples==
Any [[finite topological space|finite space]] is trivially compact. A non-trivial example of a compact space is the (closed) [[unit interval]] {{closed-closed|0,1}} of [[real number]]s.  If one chooses an infinite number of distinct points in the unit interval, then there must be some [[accumulation point]] in that interval. For instance, the odd-numbered terms of the sequence {{nowrap|1, 1/2, 1/3, 3/4, 1/5, 5/6, 1/7, 7/8, ...}} get arbitrarily close to&amp;nbsp;0, while the even-numbered ones get arbitrarily close to&amp;nbsp;1.  The given example sequence shows the importance of including the [[boundary (topology)|boundary]] points of the interval, since the [[limit point]]s must be in the space itself — an open (or half-open) interval of the real numbers is not compact. It is also crucial that the interval be [[bounded set|bounded]], since in the interval {{closed-open|0,∞}} one could choose the sequence of points {{nowrap|0, 1, 2, 3, ...}}, of which no sub-sequence ultimately gets arbitrarily close to any given real number.

In two dimensions, closed [[Disk (mathematics)|disks]] are compact since for any infinite number of points sampled from a disk, some subset of those points must get arbitrarily close either to a point within the disc, or to a point on the boundary.  However, an open disk is not compact, because a sequence of points can tend to the boundary without getting arbitrarily close to any point in the interior.  Likewise, spheres are compact, but a sphere missing a point is not since a sequence of points can tend to the missing point, thereby not getting arbitrarily close to any point ''within'' the space.  Lines and planes are not compact, since one can take a set of equally-spaced points in any given direction without approaching any point.

==Definitions==
Various definitions of compactness may apply, depending on the level of generality.  A subset of [[Euclidean space]] in particular is called compact if it is [[closed set|closed]] and [[bounded set|bounded]].  This implies, by the [[Bolzano–Weierstrass theorem]], that any infinite [[sequence (mathematics)|sequence]] from the set has a [[subsequence]] that converges to a point in the set.  Various equivalent notions of compactness, such as [[sequential compactness]] and [[limit point compact]]ness, can be developed in general [[metric space]]s.

In general [[topological space]]s, however, the different notions of compactness are not equivalent, and the most useful notion of compactness&amp;mdash;originally called ''bicompactness''&amp;mdash;is defined using [[cover (topology)|cover]]s consisting of [[open set]]s (see ''Open cover definition'' below). That this form of compactness holds for closed and bounded subsets of Euclidean space is known as the [[Heine–Borel theorem]]. Compactness, when defined in this manner, often allows one to take information that is known [[local property|locally]]&amp;mdash;in a [[neighbourhood (mathematics)|neighbourhood]] of each point of the space&amp;mdash;and to extend it to information that holds globally throughout the space.  An example of this phenomenon is Dirichlet's theorem, to which it was originally applied by Heine, that a continuous function on a compact interval is [[uniformly continuous]]; here, continuity is a local property of the function, and uniform continuity the corresponding global property.

===Open cover definition===

Formally, a [[topological space]] {{mvar|X}} is called ''compact'' if each of its [[open cover]]s has a [[finite set|finite]] [[subcover]]. That is, {{mvar|X}} is compact if for every collection {{mvar|C}} of open subsets of {{mvar|X}} such that

:&lt;math&gt;X = \bigcup_{x \in C}x&lt;/math&gt;,

there is a '''finite''' subset {{mvar|F}} of {{mvar|C}} such that

:&lt;math&gt;X = \bigcup_{x \in F}x.&lt;/math&gt;

Some branches of mathematics such as [[algebraic geometry]], typically influenced by the French school of [[Nicolas Bourbaki|Bourbaki]], use the term ''quasi-compact'' for the general notion, and reserve the term ''compact'' for topological spaces that are both [[Hausdorff space|Hausdorff]] and ''quasi-compact''.  A compact set is sometimes referred to as a ''compactum'', plural ''compacta''.

===Compactness of subsets===
A subset {{mvar|K}} of a topological space {{mvar|X}} is said to be compact if it is compact as a subspace (in the [[subspace topology]]).  That is, {{mvar|K}} is compact if for every arbitrary collection {{mvar|C}} of open subsets of {{mvar|X}} such that

:&lt;math&gt;K \subseteq \bigcup_{c \in C} c&lt;/math&gt;,

there is a '''finite''' subset {{mvar|F}} of {{mvar|C}} such that

:&lt;math&gt;K \subseteq \bigcup_{c \in F} c&lt;/math&gt;.

Compactness is a "topological" property. That is, if &lt;math&gt;K \subset Z \subset Y&lt;/math&gt;, with subset {{mvar|Z}} equipped with the subspace topology, then {{mvar|K}} is compact in {{mvar|Z}} if and only if {{mvar|K}} is compact in {{mvar|Y}}.

===Equivalent definitions===
Assuming the [[axiom of choice]], the following are equivalent:
# A topological space ''X''  is compact.
# Every [[open cover]] of ''X'' has a finite [[subcover]].
# ''X'' has a sub-base such that every cover of the space by members of the sub-base has a finite subcover ([[Alexander's sub-base theorem]])
# Any collection of closed subsets of ''X'' with the [[finite intersection property]] has nonempty intersection.
# Every [[Net (mathematics)|net]] on ''X'' has a convergent subnet (see the article on [[Net (mathematics)|nets]] for a proof).
# Every [[mathematical filter|filter]] on ''X'' has a convergent refinement.
# Every net on ''X'' has a cluster point.
# Every filter on ''X'' has a cluster point.
# Every [[ultrafilter]] on ''X'' converges to at least one point.
# Every infinite subset of ''X'' has a [[complete accumulation point]].&lt;ref&gt;{{harv|Kelley|1955|p=163}}&lt;/ref&gt;

====Euclidean space====
For any [[subset]] ''A'' of [[Euclidean space]] '''R'''&lt;sup&gt;''n''&lt;/sup&gt;, ''A'' is compact if and only if it is [[closed set|closed]] and [[bounded set|bounded]]; this is the [[Heine–Borel theorem]].

As a [[Euclidean space]] is a metric space, the conditions in the next subsection also apply to all of its subsets. Of all of the equivalent conditions, it is in practice easiest to verify that a subset is closed and bounded, for example, for a closed [[interval (mathematics)|interval]] or closed ''n''-ball.

====Metric spaces====
For any metric space (''X'', ''d''), the following are equivalent:
# (''X'', ''d'') is compact.
# (''X'', ''d'') is [[completeness (topology)|complete]] and [[totally bounded]] (this is also equivalent to compactness for [[uniform space]]s).&lt;ref&gt;{{harvnb|Arkhangel'skii|Fedorchuk|1990|loc=Theorem 5.3.7}}&lt;/ref&gt;
# (''X'', ''d'') is sequentially compact; that is, every [[sequence]] in ''X'' has a convergent subsequence whose limit is in ''X'' (this is also equivalent to compactness for [[first-countable]] [[uniform space]]s).
# (''X'', ''d'') is limit point compact; that is, every infinite subset of ''X'' has at least one [[limit point]] in ''X''.
# (''X'', ''d'') is an image of a continuous function from the [[Cantor set]].&lt;ref&gt;{{harvnb|Willard|1970}} Theorem 30.7.&lt;/ref&gt;

A compact metric space (''X'', ''d'') also satisfies the following properties:
# [[Lebesgue's number lemma]]: For every open cover of ''X'', there exists a number {{nowrap|''δ'' &gt; 0}} such that every subset of ''X'' of diameter &lt; ''δ'' is contained in some member of the cover.
# (''X'', ''d'') is [[second-countable space|second-countable]], [[Separable space|separable]] and [[Lindelöf space|Lindelöf]] – these three conditions are equivalent for metric spaces. The converse is not true; e.g., a countable discrete space satisfies these three conditions, but is not compact.
# ''X'' is closed and bounded (as a subset of any metric space whose restricted metric is ''d''). The converse may fail for a non-Euclidean space; e.g. the [[real line]] equipped with the [[discrete metric]] is closed and bounded but not compact, as the collection of all [[Singleton (mathematics)|singletons]] of the space is an open cover which admits no finite subcover. It is complete but not totally bounded.

====Characterization by continuous functions====
Let ''X'' be a topological space and C(''X'') the ring of real continuous functions on ''X''.  For each ''p''&amp;isin;''X'', the evaluation map &lt;math&gt;\operatorname{ev}_p\colon C(X)\to \mathbf{R}&lt;/math&gt;
given by ev&lt;sub&gt;''p''&lt;/sub&gt;(''f'')=''f''(''p'') is a ring homomorphism.  The [[kernel (algebra)|kernel]] of ev&lt;sub&gt;''p''&lt;/sub&gt; is a [[maximal ideal]], since the [[residue field]] {{nowrap|C(''X'')/ker ev&lt;sub&gt;''p''&lt;/sub&gt;}} is the field of real numbers, by the [[first isomorphism theorem]].  A topological space ''X'' is [[pseudocompact space|pseudocompact]] if and only if every maximal ideal in C(''X'') has residue field the real numbers.  For [[completely regular space]]s, this is equivalent to every maximal ideal being the kernel of an evaluation homomorphism.&lt;ref&gt;{{harvnb|Gillman|Jerison|1976|loc=§5.6}}&lt;/ref&gt;  There are pseudocompact spaces that are not compact, though.

In general, for non-pseudocompact spaces there are always maximal ideals ''m'' in C(''X'') such that the residue field C(''X'')/''m'' is a ([[non-archimedean field|non-archimedean]]) [[hyperreal field]].  The framework of [[non-standard analysis]] allows for the following alternative characterization of compactness:&lt;ref&gt;{{harvnb|Robinson||loc=Theorem 4.1.13}}&lt;/ref&gt; a topological space ''X'' is compact if and only if every point ''x'' of the natural extension ''*X'' is [[infinitesimal|infinitely close]] to a point ''x''&lt;sub&gt;0&lt;/sub&gt; of ''X'' (more precisely, ''x'' is contained in the [[monad (non-standard analysis)|monad]] of ''x''&lt;sub&gt;0&lt;/sub&gt;).

====Hyperreal definition====
A space ''X'' is compact if its [[hyperreal number|hyperreal extension]] ''*X'' (constructed, for example, by the [[ultrapower construction]]) has the property that every point of ''*X'' is infinitely close to some point of ''X''⊂''*X''. For example, an open real interval {{nowrap|''X'' {{=}} (0, 1)}} is not compact because its hyperreal extension *(0,1) contains infinitesimals, which are infinitely close to 0, which is not a point of ''X''.

== Properties of compact spaces ==

===Functions and compact spaces===
A [[continuous function (topology)|continuous]] image of a compact space is compact.&lt;ref&gt;{{harvnb|Arkhangel'skii|Fedorchuk|1990|loc=Theorem 5.2.2}}; See also {{planetmathref|id=4689|title=Compactness is preserved under a continuous map}}&lt;/ref&gt;
This implies the [[extreme value theorem]]: a continuous real-valued function on a nonempty compact space is bounded above and attains its supremum.&lt;ref&gt;{{harvnb|Arkhangel'skii|Fedorchuk|1990|loc=Corollary 5.2.1}}&lt;/ref&gt; (Slightly more generally, this is true for an upper semicontinuous function.) As a sort of converse to the above statements, the pre-image of a compact space under a [[proper map]] is compact.

===Compact spaces and set operations===
A closed subset of a compact space is compact,&lt;ref&gt;{{harvnb|Arkhangel'skii|Fedorchuk|1990|loc=Theorem 5.2.3}}; {{planetmathref|id=4177|title=Closed set in a compact space is compact}}; {{planetmathref|id=4691|title=Closed subsets of a compact set are compact}}&lt;/ref&gt; and a finite union of compact sets is compact.

The [[product topology|product]] of any collection of compact spaces is compact. (This is [[Tychonoff's theorem]], which is equivalent to the [[axiom of choice]].)

Every topological space ''X'' is an open [[dense topological subspace|dense subspace]] of a compact space having at most one point more than ''X'', by the [[Compactification (mathematics)|Alexandroff one-point compactification]].  By the same construction, every [[locally compact]] Hausdorff space ''X'' is an open dense subspace of a compact Hausdorff space having at most one point more than ''X''.

===Ordered compact spaces===
A nonempty compact subset of the [[real number]]s has a greatest element and a least element.

Let ''X'' be a [[total order|simply ordered]] set endowed with the [[order topology]]. Then ''X'' is compact if and only if ''X'' is a [[complete lattice]] (i.e. all subsets have suprema and infima).&lt;ref&gt;{{harv|Steen|Seebach|1995|p=67}}&lt;/ref&gt;

==Examples==
* Any [[finite topological space]], including the [[empty set]], is compact.  More generally, any space with a [[finite topology]] (only finitely many open sets) is compact; this includes in particular the [[trivial topology]].
* Any space carrying the [[cofinite topology]] is compact.
* Any [[locally compact]] Hausdorff space can be turned into a compact space by adding a single point to it, by means of [[Alexandroff one-point compactification]]. The one-point compactification of '''R''' is homeomorphic to the circle '''S'''&lt;sup&gt;1&lt;/sup&gt;; the one-point compactification of '''R'''&lt;sup&gt;2&lt;/sup&gt; is homeomorphic to the sphere '''S'''&lt;sup&gt;2&lt;/sup&gt;. Using the one-point compactification, one can also easily construct compact spaces which are not Hausdorff, by starting with a non-Hausdorff space.
* The [[right order topology]] or [[left order topology]] on any bounded [[totally ordered set]] is compact. In particular, [[Sierpiński space]] is compact.
* No [[discrete space]] with an infinite number of points is compact. The collection of all [[Singleton (mathematics)|singletons]] of the space is an open cover which admits no finite subcover. Finite discrete spaces are compact.
* In '''R''' carrying the [[lower limit topology]], no uncountable set is compact.
* In the [[cocountable topology]] on an uncountable set, no infinite set is compact. Like the previous example, the space as a whole is not [[locally compact]] but is still [[Lindelöf space|Lindelöf]].
* The closed [[unit interval]] {{closed-closed|0,1}} is compact. This follows from the [[Heine–Borel theorem]]. The open interval {{open-open|0,1}} is not compact: the [[open cover]] &lt;math&gt;\left ( \frac{1}{n}, 1 - \frac{1}{n} \right )&lt;/math&gt; for {{math|1={{mvar|n}} = 3, 4, … }} does not have a finite subcover. Similarly, the set of ''[[rational numbers]]'' in the closed interval {{closed-closed|0,1}} is not compact: the sets of rational numbers in the intervals &lt;math&gt;\left[0,\frac{1}{\pi}-\frac{1}{n}\right] \ \text{and} \ \left[\frac{1}{\pi}+\frac{1}{n},1\right]&lt;/math&gt; cover all the rationals in [0,&amp;nbsp;1] for {{math|1={{mvar|n}} = 4, 5, ... }} but this cover does not have a finite subcover. Here, the sets are open in the subspace topology even though they are not open as subsets of&amp;nbsp;'''R'''.
* The set '''R''' of all real numbers is not compact as there is a cover of open intervals that does not have a finite subcover. For example, intervals {{open-open|{{mvar|n}}−1, {{mvar|n}}+1}} , where {{mvar|n}} takes all integer values in '''Z''', cover '''R''' but there is no finite subcover.
* On the other hand, the [[extended real number line]] carrying the analogous topology ''is'' compact; note that the cover described above would never reach the points at infinity.  In fact, the set has the [[homeomorphism]] to [-1,1] of mapping each infinity to its corresponding unit and every real number to its sign multiplied by the unique number in the positive part of interval that results in its absolute value when divided by one minus itself, and since homeomorphisms preserve covers, the Heine-Borel property can be inferred.
* For every [[natural number]] {{mvar|n}}, the [[n-sphere|{{mvar|n}}-sphere]] is compact. Again from the Heine–Borel theorem, the closed unit ball of any finite-dimensional [[normed vector space]] is compact. This is not true for infinite dimensions; in fact, a normed vector space is finite-dimensional if and only if its [[closed unit ball]] is compact.
* On the other hand, the closed unit ball of the dual of a normed space is compact for the weak-* topology. ([[Alaoglu's theorem]])
* The [[Cantor set]] is compact. In fact, every compact metric space is a continuous image of the Cantor set.
* Consider the set ''K'' of all functions ''f'' : '''R''' → [0,1] from the real number line to the closed unit interval, and define a topology on ''K'' so that a sequence &lt;math&gt;\{f_n\}&lt;/math&gt; in ''K'' converges towards &lt;math&gt;f\in K&lt;/math&gt; if and only if &lt;math&gt;\{f_n(x)\}&lt;/math&gt; converges towards ''f''(''x'') for all real numbers ''x''. There is only one such topology; it is called the topology of [[pointwise convergence]] or the [[product topology]]. Then ''K'' is a compact topological space; this follows from the [[Tychonoff theorem]].
* Consider the set ''K'' of all functions ''f''&amp;nbsp;: {{closed-closed|0,1}}&amp;nbsp;→ {{closed-closed|0,1}} satisfying the [[Lipschitz condition]] |''f''(''x'')&amp;nbsp;−&amp;nbsp;''f''(''y'')|&amp;nbsp;≤ |''x''&amp;nbsp;−&amp;nbsp;''y''| for all ''x'',&amp;nbsp;''y''&amp;nbsp;∈&amp;nbsp;{{closed-closed|0,1}}.  Consider on ''K''&amp;thinsp; the metric induced by the [[uniform convergence|uniform distance]] &lt;math&gt;d(f,g)=\sup_{x \in [0, 1]} |f(x)-g(x)|.&lt;/math&gt; Then by [[Arzelà–Ascoli theorem]] the space ''K'' is compact.
* The [[spectrum of an operator|spectrum]] of any [[bounded linear operator]] on a [[Banach space]] is a nonempty compact subset of the [[complex number]]s '''C'''. Conversely, any compact subset of '''C''' arises in this manner, as the spectrum of some bounded linear operator. For instance, a diagonal operator on the Hilbert space [[sequence spaces#ℓp spaces|&lt;math&gt;\ell^2&lt;/math&gt;]] may have any compact nonempty subset of '''C''' as spectrum.

=== Algebraic examples ===
* [[Compact group]]s such as an [[orthogonal group]] are compact, while groups such as a [[general linear group]] are not.
* Since the [[p-adic numbers|''p''-adic integers]] are [[homeomorphic]] to the Cantor set, they form a compact set.
* The [[spectrum of a ring|spectrum]] of any [[commutative ring]] with the [[Zariski topology]] (that is, the set of all prime ideals) is compact, but never [[Hausdorff space|Hausdorff]] (except in trivial cases). In algebraic geometry, such topological spaces are examples of quasi-compact [[scheme (mathematics)|schemes]], "quasi" referring to the non-Hausdorff nature of the topology.
* The [[spectrum of a boolean algebra|spectrum of a Boolean algebra]] is compact, a fact which is part of the [[Stone representation theorem]]. [[Stone space]]s, compact [[totally disconnected space|totally disconnected]] Hausdorff spaces, form the abstract framework in which these spectra are studied.  Such spaces are also useful in the study of [[profinite group]]s.
* The [[structure space]] of a commutative unital [[Banach algebra]] is a compact Hausdorff space.
* The [[Hilbert cube]] is compact, again a consequence of Tychonoff's theorem.
* A [[profinite group]] (e.g., Galois group) is compact.

== See also ==
* [[Compactly generated space]]
* [[Eberlein compactum]]
* [[Exhaustion by compact sets]]
* [[Lindelöf space]]
* [[Metacompact space]]
* [[Noetherian topological space]]
* [[Orthocompact space]]
* [[Paracompact space]]

==Notes==
{{reflist}}

==References==
*{{citation |last1=Alexandrov |first1=Pavel |authorlink1=Pavel Alexandrov |last2=Urysohn |first2=Pavel |authorlink2=Pavel Urysohn |title=Mémoire sur les espaces topologiques compacts |journal=Koninklijke Nederlandse Akademie van Wetenschappen te Amsterdam, Proceedings of the section of mathematical sciences |volume=14 |year=1929}}.
*{{citation |last1=Arkhangel'skii |first1=A.V. |last2=Fedorchuk |first2=V.V. |contribution=The basic concepts and constructions of general topology |editor1=Arkhangel'skii, A.V. |editor2=Pontrjagin, L.S. |title=General topology I |publisher=Springer |year=1990 |isbn=978-0-387-18178-3 |series=Encyclopedia of the Mathematical Sciences |volume=17}}.
*{{springer|id=C/c023530|title=Compact space|first=A.V.|last=Arkhangel'skii}}.
*{{citation |first=Bernard |last=Bolzano |authorlink=Bernard Bolzano |title=Rein analytischer Beweis des Lehrsatzes, dass zwischen je zwey Werthen, die ein entgegengesetzes Resultat gewähren, wenigstens eine reele Wurzel der Gleichung liege |year=1817 |url=https://books.google.com/?id=EoW4AAAAIAAJ&amp;dq=%22Rein%20analytischer%20Beweis%20des%20Lehrsatzes%22&amp;pg=PA2-IA3#v=onepage&amp;q= |publisher=Wilhelm Engelmann}} (''Purely analytic proof of the theorem that between any two values which give results of opposite sign, there lies at least one real root of the equation'').
*{{citation |last=Borel |first=Émile |authorlink=Émile Borel |title=Sur quelques points de la théorie des fonctions |journal=[[Annales Scientifiques de l'École Normale Supérieure]]|series= 3 |volume=12 |year=1895 |pages=9–55 |url=http://www.numdam.org/numdam-bin/item?id=ASENS_1895_3_12__9_0 |jfm=26.0429.03 }}
*{{Citation | last1=Boyer | first1=Carl B. | author1-link=Carl Benjamin Boyer | title=The history of the calculus and its conceptual development | publisher=Dover Publications | location=New York | mr=0124178 | year=1959}}.
* {{citation |first=Cesare |last=Arzelà |authorlink=Cesare Arzelà |title=Sulle funzioni di linee |journal=Mem. Accad. Sci. Ist. Bologna Cl. Sci. Fis. Mat. |volume=5 |issue=5 |pages=55–74 |year=1895}}.
* {{citation |first=Cesare |last=Arzelà |authorlink=Cesare Arzelà |title=Un'osservazione intorno alle serie di funzioni |journal=Rend. Dell' Accad. R. Delle Sci. Dell'Istituto di Bologna |pages=142–159 |year=1882–1883}}.
* {{citation |first=G. |last=Ascoli |authorlink=Giulio Ascoli |title=Le curve limiti di una varietà data di curve |journal=Atti della R. Accad. Dei Lincei Memorie della Cl. Sci. Fis. Mat. Nat. |volume=18 |issue=3 |pages=521–586 |year=1883–1884}}.
*{{citation | first=Maurice |last=Fréchet |authorlink=Maurice Fréchet |title=Sur quelques points du calcul fonctionnel |year=1906 |journal= [[Rendiconti del Circolo Matematico di Palermo]] |volume=22 |doi=10.1007/BF03018603 |pages=1–72 |issue=1}}.
*{{citation | last1=Gillman|first1=Leonard|last2=Jerison|first2=Meyer|title=Rings of continuous functions|publisher=Springer-Verlag|year=1976}}.
*{{citation | last=Kelley |first=John |title=General topology |publisher=Springer-Verlag |year=1955 |series=Graduate Texts in Mathematics |volume=27}}.
*{{Citation | last1=Kline | first1=Morris | author1-link=Morris Kline | title=Mathematical thought from ancient to modern times | year=1972 | publisher=Oxford University Press | edition=3rd | isbn=978-0-19-506136-9 | publication-date=1990}}.
*{{citation |first=Henri |last=Lebesgue |title=Leçons sur l'intégration et la recherche des fonctions primitives |url=https://books.google.com/?id=VfUKAAAAYAAJ&amp;dq=%22Lebesgue%22%20%22Le%C3%A7ons%20sur%20l'int%C3%A9gration%20et%20la%20recherche%20des%20fonctions%20...%22&amp;pg=PA1#v=onepage&amp;q= |year=1904 |publisher=Gauthier-Villars}}.
*{{Citation | last1=Robinson | first1=Abraham | author1-link=Abraham Robinson | title=Non-standard analysis | publisher=Princeton University Press | isbn=978-0-691-04490-3 |mr=0205854 | year=1996}}.
*{{citation |first=C.T. |last= Scarborough |first2= A.H. |last2= Stone |title= Products of nearly compact spaces |journal= Transactions of the American Mathematical Society |volume= 124 |year=1966 |pages= 131–147 |doi=10.2307/1994440 |issue=1 |publisher=Transactions of the American Mathematical Society, Vol. 124, No. 1 |jstor=1994440|url=http://www.ams.org/tran/1966-124-01/S0002-9947-1966-0203679-7/S0002-9947-1966-0203679-7.pdf }}.
* {{Citation | last1=Steen | first1=Lynn Arthur | author1-link=Lynn Arthur Steen | last2=Seebach | first2=J. Arthur Jr. | author2-link=J. Arthur Seebach, Jr. | title=[[Counterexamples in Topology]] | origyear=1978 | publisher=Springer-Verlag | location=Berlin, New York | edition=Dover Publications reprint of 1978 | isbn=978-0-486-68735-3 | mr=507446 | year=1995}}
*{{Citation|last=Willard|first=Stephen|title=General Topology|publisher=Dover publications|year=1970|isbn=0-486-43479-6}}

==External links==
* {{planetmathref |id=1233 |title=Countably compact}}
* {{cite arXiv |last=Sundström |first=Manya Raman |eprint=1006.4131v1 |title=A pedagogical history of compactness |class=math.HO |year=2010 |version=}}
----
{{PlanetMath attribution|id=3133|title=Examples of compact spaces}}

{{Topology}}

{{DEFAULTSORT:Compact Space}}
[[Category:Compactness (mathematics)]]
[[Category:General topology]]
[[Category:Properties of topological spaces]]
[[Category:Topology]]</text>
      <sha1>jp0acnrwoa183n097aftu77yv6f0j3z</sha1>
    </revision>
  </page>
  <page>
    <title>Complex Wishart distribution</title>
    <ns>0</ns>
    <id>55731874</id>
    <revision>
      <id>809601390</id>
      <parentid>809601304</parentid>
      <timestamp>2017-11-10T05:17:55Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2487">{{Probability distribution
 | name       = Complex Wishart
 | type       =density
 | pdf_image  =
 | cdf_image  =
 | notation   ={{math|''A'' ~ ''CW&lt;sub&gt;p&lt;/sub&gt;''('''&lt;math&gt;\Gamma&lt;/math&gt;''', ''n'')}}
 | parameters ={{math|''n'' &gt; ''p'' − 1}} [[degrees of freedom (statistics)|degrees of freedom]] ([[real numbers|real]])&lt;br/&gt;{{math|'''&lt;math&gt;\Gamma&lt;/math&gt;''' &gt; 0}}  ({{math|''p'' × ''p''}} [[Hermitian matrix|Hermitian]] [[positive definite matrix|pos. def]])
 | support    ={{math|'''A''' (''p'' × ''p'')}}  [[Hermitian matrix|Hermitian]] [[positive definite matrix]]
 | pdf        =&lt;math&gt;\frac{
\det\left(\mathbf{A}\right)^{(n-p)} e^{-\operatorname{tr}(\mathbf{\Gamma}^{-1}\mathbf{A})}
}{
\det\left(\mathbf{\Gamma}\right)^{n}\cdot\widetilde{\Gamma}_p(n)
} &lt;/math&gt;
* &lt;math&gt;\widetilde{\mathbf{\Gamma}}_p&lt;/math&gt; is the &lt;math&gt;p&lt;/math&gt;-variate [[complex multivariate gamma function]]
* {{math|tr}} is the [[trace (linear algebra)|trace]] function
 | cdf        =
 | mean       =&lt;math&gt;\operatorname{E}[A]=n\Gamma&lt;/math&gt;|
 | median     =
 | mode       =&lt;math&gt;(n-p) \mathbf{\Gamma}&lt;/math&gt; for {{math|''n'' ≥ ''p'' + 1}}
 | variance   =
 | skewness   =
 | kurtosis   =
 | entropy    =
 | mgf        =
 | char       = &lt;math&gt;\det\left(I_p-i\mathbf{\Gamma}\mathbf{\Theta}\right)^{-n}&lt;/math&gt;
}}

In [[statistics]], the '''complex Wishart distribution''' is a [[complex numbers|complex]] version of the [[Wishart distribution]].  It is the distribution of &lt;math&gt;n&lt;/math&gt; times the sample Hermitian covariance matrix of &lt;math&gt;n&lt;/math&gt; zero-mean [[independence (probability)|independent]] [[Complex normal distribution#Circularly-symmetric normal distribution|Gaussian]] random variables.  It has [[support (measure theory)|support]] for &lt;math&gt;p\times p&lt;/math&gt; [[Hermitian matrix|Hermitian]] [[positive definite matrix|positive definite matrices]].&lt;ref&gt;{{cite article |author= N. R. Goodman |year= 1963 |title= The distribution of the determinant of a complex Wishart distributed matrix | journal =The Annals of Mathematical Statistics |volume= 34 |number= 1 |pages= 178–180}}&lt;/ref&gt;

==References==
{{reflist}}

{{statistics-stub}}

{{ProbDistributions|multivariate}}

{{DEFAULTSORT:Complex Wishart Distribution}}
[[Category:Continuous distributions]]
[[Category:Multivariate continuous distributions]]
[[Category:Covariance and correlation]]
[[Category:Random matrices]]
[[Category:Conjugate prior distributions]]
[[Category:Exponential family distributions]]
[[Category:Complex distributions]]</text>
      <sha1>k7tu9co5ndwrt32rv88ip76yjl8iq20</sha1>
    </revision>
  </page>
  <page>
    <title>Conformal equivalence</title>
    <ns>0</ns>
    <id>681682</id>
    <redirect title="Conformal geometry" />
    <revision>
      <id>815597569</id>
      <parentid>793499598</parentid>
      <timestamp>2017-12-15T20:52:02Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v478)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1480">#REDIRECT [[Conformal geometry]]

[[File:Riemann sphere1.svg|thumb|300px|[[Stereographic projection]] is a conformal equivalence between a portion of the [[sphere]] (with its standard metric) and the [[plane (geometry)|plane]] with the metric &lt;math&gt; \frac{4}{(1 + X^2 + Y^2)^2} \; ( dX^2 + dY^2)&lt;/math&gt;.|right]]
In [[mathematics]] and [[theoretical physics]], two [[Thurston model geometry|geometries]] are '''conformally equivalent''' if there exists a [[conformal transformation]] (an angle-preserving transformation) that maps one geometry to the other one.&lt;ref&gt;{{citation|title=Functions of One Complex Variable II|series=[[Graduate Texts in Mathematics]]|volume=159|first=John B.|last=Conway|publisher=Springer|year=1995|isbn=9780387944609|page=29|url=https://books.google.com/books?id=JN0hz3qO1eMC&amp;pg=PA29}}.&lt;/ref&gt;
More generally, two [[Riemannian metric]]s on a [[manifold]] ''M'' are conformally equivalent if one is obtained from the other by multiplication by a positive function on&amp;nbsp;''M''.&lt;ref&gt;{{citation|title=Global Calculus|first=S.|last=Ramanan|publisher=American Mathematical Society|isbn=9780821872406|year=2005|page=221|url=https://books.google.com/books?id=1INoRKtgndcC&amp;pg=PA221}}.&lt;/ref&gt; Conformal equivalence is an [[equivalence relation]] on geometries or on Riemannian metrics.

== See also ==
* [[conformal geometry]]
* [[biholomorphy|biholomorphic equivalence]]
* [[AdS/CFT correspondence]]

==References==
{{reflist}}


[[Category:Conformal geometry]]</text>
      <sha1>43xelkkx7kwl5uk6hcwwmh5kabyporc</sha1>
    </revision>
  </page>
  <page>
    <title>Connected dominating set</title>
    <ns>0</ns>
    <id>2495030</id>
    <revision>
      <id>871276723</id>
      <parentid>815798163</parentid>
      <timestamp>2018-11-30T00:30:19Z</timestamp>
      <contributor>
        <ip>206.87.223.6</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9717">In [[graph theory]], a '''connected dominating set''' and a '''maximum leaf spanning tree''' are two closely related structures defined on an [[undirected graph]].

==Definitions==
A connected dominating set of a graph ''G'' is a set ''D'' of vertices with two properties:
#Any node in ''D'' can reach any other node in ''D'' by a path that stays entirely within ''D''. That is, ''D'' [[induced subgraph|induces]] a connected subgraph of ''G''.
#Every vertex in ''G'' either belongs to ''D'' or is adjacent to a vertex in ''D''. That is, ''D'' is a [[dominating set]] of ''G''.
A '''minimum connected dominating set''' of a graph ''G'' is a connected dominating set with the smallest possible [[cardinality]] among all connected dominating sets of ''G''. The '''connected domination number''' of ''G'' is the number of vertices in the minimum connected dominating set.&lt;ref&gt;{{citation
 | last1 = Sampathkumar | first1 = E.
 | last2 = Walikar | first2 = HB
 | issue = 6
 | journal = J. Math. Phys. Sci
 | pages = 607–613
 | title = The connected domination number of a graph
 | volume = 13
 | year = 1979}}.&lt;/ref&gt;

Any [[spanning tree]] ''T'' of a graph ''G'' has at least two leaves, vertices that have only one edge of ''T'' incident to them. A maximum leaf spanning tree is a spanning tree that has the largest possible number of leaves among all spanning trees of ''G''. The '''max leaf number''' of ''G'' is the number of leaves in the maximum leaf spanning tree.&lt;ref name="ecology"&gt;{{citation
 | last1 = Fellows | first1 = Michael
 | last2 = Lokshtanov | first2 = Daniel
 | last3 = Misra | first3 = Neeldhara
 | last4 = Mnich | first4 = Matthias
 | last5 = Rosamond | first5 = Frances
 | last6 = Saurabh | first6 = Saket
 | doi = 10.1007/s00224-009-9167-9
 | issue = 4
 | journal = Theory of Computing Systems
 | pages = 822–848
 | title = The complexity ecology of parameters: an illustration using bounded max leaf number
 | volume = 45
 | year = 2009}}.&lt;/ref&gt;

==Complementarity==
If ''d'' is the connected domination number of an ''n''-vertex graph ''G'', where ''n &gt; 2'', and ''l'' is its max leaf number, then the three quantities ''d'', ''l'', and ''n'' obey the simple equation
:&lt;math&gt;\displaystyle n = d + l.&lt;/math&gt;&lt;ref name="d92"&gt;{{citation
 | last = Douglas | first = Robert J.
 | doi = 10.1016/0012-365X(92)90130-8
 | issue = 1–3
 | journal = Discrete Mathematics
 | pages = 41–47
 | title = NP-completeness and degree restricted spanning trees
 | volume = 105
 | year = 1992}}.&lt;/ref&gt;

If ''D'' is a connected dominating set, then there exists a [[spanning tree]] in ''G'' whose leaves include all vertices that are not in ''D'': form a spanning tree of the subgraph induced by ''D'', together with edges connecting each remaining vertex ''v'' that is not in ''D'' to a neighbor of ''v'' in ''D''. This shows that {{nowrap|''l'' ≥ ''n'' &amp;minus; ''d''.}}

In the other direction, if ''T'' is any spanning tree in ''G'', then the vertices of ''T'' that are not leaves form a connected dominating set of ''G''. This shows that {{nowrap|''n'' &amp;minus; ''l'' ≥ ''d''.}} Putting these two inequalities together proves the equality {{nowrap|1=''n'' = ''d'' + ''l''.}}

Therefore, in any graph, the sum of the connected domination number and the max leaf number equals the total number of vertices.
Computationally, this implies that determining the connected domination number is equally difficult as finding the max leaf number.

==Algorithms==
It is [[NP-complete]] to test whether there exists a connected dominating set with size less than a given threshold, or equivalently to test whether there exists a spanning tree with at least a given number of leaves. Therefore, it is believed that the minimum connected dominating set problem and the maximum leaf spanning tree problem cannot be solved in polynomial time.

When viewed in terms of approximation algorithms, connected domination and maximum leaf spanning trees are not the same: approximating one to within a given [[approximation ratio]] is not the same as approximating the other to the same ratio.
There exists an approximation for the minimum connected dominating set that achieves a factor of {{nowrap|2 ln &amp;Delta; + O(1)}}, where &amp;Delta; is the maximum degree of a vertex in G.&lt;ref&gt;{{citation
 | last1 = Guha | first1 = S.
 | last2 = Khuller | first2 = S.
 | doi = 10.1007/PL00009201
 | issue = 4
 | journal = Algorithmica
 | pages = 374–387
 | title = Approximation algorithms for connected dominating sets
 | volume = 20
 | year = 1998}}.&lt;/ref&gt;
The maximum leaf spanning tree problem is [[MAX-SNP]] hard, implying that no [[polynomial time approximation scheme]] is likely.&lt;ref&gt;{{citation
 | last1 = Galbiati | first1 = G.
 | last2 = Maffioli | first2 = F.
 | last3 = Morzenti | first3 = A.
 | doi = 10.1016/0020-0190(94)90139-2
 | issue = 1
 | journal = Information Processing Letters
 | pages = 45–49
 | title = A short note on the approximability of the maximum leaves spanning tree problem
 | volume = 52
 | year = 1994}}.&lt;/ref&gt; However, it can be approximated to within a factor of 2 in polynomial time.&lt;ref&gt;{{citation
 | last = Solis-Oba | first = Roberto
 | contribution = 2-approximation algorithm for finding a spanning tree with maximum number of leaves
 | doi = 10.1007/3-540-68530-8_37
 | pages = 441–452
 | publisher = Springer-Verlag
 | series = Lecture Notes in Computer Science
 | title = [[European Symposium on Algorithms|Proc. 6th European Symposium on Algorithms (ESA'98)]]
 | volume = 1461
 | year = 1998}}.&lt;/ref&gt;

Both problems may be solved, on {{mvar|n}}-vertex graphs, in time {{math|''O''(1.9&lt;sup&gt;''n''&lt;/sup&gt;)}}.&lt;ref&gt;{{citation
 | last1 = Fernau | first1 = Henning
 | last2 = Kneis | first2 = Joachim
 | last3 = Kratsch | first3 = Dieter
 | last4 = Langer | first4 = Alexander
 | last5 = Liedloff | first5 = Mathieu
 | last6 = Raible | first6 = Daniel
 | last7 = Rossmanith | first7 = Peter
 | doi = 10.1016/j.tcs.2011.07.011
 | issue = 45
 | journal = Theoretical Computer Science
 | mr = 2883043
 | pages = 6290–6302
 | title = An exact algorithm for the maximum leaf spanning tree problem
 | volume = 412
 | year = 2011}}.&lt;/ref&gt; The maximum leaf problem is [[parameterized complexity|fixed-parameter tractable]], meaning that it can be solved in time exponential in the number of leaves but only polynomial in the input graph size. The [[klam value]] of these algorithms (intuitively, a number of leaves up to which the problem can be solved within a reasonable amount of time) has gradually increased, as algorithms for the problem have improved, to approximately 37,&lt;ref&gt;{{citation
 | last1 = Binkele-Raible | first1 = Daniel
 | last2 = Fernau | first2 = Henning
 | issue = 1
 | journal = Discrete Mathematics &amp; Theoretical Computer Science
 | mr = 3188035
 | pages = 179–200
 | title = A parameterized measure-and-conquer analysis for finding a ''k''-leaf spanning tree in an undirected graph
 | volume = 16
 | year = 2014}}.&lt;/ref&gt; and it has been suggested that at least 50 should be achievable.&lt;ref&gt;{{citation
 | last1 = Fellows | first1 = Michael R. | author1-link = Michael Fellows
 | last2 = McCartin | first2 = Catherine
 | last3 = Rosamond | first3 = Frances A.
 | last4 = Stege | first4 = Ulrike
 | contribution = Coordinatized kernels and catalytic reductions: an improved FPT algorithm for max leaf spanning tree and other problems
 | doi = 10.1007/3-540-44450-5_19
 | mr = 1850108
 | pages = 240–251
 | publisher = Springer, Berlin
 | series = Lecture Notes in Comput. Sci.
 | title = FST-TCS 2000: Foundations of Software Technology and Theoretical Computer Science
 | volume = 1974
 | year = 2000}}.&lt;/ref&gt;

In graphs of maximum [[degree (graph theory)|degree]] three, the connected dominating set and its complementary maximum leaf spanning tree problem can be solved in [[polynomial time]], by transforming them into an instance of the [[matroid parity problem]] for [[linear matroid]]s.&lt;ref&gt;{{citation
 | last1 = Ueno | first1 = Shuichi
 | last2 = Kajitani | first2 = Yoji
 | last3 = Gotoh | first3 = Shin'ya
 | department = Proceedings of the First Japan Conference on Graph Theory and Applications (Hakone, 1986)
 | doi = 10.1016/0012-365X(88)90226-9
 | issue = 1-3
 | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]
 | mr = 975556
 | pages = 355–360
 | title = On the nonseparating independent set problem and feedback set problem for graphs with no vertex degree exceeding three
 | volume = 72
 | year = 1988}}&lt;/ref&gt;

==Applications==
Connected dominating sets are useful in the computation of [[routing]] for [[mobile ad hoc network]]s. In this application, a small connected dominating set is used as a backbone for communications, and nodes that are not in this set communicate by passing messages through neighbors that are in the set.&lt;ref&gt;{{citation
 | last1 = Wu | first1 = J.
 | last2 = Li | first2 = H.
 | contribution = On calculating connected dominating set for efficient routing in ad hoc wireless networks
 | doi = 10.1145/313239.313261
 | pages = 7–14
 | publisher = ACM
 | title = Proceedings of the 3rd International Workshop on Discrete Algorithms and Methods for Mobile Computing and Communications
 | year = 1999}}.&lt;/ref&gt;

The max leaf number has been employed in the development of [[fixed-parameter tractability|fixed-parameter tractable]] [[algorithm]]s: several NP-hard optimization problems may be solved in polynomial time for graphs of bounded max leaf number.&lt;ref name="ecology"/&gt;

==See also==
*[[Universal vertex]], a vertex that (when it exists) gives a minimum connected dominating set of size one

==References==
{{reflist}}

[[Category:Computational problems in graph theory]]
[[Category:Graph connectivity]]</text>
      <sha1>2u9f67psily9un8dnllg5502e7kf16z</sha1>
    </revision>
  </page>
  <page>
    <title>Edge cover</title>
    <ns>0</ns>
    <id>4577392</id>
    <revision>
      <id>828158481</id>
      <parentid>828156021</parentid>
      <timestamp>2018-02-28T22:12:52Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contribs/Theneltand|Theneltand]] ([[User talk:Theneltand|talk]]) to last version by Bender the Bot</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3455">In [[graph theory]], an '''edge cover''' of a [[Graph (discrete mathematics)|graph]] is a set of [[edge (graph theory)|edge]]s such that every [[vertex (graph theory)|vertex]] of the graph is incident to at least one edge of the set.
In [[computer science]], the '''minimum edge cover problem''' is the problem of finding an edge cover of minimum size. It is an [[optimization problem]] that belongs to the class of [[covering problem]]s and can be solved in [[polynomial time]].

{{Covering-Packing Problem Pairs}}

== Definition ==
Formally, an  edge cover of a graph ''G'' is a set of edges ''C'' such that each vertex in ''G'' is incident with at least one edge in ''C''. The set ''C'' is said to ''cover'' the vertices of ''G''. The following figure shows examples of edge coverings in two graphs.

:[[File:Edge-cover.svg]]

A '''minimum edge covering''' is an edge covering of smallest possible size.  The '''edge covering number''' &lt;math&gt;\rho(G)&lt;/math&gt; is the size of a minimum edge covering. The following figure shows examples of minimum edge coverings.

:[[File:Minimum-edge-cover.svg]]

Note that the figure on the right is not only an edge cover but also a [[Matching (graph theory)|matching]]. In particular, it is a [[perfect matching]]: a matching ''M'' in which each vertex is incident with exactly one edge in ''M''. A perfect matching (if it exists) is always a minimum edge covering.

=== Examples ===
* The set of all edges is an edge cover, assuming that there are no degree-0 vertices.
* The [[complete bipartite graph]] ''K''&lt;sub&gt;''m'',''n''&lt;/sub&gt; has edge covering number max(''m'', ''n'').

== Algorithms ==
A smallest edge cover can be found in [[polynomial time]] by finding a [[maximum matching]] and extending it greedily so that all vertices are covered.&lt;ref name="gt79"/&gt;&lt;ref&gt;{{citation|title=Combinatorial optimization: networks and matroids|first=Eugene L.|last=Lawler|publisher=Dover Publications|year=2001|isbn=978-0-486-41453-9|pages=222–223|url=https://books.google.com/books?id=m4MvtFenVjEC&amp;pg=PA222}}.&lt;/ref&gt; In the following figure, a maximum matching is marked with red; the extra edges that were added to cover unmatched nodes are marked with blue. (The figure on the right shows a graph in which a maximum matching is a [[perfect matching]]; hence it already covers all vertices and no extra edges were needed.)

:[[File:Minimum-edge-cover-from-maximum-matching.svg]]

On the other hand, the related problem of finding a smallest [[vertex cover]] is an [[NP-hard]] problem.&lt;ref name="gt79"&gt;{{harvtxt|Garey|Johnson|1979}}, p. 79, uses edge cover and vertex cover as one example of a pair of similar problems, one of which can be solved in polynomial time while the other one is NP-hard. See also p. 190.&lt;/ref&gt;

== See also ==
* [[Vertex cover]]
* [[Set cover]] – the edge cover problem is a special case of the set cover problem: the elements of the ''universe'' are vertices, and each ''subset'' covers exactly two elements.

== Notes ==
{{reflist}}

== References ==
* {{MathWorld|urlname=EdgeCover|title=Edge Cover}}
* {{citation
 | last1=Garey | first1=Michael R. | authorlink1=Michael R. Garey
 | last2=Johnson | first2=David S. | authorlink2=David S. Johnson
 | year = 1979
 | title = [[Computers and Intractability: A Guide to the Theory of NP-Completeness]]
 | publisher = W.H. Freeman
 | isbn=0-7167-1045-5
}}.

[[Category:Computational problems in graph theory]]
[[Category:Polynomial-time problems]]</text>
      <sha1>5xjsjb596o3hps0b1cbd7boukaav5dd</sha1>
    </revision>
  </page>
  <page>
    <title>Envelope (mathematics)</title>
    <ns>0</ns>
    <id>880235</id>
    <revision>
      <id>868852670</id>
      <parentid>868646800</parentid>
      <timestamp>2018-11-14T21:32:42Z</timestamp>
      <contributor>
        <username>Texvc2LaTeXBot</username>
        <id>33995001</id>
      </contributor>
      <minor/>
      <comment>Replacing deprecated latex syntax [[mw:Extension:Math/Roadmap]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="26976">{{About||the envelope of an oscillating signal|Envelope (waves)|the abstract concept|Envelope (category theory)}}
[[Image:EnvelopeAnim.gif|right|thumb|500px|Construction of the envelope of a family of curves.]]
In [[geometry]], an '''envelope''' of a [[Family of curves|family]] of [[curve]]s in the plane is a curve that is [[tangent]] to each member of the family at some point, and these points of tangency together form the whole envelope. Classically, a point on the envelope can be thought of as the intersection of two "[[infinitesimal]]ly adjacent" curves, meaning the limit of intersections of nearby curves. This idea can be generalized to an envelope of surfaces in space, and so on to higher dimensions.

To have an envelope, the individual members of the family of curves need to be [[Differentiable manifold|differentiable curve]]s, for otherwise the concept of tangency does not apply, and there has to be a smooth transition proceeding through the members. But even if these conditions are satisfied, a given family may fail to have an envelope. A simple example of this is given by a family of concentric circles of expanding radius.

==Envelope of a family of curves==
Let each curve ''C''&lt;sub&gt;''t''&lt;/sub&gt; in the family be given as the solution of an equation ''f''&lt;sub&gt;''t''&lt;/sub&gt;(''x'',&amp;nbsp;''y'')=0 (see [[implicit curve]]), where ''t'' is a parameter. Write ''F''(''t'',&amp;nbsp;''x'',&amp;nbsp;''y'')=''f''&lt;sub&gt;''t''&lt;/sub&gt;(''x'',&amp;nbsp;''y'') and assume ''F'' is differentiable.

The envelope of the family ''C''&lt;sub&gt;''t''&lt;/sub&gt; is then defined as the set &lt;math&gt;\mathcal{D}&lt;/math&gt; of points (''x'',''y'') for which, simultaneously, 
:&lt;math&gt;F(t, x, y) = 0~~\mathsf{and}~~{\partial F \over \partial t}(t, x, y) = 0&lt;/math&gt;
for some value of ''t'',
where &lt;math&gt;\partial F/\partial t&lt;/math&gt; is the [[partial derivative]] of ''F'' with respect to ''t''.&lt;ref&gt;{{Citation |first=J. W. |last=Bruce |first2=P. J. |last2=Giblin |title=Curves and Singularities |publisher=Cambridge University Press |year=1984 |ISBN=0-521-42999-4}}&lt;/ref&gt;

If ''t'' and ''u'', ''t''≠''u'' are two values of the parameter then the intersection of the curves ''C''&lt;sub&gt;''t''&lt;/sub&gt; and ''C''&lt;sub&gt;''u''&lt;/sub&gt; is given by 
:&lt;math&gt;F(t, x, y) = F(u, x, y) = 0\,&lt;/math&gt;
or, equivalently,
:&lt;math&gt;F(t, x, y) = 0~~\mathsf{and}~~\frac{F(u, x, y)-F(t, x, y)}{u-t} = 0.&lt;/math&gt;
Letting u→t gives the definition above.

An important special case is when ''F''(''t'',&amp;nbsp;''x'',&amp;nbsp;''y'') is a polynomial in ''t''. This includes, by [[clearing denominators]], the case where ''F''(''t'',&amp;nbsp;''x'',&amp;nbsp;''y'') is a rational function in ''t''. In this case, the definition amounts to ''t'' being a double root of ''F''(''t'',&amp;nbsp;''x'',&amp;nbsp;''y''), so the equation of the envelope can be found by setting the [[discriminant]] of ''F'' to 0 (because the definition demands F=0 at some t and first derivative =0 i.e. it's value 0 and it is min/max at that t).

For example, let ''C''&lt;sub&gt;''t''&lt;/sub&gt; be the line whose ''x'' and ''y'' intercepts are ''t'' and 11−''t'', this is shown in the animation above. The equation of ''C''&lt;sub&gt;''t''&lt;/sub&gt; is
:&lt;math&gt;\frac{x}{t}+\frac{y}{11-t}=1&lt;/math&gt;
or, clearing fractions,
:&lt;math&gt;x(11-t)+yt-t(11-t)=t^2+(-x+y-11)t+11x=0.\,&lt;/math&gt;
The equation of the envelope is then
:&lt;math&gt;(-x+y-11)^2-44x=(x-y)^2-22(x+y)+121=0.\,&lt;/math&gt;

Often when ''F'' is not a rational function of the parameter it may be reduced to this case by an appropriate substitution. For example, if the family is given by ''C''&lt;sub&gt;θ&lt;/sub&gt; with an equation of the form ''u''(''x'',&amp;nbsp;''y'')cosθ+''v''(''x'',&amp;nbsp;''y'')sinθ=''w''(''x'',&amp;nbsp;''y''), then putting ''t''=''e''&lt;sup&gt;''i''θ&lt;/sup&gt;, cosθ=(''t''+1/''t'')/2, sinθ=(''t''-1/''t'')/2''i'' changes the equation of the curve to
:&lt;math&gt;u{1 \over 2}(t+{1\over t})+v{1 \over 2i}(t-{1\over t})=w&lt;/math&gt;
or
:&lt;math&gt;(u-iv)t^2-2wt+(u+iv)=0.\,&lt;/math&gt;
The equation of the envelope is then given by setting the discriminant to 0:
:&lt;math&gt;(u-iv)(u+iv)-w^2=0\,&lt;/math&gt;
or 
:&lt;math&gt;u^2+v^2=w^2.\,&lt;/math&gt;

===Alternative definitions===

# The envelope ''E''&lt;sub&gt;1&lt;/sub&gt; is the limit of intersections of nearby curves ''C''&lt;sub&gt;''t''&lt;/sub&gt;.
# The envelope ''E''&lt;sub&gt;2&lt;/sub&gt; is a curve tangent to all of the ''C''&lt;sub&gt;''t''&lt;/sub&gt;.
# The envelope ''E''&lt;sub&gt;3&lt;/sub&gt; is the boundary of the region filled by the curves ''C''&lt;sub&gt;''t''&lt;/sub&gt;.

Then &lt;math&gt;E_1 \subseteq \mathcal{D}&lt;/math&gt;, &lt;math&gt;E_2 \subseteq \mathcal{D}&lt;/math&gt; and &lt;math&gt;E_3 \subseteq \mathcal{D}&lt;/math&gt;, where &lt;math&gt;\mathcal{D}&lt;/math&gt; is the set of points defined at the beginning of this subsection's parent section.

==Examples==

===Example 1===
These definitions ''E''&lt;sub&gt;1&lt;/sub&gt;, ''E''&lt;sub&gt;2&lt;/sub&gt;, and ''E''&lt;sub&gt;3&lt;/sub&gt; of the envelope may be different sets. Consider for instance the curve {{nowrap|1=''y'' = ''x''&lt;sup&gt;3&lt;/sup&gt;}} parametrised by {{nowrap|γ : '''R''' → '''R'''&lt;sup&gt;2&lt;/sup&gt;}} where {{nowrap|1=γ(''t'') = (''t'',''t''&lt;sup&gt;3&lt;/sup&gt;)}}. The one-parameter family of curves will be given by the tangent lines to γ.

First we calculate the discriminant &lt;math&gt;\mathcal D&lt;/math&gt;. The generating function is
:&lt;math&gt; F(t,(x,y)) = 3t^2x - y - 2t^3.&lt;/math&gt;
Calculating the partial derivative {{nowrap|1=''F''&lt;sub&gt;''t''&lt;/sub&gt; = 6''t''(''x'' – ''t'')}}. It follows that either {{nowrap|1=''x'' = ''t''}} or {{nowrap|1=''t'' = 0}}. First assume that {{nowrap|1=''x'' = ''t'' and ''t'' ≠ 0}}. Substituting into F: &lt;math&gt;F(t,(t,y)) = t^3 - y \, &lt;/math&gt;
and so, assuming that ''t'' ≠ 0, it follows that {{nowrap|1=''F'' = ''F''&lt;sub&gt;''t''&lt;/sub&gt; = 0}} if and only if {{nowrap|1=(''x'',''y'') = (''t'',''t''&lt;sup&gt;3&lt;/sup&gt;)}}. Next, assuming that {{nowrap|1=''t'' = 0}} and substituting into ''F'' gives {{nowrap|1=''F''(0,(''x'',''y'')) = &amp;minus;''y''}}.  So, assuming {{nowrap|1=''t'' = 0}}, it follows that {{nowrap|1=''F'' = ''F''&lt;sub&gt;''t''&lt;/sub&gt; = 0}} if and only if {{nowrap|1=''y'' = 0}}. Thus the discriminant is the original curve and its tangent line at γ(0):
:&lt;math&gt; \mathcal{D} = \{(x,y) \in \R^2 : y = x^3\} \cup \{(x,y) \in \R^2 : y = 0 \} \ . &lt;/math&gt;

Next we calculate ''E''&lt;sub&gt;1&lt;/sub&gt;. One curve is given by {{nowrap|1=''F''(''t'',(''x'',''y'')) = 0}} and a nearby curve is given by {{nowrap|''F''(''t'' + &amp;epsilon;,(''x'',''y''))}} where ε is some very small number. The intersection point comes from looking at the limit of {{nowrap|1=''F''(''t'',(''x'',''y''))&amp;nbsp;=&amp;nbsp;''F''(''t'' + &amp;epsilon;,(''x'',''y''))}} as ε tends to zero. Notice that {{nowrap|1=''F''(''t'',(''x'',''y''))&amp;nbsp;=&amp;nbsp;''F''(''t'' + &amp;epsilon;,(''x'',''y''))}} if and only if
:&lt;math&gt; L := F(t,(x,y)) - F(t+\varepsilon,(x,y)) = 2\varepsilon^3+6\varepsilon t^2+6\varepsilon^2t-(3\varepsilon^2+6\varepsilon t)x = 0. &lt;/math&gt;
If {{nowrap|1=''t'' ≠ 0}} then ''L'' has only a single factor of ε. Assuming that {{nowrap|1=''t'' ≠ 0}} then the intersection is given by
:&lt;math&gt;\lim_{\varepsilon \to 0} \frac{1}{\varepsilon} L = 6t(t-x) \ . &lt;/math&gt;
Since {{nowrap|''t'' ≠ 0}} it follows that {{nowrap|1=''x'' = ''t''}}. The ''y'' value is calculated by knowing that this point must lie on a tangent line to the original curve γ: that {{nowrap|1=''F''(''t'',(''x'',''y'')) = 0}}. Substituting and solving gives ''y'' = ''t''&lt;sup&gt;3&lt;/sup&gt;. When {{nowrap|1=''t'' = 0}}, ''L'' is divisible by ε&lt;sup&gt;2&lt;/sup&gt;. Assuming that {{nowrap|1=''t'' = 0}} then the intersection is given by
:&lt;math&gt;\lim_{\varepsilon \to 0} \frac{1}{\varepsilon^2} L = 3x \ . &lt;/math&gt;
It follows that {{nowrap|1=''x'' = 0}}, and knowing that {{nowrap|1=''F''(''t'',(''x'',''y'')) = 0}} gives {{nowrap|1=''y'' = 0}}. It follows that
:&lt;math&gt; E_1 = \{(x,y) \in \R^2 : y = x^3 \} \ . &lt;/math&gt;

Next we calculate ''E''&lt;sub&gt;2&lt;/sub&gt;. The curve itself is the curve that is tangent to all of its own tangent lines. It follows that
:&lt;math&gt; E_2 = \{(x,y) \in \R^2 : y = x^3 \} \ . &lt;/math&gt;

Finally we calculate ''E''&lt;sub&gt;3&lt;/sub&gt;.  Every point in the plane has at least one tangent line to γ passing through it, and so region filled by the tangent lines is the whole plane. The boundary ''E''&lt;sub&gt;3&lt;/sub&gt; is therefore the empty set. Indeed, consider a point in the plane, say (''x''&lt;sub&gt;0&lt;/sub&gt;,''y''&lt;sub&gt;0&lt;/sub&gt;). This point lies on a tangent line if and only if there exists a ''t'' such that
:&lt;math&gt;F(t,(x_0,y_0)) = 3t^2x_0 - y_0 - 2t^3 = 0 \ . &lt;/math&gt;
This is a cubic in ''t'' and as such has at least one real solution. It follows that at least one tangent line to γ must pass through any given point in the plane. If {{nowrap|''y'' &gt; ''x''&lt;sup&gt;3&lt;/sup&gt;}} and {{nowrap|''y'' &gt; 0}} then each point (''x'',''y'') has exactly one tangent line to γ passing through it. The same is true if {{nowrap|''y'' &lt; ''x''&lt;sup&gt;3&lt;/sup&gt;}} {{nowrap|''y'' &lt; 0}}. If {{nowrap|''y'' &lt; ''x''&lt;sup&gt;3&lt;/sup&gt;}} and {{nowrap|''y'' &gt; 0}} then each point (''x'',''y'') has exactly three distinct tangent lines to γ passing through it. The same is true if {{nowrap|''y'' &gt; ''x''&lt;sup&gt;3&lt;/sup&gt;}} and {{nowrap|''y'' &lt; 0}}. If {{nowrap|1=''y'' = ''x''&lt;sup&gt;3&lt;/sup&gt;}} and {{nowrap|''y'' ≠ 0}} then each point (''x'',''y'') has exactly two tangent lines to γ passing through it (this corresponds to the cubic having one ordinary root and one repeated root). The same is true if {{nowrap|''y'' ≠ ''x''&lt;sup&gt;3&lt;/sup&gt;}} and {{nowrap|1=''y'' = 0}}. If {{nowrap|1=''y'' = ''x''&lt;sup&gt;3&lt;/sup&gt;}} and {{nowrap|1=''x'' = 0}}, i.e., {{nowrap|1=''x'' = ''y'' = 0}}, then this point has a single tangent line to γ passing through it (this corresponds to the cubic having one real root of multiplicity 3). It follows that
:&lt;math&gt;E_3 = \varnothing. &lt;/math&gt;

===Example 2===
[[File:Envelope_string_art.svg|right|thumb|This plot gives the envelope of the family of lines connecting points (''t'',0), (0,''k'' - ''t''), in which ''k'' takes 1.]]
In [[string art]] it is common to cross-connect two lines of equally spaced pins. What curve is formed?

For simplicity, set the pins on the ''x''- and ''y''-axes; a non-[[orthogonal]] layout is a [[Coordinate rotation|rotation]] and [[scaling (geometry)|scaling]] away. A general straight-line thread connects the two points (0, ''k''&amp;minus;''t'') and (''t'', 0), where ''k'' is an arbitrary scaling constant, and the family of lines is generated by varying the parameter ''t''. From simple geometry, the equation of this straight line is ''y'' = &amp;minus;(''k''&amp;nbsp;&amp;minus;&amp;nbsp;''t'')''x''/''t'' + ''k''&amp;nbsp;&amp;minus;&amp;nbsp;''t''. Rearranging and casting in the form ''F''(''x'',''y'',''t'') = 0 gives:

:{|cellpadding=0 cellspacing=0 border=0 width=75%
|-
| &lt;math&gt;F(x,y,t)=t^2 + t(y-x-k) + kx = 0\,&lt;/math&gt;
| style="text-align:right"|(1)
|}

Now differentiate ''F''(''x'',''y'',''t'') with respect to ''t'' and set the result equal to zero, to get

:{|cellpadding=0 cellspacing=0 border=0 width=75%
|-
| &lt;math&gt;\frac{\partial F(x,y,t)}{\partial t}=2t+ y-x-k = 0\,&lt;/math&gt;
| style="text-align:right"|(2)
|}

These two equations jointly define the equation of the envelope. From (2) we have ''t'' = (&amp;minus;''y'' + ''x'' + ''k'')/2. Substituting this value of ''t'' into (1) and simplifying gives an equation for the envelope in terms of ''x'' and ''y'' only:

: &lt;math&gt;x^2 - 2xy + y^2 -2kx - 2ky + k^2 = 0\,&lt;/math&gt;

This is the familiar implicit [[conic section]] form, in this case a [[parabola]]. Parabolae remain parabolae under rotation and scaling; thus the string art forms a parabolic arc ("arc" since only a portion of the full parabola is produced). In this case an anticlockwise rotation through 45° gives the orthogonal parabolic equation ''y'' = ''x''&lt;sup&gt;2&lt;/sup&gt;/(''k''{{radic|2}}) + ''k''/(2{{radic|2}}). The final step of eliminating ''t'' may not always be possible to do analytically, depending on the form of ''F''(''x'',''y'',''t'').

=== Example 3 ===

Let ''I'' ⊂ '''R''' be an open interval and let γ : ''I'' → '''R'''&lt;sup&gt;2&lt;/sup&gt; be a smooth plane curve parametrised by [[arc length]]. Consider the one-parameter family of normal lines to γ(''I''). A line is normal to γ at γ(''t'') if it passes through γ(''t'') and is perpendicular to the [[Differential geometry of curves#Tangent vector|tangent vector]] to γ at γ(''t''). Let '''T''' denote the unit tangent vector to γ and let '''N''' denote the unit [[Differential geometry of curves#Normal or curvature vector|normal vector]]. Using a dot to denote the [[dot product]], the generating family for the one-parameter family of normal lines is given by {{nowrap|1=''F'' : ''I'' &amp;times; '''R'''&lt;sup&gt;2&lt;/sup&gt; → '''R'''}} where
:&lt;math&gt; F(t,{\mathbf x}) = ({\mathbf x} - \gamma(t)) \cdot {\mathbf T}(t) \ . &lt;/math&gt;
Clearly ('''x''' &amp;minus; γ)·'''T''' = 0 if and only if '''x''' &amp;minus; γ is perpendicular to '''T''', or equivalently, if and only if '''x''' &amp;minus; γ is [[Parallel (geometry)|parallel]] to '''N''', or equivalently, if and only if '''x''' = γ + λ'''N''' for some λ ∈ '''R'''. It follows that
:&lt;math&gt; L_{t_0} := \{ {\mathbf x} \in \R^2 : F(t_0,{\mathbf x}) = 0 \} &lt;/math&gt;
is exactly the normal line to γ at γ(''t''&lt;sub&gt;0&lt;/sub&gt;). To find the discriminant of ''F'' we need to compute its partial derivative with respect to ''t'':
:&lt;math&gt;  \frac{\partial F}{\partial t}(t,{\mathbf x}) = \kappa (t) ({\mathbf x}-\gamma(t))\cdot {\mathbf N}(t) - 1 \ , &lt;/math&gt;
where κ is the [[Curvature#Curvature of plane curves|plane curve curvature]] of γ. It has been seen that ''F'' = 0 if and only if '''x''' - γ = λ'''N''' for some λ ∈ '''R'''. Assuming that ''F'' = 0 gives
:&lt;math&gt; \frac{\partial F}{\partial t} = \lambda \kappa(t) - 1 \ . &lt;/math&gt;
Assuming that κ ≠ 0 it follows that λ = 1/κ and so
:&lt;math&gt; \mathcal{D} = \gamma(t) + \frac{1}{\kappa(t)}{\mathbf N}(t) \ . &lt;/math&gt;
This is exactly the [[evolute]] of the curve γ.

=== Example 4 ===
[[File:Envelope_astroid.svg|right|thumb|An [[astroid]] as the envelope of the family of lines connecting points (''s'',0), (0,''t'') with ''s''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''t''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;1]]
The following example shows that in some cases the envelope of a family of curves may be seen as the topologic boundary of a union of sets, whose boundaries are the curves of the envelope. For &lt;math&gt;s&gt;0&lt;/math&gt; and &lt;math&gt;t&gt;0&lt;/math&gt; consider the (open) right triangle in a Cartesian plane with vertices &lt;math&gt;(0,0)&lt;/math&gt;, &lt;math&gt;(s,0)&lt;/math&gt; and &lt;math&gt;(0,t)&lt;/math&gt;
:&lt;math&gt;T_{s,t}:=\left\{(x,y)\in\R_+^2:\ \frac{x}{s}+\frac{y}{t}&lt;1\right\}.
&lt;/math&gt;
Fix an exponent &lt;math&gt;\alpha&gt;0&lt;/math&gt;, and consider the union of all the triangles &lt;math&gt;T_{s,t} &lt;/math&gt; subjected to the constraint &lt;math&gt;\textstyle s^\alpha+t^\alpha=1 &lt;/math&gt;, that is the open set
:&lt;math&gt;\Delta_\alpha:=\bigcup_ {s^\alpha+t^\alpha=1}  T_{s,t}.&lt;/math&gt;
To write a Cartesian representation for &lt;math&gt;\textstyle\Delta_\alpha&lt;/math&gt;, start with any &lt;math&gt;\textstyle s&gt;0&lt;/math&gt;, &lt;math&gt;\textstyle t&gt;0&lt;/math&gt; satisfying &lt;math&gt;\textstyle s^\alpha+t^\alpha=1 &lt;/math&gt; and any &lt;math&gt;\textstyle(x,y)\in\R_+^2&lt;/math&gt;. The [[Hölder inequality#Notable special cases|Hölder inequality]] in &lt;math&gt;\textstyle\R^2&lt;/math&gt; with respect to the conjugated exponents &lt;math&gt;p:=1+\frac{1}{\alpha}&lt;/math&gt; and &lt;math&gt;\textstyle q:={1+\alpha}&lt;/math&gt; gives:
:&lt;math&gt;x^\frac{\alpha}{\alpha+1}+y^\frac{\alpha}{\alpha+1}\leq \left(\frac{x}{s}+\frac{y}{t}\right)^\frac{\alpha}{\alpha+1}\Big(s^\alpha+t^\alpha\Big)^\frac{1}{\alpha+1}=\left(\frac{x}{s}+\frac{y}{t}\right)^\frac{\alpha}{\alpha+1}&lt;/math&gt;,
with equality if and only if &lt;math&gt;\textstyle s:\,t=x^\frac{1}{1+\alpha}:\,y^\frac{1}{1+\alpha}&lt;/math&gt;.
In terms of a union of sets the latter inequality reads: the point &lt;math&gt;(x,y)\in\R_+^2&lt;/math&gt; belongs to the set &lt;math&gt;\textstyle\Delta_\alpha&lt;/math&gt;, that is, it belongs to some &lt;math&gt;\textstyle T_{s,t}&lt;/math&gt; with &lt;math&gt;\textstyle s^\alpha+t^\alpha=1&lt;/math&gt;, if and only if it satisfies
:&lt;math&gt;x^\frac{\alpha}{\alpha+1}+y^\frac{\alpha}{\alpha+1}&lt;1.&lt;/math&gt;
Moreover, the boundary in &lt;math&gt;\R_+^2&lt;/math&gt; of the set &lt;math&gt;\textstyle \Delta_\alpha&lt;/math&gt; is the envelope of the corresponding family of line segments
:&lt;math&gt;\left\{(x,y)\in\R_+^2:\ \frac{x}{s}+\frac{y}{t}=1\right\}\ ,\qquad s^\alpha+t^\alpha=1&lt;/math&gt;
(that is, the hypotenuses of the triangles), and has Cartesian equation
:&lt;math&gt;x^\frac{\alpha}{\alpha+1}+y^\frac{\alpha}{\alpha+1}=1.&lt;/math&gt;
Notice that, in particular, the value &lt;math&gt;\alpha=1&lt;/math&gt; gives the arc of parabola of the example 1, and the value &lt;math&gt;\alpha=2&lt;/math&gt; (meaning that all hypotenuses are unit length segments) gives the [[astroid]].

===Example 5===
[[File:Envelope cast.svg|thumbnail|The orbits' envelope of the projectiles (with constant initial speed) is a concave parabola. The initial speed is 10 m/s. We take ''g'' = 10 m/s&lt;sup&gt;2&lt;/sup&gt;.]]
We consider the following example of envelope in motion. Suppose at initial height 0, one casts a [[Trajectory of a projectile|projectile]] into the air with constant initial velocity ''v'' but different elevation angles θ. Let ''x'' be the horizontal axis in the motion surface, and let ''y'' denote the vertical axis. Then the motion gives the following differential [[dynamical system]]:
:&lt;math&gt;\frac{d^2 y}{dt^2} = -g,\; \frac{d^2 x}{dt^2} = 0, &lt;/math&gt;
which satisfies four [[initial condition]]s:
:&lt;math&gt;\frac{dx}{dt}\bigg|_{t=0} = v \cos \theta,\; \frac{dy}{dt}\bigg|_{t=0} = v \sin \theta,\; x\bigg|_{t=0} = y\bigg|_{t=0} = 0.&lt;/math&gt;
Here ''t'' denotes motion time, θ is elevation angle, ''g'' denotes [[gravitational acceleration]], and ''v'' is the constant initial speed (not [[velocity]]). The solution of the above system can take an [[implicit function|implicit form]]:
:&lt;math&gt;F(x,y,\theta) = x\tan \theta - \frac{gx^2}{2v^2 \cos^2 \theta} - y = 0.&lt;/math&gt;
To find its envelope equation, one may compute the desired derivative:
:&lt;math&gt;\frac{\partial F}{\partial \theta} = \frac{x}{\cos^2 \theta} - \frac{gx^2 \tan \theta}{v^2 \cos^2 \theta} = 0.&lt;/math&gt;
By eliminating θ, one may reach the following envelope equation:
:&lt;math&gt;y = \frac{v^2}{2g} - \frac{g}{2v^2}x^2.&lt;/math&gt;
Clearly the resulted envelope is also a [[concave function|concave]] [[parabola]].

==Envelope of a family of surfaces==
A '''one-parameter family of surfaces''' in three-dimensional Euclidean space is given by a set of equations

:&lt;math&gt;F(x,y,z,a)=0&lt;/math&gt;

depending on a real parameter ''a''.&lt;ref&gt;{{Citation |first=Luther P. |last=Eisenhart |title=A Treatise on the Differential Geometry of Curves and Surfaces |publisher=Schwarz Press |year=2008 |ISBN=1-4437-3160-9}}&lt;/ref&gt; For example, the tangent planes to a surface along a curve in the surface form such a family.

Two surfaces corresponding to different values ''a'' and ''a' '' intersect in a common curve defined by

:&lt;math&gt; F(x,y,z,a)=0,\,\,{F(x,y,z,a^\prime)-F(x,y,z,a)\over a^\prime -a}=0.&lt;/math&gt;

In the limit as ''a' '' approaches ''a'', this curve tends to a curve contained in the surface at ''a''

:&lt;math&gt; F(x,y,z,a)=0,\,\,{\partial F\over \partial a}(x,y,z,a)=0.&lt;/math&gt;

This curve is called the '''characteristic''' of the family at ''a''. As ''a'' varies the locus of these characteristic curves defines a surface called the '''envelope''' of the family of surfaces.

{{quotation|The envelope of a family of surfaces is tangent to each surface in the family along the characteristic curve in that surface.}}

==Generalisations==

The idea of an envelope of a family of smooth submanifolds follows naturally. In general, if we have a family of submanifolds with codimension ''c'' then we need to have at least a ''c''-parameter family of such submanifolds. For example: a one-parameter family of curves in three-space (''c'' = 2) does not, generically, have an envelope.

==Applications==

=== Ordinary differential equations ===
Envelopes are connected to the study of [[ordinary differential equation]]s (ODEs), and in particular [[singular solution]]s of ODEs.&lt;ref&gt;{{Citation | last1=Forsyth | first1=Andrew Russell | title=Theory of differential equations | publisher=[[Dover Publications]] | location=New York | series=Six volumes bound as three | mr=0123757 | year=1959}}, §§100-106.&lt;/ref&gt; Consider, for example, the one-parameter family of tangent lines to the parabola ''y'' = ''x''&lt;sup&gt;2&lt;/sup&gt;. These are given by the generating family {{nowrap|1=''F''(''t'',(''x'',''y'')) = ''t''&lt;sup&gt;2&lt;/sup&gt; – 2''tx'' + ''y''}}. The zero level set {{nowrap|1=''F''(''t''&lt;sub&gt;0&lt;/sub&gt;,(''x'',''y'')) = 0}} gives the equation of the tangent line to the parabola at the point (''t''&lt;sub&gt;0&lt;/sub&gt;,''t''&lt;sub&gt;0&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt;). The equation {{nowrap|1=''t''&lt;sup&gt;2&lt;/sup&gt; – 2''tx'' + ''y'' = 0}} can always be solved for ''y'' as a function of ''x'' and so, consider
:&lt;math&gt; t^2 - 2tx + y(x) = 0. \ &lt;/math&gt;
Substituting
:&lt;math&gt; t = \left(\frac{dy}{dx}\right)/2 &lt;/math&gt;
gives the ODE
:&lt;math&gt; \left(\frac{dy}{dx}\right)^2 \!\! - 4x\frac{dy}{dx} + 4y = 0. &lt;/math&gt;
Not surprisingly ''y''&amp;nbsp;=&amp;nbsp;2''tx''&amp;nbsp;&amp;minus;&amp;nbsp;''t''&lt;sup&gt;2&lt;/sup&gt; are all solutions to this ODE. However, the envelope of this one-parameter family of lines, which is the parabola ''y''&amp;nbsp;=&amp;nbsp;''x''&lt;sup&gt;2&lt;/sup&gt;, is also a solution to this ODE. Another famous example is [[Clairaut's equation]].

=== Partial differential equations ===
Envelopes can be used to construct more complicated solutions of first order [[partial differential equation]]s (PDEs) from simpler ones.&lt;ref&gt;{{Citation | last1=Evans | first1=Lawrence C. | title=Partial differential equations | publisher=[[American Mathematical Society]] | location=Providence, R.I. | isbn=978-0-8218-0772-9 | year=1998}}.&lt;/ref&gt;  Let ''F''(''x'',''u'',D''u'')&amp;nbsp;=&amp;nbsp;0 be a first order PDE, where ''x'' is a variable with values in an open set Ω&amp;nbsp;⊂&amp;nbsp;'''R'''&lt;sup&gt;''n''&lt;/sup&gt;, ''u'' is an unknown real-valued function, D''u'' is the [[gradient]] of ''u'', and ''F'' is a continuously differentiable function that is regular in D''u''.  Suppose that ''u''(''x'';''a'') is an ''m''-parameter family of solutions:  that is, for each fixed ''a''&amp;nbsp;∈&amp;nbsp;''A''&amp;nbsp;⊂&amp;nbsp;'''R'''&lt;sup&gt;''m''&lt;/sup&gt;, ''u''(''x'';''a'') is a solution of the differential equation.  A new solution of the differential equation can be constructed by first solving (if possible)
:&lt;math&gt;D_a u(x;a) = 0\,&lt;/math&gt;
for ''a''&amp;nbsp;=&amp;nbsp;φ(''x'') as a function of ''x''.  The envelope of the family of functions {''u''(·,''a'')}&lt;sub&gt;''a''∈''A''&lt;/sub&gt; is defined by
:&lt;math&gt;v(x) = u(x;\varphi(x)),\quad x\in\Omega,&lt;/math&gt;
and also solves the differential equation (provided that it exists as a continuously differentiable function).

Geometrically, the graph of ''v''(''x'') is everywhere tangent to the graph of some member of the family ''u''(''x'';''a''). Since the differential equation is first order, it only puts a condition on the tangent plane to the graph, so that any function everywhere tangent to a solution must also be a solution. The same idea underlies the solution of a first order equation as an integral of the [[Monge cone]].&lt;ref&gt;{{citation |first=Fritz |last=John |authorlink=Fritz John |title=Partial differential equations |publisher=Springer |edition=4th |year=1991 |isbn=978-0-387-90609-6}}.&lt;/ref&gt;  The Monge cone is a cone field in the '''R'''&lt;sup&gt;''n''+1&lt;/sup&gt; of the (''x'',''u'') variables cut out by the envelope of the tangent spaces to the first order PDE at each point. A solution of the PDE is then an envelope of the cone field.

In [[Riemannian geometry]], if a smooth family of [[geodesic]]s through a point ''P'' in a [[Riemannian manifold]] has an envelope, then ''P'' has a [[conjugate point]] where any geodesic of the family intersects the envelope. The same is true more generally in the [[calculus of variations]]: if a family of extremals to a functional through a given point ''P'' has an envelope, then a point where an extremal intersects the envelope is a conjugate point to ''P''.

=== Caustics ===
[[Image:Circle caustic.png|thumb|Reflective caustic generated from a [[circle]] and parallel rays]]
In [[geometrical optics]], a [[Caustic (optics)|caustic]] is the envelope of a family of [[ray (optics)|light rays]]. In this picture there is an [[circular arc|arc]] of a circle. The light rays (shown in blue) are coming from a source ''at infinity'', and so arrive parallel. When they hit the circular arc the light rays are scattered in different directions according to the [[Specular reflection|law of reflection]]. When a light ray hits the arc at a point the light will be reflected as though it had been reflected by the arc's [[tangent line]] at that point. The reflected light rays give a one-parameter family of lines in the plane. The envelope of these lines is the [[Caustic (optics)|reflective caustic]]. A reflective caustic will generically consist of [[smooth curve|smooth]] points and [[cusp (singularity)|ordinary cusp]] points.

From the point of view of the calculus of variations, [[Fermat's principle]] (in its modern form) implies that light rays are the extremals for the length functional
:&lt;math&gt;L[\gamma] = \int_a^b |\gamma'(t)|\,dt&lt;/math&gt;
among smooth curves γ on [''a'',''b''] with fixed endpoints γ(''a'') and γ(''b'').  The caustic determined by a given point ''P'' (in the image the point is at infinity) is the set of conjugate points to ''P''.&lt;ref&gt;{{Citation | last1=Born | first1=Max | author1-link=Max Born | title=Principle of Optics | publisher=[[Cambridge University Press]] | isbn=978-0-521-64222-4 | date=October 1999 }}, Appendix I: The calculus of variations.&lt;/ref&gt;

===Huygens's principle===
Light may pass through anisotropic inhomogeneous media at different rates depending on the direction and starting position of a light ray.  The boundary of the set of points to which light can travel from a given point '''q''' after a time ''t'' is known as the [[wave front]] after time ''t'', denoted here by Φ&lt;sub&gt;'''q'''&lt;/sub&gt;(''t'').  It consists of precisely the points that can be reached from '''q''' in time ''t'' by travelling at the speed of light.  [[Huygens–Fresnel principle|Huygens's principle]] asserts that the wave front set {{nowrap|&amp;Phi;&lt;sub&gt;'''q'''&lt;sub&gt;0&lt;/sub&gt;&lt;/sub&gt;(''s'' + ''t'')}} is the envelope of the family of wave fronts {{nowrap|&amp;Phi;&lt;sub&gt;'''q'''&lt;/sub&gt;(''s'')}} for '''q'''&amp;nbsp;∈&amp;nbsp;Φ&lt;sub&gt;'''q'''&lt;sub&gt;0&lt;/sub&gt;&lt;/sub&gt;(''t''). More generally, the point '''q'''&lt;sub&gt;0&lt;/sub&gt; could be replaced by any curve, surface or closed set in space.&lt;ref&gt;{{Citation | last1=Arnold | first1=V. I. | author1-link=Vladimir Arnold | title=Mathematical Methods of Classical Mechanics, 2nd ed. | publisher=[[Springer-Verlag]] | location=Berlin, New York | isbn=978-0-387-96890-2 | year=1997}}, §46.&lt;/ref&gt;

==See also==
* [[Ruled surface]]
* [[Caustic (mathematics)]]

==References==
{{Reflist}}

==External links==
* {{Britannica|189107|Envelope (Mathematics)}}
*{{MathWorld|title=Envelope|urlname=Envelope}}
*[http://mathcurve.com/courbes2d/enveloppe/enveloppe.shtml "Envelope of a family of plane curves" at MathCurve.]

{{Differential transforms of plane curves}}

{{DEFAULTSORT:Envelope (Mathematics)}}
[[Category:Differential geometry]]
[[Category:Analytic geometry]]</text>
      <sha1>lwvo7om6or1t4cegoxe0l69pnh6a77e</sha1>
    </revision>
  </page>
  <page>
    <title>FKG inequality</title>
    <ns>0</ns>
    <id>20283423</id>
    <revision>
      <id>846587287</id>
      <parentid>841682764</parentid>
      <timestamp>2018-06-19T17:53:38Z</timestamp>
      <contributor>
        <username>Bibcode Bot</username>
        <id>14394459</id>
      </contributor>
      <minor/>
      <comment>Adding 0 [[arXiv|arxiv eprint(s)]], 5 [[bibcode|bibcode(s)]] and 0 [[digital object identifier|doi(s)]]. Did it miss something? Report bugs, errors, and suggestions at [[User talk:Bibcode Bot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15238">In mathematics, the '''Fortuin–Kasteleyn–Ginibre (FKG) inequality''' is a [[correlation]] inequality, a fundamental tool in [[statistical mechanics]] and [[Combinatorics#Probabilistic combinatorics|probabilistic combinatorics]] (especially [[random graph]]s and the [[probabilistic method]]), due to {{harvs | last1=Fortuin | author1-link=Cees M. Fortuin | first1=Cees M. | last2=Kasteleyn | author2-link=Pieter Kasteleyn | first2=Pieter W. | last3=Ginibre | author3-link=Jean Ginibre | first3=Jean | title=Correlation inequalities on some partially ordered sets | url=http://projecteuclid.org/euclid.cmp/1103857443 | mr=0309498 | year=1971 | journal=Communications in Mathematical Physics   | volume=22 | pages=89–103|txt}}. Informally, it says that in many random systems, increasing events are positively correlated, while an increasing and a decreasing event are negatively correlated.

An earlier version, for the special case of [[i.i.d.]] variables, called '''Harris inequality''', is due to {{harvs|last=[[Ted Harris (mathematician)|Harris]] |first=Theodore Edward|year=1960|txt}}, see [[#A special case: the Harris inequality|below]]. One generalization of the FKG inequality is the [[#A generalization: the Holley inequality|Holley inequality (1974)]] below, and an even further generalization is the [[Ahlswede–Daykin inequality|Ahlswede–Daykin "four functions" theorem (1978)]]. Furthermore, it has the same conclusion as the [[Griffiths inequalities]], but the hypotheses are different.

==The inequality==
Let &lt;math&gt;X&lt;/math&gt; be a finite [[distributive lattice]], and ''μ'' a nonnegative function on it, that is assumed to satisfy the ('''FKG''') '''lattice condition''' (sometimes a function satisfying this condition is called '''log supermodular''') i.e.,
:&lt;math&gt;\mu(x\wedge y)\mu(x\vee y) \ge \mu(x)\mu(y)&lt;/math&gt;
for all ''x'', ''y'' in the lattice &lt;math&gt;X&lt;/math&gt;.

The FKG inequality then says that for any two monotonically increasing functions ''ƒ'' and ''g'' on &lt;math&gt;X&lt;/math&gt;, the following positive correlation inequality holds:
:&lt;math&gt; \left(\sum _{x\in X}f(x)g(x)\mu(x)\right)\left(\sum _{x\in X}\mu(x)\right) \ge \left(\sum _{x\in X}f(x)\mu(x)\right)\left(\sum _{x\in X}g(x)\mu(x)\right).&lt;/math&gt;

The same inequality (positive correlation) is true when both ''ƒ'' and ''g'' are decreasing.  If one is increasing and the other is decreasing, then they are negatively correlated and the above inequality is reversed.

Similar statements hold more generally, when &lt;math&gt;X&lt;/math&gt; is not necessarily finite, not even countable. In that case, ''μ'' has to be a finite measure, and the lattice condition has to be defined using [[cylinder (algebra)|cylinder]] events; see, e.g., Section 2.2 of {{harvtxt|Grimmett|1999}}.

For proofs, see the original {{harvtxt|Fortuin|Kasteleyn|Ginibre|1971}} or the [[Ahlswede–Daykin inequality|Ahlswede–Daykin inequality (1978)]]. Also, a rough sketch is given below, due to {{harvtxt|Holley|1974}}, using a [[Markov chain]] [[coupling (probability)|coupling]] argument.

==Variations on terminology==

The lattice condition for ''μ'' is also called '''multivariate total positivity''', and sometimes the '''strong FKG condition'''; the term ('''multiplicative''') '''FKG condition''' is also used in older literature.

The property of ''μ'' that increasing functions are positively correlated is also called having '''positive associations''', or the '''weak FKG condition'''.

Thus, the FKG theorem can be rephrased as "the strong FKG condition implies the weak FKG condition".

==A special case: the Harris inequality==

If the lattice &lt;math&gt;X&lt;/math&gt; is [[totally ordered]], then the lattice condition is satisfied trivially for any measure ''μ''. For this case, the FKG inequality is [[Chebyshev's sum inequality]]: if the two increasing functions take on values &lt;math&gt;a_1\leq a_2 \leq \cdots \leq a_n&lt;/math&gt; and &lt;math&gt;b_1\leq b_2 \leq \cdots \leq b_n&lt;/math&gt;,  then (we may assume that the measure ''μ'' is uniform) 
:&lt;math&gt;\frac{a_1b_1+\cdots+a_nb_n}{n} \geq \frac{a_1+\cdots+a_n}{n} \; \frac{b_1+\cdots+b_n}{n}.&lt;/math&gt;

More generally, for any probability measure ''μ'' on &lt;math&gt;\R&lt;/math&gt; and increasing functions ''ƒ'' and ''g'', 
:&lt;math&gt; \int_\R f(x)g(x) \,d\mu(x) \geq \int_\R f(x)\,d\mu(x) \, \int_\R g(x)\,d\mu(x),&lt;/math&gt;
which follows immediately from
:&lt;math&gt;\int_\R\int_\R [f(x)-f(y)][g(x)-g(y)]\,d\mu(x)\,d\mu(y) \geq 0.&lt;/math&gt;

The lattice condition is trivially satisfied also when the lattice is the product of totally ordered lattices, &lt;math&gt;X=X_1\times\cdots\times X_n&lt;/math&gt;, and &lt;math&gt;\mu=\mu_1\otimes\cdots\otimes\mu_n&lt;/math&gt; is a product measure. Often all the factors (both the lattices and the measures) are identical, i.e., ''μ'' is the probability distribution of [[i.i.d.]] random variables.

The FKG inequality for the case of a product measure is known also as  the '''Harris inequality''' after [[Ted Harris (mathematician)|Harris]]  {{harv|Harris|1960}}, who found and used it in his study of [[percolation theory|percolation]] in the plane.  A proof of the Harris inequality that uses the above double integral trick on &lt;math&gt;\R&lt;/math&gt; can be found, e.g., in Section 2.2 of {{harvtxt|Grimmett|1999}}.

===Simple examples===

A typical example is the following. Color each hexagon of the infinite [[honeycomb lattice]] black with probability &lt;math&gt;p&lt;/math&gt; and white with probability &lt;math&gt;1-p&lt;/math&gt;, independently of each other. Let ''a, b, c, d'' be four hexagons, not necessarily distinct. Let &lt;math&gt;a \leftrightarrow b&lt;/math&gt; and &lt;math&gt;c\leftrightarrow d&lt;/math&gt; be the events that there is a black path from ''a'' to ''b'', and a black path from ''c'' to ''d'', respectively. Then the Harris inequality says that these events are positively correlated: &lt;math&gt;\Pr(a \leftrightarrow b,\ c\leftrightarrow d) \geq \Pr(a \leftrightarrow b)\Pr(c\leftrightarrow d)&lt;/math&gt;. In other words, assuming the presence of one path can only increase the probability of the other.

Similarly, if we randomly color the hexagons inside an &lt;math&gt;n\times n&lt;/math&gt; rhombus-shaped [[hex (board game)|hex board]], then the events that there is black crossing from the left side of the board to the right side is positively correlated with having a black crossing  from the top side to the bottom. On the other hand, having a left-to-right black crossing is negatively correlated with having a top-to-bottom white crossing, since the first is an increasing event (in the amount of blackness), while the second is decreasing. In fact, in any coloring of the hex board exactly one of these two events happen — this is why hex is a well-defined game.

In the [[Erdos–Renyi model|Erdős–Rényi random graph]], the existence of a [[Hamiltonian cycle]] is negatively correlated with the [[graph coloring|3-colorability of the graph]], since the first is an increasing event, while the latter is decreasing.

==Examples from statistical mechanics==
In statistical mechanics, the usual source of measures that satisfy the lattice condition (and hence the FKG inequality) is the following:

If &lt;math&gt;S&lt;/math&gt; is an ordered set (such as &lt;math&gt;\{-1,+1\}&lt;/math&gt;), and &lt;math&gt;\Gamma&lt;/math&gt; is a finite or infinite [[Graph (discrete mathematics)|graph]], then the set &lt;math&gt;S^\Gamma&lt;/math&gt; of &lt;math&gt;S&lt;/math&gt;-valued configurations is a [[poset]] that is a distributive lattice.

Now, if &lt;math&gt;\Phi&lt;/math&gt; is a '''submodular [[Gibbs measure|potential]]''' (i.e., a family of functions
:&lt;math&gt;\Phi_\Lambda: S^\Lambda \longrightarrow \R\cup\{\infty\},&lt;/math&gt;
one for each finite &lt;math&gt;\Lambda \subset \Gamma&lt;/math&gt;, such that each &lt;math&gt;\Phi_\Lambda&lt;/math&gt; is [[submodular]]), then one defines the corresponding [[Gibbs measure|Hamiltonian]]s as
:&lt;math&gt;H_\Lambda(\varphi):=\sum_{\Delta\cap\Lambda\not=\emptyset} \Phi_\Delta(\varphi).&lt;/math&gt;

If ''μ'' is an [[Gibbs measure|extremal Gibbs measure]] for this Hamiltonian on the set of configurations &lt;math&gt;\varphi&lt;/math&gt;, then it is easy to show that ''μ'' satisfies the lattice condition, see {{harvtxt|Sheffield|2005}}.

A key example is the [[Ising model]] on a graph &lt;math&gt;\Gamma&lt;/math&gt;. Let &lt;math&gt;S=\{-1,+1\}&lt;/math&gt;, called spins, and &lt;math&gt;\beta\in [0,\infty]&lt;/math&gt;. Take the following potential:

:&lt;math&gt;\Phi_\Lambda(\varphi)=\begin{cases} 
\beta 1_{\{\varphi(x)\not=\varphi(y)\}} &amp; \text{if }\Lambda=\{x,y\}\text{ is a pair of adjacent vertices of }\Gamma;\\
0 &amp; \text{otherwise.}\end{cases}
&lt;/math&gt;

Submodularity is easy to check; intuitively, taking the min or the max of two configurations tends to decrease the number of disagreeing spins. Then, depending on the graph &lt;math&gt;\Gamma&lt;/math&gt; and the value of &lt;math&gt;\beta&lt;/math&gt;, there could be one or more extremal Gibbs measures, see, e.g., {{harvtxt|Georgii|Häggström|Maes|2001}} and {{harvtxt|Lyons|2000}}.

==A generalization: the Holley inequality==

The '''Holley inequality''', due to {{harvs|last=Holley|first=Richard|year=1974|txt}}, states that the [[expected value|expectations]]
:&lt;math&gt; \langle f\rangle_i = \frac{\sum _{x\in X}f(x)\mu_i(x)}{\sum_{x\in X}\mu_i(x)} &lt;/math&gt;

of a monotonically increasing function ''ƒ'' on a finite [[distributive lattice]] &lt;math&gt;X&lt;/math&gt; with respect to two positive functions ''μ''&lt;sub&gt;1&lt;/sub&gt;, ''μ''&lt;sub&gt;2&lt;/sub&gt; on the lattice  satisfy the condition

:&lt;math&gt; \langle f\rangle_1 \ge \langle f\rangle_2, &lt;/math&gt;

provided the functions satisfy the '''Holley condition''' ('''criterion''')

:&lt;math&gt;\mu_2(x\wedge y)\mu_1(x\vee y) \ge \mu_1(x)\mu_2(y)&lt;/math&gt;

for all ''x'', ''y'' in the lattice.

To recover the [[#The inequality|FKG inequality]]: If ''μ'' satisfies the lattice condition and ''ƒ'' and ''g'' are increasing functions on &lt;math&gt;X&lt;/math&gt;, then ''μ''&lt;sub&gt;1&lt;/sub&gt;(''x'')&amp;nbsp;=&amp;nbsp;''g''(''x'')''μ''(''x'')  and ''μ''&lt;sub&gt;2&lt;/sub&gt;(''x'')&amp;nbsp;=&amp;nbsp;''μ''(''x'') will satisfy the lattice-type condition of the Holley inequality. Then the Holley inequality states that

:&lt;math&gt; \frac{ \langle fg\rangle_\mu }{\langle g\rangle_\mu} = \langle f\rangle_1 \ge \langle f\rangle_2 =\langle f\rangle_\mu, &lt;/math&gt;

which is just the FKG inequality.

As for FKG, the Holley inequality follows from the [[Ahlswede–Daykin inequality]].

==Weakening the lattice condition: monotonicity==
Consider the usual case of &lt;math&gt;X&lt;/math&gt; being a product &lt;math&gt;\R^V&lt;/math&gt; for some finite set &lt;math&gt;V&lt;/math&gt;. The lattice condition on ''μ'' is easily seen to imply the following '''monotonicity''', which has the virtue that it is often easier to check than the lattice condition:

Whenever one fixes a vertex &lt;math&gt;v \in V&lt;/math&gt; and two configurations ''φ'' and ''ψ'' outside ''v'' such that &lt;math&gt;\varphi(w) \geq \psi(w)&lt;/math&gt; for all &lt;math&gt;w\not=v&lt;/math&gt;, the ''μ''-conditional distribution of ''φ''(''v'') given &lt;math&gt;\{\varphi(w) : w\not=v\}&lt;/math&gt; [[stochastic ordering|stochastically dominates]] the ''μ''-conditional distribution of ''ψ''(''v'') given &lt;math&gt;\{\psi(w) : w\not=v\}&lt;/math&gt;.

Now, if ''μ'' satisfies this monotonicity property, that is already enough for the FKG inequality (positive associations) to hold.

Here is a rough sketch of the proof, due to {{harvtxt|Holley|1974}}: starting from any initial configuration on &lt;math&gt;V&lt;/math&gt;, one can run a simple [[Markov chain]] (the [[Metropolis algorithm]]) that uses independent Uniform[0,1] random variables to update the configuration in each step, such that the chain has a unique stationary measure, the given ''μ''. The monotonicity of ''μ'' implies that the configuration at each step is a monotone function of independent variables, hence the [[#A special case: the Harris inequality|product measure version of Harris]] implies that it has positive associations. Therefore, the limiting stationary measure ''μ'' also has this property.

The monotonicity property has a natural version for two measures, saying that ''μ''&lt;sub&gt;1&lt;/sub&gt; conditionally pointwise dominates ''μ''&lt;sub&gt;2&lt;/sub&gt;. It is again easy to see that if ''μ''&lt;sub&gt;1&lt;/sub&gt; and ''μ''&lt;sub&gt;2&lt;/sub&gt; satisfy the lattice-type condition of the [[#A generalization: the Holley inequality|Holley inequality]], then  ''μ''&lt;sub&gt;1&lt;/sub&gt; conditionally pointwise dominates ''μ''&lt;sub&gt;2&lt;/sub&gt;. On the other hand, a Markov chain [[coupling (probability)|coupling]] argument similar to the above, but now without invoking the Harris inequality, shows that conditional pointwise domination, in fact,  implies [[stochastic ordering|stochastically domination]]. Stochastic domination is equivalent to saying that &lt;math&gt; \langle f\rangle_1 \ge \langle f\rangle_2&lt;/math&gt; for all increasing ''ƒ'', thus we get a proof of the Holley inequality. (And thus also a proof of the FKG inequality, without using the Harris inequality.)

See {{harvtxt|Holley|1974}} and {{harvtxt|Georgii|Häggström|Maes|2001}} for details.

==See also==
*[[Ahlswede–Daykin inequality]]

==References==
*{{springer|id=f/f110120|first=P.C.|last= Fishburn}}
*{{Citation | last1=Fortuin | first1=C. M. | last2=Kasteleyn | first2=P. W. | last3=Ginibre | first3=J. | title=Correlation inequalities on some partially ordered sets | url=http://projecteuclid.org/euclid.cmp/1103857443 | mr=0309498 | year=1971 | journal=Communications in Mathematical Physics   | volume=22 | pages=89–103 | doi=10.1007/BF01651330 | issue=2| bibcode=1971CMaPh..22...89F }}
*{{cite book |last1=Friedli |first=S. |last2=Velenik |first2=Y. |title=Statistical Mechanics of Lattice Systems: a Concrete Mathematical Introduction |publisher=Cambridge University Press |location=Cambridge |year=2017 |isbn=9781107184824 |url=http://www.unige.ch/math/folks/velenik/smbook/index.html}} 
*{{Citation | first1=H-O.|last1=Georgii| last2=Häggström| first2=O.| last3=Maes| first3=C.| chapter=The random geometry of equilibrium phases| arxiv=math/9905031| year=2001|title=[[Phase transitions and critical phenomena]] |volume=18|pages=1–142|publisher=Academic Press, San Diego, CA|mr=2014387| doi=10.1016/S1062-7901(01)80008-2}}
*{{Citation | last=Grimmett | first=G. R. | title=Percolation. Second edition| publisher=Springer-Verlag | year=1999|mr=1707339| isbn=3-540-64902-6 | doi=10.1007/978-3-662-03981-6}}
*{{Citation | last=Harris | first=T. E. | title=A lower bound for the critical probability in a certain percolation | year=1960 | journal=Proceedings of the Cambridge Philosophical Society | volume=56 | pages=13–20|mr=0115221 | doi=10.1017/S0305004100034241| bibcode=1960PCPS...56...13H }}
*{{Citation | last1=Holley | first1=R. | title=Remarks on the  FKG inequalities | url=http://projecteuclid.org/euclid.cmp/1103859732 | mr=0341552 | year=1974 | journal=Communications in Mathematical Physics   | volume=36 | pages=227–231 | doi=10.1007/BF01645980 | issue=3| bibcode=1974CMaPh..36..227H }}
*{{citation|last=Lyons|first=R.|year=2000|title=Phase transitions on nonamenable graphs|arxiv=math/9908177|journal=J. Math. Phys.|volume=41|pages=1099–1126|mr=1757952|doi=10.1063/1.533179|issue=3|bibcode=2000JMP....41.1099L}}
*{{Citation|last=Sheffield|first=S.|title=Random surfaces|arxiv=math/0304049|journal=Asterisque|volume=304|year=2005| mr=2251117|bibcode=2003math......4049S}}

{{DEFAULTSORT:Fkg Inequality}}
[[Category:Inequalities]]
[[Category:Statistical mechanics]]
[[Category:Covariance and correlation]]</text>
      <sha1>c4y3kke0ovsv6vn6hkd6b6445ai803s</sha1>
    </revision>
  </page>
  <page>
    <title>FL (complexity)</title>
    <ns>0</ns>
    <id>1151048</id>
    <revision>
      <id>665263506</id>
      <parentid>636072788</parentid>
      <timestamp>2015-06-03T01:57:36Z</timestamp>
      <contributor>
        <username>SJ Defender</username>
        <id>19403234</id>
      </contributor>
      <minor/>
      <comment>Disambiguating links to [[Memory space]] (link changed to [[Memory space (computational resource)]]) using [[User:Qwertyytrewqqwerty/DisamAssist|DisamAssist]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2351">In [[computational complexity theory]], the [[complexity class]] '''FL''' is the set of [[function problem]]s which can be solved by a [[deterministic Turing machine]] in a [[logarithm]]ic amount of [[Memory space (computational resource)|memory space]].&lt;ref name="abj"&gt;{{citation|contribution=Functional oracle queries as a measure of parallel time|first1=Carme|last1=Àlvarez|first2=José L.|last2=Balcázar|first3=Birgit|last3=Jenner|title=STACS 91|series=Lecture Notes in Computer Science|volume=480|year=1991|pages=422–433|publisher=Springer|doi=10.1007/BFb0020817}}.&lt;/ref&gt; As in the definition of [[L (complexity)|L]], the machine reads its input from a read-only tape and writes its output to a write-only tape; the logarithmic space restriction applies only to the read/write working tape.

Loosely speaking, a function problem takes a complicated input and produces a (perhaps equally) complicated output.  Function problems are distinguished from [[decision problem]]s, which produce only Yes or No answers and corresponds to the set '''[[L (complexity)|L]]''' of [[decision problem]]s which can be solved in deterministic logspace.  '''FL''' is a subset of '''[[FP (complexity)|FP]]''', the set of function problems which can be solved in deterministic [[polynomial time]].&lt;ref name="abj"/&gt;

'''FL''' is known to contain several natural problems, including arithmetic on numbers.  Addition, subtraction and multiplication of two numbers are fairly simple, but achieving division is a far deeper problem which was open for decades.&lt;ref&gt;{{citation|first1=A.|last1=Chiu|first2=G.|last2=Davida|first3=B.|last3=Litow|title=Division in logspace-uniform NC1|journal=RAIRO Theoretical Informatics and Applications|volume=35|pages=259–276|year=2001}}.&lt;/ref&gt;&lt;ref&gt;{{citation|contribution-url=http://ftp.cs.rutgers.edu/pub/allender/division.eatcs.pdf|contribution=The division breakthroughs|first=Eric|last=Allender|authorlink=Eric Allender|title=Current Trends in Theoretical Computer Science|publisher=World Scientific|year=2004|pages=147–164}}.&lt;/ref&gt;

Similarly one may define '''FNL''', which has the same relation with [[NL (complexity)|NL]] as [[FNP (complexity)|FNP]] has with [[NP (complexity)|NP]].&lt;ref name="abj"/&gt;

== References ==
{{reflist}}

{{DEFAULTSORT:Fl (Complexity)}}
[[Category:Complexity classes]]


{{comp-sci-theory-stub}}</text>
      <sha1>449bkezyry8s1nad94eggh9eybp01ht</sha1>
    </revision>
  </page>
  <page>
    <title>Finite difference method</title>
    <ns>0</ns>
    <id>6054681</id>
    <revision>
      <id>855335905</id>
      <parentid>853796801</parentid>
      <timestamp>2018-08-17T15:17:46Z</timestamp>
      <contributor>
        <username>Netrapt</username>
        <id>381341</id>
      </contributor>
      <minor/>
      <comment>fix misspelling typo</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="18425">{{distinguish|text="finite difference method based on variation principle", the first name of [[finite element method]]{{citation needed|date=August 2013}}}}
{{Multiple issues|
{{lead too short|date=April 2015}}
{{technical|date=April 2015}}
}}
{{Differential equations}}

In [[mathematics]], '''finite-difference methods''' (FDM) are [[numerical methods]] for solving [[differential equations]] by approximating them with [[Recurrence relation#Relationship to difference equations narrowly defined|difference equations]], in which [[Finite difference approximation|finite differences approximate]] the [[derivative]]s. FDMs are thus [[discretization]] methods.

Today, FDMs are the dominant approach to numerical solutions of [[partial differential equation]]s.&lt;ref name="GrossmannRoos2007"&gt;{{cite book|author1=Christian Grossmann|author2=Hans-G. Roos|author3=Martin Stynes|title=Numerical Treatment of Partial Differential Equations|year=2007|publisher=Springer Science &amp; Business Media|isbn=978-3-540-71584-9|page=23}}&lt;/ref&gt;

== Derivation from Taylor's polynomial ==

First, assuming the function whose derivatives are to be approximated is properly-behaved, by [[Taylor's theorem]], we can create a [[Taylor series]] expansion

:&lt;math&gt;f(x_0 + h) = f(x_0) + \frac{f'(x_0)}{1!}h + \frac{f^{(2)}(x_0)}{2!}h^2 + \cdots + \frac{f^{(n)}(x_0)}{n!}h^n + R_n(x),&lt;/math&gt;

where ''n''! denotes the [[factorial]] of ''n'', and ''R''&lt;sub&gt;''n''&lt;/sub&gt;(''x'') is a remainder term, denoting the difference between the Taylor polynomial of degree ''n'' and the original function. We will derive an approximation for the first derivative of the function "f" by first truncating the Taylor polynomial:

:&lt;math&gt;f(x_0 + h) = f(x_0) + f'(x_0)h + R_1(x),&lt;/math&gt;

Setting, x&lt;sub&gt;0&lt;/sub&gt;=a we have,

:&lt;math&gt;f(a+h) = f(a) + f'(a)h + R_1(x),&lt;/math&gt;

Dividing across by ''h'' gives:

:&lt;math&gt;{f(a+h)\over h} = {f(a)\over h} + f'(a)+{R_1(x)\over h} &lt;/math&gt;

Solving for f'(a):

:&lt;math&gt;f'(a) = {f(a+h)-f(a)\over h} - {R_1(x)\over h}&lt;/math&gt;

Assuming that &lt;math&gt;R_1(x)&lt;/math&gt; is sufficiently small, the approximation of the first derivative of "f" is:

:&lt;math&gt;f'(a)\approx {f(a+h)-f(a)\over h}.&lt;/math&gt;

== Accuracy and order ==
{{see also|Finite difference coefficient}}

The error in a method's solution is defined as the difference between the approximation and the exact analytical solution. The two sources of error in finite difference methods are [[round-off error]], the loss of precision due to computer rounding of decimal quantities, and [[truncation error]] or [[discretization error]], the difference between the exact solution of the original differential equation and the exact quantity assuming perfect arithmetic (that is, assuming no round-off).

[[File:Finite Differences.svg|right|thumb|The finite difference method relies on discretizing a function on a grid.]]
To use a finite difference method to approximate the solution to a problem, one must first discretize the problem's domain. This is usually done by dividing the domain into a uniform grid (see image to the right). Note that this means that finite-difference methods produce sets of discrete numerical approximations to the derivative, often in a "time-stepping" manner.

An expression of general interest is the [[local truncation error]] of a method. Typically expressed using [[Big-O notation]], local truncation error refers to the error from a single application of a method. That is, it is the quantity &lt;math&gt;f'(x_i) - f'_i&lt;/math&gt; if &lt;math&gt;f'(x_i)&lt;/math&gt; refers to the exact value and &lt;math&gt;f'_i&lt;/math&gt; to the numerical approximation. The remainder term of a Taylor polynomial is convenient for analyzing the local truncation error. Using the Lagrange form of the remainder from the Taylor polynomial for &lt;math&gt;f(x_0 + h)&lt;/math&gt;, which is

&lt;math&gt;
  R_n(x_0 + h) = \frac{f^{(n+1)}(\xi)}{(n+1)!} (h)^{n+1}
&lt;/math&gt;, where &lt;math&gt; x_0 &lt; \xi &lt; x_0 + h&lt;/math&gt;,

the dominant term of the local truncation error can be discovered. For example, again using the forward-difference formula for the first derivative, knowing that &lt;math&gt;f(x_i)=f(x_0+i h)&lt;/math&gt;,

:&lt;math&gt; f(x_0 + i h) = f(x_0) + f'(x_0)i h + \frac{f''(\xi)}{2!} (i h)^{2}, &lt;/math&gt;

and with some algebraic manipulation, this leads to

:&lt;math&gt; \frac{f(x_0 + i h) - f(x_0)}{i h} = f'(x_0) + \frac{f''(\xi)}{2!} i h, &lt;/math&gt;

and further noting that the quantity on the left is the approximation from the finite difference method and that the quantity on the right is the exact quantity of interest plus a remainder, clearly that remainder is the local truncation error. A final expression of this example and its order is:

:&lt;math&gt; \frac{f(x_0 + i h) - f(x_0)}{i h} = f'(x_0) + O(h). &lt;/math&gt;

This means that, in this case, the local truncation error is proportional to the step sizes. The quality and duration of simulated FDM solution depends on the discretization equation selection and the step sizes (time and space steps). The data quality and simulation duration increase significantly with smaller step size.&lt;ref name=IserlesA_1996&gt;{{cite book|author1=Arieh Iserles|title= A first course in the numerical analysis of differential equations |year=2008|publisher= Cambridge University Press |isbn=9780521734905|page=23}}&lt;/ref&gt; Therefore, a reasonable balance between data quality and simulation duration is necessary for practical usage. Large time steps are useful for increasing simulation speed in practice. However, time steps which are too large may create instabilities and affect the data quality.&lt;ref name=HoffmanJD_2001&gt;{{cite book|author1=Hoffman JD|author2=Frankel S|year=2001|title=Numerical methods for engineers and scientists|publisher=CRC Press, Boca Raton}}&lt;/ref&gt;&lt;ref name=Jaluria_CM_1994&gt;{{cite journal|author1=Jaluria Y|author2=Atluri S|year=1994|title=Computational heat transfer|journal= Computational Mechanics|volume=14|pages=385–386|doi=10.1007/BF00377593}}&lt;/ref&gt;

The [[von Neumann stability analysis|von Neumann method]] is usually applied to determine the numerical model stability.&lt;ref name=HoffmanJD_2001 /&gt;&lt;ref name=Jaluria_CM_1994 /&gt;&lt;ref&gt;{{cite book|author1=Majumdar P|year=2005|title=Computational methods for heat and mass transfer|edition=1st|publisher=Taylor and Francis, New York}}&lt;/ref&gt;&lt;ref name="SmithGD_1985"&gt;{{cite book|author1=Smith GD|year=1985|title=Numerical solution of partial differential equations: finite difference methods|publisher=Oxford University Press|edition=3rd}}&lt;/ref&gt;

== Example: ordinary differential equation ==
For example, consider the ordinary differential equation
:&lt;math&gt; u'(x) = 3u(x) + 2. \, &lt;/math&gt;
The [[Euler method]] for solving this equation uses the finite difference quotient
:&lt;math&gt;\frac{u(x+h) - u(x)}{h} \approx u'(x)&lt;/math&gt;
to approximate the differential equation by first substituting it for u'(x) then applying a little algebra (multiplying both sides by h, and then adding u(x) to both sides) to get
:&lt;math&gt; u(x+h) = u(x) + h(3u(x)+2). \, &lt;/math&gt;
The last equation is a finite-difference equation, and solving this equation gives an approximate solution to the differential equation.

==  Example: The heat equation ==

Consider the normalized [[heat equation]] in one dimension, with homogeneous [[Dirichlet boundary condition]]s

:&lt;math&gt; U_t=U_{xx} \, &lt;/math&gt;
:&lt;math&gt; U(0,t)=U(1,t)=0 \, &lt;/math&gt;  (boundary condition)
:&lt;math&gt; U(x,0) =U_0(x) \, &lt;/math&gt;   (initial condition)

One way to numerically solve this equation is to approximate all the derivatives by finite differences. We partition the domain in space using a mesh &lt;math&gt; x_0, ..., x_J &lt;/math&gt; and in time using a mesh &lt;math&gt; t_0, ...., t_N &lt;/math&gt;. We assume a uniform partition both in space and in time, so the difference between two consecutive space points will be ''h'' and between two consecutive time points will be ''k''. The points

:&lt;math&gt; u(x_j,t_n) = u_{j}^n &lt;/math&gt;

will represent the numerical approximation of &lt;math&gt; u(x_j, t_n). &lt;/math&gt;

===Explicit method===
[[File:Explicit method-stencil.svg|right|thumb|The [[Stencil (numerical analysis)|stencil]] for the most common explicit method for the heat equation.]]
Using a [[forward difference]] '''at time &lt;math&gt; t_n &lt;/math&gt;''' and a second-order [[central difference]] for the space derivative at position &lt;math&gt; x_j &lt;/math&gt; ([[FTCS scheme|FTCS]]) we get the recurrence equation:

:&lt;math&gt; \frac{u_{j}^{n+1} - u_{j}^{n}}{k} = \frac{u_{j+1}^n - 2u_{j}^n + u_{j-1}^n}{h^2}. \, &lt;/math&gt;

This is an [[explicit method]] for solving the one-dimensional [[heat equation]].

We can obtain &lt;math&gt; u_j^{n+1} &lt;/math&gt; from the other values this way:

:&lt;math&gt; u_{j}^{n+1} = (1-2r)u_{j}^{n} + ru_{j-1}^{n} + ru_{j+1}^{n}  &lt;/math&gt;

where &lt;math&gt; r=k/h^2. &lt;/math&gt;

So, with this recurrence relation, and knowing the values at time ''n'', one can obtain the corresponding values at time ''n''+1. &lt;math&gt; u_0^n &lt;/math&gt; and &lt;math&gt; u_J^n &lt;/math&gt; must be replaced by the boundary conditions, in this example they are both 0.

This explicit method is known to be [[numerically stable]] and [[limit of a sequence|convergent]] whenever &lt;math&gt; r\le 1/2 &lt;/math&gt;.&lt;ref&gt;Crank, J. ''The Mathematics of Diffusion''. 2nd Edition, Oxford, 1975, p. 143.&lt;/ref&gt; The numerical errors are proportional to the time step and the square of the space step:
:&lt;math&gt; \Delta u = O(k)+O(h^2)  \, &lt;/math&gt;

===Implicit method===
[[File:Implicit method-stencil.svg|right|thumb|The implicit method stencil.]]
If we use the [[backward difference]] '''at time &lt;math&gt; t_{n+1} &lt;/math&gt;''' and a second-order central difference for the space derivative at position &lt;math&gt; x_j &lt;/math&gt; (The Backward Time, Centered Space Method "BTCS") we get the recurrence equation:

:&lt;math&gt; \frac{u_{j}^{n+1} - u_{j}^{n}}{k} =\frac{u_{j+1}^{n+1} - 2u_{j}^{n+1} + u_{j-1}^{n+1}}{h^2}. \, &lt;/math&gt;

This is an [[implicit method]] for solving the one-dimensional [[heat equation]].

We can obtain &lt;math&gt; u_j^{n} &lt;/math&gt; from solving a system of linear equations:

:&lt;math&gt; (1+2r)u_j^{n+1} - ru_{j-1}^{n+1} - ru_{j+1}^{n+1}= u_{j}^{n} &lt;/math&gt;

The scheme is always [[numerically stable]] and convergent but usually more numerically intensive than the explicit method as it requires solving a system of numerical equations on each time step. The errors are  linear over the time step and quadratic over the space step:
:&lt;math&gt; \Delta u = O(k)+O(h^2).  \, &lt;/math&gt;

===Crank&amp;ndash;Nicolson method===
Finally if we use the central difference at time &lt;math&gt; t_{n+1/2} &lt;/math&gt; and a second-order central difference for the space derivative at position &lt;math&gt; x_j &lt;/math&gt; ("CTCS") we get the recurrence equation:

:&lt;math&gt; \frac{u_j^{n+1} - u_j^{n}}{k} = \frac{1}{2} \left(\frac{u_{j+1}^{n+1} - 2u_j^{n+1} + u_{j-1}^{n+1}}{h^2}+\frac{u_{j+1}^{n} - 2u_j^{n} + u_{j-1}^{n}}{h^2}\right).\, &lt;/math&gt;

This formula is known as the [[Crank&amp;ndash;Nicolson method]].
[[File:Crank-Nicolson-stencil.svg|right|thumb|The Crank&amp;ndash;Nicolson stencil.]]

We can obtain &lt;math&gt; u_j^{n+1} &lt;/math&gt; from solving a system of linear equations:

:&lt;math&gt; (2+2r)u_j^{n+1} - ru_{j-1}^{n+1} - ru_{j+1}^{n+1}= (2-2r)u_j^n + ru_{j-1}^n + ru_{j+1}^n &lt;/math&gt;

The scheme is always [[numerically stable]] and convergent but usually more numerically intensive as it requires solving a system of numerical equations on each time step. The errors are quadratic over both the time step and the space step:
:&lt;math&gt; \Delta u = O(k^2)+O(h^2).  \, &lt;/math&gt;

Usually the Crank&amp;ndash;Nicolson scheme is the most accurate scheme for small time steps. The explicit scheme is the least accurate and can be unstable, but is also the easiest to implement and the least numerically intensive. The implicit scheme works the best for large time steps.

===Comparison===
The figures below present the solutions given by the above methods to approximate the heat equation 

: &lt;math&gt;U_t = \alpha U_{xx}, \quad \alpha = \frac{1}{\pi^2},&lt;/math&gt;

with the boundary condition

: &lt;math&gt;U(0, t) = U(1, t) = 0.&lt;/math&gt;

The exact solution is

:&lt;math&gt;U(x, t) = \frac{1}{\pi^2}e^{-t}\sin(\pi x).&lt;/math&gt;

{{multiple image
&lt;!-- Essential parameters --&gt;
| perrow = 3
| align     = center
| direction = horizontal
| width     = 260
&lt;!-- Extra parameters --&gt;
| header = Comparison of Finite Difference Methods
| header_align = center
| header_background = 
| footer = 
| footer_align = 
| footer_background = 
| background color =

|image1=HeatEquationExplicitApproximate.svg
|width1=260
|caption1=&lt;div class="center" style="width:auto; margin-left:auto; margin-right:auto;"&gt;Explicit method (''not'' stable)&lt;/div&gt;
|alt1= ''c'' = 4

|image2=HeatEquationImplicitApproximate.svg
|width2=260
|caption2=&lt;div class="center" style="width:auto; margin-left:auto; margin-right:auto;"&gt;Implicit method (stable)&lt;/div&gt;
|alt2= ''c'' = 6

|image3=HeatEquationCNApproximate.svg
|width3=260
|caption3=&lt;div class="center" style="width:auto; margin-left:auto; margin-right:auto;"&gt;Crank-Nicolson method (stable)&lt;/div&gt;
|alt3= ''c'' = 8.5
}}

== Example: The Laplace operator ==
The (continuous) [[Laplace operator]] in &lt;math&gt; n &lt;/math&gt;-dimensions is given by &lt;math&gt; \Delta u(x) = \sum_{i=1}^n \partial_i^2 u(x) &lt;/math&gt;.
The discrete Laplace operator &lt;math&gt; \Delta_h u &lt;/math&gt; depends on the dimension &lt;math&gt; n &lt;/math&gt;.

In 1D the Laplace operator is approximated as 
:&lt;math&gt;
  \Delta u(x) = u''(x)
    \approx \frac{u(x-h)-2u(x)+u(x+h)}{h^2 }
    =: \Delta_h u(x) \,.
&lt;/math&gt;
This approximation is usually expressed via the following [[Stencil_(numerical_analysis)|stencil]]
:&lt;math&gt;
  \frac{1}{h^2} 
  \begin{bmatrix}
    1 &amp; -2 &amp; 1
  \end{bmatrix}
  \,.
&lt;/math&gt;

The 2D case shows all the characteristics of the more general nD case. Each second partial derivative needs to be approximated similar to the 1D case 
:&lt;math&gt;
  \begin{align}
  \Delta u(x,y) &amp;= u_{xx}(x,y)+u_{yy}(x,y) \\
    &amp;\approx \frac{u(x-h,y)-2u(x,y)+u(x+h,y) }{h^2}
      + \frac{u(x,y-h) -2u(x,y) +u(x,y+h)}{h^2} \\
    &amp;= \frac{u(x-h,y)+u(x+h,y) -4u(x,y)+u(x,y-h)+u(x,y+h)}{h^2} \\
    &amp;=: \Delta_h u(x, y) \,,
  \end{align}
&lt;/math&gt;
which is usually given by the following [[Stencil_(numerical_analysis)|stencil]]
:&lt;math&gt;
  \frac{1}{h^2} 
  \begin{bmatrix}
      &amp; 1 \\
    1 &amp; -4 &amp; 1 \\
      &amp; 1 
  \end{bmatrix}
  \,.
&lt;/math&gt;

=== Consistency ===
Consistency of the above-mentioned approximation can be shown for highly regular functions, such as &lt;math&gt; u \in C^4(\Omega) &lt;/math&gt;.
The statement is
:&lt;math&gt;
  \Delta u - \Delta_h u = \mathcal{O}(h^2) \,.
&lt;/math&gt;

To proof this one needs to substitute [[Taylor Series]] expansions up to order 3 into the discrete Laplace operator.

=== Properties ===
==== Subharmonic ====
Similar to [[Harmonic_function|continuous subharmonic functions]] one can define ''subharmonic functions'' for finite-difference approximations &lt;math&gt;u_h&lt;/math&gt;
:&lt;math&gt;
  -\Delta_h u_h \leq 0 \,.
&lt;/math&gt;

==== Mean value ====
One can define a general [[Stencil_(numerical_analysis)|stencil]] of ''positive type'' via
:&lt;math&gt;  
  \begin{bmatrix}
      &amp; \alpha_N \\
    \alpha_W &amp; -\alpha_C &amp; \alpha_E \\
      &amp; \alpha_S
  \end{bmatrix}
  \,, \quad \alpha_i &gt;0\,, \quad \alpha_C = \sum_{i\in \{N,E,S,W\}} \alpha_i \,.
&lt;/math&gt;

If &lt;math&gt; u_h &lt;/math&gt; is (discrete) subharmonic then the following'' mean value property'' holds
:&lt;math&gt;
  u_h(x_C) \leq \frac{ \sum_{i\in \{N,E,S,W\}} \alpha_i u_h(x_i) }{ \sum_{i\in \{N,E,S,W\}} \alpha_i } \,,
&lt;/math&gt;
where the approximation is evaluated on points of the grid, and the stencil is assumed to be of positive type.

A similar [[Harmonic_function#The_mean_value_property|mean value property]] also holds for the continuous case.

==== Maximum principle ====
For a (discrete) subharmonic function &lt;math&gt; u_h &lt;/math&gt; the following holds
:&lt;math&gt;
  \max_{\Omega_h} u_h \leq \max_{\partial \Omega_h} u_h \,,
&lt;/math&gt;
where &lt;math&gt; \Omega_h, \partial\Omega_h &lt;/math&gt; are discretizations of the continuous domain &lt;math&gt; \Omega &lt;/math&gt;, respectively the boundary &lt;math&gt; \partial \Omega &lt;/math&gt;.

A similar [[Harmonic_function#Maximum_principle|maximum principle]] also holds for the continuous case.

==See also==
{{Div col|colwidth=20em}}
* [[Finite element method]]
* [[Finite difference]]
* [[Finite difference time domain]]
* [[Infinite difference method]]
* [[Stencil (numerical analysis)]]
* [[Finite difference coefficients]]
* [[Five-point stencil]]
* [[Lax–Richtmyer theorem]]
* [[Finite difference methods for option pricing]]
* [[Upwind differencing scheme for convection]]
* [[Central differencing scheme]]
* [[Discrete Poisson equation]]
* [[Discrete Laplace operator]]
{{div col end}}

==References==
{{Reflist}}
* K.W. Morton and D.F. Mayers, ''Numerical Solution of Partial Differential Equations, An Introduction''. Cambridge University Press, 2005.
* Autar Kaw and E. Eric Kalu, ''Numerical Methods with Applications'', (2008) [http://www.autarkaw.com/books/numericalmethods/index.html]. Contains a brief, engineering-oriented introduction to FDM (for ODEs) in [http://numericalmethods.eng.usf.edu/topics/finite_difference_method.html Chapter 08.07].
* {{cite book|author=John Strikwerda|title=Finite Difference Schemes and Partial Differential Equations|year=2004|publisher=SIAM|isbn=978-0-89871-639-9|edition=2nd}}
* {{Citation
 | last = Smith
 | first = G. D.
 | title = Numerical Solution of Partial Differential Equations: Finite Difference Methods, 3rd ed.
 | publisher = Oxford University Press
 | year = 1985
}}
* {{cite book|author=Peter Olver|author-link=Peter J. Olver|title=Introduction to Partial Differential Equations|year=2013|publisher=Springer|isbn=978-3-319-02099-0|at=Chapter 5: Finite differences|url=http://www.math.umn.edu/~olver/pde.html}}.
* [[Randall J. LeVeque]], ''[http://faculty.washington.edu/rjl/fdmbook/ Finite Difference Methods for Ordinary and Partial Differential Equations]'', SIAM, 2007.

=== Various lectures and lecture notes ===
* [https://web.archive.org/web/20140302111344/http://emlab.utep.edu/ee5390cem.htm Finite-Difference Method in Electromagnetics (see and listen to lecture 9)]
*[https://web.archive.org/web/20090206071754/http://ltl.iams.sinica.edu.tw/document/training_lectures/2006/SH_Chen/Finite_Difference_Methods.pdf Lecture Notes] Shih-Hung Chen, [[National Central University]]
*[http://jwhaverkort.net23.net/documents/NumMethPDEs.pdf Numerical Methods for time-dependent Partial Differential Equations]

{{Numerical PDE}}

{{Authority control}}
{{DEFAULTSORT:Finite Difference Method}}
[[Category:Finite differences]]
[[Category:Numerical differential equations]]</text>
      <sha1>fai72nlzdyx525e59jj8v1h9zzjinc9</sha1>
    </revision>
  </page>
  <page>
    <title>Formula calculator</title>
    <ns>0</ns>
    <id>17136006</id>
    <revision>
      <id>811069987</id>
      <parentid>811068577</parentid>
      <timestamp>2017-11-19T09:57:16Z</timestamp>
      <contributor>
        <username>Mean as custard</username>
        <id>10962546</id>
      </contributor>
      <comment>Undid revision 811068577 by [[Special:Contributions/Jinan Cao|Jinan Cao]] ([[User talk:Jinan Cao|talk]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6338">A '''formula calculator''' is a [[software calculator]] that can perform a calculation in two steps:

# Enter the calculation by typing it in from the keyboard.
# Press a single button or key to see the final result.

This is unlike button-operated [[calculators]], such as the [[Windows calculator]] or the [[Calculator (Mac OS)|Mac OS X calculator]], which require the user to perform one step for each operation, by pressing buttons to calculate all the intermediate values, before the final result is shown.

In this context, a formula is also known as an [[Expression (programming)|expression]], and so formula calculators may be called ''expression'' calculators. Also in this context, calculation is known as ''evaluation'', and so they may be called formula ''evaluators'', rather than ''calculators''.

== How they work ==

Formulas as they are commonly written use [[infix notation]] for [[binary operators]], such as addition, multiplication, division and subtraction. This notation also uses:

* [[Parentheses]] to enclose parts of a formula that must be calculated first.
* In the absence of parentheses, [[operator precedence]], so that higher precedence operators, such as multiplication, must be applied before lower precedence operators, such as addition. For example, in 2 + 3*4, the multiplication, 3*4, is done first.
* Among operators with the same precedence, [[associativity]], so that the left-most operator must be applied first. For example, in 2 - 3 + 4, the subtraction, 2 - 3, is done first.

Also, formulas may contain:

* [[Non-commutative]] operators that must be applied to numbers in the correct order, such as subtraction and division.
* The same symbol used for more than one purpose, such as - for negative numbers and subtraction.

Once a formula is entered, a formula calculator follows the above rules to produce the final result by automatically:

* Analysing the formula and breaking it down into its constituent parts, such as operators, numbers and parentheses.
* Finding both [[operands]] of each binary operator.
* Working out the values of these operands.
* Applying the operator to these values, in the correct order so as to allow for non-commutative operators.
* Evaluating the parts of a formula in parentheses first.
* Taking operator precedence and associativity into account.
* Distinguishing between different uses of the same symbol.

== Types of calculator ==

The formula calculator concept can be applied to all types of calculator, including arithmetic, scientific, statistics, financial and conversion calculators.

The calculation can be typed or pasted into an edit box of:

*A software package that runs on a computer, for example as a dialog box.
*An on-line formula calculator hosted on a web site.

It can also be entered on the command line of a programming language.

== Related software packages ==

Although they are not calculators in themselves, because they have a much broader feature set, many software tools have a formula-calculation capability, in that a formula can be typed in and evaluated. These include:

*[[Spreadsheets]], where a formula can be entered to calculate a cell’s content.
*[[Databases]], where a formula can be used to define the value of a calculated field in a record.

== Declarative and imperative tools ==

Button-operated calculators are [[imperative programming|imperative]], because the user must provide details of how the calculation has to be performed.&lt;ref name="Reference 13"&gt;[[Harold Thimbleby]]. A new calculator and why it is necessary, Computing Science, Middlesex University, London, UK, September 1998. Available from: http://www.uclic.ucl.ac.uk/harold/srf/allcalcs.pdf Archived at: [https://web.archive.org/web/20070207044654/http://www.uclic.ucl.ac.uk/harold/srf/allcalcs.pdf web.archive.org]&lt;/ref&gt;

On the other hand, formula calculators are more [[declarative programming|declarative]] because the typed-in formula specifies what to do, and the user does not have to provide any details of the step-by-step order in which the calculation has to be performed.

Declarative solutions are easier to understand than imperative solutions,&lt;ref name=" Reference 13"/&gt;&lt;ref&gt;Roy E. Furman. Declarative Programming - Strategies for Solving Software Problems, http://www.articlesalley.com {{webarchive|url=http://webarchive.loc.gov/all/20121220085209/http%3A//www.articlesalley.com/ |date=2012-12-20 }}, July 2006. Available from: {{cite web |url=http://www.articlesalley.com/article.detail.php/7013/178/Education/Internet/36/Declarative_Programming_-_Strategies_for_Solving_Software_Problems |title=Archived copy |accessdate=2009-05-04 |deadurl=yes |archiveurl=https://archive.is/20120723100846/http://www.articlesalley.com/article.detail.php/7013/178/Education/Internet/36/Declarative_Programming_-_Strategies_for_Solving_Software_Problems |archivedate=2012-07-23 |df= }}&lt;/ref&gt; and so there has been a long-term trend from imperative to declarative methods.&lt;ref&gt;David A. Watt. Programming language concepts and paradigms, Prentice Hall, 1990 (citation 13 at http://citeseer.ist.psu.edu/context/14802/0)&lt;/ref&gt;&lt;ref&gt; Tatsuru Matsushita. Expressive Power of Declarative Programming Languages, PhD thesis, Department of Computer Science, University of York, October 1998 (citation 13 at http://citeseer.ist.psu.edu/context/14802/0)&lt;/ref&gt; Formula calculators are part of this trend.

Many software tools for the general user, such as [[spreadsheets]], are declarative. Formula calculators are examples of such tools.

== Hybrid calculators ==

There are hybrid calculators that combine typed-in formula and button-operated calculation. For example:

*Calculations can be entered entirely from the keyboard, or operations can be applied to typed-in numbers or formulas using buttons, in the same calculator.
*Formulas can be constructed using buttons, rather than being entered from the keyboard.
*Formula copies of button-operated calculations can be created, saved and re-loaded for application to different numbers.

== See also ==

* [[Software calculator]]
* [[Comparison of software calculators]]
* [[Calculator]]
* [[Calculator input methods]]
* [[Programmable calculator]]
* [[Scientific calculator]]

== References ==

&lt;references/&gt;

[[Category:Calculators]]
[[Category:Office equipment]]
[[Category:Mathematical notation]]</text>
      <sha1>mtg1ls0jptp5ixhuirmbckf9939g56k</sha1>
    </revision>
  </page>
  <page>
    <title>Fuss–Catalan number</title>
    <ns>0</ns>
    <id>41436516</id>
    <revision>
      <id>866625483</id>
      <parentid>866619446</parentid>
      <timestamp>2018-10-31T14:13:56Z</timestamp>
      <contributor>
        <username>Citation bot</username>
        <id>7903804</id>
      </contributor>
      <minor/>
      <comment>Add: class, eprint, pmid, issue, isbn. Removed parameters. You can [[WP:UCB|use this bot]] yourself. [[WP:DBUG|Report bugs here]]. | [[User:Headbomb|Headbomb]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11371">{{Underlinked|date=March 2016}}

In [[combinatorics|combinatorial mathematics]] and statistics, the '''Fuss–Catalan''' numbers are numbers of the form

:&lt;math&gt;A_m(p,r)\equiv\frac{r}{mp+r}\binom{mp+r}{m} = \frac{r}{m!}\prod_{i=1}^{m-1}(mp+r-i) = r\frac{\Gamma(mp+r)}{\Gamma(1+m)\Gamma(m(p-1)+r+1)}. &lt;/math&gt;

They are named after [[Nicolas Fuss|N.&amp;nbsp;I.&amp;nbsp;Fuss]] and [[Eugène Charles Catalan]].

In some publications this equation is sometimes referred to as '''Two-parameter Fuss–Catalan numbers''' or '''Raney numbers'''.  The implication is the ''single-parameter Fuss-Catalan numbers'' are when {{math|r}}=1 and {{math|p}}=2.

==Uses==
The Fuss-Catalan represents the number of legal permutations or allowed ways of arranging a number of articles, that is restricted in some way.  This means that they are related to the Binomial Coefficient.  The key difference between Fuss-Catalan and the Binomial Coefficient is that there are no "illegal" arrangement permutations within Binomial Coefficient, but there are within Fuss-Catalan.  An examples of legal and illegal permutations can be better demonstrated by a specific problem such as balanced brackets (see [[Dyck language]]).

A general problem is to count the number of balanced brackets (or legal permutations) that a string of ''m'' open and ''m'' closed brackets forms (total of ''2m'' brackets).  By legally arranged, the following rules apply:
* For the sequence as a whole, the number of open brackets must equal the number of closed brackets
* Working along the sequence, the number of open brackets must be greater than the number of closed brackets
As an numeric example how many combinations can 3 pairs of brackets be legally arranged?  From the Binomial interpretation there are &lt;math&gt;\tbinom {2m}{m}&lt;/math&gt; or numerically &lt;math&gt;\tbinom 63&lt;/math&gt; = 20 ways of arranging 3 open and 3 closed brackets.  However, there are fewer ''legal'' combinations than these when all of the above restrictions apply.  Evaluating these by hand, there are 5 legal combinations, namely: ()()(); (())(); ()(()); (()()); ((())).  This corresponds to the Fuss-Catalan formula when ''p=2, r=1'' which is the [[Catalan number]] formula &lt;math&gt;\tfrac{1}{2m+1}\tbinom{2m}{m}&lt;/math&gt; or &lt;math&gt;\tfrac{1}{4}\tbinom63&lt;/math&gt;=5.  By simple subtraction, there are &lt;math&gt;\tfrac{m}{m+1}\tbinom{2m}{m}&lt;/math&gt; or &lt;math&gt;\tfrac34\tbinom63&lt;/math&gt; =15 illegal combinations.  To further illustrate the subtlety of the problem, if one were to persist with solving the problem just using the Binomial formula, it would be realised that the 2 rules imply that the sequence must start with an open bracket and finish with a closed bracket.  This implies that there are &lt;math&gt;\tbinom{2m-2}{m-1}&lt;/math&gt; or &lt;math&gt;\tbinom42&lt;/math&gt;=6 combinations.  This is inconsistent with the above answer of 5, and the missing combination is: ())((), which is illegal and would complete the binomial interpretation.

Whilst the above is a concrete example Catalan numbers, similar problems can be evaluated using Fuss-Catalan formula:
* '''Computer Stack''': ways of arranging and completing a computer stack of instructions, each time step 1 instruction is processed and p new instructions arrive randomly.  If at the beginning of the sequence there are r instructions outstanding.
* '''Betting''': ways of losing all money when betting.  A player has a total stake pot that allows them to make r bets, and plays a game of chance that pays p times the bet stake.
* '''[[Trie]]s''': Calculating the number of order ''m'' tries on ''n'' nodes.&lt;ref&gt;{{cite thesis| year=1996| first1=David | last1=Clark |title=Compact Pat Trees |url=https://uwspace.uwaterloo.ca/handle/10012/64 | section=Compact Tries | pages=34}}&lt;/ref&gt;

==Special Cases==
:&lt;math&gt;A_0(p,r)=1&lt;/math&gt;;
:&lt;math&gt;A_1(p,r)=r&lt;/math&gt;;
:&lt;math&gt;A_{2}(p,1) = p&lt;/math&gt;.

If &lt;math&gt;p=0&lt;/math&gt;, we recover the [[Binomial coefficient]]s &lt;math&gt;A_m(0,r){{=}}\binom{r}{m}&lt;/math&gt;
:&lt;math&gt;A_m(0,1) = 1,1&lt;/math&gt;;
:&lt;math&gt;A_m(0,2) = 1,2,1&lt;/math&gt;;
:&lt;math&gt;A_m(0,3) = 1,3,3,1&lt;/math&gt;;
:&lt;math&gt;A_m(0,4) = 1,4,6,4,1&lt;/math&gt;.

If &lt;math&gt;p=1&lt;/math&gt;, [[Pascal's Triangle]] appears, read along diagonals:
:&lt;math&gt;A_m(1,1) = 1,1,1,1,1,1,1,1,1,1,\ldots&lt;/math&gt;;
:&lt;math&gt;A_m(1,2) = 1,2,3,4,5,6,7,8,9,10,\ldots&lt;/math&gt;;
:&lt;math&gt;A_m(1,3) = 1,3,6,10,15,21,28,35,45,55,\ldots&lt;/math&gt;;
:&lt;math&gt;A_m(1,4) = 1,4,10,20,35,56,84,120,165,220,\ldots&lt;/math&gt;;
:&lt;math&gt;A_m(1,5) = 1,5,15,35,70,126,210,330,495,715,\ldots&lt;/math&gt;;
:&lt;math&gt;A_m(1,6) = 1,6,21,56,126,252,462,792,1287,2002,\ldots&lt;/math&gt;.

==Examples==
For subindex &lt;math&gt;m\ge 0&lt;/math&gt; the numbers are:

Examples with &lt;math&gt;p=2&lt;/math&gt;:
:&lt;math&gt;A_m(2,1) = 1, 1, 2, 5, 14, 42, 132, 429, 1430, 4862,\ldots&lt;/math&gt; {{OEIS2C|A000108}}, known as  the [[Catalan Numbers]]
:&lt;math&gt;A_m(2,2) = 1, 2, 5, 14, 42, 132, 429, 1430, 4862, 16796, \ldots = A_{m+1}(2,1)&lt;/math&gt;
:&lt;math&gt;A_m(2,3) = 1, 3, 9, 28, 90, 297, 1001, 3432, 11934, 41990, \ldots&lt;/math&gt; {{OEIS2C|A000245}}
:&lt;math&gt;A_m(2,4) = 1, 4, 14, 48, 165, 572, 2002, 7072, 25194, 90440,\ldots&lt;/math&gt; {{OEIS2C|A002057}}

Examples with &lt;math&gt;p=3&lt;/math&gt;:
:&lt;math&gt;A_m(3,1) = 1, 1, 3, 12, 55, 273, 1428, 7752, 43263, 246675, \ldots&lt;/math&gt; {{OEIS2C|A001764}}
:&lt;math&gt;A_m(3,2) = 1, 2, 7, 30, 143, 728, 3876, 21318, 120175, 690690,\ldots &lt;/math&gt; {{OEIS2C|A006013}}
:&lt;math&gt;A_m(3,3) = 1, 3, 12, 55, 273, 1428, 7752, 43263, 246675, 1430715,\ldots = A_{m+1}(3,1)&lt;/math&gt;
:&lt;math&gt;A_m(3,4) = 1, 4, 18, 88, 455, 2448, 13566, 76912, 444015, 2601300,\ldots&lt;/math&gt; {{OEIS2C|A006629}}

Examples with &lt;math&gt;p=4&lt;/math&gt;:
:&lt;math&gt;A_m(4,1) =  1, 1, 4, 22, 140, 969, 7084, 53820, 420732, 3362260,\ldots &lt;/math&gt; {{OEIS2C|A002293}}
:&lt;math&gt;A_m(4,2) =  1, 2, 9, 52, 340, 2394, 17710, 135720, 1068012, 8579560,\ldots &lt;/math&gt; {{OEIS2C|A069271}}
:&lt;math&gt;A_m(4,3) =  1, 3, 15, 91, 612, 4389, 32890, 254475, 2017356, 16301164, \ldots &lt;/math&gt; {{OEIS2C|A006632}}
:&lt;math&gt;A_m(4,4) =  1, 4, 22, 140, 969, 7084, 53820, 420732, 3362260, 27343888,\ldots = A_{m+1}(4,1)&lt;/math&gt;

==Algebra==

===Recurrence===
:&lt;math&gt;A_m(p,r)=A_m(p,r-1)+A_{m-1}(p,p+r-1)&lt;/math&gt; ''equation (1)''
This means in particular that from
:&lt;math&gt;A_m(p,0)=0&lt;/math&gt; ''equation (2)''
and
:&lt;math&gt;A_0(p,r)=1&lt;/math&gt; ''equation (3)''
one can generate all other Fuss–Catalan numbers if {{math|p}} is an integer.

Riordan (see references) obtains a convolution type of recurrence:
:&lt;math&gt;A_m(p,s+r) = \sum_{k=0}^m A_k(p,r)A_{m-k}(p,s)&lt;/math&gt; ''equation(4)''

===Generating Function===
Paraphrasing the ''Densities of the Raney distributions'' &lt;ref&gt;{{cite journal|journal=Docum. Mathm. | year=2013 | pages=1593–1596 | volume=18
| title=Densities of the Raney distributions| first1=Wojciech | last1=Mlotkowski|first2= Karol A. | last2=Penson| first3=Karol |last3=Zyczkowski
|arxiv=1211.7259|bibcode=2012arXiv1211.7259M}}&lt;/ref&gt; paper, let the ordinary [[generating function]] with respect to the index {{math|m}} be defined as follows:
:&lt;math&gt;B_{p,r}(z):=\sum_{m=0}^\infty A_m(p,r)z^m&lt;/math&gt; ''equation (5)''.
Looking at equations (1) and (2), when {{math|r}}=1 it follows that
:&lt;math&gt;A_m(p,p)=A_{m+1}(p,1)&lt;/math&gt; ''equation (6)''.
Also note this result can be derived by similar substitutions into the other formulas representation, such as the Gamma ratio representation at the top of this article.  Using (6) and substituting into (5) an equivalent representation expressed as a generating function can be formulated as
:&lt;math&gt;B_{p,1}(z) = 1+zB_{p,p}(z)&lt;/math&gt;.
Finally, extending this result by using Lambert's equivalence 
:&lt;math&gt;B_{p,1}(z)^r=B_{p,r}(z)&lt;/math&gt;.
The following result can be derived for the ordinary generating function for all the Fuss-Catalan sequences.
:&lt;math&gt;B_{p,r}(z) = [1+zB_{p,r}(z)^{p/r}]^r&lt;/math&gt;.

=== Recursion Representation ===
Recursion forms of this are as follows:
The most obvious form is:
:&lt;math&gt;A_m(p,r) = \frac{m-1}{m}\frac{\binom{mp+r-1}{m-1}}{\binom{(m-1)p+r-1}{m-2}} A_{m-1}(p,r)  &lt;/math&gt;

Also, a less obvious form is
:&lt;math&gt;A_m(p,r) = \frac{(m-1)p+r}{m}\frac{\binom{mp+r-1}{p-1}}{\binom{m(p-1)+r}{p-1}} A_{m-1}(p,r)  &lt;/math&gt;

===Alternate Representations===
In some problems it is easier to use different formula configurations or variations.  Below are a two examples using just the binomial function:
  
:&lt;math&gt;A_m(p,r)\equiv\frac{r}{mp+r}\binom{mp+r}{m} = \frac{r}{m(p-1)+r}\binom{mp+r-1}{m} = \frac{r}{m}\binom{mp+r-1}{m-1}   &lt;/math&gt;

These variants can be converted into product, Gamma or Factorial representations too.

==See also==
* [[Binomial coefficient]]
* [[Binomial Distribution]]
* [[Catalan number]]
* [[Dyck language]]
* [[Pascal's triangle]]

==References==
{{reflist}}

*{{cite journal|first1=N. I. | last1=Fuss | year=1791 | title=Solutio quaestionis, quot modis polygonum n laterum in polygona m laterum, per diagonales resolvi queat | journal=Nova Acta Academiae Scientiarum Imperialis Petropolitanae | volume=9 | pages=243–251 }}
*{{cite book| year=1968| first1=J. | last1=Riordan |title=Combinatorial identities|publisher=Wiley|isbn=978-0471722755}}
*{{cite journal| first1=Dietmar | last1=Bisch | first2=Vaughan | last2=Jones
|title=Algebras associated to intermediate subfactors
|journal=Invent. Math. | year=1997 | volume=128 | number=1| pages=89–157
|doi=10.1007/s002220050137|bibcode = 1997InMat.128...89J }}
*{{cite journal | first1=Jozef H. | last1=Przytycki | first2=Adam S. |last2=Sikora
|title=Polygon dissections and Euler, Fuss, Kirkman , and Cayley Numbers | year=2000
|journal=J. Combinat. Theory A |volume=92 | pages=68–76 | doi =10.1006/jcta.1999.3042}}
*{{cite journal|journal=Discrete Math. | year=2008 | number=308 | volume=20
|pages=4660–4669 | title=Multivariate Fuss-Catalan numbers
|first1=Jean-Christophe | last1=Aval | doi=10.1016/j.disc.2007.08.100}}
*{{cite journal| first1=N. | last1=Alexeev | first2=F | last2=G&amp;ouml;tze | first3=A. 
|last3=Tikhomirov | title=Asymptotic distribution of singular values of powers of random matrices
|year=2010 | journal=Lith. Math. J. | volume=50 | number=2 | pages=121–132 |doi=10.1007/s10986-010-9074-4| arxiv=1012.2743 }}
*{{cite journal|journal=Docum. Mathm. | year=2010 | pages=939–955 | volume=15
| title=Fuss-Catalan Numbers in noncommutative probability | first1=Wojciech | last1=Mlotkowski
|url=https://eudml.org/doc/222801}}
*{{cite journal| first1=Karol A. | last1=Penson | first2=Karol | last2=Zyczkowski
|title=Product of Ginibre matrices: Fuss-Catalan and Raney distributions
|journal=Phys. Rev. E | year=2011 |  volume=83 | issue=6 | page=061118 | doi=10.1103/PhysRevE.83.061118
| pmid=21797313 |bibcode=2011PhRvE..83f1118P |arxiv = 1103.3453 }}
*{{cite journal| year=2012 | first1=Ian G. | last1=Gordon | first2=Stephen | last2=Griffeth
|title=Catalan numbers for complex reflection groups|pages=1491–1502
|journal=Am. J. Math. | volume=134 | number=6 |doi=10.1353/ajm.2012.0047|arxiv=0912.1578}}
*{{cite arxiv| first1=W. | last1= Mlotkowski | first2=K. A. | last2=Penson
|year=2015 | title= A Fuss-type family of positive definite sequences |eprint=1507.07312 | class= math.PR }}
*{{cite journal|year=2017|first1=Tian-Xiao|last1=He|first2=Louis W.|last2=Shapiro
|journal=Lin. Alg. Applic.|volume=532|pages=25–42|doi=10.1016/j.laa.2017.06.025
|title=Fuss-Catalan matrices, their weighted sums, and stabilizer subgroups of the Riordan group}}

{{Classes of natural numbers}}

{{DEFAULTSORT:Catalan Number}}
[[Category:Factorial and binomial topics]]
[[Category:Enumerative combinatorics]]</text>
      <sha1>dx5285a9p5sjapkeayfb4rk59kn9ofo</sha1>
    </revision>
  </page>
  <page>
    <title>Geometric terms of location</title>
    <ns>0</ns>
    <id>42094122</id>
    <revision>
      <id>845280007</id>
      <parentid>810174970</parentid>
      <timestamp>2018-06-10T18:36:14Z</timestamp>
      <contributor>
        <username>UU</username>
        <id>651047</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2178">
{{unreferenced|date=March 2014}}
'''Geometric terms of location''' describe directions or positions relative to the shape of an object. These terms are used in descriptions of engineering, physics, and other sciences, as well as ordinary day to day discourse.

Though these terms by themselves may be somewhat ambiguous, they are usually used in a context in which their meaning is clear. For example, when referring to a [[drive shaft]] it is clear what is meant by axial or radial directions. Or, in a [[free body diagram]], one may similarly infer a sense of orientation by the forces or other vectors represented.

==Examples==
Common geometric terms of location are:
* '''Adjacent''' - next to
* '''Axial''' – along the center of a round body, or the [[axis of rotation]] of a body
* '''Azimuthal''' or '''circumferential''' – following around a curve or [[circumference]] of an object. For instance, the pattern of cells in [[Taylor–Couette flow]] varies along the azimuth of the experiment.
* '''Collinear''' - in the same line
* '''Degree of freedom''' - axis direction, see [[six degrees of freedom]]
* '''Lateral''' – spanning the width of a body. The distinction between width and length may be unclear out of context.
* '''Lineal''' – following along a given path. The shape of the path is not necessarily straight (compare to [[linear]]). For instance, a length of rope might be measured in lineal [[meters]] or [[Foot (unit)|feet]]. See [[arc length]].
* '''Longitudinal''' – spanning the length of a body. 
* '''Orthogonal''' – at right angles to a line, or more generally, on a different axis.
* '''Parallel''' - in the same direction
* '''Perpendicular''' - at right angles to, synonym to orthogonal
* '''Radial''' – along a direction pointing along a [[radius]] from the center of an object, or perpendicular to a curved path.
* '''Tangential''' – intersecting a curve at a point and parallel to the curve at that point.
* '''Transverse''' – orthogonal to a specified direction, such as a particle trajectory or an axis of rotation.
* '''Vertical''' – spanning the length of a body. 

==References==
&lt;references /&gt;

[[Category:Geometry]]</text>
      <sha1>gfsy8272gyqseobiomidicillofj1ge</sha1>
    </revision>
  </page>
  <page>
    <title>Geometry processing</title>
    <ns>0</ns>
    <id>2214847</id>
    <revision>
      <id>871602771</id>
      <parentid>870779123</parentid>
      <timestamp>2018-12-02T06:02:36Z</timestamp>
      <contributor>
        <username>ErisZhang</username>
        <id>35274257</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="28367">[[File:Polygon_Mesh_Processing_Book_Cover.jpg|thumb|right| ''Polygon Mesh Processing'' by Mario Botsch et al. is a textbook on the topic of Geometry Processing.&lt;ref name=PMP2010/&gt;]]

'''Geometry processing''', or [[polygon mesh|mesh]] processing, is an area of [[research]] that uses concepts from [[applied mathematics]], [[computer science]] and [[engineering]] to design efficient [[algorithm]]s for the acquisition, reconstruction, analysis, manipulation, simulation and transmission of complex 3D models.  As the name implies, geometry processing is meant to be analogous to [[signal processing]] and [[image processing]]. Many concepts, data structures and algorithms are directly analogous. For example, where [[image smoothing]] might convolve an intensity signal with a blur kernel formed using the [[Laplace operator]], [[Laplacian smoothing|geometric smoothing]] might be achieved by convolving a [[Surface (mathematics)|surface]] geometry with a blur kernel formed using the [[Laplace-Beltrami operator]].

Applications of geometry processing algorithms already cover a wide range of areas from [[multimedia]], [[entertainment]] and classical [[computer-aided design]], to [[biomedical computing]], [[reverse engineering]] and [[scientific computing]].&lt;ref name=PMP2010&gt;
{{cite book |last=Botsch |first=Mario|last2= Kobbelt |first2=Leif |last3= Pauly |first3=Mark |last4= Alliez |first4=Pierre |date=2010 |title=Polygon Mesh Processing |url= |location= |publisher=[[CRC Press]] |page= |isbn=9781568814261 |author-link= }}&lt;/ref&gt;

Geometry processing is a common research topic at [[SIGGRAPH]], the premier [[computer graphics]] academic conference, and the main topic of the annual [[Symposium on Geometry Processing]].

== Geometry processing as a life cycle ==
[[File:Cactus Gaussian Curvature.png|thumb|A mesh of a cactus showing the Gaussian Curvature at each vertex, using the angle defect method.]]

Geometry processing involves working with a [[shape]], usually in 2D or 3D, although the shape can live in a space of arbitrary dimensions. The processing of a shape involves three stages, which is known as its life cycle. At its "birth," a shape can be instantiated through one of three methods: a [[3D modeling|model]], a [[Function (mathematics)|mathematical representation]], or a [[3D scanner|scan]]. After a shape is born, it can be analyzed and edited repeatedly in a cycle. This usually involves acquiring different measurements, such as the distances between the points of the shape, the smoothness of the shape, or its [[Euler characteristic]]. Editing may involve denoising, deforming, or performing [[Rigid transformation|rigid transformations]]. At the final stage of the shape's "life," it is consumed. This can mean it is consumed by a viewer as a rendered asset in a game or movie, for instance. The end of a shape's life can also be defined by a decision about the shape, like whether or not it satisfies some criteria. Or it can even be [[Digital modeling and fabrication|fabricated]] in the real world, through a method such as 3D printing or laser cutting.

== Discrete Representation of a Shape ==
Like any other shape, the shapes used in geometry processing have properties pertaining to their [[geometry]] and [[topology]]. The geometry of a shape concerns the position of the shape's [[Euclidean space|points in space]], [[tangent]]s, [[Normal (geometry)|normals]], and [[curvature]]. It also includes the dimension in which the shape lives (ex. &lt;math&gt;R^2&lt;/math&gt; or &lt;math&gt;R^3&lt;/math&gt;). The [[topology]] of a shape is a collection of properties that do not change even after smooth transformations have been applied to the shape. It concerns dimensions such as the number of [[Holes (topology)|holes]] and [[Boundary (topology)|boundaries]], as well as the [[orientability]] of the shape. One example of a non-orientable shape is the [[Möbius strip|Mobius strip]].

In computers, everything must be discretized. Shapes in geometry processing are usually represented as [[Triangle mesh|triangle meshes]], which can be seen as a [[graph (discrete mathematics)|graph]]. Each node in the graph is a vertex (usually in &lt;math&gt;R^3&lt;/math&gt;), which has a position. This encodes the geometry of the shape. Directed edges connect these vertices into triangles, which by the right hand rule, then have a direction called the normal. Each triangle forms a face of the mesh. These are combinatoric in nature and encode the topology of the shape. In addition to triangles, a more general class of [[Polygon mesh|polygon meshes]] can also be used to represent a shape. More advanced representations like [[progressive meshes]] encode a coarse representation along with a sequence of transformations, which produce a fine or high resolution representation of the shape once applied. These meshes are useful in a variety of applications, including geomorphs, progressive transmission, mesh compression, and selective refinement.&lt;ref&gt;{{cite web|url=http://hhoppe.com/pm.pdf|title=Progressive Meshes|author=Hugues Hoppe}}&lt;/ref&gt;
[[File:Mesh bunny.png|left|thumb|274x274px|A mesh of the famous Stanford bunny. Shapes are usually represented as a mesh, a collection of polygons that delineate the contours of the shape.]]

=== Properties of a shape ===
==== Euler Characteristic ====
One particularly important property of a 3D shape is its [[Euler characteristic]], which can alternatively be defined in terms of its [[Genus_(mathematics)|genus]]. The formula for this in the continuous sense is &lt;math&gt;\chi= 2c - 2h - b&lt;/math&gt;, where &lt;math&gt;c&lt;/math&gt; is the number of connected components, &lt;math&gt;h&lt;/math&gt; is number of holes (as in donut holes, see [[torus]]), and &lt;math&gt;b&lt;/math&gt; is the number of connected components of the boundary of the surface. A concrete example of this is a mesh of a pair of [[Pair_of_pants_(mathematics)|pants]]. There is one connected component, 0 holes, and 3 connected components of the boundary (the waist and two leg holes). So in this case, the Euler characteristic is -1.To bring this into the discrete world, the Euler characteristic of a mesh is computed in terms of its vertices, edges, and faces. &lt;math&gt;\chi = |V| - |E| + |F|&lt;/math&gt;.

[[File:Pants mesh.png|thumb|This image shows a mesh of a pair of pants, with Euler characteristic -1. This is explained by the equation to compute the characteristic: 2c - 2h - b. The mesh has 1 connected component, 0 topological holes, and 3 boundaries (the waist hole and each leg hole): 2 - 0 - 3 = -1.]]

== Surface reconstruction ==
=== Poisson reconstruction from surface points to mesh ===
[[File:Mesh reconstruction.gif|thumb|339x339px|A triangle mesh is constructed out of a point cloud. Sometimes shapes are initialized only as "point clouds," a collection of sampled points from the shape's surface. Often, these point clouds need to be converted to meshes.|left]]
Depending on how a shape is initialized or "birthed," the shape might exist only as a nebula of sampled points that represent its surface in space. To transform the surface points into a mesh, the Poisson reconstruction&lt;ref name=":0"&gt;{{Cite web|url=http://hhoppe.com/proj/poissonrecon/|title=Poisson surface reconstruction|website=hhoppe.com|language=en-us|access-date=2017-01-26}}&lt;/ref&gt; strategy can be employed. This method states that the [[indicator function]], a function that determines which points in space belong to the surface of the shape, can actually be computed from the sampled points. The key concept is that gradient of the indicator function is ''0'' everywhere, except at the sampled points, where it is equal to the inward surface normal. More formally, suppose the collection of sampled points from the surface is denoted by &lt;math&gt;S&lt;/math&gt;, each point in the space by &lt;math&gt;p_i&lt;/math&gt;, and the corresponding normal at that point by &lt;math&gt;n_i&lt;/math&gt;. Then the gradient of the indicator function is defined as:

&lt;math&gt;\triangledown g = \begin{cases} \textbf{n}_i, &amp; \forall p_i \in S \\ 0, &amp; \text{otherwise}\end{cases}&lt;/math&gt;

The task of reconstruction then becomes a [[Calculus of variations|variational]] problem. To find the indicator function of the surface, we must find a function &lt;math&gt;\chi&lt;/math&gt; such that &lt;math&gt;\lVert \triangledown \chi - \textbf{V} \rVert&lt;/math&gt; is minimized, where &lt;math&gt;\textbf{V}&lt;/math&gt; is the vector field defined by the samples. As a variational problem, one can view the minimizer &lt;math&gt;\chi &lt;/math&gt;as a solution of [[Poisson's equation|Poisson's equation.]]&lt;ref name=":0" /&gt; After obtaining a good approximation for &lt;math&gt;\chi &lt;/math&gt; and a value &lt;math&gt;\sigma&lt;/math&gt;  for which the points &lt;math&gt;(x,y,z)&lt;/math&gt; with &lt;math&gt;\chi(x,y,z)= \sigma&lt;/math&gt; lie on the surface to be reconstructed, the [[marching cubes]] algorithm can be used to construct a [[triangle mesh]] from the function  &lt;math&gt;\chi &lt;/math&gt; , which then be can applied in subsequent computer graphics applications.

== Registration ==
{{multiple image
 | align = right
 | direction = vertical
 | width = 
| image1 = Point-to-point-registration.gif
 | alt1 = Point to point registration
 | caption1 = An animation depicting registration of a partial mesh onto a complete mesh, with piecewise constant approximation of the projection function.
 | image2 = Point-to-Plane Registration.gif
 | alt2 = Point to plane registration
 | caption2 = An animation depicting the same registration procedure as above, but with piecewise linear approximation of the projection function. Note that it converges much faster.
}}

One common problem encountered in geometry processing is how to merge multiple views of a single object captured from different angles or positions. This problem is known as [[Point set registration|registration]]. In registration, we wish to find an optimal [[rigid transformation]] that will align surface &lt;math&gt;X&lt;/math&gt; with surface &lt;math&gt;Y&lt;/math&gt;. More formally, if &lt;math&gt;P_Y(x)&lt;/math&gt; is the projection of a point ''x'' from surface &lt;math&gt;X&lt;/math&gt; onto surface &lt;math&gt;Y&lt;/math&gt;, we want to find the optimal rotation matrix &lt;math&gt;R&lt;/math&gt; and translation vector &lt;math&gt;t&lt;/math&gt; that minimize the following objective function:

&lt;math&gt;\int_{x \in X}||Rx + t - P_Y(x)|| dx&lt;/math&gt;

While rotations are non-linear in general, small rotations can be linearized as skew-symmetric matrices. Moreover, the distance function &lt;math&gt;x - P_Y(x)&lt;/math&gt; is non-linear, but is amenable to linear approximations if the change in &lt;math&gt;X&lt;/math&gt; is small. An iterative solution such as [[Iterative closest point|Iterative Closest Point (ICP)]] is therefore employed to solve for small transformations iteratively, instead of solving for the potentially large transformation in one go. In ICP, ''n'' random sample points from &lt;math&gt;X&lt;/math&gt; are chosen and projected onto &lt;math&gt;Y&lt;/math&gt;. In order to sample points uniformly at random across the surface of the triangle mesh, the random sampling is broken into two stages: uniformly sampling points within a triangle; and non-uniformly sampling triangles, such that each triangle's associated probability is proportional to its surface area.&lt;ref&gt;{{cite web|url=http://www.pcl-users.org/file/n4037867/Rusinkiewicz_Effcient_Variants_of_ICP.pdf|title=Efficient Variants of the ICP Algorithm|author=Szymon Rusinkiewicz, Marc Levoy}}&lt;/ref&gt; Thereafter, the optimal transformation is calculated based on the difference between each &lt;math&gt;x&lt;/math&gt; and its projection. In the following iteration, the projections are calculated based on the result of applying the previous transformation on the samples. The process is repeated until convergence.

== Smoothing ==
When shapes are defined or scanned, there may be accompanying noise, either to a signal acting upon the surface or to the actual surface geometry. Reducing noise on the former is known as [[Noise reduction|data denoising]], while noise reduction on the latter is known as [[surface fairing]]. The task of geometric smoothing is analogous to signal noise reduction, and consequently employs similar approaches.

The pertinent Lagrangian to be minimized is derived by recording the conformity to the initial signal &lt;math&gt;\bar f&lt;/math&gt; and the smoothness of the resulting signal, which approximated by the magnitude of the gradient with a weight &lt;math&gt;\lambda&lt;/math&gt;:

&lt;math&gt;\mathcal{L}(f) = \int_{\Omega}\|f - \bar f\|^2 + \lambda \|\nabla f\|^2 dx&lt;/math&gt;.

Taking a variation &lt;math&gt;\delta f&lt;/math&gt; on &lt;math&gt;\mathcal{L}&lt;/math&gt; emits the necessary condition

&lt;math&gt;0 = \delta\mathcal{L}(f) = \int_{\Omega}\delta f(\mathbf{I} + \lambda \nabla^2) f - \delta f \bar f dx&lt;/math&gt;.

By discretizing this onto piecewise-constant elements with our signal on the vertices we obtain

&lt;math&gt;
\begin{align}
\sum_{i} M_i \delta f_i \bar f_i &amp;=  \sum_i M_i\delta f_i \sum_j (\mathbf{I} + \lambda \nabla^2) f_j =  \sum_i \delta f_i \sum_j (M + \lambda M\nabla^2) f_j,
\end{align}
&lt;/math&gt;[[File:Noisy sphere.gif|thumb|A noisy sphere being iteratively smoothed.|273x273px|left]]where our choice of &lt;math&gt;\nabla^2&lt;/math&gt; is chosen to be &lt;math&gt;M^{-1}\mathbf{L}&lt;/math&gt; for the cotangent Laplacian &lt;math&gt;\mathbf{L}&lt;/math&gt; and the &lt;math&gt;M^{-1}&lt;/math&gt; term is to map the image of the Laplacian from areas to points. Because the variation is free, this results in a self-adjoint linear problem to solve with a parameter &lt;math&gt;\lambda&lt;/math&gt;: &lt;math&gt;
\bar f =(M + \lambda \mathbf{L}) f.
&lt;/math&gt; When working with triangle meshes one way to determine the values of the Laplacian matrix &lt;math&gt;L&lt;/math&gt; is through analyzing the geometry of connected triangles on the mesh.

&lt;math&gt;
L_{ij} =  
\begin{cases} 
      \frac{1}{2}(\cot(\alpha_{ij}) + \cot(\beta_{ij})) &amp; \text{edge ij exists} \\
      -\sum\limits_{i \neq j}L_{ij} &amp; i = j  \\
      0 &amp; \text{otherwise}
     
\end{cases}
&lt;/math&gt;

Where &lt;math&gt;\alpha_{ij}&lt;/math&gt; and &lt;math&gt;\beta_{ij}&lt;/math&gt; are the angles opposite the edge &lt;math&gt;(i,j)&lt;/math&gt;&lt;ref&gt;{{Cite web|url=http://www.ctralie.com/Teaching/LapMesh/|title=Chris Tralie : Laplacian Meshes|website=www.ctralie.com|access-date=2017-03-16}}&lt;/ref&gt;
The [[mass matrix]] M as an operator computes the local integral of a function's value and is often set for a mesh with m triangles as follows:

&lt;math&gt;
M_{ij} =  
\begin{cases} 
      \frac{1}{3}\sum\limits_{t=1}^m\begin{cases}
Area(t) &amp; \text{if triangle t contains vertex i} \\
0 &amp; \text{otherwise} \end{cases} &amp; \text{if i=j} \\
      0 &amp; \text{otherwise} 
\end{cases}
&lt;/math&gt;

== Parameterization ==
Occasionally, we need to flatten a 3D surface onto a flat plane. This process is known as [[Mesh parameterization|parameterization]]. The goal is to find coordinates ''u'' and ''v'' onto which we can map the surface so that distortions are minimized. In this manner, parameterization can be seen as an optimization problem. One of the major applications of mesh parameterization is [[texture mapping]].

=== Mass springs method ===
[[File:Tutte Embedding on Beetle.gif|thumb|378x378px|The Tutte Embedding shows non-smooth parameterizations on the side of the beetle.]]
One way to measure the distortion accrued in the mapping process is to measure how much the length of the edges on the 2D mapping differs from their lengths in the original 3D surface. In more formal terms, the objective function can be written as:

&lt;math&gt;\underset{U}{\text{min}} \sum_{ij \in E} ||u_i - u_j||^2&lt;/math&gt;

Where &lt;math&gt;E&lt;/math&gt; is the set of mesh edges and &lt;math&gt;U&lt;/math&gt; is the set of vertices. However, optimizing this objective function would result in a solution that maps all of the vertices to a single vertex in the ''uv''-coordinates. Borrowing an idea from graph theory, we apply the [[Tutte embedding|Tutte Mapping]] and restrict the boundary vertices of the mesh onto a unit circle or other [[convex polygon]]. Doing so prevents the vertices from collapsing into a single vertex when the mapping is applied. The non-boundary vertices are then positioned at the [[Barycentric coordinate system|barycentric interpolation]] of their neighbours. The Tutte Mapping, however, still suffers from severe distortions as it attempts to make the edge lengths equal, and hence does not correctly account for the triangle sizes on the actual surface mesh.

=== Least-squares conformal mappings ===
[[File:Tutte Vs LSCM .gif|left|thumb|394x394px|A comparison of the Tutte Embedding and Least-Squares-Conformal-Mapping parameterization. Notice how the LSCM parameterization is smooth on the side of the beetle.]]
Another way to measure the distortion is to consider the [[Differential calculus|variations]] on the ''u'' and ''v'' coordinate functions. The wobbliness and distortion apparent in the mass springs methods are due to high variations in the ''u'' and ''v'' coordinate functions. With this approach, the objective function becomes the [[Dirichlet's energy|Dirichlet energy]] on ''u'' and ''v:''

&lt;math&gt;\underset{u,v}{\text{min}} \int_S ||\nabla u||^2 + ||\nabla v||^2 dA&lt;/math&gt;

There are a few other things to consider. We would like to minimize the angle distortion to [[Conformal map|preserve orthogonality]]. That means we would like &lt;math&gt;\nabla u = \nabla v^{\perp}&lt;/math&gt;. In addition, we would also like the mapping to have proportionally similar sized regions as the original. This results to setting the Jacobian of the ''u'' and ''v'' coordinate functions to 1.

&lt;math&gt;\begin{bmatrix}
  \dfrac{\partial u}{\partial x} &amp; \dfrac{\partial u}{\partial y}\\[1em]
  \dfrac{\partial v}{\partial x} &amp; \dfrac{\partial v}{\partial y} \end{bmatrix}
= 1&lt;/math&gt;

Putting these requirements together, we can augment the Dirichlet energy so that our objective function becomes:&lt;ref&gt;{{Cite journal|last=Desbrun|first=Mathieu|year=2002|title=Intrinsic Parameterizations of Surface Meshes|url=http://www.geometry.caltech.edu/pubs/DMA02.pdf|journal=Eurographics|volume=21|pages=|via=}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|last=Levy|first=Bruno|year=2002|title=Least squares conformal maps for automatic texture atlas generation|url=https://members.loria.fr/Bruno.Levy/papers/LSCM_SIGGRAPH_2002.pdf|journal=ACM Transactions on Graphics (TOG) - Proceedings of ACM SIGGRAPH 2002|volume=21|pages=362–371|via=}}&lt;/ref&gt;

&lt;math&gt;\underset{u,v}{\text{min}} \int_S \frac{1}{2} ||\nabla u||^2 + \frac{1}{2} ||\nabla v||^2 - \nabla u \cdot \nabla v^{\perp} &lt;/math&gt;

To avoid the problem of having all the vertices mapped to a single point, we also require that the solution to the optimization problem must have a non-zero norm and that it is orthogonal to the trivial solution.

== Deformation ==
[[File:Asrigidaspossible-man.gif|thumb|394x394px|An example of as-rigid-as-possible deformation.]]
Deformation is concerned with transforming some rest shape to a new shape. Typically, these transformations are continuous and do not alter the topology of the shape. Modern mesh-based shape deformation methods satisfy user deformation constraints at handles (selected vertices or regions on the mesh) and propagate these handle deformations to the rest of shape smoothly and without removing or distorting details. Some common forms of interactive deformations are point-based, skeleton-based, and cage-based&lt;ref&gt;{{Cite journal|last1=Jacobson|first1=Alec|last2=Baran|first2=Ilya|last3=Popović|first3=Jovan|last4=Sorkine|first4=Olga|year=2011|title=Bounded Biharmonic Weights for Real-Time Deformation|url=https://igl.ethz.ch/projects/bbw/bounded-biharmonic-weights-siggraph-2011-jacobson-et-al.pdf|journal=ACM Transactions on Graphics - Proceedings of ACM SIGGRAPH 2011|volume=30}}&lt;/ref&gt;. In point-based deformation, a user can apply transformations to small set of points, called handles, on the shape. Skeleton-based deformation defines a [[Skeleton|skeleton]] for the shape, which allows a user to move the bones and rotate the joints. Cage-based deformation requires a cage to be drawn around all or part of a shape so that, when the user manipulates points on the cage, the volume it encloses changes accordingly.

=== Point-based deformation ===
Handles provide a sparse set of constraints for the deformation: as the user moves one point, the others must stay in place.

A rest surface &lt;math&gt;\hat{S}&lt;/math&gt; [[Immersion_(mathematics)|immersed]] in &lt;math&gt;\R^3&lt;/math&gt; can be described with a mapping &lt;math&gt;\hat{x} : \Omega \rightarrow \R^3&lt;/math&gt;, where &lt;math&gt;\Omega&lt;/math&gt; is a 2D parametric domain. The same can be done with another mapping &lt;math&gt;x&lt;/math&gt; for the transformed surface &lt;math&gt;S&lt;/math&gt;. Ideally, the transformed shape adds as little distortion as possible to the original. One way to model this distortion is in terms of displacements &lt;math&gt;d = x - \hat{x}&lt;/math&gt; with a Laplacian-based energy&lt;ref&gt;{{Cite journal|last=Marc|first=Alexa|year=2003|title=Differential coordinates for local mesh morphing and deformation|journal=The Visual Computer|volume=19|pages=105–114}}&lt;/ref&gt;. Applying the Laplace operator to these mappings allows us to measure how the position of a point changes relative to its neighborhood, which keeps the handles smooth. Thus, the energy we would like to minimize can be written as:

&lt;math&gt;\min_{\textbf{d}} \int_{\Omega} || \Delta \textbf{d} ||^2 dA&lt;/math&gt;.

While this method is translation invariant, it is unable to account for rotations. The As-Rigid-As-Possible deformation scheme&lt;ref&gt;{{Cite journal|last1=Sorkine|first1=Olga|last2=Alexa|first2=Marc|year=2007|title=As-Rigid-As-Possible Surface Modeling|url=https://www.igl.ethz.ch/projects/ARAP/arap_web.pdf|journal=Proceedings of EUROGRAPHICS/ACM SIGGRAPH Symposium on Geometry Processing|pages=109-116}}&lt;/ref&gt; applies a rigid transformation &lt;math&gt;x_i = R\hat{x_i} + t&lt;/math&gt; to each handle i, where &lt;math&gt;R \in SO(3) \subset \R^3&lt;/math&gt; is a rotation matrix and &lt;math&gt;t \in \R^3&lt;/math&gt; is a translation vector. Unfortunately, there’s no way to know the rotations in advance, so instead we pick a “best” rotation that minimizes displacements. To achieve local rotation invariance, however, requires a function &lt;math&gt;\textbf{R} : \Omega \rightarrow SO(3)&lt;/math&gt; which outputs the best rotation for every point on the surface. The resulting energy, then, must optimize over both &lt;math&gt;\textbf{x}&lt;/math&gt; and &lt;math&gt;\textbf{R}&lt;/math&gt;: 

&lt;math&gt;\min_{\textbf{x,R} \in SO(3)} \int_{\Omega} || \nabla \textbf{x} - \textbf{R} \nabla \hat{\textbf{x}} ||^2 dA&lt;/math&gt;

Note that the translation vector is not present in the final objective function because translations have constant gradient.

== Inside-Outside Segmentation ==
While seemingly trivial, in many cases, determining the inside from the outside of a triangle mesh is not an easy problem. In general, given a surface &lt;math&gt;S&lt;/math&gt; we pose this problem as determining a function &lt;math&gt;isInside(q)&lt;/math&gt; which will return &lt;math&gt;1&lt;/math&gt; if the point &lt;math&gt;q&lt;/math&gt; is inside &lt;math&gt;S&lt;/math&gt;, and &lt;math&gt;0&lt;/math&gt; otherwise.

In the simplest case, the shape is closed. In this case, to determine if a point &lt;math&gt;q&lt;/math&gt; is inside or outside the surface, we can cast a ray &lt;math&gt;r&lt;/math&gt; in any direction from a query point, and count the number of times &lt;math&gt;count_r&lt;/math&gt; it passes through the surface. If &lt;math&gt;q&lt;/math&gt; was outside &lt;math&gt;S&lt;/math&gt; then the ray must either not pass through &lt;math&gt;S&lt;/math&gt; (in which case &lt;math&gt;count_r = 0&lt;/math&gt;) or, each time it enters &lt;math&gt;S&lt;/math&gt; it must pass through twice, because S is bounded, so any ray entering it must exit. So if &lt;math&gt;q&lt;/math&gt; is outside, &lt;math&gt;count_r&lt;/math&gt; is even. Likewise if &lt;math&gt;q&lt;/math&gt; is inside, the same logic applies to the previous case, but the ray must intersect &lt;math&gt;S&lt;/math&gt; one extra time for the first time it leaves &lt;math&gt;S&lt;/math&gt;. So: 

&lt;math&gt;
isInside_r(q) = \left\{
        \begin{array}{ll}
            1 &amp; count_r \ is \ odd \\
            0 &amp; count_r \ is \ even\\
        \end{array}
    \right.
&lt;/math&gt;

Now, often times we cannot guarantee that the &lt;math&gt;S&lt;/math&gt; is closed. Take the pair of pants example from the top of this article. This mesh clearly has a semantic inside-and-outside, despite there being holes at the waist and the legs. 

[[File:Point shooting.gif|thumb|Approximating inside-outside segmentation by shooting rays from a query point for varying number of rays.]]

The naive attempt to solve this problem is to shoot many rays in random directions, and classify &lt;math&gt;q&lt;/math&gt; as being inside if and only if most of the rays intersected &lt;math&gt;S&lt;/math&gt; an odd number of times. To quantify this, let us say we cast &lt;math&gt;k&lt;/math&gt; rays, &lt;math&gt;r_1,r_2,\dots,r_k&lt;/math&gt;. We associate a number &lt;math&gt;rayTest(q) = \frac{1}{k}\sum_{i=1}^{k} isInside_{r_i}(q)&lt;/math&gt; which is the average value of &lt;math&gt;isInside_r&lt;/math&gt; from each ray. Therefore:

&lt;math&gt;
isInside(q) = \left\{
        \begin{array}{ll}
            1 &amp; rayTest(q) \geq 0.5 \\
            0 &amp; rayTest(q) &lt; 0.5\\
        \end{array}
    \right.
&lt;/math&gt;

In the limit of shooting many, many rays, this method handles open meshes, however it in order to become accurate, far too many rays are required for this method to be computationally ideal. Instead, a more robust approach is the Generalized Winding Number&lt;ref&gt;{{Cite journal|last1=Jacobson|first1=Alec|last2=Ladislav|first2=Kavan|last3=Sorkine-Hornung|first3=Olga|year=2013|title=Robust Inside-Outside Segmentation using Generalized Winding Numbers|url=http://igl.ethz.ch/projects/winding-number/robust-inside-outside-segmentation-using-generalized-winding-numbers-siggraph-2013-jacobson-et-al.pdf|journal=ACM Transactions on Graphics (proceedings of ACM SIGGRAPH)|volume=32|pages=33:1--33:12|via=}}&lt;/ref&gt;. Inspired by the 2D [[winding number]], this approach uses the [[solid angle]] at &lt;math&gt;q&lt;/math&gt; of each triangle in the mesh to determine if &lt;math&gt;q&lt;/math&gt; is inside or outside. The value of the Generalized Winding Number at &lt;math&gt;q&lt;/math&gt;, &lt;math&gt;wn(q)&lt;/math&gt; is proportional to the sum of the solid angle contribution from each triangle in the mesh:

&lt;math&gt;wn(q) = \frac{1}{4\pi}\sum_{t \in F} solidAngle(t)&lt;/math&gt;

For a closed mesh, &lt;math&gt;wn(q)&lt;/math&gt; is equivalent to the characteristic function for the volume represented by &lt;math&gt;S&lt;/math&gt;. Therefore, we say:

&lt;math&gt;
isInside(q) = \left\{
        \begin{array}{ll}
            1 &amp; wn(q) \geq 0.5 \\
            0 &amp; wn(q) &lt; 0.5 \\
        \end{array}
    \right.
&lt;/math&gt;

Because &lt;math&gt;wn(q)&lt;/math&gt; is a [[harmonic function]], it degrades gracefully, meaning the inside-outside segmentation would not change much if we poked holes in a closed mesh. For this reason, the Generalized Winding Number handles open meshes robustly. The boundary between inside and outside smoothly passes over holes in the mesh. In fact, in the limit, the Generalized Winding Number is equivalent to the ray-casting method as the number of rays goes to infinity.

== Applications ==
* [[Computer-aided design]] (CAD)
* 3D Surface Reconstruction, ''e.g.'', range scanners in airport security, autonomous vehicles, medical scanner data reconstruction
* Image-to-world Registration, ''e.g.'', [[Image-guided Surgery]] (Image-guided Surgery)
* [[Architecture]], ''e.g.'' creating, [[reverse engineering]]
* Physics simulations
* Computer games ''e.g.'' [[collision detection]]
* [[Geologic modelling]]

== See also ==
* [[Computer graphics]]
* [[Computer-aided design]] (CAD)
* [[Industrial CT scanning]]
* [[List of interactive geometry software]]
* [[Digital image processing]]
* [[Signal processing]]
* [[Discrete differential geometry]]
* [[Glossary of differential geometry and topology]]
* [[Topology]]
* [[Calculus of variations]]

==References==
{{Reflist}}

== External links ==
* [http://www.geometryprocessing.org/ Symposium on Geometry Processing]
* [http://www.multires.caltech.edu/ Multi-Res Modeling Group], [[Caltech]] 
* [http://geom.mi.fu-berlin.de/index.html Mathematical Geometry Processing Group], [[Free University of Berlin]]
* [http://www.graphics.rwth-aachen.de Computer Graphics Group], [[RWTH Aachen University]]
* [http://www.pmp-book.org/ Polygonal Mesh Processing Book]
* [http://www.cs.cmu.edu/~kmcrane/Projects/DGPDEC/ Discrete Differential Geometry: An Applied Introduction], course notes by Keenan Crane et al.
* [https://www.youtube.com/playlist?list=PLOp-ngXvomHArqntgLVNzuJNdzNx3rDjZ Video tutorials] from [[Symposium on Geometry Processing|SGP 2017]] grad school

[[Category:Geometry processing]]
[[Category:3D imaging]]
[[Category:3D computer graphics]]
[[Category:Geometry]]
[[Category:Computational geometry]]
[[Category:Differential geometry]]</text>
      <sha1>koeco3skiel69kpupeirfcg5teo1nl2</sha1>
    </revision>
  </page>
  <page>
    <title>Hamnet Holditch</title>
    <ns>0</ns>
    <id>41808008</id>
    <revision>
      <id>859563115</id>
      <parentid>857358944</parentid>
      <timestamp>2018-09-14T20:55:10Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>Removing from [[Category:English mathematicians]] (parent category) using [[c:Help:Cat-a-lot|Cat-a-lot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5658">{{Infobox scientist
|name              = Reverend Hamnet Holditch
|image             = 
|image_size        =
|caption           = 
|birth_date        = 1800
|death_date        = 12 December 1867
|birth_place       = [[King's Lynn|Lynn, Norfolk]], England
|death_place       = [[Cambridge]], England
|residence         = England
|citizenship       =
|nationality       = English
|ethnicity         =
|fields            = [[Mathematics]]
|workplaces        =
|alma_mater        =
|doctoral_advisor  = &lt;!--There was no PhD at Cambridge before 1919--&gt;
|academic_advisors =
|doctoral_students =
|notable_students  =
|known_for         = [[Holditch's theorem]]
|author_abbrev_bot =
|author_abbrev_zoo =
|influences        = 
|influenced        =
|awards            = [[Smith's Prize]] (1822)
|signature         = &lt;!--(filename only)--&gt;
|footnotes         =
}}

Rev. '''Hamnet Holditch''', also spelled '''Hamnett Holditch''' (1800 – 12 December 1867), was an English mathematician who was president of [[Gonville and Caius College]], [[Cambridge]].

In 1858, he introduced the result in [[geometry]] now known as [[Holditch's theorem]].

Hamnet Holditch was born in 1800 at [[King's Lynn|Lynn, Norfolk]]. In 1818, he began his studies of mathematics at the [[University of Cambridge]] ([[Caius College]]), having obtained his bachelor's degree (B.A.) in 1822 and his master's degree (M.A.) in 1825. He was Senior Wrangler in the Tripos and was awarded the Smith's Prize of 1822. He was a Fellow of Caius College, and its President from 1835 until 1867, when he died.&lt;ref&gt;David Taylor, [http://www.holdiches.com/2013/06/reverend-hamnett-holditch-1800-1867/ "Reverend Hamnett Holditch (1800–1867)"], ''HFHS Journal'', Issue 24 (May 2003).&lt;/ref&gt;

He was the only son of George Holditch, and had two sisters.

==Bibliography==
*Rev. Hamnett Holditch, "Concise Demonstration of the Property of the Parabola", ''The London and Edinburgh Philosophical Magazine and Journal of Science'', vol. 10, 1837, pp.&amp;nbsp;35–36. ([https://books.google.com/books?id=HKCAfbpk-IkC&amp;pg=PA35 Google Books])
*Hamnett Holditch, "On Rolling Curves", ''Transactions of the Cambridge Philosophical Society'', vol. 7, (1842), pp.&amp;nbsp;61–86. ([https://books.google.com/books?id=yxxYAAAAYAAJ&amp;pg=PA61 Google Books])
*Rev. H. Holditch, "On Small Finite Oscillations", ''Transactions of the Cambridge Philosophical Society'', Volume the Eighth, Cambridge, 1849, pp.&amp;nbsp;89–104. ([https://books.google.com/books?id=tQYFAAAAQAAJ&amp;pg=PA89 Google Books])
*Rev. Hamnet Holditch, "On the Caustic by Reflection from a Spherical Surface", ''The Quarterly Journal of Pure and Applied Mathematics'', vol. 1, London, 1857, pp.&amp;nbsp;93–111. ([https://books.google.com/books?id=r59EAAAAcAAJ&amp;pg=PA93 Google Books])
*Rev. Hamnet Holditch, "Geometrical Theorem", ''The Quarterly Journal of Pure and Applied Mathematics'', vol. 2, London, 1858, p.&amp;nbsp;38. ([https://books.google.com/books?id=sp9EAAAAcAAJ&amp;pg=PA38 Google Books]; [https://archive.org/details/quarterlyjourna45unkngoog Internet Archive])
*Rev. Hamnet Holditch, "On the n&lt;sup&gt;th&lt;/sup&gt; Caustic, by Reflexion from a Circle", ''The Quarterly Journal of Pure and Applied Mathematics'', vol. 2, London, 1858, pp.&amp;nbsp;301–322. ([https://books.google.com/books?id=sp9EAAAAcAAJ&amp;pg=PA301 Google Books])
*Rev. Hamnet Holditch, "On the n&lt;sup&gt;th&lt;/sup&gt; Evolutes and Involutes of Curves", ''The Quarterly Journal of Pure and Applied Mathematics'', vol. 3, London, 1860, pp.&amp;nbsp;236–246. ([https://books.google.com/books?id=HsQ_AQAAIAAJ&amp;pg=PA236 Google Books])
*Rev. Hamnet Holditch, "Theorems on Related Curves", ''The Quarterly Journal of Pure and Applied Mathematics'', vol. 3, London, 1860, pp.&amp;nbsp;271–274. ([https://books.google.com/books?id=u59EAAAAcAAJ&amp;pg=PA271 Google Books])
*Rev. Hamnet Holditch, "On Double Tangents", ''The Quarterly Journal of Pure and Applied Mathematics'', vol. 4, London, 1861, pp.&amp;nbsp;28–44. ([https://books.google.com/books?id=y9xEAAAAcAAJ&amp;pg=PA28 Google Books])
*Rev. Hamnet Holditch, "On a Magic Square", ''The Quarterly Journal of Pure and Applied Mathematics'', vol. 6, London, 1864, pp.&amp;nbsp;181–189. ([https://books.google.com/books?id=pnc_AQAAIAAJ&amp;pg=PA181 Google Books])

==Notes==
{{reflist}}

==References==
*Henry James Hillen. ''History of the Borough of King's Lynn'', Volume 2, p.&amp;nbsp;632. ([https://archive.org/details/historyofborough02hill Internet Archive])
*[[John Venn]]. ''Biographical history of Gonville and Caius college, 1349–1897; containing a list of all known members of the college from the foundation to the present time, with biographical notes'' (1897), Volume 2 (1713–1897), p.&amp;nbsp;170. ([https://archive.org/details/biographicalhist02vennuoft Internet Archive])
*Sylvanus Urban. ''The Gentleman's Magazine and Historical Review'', New series: Volume 5, Jan.–May, 1868., p.&amp;nbsp;255. ([https://books.google.com/books?id=m5tQAAAAcAAJ&amp;pg=PA255 Google Books])
*[http://www.holdiches.com/2013/11/rev-hamnet-holditch-1800-1867-his-will/ The Last Will and Testament  of Hamnet Holditch]
*[http://www.holdiches.com/2013/06/reverend-hamnett-holditch-1800-1867/ David Taylor. "Reverend Hamnett Holditch" (1800–1867)]
*A. D. D. Craik. ''Mr Hopkins' Men: Cambridge Reform and British Mathematics in the 19th Century'', p.&amp;nbsp;44. ([https://books.google.com/books?id=7_fAXi3ZP4YC&amp;pg=PA44 Google Books])

{{authority control}}

{{DEFAULTSORT:Holditch, Hamnet}}
[[Category:1800 births]]
[[Category:1867 deaths]]
[[Category:Senior Wranglers]]
[[Category:Fellows of Gonville and Caius College, Cambridge]]
[[Category:Geometers]]
[[Category:19th-century English mathematicians]]</text>
      <sha1>dd8oo49j8ehbv9d8pbq9vvraxmacbx6</sha1>
    </revision>
  </page>
  <page>
    <title>Heavy-tailed distribution</title>
    <ns>0</ns>
    <id>8092200</id>
    <revision>
      <id>843957769</id>
      <parentid>843646466</parentid>
      <timestamp>2018-06-01T16:24:00Z</timestamp>
      <contributor>
        <username>Tnemtsoni</username>
        <id>15288447</id>
      </contributor>
      <comment>/* Common heavy-tailed distributions */ Log-logistic distribution</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17671">In [[probability theory]], '''heavy-tailed distributions''' are [[probability distribution]]s whose tails are not exponentially bounded:&lt;ref name="Asmussen"&gt;{{Cite book | doi = 10.1007/0-387-21525-5_10 | first = S. R. | last = Asmussen| chapter = Steady-State Properties of GI/G/1 | title = Applied Probability and Queues | series = Stochastic Modelling and Applied Probability | volume = 51 | pages = 266–301 | year = 2003 | isbn = 978-0-387-00211-8 | pmid =  | pmc = }}&lt;/ref&gt; that is, they have heavier tails than the [[exponential distribution]].  In many applications it is the right tail of the distribution that is of interest, but a distribution may have a heavy left tail, or both tails may be heavy.

There are three important subclasses of heavy-tailed distributions: the [[fat-tailed distribution]]s, the [[long-tailed distribution]]s and the '''subexponential distributions'''.  In practice, all commonly used heavy-tailed distributions belong to the subexponential class.

There is still some discrepancy over the use of the term '''heavy-tailed'''.  There are two other definitions in use.  Some authors  use the term to refer to those distributions which do not have all their power [[Moment (mathematics)|moments]] finite; and some others to those distributions that do not have a finite [[variance]].  The definition given in this article is the most general in use, and includes all distributions encompassed by the alternative definitions, as well as those distributions such as [[log-normal]] that possess all their power moments, yet which are generally acknowledged to be heavy-tailed.  (Occasionally, heavy-tailed is used for any distribution that has heavier tails than the normal distribution.)

==Definitions==

===Definition of heavy-tailed distribution===

The distribution of a [[random variable]] ''X'' with [[cumulative distribution function|distribution function]] ''F'' is said to have a heavy (right) tail if the [[moment generating function]] of ''F'', ''M&lt;sub&gt;F&lt;/sub&gt;''(''t''), is infinite for all ''t''&amp;nbsp;&gt;&amp;nbsp;0.&lt;ref&gt;Rolski, Schmidli, Scmidt, Teugels, ''Stochastic Processes for Insurance and Finance'', 1999&lt;/ref&gt;

That means
:&lt;math&gt;
\int_{-\infty}^\infty e^{t x} \,dF(x) = \infty \quad \mbox{for all } t&gt;0.
&lt;/math&gt; &lt;ref&gt; S. Foss, D. Korshunov, S. Zachary, ''An Introduction to Heavy-Tailed and Subexponential Distributions'', Springer Science &amp; Business Media, 21 May 2013 &lt;/ref&gt;

An implication of this is that
:&lt;math&gt;
\lim_{x \to \infty} e^{t x}\Pr[X&gt;x] = \infty \quad \mbox{for all } t&gt;0.\,
&lt;/math&gt; &lt;ref&gt;Rolski, Schmidli, Scmidt, Teugels, ''Stochastic Processes for Insurance and Finance'', 1999&lt;/ref&gt;

This is also written in terms of the tail distribution function

: &lt;math&gt;\overline{F}(x) \equiv \Pr[X&gt;x] \, &lt;/math&gt;

as

:&lt;math&gt;
\lim_{x \to \infty} e^{\lambda x}\overline{F}(x) = \infty \quad \mbox{for all } \lambda&gt;0.\,
&lt;/math&gt;

===Definition of long-tailed distribution===

The distribution of a [[random variable]] ''X'' with [[cumulative distribution function|distribution function]] ''F'' is said to have a long right tail&lt;ref name="Asmussen"/&gt; if for all ''t''&amp;nbsp;&gt;&amp;nbsp;0,

:&lt;math&gt;
\lim_{x \to \infty} \Pr[X&gt;x+t\mid X&gt;x] =1, \,
&lt;/math&gt;

or equivalently

:&lt;math&gt;
\overline{F}(x+t) \sim \overline{F}(x) \quad \mbox{as } x \to \infty. \,
&lt;/math&gt;

This has the intuitive interpretation for a right-tailed long-tailed distributed quantity that if the long-tailed quantity exceeds some high level, the probability approaches 1 that it will exceed any other higher level.

All long-tailed distributions are heavy-tailed, but the converse is false, and it is possible to construct heavy-tailed distributions that are not long-tailed.

===Subexponential distributions===

Subexponentiality is defined in terms of [[Convolution of probability distributions|convolutions of probability distributions]]. For two independent, identically distributed [[random variables]] &lt;math&gt; X_1,X_2&lt;/math&gt; with common distribution function &lt;math&gt;F&lt;/math&gt; the convolution of &lt;math&gt;F&lt;/math&gt; with itself, &lt;math&gt;F^{*2}&lt;/math&gt; is defined, using [[Lebesgue–Stieltjes integration]], by:

:&lt;math&gt;
\Pr[X_1+X_2 \leq x] = F^{*2}(x) = \int_{- \infty}^\infty F(x-y)\,dF(y).
&lt;/math&gt;

The ''n''-fold convolution &lt;math&gt;F^{*n}&lt;/math&gt; is defined in the same way. The tail distribution function &lt;math&gt;\overline{F}&lt;/math&gt; is defined as &lt;math&gt;\overline{F}(x) = 1-F(x)&lt;/math&gt;.

A distribution &lt;math&gt;F&lt;/math&gt; on the positive half-line is subexponential &lt;ref name="Asmussen"/&gt;&lt;ref&gt;V. P. Chistyakov, A Theorem on Sums of Independent Positive Random Variables and Its Applications to Branching Random Processes, Theory of Probability and Its Applications 1964 https://www.researchgate.net/publication/242637603_A_Theorem_on_Sums_of_Independent_Positive_Random_Variables_and_Its_Applications_to_Branching_Random_Processes&lt;/ref&gt;&lt;ref&gt;J.L. Teugels, The Class of Subexponential Distributions, Annals of Probability 1975 http://projecteuclid.org/download/pdf_1/euclid.aop/1176996225&lt;/ref&gt; if

:&lt;math&gt;
\overline{F^{*2}}(x) \sim 2\overline{F}(x) \quad \mbox{as } x \to \infty. 
&lt;/math&gt;

This implies&lt;ref name="Embrechts"&gt;{{cite book |author1=Embrechts P. |author2=Klueppelberg C. |author3=Mikosch T. |title=Modelling extremal events for insurance and finance |publisher=Springer | series = Stochastic Modelling and Applied Probability|location=Berlin |year=1997  | volume=33| doi = 10.1007/978-3-642-33483-2}}&lt;/ref&gt; that, for any &lt;math&gt;n \geq 1&lt;/math&gt;,

:&lt;math&gt;
\overline{F^{*n}}(x) \sim n\overline{F}(x) \quad \mbox{as } x \to \infty. 
&lt;/math&gt;

The probabilistic interpretation&lt;ref name="Embrechts"/&gt; of this is that, for a sum of &lt;math&gt;n&lt;/math&gt; [[statistical independence|independent]] [[random variables]] &lt;math&gt;X_1,\ldots,X_n&lt;/math&gt; with common distribution &lt;math&gt;F&lt;/math&gt;,

:&lt;math&gt;
\Pr[X_1+ \cdots +X_n&gt;x] \sim \Pr[\max(X_1, \ldots,X_n)&gt;x] \quad \text{as } x \to \infty. 
&lt;/math&gt;

This is often known as the principle of the single big jump&lt;ref&gt;{{Cite journal | last1 = Foss | first1 = S. | last2 = Konstantopoulos | first2 = T. | last3 = Zachary | first3 = S. | doi = 10.1007/s10959-007-0081-2 | title = Discrete and Continuous Time Modulated Random Walks with Heavy-Tailed Increments | journal = Journal of Theoretical Probability| volume = 20 | issue = 3 | pages = 581 | year = 2007 | arxiv = math/0509605| pmid =  | url = http://www.math.nsc.ru/LBRT/v1/foss/fkz_revised.pdf| pmc = }}&lt;/ref&gt; or catastrophe principle.&lt;ref&gt;{{cite web| url = http://rigorandrelevance.wordpress.com/2014/01/09/catastrophes-conspiracies-and-subexponential-distributions-part-iii/ | title = Catastrophes, Conspiracies, and Subexponential Distributions (Part III) | first = Adam | last = Wierman | authorlink = Adam Wierman | date = January 9, 2014 | accessdate = January 9, 2014 | website = Rigor + Relevance blog | publisher = RSRG, Caltech}}&lt;/ref&gt;

A distribution &lt;math&gt;F&lt;/math&gt; on the whole real line is subexponential if the distribution
&lt;math&gt;F I([0,\infty))&lt;/math&gt; is.&lt;ref&gt;{{cite journal | last = Willekens | first =  E. | title = Subexponentiality on the real line | journal = Technical Report | publisher = K.U. Leuven | year = 1986}}&lt;/ref&gt; Here &lt;math&gt;I([0,\infty))&lt;/math&gt; is the [[indicator function]]
of the positive half-line.  Alternatively, a random variable &lt;math&gt;X&lt;/math&gt; supported on the real line is subexponential if and only if &lt;math&gt;X^+ = \max(0,X)&lt;/math&gt; is subexponential.

All subexponential distributions are long-tailed, but examples can be constructed of long-tailed distributions that are not subexponential.

==Common heavy-tailed distributions==

All commonly used heavy-tailed distributions are subexponential.&lt;ref name="Embrechts"/&gt;

Those that are one-tailed include:
*the [[Pareto distribution]];
*the [[Log-normal distribution]];
*the [[Lévy distribution]];
*the [[Weibull distribution]] with shape parameter greater than 0 but less than 1;
*the [[Burr distribution]];
*the [[log-logistic distribution]];
*the [[log-gamma distribution]];
*the [[log-Cauchy distribution]], sometimes described as having a "super-heavy tail" because it exhibits [[logarithmic growth|logarithmic decay]] producing a heavier tail than the Pareto distribution.&lt;ref&gt;{{cite book|title=Laws of Small Numbers: Extremes and Rare Events|author=Falk, M., Hüsler, J. &amp; Reiss, R.|page=80|year=2010|publisher=Springer|isbn=978-3-0348-0008-2}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Statistical inference for heavy and super-heavy tailed distributions|url=http://docentes.deio.fc.ul.pt/fragaalves/SuperHeavy.pdf|author=Alves, M.I.F., de Haan, L. &amp; Neves, C.|date=March 10, 2006}}&lt;/ref&gt;

Those that are two-tailed include:
*The [[Cauchy distribution]], itself a special case of both the stable distribution and the t-distribution;
*The family of  [[stable distributions]],&lt;ref&gt;{{cite web |author=John P. Nolan | title=Stable Distributions: Models for Heavy Tailed Data| year=2009 | url=http://academic2.american.edu/~jpnolan/stable/chap1.pdf | format=PDF | accessdate=2009-02-21}}&lt;/ref&gt; excepting the special case of the normal distribution within that family. Some stable distributions are one-sided (or supported by a half-line), see e.g. [[Lévy distribution]]. See also ''[[financial models with long-tailed distributions and volatility clustering]]''.
*The [[Student's t-distribution|t-distribution]].
*The skew lognormal cascade distribution.&lt;ref&gt;{{cite web |author=Stephen Lihn | title=Skew Lognormal Cascade Distribution| year=2009 | url=http://www.skew-lognormal-cascade-distribution.org/ }}&lt;/ref&gt;

== Relationship to fat-tailed distributions ==
A [[fat-tailed distribution]] is a distribution for which the probability density function, for large x, goes to zero as a power &lt;math&gt;x^{-a}&lt;/math&gt;.  Since such a power is always bounded below by the probability density function of an exponential distribution, fat-tailed distributions are always heavy-tailed.  Some distributions, however, have a tail which goes to zero slower than an exponential function (meaning they are heavy-tailed), but faster than a power (meaning they are not fat-tailed). An example is the [[log-normal distribution]].  Many other heavy-tailed distributions such as the [[log-logistic distribution|log-logistic]] and [[Pareto distribution|Pareto]] distribution are however also fat-tailed.

== Estimating the tail-index{{definition|date=January 2018}} ==

There are parametric (see Embrechts et al.&lt;ref name="Embrechts"/&gt;) and non-parametric (see, e.g., Novak&lt;ref name="Novak2011"&gt;{{cite book
| author=Novak S.Y. 
| title=Extreme value methods with applications to finance
| year=2011
| series=London: CRC
| isbn=978-1-43983-574-6 
}}&lt;/ref&gt;) approaches to the problem of the tail-index estimation.

To estimate the tail-index using the parametric approach, some authors employ  [[GEV distribution]] or [[Pareto distribution]]; they may apply the maximum-likelihood estimator (MLE).

=== Pickand's tail-index estimator ===

With &lt;math&gt;(X_n , n \geq 1)&lt;/math&gt; a random sequence of independent and same  density function &lt;math&gt;F \in D(H(\xi))&lt;/math&gt;, the Maximum Attraction Domain&lt;ref name=Pickands&gt;{{cite journal|last=Pickands III|first=James|title=Statistical Inference Using Extreme Order Statistics|journal=The Annals of Statistics|date=Jan 1975|volume=3|issue=1|pages=119–131|jstor=2958083|doi=10.1214/aos/1176343003}}&lt;/ref&gt;  of the generalized extreme value density &lt;math&gt; H &lt;/math&gt;, where &lt;math&gt;\xi \in \mathbb{R}&lt;/math&gt;. If &lt;math&gt;\lim_{n\to\infty} k(n) = \infty  &lt;/math&gt; and  &lt;math&gt;\lim_{n\to\infty} \frac{k(n)}{n}= 0&lt;/math&gt;, then the ''Pickands'' tail-index estimation is&lt;ref name="Embrechts"/&gt;&lt;ref name="Pickands"/&gt;
:&lt;math&gt;
\xi^\text{Pickands}_{(k(n),n)} =\frac{1}{\ln 2} \ln \left(  \frac{X_{(n-k(n)+1,n)} - X_{(n-2k(n)+1,n)}}{X_{(n-2k(n)+1,n)} - X_{(n-4k(n)+1,n)}}\right)
&lt;/math&gt;
where &lt;math&gt;X_{(n-k(n)+1,n)}=\max \left(X_{n-k(n)+1},\ldots  ,X_{n}\right)&lt;/math&gt;. This estimator converge in probability to &lt;math&gt;\xi&lt;/math&gt;.

=== Hill's tail-index estimator ===

Let &lt;math&gt;(X_t , t \geq 1)&lt;/math&gt; be a sequence of independent and identically distributed random variables with distribution function &lt;math&gt;F \in D(H(\xi))&lt;/math&gt;, the maximum domain of attraction of the [[generalized extreme value distribution]] &lt;math&gt; H &lt;/math&gt;, where &lt;math&gt;\xi \in \mathbb{R}&lt;/math&gt;. The sample path is &lt;math&gt;{X_t: 1 \leq t \leq n}&lt;/math&gt; where &lt;math&gt;n&lt;/math&gt; is the sample size. If 
&lt;math&gt;\{k(n)\}&lt;/math&gt; is an intermediate order sequence, i.e. &lt;math&gt;k(n) \in \{1,\ldots,n-1\}, &lt;/math&gt;, &lt;math&gt;k(n) \to \infty&lt;/math&gt; and  &lt;math&gt;k(n)/n \to 0&lt;/math&gt;, then the Hill tail-index estimator is&lt;ref&gt;Hill B.M. (1975) A simple general approach to inference about  the tail of a distribution. Ann. Stat., v. 3, 1163–1174.&lt;/ref&gt; 

: &lt;math&gt;
\xi^\text{Hill}_{(k(n),n)} = \left(\frac 1 {k(n)} \sum_{i=n-k(n)+1}^n \ln(X_{(i,n)}) - \ln (X_{(n-k(n)+1,n)})\right)^{-1},
&lt;/math&gt;

where &lt;math&gt;X_{(i,n)}&lt;/math&gt; is the &lt;math&gt;i&lt;/math&gt;-th [[order statistic]] of &lt;math&gt;X_1, \dots, X_n&lt;/math&gt;.
This estimator converges in probability to &lt;math&gt;\xi&lt;/math&gt;, and is asymptotically normal provided &lt;math&gt;k(n) \to \infty  &lt;/math&gt; is restricted based on a higher order regular variation property&lt;ref&gt;Hall, P.(1982) On some estimates of an exponent of regular variation. J. R. Stat. Soc. Ser. B., v. 44, 37–42.&lt;/ref&gt; 
.&lt;ref&gt;Haeusler, E. and J. L. Teugels (1985) On asymptotic normality of Hill's estimator for the exponent of regular variation. Ann. Stat., v. 13, 743–756.&lt;/ref&gt; Consistency and asymptotic normality extend to a large class of dependent and heterogeneous sequences,&lt;ref&gt;Hsing, T. (1991) On tail index estimation using dependent data. Ann. Stat., v. 19, 1547–1569.&lt;/ref&gt;&lt;ref&gt;Hill, J. (2010) On tail index estimation for dependent, heterogeneous data. Econometric Th., v. 26, 1398–1436.&lt;/ref&gt; irrespective of whether &lt;math&gt;X_t&lt;/math&gt; is observed, or a computed residual or filtered data from a large class of models and estimators, including mis-specified models and models with errors that are dependent.&lt;ref&gt;Resnick, S. and Starica, C. (1997). Asymptotic behavior of Hill’s estimator for autoregressive data. Comm. Statist. Stochastic Models 13, 703–721.&lt;/ref&gt;&lt;ref&gt;Ling, S. and Peng, L. (2004). Hill’s estimator for the tail index of an ARMA model. J. Statist. Plann. Inference 123, 279–293.&lt;/ref&gt;&lt;ref&gt;Hill, J. B. (2015). Tail index estimation for a filtered dependent time series. Stat. Sin. 25, 609–630.&lt;/ref&gt;

=== Ratio estimator of the tail-index ===

The ratio estimator (RE-estimator) of the tail-index was introduced by Goldie  
and Smith.&lt;ref&gt;Goldie C.M., Smith R.L. (1987) Slow variation with remainder:
 theory and applications. Quart. J. Math. Oxford, v. 38, 45–71.&lt;/ref&gt; 
It is constructed similarly to Hill's estimator but uses a non-random "tuning parameter".

A comparison of Hill-type and RE-type estimators can be found in Novak.&lt;ref name="Novak2011"/&gt;

===Software===
* [http://www.cs.bu.edu/~crovella/aest.html aest], [[C (programming language)|C]] tool for estimating the heavy-tail index.&lt;ref&gt;{{Cite journal | last1 = Crovella | first1 = M. E. | last2 = Taqqu | first2 = M. S. | title = Estimating the Heavy Tail Index from Scaling Properties| journal = Methodology and Computing in Applied Probability | volume = 1 | pages = 55 | year = 1999 | doi = 10.1023/A:1010012224103 | url = http://www.cs.bu.edu/~crovella/paper-archive/aest.ps| pmid =  | pmc = }}&lt;/ref&gt;

==Estimation of heavy-tailed density==

Nonparametric approaches to estimate heavy- and superheavy-tailed probability density functions were given in 
Markovich.&lt;ref name="Markovich2007"&gt;{{cite book
| author=Markovich N.M. 
| title=Nonparametric Analysis of Univariate Heavy-Tailed data: Research and Practice
| year=2007
| series=Chitester: Wiley
| isbn=978-0-470-72359-3
}}&lt;/ref&gt; These are approaches based on variable bandwidth and long-tailed kernel estimators;  on the preliminary data transform to a new random variable at finite or infinite intervals which is more convenient for the estimation and then inverse transform of the obtained density estimate; and "piecing-together approach" which provides a certain parametric model for the tail of the density and a non-parametric model to approximate the mode of the density. Nonparametric estimators require an appropriate selection of tuning (smoothing) parameters like a bandwidth of kernel estimators and the bin width of the histogram. The well known data-driven methods of such selection are a cross-validation and its modifications,  methods based on the minimization of the mean squared error (MSE) and its asymptotic and their upper bounds.&lt;ref name="WandJon1995"&gt;{{cite book
| author=Wand M.P., Jones M.C. 
| title=Kernel smoothing
| year=1995
| series=New York: Chapman and Hall,
| isbn=978-0412552700
}}&lt;/ref&gt; A discrepancy method which uses well-known nonparametric statistics like Kolmogorov-Smirnov's, von Mises and Anderson-Darling's ones as a metric in the space of distribution functions (dfs) and quantiles of the later statistics as a known uncertainty or a discrepancy value can be found in.&lt;ref name="Markovich2007"/&gt; Bootstrap is another tool to find smoothing parameters using  approximations of unknown MSE by different schemes of re-samples selection, see e.g.&lt;ref name="Hall1992"&gt;{{cite book
| author=Hall P. 
| title=The Bootstrap and Edgeworth Expansion
| year=1992
| series=Springer,
| isbn=9780387945088
}}&lt;/ref&gt;

==See also==
*[[Leptokurtic distribution]]
*[[Outlier]]
*[[The Long Tail]]
*[[Power law]]
*[[Seven states of randomness]]
*[[Fat-tailed distribution]]

==References==

&lt;references/&gt;

[[Category:Tails of probability distributions]]
[[Category:Types of probability distributions]]
[[Category:Actuarial science]]
[[Category:Risk]]</text>
      <sha1>cn5ri0lt4ubavl2qxknnr3ez3zx4wqn</sha1>
    </revision>
  </page>
  <page>
    <title>Hilbert scheme</title>
    <ns>0</ns>
    <id>6612581</id>
    <revision>
      <id>862709371</id>
      <parentid>855227903</parentid>
      <timestamp>2018-10-06T05:20:32Z</timestamp>
      <contributor>
        <username>Cydebot</username>
        <id>1215485</id>
      </contributor>
      <minor/>
      <comment>Robot - Removing category Eponymous scientific concepts per [[WP:CFD|CFD]] at [[Wikipedia:Categories for discussion/Log/2018 September 22]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14296">In [[algebraic geometry]], a branch of [[mathematics]], a '''Hilbert scheme''' is a [[scheme theory|scheme]] that is the parameter space for the [[closed subscheme]]s of some projective space (or a more general projective scheme), refining the [[Chow variety]].  The Hilbert scheme is a disjoint union of [[projective subscheme]]s corresponding to [[Hilbert polynomial]]s. The basic theory of Hilbert schemes was developed by {{harvs|first=Alexander|last= Grothendieck|authorlink=Alexander Grothendieck|year=1961|txt}}. [[Hironaka's example]] shows that non-projective varieties need not have Hilbert schemes.

==Hilbert scheme of projective space==
The Hilbert scheme {{math|'''Hilb'''(''n'')}} of {{math|'''P'''&lt;sup&gt;''n''&lt;/sup&gt;}} classifies closed subschemes of projective space in the following sense: For any [[locally Noetherian scheme]] {{mvar|S}}, the set of {{mvar|S}}-valued points

:{{math|Hom(''S'', '''Hilb'''(''n''))}}

of the Hilbert scheme is naturally isomorphic to the set of closed subschemes of {{math|'''P'''&lt;sup&gt;''n''&lt;/sup&gt; × ''S''}} that are [[flat morphism|flat]] over {{mvar|S}}. The closed subschemes of {{math|'''P'''&lt;sup&gt;''n''&lt;/sup&gt; × ''S''}} that are flat over {{mvar|S}} can informally be thought of as the families of subschemes of projective space parameterized by {{mvar|S}}. The Hilbert scheme {{math|'''Hilb'''(''n'')}} breaks up as a disjoint union of pieces {{math|'''Hilb'''(''n'', ''P'')}} corresponding to the Hilbert polynomial of the subschemes of projective space with Hilbert polynomial {{mvar|P}}. Each of these pieces is projective over {{math|Spec('''Z''')}}.

===Construction===
Grothendieck constructed the Hilbert scheme {{math|'''Hilb'''(''n'')&lt;sub&gt;''S''&lt;/sub&gt;}} of {{mvar|n}}-dimensional projective space over a Noetherian scheme {{mvar|S}} as a subscheme of a [[Grassmannian]] defined by the vanishing of various [[determinant]]s. Its fundamental property is that for a scheme {{mvar|T}} over {{mvar|S}}, it represents the functor whose {{mvar|T}}-valued points are the closed subschemes of {{math|'''P'''&lt;sup&gt;''n''&lt;/sup&gt; ×&lt;sub&gt;''S''&lt;/sub&gt; ''T''}} that are flat over {{mvar|T}}.

If {{mvar|X}} is a subscheme of {{mvar|n}}-dimensional projective space, then {{mvar|X}} corresponds to a graded ideal {{math|''I&lt;sub&gt;X&lt;/sub&gt;''}} of the polynomial ring {{mvar|S}} in {{math|''n'' + 1}} variables, with graded pieces {{math|''I&lt;sub&gt;X&lt;/sub&gt;''(''m'')}}. For sufficiently large {{mvar|m}}, depending only on the Hilbert polynomial {{mvar|P}} of {{mvar|X}}, all higher cohomology groups of {{mvar|X}} with coefficients in {{math|''O''(''m'')}} vanish, so in particular {{math|''I&lt;sub&gt;X&lt;/sub&gt;''(''m'')}} has dimension {{math|''Q''(''m'') − ''P''(''m'')}}, where {{mvar|Q}} is the Hilbert polynomial of projective space.

Pick a sufficiently large value of {{mvar|m}}. The {{math|(''Q''(''m'') − ''P''(''m''))}}-dimensional space {{math|''I&lt;sub&gt;X&lt;/sub&gt;''(''m'')}} is a subspace of the {{math|''Q''(''m'')}}-dimensional space {{math|''S''(''m'')}}, so represents a point of the Grassmannian {{math|'''Gr'''(''Q''(''m'') − ''P''(''m''), ''Q''(''m''))}}. This will give an embedding of the piece of the Hilbert scheme corresponding to the Hilbert polynomial {{mvar|P}} into this Grassmannian.

It remains to  describe the scheme structure on this image, in other words to describe enough elements for the ideal corresponding to it. Enough such elements are given by the conditions that the map {{math|''I&lt;sub&gt;X&lt;/sub&gt;''(''m'') ⊗ ''S''(''k'') → ''S''(''k'' + ''m'')}} has rank at most {{math|dim(''I&lt;sub&gt;X&lt;/sub&gt;''(''k'' + ''m''))}} for all positive {{mvar|k}}, which is equivalent to the vanishing of various determinants. (A more careful analysis shows that  it is enough just to take {{math|''k'' {{=}} 1}}.)

===Variations===
The Hilbert scheme {{math|'''Hilb'''(''X'')&lt;sub&gt;''S''&lt;/sub&gt;}} is defined and constructed for any projective scheme {{mvar|X}} in a similar way. Informally, its points correspond to closed subschemes of {{mvar|X}}.

===Properties===
{{harvtxt|Macaulay|1927}} determined for which polynomials the Hilbert scheme {{math|'''Hilb'''(''n'', ''P'')}} is non-empty, and {{harvtxt|Hartshorne|1966}} showed that if {{math|'''Hilb'''(''n'', ''P'')}} is non-empty then it is  linearly connected. So two subschemes of projective space are in the same connected component of the Hilbert scheme if and only if they have the same Hilbert polynomial.

Hilbert schemes can have bad singularities, such as irreducible components that are non-reduced at all points. They can also have irreducible components of unexpectedly high dimension. For example, one might expect the Hilbert scheme of {{mvar|d}} points (more precisely dimension 0, length {{mvar|d}} subschemes) of a scheme of dimension {{mvar|n}} to have dimension {{mvar|dn}}, but if {{math|''n'' ≥ 3}} its irreducible components can have much larger dimension.

== Examples ==
* The hilbert scheme of points of a closed point is just a point.
* The hilbert scheme of degree k hypersurfaces in &lt;math&gt;\mathbb{P}^n&lt;/math&gt; is given by the projectivization &lt;math&gt;\mathbb{P}(\Gamma(\mathcal{O}(k)))&lt;/math&gt;. For example, the hilbert scheme of degree 2 hypersurfaces in &lt;math&gt;\mathbb{P}^1&lt;/math&gt;is &lt;math&gt;\mathbb{P}^2&lt;/math&gt;with the universal hypersurface given by
&lt;blockquote&gt;&lt;math&gt;\text{Proj}(k[x_0,x_1][\alpha,\beta,\gamma]/(\alpha x_0^2 + \beta x_0x_1 + \gamma x_1^2)) \subseteq \mathbb{P}_{x_0,x_1}^1\times\mathbb{P}^2_{\alpha,\beta,\gamma}&lt;/math&gt;&lt;/blockquote&gt;where the underlying ring is bigraded.

== Hilbert scheme of points on a manifold ==
"Hilbert scheme" sometimes refers to the '''punctual Hilbert scheme''' of 0-dimensional subschemes on a scheme. Informally this can be thought of as something like finite collections of points on a scheme, though this picture can be very misleading when several points coincide.

There is a '''Hilbert-Chow morphism''' from the reduced Hilbert scheme of points to the Chow variety of cycles taking any 0-dimensional scheme to its associated 0-cycle. {{harvs|last=Fogarty|year1=1968|year2=1969|year3=1973}}.

The Hilbert scheme {{math|''M''&lt;sup&gt;[''n'']&lt;/sup&gt;}} of {{mvar|n}} points on {{mvar|M}} is equipped with a natural morphism to an {{mvar|n}}-th symmetric product of {{mvar|M}}. This morphism is birational for {{mvar|M}} of dimension at most 2. For {{mvar|M}} of dimension at least 3 the morphism is not birational for large {{mvar|n}}: the Hilbert scheme is in general reducible and has components of dimension much larger than that of the symmetric product.

The Hilbert scheme of points on a curve {{mvar|C}} (a dimension-1 complex manifold) is isomorphic to a [[Symmetric product of an algebraic curve|symmetric power]] of {{mvar|C}}. It is smooth.

The Hilbert scheme of {{mvar|n}} points on a [[Complex surface|surface]] is also smooth (Grothendieck). If {{math|''n'' {{=}} 2}}, it is obtained from {{math|''M'' × ''M''}} by blowing up the diagonal and then dividing by the {{math|'''Z'''/2'''Z'''}} action induced by {{math|(''x'', ''y'') ↦ (''y'', ''x'')}}. It was used by [[Mark Haiman]] in his proof of the positivity of the coefficients of some [[Macdonald polynomial]]s.

The Hilbert scheme of a smooth manifold of dimension 3 or more is usually not smooth.

== Hilbert schemes and hyperkähler geometry ==
Let {{mvar|M}} be a complex [[Kähler manifold|Kähler]] surface with {{math|''c''&lt;sub&gt;1&lt;/sub&gt; {{=}} 0}} ([[K3 surface]] or a torus). The canonical bundle of {{mvar|M}} is trivial, as follows from [[Enriques–Kodaira classification|Kodaira classification of surfaces]]. Hence {{mvar|M}} admits a holomorphic [[Symplectic geometry|symplectic]] form. It was observed by [[Akira Fujiki|Fujiki]] (for {{math|''n'' {{=}} 2}}) and [[Arnaud Beauville|Beauville]] that {{math|''M''&lt;sup&gt;[''n'']&lt;/sup&gt;}} is also holomorphically symplectic. This is not very difficult to see, e.g., for {{math|''n'' {{=}} 2}}. Indeed, {{math|''M''&lt;sup&gt;[2]&lt;/sup&gt;}} is a blow-up of a symmetric square of {{mvar|M}}. Singularities of {{math|Sym&lt;sup&gt;2&lt;/sup&gt; ''M''}} are locally isomorphic to {{math|'''C'''&lt;sup&gt;2&lt;/sup&gt; × '''C'''&lt;sup&gt;2&lt;/sup&gt;/{±1}.}} The blow-up of {{math|'''C'''&lt;sup&gt;2&lt;/sup&gt;/{±1} }} is {{math|''T''&lt;sup&gt;&amp;thinsp;∗&lt;/sup&gt;'''P'''&lt;sup&gt;1&lt;/sup&gt;('''C''')}}, and this space is symplectic. This is used to show that the symplectic form is naturally extended to the smooth part of the exceptional divisors of {{math|''M''&lt;sup&gt;[''n'']&lt;/sup&gt;}}. It is extended to the rest of {{math|''M''&lt;sup&gt;[''n'']&lt;/sup&gt;}} by [[Hartogs' principle]].

A holomorphically symplectic, [[Kähler manifold]] is [[Hyperkähler manifold|hyperkähler]], as follows from [[Calabi conjecture|Calabi–Yau theorem]]. Hilbert schemes of points on [[K3 surface|K3]] and a 4-dimensional torus give two series of examples of [[hyperkähler manifold]]s: a Hilbert scheme of points on K3 and a generalized Kummer manifold.

==See also==
*[[Quot scheme]]
*[[Matsusaka's big theorem]]

==References==
*{{Citation | last1=Beauville | first1=Arnaud | title=Variétés Kähleriennes dont la première classe de Chern est nulle | mr=730926 | year=1983 | journal=Journal of Differential Geometry   | volume=18 | issue=4 | pages=755–782}}
*{{springer|id=H/h047320|authorlink=Dolgachev|author=I. Dolgachev|title=Hilbert scheme}}
*{{Citation | last1=Fantechi | first1=Barbara | last2=Göttsche | first2=Lothar | last3=Illusie | first3=Luc | author3-link=Luc Illusie | last4=Kleiman | first4=Steven L. | author4-link=Steven Kleiman | last5=Nitsure | first5=Nitin | last6=Vistoli | first6=Angelo | title=Fundamental algebraic geometry | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys and Monographs | isbn=978-0-8218-3541-8 | mr=2222646 | year=2005 | volume=123|url=https://books.google.com/books?id=JhDloxGpOA0C}}
*{{Citation | last1=Fogarty | first1=John | title=Algebraic families on an algebraic surface | mr=0237496 | year=1968 | journal=[[American Journal of Mathematics]] | volume=90 | pages=511–521 | doi=10.2307/2373541 | jstor=2373541 | issue=2 | publisher=The Johns Hopkins University Press}}
*{{Citation | last1=Fogarty | first1=John | title=Truncated Hilbert functors | url=http://gdz.sub.uni-goettingen.de/no_cache/en/dms/load/img/?IDDOC=252601 | mr=0244268 | year=1969 | journal=[[Journal für die reine und angewandte Mathematik]] | volume=234 | pages=65–88}}
*{{Citation | last1=Fogarty | first1=John | title=Algebraic families on an algebraic surface. II. The Picard scheme of the punctual Hilbert scheme | mr=0335512 | year=1973 | journal=[[American Journal of Mathematics]] | volume=95 | pages=660–687 | doi=10.2307/2373734 | jstor=2373734 | issue=3 | publisher=The Johns Hopkins University Press}}
*{{Citation | last1=Göttsche | first1=Lothar | title=Hilbert schemes of zero-dimensional subschemes of smooth varieties | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Lecture Notes in Mathematics | isbn=978-3-540-57814-7 | doi=10.1007/BFb0073491 | mr=1312161 | year=1994 | volume=1572}}
*{{Citation | last1=Grothendieck | first1=Alexander | author1-link=Alexander Grothendieck | series=Séminaire Bourbaki 221 |  year=1961 | title=Techniques de construction et théorèmes d'existence en géométrie algébrique. IV. Les schémas de Hilbert|url=http://www.numdam.org/item?id=SB_1960-1961__6__249_0 }} Reprinted in {{Citation | title=Séminaire Bourbaki, Vol. 6 | publisher=[[Société Mathématique de France]] | location=Paris | mr=1611822 | year=1995  | pages=249–276|isbn= 2-85629-039-6 |author1=Adrien Douady |author2=Roger Godement |author3=Alain Guichardet ... }}
*{{Citation | last1=Hartshorne | first1=Robin | author1-link=Robin Hartshorne | title=Connectedness of the Hilbert scheme | url=http://www.numdam.org/item?id=PMIHES_1966__29__5_0 | mr=0213368 | year=1966 | journal=[[Publications Mathématiques de l'IHÉS]] | issue=29 | pages=5–48}}
*{{citation|authorlink=F. S. Macaulay|last=Macaulay|first= F. S.|year=1927 |title=Some properties of enumeration in the theory of modular systems
|journal=Proceedings L. M. S. Series 2 |volume=26|pages= 531–555|doi=10.1112/plms/s2-26.1.531}}
*{{Citation | last1=Mumford | first1=David | author1-link=David Mumford | title=Lectures on Curves on an Algebraic Surface | publisher=[[Princeton University Press]] | series=Annals of Mathematics Studies | isbn=978-0-691-07993-6 | volume=59}}
*{{Citation | last1=Nakajima | first1=Hiraku | title=Lectures on Hilbert schemes of points on surfaces | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=University Lecture Series | isbn=978-0-8218-1956-2 | mr=1711344 | year=1999 | volume=18}}
*{{Citation | last1=Nitsure | first1=Nitin | title=Fundamental algebraic geometry | arxiv=math/0504590 | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Math. Surveys Monogr. | mr=2223407 | year=2005 | volume=123 | chapter=Construction of Hilbert and Quot schemes | pages=105–137| bibcode=2005math......4590N }}
*{{Citation | last1=Qin | first1=Zhenbo | title=Hilbert schemes of points and infinite dimensional Lie algebras | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Mathematical Surveys and Monographs | isbn=978-1-4704-4188-3 | year=2018 | volume=228}}

==External links==
*{{citation|last=Bertram|first=Aaron|url=http://www.math.utah.edu/~bertram/courses/hilbert/|title=Construction of the Hilbert scheme|year=1999|accessdate=2008-09-06}}
*{{citation|last1=Bolognese|first1=Barbara|last2=Losev|first2=Ivan|title=A general introduction to the Hilbert scheme of points on the plane|url=http://www.northeastern.edu/iloseu/Barbara_14_complete.pdf|deadurl=bot: unknown|archiveurl=https://web.archive.org/web/20170830033238/http://www.northeastern.edu/iloseu/Barbara_14_complete.pdf|archivedate=2017-08-30|df=}}
*{{citation |last=Maclagan |first=Diane |title=Notes on Hilbert Schemes |url=http://homepages.warwick.ac.uk/staff/D.Maclagan/papers/HilbertSchemesNotes.pdf |deadurl=bot: unknown |archiveurl=https://web.archive.org/web/20160307150146/http://homepages.warwick.ac.uk/staff/D.Maclagan/papers/HilbertSchemesNotes.pdf |archivedate=2016-03-07 |df= }}

[[Category:Scheme theory]]
[[Category:Algebraic geometry]]
[[Category:Differential geometry]]
[[Category:Moduli theory]]</text>
      <sha1>hvkr1gw5qqqowkbivne3i3k3g0e723d</sha1>
    </revision>
  </page>
  <page>
    <title>John von Neumann Lecture</title>
    <ns>0</ns>
    <id>53878654</id>
    <revision>
      <id>858512127</id>
      <parentid>797754208</parentid>
      <timestamp>2018-09-07T18:22:03Z</timestamp>
      <contributor>
        <username>Plucas58</username>
        <id>9766640</id>
      </contributor>
      <comment>Update to 2018</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4616">{{primary sources|date=April 2017}}

{{Infobox award
| name        = John von Neumann Lecture Prize
| image       = 
| caption     = 
| image_size  =
| description = outstanding contributions to applied mathematical sciences and for effective communication of these ideas to the community
| presenter   = [[Society for Industrial and Applied Mathematics]]
| country     = [[United States]]
| reward      = USD $5,000
| year        = {{start date and age|1959}}
| year2       = 
| website     = {{URL|https://www.siam.org/prizes/sponsored/vonneumann.php}}
}}
The '''John von Neumann Lecture Prize''' was established in 1959 with funds from [[IBM]] and other industry corporations, and is awarded for "outstanding and distinguished contributions to the field of applied mathematical sciences and for the effective communication of these ideas to the community".&lt;ref&gt;{{cite web|title=The John von Neumann Lecture|url=https://www.siam.org/prizes/sponsored/vonneumann.php|website=Society for Industrial and Applied Mathematics|accessdate=25 April 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Morris|first1=Amanda|title=Matkowsky Receives John von Neumann Lecture Prize|url=http://www.mccormick.northwestern.edu/news/articles/2017/03/matkowsky-receives-john-von-neumann-lecture-prize.html|website=Northwestern Engineering|accessdate=25 April 2017|date=1 Mar 2017}}&lt;/ref&gt; It is considered the highest honor bestowed by the [[Society for Industrial and Applied Mathematics]] (SIAM).&lt;ref&gt;{{cite web|title=ACM NEWS Donald E. Knuth Awarded SIAM’s Highest Honor, Delivers the John von Neumann Lecture|url=http://cacm.acm.org/news/205664-donald-e-knuth-awarded-siams-highest-honor-delivers-the-john-von-neumann-lecture/fulltext|website=Communications of the ACM|publisher=Association for Computing Machinery|accessdate=25 April 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last1=Fung|first1=Aliza|title=Prize Speakers|url=http://www.iciam2011.com/index.php?option=com_content&amp;view=article&amp;id=3&amp;Itemid=27|website=International Congress on Industrial and Applied Mathematics|accessdate=25 April 2017|language=en-gb}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|last1=Manteuffel|first1=Tom|last2=Crowley|first2=Jim|title=SIAM Turns 50|journal=Notices of the AMS|date=March 2002|volume=49|issue=3|url=http://www.ams.org/notices/200203/commentary.pdf|accessdate=26 April 2017}}&lt;/ref&gt; The recipient receives a monetary award and presents a survey lecture at the SIAM Annual Meeting.&lt;ref&gt;{{cite web|last1=Maturi|first1=Richard|title=John Von Neumann Revolutionized Economic Theory|url=http://www.investors.com/news/management/leaders-and-success/john-von-neumann-developed-new-economic-and-business-principles/|website=Investor's Business Daily|accessdate=25 April 2017|date=12 December 2014}}&lt;/ref&gt;

==Past lecturers==
Source: [https://www.siam.org/Prizes-Recognition/Major-Prizes-Lectures/Detail/john-von-neumann-lecture Society for Industrial and Applied Mathematics]
*1960 [[Lars Valerian Ahlfors]]
*1961 [[Mark Kac]]
*1962 [[Jean Leray]]
*1963 [[Stanislaw M. Ulam]]
*1964 [[Solomon Lefschetz]]
*1965 [[Freeman J. Dyson]]
*1966 [[Eugene P. Wigner]]
*1967 [[Chia-Chiao Lin]]
*1968 [[Peter D. Lax]]
*1969 [[George F. Carrier]]
*1970 [[James H. Wilkinson]]
*1971 [[Paul A. Samuelson]]
*1974 [[Jule Charney]]
*1975 Sir [[James Lighthill]]
*1976 [[Rene Thom]]
*1977 [[Kenneth J. Arrow]]
*1978 [[Peter Henrici (mathematician)|Peter Henrici]]
*1979 [[Kurt O. Friedrichs]]
*1980 [[Keith Stewartson]]
*1981 [[Garrett Birkhoff]]
*1982 [[David Slepian]]
*1983 [[Joseph B. Keller]]
*1984 [[Jurgen Moser]]
*1985 [[John W. Tukey]]
*1986 [[Jacques-Louis Lions]]
*1987 [[Richard M. Karp]]
*1988 [[Germund Dahlquist]]
*1989 [[Stephen Smale]]
*1990 [[Andrew J. Majda]]
*1992 [[R. Tyrrell Rockafellar]]
*1994 [[Martin D. Kruskal]]
*1996 [[Carl de Boor]]
*1997 [[William Kahan]]
*1998 [[Olga Ladyzhenskaya]]
*1999 [[Charles S. Peskin]]
*2000 [[Persi Diaconis]]
*2001 [[David L. Donoho]]
*2002 [[Eric S. Lander]]
*2003 [[Heinz-Otto Kreiss]]
*2004 [[Alan C. Newell]]
*2005 [[Jerrold E. Marsden]]
*2006 [[George C. Papanicolaou]]
*2007 [[Nancy Kopell]]
*2008 [[David Gottlieb (mathematician)|David Gottlieb]]
*2009 [[Franco Brezzi]]
*2010 [[Bernd Sturmfels]]
*2011 [[Ingrid Daubechies]]
*2012 Sir [[John M. Ball]]
*2013 [[Stanley J. Osher]]
*2014 [[Leslie F. Greengard]]
*2015 [[Jennifer Tour Chayes]]
*2016 [[Donald E. Knuth]]
*2017 [[Bernard J. Matkowsky]]
*2018 [[Charles F. Van Loan]]

==References==
{{Reflist}}

{{Society for Industrial and Applied Mathematics}}
[[Category:Mathematics awards]]
[[Category:Society for Industrial and Applied Mathematics]]
[[Category:Awards established in 1959]]</text>
      <sha1>grkh0vg2ey59w6oe2kilyr29fhxgzbu</sha1>
    </revision>
  </page>
  <page>
    <title>Key derivation function</title>
    <ns>0</ns>
    <id>449781</id>
    <revision>
      <id>858212346</id>
      <parentid>858212310</parentid>
      <timestamp>2018-09-05T19:30:29Z</timestamp>
      <contributor>
        <username>Entranced98</username>
        <id>27199084</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contribs/74.50.209.52|74.50.209.52]] ([[User talk:74.50.209.52|talk]]) to last version by Quondum</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11334">{{more footnotes|date=July 2013}}
{{Use dmy dates|date=January 2016}}
In [[cryptography]], a '''key derivation function''' ('''KDF''') derives one or more [[key (cryptography)|secret key]]s from a secret value such as a master key, a [[password]], or a [[passphrase]] using a [[pseudorandom function]].&lt;ref&gt;{{cite book|author=Bezzi, Michele|chapter=Data privacy|editors=Camenisch, Jan|title=Privacy and Identity Management for Life|publisher=Springer|year=2011|isbn=9783642203176|pages=185–186|url=https://books.google.com/books?id=vYxzh3C6OPUC&amp;pg=PA185|display-authors=etal|display-editors=etal}}&lt;/ref&gt;&lt;ref&gt;{{cite web|author=Kaliski, Burt|author2=[[RSA Laboratories]] |title=RFC 2898 – PKCS #5: Password-Based Cryptography Specification, Version 2.0|work=IETF|url=https://www.ietf.org/rfc/rfc2898.txt}}&lt;/ref&gt; KDFs can be used to stretch keys into longer keys or to obtain keys of a required format, such as converting a group element that is the result of a [[Diffie–Hellman key exchange]] into a symmetric key for use with [[Advanced Encryption Standard|AES]]. Keyed [[cryptographic hash function]]s are popular examples of pseudorandom functions used for key derivation.&lt;ref&gt;{{cite book|author=Zdziarski, Jonathan|title=Hacking and Securing IOS Applications: Stealing Data, Hijacking Software, and How to Prevent It|publisher=O'Reilly Media|year=2012|isbn=9781449318741|pages=252–253|url=https://books.google.com/books?id=2D50GNA1ULsC&amp;pg=PA252}}&lt;/ref&gt;

==Uses of KDFs==
* In conjunction with non-secret parameters to derive one or more keys from a common secret value (which is sometimes also referred to as "key diversification"). Such use may prevent an attacker who obtains a derived key from learning useful information about either the input secret value or any of the other derived keys. A KDF may also be used to ensure that derived keys have other desirable properties, such as avoiding "weak keys" in some specific encryption systems.
* The most common{{citation needed|date=October 2017}} use of KDFs is the '''password hashing''' approach to [[cryptographic hash function#Password verification|password verification]], as used by the [[passwd]] file or [[shadow password]] file. KDFs happen to have the characteristics desired for a "password hash function", even though they were not originally designed for this purpose.{{citation needed|date=October 2017}} The non-secret parameters are called "[[salt (cryptography)|salt]]" in this context.
: In 2013 a [[Password Hashing Competition]] was announced to choose a new, standard algorithm for password hashing. On 20 July 2015 the competition ended and [[Argon2]] was announced as the final winner. Four other algorithms received special recognition: Catena, Lyra2, Makwa and yescrypt.&lt;ref&gt;[https://password-hashing.net/ "Password Hashing Competition"]&lt;/ref&gt;
* As components of multiparty [[key-agreement protocol]]s. Examples of such key derivation functions include [[KDF1]], defined in [[IEEE P1363|IEEE Std 1363-2000]], and similar functions in [[ANSI X9.42]].
* To derive keys from secret passwords or passphrases.
* To derive keys of different length from the ones provided: one example of KDFs designed for this purpose is [[HKDF]].
* [[Key stretching]] and key strengthening.

===Key stretching and key strengthening===
{{Main article|Key stretching}}
Key derivation functions are also used in applications to derive keys from secret passwords or passphrases, which typically do not have the desired properties to be used directly as cryptographic keys. In such applications, it is generally recommended that the key derivation function be made deliberately slow so as to frustrate [[brute-force attack]] or [[dictionary attack]] on the password or passphrase input value.

Such use may be expressed as {{math|1=DK = KDF(key, salt, iterations)}}, where {{math|1=DK}} is the derived key, {{math|1=KDF}} is the key derivation [[subroutine|function]], {{math|1=key}} is the original key or password, {{math|1=salt}} is a random number which acts as cryptographic [[salt (cryptography)|salt]], and {{math|1=iterations}} refers to the number of [[iteration]]s of a sub-function. The derived key is used instead of the original key or password as the key to the system. The values of the salt and the number of iterations (if it is not fixed) are stored with the hashed password or sent as plaintext with an encrypted message.&lt;ref name=salthash&gt;{{cite web|title=Salted Password Hashing – Doing it Right|url=https://crackstation.net/hashing-security.htm|website=CrackStation.net|accessdate=29 January 2015}}&lt;/ref&gt;

The difficulty of a brute force attack increases with the number of iterations.  A practical limit on the iteration count is the unwillingness of users to tolerate a perceptible delay in logging into a computer or seeing a decrypted message. The use of [[salt (cryptography)|salt]] prevents the attackers from precomputing a dictionary of derived keys.&lt;ref name="salthash" /&gt;

An alternative approach, called '''key strengthening''', extends the key with a random salt, but then (unlike in key stretching) securely deletes the salt.&lt;ref&gt;Abadi, Martın, T. Mark A. Lomas, and Roger Needham. "Strengthening passwords." Digital System Research Center, Tech. Rep 33 (1997): 1997.&lt;/ref&gt; This forces both the attacker and legitimate users to perform a brute-force search for the salt value.&lt;ref&gt;U. Manber, "A Simple Scheme to Make Passwords Based on One-Way Functions Much Harder to Crack," Computers &amp; Security, v.15, n.2, 1996, pp.171–176.&lt;/ref&gt; Although the paper that introduced key stretching&lt;ref name="low-entropy"&gt;[http://www.schneier.com/paper-low-entropy.html Secure Applications of Low-Entropy Keys], [[John Kelsey (cryptanalyst)|J. Kelsey]], [[Bruce Schneier|B. Schneier]], C. Hall, and [[David A. Wagner|D. Wagner]] (1997)&lt;/ref&gt; referred to this earlier technique and intentionally chose a different name, the term "key strengthening" is now often (arguably incorrectly) used to refer to key stretching.

==History==
The first{{citation needed|date=June 2015}} deliberately slow (key stretching) password-based key derivation function was called "[[crypt (C)|crypt]]" (or "crypt(3)" after its [[manual page (Unix)|man page]]), and was invented by [[Robert Morris (cryptographer)|Robert Morris]] in 1978. It would encrypt a constant (zero), using the first 8 characters of the user's password as the key, by performing 25 iterations of a modified [[Data Encryption Standard|DES]] encryption algorithm (in which a 12-bit number read from the real-time computer clock is used to perturb the calculations). The resulting 64-bit number is encoded as 11 printable characters and then stored in the [[Unix]] password file.&lt;ref&gt;{{cite web | url=http://cm.bell-labs.com/cm/cs/who/dmr/passwd.ps | archive-url=https://web.archive.org/web/20030322053727/http://cm.bell-labs.com/cm/cs/who/dmr/passwd.ps | dead-url=yes | archive-date=2003-03-22 | title=Password Security: A Case History. | work=Bell Laboratories | author1=Morris, Robert | author2=Thompson, Ken | date=1978-04-03 | accessdate=2011-05-09 }}&lt;/ref&gt; While it was a great advance at the time, increases in processor speeds since the [[PDP-11]] era have made brute-force attacks against crypt feasible, and advances in storage have rendered the 12-bit salt inadequate. The crypt function's design also limits the user password to 8 characters, which limits the keyspace and makes strong [[passphrase]]s impossible.{{citation needed|date=July 2013}}

Modern password-based key derivation functions, such as [[PBKDF2]] (specified in RFC 2898), use a cryptographic hash, such as [[SHA-2]], more salt (e.g. 64 bits and greater) and a high iteration count (often tens or hundreds of thousands).

[[NIST]] requires at least 128 bits of random salt and a NIST-approved cryptographic function, such as the SHA series or AES (MD5 is not approved).&lt;ref&gt;[http://nvlpubs.nist.gov/nistpubs/Legacy/SP/nistspecialpublication800-132.pdf NIST SP 800-132] Section 5.1&lt;/ref&gt; Although high throughput is a desirable property in general-purpose hash functions, the opposite is true in password security applications in which defending against brute-force cracking is a primary concern. The growing use of massively-parallel hardware such as GPUs, FPGAs, and even ASICs for brute-force cracking has made the selection of a suitable algorithms even more critical because the good algorithm should not only enforce a certain amount of computational cost not only on CPUs, but also resist the cost/performance advantages of modern massively-parallel platforms for such tasks.  Various algorithms have been designed specifically for this purpose, including [[bcrypt]], [[scrypt]] and, more recently, [[Argon2]] (the winner of the [[Password Hashing Competition]]). The large-scale [[Ashley Madison data breach]] in which roughly 36 million passwords hashes were stolen by attackers illustrated the importance of algorithm selection in securing passwords. Although bcrypt was employed to protect the hashes (making large scale brute-force cracking expensive and time-consuming), a significant portion of the accounts in the compromised data also contained a password hash based on the general-purpose [[MD5]] algorithm which made it possible for over 11 million of the passwords to be cracked in a matter of weeks.&lt;ref&gt;{{cite web|url=https://arstechnica.com/security/2015/09/once-seen-as-bulletproof-11-million-ashley-madison-passwords-already-cracked/|title=Once seen as bulletproof, 11 million+ Ashley Madison passwords already cracked|work=[[Ars Technica]]|last=Goodin|first=Dan|date=10 September 2015|accessdate=10 September 2015}}&lt;/ref&gt;

In June 2017, NIST issued a new revision of their digital authentication guidelines, NIST SP 800-63B-3,&lt;ref name=sp800-63B&gt;{{cite journal | url = https://doi.org/10.6028/NIST.SP.800-63b | title = SP 800-63B-3 – Digital Identity Guidelines, Authentication and Lifecycle Management | format = PDF | publisher = NIST | date = June 2017 | accessdate = August 6, 2017 | doi=10.6028/NIST.SP.800-63b | author=Grassi Paul A}}&lt;/ref&gt;{{rp|5.1.1.1}} stating that: "Verifiers SHALL store memorized secrets [i.e. passwords] in a form that is resistant to offline attacks. Memorized secrets SHALL be salted and hashed using a suitable one-way key derivation function. Key derivation functions take a password, a salt, and a cost factor as inputs then generate a password hash. Their purpose is to make each password guessing trial by an attacker who has obtained a password hash file expensive and therefore the cost of a guessing attack high or prohibitive." and that "The salt SHALL be at least 32 bits in length and be chosen arbitrarily so as to minimize salt value collisions among stored hashes."

== References ==
{{Reflist}}

== Further reading ==
* {{cite web
| last = Percival| first = Colin 
| title = Stronger Key Derivation via Sequential Memory-Hard Functions
| date = May 2009
| url = http://www.tarsnap.com/scrypt/scrypt.pdf
| format = PDF
| work = BSDCan'09 Presentation
| accessdate = 2009-05-19 }}
* [https://web.archive.org/web/20101229081854/http://di-mgt.com.au/cryptoKDFs.html Key Derivation Functions]

{{Cryptography navbox}}

{{DEFAULTSORT:Key Derivation Function}}
[[Category:Cryptography]]
[[Category:Key management]]
[[Category:Key derivation functions|*]]</text>
      <sha1>9e032pw9i1udhdo786knnl8km895jhy</sha1>
    </revision>
  </page>
  <page>
    <title>Lattice word</title>
    <ns>0</ns>
    <id>22361644</id>
    <revision>
      <id>858438404</id>
      <parentid>829108839</parentid>
      <timestamp>2018-09-07T05:30:59Z</timestamp>
      <contributor>
        <username>Maxal</username>
        <id>237258</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1359">In [[mathematics]],  a '''lattice word''' (or '''lattice permutation''') is a [[string (computer science)#Formal theory|string]] composed of positive [[integer]]s, in which every [[Prefix (computer science)|prefix]] contains at least as many positive integers ''i'' as integers ''i''&amp;nbsp;+ 1. 

A '''reverse lattice word''', or '''Yamanouchi word''', is a string whose [[String (computer science)#Reversal|reversal]] is a lattice word.

== Examples ==

For instance, 11122121 is a lattice permutation, so 12122111 is a Yamanouchi word, but 12122111 is not a lattice permutation, since the sub-word 12122 contains more two's than one's.

== See also ==
* [[Dyck word]]

==References==
*{{Citation | last1=Fulton | first1=William | author1-link=William Fulton (mathematician) | title=Young tableaux | publisher=[[Cambridge University Press]] | series=London Mathematical Society Student Texts &lt;!--| isbn=978-0-521-56144-0--&gt;|isbn= 978-0-521-56724-4 |mr=1464693 | year=1997 | volume=35}}
*{{Citation | last1=Macdonald | first1=Ian G. | author1-link=Ian G. Macdonald | title=Symmetric functions and Hall polynomials | edition=Second | series=Oxford Mathematical Monographs | publisher=[[The Clarendon Press]] and [[Oxford University Press]] | year=1995 | isbn=0-19-853489-2 |mr=1354144 }}

[[Category:Algebraic combinatorics]]
[[Category:Combinatorics on words]]</text>
      <sha1>5hpm86yyw9ujrg5b0e7smecbwek1mbo</sha1>
    </revision>
  </page>
  <page>
    <title>Lie group</title>
    <ns>0</ns>
    <id>17945</id>
    <revision>
      <id>868886810</id>
      <parentid>868886747</parentid>
      <timestamp>2018-11-15T02:32:15Z</timestamp>
      <contributor>
        <username>Mathphysman</username>
        <id>19811191</id>
      </contributor>
      <minor/>
      <comment>/* The exponential map */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="58723"> 
{{Lie groups}}
{{Group theory sidebar}}
{{confuse|Group of Lie type}}
In [[mathematics]], a '''Lie group''' (pronounced {{IPAc-en|l|iː}} "Lee") is a [[group (mathematics)|group]] that is also a [[differentiable manifold]], with the property that the group operations are smooth. Lie groups are named after Norwegian mathematician [[Sophus Lie]], who laid the foundations of the theory of continuous [[transformation group]]s.

In rough terms, a Lie group is a continuous group, that is, one whose elements are described by several real parameters. As such, Lie groups provide a natural model for the concept of continuous symmetry, such as rotational symmetry in three dimensions. Lie groups are widely used in many parts of modern mathematics and physics. Lie's original motivation for introducing Lie groups was to model the continuous symmetries of [[differential equations]], in much the same way that finite groups are used in [[Galois theory]] to model the discrete symmetries of [[algebraic equations]].

== Overview ==
[[File:Circle as Lie group.svg|right|thumb|The set of all [[complex number]]s with [[absolute value]] 1 (corresponding to points on the [[circle]] of center 0 and radius 1 in the [[complex plane]]) is a Lie group under complex multiplication:  the [[circle group]].]]
Lie groups are [[smoothness|smooth]] [[differentiable manifold]]s and as such can be studied using [[differential calculus]], in contrast with the case of more general [[topological group]]s. One of the key ideas in the theory of Lie groups is to replace the ''global'' object, the group, with its ''local'' or linearized version, which Lie himself called its "infinitesimal group" and which has since become known as its [[Lie algebra]].

Lie groups play an enormous role in modern [[geometry]], on several different levels. [[Felix Klein]] argued in his [[Erlangen program]] that one can consider various "geometries" by specifying an appropriate transformation group that leaves certain geometric properties [[Invariant (mathematics)|invariant]]. Thus [[Euclidean geometry]] corresponds to the choice of the group [[Euclidean group|E(3)]] of distance-preserving transformations of the Euclidean space '''R'''&lt;sup&gt;3&lt;/sup&gt;, [[conformal geometry]] corresponds to enlarging the group to the [[conformal group]], whereas in [[projective geometry]] one is interested in the properties invariant under the [[projective group]]. This idea later led to the notion of a [[G-structure]], where ''G'' is a Lie group of "local" symmetries of a manifold.

Lie groups (and their associated Lie algebras) play a major role in modern physics, with the Lie group typically playing the role of a symmetry of a physical system. Here, the [[Representation of a Lie group|representations]] of the Lie group (or of its [[Lie algebra representation|Lie algebra]]) are especially important. Representation theory [[Particle physics and representation theory|is used extensively in particle physics]]. Groups whose representations are of particular importance include [[3D_rotation_group|the rotation group SO(3)]] (or its [[3D_rotation_group#Connection_between_SO(3)_and_SU(2)|double cover SU(2)]]),  [[Clebsch–Gordan coefficients for SU(3)#Representations of the SU.283.29 group|the special unitary group SU(3)]] and the [[Representation theory of the Poincaré group|Poincaré group]].

On a "global" level, whenever a Lie group [[group action|acts]] on a geometric object, such as a [[Riemannian manifold|Riemannian]] or a [[symplectic manifold]], this action provides a measure of rigidity and yields a rich algebraic structure. The presence of continuous symmetries expressed via a [[Lie group action]] on a manifold places strong constraints on its geometry and facilitates [[global analysis|analysis]] on the manifold. Linear actions of Lie groups are especially important, and are studied in [[representation theory]].

In the 1940s&amp;ndash;1950s, [[Ellis Kolchin]], [[Armand Borel]], and [[Claude Chevalley]] realised that many foundational results concerning Lie groups can be developed completely algebraically, giving rise to the theory of [[algebraic group]]s defined over an arbitrary [[field (mathematics)|field]]. This insight opened new possibilities in pure algebra, by providing a uniform construction for most [[finite simple group]]s, as well as in [[algebraic geometry]]. The theory of [[automorphic form]]s, an important branch of modern [[number theory]], deals extensively with analogues of Lie groups over [[adele ring]]s; [[p-adic number|''p''-adic]] Lie groups play an important role, via their connections with Galois representations in number theory.

== Definitions and examples ==
A '''real Lie group''' is a [[group (mathematics)|group]] that is also a finite-dimensional real [[Differentiable manifold#Definition|smooth manifold]], in which the group operations of [[multiplication]] and inversion are [[smooth map]]s.  Smoothness of the group multiplication

:&lt;math&gt; \mu:G\times G\to G\quad \mu(x,y)=xy&lt;/math&gt;

means that μ is a smooth mapping of the [[Manifold#Cartesian products|product manifold]] ''G''×''G'' into ''G''.  These two requirements can be combined to the single requirement that the mapping
:&lt;math&gt;(x,y)\mapsto x^{-1}y&lt;/math&gt;
be a smooth mapping of the product manifold into ''G''.

=== First examples ===
* The 2×2 [[real number|real]] [[invertible matrix|invertible matrices]] form a group under multiplication, denoted by [[general linear group|GL(2, '''R''')]] or by GL&lt;sub&gt;2&lt;/sub&gt;('''R'''):

:: &lt;math&gt; \operatorname{GL}(2, \mathbf{R}) = \left\{A=\begin{pmatrix}a&amp;b\\c&amp;d\end{pmatrix}: \det A=ad-bc \ne 0\right\}. &lt;/math&gt;

: This is a four-dimensional [[compact space|noncompact]] real Lie group; it is an open subset of &lt;math&gt;\mathbb R^4&lt;/math&gt;. This group is [[connected space|disconnected]]; it has two connected components corresponding to the positive and negative values of the [[determinant]].

* The [[rotation (mathematics)|rotation]] matrices form a [[subgroup]] of GL(2, '''R'''), denoted by SO(2, '''R'''). It is a Lie group in its own right: specifically, a one-dimensional compact connected Lie group which is [[diffeomorphic]] to the [[circle]]. Using the rotation angle &lt;math&gt;\varphi&lt;/math&gt; as a parameter, this group can be [[parametric equations|parametrized]] as follows:
:: &lt;math&gt; \operatorname{SO}(2, \mathbf{R}) =\left\{\begin{pmatrix} \cos\varphi &amp; -\sin \varphi \\ \sin \varphi &amp; \cos \varphi \end{pmatrix}:  \varphi\in\mathbf{R}/2\pi\mathbf{Z}\right\}. &lt;/math&gt;

:Addition of the angles corresponds to multiplication of the elements of SO(2, '''R'''), and taking the opposite angle corresponds to inversion. Thus both multiplication and inversion are differentiable maps.

* The [[Affine group#Matrix representation|affine group of one dimension]] is a two-dimensional matrix Lie group, consisting of &lt;math&gt;2\times 2&lt;/math&gt; real, upper-triangular matrices, with the first diagonal entry being positive and the second diagonal entry being 1. Thus, the group consists of matrices of the form
::&lt;math&gt; A= \left( \begin{array}{cc} a &amp; b\\ 0 &amp; 1 \end{array}\right),\quad a&gt;0,\, b\in\mathbb{R}.&lt;/math&gt;

=== Counterexample ===
We now present an example of a group with an uncountable number of elements that is not a Lie group under a certain topology.   The group given by 

:&lt;math&gt;H = \left\{\left .\left(\begin{matrix}e^{2\pi i\theta} &amp; 0\\0 &amp; e^{2\pi ia\theta}\end{matrix}\right) \right| \theta \in \mathbb{R}\right\} \subset \mathbb{T}^2 = \left\{\left .\left(\begin{matrix}e^{2\pi i\theta} &amp; 0\\0 &amp; e^{2\pi i\phi}\end{matrix}\right)\right| \theta, \phi \in \mathbb{R}\right\},&lt;/math&gt;

with &lt;math&gt;a \in \mathbb P = \mathbb R\setminus\mathbb Q&lt;/math&gt; a ''fixed'' [[irrational number]], is a subgroup of the [[torus]] &lt;math&gt;\mathbb T^2&lt;/math&gt; that is not a Lie group when given the [[subspace topology]].&lt;ref&gt;{{harvnb|Rossmann|2001|loc=Chapter 2.}}&lt;/ref&gt; If we take any small neighborhood &lt;math&gt;U&lt;/math&gt; of a point &lt;math&gt;h&lt;/math&gt; in &lt;math&gt;H&lt;/math&gt;, for example, the portion of &lt;math&gt;H&lt;/math&gt; in &lt;math&gt;U&lt;/math&gt; is disconnected. The group &lt;math&gt;H&lt;/math&gt; winds repeatedly around the torus and forms a [[dense set|dense]] subgroup of &lt;math&gt;\mathbb T^2&lt;/math&gt;.

[[File:Irrational line on a torus.png|thumb|right|A portion of the group &lt;math&gt;H&lt;/math&gt; inside &lt;math&gt;\mathbb T^2&lt;/math&gt;. Small neighborhoods of the element &lt;math&gt;h\in H&lt;/math&gt; are disconnected in the subset topology on &lt;math&gt;H&lt;/math&gt;]]

The group &lt;math&gt;H&lt;/math&gt; can, however, be given a different topology, in which the distance between two points &lt;math&gt;h_1,h_2\in H&lt;/math&gt; is defined as the length of the shortest path ''in the group H'' joining &lt;math&gt;h_1&lt;/math&gt; to &lt;math&gt;h_2&lt;/math&gt;. In this topology, &lt;math&gt;H&lt;/math&gt; is identified homeomorphically with the real line by identifying each element with the number &lt;math&gt;\theta&lt;/math&gt; in the definition of &lt;math&gt;H&lt;/math&gt;. With this topology, &lt;math&gt;H&lt;/math&gt; is just the group of real numbers under addition and is therefore a Lie group.

The group &lt;math&gt;H&lt;/math&gt; is an example of a "[[Lie subgroup]]" of a Lie group that is not closed. See the discussion below of Lie subgroups in the section on basic concepts.

===Matrix Lie groups===

Let GL(''n''; '''C''') denote the group of {{nowrap|''n'' × ''n''}} invertible matrices with entries in '''C'''. Any [[Closed subgroup theorem|closed subgroup]] of GL(''n'', '''C''') is a Lie group;&lt;ref&gt;{{harvnb|Hall|2015}} Corollary 3.45&lt;/ref&gt; Lie groups of this sort are called '''matrix Lie groups.''' Since most of the interesting examples of Lie groups can be realized as matrix Lie groups, some textbooks restrict attention to this class, including those of Hall&lt;ref&gt;{{harvnb|Hall|2015}}&lt;/ref&gt; and Rossmann.&lt;ref&gt;{{harvnb|Rossmann|2001}}&lt;/ref&gt; Restricting attention to matrix Lie groups simplifies the definition of the Lie algebra and the exponential map. The following are standard examples of matrix Lie groups.
*The [[special linear group]]s over '''R''' and '''C''', SL(''n'', '''R''') and SL(''n'', '''C'''), consisting of {{nowrap|''n'' × ''n''}} matrices with determinant one and entries in '''R''' or '''C'''
*The [[unitary group]]s and special unitary groups, U(''n'') and SU(''n''), consisting of {{nowrap|''n'' × ''n''}} complex matrices satisfying &lt;math&gt;U^*=U^{-1}&lt;/math&gt; (and also &lt;math&gt;\mathrm{det}(U)=1&lt;/math&gt; in the case of SU(''n''))
*The [[orthogonal group]]s and special orthogonal groups, O(''n'') and SO(''n''), consisting of {{nowrap|''n'' × ''n''}} real matrices satisfying &lt;math&gt;R^\mathrm{T}=R^{-1}&lt;/math&gt; (and also &lt;math&gt;\mathrm{det}(R)=1&lt;/math&gt; in the case of SO(''n''))
All of the preceding examples fall under the heading of the [[classical group]]s.

=== Related concepts ===
A '''[[complex Lie group]]''' is defined in the same way using [[complex manifold]]s rather than real ones (example: SL(2, '''C''')), and similarly, using an alternate [[Complete metric space#Completion|metric completion]] of '''Q''', one can define a '''''p''-adic Lie group''' over the [[p-adic number|''p''-adic numbers]], a topological group in which each point has a ''p''-adic neighborhood. [[Hilbert's fifth problem]] asked whether replacing differentiable manifolds with topological or analytic ones can yield new examples. The answer to this question turned out to be negative: in 1952, [[Andrew Gleason|Gleason]], [[Deane Montgomery|Montgomery]] and [[Leo Zippin|Zippin]] showed that if ''G'' is a topological manifold with continuous group operations, then there exists exactly one analytic structure on ''G'' which turns it into a Lie group (see also [[Hilbert&amp;ndash;Smith conjecture]]). If the underlying manifold is allowed to be infinite-dimensional (for example, a [[Hilbert manifold]]), then one arrives at the notion of an infinite-dimensional Lie group.  It is possible to define analogues of many [[group of Lie type|Lie groups over finite fields]], and these give most of the examples of [[finite simple group]]s.

The language of [[category theory]] provides a concise definition for Lie groups: a Lie group is a [[group object]] in the [[category (mathematics)|category]] of smooth manifolds. This is important, because it allows generalization of the notion of a Lie group to [[supergroup (physics)|Lie supergroups]].

== More examples of Lie groups ==
{{see also|Table of Lie groups|List of simple Lie groups}}

Lie groups occur in abundance throughout mathematics and physics. [[Matrix group]]s or [[algebraic group]]s are (roughly) groups of matrices (for example, [[orthogonal group|orthogonal]] and [[symplectic group]]s), and these give most of the more common examples of Lie groups.

===Dimensions one and two===
The only connected Lie groups with dimension one are the real line &lt;math&gt;\mathbb{R}&lt;/math&gt; (with the group operation being addition) and the group &lt;math&gt;S^1&lt;/math&gt; of complex numbers with absolute value one (with the group operation being multiplication). The &lt;math&gt;S^1&lt;/math&gt; group is often denoted as &lt;math&gt;U(1)&lt;/math&gt;, the group of &lt;math&gt;1\times 1&lt;/math&gt; unitary matrices.

In two dimensions, if we restrict attention to simply connected groups, then they are classified by their Lie algebras. There are (up to isomorphism) only two Lie algebras of dimension two. The associated simply connected Lie groups are &lt;math&gt;\mathbb{R}^2&lt;/math&gt; (with the group operation being vector addition) and the affine group in dimension one, described in the previous subsection under "first examples."

=== Additional examples ===
*The [[Special unitary group#n_.3D_2|group SU(2)]] is the group of &lt;math&gt;2\times 2&lt;/math&gt; unitary matrices with determinant 1. Topologically, SU(2) is the 3-sphere &lt;math&gt;S^3&lt;/math&gt;; as a group, it may be identified with the group of unit [[quaternion]]s. 
*The [[Heisenberg group]] is a connected [[nilpotent group|nilpotent]] Lie group of dimension 3, playing a key role in [[quantum mechanics]].
*The [[Lorentz group]] is a 6-dimensional Lie group of linear [[isometry|isometries]] of the [[Minkowski space]].
*The [[Poincaré group]] is a 10-dimensional Lie group of [[affine transformation|affine]] isometries of the Minkowski space.
*The [[exceptional Lie group]]s of types [[G2 (mathematics)|''G''&lt;sub&gt;2&lt;/sub&gt;]], [[F4 (mathematics)|''F''&lt;sub&gt;4&lt;/sub&gt;]], [[E6 (mathematics)|''E''&lt;sub&gt;6&lt;/sub&gt;]], [[E7 (mathematics)|''E''&lt;sub&gt;7&lt;/sub&gt;]], [[E8 (mathematics)|''E''&lt;sub&gt;8&lt;/sub&gt;]] have dimensions 14, 52, 78, 133, and 248. Along with the A-B-C-D series of [[simple Lie group]]s, the exceptional groups complete the list of simple Lie groups. 
*The [[symplectic group]] Sp(2''n'', '''R''') consists of all 2''n'' × 2''n'' matrices preserving a ''[[symplectic form]]'' on '''R'''&lt;sup&gt;2''n''&lt;/sup&gt;. It is a connected Lie group of dimension 2''n''&lt;sup&gt;2&lt;/sup&gt; + ''n''.

=== Constructions ===
There are several standard ways to form new Lie groups from old ones:
*The product of two Lie groups is a Lie group.
*Any [[Closed set|topologically closed]] subgroup of a Lie group is a Lie group. This is known as the [[Closed subgroup theorem]] or '''Cartan's theorem'''.
*The quotient of a Lie group by a closed normal subgroup is a Lie group.
*The [[universal cover]] of a connected Lie group is a Lie group. For example, the group '''R''' is the universal cover of the circle group '''S'''&lt;sup&gt;1&lt;/sup&gt;. In fact any covering of a differentiable manifold is also a differentiable manifold, but by specifying ''universal'' cover, one guarantees a group structure (compatible with its other structures).

=== Related notions ===
Some examples of groups that are ''not'' Lie groups (except in the trivial sense that any group can be viewed as a 0-dimensional Lie group, with the [[discrete topology]]), are:

*Infinite-dimensional groups, such as the additive group of an infinite-dimensional real vector space. These are not Lie groups as they are not ''finite-dimensional'' manifolds.
*Some [[totally disconnected group]]s, such as the [[Galois group]] of an infinite extension of fields,  or the additive group of the ''p''-adic numbers. These are not Lie groups because their underlying spaces are not real manifolds. (Some of these groups are "''p''-adic Lie groups".) In general, only topological groups having similar [[local property|local properties]] to '''R'''&lt;sup&gt;''n''&lt;/sup&gt; for some positive integer ''n'' can be Lie groups (of course they must also have a differentiable structure).

== Basic concepts ==

=== The Lie algebra associated with a Lie group ===
{{main|Lie group–Lie algebra correspondence}}
To every Lie group we can associate a Lie algebra whose underlying vector space is the tangent space of the Lie group at the identity element and which completely captures the local structure of the group. Informally we can think of elements of the Lie algebra as elements of the group that are "[[infinitesimal]]ly close" to the identity, and the Lie bracket of the Lie algebra is related to the [[commutator]] of two such infinitesimal elements. Before giving the abstract definition we give a few examples:
* The Lie algebra of the vector space '''R'''&lt;sup&gt;''n''&lt;/sup&gt; is just '''R'''&lt;sup&gt;''n''&lt;/sup&gt; with the Lie bracket given by &lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; [''A'',&amp;nbsp;''B''] = 0. &lt;br /&gt; (In general the Lie bracket of a connected Lie group is always 0 if and only if the Lie group is abelian.)
* The Lie algebra of the [[general linear group]] GL(''n'', '''C''') of invertible matrices is the vector space M(''n'', '''C''') of square matrices with the Lie bracket given by &lt;br /&gt;&amp;nbsp;&amp;nbsp;&amp;nbsp; [''A'',&amp;nbsp;''B''] = ''AB''&amp;nbsp;&amp;minus;&amp;nbsp;''BA''.
*If ''G'' is a closed subgroup of GL(''n'', '''C''') then the Lie algebra of ''G'' can be thought of informally as the matrices ''m'' of M(''n'', '''R''') such that 1&amp;nbsp;+&amp;nbsp;ε''m'' is in ''G'', where ε is an infinitesimal positive number with ε&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;0 (of course, no such real number ε exists). For example, the orthogonal group O(''n'', '''R''') consists of matrices ''A'' with ''AA''&lt;sup&gt;T&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;1, so the Lie algebra consists of the matrices ''m'' with (1&amp;nbsp;+&amp;nbsp;ε''m'')(1&amp;nbsp;+&amp;nbsp;ε''m'')&lt;sup&gt;T&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;1, which is equivalent to ''m''&amp;nbsp;+&amp;nbsp;''m''&lt;sup&gt;T&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;0 because ε&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;=&amp;nbsp;0.
*The preceding description can be made more rigorous as follows. The Lie algebra of a closed subgroup ''G'' of GL(''n'', '''C'''), may be computed as
:&lt;math&gt;\operatorname{Lie}(G) = \{ X \in M(n;\mathbb{C}) | \operatorname{exp}(tX) \in G \text{ for all } t \text{ in } \mathbb{\mathbb{R}} \},&lt;/math&gt;&lt;ref&gt;{{harvnb|Helgason|1978|loc=Ch. II, § 2, Proposition 2.7.}}&lt;/ref&gt;&lt;ref&gt;{{harvnb|Hall|2015|}}&lt;/ref&gt;
where exp(''tX'') is defined using the [[matrix exponential]]. It can then be shown that the Lie algebra of ''G'' is a real vector space that is closed under the bracket operation, &lt;math&gt;[X,Y]=XY-YX&lt;/math&gt;.&lt;ref&gt;{{harvnb|Hall|2015}} Theorem 3.20&lt;/ref&gt;

The concrete definition given above for matrix groups is easy to work with, but has some minor problems: to use it we first need to represent a Lie group as a group of matrices, but not all Lie groups can be represented in this way, and even it is not obvious that the Lie algebra is independent of the representation we use.&lt;ref&gt;But see {{harvnb|Hall|2015}}, Proposition 3.30 and Exercise 8 in Chapter 3&lt;/ref&gt; To get around these problems we give 
the general definition of the Lie algebra of a Lie group (in 4 steps):
#Vector fields on any smooth manifold ''M'' can be thought of as [[Derivation (abstract algebra)|derivations]] ''X'' of the ring of smooth functions on the manifold, and therefore form a Lie algebra under the Lie bracket [''X'',&amp;nbsp;''Y'']&amp;nbsp;=&amp;nbsp;''XY''&amp;nbsp;&amp;minus;&amp;nbsp;''YX'', because the [[Lie bracket of vector fields|Lie bracket]] of any two derivations is a derivation.
#If ''G'' is any group acting smoothly on the manifold ''M'', then it acts on the vector fields, and the vector space  of vector fields fixed by the group is closed under the Lie bracket and therefore also forms a Lie algebra.
#We apply this construction to the case when the manifold ''M'' is the underlying space of a Lie group&amp;nbsp;''G'', with ''G'' acting on ''G''&amp;nbsp;=&amp;nbsp;''M'' by left translations ''L&lt;sub&gt;g&lt;/sub&gt;''(''h'')&amp;nbsp;=&amp;nbsp;''gh''. This shows that the space of left invariant vector fields (vector fields satisfying ''L&lt;sub&gt;g&lt;/sub&gt;''&lt;sub&gt;*&lt;/sub&gt;''X&lt;sub&gt;h&lt;/sub&gt;'' =&amp;nbsp;''X&lt;sub&gt;gh&lt;/sub&gt;'' for every ''h'' in ''G'', where ''L&lt;sub&gt;g&lt;/sub&gt;''&lt;sub&gt;*&lt;/sub&gt; denotes the differential of ''L&lt;sub&gt;g&lt;/sub&gt;'') on a Lie group is a Lie algebra under the Lie bracket of vector fields.
#Any tangent vector at the identity of a Lie group can be extended to a left invariant vector field by left translating the tangent vector to other points of the manifold. Specifically, the left invariant extension of an element ''v'' of the tangent space at the identity is the vector field defined by ''v''^&lt;sub&gt;''g''&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''L&lt;sub&gt;g&lt;/sub&gt;''&lt;sub&gt;*&lt;/sub&gt;''v''. This identifies the [[tangent space]] ''T&lt;sub&gt;e&lt;/sub&gt;G'' at the identity with the space of left invariant vector fields, and therefore makes the tangent space at the identity into a Lie algebra, called the Lie algebra of ''G'', usually denoted by a [[Fraktur (typeface sub-classification)|Fraktur]] &lt;math&gt;\mathfrak{g}.&lt;/math&gt; Thus the Lie bracket on &lt;math&gt;\mathfrak{g}&lt;/math&gt; is given explicitly by [''v'',&amp;nbsp;''w'']&amp;nbsp;=&amp;nbsp;[''v''^,&amp;nbsp;''w''^]&lt;sub&gt;''e''&lt;/sub&gt;.

This Lie algebra &lt;math&gt;\mathfrak{g}&lt;/math&gt; is finite-dimensional and it has the same dimension as the manifold ''G''. The Lie algebra of ''G'' determines ''G'' up to "local isomorphism", where two Lie groups are called '''locally isomorphic''' if they look the same near the identity element.
Problems about Lie groups are often solved by first solving the corresponding problem for the Lie algebras, and the result for groups then usually follows easily.  
For example, simple Lie groups are usually classified by first classifying the corresponding Lie algebras.

We could also define a Lie algebra structure on ''T&lt;sub&gt;e&lt;/sub&gt;'' using right invariant vector fields instead of left invariant vector fields. This leads to the same Lie algebra, because the inverse map on ''G'' can be used to identify left invariant vector fields with right invariant vector fields, and acts as &amp;minus;1 on the tangent space ''T&lt;sub&gt;e&lt;/sub&gt;''.

The Lie algebra structure on ''T&lt;sub&gt;e&lt;/sub&gt;'' can also be described as follows:
the commutator operation

: (''x'', ''y'') → ''xyx''&lt;sup&gt;&amp;minus;1&lt;/sup&gt;''y''&lt;sup&gt;&amp;minus;1&lt;/sup&gt;

on ''G'' &amp;times; ''G'' sends (''e'',&amp;nbsp;''e'') to ''e'', so its derivative yields a [[bilinear operator|bilinear operation]] on ''T&lt;sub&gt;e&lt;/sub&gt;G''. This bilinear operation is actually the zero map, but the second derivative, under the proper identification of tangent spaces, yields an operation that satisfies the axioms of a [[Lie algebra#Definition and first properties|Lie bracket]], and it is equal to twice the one defined through left-invariant vector fields.

=== Homomorphisms and isomorphisms ===
If ''G'' and ''H'' are Lie groups, then a Lie group homomorphism ''f'' : ''G'' → ''H''  is a smooth [[group homomorphism]]. In the case of complex Lie groups, such a homomorphism is required to be a [[holomorphic map]]. However, these requirements are a bit stringent; every continuous homomorphism between real Lie groups turns out to be (real) [[analytic map|analytic]].&lt;ref&gt;{{harvnb|Hall|2015}} Corollary 3.50. Hall only claims smoothness, but the same argument shows analyticity.&lt;/ref&gt;

The composition of two Lie homomorphisms is again a homomorphism, and the class of all Lie groups, together with these morphisms, forms a [[category theory|category]]. Moreover, every Lie group homomorphism induces a homomorphism between the corresponding Lie algebras. Let &lt;math&gt;\phi\colon G \to H&lt;/math&gt; be a Lie group homomorphism and let &lt;math&gt;\phi_{*}&lt;/math&gt; be its [[Pushforward (differential)|derivative]] at the identity. If we identify the Lie algebras of ''G'' and ''H'' with their [[tangent space]]s at the identity elements then &lt;math&gt;\phi_{*}&lt;/math&gt; is a map between the corresponding Lie algebras:
:&lt;math&gt;\phi_{*}\colon\mathfrak g \to \mathfrak h&lt;/math&gt;
One can show that &lt;math&gt;\phi_{*}&lt;/math&gt; is actually a [[Lie algebra homomorphism]] (meaning that it is a [[linear map]] which preserves the [[Lie bracket]]). In the language of [[category theory]], we then have a covariant [[functor]] from the category of Lie groups to the category of Lie algebras which sends a Lie group to its Lie algebra and a Lie group homomorphism to its derivative at the identity.

Two Lie groups are called ''isomorphic'' if there exists a [[bijective]] homomorphism between them whose inverse is also a Lie group homomorphism. Equivalently, it is a [[diffeomorphism]] which is also a group homomorphism. 

===Lie group versus Lie algebra isomorphisms===
Isomorphic Lie groups necessarily have isomorphic Lie algebras; it is then reasonable to ask how isomorphism classes of Lie groups relate to isomorphism classes of Lie algebras.

The first result in this direction is [[Lie's third theorem]], which states that every finite-dimensional, real Lie algebra is the Lie algebra of  some (linear) Lie group. One way to prove Lie's third theorem is to use [[Ado's theorem]], which says every finite-dimensional real Lie algebra is isomorphic to a matrix Lie algebra. Meanwhile, for every finite-dimensional matrix Lie algebra, there is a linear group (matrix Lie group) with this algebra as its Lie algebra.&lt;ref&gt;{{harvnb|Hall|2015}} Theorem 5.20&lt;/ref&gt;

On the other hand, Lie groups with isomorphic Lie algebras need not be isomorphic. Furthermore, this result remains true even if we assume the groups are connected. To put it differently, the ''global'' structure of a Lie group is not determined by its Lie algebra; for example, if ''Z'' is any discrete subgroup of the center of ''G'' then ''G'' and ''G''/''Z'' have the same Lie algebra (see the [[table of Lie groups]] for examples). An example of importance in physics are the groups [[Special_unitary_group#The_group_SU(2)|SU(2)]] and [[Rotation group SO(3)|SO(3)]]. These two groups have isomorphic Lie algebras&lt;ref&gt;{{harvnb|Hall|2015}} Example 3.27&lt;/ref&gt;, but the groups themselves are not isomorphic, because SU(2) is simply connected but SO(3) is not.&lt;ref&gt;{{harvnb|Hall|2015}} Section 1.3.4&lt;/ref&gt;

On the other hand, if we require that the Lie group be [[simply connected]], then the global structure is determined by its Lie algebra: two simply connected Lie groups with isomorphic Lie algebras are isomorphic.&lt;ref&gt;{{harvnb|Hall|2015}} Corollary 5.7&lt;/ref&gt; (See the next subsection for more information about simply connected Lie groups.) In light of Lie's third theorem, we may therefore say that there is a one-to-one correspondence between isomorphism classes of finite-dimensional real Lie algebras and isomorphism classes of simply connected Lie groups.

===Simply connected Lie groups===
{{see also|Lie group–Lie algebra correspondence|Fundamental group#Lie groups}}
A Lie group ''G'' is said to be '''[[Simply connected space|simply connected]]''' if every loop in ''G'' can be shrunk continuously to a point in ''G''. This notion is important because of the following result that has simple connectedness as a hypothesis:
:'''Theorem'''&lt;ref&gt;{{harvnb|Hall|2015}} Theorem 5.6&lt;/ref&gt;: Suppose &lt;math&gt;G&lt;/math&gt; and &lt;math&gt;H&lt;/math&gt; are Lie groups with Lie algebras &lt;math&gt;\mathfrak g&lt;/math&gt; and &lt;math&gt;\mathfrak h&lt;/math&gt; and that &lt;math&gt;f:\mathfrak{g}\rightarrow\mathfrak{h}&lt;/math&gt; is a Lie algebra homomorphism. If &lt;math&gt;G&lt;/math&gt; is simply connected, then there is a unique Lie group homomorphism &lt;math&gt;\phi:G\rightarrow H&lt;/math&gt; such that &lt;math&gt;\phi_*=f&lt;/math&gt;, where &lt;math&gt;\phi_*&lt;/math&gt; is the differential of &lt;math&gt;\phi&lt;/math&gt; at the identity.
[[Lie group–Lie algebra correspondence#The correspondence|Lie's third theorem]] says that every finite-dimensional real Lie algebra is the Lie algebra of a Lie group. It follows from Lie's third theorem and the preceding result that every finite-dimensional real Lie algebra is the Lie algebra of a ''unique'' simply connected Lie group.

An example of a simply connected group is the special unitary group [[Special unitary group#n_.3D_2|SU(2)]], which as a manifold is the 3-sphere. The [[rotation group SO(3)]], on the other hand, is not simply connected. (See [[Rotation group SO(3)#Topology|Topology of SO(3)]].) The failure of SO(3) to be simply connected is intimately connected to the distinction between [[integer spin]] and [[half-integer spin]] in quantum mechanics. Other examples of simply connected Lie groups include the special unitary group [[SU(n)]], the spin group (double cover of rotation group) [[Spin(n)]] for &lt;math&gt;n\geq 3&lt;/math&gt;, and the compact symplectic group [[Symplectic group#Sp.28n.29|Sp(n)]].&lt;ref&gt;{{harvnb|Hall|2015}} Section 13.2&lt;/ref&gt;

Methods for determining whether a Lie group is simply connected or not are discussed in the article on [[Fundamental group#Lie groups|fundamental groups of Lie groups]].

===The exponential map===
{{Main|Exponential map (Lie theory)}}
{{see also|derivative of the exponential map|normal coordinates}}

The [[exponential map (Lie theory)|exponential map]] from the Lie algebra &lt;math&gt;M(n;\mathbb C)&lt;/math&gt;  of the [[general linear group]] &lt;math&gt;GL(n;\mathbb C)&lt;/math&gt; to &lt;math&gt;GL(n;\mathbb C)&lt;/math&gt; is defined by the [[matrix exponential]], given by the usual power series:

:&lt;math&gt;\exp(X) = 1 + X + \frac{X^2}{2!} + \frac{X^3}{3!} + \cdots &lt;/math&gt;

for matrices &lt;math&gt;X&lt;/math&gt;. If &lt;math&gt;G&lt;/math&gt; is a closed subgroup of &lt;math&gt;GL(n;\mathbb C)&lt;/math&gt;, then the exponential map takes the Lie algebra of &lt;math&gt;G&lt;/math&gt; into &lt;math&gt;G&lt;/math&gt;; thus, we have an exponential map for all matrix groups. Every element of &lt;math&gt;G&lt;/math&gt; that is sufficiently close to the identity is the exponential of a matrix in the Lie algebra.&lt;ref&gt;{{harvnb|Hall|2015}} Theorem 3.42&lt;/ref&gt;

The definition above is easy to use, but it is not defined for Lie groups that are not matrix groups, and it is not clear that the exponential map of a Lie group does not depend on its representation as a matrix group. We can solve both problems using a more abstract definition of the exponential map that works for all Lie groups, as follows.

For each vector &lt;math&gt;X&lt;/math&gt; in the Lie algebra &lt;math&gt;\mathfrak{g}&lt;/math&gt; of &lt;math&gt;G&lt;/math&gt; (i.e., the tangent space to &lt;math&gt;G&lt;/math&gt; at the identity), one proves that there is a unique one-parameter subgroup &lt;math&gt;c:\mathbb R\rightarrow G&lt;/math&gt; such that &lt;math&gt;c'(0)=X&lt;/math&gt;. Saying that &lt;math&gt;c&lt;/math&gt; is a one-parameter subgroup means simply that &lt;math&gt;c&lt;/math&gt; is a smooth map into &lt;math&gt;G&lt;/math&gt; and that 

:&lt;math&gt;c(s + t) = c(s) c(t)\ &lt;/math&gt;

for all ''s'' and ''t''. The operation on the right hand side is the group multiplication in ''G''. The formal similarity of this formula with the one valid for the [[exponential function]] justifies the definition

:&lt;math&gt;\exp(X) = c(1).\ &lt;/math&gt;

This is called the '''exponential map''', and it maps the Lie algebra &lt;math&gt;\mathfrak{g}&lt;/math&gt; into the Lie group ''G''. It provides a [[diffeomorphism]] between a [[neighborhood (topology)|neighborhood]] of 0 in &lt;math&gt;\mathfrak{g}&lt;/math&gt; and a neighborhood of ''e'' in ''G''. This exponential map is a generalization of the exponential function for real numbers (because '''R''' is the Lie algebra of the Lie group of [[positive real numbers]] with multiplication), for complex numbers (because '''C''' is the Lie algebra of the Lie group of non-zero complex numbers with multiplication) and for [[matrix (math)|matrices]] (because M(''n'', '''R''') with the regular commutator is the Lie algebra of the Lie group GL(''n'', '''R''') of all invertible matrices).

Because the exponential map is surjective on some neighbourhood ''N'' of ''e'', it is common to call elements of the Lie algebra '''infinitesimal generators''' of the group ''G''. The subgroup of ''G'' generated by ''N'' is  the identity component of ''G''.

The exponential map and the Lie algebra determine the ''local group structure'' of every connected Lie group, because of the [[Baker&amp;ndash;Campbell&amp;ndash;Hausdorff formula]]: there exists a neighborhood ''U'' of the zero element of &lt;math&gt;\mathfrak{g}&lt;/math&gt;, such that for ''X'', ''Y'' in ''U'' we have

:&lt;math&gt; \exp(X)\,\exp(Y) = \exp\left(X + Y + \tfrac{1}{2}[X,Y] + \tfrac{1}{12}[\,[X,Y],Y] - \tfrac{1}{12}[\,[X,Y],X] - \cdots \right),&lt;/math&gt;

where the omitted terms are known and involve Lie brackets of four or more elements. In case ''X'' and ''Y'' commute, this formula reduces to the familiar exponential law {{nowrap|exp(''X'') exp(''Y'') {{=}}}} {{nowrap|exp(''X'' + ''Y'')}}.

The exponential map relates Lie group homomorphisms. That is, if &lt;math&gt;\phi: G \to H&lt;/math&gt; is a Lie group homomorphism and &lt;math&gt;\phi_*: \mathfrak{g} \to \mathfrak{h}&lt;/math&gt; the induced map on the corresponding Lie algebras, then for all &lt;math&gt;x\in\mathfrak g&lt;/math&gt; we have
:&lt;math&gt;\phi(\exp(x)) = \exp(\phi_{*}(x)).\,&lt;/math&gt;
In other words, the following diagram [[commutative diagram|commutes]],&lt;ref group=Note&gt;{{cite web|url=http://www.math.sunysb.edu/~vkiritch/MAT552/ProblemSet1.pdf |title=Archived copy |accessdate=2014-10-11 |deadurl=yes |archiveurl=https://web.archive.org/web/20110928024044/http://www.math.sunysb.edu/~vkiritch/MAT552/ProblemSet1.pdf |archivedate=2011-09-28 |df= }}&lt;/ref&gt;
[[File:ExponentialMap-01.png|center]]

(In short, exp is a [[natural transformation]] from the functor Lie to the identity functor on the category of Lie groups.)

The exponential map from the Lie algebra to the Lie group is not always [[Surjective function|onto]], even if the group is connected (though it does map onto the Lie group for connected groups that are either compact or nilpotent). For example, the exponential map of [[SL2(R)|SL(2, '''R''')]] is not surjective. Also, exponential map is not surjective nor injective for infinite-dimensional (see below) Lie groups modelled on [[Smooth function#Differentiability classes|''C''&lt;sup&gt;∞&lt;/sup&gt;]] [[Fréchet space]], even from arbitrary small neighborhood of 0 to corresponding neighborhood of 1.

=== Lie subgroup ===
A '''Lie subgroup''' ''H'' of a Lie group ''G'' is a Lie group that is a [[subset]] of ''G'' and such that the [[inclusion map]] from ''H'' to ''G'' is an [[injective]] [[Immersion (mathematics)|immersion]] and [[group homomorphism]]. According to [[Closed subgroup theorem|Cartan's theorem]], a closed [[subgroup]] of ''G'' admits a unique smooth structure which makes it an [[embedding|embedded]] Lie subgroup of ''G''—i.e. a Lie subgroup such that the inclusion map is a smooth embedding.

Examples of non-closed subgroups are plentiful; for example take ''G'' to be a torus of dimension ≥ 2, and let ''H'' be a [[one-parameter subgroup]] of ''irrational slope'', i.e. one that winds around in ''G''. Then there is a Lie group [[homomorphism]] φ : '''R''' → ''G'' with ''H'' as its image. The [[closure (topology)|closure]] of ''H'' will be a sub-torus in ''G''.

The [[exponential map (Lie theory)|exponential map]] gives a [[Lie group–Lie algebra correspondence#The correspondence|one-to-one correspondence]] between the connected Lie subgroups of a connected Lie group ''G'' and the subalgebras of the Lie algebra of ''G''.&lt;ref&gt;{{harvnb|Hall|2015}} Theorem 5.20&lt;/ref&gt;  Typically, the subgroup corresponding to a subalgebra is not a  closed subgroup.  There is no criterion solely based on the structure of ''G'' which determines which subalgebras correspond to closed subgroups.

==Representations==
{{main|Representation of a Lie group}}
{{see also|Compact group#Representation theory of a connected compact Lie group|Lie algebra representation}}
One important aspect of the study of Lie groups is their representations, that is, the way they can act (linearly) on vector spaces. In physics, Lie groups often encode the symmetries of a physical system. The way one makes use of this symmetry to help analyze the system is often through representation theory. Consider, for example, the time-independent [[Schrödinger equation]] in quantum mechanics, &lt;math&gt;\hat{H}\psi = E\psi&lt;/math&gt;. Assume the system in question has the [[rotation group SO(3)]] as a symmetry, meaning that the Hamiltonian operator &lt;math&gt;\hat{H}&lt;/math&gt; commutes with the action of SO(3) on the wave function &lt;math&gt;\psi&lt;/math&gt;. (One important example of such a system is the [[Hydrogen atom]].) This assumption does not necessarily mean that the solutions &lt;math&gt;\psi&lt;/math&gt; are rotationally invariant functions. Rather, it means that the ''space'' of solutions to &lt;math&gt;\hat{H}\psi = E\psi&lt;/math&gt; is invariant under rotations (for each fixed value of &lt;math&gt;E&lt;/math&gt;). This space, therefore, constitutes a representation of SO(3). These representations have been [[Representation of a Lie group#An example: The rotation group SO.283.29|classified]] and the classification leads to a substantial [[Hydrogen-like atom|simplification of the problem]], essentially converting a three-dimensional partial differential equation to a one-dimensional ordinary differential equation.

The case of a connected compact Lie group ''K'' (including the just-mentioned case of SO(3)) is particularly tractable.&lt;ref&gt;{{harvnb|Hall|2015}} Part III&lt;/ref&gt; In that case, every finite-dimensional representation of ''K'' decomposes as a direct sum of irreducible representations. The irreducible representations, in turn, were classified by [[Hermann Weyl]]. [[Compact group#Representation theory of a connected compact Lie group|The classification]] is in terms of the "highest weight" of the representation. The classification is closely related to the [[Lie algebra representation#Classifying finite-dimensional representations of Lie algebras|classification of representations of a semisimple Lie algebra]].

One can also study (in general infinite-dimensional) unitary representations of an arbitrary Lie group (not necessarily compact). For example, it is possible to give a relatively simple explicit description of the [[Representation theory of SL2(R)|representations of the group SL(2,R)]] and the [[Wigner%27s classification|representations of the Poincaré group]].

== Early history ==
According to the most authoritative source on the early history of Lie groups (Hawkins, p.&amp;nbsp;1), [[Sophus Lie]] himself considered the winter of 1873–1874 as the birth date of his theory of continuous groups. Hawkins, however, suggests that it was "Lie's prodigious research activity during the four-year period from the fall of 1869 to the fall of 1873" that led to the theory's creation (''ibid''). Some of Lie's early ideas were developed in close collaboration with [[Felix Klein]].  Lie met with Klein every day from October 1869 through 1872: in Berlin from the end of October 1869 to the end of February 1870, and in Paris, Göttingen and Erlangen in the subsequent two years (''ibid'', p.&amp;nbsp;2). Lie stated that all of the principal results were obtained by 1884. But during the 1870s all his papers (except the very first note) were published in Norwegian journals, which impeded recognition of the work throughout the rest of Europe (''ibid'', p.&amp;nbsp;76). In 1884 a young German mathematician, [[Friedrich Engel (mathematician)|Friedrich Engel]], came to work with Lie on a systematic treatise to expose his theory of continuous groups. From this effort resulted the three-volume ''Theorie der Transformationsgruppen'', published in 1888, 1890, and 1893. The term ''groupes de Lie'' first appeared in French in 1893 in the thesis of Lie's student Arthur Tresse.&lt;ref&gt;{{cite journal |title= Sur les invariants différentiels des groupes continus de transformations | author= Arthur Tresse |journal=Acta Mathematica|volume=18|year=1893|pages=1–88 |doi=10.1007/bf02418270}}&lt;/ref&gt;

Lie's ideas did not stand in isolation from the rest of mathematics. In fact, his interest in the geometry of differential equations was first motivated by the work of [[Carl Gustav Jacobi]], on the theory of [[partial differential equation]]s of first order and on the equations of [[classical mechanics]].  Much of Jacobi's work was published posthumously in the 1860s, generating enormous interest in France and Germany (Hawkins, p.&amp;nbsp;43). Lie's ''idée fixe'' was to develop a theory of symmetries of differential equations that would accomplish for them what [[Évariste Galois]] had done for algebraic equations: namely, to classify them in terms of group theory. Lie and other mathematicians showed that the most important equations for [[special functions]] and [[orthogonal polynomials]] tend to arise from group theoretical symmetries. In Lie's early work, the idea was to construct a theory of ''continuous groups'', to complement the theory of [[discrete group]]s that had developed in the theory of [[modular form]]s, in the hands of [[Felix Klein]] and [[Henri Poincaré]]. The initial application that Lie had in mind was to the theory of [[differential equation]]s. On the model of [[Galois theory]] and [[polynomial equation]]s, the driving conception was of a theory capable of unifying, by the study of [[symmetry]], the whole area of [[ordinary differential equation]]s. However, the hope that Lie Theory would unify the entire field of ordinary differential equations was not fulfilled. Symmetry methods for ODEs continue to be studied, but do not dominate the subject. There is a [[differential Galois theory]], but it was developed by others, such as Picard and Vessiot, and it provides a theory of [[quadrature (mathematics)|quadrature]]s, the [[indefinite integral]]s required to express solutions.

Additional impetus to consider continuous groups came from ideas of [[Bernhard Riemann]], on the foundations of geometry, and their further development in the hands of Klein. Thus three major themes in 19th century mathematics were combined by Lie in creating his new theory: the idea of symmetry, as exemplified by Galois through the algebraic notion of a [[group (mathematics)|group]]; geometric theory and the explicit solutions of [[differential equation]]s of mechanics, worked out by [[Siméon Denis Poisson|Poisson]] and Jacobi; and the new understanding of [[geometry]] that emerged in the works of [[Julius Plücker|Plücker]], [[August Ferdinand Möbius|Möbius]], [[Grassmann]] and others, and culminated in Riemann's revolutionary vision of the subject.

Although today Sophus Lie is rightfully recognized as the creator of the theory of continuous groups, a major stride in the development of their structure theory, which was to have a profound influence on subsequent development of mathematics, was made by [[Wilhelm Killing]], who in 1888 published the first paper in a series entitled ''Die Zusammensetzung der stetigen endlichen Transformationsgruppen'' (''The composition of continuous finite transformation groups'') (Hawkins, p.&amp;nbsp;100). The work of Killing, later refined and generalized by [[Élie Cartan]], led to classification of [[semisimple Lie algebra]]s, Cartan's theory of [[Riemannian symmetric space|symmetric spaces]], and [[Hermann Weyl]]'s description of [[group representation|representations]] of compact and semisimple Lie groups using [[highest weight]]s.

In 1900 [[David Hilbert]] challenged Lie theorists with his [[Hilbert's fifth problem|Fifth Problem]] presented at the [[International Congress of Mathematicians]] in Paris.

Weyl brought the early period of the development of the theory of Lie groups to fruition, for not only did he classify irreducible representations of semisimple Lie groups and connect the theory of groups with quantum mechanics, but he also put Lie's theory itself on firmer footing by clearly enunciating the distinction between Lie's ''infinitesimal groups'' (i.e., Lie algebras) and the Lie groups proper, and began investigations of topology of Lie groups.{{sfnp|Borel|2001}} The theory of Lie groups was systematically reworked in modern mathematical language in a monograph by [[Claude Chevalley]].
&lt;!--
Need specific reference from Borel's book to Weyl's work, in particular, distinction mentioned in the text
--&gt;

==The concept of a Lie group, and possibilities of classification==
Lie groups may be thought of as smoothly varying families of symmetries. Examples of symmetries include rotation about an axis.  What must be understood is the nature of 'small' transformations, for example, rotations through tiny angles, that link nearby transformations. The mathematical object capturing this structure is called a Lie algebra ([[Sophus Lie|Lie]] himself called them "infinitesimal groups"). It can be defined because Lie groups are smooth manifolds, so have [[tangent space]]s at each point.

The Lie algebra of any compact Lie group (very roughly: one for which the symmetries form a bounded set) can be decomposed as a [[Direct sum of modules|direct sum]] of an [[abelian Lie algebra]] and some number of [[simple Lie group|simple]] ones. The structure of an abelian Lie algebra is mathematically uninteresting (since the Lie bracket is identically zero); the interest is in the simple summands. Hence the question arises: what are the [[simple Lie group|simple Lie algebras]] of compact groups? It turns out that they mostly fall into four infinite families, the "classical Lie algebras" A&lt;sub&gt;''n''&lt;/sub&gt;, B&lt;sub&gt;''n''&lt;/sub&gt;, C&lt;sub&gt;''n''&lt;/sub&gt; and D&lt;sub&gt;''n''&lt;/sub&gt;, which have simple descriptions in terms of symmetries of Euclidean space. But there are also just five "exceptional Lie algebras" that do not fall into any of these families. E&lt;sub&gt;8&lt;/sub&gt; is the largest of these.

Lie groups are classified according to their algebraic properties ([[simple group|simple]], [[semisimple group|semisimple]], [[solvable group|solvable]], [[nilpotent group|nilpotent]], [[abelian group|abelian]]), their [[connectedness]] ([[connected space|connected]] or [[simply connected space|simply connected]]) and their [[compact space|compactness]].

A first key result is the [[Levi decomposition]], which says that every simply connected Lie group is the semidirect product of a solvable normal subgroup and a semisimple subgroup. 

*Connected [[compact Lie group]]s are all known: they are finite central quotients of a product of copies of the circle group '''S'''&lt;sup&gt;1&lt;/sup&gt; and simple compact Lie groups (which correspond to connected [[Dynkin diagram]]s).
*Any simply connected solvable Lie group is isomorphic to a closed subgroup of the group of invertible upper triangular matrices of some rank, and any finite-dimensional irreducible representation of such a group is 1-dimensional. Solvable groups are too messy to classify except in a few small dimensions.
*Any simply connected nilpotent Lie group is isomorphic to a closed subgroup of the group of invertible upper triangular matrices with 1's on the diagonal of some rank, and any finite-dimensional irreducible representation of such a group is 1-dimensional. Like solvable groups, nilpotent  groups are too messy to classify except in a few small dimensions.
*[[Simple Lie group]]s are sometimes defined to be those that are simple as abstract groups, and sometimes defined to be connected Lie groups with a simple Lie algebra. For example, [[SL2(R)|SL(2, '''R''')]] is simple according to the second definition but not according to the first. They have all been [[list of simple Lie groups|classified]] (for either definition).
*[[Semisimple group|Semisimple]] Lie groups are Lie groups whose Lie algebra is a product of simple Lie algebras.&lt;ref&gt;{{cite book |first=Sigurdur |last=Helgason |title=Differential Geometry, Lie Groups, and Symmetric Spaces |location=New York |publisher=Academic Press |year=1978 |page=131 |isbn=0-12-338460-5 }}&lt;/ref&gt; They are central extensions of products of simple Lie groups.

The [[identity component]] of any Lie group is an open [[normal subgroup]], and the [[quotient group]] is a [[discrete group]]. The universal cover of any connected Lie group is a simply connected Lie group, and conversely any connected Lie group is a quotient of a simply connected Lie group by a discrete normal subgroup of the center. Any Lie group ''G'' can be decomposed into discrete, simple, and abelian groups in a canonical way as follows.  Write 
:''G''&lt;sub&gt;con&lt;/sub&gt; for the connected component of the identity
:''G''&lt;sub&gt;sol&lt;/sub&gt; for the largest connected normal solvable subgroup
:''G''&lt;sub&gt;nil&lt;/sub&gt; for the largest connected normal nilpotent subgroup
so that we have a sequence of normal subgroups
:1 ⊆ ''G''&lt;sub&gt;nil&lt;/sub&gt; ⊆ ''G''&lt;sub&gt;sol&lt;/sub&gt; ⊆ ''G''&lt;sub&gt;con&lt;/sub&gt; ⊆ ''G''.
Then
:''G''/''G''&lt;sub&gt;con&lt;/sub&gt; is discrete
:''G''&lt;sub&gt;con&lt;/sub&gt;/''G''&lt;sub&gt;sol&lt;/sub&gt; is a [[group extension|central extension]] of a product of [[list of simple Lie groups|simple connected Lie groups]].
:''G''&lt;sub&gt;sol&lt;/sub&gt;/''G''&lt;sub&gt;nil&lt;/sub&gt; is abelian. A connected abelian Lie group is isomorphic to a product of copies of  '''R''' and the [[circle group]] ''S''&lt;sup&gt;1&lt;/sup&gt;.
:''G''&lt;sub&gt;nil&lt;/sub&gt;/1 is nilpotent, and therefore its ascending central series has all quotients abelian.

This can be used to reduce some problems about Lie groups (such as finding their unitary representations) to the same problems for connected simple groups and nilpotent and solvable subgroups of smaller dimension.

* The [[diffeomorphism|diffeomorphism group]] of a Lie group acts transitively on the Lie group
* Every Lie group is [[parallelizable]], and hence an [[orientable manifold]] (there is a [[fibre bundle|bundle isomorphism]] between its [[tangent bundle]] and the product of itself with the [[tangent space]] at the identity)

==Infinite-dimensional Lie groups==
Lie groups are often defined to be finite-dimensional, but there are many groups that resemble Lie groups, except for being infinite-dimensional. The simplest way to define infinite-dimensional Lie groups is to  model them locally on [[Banach space]]s (as opposed to [[Euclidean space]] in the finite-dimensional case), and in this case much of the basic theory is similar to that of finite-dimensional Lie groups. However this is inadequate for many applications, because many natural examples of infinite-dimensional Lie groups are not Banach manifolds. Instead one needs to define Lie groups modeled on more general [[Locally convex space|locally convex]] topological vector spaces. In this case the relation between the Lie algebra and the Lie group becomes rather subtle, and several results about finite-dimensional Lie groups no longer hold.

The literature is not entirely uniform in its terminology as to exactly which properties of infinite-dimensional groups qualify the group for the prefix ''Lie'' in ''Lie group''. On the Lie algebra side of affairs, things are simpler since the qualifying criteria for the prefix ''Lie'' in ''Lie algebra'' are purely algebraic. For example, an infinite-dimensional Lie algebra may or may not have a corresponding Lie group. That is, there may be a group corresponding to the Lie algebra, but it might not be nice enough to be called a Lie group, or the connection between the group and the Lie algebra might not be nice enough (for example, failure of the exponential map to be onto a neighborhood of the identity). It is the "nice enough" that is not universally defined.

Some of the examples that have been studied include:
*The group of [[diffeomorphism]]s of a manifold. Quite a lot is known about the group of diffeomorphisms of the circle. Its Lie algebra is (more or less) the [[Witt algebra]], whose [[Lie algebra extension|central extension]] the [[Virasoro algebra]] (see [[Lie algebra extension#Virasoro algebra|Virasoro algebra from Witt algebra]] for a derivation of this fact) is the symmetry algebra of [[two-dimensional conformal field theory]]. Diffeomorphism groups of compact manifolds of larger dimension are [[Convenient vector space#Regular Lie groups|regular Fréchet Lie groups]]; very little about their structure is known.
The diffeomorphism group of spacetime sometimes appears in attempts to [[Quantization (physics)|quantize]] gravity.
*The group of smooth maps from a manifold to a finite-dimensional Lie group is an example of a [[gauge group]] (with operation of [[pointwise multiplication]]), and is used  in [[quantum field theory]] and [[Donaldson theory]]. If the manifold is a circle these are called [[loop group]]s, and have central extensions whose Lie algebras are (more or less) [[Kac&amp;ndash;Moody algebra]]s.
*There are infinite-dimensional analogues of general linear groups, orthogonal groups, and so on.&lt;ref&gt;{{harvnb|Bäruerle|de Kerf|ten Kroode|1997}}&lt;/ref&gt; One important aspect is that these may have ''simpler'' topological properties: see for example [[Kuiper's theorem]]. In [[M-theory]], for example, a 10 dimensional SU(N) gauge theory becomes an 11 dimensional theory when N becomes infinite.

==See also==
{{Div col}}
*[[Adjoint representation of a Lie group]]
*[[Compact group]]
*[[Haar measure]]
*[[Homogeneous space]]
*[[Lie algebra]]
*[[List of Lie group topics]]
*[[Representations of Lie groups]]
*[[Symmetry in quantum mechanics]]
{{Div col end}}

==Notes==

===Explanatory notes===
{{reflist|group=Note}}

===Citations===
{{Reflist}}

==References==
* {{citation|authorlink=John Frank Adams|first=John Frank|last= Adams|title=Lectures on Lie Groups|series=Chicago Lectures in Mathematics|isbn= 0-226-00527-5|year=1969|publisher=Univ. of Chicago Press|location=Chicago | mr=0252560}}.
*{{Citation | last1=Borel | first1=Armand | author1-link=Armand Borel | title=Essays in the history of Lie groups and algebraic groups | url=https://books.google.com/books?isbn=0821802887 | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=History of Mathematics | isbn=978-0-8218-0288-5 | mr=1847105 | year=2001 | volume=21}}
* {{citation|first=Nicolas|last= Bourbaki|authorlink=Nicolas Bourbaki|title=Elements of mathematics: Lie groups and Lie algebras}}. Chapters 1&amp;ndash;3 {{isbn|3-540-64242-0}}, Chapters 4&amp;ndash;6 {{isbn|3-540-42650-7}}, Chapters 7&amp;ndash;9 {{isbn|3-540-43405-4}}
*{{cite book|ref=harv|last1=Bäuerle|first1=G.G.A|last2=de Kerf|first2=E.A.|last3=ten Kroode|first3=A. P. E.|title=Finite and infinite dimensional Lie algebras and their application in physics|year=1997|series=Studies in mathematical physics|volume=7|editor1=A. van Groesen|editor2=E.M. de Jager|publisher=North-Holland|isbn=978-0-444-82836-1|url=http://www.sciencedirect.com/science/bookseries/09258582|via=[[ScienceDirect]]|subscription=yes}}
* {{citation|last=Chevalley|first=Claude|title=Theory of Lie groups|isbn=0-691-04990-4|year=1946|publisher=Princeton University Press|location=Princeton}}.
* [[P. M. Cohn]] (1957) ''Lie Groups'', Cambridge Tracts in Mathematical Physics.
* [[J. L. Coolidge]] (1940) ''A History of Geometrical Methods'', pp 304–17, [[Oxford University Press]] ([[Dover Publications]] 2003).
* {{Fulton-Harris}}
* Robert Gilmore (2008) ''Lie groups, physics, and geometry: an introduction for physicists, engineers and chemists'', [[Cambridge University Press]] {{isbn|9780521884006}} {{doi|10.1017/CBO9780511791390}}.
* {{citation|first=Brian C.|last=Hall|title=Lie Groups, Lie Algebras, and Representations: An Elementary Introduction|edition= 2nd|series=Graduate Texts in Mathematics|volume=222 |publisher=Springer|year=2015|isbn=978-3319134666|doi=10.1007/978-3-319-13467-3}}.
* F. Reese Harvey (1990) ''Spinors and calibrations'', [[Academic Press]], {{isbn|0-12-329650-1}}.
*{{Citation | last1=Hawkins | first1=Thomas | title=Emergence of the theory of Lie groups | url=https://books.google.com/books?isbn=978-0-387-98963-1 | publisher=[[Springer-Verlag]] | location=Berlin, New York | series=Sources and Studies in the History of Mathematics and Physical Sciences | isbn=978-0-387-98963-1 | mr=1771134 | year=2000 | doi=10.1007/978-1-4612-1202-7}} [https://www.jstor.org/stable/2695575 Borel's review]
*{{Citation | last1=Helgason | first1=Sigurdur | authorlink=Sigurður Helgason (mathematician) | title=Differential geometry, Lie groups, and symmetric spaces | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=Graduate Studies in Mathematics | isbn=978-0-8218-2848-9 |mr=1834454 | year=2001 | volume=34 | doi=10.1090/gsm/034}}
* {{citation|last=Knapp|first=Anthony W.|authorlink=Anthony Knapp|title=Lie Groups Beyond an Introduction|edition= 2nd|series=Progress in Mathematics|volume=140|publisher=Birkhäuser|place= Boston|year= 2002|isbn=0-8176-4259-5}}.
* {{cite journal |author=Nijenhuis, Albert |authorlink=Albert Nijenhuis |title=Review: ''Lie groups'', by P. M. Cohn |journal=[[Bulletin of the American Mathematical Society]] |year=1959  |volume=65 |issue=6 |pages=338–341 |url=http://www.ams.org/journals/bull/1959-65-06/S0002-9904-1959-10358-X/ |doi=10.1090/s0002-9904-1959-10358-x}}
* {{citation|last=Rossmann|first= Wulf |title=Lie Groups: An Introduction Through Linear Groups|series= Oxford Graduate Texts in Mathematics|publisher= Oxford University Press|isbn= 978-0-19-859683-7|year=2001}}. The 2003 reprint corrects several typographical mistakes.
*{{cite book |first=David H. |last=Sattinger |first2=O. L. |last2=Weaver |year=1986 |title=Lie groups and algebras with applications to physics, geometry, and mechanics |publisher=Springer-Verlag |isbn=3-540-96240-9 | mr=0835009 |doi=10.1007/978-1-4757-1910-9}}
* {{citation|authorlink=J.-P. Serre|first=Jean-Pierre|last=Serre|title= Lie Algebras and Lie Groups: 1964 Lectures given at Harvard University|series=Lecture notes in mathematics|volume= 1500|publisher=Springer|isbn= 3-540-55008-9|year=1965}}.
*{{cite book |authorlink=John Stillwell |first=John |last=Stillwell |year=2008 |title=Naive Lie Theory |publisher=Springer |isbn=978-0387782140 |doi=10.1007/978-0-387-78214-0}}
* Heldermann Verlag [http://www.heldermann.de/JLT/jltcover.htm Journal of Lie Theory]
*{{Citation | last1=Warner | first1=Frank W. | title=Foundations of differentiable manifolds and Lie groups | publisher=[[Springer-Verlag]] | location=New York Berlin Heidelberg | series=Graduate Texts in Mathematics | isbn=978-0-387-90894-6 |mr=0722297 | year=1983 | volume=94|doi=10.1007/978-1-4757-1799-0}}
* {{citation|first=Willi-Hans|last=Steeb|title=Continuous Symmetries, Lie algebras, Differential Equations and Computer Algebra: second edition | publisher=World Scientific Publishing | year=2007|isbn=981-270-809-X | mr=2382250 | doi=10.1142/6515}}.
*[http://www.math.upenn.edu/~wziller/math650/LieGroupsReps.pdf Lie Groups. Representation Theory and Symmetric Spaces] Wolfgang Ziller, Vorlesung 2010

{{Authority control}}

[[Category:Lie groups]]
[[Category:Manifolds]]
[[Category:Symmetry]]</text>
      <sha1>fh7mat6nnc00j824cz5o4kbdms1h92q</sha1>
    </revision>
  </page>
  <page>
    <title>MIMO</title>
    <ns>0</ns>
    <id>13544419</id>
    <revision>
      <id>869013928</id>
      <parentid>868732283</parentid>
      <timestamp>2018-11-15T21:30:50Z</timestamp>
      <contributor>
        <username>Omnipaedista</username>
        <id>8524693</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="45432">{{about|MIMO in wireless communication}}
[[File:MIMO with building.png|thumb|MIMO exploits multipath propagation to multiply link capacity]]
{{Antennas|techniques}}

In [[radio]], '''multiple-input and multiple-output''', or '''MIMO''' ({{IPAc-en|ˈ|m|aɪ|m|oʊ|,_|ˈ|m|iː|m|oʊ}}), is a method for multiplying the capacity of a radio link using multiple transmit and receive antennas to exploit [[multipath propagation]].&lt;ref&gt;{{cite techreport |first=Hermann |last= Lipfert | title= MIMO OFDM Space Time Coding – Spatial Multiplexing, Increasing Performance and Spectral Efficiency in Wireless Systems, Part I Technical Basis |number= |institution= Institut für Rundfunktechnik | url= http://www.irt.de/webarchiv/showdoc.php?z=OTAyIzEwMDYwMTMxMCNwZGY= |year=August 2007 }}&lt;/ref&gt; MIMO has become an essential element of wireless communication standards including [[IEEE 802.11n]] (Wi-Fi), [[IEEE 802.11ac]] (Wi-Fi), [[Evolved HSPA|HSPA+]] (3G), [[WiMAX]] (4G), and [[LTE (telecommunication)|Long Term Evolution]] (LTE 4G). More recently, MIMO has been applied to [[power-line communication]] for 3-wire installations as part of ITU [[G.hn]] standard and [[HomePlug]] AV2 specification.&lt;ref name="BERGU14"&gt;{{cite book|author1=Berger, Lars T.|author2=Schwager, Andreas |author3=Pagani, Pascal|author4=Schneider, Daniel M.|date=February 2014|title=MIMO Power Line Communications: Narrow and Broadband Standards, EMC, and Advanced Processing|publisher=CRC Press|series=Devices, Circuits, and Systems|isbn=9781466557529|url=http://www.crcnetbase.com/doi/pdfplus/10.1201/b16540-1}}&lt;/ref&gt;&lt;ref&gt;{{cite techreport | title= HomePlug AV2 Technology |number= |institution= HomePlug Powerline Alliance, Inc. | url= http://www.homeplug.org/media/filer_public/2c/32/2c327fc8-25bb-409e-abf7-c398534c24dc/homeplug_av2_whitepaper_130909.pdf  |year=2013 }}&lt;/ref&gt;

At one time, in wireless the term "MIMO" referred to the use of multiple antennas at the transmitter and the receiver. In modern usage, "MIMO" specifically refers to a practical technique for sending and receiving more than one data signal simultaneously over the same radio channel by exploiting multipath propagation. MIMO is fundamentally different from smart antenna techniques developed to enhance the performance of a single data signal, such as [[beamforming]] and [[antenna diversity|diversity]].

==History==

===Early research===
MIMO is often traced back to 1970s research papers concerning multi-channel digital transmission systems and interference (crosstalk) between wire pairs in a cable bundle: AR Kaye and DA George (1970),&lt;ref&gt;{{Cite journal| first1 = AR | last1 = Kaye | first2 = DA | last2 = George | title = Transmission of multiplexed PAM signals over multiple channel and diversity systems | publisher = IEEE | journal = IEEE Transactions on Communication Technology | volume = 18 | number = 5 | pages = 520–526 |date=October 1970| doi=10.1109/TCOM.1970.1090417}}&lt;/ref&gt; Branderburg and Wyner (1974),&lt;ref&gt;{{Cite journal| first1 = LH | last1 = Brandenburg | first2 = AD | last2 = Wyner | title = Capacity of the Gaussian Channel with Memory: The Multivariate Case | publisher = Bell | journal = Syst. Tech. J. | volume = 53 | number = 5 | pages = 745–78 |date=May–June 1974| doi=10.1002/j.1538-7305.1974.tb02768.x}}&lt;/ref&gt; and W. van Etten (1975, 1976).&lt;ref&gt;{{Cite journal| first = W | last1 = Van Etten | title = Maximum likelihood receiver for multiple channel transmission systems | publisher = IEEE | journal = Transactions on Communications| volume = 24 | number = 2 | pages = 276–283 |date=February 1976| doi=10.1109/TCOM.1976.1093265}}&lt;/ref&gt; Although these are not examples of exploiting multipath propagation to send multiple information streams, some of the mathematical techniques for dealing with mutual interference proved useful to MIMO development. In the mid-1980s Jack Salz at [[Bell Labs|Bell Laboratories]]  took this research a step further, investigating multi-user systems operating over "mutually cross-coupled linear networks with additive noise sources" such as time-division multiplexing and dually-polarized radio systems.&lt;ref&gt;{{Cite journal| first = J | last = Salz | title = Digital transmission over cross-coupled linear channels |doi=10.1002/j.1538-7305.1985.tb00269.x| publisher = AT&amp;T | journal = Technical Journal | volume = 64 | issue = 6 | pages = 1147–59 |date=July–August 1985}}&lt;/ref&gt;

Methods were developed to improve the performance of cellular radio networks and enable more aggressive frequency reuse in the early 1990s. [[Space-division multiple access]] (SDMA) uses directional or smart antennas to communicate on the same frequency with users in different locations within range of the same base station. An SDMA system was proposed by Richard Roy and [[Björn Ottersten]], researchers at [[ArrayComm]], in 1991. Their US patent (No. 5515378 issued in 1996&lt;ref&gt;{{cite patent | country=US |number=5515378 |title=Spatial division multiple access wireless communication systems}}&lt;/ref&gt;) describes a method for increasing capacity using "an array of receiving antennas at the base station" with a "plurality of remote users."

===Invention===
[[Arogyaswami Paulraj]] and [[Thomas Kailath]] proposed an SDMA-based inverse multiplexing technique in 1993. Their US patent (No. 5,345,599 issued in 1994&lt;ref&gt;{{cite patent|country=US|number=5345599|title=Increasing capacity in wireless broadcast systems using distributed transmission/directional reception (DTDR)}}&lt;/ref&gt;) described a method of broadcasting at high data rates by splitting a high-rate signal "into several low-rate signals" to be transmitted from "spatially separated transmitters" and recovered by the receive antenna array based on differences in "directions-of-arrival." Paulraj was awarded the prestigious Marconi Prize in 2014 for "his pioneering contributions to developing the theory and applications of MIMO antennas. ...  His idea for using multiple antennas at both the transmitting and receiving stations – which is at the heart of the current high speed WiFi and 4G mobile systems – has revolutionized high speed wireless."&lt;ref&gt;{{Cite web|url=http://marconisociety.org/fellows/arogyaswami-paulraj/|title=Arogyaswami Paulraj – Marconi Society|website=marconisociety.org|language=en-US|access-date=2017-01-21}}&lt;/ref&gt;

In an April 1996 paper and subsequent patent, [[Gregory Raleigh|Greg Raleigh]] proposed that natural multipath propagation can be exploited to transmit multiple, independent information streams using co-located antennas and multi-dimensional signal processing.&lt;ref&gt;{{cite conference |first= Gregory |last= Raleigh |first2= John M. |last2= Cioffi | authorlink= | title= Spatio-temporal coding for wireless communications | conference= Global Telecommunications Conference, 1996. London, UK November 18–22, 1996 |year=1996 |url= http://kemere.org/web/oldWWW/work/ee359/stcoding.pdf  |format=PDF }}&lt;/ref&gt; The paper also identified practical solutions for modulation ([[MIMO-OFDM]]), coding, synchronization, and channel estimation. Later that year (September 1996) [[Gerard J. Foschini]] submitted a paper that also suggested it is possible to multiply the capacity of a wireless link using what the author described as "layered space-time architecture."&lt;ref&gt;{{Cite journal| first = GJ | last = Foschini | title = Layered space–time architecture for wireless communication in a fading environment when using multiple antennas | publisher = Bell | journal = Labs Syst. Tech. J. | volume = 1 | pages = 41–59 |date=Autumn 1996}}&lt;/ref&gt;

Greg Raleigh, V. K. Jones, and Michael Pollack founded Clarity Wireless in 1996, and built and field-tested a prototype MIMO system.&lt;ref&gt;{{cite conference |first= V.K. |last= Jones |first2= G.G. |last2= Raleigh | authorlink= | title= Channel estimation for wireless OFDM systems | conference= IEEE GLOBECOM 1998 Conference. Sydney, Australia 08 Nov 1998-12 Nov 1998 | volume=2 |pages=980–985 |DOI=10.1109/GLOCOM.1998.776875}}&lt;/ref&gt; Cisco Systems acquired Clarity Wireless in 1998.&lt;ref&gt;{{cite news |last= Junnarkar |first= Sandeep |date=15 September 1998 |title= Cisco to buy Clarity Wireless |url= http://news.cnet.com/Cisco-to-buy-Clarity-Wireless/2100-1001_3-215538.html |location= |publisher= CBS Interactive Inc. |accessdate=28 October 2013 }}&lt;/ref&gt; Bell Labs built a laboratory prototype demonstrating its V-BLAST (Vertical-Bell Laboratories Layered Space-Time) technology in 1998.&lt;ref&gt;{{Cite journal| first1 = GD | last1 = Golden | first2 = GJ | last2 = Foschini | first3 = RA | last3 = Valenzuela | first4 = PW | last4 = Wolniansky|doi=10.1049/el:19990058 | title = Detection algorithm and initial laboratory results using V-BLAST space–time communication architecture | journal = Electron. Lett. | volume = 35 | pages = 14–16 |date=Jan 1999}}&lt;/ref&gt; Arogyaswami Paulraj founded Iospan Wireless in late 1998 to develop MIMO-OFDM products. Iospan was acquired by Intel in 2003.&lt;ref&gt;{{cite news |last= Gregson |first= Reily |date=27 February 2003 |title= Iospan ceases operations |url= http://www.rcrwireless.com/20030227/archived-articles/iospan-ceases-operations |location= |publisher= RCR Wireless |accessdate=22 January 2015 }}&lt;/ref&gt; V-BLAST was never commercialized, and neither Clarity Wireless nor Iospan Wireless shipped MIMO-OFDM products before being acquired.&lt;ref&gt;{{cite journal |last= Sampath |first= Hemanth|year= 2002 |title= A fourth-generation MIMO-OFDM broadband wireless system: design, performance, and field trial results |citeseerx = 10.1.1.4.7852| journal= IEEE Communications Magazine |volume=40 |issue=9 |pages=143–149 |doi=10.1109/MCOM.2002.1031841 |accessdate=&lt;!-- 28 October 2013 --&gt;|display-authors=etal}}&lt;/ref&gt;

===Standards and commercialization===
{{see also|MIMO technology in WiMAX|MIMO technology in 3G mobile standards}}
MIMO technology has been standardized for [[wireless LAN]]s, [[3G]] mobile phone networks, and [[4G]] mobile phone networks and is now in widespread commercial use. Greg Raleigh and V. K. Jones founded [[Airgo Networks]] in 2001 to develop [[MIMO-OFDM]] chipsets for wireless LANs. The [[Institute of Electrical and Electronics Engineers]] (IEEE) created a task group in late 2003 to develop a wireless LAN standard delivering at least 100 Mbit/s of user data throughput. There were two major competing proposals: TGn Sync was backed by companies including Intel and [[Philips]], and WWiSE was supported by companies including Airgo Networks, [[Broadcom]], and [[Texas Instruments]]. Both groups agreed that the 802.11n standard would be based on MIMO-OFDM with 20&amp;nbsp;MHz and 40&amp;nbsp;MHz channel options.&lt;ref&gt;{{cite news |last= Cox |first= John |date=8 February 2005 |title= 802.11n update: TGn Sync vs WWiSE |url= http://www.networkworld.com/net.worker/news/2005/020705netleadside.html |newspaper=Network World |publisher= IDG |accessdate=28 October 2013 }}&lt;/ref&gt;  TGn Sync, WWiSE, and a third proposal (MITMOT, backed by [[Motorola]] and [[Mitsubishi]]) were merged to create what was called the Joint Proposal.&lt;ref&gt;{{cite news |last= Smith |first= Tony |date=1 August 2005 |title= 802.11n rivals agree to merge |url= https://www.theregister.co.uk/2005/08/01/tgn_sync_wwise_join_forces |newspaper=UK Register |publisher=  |accessdate=28 October 2013 }}&lt;/ref&gt; In 2004, Airgo became the first company to ship MIMO-OFDM products.&lt;ref&gt;{{cite book |editor-first= Ramjee |editor-last= Prasad|title= Globalization of Mobile and Wireless Communications: Today and in 2020 |publisher=Springer |year= 2011 |pages=115 |chapter= |isbn=978-9-400-70106-9|display-editors=etal}}&lt;/ref&gt; Qualcomm acquired Airgo Networks in late 2006.&lt;ref&gt;{{cite news |date=4 December 2006 |title= Qualcomm buys Airgo, RFMD's Bluetooth business |url= http://www.eetimes.com/document.asp?doc_id=1245469  |newspaper=EE Times |publisher= UBM Tech |accessdate=28 October 2013 }}&lt;/ref&gt; The final 802.11n standard supported speeds up to 600 Mbit/s (using four simultaneous data streams) and was published in late 2009.&lt;ref&gt;{{cite news |last= Ngo |first= Dong |date=11 September 2009 |title= 802.11n Wi-Fi standard finally approved |url= http://news.cnet.com/8301-1035_3-10351215-94.html |newspaper=CNET |publisher= CBS Interactive Inc. |accessdate=28 October 2013 }}&lt;/ref&gt;

Surendra Babu Mandava and Arogyaswami Paulraj founded Beceem Communications in 2004 to produce MIMO-OFDM chipsets for [[WiMAX]]. The company was acquired by Broadcom in 2010.&lt;ref&gt;{{cite news |last= Gardner |first= W. David |date=13 October 2010 |title= Broadcom to Acquire Beceem for $316 Million |url= http://www.informationweek.com/infrastructure/management/broadcom-to-acquire-beceem-for-316-milli/227701262 |newspaper=InformationWeek |publisher= UBM Tech |accessdate=28 October 2013 }}&lt;/ref&gt; WiMAX was developed as an alternative to cellular standards, is based on the [[802.16e]] standard, and uses MIMO-OFDM to deliver speeds up to 138 Mbit/s. The more advanced 802.16m standard enables download speeds up to 1 Gbit/s.&lt;ref&gt;{{cite web |url=http://www.wimaxforum.org/sites/wimaxforum.org/files/document_library/wimax_802.16m.pdf |title=WiMAX and the IEEE 802.16m Air Interface Standard |date=April 2010 |website=WiMAXforum.org |publisher=WiMAX Forum |accessdate=28 October 2013 |deadurl=yes |archiveurl=https://web.archive.org/web/20131207054411/http://www.wimaxforum.org/sites/wimaxforum.org/files/document_library/wimax_802.16m.pdf |archivedate=7 December 2013 |df= }}&lt;/ref&gt; A nationwide WiMAX network was built in the United States by [[Clearwire]], a subsidiary of [[Sprint-Nextel]], covering 130 million [[point of presence|points of presence]] (PoP) by mid-2012.&lt;ref&gt;{{cite web |url= http://www.fcc.gov/document/16th-mobile-competition-report |title= Annual Report and Analysis of Competitive Market Conditions With Respect to Mobile Wireless, Including Commercial Mobile Services |date= 21 March 2013 |pages=8 |website=FCC.gov |publisher= Federal Communications Commission |accessdate=28 October 2013}}&lt;/ref&gt; Sprint subsequently announced plans to deploy LTE (the cellular 4G standard) covering 31 cities by mid-2013&lt;ref&gt;{{cite web |url= http://gigaom.com/2011/12/13/clearwire-green-lights-lte-build-by-raising-734-million/ |title= Clearwire green-lights LTE build by raising $734 million |author= Kevin Fitchard |date= 13 December 2011 |pages= |website=GIGAOM.com |publisher= GIGAOM |accessdate=28 October 2013}}&lt;/ref&gt; and to shut down its WiMAX network by the end of 2015.&lt;ref&gt;{{cite news |last= Goldstein |first= Phil |date=7 October 2014 |title= Sprint to shutter WiMAX network around Nov. 6, 2015 |url= http://www.fiercewireless.com/story/sprint-shutter-wimax-network-around-nov-6-2015/2014-10-07|newspaper=FierceWireless |publisher= FierceMarkets |accessdate=22 January 2015 }}&lt;/ref&gt;
 
The first 4G cellular standard was proposed by [[NTT DoCoMo]] in 2004.&lt;ref&gt;{{cite news|last=Alabaster |first=Jay |date=20 August 2012 |title=Japan's NTT DoCoMo signs up 1 million LTE users in a month, hits 5 million total |url=http://www.networkworld.com/news/2012/082012-japan39s-ntt-docomo-signs-up-261759.html |newspaper=Network World |publisher=IDG |accessdate=29 October 2013 |archiveurl=https://web.archive.org/web/20131203010826/http://www.networkworld.com/news/2012/082012-japan39s-ntt-docomo-signs-up-261759.html |archivedate=3 December 2013 |deadurl=yes |df= }}&lt;/ref&gt; Long term evolution (LTE) is based on MIMO-OFDM and continues to be developed by the [[3rd Generation Partnership Project]] (3GPP). LTE specifies downlink rates up to 300 Mbit/s, uplink rates up to 75 Mbit/s, and quality of service parameters such as low latency.&lt;ref&gt;{{cite web |url= http://www.3gpp.org/LTE
 |title= LTE |author= Magdalena Nohrborg |date=  |pages= |website=3GPP.org |publisher= 3rd Generation Partnership Project |accessdate=29 October 2013}}&lt;/ref&gt; [[LTE Advanced]] adds support for picocells, femtocells, and multi-carrier channels up to 100&amp;nbsp;MHz wide. LTE has been embraced by both GSM/UMTS and CDMA operators.&lt;ref&gt;{{cite web |url= http://www.3gpp.org/lte-advanced |title= LTE Advanced |author= Jeanette Wannstrom
 |date=May 2012  |pages= |website=3GPP.org |publisher= 3rd Generation Partnership Project |accessdate=29 October 2013}}&lt;/ref&gt;

The first LTE services were launched in Oslo and Stockholm by [[TeliaSonera]] in 2009.&lt;ref&gt;{{cite web |url= http://gigaom.com/2009/12/14/teliasonera-lte-4g |title= Stockholm, Oslo First to Get Commercial LTE |author= Om Malik |date= 14 December 2009 |pages= |website=GIGAOM.com |publisher= GIGAOM |accessdate=29 October 2013}}&lt;/ref&gt; There are currently more than 360 LTE networks in 123 countries operational with approximately 373 million connections (devices).&lt;ref&gt;{{cite web |url= http://www.gsacom.com/news/statistics |title= 4G/LTE is mainstream |date= 7 January 2015 |pages= |website=Gsacom.com |publisher= Global mobile Suppliers Association |accessdate=22 January 2015}}&lt;/ref&gt;

==Functions==
MIMO can be sub-divided into three main categories: [[precoding]], [[spatial multiplexing]] (SM), and [[Diversity Coding|diversity coding]].

'''[[Precoding]]''' is multi-stream [[beamforming]], in the narrowest definition. In more general terms, it is considered to be all spatial processing that occurs at the transmitter. In (single-stream) beamforming, the same signal is emitted from each of the transmit antennas with appropriate phase and gain weighting such that the signal power is maximized at the receiver input. The benefits of beamforming are to increase the received signal gain – by making signals emitted from different antennas add up constructively – and to reduce the multipath fading effect. In [[line-of-sight propagation]], beamforming results in a well-defined directional pattern. However, conventional beams are not a good analogy in cellular networks, which are mainly characterized by [[multipath propagation]]. When the receiver has multiple antennas, the transmit beamforming cannot simultaneously maximize the signal level at all of the receive antennas, and precoding with multiple streams is often beneficial. Note that precoding requires knowledge of [[channel state information]] (CSI) at the transmitter and the receiver.

'''[[Spatial multiplexing]]''' requires MIMO antenna configuration. In spatial multiplexing,&lt;ref name="GOLBO16"&gt;{{Cite book |url=http://cdn.intechopen.com/pdfs-wm/53332.pdf |doi=10.5772/66.399| isbn=9781466557529|title=Beamforming in Wireless Networks|year=2016|last1=Golbon-Haghighi|first1=M.H. |journal=InTech Open| pages=163–199}}&lt;/ref&gt;&lt;ref&gt;{{cite book|author1=[[Rakhesh Singh Kshetrimayum]] |title=Fundamentals of MIMO Wireless Communications|work=Cambridge University Press |year=2017}}&lt;/ref&gt; a high-rate signal is split into multiple lower-rate streams and each stream is transmitted from a different transmit antenna in the same frequency channel. If these signals arrive at the receiver antenna array with sufficiently different spatial signatures and the receiver has accurate CSI, it can separate these streams into (almost) parallel channels. Spatial multiplexing is a very powerful technique for increasing channel capacity at higher signal-to-noise ratios (SNR). The maximum number of spatial streams is limited by the lesser of the number of antennas at the transmitter or receiver. Spatial multiplexing can be used without CSI at the transmitter, but can be combined with [[precoding]] if CSI is available. Spatial multiplexing can also be used for simultaneous transmission to multiple receivers, known as [[space-division multiple access]] or [[multi-user MIMO]], in which case CSI is required at the transmitter.&lt;ref&gt;{{Cite journal|author1=D. Gesbert |author2=M. Kountouris |author3=R. W. Heath, Jr. |author4=C.-B. Chae |author5=T. Sälzer  |last-author-amp=yes |title=Shifting the MIMO Paradigm: From Single User to Multiuser Communications|journal= IEEE Signal Processing Magazine|volume= 24|issue= 5|pages= 36–46|date=Oct 2007|doi=10.1109/msp.2007.904815|bibcode=2007ISPM...24...36G}}&lt;/ref&gt; The scheduling of receivers with different spatial signatures allows good separability.

'''[[Diversity coding]]''' techniques are used when there is no [[channel state information|channel knowledge]] at the transmitter. In diversity methods, a single stream (unlike multiple streams in spatial multiplexing) is transmitted, but the signal is coded using techniques called [[space-time coding]]. The signal is emitted from each of the transmit antennas with full or near orthogonal coding. Diversity coding exploits the independent fading in the multiple antenna links to enhance signal diversity. Because there is no channel knowledge, there is no beamforming or [[array gain]] from diversity coding.
Diversity coding can be combined with spatial multiplexing when some channel knowledge is available at the transmitter.

==Forms==
[[File:LteMimoAntennen.jpg|thumb| Example of an antenna for [[3GPP Long Term Evolution|LTE]] with 2 port [[antenna diversity]]]]

===Multi-antenna types===
Multi-antenna MIMO (or Single user MIMO) technology has been developed and implemented in some standards, e.g., 802.11n products.
* [[Single-Input and Single-Output|SISO]]/SIMO/MISO are special cases of MIMO
** Multiple-input and single-output (MISO) is a special case when the receiver has a single antenna.
** Single-input and multiple-output (SIMO) is a special case when the transmitter has a single antenna.
** [[Single-input single-output]] (SISO) is a conventional radio system where neither transmitter nor receiver has multiple antenna.
* Principal single-user MIMO techniques
** [[Bell Laboratories Layered Space-Time|Bell Laboratories Layered Space-Time (BLAST)]], Gerard. J. Foschini (1996)
** Per Antenna Rate Control (PARC), Varanasi, Guess (1998), Chung, Huang, Lozano (2001)
** Selective Per Antenna Rate Control (SPARC), Ericsson (2004)
* Some limitations
** The physical antenna spacing is selected to be large; multiple [[wavelengths]] at the base station. The antenna separation at the receiver is heavily space-constrained in handsets, though advanced antenna design and algorithm techniques are under discussion. ''Refer to: [[multi-user MIMO]]''

===Multi-user types===
{{main|Multi-user MIMO}}
Recently, results of research on multi-user MIMO technology have been emerging. While full multi-user MIMO (or network MIMO) can have a higher potential, practically, the research on (partial) multi-user MIMO (or multi-user and multi-antenna MIMO) technology is more active.&lt;ref&gt;{{cite book|author1=B. Kumbhani, R S Kshetrimayum |title=MIMO Wireless Communications over Generalized Fading Channels |work=CRC Press |year=2017}}&lt;/ref&gt;&lt;ref name="GOLBO16"/&gt;
* [[Multi-user MIMO|Multi-user MIMO (MU-MIMO)]]
** In recent [[3GPP]] and [[WiMAX]] standards, MU-MIMO is being treated as one of the candidate technologies adoptable in the specification by a number of companies, including Samsung, Intel, Qualcomm, Ericsson, TI, Huawei, Philips, Nokia, and Freescale. For these and other firms active in the mobile hardware market, MU-MIMO is more feasible for low-complexity cell phones with a small number of reception antennas, whereas single-user SU-MIMO's higher per-user throughput is better suited to more complex user devices with more antennas.
** Enhanced multiuser MIMO: 1) Employs advanced decoding techniques, 2) Employs advanced precoding techniques
** SDMA represents either [[space-division multiple access]] or super-division multiple access where ''super'' emphasises that orthogonal division such as frequency and time division is not used but non-orthogonal approaches such as superposition coding are used.
* [[Cooperative MIMO|Cooperative MIMO (CO-MIMO)]]
** Uses multiple neighboring base stations to jointly transmit/receive data to/from users. As a result, neighboring base stations don't cause intercell interference as in the conventional MIMO systems.&lt;ref name="GOLBO16"/&gt;
* [[Macrodiversity]] MIMO
** A form of space diversity scheme which uses multiple transmit or receive base stations for communicating coherently with single or multiple users which are possibly distributed in the coverage area, in the same time and frequency resource.&lt;ref name=Karakayali&gt;{{Cite journal|doi=10.1109/MWC.2006.1678166 |pages= 56–61|title= Advances in smart antennas - Network coordination for spectrally efficient communications in cellular systems|journal= IEEE Wireless Communications|volume= 13|issue= 4|year= 2006|last1= Karakayali|first1= M.K.|last2= Foschini|first2= G.J.|last3= Valenzuela|first3= R.A.}}&lt;/ref&gt;&lt;ref name=Gesbert2010&gt;{{Cite journal|doi=10.1109/JSAC.2010.101202 |pages= 1380–1408|title= Multi-Cell MIMO Cooperative Networks: A New Look at Interference|journal= IEEE Journal on Selected Areas in Communications|volume= 28|issue= 9|year= 2010|last1= Gesbert|first1= David|last2= Hanly|first2= Stephen|last3= Huang|first3= Howard|last4= Shamai Shitz|first4= Shlomo|last5= Simeone|first5= Osvaldo|last6= Yu|first6= Wei}}&lt;/ref&gt;&lt;ref name=fnt2013&gt;{{cite journal|url=http://kth.diva-portal.org/smash/get/diva2:608533/FULLTEXT01 |doi=10.1561/0100000069|volume= 9|issue= 2–3|pages= 113–381|year=2013|title=Optimal Resource Allocation in Coordinated Multi-Cell Systems|journal=Foundations and Trends in Communications and Information Theory|last1=Björnson|first1=Emil|last2=Jorswieck|first2=Eduard}}&lt;/ref&gt;
** The transmitters are far apart in contrast to traditional microdiversity MIMO schemes such as single-user MIMO. In a multi-user macrodiversity MIMO scenario, users may also be far apart. Therefore, every constituent link in the virtual MIMO link has distinct average link [[Signal-to-noise ratio|SNR]]. This difference is mainly due to the different long-term channel impairments such as path loss and shadow fading which are experienced by different links.
** Macrodiversity MIMO schemes pose unprecedented theoretical and practical challenges. Among many theoretical challenges, perhaps the most fundamental challenge is to understand how the different average link SNRs affect the overall system capacity and individual user performance in fading environments.&lt;ref name=Basnayaka&gt;{{Cite journal|doi=10.1109/TWC.2013.032113.120798|pages= 2240–2251|title= Performance Analysis of Macrodiversity MIMO Systems with MMSE and ZF Receivers in Flat Rayleigh Fading|journal= IEEE Transactions on Wireless Communications|volume= 12|issue= 5|year= 2013|last1= Basnayaka|first1= Dushyantha A.|last2= Smith|first2= Peter J.|last3= Martin|first3= Phillipa A.|arxiv= 1207.6678}}&lt;/ref&gt;
* MIMO [[Routing]]
** Routing a cluster by a cluster in each hop, where the number of nodes in each cluster is larger or equal to one. MIMO routing is different from conventional (SISO) routing since conventional routing protocols route node-by-node in each hop.&lt;ref&gt;{{cite journal|author1=S. Cui |author2=A. J. Goldsmith |author3=A. Bahai  |last-author-amp=yes |title=Energy-efficiency of MIMO and Cooperative MIMO in Sensor Networks|journal=IEEE J. Select. Areas of Commun.|pages=1089–1098|volume=22|issue=6|date=August 2004|doi=10.1109/JSAC.2004.830916}}&lt;/ref&gt;
* Massive MIMO is a technology where the number of terminals is much less than the number of base station (mobile station) antennas.&lt;ref&gt;T. L. Marzetta, [https://dx.doi.org/10.1109/TWC.2010.092810.091092 Noncooperative cellular wireless with unlimited numbers of base station antennas], IEEE Trans. Wireless Commun., vol. 9, no. 11, pp. 3590 – 3600, Nov. 2010.&lt;/ref&gt; In a rich scattering environment, the full advantages of the massive MIMO system can be exploited using simple beamforming strategies such as maximum ratio transmission (MRT),&lt;ref&gt;T. K. Y. Lo, "Maximum ratio transmission," IEEE. Trans. Commun., vol. 47, no. 10, pp. 1458-1461, Oct. 1999.&lt;/ref&gt; maximum ratio-combining (MRC)&lt;ref&gt;W. C. Jakes, Jr., Mobile Microwave Communication. New York: Wiley, 1974.&lt;/ref&gt; or zero forcing (ZF). To achieve these benefits of massive MIMO, accurate CSI must be available perfectly. However, in practice, the channel between the transmitter and receiver is estimated from orthogonal pilot sequences which are limited by the coherence time of the channel. Most importantly, in a multicell setup, the reuse of pilot sequences of several co-channel cells will create pilot contamination. When there is pilot contamination, the performance of massive MIMO degrades quite drastically. To alleviate the effect of pilot contamination, the work of&lt;ref&gt;T. E. Bogale and L. B. Le, [https://arxiv.org/abs/1402.0045 Pilot optimization and channel estimation for multiuser massive MIMO systems] in Proc. IEEE Conference on Information Sciences and Systems (CISS), Princeton, USA, Mar. 2014.&lt;/ref&gt; proposes a simple pilot assignment and channel estimation method from limited training sequences.

==Applications==
Spatial multiplexing techniques make the receivers very complex, and therefore they are typically combined with [[Orthogonal frequency-division multiplexing]] (OFDM) or with [[Orthogonal Frequency Division Multiple Access]] (OFDMA) modulation, where the problems created by a multi-path channel are handled efficiently. The IEEE [[802.16e]] standard incorporates MIMO-OFDMA. The IEEE 802.11n standard, released in October 2009, recommends MIMO-OFDM.

MIMO is also planned to be used in [[Mobile radio telephone]] standards such as recent [[3GPP]] and [[3GPP2]]. In 3GPP, [[HSPA+|High-Speed Packet Access plus (HSPA+)]] and [[3GPP Long Term Evolution|Long Term Evolution (LTE)]] standards take MIMO into account. Moreover, to fully support cellular environments, MIMO research consortia including IST-MASCOT propose to develop advanced MIMO techniques, e.g., [[Multi-user MIMO|multi-user MIMO (MU-MIMO)]].

MIMO technology can be used in non-wireless communications systems. One example is the home networking standard [[ITU-T]] [[G.9963]], which defines a powerline communications system that uses MIMO techniques to transmit multiple signals over multiple AC wires (phase, neutral and ground).&lt;ref name="BERGU14"/&gt;

==Mathematical description==
[[File:Kanalmatrix MIMO.png|thumb|280px|MIMO channel model]]
In MIMO systems, a transmitter sends multiple streams by multiple transmit antennas.&lt;ref name="GOLBO16"/&gt; The transmit streams go through a [[matrix (mathematics)|matrix]] channel which consists of all &lt;math&gt;\scriptstyle N_t N_r&lt;/math&gt; paths between the &lt;math&gt;\scriptstyle N_t&lt;/math&gt; transmit antennas at the transmitter and &lt;math&gt;\scriptstyle N_r&lt;/math&gt; receive antennas at the receiver. Then, the receiver gets the received signal [[Vector space|vectors]] by the multiple receive antennas and decodes the received signal vectors into the original information. A [[narrowband]] [[flat fading]] MIMO system is modelled as:&lt;ref&gt;{{cite journal|last1=Nasseri|first1=Mona|title=Iterative Channel Estimation Algorithm in Multiple Input Multiple Output Orthogonal Frequency Division Multiplexing Systems|journal=Journal of Computer Science|date=1 February 2010|volume=6|issue=2|pages=224–228|doi=10.3844/jcssp.2010.224.228}}&lt;/ref&gt;
:&lt;math&gt;\mathbf{y} = \mathbf{H}\mathbf{x} + \mathbf{n}&lt;/math&gt;
where &lt;math&gt;\scriptstyle\mathbf{y}&lt;/math&gt; and &lt;math&gt;\scriptstyle\mathbf{x}&lt;/math&gt; are the receive and transmit vectors, respectively, and &lt;math&gt;\scriptstyle\mathbf{H}&lt;/math&gt; and &lt;math&gt;\scriptstyle\mathbf{n}&lt;/math&gt; are the channel matrix and the noise vector, respectively.

Referring to [[information theory]], the ergodic [[channel capacity]] of MIMO systems where both the transmitter and the receiver have perfect instantaneous [[channel state information]] is&lt;ref name=dlove&gt;{{Cite journal|url=http://www.eurecom.fr/~gesbert/papers/JSAC_limitedfeedback_tutorial.pdf |pages=1341–1365|doi=10.1109/JSAC.2008.081002|title=An overview of limited feedback in wireless communication systems|journal=IEEE Journal on Selected Areas in Communications|volume=26|issue=8|year=2008|last1=Love|first1=David|last2=Heath|first2=Robert|last3=n. Lau|first3=Vincent|last4=Gesbert|first4=David|last5=Rao|first5=Bhaskar|last6=Andrews|first6=Matthew}}&lt;/ref&gt;
:&lt;math&gt;C_\mathrm{perfect-CSI} = E\left[\max_{\mathbf{Q}; \, \mbox{tr}(\mathbf{Q}) \leq 1} \log_2 \det\left(\mathbf{I} + \rho \mathbf{H}\mathbf{Q}\mathbf{H}^{H}\right)\right] = E\left[\log_2 \det\left(\mathbf{I} + \rho \mathbf{D}\mathbf{S} \mathbf{D} \right)\right]&lt;/math&gt;
where &lt;math&gt;\scriptstyle ()^H&lt;/math&gt; denotes [[Conjugate transpose|Hermitian transpose]] and &lt;math&gt;\scriptstyle\rho&lt;/math&gt; is the ratio between transmit power and noise power (i.e., transmit [[Signal-to-noise ratio|SNR]]). The optimal signal covariance &lt;math&gt;\scriptstyle \mathbf{Q}=\mathbf{VSV}^H&lt;/math&gt; is achieved through [[singular value decomposition]] of the channel matrix &lt;math&gt;\scriptstyle\mathbf{UDV}^H \,=\, \mathbf{H}&lt;/math&gt; and an optimal diagonal power allocation matrix &lt;math&gt;\scriptstyle \mathbf{S}=\textrm{diag}(s_1,\ldots,s_{\min(N_t, N_r)},0,\ldots,0)&lt;/math&gt;. The optimal power allocation is achieved through [[Water filling algorithm|waterfilling]],&lt;ref&gt;D. Tse and P. Viswanath, [http://www.eecs.berkeley.edu/~dtse/book.html Fundamentals of Wireless Communication] {{webarchive|url=https://web.archive.org/web/20070810052329/http://www.eecs.berkeley.edu/~dtse/book.html |date=2007-08-10 }}, Cambridge University Press, 2005.&lt;/ref&gt; that is
:&lt;math&gt;s_i = \left(\mu - \frac{1}{\rho d_i^2} \right)^+, \quad \textrm{for} \,\, i=1,\ldots,\min(N_t, N_r),&lt;/math&gt;
where &lt;math&gt;\scriptstyle d_1,\ldots,d_{\min(N_t, N_r)}&lt;/math&gt; are the diagonal elements of &lt;math&gt;\scriptstyle \mathbf{D}&lt;/math&gt;, &lt;math&gt;\scriptstyle (\cdot)^+&lt;/math&gt; is zero if its argument is negative, and &lt;math&gt;\mu&lt;/math&gt; is selected such that &lt;math&gt;\scriptstyle s_1+\ldots+s_{\min(N_t, N_r)}=N_t&lt;/math&gt;.

If the transmitter has only statistical [[channel state information]], then the ergodic [[channel capacity]] will decrease as the signal covariance &lt;math&gt;\scriptstyle \mathbf{Q}&lt;/math&gt; can only be optimized in terms of the average [[mutual information]] as&lt;ref name=dlove /&gt;
:&lt;math&gt;C_\mathrm{statistical-CSI} = \max_{\mathbf{Q}} E\left[\log_2 \det\left(\mathbf{I} + \rho \mathbf{H}\mathbf{Q}\mathbf{H}^{H}\right)\right].&lt;/math&gt;
The [[Spatial Correlation|spatial correlation]] of the channel has a strong impact on the ergodic [[channel capacity]] with statistical information.

If the transmitter has no [[channel state information]] it can select the signal covariance &lt;math&gt;\scriptstyle \mathbf{Q}&lt;/math&gt; to maximize channel capacity under worst-case statistics, which means &lt;math&gt;\scriptstyle \mathbf{Q}=1/N_t \mathbf{I}&lt;/math&gt; and accordingly
:&lt;math&gt;C_\mathrm{no-CSI} = E\left[\log_2 \det\left(\mathbf{I} + \frac{\rho}{N_t}\mathbf{H}\mathbf{H}^{H}\right)\right].&lt;/math&gt;

Depending on the statistical properties of the channel, the ergodic capacity is no greater than &lt;math&gt;\scriptstyle\min(N_t, N_r)&lt;/math&gt; times larger than that of a SISO system.

==Testing==
MIMO signal testing focuses first on the transmitter/receiver system. The random phases of the sub-carrier signals can produce instantaneous power levels that cause the amplifier to compress, momentarily causing distortion and ultimately symbol errors. Signals with a high '''PAR''' ([[peak-to-average ratio]]) can cause amplifiers to compress unpredictably during transmission. OFDM signals are very dynamic and compression problems can be hard to detect because of their noise-like nature.&lt;ref&gt;Stefan Schindler, Heinz Mellein, [http://www.rohde-schwarz.com/appnote/1SP18.pdf, "Assessing a MIMO Channel"]{{dead link|date=May 2017 |bot=InternetArchiveBot |fix-attempted=yes }}, Rohde &amp; Schwarz, pg. 11.&lt;/ref&gt;

Knowing the quality of the signal channel is also critical. A [[radio channel emulator|channel emulator]] can simulate how a device performs at the cell edge, can add noise or can simulate what the channel looks like at speed. To fully qualify the performance of a receiver, a calibrated transmitter, such as a [[Signal generator#Vector signal generators|vector signal generator]] (VSG), and channel emulator can be used to test the receiver under a variety of different conditions. Conversely, the transmitter's performance under a number of different conditions can be verified using a channel emulator and a calibrated receiver, such as a [[vector signal analyzer]] (VSA).

Understanding the channel allows for manipulation of the phase and amplitude of each transmitter in order to form a beam. To correctly form a beam, the transmitter needs to understand the characteristics of the channel. This process is called ''channel sounding'' or [[channel estimation]]. A known signal is sent to the mobile device that enables it to build a picture of the channel environment. The mobile device sends back the channel characteristics to the transmitter. The transmitter can then apply the correct phase and amplitude adjustments to form a beam directed at the mobile device. This is called a closed-loop MIMO system. For [[beamforming]], it is required to adjust the phases and amplitude of each transmitter. In a beamformer optimized for spatial diversity or spatial multiplexing, each antenna element simultaneously transmits a weighted combination of two data symbols.&lt;ref&gt;{{Cite web|url=http://rfmw.em.keysight.com/wireless/helpfiles/n5106a/5989-8973en.pdf|title=MIMO Channel Modeling and Emulation Test Challenges|last=|first=|date=|website=Keysight|archive-url=|archive-date=|dead-url=|access-date=}}&lt;/ref&gt;

==Literature==

===Principal researchers===
Papers by Gerard J. Foschini and Michael J. Gans,&lt;ref&gt;{{cite journal|author1=Gerard J. Foschini  |author2=Michael. J. Gans |lastauthoramp=yes |title=On limits of wireless communications in a fading environment when using multiple antennas|journal=Wireless Personal Communications|pages=311–335|volume=6|issue=3|date=January 1998|doi=10.1023/A:1008889222784}}&lt;/ref&gt; Foschini&lt;ref&gt;{{cite journal|author=Gerard J. Foschini|title=Layered space-time architecture for wireless communications in a fading environment when using multi-element antennas|journal=Bell Labs Technical Journal |pages=41–59|volume=1|date=Autumn 1996|doi=10.1002/bltj.2015|issue=2}}&lt;/ref&gt; and Emre Telatar&lt;ref name=telatar&gt;{{cite journal|first=Emre|last=Telatar|title=Capacity of Multi-antenna Gaussian Channels|journal=European Transactions on Telecommunications|pages=585–95|volume=10|year=1999|url=http://mars.bell-labs.com/papers/proof/|issue=6|doi=10.1002/ett.4460100604|deadurl=yes|archiveurl=https://web.archive.org/web/20120208121926/http://mars.bell-labs.com/papers/proof/|archivedate=2012-02-08|df=}}&lt;/ref&gt; have shown that the [[channel capacity]] (a theoretical upper bound on system throughput) for a MIMO system is increased as the number of antennas is increased, proportional to the smaller of the number of transmit antennas and the number of receive antennas. This is known as the multiplexing gain and this basic finding in [[information theory]] is what led to a spurt of research in this area. Despite the simple propagation models used in the aforementioned seminal works, the multiplexing gain is a fundamental property that can be proved under almost any physical channel propagation model and with practical hardware that is prone to transceiver impairments.&lt;ref&gt;{{cite journal|author=Emil Björnson, Per Zetterberg, Mats Bengtsson, Björn Ottersten|title=Capacity Limits and Multiplexing Gains of MIMO Channels with Transceiver Impairments|journal=IEEE Communications Letters |pages=91–94|volume=17|date=January 2013|issue=1|bibcode=2012arXiv1209.4093B|last2=Zetterberg|last3=Bengtsson|last4=Ottersten|arxiv=1209.4093|doi=10.1109/LCOMM.2012.112012.122003}}&lt;/ref&gt;

Papers by Dr. Fernando Rosas and Dr. Christian Oberli  have shown that the entire MIMO SVD link can be approximated by the average of the SER of Nakagami-m channels.&lt;ref&gt;{{cite journal|author1=Rosas, F.  |author2=Oberli, C. |lastauthoramp=yes |title=Nakagami-m approximations for multiple-input multiple-output singular value decomposition transmissions|journal=Communications, IET|pages=554–561|volume=7|issue=6|date=April 16, 2013|doi=10.1049/iet-com.2012.0400}}&lt;/ref&gt; This leads to characterise the eigenchannels of N × N MIMO channels with N larger than 14, showing that the smallest eigenchannel distributes as a Rayleigh channel, the next four eigenchannels closely distributes as Nakagami-m channels with m = 4, 9, 25 and 36, and the N – 5 remaining eigenchannels have statistics similar to an additive white Gaussian noise (AWGN) channel within 1&amp;nbsp;dB signal-to-noise ratio. It is also shown that 75% of the total mean power gain of the MIMO SVD channel goes to the top third of all the eigenchannels.

A textbook by A. Paulraj, R. Nabar and D. Gore has published an introduction to this area.&lt;ref&gt;{{cite book|author1=A. Paulraj, R. Nabar  |author2=D. Gore |lastauthoramp=yes |title=Introduction to Space-time Communications|work= Cambridge University Press|year=2003}}&lt;/ref&gt; There are many other principal textbooks available as well.&lt;ref&gt;{{cite book|author1=David Tse |author2=Pramod Viswanath |title=Fundamentals of Wireless Communication|work=Cambridge |year=2005}}&lt;/ref&gt;&lt;ref&gt;{{cite book|author1=Claude Oestges |author2=Bruno Clerckx |title=MIMO Wireless Communications: From Real-world Propagation to Space-time Code Design|work=Academic Press |year=2007}}&lt;/ref&gt;&lt;ref&gt;{{cite book|author1=Ezio Biglieri |author2=Robert Calderbank |author3=Anthony Constantinides |author4=Andrea Goldsmith |author5=Arogyaswami Paulraj |author6=H. Vincent Poor |title=MIMO Wireless Communications|work=Cambridge University Press |year=2010}}&lt;/ref&gt;

=== Diversity-multiplexing tradeoff ===
There exists a fundamental tradeoff between transmit diversity and spatial multiplexing gains in a MIMO system (Zheng and Tse, 2003).&lt;ref&gt;{{cite journal|author1=L. Zheng  |author2=D. N. C. Tse |lastauthoramp=yes |title=Diversity and multiplexing: A fundamental tradeoff in multiple-antenna channels|journal=IEEE Trans. Inf. Theory|pages=1073–1096|volume=49|issue=5|date=May 2003|doi=10.1109/TIT.2003.810646}}&lt;/ref&gt; In particular, achieving high spatial multiplexing gains is of profound importance in modern wireless systems.&lt;ref&gt;{{cite journal|author1=A. Lozano  |author2=N. Jindal |lastauthoramp=yes |title=Transmit diversity vs. spatial multiplexing in modern MIMO systems|journal=IEEE Trans. Wireless Commun.|pages=186–197|volume=9|issue=1|year=2010|url=http://www.dtic.upf.edu/~alozano/papers/Diversity.pdf|doi=10.1109/TWC.2010.01.081381}}&lt;/ref&gt;

===Other applications===
Given the nature of MIMO, it is not limited to wireless communication. It can be used for [[wire line]] communication as well. For example, a new type of [[DSL]] technology (gigabit DSL) has been proposed based on binder MIMO channels.

===Sampling theory in MIMO systems===
An important question which attracts the attention of engineers and mathematicians is how to use the multi-output signals at the receiver to recover the multi-input signals at the transmitter. In Shang, Sun and Zhou (2007), sufficient and necessary conditions are established to guarantee the complete recovery of the multi-input signals.&lt;ref&gt;{{cite journal|author1=Z. Shang, W. Sun  |author2=X. Zhou |lastauthoramp=yes |title= Vector sampling expansions in shift invariant subspaces |journal= Journal of Mathematical Analysis and Applications|pages=898–919|volume=325|issue=2|date=January 2007|doi=10.1016/j.jmaa.2006.02.033}}&lt;/ref&gt;

==See also==
{{Portal|Telecommunication|Radio}}
{{colbegin}}
* [[Antenna diversity]]
* [[Beamforming]]
* [[Channel bonding]]
* [[Channel state information]]
* [[Dirty paper coding]]
* [[Duplex (telecommunications)]]
* [[History of smart antennas]]
* [[IEEE 802.11]]
* [[IEEE 802.16]]
* [[Macrodiversity]]
* [[MIMO-OFDM]]
* [[Multi-user MIMO]]
* [[Per-User Unitary Rate Control]]
* [[Phased array]]
* [[Precoding]]
* [[Single-frequency network]] (SFN)
* [[Smart antenna]]
* [[Space–time block code]]
* [[Space–time code]]
* [[Spatial multiplexing]]
* [[Wi-Fi]]
* [[WiMAX MIMO]]
{{colend}}

==References==
{{reflist|3}}

==External links==
* [http://www-x.antd.nist.gov/uwb/main.html NIST UWB-MIMO Channel Propagation Measurements in the 2–8 GHz Spectrum]
* [http://www.rfglobalnet.com/article.mvc/Meeting-The-Test-Challenges-Of-4G-LTE-0001 Meeting The Test Challenges Of 4G LTE]
* [http://www.rfglobalnet.com/article.mvc/The-Basics-Of-OFDM-0001 The Basics Of OFDM]
* [http://www.rfglobalnet.com/article.mvc/MIMO-The-Future-Of-Wireless-0001 MIMO: The Future Of Wireless: Test Challenges For WiMAX, HSPA+, And LTE]
* [http://mobiledevdesign.com/hardware_news/radio_challenges_moving_mimo/ The challenges of moving to MIMO systems]
* [https://web.archive.org/web/20110715181143/http://rfdesign.com/microwave_millimeter_tech/test_and_measurement/711RFD30.pdf RF test system tackles 4 × 4 MIMO signals]
* [http://www.rfglobalnet.com/article.mvc/EVM-Measurements-In-Amplifier-Modulation-0001 The Role Of EVM Measurements In Characterizing Amplifier Modulation Performance]
* [http://www.rfglobalnet.com/article.mvc/Industry-Views-4G-Systems-Bring-New-Design-An-0002 Industry Views: 4G Systems Bring New Design Testing Challenges]
* [http://www.ofcom.org.uk/static/archive/ra/topics/research/topics/propagation/mimo.pdf Literature review of MIMO]
* [https://ieeexplore.ieee.org/document/4554910/ Antenna and Wireless Multipath Virtual Channel Interaction]

{{DEFAULTSORT:Mimo}}
[[Category:IEEE 802]]
[[Category:Information theory]]
[[Category:Radio resource management]]
[[Category:Control engineering]]</text>
      <sha1>2n66ir2oela3mpdfgvpfnf1cq0frdzt</sha1>
    </revision>
  </page>
  <page>
    <title>Macdonald polynomials</title>
    <ns>0</ns>
    <id>10354285</id>
    <revision>
      <id>848894634</id>
      <parentid>846671584</parentid>
      <timestamp>2018-07-05T02:43:34Z</timestamp>
      <contributor>
        <username>Latex-yow</username>
        <id>27692366</id>
      </contributor>
      <minor/>
      <comment>/* Combinatorial formula for the Macdonald polynomials */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19205">{{multiple issues|
{{primary sources|date=April 2014}}
{{more footnotes|date=April 2014}}
}}

In mathematics, '''Macdonald polynomials''' ''P''&lt;sub&gt;λ&lt;/sub&gt;(''x''; ''t'',''q'') are a  family of [[orthogonal polynomials|orthogonal]] [[Symmetric polynomial|symmetric]] polynomials in several variables,   introduced by {{harvs|txt|authorlink=Ian G. Macdonald|last=Macdonald|year=1987}}. He later introduced a non-symmetric generalization in 1995. Macdonald originally associated his polynomials with weights λ of finite root systems and used just one variable ''t'', but later realized that it is more natural to associate them with  [[affine root system]]s rather than finite root systems, in which case the variable ''t'' can be replaced by several different variables ''t''=(''t''&lt;sub&gt;1&lt;/sub&gt;,...,''t''&lt;sub&gt;''k''&lt;/sub&gt;), one for each of the ''k'' orbits of roots in the affine root system. The Macdonald polynomials are polynomials in ''n'' variables ''x''=(''x''&lt;sub&gt;1&lt;/sub&gt;,...,''x''&lt;sub&gt;''n''&lt;/sub&gt;), where ''n'' is the rank of the affine root system.   They generalize many other families of orthogonal polynomials, such as [[Jack polynomial]]s and [[Hall–Littlewood polynomial]]s and [[Askey–Wilson polynomials]], which in turn include most of the named 1-variable orthogonal polynomials as special cases. [[Koornwinder polynomials]] are  Macdonald polynomials of certain non-reduced root systems. They have deep relationships with [[affine Hecke algebra]]s and [[Hilbert scheme]]s, which were used to prove several conjectures made by Macdonald about them.

==Definition==
First fix some notation:
*''R'' is a finite [[root system]] in a real vector space ''V''.
*''R&lt;sup&gt;+&lt;/sup&gt;'' is a choice of [[root system#Positive roots and simple roots|positive roots]], to which corresponds a positive [[Weyl chamber]].
*''W'' is the [[Weyl group]] of ''R''.
*''Q'' is the root lattice of ''R'' (the lattice spanned by the roots).
*''P'' is the [[weight lattice]] of ''R'' (containing ''Q'').
* An [[weight lattice#Ordering on the space of weights|ordering on the weights]]: &lt;math&gt;\mu \le \lambda&lt;/math&gt; if and only if &lt;math&gt;\lambda-\mu&lt;/math&gt; is a nonnegative linear combination of [[Root system#Positive roots and simple roots|simple roots]].
*''P&lt;sup&gt;+&lt;/sup&gt;'' is the set of dominant weights: the elements of ''P'' in the positive Weyl chamber.
*ρ is the [[Weyl vector]]: half the sum of the positive roots; this is a special element of ''P''&lt;sup&gt;+&lt;/sup&gt; in the interior of the positive Weyl chamber.
*''F'' is a field of characteristic 0, usually the rational numbers.
*''A'' = ''F''(''P'') is the [[group algebra]] of ''P'', with a basis of elements written ''e''&lt;sup&gt;λ&lt;/sup&gt; for λ ∈ ''P''.
* If ''f'' = ''e''&lt;sup&gt;λ&lt;/sup&gt;, then &lt;span style="text-decoration: overline"&gt;''f''&lt;/span&gt;  means ''e''&lt;sup&gt;&amp;minus;λ&lt;/sup&gt;, and this is extended by linearity to the whole group algebra.
*''m''&lt;sub&gt;μ&lt;/sub&gt; = Σ&lt;sub&gt;λ ∈ ''W''μ&lt;/sub&gt;''e''&lt;sup&gt;λ&lt;/sup&gt; is an orbit sum; these elements form a basis for the subalgebra ''A''&lt;sup&gt;''W''&lt;/sup&gt; of elements fixed by ''W''.
*&lt;math&gt;(a;q)_\infty = \prod_{r\ge0}(1-aq^r)&lt;/math&gt;, the [[Q-Pochhammer symbol|infinite q-Pochhammer symbol]].
*&lt;math&gt;\Delta= \prod_{\alpha\in R} {(e^\alpha; q)_\infty \over (te^\alpha; q)_\infty}. &lt;/math&gt;
*&lt;math&gt;\langle f,g\rangle=(\text{constant term of }f \overline g \Delta)/|W|&lt;/math&gt; is the inner product of two elements of ''A'', at least when ''t'' is a positive integer power of ''q''.

The '''Macdonald polynomials''' ''P''&lt;sub&gt;λ&lt;/sub&gt; for λ ∈ ''P''&lt;sup&gt;+&lt;/sup&gt; are uniquely defined by the following two conditions:
:&lt;math&gt;P_\lambda=\sum_{\mu\le \lambda}u_{\lambda\mu}m_\mu&lt;/math&gt; where ''u''&lt;sub&gt;&amp;lambda;&amp;mu;&lt;/sub&gt; is a rational function of ''q'' and ''t'' with ''u''&lt;sub&gt;&amp;lambda;&amp;lambda;&lt;/sub&gt; = 1;
: ''P''&lt;sub&gt;&amp;lambda;&lt;/sub&gt; and ''P''&lt;sub&gt;&amp;mu;&lt;/sub&gt; are orthogonal if &amp;lambda;&amp;nbsp;&lt;&amp;nbsp;&amp;mu;.

In other words, the Macdonald polynomials are obtained by orthogonalizing the obvious basis for ''A''&lt;sup&gt;''W''&lt;/sup&gt;. The existence of polynomials with these properties is easy to show (for any inner product). A key property of the Macdonald polynomials is that they are '''orthogonal''': 〈''P''&lt;sub&gt;λ&lt;/sub&gt;, ''P''&lt;sub&gt;μ&lt;/sub&gt;〉 = 0 whenever λ&amp;nbsp;≠&amp;nbsp;μ. This is not a trivial consequence of the definition because ''P''&lt;sup&gt;+&lt;/sup&gt; is not totally ordered, and so has plenty of elements that are incomparable.  Thus one must check that the corresponding polynomials are still orthogonal. The orthogonality can be proved by showing that the Macdonald polynomials are eigenvectors 
for an algebra of commuting self adjoint operators with 1-dimensional eigenspaces, and using the fact that eigenspaces for different eigenvalues must be orthogonal.

In the case of non-simply-laced root systems (B, C, F, G), the parameter ''t'' can be chosen to vary with the length of the root, giving a three-parameter family of Macdonald polynomials.  One can also extend the definition to the nonreduced root system BC, in which case one obtains a six-parameter family (one ''t'' for each orbit of roots, plus ''q'') known as [[Koornwinder polynomials]]. It is sometimes better to regard Macdonald polynomials as depending on a possibly non-reduced affine root system. In this case there is one parameter ''t'' associated to each orbit of roots in the affine root system, plus one parameter ''q''. The number of orbits of roots can vary from 1 to 5.

==Examples==

*If ''q'' = ''t'' the Macdonald polynomials become the [[Weyl character]]s of the representations of the compact group of the root system, or the Schur functions in the case of root systems of type ''A''.
*If ''q'' = 0 the Macdonald polynomials become the (rescaled) [[zonal spherical function]]s for a semisimple ''p''-adic group, or the [[Hall–Littlewood polynomials]] when the root system has type ''A''.
*If ''t''=1 the Macdonald polynomials become the sums over ''W'' orbits, which are the monomial symmetric functions when the root system has type ''A''.
*If we put ''t'' = ''q''&lt;sup&gt;α&lt;/sup&gt; and let ''q'' tend to 1 the Macdonald polynomials become [[Jack polynomial]]s when the root system is of type ''A'', and [[Heckman–Opdam polynomials]] for more general root systems.
*For the affine root system ''A''&lt;sub&gt;1&lt;/sub&gt;, the Macdonald polynomials are the [[Rogers polynomials]].
*For the non-reduced rank 1 affine root system of type (''C''{{su|b=1|p=&amp;or;}}, ''C''&lt;sub&gt;1&lt;/sub&gt;), the Macdonald polynomials are the [[Askey–Wilson polynomials]], which in turn include as special cases most of the named families of orthogonal polynomials in 1 variable.
*For the non-reduced  affine root system of type (''C''{{su|b=''n''|p=&amp;or;}}, ''C''&lt;sub&gt;''n''&lt;/sub&gt;), the Macdonald polynomials are the [[Koornwinder polynomials]].

==The Macdonald constant term conjecture==

If ''t'' = ''q''&lt;sup&gt;''k''&lt;/sup&gt; for some positive integer ''k'', then the norm of the Macdonald polynomials is given by

:&lt;math&gt;\langle P_\lambda, P_\lambda\rangle = \prod_{\alpha\in R, \alpha&gt;0} \prod_{0&lt;i&lt;k} {1-q^{(\lambda+k\rho,2\alpha/(\alpha,\alpha))+i} \over 1-q^{(\lambda+k\rho,2\alpha/(\alpha,\alpha))-i}}.&lt;/math&gt;

This was conjectured by Macdonald (1982) as a generalization of the [[Dyson conjecture]], and proved for all (reduced) root systems by Cherednik (1995) using properties of [[double affine Hecke algebra]]s. The conjecture had previously been proved case-by-case for all roots systems except those of type ''E''&lt;sub&gt;''n''&lt;/sub&gt; by several authors.

There are two other conjectures which together with the norm conjecture are collectively referred to as the Macdonald conjectures in this context: in addition to the formula for the norm, Macdonald conjectured a formula for the value of ''P''&lt;sub&gt;λ&lt;/sub&gt; at the point ''t''&lt;sup&gt;ρ&lt;/sup&gt;, and a symmetry

:&lt;math&gt;\frac{P_\lambda(\dots,q^{\mu_i}t^{\rho_i},\dots)}{P_\lambda(t^\rho)}
=
\frac{P_\mu(\dots,q^{\lambda_i}t^{\rho_i},\dots)}{P_\mu(t^\rho)}.&lt;/math&gt;

Again, these were proved for general reduced root systems by {{harvs|txt|last=Cherednik|year=1995|authorlink=Ivan Cherednik}}, using [[double affine Hecke algebra]]s, with the extension to the BC case following shortly thereafter via work of van Diejen, Noumi, and Sahi.

==The Macdonald positivity conjecture==

In the case of roots systems of type ''A''&lt;sub&gt;''n''&amp;minus;1&lt;/sub&gt; the Macdonald polynomials
are simply symmetric polynomials in ''n'' variables with coefficients that are rational functions of ''q'' and ''t''. A certain transformed version &lt;math&gt;\widetilde{H}_\mu&lt;/math&gt; of the Macdonald polynomials (see [[#Combinatorial formula for the Macdonald polynomials|Combinatorial formula]] below) form an orthogonal basis of the space of symmetric functions over &lt;math&gt;\mathbb{Q}(q,t)&lt;/math&gt;, and therefore can be expressed in terms of [[Schur polynomial|Schur functions]] &lt;math&gt;s_\lambda&lt;/math&gt;.  The coefficients ''K''&lt;sub&gt;λμ&lt;/sub&gt;(''q'',''t'') of these relations are called '''Kostka&amp;ndash;Macdonald coefficients''' or ''qt''-Kostka coefficients.
Macdonald conjectured that the Kostka&amp;ndash;Macdonald coefficients were polynomials in ''q'' and ''t'' with non-negative integer coefficients. These conjectures are now proved; the hardest and final step was proving the positivity, which was  done by [[Mark Haiman]] (2001), by proving the [[n! conjecture|''n''!&amp;nbsp;conjecture]].

It is still a central open problem in algebraic combinatorics to find a combinatorial formula for the ''qt''-Kostka coefficients.

==n! conjecture==
The [[n! conjecture|''n''!&amp;nbsp;conjecture]] of [[Adriano Garsia]] and [[Mark Haiman]] states that for each partition μ of ''n'' the space

:&lt;math&gt;D_\mu =C[\partial x,\partial y]\,\Delta_\mu&lt;/math&gt;

spanned by all higher partial derivatives  of

:&lt;math&gt;\Delta_\mu = \det (x_i^{p_j}y_i^{q_j})_{1\le i,j,\le n}&lt;/math&gt;

has dimension ''n''!, where (''p''&lt;sub&gt;''j''&lt;/sub&gt;, ''q''&lt;sub&gt;''j''&lt;/sub&gt;) run through the ''n'' elements of the diagram of the partition μ, regarded as a subset of the pairs of non-negative integers. 
For example, if μ is the partition 3&amp;nbsp;=&amp;nbsp;2&amp;nbsp;+&amp;nbsp;1 of ''n''&amp;nbsp;=&amp;nbsp;3 then the pairs (''p''&lt;sub&gt;''j''&lt;/sub&gt;, ''q''&lt;sub&gt;''j''&lt;/sub&gt;) are
(0,&amp;nbsp;0), (0,&amp;nbsp;1), (1,&amp;nbsp;0), and the space ''D''&lt;sub&gt;μ&lt;/sub&gt; is spanned by
:&lt;math&gt;\Delta_\mu=x_1y_2+x_2y_3+x_3y_1-x_2y_1-x_3y_2-x_1y_3&lt;/math&gt;
:&lt;math&gt;y_2-y_3&lt;/math&gt;
:&lt;math&gt;y_3-y_1&lt;/math&gt;
:&lt;math&gt;x_3-x_2&lt;/math&gt;
:&lt;math&gt;x_1-x_3&lt;/math&gt;
:&lt;math&gt;1&lt;/math&gt;
which has dimension 6&amp;nbsp;=&amp;nbsp;3&lt;nowiki&gt;!&lt;/nowiki&gt;.

Haiman's proof of the Macdonald positivity conjecture and the ''n''! conjecture involved showing that the [[isospectral Hilbert scheme]] of ''n'' points in a plane was [[Cohen&amp;ndash;Macaulay]] (and even [[Gorenstein ring|Gorenstein]]). Earlier results of Haiman and Garsia had already shown that this implied the ''n''! conjecture, and that the ''n''! conjecture implied that the Kostka&amp;ndash;Macdonald coefficients were graded character multiplicities for the modules ''D''&lt;sub&gt;μ&lt;/sub&gt;. This immediately implies the Macdonald positivity conjecture because character multiplicities have to be non-negative integers.

Ian Grojnowski and Mark Haiman found another proof of the Macdonald positivity conjecture by proving a positivity conjecture for [[LLT polynomial]]s.

==Combinatorial formula for the Macdonald polynomials==

In 2005, J. Haglund, M. Haiman and N. Loehr{{sfn|Haglund|2005}} gave the first proof of a combinatorial interpretation of the 
Macdonald polynomials.  While very useful for computation and interesting in its own right, this combinatorial formula does not immediately imply positivity of the Kostka-Macdonald coefficients &lt;math&gt;K_{\lambda \mu}(q,t),&lt;/math&gt; as it gives the decomposition of the Macdonald polynomials into monomial symmetric functions rather than into Schur functions.

The formula, which involves the ''transformed Macdonald polynomials'' &lt;math&gt;\widetilde{H}_\mu&lt;/math&gt; rather than the usual &lt;math&gt;P_\lambda&lt;/math&gt;, is given as

:&lt;math&gt;\widetilde{H}_\mu(x;q,t) = \sum_{\sigma:\mu \to \Z_+} q^{inv(\sigma)}t^{maj(\sigma)} x^{\sigma}&lt;/math&gt;

where σ is a filling of the [[Young tableau|Young diagram]] of shape μ, ''inv'' and ''maj'' are certain combinatorial statistics (functions) defined on the filling σ. This formula expresses the Macdonald polynomials in infinitely many variables.  To obtain the polynomials in ''n'' variables, simply restrict the formula to fillings that only use the integers 1, 2, ..., ''n''. The term ''x''&lt;sup&gt;σ&lt;/sup&gt; should be interpreted as &lt;math&gt;x_1^{\sigma_1} x_2^{\sigma_2} \cdots &lt;/math&gt; where ''σ&lt;sub&gt;i&lt;/sub&gt;'' is the number of boxes in the filling of μ with content ''i''.

[[File:ArmLeg.jpg|thumb|This depicts the arm and the leg of a square of a Young diagram. The arm is the number of squares to its right, and the leg is the number of squares above it.]]

The transformed Macdonald polynomials &lt;math&gt;\widetilde{H}_\mu(x;q,t)&lt;/math&gt; in the formula above are related to the classical Macdonald polynomials &lt;math&gt;P_{\lambda}&lt;/math&gt; via a sequence of transformations.  First, the ''integral form'' of the Macdonald polynomials, denoted &lt;math&gt;J_\lambda(x;q,t)&lt;/math&gt;, is a re-scaling of &lt;math&gt;P_\lambda(x;q,t)&lt;/math&gt; that clears the denominators of the coefficients:

:&lt;math&gt;J_\lambda(x;q,t)=\prod_{s\in D(\lambda)}(1-q^{a(s)}t^{1+l(s)})\cdot P_\lambda(x;q,t)&lt;/math&gt;

where &lt;math&gt;D(\lambda)&lt;/math&gt; is the collection of squares in the Young diagram of &lt;math&gt;\lambda&lt;/math&gt;, and &lt;math&gt;a(s)&lt;/math&gt; and &lt;math&gt;l(s)&lt;/math&gt; denote the ''arm'' and ''leg'' of the square &lt;math&gt;s&lt;/math&gt;, as shown in the figure.  ''Note: The figure at right uses French notation for tableau, which is flipped vertically from the English notation used on the Wikipedia page for Young diagrams.  French notation is more commonly used in the study of Macdonald polynomials.''

The transformed Macdonald polynomials &lt;math&gt;\widetilde{H}_\mu(x;q,t)&lt;/math&gt; can then be defined in terms of the &lt;math&gt;J_\mu&lt;/math&gt;'s.  We have

:&lt;math&gt;\widetilde{H}_\mu(x;q,t)=t^{-n(\mu)}J_\mu\left[\frac{X}{1-t^{-1}};q,t^{-1}\right]&lt;/math&gt;

where 

:&lt;math&gt;n(\mu)=\sum_{i}\mu_i\cdot (i-1).&lt;/math&gt;

The bracket notation above denotes [[plethystic substitution]].

This formula can be used to prove Knop and Sahi's formula for the [[Jack function|Jack polynomials]]. T

==Non-symmetric Macdonald polynomials==

In 1995, Macdonald introduced a non-symmetric analogue of the symmetric Macdonald polynomials,
and the symmetric Macdonald polynomials can easily be recovered from the non-symmetric counterpart.
In his original definition, he shows that the non-symmetric Macdonald polynomials are a unique family of 
polynomials orthogonal to a certain inner product, as well as satisfying a 
triangularity property when expanded in the monomial basis.

In 2007, Haglund, Haiman and Loehr gave a combinatorial formula for the non-symmetric Macdonald polynomials.

The non-symmetric Macdonald polynomials specialize to Demazure characters by taking q=t=0,
and to key polynomials when q=t=∞.

==References==

*{{Citation | last1=Cherednik | first1=Ivan | title=Double Affine Hecke Algebras and Macdonald's Conjectures | publisher=Annals of Mathematics | series=Second Series | year=1995 | journal=[[Annals of Mathematics]] | issn=0003-486X | volume=141 | issue=1 | pages= 191–216 | jstor=2118632 | doi=10.2307/2118632}}
*{{citation|first1=Adriano |last1=Garsia |first2= Jeffrey B. |last2=Remmel |url=http://www.pnas.org/cgi/content/full/102/11/3891 |title=''Breakthroughs in the theory of Macdonald polynomials''|journal=PNAS  |date=March 15, 2005|volume= 102|pages=3891&amp;ndash;3894|issue= 11|doi=10.1073/pnas.0409705102 |pmid=15753285 |pmc=554818|bibcode=2005PNAS..102.3891G}}
*Mark Haiman [http://projecteuclid.org/Dienst/UI/1.0/Summarize/euclid.cdm/1088530398 ''Combinatorics, symmetric functions, and Hilbert schemes''] Current Developments in Mathematics  2002, no. 1 (2002), 39&amp;ndash;111.
*{{Citation | last1=Haglund | first1=J. |first2= M. |last2=Haiman | first3=N. | last3=Loehr | url=http://www.ams.org/journals/jams/2005-18-03/S0894-0347-05-00485-6/ |title=''A combinatorial formula for Macdonald polynomials'' | publisher=American Mathematical Society | year=1995 | journal=[[J. Amer. Math. Soc.]] | issn=1088-6834 | volume=18 | pages= 735–761}}
* Haiman, Mark [http://math.berkeley.edu/~mhaiman/ftp/newt-sf-2001/newt.pdf ''Notes on Macdonald polynomials and the geometry of Hilbert schemes.'']  Symmetric functions 2001: surveys of developments and perspectives,  1&amp;ndash;64, NATO Sci. Ser. II Math. Phys. Chem., 74, Kluwer Acad. Publ., Dordrecht, 2002.{{MR|2059359}}
*{{citation|first=Mark|last=Haiman |arxiv=math.AG/0010246 |title=''Hilbert schemes, polygraphs, and the Macdonald positivity conjecture''|journal= J. Amer. Math. Soc. |volume=14 |year=2001|pages= 941&amp;ndash;1006|doi=10.1090/S0894-0347-01-00373-3|issue=4}}
*{{Citation | last1=Haglund | first1=J. | last2=Haiman | first2=M. | last3=Loehr | first3=N. | title=A combinatorial formula for Macdonald polynomials | doi=10.1090/S0894-0347-05-00485-6  |mr=2138143 | year=2005 | journal=[[Journal of the American Mathematical Society]] | issn=0894-0347 | volume=18 | issue=3 | pages=735–761}}
*{{citation|first=A. A. |last=Kirillov |url=http://www.ams.org/bull/1997-34-03/S0273-0979-97-00727-1/home.html |title=Lectures on affine Hecke algebras and Macdonald's conjectures|journal=  [[Bull. Amer. Math. Soc.]] |volume=34 |year=1997|pages= 251&amp;ndash;292|doi=10.1090/S0273-0979-97-00727-1|issue=3}}
*{{Citation | last1=Macdonald | first1=I. G. | title=Some conjectures for root systems | doi=10.1137/0513070 |mr=674768 | year=1982 | journal=SIAM Journal on Mathematical Analysis | issn=0036-1410 | volume=13 | issue=6 | pages=988–1007}}
*Macdonald, I. G. ''Symmetric functions and Hall polynomials.'' Second edition. Oxford Mathematical Monographs. Oxford Science Publications. The Clarendon Press, Oxford University Press, New York, 1995. x+475 pp. {{isbn|0-19-853489-2}} {{MR|1354144}}
*Macdonald, I. G. ''Symmetric functions and orthogonal polynomials.'' Dean Jacqueline B. Lewis Memorial Lectures presented at Rutgers University, New Brunswick, NJ. University Lecture Series, 12. American Mathematical Society, Providence, RI, 1998. xvi+53 pp. {{isbn|0-8218-0770-6}} {{MR|1488699}}
*Macdonald, I. G. ''Affine Hecke algebras and orthogonal polynomials.'' Séminaire Bourbaki 797 (1995).
*{{citation|last=Macdonald|first= I. G.|arxiv=math.QA/0011046|title= Orthogonal polynomials associated with root systems|journal= [[Séminaire Lotharingien de Combinatoire]]|volume= 45 |year=2000–01|pages= Art. B45a|mr=1817334 }}
*{{Citation | last=Macdonald | first=I. G. | title = Affine Hecke algebras and orthogonal polynomials | location=Cambridge | series=Cambridge Tracts in Mathematics | volume=157 | publisher=Cambridge University Press | year=2003 | pages=x+175 | isbn=978-0-521-82472-9| doi=10.2277/0521824729 |mr=1976581}}

==External links==
*Mike Zabrocki's page about [http://garsia.math.yorku.ca/MPWP/ Macdonald polynomials].
*Some of [http://math.berkeley.edu/~mhaiman/ Haiman's papers] about Macdonald polynomials.

[[Category:Algebraic combinatorics]]
[[Category:Algebraic geometry]]
[[Category:Orthogonal polynomials]]</text>
      <sha1>661zfekepyl721ibfdi1p3z00i0ln60</sha1>
    </revision>
  </page>
  <page>
    <title>Manipulation check</title>
    <ns>0</ns>
    <id>23244381</id>
    <revision>
      <id>745389003</id>
      <parentid>742056307</parentid>
      <timestamp>2016-10-20T20:47:53Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <comment>removed [[Category:Social sciences]]; added [[Category:Social research]] using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2193">{{unreferenced|date=July 2011}}
'''Manipulation check''' is a term in [[experimental research]] in the [[social science]]s which refers to certain kinds of secondary evaluations of an experiment.

== Overview ==
Manipulation checks are measured variables that show what the manipulated variables concurrently affect besides the dependent variable of interest.

In experiments, an experimenter manipulates some aspect of a process or task and [[Random assignment|randomly assigns]] subjects to different levels of the manipulation ("experimental conditions"). The experimenter then observes whether variation in the manipulated variables cause differences in the [[dependent and independent variables|dependent variable]]. Manipulation checks are targeted at variables beside the dependent variable of interest.

Manipulations are not intended to verify that the manipulated factor caused variation in the dependent variable.  This is verified by random assignment, manipulation before measurement of the dependent variable, and [[statistical hypothesis testing|statistical tests]] of effect of the manipulated variable on the dependent variable.  Thus, a failed manipulation check does not refute the hypothesis that the manipulation caused variation in the dependent variable.

In contrast, a successful manipulation check can help an experimenter rule out reasons that a manipulation may have failed to influence a dependent variable.  When a manipulation creates significant differences between experimental conditions in both (1) the dependent variable and (2) the measured manipulation check variable, the interpretation is that (1) the manipulation "causes" variation in the dependent variable (the "effect") and (2) the manipulation also explains variation in some other, more theoretically obvious measured variable that it is expected to concurrently influence, which assists in ''interpreting'' the "cause" (i.e., it only help interpret the "cause"; it is not necessary to ''affirm'' that the "cause" ''causes'' an effect).{{Clarify|date=August 2016}}


== See also ==
* [[Design of experiments]]

[[Category:Social research]]
[[Category:Design of experiments]]


{{statistics-stub}}</text>
      <sha1>rdpmgqc9me5d366jpbkiujhxrlhg2eo</sha1>
    </revision>
  </page>
  <page>
    <title>Margaret K. Butler</title>
    <ns>0</ns>
    <id>44714408</id>
    <revision>
      <id>857400702</id>
      <parentid>778593548</parentid>
      <timestamp>2018-08-31T13:06:44Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>/* References */add authority control, test</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6336">{{Infobox person
| name        = Margaret Kampschaefer Butler
| image       = &lt;!-- just the filename, without the File: or Image: prefix or enclosing [[brackets]] --&gt;
| alt         = 
| caption     = 
| birth_name  = &lt;!-- only use if different from name --&gt;
| birth_date  = {{Birth date|1924|03|27}}
| birth_place = [[Evansville, Indiana]]
| death_date  = {{Death date and age|2013|03|08|1924|03|27}}
| death_place = 
| nationality = [[United States]]
| other_names = 
| occupation  = [[Mathematician]]
| known_for   = 
}}
'''Margaret Kampschaefer Butler''' (March 27, 1924 – March 8, 2013) was a longtime [[mathematician]] who participated in creating and updating [[computer software]]. During the early 1950s, Butler contributed to the development of [[History of computing hardware|early computers]]. Butler was the first female fellow at the [[American Nuclear Society]] and director of the National Energy Software Center at Argonne. Butler held leadership positions within multiple scientific organizations and women's groups.&lt;ref name=UPI&gt;{{cite news|title=Computer pioneer Margaret Butler dies|url=http://www.upi.com/Science_News/2013/03/19/Computer-pioneer-Margaret-Butler-dies/UPI-79551363732047/|accessdate=18 December 2014|work=United Press International|date=March 19, 2013}}&lt;/ref&gt; She was the creator and director of the National Energy Software Center. Here, Butler operated an exchange for the editing of computer programs in regards to [[nuclear power]] and developed early principles for computer technology.&lt;ref name=Mothers&gt;"Mothers and Daughters of Invention." Google Books. N.p., n.d. Web. 14 December 2014.&lt;/ref&gt;

==Early life and education==
Butler was born on March 27, 1924 in Evansville, Indiana.&lt;ref&gt;{{Cite news|url=http://www.upi.com/Science_News/2013/03/19/Computer-pioneer-Margaret-Butler-dies/UPI-79551363732047/|title=Computer pioneer Margaret Butler dies|newspaper=UPI|access-date=2017-02-15|language=en}}&lt;/ref&gt;

==Career==
Butler began her career in 1944 working as a statistician at the [[Bureau of Labor Statistics]].&lt;ref name=Chicago&gt;{{cite web|last1=Giangrasse Kates|first1=Joan|title=Margaret K. Butler: 1924-2013|url=http://articles.chicagotribune.com/2013-03-19/news/ct-met-margaret-butler-obit-20130319_1_argonne-national-laboratory-league-of-women-voters-computers|website=Chicago Tribune|publisher=Chicago Tribune|accessdate=12 December 2014}}&lt;/ref&gt; While she worked there, she also taught math at the [[United States Department of Agriculture]] Graduate School and took graduate courses related to sampling theory.&lt;ref name=Chicago /&gt; About a year later, she joined the [[United States Air Force]] and worked as a civilian in Germany.&lt;ref name=Chicago /&gt; She returned to the United States after two years and began working in the Naval Reactors Division of [[Argonne National Laboratory]] as a junior mathematician.&lt;ref name=Chicago /&gt; While working at Argonne, Butler made calculations for physicists creating a prototype for a [[nuclear submarine|submarine reactor]] and attended [[atomic physics]] and [[reactor design]] classes.&lt;ref name=ANL&gt;{{cite web|title=In memoriam: The remarkable career of Margaret Butler|url=http://www.anl.gov/articles/memoriam-remarkable-career-margaret-butler|website=Argonne National Library|publisher=Argonne National Laboratory|accessdate=15 December 2014}}&lt;/ref&gt; In 1949, she worked at the Bureau of Labor Statistics in Minnesota but returned to Argonne National Laboratory in 1951.&lt;ref name=ANL /&gt; Following her return to Argonne, Butler became an assistant mathematician in the Reactor Engineering Division and worked on [[AVIDAC]], an early computer.&lt;ref name=Chicago /&gt; In the 1950s Butler wrote software, reactor applications, [[mathematical software|mathematical subroutines]], and [[utility program|utilities]] for three other Argonne computers, the [[ORACLE (computer)|ORACLE]], [[GEORGE (computer)|GEORGE]], and [[UNIVAC]].&lt;ref name=ANL /&gt; From the late 1950s to early 1960s she led Argonne's Applied Mathematics Division's Application Programming.&lt;ref name=ANL /&gt; While working in this department, Butler developed teams to fix program problems in reactors, biology, chemistry, physics, management, and high energy physics applications.&lt;ref name=ANL /&gt; In 1960, Butler worked with others to establish the Argonne Code Center, which later became the National Energy Software Center (NESC).&lt;ref name=ANL /&gt; Butler would later become director of the NESC from 1972&amp;ndash;1991.&lt;ref name=ANL /&gt; In 1980, Butler was promoted to Senior Computer Scientist at Argonne. Butler officially retired in 1991, but continued to work at Argonne from 1993 to 2006 as a "special term appointee".&lt;ref name=ANL /&gt;

==Impact==
During her time in Argonne, Butler was very supportive of her female coworkers.&lt;ref name=Chicago /&gt; Women working at Argonne described her as a role model with a welcoming presence.&lt;ref name=ANL /&gt; According to Margaret's son Jay, she thought women were "given all the responsibilities and none of the authorities" and had to work "harder and smarter" yet were still not treated as individuals.&lt;ref name=Chicago /&gt;&lt;ref&gt;{{cite web|last1=Stump|first1=Holly|title=Margaret Butler: One Woman’s Life in Science|url=https://www.semiwiki.com/forum/content/2128-margaret-butler-one-woman%92s-life-science.html|website=SemiWiki|accessdate=18 December 2014}}&lt;/ref&gt; When Butler rose in the ranks at Argonne, she made sure to hire women and recommend them for promotions.&lt;ref name=Chicago /&gt; Margaret worked with other women to organize an [[Association for Women in Science]] in Chicago.&lt;ref name=ANL /&gt; While in AWIS, Margaret held executive board positions and led two conferences for high school students, teachers, and administration.&lt;ref name=Chicago /&gt;&lt;ref name=ANL /&gt;

==References==
{{reflist|30em}}

&lt;!---article uses short references format. It needs to have a bibliography appended.---&gt;

{{authority control}}

{{DEFAULTSORT:Butler, Margaret K.}}
[[Category:1924 births]]
[[Category:2013 deaths]]
[[Category:Women mathematicians]]
[[Category:Women statisticians]]
[[Category:Women computer scientists]]
[[Category:People from Evansville, Indiana]]
[[Category:Indiana University alumni]]
[[Category:20th-century American mathematicians]]
[[Category:21st-century American mathematicians]]
[[Category:Argonne National Laboratory people]]</text>
      <sha1>583l6trkibn8js7mq549dbkbrl7kxge</sha1>
    </revision>
  </page>
  <page>
    <title>Martin Farach-Colton</title>
    <ns>0</ns>
    <id>47188724</id>
    <revision>
      <id>859897793</id>
      <parentid>809225347</parentid>
      <timestamp>2018-09-17T00:53:57Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>add authority control, test</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6299">'''Martin Farach-Colton''' is an American [[computer scientist]], known for his work in [[streaming algorithm]]s, [[suffix tree]] construction, [[pattern matching]] in [[data compression|compressed data]], [[cache-oblivious algorithm]]s, and [[lowest common ancestor]] [[data structure]]s. He is a professor of computer science at [[Rutgers University]],&lt;ref&gt;[http://www.cs.rutgers.edu/people/directory.php?type=faculty Faculty listing], Computer Science, Rutgers, retrieved 2015-07-08.&lt;/ref&gt; and a co-founder of storage technology startup company Tokutek.&lt;ref name="odbms"&gt;{{citation|title=Scaling MySQL and MariaDB to TBs: Interview with Martín Farach-Colton|first=Roberto V.|last=Zicari|date=October 8, 2012|url=http://www.odbms.org/blog/2012/10/scaling-mysql-and-mariadb-to-tbs-interview-with-martin-farach-colton/|work=ODBMS Industry Watch}}.&lt;/ref&gt;

Farach-Colton is of [[Argentines|Argentine]] descent, and grew up in [[South Carolina]]. While attending [[medical school]], he came out as [[homosexuality|gay]], and met his future husband, with whom he now has twin children.&lt;ref&gt;{{citation|url=https://lucatrevisan.wordpress.com/2012/07/10/turing-centennial-post-5-martin-farach-colton/|first=Martin|last=Farach-Colton|title=Turing Centennial Post 5: Martin Farach-Colton|date=July 10, 2012|work=in theory|editor-first=Luca|editor-last=Trevisan|editor-link=Luca Trevisan}}.&lt;/ref&gt; Farach-Colton is an avid [[Brazilian jiu-jitsu]] practitioner and received a bronze medal at the 2015 World Master Jiu-Jitsu IBJJF Championship.&lt;ref&gt;[http://static.ibjjfdb.com/Campeonato/000450/en-US/Results.pdf World Master Jiu-Jitsu IBJJF Championship 2015]&lt;/ref&gt; He obtained his Ph.D. in 1991 from the [[University of Maryland, College Park]] under the supervision of Amihood Amir.&lt;ref&gt;{{mathgenealogy|id=70210}}&lt;/ref&gt; He was program chair of the 14th ACM-SIAM [[Symposium on Discrete Algorithms]] (SODA 2003).&lt;ref&gt;[http://www.siam.org/meetings/da03/ 14th ACM-SIAM Symposium on Discrete Algorithms], SIAM, retrieved 2015-07-08.&lt;/ref&gt;

The [[cache-oblivious algorithm|cache-oblivious]] [[B-tree]] data structures studied by Bender, Demaine, and Farach-Colton beginning in 2000 became the basis for the [[fractal tree index]] used by Tokutek's products [[TokuDB]] and [[TokuMX]].&lt;ref name="odbms"/&gt;

Farach-Colton also serves on several charity boards including the Ali Forney Center and Lamda Legal.&lt;ref&gt;{{Cite web|url=https://www.aliforneycenter.org/about-us/board-members1/martin-farach-colton/|title=Martin Farach-Colton|website=www.aliforneycenter.org|language=en|access-date=2017-11-07}}&lt;/ref&gt;

==Selected publications==
*{{citation
 | last1 = Amir | first1 = Amihood
 | last2 = Benson | first2 = Gary
 | last3 = Farach | first3 = Martin
 | doi = 10.1006/jcss.1996.0023
 | issue = 2
 | journal = [[Journal of Computer and System Sciences]]
 | mr = 1393996
 | pages = 299–307
 | title = Let sleeping files lie: pattern matching in Z-compressed files
 | volume = 52
 | date = April 1996
 | citeseerx = 10.1.1.45.6476
 | url = http://tandem.bu.edu/papers/let.sleeping.files.lie.jcss.1996.pdf}}.
*{{citation
 | last = Farach | first = Martin
 | contribution = Optimal suffix tree construction with large alphabets
 | doi = 10.1109/SFCS.1997.646102
 | pages = 137–143
 | publisher = IEEE Computer Society
 | title = 38th Annual [[Symposium on Foundations of Computer Science]], FOCS '97, Miami Beach, Florida, USA, October 19-22, 1997
 | year = 1997
 | citeseerx = 10.1.1.45.4336
 | contribution-url = http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.4336&amp;type=pdf}}.
*{{citation
 | last1 = Farach | first1 = M.
 | last2 = Thorup | first2 = M. | author2-link = Mikkel Thorup
 | doi = 10.1007/PL00009202
 | issue = 4
 | journal = [[Algorithmica]]
 | mr = 1600834
 | pages = 388–404
 | title = String matching in Lempel-Ziv compressed strings
 | volume = 20
 | date = April 1998
 | citeseerx = 10.1.1.45.5484
 | url = http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.45.5484&amp;type=pdf}}.
*{{citation
 | last1 = Bender | first1 = Michael A.
 | last2 = Farach-Colton | first2 = Martin
 | editor1-last = Gonnet | editor1-first = Gaston H.
 | editor2-last = Panario | editor2-first = Daniel
 | editor3-last = Viola | editor3-first = Alfredo
 | contribution = The LCA problem revisited
 | doi = 10.1007/10719839_9
 | pages = 88–94
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = LATIN 2000: Theoretical Informatics, 4th Latin American Symposium, Punta del Este, Uruguay, April 10-14, 2000, Proceedings
 | volume = 1776
 | year = 2000
 | contribution-url = http://www.ics.uci.edu/%7Eeppstein/261/BenFar-LCA-00.pdf}}.
*{{citation
 | last1 = Charikar | first1 = Moses | author1-link = Moses Charikar
 | last2 = Chen | first2 = Kevin
 | last3 = Farach-Colton | first3 = Martin
 | doi = 10.1016/S0304-3975(03)00400-6
 | issue = 1
 | journal = [[Theoretical Computer Science (journal)|Theoretical Computer Science]]
 | mr = 2045483
 | pages = 3–15
 | title = Finding frequent items in data streams
 | volume = 312
 | year = 2004
 | citeseerx = 10.1.1.145.8413
 | url = https://www.cs.rutgers.edu/~farach/pubs/FrequentStream.pdf}}. Previously announced in ICALP 2002.
*{{citation
 | last1 = Bender | first1 = Michael A.
 | last2 = Demaine | first2 = Erik D. | author2-link = Erik Demaine
 | last3 = Farach-Colton | first3 = Martin
 | doi = 10.1137/S0097539701389956
 | issue = 2
 | journal = [[SIAM Journal on Computing]]
 | mr = 2191447
 | pages = 341–358
 | title = Cache-oblivious B-trees
 | url = http://erikdemaine.org/papers/CacheObliviousBTrees_SICOMP/
 | volume = 35
 | year = 2005
 | citeseerx = 10.1.1.32.4093}}. Previously announced at FOCS 2000.

==References==
{{reflist}}

==External links==
*[http://www.cs.rutgers.edu/~farach/ Home page]
*[https://scholar.google.com/citations?user=2uHUSa8AAAAJ Google scholar profile]

{{authority control}}

{{DEFAULTSORT:Farach-Colton, Martin}}
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]
[[Category:American people of Argentine descent]]
[[Category:LGBT scientists from the United States]]
[[Category:American computer scientists]]
[[Category:Theoretical computer scientists]]
[[Category:University of Maryland, College Park alumni]]
[[Category:Rutgers University faculty]]</text>
      <sha1>843lyfkwizjggtu9rhx7pxcsf6wlq47</sha1>
    </revision>
  </page>
  <page>
    <title>Maxime Bôcher</title>
    <ns>0</ns>
    <id>3180285</id>
    <revision>
      <id>856373753</id>
      <parentid>853816371</parentid>
      <timestamp>2018-08-24T19:04:46Z</timestamp>
      <contributor>
        <username>Badger M.</username>
        <id>10314598</id>
      </contributor>
      <comment>/* External links */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7604">{{short description|American mathematician}}
{{Infobox scientist
|name              = Maxime Bôcher
|image             = Maxime Bôcher.jpg
|image_size       =
|caption           = 
|birth_date        = {{Birth date|1867|08|28|mf=y}}
|birth_place       = [[Boston, Massachusetts]]
|death_date        = {{death date and age|1918|09|12|1867|08|28|mf=y}}
|death_place       = [[Cambridge, Massachusetts]]
|nationality       = [[United States|American]]
|field             = [[Mathematics]]
|work_institutions = [[Harvard University]]
|alma_mater        = [[Harvard University]]&lt;br&gt;[[University of Göttingen]]
|doctoral_advisor  = [[Felix Klein]]
|doctoral_students = [[William Charles Brenke|William Brenke]]&lt;br&gt;[[David Raymond Curtiss|David R. Curtiss]]&lt;br&gt;[[Griffith C. Evans]]&lt;br&gt;[[Lester R. Ford]]&lt;br&gt;[[Walter Burton Ford|Walter B. Ford]]&lt;br&gt;[[James Waterman Glover|James W. Glover]]&lt;br&gt;[[Charles Napoleon Moore|Charles N. Moore]]&lt;br&gt;[[William Henry Roever|William H. Roever]]&lt;br&gt;[[Joseph Leonard Walsh|Joseph L. Walsh]]
|known_for         = [[Differential equation]]s, [[series (mathematics)|series]], and [[algebra]]
|prizes            = 
}}
'''Maxime Bôcher''' (August 28, 1867 – September 12, 1918) was an [[United States|American]] [[mathematician]] who published about 100 papers on [[differential equation]]s, [[series (mathematics)|series]], and [[algebra]].&lt;ref&gt;{{cite journal|author=Birkhoff, George D.|authorlink=George David Birkhoff|title=The scientific work of Maxime Bôcher|journal=Bull. Amer. Math. Soc.|year=1919|volume=25|issue=5|pages=197–215|mr=1560177|doi=10.1090/s0002-9904-1919-03172-3}}&lt;/ref&gt; He also wrote elementary texts such as ''[[Trigonometry]]'' and ''[[Analytic Geometry]]''.&lt;ref&gt;{{cite journal|author=Osgood, William F.|authorlink=William Fogg Osgood|title=The life and services of Maxime Bôcher|journal=Bull. Amer. Math. Soc.|year=1919|volume=25|issue=8|pages=337–350|mr=1560199|doi=10.1090/s0002-9904-1919-03198-x}}&lt;/ref&gt; [[Bôcher's theorem]], [[Bôcher's equation]], and the [[Bôcher Memorial Prize]] are named after him.

== Life ==

Bôcher was born in [[Boston, Massachusetts]]. His parents were Caroline Little and [[Ferdinand Bôcher]]. Maxime's father was professor of modern languages at the [[Massachusetts Institute of Technology]] when Maxime was born, and became Professor of [[French language|French]] at [[Harvard]] in 1872.

Bôcher received an excellent education from his parents and from a number of public and private schools in Massachusetts. He graduated from the [[Cambridge Rindge and Latin School|Cambridge Latin School]] in 1883. He received his first degree from Harvard in 1888. At Harvard, he studied a wide range of topics, including [[mathematics]], [[Latin]], [[chemistry]], philosophy, zoology, [[geography]], [[geology]], meteorology, Roman art, and [[music]].

Bôcher was awarded many prestigious prizes, which allowed him to travel to Europe to do research. [[University of Göttingen|Göttingen]] was then the leading mathematics university, and he attended lectures by [[Felix Klein|Klein]], [[Schönflies]], [[Hermann Schwarz|Schwarz]], [[Issai Schur|Schur]] and [[Woldemar Voigt|Voigt]]. He was awarded a doctorate in 1891 for his dissertation ''Über die Reihenentwicklungen der Potentialtheorie'' ([[German language|German]] for "On the Development of the Potential Function into Series"), he was encouraged to study this topic by Klein. He received a Göttingen university prize for this work.

In Göttingen he met Marie Niemann, and they were married in July 1891. They had three children, Helen, Esther, and Frederick. He returned with his wife to Harvard where he was appointed as an instructor. In 1894 he was promoted to assistant professor, due to his impressive record. He became a full professor of mathematics in 1904. He was president of the [[American Mathematical Society]] from 1908 to 1910.&lt;ref name=amer&gt;{{Americana|wstitle=Bocher, Maxime|year=1920|inline=1}}&lt;/ref&gt;

Although he was only 46 years old, there were already signs that his weak health was failing. He died at his [[Cambridge, Massachusetts|Cambridge]] home after suffering a prolonged illness.

== Bôcher's theorem ==

[[Bôcher's theorem]] states that the finite zeros of the derivative &lt;math&gt; r'(z) &lt;/math&gt; of a nonconstant rational function &lt;math&gt; r(z) &lt;/math&gt; that are not multiple zeros of &lt;math&gt; r(z) &lt;/math&gt; are the positions of equilibrium in the field of force due to particles of positive mass at the zeros of &lt;math&gt; r(z) &lt;/math&gt; and particles of [[negative mass]] at the poles of &lt;math&gt; r(z) &lt;/math&gt;, with masses numerically equal to the respective multiplicities, where each particle repels with a force equal to the mass times the inverse distance.

== Bôcher's equation ==

Bôcher's equation is a second-order ordinary differential equation of the form:

:&lt;math&gt; y''+\frac{1}{2} \left [\frac{m_1}{x-a_1}+ \cdots +\frac{m_{n-1}}{x-a_{n-1}} \right] y' +\frac{1}{4} \left [\frac{A_0+A_1x+ \cdots +A_\ell x^\ell}{(x-a_1)^m_1 (x-a_2)^m_2 \cdots (x-a_{n-1})^m_{n-1}} \right]y=0. &lt;/math&gt;

== The Bôcher Memorial Prize ==

The [[Bôcher Memorial Prize]] is awarded by the American Mathematical Society every five years for notable research in analysis that has appeared in a recognized North American journal.

Winners have included [[James W. Alexander II]] (1928), [[Eric Temple Bell]] (1924), [[George D. Birkhoff]] (1923), [[Paul Cohen (mathematician)|Paul J. Cohen]] (1964), [[Solomon Lefschetz]] (1924), [[Marston Morse]]  and [[Norbert Wiener]] (1933), and [[John von Neumann]] (1938).

== Works ==
* 1894: [https://archive.org/details/acv1533.0001.001.umich.edu Ueber die Reihenentwicklungen der Potentialtheorie] via [[Internet Archive]]
* 1900: "Randwertaufgaben bei Gewöhnlich Differentialgleichung", [[Klein's encyclopedia|''Encyclopädie der mathematischen Wissenschaften'']] Band 2–1–1.
* 1907: (with E.P.R.DuVal) [https://babel.hathitrust.org/cgi/pt?id=uc1.b4248862;view=1up;seq=7 Introduction to Higher Algebra] via [[HathiTrust]]
* 1909: [https://archive.org/details/introductiontost00bcuoft Introduction to the study of Integral Equations] via Internet Archive
* 1917: [https://archive.org/details/leonssurlesm00bcuoft Leçons sur les méthodes de Sturm dans la théorie des équations différentielles linéaires et leurs développements modernes] via Internet Archive.

Bôcher was one of the editors of the ''[[Annals of Mathematics]]'', of the [[Transactions of the American Mathematical Society|''Transactions'' of the American Mathematical Society]].&lt;ref name="amer" /&gt;

== References ==
{{reflist}}

== External links ==
*{{MacTutor Biography|id=Bocher}}
* [http://books.nap.edu/html/biomems/mbocher.html Maxime Bocher] biographical memoirs of the national academy of sciences.
*{{MathGenealogy|id=7431}}
*[http://www.nasonline.org/publications/biographical-memoirs/memoir-pdfs/bcher-maxime.pdf National Academy of Sciences Biographical Memoir]

{{AMS Presidents}}

{{Authority control}}

{{DEFAULTSORT:Bocher, Maxime}}
[[Category:19th-century American mathematicians]]
[[Category:20th-century American mathematicians]]
[[Category:Mathematical analysts]]
[[Category:Harvard University faculty]]
[[Category:Harvard University alumni]]
[[Category:University of Göttingen alumni]]
[[Category:1867 births]]
[[Category:1918 deaths]]
[[Category:Presidents of the American Mathematical Society]]
[[Category:Cambridge Rindge and Latin School alumni]]
[[Category:People from Boston]]
[[Category:Mathematicians from Massachusetts]]
[[Category:Members of the United States National Academy of Sciences]]</text>
      <sha1>4kv1rdlwlosduhvw1q66q5w8teqwfih</sha1>
    </revision>
  </page>
  <page>
    <title>MyMaths</title>
    <ns>0</ns>
    <id>35879943</id>
    <revision>
      <id>868677115</id>
      <parentid>868677077</parentid>
      <timestamp>2018-11-13T18:51:18Z</timestamp>
      <contributor>
        <username>Crystallizedcarbon</username>
        <id>22150306</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/129.18.157.62|129.18.157.62]] ([[User talk:129.18.157.62|talk]]) ([[WP:HG|HG]]) (3.1.22)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2478">{{Orphan|date=September 2016}}
{{Infobox dot-com company
| name = MyMaths
| logo =
| Description_of_logo =
| company_type = [[Learning]]
| foundation = 
| area_served =
| key_people =
| industry = [[Education]]
| services = [[Subscription]]
| revenue =
| operating_income =
| net_income =
| owner = [[Oxford University Press]]
| num_employees =
| parent =
| divisions =
| subsid =
| url = [http://www.mymaths.co.uk/ www.mymaths.co.uk]
| alexa = {{Increase}} 12,765 ({{as of|2016|6|9|alt=June 2016}})&lt;ref name="alexa"&gt;{{cite web|url= http://www.alexa.com/siteinfo/mymaths.co.uk |title= Mymaths.co.uk Site Info | publisher= [[Alexa Internet]] |accessdate=9 June 2016}}&lt;/ref&gt;
| screenshot =
| caption =
| website_type = Learning
| language = English and many others
| advertising =
| launch_date =
| current_status = Live
| footnotes =
| intl =
}}
'''MyMaths''' is a subscription-based [[mathematics]] website based on [[Adobe Flash]] which can be used on interactive whiteboards or by students and teachers at home.&lt;ref&gt;{{cite book|last=Cowan|first=Pamela|page=112|title=Teaching Mathematics: A Handbook for Primary and Secondary School Teachers|publisher=Psychology Press|year=2006|isbn=9780415335171}}&lt;/ref&gt;&lt;ref&gt;{{cite book|title=Basic Maths for Dummies|last=Beveridge|first=Colin|publisher=John Wiley &amp; Sons|year=2011|page=314|isbn=9781119975618}}&lt;/ref&gt; It is owned and operated by [[Oxford University Press]], who acquired the site in 2011. MyMaths is currently used in over 80% of secondary schools in the UK. As of now{{when|date=October 2018}} MyMaths has over 4 million student users in over 70 countries worldwide, many of whom are in England.&lt;ref&gt;{{cite web|title=Moreton Online Services|publisher=Morton School|url=http://moreton-school.org/moreton-online-services/ |date=9 June 2016}}&lt;/ref&gt;

==Usage==
Users are required to have subscriptions, after which they are given a username and password to log in and access MyMaths. MyMaths has a wide range of curriculum materials and resources aimed at students in primary school and high school.&lt;ref name="1c"&gt;{{cite web|title=MyMaths|publisher=HeyGrove (Somerset Educational Institution)|url=http://www.haygrove.somerset.sch.uk/document_1.aspx?id=0:61412&amp;id=0:35714&amp;id=0:35701&amp;id=0:35626 |date=7 September 2011 |accessdate=22 May 2012}}&lt;/ref&gt; 

== References ==
{{reflist}}

==External links==
* [http://www.mymaths.co.uk/ Official website]

[[Category:Educational websites]]

{{edu-website-stub}}
{{mathematics-lit-stub}}</text>
      <sha1>8e01tyrxbn25wrvhpuquf2iqb78k7vc</sha1>
    </revision>
  </page>
  <page>
    <title>Negligible function</title>
    <ns>0</ns>
    <id>8426019</id>
    <revision>
      <id>864802767</id>
      <parentid>864802677</parentid>
      <timestamp>2018-10-19T15:28:08Z</timestamp>
      <contributor>
        <username>Hasan.iqbal.anik</username>
        <id>27646092</id>
      </contributor>
      <minor/>
      <comment>/* Examples */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7137">In mathematics, a '''negligible function''' is a [[Function (mathematics)|function]] &lt;math&gt;\mu:\mathbb{N}\to\mathbb{R}&lt;/math&gt; such that for every positive integer ''c'' there exists an integer ''N''&lt;sub&gt;''c''&lt;/sub&gt; such that for all ''x''&amp;nbsp;&gt;&amp;nbsp;''N''&lt;sub&gt;''c''&lt;/sub&gt;,

:&lt;math&gt;|\mu(x)|&lt;\frac{1}{x^c}.&lt;/math&gt;

Equivalently, we may also use the following definition.
A function &lt;math&gt;\mu(x):\mathbb{N}\to\mathbb{R}&lt;/math&gt; is '''negligible''', if for every [[positive polynomial]] poly(·) there exists an integer ''N''&lt;sub&gt;poly&lt;/sub&gt;&amp;nbsp;&gt;&amp;nbsp;0 such that for all ''x''&amp;nbsp;&gt;&amp;nbsp;''N''&lt;sub&gt;poly&lt;/sub&gt;

: &lt;math&gt;|\mu(x)|&lt;\frac{1}{\text{poly}(x)}.&lt;/math&gt;

==History==
The concept of ''negligibility'' can find its trace back to sound models of analysis.  Though the concepts of "[[continuous function|continuity]]" and "[[infinitesimal]]" became important in mathematics during [[Isaac Newton|Newton]] and [[Gottfried Leibniz|Leibniz]]'s time (1680s), they were not well-defined until the late 1810s.  The first reasonably rigorous definition of ''continuity'' in [[mathematical analysis]] was due to [[Bernard Bolzano]], who wrote in 1817 the modern definition of continuity.  Later [[Augustin Louis Cauchy|Cauchy]], [[Karl Weierstrass|Weierstrass]] and [[Eduard Heine|Heine]] also defined as follows (with all numbers in the real number domain &lt;math&gt;\mathbb{R}&lt;/math&gt;):

:'''([[Continuous function]])''' A function &lt;math&gt;f:\mathbb{R}{\rightarrow}\mathbb{R}&lt;/math&gt; is ''continuous'' at &lt;math&gt;x=x_0&lt;/math&gt; if for every &lt;math&gt;\epsilon&gt;0&lt;/math&gt;, there exists a positive number &lt;math&gt;\delta&gt;0&lt;/math&gt; such that &lt;math&gt;|x-x_0|&lt;\delta&lt;/math&gt; implies &lt;math&gt;|f(x)-f(x_0)|&lt;\epsilon.&lt;/math&gt;

This classic definition of continuity can be transformed into the
definition of negligibility in a few steps by changing parameters used in the definition.  First, in the case &lt;math&gt;x_0=\infty&lt;/math&gt; with &lt;math&gt;f(x_0)=0&lt;/math&gt;, we must define the concept of "''infinitesimal function''":

:'''([[Infinitesimal]])''' A continuous function &lt;math&gt;\mu:\mathbb{R}\to\mathbb{R}&lt;/math&gt; is ''infinitesimal'' (as &lt;math&gt;x&lt;/math&gt; goes to infinity) if for every &lt;math&gt;\epsilon&gt;0&lt;/math&gt; there exists &lt;math&gt;N_{\epsilon}&lt;/math&gt; such that for all &lt;math&gt;x&gt;N_{\epsilon}&lt;/math&gt;
::&lt;math&gt;|\mu(x)|&lt;\epsilon\,.&lt;/math&gt;{{Citation needed|date=August 2007}}

Next, we replace &lt;math&gt;\epsilon&gt;0&lt;/math&gt; by the functions &lt;math&gt;1/x^c&lt;/math&gt; where &lt;math&gt;c&gt;0&lt;/math&gt; or by &lt;math&gt;1/poly(x)&lt;/math&gt; where &lt;math&gt;poly(x)&lt;/math&gt; is a positive polynomial. This leads to the definitions of negligible functions given at the top of this article. Since the constants &lt;math&gt;\epsilon&gt;0&lt;/math&gt; can be expressed as &lt;math&gt;1/poly(x)&lt;/math&gt; with a constant polynomial this shows that negligible functions are a subset of the infinitesimal functions.

==Use in cryptography==

In complexity-based modern [[cryptography]], a security scheme is
''[[provable security|provably secure]]'' if the probability of security failure (e.g.,
inverting a [[one-way function]], distinguishing [[Cryptographically secure pseudorandom number generator|cryptographically strong pseudorandom bits]] from truly random bits) is '''negligible''' in terms of the input &lt;math&gt;x&lt;/math&gt; = cryptographic key length &lt;math&gt;n&lt;/math&gt;.  Hence comes the definition at the top of the page because key length &lt;math&gt;n&lt;/math&gt; must be a natural number.

Nevertheless, the general notion of negligibility doesn't require that the input parameter &lt;math&gt;x&lt;/math&gt; is the key length &lt;math&gt;n&lt;/math&gt;.  Indeed, &lt;math&gt;x&lt;/math&gt; can be any predetermined system metric and corresponding mathematical analysis would illustrate some hidden analytical behaviors of the system.

The reciprocal-of-polynomial formulation is used for the same reason that [[Computationally bounded adversary|computational boundedness]] is defined as polynomial running time: it has mathematical closure properties that make it tractable in the [[asymptotic security|asymptotic]] setting (see [[#Closure properties]]). For example, if an attack succeeds in violating a security condition only with negligible probability, and the attack is repeated a polynomial number of times, the success probability of the overall attack still remains negligible.

In practice one might want to have more [[concrete security|concrete]] functions bounding the adversary's success probability and to choose the security parameter large enough that this probability is smaller than some threshold, say&amp;nbsp;2&lt;sup&gt;&amp;minus;128&lt;/sup&gt;.

==Closure properties==

One of the reasons that negligible functions are used in foundations of [[Computational complexity theory|complexity-theoretic]] cryptography is that they obey closure properties.&lt;ref&gt;{{Cite book|url=https://www.worldcat.org/oclc/893721520|title=Introduction to modern cryptography|last=Katz|first=Johnathan|others=Lindell, Yehuda|isbn=9781466570269|edition= Second|location=Boca Raton|oclc=893721520}}&lt;/ref&gt; Specifically,

# If &lt;math&gt;f,g:\mathbb{N}\to\mathbb{R}&lt;/math&gt; are negligible, then the function &lt;math&gt;x\mapsto f(x)+g(x)&lt;/math&gt; is negligible.
# If &lt;math&gt;f:\mathbb{N}\to\mathbb{R}&lt;/math&gt; is negligible and &lt;math&gt;p&lt;/math&gt; is any real polynomial, then the function &lt;math&gt;x\mapsto p(x)\cdot f(x)&lt;/math&gt; is negligible.

[[Converse (logic)|Conversely]], if &lt;math&gt;f:\mathbb{N}\to\mathbb{R}&lt;/math&gt; is not negligible, then neither is &lt;math&gt;x\mapsto f(x)/p(x)&lt;/math&gt; for any real polynomial &lt;math&gt;p&lt;/math&gt;.

==Examples==

{{Expand section|date=March 2018}}

* &lt;math&gt;n\mapsto x^{-n}&lt;/math&gt; is negligible for any &lt;math&gt;x\geq 2&lt;/math&gt;.
*&lt;math&gt; f(n) = 3^{-\sqrt{n}}&lt;/math&gt; is negligible.

==See also==
*[[Negligible set]]
*[[Colombeau algebra]]
*[[Non-standard analysis|Nonstandard numbers]]
*[[Gromov's theorem on groups of polynomial growth]]
*[[Non-standard calculus]]

==References==
{{Reflist}}
* {{cite book | last = Goldreich | first = Oded | date = 2001 | title = Foundations of Cryptography: Volume 1, Basic Tools | publisher = Cambridge University Press | isbn = 0-521-79172-3 | url = http://www.wisdom.weizmann.ac.il/~oded/frag.html }}
* {{cite book| first = Michael | last = Sipser | authorlink = Michael Sipser | date = 1997 | title = Introduction to the Theory of Computation | publisher = PWS Publishing | isbn = 0-534-94728-X | chapter = Section 10.6.3: One-way functions | pages = 374–376 }}
* {{cite book| first = Christos | last = Papadimitriou | authorlink = Christos Papadimitriou | date = 1993 | title = Computational Complexity | publisher = Addison Wesley | edition = 1st | isbn = 0-201-53082-1 | chapter = Section 12.1: One-way functions | pages = 279–298 }}
* {{cite book | first = Jean François | last = Colombeau | authorlink = Jean François Colombeau | date = 1984 | title = New Generalized Functions and Multiplication of Distributions | publisher = Mathematics Studies 84, North Holland | isbn = 0-444-86830-5}}
* {{cite paper | first = Mihir | last = Bellare | date = 1997 | citeseerx = 10.1.1.43.7900 | title = A Note on Negligible Functions | publisher = Dept. of Computer Science &amp; Engineering University of California at San Diego }}

[[Category:Mathematical analysis]]
[[Category:Types of functions]]</text>
      <sha1>dprkiojy0gzv2edcp53z6b7j5nb11tq</sha1>
    </revision>
  </page>
  <page>
    <title>Network scheduler</title>
    <ns>0</ns>
    <id>38050347</id>
    <revision>
      <id>867718309</id>
      <parentid>863734676</parentid>
      <timestamp>2018-11-07T15:27:53Z</timestamp>
      <contributor>
        <username>Kvng</username>
        <id>910180</id>
      </contributor>
      <comment>review complete: combine short related sections. ce.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11975">{{About|scheduling in networking|scheduling in computing generally|Scheduling (computing)}}
[[File:Data Queue.svg|thumb|right|250px|Packets queuing in a [[FIFO (computing and electronics)|FIFO]] (first in, first out) data structure.]]

A '''network scheduler''', also called '''packet scheduler''', '''queueing discipline''', '''qdisc''' or '''queueing algorithm''', is an [[Arbiter (electronics)|arbiter]] on a [[Node (networking)|node]] in [[packet switching]] communication network. It manages the sequence of [[network packet]]s in the transmit and receive [[queue (abstract data type)|queues]] of the [[network interface controller]]. There are several network schedulers available for the different [[operating system]]s, that implement many of the existing network [[scheduling algorithm]]s.

The network scheduler logic decides which network packet to forward next. The network scheduler is associated with a queuing system, storing the network packets temporarily until they are transmitted. Systems may have a single or multiple queues in which case each may hold the packets of one [[Traffic_flow_(computer_networking)|flow]], classification, or priority.

In some cases it may not be possible to schedule all transmissions within the constraints of the system. In these cases the network scheduler is responsible for deciding which traffic to forward and what gets [[Packet loss|dropped]].

== Algorithms ==
In the course of time many network queueing disciplines have been developed. Each of these provides specific reordering or dropping of network packets inside various transmit or receive [[Data buffer|buffers]].&lt;ref&gt;{{cite web
 | url = http://www.tldp.org/HOWTO/Traffic-Control-HOWTO/classless-qdiscs.html
 | title = Traffic Control HOWTO: Classless Queuing Disciplines (qdiscs)
 | accessdate = {{date|2013-11-24|mdy}}
 | website = tldp.org
}}&lt;/ref&gt;&lt;ref&gt;{{cite web
 | url = http://qos.ittc.ku.edu/howto/node8.html
 | title = QoS Support in Linux: Queuing Disciplines
 | date = {{date|1999-09-30|mdy}} | accessdate = {{date|2014-03-18|mdy}}
 | author = Saravanan Radhakrishnan  | website = qos.ittc.ku.edu
}}&lt;/ref&gt;
Queuing disciplines are commonly used as attempts to compensate for various networking conditions, like reducing the [[Network latency|latency]] for certain classes of network packets, and are generally used as part of [[quality of service]] (QoS) measures.&lt;ref&gt;{{cite web
 | url = http://www.tldp.org/HOWTO/Traffic-Control-HOWTO/components.html
 | title = Traffic Control HOWTO: Components of Linux Traffic Control
 | accessdate = {{date|2013-11-24|mdy}}
 | website = tldp.org
}}&lt;/ref&gt;&lt;ref&gt;{{cite web
 | url = http://www.tldp.org/HOWTO/Traffic-Control-HOWTO/elements.html
 | title = Traffic Control HOWTO: Traditional Elements of Traffic Control
 | accessdate = {{date|2013-11-24|mdy}}
 | website = tldp.org
}}&lt;/ref&gt;&lt;ref&gt;{{cite web
 | url = http://www.eng.tau.ac.il/~netlab/resources/booklet/lab9.pdf
 | title = Queuing Disciplines: Order of Packet Transmission and Dropping 
 | date = {{date|2006-10-25|mdy}} | accessdate = {{date|2014-03-18|mdy}}
 | website = tau.ac.il | format = PDF
}}&lt;/ref&gt;

Examples of algorithms suitable for managing network traffic include:

{{Div col|colwidth=30em}}
* AVQ ([[adaptive virtual queue]])&lt;ref&gt;http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.108.4477&amp;rep=rep1&amp;type=pdf&lt;/ref&gt;
* CBQ ([[class-based queueing]]) discipline
* CHOKe (CHOose and Keep for responsive flows, CHOose and Kill for unresponsive flows) is a variant of RED
* [[CoDel]] (controlled delay) and fair/flow queue CoDel
* [[Credit-based fair queuing]]
* DRR ([[deficit round robin]]) and DWRR, implementation e.g. written by Patrick McHardy for the [[Linux kernel]]&lt;ref&gt;{{cite web |url=https://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git/tree/net/sched/sch_drr.c |title=DRR Linux kernel network scheduler module |publisher=[[kernel.org]] |accessdate=2013-09-07}}&lt;/ref&gt; and published under the [[GNU General Public License]].
* FaQ (FavourQueue)&lt;ref&gt;{{cite web |url=http://oatao.univ-toulouse.fr/10282/1/Lochin_10282.pdf |title=FavorQueue: a Parameterless Active Queue Management to Improve TCP Traffic Performance}}&lt;/ref&gt;
* GCRA ([[generic cell rate algorithm]])
* HFF ([[heavy-hitter filter]])&lt;ref&gt;{{cite web |url=https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=10239edf86f137ce4c39b62ea9575e8053c549a0 |title=Heavy-Hitter Filter qdisc |publisher=[[kernel.org]]}}&lt;/ref&gt;
* HFSC ([[hierarchical fair-service curve]])
* HTB ([[Token Bucket#Hierarchical token bucket|hierarchical token bucket]])&lt;ref&gt;{{cite web |url=https://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git/tree/net/sched/sch_htb.c |title=HTB Linux kernel network scheduler module |publisher=[[kernel.org]] |accessdate=2013-09-07}}&lt;/ref&gt;
* QFQ ([[quick fair queueing]])&lt;ref&gt;{{cite web |url=https://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git/tree/net/sched/sch_qfq.c |title=QFQ Linux kernel network scheduler module |publisher=[[kernel.org]] |accessdate=2013-09-07}}&lt;/ref&gt;
* FQ ([[fair queuing]]) and WFQ ([[weighted fair queuing]])
* FIFO ([[FIFO (computing and electronics)|first in, first out]])
* pkt_sched: fq: fair queue packet scheduler &lt;ref&gt;{{cite web |url=https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=afe4fd062416b158a8a8538b23adc1930a9b88dc |title=Fair Queue packet scheduler committed to Linux kernel 3.12}}&lt;/ref&gt;
* NETEM network emulator&lt;ref&gt;{{cite web |url=https://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git/tree/net/sched/sch_netem.c |title=Network emulator Linux kernel network scheduler module |publisher=[[kernel.org]] |accessdate=2013-09-07}}&lt;/ref&gt;
* PIE ([[proportional integral controller enhanced]])&lt;ref&gt;{{cite web |url=https://git.kernel.org/cgit/linux/kernel/git/torvalds/linux.git/commit/?id=d4b36210c2e6ecef0ce52fb6c18c51144f5c2d88 |title=Proportional Integral controller Enhanced (PIE) |publisher=[[kernel.org]] }}&lt;/ref&gt;
* RED ([[random early detection]])
** ARED ([[random early detection#ARED|advanced random early detection]])
** GRED ([[generalized random early detection]])
** RRED ([[robust random early detection]]) 
** WRED ([[weighted random early detection]])
* RR ([[Round-robin scheduling|round-robin]]) and WRR ([[weighted round robin]])
* SFB ([[Blue (queue management algorithm)|stochastic fair blue]]) as well as RSFB (resilient SFB)
* {{visible anchor|SFQ}} (stochastic fairness queuing)&lt;ref&gt;{{cite web |url=https://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git/tree/net/sched/sch_sfq.c |title=SFQ Linux kernel network scheduler module |publisher=[[kernel.org]] |accessdate=2013-09-07}}&lt;/ref&gt;
* TBF ([[token bucket filter]])&lt;ref&gt;{{cite web |url=https://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git/tree/net/sched/sch_tbf.c |title=TBF Linux kernel network scheduler module |publisher=[[kernel.org]] |accessdate=2013-09-07}}&lt;/ref&gt;
* TEQL ([[trivial link equalizer]])
{{div col end}}

Several of the above have been implemented as [[Linux kernel module]]s&lt;ref&gt;{{cite web|url=https://git.kernel.org/cgit/linux/kernel/git/stable/linux-stable.git/tree/net/sched|title=The Linux kernel network scheduler|publisher=[[kernel.org]]|date=2012-12-26|accessdate=2013-09-07}}&lt;/ref&gt; and are [[free and open-source software|freely available]].

== Bufferbloat ==
[[Bufferbloat]] is a phenomenon in packet-switched networks in which excess [[buffer (telecommunication)|buffer]]ing of packets causes high [[Latency (engineering)|latency]] and [[packet delay variation]]. Bufferbloat can be addressed by a network scheduler that strategically discards packets to avoid an unnecessarily high buffering backlog. Examples include [[CoDel]] and [[Random early detection]].

== Terminology and responsibilities ==
A network scheduler may have responsibility in implementation of specific [[network traffic control]] initiatives. Network traffic control is an umbrella term for all measures aimed at reducing congest, latency and packet loss. Specifically, [[active queue management]] (AQM) is the selective dropping of queued network packets to achieve the larger goal of preventing excessive network congestion. The scheduler must choose which packets to drop. [[Traffic shaping]] smooths the bandwidth requirements of traffic flows by delaying transmission packets when they are queued in bursts. The scheduler decides the timing for the transmitted packets. [[Quality of service]] is the prioritization of traffic based on service class ([[Differentiated services]]) or reserved connection ([[Integrated services]]).

== Implementations ==
{{incomplete section|reason=What about network equipment, non-Unix-like OSs?|date=October 2018}}

=== Linux kernel ===
[[File:Simplified Structure of the Linux Kernel.svg|thumb|The Linux kernel's packet scheduler is part of the network stack, together with [[netfilter]], [[nftables]], and [[Berkeley Packet Filter]].]]

The Linux kernel packet scheduler is an integral part of the Linux kernel's network stack and manages the transmit and receive [[ring buffer]]s of all NICs, by working on the [[data link layer|layer 2]] of the [[OSI model]] and handling [[Ethernet frame]]s, for example. 

The packet scheduler is configured using the utility called &lt;code&gt;[[Tc (Linux)|tc]]&lt;/code&gt; (short for "traffic control"). As the default queuing discipline, the packet scheduler uses a FIFO implementation called ''pfifo_fast'',&lt;ref&gt;{{cite web
 | url = http://www.lartc.org/howto/lartc.qdisc.classless.html
 | title = Linux Advanced Routing and Traffic Control HOWTO, Section 9.2.1. pfifo_fast
 | date = 2012-05-19 | accessdate = 2014-09-19
 | website = lartc.org
}}&lt;/ref&gt; although [[systemd]] since its version 217 changes the default queuing discipline to &lt;tt&gt;fq_codel&lt;/tt&gt;.&lt;ref&gt;{{cite web
 | url = http://cgit.freedesktop.org/systemd/systemd/tree/NEWS
 | title = systemd System and Service Manager: NEWS file
 | date = 2015-05-22 | accessdate = 2015-06-09
 | website = freedesktop.org
}}&lt;/ref&gt; 

The &lt;code&gt;[[ifconfig]]&lt;/code&gt; and &lt;code&gt;[[Iproute2|ip]]&lt;/code&gt; utilities enable system administrators to configure the buffer sizes &lt;code&gt;txqueuelen&lt;/code&gt; and &lt;code&gt;rxqueuelen&lt;/code&gt; for each device separately in terms of number of Ethernet frames regardless of their size. The Linux kernel's network stack contains several other buffers, which are not managed by the network scheduler.{{efn|The overall size of all buffers has been the point of critique by the [[Bufferbloat]] project, which provided a partial solution with CoDel that has been primarily tested in [[OpenWrt]].}}

[[Berkeley Packet Filter]] filters can be attached to the packet scheduler's classifiers. The [[eBPF]] functionality brought by version 4.1 of the Linux kernel in 2015 extends the classic BPF programmable classifiers to eBPF.&lt;ref&gt;{{cite web |url=http://kernelnewbies.org/Linux_4.1#head-c64a7aa1ccb73618b4a7b5f3bef64ff435738098 |title=Linux kernel 4.1, Section 11. Networking |date=2015-06-21 |website = kernelnewbies.org}}&lt;/ref&gt;  These can be compiled using the [[LLVM]] eBPF backend and loaded into a running kernel using the &lt;code&gt;tc&lt;/code&gt; utility.&lt;ref&gt;{{cite web |url=https://cilium.readthedocs.io/en/latest/bpf/ |title=BPF and XDP Reference Guide |website=Cilium documentation web site}}&lt;/ref&gt;&lt;!--[[User:Kvng/RTH]]--&gt;

=== BSD and OpenBSD ===
[[ALTQ]] is the implementation of a network scheduler for [[Berkeley Software Distribution|BSDs]]. As of OpenBSD version 5.5 ALTQ was replaced by the HFSC scheduler. 

== See also ==
{{Portal|Computer networking}}

* [[Network congestion]]
* [[Queue (abstract data type)]]
* [[Queueing theory]]
* [[Statistical time division multiplexing]]
* [[Traffic shaping]]
* [[Traffic classification]]
* [[Type of service]]

== Notes ==
{{Notelist}}

== References ==
{{Reflist}}

{{Computer science}}

[[Category:Linux kernel features]]
[[Category:Network performance]]
[[Category:Network scheduling algorithms]]
[[Category:Network theory]]</text>
      <sha1>2133pi6h9izpq60oqv60vqi1gyfmr8d</sha1>
    </revision>
  </page>
  <page>
    <title>Ordinary differential equation</title>
    <ns>0</ns>
    <id>32742753</id>
    <revision>
      <id>871053563</id>
      <parentid>871053496</parentid>
      <timestamp>2018-11-28T17:01:25Z</timestamp>
      <contributor>
        <username>Dorintosh</username>
        <id>31140333</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/169.149.35.98|169.149.35.98]] ([[User talk:169.149.35.98|talk]]) ([[WP:HG|HG]]) (3.4.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="39461">In [[mathematics]], an '''ordinary differential equation''' ('''ODE''') is a [[differential equation]] containing one or more functions of one [[independent variable]] and its [[derivative]]s. The term ''ordinary'' is used in contrast with the term [[partial differential equation]] which may be with respect to ''more than'' one independent variable.&lt;ref&gt;{{cite web|url=http://hsm.stackexchange.com/a/5032/1772|title=What is the origin of the term "ordinary differential equations"? |accessdate=2016-07-28|website=hsm.stackexchange.com |publisher=[[Stack Exchange]] }}&lt;/ref&gt;

A [[linear differential equation]] is a differential equation that is defined by a [[linear polynomial]] in the unknown function and its derivatives, that is an [[equation]] of the form 
:&lt;math&gt;a_0(x)y +a_1(x)y' + a_2(x)y'' +\cdots +a_n(x)y^{(n)}+b(x)=0,&lt;/math&gt;
where {{tmath|a_0(x)}}, ..., {{tmath|a_n(x)}} and {{tmath|b(x)}} are arbitrary [[differentiable function]]s that do not need to be linear, and  {{tmath|y', \ldots, y^{(n)} }} are the successive derivatives of the unknown function {{mvar|y}} of the variable {{mvar|x}}.

Among ordinary differential equations, linear differential equations play a prominent role for several reasons. Most [[elementary function|elementary]] and [[special functions|special]] functions that are encountered in [[physics]] and [[applied mathematics]] are solutions of linear differential equations (see [[Holonomic function]]). When physical phenomena are modeled with non-linear equations, they are generally approximated by linear differential equations for an easier solution. The few non-linear ODEs that can be solved explicitly are generally solved by transforming the equation into an equivalent linear ODE (see, for example [[Riccati equation]]).

Some ODEs may be solved explicitly in terms of known functions and [[antiderivative|integrals]]. When it is not possible, one may often use the equation for computing the [[Taylor series]] of the solutions. For applied problems, one generally use [[numerical methods for ordinary differential equations]] for getting an approximation of the desired solution.

==Background==
[[Image:Parabolic trajectory.svg|right|thumb|250px|alt=parabolic projectile motion showing velocity vector|The [[trajectory]] of a [[projectile]] launched from a [[cannon]] follows a curve determined by an ordinary differential equation that is derived from Newton's second law.]]

Ordinary differential equations (ODEs) arise in many contexts of mathematics and science ([[social science|social]] as well as [[natural science|natural]]). Mathematical descriptions of change use differentials and derivatives. Various differentials, derivatives, and functions become related to each other via equations, and thus a differential equation is a result that describes dynamically changing phenomena, evolution, and variation. Often, quantities are defined as the rate of change of other quantities (for example, derivatives of displacement with respect to time), or gradients of quantities, which is how they enter differential equations.

Specific mathematical fields include [[geometry]] and [[analytical mechanics]]. Scientific fields include much of [[physics]] and [[astronomy]] (celestial mechanics), [[meteorology]] (weather modelling), [[chemistry]] (reaction rates),&lt;ref&gt;Mathematics for Chemists, D.M. Hirst, [[Macmillan Publishers|Macmillan Press]], 1976, (No ISBN) SBN: 333-18172-7&lt;/ref&gt; [[biology]] (infectious diseases, genetic variation), [[ecology]] and [[population modelling]] (population competition), [[economics]] (stock trends, interest rates and the market equilibrium price changes).

Many mathematicians have studied differential equations and contributed to the field, including [[Isaac Newton|Newton]], [[Gottfried Leibniz|Leibniz]], the [[Bernoulli family]], [[Riccati]],  [[Alexis Claude Clairaut|Clairaut]], [[d'Alembert]], and [[Euler]].

A simple example is [[Newton's second law]] of motion &amp;mdash; the relationship between the displacement ''x'' and the time ''t'' of an object under the force ''F'', is given by the differential equation

:&lt;math&gt;m \frac{\mathrm{d}^2 x(t)}{\mathrm{d}t^2} = F(x(t))\,&lt;/math&gt;

which constrains the motion of a particle of constant mass ''m''. In general, ''F'' is a function of the position ''x''(''t'') of the particle at time ''t''. The unknown function ''x''(''t'') appears on both sides of the differential equation, and is indicated in the notation ''F''(''x''(''t'')).&lt;ref&gt;{{harvtxt|Kreyszig|1972|p=64}}&lt;/ref&gt;&lt;ref&gt;{{harvtxt|Simmons|1972|pp=1,2}}&lt;/ref&gt;&lt;ref&gt;{{harvtxt|Halliday|Resnick|1977|p=78}}&lt;/ref&gt;&lt;ref&gt;{{harvtxt|Tipler|1991|pp=78–83}}&lt;/ref&gt;

==Definitions==
In what follows, let ''y'' be a [[Dependent and independent variables|dependent variable]] and ''x'' an [[Dependent and independent variables|independent variable]], and ''y'' = ''f''(''x'') is an unknown function of ''x''. The [[notation for differentiation]] varies depending upon the author and upon which notation is most useful for the task at hand. In this context, the [[notation for differentiation#Leibniz's notation|Leibniz's notation]] (''dy''/''dx'',''d''&lt;sup&gt;2&lt;/sup&gt;''y''/''dx''&lt;sup&gt;2&lt;/sup&gt;,...,''d''&lt;sup&gt;''n''&lt;/sup&gt;''y''/''dx''&lt;sup&gt;''n''&lt;/sup&gt;) is more useful for differentiation and [[Integration (mathematics)|integration]], whereas [[notation for differentiation#Lagrange's notation|Lagrange's notation]] (''y&amp;prime;'',''y&amp;prime;&amp;prime;'', ..., ''y''&lt;sup&gt;(''n'')&lt;/sup&gt;) is more useful for representing derivatives of any order compactly, and [[notation for differentiation#Newton's notation|Newton's notation]] &lt;math&gt;(\dot y, \ddot y, \overset{...}{y})&lt;/math&gt; is often used in physics for representing derivatives of low order with respect to time.

===General definition===
Given ''F'', a function of ''x'', ''y'', and derivatives of ''y''. Then an equation of the form

:&lt;math&gt;F\left (x,y,y',\ldots, y^{(n-1)} \right )=y^{(n)}&lt;/math&gt;

is called an [[Implicit and explicit functions|explicit]] ''ordinary differential equation'' of ''order'' ''n''.&lt;ref name="Harper 1976 127"&gt;{{harvtxt|Harper|1976|p=127}}&lt;/ref&gt;&lt;ref&gt;{{harvtxt|Kreyszig|1972|p=2}}&lt;/ref&gt;

More generally, an ''[[Implicit and explicit functions|implicit]]'' ordinary differential equation of order ''n'' takes the form:&lt;ref&gt;{{harvtxt|Simmons|1972|p=3}}&lt;/ref&gt;

:&lt;math&gt;F\left(x, y, y', y'',\ \ldots,\ y^{(n)}\right) = 0&lt;/math&gt;

There are further classifications:
{{glossary}}
{{term|[[Autonomous system (mathematics)|Autonomous]]}}{{defn|A differential equation not depending on ''x'' is called ''[[Autonomous system (mathematics)|autonomous]]''.}}
{{term|[[Linear differential equation|Linear]]}}{{defn|
A differential equation is said to be ''linear'' if ''F'' can be written as a [[linear combination]] of the derivatives of ''y'':
:&lt;math&gt;y^{(n)} = \sum_{i=0}^{n - 1} a_i(x) y^{(i)} + r(x)&lt;/math&gt;

where ''a''&lt;sub&gt;''i''&lt;/sub&gt;(''x'') and ''r''(''x'') are continuous functions in ''x''.&lt;ref name="Harper 1976 127"/&gt;&lt;ref name="Kreyszig 1972 24"&gt;{{harvtxt|Kreyszig|1972|p=24}}&lt;/ref&gt;&lt;ref&gt;{{harvtxt|Simmons|1972|p=47}}&lt;/ref&gt;

The function ''r''(''x'') is called the ''source term'', leading to two further important classifications:&lt;ref name="Kreyszig 1972 24"/&gt;&lt;ref&gt;{{harvtxt|Harper|1976|p=128}}&lt;/ref&gt;}}
{{glossary}}
{{term|Homogeneous}}{{defn|If ''r''(''x'') {{=}} 0, and consequently one "automatic" solution is the [[trivial solution]], ''y'' {{=}} 0. The solution of a linear homogeneous equation is a '''complementary function''', denoted here by ''y&lt;sub&gt;c&lt;/sub&gt;''.
{{term|Nonhomogeneous (or inhomogeneous)}}{{defn|If ''r''(''x'') ≠ 0. The additional solution to the complementary function is the '''particular integral''', denoted here by ''y&lt;sub&gt;p&lt;/sub&gt;''.}}
{{glossary end}}
The general solution to a linear equation can be written as ''y'' {{=}} ''y&lt;sub&gt;c&lt;/sub&gt;'' + ''y&lt;sub&gt;p&lt;/sub&gt;''.
}}
{{term|[[Non-linear differential equation|Non-linear]]}}{{defn|A differential equation that cannot be written in the form of a linear combination.}}
{{glossary end}}

===System of ODEs===
A number of coupled differential equations form a system of equations. If '''y''' is a vector whose elements are functions; '''y'''(''x'') = [''y''&lt;sub&gt;1&lt;/sub&gt;(''x''), ''y''&lt;sub&gt;2&lt;/sub&gt;(''x''),..., ''y&lt;sub&gt;m&lt;/sub&gt;''(''x'')], and '''F''' is a [[vector-valued function]] of '''y''' and its derivatives, then

:&lt;math&gt;\mathbf{y}^{(n)} = \mathbf{F}\left(x,\mathbf{y},\mathbf{y}',\mathbf{y}'',\ldots, \mathbf{y}^{(n-1)} \right)&lt;/math&gt;

is an ''explicit system of ordinary differential equations'' of ''order'' ''n'' and ''dimension'' ''m''. In [[column vector]] form:

:&lt;math&gt;\begin{pmatrix}
y_1^{(n)} \\
y_2^{(n)} \\
\vdots \\
y_m^{(n)}
\end{pmatrix} =

\begin{pmatrix}
f_1 \left (x,\mathbf{y},\mathbf{y}',\mathbf{y}'',\ldots, \mathbf{y}^{(n-1)} \right ) \\
f_2 \left (x,\mathbf{y},\mathbf{y}',\mathbf{y}'',\ldots, \mathbf{y}^{(n-1)} \right ) \\
\vdots \\
f_m \left (x,\mathbf{y},\mathbf{y}',\mathbf{y}'',\ldots, \mathbf{y}^{(n-1)} \right)
\end{pmatrix}&lt;/math&gt;

These are not necessarily linear. The ''implicit'' analogue is:

:&lt;math&gt;\mathbf{F} \left(x,\mathbf{y},\mathbf{y}',\mathbf{y}'',\ldots, \mathbf{y}^{(n)} \right) = \boldsymbol{0}&lt;/math&gt;

where '''0''' = (0, 0,... 0) is the [[zero vector]]. In matrix form

:&lt;math&gt;\begin{pmatrix}
f_1(x,\mathbf{y},\mathbf{y}',\mathbf{y}'',\ldots, \mathbf{y}^{(n)}) \\
f_2(x,\mathbf{y},\mathbf{y}',\mathbf{y}'',\ldots, \mathbf{y}^{(n)}) \\
\vdots \\
f_m(x,\mathbf{y},\mathbf{y}',\mathbf{y}'',\ldots, \mathbf{y}^{(n)})
\end{pmatrix}=\begin{pmatrix}
0\\
0\\
\vdots\\
0
\end{pmatrix}&lt;/math&gt;

For a system of the form &lt;math&gt;\mathbf{F} \left(x,\mathbf{y},\mathbf{y}'\right) = \boldsymbol{0}&lt;/math&gt;, some sources also require that the [[Jacobian matrix]] &lt;math&gt;\frac{\partial\mathbf{F}(x,\mathbf{u},\mathbf{v})}{\partial \mathbf{v}}&lt;/math&gt; be [[singular matrix|non-singular]] in order to call this an implicit ODE [system]; an implicit ODE system satisfying this Jacobian non-singularity condition can be transformed into an explicit ODE system. In the same sources, implicit ODE systems with a singular Jacobian are termed [[differential algebraic equation]]s (DAEs). This distinction is not merely one of terminology; DAEs have fundamentally different characteristics and are generally more involved to solve than (nonsingular) ODE systems.&lt;ref name="AscherPetzold1998"&gt;{{cite book|author1=Uri M. Ascher|author2=Linda R. Petzold|title=Computer Methods for Ordinary Differential Equations and Differential-Algebraic Equations|year=1998|publisher=SIAM|isbn=978-1-61197-139-2|page=12}}&lt;/ref&gt;&lt;ref name="IlchmannReis2014"&gt;{{cite book|author1=Achim Ilchmann|author2=Timo Reis|title=Surveys in Differential-Algebraic Equations II|year=2014|publisher=Springer|isbn=978-3-319-11050-9|pages=104–105}}&lt;/ref&gt; Presumably for additional derivatives, the [[Hessian matrix]] and so forth are also assumed non-singular according to this scheme, {{citation needed|date=December 2014}} although note that [[#Reduction to a first-order system|any ODE of order greater than one can be [and usually is] rewritten as system of ODEs of first order]],&lt;ref name="AscherPetzold1998b"&gt;{{cite book|author1=Uri M. Ascher|author2=Linda R. Petzold|title=Computer Methods for Ordinary Differential Equations and Differential-Algebraic Equations|year=1998|publisher=SIAM|isbn=978-1-61197-139-2|page=5}}&lt;/ref&gt; which makes the Jacobian singularity criterion sufficient for this taxonomy to be comprehensive at all orders.

The behavior of a system of ODEs can be visualized through the use of a [[phase portrait]].

=== Solutions ===
Given a differential equation
:&lt;math&gt;F\left(x, y, y', \ldots, y^{(n)} \right) = 0&lt;/math&gt;
a function {{nowrap|''u'': ''I'' ⊂ '''R''' → '''R'''}} is called a ''solution'' or [[integral curve]] for ''F'', if ''u'' is ''n''-times differentiable on ''I'', and
:&lt;math&gt;F(x,u,u',\ \ldots,\ u^{(n)})=0 \quad x \in I.&lt;/math&gt;

Given two solutions {{nowrap|''u'': ''J'' ⊂ '''R''' → '''R'''}} and {{nowrap|''v'': ''I'' ⊂ '''R''' → '''R'''}}, ''u'' is called an ''extension'' of ''v'' if {{nowrap|''I'' ⊂ ''J''}} and
:&lt;math&gt;u(x) = v(x) \quad x \in I.\,&lt;/math&gt;
A solution that has no extension is called a ''maximal solution''. A solution defined on all of '''R''' is called a ''global solution''.

A ''general solution'' of an ''n''th-order equation is a solution containing ''n'' arbitrary independent [[constant of integration|constants of integration]].  A ''particular solution'' is derived from the general solution by setting the constants to particular values, often chosen to fulfill set '[[initial value problem|initial conditions]] or [[boundary value problem|boundary conditions]]'.&lt;ref&gt;{{harvtxt|Kreyszig|1972|p=78}}&lt;/ref&gt;  A [[singular solution]] is a solution that cannot be obtained by assigning definite values to the arbitrary constants in the general solution.&lt;ref&gt;{{harvtxt|Kreyszig|1972|p=4}}&lt;/ref&gt;

==Theories==

===Singular solutions===
The theory of [[singular solution]]s of ordinary and [[partial differential equation]]s was a subject of research from the time of Leibniz, but only since the middle of the nineteenth century did it receive special attention. A valuable but little-known work on the subject is that of Houtain (1854). [[Jean Gaston Darboux|Darboux]] (starting in 1873) was a leader in the theory, and in the geometric interpretation of these solutions he opened a field worked by various
writers, notable ones being [[Felice Casorati (mathematician)|Casorati]] and [[Arthur Cayley|Cayley]]. To the latter is due (1872) the theory of singular solutions of differential equations of the first order as accepted circa 1900.

===Reduction to quadratures===
The primitive attempt in dealing with differential equations had in view a reduction to [[quadrature (mathematics)|quadrature]]s. As it had been the hope of eighteenth-century algebraists to find a method for solving the general equation of the ''n''th degree, so it was the hope of analysts to find a general method for integrating any differential equation. [[Carl Friedrich Gauss|Gauss]] (1799) showed, however, that the differential equation meets its limitations very soon unless [[complex number]]s are introduced. Hence, analysts began to substitute the study of functions, thus opening a new and fertile field. [[Cauchy]] was the first to appreciate the importance of this view.  Thereafter, the real question was to be not whether a solution is possible by means of known functions or their integrals but whether a given differential equation suffices for the definition of a function of the independent variable or variables, and, if so, what are the characteristic properties of this function.

===Fuchsian theory===
{{main article|Frobenius method}}
Two memoirs by [[Lazarus Fuchs|Fuchs]] (''Crelle'', 1866, 1868), inspired a novel approach, subsequently elaborated by Thomé and [[Ferdinand Georg Frobenius|Frobenius]]. Collet was a prominent contributor beginning in 1869, although his method for integrating a non-linear system was communicated to Bertrand in 1868. [[Alfred Clebsch|Clebsch]] (1873) attacked the theory along lines parallel to those followed in his theory of [[Abelian integral]]s. As the latter can be classified according to the properties of the fundamental curve that remains unchanged under a rational transformation, so Clebsch proposed to classify the transcendent functions defined by the differential equations according to the invariant properties of the corresponding surfaces ''f'' = 0 under rational one-to-one transformations.

===Lie's theory===
From 1870, [[Sophus Lie]]'s work put the theory of differential equations on a more satisfactory foundation. He showed that the integration theories of the older mathematicians can, by the introduction of what are now called [[Lie group]]s, be referred to a common source, and that ordinary differential equations that admit the same [[infinitesimal transformation]]s present comparable difficulties of integration. He also emphasized the subject of [[contact transformation|transformations of contact]].

Lie's group theory of differential equations has been certified, namely: (1) that it unifies the many ad hoc methods known for solving differential equations, and (2) that it provides powerful new ways to find solutions. The theory has applications to both ordinary and partial differential equations.&lt;ref&gt;{{harvtxt|Lawrence|1999|p=9}}&lt;/ref&gt;

A general approach to solve DEs uses the symmetry property of differential equations, the continuous [[infinitesimal transformation]]s of solutions to solutions ([[Lie theory]]). Continuous [[group theory]], [[Lie algebras]], and [[differential geometry]] are used to understand the structure of linear and nonlinear (partial) differential equations for generating integrable equations, to find its [[Lax pair]]s, recursion operators, [[Bäcklund transform]], and finally finding exact analytic solutions to the DE.

Symmetry methods have been recognized to study differential equations, arising in mathematics, physics, engineering, and many other disciplines.

=== Sturm–Liouville theory ===
{{main article|Sturm–Liouville theory}}
Sturm–Liouville theory is a theory of a special type of second order linear ordinary differential equations. Their solutions are based on eigenvalues and corresponding eigenfunctions of linear operators defined in terms of second-order homogeneous linear equations. The problems are identified as Sturm-Liouville Problems (SLP) and are named after [[Jacques Charles François Sturm|J.C.F. Sturm]] and [[Joseph Liouville|J. Liouville]], who studied such problems in the mid-1800s. The interesting fact about regular SLPs is that they have an infinite number of eigenvalues, and the corresponding eigenfunctions form a complete, orthogonal set, which makes orthogonal expansions possible. This is a key idea in applied mathematics, physics, and engineering.&lt;ref&gt;Logan, J. (2013). Applied mathematics (Fourth ed.).&lt;/ref&gt; SLPs are also useful in the analysis of certain partial differential equations.

==Existence and uniqueness of solutions==

There are several theorems that establish existence and uniqueness of solutions to initial value problems involving ODEs both locally and globally. The two main theorems are

:{| class="wikitable"
|-
! Theorem
! Assumption
! Conclusion
|-
|[[Peano existence theorem]]
||F [[continuous function|continuous]]
||local existence only
|-
|[[Picard–Lindelöf theorem]]
||F [[Lipschitz continuous]]
||local existence and uniqueness
|-
|}

In their basic form both of these theorems only guarantee local results, though the latter can be extended to give a global result, for example, if the conditions of [[Grönwall's inequality]] are met.

Also, uniqueness theorems like the Lipschitz one above do not apply to [[Differential algebraic equation|DAE]] systems, which may have multiple solutions stemming from their (non-linear) algebraic part alone.&lt;ref name="AscherPetzold1998c"&gt;{{cite book|author1=Uri M. Ascher|author2=Linda R. Petzold|title=Computer Methods for Ordinary Differential Equations and Differential-Algebraic Equations|year=1998|publisher=SIAM|isbn=978-1-61197-139-2|page=13}}&lt;/ref&gt;

===Local existence and uniqueness theorem simplified===

The theorem can be stated simply as follows.&lt;ref name= "EDEBVP" &gt;Elementary Differential Equations and Boundary Value Problems (4th Edition), W.E. Boyce, R.C. Diprima, Wiley International, John Wiley &amp; Sons, 1986, {{isbn|0-471-83824-1}}&lt;/ref&gt; For the equation and initial value problem:

:&lt;math&gt; y' = F(x,y)\,,\quad y_0 = y(x_0)&lt;/math&gt;

if ''F'' and ∂''F''/∂''y'' are continuous in a closed rectangle

:&lt;math&gt;R=[x_0-a,x_0+a]\times [y_0-b,y_0+b]&lt;/math&gt;

in the ''x-y'' plane, where ''a'' and ''b'' are [[real number|real]] (symbolically: ''a, b'' ∈ ℝ) and × denotes the [[cartesian product]], square brackets denote [[interval notation|closed intervals]], then there is an interval

:&lt;math&gt;I = [x_0-h,x_0+h] \subset [x_0-a,x_0+a]&lt;/math&gt;

for some ''h'' ∈ ℝ where ''the'' solution to the above equation and initial value problem can be found. That is, there is a solution and it is unique. Since there is no restriction on ''F'' to be linear, this applies to non-linear equations that take the form ''F''(''x, y''), and it can also be applied to systems of equations.

===Global uniqueness and maximum domain of solution===
When the hypotheses of the Picard–Lindelöf theorem are satisfied, then local existence and uniqueness can be extended to a global result. More precisely:&lt;ref&gt;Boscain; Chitour 2011, p.&amp;nbsp;21&lt;/ref&gt;

For each initial condition (''x''&lt;sub&gt;0&lt;/sub&gt;, ''y''&lt;sub&gt;0&lt;/sub&gt;) there exists a unique maximum (possibly infinite) open interval

:&lt;math&gt;I_{\max} = (x_-,x_+), x_\pm \in \R, x_0 \in I_{\max}&lt;/math&gt;

such that any solution that satisfies this initial condition is a [[Restriction (mathematics)|restriction]] of the solution that satisfies this initial condition with domain &lt;math&gt;I_\max&lt;/math&gt;.

In the case that &lt;math&gt;x_\pm \nrightarrow \pm\infty&lt;/math&gt;, there are exactly two possibilities

*explosion in finite time: &lt;math&gt;\limsup_{x \to x_\pm} \|y(x)\| \to \infty&lt;/math&gt;
*leaves domain of definition: &lt;math&gt;\lim_{x \to x_\pm} y(x)\ \in \partial \bar{\Omega}&lt;/math&gt;

where Ω is the open set in which ''F'' is defined, and &lt;math&gt;\partial \bar{\Omega}&lt;/math&gt; is its boundary.

Note that the maximum domain of the solution

* is always an interval (to have uniqueness)
* may be smaller than &lt;math&gt;\R&lt;/math&gt;
* may depend on the specific choice of (''x''&lt;sub&gt;0&lt;/sub&gt;, ''y''&lt;sub&gt;0&lt;/sub&gt;).

;Example.

:&lt;math&gt;y' = y^2&lt;/math&gt;

This means that ''F''(''x, y'') = ''y''&lt;sup&gt;2&lt;/sup&gt;, which is ''C''&lt;sup&gt;1&lt;/sup&gt; and therefore locally Lipschitz continuous, satisfying the Picard–Lindelöf theorem.

Even in such a simple setting, the maximum domain of solution cannot be all &lt;math&gt;\R&lt;/math&gt; since the solution is

:&lt;math&gt;y(x) = \frac{y_0}{(x_0-x)y_0+1}&lt;/math&gt;

which has maximum domain:

:&lt;math&gt;\begin{cases}\R &amp; y_0 = 0 \\[4pt] \left (-\infty, x_0+\frac{1}{y_0} \right ) &amp; y_0 &gt; 0 \\[4pt] \left (x_0+\frac{1}{y_0},+\infty \right ) &amp; y_0 &lt; 0 \end{cases}&lt;/math&gt;

This shows clearly that the maximum interval may depend on the initial conditions. The domain of ''y'' could be taken as being &lt;math&gt;\R \setminus (x_0+ 1/y_0),&lt;/math&gt; but this would lead to a domain that is not an interval, so that the side opposite to the initial condition would be disconnected from the initial condition, and therefore not uniquely determined by it.

The maximum domain is not &lt;math&gt;\R&lt;/math&gt; because

:&lt;math&gt;\lim_{x \to x_\pm} \|y(x)\| \to \infty,&lt;/math&gt;

which is one of the two possible cases according to the above theorem.

== Reduction of order ==
Differential equations can usually be solved more easily if the order of the equation can be reduced.

=== Reduction to a first-order system === &lt;!-- Redirect [[Stiff equation]] links here --&gt;
Any explicit differential equation of order ''n'',

:&lt;math&gt;F\left(x, y, y', y'',\ \ldots,\ y^{(n-1)}\right) = y^{(n)}&lt;/math&gt;

can be written as a system of ''n'' first-order differential equations by defining a new family of unknown functions

:&lt;math&gt;y_i = y^{(i-1)}.\!&lt;/math&gt;

for ''i'' = 1, 2,..., ''n''. The ''n''-dimensional system of first-order coupled differential equations is then

:&lt;math&gt;\begin{array}{rcl}
  y_1'&amp;=&amp;y_2\\
  y_2'&amp;=&amp;y_3\\
  &amp;\vdots&amp;\\
  y_{n-1}'&amp;=&amp;y_n\\
  y_n'&amp;=&amp;F(x,y_1,\ldots,y_n).
\end{array}
&lt;/math&gt;

more compactly in vector notation:

:&lt;math&gt;\mathbf{y}'=\mathbf{F}(x,\mathbf{y})&lt;/math&gt;

where
:&lt;math&gt;\mathbf{y}=(y_1,\ldots,y_n),\quad \mathbf{F}(x,y_1,\ldots,y_n)=(y_2,\ldots,y_n,F(x,y_1,\ldots,y_n)).&lt;/math&gt;

==Summary of exact solutions==
&lt;!--Please leave the table alone. If a table format is really disliked then instead of simply deleting it - expand into prose. For now it is to summarize some common forms of equations and their solutions. Details are in the other articles. --&gt;

Some differential equations have solutions that can be written in an exact and closed form. Several important classes are given here.

In the table below, ''P''(''x''), ''Q''(''x''), ''P''(''y''), ''Q''(''y''), and ''M''(''x'',''y''), ''N''(''x'',''y'') are any [[integrable]] functions of ''x'', ''y'', and ''b'' and ''c'' are real given constants, and ''C''&lt;sub&gt;1&lt;/sub&gt;, ''C''&lt;sub&gt;2&lt;/sub&gt;,... are arbitrary constants ([[complex number|complex]] in general). The differential equations are in their equivalent and alternative forms that lead to the solution through integration.

In the integral solutions, λ and ε are dummy variables of integration (the continuum analogues of indices in [[summation]]), and the notation ∫&lt;sup&gt;''x''&lt;/sup&gt;''F''(''λ'')&amp;nbsp;''dλ'' just means to integrate ''F''(''λ'') with respect to ''λ'', then ''after'' the integration substitute ''λ'' = ''x'', without adding constants (explicitly stated).

:{| class="wikitable"
|-
! scope="col" | Type
! scope="col" | Differential equation
! scope="col" | Solution method
! scope="col" | General solution
|-
| rowspan="4" |Separable
| First-order, separable in ''x'' and ''y'' (general case, see below for special cases)&lt;ref name="MHFT"&gt;Mathematical Handbook of Formulas and Tables (3rd edition), S. Lipschutz, M. R. Spiegel, J. Liu, Schuam's Outline Series, 2009, ISC_2N 978-0-07-154855-7&lt;/ref&gt;

&lt;math&gt;\begin{align}
  P_1(x)Q_1(y) + P_2(x)Q_2(y)\,\frac{dy}{dx} &amp;= 0 \\
         P_1(x)Q_1(y)\,dx + P_2(x)Q_2(y)\,dy &amp;= 0
\end{align}&lt;/math&gt;
| Separation of variables (divide by ''P''&lt;sub&gt;2&lt;/sub&gt;''Q''&lt;sub&gt;1&lt;/sub&gt;).
| &lt;math&gt; \int^x \frac{P_1(\lambda)}{P_2(\lambda)}\,d\lambda + \int^y \frac{Q_2(\lambda)}{Q_1(\lambda)}\,d\lambda = C \,\!&lt;/math&gt;
|-
| First-order, separable in ''x''&lt;ref name= "EDEBVP" /&gt;
&lt;math&gt;\begin{align}
  \frac{dy}{dx} &amp;= F(x) \\
             dy &amp;= F(x) \, dx
\end{align}&lt;/math&gt;
| Direct integration.
| &lt;math&gt;y= \int^x F(\lambda) \, d\lambda + C \,\!&lt;/math&gt;
|-
| First-order, autonomous, separable in ''y''&lt;ref name= "EDEBVP" /&gt;

&lt;math&gt;\begin{align}
  \frac{dy}{dx} &amp;= F(y) \\
             dy &amp;= F(y) \, dx
\end{align}&lt;/math&gt;
| [[Separation of variables]] (divide by ''F'').
| &lt;math&gt;x=\int^y \frac{d\lambda}{F(\lambda)}+C\,\!&lt;/math&gt;
|-
| First-order, separable in ''x'' and ''y''&lt;ref name= "EDEBVP" /&gt;

&lt;math&gt;\begin{align}
  P(y)\frac{dy}{dx} + Q(x) &amp;= 0 \\
       P(y)\,dy + Q(x)\,dx &amp;= 0
\end{align}&lt;/math&gt;
| Integrate throughout.
| &lt;math&gt;\int^y P(\lambda)\, d\lambda + \int^x Q(\lambda)\,d\lambda = C\,\!&lt;/math&gt;
|-
| rowspan="4" |General first-order
| First-order, homogeneous&lt;ref name="EDEBVP" /&gt;

&lt;math&gt;\frac{dy}{dx} = F \left( \frac y x \right ) \,\!&lt;/math&gt;
| Set ''y = ux'', then solve by separation of variables in ''u'' and ''x''.
| &lt;math&gt; \ln (Cx) = \int^{y/x} \frac{d\lambda}{F(\lambda) - \lambda} \, \! &lt;/math&gt;
|-
| First-order, separable&lt;ref name= "MHFT" /&gt;

&lt;math&gt;\begin{align}
  yM(xy) + xN(xy)\,\frac{dy}{dx} &amp;= 0 \\
         yM(xy)\,dx + xN(xy)\,dy &amp;= 0
\end{align}&lt;/math&gt;

| Separation of variables (divide by ''xy'').
|
&lt;math&gt; \ln (Cx) = \int^{xy} \frac{N(\lambda)\,d\lambda}{\lambda [N(\lambda)-M(\lambda)] } \,\!&lt;/math&gt;

If ''N'' = ''M'', the solution is ''xy'' = ''C''.
|-
| [[Exact differential equation|Exact differential]], first-order&lt;ref name= "EDEBVP" /&gt;

&lt;math&gt;\begin{align}
  M(x,y) \frac{dy}{dx} + N(x,y) &amp;= 0 \\
        M(x,y)\,dy + N(x,y)\,dx &amp;= 0
\end{align}&lt;/math&gt;

where &lt;math&gt; \frac{\partial M}{\partial x} = \frac{\partial N}{\partial y} \, \! &lt;/math&gt;

| Integrate throughout.
| &lt;math&gt;\begin{align}
  F(x,y) = &amp;\int^y M(x,\lambda)\,d\lambda + \int^x N(\lambda,y)\,d\lambda \\
           &amp;+ Y(y) + X(x) = C 
\end{align}&lt;/math&gt;

where ''Y''(''y'') and ''X''(''x'') are functions from the integrals rather than constant values, which are set to make the final function ''F''(''x, y'') satisfy the initial equation.
|-
| [[Inexact differential equation|Inexact differential]], first-order&lt;ref name= "EDEBVP" /&gt;

&lt;math&gt;\begin{align}
  M(x,y) \frac{dy}{dx} + N(x,y) &amp;= 0 \\
        M(x,y)\,dy + N(x,y)\,dx &amp;= 0
\end{align}&lt;/math&gt;

where &lt;math&gt; \frac{\partial M}{\partial x} \neq \frac{\partial N}{\partial y} \, \! &lt;/math&gt;
| [[Integration factor]] μ(''x, y'') satisfying

&lt;math&gt; \frac{\partial (\mu M)}{\partial x} = \frac{\partial (\mu N)}{\partial y} \, \! &lt;/math&gt;
| If ''μ''(''x'', ''y'') can be found:

&lt;math&gt;\begin{align}
  F(x,y) = &amp;\int^y \mu(x,\lambda)M(x,\lambda)\,d\lambda + \int^x \mu(\lambda,y)N(\lambda,y)\,d\lambda \\
           &amp;+ Y(y) + X(x) = C
\end{align}&lt;/math&gt;
|-
|General second-order
| Second-order, autonomous&lt;ref&gt;Further Elementary Analysis, R. Porter, G.Bell &amp; Sons (London), 1978, {{isbn|0-7135-1594-5}}&lt;/ref&gt;

&lt;math&gt;\frac{d^2y}{dx^2} = F(y) \,\!&lt;/math&gt;
| Multiply both sides of equation by 2''dy''/''dx'', substitute &lt;math&gt;2 \frac{dy}{dx}\frac{d^2y}{dx^2} = \frac{d}{dx}\left(\frac{dy}{dx}\right)^2 \,\!&lt;/math&gt;, then integrate twice.
| &lt;math&gt; x = \pm \int^y \frac{ d \lambda}{\sqrt{2 \int^\lambda F(\varepsilon) \, d \varepsilon + C_1}} + C_2 \, \! &lt;/math&gt;
|-
| rowspan="3" |Linear to ''n''th order
| First-order, linear, inhomogeneous, function coefficients&lt;ref name="EDEBVP" /&gt;

&lt;math&gt;\frac{dy}{dx} + P(x)y = Q(x)\,\!&lt;/math&gt;

| Integrating factor: &lt;math&gt;e^{\int^x P(\lambda)\,d\lambda}.&lt;/math&gt;
| &lt;math&gt;y = e^{- \int^x P(\lambda) \, d\lambda}\left[\int^x e^{\int^\lambda P(\varepsilon) \, d\varepsilon}Q(\lambda) \, d\lambda +C \right]&lt;/math&gt;
|-
| Second-order, linear, inhomogeneous, constant coefficients&lt;ref name= "MMPE" &gt;Mathematical methods for physics and engineering, K.F. Riley, M.P. Hobson, S.J. Bence, Cambridge University Press, 2010, ISC_2N 978-0-521-86153-3&lt;/ref&gt;

&lt;math&gt;\frac{d^2y}{dx^2} + b\frac{dy}{dx} + cy = r(x)\,\!&lt;/math&gt;
| Complementary function ''y&lt;sub&gt;c&lt;/sub&gt;'': assume ''y&lt;sub&gt;c&lt;/sub&gt;'' = ''e''&lt;sup&gt;α''x''&lt;/sup&gt;, substitute and solve polynomial in α, to find the [[linearly independent]] functions &lt;math&gt;e^{\alpha_j x}&lt;/math&gt;.

Particular integral ''y&lt;sub&gt;p&lt;/sub&gt;'': in general the [[method of variation of parameters]], though for very simple ''r''(''x'') inspection may work.&lt;ref name= "EDEBVP" /&gt;
| &lt;math&gt;y = y_c + y_p&lt;/math&gt;

If {{math|''b''&lt;sup&gt;2&lt;/sup&gt; &gt; 4''c''}}, then
:&lt;math&gt;y_c = C_1e^{ -\frac x2\,\left(b + \sqrt{b^2 - 4c}\right)} + C_2e^{-\frac x2\,\left(b - \sqrt{b^2 - 4c}\right)}&lt;/math&gt;

If {{math|1=''b''&lt;sup&gt;2&lt;/sup&gt; = 4''c''}}, then

: &lt;math&gt;y_c = (C_1x + C_2)e^{-\frac{bx}{2}}&lt;/math&gt;

If {{math|''b''&lt;sup&gt;2&lt;/sup&gt; &lt; 4''c''}}, then

: &lt;math&gt;y_c = e^{ -\frac{bx}{2}} \left[ C_1 \sin\left( x\,\frac{\sqrt{4c-b^2}}{2}\right) + C_2\cos\left( x\,\frac{\sqrt{4c-b^2}}{2}\right)\right]&lt;/math&gt;
|-
| ''n''th-order, linear, inhomogeneous, constant coefficients&lt;ref name= "MMPE" /&gt;

&lt;math&gt; \sum_{j=0}^n b_j \frac{d^j y}{dx^j} = r(x)\,\!&lt;/math&gt;
| Complementary function ''y&lt;sub&gt;c&lt;/sub&gt;'': assume ''y&lt;sub&gt;c&lt;/sub&gt;'' = ''e''&lt;sup&gt;α''x''&lt;/sup&gt;, substitute and solve polynomial in α, to find the [[linearly independent]] functions &lt;math&gt;e^{\alpha_j x}&lt;/math&gt;.

Particular integral ''y&lt;sub&gt;p&lt;/sub&gt;'': in general the [[method of variation of parameters]], though for very simple ''r''(''x'') inspection may work.&lt;ref name= "EDEBVP" /&gt;
| &lt;math&gt;y = y_c + y_p&lt;/math&gt;

Since α&lt;sub&gt;''j''&lt;/sub&gt; are the solutions of the [[polynomial]] of [[Degree of a polynomial|degree]] ''n'': &lt;math&gt; \prod_{j=1}^n ( \alpha - \alpha_j) = 0 \,\!&lt;/math&gt;, then:

for ''α''&lt;sub&gt;''j''&lt;/sub&gt; all different,

: &lt;math&gt; y_c = \sum_{j=1}^n C_j e^{\alpha_j x} \,\!&lt;/math&gt;

for each root α&lt;sub&gt;''j''&lt;/sub&gt; repeated ''k&lt;sub&gt;j&lt;/sub&gt;'' times,

: &lt;math&gt; y_c = \sum_{j=1}^n \left( \sum_{\ell=1}^{k_j} C_{j,\ell} x^{\ell-1}\right )e^{\alpha_j x} \,\!&lt;/math&gt;

for some α&lt;sub&gt;''j''&lt;/sub&gt; complex, then setting α = χ&lt;sub&gt;''j''&lt;/sub&gt; + ''iγ''&lt;sub&gt;''j''&lt;/sub&gt;, and using [[Euler's formula]], allows some terms in the previous results to be written in the form

:&lt;math&gt; C_je^{\alpha_j x} = C_j e^{\chi_j x}\cos(\gamma_j x + \varphi_j)\,\!&lt;/math&gt;

where ''ϕ''&lt;sub&gt;''j''&lt;/sub&gt; is an arbitrary constant (phase shift).
|}

== Software for ODE solving ==
* [[Maxima (software)|Maxima]], an open-source [[computer algebra system]].
* [[COPASI]], a free ([[Artistic license|Artistic License 2.0]]) software package for the integration and analysis of ODEs.
* [[MATLAB]], a technical computing application (MATrix LABoratory)
* [[GNU Octave]], a high-level language, primarily intended for numerical computations.
* [[Scilab]], an open source application for numerical computation.
* [[Maple (software)|Maple]], a proprietary application for symbolic calculations. 
* [[Mathematica]], a proprietary application primarily intended for symbolic calculations. 
* [[Julia (programming language)]], a high-level, multi-paradigm, open-source, dynamic programming language primarily intended for numerical computations, although it is flexible enough for general-purpose programming.
* [[SageMath]], an open-source application that uses a Python-like syntax with a wide range of capabilities spanning several branches of mathematics.
* [[SciPy]], a Python package that includes an ODE integration module.
* [[Chebfun]], an open-source package, written in [[MATLAB]], for computing with functions to 15-digit accuracy.
* [[GNU R]], an open source computational environment primarily intended for statistics, which includes package for ODE solving.
* [http://erodes.net/ EROS.NET] a free ODE solver for .NET.

==See also==
*[[Examples of differential equations]]
*[[Boundary value problem]]
*[[Laplace transform applied to differential equations]]
*[[List of dynamical systems and differential equations topics]]
*[[Matrix differential equation]]
*[[Method of undetermined coefficients]]
*[[Numerical methods for ordinary differential equations]]
*[[Recurrence relation]]
*[[Separation of variables]]

== Notes ==
{{reflist|2}}

==References==
* {{citation | first1 = David | last1 = Halliday | first2 = Robert | last2 = Resnick | year = 1977 | isbn = 0-471-71716-9 | title = Physics | edition = 3rd | publisher = [[John Wiley &amp; Sons|Wiley]] | location = New York }}
* {{citation | first1 = Charlie | last1 = Harper | year = 1976 | isbn = 0-13-487538-9 | title = Introduction to Mathematical Physics | publisher = [[Prentice-Hall]] | location = New Jersey }}
* {{citation | first1 = Erwin | last1 = Kreyszig | authorlink = Erwin Kreyszig | year = 1972 | isbn = 0-471-50728-8 | title = Advanced Engineering Mathematics | edition = 3rd | publisher = [[John Wiley &amp; Sons|Wiley]] | location = New York }}.
* Polyanin, A. D. and V. F. Zaitsev, ''Handbook of Exact Solutions for Ordinary Differential Equations (2nd edition)", Chapman &amp; Hall/CRC Press, Boca Raton, 2003. {{isbn|1-58488-297-2}}
* {{citation | first1 = George F. | last1 = Simmons | authorlink = George F. Simmons | year = 1972 | title = Differential Equations with Applications and Historical Notes | publisher = [[McGraw-Hill]] | location = New York | lccn = 75173716 }}
* {{citation | first1 = Paul A. | last1 = Tipler | year = 1991 | isbn = 0-87901-432-6 | title = Physics for Scientists and Engineers: Extended version | edition = 3rd | publisher = [[Worth Publishers]] | location = New York }}
* {{citation | first1 = Ugo | last1 = Boscain | first2 = Yacine | last2 = Chitour | year = 2011 | title = Introduction à l'automatique | url = http://www.cmapx.polytechnique.fr/~boscain/poly2011.pdf | language = french}}
* {{citation | first1 = Lawrence | last1 = Dresner | year = 1999 | title = Applications of Lie's Theory of Ordinary and Partial Differential Equations | isbn = 978-0750305303 | publisher = [[Institute of Physics Publishing]] | location = Bristol and Philadelphia }}

== Bibliography ==
* {{cite book | last1=Coddington | first1=Earl A. | last2=Levinson | first2=Norman | title=Theory of Ordinary Differential Equations | publisher=[[McGraw-Hill]] | location=New York | year=1955}}
* {{Citation | last1=Hartman | first1=Philip |authorlink=Philip Hartman |title=Ordinary differential equations | origyear=1964 | url=https://books.google.com/books?id=CENAPMUEpfoC | publisher=[[Society for Industrial and Applied Mathematics]] | location=Philadelphia | series=Classics in Applied Mathematics | isbn=978-0-89871-510-1 |mr=1929104 | year=2002 | volume=38 | doi=10.1137/1.9780898719222}}
* W. Johnson, [http://www.hti.umich.edu/cgi/b/bib/bibperm?q1=abv5010.0001.001 ''A Treatise on Ordinary and Partial Differential Equations''], John Wiley and Sons, 1913, in [http://hti.umich.edu/u/umhistmath/ University of Michigan Historical Math Collection]
* {{Citation | last1=Ince | first1=Edward L. |authorlink=Edward Lindsay Ince | title=Ordinary Differential Equations | origyear=1926 | url=https://archive.org/details/ordinarydifferen029666mbp | publisher=Dover Publications, New York | isbn=978-0-486-60349-0 | mr=0010757  | year=1944}}
* [[Witold Hurewicz]], ''Lectures on Ordinary Differential Equations'', Dover Publications, {{isbn|0-486-49510-8}}
*{{Cite book |first=Nail H |last=Ibragimov |title=CRC Handbook of Lie Group Analysis of Differential Equations Vol. 1-3 |publisher=CRC-Press |location=Providence |year=1993 |isbn=0-8493-4488-3 |postscript=&lt;!-- Bot inserted parameter. Either remove it; or change its value to "." for the cite to end in a ".", as necessary. --&gt;{{inconsistent citations}} }}.
* {{cite book| last = Teschl| given = Gerald|authorlink=Gerald Teschl| title = Ordinary Differential Equations and Dynamical Systems| publisher=[[American Mathematical Society]]| place = [[Providence, Rhode Island|Providence]]| year = 2012| isbn= 978-0-8218-8328-0| url = http://www.mat.univie.ac.at/~gerald/ftp/book-ode/}}
* [[Andrei Polyanin|A. D. Polyanin]], V. F. Zaitsev, and A. Moussiaux, Handbook of First Order Partial Differential Equations'', Taylor &amp; Francis, London, 2002. {{isbn|0-415-27267-X}}
* D. Zwillinger, ''Handbook of Differential Equations (3rd edition)'', Academic Press, Boston, 1997.

==External links==
{{wikibooks|Calculus/Ordinary differential equations}}
{{Commonscat|Ordinary differential equations}}
*{{springer|title=Differential equation, ordinary|id=p/d031910}}
*{{dmoz|Science/Math/Differential_Equations/|Differential Equations}} (includes a list of software for solving differential equations).
*[http://eqworld.ipmnet.ru/index.htm EqWorld: The World of Mathematical Equations], containing a list of ordinary differential equations with their solutions.
*[http://tutorial.math.lamar.edu/classes/de/de.aspx Online Notes / Differential Equations] by Paul Dawkins, [[Lamar University]].
*[http://www.sosmath.com/diffeq/diffeq.html Differential Equations], S.O.S. Mathematics.
*[http://numericalmethods.eng.usf.edu/mws/gen/08ode/mws_gen_ode_bck_primer.pdf A primer on analytical solution of differential equations] from the Holistic Numerical Methods Institute, University of South Florida.
*[http://www.mat.univie.ac.at/~gerald/ftp/book-ode/ Ordinary Differential Equations and Dynamical Systems] lecture notes by [[Gerald Teschl]].
*[http://www.jirka.org/diffyqs/ Notes on Diffy Qs: Differential Equations for Engineers] An introductory textbook on differential equations by Jiri Lebl of [[UIUC]].
*[http://www.openeering.com/sites/default/files/LHY_Scilab_Tutorial_Part1.pdf Modeling with ODEs using Scilab] A tutorial on how to model a physical system described by ODE using Scilab standard programming language by Openeering team.
*[http://www.wolframalpha.com/input/?i=y%27%27+%2B+2xy+%3D+0 Solving an ordinary differential equation in Wolfram|Alpha]

{{Differential equations topics}}
{{Authority control}}

[[Category:Differential calculus]]
[[Category:Ordinary differential equations| ]]</text>
      <sha1>3r5za6mnmwpfpxj6mcyv9r2hf5h16s5</sha1>
    </revision>
  </page>
  <page>
    <title>Overflow flag</title>
    <ns>0</ns>
    <id>4252320</id>
    <revision>
      <id>865116108</id>
      <parentid>865116000</parentid>
      <timestamp>2018-10-21T20:57:06Z</timestamp>
      <contributor>
        <username>Drmies</username>
        <id>5183450</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contribs/14.139.238.98|14.139.238.98]] ([[User talk:14.139.238.98|talk]]) to last version by 94.245.25.149</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2584">{{refimprove|date=January 2008}}

In computer processors, the '''overflow flag''' (sometime called V flag) is usually a single bit in a system status register used to indicate when an arithmetic overflow has occurred in an operation, indicating that the signed two's-complement result would not fit in the number of bits used for the operation (the ALU width). Some architectures may be configured to automatically generate an exception on an operation resulting in overflow.


An illustrative example is what happens if we add 127 and 127 using 8-bit registers. 127+127 is 254, but using 8-bit arithmetic the result would be 1111 1110 binary, which is -2 in [[two's complement]], and thus negative. A negative result out of positive operands (or vice versa) is an overflow. The overflow flag would then be set so the program can be aware of the problem and mitigate this or signal an error. The overflow flag is thus set when the most significant bit (here considered the sign bit) is changed by adding two numbers with the same sign (or subtracting two numbers with opposite signs). Overflow never occurs when the sign of two addition operands are different (or the sign of two subtraction operands are the same).

Internally, the overflow flag is usually generated by an [[exclusive or]] of the internal carry ''into'' and ''out of'' the sign bit. As the sign bit is the same as the most significant bit of a number ''considered'' unsigned, the overflow flag is "meaningless" and normally ignored when unsigned numbers are added or subtracted.

The overflow flag is typically changed by all arithmetic operations, including compare instructions (equivalent to a subtract instruction without storing the result). In many processor architectures, the overflow flag is cleared by bitwise operations (and, or, xor, not), possibly including shifts and rotates, but it may also be left undefined by these. Instructions such as multiply and divide often leave the flag undefined, or affected by the last partial result.

On many processors (not only [[x86]]), addition and subtraction instructions affect both the carry/borrow and overflow flags, though only one of them will normally be of interest, depending on whether the operands represented signed or unsigned numbers.&lt;ref&gt;http://teaching.idallen.com/dat2343/10f/notes/040_overflow.txt&lt;/ref&gt;

==References==
*[http://teaching.idallen.com/dat2343/10f/notes/040_overflow.txt Very detailed explanation of Overflow and Carry flags evaluation techniques.]

;Notes
&lt;references /&gt;

{{X86 assembly topics}}

[[Category:Computer arithmetic]]</text>
      <sha1>0isfdwcgael2p9i41f0m0z6grh1pd97</sha1>
    </revision>
  </page>
  <page>
    <title>P-variation</title>
    <ns>0</ns>
    <id>53707452</id>
    <revision>
      <id>870224634</id>
      <parentid>870172024</parentid>
      <timestamp>2018-11-23T09:16:33Z</timestamp>
      <contributor>
        <username>Sokka2D</username>
        <id>31242872</id>
      </contributor>
      <minor/>
      <comment>Revert formatting mistake</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9021">In [[mathematical analysis]], '''p-variation''' is a collection of [[Norm (mathematics)|seminorm]]s on functions from an ordered set to a [[metric space]], indexed by a real number &lt;math&gt;p\geq 1&lt;/math&gt;. ''p''-variation is a measure of the regularity or smoothness of a function. Specifically, if &lt;math&gt;f:I\to(M,d)&lt;/math&gt;, where &lt;math&gt;(M,d)&lt;/math&gt; is a metric space and ''I'' a totally ordered set, its ''p''-variation is

:&lt;math&gt; \| f \|_{p\text{-var}} = \left(\sup_D\sum_{t_k\in D}d(f(t_k),f(t_{k-1}))^p\right)^{1/p}&lt;/math&gt;

where ''D'' ranges over all finite [[partition of an interval|partitions of the interval]] ''I''.

The ''p'' variation of a function decreases with ''p''. If ''f'' has finite ''p''-variation and ''g'' is an ''α''-Hölder continuous function, then &lt;math&gt;g\circ f&lt;/math&gt; has finite &lt;math&gt;\frac{p}{\alpha}&lt;/math&gt;-variation.

The case when ''p'' is one is called [[total variation]], and functions with a finite 1-variation are called [[bounded variation]] functions.

== Link with Hölder norm ==

One can interpret the ''p''-variation as a parameter-independent version of the Hölder norm, which also extends to discontinuous functions. If ''f'' is ''α''&amp;ndash;[[Hölder continuous]] (i.e. its α&amp;ndash;Hölder norm is finite) then its &lt;math&gt;\frac1{\alpha}&lt;/math&gt;-variation is finite. Specifically, on an interval [''a'',''b''], &lt;math&gt;\| f \|_{\frac1\alpha\text{-var}}\le \| f \|_{\alpha}(b-a)^\alpha&lt;/math&gt;. Conversely, if ''f'' is continuous and has finite ''p-''variation, there exists a reparameterisation, &lt;math&gt;\tau&lt;/math&gt;, such that &lt;math&gt;f\circ\tau&lt;/math&gt; is &lt;math&gt;1/p-&lt;/math&gt;Hölder continuous.

If ''p'' is less than ''q'' then the space of functions of finite ''p''-variation on a compact set is continuously embedded with norm 1 into those of finite ''q''-variation. I.e. 
&lt;math&gt;\|f\|_{q\text{-var}}\le \|f\|_{p\text{-var}}&lt;/math&gt;. However unlike the analogous situation with Hölder spaces the embedding is not compact. &lt;!--(The set of such functions bounded in ''p''-variation by a single control ''is'' compactly embedded.) --&gt;
For example, consider the real functions on [0,1] given by &lt;math&gt;f_n(x)=x^n&lt;/math&gt;. They are uniformly bounded in 1-variation and converge pointwise to a discontinuous function ''f'' but this not only is not a convergence in ''p''-variation for any ''p'' but also is not uniform convergence.

== Application to Riemann–Stieltjes integration ==

If ''f'' and ''g'' are functions from &amp;nbsp;[''a'',&amp;nbsp;''b''] to ℝ with no common discontinuities and with ''f'' having finite ''p''-variation and ''g'' having finite ''q''-variation, with &lt;math&gt;\frac1p+\frac1q&gt;1&lt;/math&gt; then the [[Riemann–Stieltjes Integral]]
:&lt;math&gt;\int_a^b f(x) \, dg(x):=\lim_{|D|\to 0}\sum_{t_k\in D}f(t_k)[g(t_{k+1})-g({t_k})]&lt;/math&gt;
is well-defined. This integral is known as the ''Young integral'' because it comes from {{harvtxt|Young|1936}}.&lt;ref&gt;https://fabricebaudoin.wordpress.com/2012/12/25/lecture-7-youngs-integral/&lt;/ref&gt; The value of this definite integral is bounded by the Young-Loève estimate as follows 
:&lt;math&gt;\left|\int_a^b f(x) \, dg(x)-f(\xi)[g(b)-g(a)]\right|\le C\,\|f\|_{p\text{-var}}\|\,g\|_{q\text{-var}}&lt;/math&gt;
&lt;!--Why does slide 7 of http://web.sgh.waw.pl/~rlocho/UCT_talk.pdf have an extra factor of 2 on the RHS? Other sources don't? XXXXX --&gt;
where ''C'' is a constant which only depends on ''p'' and ''q'' and ξ is any number between ''a'' and ''b''.&lt;ref&gt;{{cite book|last1=Friz|first1=Peter K.|last2=Victoir|first2=Nicolas|title=Multidimensional Stochastic Processes as Rough Paths: Theory and Applications|date=2010|publisher=Cambridge University Press|edition=Cambridge Studies in Advanced Mathematics}}&lt;/ref&gt;
If ''f'' and ''g'' are continuous, the indefinite integral &lt;math&gt;F(w)=\int_a^w f(x) \, dg(x)&lt;/math&gt; is a continuous function with finite ''q''-variation: If ''a'' ≤ ''s'' ≤ ''t'' ≤ ''b'' then &lt;math&gt;\|F\|_{q\text{-var};[s,t]}&lt;/math&gt;, its ''q''-variation on [''s'',''t''], is bounded by 
&lt;math&gt;C\|g\|_{q\text{-var};[s,t]}(\|f\|_{p\text{-var};[s,t]}+\|f\|_{\infty;[s,t]})\le2C\|g\|_{q\text{-var};[s,t]}(\|f\|_{p\text{-var};[a,b]}+f(a))&lt;/math&gt;
where ''C'' is a constant which only depends on ''p'' and ''q''.&lt;ref&gt;{{cite book|last1=Lyons|first1=Terry|last2=Caruana|first2=Michael|last3=Levy|first3=Thierry|title=Differential equations driven by rough paths, vol. 1908 of Lecture Notes in Mathematics|date=2007|publisher=Springer}}&lt;/ref&gt;
&lt;!--
More generally, the same integral can be defined if ''f'' is an ℝ&lt;sup&gt;''e''&lt;/sup&gt;-valued one-form on ℝ&lt;sup&gt;''d''&lt;/sup&gt; (i.e. it is a function from ℝ&lt;sup&gt;''d''&lt;/sup&gt; to ''e''&amp;nbsp;×&amp;nbsp;''d'' matrices) and ''g'' is a function from the interval [''a'',&amp;nbsp;''b''] to ℝ&lt;sup&gt;''d''&lt;/sup&gt;. RUBBISH?--&gt;
&lt;!--need to define p-variation of a function like f! --&gt;
&lt;!--== Controls ==--&gt;

== Young differential equations ==
A function from ℝ&lt;sup&gt;''d''&lt;/sup&gt; to ''e''&amp;nbsp;×&amp;nbsp;''d'' real matrices is called an  ℝ&lt;sup&gt;''e''&lt;/sup&gt;-valued one-form on ℝ&lt;sup&gt;''d''&lt;/sup&gt;.

If ''f'' is a Lipschitz continuous ℝ&lt;sup&gt;''e''&lt;/sup&gt;-valued one-form on ℝ&lt;sup&gt;''d''&lt;/sup&gt;, and ''X'' is a continuous function from the interval [''a'',&amp;nbsp;''b''] to ℝ&lt;sup&gt;''d''&lt;/sup&gt; with finite ''p''-variation with ''p'' less than 2, then the integral of ''f'' on ''X'', &lt;math&gt;\int_a^b f(X(t))\,dX(t)&lt;/math&gt;, can be calculated because each component of ''f''(''X''(''t'')) will be a path of finite ''p''-variation and the integral is a sum of finitely many Young integrals. It provides the solution to the equation &lt;math&gt;dY=f(X)\,dX&lt;/math&gt; driven by the path ''X''.

More significantly, if ''f'' is a Lipschitz continuous ℝ&lt;sup&gt;''e''&lt;/sup&gt;-valued one-form on ℝ&lt;sup&gt;''e''&lt;/sup&gt;, and ''X'' is a continuous function from the interval [''a'',&amp;nbsp;''b''] to ℝ&lt;sup&gt;''d''&lt;/sup&gt; with finite ''p''-variation with ''p'' less than 2, then Young integration is enough to establish the solution of the equation &lt;math&gt;dY=f(Y)\,dX&lt;/math&gt; driven by the path ''X''.&lt;ref&gt;https://fabricebaudoin.wordpress.com/2012/12/26/lecture-8-youngs-differential-equations/&lt;/ref&gt;

== Rough differential equations ==
The theory of [[rough path]]s generalises the Young integral and Young differential equations and makes heavy use of the concept of ''p''-variation.

== For Brownian motion ==

''p''-variation should be contrasted with the [[quadratic variation]] which is used in [[stochastic analysis]], where it takes one stochastic process to another. Quadratic variation is defined as a limit as the partition gets finer, whereas ''p''-variation is a supremum over all partitions. Thus the quadratic variation of a process could be smaller than its 2-variation. If ''W''&lt;sub&gt;''t''&lt;/sub&gt; is a standard [[Wiener process|Brownian motion]] on [0,&amp;nbsp;''T''] then with probability one its ''p''-variation is infinite for &lt;math&gt;p\le2&lt;/math&gt; and finite otherwise.  The quadratic variation of ''W'' is &lt;math&gt;[W]_T=T&lt;/math&gt;.

== Computation of ''p''-variation for discrete time series ==

For a discrete time series of observations ''X&lt;sub&gt;0&lt;/sub&gt;,...,X&lt;sub&gt;N&lt;/sub&gt;'' it is straightforward to compute its ''p''-variation with complexity of [[Big-O notation|''O'']](''N&lt;sup&gt;2&lt;/sup&gt;''). Here is an example C++ code using [[dynamic programming]]:
&lt;source lang="cpp"&gt;
double p_var(const std::vector&lt;double&gt;&amp; X, double p) {
	if (X.size() == 0)
		return 0.0;
	std::vector&lt;double&gt; cum_p_var(X.size(), 0.0);   // cumulative p-variation
	for (size_t n = 1; n &lt; X.size(); n++) {
		for (size_t k = 0; k &lt; n; k++) {
			cum_p_var[n] = std::max(cum_p_var[n], cum_p_var[k] + std::pow(std::abs(X[n] - X[k]), p));
		}
	}
	return std::pow(cum_p_var.back(), 1./p);
}
&lt;/source&gt;
There exist much more efficient, but also more complicated, algorithms for ℝ-valued processes&lt;ref&gt;
{{Cite journal
  | last1 = Butkus | first1 = V.
  | last2 = Norvaiša | first2 = R.
  | year = 2018
  | title = Computation of p-variation
  | journal = Lithuanian Mathematical Journal
  | doi = 10.1007/s10986-018-9414-3
  }}&lt;/ref&gt;
and for processes in arbitrary metric spaces&lt;ref&gt;https://github.com/khumarahn/p-var&lt;/ref&gt;.

== References ==
&lt;!--- See http://en.wikipedia.org/wiki/Wikipedia:Footnotes on how to create references using&lt;ref&gt;&lt;/ref&gt; tags, these references will then appear here automatically --&gt;
{{Reflist}}
*{{citation | last=Young|first=L.C.|title=An inequality of the Hölder type, connected with Stieltjes integration|journal=Acta Mathematica|volume=67|year=1936|issue=1|pages=251&amp;ndash;282|doi=10.1007/bf02401743}}.

== External links ==
* [https://fabricebaudoin.wordpress.com/2012/12/24/lecture-6-continuous-paths-with-bounded-p-variation/ Continuous Paths with bounded p-variation] Fabrice Baudoin
* [http://web.sgh.waw.pl/~rlocho/UCT_talk.pdf On the Young integral, truncated variation and rough paths] Rafał M. Łochowski

&lt;!--- Categories ---&gt;

[[Category:Mathematical analysis]]
[[Category:Articles created via the Article Wizard]]
&lt;!-- link from variation disambiguation, bounded variation, quadratic variation, total variation, Hölder condition
--&gt;</text>
      <sha1>3yctnk0v7etkyvvy795a1s59h6klb0w</sha1>
    </revision>
  </page>
  <page>
    <title>Quiver (mathematics)</title>
    <ns>0</ns>
    <id>355100</id>
    <revision>
      <id>868202479</id>
      <parentid>864312478</parentid>
      <timestamp>2018-11-10T17:46:38Z</timestamp>
      <contributor>
        <username>JCW-CleanerBot</username>
        <id>31737083</id>
      </contributor>
      <minor/>
      <comment>/* Sources */clean up, replaced: Uspehi Mat. Nauk → Uspekhi Mat. Nauk</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11807">In [[mathematics]], a '''quiver''' is a [[directed graph]] where loops and multiple arrows between two [[vertex (graph theory)|vertices]] are allowed, i.e. a [[multidigraph]]. They are commonly used in [[representation theory]]: a representation&amp;nbsp;''V'' of a quiver assigns a [[vector space]]&amp;nbsp;''V''(''x'') to each vertex&amp;nbsp;''x'' of the quiver and a [[linear map]]&amp;nbsp;''V''(''a'') to each arrow&amp;nbsp;''a''.

In [[category theory]], a quiver can be understood to be the underlying structure of a [[category (mathematics)|category]], but without composition or a designation of identity morphisms.  That is, there is a [[forgetful functor]] from '''Cat''' to '''Quiv'''.  Its [[left adjoint]] is a [[free functor]] which, from a quiver, makes the corresponding [[free category]].

==Definition==

A quiver Γ consists of:
* The set ''V'' of vertices of Γ
* The set ''E'' of edges of Γ
* Two functions: ''s'': ''E'' → ''V'' giving the ''start'' or ''source'' of the edge, and another function, ''t'': ''E'' → ''V'' giving the ''target'' of the edge.

This definition is identical to that of a [[multidigraph]].

A [[morphism]] of quivers is defined as follows.  If &lt;math&gt;\Gamma=(V,E,s,t)&lt;/math&gt; and &lt;math&gt;\Gamma'=(V',E',s',t')&lt;/math&gt; are two quivers, then a morphism &lt;math&gt;m=(m_v, m_e)&lt;/math&gt; of quivers consist of two functions &lt;math&gt;m_v: V\to V'&lt;/math&gt; and &lt;math&gt;m_e: E\to E'&lt;/math&gt; such that following [[commuting diagram|diagrams commute]]:  
:&lt;math&gt;m_v \circ s = s' \circ m_e&lt;/math&gt;
and
:&lt;math&gt;m_v \circ t = t' \circ m_e&lt;/math&gt;

==Category-theoretic definition==
The above definition is based in [[set theory]]; the category-theoretic definition generalizes this into a [[functor]] from the ''free quiver''  to the [[category of sets]].

The '''free quiver''' (also called the '''walking quiver''', '''Kronecker quiver''', '''2-Kronecker quiver''' or '''Kronecker category''') ''Q'' is a category with two objects, and four morphisms: The objects are ''V'' and ''E''.  The four morphisms are ''s'': ''E'' → ''V'', ''t'': ''E'' → ''V'', and the [[identity morphism]]s id&lt;sub&gt;''V''&lt;/sub&gt;: ''V'' → ''V'' and id&lt;sub&gt;''E''&lt;/sub&gt;: ''E'' → ''E''.  That is, the free quiver is

:&lt;math&gt;E 
\;\begin{matrix}  s \\[-6pt] \rightrightarrows \\[-4pt] t \end{matrix}\; V&lt;/math&gt;

A quiver is then a [[functor]] Γ: ''Q'' → '''Set'''.

More generally, a quiver in a category ''C'' is a functor Γ: ''Q'' → ''C''.  The category '''Quiv'''(''C'') of quivers in ''C'' is the [[functor category]] where:

* objects are functors Γ: ''Q'' → ''C'',
* morphisms are [[natural transformation]]s between functors.

Note that '''Quiv''' is the [[category of presheaves]] on the [[opposite category]] ''Q''&lt;sup&gt;op&lt;/sup&gt;.

==Path algebra==
If Γ is a quiver, then a '''path''' in Γ is a sequence of arrows ''a''&lt;sub&gt;''n''&lt;/sub&gt;&amp;nbsp;''a''&lt;sub&gt;''n''−1&lt;/sub&gt;&amp;nbsp;...&amp;nbsp;''a''&lt;sub&gt;3&lt;/sub&gt;&amp;nbsp;''a''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;''a''&lt;sub&gt;''1''&lt;/sub&gt; such that the head of ''a''&lt;sub&gt;''i''+1&lt;/sub&gt; =&amp;nbsp;tail of&amp;nbsp;''a''&lt;sub&gt;''i''&lt;/sub&gt;, using the convention of concatenating paths from right to left.

If ''K'' is a [[field (mathematics)|field]] then the '''quiver algebra''' or  '''path algebra''' ''K''Γ is defined as a vector space having all the paths (of length&amp;nbsp;≥&amp;nbsp;0) in the quiver as basis (including, for each vertex ''i'' of the quiver Γ, a ''trivial path'' &lt;math&gt;e_i&lt;/math&gt; of length 0; these paths are ''not'' assumed to be equal for different ''i''), and multiplication given by concatenation of paths. If two paths cannot be concatenated because the end vertex of the first is not equal to the starting vertex of the second, their product is defined to be zero. This defines an [[associative algebra]] over ''K''. This algebra has a unit element if and only if the quiver has only finitely many vertices. In this case, the [[module (mathematics)|modules]] over ''K''Γ are naturally identified with the representations of Γ. If the quiver has infinitely many vertices, then ''K''Γ has an [[approximate identity]] given by &lt;math&gt;e_E:=\sum_{v\in E} 1_v&lt;/math&gt; where ''E'' ranges over finite subsets of the vertex set of Γ.

If the quiver has finitely many vertices and arrows, and the end vertex and starting vertex of any path are always distinct (i.e. ''Q'' has no oriented cycles), then ''K''Γ is a finite-[[dimension of a vector space|dimensional]] [[hereditary algebra]] over ''K''. Conversely, if ''K'' is algebraically closed, then any finite-dimensional, hereditary, associative algebra over ''K'' is [[Morita equivalence|Morita equivalent]] to the path algebra of its [[Ext quiver]] (i.e., they have equivalent module categories).

== Representations of quivers ==

A representation of a quiver ''Q'' is an association of an ''R''-module to each vertex of ''Q'', and a morphism between each module for each arrow.

A representation ''V'' of a quiver ''Q'' is said to be ''trivial'' if ''V''(''x'')&amp;nbsp;=&amp;nbsp;0 for all vertices ''x'' in&amp;nbsp;''Q''.

A ''morphism'', ''f'':&amp;nbsp;''V''&amp;nbsp;→&amp;nbsp;''V′'', between representations of the quiver ''Q'', is a collection of linear maps &lt;math&gt;f(x):V(x)\rightarrow V'(x)&lt;/math&gt; such that for every arrow ''a'' in ''Q'' from ''x'' to ''y'' &lt;math&gt;V'(a) f(x) = f(y) V(a) &lt;/math&gt;, i.e. the squares that ''f'' forms with the arrows of ''V'' and ''V′'' all commute.  A morphism, ''f'', is an ''isomorphism'', if ''f''(''x'') is invertible for all vertices ''x'' in the quiver.  With these definitions the representations of a quiver form a [[Category (mathematics)|category]].

If ''V'' and ''W'' are representations of a quiver ''Q'', then the direct sum of these representations, &lt;math&gt;V\oplus W&lt;/math&gt;, is defined by &lt;math&gt;(V\oplus W)(x)=V(x)\oplus W(x)&lt;/math&gt; for all vertices ''x'' in ''Q'' and &lt;math&gt;(V\oplus W)(a)&lt;/math&gt; is the direct sum of the linear mappings ''V''(''a'') and&amp;nbsp;''W''(''a'').

A representation is said to be ''decomposable'' if it is isomorphic to the direct sum of non-zero representations.

A [[Category theory|categorical]] definition of a quiver representation can also be given.  The quiver itself can be considered a category, where the vertices are objects and paths are morphisms.  Then a representation of ''Q'' is just a covariant [[functor]] from this category to the category of finite dimensional [[vector space]]s.  Morphisms of representations of ''Q'' are precisely [[natural transformations]] between the corresponding functors.

For a finite quiver Γ (a quiver with finitely many vertices and edges), let ''K''Γ be its path algebra.  Let ''e''&lt;sub&gt;''i''&lt;/sub&gt; denote the trivial path at vertex&amp;nbsp;''i''.  Then we can associate to the vertex&amp;nbsp;''i'' the [[projective module|projective]] ''K''Γ-module ''K''Γ''e&lt;sub&gt;i&lt;/sub&gt;'' consisting of linear combinations of paths which have starting vertex&amp;nbsp;''i''.  This corresponds to the representation of Γ obtained by putting a copy of ''K'' at each vertex which lies on a path starting at ''i'' and 0 on each other vertex.  To each edge joining two copies of ''K'' we associate the identity map.

== Quiver with relations ==
To enforce commutativity of some squares inside a quiver a generalization is the notion of quivers with relations (also named bound quivers).
A relation on a quiver &lt;math&gt;Q&lt;/math&gt; is a &lt;math&gt;K&lt;/math&gt; linear combination of paths from &lt;math&gt;Q&lt;/math&gt;.
A quiver with relation is a pair &lt;math&gt;(Q, I)&lt;/math&gt; with &lt;math&gt;Q&lt;/math&gt; a quiver and &lt;math&gt;I \subseteq K\Gamma&lt;/math&gt; an
ideal of the path algebra. The quotient &lt;math&gt;K\Gamma/I&lt;/math&gt; is the path algebra of &lt;math&gt;(Q, I)&lt;/math&gt;.

===Quiver Variety===

Given the dimensions of the vector spaces assigned to every vertex, one can form a variety which characterizes all representations of that quiver with those specified dimensions, and consider stability conditions. These give quiver varieties, as constructed by {{harvtxt|King|1994}}.

== Gabriel's theorem ==

{{main|Gabriel's theorem}}
A quiver is of '''finite type''' if it has only finitely many isomorphism classes of indecomposable representations. {{harvtxt|Gabriel|1972}} classified all quivers of finite type, and also their indecomposable representations. More precisely, Gabriel's theorem states that:

# A (connected) quiver is of finite type if and only if its underlying graph (when the directions of the arrows are ignored) is one of the [[ADE classification|ADE]] [[Dynkin diagram]]s: &lt;math&gt;A_n&lt;/math&gt;, &lt;math&gt;D_n&lt;/math&gt;, &lt;math&gt;E_6&lt;/math&gt;, &lt;math&gt;E_7&lt;/math&gt;, &lt;math&gt;E_8&lt;/math&gt;.
# The indecomposable representations are in a one-to-one correspondence with the positive roots of the [[root system]] of the Dynkin diagram.

{{harvtxt|Dlab|Ringel|1973}} found a generalization of Gabriel's theorem in which all Dynkin diagrams of finite dimensional semisimple Lie algebras occur.

== See also ==

* [[ADE classification]]
* [[Adhesive category]]
* [[Graph algebra]]
* [[Group algebra]]
* [[Incidence algebra]]
* [[Quiver diagram]]
* [[Semi-invariant of a quiver]]

== References ==
{{Reflist}}

=== Lecture Notes ===
*{{Citation | first=William | last=Crawley-Boevey | title=Lectures on Representations of Quivers | url=http://www1.maths.leeds.ac.uk/~pmtwc/quivlecs.pdf | deadurl=bot: unknown | archiveurl=https://web.archive.org/web/20170820013659/http://www1.maths.leeds.ac.uk/~pmtwc/quivlecs.pdf | archivedate=2017-08-20 | df= }}

==Sources==
{{commons category|Quivers (graph theory)}}
* {{Citation | url = http://www.ams.org/notices/200502/fea-weyman.pdf | title = Quiver Representations | first1 = Harm | last1 = Derksen | first2 = Jerzy | last2 = Weyman | journal = [[Notices of the American Mathematical Society]] | volume = 52 | issue = 2 |date=February 2005 }}
*{{Citation | last1=Dlab | first1=Vlastimil | last2=Ringel | first2=Claus Michael | title=On algebras of finite representation type | url=https://books.google.com/books?id=_JrnAAAAMAAJ | publisher=Department of Mathematics, Carleton Univ., Ottawa, Ont. | series=Carleton Mathematical Lecture Notes | mr=0347907 | year=1973 | volume=2}}
* {{citation |last=Crawley-Boevey |first=William|authorlink= William Crawley-Boevey |year=1992 |title=Notes on Quiver Representations |url=http://www.amsta.leeds.ac.uk/~pmtwc/quivlecs.pdf | publisher=[[Oxford University]] }}
*{{Citation | last1=Gabriel | first1=Peter | title=Unzerlegbare Darstellungen. I | doi=10.1007/BF01298413 | mr=0332887 | year=1972 | journal=[[Manuscripta Mathematica]] | issn=0025-2611 | volume=6 | issue=1 | pages=71–103}}. [http://www.amsta.leeds.ac.uk/~pmtwc/quivlecs-corrections.txt Errata].
* {{citation |last=King |first=Alastair| journal=Quart. J. Math. |year=1994 |title= Moduli of representations of finite-dimensional algebras | volume=45 |issue= 180 | pages=515–530 }}  
* {{Citation |last=Savage |first=Alistair |orig-year=2005 |chapter=Finite-dimensional algebras and quivers |year=2006 |volume=2 |pages=313–320 |title=[[Encyclopedia of Mathematical Physics]] |editor1-last=Francoise |editor1-first=J.-P. |editor2-last=Naber |editor2-first=G. L. |editor3-last=Tsou |editor3-first=S.T. |publisher=Elsevier |arxiv=math/0505082 |isbn=|bibcode=2005math......5082S }}
* {{citation | title=Elements of the Representation Theory of Associative Algebras|first1=Daniel|last1= Simson|first2= Andrzej|last2=Skowronski|first3= Ibrahim|last3= Assem|publisher= [[Cambridge University Press]] |year= 2007|isbn=978-0-521-88218-7}}
*Bernšteĭn, I. N.; Gelʹfand, I. M.; Ponomarev, V. A., "Coxeter functors, and Gabriel's theorem" (Russian), ''Uspekhi Mat. Nauk'' '''28''' (1973), no. 2(170), 19–33. [http://www.math.tau.ac.il/~bernstei/Publication_list/publication_texts/BGG-CoxeterF-Usp.pdf Translation on Bernstein's website].
* {{nlab|id=quiver|title=Quiver}}

[[Category:Category theory]]
[[Category:Representation theory]]
[[Category:Directed graphs]]</text>
      <sha1>h6zpfq6aqay3d1262oklbyl2satwyfn</sha1>
    </revision>
  </page>
  <page>
    <title>Redundant binary representation</title>
    <ns>0</ns>
    <id>18584624</id>
    <revision>
      <id>822369861</id>
      <parentid>696567450</parentid>
      <timestamp>2018-01-26T00:02:27Z</timestamp>
      <contributor>
        <username>Yayay</username>
        <id>61660</id>
      </contributor>
      <comment>update source link</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7747">A '''redundant binary representation (RBR)''' is a [[numeral system]] that uses more bits than needed to represent a single binary [[Numerical digit|digit]] so that most numbers have several representations. An RBR is unlike usual [[binary numeral system]]s, including [[two's complement]], which use a single bit for each digit. Many of an RBR's properties differ from those of regular binary representation systems. Most importantly, an RBR allows addition without using a typical carry.&lt;ref&gt;{{Cite journal |first1=Dhananjay S. |last1=Phatak |first2=Israel |last2=Koren |title=Hybrid Signed-Digit Number Systems: A Unified Framework for Redundant Number Representations with Bounded Carry Propagation Chains |date=August 1994 |journal=IEEE Transactions on Computers |volume=43 |issue=8 |pages=880&amp;ndash;891 |doi=10.1109/12.295850 |url=http://euler.ecs.umass.edu/research/PhKo-IEEETC-1994.pdf}}&lt;/ref&gt; When compared to non-redundant representation, an RBR makes [[bitwise operation|bitwise logical operation]] slower, but [[arithmetic operation#Arithmetic operations|arithmetic operations]] are faster when a greater bit width is used.&lt;ref&gt;{{cite web |first=Louis Philippe |last=Lessard |title=Fast Arithmetic on FPGA Using Redundant Binary Apparatus |year=2008 |url=http://www.louislessard.com/rbin/ |accessdate=2015-09-12}}&lt;/ref&gt;  Usually, each digit has its own sign that is not necessarily the same as the sign of the number represented. When digits have signs, that RBR is also a [[signed-digit representation]].

==Conversion from RBR==
An RBR is a [[positional notation|place-value notation system]]. In an RBR, [[Numerical digit|digit]]s are ''pairs'' of bits, that is, for every place, an RBR uses a pair of bits. The value represented by a redundant digit can be found using a translation table. This table indicates the mathematical value of each possible pair of bits.

As in conventional binary representation, the [[integer]] value of a given representation is a weighted sum of the values of the digits. The weight starts at 1 for the rightmost position and goes up by a factor of 2 for each next position. Usually, an RBR allows negative values. There is no single sign bit that tells if a redundantly represented number is positive or negative. Most integers have several possible representations in an RBR.

Often one of the several possible representations of an integer is chosen as the "canonical" form, so each integer has only one possible "canonical" representation; [[non-adjacent form]] and two's complement are popular choices for that canonical form.

An [[integer]] value can be converted back from an RBR using the following formula, where ''n'' is the number of digit and ''d&lt;sub&gt;k&lt;/sub&gt;'' is the interpreted value of the ''k''-th digit, where ''k'' starts at 0 at the rightmost position:
:&lt;math&gt;
\sum_{k=0}^{n-1} d_k 2^k
&lt;/math&gt;

The conversion from an RBR to ''n''-bit two's complement can be done in O(log(''n'')) time using a [[prefix adder]].&lt;ref&gt;{{Cite conference |first1=Sreehari |last1=Veeramachaneni |first2=M. Kirthi |last2=Krishna |first3=Lingamneni |last3=Avinash |first4=Sreekanth |last4=Reddy P. |first5=M.B. |last5=Srinivas |title=Novel High-Speed Redundant Binary to Binary converter using Prefix Networks  |conference=IEEE International Symposium on Circuits and Systems (ISCAS 2007) |date=May 2007 |location=New Orleans |doi=10.1109/ISCAS.2007.378170 |url=http://euler.ecs.umass.edu/research/PhKo-IEEETC-1994.pdf}}&lt;/ref&gt;

==Example of redundant binary representation==
{| class="wikitable" align="right" style="margin-left: 3em; text-align:center;"
|+ Example of translation table for a digit
! Digit !! Interpreted value
|-
| 00 || −1
|-
| 01 || &amp;nbsp;0
|-
| 10 || &amp;nbsp;0
|-
| 11 || &amp;nbsp;1
|}
Not all redundant representations have the same properties. For example, using the translation table on the right, the number 1 can be represented in this RBR in many ways: "01·01·01·11" (0+0+0+1), "01·01·10·11" (0+0+0+1), "01·01·11·00" (0+0+2−1), or "11·00·00·00" (8−4−2−1).  Also, for this translation table, flipping all bits ([[NOT gate]]) corresponds to finding the [[additive inverse]] ([[multiplication]] by [[−1]]) of the integer represented.&lt;ref&gt;{{Cite journal |first1=Marcel |last1=Lapointe |first2=Huu Tue |last2=Huynh |first3=Paul |last3=Fortier |title=Systematic Design of Pipelined Recursive Filters |journal=IEEE Transactions on Computers |volume=42 |issue=4 |date=April 1993 |pages=413&amp;ndash;426 |doi=10.1109/12.214688}}&lt;/ref&gt;

In this case: &lt;math&gt;d_k \isin \{ -1, 0, 1 \}&lt;/math&gt;

==Arithmetic operations==
Redundant representations are commonly used inside high-speed [[arithmetic logic unit]]s.

In particular, a [[carry-save adder]] uses a redundant representation.{{Cn|date=September 2015}}

===Addition===
[[Image:Redundant binary adder.png|thumb|right|Schematic of an adder unit using [[full adder]] block (z = x + y)]]
The addition operation in all RBRs is carry-free, which means that the carry does not have to propagate through the full width of the addition unit. In effect, the addition in all RBRs is a constant-time operation. The addition will always take the same amount of time independently of the bit-width of the [[operand]]s. This does not imply that the addition is always faster in an RBR than its [[two's complement]] equivalent, but that the addition will eventually be faster in an RBR with increasing bit width because the two's complement addition unit's delay is proportional to log(''n'') (where ''n'' is the bit width).&lt;ref&gt;{{Cite conference |author1=Yu-Ting Pai |author2=Yu-Kumg Chen |title=The fastest carry lookahead adder |conference=Second IEEE International Workshop on Electronic Design, Test and Applications (DELTA '04) |location=Perth |date=January 2004 |doi=10.1109/DELTA.2004.10071 |url=http://galia.fc.uaslp.mx/~rmariela/digital/FastCLA.pdf}}&lt;/ref&gt;  Addition in an RBR takes a constant time because each digit of the result can be calculated independently of one another, implying that each digit of the result can be calculated in parallel.&lt;ref&gt;{{Cite conference |first1=Bijoy |last1=Jose |first2=Damu |last2=Radhakrishnan |title=Delay Optimized Redundant Binary Adders |conference=13th IEEE International Conference on Electronics, Circuits and Systems, 2006. (ICECS '06) |date=December 2006 |location=Nice |doi=10.1109/ICECS.2006.379838}}&lt;/ref&gt;

===Subtraction===
Subtraction is the same as the addition except that the additive inverse of the second operand needs to be computed first. For common representations, this can be done on a digit-by-digit basis.

==Logical operations==
Bitwise logical operations, such as [[AND gate|AND]], [[logical disjunction|OR]] and [[XOR]], are not possible in redundant representations.  While it is possible to do [[bitwise operation]]s directly on the underlying bits inside an RBR, it is not clear that this is a meaningful operation; there are many ways to represent a value in an RBR, and the value of the result would depend on the representation used.

To get the expected results, it is necessary to convert the two operands first to non-redundant representations. Consequently, logical operations are slower in an RBR. More precisely, they take a time proportional to log(''n'') (where ''n'' is the number of digit) compared to a constant-time in [[two's complement]].

It is, however, possible to ''partially'' convert only the least-significant portion of a redundantly represented number to non-redundant form.  This allows operations such as masking off the low ''k'' bits can be done in log(''k'') time.

==References==
{{reflist}}

{{DEFAULTSORT:Redundant Binary Representation}}
[[Category:Binary arithmetic]]
[[Category:Non-standard positional numeral systems]]</text>
      <sha1>7n8cjuqdldlmngckiftq9lljnofler0</sha1>
    </revision>
  </page>
  <page>
    <title>Regular icosahedron</title>
    <ns>0</ns>
    <id>14968</id>
    <revision>
      <id>870040684</id>
      <parentid>863451983</parentid>
      <timestamp>2018-11-22T00:17:37Z</timestamp>
      <contributor>
        <ip>2600:1:9277:EBE8:3E12:2B5A:DCA4:8829</ip>
      </contributor>
      <comment>/* Cartesian coordinates */Trim</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="33624">{{See also|Great icosahedron}}
{{Reg polyhedra db|Reg polyhedron stat table|I}}
In [[geometry]], a '''regular [[icosahedron]]''' ({{IPAc-en|ˌ|aɪ|k|ɒ|s|ə|ˈ|h|iː|d|r|ən|,_|-|k|ə|-|,_|-|k|oʊ|-}} or {{IPAc-en|aɪ|ˌ|k|ɒ|s|ə|ˈ|h|iː|d|r|ən}}{{refn|{{Citation |last=Jones |first=Daniel |author-link=Daniel Jones (phonetician) |title=English Pronouncing Dictionary |editors=Peter Roach, James Hartmann and Jane Setter |place=Cambridge |publisher=Cambridge University Press |orig-year=1917 |year=2003 |isbn=3-12-539683-2 }}}}) is a convex [[polyhedron]] with 20 faces, 30 edges and 12 vertices. It is one of the five [[Platonic solid]]s, and the one with the most sides.

It has five equilateral triangular faces meeting at each vertex. It is represented by its [[Schläfli symbol]] {3,5}, or sometimes by its [[vertex figure]] as 3.3.3.3.3 or 3&lt;sup&gt;5&lt;/sup&gt;. It is the [[dual polyhedron|dual]] of the [[Regular dodecahedron|dodecahedron]], which is represented by {5,3}, having three pentagonal faces around each vertex.

A regular icosahedron is a [[Johnson solid#Names|gyroelongated]] [[pentagonal bipyramid]] and a biaugmented [[pentagonal antiprism]] in any of six orientations.

The name comes {{ety|gre|''εἴκοσι'' (eíkosi)|twenty||''ἕδρα'' (hédra)|seat}}. The plural can be either "icosahedrons" or "icosahedra" ({{IPAc-en|-|d|r|ə}}).

==Dimensions==
[[File:Icosaedro desarrollo.gif|thumb|Net folding into icosahedron]]
If the edge length of a regular icosahedron is ''a'', the [[radius]] of a circumscribed [[sphere]] (one that touches the icosahedron at all vertices) is
:&lt;math&gt;r_u = \frac{a}{2} \sqrt{\phi \sqrt{5}} = \frac{a}{4} \sqrt{10 +2\sqrt{5}} = a\sin\frac{2\pi}{5} \approx 0.951\,056\,5163 \cdot a&lt;/math&gt; {{OEIS2C|A019881}}

and the radius of an inscribed sphere ([[tangent]] to each of the icosahedron's faces) is
:&lt;math&gt;r_i = \frac{\phi^2 a}{2 \sqrt{3}} = \frac{\sqrt{3}}{12} \left(3+ \sqrt{5} \right) a \approx 0.755\,761\,3141\cdot a&lt;/math&gt; {{OEIS2C|A179294}}

while the midradius, which touches the middle of each edge, is
:&lt;math&gt;r_m = \frac{a \phi}{2} = \frac{1}{4} \left(1+\sqrt{5}\right) a = a\cos\frac{\pi}{5} \approx 0.809\,016\,99\cdot a&lt;/math&gt; {{OEIS2C|A019863}}

where ''ϕ'' is the [[golden ratio]].

==Area and volume==
The surface area ''A'' and the [[volume]] ''V'' of a regular icosahedron of edge length ''a'' are:

:&lt;math&gt;A = 5\sqrt{3}a^2 \approx 8.660\,254\,04a^2&lt;/math&gt; {{OEIS2C|A010527}}

:&lt;math&gt;V = \frac{5}{12} (3+\sqrt{5})a^3 \approx 2.181\,694\,99a^3&lt;/math&gt; {{OEIS2C|A102208}}

The latter is ''F''&amp;nbsp;=&amp;nbsp;''20'' times the volume of a general [[tetrahedron]] with apex at the center of the
inscribed sphere, where the volume of the tetrahedron is one third times the base area {{sfrac|{{sqrt|3}}''a''&lt;sup&gt;2&lt;/sup&gt;|4}} times its height ''r&lt;sub&gt;i&lt;/sub&gt;''.

The volume filling factor of the circumscribed sphere is:

:&lt;math&gt;f=\frac{V}{\frac43 \pi r_u^3} = \frac{20(3+\sqrt{5})}{(2\sqrt{5}+10)^{\frac32}\pi}\approx 0.605\,461\,3829&lt;/math&gt;

==Cartesian coordinates==
[[Image:Icosahedron-golden-rectangles.svg|200px|thumb|right|Icosahedron vertices form three orthogonal golden rectangles]]
The vertices of an icosahedron centered at the origin with an edge-length of 2 and a [[circumradius]] of &lt;math&gt;\sqrt{\phi+2} \approx 1.9&lt;/math&gt; are described by [[Circular shift|circular permutations]] of:&lt;ref&gt;{{mathworld |title=Icosahedral group |urlname=IcosahedralGroup}}&lt;/ref&gt;
:(0, ±1, ±''ϕ'')
where ''ϕ''&amp;nbsp;=&amp;nbsp;{{sfrac|1 + {{sqrt|5}}|2}} is the [[golden ratio]].

Taking all permutations (not just cyclic ones) results in the [[Compound of two icosahedra]].

Note that these vertices form five sets of three concentric, mutually [[orthogonal]] [[golden rectangle]]s, whose edges form [[Borromean rings]].

If the original icosahedron has edge length 1, its dual [[dodecahedron]] has edge length {{sfrac|{{sqrt|5}} − 1|2}} = {{sfrac|1|''ϕ''}} = ''ϕ''&amp;nbsp;−&amp;nbsp;1.

[[File:Icosahedron model.JPG|right|200px|thumb|Model of an icosahedron made with metallic spheres and magnetic connectors]]
The 12 edges of a regular [[octahedron]] can be subdivided in the golden ratio so that the resulting vertices define a regular icosahedron. This is done by first placing vectors along the octahedron's edges such that each face is bounded by a cycle, then similarly subdividing each edge into the golden mean along the direction of its vector. The [[Compound of five octahedra|five octahedra]] defining any given icosahedron form a regular [[polyhedral compound]], while the [[compound of two icosahedra|two icosahedra]] that can be defined in this way from any given octahedron form a [[uniform polyhedron compound]].

[[File:Вписанный правильный икосаэдр и четыре плоскости.gif|thumb|right|Regular icosahedron and its [[circumscribed sphere]]. Vertices of the regular icosahedron lie in four parallel planes, forming in them four [[equilateral triangle]]s; this was proved by [[Pappus of Alexandria]]]]
=== Spherical coordinates ===
The locations of the vertices of a regular icosahedron can be described using [[spherical coordinates]], for instance as [[latitude and longitude]]. If two vertices are taken to be at the north and south poles (latitude ±90°), then the other ten vertices are at latitude ±[[arctan]]({{sfrac|1|2}}) ≈ ±26.57°. These ten vertices are at evenly spaced longitudes (36° apart), alternating between north and south latitudes.

This scheme takes advantage of the fact that the regular icosahedron is a pentagonal [[gyroelongated bipyramid]], with D&lt;sub&gt;5d&lt;/sub&gt; [[Dihedral symmetry in three dimensions|dihedral symmetry]]—that is, it is formed of two congruent pentagonal pyramids joined by a pentagonal [[antiprism]].

==Orthogonal projections==
The icosahedron has three special [[orthogonal projection]]s, centered on a face, an edge and a vertex:
{|class=wikitable width=360
|+ Orthogonal projections
|-
!Centered by
!Face
!Edge
!Vertex
|- align=center
![[Coxeter plane]]
!A&lt;sub&gt;2&lt;/sub&gt;
!A&lt;sub&gt;3&lt;/sub&gt;
!H&lt;sub&gt;3&lt;/sub&gt;
|- align=center
!Graph
|[[File:Icosahedron A2 projection.svg|100px]]
|[[File:Icosahedron graph A3 1.png|100px]]
|[[File:Icosahedron H3 projection.svg|100px]]
|- align=center
!Projective&lt;BR&gt;symmetry
|[6]
|[2]
|[10]
|- align=center
!Graph
|[[File:Icosahedron fnormal.png|100px]]&lt;BR&gt;Face normal
|[[File:Icosahedron graph A3 2.png|100px]]&lt;BR&gt;Edge normal
|[[File:Icosahedron vnormal.png|100px]]&lt;BR&gt;Vertex normal
|}

==Spherical tiling==
The icosahedron can also be represented as a [[spherical tiling]], and projected onto the plane via a [[stereographic projection]]. This projection is [[Conformal map|conformal]], preserving angles but not areas or lengths. Straight lines on the sphere are projected as circular arcs on the plane.
{|class=wikitable
|- align=center valign=top
|[[File:Uniform tiling 532-t2.png|160px]]
|[[File:icosahedron stereographic projection.svg|160px]]
|-
![[Orthographic projection]]
!colspan=1|[[Stereographic projection]]
|}

==Other facts==
*An icosahedron has 43,380 distinct [[net (polyhedron)|nets]].&lt;ref&gt;{{mathworld |urlname=RegularIcosahedron |title=Regular Icosahedron}}&lt;/ref&gt;
*To color the icosahedron, such that no two adjacent faces have the same color, requires at least 3 colors.&lt;ref&gt;This is true for all convex polyhedra with triangular faces except for the tetrahedron, by applying [[Brooks' theorem]] to the [[dual graph]] of the polyhedron.&lt;/ref&gt;
*A problem dating back to the ancient Greeks is to determine which of two shapes has larger volume, an icosahedron inscribed in a sphere, or a [[dodecahedron]] inscribed in the same sphere. The problem was solved by [[Hero of Alexandria|Hero]], [[Pappus of Alexandria|Pappus]], and [[Fibonacci]], among others.&lt;ref&gt;{{citation|title=A Mathematical History of the Golden Number|first=Roger|last=Herz-Fischler|publisher=Courier Dover Publications|year=2013|isbn=9780486152325|pages=138–140|url=https://books.google.com/books?id=aYjXZJwLARQC&amp;pg=PA138}}.&lt;/ref&gt; [[Apollonius of Perga]] discovered the curious result that the ratio of volumes of these two shapes is the same as the ratio of their surface areas.&lt;ref&gt;{{citation|title=Calculus Gems: Brief Lives and Memorable Mathematics|first=George F.|last=Simmons|publisher=Mathematical Association of America|year=2007|isbn=9780883855614|page=50|url=https://books.google.com/books?id=3KOst4Mon90C&amp;pg=PA50}}.&lt;/ref&gt; Both volumes have formulas involving the [[golden ratio]], but taken to different powers.&lt;ref&gt;{{citation|title=Platonic &amp; Archimedean Solids|series=Wooden Books|first=Daud|last=Sutton|publisher=Bloomsbury Publishing USA|year=2002|isbn=9780802713865|page=55|url=https://books.google.com/books?id=vgo7bTxDmIsC&amp;pg=PA55}}.&lt;/ref&gt; As it turns out, the icosahedron occupies less of the sphere's volume (60.54%) than the dodecahedron (66.49%).&lt;ref&gt;Numerical values for the volumes of the inscribed Platonic solids may be found in {{citation|title=The Platonic Solids (Solution to problem E2053)|first1=W. E.|last1=Buker|first2=R. B.|last2=Eggleton|journal=[[American Mathematical Monthly]]|volume=76|issue=2|year=1969|page=192|jstor= 2317282|doi=10.2307/2317282}}.&lt;/ref&gt;

==Construction by a system of equiangular lines==
{| class=wikitable align=right width=320
|- align=center
|[[File:Icosahedron H3 projection.svg|160px]]&lt;BR&gt;Icosahedron&lt;BR&gt;H&lt;sub&gt;3&lt;/sub&gt; Coxeter plane
|[[File:6-cube t5 B5.svg|160px]]&lt;BR&gt;[[6-orthoplex]]&lt;BR&gt;D&lt;sub&gt;6&lt;/sub&gt; Coxeter plane
|-
|colspan=2|This construction can be geometrically seen as the 12 vertices of the [[6-orthoplex]] projected to 3 dimensions. This represents a [[Coxeter–Dynkin diagram#Geometric folding|geometric folding]] of the D&lt;sub&gt;6&lt;/sub&gt; to H&lt;sub&gt;3&lt;/sub&gt; [[Coxeter group]]s: [[File:Geometric folding Coxeter graph D6 H3.png]]
Seen by these 2D [[Coxeter plane]] orthogonal projections, the two overlapping central vertices define the third axis in this mapping. 
|}

The following construction of the icosahedron avoids tedious computations in the [[Algebraic number field|number field]] {{math|ℚ}}[{{sqrt|5}}] necessary in more elementary approaches.

The existence of the icosahedron amounts to the existence of six [[equiangular lines]] in {{math|ℝ}}{{sup|3}}. Indeed, intersecting such a system of equiangular lines with a Euclidean sphere centered at their common intersection yields the twelve vertices of a regular icosahedron as can easily be checked. Conversely, supposing the existence of a regular icosahedron, lines defined by its six pairs of opposite vertices form an equiangular system.

In order to construct such an equiangular system, we start with this 6&amp;nbsp;×&amp;nbsp;6 square [[Matrix (mathematics)|matrix]]:
:&lt;math&gt;A=\left(\begin{array}{crrrrr}
0&amp;1&amp;1&amp;1&amp;1&amp;1\\
1&amp;0&amp;1&amp;-1&amp;-1&amp;1\\
1&amp;1&amp;0&amp;1&amp;-1&amp;-1\\
1&amp;-1&amp;1&amp;0&amp;1&amp;-1\\
1&amp;-1&amp;-1&amp;1&amp;0&amp;1\\
1&amp;1&amp;-1&amp;-1&amp;1&amp;0\end{array}\right).&lt;/math&gt;

A straightforward computation yields {{nowrap|''A''&lt;sup&gt;2&lt;/sup&gt; {{=}} 5''{{math|I}}''}} (where ''{{math|I}}'' is the 6&amp;nbsp;×&amp;nbsp;6 identity matrix). This implies that ''A'' has [[Eigenvalue, eigenvector and eigenspace|eigenvalues]] –{{sqrt|5}} and {{sqrt|5}}, both with multiplicity 3 since ''A'' is [[Symmetric matrix|symmetric]] and of [[Trace (linear algebra)|trace]] zero.

The matrix {{nowrap|''A'' + {{sqrt|5}}''{{math|I}}''}} induces thus a [[Euclidean space|Euclidean structure]] on the [[Quotient space (linear algebra)|quotient space]] {{nowrap|{{math|ℝ}}{{sup|6}} / ker(''A'' + {{sqrt|5}}''{{math|I}}'')}}, which is [[Isomorphism|isomorphic]] to {{math|ℝ}}{{sup|3}} since the [[Kernel (linear operator)|kernel]] {{nowrap|ker(''A'' + {{sqrt|5}}''{{math|I}}'')}} of {{nowrap|''A'' + {{sqrt|5}}''{{math|I}}''}} has [[dimension]] 3. The image under the [[Projection (linear algebra)|projection]] {{nowrap|{{pi}} : {{math|ℝ}}{{sup|6}} → {{math|ℝ}}{{sup|6}} / ker(''A'' + {{sqrt|5}}''{{math|I}}'')}} of the six coordinate axes {{math|ℝ}}''v''{{sub|1}}, …, {{math|ℝ}}''v''{{sub|6}} in {{math|ℝ}}{{sup|6}} forms thus a system of six equiangular lines in {{math|ℝ}}{{sup|3}} intersecting pairwise at a common acute angle of arccos&amp;nbsp;{{frac|1|{{sqrt|5}}}}. Orthogonal projection of ±''v''&lt;sub&gt;1&lt;/sub&gt;, …, ±''v''&lt;sub&gt;6&lt;/sub&gt; onto the [[Eigenvalue, eigenvector and eigenspace|{{sqrt|5}}-eigenspace]] of ''A'' yields thus the twelve vertices of the icosahedron.

A second straightforward construction of the icosahedron uses [[Group representation|representation theory]] of the [[alternating group]] ''A''&lt;sub&gt;5&lt;/sub&gt; acting by direct [[Isometry|isometries]] on the icosahedron.

==Symmetry==
[[File:Sphere symmetry group ih.png|thumb|Full [[Icosahedral symmetry]] has 15 mirror planes (seen as cyan [[great circle]]s on this sphere) meeting at order {{sfrac|{{pi}}|5}}, {{sfrac|{{pi}}|3}}, {{sfrac|{{pi}}|2}} angles, dividing a sphere into 120 triangle [[fundamental domain]]s. There are 6 5-fold axes (blue), 10 3-fold axes (red), and 15 2-fold axes (magenta). The vertices of the regular icosahedron exist at the 5-fold rotation axis points.]]
{{main|Icosahedral symmetry}}
The rotational [[symmetry group]] of the regular icosahedron is [[isomorphic]] to the [[alternating group]] on five letters. This non-[[abelian group|abelian]] [[simple group]] is the only non-trivial [[normal subgroup]] of the [[symmetric group]] on five letters. Since the [[Galois group]] of the general [[quintic equation]] is isomorphic to the symmetric group on five letters, and this normal subgroup is simple and non-abelian, the general quintic equation does not have a solution in radicals. The proof of the [[Abel–Ruffini theorem]] uses this simple fact, and [[Felix Klein]] wrote a book that made use of the theory of icosahedral symmetries to derive an analytical solution to the general quintic equation, {{Harv|Klein|1884}}. See [[Icosahedral symmetry#Related geometries|icosahedral symmetry: related geometries]] for further history, and related symmetries on seven and eleven letters.

The full symmetry group of the icosahedron (including reflections) is known as the [[full icosahedral group]], and is isomorphic to the product of the rotational symmetry group and the group ''C''&lt;sub&gt;2&lt;/sub&gt; of size two, which is generated by the reflection through the center of the icosahedron.

==Stellations==
The icosahedron has a large number of [[stellation]]s. According to specific rules defined in the book ''[[The Fifty-Nine Icosahedra]]'', 59 stellations were identified for the regular icosahedron. The first form is the icosahedron itself. One is a regular [[Kepler–Poinsot polyhedron]]. Three are [[Polyhedral compound#Regular compounds|regular compound polyhedra]].&lt;ref&gt;{{Citation |last1=Coxeter |first1=Harold Scott MacDonald |author1-link=Harold Scott MacDonald Coxeter |last2=Du Val |first2=P. |last3=Flather |first3=H.T. |last4=Petrie |first4=J.F. |title=The Fifty-Nine Icosahedra |publisher=Tarquin |edition=3rd |isbn=978-1-899618-32-3 |mr=676126  |year=1999}} (1st Edn University of Toronto (1938))&lt;/ref&gt;

{| class=wikitable
|+ 21 of 59 stellations
|-
|rowspan=3 width=200|[[File:Stellation diagram of icosahedron.svg|200px]]&lt;br&gt;The faces of the icosahedron extended outwards as planes intersect, defining regions in space as shown by this [[stellation diagram]] of the intersections in a single plane.
|[[Image:Zeroth stellation of icosahedron.png|50px]]
|[[Image:First stellation of icosahedron.png|50px]]
|[[Image:Second stellation of icosahedron.png|50px]]
|[[Image:Third stellation of icosahedron.png|50px]]
|[[Image:Fourth stellation of icosahedron.png|50px]]
|[[Image:Fifth stellation of icosahedron.png|50px]]
|[[Image:Sixth stellation of icosahedron.png|50px]]
|-
|[[Image:Seventh stellation of icosahedron.png|50px]]
|[[Image:Eighth stellation of icosahedron.png|50px]]
|[[Image:Ninth stellation of icosahedron.png|50px]]
|[[Image:Tenth stellation of icosahedron.png|50px]]
|[[Image:Eleventh stellation of icosahedron.png|50px]]
|[[Image:Twelfth stellation of icosahedron.png|50px]]
|[[Image:Thirteenth stellation of icosahedron.png|50px]]
|-
|[[Image:Fourteenth stellation of icosahedron.png|50px]]
|[[Image:Fifteenth stellation of icosahedron.png|50px]]
|[[Image:Sixteenth stellation of icosahedron.png|50px]]
|[[Image:Seventeenth stellation of icosahedron.png|50px]]
|[[Image:First compound stellation of icosahedron.png|50px]]
|[[Image:Second compound stellation of icosahedron.png|50px]]
|[[Image:Third compound stellation of icosahedron.png|50px]]
|}

== Facetings==
The [[small stellated dodecahedron]], [[great dodecahedron]], and [[great icosahedron]] are three [[faceting]]s of the regular icosahedron. They share the same [[vertex arrangement]]. They all have 30 edges. The regular icosahedron and great dodecahedron share the same [[edge arrangement]] but differ in faces (triangles vs pentagons), as do the small stellated dodecahedron and great icosahedron (pentagrams vs triangles).
{| class=wikitable
!Convex
!colspan=3|Regular stars
|-
!'''icosahedron'''
![[great dodecahedron]]
![[small stellated dodecahedron]]
![[great icosahedron]]
|- align=center
|[[File:Icosahedron.png|150px]]
|[[File:Great dodecahedron.png|150px]]
|[[File:Small stellated dodecahedron.png|150px]]
|[[File:Great icosahedron.png|150px]]
|}

==Geometric relations==
There are distortions of the icosahedron that, while no longer regular, are nevertheless [[vertex-uniform]]. These are [[invariant (mathematics)|invariant]] under the same [[rotation]]s as the tetrahedron, and are somewhat analogous to the [[snub cube]] and [[snub dodecahedron]], including some forms which are [[chirality (mathematics)|chiral]] and some with T&lt;sub&gt;h&lt;/sub&gt;-symmetry, i.e. have different planes of symmetry from the tetrahedron.

The icosahedron is unique among the [[Platonic solids]] in possessing a [[dihedral angle]] not less than 120°. Its dihedral angle is approximately 138.19°. Thus, just as hexagons have angles not less than 120° and cannot be used as the faces of a convex regular polyhedron because such a construction would not meet the requirement that at least three faces meet at a vertex and leave a positive [[defect (geometry)|defect]] for folding in three dimensions, icosahedra cannot be used as the [[cell (geometry)|cells]] of a convex regular [[polychoron]] because, similarly, at least three cells must meet at an edge and leave a positive defect for folding in four dimensions (in general for a convex [[polytope]] in ''n'' dimensions, at least three [[facet (mathematics)|facets]] must meet at a [[peak (geometry)|peak]] and leave a positive defect for folding in ''n''-space). However, when combined with suitable cells having smaller dihedral angles, icosahedra can be used as cells in semi-regular polychora (for example the [[snub 24-cell]]), just as hexagons can be used as faces in semi-regular polyhedra (for example the [[truncated icosahedron]]). Finally, non-convex polytopes do not carry the same strict requirements as convex polytopes, and icosahedra are indeed the cells of the [[icosahedral 120-cell]], one of the ten [[Schläfli–Hess polychoron|non-convex regular polychora]].

An icosahedron can also be called a [[Gyroelongated dipyramid|gyroelongated pentagonal bipyramid]]. It can be decomposed into a [[gyroelongated pentagonal pyramid]] and a [[pentagonal pyramid]] or into a [[pentagonal antiprism]] and two equal pentagonal pyramids.

===Relation to the 6-cube and rhombic triacontahedron===
:[[File:6demicube-odd-icosahedron.png|240px|left]]&lt;br&gt;
It can be projected to 3D from the 6D [[6-demicube]] using the same basis vectors that form the hull of the [[Rhombic triacontahedron]] from the [[6-cube]]. Shown here including the inner 20 vertices which are not connected by the 30 outer hull edges of 6D norm length {{sqrt|2}}. The inner vertices form an [[dodecahedron]].

&lt;br&gt;The 3D projection basis vectors [u,v,w] used are:
:u = (1, ''φ'', 0, -1, ''φ'', 0)
:v = (''φ'', 0, 1, ''φ'', 0, -1)
:w = (0, 1, ''φ'', 0, -1, ''φ'')

==Uniform colorings and subsymmetries==
[[File:Icosahedral subgroup tree.png|thumb|[[Icosahedral symmetry]] subgroups]]
There are 3 [[uniform coloring]]s of the icosahedron. These colorings can be represented as 11213, 11212, 11111, naming the 5 triangular faces around each vertex by their color.

The icosahedron can be considered a snub tetrahedron, as [[snub (geometry)|snubification]] of a regular tetrahedron gives a regular icosahedron having chiral [[tetrahedral symmetry]]. It can also be constructed as an alternated truncated octahedron, having [[pyritohedral symmetry]]. The pyritohedral symmetry version is sometimes called a [[pseudoicosahedron]], and is dual to the [[pyritohedron]].
{| class=wikitable
|- align=center
!
!Regular
!colspan=3|Uniform
!colspan=3|2-uniform
|- align=center
!Name
|Regular&lt;BR&gt;icosahedron
|[[Icosahedron#Pyritohedral symmetry|Snub&lt;BR&gt;octahedron]]
|[[Snub tetratetrahedron|Snub&lt;BR&gt;tetratetrahedron]]
|Snub square&lt;BR&gt;bipyramid
|Pentagonal&lt;br&gt;[[gyroelongated bipyramid|Gyroelongated&lt;BR&gt;bipyramid]]
|[[gyrobianticupola|Triangular&lt;BR&gt;gyrobianticupola]]
|Snub triangular&lt;BR&gt;antiprism&lt;ref&gt;[http://www.orchidpalms.com/polyhedra/snub-anti-prisms/prisms1.html Snub Anti-Prisms ]&lt;/ref&gt;
|- align=center
!Image
|[[Image:Uniform polyhedron-53-t2.png|80px]]
|[[Image:Uniform polyhedron-43-h01.svg|80px]]
|[[Image:Uniform polyhedron-33-s012.png|80px]]
|[[File:snub_square_bipyramid.png|80px]]
|[[Image:Pentagonal gyroelongated bipyramid.png|80px]]
|[[File:Regular triangular gyrobianticupola.png|80px]]
|[[File:snub_triangular_antiprism.png|80px]]
|- align=center valign=top
![[uniform coloring|Face&lt;BR&gt;coloring]]
|(11111)
|(11212)
|(11213)
|(11212)
|(11122)&lt;BR&gt;(22222)
|(12332)&lt;BR&gt;(23333)
|(11213)&lt;BR&gt;(11212)
|- align=center
![[Coxeter diagram|Coxeter&lt;BR&gt;diagram]]
|{{CDD|node_1|3|node|5|node}}
|{{CDD|node_h|3|node_h|4|node}}
|{{CDD|node_h|split1|nodes_hh}}
|
|
|
|
|- align=center
![[Schläfli symbol|Schläfli&lt;BR&gt;symbol]]
|{3,5}
|s{3,4}
|sr{3,3}
|sdt{2,4}
|() &lt;nowiki&gt;||&lt;/nowiki&gt; {n} &lt;nowiki&gt;||&lt;/nowiki&gt; r{n} &lt;nowiki&gt;||&lt;/nowiki&gt; ()
|
|ss{2,6}
|- align=center
![[Conway polyhedron notation|Conway]]
|I
|HtO
|sT
|HtdP4
|k5A5
|
|sY3 = HtA3
|- align=center
![[List of spherical symmetry groups|Symmetry]]
|I&lt;sub&gt;h&lt;/sub&gt;&lt;br&gt;[5,3]&lt;br&gt;(*532)
|T&lt;sub&gt;h&lt;/sub&gt;&lt;br&gt;[3&lt;sup&gt;+&lt;/sup&gt;,4]&lt;br&gt;(3*2)
|T&lt;br&gt;[3,3]&lt;sup&gt;+&lt;/sup&gt;&lt;br&gt;(332)
|D&lt;sub&gt;2h&lt;/sub&gt;&lt;br&gt;[2,2]&lt;br&gt;(*222)
|D&lt;sub&gt;5d&lt;/sub&gt;&lt;br&gt;[2&lt;sup&gt;+&lt;/sup&gt;,10]&lt;br&gt;(2*5)
|D&lt;sub&gt;3d&lt;/sub&gt;&lt;br&gt;[2&lt;sup&gt;+&lt;/sup&gt;,6]&lt;br&gt;(2*3)
|D&lt;sub&gt;3&lt;/sub&gt;&lt;br&gt;[3,2]&lt;sup&gt;+&lt;/sup&gt;&lt;br&gt;(322)
|- align=center
!Symmetry&lt;BR&gt;order
|60
|24
|12
|8
|20
|12
|6
|}

==Uses and natural forms==
[[File:Twin2.jpg|thumb|upright|[[Gold]] nanoparticle viewed in electron microscope. ]]
[[File:Gamma-bor.jpg|thumb|upright|Structure of γ-boron.]]

=== Biology ===
Many [[virus]]es, e.g. [[Herpesviridae|herpes virus]], have icosahedral [[capsid|shells]].&lt;ref&gt;C. Michael Hogan. 2010. [http://www.eoearth.org/article/Virus?topic=49496 ''Virus''. Encyclopedia of Earth. National Council for Science and the Environment]. eds. S. Draggan and C. Cleveland&lt;/ref&gt; Viral structures are built of repeated identical [[protein]] subunits known as [[capsomere]]s, and the icosahedron is the easiest shape to assemble using these subunits. A ''regular'' polyhedron is used because it can be built from a single basic unit protein used over and over again; this saves space in the viral [[genome]].

Various bacterial organelles with an icosahedral shape were also found.&lt;ref name=Bobik2007&gt;{{Citation |author=Bobik, T.A. |title=Bacterial Microcompartments |year=2007 |journal=Microbe |volume=2 |pages=25–31 |url=http://www.microbemagazine.org/index.php/01-2007-home/2308-bacterial-microcompartments |publisher=Am. Soc. Microbiol. |deadurl=yes |archiveurl=https://web.archive.org/web/20130729083846/http://www.microbemagazine.org/index.php/01-2007-home/2308-bacterial-microcompartments |archivedate=2013-07-29 |df= }}&lt;/ref&gt; The icosahedral shell encapsulating enzymes and labile intermediates are built of different types of proteins with [[BMC domain]]s.

In 1904, [[Ernst Haeckel]] described a number of species of [[Radiolaria]], including ''Circogonia icosahedra'', whose skeleton is shaped like a regular icosahedron. A copy of Haeckel's illustration for this radiolarian appears in the article on [[regular polyhedra]].

=== Chemistry ===
The [[closo cluster|closo]]-[[carboranes]] are chemical compounds with shape very close to icosahedron. [[icosahedral twins|Icosahedral]] [[Crystal twinning|twinning]] also occurs in crystals, especially [[nanoparticle]]s.

Many [[Crystal structure of boron-rich metal borides|borides]] and [[allotropes of boron]] contain boron B&lt;sub&gt;12&lt;/sub&gt; icosahedron as a basic structure unit.

=== Toys and games ===
[[Image:20-sided dice 250.jpg|100px|thumb|Twenty-sided [[dice|die]]]]
[[File:Twenty-sided die (icosahedron) with faces inscribed with Greek letters MET 10.130.1158 001.jpg|thumb|left|Twenty-sided die from [[Ptolemaic Kingdom|Ptolemaic Egypt]]]]
Icosahedral [[dice]] with twenty sides have been used since ancient times.&lt;ref&gt;Cromwell, Peter R. "Polyhedra" (1997) Page 327.&lt;/ref&gt;

In several [[roleplaying game]]s, such as ''[[Dungeons &amp; Dragons]]'', the twenty-sided die ([[Dice#Non-cubical dice|d20]] for short) is commonly used in determining success or failure of an action. This die is in the form of a regular icosahedron. It may be numbered from "0" to "9" twice (in which form it usually serves as a ten-sided die, or [[Dice#Non-cubical dice|d10]]), but most modern versions are labeled from "1" to "20". See [[d20 System]].

An icosahedron is the three-dimensional game board for Icosagame, formerly known as the Ico Crystal Game.

An icosahedron is used in the board game ''[[Scattergories]]'' to choose a letter of the alphabet. Six letters are omitted (Q, U, V, X, Y, and Z).

In the ''Nintendo 64'' game ''[[Kirby 64: The Crystal Shards]]'', the boss Miracle Matter is a regular icosahedron.

Inside a [[Magic 8-Ball]], various answers to [[yes-no question]]s are inscribed on a regular icosahedron.

=== Others ===

[[R. Buckminster Fuller]] and Japanese [[cartographer]] [[Shoji Sadao]]&lt;ref&gt;{{Cite web |url=http://library.stanford.edu/depts/spc/exhibits/fullersadao.html |title=Fuller and Sadao: Partners in Design |date=September 19, 2006 |accessdate=2010-01-26 |deadurl=yes |archiveurl=https://web.archive.org/web/20100816141849/http://library.stanford.edu/depts/spc/exhibits/fullersadao.html |archivedate=August 16, 2010 |df= }}&lt;/ref&gt; designed a world map in the form of an unfolded icosahedron, called the [[Fuller projection]], whose maximum [[distortion#Map projections|distortion]] is only 2%. The American [[electronic music]] duo [[Odesza|ODESZA]] use a regular icosahedron as their logo.

==Icosahedral graph==
{{Infobox graph
 | name = Regular icosahedron graph
 | image = [[File:Icosahedron graph.svg|160px]]
 | image_caption = 3-fold symmetry
 | namesake = 
 | vertices = 12
 | edges = 30
 | automorphisms = 120 ([[Symmetric group|S&lt;sub&gt;5&lt;/sub&gt;]])
 | radius = 3
 | diameter = 3
 | girth = 3
 | chromatic_number = 4
 | chromatic_index = 
 | fractional_chromatic_index = 
 | properties = [[Hamiltonian graph|Hamiltonian]], [[regular graph|regular]], [[symmetric graph|symmetric]], [[distance-regular graph|distance-regular]], [[distance-transitive graph|distance-transitive]], [[K-vertex-connected graph|3-vertex-connected]], [[planar graph]]
}}
The [[n-skeleton|skeleton]] of the icosahedron (the vertices and edges) forms a [[Graph (discrete mathematics)|graph]].  It is one of 5 [[Platonic graph]]s, each a skeleton of its [[Platonic solid]].

The high degree of symmetry of the polygon is replicated in the properties of this graph, which is [[distance-transitive graph|distance-transitive]] and [[symmetric graph|symmetric]]. The [[graph automorphism|automorphism group]] has order 120. The vertices can be [[graph coloring|colored]] with 4 colors, the edges with 5 colors, and the [[graph diameter|diameter]] is 3.&lt;ref&gt;{{MathWorld |urlname=IcosahedralGraph |title=Icosahedral Graph}}&lt;/ref&gt;

The icosahedral graph is [[Hamiltonian graph|Hamiltonian]]: there is a cycle containing all the vertices.  It is also a [[planar graph]].

{| class=wikitable
|+ [[Orthogonal projection]]
|[[File:Icosahedron A2 projection.svg|160px]]
|}
{{Clear}}

== Diminished regular icosahedra==
There are 4 related [[Johnson solid]]s, including pentagonal faces with a subset of the 12 vertices. The similar [[dissected regular icosahedron]] has 2 adjacent vertices diminished, leaving two trapezoidal faces, and a bifastigium has 2 opposite sets of vertices removed and 4 trapezoidal faces. The pentagonal antiprism is formed by removing two opposite vertices.
{| class=wikitable
!Form
![[Pentagonal pyramid|J2]]
![[Bifastigium]]
![[Tridiminished icosahedron|J63]]
![[Metabidiminished icosahedron|J62]]
![[Dissected regular icosahedron|Dissected&lt;BR&gt;icosahedron]]
![[Pentagonal antiprism|s{2,10}]]
![[Gyroelongated pentagonal pyramid|J11]]
|-
!Vertices
!6 of 12
!8 of 12
!9 of 12
!colspan=3|10 of 12
!11 of 12
|- align=center
![[List of finite spherical symmetry groups|Symmetry]]
|''C''&lt;sub&gt;5v&lt;/sub&gt;, [5], (*55)&lt;BR&gt;order 10
| D&lt;sub&gt;2h&lt;/sub&gt;, [2,2], *222&lt;BR&gt;order 8
|''C''&lt;sub&gt;3v&lt;/sub&gt;, [3], (*33)&lt;BR&gt;order 6
|colspan=2|''C''&lt;sub&gt;2v&lt;/sub&gt;, [2], (*22)&lt;BR&gt;order 4
|D&lt;sub&gt;5d&lt;/sub&gt;, [2&lt;sup&gt;+&lt;/sup&gt;,10], (2*5)&lt;BR&gt;order 20
|''C''&lt;sub&gt;5v&lt;/sub&gt;, [5], (*55)&lt;BR&gt;order 10
|-
!Image
|[[File:Pentagonal pyramid.png|80px]]
|[[File:4-diminished_icosahedron.png|80px]]
|[[File:Tridiminished icosahedron.png|80px]]
|[[File:Metabidiminished icosahedron.png|80px]]
|[[File:Dissected regular icosahedron.png|80px]]
|[[File:Pentagonal antiprism.png|80px]]
|[[File:Gyroelongated pentagonal pyramid.png|80px]]
|}

==Related polyhedra and polytopes==

The icosahedron can be transformed by a [[Truncation (geometry)|truncation]] sequence into its [[Dual polyhedron|dual]], the dodecahedron:
{{Icosahedral truncations}}

As a snub tetrahedron, and alternation of a truncated octahedron it also exists in the tetrahedral and octahedral symmetry families:
{{Tetrahedron family}}
{{Octahedral truncations}}

This polyhedron is topologically related as a part of sequence of regular polyhedra with [[Schläfli symbol]]s {3,''n''}, continuing into the [[Hyperbolic space|hyperbolic plane]].
{{Triangular regular tiling}}

The regular icosahedron, seen as a ''snub tetrahedron'', is a member of a sequence of [[Snub (geometry)|snubbed]] polyhedra and tilings with vertex figure (3.3.3.3.''n'') and [[Coxeter–Dynkin diagram]] {{CDD|node_h|n|node_h|3|node_h}}. These figures and their duals have (''n''32) rotational [[Orbifold notation|symmetry]], being in the Euclidean plane for ''n'' = 6, and hyperbolic plane for any higher ''n''. The series can be considered to begin with ''n'' = 2, with one set of faces degenerated into [[digon]]s.

{{Snub table}}

{{Order-5 regular tilings}}

The icosahedron can tessellate hyperbolic space in the [[order-3 icosahedral honeycomb]], with 3 icosahedra around each edge, 12 icosahedra around each vertex, with [[Schläfli symbol]] {3,5,3}. It is [[List of regular polytopes#Tessellations of hyperbolic 3-space|one of four regular tessellations]] in the hyperbolic 3-space.
{| class=wikitable width=240
|[[File:Hyperb icosahedral hc.png|240px]]&lt;br&gt;It is shown here as an edge framework in a [[Poincaré disk model]], with one icosahedron visible in the center.
|}

==See also==
*[[Geodesic grid]]s use an iteratively bisected icosahedron to generate grids on a sphere
*[[Icosahedral twins]]
*[[Infinite skew polyhedron]]
*[[Jessen's icosahedron]]
*[[Regular polyhedron]]
*[[Truncated icosahedron]]

==References==
&lt;!--See [[Wikipedia:Footnotes]] for instructions.--&gt;
{{reflist}}
{{refbegin}}
* {{Citation |last=Klein |first=Felix |authorlink =Felix Klein|title=Lectures on the ikosahedron and the solution of equations of the fifth degree |year=1888 |url=http://digital.library.cornell.edu/cgi/t/text/text-idx?c=math;cc=math;view=toc;subview=short;idno=03070001 |postscript=, Dover edition {{isbn|978-0-486-49528-6}}}}, translated from&lt;br&gt;{{cite book |last =Klein |first =Felix |title =Vorlesungen über das Ikosaeder und die Auflösung der Gleichungen vom fünften Grade |publisher =Teubner |year =1884 }}
{{refend}}

==External links==
{{commons category|Icosahedron}}
{{Wikisource1911Enc|Icosahedron}}
{{Wiktionary|icosahedron}}
*{{KlitzingPolytopes|polyhedra.htm|3D convex uniform polyhedra|x3o5o – ike}}
*{{cite web|first=Michael|last=Hartley| url=http://www.dr-mikes-math-games-for-kids.com/polyhedral-nets.html?net=PKYcLAHcKk9lKCGWdoeJHvnDe7jSJyKXIghPYlKcV5PxkeWqGqwmeoyXUilmvzkXsQnxEoduPHUYFqOf20B8EwLz8CufLOUuc4N5&amp;name=Icosahedron#applet |title=Dr Mike's Math Games for Kids}}
*[http://www.kjmaclean.com/Geometry/GeometryHome.html K.J.M. MacLean, A Geometric Analysis of the Five Platonic Solids and Other Semi-Regular Polyhedra]
*[http://www.georgehart.com/virtual-polyhedra/vp.html Virtual Reality Polyhedra] The Encyclopedia of Polyhedra
*[https://web.archive.org/web/20040922084928/http://www.tulane.edu/~dmsander/WWW/335/335Structure.html Tulane.edu] A discussion of viral structure and the icosahedron
*[https://www.flickr.com/photos/pascalin/sets/72157594234292561/ Origami Polyhedra] – Models made with Modular Origami
*[https://www.youtube.com/watch?v=r6JQWCi_lNM Video of icosahedral mirror sculpture]
*[https://archive.is/20121224154747/http://web.uct.ac.za/depts/mmi/stannard/virarch.html] Principle of virus architecture

{{Polyhedra}}
{{Polyhedron navigator}}
{{Polytopes}}
{{Icosahedron stellations}}

[[Category:Deltahedra]]
[[Category:Planar graphs]]
[[Category:Platonic solids]]</text>
      <sha1>nn7wd7pr9clko2mxv3w7fvd122ducfc</sha1>
    </revision>
  </page>
  <page>
    <title>Relationship between mathematics and physics</title>
    <ns>0</ns>
    <id>42877569</id>
    <revision>
      <id>861868458</id>
      <parentid>861868332</parentid>
      <timestamp>2018-09-30T16:45:27Z</timestamp>
      <contributor>
        <username>Citation bot</username>
        <id>7903804</id>
      </contributor>
      <minor/>
      <comment>Alter: pages, isbn. Add: issue, pmid. Removed parameters. Formatted [[WP:ENDASH|dashes]]. You can [[WP:UCB|use this bot]] yourself. [[WP:DBUG|Report bugs here]]. | [[User:Headbomb|Headbomb]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="18741">[[File:CyloidPendulum.png|right|thumb|A [[cycloidal pendulum]] is isochronous, a fact discovered and proved by [[Christiaan Huygens]] under certain mathematical assumptions.&lt;ref&gt;{{cite book|author1=Jed Z. Buchwald|author2=Robert Fox|title=The Oxford Handbook of the History of Physics|url=https://books.google.com/books?id=1SxoAgAAQBAJ&amp;pg=PA128|date=10 October 2013|publisher=OUP Oxford|isbn=978-0-19-151019-9|pages=128}}&lt;/ref&gt;]]

[[File:Conic Sections.svg|thumb|240px|right|Mathematics was developed by the [[Ancient Greece|Ancient Greeks]] for intellectual challenge and pleasure. Surprisingly, many of their discoveries later played prominent roles in physical theories, as in the case of the conic sections in [[celestial mechanics]].]]

The '''relationship between mathematics and physics''' has been a subject of study of [[philosopher]]s, [[mathematician]]s and [[physicist]]s since [[Classical antiquity|Antiquity]], and more recently also by [[historian]]s and [[educator]]s.&lt;ref&gt;{{cite journal|last1=Uhden|first1=Olaf|last2=Karam|first2=Ricardo|last3=Pietrocola|first3=Maurício|last4=Pospiech|first4=Gesche|title=Modelling Mathematical Reasoning in Physics Education|journal=Science &amp; Education|date=20 October 2011|volume=21|issue=4|pages=485–506|doi=10.1007/s11191-011-9396-6|bibcode = 2012Sc&amp;Ed..21..485U }}&lt;/ref&gt; Generally considered a relationship of great intimacy,&lt;ref&gt;{{cite book|author1=Francis Bailly|author2=Giuseppe Longo|title=Mathematics and the Natural Sciences: The Physical Singularity of Life|url=https://books.google.com/books?id=7-dGHyIyI-AC&amp;pg=PA149|year=2011|publisher=World Scientific|isbn=978-1-84816-693-6|pages=149}}&lt;/ref&gt; [[mathematics]] has already been described as "an essential tool for physics"&lt;ref&gt;{{cite book|author1=Sanjay Moreshwar Wagh|author2=Dilip Abasaheb Deshpande|title=Essentials of Physics|url=https://books.google.com/books?id=-DmfVjBUPksC&amp;pg=PA3|date=27 September 2012|publisher=PHI Learning Pvt. Ltd.|isbn=978-81-203-4642-0|pages=3}}&lt;/ref&gt; and [[physics]] has already been described as "a rich source of inspiration and insight in mathematics".&lt;ref&gt;{{cite conference |url=http://www.mathunion.org/ICM/ICM1990.1/Main/icm1990.1.0031.0036.ocr.pdf |title=On the Work of Edward Witten |last1=Atiyah |first1=Michael |author-link1=Michael Atiyah |year=1990 |conference=International Congress of Mathematicians |publisher= |book-title= |pages=31–35 |location=Japan |id= |deadurl=yes |archiveurl=https://web.archive.org/web/20170301004342/http://www.mathunion.org/ICM/ICM1990.1/Main/icm1990.1.0031.0036.ocr.pdf |archivedate=2017-03-01 |df= }}&lt;/ref&gt;

In his work ''[[Physics (Aristotle)|Physics]]'', one of the topics treated by [[Aristotle]] is about how the study carried out by mathematicians differs from that carried out by physicists.&lt;ref&gt;{{cite book|last1=Lear|first1=Jonathan|title=Aristotle: the desire to understand|date=1990|publisher=Cambridge Univ. Press|location=Cambridge [u.a.]|isbn=9780521347624|page=232|edition=Repr.}}&lt;/ref&gt; Considerations about mathematics being the language of [[nature]] can be found in the ideas of the [[Pythagoreans]]: the convictions that "Numbers rule the world" and "All is number",&lt;ref&gt;{{cite book|author1=Gerard Assayag|author2=Hans G. Feichtinger|author3=José-Francisco Rodrigues|title=Mathematics and Music: A Diderot Mathematical Forum|url=https://books.google.com/books?id=bjsD8ClsFKEC&amp;pg=PA216|date=10 July 2002|publisher=Springer|isbn=978-3-540-43727-7|pages=216}}&lt;/ref&gt;&lt;ref&gt;{{cite web|first=Ibrahim |last=Al-Rasasi |title= All is number |url=http://faculty.kfupm.edu.sa/math/irasasi/Allisnumber.pdf |publisher=King Fahd University of Petroleum and Minerals |date=21 June 2004 |accessdate=13 June 2015}}&lt;/ref&gt; and two millennia later were also expressed by [[Galileo Galilei]]: "The book of nature is written in the language of mathematics".&lt;ref&gt;{{cite book|author=Aharon Kantorovich|title=Scientific Discovery: Logic and Tinkering|url=https://books.google.com/books?id=vMFc43w0FfEC&amp;pg=PA59|date=1 July 1993|publisher=SUNY Press|isbn=978-0-7914-1478-1|pages=59}}&lt;/ref&gt;&lt;ref&gt;Kyle Forinash, William Rumsey, Chris Lang, [http://homepages.ius.edu/kforinas/K/pdf/Galileo.pdf Galileo's Mathematical Language of Nature].&lt;/ref&gt;

Before giving a [[mathematical proof]] for the formula for the [[volume]] of a [[sphere]], [[Archimedes]] used physical reasoning to discover the solution (imagining the balancing of bodies on a scale).&lt;ref&gt;{{cite book|author=Arthur Mazer|title=The Ellipse: A Historical and Mathematical Journey|url=https://books.google.com/books?id=twWkDe1Y9YQC&amp;pg=SA5-PA28|date=26 September 2011|publisher=John Wiley &amp; Sons|isbn=978-1-118-21143-4|pages=5}}&lt;/ref&gt; From the seventeenth century, many of the most important advances in mathematics appeared motivated by the study of physics, and this continued in the following centuries (although, it has already been appointed that from the nineteenth century, mathematics started to become increasingly independent from physics).&lt;ref name="post"&gt;E. J. Post, [http://www22.pair.com/csdc/pdf/philos.pdf A History of Physics as an Exercise in Philosophy, p. 76.]&lt;/ref&gt;&lt;ref&gt;Arkady Plotnitsky, [https://books.google.com/books?id=dmdUp97S4AYC&amp;pg=PA177 Niels Bohr and Complementarity: An Introduction, p. 177].&lt;/ref&gt; The creation and development of [[calculus]] were strongly linked to the needs of physics:&lt;ref&gt;{{cite book|author=Roger G. Newton|title=The Truth of Science: Physical Theories and Reality|url=https://books.google.com/books?id=SzxsjN3t4i0C&amp;pg=PA125|year=1997|publisher=Harvard University Press|isbn=978-0-674-91092-8|pages=125–126}}&lt;/ref&gt; There was a need for a new mathematical language to deal with the new [[Dynamics (mechanics)|dynamics]] that had arisen from the work of scholars such as Galileo Galilei and [[Isaac Newton]].&lt;ref&gt;Eoin P. O'Neill (editor), [https://books.google.com/books?id=h8TaAAAAMAAJ&amp;redir_esc=y What Did You Do Today, Professor?: Fifteen Illuminating Responses from Trinity College Dublin, p. 62].&lt;/ref&gt; During this period there was little distinction between physics and mathematics;&lt;ref&gt;{{cite book|author1=Timothy Gowers|author-link=Timothy Gowers|author2=June Barrow-Green|author3=Imre Leader|title=The Princeton Companion to Mathematics|url=https://books.google.com/books?id=ZOfUsvemJDMC&amp;pg=PA7|date=18 July 2010|publisher=Princeton University Press|isbn=978-1-4008-3039-8|pages=7}}&lt;/ref&gt; as an example, Newton regarded [[geometry]] as a branch of [[mechanics]].&lt;ref&gt;{{cite journal|author=David E. Rowe|author-link=David E. Rowe|title=Euclidean Geometry and Physical Space|journal=[[The Mathematical Intelligencer]]|year=2008|volume=28|issue=2|pages=51–59|doi=10.1007/BF02987157}}&lt;/ref&gt; As time progressed, increasingly sophisticated mathematics started to be used in physics. The current situation is that the mathematical knowledge used in physics is becoming increasingly sophisticated, as in the case of [[superstring theory]].&lt;ref&gt;{{cite web |url=http://www.particlecentral.com/strings_page.html |title=String theories |author=&lt;!--Staff writer(s); no by-line.--&gt; |website=Particle Central |publisher=Four Peaks Technologies |access-date=13 June 2015 }}&lt;/ref&gt;

== Philosophical problems ==
Some of the problems considered in the [[philosophy of mathematics]] are the following:

*Explain the effectiveness of mathematics in the study of the physical world: "At this point an enigma presents itself which in all ages has agitated inquiring minds. How can it be that mathematics, being after all a product of human thought which is independent of experience, is so admirably appropriate to the objects of reality?" —[[Albert Einstein]], in ''Geometry and Experience'' (1921).&lt;ref&gt;[[Albert Einstein]], [http://www-groups.dcs.st-and.ac.uk/~history/Extras/Einstein_geometry.html Geometry and Experience].&lt;/ref&gt;
*Clearly delineate mathematics and physics: For some results or discoveries, it is difficult to say to which area they belong: to the mathematics or to physics.&lt;ref&gt;Pierre Bergé, [https://books.google.com/books?id=umFTtQAACAAJ&amp;dq=%22Des+rythmes+au+chaos%22&amp;hl=pt-BR&amp;sa=X&amp;ei=65aEU7LAKeHL8AHc74HwBg&amp;ved=0CC4Q6AEwAA Des rythmes au chaos].&lt;/ref&gt;
*What is the geometry of physical space?&lt;ref&gt;{{cite book|author=Gary Carl Hatfield|title=The Natural and the Normative: Theories of Spatial Perception from Kant to Helmholtz|url=https://books.google.com/books?id=JikeeDbYeUQC&amp;pg=PA223|year=1990|publisher=MIT Press|isbn=978-0-262-08086-6|page=223}}&lt;/ref&gt;
*What is the origin of the axioms of mathematics?&lt;ref&gt;{{cite book|author1=Gila Hanna|author2=Hans Niels Jahnke|author3=Helmut Pulte|title=Explanation and Proof in Mathematics: Philosophical and Educational Perspectives|url=https://books.google.com/books?id=3bLHye8kSAwC&amp;pg=PA29|date=4 December 2009|publisher=Springer Science &amp; Business Media|isbn=978-1-4419-0576-5|pages=29–30}}&lt;/ref&gt;
*How does the already existing mathematics influence in the creation and development of [[Theoretical physics#Overview|physical theories]]?&lt;ref&gt;{{cite web |url=http://fqxi.org/community/essay/rules |title=FQXi Community Trick or Truth: the Mysterious Connection Between Physics and Mathematics |access-date=16 April 2015}}&lt;/ref&gt;
*Is arithmetic ''a priori'' or synthetic? (from [[Immanuel Kant|Kant]], see [[Analytic–synthetic distinction]])&lt;ref&gt;{{cite book|author=James Van Cleve Professor of Philosophy Brown University|title=Problems from Kant|url=https://books.google.com/books?id=6WHAgt-Mg1AC&amp;pg=PA22|date=16 July 1999|publisher=Oxford University Press, USA|isbn=978-0-19-534701-2|pages=22}}&lt;/ref&gt;
*What is essentially different between doing a physical experiment to see the result and making a mathematical calculation to see the result? (from the [[Alan Turing|Turing]]–[[Ludwig Wittgenstein|Wittgenstein]] debate)&lt;ref&gt;{{cite book|author1=Ludwig Wittgenstein|author2=R. G. Bosanquet|author3=Cora Diamond|title=Wittgenstein's Lectures on the Foundations of Mathematics, Cambridge, 1939|url=https://books.google.com/books?id=d4YUZVq1JSEC&amp;pg=PA96|date=15 October 1989|publisher=University of Chicago Press|isbn=978-0-226-90426-9|page=96}}&lt;/ref&gt;
*Do [[Gödel's incompleteness theorems]] imply that physical theories will always be incomplete? (from [[Stephen Hawking]])&lt;ref&gt;{{cite book|first=Pavel|last=Pudlák|title=Logical Foundations of Mathematics and Computational Complexity: A Gentle Introduction|url=https://books.google.com/books?id=obxDAAAAQBAJ&amp;pg=PA659|year=2013|publisher=Springer Science &amp; Business Media|isbn=978-3-319-00119-7|page=659}}&lt;/ref&gt;&lt;ref&gt;[http://www.hawking.org.uk/godel-and-the-end-of-physics.html Stephen Hawking. "Godel and the End of the Universe"]&lt;/ref&gt;
*Is math invented or discovered? (millennium-old question, raised among others by [[Mario Livio]])&lt;ref&gt;{{Cite journal | author = Mario Livio | authorlink = Mario Livio| doi =  | title = Why math works? | journal = Scientific American | volume =  | pages = 80–83 | year = August 2011 | url = http://www.scientificamerican.com/article/why-math-works/| pmid =  | pmc = }}&lt;/ref&gt;

== Education ==
In recent times the two disciplines have most often been taught separately, despite all the interrelations between physics and mathematics.&lt;ref&gt;Karam; Pospiech; &amp; Pietrocola (2010). "[http://www.univ-reims.fr/site/evenement/girep-icpe-mptl-2010-reims-international-conference/gallery_files/site/1/90/4401/22908/29476/30505.pdf Mathematics in physics lessons: developing structural skills]"&lt;/ref&gt; This led some professional mathematicians who were also interested in [[mathematics education]], such as [[Felix Klein]], [[Richard Courant]], [[Vladimir Arnold]] and [[Morris Kline]], to strongly advocate teaching mathematics in a way more closely related to the physical sciences.&lt;ref&gt;Stakhov "[http://www2.fisica.unlp.edu.ar/materias/algebralineal/documentos/mathharm.pdf Dirac’s Principle of Mathematical Beauty, Mathematics of Harmony]"&lt;/ref&gt;&lt;ref&gt;{{cite book|author1=Richard Lesh|author2=Peter L. Galbraith|author3=Christopher R. Haines|author4=Andrew Hurford|title=Modeling Students' Mathematical Modeling Competencies: ICTMA 13|url=https://books.google.com/books?id=Jj5tfi2594kC&amp;pg=PA14|year=2009|publisher=Springer|isbn=978-1-4419-0561-1|page=14}}&lt;/ref&gt;

== See also ==
{{Col-begin}}
{{Col-1-of-2}}
*[[Pure mathematics]]
*[[Applied mathematics]]
*[[Theoretical physics]]
*[[Mathematical physics]]
*[[Non-Euclidean geometry]]
*[[Fourier series]]
*[[Conic section]]
*[[Kepler's laws of planetary motion]]
*[[Saving the phenomena]]
*[[Positron#History]]
* [[The Unreasonable Effectiveness of Mathematics in the Natural Sciences]]
*[[Mathematical universe hypothesis]]
{{Col-2-of-2}}
*[[Zeno's paradoxes]]
*[[Axiomatic system]]
*[[Mathematical model]]
*[[Hilbert's sixth problem]]
*[[Empiricism]]
*[[Formalism (mathematics)]]
*[[Mathematics of general relativity]]
*[[Nicolas Bourbaki|Bourbaki]]
*[[Experimental mathematics]]
*[[History of Maxwell's equations]]
*[[Philosophy of mathematics#Platonism]]
*[[History of astronomy]]
{{col-end}}

== References ==
{{reflist}}

== Further reading ==
*{{cite journal|last=Arnold|first=V. I.|author-link=Vladimir Arnold |title=Mathematics and physics: mother and daughter or sisters?|journal=Physics-Uspekhi|volume=42|issue=12|pages=1205–1217|url=http://iopscience.iop.org/1063-7869/42/12/A03/|others=|year=1999|accessdate=30 May 2014|doi=10.1070/pu1999v042n12abeh000673|bibcode = 1999PhyU...42.1205A }}
*{{cite journal|last=Arnold|first=V. I.|author-link=Vladimir Arnold |title=On teaching mathematics|journal=Russian Mathematical Surveys|volume=53|issue=1|pages=229&amp;ndash;236|url=http://pauli.uni-muenster.de/~munsteg/arnold.html|translator= A. V. Goryunov |year=1998|accessdate=29 May 2014|bibcode = 1998RuMaS..53..229A |doi = 10.1070/RM1998v053n01ABEH000005 }}
*{{cite journal|last=Atiyah|first=M.|author-link=Michael Atiyah |title=Geometry and physics|journal=Philosophical Transactions of the Royal Society A: Mathematical, Physical and Engineering Sciences|date=1 February 2010|volume=368|issue=1914|pages=913–926|doi=10.1098/rsta.2009.0227 |pmid=20123740|url=http://rsta.royalsocietypublishing.org/content/368/1914/913.full.pdf |accessdate=29 May 2014|last2=Dijkgraaf|first2=R.|last3=Hitchin|first3=N.|bibcode = 2010RSPTA.368..913A |pmc=3263806}}
*{{cite book|editor-last=Boniolo|editor-first=Giovanni|editor2-first=Paolo |editor2-last=Budinich |editor3-first=Majda |editor3-last=Trobok |title=The Role of Mathematics in Physical Sciences: Interdisciplinary and Philosophical Aspects|date=2005|publisher=Springer|location=Dordrecht|isbn=9781402031069}}
*{{cite journal |first=Mark |last=Colyvan |url=http://www.colyvan.com/papers/miracle.pdf |format=pdf |title=The Miracle of Applied Mathematics |journal=Synthese |volume=127 |issue=3 |pages=265&amp;ndash;277 |year=2001 |accessdate=30 May 2014 | doi = 10.1023/A:1010309227321}}
*{{cite journal |first=Paul |last=Dirac |author-link=Paul Dirac |url=http://www.damtp.cam.ac.uk/events/strings02/dirac/speach.html |title=The Relation between Mathematics and Physics |journal = Proceedings of the Royal Society of Edinburgh |volume=59 Part II |year=1938–1939 |pages=122&amp;ndash;129 |accessdate=30 March 2014}}
*{{cite book|last=Feynman|first=Richard P. |authorlink=Richard Feynman |chapter=The Relation of Mathematics to Physics |title=The Character of Physical Law|date=1992|publisher=Penguin Books|location=London|isbn=978-0140175059|pages=35&amp;ndash;58|edition=Reprint}}
*{{cite book|first=G. H. |last=Hardy|authorlink=G. H. Hardy |url=http://www.math.ualberta.ca/mss/misc/A%20Mathematician's%20Apology.pdf |title=A Mathematician's Apology |edition=First electronic |year=2005 |publisher=University of Alberta Mathematical Sciences Society |accessdate=30 May 2014}}
*{{cite journal|last=Hitchin|first=Nigel|author-link=Nigel Hitchin |title=Interaction between mathematics and physics|journal=ARBOR Ciencia, Pensamiento y Cultura|volume=725|url=http://arbor.revistas.csic.es/index.php/arbor/article/viewFile/115/116|year=2007|accessdate=31 May 2014}}
*{{Cite journal|arxiv=1212.5854|last1=Harvey|first1=Alex|title=The Reasonable Effectiveness of Mathematics in the Physical Sciences|journal=Relativity and Gravitation|volume=43|issue=2011|pages=3057–3064|year=2012|doi=10.1007/s10714-011-1248-9|bibcode = 2011GReGr..43.3657H }}
*{{cite journal |first=John von |last=Neumann |author-link=John von Neumann |title=The Mathematician |journal=Works of the Mind|volume=1|number=1|pages=180&amp;ndash;196 |year=1947}} ([http://www-history.mcs.st-andrews.ac.uk/Extras/Von_Neumann_Part_1.html part 1]) ([http://www-history.mcs.st-andrews.ac.uk/Extras/Von_Neumann_Part_2.html part 2]).
*{{cite book|first=Henri |last=Poincaré |author-link = Henri Poincaré |translator=George Bruce Halsted |title=The Value of Science |publisher=The Science Press|location=New York |year=1907 |url=http://www3.nd.edu/~powers/ame.60611/poincare.pdf}}
*{{cite book|editor-last=Schlager|editor-first=Neil|editor2-first=Josh |editor2-last=Lauer |chapter= The Intimate Relation between Mathematics and Physics|title=Science and Its Times: Understanding the Social Significance of Scientific Discovery|volume=7: 1950 to Present|date=2000|publisher=Gale Group|isbn=978-0-7876-3939-6|pages=226&amp;ndash;229}}
*{{cite book|last=Vafa|first=Cumrun |authorlink=Cumrun Vafa |chapter=On the Future of Mathematics/Physics Interaction|title=Mathematics: Frontiers and Perspectives|date=2000|publisher=AMS|location=USA|isbn=978-0-8218-2070-4|pages=321&amp;ndash;328}}
*{{cite conference|first=Edward|last=Witten|author-link=Edward Witten |title=Physics and Geometry |conference=Proceedings of the International Conference of Mathematicians |location = Berkeley, California |year=1986 |pages=267&amp;ndash;303 |url=http://www.mathunion.org/ICM/ICM1986.1/Main/icm1986.1.0267.0306.ocr.pdf}}
*{{Cite journal|author=Eugene Wigner|author-link=Eugene Wigner|title=The Unreasonable Effectiveness of Mathematics in the Natural Sciences|journal=[[Communications on Pure and Applied Mathematics]]|volume=13|issue=1|pages=1–14|year=1960|url=http://www.dartmouth.edu/~matc/MathDrama/reading/Wigner.html|doi=10.1002/cpa.3160130102|bibcode = 1960CPAM...13....1W }}

== External links ==
* [http://www.physics.rutgers.edu/~gmoore/PhysicalMathematicsAndFuture.pdf Gregory W. Moore – Physical Mathematics and the Future (July 4, 2014)]
* [http://www.iop.org/publications/iop/2014/file_64122.pdf IOP Institute of Physics – Mathematical Physics: What is it and why do we need it? (September 2014)]

[[Category:Philosophy of physics]]
[[Category:Philosophy of mathematics]]
[[Category:History of science]]
[[Category:Mathematics education]]
[[Category:Physics education]]
[[Category:Foundations of mathematics]]
[[Category:History of mathematics]]
[[Category:History of physics]]</text>
      <sha1>41ydwooivopqc8bz7pvwez5drzykwn4</sha1>
    </revision>
  </page>
  <page>
    <title>Reservoir sampling</title>
    <ns>0</ns>
    <id>25190127</id>
    <revision>
      <id>867188550</id>
      <parentid>867188203</parentid>
      <timestamp>2018-11-04T06:30:27Z</timestamp>
      <contributor>
        <username>HaykinCS</username>
        <id>31414122</id>
      </contributor>
      <comment>/* Example: Sample size 10 */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19634">{{expert needed|computing|date=February 2010}}

'''Reservoir sampling''' is a family of [[randomized algorithm]]s for randomly choosing a [[Sampling (statistics)|sample]] of &lt;math&gt;k&lt;/math&gt; items from a list &lt;math&gt;S&lt;/math&gt; containing &lt;math&gt;n&lt;/math&gt; items, where &lt;math&gt;n&lt;/math&gt; is either a very large or unknown number. Typically, &lt;math&gt;n&lt;/math&gt; is too large to fit the whole list into [[main memory]].

== Example: Sample size 10 ==
Suppose we see a sequence of items, one at a time. We want to keep ten items in memory, and we want them to be selected at random from the sequence. If we know the total number of items &lt;math&gt;n&lt;/math&gt;, then the solution is easy: select 10 distinct indices &lt;math&gt;i&lt;/math&gt; between 1 and &lt;math&gt;n&lt;/math&gt; with equal probability, and keep the &lt;math&gt;i&lt;/math&gt;-th elements. The problem is that we do not always know the exact &lt;math&gt;n&lt;/math&gt; in advance. A possible solution is the following:
* Keep the first ten items in memory.
* When the {{mvar|i}}-th item arrives (for &lt;math&gt;i&gt;10&lt;/math&gt;):
** with probability &lt;math&gt;10/i&lt;/math&gt;, keep the new item (discard an old one, selecting which to replace at random, each with chance 1/10)
** with probability &lt;math&gt;1-10/i&lt;/math&gt;, keep the old items (ignore the new one)

Thus,
* when there are 10 items or fewer, each is kept with probability 1; 
* when there are 11 items, each of them is kept with probability &lt;math&gt;10/11&lt;/math&gt;; for the old items, that is &lt;math&gt;(1)(1/11 + (10/11)(9/10)) = 1/11 + 9/11 = 10/11&lt;/math&gt;
**In other words, the 10 old items are kept either if the new one is not selected &lt;math&gt;1/11&lt;/math&gt; or the new one is selected to replace one of the other 9 items &lt;math&gt;10/11 \times 9/10&lt;/math&gt;, and since "or" is represented with addition, we derive &lt;math&gt;(1/11 + (10/11)(9/10))&lt;/math&gt;.
* when there are 12 items, the twelfth item is kept with probability &lt;math&gt;10/12&lt;/math&gt;, and each of the previous 11 items are also kept with probability &lt;math&gt;(10/11)(2/12 + (10/12)(9/10)) = (10/11)(11/12) = 10/12 &lt;/math&gt;;
* by induction, it is easy to prove that when there are &lt;math&gt;n&lt;/math&gt; items, each item is kept with probability &lt;math&gt;10/n&lt;/math&gt;.

== Algorithm R ==
The most common example was labelled ''Algorithm R'' by [[Jeffrey Vitter]] in his paper on the subject.&lt;ref&gt;{{cite journal|last1=Vitter|first1=Jeffrey S.|title=Random sampling with a reservoir|journal=ACM Transactions on Mathematical Software|date=1 March 1985|volume=11|issue=1|pages=37–57|doi=10.1145/3147.3165|url=http://www.cs.umd.edu/~samir/498/vitter.pdf}}&lt;/ref&gt; This simple O(''n'') algorithm as described in the ''Dictionary of Algorithms and Data Structures''&lt;ref&gt;{{cite web
|title=Reservoir sampling
|work=Dictionary of Algorithms and Data Structures
|first=Paul E.
|last=Black
|date= 26 January 2015
|accessdate=2017-02-09
|url= https://xlinux.nist.gov/dads/HTML/reservoirSampling.html
}}&lt;/ref&gt; consists of the following steps (assuming ''k &lt; n'' and using one-based array [[Zero-based numbering|indexing]]):

&lt;source lang="pascal"&gt;
(*
  S has items to sample, R will contain the result
 *)
ReservoirSample(S[1..n], R[1..k])
  // fill the reservoir array
  for i = 1 to k
      R[i] := S[i]

  // replace elements with gradually decreasing probability
  for i = k+1 to n
    j := random(1, i)   // important: inclusive range
    if j &lt;= k
        R[j] := S[i]
&lt;/source&gt;

The algorithm creates a "reservoir" array of size &lt;math&gt;k&lt;/math&gt; and populates it with the first &lt;math&gt;k&lt;/math&gt; items of &lt;math&gt;S&lt;/math&gt;. It then iterates through the remaining elements of &lt;math&gt;S&lt;/math&gt; until &lt;math&gt;S&lt;/math&gt; is exhausted. At the {{ordinal|&lt;math&gt;i&lt;/math&gt;|sup=yes}} element of &lt;math&gt;S&lt;/math&gt;, the algorithm generates a random number &lt;math&gt;j&lt;/math&gt; between 1 and &lt;math&gt;i&lt;/math&gt;. If &lt;math&gt;j&lt;/math&gt; is less than or equal to &lt;math&gt;k&lt;/math&gt;, the {{ordinal|&lt;math&gt;j&lt;/math&gt;|sup=yes}} element of the reservoir array is replaced with the {{ordinal|&lt;math&gt;i&lt;/math&gt;|sup=yes}} element of &lt;math&gt;S&lt;/math&gt;. In effect, for all &lt;math&gt;i&lt;/math&gt;, the {{ordinal|&lt;math&gt;i&lt;/math&gt;|sup=yes}} element of &lt;math&gt;S&lt;/math&gt; is chosen to be included in the reservoir with probability &lt;math&gt;k/i&lt;/math&gt;. Similarly, at each iteration the {{ordinal|&lt;math&gt;j&lt;/math&gt;|sup=yes}} element of the reservoir array is chosen to be replaced with probability &lt;math&gt;1/k * k/i&lt;/math&gt;, which simplifies to &lt;math&gt;1/i&lt;/math&gt;. It can be shown that when the algorithm has finished executing, each item in &lt;math&gt;S&lt;/math&gt; has equal probability (i.e. &lt;math&gt;k/length(S)&lt;/math&gt;) of being chosen for the reservoir.

To see this, consider the following proof by [[Mathematical induction|induction]]. After the {{ordinal|&lt;math&gt;(i-1)&lt;/math&gt;|sup=yes}} round, let us assume, the probability of a number being in the reservoir array is &lt;math&gt;k/(i-1)&lt;/math&gt;. Since the probability of the number being replaced in the {{ordinal|&lt;math&gt;i&lt;/math&gt;|sup=yes}} round is &lt;math&gt;1/i&lt;/math&gt;, the probability that it survives the {{ordinal|&lt;math&gt;i&lt;/math&gt;|sup=yes}} round is &lt;math&gt;(i-1)/i&lt;/math&gt;. Thus, the probability that a given number is in the reservoir after the {{ordinal|&lt;math&gt;i&lt;/math&gt;|sup=yes}} round is the product of these two probabilities, i.e. the probability of being in the reservoir after the {{ordinal|&lt;math&gt;(i-1)&lt;/math&gt;|sup=yes}} round, and surviving replacement in the {{ordinal|&lt;math&gt;i&lt;/math&gt;|sup=yes}} round. This is &lt;math&gt;(k/(i-1)) * ((i-1)/i)=k/i&lt;/math&gt;. Hence, the result holds for &lt;math&gt;i&lt;/math&gt;, and is therefore true by induction.

== Reservoir with Random Sort ==
A simple reservoir-based algorithm can be designed using random sort&lt;ref name=Sunter1977&gt;{{cite journal|last1=Sunter|first1=A. B.|title=List Sequential Sampling with Equal or Unequal Probabilities without Replacement|journal=Applied Statistics|date=1977|volume=26|issue=3|pages=261|doi=10.2307/2346966}}&lt;/ref&gt; and implemented using priority queue data structure. This algorithm assigns random number as keys to each item and maintain k items with minimum value for keys. In essence, this is equivalent to assigning a random number to each item as key, sorting items using these keys and taking top k items. The worse case run time of the algorithm is &lt;math&gt;O(n \log k)&lt;/math&gt; while the best case runtime is &lt;math&gt;O(n)&lt;/math&gt;. Even though the worse case runtime is not as good as Algorithm R, this algorithm can easily be extended to weighted sampling. Note that both algorithms can operate on streams of unspecified lengths.

&lt;source lang="pascal"&gt;
(*
  S is a stream of items to sample, R will contain the result
  S.Current returns current item in stream
  S.Next advances stream to next position
  min-priority-queue supports:
    Count -&gt; number of items in priority queue
    Minimum -&gt; returns minimum key value of all items
    Extract-Min() -&gt; Remove the item with min key
    Insert(key, Item) -&gt; Adds item with specified key
 *)
ReservoirSample(S[1..?], R[1..k])
  H = new min-priority-queue
  while S has data
    r = Random(0,1)
    if H.Count &lt; k
      H.Insert(r, S.Current)
    else
      if H.Minimum &lt; r
        H.Extract-Min()
        H.Insert(r, S.Current)
    S.Next
&lt;/source&gt;

== Weighted Random Sampling using Reservoir ==
In many applications sampling is required to be according to the weights that are assigned to each items available in set. For example, it might be required to sample queries in a search engine with weight as number of times they were performed so that the sample can be analyzed for overall impact on user experience. There are two ways to interpret weights assigned to each item in the set:&lt;ref name=":0"&gt;{{Cite journal|title = Weighted random sampling with a reservoir|url = http://www.sciencedirect.com/science/article/pii/S002001900500298X|journal = Information Processing Letters|date = 2006-03-16|pages = 181–185|volume = 97|issue = 5|doi = 10.1016/j.ipl.2005.11.003|first = Pavlos S.|last = Efraimidis|first2 = Paul G.|last2 = Spirakis}}&lt;/ref&gt; 
# Let the weight of each item be &lt;math&gt;w_i&lt;/math&gt; and sum of all weights be {{mvar|W}}. We can convert weight to probability of item getting selected in sample as &lt;math&gt;P_i = w_i / W&lt;/math&gt;.
# Let the weight of two items {{mvar|i}} and {{mvar|j}} be &lt;math&gt;w_i&lt;/math&gt; and &lt;math&gt;w_j&lt;/math&gt;. Let the probability of item {{mvar|i}} getting selected in sample be &lt;math&gt;p_i&lt;/math&gt;, then we give &lt;math&gt;p_j = \min(1, p_i \frac{w_j}{w_i})&lt;/math&gt;.

=== Algorithm A-Res ===
The following algorithm was given by Efraimidis and Spirakis that uses interpretation 1:&lt;ref name=":0" /&gt;&lt;syntaxhighlight lang="pascal"&gt;
(*
  S is a stream of items to sample, R will contain the result
  S.Current returns current item in stream
  S.Weight  returns weight of current item in stream
  S.Next advances stream to next position
  The power operator is represented by ^
  min-priority-queue supports:
    Count -&gt; number of items in priority queue
    Minimum() -&gt; returns minimum key value of all items
    Extract-Min() -&gt; Remove the item with minimum key
    Insert(key, Item) -&gt; Adds item with specified key
 *)
ReservoirSample(S[1..?], R[1..k])
  H = new min-priority-queue
  while S has data
    r = Random(0,1) ^ (1/S.Weight)  // important: inclusive range
    if H.Count &lt; k
      H.Insert(r, S.Current)
    else
      if H.Minimum &lt; r
        H.Extract-Min()
        H.Insert(r, S.Current)
    S.Next
&lt;/syntaxhighlight&gt;This algorithm is identical to the algorithm given in [[Reservoir sampling#Reservoir with Random Sort|Reservoir Sampling with Random Sort]] except for the line how we generate the key using random number generator. The algorithm is equivalent to assigning each item a key &lt;math&gt;r^{1/w_i}&lt;/math&gt; where {{mvar|r}} is the random number and then sort items using these keys and finally select top k items for the sample.

=== Algorithm A-Chao ===
Following algorithm was given by M. T. Chao uses interpretation 2:&lt;ref name=":1" /&gt;&lt;syntaxhighlight lang="pascal"&gt;
(*
  S has items to sample, R will contain the result
  S[i].Weight contains weight for each item
 *)
WeightedReservoir-Chao(S[1..n], R[1..k])
  WSum = 0
  // fill the reservoir array
  for i = 1 to k
      R[i] := S[i]
      WSum = WSum + S[i].Weight/k
  for i = k+1 to n
    WSum = WSum + S[i].Weight/k
    p = S[i].Weight / WSum // probability for this item
    j := random(0, 1);     // important: inclusive range
    if j &lt;= p              // select item according to probability
        R[random(1,k)] := S[i]  //uniform selection in reservoir for replacement

&lt;/syntaxhighlight&gt;For each item, its relative weight is calculated and used to randomly decide if the item will be added into the reservoir. If the item is selected, then one of the existing items of the reservoir is uniformly selected and replaced with the new item. The trick here is that, if the probabilities of all items in the reservoir are already proportional to their weights, then by selecting uniformly which item to replace, the probabilities of all items remain proportional to their weight after the replacement.

== Distributed Reservoir Sampling ==
In many applications, amount of data from which a small sample is needed is too large and it is desirable to distribute sampling tasks among many machines in parallel to speed up the process. A simple approach that is often used, although less performant, is to assign a random number as key to each item and then perform a distributed sort and finally obtain a sample of desired size from top k items. If weighted sample is desired then key is computed using &lt;math&gt;r^{1/w_i}&lt;/math&gt; where {{mvar|r}} is the random number and &lt;math&gt;w_i&lt;/math&gt; is the weight of an item. The inefficiency in this approach obviously arises from required distributed sort on very large amount of data.

Another more efficient approach for distributed weighted random sampling is as follows:&lt;ref&gt;{{Cite web|title = Gregable: Reservoir Sampling - Sampling from a stream of elements|url = http://gregable.com/2007/10/reservoir-sampling.html|website = gregable.com|accessdate = 2015-07-23}}&lt;/ref&gt;
# Distribute data among m machines.
# Each machine does its own weighted sampling using key &lt;math&gt;r^{1/w_i}&lt;/math&gt; as described in previous section and produces a sample of size &lt;= k items. 
# Collects all m samples of size &lt;= k. We should have total items &lt;math&gt;n' &lt;= mk&lt;/math&gt;.
# Now sample k items from &lt;math&gt;n'&lt;/math&gt; items from step 3 using key that was already computed in Step 2. This means instead of re-generating key using random number generator in sampling algorithm, we use the key we already had assigned in step 2.
The Step 4 uses keys from Step 2 because we might have unbalanced data distribution on machines. For example, lets say k = 1, machine m1 only gets 1 item with weight 10 while machine m2 gets 2 items each with weight 100. Intuitively probability for items from m1 getting in final sample is 10/210. In Step 3, we will get 1 item from m1 as well as m2. If we recalculate keys in step 4 then the probability that item from m1 will be in final sample is 10/110 instead of required 10/210. Now observe that weighted reservoir sampling algorithm from previous section decreases max key value in priority queue as it processes more items. Therefore, items sampled from machine with larger chunk will have lower key values and thus higher chance of getting selected.

== Relation to Fisher-Yates shuffle ==
Suppose one wanted to draw ''k'' random cards from a deck of playing cards (i.e., ''n=52'').
A natural approach would be to shuffle the deck and then take the top ''k'' cards.
In the general case, the shuffle also needs to work even if the number of cards in the deck is not known in advance, a condition which is satisfied by the inside-out version of the [[Fisher-Yates shuffle]]:

   To initialize an array ''a'' of ''n'' elements to a randomly shuffled copy of ''S'', both 0-based: &lt;br /&gt;
 &amp;nbsp;&amp;nbsp;&amp;nbsp;''a''[0] &amp;#8592; ''S''[0] &lt;br /&gt;
 &amp;nbsp;&amp;nbsp;&amp;nbsp;'''for''' ''i'' '''from''' 1 '''to''' ''n'' - 1 '''do''' &lt;br /&gt;
 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;''r'' &amp;#8592; ''random'' (0 .. ''i'') &lt;br /&gt;
 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;''a''[''i''] &amp;#8592; ''a''[''r''] &lt;br /&gt;
 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;''a''[''r''] &amp;#8592; ''S''[''i'']

Note that although the rest of the cards are shuffled, only the top ''k'' are important in the present context.
Therefore, the array ''a'' need only track the cards in the top ''k'' positions while performing the shuffle, reducing the amount of memory needed.
Truncating ''a'' to length ''k'', the algorithm is modified accordingly:
   To initialize an array ''a'' to ''k'' random elements of ''S'' (which is of length ''n''), both 0-based: &lt;br /&gt;
 &amp;nbsp;&amp;nbsp;&amp;nbsp;''a''[0] &amp;#8592; ''S''[0] &lt;br /&gt;
 &amp;nbsp;&amp;nbsp;&amp;nbsp;'''for''' ''i'' '''from''' 1 '''to''' ''k'' - 1 '''do''' &lt;br /&gt;
 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;''r'' &amp;#8592; ''random'' (0 .. ''i'') &lt;br /&gt;
 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;''a''[''i''] &amp;#8592; ''a''[''r''] &lt;br /&gt;
 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;''a''[''r''] &amp;#8592; ''S''[''i'']  &lt;br /&gt;
 &amp;nbsp;&amp;nbsp;&amp;nbsp;'''for''' ''i'' '''from''' k '''to''' ''n'' - 1 '''do''' &lt;br /&gt;
 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;''r'' &amp;#8592; ''random'' (0 .. ''i'') &lt;br /&gt;
 &amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;'''if''' (''r'' &lt; ''k'') '''then''' ''a''[''r''] &amp;#8592; ''S''[''i'']

Since the order of the first ''k'' cards is immaterial, the first loop can be removed and ''a'' can be initialized to be the first ''k'' items of ''S''.
This yields ''Algorithm R''.

== Fast Approximation ==

A [https://erikerlandson.github.io/blog/2015/11/20/very-fast-reservoir-sampling/ fast approximation] to reservoir sampling.&lt;ref&gt;{{Cite web |url = https://erikerlandson.github.io/blog/2015/11/20/very-fast-reservoir-sampling/ |title = Very Fast Reservoir Sampling |last1 = Erlandson |first1 = Erik J. |date = 2015-11-20}}&lt;/ref&gt;
Uses a good-quality approximation to the sampling-gap distribution to skip over the gaps; i.e. consecutive runs of data that
are not sampled.

&lt;source lang="pascal"&gt;
(*
  S has items to sample, R will contain the result
  The reservoir size is (k)
 *)
FastApproximateReservoirSample(S[1..n], R[1..k])
  // fill the reservoir array
  for i = 1 to k
      R[i] := S[i]

  // Threshold (t) determines when to start fast sampling logic
  // The optimal value for (t) may vary depending on RNG performance characteristics
  t := 4 * k

  // Normal reservoir sampling is fastest up to (t) samples
  i := 1 + k
  while (i &lt;= n  &amp;&amp;  i &lt;= t)
    j := random(1, i)  // integer from 1 to i, inclusive
    if j &lt;= k
        R[j] := S[i]
    i := i + 1

  // Once gap sizes become significant, it pays to use
  // fast sampling using an approximate sampling gap distribution
  while (i &lt;= n)
    // draw gap size (g) from geometric distribution with probability p = k / i
    p := k / i
    u := randomFloat() // random float &gt; 0.0 and &lt;= 1.0
    g := floor(log(u) / log(1-p))
    // advance over the gap, and assign next element to the reservoir
    i := i + g
    if (i &lt;= n)
      j := random(1, k)  // integer 1 to k, inclusive
      R[j] := S[i]
      i := i + 1
&lt;/source&gt;

== Example implementation ==
The following is a simple implementation of the algorithm in [[Python (programming language)|Python]] that samples the set of English Wikipedia page titles:
&lt;source lang="python"&gt;
import random
SAMPLE_COUNT = 10

# Force the value of the seed so the results are repeatable
random.seed(12345)

sample_titles = []
for index, line in enumerate(open("enwiki-20091103-all-titles-in-ns0")):
        # Generate the reservoir
        if index &lt; SAMPLE_COUNT:
                sample_titles.append(line)
        else:
                # Randomly replace elements in the reservoir
                # with a decreasing probability.
                # Choose an integer between 0 and index (inclusive)
                r = random.randint(0, index)
                if r &lt; SAMPLE_COUNT:
                        sample_titles[r] = line
print sample_titles
&lt;/source&gt;

== Statistical properties ==
Probabilities of selection of the reservoir methods are discussed in Chao (1982)&lt;ref name=":1"&gt;[http://biomet.oxfordjournals.org/content/69/3/653.abstract Chao, M.T. (1982) A general purpose unequal probability sampling plan. Biometrika, 69 (3): 653-656.]&lt;/ref&gt; and Tillé (2006).&lt;ref&gt;[https://www.amazon.com/Sampling-Algorithms-Springer-Series-Statistics/dp/0387308148 Tillé, Y. (2006). Sampling Algorithms. Springer]&lt;/ref&gt; While the first-order selection probabilities are equal to &lt;math&gt;k/n&lt;/math&gt; (or, in case of Chao's procedure, to an arbitrary set of unequal probabilities), the second order selection probabilities depend on the order in which the records are sorted in the original reservoir. The problem is overcome by the [[cube sampling]] method of Deville and Tillé (2004).&lt;ref&gt;[http://biomet.oxfordjournals.org/content/91/4/893.short Deville, J.-C., and Y. Tillé (2004). Efficient balanced sampling: The cube method. Biometrika 91 (4): 893-912.]&lt;/ref&gt;

== Limitations ==
Reservoir sampling makes the assumption that the desired sample fits into [[main memory]], often implying that &lt;math&gt;k&lt;/math&gt; is a constant independent of &lt;math&gt;n&lt;/math&gt;. In applications where we would like to select a large subset of the input list (say a third, i.e. &lt;math&gt;k=n/3&lt;/math&gt;), other methods need to be adopted. Distributed implementations for this problem have been proposed.&lt;ref&gt;[http://had00b.blogspot.com/2013/07/random-subset-in-mapreduce.html Reservoir Sampling in MapReduce]&lt;/ref&gt;

==See also==
* [[Moving average]]

==References==
{{reflist}}

{{DEFAULTSORT:Reservoir Sampling}}
[[Category:Algorithms]]
[[Category:Analysis of algorithms]]
[[Category:Randomized algorithms]]</text>
      <sha1>fqn37eqq3zpsbd7p9ya19dqd3w4f3p6</sha1>
    </revision>
  </page>
  <page>
    <title>Rose–Vinet equation of state</title>
    <ns>0</ns>
    <id>39214059</id>
    <revision>
      <id>867230981</id>
      <parentid>800004782</parentid>
      <timestamp>2018-11-04T14:07:04Z</timestamp>
      <contributor>
        <username>Nemo bis</username>
        <id>2584239</id>
      </contributor>
      <comment>Added free to read link in citations with [[WP:OABOT|OAbot]] #oabot</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1885">The '''Rose–Vinet equation of state''' are a set of equations used to describe the [[equation of state]] of solid objects. It is an modification of the [[Birch–Murnaghan equation of state]].&lt;ref&gt;{{cite journal|url=https://dx.doi.org/10.1103/PhysRevB.35.1945 |authors=Pascal Vinet, John R. Smith, John Ferrante and James H. Rose  | title=Temperature effects on the universal equation of state of solids | journal=Physical Review B | volume=35| pages=1945–1953 |year=1987 | doi=10.1103/physrevb.35.1945|hdl=2060/19860019304}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.sklogwiki.org/SklogWiki/index.php?title=Rose-Vinet_(Universal)_equation_of_state&amp;action=edit&amp;section=1 | website=SklogWiki | title=Rose-Vinet (Universal) equation of state}}&lt;/ref&gt;
The initial paper discusses how the equation only depends on four inputs: the isothermal [[bulk modulus]] &lt;math&gt;B_0&lt;/math&gt;, the derivative of bulk modulus with respect to pressure &lt;math&gt;B_0'&lt;/math&gt;, the volume &lt;math&gt;V_0&lt;/math&gt;, and the thermal expansion; all evaluated zero pressure (&lt;math&gt;P=0&lt;/math&gt;) and at a single (reference) temperature. And the same equation holds for all classes of solids and a wide range of temperatures.

Let the cube root of the specific volume be

:&lt;math&gt;\eta=\sqrt[3]{\frac{V}{V_0}}&lt;/math&gt;

then the equation of state is:

:&lt;math&gt;P=3B_0\left(\frac{1-\eta}{\eta^2}\right)e^{\frac{3}{2}(B_0'-1)(1-\eta)}&lt;/math&gt;

A similar equation was published by Stacey et al. in 1981.&lt;ref&gt;{{cite journal | author=F. D. Stacey | author2= B. J. Brennan | author3=R. D. Irvine | title=Finite strain theories and comparisons with seismological data | journal= Surveys in Geophysics | number=4 | pages=189–232 | year=1981 | doi=10.1007/BF01449185 | volume=4}}&lt;/ref&gt;

== References ==
&lt;references/&gt;

{{DEFAULTSORT:Rose-Vinet equation of state}}
[[Category:Solid mechanics]]
[[Category:Equations of state]]


{{applied-math-stub}}</text>
      <sha1>o5t5fr6qhsvgfg03ufabswnznxh18sc</sha1>
    </revision>
  </page>
  <page>
    <title>Sion's minimax theorem</title>
    <ns>0</ns>
    <id>17528854</id>
    <revision>
      <id>780458700</id>
      <parentid>780422005</parentid>
      <timestamp>2017-05-15T07:15:22Z</timestamp>
      <contributor>
        <username>Tsirel</username>
        <id>113936</id>
      </contributor>
      <comment>Undid revision 780422005 by [[Special:Contributions/128.100.3.110|128.100.3.110]] ([[User talk:128.100.3.110|talk]]) Wrong; see talk</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1551">In [[mathematics]], and in particular [[game theory]], '''Sion's minimax theorem''' is a generalization of [[John von Neumann]]'s [[minimax theorem]], named after [[Maurice Sion]].

It states:

Let &lt;math&gt;X&lt;/math&gt; be a [[compact space|compact]] [[Convex set|convex]] subset of a [[linear topological space]] and &lt;math&gt;Y&lt;/math&gt; a convex subset of a linear topological space.  If &lt;math&gt;f&lt;/math&gt; is a real-valued [[Function (mathematics)|function]] on &lt;math&gt;X\times Y&lt;/math&gt; with

: &lt;math&gt;f(x,\cdot)&lt;/math&gt; [[upper semicontinuous]]  and [[quasiconvex function|quasiconcave]] on &lt;math&gt;Y&lt;/math&gt;, &lt;math&gt;\forall x\in X&lt;/math&gt;, and
: &lt;math&gt;f(\cdot,y)&lt;/math&gt; lower semicontinuous and quasi-convex on &lt;math&gt;X&lt;/math&gt;, &lt;math&gt;\forall y\in Y&lt;/math&gt;

then,

: &lt;math&gt;\min_{x\in X}\sup_{y\in Y} f(x,y)=\sup_{y\in Y}\min_{x\in X}f(x,y).&lt;/math&gt;

==See also==
*[[Parthasarathy's theorem]]
*[[Saddle point]]

==References==
* {{cite journal |first=Maurice |last=Sion |title=On general minimax theorems |journal=[[Pacific Journal of Mathematics]] |volume=8 |issue=1 |year=1958 |pages=171–176 |zbl=0081.11502 |mr=0097026 |doi=10.2140/pjm.1958.8.171}}
* {{cite journal |first=Hidetoshi |last=Komiya |year=1988 |title=Elementary proof for Sion's minimax theorem |journal=[[Kodai Mathematical Journal]] |volume=11 |issue=1 |pages=5–7 |mr=0930413 |zbl=0646.49004 |doi=10.2996/kmj/1138038812}}

{{DEFAULTSORT:Sion's Minimax Theorem}}
[[Category:Game theory]]
[[Category:Mathematical optimization]]
[[Category:Mathematical theorems]]


{{mathanalysis-stub}}
{{gametheory-stub}}</text>
      <sha1>kvbfhyoebk3f6041ix8l2w402eu9h2c</sha1>
    </revision>
  </page>
  <page>
    <title>State space enumeration</title>
    <ns>0</ns>
    <id>21676935</id>
    <revision>
      <id>532110248</id>
      <parentid>526593891</parentid>
      <timestamp>2013-01-09T04:04:21Z</timestamp>
      <contributor>
        <username>Addbot</username>
        <id>6569922</id>
      </contributor>
      <minor/>
      <comment>[[User:Addbot|Bot:]] Removing Orphan Tag (No longer an Orphan) ([[User_talk:Addbot|Report Errors]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1247">In [[computer science]], '''state space enumeration''' are methods that consider each reachable [[program state]] to determine whether a program [[satisfiability|satisfies]] a given property.&lt;ref&gt;"A Compact Petri Net Representation for Concurrent Programs", Matthew B. Dwyer, Lori A. Clarke, Kari A. Niesy, Department of Computer Science, University of Massachusetts, Amherst Amherst, MA 01003&lt;/ref&gt; As programs increase in size and complexity, the [[state space]] grows exponentially. The state space used by these methods can be reduced by maintaining only the parts of the state space that are relevant to the analysis. However, the use of state and memory reduction techniques makes runtime a major limiting factor.&lt;ref&gt;"Proceedings of the conference on Application and theory of petri nets: formal methods in software engineering and defence systems - Volume 12", ACM International Conference Proceeding Series, Vol. 145, by Marko Mäkelä, Laboratory for Theoretical Computer Science, Helsinki University of Technology, Espoo, Finland&lt;/ref&gt;

==See also==
* [[Formal methods]]
* [[Model checking]]

==References==
{{reflist}}

[[Category:Formal methods]]
[[Category:Logic in computer science]]
[[Category:Programming language implementation]]</text>
      <sha1>7tasqpjh83peljd7y6h9qesy1ib1fbe</sha1>
    </revision>
  </page>
  <page>
    <title>Student (mathematician)</title>
    <ns>0</ns>
    <id>34339552</id>
    <redirect title="William Sealy Gosset" />
    <revision>
      <id>470663756</id>
      <timestamp>2012-01-10T19:29:45Z</timestamp>
      <contributor>
        <username>Ntsimp</username>
        <id>1219859</id>
      </contributor>
      <comment>make redirect</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="76">#REDIRECT [[William Sealy Gosset]]

[[Category:Pseudonymous mathematicians]]</text>
      <sha1>qed85sfy0yhism8tb3djve1ffgjij0d</sha1>
    </revision>
  </page>
  <page>
    <title>Substitution–permutation network</title>
    <ns>0</ns>
    <id>385162</id>
    <revision>
      <id>856967824</id>
      <parentid>850278872</parentid>
      <timestamp>2018-08-28T17:55:58Z</timestamp>
      <contributor>
        <username>Jonesey95</username>
        <id>9755426</id>
      </contributor>
      <minor/>
      <comment>Fix Linter error using [[WP:AutoEd|AutoEd]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6559">[[Image:SubstitutionPermutationNetwork2.png|thumb|360px|right|A sketch of a substitution–permutation network with 3 rounds, encrypting a plaintext block of 16 bits into a ciphertext block of 16 bits. The S-boxes are the ''S&lt;sub&gt;i&lt;/sub&gt;''’s, the P-boxes are the same ''P'', and the round keys are the ''K&lt;sub&gt;i&lt;/sub&gt;''’s.]]

In [[cryptography]], an '''SP-network''', or '''substitution–permutation network''' ('''SPN'''), is a series of linked mathematical operations used in [[block cipher]] algorithms such as [[Advanced Encryption Standard|AES (Rijndael)]], [[3-Way]], [[Kalyna (cipher)| Kalyna]], [[Kuznyechik]], [[PRESENT (cipher)|PRESENT]], [[SAFER]], [[SHARK]], and [[Square (cipher)|Square]].

Such a network takes a block of the [[plaintext]] and the [[key (cryptography)|key]] as inputs, and applies several alternating "rounds" or "layers" of [[Substitution box|substitution boxes (S-boxes)]] and [[Permutation box|permutation boxes (P-boxes)]] to produce the [[ciphertext]] block. The S-boxes and P-boxes transform {{nobr|(sub-)blocks}} of input [[bit]]s into output bits.  It is common for these transformations to be operations that are efficient to perform in hardware, such as [[Exclusive disjunction|exclusive or]] (XOR) and [[bitwise rotation]]. The key is introduced in each round, usually in the form of "round keys" derived from it. (In some designs, the [[S-box]]es themselves depend on the key.)

[[Decryption]] is done by simply reversing the process (using the inverses of the S-boxes and P-boxes and applying the round keys in reversed order).

An '''S-box''' substitutes a small block of bits (the input of the S-box) by another block of bits (the output of the S-box). This substitution should be [[bijective|one-to-one]], to ensure invertibility (hence decryption). In particular, the length of the output should be the same as the length of the input (the picture on the right has S-boxes with 4 input and 4 output bits), which is different from S-boxes in general that could also change the length, as in [[Data Encryption Standard|DES (Data Encryption Standard)]], for example. An S-box is usually not simply a [[permutation]] of the bits. Rather, a good S-box will have the property that changing one input bit will change about half of the output bits (or an [[avalanche effect]]). It will also have the property that each output bit will depend on every input bit.

A '''P-box''' is a [[permutation]] of all the bits: it takes the outputs of all the S-boxes of one round, permutes the bits, and feeds them into the S-boxes of the next round. A good P-box has the property that the output bits of any S-box are distributed to as many S-box inputs as possible.

At each round, the '''round key''' (obtained from the [[key (cryptography)|key]] with some simple operations, for instance, using S-boxes and P-boxes) is combined using some group operation, typically [[XOR]].

A single typical S-box or a single P-box alone does not have much cryptographic strength: an S-box could be thought of as a [[substitution cipher]], while a P-box could be thought of as a [[transposition cipher]]. However, a well-designed SP network with several alternating rounds of S- and P-boxes already satisfies '''Shannon's [[confusion and diffusion]] properties''':

* The reason for '''diffusion''' is the following: If one changes one bit of the plaintext, then it is fed into an S-box, whose output will change at several bits, then all these changes are distributed by the P-box among several S-boxes, hence the outputs of all of these S-boxes are again changed at several bits, and so on. Doing several rounds, each bit changes several times back and forth, therefore, by the end, the ciphertext has changed completely, in a [[pseudorandom]] manner. In particular, for a randomly chosen input block, if one flips the ''i''-th bit, then the probability that the ''j''-th output bit will change is approximately a half, for any ''i'' and ''j'', which is the [[Strict Avalanche Criterion]]. Vice versa, if one changes one bit of the ciphertext, then attempts to decrypt it, the result is a message completely different from the original plaintext—SP ciphers are not easily [[malleability (cryptography)|malleable]].
* The reason for '''confusion''' is exactly the same as for diffusion: changing one bit of the key changes several of the round keys, and every change in every round key [[diffuse]]s over all the bits, changing the ciphertext in a very complex manner.
* Even if an attacker somehow obtains one plaintext corresponding to one ciphertext—a [[known-plaintext attack]], or worse, a [[chosen plaintext]] or [[chosen-ciphertext attack]]—the confusion and diffusion make it difficult for the attacker to recover the key.

Although a [[Feistel network]] that uses S-boxes (such as [[Data Encryption Standard|DES]]) is quite similar to SP networks, there are some differences that make either this or that more applicable in certain situations. For a given amount of [[confusion and diffusion]], an SP network has more "inherent parallelism"&lt;ref&gt;
[http://www.ddj.com/184410756 "Principles and Performance of Cryptographic Algorithms"] by Bart Preneel, Vincent Rijmen, and Antoon Bosselaers.
&lt;/ref&gt;
and so — given a CPU with a large number of [[execution unit]]s — can be computed faster than a Feistel network.&lt;ref&gt;
[http://www.schneier.com/skein1.1.pdf "The Skein Hash Function Family"] 2008
by [[Niels Ferguson]], [[Stefan Lucks]], [[Bruce Schneier]], Doug Whiting, [[Mihir Bellare]], Tadayoshi Kohno, [[Jon Callas]], Jesse Walker
page 40.
&lt;/ref&gt;
CPUs with few execution units — such as most [[smart card]]s — cannot take advantage of this inherent parallelism. Also SP ciphers require S-boxes to be invertible (to perform decryption); Feistel inner functions have no such restriction and can be constructed as [[one-way function]]s.

== See also ==
* [[Feistel network]]
* [[Product cipher]]
* [[Square (cipher)]]
* [[International Data Encryption Algorithm]]

==References==
{{Reflist}}

==Further reading==
*{{cite book |first=Jonathan |last=Katz |first2=Yehuda |last2=Lindell |title=Introduction to Modern Cryptography |publisher=CRC Press |year=2007 |isbn=9781584885511 }}
*{{cite book |first=Douglas R. |last=Stinson |title=Cryptography. Theory and Practice |edition=Third |publisher=Chapman &amp; Hall/CRC |year=2006 |isbn=1584885084 }}

{{Cryptography navbox | block}}

{{DEFAULTSORT:Substitution-permutation network}}
[[Category:Cryptographic algorithms]]
[[Category:Block ciphers]]
[[Category:Permutations]]</text>
      <sha1>sfwjfwwpr0nqicu4ke5ql3ky8weefi9</sha1>
    </revision>
  </page>
  <page>
    <title>Theory of regions</title>
    <ns>0</ns>
    <id>27115797</id>
    <revision>
      <id>787706750</id>
      <parentid>766432826</parentid>
      <timestamp>2017-06-27T01:42:24Z</timestamp>
      <contributor>
        <username>PrussianOwl</username>
        <id>20083999</id>
      </contributor>
      <minor/>
      <comment>/* top */ Added "The"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2055">{{Multiple issues|
{{Underlinked|date=January 2017}}
{{Orphan|date=January 2017}}
{{refimprove|date=April 2016}}
{{context|date=February 2017}}
{{technical|date=February 2017}}
}}
The '''Theory of regions''' is an approach for synthesizing a [[Petri net]] from a [[transition system]]. As such, it aims at recovering concurrent, independent behaviour from transitions between global states. Theory of regions handles elementary net systems as well as P/T nets and other kinds of nets. An important point is that the approach is aimed at the synthesis of unlabeled Petri nets only.
&lt;!-- or is there any extension to non-trivially labeled ones yet in the literature?
     otherwise, any finite ts is trivially encodable as a petri net. --&gt;

&lt;!--- Often, infinite nets would be obtained, but there are actual algorithms ---&gt;

&lt;!--- The first appearance of regions is in &lt;ref&gt;Ehrenfeucht and Rozenberg, ''partial (set) 2-structures (parts I and II)''&lt;/ref&gt;; Mukund &lt;ref&gt;&lt;/ref&gt; does away with the 2-structures ---&gt;

== Definition ==

&lt;!--- start with easily digested definition + some SVGs ---&gt;

A region of a transition system &lt;math&gt;(S, \Lambda, \rightarrow)&lt;/math&gt; is a mapping assigning to each state &lt;math&gt;s \in S&lt;/math&gt; a number &lt;math&gt;\sigma(s)&lt;/math&gt; (natural number for P/T nets, binary for ENS) and to each transition label a number &lt;!-- or two natural numbers, to allow self-loops, as in Mukund --&gt; &lt;math&gt;\tau(\ell)&lt;/math&gt; such that consistency conditions &lt;math&gt;\sigma(s') = \sigma(s) + \tau(\ell)&lt;/math&gt; holds whenever &lt;math&gt;(s,\ell,s') \in \rightarrow&lt;/math&gt;.&lt;ref name="mukund"&gt;http://www.cmi.ac.in/~madhavan/papers/m-ijfocs92.php&lt;/ref&gt;

=== Intuitive explanation ===

Each region represents a potential place of a Petri net.

Mukund: event/state separation property, state separation property.
&lt;!--- exploitation of duality for further results ---&gt;

== References ==
*Badouel, E and Darondeau, P. "Theory of Regions"
{{reflist}}

&lt;!--- Categories ---&gt;
[[Category:Articles created via the Article Wizard]]
[[Category:Set theory]]


{{Comp-sci-stub}}</text>
      <sha1>bswekt2i9b6jro5s4cp9m8bha3z1kmp</sha1>
    </revision>
  </page>
  <page>
    <title>Trichotomy (mathematics)</title>
    <ns>0</ns>
    <id>373956</id>
    <revision>
      <id>870526959</id>
      <parentid>870504129</parentid>
      <timestamp>2018-11-25T11:40:34Z</timestamp>
      <contributor>
        <username>Jochen Burghardt</username>
        <id>17350134</id>
      </contributor>
      <comment>suggest to move group example down; in the lead, the reals example should suffice to give an intuition to everyone</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3676">In mathematics, the law of '''trichotomy''' states that every real number is either positive, negative, or zero.&lt;ref name="mathworld"&gt;[http://mathworld.wolfram.com/TrichotomyLaw.html Trichotomy Law] at [[MathWorld]]&lt;/ref&gt;

More generally, a [[binary relation]] ''R'' on some set ''X'' is '''trichotomous''' if for all ''x'' and ''y'' in ''X'' exactly one of ''xRy'', ''yRx'' or ''x''=''y'' holds, formally, if
:&lt;math&gt;\forall x \in X \, \forall y \in X \, (
  [       x &lt; y  \, \land \, \lnot(y &lt; x) \, \land \, \lnot(x = y) ] \, \lor \,
  [ \lnot(x &lt; y) \, \land \,       y &lt; x  \, \land \, \lnot(x = y) ] \, \lor \,
  [ \lnot(x &lt; y) \, \land \, \lnot(y &lt; x) \, \land \,       x = y  ]
) \,.&lt;/math&gt;

==Properties==
* A relation is trichotomous if, and only if, it is [[irreflexive]], [[asymmetric relation|asymmetric]], and a [[semi-connex relation]].
* If a trichotomous relation is also transitive, then it is a [[Total order#Strict total order|strict total order]]; this is a special case of a [[strict weak  order]].&lt;ref&gt;[[Jerrold E. Marsden]] &amp; Michael J. Hoffman (1993) ''Elementary Classical Analysis'', page 27, [[W. H. Freeman and Company]] {{ISBN|0-7167-2105-8}}&lt;/ref&gt;&lt;ref&gt;H.S. Bear (1997) ''An Introduction to Mathematical Analysis'', page 11, [[Academic Press]] {{ISBN|0-12-083940-7}}&lt;/ref&gt;

==Examples==
* On the set ''X'' = {''a'',''b'',''c''}, the relation ''R'' = { (''a'',''b''), (''a'',''c''), (''b'',''c'') } is transitive and trichotomous, and hence a strict [[total order]].
* On the same set, the cyclic relation ''R'' = { (''a'',''b''), (''b'',''c''), (''c'',''a'') } is trichotomous, but not transitive; it is even [[antitransitive]].

==Trichotomy on numbers==
A '''law of trichotomy''' on some set ''X'' of numbers usually expresses that some tacitly given ordering relation on ''X'' is a trichotomous one.
An example is the law "For arbitrary real numbers ''x'' and ''y'', exactly one of ''x''&lt;''y'', ''y''&lt;''x'', or ''x''=''y'' applies"; some authors even fix ''y'' to be zero,&lt;ref name="mathworld"/&gt; relying on the real number's additive [[linearly ordered group]] structure. The latter is a [[group (mathematics)|group]] equipped with a trichotomous order.

In classical logic, this '''axiom of trichotomy''' holds for ordinary comparison between [[real number]]s and therefore also for comparisons between [[integer]]s and between [[rational number]]s.{{clarify|reason=In which axiomatization? Usually, the natural numbers N are introduced based on the Peano axioms, 'is less than' is defined recursively on N, and its trichotomy is proven by induction; integers, rationals, and reals are constructed step by step; again, trichotomy is proven for every of these domains.|date=May 2018}} The law does not hold in general in [[intuitionistic logic]].{{cn|reason=Also give an example where the law doesn't hold.|date=May 2018}}

In [[Zermelo–Fraenkel set theory]] and [[Von Neumann–Bernays–Gödel set theory|Bernays set theory]], the law of trichotomy holds between the [[cardinal number]]s of well-orderable sets even without the [[axiom of choice]]. If the axiom of choice holds, then trichotomy holds between arbitrary cardinal numbers (because they are all well-orderable in that case).&lt;ref&gt;{{cite book | author=Bernays, Paul | title=Axiomatic Set Theory | publisher=Dover Publications | year=1991 | isbn=0-486-66637-9}}&lt;/ref&gt;

== See also ==
* ''[[Begriffsschrift]]'' contains an early formulation of the law of trichotomy
* [[Dichotomy]]
* [[Law of noncontradiction]] 
* [[Law of excluded middle]]
* [[Three-way comparison]]

== References ==
{{reflist}}


[[Category:Order theory]]
[[Category:Binary relations]]
[[Category:3 (number)]]</text>
      <sha1>b1m8f25hjcps22eicc9t77iuowbwjii</sha1>
    </revision>
  </page>
  <page>
    <title>UTOPIA (bioinformatics tools)</title>
    <ns>0</ns>
    <id>9107270</id>
    <revision>
      <id>867559900</id>
      <parentid>850190451</parentid>
      <timestamp>2018-11-06T14:18:43Z</timestamp>
      <contributor>
        <ip>81.133.69.255</ip>
      </contributor>
      <comment>/* References */ Categories</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10326">{{Infobox software
| name                   = 
| title                  = UTOPIA
| logo                   = 
| logo caption           = 
| screenshot             = Image:Utopia-desktop600.jpg
| caption                = UTOPIA tools on a Mac
| collapsible            = 
| author                 = 
| developer              = Steve Pettifer,&lt;br/&gt;[[Terri Attwood]],&lt;br/&gt;David Parry-Smith,&lt;br/&gt;D.N.Perkins,&lt;br/&gt;A.W. Payne,&lt;br/&gt;A.D. Michie,&lt;br/&gt;Phillip W.Lord,&lt;br/&gt;J.N.Selley,&lt;br/&gt;Phil McDermott,&lt;br/&gt;James Marsh,&lt;br/&gt;James Sinnott,&lt;br/&gt;Dave Thorne,&lt;br/&gt;Benjamin Blundell
| released               = &lt;!-- {{Start date|YYYY|MM|DD|df=yes/no}} --&gt;
| discontinued           = 
| latest release version = 
| latest release date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| latest preview version = 
| latest preview date    = &lt;!-- {{Start date and age|YYYY|MM|DD|df=yes/no}} --&gt;
| frequently updated     = &lt;!-- DO NOT include this parameter unless you know what it does --&gt;
| programming language   = C++, Python
| operating system       = Linux, Mac and Windows
| platform               = 
| size                   = 
| language               = 
| status                 = 
| genre                  = 
| license                = 
| alexa                  = 
| website                = {{URL|utopia.cs.manchester.ac.uk}}, {{URL|getutopia.com}}
}}
'''UTOPIA''' ('''User-friendly Tools for Operating Informatics Applications''') is a suite of free tools for visualising and analysing [[bioinformatics]] data. Based on an [[ontology (computer science)|ontology]]-driven data model, it contains applications for viewing and aligning [[peptide sequence|protein sequences]], rendering complex molecular structures in 3D, and for finding and using resources such as web services and data objects.&lt;ref name="CaFG"&gt;{{Cite journal | last1 = Pettifer | first1 = S. R. | authorlink1 = Steve Pettifer| last2 = Sinnott | first2 = J. R. | last3 = Attwood | first3 = T. K. | authorlink3 = Terri Attwood| doi = 10.1002/cfg.359 | title = UTOPIA—User-Friendly Tools for Operating Informatics Applications | journal = Comparative and Functional Genomics | volume = 5 | issue = 1 | pages = 56–60 | year = 2004 | pmid =  18629035| pmc =2447318 }}&lt;/ref&gt;&lt;ref&gt;{{Cite book | last1 = McDermott | first1 = P. | last2 = Sinnott | first2 = J. | last3 = Thorne | first3 = D. | last4 = Pettifer | first4 = S. | authorlink4 = Steve Pettifer| last5 = Attwood | first5 = T. | authorlink5 = Terri Attwood| doi = 10.1109/CMV.2006.3 | chapter = An Architecture for Visualisation and Interactive Analysis of Proteins | title = Fourth International Conference on Coordinated &amp; Multiple Views in Exploratory Visualization (CMV'06) | pages = 55 | year = 2006 | isbn = 0-7695-2605-5 | pmid =  | pmc = }}&lt;/ref&gt;&lt;ref name="rescue"&gt;{{Cite journal 
| last1 = Attwood | first1 = T. K. 
| authorlink1 = Terri Attwood
| last2 = Kell | first2 = D. B. 
| authorlink2 = Douglas Kell
| last3 = McDermott | first3 = P. 
| last4 = Marsh | first4 = J. 
| last5 = Pettifer | first5 = S. R. 
| authorlink5 = Steve Pettifer
| last6 = Thorne | first6 = D. 
| doi = 10.1042/BJ20091474 
| title = Calling International Rescue: Knowledge lost in literature and data landslide! 
| journal = Biochemical Journal 
| volume = 424 
| issue = 3 
| pages = 317–333 
| year = 2009 
| pmid = 19929850 
| pmc =2805925 
}}&lt;/ref&gt;&lt;ref name="viz"&gt;{{Cite journal 
| last1 = Pettifer | first1 = S. 
| authorlink1 = Steve Pettifer
| last2 = Thorne | first2 = D. 
| last3 = McDermott | first3 = P. 
| last4 = Marsh | first4 = J. 
| last5 = Villéger | first5 = A. 
| last6 = Kell | first6 = D. B. 
| authorlink6 = Douglas Kell
| last7 = Attwood | first7 = T. K. 
| authorlink7 = Terri Attwood
| title = Visualising biological data: A semantic approach to tool and database integration 
| doi = 10.1186/1471-2105-10-S6-S19 
| journal = BMC Bioinformatics 
| volume = 10 
| pages = S19 
| year = 2009 
| pmid = 19534744 
| pmc =2697642 
}}&lt;/ref&gt; There are two major components, the protein analysis suite and UTOPIA documents.

==Utopia Protein Analysis suite==
The Utopia [[Proteomics|Protein Analysis]] suite is a collection of interactive tools for analysing protein sequence and [[protein structure]]. Up front are user-friendly and responsive visualisation applications, behind the scenes a sophisticated model that allows these to work together and hides much of the tedious work of dealing with [[file format]]s and [[web service]]s.&lt;ref name="CaFG"/&gt;

==Utopia Documents==
[[Utopia Documents]] brings a fresh new perspective to reading the scientific literature, combining the convenience and reliability of the [[Portable Document Format| Portable Document Format (pdf)]] with the flexibility and power of the web.&lt;ref name="rescue"/&gt;&lt;ref name="udocs"&gt;{{Cite journal 
| last1 = Attwood | first1 = T. K.
| authorlink1 = Terri Attwood 
| last2 = Kell | first2 = D. B. 
| authorlink2 = Douglas Kell
| last3 = McDermott | first3 = P. 
| last4 = Marsh | first4 = J. 
| last5 = Pettifer | first5 = S. R. 
| authorlink5 = Steve Pettifer
| last6 = Thorne | first6 = D. 
| doi = 10.1093/bioinformatics/btq383 
| title = Utopia documents: Linking scholarly literature with research data 
| journal = Bioinformatics 
| volume = 26 
| issue = 18 
| pages = i568–i574 
| year = 2010 
| pmid = 20823323 
| pmc =2935404 
}}&lt;/ref&gt;&lt;ref name="hamburger"&gt;{{Cite journal | last1 = Pettifer | first1 = S. | authorlink1= Steve Pettifer| last2 = McDermott | first2 = P. | last3 = Marsh | first3 = J. | last4 = Thorne | first4 = D. | last5 = Villeger | first5 = A. | last6 = Attwood | first6 = T. K.| authorlink6 = Terri Attwood | doi = 10.1087/20110309 | title = Ceci n'est pas un hamburger: Modelling and representing the scholarly article | journal = Learned Publishing | volume = 24 | issue = 3 | pages = 207 | year = 2011 | pmid =  | pmc = }}&lt;/ref&gt;

== History ==
Between 2003 and 2005 work on UTOPIA was funded via [https://web.archive.org/web/20070820201835/http://www.esnw.ac.uk/ The e-Science North West Centre] based at [[The University of Manchester]] by the [[Engineering and Physical Sciences Research Council]], UK [[Department of Trade and Industry (United Kingdom)|Department of Trade And Industry]], and the [[EMBnet|European Molecular Biology Network (EMBnet)]]. Since 2005 work continues under the [http://www.embracegrid.info EMBRACE European Network of Excellence].

UTOPIA's CINEMA (Colour INteractive Editor for Multiple Alignments), a tool for [[Sequence alignment|Sequence Alignment]], is the latest incarnation of software originally developed at The [[University of Leeds]] to aid the analysis of [[G protein-coupled receptor]]s (GPCRs).&lt;ref name="gpcr"&gt;{{Cite journal | last1 = Vroling | first1 = B. | last2 = Thorne | first2 = D. | last3 = McDermott | first3 = P. | last4 = Attwood | first4 = T. K. | authorlink4= Terri Attwood| last5 = Vriend | first5 = G. | last6 = Pettifer | first6 = S. | authorlink6 = Steve Pettifer| doi = 10.1186/1471-2105-12-362 | title = Integrating GPCR-specific information with full text articles | journal = BMC Bioinformatics | volume = 12 | pages = 362 | year = 2011 | pmid =  21910883| pmc =3179973 }}&lt;/ref&gt; SOMAP,&lt;ref name="somap"&gt;{{Cite journal | last1 = Parry-Smith | first1 = D. J. | last2 = Attwood | first2 = T. K. | authorlink2 = Terri Attwood| doi = 10.1093/bioinformatics/7.2.233 | title = SOMAP: A novel interactive approach to multiple protein sequences alignment | journal = Bioinformatics | volume = 7 | issue = 2 | pages = 233 | year = 1991 | pmid =  | pmc = }}&lt;/ref&gt; a Screen Oriented Multiple Alignment Procedure was developed in the late 1980s on the [[OpenVMS|VMS]] computer operating system, used a monochrome text-based [[VT100]] video terminal, and featured [[Context-Sensitive Help|context-sensitive help]] and [[Pull-down menu|pulldown menus]] some time before these were standard operating system features.

SOMAP was followed by a [[Unix]] tool called VISTAS&lt;ref name="vistas"&gt;{{Cite journal | last1 = Perkins | first1 = D. N. | last2 = Attwood | first2 = T. K. | authorlink2 = Terri Attwood| doi = 10.1016/0263-7855(94)00013-I | title = VISTAS: A package for VIsualizing STructures and sequences of proteins | journal = Journal of Molecular Graphics | volume = 13 | issue = 1 | pages = 73–75, 62 | year = 1995 | pmid =  7794837| pmc = }}&lt;/ref&gt; (VIsualizing STructures And Sequences) which included the ability to render 3D molecular structure and generate plots and statistical representations of sequence properties.

The first tool under the CINEMA&lt;ref name="cinema"&gt;{{Cite journal | last1 = Parry-Smith | first1 = D. J. | last2 = Payne | first2 = A. W. R. | last3 = Michie | first3 = A. D. | last4 = Attwood | first4 = T. K. | authorlink4 = Terri Attwood| title = CINEMA—a novel Colour INteractive Editor for Multiple Alignments | doi = 10.1016/S0378-1119(97)00650-1 | journal = Gene | volume = 221 | issue = 1 | pages = GC57–GC63 | year = 1998 | pmid =  9852962| pmc = }}&lt;/ref&gt; banner developed at The University of Manchester was a [[Java (programming language)|Java]]-based applet launched via web pages, which is [http://www.bioinf.manchester.ac.uk/dbbrowser/CINEMA2.1/  still available] but is no longer maintained. A standalone Java version, called CINEMA-MX,&lt;ref name="cinemamx"&gt;{{Cite journal | last1 = Lord | first1 = P. W. | last2 = Selley | first2 = J. N. | last3 = Attwood | first3 = T. K. | authorlink3 = Terri Attwood| doi = 10.1093/bioinformatics/18.10.1402 | title = CINEMA-MX: A modular multiple alignment editor | journal = Bioinformatics | volume = 18 | issue = 10 | pages = 1402–1403 | year = 2002 | pmid =  12376388| pmc = }}&lt;/ref&gt; was also released but is no longer readily available.

A [[C++]] version of CINEMA, called CINEMA5 was developed early on as part of the UTOPIA project, and was released as a stand-alone sequence alignment application. It has now been replaced by a version of the tool integrated with UTOPIA's other visualisation applications, and its name has reverted simply to CINEMA.

==References==
{{reflist|2}}

[[Category:Bioinformatics software]]
[[Category:Computational science]]
[[Category:Engineering and Physical Sciences Research Council]]
[[Category:School of Computer Science, University of Manchester]]
[[Category:Science and technology in Greater Manchester]]</text>
      <sha1>52c0qzo5i0og6zprl9wd83mrhbhk8lv</sha1>
    </revision>
  </page>
  <page>
    <title>Vibration</title>
    <ns>0</ns>
    <id>20646772</id>
    <revision>
      <id>871748818</id>
      <parentid>871748723</parentid>
      <timestamp>2018-12-03T04:52:17Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <comment>/* See also */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="44143">{{for|mechanical oscillations in the form of machining context|Machining vibrations}}
{{other uses|Vibration (disambiguation)|Vibrate (disambiguation)}}
{{pp-move-indef}}
{{Classical mechanics|core}}
'''Vibration''' is a mechanical phenomenon whereby [[oscillation]]s occur about an [[equilibrium point]]. The word comes from Latin ''vibrationem'' ("shaking, brandishing"). The oscillations may be [[periodic function|periodic]], such as the motion of a pendulum—or [[random]], such as the movement of a tire on a gravel road.

Vibration can be desirable: for example, the motion of a [[tuning fork]], the [[Reed (music)|reed]] in a [[woodwind instrument]] or [[harmonica]], a [[mobile phone]], or the cone of a [[loudspeaker]].

In many cases, however, vibration is undesirable, wasting [[energy]] and creating unwanted [[sound]]. For example, the vibrational motions of [[engine]]s, [[electric motor]]s, or any [[Machine|mechanical device]] in operation are typically unwanted. Such vibrations could be caused by [[Engine balance|imbalances]] in the rotating parts, uneven [[friction]], or the meshing of [[gear]] teeth. Careful designs usually minimize unwanted vibrations.

The studies of sound and vibration are closely related.  Sound, or pressure [[wave]]s, are generated by vibrating structures (e.g. [[vocal cords]]); these pressure waves can also induce the vibration of structures (e.g. [[ear drum]]).  Hence, attempts to reduce noise are often related to issues of vibration.
[[File:Drum vibration mode21.gif|thumb|One of the possible modes of [[Vibrations of a circular drum|vibration of a circular drum]] (see [[:commons:Category:Drum vibration animations|other modes]]).]]
[[Image:suspension.jpg|thumb|Car Suspension: designing vibration control is undertaken as part of [[Acoustical engineering|acoustic]], [[Automotive engineering|automotive]] or [[Mechanical engineering|mechanical]] [[engineering]].]]

==Types of vibration==

''' Free vibration''' occurs when a mechanical system is set in motion with an initial input and allowed to vibrate freely.  Examples of this type of vibration are pulling a child back on a swing and letting it go, or hitting a tuning fork and letting it ring. The mechanical system vibrates at one or more of its [[resonance|natural frequencies]] and [[damping|damps]] down to motionlessness.

'''Forced  vibration''' is when a time-varying disturbance (load, displacement or velocity) is applied to a mechanical system. The disturbance can be a periodic and steady-state input, a transient input, or a random input. The periodic input can be a harmonic or a non-harmonic disturbance. Examples of these types of vibration include a washing machine shaking due to an imbalance, transportation vibration caused by an engine or uneven road, or the vibration of a building during an earthquake. For linear systems, the frequency of the steady-state vibration response resulting from the application of a periodic, harmonic input is equal to the frequency of the applied force or motion, with the response magnitude being dependent on the actual mechanical system.

'''Damped vibration:''' When the energy of a vibrating system is gradually dissipated by friction and other resistances, the vibrations are said to be damped. The vibrations gradually reduce or change in frequency or intensity or cease and the system rests in its equilibrium position. An example of this type of vibration is the [[Vehicle suspension|vehicular suspension]] dampened by the [[shock absorber]].

==Vibration testing==

Vibration testing is accomplished by introducing a forcing function into a structure, usually with some type of shaker. Alternately, a DUT (device under test) is attached to the "table" of a shaker. Vibration testing is performed to examine the response of a device under test (DUT) to a defined vibration environment. The measured response may be fatigue life, resonant frequencies or squeak and rattle sound output ([[Noise, vibration, and harshness|NVH]]). Squeak and rattle testing is performed with a special type of ''quiet shaker'' that produces very low sound levels while under operation.

For relatively low frequency forcing, servohydraulic (electrohydraulic) shakers are used.  For higher frequencies, electrodynamic shakers are used.  Generally, one or more "input" or "control" points located on the DUT-side of a fixture is kept at a specified acceleration.&lt;ref name="Tustin06"&gt;Tustin, Wayne. ''[https://www.evaluationengineering.com/where-to-place-the-control-accelerometer Where to place the control accelerometer: one of the most critical decisions in developing random vibration tests also is the most neglected]'', EE-Evaluation Engineering, 2006&lt;/ref&gt; Other "response" points experience maximum vibration level (resonance) or minimum vibration level (anti-resonance). It is often desirable to achieve anti-resonance to keep a system from becoming too noisy, or to reduce strain on certain parts due to vibration modes caused by specific vibration frequencies .&lt;ref&gt;{{Cite web | title =Polytec InFocus 1/2007 | url =http://www.polytec.com/fileadmin/user_uploads/Applications/InFocus/Documents/OM_InFocus_2007_01_US.pdf}}&lt;/ref&gt;

The most common types of vibration testing services conducted by vibration test labs are Sinusoidal and Random. Sine (one-frequency-at-a-time) tests are performed to survey the structural response of the device under test (DUT). A random (all frequencies at once) test is generally considered to more closely replicate a real world environment, such as road inputs to a moving automobile.

Most vibration testing is conducted in a 'single DUT axis' at a time, even though most real-world vibration occurs in various axes simultaneously.  MIL-STD-810G, released in late 2008, Test Method 527, calls for multiple exciter testing. The ''vibration test fixture'' used to attach the DUT to the shaker table must be designed for the frequency range of the vibration test spectrum. Generally for smaller fixtures and lower frequency ranges, the designer targets a fixture design that is free of resonances in the test frequency range. This becomes more difficult as the DUT gets larger and as the test frequency increases. In these cases multi-point control strategies can mitigate some of the resonances that may be present in the future. Devices specifically designed to trace or record vibrations are called [[vibroscope]]s.

==Vibration analysis==
{{unreferenced section|date=July 2013}}
Vibration Analysis (VA), applied in an industrial or maintenance environment aims to reduce maintenance costs and equipment downtime by detecting equipment faults.&lt;ref&gt;Crawford, Art; Simplified Handbook of Vibration Analysis&lt;/ref&gt;&lt;ref&gt;Eshleman, R 1999, Basic machinery vibrations: An introduction to machine testing, analysis, and monitoring&lt;/ref&gt; VA is a key component of a Condition Monitoring (CM) program, and is often referred to as Predictive Maintenance (PdM).&lt;ref&gt;Mobius Institute; Vibration Analyst Category 2 - Course Notes 2013&lt;/ref&gt; Most commonly VA is used to detect faults in rotating equipment (Fans, Motors, Pumps, and Gearboxes etc.) such as Unbalance, Misalignment, rolling element bearing faults and resonance conditions.

VA can use the units of Displacement, Velocity and Acceleration displayed as a [[Waveform|time waveform]] (TWF), but most commonly the spectrum is used, derived from a [[fast Fourier transform]] of the TWF. The vibration spectrum provides important frequency information that can pinpoint the faulty component.

The fundamentals of vibration analysis can be understood by studying the simple [[Mass-spring-damper]] model.  Indeed, even a complex structure such as an automobile body can be modeled as a "summation" of simple mass&amp;ndash;spring&amp;ndash;damper models.  The mass&amp;ndash;spring&amp;ndash;damper model is an example of a [[simple harmonic oscillator]]. The mathematics used to describe its behavior is identical to other simple harmonic oscillators such as the [[RLC circuit]].

Note: This article does not include the step-by-step mathematical derivations, but focuses on major vibration analysis equations and concepts.  Please refer to the references at the end of the article for detailed derivations.

===Free vibration without damping===

[[File:Mass spring.svg|200px|right|Simple Mass Spring Model]] To start the investigation of the mass–spring–damper assume the damping is negligible and that there is no external force applied to the mass (i.e. free vibration). The force applied to the mass by the spring is proportional to the amount the spring is stretched "x" (assuming the spring is already compressed due to the weight of the mass). The proportionality constant, k, is the stiffness of the spring and has units of force/distance (e.g. lbf/in or N/m). The negative sign indicates that the force is always opposing the motion of the mass attached to it:
:&lt;math&gt;
F_s=- k x. \!
&lt;/math&gt;
The force generated by the mass is proportional to the acceleration of the mass as given by [[Newton's laws of motion|Newton's second law of motion]]:
:&lt;math&gt; 
\Sigma\ F = ma = m \ddot{x} = m \frac{d^2x}{dt^2}. 
&lt;/math&gt;
The sum of the forces on the mass then generates this [[ordinary differential equation]]: &lt;math&gt;  \ m \ddot{x} + k x = 0.&lt;/math&gt; 
[[File:Simple harmonic oscillator.gif|thumb|100px|right|Simple harmonic motion of the mass–spring system]] Assuming that the initiation of vibration begins by stretching the spring by the distance of ''A'' and releasing, the solution to the above equation that describes the motion of mass is: 
:&lt;math&gt;
x(t) =  A \cos (2 \pi f_n  t). \!
&lt;/math&gt;
This solution says that it will oscillate with [[simple harmonic motion]] that has an [[amplitude]] of ''A'' and a frequency of ''f&lt;sub&gt;n&lt;/sub&gt;''. The number ''f&lt;sub&gt;n&lt;/sub&gt;'' is called the '''undamped natural frequency'''. For the simple mass–spring system, ''f&lt;sub&gt;n&lt;/sub&gt;'' is defined as:

:&lt;math&gt;
f_n = {1\over {2 \pi}} \sqrt{k \over m}. \!
&lt;/math&gt;

Note: [[angular frequency]] ω (ω=2 π ''f'') with the units of radians per second is often used in equations because it simplifies the equations, but is normally converted to [[ordinary frequency]] (units of [[hertz|Hz]] or equivalently cycles per second) when stating the frequency of a system. If the mass and stiffness of the system is known, the formula above can determine the frequency at which the system vibrates once set in motion by an initial disturbance. Every vibrating system has one or more natural frequencies that it vibrates at once disturbed. This simple relation can be used to understand in general what happens to a more complex system once we add mass or stiffness. For example, the above formula explains why, when a car or truck is fully loaded, the suspension feels ″softer″ than unloaded—the mass has increased, reducing the natural frequency of the system.

====What causes the system to vibrate: from conservation of energy point of view====

Vibrational motion could be understood in terms of [[conservation of energy]]. In the above example the spring has been extended by a value of x and therefore some [[potential energy]] (&lt;math&gt;\tfrac {1}{2} k x^2&lt;/math&gt;) is stored in the spring. Once released, the spring tends to return to its un-stretched state (which is the minimum potential energy state) and in the process accelerates the mass. At the point where the spring has reached its un-stretched state all the potential energy that we supplied by stretching it has been transformed into [[kinetic energy]] (&lt;math&gt;\tfrac {1}{2} m v^2&lt;/math&gt;). The mass then begins to decelerate because it is now compressing the spring and in the process transferring the kinetic energy back to its potential. Thus oscillation of the spring amounts to the transferring back and forth of the kinetic energy into potential energy. In this simple model the mass continues to oscillate forever at the same magnitude—but in a real system, ''damping'' always dissipates the energy, eventually bringing the spring to rest.

===Free vibration with damping===
[[File:Mass spring damper.svg|thumb|right|Mass–spring–damper model]]

When a "viscous" damper is added to the model this outputs a force that is proportional to the velocity of the mass.  The damping is called viscous because it models the effects of a fluid  within an object.  The proportionality constant ''c'' is called the damping coefficient and has units of Force over velocity (lbf⋅s/in or N⋅s/m).

:&lt;math&gt; F_\text{d}  =  - c v  = - c \dot{x} =  - c \frac{dx}{dt}. &lt;/math&gt;

Summing the forces on the mass results in the following ordinary differential equation:

:&lt;math&gt;m \ddot{x} + c \dot{x} + kx = 0.&lt;/math&gt;

The solution to this equation depends on the amount of damping.  If the damping is small enough, the system still vibrates—but eventually, over time, stops vibrating.  This case is called underdamping, which is important in vibration analysis. If  damping is increased just to the point where the system no longer oscillates, the system has reached the point of [[critical damping]]. If the damping is increased past critical damping, the system is [[overdamped]].   The value that the damping coefficient must reach for critical damping in the [[mass-spring-damper model]] is:

:&lt;math&gt;c_\text{c} = 2 \sqrt{\text{km}}.&lt;/math&gt;

To characterize the amount of damping in a system a ratio called the [[damping ratio]] (also known as damping factor and % critical damping) is used.  This damping ratio is just a ratio of the actual damping over the amount of damping required to reach critical damping.  The formula for the damping ratio (&lt;math&gt;\zeta &lt;/math&gt;) of the mass-spring-damper model is:

:&lt;math&gt;\zeta = { c \over 2 \sqrt{\text{km}} }.&lt;/math&gt;

For example, metal structures (e.g., airplane fuselages, engine crankshafts) have damping factors less than 0.05, while automotive suspensions are in the range of 0.2&amp;ndash;0.3. The solution to the [[underdamped system]] for the mass-spring-damper model is the following:

:&lt;math&gt;x(t)=X  e^{-\zeta \omega_n t} \cos\left( \sqrt{1-\zeta^2} \omega_n t - \phi \right) , \qquad \omega_n = 2\pi f_n. &lt;/math&gt;

[[File:Damped Free Vibration.png|thumb|right|Free vibration with 0.1 and 0.3 damping ratio]]

The value of ''X'', the initial magnitude, and &lt;math&gt; \phi, &lt;/math&gt; the [[Phase (waves)#Phase shift|phase shift]], are determined by the amount the spring is stretched.  The formulas for these values can be found in the references.

====Damped and undamped natural frequencies====
&lt;!--A link from Damping links here--&gt;

The major points to note from the solution are the exponential term and the cosine function.  The exponential term defines how quickly the system “damps” down – the larger the damping ratio, the quicker it damps to zero.  The cosine function is the oscillating portion of the solution, but the frequency of the oscillations is different from the undamped case.

The frequency in this case is called the "damped natural frequency", &lt;math&gt; f_\text{d}, &lt;/math&gt; and is related to the undamped natural frequency by the following formula:

:&lt;math&gt;f_\text{d}= f_n\sqrt{1-\zeta^2}.&lt;/math&gt;

The damped natural frequency is less than the undamped natural frequency, but for many practical cases the damping ratio is relatively small and hence the difference is negligible. Therefore, the damped and undamped description are often dropped when stating the natural frequency (e.g. with 0.1 damping ratio, the damped natural frequency is only 1% less than the undamped).

The plots to the side present how 0.1 and 0.3 damping ratios effect how the system “rings” down over time. What is often done in practice is to experimentally measure the free vibration after an impact (for example by a hammer) and then determine the natural frequency of the system by measuring the rate of oscillation, as well as the damping ratio by measuring the rate of decay. The natural frequency and damping ratio are not only important in free vibration, but also characterize how a system behaves under forced vibration.{{multiple image
|align = left 
|width1 = 300
|image1 = Spring-mass undamped.gif
|caption1 = Spring mass undamped
|width2 = 300
|image2 = Spring-mass under-damped.gif
|caption2 = Spring mass underdamped
|width3 = 300
|image3 = Spring-mass critically-damped.gif
|caption3 = Spring mass critically damped
|width4 = 300
|image4 = Spring-mass over-damped.gif
|caption4 = Spring mass overdamped
}}
&lt;ref name="Simionescu 2014"&gt;
{{cite book|last=Simionescu|first=P.A.|title=Computer Aided Graphing and Simulation Tools for AutoCAD Users|year=2014|publisher=CRC Press|location=Boca Raton, FL|isbn=978-1-4822-5290-3|edition=1st}}&lt;/ref&gt;

===Forced vibration with damping===
The behavior of the spring mass damper model varies with the addition of a harmonic force. A force of this type could, for example, be generated by a rotating imbalance.

:&lt;math&gt;F= F_0 \sin(2 \pi f t). \!&lt;/math&gt;

Summing the forces on the mass results in the following ordinary differential equation:

:&lt;math&gt;m \ddot{x} + c\dot{x} + k x = F_0 \sin(2 \pi f t). &lt;/math&gt;

The [[steady state]] solution of this problem can be written as:

:&lt;math&gt;x(t)= X \sin(2 \pi f t +\phi). \!&lt;/math&gt;

The result states that the mass will oscillate at the same frequency, ''f'', of the applied force, but with a phase shift &lt;math&gt; \phi. &lt;/math&gt;

The amplitude of the vibration “X” is defined by the following formula.

:&lt;math&gt;X= {F_0 \over k} {1 \over \sqrt{(1-r^2)^2 + (2 \zeta r)^2}}.&lt;/math&gt;

Where “r” is defined as the ratio of the harmonic force frequency over the undamped natural frequency of the mass&amp;ndash;spring&amp;ndash;damper model.

:&lt;math&gt;r=\frac{f}{f_n}.&lt;/math&gt;

The phase shift, &lt;math&gt;\phi,&lt;/math&gt; is defined by the following formula.

:&lt;math&gt;\phi= \arctan\left (\frac{-2 \zeta r}{1-r^2} \right). &lt;/math&gt;

[[File:Forced Vibration Response.png|700px|Forced Vibration Response]]

The plot of these functions, called "the frequency response of the system", presents one of the most important features in forced vibration.  In a lightly damped system when the forcing frequency nears the natural frequency (&lt;math&gt;r \approx 1 &lt;/math&gt;) the amplitude of the vibration can get extremely high.  This phenomenon is called '''[[mechanical resonance|resonance]]''' (subsequently the natural frequency of a system is often referred to as the resonant frequency).  In rotor bearing systems any rotational speed that excites a resonant frequency is referred to as a [[critical speed]].

If resonance occurs in a mechanical system it can be very harmful – leading to eventual failure of the system. Consequently, one of the major reasons for vibration analysis is to predict when this type of resonance may occur and then to determine what steps to take to prevent it from occurring. As the amplitude plot shows, adding damping can significantly reduce the magnitude of the vibration.  Also, the magnitude can be reduced if the natural frequency can be shifted away from the forcing frequency by changing the stiffness or mass of the system. If the system cannot be changed, perhaps the forcing frequency can be shifted (for example, changing the speed of the machine generating the force).

The following are some other points in regards to the forced vibration shown in the frequency response plots.

*At a given frequency ratio, the amplitude of the vibration, ''X'', is directly proportional to the amplitude of the force &lt;math&gt;F_0 &lt;/math&gt; (e.g. if you double the force, the vibration doubles)
*With little or no damping, the vibration is in phase with the forcing frequency when the frequency ratio ''r''&amp;nbsp;&lt;&amp;nbsp;1 and 180 degrees out of phase when the frequency ratio ''r''&amp;nbsp;&gt;&amp;nbsp;1
*When ''r''&amp;nbsp;≪&amp;nbsp;1 the amplitude is just the deflection of the spring under the static force &lt;math&gt;F_0. &lt;/math&gt;  This deflection is called the static deflection &lt;math&gt;\delta_{st}.&lt;/math&gt; Hence, when ''r''&amp;nbsp;≪&amp;nbsp;1 the effects of the damper and the mass are minimal.
*When ''r''&amp;nbsp;≫&amp;nbsp;1 the amplitude of the vibration is actually less than the static deflection &lt;math&gt;\delta_{st}.&lt;/math&gt;  In this region the force generated by the mass (''F''&amp;nbsp;=&amp;nbsp;''ma'') is dominating because the acceleration seen by the mass increases with the frequency. Since the deflection seen in the spring, ''X'', is reduced in this region, the force transmitted by the spring (''F''&amp;nbsp;=&amp;nbsp;''kx'') to the base is reduced. Therefore, the mass&amp;ndash;spring&amp;ndash;damper system is isolating the harmonic force from the mounting base – referred to as [[vibration isolation]]. More damping actually reduces the effects of vibration isolation when ''r''&amp;nbsp;≫&amp;nbsp;1 because the damping force (''F''&amp;nbsp;=&amp;nbsp;''cv'') is also transmitted to the base.
* whatever the damping is, the vibration is 90 degrees out of phase with the forcing frequency when the frequency ratio ''r''&amp;nbsp;=&amp;nbsp;1, which is very helpful when it comes to determining the natural frequency of the system.
* whatever the damping is, when ''r''&amp;nbsp;≫&amp;nbsp;1, the vibration is 180 degrees out of phase with the forcing frequency
* whatever the damping is, when ''r''&amp;nbsp;≪&amp;nbsp;1, the vibration is in phase with the forcing frequency

====Resonance causes====

Resonance is simple to understand if the spring and mass are viewed as energy storage elements – with the mass storing kinetic energy and the spring storing potential energy. As discussed earlier, when the mass and spring have no external force acting on them they transfer energy back and forth at a rate equal to the natural frequency. In other words, to efficiently pump energy into both mass and spring requires that the energy source feed the energy in at a rate equal to the natural frequency. Applying a force to the mass and spring is similar to pushing a child on swing, a push is needed at the correct moment to make the swing get higher and higher. As in the case of the swing, the force applied need not be high to get large motions, but must just add energy to the system.

The damper, instead of storing energy, dissipates energy. Since the damping force is proportional to the velocity, the more the motion, the more the damper dissipates the energy. Therefore, there is a point when the energy dissipated by the damper equals the energy added by the force. At this point, the system has reached its maximum amplitude and will continue to vibrate at this level as long as the force applied stays the same. If no damping exists, there is nothing to dissipate the energy and, theoretically, the motion will continue to grow into infinity.

====Applying "complex" forces to the mass&amp;ndash;spring&amp;ndash;damper model====

In a previous section only a simple harmonic force was applied to the model, but this can be extended considerably using two powerful mathematical tools. The first is the [[Fourier transform]] that takes a signal as a function of time ([[time domain]]) and breaks it down into its harmonic components as a function of frequency ([[frequency domain]]). For example, by applying a force to the mass&amp;ndash;spring&amp;ndash;damper model that repeats the following cycle – a force equal to 1&amp;nbsp;[[Newton (unit)|newton]] for 0.5&amp;nbsp;second and then no force for 0.5&amp;nbsp;second. This type of force has the shape of a 1&amp;nbsp;Hz [[square wave]].

[[File:Square wave frequency spectrum animation.gif|thumb|300px|How a 1 Hz square wave can be represented as a summation of sine waves(harmonics) and the corresponding frequency spectrum. Click and go to full resolution for an animation]]

The Fourier transform of the square wave generates a [[frequency spectrum]] that presents the magnitude of the harmonics that make up the square wave (the phase is also generated, but is typically of less concern and therefore is often not plotted). The Fourier transform can also be used to analyze non-[[periodic function|periodic]] functions such as transients (e.g. impulses) and random functions. The Fourier transform is almost always computed using the [[fast Fourier transform]] (FFT) computer algorithm in combination with a [[window function]].

In the case of our square wave force, the first component is actually a constant force of 0.5 newton and is represented by a value at 0&amp;nbsp;Hz in the frequency spectrum. The next component is a 1&amp;nbsp;Hz sine wave with an amplitude of 0.64.  This is shown by the line at 1&amp;nbsp;Hz. The remaining components are at odd frequencies and it takes an infinite amount of sine waves to generate the perfect square wave.  Hence, the Fourier transform allows you to interpret the force as a sum of sinusoidal forces being applied instead of a more "complex" force (e.g. a square wave).

In the previous section, the vibration solution was given for a single harmonic force, but the Fourier transform in general gives multiple harmonic forces.  The second mathematical tool, "the principle of [[superposition principle|superposition"]], allows the summation of the solutions from multiple forces if the system is [[linear system|linear]]. In the case of the spring–mass–damper model, the system is linear if the spring force is proportional to the displacement and the damping is proportional to the velocity over the range of motion of interest.  Hence, the solution to the problem with a square wave is summing the predicted vibration from each one of the harmonic forces found in the frequency spectrum of the square wave.

====Frequency response model====

The solution of a vibration problem can be viewed as an input/output relation – where the force is the input and the output is the vibration.  Representing the force and vibration in the frequency domain (magnitude and phase) allows the following relation:

:&lt;math&gt;X(i\omega)=H(i\omega)\cdot F(i\omega) \text{ or } H(i\omega)= {X(i\omega) \over F(i\omega)}.&lt;/math&gt;

&lt;math&gt;H(i\omega)&lt;/math&gt; is called the [[frequency response]] function (also referred to as the [[transfer function]], but not technically as accurate) and has both a magnitude and phase component (if represented as a [[complex number]], a real and imaginary component). The magnitude of the frequency response function (FRF) was presented earlier for the mass&amp;ndash;spring&amp;ndash;damper system.

:&lt;math&gt;|H(i\omega)|=\left |{X(i\omega) \over F(i\omega)} \right|= {1 \over k} {1 \over \sqrt{(1-r^2)^2 + (2 \zeta r)^2}}, \text{ where } r=\frac{f}{f_n}=\frac{\omega}{\omega_n}.&lt;/math&gt;

The phase of the FRF was also presented earlier as:

:&lt;math&gt;\angle H(i\omega)= -\arctan\left (\frac{2 \zeta r}{1-r^2} \right). &lt;/math&gt;

For example, calculating the FRF for a mass&amp;ndash;spring&amp;ndash;damper system with a mass of 1&amp;nbsp;kg, spring stiffness of 1.93&amp;nbsp;N/mm and a damping ratio of 0.1. The values of the spring and mass give a natural frequency of 7&amp;nbsp;Hz for this specific system. Applying the 1&amp;nbsp;Hz square wave from earlier allows the calculation of the predicted vibration of the mass. The figure illustrates the resulting vibration. It happens in this example that the fourth harmonic of the square wave falls at 7&amp;nbsp;Hz. The frequency response of the mass&amp;ndash;spring&amp;ndash;damper therefore outputs a high 7&amp;nbsp;Hz vibration even though the input force had a relatively low 7&amp;nbsp;Hz harmonic. This example highlights that the resulting vibration is dependent on both the forcing function and the system that the force is applied to.

[[File:Frequency response example.png|thumb|left|300px|Frequency response model]]

The figure also shows the time domain representation of the resulting vibration. This is done by performing an inverse Fourier Transform that converts frequency domain data to time domain. In practice, this is rarely done because the frequency spectrum provides all the necessary information.

The frequency response function (FRF) does not necessarily have to be calculated from the knowledge of the mass, damping, and stiffness of the system—but can be measured experimentally. For example, if a known force over a range of frequencies is applied, and if the associated vibrations are measured, the frequency response function can be calculated, thereby characterizing the system. This technique is used in the field of experimental [[modal analysis]] to determine the vibration characteristics of a structure.

==Multiple degrees of freedom systems and mode shapes==

The simple mass&amp;ndash;spring&amp;ndash;damper model is the foundation of vibration analysis, but what about more complex systems?  The mass&amp;ndash;spring&amp;ndash;damper model described above is called a single [[degrees of freedom (engineering)|degree of freedom]] (SDOF) model since the mass is assumed to only move up and down.  In more complex systems, the system must be discretized into more masses that move in more than one direction, adding degrees of freedom.  The major concepts of multiple degrees of freedom (MDOF) can be understood by looking at just a 2 degree of freedom model as shown in the figure.
[[File:2dof model.gif|thumb|300px|right|2 degree of freedom model]]

The equations of motion of the 2DOF system are found to be:

:&lt;math&gt;
m_1 \ddot{x_1} + (c_1+c_2) \dot{x_1} - c_2 \dot{x_2}+ (k_1+k_2) x_1 - k_2 x_2= f_1,
&lt;/math&gt;

:&lt;math&gt;
m_2 \ddot{x_2} - c_2 \dot{x_1}+ (c_2+c_3) \dot{x_2} - k_2 x_1+ (k_2+k_3) x_2 = f_2. \!
&lt;/math&gt;

This can be rewritten in [[Matrix (mathematics)|matrix]] format:

:&lt;math&gt;
\begin{bmatrix}m_1 &amp; 0\\ 0 &amp; m_2\end{bmatrix}\begin{Bmatrix}\ddot{x_1}\\ \ddot{x_2} \end{Bmatrix} + \begin{bmatrix} c_1+c_2 &amp; -c_2\\ -c_2 &amp; c_2+c_3\end{bmatrix}\begin{Bmatrix}\dot{x_1}\\ \dot{x_2}\end{Bmatrix}+\begin{bmatrix}k_1+k_2 &amp; -k_2\\ -k_2 &amp; k_2+k_3\end{bmatrix}\begin{Bmatrix} x_1\\ x_2\end{Bmatrix}=\begin{Bmatrix} f_1\\ f_2\end{Bmatrix}.
&lt;/math&gt;

A more compact form of this matrix equation can be written as:

:&lt;math&gt;
\begin{bmatrix}M\end{bmatrix}\begin{Bmatrix}\ddot{x}\end{Bmatrix}+\begin{bmatrix}C\end{bmatrix}\begin{Bmatrix}\dot{x}\end{Bmatrix}+\begin{bmatrix}K\end{bmatrix}\begin{Bmatrix} x\end{Bmatrix}=\begin{Bmatrix} f \end{Bmatrix}
&lt;/math&gt;

where &lt;math&gt;\begin{bmatrix}M\end{bmatrix},&lt;/math&gt; &lt;math&gt;\begin{bmatrix}C\end{bmatrix},&lt;/math&gt; and &lt;math&gt;\begin{bmatrix}K\end{bmatrix}&lt;/math&gt; are [[symmetric matrices]] referred respectively as the mass, damping, and stiffness matrices.  The matrices are NxN square matrices where N is the number of degrees of freedom of the system.

The following analysis involves the case where there is no damping and no applied forces (i.e. free vibration). The solution of a viscously damped system is somewhat more complicated.&lt;ref name="MaiaSilva97"&gt;Maia, Silva. ''Theoretical And Experimental Modal Analysis'', Research Studies Press Ltd., 1997, {{ISBN|0-471-97067-0}}&lt;/ref&gt;

:&lt;math&gt;\begin{bmatrix}M\end{bmatrix}\begin{Bmatrix}\ddot{x}\end{Bmatrix}+\begin{bmatrix}K\end{bmatrix}\begin{Bmatrix} x\end{Bmatrix}=0.&lt;/math&gt;

This differential equation can be solved by assuming the following type of solution:

:&lt;math&gt;
\begin{Bmatrix} x\end{Bmatrix}=\begin{Bmatrix} X\end{Bmatrix}e^{i\omega t}.
&lt;/math&gt;

Note: Using the exponential solution of &lt;math&gt; \begin{Bmatrix} X\end{Bmatrix}e^{i\omega t}&lt;/math&gt; is a mathematical trick used to solve linear differential equations. Using [[Euler's formula]] and taking only the real part of the solution it is the same cosine solution for the 1 DOF system.  The exponential solution is only used because it is easier to manipulate mathematically.

The equation then becomes:

:&lt;math&gt;\begin{bmatrix}-\omega^2 \begin{bmatrix} M \end{bmatrix} + \begin{bmatrix} K \end{bmatrix} \end{bmatrix} \begin{Bmatrix}X\end{Bmatrix}e^{i\omega t}=0.&lt;/math&gt;

Since &lt;math&gt;e^{i\omega t}&lt;/math&gt; cannot equal zero the equation reduces to the following.

:&lt;math&gt;\begin{bmatrix}\begin{bmatrix}K\end{bmatrix}-\omega^2 \begin{bmatrix} M \end{bmatrix} \end{bmatrix} \begin{Bmatrix} X \end{Bmatrix}=0.&lt;/math&gt;

===Eigenvalue problem===

This is referred to an [[eigenvalue]] problem in mathematics and can be put in the standard format by pre-multiplying the equation by &lt;math&gt;\begin{bmatrix}M\end{bmatrix}^{-1}&lt;/math&gt;

:&lt;math&gt;\begin{bmatrix}\begin{bmatrix}M\end{bmatrix}^{-1}\begin{bmatrix}K\end{bmatrix}-\omega^2 \begin{bmatrix} M \end{bmatrix}^{-1} \begin{bmatrix}M\end{bmatrix}\end{bmatrix}\begin{Bmatrix}X\end{Bmatrix}=0&lt;/math&gt;

and if: &lt;math&gt;\begin{bmatrix}M\end{bmatrix}^{-1}\begin{bmatrix}K\end{bmatrix}=\begin{bmatrix}A\end{bmatrix}&lt;/math&gt; and &lt;math&gt;\lambda=\omega^2 \,&lt;/math&gt;

:&lt;math&gt;\begin{bmatrix}\begin{bmatrix}A\end{bmatrix}-\lambda\begin{bmatrix}I\end{bmatrix}\end{bmatrix}\begin{Bmatrix}X\end{Bmatrix}=0.&lt;/math&gt;

The solution to the problem results in N '''eigenvalues''' (i.e. &lt;math&gt;\omega_1^2,\omega_2^2,\cdots\omega_N^2&lt;/math&gt;), where N corresponds to the number of degrees of freedom.  The eigenvalues provide the natural frequencies of the system. When these eigenvalues are substituted back into the original set of equations, the values of &lt;math&gt;\begin{Bmatrix}X\end{Bmatrix}&lt;/math&gt; that correspond to each eigenvalue are called the '''eigenvectors'''.  These eigenvectors represent the [[mode shape]]s of the system. The solution of an eigenvalue problem can be quite cumbersome (especially for problems with many degrees of freedom), but fortunately most math analysis programs have eigenvalue routines.

The eigenvalues and eigenvectors are often written in the following matrix format and describe the modal model of the system:

:&lt;math&gt;\begin{bmatrix}^\diagdown \omega_{r\diagdown}^2 \end{bmatrix}=\begin{bmatrix} \omega_1^2 &amp; \cdots &amp; 0 \\ \vdots &amp; \ddots &amp; \vdots \\ 0 &amp; \cdots &amp; \omega_N^2 \end{bmatrix} \text{ and } \begin{bmatrix} \Psi \end{bmatrix}=\begin{bmatrix} \begin{Bmatrix} \psi_1 \end{Bmatrix} \begin{Bmatrix} \psi_2 \end{Bmatrix} \cdots \begin{Bmatrix} \psi_N \end{Bmatrix} \end{bmatrix}.&lt;/math&gt;

A simple example using the 2 DOF model can help illustrate the concepts. Let both masses have a mass of 1&amp;nbsp;kg and the stiffness of all three springs equal 1000 N/m.  The mass and stiffness matrix for this problem are then:

:&lt;math&gt;\begin{bmatrix}M\end{bmatrix}=\begin{bmatrix}1 &amp; 0\\ 0 &amp; 1\end{bmatrix}&lt;/math&gt; and &lt;math&gt;\begin{bmatrix}K\end{bmatrix}=\begin{bmatrix}2000 &amp; -1000\\ -1000 &amp; 2000\end{bmatrix}.&lt;/math&gt;

Then &lt;math&gt;\begin{bmatrix}A\end{bmatrix}=\begin{bmatrix}2000 &amp; -1000\\ -1000 &amp; 2000\end{bmatrix}.&lt;/math&gt;

The eigenvalues for this problem given by an eigenvalue routine is:

:&lt;math&gt;\begin{bmatrix} ^\diagdown \omega_{r\diagdown}^2 \end{bmatrix}=\begin{bmatrix} 1000 &amp;  0 \\ 0 &amp; 3000 \end{bmatrix}.&lt;/math&gt;

The natural frequencies in the units of hertz are then (remembering  &lt;math&gt;\scriptstyle \omega=2 \pi f&lt;/math&gt;) &lt;math&gt;\scriptstyle f_1=5.033 \mathrm {\ Hz}&lt;/math&gt; and &lt;math&gt;\scriptstyle f_2=8.717 \text{ Hz}.&lt;/math&gt;

The two mode shapes for the respective natural frequencies are given as:

:&lt;math&gt;\begin{bmatrix} \Psi \end{bmatrix}=\begin{bmatrix} \begin{Bmatrix} \psi_1 \end{Bmatrix} \begin{Bmatrix} \psi_2 \end{Bmatrix} \end{bmatrix}= \begin{bmatrix} \begin{Bmatrix} -0.707 \\ -0.707 \end{Bmatrix}_1 \begin{Bmatrix} 0.707  \\ -0.707  \end{Bmatrix}_2 \end{bmatrix}. &lt;/math&gt;

Since the system is a 2 DOF system, there are two modes with their respective natural frequencies and shapes.  The mode shape vectors are not the absolute motion, but just describe relative motion of the degrees of freedom.  In our case the first mode shape vector is saying that the masses are moving together in phase since they have the same value and sign.  In the case of the second mode shape vector, each mass is moving in opposite direction at the same rate.

===Illustration of a multiple DOF problem===
When there are many degrees of freedom, one method of visualizing the mode shapes is by animating them using structural analysis software such as [[Femap]] or [[ANSYS]]. An example of animating mode shapes is shown in the figure below for a [[cantilever]]ed [[I-beam|{{ibeam}}-beam]] as demonstrated using [[modal analysis]] on ANSYS. In this case, the [[finite element method]] was used to generate an approximation of the mass and stiffness matrices by meshing the object of interest in order to solve a [[Eigenvalues and eigenvectors#Vibration analysis|discrete eigenvalue problem]]. Note that, in this case, the finite element method provides an approximation of the meshed surface (for which there exists an infinite number of vibration modes and frequencies). Therefore, this relatively simple model that has over 100 degrees of freedom and hence as many natural frequencies and mode shapes, provides a good approximation for the first natural frequencies and modes{{ref|1|†}}. Generally, only the first few modes are important for practical applications.

&lt;center&gt;
{| width="1000" class="wikitable"

|-
| colspan=3|In this table the first and second (top and bottom respectively) [[Horizontal plane|horizontal]] [[bending]] (left), [[Torsion (mechanics)|torsion]]al (middle), and [[Vertical direction|vertical]] bending (right) vibrational modes of an [[I-beam|{{ibeam}}-beam]] are visualized. There also exist other kinds of vibrational modes in which the beam gets [[Compression (physical)|compressed]]/[[Stress (mechanics)|stretched]] out in the height, width and length directions respectively.
|-
! colspan=3|The mode shapes of a cantilevered I-beam
|-
|align="center"|[[File:beam mode 1.gif|200px|center]]
|align="center"|[[File:beam mode 2.gif|200px|center]]
|align="center"|[[File:beam mode 3.gif|200px|center]]
|-
|align="center"|[[File:beam mode 4.gif|200px|center]]
|align="center"|[[File:beam mode 5.gif|200px|center]]
|align="center"|[[File:beam mode 6.gif|200px|center]]
|-
|}
&lt;/center&gt;

{{note|1}} Note that when performing a numerical approximation of any mathematical model, convergence of the parameters of interest must be ascertained.

===Multiple DOF problem converted to a single DOF problem===

The eigenvectors have very important properties called orthogonality properties.  These properties can be used to greatly simplify the solution of multi-degree of freedom models.  It can be shown that the eigenvectors have the following properties:

:&lt;math&gt;\begin{bmatrix}\Psi\end{bmatrix}^{T}\begin{bmatrix}M\end{bmatrix}\begin{bmatrix}\Psi\end{bmatrix}=\begin{bmatrix} ^\diagdown m_{r\diagdown} \end{bmatrix},&lt;/math&gt;

:&lt;math&gt;\begin{bmatrix}\Psi\end{bmatrix}^{T}\begin{bmatrix}K\end{bmatrix}\begin{bmatrix}\Psi\end{bmatrix}=\begin{bmatrix} ^\diagdown k_{r\diagdown} \end{bmatrix}.&lt;/math&gt;

&lt;math&gt;\begin{bmatrix} ^\diagdown m_{r\diagdown} \end{bmatrix}&lt;/math&gt; and &lt;math&gt;\begin{bmatrix} ^\diagdown k_{r\diagdown} \end{bmatrix}&lt;/math&gt; are [[diagonal matrix|diagonal matrices]] that contain the modal mass and stiffness values for each one of the modes.  (Note: Since the eigenvectors (mode shapes) can be arbitrarily scaled, the orthogonality properties are often used to scale the eigenvectors so the modal mass value for each mode is equal to 1.  The modal mass matrix is therefore an [[identity matrix]])

These properties can be used to greatly simplify the solution of multi-degree of freedom models by making the following coordinate transformation.

:&lt;math&gt;\begin{Bmatrix} x \end{Bmatrix}= \begin{bmatrix} \Psi \end{bmatrix} \begin{Bmatrix} q \end{Bmatrix}. &lt;/math&gt;

Using this coordinate transformation in the original free vibration differential equation results in the following equation.

: &lt;math&gt;\begin{bmatrix}M\end{bmatrix}\begin{bmatrix} \Psi \end{bmatrix} \begin{Bmatrix} \ddot{q} \end{Bmatrix} + \begin{bmatrix} K \end{bmatrix} \begin{bmatrix} \Psi \end{bmatrix} \begin{Bmatrix} q\end{Bmatrix}=0.&lt;/math&gt;

Taking advantage of the orthogonality properties by premultiplying this equation by &lt;math&gt;\begin{bmatrix}\Psi\end{bmatrix}^{T}&lt;/math&gt;

:&lt;math&gt;\begin{bmatrix}\Psi\end{bmatrix}^{T}\begin{bmatrix}M\end{bmatrix}\begin{bmatrix} \Psi \end{bmatrix}\begin{Bmatrix}\ddot{q}\end{Bmatrix}+\begin{bmatrix}\Psi\end{bmatrix}^{T}\begin{bmatrix}K\end{bmatrix} \begin{bmatrix} \Psi \end{bmatrix} \begin{Bmatrix} q\end{Bmatrix}=0.&lt;/math&gt;

The orthogonality properties then simplify this equation to:

:&lt;math&gt;\begin{bmatrix} ^\diagdown m_{r\diagdown} \end{bmatrix} \begin{Bmatrix}\ddot{q}\end{Bmatrix}+\begin{bmatrix}^\diagdown k_{r\diagdown}\end{bmatrix} \begin{Bmatrix} q\end{Bmatrix}=0.&lt;/math&gt;

This equation is the foundation of vibration analysis for multiple degree of freedom systems. A similar type of result can be derived for damped systems.&lt;ref name="MaiaSilva97"/&gt; The key is that the modal mass and stiffness matrices are diagonal matrices and therefore the equations have been "decoupled".  In other words, the problem has been transformed from a large unwieldy multiple degree of freedom problem into many single degree of freedom problems that can be solved using the same methods outlined above.

Solving for ''x'' is replaced by solving for ''q'', referred to as the modal coordinates or modal participation factors.

It may be clearer to understand if  &lt;math&gt;\begin{Bmatrix} x \end{Bmatrix}= \begin{bmatrix} \Psi \end{bmatrix} \begin{Bmatrix} q \end{Bmatrix} &lt;/math&gt; is written as:

:&lt;math&gt;\begin{Bmatrix} x_n \end{Bmatrix}= q_1\begin{Bmatrix} \psi \end{Bmatrix}_1  +q_2\begin{Bmatrix} \psi \end{Bmatrix}_2  +q_3\begin{Bmatrix} \psi \end{Bmatrix}_3 +\cdots +  q_N\begin{Bmatrix} \psi \end{Bmatrix}_N.&lt;/math&gt;

Written in this form it can be seen that the vibration at each of the degrees of freedom is just a linear sum of the mode shapes.  Furthermore, how much each mode "participates" in the final vibration is defined by q, its modal participation factor.

=== Rigid-body mode ===
An unrestrained multi-degree of freedom system experiences both rigid-body translation and/or rotation and vibration. The existence of a rigid-body mode results in a zero natural frequency. The corresponding mode shape is called the rigid-body mode. 

==See also==
{{Div col|colwidth=22em}}
*[[Acoustical engineering|Acoustic engineering]]
*[[Anti-vibration compound]]
*[[Balancing machine]]
*[[Base isolation]]
*[[Cushioning]]
*[[Critical speed]]
*[[Damping]]
*[[Dunkerley's method]]
*[[Earthquake engineering]]
*[[Fast Fourier transform]]
*[[Mechanical engineering]]
*[[Mechanical resonance]]
*[[Modal analysis]]
*[[Mode shape]]
*[[Noise and vibration on maritime vessels]]
*[[Noise, vibration, and harshness]]
*[[Pallesthesia]]
*[[Passive heave compensation]]
*[[Pendulum]]
*[[Quantum vibration]]
*[[Random vibration]]
*[[Ride quality]]
*[[Rayleigh's quotient in vibrations analysis]]
*[[Shaker (testing device)]]
*[[Shock (mechanics)|Shock]]
*[[Shock and vibration data logger]]
*[[Simple harmonic oscillator]]
*[[Spring pendulum]]
*[[Sound]]
*[[Structural acoustics]]
*[[Structural dynamics]]
*[[Tire balance]]
*[[Torsional vibration]]
*[[Tuned mass damper]]
*[[Vibration Calibrator|Vibration calibrator]]
*[[Vibration control]]
*[[Vibration isolation]]
*[[Vibration of rotating structures]]
*[[Wave]]
*[[Whole body vibration]]
{{div col end}}

==References==
{{reflist}}

==Further reading==
{{refbegin}}
*Tongue, Benson, ''Principles of Vibration'', Oxford University Press, 2001, {{ISBN|0-19-514246-2}}
*Inman, Daniel J., ''Engineering Vibration'', Prentice Hall, 2001, {{ISBN|0-13-726142-X}}
*Thompson, W.T., ''Theory of Vibrations'', Nelson Thornes Ltd, 1996, {{ISBN|0-412-78390-8}}
*Hartog, Den, ''Mechanical Vibrations'', Dover Publications, 1985, {{ISBN|0-486-64785-4}}
*{{cite book|last1=Reynolds|first1=Douglas D.|title=Engineering Principles of Mechanical Vibration.|date=2016|publisher=Trafford On Demand Publishing|location=Bloomington, Indiana, USA|isbn=978-1490714370|pages=485|edition=4th|accessdate=|language=English (U.S)}}
* [[Institute for Occupational Safety and Health of the German Social Accident Insurance]]: [http://www.dguv.de/ifa/fachinfos/vibrationen/index-2.jsp Whole-body and hand-arm vibration]

{{refend}}

==External links==
{{wiktionary|vibration}}
{{wikidata property|P2806}}
*[http://www.noisestructure.com/products/MPE_SDOF.php Free Excel sheets to estimate modal parameters]
*[https://www.mobiusinstitute.com/site2/detail.asp?LinkID=55 Vibration Analysis Reference - Mobius Institute]

[[Category:Mechanical vibrations| ]]
[[Category:Applied mathematics|V]]
[[nl:Trilling]]
[[pt:Vibração]]</text>
      <sha1>i9pwhofc0o652snn6o7ex3t0gjrhojc</sha1>
    </revision>
  </page>
</mediawiki>
