<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>Absolutely convex set</title>
    <ns>0</ns>
    <id>1734292</id>
    <revision>
      <id>847814922</id>
      <parentid>846409005</parentid>
      <timestamp>2018-06-27T23:48:53Z</timestamp>
      <contributor>
        <username>FrescoBot</username>
        <id>9021902</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:FrescoBot/Links|link syntax]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2647">A [[Set (mathematics)|set]] ''C'' in a [[real number|real]] or [[complex number|complex]] [[vector space]] is said to be '''absolutely convex''' or '''disked''' if it is [[convex set|convex]] and [[balanced set|balanced]] (circled), in which case it is called a '''disk'''.

== Properties ==

A set &lt;math&gt;C&lt;/math&gt; is absolutely convex if and only if for any points &lt;math&gt;x_1, \, x_2&lt;/math&gt; in &lt;math&gt;C&lt;/math&gt; and any numbers &lt;math&gt;\lambda_1, \, \lambda_2&lt;/math&gt; satisfying
&lt;math&gt;|\lambda_1| + |\lambda_2| \leq 1 &lt;/math&gt; the sum &lt;math&gt;\lambda_1 x_1 + \lambda_2 x_2&lt;/math&gt;
belongs to &lt;math&gt;C&lt;/math&gt;.

The intersection of arbitrarily many absolutely convex sets is again absolutely convex; however, unions of absolutely convex sets need not be absolutely convex anymore.

== Absolutely convex hull ==

[[File:Absolute convex hull.svg|thumb|right|The light gray area is the absolutely convex hull of the cross.]]

Since the intersection of any collection of absolutely convex sets is absolutely convex, one can define
for any subset ''A'' of a vector space its '''absolutely convex hull''' as the intersection of all absolutely convex sets containing ''A'', analogous to the well-known construction of the [[convex hull]]. 

More explicitly, one can define the absolutely convex hull of the set ''A'' via

&lt;math&gt;\mbox{absconv} A = \left\{\sum_{i=1}^n\lambda_i x_i : n \in \N, \, x_i \in A, \, \sum_{i=1}^n|\lambda_i| \leq 1 \right\},&lt;/math&gt;

where the λ&lt;sub&gt;i&lt;/sub&gt; are elements of the underlying field.

The absolutely convex hull of a [[Bounded_set_(topological_vector_space)|bounded set]] in a topological vector space is again bounded, the absolutely convex hull of a [[closed set]] is again closed. The same does not hold for [[Open_set|open sets]].

== See also ==
{{Wikibooks|Algebra|Vector spaces}}
* [[vector (geometric)]], for vectors in physics
* [[Vector field]]

== References ==
* {{cite book |last=Robertson |first=A.P. |author2= W.J. Robertson |title= Topological vector spaces |series=Cambridge Tracts in Mathematics |volume=53 |year=1964 |publisher= [[Cambridge University Press]] | pages=4–6 }}
* {{cite book 
|last= Narici 
|first= Lawrence
|last2= Beckenstein
|first2= Edward
|edition= Second
|publisher= Chapman and Hall/CRC
|date= July 26, 2010
|series= Pure and Applied Mathematics 
|number= 296
|title= Topological Vector Spaces, Second Edition
}}

* {{cite book |last=Schaefer |first=H.H. |title= Topological vector spaces |year=1999 |publisher= [[Springer-Verlag Press]] | pages=39 }}
{{Functional Analysis}}

[[Category:Abstract algebra]]
[[Category:Linear algebra]]
[[Category:Group theory]]
[[Category:Convex geometry]]</text>
      <sha1>1jrcxaabl24tqbtu2g71d2fnu3mxndt</sha1>
    </revision>
  </page>
  <page>
    <title>Ad hoc polymorphism</title>
    <ns>0</ns>
    <id>390418</id>
    <revision>
      <id>870356369</id>
      <parentid>846169797</parentid>
      <timestamp>2018-11-24T06:44:55Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <comment>adding link to references using [[Google Scholar]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6859">{{Polymorphism}}
In [[programming languages]], '''ad hoc polymorphism'''&lt;ref&gt;C. Strachey, [http://www.ics.uci.edu/~jajones/INF102-S18/readings/05_stratchey_1967.pdf Fundamental concepts in programming languages]. Lecture notes for International Summer School in Computer Programming, Copenhagen, August 1967&lt;/ref&gt; is a kind of [[polymorphism (computer science)|polymorphism]] in which polymorphic functions can be applied to arguments of different types, because a polymorphic function can denote a number of distinct and potentially heterogeneous implementations depending on the type of argument(s) to which it is applied. It is also known as [[function overloading]] or [[operator overloading]]. The term [[ad hoc]] in this context is not intended to be pejorative; it refers simply to the fact that this type of polymorphism is not a fundamental feature of the [[type system]]. This is in contrast to [[parametric polymorphism]], in which polymorphic functions are written without mention of any specific type, and can thus apply a single abstract implementation to any number of types in a transparent way. This classification was introduced by [[Christopher Strachey]] in 1967.

==Early binding==
Ad hoc polymorphism is a [[dynamic dispatch|dispatch]] mechanism: control moving through one named function is dispatched to various other functions without having to specify the exact function being called. Overloading allows multiple functions taking different types to be defined with the same name; the [[compiler]] or [[interpreter (computing)|interpreter]] automatically ensures that the right function is called. This way, functions appending lists of integers, lists of strings, lists of real numbers, and so on could be written, and all be called ''append''&amp;mdash;and the right ''append'' function would be called based on the type of lists being appended. This differs from parametric polymorphism, in which the function would need to be written ''generically'', to work with any kind of list. Using overloading, it is possible to have a function perform two completely different things based on the type of input passed to it; this is not possible with parametric polymorphism. Another way to look at overloading is that a routine is uniquely identified not by its name, but by the combination of its name and the number, order and types of its parameters.

This type of polymorphism is common in [[object-oriented programming]] languages, many of which allow [[operator (programming)|operator]]s to be overloaded in a manner similar to functions (see [[operator overloading]]). Some languages that are not dynamically typed and lack ad hoc  polymorphism (including type classes) have longer function names such as &lt;code&gt;print_int&lt;/code&gt;, &lt;code&gt;print_string&lt;/code&gt;, etc. This can be seen as advantage (more descriptive) or a disadvantage (overly verbose) depending on one's point of view.

An advantage that is sometimes gained from overloading is the appearance of specialization, e.g., a function with the same name can be implemented in multiple different ways, each optimized for the particular data types that it operates on. This can provide a convenient interface for code that needs to be specialized to multiple situations for performance reasons. The downside is that the type system cannot guarantee the consistency of the different implementations.

Since overloading is done at compile time, it is not a substitute for [[late binding]] as found in [[subtyping polymorphism]].

==Late binding==
The previous section notwithstanding, there are other ways in which ''ad hoc'' polymorphism can work out. Consider for example the Smalltalk language. In [[Smalltalk]], the overloading is done at run time, as the methods ("function implementation") for each overloaded message ("overloaded function") are resolved when they are about to be executed. This happens at run time, after the program is compiled. Therefore, polymorphism is given by [[subtyping polymorphism]] as in other languages, and it is also extended in functionality by ''ad hoc'' polymorphism at run time.

A closer look will also reveal that Smalltalk provides a slightly different variety of ''ad hoc'' polymorphism. Since Smalltalk has a late bound execution model, and since it provides objects the ability to handle messages that are not understood, it is possible to go ahead and implement functionality using polymorphism without explicitly overloading a particular message. This may not be generally recommended practice for everyday programming, but it can be quite useful when implementing proxies.

Also, while in general terms common class method and constructor overloading is not considered polymorphism, there are more uniform languages in which classes are regular objects. In Smalltalk, for instance, classes are regular objects. In turn, this means messages sent to classes can be overloaded, and it is also possible to create objects that behave like classes without their classes inheriting from the hierarchy of classes. These are effective techniques which can be used to take advantage of Smalltalk's powerful reflection capabilities. Similar arrangements are also possible in languages such as [[Self (programming language)|Self]] and Newspeak.

==Example==
Imagine an operator &lt;code&gt;+&lt;/code&gt; that may be used in the following ways:
# &lt;code&gt;1 + 2 = 3&lt;/code&gt;
# &lt;code&gt;3.14 + 0.0015 = 3.1415&lt;/code&gt;
# &lt;code&gt;1 + 3.7 = 4.7&lt;/code&gt;
# &lt;code&gt;[1, 2, 3] + [4, 5, 6] = [1, 2, 3, 4, 5, 6]&lt;/code&gt;
# &lt;code&gt;[true, false] + [false, true] = [true, false, false, true]&lt;/code&gt;
# &lt;code&gt;"bab" + "oon" = "baboon"&lt;/code&gt;

===Overloading===
To handle these six function calls, four different pieces of code are needed&amp;mdash;or ''three'', if strings are considered to be lists of characters:
* In the first case, [[integer (computer science)|integer]] addition must be invoked.
* In the second and third cases, [[floating point|floating-point]] addition must be invoked (with [[type promotion]], or [[type coercion]], in the third case).
* In the fourth and fifth cases, [[List (computing)|list]] [[concatenation]] must be invoked.
* In the last case, [[literal string|string]] concatenation must be invoked.
Thus, the name &lt;code&gt;+&lt;/code&gt; actually refers to three or four completely different functions. This is an example of ''overloading''.
(Note that string types used in the last case do not, by themselves, lend themselves to the programmer ''naturally'' assuming concatenation, rather than addition, is meant; consider "123" + "456", which might reasonably be expected to yield "579". Overloading can therefore provide different meaning, or semantics, for an operation, as well as differing implementations.)

==References==
{{reflist}}

&lt;!--Categories--&gt;
[[Category:Polymorphism (computer science)]]
[[Category:Programming language topics]]
[[Category:Type theory]]</text>
      <sha1>gky00g29vmtue323d0gkyyw21wowkeg</sha1>
    </revision>
  </page>
  <page>
    <title>Aluthge transform</title>
    <ns>0</ns>
    <id>58039179</id>
    <revision>
      <id>858522765</id>
      <parentid>853936678</parentid>
      <timestamp>2018-09-07T19:56:07Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2545">In mathematics and more precisely in [[functional analysis]], the '''Aluthge transformation''' is an operation defined on the set of [[bounded operators]] of a [[Hilbert space]]. It was introduced by [[Ariyadasa Aluthge]] to study [[P-hyponormal operators|p-hyponormal]] [[linear operator]]s.&lt;ref&gt;{{Cite journal|last=Aluthge|first=Ariyadasa|date=1990|title=On p-hyponormal operators for 0 &lt; ''p'' &lt; 1|url=|journal=Integral Equations Operator Theory|volume=13|pages=307–315|via=}}&lt;/ref&gt;

== Definition ==
Let  &lt;math&gt;H&lt;/math&gt; be a [[Hilbert space]] and let  &lt;math&gt;B(H)&lt;/math&gt; be the algebra of linear operators from &lt;math&gt;H&lt;/math&gt; to &lt;math&gt;H&lt;/math&gt;. By the [[polar decomposition]] theorem, there exists an unique [[partial isometry]]  &lt;math&gt;U&lt;/math&gt; such that  &lt;math&gt;T=U|T|&lt;/math&gt; and  &lt;math&gt;\ker(U)\supset\ker(T)&lt;/math&gt;, where  &lt;math&gt;|T|&lt;/math&gt; is the [[Square root of a matrix|square root of the operator]]  &lt;math&gt; T^*T&lt;/math&gt;.  If  &lt;math&gt;T\in B(H)&lt;/math&gt; and &lt;math&gt; T=U|T|&lt;/math&gt; is its polar decomposition, the Aluthge transform of  &lt;math&gt;T&lt;/math&gt; is the operator &lt;math&gt;\Delta(T)&lt;/math&gt; defined as:
: &lt;math&gt;\Delta(T)=|T|^{\frac12}U|T|^{\frac12}.&lt;/math&gt;

More generally, for any real number &lt;math&gt;\lambda\in [0,1]&lt;/math&gt;, the &lt;math&gt;\lambda&lt;/math&gt;-Aluthge transformation is defined as 
: &lt;math&gt;\Delta_\lambda(T):=|T|^{\lambda}U|T|^{1-\lambda}\in B(H).&lt;/math&gt;

== Example ==
For vectors &lt;math&gt;x,y \in H&lt;/math&gt;, let  &lt;math&gt;x\otimes y&lt;/math&gt; denote the operator defined as 
: &lt;math&gt;\forall z\in H\quad x\otimes y(z)=\langle z,y\rangle x.&lt;/math&gt;

An elementary calculation&lt;ref&gt;{{cite journal |last1=Chabbabi |first1=Fadil |last2=Mbekhta |first2=Mostafa |title=Jordan product maps commuting with the λ-Aluthge transform |journal=Journal of Mathematical Analysis and Applications |date=June 2017 |volume=450 |issue=1 |pages=293–313}}&lt;/ref&gt; shows that if &lt;math&gt;y\ne0&lt;/math&gt;, then &lt;math&gt;\Delta_\lambda(x\otimes y)=\Delta(x\otimes y)=\frac{\langle x,y\rangle}{\lVert y \rVert^2} y\otimes y.&lt;/math&gt;

== Notes ==
{{Reflist}}

== References ==

* {{Cite journal|last=Antezana|first=Jorge|last2=Pujals|first2=Enrique R.|last3=Stojanoff|first3=Demetrio|date=2008|title=Iterated Aluthge transforms: a brief survey|url=http://www.scielo.org.ar/scielo.php?script=sci_arttext&amp;pid=S0041-69322008000100004|journal=Revista de la Unión Matemática Argentina|volume=49|pages=29–41|via=}}

== External links ==

* {{MathGenealogy|id=59270|59270|title=Ariyadasa Aluthge|Ariyadasa Aluthge}}

[[Category:Bilinear forms]]
[[Category:Matrices]]
[[Category:Topology]]</text>
      <sha1>nxf0n5zskyc64c86tkbify3hxb7ecjc</sha1>
    </revision>
  </page>
  <page>
    <title>Atiyah conjecture on configurations</title>
    <ns>0</ns>
    <id>35131681</id>
    <revision>
      <id>823445513</id>
      <parentid>667618681</parentid>
      <timestamp>2018-02-01T08:34:16Z</timestamp>
      <contributor>
        <username>Bibcode Bot</username>
        <id>14394459</id>
      </contributor>
      <minor/>
      <comment>Adding 0 [[arXiv|arxiv eprint(s)]], 1 [[bibcode|bibcode(s)]] and 0 [[digital object identifier|doi(s)]]. Did it miss something? Report bugs, errors, and suggestions at [[User talk:Bibcode Bot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1108">In mathematics, the '''Atiyah conjecture on configurations''' is a [[conjecture]] introduced by {{harvs|txt|last=Atiyah|authorlink=Michael Atiyah|year1=2000|year2=2001}} stating that a certain ''n'' by ''n'' [[matrix (mathematics)|matrix]] depending on ''n'' points in '''R'''&lt;sup&gt;3&lt;/sup&gt; is always  [[non-singular matrix|non-singular]].

==See also==

*[[Berry–Robbins problem]]

==References==

*{{Citation | last1=Atiyah | first1=Michael | author1-link=Michael Atiyah | title=Surveys in differential geometry | publisher=Int. Press, Somerville, MA | series=Surv. Differ. Geom., VII | mr=1919420  | year=2000 | chapter=The geometry of classical particles | pages=1–15}}
*{{Citation | last1=Atiyah | first1=Michael | author1-link=Michael Atiyah | title=Configurations of points | doi=10.1098/rsta.2001.0840 | mr=1853626  | year=2001 | journal=The Royal Society of London. Philosophical Transactions. Series A. Mathematical, Physical and Engineering Sciences | issn=1364-503X | volume=359 | issue=1784 | pages=1375–1387| bibcode=2001RSPTA.359.1375A }}

{{Portal bar|Mathematics}}

[[Category:Geometry]]</text>
      <sha1>ikfhpvt7u50cmea6rt3yjzbnhaw6l0w</sha1>
    </revision>
  </page>
  <page>
    <title>Beta distribution</title>
    <ns>0</ns>
    <id>207074</id>
    <revision>
      <id>870322996</id>
      <parentid>870322091</parentid>
      <timestamp>2018-11-24T00:40:46Z</timestamp>
      <contributor>
        <username>Michael Hardy</username>
        <id>4626</id>
      </contributor>
      <comment>/* Effect of different prior probability choices on the posterior beta distribution */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="244442">{{Distinguish|Beta function}}
{{Probability distribution
| name       = Beta
| type       = density
| pdf_image  = [[File:Beta distribution pdf.svg|325px|Probability density function for the Beta distribution]]
| cdf_image  = [[File:Beta distribution cdf.svg|325px|Cumulative distribution function for the Beta distribution]]
| notation   = Beta(''α'', ''β'')
| parameters = ''α'' &gt; 0 [[shape parameter|shape]] ([[real number|real]])&lt;br /&gt;''β'' &gt; 0 [[shape parameter|shape]] ([[real number|real]])
| support    = &lt;math&gt;x \in [0, 1]\!&lt;/math&gt; or &lt;math&gt;x \in (0, 1)\!&lt;/math&gt;
| pdf        = &lt;math&gt;\frac{x^{\alpha-1}(1-x)^{\beta-1}} {\Beta(\alpha,\beta)}\!&lt;/math&gt;&lt;br /&gt;where &lt;math&gt;\Beta(\alpha,\beta) = \frac{\Gamma(\alpha)\Gamma(\beta)}{\Gamma(\alpha + \beta)}&lt;/math&gt;
| cdf    = &lt;math&gt;I_x(\alpha,\beta)\!&lt;/math&gt;
(the [[Beta function#Incomplete beta function|regularised incomplete beta function]])
| mean   = &lt;math&gt;\operatorname{E}[X] = \frac{\alpha}{\alpha+\beta}\!&lt;/math&gt;&lt;br /&gt;&lt;math&gt;\operatorname{E}[\ln X] = \psi(\alpha) - \psi(\alpha + \beta)\!&lt;/math&gt;&lt;br /&gt;&lt;br /&gt;&lt;math&gt;\operatorname{E}[X \, \ln X] = \frac{\alpha}{\alpha+\beta}\,\left[\psi(\alpha+1)-\psi(\alpha+\beta+1)\right]\!&lt;/math&gt;&lt;br /&gt;(see [[digamma function]] and see section: [[#Geometric mean|Geometric mean]])
| median = &lt;math&gt;\begin{matrix}I_{\frac{1}{2}}^{[-1]}(\alpha,\beta)\text{ (in general) }\\[0.5em]
\approx \frac{ \alpha - \tfrac{1}{3} }{ \alpha + \beta - \tfrac{2}{3} }\text{ for }\alpha, \beta &gt;1\end{matrix}&lt;/math&gt;
| mode     = 
&lt;math&gt;\frac{\alpha-1}{\alpha+\beta-2}\!&lt;/math&gt; for ''α'', ''β'' &gt;1

any value in &lt;math&gt;(0,1)&lt;/math&gt; for ''α'', ''β'' = 1

0 for ''α'' = 1, ''β'' &gt; 1

1 for ''α'' &gt; 1, ''β'' = 1
| variance = &lt;math&gt;\operatorname{var}[X] = \frac{\alpha\beta}{(\alpha+\beta)^2(\alpha+\beta+1)}\!&lt;/math&gt;&lt;br /&gt;&lt;math&gt;\operatorname{var}[\ln X] = \psi_1(\alpha) - \psi_1(\alpha + \beta)\!&lt;/math&gt;&lt;br /&gt;(see [[trigamma function]] and see section: [[#Geometric variance and covariance|Geometric variance]])
| skewness = &lt;math&gt;\frac{2\,(\beta-\alpha)\sqrt{\alpha+\beta+1}}{(\alpha+\beta+2)\sqrt{\alpha\beta}}&lt;/math&gt;
| kurtosis = &lt;math&gt;\frac{6[(\alpha - \beta)^2 (\alpha +\beta + 1) - \alpha \beta (\alpha + \beta + 2)]}{\alpha \beta (\alpha + \beta + 2) (\alpha + \beta + 3)}&lt;/math&gt;
| entropy  = &lt;math&gt;\begin{matrix}\ln\Beta(\alpha,\beta)-(\alpha-1)\psi(\alpha)-(\beta-1)\psi(\beta)\\[0.5em]
+(\alpha+\beta-2)\psi(\alpha+\beta)\end{matrix}&lt;/math&gt;
| mgf    = &lt;math&gt;1  +\sum_{k=1}^{\infty} \left( \prod_{r=0}^{k-1} \frac{\alpha+r}{\alpha+\beta+r} \right) \frac{t^k}{k!}&lt;/math&gt;
| char   = &lt;math&gt;{}_1F_1(\alpha; \alpha+\beta; i\,t)\!&lt;/math&gt; (see [[Confluent hypergeometric function]])
| fisher = &lt;math&gt;\begin{bmatrix} \operatorname{var}[\ln X] &amp;\operatorname{cov}[\ln X, \ln(1-X)] \\ \operatorname{cov}[\ln X, \ln(1-X)] &amp; \operatorname{var}[\ln (1-X)]\end{bmatrix}&lt;/math&gt; &lt;br /&gt;see section: [[#Fisher information matrix|Fisher information matrix]]
}}
In [[probability theory]] and [[statistics]], the '''beta distribution''' is a family of continuous [[probability distribution]]s defined on the interval [0, 1] [[Parametrization|parametrized]] by two positive [[shape parameter]]s, denoted by ''α'' and ''β'', that appear as exponents of the random variable and control the shape of the distribution.

The beta distribution has been applied to model the behavior of [[random variables]] limited to intervals of finite length in a wide variety of disciplines.

In [[Bayesian inference]], the beta distribution is the [[conjugate prior distribution|conjugate prior probability distribution]] for the [[Bernoulli distribution|Bernoulli]], [[binomial distribution|binomial]], [[negative binomial distribution|negative binomial]] and [[geometric distribution|geometric]] distributions. For example, the beta distribution can be used in Bayesian analysis to describe initial knowledge concerning probability of success such as the probability that a space vehicle will successfully complete a specified mission. The beta distribution is a suitable model for the random behavior of percentages and proportions.

The usual formulation of the beta distribution is also known as the '''beta distribution of the first kind''', whereas ''beta distribution of the second kind'' is an alternative name for the [[beta prime distribution]].

==Characterization==

===Probability density function===
The [[probability density function]] (pdf) of the beta distribution, for {{nowrap|0 ≤ ''x'' ≤ 1}}, and shape parameters ''α'', ''β'' &gt; 0, is a [[power function]] of the variable&amp;nbsp;''x'' and of its [[Reflection formula|reflection]] {{nowrap|(1 − ''x'')}} as follows:

: &lt;math&gt;
\begin{align}
f(x;\alpha,\beta) &amp; = \mathrm{constant}\cdot x^{\alpha-1}(1-x)^{\beta-1} \\[3pt]
&amp; = \frac{x^{\alpha-1}(1-x)^{\beta-1}}{\displaystyle \int_0^1 u^{\alpha-1} (1-u)^{\beta-1}\, du} \\[6pt]
&amp; = \frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\, x^{\alpha-1}(1-x)^{\beta-1} \\[6pt]
&amp; = \frac{1}{\Beta(\alpha,\beta)} x^{\alpha-1}(1-x)^{\beta-1}
\end{align}
&lt;/math&gt;

where Γ(''z'') is the [[gamma function]].  The [[beta function]], &lt;math&gt;\Beta&lt;/math&gt;, is a [[normalization constant]] to ensure that the total probability is&amp;nbsp;1. In the above equations ''x'' is a [[Realization (probability)|realization]]&amp;mdash;an observed value that actually occurred&amp;mdash;of a [[random process]]&amp;nbsp;''X''.

This definition includes both ends {{nowrap|1=''x'' = 0}} and {{nowrap|1=''x'' = 1}}, which is consistent with definitions for other [[List of probability distributions|continuous distributions supported on a bounded interval]] which are special cases of the beta distribution, for example the [[arcsine distribution]], and consistent with several authors, like [[Norman Lloyd Johnson|N. L. Johnson]] and [[Samuel Kotz|S. Kotz]].&lt;ref name=JKB /&gt;&lt;ref name=Keeping&gt;{{cite book|last=Keeping|first=E. S.|title=Introduction to Statistical Inference|year=2010|publisher=Dover Publications|isbn=978-0486685021}}&lt;/ref&gt;&lt;ref name=Wadsworth /&gt;&lt;ref name="Hahn and Shapiro"&gt;{{cite book|last1=Hahn|first1=Gerald J.|last2=Shapiro|first2=S.|title=Statistical Models in Engineering (Wiley Classics Library)|year=1994|publisher=Wiley-Interscience|isbn=978-0471040651}}&lt;/ref&gt; However, the inclusion of {{nowrap|1=''x'' = 0}} and {{nowrap|1=''x'' = 1}} does not work for {{nowrap|''α'', ''β'' &lt; 1}}; accordingly, several other authors, including [[William Feller|W. Feller]],&lt;ref name=Feller&gt;{{cite book|last=Feller|first=William|title=An Introduction to Probability Theory and Its Applications, Vol. 2|year=1971|publisher=Wiley|isbn=978-0471257097}}&lt;/ref&gt;&lt;ref name="Handbook of Beta Distribution" /&gt;&lt;ref name=Panik /&gt; choose to exclude the ends {{nowrap|1=''x'' = 0}} and {{nowrap|1=''x'' = 1}}, (so that the two ends are not actually part of the domain of the density function) and consider instead {{nowrap|0 &lt; ''x'' &lt; 1}}.

Several authors, including [[Norman Lloyd Johnson|N. L. Johnson]] and [[Samuel Kotz|S. Kotz]],&lt;ref name=JKB /&gt; use the symbols ''p'' and ''q'' (instead of ''α'' and ''β'') for the shape parameters of the beta distribution, reminiscent of the symbols traditionally used for the parameters of the [[Bernoulli distribution]], because the beta distribution approaches the Bernoulli distribution in the limit when both shape parameters ''α'' and ''β'' approach the value of zero.

In the following, a random variable ''X'' beta-distributed with parameters ''α'' and ''β'' will be denoted by:&lt;ref name="Mathematical Statistics with MATHEMATICA"/&gt;&lt;ref name=Kruschke /&gt;

:&lt;math&gt;X \sim \operatorname{Beta}(\alpha, \beta)&lt;/math&gt;

Other notations for beta-distributed random variables used in the statistical literature are &lt;math&gt;X \sim \mathcal{B}e(\alpha, \beta)&lt;/math&gt;&lt;ref name=BergerDecisionTheory&gt;{{cite book |last=Berger |first=James O. |title=Statistical Decision Theory and Bayesian Analysis |edition=2nd |year=2010 |publisher=Springer |isbn=978-1441930743}}&lt;/ref&gt; and &lt;math&gt;X \sim \beta_{\alpha, \beta}&lt;/math&gt;.&lt;ref name=Feller /&gt;

===Cumulative distribution function===
[[File:CDF for symmetric Beta distribution vs. x and alpha=beta - J. Rodal.jpg|thumb|CDF for symmetric beta distribution vs. ''x'' and&amp;nbsp;''α''&amp;nbsp;=&amp;nbsp;''β'']]
[[File:CDF for skewed Beta distribution vs. x and beta= 5 alpha - J. Rodal.jpg|thumb|CDF for skewed beta distribution vs. ''x'' and&amp;nbsp;''β''&amp;nbsp;=&amp;nbsp;5''α'']]

The [[cumulative distribution function]] is

:&lt;math&gt;F(x;\alpha,\beta) = \frac{\Beta{}(x;\alpha,\beta)}{\Beta{}(\alpha,\beta)} = I_x(\alpha,\beta)&lt;/math&gt;

where &lt;math&gt;\Beta(x;\alpha,\beta)&lt;/math&gt; is the [[beta function#Incomplete beta function|incomplete beta function]] and &lt;math&gt;I_x(\alpha,\beta)&lt;/math&gt; is the [[regularized incomplete beta function]].

==Properties==

===Measures of central tendency===

====Mode====
The [[Mode (statistics)|mode]] of a Beta distributed [[random variable]] ''X'' with ''α'', ''β'' &gt; 1 is the most likely value of the distribution (corresponding to the peak in the PDF), and is given by the following expression:&lt;ref name=JKB&gt;{{cite book|last1=Johnson|first1= Norman L. |first2= Samuel|last2= Kotz |first3= N. |last3= Balakrishnan| year=1995 |title=Continuous Univariate Distributions Vol. 2 |edition=2nd |publisher= Wiley |isbn= 978-0-471-58494-0 |chapter= Chapter 21:Beta Distributions}}&lt;/ref&gt;

:&lt;math&gt;\frac{\alpha - 1} {\alpha + \beta - 2} .&lt;/math&gt;

When both parameters are less than one (''α'', ''β'' &lt; 1), this is the anti-mode: the lowest point of the probability density curve.&lt;ref name=Wadsworth&gt;{{cite book|last=Wadsworth |first=George P. and Joseph Bryan |title=Introduction to Probability and Random Variables|year=1960|publisher=McGraw-Hill}}&lt;/ref&gt;

Letting ''α'' = ''β'', the expression for the mode simplifies to 1/2, showing that for ''α'' = ''β'' &gt; 1 the mode (resp. anti-mode when {{nowrap|''α'', ''β'' &lt; 1}}), is at the center of the distribution: it is symmetric in those cases.  See [[Beta distribution#Shapes|Shapes]] section in this article for a full list of mode cases, for arbitrary values of ''α'' and ''β''. For several of these cases, the maximum value of the density function occurs at one or both ends.  In some cases the (maximum) value of the density function occurring at the end is finite. For example, in the case of ''α'' = 2, ''β'' = 1 (or ''α'' = 1, ''β'' = 2), the density function becomes a [[Triangular distribution|right-triangle distribution]] which is finite at both ends. In several other cases there is a [[Mathematical singularity|singularity]] at one end, where the value of the density function approaches infinity. For example, in the case ''α'' = ''β'' = 1/2, the Beta distribution simplifies to become the [[arcsine distribution]].  There is debate among mathematicians about some of these cases and whether the ends (''x'' = 0, and ''x'' = 1) can be called ''modes'' or not.&lt;ref name="Handbook of Beta Distribution" /&gt;&lt;ref name="Mathematical Statistics with MATHEMATICA"&gt;{{cite book |last1=Rose |first1=Colin |last2=Smith |first2=Murray D. |title=Mathematical Statistics with MATHEMATICA |year=2002 |publisher=Springer |isbn=978-0387952345}}&lt;/ref&gt;
[[File:Mode Beta Distribution for alpha and beta from 1 to 5 - J. Rodal.jpg|325px|thumb|Mode for Beta distribution for 1 ≤ ''α'' ≤ 5 and 1 ≤ β ≤ 5]]

* Whether the ends are part of the [[Domain of a function|domain]] of the density function
* Whether a [[Mathematical singularity|singularity]] can ever be called a ''mode''
* Whether cases with two maxima should be called ''bimodal''

====Median====
[[File:Median Beta Distribution for alpha and beta from 0 to 5 - J. Rodal.jpg|325px|thumb|Median for Beta distribution for 0 ≤ ''α'' ≤ 5 and 0 ≤ ''β'' ≤ 5]]
[[File:(Mean - Median) for Beta distribution versus alpha and beta from 0 to 2 - J. Rodal.jpg|thumb|(Mean–Median) for Beta distribution versus alpha and beta from 0 to 2]]

The median of the beta distribution is the unique real number &lt;math&gt;x = I_{\frac{1}{2}}^{[-1]}(\alpha,\beta)&lt;/math&gt; for which the [[regularized incomplete beta function]] &lt;math&gt;I_x(\alpha,\beta) = \tfrac{1}{2} &lt;/math&gt;. There is no general [[closed-form expression]] for the [[median]] of the beta distribution for arbitrary values of ''α'' and ''β''.  [[Closed-form expression]]s for particular values of the parameters ''α'' and ''β'' follow:{{citation needed|date=February 2013}}

* For symmetric cases  ''α'' = ''β'', median = 1/2.
* For ''α'' = 1 and ''β'' &gt; 0, median &lt;math&gt; =1-2^{-\frac{1}{\beta}}&lt;/math&gt; (this case is the [[mirror image|mirror-image]] of the power function [0,1] distribution)
* For ''α'' &gt; 0 and ''β'' = 1, median = &lt;math&gt;2^{-\frac{1}{\alpha}}&lt;/math&gt; (this case is the power function [0,1] distribution&lt;ref name="Handbook of Beta Distribution" /&gt;)
* For ''α'' = 3 and ''β'' = 2, median = 0.6142724318676105..., the real solution to the [[Quartic function|quartic equation]] 1 − 8''x''&lt;sup&gt;3&lt;/sup&gt; + 6''x''&lt;sup&gt;4&lt;/sup&gt; = 0, which lies in [0,1].
* For ''α'' = 2 and ''β'' = 3, median = 0.38572756813238945... = 1−median(Beta(3, 2))

The following are the limits with one parameter finite (non-zero) and the other approaching these limits:{{citation needed|date=February 2013}}

:&lt;math&gt; \begin{align}
\lim_{\beta \to  0} \text{median}= \lim_{\alpha \to \infty} \text{median} = 1,\\
\lim_{\alpha\to  0} \text{median}= \lim_{\beta \to  \infty} \text{median} = 0.
\end{align}&lt;/math&gt;

A reasonable approximation of the value of the median of the beta distribution, for both α and β greater or equal to one, is given by the formula&lt;ref name=Kerman2011/&gt;

:&lt;math&gt;\text{median} \approx \frac{\alpha - \tfrac{1}{3}}{\alpha + \beta - \tfrac{2}{3}} \text{ for } \alpha, \beta \ge 1.&lt;/math&gt;

When α, β ≥ 1, the [[relative error]] (the [[approximation error|absolute error]] divided by the median) in this approximation is less than 4% and for both α ≥ 2 and β ≥ 2 it is less than 1%. The [[approximation error|absolute error]] divided by the difference between the mean and the mode is similarly small:

[[File:Relative Error for Approximation to Median of Beta Distribution for alpha and beta from 1 to 5 - J. Rodal.jpg|325px|Abs[(Median-Appr.)/Median] for Beta distribution for 1 ≤ ''α'' ≤ 5 and 1 ≤ ''β'' ≤ 5]][[File:Error in Median Apprx. relative to Mean-Mode distance for Beta Distribution with alpha and beta from 1 to 5 - J. Rodal.jpg|325px|Abs[(Median-Appr.)/(Mean-Mode)] for Beta distribution for 1≤α≤5 and 1≤β≤5]]

====Mean====
[[File:Mean Beta Distribution for alpha and beta from 0 to 5 - J. Rodal.jpg|325px|thumb|Mean for Beta distribution for {{nowrap|0 ≤ ''α'' ≤ 5}} and {{nowrap|0 ≤ ''β'' ≤ 5}}]]
The [[expected value]] (mean) (''μ'') of a Beta distribution [[random variable]] ''X'' with two parameters ''α'' and ''β'' is a function of only the ratio ''β''/''α'' of these parameters:&lt;ref name=JKB /&gt;

:&lt;math&gt; \begin{align}
\mu = \operatorname{E}[X]
     &amp;= \int_0^1 x f(x;\alpha,\beta)\,dx \\
     &amp;= \int_0^1 x \,\frac{x^{\alpha-1}(1-x)^{\beta-1}}{\Beta(\alpha,\beta)}\,dx \\
     &amp;= \frac{\alpha}{\alpha + \beta} \\
     &amp;= \frac{1}{1 + \frac{\beta}{\alpha}}
\end{align}&lt;/math&gt;

Letting {{nowrap|1=''α'' = ''β''}} in the above expression one obtains {{nowrap|1=''μ'' = 1/2}}, showing that for {{nowrap|1=''α'' = ''β''}} the mean is at the center of the distribution: it is symmetric. Also, the following limits can be obtained from the above expression:

:&lt;math&gt; \begin{align}
\lim_{\frac{\beta}{\alpha} \to  0} \mu = 1\\
\lim_{\frac{\beta}{\alpha} \to  \infty} \mu = 0
\end{align}&lt;/math&gt;

Therefore, for ''β''/''α'' → 0, or for ''α''/''β'' → ∞, the mean is located at the right end, {{nowrap|1=''x'' = 1}}. For these limit ratios, the beta distribution becomes a one-point [[degenerate distribution]] with a [[Dirac delta function]] spike at the right end, {{nowrap|1=''x'' = 1}}, with probability&amp;nbsp;1, and zero probability everywhere else. There is 100% probability (absolute certainty) concentrated at the right end, {{nowrap|1=''x'' = 1}}.

Similarly, for ''β''/''α'' → ∞, or for ''α''/''β'' → 0, the mean is located at the left end, {{nowrap|1=''x'' = 0}}.  The beta distribution becomes a 1-point [[Degenerate distribution]] with a [[Dirac delta function]] spike at the left end, ''x'' = 0, with probability 1, and zero probability everywhere else. There is 100% probability (absolute certainty) concentrated at the left end, ''x'' = 0. Following are the limits with one parameter finite (non-zero) and the other approaching these limits:

:&lt;math&gt; \begin{align}
\lim_{\beta \to  0} \mu = \lim_{\alpha \to  \infty} \mu = 1\\
\lim_{\alpha\to  0} \mu = \lim_{\beta \to  \infty} \mu = 0
\end{align}&lt;/math&gt;

While for typical unimodal distributions (with centrally located modes, inflexion points at both sides of the mode, and longer tails) (with Beta(''α'',&amp;nbsp;''β'') such that {{nowrap|''α'', ''β'' &gt; 2}}) it is known that the sample mean (as an estimate of location) is not as [[Robust statistics|robust]] as the sample median, the opposite is the case for uniform or "U-shaped" bimodal distributions (with Beta(''α'',&amp;nbsp;''β'') such that {{nowrap|''α'', ''β'' ≤ 1}}), with the modes located at the ends of the distribution.  As Mosteller and Tukey remark (&lt;ref name=MostellerTukey&gt;{{cite book|last=Mosteller|first=Frederick and John Tukey|title=Data Analysis and Regression: A Second Course in Statistics|year=1977|publisher=Addison-Wesley Pub. Co.,|isbn=978-0201048544}}&lt;/ref&gt; p.&amp;nbsp;207) "the average of the two extreme observations uses all the sample information. This illustrates how, for short-tailed distributions, the extreme observations should get more weight."  By contrast, it follows that the median of "U-shaped" bimodal distributions with modes at the edge of the distribution (with Beta(''α'',&amp;nbsp;''β'') such that {{nowrap|''α'', ''β'' ≤ 1}}) is not robust, as the sample median drops the extreme sample observations from consideration.  A practical application of this occurs for example for [[random walk]]s, since the probability for the time of the last visit to the origin in a random walk is distributed as the [[arcsine distribution]] Beta(1/2,&amp;nbsp;1/2):&lt;ref name=Feller/&gt;&lt;ref name=WillyFeller1/&gt; the mean of a number of [[realization (probability)|realizations]] of a random walk is a much more robust estimator than the median (which is an inappropriate sample measure estimate in this case).

====Geometric mean====
[[File:(Mean - GeometricMean) for Beta Distribution versus alpha and beta from 0 to 2 - J. Rodal.jpg|thumb|(Mean − GeometricMean) for Beta distribution versus ''α'' and ''β'' from 0 to 2, showing the asymmetry between ''α'' and ''β'' for the geometric mean]]
[[File:Geometric Means for Beta distribution Purple=G(X), Yellow=G(1-X), smaller values alpha and beta in front - J. Rodal.jpg|thumb|Geometric means for Beta distribution Purple = ''G''(''x''), Yellow = ''G''(1&amp;nbsp;−&amp;nbsp;''x''), smaller values ''α'' and ''β'' in front]]
[[File:Geometric Means for Beta distribution Purple=G(X), Yellow=G(1-X), larger values alpha and beta in front - J. Rodal.jpg|thumb|Geometric means for Beta distribution. purple = ''G''(''x''), yellow = ''G''(1&amp;nbsp;−&amp;nbsp;''x''), larger values ''α'' and ''β'' in front]]

The logarithm of the [[geometric mean]] ''G&lt;sub&gt;X&lt;/sub&gt;'' of a distribution with [[random variable]] ''X'' is the arithmetic mean of ln(''X''), or, equivalently, its expected value:

:&lt;math&gt;\ln G_X = \operatorname{E}[\ln X]&lt;/math&gt;

For a beta distribution, the expected value integral gives:

:&lt;math&gt;\begin{align}
\operatorname{E}[\ln X]
&amp;= \int_0^1 \ln x\, f(x;\alpha,\beta)\,dx \\[4pt]
&amp;= \int_0^1 \ln x \,\frac{ x^{\alpha-1}(1-x)^{\beta-1}}{\Beta(\alpha,\beta)}\,dx \\[4pt]
&amp;= \frac{1}{\Beta(\alpha,\beta)} \, \int_0^1 \frac{\partial x^{\alpha-1}(1-x)^{\beta-1}}{\partial \alpha}\,dx \\[4pt]
&amp;= \frac{1}{\Beta(\alpha,\beta)} \frac{\partial}{\partial \alpha} \int_0^1 x^{\alpha-1}(1-x)^{\beta-1}\,dx \\[4pt]
&amp;= \frac{1}{\Beta(\alpha,\beta)} \frac{\partial \Beta(\alpha,\beta)}{\partial \alpha} \\[4pt]
&amp;= \frac{\partial \ln \Beta(\alpha,\beta)}{\partial \alpha} \\[4pt]
&amp;= \frac{\partial \ln \Gamma(\alpha)}{\partial \alpha} - \frac{\partial \ln \Gamma(\alpha + \beta)}{\partial \alpha} \\[4pt]
&amp;= \psi(\alpha) - \psi(\alpha + \beta)
\end{align}&lt;/math&gt;

where ''ψ'' is the [[digamma function]].

Therefore, the geometric mean of a beta distribution with shape parameters ''α'' and ''β'' is the exponential of the digamma functions of ''α'' and ''β'' as follows:

:&lt;math&gt;G_X =e^{\operatorname{E}[\ln X]}= e^{\psi(\alpha) - \psi(\alpha + \beta)}&lt;/math&gt;

While for a beta distribution with equal shape parameters α = β, it follows that skewness = 0 and mode = mean = median = 1/2, the geometric mean is less than 1/2: {{nowrap|0 &lt; ''G''&lt;sub&gt;''X''&lt;/sub&gt; &lt; 1/2}}.  The reason for this is that the logarithmic transformation strongly weights the values of ''X'' close to zero, as ln(''X'') strongly tends towards negative infinity as ''X'' approaches zero, while ln(''X'') flattens towards zero as {{nowrap|''X'' → 1}}.

Along a line {{nowrap|1=''α'' = ''β''}}, the following limits apply:

:&lt;math&gt; \begin{align}
&amp;\lim_{\alpha = \beta \to 0} G_X = 0 \\
&amp;\lim_{\alpha = \beta \to \infty} G_X =\tfrac{1}{2}
\end{align}&lt;/math&gt;

Following are the limits with one parameter finite (non-zero) and the other approaching these limits:

:&lt;math&gt; \begin{align}
\lim_{\beta \to  0} G_X = \lim_{\alpha \to  \infty} G_X = 1\\
\lim_{\alpha\to  0} G_X = \lim_{\beta \to  \infty} G_X = 0
\end{align}&lt;/math&gt;

The accompanying plot shows the difference between the mean and the geometric mean for shape parameters α and β from zero to 2.  Besides the fact that the difference between them approaches zero as α and β approach infinity and that the difference becomes large for values of α and β approaching zero, one can observe an evident asymmetry of the geometric mean with respect to the shape parameters α and β. The difference between the geometric mean and the mean is larger for small values of α in relation to β than when exchanging the magnitudes of β and α.

[[Norman Lloyd Johnson|N. L.Johnson]] and [[Samuel Kotz|S. Kotz]]&lt;ref name=JKB /&gt; suggest the logarithmic approximation to the digamma function ''ψ''(''α'') ≈ ln(''α''&amp;nbsp;−&amp;nbsp;1/2) which results in the following approximation to the geometric mean:

:&lt;math&gt;G_X \approx \frac{\alpha \, - \frac{1}{2}}{\alpha +\beta - \frac{1}{2}}\text{ if } \alpha, \beta &gt; 1.&lt;/math&gt;

Numerical values for the [[relative error]] in this approximation follow: [{{nowrap|1=(''α'' = ''β'' = 1): 9.39%}}]; [{{nowrap|1=(''α'' = ''β'' = 2): 1.29%}}];  [{{nowrap|1=(''α'' = 2, ''β'' = 3): 1.51%}}];  [{{nowrap|1=(''α'' = 3, ''β'' = 2): 0.44%}}]; [{{nowrap|1=(''α'' = ''β'' = 3): 0.51%}}]; [{{nowrap|1=(''α'' = ''β'' = 4): 0.26%}}]; [{{nowrap|1=(''α'' = 3, ''β'' = 4): 0.55%}}];  [{{nowrap|1=(''α'' = 4, ''β'' = 3): 0.24%}}].

Similarly, one can calculate the value of shape parameters required for the geometric mean to equal&amp;nbsp;1/2. Given the value of the parameter ''β'', what would be the value of the other parameter,&amp;nbsp;''α'', required for the geometric mean to equal&amp;nbsp;1/2?.  The answer is that (for {{nowrap|''β'' &gt; 1}}), the value of ''α'' required tends towards {{nowrap|''β'' + 1/2}} as {{nowrap|''β'' → ∞}}. For example, all these couples have the same geometric mean of&amp;nbsp;1/2: [{{nowrap|1=''β'' = 1, ''α'' = 1.4427}}], [{{nowrap|1=''β'' = 2, ''α'' = 2.46958}}], [{{nowrap|1=''β'' = 3, ''α'' = 3.47943}}], [{{nowrap|1=''β'' = 4, ''α'' = 4.48449}}], [{{nowrap|1=''β'' = 5, ''α'' = 5.48756}}], [{{nowrap|1=''β'' = 10, ''α'' = 10.4938}}], [{{nowrap|1=''β'' = 100, ''α'' = 100.499}}].

The fundamental property of the geometric mean, which can be proven to be false for any other mean, is

:&lt;math&gt;G\left(\frac{X_i}{Y_i}\right) = \frac{G(X_i)}{G(Y_i)}&lt;/math&gt;

This makes the geometric mean the only correct mean when averaging ''normalized'' results, that is results that are presented as ratios to reference values.&lt;ref&gt;Philip J. Fleming and John J. Wallace. ''How not to lie with statistics: the correct way to summarize benchmark results''. Communications of the ACM, 29(3):218–221, March 1986.&lt;/ref&gt;  This is relevant because the beta distribution is a suitable model for the random behavior of percentages and it is particularly suitable to the statistical modelling of proportions.  The geometric mean plays a central role in maximum likelihood estimation, see section "Parameter estimation, maximum likelihood." Actually, when performing maximum likelihood estimation, besides the [[geometric mean]] ''G&lt;sub&gt;X&lt;/sub&gt;'' based on the random variable X, also another geometric mean appears naturally: the [[geometric mean]] based on the linear transformation ––{{nowrap|(1 − ''X'')}}, the mirror-image of ''X'', denoted by ''G''&lt;sub&gt;(1−''X'')&lt;/sub&gt;:

:&lt;math&gt;G_{(1-X)} = e^{\operatorname{E}[\ln(1-X)] } = e^{\psi(\beta) - \psi(\alpha + \beta)}&lt;/math&gt;

Along a line {{nowrap|1=''α'' = ''β''}}, the following limits apply:

:&lt;math&gt; \begin{align}
&amp;\lim_{\alpha = \beta \to 0} G_{(1-X)} =0 \\
&amp;\lim_{\alpha = \beta \to \infty} G_{(1-X)} =\tfrac{1}{2}
\end{align}&lt;/math&gt;

Following are the limits with one parameter finite (non-zero) and the other approaching these limits:

:&lt;math&gt; \begin{align}
\lim_{\beta \to  0} G_{(1-X)} = \lim_{\alpha \to  \infty} G_{(1-X)} = 0\\
\lim_{\alpha\to  0} G_{(1-X)} = \lim_{\beta \to  \infty} G_{(1-X)} = 1
\end{align}&lt;/math&gt;

It has the following approximate value:

:&lt;math&gt;G_{(1-X)} \approx \frac{\beta - \frac{1}{2}}{\alpha+\beta-\frac{1}{2}}\text{ if } \alpha, \beta &gt; 1.&lt;/math&gt;

Although both ''G''&lt;sub&gt;''X''&lt;/sub&gt; and  ''G''&lt;sub&gt;(1−''X'')&lt;/sub&gt; are asymmetric, in the case that both shape parameters are equal {{nowrap|1=''α'' = ''β''}}, the geometric means are equal: ''G''&lt;sub&gt;''X''&lt;/sub&gt; = ''G''&lt;sub&gt;(1−''X'')&lt;/sub&gt;.  This equality follows from the following symmetry displayed between both geometric means:

:&lt;math&gt;G_X (\Beta(\alpha, \beta) )=G_{(1-X)}(\Beta(\beta, \alpha) ). &lt;/math&gt;

====Harmonic mean====
[[File:Harmonic mean for Beta distribution for alpha and beta ranging from 0 to 5 - J. Rodal.jpg|thumb|Harmonic mean for beta distribution for 0&amp;nbsp;&lt;&amp;nbsp;''α''&amp;nbsp;&lt;&amp;nbsp;5 and 0&amp;nbsp;&lt;&amp;nbsp;''β''&amp;nbsp;&lt;&amp;nbsp;5]]
[[File:(Mean - HarmonicMean) for Beta distribution versus alpha and beta from 0 to 2 - J. Rodal.jpg|thumb|Harmonic mean for beta distribution versus ''α'' and ''β'' from 0 to 2]]
[[File:Harmonic Means for Beta distribution Purple=H(X), Yellow=H(1-X), smaller values alpha and beta in front - J. Rodal.jpg|thumb|Harmonic means for beta distribution Purple = ''H''(''X''), Yellow = ''H''(1&amp;nbsp;−&amp;nbsp;''X''), smaller values ''α'' and ''β'' in front]]
[[File:Harmonic Means for Beta distribution Purple=H(X), Yellow=H(1-X), larger values alpha and beta in front - J. Rodal.jpg|thumb|Harmonic Means for Beta distribution Purple = ''H''(''X''), Yellow = ''H''(1&amp;nbsp;−&amp;nbsp;''X''), larger values ''α'' and ''β'' in front]]

The inverse of the [[harmonic mean]] (''H&lt;sub&gt;X&lt;/sub&gt;'') of a distribution with [[random variable]] ''X'' is the arithmetic mean of 1/''X'', or, equivalently, its expected value. Therefore, the [[harmonic mean]] (''H&lt;sub&gt;X&lt;/sub&gt;'') of a beta distribution with shape parameters ''α'' and ''β'' is:

:&lt;math&gt; \begin{align}
H_X &amp;= \frac{1}{\operatorname{E}\left[\frac{1}{X}\right]} \\
    &amp;=\frac{1}{\int_0^1 \frac{f(x;\alpha,\beta)}{x}\,dx} \\
    &amp;=\frac{1}{\int_0^1 \frac{x^{\alpha-1}(1-x)^{\beta-1}}{x \Beta(\alpha,\beta)}\,dx} \\
    &amp;= \frac{\alpha - 1}{\alpha + \beta - 1}\text{ if } \alpha &gt; 1 \text{ and }  \beta &gt; 0 \\
\end{align}&lt;/math&gt;

The [[harmonic mean]] (''H&lt;sub&gt;X&lt;/sub&gt;'') of a Beta distribution with ''α'' &lt; 1 is undefined, because its defining expression is not bounded in [0, 1] for shape parameter ''α'' less than unity.

Letting ''α'' = ''β'' in the above expression one obtains

:&lt;math&gt;H_X = \frac{\alpha-1}{2\alpha-1},&lt;/math&gt;

showing that for ''α'' = ''β'' the harmonic mean ranges from 0, for ''α'' = ''β'' = 1, to 1/2, for ''α'' = ''β'' → ∞.

Following are the limits with one parameter finite (non-zero) and the other approaching these limits:

:&lt;math&gt; \begin{align}
&amp;\lim_{\alpha\to  0} H_X \text{ is undefined} \\
&amp;\lim_{\alpha\to  1} H_X = \lim_{\beta \to  \infty} H_X  =  0 \\
&amp;\lim_{\beta \to  0} H_X = \lim_{\alpha \to  \infty} H_X = 1
\end{align}&lt;/math&gt;

The harmonic mean plays a role in maximum likelihood estimation for the four parameter case, in addition to the geometric mean. Actually, when performing maximum likelihood estimation for the four parameter case, besides the harmonic mean ''H&lt;sub&gt;X&lt;/sub&gt;'' based on the random variable ''X'', also another harmonic mean appears naturally: the harmonic mean based on the linear transformation (1&amp;nbsp;−&amp;nbsp;''X''), the mirror-image of ''X'', denoted by ''H''&lt;sub&gt;1&amp;nbsp;−&amp;nbsp;''X''&lt;/sub&gt;:

:&lt;math&gt;H_{1-X} = \frac{1}{\operatorname{E} \left[\frac 1 {1-X}\right]} = \frac{\beta - 1}{\alpha + \beta-1} \text{ if } \beta &gt; 1, \text{ and } \alpha&gt; 0. &lt;/math&gt;

The [[harmonic mean]] (''H''&lt;sub&gt;(1&amp;nbsp;−&amp;nbsp;''X'')&lt;/sub&gt;) of a Beta distribution with ''β'' &lt; 1 is undefined, because its defining expression is not bounded in [0, 1] for shape parameter ''β'' less than unity.

Letting ''α'' = ''β'' in the above expression one obtains

:&lt;math&gt;H_{(1-X)} = \frac{\beta-1}{2\beta-1},&lt;/math&gt;

showing that for ''α'' = ''β'' the harmonic mean ranges from 0, for ''α'' = ''β'' = 1, to 1/2, for ''α'' = ''β'' → ∞.

Following are the limits with one parameter finite (non-zero) and the other approaching these limits:

:&lt;math&gt; \begin{align}
&amp;\lim_{\beta\to  0} H_{1-X} \text{ is undefined} \\
&amp;\lim_{\beta\to  1} H_{1-X} = \lim_{\alpha\to  \infty} H_{1-X}  =  0 \\
&amp;\lim_{\alpha\to 0} H_{1-X} = \lim_{\beta\to  \infty} H_{1-X} = 1
\end{align}&lt;/math&gt;

Although both ''H''&lt;sub&gt;''X''&lt;/sub&gt; and  ''H''&lt;sub&gt;1−''X''&lt;/sub&gt; are asymmetric, in the case that both shape parameters are equal ''α'' = ''β'', the harmonic means are equal: ''H''&lt;sub&gt;''X''&lt;/sub&gt; = ''H''&lt;sub&gt;1−''X''&lt;/sub&gt;.  This equality follows from the following symmetry displayed between both harmonic means:

:&lt;math&gt;H_X (\Beta(\alpha, \beta) )=H_{1-X}(\Beta(\beta, \alpha) ) \text{ if } \alpha, \beta&gt; 1.&lt;/math&gt;

===Measures of statistical dispersion===

====Variance====
The [[variance]] (the second moment centered on the mean) of a Beta distribution [[random variable]] ''X'' with parameters α and β is:&lt;ref name=JKB /&gt;&lt;ref&gt;{{cite web | url = http://www.itl.nist.gov/div898/handbook/eda/section3/eda366h.htm | title = NIST/SEMATECH e-Handbook of Statistical Methods 1.3.6.6.17. Beta Distribution | website = [[National Institute of Standards and Technology]] Information Technology Laboratory | accessdate = May 31, 2016 |date = April 2012 }}&lt;/ref&gt;

:&lt;math&gt;\operatorname{var}(X) = \operatorname{E}[(X - \mu)^2] = \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}&lt;/math&gt;

Letting α = β in the above expression one obtains

:&lt;math&gt;\operatorname{var}(X) = \frac{1}{4(2\beta + 1)},&lt;/math&gt;

showing that for ''α'' = ''β'' the variance decreases monotonically as {{nowrap|1=''α'' = ''β''}} increases. Setting {{nowrap|1=''α'' = ''β'' = 0}} in this expression, one finds the maximum variance var(''X'') = 1/4&lt;ref name=JKB /&gt; which only occurs approaching the limit, at {{nowrap|1=''α'' = ''β'' = 0}}.

The beta distribution may also be [[Parametrization|parametrized]] in terms of its mean ''μ'' {{nowrap|1=(0 &lt; ''μ'' &lt; 1)}} and sample size {{nowrap|1=''ν'' = ''α'' + ''β''}} ({{nowrap|''ν'' &gt; 0}}) (see section below titled "Mean and sample size"):

:&lt;math&gt; \begin{align}
  \alpha &amp;= \mu \nu, \text{ where }\nu =(\alpha + \beta) &gt;0\\
  \beta &amp;= (1 - \mu) \nu, \text{ where }\nu =(\alpha + \beta)  &gt;0.
\end{align}&lt;/math&gt;

Using this [[parametrization]], one can express the variance in terms of the mean ''μ'' and the sample size ''ν'' as follows:

:&lt;math&gt;\operatorname{var}(X) = \frac{\mu (1-\mu)}{1 + \nu}&lt;/math&gt;

Since {{nowrap|1=''ν'' = (''α'' + ''β'') &gt; 0}}, it must follow that {{nowrap|var(''X'') &lt; ''μ''(1 − ''μ'')}}.

For a symmetric distribution, the mean is at the middle of the distribution, {{nowrap|1=''μ'' = 1/2 }}, and therefore:

:&lt;math&gt;\operatorname{var}(X) = \frac{1}{4 (1 + \nu)} \text{ if } \mu = \tfrac{1}{2}&lt;/math&gt;

Also, the following limits (with only the noted variable approaching the limit) can be obtained from the above expressions:

:&lt;math&gt; \begin{align}
&amp;\lim_{\beta\to 0} \operatorname{var}(X) =\lim_{\alpha \to 0} \operatorname{var}(X) =\lim_{\beta\to \infty} \operatorname{var}(X) =\lim_{\alpha \to  \infty} \operatorname{var}(X) = \lim_{\nu \to  \infty} \operatorname{var}(X) =\lim_{\mu \to  0} \operatorname{var}(X) =\lim_{\mu \to  1} \operatorname{var}(X) = 0\\
&amp;\lim_{\nu \to  0} \operatorname{var}(X) = \mu (1-\mu)
\end{align}&lt;/math&gt;

[[File:Variance for Beta Distribution for alpha and beta ranging from 0 to 5 - J. Rodal.jpg|325px]]

====Geometric variance and covariance====
[[File:Beta distribution log geometric variances front view - J. Rodal.png|thumb|log geometric variances vs. α and β]]
[[File:Beta distribution log geometric variances back view - J. Rodal.png|thumb|log geometric variances vs. α and β]]

The logarithm of the geometric variance, ln(var&lt;sub&gt;''GX''&lt;/sub&gt;), of a distribution with [[random variable]] ''X'' is the second moment of the logarithm of ''X'' centered on the geometric mean of ''X'', ln(''G&lt;sub&gt;X&lt;/sub&gt;''):

:&lt;math&gt;\begin{align}
\ln \operatorname{var}_{GX} &amp;= \operatorname{E} \left [(\ln X - \ln G_X)^2 \right ] \\
&amp;= \operatorname{E}[(\ln X - \operatorname{E}\left [\ln X])^2 \right] \\
&amp;= \operatorname{E}\left[(\ln X)^2 \right] - (\operatorname{E}[\ln X])^2\\
&amp;= \operatorname{var}[\ln X]
\end{align}&lt;/math&gt;

and therefore, the geometric variance is:

:&lt;math&gt;\operatorname{var}_{GX} = e^{\operatorname{var}[\ln X]}&lt;/math&gt;

In the [[Fisher information]] matrix, and the curvature of the log [[likelihood function]], the logarithm of the geometric variance of the [[reflection formula|reflected]] variable 1&amp;nbsp;−&amp;nbsp;''X'' and the logarithm of the geometric covariance between ''X'' and 1&amp;nbsp;−&amp;nbsp;''X'' appear:

:&lt;math&gt;\begin{align}
\ln \operatorname{var_{G(1-X)}} &amp;= \operatorname{E}[(\ln (1-X) - \ln G_{1-X})^2] \\
&amp;= \operatorname{E}[(\ln (1-X) - \operatorname{E}[\ln (1-X)])^2] \\
&amp;= \operatorname{E}[(\ln (1-X))^2] - (\operatorname{E}[\ln (1-X)])^2\\
&amp;= \operatorname{var}[\ln (1-X)] \\
&amp; \\
\operatorname{var_{G(1-X)}} &amp;= e^{\operatorname{var}[\ln (1-X)]} \\
&amp; \\
\ln \operatorname{cov_{G{X,1-X}}} &amp;= \operatorname{E}[(\ln X - \ln G_X)(\ln (1-X) - \ln G_{1-X})] \\
&amp;= \operatorname{E}[(\ln X - \operatorname{E}[\ln X])(\ln (1-X) - \operatorname{E}[\ln (1-X)])] \\
&amp;= \operatorname{E}\left[\ln X \ln(1-X)\right] - \operatorname{E}[\ln X]\operatorname{E}[\ln(1-X)]\\
&amp;= \operatorname{cov}[\ln X, \ln(1-X)] \\
&amp; \\
\operatorname{cov}_{G{X,(1-X)}} &amp;= e^{\operatorname{cov}[\ln X, \ln(1-X)]}
\end{align}&lt;/math&gt;

For a beta distribution, higher order logarithmic moments can be derived by using the representation of a beta distribution as a proportion of two Gamma distributions and differentiating through the integral. They can be expressed in terms of higher order poly-gamma functions. See the section titled "Other moments, Moments of transformed random variables, Moments of logarithmically transformed random variables".  The [[variance]] of the logarithmic variables and [[covariance]] of ln&amp;nbsp;''X'' and ln(1−''X'') are:

: &lt;math&gt;\operatorname{var}[\ln X]= \psi_1(\alpha) - \psi_1(\alpha + \beta)&lt;/math&gt;
: &lt;math&gt;\operatorname{var}[\ln (1-X)] = \psi_1(\beta) - \psi_1(\alpha + \beta)&lt;/math&gt;
: &lt;math&gt;\operatorname{cov}[\ln X, \ln(1-X)] = -\psi_1(\alpha+\beta)&lt;/math&gt;

where the '''[[trigamma function]]''', denoted ψ&lt;sub&gt;1&lt;/sub&gt;(α), is the second of the [[polygamma function]]s, and is defined as the derivative of the [[digamma function]]:

:&lt;math&gt;\psi_1(\alpha) = \frac{d^2\ln\Gamma(\alpha)}{d\alpha^2}= \frac{d \, \psi(\alpha)}{d\alpha}.&lt;/math&gt;

Therefore,

:&lt;math&gt; \ln \operatorname{var}_{GX}=\operatorname{var}[\ln X]= \psi_1(\alpha) - \psi_1(\alpha + \beta) &lt;/math&gt;
:&lt;math&gt; \ln \operatorname{var}_{G(1-X)} =\operatorname{var}[\ln (1-X)] = \psi_1(\beta) - \psi_1(\alpha + \beta)&lt;/math&gt;
:&lt;math&gt; \ln \operatorname{cov}_{GX,1-X} =\operatorname{cov}[\ln X, \ln(1-X)] = -\psi_1(\alpha+\beta)&lt;/math&gt;

The accompanying plots show the log geometric variances and log geometric covariance versus the shape parameters ''α'' and ''β''.  The plots show that the log geometric variances and log geometric covariance are close to zero for shape parameters α and β greater than 2, and that the log geometric variances rapidly rise in value for shape parameter values ''α'' and ''β'' less than unity. The log geometric variances are positive for all values of the shape parameters. The log geometric covariance is negative for all values of the shape parameters, and it reaches large negative values for ''α'' and ''β'' less than unity.

Following are the limits with one parameter finite (non-zero) and the other approaching these limits:

:&lt;math&gt; \begin{align}
&amp;\lim_{\alpha\to  0} \ln \operatorname{var}_{GX} =  \lim_{\beta\to  0} \ln \operatorname{var}_{G(1-X)}  =\infty \\
&amp;\lim_{\beta \to  0} \ln \operatorname{var}_{GX} = \lim_{\alpha \to  \infty} \ln \operatorname{var}_{GX} = \lim_{\alpha \to  0} \ln \operatorname{var}_{G(1-X)} = \lim_{\beta\to  \infty} \ln \operatorname{var}_{G(1-X)} = \lim_{\alpha\to  \infty} \ln \operatorname{cov}_{GX,(1-X)} =  \lim_{\beta\to  \infty} \ln \operatorname{cov}_{GX,(1-X)} = 0\\
&amp;\lim_{\beta \to  \infty} \ln \operatorname{var}_{GX} =  \psi_1(\alpha)\\
&amp;\lim_{\alpha\to  \infty}  \ln \operatorname{var}_{G(1-X)} =  \psi_1(\beta)\\
&amp;\lim_{\alpha\to  0} \ln \operatorname{cov}_{GX,(1-X)} = - \psi_1(\beta)\\
&amp;\lim_{\beta\to  0}  \ln \operatorname{cov}_{GX,(1-X)} = - \psi_1(\alpha)
\end{align}&lt;/math&gt;

Limits with two parameters varying:

:&lt;math&gt; \begin{align}
&amp;\lim_{\alpha\to  \infty}( \lim_{\beta \to \infty} \ln \operatorname{var}_{GX}) = \lim_{\beta \to  \infty}( \lim_{\alpha\to  \infty} \ln \operatorname{var}_{G(1-X)}) = \lim_{\alpha\to  \infty} (\lim_{\beta \to  0} \ln \operatorname{cov}_{GX,(1-X)}) = \lim_{\beta\to  \infty}( \lim_{\alpha\to  0} \ln \operatorname{cov}_{GX,(1-X)}) =0\\
&amp;\lim_{\alpha\to  \infty} (\lim_{\beta \to  0} \ln \operatorname{var}_{GX}) = \lim_{\beta\to \infty} (\lim_{\alpha\to  0} \ln \operatorname{var}_{G(1-X)}) = \infty\\
&amp;\lim_{\alpha\to  0} (\lim_{\beta \to  0} \ln \operatorname{cov}_{GX,(1-X)}) = \lim_{\beta\to 0} (\lim_{\alpha\to  0} \ln \operatorname{cov}_{GX,(1-X)}) = - \infty
\end{align}&lt;/math&gt;

Although both ln(var&lt;sub&gt;''GX''&lt;/sub&gt;) and ln(var&lt;sub&gt;''G''(1&amp;nbsp;−&amp;nbsp;''X'')&lt;/sub&gt;) are asymmetric, when the shape parameters are equal, α = β, one has: ln(var&lt;sub&gt;''GX''&lt;/sub&gt;) = ln(var&lt;sub&gt;''G(1−X)''&lt;/sub&gt;). This equality follows from the following symmetry displayed between both log geometric variances:

:&lt;math&gt;\ln \operatorname{var}_{GX}(\Beta(\alpha, \beta))=\ln \operatorname{var}_{G(1-X)}(\Beta(\beta, \alpha)).&lt;/math&gt;

The log geometric covariance is symmetric:

:&lt;math&gt;\ln \operatorname{cov}_{GX,(1-X)}(\Beta(\alpha, \beta) )=\ln \operatorname{cov}_{GX,(1-X)}(\Beta(\beta, \alpha))&lt;/math&gt;

====Mean absolute deviation around the mean====
[[File:Ratio of Mean Abs. Dev. to Std.Dev. Beta distribution with alpha and beta from 0 to 5 - J. Rodal.jpg|thumb|Ratio of Mean Abs.Dev. to Std.Dev. for Beta distribution with α and β ranging from 0 to 5]]
[[File:Ratio of Mean Abs. Dev. to Std.Dev. Beta distribution vs. nu from 0 to 10 and vs. mean - J. Rodal.jpg|thumb|Ratio of Mean Abs.Dev. to Std.Dev. for Beta distribution with mean 0 ≤ μ ≤ 1 and sample size 0 &lt; ν ≤ 10]]
The [[mean absolute deviation]] around the mean for the beta distribution with shape parameters α and β is:&lt;ref name="Handbook of Beta Distribution" /&gt;

:&lt;math&gt;\operatorname{E}[|X - E[X]|] = \frac{2 \alpha^{\alpha} \beta^{\beta}}{\Beta(\alpha,\beta)(\alpha + \beta)^{\alpha + \beta + 1}} &lt;/math&gt;

The mean absolute deviation around the mean is a more [[Robust statistics|robust]] [[estimator]] of [[statistical dispersion]] than the standard deviation for beta distributions with tails and inflection points at each side of the mode, Beta(''α'',&amp;nbsp;''β'') distributions with ''α'',''β'' &gt; 2, as it depends on the linear (absolute) deviations rather than the square deviations from the mean.  Therefore, the effect of very large deviations from the mean are not as overly weighted.

Using [[Stirling's approximation]] to the [[Gamma function]], [[Norman Lloyd Johnson|N.L.Johnson]] and [[Samuel Kotz|S.Kotz]]&lt;ref name=JKB /&gt; derived the following approximation for values of the shape parameters greater than unity (the relative error for this approximation is only −3.5% for ''α'' = ''β'' = 1, and it decreases to zero as ''α'' → ∞, ''β'' → ∞):

:&lt;math&gt; \begin{align}
\frac{\text{mean abs. dev. from mean}}{\text{standard deviation}} &amp;=\frac{\operatorname{E}[|X - E[X]|]}{\sqrt{\operatorname{var}(X)}}\\
&amp;\approx \sqrt{\frac{2}{\pi}} \left(1+\frac{7}{12 (\alpha+\beta)}{}-\frac{1}{12 \alpha}-\frac{1}{12 \beta} \right), \text{ if } \alpha, \beta &gt; 1.
\end{align}&lt;/math&gt;

At the limit α → ∞, β → ∞, the ratio of the mean absolute deviation to the standard deviation (for the beta distribution) becomes equal to the ratio of the same measures for the normal distribution: &lt;math&gt;\sqrt{\frac{2}{\pi}}&lt;/math&gt;.  For α = β = 1 this ratio equals &lt;math&gt;\frac{\sqrt{3}}{2}&lt;/math&gt;, so that from α = β = 1 to α, β → ∞ the ratio decreases by 8.5%.  For α = β = 0 the standard deviation is exactly equal to the mean absolute deviation around the mean. Therefore, this ratio decreases by 15% from α = β = 0 to α = β = 1, and by 25% from α = β = 0 to α, β → ∞ . However, for skewed beta distributions such that α → 0 or β → 0, the ratio of the standard deviation to the mean absolute deviation approaches infinity (although each of them, individually, approaches zero) because the mean absolute deviation approaches zero faster than the standard deviation.

Using the [[parametrization]] in terms of mean μ and sample size ν = α + β &gt; 0:

:α = μν, β = (1−μ)ν

one can express the mean [[absolute deviation]] around the mean in terms of the mean μ and the sample size ν as follows:

:&lt;math&gt;\operatorname{E}[| X - E[X]|] = \frac{2 \mu^{\mu\nu} (1-\mu)^{(1-\mu)\nu}}{\nu \Beta(\mu \nu,(1-\mu)\nu)}&lt;/math&gt;

For a symmetric distribution, the mean is at the middle of the distribution, μ = 1/2, and therefore:

:&lt;math&gt; \begin{align}
\operatorname{E}[|X - E[X]|]  = \frac{2^{1-\nu}}{\nu \Beta(\tfrac{\nu}{2} ,\tfrac{\nu}{2})} &amp;= \frac{2^{1-\nu}\Gamma(\nu)}{\nu (\Gamma(\tfrac{\nu}{2}))^2 } \\
\lim_{\nu \to 0} \left (\lim_{\mu \to \frac{1}{2}} \operatorname{E}[|X - E[X]|] \right ) &amp;= \tfrac{1}{2}\\
\lim_{\nu \to \infty} \left (\lim_{\mu \to \frac{1}{2}} \operatorname{E}[| X - E[X]|] \right ) &amp;= 0
\end{align}&lt;/math&gt;

Also, the following limits (with only the noted variable approaching the limit) can be obtained from the above expressions:

:&lt;math&gt; \begin{align}
\lim_{\beta\to  0} \operatorname{E}[|X - E[X]|] &amp;=\lim_{\alpha \to  0} \operatorname{E}[|X - E[X]|]= 0 \\
\lim_{\beta\to  \infty} \operatorname{E}[|X - E[X]|] &amp;=\lim_{\alpha \to  \infty} \operatorname{E}[|X - E[X]|] = 0\\
\lim_{\mu \to  0} \operatorname{E}[|X - E[X]|]&amp;=\lim_{\mu \to  1} \operatorname{E}[|X - E[X]|] = 0\\
\lim_{\nu \to  0} \operatorname{E}[|X - E[X]|] &amp;= \sqrt{\mu (1-\mu)} \\
\lim_{\nu \to  \infty} \operatorname{E}[|X - E[X]|] &amp;= 0
\end{align}&lt;/math&gt;

====Mean absolute difference====

The [[mean absolute difference]] for the Beta distribution is:

:&lt;math&gt;\mathrm{MD} = \int_0^1 \int_0^1 f(x;\alpha,\beta)\,f(y;\alpha,\beta)\,|x-y|\,dx\,dy = \left(\frac{4}{\alpha+\beta}\right)\frac{B(\alpha+\beta,\alpha+\beta)}{B(\alpha,\alpha)B(\beta,\beta)}&lt;/math&gt;

The [[Gini coefficient]] for the Beta distribution is half of the relative mean absolute difference:

:&lt;math&gt;\mathrm{G} = \left(\frac{2}{\alpha}\right)\frac{B(\alpha+\beta,\alpha+\beta)}{B(\alpha,\alpha)B(\beta,\beta)}&lt;/math&gt;

===Skewness===
[[File:Skewness for Beta Distribution as a function of the variance and the mean - J. Rodal.jpg|325px|thumb|Skewness for Beta Distribution as a function of variance and mean]]

The [[skewness]] (the third moment centered on the mean, normalized by the 3/2 power of the variance) of the beta distribution is&lt;ref name=JKB /&gt;

:&lt;math&gt;\gamma_1 =\frac{\operatorname{E}[(X - \mu)^3]}{(\operatorname{var}(X))^{3/2}} = \frac{2(\beta - \alpha)\sqrt{\alpha + \beta + 1}}{(\alpha + \beta + 2) \sqrt{\alpha \beta}} .&lt;/math&gt;

Letting α = β in the above expression one obtains γ&lt;sub&gt;1&lt;/sub&gt; = 0, showing once again that for α = β the distribution is symmetric and hence the skewness is zero. Positive skew (right-tailed) for α &lt; β, negative skew (left-tailed) for α &gt; β.

Using the [[parametrization]] in terms of mean μ and sample size ν = α + β:

:&lt;math&gt; \begin{align}
  \alpha &amp; {} = \mu \nu ,\text{ where }\nu =(\alpha + \beta)  &gt;0\\
  \beta &amp; {} = (1 - \mu) \nu , \text{ where }\nu =(\alpha + \beta)  &gt;0.
\end{align}&lt;/math&gt;

one can express the skewness in terms of the mean μ and the sample size ν as follows:

:&lt;math&gt;\gamma_1 =\frac{\operatorname{E}[(X - \mu)^3]}{(\operatorname{var}(X))^{3/2}} = \frac{2(1-2\mu)\sqrt{1+\nu}}{(2+\nu)\sqrt{\mu (1 - \mu)}}.&lt;/math&gt;

The skewness can also be expressed just in terms of the variance ''var'' and the mean μ as follows:

:&lt;math&gt;\gamma_1 =\frac{\operatorname{E}[(X - \mu)^3]}{(\operatorname{var}(X))^{3/2}} = \frac{2(1-2\mu)\sqrt{\text{ var }}}{ \mu(1-\mu) + \operatorname{var}}\text{ if } \operatorname{var} &lt; \mu(1-\mu)&lt;/math&gt;

The accompanying plot of skewness as a function of variance and mean shows that maximum variance (1/4) is coupled with zero skewness and the symmetry condition (μ = 1/2), and that maximum skewness (positive or negative infinity) occurs when the mean is located at one end or the other, so that the "mass" of the probability distribution is concentrated at the ends (minimum variance).

The following expression for the square of the skewness, in terms of the sample size ν = α + β and the variance ''var'', is useful for the method of moments estimation of four parameters:

:&lt;math&gt;(\gamma_1)^2 =\frac{(\operatorname{E}[(X - \mu)^3])^2}{(\operatorname{var}(X))^3} = \frac{4}{(2+\nu)^2}\bigg(\frac{1}{\text{var}}-4(1+\nu)\bigg)&lt;/math&gt;

This expression correctly gives a skewness of zero for α = β, since in that case (see section titled "Variance"): &lt;math&gt;\operatorname{var} = \frac{1}{4 (1 + \nu)}&lt;/math&gt;.

For the symmetric case (α = β), skewness = 0 over the whole range, and the following limits apply:

:&lt;math&gt;\lim_{\alpha = \beta \to 0} \gamma_1 = \lim_{\alpha = \beta \to \infty} \gamma_1 =\lim_{\nu \to 0} \gamma_1=\lim_{\nu \to \infty} \gamma_1=\lim_{\mu \to \frac{1}{2}} \gamma_1 = 0&lt;/math&gt;

For the asymmetric cases (α ≠ β) the following limits (with only the noted variable approaching the limit) can be obtained from the above expressions:

:&lt;math&gt; \begin{align}
&amp;\lim_{\alpha\to  0} \gamma_1 =\lim_{\mu\to  0} \gamma_1 = \infty\\
&amp;\lim_{\beta \to  0} \gamma_1  = \lim_{\mu\to  1} \gamma_1= - \infty\\
&amp;\lim_{\alpha\to \infty} \gamma_1 = -\frac{2}{\beta},\quad \lim_{\beta \to  0}(\lim_{\alpha\to  \infty} \gamma_1) = -\infty,\quad \lim_{\beta \to \infty}(\lim_{\alpha\to \infty} \gamma_1) = 0\\
&amp;\lim_{\beta\to  \infty} \gamma_1 = \frac{2}{\alpha},\quad \lim_{\alpha \to  0}(\lim_{\beta \to  \infty} \gamma_1) = \infty,\quad \lim_{\alpha \to  \infty}(\lim_{\beta \to  \infty} \gamma_1) = 0\\
&amp;\lim_{\nu \to  0} \gamma_1 = \frac{1 - 2 \mu}{\sqrt{\mu (1-\mu)}},\quad \lim_{\mu \to  0}(\lim_{\nu \to  0} \gamma_1)  = \infty,\quad \lim_{\mu \to  1}(\lim_{\nu \to  0} \gamma_1) = - \infty
\end{align}&lt;/math&gt;

[[File:Skewness Beta Distribution for alpha and beta from 1 to 5 - J. Rodal.jpg|325px]][[File:Skewness Beta Distribution for alpha and beta from .1 to 5 - J. Rodal.jpg|325px]]

===Kurtosis===
[[File:Excess Kurtosis for Beta Distribution as a function of variance and mean - J. Rodal.jpg|325px|thumb|Excess Kurtosis for Beta Distribution as a function of variance and mean]]

The beta distribution has been applied in acoustic analysis to assess damage to gears, as the kurtosis of the beta distribution has been reported to be a good indicator of the condition of a gear.&lt;ref name=Oguamanam&gt;{{cite journal |last1=Oguamanam |first1=D.C.D. |last2=Martin |first2=H. R. |last3=Huissoon |first3=J. P. |title=On the application of the beta distribution to gear damage analysis |journal=Applied Acoustics |year=1995 |volume=45 |issue=3 |pages=247–261 |doi=10.1016/0003-682X(95)00001-P}}&lt;/ref&gt; Kurtosis has also been used to distinguish the seismic signal generated by a person's footsteps from other signals. As persons or other targets moving on the ground generate continuous signals in the form of seismic waves, one can separate different targets based on the seismic waves they generate. Kurtosis is sensitive to impulsive signals, so it's much more sensitive to the signal generated by human footsteps than other signals generated by vehicles, winds, noise, etc.&lt;ref name=Liang&gt;{{cite journal|author1=Zhiqiang Liang |author2=Jianming Wei |author3=Junyu Zhao |author4=Haitao Liu |author5=Baoqing Li |author6=Jie Shen |author7=Chunlei Zheng |title=The Statistical Meaning of Kurtosis and Its New Application to Identification of Persons Based on Seismic Signals |journal=Sensors |date=27 August 2008 |volume=8 |pages=5106–5119 |doi=10.3390/s8085106}}&lt;/ref&gt;  Unfortunately, the notation for kurtosis has not been standardized. Kenney and Keeping&lt;ref name="Kenney and Keeping"&gt;{{cite book|last=Kenney|first=J. F., and E. S. Keeping|title=Mathematics of Statistics Part Two,  2nd edition|year=1951|publisher=D. Van Nostrand Company Inc.}}&lt;/ref&gt;  use the symbol γ&lt;sub&gt;2&lt;/sub&gt; for the [[excess kurtosis]], but [[Abramowitz and Stegun]]&lt;ref name=Abramowitz&gt;{{cite book|last=Abramowitz|first=Milton and Irene A. Stegun|title=Handbook Of Mathematical Functions With Formulas, Graphs, And Mathematical Tables|year=1965|publisher=Dover|isbn=978-0-486-61272-0}}&lt;/ref&gt;  use different terminology.  To prevent confusion&lt;ref name=Weisstein.Kurtosi&gt;{{cite web|last=Weisstein.|first=Eric W.|title=Kurtosis|url=http://mathworld.wolfram.com/Kurtosis.html|publisher=MathWorld--A Wolfram Web Resource|accessdate=13 August 2012}}&lt;/ref&gt;  between kurtosis (the fourth moment centered on the mean, normalized by the square of the variance) and excess kurtosis, when using symbols, they will be spelled out as follows:&lt;ref name="Handbook of Beta Distribution"&gt;{{cite book|last=Gupta (Editor)|first=Arjun K.|title=Handbook of Beta Distribution and Its Applications|year=2004|publisher=CRC Press|isbn=978-0824753962}}&lt;/ref&gt;&lt;ref name=Panik&gt;{{cite book|last=Panik|first=Michael J|title=Advanced Statistics from an Elementary Point of View|year=2005|publisher=Academic Press|isbn=978-0120884940}}&lt;/ref&gt;

:&lt;math&gt;\begin{align}
\text{excess kurtosis}
     &amp;=\text{kurtosis} - 3\\
     &amp;=\frac{\operatorname{E}[(X - \mu)^4]}{{(\operatorname{var}(X))^{2}}}-3\\
     &amp;=\frac{6[\alpha^3-\alpha^2(2\beta - 1) + \beta^2(\beta + 1) - 2\alpha\beta(\beta + 2)]}{\alpha \beta (\alpha + \beta + 2)(\alpha + \beta + 3)}\\
     &amp;=\frac{6[(\alpha - \beta)^2 (\alpha +\beta + 1) - \alpha \beta (\alpha + \beta + 2)]}
{\alpha \beta (\alpha + \beta + 2) (\alpha + \beta + 3)} .
\end{align}&lt;/math&gt;

Letting α = β in the above expression one obtains

:&lt;math&gt;\text{excess kurtosis} =- \frac{6}{3+2\alpha} \text{ if }\alpha=\beta &lt;/math&gt;.

Therefore, for symmetric beta distributions, the excess kurtosis is negative, increasing from a minimum value of −2 at the limit as {α = β} → 0, and approaching a maximum value of zero as {α = β} → ∞.  The value of −2 is the minimum value of excess kurtosis that any distribution (not just beta distributions, but any distribution of any possible kind) can ever achieve.  This minimum value is reached when all the probability density is entirely concentrated at each end ''x'' = 0 and ''x'' = 1, with nothing in between: a 2-point [[Bernoulli distribution]] with equal probability 1/2 at each end (a coin toss: see section below "Kurtosis bounded by the square of the skewness" for further discussion).  The description of [[kurtosis]] as a measure of the "potential outliers" (or "potential rare, extreme values") of the probability distribution, is correct for all distributions including the beta distribution. When rare, extreme values can occur in the beta distribution, the higher its kurtosis; otherwise, the kurtosis is lower. For α ≠ β, skewed beta distributions, the excess kurtosis can reach unlimited positive values (particularly for α → 0 for finite β, or for β → 0 for finite α) because the side away from the mode will produce occasional extreme values.  Minimum kurtosis takes place when the mass density is concentrated equally at each end (and therefore the mean is at the center), and there is no probability mass density in between the ends.

Using the [[parametrization]] in terms of mean μ and sample size ν = α + β:

:&lt;math&gt; \begin{align}
  \alpha &amp; {} = \mu \nu ,\text{ where }\nu =(\alpha + \beta)  &gt;0\\
  \beta &amp; {} = (1 - \mu) \nu , \text{ where }\nu =(\alpha + \beta)  &gt;0.
\end{align}&lt;/math&gt;

one can express the excess kurtosis in terms of the mean μ and the sample size ν as follows:

:&lt;math&gt;\text{excess kurtosis} =\frac{6}{3 + \nu}\bigg (\frac{(1 - 2 \mu)^2 (1 + \nu)}{\mu (1 - \mu) (2 + \nu)} - 1 \bigg )&lt;/math&gt;

The excess kurtosis can also be expressed in terms of just the following two parameters: the variance ''var'', and the sample size ν as follows:

:&lt;math&gt;\text{excess kurtosis} =\frac{6}{(3 + \nu)(2 + \nu)}\left(\frac{1}{\text{ var }} - 6 - 5 \nu \right)\text{ if }\text{ var }&lt; \mu(1-\mu)&lt;/math&gt;

and, in terms of the variance ''var'' and the mean μ as follows:

:&lt;math&gt;\text{excess kurtosis} =\frac{6 \text{ var } (1 - \text{ var } - 5 \mu (1 - \mu) )}{(\text{var } + \mu (1 - \mu))(2\text{ var } + \mu (1 - \mu) )}\text{ if }\text{ var }&lt; \mu(1-\mu)&lt;/math&gt;

The plot of excess kurtosis as a function of the variance and the mean shows that the minimum value of the excess kurtosis (−2, which is the minimum possible value for excess kurtosis for any distribution) is intimately coupled with the maximum value of variance (1/4) and the symmetry condition: the mean occurring at the midpoint (μ = 1/2). This occurs for the symmetric case of α = β = 0, with zero skewness.  At the limit, this is the 2 point [[Bernoulli distribution]] with equal probability 1/2 at each [[Dirac delta function]] end ''x'' = 0 and ''x'' = 1 and zero probability everywhere else. (A coin toss: one face of the coin being ''x'' = 0 and the other face being ''x'' = 1.)  Variance is maximum because the distribution is bimodal with nothing in between the two modes (spikes) at each end.  Excess kurtosis is minimum: the probability density "mass" is zero at the mean and it is concentrated at the two peaks at each end.  Excess kurtosis reaches the minimum possible value (for any distribution) when the probability density function has two spikes at each end: it is bi-"peaky" with nothing in between them.

On the other hand, the plot shows that for extreme skewed cases, where the mean is located near one or the other end (μ = 0 or μ = 1), the variance is close to zero, and the excess kurtosis rapidly approaches infinity when the mean of the distribution approaches either end.

Alternatively, the excess kurtosis can also be expressed in terms of just the following two parameters: the square of the skewness, and the sample size ν as follows:

:&lt;math&gt;\text{excess kurtosis} =\frac{6}{3 + \nu}\bigg(\frac{(2 + \nu)}{4} (\text{skewness})^2 - 1\bigg)\text{ if (skewness)}^2-2&lt; \text{excess kurtosis}&lt; \frac{3}{2} (\text{skewness})^2&lt;/math&gt;

From this last expression, one can obtain the same limits published practically a century ago by [[Karl Pearson]] in his paper,&lt;ref name=Pearson /&gt; for the beta distribution (see section below titled "Kurtosis bounded by the square of the skewness"). Setting α + β= ν =  0 in the above expression, one obtains Pearson's lower boundary (values for the skewness and excess kurtosis below the boundary (excess kurtosis + 2 − skewness&lt;sup&gt;2&lt;/sup&gt; = 0) cannot occur for any distribution, and hence [[Karl Pearson]] appropriately called the region below this boundary the "impossible region"). The limit of α + β = ν → ∞ determines Pearson's upper boundary.

:&lt;math&gt; \begin{align}
&amp;\lim_{\nu \to  0}\text{excess kurtosis}  = (\text{skewness})^2 - 2\\
&amp;\lim_{\nu \to \infty}\text{excess kurtosis}  = \tfrac{3}{2} (\text{skewness})^2
\end{align}&lt;/math&gt;

therefore:

:&lt;math&gt;(\text{skewness})^2-2&lt; \text{excess kurtosis}&lt; \tfrac{3}{2} (\text{skewness})^2&lt;/math&gt;

Values of ν = α + β such that ν ranges from zero to infinity, 0 &lt; ν &lt; ∞, span the whole region of the beta distribution in the plane of excess kurtosis versus squared skewness.

For the symmetric case (α = β), the following limits apply:

:&lt;math&gt; \begin{align}
&amp;\lim_{\alpha = \beta \to 0} \text{excess kurtosis} =  - 2 \\
&amp;\lim_{\alpha = \beta \to \infty} \text{excess kurtosis} = 0 \\
&amp;\lim_{\mu \to \frac{1}{2}} \text{excess kurtosis} = - \frac{6}{3 + \nu}
\end{align}&lt;/math&gt;

For the unsymmetric cases (α ≠ β) the following limits (with only the noted variable approaching the limit) can be obtained from the above expressions:

:&lt;math&gt; \begin{align}
&amp;\lim_{\alpha\to  0}\text{excess kurtosis}  =\lim_{\beta \to  0} \text{excess kurtosis}  = \lim_{\mu \to  0}\text{excess kurtosis}  = \lim_{\mu \to  1}\text{excess kurtosis}  =\infty\\
&amp;\lim_{\alpha \to  \infty}\text{excess kurtosis}  = \frac{6}{\beta},\text{    }  \lim_{\beta \to  0}(\lim_{\alpha\to  \infty} \text{excess kurtosis})  = \infty,\text{    }  \lim_{\beta \to  \infty}(\lim_{\alpha\to  \infty} \text{excess kurtosis})  = 0\\
&amp;\lim_{\beta \to  \infty}\text{excess kurtosis}  = \frac{6}{\alpha},\text{    }  \lim_{\alpha \to  0}(\lim_{\beta \to  \infty} \text{excess kurtosis})  = \infty,\text{    }  \lim_{\alpha \to  \infty}(\lim_{\beta \to  \infty} \text{excess kurtosis})  = 0\\
&amp;\lim_{\nu \to  0} \text{excess kurtosis}  = - 6 + \frac{1}{\mu (1 - \mu)},\text{    }  \lim_{\mu \to  0}(\lim_{\nu \to  0} \text{excess kurtosis})  = \infty,\text{    }  \lim_{\mu \to  1}(\lim_{\nu \to  0} \text{excess kurtosis})  = \infty
\end{align}&lt;/math&gt;

[[File:Excess Kurtosis for Beta Distribution with alpha and beta ranging from 1 to 5 - J. Rodal.jpg|325px]][[File:Excess Kurtosis for Beta Distribution with alpha and beta ranging from 0.1 to 5 - J. Rodal.jpg|325px]]

===Characteristic function===
[[File:Re(CharacteristicFunction) Beta Distr alpha=beta from 0 to 25 Back - J. Rodal.jpg|325px|thumb|[[Characteristic function (probability theory)|Re(characteristic function)]] symmetric case α = β ranging from 25 to 0]][[File:Re(CharacteristicFunc) Beta Distr alpha=beta from 0 to 25 Front- J. Rodal.jpg|325px|thumb|[[Characteristic function (probability theory)|Re(characteristic function)]] symmetric case α = β ranging from 0 to 25]][[File:Re(CharacteristFunc) Beta Distr alpha from 0 to 25 and beta=alpha+0.5 Back - J. Rodal.jpg|325px|thumb|[[Characteristic function (probability theory)|Re(characteristic function)]] β = α + 1/2; α ranging from 25 to 0]][[File:Re(CharacterFunc) Beta Distrib. beta from 0 to 25, alpha=beta+0.5 Back - J. Rodal.jpg|325px|thumb|[[Characteristic function (probability theory)|Re(characteristic function)]] α = β + 1/2; β ranging from 25 to 0]][[File:Re(CharacterFunc) Beta Distr. beta from 0 to 25, alpha=beta+0.5 Front - J. Rodal.jpg|325px|thumb|[[Characteristic function (probability theory)|Re(characteristic function)]] α = β + 1/2; β ranging from 0 to 25]]

The [[Characteristic function (probability theory)|characteristic function]] is the [[Fourier transform]] of the probability density function.  The characteristic function of the beta distribution is [[confluent hypergeometric function|Kummer's confluent hypergeometric function]] (of the first kind):&lt;ref name=JKB /&gt;&lt;ref name=Abramowitz /&gt;&lt;ref name="Zwillinger_2014"&gt;{{cite book |author-first1=Izrail Solomonovich |author-last1=Gradshteyn |author-link1=Izrail Solomonovich Gradshteyn |author-first2=Iosif Moiseevich |author-last2=Ryzhik |author-link2=Iosif Moiseevich Ryzhik |author-first3=Yuri Veniaminovich |author-last3=Geronimus |author-link3=Yuri Veniaminovich Geronimus |author-first4=Michail Yulyevich |author-last4=Tseytlin |author-link4=Michail Yulyevich Tseytlin |author-first5=Alan |author-last5=Jeffrey |editor-first1=Daniel |editor-last1=Zwillinger |editor-first2=Victor Hugo |editor-last2=Moll |translator=Scripta Technica, Inc. |title=Table of Integrals, Series, and Products |publisher=[[Academic Press, Inc.]] |date=2015 |orig-year=October 2014 |edition=8 |language=English |isbn=0-12-384933-0 |id={{ISBN|978-0-12-384933-5}} |lccn=2014010276 &lt;!-- |url=https://books.google.com/books?id=NjnLAwAAQBAJ |access-date=2016-02-21--&gt;|title-link=Gradshteyn and Ryzhik}}&lt;/ref&gt;
:&lt;math&gt;\begin{align}
\varphi_X(\alpha;\beta;t)
&amp;= \operatorname{E}\left[e^{itX}\right]\\
&amp;= \int_0^1 e^{itx} f(x;\alpha,\beta) dx \\
&amp;={}_1F_1(\alpha; \alpha+\beta; it)\!\\
&amp;=\sum_{n=0}^\infty \frac {\alpha^{(n)} (it)^n} {(\alpha+\beta)^{(n)} n!}\\
&amp;= 1  +\sum_{k=1}^{\infty} \left( \prod_{r=0}^{k-1} \frac{\alpha+r}{\alpha+\beta+r} \right) \frac{(it)^k}{k!}
\end{align}&lt;/math&gt;

where

: &lt;math&gt;x^{(n)}=x(x+1)(x+2)\cdots(x+n-1)&lt;/math&gt;

is the [[rising factorial]], also called the "Pochhammer symbol".  The value of the characteristic function for ''t'' = 0, is one:

:&lt;math&gt; \varphi_X(\alpha;\beta;0)={}_1F_1(\alpha; \alpha+\beta; 0) = 1  &lt;/math&gt;.

Also, the real and imaginary parts of the characteristic function enjoy the following symmetries with respect to the origin of variable ''t'':

:&lt;math&gt; \textrm{Re} \left [ {}_1F_1(\alpha; \alpha+\beta; it) \right ] = \textrm{Re} \left [ {}_1F_1(\alpha; \alpha+\beta; - it) \right ]  &lt;/math&gt;
:&lt;math&gt; \textrm{Im} \left [ {}_1F_1(\alpha; \alpha+\beta; it) \right ] = - \textrm{Im} \left  [ {}_1F_1(\alpha; \alpha+\beta; - it) \right ]  &lt;/math&gt;

The symmetric case α = β simplifies the characteristic function of the beta distribution to a [[Bessel function]], since in the special case α + β = 2α the [[confluent hypergeometric function]] (of the first kind) reduces to a [[Bessel function]] (the modified Bessel function of the first kind &lt;math&gt;I_{\alpha-\frac 1 2}&lt;/math&gt; ) using [[Ernst Kummer|Kummer's]] second transformation as follows:

:&lt;math&gt;\begin{align} {}_1F_1(\alpha;2\alpha; it) &amp;= e^{\frac{it}{2}} {}_0F_1 \left(; \alpha+\tfrac{1}{2}; \frac{(it)^2}{16} \right) \\
&amp;= e^{\frac{it}{2}} \left(\frac{it}{4}\right)^{\frac{1}{2}-\alpha} \Gamma\left(\alpha+\tfrac{1}{2}\right) I_{\alpha-\frac 1 2}\left(\frac{it}{2}\right).\end{align}&lt;/math&gt;

In the accompanying plots, the [[Complex number|real part]] (Re) of the [[Characteristic function (probability theory)|characteristic function]] of the beta distribution is displayed for symmetric (α = β) and skewed (α ≠ β) cases.

===Other moments===

====Moment generating function====
It also follows&lt;ref name=JKB /&gt;&lt;ref name="Handbook of Beta Distribution" /&gt; that the [[moment generating function]] is

:&lt;math&gt;\begin{align}
M_X(\alpha; \beta; t)
&amp;= \operatorname{E}\left[e^{tX}\right] \\[4pt]
&amp;= \int_0^1 e^{tx} f(x;\alpha,\beta)\,dx \\[4pt]
&amp;= {}_1F_1(\alpha; \alpha+\beta; t) \\[4pt]
&amp;= \sum_{n=0}^\infty \frac {\alpha^{(n)}} {(\alpha+\beta)^{(n)}}\frac {t^n}{n!}\\[4pt]
&amp;= 1  +\sum_{k=1}^{\infty} \left( \prod_{r=0}^{k-1} \frac{\alpha+r}{\alpha+\beta+r} \right) \frac{t^k}{k!}
\end{align}&lt;/math&gt;

In particular ''M''&lt;sub&gt;''X''&lt;/sub&gt;(''α''; ''β''; 0) = 1.

====Higher moments====
Using the [[moment generating function]], the ''k''-th [[raw moment]] is given by&lt;ref name=JKB/&gt; the factor

:&lt;math&gt;\prod_{r=0}^{k-1} \frac{\alpha+r}{\alpha+\beta+r} &lt;/math&gt;

multiplying the (exponential series) term &lt;math&gt;\left(\frac{t^k}{k!}\right)&lt;/math&gt; in the series of the [[moment generating function]]

:&lt;math&gt;\operatorname{E}[X^k]= \frac{\alpha^{(k)}}{(\alpha + \beta)^{(k)}} = \prod_{r=0}^{k-1} \frac{\alpha+r}{\alpha+\beta+r}&lt;/math&gt;

where (''x'')&lt;sup&gt;(''k'')&lt;/sup&gt; is a [[Pochhammer symbol]] representing rising factorial. It can also be written in a recursive form as

:&lt;math&gt;\operatorname{E}[X^k] = \frac{\alpha + k - 1}{\alpha + \beta + k - 1}\operatorname{E}[X^{k - 1}].&lt;/math&gt;

Since the moment generating function &lt;math&gt;M_X(\alpha; \beta; \cdot)&lt;/math&gt; has a positive radius of convergence, the beta distribution is [[Moment problem|determined by its moments]].&lt;ref&gt;{{cite book|last1=Billingsley|first1=Patrick|title=Probability and measure|date=1995|publisher=Wiley-Interscience|isbn=0-471-00710-2|edition=3rd|chapter=30}}&lt;/ref&gt;

====Moments of transformed random variables====

=====Moments of linearly transformed, product and inverted random variables=====
One can also show the following expectations for a transformed random variable,&lt;ref name=JKB/&gt; where the random variable ''X'' is Beta-distributed with parameters α and β: ''X'' ~ Beta(α, β).  The expected value of the variable 1&amp;nbsp;−&amp;nbsp;''X'' is the mirror-symmetry of the expected value based on ''X'':

:&lt;math&gt;\begin{align}
&amp; \operatorname{E}[1-X] = \frac{\beta}{\alpha + \beta } \\
&amp; \operatorname{E}[X (1-X)] =\operatorname{E}[(1-X)X ] =\frac{\alpha \beta}{(\alpha + \beta)(\alpha +\beta + 1) }
\end{align}&lt;/math&gt;

Due to the mirror-symmetry of the probability density function of the beta distribution, the variances based on variables ''X'' and 1&amp;nbsp;−&amp;nbsp;''X'' are identical, and the covariance on ''X''(1&amp;nbsp;−&amp;nbsp;''X'' is the negative of the variance:

:&lt;math&gt;\operatorname{var}[(1-X)]=\operatorname{var}[X] = -\operatorname{cov}[X,(1-X)]= \frac{\alpha \beta}{(\alpha + \beta)^2(\alpha + \beta + 1)}&lt;/math&gt;

These are the expected values for inverted variables, (these are related to the harmonic means, see section titled "Harmonic mean"):

:&lt;math&gt;\begin{align}
&amp; \operatorname{E} \left [\frac{1}{X} \right ] = \frac{\alpha+\beta-1  }{\alpha -1 } \text{ if  } \alpha &gt; 1\\
&amp; \operatorname{E}\left [\frac{1}{1-X} \right ] =\frac{\alpha+\beta-1  }{\beta-1 } \text{ if } \beta &gt; 1
\end{align}&lt;/math&gt;

The following transformation by dividing the variable ''X'' by its mirror-image ''X''/(1&amp;nbsp;−&amp;nbsp;''X'') results in the expected value of the "inverted beta distribution" or [[beta prime distribution]] (also known as beta distribution of the second kind or [[Pearson distribution|Pearson's Type VI]]):&lt;ref name=JKB/&gt;

:&lt;math&gt; \begin{align}
&amp; \operatorname{E}\left[\frac{X}{1-X}\right] =\frac{\alpha}{\beta - 1 } \text{ if }\beta &gt; 1\\
&amp; \operatorname{E}\left[\frac{1-X}{X}\right] =\frac{\beta}{\alpha- 1 }\text{ if }\alpha &gt; 1
\end{align} &lt;/math&gt;

Variances of these transformed variables can be obtained by integration, as the expected values of the second moments centered on the corresponding variables:

:&lt;math&gt;\operatorname{var} \left[\frac{1}{X} \right] =\operatorname{E}\left[\left(\frac{1}{X} - \operatorname{E}\left[\frac{1}{X} \right ] \right )^2\right]=&lt;/math&gt;
:&lt;math&gt;\operatorname{var}\left [\frac{1-X}{X} \right ] =\operatorname{E} \left [\left (\frac{1-X}{X} - \operatorname{E}\left [\frac{1-X}{X} \right ] \right )^2 \right ]= \frac{\beta (\alpha+\beta-1)}{(\alpha -2)(\alpha-1)^2 } \text{ if }\alpha &gt; 2&lt;/math&gt;

The following variance of the variable ''X'' divided by its mirror-image (''X''/(1−''X'') results in the variance of the "inverted beta distribution" or [[beta prime distribution]] (also known as beta distribution of the second kind or [[Pearson distribution|Pearson's Type VI]]):&lt;ref name=JKB/&gt;

:&lt;math&gt;\operatorname{var} \left [\frac{1}{1-X} \right ] =\operatorname{E} \left [\left(\frac{1}{1-X} - \operatorname{E} \left [\frac{1}{1-X} \right ] \right)^2 \right ]=\operatorname{var} \left [\frac{X}{1-X} \right ] =&lt;/math&gt;
:&lt;math&gt;\operatorname{E} \left [\left (\frac{X}{1-X} - \operatorname{E} \left [\frac{X}{1-X} \right ] \right )^2 \right ]= \frac{\alpha(\alpha+\beta-1)}{(\beta-2)(\beta-1)^2 } \text{ if }\beta &gt; 2&lt;/math&gt;

The covariances are:

:&lt;math&gt;\operatorname{cov}\left [\frac{1}{X},\frac{1}{1-X} \right ] = \operatorname{cov}\left[\frac{1-X}{X},\frac{X}{1-X} \right] =\operatorname{cov}\left[\frac{1}{X},\frac{X}{1-X}\right ] = \operatorname{cov}\left[\frac{1-X}{X},\frac{1}{1-X} \right] =\frac{\alpha+\beta-1}{(\alpha-1)(\beta-1) } \text{ if } \alpha, \beta &gt; 1&lt;/math&gt;

These expectations and variances appear in the four-parameter Fisher information matrix (section titled "Fisher information," "four parameters")

=====Moments of logarithmically transformed random variables=====
[[File:Logit.svg|thumbnail|right|350px|Plot of logit(''X'') = ln(''X''/(1−''X'')) (vertical axis) vs. ''X'' in the domain of 0 to 1 (horizontal axis). Logit transformations are interesting, as they usually transform various shapes (including J-shapes) into (usually skewed) bell-shaped densities over the logit variable, and they may remove the end singularities over the original variable]]

Expected values for logarithmic transformations (useful for [[maximum likelihood]] estimates, see section titled "Parameter estimation, Maximum likelihood" below) are discussed in this section.  The following logarithmic linear transformations are related to the geometric means ''G&lt;sub&gt;X&lt;/sub&gt;'' and  ''G''&lt;sub&gt;(1−''X'')&lt;/sub&gt; (see section titled "Geometric mean"):

:&lt;math&gt;\begin{align}
\operatorname{E}[\ln(X)] &amp;= \psi(\alpha) - \psi(\alpha + \beta)= - \operatorname{E}\left[\ln \left (\frac{1}{X} \right )\right],\\
\operatorname{E}[\ln(1-X)] &amp;=\psi(\beta) - \psi(\alpha + \beta)= - \operatorname{E} \left[\ln \left (\frac{1}{1-X} \right )\right].
\end{align}&lt;/math&gt;

Where the '''[[digamma function]]''' ψ(α) is defined as the [[logarithmic derivative]] of the [[gamma function]]:&lt;ref name=Abramowitz/&gt;

:&lt;math&gt;\psi(\alpha) = \frac{d \ln\Gamma(\alpha)}{d\alpha}&lt;/math&gt;

[[Logit]] transformations are interesting,&lt;ref name=MacKay&gt;{{cite book|last=MacKay|first=David|title=Information Theory, Inference and Learning Algorithms|year=2003| publisher=Cambridge University Press; First Edition |isbn=978-0521642989}}&lt;/ref&gt; as they usually transform various shapes (including J-shapes) into (usually skewed) bell-shaped densities over the logit variable, and they may remove the end singularities over the original variable:

:&lt;math&gt;\begin{align}
\operatorname{E}\left[\ln \left (\frac{X}{1-X} \right ) \right] &amp;=\psi(\alpha) - \psi(\beta)= \operatorname{E}[\ln(X)] +\operatorname{E} \left[\ln \left (\frac{1}{1-X} \right) \right],\\
\operatorname{E}\left [\ln \left (\frac{1-X}{X} \right ) \right ] &amp;=\psi(\beta) - \psi(\alpha)= - \operatorname{E} \left[\ln \left (\frac{X}{1-X} \right) \right] .
\end{align}&lt;/math&gt;

Johnson&lt;ref name=JohnsonLogInv&gt;{{cite journal|last=Johnson|first=N.L.|title=Systems of frequency curves generated by methods of translation| journal=Biometrika|year=1949 |volume=36 |pages=149–176|doi=10.1093/biomet/36.1-2.149}}&lt;/ref&gt;  considered the distribution of the [[logit]] - transformed variable ln(''X''/1−''X''), including its moment generating function and approximations for large values of the shape parameters.  This transformation extends the finite support [0, 1] based on the original variable ''X'' to infinite support in both directions of the real line (−∞, +∞).

Higher order logarithmic moments can be derived by using the representation of a beta distribution as a proportion of two Gamma distributions and differentiating through the integral. They can be expressed in terms of higher order poly-gamma functions as follows:

:&lt;math&gt;\begin{align}
\operatorname{E} \left [\ln^2(X) \right ] &amp;= (\psi(\alpha) - \psi(\alpha + \beta))^2+\psi_1(\alpha)-\psi_1(\alpha+\beta), \\
\operatorname{E} \left [\ln^2(1-X) \right ] &amp;= (\psi(\beta) - \psi(\alpha + \beta))^2+\psi_1(\beta)-\psi_1(\alpha+\beta), \\
\operatorname{E} \left [\ln (X)\ln(1-X) \right ] &amp;=(\psi(\alpha) - \psi(\alpha + \beta))(\psi(\beta) - \psi(\alpha + \beta)) -\psi_1(\alpha+\beta).
\end{align}&lt;/math&gt;

therefore the [[variance]] of the logarithmic variables and [[covariance]] of ln(''X'') and ln(1−''X'') are:

:&lt;math&gt;\begin{align}
\operatorname{cov}[\ln(X), \ln(1-X)] &amp;= \operatorname{E}\left[\ln(X)\ln(1-X)\right] - \operatorname{E}[\ln(X)]\operatorname{E}[\ln(1-X)] = -\psi_1(\alpha+\beta) \\
&amp; \\
\operatorname{var}[\ln X] &amp;= \operatorname{E}[\ln^2(X)] - (\operatorname{E}[\ln(X)])^2 \\
&amp;= \psi_1(\alpha) - \psi_1(\alpha + \beta) \\
&amp;= \psi_1(\alpha) + \operatorname{cov}[\ln(X), \ln(1-X)] \\
&amp; \\
\operatorname{var}[\ln (1-X)] &amp;= \operatorname{E}[\ln^2 (1-X)] - (\operatorname{E}[\ln (1-X)])^2 \\
&amp;= \psi_1(\beta) - \psi_1(\alpha + \beta) \\
&amp;= \psi_1(\beta) + \operatorname{cov}[\ln (X), \ln(1-X)]
\end{align}&lt;/math&gt;

where the '''[[trigamma function]]''', denoted ψ&lt;sub&gt;1&lt;/sub&gt;(α), is the second of the [[polygamma function]]s, and is defined as the derivative of the [[digamma]] function:

:&lt;math&gt;\psi_1(\alpha) = \frac{d^2\ln\Gamma(\alpha)}{d\alpha^2}= \frac{d \psi(\alpha)}{d\alpha}&lt;/math&gt;.

The variances and covariance of the logarithmically transformed variables ''X'' and (1−''X'') are different, in general, because the logarithmic transformation destroys the mirror-symmetry of the original variables ''X'' and (1−''X''), as the logarithm approaches negative infinity for the variable approaching zero.

These logarithmic variances and covariance are the elements of the [[Fisher information]] matrix for the beta distribution.  They are also a measure of the curvature of the log likelihood function (see section on Maximum likelihood estimation).

The variances of the log inverse variables are identical to the variances of the log variables:

:&lt;math&gt;\begin{align}
\operatorname{var}\left[\ln \left (\frac{1}{X} \right ) \right] &amp; =\operatorname{var}[\ln(X)] = \psi_1(\alpha) - \psi_1(\alpha + \beta), \\
\operatorname{var}\left[\ln \left (\frac{1}{1-X} \right ) \right] &amp;=\operatorname{var}[\ln (1-X)] = \psi_1(\beta) - \psi_1(\alpha + \beta), \\
\operatorname{cov}\left[\ln \left (\frac{1}{X} \right), \ln \left (\frac{1}{1-X}\right ) \right] &amp;=\operatorname{cov}[\ln(X),\ln(1-X)]= -\psi_1(\alpha + \beta).\end{align}&lt;/math&gt;

It also follows that the variances of the [[logit]] transformed variables are:

:&lt;math&gt;\operatorname{var}\left[\ln \left (\frac{X}{1-X} \right )\right]=\operatorname{var}\left[\ln \left (\frac{1-X}{X} \right ) \right]=-\operatorname{cov}\left [\ln \left (\frac{X}{1-X} \right ), \ln \left (\frac{1-X}{X} \right ) \right]= \psi_1(\alpha) + \psi_1(\beta)&lt;/math&gt;

===Quantities of information (entropy)===
Given a beta distributed random variable, ''X'' ~ Beta(''α'', ''β''), the [[information entropy|differential entropy]] of ''X'' is&lt;ref&gt;A. C. G. Verdugo Lazo and P. N. Rathie. "On the entropy of continuous probability distributions," ''IEEE Trans. Inf. Theory'', IT-24:120–122, 1978.&lt;/ref&gt;(measured in [[Nat (unit)|nats]]), the expected value of the negative of the logarithm of the [[probability density function]]:

:&lt;math&gt;\begin{align}
h(X) &amp;= \operatorname{E}[-\ln(f(x;\alpha,\beta))] \\[4pt]
&amp;=\int_0^1 -f(x;\alpha,\beta)\ln(f(x;\alpha,\beta)) \, dx \\[4pt]
&amp;= \ln(\Beta(\alpha,\beta))-(\alpha-1)\psi(\alpha)-(\beta-1)\psi(\beta)+(\alpha+\beta-2) \psi(\alpha+\beta)
\end{align}&lt;/math&gt;

where ''f''(''x''; ''α'', ''β'') is the [[probability density function]] of the beta distribution:

:&lt;math&gt;f(x;\alpha,\beta) = \frac{1}{\Beta(\alpha,\beta)} x^{\alpha-1}(1-x)^{\beta-1}&lt;/math&gt;

The [[digamma function]] ''ψ'' appears in the formula for the differential entropy as a consequence of Euler's integral formula for the [[harmonic number]]s which follows from the integral:

:&lt;math&gt;\int_0^1 \frac {1-x^{\alpha-1}}{1-x} \, dx = \psi(\alpha)-\psi(1)&lt;/math&gt;

The [[information entropy|differential entropy]] of the beta distribution is negative for all values of ''α'' and ''β'' greater than zero, except at ''α'' = ''β'' = 1 (for which values the beta distribution is the same as the [[Uniform distribution (continuous)|uniform distribution]]), where the [[information entropy|differential entropy]] reaches its [[Maxima and minima|maximum]] value of zero.  It is to be expected that the maximum entropy should take place when the beta distribution becomes equal to the uniform distribution, since uncertainty is maximal when all possible events are equiprobable.

For ''α'' or ''β'' approaching zero, the [[information entropy|differential entropy]] approaches its [[Maxima and minima|minimum]] value of negative infinity. For (either or both) ''α'' or ''β'' approaching zero, there is a maximum amount of order: all the probability density is concentrated at the ends, and there is zero probability density at points located between the ends. Similarly for (either or both) ''α'' or ''β'' approaching infinity, the differential entropy approaches its minimum value of negative infinity, and a maximum amount of order.  If either ''α'' or ''β'' approaches infinity (and the other is finite) all the probability density is concentrated at an end, and the probability density is zero everywhere else.  If both shape parameters are equal (the symmetric case), ''α'' = ''β'', and they approach infinity simultaneously, the probability density becomes a spike ([[Dirac delta function]]) concentrated at the middle ''x'' = 1/2, and hence there is 100% probability at the middle ''x'' = 1/2 and zero probability everywhere else.

[[File:Differential Entropy Beta Distribution for alpha and beta from 1 to 5 - J. Rodal.jpg|325px]][[File:Differential Entropy Beta Distribution for alpha and beta from 0.1 to 5 - J. Rodal.jpg|325px]]

The (continuous case) [[information entropy|differential entropy]] was introduced by Shannon in his original paper (where he named it the "entropy of a continuous distribution"), as the concluding part&lt;ref&gt;Shannon, Claude E., "A Mathematical Theory of Communication," ''Bell System Technical Journal'', 27 (4):623–656,1948.[http://www.alcatel-lucent.com/bstj/vol27-1948/articles/bstj27-4-623.pdf PDF]&lt;/ref&gt; of the same paper where he defined the [[information entropy|discrete entropy]].  It is known since then that the differential entropy may differ from the infinitesimal limit of the discrete entropy by an infinite offset, therefore the differential entropy can be negative (as it is for the beta distribution). What really matters is the relative value of entropy.

Given two beta distributed random variables, ''X''&lt;sub&gt;1&lt;/sub&gt; ~ Beta(''α'', ''β'') and ''X''&lt;sub&gt;2&lt;/sub&gt; ~ Beta(''α''&amp;prime;, ''β''&amp;prime;), the [[cross entropy]] is (measured in nats)&lt;ref name="Cover and Thomas"&gt;{{cite book|last=Cover|first=Thomas M.  and Joy A. Thomas|title=Elements of Information Theory 2nd Edition (Wiley Series in Telecommunications and Signal Processing) |year=2006 |publisher=Wiley-Interscience; 2 edition |isbn=978-0471241959}}&lt;/ref&gt;

:&lt;math&gt;\begin{align}
H(X_1,X_2) &amp;= \int_0^1 - f(x;\alpha,\beta) \ln (f(x;\alpha',\beta')) \,dx \\[4pt]
&amp;= \ln \left(\Beta(\alpha',\beta')\right)-(\alpha'-1)\psi(\alpha)-(\beta'-1)\psi(\beta)+(\alpha'+\beta'-2)\psi(\alpha+\beta).
\end{align}&lt;/math&gt;

The [[cross entropy]] has been used as an error metric to measure the distance between two hypotheses.&lt;ref name=Plunkett&gt;{{cite book|last=Plunkett|first=Kim, and Jeffrey Elman|title=Exercises in Rethinking Innateness: A Handbook for Connectionist Simulations (Neural Network Modeling and Connectionism)|year=1997|publisher=A Bradford Book|location=p. 166|isbn=978-0262661058}}&lt;/ref&gt;&lt;ref name=Nallapati&gt;{{cite book|last=Nallapati|first=Ramesh|title=The smoothed dirichlet distribution: understanding cross-entropy ranking in information retrieval|year=2006|publisher=Computer Science Dept., University of Massachusetts Amherst|location=Ph.D. thesis|url=http://maroo.cs.umass.edu/pub/web/getpdf.php?id=679}}&lt;/ref&gt;  Its absolute value is minimum when the two distributions are identical. It is the information measure most closely related to the log maximum likelihood &lt;ref name="Cover and Thomas" /&gt;(see section on "Parameter estimation. Maximum likelihood estimation")).

The relative entropy, or [[Kullback&amp;ndash;Leibler divergence]] ''D''&lt;sub&gt;KL&lt;/sub&gt;(''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;), is a measure of the inefficiency of assuming that the distribution is ''X''&lt;sub&gt;2&lt;/sub&gt; ~ Beta(''α''&amp;prime;, ''β''&amp;prime;)  when the distribution is really ''X''&lt;sub&gt;1&lt;/sub&gt; ~ Beta(''α'', ''β''). It is defined as follows (measured in nats).

:&lt;math&gt;\begin{align}
D_{\mathrm{KL}}(X_1,X_2) &amp;= \int_0^1 f(x;\alpha,\beta) \ln \left (\frac{f(x;\alpha,\beta)}{f(x;\alpha',\beta')} \right ) \, dx \\[4pt]
&amp;= \left (\int_0^1 f(x;\alpha,\beta) \ln (f(x;\alpha,\beta)) \,dx \right )- \left (\int_0^1 f(x;\alpha,\beta) \ln (f(x;\alpha',\beta')) \, dx \right )\\[4pt]
&amp;= -h(X_1) + H(X_1,X_2)\\[4pt]
&amp;= \ln\left(\frac{\Beta(\alpha',\beta')}{\Beta(\alpha,\beta)}\right)+(\alpha-\alpha')\psi(\alpha)+(\beta-\beta')\psi(\beta)+(\alpha'-\alpha+\beta'-\beta)\psi (\alpha + \beta).
\end{align} &lt;/math&gt;

The relative entropy, or [[Kullback&amp;ndash;Leibler divergence]], is always non-negative.  A few numerical examples follow:

*''X''&lt;sub&gt;1&lt;/sub&gt; ~ Beta(1, 1) and ''X''&lt;sub&gt;2&lt;/sub&gt; ~ Beta(3, 3); ''D''&lt;sub&gt;KL&lt;/sub&gt;(''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;) = 0.598803; ''D''&lt;sub&gt;KL&lt;/sub&gt;(''X''&lt;sub&gt;2&lt;/sub&gt;, ''X''&lt;sub&gt;1&lt;/sub&gt;) = 0.267864; ''h''(''X''&lt;sub&gt;1&lt;/sub&gt;) = 0; ''h''(''X''&lt;sub&gt;2&lt;/sub&gt;) = −0.267864
*''X''&lt;sub&gt;1&lt;/sub&gt; ~ Beta(3, 0.5) and ''X''&lt;sub&gt;2&lt;/sub&gt; ~ Beta(0.5, 3); ''D''&lt;sub&gt;KL&lt;/sub&gt;(''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;) = 7.21574; ''D''&lt;sub&gt;KL&lt;/sub&gt;(''X''&lt;sub&gt;2&lt;/sub&gt;, ''X''&lt;sub&gt;1&lt;/sub&gt;) = 7.21574; ''h''(''X''&lt;sub&gt;1&lt;/sub&gt;) = −1.10805; ''h''(''X''&lt;sub&gt;2&lt;/sub&gt;) = −1.10805.

The [[Kullback&amp;ndash;Leibler divergence]] is not symmetric ''D''&lt;sub&gt;KL&lt;/sub&gt;(''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;) ≠ ''D''&lt;sub&gt;KL&lt;/sub&gt;(''X''&lt;sub&gt;2&lt;/sub&gt;, ''X''&lt;sub&gt;1&lt;/sub&gt;)  for the case in which the individual beta distributions Beta(1, 1) and Beta(3, 3) are symmetric, but have different entropies ''h''(''X''&lt;sub&gt;1&lt;/sub&gt;) ≠ ''h''(''X''&lt;sub&gt;2&lt;/sub&gt;). The value of the Kullback divergence depends on the direction traveled: whether going from a higher (differential) entropy to a lower (differential) entropy or the other way around. In the numerical example above, the Kullback divergence measures the inefficiency of assuming that the distribution is (bell-shaped) Beta(3, 3), rather than (uniform) Beta(1, 1). The "h" entropy of Beta(1, 1) is higher than the "h" entropy of Beta(3, 3) because the uniform distribution Beta(1, 1) has a maximum amount of disorder. The Kullback divergence is more than two times higher (0.598803 instead of 0.267864) when measured in the direction of decreasing entropy: the direction that assumes that the (uniform) Beta(1, 1) distribution is (bell-shaped) Beta(3, 3) rather than the other way around. In this restricted sense, the Kullback divergence is consistent with the [[second law of thermodynamics]].

The [[Kullback&amp;ndash;Leibler divergence]] is symmetric ''D''&lt;sub&gt;KL&lt;/sub&gt;(''X''&lt;sub&gt;1&lt;/sub&gt;, ''X''&lt;sub&gt;2&lt;/sub&gt;) = ''D''&lt;sub&gt;KL&lt;/sub&gt;(''X''&lt;sub&gt;2&lt;/sub&gt;, ''X''&lt;sub&gt;1&lt;/sub&gt;) for the skewed cases Beta(3, 0.5) and Beta(0.5, 3) that have equal differential entropy ''h''(''X''&lt;sub&gt;1&lt;/sub&gt;) = ''h''(''X''&lt;sub&gt;2&lt;/sub&gt;).

The symmetry condition:

:&lt;math&gt;D_{\mathrm{KL}}(X_1,X_2) = D_{\mathrm{KL}}(X_2,X_1),\text{ if }h(X_1) = h(X_2),\text{ for (skewed) }\alpha \neq \beta&lt;/math&gt;

follows from the above definitions and the mirror-symmetry ''f''(''x''; ''α'', ''β'') = ''f''(1−''x''; ''α'', ''β'') enjoyed by the beta distribution.

===Relationships between statistical measures===

====Mean, mode and median relationship====
If 1 &lt; α &lt; β then mode ≤ median ≤ mean.&lt;ref name=Kerman2011&gt;Kerman J (2011) "A closed-form approximation for the median of the beta distribution". {{arxiv|1111.0433v1}}&lt;/ref&gt;  Expressing the mode (only for α, β &gt; 1), and the mean in terms of α and β:

: &lt;math&gt; \frac{ \alpha - 1 }{ \alpha + \beta - 2 } \le \text{median}  \le \frac{ \alpha }{ \alpha + \beta } ,&lt;/math&gt;

If 1 &lt; β &lt; α then the order of the inequalities are reversed. For α, β &gt; 1 the absolute distance between the mean and the median is less than 5% of the distance between the maximum and minimum values of ''x''.  On the other hand, the absolute distance between the mean and the mode can reach 50% of the distance between the maximum and minimum values of ''x'', for the ([[Pathological (mathematics)|pathological]]) case of α = 1 and β = 1 (for which values the beta distribution approaches the uniform distribution and the [[information entropy|differential entropy]] approaches its [[Maxima and minima|maximum]] value, and hence maximum "disorder").

For example, for α = 1.0001 and β = 1.00000001:
* mode   = 0.9999;   PDF(mode) = 1.00010
* mean   = 0.500025; PDF(mean) = 1.00003
* median = 0.500035; PDF(median) = 1.00003
* mean − mode   = −0.499875
* mean − median = −9.65538 × 10&lt;sup&gt;−6&lt;/sup&gt;

(where PDF stands for the value of the [[probability density function]])

[[File:Mean Median Difference - Beta Distribution for alpha and beta from 1 to 5 - J. Rodal.jpg|325px]]
[[File:Mean Mode Difference - Beta Distribution for alpha and beta from 1 to 5 - J. Rodal.jpg|325px]]

====Mean, geometric mean and harmonic mean relationship====
[[File:Mean, Median, Geometric Mean and Harmonic Mean for Beta distribution with alpha = beta from 0 to 5 - J. Rodal.png|thumb|:Mean, Median, Geometric Mean and Harmonic Mean for Beta distribution with 0 &lt; α = β &lt; 5]]

It is known from the [[inequality of arithmetic and geometric means]] that the geometric mean is lower than the mean.  Similarly, the harmonic mean is lower than the geometric mean.  The accompanying plot shows that for α = β, both the mean and the median are exactly equal to 1/2, regardless of the value of α = β, and the mode is also equal to 1/2 for α = β &gt; 1, however the geometric and harmonic means are lower than 1/2 and they only approach this value asymptotically as α = β → ∞.

====Kurtosis bounded by the square of the skewness====
[[File:(alpha and beta) Parameter estimates vs. excess Kurtosis and (squared) Skewness Beta distribution - J. Rodal.png|thumb|left|Beta distribution α and β parameters vs. excess Kurtosis and squared Skewness]]

As remarked by [[William Feller|Feller]],&lt;ref name=Feller /&gt; in the [[Pearson distribution|Pearson system]] the beta probability density appears as [[Pearson distribution|type I]] (any difference between the beta distribution and Pearson's type I distribution is only superficial and it makes no difference for the following discussion regarding the relationship between kurtosis and skewness). [[Karl Pearson]] showed, in Plate 1 of his paper &lt;ref name=Pearson&gt;{{cite journal
 | last = Pearson
 | first = Karl
 | authorlink = Karl Pearson
 | year = 1916
 | title = Mathematical contributions to the theory of evolution, XIX: Second supplement to a memoir on skew variation
 | journal = Philosophical Transactions of the Royal Society A
 | volume = 216
 | issue =538–548
 | pages = 429&amp;ndash;457
 | doi = 10.1098/rsta.1916.0009
 | id =
 | jstor=91092|bibcode = 1916RSPTA.216..429P }}&lt;/ref&gt;  published in 1916,  a graph with the [[kurtosis]] as the vertical axis ([[ordinate]]) and the square of the [[skewness]] as the horizontal axis ([[abscissa]]), in which a number of distributions were displayed.&lt;ref name=Egon&gt;{{cite journal|last=Pearson|first=Egon S.|title=Some historical reflections traced through the development of the use of frequency curves|journal=THEMIS Statistical Analysis Research Program, Technical Report 38|date=July 1969|volume=Office of Naval Research, Contract N000014-68-A-0515|issue=Project NR 042-260|url=http://www.smu.edu/Dedman/Academics/Departments/Statistics/Research/TechnicalReports}}&lt;/ref&gt;  The region occupied by the beta distribution is bounded by the following two [[Line (geometry)|lines]] in the (skewness&lt;sup&gt;2&lt;/sup&gt;,kurtosis) [[Cartesian coordinate system|plane]], or the (skewness&lt;sup&gt;2&lt;/sup&gt;,excess kurtosis) [[Cartesian coordinate system|plane]]:

:&lt;math&gt;(\text{skewness})^2+1&lt; \text{kurtosis}&lt; \frac{3}{2} (\text{skewness})^2 + 3&lt;/math&gt;

or, equivalently,

:&lt;math&gt;(\text{skewness})^2-2&lt; \text{excess kurtosis}&lt; \frac{3}{2} (\text{skewness})^2&lt;/math&gt;

(At a time when there were no powerful digital computers), [[Karl Pearson]] accurately computed further boundaries,&lt;ref name="Hahn and Shapiro" /&gt;&lt;ref name=Pearson /&gt; for example, separating the "U-shaped" from the "J-shaped" distributions. The lower boundary line (excess kurtosis + 2 − skewness&lt;sup&gt;2&lt;/sup&gt; = 0) is produced by skewed "U-shaped" beta distributions with both values of shape parameters α and β close to zero.  The upper boundary line (excess kurtosis − (3/2) skewness&lt;sup&gt;2&lt;/sup&gt; = 0) is produced by extremely skewed distributions with very large values of one of the parameters and very small values of the other parameter.  [[Karl Pearson]] showed &lt;ref name=Pearson/&gt; that this upper boundary line (excess kurtosis − (3/2) skewness&lt;sup&gt;2&lt;/sup&gt; = 0) is also the intersection with Pearson's distribution III, which has unlimited support in one direction (towards positive infinity), and can be bell-shaped or J-shaped. His son, [[Egon Pearson]], showed &lt;ref name=Egon/&gt; that the region (in the kurtosis/squared-skewness plane) occupied by the beta distribution (equivalently, Pearson's distribution I) as it approaches this boundary (excess kurtosis − (3/2) skewness&lt;sup&gt;2&lt;/sup&gt; = 0) is shared with the [[noncentral chi-squared distribution]].  Karl Pearson&lt;ref name=Pearson1895&gt;{{cite journal | last = Pearson | first = Karl | authorlink = Karl Pearson | year = 1895 | title = Contributions to the mathematical theory of evolution, II: Skew variation in homogeneous material | journal = Philosophical Transactions of the Royal Society | volume = 186 | issue = | pages = 343&amp;ndash;414 | doi = 10.1098/rsta.1895.0010 | id = | jstor=90649 | bibcode=1895RSPTA.186..343P}}&lt;/ref&gt; (Pearson 1895, pp.&amp;nbsp;357, 360, 373–376) also showed that the gamma distribution is a Pearson type III distribution. Hence this boundary line for Pearson's type III distribution is known as the gamma line. (This can be shown from the fact that the excess kurtosis of the [[gamma distribution]] is 6/''k'' and the square of the skewness is 4/''k'', hence (excess kurtosis − (3/2) skewness&lt;sup&gt;2&lt;/sup&gt; = 0) is identically satisfied by the [[gamma distribution]] regardless of the value of the parameter "k"). Pearson later noted that the [[chi-squared distribution]] is a special case of Pearson's type III and also shares this boundary line (as it is apparent from the fact that for the [[chi-squared distribution]] the excess kurtosis is 12/''k'' and the square of the skewness is 8/''k'', hence (excess kurtosis − (3/2) skewness&lt;sup&gt;2&lt;/sup&gt; = 0) is identically satisfied regardless of the value of the parameter "k"). This is to be expected, since the chi-squared distribution ''X'' ~ χ&lt;sup&gt;2&lt;/sup&gt;(''k'') is a special case of the gamma distribution, with parametrization X ~ Γ(k/2, 1/2) where k is a positive integer that specifies the "number of degrees of freedom" of the chi-squared distribution.

An example of a beta distribution near the upper boundary (excess kurtosis − (3/2) skewness&lt;sup&gt;2&lt;/sup&gt; = 0) is given by α = 0.1, β = 1000, for which the ratio (excess kurtosis)/(skewness&lt;sup&gt;2&lt;/sup&gt;) = 1.49835 approaches the upper limit of 1.5 from below. An example of a beta distribution near the lower boundary (excess kurtosis + 2 − skewness&lt;sup&gt;2&lt;/sup&gt; = 0) is given by α= 0.0001, β = 0.1, for which values the expression (excess kurtosis + 2)/(skewness&lt;sup&gt;2&lt;/sup&gt;) = 1.01621 approaches the lower limit of 1 from above. In the infinitesimal limit for both α and β approaching zero symmetrically, the excess kurtosis reaches its minimum value at −2.  This minimum value occurs at the point at which the lower boundary line intersects the vertical axis ([[ordinate]]). (However, in Pearson's original chart, the ordinate is kurtosis, instead of excess kurtosis, and it increases downwards rather than upwards).

Values for the skewness and excess kurtosis below the lower boundary (excess kurtosis + 2 − skewness&lt;sup&gt;2&lt;/sup&gt; = 0) cannot occur for any distribution, and hence [[Karl Pearson]] appropriately called the region below this boundary the "impossible region." The boundary for this "impossible region" is determined by (symmetric or skewed) bimodal "U"-shaped distributions for which parameters α and β approach zero and hence all the probability density is concentrated at the ends: ''x'' = 0, 1 with practically nothing in between them.  Since for α ≈ β ≈ 0 the probability density is concentrated at the two ends ''x'' = 0 and ''x'' = 1, this "impossible boundary" is determined by a 2-point distribution: the probability can only take 2 values ([[Bernoulli distribution]]), one value with probability p and the other with probability ''q'' = 1−''p''. For cases approaching this limit boundary with symmetry α = β, skewness ≈ 0, excess kurtosis ≈ −2 (this is the lowest excess kurtosis possible for any distribution), and the probabilities are ''p'' ≈ ''q'' ≈ 1/2.  For cases approaching this limit boundary with skewness, excess kurtosis ≈ −2 + skewness&lt;sup&gt;2&lt;/sup&gt;, and the probability density is concentrated more at one end than the other end (with practically nothing in between), with probabilities &lt;math&gt;p = \tfrac{\beta}{\alpha + \beta}&lt;/math&gt; at the left end ''x'' = 0 and &lt;math&gt;q = 1-p = \tfrac{\alpha}{\alpha + \beta}&lt;/math&gt; at the right end ''x'' = 1.

===Symmetry===
All statements are conditional on α, β &gt; 0

* '''Probability density function''' [[Symmetry|reflection symmetry]]
::&lt;math&gt;f(x;\alpha,\beta) = f(1-x;\beta,\alpha)&lt;/math&gt;

* '''Cumulative distribution function''' [[Symmetry|reflection symmetry]] plus unitary [[Symmetry|translation]]
::&lt;math&gt;F(x;\alpha,\beta) = I_x(\alpha,\beta) = 1- F(1- x;\beta,\alpha) = 1 - I_{1-x}(\beta,\alpha)&lt;/math&gt;

* '''Mode''' [[Symmetry|reflection symmetry]] plus unitary [[Symmetry|translation]]
::&lt;math&gt;\operatorname{mode}(\Beta(\alpha, \beta))= 1-\operatorname{mode}(\Beta(\beta, \alpha)),\text{ if }\Beta(\beta, \alpha)\ne \Beta(1,1)&lt;/math&gt;

* '''Median''' [[Symmetry|reflection symmetry]] plus unitary [[Symmetry|translation]]
::&lt;math&gt;\operatorname{median} (\Beta(\alpha, \beta) )= 1 - \operatorname{median} (\Beta(\beta, \alpha))&lt;/math&gt;

* '''Mean''' [[Symmetry|reflection symmetry]] plus unitary [[Symmetry|translation]]
::&lt;math&gt;\mu (\Beta(\alpha, \beta) )= 1 - \mu (\Beta(\beta, \alpha) )&lt;/math&gt;

* '''Geometric Means''' each is individually asymmetric, the following symmetry applies between the geometric mean based on ''X'' and the geometric mean based on its [[Reflection formula|reflection]] (1-X)
::&lt;math&gt;G_X (\Beta(\alpha, \beta) )=G_{(1-X)}(\Beta(\beta, \alpha) ) &lt;/math&gt;

* '''Harmonic means''' each is individually asymmetric, the following symmetry applies between the harmonic mean based on ''X'' and the harmonic mean based on its [[Reflection formula|reflection]] (1-X)
::&lt;math&gt;H_X (\Beta(\alpha, \beta) )=H_{(1-X)}(\Beta(\beta, \alpha) ) \text{ if } \alpha, \beta &gt; 1 &lt;/math&gt; .

* '''Variance''' symmetry
::&lt;math&gt;\operatorname{var} (\Beta(\alpha, \beta) )=\operatorname{var} (\Beta(\beta, \alpha) )&lt;/math&gt;

* '''Geometric variances''' each is individually asymmetric, the following symmetry applies between the log geometric variance based on X and the log geometric variance based on its [[Reflection formula|reflection]] (1-X)
::&lt;math&gt;\ln(\operatorname{var_{GX}} (\Beta(\alpha, \beta))) = \ln(\operatorname{var_{G(1-X)}}(\Beta(\beta, \alpha))) &lt;/math&gt;

* '''Geometric covariance''' symmetry
::&lt;math&gt;\ln \operatorname{cov_{GX,(1-X)}}(\Beta(\alpha, \beta))=\ln \operatorname{cov_{GX,(1-X)}}(\Beta(\beta, \alpha))&lt;/math&gt;

* '''Mean [[absolute deviation]] around the mean''' symmetry
::&lt;math&gt;\operatorname{E}[|X - E[X]| ] (\Beta(\alpha, \beta))=\operatorname{E}[| X - E[X]|] (\Beta(\beta, \alpha))&lt;/math&gt;

* '''Skewness''' [[Symmetry (mathematics)|skew-symmetry]]
::&lt;math&gt;\operatorname{skewness} (\Beta(\alpha, \beta) )= - \operatorname{ skewness} (\Beta(\beta, \alpha) )&lt;/math&gt;

* '''Excess kurtosis''' symmetry
::&lt;math&gt;\text{excess kurtosis} (\Beta(\alpha, \beta) )= \text{excess kurtosis} (\Beta(\beta, \alpha) )&lt;/math&gt;

* '''Characteristic function''' symmetry of [[Real part]] (with respect to the origin of variable "t")
::&lt;math&gt; \text{Re} [{}_1F_1(\alpha; \alpha+\beta; it) ] = \text{Re} [ {}_1F_1(\alpha; \alpha+\beta; - it)]  &lt;/math&gt;

* '''Characteristic function''' [[Symmetry (mathematics)|skew-symmetry]] of [[Imaginary part]] (with respect to the origin of variable "t")
::&lt;math&gt; \text{Im} [{}_1F_1(\alpha; \alpha+\beta; it) ] = - \text{Im} [ {}_1F_1(\alpha; \alpha+\beta; - it) ]  &lt;/math&gt;

* '''Characteristic function''' symmetry of [[Absolute value]] (with respect to the origin of variable "t")
::&lt;math&gt; \text{Abs} [ {}_1F_1(\alpha; \alpha+\beta; it) ] = \text{Abs} [ {}_1F_1(\alpha; \alpha+\beta; - it) ]  &lt;/math&gt;

* '''Differential entropy''' symmetry
::&lt;math&gt;h(\Beta(\alpha, \beta) )= h(\Beta(\beta, \alpha) )&lt;/math&gt;

* '''Relative Entropy (also called [[Kullback&amp;ndash;Leibler divergence]])''' symmetry
::&lt;math&gt;D_{\mathrm{KL}}(X_1,X_2) = D_{\mathrm{KL}}(X_2,X_1), \text{ if }h(X_1) = h(X_2)\text{, for (skewed) }\alpha \neq \beta&lt;/math&gt;

* '''Fisher information matrix''' symmetry
::&lt;math&gt;{\mathcal{I}}_{i, j} = {\mathcal{I}}_{j, i}&lt;/math&gt;

===Geometry of the probability density function===

====Inflection points====
[[File:Inflexion points Beta Distribution alpha and beta ranging from 0 to 5 large ptl view - J. Rodal.jpg|thumb|Inflection point location versus α and β showing regions with one inflection point]]
[[File:Inflexion points Beta Distribution alpha and beta ranging from 0 to 5 large ptr view - J. Rodal.jpg|thumb|Inflection point location versus α and β showing region with two inflection points]]

For certain values of the shape parameters α and β, the [[probability density function]] has [[inflection points]], at which the [[curvature]] changes sign.  The position of these inflection points can be useful as a measure of the [[Statistical dispersion|dispersion]] or spread of the distribution.

Defining the following quantity:

:&lt;math&gt;\kappa =\frac{\sqrt{\frac{(\alpha-1)(\beta-1)}{\alpha+\beta-3}}}{\alpha+\beta-2}&lt;/math&gt;

Points of inflection occur,&lt;ref name=JKB /&gt;&lt;ref name=Wadsworth /&gt;&lt;ref name="Handbook of Beta Distribution" /&gt;&lt;ref name=Panik /&gt; depending on the value of the shape parameters α and β, as follows:

*(α &gt; 2, β &gt; 2) The distribution is bell-shaped (symmetric for α = β and skewed otherwise), with '''two inflection points''', equidistant from the mode:
::&lt;math&gt;x = \text{mode} \pm \kappa = \frac{\alpha -1 \pm \sqrt{\frac{(\alpha-1)(\beta-1)}{\alpha+\beta-3}}}{\alpha+\beta-2}&lt;/math&gt;

* (α = 2, β &gt; 2) The distribution is unimodal, positively skewed, right-tailed, with '''one inflection point''', located to the right of the mode:
::&lt;math&gt;x =\text{mode} + \kappa = \frac{2}{\beta}&lt;/math&gt;

* (α &gt; 2, β = 2) The distribution is unimodal, negatively skewed, left-tailed, with '''one inflection point''', located to the left of the mode:
::&lt;math&gt;x = \text{mode} - \kappa = 1 - \frac{2}{\alpha}&lt;/math&gt;

* (1 &lt; α &lt; 2, β &gt; 2, α+β&gt;2) The distribution is unimodal, positively skewed, right-tailed, with '''one inflection point''', located to the right of the mode:
::&lt;math&gt;x =\text{mode} + \kappa = \frac{\alpha -1 +\sqrt{\frac{(\alpha-1)(\beta-1)}{\alpha+\beta-3}}}{\alpha+\beta-2}&lt;/math&gt;

*(0 &lt; α &lt; 1, 1 &lt; β &lt; 2) The distribution has a mode at the left end ''x'' = 0 and it is positively skewed, right-tailed. There is '''one inflection point''', located to the right of the mode:
::&lt;math&gt;x = \frac{\alpha -1 +\sqrt{\frac{(\alpha-1)(\beta-1)}{\alpha+\beta-3}}}{\alpha+\beta-2}&lt;/math&gt;

*(α &gt; 2, 1 &lt; β &lt; 2) The distribution is unimodal negatively skewed, left-tailed, with '''one inflection point''', located to the left of the mode:
::&lt;math&gt;x =\text{mode} - \kappa = \frac{\alpha -1 -\sqrt{\frac{(\alpha-1)(\beta-1)}{\alpha+\beta-3}}}{\alpha+\beta-2}&lt;/math&gt;

*(1 &lt; α &lt; 2,  0 &lt; β &lt; 1) The distribution has a mode at the right end ''x''=1 and it is negatively skewed, left-tailed. There is '''one inflection point''', located to the left of the mode:
::&lt;math&gt;x = \frac{\alpha -1 -\sqrt{\frac{(\alpha-1)(\beta-1)}{\alpha+\beta-3}}}{\alpha+\beta-2}&lt;/math&gt;

There are no inflection points in the remaining (symmetric and skewed) regions: U-shaped: (α, β &lt; 1) upside-down-U-shaped: (1 &lt; α &lt; 2, 1 &lt; β &lt; 2), reverse-J-shaped (α &lt; 1, β &gt; 2) or J-shaped: (α &gt; 2, β &lt; 1)

The accompanying plots show the inflection point locations (shown vertically, ranging from 0 to 1) versus α and β (the horizontal axes ranging from 0 to 5). There are large cuts at surfaces intersecting the lines α = 1, β = 1, α = 2, and β = 2 because at these values the beta distribution change from 2 modes, to 1 mode to no mode.

====Shapes====
[[File:PDF for symmetric beta distribution vs. x and alpha=beta from 0 to 30 - J. Rodal.jpg|thumb|PDF for symmetric beta distribution vs. ''x'' and ''α''&amp;nbsp;=&amp;nbsp;''β'' from 0 to 30]]
[[File:PDF for symmetric beta distribution vs. x and alpha=beta from 0 to 2 - J. Rodal.jpg|thumb|PDF for symmetric beta distribution vs. x and ''α''&amp;nbsp;=&amp;nbsp;''β'' from 0 to 2]]
[[File:PDF for skewed beta distribution vs. x and beta= 2.5 alpha from 0 to 9 - J. Rodal.jpg|thumb|PDF for skewed beta distribution vs. ''x'' and ''β''&amp;nbsp;=&amp;nbsp;2.5''α'' from 0 to 9]]
[[File:PDF for skewed beta distribution vs. x and beta= 5.5 alpha from 0 to 9 - J. Rodal.jpg|thumb|PDF for skewed beta distribution vs. x and ''β''&amp;nbsp;=&amp;nbsp;5.5''α'' from 0 to 9]]
[[File:PDF for skewed beta distribution vs. x and beta= 8 alpha from 0 to 10 - J. Rodal.jpg|thumb|PDF for skewed beta distribution vs. x and ''β''&amp;nbsp;=&amp;nbsp;8''α'' from 0 to 10]]

The beta density function can take a wide variety of different shapes depending on the values of the two parameters ''α'' and ''β''.  The ability of the beta distribution to take this great diversity of shapes (using only two parameters) is partly responsible for finding wide application for modeling actual measurements:

=====Symmetric (''α'' = ''β'')=====
* the density function is [[symmetry|symmetric]] about 1/2 (blue &amp; teal plots).
* median = mean = 1/2.
*skewness  = 0.
*'''α = β &lt; 1'''
**U-shaped (blue plot).
**bimodal: left mode = 0,  right mode =1, anti-mode = 1/2
**1/12 &lt; var(''X'') &lt; 1/4&lt;ref name=JKB/&gt;
**−2 &lt; excess kurtosis(''X'') &lt; −6/5
** α = β = 1/2 is the [[arcsine distribution]]
*** var(''X'') = 1/8
***excess kurtosis(''X'') = −3/2
***CF = Rinc (t) &lt;ref&gt;{{Cite journal|last=Buchanan|first=K.|last2=Rockway|first2=J.|last3=Sternberg|first3=O.|last4=Mai|first4=N. N.|date=May 2016|title=Sum-difference beamforming for radar applications using circularly tapered random arrays|url=http://ieeexplore.ieee.org/document/7485289/|journal=2016 IEEE Radar Conference (RadarConf)|pages=1–5|doi=10.1109/RADAR.2016.7485289}}&lt;/ref&gt;
** α = β → 0 is a 2-point [[Bernoulli distribution]] with equal probability 1/2 at each [[Dirac delta function]] end ''x'' = 0 and ''x'' = 1 and zero probability everywhere else. A coin toss: one face of the coin being ''x'' = 0 and the other face being ''x'' = 1.
*** &lt;math&gt; \lim_{\alpha = \beta \to  0} \operatorname{var}(X) = \tfrac{1}{4} &lt;/math&gt;
*** &lt;math&gt; \lim_{\alpha = \beta \to  0} \operatorname{excess \ kurtosis}(X) = - 2&lt;/math&gt;  a lower value than this is impossible for any distribution to reach.
*** The [[information entropy|differential entropy]] approaches a [[Maxima and minima|minimum]] value of −∞
*'''α = β = 1'''
**the [[uniform distribution (continuous)|uniform [0, 1] distribution]]
**no mode
**var(''X'') = 1/12
**excess kurtosis(''X'') = −6/5
**The (negative anywhere else) [[information entropy|differential entropy]] reaches its [[Maxima and minima|maximum]] value of zero
**CF = Sinc (t)
*'''''α'' = ''β'' &gt; 1'''
**symmetric [[unimodal]]
** mode = 1/2.
**0 &lt; var(''X'') &lt; 1/12&lt;ref name=JKB/&gt;
**−6/5 &lt; excess kurtosis(''X'') &lt; 0
**''α'' = ''β'' = 3/2 is a semi-elliptic [0, 1] distribution, see: [[Wigner semicircle distribution]] &lt;ref&gt;{{Cite journal|last=Buchanan|first=K.|last2=Flores|first2=C.|last3=Wheeland|first3=S.|last4=Jensen|first4=J.|last5=Grayson|first5=D.|last6=Huff|first6=G.|date=May 2017|title=Transmit beamforming for radar applications using circularly tapered random arrays|url=http://ieeexplore.ieee.org/document/7944181/|journal=2017 IEEE Radar Conference (RadarConf)|pages=0112–0117|doi=10.1109/RADAR.2017.7944181}}&lt;/ref&gt;
***var(''X'') = 1/16.
***excess kurtosis(''X'') = −1
***CF = 2 Jinc (t)
**''α'' = ''β'' = 2 is the parabolic [0, 1] distribution
***var(''X'') = 1/20
***excess kurtosis(''X'') = −6/7
***CF = 3 Tinc (t) &lt;ref&gt;{{Cite journal|last=Ryan|first=Buchanan, Kristopher|date=2014-05-29|title=Theory and Applications of Aperiodic (Random) Phased Arrays|url=http://oaktrust.library.tamu.edu/handle/1969.1/157918|language=en}}&lt;/ref&gt;
**''α'' = ''β'' &gt; 2 is bell-shaped, with [[inflection point]]s located to either side of the mode
***0 &lt; var(''X'') &lt; 1/20
***−6/7 &lt; excess kurtosis(''X'') &lt; 0
**''α'' = ''β'' → ∞ is a 1-point [[Degenerate distribution]] with a [[Dirac delta function]] spike at the midpoint ''x'' = 1/2 with probability 1, and zero probability everywhere else. There is 100% probability (absolute certainty) concentrated at the single point ''x'' = 1/2.
***&lt;math&gt; \lim_{\alpha = \beta \to  \infty} \operatorname{var}(X) = 0 &lt;/math&gt;
***&lt;math&gt; \lim_{\alpha = \beta \to  \infty} \operatorname{excess \ kurtosis}(X) = 0&lt;/math&gt;
***The [[information entropy|differential entropy]] approaches a [[Maxima and minima|minimum]] value of −∞

=====Skewed (''α'' ≠ ''β'')=====
The density function is [[Skewness|skewed]].  An interchange of parameter values yields the [[mirror image]] (the reverse) of the initial curve, some more specific cases:
*'''''α'' &lt; 1, ''β'' &lt; 1'''
** U-shaped
** Positive skew for α &lt; β, negative skew for α &gt; β.
** bimodal: left mode = 0, right mode = 1,  anti-mode = &lt;math&gt;\tfrac{\alpha-1}{\alpha + \beta-2} &lt;/math&gt;
** 0 &lt; median &lt; 1.
** 0 &lt; var(''X'') &lt; 1/4
*'''α &gt; 1, β &gt; 1'''
** [[unimodal]] (magenta &amp; cyan plots),
**Positive skew for α &lt; β, negative skew for α &gt; β.
**&lt;math&gt;\text{mode }= \tfrac{\alpha-1}{\alpha + \beta-2} &lt;/math&gt;
** 0 &lt; median &lt; 1
** 0 &lt; var(''X'') &lt; 1/12
*'''α &lt; 1, β ≥ 1'''
**reverse J-shaped with a right tail,
**positively skewed,
**strictly decreasing, [[convex function|convex]]
** mode = 0
** 0 &lt; median &lt; 1/2.
** &lt;math&gt;0 &lt; \operatorname{var}(X) &lt; \tfrac{-11+5 \sqrt{5}}{2}, &lt;/math&gt; (maximum variance occurs for &lt;math&gt;\alpha=\tfrac{-1+\sqrt{5}}{2}, \beta=1&lt;/math&gt;, or α = '''Φ''' the [[Golden ratio|golden ratio conjugate]])
*'''α ≥ 1, β &lt; 1'''
**J-shaped with a left tail,
**negatively skewed,
**strictly increasing, [[convex function|convex]]
** mode = 1
** 1/2 &lt; median &lt; 1
** &lt;math&gt;0 &lt; \operatorname{var}(X) &lt; \tfrac{-11+5 \sqrt{5}}{2},&lt;/math&gt; (maximum variance occurs for &lt;math&gt;\alpha=1, \beta=\tfrac{-1+\sqrt{5}}{2}&lt;/math&gt;, or β = '''Φ''' the [[Golden ratio|golden ratio conjugate]])
*'''α = 1, β &gt; 1'''
**positively skewed,
**strictly decreasing (red plot),
**a reversed (mirror-image) power function [0,1] distribution
** mode = 0
**α = 1, 1 &lt; β &lt; 2
***[[concave function|concave]]
*** &lt;math&gt;1-\tfrac{1}{\sqrt{2}}&lt; \text{median} &lt; \tfrac{1}{2}&lt;/math&gt;
*** 1/18 &lt; var(''X'') &lt; 1/12.
**α = 1, β = 2
***a straight line with slope −2, the right-[[triangular distribution]] with right angle at the left end, at ''x'' = 0
*** &lt;math&gt;\text{median}=1-\tfrac {1}{\sqrt{2}}&lt;/math&gt;
*** var(''X'') = 1/18
**α = 1, β &gt; 2
***reverse J-shaped with a right tail,
***[[convex function|convex]]
*** &lt;math&gt;0 &lt; \text{median} &lt; 1-\tfrac{1}{\sqrt{2}}&lt;/math&gt;
*** 0 &lt; var(''X'') &lt; 1/18
*'''α &gt; 1, β = 1'''
**negatively skewed,
**strictly increasing (green plot),
**the power function [0, 1] distribution&lt;ref name="Handbook of Beta Distribution" /&gt;
**mode =1
**2 &gt; α &gt; 1, β = 1
***[[concave function|concave]]
*** &lt;math&gt;\tfrac{1}{2} &lt; \text{median} &lt; \tfrac{1}{\sqrt{2}}&lt;/math&gt;
*** 1/18 &lt; var(''X'') &lt; 1/12
** α = 2, β = 1
***a straight line with slope +2, the right-[[triangular distribution]] with right angle at the right end, at ''x'' = 1
*** &lt;math&gt;\text{median}=\tfrac {1}{\sqrt{2}}&lt;/math&gt;
*** var(''X'') = 1/18
**α &gt; 2, β = 1
***J-shaped with a left tail, [[convex function|convex]]
***&lt;math&gt;\tfrac{1}{\sqrt{2}} &lt; \text{median} &lt; 1&lt;/math&gt;
*** 0 &lt; var(''X'') &lt; 1/18

==Parameter estimation==

===Method of moments===

====Two unknown parameters====
Two unknown parameters (&lt;math&gt; (\hat{\alpha}, \hat{\beta})&lt;/math&gt;  of a beta distribution supported in the [0,1] interval) can be estimated, using the method of moments, with the first two moments (sample mean and sample variance) as follows.  Let:

: &lt;math&gt;\text{sample mean(X)}=\bar{x} = \frac{1}{N}\sum_{i=1}^N X_i&lt;/math&gt;

be the [[sample mean]] estimate and

: &lt;math&gt;\text{sample variance(X)} =\bar{v} = \frac{1}{N-1}\sum_{i=1}^N (X_i - \bar{x})^2&lt;/math&gt;

be the [[sample variance]] estimate.  The [[method of moments (statistics)|method-of-moments]] estimates of the parameters are

:&lt;math&gt;\hat{\alpha} = \bar{x} \left(\frac{\bar{x} (1 - \bar{x})}{\bar{v}} - 1 \right),&lt;/math&gt; if &lt;math&gt;\bar{v} &lt;\bar{x}(1 - \bar{x}),&lt;/math&gt;
: &lt;math&gt;\hat{\beta} = (1-\bar{x}) \left(\frac{\bar{x} (1 - \bar{x})}{\bar{v}} - 1 \right),&lt;/math&gt; if &lt;math&gt;\bar{v}&lt;\bar{x}(1 - \bar{x}).&lt;/math&gt;
&lt;!--  MLE's should be in this section too.  Maybe I'll be back.... --&gt;

When the distribution is required over a known interval other than [0, 1] with random variable ''X'', say [''a'', ''c''] with random variable ''Y'', then replace &lt;math&gt;\bar{x}&lt;/math&gt; with &lt;math&gt;\frac{\bar{y}-a}{c-a},&lt;/math&gt; and &lt;math&gt;\bar{v}&lt;/math&gt; with &lt;math&gt;\frac{\bar{v_Y}}{(c-a)^2}&lt;/math&gt; in the above couple of equations for the shape parameters (see the "Alternative parametrizations, four parameters" section below).,&lt;ref&gt;[http://www.itl.nist.gov/div898/handbook/eda/section3/eda366h.htm Engineering Statistics Handbook]&lt;/ref&gt; where:

: &lt;math&gt;\text{sample mean(Y)}=\bar{y} = \frac{1}{N}\sum_{i=1}^N Y_i&lt;/math&gt;
: &lt;math&gt;\text{sample variance(Y)} = \bar{v_Y} = \frac{1}{N-1}\sum_{i=1}^N (Y_i - \bar{y})^2&lt;/math&gt;

====Four unknown parameters====
[[File:(alpha and beta) Parameter estimates vs. excess Kurtosis and (squared) Skewness Beta distribution - J. Rodal.png|thumb|Solutions for parameter estimates vs. (sample) excess Kurtosis and (sample) squared Skewness Beta distribution]]

All four parameters (&lt;math&gt;\hat{\alpha}, \hat{\beta}, \hat{a}, \hat{c}&lt;/math&gt; of a beta distribution supported in the [''a'', ''c''] interval -see section [[Beta distribution#Four parameters 2|"Alternative parametrizations, Four parameters"]]-) can be estimated, using the method of moments developed by [[Karl Pearson]], by equating sample and population values of the first four central moments (mean, variance, skewness and excess kurtosis).&lt;ref name=JKB/&gt;&lt;ref name=Elderton1906/&gt;&lt;ref name="Elderton and Johnson"&gt;{{cite book|last=Elderton|first=William Palin and Norman Lloyd Johnson|title=Systems of Frequency Curves|year=2009|publisher=Cambridge University Press|isbn=978-0521093361}}&lt;/ref&gt; The excess kurtosis was expressed in terms of the square of the skewness, and the sample size ν = α + β, (see previous section [[Beta distribution#Kurtosis|"Kurtosis"]]) as follows:

:&lt;math&gt;\text{excess kurtosis} =\frac{6}{3 + \nu}\left(\frac{(2 + \nu)}{4} (\text{skewness})^2 - 1\right)\text{ if (skewness)}^2-2&lt; \text{excess kurtosis}&lt; \tfrac{3}{2} (\text{skewness})^2&lt;/math&gt;

One can use this equation to solve for the sample size ν= α + β in terms of the square of the skewness and the excess kurtosis as follows:&lt;ref name=Elderton1906/&gt;

:&lt;math&gt;\hat{\nu} = \hat{\alpha} + \hat{\beta} = 3\frac{(\text{sample excess kurtosis})  - (\text{sample skewness})^2+2}{\frac{3}{2} (\text{sample skewness})^2 - \text{(sample excess kurtosis)}}&lt;/math&gt;
:&lt;math&gt;\text{ if (sample skewness)}^2-2&lt; \text{sample excess kurtosis}&lt; \tfrac{3}{2} (\text{sample skewness})^2&lt;/math&gt;

This is the ratio (multiplied by a factor of 3) between the previously derived limit boundaries for the beta distribution in a space (as originally done by Karl Pearson&lt;ref name=Pearson /&gt;) defined with coordinates of the square of the skewness in one axis and the excess kurtosis in the other axis (see previous section titled  "Kurtosis bounded by the square of the skewness"):

The case of zero skewness, can be immediately solved because for zero skewness, α = β and hence ν = 2α = 2β, therefore α = β = ν/2

: &lt;math&gt;\hat{\alpha} = \hat{\beta} = \frac{\hat{\nu}}{2}= \frac{\frac{3}{2}(\text{sample excess kurtosis}) +3}{- \text{(sample excess kurtosis)}}&lt;/math&gt;
: &lt;math&gt; \text{ if sample skewness}= 0 \text{ and } -2&lt;\text{sample excess kurtosis}&lt;0&lt;/math&gt;

(Excess kurtosis is negative for the beta distribution with zero skewness, ranging from -2 to 0, so that &lt;math&gt;\hat{\nu}&lt;/math&gt; -and therefore the sample shape parameters- is positive, ranging from zero when the shape parameters approach zero and the excess kurtosis approaches -2, to infinity when the shape parameters approach infinity and the excess kurtosis approaches zero).

For non-zero sample skewness one needs to solve a system of two coupled equations. Since the skewness and the excess kurtosis are independent of the parameters &lt;math&gt;\hat{a}, \hat{c}&lt;/math&gt;, the parameters &lt;math&gt;\hat{\alpha}, \hat{\beta}&lt;/math&gt; can be uniquely determined from the sample skewness and the sample excess kurtosis, by solving the coupled equations with two known variables (sample skewness and sample excess kurtosis) and two unknowns (the shape parameters):

:&lt;math&gt;(\text{sample skewness})^2 = \frac{4(\hat{\beta}-\hat{\alpha})^2 (1 + \hat{\alpha} + \hat{\beta})}{\hat{\alpha} \hat{\beta} (2 + \hat{\alpha} + \hat{\beta})^2}&lt;/math&gt;
:&lt;math&gt;\text{sample excess kurtosis} =\frac{6}{3 + \hat{\alpha} + \hat{\beta}}\left(\frac{(2 + \hat{\alpha} + \hat{\beta})}{4} (\text{sample skewness})^2 - 1\right)&lt;/math&gt;
:&lt;math&gt;\text{ if (sample skewness)}^2-2&lt; \text{sample excess kurtosis}&lt; \tfrac{3}{2}(\text{sample skewness})^2&lt;/math&gt;

resulting in the following solution:&lt;ref name=Elderton1906/&gt;

: &lt;math&gt;\hat{\alpha}, \hat{\beta} = \frac{\hat{\nu}}{2} \left (1 \pm \frac{1}{ \sqrt{1+ \frac{16 (\hat{\nu} + 1)}{(\hat{\nu} + 2)^2(\text{sample skewness})^2}}} \right )&lt;/math&gt;

: &lt;math&gt;\text{ if sample skewness}\neq 0 \text{    and   } (\text{sample skewness})^2-2&lt; \text{sample excess kurtosis}&lt; \tfrac{3}{2} (\text{sample skewness})^2&lt;/math&gt;

Where one should take the solutions as follows: &lt;math&gt;\hat{\alpha}&gt;\hat{\beta}&lt;/math&gt; for (negative) sample skewness &lt; 0, and &lt;math&gt;\hat{\alpha}&lt;\hat{\beta}&lt;/math&gt; for (positive) sample skewness &gt; 0.

The accompanying plot shows these two solutions as surfaces in a space with horizontal axes of (sample excess kurtosis) and (sample squared skewness) and the shape parameters as the vertical axis. The surfaces are constrained by the condition that the sample excess kurtosis must be bounded by the sample squared skewness as stipulated in the above equation.  The two surfaces meet at the right edge defined by zero skewness. Along this right edge, both parameters are equal and the distribution is symmetric U-shaped for α = β &lt; 1, uniform for α = β = 1, upside-down-U-shaped for 1 &lt; α = β &lt; 2 and bell-shaped for α = β &gt; 2.  The surfaces also meet at the front (lower) edge defined by "the impossible boundary" line (excess kurtosis + 2 - skewness&lt;sup&gt;2&lt;/sup&gt; = 0). Along this front (lower) boundary both shape parameters approach zero, and the probability density is concentrated more at one end than the other end (with practically nothing in between), with probabilities &lt;math&gt;p=\tfrac{\beta}{\alpha + \beta}&lt;/math&gt; at the left end ''x'' = 0 and &lt;math&gt;q = 1-p = \tfrac{\alpha}{\alpha + \beta}  &lt;/math&gt; at the right end ''x'' = 1.  The two surfaces become further apart towards the rear edge.  At this rear edge the surface parameters are quite different from each other.  As remarked, for example, by Bowman and Shenton,&lt;ref name="BowmanShenton"&gt;{{cite journal|last=Bowman|first=K. O.|author1-link=K. O. Bowman|author2=Shenton, L. R.|title=The beta distribution, moment method, Karl Pearson and R.A. Fisher|journal=Far East J. Theo. Stat.|year=2007|volume=23|issue=2|pages=133–164| url=http://www.csm.ornl.gov/~bowman/fjts232.pdf }}&lt;/ref&gt; sampling in the neighborhood of the line (sample excess kurtosis - (3/2)(sample skewness)&lt;sup&gt;2&lt;/sup&gt; = 0) (the just-J-shaped portion of the rear edge where blue meets beige), "is dangerously near to chaos", because at that line the denominator of the expression above for the estimate ν = α + β becomes zero and hence ν approaches infinity as that line is approached.  Bowman and Shenton &lt;ref name="BowmanShenton" /&gt; write that "the higher moment parameters (kurtosis and skewness) are extremely fragile (near that line). However the mean and standard deviation are fairly reliable." Therefore, the problem is for the case of four parameter estimation for very skewed distributions such that the excess kurtosis approaches (3/2) times the square of the skewness.  This boundary line is produced by extremely skewed distributions with very large values of one of the parameters and very small values of the other parameter.  See section titled "Kurtosis bounded by the square of the skewness" for a numerical example and further comments about this rear edge boundary line (sample excess kurtosis - (3/2)(sample skewness)&lt;sup&gt;2&lt;/sup&gt; = 0).  As remarked by Karl Pearson himself &lt;ref name=Pearson1936/&gt; this issue may not be of much practical importance as this trouble arises only for very skewed J-shaped (or mirror-image J-shaped) distributions with very different values of shape parameters that are unlikely to occur much in practice).  The usual skewed-bell-shape distributions that occur in practice do not have this parameter estimation problem.

The remaining two parameters &lt;math&gt;\hat{a}, \hat{c}&lt;/math&gt; can be determined using the sample mean and the sample variance using a variety of equations.&lt;ref name="JKB"/&gt;&lt;ref name=Elderton1906/&gt;  One alternative is to calculate the support interval range &lt;math&gt;(\hat{c}-\hat{a})&lt;/math&gt; based on the sample variance and the sample kurtosis.  For this purpose one can solve, in terms of the range &lt;math&gt;(\hat{c}- \hat{a})&lt;/math&gt;, the equation expressing the excess kurtosis in terms of the sample variance, and the sample size ν (see section titled "Kurtosis" and "Alternative parametrizations, four parameters"):

:&lt;math&gt;\text{sample excess kurtosis} =\frac{6}{(3 + \hat{\nu})(2 + \hat{\nu})}\bigg(\frac{(\hat{c}- \hat{a})^2}{\text{(sample variance)}} - 6 - 5 \hat{\nu} \bigg)&lt;/math&gt;

to obtain:

:&lt;math&gt; (\hat{c}- \hat{a}) = \sqrt{\text{(sample variance)}}\sqrt{6+5\hat{\nu}+\frac{(2+\hat{\nu})(3+\hat{\nu})}{6}\text{(sample excess kurtosis)}}&lt;/math&gt;

Another alternative is to calculate the support interval range &lt;math&gt;(\hat{c}-\hat{a})&lt;/math&gt; based on the sample variance and the sample skewness.&lt;ref name=Elderton1906/&gt;  For this purpose one can solve, in terms of the range &lt;math&gt;(\hat{c}-\hat{a})&lt;/math&gt;, the equation expressing the squared skewness in terms of the sample variance, and the sample size ν (see section titled "Skewness" and "Alternative parametrizations, four parameters"):

:&lt;math&gt;(\text{sample skewness})^2 = \frac{4}{(2+\hat{\nu})^2}\bigg(\frac{(\hat{c}- \hat{a})^2}{ \text{(sample variance)}}-4(1+\hat{\nu})\bigg)&lt;/math&gt;

to obtain:&lt;ref name=Elderton1906/&gt;

:&lt;math&gt; (\hat{c}- \hat{a}) = \frac{\sqrt{\text{(sample variance)}}}{2}\sqrt{(2+\hat{\nu})^2(\text{sample skewness})^2+16(1+\hat{\nu})}&lt;/math&gt;

The remaining parameter can be determined from the sample mean and the previously obtained parameters: &lt;math&gt;(\hat{c}-\hat{a}), \hat{\alpha}, \hat{\nu} = \hat{\alpha}+\hat{\beta}&lt;/math&gt;:

:&lt;math&gt;  \hat{a} = (\text{sample mean}) -  \left(\frac{\hat{\alpha}}{\hat{\nu}}\right)(\hat{c}-\hat{a}) &lt;/math&gt;

and finally, of course, &lt;math&gt;\hat{c}= (\hat{c}- \hat{a}) + \hat{a}  &lt;/math&gt;.

In the above formulas one may take, for example, as estimates of the sample moments:

:&lt;math&gt;\begin{align}
\text{sample mean} &amp;=\overline{y} = \frac{1}{N}\sum_{i=1}^N Y_i \\
\text{sample variance} &amp;= \overline{v}_Y = \frac{1}{N-1}\sum_{i=1}^N (Y_i - \overline{y})^2 \\
\text{sample skewness} &amp;= G_1 = \frac{N}{(N-1)(N-2)} \frac{\sum_{i=1}^N (Y_i-\overline{y})^3}{\overline{v}_Y^{\frac{3}{2}} } \\
\text{sample excess kurtosis} &amp;= G_2 = \frac{N(N+1)}{(N-1)(N-2)(N-3)} \frac{\sum_{i=1}^N (Y_i - \overline{y})^4}{\overline{v}_Y^2} - \frac{3(N-1)^2}{(N-2)(N-3)}
\end{align}&lt;/math&gt;

The estimators ''G''&lt;sub&gt;1&lt;/sub&gt; for [[skewness|sample skewness]] and ''G''&lt;sub&gt;2&lt;/sub&gt; for [[kurtosis|sample kurtosis]] are used by [[DAP (software)|DAP]]/[[SAS System|SAS]], [[PSPP]]/[[SPSS]], and [[Microsoft Excel|Excel]].  However, they are not used by [[BMDP]] and (according to &lt;ref name="Joanes and Gill"/&gt;) they were not used by [[MINITAB]] in 1998. Actually, Joanes and Gill in their 1998 study&lt;ref name="Joanes and Gill"&gt;{{cite journal|last=Joanes|first=D. N.|author2=C. A. Gill|title=Comparing measures of sample skewness and kurtosis|journal=The Statistician|year=1998|volume=47|issue=Part 1|pages=183–189|doi=10.1111/1467-9884.00122}}&lt;/ref&gt;  concluded that the skewness and kurtosis estimators used in [[BMDP]] and in [[MINITAB]] (at that time) had smaller variance and mean-squared error in normal samples, but the skewness and kurtosis estimators used in  [[DAP (software)|DAP]]/[[SAS System|SAS]], [[PSPP]]/[[SPSS]], namely ''G''&lt;sub&gt;1&lt;/sub&gt; and ''G''&lt;sub&gt;2&lt;/sub&gt;, had smaller mean-squared error in samples from a very skewed distribution.  It is for this reason that we have spelled out "sample skewness", etc., in the above formulas, to make it explicit that the user should choose the best estimator according to the problem at hand, as the best estimator for skewness and kurtosis depends on the amount of skewness (as shown by Joanes and Gill&lt;ref name="Joanes and Gill"/&gt;).

===Maximum likelihood===

====Two unknown parameters====
[[File:Max (Joint Log Likelihood per N) for Beta distribution Maxima at alpha=beta=2 - J. Rodal.png|thumb|Max (joint log likelihood/''N'') for beta distribution maxima at ''&amp;alpha;''&amp;nbsp;=&amp;nbsp;''&amp;beta;''&amp;nbsp;=&amp;nbsp;2]]
[[File:Max (Joint Log Likelihood per N) for Beta distribution Maxima at alpha=beta= 0.25,0.5,1,2,4,6,8 - J. Rodal.png|thumb|Max (joint log likelihood/''N'') for Beta distribution maxima at ''&amp;alpha;''&amp;nbsp;=&amp;nbsp;''&amp;beta;''&amp;nbsp;&amp;isin;&amp;nbsp;{0.25,0.5,1,2,4,6,8}]]

As is also the case for [[maximum likelihood]] estimates for the [[gamma distribution]], the maximum likelihood estimates for the beta distribution do not have a general closed form solution for arbitrary values of the shape parameters. If ''X''&lt;sub&gt;1&lt;/sub&gt;, ..., ''X&lt;sub&gt;N&lt;/sub&gt;'' are independent random variables each having a beta distribution, the joint log likelihood function for ''N'' [[independent and identically distributed random variables|iid]] observations is:

:&lt;math&gt;\begin{align}
\ln\, \mathcal{L} (\alpha, \beta\mid X) &amp;= \sum_{i=1}^N \ln \left (\mathcal{L}_i (\alpha, \beta\mid X_i) \right )\\
&amp;= \sum_{i=1}^N \ln \left (f(X_i;\alpha,\beta) \right ) \\
&amp;= \sum_{i=1}^N \ln \left (\frac{X_i^{\alpha-1}(1-X_i)^{\beta-1}}{\Beta(\alpha,\beta)} \right ) \\
&amp;= (\alpha - 1)\sum_{i=1}^N \ln (X_i) + (\beta- 1)\sum_{i=1}^N  \ln (1-X_i) - N \ln \Beta(\alpha,\beta)
\end{align}&lt;/math&gt;

Finding the maximum with respect to a shape parameter involves taking the partial derivative with respect to the shape parameter and setting the expression equal to zero yielding the [[maximum likelihood]] estimator of the shape parameters:

:&lt;math&gt;\frac{\partial \ln \mathcal{L}(\alpha,\beta\mid X)}{\partial \alpha} = \sum_{i=1}^N \ln X_i -N\frac{\partial \ln \Beta(\alpha,\beta)}{\partial \alpha}=0&lt;/math&gt;
:&lt;math&gt;\frac{\partial \ln \mathcal{L}(\alpha,\beta\mid X)}{\partial \beta} = \sum_{i=1}^N  \ln (1-X_i)- N\frac{\partial \ln \mathrm{B}(\alpha,\beta)}{\partial \beta}=0&lt;/math&gt;

where:

:&lt;math&gt;\frac{\partial \ln \Beta(\alpha,\beta)}{\partial \alpha} = -\frac{\partial \ln \Gamma(\alpha+\beta)}{\partial \alpha}+ \frac{\partial \ln \Gamma(\alpha)}{\partial \alpha}+ \frac{\partial \ln \Gamma(\beta)}{\partial \alpha}=-\psi(\alpha + \beta) + \psi(\alpha) + 0&lt;/math&gt;
:&lt;math&gt;\frac{\partial \ln \Beta(\alpha,\beta)}{\partial \beta}= - \frac{\partial \ln \Gamma(\alpha+\beta)}{\partial \beta}+ \frac{\partial \ln \Gamma(\alpha)}{\partial \beta} + \frac{\partial \ln \Gamma(\beta)}{\partial \beta}=-\psi(\alpha + \beta) + 0 + \psi(\beta)&lt;/math&gt;

since the '''[[digamma function]]''' denoted ψ(α) is defined as the [[logarithmic derivative]] of the [[gamma function]]:&lt;ref name=Abramowitz/&gt;

:&lt;math&gt;\psi(\alpha) =\frac {\partial\ln \Gamma(\alpha)}{\partial \alpha}&lt;/math&gt;

To ensure that the values with zero tangent slope are indeed a maximum (instead of a saddle-point or a minimum) one has to also satisfy the condition that the curvature is negative.  This amounts to satisfying that the second partial derivative with respect to the shape parameters is negative

:&lt;math&gt;\frac{\partial^2\ln \mathcal{L}(\alpha,\beta\mid X)}{\partial \alpha^2}= -N\frac{\partial^2\ln \Beta(\alpha,\beta)}{\partial \alpha^2}&lt;0&lt;/math&gt;
:&lt;math&gt;\frac{\partial^2\ln \mathcal{L}(\alpha,\beta\mid X)}{\partial \beta^2} = -N\frac{\partial^2\ln \Beta(\alpha,\beta)}{\partial \beta^2}&lt;0&lt;/math&gt;

using the previous equations, this is equivalent to:

:&lt;math&gt;\frac{\partial^2\ln \Beta(\alpha,\beta)}{\partial \alpha^2} = \psi_1(\alpha)-\psi_1(\alpha + \beta) &gt; 0&lt;/math&gt;
:&lt;math&gt;\frac{\partial^2\ln \Beta(\alpha,\beta)}{\partial \beta^2} = \psi_1(\beta) -\psi_1(\alpha + \beta) &gt; 0&lt;/math&gt;

where the '''[[trigamma function]]''', denoted ''ψ''&lt;sub&gt;1&lt;/sub&gt;(''α''), is the second of the [[polygamma function]]s, and is defined as the derivative of the [[digamma]] function:

:&lt;math&gt;\psi_1(\alpha) = \frac{\partial^2\ln\Gamma(\alpha)}{\partial \alpha^2}=\, \frac{\partial\, \psi(\alpha)}{\partial \alpha}.&lt;/math&gt;

These conditions are equivalent to stating that the variances of the logarithmically transformed variables are positive, since:

:&lt;math&gt;\operatorname{var}[\ln (X)] = \operatorname{E}[\ln^2 (X)] - (\operatorname{E}[\ln (X)])^2 = \psi_1(\alpha) - \psi_1(\alpha + \beta) &lt;/math&gt;
:&lt;math&gt;\operatorname{var}[\ln (1-X)] = \operatorname{E}[\ln^2 (1-X)] - (\operatorname{E}[\ln (1-X)])^2 = \psi_1(\beta) - \psi_1(\alpha + \beta) &lt;/math&gt;

Therefore, the condition of negative curvature at a maximum is equivalent to the statements:

: &lt;math&gt;  \operatorname{var}[\ln (X)] &gt; 0&lt;/math&gt;
: &lt;math&gt;  \operatorname{var}[\ln (1-X)] &gt; 0&lt;/math&gt;

Alternatively, the condition of negative curvature at a maximum is also equivalent to stating that the following [[logarithmic derivative]]s of the [[geometric mean]]s ''G&lt;sub&gt;X&lt;/sub&gt;'' and ''G&lt;sub&gt;(1−X)&lt;/sub&gt;'' are positive, since:

: &lt;math&gt;\psi_1(\alpha) - \psi_1(\alpha + \beta) = \frac{\partial \ln G_X}{\partial \alpha} &gt; 0&lt;/math&gt;
: &lt;math&gt;\psi_1(\beta)  - \psi_1(\alpha + \beta) = \frac{\partial \ln G_{(1-X)}}{\partial \beta} &gt; 0&lt;/math&gt;

While these slopes are indeed positive, the other slopes are negative:

:&lt;math&gt;\frac{\partial\, \ln G_X}{\partial \beta}, \frac{\partial \ln G_{(1-X)}}{\partial \alpha} &lt; 0.&lt;/math&gt;

The slopes of the mean and the median with respect to ''α'' and ''β'' display similar sign behavior.

From the condition that at a maximum, the partial derivative with respect to the shape parameter equals zero, we obtain the following system of coupled [[maximum likelihood estimate]] equations (for the average log-likelihoods) that needs to be inverted to obtain the  (unknown) shape parameter estimates &lt;math&gt;\hat{\alpha},\hat{\beta}&lt;/math&gt; in terms of the (known) average of logarithms of the samples ''X''&lt;sub&gt;1&lt;/sub&gt;, ..., ''X&lt;sub&gt;N&lt;/sub&gt;'':&lt;ref name=JKB /&gt;

:&lt;math&gt;\begin{align}
\hat{\operatorname{E}}[\ln (X)] &amp;= \psi(\hat{\alpha}) - \psi(\hat{\alpha} + \hat{\beta})=\frac{1}{N}\sum_{i=1}^N \ln X_i =  \ln \hat{G}_X \\
\hat{\operatorname{E}}[\ln(1-X)] &amp;= \psi(\hat{\beta}) - \psi(\hat{\alpha} + \hat{\beta})=\frac{1}{N}\sum_{i=1}^N \ln (1-X_i)= \ln \hat{G}_{(1-X)}
\end{align}&lt;/math&gt;

where we recognize &lt;math&gt;\log \hat{G}_X&lt;/math&gt; as the logarithm of the sample [[geometric mean]] and &lt;math&gt;\log \hat{G}_{(1-X)}&lt;/math&gt; as the logarithm of the sample [[geometric mean]] based on (1&amp;nbsp;−&amp;nbsp;''X''), the mirror-image of&amp;nbsp;''X''. For &lt;math&gt;\hat{\alpha}=\hat{\beta}&lt;/math&gt;, it follows that  &lt;math&gt;\hat{G}_X=\hat{G}_{(1-X)} &lt;/math&gt;.

:&lt;math&gt;\begin{align}
\hat{G}_X &amp;= \prod_{i=1}^N (X_i)^{1/N} \\
\hat{G}_{(1-X)} &amp;= \prod_{i=1}^N (1-X_i)^{1/N}
\end{align}&lt;/math&gt;

These coupled equations containing [[digamma function]]s of the shape parameter estimates &lt;math&gt;\hat{\alpha},\hat{\beta}&lt;/math&gt; must be solved by numerical methods as done, for example, by Beckman et al.&lt;ref&gt;{{cite journal|last=Beckman|first=R. J.|author2=G. L. Tietjen|title=Maximum likelihood estimation for the beta distribution|journal=Journal of Statistical Computation and Simulation|year=1978|volume=7|issue=3-4|pages=253–258|doi=10.1080/00949657808810232}}&lt;/ref&gt; Gnanadesikan et al. give numerical solutions for a few cases.&lt;ref&gt;{{cite journal |last=Gnanadesikan |first=R.,Pinkham and Hughes|title=Maximum likelihood estimation of the parameters of the beta distribution from smallest order statistics |journal=Technometrics |year=1967|volume=9|pages=607–620 |doi=10.2307/1266199}}&lt;/ref&gt; [[Norman Lloyd Johnson|N.L.Johnson]] and [[Samuel Kotz|S.Kotz]]&lt;ref name=JKB /&gt; suggest that for "not too small" shape parameter estimates &lt;math&gt;\hat{\alpha},\hat{\beta}&lt;/math&gt;, the logarithmic approximation to the digamma function &lt;math&gt;\psi(\hat{\alpha}) \approx \ln(\hat{\alpha}-\tfrac{1}{2})&lt;/math&gt; may be used to obtain initial values for an iterative solution, since the equations resulting from this approximation can be solved exactly:

:&lt;math&gt;\ln \frac{\hat{\alpha} - \frac{1}{2}}{\hat{\alpha} + \hat{\beta} - \frac{1}{2}}  \approx  \ln \hat{G}_X &lt;/math&gt;
:&lt;math&gt;\ln \frac{\hat{\beta} - \frac{1}{2}}{\hat{\alpha} + \hat{\beta} - \frac{1}{2}}\approx \ln \hat{G}_{(1-X)} &lt;/math&gt;

which leads to the following solution for the initial values (of the estimate shape parameters in terms of the sample geometric means) for an iterative solution:

:&lt;math&gt;\hat{\alpha}\approx \tfrac{1}{2} + \frac{\hat{G}_{X}}{2(1-\hat{G}_X-\hat{G}_{(1-X)})} \text{ if } \hat{\alpha} &gt;1&lt;/math&gt;
:&lt;math&gt;\hat{\beta}\approx \tfrac{1}{2} + \frac{\hat{G}_{(1-X)}}{2(1-\hat{G}_X-\hat{G}_{(1-X)})} \text{ if } \hat{\beta} &gt; 1&lt;/math&gt;

Alternatively, the estimates provided by the method of moments can instead be used as initial values for an iterative solution of the maximum likelihood coupled equations in terms of the digamma functions.

When the distribution is required over a known interval other than [0, 1]  with random variable ''X'', say [''a'', ''c''] with random variable ''Y'', then replace ln(''X&lt;sub&gt;i&lt;/sub&gt;'') in the first equation with

:&lt;math&gt;\ln \frac{Y_i-a}{c-a},&lt;/math&gt;

and replace ln(1−''X&lt;sub&gt;i&lt;/sub&gt;'') in the second equation with

:&lt;math&gt;\ln \frac{c-Y_i}{c-a}&lt;/math&gt;

(see "Alternative parametrizations, four parameters" section below).

If one of the shape parameters is known, the problem is considerably simplified.  The following [[logit]] transformation can be used to solve for the unknown shape parameter (for skewed cases such that &lt;math&gt;\hat{\alpha}\neq\hat{\beta}&lt;/math&gt;, otherwise, if symmetric, both -equal- parameters are known when one is known):

:&lt;math&gt;\hat{\operatorname{E}} \left[\ln \left(\frac{X}{1-X} \right) \right]=\psi(\hat{\alpha}) - \psi(\hat{\beta})=\frac{1}{N}\sum_{i=1}^N \ln\frac{X_i}{1-X_i} =  \ln \hat{G}_X - \ln \left(\hat{G}_{(1-X)}\right) &lt;/math&gt;

This [[logit]] transformation is the logarithm of the transformation that divides the variable ''X'' by its mirror-image (''X''/(1 - ''X'') resulting in the "inverted beta distribution"  or [[beta prime distribution]] (also known as beta distribution of the second kind or [[Pearson distribution|Pearson's Type VI]]) with support [0, +∞). As previously discussed in the section "Moments of logarithmically transformed random variables," the [[logit]] transformation &lt;math&gt;\ln\frac{X}{1-X}&lt;/math&gt;, studied by Johnson,&lt;ref name=JohnsonLogInv/&gt; extends the finite support [0, 1] based on the original variable ''X'' to infinite support in both directions of the real line (−∞, +∞).

If, for example, &lt;math&gt;\hat{\beta}&lt;/math&gt; is known, the unknown parameter &lt;math&gt;\hat{\alpha}&lt;/math&gt; can be obtained in terms of the inverse&lt;ref name=invpsi.m&gt;{{cite web|last=Fackler |first=Paul|title=Inverse Digamma Function (Matlab)|url=http://hips.seas.harvard.edu/content/inverse-digamma-function-matlab|publisher=Harvard University School of Engineering and Applied Sciences|accessdate=2012-08-18}}&lt;/ref&gt; digamma function of the right hand side of this equation:

:&lt;math&gt;\psi(\hat{\alpha})=\frac{1}{N}\sum_{i=1}^N \ln\frac{X_i}{1-X_i} + \psi(\hat{\beta}) &lt;/math&gt;
:&lt;math&gt;\hat{\alpha}=(\operatorname{Inverse digamma})[\ln \hat{G}_X - \ln \hat{G}_{(1-X)} + \psi(\hat{\beta})] &lt;/math&gt;

In particular, if one of the shape parameters has a value of unity, for example for &lt;math&gt;\hat{\beta} = 1&lt;/math&gt; (the power function distribution with bounded support [0,1]), using the identity ψ(''x'' + 1) = ψ(''x'') + 1/''x'' in the equation &lt;math&gt;\psi(\hat{\alpha}) - \psi(\hat{\alpha} + \hat{\beta})= \ln \hat{G}_X&lt;/math&gt;, the maximum likelihood estimator for the unknown parameter &lt;math&gt;\hat{\alpha}&lt;/math&gt; is,&lt;ref name=JKB /&gt; exactly:

:&lt;math&gt;\hat{\alpha}= - \frac{1}{\frac{1}{N}\sum_{i=1}^N \ln X_i}= - \frac{1}{ \ln \hat{G}_X} &lt;/math&gt;

The beta has support [0, 1], therefore &lt;math&gt;\hat{G}_X &lt; 1&lt;/math&gt;, and hence &lt;math&gt;(-\ln \hat{G}_X) &gt;0&lt;/math&gt;, and therefore &lt;math&gt;\hat{\alpha} &gt;0.&lt;/math&gt;

In conclusion, the maximum likelihood estimates of the shape parameters of a beta distribution are (in general) a complicated function of the sample [[geometric mean]], and of the sample [[geometric mean]] based on ''(1−X)'', the mirror-image of ''X''.  One may ask, if the variance (in addition to the mean) is necessary to estimate two shape parameters with the method of moments, why is the (logarithmic or geometric) variance not necessary to estimate two shape parameters with the maximum likelihood method, for which only the geometric means suffice?  The answer is because the mean does not provide as much information as the geometric mean.  For a beta distribution with equal shape parameters ''α''&amp;nbsp;=&amp;nbsp;''β'', the mean is exactly 1/2, regardless of the value of the shape parameters, and therefore regardless of the value of the statistical dispersion (the variance).  On the other hand, the geometric mean of a beta distribution with equal shape parameters ''α''&amp;nbsp;=&amp;nbsp;''β'', depends on the value of the shape parameters, and therefore it contains more information.  Also, the geometric mean of a beta distribution does not satisfy the symmetry conditions satisfied by the mean, therefore, by employing both the geometric mean based on ''X'' and geometric mean based on (1&amp;nbsp;−&amp;nbsp;''X''), the maximum likelihood method is able to provide best estimates for both parameters ''α''&amp;nbsp;=&amp;nbsp;''β'', without need of employing the variance.

One can express the joint log likelihood per ''N'' [[independent and identically distributed random variables|iid]] observations in terms of the ''[[sufficient statistic]]s'' (the sample geometric means) as follows:

:&lt;math&gt;\frac{\ln \mathcal{L} (\alpha, \beta\mid X)}{N} = (\alpha - 1)\ln \hat{G}_X + (\beta- 1)\ln \hat{G}_{(1-X)}- \ln \Beta(\alpha,\beta).&lt;/math&gt;

We can plot the joint log likelihood per ''N'' observations for fixed values of the sample geometric means to see the behavior of the likelihood function as a function of the shape parameters α and β. In such a plot, the shape parameter estimators &lt;math&gt;\hat{\alpha},\hat{\beta}&lt;/math&gt; correspond to the maxima of the likelihood function. See the accompanying graph that shows that all the likelihood functions intersect at α = β = 1, which corresponds to the values of the shape parameters that give the maximum entropy (the maximum entropy occurs for shape parameters equal to unity: the uniform distribution).  It is evident from the plot that the likelihood function gives sharp peaks for values of the shape parameter estimators close to zero, but that for values of the shape parameters estimators greater than one, the likelihood function becomes quite flat, with less defined peaks.  Obviously, the maximum likelihood parameter estimation method for the beta distribution becomes less acceptable for larger values of the shape parameter estimators, as the uncertainty in the peak definition increases with the value of the shape parameter estimators.  One can arrive at the same conclusion by noticing that the expression for the curvature of the likelihood function is in terms of the geometric variances

:&lt;math&gt;\frac{\partial^2\ln \mathcal{L}(\alpha,\beta\mid X)}{\partial \alpha^2}= -\operatorname{var}[\ln X]&lt;/math&gt;
:&lt;math&gt;\frac{\partial^2\ln \mathcal{L}(\alpha,\beta\mid X)}{\partial \beta^2} = -\operatorname{var}[\ln (1-X)]&lt;/math&gt;

These variances (and therefore the curvatures) are much larger for small values of the shape parameter α and β. However, for shape parameter values α, β &gt; 1, the variances (and therefore the curvatures) flatten out.  Equivalently, this result follows from the [[Cramér–Rao bound]], since the [[Fisher information]] matrix components for the beta distribution are these logarithmic variances. The [[Cramér–Rao bound]] states that the [[variance]] of any ''unbiased'' estimator &lt;math&gt;\hat{\alpha}&lt;/math&gt; of α is bounded by the [[multiplicative inverse|reciprocal]] of the [[Fisher information]]:

:&lt;math&gt;\mathrm{var}(\hat{\alpha})\geq\frac{1}{\operatorname{var}[\ln X]}\geq\frac{1}{\psi_1(\hat{\alpha}) - \psi_1(\hat{\alpha} + \hat{\beta})}&lt;/math&gt;
:&lt;math&gt;\mathrm{var}(\hat{\beta}) \geq\frac{1}{\operatorname{var}[\ln (1-X)]}\geq\frac{1}{\psi_1(\hat{\beta}) - \psi_1(\hat{\alpha} + \hat{\beta})}&lt;/math&gt;

so the variance of the estimators increases with increasing α and β, as the logarithmic variances decrease.

Also one can express the joint log likelihood per ''N'' [[independent and identically distributed random variables|iid]] observations in terms of the [[digamma function]] expressions for the logarithms of the sample geometric means as follows:

:&lt;math&gt;\frac{\ln\, \mathcal{L} (\alpha, \beta\mid X)}{N} = (\alpha - 1)(\psi(\hat{\alpha}) - \psi(\hat{\alpha} + \hat{\beta}))+(\beta- 1)(\psi(\hat{\beta}) - \psi(\hat{\alpha} + \hat{\beta}))- \ln \Beta(\alpha,\beta)&lt;/math&gt;

this expression is identical to the negative of the cross-entropy (see section on "Quantities of information (entropy)").  Therefore, finding the maximum of the joint log likelihood of the shape parameters, per ''N'' [[independent and identically distributed random variables|iid]] observations, is identical to finding the minimum of the cross-entropy for the beta distribution, as a function of the shape parameters.

:&lt;math&gt;\frac{\ln\, \mathcal{L} (\alpha, \beta\mid X)}{N} = - H = -h - D_{\mathrm{KL}} = -\ln\Beta(\alpha,\beta)+(\alpha-1)\psi(\hat{\alpha})+(\beta-1)\psi(\hat{\beta})-(\alpha+\beta-2)\psi(\hat{\alpha}+\hat{\beta})&lt;/math&gt;

with the cross-entropy defined as follows:

:&lt;math&gt;H = \int_{0}^1 - f(X;\hat{\alpha},\hat{\beta}) \ln (f(X;\alpha,\beta)) \, {\rm d}X &lt;/math&gt;

====Four unknown parameters====
The procedure is similar to the one followed in the two unknown parameter case. If ''Y''&lt;sub&gt;1&lt;/sub&gt;, ..., ''Y&lt;sub&gt;N&lt;/sub&gt;'' are independent random variables each having a beta distribution with four parameters, the joint log likelihood function for ''N'' [[independent and identically distributed random variables|iid]] observations is:

:&lt;math&gt;\begin{align}
\ln\, \mathcal{L} (\alpha, \beta, a, c\mid Y) &amp;= \sum_{i=1}^N \ln\,\mathcal{L}_i (\alpha, \beta, a, c\mid Y_i)\\
&amp;= \sum_{i=1}^N \ln\,f(Y_i; \alpha, \beta, a, c) \\
&amp;= \sum_{i=1}^N \ln\,\frac{(Y_i-a)^{\alpha-1} (c-Y_i)^{\beta-1} }{(c-a)^{\alpha+\beta-1}\Beta(\alpha, \beta)}\\
&amp;= (\alpha - 1)\sum_{i=1}^N  \ln (Y_i - a) + (\beta- 1)\sum_{i=1}^N  \ln (c - Y_i)- N \ln \Beta(\alpha,\beta) - N (\alpha+\beta - 1) \ln (c - a)
\end{align}&lt;/math&gt;

Finding the maximum with respect to a shape parameter involves taking the partial derivative with respect to the shape parameter and setting the expression equal to zero yielding the [[maximum likelihood]] estimator of the shape parameters:

:&lt;math&gt;\frac{\partial \ln \mathcal{L} (\alpha, \beta, a, c\mid Y) }{\partial \alpha}= \sum_{i=1}^N  \ln (Y_i - a) - N(-\psi(\alpha + \beta) + \psi(\alpha))- N \ln (c - a)= 0&lt;/math&gt;
:&lt;math&gt;\frac{\partial \ln \mathcal{L} (\alpha, \beta, a, c\mid Y) }{\partial \beta} = \sum_{i=1}^N  \ln (c - Y_i) - N(-\psi(\alpha + \beta)  + \psi(\beta))- N \ln (c - a)= 0&lt;/math&gt;
:&lt;math&gt;\frac{\partial \ln \mathcal{L} (\alpha, \beta, a, c\mid Y) }{\partial a} = -(\alpha - 1) \sum_{i=1}^N  \frac{1}{Y_i - a} \,+ N (\alpha+\beta - 1)\frac{1}{c - a}= 0&lt;/math&gt;
:&lt;math&gt;\frac{\partial \ln \mathcal{L} (\alpha, \beta, a, c\mid Y) }{\partial c} = (\beta- 1) \sum_{i=1}^N  \frac{1}{c - Y_i} \,- N (\alpha+\beta - 1) \frac{1}{c - a} = 0&lt;/math&gt;

these equations can be re-arranged as the following system of four coupled equations (the first two equations are geometric means and the second two equations are the harmonic means) in terms of the maximum likelihood estimates for the four parameters &lt;math&gt;\hat{\alpha}, \hat{\beta}, \hat{a}, \hat{c}&lt;/math&gt;:

:&lt;math&gt;\frac{1}{N}\sum_{i=1}^N  \ln \frac{Y_i - \hat{a}}{\hat{c}-\hat{a}} = \psi(\hat{\alpha})-\psi(\hat{\alpha} +\hat{\beta} )=  \ln \hat{G}_X&lt;/math&gt;
:&lt;math&gt;\frac{1}{N}\sum_{i=1}^N  \ln \frac{\hat{c} - Y_i}{\hat{c}-\hat{a}} =  \psi(\hat{\beta})-\psi(\hat{\alpha} + \hat{\beta})=  \ln \hat{G}_{1-X}&lt;/math&gt;
:&lt;math&gt;\frac{1}{\frac{1}{N}\sum_{i=1}^N  \frac{\hat{c} - \hat{a}}{Y_i - \hat{a}}} = \frac{\hat{\alpha} - 1}{\hat{\alpha}+\hat{\beta} - 1}=  \hat{H}_X&lt;/math&gt;
:&lt;math&gt;\frac{1}{\frac{1}{N}\sum_{i=1}^N  \frac{\hat{c} - \hat{a}}{\hat{c} - Y_i}} = \frac{\hat{\beta}- 1}{\hat{\alpha}+\hat{\beta} - 1} =  \hat{H}_{1-X}&lt;/math&gt;

with sample geometric means:

:&lt;math&gt;\hat{G}_X = \prod_{i=1}^{N} \left (\frac{Y_i - \hat{a}}{\hat{c}-\hat{a}} \right )^{\frac{1}{N}}&lt;/math&gt;
:&lt;math&gt;\hat{G}_{(1-X)} = \prod_{i=1}^{N} \left (\frac{\hat{c} - Y_i}{\hat{c}-\hat{a}} \right )^{\frac{1}{N}}&lt;/math&gt;

The parameters &lt;math&gt;\hat{a}, \hat{c}&lt;/math&gt; are embedded inside the geometric mean expressions in a nonlinear way (to the power 1/''N'').  This precludes, in general, a closed form solution, even for an initial value approximation for iteration purposes.  One alternative is to use as initial values for iteration the values obtained from the method of moments solution for the four parameter case.  Furthermore, the expressions for the harmonic means are well-defined only for &lt;math&gt;\hat{\alpha}, \hat{\beta} &gt; 1&lt;/math&gt;, which precludes a maximum likelihood solution for shape parameters less than unity in the four-parameter case. Fisher's information matrix for the four parameter case is [[Positive-definite matrix|positive-definite]] only for α, β &gt; 2 (for further discussion, see section on Fisher information matrix, four parameter case), for bell-shaped (symmetric or unsymmetric) beta distributions, with inflection points located to either side of the mode. The following Fisher information components (that represent the expectations of the curvature of the log likelihood function) have [[mathematical singularity|singularities]] at the following values:

:&lt;math&gt;\alpha = 2: \quad \operatorname{E} \left [- \frac{1}{N} \frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial a^2} \right ]= {\mathcal{I}}_{a, a}&lt;/math&gt;
:&lt;math&gt;\beta = 2: \quad \operatorname{E}\left [- \frac{1}{N} \frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial c^2} \right ] = {\mathcal{I}}_{c, c}&lt;/math&gt;
:&lt;math&gt;\alpha = 2: \quad \operatorname{E}\left [- \frac{1}{N}\frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial \alpha \partial a}\right ] = {\mathcal{I}}_{\alpha, a} &lt;/math&gt;
:&lt;math&gt;\beta = 1: \quad \operatorname{E}\left [- \frac{1}{N}\frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial \beta \partial c} \right ] = {\mathcal{I}}_{\beta, c}  &lt;/math&gt;

(for further discussion see section on Fisher information matrix). Thus, it is not possible to strictly carry on the maximum likelihood estimation for some well known distributions belonging to the four-parameter beta distribution family, like the [[continuous uniform distribution|uniform distribution]] (Beta(1, 1, ''a'', ''c'')), and the [[arcsine distribution]] (Beta(1/2, 1/2, ''a'', ''c'')).  [[Norman Lloyd Johnson|N.L.Johnson]] and [[Samuel Kotz|S.Kotz]]&lt;ref name=JKB /&gt; ignore the equations for the harmonic means and instead suggest "If a and c are unknown, and maximum likelihood estimators of ''a'', ''c'', α and β are required, the above procedure (for the two unknown parameter case, with ''X'' transformed as ''X'' = (''Y''&amp;nbsp;−&amp;nbsp;''a'')/(''c''&amp;nbsp;−&amp;nbsp;''a'')) can be repeated using a succession of trial values of ''a'' and ''c'', until the pair (''a'', ''c'') for which maximum likelihood (given ''a'' and ''c'') is as great as possible, is attained" (where, for the purpose of clarity, their notation for the parameters has been translated into the present notation).

===Fisher information matrix===
Let a random variable X have a probability density ''f''(''x'';''α''). The partial derivative with respect to the (unknown, and to be estimated) parameter α of the log [[likelihood function]] is called the [[score (statistics)|score]].  The second moment of the score is called the [[Fisher information]]:

:&lt;math&gt;\mathcal{I}(\alpha)=\operatorname{E} \left [\left (\frac{\partial}{\partial\alpha} \ln \mathcal{L}(\alpha\mid X) \right )^2 \right],&lt;/math&gt;

The [[expected value|expectation]] of the [[score (statistics)|score]] is zero, therefore the Fisher information is also the second moment centered on the mean of the score: the [[variance]] of the score.

If the log [[likelihood function]] is twice differentiable with respect to the parameter α, and under certain regularity conditions,&lt;ref name=Silvey&gt;{{cite book|last=Silvey|first=S.D.|title=Statistical Inference|year=1975|publisher=Chapman and Hal|location=page 40|isbn=978-0412138201}}&lt;/ref&gt; then the Fisher information may also be written as follows (which is often a more convenient form for calculation purposes):

:&lt;math&gt;\mathcal{I}(\alpha) = - \operatorname{E} \left [\frac{\partial^2}{\partial\alpha^2} \ln (\mathcal{L}(\alpha\mid X)) \right].&lt;/math&gt;

Thus, the Fisher information is the negative of the expectation of the second [[derivative]]  with respect to the parameter α of the log [[likelihood function]]. Therefore, Fisher information is a measure of the [[curvature]] of the log likelihood function of α. A low [[curvature]] (and therefore high [[Radius of curvature (mathematics)|radius of curvature]]), flatter log likelihood function curve has low Fisher information; while a log likelihood function curve with large [[curvature]] (and therefore low [[Radius of curvature (mathematics)|radius of curvature]]) has high Fisher information. When the Fisher information matrix is computed at the evaluates of the parameters ("the observed Fisher information matrix") it is equivalent to the replacement of the true log likelihood surface by a Taylor's series approximation, taken as far as the quadratic terms.&lt;ref name=EdwardsLikelihood&gt;{{cite book|last=Edwards|first=A. W. F.|title=Likelihood|year=1992 |publisher=The Johns Hopkins University Press|isbn=978-0801844430}}&lt;/ref&gt;  The word information, in the context of Fisher information, refers to information about the parameters. Information such as: estimation, sufficiency and properties of variances of estimators.  The [[Cramér–Rao bound]] states that the inverse of the Fisher information is a lower bound on the variance of any [[estimator]] of a parameter α:

:&lt;math&gt;\operatorname{var}[\hat\alpha] \geq \frac{1}{\mathcal{I}(\alpha)}.&lt;/math&gt;

The precision to which one can estimate the estimator of a parameter α is limited by the Fisher Information of the log likelihood function. The Fisher information is a measure of the minimum error involved in estimating a parameter of a distribution and it can be viewed as a measure of the resolving power of an experiment needed to discriminate between two alternative hypothesis of a parameter.&lt;ref name=Jaynes&gt;{{cite book|last=Jaynes|first=E.T.|title=Probability theory, the logic of science|year=2003|publisher=Cambridge University Press|isbn=978-0521592710}}&lt;/ref&gt;

When there are ''N'' parameters

:&lt;math&gt; \begin{bmatrix} \theta_1 \\ \theta_{2} \\ \dots \\ \theta_{N} \end{bmatrix},&lt;/math&gt;

then the Fisher information takes the form of an ''N''×''N'' [[positive semidefinite matrix|positive semidefinite]] [[symmetric matrix]], the Fisher Information Matrix, with typical element:

:&lt;math&gt;{(\mathcal{I}(\theta))}_{i, j}=\operatorname{E} \left [\left (\frac{\partial}{\partial\theta_i} \ln \mathcal{L} \right) \left(\frac{\partial}{\partial\theta_j} \ln \mathcal{L} \right) \right ].&lt;/math&gt;

Under certain regularity conditions,&lt;ref name=Silvey/&gt; the Fisher Information Matrix may also be written in the following form, which is often more convenient for computation:

:&lt;math&gt;{(\mathcal{I}(\theta))}_{i, j} = - \operatorname{E} \left [\frac{\partial^2}{\partial\theta_i \, \partial\theta_j} \ln (\mathcal{L}) \right ]\,.&lt;/math&gt;

With ''X''&lt;sub&gt;1&lt;/sub&gt;, ..., ''X&lt;sub&gt;N&lt;/sub&gt;'' [[iid]] random variables, an ''N''-dimensional "box" can be constructed with sides ''X''&lt;sub&gt;1&lt;/sub&gt;, ..., ''X&lt;sub&gt;N&lt;/sub&gt;''. Costa and Cover&lt;ref name=CostaCover&gt;{{cite book|last=Costa|first=Max, and Cover, Thomas|title=On the similarity of the entropy power inequality and the Brunn Minkowski inequality|date=September 1983|publisher=Tech.Report 48, Dept. Statistics, Stanford University |url=http://statistics.stanford.edu/~ckirby/techreports/NSF/COV%20NSF%2048.pdf}}&lt;/ref&gt;  show that the (Shannon) differential entropy ''h''(''X'') is related to the volume of the typical set (having the sample entropy close to the true entropy), while the Fisher information is related to the surface of this typical set.

====Two parameters====
For ''X''&lt;sub&gt;1&lt;/sub&gt;, ..., ''X''&lt;sub&gt;''N''&lt;/sub&gt; independent random variables each having a beta distribution parametrized with shape parameters ''α'' and ''β'', the joint log likelihood function for ''N'' [[independent and identically distributed random variables|iid]] observations is:

:&lt;math&gt;\ln (\mathcal{L} (\alpha, \beta\mid X) )= (\alpha - 1)\sum_{i=1}^N \ln X_i + (\beta- 1)\sum_{i=1}^N  \ln (1-X_i)- N \ln \Beta(\alpha,\beta) &lt;/math&gt;

therefore the joint log likelihood function per ''N'' [[independent and identically distributed random variables|iid]] observations is:

:&lt;math&gt;\frac{1}{N} \ln(\mathcal{L} (\alpha, \beta\mid X)) = (\alpha - 1)\frac{1}{N}\sum_{i=1}^N  \ln X_i + (\beta- 1)\frac{1}{N}\sum_{i=1}^N  \ln (1-X_i)-\, \ln \Beta(\alpha,\beta)&lt;/math&gt;

For the two parameter case, the Fisher information has 4 components: 2 diagonal and 2 off-diagonal. Since the Fisher information matrix is symmetric, one of these off diagonal components is independent. Therefore, the Fisher information matrix has 3 independent components (2 diagonal and 1 off diagonal).
 	
Aryal and Nadarajah&lt;ref name=Aryal&gt;{{cite journal|last=Aryal|first=Gokarna|author2=Saralees Nadarajah|title=Information matrix for beta distributions|journal=Serdica Mathematical Journal (Bulgarian Academy of Science)| year=2004| volume=30|pages=513–526|url=http://www.math.bas.bg/serdica/2004/2004-513-526.pdf}}&lt;/ref&gt; calculated Fisher's information matrix for the four-parameter case, from which the two parameter case can be obtained as follows:

:&lt;math&gt;- \frac{\partial^2\ln \mathcal{L}(\alpha,\beta\mid X)}{N\partial \alpha^2}=  \operatorname{var}[\ln (X)]= \psi_1(\alpha) - \psi_1(\alpha + \beta) ={\mathcal{I}}_{\alpha, \alpha}= \operatorname{E}\left [- \frac{\partial^2\ln \mathcal{L}(\alpha,\beta\mid X)}{N\partial \alpha^2} \right ] = \ln \operatorname{var}_{GX} &lt;/math&gt;
:&lt;math&gt;- \frac{\partial^2\ln \mathcal{L}(\alpha,\beta\mid X)}{N\,\partial \beta^2} = \operatorname{var}[\ln (1-X)] = \psi_1(\beta) - \psi_1(\alpha + \beta) ={\mathcal{I}}_{\beta, \beta}=  \operatorname{E}\left [- \frac{\partial^2\ln \mathcal{L}(\alpha,\beta\mid X)}{N\partial \beta^2} \right]= \ln \operatorname{var}_{G(1-X)} &lt;/math&gt;
:&lt;math&gt;- \frac{\partial^2\ln \mathcal{L}(\alpha,\beta\mid X)}{N \, \partial \alpha \, \partial \beta} = \operatorname{cov}[\ln X,\ln(1-X)]  = -\psi_1(\alpha+\beta) ={\mathcal{I}}_{\alpha, \beta}=  \operatorname{E}\left [- \frac{\partial^2\ln \mathcal{L}(\alpha,\beta\mid X)}{N\,\partial \alpha\,\partial \beta} \right] = \ln \operatorname{cov}_{G{X,(1-X)}}&lt;/math&gt;

Since the Fisher information matrix is symmetric

:&lt;math&gt; \mathcal{I}_{\alpha, \beta}= \mathcal{I}_{\beta, \alpha}= \ln \operatorname{cov}_{G{X,(1-X)}}&lt;/math&gt;

The Fisher information components are equal to the log geometric variances and log geometric covariance. Therefore, they can be expressed as '''[[trigamma function]]s''', denoted ψ&lt;sub&gt;1&lt;/sub&gt;(α),  the second of the [[polygamma function]]s, defined as the derivative of the [[digamma]] function:

:&lt;math&gt;\psi_1(\alpha) = \frac{d^2\ln\Gamma(\alpha)}{\partial\alpha^2}=\, \frac{\partial \psi(\alpha)}{\partial\alpha}. &lt;/math&gt;

These derivatives are also derived in the section titled "Parameter estimation", "Maximum likelihood", "Two unknown parameters," and plots of the log likelihood function are also shown in that section.  The section titled "Geometric variance and covariance" contains plots and further discussion of the Fisher information matrix components: the log geometric variances and log geometric covariance as a function of the shape parameters α and β.  The section titled "Other moments", "Moments of transformed random variables", "Moments of logarithmically transformed random variables" contains formulas for moments of logarithmically transformed random variables. Images for the Fisher information components &lt;math&gt;\mathcal{I}_{\alpha, \alpha}, \mathcal{I}_{\beta, \beta}&lt;/math&gt; and &lt;math&gt;\mathcal{I}_{\alpha, \beta}&lt;/math&gt; are shown in the section titled "Geometric variance".

The determinant of Fisher's information matrix is of interest (for example for the calculation of [[Jeffreys prior]] probability).  From the expressions for the individual components of the Fisher information matrix, it follows that the determinant of Fisher's (symmetric) information matrix for the beta distribution is:

:&lt;math&gt;\begin{align}
\det(\mathcal{I}(\alpha, \beta))&amp;= \mathcal{I}_{\alpha, \alpha} \mathcal{I}_{\beta, \beta}-\mathcal{I}_{\alpha, \beta} \mathcal{I}_{\alpha, \beta} \\[4pt]
&amp;=(\psi_1(\alpha) - \psi_1(\alpha + \beta))(\psi_1(\beta) - \psi_1(\alpha + \beta))-( -\psi_1(\alpha+\beta))( -\psi_1(\alpha+\beta))\\[4pt]
&amp;= \psi_1(\alpha)\psi_1(\beta)-( \psi_1(\alpha)+\psi_1(\beta))\psi_1(\alpha + \beta)\\[4pt]
\lim_{\alpha\to 0} \det(\mathcal{I}(\alpha, \beta)) &amp;=\lim_{\beta \to  0} \det(\mathcal{I}(\alpha, \beta)) = \infty\\[4pt]
\lim_{\alpha\to \infty} \det(\mathcal{I}(\alpha, \beta)) &amp;=\lim_{\beta \to  \infty} \det(\mathcal{I}(\alpha, \beta)) = 0
\end{align}&lt;/math&gt;

From [[Sylvester's criterion]] (checking whether the diagonal elements are all positive), it follows that the Fisher information matrix for the two parameter case is [[Positive-definite matrix|positive-definite]] (under the standard condition that the shape parameters are positive ''α''&amp;nbsp;&gt;&amp;nbsp;0 and&amp;nbsp;''β''&amp;nbsp;&gt;&amp;nbsp;0).

====Four parameters====
[[File:Fisher Information I(a,a) for alpha=beta vs range (c-a) and exponent alpha=beta - J. Rodal.png|thumb|Fisher Information ''I''(''a'',''a'') for ''α''&amp;nbsp;=&amp;nbsp;''β'' vs range (''c''&amp;nbsp;−&amp;nbsp;''a'') and exponent&amp;nbsp;''α''&amp;nbsp;=&amp;nbsp;''β'']]
[[File:Fisher Information I(alpha,a) for alpha=beta, vs. range (c - a) and exponent alpha=beta - J. Rodal.png|thumb|Fisher Information ''I''(''α'',''a'') for ''α''&amp;nbsp;=&amp;nbsp;''β'', vs. range (''c''&amp;nbsp;−&amp;nbsp;''a'') and exponent ''α''&amp;nbsp;=&amp;nbsp;''β'']]

If ''Y''&lt;sub&gt;1&lt;/sub&gt;, ..., ''Y&lt;sub&gt;N&lt;/sub&gt;'' are independent random variables each having a beta distribution with four parameters: the exponents ''α'' and ''β'', and also ''a'' (the minimum of the distribution range), and ''c'' (the maximum of the distribution range) (section titled "Alternative parametrizations", "Four parameters"), with [[probability density function]]:

:&lt;math&gt;f(y; \alpha, \beta, a, c) = \frac{f(x;\alpha,\beta)}{c-a} =\frac{ \left (\frac{y-a}{c-a} \right )^{\alpha-1} \left (\frac{c-y}{c-a} \right)^{\beta-1} }{(c-a)B(\alpha, \beta)}=\frac{ (y-a)^{\alpha-1} (c-y)^{\beta-1} }{(c-a)^{\alpha+\beta-1}B(\alpha, \beta)}.&lt;/math&gt;

the joint log likelihood function per ''N'' [[independent and identically distributed random variables|iid]] observations is:

:&lt;math&gt;\frac{1}{N} \ln(\mathcal{L} (\alpha, \beta, a, c\mid Y))= \frac{\alpha -1}{N}\sum_{i=1}^N  \ln (Y_i - a) + \frac{\beta -1}{N}\sum_{i=1}^N  \ln (c - Y_i)- \ln \Beta(\alpha,\beta) - (\alpha+\beta -1) \ln (c-a) &lt;/math&gt;

For the four parameter case, the Fisher information has 4*4=16 components.  It has 12 off-diagonal components = (4×4 total − 4 diagonal). Since the Fisher information matrix is symmetric, half of these components (12/2=6) are independent. Therefore, the Fisher information matrix has 6 independent off-diagonal + 4 diagonal = 10 independent components.  Aryal and Nadarajah&lt;ref name=Aryal/&gt; calculated Fisher's information matrix for the four parameter case as follows:

:&lt;math&gt;- \frac{1}{N} \frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial \alpha^2}=  \operatorname{var}[\ln (X)]= \psi_1(\alpha) - \psi_1(\alpha + \beta) = \mathcal{I}_{\alpha, \alpha}= \operatorname{E}\left [- \frac{1}{N} \frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial \alpha^2} \right ] = \ln (\operatorname{var_{GX}}) &lt;/math&gt;
:&lt;math&gt;-\frac{1}{N} \frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial \beta^2} = \operatorname{var}[\ln (1-X)] = \psi_1(\beta) - \psi_1(\alpha + \beta) ={\mathcal{I}}_{\beta, \beta}=  \operatorname{E} \left [- \frac{1}{N} \frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial \beta^2} \right ] = \ln(\operatorname{var_{G(1-X)}}) &lt;/math&gt;
:&lt;math&gt;-\frac{1}{N} \frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial \alpha\,\partial \beta} = \operatorname{cov}[\ln X,(1-X)]  = -\psi_1(\alpha+\beta) =\mathcal{I}_{\alpha, \beta}=  \operatorname{E} \left [- \frac{1}{N}\frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial \alpha \, \partial \beta} \right ] = \ln(\operatorname{cov}_{G{X,(1-X)}})&lt;/math&gt;

In the above expressions, the use of ''X'' instead of ''Y'' in the expressions var[ln(''X'')] = ln(var&lt;sub&gt;''GX''&lt;/sub&gt;) is ''not an error''. The expressions in terms of the log geometric variances and log geometric covariance occur as functions of the two parameter ''X'' ~ Beta(''α'', ''β'') parametrization because when taking the partial derivatives with respect to the exponents (''α'', ''β'') in the four parameter case, one obtains the identical expressions as for the two parameter case: these terms of the four parameter Fisher information matrix are independent of the minimum ''a'' and maximum ''c'' of the distribution's range. The only non-zero term upon double differentiation of the log likelihood function with respect to the exponents ''α'' and ''β'' is the second derivative of the log of the beta function: ln(B(''α'', ''β'')). This term is independent of the minimum ''a'' and maximum ''c'' of the distribution's range. Double differentiation of this term results in trigamma functions.  The sections titled "Maximum likelihood", "Two unknown parameters" and "Four unknown parameters" also show this fact.

The Fisher information for ''N'' [[i.i.d.]] samples is ''N'' times the individual Fisher information (eq. 11.279, page 394 of Cover and Thomas&lt;ref name="Cover and Thomas"/&gt;).  (Aryal and Nadarajah&lt;ref name=Aryal/&gt; take a single observation, ''N'' = 1, to calculate the following components of the Fisher information, which leads to the same result as considering the derivatives of the log likelihood per ''N'' observations. Moreover, below the erroneous expression for &lt;math&gt;{\mathcal{I}}_{a, a}&lt;/math&gt; in Aryal and Nadarajah has been corrected.)

:&lt;math&gt;\begin{align}
\alpha &gt; 2: \quad \operatorname{E}\left [- \frac{1}{N} \frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial a^2} \right ] &amp;= {\mathcal{I}}_{a, a}=\frac{\beta(\alpha+\beta-1)}{(\alpha-2)(c-a)^2} \\
\beta &gt; 2: \quad \operatorname{E}\left[-\frac{1}{N} \frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial c^2} \right ] &amp;= \mathcal{I}_{c, c} = \frac{\alpha(\alpha+\beta-1)}{(\beta-2)(c-a)^2} \\
\operatorname{E}\left[- \frac{1}{N} \frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial a \, \partial c} \right ] &amp;= {\mathcal{I}}_{a, c}  = \frac{(\alpha+\beta-1)}{(c-a)^2} \\
\alpha &gt; 1: \quad \operatorname{E}\left[- \frac{1}{N} \frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial \alpha \, \partial a} \right ] &amp;=\mathcal{I}_{\alpha, a}  = \frac{\beta}{(\alpha-1)(c-a)} \\
\operatorname{E}\left[- \frac{1}{N} \frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial \alpha \, \partial c} \right ] &amp;= {\mathcal{I}}_{\alpha, c} = \frac{1}{(c-a)} \\
\operatorname{E}\left[- \frac{1}{N} \frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial \beta \,\partial a} \right ] &amp;= {\mathcal{I}}_{\beta, a} = -\frac{1}{(c-a)} \\
\beta &gt; 1: \quad \operatorname{E}\left[- \frac{1}{N} \frac{\partial^2\ln \mathcal{L} (\alpha, \beta, a, c\mid Y)}{\partial \beta \, \partial c} \right ] &amp;= \mathcal{I}_{\beta, c}  = -\frac{\alpha}{(\beta-1)(c-a)}
\end{align}&lt;/math&gt;

The lower two diagonal entries of the Fisher information matrix, with respect to the parameter "a" (the minimum of the distribution's range): &lt;math&gt;\mathcal{I}_{a, a}&lt;/math&gt;, and with respect to the parameter "c" (the maximum of the distribution's range): &lt;math&gt;\mathcal{I}_{c, c}&lt;/math&gt; are only defined for exponents α &gt; 2 and β &gt; 2 respectively. The Fisher information matrix component &lt;math&gt;\mathcal{I}_{a, a}&lt;/math&gt; for the minimum "a" approaches infinity for exponent α approaching 2 from above, and the Fisher information matrix component &lt;math&gt;\mathcal{I}_{c, c}&lt;/math&gt; for the maximum "c" approaches infinity for exponent β approaching 2 from above.

The Fisher information matrix for the four parameter case does not depend on the individual values of the minimum "a" and the maximum "c", but only on the total range (''c''−''a'').  Moreover, the components of the Fisher information matrix that depend on the range (''c''−''a''), depend only through its inverse (or the square of the inverse), such that the Fisher information decreases for increasing range (''c''−''a'').

The accompanying images show the Fisher information components &lt;math&gt;\mathcal{I}_{a, a}&lt;/math&gt; and &lt;math&gt;\mathcal{I}_{\alpha, a}&lt;/math&gt;. Images for the Fisher information components &lt;math&gt;\mathcal{I}_{\alpha, \alpha}&lt;/math&gt; and &lt;math&gt;\mathcal{I}_{\beta, \beta}&lt;/math&gt; are shown in the section titled "Geometric variance".  All these Fisher information components look like a basin, with the "walls" of the basin being located at low values of the parameters.

The following four-parameter-beta-distribution Fisher information components can be expressed in terms of the two-parameter: ''X'' ~ Beta(α, β) expectations of the transformed ratio ((1-''X'')/''X'') and of its mirror image (''X''/(1-''X'')), scaled by the range (''c''−''a''), which may be helpful for interpretation:

:&lt;math&gt;\mathcal{I}_{\alpha, a} =\frac{\operatorname{E} \left[\frac{1-X}{X} \right ]}{c-a}= \frac{\beta}{(\alpha-1)(c-a)} \text{ if }\alpha &gt; 1&lt;/math&gt;
:&lt;math&gt;\mathcal{I}_{\beta, c} = -\frac{\operatorname{E} \left [\frac{X}{1-X} \right ]}{c-a}=- \frac{\alpha}{(\beta-1)(c-a)}\text{ if }\beta&gt; 1&lt;/math&gt;

These are also the expected values of the "inverted beta distribution" or [[beta prime distribution]] (also known as beta distribution of the second kind or [[Pearson distribution|Pearson's Type VI]]) &lt;ref name=JKB/&gt; and its mirror image, scaled by the range (''c''&amp;nbsp;−&amp;nbsp;''a'').

Also, the following Fisher information components can be expressed in terms of the harmonic (1/X) variances or of variances based on the ratio transformed variables ((1-X)/X) as follows:

:&lt;math&gt;\begin{align}
\alpha &gt; 2: \quad \mathcal{I}_{a,a} &amp;=\operatorname{var} \left [\frac{1}{X} \right] \left (\frac{\alpha-1}{c-a} \right )^2 =\operatorname{var} \left [\frac{1-X}{X} \right ] \left (\frac{\alpha-1}{c-a} \right)^2 = \frac{\beta(\alpha+\beta-1)}{(\alpha-2)(c-a)^2} \\
\beta &gt; 2: \quad \mathcal{I}_{c, c} &amp;= \operatorname{var} \left [\frac{1}{1-X} \right ] \left (\frac{\beta-1}{c-a} \right )^2 = \operatorname{var} \left [\frac{X}{1-X} \right ] \left (\frac{\beta-1}{c-a} \right )^2  =\frac{\alpha(\alpha+\beta-1)}{(\beta-2)(c-a)^2}  \\
\mathcal{I}_{a, c} &amp;=\operatorname{cov} \left [\frac{1}{X},\frac{1}{1-X} \right ]\frac{(\alpha-1)(\beta-1)}{(c-a)^2}  = \operatorname{cov} \left [\frac{1-X}{X},\frac{X}{1-X} \right ] \frac{(\alpha-1)(\beta-1)}{(c-a)^2} =\frac{(\alpha+\beta-1)}{(c-a)^2}
\end{align}&lt;/math&gt;

See section "Moments of linearly transformed, product and inverted random variables" for these expectations.

The determinant of Fisher's information matrix is of interest (for example for the calculation of [[Jeffreys prior]] probability).  From the expressions for the individual components, it follows that the determinant of Fisher's (symmetric) information matrix for the beta distribution with four parameters is:

:&lt;math&gt;\begin{align}
\det(\mathcal{I}(\alpha,\beta,a,c)) = {} &amp; -\mathcal{I}_{a,c}^2 \mathcal{I}_{\alpha,a} \mathcal{I}_{\alpha,\beta }+\mathcal{I}_{a,a} \mathcal{I}_{a,c} \mathcal{I}_{\alpha,c} \mathcal{I}_{\alpha ,\beta}+\mathcal{I}_{a,c}^2 \mathcal{I}_{\alpha ,\beta}^2 -\mathcal{I}_{a,a} \mathcal{I}_{c,c} \mathcal{I}_{\alpha,\beta}^2\\
&amp; {} -\mathcal{I}_{a,c} \mathcal{I}_{\alpha,a} \mathcal{I}_{\alpha ,c} \mathcal{I}_{\beta,a}+\mathcal{I}_{a,c}^2 \mathcal{I}_{\alpha ,\alpha} \mathcal{I}_{\beta,a}+2 \mathcal{I}_{c,c} \mathcal{I}_{\alpha,a} \mathcal{I}_{\alpha,\beta} \mathcal{I}_{\beta,a}\\
&amp; {}-2\mathcal{I}_{a,c} \mathcal{I}_{\alpha ,c} \mathcal{I}_{\alpha,\beta} \mathcal{I}_{\beta ,a}+\mathcal{I}_{\alpha ,c}^2 \mathcal{I}_{\beta ,a}^2-\mathcal{I}_{c,c} \mathcal{I}_{\alpha,\alpha} \mathcal{I}_{\beta ,a}^2+\mathcal{I}_{a,c} \mathcal{I}_{\alpha ,a}^2 \mathcal{I}_{\beta ,c}\\
&amp; {}-\mathcal{I}_{a,a} \mathcal{I}_{a,c} \mathcal{I}_{\alpha ,\alpha } \mathcal{I}_{\beta ,c}-\mathcal{I}_{a,c} \mathcal{I}_{\alpha ,a} \mathcal{I}_{\alpha ,\beta } \mathcal{I}_{\beta ,c}+\mathcal{I}_{a,a} \mathcal{I}_{\alpha ,c} \mathcal{I}_{\alpha ,\beta } \mathcal{I}_{\beta ,c}\\
&amp; {}-\mathcal{I}_{\alpha ,a} \mathcal{I}_{\alpha ,c} \mathcal{I}_{\beta ,a} \mathcal{I}_{\beta ,c}+\mathcal{I}_{a,c} \mathcal{I}_{\alpha ,\alpha } \mathcal{I}_{\beta ,a} \mathcal{I}_{\beta ,c}-\mathcal{I}_{c,c} \mathcal{I}_{\alpha ,a}^2 \mathcal{I}_{\beta ,\beta }\\
&amp; {}+2 \mathcal{I}_{a,c} \mathcal{I}_{\alpha ,a} \mathcal{I}_{\alpha, c} \mathcal{I}_{\beta ,\beta }-\mathcal{I}_{a,a} \mathcal{I}_{\alpha ,c}^2 \mathcal{I}_{\beta ,\beta }-\mathcal{I}_{a,c}^2 \mathcal{I}_{\alpha ,\alpha } \mathcal{I}_{\beta ,\beta }+\mathcal{I}_{a,a} \mathcal{I}_{c,c} \mathcal{I}_{\alpha ,\alpha } \mathcal{I}_{\beta ,\beta }\text{ if }\alpha, \beta&gt; 2
\end{align}&lt;/math&gt;

Using [[Sylvester's criterion]] (checking whether the diagonal elements are all positive), and since diagonal components &lt;math&gt;{\mathcal{I}}_{a, a}&lt;/math&gt; and &lt;math&gt;{\mathcal{I}}_{c, c}&lt;/math&gt; have [[Mathematical singularity|singularities]] at α=2 and β=2 it follows that the Fisher information matrix for the four parameter case is [[Positive-definite matrix|positive-definite]] for α&gt;2 and β&gt;2.  Since for α &gt; 2 and β &gt; 2 the beta distribution is (symmetric or unsymmetric) bell shaped, it follows that the Fisher information matrix is positive-definite only for bell-shaped (symmetric or unsymmetric) beta distributions, with inflection points located to either side of the mode. Thus, important well known distributions belonging to the four-parameter beta distribution family, like the parabolic distribution (Beta(2,2,a,c)) and the [[continuous uniform distribution|uniform distribution]] (Beta(1,1,a,c)) have Fisher information components (&lt;math&gt;\mathcal{I}_{a, a},\mathcal{I}_{c, c},\mathcal{I}_{\alpha, a},\mathcal{I}_{\beta, c}&lt;/math&gt;) that blow up (approach infinity) in the four-parameter case (although their Fisher information components are all defined for the two parameter case).  The four-parameter [[Wigner semicircle distribution]] (Beta(3/2,3/2,''a'',''c'')) and [[arcsine distribution]] (Beta(1/2,1/2,''a'',''c'')) have negative Fisher information determinants for the four-parameter case.

==Generating beta-distributed random variates==
If ''X'' and ''Y'' are independent, with &lt;math&gt;X \sim \Gamma(\alpha, \theta)&lt;/math&gt; and &lt;math&gt;Y \sim \Gamma(\beta, \theta)&lt;/math&gt; then

:&lt;math&gt;\frac{X}{X+Y} \sim \Beta(\alpha, \beta).&lt;/math&gt;

So one algorithm for generating beta variates is to generate &lt;math&gt;\frac{X}{X + Y}&lt;/math&gt;, where ''X'' is a [[Gamma distribution|gamma variate]] with parameters (α, 1) and ''Y'' is an independent gamma variate with parameters (β, 1).&lt;ref&gt;van der Waerden, B. L., "Mathematical Statistics", Springer, {{ISBN|978-3-540-04507-6}}.&lt;/ref&gt;  In fact, here &lt;math&gt;\frac{X}{X+Y}&lt;/math&gt; and &lt;math&gt;X+Y&lt;/math&gt; are independent, and &lt;math&gt;X+Y \sim \Gamma(\alpha + \beta, \theta)&lt;/math&gt;. If &lt;math&gt;Z \sim \Gamma(\gamma, \theta)&lt;/math&gt; and &lt;math&gt;Z&lt;/math&gt; is independent of &lt;math&gt;X&lt;/math&gt; and &lt;math&gt;Y&lt;/math&gt;, then &lt;math&gt;\frac{X+Y}{X+Y+Z} \sim \Beta(\alpha+\beta,\gamma)&lt;/math&gt; and &lt;math&gt;\frac{X+Y}{X+Y+Z}&lt;/math&gt; is independent of &lt;math&gt;\frac{X}{X+Y}&lt;/math&gt;. This shows that the product of independent &lt;math&gt;\Beta(\alpha,\beta)&lt;/math&gt; and &lt;math&gt;\Beta(\alpha+\beta,\gamma)&lt;/math&gt; random variables is a &lt;math&gt;\Beta(\alpha,\beta+\gamma)&lt;/math&gt; random variable.

Also, the ''k''th [[order statistic]] of ''n'' [[Uniform distribution (continuous)|uniformly distributed]] variates is &lt;math&gt;\Beta(k, n+1-k)&lt;/math&gt;, so an alternative if α and β are small integers is to generate α + β − 1 uniform variates and choose the α-th smallest.&lt;ref name=David1/&gt;

Another way to generate the Beta distribution is by [[Pólya urn model]]. According to this [http://www.tc.umn.edu/~horte005/docs/Dirichletdistribution.pdf method], one start with an "urn" with α "black" balls and β "white" balls and draw uniformly with replacement. Every trial an additional ball is added according to the color of the last ball which was drawn. Asymptotically, the proportion of black and white balls will be distributed according to the Beta distribution, where each repetition of the experiment will produce a different value.

==Related distributions==

===Transformations===
* If ''X'' ~ Beta(''α'', ''β'') then 1 − ''X'' ~ Beta(''β'', ''α'') [[Mirror image|mirror-image]] symmetry
* If ''X'' ~ Beta(''α'', ''β'') then &lt;math&gt;\tfrac{X}{1-X} \sim {\beta^{'}}(\alpha,\beta)&lt;/math&gt;. The [[beta prime distribution]], also called "beta distribution of the second kind".
* If ''X'' ~ Beta(''n''/2, ''m''/2) then &lt;math&gt;\tfrac{mX}{n(1-X)} \sim F(n,m)&lt;/math&gt; (assuming ''n'' &gt; 0 and ''m'' &gt; 0), the [[F-distribution|Fisher–Snedecor F distribution]].
* If &lt;math&gt;X \sim \operatorname{Beta}\left(1+\lambda\tfrac{m-\min}{\max-\min}, 1 + \lambda\tfrac{\max-m}{\max-\min}\right)&lt;/math&gt; then min + ''X''(max − min) ~ PERT(min, max, ''m'', ''λ'') where ''PERT'' denotes a [[PERT distribution]] used in [[PERT]] analysis, and ''m''=most likely value.&lt;ref name=NewPERT&gt;Herrerías-Velasco, José Manuel and Herrerías-Pleguezuelo, Rafael and René van Dorp, Johan. (2011). Revisiting the PERT mean and Variance. European Journal of Operational Research (210), p. 448&amp;ndash;451.&lt;/ref&gt; Traditionally&lt;ref name=Malcolm /&gt; ''λ'' = 4 in PERT analysis.
* If ''X'' ~ Beta(1, ''β'') then ''X'' ~ [[Kumaraswamy distribution]] with parameters (1, ''β'')
* If ''X'' ~ Beta(''α'', 1) then ''X'' ~ [[Kumaraswamy distribution]] with parameters (''α'', 1)
* If ''X'' ~ Beta(''α'', 1) then −ln(''X'') ~ Exponential(''α'')

===Special and limiting cases===
[[File:Random Walk example.svg|thumb|Example of eight realizations of a random walk in one dimension starting at 0: the probability for the time of the last visit to the origin is distributed as Beta(1/2, 1/2)]]
[[File:Arcsin density.svg|thumb|Beta(1/2, 1/2): The [[arcsine distribution]] probability density was proposed by [[Harold Jeffreys]] to represent uncertainty for a [[Bernoulli distribution|Bernoulli]] or a [[binomial distribution]] in [[Bayesian inference]], and is now commonly referred to as [[Jeffreys prior]]: ''p''&lt;sup&gt;−1/2&lt;/sup&gt;(1&amp;nbsp;−&amp;nbsp;''p'')&lt;sup&gt;−1/2&lt;/sup&gt;. This distribution also appears in several [[random walk]] fundamental theorems]]

* Beta(1, 1) ~ [[uniform distribution (continuous)|U(0, 1)]].
* If ''X'' ~ Beta(3/2, 3/2) and ''r'' &gt; 0 then 2''rX''&amp;nbsp;−&amp;nbsp;''r'' ~ [[Wigner semicircle distribution]].
* Beta(1/2, 1/2) is equivalent to the [[arcsine distribution]]. This distribution is also [[Jeffreys prior]] probability for the [[Bernoulli distribution|Bernoulli]] and  [[binomial distribution]]s . The arcsine probability density is a distribution that appears in several random walk fundamental theorems. In a fair coin toss [[random walk]], the probability for the time of the last visit to the origin is distributed as an (U-shaped) [[arcsine distribution]].&lt;ref name=Feller/&gt;&lt;ref name=WillyFeller1&gt;{{cite book|last=Feller|first=William|title=An Introduction to Probability Theory and Its Applications, Vol. 1, 3rd Edition|year=1968|isbn=978-0471257080}}&lt;/ref&gt;  In a two-player fair-coin-toss game, a player is said to be in the lead if the random walk (that started at the origin) is above the origin.  The most probable number of times that a given player will be in the lead, in a game of length 2''N'', is not ''N''.  On the contrary, ''N'' is the least likely number of times that the player will be in the lead. The most likely number of times in the lead is 0 or 2''N'' (following the [[arcsine distribution]]).
* &lt;math&gt;\lim_{n \to \infty} n \operatorname{Beta}(1,n) =  \operatorname{Exponential}(1)&lt;/math&gt;  the [[exponential distribution]]
* &lt;math&gt;\lim_{n \to \infty} n \operatorname{Beta}(k,n) = \operatorname{Gamma}(k,1)&lt;/math&gt; the [[gamma distribution]]

===Derived from other distributions===
* The ''k''th [[order statistic]] of a sample of size ''n'' from the [[Uniform distribution (continuous)|uniform distribution]] is a beta random variable, ''U''&lt;sub&gt;(''k'')&lt;/sub&gt; ~ Beta(''k'', ''n''+1−''k'').&lt;ref name=David1/&gt;
* If ''X'' ~ Gamma(α, θ) and ''Y'' ~ Gamma(β, θ) are independent, then &lt;math&gt;\tfrac{X}{X+Y} \sim \operatorname{Beta}(\alpha, \beta)\,&lt;/math&gt;.
* If &lt;math&gt;X \sim \chi^2(\alpha)\,&lt;/math&gt; and &lt;math&gt;Y \sim \chi^2(\beta)\,&lt;/math&gt; are independent, then &lt;math&gt;\tfrac{X}{X+Y} \sim \operatorname{Beta}(\tfrac{\alpha}{2}, \tfrac{\beta}{2})&lt;/math&gt;.
* If ''X'' ~ U(0, 1) and ''α'' &gt; 0 then ''X''&lt;sup&gt;1/''α''&lt;/sup&gt; ~ Beta(''α'', 1). The power function distribution.

===Combination with other distributions===
* ''X'' ~ Beta(''α'', ''β'') and ''Y'' ~ F(2''β'',2''α'') then  &lt;math&gt;\Pr(X \leq \tfrac \alpha {\alpha+\beta x}) = \Pr(Y \geq x)\,&lt;/math&gt; for all ''x'' &gt; 0.

===Compounding with other distributions===
* If ''p'' ~ Beta(α, β) and ''X'' ~ Bin(''k'', ''p'') then ''X'' ~ [[beta-binomial distribution]]
* If ''p'' ~ Beta(α, β) and ''X'' ~ NB(''r'', ''p'') then ''X'' ~ [[beta negative binomial distribution]]

===Generalisations===
* The [[Dirichlet distribution]] is a multivariate generalization of the beta distribution. Univariate marginals of the Dirichlet distribution have a beta distribution.  The beta distribution is [[Conjugate prior|conjugate]] to the binomial and Bernoulli distributions in exactly the same way as the [[Dirichlet distribution]] is conjugate to the [[multinomial distribution]] and [[categorical distribution]].
* The [[Pearson distribution#The Pearson type I distribution|Pearson type I distribution]] is identical to the beta distribution (except for arbitrary shifting and re-scaling that can also be accomplished with the four parameter parametrization of the beta distribution).
* &lt;math&gt;\operatorname{Beta}(\alpha, \beta) = \lim_{\delta \to 0}{\rm NonCentralBeta}(\alpha,\beta,\delta)&lt;/math&gt; the [[noncentral beta distribution]]
* The [[generalized beta distribution]] is a five-parameter distribution family which has the beta distribution as a special case.
* The [[matrix variate beta distribution]] is a distribution for [[positive-definite matrices]].

==Applications==

===Order statistics===
{{Main|Order statistic}}
The beta distribution has an important application in the theory of [[order statistic]]s. A basic result is that the distribution of the ''k''th smallest of a sample of size ''n'' from a continuous [[Uniform distribution (continuous)|uniform distribution]] has a beta distribution.&lt;ref name=David1&gt;David, H. A., Nagaraja, H. N. (2003) ''Order Statistics'' (3rd Edition). Wiley, New Jersey pp 458. {{ISBN|0-471-38926-9}}&lt;/ref&gt; This result is summarized as:

:&lt;math&gt;U_{(k)} \sim \operatorname{Beta}(k,n+1-k).&lt;/math&gt;

From this, and application of the theory related to the [[probability integral transform]], the distribution of any individual order statistic from any [[continuous distribution]] can be derived.&lt;ref name=David1/&gt;

===Rule of succession===
{{Main|Rule of succession}}
A classic application of the beta distribution is the [[rule of succession]], introduced in the 18th century by [[Pierre-Simon Laplace]]&lt;ref name=Laplace&gt;{{cite book|last=Laplace|first=Pierre Simon, marquis de|title=A philosophical essay on probabilities|year=1902|publisher=New York : J. Wiley ; London : Chapman &amp; Hall|isbn=1-60206-328-1|url=https://archive.org/details/philosophicaless00lapliala}}&lt;/ref&gt;  in the course of treating the [[sunrise problem]].  It states that, given ''s'' successes in ''n'' [[conditional independence|conditionally independent]] [[Bernoulli trial]]s with probability ''p,'' that the estimate of the expected value in the next trial is &lt;math&gt;\frac{s+1}{n+2}&lt;/math&gt;.  This estimate is the expected value of the posterior distribution over ''p,'' namely Beta(''s''+1, ''n''−''s''+1), which is given by [[Bayes' rule]] if one assumes a uniform prior probability over ''p'' (i.e., Beta(1, 1)) and then observes that ''p'' generated ''s'' successes in ''n'' trials.  Laplace's rule of succession has been criticized by prominent scientists.  R. T. Cox described Laplace's application of the rule of succession to the [[sunrise problem]] (&lt;ref name=CoxRT&gt;{{cite book|last=Cox|first=Richard T.|title=Algebra of Probable Inference|year=1961|publisher=The Johns Hopkins University Press|isbn=978-0801869822}}&lt;/ref&gt; p.&amp;nbsp;89) as "a travesty of the proper use of the principle."  Keynes remarks  (&lt;ref name=KeynesTreatise&gt;{{cite book|last=Keynes|first=John Maynard|title=A Treatise on Probability: The Connection Between Philosophy and the History of Science|origyear=1921|year=2010|publisher=Wildside Press|isbn=978-1434406965}}&lt;/ref&gt; Ch.XXX, p.&amp;nbsp;382)  "indeed this is so foolish a theorem that to entertain it is discreditable."  Karl Pearson&lt;ref name=PearsonRuleSuccession&gt;{{cite journal|last=Pearson|first=Karl|title=On the Influence of Past Experience on Future Expectation|journal=Philosophical Magazine|year=1907|volume=6|issue=13|pages=365–378}}&lt;/ref&gt;  showed that the probability that the next (''n''&amp;nbsp;+&amp;nbsp;1) trials will be successes, after n successes in n trials, is only 50%, which has been considered too low by scientists like Jeffreys and unacceptable as a representation of the scientific process of experimentation to test a proposed scientific law.  As pointed out by Jeffreys (&lt;ref name=Jeffreys/&gt; p.&amp;nbsp;128) (crediting [[C. D. Broad]]&lt;ref name=BroadMind&gt;{{cite journal|last=Broad|first=C. D.|title=On the relation between induction and probability|journal=MIND, a quarterly review of Psychology and Philosophy|date=October 1918|volume=27 (New Series)|issue=108|pages=389–404|jstor=2249035}}&lt;/ref&gt; ) Laplace's rule of succession establishes a high probability of success ((n+1)/(n+2)) in the next trial, but only a moderate probability (50%) that a further sample (n+1) comparable in size will be equally successful.  As pointed out by Perks,&lt;ref name=Perks&gt;{{cite journal|last=Perks|first=Wilfred|title=Some observations on inverse probability including a new indifference rule|journal=Journal of the Institute of Actuaries [JIA]|date=January 1947|volume=73|pages=285–334|url=http://www.actuaries.org.uk/research-and-resources/documents/some-observations-inverse-probability-including-new-indifference-ru}}&lt;/ref&gt; "The rule of succession itself is hard to accept. It assigns a probability to the next trial which implies the assumption that the actual run observed is an average run and that we are always at the end of an average run. It would, one would think, be more reasonable to assume that we were in the middle of an average run. Clearly a higher value for both probabilities is necessary if they are to accord with reasonable belief." These problems with Laplace's rule of succession motivated Haldane, Perks, Jeffreys and others to search for other forms of prior probability (see the next [[Beta distribution#Bayesian inference|section titled "Bayesian inference"]]).  According to Jaynes,&lt;ref name=Jaynes/&gt; the main problem with the rule of succession is that it is not valid when s=0 or s=n (see [[rule of succession]], for an analysis of its validity).

===Bayesian inference===
{{Main|Bayesian inference}}

[[File:Beta(1,1) Uniform distribution - J. Rodal.png|thumb|&lt;math&gt;Beta(1,1)&lt;/math&gt;: The [[uniform distribution (continuous)|uniform distribution]] probability density was proposed by [[Thomas Bayes]] to represent ignorance of prior probabilities in [[Bayesian inference]].  It does '''''not'''''  describe a state of complete ignorance, but the state of knowledge in which we have observed at least one success and one failure, and therefore ''we have prior knowledge that '''both''' states are physically possible''.]]

The use of Beta distributions in [[Bayesian inference]] is due to the fact that they provide a family of [[conjugate prior distribution|conjugate prior probability distribution]]s for [[binomial distribution|binomial]] (including [[Bernoulli distribution|Bernoulli]]) and [[geometric distribution]]s.  The domain of the beta distribution can be viewed as a probability, and in fact the beta distribution is often used to describe the distribution of a probability value ''p'':&lt;ref name=MacKay/&gt;

:&lt;math&gt;P(p;\alpha,\beta) = \frac{p^{\alpha-1}(1-p)^{\beta-1}}{\Beta(\alpha,\beta)}.&lt;/math&gt;

Examples of beta distributions used as prior probabilities to represent ignorance of prior parameter values in Bayesian inference are Beta(1,1), Beta(0,0) and Beta(1/2,1/2).

====Bayes' prior probability (Beta(1,1))====
The beta distribution achieves maximum differential entropy for Beta(1,1): the [[Uniform density|uniform]] probability density, for which all values in the domain of the distribution have equal density.  This uniform distribution Beta(1,1) was suggested ("with a great deal of doubt") by [[Thomas Bayes]]&lt;ref name="ThomasBayes"/&gt; as the prior probability distribution to express ignorance about the correct prior distribution. This prior distribution was adopted (apparently, from his writings, with little sign of doubt&lt;ref name=Laplace/&gt;) by [[Pierre-Simon Laplace]], and hence it was also known as the "Bayes-Laplace rule" or the "Laplace rule" of "[[inverse probability]]" in publications of the first half of the 20th century. In the later part of the 19th century and early part of the 20th century, scientists realized that the assumption of uniform "equal" probability density depended on the actual functions (for example whether a linear or a logarithmic scale was most appropriate) and parametrizations used.  In particular, the behavior near the ends of distributions with finite support (for example near ''x'' = 0, for a distribution with initial support at ''x'' = 0) required particular attention. Keynes (&lt;ref name=KeynesTreatise/&gt; Ch.XXX, p.&amp;nbsp;381) criticized the use of Bayes's uniform prior probability (Beta(1,1)) that all values between zero and one are equiprobable, as follows: "Thus experience, if it shows anything, shows that there is a very marked clustering of statistical ratios in the neighborhoods of zero and unity, of those for positive theories and for correlations between positive qualities in the neighborhood of zero, and of those for negative theories and for correlations between negative qualities in the neighborhood of unity. "

===={{Anchor|Haldane prior}}Haldane's prior probability (Beta(0,0))====
[[File:Beta distribution for alpha and beta approaching zero - J. Rodal.png|thumb|&lt;math&gt;Beta(0,0)&lt;/math&gt;: The Haldane prior probability expressing total ignorance about prior information, where we are not even sure whether it is physically possible for an experiment to yield either a success or a failure. As α, β → 0, the beta distribution approaches a two-point [[Bernoulli distribution]] with all probability density concentrated at each [[Dirac delta function]] end, at 0 and 1, and nothing in between. A coin-toss: one face of the coin being at 0 and the other face being at 1. ]]

The Beta(0,0) distribution was proposed by [[J.B.S. Haldane]],&lt;ref&gt;{{cite journal|last=Haldane|first=J.B.S.|title=A note on inverse probability|journal=Mathematical Proceedings of the Cambridge Philosophical Society|year=1932|volume=28|pages=55–61|url=http://journals.cambridge.org/action/displayAbstract?aid=1733860|doi=10.1017/s0305004100010495|bibcode=1932PCPS...28...55H}}&lt;/ref&gt; who suggested that the prior probability representing complete uncertainty should be proportional to ''p''&lt;sup&gt;−1&lt;/sup&gt;(1−''p'')&lt;sup&gt;−1&lt;/sup&gt;. The function ''p''&lt;sup&gt;−1&lt;/sup&gt;(1−''p'')&lt;sup&gt;−1&lt;/sup&gt; can be viewed as the limit of the numerator of the beta distribution as both shape parameters approach zero: α, β → 0. The Beta function (in the denominator of the beta distribution) approaches infinity, for both parameters approaching zero, α, β → 0. Therefore, ''p''&lt;sup&gt;−1&lt;/sup&gt;(1−''p'')&lt;sup&gt;−1&lt;/sup&gt; divided by the Beta function approaches a 2-point [[Bernoulli distribution]] with equal probability 1/2 at each [[Dirac delta function]] end, at 0 and 1, and nothing in between, as α, β → 0. A coin-toss: one face of the coin being at 0 and the other face being at 1.  The Haldane prior probability distribution Beta(0,0) is an "[[improper prior]]" because its integration (from 0 to 1) fails to strictly converge to 1 due to the [[Dirac delta function]] singularities at each end. However, this is not an issue for computing posterior probabilities unless the sample size is very small.  Furthermore, Zellner&lt;ref name=Zellner&gt;{{cite book|last=Zellner |first=Arnold|title=An Introduction to Bayesian Inference in Econometrics|year=1971|publisher=Wiley-Interscience|isbn=978-0471169376}}&lt;/ref&gt; points out that on the [[log-odds]] scale, (the [[logit]] transformation ln(''p''/1−''p'')), the Haldane prior is the uniformly flat prior. The fact that a uniform prior probability on the [[logit]] transformed variable ln(''p''/1−''p'') (with domain (-∞, ∞)) is equivalent to the Haldane prior on the domain [0, 1] was pointed out by [[Harold Jeffreys]] in the first edition (1939) of his book Theory of Probability (&lt;ref name=Jeffreys/&gt; p.&amp;nbsp;123).  Jeffreys writes "Certainly if we take the Bayes-Laplace rule right up to the extremes we are led to results that do not correspond to anybody's way of thinking. The (Haldane) rule d''x''/(''x''(1−''x'')) goes too far the other way.  It would lead to the conclusion that if a sample is of one type with respect to some property there is a probability 1 that the whole population is of that type."  The fact that "uniform" depends on the parametrization, led Jeffreys to seek a form of prior that would be invariant under different parametrizations.

====Jeffreys' prior probability (Beta(1/2,1/2) for a Bernoulli or for a binomial distribution)====
{{Main|Jeffreys prior}}

[[File:Jeffreys prior probability for the beta distribution - J. Rodal.png|thumb|[[Jeffreys prior]] probability for the beta distribution: the square root of the determinant of [[Fisher's information]] matrix: &lt;math&gt;\scriptstyle\sqrt{\det(\mathcal{I}(\alpha, \beta))} = \sqrt{\psi_1(\alpha)\psi_1(\beta)-( \psi_1(\alpha)+\psi_1(\beta) )\psi_1(\alpha + \beta)}&lt;/math&gt; is a function of the [[trigamma function]] ψ&lt;sub&gt;1&lt;/sub&gt; of shape parameters α, β]]

[[File:Beta distribution for 3 different prior probability functions - J. Rodal.png|thumb|Posterior Beta densities with samples having success = "s", failure = "f" of ''s''/(''s'' + ''f'') = 1/2, and ''s'' + ''f'' = {3,10,50}, based on 3 different prior probability functions: Haldane (Beta(0,0), Jeffreys (Beta(1/2,1/2)) and Bayes (Beta(1,1)). The image shows that there is little difference between the priors for the posterior with sample size of 50 (with more pronounced peak near ''p''&amp;nbsp;=&amp;nbsp;1/2). Significant differences appear for very small sample sizes (the flatter distribution for sample size of&amp;nbsp;3)]]

[[File:Beta distribution for 3 different prior probability functions, skewed case - J. Rodal.png|thumb|Posterior Beta densities with samples having success = "s", failure = "f" of ''s''/(''s'' + ''f'') = 1/4, and ''s'' + ''f'' &amp;isin; {3,10,50}, based on three different prior probability functions: Haldane (Beta(0,0), Jeffreys (Beta(1/2,1/2)) and Bayes (Beta(1,1)). The image shows that there is little difference between the priors for the posterior with sample size of 50 (with more pronounced peak near ''p'' = 1/4).  Significant differences appear for very small sample sizes (the very skewed distribution for the degenerate case of sample size&amp;nbsp;=&amp;nbsp;3, in this degenerate and unlikely case the Haldane prior results in a reverse "J" shape with mode at ''p''&amp;nbsp;=&amp;nbsp;0 instead of ''p''&amp;nbsp;=&amp;nbsp;1/4.  If there is sufficient [[Sample (statistics)|sampling data]], the three priors of Bayes (Beta(1,1)), Jeffreys (Beta(1/2,1/2)) and Haldane (Beta(0,0)) should yield similar [[posterior probability|''posterior'' probability]] densities.]]

[[File:Beta distribution for 3 different prior probability functions, skewed case sample size = (4,12,40) - J. Rodal.png|thumb|Posterior Beta densities with samples having success = ''s'', failure = ''f'' of ''s''/(''s'' + ''f'') = 1/4, and ''s'' + ''f'' &amp;isin; {4,12,40}, based on three different prior probability functions: Haldane (Beta(0,0), Jeffreys (Beta(1/2,1/2)) and Bayes (Beta(1,1)). The image shows that there is little difference between the priors for the posterior with sample size of 40 (with more pronounced peak near ''p''&amp;nbsp;=&amp;nbsp;1/4). Significant differences appear for very small sample sizes]]

[[Harold Jeffreys]]&lt;ref name=Jeffreys&gt;{{cite book|last=Jeffreys|first=Harold|title=Theory of Probability|year=1998|publisher=Oxford University Press, 3rd edition|isbn=978-0198503682}}&lt;/ref&gt;&lt;ref name=JeffreysPRIOR&gt;{{cite journal|last=Jeffreys|first=Harold|title=An Invariant Form for the Prior Probability in Estimation Problems|journal=Proceedings of the Royal Society|date=September 1946|volume=186|series=A 24|issue=1007|pages=453–461|doi=10.1098/rspa.1946.0056|bibcode=1946RSPSA.186..453J}}&lt;/ref&gt; proposed to use an [[uninformative prior]] probability measure that should be [[Parametrization#Parametrization invariance|invariant under reparameterization]]: proportional to the square root of the [[determinant]] of [[Fisher's information]] matrix.  For the [[Bernoulli distribution]], this can be shown as follows: for a coin that is "heads" with probability ''p'' ∈ [0, 1] and is "tails" with probability 1 − ''p'', for a given (H,T) ∈ {(0,1), (1,0)} the probability is ''p&lt;sup&gt;H&lt;/sup&gt;''(1 − ''p'')&lt;sup&gt;''T''&lt;/sup&gt;.  Since ''T'' = 1 − ''H'', the [[Bernoulli distribution]] is ''p&lt;sup&gt;H&lt;/sup&gt;''(1 − ''p'')&lt;sup&gt;1 − ''H''&lt;/sup&gt;. Considering ''p'' as the only parameter, it follows that the log likelihood for the Bernoulli distribution is

:&lt;math&gt;\ln  \mathcal{L} (p\mid H) = H \ln(p)+ (1-H) \ln(1-p).&lt;/math&gt;

The Fisher information matrix has only one component (it is a scalar, because there is only one parameter: ''p''), therefore:

:&lt;math&gt;\begin{align}
\sqrt{\mathcal{I}(p)} &amp;= \sqrt{\operatorname{E}\!\left[ \left( \frac{d}{dp} \ln(\mathcal{L} (p\mid H)) \right)^2\right]} \\[6pt]
&amp;= \sqrt{\operatorname{E}\!\left[ \left( \frac{H}{p} - \frac{1-H}{1-p}\right)^2 \right]} \\[6pt]
&amp;= \sqrt{p^1 (1-p)^0 \left( \frac{1}{p} - \frac{0}{1-p}\right)^2 + p^0 (1-p)^1 \left(\frac{0}{p} - \frac{1}{1-p}\right)^2} \\
&amp;= \frac{1}{\sqrt{p(1-p)}}.
\end{align}&lt;/math&gt;

Similarly, for the [[Binomial distribution]] with ''n'' [[Bernoulli trials]], it can be shown that

:&lt;math&gt;\sqrt{\mathcal{I}(p)}= \frac{\sqrt{n}}{\sqrt{p(1-p)}}.&lt;/math&gt;

Thus, for the [[Bernoulli distribution|Bernoulli]], and [[Binomial distribution]]s, [[Jeffreys prior]] is proportional to &lt;math&gt;\scriptstyle \frac{1}{\sqrt{p(1-p)}}&lt;/math&gt;, which happens to be proportional to a beta distribution with domain variable ''x'' = ''p'', and shape parameters α = β = 1/2, the [[arcsine distribution]]:

:&lt;math&gt;Beta(\tfrac{1}{2}, \tfrac{1}{2}) = \frac{1}{\pi \sqrt{p(1-p)}}.&lt;/math&gt;

It will be shown in the next section that the normalizing constant for Jeffreys prior is immaterial to the final result because the normalizing constant cancels out in Bayes theorem for the posterior probability.  Hence Beta(1/2,1/2) is used as the Jeffreys prior for both Bernoulli and binomial distributions. As shown in the next section, when using this expression as a prior probability times the likelihood in [[Bayes theorem]], the posterior probability turns out to be a beta distribution. It is important to realize, however, that Jeffreys prior is proportional to &lt;math&gt;\scriptstyle \frac{1}{\sqrt{p(1-p)}}&lt;/math&gt; for the Bernoulli and binomial distribution, but not for the beta distribution.  Jeffreys prior for the beta distribution is given by the determinant of Fisher's information for the beta distribution, which, as shown in the [[Beta distribution#Fisher information matrix|section titled "Fisher information matrix"]]  is a function of the [[trigamma function]] ψ&lt;sub&gt;1&lt;/sub&gt; of shape parameters α and β as follows:

:&lt;math&gt; \begin{align}
\sqrt{\det(\mathcal{I}(\alpha, \beta))} &amp;= \sqrt{\psi_1(\alpha)\psi_1(\beta)-(\psi_1(\alpha)+\psi_1(\beta))\psi_1(\alpha + \beta)} \\
\lim_{\alpha\to 0} \sqrt{\det(\mathcal{I}(\alpha, \beta))} &amp;=\lim_{\beta \to 0} \sqrt{\det(\mathcal{I}(\alpha, \beta))} = \infty\\
\lim_{\alpha\to \infty} \sqrt{\det(\mathcal{I}(\alpha, \beta))} &amp;=\lim_{\beta \to \infty} \sqrt{\det(\mathcal{I}(\alpha, \beta))} = 0
\end{align}&lt;/math&gt;

As previously discussed, Jeffreys prior for the Bernoulli and binomial distributions is proportional to the [[arcsine distribution]] Beta(1/2,1/2), a one-dimensional ''curve'' that looks like a basin as a function of the parameter ''p'' of the Bernoulli and binomial distributions. The walls of the basin are formed by ''p'' approaching the singularities at the ends ''p'' → 0 and ''p'' → 1, where Beta(1/2,1/2) approaches infinity. Jeffreys prior for the beta distribution is a ''2-dimensional surface'' (embedded in a three-dimensional space) that looks like a basin with only two of its walls meeting at the corner α = β = 0 (and missing the other two walls) as a function of the shape parameters α and β of the beta distribution. The two adjoining walls of this 2-dimensional surface are formed by the shape parameters α and β approaching the singularities (of the trigamma function) at α, β → 0. It has no walls for α, β → ∞ because in this case the determinant of Fisher's information matrix for the beta distribution approaches zero.

It will be shown in the next section that Jeffreys prior probability results in posterior probabilities (when multiplied by the binomial likelihood function) that are intermediate between the posterior probability results of the Haldane and Bayes prior probabilities.

Jeffreys prior may be difficult to obtain analytically, and for some cases it just doesn't exist (even for simple distribution functions like the asymmetric [[triangular distribution]]). Berger, Bernardo and Sun, in a 2009 paper&lt;ref name="BergerBernardoSun"&gt;{{cite journal|last=Berger|first=James |author2=Bernardo, Jose |author3=Sun, Dongchu|title=The formal definition of reference priors|journal=The Annals of Statistics|year=2009|volume=37|issue=2|pages=905–938|doi=10.1214/07-AOS587|url= http://projecteuclid.org/DPubS/Repository/1.0/Disseminate?view=body&amp;id=pdfview_1&amp;handle=euclid.aos/1236693154|arxiv=0904.0156}}&lt;/ref&gt;  defined a reference prior probability distribution that (unlike Jeffreys prior) exists for the asymmetric [[triangular distribution]]. They cannot obtain a closed-form expression for their reference prior, but numerical calculations show it to be nearly perfectly fitted by the (proper) prior

:&lt;math&gt; \operatorname{Beta}(\tfrac{1}{2}, \tfrac{1}{2}) \sim\frac{1}{\sqrt{\theta(1-\theta)}}&lt;/math&gt;

where θ is the vertex variable for the asymmetric triangular distribution with support [0, 1] (corresponding to the following parameter values in Wikipedia's article on the [[triangular distribution]]: vertex ''c'' = ''θ'', left end ''a'' = 0,and right end ''b'' = 1). Berger et al. also give a heuristic argument that Beta(1/2,1/2) could indeed be the exact Berger–Bernardo–Sun reference prior for the asymmetric triangular distribution. Therefore, Beta(1/2,1/2) not only is Jeffreys prior for the Bernoulli and binomial distributions, but also seems to be the Berger–Bernardo–Sun reference prior for the asymmetric triangular distribution (for which the Jeffreys prior does not exist), a distribution used in project management and [[PERT]] analysis to describe the cost and duration of project tasks.

Clarke and Barron&lt;ref&gt;{{cite journal|last=Clarke|first=Bertrand S.|author2=Andrew R. Barron|title=Jeffreys' prior is asymptotically least favorable under entropy risk|journal=Journal of Statistical Planning and Inference|year=1994|volume=41|pages=37–60|url=http://www.stat.yale.edu/~arb4/publications_files/jeffery's%20prior.pdf|doi=10.1016/0378-3758(94)90153-8}}&lt;/ref&gt; prove that, among continuous positive priors, Jeffreys prior (when it exists) asymptotically maximizes Shannon's [[mutual information]] between a sample of size n and the parameter, and therefore ''Jeffreys prior is the most uninformative prior'' (measuring information as Shannon information). The proof rests on an examination of the [[Kullback–Leibler distance]] between probability density functions for [[iid]] random variables.

====Effect of different prior probability choices on the posterior beta distribution====
If samples are drawn from the population of a random variable ''X'' that result in ''s'' successes and ''f'' failures in "n" [[Bernoulli trial]]s ''n''&amp;nbsp;=&amp;nbsp;''s''&amp;nbsp;+&amp;nbsp;''f'', then the [[likelihood function]] for parameters ''s'' and ''f'' given ''x''&amp;nbsp;=&amp;nbsp;''p'' (the notation ''x''&amp;nbsp;=&amp;nbsp;''p'' in the expressions below will emphasize that the domain ''x'' stands for the value of the parameter ''p'' in the binomial distribution), is the following [[binomial distribution]]:

:&lt;math&gt;\mathcal{L}(s,f\mid x=p) = {s+f \choose s} x^s(1-x)^f = {n \choose s} x^s(1-x)^{n - s}. &lt;/math&gt;

If beliefs about [[prior probability]] information are reasonably well approximated by a beta distribution with parameters ''α''&amp;nbsp;Prior and ''β''&amp;nbsp;Prior, then:

:&lt;math&gt;{\operatorname{PriorProbability}}(x=p;\alpha \operatorname{Prior},\beta \operatorname{Prior}) = \frac{ x^{\alpha \operatorname{Prior}-1}(1-x)^{\beta \operatorname{Prior}-1}}{\Beta(\alpha \operatorname{Prior},\beta \operatorname{Prior})}&lt;/math&gt;

According to [[Bayes' theorem]] for a continuous event space, the [[posterior probability]] is given by the product of the [[prior probability]] and the likelihood function (given the evidence ''s'' and ''f''&amp;nbsp;=&amp;nbsp;''n''&amp;nbsp;−&amp;nbsp;''s''), normalized so that the area under the curve equals one, as follows:

:&lt;math&gt;\begin{align}
&amp; \operatorname{posterior probability}(x=p\mid s,n-s) \\[6pt]
= {} &amp; \frac{\operatorname{PriorProbability}(x=p;\alpha \operatorname{Prior},\beta \operatorname{Prior}) \mathcal{L}(s,f\mid x=p)} {\int_0^1\operatorname{PriorProbability}(x=p;\alpha \operatorname{Prior},\beta \operatorname{Prior}) \mathcal{L}(s,f\mid x=p) dx} \\[6pt]
= {} &amp; \frac{{{n \choose s} x^{s+\alpha \operatorname{Prior}-1}(1-x)^{n-s+\beta \operatorname{Prior}-1} / \Beta(\alpha \operatorname{Prior},\beta \operatorname{Prior})}}{\int_0^1 \left({n \choose s} x^{s+\alpha \operatorname{Prior}-1}(1-x)^{n-s+\beta \operatorname{Prior}-1} /\Beta(\alpha \operatorname{Prior}, \beta \operatorname{Prior})\right) dx} \\[6pt]
= {} &amp; \frac{x^{s+\alpha \operatorname{Prior}-1}(1-x)^{n-s+\beta \operatorname{Prior}-1}}{\int_0^1 \left(x^{s+\alpha \operatorname{Prior}-1}(1-x)^{n-s+\beta \operatorname{Prior}-1}\right) dx} \\[6pt]
= {} &amp; \frac{x^{s+\alpha \operatorname{Prior}-1}(1-x)^{n-s+\beta \operatorname{Prior}-1}}{\Beta(s+\alpha \operatorname{Prior},n-s+\beta \operatorname{Prior})}.
\end{align}&lt;/math&gt;

The [[binomial coefficient]]

:&lt;math&gt;{s+f \choose s}={n \choose s}=\frac{(s+f)!}{s! f!}=\frac{n!}{s!(n-s)!}&lt;/math&gt;

appears both in the numerator and the denominator of the posterior probability, and it does not depend on the integration variable ''x'', hence it cancels out, and it is irrelevant to the final result.  Similarly the normalizing factor for the prior probability, the beta function B(αPrior,βPrior) cancels out and it is immaterial to the final result. The same posterior probability result can be obtained if one uses an un-normalized prior

:&lt;math&gt;x^{\alpha \operatorname{Prior}-1}(1-x)^{\beta \operatorname{Prior}-1}&lt;/math&gt;

because the normalizing factors all cancel out. Several authors (including Jeffreys himself) thus use an un-normalized prior formula since the normalization constant cancels out.  The numerator of the posterior probability ends up being just the (un-normalized) product of the prior probability and the likelihood function, and the denominator is its integral from zero to one. The beta function in the denominator, B(''s''&amp;nbsp;+&amp;nbsp;''α''&amp;nbsp;Prior,&amp;nbsp;''n''&amp;nbsp;−&amp;nbsp;''s''&amp;nbsp;+&amp;nbsp;''β''&amp;nbsp;Prior), appears as a normalization constant to ensure that the total posterior probability integrates to unity.

The ratio ''s''/''n'' of the number of successes to the total number of trials is a [[sufficient statistic]] in the binomial case, which is relevant for the following results.

For the '''Bayes'''' prior probability (Beta(1,1)), the posterior probability is:

:&lt;math&gt;\operatorname{posterior probability}(p=x\mid s,f) = \frac{x^{s}(1-x)^{n-s}}{\Beta(s+1,n-s+1)}, \text{ with mean }=\frac{s+1}{n+2},\text{ (and mode }=\frac{s}{n}\text{ if } 0 &lt; s &lt; n).&lt;/math&gt;

For the '''Jeffreys'''' prior probability (Beta(1/2,1/2)), the posterior probability is:

:&lt;math&gt;\operatorname{posterior probability}(p=x\mid s,f) = {x^{s-\tfrac{1}{2}}(1-x)^{n-s-\frac{1}{2}} \over \Beta(s+\tfrac{1}{2},n-s+\tfrac{1}{2})} ,\text{ with mean } = \frac{s+\tfrac{1}{2}}{n+1},\text{ (and mode= }\frac{s-\tfrac{1}{2}}{n-1}\text{ if } \tfrac{1}{2} &lt; s &lt; n-\tfrac{1}{2}).&lt;/math&gt;

and for the '''Haldane''' prior probability (Beta(0,0)), the posterior probability is:

:&lt;math&gt;\operatorname{posterior probability}(p=x\mid s,f) = \frac{x^{s-1}(1-x)^{n-s-1}}{\Beta(s,n-s)}, \text{ with mean} = \frac{s}{n},\text{ (and mode= }\frac{s-1}{n-2}\text{ if } 1 &lt; s &lt; n -1).&lt;/math&gt;

From the above expressions it follows that for ''s''/''n''&amp;nbsp;=&amp;nbsp;1/2) all the above three prior probabilities result in the identical location for the posterior probability mean&amp;nbsp;=&amp;nbsp;mode&amp;nbsp;=&amp;nbsp;1/2.  For ''s''/''n''&amp;nbsp;&lt;&amp;nbsp;1/2, the mean of the posterior probabilities, using the following priors, are such that: mean for Bayes prior &gt;&amp;nbsp;mean for Jeffreys prior &gt;&amp;nbsp;mean for Haldane prior. For ''s''/''n''&amp;nbsp;&gt;&amp;nbsp;1/2 the order of these inequalities is reversed such that the Haldane prior probability results in the largest posterior mean. The ''Haldane'' prior probability Beta(0,0) results in a posterior probability density with ''mean'' (the expected value for the probability of success in the "next" trial) identical to the ratio ''s''/''n'' of the number of successes to the total number of trials. Therefore, the Haldane prior results in a posterior probability with expected value in the next trial equal to the maximum likelihood. The ''Bayes'' prior probability Beta(1,1) results in a posterior probability density with ''mode'' identical to the ratio ''s''/''n'' (the maximum likelihood).

In the case that 100% of the trials have been successful ''s''&amp;nbsp;=&amp;nbsp;''n'', the ''Bayes'' prior probability Beta(1,1) results in a posterior expected value equal to the rule of succession (''n''&amp;nbsp;+&amp;nbsp;1)/(''n''&amp;nbsp;+&amp;nbsp;2), while the Haldane prior Beta(0,0) results in a posterior expected value of 1 (absolute certainty of success in the next trial).  Jeffreys prior probability results in a posterior expected value equal to (''n''&amp;nbsp;+&amp;nbsp;1/2)/(''n''&amp;nbsp;+&amp;nbsp;1). Perks&lt;ref name=Perks/&gt; (p.&amp;nbsp;303) points out: "This provides a new rule of succession and expresses a 'reasonable' position to take up, namely, that after an unbroken run of n successes we assume a probability for the next trial equivalent to the assumption that we are about half-way through an average run, i.e. that we expect a failure once in (2''n''&amp;nbsp;+&amp;nbsp;2) trials. The Bayes–Laplace rule implies that we are about at the end of an average run or that we expect a failure once in (''n''&amp;nbsp;+&amp;nbsp;2) trials. The comparison clearly favours the new result (what is now called Jeffreys prior) from the point of view of 'reasonableness'."

Conversely, in the case that 100% of the trials have resulted in failure (''s''&amp;nbsp;=&amp;nbsp;0), the ''Bayes'' prior probability Beta(1,1) results in a posterior expected value for success in the next trial equal to 1/(''n''&amp;nbsp;+&amp;nbsp;2), while the Haldane prior Beta(0,0) results in a posterior expected value of success in the next trial of 0 (absolute certainty of failure in the next trial). Jeffreys prior probability results in a posterior expected value for success in the next trial equal to (1/2)/(''n''&amp;nbsp;+&amp;nbsp;1), which Perks&lt;ref name=Perks/&gt; (p.&amp;nbsp;303) points out: "is a much more reasonably remote result than the Bayes-Laplace result&amp;nbsp;1/(''n''&amp;nbsp;+&amp;nbsp;2)".

Jaynes&lt;ref name=Jaynes/&gt; questions (for the uniform prior Beta(1,1)) the use of these formulas for the cases ''s''&amp;nbsp;=&amp;nbsp;0 or ''s''&amp;nbsp;=&amp;nbsp;''n'' because the integrals do not converge (Beta(1,1) is an improper prior for ''s''&amp;nbsp;=&amp;nbsp;0 or ''s''&amp;nbsp;=&amp;nbsp;''n''). In practice, the conditions 0&lt;s&lt;n necessary for a mode to exist between both ends for the Bayes prior are usually met, and therefore the Bayes prior (as long as 0&amp;nbsp;&lt;&amp;nbsp;''s''&amp;nbsp;&lt;&amp;nbsp;''n'') results in a posterior mode located between both ends of the domain.

As remarked in the section on the rule of succession, K. Pearson showed that after n successes in n trials the posterior probability (based on the Bayes Beta(1,1) distribution as the prior probability) that the next (''n''&amp;nbsp;+&amp;nbsp;1) trials will all be successes is exactly 1/2, whatever the value of&amp;nbsp;''n''. Based on the Haldane Beta(0,0) distribution as the prior probability, this posterior probability is 1 (absolute certainty that after n successes in ''n'' trials the next (''n''&amp;nbsp;+&amp;nbsp;1) trials will all be successes). Perks&lt;ref name=Perks/&gt; (p.&amp;nbsp;303) shows that, for what is now known as the Jeffreys prior, this probability is ((''n''&amp;nbsp;+&amp;nbsp;1/2)/(''n''&amp;nbsp;+&amp;nbsp;1))((''n''&amp;nbsp;+&amp;nbsp;3/2)/(''n''&amp;nbsp;+&amp;nbsp;2))...(2''n''&amp;nbsp;+&amp;nbsp;1/2)/(2''n''&amp;nbsp;+&amp;nbsp;1), which for ''n''&amp;nbsp;=&amp;nbsp;1,&amp;nbsp;2,&amp;nbsp;3 gives 15/24, 315/480, 9009/13440; rapidly approaching a limiting value of &lt;math&gt;1/\sqrt{2} = 0.70710678\ldots&lt;/math&gt; as n tends to infinity.  Perks remarks that what is now known as the Jeffreys prior: "is clearly more 'reasonable' than either the Bayes-Laplace result or the result on the (Haldane) alternative rule rejected by Jeffreys which gives certainty as the probability. It clearly provides a very much better correspondence with the process of induction. Whether it is 'absolutely' reasonable for the purpose, i.e. whether it is yet large enough, without the absurdity of reaching unity, is a matter for others to decide. But it must be realized that the result depends on the assumption of complete indifference and absence of knowledge prior to the sampling experiment."

Following are the variances of the posterior distribution obtained with these three prior probability distributions:

for the '''Bayes'''' prior probability (Beta(1,1)), the posterior variance is:

:&lt;math&gt;\text{variance} = \frac{(n-s+1)(s+1)}{(3+n)(2+n)^2},\text{ which for  } s=\frac{n}{2} \text{ results in variance} =\frac{1}{12+4n}&lt;/math&gt;

for the '''Jeffreys'''' prior probability (Beta(1/2,1/2)), the posterior variance is:

: &lt;math&gt;\text{variance} = \frac{(n-s+\frac{1}{2})(s+\frac{1}{2})}{(2+n)(1+n)^2} ,\text{ which for } s=\frac n 2 \text{ results in var} = \frac 1 {8 + 4n}&lt;/math&gt;

and for the '''Haldane''' prior probability (Beta(0,0)), the posterior variance is:

:&lt;math&gt;\text{variance} = \frac{(n-s)s}{(1+n)n^2}, \text{ which for  }s=\frac{n}{2}\text{ results in variance} =\frac{1}{4+4n}&lt;/math&gt;

So, as remarked by Silvey,&lt;ref name=Silvey/&gt; for large ''n'', the variance is small and hence the posterior distribution is highly concentrated, whereas the assumed prior distribution was very diffuse.  This is in accord with what one would hope for, as vague prior knowledge is transformed (through Bayes theorem) into a more precise posterior knowledge by an informative experiment.  For small ''n'' the Haldane Beta(0,0) prior results in the largest posterior variance while the Bayes Beta(1,1) prior results in the more concentrated posterior.  Jeffreys prior Beta(1/2,1/2) results in a posterior variance in between the other two.  As ''n'' increases, the variance rapidly decreases so that the posterior variance for all three priors converges to approximately the same value (approaching zero variance as ''n'' → ∞). Recalling the previous result that the ''Haldane'' prior probability Beta(0,0) results in a posterior probability density with ''mean'' (the expected value for the probability of success in the "next" trial) identical to the ratio s/n of the number of successes to the total number of trials, it follows from the above expression that also the ''Haldane'' prior Beta(0,0) results in a posterior with ''variance'' identical to the variance expressed in terms of the max. likelihood estimate s/n and sample size (in section titled "Variance"):

:&lt;math&gt;\text{variance} = \frac{\mu(1-\mu)}{1 + \nu}= \frac{(n-s)s}{(1+n) n^2} &lt;/math&gt;

with the mean ''μ''&amp;nbsp;=&amp;nbsp;''s''/''n'' and the sample size&amp;nbsp;''ν''&amp;nbsp;=&amp;nbsp;''n''.

In Bayesian inference, using a [[prior distribution]] Beta(''α''Prior,''β''Prior) prior to a binomial distribution is equivalent to adding (''α''Prior&amp;nbsp;−&amp;nbsp;1) pseudo-observations of "success" and (''β''Prior&amp;nbsp;−&amp;nbsp;1) pseudo-observations of "failure" to the actual number of successes and failures observed, then estimating the parameter ''p'' of the binomial distribution by the proportion of successes over both real- and pseudo-observations.  A uniform prior Beta(1,1) does not add (or subtract) any pseudo-observations since for Beta(1,1) it follows that (''α''Prior&amp;nbsp;−&amp;nbsp;1)&amp;nbsp;=&amp;nbsp;0 and (''β''Prior&amp;nbsp;−&amp;nbsp;1)&amp;nbsp;=&amp;nbsp;0. The Haldane prior Beta(0,0) subtracts one pseudo observation from each and Jeffreys prior Beta(1/2,1/2) subtracts 1/2 pseudo-observation of success and an equal number of failure. This subtraction has the effect of [[smoothing]] out the posterior distribution.  If the proportion of successes is not 50% (''s''/''n''&amp;nbsp;≠&amp;nbsp;1/2) values of ''α''Prior and ''β''Prior less than&amp;nbsp;1 (and therefore negative (''α''Prior&amp;nbsp;−&amp;nbsp;1) and (''β''Prior&amp;nbsp;−&amp;nbsp;1)) favor sparsity, i.e. distributions where the parameter ''p'' is closer to either 0 or&amp;nbsp;1.  In effect, values of ''α''Prior and ''β''Prior between 0 and 1, when operating together, function as a [[concentration parameter]].

The accompanying plots show the posterior probability density functions for sample sizes ''n''&amp;nbsp;&amp;isin;&amp;nbsp;{3,10,50}, successes ''s''&amp;nbsp;&amp;isin;&amp;nbsp;{''n''/2,''n''/4} and Beta(''α''Prior,''β''Prior)&amp;nbsp;&amp;isin;&amp;nbsp;{Beta(0,0),Beta(1/2,1/2),Beta(1,1)}. Also shown are the cases for ''n''&amp;nbsp;=&amp;nbsp;{4,12,40}, success ''s''&amp;nbsp;=&amp;nbsp;{''n''/4} and Beta(''α''Prior,''β''Prior)&amp;nbsp;&amp;isin;&amp;nbsp;{Beta(0,0),Beta(1/2,1/2),Beta(1,1)}. The first plot shows the symmetric cases, for successes ''s''&amp;nbsp;&amp;isin;&amp;nbsp;{n/2}, with mean&amp;nbsp;=&amp;nbsp;mode&amp;nbsp;=&amp;nbsp;1/2 and the second plot shows the skewed cases ''s''&amp;nbsp;&amp;isin;&amp;nbsp;{''n''/4}.  The images show that there is little difference between the priors for the posterior with sample size of 50 (characterized by a more pronounced peak near ''p''&amp;nbsp;=&amp;nbsp;1/2). Significant differences appear for very small sample sizes (in particular for the flatter distribution for the degenerate case of sample size&amp;nbsp;=&amp;nbsp;3). Therefore, the skewed cases, with successes ''s''&amp;nbsp;=&amp;nbsp;{''n''/4}, show a larger effect from the choice of prior, at small sample size, than the symmetric cases.  For symmetric distributions, the Bayes prior Beta(1,1) results in the most "peaky" and highest posterior distributions and the Haldane prior Beta(0,0) results in the flattest and lowest peak distribution.  The Jeffreys prior Beta(1/2,1/2) lies in between them.  For nearly symmetric, not too skewed distributions the effect of the priors is similar.  For very small sample size (in this case for a sample size of 3) and skewed distribution (in this example for ''s''&amp;nbsp;&amp;isin;&amp;nbsp;{''n''/4}) the Haldane prior can result in a reverse-J-shaped distribution with a singularity at the left end.  However, this happens only in degenerate cases (in this example ''n''&amp;nbsp;=&amp;nbsp;3 and hence ''s''&amp;nbsp;=&amp;nbsp;3/4&amp;nbsp;&lt;&amp;nbsp;1, a degenerate value because s should be greater than unity in order for the posterior of the Haldane prior to have a mode located between the ends, and because ''s''&amp;nbsp;=&amp;nbsp;3/4 is not an integer number, hence it violates the initial assumption of a binomial distribution for the likelihood) and it is not an issue in generic cases of reasonable sample size (such that the condition 1&amp;nbsp;&lt;&amp;nbsp;''s''&amp;nbsp;&lt;&amp;nbsp;''n''&amp;nbsp;−&amp;nbsp;1, necessary for a mode to exist between both ends, is fulfilled).

In Chapter 12 (p.&amp;nbsp;385) of his book, Jaynes&lt;ref name=Jaynes/&gt; asserts that the ''Haldane prior'' Beta(0,0) describes a ''prior state of knowledge of complete ignorance'', where we are not even sure whether it is physically possible for an experiment to yield either a success or a failure, while the ''Bayes (uniform) prior Beta(1,1) applies if'' one knows that ''both binary outcomes are possible''. Jaynes states: "''interpret the Bayes-Laplace (Beta(1,1)) prior as describing not a state of complete ignorance'', but the state of knowledge in which we have observed one success and one failure...once we have seen at least one success and one failure, then we know that the experiment is a true binary one, in the sense of physical possibility." Jaynes &lt;ref name=Jaynes/&gt; does not specifically discuss Jeffreys prior Beta(1/2,1/2) (Jaynes discussion of "Jeffreys prior" on pp.&amp;nbsp;181, 423 and on chapter 12 of Jaynes book&lt;ref name=Jaynes/&gt; refers instead to the improper, un-normalized, prior "1/''p''&amp;nbsp;''dp''" introduced by Jeffreys in the 1939 edition of his book,&lt;ref name=Jeffreys/&gt; seven years before he introduced what is now known as Jeffreys' invariant prior: the square root of the determinant of Fisher's information matrix. ''"1/p" is Jeffreys' (1946) invariant prior for the [[exponential distribution]], not for the Bernoulli or binomial distributions''). However, it follows from the above discussion that Jeffreys Beta(1/2,1/2) prior represents a state of knowledge in between the Haldane Beta(0,0) and Bayes Beta (1,1) prior.

Similarly, [[Karl Pearson]] in his 1892 book [[The Grammar of Science]]&lt;ref name=PearsonGrammar&gt;{{cite book| last=Pearson|first=Karl|title=The Grammar of Science,|year=1892|publisher=Walter Scott,  London|url=https://books.google.com/books?hl=en&amp;lr=&amp;id=IvdsEcFwcnsC&amp;oi=fnd&amp;pg=PR19&amp;dq=grammar+of+science&amp;ots=az9FBXRPBB&amp;sig=fXi0R3vOzl1UnCDRAHPoSTx-sCQ#v=onepage&amp;q=grammar%20of%20science&amp;f=false}}&lt;/ref&gt;&lt;ref name=PearsnGrammar2009&gt;{{cite book|last=Pearson|first=Karl|title=The Grammar of Science|year=2009|publisher=BiblioLife|isbn=978-1110356119}}&lt;/ref&gt; (p.&amp;nbsp;144 of 1900 edition)  maintained that the Bayes (Beta(1,1) uniform prior was not a complete ignorance prior, and that it should be used when prior information justified to "distribute our ignorance equally"".  K. Pearson wrote: "Yet the only supposition that we appear to have made is this: that, knowing nothing of nature, routine and anomy (from the Greek ανομία, namely: a- "without", and nomos "law") are to be considered as equally likely to occur.  Now we were not really justified in making even this assumption, for it involves a knowledge that we do not possess regarding nature.  We use our ''experience'' of the constitution and action of coins in general to assert that heads and tails are equally probable, but we have no right to assert before experience that, as we know nothing of nature, routine and breach are equally probable. In our ignorance we ought to consider before experience that nature may consist of all routines, all anomies (normlessness), or a mixture of the two in any proportion whatever, and that all such are equally probable. Which of these constitutions after experience is the most probable must clearly depend on what that experience has been like."

If there is sufficient [[Sample (statistics)|sampling data]], ''and the posterior probability mode is not located at one of the extremes of the domain'' (x=0 or x=1), the three priors of Bayes (Beta(1,1)), Jeffreys (Beta(1/2,1/2)) and Haldane (Beta(0,0)) should yield similar [[posterior probability|''posterior'' probability]] densities.  Otherwise, as Gelman et al.&lt;ref name=Gelman&gt;{{cite book|last=Gelman|first=A., Carlin, J. B., Stern, H. S., and Rubin, D. B.|title=Bayesian Data Analysis| year=2003|publisher=Chapman and Hall/CRC|isbn=978-1584883883}}&lt;/ref&gt; (p.&amp;nbsp;65) point out, "if so few data are available that the choice of noninformative prior distribution makes a difference, one should put relevant information into the prior distribution", or as Berger&lt;ref name=BergerDecisionTheory/&gt; (p.&amp;nbsp;125) points out "when different reasonable priors yield substantially different answers, can it be right to state that there ''is'' a single answer? Would it not be better to admit that there is scientific uncertainty, with the conclusion depending on prior beliefs?."

===Subjective logic===
{{Main|Subjective logic}}

In standard logic, propositions are considered to be either true or false. In contradistinction, [[subjective logic]] assumes that humans cannot determine with absolute certainty whether a proposition about the real world is absolutely true or false. In [[subjective logic]] the [[A posteriori|posteriori]] probability estimates of binary events can be represented by beta distributions.&lt;ref name="J01"&gt;A. Jøsang. A Logic for Uncertain Probabilities. ''[[International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems]].'' 9(3), pp.279-311, June 2001. [http://www.unik.no/people/josang/papers/Jos2001-IJUFKS.pdf PDF]&lt;/ref&gt;

===Wavelet analysis===
{{Main|Beta wavelet}}

A [[wavelet]] is a wave-like [[oscillation]] with an [[amplitude]] that starts out at zero, increases, and then decreases back to zero. It can typically be visualized as a "brief oscillation" that promptly decays. Wavelets can be used to extract information from many different kinds of data, including&amp;nbsp;– but certainly not limited to&amp;nbsp;– audio signals and images. Thus, wavelets are purposefully crafted to have specific properties that make them useful for [[signal processing]]. Wavelets are localized in both time and [[frequency]] whereas the standard [[Fourier transform]] is only localized in frequency. Therefore, standard Fourier Transforms are only applicable to [[stationary process]]es, while [[wavelet]]s are applicable to non-[[stationary process]]es.  Continuous wavelets can be constructed based on the beta distribution. [[Beta wavelet]]s&lt;ref name="wavelet oliveira"&gt;H.M. de Oliveira and G.A.A. Araújo,. Compactly Supported One-cyclic Wavelets Derived from Beta Distributions. ''Journal of Communication and Information Systems.'' vol.20, n.3, pp.27-33, 2005.&lt;/ref&gt; can be viewed as a soft variety of [[Haar wavelet]]s whose shape is fine-tuned by two shape parameters α and β.

===Project management: task cost and schedule modeling===
The beta distribution can be used to model events which are constrained to take place within an interval defined by a minimum and maximum value. For this reason, the beta distribution&amp;nbsp;— along with the [[triangular distribution]]&amp;nbsp;— is used extensively in [[PERT]], [[critical path method]] (CPM), Joint Cost Schedule Modeling (JCSM) and other [[project management]]/control systems to describe the time to completion and the cost of a task. In project management, shorthand computations are widely used to estimate the [[mean]] and [[standard deviation]] of the beta distribution:&lt;ref name=Malcolm&gt;{{cite journal |last1=Malcolm |first1=D. G. |last2=Roseboom |first2=J. H. |last3=Clark |first3=C. E. |last4=Fazar |first4=W. |title=Application of a Technique for Research and Development Program Evaluation |journal=Operations Research |date=September–October 1958 |volume=7 |issue=5 |pages=646–669 |doi=10.1287/opre.7.5.646 |issn=0030-364X}}&lt;/ref&gt;

:&lt;math&gt; \begin{align}
  \mu(X) &amp; = \frac{a + 4b + c}{6} \\
  \sigma(X) &amp; = \frac{c-a}{6}
\end{align}&lt;/math&gt;

where ''a'' is the minimum, ''c'' is the maximum, and ''b'' is the most likely value (the [[Mode (statistics)|mode]] for ''α'' &gt; 1 and ''β'' &gt; 1).

The above estimate for the [[mean]] &lt;math&gt;\mu(X)= \frac{a + 4b + c}{6}&lt;/math&gt; is known as the [[PERT]] [[three-point estimation]] and it is exact for either of the following values of ''β'' (for arbitrary α within these ranges):

:''β'' = ''α'' &gt; 1 (symmetric case) with [[standard deviation]] &lt;math&gt;\sigma(X) = \frac{c-a}{2 \sqrt {1+2\alpha}}&lt;/math&gt;, [[skewness]] = 0, and [[excess kurtosis]] = &lt;math&gt; \frac{-6}{3+2 \alpha}&lt;/math&gt;

[[File:Beta Distribution beta=alpha from 1.05 to 4.95 - J. Rodal.jpg]]

or

:''β'' = 6 − ''α'' for 5 &gt; ''α'' &gt; 1 (skewed case) with [[standard deviation]]

:&lt;math&gt;\sigma(X) = \frac{(c-a)\sqrt{\alpha(6-\alpha)}}{6 \sqrt 7},&lt;/math&gt;

[[skewness]] = &lt;math&gt;\frac{(3-\alpha) \sqrt 7}{2\sqrt{\alpha(6-\alpha)}}&lt;/math&gt;, and [[excess kurtosis]] = &lt;math&gt;\frac{21}{\alpha (6- \alpha)} - 3&lt;/math&gt;

[[File:Beta Distribution for beta=6-alpha and alpha ranging from 1.05 to 3 - J. Rodal.jpg]]

The above estimate for the [[standard deviation]] ''σ''(''X'') = (''c'' − ''a'')/6 is exact for either of the following values of ''α'' and ''β'':

:''α'' = ''β'' = 4 (symmetric) with [[skewness]] = 0, and [[excess kurtosis]] = −6/11.
:''β'' = 6 − ''α'' and &lt;math&gt;\alpha = 3 - \sqrt2&lt;/math&gt; (right-tailed, positive skew) with [[skewness]] &lt;math&gt;=\frac{1}{\sqrt 2}&lt;/math&gt;, and [[excess kurtosis]] = 0
:''β'' = 6 − ''α'' and &lt;math&gt;\alpha = 3 + \sqrt2&lt;/math&gt; (left-tailed, negative skew) with [[skewness]] &lt;math&gt;= \frac{-1}{\sqrt 2}&lt;/math&gt;, and [[excess kurtosis]] = 0

[[File:Beta Distribution for alpha=beta=4 and (alpha=3--+Sqrt(2), beta=6-alpha) J. Rodal.jpg]]

Otherwise, these can be poor approximations for beta distributions with other values of α and β, exhibiting average errors of 40% in the mean and 549% in the variance.&lt;ref&gt;Keefer, Donald L. and Verdini, William A. (1993). Better Estimation of PERT Activity Time Parameters. Management Science 39(9), p. 1086&amp;ndash;1091.&lt;/ref&gt;&lt;ref&gt;Keefer, Donald L. and Bodily, Samuel E. (1983). Three-point Approximations for Continuous Random variables. Management Science 29(5), p. 595&amp;ndash;609.&lt;/ref&gt;&lt;ref&gt;[http://www.nps.edu/drmi/docs/1apr05-newsletter.pdf DRMI Newsletter, Issue 12, April 8, 2005]&lt;/ref&gt;

==Alternative parametrizations==

===Two parameters===

====Mean and sample size====
The beta distribution may also be reparameterized in terms of its mean ''μ'' {{nowrap|1=(0 &lt; ''μ'' &lt; 1)}} and the addition of both shape parameters {{nowrap|1= ''ν'' = ''α'' + ''β'' &gt; 0}}(&lt;ref name=Kruschke&gt;{{cite book|last=Kruschke|first=John K.|title=Doing Bayesian data analysis: A tutorial with R and BUGS|year=2011|publisher=Academic Press / Elsevier|location=p. 83|isbn=978-0123814852}}&lt;/ref&gt; p.&amp;nbsp;83). Denoting by αPosterior and βPosterior the shape parameters of the posterior beta distribution resulting from applying Bayes theorem to a binomial likelihood function and a prior probability,  the interpretation of the addition of both shape parameters to be sample size = ''ν'' = ''α''&amp;middot;Posterior + ''β''&amp;middot;Posterior is only correct for the Haldane prior probability Beta(0,0).  Specifically, for the Bayes (uniform) prior Beta(1,1) the correct interpretation would be sample size = ''α''&amp;middot;Posterior + ''β''&amp;nbsp;Posterior − 2, or ''ν'' = (sample size) + 2.  Of course, for sample size much larger than 2, the difference between these two priors becomes negligible.  (See section [[#Bayesian inference|Bayesian inference]] for further details.) In the rest of this article ν = α + β will be referred to as "sample size", but one should remember that it is, strictly speaking, the "sample size" of a binomial likelihood function only when using a Haldane Beta(0,0) prior in Bayes theorem.

This parametrization may be useful in Bayesian parameter estimation. For example, one may administer a test to a number of individuals. If it is assumed that each person's score (0 ≤ ''θ'' ≤ 1) is drawn from a population-level Beta distribution, then an important statistic is the mean of this population-level distribution. The mean and sample size parameters are related to the shape parameters α and β via&lt;ref name=Kruschke/&gt;

: ''α'' = ''μν'', ''β'' = (1 − ''μ'')''ν''

Under this [[parametrization]], one may place an [[uninformative prior]] probability over the mean, and a vague prior probability (such as an exponential or gamma distribution) over the positive reals for the sample size, if they are independent, and prior data and/or beliefs justify it.

====Mode and concentration====
The mode and "concentration" &lt;math&gt;\kappa = \alpha + \beta&lt;/math&gt; can also be used to calculate the parameters for a beta distribution.&lt;ref name="Kruschke2015"&gt;{{cite book|last=Kruschke|first=John K.|title=Doing Bayesian Data Analysis: A Tutorial with R, JAGS and Stan|year=2015|publisher=Academic Press / Elsevier|isbn=978-0-12-405888-0}}&lt;/ref&gt;
:&lt;math&gt;\begin{align}
\alpha &amp;= \omega (\kappa - 2) + 1\\
\beta  &amp;= (1 - \omega)(\kappa - 2) + 1
\end{align}&lt;/math&gt;

====Mean (allele frequency) and  (Wright's) genetic distance between two populations====
The [[Balding–Nichols model]]&lt;ref name=Balding&gt;{{cite journal |last1=Balding |first1=David J. |authorlink1=David Balding |last2=Nichols |first2=Richard A. |year=1995 |title=A method for quantifying differentiation between populations at multi-allelic loci and its implications for investigating identity and paternity |journal=Genetica |volume=96 |issue=1–2 |pages=3–12 |publisher=Springer |doi=10.1007/BF01441146 |url=http://www.springerlink.com/content/u27738g2626601p1/ |pmid=7607457}}&lt;/ref&gt; is a two-parameter [[parametrization]] of the beta distribution used in [[population genetics]].  It is a statistical description of the [[allele frequencies]] in the components of a sub-divided population:

:&lt;math&gt;
  \begin{align}
    \alpha &amp;= \mu \nu,\\
    \beta  &amp;= (1 - \mu) \nu,
  \end{align}
&lt;/math&gt;
where &lt;math&gt;\nu =\alpha+\beta= \frac{1-F}{F}&lt;/math&gt; and &lt;math&gt;0 &lt; F &lt; 1&lt;/math&gt;; here ''F'' is (Wright's) genetic distance between two populations.

See the articles [[Balding–Nichols model]], [[F-statistics]], [[fixation index]] and [[coefficient of relationship]], for further information.

====Mean and variance====

Solving the system of (coupled) equations given in the above sections as the equations for the mean and the variance of the beta distribution in terms of the original parameters ''α'' and ''β'', one can express the ''α'' and ''β'' parameters in terms of the mean (''μ'') and the variance (var):

:&lt;math&gt; \begin{align}
\nu &amp;= \alpha + \beta = \frac{\mu(1-\mu)}{\mathrm{var}}-1, \text{ where }\nu =(\alpha + \beta)  &gt;0,\text{ therefore: }\text{var}&lt; \mu(1-\mu)\\
\alpha&amp;= \mu \nu =\mu \left(\frac{\mu(1-\mu)}{\text{var}}-1\right), \text{ if } \text{var}&lt; \mu(1-\mu)\\
\beta &amp;= (1 - \mu) \nu = (1 - \mu)\left(\frac{\mu(1-\mu)}{\text{var}}-1\right), \text{ if }\text{var}&lt; \mu(1-\mu).
\end{align}&lt;/math&gt;

This [[parametrization]] of the beta distribution may lead to a more intuitive understanding than the one based on the original parameters ''α'' and ''β''. For example, by expressing the mode, skewness, excess kurtosis and differential entropy in terms of the mean and the variance:

[[File:Mode Beta Distribution for both alpha and beta greater than 1 - J. Rodal.jpg|325px]][[File:Mode Beta Distribution for both alpha and beta greater than 1 - another view - J. Rodal.jpg|325px]]
[[File:Skewness Beta Distribution for mean full range and variance between 0.05 and 0.25 - Dr. J. Rodal.jpg|325px]][[File:Skewness Beta Distribution for mean and variance both full range - J. Rodal.jpg|325px]]
[[File:Excess Kurtosis Beta Distribution with mean for full range and variance from 0.05 to 0.25 - J. Rodal.jpg|325px]][[File:Excess Kurtosis Beta Distribution with mean and variance for full range - J. Rodal.jpg|325px]]
[[File:Differential Entropy Beta Distribution with mean from 0.2 to 0.8 and variance from 0.01 to 0.09 - J. Rodal.jpg|325px]][[File:Differential Entropy Beta Distribution with mean from 0.3 to 0.7 and variance from 0 to 0.2 - J. Rodal.jpg|325px]]

===Four parameters===
A beta distribution with the two shape parameters α and β is supported on the range [0,1] or (0,1).  It is possible to alter the location and scale of the distribution by introducing two further parameters representing the minimum, ''a'', and maximum ''c'' (''c'' &gt; ''a''), values of the distribution,&lt;ref name=JKB/&gt;  by a linear transformation substituting the non-dimensional variable ''x'' in terms of the new variable ''y'' (with support [''a'',''c''] or (''a'',''c'')) and the parameters ''a'' and ''c'':

:&lt;math&gt;y = x(c-a) + a,  \text{ therefore    }x = \frac{y-a}{c-a}.&lt;/math&gt;

The [[probability density function]] of the four parameter beta distribution is equal to the two parameter distribution, scaled by the range (''c''-''a''), (so that the total area under the density curve equals a probability of one), and with the "y" variable shifted and scaled as follows:
::&lt;math&gt;f(y; \alpha, \beta, a, c) = \frac{f(x;\alpha,\beta)}{c-a} =\frac{\left(\frac{y-a}{c-a}\right)^{\alpha-1} \left (\frac{c-y}{c-a} \right)^{\beta-1} }{(c-a)B(\alpha, \beta)}=\frac{ (y-a)^{\alpha-1} (c-y)^{\beta-1} }{(c-a)^{\alpha+\beta-1}B(\alpha, \beta)}.&lt;/math&gt;

That a random variable ''Y'' is Beta-distributed with four parameters α, β, ''a'', and ''c'' will be denoted by:

:&lt;math&gt;Y \sim Beta(\alpha, \beta, a, c).&lt;/math&gt;

The measures of central location are scaled (by (''c''-''a'')) and shifted (by ''a''), as follows:

:&lt;math&gt; \begin{align}
\text{mean}(Y) &amp;= \text{mean} (X)(c-a) + a =  \left(\frac{\alpha}{\alpha+\beta}\right)(c-a) + a = \frac{\alpha c+ \beta a}{\alpha+\beta} \\
\text{mode}(Y) &amp;=\text{mode}(X)(c-a) + a  = \left(\frac{\alpha - 1}{\alpha+\beta - 2}\right)(c-a) + a = \frac{(\alpha-1) c+(\beta-1) a}{\alpha+\beta-2}\ ,\qquad \text{ if } \alpha, \beta&gt;1 \\
\text{median}(Y) &amp;= \text{median}(X)(c-a) + a  = \left (I_{\frac{1}{2}}^{[-1]}(\alpha,\beta) \right )(c-a)+a \\
G_Y &amp;= G_X(c-a) + a  = \left (e^{\psi(\alpha) - \psi(\alpha + \beta)} \right )(c-a)+a \\
H_Y &amp;= H_X(c-a) + a = \left (\frac{\alpha - 1}{\alpha + \beta - 1} \right)(c-a)+a, \,\qquad \text{if  } \alpha, \beta &gt; 0
\end{align}&lt;/math&gt;

The statistical dispersion measures are scaled (they do not need to be shifted because they are already centered on the mean) by the range (c-a), linearly for the mean deviation and nonlinearly for the variance:

::&lt;math&gt;\text{(mean deviation around mean)}(Y)=&lt;/math&gt;
::&lt;math&gt;(\text{(mean deviation around mean)}(X))(c-a) =\frac{2 \alpha^{\alpha} \beta^{\beta}}{\Beta(\alpha,\beta)(\alpha + \beta)^{\alpha + \beta + 1}}(c-a)&lt;/math&gt;
::&lt;math&gt; \text{var}(Y) =\text{var}(X)(c-a)^2 =\frac{\alpha\beta (c-a)^2}{(\alpha+\beta)^2(\alpha+\beta+1)}.&lt;/math&gt;

Since the [[skewness]] and [[excess kurtosis]] are non-dimensional quantities (as [[Moment (mathematics)|moments]] centered on the mean and normalized by the [[standard deviation]]), they are independent of the parameters ''a'' and ''c'', and therefore equal to the expressions given above in terms of ''X'' (with support [0,1] or (0,1)):

::&lt;math&gt; \text{skewness}(Y) =\text{skewness}(X) = \frac{2 (\beta - \alpha) \sqrt{\alpha + \beta + 1} }{(\alpha + \beta + 2) \sqrt{\alpha \beta}}.&lt;/math&gt;

::&lt;math&gt; \text{kurtosis excess}(Y) =\text{kurtosis excess}(X)=\frac{6[(\alpha - \beta)^2 (\alpha +\beta + 1) - \alpha \beta (\alpha + \beta + 2)]}
{\alpha \beta (\alpha + \beta + 2) (\alpha + \beta + 3)} &lt;/math&gt;

==History==
[[File:Karl Pearson 2.jpg|thumb|220px|[[Karl Pearson]] analyzed the beta distribution as the solution Type I of Pearson distributions ]]

The first systematic modern discussion of the beta distribution is probably due to [[Karl Pearson]] [[Fellow of the Royal Society|FRS]]&lt;ref name="frs"&gt;{{Cite journal | last1 = Yule | first1 = G. U. | authorlink1 = Udny Yule| last2 = Filon | first2 = L. N. G. | doi = 10.1098/rsbm.1936.0007 | title = [[Karl Pearson]]. 1857-1936 | journal = [[Obituary Notices of Fellows of the Royal Society]] | volume = 2 | issue = 5 | pages = 72 | year = 1936 | jstor = 769130| pmid =  | pmc = }}&lt;/ref&gt; (27 March 1857&amp;nbsp;&amp;ndash; 27 April 1936&lt;ref name=rscat&gt;{{cite web|url=http://www2.royalsociety.org/DServe/dserve.exe?dsqIni=Dserve.ini&amp;dsqApp=Archive&amp;dsqCmd=Show.tcl&amp;dsqDb=Persons&amp;dsqPos=0&amp;dsqSearch=%28%28text%29%3D%27%20%20Pearson%3A%20Karl%20%281857%20-%201936%29%20%20%27%29%29| accessdate=2011-07-01|title=Library and Archive catalogue|work=Sackler Digital Archive|publisher=Royal Society
}}&lt;/ref&gt;), an influential [[England|English]] [[mathematician]] who has been credited with establishing the discipline
of [[mathematical statistics]].&lt;ref name=year150&gt;{{cite web|url=http://www.economics.soton.ac.uk/staff/aldrich/KP150.htm|accessdate=2008-07-25|title=Karl Pearson sesquicentenary conference|date=2007-03-03|publisher=Royal Statistical Society}}&lt;/ref&gt;  In Pearson's papers&lt;ref name=Pearson /&gt;&lt;ref name=Pearson1895/&gt; the beta distribution is couched as a solution of a differential equation: [[Pearson distribution|Pearson's Type I distribution]] which it is essentially identical to except for arbitrary shifting and re-scaling (the beta and Pearson Type I distributions can always be equalized by proper choice of parameters). In fact, in several English books and journal articles in the few decades prior to World War II, it was common to refer to the beta distribution as Pearson's Type I distribution.  [[William Palin Elderton|William P. Elderton]] (1877–1962) in his 1906 monograph "Frequency curves and correlation"&lt;ref name=Elderton1906&gt;{{cite book|last=Elderton|first=William Palin|title=Frequency-Curves and Correlation |year=1906|publisher=Charles and Edwin Layton (London)|url=https://archive.org/details/frequencycurvesc00elderich}}&lt;/ref&gt; further analyzes the beta distribution as Pearson's Type I distribution, including a full discussion of the method of moments for the four parameter case, and diagrams of (what Elderton describes as) U-shaped, J-shaped, twisted J-shaped, "cocked-hat" shapes, horizontal and angled straight-line cases.  Elderton wrote "I am chiefly indebted to Professor Pearson, but the indebtedness is of a kind for which it is impossible to offer formal thanks."  [[William Palin Elderton|Elderton]] in his 1906 monograph &lt;ref name=Elderton1906/&gt; provides an impressive amount of information on the beta distribution, including equations for the origin of the distribution chosen to be the mode, as well as for other Pearson distributions: types I through VII. Elderton also included a number of appendixes, including one appendix ("II") on the beta and gamma functions. In later editions, Elderton added equations for the origin of the distribution chosen to be the mean, and analysis of Pearson distributions VIII through XII.

As remarked by Bowman and Shenton &lt;ref name="BowmanShenton"/&gt; "Fisher and Pearson had a difference of opinion in the approach to (parameter) estimation, in particular relating to (Pearson's method of) moments and (Fisher's method of) maximum likelihood in the case of the Beta distribution." Also according to Bowman and Shenton, "the case of a Type I (beta distribution) model being the center of the controversy was pure serendipity. A more difficult model of 4 parameters would have been hard to find."
[[Ronald Fisher]] (17 February 1890&amp;nbsp;– 29 July 1962) was one of the giants of statistics in the first half of the 20th century, and his long running public conflict with Karl Pearson can be followed in a number of articles in prestigious journals.  For example, concerning the estimation of the four parameters for the beta distribution, and Fisher's criticism of Pearson's method of moments as being arbitrary, see Pearson's article "Method of moments and method of maximum likelihood" &lt;ref name=Pearson1936&gt;{{cite journal|last=Pearson|first=Karl|title=Method of moments and method of maximum likelihood|journal=Biometrika|date=June 1936|volume=28|issue=1/2|url=https://www.jstor.org/discover/10.2307/2334123?uid=3739776&amp;uid=2134&amp;uid=2&amp;uid=70&amp;uid=4&amp;uid=3739256&amp;sid=21101030309093|doi=10.2307/2334123|pages=34}}&lt;/ref&gt; (published three years after his retirement from University College, London, where his position had been divided between Fisher and Pearson's son Egon) in which Pearson writes "I read (Koshai's paper in the Journal of the Royal Statistical Society, 1933) which as far as I am aware is the only case at present published of the application of Professor Fisher's method. To my astonishment that method depends on first working out the constants of the frequency curve by the (Pearson) Method of Moments and then superposing on it, by what Fisher terms "the Method of Maximum Likelihood" a further approximation to obtain, what he holds, he will thus get, "more efficient values" of the curve constants."

David and Edwards's treatise on the history of statistics&lt;ref name="David History"&gt;{{cite book|last=David|first=H. A. and A.W.F. Edwards|title=Annotated Readings in the History of Statistics|year=2001|publisher=Springer; 1 edition|isbn=978-0387988443}}&lt;/ref&gt; cites the first modern treatment of the beta distribution, in 1911,&lt;ref&gt;{{cite journal |last=Gini |first=Corrado |title=Considerazioni Sulle Probabilità Posteriori e Applicazioni al Rapporto dei Sessi Nelle Nascite Umane |journal=Studi Economico-Giuridici della Università de Cagliari |year=1911 |volume=Anno III |issue=reproduced in Metron 15, 133,171, 1949 |pages=5–41}}&lt;/ref&gt;  using the beta designation that has become standard, due to [[Corrado Gini]], an Italian [[statistician]], [[demography|demographer]], and [[sociology|sociologist]], who developed the [[Gini coefficient]]. [[Norman Lloyd Johnson|N.L.Johnson]] and [[Samuel Kotz|S.Kotz]], in their comprehensive and very informative monograph&lt;ref&gt;{{cite book|last=Johnson (Editor)|first=Norman L. and Samuel Kotz|title=Leading Personalities in Statistical Sciences: From the Seventeenth Century to the Present (Wiley Series in Probability and Statistics|year=1997|publisher=Wiley|isbn=978-0471163817}}&lt;/ref&gt;  on leading historical personalities in statistical sciences credit [[Corrado Gini]]&lt;ref&gt;{{cite web|last=Metron journal. |title=Biography of Corrado Gini |url=http://www.metronjournal.it/storia/ginibio.htm |publisher=Metron Journal |accessdate=2012-08-18 |deadurl=yes |archiveurl=https://web.archive.org/web/20120716202225/http://www.metronjournal.it/storia/ginibio.htm |archivedate=2012-07-16 |df= }}&lt;/ref&gt;  as "an early Bayesian...who dealt with the problem of eliciting the parameters of an initial Beta distribution, by singling out techniques which anticipated the advent of the so called empirical Bayes approach." [[Thomas Bayes|Bayes]], in a posthumous paper &lt;ref name="ThomasBayes"&gt;{{cite journal|last=Bayes|first=Thomas|author2=communicated by Richard Price|title=An Essay towards solving a Problem in the Doctrine of Chances|journal=Philosophical Transactions of the Royal Society|year=1763|volume=53|pages=370–418|jstor=10.2307/105741|doi=10.1098/rstl.1763.0053}}&lt;/ref&gt; published in 1763 by [[Richard Price]], obtained a beta distribution as the density of the probability of success in Bernoulli trials (see the section titled "Applications, Bayesian inference" in this article), but the paper does not analyze any of the moments of the beta distribution or discuss any of its properties.

==References==
{{Reflist}}

==External links==
{{Commons category}}
*[http://demonstrations.wolfram.com/BetaDistribution/ "Beta Distribution"] by Fiona Maclachlan, the [[Wolfram Demonstrations Project]], 2007.
*[http://www.xycoon.com/beta.htm Beta Distribution&amp;nbsp;&amp;ndash; Overview and Example], xycoon.com
*[https://web.archive.org/web/20120829140915/http://www.brighton-webs.co.uk/distributions/beta.htm Beta Distribution], brighton-webs.co.uk
*[http://www.exstrom.com/blog/snark/posts/dancingbeta.html Beta Distribution Video], exstrom.com
*{{springer|title=Beta-distribution|id=p/b015950}}
*{{MathWorld|urlname=BetaDistribution|title=Beta Distribution}}
*[https://www.youtube.com/watch?v=UZjlBQbV1KU Harvard University Statistics 110 Lecture 23 Beta Distribution, Prof. Joe Blitzstein]

{{ProbDistributions|continuous-bounded}}

{{DEFAULTSORT:Beta Distribution}}
[[Category:Continuous distributions]]
[[Category:Factorial and binomial topics]]
[[Category:Conjugate prior distributions]]
[[Category:Exponential family distributions]]</text>
      <sha1>h74w8xc7lu42a8x0ipdfoq20uokda9t</sha1>
    </revision>
  </page>
  <page>
    <title>Biordered set</title>
    <ns>0</ns>
    <id>22367851</id>
    <revision>
      <id>816315914</id>
      <parentid>790661263</parentid>
      <timestamp>2017-12-20T16:06:44Z</timestamp>
      <contributor>
        <username>Bigbossfarin</username>
        <id>10614517</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12875">{{Inappropriate tone|date=November 2012}}
A '''biordered set''' ("boset") is a [[mathematical object]] that occurs in the description of the  [[structure]] of the set of [[idempotent]]s in a [[semigroup]]. The concept and the terminology were developed by [[K S S Nambooripad]] in the early 1970s.&lt;ref&gt;{{cite book|last=Nambooripad|first=K S S|title=Structure of regular semigroups|publisher=[[University of Kerala]], [[Thiruvananthapuram]], [[India]]|date=1973|isbn=0-8218-2224-1}}&lt;/ref&gt;&lt;ref&gt;{{cite journal|doi=10.1007/BF02194864|last=Nambooripad|first=K S S|date=1975|title=Structure of regular semigroups I . Fundamental regular semigroups|journal=[[Semigroup Forum]]|volume=9|issue=4|pages=354–363}}&lt;/ref&gt;&lt;ref name="namb"&gt;{{cite book|last=Nambooripad|first=K S S|title=Structure of regular semigroups – I|publisher=American Mathematical Society|date=1979|series=Memoirs of the American Mathematical Society|volume=224|isbn=978-0-8218-2224-1}}&lt;/ref&gt;
The defining properties of a biordered set are expressed in terms of two [[quasiorder]]s defined on the set and hence the name biordered set. Patrick Jordan, while a master's student at University of   Sydney, introduced in 2002 the term '''boset'''  as an abbreviation of biordered set.&lt;ref&gt;Patrick K. Jordan. ''On biordered sets, including an alternative approach to fundamental regular semigroups''. Master’s thesis, University of Sydney, 2002.&lt;/ref&gt;

According to Mohan S. Putcha, "The axioms defining a biordered set are quite complicated. However, considering the general nature of semigroups, it is rather surprising that such a finite axiomatization is even possible."&lt;ref name="putcha"&gt;{{cite book|last=Putcha|first=Mohan S|title=Linear algebraic monoids|publisher=Cambridge University Press|date=1988|series=London Mathematical Society Lecture Note Series|volume=133|pages=121–122|isbn=978-0-521-35809-5}}&lt;/ref&gt;  Since the publication of the original definition of the biordered set by Nambooripad, several variations in the definition have been proposed. [[David Easdown]] simplified the definition and formulated the axioms in a special arrow notation invented by him.&lt;ref&gt;{{cite journal|last=Easdown|first=David|date=1984|title=Biordered sets are biordered subsets of idempotents of semigroups|journal=Journal of Australian Mathematical Society|volume=Series A, 32|issue=2|pages=258–268}}&lt;/ref&gt;

The set of idempotents in a semigroup is a biordered set and every biordered set is the set of idempotents of some semigroup.&lt;ref name="namb"/&gt;&lt;ref&gt;{{cite journal|doi=10.1016/0021-8693(85)90028-6|last=Easdown|first=David|date=1985|title=Biordered sets come from semigroups|journal=Journal of Algebra|volume=96|pages=581–91}}&lt;/ref&gt;
A regular biordered set is a biordered set with an additional  property. The set of idempotents in a [[regular semigroup]]  is a regular biordered set, and  every  regular biordered set is the set of idempotents of some regular semigroup.&lt;ref name="namb"/&gt;  
 
== Definition ==
The formal definition of a biordered set given by Nambooripad&lt;ref name="namb"/&gt; requires some preliminaries. 

=== Preliminaries ===

If ''X'' and ''Y'' be [[Set (mathematics)|sets]] and  ρ⊆ ''X'' × ''Y'', let ρ ( ''y'' ) = { ''x'' ∈ ''X'' : ''x'' ρ ''y'' }. 
  
Let ''E''  be a [[Set (mathematics)|set]] in which  a [[Partial function|partial]] [[binary operation]], indicated by juxtaposition, is defined.  If ''D''&lt;sub&gt;''E''&lt;/sub&gt; is the [[Domain (mathematics)|domain]] of the partial binary operation on ''E'' then ''D''&lt;sub&gt;''E''&lt;/sub&gt; is a [[Relation (mathematics)|relation]] on ''E'' and (''e'',''f'') is in  ''D''&lt;sub&gt;''E''&lt;/sub&gt;  if and only if the product ''ef'' exists in ''E''. The following relations can be defined in ''E'':

:&lt;math&gt;\omega^r = \{(e,f) \, :\, fe = e\}  &lt;/math&gt;

:&lt;math&gt;\omega^l = \{ (e,f)\, :\, ef = e \}  &lt;/math&gt;

:&lt;math&gt; R = \omega^r\, \cap \, (\omega^r)^{-1} &lt;/math&gt;

:&lt;math&gt; L = \omega^l\, \cap \, (\omega^l)^{-1} &lt;/math&gt;

:&lt;math&gt; \omega = \omega^r \, \cap \, \omega^l &lt;/math&gt;

If ''T'' is any [[Proposition (mathematics)|statement]] about ''E'' involving the partial binary operation and the above relations in ''E'', one can define the left-right [[Dual (mathematics)|dual]] of ''T'' denoted by ''T''*. If ''D''&lt;sub&gt;''E''&lt;/sub&gt; is [[symmetric relation|symmetric]] then ''T''* is meaningful whenever ''T'' is. 

=== Formal definition ===

The set ''E''  is called a biordered set if the following [[axiom]]s and their duals hold for arbitrary elements ''e'', ''f'', ''g'', etc. in ''E''.

:(B1)&amp;#8195; ω&lt;sup&gt;''r'' &lt;/sup&gt;   and ω&lt;sup&gt;''l''&lt;/sup&gt;  are [[Reflexive relation|reflexive]] and [[Transitive relation|transitive]] relations on ''E'' and ''D''&lt;sub&gt;''E''&lt;/sub&gt; = ( ω&lt;sup&gt;''r''&lt;/sup&gt; ∪ ω &lt;sup&gt;''l''&lt;/sup&gt; ) ∪ ( ω&lt;sup&gt;''r ''&lt;/sup&gt; ∪ ω&lt;sup&gt;''l''&lt;/sup&gt; )&lt;sup&gt;−1&lt;/sup&gt;.

:(B21)&amp;#8194; If ''f'' is in ω&lt;sup&gt;''r''&lt;/sup&gt;( ''e'' ) then ''f R fe '' ω ''e''.  

:(B22)&amp;#8194; If ''g'' ω&lt;sup&gt;''l''&lt;/sup&gt; ''f'' and if ''f'' and ''g'' are in ω&lt;sup&gt;''r''&lt;/sup&gt; ( ''e'' ) then ''ge'' ω&lt;sup&gt;''l''&lt;/sup&gt; ''fe''.

:(B31)&amp;#8194; If ''g'' ω&lt;sup&gt;''r''&lt;/sup&gt; ''f'' and ''f'' ω&lt;sup&gt;''r''&lt;/sup&gt; ''e'' then ''gf'' = ( ''ge'' )''f''.

:(B32)&amp;#8194; If ''g'' ω&lt;sup&gt;''l''&lt;/sup&gt; ''f'' and if ''f'' and ''g'' are in ω&lt;sup&gt;''r''&lt;/sup&gt; ( ''e'' ) then ( ''fg'' )''e'' = ( ''fe'' )( ''ge'' ).

In ''M'' ( ''e'', ''f'' ) = ω&lt;sup&gt;''l''&lt;/sup&gt; ( ''e'' ) ∩ ω&lt;sup&gt;''r''&lt;/sup&gt; ( ''f'' ) (the '''''M''-set''' of ''e'' and ''f'' in that order), define a relation &lt;math&gt;\prec&lt;/math&gt; by 

:&lt;math&gt;g \prec h\quad \Longleftrightarrow  \quad eg \,\,\omega^r\,\, eh\,,\,\,\,   gf\,\, \omega^l \,\,hf&lt;/math&gt;.  

Then the set

:&lt;math&gt; S(e,f) = \{ h\in M(e,f) : g\prec h \text{ for all } g\in M(e,f) \} &lt;/math&gt;

is called the '''sandwich set''' of ''e'' and ''f'' in that order. 

:(B4)&amp;#8195; If ''f'' and ''g'' are in ω&lt;sup&gt;''r''&lt;/sup&gt; ( ''e'' ) then ''S''( ''f'', ''g'' )''e'' = ''S'' ( ''fe'', ''ge'' ).

=== ''M''-biordered sets and regular biordered sets ===

We say that a biordered set ''E'' is an '''''M''-biordered set''' if ''M'' ( ''e'', ''f'' ) ≠ ∅ for all ''e'' and ''f'' in ''E''.  
Also, ''E'' is called a  '''regular biordered set''' if ''S'' ( ''e'', ''f'' ) ≠ ∅ for all ''e'' and ''f'' in ''E''.

In 2012 Roman S. Gigoń gave a simple proof that ''M''-biordered sets arise from [[E-inversive semigroup|''E''-inversive semigroup]]s.&lt;ref&gt;Gigoń, Roman (2012). "Some results on ''E''-inversive semigroups". Quasigroups and Related Systems '''20''': 53-60.&lt;/ref&gt;{{clarify|date=August 2014}}

== Subobjects and morphisms ==
=== Biordered subsets ===
A subset ''F''  of a biordered set ''E''  is a biordered subset (subboset) of ''E'' if ''F'' is a biordered set under the partial binary operation inherited from ''E''. 

For any ''e'' in ''E'' the sets ω&lt;sup&gt;''r''&lt;/sup&gt; ( ''e'' ),  ω&lt;sup&gt;''l''&lt;/sup&gt; ( ''e'' ) and ω  ( ''e'' ) are biordered subsets of ''E''.&lt;ref name="namb"/&gt;

=== Bimorphisms ===

A mapping φ : ''E'' → ''F'' between two biordered sets ''E'' and ''F'' is a biordered set  homomorphism (also called a bimorphism) if for all ( ''e'', ''f'' ) in ''D''&lt;sub&gt;''E''&lt;/sub&gt; we have ( ''e''φ ) ( ''f''φ ) = ( ''ef'' )φ.

== Illustrative examples ==
=== Vector space example ===

Let ''V'' be a [[vector space]] and 

:''E'' =  { ( ''A'', ''B'' ) | ''V'' =  ''A''  ⊕  ''B'' }

where ''V''  = ''A''  ⊕  ''B''  means that ''A'' and ''B'' are [[Linear subspace|subspaces]] of ''V'' and ''V'' is the  [[internal direct sum]] of ''A'' and ''B''. 
The partial binary operation ⋆ on E defined by 

:( ''A'', ''B'' ) ⋆ ( ''C'', ''D'' ) = ( ''A'' + ( ''B'' ∩ ''C'' ), ( ''B'' + ''C'' ) ∩ ''D '') 

makes ''E'' a biordered set. The quasiorders in ''E'' are characterised as follows: 

:( ''A'', ''B'' ) ω&lt;sup&gt;''r''&lt;/sup&gt; ( ''C'', ''D'' ) ⇔ ''A'' ⊇ ''C''
:( ''A'', ''B'' ) ω&lt;sup&gt;''l''&lt;/sup&gt; ( ''C'', ''D'' ) ⇔ ''B'' ⊆ ''D''

=== Biordered set of a semigroup ===

The set ''E'' of idempotents in a semigroup ''S'' becomes a biordered set if a partial binary operation is defined in ''E'' as follows: ''ef'' is defined in ''E'' if and only if ''ef'' = ''e'' or ''ef''= ''f'' or ''fe'' = ''e'' or ''fe'' = ''f'' holds in ''S''. If ''S'' is a regular semigroup then ''E'' is a regular biordered set.

As a concrete example, let ''S'' be the semigroup of all mappings of ''X'' = { 1, 2, 3 } into itself. Let the symbol (''abc'') denote the map for which 1 → ''a'',  2 → ''b'', and 3 → ''c''. The set ''E'' of idempotents in ''S'' contains the following elements:

:(111), (222), (333)  (constant maps)

:(122), (133), (121), (323), (113), (223)

:(123) (identity map)

The following table (taking composition of mappings in the diagram order) describes the partial binary operation in ''E''. An '''X''' in a cell indicates that the corresponding multiplication is not defined. 
&lt;center&gt;
{| class="wikitable" border="2"
|-
! style="background:#ADD8E6"|∗
!style="background:#ADD8E6"|&amp;nbsp;(111)&amp;nbsp;
!style="background:#ADD8E6"|&amp;nbsp;(222)&amp;nbsp;
!style="background:#ADD8E6"|&amp;nbsp;(333)&amp;nbsp;
!style="background:#ADD8E6"|&amp;nbsp;(122)&amp;nbsp;
!style="background:#ADD8E6"|&amp;nbsp;(133)&amp;nbsp;
!style="background:#ADD8E6"|&amp;nbsp;(121)&amp;nbsp;
!style="background:#ADD8E6"|&amp;nbsp;(323)&amp;nbsp;
!style="background:#ADD8E6"|&amp;nbsp;(113)&amp;nbsp;
!style="background:#ADD8E6"|&amp;nbsp;(223)&amp;nbsp;
!style="background:#ADD8E6"|&amp;nbsp;(123)&amp;nbsp;
|-style="background:#FFFDD0"
!style="background:#ADD8E6"|&amp;nbsp;(111)&amp;nbsp;
|&amp;nbsp;(111)||&amp;nbsp;(222)||&amp;nbsp;(333)||&amp;nbsp;(111)||&amp;nbsp;(111)||&amp;nbsp;(111)||&amp;nbsp;(333)||&amp;nbsp;(111)||&amp;nbsp;(222)||&amp;nbsp;(111)
|-style="background:#FFFDD0"
!style="background:#ADD8E6"|&amp;nbsp;(222)&amp;nbsp;
|&amp;nbsp;(111)||&amp;nbsp;(222)||&amp;nbsp;(333)||&amp;nbsp;(222)||&amp;nbsp;(333)||&amp;nbsp;(222)||&amp;nbsp;(222)||&amp;nbsp;(111)||&amp;nbsp;(222)||&amp;nbsp;(222)
|-style="background:#FFFDD0"
!style="background:#ADD8E6"|&amp;nbsp;(333)&amp;nbsp;
|&amp;nbsp;(111)||&amp;nbsp;(222)||&amp;nbsp;(333)||&amp;nbsp;(222)||&amp;nbsp;(333)||&amp;nbsp;(111)||&amp;nbsp;(333)||&amp;nbsp;(333)||&amp;nbsp;(333)||&amp;nbsp;(333)
|-
!style="background:#ADD8E6"|&amp;nbsp;(122)&amp;nbsp;
|style="background:#FFFDD0"|&amp;nbsp;(111)||style="background:#FFFDD0"|&amp;nbsp;(222)||style="background:#FFFDD0"|&amp;nbsp;(333)||&amp;nbsp;(122)||&amp;nbsp;(122)||&amp;nbsp;(121)||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||&amp;nbsp;(122)
|-
!style="background:#ADD8E6"|&amp;nbsp;(133)&amp;nbsp;
|style="background:#FFFDD0"|&amp;nbsp;(111)||style="background:#FFFDD0"|&amp;nbsp;(222)||style="background:#FFFDD0"|&amp;nbsp;(333)||&amp;nbsp;(122)||&amp;nbsp;(133)||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||&amp;nbsp;(133)||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||&amp;nbsp;(133)
|-
!style="background:#ADD8E6"|&amp;nbsp;(121)&amp;nbsp;
|style="background:#FFFDD0"|&amp;nbsp;(111)||style="background:#FFFDD0"|&amp;nbsp;(222)||style="background:#FFFDD0"|&amp;nbsp;(333)||&amp;nbsp;(121) ||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||&amp;nbsp;(121)||&amp;nbsp;(323)||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||&amp;nbsp;(121)
|-
!style="background:#ADD8E6"|&amp;nbsp;(323)&amp;nbsp;
|style="background:#FFFDD0"|&amp;nbsp;(111)||style="background:#FFFDD0"|&amp;nbsp;(222)||style="background:#FFFDD0"|&amp;nbsp;(333)||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||&amp;nbsp;(121)||&amp;nbsp;&amp;nbsp;(323)||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||&amp;nbsp;(323)||&amp;nbsp;(323)
|-
!style="background:#ADD8E6"|&amp;nbsp;(113)&amp;nbsp;
|style="background:#FFFDD0"|&amp;nbsp;(111)||style="background:#FFFDD0"|&amp;nbsp;(222)||style="background:#FFFDD0"|&amp;nbsp;(333)||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||&amp;nbsp;(113)||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||&amp;nbsp;(113)||&amp;nbsp;(223)||&amp;nbsp;(113)
|-
!style="background:#ADD8E6"|&amp;nbsp;(223)&amp;nbsp;
|style="background:#FFFDD0"|&amp;nbsp;(111)||style="background:#FFFDD0"|&amp;nbsp;(222)||style="background:#FFFDD0"|&amp;nbsp;(333)||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||style="background:silver; color:red"|&amp;nbsp;&amp;nbsp;&amp;nbsp;'''X'''||&amp;nbsp;(233)||&amp;nbsp;(113)||&amp;nbsp;(223)||&amp;nbsp;(223)
|-
!style="background:#ADD8E6"|&amp;nbsp;(123)&amp;nbsp;
|style="background:#FFFDD0"|&amp;nbsp;(111)||style="background:#FFFDD0"|&amp;nbsp;(222)||style="background:#FFFDD0"|&amp;nbsp;(333)||&amp;nbsp;(122)||&amp;nbsp;(133)||&amp;nbsp;(121)||&amp;nbsp;(323)||&amp;nbsp;(113)||&amp;nbsp;(223)||&amp;nbsp;(123)
|}
&lt;/center&gt;
== References ==

{{reflist}}

[[Category:Semigroup theory]]
[[Category:Algebraic structures]]
[[Category:Mathematical structures]]</text>
      <sha1>sw83jm6th7f46ufe4kdqmpoh4e7zfqk</sha1>
    </revision>
  </page>
  <page>
    <title>Bojan Mohar</title>
    <ns>0</ns>
    <id>34008046</id>
    <revision>
      <id>861299987</id>
      <parentid>834429461</parentid>
      <timestamp>2018-09-26T13:15:13Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>/* External links */add authority control, test</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2465">'''Bojan Mohar''' is a Slovenian and Canadian mathematician, specializing in [[graph theory]]. He is a professor of mathematics at the [[University of Ljubljana]]&lt;ref&gt;[http://www.fmf.uni-lj.si/en/directory/oddelek/2718/ Staff listing], Department of Mathematics, University of Ljubljana, accessed 2011-12-09.&lt;/ref&gt; and the holder of a [[Canada Research Chair]] in graph theory at [[Simon Fraser University]] in [[Vancouver|Vancouver, British Columbia]], Canada.&lt;ref name="crc"&gt;[http://www.chairs-chaires.gc.ca/chairholders-titulaires/profile-eng.aspx?profileID=1734 Canada Research Chairs – Chairholders – Bojan Mohar], accessed 2011-12-09.&lt;/ref&gt;

==Education==
Mohar received his Ph.D. from the University of Ljubljana in 1986, under the supervision of [[Tomaž Pisanski|Tomo Pisanski]].&lt;ref&gt;{{mathgenealogy|name=Bojan Mohar|id=20390}}&lt;/ref&gt;

==Research==
Mohar's research concerns [[topological graph theory]], [[algebraic graph theory]], [[Minor (graph theory)|graph minor]]s, and [[graph coloring]].&lt;ref name="crc"/&gt;

With [[Carsten Thomassen]] he is the co-author of the book ''Graphs on Surfaces'' (Johns Hopkins University Press, 2001).

==Awards and honors==
Mohar was a [[Fulbright Program|Fulbright visiting scholar]] at [[Ohio State University]] in 1988, and won the [[Boris Kidrič]] prize of the [[Socialist Republic of Slovenia]] in 1990.&lt;ref name="ias"/&gt; He has been a member of the Slovenian Academy of Engineering since 1999.&lt;ref name="ias"&gt;[http://www.ias.si/clanstvo/redni-in-izredni-clani/predstavitev-rednih-clanov/ Introduction of Regular Members] (in Slovenian), Slovenian Academy of Engineering, accessed 2011-12-09.&lt;/ref&gt;
He was named a [[SIAM Fellow]] in 2018.&lt;ref&gt;{{citation|url=https://sinews.siam.org/Details-Page/siam-announces-class-of-2018-fellows|title=SIAM Announces Class of 2018 Fellows|magazine=SIAM News|date=March 29, 2018}}&lt;/ref&gt;

==References==
{{reflist|2}}

==External links==
*[http://www.fmf.uni-lj.si/~mohar/ Home page at U. Ljubljana]

{{authority control}}

{{DEFAULTSORT:Mohar, Bojan}}
[[Category:Year of birth missing (living people)]]
[[Category:Living people]]
[[Category:Canadian mathematicians]]
[[Category:Slovenian mathematicians]]
[[Category:Graph theorists]]
[[Category:Canada Research Chairs]]
[[Category:University of Ljubljana alumni]]
[[Category:Simon Fraser University faculty]]
[[Category:University of Ljubljana faculty]]
[[Category:Fellows of the Society for Industrial and Applied Mathematics]]</text>
      <sha1>48xwvfvl314qft7b4mwe4zk4zt5ofnk</sha1>
    </revision>
  </page>
  <page>
    <title>Chaff algorithm</title>
    <ns>0</ns>
    <id>2045930</id>
    <revision>
      <id>792773626</id>
      <parentid>729869959</parentid>
      <timestamp>2017-07-28T15:51:35Z</timestamp>
      <contributor>
        <username>Widefox</username>
        <id>1588193</id>
      </contributor>
      <comment>Added {{[[Template:no footnotes|no footnotes]]}} tag to article ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1610">{{no footnotes|date=July 2017}}
'''Chaff''' is an [[algorithm]] for solving instances of the [[Boolean satisfiability problem]] in programming. It was designed by researchers at [[Princeton University]], United States. The algorithm is an instance of the [[DPLL algorithm]] with a number of enhancements for efficient implementation.

== Implementations ==

Some available implementations of the algorithm in software are mChaff and '''zChaff''', the latter one being the most widely known and used. zChaff was originally written by Dr. Lintao Zhang, now at [[Microsoft Research]], hence the “z”. It is now maintained by researchers at [[Princeton University]] and available for [[download]] as both source code and binaries on [[Linux]]. zChaff is free for non-commercial use.

== References ==
{{reflist}}
* M. Moskewicz, C. Madigan, Y. Zhao, L. Zhang, S. Malik. ''[http://www.princeton.edu/~chaff/publication/DAC2001v56.pdf Chaff: Engineering an Efficient SAT Solver]'', 39th Design Automation Conference (DAC 2001), Las Vegas, ACM 2001.
* {{Cite journal | last1 = Vizel | first1 = Y. | last2 = Weissenbacher | first2 = G. | last3 = Malik | first3 = S. | journal = Proceedings of the IEEE | volume = 103 | issue = 11 | year = 2015 | doi = 10.1109/JPROC.2015.2455034|title=Boolean Satisfiability Solvers and Their Applications in Model Checking}}
{{refend}}

== External links ==
* [http://www.princeton.edu/~chaff/zchaff.html Web page about zChaff]

[[Category:SAT solvers]]
[[Category:Boolean algebra]]
[[Category:Automated theorem proving]]
[[Category:Constraint programming]]


{{formalmethods-stub}}</text>
      <sha1>hh4v1ov8y6qzita3kntygbo0okrplzb</sha1>
    </revision>
  </page>
  <page>
    <title>Clique cover</title>
    <ns>0</ns>
    <id>17113364</id>
    <revision>
      <id>857734800</id>
      <parentid>802620185</parentid>
      <timestamp>2018-09-02T17:43:56Z</timestamp>
      <contributor>
        <ip>31.80.31.165</ip>
      </contributor>
      <comment>/* Approximation */ clarify order of quantification</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6650">In [[graph theory]], a '''clique cover''' or '''partition into cliques''' of a given [[undirected graph]] is a [[partition of a set|partition]] of the [[vertex (graph theory)|vertices of the graph]] into [[clique (graph theory)|cliques]], subsets of vertices within which every two vertices are adjacent. A '''minimum clique cover''' is a clique cover that uses as few cliques as possible. The minimum ''k'' for which a clique cover exists is called the '''clique cover number''' of the given graph.

==Relation to coloring==
A clique cover of a graph ''G'' may be seen as a [[graph coloring]] of the [[complement graph]] of ''G'', the graph on the same vertex set that has edges between non-adjacent vertices of ''G''. Like clique covers, graph colorings are partitions of the set of vertices, but into subsets with no adjacencies ([[independent set (graph theory)|independent sets]]) rather than cliques. A subset of vertices is a clique in ''G'' if and only if it is an independent set in the complement of ''G'', so a partition of the vertices of ''G'' is a clique cover of ''G'' if and only if it is a coloring of the complement of ''G''.

==Computational complexity==
The '''clique cover problem''' in [[computational complexity theory]] is the [[algorithm]]ic problem of finding a minimum clique cover, or (rephrased as a [[decision problem]]) finding a clique cover whose number of cliques is below a given threshold. Finding a minimum clique cover is [[NP-hard]], and its [[decision problem|decision version]] is [[NP-complete]]. It was one of [[Karp's 21 NP-complete problems|Richard Karp's original 21 problems]] shown NP-complete in his 1972 paper "Reducibility Among Combinatorial Problems".&lt;ref&gt;{{Citation
| first = Richard
| last = Karp
| authorlink = Richard Karp
| contribution = Reducibility Among Combinatorial Problems
| title = Proceedings of a Symposium on the Complexity of Computer Computations
| editor-first = R. E.
| editor-last = Miller
| editor2-first = J. W.
| editor2-last = Thatcher
| publisher = Plenum Press
| date = 1972
| pages = 85–103
| contribution-url = http://www.cs.berkeley.edu/~luca/cs172/karp.pdf
| accessdate=2008-08-29}}&lt;/ref&gt;

The equivalence between clique covers and coloring is a [[Reduction (complexity)|reduction]] that can be used to prove the NP-completeness of the clique cover problem from the known NP-completeness of graph coloring.&lt;ref&gt;{{Citation | first1 = Michael R. | last1 = Garey | author1-link = Michael R. Garey | first2 = David S. | last2 = Johnson | author2-link = David S. Johnson | year = 1979 | title = Computers and Intractability: A Guide to the Theory of NP-Completeness | publisher = W.H. Freeman  | isbn = 0-7167-1045-5 }} A1.2: GT19, pg.194.&lt;/ref&gt;

==In special classes of graphs==
[[Perfect graph]]s are defined as the graphs in which, for every [[induced subgraph]], the chromatic number (minimum number of colors in a coloring) equals the size of the [[maximum clique]].
According to the [[Perfect graph theorem|weak perfect graph theorem]], the complement of a perfect graph is also perfect. Therefore, the perfect graphs are also the graphs in which, for every induced subgraph, the clique cover number equals the size of the [[maximum independent set]]. It is possible to compute the clique cover number in perfect graphs in [[polynomial time]].

Another class of graphs in which the minimum clique cover can be found in polynomial time are the [[triangle-free graph]]s. In these graphs, every clique cover consists of a [[Matching (graph theory)|matching]] (a set of disjoint pairs of adjacent vertices) together with [[singleton set]]s for the remaining unmatched vertices. The number of cliques equals the number of vertices minus the number of matched pairs. Therefore, in triangle-free graphs, the minimum clique cover can be found by using an algorithm for [[maximum matching]].

The optimum partition into cliques can also be found in polynomial time for graphs of bounded [[clique-width]].&lt;ref&gt;{{citation
 | last1 = Espelage | first1 = Wolfgang
 | last2 = Gurski | first2 = Frank
 | last3 = Wanke | first3 = Egon
 | contribution = How to solve NP-hard graph problems on clique-width bounded graphs in polynomial time
 | doi = 10.1007/3-540-45477-2_12
 | pages = 117–128
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = International Workshop on Graph-Theoretic Concepts in Computer Science (WG 2001)
 | volume = 2204
 | year = 2001}}.&lt;/ref&gt; These include, among other graphs, the [[cograph]]s and [[distance-hereditary graph]]s, which are both also classes of perfect graphs.

==Approximation==
The same [[hardness of approximation]] results that are known for graph coloring also apply to clique cover. Therefore, unless [[P = NP]], there can be no [[polynomial time]] [[approximation algorithm]] for any {{math|''&amp;epsilon;'' &gt; 0}} that, on {{mvar|n}}-vertex graphs, achieves an [[approximation ratio]] better than {{math|''n''&lt;sup&gt;1 &amp;minus; ''&amp;epsilon;''&lt;/sup&gt;}}.&lt;ref&gt;{{Citation
 | last = Zuckerman | first = D.
 | title = Linear degree extractors and the inapproximability of Max Clique and Chromatic Number
 | journal = [[Theory of Computing (journal)|Theory of Computing]]
 | volume = 3
 | pages = 103–128
 | year = 2007
 | doi = 10.4086/toc.2007.v003a006
}}.&lt;/ref&gt;

In graphs where every vertex has [[degree (graph theory)|at most three neighbors]], the clique cover remains NP-hard, and there is a constant {{math|''&amp;rho;'' &gt; 1}} such that it is NP-hard to approximate with [[approximation ratio]] {{mvar|&amp;rho;}} or better. Nevertheless, in [[polynomial time]] it is possible to find an approximation with ratio 5/4. That is, this approximation algorithm finds a clique cover whose number of cliques is no more than 5/4 times the optimum.&lt;ref&gt;{{citation
 | last1 = Cerioli | first1 = M.R.
 | last2 = Faria | first2 = L.
 | last3 = Ferreira | first3 = T.O.
 | last4 = Martinhon | first4 = C.A.J.
 | last5 = Protti | first5 = F.
 | last6 = Reed | first6 = B.
 | date = June 2008
 | doi = 10.1016/j.dam.2007.10.015
 | issue = 12
 | journal = [[Discrete Applied Mathematics]]
 | pages = 2270–2278
 | title = Partition into cliques for cubic graphs: Planar case, complexity and approximation
 | volume = 156}}.&lt;/ref&gt;

==Related problems==
The related [[clique edge cover]] problem concerns partitioning the edges of a graph, rather than the vertices, into subgraphs induced by cliques. It is also NP-complete.&lt;ref&gt;{{harvtxt|Garey|Johnson|1979}}, Problem GT59.&lt;/ref&gt;

== References ==
{{reflist}}

[[Category:Graph theory objects]]
[[Category:NP-complete problems]]
[[Category:Computational problems in graph theory]]</text>
      <sha1>i2ilozo06uxnufszvrjnwep3jm7vmnp</sha1>
    </revision>
  </page>
  <page>
    <title>Count On</title>
    <ns>0</ns>
    <id>20644971</id>
    <revision>
      <id>795347330</id>
      <parentid>754742716</parentid>
      <timestamp>2017-08-13T17:24:22Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 3 sources and tagging 0 as dead. #IABot (v1.5beta)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2857">'''Count On''' is a major [[mathematics education]] project in the [[United Kingdom]] which was announced by [[Secretary of State for Education|education secretary]] [[David Blunkett]] at the end of 2000. It was the follow-on to [[Maths Year 2000]] which was the UK's contribution to [[UNICEF]]'s [[World Mathematical Year]].&lt;ref&gt;{{citation|title=English pupils lag behind in maths|journal=[[BBC News]]|url=http://news.bbc.co.uk/2/hi/uk_news/education/1055704.stm|date=5 December 2000}}.&lt;/ref&gt;

Count On had two main strands:
* The website www.counton.org&lt;ref&gt;{{citation|title=My Media: Kate Scarborough|journal=[[The Guardian]]|url=https://www.theguardian.com/media/2006/jul/31/mondaymediasection4|date=31 July 2006}}.&lt;/ref&gt; which won the 2002 [[BETT]] prize for best free online learning resource.&lt;ref&gt;{{citation|title=Deputy logs on to £100,000|journal=[[Times Educational Supplement]]|url=http://www.tes.co.uk/teaching-resource/Deputy-logs-on-to-pounds-100-000-358319/|date=18 January 2002}}.&lt;/ref&gt;
* "MathFests", which were maths funfairs held around the country, aimed particularly at those who would not normally come into contact with mathematical ideas.&lt;ref&gt;{{citation|title=No doubt about it - we're addicted to maths|journal=[[Times Educational Supplement]]|url=http://www.tes.co.uk/article.aspx?storycode=342720|date=19 January 2001}}.&lt;/ref&gt;

The MathFests were run largely by [[MatheMagic]] and the [[University of York]].{{Citation needed|date=July 2011}}

The project has now been handed over to the [[NCETM]].{{Citation needed|date=July 2011}}

==Popularisation of Mathematics==
''Count On'' and ''Maths Year 2000'' were some of the first big Popularisation of Mathematics projects. Others are listed below.
===International===
* [[World Mathematical Year 2000]] [https://web.archive.org/web/20130113071154/http://wmy2000.math.jussieu.fr/]
* Statistics 2013[http://statistics2013.org/]
* [[World Maths Day]] (orig. Australian) - next one is 6 March 2013 [https://web.archive.org/web/20110925091442/http://www.worldmathsday.com/]
===Australia===
* [[World Maths Day]][https://web.archive.org/web/20110925091442/http://www.worldmathsday.com/]
===India===
* [[National Mathematics Year]][http://theindianawaaz.com/index.php?option=com_content&amp;view=article&amp;id=5405&amp;catid=43]
===Ireland===
* Maths Week Ireland [http://www.mathsweek.ie/2012/schools/Mangahigh-online-maths-games]

===Nigeria===
* [[National Mathematics Year]]
===Spain===
* Matematica Vital [http://matematicavital.com/]
* Paul Boron[http://www.paulborons.com/]
===United Kingdom===
* Maths Year 2000 Scotland
* Maths Cymru (Wales)
===USA===
* Steven Strogatz's blog [http://opinionator.blogs.nytimes.com/author/steven-strogatz/]

==References==
{{reflist}}
{{Use dmy dates|date=July 2011}}

[[Category:Mathematics education in the United Kingdom]]

{{UK-edu-stub}}
{{math-stub}}</text>
      <sha1>q1mf6m6z8m0l6qwp3mrmc4urhk46h4l</sha1>
    </revision>
  </page>
  <page>
    <title>Cours d'Analyse</title>
    <ns>0</ns>
    <id>41007672</id>
    <revision>
      <id>788940413</id>
      <parentid>766409882</parentid>
      <timestamp>2017-07-04T11:41:16Z</timestamp>
      <contributor>
        <username>Suslindisambiguator</username>
        <id>12329968</id>
      </contributor>
      <comment>/* Bibliography */ added Category:Mathematics textbooks</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6821">{{italic title}}
'''''Cours d'Analyse de l’École Royale Polytechnique; I.re Partie. Analyse algébrique''''' is a seminal textbook in [[infinitesimal calculus]] published by [[Augustin-Louis Cauchy]] in 1821. The article follows the translation by Bradley and Sandifer in describing its contents.

==Introduction==
On page 1 of the Introduction, Cauchy writes: "In speaking of the continuity of functions, I could not dispense with a treatment of the principal properties of [[infinitely small]] quantities, properties which serve as the foundation of the infinitesimal calculus." The translators comment in a footnote: "It is interesting that Cauchy does not also mention limits here."

Cauchy continues: "As for the methods, I have sought to give them all the rigor which one demands from geometry, so that one need never rely on arguments drawn from the [[generality of algebra]]."

==Preliminaries==
On page 6, Cauchy first discusses variable quantities and then introduces the limit notion in the following terms: "When the values successively attributed to a particular variable indefinitely approach a fixed value in such a way as to end up by differing from it by as little as we wish, this fixed value is called the ''limit'' of all the other values."

On page 7, Cauchy defines an [[infinitesimal]] as follows: "When the successive numerical values of such a variable decrease indefinitely, in such a way as to fall below any given number, this variable becomes what we call ''infinitesimal'', or an ''infinitely small quantity''." Cauchy adds: "A variable of this kind has zero as its limit."

On page 10, Bradley and Sandifer confuse the [[versed cosine]] with the [[coversed sine]]. Cauchy originally defined the ''[[sinus versus]]'' ([[versine]]) as siv(''&amp;theta;'')&amp;nbsp;=&amp;nbsp;1-cos(''&amp;theta;'') and the ''[[cosinus versus]]'' (what is now also known as [[coversine]]) as cosiv(''&amp;theta;'')&amp;nbsp;=&amp;nbsp;1-sin(''&amp;theta;''). In the translation, however, the ''cosinus versus'' (and cosiv) are incorrectly associated with the ''versed cosine'' (what is now also known as [[vercosine]]) rather than the ''coversed sine''.

The notation

:lim

is introduced on page 12. The translators observe in a footnote: "The notation “Lim.” for limit was first used by [[Simon Antoine Jean L'Huilier]] (1750–1840) in [L’Huilier 1787, p.&amp;nbsp;31]. Cauchy wrote this as “lim.” in [Cauchy 1821, p.&amp;nbsp;13]. The period had disappeared by [Cauchy 1897, p.&amp;nbsp;26]."

==Chapter 2==
This chapter has the long title "On infinitely small and infinitely large quantities, and on the continuity of functions. Singular values of functions in various particular cases." On page 21, Cauchy writes: "We say that a variable quantity becomes ''infinitely small'' when its numerical value decreases indefinitely in such a way as to converge towards the limit zero." On the same page, we find the only explicit example of such a variable to be found in Cauchy, namely 
:&lt;math&gt;\frac{1}{4}, \frac{1}{3},\frac{1}{6},  \frac{1}{5}, \frac{1}{8}, \frac{1}{7}, \ldots&lt;/math&gt; 
On page 22, Cauchy starts the discussion of orders of magnitude of infinitesimals as follows: "Let &lt;math&gt;\alpha&lt;/math&gt; be an infinitely small quantity, that is a variable whose numerical value decreases indefinitely. When the various integer powers of &lt;math&gt;\alpha&lt;/math&gt;, namely 
:&lt;math&gt;\alpha, \alpha^2, \alpha^3, \ldots&lt;/math&gt;
enter into the same calculation, these various powers are called, respectively, infinitely small of the ''first'', the ''second'', the ''third order'', etc. Cauchy notes that "the general form of infinitely small quantities of order ''n'' (where ''n'' represents an integer number) will be
:&lt;math&gt;k\alpha^n\quad{}&lt;/math&gt;  or at least   &lt;math&gt;{}\quad k\alpha^n(1\pm \varepsilon)&lt;/math&gt;.

On pages 23-25, Cauchy presents eight theorems on properties of infinitesimals of various orders.

==Section 2.2==
This is entitled "Continuity of functions". Cauchy writes: "If, beginning with a value of ''x'' contained between these limits, we add to the variable ''x'' an
infinitely small increment &lt;math&gt;\alpha&lt;/math&gt;, the function itself is incremented by the difference 
:&lt;math&gt;f(x+\alpha)-f(x)&lt;/math&gt;"
and states that 
:"the function ''f(x)'' is a continuous function of ''x'' between the assigned limits if, for each value of ''x'' between these limits, the numerical value of the difference &lt;math&gt;f(x+\alpha)-f(x)&lt;/math&gt; decreases indefinitely with the numerical value of &lt;math&gt;\alpha&lt;/math&gt;." 
Cauchy goes on to provide an italicized definition of continuity in the following terms: 
:"''the function ''f(x)'' is continuous with respect to ''x'' between the given limits if, between these limits, an infinitely small increment in the variable always produces an infinitely small increment in the function itself.''"

On page 32 Cauchy states the [[intermediate value theorem]].

==Sum theorem==
In Theorem I in section 6.1 (page 90 in the translation by Bradley and Sandifer), Cauchy presents the sum theorem in the following terms.

''When the various terms of series (1) are functions of the same variable x, continuous with respect to this variable in the neighborhood of a particular value for which the series converges, the sum s of the series is also a continuous function of x in the neighborhood of this particular value.''

Here the series (1) appears on page 86: (1) &lt;math&gt;u_0, u_1, u_2, \ldots, u_n, u_{n+1},\ldots&lt;/math&gt;

==Bibliography==
*{{cite book | author-first=Augustin-Louis | author-last=Cauchy | author-link=Augustin-Louis Cauchy | title=Cours d'Analyse de l'Ecole royale polytechnique | volume=1 | chapter=Analyse Algébrique | date=1821 | publisher=L'Imprimerie Royale, Debure frères, Libraires du Roi et de la Bibliothèque du Roi | url=https://books.google.com/books?id=UrT0KsbDmDwC&amp;pg=PA11&amp;lpg=PA11 | access-date=2015-11-07}}
*{{cite book | author1-first=Robert E. | author1-last=Bradley | author2-first=C. Edward | author2-last=Sandifer | editor1-first=J.Z. | editor1-last=Buchwald | others=[[Augustin-Louis Cauchy|Cauchy, Augustin-Louis]] | title=Cauchy’s Cours d’analyse: An Annotated Translation | work=Sources and Studies in the History of Mathematics and Physical Sciences | orig-year=2009 | date=2010-01-14 | publisher=[[Springer Science+Business Media, LLC]] | id=1441905499, 978-1-4419-0549-9 | isbn=978-1-4419-0548-2 | doi=10.1007/978-1-4419-0549-9 | lccn=2009932254 |pages=10, 285 | url=https://books.google.com/books?id=M0or-HGe7D0C
| access-date=2015-11-09}}
* {{cite book |first=Judith V. |last=Grabiner |title=The Origins of Cauchy's Rigorous Calculus |location=Cambridge |publisher=MIT Press |year=1981 |isbn=0-387-90527-8 }}

{{Infinitesimals}}

[[Category:Mathematics of infinitesimals]]
[[Category:Calculus]]
[[Category:History of calculus]]
[[Category:Mathematics textbooks]]</text>
      <sha1>bjpfqvb4jj1n9qxdlkcw9ij5v7vk3su</sha1>
    </revision>
  </page>
  <page>
    <title>De Branges's theorem</title>
    <ns>0</ns>
    <id>252075</id>
    <revision>
      <id>841508041</id>
      <parentid>819195017</parentid>
      <timestamp>2018-05-16T08:20:04Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>/* References */Journal cites, added 3 DOIs</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15927">{{lowercase title}}
In [[complex analysis]], '''de Branges's theorem''', or the '''Bieberbach conjecture''', is a theorem that gives a [[necessary condition]] on a [[holomorphic function]] in order for it to map the [[unit disc|open unit disk]] of the [[complex plane]] [[injective]]ly to the complex plane. It was posed by {{harvs|txt|first=Ludwig |last=Bieberbach|authorlink=Ludwig Bieberbach|year=1916}} and finally proven by {{harvs|txt|authorlink=Louis de Branges de Bourcia|first=Louis |last=de Branges|year=1985}}.

The statement concerns the [[Taylor series|Taylor coefficient]]s ''a&lt;sub&gt;n&lt;/sub&gt;'' of a [[univalent function]], i.e. a one-to-one holomorphic function that maps the unit disk into the complex plane, normalized as is always possible so that ''a''&lt;sub&gt;0&lt;/sub&gt; = 0 and ''a''&lt;sub&gt;1&lt;/sub&gt; = 1. That is, we consider a function defined on the open unit disk which is [[holomorphic function|holomorphic]]  and injective (''[[Univalent function|univalent]]'') with Taylor series of the form

:&lt;math&gt;f(z)=z+\sum_{n\geq 2} a_n z^n.&lt;/math&gt;

Such functions are called ''schlicht''.  The theorem then states that

:&lt;math&gt; |a_n| \leq n \quad \text{for all }n\geq 2.&lt;/math&gt;

The [[Koebe function]] (see below) is a function in which ''a&lt;sub&gt;n&lt;/sub&gt;''&amp;nbsp;=&amp;nbsp;''n'' for all ''n'', and it is schlicht, so we cannot find a stricter limit on the absolute value of the ''n''th coefficient.

==Schlicht functions==

The normalizations

:''a''&lt;sub&gt;0&lt;/sub&gt; = 0 and ''a''&lt;sub&gt;1&lt;/sub&gt; = 1

mean that

:''f''(0) = 0 and ''f'' '(0) = 1.

This can always be obtained by an [[affine transformation]]: starting with an arbitrary injective holomorphic function ''g'' defined on the open unit disk and setting

:&lt;math&gt;f(z)=\frac{g(z)-g(0)}{g'(0)}.&lt;/math&gt;

Such functions ''g'' are of  interest because they appear in the  [[Riemann mapping theorem]].

A '''schlicht function''' is defined as an analytic function ''f'' that is one-to-one and satisfies ''f''(0) = 0 and ''f'' '(0) = 1.  A  family of schlicht functions are the [[rotated Koebe function]]s

:&lt;math&gt;f_\alpha(z)=\frac{z}{(1-\alpha z)^2}=\sum_{n=1}^\infty n\alpha^{n-1} z^n&lt;/math&gt;

with α a complex number of [[absolute value]] 1. If ''f'' is a schlicht function and |''a''&lt;sub&gt;''n''&lt;/sub&gt;| = ''n'' for some ''n'' ≥ 2, then ''f'' is a rotated Koebe function.

The condition of de Branges' theorem is not sufficient to show the function is schlicht, as the function
:&lt;math&gt;f(z)=z+z^2 = (z+1/2)^2 - 1/4&lt;/math&gt;
shows: it is holomorphic on the unit disc and satisfies |''a''&lt;sub&gt;''n''&lt;/sub&gt;|≤''n'' for all ''n'', but it is not injective since ''f''(&amp;minus;1/2&amp;nbsp;+&amp;nbsp;''z'') = ''f''(&amp;minus;1/2&amp;nbsp;&amp;minus;&amp;nbsp;''z'').

==History ==
A survey of the history is given by [http://kobra.bibliothek.uni-kassel.de/bitstream/urn:nbn:de:hebis:34-200604038936/1/prep0513.pdf Koepf (2007)].

{{harvtxt|Bieberbach|1916}} proved |''a''&lt;sub&gt;2&lt;/sub&gt;| ≤ 2, and stated the conjecture that |''a''&lt;sub&gt;''n''&lt;/sub&gt;| ≤ ''n''. {{harvtxt|Loewner|1917}} and  {{harvtxt|Nevanlinna|1921}} independently proved the conjecture for [[Nevanlinna's criterion#Application to Bieberbach conjecture|starlike functions]].
Then [[Charles Loewner]] ({{harvtxt|Löwner|1923}}) proved |''a''&lt;sub&gt;3&lt;/sub&gt;| ≤ 3, using the [[Löwner equation]]. His work was used by most later attempts, and is also applied in the theory of [[Schramm–Loewner evolution]].

{{harvtxt|Littlewood|1925|loc=theorem 20}} proved that |''a''&lt;sub&gt;''n''&lt;/sub&gt;| ≤ ''en'' for all ''n'', showing that the Bieberbach conjecture is true up to a factor of ''e'' = 2.718... Several authors later reduced the constant in the inequality below ''e''.
 
If ''f''(''z'') = ''z'' + ...  is a schlicht function then φ(''z'') = ''f''(''z''&lt;sup&gt;2&lt;/sup&gt;)&lt;sup&gt;1/2&lt;/sup&gt; is an odd schlicht function. 
{{harvs|txt|authorlink=Raymond Paley|last=Paley|author2-link=John Edensor Littlewood|last2=Littlewood|year=1932}} showed that its Taylor coefficients  satisfy ''b''&lt;sub&gt;''k''&lt;/sub&gt; ≤ 14 for all ''k''. They conjectured that 14 can be replaced by 1 as a natural generalization of the Bieberbach conjecture. The Littlewood–Paley conjecture easily implies the Bieberbach conjecture using the Cauchy inequality, but it was soon disproved by {{harvtxt|Fekete|Szegö|1933}}, who showed there is an odd schlicht function with ''b''&lt;sub&gt;5&lt;/sub&gt; = 1/2 + exp(&amp;minus;2/3) = 1.013..., and that this is the maximum possible value of ''b''&lt;sub&gt;5&lt;/sub&gt;. [[Isaak Milin]] later showed that 14 can be replaced by 1.14, and Hayman showed that the numbers ''b''&lt;sub&gt;''k''&lt;/sub&gt; have a limit less than 1 if ''f'' is not a Koebe function (for which the ''b''&lt;sub&gt;2''k''+1&lt;/sub&gt; are all 1). So the limit is always less than or equal to 1, meaning that Littlewood and Paley's conjecture is true for all but a finite number of coefficients. A weaker form of Littlewood and Paley's conjecture was found by {{harvtxt|Robertson|1936}}.

The '''Robertson conjecture''' states that if

:&lt;math&gt;\phi(z) = b_1z+b_3z^3+b_5z^5+\cdots&lt;/math&gt;

is an odd schlicht function in the unit disk with ''b''&lt;sub&gt;1&lt;/sub&gt;=1 then for all positive integers ''n'', 
:&lt;math&gt;\sum_{k=1}^n|b_{2k+1}|^2\le n.&lt;/math&gt;

Robertson observed that his conjecture is still strong enough to imply the Bieberbach conjecture, and proved it for ''n'' = 3. This conjecture introduced the key idea of bounding various quadratic functions of the coefficients rather than the coefficients themselves, which is equivalent to bounding norms of elements in certain Hilbert spaces of schlicht functions.

There were several proofs of the Bieberbach conjecture for certain higher values of ''n'', in particular {{harvtxt|Garabedian|Schiffer|1955}} proved |''a''&lt;sub&gt;4&lt;/sub&gt;| ≤ 4, {{harvtxt|Ozawa|1969}} and {{harvtxt|Pederson|1968}} proved |''a''&lt;sub&gt;6&lt;/sub&gt;| ≤ 6, and {{harvtxt|Pederson|Schiffer|1972}} proved |''a''&lt;sub&gt;5&lt;/sub&gt;| ≤ 5.

{{harvtxt|Hayman|1955}} proved that the limit of ''a''&lt;sub&gt;''n''&lt;/sub&gt;/''n'' exists, and has absolute value less than 1 unless ''f'' is a Koebe function. In particular this showed that for any ''f'' there can be at most a finite number of exceptions to the Bieberbach conjecture.

The '''Milin conjecture''' states that for each schlicht function on the unit disk, and for all positive integers ''n'', 
:&lt;math&gt;\sum^n_{k=1} (n-k+1)(k|\gamma_k|^2-1/k)\le 0&lt;/math&gt;

where the '''logarithmic coefficients''' γ&lt;sub&gt;''n''&lt;/sub&gt; of  ''f'' are given by

:&lt;math&gt;\log(f(z)/z)=2 \sum^\infty_{n=1}\gamma_nz^n.&lt;/math&gt;
  
{{harvtxt|Milin|1977}} showed using the [[Lebedev–Milin inequality]] that the Milin conjecture (later proved by de Branges) implies the Robertson conjecture and therefore the Bieberbach conjecture.

Finally {{harvtxt|De Branges|1985}} proved |''a''&lt;sub&gt;''n''&lt;/sub&gt;| ≤ ''n'' for all ''n''.

==de Branges's proof==
The proof uses a type of [[Hilbert space]]s of [[entire function]]s. The study of these spaces grew into a sub-field of complex analysis and the spaces have come to be called [[de Branges space]]s. De Branges  proved the stronger  Milin conjecture {{harv|Milin|1971}} on logarithmic coefficients. This was already known to imply the Robertson conjecture {{harv|Robertson|1936}} about odd univalent functions, &lt;!--the Rogosinski conjecture {{harv|Rogosinski|1943}} about subordinate functions,--&gt; which in turn was known to imply  the Bieberbach conjecture about schlicht functions {{harv|Bieberbach|1916}}. His proof uses the [[Loewner equation]], the [[Askey–Gasper inequality]] about [[Jacobi polynomial]]s, and  the [[Lebedev–Milin inequality]] on exponentiated power series.

De Branges reduced the conjecture to some inequalities for Jacobi polynomials, and verified the first few by hand. [[Walter Gautschi]] verified more of these inequalities by computer for de Branges (proving the Bieberbach conjecture for the first 30 or so coefficients) and then asked [[Richard Askey]] whether he knew of any similar inequalities. Askey pointed out that {{harvtxt|Askey|Gasper|1976}} had proved the necessary inequalities eight years before, which allowed de Branges to complete his proof. The first version was very long and had some minor mistakes which caused some skepticism about it, but these were corrected with the help of members of the Leningrad seminar on Geometric Function Theory ([[St. Petersburg Department of Steklov Institute of Mathematics of Russian Academy of Sciences|Leningrad Department of Steklov Mathematical Institute]]) when de Branges visited in 1984.

De Branges proved the following result, which for ν&amp;nbsp;=&amp;nbsp;0 implies the Milin conjecture (and therefore the Bieberbach conjecture). 
Suppose that ν&amp;nbsp;&amp;gt;&amp;nbsp;&amp;minus;3/2 and σ&lt;sub&gt;''n''&lt;/sub&gt; are real numbers for positive integers ''n'' with limit 0 and such that
:&lt;math&gt; \rho_n=\frac{\Gamma(2\nu+n+1)}{\Gamma(n+1)}(\sigma_n-\sigma_{n+1}) &lt;/math&gt;
is non-negative, non-increasing, and has limit 0. Then for all Riemann mapping functions ''F''(''z'') =&amp;nbsp;''z''&amp;nbsp;+&amp;nbsp;... univalent in the unit disk with
:&lt;math&gt;\frac{F(z)^\nu-z^\nu} {\nu}= \sum_{n=1}^{\infty} a_nz^{\nu+n}&lt;/math&gt;
the maximinum value of 
:&lt;math&gt;\sum_{n=1}^\infty(\nu+n)\sigma_n|a_n|^2&lt;/math&gt;
is achieved by the Koebe function ''z''/(1&amp;nbsp;&amp;minus;&amp;nbsp;''z'')&lt;sup&gt;2&lt;/sup&gt;.

A simplified version of the proof was published in 1985 by [[Carl FitzGerald]] and [[Christian Pommerenke]] ({{harvtxt|FitzGerald|Pommerenke|1985}}), and an even shorter description by [[Jacob Korevaar]] ({{harvtxt|Korevaar|1986}}).

==References==

*{{Citation |author1-link=Richard Askey |last1=Askey |first1=Richard |last2=Gasper |first2=George |title=Positive Jacobi polynomial sums. II |mr=0430358 |year=1976 |journal=[[American Journal of Mathematics]] |issn=0002-9327 |volume=98 |issue=3 |pages=709–737 |doi=10.2307/2373813 |jstor=2373813}}
*{{Citation |editor1-last=Baernstein |editor1-first=Albert |editor2-last=Drasin |editor2-first=David |editor3-last=Duren |editor3-first=Peter |editor4-last=Marden |editor4-first=Albert |display-editors=3 |title=The Bieberbach conjecture |publisher=[[American Mathematical Society]] |location=Providence, R.I. |series=Mathematical Surveys and Monographs |isbn=978-0-8218-1521-2 |mr=875226 |year=1986 |volume=21 |pages=xvi+218 |doi=10.1090/surv/021}}
*{{citation |first=L. |last=Bieberbach |title=Über die Koeffizienten derjenigen Potenzreihen, welche eine schlichte Abbildung des Einheitskreises vermitteln |journal=Sitzungsber. Preuss. Akad. Wiss. Phys-Math. Kl. |year=1916 |pages=940–955}}
* {{Citation |last1=Conway |first1=John B. |author1-link=John B. Conway |title=Functions of One Complex Variable II |publisher=[[Springer-Verlag]] |location=Berlin, New York |isbn=978-0-387-94460-9 |year=1995}}
*{{Citation |last1=de Branges |first1=Louis |author1-link=Louis de Branges de Bourcia |title=A proof of the Bieberbach conjecture |doi=10.1007/BF02392821 |mr=772434 |year=1985 |journal=[[Acta Mathematica]] |volume=154 |issue=1 |pages=137–152}}
*{{Citation |last1=de Branges |first1=Louis |author1-link=Louis de Branges de Bourcia |title=Proceedings of the International Congress of Mathematicians, Vol. 1, 2 (Berkeley, Calif., 1986) |publisher=[[American Mathematical Society]] |location=Providence, R.I. |mr=934213 |year=1987 |chapter=Underlying concepts in the proof of the Bieberbach conjecture |pages=25–42}}
*{{citation |mr=875226 |title=The Bieberbach conjecture |series=Mathematical Surveys and Monographs |volume=21 |work=Proceedings of the symposium on the occasion of the proof of the Bieberbach conjecture held at Purdue University, West Lafayette, Ind., March 11—14, 1985 |editor1-last=Drasin |editor1-first=David |editor2-last=Duren |editor2-first=Peter |editor3-last=Marden |editor3-first=Albert |publisher=American Mathematical Society |place=Providence, RI |year=1986 |pages=xvi+218 |isbn=0-8218-1521-0 |doi=10.1090/surv/021}}
*{{citation |first=M. |last=Fekete |first2=G. |last2=Szegö |title=Eine Bemerkung Über Ungerade Schlichte Funktionen |journal=J. London Math. Soc. |year=1933 |pages=85–89 |doi=10.1112/jlms/s1-8.2.85 |volume=s1-8 |issue=2 }}
*{{citation |first=Carl |last=FitzGerald |first2=Christian |last2=Pommerenke |title=The de Branges theorem on univalent functions |journal=Trans. Amer. Math. Soc. |year=1985 |doi=10.2307/2000306 |volume=290 |page=683}}
*{{springer |id=B/b016150 |first=E.G. |last=Goluzina |title=Bieberbach conjecture}}
*{{Citation |last1=Grinshpan |first1=Arcadii Z. |title=The Bieberbach conjecture and Milin's functionals |doi=10.2307/2589676 |mr=1682341 |year=1999 |journal=[[American Mathematical Monthly|The American Mathematical Monthly]] |volume=106 |issue=3 |pages=203–214 |jstor=2589676 }}
*{{Citation |editor-last=Kuhnau |editor-first=Reiner (ed.) |last=Grinshpan |first=Arcadii Z. |author-link= |contribution=Logarithmic Geometry, Exponentiation, and Coefficient Bounds in the Theory of Univalent Functions and Nonoverlapping Domains |contribution-url= |title=Geometric Function Theory |place=[[Amsterdam]] |publisher=[[North-Holland Publishing Company|North-Holland]] |series=Handbook of Complex Analysis |volume=Volume 1 |year=2002 |pages=273–332 |url= |doi= 10.1016/S1874-5709(02)80012-9|id= |isbn=0-444-82845-1 |mr=1966197 |zbl=1083.30017 }}.
*{{Citation |last1=Hayman |first1=W. K. |title=The asymptotic behaviour of p-valent functions |doi=10.1112/plms/s3-5.3.257 |mr=0071536 |year=1955 |journal=Proceedings of the London Mathematical Society |series=Third Series |volume=5 |pages=257–284 |issue=3}}
* Koepf, Wolfram (2007), ''[http://kobra.bibliothek.uni-kassel.de/bitstream/urn:nbn:de:hebis:34-200604038936/1/prep0513.pdf Bieberbach’s Conjecture, the de Branges and Weinstein Functions and the Askey-Gasper Inequality]''    
*{{Citation |last1=Korevaar |first1=Jacob |authorlink=Jacob Korevaar |title=Ludwig Bieberbach's conjecture and its proof by Louis de Branges |mr=856290 |year=1986 |journal=[[American Mathematical Monthly|The American Mathematical Monthly]] |issn=0002-9890 |volume=93 |issue=7 |pages=505–514 |doi=10.2307/2323021 |jstor=2323021 |url=http://www.maa.org/programs/maa-awards/writing-awards/ludwig-bieberbachs-conjecture-and-its-proof-by-louis-de-branges}}
*{{citation |first=J. E. |last=Littlewood |title=On Inequalities in the Theory of Functions |journal=Proc. London Math. Soc. |year=1925 |pages=481–519 |doi=10.1112/plms/s2-23.1.481 |volume=s2-23 }}
*{{citation |first1=J.E. |last1=Littlewood |first2=E. A. C. |last2=Paley |title=A Proof That An Odd Schlicht Function Has Bounded Coefficients |journal=J. London Math. Soc. |year=1932 |pages=167–169 |doi=10.1112/jlms/s1-7.3.167 |volume=s1-7 |issue=3 }}
*{{citation |first=C. |last=Loewner |title=Untersuchungen über die Verzerrung bei konformen Abbildungen des Einheitskreises /z/ &lt; 1, die durch Funktionen mit nicht verschwindender Ableitung geliefert werden |journal=Ber. Verh. Sachs. Ges. Wiss. Leipzig |volume=69 |year=1917 |pages=89–106}}
*{{citation |first=C. |last=Loewner |title=Untersuchungen über schlichte konforme Abbildungen des Einheitskreises. I |journal=Math. Ann. |volume=89 |year=1923 |pages=103–121 |jfm=49.0714.01 |doi=10.1007/BF01448091}}
*{{Citation |last1=Milin |first1=I. M. |title=Univalent functions and orthonormal systems |publisher=[[American Mathematical Society]] |location=Providence, R.I. |mr=0369684 |year=1977}} (Translation of the 1971 Russian edition)
*{{citation |last=Nevanlinna |first=R. |title=Über die konforme Abbildung von Sterngebieten |journal=Ofvers. Finska Vet. Soc. Forh. |volume=53 |year=1921 |pages=1–21}}
*{{Citation |last1=Robertson |first1=M. S. |title=A remark on the odd schlicht functions |url=http://www.ams.org/bull/1936-42-06/S0002-9904-1936-06300-7/ |doi=10.1090/S0002-9904-1936-06300-7 |year=1936 |journal=[[Bulletin of the American Mathematical Society]] |volume=42 |pages=366–370 |issue=6}}

[[Category:Theorems in complex analysis]]
[[Category:Conjectures]]
[[Category:Conjectures that have been proved]]</text>
      <sha1>jnqtzcplljkfmgi9pgzccl8qzlq033k</sha1>
    </revision>
  </page>
  <page>
    <title>Donaldson theory</title>
    <ns>0</ns>
    <id>7684340</id>
    <revision>
      <id>780661275</id>
      <parentid>776948326</parentid>
      <timestamp>2017-05-16T13:21:03Z</timestamp>
      <contributor>
        <username>Omnipaedista</username>
        <id>8524693</id>
      </contributor>
      <comment>per MOS:CAPS</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2001">'''Donaldson theory''' is the study of the topology of smooth [[4-manifold]]s using moduli spaces of anti-self-dual [[Instanton#Mathematics|instantons]]. It was started by [[Simon Donaldson]] (1983) who proved [[Donaldson's theorem]] restricting the possible quadratic forms on the second cohomology group of a compact simply connected 4-manifold.  Important consequences of this theorem include the existence of an [[Exotic R4]] and the failure of the smooth [[H-Cobordism theorem|h-cobordism theorem]] in 4 dimensions. The results of Donaldson theory depend therefore on the manifold having a differential structure, and are largely false for topological 4-manifolds. 

Many of the theorems in Donaldson theory can now be proved more easily using [[Seiberg&amp;ndash;Witten theory]], though there are a number of open problems remaining in Donaldson theory, such as the [[Witten conjecture]] and the [[Atiyah-Floer conjecture]].

==See also==

*[[Kronheimer–Mrowka basic class]]
*[[Instanton]]
*[[Floer homology]]

==References==
*{{Citation |last=Donaldson |first=Simon |title=An Application of Gauge Theory to Four Dimensional Topology |journal=Journal of Differential Geometry |volume=18 |issue=2 |year=1983 |pages=279–315 |doi= |mr=710056 }}.
*{{Citation |first=S. K. |last=Donaldson |first2=P. B. |last2=Kronheimer |authorlink2=Peter B. Kronheimer |title=The Geometry of Four-Manifolds |series=Oxford Mathematical Monographs |location=Oxford |publisher=Clarendon Press |year=1997 |isbn=0-19-850269-9 }}.
*{{Citation |first=D. S. |last=Freed |first2=K. K. |last2=Uhlenbeck |authorlink2=Karen Uhlenbeck |title=Instantons and four-manifolds |location=New York |publisher=Springer |year=1984 |isbn=0-387-96036-8 }}.
*{{Citation |first=A. |last=Scorpan |title=The wild world of 4-manifolds |location=Providence |publisher=American Mathematical Society |year=2005 |isbn=0-8218-3749-4 }}.

[[Category:Geometric topology]]
[[Category:4-manifolds]]
[[Category:Differential topology]]


{{topology-stub}}</text>
      <sha1>pc0e34l4txfhq6pgsfvwfza5n65vl5x</sha1>
    </revision>
  </page>
  <page>
    <title>Double pushout graph rewriting</title>
    <ns>0</ns>
    <id>35717039</id>
    <revision>
      <id>860342251</id>
      <parentid>843055665</parentid>
      <timestamp>2018-09-20T00:41:34Z</timestamp>
      <contributor>
        <username>Jarble</username>
        <id>7226930</id>
      </contributor>
      <minor/>
      <comment>linking</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4654">In [[computer science]], '''double pushout graph rewriting''' (or DPO graph rewriting) refers to a mathematical framework for [[graph rewriting]]. It was introduced as one of the first algebraic approaches to graph rewriting in the article "Graph-grammars: An algebraic approach" (1973).&lt;ref name="ehrig73"&gt;"Graph-grammars: An algebraic approach", Ehrig, Hartmut and Pfender, Michael and Schneider, Hans-Jürgen, Switching and Automata Theory, 1973. SWAT'08. IEEE Conference Record of 14th Annual Symposium on, pp. 167-180, 1973, IEEE&lt;/ref&gt; It has since been generalized to allow rewriting structures which are not graphs, and to handle negative application conditions,&lt;ref name="ehrig2"&gt;"Constraints and application conditions: From graphs to high-level structures", Ehrig, Ehrig, Habel and Pennemann, Graph transformations, pp. 287--303, Springer&lt;/ref&gt; among other extensions.

== Definition ==

A DPO graph transformation system (or [[graph grammar]]) consists of a finite [[Graph (discrete mathematics)|graph]], which is the starting state, and a finite or countable set of labeled [[Span (category theory)|spans]] in the [[Category (mathematics)|category]] of finite graphs and graph homomorphisms, which serve as derivation rules. The rule spans are generally taken to be composed of [[monomorphism]]s, but the details can vary.&lt;ref name="habel01"&gt;"Double-pushout graph transformation revisited", Habel, Annegret and Müller, Jürgen and Plump, Detlef, Mathematical Structures in Computer Science, vol. 11, no. 05., pp. 637--688, 2001, Cambridge University Press&lt;/ref&gt;

Rewriting is performed in two steps: deletion and addition.

After a match from the left hand side to &lt;math&gt;G&lt;/math&gt; is fixed, nodes and edges that are not in the right hand side are deleted. The right hand side is then glued in.

Gluing graphs is in fact a [[Pushout_(category_theory)|pushout]] construction in the [[category (mathematics)|category]] of graphs, and the deletion is the same as finding a pushout complement, hence the name.

== Uses ==

Double pushout graph rewriting allows the specification of graph transformations by specifying a pattern of fixed size and composition to be found and replaced, where part of the pattern can be preserved. The application of a rule is potentially non-deterministic: several distinct matches can be possible. These can be non-overlapping, or share only preserved items, thus showing a kind of [[concurrency (computer science)|concurrency]] known as parallel independence,&lt;ref name="corradini"&gt;"Concurrent computing: from Petri nets to graph grammars", Corradini, Andrea, ENTCS, vol. 2, pp. 56--70, 1995, Elsevier&lt;/ref&gt; or they may be incompatible, in which case either the applications can sometimes be executed sequentially, or one can even preclude the other.

It can be used as a language for software design and programming (usually a variant working on richer structures than graphs is chosen). [[Halting problem|Termination]] for DPO graph rewriting is [[undecidable problem|undecidable]] because the [[Post correspondence problem]] can be reduced to it.&lt;ref name="plump1998termination"&gt;, "Termination of graph rewriting is undecidable", Detlef Plump, Fundamenta Informaticae, vol. 33, no. 2, pp. 201--209, 1998, IOS Press&lt;/ref&gt;
&lt;!-- does an implementation exist which is more than a prototype? for example, GROOVE can be used to simulate DPO rewriting --&gt;

DPO graph rewriting can be viewed as a generalization of [[Petri nets]].&lt;ref name="corradini"/&gt;

== Generalization ==
Axioms have been sought to describe categories in which DPO rewriting will work. One possibility is the notion of an [[adhesive category]], which also enjoys many closure properties. Related notions are HLR systems, quasi-adhesive categories and &lt;math&gt;\mathcal{M}&lt;/math&gt;-adhesive categories, adhesive HLR categories.&lt;ref name="ahlr"&gt;Hartmut Ehrig and Annegret Habel and Julia Padberg and Ulrike Prange, "Adhesive high-level replacement categories and systems", 2004, Springer&lt;/ref&gt;

The concepts of [[adhesive category]] and HLR system are related (an adhesive category with [[coproduct]]s is a HLR system&lt;ref name="Lack"&gt;"Adhesive categories", Stephen Lack and Paweł Sobociński, in ''Foundations of software science and computation structures'', pp. 273--288, Springer 2004&lt;/ref&gt;).

[[Hypergraph]], [[typed graph]] and [[attributed graph]] rewriting,&lt;ref&gt;"Fundamentals of Algebraic Graph Transformation", Hartmut Ehrig, Karsten Ehrig, Ulrike Prange and Gabriele Taentzer&lt;/ref&gt; for example, can be handled because they can be cast as adhesive HLR systems.

== Notes ==
&lt;references/&gt;

[[Category:Graph algorithms]]
[[Category:Graph rewriting]]</text>
      <sha1>oitxr5x6ej4vc37rgnlz0jbh7o4cv3j</sha1>
    </revision>
  </page>
  <page>
    <title>Evolution and the Theory of Games</title>
    <ns>0</ns>
    <id>1614212</id>
    <revision>
      <id>775221411</id>
      <parentid>761199792</parentid>
      <timestamp>2017-04-13T13:45:08Z</timestamp>
      <contributor>
        <username>RDiMartino</username>
        <id>7439756</id>
      </contributor>
      <minor/>
      <comment>/* Oveview */ fixed spelling</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3529">{{Infobox book
|name       = Evolution and the Theory of Games
|author     = [[John Maynard Smith]]
|image      = Evolution and the Theory of Games.jpg
|image_size = 
|caption    = Cover of ''Evolution and the Theory of Games'', with an exemplary [[ternary plot]] of frequency changes of three different strategies.
|isbn       = 0-521-28884-3
|dewey      = 575 19
|congress   = QH371 .M325 1982
|oclc       = 8034750
|pub_date   = December 1982
|genre      = Non-fiction
|subject    = [[Evolutionary game theory]]
|pages      = 234 pp.
|publisher  = [[Cambridge University Press]]
|country    = United Kingdom
|language   = English
}}

'''''Evolution and the Theory of Games''''' is a book by the British [[evolutionary biology|evolutionary biologist]] [[John Maynard Smith]] on [[evolutionary game theory]].&lt;ref&gt;{{cite web|title=Evolution and the Theory of Games  AUTHOR: John Maynard Smith, University of Sussex|url=http://www.cambridge.org/us/academic/subjects/life-sciences/evolutionary-biology/evolution-and-theory-games?format=PB&amp;isbn=9780521288842|website=[[Cambridge University Press]]|publisher=cambridge.org|accessdate=27 October 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Evolution and the Theory of Games by John Maynard Smith|url=http://www.goodreads.com/book/show/41862.Evolution_and_the_Theory_of_Games|website=[[Goodreads]]|publisher=goodreads.com|accessdate=27 October 2016}}&lt;/ref&gt;&lt;ref&gt;{{cite web|title=Evolution and the Theory of Games, John Maynard Smith, Cambridge University Press, 1982|url=http://dannyreviews.com/h/Evolution_and_the_Theory_of_Games.html|publisher=dannyreviews.com|accessdate=27 October 2016}}&lt;/ref&gt; The book was initially published in December 1982 by [[Cambridge University Press]].

==Overview==
In the book, [[John Maynard Smith]] summarises work on [[evolutionary game theory]] that had developed in the 1970s, to which he made several important contributions.  The book is also noted for being well written and not overly mathematically challenging.

The main contribution to be had from this book is the introduction of the [[Evolutionarily Stable Strategy]], or ESS, concept, which states that for a set of behaviours to be conserved over evolutionary time, they must be the most profitable avenue of action when common, so that no alternative behaviour can invade.  So, for instance, suppose that in a population of frogs, males fight to the death over breeding ponds.  This would be an ESS if any one cowardly frog that does not fight to the death always fares worse (in fitness terms, of course).  A more likely scenario is one where fighting to the death is not an ESS because a frog might arise that will stop fighting if it realises that it is going to lose.  This frog would then reap the benefits of fighting, but not the ultimate cost.  Hence, fighting to the death would easily be invaded by a mutation that causes this sort of "informed fighting."  Much complexity can be built from this, and Maynard Smith is outstanding at explaining in clear prose and with simple math.

==Reception==
{{empty section|date=January 2017}}
&lt;!--would expect to see cited reviews of the book here to establish notability--&gt;

==See also==
{{Portal|Evolutionary biology}}
* [[Evolutionary biology]]

==References==
{{Reflist}}

==External links==
* [http://www.cambridge.org/uk/catalogue/catalogue.asp?isbn=0521288843 Cambridge University Press]

[[Category:1982 books]]
[[Category:Books about evolution]]
[[Category:Game theory]]
[[Category:Evolutionary game theory]]
[[Category:Mathematics books]]</text>
      <sha1>tsqq4b3kfl591y0j3h3ztov0srzfpbu</sha1>
    </revision>
  </page>
  <page>
    <title>Feedback vertex set</title>
    <ns>0</ns>
    <id>1860368</id>
    <revision>
      <id>846588289</id>
      <parentid>841539970</parentid>
      <timestamp>2018-06-19T18:01:37Z</timestamp>
      <contributor>
        <username>Bibcode Bot</username>
        <id>14394459</id>
      </contributor>
      <minor/>
      <comment>Adding 0 [[arXiv|arxiv eprint(s)]], 1 [[bibcode|bibcode(s)]] and 0 [[digital object identifier|doi(s)]]. Did it miss something? Report bugs, errors, and suggestions at [[User talk:Bibcode Bot]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12319">In the [[mathematics|mathematical]] discipline of [[graph theory]], a '''feedback vertex set''' of a [[Graph (discrete mathematics)|graph]] is a set of vertices whose removal leaves a graph without [[cycle (graph theory)|cycles]]. In other words, each feedback vertex set contains at least one vertex of any cycle in the graph.
The '''feedback vertex set problem''' is an [[NP-complete]] problem in [[computational complexity theory]]. It was among the [[Karp's 21 NP-complete problems|first problems shown to be NP-complete]]. It has wide applications in [[operating system]]s, [[database system]]s, and [[VLSI]] chip design.

== Definition ==

The [[decision problem]] is as follows:

:INSTANCE: An (undirected or directed) [[Graph (discrete mathematics)|graph]] &lt;math&gt;G = (V, E)&lt;/math&gt; and a positive integer &lt;math&gt;k&lt;/math&gt;.
:QUESTION: Is there a subset &lt;math&gt;X \subseteq V&lt;/math&gt; with &lt;math&gt;|X| \leq k&lt;/math&gt; such that &lt;math&gt;G&lt;/math&gt; with the vertices from &lt;math&gt;X&lt;/math&gt; deleted is [[cycle (graph theory)|cycle-free]]?

The graph &lt;math&gt;G[V \setminus X]&lt;/math&gt; that remains after removing &lt;math&gt;X&lt;/math&gt; from &lt;math&gt;G&lt;/math&gt; is an induced [[forest (graph theory)|forest]] (resp. an induced [[directed acyclic graph]] in the case of [[directed graph]]s). Thus, finding a minimum feedback vertex set in a graph is equivalent to finding a maximum induced forest (resp. maximum induced directed acyclic graph in the case of [[directed graph]]s).

==NP-hardness==
{{harvtxt|Karp|1972}} showed that the feedback vertex set problem for [[directed graph]]s is [[NP-complete]]. The problem remains NP-complete on directed graphs with maximum in-degree and out-degree two, and on directed planar graphs with maximum in-degree and out-degree three.&lt;ref&gt;unpublished results due to Garey and Johnson, cf. {{harvtxt|Garey|Johnson|1979}}: GT7&lt;/ref&gt; Karp's reduction also implies the NP-completeness of the feedback vertex set problem on undirected graphs, where the problem stays NP-hard on graphs of [[Degree (graph theory)|maximum degree]] four. The feedback vertex set problem can be solved in polynomial time on graphs of [[Degree (graph theory)|maximum degree]] at most three.&lt;ref&gt;{{harvtxt|Ueno|Kajitani|Gotoh|1988}}; {{harvtxt|Li|Liu|1999}}&lt;/ref&gt;

Note that the problem of deleting as few ''edges'' as possible to make the graph cycle-free is equivalent to finding a [[spanning tree]], which can be done in [[polynomial time]]. In contrast, the problem of deleting edges from a [[directed graph]] to make it [[Directed acyclic graph|acyclic]], the [[feedback arc set]] problem, is NP-complete.&lt;ref name="k72"&gt;{{harvtxt|Karp|1972}}&lt;/ref&gt;

==Exact algorithms==
The corresponding [[NP optimization problem]] of finding the size of a minimum feedback vertex set can be solved in time  ''O''(1.7347&lt;sup&gt;''n''&lt;/sup&gt;), where  ''n'' is the number of vertices in the graph.&lt;ref&gt;{{harvtxt|Fomin|Villanger|2010}}&lt;/ref&gt; This algorithm actually computes a maximum induced forest, and when such a forest is obtained, its complement is a minimum feedback vertex set.  The number of minimal feedback vertex sets in a graph is bounded by ''O''(1.8638&lt;sup&gt;''n''&lt;/sup&gt;).{{sfnp|Fomin|Gaspers|Pyatkin|Razgon|2008}} The directed feedback vertex set problem can still be solved in time ''O*''(1.9977&lt;sup&gt;''n''&lt;/sup&gt;), where&amp;nbsp;''n'' is the number of vertices in the given directed graph.{{sfnp|Razgon|2007}} The parameterized versions of the directed and undirected problems are both [[fixed-parameter tractable]].{{sfnp|Chen|Liu|Lu|O'Sullivan|2008}}

In undirected graphs of maximum [[degree (graph theory)|degree]] three, the feedback vertex set problem can be solved in [[polynomial time]], by transforming it into an instance of the [[matroid parity problem]] for [[linear matroid]]s.{{sfnp|Ueno|Kajitani|Gotoh|1988}}

==Approximation==
The problem is [[APX|APX-complete]], which directly follows from the APX-completeness of the [[vertex cover|vertex cover problem]],&lt;ref&gt;{{harvnb|Dinur|Safra|2005}}&lt;/ref&gt; and the existence of an approximation preserving [[L-reduction]] from the vertex cover problem to it.&lt;ref name="k72"/&gt; The best known approximation algorithm on undirected graphs is by a factor of two.&lt;ref&gt;{{harvtxt|Becker|Geiger|1996}}. See also {{harvtxt|Bafna|Berman|Fujito|1999}} for an alternative approximation algorithm with the same approximation ratio.&lt;/ref&gt;

== Bounds ==
According to the [[Erdős–Pósa theorem]], the size of a minimum feedback vertex set is within a logarithmic factor of the maximum number of vertex-disjoint cycles in the given graph.{{sfnp|Erdős|Pósa|1965}}

== Applications ==
In [[operating system]]s, feedback vertex sets play a prominent role in the study of [[deadlock]] recovery. In the [[wait-for graph]] of an operating system, each directed cycle corresponds to a deadlock situation. In order to resolve all deadlocks, some blocked processes have to be aborted. A minimum feedback vertex set in this graph corresponds to a minimum number of processes that one needs to abort.{{sfnp|Silberschatz|Galvin|Gagne|2008}}

Furthermore, the feedback vertex set problem has applications in [[VLSI]] chip design.{{sfnp|Festa|Pardalos|Resende|2000}}

==Notes==
{{reflist|colwidth=25em}}

== References ==

===Research articles===
{{refbegin|colwidth=25em}}
*{{citation
 | last1 = Bafna | first1 = Vineet
 | last2 = Berman | first2 = Piotr
 | last3 = Fujito | first3 = Toshihiro
 | doi = 10.1137/S0895480196305124
 | issue = 3
 | journal = SIAM Journal on Discrete Mathematics
 | mr = 1710236
 | pages = 289–297 (electronic)
 | title = A 2-approximation algorithm for the undirected feedback vertex set problem
 | volume = 12
 | year = 1999}}.
*{{citation
 | last1 = Becker | first1 = Ann
 | last2 = Bar-Yehuda | first2 = Reuven
 | last3 = Geiger | first3 = Dan
 | arxiv = 1106.0225
 | doi = 10.1613/jair.638
 | journal = [[Journal of Artificial Intelligence Research]]
 | mr = 1765590
 | pages = 219–234
 | title = Randomized algorithms for the loop cutset problem
 | volume = 12
 | year = 2000}}
*{{citation
 | last1 = Becker | first1 = Ann
 | last2 = Geiger | first2 = Dan 
 | title = Optimization of Pearl's method of conditioning and greedy-like approximation algorithms for the vertex feedback set problem.
 | journal = [[Artificial Intelligence (journal)|Artificial Intelligence]]
 | volume = 83
 | issue = 1
 | year = 1996
 | pages = 167–188
 | doi = 10.1016/0004-3702(95)00004-6 }}
*{{citation
 | last1 = Cao | first1 = Yixin
 | last2 = Chen | first2 = Jianer
 | last3 = Liu | first3 = Yang
 | editor-last = Kaplan | editor-first = Haim
 | contribution = On feedback vertex set: new measure and new structures
 | doi = 10.1007/978-3-642-13731-0_10
 | pages = 93–104
 | series = Lecture Notes in Computer Science
 | title = Proc. 12th Scandinavian Symposium and Workshops on Algorithm Theory (SWAT 2010), Bergen, Norway, June 21-23, 2010
 | volume = 6139
 | year = 2010| arxiv = 1004.1672| bibcode = 2010LNCS.6139...93C}}
*{{citation
 | last1 = Chen | first1 = Jianer
 | last2 = Fomin | first2 = Fedor V.
 | last3 = Liu | first3 = Yang
 | last4 = Lu | first4 = Songjian
 | last5 = Villanger | first5 = Yngve
 | doi = 10.1016/j.jcss.2008.05.002
 | issue = 7
 | journal = [[Journal of Computer and System Sciences]]
 | mr = 2454063
 | pages = 1188–1198
 | title = Improved algorithms for feedback vertex set problems
 | volume = 74
 | year = 2008}}
*{{citation
 | last1 = Chen | first1 = Jianer
 | last2 = Liu | first2 = Yang
 | last3 = Lu | first3 = Songjian
 | last4 = O'Sullivan | first4 = Barry
 | last5 = Razgon | first5 = Igor
 | doi = 10.1145/1411509.1411511
 | issue = 5
 | journal = [[Journal of the ACM]]
 | mr = 2456546
 | at = Art. 21
 | title = A fixed-parameter algorithm for the directed feedback vertex set problem
 | volume = 55
 | year = 2008}}
*{{citation
 | last1 = Dinur | first1 = Irit | author1-link = Irit Dinur
 | last2 = Safra | first2 = Samuel
 | doi = 10.4007/annals.2005.162.439
 | issue = 1
 | journal = [[Annals of Mathematics]]
 | mr = 2178966
 | pages = 439–485
 | series = Second Series
 | title = On the hardness of approximating minimum vertex cover
 | url = http://www.cs.huji.ac.il/~dinuri/mypapers/vc.pdf
 | volume = 162
 | year = 2005}}
*{{citation
 | last1 = Erdős | first1 = Paul | author1-link = Paul Erdős
 | last2 = Pósa | first2 = Lajos | author2-link = Lajos Pósa (mathematician)
 | doi = 10.4153/CJM-1965-035-8
 | journal = [[Canadian Journal of Mathematics]]
 | pages = 347–352
 | title = On independent circuits contained in a graph
 | url = http://www.renyi.hu/~p_erdos/1965-05.pdf
 | volume = 17
 | year = 1965}}
*{{citation
 | last1 = Fomin | first1 = Fedor V.
 | last2 = Gaspers | first2 = Serge
 | last3 = Pyatkin | first3 = Artem
 | last4 = Razgon | first4 = Igor
 | title = On the minimum feedback vertex set problem: exact and enumeration algorithms.
 | journal = [[Algorithmica]]
 | volume = 52
 | issue = 2
 | year = 2008
 | pages = 293–307
 | doi = 10.1007/s00453-007-9152-0}}
*{{citation
 | last1 = Fomin | first1 = Fedor V.
 | last2 = Villanger | first2 = Yngve
 | contribution = Finding induced subgraphs via minimal triangulations
 | series = Leibniz International Proceedings in Informatics (LIPIcs)
 | title = Proc. 27th International Symposium on Theoretical Aspects of Computer Science (STACS 2010)
 | volume = 5
 | year = 2010
 | pages = 383–394
 | doi = 10.4230/LIPIcs.STACS.2010.2470}}
*{{citation
 | last = Karp | first = Richard M. | authorlink = Richard Karp
 | contribution = Reducibility Among Combinatorial Problems
 | location = New York
 | pages = 85–103
 | publisher = Plenum
 | title = Proc. Symposium on Complexity of Computer Computations, IBM Thomas J. Watson Res. Center, Yorktown Heights, N.Y.
 | year = 1972}}
*{{citation
 | last1 = Li | first1 = Deming
 | last2 = Liu | first2 = Yanpei
 | issue = 4
 | journal = Acta Mathematica Scientia
 | mr = 1735603
 | pages = 375–381
 | title = A polynomial algorithm for finding the minimum feedback vertex set of a 3-regular simple graph
 | volume = 19
 | year = 1999
 | doi=10.1016/s0252-9602(17)30520-9}}
*{{citation
 | last = Razgon | first = I.
 | editor1-last = Italiano | editor1-first = Giuseppe F. | editor1-link = Giuseppe F. Italiano
 | editor2-last = Moggi | editor2-first = Eugenio
 | editor3-last = Laura | editor3-first = Luigi
 | contribution = Computing minimum directed feedback vertex set in O*(1.9977&lt;sup&gt;''n''&lt;/sup&gt;)
 | pages = 70–81
 | publisher = World Scientific
 | title = Proceedings of the 10th Italian Conference on Theoretical Computer Science
 | url = http://www.cs.le.ac.uk/people/ir45/papers/ictcsIgorCamera.pdf
 | year = 2007}}
*{{citation
 | last1 = Ueno | first1 = Shuichi
 | last2 = Kajitani | first2 = Yoji
 | last3 = Gotoh | first3 = Shin'ya
 | doi = 10.1016/0012-365X(88)90226-9
 | issue = 1-3
 | journal = [[Discrete Mathematics (journal)|Discrete Mathematics]]
 | mr = 975556
 | pages = 355–360
 | title = On the nonseparating independent set problem and feedback set problem for graphs with no vertex degree exceeding three
 | volume = 72
 | year = 1988}}
{{refend}}

===Textbooks and survey articles===
*{{citation
 | last1 = Festa | first1 = P.
 | last2 = Pardalos | first2 = P. M.
 | last3 = Resende | first3 = M.G.C.
 | editor1-last = Du | editor1-first = D.-Z.
 | editor2-last = Pardalos | editor2-first = P. M.
 | contribution = Feedback set problems
 | pages = 209–259
 | publisher = Kluwer Academic Publishers
 | title = Handbook of Combinatorial Optimization, Supplement vol. A
 | url = http://www.research.att.com/~mgcr/doc/sfsp.pdf
 | year = 2000}}
*{{citation
 | last1 = Garey | first1 = Michael R. | author1-link = Michael R. Garey
 | last2 = Johnson | first2 = David S. | author2-link = David S. Johnson
 | at =  A1.1: GT7, p.&amp;nbsp;191
 | isbn = 0-7167-1045-5
 | publisher = W.H. Freeman
 | title = [[Computers and Intractability: A Guide to the Theory of NP-Completeness]]
 | year = 1979}}
*{{citation
 | last1 = Silberschatz | first1 = Abraham
 | last2 = Galvin | first2 = Peter Baer
 | last3 = Gagne | first3 = Greg
 | edition = 8th
 | isbn = 978-0-470-12872-5
 | publisher = John Wiley &amp; Sons. Inc
 | title = Operating System Concepts
 | year = 2008}}

{{DEFAULTSORT:Feedback Vertex Set}}
[[Category:NP-complete problems]]
[[Category:Computational problems in graph theory]]</text>
      <sha1>k71bhqqt9vnv7bz6siz5jt3gdi8qn07</sha1>
    </revision>
  </page>
  <page>
    <title>Ferdinand Rudio</title>
    <ns>0</ns>
    <id>40761970</id>
    <revision>
      <id>854226222</id>
      <parentid>803972354</parentid>
      <timestamp>2018-08-09T20:19:04Z</timestamp>
      <contributor>
        <username>Suslindisambiguator</username>
        <id>12329968</id>
      </contributor>
      <comment>added ref to 1912 ICM talk by Rudio</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4936">[[File:ETH-BIB-Rudio, Ferdinand (1856-1929)-Portrait-Portr 09009.tif|thumb|Ferdinand Rudio, 1884]] 
'''Ferdinand Rudio''' (born 2 August 1856 in [[Wiesbaden]], died 21 June 1929 in [[Zurich]]) was a German and Swiss mathematician and [[History of mathematics|historian of mathematics]].&lt;ref name="mactutor"&gt;{{MacTutor|id=Rudio|title=Ferdinand Rudio}}&lt;/ref&gt;&lt;ref name="ds02"&gt;{{citation|title=Writing the History of Mathematics – Its Historical Development|volume=27|series=Science Networks|editor1-first=Joseph W.|editor1-last=Dauben|editor2-first=Christoph J.|editor2-last=Scriba|publisher=Springer|year=2002|isbn=9783764361679|pages=510–513}}.&lt;/ref&gt;

==Education and career==
Rudio's father and maternal grandfather were both public officials in the independent [[Duchy of Nassau]], which was annexed by [[Prussia]] when Rudio was 10. He was educated at the local gymnasium and Realgymnasium in Wiesbaden, and then in 1874 began studying at [[ETH Zurich]], then known as the Eidgenössische Polytechnikum Zürich. His initial courses in Zurich were in civil engineering, but in his second year (under the influence of [[Karl Geiser]]) he switched to mathematics and physics. Finishing at Zurich in 1877, he went on to graduate studies at the [[University of Berlin]] from 1877 to 1880, earning his Ph.D. under the joint supervision of [[Ernst Kummer]] and [[Karl Weierstrass]].
Next, Rudio returned to ETH Zurich, earning his [[habilitation]] in 1881 and becoming at that time a [[privatdozent]]. He became an extraordinary professor at Zurich in 1885, and a full professor in 1889.&lt;ref name="mactutor"/&gt;&lt;ref&gt;{{mathgenealogy|name=Ferdinand Rudio|id=42452}}&lt;/ref&gt;

Rudio was one of the organizers of the first [[International Congress of Mathematicians]] (ICM) in 1897. He served as General Secretary of the congress, and as editor of the proceedings of the congress. He was the editor of the quarterly journal of the Zürich Natural Sciences Society from 1893 until 1912, and was also president of the society.&lt;ref name="mactutor"/&gt;

In 1919, the [[University of Zurich]] gave Rudio an honorary doctorate. By 1928, he was in poor health, and retired from his position at Zurich. He died a year later.&lt;ref name="mactutor"/&gt;

==Contributions==
Rudio's research ranged over [[group theory]], [[abstract algebra]], and [[geometry]]. His
thesis research concerned the use of [[differential equation]]s to characterize surface by the properties of their sets of [[Center of curvature|centers of curvature]],&lt;ref name="mactutor"/&gt; and he was also known for the first proof of [[Convergence (mathematics)|convergence]] of [[Viète's formula|Viète's infinite product]] for&amp;nbsp;[[Pi|π]].&lt;ref&gt;{{Citation | last1=Beckmann | first1=Petr | author1-link=Petr Beckmann | title=A history of π | publisher=The Golem Press, Boulder, Colo. | edition=2nd | isbn=978-0-88029-418-8 | mr = 0449960 | year=1971 | pages = 94–95 | url = https://books.google.com/books?id=XqqUUSyz138C&amp;pg=PA94}}.&lt;/ref&gt; He also authored the textbook ''Die Elemente Der Analytischen Geometrie'', in [[analytic geometry]], published in 1908.&lt;ref name="mactutor"/&gt;

Beginning in 1883, with a speech Rudio gave at a celebration of the centennial of [[Leonhard Euler]]'s death, Rudio became interested in Euler's life and works. At the first ICM and again at a celebration in 1907 of Euler's 200th birthday, Rudio urged the compilation of a set of Euler's complete works. In 1909 the Swiss Society of Natural Sciences took up the project and appointed Rudio as editor. He finished two volumes of this project, and assisted in the editing of the next three.&lt;ref name="mactutor"/&gt; He gave a talk ''Mitteilungen über die Eulerausgabe'' (news about the Euler edition) at the fifth ICM in [[Cambridge, England]] in August 1912.&lt;ref&gt;{{cite book|chapter=Mitteilungen über die Eulerausgabe|author=Rudio, F.|volume=vol. 2|pages=529–532|title=Proceedings of the Fifth International Congress of Mathematicians (Cambridge, 22–28 August 1912)|chapter-url=https://babel.hathitrust.org/cgi/pt?id=njp.32101039891054;view=1up;seq=535}}&lt;/ref&gt; By the time he retired as general editor of the series in 1928, 20 volumes of the series had been published of what would eventually be over 80 volumes.&lt;ref name="ds02"/&gt;

Other work in the history of mathematics by Rudio included the book ''Der Bericht des Simplicius über die Quadraturen des Antiphon und des Hippokrates'' (1902) on the ancient problem of [[squaring the circle]], and a collection of biographies of mathematicians including [[Gotthold Eisenstein]].&lt;ref name="mactutor"/&gt;

==References==
{{reflist}}

{{Authority control}}

{{DEFAULTSORT:Rudio, Ferdinand}}
[[Category:German mathematicians]]
[[Category:Swiss mathematicians]]
[[Category:Historians of mathematics]]
[[Category:ETH Zurich alumni]]
[[Category:Humboldt University of Berlin alumni]]
[[Category:ETH Zurich faculty]]
[[Category:1856 births]]
[[Category:1929 deaths]]</text>
      <sha1>bq0fjgbatbbdre359d125500tbhufz8</sha1>
    </revision>
  </page>
  <page>
    <title>Hermann Hankel</title>
    <ns>0</ns>
    <id>932515</id>
    <revision>
      <id>863661569</id>
      <parentid>863661418</parentid>
      <timestamp>2018-10-12T05:13:49Z</timestamp>
      <contributor>
        <username>Astrohist~dewiki</username>
        <id>24905949</id>
      </contributor>
      <comment>place of death corrected</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6362">{{Infobox scientist
| name              = Hermann Hankel
| native_name       = 
| native_name_lang  = 
| image             = Hankel.jpeg
| image_size        = 
| alt               = 
| caption           = 
| birth_date        = {{Birth date|df=y|1839|2|14}}
| birth_place       = [[Halle (Saale)|Halle]]
| death_date        = {{Death date and age|df=y|1873|8|29|1839|2|14}}
| death_place       = [[Schramberg]], [[Black Forest]]
| other_names       = 
| residence         = 
| citizenship       = 
| nationality       = German
| fields            = [[Mathematical analysis]]&lt;br/&gt; [[Special functions]]
| workplaces        = 
| patrons           =
| alma_mater        = 
| thesis_title      = 
| thesis_url        = 
| thesis_year       = 
| doctoral_advisor  = 
| academic_advisors = 
| doctoral_students = 
| notable_students  = 
| known_for         = 
| author_abbrev_bot = 
| author_abbrev_zoo = 
| influences        = 
| influenced        = 
| awards            = 
| signature         = &lt;!--(filename only)--&gt;
| signature_alt     = 
| website           = &lt;!-- {{URL|www.example.com}} --&gt;
| footnotes         = 
| spouse            = 
| children          = 
}}
'''Hermann Hankel''' (14 February 1839 – 29 August 1873) was a [[Germany|German]] [[mathematician]] who was born in [[Halle, Saxony-Anhalt|Halle]], [[German Confederation|Germany]] and died in [[Schramberg]] ([[Black Forest]]), [[Imperial Germany]].

He studied and worked with, among others, [[August Ferdinand Möbius|Möbius]], [[Riemann]], [[Weierstrass]] and [[Leopold Kronecker|Kronecker]].

His 1867 exposition on [[complex number]]s and [[quaternion]]s is particularly memorable. For example, Fischbein notes that he solved the problem of products of [[negative number]]s by proving the following theorem: "The only multiplication in R which may be considered as an extension of the usual multiplication in R&lt;sup&gt;+&lt;/sup&gt; by ''respecting the law of distributivity'' to the left and the right is that which conforms to the rule of signs."&lt;ref&gt;See {{harv|Fischbein|1987|p=99}}.&lt;/ref&gt;
Furthermore, Hankel draws attention&lt;ref&gt;See {{harv|Hankel|1867|p=16}}.&lt;/ref&gt; to the [[linear algebra]] that [[Hermann Grassmann]] had developed in his ''Extension Theory'' in two publications. This was the first of many references later made to Grassmann's early insights on the nature of [[space (mathematics)|space]].

==Selected publications==
* Hermann Hankel (1863) ''[https://archive.org/details/dieeulerschenin00hankgoog Die Euler'schen Integrale bei unbeschränkter Variabilität des Argumentes]'', Voss, Leipzig.
* Hermann Hankel (1867) ''[https://books.google.com/books?id=754KAAAAYAAJ&amp;printsec=frontcover&amp;dq=%22hermann+Hankel%22&amp;lr=&amp;cd=15#v=onepage&amp;q=&amp;f=false Vorlesungen über die complexen Zahlen und ihre Functionen]'', Voss, Leipzig.
* Hermann Hankel (1869) ''[https://books.google.com/books?id=TE3kAAAAMAAJ&amp;printsec=frontcover&amp;dq=%22hermann+Hankel%22&amp;lr=&amp;cd=13#v=onepage&amp;q=&amp;f=false Die Entwickelung der Mathematik in den letzten Jahrhunderten]'', Fues, Tübingen.
* Hermann Hankel (1870) ''[http://visualiseur.bnf.fr/ark:/12148/bpt6k995758 Untersuchungen über die unendlich oft oscillirenden und unstetigen Functionen]'', Fues, Tübingen.
* Hermann Hankel (1874) ''[http://visualiseur.bnf.fr/ark:/12148/bpt6k82883t Zur Geschichte der Mathematik in Alterthum und Mittelalter]'', Teubner, Leipzig.
* Hermann Hankel (1875) ''[https://books.google.com/books?id=QnQLAAAAYAAJ&amp;printsec=frontcover&amp;dq=%22hermann+Hankel%22&amp;cd=10#v=onepage&amp;q=&amp;f=false Die Elemente der projectivischen Geometrie in synthetischer Behandlung]'', Teubner, Leipzig.

== See also==
* [[Hankel matrix]]/Hankel operator
* [[Hankel function]]s in the theory of [[Bessel function]]s
* [[Hankel contour]]
* [[Hankel transform]]

==Notes==
{{reflist}}

==References==
*{{Citation
 | last =Fischbein
 | first =Efraim
 | author-link =
 | title =Intuition in Science and Mathematics: An Educational Approach
 | place =[[Dordercht]]
 | publisher =[[Kluwer Academic Publishers]]
 | series =Mathematics Education Library
 | volume =
 | year =1987
 | edition =
 | chapter =
 | chapterurl =
 | pages =xiv+225
 | language =
 | url =https://books.google.com/books?id=qqGWlEwWj5UC&amp;printsec=frontcover&amp;hl=it#v=onepage&amp;q&amp;f=true
 | doi =
 | id =
 | isbn =90-277-2506-3
 | mr =0921434 
 | zbl =
}}.
*{{Citation
 |last        = Letta
 |first       = Giorgio
 |author-link = Giorgio Letta
 |title       = Le condizioni di Riemann per l'integrabilità e il loro influsso sulla nascita del concetto di misura
 |journal     = [[Rendiconti della Accademia Nazionale delle Scienze detta dei XL, Memorie di Matematica e applicazioni]]
 |volume      = XVIII
 |issue       = 1
 |pages       = 143–169
 |origyear    = 112°
 |year        = 1994
 |language    = Italian
 |url         = http://media.accademiaxl.it/memorie/Serie5_V18_P1.pdf
 |doi         = 
 |id          = 
 |mr          = 1327463
 |zbl         = 0852.28001
 |deadurl     = yes
 |archiveurl  = https://web.archive.org/web/20140228144250/http://media.accademiaxl.it/memorie/Serie5_V18_P1.pdf
 |archivedate = 2014-02-28
 |df          = 
}}. "''Riemann's conditions for integrability and their influence on the birth of the concept of measure''" (English translation of title) is an article on the history of measure theory, analyzing deeply and comprehensively every early contribution to the field, starting from Riemann's work and going to the works of Hermann Hankel, [[Gaston Darboux]], [[Giulio Ascoli]], [[Henry John Stephen Smith]], [[Ulisse Dini]], [[Vito Volterra]], [[Paul David Gustav du Bois-Reymond]] and [[Carl Gustav Axel Harnack]].

== External links ==
* {{MacTutor Biography|id=Hankel}}
* {{MathGenealogy|id=34188}}

{{Authority control}}

{{DEFAULTSORT:Hankel, Hermann}}
[[Category:1839 births]]
[[Category:1873 deaths]]
[[Category:19th-century German mathematicians]]
[[Category:Historians of mathematics]]
[[Category:Mathematical analysts]]
[[Category:German mathematicians]]
[[Category:People from Halle (Saale)]]
[[Category:People from the Province of Saxony]]
[[Category:University of Göttingen alumni]]
[[Category:Humboldt University of Berlin alumni]]
[[Category:Leipzig University alumni]]
[[Category:Leipzig University faculty]]
[[Category:University of Erlangen-Nuremberg faculty]]
[[Category:University of Tübingen faculty]]


{{Germany-mathematician-stub}}</text>
      <sha1>6ey569jxf4hae99p900nvku9tc1mqoy</sha1>
    </revision>
  </page>
  <page>
    <title>Higher-order compact finite difference scheme</title>
    <ns>0</ns>
    <id>39585389</id>
    <revision>
      <id>863547417</id>
      <parentid>827732436</parentid>
      <timestamp>2018-10-11T13:38:34Z</timestamp>
      <contributor>
        <username>Mittal2988</username>
        <id>19135209</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6671">{{Multiple issues|{{Original research|date=June 2013}}{{Cleanup-weighted|date=August 2013}}{{Orphan|date=June 2013}}}}

[[File:HOC stencil.jpg|thumb|The nine-point HOC stencil]]
'''High-order compact finite difference schemes''' are used for solving third-order [[differential equations]] created during the study of [[Obstacle problem|obstacle boundary value problems]]. They have been shown to be highly accurate and efficient. They are constructed by modifying the second-order scheme that was developed by Noor and Al-Said in 2002. The [[convergence rate]] of the high-order compact scheme is third order, the second-order scheme is fourth order.&lt;ref&gt;{{Cite journal | last1 = Xie | first1 = S. | last2 = Li | first2 = P. | last3 = Gao | first3 = Z. | last4 = Wang | first4 = H. | title = High order compact finite difference schemes for a system of third order boundary value problem | doi = 10.1016/j.amc.2012.08.091 | journal = Applied Mathematics and Computation | volume = 219 | issue = 5 | pages = 2564 | year = 2012 | pmid =  | pmc = }}&lt;/ref&gt;

[[Differential equation]]s are essential tools in [[mathematical modelling]]. Most [[physical system]]s are described in terms of mathematical models that include convective and diffusive transport of some variables. [[Finite difference method]]s are amongst the most popular methods that have been applied most frequently in solving such differential equations.  A finite difference scheme is compact in the sense that the discretised formula comprises at most nine point [[Stencil (numerical analysis)|stencils]] which includes a [[Node (autonomous system)|node]] in the middle about which differences are taken. In addition, greater order of accuracy (more than two) justifies the terminology 'higher-order compact finite difference scheme' (HOC). This can be achieved in several ways. The higher-order compact scheme considered here &lt;ref name="Kalita 2002"&gt;Kalita JC, Dalal DC and Dass AK.,  A class of higher-order compact schemes for the unsteady two-dimensional convection-diffusion equations with variable convection coefficients., Int. J. Numer. Meth. Fluids, Vol. 101,(2002), pp. 1111–1131&lt;/ref&gt; is by using the original differential equation to substitute for the leading [[Truncation error (numerical integration)|truncation error]] terms in the finite difference equation. Overall, the scheme is found to be robust, efficient and accurate for most [[computational fluid dynamics]] (CFD) applications discussed here further.

The simplest problem for the validation of the numerical algorithms is the Lid Driven cavity problem. Computed results in form of tables, graphs and figures for a fluid with Prandtl number = 0.71 with Rayleigh number (Ra) ranging from 10&lt;sup&gt;3&lt;/sup&gt; to 10&lt;sup&gt;7&lt;/sup&gt; are available in the literature.&lt;ref name="Kalita 2002"/&gt; The efficacy of the scheme is proved when it very clearly captures the secondary and tertiary vortices at the sides of the cavity at high values of Ra.

Another milestone was the development of these schemes for solving two dimensional steady/unsteady convection diffusion equations. A comprehensive study of flow past an impulsively started circular cylinder was made.&lt;ref&gt;J. C. Kalita, and R. K. Ray., A transformation-free HOC scheme for incompressible viscous flows past an impulsively started circular cylinder, Int. J. Numer. Meth. Fluids, Vol. 228,(2009), pp. 5207–5236&lt;/ref&gt; The problem of flow past a circular cylinder has continued to generate tremendous interest{{Clarify|reason=vague|date=January 2016}} amongst researchers working in CFD mainly because it displays almost all the fluid mechanical phenomena for incompressible, viscous flows in the simplest of geometrical settings. It was able to analyze and visualize the flow patterns more accurately for Reynold's number (Re) ranging from 10 to 9500 compared to the existing numerical results. This was followed by its extension to rotating counterpart of the cylinder surface for Re ranging from 200 to 1000.&lt;ref&gt;R. K. Ray., A transformation free HOC scheme for incompressible viscous flow past a rotating and translating circular cylinder, J. Sci. Comput., Vol. 46,(2011), pp. 265–293&lt;/ref&gt; More complex phenomenon that involves a circular cylinder undergoing rotational oscillations while translating in a fluid is studied for Re as high as 500.&lt;ref&gt;Rajendra K. Ray and H. V. R. Mittal, A transform-free HOC scheme for incompressible viscous flow past a rotationally oscillating circular cylinder, Proc. World Academy of Science Engineering and Technology, Bangkok, Issue 72,(December 2012), pp. 1069–1075&lt;/ref&gt; &lt;ref&gt;H. V. R. Mittal, Rajendra K. Ray and Qasem M. Al-Mdallal, A numerical study of initial flow past an impulsively started rotationally oscillating circular cylinder using a transformation-free HOC scheme, Physics of Fluids, vol. 29, no. 9 (2017), pp. 093603&lt;/ref&gt; &lt;ref&gt;H. V. R. Mittal, Qasem M. Al-Mdallal and Rajendra K. Ray, Locked-on vortex shedding modes from a rotationally oscillating circular cylinder, Ocean Engineering, vol. 146 (2017), pp. 324-338&lt;/ref&gt; 

Another benchmark in the history is its extension to multiphase flow phenomena. Natural processes such as gas bubble in oil, ice melting, wet steam  are observed everywhere in nature.  Such processes also play an important role with the practical applications in the area of biology, medicine, [[environmental remediation]]. The scheme has been successively implemented to solve one and two dimensional elliptic and parabolic equation with discontinuous coefficients and singular source terms.&lt;ref&gt;Rajendra K. Ray, J. C. Kalita, and A. K. Dass, An efficient HOC scheme for transient convection–diffusion reaction equations with discontinuous coefficients and singular source terms, Proc. Appl. Math. Mech., vol. 7, no. 1(2007), pp. 1025603–1025604&lt;/ref&gt; These type of problems hold importance numerically because they usually lead to non-smooth or discontinuous solutions across the interfaces.  Expansion of this idea from fixed to moving interfaces with both regular and irregular geometries is currently going on &lt;ref&gt;H. V. R. Mittal, Jiten C. Kalita and Rajendra K. Ray, A class of finite difference schemes for interface problems with an HOC approach, International Journal for Numerical Methods in Fluids, vol. 82, no. 9 (2016), pp. 567-606&lt;/ref&gt;, &lt;ref&gt; H. V. R. Mittal, Ray, Rajendra K. Ray, Solving Immersed Interface Problems Using a New Interfacial Points-Based Finite Difference Approach, SIAM Journal on Scientific Computing, vol. 40, no. 3 (2018), pp. A1860-A1883 &lt;/ref&gt;.

==References==
{{reflist}}

[[Category:Finite differences]]
[[Category:Numerical differential equations]]</text>
      <sha1>q1t804uo6uf5k4ya8uxnf37v26db2kn</sha1>
    </revision>
  </page>
  <page>
    <title>Hélène Barcelo</title>
    <ns>0</ns>
    <id>55711992</id>
    <revision>
      <id>864479665</id>
      <parentid>864479616</parentid>
      <timestamp>2018-10-17T13:59:49Z</timestamp>
      <contributor>
        <username>Jmertel23</username>
        <id>32942831</id>
      </contributor>
      <minor/>
      <comment>removing whitespace</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5172">{{short description|mathematician}}
{{Infobox academic
| honorific_prefix   = &lt;!-- see [[MOS:HONORIFIC]] --&gt;
| name               = 
| honorific_suffix   = 
| image              = 
| image_size         = 
| alt                = 
| caption            = 
| native_name        = 
| native_name_lang   = 
| birth_name         = &lt;!-- use only if different from full/othernames --&gt;
| birth_date         = {{birth year and age|1954}}  
| birth_place        = [[Quebec, Canada]]
| death_date         = &lt;!-- {{death date and age|YYYY|MM|DD|YYYY|MM|DD}} (death date then birth date) --&gt;
| death_place        = 
| death_cause        = 
| region             = 
| nationality        = 
| citizenship        = 
| residence          = United States
| other_names        = 
| occupation         = 
| period             = 
| known_for          = 
| home_town          = 
| title              = [[Professor emeritus]]
| boards             = &lt;!--board or similar positions extraneous to main occupation--&gt;
| spouse             = 
| children           = 
| parents            =
| relatives          =
| awards             = &lt;!--notable national level awards only--&gt;
| website            = 
| education          = 
| alma_mater         = [[University of California, San Diego]]
| thesis_title       = On the Action of the Symmetric Group on the Free Lie Algebra and on the Homology and Cohomology of the Partition Lattice
| thesis_url         = 
| thesis_year        = 1988
| school_tradition   = 
| doctoral_advisor   = [[Adriano Garsia]]
| academic_advisors  = 
| influences         = &lt;!--must be referenced from a third party source--&gt;
| era                = 
| discipline         = [[Mathematics]]
| sub_discipline     = [[Algebraic combinatorics]]
| workplaces         = [[Arizona State University]] &lt;br&gt; [[Mathematical Sciences Research Institute]]
| doctoral_students  = &lt;!--only those with WP articles--&gt;
| notable_students   = &lt;!--only those with WP articles--&gt;
| main_interests     = 
| notable_works      = 
| notable_ideas      = 
| influenced         = &lt;!--must be referenced from a third party source--&gt;
| signature          = 
| signature_alt      = 
| signature_size     = 
| footnotes          = 
}}

'''Hélène Barcelo''' (born 1954)&lt;ref&gt;Birth year from [http://worldcat.org/identities/lccn-n94077088/ WorldCat]&lt;/ref&gt; is a mathematician from Québec specializing in [[algebraic combinatorics]]. Within that field, her interests include combinatorial [[representation theory]], [[homotopy theory]], and [[Arrangement of hyperplanes|arrangements of hyperplanes]].{{r|depdir}}
She is a professor emeritus of mathematics at [[Arizona State University]], and deputy director of the [[Mathematical Sciences Research Institute]] (MSRI). She was editor-in-chief of the ''[[Journal of Combinatorial Theory]]'', Series A, from 2001 to 2009.{{r|msri}}

Barcelo completed her Ph.D. from the [[University of California, San Diego]] in 1988. Her dissertation, ''On the Action of the Symmetric Group on the Free Lie Algebra and on the Homology and Cohomology of the Partition Lattice'', was supervised by [[Adriano Garsia]].{{r|mgp}} She joined the Arizona State faculty after postdoctoral studies at the [[University of Michigan]].{{r|msri}} She retired from Arizona State, becoming a professor emerita there, and became deputy director at MSRI in 2008.{{r|depdir}}

She was elected to the 2018 class of [[fellow]]s of the [[American Mathematical Society]]{{r|fams}}
and the 2019 class of fellows of the [[Association for Women in Mathematics]].{{r|fawm}}

==References==
{{reflist|refs=

&lt;ref name=depdir&gt;{{citation|url=http://www.msri.org/people/staff/hbarcelo/|title=Hélène Barcelo: Deputy Director|publisher=Mathematical Sciences Research Institute|accessdate=2017-11-20}}&lt;/ref&gt;

&lt;ref name=fams&gt;{{citation|url=http://ams.org/profession/ams-fellows/new-fellows|title=2018 Class of the Fellows of the AMS|publisher=[[American Mathematical Society]]|accessdate=2017-11-03}}&lt;/ref&gt;

&lt;ref name=fawm&gt;{{citation|url=https://sites.google.com/site/awmmath/awm-fellows|title=2019 Class of AWM Fellows|publisher=[[Association for Women in Mathematics]]|accessdate=2018-10-07}}&lt;/ref&gt;

&lt;ref name=mgp&gt;{{mathgenealogy|id=39975}}&lt;/ref&gt;

&lt;ref name=msri&gt;{{citation|url=https://www.msri.org/people/12615|title=Personal Profile of Dr. Hélène Barcelo|publisher=Mathematical Sciences Research Institute|accessdate=2017-11-04}}&lt;/ref&gt;

}}

==External links==
*[http://www.msri.org/m/people/staff/hbarcelo/ Home page]

{{Authority control}}
{{DEFAULTSORT:Barcelo, Helene}}
[[Category:1954 births]]
[[Category:Living people]]
[[Category:20th-century American mathematicians]]
[[Category:21st-century American mathematicians]]
[[Category:20th-century Canadian mathematicians]]
[[Category:21st-century Canadian mathematicians]]
[[Category:Canadian women mathematicians]]
[[Category:American women mathematicians]]
[[Category:Arizona State University faculty]]
[[Category:Fellows of the American Mathematical Society]]
[[Category:Fellows of the Association for Women in Mathematics]]
[[Category:Canadian expatriate academics in the United States]]
[[Category:University of Michigan people]]


{{math-bio-stub}}</text>
      <sha1>0ydpzud2w292g5anvjdlk018716dl7d</sha1>
    </revision>
  </page>
  <page>
    <title>Incomplete LU factorization</title>
    <ns>0</ns>
    <id>8517337</id>
    <revision>
      <id>857135360</id>
      <parentid>830365386</parentid>
      <timestamp>2018-08-29T19:55:48Z</timestamp>
      <contributor>
        <username>Gene Wilson</username>
        <id>6713489</id>
      </contributor>
      <comment>/* Generalizations */AccessDate parameter removed.....  No URL in citation.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5461">In [[numerical linear algebra]], an '''incomplete LU factorization''' (abbreviated as '''ILU''') of a [[matrix (mathematics)|matrix]] is a [[sparse matrix|sparse]] approximation of the [[LU factorization]] often used as a [[preconditioner]].

== Introduction ==
Consider a sparse linear system &lt;math&gt;Ax = b&lt;/math&gt;. These are often solved by computing the factorization &lt;math&gt;A = LU&lt;/math&gt;, with ''L'' [[Triangular_matrix#Unitriangular_matrix|lower unitriangular]] and ''U'' [[Triangular_matrix|upper triangular]].
One then solves &lt;math&gt;Ly = b&lt;/math&gt;, &lt;math&gt;Ux = y&lt;/math&gt;, which can be done efficiently because the matrices are triangular.

For a typical sparse matrix, the LU factors can be much less sparse than the original matrix &amp;mdash; a phenomenon called ''fill-in''. 
The memory requirements for using a direct solver can then become a bottleneck in solving linear systems. One can combat this problem by using fill-reducing reorderings of the matrix's unknowns, such as the [[Cuthill-McKee algorithm|Cuthill-McKee ordering]].

An incomplete factorization instead seeks triangular matrices ''L'', ''U'' such that &lt;math&gt;A \approx LU&lt;/math&gt; rather than &lt;math&gt;A = LU&lt;/math&gt;. Solving for &lt;math&gt;LUx = b&lt;/math&gt; can be done quickly but does not yield the exact solution to &lt;math&gt;Ax = b&lt;/math&gt;. So, we instead use the matrix &lt;math&gt;M = LU&lt;/math&gt; as a preconditioner in another iterative solution algorithm such as the [[conjugate gradient method]] or [[GMRES]].

== Definition ==
For a given matrix &lt;math&gt; A \in \R^{n \times n} &lt;/math&gt; one defines the graph &lt;math&gt; G(A) &lt;/math&gt; as
:&lt;math&gt;
  G(A) := \left\lbrace (i,j) \in \N^2 : A_{ij} \neq 0 \right\rbrace \,,
&lt;/math&gt;
which is used to define the conditions a ''sparsity patterns'' &lt;math&gt; S &lt;/math&gt; needs to fulfill
:&lt;math&gt;
  S \subset \left\lbrace 1, \dots , n \right\rbrace^2
  \,, \quad
  \left\lbrace (i,i) : 1 \leq i \leq n \right\rbrace \subset S
  \,, \quad
  G(A) \subset S 
  \,.
&lt;/math&gt;

A decomposition of the form &lt;math&gt; A = LU - R &lt;/math&gt; where the following hold
* &lt;math&gt; L \in \R^{n \times n} &lt;/math&gt; is a [[Triangular_matrix#Unitriangular_matrix|lower unitriangular]] matrix 
* &lt;math&gt; U \in \R^{n \times n} &lt;/math&gt; is an [[Triangular_matrix|upper triangular]] matrix 
* &lt;math&gt; L,U &lt;/math&gt; are zero outside of the sparsity pattern: &lt;math&gt; L_{ij}=U_{ij}=0 \quad \forall \; (i,j) \notin S &lt;/math&gt;
* &lt;math&gt; R \in \R^{n \times n} &lt;/math&gt; is zero within the sparsity pattern: &lt;math&gt; R_{ij}=0 \quad \forall \; (i,j) \in S &lt;/math&gt;
is called an '''incomplete LU decomposition''' (w.r.t. the sparsity pattern &lt;math&gt; S &lt;/math&gt;).

The sparsity pattern of ''L'' and ''U'' is often chosen to be the same as the sparsity pattern of the original matrix ''A''. If the underlying matrix structure can be referenced by pointers instead of copied, the only extra memory required is for the entries of ''L'' and ''U''. This preconditioner is called ILU(0).

== Stability ==
Concerning the stability of the ILU the following theorem was proven by Meijerink an van der Vorst&lt;ref&gt;{{Cite journal|last=Meijerink|first=J. A.|last2=Vorst|first2=Van Der|last3=A|first3=H.|date=1977|title=An iterative solution method for linear systems of which the coefficient matrix is a symmetric 𝑀-matrix|url=http://www.ams.org/home/page/|journal=Mathematics of Computation|language=en-US|volume=31|issue=137|pages=148–162|doi=10.1090/S0025-5718-1977-0438681-4|issn=0025-5718}}&lt;/ref&gt;.

Let &lt;math&gt; A &lt;/math&gt; be an [[M-matrix]], the (complete) LU decomposition given by &lt;math&gt; A=\hat{L} \hat{U} &lt;/math&gt;, and the ILU by &lt;math&gt; A=LU-R &lt;/math&gt;.
Then
:&lt;math&gt;
  |L_{ij}| \leq |\hat{L}_{ij}|
  \quad \forall \; i,j
&lt;/math&gt;
holds.
Thus, the ILU is at least as stable as the (complete) LU decomposition.

== Generalizations ==
One can obtain a more accurate preconditioner by allowing some level of extra fill in the factorization. A common choice is to use the sparsity pattern of ''A&lt;sup&gt;2&lt;/sup&gt;'' instead of ''A''; this matrix is appreciably more dense than ''A'', but still sparse over all. This preconditioner is called ILU(1). One can then generalize this procedure; the ILU(k) preconditioner of a matrix ''A'' is the incomplete LU factorization with the sparsity pattern of the matrix ''A&lt;sup&gt;k+1&lt;/sup&gt;''.

More accurate ILU preconditioners require more memory, to such an extent that eventually the running time of the algorithm increases even though the total number of iterations decreases. Consequently, there is a cost/accuracy trade-off that users must evaluate, typically on a case-by-case basis depending on the family of linear systems to be solved.

The ILU factorization can be performed as a [[fixed-point iteration]] in a highly parallel way.&lt;ref&gt;{{cite journal|last1=Chow|first1=Edmond|last2=Patel|first2=Aftab|title=Fine-grained parallel incomplete LU factorization|journal=SIAM Journal on Scientific Computing|date=2015|volume=37|issue=2|page=C169-C193|ref=iterativeILU}}&lt;/ref&gt;

== See also ==
* [[Incomplete Cholesky factorization]]

==References==
* {{Citation | last1=Saad | first1=Yousef | authorlink=Yousef Saad |title=Iterative methods for sparse linear systems | publisher=PWS | location=Boston | edition=1st | isbn=978-0-534-94776-7 | year=1996}}. See Section 10.3 and further.
&lt;references /&gt;


==External links==
* [http://www.cfd-online.com/Wiki/Incomplete_LU_factorization_-_ILU Incomplete LU Factorization on CFD Wiki]

{{mathapplied-stub}}

{{Numerical linear algebra}}

[[Category:Numerical linear algebra]]</text>
      <sha1>hc4dvvjqiny98cw3jo0yth95bh6wm0o</sha1>
    </revision>
  </page>
  <page>
    <title>Independence of premise</title>
    <ns>0</ns>
    <id>22833480</id>
    <revision>
      <id>607144839</id>
      <parentid>585909780</parentid>
      <timestamp>2014-05-05T09:48:45Z</timestamp>
      <contributor>
        <username>Yobot</username>
        <id>7328338</id>
      </contributor>
      <minor/>
      <comment>[[WP:CHECKWIKI]] error fixes using [[Project:AWB|AWB]] (10093)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2931">In [[proof theory]] and [[constructive mathematics]], the principle of '''independence of premise''' states that if φ and ∃ ''x'' θ are sentences in a formal theory and  {{nowrap|1=&amp;phi; &amp;rarr; &amp;exist; ''x'' &amp;theta;}} is provable, then {{nowrap|1=&amp;exist; ''x'' (&amp;phi; &amp;rarr; &amp;theta;)}} is provable. Here ''x'' cannot be a [[free variable]] of φ.

The principle is valid in classical logic. Its main application is in the study of intuitionistic logic, where the principle is not always valid.

== In classical logic ==

The principle of independence of premise is valid in classical logic because of the [[law of the excluded middle]]. Assume that  {{nowrap|1=&amp;phi; &amp;rarr; &amp;exist; ''x'' &amp;theta;}} is provable. Then, if φ holds, there is an ''x'' satisfying &lt;span class="nowrap"&gt;φ → θ&lt;/span&gt; but if φ does not hold then ''any'' ''x'' satisfies &lt;span class="nowrap"&gt;φ → θ&lt;/span&gt;. In either case, there is some ''x'' such that φ→θ. Thus {{nowrap|1=&amp;exist; ''x'' (&amp;phi; &amp;rarr; &amp;theta;)}} is provable.

== In intuitionistic logic ==

The principle of independence of premise is not generally valid in intuitionistic logic (Avigad and Feferman 1999). This can be illustrated by the [[BHK interpretation]], which says that in order to prove {{nowrap|1=&amp;phi; &amp;rarr; &amp;exist; ''x'' &amp;theta;}} intuitionistically, one must create a function that takes a proof of φ and returns a proof of {{nowrap|&amp;exist; ''x'' &amp;theta;}}. Here the proof itself is an input to the function and may be used to construct ''x''. On the other hand, a proof of {{nowrap|1=&amp;exist; ''x'' (&amp;phi; &amp;rarr; &amp;theta;)}} must first demonstrate a particular ''x'', and then provide a function that converts a proof of φ into a proof of θ in which ''x'' has that particular value.

As a [[weak counterexample]], suppose θ(''x'') is some decidable predicate of a natural number such that it is not known whether any ''x'' satisfies θ. For example, θ may say that ''x'' is a formal proof of some mathematical conjecture whose provability is not known. Let φ the formula {{nowrap|1=&amp;exist; ''z'' &amp;theta;(''z'')}}. Then {{nowrap|1=&amp;phi; &amp;rarr; &amp;exist; ''x'' &amp;theta;}} is trivially provable. However, to prove  {{nowrap|1=&amp;exist; ''x'' (&amp;phi; &amp;rarr; &amp;theta;)}}, one must demonstrate a particular value of ''x'' such that, if any value of ''x'' satisfies θ, then the one that was chosen satisfies θ. This cannot be done without already knowing whether {{nowrap|1=&amp;exist; ''x'' &amp;theta;}} holds, and thus {{nowrap|1=&amp;exist; ''x'' (&amp;phi; &amp;rarr; &amp;theta;)}} is not intuitionistically provable in this situation.

== References ==

* {{cite book
| title = Gödel's functional ("Dialectica") interpretation
| author = Jeremy Avigad and Solomon Feferman
| publisher =  in S. Buss ed., The Handbook of Proof Theory, North-Holland
| year = 1999
| pages = 337&amp;ndash;405
| url = http://www.andrew.cmu.edu/user/avigad/Papers/dialect.pdf
}}

[[Category:Predicate logic]]</text>
      <sha1>3je7vqpelvb6azpa2dg6d1e7d1rj8ds</sha1>
    </revision>
  </page>
  <page>
    <title>John Morgan (mathematician)</title>
    <ns>0</ns>
    <id>8487966</id>
    <revision>
      <id>847876177</id>
      <parentid>842718440</parentid>
      <timestamp>2018-06-28T11:32:21Z</timestamp>
      <contributor>
        <ip>128.179.157.6</ip>
      </contributor>
      <comment>Morgan is not anymore director of the Simons Center for Geometry and Physics.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="7809">{{BLP sources|date=February 2013}}
{{Infobox scientist
| name              = John Morgan
| image             = &lt;!--(filename only)--&gt;
| image_size        = 
| caption           = 
| birth_date        = {{birth date and age|1946|3|21}}
| birth_place       = [[Philadelphia]]
| death_date        = 
| death_place       = 
| nationality       = [[United States|American]]
| fields            = [[Mathematics]]
| workplaces        = [[Stony Brook University]] &lt;br&gt; [[Columbia University]]
| alma_mater        = [[Rice University]]
| doctoral_advisor  = [[Morton L. Curtis]]
| doctoral_students = [[:ja:小島定吉|Sadayoshi Kojima]]&lt;br&gt;[[Peter Ozsváth]]&lt;br&gt;[[Zoltán Szabó (mathematician)|Zoltán Szabó]]&lt;br&gt;[[Pedram Safari]]
| known_for         = 
| awards            = 
}}
'''John Willard Morgan''' (born March 21, 1946) is an [[United States|American]] [[mathematician]], with contributions to [[topology]] and [[geometry]].

==Life==
He received his [[Bachelor of Arts|B.A.]] in 1968 and [[Doctor of Philosophy|Ph.D.]] in 1969, both from [[Rice University]].  His Ph.D. thesis, entitled ''Stable tangential homotopy equivalences'', was written under the supervision of  [[Morton L. Curtis]].  He was an instructor at [[Princeton University]] from 1969 to 1972, and an assistant professor at [[MIT]] from 1972 to 1974.  He has been on the faculty at [[Columbia University]] since 1974.  In July 2009, he moved to [[Stony Brook University]] to become the first director of the [[Simons Center for Geometry and Physics]], a research center devoted to the interface between mathematics and physics.

He is an editor of the [[Journal of the American Mathematical Society]] and [[Geometry and Topology]].

He collaborated with [[Gang Tian]] in verifying [[Grigori Perelman]]'s proof of the [[Poincaré conjecture]].&lt;ref name="morgan and tian"&gt;{{cite arXiv | eprint = math.DG/0607607 | title = Ricci Flow and the Poincaré Conjecture | first = John W. | last = Morgan |author2=Gang Tian | date = 25 July 2006}}&lt;/ref&gt;  The Morgan–Tian team was one of three teams formed for this purpose; the other teams were those of [[Huai-Dong Cao]] and [[Xi-Ping Zhu]], and [[Bruce Kleiner]] and [[John Lott (mathematician)|John Lott]].  Morgan gave a [[list of International Congresses of Mathematicians Plenary and Invited Speakers|plenary lecture at the International Congress of Mathematicians]] in [[Madrid]] on August 24, 2006, declaring that "in 2003, Perelman solved the [[Poincaré conjecture]]."

==Awards and honors==
In 2008 he was awarded a [[Gauss Lectureship]] by the [[German Mathematical Society]]. In 2009 he was elected to the [[National Academy of Sciences]]. In 2012 he became a fellow of the [[American Mathematical Society]].&lt;ref&gt;[http://www.ams.org/profession/fellows-list List of Fellows of the American Mathematical Society], retrieved 2013-02-10.&lt;/ref&gt;

==Selected publications==

===Articles===
* [[Pierre Deligne]], [[Phillip Griffiths]], John Morgan, [[Dennis Sullivan]], ''Real homotopy theory of Kähler manifolds'', [[Inventiones Mathematicae]] 29 (1975), no. 3, 245–274. {{MR|0382702}}
* John W. Morgan, ''The algebraic topology of smooth algebraic varieties'', [[Publications Mathématiques de l'IHÉS]] 48 (1978), 137–204. {{MR|0516917}}
* John W. Morgan, ''Trees and hyperbolic geometry'', Proceedings of the International Congress of Mathematicians, Vol. 1, 2 (Berkeley, CA, 1986), 590–597, Amer. Math. Soc., Providence, RI, 1987.  {{MR|0934260}}
* John W. Morgan, [[Zoltán Szabó (mathematician)|Zoltán Szabó]], [[Clifford Taubes|Clifford Henry Taubes]], ''A product formula for the Seiberg-Witten invariants and the generalized Thom conjecture'', Journal of Differential Geometry 44 (1996), no. 4, 706–788.  {{MR|1438191}}
* John W. Morgan, ''Recent progress on the Poincaré conjecture and the classification of 3-manifolds'', [[Bulletin of the American Mathematical Society]] 42 (2005), no. 1, 57–78.  {{MR|2115067}}

===Books===
* Quantum fields and strings: a course for mathematicians. Vol. 1, 2. Material from the Special Year on Quantum Field Theory held at the Institute for Advanced Study, Princeton, NJ, 1996–1997. Edited by [[Pierre Deligne]], [[Pavel Etingof]], [[Daniel S. Freed]], [[Lisa C. Jeffrey]], [[David Kazhdan]], John W. Morgan, [[David R. Morrison (mathematician)|David R. Morrison]] and [[Edward Witten]]. American Mathematical Society, Providence, RI; Institute for Advanced Study (IAS), Princeton, NJ, 1999. Vol. 1: xxii+723 pp.; Vol. 2: pp. i--xxiv and 727–1501. {{ISBN|0-8218-1198-3}}, 81-06 (81T30 81Txx)
* Phillip A. Griffiths, John W. Morgan, "Rational homotopy theory and differential forms", Progress in Mathematics, vol. 16, Birkhäuser, Boston, MA, 1981. {{ISBN|3-7643-3041-4}}&lt;ref&gt;{{cite journal|author=Chen, Kuo-Tsai|title=Review: ''Rational homotopy theory and differential forms'', by P. A. Griffiths and J. W. Morgan|journal=Bull. Amer. Math. Soc. (N.S.)|year=1983|volume=8|issue=3|pages=496–498|url=http://www.ams.org/journals/bull/1983-08-03/S0273-0979-1983-15135-2/|doi=10.1090/s0273-0979-1983-15135-2}}&lt;/ref&gt;
* "The [[Smith conjecture]]", Papers presented at the symposium held at Columbia University, New York, 1979. Edited by John W. Morgan and [[Hyman Bass]]. Pure and Applied Mathematics, vol. 112, [[Academic Press]], Orlando, FL, 1984.  {{ISBN|0-12-506980-4}}
* John W. Morgan, Tomasz Mrowka, Daniel Ruberman, "The L&lt;sup&gt;2&lt;/sup&gt;-moduli space and a vanishing theorem for Donaldson polynomial invariants", Monographs in Geometry and Topology, II. International Press, Cambridge, MA, 1994.  {{ISBN|1-57146-006-3}}
* Robert Friedman, John W. Morgan, "Smooth four-manifolds and complex surfaces", [[Ergebnisse der Mathematik und ihrer Grenzgebiete]], vol. 27, [[Springer-Verlag]], Berlin, 1994.  {{ISBN|3-540-57058-6}}
* John W. Morgan, "The Seiberg-Witten equations and applications to the topology of smooth four-manifolds", Mathematical Notes, vol. 44, [[Princeton University Press]], Princeton, NJ, 1996. {{ISBN|0-691-02597-5}}
*{{cite book | first = John | last = Morgan |author2=[[Gang Tian]]  | title = Ricci Flow and the Poincaré Conjecture|series=[[Clay Mathematics Monographs]]|volume=3 |publisher= [[Clay Mathematics Institute]] |isbn = 0-8218-4328-1| year = 2007 }}
*{{Cite book
 | title = Ricci Flow and Geometrization of 3-Manifolds
 | year = 2010
 | series = University Lecture Series
 | url = http://www.ams.org/bookstore-getitem/item=ulect-53
 | last1 = Morgan	 | first1 = John W.
 | last2 = Fong	 | first2 = Frederick Tsz-Ho
 | isbn = 978-0-8218-4963-7
 | accessdate = 2010-09-26	}}

==References==
&lt;!--See http://en.wikipedia.org/wiki/Wikipedia:Footnotes
for an explanation of how to generate footnotes using the &lt;ref(erences/)&gt; tags--&gt;
{{Reflist}}

==External links==
*{{MathGenealogy |id=389 |name=John Morgan }}
*[https://web.archive.org/web/20061019094202/http://www.math.columbia.edu/~jm/ Home page] at Columbia University
*[http://www.ims.cuhk.edu.hk/talkwithmasters/jmorgan_cv.pdf Biographical sketch]  at the [[Chinese University of Hong Kong]]
*[https://web.archive.org/web/20070209190512/http://www.math.columbia.edu/~petero/MorganConference.html Conference in Honor of the 60th Birthday of John Morgan] at Columbia University

{{Authority control}}

{{DEFAULTSORT:Morgan, John}}
[[Category:20th-century American mathematicians]]
[[Category:21st-century American mathematicians]]
[[Category:Columbia University faculty]]
[[Category:State University of New York at Stony Brook faculty]]
[[Category:Princeton University staff]]
[[Category:Geometers]]
[[Category:Living people]]
[[Category:Rice University alumni]]
[[Category:Topologists]]
[[Category:Fellows of the American Mathematical Society]]
[[Category:Members of the United States National Academy of Sciences]]
[[Category:1946 births]]</text>
      <sha1>ksv06lxvkkxop6jsuolhco7heb5triw</sha1>
    </revision>
  </page>
  <page>
    <title>Kleene–Brouwer order</title>
    <ns>0</ns>
    <id>9042378</id>
    <revision>
      <id>814879958</id>
      <parentid>790712167</parentid>
      <timestamp>2017-12-11T13:30:00Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 1 sources and tagging 0 as dead. #IABot (v1.6.1)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6813">In [[descriptive set theory]], the '''Kleene–Brouwer order''' or '''Lusin–Sierpiński order'''&lt;ref name="moschovakis"&gt;{{Citation | last1=Moschovakis | first1=Yiannis | title=Descriptive Set Theory | publisher=American Mathematical Society | location=Rhode Island | edition=2nd | isbn=978-0-8218-4813-5 | year=2009|pages=148–149, 203–204}}&lt;/ref&gt; is a [[linear order]] on finite sequences over some linearly ordered set &lt;math&gt;(X, &lt;)&lt;/math&gt;, that differs from the more commonly used [[lexicographic order]] in how it handles the case when one sequence is a [[substring|prefix]] of the other. In the Kleene–Brouwer order, the prefix is later than the longer sequence containing it, rather than earlier.

The Kleene–Brouwer order generalizes the notion of a [[postorder traversal]] from finite trees to trees that are not necessarily finite. For trees over a well-ordered set,  the Kleene–Brouwer order is itself a well-ordering if and only if the tree has no infinite branch. It is named after [[Stephen Cole Kleene]], [[Luitzen Egbertus Jan Brouwer]], [[Nikolai Luzin]], and [[Wacław Sierpiński]].

==Definition==
If &lt;math&gt;t&lt;/math&gt; and &lt;math&gt;s&lt;/math&gt; are finite sequences of elements from &lt;math&gt;X&lt;/math&gt;, we say that &lt;math&gt;t &lt;_{KB} s&lt;/math&gt; when there is an &lt;math&gt;n&lt;/math&gt; such that either:
* &lt;math&gt;t\upharpoonright n = s\upharpoonright n&lt;/math&gt; and &lt;math&gt;t(n)&lt;/math&gt; is defined but &lt;math&gt;s(n)&lt;/math&gt; is undefined (i.e. &lt;math&gt;t&lt;/math&gt; properly extends &lt;math&gt;s&lt;/math&gt;), or
* both &lt;math&gt;s(n)&lt;/math&gt; and &lt;math&gt;t(n)&lt;/math&gt; are defined, &lt;math&gt;t(n)&lt;s(n)&lt;/math&gt;, and &lt;math&gt;t\upharpoonright n = s\upharpoonright n&lt;/math&gt;.
Here, the notation &lt;math&gt;t\upharpoonright n&lt;/math&gt; refers to the [[Substring|prefix]] of &lt;math&gt;t&lt;/math&gt; up to but not including &lt;math&gt;t(n)&lt;/math&gt;.
In simple terms, &lt;math&gt;t &lt;_{KB} s&lt;/math&gt; whenever &lt;math&gt;s&lt;/math&gt; is a prefix of &lt;math&gt;t&lt;/math&gt; (i.e. &lt;math&gt;s&lt;/math&gt; terminates before &lt;math&gt;t&lt;/math&gt;, and they are equal up to that point) or &lt;math&gt;t&lt;/math&gt; is to the "left" of &lt;math&gt;s&lt;/math&gt; on the first place they differ.&lt;ref name="moschovakis"/&gt;

==Tree interpretation==
A [[Tree (descriptive set theory)|tree]], in descriptive set theory, is defined as a set of finite sequences that is closed under prefix operations. The parent in the tree of any sequence is the shorter sequence formed by removing its final element. Thus, any set of finite sequences can be augmented to form a tree, and the Kleene–Brouwer order is a natural ordering that may be given to this tree. It is a generalization to potentially-infinite trees of the [[postorder traversal]] of a finite tree: at every node of the tree, the child subtrees are given their left to right ordering, and the node itself comes after all its children. The fact that the Kleene–Brouwer order is a linear ordering (that is, that it is transitive as well as being total) follows immediately from this, as any three sequences on which transitivity is to be tested form (with their prefixes) a finite tree on which the Kleene–Brouwer order coincides with the postorder.

The significance of the Kleene–Brouwer ordering comes from the fact that if &lt;math&gt;X&lt;/math&gt; is [[well-ordered]], then a tree over  &lt;math&gt;X&lt;/math&gt; is [[Well-founded relation|well-founded]] (having no infinitely long branches) if and only if the Kleene–Brouwer ordering is a well-ordering of the elements of the tree.&lt;ref name="moschovakis"/&gt;

==Recursion theory==
In [[recursion theory]], the Kleene–Brouwer order may be applied to the [[computation tree]]s of implementations of [[Computable function|total recursive]] [[Higher-order function|functionals]]. A computation tree is well-founded if and only if the computation performed by it is total recursive. Each state &lt;math&gt;x&lt;/math&gt; in a computation tree may be assigned an [[ordinal number]] &lt;math&gt;||x||&lt;/math&gt;, the supremum of the ordinal numbers &lt;math&gt;1+||y||&lt;/math&gt; where &lt;math&gt;y&lt;/math&gt; ranges over the children of &lt;math&gt;x&lt;/math&gt; in the tree. In this way, the total recursive functionals themselves can be classified into a hierarchy, according to the minimum value of the ordinal at the root of a computation tree, minimized over all computation trees that implement the functional. The Kleene–Brouwer order of a well-founded computation tree is itself a recursive well-ordering, and at least as large as the ordinal assigned to the tree, from which it follows that the levels of this hierarchy are indexed by [[recursive ordinal]]s.&lt;ref&gt;{{citation
 | last1 = Schwichtenberg | first1 = Helmut
 | last2 = Wainer | first2 = Stanley S.
 | contribution = 2.8 Recursive type-2 functionals and well-foundedness
 | isbn = 978-0-521-51769-0
 | location = Cambridge
 | mr = 2893891
 | pages = 98–101
 | publisher = Cambridge University Press
 | series = Perspectives in Logic
 | title = Proofs and computations
 | year = 2012}}.&lt;/ref&gt;

==History==
This ordering was used by {{harvtxt|Lusin|Sierpinski|1923}},&lt;ref&gt;{{citation
 |last1        = Lusin
 |first1       = Nicolas
 |author1-link = Nikolai Luzin
 |last2        = Sierpinski
 |first2       = Waclaw
 |author2-link = Wacław Sierpiński
 |issue        = 2
 |journal      = Journal de Mathématiques Pure et Appliquées
 |pages        = 53–72
 |title        = Sur un ensemble non measurable B
 |url          = http://eudml.org/doc/57945
 |volume       = 9
 |year         = 1923
 |deadurl      = yes
 |archiveurl   = https://archive.is/20130414182741/http://eudml.org/doc/57945
 |archivedate  = 2013-04-14
 |df           = 
}}.&lt;/ref&gt; and then again by {{harvtxt|Brouwer|1924}}.&lt;ref&gt;{{citation
 | last = Brouwer | first = L. E. J. | authorlink = Luitzen Egbertus Jan Brouwer
 | journal = Koninklijke Nederlandse Akademie van Wetenschappen, Proc. Section of Sciences
 | pages = 189–193
 | title = Beweis, dass jede volle Funktion gleichmässig stetig ist
 | volume = 27
 | year = 1924}}. As cited by {{harvtxt|Kleene|1955}}.&lt;/ref&gt; Brouwer does not cite any references, but [[Yiannis N. Moschovakis|Moschovakis]] argues that he may either have seen {{harvtxt|Lusin|Sierpinski|1923}}, or have been influenced by earlier work of the same authors leading to this work. Much later, {{harvtxt|Kleene|1955}} studied the same ordering, and credited it to Brouwer.&lt;ref&gt;{{citation
 | last = Kleene | first = S. C. | authorlink = Stephen Cole Kleene
 | doi = 10.2307/2372632
 | journal = American Journal of Mathematics
 | jstor = 2372632
 | mr = 0070595
 | pages = 405–428
 | title = On the forms of the predicates in the theory of constructive ordinals. II
 | volume = 77
 | year = 1955}}. See in particular section 26, "A digression concerning recursive linear orderings", pp. 419–422.&lt;/ref&gt;

== References ==
{{reflist}}

{{DEFAULTSORT:Kleene-Brouwer Order}}
[[Category:Descriptive set theory]]
[[Category:Order theory]]
[[Category:Wellfoundedness]]</text>
      <sha1>ayn107dysto4ha95m79vgk0vhmntgxm</sha1>
    </revision>
  </page>
  <page>
    <title>Linear subspace</title>
    <ns>0</ns>
    <id>56357</id>
    <revision>
      <id>871627834</id>
      <parentid>860532912</parentid>
      <timestamp>2018-12-02T11:28:59Z</timestamp>
      <contributor>
        <username>Toddy1</username>
        <id>3132940</id>
      </contributor>
      <comment>Undid revision 860532912 by [[Special:Contributions/L50g|L50g]] ([[User talk:L50g|talk]]) sockpuppet edit that removed useful link to section of Russian language Wikipedia article</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="30435">{{too technical|date=April 2013}}
{|  style="width:248px; float:right; margin-left:1em;"
 |[[Image:Projectivisation F5P^1.svg|120px]][[Image:Projectivisation F5P^1.svg|120px]]&lt;br/&gt;[[Image:Projectivisation F5P^1.svg|120px]][[Image:Projectivisation F5P^1.svg|120px]]
 |-
 | style="font-size:87%" |One-dimensional subspaces in the two-dimensional vector space over the [[finite field]] '''F'''&lt;sub&gt;5&lt;/sub&gt;. The [[origin (mathematics)|origin]] (0,&amp;#8239;0), marked with green circles, belongs to any of six 1-subspaces, while each of 24 remaining points belongs to exactly one; a property which holds for 1-subspaces over any field and in all dimensions. All '''F'''&lt;sub&gt;5&lt;/sub&gt;&lt;sup&gt;2&lt;/sup&gt; (i.e. a 5&amp;#8239;×&amp;#8239;5 square) is pictured four times for a better visualization
 |}

In [[linear algebra]] and related fields of [[mathematics]], a '''linear subspace''', also known as a '''vector subspace''', or, in the older literature, a '''linear manifold''',&lt;ref&gt;{{Cite book|title=Finite-Dimensional Vector Spaces|last=Halmos|first=P. R.|publisher=Princeton University Press|year=1942|isbn=978-1-61427-281-6|location=Princeton, NJ|pages=14|via=}}&lt;/ref&gt;&lt;ref&gt;The term ''linear manifold'' also has two other related but distinct definitions: (i) a subspace of a Hilbert space, closed under addition and scalar multiplication, or (ii) a subset of a vector space consisting of the vectors of a linear subspace shifted by a constant vector, i.e. the subset ''L'' + '''v''' of vector space ''V'', where ''L'' is a linear subspace of ''V'' and '''v''' is a vector in ''V''.&lt;/ref&gt; is a [[vector space]] that is a [[subset]] of some other ([[Dimension (vector space)|higher-dimension]]) vector space. A linear subspace is usually called simply a ''subspace'' when the context serves to distinguish it from other kinds of subspace.

== Definition==

Let ''K'' be a [[field (mathematics)|field]] (such as the [[real number]]s), ''V'' be a [[vector space]] over ''K'', and let ''W'' be a subset of ''V''.
Then ''W'' is a '''subspace''' if:
#The [[zero vector]], '''0''', is in ''W''.
#If '''u''' and '''v''' are elements of ''W'', then the sum {{nowrap|'''u''' + '''v'''}} is an element of ''W''.
#If '''u''' is an element of ''W'' and ''c'' is a scalar from ''K'', then the scalar product ''c'''''u''' is an element of ''W''.

== Examples ==
'''Example I:'''
Let the field ''K'' be the set '''R''' of [[real number]]s, and let the vector space ''V'' be the [[real coordinate space]] '''R'''&lt;sup&gt;3&lt;/sup&gt;.
Take ''W'' to be the set of all vectors in ''V'' whose last component is 0.
Then ''W'' is a subspace of ''V''.

''Proof:''
#Given '''u''' and '''v''' in ''W'', then they can be expressed as '''u'''&amp;nbsp;= (''u''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;''u''&lt;sub&gt;2&lt;/sub&gt;,&amp;#8239;0) and '''v'''&amp;nbsp;= (''v''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;''v''&lt;sub&gt;2&lt;/sub&gt;,&amp;#8239;0). Then '''u'''&amp;nbsp;+&amp;nbsp;'''v'''&amp;nbsp;= (''u''&lt;sub&gt;1&lt;/sub&gt;+''v''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;''u''&lt;sub&gt;2&lt;/sub&gt;+''v''&lt;sub&gt;2&lt;/sub&gt;,&amp;#8239;0+0)&amp;nbsp;= (''u''&lt;sub&gt;1&lt;/sub&gt;+''v''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;''u''&lt;sub&gt;2&lt;/sub&gt;+''v''&lt;sub&gt;2&lt;/sub&gt;,&amp;#8239;0). Thus, '''u'''&amp;nbsp;+&amp;nbsp;'''v''' is an element of&amp;nbsp;''W'', too.
#Given '''u''' in ''W'' and a scalar ''c'' in '''R''', if '''u'''&amp;nbsp;= (''u''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;''u''&lt;sub&gt;2&lt;/sub&gt;,&amp;#8239;0) again, then ''c'''''u'''&amp;nbsp;= (''cu''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;''cu''&lt;sub&gt;2&lt;/sub&gt;,&amp;#8239;''c''0)&amp;nbsp;= (''cu''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;''cu''&lt;sub&gt;2&lt;/sub&gt;,0). Thus, ''c'''''u''' is an element of ''W'' too.

'''Example II:'''
Let the field be '''R''' again, but now let the vector space be the [[Cartesian plane]] '''R'''&lt;sup&gt;2&lt;/sup&gt;.
Take ''W'' to be the set of points (''x'',&amp;#8239;''y'') of '''R'''&lt;sup&gt;2&lt;/sup&gt; such that ''x''&amp;nbsp;=&amp;nbsp;''y''.
Then ''W'' is a subspace of '''R'''&lt;sup&gt;2&lt;/sup&gt;.

''Proof:''
#Let '''p'''&amp;nbsp;= (''p''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;''p''&lt;sub&gt;2&lt;/sub&gt;) and '''q'''&amp;nbsp;= (''q''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;''q''&lt;sub&gt;2&lt;/sub&gt;) be elements of ''W'', that is, points in the plane such that ''p''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''p''&lt;sub&gt;2&lt;/sub&gt; and ''q''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''q''&lt;sub&gt;2&lt;/sub&gt;. Then '''p'''&amp;nbsp;+&amp;nbsp;'''q'''&amp;nbsp;= (''p''&lt;sub&gt;1&lt;/sub&gt;+''q''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;''p''&lt;sub&gt;2&lt;/sub&gt;+''q''&lt;sub&gt;2&lt;/sub&gt;); since ''p''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''p''&lt;sub&gt;2&lt;/sub&gt; and ''q''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''q''&lt;sub&gt;2&lt;/sub&gt;, then ''p''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;''q''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;= ''p''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;''q''&lt;sub&gt;2&lt;/sub&gt;, so '''p'''&amp;nbsp;+&amp;nbsp;'''q''' is an element of ''W''.
#Let '''p'''&amp;nbsp;=&amp;nbsp;(''p''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;''p''&lt;sub&gt;2&lt;/sub&gt;) be an element of ''W'', that is, a point in the plane such that ''p''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''p''&lt;sub&gt;2&lt;/sub&gt;, and let ''c'' be a scalar in '''R'''. Then ''c'''''p'''&amp;nbsp;= (''cp''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;''cp''&lt;sub&gt;2&lt;/sub&gt;); since ''p''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''p''&lt;sub&gt;2&lt;/sub&gt;, then ''cp''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;=&amp;nbsp;''cp''&lt;sub&gt;2&lt;/sub&gt;, so ''c'''''p''' is an element of ''W''.

In general, any subset of the [[real coordinate space]] '''R'''&lt;sup&gt;''n''&lt;/sup&gt; that is defined by a system of homogeneous [[linear equation]]s will yield a subspace.
(The equation in example I was ''z''&amp;nbsp;=&amp;nbsp;0, and the equation in example II was ''x''&amp;nbsp;=&amp;nbsp;''y''.)
Geometrically, these subspaces are points, lines, planes, and so on, that pass through the point '''0'''.

=== Examples related to calculus ===

'''Example III:'''
Again take the field to be '''R''', but now let the vector space ''V'' be the set '''R'''&lt;sup&gt;'''R'''&lt;/sup&gt; of all [[function (mathematics)|function]]s from '''R''' to '''R'''.
Let C('''R''') be the subset consisting of [[continuous function|continuous]] functions.
Then C('''R''') is a subspace of '''R'''&lt;sup&gt;'''R'''&lt;/sup&gt;.

''Proof:''
#We know from calculus that {{nowrap|0 ∈ C('''R''') ⊂ '''R'''&lt;sup&gt;'''R'''&lt;/sup&gt;}}.
#We know from calculus that the sum of continuous functions is continuous.
#Again, we know from calculus that the product of a continuous function and a number is continuous.

'''Example IV:'''
Keep the same field and vector space as before, but now consider the set Diff('''R''') of all [[differentiable function]]s.
The same sort of argument as before shows that this is a subspace too.

Examples that extend these themes are common in [[functional analysis]].

== Properties of subspaces ==

A way to characterize subspaces is that they are [[Closure (mathematics)|closed]] under [[linear combination]]s.
That is, a nonempty set ''W'' is a subspace [[if and only if]] every linear combination of ([[finite set|finite]]ly many) elements of ''W'' also belongs to ''W''.
Conditions 2 and 3 for a subspace are simply the most basic kinds of linear combinations.

In a [[topological vector space]] ''X'', a subspace ''W'' need not be [[closed set|closed]] in general, but a [[finite-dimensional]] subspace is always closed.&lt;ref&gt;See {{cite web|title=Basic Facts About Hilbert Space|url=http://www.math.colostate.edu/~pauld/M645/BasicHS.pdf|format=PDF|accessdate=September 17, 2012|author=Paul DuChateau}} for [[Hilbert space]]s&lt;/ref&gt; The same is true for subspaces of finite [[codimension]], i.e. determined by a finite number of continuous [[linear functional]]s.

==Descriptions==
Descriptions of subspaces include the solution set to a homogeneous [[system of linear equations]], the subset of Euclidean space described by a system of homogeneous linear [[parametric equations]], the [[linear span|span]] of a collection of vectors, and the [[null space]], [[column space]], and [[row space]] of a [[matrix (mathematics)|matrix]]. Geometrically (especially, over the field of [[real number]]s and its subfields), a subspace is a [[flat (geometry)|flat]] in an ''n''-space that passes through the origin.

A natural description of an 1-subspace is the [[scalar multiplication]] of one non-[[additive identity|zero]] vector '''v''' to all possible scalar values. 1-subspaces specified by two vectors are equal if and only if one vector can be obtained from another with scalar multiplication:
:&lt;math&gt;\exist c\in K: \mathbf{v}' = c\mathbf{v}\text{ (or }\mathbf{v} = \frac{1}{c}\mathbf{v}'\text{)}&lt;/math&gt;
This idea is generalized for higher dimensions with [[linear span]], but criteria for [[equality (mathematics)|equality]] of ''k''-spaces specified by sets of ''k'' vectors are not so simple.

A [[duality (mathematics)|dual]] description is provided with [[linear functional]]s (usually implemented as [[linear equation]]s). One non-[[additive identity|zero]] linear functional '''F''' specifies its [[kernel (linear algebra)|kernel]] subspace '''F'''&amp;nbsp;=&amp;nbsp;0 of [[codimension]] 1. Subspaces of codimension 1 specified by two linear functionals are equal if and only if one functional can be obtained from another with scalar multiplication (in the [[dual space]]):
:&lt;math&gt;\exist c\in K: \mathbf{F}' = c\mathbf{F}\text{ (or }\mathbf{F} = \frac{1}{c}\mathbf{F}'\text{)}&lt;/math&gt;
It is generalized for higher codimensions with a [[system of equations]]. The following two subsections will present this latter description in details, and [[#Span of vectors|the remaining]] four subsections further describe the idea of linear span.

===Systems of linear equations===
The solution set to any homogeneous [[system of linear equations]] with ''n'' variables is a subspace in the [[coordinate space]] ''K''&lt;sup&gt;''n''&lt;/sup&gt;:

:&lt;math&gt;\left\{ \left[\!\! \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_n \end{array} \!\!\right]  \in K^n : \begin{alignat}{6}
a_{11} x_1 &amp;&amp;\; + \;&amp;&amp; a_{12} x_2 &amp;&amp;\; + \cdots + \;&amp;&amp; a_{1n} x_n &amp;&amp;\; = 0&amp;    \\
a_{21} x_1 &amp;&amp;\; + \;&amp;&amp; a_{22} x_2 &amp;&amp;\; + \cdots + \;&amp;&amp; a_{2n} x_n &amp;&amp;\; = 0&amp;    \\
\vdots\;\;\; &amp;&amp;     &amp;&amp; \vdots\;\;\; &amp;&amp;              &amp;&amp; \vdots\;\;\; &amp;&amp; \vdots\,&amp; \\
a_{m1} x_1 &amp;&amp;\; + \;&amp;&amp; a_{m2} x_2 &amp;&amp;\; + \cdots + \;&amp;&amp; a_{mn} x_n &amp;&amp;\; = 0&amp;
\end{alignat} \right\}. &lt;/math&gt;

For example (over real or [[rational number]]s), the set of all vectors (''x'',&amp;#8239;''y'',&amp;#8239;''z'') satisfying the equations

:&lt;math&gt;x + 3y + 2z = 0 \;\;\;\;\text{and}\;\;\;\; 2x - 4y + 5z = 0&lt;/math&gt;

is a one-dimensional subspace. More generally, that is to say that given a set of ''n'' independent functions, the dimension of the subspace in ''K''&lt;sup&gt;''k''&lt;/sup&gt; will be the dimension of the [[null set]] of ''A'', the composite matrix of the ''n'' functions.

===Null space of a matrix===
{{main|Null space}}
In a finite-dimensional space, a homogeneous system of linear equations can be written as a single [[matrix (mathematics)|matrix]] equation:

:&lt;math&gt;A\mathbf{x} = \mathbf{0}.&lt;/math&gt;

The set of solutions to this equation is known as the [[null space]] of the matrix. For example, the subspace described above is the null space of the matrix

:&lt;math&gt;A = \left[ \begin{alignat}{3} 1 &amp;&amp; 3 &amp;&amp; 2 &amp;\\ 2 &amp;&amp; \;\;-4 &amp;&amp; \;\;\;\;5 &amp;\end{alignat} \,\right]\text{.}&lt;/math&gt;

Every subspace of ''K''&lt;sup&gt;''n''&lt;/sup&gt; can be described as the null space of some matrix (see [[#Algorithms|algorithms]], below).

===Linear parametric equations===
The subset of ''K''&lt;sup&gt;''n''&lt;/sup&gt; described by a system of homogeneous linear [[parametric equations]] is a subspace:

:&lt;math&gt;\left\{ \left[\!\! \begin{array}{c} x_1 \\ x_2 \\ \vdots \\ x_n \end{array} \!\!\right]  \in K^n : \begin{alignat}{7}
x_1 &amp;&amp;\; = \;&amp;&amp; a_{11} t_1 &amp;&amp;\; + \;&amp;&amp; a_{12} t_2 &amp;&amp;\; + \cdots + \;&amp;&amp; a_{1m} t_m &amp;    \\
x_2 &amp;&amp;\; = \;&amp;&amp; a_{21} t_1 &amp;&amp;\; + \;&amp;&amp; a_{22} t_2 &amp;&amp;\; + \cdots + \;&amp;&amp; a_{2m} t_m &amp;    \\
\vdots \,&amp;&amp;  &amp;&amp; \vdots\;\;\; &amp;&amp;     &amp;&amp; \vdots\;\;\; &amp;&amp;              &amp;&amp; \vdots\;\;\; &amp;  \\
x_n &amp;&amp;\; = \;&amp;&amp; a_{n1} t_1 &amp;&amp;\; + \;&amp;&amp; a_{n2} t_2 &amp;&amp;\; + \cdots + \;&amp;&amp; a_{nm} t_m &amp;    \\
\end{alignat} \text{ for some } t_1,\ldots,t_m\in K \right\}. &lt;/math&gt;

For example, the set of all vectors (''x'',&amp;#8239;''y'',&amp;#8239;''z'') parameterized by the equations

:&lt;math&gt;x = 2t_1 + 3t_2,\;\;\;\;y = 5t_1 - 4t_2,\;\;\;\;\text{and}\;\;\;\;z = -t_1 + 2t_2&lt;/math&gt;

is a two-dimensional subspace of ''K''&lt;sup&gt;3&lt;/sup&gt;, if ''K'' is a [[number field]] (such as real or rational numbers).&lt;ref name="fields"&gt;Generally, ''K'' can be any field of such [[characteristic (algebra)|characteristic]] that the given integer matrix has the appropriate [[rank (matrix theory)|rank]] in it. All fields include [[integer]]s, but some integers may equal to zero in some fields.&lt;/ref&gt;

===Span of vectors===
{{main|Linear span}}
In linear algebra, the system of parametric equations can be written as a single vector equation:

:&lt;math&gt;\begin{bmatrix} x&amp; \\ y&amp; \\ z&amp; \end{bmatrix} \;=\; t_1 \!\begin{bmatrix} 2&amp; \\ 5&amp; \\ -1&amp; \end{bmatrix} + t_2 \!\begin{bmatrix} 3&amp; \\ -4&amp; \\ 2&amp; \end{bmatrix}.&lt;/math&gt;

The expression on the right is called a [[linear combination]] of the vectors
(2,&amp;#8239;5,&amp;#8239;−1) and (3,&amp;#8239;−4,&amp;#8239;2). These two vectors are said to '''span''' the resulting subspace.

In general, a '''linear combination''' of vectors '''v'''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;'''v'''&lt;sub&gt;2&lt;/sub&gt;,&amp;#8239;...&amp;#8239;,&amp;#8239;'''v'''&lt;sub&gt;''k''&lt;/sub&gt; is any vector of the form

:&lt;math&gt;t_1 \mathbf{v}_1 + \cdots + t_k \mathbf{v}_k.&lt;/math&gt;

The set of all possible linear combinations is called the '''span''':

:&lt;math&gt;\text{Span} \{ \mathbf{v}_1, \ldots, \mathbf{v}_k \}
= \left\{ t_1 \mathbf{v}_1 + \cdots + t_k \mathbf{v}_k : t_1,\ldots,t_k\in K \right\} .&lt;/math&gt;

If the vectors '''v'''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;...&amp;#8239;,&amp;#8239;'''v'''&lt;sub&gt;''k''&lt;/sub&gt; have ''n'' components, then their span is a subspace of ''K''&lt;sup&gt;''n''&lt;/sup&gt;. Geometrically, the span is the flat through the origin in ''n''-dimensional space determined by the points '''v'''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;...&amp;#8239;,&amp;#8239;'''v'''&lt;sub&gt;''k''&lt;/sub&gt;.

; Example
: The ''xz''-plane in '''R'''&lt;sup&gt;3&lt;/sup&gt; can be parameterized by the equations
::&lt;math&gt;x = t_1, \;\;\; y = 0, \;\;\; z = t_2.&lt;/math&gt;

:As a subspace, the ''xz''-plane is spanned by the vectors (1,&amp;#8239;0,&amp;#8239;0) and (0,&amp;#8239;0,&amp;#8239;1). Every vector in the ''xz''-plane can be written as a linear combination of these two:

::&lt;math&gt;(t_1, 0, t_2) = t_1(1,0,0) + t_2(0,0,1)\text{.}&lt;/math&gt;

:Geometrically, this corresponds to the fact that every point on the ''xz''-plane can be reached from the origin by first moving some distance in the direction of (1,&amp;#8239;0,&amp;#8239;0) and then moving some distance in the direction of (0,&amp;#8239;0,&amp;#8239;1).

===Column space and row space===
{{main|Row and column spaces}}
A system of linear parametric equations in a finite-dimensional space can also be written as a single matrix equation:

:&lt;math&gt;\mathbf{x} = A\mathbf{t}\;\;\;\;\text{where}\;\;\;\;A = \left[ \begin{alignat}{2} 2 &amp;&amp; 3 &amp; \\ 5 &amp;&amp; \;\;-4 &amp; \\ -1 &amp;&amp; 2 &amp; \end{alignat} \,\right]\text{.}&lt;/math&gt;

In this case, the subspace consists of all possible values of the vector '''x'''. In linear algebra, this subspace is known as the [[column space]] (or [[image (mathematics)|image]]) of the matrix ''A''. It is precisely the subspace of ''K''&lt;sup&gt;''n''&lt;/sup&gt; spanned by the column vectors of ''A''.

The [[row space]] of a matrix is the subspace spanned by its row vectors. The row space is interesting because it is the [[orthogonal complement]] of the null space (see below).

===Independence, basis, and dimension===
{{main|Linear independence|Basis (linear algebra)|Dimension (vector space)}}
[[File:Basis for a plane.svg|thumb|280px|right|The vectors '''u''' and '''v''' are a basis for this two-dimensional subspace of '''R'''&lt;sup&gt;3&lt;/sup&gt;.]]
In general, a subspace of ''K''&lt;sup&gt;''n''&lt;/sup&gt; determined by ''k'' parameters (or spanned by ''k'' vectors) has dimension ''k''. However, there are exceptions to this rule. For example, the subspace of ''K''&lt;sup&gt;3&lt;/sup&gt; spanned by the three vectors (1,&amp;#8239;0,&amp;#8239;0), (0,&amp;#8239;0,&amp;#8239;1), and (2,&amp;#8239;0,&amp;#8239;3) is just the ''xz''-plane, with each point on the plane described by infinitely many different values of {{nowrap| ''t''&lt;sub&gt;1&lt;/sub&gt;, ''t''&lt;sub&gt;2&lt;/sub&gt;, ''t''&lt;sub&gt;3&lt;/sub&gt;}}.

In general, vectors '''v'''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;...&amp;#8239;,&amp;#8239;'''v'''&lt;sub&gt;''k''&lt;/sub&gt; are called '''linearly independent''' if

:&lt;math&gt;t_1 \mathbf{v}_1 + \cdots + t_k \mathbf{v}_k \;\ne\; u_1 \mathbf{v}_1 + \cdots + u_k \mathbf{v}_k&lt;/math&gt;

for
(''t''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;''t''&lt;sub&gt;2&lt;/sub&gt;,&amp;#8239;...&amp;#8239;,&amp;#8239;''t&lt;sub&gt;k&lt;/sub&gt;'')&amp;nbsp;≠ (''u''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;''u''&lt;sub&gt;2&lt;/sub&gt;,&amp;#8239;...&amp;#8239;,&amp;#8239;''u&lt;sub&gt;k&lt;/sub&gt;'').&lt;ref&gt;This definition is often stated differently: vectors '''v'''&lt;sub&gt;1&lt;/sub&gt;, ..., '''v'''&lt;sub&gt;''k''&lt;/sub&gt; are linearly independent if 
{{nowrap| ''t''&lt;sub&gt;1&lt;/sub&gt;'''v'''&lt;sub&gt;1&lt;/sub&gt; + ··· + ''t&lt;sub&gt;k&lt;/sub&gt;'''''v'''&lt;sub&gt;''k''&lt;/sub&gt; ≠ '''0'''}} for {{nowrap| (''t''&lt;sub&gt;1&lt;/sub&gt;, ''t''&lt;sub&gt;2&lt;/sub&gt;, ..., ''t&lt;sub&gt;k&lt;/sub&gt;'') ≠ (0, 0, ..., 0)}}. The two definitions are equivalent.&lt;/ref&gt;
If {{nowrap| '''v'''&lt;sub&gt;1&lt;/sub&gt;, ..., '''v'''&lt;sub&gt;''k''&lt;/sub&gt; }} are linearly independent, then the '''coordinates''' {{nowrap| ''t''&lt;sub&gt;1&lt;/sub&gt;, ..., ''t&lt;sub&gt;k&lt;/sub&gt;''}} for a vector in the span are uniquely determined.

A '''basis''' for a subspace ''S'' is a set of linearly independent vectors whose span is ''S''. The number of elements in a basis is always equal to the geometric dimension of the subspace. Any spanning set for a subspace can be changed into a basis by removing redundant vectors (see [[#Algorithms|algorithms]], below).

; Example
: Let ''S'' be the subspace of '''R'''&lt;sup&gt;4&lt;/sup&gt; defined by the equations
::&lt;math&gt;x_1 = 2 x_2\;\;\;\;\text{and}\;\;\;\;x_3 = 5x_4.&lt;/math&gt;
:Then the vectors (2,&amp;#8239;1,&amp;#8239;0,&amp;#8239;0) and (0,&amp;#8239;0,&amp;#8239;5,&amp;#8239;1) are a basis for ''S''. In particular, every vector that satisfies the above equations can be written uniquely as a linear combination of the two basis vectors:

::&lt;math&gt;(2t_1, t_1, 5t_2, t_2) = t_1(2, 1, 0, 0) + t_2(0, 0, 5, 1).&lt;/math&gt;

:The subspace ''S'' is two-dimensional. Geometrically, it is the plane in '''R'''&lt;sup&gt;4&lt;/sup&gt; passing through the points (0,&amp;#8239;0,&amp;#8239;0,&amp;#8239;0), (2,&amp;#8239;1,&amp;#8239;0,&amp;#8239;0), and (0,&amp;#8239;0,&amp;#8239;5,&amp;#8239;1).

==Operations and relations on subspaces==

=== Inclusion ===
&lt;!-- some illustration, please --&gt;
The [[inclusion relation|set-theoretical inclusion]] binary relation specifies a [[partial order]] on the set of all subspaces (of any dimension).

A subspace cannot lie in any subspace of lesser dimension. If dim&amp;nbsp;''U''&amp;nbsp;=&amp;nbsp;''k'', a finite number, and ''U''&amp;nbsp;⊂&amp;nbsp;''W'', then dim&amp;nbsp;''W''&amp;nbsp;=&amp;nbsp;''k'' if and only if ''U''&amp;nbsp;=&amp;nbsp;''W''.

===Intersection===
[[File:Intersecting Planes 2.svg|thumb|right|In '''R'''&lt;sup&gt;3&lt;/sup&gt;, the intersection of two distinct two-dimensional subspaces is one-dimensional]]
Given subspaces ''U'' and ''W'' of a vector space ''V'', then their [[intersection (set theory)|intersection]] ''U''&amp;nbsp;∩&amp;nbsp;''W''&amp;nbsp;:= {'''v'''&amp;nbsp;∈&amp;nbsp;''V''&amp;nbsp;: '''v'''&amp;nbsp;is an element of both ''U'' and&amp;nbsp;''W''} is also a subspace of ''V''.&lt;ref&gt;{{harvtxt|Nering|1970|p=21}}&lt;/ref&gt;

''Proof:''
# Let '''v''' and '''w''' be elements of ''U''&amp;nbsp;∩&amp;nbsp;''W''. Then '''v''' and '''w''' belong to both ''U'' and ''W''. Because ''U'' is a subspace, then '''v'''&amp;nbsp;+&amp;nbsp;'''w''' belongs to ''U''. Similarly, since ''W'' is a subspace, then '''v'''&amp;nbsp;+&amp;nbsp;'''w''' belongs to ''W''. Thus, '''v'''&amp;nbsp;+&amp;nbsp;'''w''' belongs to ''U''&amp;nbsp;∩&amp;nbsp;''W''.
# Let '''v''' belong to ''U''&amp;nbsp;∩&amp;nbsp;''W'', and let ''c'' be a scalar. Then '''v''' belongs to both ''U'' and ''W''. Since ''U'' and ''W'' are subspaces, ''c'''''v''' belongs to both ''U'' and&amp;nbsp;''W''.
# Since ''U'' and ''W'' are vector spaces, then '''0''' belongs to both sets. Thus, '''0''' belongs to ''U''&amp;nbsp;∩&amp;nbsp;''W''.

For every vector space ''V'', the [[zero vector space|set {'''0'''}]] and ''V'' itself are subspaces of ''V''.&lt;ref&gt;{{harvtxt|Nering|1970|p=20}}&lt;/ref&gt;

===Sum===
If ''U'' and ''W'' are subspaces, their '''sum''' is the subspace

:&lt;math&gt;U + W = \left\{ \mathbf{u} + \mathbf{w} \colon \mathbf{u}\in U, \mathbf{w}\in W \right\}.&lt;/math&gt;&lt;ref&gt;{{harvtxt|Nering|1970|p=21}}&lt;/ref&gt;

For example, the sum of two lines is the plane that contains them both. The dimension of the sum satisfies the inequality

:&lt;math&gt;\max(\dim U,\dim W) \leq \dim(U + W) \leq \dim(U) + \dim(W).&lt;/math&gt;

Here the minimum only occurs if one subspace is contained in the other, while the maximum is the most general case. The dimension of the intersection and the sum are related:

:&lt;math&gt;\dim(U+W) = \dim(U) + \dim(W) - \dim(U \cap W).&lt;/math&gt;&lt;ref&gt;{{harvtxt|Nering|1970|p=22}}&lt;/ref&gt;

=== Lattice of subspaces ===
The operations [[Linear subspace#Intersection|intersection]] and [[Linear subspace#Sum|sum]] make the set of all subspaces a bounded [[modular lattice]], where the [[zero vector space|{0} subspace]], the [[least element]], is an [[identity element]] of the sum operation, and the identical subspace ''V'', the greatest element, is an [[identity element]] of the intersection operation.

=== Other ===
{{unreferenced section|date=April 2013}}
If ''V'' is an [[inner product space]], then the [[orthogonal complement]] ⊥ of any subspace of ''V'' is again a subspace. This operation, understood as [[negation]] (¬), makes the lattice of subspaces a (possibly [[infinite set|infinite]]) orthocomplemented lattice (it is not a distributive lattice).

In a [[pseudo-Euclidean space]] there are orthogonal complements too, but such operation does not form a Boolean algebra (nor a [[Heyting algebra]]) because of [[null vector|null]] subspaces, for which {{nowrap|1=''N'' ∩ ''N''&lt;sup&gt;⊥&lt;/sup&gt; = ''N'' ≠ {0}.}} The same case presents the &lt;sup&gt;⊥&lt;/sup&gt; operation in [[symplectic vector space]]s.

==Algorithms==
Most algorithms for dealing with subspaces involve [[row reduction]]. This is the process of applying [[elementary row operation]]s to a matrix until it reaches either [[row echelon form]] or [[reduced row echelon form]]. Row reduction has the following important properties:
# The reduced matrix has the same null space as the original.
# Row reduction does not change the span of the row vectors, i.e. the reduced matrix has the same row space as the original.
# Row reduction does not affect the linear dependence of the column vectors.

===Basis for a row space===
:'''Input''' An ''m''&amp;#8239;×&amp;#8239;''n'' matrix ''A''.
:'''Output''' A basis for the [[row space]] of ''A''.
:# Use elementary row operations to put ''A'' into row echelon form.
:# The nonzero rows of the echelon form are a basis for the row space of ''A''.
See the article on [[row space]] for an [[Row and column spaces#Basis 2|example]].

If we instead put the matrix ''A'' into reduced row echelon form, then the resulting basis for the row space is uniquely determined. This provides an algorithm for checking whether two row spaces are equal and, by extension, whether two subspaces of ''K''&lt;sup&gt;''n''&lt;/sup&gt; are equal.

===Subspace membership===
:'''Input''' A basis {'''b'''&lt;sub&gt;1&lt;/sub&gt;, '''b'''&lt;sub&gt;2&lt;/sub&gt;, ..., '''b'''&lt;sub&gt;''k''&lt;/sub&gt;} for a subspace ''S'' of ''K''&lt;sup&gt;''n''&lt;/sup&gt;, and a vector '''v''' with ''n'' components.
:'''Output''' Determines whether '''v''' is an element of ''S''
:# Create a (''k''&amp;nbsp;+&amp;nbsp;1)&amp;#8239;×&amp;#8239;''n'' matrix ''A'' whose rows are the vectors '''b'''&lt;sub&gt;1&lt;/sub&gt;,&amp;#8239;...&amp;#8239;,&amp;#8239;'''b'''&lt;sub&gt;''k''&lt;/sub&gt; and '''v'''.
:# Use elementary row operations to put ''A'' into row echelon form.
:# If the echelon form has a row of zeroes, then the vectors {{nowrap| {'''b'''&lt;sub&gt;1&lt;/sub&gt;, ..., '''b'''&lt;sub&gt;''k''&lt;/sub&gt;, '''v'''} }} are linearly dependent, and therefore {{nowrap| '''v''' ∈ ''S''}}.

===Basis for a column space===
:'''Input''' An ''m''&amp;#8239;×&amp;#8239;''n'' matrix ''A''
:'''Output''' A basis for the [[column space]] of ''A''
:# Use elementary row operations to put ''A'' into row echelon form.
:# Determine which columns of the echelon form have [[Row echelon form|pivots]]. The corresponding columns of the original matrix are a basis for the column space.
See the article on [[column space]] for an [[Column space#Basis|example]].

This produces a basis for the column space that is a subset of the original column vectors. It works because the columns with pivots are a basis for the column space of the echelon form, and row reduction does not change the linear dependence relationships between the columns.

===Coordinates for a vector===
:'''Input''' A basis {'''b'''&lt;sub&gt;1&lt;/sub&gt;, '''b'''&lt;sub&gt;2&lt;/sub&gt;, ..., '''b'''&lt;sub&gt;''k''&lt;/sub&gt;} for a subspace ''S'' of ''K''&lt;sup&gt;''n''&lt;/sup&gt;, and a vector {{nowrap| '''v''' ∈ ''S''}}
:'''Output''' Numbers ''t''&lt;sub&gt;1&lt;/sub&gt;, ''t''&lt;sub&gt;2&lt;/sub&gt;, ..., ''t''&lt;sub&gt;''k''&lt;/sub&gt; such that {{nowrap|1= '''v''' = ''t''&lt;sub&gt;1&lt;/sub&gt;'''b'''&lt;sub&gt;1&lt;/sub&gt; + ··· + ''t''&lt;sub&gt;''k''&lt;/sub&gt;'''b'''&lt;sub&gt;''k''&lt;/sub&gt;}}
:# Create an [[augmented matrix]] ''A'' whose columns are '''b'''&lt;sub&gt;1&lt;/sub&gt;,...,'''b'''&lt;sub&gt;''k''&lt;/sub&gt; , with the last column being '''v'''.
:# Use elementary row operations to put ''A'' into reduced row echelon form.
:# Express the final column of the reduced echelon form as a linear combination of the first ''k'' columns. The coefficients used are the desired numbers {{nowrap| ''t''&lt;sub&gt;1&lt;/sub&gt;, ''t''&lt;sub&gt;2&lt;/sub&gt;, ..., ''t''&lt;sub&gt;''k''&lt;/sub&gt;}}. (These should be precisely the first ''k'' entries in the final column of the reduced echelon form.)
If the final column of the reduced row echelon form contains a pivot, then the input vector '''v''' does not lie in ''S''.

===Basis for a null space===
:'''Input''' An ''m''&amp;#8239;×&amp;#8239;''n'' matrix ''A''.
:'''Output''' A basis for the null space of ''A''
:# Use elementary row operations to put ''A'' in reduced row echelon form.
:# Using the reduced row echelon form, determine which of the variables {{nowrap| ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, ..., ''x&lt;sub&gt;n&lt;/sub&gt;''}} are free. Write equations for the dependent variables in terms of the free variables.
:# For each free variable ''x&lt;sub&gt;i&lt;/sub&gt;'', choose a vector in the null space for which {{nowrap|1= ''x&lt;sub&gt;i&lt;/sub&gt;'' = 1}} and the remaining free variables are zero. The resulting collection of vectors is a basis for the null space of ''A''.
See the article on [[null space]] for an [[Kernel (matrix)#Basis|example]].

===Basis for the sum and intersection of two subspaces===

Given two subspaces {{mvar|U}} and {{mvar|W}} of {{mvar|V}}, a basis of the sum &lt;math&gt;U + W&lt;/math&gt; and the intersection &lt;math&gt;U \cap W&lt;/math&gt; can be calculated using the [[Zassenhaus algorithm]]

===Equations for a subspace===
:'''Input''' A basis {'''b'''&lt;sub&gt;1&lt;/sub&gt;, '''b'''&lt;sub&gt;2&lt;/sub&gt;, ..., '''b'''&lt;sub&gt;''k''&lt;/sub&gt;} for a subspace ''S'' of ''K''&lt;sup&gt;''n''&lt;/sup&gt;
:'''Output''' An (''n''&amp;nbsp;−&amp;nbsp;''k'')&amp;#8239;×&amp;#8239;''n'' matrix whose null space is ''S''.
:# Create a matrix ''A'' whose rows are {{nowrap| '''b'''&lt;sub&gt;1&lt;/sub&gt;, '''b'''&lt;sub&gt;2&lt;/sub&gt;, ..., '''b'''&lt;sub&gt;''k''&lt;/sub&gt;}}.
:# Use elementary row operations to put ''A'' into reduced row echelon form.
:# Let {{nowrap| '''c'''&lt;sub&gt;1&lt;/sub&gt;, '''c'''&lt;sub&gt;2&lt;/sub&gt;, ..., '''c'''&lt;sub&gt;''n''&lt;/sub&gt; }} be the columns of the reduced row echelon form. For each column without a pivot, write an equation expressing the column as a linear combination of the columns with pivots.
:# This results in a homogeneous system of ''n'' − ''k'' linear equations involving the variables '''c'''&lt;sub&gt;1&lt;/sub&gt;,...,'''c'''&lt;sub&gt;''n''&lt;/sub&gt;. The {{nowrap| (''n'' − ''k'') × ''n''}} matrix corresponding to this system is the desired matrix with nullspace ''S''.
; Example
:If the reduced row echelon form of ''A'' is

::&lt;math&gt;\left[ \begin{alignat}{6}
1 &amp;&amp; 0 &amp;&amp; -3 &amp;&amp; 0 &amp;&amp;  2 &amp;&amp; 0 \\
0 &amp;&amp; 1 &amp;&amp;  5 &amp;&amp; 0 &amp;&amp; -1 &amp;&amp; 4 \\
0 &amp;&amp; 0 &amp;&amp;  0 &amp;&amp; 1 &amp;&amp;  7 &amp;&amp; -9 \\
0 &amp;&amp; \;\;\;\;\;0 &amp;&amp;  \;\;\;\;\;0 &amp;&amp; \;\;\;\;\;0 &amp;&amp;  \;\;\;\;\;0 &amp;&amp; \;\;\;\;\;0 \end{alignat} \,\right] &lt;/math&gt;

:then the column vectors {{nowrap| '''c'''&lt;sub&gt;1&lt;/sub&gt;, ..., '''c'''&lt;sub&gt;6&lt;/sub&gt;}} satisfy the equations

::&lt;math&gt; \begin{alignat}{1}
\mathbf{c}_3 &amp;= -3\mathbf{c}_1 + 5\mathbf{c}_2 \\
\mathbf{c}_5 &amp;= 2\mathbf{c}_1 - \mathbf{c}_2 + 7\mathbf{c}_4 \\
\mathbf{c}_6 &amp;= 4\mathbf{c}_2 - 9\mathbf{c}_4
\end{alignat}&lt;/math&gt;

:It follows that the row vectors of ''A'' satisfy the equations

::&lt;math&gt; \begin{alignat}{1}
x_3 &amp;= -3x_1 + 5x_2 \\
x_5 &amp;= 2x_1 - x_2 + 7x_4 \\
x_6 &amp;= 4x_2 - 9x_4.
\end{alignat}&lt;/math&gt;

:In particular, the row vectors of ''A'' are a basis for the null space of the corresponding matrix.

==See also==
* [[Signal subspace]]
* [[Multilinear subspace learning]]
* [[Cyclic subspace]]

==Textbooks==
* {{Citation
 | last = Axler
 | first = Sheldon Jay
 | year = 1997
 | title = Linear Algebra Done Right
 | publisher = Springer-Verlag
 | edition = 2nd
 | isbn = 0-387-98259-0
}}
* {{Citation
 | last = Lay
 | first = David C.
 | date = August 22, 2005
 | title = Linear Algebra and Its Applications
 | publisher = Addison Wesley
 | edition = 3rd
 | isbn = 978-0-321-28713-7
}}
* {{Citation
 |last=Meyer 
 |first=Carl D. 
 |date=February 15, 2001 
 |title=Matrix Analysis and Applied Linear Algebra 
 |publisher=Society for Industrial and Applied Mathematics (SIAM) 
 |isbn=978-0-89871-454-8 
 |url=http://www.matrixanalysis.com/DownloadChapters.html 
 |archive-url=https://web.archive.org/web/20010301161440/http://matrixanalysis.com/DownloadChapters.html 
 |dead-url=yes 
 |archive-date=March 1, 2001 
 |df= 
}}
* {{Citation
 | last = Poole
 | first = David
 | year = 2006
 | title = Linear Algebra: A Modern Introduction
 | publisher = Brooks/Cole
 | edition = 2nd
 | isbn = 0-534-99845-3
}}
* {{Citation
 | last = Anton
 | first = Howard
 | year = 2005
 | title = Elementary Linear Algebra (Applications Version)
 | publisher = Wiley International
 | edition = 9th
}}
* {{Citation
 | last = Leon
 | first = Steven J.
 | year = 2006
 | title = Linear Algebra With Applications
 | publisher = Pearson Prentice Hall
 | edition = 7th
}}

==Notes==
{{Reflist}}

==External links==
* {{planetmath reference|id=624|title=Vector subspace}}.
* {{aut|[[Gilbert Strang]]}}, [http://ocw.mit.edu/OcwWeb/Mathematics/18-06Spring-2005/VideoLectures/detail/lecture10.htm MIT Linear Algebra Lecture on the Four Fundamental Subspaces] at Google Video, from [[MIT OpenCourseWare]]

==References==
* {{ citation | first1 = Evar D. | last1 = Nering | year = 1970 | title = Linear Algebra and Matrix Theory | edition = 2nd | publisher = [[John Wiley &amp; Sons|Wiley]] | location = New York | lccn = 76091646 | url=https://archive.org/details/LinearAlgebraAndMatrixTheory }}

{{DEFAULTSORT:Linear Subspace}}
[[Category:Linear algebra]]
[[Category:Articles containing proofs]]
[[Category:Operator theory]]
[[Category:Functional analysis]]

[[ru:Векторное подпространство]]</text>
      <sha1>ojkd9xre3kzdgf5vqbit98kfd1sxslh</sha1>
    </revision>
  </page>
  <page>
    <title>Mathematical operators and symbols in Unicode</title>
    <ns>0</ns>
    <id>5921376</id>
    <revision>
      <id>844777281</id>
      <parentid>831993979</parentid>
      <timestamp>2018-06-07T03:01:28Z</timestamp>
      <contributor>
        <username>Drmccreedy</username>
        <id>1543097</id>
      </contributor>
      <comment>/* Characters in other blocks */ Bump version for Unicode 11.0</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="13180">{{redirect|⊞|the key|Windows key}}
[[File:0009-cmls-2012_preview_ecran.jpg|thumb|300px|Blackboard at the Laurent Schwartz Center for Mathematics, [[École Polytechnique]]]]
{{SpecialChars}}
The [[Unicode]] Standard encodes almost all standard characters used in mathematics.&lt;ref name="utr25"&gt;{{cite web | title = Unicode Technical Report #25: Unicode Support for Mathematics | publisher = The Unicode Consortium | date = 2 April 2012 | url = https://www.unicode.org/reports/tr25/ | format = PDF | accessdate = 14 August 2014}}&lt;/ref&gt;
Unicode Technical Report #25 provides comprehensive information about the character repertoire, their properties, and guidelines for implementation.&lt;ref name="utr25"/&gt;
[[Operation (mathematics)|Mathematical operators]] and [[List of mathematical symbols|symbols]] are in multiple [[Unicode block]]s.  Some of these blocks are dedicated to, or primarily contain, mathematical characters while others are a mix of mathematical and non-mathematical characters.  This article covers all Unicode characters with a derived property of "Math".&lt;ref&gt;{{cite web | title = Unicode Character Database: Derived Core Properties | publisher = The Unicode Consortium | date = 19 February 2014 | url = https://www.unicode.org/Public/UCD/latest/ucd/DerivedCoreProperties.txt | accessdate = 14 August 2014 
}}&lt;/ref&gt;&lt;ref&gt;{{cite web | title = Unicode Technical Annex #44: Unicode Character Database | publisher = The Unicode Consortium | date = 25 September 2013 | url = https://www.unicode.org/reports/tr44/ | format = PDF | accessdate = 14 August 2014}}&lt;/ref&gt;
&lt;!-- Note that capitalization below intentionally uses Unicode block name capitalization instead of the normal Wikipedia sentence case for headings. --&gt;
==Dedicated blocks==

===Mathematical Operators block===
{{Main|Mathematical Operators|l1=Mathematical Operators (Unicode block)}}
The Mathematical Operators block (U+2200&amp;ndash;U+22FF) contains characters for mathematical, logical, and set notation.
{{Unicode chart Mathematical Operators}}

===Supplemental Mathematical Operators block===
{{Main|Supplemental Mathematical Operators|l1=Supplemental Mathematical Operators (Unicode block)}}
The Supplemental Mathematical Operators block (U+2A00&amp;ndash;U+2AFF) contains various mathematical symbols, including N-ary operators, summations and integrals, intersections and unions, logical and relational operators, and subset/superset relations.
{{Unicode chart Supplemental Mathematical Operators}}

===&lt;span id="Mathematical Alphanumeric Symbols block"&gt;&lt;/span&gt;Mathematical Alphanumeric Symbols block===
{{Main|Mathematical Alphanumeric Symbols|l1=Mathematical Alphanumeric Symbols (Unicode block)}}
The Mathematical Alphanumeric Symbols block (U+1D400&amp;ndash;U+1D7FF) contains Latin and Greek letters and decimal digits that enable mathematicians to denote different notions with different letter styles.  The "holes" in the alphabetic ranges are filled by previously defined characters in the Letter like Symbols block [[#Letterlike Symbols block|shown below]].

{{Unicode chart Mathematical Alphanumeric Symbols}}

===&lt;span id="Letterlike Symbols block"&gt;&lt;/span&gt;Letterlike Symbols block===
{{Main|Letterlike Symbols|l1=Letterlike Symbols (Unicode block)}}
The Letterlike Symbols block (U+2100&amp;ndash;U+214F) includes variables.  Most alphabetic math symbols are in the Mathematical Alphanumeric Symbols block [[#Mathematical Alphanumeric Symbols block|shown above]].

The math subset of this block is U+2102, U+2107, U+210A&amp;ndash;U+2113, U+2115, U+2118&amp;ndash;U+2119, U+2124, U+2128&amp;ndash;U+2129, U+212C, U+212F, U+2133, U+2135, U+213C&amp;ndash;U+2149, and U+214B.

{{Unicode chart Letterlike Symbols}}

===Miscellaneous Mathematical Symbols-A block===
{{Main|Miscellaneous Mathematical Symbols-A|l1=Miscellaneous Mathematical Symbols-A (Unicode block)}}
The Miscellaneous Mathematical Symbols-A block (U+27C0&amp;ndash;U+27EF) contains characters for mathematical, logical, and database notation.
{{Unicode chart Miscellaneous Mathematical Symbols-A}}

===Miscellaneous Mathematical Symbols-B block===
{{Main|Miscellaneous Mathematical Symbols-B|l1=Miscellaneous Mathematical Symbols-B (Unicode block)}}
The Miscellaneous Mathematical Symbols-B block (U+2980&amp;ndash;U+29FF) contains miscellaneous mathematical symbols, including brackets, angles, and circle symbols.
{{Unicode chart Miscellaneous Mathematical Symbols-B}}

===Miscellaneous Technical block===
{{Main|Miscellaneous Technical|l1=Miscellaneous Technical (Unicode block)}}
The Miscellaneous Technical block (U+2300&amp;ndash;U+23FF) includes braces and operators.

The math subset of this block is U+2308&amp;ndash;U+230B, U+2320-U+2321, U+237C, U+239B-U+23B5, 23B7, U+23D0, and U+23DC-U+23E2.
{{Unicode chart Miscellaneous Technical}}

===Geometric Shapes block===
{{Main|Geometric Shapes|l1=Geometric Shapes (Unicode block)}}
The Geometric Shapes block (U+25A0&amp;ndash;U+25FF) contains geometric shape symbols.

The math subset of this block is U+25A0&amp;ndash;25A1, U+25AE&amp;ndash;25B7, U+25BC&amp;ndash;25C1, U+25C6&amp;ndash;25C7, U+25CA&amp;ndash;25CB, U+25CF&amp;ndash;25D3, U+25E2, U+25E4, U+25E7&amp;ndash;25EC, and U+25F8&amp;ndash;25FF.
{{Unicode chart Geometric Shapes}}

===Miscellaneous Symbols and Arrows block===
{{Main|Miscellaneous Symbols and Arrows|l1=Miscellaneous Symbols and Arrows (Unicode block)}}
The Miscellaneous Symbols and Arrows block (U+2B00&amp;ndash;U+2BFF Arrows) contains arrows and geometric shapes with various fills.

The math subset of this block is U+2B30&amp;ndash;2B44 and U+2B47&amp;ndash;2B4C.
{{Unicode chart Miscellaneous Symbols and Arrows}}

===Arrows block===
{{Main|Arrows (Unicode block)}}
The Arrows block (U+2190&amp;ndash;U+21FF) contains line, curve, and semicircle arrows and arrow-like operators.
{{Unicode chart Arrows}}

===Supplemental Arrows-A block===
{{Main|Supplemental Arrows-A|l1=Supplemental Arrows-A (Unicode block)}}
The Supplemental Arrows-A block (U+27F0&amp;ndash;U+27FF) contains arrows and arrow-like operators.
{{Unicode chart Supplemental Arrows-A}}

===Supplemental Arrows-B block===
{{Main|Supplemental Arrows-B|l1=Supplemental Arrows-B (Unicode block)}}
The Supplemental Arrows-B block (U+2900&amp;ndash;U+297F) contains arrows and arrow-like operators (arrow tails, crossing arrows, curved arrows, and harpoons).
{{Unicode chart Supplemental Arrows-B}}

===Combining Diacritical Marks for Symbols block===
{{Main|Combining Diacritical Marks for Symbols|l1=Combining Diacritical Marks for Symbols (Unicode block)}}
The Combining Diacritical Marks for Symbols block contains arrows, dots, enclosures, and overlays for modifying symbol characters.

The math subset of this block is U+20D0&amp;ndash;U+20DC, U+20E1, U+20E5&amp;ndash;U+20E6, and U+20EB&amp;ndash;U+20EF.
{{Unicode chart Combining Diacritical Marks for Symbols}}

===Arabic Mathematical Alphabetic Symbols block===
{{Main|Arabic Mathematical Alphabetic Symbols|l1=Arabic Mathematical Alphabetic Symbols block}}
The Arabic Mathematical Alphabetic Symbols block (U+1EE00&amp;ndash;U+1EEFF) contains characters used in Arabic mathematical expressions.
{{Unicode chart Arabic Mathematical Alphabetic Symbols}}

==Characters in other blocks==
Mathematical characters also appear in other blocks. Below is a list of these characters as of Unicode version 11.0:

{|
|- style="vertical-align:top;"
| &lt;!-- newspaper column 1 --&gt;
*[[Basic Latin (Unicode block)|Basic Latin block]]
{|
| U+002B || {{Large|+}} || PLUS SIGN
|-
| U+002D || {{Large|-}} || HYPHEN-MINUS
|-
| U+003C || {{Large|&amp;lt;}} || LESS-THAN SIGN
|-
| U+003D || {{Large|1==}} || EQUALS SIGN
|-
| U+003E || {{Large|&amp;gt;}} || GREATER-THAN SIGN
|-
| U+005E || {{Large|^}} || CIRCUMFLEX ACCENT
|-
| U+007C || {{Large|&lt;nowiki&gt;|&lt;/nowiki&gt;}} || VERTICAL LINE
|-
| U+007E || {{Large|~}} || TILDE
|}
*[[Latin-1 Supplement (Unicode block)|Latin-1 Supplement block]]
{|
| U+00AC || {{Large|¬}} || NOT SIGN
|-
| U+00B1 || {{Large|±}} || PLUS-MINUS SIGN
|-
| U+00D7 || {{Large|×}} || MULTIPLICATION SIGN
|-
| U+00F7 || {{Large|÷}} || DIVISION SIGN
|}
*[[Greek and Coptic|Greek and Coptic block]]
{|
| U+03D0 || {{Large|ϐ}} || GREEK BETA SYMBOL
|-
| U+03D1 || {{Large|ϑ}} || GREEK THETA SYMBOL
|-
| U+03D2 || {{Large|ϒ}} || GREEK UPSILON WITH HOOK SYMBOL
|-
| U+03D5 || {{Large|ϕ}} || GREEK PHI SYMBOL
|-
| U+03F0 || {{Large|ϰ}} || GREEK KAPPA SYMBOL
|-
| U+03F1 || {{Large|ϱ}} || GREEK RHO SYMBOL
|-
| U+03F4 || {{Large|ϴ}} || GREEK CAPITAL THETA SYMBOL
|-
| U+03F5 || {{Large|ϵ}} || GREEK LUNATE EPSILON SYMBOL
|-
| U+03F6 || {{Large|϶}} || GREEK REVERSED LUNATE EPSILON SYMBOL
|}
*[[Arabic (Unicode block)|Arabic block]]
{|
| U+0606 || {{Large|؆}} || ARABIC-INDIC CUBE ROOT
|-
| U+0607 || {{Large|؇}} || ARABIC-INDIC FOURTH ROOT
|-
| U+0608 || {{Large|؈}} || ARABIC RAY
|}
*[[General Punctuation (Unicode block)|General Punctuation block]]
{|
| U+2016 || {{Large|‖}} || DOUBLE VERTICAL LINE
|-
| U+2032 || {{Large|′}} || PRIME
|-
| U+2033 || {{Large|″}} || DOUBLE PRIME
|-
| U+2034 || {{Large|‴}} || TRIPLE PRIME
|-
| U+2040 || {{Large|⁀}} || CHARACTER TIE
|-
| U+2044 || {{Large|⁄}} || FRACTION SLASH
|-
| U+2052 || {{Large|⁒}} || COMMERCIAL MINUS SIGN
|-
| U+2061 || {{small|''note''}} || FUNCTION APPLICATION
|-
| U+2062 || {{small|''note''}} || INVISIBLE TIMES
|-
| U+2063 || {{small|''note''}} || INVISIBLE SEPARATOR
|-
| U+2064 || {{small|''note''}} || INVISIBLE PLUS
|}
''Note: non-marking character''
| &lt;!-- newspaper column 2 --&gt;
*[[Superscripts and Subscripts|Superscripts and Subscripts block]]
{|
| U+207A || {{Large|⁺}} || SUPERSCRIPT PLUS SIGN
|-
| U+207B || {{Large|⁻}} || SUPERSCRIPT MINUS
|-
| U+207C || {{Large|⁼}} || SUPERSCRIPT EQUALS SIGN
|-
| U+207D || {{Large|⁽}} || SUPERSCRIPT LEFT PARENTHESIS
|-
| U+207E || {{Large|⁾}} || SUPERSCRIPT RIGHT PARENTHESIS
|-
| U+208A || {{Large|₊}} || SUBSCRIPT PLUS SIGN
|-
| U+208B || {{Large|₋}} || SUBSCRIPT MINUS
|-
| U+208C || {{Large|₌}} || SUBSCRIPT EQUALS SIGN
|-
| U+208D || {{Large|₍}} || SUBSCRIPT LEFT PARENTHESIS
|-
| U+208E || {{Large|₎}} || SUBSCRIPT RIGHT PARENTHESIS
|}
*[[Miscellaneous Symbols|Miscellaneous Symbols block]]
{|
| U+2605 || {{Large|★}} || BLACK STAR
|-
| U+2606 || {{Large|☆}} || WHITE STAR
|-
| U+2640 || {{Large|♀}} || FEMALE SIGN
|-
| U+2642 || {{Large|♂}} || MALE SIGN
|-
| U+2660 || {{Large|♠}} || BLACK SPADE SUIT
|-
| U+2661 || {{Large|♡}} || WHITE HEART SUIT
|-
| U+2662 || {{Large|♢}} || WHITE DIAMOND SUIT
|-
| U+2663 || {{Large|♣}} || BLACK CLUB SUIT
|-
| U+266D || {{Large|♭}} || MUSIC FLAT SIGN
|-
| U+266E || {{Large|♮}} || MUSIC NATURAL SIGN
|-
| U+266F || {{Large|♯}} || MUSIC SHARP SIGN
|}
*[[Alphabetic Presentation Forms|Alphabetic Presentation Forms block]]
{|
| U+FB29 || {{Large|﬩}} || HEBREW LETTER ALTERNATIVE PLUS SIGN
|}
*[[Small Form Variants|Small Form Variants block]]
{|
| U+FE61 || {{Large|﹡}} || SMALL ASTERISK
|-
| U+FE62 || {{Large|﹢}} || SMALL PLUS SIGN
|-
| U+FE63 || {{Large|﹣}} || SMALL HYPHEN-MINUS
|-
| U+FE64 || {{Large|﹤}} || SMALL LESS-THAN SIGN
|-
| U+FE65 || {{Large|﹥}} || SMALL GREATER-THAN SIGN
|-
| U+FE66 || {{Large|﹦}} || SMALL EQUALS SIGN
|-
| U+FE68 || {{Large|﹨}} || SMALL REVERSE SOLIDUS
|}
*[[Halfwidth and fullwidth forms|Halfwidth and Fullwidth Forms block]]
{|
| U+FF0B || {{Large|＋}} || FULLWIDTH PLUS SIGN
|-
| U+FF1C || {{Large|＜}} || FULLWIDTH LESS-THAN SIGN
|-
| U+FF1D || {{Large|＝}} || FULLWIDTH EQUALS SIGN
|-
| U+FF1E || {{Large|＞}} || FULLWIDTH GREATER-THAN SIGN
|-
| U+FF3C || {{Large|＼}} || FULLWIDTH REVERSE SOLIDUS
|-
| U+FF3E || {{Large|＾}} || FULLWIDTH CIRCUMFLEX ACCENT
|-
| U+FF5C || {{Large|｜}} || FULLWIDTH VERTICAL LINE
|-
| U+FF5E || {{Large|～}} || FULLWIDTH TILDE
|-
| U+FFE2 || {{Large|￢}} || FULLWIDTH NOT SIGN
|-
| U+FFE9 || {{Large|￩}} || HALFWIDTH LEFTWARDS ARROW
|-
| U+FFEA || {{Large|￪}} || HALFWIDTH UPWARDS ARROW
|-
| U+FFEB || {{Large|￫}} || HALFWIDTH RIGHTWARDS ARROW
|-
| U+FFEC || {{Large|￬}} || HALFWIDTH DOWNWARDS ARROW
|}
|}

== See also ==
* [[List of mathematical symbols]]
* [[List of logic symbols]]
* [[Greek letters used in mathematics, science, and engineering]]
* [[List of letters used in mathematics and science]]
* [[Latin letters used in mathematics]]
* [[Unicode subscripts and superscripts]]
* [[Unicode symbols]]
* [[CJK Compatibility|CJK Compatibility Unicode symbols]] includes symbols for [[International System of Units|SI]] units
* [[Order of magnitude|Units for order of magnitude]] shows position of [[International System of Units|SI]] units

== References ==
{{reflist}}

== External links ==
*{{cite book | title = Mathematical Markup Language (MathML) W3C Recommendation  | publisher = W3C | version = 3.0 | edition = 2nd | date = 10 April 2014 | url = http://www.w3.org/TR/MathML/}}
*Images of glyphs in [http://www.w3.org/TR/MathML2/chapter6.html#chars.16x16-tables section 6.3.3] of the {{cite book | title = Mathematical Markup Language (MathML) W3C Recommendation  | publisher = W3C
 | version = 2.0 | edition = 2nd | date = 21 February 2001 | url = http://www.w3.org/TR/2001/REC-MathML2-20010221/}}

{{Unicode navigation}}

[[Category:Unicode blocks|Mathematical operators]]
[[Category:Mathematical symbols]]</text>
      <sha1>i56iqc2rrmn9xjp7c6unqeujxp0sfqi</sha1>
    </revision>
  </page>
  <page>
    <title>Matrix norm</title>
    <ns>0</ns>
    <id>1543735</id>
    <revision>
      <id>868766605</id>
      <parentid>865839515</parentid>
      <timestamp>2018-11-14T08:59:41Z</timestamp>
      <contributor>
        <username>دادمهر</username>
        <id>34936650</id>
      </contributor>
      <comment>/* Frobenius norm */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="17781">In [[mathematics]], a '''matrix norm''' is a [[vector norm]] in a vector space whose elements (vectors) are [[matrix (mathematics)|matrices]] (of given dimensions).

== Definition ==

In what follows, &lt;math&gt;K&lt;/math&gt; will denote a [[field (mathematics)|field]] of either [[real number|real]] or [[complex number]]s. 

Let &lt;math&gt;K^{m \times n}&lt;/math&gt; denote the [[vector space]] of all matrices of size &lt;math&gt;m \times n&lt;/math&gt; (with &lt;math&gt;m&lt;/math&gt; rows and &lt;math&gt;n&lt;/math&gt; columns) with entries in the field &lt;math&gt;K&lt;/math&gt;.

A matrix norm is a [[Norm (mathematics)|norm]] on the vector space &lt;math&gt;K^{m \times n}&lt;/math&gt;. Thus, the matrix norm is a [[Function (mathematics)|function]] &lt;math&gt;\|\cdot\| : K^{m \times n} \to \mathbb{R}&lt;/math&gt; that must satisfy the following properties: 

For all scalars &lt;math&gt;\alpha&lt;/math&gt; in &lt;math&gt;K&lt;/math&gt; and for all matrices &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; in &lt;math&gt;K^{m \times n}&lt;/math&gt;,

*&lt;math&gt;\|\alpha A\|=|\alpha| \|A\|&lt;/math&gt;     (being ''absolutely homogeneous'')
*&lt;math&gt;\|A+B\| \le \|A\|+\|B\|&lt;/math&gt;     (being ''sub-additive'' or satisfying the ''triangle inequality'')
*&lt;math&gt;\|A\|\ge 0&lt;/math&gt;     (being ''positive-valued'')
*&lt;math&gt;\|A\|= 0&lt;/math&gt; [[iff]] &lt;math&gt;A=0_{m,n}&lt;/math&gt;     (being ''definite'') 

Additionally, in the case of square matrices (thus, {{nowrap|1=''m'' = ''n''}}), some (but not all) matrix norms satisfy the following condition, which is related to the fact that matrices are more than just vectors:

*&lt;math&gt;\|AB\| \le \|A\|\|B\| &lt;/math&gt; for all matrices &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; in &lt;math&gt;K^{n \times n}.&lt;/math&gt;

A matrix norm that satisfies this additional property is called a '''sub-multiplicative norm''' (in some books, the terminology ''matrix norm'' is used only for those norms which are sub-multiplicative). The set of all &lt;math&gt;n\times n&lt;/math&gt; matrices, together with such a sub-multiplicative norm, is an example of a [[Banach algebra]].

The definition of sub-multiplicativity is sometimes extended to non-square matrices, for instance in the case of the induced ''p''-norm, where for &lt;math&gt;A\in{K}^{m\times n}&lt;/math&gt; and &lt;math&gt;B\in{K}^{n\times k}&lt;/math&gt; holds that &lt;math&gt;\|AB\|_q\leq\|A\|_p\|B\|_q&lt;/math&gt;. Here &lt;math&gt;\|\cdot\|_p&lt;/math&gt; and &lt;math&gt;\|\cdot\|_q&lt;/math&gt; are the norms induced from &lt;math&gt;K^{n}&lt;/math&gt; and &lt;math&gt;K^{k}&lt;/math&gt;, respectively, and {{nowrap|''p'',''q'' ≥ 1}}.

There are three types of matrix norms which will be discussed below:
* Matrix norms induced by vector norms,
* Entrywise matrix norms, and
* Schatten norms.

==Matrix norms induced by vector norms==
{{Main article|Operator norm}}
Suppose a [[vector norm]] &lt;math&gt;\|\cdot\|&lt;/math&gt; on &lt;math&gt;K^d&lt;/math&gt; is given (for {{mvar|K}} a field).  Any &lt;math&gt;m \times n&lt;/math&gt; matrix {{mvar|A}} induces a linear operator from &lt;math&gt;K^n&lt;/math&gt; to &lt;math&gt;K^m&lt;/math&gt; with respect to the standard basis, and one defines the corresponding ''[[induced norm]]'' or ''[[operator norm]]'' on the space &lt;math&gt;K^{m \times n}&lt;/math&gt; of all &lt;math&gt;m \times n&lt;/math&gt; matrices as follows:
:&lt;math&gt; \begin{align}
\|A\| &amp;= \sup\{\|Ax\| : x\in K^n \text{ with }\|x\|= 1\} \\
&amp;= \sup\left\{\frac{\|Ax\|}{\|x\|} : x\in K^n \text{ with }x\ne 0\right\}.
\end{align} &lt;/math&gt;

In particular, if the [[Vector norm#p-norm|''p''-norm for vectors]] ({{math|1 &amp;le; ''p'' &amp;le; &amp;infin;}}) is used for both spaces &lt;math&gt;K^n&lt;/math&gt; and &lt;math&gt;K^m&lt;/math&gt;,
then the corresponding induced operator norm is:

:&lt;math&gt; \|A\| _p = \sup \limits _{x \ne 0} \frac{\| A x\| _p}{\|x\|_p}. &lt;/math&gt;

These induced norms are different from the [[#"Entrywise" matrix norms|"entrywise"]] ''p''-norms and the [[Robert Schatten|Schatten]] ''p''-norms for matrices treated below, which are also usually denoted by &lt;math&gt; \|A\|_p .&lt;/math&gt;  

:'''Note:''' We have described above the ''induced operator norm'' when the same vector norm was used in the "departure space" &lt;math&gt;K^n&lt;/math&gt; and the "arrival space" &lt;math&gt;K^m&lt;/math&gt; of the operator &lt;math&gt;A \in K^{m\times n}&lt;/math&gt;. This is not a necessary restriction. More generally, given a norm &lt;math&gt;\|\cdot\|_\alpha&lt;/math&gt; on &lt;math&gt;K^n&lt;/math&gt;, and a norm &lt;math&gt;\|\cdot\|_\beta&lt;/math&gt; on &lt;math&gt;K^m&lt;/math&gt;,  one can define a matrix norm on &lt;math&gt;K^{m\times n}&lt;/math&gt; induced by these norms:

::&lt;math&gt; \|A\|_{\alpha,\beta} = \max_{x \ne 0} \frac{\|A x\|_\beta}{\|x\|_\alpha}. &lt;/math&gt;

:The matrix norm &lt;math&gt;\|A\|_{\alpha,\beta}&lt;/math&gt; is sometimes called a subordinate norm. Subordinate norms are consistent with the norms that induce them, giving
::&lt;math&gt;
\|Ax\|_\beta\leq \|A\|_{\alpha,\beta}\|x\|_\alpha.
&lt;/math&gt;

Any induced operator norm is a sub-multiplicative matrix norm: &lt;math&gt; \| AB\| \leq \| A \| \| B \| &lt;/math&gt;; this follows from
:&lt;math&gt;\| ABx \| \leq \| A \| \| Bx \| \leq \| A \| \| B \| \| x \|&lt;/math&gt; 
and &lt;math&gt; \max \limits _{ \| x \| = 1} \| AB x \| = \| AB\| . &lt;/math&gt;  

Moreover, any induced norm satisfies the inequality
{{NumBlk|:|&lt;math&gt;\|A^r\|^{1/r} \ge \rho(A), &lt;/math&gt;|{{EquationRef|1}}}}
where {{math|&amp;rho;(''A'')}} is the [[spectral radius]] of {{mvar|A}}. For [[Symmetric matrix|symmetric]] or [[Hermitian matrix|hermitian]] {{mvar|A}}, we have equality in ({{EquationNote|1}}) for the 2-norm, since in this case the 2-norm ''is'' precisely the spectral radius of {{mvar|A}}. For an arbitrary matrix, we may not have equality for any norm; a counterexample being given by &lt;math&gt;
      A = \begin{bmatrix}
           0 &amp; 1  \\
           0 &amp; 0
         \end{bmatrix}
&lt;/math&gt;, which has vanishing spectral radius.  In any case, for square matrices
we have the [[spectral radius formula]]:
: &lt;math&gt;\lim_{r\rarr\infty}\|A^r\|^{1/r}=\rho(A). &lt;/math&gt;

=== Special cases ===
In the special cases of &lt;math&gt;p = 1, 2, \infty,&lt;/math&gt;  the induced matrix norms can be computed or estimated by

:&lt;math&gt; \|A\|_1 = \max_{1 \leq j \leq n} \sum_{i=1}^m | a_{ij} |, &lt;/math&gt;

which is simply the maximum absolute column sum of the matrix;

:&lt;math&gt; \|A\|_\infty = \max_{1 \leq i \leq m} \sum _{j=1}^n | a_{ij} |, &lt;/math&gt;

which is simply the maximum absolute row sum of the matrix;

:&lt;math&gt; \|A\| _2 = \sigma_{\max}(A) \leq \left(\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2\right)^{1/2}= \|A\|_{\rm F},&lt;/math&gt; 

where in left hand side &lt;math&gt;\sigma_{\max}(A)&lt;/math&gt; represents the largest singular value of matrix &lt;math&gt;A&lt;/math&gt;, and on the right hand side &lt;math&gt;\|A\|_{\rm F}&lt;/math&gt; is the [[#Frobenius norm|Frobenius norm]]. The first inequality can be derived from the fact that the trace of a matrix is equal to the sum of its eigenvalues. The equality holds if and only if the matrix &lt;math&gt;A&lt;/math&gt; is a rank-one matrix or a zero matrix. 

For example, if the matrix &lt;math&gt;A&lt;/math&gt; is defined by
:&lt;math&gt; 
      A = \begin{bmatrix}
            -3 &amp; 5 &amp; 7 \\
             2 &amp; 6 &amp; 4 \\
             0 &amp; 2 &amp; 8 \\
          \end{bmatrix},
&lt;/math&gt;

then we have
:&lt;math&gt;\|A\|_1 = \max(|{-3}|+2+0; 5+6+2; 7+4+8) = \max(5,13,19) = 19&lt;/math&gt;

and
:&lt;math&gt;\|A\|_\infty = \max(|{-3}|+5+7; 2+6+4;0+2+8) = \max(15,12,10) = 15 .&lt;/math&gt;

{{Anchor|Spectral norm}}In the special case of &lt;math&gt;p = 2&lt;/math&gt; (the [[Euclidean norm]] or &lt;math&gt;\ell_2&lt;/math&gt;-norm for vectors), the induced matrix norm is the ''spectral norm''. 
The spectral norm of a matrix &lt;math&gt;A&lt;/math&gt; is the largest [[singular value]] of &lt;math&gt;A&lt;/math&gt; i.e. the square root of the largest [[eigenvalue]] of the [[positive-semidefinite matrix]] &lt;math&gt;A^{*}A&lt;/math&gt;:
:&lt;math&gt; \|A\|_2 = \sqrt{\lambda_{\max}\left(A^{^*} A\right)} = \sigma_{\max}(A)&lt;/math&gt;&lt;ref&gt;Carl D. Meyer,
Matrix Analysis and Applied Linear Algebra, §5.2, p.281, Society for Industrial &amp; Applied Mathematics, June 2000.&lt;/ref&gt; where &lt;math&gt;A^*&lt;/math&gt; denotes the [[conjugate transpose]] of &lt;math&gt;A&lt;/math&gt;.

=="Entrywise" matrix norms==

These norms treat an &lt;math&gt; m \times n &lt;/math&gt; matrix as a vector of size &lt;math&gt; m n &lt;/math&gt;, and 
use one of the familiar vector norms.

For example, using the ''p''-norm for vectors, {{nowrap|''p'' ≥ 1}}, we get:

:&lt;math&gt;\Vert A \Vert_p = \Vert \mathrm{vec}(A) \Vert_p = \left( \sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^p \right)^{1/p}&lt;/math&gt;

This is a different norm from the induced ''p''-norm (see above) and the Schatten ''p''-norm (see below), but the notation is the same.

The special case ''p'' = 2 is the Frobenius norm, and ''p'' = &amp;infin; yields the maximum norm.

==={{math|''L''&lt;sub&gt;2,1&lt;/sub&gt;}} and {{math|''L&lt;sub&gt;p,q&lt;/sub&gt;''}} norms===

Let &lt;math&gt;(a_1, \ldots, a_n) &lt;/math&gt; be the columns of matrix &lt;math&gt;A&lt;/math&gt;. The &lt;math&gt;L_{2,1}&lt;/math&gt; norm&lt;ref&gt;{{cite conference | last1=Ding | first1=Chris | last2=Zhou | first2=Ding | last3=He | first3=Xiaofeng | last4=Zha | first4=Hongyuan | title=R1-PCA: Rotational Invariant L1-norm Principal Component Analysis for Robust Subspace Factorization | booktitle=Proceedings of the 23rd International Conference on Machine Learning | series=ICML '06 |date=June 2006 | isbn=1-59593-383-2 | location=Pittsburgh, Pennsylvania, USA | pages=281–288 | doi=10.1145/1143844.1143880 | publisher=ACM }}&lt;/ref&gt; is the sum of the Euclidean norms of the columns of the matrix:

:&lt;math&gt;\Vert A \Vert_{2,1} 
= \sum_{j=1}^n \Vert a_{j} \Vert_2
= \sum_{j=1}^n \left( \sum_{i=1}^m |a_{ij}|^2 \right)^{1/2}&lt;/math&gt;

The &lt;math&gt;L_{2,1}&lt;/math&gt; norm as an error function is more robust
since the error for each data point (a column) is not squared. It is used in [[robust data analysis]] and [[sparse coding]].

The &lt;math&gt;L_{2,1}&lt;/math&gt; norm can be generalized to the &lt;math&gt;L_{p,q}&lt;/math&gt; norm, {{nowrap|''p'', ''q'' ≥ 1}}, defined by 
:&lt;math&gt;\Vert A \Vert_{p,q} 
=  \left(\sum_{j=1}^n \left( \sum_{i=1}^m |a_{ij}|^p \right)^{q/p}\right)^{1/q}
&lt;/math&gt;

===Frobenius norm===

{{Main article|Hilbert–Schmidt operator}}
{{see also|Frobenius inner product}}

When {{nowrap|1=''p'' = ''q'' = 2}} for the &lt;math&gt;L_{p,q}&lt;/math&gt; norm, it is called the '''Frobenius norm''' or the '''Hilbert–Schmidt norm''', though the latter term is used more frequently in the context of operators on (possibly infinite-dimensional) [[Hilbert space]]. This norm can be defined in various ways:

:&lt;math&gt;\|A\|_{\rm F} = \sqrt{\sum_{i=1}^m \sum_{j=1}^n |a_{ij}|^2} = \sqrt{\operatorname{trace}\left(A^* A\right)} = \sqrt{\sum_{i=1}^{\min\{m, n\}} \sigma_i^2(A)},&lt;/math&gt;

where &lt;math&gt;\sigma_i(A)&lt;/math&gt; are the [[singular value]]s of &lt;math&gt;A&lt;/math&gt;. Recall that the [[trace (matrix)|trace function]] returns the sum of diagonal entries of a square matrix. 

The Frobenius norm is the Euclidean norm on &lt;math&gt;K^{n \times n}&lt;/math&gt; and comes from the [[Frobenius inner product]] on the space of all matrices.

The Frobenius norm is sub-multiplicative and is very useful for [[numerical linear algebra]]. This norm is often easier to compute than induced norms and has the useful property of being invariant under [[rotation matrix|rotations]], that is, &lt;math&gt;\|A\|_{\rm F}^2 = \|AR\|_{\rm F}^2 = \|RA\|_{\rm F}^2&lt;/math&gt; for any rotation matrix &lt;math&gt;R&lt;/math&gt;. This property follows from the trace definition restricted to real matrices:

:&lt;math&gt;\|AR\|_{\rm F}^2
  = \operatorname{trace}\left( R^\textsf{T} A^\textsf{T}A R \right)
  = \operatorname{trace}\left( RR^\textsf{T} A^\textsf{T}A \right)
  = \operatorname{trace}\left( A^\textsf{T} A \right)
  = \|A\|_{\rm F}^2
&lt;/math&gt;

and

:&lt;math&gt;\|RA\|_{\rm F}^{2}
  = \operatorname{trace}\left( A^\textsf{T} R^\textsf{T} RA \right)
  = \operatorname{trace}\left( A^\textsf{T} A \right)
  = \|A\|_{\rm F}^2,
&lt;/math&gt; 

where we have used the orthogonal nature of &lt;math&gt;R&lt;/math&gt; (that is, &lt;math&gt;R^\textsf{T} R = R R^\textsf{T} = \mathbf{I}&lt;/math&gt;) and the cyclic nature of the trace (&lt;math&gt;\operatorname{trace}(XYZ) = \operatorname{trace}(ZXY)&lt;/math&gt;). More generally the norm is invariant under a unitary transformation for complex matrices.

It also satisfies 

:&lt;math&gt;\|A^{\rm T}A\|_{\rm F} = \|AA^{\rm T}\|_{\rm F} \leq \|A\|_{\rm F}^2&lt;/math&gt;
and 
:&lt;math&gt;\|A+B\|_{\rm F}^2 = \|A\|_{\rm F}^2 + \|B\|_{\rm F}^2 + 2 \langle A, B \rangle_\mathrm{F},&lt;/math&gt;

where &lt;math&gt;\langle A, B \rangle_\mathrm{F}&lt;/math&gt; is the [[Frobenius inner product]].

===Max norm===

The '''max norm''' is the elementwise norm with ''p'' = ''q'' = &amp;infin;:
:&lt;math&gt; \|A\|_{\max} = \max_{ij} |a_{ij}|. &lt;/math&gt; 
This norm is not [[Matrix norm#Definition|sub-multiplicative]].

==Schatten norms==
{{details|Schatten norm}}

The Schatten ''p''-norms arise when applying the ''p''-norm to the vector of [[singular value decomposition|singular values]] of a matrix. If the singular values are denoted by ''&amp;sigma;&lt;sub&gt;i&lt;/sub&gt;'', then the Schatten ''p''-norm is defined by
:&lt;math&gt; \|A\|_p = \left( \sum_{i=1}^{\min\{m,\,n\}} \sigma_{i}^p(A) \right)^{1/p}. \, &lt;/math&gt;
These norms again share the notation with the induced and entrywise ''p''-norms, but they are different.

All Schatten norms are sub-multiplicative. They are also unitarily invariant, which means that &lt;math&gt;\|A\| = \|UAV\|&lt;/math&gt; for all matrices &lt;math&gt;A&lt;/math&gt; and all [[unitary matrix|unitary matrices]] &lt;math&gt;U&lt;/math&gt; and &lt;math&gt;V&lt;/math&gt;.

The most familiar cases are ''p'' = 1, 2, &amp;infin;. The case ''p'' = 2 yields the Frobenius norm, introduced before. The case ''p'' = &amp;infin; yields the spectral norm, which is the operator norm induced by the vector 2-norm (see above). Finally, ''p'' = 1 yields the '''nuclear norm''' (also known as the ''trace norm'', or the [[Singular Value Decomposition#Ky Fan norms|Ky Fan]] 'n'-norm&lt;ref&gt;{{Cite journal|last=Fan|first=Ky.|date=1951|title=Maximum properties and inequalities for the eigenvalues of completely continuous operators|url=http://www.pnas.org/content/37/11/760.citation|journal=Proceedings of the National Academy of Sciences of the United States of America|volume=37|issue=11|pages=760–766|doi=10.1073/pnas.37.11.760|pmc=1063464}}&lt;/ref&gt;), defined as

:&lt;math&gt;\|A\|_{*} = \operatorname{trace} \left(\sqrt{A^*A}\right) = \sum_{i=1}^{\min\{m,\,n\}}\!\sigma_{i}(A).&lt;/math&gt;

(Here &lt;math&gt;\sqrt{A^*A}&lt;/math&gt; denotes a positive semidefinite matrix &lt;math&gt;B&lt;/math&gt; such that &lt;math&gt;BB=A^*A&lt;/math&gt;. More precisely, since &lt;math&gt;A^*A&lt;/math&gt; is a [[positive semidefinite matrix]], its [[square root of a matrix|square root]] is well-defined.)

==Consistent norms==
A matrix norm &lt;math&gt;\| \cdot \|&lt;/math&gt; on &lt;math&gt;K^{m \times n}&lt;/math&gt; is called ''consistent'' with a vector norm &lt;math&gt;\| \cdot \|_{a}&lt;/math&gt; on &lt;math&gt;K^n&lt;/math&gt; and a vector norm &lt;math&gt;\| \cdot \|_{b}&lt;/math&gt; on &lt;math&gt;K^m&lt;/math&gt; if:
:&lt;math&gt;\|Ax\|_b \leq \|A\| \|x\|_a&lt;/math&gt;
for all &lt;math&gt;A \in K^{m \times n}, x \in K^n&lt;/math&gt;.  All induced norms are consistent by definition.

==Compatible norms==
A matrix norm &lt;math&gt;\| \cdot \|&lt;/math&gt; on &lt;math&gt;K^{n \times n}&lt;/math&gt; is called ''compatible'' with a vector norm &lt;math&gt;\| \cdot \|_{a}&lt;/math&gt; on &lt;math&gt;K^n&lt;/math&gt; if:
:&lt;math&gt;\|Ax\|_a \leq \|A\| \|x\|_a&lt;/math&gt;
for all &lt;math&gt;A \in K^{n \times n}, x \in K^n&lt;/math&gt;. Induced norms are compatible by definition.

==Equivalence of norms==

For any two matrix norms &lt;math&gt;\|\cdot\|_{\alpha}&lt;/math&gt; and &lt;math&gt;\|\cdot\|_{\beta}&lt;/math&gt;, we have

:&lt;math&gt;r\left\|A\right\|_\alpha\leq\left\|A\right\|_\beta\leq s\left\|A\right\|_\alpha&lt;/math&gt;

for some positive numbers ''r'' and ''s'', for all matrices&amp;nbsp;''A'' in &lt;math&gt;K^{m \times n}&lt;/math&gt;.  In other words, all norms on &lt;math&gt;K^{m \times n}&lt;/math&gt; are ''equivalent''; they induce the same [[topology (structure)|topology]] on &lt;math&gt;K^{m \times n}&lt;/math&gt;. This is true because the vector space&amp;nbsp;&lt;math&gt;K^{m \times n}&lt;/math&gt; has the finite [[dimension (mathematics)|dimension]]&amp;nbsp;&lt;math&gt;m \times n&lt;/math&gt;.

Moreover, for every vector norm&amp;nbsp;&lt;math&gt;\|\cdot\|&lt;/math&gt; on &lt;math&gt;\mathbb{R}^{n\times n}&lt;/math&gt;, there exists a unique positive real number&amp;nbsp;&lt;math&gt;k&lt;/math&gt; such that &lt;math&gt;l\|\cdot\|&lt;/math&gt; is a sub-multiplicative matrix norm for every &lt;math&gt;l \ge k&lt;/math&gt;.

A sub-multiplicative matrix norm &lt;math&gt;\|\cdot\|_{\alpha}&lt;/math&gt; is said to be ''minimal'' if there exists no other sub-multiplicative matrix norm &lt;math&gt;\|\cdot\|_{\beta}&lt;/math&gt; satisfying &lt;math&gt;\|\cdot\|_{\beta} &lt; \|\cdot\|_{\alpha}&lt;/math&gt;.

===Examples of norm equivalence===

Let &lt;math&gt;\|A\|_p&lt;/math&gt; once again refer to the norm induced by the vector ''p''-norm (as above in the Induced Norm section).

For matrix&amp;nbsp;&lt;math&gt;A\in\mathbb{R}^{m\times n}&lt;/math&gt; of [[Rank (linear algebra)|rank]] &lt;math&gt;r&lt;/math&gt;, the following inequalities hold:&lt;ref&gt;
[[Gene Golub|Golub, Gene]]; [[Charles Van Loan|Charles F. Van Loan]] (1996). Matrix Computations – Third Edition. Baltimore: The Johns Hopkins University Press, 56–57. {{ISBN|0-8018-5413-X}}.&lt;/ref&gt;&lt;ref&gt;
Roger Horn and Charles Johnson. ''Matrix Analysis,'' Chapter 5, Cambridge University Press, 1985. {{ISBN|0-521-38632-2}}.&lt;/ref&gt;

*&lt;math&gt;\|A\|_2\le\|A\|_F\le\sqrt{r}\|A\|_2&lt;/math&gt;
*&lt;math&gt;\|A\|_F \le \|A\|_{*} \le \sqrt{r} \|A\|_F&lt;/math&gt;
*&lt;math&gt;\|A\|_{\max} \le \|A\|_2 \le \sqrt{mn}\|A\|_{\max}&lt;/math&gt;
*&lt;math&gt;\frac{1}{\sqrt{n}}\|A\|_\infty\le\|A\|_2\le\sqrt{m}\|A\|_\infty&lt;/math&gt;
*&lt;math&gt;\frac{1}{\sqrt{m}}\|A\|_1\le\|A\|_2\le\sqrt{n}\|A\|_1.&lt;/math&gt;

Another useful inequality between matrix norms is
:&lt;math&gt;\|A\|_2\le\sqrt{\|A\|_1\|A\|_\infty},&lt;/math&gt;
which is a special case of [[Hölder's inequality]].

==Notes==
{{reflist}}

==References==
* [[James W. Demmel]], Applied Numerical Linear Algebra, section 1.7, published by SIAM, 1997.
* Carl D. Meyer, Matrix Analysis and Applied Linear Algebra, published by SIAM, 2000. [http://www.matrixanalysis.com]
* [[John Watrous (computer scientist)|John Watrous]], Theory of Quantum Information, [https://web.archive.org/web/20160304053759/https://cs.uwaterloo.ca/~watrous/CS766/LectureNotes/02.pdf 2.3 Norms of operators], lecture notes, University of Waterloo, 2011.
* [[Kendall Atkinson]], An Introduction to Numerical Analysis, published by John Wiley &amp; Sons, Inc 1989
*

&lt;!--Categories--&gt;
[[Category:Norms (mathematics)]]
[[Category:Linear algebra]]</text>
      <sha1>6qouahv5av6dr4x25djhh7bsvi2x422</sha1>
    </revision>
  </page>
  <page>
    <title>Most significant bit</title>
    <ns>0</ns>
    <id>149922</id>
    <redirect title="Bit numbering" />
    <revision>
      <id>835259023</id>
      <parentid>831465945</parentid>
      <timestamp>2018-04-07T15:44:07Z</timestamp>
      <contributor>
        <username>Klbrain</username>
        <id>11677590</id>
      </contributor>
      <comment>Merge to [[Bit numbering]] following 2016 proposal with consensus for alternative target; see [[Talk:Least significant bit#Merge with Most significant bit ?]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="114">#REDIRECT [[Bit numbering#Most significant bit]] {{R from merge}} {{R to section}}

[[Category:Binary arithmetic]]</text>
      <sha1>cpnxpe1ahxjthx2h601x66akoz7mtjd</sha1>
    </revision>
  </page>
  <page>
    <title>Moving least squares</title>
    <ns>0</ns>
    <id>6582659</id>
    <revision>
      <id>867674175</id>
      <parentid>836702702</parentid>
      <timestamp>2018-11-07T07:43:51Z</timestamp>
      <contributor>
        <ip>96.48.181.227</ip>
      </contributor>
      <comment>/* Definition */ remove redundant statement of f domain.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2620">'''Moving least squares''' is a method of reconstructing [[continuous function]]s from a [[set (mathematics)|set]] of unorganized point samples via the calculation of a [[weighted least squares]] [[measure (mathematics)|measure]] biased towards the region around the point at which the reconstructed value is requested.

In [[computer graphics]], the moving least squares method is useful for reconstructing a surface from a set of points. Often it is used to create a 3D surface from a [[point cloud]] through either [[downsampling]] or [[upsampling]].

==Definition==
[[Image:Moving_Least_Squares2.png|thumb|200px|Here is a 2D example. The circles are the samples  and the polygon is a linear interpolation. The blue curve is a smooth interpolation of order 3.]]
Consider a function &lt;math&gt;f: \mathbb{R}^n \to \mathbb{R}&lt;/math&gt; and a set of sample points &lt;math&gt;S = \{ (x_i,f_i) | f(x_i) = f_i \} &lt;/math&gt;. Then, the moving least square approximation of degree &lt;math&gt;m&lt;/math&gt; at the point &lt;math&gt;x&lt;/math&gt; is &lt;math&gt;\tilde{p}(x)&lt;/math&gt; where &lt;math&gt;\tilde{p}&lt;/math&gt; minimizes the weighted least-square error 
:&lt;math&gt;\sum_{i \in I} (p(x_i)-f_i)^2\theta(\|x-x_i\|)&lt;/math&gt;
over all polynomials &lt;math&gt;p&lt;/math&gt; of degree &lt;math&gt;m&lt;/math&gt; in &lt;math&gt;\mathbb{R}^n&lt;/math&gt;. &lt;math&gt;\theta(s)&lt;/math&gt; is the weight and it tends to zero as &lt;math&gt;s\to \infty&lt;/math&gt;.

In the example &lt;math&gt;\theta(s) = e^{-s^2}&lt;/math&gt;.  The smooth interpolator of "order 3" is a quadratic interpolator.

==See also==
*[[Local regression]]
*[[Diffuse element method]]
*[[Moving average]]

==References==
{{Reflist}}
*[http://dl.acm.org/citation.cfm?id=301704 The approximation power of moving least squares] David Levin, Mathematics of Computation, Volume 67, 1517-1531, 1998 [http://www.ams.org/mcom/1998-67-224/S0025-5718-98-00974-0/S0025-5718-98-00974-0.pdf ]
*[http://www.sciencedirect.com/science/article/pii/S0045794905000726/ Moving least squares response surface approximation: Formulation and metal forming applications] Piotr Breitkopf; Hakim Naceur; Alain Rassineux; Pierre Villon, Computers and Structures, Volume 83, 17-18, 2005.
* [http://www.springerlink.com/content/v7164702238848p1/ Generalizing the finite element method: diffuse approximation and diffuse elements], B Nayroles, G Touzot. Pierre Villon, P, Computational Mechanics Volume 10, pp 307-318, 1992

==External links==
* [http://www.nealen.net/projects/mls/asapmls.pdf An As-Short-As-Possible Introduction to the Least Squares, Weighted Least Squares and Moving Least Squares Methods for Scattered Data Approximation and Interpolation]

[[Category:Least squares]]

{{mathapplied-stub}}</text>
      <sha1>975kz21yw42h985no4arydomjij67v3</sha1>
    </revision>
  </page>
  <page>
    <title>Networks in labor economics</title>
    <ns>0</ns>
    <id>46897892</id>
    <revision>
      <id>741168787</id>
      <parentid>723030229</parentid>
      <timestamp>2016-09-25T20:38:05Z</timestamp>
      <contributor>
        <username>Marcocapelle</username>
        <id>14965160</id>
      </contributor>
      <minor/>
      <comment>sort key</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4999">{{Multiple issues|
{{essay-like|date=June 2015}}
{{lead rewrite|date=June 2015}}
}}

The importance of social ties in job searching is known, and empirically proved for quite a while, workers often find jobs through their friends and relatives. However, the exploration of the role of [[social network]]s in [[labor market]] outcomes has just recently started. New evidence shows that [[social networks]] not only increase the productivity of job searching but partly explain wage differences, and help decreasing the information asymmetry between the employer and employee.

==The model of Calvo-Armegnol and Jackson==
In economics researches, the role of social ties’ formalization in job searching often uses exogenous job networks therefore, the graph of the network is initially given. Using a similar framework, Calvo-Armegnol and Jackson&lt;ref&gt;Antoni Calvó-Armengol &amp; Matthew O. Jackson, 2004. "The Effects of Social Networks on Employment and Inequality," American Economic Review, American Economic Association, vol. 94(3), pages 426-454, June.&lt;/ref&gt;&lt;ref&gt;Calvo-Armengol, Antoni &amp; Jackson, Matthew O., 2007. "Networks in labor markets: Wage and employment dynamics and inequality," Journal of Economic Theory, Elsevier, vol. 132(1), pages 27-46, January.&lt;/ref&gt; were able to point out some network related labor market issues.

===The model===
In their basic model, in which they attempt to formalize the transmission of job information among individuals, the agents can be either employed with some non-zero, or unemployed with zero wages. The agents can get information about a job, and when they do so, they can decide whether to keep that information for themselves or pass it to their contacts. In the other phase, employed agents can lose their job with a given probability.

===Implications===
Important indication of their model is that if someone who is employed has the information about a job, she will pass it to her unemployed acquaintances who will then become employed. Therefore, there is a positive correlation between labor outcomes of an individual and her contacts. On the other hand, it can also give an explanation for long term unemployment. If someone’s acquaintances are unemployed as well, she has less chance to hear of some job opportunity. They also conclude that that different initial wage and employment can cause different drop-outs rates from the labor market, thus, it can explain the existence of wage inequalities across social groups. Calvo-Armengol and Jackson prove that position in the network, and structure of the network affect the probability of being unemployed as well.

==Referral based job search==
The effectiveness of job searching with personal contacts is the consequence not only the individuals’ but the employers’ behavior as well. They often choose to hire acquaintances of their current employees instead of using a bigger pool of applicants. It is due to the information asymmetry as they hardly know anything about the productivity of the applicant, and revealing it would be rather time consuming and expensive. However, employees might be aware both their contacts unobserved characteristics and the specific expectations of employers so they can enhance this imbalance. Another benefit for the firm is that due to the personal bond, present employees are motivated to choose a candidate who will perform well, since after the recommendation, their reputation is also at stake.

Dustman, Glitz and Schönberg&lt;ref&gt;Dustmann, Christian &amp; Glitz, Albrecht &amp; Schönberg, Uta, 2011. "Referral-based Job Search Networks," IZA Discussion Papers 5777, Institute for the Study of Labor (IZA).&lt;/ref&gt; showed that using personal connections in job search increases the initial wage and decreases the probability of leaving the firm.

Referral based job network can function even if there is no direct link between the referee and the potential worker. In the model of Finneran and Kelly,&lt;ref&gt;Finneran, Lisa &amp; Kelly, Morgan, 2003. "Social networks and inequality," Journal of Urban Economics, Elsevier, vol. 53(2), pages 282-299, March.&lt;/ref&gt; there is a hierarchical network in which workers has the opportunity to refer their acquaintances if their employer hires. Workers are referred for a job with some increasing  probability in regards to their ability, and productivity. In a hierarchical model like this, workers who take place on a lower level, far from the information, never get an offer. However, the authors have showed that there is a threshold of this referral probability over which even those skilled worker can be referred who are low in the hierarchy.  So there is a critical density of referral linkages exists under which no qualified workers can be referred, however, if the density of these linkages is high enough, all qualified workers will match with a job, despite their position in the network.

==References==
{{reflist}}

[[Category:Network theory]]
[[Category:Labour economics]]
[[Category:Social networks]]</text>
      <sha1>pvo66ji1atjwh5bumj8xc8ytsek2kyk</sha1>
    </revision>
  </page>
  <page>
    <title>Nullspace property</title>
    <ns>0</ns>
    <id>49664155</id>
    <revision>
      <id>853076612</id>
      <parentid>813532222</parentid>
      <timestamp>2018-08-02T08:22:49Z</timestamp>
      <contributor>
        <ip>87.182.229.180</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4539">In [[compressed sensing]], the [[Kernel (linear algebra)|nullspace]] property gives necessary and sufficient conditions on the reconstruction of sparse signals using the techniques of [[Relaxation (approximation)|&lt;math&gt;\ell_1&lt;/math&gt;-relaxation]]. The term "nullspace property" originates from Cohen, Dahmen, and DeVore.&lt;ref&gt;{{Cite journal|last=Cohen|first=Albert|last2=Dahmen|first2=Wolfgang|last3=DeVore|first3=Ronald|date=2009-01-01|title=Compressed sensing and best 𝑘-term approximation|url=http://www.ams.org/jams/2009-22-01/S0894-0347-08-00610-3/|journal=Journal of the American Mathematical Society|volume=22|issue=1|pages=211–231|doi=10.1090/S0894-0347-08-00610-3|issn=0894-0347}}&lt;/ref&gt; The nullspace property is often difficult to check in practice, and the [[restricted isometry property]] is a more modern condition in the field of compressed sensing.

== The technique of [[Relaxation (approximation)|&lt;math&gt;\ell_1&lt;/math&gt;]]-relaxation ==
The [[Convex optimization|non-convex]] &lt;math&gt;\ell_0&lt;/math&gt;-minimization problem,

&lt;math&gt;\min\limits_{x} \|x\|_0&lt;/math&gt;  subject to &lt;math&gt;Ax = b&lt;/math&gt;,

is a standard problem in compressed sensing. However, &lt;math&gt;\ell_0&lt;/math&gt;-minimization is known to be [[NP-hardness|NP-hard]] in general.&lt;ref&gt;{{Cite journal|last=Natarajan|first=B. K.|date=1995-04-01|title=Sparse Approximate Solutions to Linear Systems|url=https://dx.doi.org/10.1137/S0097539792240406|journal=SIAM J. Comput.|volume=24|issue=2|pages=227–234|doi=10.1137/S0097539792240406|issn=0097-5397}}&lt;/ref&gt; As such, the technique of [[Relaxation (approximation)|&lt;math&gt;\ell_1&lt;/math&gt;]]-relaxation is sometimes employed to circumvent the difficulties of signal reconstruction using the &lt;math&gt;\ell_0&lt;/math&gt;-norm. In [[Relaxation (approximation)|&lt;math&gt;\ell_1&lt;/math&gt;]]-relaxation, the [[Relaxation (approximation)|&lt;math&gt;\ell_1&lt;/math&gt;]] problem,

&lt;math&gt;\min\limits_{x} \|x\|_1&lt;/math&gt;  subject to &lt;math&gt;Ax = b&lt;/math&gt;,

is solved in place of the &lt;math&gt;\ell_0&lt;/math&gt; problem. Note that this relaxation is convex and hence amenable to the standard techniques of [[linear programming]] - a computationally desirable feature. Naturally we wish to know when [[Relaxation (approximation)|&lt;math&gt;\ell_1&lt;/math&gt;]]-relaxation will give the same answer as the &lt;math&gt;\ell_0&lt;/math&gt; problem. The nullspace property is one way to guarantee agreement.

== Definition ==
An &lt;math&gt;m \times n&lt;/math&gt; complex matrix  &lt;math&gt;A&lt;/math&gt; has the nullspace property of order &lt;math&gt;n&lt;/math&gt; if for all index sets &lt;math&gt;S&lt;/math&gt; with &lt;math&gt;s=|S| \leq n&lt;/math&gt; we have that: &lt;math&gt;\|\eta_S\|_1 &lt; \|\eta_{S^C}\|_1&lt;/math&gt;  for all &lt;math&gt;\eta \in \ker{A} \setminus \left\{0\right\}&lt;/math&gt;.

== Recovery Condition ==
The following theorem gives necessary and sufficient condition on the recoverability of a given &lt;math&gt;s&lt;/math&gt;-sparse vector in &lt;math&gt;\mathbb{C}^n&lt;/math&gt;. The proof of the theorem is a standard one, and the proof supplied here is summarized from Holger Rauhut.&lt;ref&gt;{{Cite book|url=http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.185.3754|title=Compressive Sensing and Structured Random Matrices|last=Rauhut|first=Holger}}&lt;/ref&gt;

&lt;math&gt;\textbf{Theorem:}&lt;/math&gt; Let &lt;math&gt;A&lt;/math&gt;  be a  &lt;math&gt;m \times n&lt;/math&gt; complex matrix. Then every &lt;math&gt;s&lt;/math&gt;-sparse signal &lt;math&gt;x \in \mathbb{C}^n&lt;/math&gt; is the unique solution to the &lt;math&gt;\ell_1&lt;/math&gt;-relaxation problem with &lt;math&gt;b = Ax&lt;/math&gt; if and only if &lt;math&gt;A&lt;/math&gt; satisfies the nullspace property with order &lt;math&gt;s&lt;/math&gt;.

&lt;math&gt;\textit{Proof:}&lt;/math&gt; For the forwards direction notice that &lt;math&gt;\eta_S&lt;/math&gt; and &lt;math&gt;-\eta_{S^C}&lt;/math&gt; are distinct vectors with &lt;math&gt;A(-\eta_{S^C}) = A(\eta_S)&lt;/math&gt; by the linearity of &lt;math&gt;A&lt;/math&gt;, and hence by uniqueness we must have &lt;math&gt;\|\eta_S\|_1 &lt; \|\eta_{S^C}\|_1&lt;/math&gt; as desired. For the backwards direction, let &lt;math&gt;x&lt;/math&gt; be &lt;math&gt;s&lt;/math&gt;-sparse and &lt;math&gt;z&lt;/math&gt; another (not necessary &lt;math&gt;s&lt;/math&gt;-sparse) vector such that &lt;math&gt;z \neq x&lt;/math&gt; and &lt;math&gt;Az = Ax&lt;/math&gt;. Define the (non-zero) vector &lt;math&gt;\eta = x - z&lt;/math&gt; and notice that it lies in the nullspace of &lt;math&gt;A&lt;/math&gt;. Call &lt;math&gt;S&lt;/math&gt; the support of &lt;math&gt;x&lt;/math&gt;, and then the result follows from an elementary application of the [[triangle inequality]]: &lt;math&gt;\|x\|_1 \leq \|x - z_S\|_1 + \|z_S\|_1 = \|\eta_S\|_1+\|z_S\|_1 &lt; \|\eta_{S^C}\|_1+\|z_S\|_1 = \|-z_{S^C}\|_1+\|z_S\|_1 = \|z\|_1&lt;/math&gt;, establishing the minimality of &lt;math&gt;x&lt;/math&gt;. &lt;math&gt;\square&lt;/math&gt;

==References==
{{Reflist}}

== External links ==

[[Category:Linear algebra]]</text>
      <sha1>ahyyk87x65kzze6up7rwfe9d9tburir</sha1>
    </revision>
  </page>
  <page>
    <title>OLGA (technology)</title>
    <ns>0</ns>
    <id>27908263</id>
    <revision>
      <id>741336832</id>
      <parentid>741336651</parentid>
      <timestamp>2016-09-26T21:21:09Z</timestamp>
      <contributor>
        <ip>12.6.145.178</ip>
      </contributor>
      <comment>/* References */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3100">'''OLGA''' is a modelling tool for [[transportation]] of [[Petroleum|oil]], [[natural gas]] and [[water]] in the same [[Pipeline transport|pipeline]], so-called [[Multiphase flow|multiphase]] transportation. The name is short for "oil and gas simulator". The main challenge with multiphase fluid flow is the formation of slugs (plugs of oil and water) in the pipelines, which causes large problems at the receiving end at the platform or the onshore plant. The modelling tool makes it possible to calculate the fluid flow and safely bring the flow to the receiving destination on shore, on a platform or a production ship through the pipes.

==History==

The idea for the tool was conceived in 1979 by two researchers at [[Institute for Energy Technology|IFE]], Norway: Dag Malnes and Kjell Bendiksen.&lt;ref&gt;http://www.ife.no/departments/process_and_fluid_flow_tech/historienomolga/view?searchterm=olga&lt;/ref&gt; The first version of OLGA was financed by [[Statoil]] and was ready in 1980. The tool was developed further by IFE in collaboration with [[SINTEF]] in the 1980s.

January 1, 1984 a joint industry agreement was signed by Statoil, IFE and SINTEF on the continued development of OLGA. IFE had the main responsibility for developing the model, while the technical experiments were performed in SINTEF’s laboratory at Tiller.

Until 2012 the SPT Group&lt;ref&gt;http://www.sptgroup.com/&lt;/ref&gt; owned the rights to OLGA. In March of 2012, Schlumberger announced an agreement with Altor Fund II for the acquisition of SPT Group.  The acquisition was completed in Q2.  SPT Group, founded in 1971, was headquartered in Norway employing approximately 280 people in 11 countries at the time of the acquisition.  The tool has been under continuous and still ongoing development, among others in the HORIZON II project where IFE and SPT Group are partners. OLGA has a global market share of about 90%. The technology is regarded as a central success for Norwegian petroleum research.

OLGA has enabled the development of oil and gas fields at deeper seas and farther from shore than would otherwise be possible without this technology, for example the fields Troll, Ormen Lange and Snøhvit.&lt;ref&gt;http://www.sintef.no/cgi-bin/MsmGo.exe?grab_id=0&amp;page_id=2822&amp;query=olga&amp;hiword=OLGAS%20olga%20&lt;/ref&gt;

==References==
{{Reflist}}
{{refbegin}}
* [http://www.ife.no/departments/process_and_fluid_flow_tech/historienomolga/view?searchterm=olga The history of OLGA]
* [http://www.offshore-technology.com/features/feature41077/ Go with the Flow - article 03 Sep 2008]
* [http://www.onepetro.org/mslib/servlet/onepetropreview?id=00024790&amp;soc=SPE A Transient Multiphase Temperature Prediction Program (Erickson, D.D., Mai, M.C., Conoco Inc)]
* [http://www.slb.com/news/press_releases/2012/2012_0320_slbsptgroup_pr.aspx= Schlumberger Announces Agreement to Acquire SPT Group]
{{refend}}

==External links==
* [http://www.software.slb.com/OLGA]

[[Category:Petroleum technology]]
[[Category:Multiphase flow]]
[[Category:Computational science]]
[[Category:Applied mathematics]]
[[Category:Research and development in Norway]]</text>
      <sha1>t3z1xdkg3o3pnru559d17vyiwb8nz40</sha1>
    </revision>
  </page>
  <page>
    <title>Odd number theorem</title>
    <ns>0</ns>
    <id>9156022</id>
    <revision>
      <id>822449745</id>
      <parentid>801113559</parentid>
      <timestamp>2018-01-26T12:50:11Z</timestamp>
      <contributor>
        <username>KolbertBot</username>
        <id>31691822</id>
      </contributor>
      <minor/>
      <comment>Bot: [[User:KolbertBot|HTTP→HTTPS]] (v481)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3124">{{no footnotes|date=August 2009}}

The '''odd number theorem''' is a theorem in [[strong gravitational lensing]] which comes directly from [[differential topology]].

The theorem states that ''the number of multiple images produced by a bounded transparent lens must be odd''.

== Formulation ==
The gravitational lensing is a thought to mapped from what's known as ''image plane'' to ''source plane'' following the formula :

&lt;math&gt;M: (u,v) \mapsto (u',v')&lt;/math&gt;.

== Argument ==
If we use direction [[cosine]]s describing the bent [[light ray]]s, we can write a [[vector field]] on &lt;math&gt;(u,v)&lt;/math&gt; plane &lt;math&gt;V:(s,w)&lt;/math&gt;.

However, only in some specific directions &lt;math&gt;V_0:(s_0,w_0)&lt;/math&gt;, will the bent light rays reach the observer, i.e., the images only form where &lt;math&gt; D=\delta V=0|_{(s_0,w_0)}&lt;/math&gt;. Then we can directly apply the [[Poincaré–Hopf theorem]] &lt;math&gt;\chi=\sum \text{index}_D = \text{constant}&lt;/math&gt;.

The index of sources and sinks is +1, and that of saddle points is &amp;minus;1. So the [[Euler characteristic]] equals the difference between the number of positive indices &lt;math&gt;n_{+}&lt;/math&gt; and the number of negative indices &lt;math&gt;n_{-}&lt;/math&gt;. For the far field case, there is only one image, i.e., &lt;math&gt; \chi=n_{+}-n_{-}=1&lt;/math&gt;. So the total number of images is &lt;math&gt; N=n_{+}+n_{-}=2n_{-}+1 &lt;/math&gt;, i.e., odd. The strict proof needs Uhlenbeck’s [[Morse theory]] of [[null geodesic]]s.

==References==
{{reflist}}
* Chwolson O., 1924. "Über eine mögliche Form fiktiver Doppelsterne", "Astronomische Nachrichten" 221, 329-330.
* Burke W.L., 1981. "Multiple gravitational imaging by distributed masses", ''Astrophysical Journal'' '''244''', L1.
* McKenzie R.H., 1985. "A gravitational lens produced an odd number of images", ''Journal of Mathematical Physics'' '''26''', 1592.
* Kozameh C, Lamberti P. W., Reula O. Global aspects of light cone cuts. J. Math. Phys. 32, 3423-3426 (1991).
* Lombardi M., An application of the topological degree to gravitational lenses. Modern Phys. Lett. A 13, 83-86 (1998).
* Wambsganss J., 1998. "Gravitational lensing in astronomy" http://relativity.livingreviews.org/Articles/lrr-1998-12/download/lrr-1998-12BW.pdf
* Schneider P., Ehlers J., Falco E. E. 1999. "Gravitational Lenses" Astronomy and Astrophysics Library. Springer
* Giannoni F., Lombardi M, 1999. "Gravitational lenses: Odd or even images?" "Class. Quantum Grav." 16, 375-415.
* Fritelli S., Newman E. T., 1999. "Exact universal gravitational lens equations" "Phys. Rev." D 59, 124001
* Perlick V., Gravitational lensing in asymptotically simple and empty spacetimes, Annalen der Physik 9, SI139-SI142 (2000)
* Perlick V., 2010. "Gravitational Lensing from a Spacetime Perspective" https://arxiv.org/abs/1010.3416
* Perlick V., Gravitational lensing from a geometric viewpoint, in B. Schmidt (ed.) "Einstein's field equations and their physical interpretations" Selected Essays in Honour of Jürgen Ehlers, Springer, Heidelberg (2000) pp.&amp;nbsp;373–425

[[Category:Gravitational lensing]]
[[Category:Physics theorems]]


{{astrology-stub}}
{{astronomy-stub}}
{{topology-stub}}</text>
      <sha1>2ilqxd5kcokt6ygkah800dr4b6ve7kc</sha1>
    </revision>
  </page>
  <page>
    <title>Order (mathematics)</title>
    <ns>0</ns>
    <id>19074048</id>
    <revision>
      <id>863991785</id>
      <parentid>841381628</parentid>
      <timestamp>2018-10-14T11:34:45Z</timestamp>
      <contributor>
        <ip>41.177.155.73</ip>
      </contributor>
      <comment>there were two "set theory" sections... Not anymore!</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3915">{{For|order in other disciplines|Order (disambiguation)}}

'''Order''' in mathematics may refer to:
{{TOC right}}
==Set theory==
* [[Total order]] and [[partial order]], a binary relation generalizing the usual ordering of numbers and of words in a dictionary
* [[Ordered set]]
* Order in [[Ramsey theory]], uniform structures in consequence to critical set cardinality

==Algebra==
*[[Order (group theory)]], the cardinality of a group or period of an element
* [[Order of a polynomial (disambiguation)]]
*[[square matrix|Order of a square matrix]], its dimension
*[[Order (ring theory)]], an algebraic structure
*[[Ordered group]]
*[[Ordered field]]

==Analysis==
*[[Order (differential equation)]] or [[Derivative#Higher derivatives|order of highest derivative]], of a differential equation
*[[Leading-order]] terms
*NURBS order, a number one greater than the degree of the polynomial representation of a [[non-uniform rational B-spline]]
*[[Order of convergence]], a measurement of convergence
*[[Order of derivation]]
*Order of an [[entire function]]
*[[Power series#Order of a power series|Order of a power series]], the lowest degree of its terms
*Ordered list, a [[sequence]] or [[tuple]]
*[[Orders of approximation]] in Big O notation
*[[Z-order (curve)]], a space-filling curve

==Arithmetic==
*[[Multiplicative order]] in modular arithmetic
*[[Order of operations]]
*[[Orders of magnitude]], a class of scale or magnitude of any amount

==Combinatorics==
*Order in the [[Josephus permutation]]
*[[Twelvefold way#Ordered selections and partitions|Ordered selections and partitions]] of the twelvefold way in combinatorics
*Ordered set, a  [[bijection]], [[cyclic order]], or [[permutation]]
*Unordered [[subset]] or combination
*[[Weak order of permutations]]

==Fractals==
*[[Complexor]], or complex order in fractals
*Order of extension in [[Lakes of Wada]]
*Order of [[fractal dimension]] (Rényi dimensions)
*Orders of construction in the [[Pythagoras tree (fractal)|Pythagoras tree]]

==Geometry==
*Long-range aperiodic order, in [[pinwheel tiling]], for instance

==Graphs==
*[[Glossary of graph theory terms#order|Graph order]], the number of nodes in a graph
*First order and second order [[logic of graphs]]
*[[Topological ordering]] of directed acyclic graphs
*[[Degeneracy (graph theory)|Degeneracy ordering]] of undirected graphs
*[[Chordal graph#Perfect elimination and efficient recognition |Elimination ordering]] of chordal graphs
*Order, the complexity of a structure within a graph: see [[haven (graph theory)]] and [[bramble (graph theory)]]

== Logic ==
In logic, model theory and type theory:

* [[Zeroth-order logic]]
* [[First-order logic]]
* [[Second-order logic]]
* [[Higher-order logic]]

== Order theory ==

*[[Order (journal)|''Order'' (journal)]], an academic journal on order theory
*[[Dense order]], a total order wherein between any unequal pair of elements there is always an intervening element in the order
*[[Glossary of order theory]]
*[[Lexicographical order]], an ordering method on sequences analogous to alphabetical order on words
*[[List of order topics]], list of order theory topics
*[[Order theory]], study of various binary relations known as orders
*[[Order topology]], a topology of total order for totally ordered sets
*[[Ordinal numbers]], numbers assigned to sets based on their set-theoretic order
*[[Partial order]], often called just "order" in order theory texts, a transitive antisymmetric relation
*[[Total order]], a partial order that is also total, in that either the relation or its inverse holds between any unequal elements

== Statistics ==
*[[Order statistics]]
*[[First-order statistics]], e.g., [[arithmetic mean]], median, quantiles
*[[Second-order statistics]], e.g., [[correlation]], power spectrum, variance
*[[Higher-order statistics]], e.g., bispectrum, kurtosis, skewness

{{SIA|mathematics}}
[[Category:Mathematical terminology]]</text>
      <sha1>ov1wseuvaahd6azy8g9t9wrkoplj8ts</sha1>
    </revision>
  </page>
  <page>
    <title>PCLake</title>
    <ns>0</ns>
    <id>34995809</id>
    <revision>
      <id>811353914</id>
      <parentid>666728890</parentid>
      <timestamp>2017-11-21T02:42:57Z</timestamp>
      <contributor>
        <username>Brenont</username>
        <id>4034676</id>
      </contributor>
      <minor/>
      <comment>Typo correction - [[Wikipedia:Typo|You can help!]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6433">'''PCLake''' is a dynamic, [[mathematical model]] used to study [[eutrophication]] effects in shallow [[lake]]s and ponds. PCLake models explicitly the most important [[Biotic component|biotic]] groups and their interrelations, within the general framework of [[nutrient cycle]]s. PCLake is used both by scientist and water managers.

==Background==
Typically, shallow lakes are in one of two contrasting [[alternative stable state]]s:&lt;ref&gt;Scheffer M, 1993. Alternative equilibria in shallow lakes. Trends in Ecology &amp; Evolution 8: 275–-279&lt;/ref&gt;  a clear state with submerged macrophytes and piscivorous fish, or a turbid state dominated by phytoplankton and benthivorous fish. A switch from one state to the other is largely driven by the input of nutrients ([[phosphorus]] and [[nitrogen]]) to the ecosystem.&lt;ref&gt;Janse JH, 1997. A model of nutrient dynamics in shallow lakes in relation to multiple stable states. Hydrobiologia 342/343: 1–8&lt;/ref&gt; If the nutrient loading exceeds a critical value, eutrophication causes a switch from the clear to the turbid state. As a result of urban water pollution and/or [[intensive agriculture]] in [[drainage basin|catchment areas]], many of the world’s shallow lakes and ponds are in a eutrophic state with turbid waters and poor ecological quality. In this turbid state, the lake also becomes subject to algal blooms of toxic [[cyanobacteria]] (also called blue-green algae). Recovery of the clear state however is difficult as the critical nutrient loading for the switch back is often found to be lower than the critical loading towards the turbid state. Lowering the nutrient input thus does not automatically lead to a switch back to the clear water phase. Hence, the system shows [[hysteresis]].

==Application==
PCLake is designed to study the effects of eutrophication on shallow lakes and ponds.&lt;ref&gt;Janse JH, 2005. Model studies on the eutrophication of shallow lakes and ditches. PhD thesis. Wageningen University&lt;/ref&gt; On one hand, the model is used by scientists to study the general behavior of these ecosystems. For example, PCLake is used to understand the phenomena of alternative stable states and hysteresis, and in that light, the relative importance of lake features such as water depth or [[fetch (geography)|fetch length]].&lt;ref&gt;Janse JH and others, 2008. Critical phosphorus loading of different types of shallow lakes and the consequences for management estimated with the ecosystem model PCLake. Limnologica 38: 2003–2019&lt;/ref&gt; Also the potential effects of climate warming for shallow lakes have been studied.&lt;ref&gt;Mooij WM and others, 2007. Predicting the effect of climate change on temperate shallow lakes with the ecosystem model PCLake. Hydrobiologia 584: 443–454&lt;/ref&gt; On the other hand, PCLake is applied by lake water resource managers that consider the turbid state as undesirable. They can use the model to define the critical loadings for their specific lakes and evaluate the effectiveness of restoration measures. For this purpose also a meta-model has been developed.&lt;ref&gt;http://themasites.pbl.nl/modellen/pclake/&lt;/ref&gt; The meta-model can be used by water managers to derive an estimate of the critical loading values for a certain lake based on only a few important parameters, without the need of running the full dynamical model.&lt;ref&gt;Schep, S. (in Dutch) 2010. Neuraal netwerk PCLake ten behoeve van KRW-verkenner. Witteveen+Bos; rapportnr. UT565-2-1&lt;/ref&gt;

==Model content==
Mathematically, PCLake is composed of a set of coupled [[differential equation]]s. With a large number of [[state variables]] (&gt;100) and [[parameters]] (&gt;300), the model may be characterized as relatively complex. The main biotic variables are [[phytoplankton]] and [[submerged aquatic vegetation]], describing [[primary production]]. A simplified [[food web]] is made up of [[zooplankton]], zoo[[benthos]], young and adult whitefish and piscivorous fish. The main abiotic factors are transparency and the nutrients phosphorus (P), nitrogen (N) and silica (Si). At the base of the model are the water and nutrient budgets (in- and outflow). The model describes a completely mixed water body and comprises both the water column and the upper sediment layer. The overall nutrient cycles for N, P and Si are described as completely closed (except for in- and outflow and [[denitrification]]). Inputs to the model are: lake hydrology, nutrient loading, dimensions and sediment characteristics. The model calculates [[chlorophyll]]-a, transparency, cyanobacteria, vegetation cover and fish biomass, as well as the concentrations and fluxes of nutrients N, P and Si, and oxygen. Optionally, a wetland zone with marsh vegetation and water exchange with the lake can be included.

PCLake is calibrated against nutrient, transparency, chlorophyll and vegetation data on more than 40 European (but mainly Dutch) lakes, and systematic sensitivity and uncertainty analysis have been performed.&lt;ref&gt;Janse JH and others, 2010. Estimating the critical phosphorus loading of shallow lakes with the ecosystem model PCLake: sensitivity, calibration and uncertainty. Ecological Modelling 221: 654–665&lt;/ref&gt;
Although PCLake is primarily used for Dutch lakes, it is likely that the model is also applicable to comparable non-[[stratification (water)|stratifying]] lakes in other regions, if parameters are adjusted or some small changes to the model are made.

==Model development==
The first version of PCLake (by then called PCLoos) was built in the early 1990s at the [[Netherlands National Institute for Public Health and the Environment]] (RIVM), within the framework of a research and restoration project on Lake Loosdrecht. It has been extended and improved since then. Parallel to PCLake, [[PCDitch]] was created, which is an ecosystem model for ditches and other linear water bodies. The models were further developed by dr. Jan H. Janse and colleagues at the [[Netherlands Environmental Assessment Agency]] (PBL), formerly part of the RIVM. Since 2009, the model is jointly owned by PBL and the Netherlands Institute of Ecology, where further development and application of PCLake is taking place, related to aquatic-ecological research.

==See also==
{{Portal|Ecology|Environment|Water}}
* [[Ecosystem model]]
* [[Water quality modelling]]
* [[Ecopath]]
* [[PCDitch]]

==References==
{{Reflist|2}}

[[Category:Mathematical modeling]]
[[Category:Environmental chemistry]]</text>
      <sha1>csxscx4odnkbtr6ei734nh78zl3g48d</sha1>
    </revision>
  </page>
  <page>
    <title>Path (graph theory)</title>
    <ns>0</ns>
    <id>638889</id>
    <revision>
      <id>854763314</id>
      <parentid>830341911</parentid>
      <timestamp>2018-08-13T16:18:13Z</timestamp>
      <contributor>
        <username>Shmurak</username>
        <id>32263378</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6134">{{for|the family of graphs known as paths|Path graph}}

[[File:Snake-in-the-box and Hamiltonian path.svg|thumb|right|A [[hypercube graph]] showing a [[Hamiltonian path]] in red, and a [[Snake-in-the-box|longest induced path]] in bold black.]]

In [[graph theory]], a '''path''' in a [[Graph (discrete mathematics)|graph]] is a finite or infinite [[sequence]] of [[edge (graph theory)|edges]] which connect a sequence of [[vertex (graph theory)|vertices]] which, by most definitions, are all distinct from one another.  In a [[directed graph]], a '''directed path''' (sometimes called '''dipath'''&lt;ref&gt;Graph Structure Theory: Proceedings of the AMS-IMS-SIAM Joint Summer Research Conference on Graph Minors, Held June 22 to July 5, 1991,, [https://books.google.com/books?id=idigH5CTGWAC&amp;pg=PA205 p.205]&lt;/ref&gt;) is again a sequence of edges (or arcs) which connect a sequence of vertices, but with the added restriction that the edges all be directed in the same direction.

Paths are fundamental concepts of graph theory, described in the introductory sections of most graph theory texts. See e.g. Bondy and Murty (1976), Gibbons (1985), or Diestel (2005). Korte et al. (1990) cover more advanced [[algorithm]]ic topics concerning paths in graphs.

== Definitions ==
A path is a [[trail (graph theory)|trail]] in which all vertices (except possibly the first and last) are distinct.
A trail is a [[walk (graph theory)|walk]] in which all edges are distinct.
A walk of length &lt;math&gt;k&lt;/math&gt; in a graph is an alternating sequence of vertices and edges, &lt;math&gt;v_0,e_0,v_1,e_1,v_2,\ldots, v_{k-1},e_{k-1},v_k&lt;/math&gt;, which begins and ends with vertices.  If the graph is undirected, then the endpoints of &lt;math&gt;e_i&lt;/math&gt; are &lt;math&gt;v_i&lt;/math&gt; and &lt;math&gt;v_{i+1}&lt;/math&gt;.  If the graph is directed, then &lt;math&gt;e_i&lt;/math&gt; is an arc from &lt;math&gt;v_i&lt;/math&gt; to &lt;math&gt;v_{i+1}&lt;/math&gt;.  An infinite path is an alternating sequence of the same type described here, but with no first or last vertex, and a semi-infinite path (also [[end (graph theory)|ray]]) has a first vertex, &lt;math&gt;v_0&lt;/math&gt;, but no last vertex.  Most authors require that all of the edges and vertices be distinct from one another. &lt;!-- refer to related [[trail]] and [[walk]] and [[cycle]] --&gt; However, some authors do not make this requirement, and instead use the term '''simple path''' to refer to a path which contains no repeated vertices.

A [[weighted graph]] associates a value (''weight'') with every edge in the graph. The ''weight of a path'' in a weighted graph is the sum of the weights of the traversed edges. Sometimes the words ''cost'' or ''length'' are used instead of weight.

== Examples ==

* A graph is [[Connectivity (graph theory)|connected]] if there are paths containing each pair of vertices.
* A directed graph is [[Strongly-connected digraph|strongly connected]] if there are oppositely oriented directed paths containing each pair of vertices.
* A path such that no graph edges connect two nonconsecutive path vertices is called an [[induced path]].
* A path that includes every vertex of the graph is known as a [[Hamiltonian path]].
* Two paths are ''vertex-independent'' (alternatively, ''internally vertex-disjoint'') if they do not have any internal vertex in common. Similarly, two paths are ''edge-independent'' (or ''edge-disjoint'') if they do not have any internal edge in common.  Two internally vertex-disjoint paths are edge-disjoint, but the converse is not necessarily true.
* The [[Distance (graph theory)|distance]] between two vertices in a graph is the length of a shortest path between them, if one exists, and otherwise the distance is infinity.
* The diameter of a connected graph is the largest distance (defined above) between pairs of vertices of the graph.

== Finding paths ==
Several algorithms exist to find [[Shortest path problem|shortest]] and [[Longest path problem|longest]] paths in graphs, with the important distinction that the former problem is computationally much easier than the latter.

[[Dijkstra's algorithm]] produces a list of shortest paths from a source vertex to every other vertex in directed and undirected graphs with non-negative edge weights (or no edge weights), whilst the [[Bellman–Ford algorithm]] can be applied to directed graphs with negative edge weights.  The [[Floyd–Warshall algorithm]] can be used to find the shortest paths between all pairs of vertices in weighted directed graphs.

== See also ==

* [[Glossary of graph theory]]
* [[Path graph]]
* [[Polygonal chain]]
* [[Shortest path problem]]
* [[Longest path problem]]
* [[Dijkstra's algorithm]]
* [[Bellman–Ford algorithm]]
* [[Floyd–Warshall algorithm]]
* [[Self-avoiding walk]]

== References ==
{{Reflist}}

*{{cite book
 |author      = [[John Adrian Bondy|Bondy, J. A.]]; [[U. S. R. Murty|Murty, U. S. R.]]
 |title       = Graph Theory with Applications
 |year        = 1976
 |publisher   = North Holland
 |isbn        = 0-444-19451-7
 |pages       = 12–21
 |url         = http://www.ecp6.jussieu.fr/pageperso/bondy/books/gtwa/gtwa.html
 |deadurl     = yes
 |archiveurl  = https://web.archive.org/web/20100413104345/http://www.ecp6.jussieu.fr/pageperso/bondy/books/gtwa/gtwa.html
 |archivedate = 2010-04-13
 |df          = 
}}
*{{cite book
 | author = Diestel, Reinhard
 | title = Graph Theory
 | edition = 3rd
 | url = http://www.math.uni-hamburg.de/home/diestel/books/graph.theory/
 | publisher = [[Graduate Texts in Mathematics]], vol. 173, Springer-Verlag
 | year = 2005
 | isbn = 3-540-26182-6
 | pages = 6–9}}
*{{cite book
 | author = Gibbons, A.
 | title = Algorithmic Graph Theory
 | year = 1985
 | publisher = Cambridge University Press
 | pages = 5–6
 | isbn = 0-521-28881-9}}
*{{cite book
 | author = [[Bernhard Korte|Korte, Bernhard]]; [[László Lovász|Lovász, László]]; Prömel, Hans Jürgen; [[Alexander Schrijver|Schrijver, Alexander]] (Eds.)
 | title = Paths, Flows, and VLSI-Layout
 | publisher = Algorithms and Combinatorics 9, Springer-Verlag
 | year = 1990
 | isbn = 0-387-52685-4}}

[[Category:Graph theory objects]]
[[Category:Graph connectivity]]

{{interwiki extra|qid=Q917421}}</text>
      <sha1>asc8b0hg66kqmjpy9fw2bz9lbpv18su</sha1>
    </revision>
  </page>
  <page>
    <title>Perimeter</title>
    <ns>0</ns>
    <id>23636</id>
    <revision>
      <id>864644563</id>
      <parentid>864593305</parentid>
      <timestamp>2018-10-18T15:14:54Z</timestamp>
      <contributor>
        <username>Kwpolska</username>
        <id>11115705</id>
      </contributor>
      <comment>Undid revision 864593305 by [[Special:Contributions/27.99.58.122|27.99.58.122]] ([[User talk:27.99.58.122|talk]]): vandalism.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10583">{{Other uses}}

[[File:Perimiters.svg|thumb|250px|Perimeter is the distance around a two dimensional shape, a measurement of the distance around something; the length of the boundary.]]

A '''perimeter''' is a path that surrounds a [[two-dimensional]] [[shape]]. The term may be used either for the path or its length&amp;mdash;it can be thought of as the length of the outline of a shape. The perimeter of a [[circle]] or [[ellipse]] is called its [[circumference]].

Calculating the perimeter has several practical applications. A calculated perimeter is the length of fence required to surround a yard or garden. The perimeter of a wheel (its circumference) describes how far it will roll in one [[revolution (geometry)|revolution]]. Similarly, the amount of string wound around a spool is related to the spool's perimeter.

== Formulas ==
{| class="wikitable sortable mw-collapsible mw-collapsed"
|+
! shape !! formula || variables
|-
| [[circle]] || &lt;math&gt;2 \pi r = \pi d&lt;/math&gt; || where &lt;math&gt;r&lt;/math&gt; is the radius of the circle and &lt;math&gt;d&lt;/math&gt; is the diameter.
|-
| [[triangle]] || &lt;math&gt;a + b + c\,&lt;/math&gt; || where &lt;math&gt;a&lt;/math&gt;, &lt;math&gt;b&lt;/math&gt; and &lt;math&gt;c&lt;/math&gt; are the lengths of the sides of the triangle.
|- 
| [[square (geometry)|square]]/[[rhombus]] || &lt;math&gt;4a&lt;/math&gt; || where &lt;math&gt;a&lt;/math&gt; is the side length.
|- 
| [[rectangle]] || &lt;math&gt;2(l+w)&lt;/math&gt; || where &lt;math&gt;l&lt;/math&gt; is the length and &lt;math&gt;w&lt;/math&gt; is the width.
|-
| [[equilateral polygon]] || &lt;math&gt;n \times a\,&lt;/math&gt; || where &lt;math&gt;n&lt;/math&gt; is the number of sides and &lt;math&gt;a&lt;/math&gt; is the length of one of the sides.
|-
| [[regular polygon]] || &lt;math&gt;2nb \sin\left(\frac{\pi}{n}\right)&lt;/math&gt; || where &lt;math&gt;n&lt;/math&gt; is the number of sides and &lt;math&gt;b&lt;/math&gt; is the distance between center of the polygon and one of the [[Vertex (geometry)|vertices]] of the polygon.
|-
| general [[polygon]] || &lt;math&gt;a_1 + a_2 + a_3 + \cdots + a_n = \sum_{i=1}^n a_i&lt;/math&gt; || where &lt;math&gt;a_{i}&lt;/math&gt; is the length of the &lt;math&gt;i&lt;/math&gt;-th (1st, 2nd, 3rd ... ''n''th) side of an ''n''-sided polygon.
|}
[[File:Herzkurve2.svg|thumb|upright=1.0|[[cardoid]] &lt;math&gt;\gamma:[0,2\pi]\rightarrow \mathbb{R}^2 &lt;/math&gt;&lt;br/&gt;(drawing with &lt;math&gt;a=1&lt;/math&gt;)&lt;br/&gt;&lt;math&gt;x(t) = 2 a \cos(t) (1 + \cos(t))&lt;/math&gt;&lt;br/&gt;&lt;math&gt;y(t) =  2 a \sin(t) (1 + \cos (t))&lt;/math&gt;&lt;br/&gt;&lt;math&gt;L = \int\limits_0^{2\pi}\sqrt{x'(t)^2+y'(t)^2}\,\mathrm dt=16a&lt;/math&gt;]]
The perimeter is the distance around a shape. Perimeters for more general shapes can be calculated, as any path, with &lt;math&gt;\int_0^L \mathrm{d}s&lt;/math&gt;, where &lt;math&gt;L&lt;/math&gt; is the length of the path and &lt;math&gt;ds&lt;/math&gt; is an infinitesimal line element. Both of these must be replaced with by algebraic forms in order to be practically calculated. If the perimeter is given as a closed [[plane curve|piecewise smooth plane curve]] &lt;math&gt; \gamma:[a,b]\rightarrow \mathbb{R}^2&lt;/math&gt; with
:&lt;math&gt; \gamma(t)=\begin{pmatrix}x(t)\\y(t)\end{pmatrix}&lt;/math&gt; 
then its length &lt;math&gt;L&lt;/math&gt; can be computed as follows:
: &lt;math&gt;L = \int\limits_a^b\sqrt{x'(t)^2+y'(t)^2}\,\mathrm dt&lt;/math&gt;

A generalized notion of perimeter, which includes [[hypersurface]]s bounding volumes in &lt;math&gt;n&lt;/math&gt;-[[Dimension (mathematics)|dimensional]] [[Euclidean space]]s, is described by the theory of [[Caccioppoli set]]s.

==Polygons==
[[File:PerimeterRectangle.svg|thumb|Perimeter of a rectangle.]]
[[Polygon]]s are fundamental to determining perimeters, not only because they are the simplest shapes but also because the perimeters of many shapes are calculated by [[Approximation#Mathematics|approximating]] them with [[limit of a sequence|sequences]] of polygons tending to these shapes. The first mathematician known to have used this kind of reasoning is [[Archimedes]], who approximated the perimeter of a circle by surrounding it with [[regular polygon]]s.

The perimeter of a polygon equals the [[summation|sum]] of the lengths of its [[Edge (geometry)|sides (edges)]]. In particular, the perimeter of a [[rectangle]] of width &lt;math&gt;w&lt;/math&gt; and length &lt;math&gt;\ell&lt;/math&gt; equals &lt;math&gt;2w + 2\ell.&lt;/math&gt;

An [[equilateral polygon]] is a polygon which has all sides of the same length (for example, a [[rhombus]] is a 4-sided equilateral polygon). To calculate the perimeter of an equilateral polygon, one must multiply the common length of the sides by the number of sides.

A [[regular polygon]] may be characterized by the number of its sides and by its [[circumradius]], that is to say, the constant distance between its [[Centre (geometry)|centre]] and each of its [[Vertex (geometry)|vertices]]. The length of its sides can be calculated using [[trigonometry]]. If {{math|''R''}} is a regular polygon's radius and {{math|''n''}} is the number of its sides, then its perimeter is 
:&lt;math&gt;2nR \sin\left(\frac{180^{\circ}}{n}\right).&lt;/math&gt;

A [[splitter (geometry)|splitter]] of a [[triangle]] is a [[cevian]] (a segment from a vertex to the opposite side) that divides the perimeter into two equal lengths, this common length being called the [[semiperimeter]] of the triangle. The three splitters of a triangle [[concurrent lines|all intersect each other]] at the [[Nagel point]] of the triangle.

A [[cleaver (geometry)|cleaver]] of a triangle is a segment from the midpoint of a side of a triangle to the opposite side such that the perimeter is divided into two equal lengths. The three cleavers of a triangle all intersect each other at the triangle's [[Spieker center]].

==Circumference of a circle==
[[File:Pi-unrolled-720.gif|right|300px|thumb|If the diameter of a circle is 1, its circumference equals {{pi}}.]]
{{main|Circumference}}
The perimeter of a [[circle]], often called the circumference, is proportional to its [[diameter]] and its [[radius]]. That is to say, there exists a constant number [[pi]], {{pi}} (the [[ancient greek|Greek]] ''p'' for perimeter), such that if {{math|''P''}} is the circle's perimeter and {{math|''D''}} its diameter then,
:&lt;math&gt;P = \pi\cdot{D}.\!&lt;/math&gt;

In terms of the radius {{math|''r''}} of the circle, this formula becomes,

:&lt;math&gt;P=2\pi\cdot r.&lt;/math&gt;

To calculate a circle's perimeter, knowledge of its radius or diameter and the number {{pi}} suffices. The problem is that {{pi}} is not [[rational number|rational]] (it cannot be expressed as the [[quotient]] of two [[integer]]s), nor is it [[algebraic number|algebraic]] (it is not a root of a polynomial equation with rational coefficients). So, obtaining an accurate approximation of {{pi}} is important in the calculation. The computation of the digits of {{pi}} is relevant to many fields, such as [[mathematical analysis]], [[algorithmics]] and [[computer science]].

== Perception of perimeter==
[[File:Hexaflake.gif|thumb|left|upright=0.6|The more one cuts this shape, the lesser the area and the greater the perimeter. The [[convex hull]] remains the same.]]
[[File:Neuf Brisach.jpg|thumb|The [[Neuf-Brisach]] fortification perimeter is complicated. The shortest path around it is along its [[convex hull]].]]
{{Main|Area (geometry)|convex hull}}
The perimeter and the [[area (geometry)|area]] are two main measures of geometric figures. Confusing them is a common error, as well as believing that the greater one of them is, the greater the other must be. Indeed, a commonplace observation is that an enlargement (or a reduction) of a shape make its area grow (or decrease) as well as its perimeter. For example, if a field is drawn on a 1/{{formatnum:10000}} scale map, the actual field perimeter can be calculated multiplying the drawing perimeter by {{formatnum:10000}}. The real area is {{formatnum:10000}}{{sup|2}} times the area of the shape on the map. Nevertheless, there is no relation between the area and the perimeter of an ordinary shape. For example, the perimeter of a rectangle of width 0.001 and length 1000 is slightly above 2000, while the perimeter of a rectangle of width 0.5 and length 2 is 5. Both areas equal to 1.

[[Proclus]] (5th century) reported that Greek peasants "fairly" parted fields relying on their perimeters.&lt;ref&gt;{{cite book|first1=T.|last1=Heath|title=A History of Greek Mathematics|volume=2|publisher=[[Dover Publications]]|year= 1981|page= 206|isbn=0-486-24074-6}}&lt;/ref&gt;  However, a field's production is proportional to its area, not to its perimeter, so many naive peasants may have gotten fields with long perimeters but small areas (thus, few crops).

If one removes a piece from a figure, its area decreases but its perimeter may not. In the case of very irregular shapes, confusion between the perimeter and the [[convex hull]] may arise. The convex hull of a figure may be visualized as the shape formed by a rubber band stretched around it. In the animated picture on the left, all the figures have the same convex hull; the big, first [[hexagon]].

== Isoperimetry ==
{{Further|Isoperimetric inequality}}

The isoperimetric problem is to determine a figure with the largest area, amongst those having a given perimeter. The solution is intuitive; it is the [[circle]]. In particular, this can be used to explain why drops of fat on a [[broth]] surface are circular.

This problem may seem simple, but its mathematical proof requires some sophisticated theorems. The isoperimetric problem is sometimes simplified by restricting the type of figures to be used. In particular, to find the [[quadrilateral]], or the triangle, or another particular figure, with the largest area amongst those with the same shape having a given perimeter. The solution to the quadrilateral isoperimetric problem is the [[square]], and the solution to the triangle problem is the [[equilateral triangle]]. In general, the polygon with {{math|''n''}} sides having the largest area and a given perimeter is the [[regular polygon]], which is closer to being a circle than is any irregular polygon with the same number of sides.

==Etymology==
The word comes from the [[Ancient Greek|Greek]] περίμετρος ''perimetros'' from περί ''peri'' "around" and μέτρον ''metron'' "measure".

== See also ==
* [[Arclength]]
* [[Area]]
* [[Coastline paradox]]
* [[Girth (geometry)]]
* [[Pythagorean theorem]]
* [[Surface area]]
* [[Volume]]
* [[Wetted perimeter]]

==References==
&lt;references/&gt;

== External links ==
{{wiktionary}}
{{wikibooks|Geometry|Chapter 8|Perimeters, areas and volumes}}
{{wikibooks|Geometry|Perimeter and Arclength}}
{{wikibooks|Geometry|Circles/Arcs|Arcs}}
* {{MathWorld |urlname=Perimeter |title=Perimeter}}
* {{MathWorld |urlname=Semiperimeter |title=Semiperimeter}}

[[Category:Elementary geometry]]
[[Category:Length]]</text>
      <sha1>t683f1vno67vfj35q1icauknw0sch57</sha1>
    </revision>
  </page>
  <page>
    <title>Philco computers</title>
    <ns>0</ns>
    <id>54747507</id>
    <revision>
      <id>870659573</id>
      <parentid>870659139</parentid>
      <timestamp>2018-11-26T06:50:16Z</timestamp>
      <contributor>
        <username>Snori</username>
        <id>545864</id>
      </contributor>
      <comment>move into body</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14187">[[Philco]] was one of the pioneers of transistorized computers.  After the company developed the surface barrier [[transistor]], which was much faster than previous point-contact types, it was awarded contracts for military and government computers.  Commercialized derivatives of some of these designs became successful business and scientific computers. The TRANSAC (Transistor Automatic Computer) Model S-1000 was released as a scientific computer. The TRANSAC S-2000 mainframe computer system  was first produced in 1958, and a family of compatible machines, with increasing performance, was released over the next several years.

However, the mainframe computer market was dominated by [[IBM]]. Other companies could not deploy resources for development, customer support and marketing on the scale that IBM could afford, making competition in this segment difficult after the introduction of the [[IBM 360]] family.  Philco went bankrupt and was purchased in 1961 by [[Ford Motor Company]], but the computer division carried on until the Philco division of Ford exited the computer business in 1963. The Ford company maintained one Philco mainframe in use until 1981.

==The surface-barrier transistor==
The [[surface-barrier transistor]] developed by Philco in 1953 had a much higher frequency response than the original point-contact transistors.  The transistor was made of a thin crystal of germanium, which was electrolytically etched with pits on either side forming a very thin base region, on the order of  5 micrometers. Philco's process for etching was United States patent number  2,885,571.  Philco surface-barrier transistors were used in [[TX-0]], and in early models of what would become the DEC [[Programmed Data Processor|PDP]] product line. Although relatively fast, the small size of the devices limited their power to circuits operating at a few tens of milliwatts. 
[[File:Philco SB100 surface barrier transistor ad=1955.jpg|thumb|right|The surface-barrier transistor was the first type that could compete with vacuum tubes in speed.]]

==Military and government ==

Between 1955 and 1957, Philco built transistor computers for use in aircraft, models C-1000, C-1100, and C-1102, intended for airborne real-time applications. By 1957, the C-1102 had been used by a civilian sector customer.&lt;ref name=EXHIBIT&gt;{{cite web|url=https://archive.org/details/bitsavers_charlesRivit14971part1Jul80_14957277|title=Historical Narrative Statement of Richard B. Mancke, Franklin M. Fisher and James W. McKie, Exhibit 14971, US vs. IBM, Part 1|pages=238-240|date=July 1980}}&lt;/ref&gt;
The BASICPAC&lt;ref&gt;{{cite web|title=1960s computer ads created by the real Mad Men|url=https://royal.pingdom.com/2013/05/30/computer-ads-1960s-mad-men/|website=Pingdom Royal|location=A computer on a truck|date=30 May 2013}} BASICPAC ad from 1960, possibly from [http://bitsavers.trailing-edge.com/pdf/computersAndAutomation/ ''Computers and Automation'']&lt;/ref&gt; AN/TYK 6V (first delivery in 1961),&lt;ref&gt;{{cite journal|title=MILITARY NOTES - 'Basicpac' Computer Systems|journal=Military review.  v.41:2 (1961)|date=1961|volume=XLI|issue=2|page=98|url=https://hdl.handle.net/2027/uiug.30112106755934?urlappend=%3Bseq=100|language=en}}&lt;/ref&gt; COMPAC AN/TYK 4V (not completed),&lt;ref&gt;{{cite journal|title=Data processing equipment|journal=Signals / Armed Forces Communications Association.  v.14 no.1-12 1959-1960|date=1960|volume=XIV|issue=7|page=60|url=https://hdl.handle.net/2027/mdp.39015013429793?urlappend=%3Bseq=512|language=en}}&lt;/ref&gt;&lt;ref&gt;{{cite book|url=https://hdl.handle.net/2027/mdp.39015023453221?urlappend=%3Bseq=60|title=A third survey of domestic electronic digital computing systems.|last1=Weik|first1=Martin H.|date=1961|publisher=Aberdeen Proving Ground, Md.,|others=BASICPAC pp. 52-53|year=|isbn=|location=|pages=50–53|ref=harv}}&lt;/ref&gt;&lt;ref&gt;{{Cite book|url=https://books.google.com/books?hl=en&amp;id=mQb0AAAAMAAJ&amp;dq=philco+%22compac%22&amp;focus=searchwithinvolume&amp;q=%22compac%22|title=Missiles and Rockets|last=|first=|date=1967|publisher=American Aviation Publications|year=|isbn=|location=|pages=37|language=en}}&lt;/ref&gt; and LOGICPAC&lt;ref&gt;{{cite book|title=Quadrennial report of the Chief Signal Officer, U. S. Army.|date=1959|publisher=[Washington?],|pages=20–21|url=https://hdl.handle.net/2027/mdp.39015022453172?urlappend=%3Bseq=34}}&lt;/ref&gt; systems were built for the US Army as transportable computer systems for use with their [[Fieldata]] concept of integrated information management.

BASICPAC was a transistorized computer with up to 28,672 words of 38-bit [[core memory]] (including sign and parity), available in several configurations from a minimum system, to a truck-borne mobile version, to a fully expanded system.  Basic clock periods was 1 microsecond (which gives a [[clock rate]] of 1 [[Hertz|MHz]]),&lt;ref&gt;{{Cite book|url=https://books.google.com/books?id=LXUGSruiYq4C&amp;lpg=PA67&amp;dq=%22clock%20frequency%22%20of%201%20microsecond&amp;pg=PA67#v=onepage&amp;q=%22clock%20frequency%22%20of%201%20microsecond&amp;f=false|title=Hardware and Computer Organization: The Software Perspective|last=Berger|first=Arnold S.|date=2005|publisher=Newnes|year=|isbn=9780750678865|location=|pages=66–67|language=en}}&lt;/ref&gt;&lt;ref&gt;{{Cite web|url=http://www.pcguide.com/intro/fun/clockClocks-c.html|title=Clock Signals, Cycle Time and Frequency|website=www.pcguide.com|access-date=2018-03-14}}&lt;/ref&gt; with 12 microsecond memory access and a fixed-point multiplication taking 242 microseconds. Input/output was by paper tape reader and punch, or through a teletypewriter.  With additional hardware, magnetic tape storage was also available, with up to seven I/O devices. The instruction set had 31 basic operation codes and nine opcodes for I/O &lt;ref&gt;{{cite web|url=http://bitsavers.org/pdf/philco/basicpac/AN_TYK-6v_BASICPAC_Pgmg_Jan61.pdf|title=BASICPAC Programming Manual|at=section 1|date=January 1961|publisher=Philco}}&lt;/ref&gt;

===CXPQ===
Philco was contracted by the US Navy to build the CXPQ computer. One model was completed and installed at the [[David Taylor Model Basin]]. This design was later adapted to become the commercial TRANSAC S-2000.&lt;ref name=KF10&gt;Kenneth Flamm, ''Creating the Computer: Government, Industry and High Technology'', Brookings Institution Press, 2010, {{ISBN|0815707215}}, page 122&lt;/ref&gt;  Only one CXPQ was built.&lt;ref name=THELEN/&gt;

===SOLO===
In 1955, the [[National Security Agency]] through the US Navy contracted with Philco to produce a computer suitable for use as a workstation, with an architecture based on the vacuum-tube computer system called Atlas II already in use at the NSA, and similar to the commercial [[UNIVAC 1103]]. At the time, Philco was the largest producer of surface barrier transistors, which were the only type available with the speed and quantities required for a computer. The SOLO prototype was delivered in 1958, but required extensive debugging at NSA and no further instances were ordered. Difficulties were encountered with core memory and power supplies.  SOLO used [[paper tape]] and teleprinter machines for input and output.&lt;ref name=DLB03&gt;David L. Boslaugh, ''When Computers Went to Sea: The Digitization of the United States Navy'', John Wiley &amp; Sons, 2003, {{ISBN|0471472204}}, pp. 112-113&lt;/ref&gt;  SOLO cost about $1 million US, and contained 8,000 transistors.  While the system was extensively used for training, testing, research and development, no additional units were ordered. SOLO was removed from active service in 1963.&lt;ref name=NSA64&gt;{{cite web|url=https://www.governmentattic.org/3docs/NSA-HGPEDC_1964.pdf|title=History of NSA General-Purpose Electronic Digital Computers|pages=29-31|year=1964}}&lt;/ref&gt; The design of the SOLO became commercialized as Philco's TRANSAC Model S-1000.

==Commercial==
===S-1000===
The TRANSAC S-1000 was a scientific computer with a 36-bit word length and 4096 words of core memory.  It was packaged in a container about the size of a large office desk, and used only 1.2 kilowatts, much less than vacuum-tube-based computers of similar capacity.&lt;ref&gt;{{cite proceedings|url=http://dl.acm.org/citation.cfm?id=1455538|title=The TRANSAC S-1000 Computer|author1=J. L. Maddox|author2=J. B. O'Toole|author3=S. Y. Wong|year=1956|conference=Eastern Joint Computer Conference|access-date=August 27, 2018|doi=10.1145/1455533.1455538}}&lt;/ref&gt;
In a 1961 survey, about 15 S-1000 computer installations had been identified.&lt;ref name=THELEN&gt;{{cite book|url=http://ed-thelen.org/comp-hist/BRL64.html#TOC|title=A Fourth Survey Of Domestic Electronic Digital Computing Systems|author=Martin H. Weik|chapter-url=http://ed-thelen.org/comp-hist/BRL64-ch3.html|chapter=Chapter III Tables of Computer Characteristics|date=January 1964|access-date=August 27, 2018}}&lt;/ref&gt;

It weighed about {{convert|1650|lb|kg}}.{{sfn|Weik|1964|loc=PHILCO 1000}}

===S-2000===
[[File:Philco 2000 computer.jpg|right|thumb|Philco 2000]]
The TRANSAC S-2000 was a large mainframe system intended for both business and scientific work. It  had a 48-bit word length and supported calculations in fixed point, floating point and binary-coded decimal formats. The original S-2000 "TRANSAC" (Transistor Automatic Computer) released in 1958&lt;ref&gt;{{Cite journal|last=United States.|first=|date=1959|title=COMPUTERS AND DATA PROCESSORS, NORTH AMERICA - Philco Corp., Transac S-2000, Philadelphia, Pennsylvania 4|url=https://hdl.handle.net/2027/nyp.33433108192331?urlappend=%3Bseq=108|journal=Digital computer newsletter|volume=11|issue=2|pages=4|via=}}&lt;/ref&gt; was later designated Model 210; it was used internally at Philco.  Similar to the [[Control Data Corporation]] Model [[CDC 1604|1604]], it was a 48-bit fully transistorized computer. Three succeeding models were released in the series, all compatible with the software of the original model. The Model 211 was introduced in 1960, using micro-alloy diffused [[field-effect transistor]]s, requiring significant redesign of circuits compared to the original. 

The TRANSAC S-2000/Philco 210/211 weighed about {{convert|2000|lb|kg}}.&lt;ref&gt;{{Cite web|url=http://www.ed-thelen.org/comp-hist/BRL64.html|title=PHILCO 210 &amp; 211|last=Weik|first=Martin H.|date=Jan 1964|website=ed-thelen.org|series=A Fourth Survey of Domestic Electronic Digital Computing Systems|archive-url=|archive-date=|dead-url=|access-date=|ref={{harvid|Weik|1964}}}}&lt;/ref&gt;

By 1964, 18 Model 210, and 18 Model 211 and 7 Model 212 systems had been sold.&lt;ref name=THELEN/&gt;

After Philco was purchased by [[Ford Motor Company]], the Model 212 was introduced in 1962&lt;ref&gt;{{Cite web|url=http://www.computerhistory.org/brochures/m-p/philco-corporation/|title=Philco Corporation {{!}}  Selling the Computer Revolution {{!}} Computer History Museum|website=www.computerhistory.org|language=en|access-date=2018-03-15}}&lt;/ref&gt; and released in 1963. It had 65,535 words of 48-bit memory. Initially made with 6-microsecond core memory, it had better performance than the [[IBM 7094]] transistor computer. It was later upgraded in 1964 to 2-microsecond core memory, which gave the machine floating-point performance greater than the [[IBM 7030 Stretch]] computer. A Model 213 was announced in 1964 but never built. By that time competition from IBM had made the Philco computer operations no longer profitable for Ford, and the division was closed down.&lt;ref&gt;{{Cite book|url=https://books.google.com/books?id=JTYPKxug49IC&amp;lpg=PA249&amp;dq=1573565210&amp;hl=en&amp;pg=PA201#v=onepage&amp;q=philco%20%22model%20212%22&amp;f=false|title=Milestones in Computer Science and Information Technology|last=Reilly|first=Edwin D.|date=2003|publisher=Greenwood Publishing Group|year=|isbn=9781573565219|location=|pages=201|language=en}}&lt;/ref&gt;&lt;ref&gt;{{Cite book|url=https://books.google.com/books?id=P10pDwAAQBAJ&amp;lpg=PA329&amp;dq=144389625X&amp;hl=en&amp;pg=PA229#v=onepage&amp;q=philco%20%22model%20212%22&amp;f=false|title=Birthing the Computer: From Drums to Cores|last=Kaisler|first=Stephen H.|date=2017-06-20|publisher=Cambridge Scholars Publishing|year=|isbn=9781443896252|location=|pages=232–237|language=en|chapter=Chapter Eleven - Philco Ford computers}}&lt;/ref&gt;

The Model 212 could carry out a floating-point multiplication in 22 microseconds. Each word contained two 24-bit instructions with 16 bits of address information and eight bits for the [[opcode]].  There were 225 different valid opcodes in the Model 212; invalid opcodes were detected and halted the machine.  The CPU had an accumulator register of 48 bits, three general-purpose registers of 24 bits, and 32 index registers of 15 bits. Main memory size ranged from 4K words to 64K words. Only the first model had a magnetic drum memory; later editions used tape drives.

The Model 212 weighed about {{convert|6500|lb|ST MT}}.{{sfn|Weik|1964|loc=PHILCO 212}}

Software for the S-2000 initially consisted of TAC (Translator-Assember-Compiler), and ALTAC, a [[FORTRAN]] II-like language with some differences from the [[IBM 704]] FORTRAN implementation.  A [[COBOL]] compiler was also available, targeted at business applications.

The Philco 2400 was the input/output system for the S-2000. Operations such as reading cards or printing were carried out through magnetic tapes, thereby offloading the S-2000 from relatively slow input/output processing.&lt;ref name="SHK17"&gt;Stephen H. Kaisler, ''Birthing the Computer: From Drums to Cores'', Cambridge Scholars Publishing, 2017, {{ISBN|144389625X}}, pages 232-237&lt;/ref&gt; The 2400 had a 24-bit word length and could be supplied with 4K to 32K characters (1K to 8K words) of core memory, rated at 3-microsecond cycle time. The instruction set was aimed at character I/O use.

The last Philco TRANSAC S-2000 Model 212 was taken out of service in December 1981, after 19 years service at Ford.&lt;ref&gt;{{cite web |title=Retirement video for the Philco 212 Mainframe Computer |url=https://www.youtube.com/watch?v=hwOkVgGw1z8 |publisher=Computer History Museum |accessdate=26 November 2018}}&lt;/ref&gt;

==References==
&lt;references/&gt;

==External links==
*[https://www.youtube.com/watch?v=hwOkVgGw1z8 The last S-2000 retirement at Ford, 1981]

[[Category:36-bit computers]]
[[Category:Defunct computer companies of the United States]]
[[Category:Military computers]]
[[Category:Cryptography]]</text>
      <sha1>cgad9geqnbu842djtiukj098n3ds2iz</sha1>
    </revision>
  </page>
  <page>
    <title>Philosophy of logic</title>
    <ns>0</ns>
    <id>21368888</id>
    <revision>
      <id>868499469</id>
      <parentid>857312619</parentid>
      <timestamp>2018-11-12T16:08:23Z</timestamp>
      <contributor>
        <username>Josebarbosa</username>
        <id>18831016</id>
      </contributor>
      <comment>/* Important figures */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12572">Following the developments in [[formal logic]] with [[symbolic logic]] in the late nineteenth century and [[mathematical logic]] in the twentieth, topics traditionally treated by [[logic]] not being part of formal logic have tended to be termed either ''philosophy of logic'' or ''philosophical logic'' if no longer simply ''logic''.

Compared to the history of logic the demarcation between philosophy of logic and [[philosophical logic]] is of recent coinage and not always entirely clear. Characterisations include
* '''Philosophy of logic''' is the area of philosophy devoted to examining the scope and nature of logic.&lt;ref name="The Cambridge Dictionary of Philosophy"&gt;{{cite book|title=The Cambridge Dictionary of Philosophy|editor=Audi, Robert|publisher=CUP|year=1999|edition=2nd}}&lt;/ref&gt;
* '''Philosophy of logic''' is the investigation, critical analysis and intellectual reflection on issues arising in [[logic]]. The field is considered to be distinct from [[philosophical logic]].
* '''Philosophical logic''' is the branch of study that concerns questions about [[reference]], [[predicate (mathematical logic)|predication]], [[identity (philosophy)|identity]], [[theories of truth|truth]], [[Quantification (logic)|quantification]], [[existence]], [[logical entailment|entailment]], [[modal logic|modality]], and [[logical necessity|necessity]].&lt;ref&gt;Lowe, E. J.. ''Forms of Thought: A Study in Philosophical Logic''. New York: Cambridge University Press, 2013.&lt;/ref&gt;
* '''Philosophical logic''' is the application of formal logical techniques to philosophical problems.&lt;ref&gt;Russell, Gillian [http://tar.weatherson.org/2007/02/27/jcs-oped/ Thoughts, Arguments, and Rants], ''Jc's Column''.&lt;/ref&gt;

This article outlines issues in philosophy of logic or provides links to relevant articles or both.

==Introduction==
This article makes use of the following terms and concepts:
* [[Type–token distinction]]
* [[Use–mention distinction]]

==Truth==
Aristotle said ''To say that that which is, is not or that which is not is, is a falsehood; and to say that which is, is and that which is not is not, is true''&lt;ref&gt;Aristotle, Metaphysics,Books Γ, Δ, Ε 2nd edition 1011b25 (1993) trans Kirwan,: OUP&lt;/ref&gt;

This apparent truism has not proved unproblematic.

===Truthbearers===
Logic uses such terms as true, false, inconsistent, valid, and self-contradictory. Questions arise as Strawson (1952) writes&lt;ref&gt;{{cite book|last=Strawson |first=P.F.|title=Introduction to Logical Theory |publisher=Methuen: London|year=1952|pages=3}}&lt;/ref&gt;
:&lt;blockquote&gt;(a) when we use these words of logical appraisal, what is it exactly that we are appraising? and (b) how does logical appraisal become possible?&lt;/blockquote&gt;
{{main|Truthbearer}}
:''See also:'' [[Sentence (linguistics)|Sentence]], [[Statement (logic)|Statement]], [[Proposition]].

===Tarski's definition of truth===
See:
* [[Semantic theory of truth#Tarski's Theory|Semantic theory of truth § Tarski's Theory]]
* [[T-schema]]
* [http://plato.stanford.edu/entries/tarski-truth/ Stanford Encyclopedia of Philosophy entry on Tarski's Truth Definitions]
* [http://plato.stanford.edu/entries/self-reference/#Matt-sema Self-reference:2.1 Consequences of the Semantic Paradoxes in Stanford Encyclopedia of Philosophy]

===Analytic truths, logical truth, validity, logical consequence and entailment===
Since the use, meaning, if not the meaningfulness, of the terms is part of the debate, it is possible only to give the following working definitions for the purposes of the discussion:
* A [[necessary truth]] is one that is true no matter what the state of the world or, as it is sometimes put, in all possible worlds.&lt;ref&gt;Wolfram (1989) p. 80&lt;/ref&gt;
* [[Logical truth]]s are those necessary truths that are necessarily true owing to the meaning of their logical constants only.&lt;ref&gt;Wolfram (1989), p. 273&lt;/ref&gt;
* In formal logic a ''logical truth'' is just a "statement" (string of symbols in which no variable occurs free) which is true under all possible [[interpretation (logic)|interpretations]].
* An analytic truth is one whose predicate concept is contained in its subject concept.

The concept of logical truth is intimately linked with those of [[validity (logic)|validity]], [[logical consequence]] and [[entailment]] (as well as self-contradiction, necessarily false etc.).
* If ''q'' is a logical truth, then ''p therefore q'' will be a valid argument.
* If ''p1, p2, p3 ... pn therefore q'' is a valid argument then its [[corresponding conditional (logic)|corresponding conditional]] will be a logical truth.
* If ''p1 &amp; p2 &amp; p3 ... pn entails q'' then ''If (p1 &amp; p2 &amp; p3 ... pn) then q'' is a logical truth.
* If ''q'' is a logical consequence of ''p1 &amp; p2 &amp; p3 ... pn'' if and only if ''p1 &amp; p2 &amp; p3 ... pn entails q'' and if and only if ''If (p1 &amp; p2 &amp; p3..pn) then q'' is a logical truth

Issues that arise include:
* If there are truths that must be true, what makes them so?
* Are there analytic truths that are not logical truths?
* Are there necessary truths that are not analytic truths?
* Are there necessary truths that are not logical truths?
* Is the distinction between analytic truth and synthetic truth spurious?

{{main|necessary truth|logical truth|validity (logic)|Corresponding conditional (logic)|logical consequence|entailment|Analytic-synthetic distinction}}
See also [http://plato.stanford.edu/entries/logical-truth/]

===Paradox===
{{main|Paradox}}

==Meaning and reference==
See
* [[Sense and reference]]
* [[Theory of reference]]
* [[Mediated reference theory]]
* [[Direct reference theory]]
* [[Causal theory of reference]] (section References)
* [[Descriptivist theory of names]] (section References)
* [[Saul Kripke]] (section References)
* [[Frege's Puzzle]] (section New Theories of Reference and the Return of Frege's Puzzle)
* [[Gottlob Frege]] (section References)
* [[Failure of reference]] (section References)
* [[Rigid designator]] (section Causal-Historical Theory of Reference)
* [[Philosophy of language]] (section References)
* [[Index of philosophy of language articles]]
* [[Supposition theory]] (section References)
* [[Referring expression]]
* [[Meaning (philosophy of language)]]
* [[Denotation]] and [[Connotation]]
* [[Extension (semantics)|Extension]] and [[Intension]]
* [[Extensional definition]]
* [[Intensional definition]]
* [[Metacommunicative competence]]

==Names and descriptions==
* [[Failure to refer]]
* [[Proper name (philosophy)]]
* [[Definite description]]
* [[Descriptivist theory of names]]
* [[Theory of descriptions]]
* [[Singular term]]
* [[Term logic#Singular terms|Term logic § Singular terms]]
* [[Empty name]]
* [[Bas van Fraassen#Singular Terms, Truth-value Gaps, and Free Logic|Bas van Fraassen § Singular Terms, Truth-value Gaps, and Free Logic]]
* [[The Foundations of Arithmetic#Development of Frege's own view of a number|The Foundations of Arithmetic § Development of Frege's own view of a number]]
* [[Philosophy of language#references|Philosophy of language § references]]
* [[Direct reference]]
* [[Mediated reference theory]]

===Formal and material consequence===
* The problem of the material conditional: see [[Material conditional]]

==Logical constants and connectives==
{{main|Logical constant|Logical connective}}

==Quantifiers and quantificational theory==
{{main|Quantifier (logic)}}

==Modal logic==
{{main|Modal logic}}

==Deviant logics==
{{main|Deviant logic}}

===Classical v. non-classical logics===
{{main|Classical logic}}

==Philosophical theories of logic==
*[[Conceptualism]]
*[[Constructivism (mathematics)|Constructivism]]
*[[Dialetheism]]
*[[Fictionalism]]
*[[Finitism]]
*[[Formalism (mathematics)|Formalism]]
*[[Intuitionism]]
*[[Logical atomism]]
*[[Logicism]]
*[[Nominalism]]
*[[Philosophical realism|Realism]]
*[[Platonic realism]]
*[[Structuralism]]

==Other topics==
* Leibniz's Law: see [[Identity of indiscernibles]]
* Vacuous names
* Do predicates have properties?: See [[Second-order logic]]
* Sense, Reference, Connotation, Denotation, Extension, Intension
* The status of the Laws of Logic
* Classical Logic
* Intuitionism
* Realism: see [[Platonic realism]], [[Philosophical realism]]
* The Law of Excluded Middle: see [[Law of excluded middle]]
* Modality, Intensionality and Propositional Attitude
* Counter-factuals
*Psychologism

==See also==
* "[[Is Logic Empirical?]]"
* [[Type-token distinction]]
* [[Use–mention distinction]]
* [[Pierce's type-token distinction]]
* [[Concatenation theory]]

===Important figures===

Important figures in the philosophy of logic include (but are not limited to):
{{col-begin}}
{{col-break}}
* [[Aristotle]]
* [[William of Ockham]]
* [[John Buridan]]
* [[Peter of Spain]]
* [[George Boole]]
* [[George Boolos]]
* [[Rudolf Carnap]]
* [[Gordon Clark]]
* [[Alonzo Church]]
* [[Augustus De Morgan]]
* [[Michael Dummett]]
* [[Gottlob Frege]]
* [[Kurt Gödel]]
* [[Georg Hegel]]
* [[Immanuel Kant]]
* [[Gottfried Leibniz]]
* [[David Kellogg Lewis|David Lewis]]
* [[John Stuart Mill]]
{{col-break}}
* [[Charles Sanders Peirce]]
* [[Alvin Plantinga]]
* [[Arthur Prior]]
* [[Willard Van Orman Quine]]
* [[Bertrand Russell]]
* [[Alfred Tarski]]
* [[Ludwig Wittgenstein]]
{{col-end}}

=== Philosophers of logic ===
{{col-begin}}
{{col-break}}
*[[W.V.O. Quine]]
*[[Bertrand Russell]]
*[[Ludwig Wittgenstein]]
{{col-break}}
*[[Michael Dummett]]
*[[Hilary Putnam]]
*[[Saul Kripke]]
{{col-break}}
*[[Charles Sanders Peirce]]
*[[Alfred Tarski]]
*[[Donald Davidson (philosopher)|Donald Davidson]]
{{col-break}}
*[[Augustus De Morgan]]
*[[Gordon Clark]]
*[[Aristotle]]
{{col-end}}

==References==
{{reflist}}

==Sources==
* [[Susan Haack|Haack, Susan]].  1978.  ''Philosophy of Logics''.  [[Cambridge University Press]].  ({{ISBN|0-521-29329-4}})
* [[W. V. O. Quine|Quine, W. V. O.]]  2004.  ''Philosophy of Logic''.  2nd ed.  [[Harvard University Press]].  ({{ISBN|0-674-66563-5}})
* [[Alfred Tarski]]. 1983. The concept of truth in formalized languages, pp. 152–278, Logic,semantics, metamathematics, papers from 1923 to 1938, ed. [[John Corcoran (logician)]], Hackett,Indianapolis 1983.

==Further reading==
* Fisher Jennifer, On the Philosophy of Logic, Thomson Wadworth, 2008, {{ISBN|978-0-495-00888-0}}
* Goble, Lou, ed., 2001. [https://books.google.com/books?id=aaO2f60YAwIC&amp;dq=The+Blackwell+Guide+to)+Philosophical+Logic&amp;pg=PP1&amp;ots=y5wP1HQkbh&amp;source=bn&amp;sig=gwaoap-A-Xxzx8daLihFbp2kdCk&amp;hl=en&amp;sa=X&amp;oi=book_result&amp;resnum=5&amp;ct=result ''(The Blackwell Guide to) Philosophical Logic.''] Oxford: [[Blackwell Publishing|Blackwell]]. {{ISBN|0-631-20693-0}}.
* [[A. C. Grayling|Grayling, A. C.]], 1997. [https://books.google.com/books?id=xGHMFdt7w30C&amp;printsec=frontcover ''An Introduction to Philosophical Logic.''] 3rd ed. Oxford: Blackwell. {{ISBN|0-631-19982-9}}.
* Jacquette, Dale, ed., 2002. [https://books.google.com/books?id=-pQ1DpdcPy4C&amp;printsec=frontcover#PPP1,M1 '' A Companion to Philosophical Logic.''] Oxford Blackwell. {{ISBN|1-4051-4575-7}}.
* {{cite book|last=Kneale|first=W&amp;M|title=The development of logic|publisher=Oxford|year=1962|url=https://books.google.com/books?id=FtXAwgy1w9cC&amp;printsec=frontcover#v=onepage&amp;q&amp;f=false}}
* McGinn, Colin,  2000. ''Logical Properties: Identity, Existence, Predication, Necessity, Truth''. Oxford: [[Oxford University Press]]. {{ISBN|0-19-926263-2}}.
* {{cite book|last=[[Willard Van Orman Quine|Quine]]|first=Willard Van Orman|title=Philosophy Of Logic|publisher=Prentice Hall: New JerseyUSA.|year=1970}}
* Sainsbury, Mark, 2001. [https://books.google.com/books?id=xAFKgdsB_akC&amp;printsec=frontcover ''Logical Forms: An Introduction to Philosophical Logic.''] 2nd ed. Oxford: Blackwell. {{ISBN|0-631-21679-0}}.
* {{cite book|last=[[P. F. Strawson|Strawson]] |first=PF|title=Philosophical Logic|publisher=OUP|year=1967}}
* [[Alfred Tarski]],1983. The concept of truth in formalized languages, pp. 152–278, Logic,semantics, metamathematics, papers from 1923 to 1938, ed. [[John Corcoran (logician)]], Hackett,Indianapolis 1983.
* Wolfram, Sybil, 1989. ''Philosophical Logic: An Introduction.'' London: [[Routledge]]. 290 pages. {{ISBN|0-415-02318-1}}, {{ISBN|978-0-415-02318-4}}
* [https://www.springer.com/10992 Journal of Philosophical Logic], Springer SBM

==External links==
* [http://www.rep.routledge.com/article/X046 Routledge Encyclopedia of Philosophy entry]

{{Philosophy topics}}
{{logic}}

{{DEFAULTSORT:Philosophy Of Logic}}
[[Category:Logic]]
[[Category:Philosophical logic]]
[[Category:Philosophy of mathematics]]
[[Category:Philosophy of logic| ]]

&lt;!--[[de:Universalwissenschaft]]--&gt;</text>
      <sha1>hwdfwto87mm2cobfo3zu6am50m0zq7v</sha1>
    </revision>
  </page>
  <page>
    <title>Quadratic irrational number</title>
    <ns>0</ns>
    <id>305331</id>
    <revision>
      <id>857619756</id>
      <parentid>829642853</parentid>
      <timestamp>2018-09-01T22:32:13Z</timestamp>
      <contributor>
        <username>Toploftical</username>
        <id>16162765</id>
      </contributor>
      <comment>/* Square root of non-square is irrational */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10202">In [[mathematics]], a '''quadratic irrational number''' (also known as a '''quadratic irrational''', a '''quadratic irrationality''' or '''quadratic surd''') is an [[irrational number]] that is the solution to some [[quadratic equation]] with rational coefficients which is irreducible over the set of [[rational number]]s.&lt;ref&gt;Jörn Steuding, ''Diophantine Analysis'', (2005), Chapman &amp; Hall, p.72.&lt;/ref&gt;  Since fractions in the coefficients of a quadratic equation can be cleared by multiplying both sides by their [[common denominator]], a quadratic irrational is an irrational root of some quadratic equation whose coefficients are [[integer]]s. The quadratic irrational numbers, a subset of the [[complex number]]s, are [[algebraic number]]s of degree 2, and can therefore be expressed as

:&lt;math&gt;{a+b\sqrt{c} \over d},&lt;/math&gt;

for [[integer]]s {{math|''a'', ''b'', ''c'', ''d''}}; with {{math|''b''}}, {{math|''c''}} and {{math|''d''}} non-zero, and with {{math|''c''}} [[Square-free integer|square-free]]. When {{math|''c''}} is positive, we get '''real quadratic irrational numbers''', while a negative {{math|''c''}} gives '''complex quadratic irrational numbers''' which are not real numbers. This implies that the quadratic irrationals have the same [[cardinality]] as ordered quadruples of integers, and are therefore [[countable]].

Quadratic irrationals are used in [[field theory (mathematics)|field theory]] to construct [[field extension]]s of the [[rational field]] {{math|ℚ}}. Given the square-free integer {{math|''c''}}, the augmentation of {{math|ℚ}} by quadratic irrationals using {{math|{{sqrt|''c''}}}} produces a [[quadratic field]] {{math|ℚ({{sqrt|''c''}}}}). For example, the inverses of elements of {{math|ℚ({{sqrt|''c''}}}}) are of the same form as the above algebraic numbers:

:&lt;math&gt;{d \over a+b\sqrt{c}} = {ad - bd\sqrt{c} \over a^2-b^2c}. &lt;/math&gt;

Quadratic irrationals have useful properties, especially in relation to [[continued fraction]]s, where we have the result that ''all'' real quadratic irrationals, and ''only'' real quadratic irrationals, have [[periodic continued fraction]] forms. For example

:&lt;math&gt;\sqrt{3}=1.732\ldots=[1;1,2,1,2,1,2,\ldots]&lt;/math&gt;

==Real quadratic irrational numbers and indefinite binary quadratic forms==
We may rewrite a quadratic irrationality as follows:

:&lt;math&gt;\frac{a+b\sqrt{c}} d = \frac{a+\sqrt{b^2c}} d.&lt;/math&gt;

It follows that every quadratic irrational number can be written in the form

:&lt;math&gt;\frac{a+\sqrt{c}} d.&lt;/math&gt;

This expression is not unique.

Fix a nonsquare, positive integer &lt;math&gt;c&lt;/math&gt; congruent to &lt;math&gt;0&lt;/math&gt; or &lt;math&gt;1&lt;/math&gt; modulo &lt;math&gt;4&lt;/math&gt;, and define a set &lt;math&gt;S_c&lt;/math&gt; as 

: &lt;math&gt;S_c = \left\{ \, \frac{a+\sqrt{c}} d \colon a, d \text{ integers, } d \text{ even},  \, \, a^2 \equiv c \pmod{2d} \, \right\}.&lt;/math&gt;

Every quadratic irrationality is in some set &lt;math&gt;S_c&lt;/math&gt;, since the congruence conditions can be met by scaling the numerator and denominator by an appropriate factor. 

A matrix

:&lt;math&gt;\begin{pmatrix} \alpha &amp; \beta\\ \gamma &amp; \delta\end{pmatrix}&lt;/math&gt;

with integer entries and &lt;math&gt;\alpha \delta-\beta \gamma=1&lt;/math&gt; can be used to transform a number &lt;math&gt;y&lt;/math&gt; in &lt;math&gt;S_c&lt;/math&gt;.  The transformed number is 

:&lt;math&gt;z = \frac{\alpha y+\beta}{\gamma y+\delta}&lt;/math&gt;

If &lt;math&gt;y&lt;/math&gt; is in &lt;math&gt;S_c&lt;/math&gt;, then &lt;math&gt;z&lt;/math&gt; is too.

The relation between &lt;math&gt;y&lt;/math&gt; and &lt;math&gt;z&lt;/math&gt; above is an [[equivalence relation]].  (This follows, for instance, because the above transformation gives a [[group action]] of the group of integer matrices with determinant 1 on the set &lt;math&gt;S_c&lt;/math&gt;.) Thus, &lt;math&gt;S_c&lt;/math&gt; partitions into equivalence classes.  Each equivalence class comprises a collection of quadratic irrationalities with each pair equivalent through the action of some matrix.  Serret's theorem implies that the regular continued fraction expansions of equivalent quadratic irrationalities are eventually the same, that is, their sequences of partial quotients have the same tail.  Thus, all numbers in an equivalence class have continued fraction expansions that are eventually periodic with the same tail.

There are finitely many equivalence classes of quadratic irrationalities in &lt;math&gt;S_c&lt;/math&gt;. The standard proof of this involves considering the map &lt;math&gt;\phi&lt;/math&gt; from [[binary quadratic form|binary quadratic forms]] of discriminant &lt;math&gt;c&lt;/math&gt; to &lt;math&gt;S_c&lt;/math&gt; given by

:&lt;math&gt; \phi (tx^2 + uxy + vy^2) = \frac{-u + \sqrt{c}}{2t}&lt;/math&gt;

A computation shows that &lt;math&gt;\phi&lt;/math&gt; is a bijection between that respects the matrix action on each set.  The equivalence classes of quadratic irrationalities are then in bijection with the equivalence classes of binary quadratic forms, and Lagrange showed that there are finitely many equivalence classes of binary quadratic forms of given discriminant.  

Through the bijection &lt;math&gt;\phi&lt;/math&gt;, expanding a number in &lt;math&gt;S_c&lt;/math&gt; in a continued fraction corresponds to reducing the quadratic form.  The eventually periodic nature of the continued fraction is then reflected in the eventually periodic nature of the orbit of a quadratic form under reduction, with reduced quadratic irrationalities (those with a purely periodic continued fraction) corresponding to reduced quadratic forms.

==Square root of non-square is irrational==
The definition of quadratic irrationals requires them to satisfy two conditions: they must satisfy a quadratic equation and they must be irrational.  The solutions to the quadratic equation ''ax''&lt;sup&gt;2&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;''bx''&amp;nbsp;+&amp;nbsp;''c''&amp;nbsp;=&amp;nbsp;0 are

:&lt;math&gt;\frac{-b\pm\sqrt{b^2-4ac}}{2a}.&lt;/math&gt;

Thus quadratic irrationals are precisely those [[real number]]s in this form that are not rational.  Since ''b'' and 2''a'' are both integers, asking when the above quantity is irrational is the same as asking when the square root of an integer is irrational.  The answer to this is that the square root of any natural number that is not a [[square number]] is irrational.

The [[square root of 2]] was the first such number to be proved irrational. [[Theodorus of Cyrene]] proved the irrationality of the square roots of whole numbers up to 17 (except those few that are [[square number]]s, such as 16), but stopped there, probably because the algebra he used could not be applied to the square root of numbers greater than 17. Euclid's Elements Book 10 is dedicated to classification of irrational magnitudes. The original proof of the irrationality of the non-square natural numbers depends on [[Euclid's lemma]].

Many proofs of the irrationality of the square roots of non-square natural numbers implicitly assume the [[fundamental theorem of arithmetic]], which was first proven by [[Carl Friedrich Gauss]] in his [[Disquisitiones Arithmeticae]]. This asserts that every integer has a unique factorization into primes. For any rational non-integer in lowest terms there must be a prime in the denominator which does not divide into the numerator. When the numerator is squared that prime will still not divide into it because of the unique factorization. Therefore, the square of a rational non-integer is always a non-integer; by [[contrapositive]], the square root of an integer is always either another integer, or irrational.

[[Euclid]] used a restricted version of the fundamental theorem and some careful argument to prove the theorem. His proof is in [[Euclid's Elements]] Book X Proposition 9.&lt;ref&gt;{{cite web | url=http://aleph0.clarku.edu/~djoyce/java/elements/bookX/propX9.html |title=Euclid's Elements Book X Proposition 9 |accessdate=2008-10-29 |author=Euclid | publisher=D.E.Joyce, Clark University }}&lt;/ref&gt;

The fundamental theorem of arithmetic is not actually required to prove the result, however.  There are self-contained proofs by [[Richard Dedekind]],&lt;ref&gt;{{cite web |author=[[Alexander Bogomolny|Bogomolny, Alexander]]|title=Square root of 2 is irrational | website=Interactive Mathematics Miscellany and Puzzles |url=http://www.cut-the-knot.org/proofs/sq_root.shtml |accessdate=May 5, 2016}}&lt;/ref&gt; among others.  The following proof was adapted by Colin Richard Hughes from a proof of the irrationality of the square root of two found by [[Theodor Estermann]] in 1975.&lt;ref&gt;{{cite journal |first=Colin Richard |last= Hughes |title=Irrational roots |journal=[[Mathematical Gazette]] |volume=83 |number=498 |year=1999 |pages=502–503}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |first=Theodor |last=Estermann |title=The irrationality of √2 | journal=Mathematical Gazette |volume=59 |number=408 |year=1975 |page=110}}&lt;/ref&gt;

Assume ''D'' is a non-square natural number, then there is a number ''n'' such that:

:''n''&lt;sup&gt;2&lt;/sup&gt; &lt; ''D'' &lt; (''n''&amp;nbsp;+&amp;nbsp;1)&lt;sup&gt;2&lt;/sup&gt;,

so in particular

:0 &lt; {{radic|''D''}} &amp;minus; ''n'' &lt; 1.

Assume the square root of ''D'' is a rational number ''p''/''q'', assume the ''q'' here is the smallest for which this is true, hence the smallest number for which ''q''{{radic|''D''}} is also an integer. Then:

:({{radic|''D''}} &amp;minus; ''n'')''q''{{radic|''D''}} = ''qD'' &amp;minus; ''nq''{{radic|''D''}}

is also an integer.  But 0&amp;nbsp;&lt;&amp;nbsp;({{radic|''D''}}&amp;nbsp;&amp;minus;&amp;nbsp;''n'')&amp;nbsp;&lt;&amp;nbsp;1 so ({{radic|''D''}}&amp;nbsp;&amp;minus;&amp;nbsp;''n'')''q''&amp;nbsp;&lt;&amp;nbsp;''q''.  Hence ({{radic|''D''}}&amp;nbsp;&amp;minus;&amp;nbsp;''n'')''q'' is an integer smaller than ''q''.  This is a contradiction since ''q'' was defined to be the smallest number with this property; hence {{radic|''D''}} cannot be rational.

==See also==
* [[Algebraic number field]]
* [[Apotome (mathematics)]]
* [[Periodic continued fraction]]
* [[Restricted partial quotients]]
* [[Quadratic integer]]

==References==
{{reflist}}

==External links==
* {{Mathworld|QuadraticSurd}}
* [http://www.numbertheory.org/php/surd.html Continued fraction calculator for quadratic irrationals]
* [https://web.archive.org/web/20050324110521/http://planetmath.org/encyclopedia/EIsIrrational.html Proof that e is not a quadratic irrational]

{{Algebraic numbers}}

[[Category:Number theory]]
[[Category:Quadratic irrational numbers| ]]&lt;!--
[[Category:Irrational numbers]]
[[Category:Algebraic numbers]]--&gt;</text>
      <sha1>2dnjov4m48xvg6pvhu948wa90ilnmsf</sha1>
    </revision>
  </page>
  <page>
    <title>Quotient algebra</title>
    <ns>0</ns>
    <id>310953</id>
    <revision>
      <id>836216616</id>
      <parentid>806392748</parentid>
      <timestamp>2018-04-13T11:42:59Z</timestamp>
      <contributor>
        <ip>177.89.61.124</ip>
      </contributor>
      <comment>correcting "agebra" -&gt; "algebra"</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8345">In [[mathematics]], a '''quotient algebra''', (where ''algebra'' means [[algebraic structure]] in the sense of [[universal algebra]]), also called a '''factor algebra''',&lt;ref&gt;A. G. Kurosh, Lectures on General Algebra, Translated from the Russian edition (Moscow, 1960), Chelsea, New York, 1963.&lt;/ref&gt; is obtained by [[Partition of a set|partition]]ing the elements of an algebra into [[equivalence class]]es given by a [[congruence relation]], that is an [[equivalence relation]] that is additionally ''compatible'' with all the [[Operation (mathematics)|operations]] of the algebra, in the formal sense described below.

== Compatible relation ==
Let ''A'' be the set of the elements of an algebra &lt;math&gt;\mathcal{A}&lt;/math&gt;, and let ''E'' be an equivalence relation on the set ''A''. The relation ''E'' is said to be ''compatible'' with (or have the ''substitution property'' with respect to) an ''n''-ary operation ''f'', if &lt;math&gt;(a_i,\; b_i) \in E&lt;/math&gt; for &lt;math&gt;1 \le i \le n&lt;/math&gt; implies &lt;math&gt;(f (a_1, a_2, \ldots, a_n), f (b_1, b_2, \ldots, b_n)) \in E&lt;/math&gt; for any &lt;math&gt;a_i,\; b_i \in A&lt;/math&gt; with &lt;math&gt;1 \le i \le n&lt;/math&gt;. An equivalence relation compatible with all the operations of an algebra is called a congruence with respect to this algebra.

== Quotient algebras and homomorphisms ==
Any equivalence relation ''E'' in a set ''A'' partitions this set in [[equivalence class]]es. The set of these equivalence classes is usually called the [[quotient set]], and denoted ''A''/''E''. For an algebra &lt;math&gt;\mathcal{A}&lt;/math&gt;, it is straightforward to define the operations induced on the elements of ''A''/''E'' if ''E'' is a congruence. Specifically, for any operation &lt;math&gt;f^{\mathcal{A}}_i&lt;/math&gt; of [[arity]] &lt;math&gt;n_i&lt;/math&gt; in &lt;math&gt;\mathcal{A}&lt;/math&gt; (where the superscript simply denotes that it is an operation in &lt;math&gt;\mathcal{A}&lt;/math&gt;, and the subscript &lt;math&gt;i \in I&lt;/math&gt; enumerates the functions in &lt;math&gt;\mathcal{A}&lt;/math&gt; and their arities) define &lt;math&gt;f^{\mathcal{A}/E}_i : (A/E)^{n_i} \to A/E&lt;/math&gt; as &lt;math&gt;f^{\mathcal{A}/E}_i ([a_1]_E, \ldots, [a_{n_i}]_E) = [f^{\mathcal{A}}_i(a_1,\ldots, a_{n_i})]_E&lt;/math&gt;, where &lt;math&gt;[x]_E \in A/E&lt;/math&gt; denotes the equivalence class of &lt;math&gt;x \in A&lt;/math&gt; generated by ''E'' ("''x''&amp;nbsp;modulo&amp;nbsp;''E''").

For an algebra &lt;math&gt;\mathcal{A} = (A, (f^{\mathcal{A}}_i)_{i \in I})&lt;/math&gt;, given a congruence ''E'' on &lt;math&gt;\mathcal{A}&lt;/math&gt;, the algebra &lt;math&gt;\mathcal{A}/E = (A/E, (f^{\mathcal{A}/E}_i)_{i \in I})&lt;/math&gt; is called the ''quotient algebra'' (or ''factor algebra'') of &lt;math&gt;\mathcal{A}&lt;/math&gt; modulo ''E''. There is a natural [[homomorphism]] from &lt;math&gt;\mathcal{A}&lt;/math&gt; to &lt;math&gt;\mathcal{A}/E&lt;/math&gt; mapping every element to its equivalence class. In fact, every homomorphism ''h'' determines a congruence relation via the [[Kernel (algebra)#Universal algebra|kernel]] of the homomorphism, &lt;math&gt; \mathop{\mathrm{ker}}\,h = \{(a,a') \in A^2\, |\, h(a) = h(a')\}\subseteq A^2&lt;/math&gt;.

Given an algebra &lt;math&gt;\mathcal{A}&lt;/math&gt;, a homomorphism ''h'' thus defines two algebras homomorphic to &lt;math&gt;\mathcal{A}&lt;/math&gt;, the [[Image (mathematics)|image]] h(&lt;math&gt;\mathcal{A}&lt;/math&gt;) and &lt;math&gt;\mathcal{A}/\mathop{\mathrm{ker}}\,h&lt;/math&gt; The two are [[isomorphic]], a result known as the ''homomorphic image theorem'' or as the [[Isomorphism theorem#First Isomorphism Theorem 4|first isomorphism theorem]] for universal algebra. Formally, let &lt;math&gt; h : \mathcal{A} \to \mathcal{B} &lt;/math&gt; be a [[surjective]] homomorphism. Then, there exists a unique isomorphism ''g'' from &lt;math&gt;\mathcal{A}/\mathop{\mathrm{ker}}\,h&lt;/math&gt; onto &lt;math&gt;\mathcal{B} &lt;/math&gt; such that ''g'' [[function composition|composed]] with the natural homomorphism induced by &lt;math&gt;\mathop{\mathrm{ker}}\,h&lt;/math&gt; equals ''h''.

== Congruence lattice ==
For every algebra &lt;math&gt;\mathcal{A}&lt;/math&gt; on the set ''A'', the [[identity relation]] on A, and &lt;math&gt;A \times A&lt;/math&gt; are trivial congruences. An algebra with no other congruences is called ''simple''.

Let &lt;math&gt;\mathrm{Con}(\mathcal{A})&lt;/math&gt; be the set of congruences on the algebra &lt;math&gt;\mathcal{A}&lt;/math&gt;. Because congruences are closed under intersection, we can define a [[Meet (mathematics)|meet operation]]: &lt;math&gt; \wedge : \mathrm{Con}(\mathcal{A}) \times \mathrm{Con}(\mathcal{A}) \to \mathrm{Con}(\mathcal{A})&lt;/math&gt; by simply taking the intersection of the congruences &lt;math&gt;E_1 \wedge E_2 = E_1\cap E_2&lt;/math&gt;.

On the other hand, congruences are not closed under union. However, we can define the [[Closure operator|closure]] of any [[binary relation]] ''E'', with respect to a fixed algebra &lt;math&gt;\mathcal{A}&lt;/math&gt;, such that it is a congruence, in the following way: &lt;math&gt; \langle E \rangle_{\mathcal{A}} = \bigcap \{ F \in \mathrm{Con}(\mathcal{A}) | E \subseteq F \}&lt;/math&gt;. Note that the (congruence) closure of a binary relation depends on the operations in &lt;math&gt;\mathcal{A}&lt;/math&gt;, not just on the carrier set. Now define &lt;math&gt; \vee: \mathrm{Con}(\mathcal{A}) \times \mathrm{Con}(\mathcal{A}) \to \mathrm{Con}(\mathcal{A})&lt;/math&gt; as &lt;math&gt;E_1 \vee E_2 = \langle E_1\cup E_2 \rangle_{\mathcal{A}} &lt;/math&gt;.

For every algebra &lt;math&gt;\mathcal{A}&lt;/math&gt;, &lt;math&gt;(\mathrm{Con}(\mathcal{A}), \wedge, \vee)&lt;/math&gt; with the two operations defined above forms a [[Lattice (order)|lattice]], called the ''congruence lattice'' of &lt;math&gt;\mathcal{A}&lt;/math&gt;. It's a [[distributive lattice]].

== Maltsev conditions ==
If two congruences ''permute'' (commute) with the [[composition of relations]] as operation, i.e. &lt;math&gt;\alpha\circ\beta = \beta\circ\alpha&lt;/math&gt;, then their join (in the congruence lattice) is equal to their composition: &lt;math&gt;\alpha\circ\beta = \alpha\vee\beta&lt;/math&gt;. An algebra is called ''congruence-permutable'' if every pair of its congruences permutes; likewise a [[Variety (universal algebra)|variety]] is said to be congruence-permutable if all its members are 
congruence-permutable algebras.

In 1954, [[Anatoly Maltsev]] established the following characterization of congruence-permutable varieties: a variety is congruence permutable if and only if there exist a ternary term {{nowrap|''q''(''x'', ''y'', ''z'')}} such that {{nowrap|''q''(''x'', ''y'', ''y'') ≈ ''x'' ≈ ''q''(''y'', ''y'', ''x'')}}; this is called a Maltsev term and varieties with this property are called Maltsev varieties. Maltsev's characterization explains a large number of similar results in groups (take {{nowrap|1=''q'' = ''xy''&lt;sup&gt;−1&lt;/sup&gt;''z''}}), rings, [[quasigroup]]s (take {{nowrap|1=''q'' =  (x / (y \ y))(y \ z))}}, [[complemented lattice]]s, [[Heyting algebra]]s etc. Furthermore, every congruence-permutable algebra is congruence-modular, i.e. its lattice of congruences is [[modular lattice]] as well; the converse is not true however.

After Maltsev's result, other researchers found characterizations based on conditions similar to that found by Maltsev but for other kinds of properties, e.g. in 1967 [[Bjarni Jónsson]] found the conditions for varieties having congruence lattices that are distributive (thus called congruence-distributive varieties). Generically, such conditions are called Maltsev conditions.

This line of research led to the [[Pixley–Wille algorithm]] for generating Maltsev conditions associated
with congruence identities.&lt;ref name="KearnesKiss2013"&gt;{{cite book|author1=Keith Kearnes|author2=Emil W. Kiss|title=The Shape of Congruence Lattices|year=2013|publisher=American Mathematical Soc.|isbn=978-0-8218-8323-5|page=4}}&lt;/ref&gt;

== See also ==
* [[Quotient ring]]
* [[Congruence lattice problem]]
* [[Lattice of subgroups]]

==Notes==
{{reflist}}

== References ==
* {{cite book|author1=Klaus Denecke|author2=Shelly L. Wismath|title=Universal algebra and coalgebra|url=https://books.google.com/books?id=NgTAzhC8jVAC&amp;pg=PA14|year=2009|publisher=World Scientific|isbn=978-981-283-745-5|pages=14–17}}
* {{cite book|author=Purna Chandra Biswal|title=Discrete mathematics and graph theory|url=https://books.google.com/books?id=hLX6OG1U5W8C&amp;pg=PA215|year=2005|publisher=PHI Learning Pvt. Ltd.|isbn=978-81-203-2721-4|page=215}}
* {{cite book|author=Clifford Bergman|title=Universal Algebra: Fundamentals and Selected Topics|year=2011|publisher=CRC Press|isbn=978-1-4398-5129-6|pages=122–124, 137 (Maltsev varieties)}}

[[Category:Universal algebra]]</text>
      <sha1>twbjtjuu2o0x4o9nzyvb7h56ym41eej</sha1>
    </revision>
  </page>
  <page>
    <title>Random modulation</title>
    <ns>0</ns>
    <id>30001920</id>
    <revision>
      <id>793432776</id>
      <parentid>752809357</parentid>
      <timestamp>2017-08-01T18:29:58Z</timestamp>
      <contributor>
        <username>Rjwilmsi</username>
        <id>203434</id>
      </contributor>
      <minor/>
      <comment>Journal cites: fix page range,  using [[Project:AWB|AWB]] (12158)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2747">In the theories of [[modulation]] and of [[stochastic process]]es, '''random modulation''' is the creation of a new signal from two other signals by the process of [[quadrature amplitude modulation]]. In particular, the two signals are considered as being [[random process]]es. For applications, the two original signals need have a limited frequency range, and these are used to modulate a third sinusoidal [[carrier signal]] whose frequency is above the range of frequencies contained in the original signals.

==Details==

The random modulation procedure starts with two stochastic [[Baseband signal#Baseband signal|baseband signals]], &lt;math&gt;x_c(t)&lt;/math&gt; and &lt;math&gt;x_s(t)&lt;/math&gt;, whose [[frequency spectrum]] is non-zero only for &lt;math&gt;f \in [-B/2,B/2]&lt;/math&gt;. It applies [[quadrature modulation]] to combine these with a carrier frequency &lt;math&gt;f_0&lt;/math&gt; (with &lt;math&gt;f_0 &gt; B/2&lt;/math&gt;) to form the signal &lt;math&gt;x(t)&lt;/math&gt; given by
:&lt;math&gt;x(t)=x_c(t)\cos(2 \pi f_0 t)-x_s(t)\sin(2 \pi f_0 t)= \Re \left \{ \underline{x}(t)e^{j 2 \pi f_0 t}\right \} ,&lt;/math&gt;
where &lt;math&gt;\underline{x}(t)&lt;/math&gt; is the [[Baseband signal#Equivalent baseband signal|equivalent baseband representation]] of the modulated signal &lt;math&gt;x(t)&lt;/math&gt;
:&lt;math&gt;\underline{x}(t)=x_c(t)+j x_s(t).&lt;/math&gt;

In the following it is assumed that &lt;math&gt;x_c(t)&lt;/math&gt; and &lt;math&gt;x_s(t)&lt;/math&gt; are two real jointly [[Wide sense stationary#Weak or wide-sense stationarity|wide sense stationary]] processes. It can be shown{{citation needed|date=August 2011}} that the new signal &lt;math&gt;x(t)&lt;/math&gt; is wide sense stationary [[iff|if and only if]] &lt;math&gt;\underline{x}(t)&lt;/math&gt; is circular complex, i.e. if and only if &lt;math&gt;x_c(t)&lt;/math&gt; and &lt;math&gt;x_s(t)&lt;/math&gt; are such that
:&lt;math&gt;R_{x_c x_c}(\tau)=R_{x_s x_s}(\tau) \qquad  \text{and  }\qquad   R_{x_c x_s}(\tau)=-R_{x_s x_c}(\tau).&lt;/math&gt;

{{more footnotes|date=August 2011}}

== Bibliography ==
*{{cite book |title=Probability, random variables and stochastic processes |last1= Papoulis|first1= Athanasios|authorlink1= Athanasios Papoulis|first2=S. Unnikrishna|last2= Pillai |year= 2002|publisher= McGraw-Hill Higher Education|edition= 4th|chapter=Random walks and other applications|pages=463–473}}
*{{cite book |title=Segnali, Processi Aleatori, Stima |last1= Scarano|first1= Gaetano|year= 2009|publisher= Centro Stampa d'Ateneo|language=it}}
*{{Cite journal | last1 = Papoulis | first1 = A. | doi = 10.1109/TASSP.1983.1164046 | title = Random modulation: A review | journal = IEEE Transactions on Acoustics, Speech, and Signal Processing | volume = 31 | pages = 96–105| year = 1983 | pmid =  | pmc = }}

{{DEFAULTSORT:Random Modulation}}
[[Category:Statistical signal processing]]


{{Signal-processing-stub}}
{{stat-stub}}</text>
      <sha1>jkvlw0bvjcwsj3q3thnv1rvijrhhdqz</sha1>
    </revision>
  </page>
  <page>
    <title>Representation of a Lie superalgebra</title>
    <ns>0</ns>
    <id>852522</id>
    <revision>
      <id>790774618</id>
      <parentid>606964581</parentid>
      <timestamp>2017-07-16T00:13:09Z</timestamp>
      <contributor>
        <username>Deacon Vorbis</username>
        <id>29330520</id>
      </contributor>
      <minor/>
      <comment>/* top */LaTeX spacing clean up, replaced: \,&lt;/math&gt; → &lt;/math&gt; (4) using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2623">{{unreferenced|date=May 2014}}
In the [[mathematics|mathematical]] field of [[representation theory]], a '''representation of a Lie superalgebra''' is an [[semigroup action|action]] of [[Lie superalgebra]] ''L'' on a [[graded vector space|'''Z'''&lt;sub&gt;2&lt;/sub&gt;-graded vector space]] ''V'', such that if ''A'' and ''B'' are any two pure elements of ''L'' and ''X'' and ''Y'' are any two pure elements of ''V'', then

:&lt;math&gt;(c_1 A+c_2 B)\cdot X=c_1 A\cdot X + c_2 B\cdot X&lt;/math&gt;

:&lt;math&gt;A\cdot (c_1 X + c_2 Y)=c_1 A\cdot X + c_2 A\cdot Y&lt;/math&gt;

:&lt;math&gt;(-1)^{A\cdot X}=(-1)^A(-1)^X&lt;/math&gt;

:&lt;math&gt;[A,B]\cdot X=A\cdot (B\cdot X)-(-1)^{AB}B\cdot (A\cdot X).&lt;/math&gt;

Equivalently, a representation of ''L'' is a '''Z'''&lt;sub&gt;2&lt;/sub&gt;-graded representation of the [[universal enveloping algebra]] of ''L'' which respects the third equation above.

==Unitary representation of a star Lie superalgebra==
A &lt;sup&gt;*&lt;/sup&gt; [[Lie superalgebra]] is a complex Lie superalgebra equipped with an [[Involution (mathematics)|involutive]] [[antilinear]] [[map]] &lt;sup&gt;*&lt;/sup&gt; such that * respects the grading and 

:[a,b]&lt;sup&gt;*&lt;/sup&gt;=[b&lt;sup&gt;*&lt;/sup&gt;,a&lt;sup&gt;*&lt;/sup&gt;].

A [[unitary representation]] of such a Lie algebra is a '''Z'''&lt;sub&gt;2&lt;/sub&gt; [[graded vector space|graded]] [[Hilbert space]] which is a representation of a Lie superalgebra as above together with the requirement that [[self-adjoint]] elements of the Lie superalgebra are represented by [[Hermitian]] transformations.

This is a major concept in the study of [[supersymmetry]] together with representation of a Lie superalgebra on an algebra. Say A is an [[star-algebra|*-algebra]] representation of the Lie superalgebra (together with the additional requirement that * respects the grading and L[a]&lt;sup&gt;*&lt;/sup&gt;=-(-1)&lt;sup&gt;La&lt;/sup&gt;L&lt;sup&gt;*&lt;/sup&gt;[a&lt;sup&gt;*&lt;/sup&gt;]) and '''H''' is the unitary rep and also, '''H''' is a [[unitary representation]] of A.

These three reps are all compatible if for pure elements a in A, |ψ&gt; in '''H''' and L in the Lie superalgebra,

:L[a|ψ&gt;)]=(L[a])|ψ&gt;+(-1)&lt;sup&gt;La&lt;/sup&gt;a(L[|ψ&gt;]).

Sometimes, the Lie superalgebra is [[embedding|embedded]] within A in the sense that there is a homomorphism from the [[universal enveloping algebra]] of the Lie superalgebra to A. In that case, the equation above reduces to

:L[a]=La-(-1)&lt;sup&gt;La&lt;/sup&gt;aL.

This approach avoids working directly with a Lie supergroup, and hence avoids the use of auxiliary [[Grassmann number]]s.

==See also==
* [[Graded vector space]]
* [[Lie algebra representation]]
* [[Representation theory of Hopf algebras]]

[[Category:Representation theory of Lie algebras]]



{{algebra-stub}}</text>
      <sha1>fnfo0nvyx2faj3g84xdgedzsq94dkj9</sha1>
    </revision>
  </page>
  <page>
    <title>Sequential algorithm</title>
    <ns>0</ns>
    <id>23868049</id>
    <revision>
      <id>671141953</id>
      <parentid>618605763</parentid>
      <timestamp>2015-07-12T18:49:15Z</timestamp>
      <contributor>
        <username>Fgnievinski</username>
        <id>6727347</id>
      </contributor>
      <comment>==See also== * [[Online algorithm]] * [[Streaming algorithm]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1290">In [[computer science]], a '''sequential algorithm''' or '''serial algorithm''' is an [[algorithm]] that is executed sequentially – once through, from start to finish, without other processing executing – as opposed to [[concurrent computing|concurrently]] or in [[parallel computing|parallel]]. The term is primarily used to contrast with ''[[concurrent algorithm]]'' or ''[[parallel algorithm]];'' most standard computer algorithms are sequential algorithms, and not specifically identified as such, as sequentialness is a background assumption. Concurrency and parallelism are in general distinct concepts, but they often overlap – many [[distributed algorithm]]s are both concurrent and parallel – and thus "sequential" is used to contrast with both, without distinguishing which one. If these need to be distinguished, the opposing pairs sequential/concurrent and serial/parallel may be used.

"Sequential algorithm" may also refer specifically to an algorithm for decoding a [[convolutional code]].&lt;ref&gt;{{cite web|url=http://www.encyclopedia.com/doc/1O11-sequentialalgorithm.html|title=A Dictionary of Computing at Encyclopedia.com}}&lt;/ref&gt;

==See also==
* [[Online algorithm]]
* [[Streaming algorithm]]

==References==
{{Reflist}}

[[Category:Algorithms]]


{{algorithm-stub}}</text>
      <sha1>jnm9n03t6at8y2w4d7pv0wd0rmfc1ju</sha1>
    </revision>
  </page>
  <page>
    <title>Stream (computer science)</title>
    <ns>0</ns>
    <id>46510274</id>
    <revision>
      <id>738858064</id>
      <parentid>736657876</parentid>
      <timestamp>2016-09-11T13:41:55Z</timestamp>
      <contributor>
        <username>Matthiaspaul</username>
        <id>13467261</id>
      </contributor>
      <comment>preparing disamb page</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="833">{{for|the more applied concept in computing|Stream (computing)}}
In [[type theory]] and [[functional programming]], a '''stream''' is a potentially infinite analog of a [[list (computing)|list]], given by the [[Algebraic data type|coinductive definition]]:

&lt;source lang=haskell&gt;
data Stream α = Nil | Cons α (Stream α)
&lt;/source&gt;

Generating and computing with streams requires [[lazy evaluation]], either implicitly in a lazily evaluated language or by creating and forcing [[thunk (functional programming)|thunk]]s in an eager language. In [[total language]]s they must be defined as [[codata (computer science)|codata]] and can be iterated over using (guarded) [[corecursion]].

== See also ==
* [[Coinduction]]

{{plt-stub}}

[[Category:Type theory]]
[[Category:Functional programming]]
[[Category:Functional data structures]]</text>
      <sha1>s7up564olgcoa9xqynz09dvo5g06pxs</sha1>
    </revision>
  </page>
  <page>
    <title>Ten rays model</title>
    <ns>0</ns>
    <id>52238391</id>
    <revision>
      <id>790777659</id>
      <parentid>785379638</parentid>
      <timestamp>2017-07-16T00:37:42Z</timestamp>
      <contributor>
        <username>Deacon Vorbis</username>
        <id>29330520</id>
      </contributor>
      <minor/>
      <comment>/* Analysis for antennas of heights different heights located in street's any point */LaTeX spacing clean up, replaced: \, &lt;/math&gt; → &lt;/math&gt; using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8834">[[File:Figure 1. Top view of 10 rays.svg|thumb|284x284px|Top view of 10 rays]]
[[File:Grafica 2 ingles.svg|thumb|284x284px|Characteristic rays of the model]]
[[File:NUEVA (ingles).svg|thumb|284x284px|Overhead View of the Ten-Ray Model (Types of reflections)]]

The ten-ray model is a model applied to the transmissions in the urban area, to generate a model of ten rays typically four rays more are added to the  [[Six rays model|six rays model]], these are (&lt;math&gt;R3&lt;/math&gt; and &lt;math&gt;R4&lt;/math&gt; bouncing on both sides of the wall); This incorporate paths from one to three reflections: specifically, there is the LOS ([[Line-of-sight propagation|Line of sight]]), GR (ground reflected), SW (single-wall reflected), DW (double-wall reflected), TW (triple-wall reflected), WG (wall-ground reflected) and GW (ground-wall reflected paths). Where each one of the paths bounces on both sides of the wall.

Experimentally, it has been demonstrated that the ten ray model simulates or can represent the  [[Radio propagation|propagation]] of signals through a [[Dielectric|dielectric]] canyon, in it which the rays that travel from a [[Transmitter|transmitter]]   point to a receiver point bounce many times.

As example for this model it is assume: a rectilinear free space with two walls, one upper and the other lower, from which two vertical bases are positioned at their ends, these are the transmitting and receiving [[Antenna (radio)|antennas]] that it’s locate in such a way that their heights don’t surpass the limits of the top wall; Achieved this the structure acts as free space for its functioning similar to that of a dielectric canyon of signals propagation, since the rays transmitted from the transmitting antenna will collide each side of the upper and lower walls infinity of times (for this example up to 3 reflections) until reaching the receiving antenna. During the course of the rays for each reflection they suffer, part of the energy of the signal is dissipated in each reflection, normally after the third reflection of said ray its resulting component which is a retro-reflected ray is insignificant with a negligible energy.&lt;ref&gt;{{Cite book|title=Wireless Communications.|last=Goldsmith|first=Andrea|publisher=Cambridge University Press, ed.|year=2005|isbn=978-0521837163|location=New York.|pages=|quote=|via=}}&lt;/ref&gt;


== Mathematical deduction ==


=== Analysis for antennas of heights different heights located in street's any point ===

For the [[mathematical modeling]] of the propagation of ten rays, One has in account a side view and this starts off modeling the two first rays (line by sight and his respective reflection), Considering that antennas have different heights, Then &lt;math&gt;h_{Tx}\neq{h_{Rx}}&lt;/math&gt;, and they have a direct distance d that separates the two antennas; The first ray is formed applying Pitágoras theorem:

: &lt;math&gt;R_0=\sqrt{d^2+(h_t-h_r)^2}&lt;/math&gt;

The second ray or the reflected ray is made in a similar way to the first, but in this case the heights of the antennas to form the right angled triangle for the reflection of the height of the transmitter are added up.

: &lt;math&gt;R_0' =\sqrt{d^2+(h_t+h_r)^2}&lt;/math&gt;

In the deduction of the third ray it is necessary find the angle between the direct distance &lt;math&gt;d&lt;/math&gt; and the distance of line of view &lt;math&gt;R_0&lt;/math&gt;'''&lt;sub&gt;.&lt;/sub&gt;'''

: &lt;math&gt;\cos\theta=\frac{h_t-h_r}{R_0}&lt;/math&gt;

Viewing the model with a side view, it is necessary to find a flat distance between the transmitter and receiver called  &lt;math&gt;d'&lt;/math&gt;.

: &lt;math&gt;d'=\sqrt{d^2-(w_{r2}-w_{t2})^2}&lt;/math&gt;

Now we deduce the remaining height of the wall from the height of the receiver called  &lt;math&gt;z&lt;/math&gt; by the similarity of triangles:

: &lt;math&gt;\frac z a =\frac{h_t}{d'}&lt;/math&gt;

: &lt;math&gt; z =\frac{ {h_t}a }{d'}&lt;/math&gt;

By likeness of triangles we can deduce the distance from where collides the ray to wall until the perpendicular of the receiver called &lt;math&gt;a&lt;/math&gt;, getting:

: &lt;math&gt;\frac a d' = \frac{w_{r2}}{w_{t2}+w_{r2}} &lt;/math&gt;

: &lt;math&gt;a=\frac{d'w_{r2}}{w_{t2}+w_{r2}}&lt;/math&gt;

The third ray is defined as a model of two-rays, by which is:

: &lt;math&gt;R_1=\frac{\sqrt{(h_t-h_r-z)^2+(d'-a)^2}+\sqrt{z^2+a^2}}{\cos\theta}&lt;/math&gt;

Taking a side view it is achieves to evidence the reflected ray that there in &lt;math&gt;R_{1}&lt;/math&gt;  and is find as following manner:

: &lt;math&gt;R_1' =\frac{\sqrt{(h_t+h_r+z)^2+(d'-a)^2}+\sqrt{(2h_r+z)^2+a^2}}{\cos\theta} &lt;/math&gt;

As exist two rays that collide once on the wall, then is find the fifth ray, equating it to the third.

: &lt;math&gt;R_2=R_1 &lt;/math&gt;

Similarly, is equalized the sixth ray with the fourth ray, since they have the same characteristics.

: &lt;math&gt;R_2'=R_1'&lt;/math&gt;

[[File:Grafica 3 me ingles.svg|thumb|311x311px|Side view of two transmitted beams reflected from one wall to the other side and reflected to the receiver on antennas of different heights at any point on the street.]]

To model the rays that collide with the wall twice, is used the Pythagoras theorem because of the direct distance &lt;math&gt;d&lt;/math&gt; and the sum of the distances between the receiver to each wall with double of distance of the transmitter to the wall  &lt;math&gt;w_{t2}&lt;/math&gt;, this divides on the angle formed between the direct distance and the reflected ray.

: &lt;math&gt;R_3=\frac{\sqrt{d^2+(w_{r2}+2w_{t2}+w_{r1})^2}}{\cos\theta}&lt;/math&gt;

For the eighth ray is calculate a series of variables that allow to deduce the complete equation, which is composed by distances and heights that were found by likeness of triangles'''.'''

In first instance is take the flat distance between the wall of the second shock and the receiver:

: &lt;math&gt;x=\frac{d' w_{r1}}{w_{r2}+w_{r1}}&lt;/math&gt;

Is found the flat distance between the transmitter and the wall in the first shock.

: &lt;math&gt;h_{p2}=h_r+z_2&lt;/math&gt;

: &lt;math&gt;y=\frac{d'w_{t2}}{w_{r2}+w_{r1}}&lt;/math&gt;

Finding the distance between the height of the wall of the second shock with respect to the first shock, is obtain:

: &lt;math&gt;z_1=\frac{h_t(d'-(x+y))}{d'}&lt;/math&gt;

Deducing also the distance between the height of the wall of the second shock with respect to the receiver:

: &lt;math&gt;z_{2}=\frac{h_t x}{d'}&lt;/math&gt;

Calculating the height of the wall where occurs the first hit:

: &lt;math&gt;h_{p1}=h_r+z_1+z_2&lt;/math&gt;

Calculating the height of the wall where occurs the second shock:

: &lt;math&gt;h_{p2}=h_r+z_2&lt;/math&gt;

With these parameters is calculate the equation for the eighth ray:

: &lt;math&gt;R_3'=\frac{\sqrt{y^2+(h_t+h_{p1})^2}+\sqrt{(d'-(x+y))^2+(h_{p1}+h_{p2}^2}+\sqrt{x^2+(h_r+h_{p2})^2}}{\cos\theta}&lt;/math&gt;

For the ninth ray, equation is the same as the seventh ray due to its characteristics:

: &lt;math&gt;R_4=R_3&lt;/math&gt;

For the tenth ray, the equation is the same as the eighth ray due to its reflected ray shape:

: &lt;math&gt;R_4'=R_3'&lt;/math&gt;

=== Losses for trajectory of free space ===
[[File:Perdidas_10_rayos.jpg|thumb|276x276px|Modeling of the losses by free space trajectory in the 6-ray model when the distances of the wall and the heights are different.]]
Is considered a signal transmitted through free space to a receiver located at a distance ''d'' from the transmitter.

Assuming there are no obstacles between the transmitter and the receiver, the signal propagates along a straight line between the two. The beam model associated with this transmission is denominated line of sight (LOS), and the signal received corresponding is called the LOS signal or beam.&lt;ref&gt;{{Cite book|title=Wireless &amp; Cellular Communications Class Notes for TLEN-5510-Fall.|last=Schwengler|first=Thomas|publisher=Universidad de Colorado.|year=2016|isbn=|location=|pages=http://morse.colorado.edu/~tlen5510/text/classwebch3.html|quote=Chapter 3: Radio Propagation Modeling|via=}}&lt;/ref&gt;

The trajectory losses of the ten-ray model in free space is defined as:

: &lt;math&gt;p_{0}(t)=\sqrt{(G_iG_R)}\frac \lambda {4\pi} \left(\frac{\exp(j2\pi R_0/{\lambda})}{R_0} + \Gamma \frac{\exp(j2\pi R_0'/{\lambda})}{R_0'}\right)&lt;/math&gt;

: &lt;math&gt;p_1(t)=\sqrt{(G_iG_R)}\frac \lambda {4\pi}~\Gamma_1\left(\frac{\exp(j2\pi R_1/\lambda)}{R_1}+\Gamma\frac{\exp(j2\pi R_1' / \lambda)}{R_1'}\right)&lt;/math&gt;

: &lt;math&gt;p_2(t)=\sqrt{(G_iG_R)}\frac \lambda {4\pi}~\Gamma_1\left(\frac{\exp(j2\pi R_2/\lambda)}{R_2}+\Gamma\frac{\exp(j2\pi R_2'/\lambda)}{R_2'}\right)&lt;/math&gt;

: &lt;math&gt;p_3(t)=\sqrt{(G_iG_R)}\frac \lambda {4\pi}~\Gamma_1\left(\frac{\exp(j2\pi R_3/\lambda)}{R_3}+\Gamma\frac{\exp(j2\pi R_3'/\lambda)}{R_3'}\right)&lt;/math&gt;

: &lt;math&gt;p_4(t)=\sqrt{(G_iG_R)}\frac \lambda {4\pi}~\Gamma_1\left(\frac{\exp(j2\pi R_4/\lambda)}{R_4}+\Gamma\frac{\exp(j2\pi R_4'/\lambda)}{R_4'}\right)&lt;/math&gt;

: &lt;math&gt;P_L~(dB)=20\log\left| \sum_{i=0}^N P_i \right|&lt;/math&gt;

== See also ==
* [[Six rays model]]
* [[Ray tracing (physics)]]
* [[Two-ray ground-reflection model]]

== References ==
&lt;references /&gt;

[[Category:Mathematical modeling]]</text>
      <sha1>86r7eiia5kzynqizwodiyngcu2jlwlj</sha1>
    </revision>
  </page>
  <page>
    <title>Thomas Ranken Lyle Medal</title>
    <ns>0</ns>
    <id>27622712</id>
    <revision>
      <id>848006459</id>
      <parentid>801981156</parentid>
      <timestamp>2018-06-29T06:14:54Z</timestamp>
      <contributor>
        <username>InternetArchiveBot</username>
        <id>27015025</id>
      </contributor>
      <comment>Rescuing 3 sources and tagging 0 as dead. #IABot (v2.0beta)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12340">The '''Thomas Ranken Lyle Medal''' is awarded at most every two years by the [[Australian Academy of Science]] to a [[mathematician]] or [[physicist]] for his or her outstanding research accomplishments.&lt;ref name="trlm"&gt;[http://www.science.org.au/awards/awards/lyle.html Thomas Ranken Lyle Medal] {{Webarchive|url=https://web.archive.org/web/20101128133557/http://science.org.au/awards/awards/lyle.html |date=28 November 2010 }}, [[Australian Academy of Science]], retrieved 2010-06-06.&lt;/ref&gt; It is named after [[Thomas Ranken Lyle]], an Irish [[mathematical physics|mathematical physicist]] who became a professor at the [[University of Melbourne]].  The award takes the form of a [[bronze]] medal&lt;ref name="bc32"/&gt; bearing the design of the head of Thomas Lyle, as sculpted by [[Rayner Hoff]].&lt;ref name="a35"&gt;{{citation |title=Lyle Medal Award |journal=[[The Argus (Australia)]] |url=http://nla.gov.au/nla.news-article11007872 |date=16 January 1935}}.&lt;/ref&gt; 

The medal was founded by the [[Australian National Research Council]] (ANRC) in 1932,&lt;ref name="bc32"&gt;{{citation |journal=[[Brisbane Courier]] |url=http://nla.gov.au/nla.news-article21991888 |title=National Research: Annual Meeting of Council |date=18 August 1932}}.&lt;/ref&gt;&lt;ref&gt;{{citation |title=University Senate |url=http://nla.gov.au/nla.news-article16998544 |journal=[[Sydney Morning Herald]] |date=16 August 1933}}.&lt;/ref&gt; and first awarded in 1935.&lt;ref name="trlm"/&gt;&lt;ref name="a35"/&gt; When the Australian Academy of Science was established in 1954, it took over the roles of the ANRC, including administration of the medal.

==Recipients==
{| class="wikitable sortable"
|-
! Year !! Recipients&lt;ref name="trlm"/&gt; !! Contribution
|-
| 1935 || {{sortname|John Raymond |Wilton}} &lt;ref name="a35"/&gt; ||
|-
| 1941 || {{sortname|George Henry |Briggs}}&lt;ref name="smh41"&gt;{{citation |title=Lyle Medals Awarded |journal=[[Sydney Morning Herald]] |url=http://nla.gov.au/nla.news-article17747686|date=10 July 1941}}.&lt;/ref&gt; ||
|-
| 1941 || {{sortname|Thomas Gerald |Room}}&lt;ref name="smh41"/&gt;&lt;ref&gt;{{citation | last1 = Hirschfeld | first1 = J. W. P. | last2 = Wall | first2 = G. E. | journal = Biographical Memoirs of Fellows of the Royal Society | pages = 575–601 | title = Thomas Gerald Room. 10 November 1902–2 April 1986 | jstor = 769963
 | volume = 33 | year = 1987 | doi=10.1098/rsbm.1987.0020}}. Also published in ''Historical Records of Australian Science'' '''7''' (1): 109–122, {{doi|10.1071/HR9870710109}}. An abridged version is [http://www.asap.unimelb.edu.au/bsparcs/aasmemoirs/room.htm online] at the Bright Sparcs web site of the Australian Academy of Science].&lt;/ref&gt; ||
|-
| 1947 || {{sortname|John Conrad |Jaeger}}&lt;ref name="prcm"&gt;{{citation |title=Physicists receive coveted medals |url=http://nla.gov.au/nla.news-article22445914 |journal=[[The Argus (Australia)]] |date=20 August 1947}}.&lt;/ref&gt; ||
|-
| 1947 || {{sortname|David Forbes |Martyn}}&lt;ref name="prcm"/&gt; || [[atmospheric tide]]s&lt;ref&gt;{{citation |title=Tides found in atmosphere |url=https://news.google.com/newspapers?id=X30QAAAAIBAJ&amp;sjid=wZMDAAAAIBAJ&amp;pg=3035,1211515&amp;dq=lyle-medal&amp;hl=en |journal=[[Sydney Morning Herald]] |date=9 September 1947}}.&lt;/ref&gt;
|-
| 1949 || {{sortname|Keith Edward |Bullen}} ||
|-
| 1951 || {{sortname|Thomas MacFarland |Cherry}} ||
|-
| 1953 || {{sortname|Joseph Lade |Pawsey}}&lt;ref&gt;{{citation |url=http://nla.gov.au/nla.news-article2905353 |title=Two scientists honoured |journal=[[Canberra Times]] |date=13 January 1954}}.&lt;/ref&gt; ||
|-
| 1957 || {{sortname|Bernard Y. |Mills}} ||
|-
| 1959 || {{sortname|Eric |Barnes|Eric Stephen Barnes}}&lt;ref&gt;G.E. Wall, Jane Pitman and [[Ren Potts]],[http://www.science.org.au/fellows/PDF/barnes.pdf "Eric Stephen Barnes 1924-2000"] {{Webarchive|url=https://web.archive.org/web/20110307235303/http://science.org.au/fellows/PDF/barnes.pdf |date=7 March 2011 }}, ''Historical Records of Australian Science, 2004, '''15''', 21-45&lt;/ref&gt; || 
|-
| 1961 || {{sortname|H.O. |Lancaster}} ||
|-
| 1963 || {{sortname|Graeme Reade Anthony |Ellis}}&lt;ref name="smh64"&gt;{{citation |journal=[[Sydney Morning Herald]] |date=7 September 1964 |url=https://news.google.com/newspapers?id=x9InAAAAIBAJ&amp;sjid=E-YDAAAAIBAJ&amp;pg=971,2489066&amp;dq=lyle-medal&amp;hl=en |title=Australian Scientists: Two Australian professors have been jointly awarded the Thomas Lyle Ranken Medal for 1963}}.&lt;/ref&gt; ||
|-
| 1963 || {{sortname|Patrick A. P. |Moran|Pat Moran (statistician)}}&lt;ref name="smh64"/&gt; ||
|-
| 1966 || {{sortname|Stuart Thomas |Butler}} || [[nuclear reaction]] theory, [[Plasma (physics)|plasma physics]], and [[atmospheric tide]]s&lt;ref&gt;{{citation |url=http://www.asap.unimelb.edu.au/bsparcs/aasmemoirs/butler.htm|title=Stuart Thomas Butler 1926–1982 |first=C.N. |last=Watson-Munro |journal=Historical Records of Australian Science |volume=5 |issue=4 |year=1983}}.&lt;/ref&gt; 
|-
| 1968 || {{sortname|George |Szekeres}} || "a wide range of mathematical disciplines" including&lt;br&gt;[[Iterated function|fractional iteration of functions]], [[numerical integration]], [[graph theory]], and [[Theory of relativity|relativistic]] [[kinematics]]&lt;ref&gt;{{citation |last1=Giles |first1=J. R. |last2=Wallis |first2=J. S. |title=George Szekeres. With affection and respect |journal=Journal of the Australian Mathematical Society Series A |volume=21 |year=1976 |issue=4 |pages=385–392 |doi=10.1017/S1446788700019212}}.&lt;/ref&gt;
|-
| 1970 || {{sortname|Robert Hanbury |Brown}} ||
|-
| 1972 || {{sortname|Hans |Buchdahl}} ||
|-
| 1975 || {{sortname|John Paul |Wild}} || [[radio astronomy]] of the [[sun]]&lt;ref&gt;{{citation |title=Thomas Ranken Lyle Medal 1975: John Paul Wild |journal=Historical Records of Australian Science |volume=3 |issue=2 |year=1975 |doi=10.1071/HR9760320112|page=112}}.&lt;/ref&gt;
|-
| 1977 || {{sortname|Kurt |Mahler}} || [[number theory]]&lt;ref&gt;{{citation |title=Thomas Ranken Lyle Medal 1977: Kurt Mahler |journal=Historical Records of Australian Science |volume=3 |issue=3–4 |year=1977 |doi=10.1071/HR9770340189 |page=189}}.&lt;/ref&gt;
|-
| 1979 || {{sortname|Edward J. |Hannan}} || [[statistics]] of [[stationary process]]es&lt;ref&gt;{{citation |title=Thomas Ranken Lyle Medal 1979: E.J. Hannan |journal=Historical Records of Australian Science |volume=4 |issue=2 |year=1979 |doi=10.1071/HR9790420109 |page=109}}.&lt;/ref&gt;
|-
| 1981 || {{sortname|J.R. |Philip}} ||
|-
| 1981 || {{sortname|D.W. |Robinson}} ||
|-
| 1983 || {{sortname|Rodney J.| Baxter}} ||
|-
| 1985 || {{sortname|Allan |Snyder}} ||
|-
| 1987 || {{sortname|Donald |Melrose}} ||
|-
| 1989 || {{sortname|Robert |Delbourgo}} ||
|-
| 1989 || {{sortname|Peter Gavin |Hall}} ||
|-
| 1991 || {{sortname|Bruce H.J. |McKellar}} ||
|-
| 1993 || {{sortname|Neville Horner |Fletcher}} ||
|-
| 1993 || {{sortname|Erich |Weigold}} ||
|-
| 1995 || {{sortname|Chris |Heyde}} || [[Martingale central limit theorem|martingale limit theory]]&lt;ref&gt;{{citation |title=Lyle Medal to Heyde |url=http://www.columbia.edu/cu/record/archives/vol20/vol20_iss30/record2030.33.html |journal=Columbia University Record |date=26 May 1995 |volume=20 |issue=30}}.&lt;/ref&gt;
|-
| 1997 || {{sortname|Anthony W. |Thomas}} || [[quark]]s and [[nucleon]] structure&lt;ref&gt;{{citation |title=Medals awarded at AGM: Lyle Medal, Anthony Thomas |url=http://www.science.org.au/publications/newsletters/Documents/AAS36.PDF |journal=Australian Academy of Science Newsletter |page=4 |volume=36 |date=April–June 1997 |deadurl=yes |archiveurl=https://web.archive.org/web/20110302060511/http://science.org.au/publications/newsletters/documents/AAS36.PDF |archivedate=2 March 2011 |df=dmy-all }}.&lt;/ref&gt;
|-
| 1999 || {{sortname|Ernie |Tuck}} ||
|-
| 2001 || {{sortname|Ian |Sloan}} ||
|-
| 2003 || {{sortname|George |Dracoulis}} || [[nuclear structure]]&lt;ref&gt;{{citation |title=Senior Award Presentations News and Views: Australian Academy of Science 50th Anniversary Annual General Meeting 5–7 May 2004, 2004 |journal=Nuclear Physics News |volume=14 |issue=4 |year=2004 |page=33 |doi=10.1080/10506890491034974}}.&lt;/ref&gt;
|-
| 2005 || {{sortname|Anthony J. |Guttmann}}&lt;ref&gt;{{citation |title=Awards and other achievements |page=136 |volume=32 |issue=2 |year=2005 |journal=Gazette of the Australian Mathematical Society |url=http://www.austms.org.au/Publ/Gazette/2005/May05/May05.pdf}}.&lt;/ref&gt; ||
|-
| 2007 || {{sortname|Yuri |Kivshar}} || [[nonlinear optics]]&lt;ref&gt;[https://www.science.org.au/past-winners/2007-awardees 2007 award citation], [[Australian Academy of Science]], retrieved 2010-06-08.&lt;br&gt;
:Professor Yuri Kivshar, Head, Nonlinear Physics Centre, Research School of Physical Sciences and Engineering, Australian National University
:''Yuri Kivshar is a world leader in nonlinear physics and optics, widely recognised for his contributions to our understanding of self-trapping and energy localisation, pioneering results in the theory of optical solitons and vortices, and the world-first predictions of many important effects in nonlinear physics of periodic photonic structures. Most of his theoretical predictions have been verified and demonstrated experimentally. Yuri is a leading figure in the interchange of ideas between nonlinear optics and atom optics. His research is multidisciplinary in background and focus.&lt;/ref&gt;
|-
| 2009 || {{sortname|Victor V. |Flambaum}} || [[unified field theory]], [[Parity (physics)|parity violations]], [[Physical constant|fundamental constants]]&lt;ref&gt;[https://www.science.org.au/past-winners/2009-awardees 2009 award citation], [[Australian Academy of Science]], retrieved 2010-06-08.&lt;br&gt;
:Professor Victor Flambaum FAA, Scientia Professor and Chair of Theoretical Physics, School of Physics, The University of New South Wales
:''Victor Flambaum has performed pioneering research in the area of the violation of fundamental symmetries and tests of unification theories of elementary particles. With collaborators he developed a new method to perform the most accurate atomic calculations of parity violation. These calculations allowed the standard model of elementary particles to be tested. Recently he proposed new ideas which have led to fresh directions in the search for variations of the fundamental constants of nature, including astrophysics (Big Bang nucleosynthesis, quasar spectra), nuclear physics (nuclear clock), and atomic and molecular spectroscopy (atomic clocks).&lt;/ref&gt;
|-
| 2011 || {{sortname|James Stanislaus |Williams}} || &lt;ref&gt;[https://www.science.org.au/past-winners/2011-awardees 2011 award citation], [[Australian Academy of Science]], retrieved 2011-03-10.&lt;br&gt;
:Professor James Stanislaus Williams FAA, Director, Research School of Physics and Engineering, Australian National University
:''James Williams developed ion implantation processes which are widely used in the microelectronics industry for manufacturing computer chips. He has developed phase change memory technology based on silicon which is expected to play an important role in next generation of high density memory devices. His work on compound semiconductors has made an impact in optoelectronic device technology. He has provided exceptional leadership in materials science in Australia and is highly regarded internationally for his contributions in electronic materials.&lt;/ref&gt;
|-
| 2013 || {{sortname|Cheryl |Praeger}} || &lt;ref&gt;[https://www.science.org.au/past-winners/2013-awardees 2013 award citation], [[Australian Academy of Science]], retrieved 2013-06-04.&lt;br&gt;
:Professor Cheryl Elisabeth Praeger AM FAA, School of Mathematics and Statistics, The University of Western Australia
:''Professor Cheryl Praeger has transformed our understanding of groups acting on large systems, producing new theories, algorithms and designs that have advanced every �field that exploits the symmetry of large systems. Her research has led to significant new directions taken up by mathematicians internationally. Her algorithms have enhanced powerful computer algebra systems which have transformed research and teaching of algebra.''&lt;/ref&gt; 
|-
| 2015 || {{sortname|Michelle |Simmons}} ||
|-
| 2017 || {{sortname|Joss |Bland-Hawthorn}} ||
|}

==References==
{{Reflist|2}}
{{Use dmy dates|date=September 2010}}

{{Australian Academy of Science}}

{{DEFAULTSORT:Lyle Medal}}
[[Category:Mathematics awards]]
[[Category:Physics awards]]
[[Category:Awards established in 1932]]
[[Category:Australian Academy of Science Awards]]
[[Category:Australian awards]]</text>
      <sha1>3nrg3hrirve07jb1ulr1azfhz8f3blb</sha1>
    </revision>
  </page>
  <page>
    <title>Tree-graded space</title>
    <ns>0</ns>
    <id>8234367</id>
    <revision>
      <id>580104822</id>
      <parentid>580103528</parentid>
      <timestamp>2013-11-04T03:43:31Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>[[Category:Trees (topology)]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1214">A [[geodesic]] [[metric space]] &lt;math&gt;X&lt;/math&gt; is called '''''tree-graded space''''', with respect to a collection of connected proper subsets called ''pieces'', if any two distinct pieces [[Intersection (set theory)|intersect]] by at most one point, and every non-trivial simple geodesic [[triangle]] of &lt;math&gt;X&lt;/math&gt; is contained in one of the pieces.

Thus, for pieces of bounded [[diameter]], tree-graded spaces behave like [[real tree]]s in their [[Quasi-isometry|coarse geometry]] (in the sense of [[Mikhail Leonidovich Gromov|Gromov]]) while allowing non-tree-like behavior within the pieces.

Tree-graded spaces were introduced by {{harvtxt|Druţu|Sapir|2005}} in their study of the [[Ultralimit#Asymptotic cones|asymptotic cones]] of [[hyperbolic group]]s.

==References==
*{{citation
 | last1 = Druţu | first1 = Cornelia | author1-link = Cornelia Druţu
 | last2 = Sapir | first2 = Mark
 | doi = 10.1016/j.top.2005.03.003
 | issue = 5
 | journal = Topology
 | mr = 2153979
 | pages = 959–1058
 | title = Tree-graded spaces and asymptotic cones of groups
 | volume = 44
 | year = 2005}}.

{{DEFAULTSORT:Tree-Graded Space}}
[[Category:Metric geometry]]
[[Category:Trees (topology)]]


{{Geometry-stub}}</text>
      <sha1>tv0j8zcgon49jpu3blyscagfhh18obe</sha1>
    </revision>
  </page>
  <page>
    <title>Twistor correspondence</title>
    <ns>0</ns>
    <id>15890221</id>
    <revision>
      <id>577098767</id>
      <parentid>563661850</parentid>
      <timestamp>2013-10-14T07:04:41Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>{{applied-math-stub}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="437">{{unreferenced|date=August 2012}}
In [[mathematical physics]], the '''twistor correspondence''' is a [[natural transformation|natural]] [[isomorphism]] between [[mass]]less [[Yang-Mills field]]s on [[Minkowski space]] and [[sheaf cohomology]] classes on a real [[hypersurface]] of [[complex projective space|'''CP'''&lt;sup&gt;3&lt;/sup&gt;]].

[[Category:Mathematical physics]]


{{Physics-stub}}
{{applied-math-stub}}

{{Topics of twistor theory}}</text>
      <sha1>hn2cssycfdp4l35zahzuvycz6i5zgaq</sha1>
    </revision>
  </page>
  <page>
    <title>Wedderburn–Etherington number</title>
    <ns>0</ns>
    <id>972328</id>
    <revision>
      <id>856587078</id>
      <parentid>846814481</parentid>
      <timestamp>2018-08-26T08:06:30Z</timestamp>
      <contributor>
        <username>Runawayangel</username>
        <id>7340759</id>
      </contributor>
      <minor/>
      <comment>/* Additional reading */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10910">The '''Wedderburn–Etherington numbers''' are an [[integer sequence]] named for [[Ivor Malcolm Haddon Etherington]]&lt;ref name="e37"&gt;{{citation
 | last = Etherington | first = I. M. H. | author-link = Ivor Malcolm Haddon Etherington
 | doi = 10.2307/3605743
 | issue = 242
 | journal = [[Mathematical Gazette]]
 | pages = 36–39, 153
 | title = Non-associate powers and a functional equation
 | volume = 21
 | year = 1937}}.&lt;/ref&gt;&lt;ref name="e39"&gt;{{citation
 | last = Etherington | first = I. M. H. | author-link = Ivor Malcolm Haddon Etherington
 | issue = 2
 | journal = Proc. Royal Soc. Edinburgh
 | pages = 153–162
 | title = On non-associative combinations
 | volume = 59
 | year = 1939}}.&lt;/ref&gt; and [[Joseph Wedderburn]]&lt;ref name="w"&gt;{{citation
 | last = Wedderburn | first = J. H. M. | author-link = Joseph Wedderburn
 | doi = 10.2307/1967710
 | issue = 2
 | journal = [[Annals of Mathematics]]
 | pages = 121–140
 | title = The functional equation &lt;math&gt;g(x^2) = 2ax + [g(x)]^2&lt;/math&gt;
 | volume = 24
 | year = 1923}}.&lt;/ref&gt; that can be used to count certain kinds of [[binary tree]]s. The first few numbers in the sequence are
:0, 1, 1, 1, 2, 3, 6, 11, 23, 46, 98, 207, 451, 983, 2179, 4850, 10905, 24631, 56011, ... ({{OEIS2C|A001190}})

==Combinatorial interpretation==
[[File:Wedderburn-Etherington trees.svg|thumb|360px|Otter trees and weakly binary trees, two types of rooted binary tree counted by the Wedderburn–Etherington numbers]]
These numbers can be used to solve several problems in [[combinatorial enumeration]]. The ''n''th number in the sequence (starting with the number 0 for ''n''&amp;nbsp;=&amp;nbsp;0)
counts
*The number of unordered [[rooted tree]]s with ''n'' leaves in which all nodes including the root have either zero or exactly two children.&lt;ref name="oeis"&gt;{{Cite OEIS|A001190}}.&lt;/ref&gt; These trees have been called [[Otter tree]]s,&lt;ref&gt;{{citation
 | last1 = Bóna | first1 = Miklós | author1-link = Miklós Bóna
 | last2 = Flajolet | first2 = Philippe | author2-link = Philippe Flajolet
 | arxiv = 0901.0696
 | doi = 10.1239/jap/1261670685
 | issue = 4
 | journal = Journal of Applied Probability
 | mr = 2582703
 | pages = 1005–1019
 | title = Isomorphism and symmetries in random phylogenetic trees
 | volume = 46
 | year = 2009}}.&lt;/ref&gt; after the work of [[Richard Otter]] on their combinatorial enumeration.&lt;ref&gt;{{citation
 | last = Otter | first = Richard
 | doi = 10.2307/1969046
 | journal = [[Annals of Mathematics]]
 | mr = 0025715
 | pages = 583–599
 | series = Second Series
 | title = The number of trees
 | volume = 49
 | year = 1948}}.&lt;/ref&gt; They can also be interpreted as unlabeled and unranked [[dendrogram]]s with the given number of leaves.&lt;ref name="dendrogram"&gt;{{citation
 | last = Murtagh | first = Fionn
 | doi = 10.1016/0166-218X(84)90066-0
 | issue = 2
 | journal = Discrete Applied Mathematics
 | mr = 727923
 | pages = 191–199
 | title = Counting dendrograms: a survey
 | volume = 7
 | year = 1984}}.&lt;/ref&gt;
*The number of unordered rooted trees with ''n'' nodes in which the root has degree zero or one and all other nodes have at most two children.&lt;ref name="oeis"/&gt; Trees in which the root has at most one child are called [[planted tree]]s, and the additional condition that the other nodes have at most two children defines the [[weakly binary tree]]s. In [[chemical graph theory]], these trees can be interpreted as [[isomer]]s of [[polyene]]s with a designated leaf atom chosen as the root.&lt;ref&gt;{{citation
 | last1 = Cyvin | first1 = S. J.
 | last2 = Brunvoll | first2 = J.
 | last3 = Cyvin | first3 = B.N.
 | doi = 10.1016/0166-1280(95)04329-6
 | issue = 3
 | journal = Journal of Molecular Structure: THEOCHEM
 | pages = 255–261
 | title = Enumeration of constitutional isomers of polyenes
 | volume = 357
 | year = 1995}}.&lt;/ref&gt;
*The number of different ways of organizing a [[single-elimination tournament]] for ''n'' players (with the player names left blank, prior to seeding players into the tournament).&lt;ref&gt;{{citation
 | last = Maurer | first = Willi
 | journal = The Annals of Statistics
 | jstor = 2958441
 | mr = 0371712
 | pages = 717–727
 | title = On most effective tournament plans with fewer games than competitors
 | volume = 3
 | year = 1975
 | doi = 10.1214/aos/1176343135}}.&lt;/ref&gt; The pairings of such a tournament may be described by an Otter tree.
*The number of different results that could be generated by different ways of grouping the expression &lt;math&gt;x^n&lt;/math&gt; for a binary multiplication operation that is assumed to be [[commutative]] but neither [[associative]] nor [[idempotent]].&lt;ref name="oeis"/&gt; For instance &lt;math&gt;x^5&lt;/math&gt; can be grouped into binary multiplications in three ways, as &lt;math&gt;x(x(x(xx)))&lt;/math&gt;, &lt;math&gt;x((xx)(xx))&lt;/math&gt;, or &lt;math&gt;(xx)(x(xx))&lt;/math&gt;. This was the interpretation originally considered by both Etherington&lt;ref name="e37"/&gt;&lt;ref name="e39"/&gt; and Wedderburn.&lt;ref name="w"/&gt; An Otter tree can be interpreted as a grouped expression in which each leaf node corresponds to one of the copies of &lt;math&gt;x&lt;/math&gt; and each non-leaf node corresponds to a multiplication operation. In the other direction, the set of all Otter trees, with a binary multiplication operation that combines two trees by making them the two subtrees of a new root node, can be interpreted as the free commutative [[Magma (algebra)|magma]] on one generator &lt;math&gt;x&lt;/math&gt; (the tree with one node). In this algebraic structure, each grouping of &lt;math&gt;x^n&lt;/math&gt; has as its value one of the ''n''-leaf Otter trees.&lt;ref&gt;This equivalence between trees and elements of the free commutative magma on one generator is stated to be "well known and easy to see" by {{citation
 | last = Rosenberg | first = I. G.
 | doi = 10.1016/0166-218X(86)90068-5
 | issue = 1
 | journal = Discrete Applied Mathematics
 | mr = 829338
 | pages = 41–59
 | title = Structural rigidity. II. Almost infinitesimally rigid bar frameworks
 | volume = 13
 | year = 1986}}.&lt;/ref&gt;

==Formula==
The Wedderburn–Etherington numbers may be calculated using the [[recurrence relation]]
:&lt;math&gt;a_{2n-1}=\sum_{i=1}^{n-1} a_i a_{2n-i-1}&lt;/math&gt;
:&lt;math&gt;a_{2n}=\frac{a_n(a_n+1)}{2}+\sum_{i=1}^{n-1} a_i  a_{2n-i}&lt;/math&gt;
beginning with the base case &lt;math&gt;a_1=1&lt;/math&gt;.&lt;ref name="oeis"/&gt;

In terms of the interpretation of these numbers as counting rooted binary trees with ''n'' leaves, the summation in the recurrence counts the different ways of partitioning these leaves into two subsets, and of forming a subtree having each subset as its leaves. The formula for even values of ''n'' is slightly more complicated than the formula for odd values in order to avoid double counting trees with the same number of leaves in both subtrees.&lt;ref name="dendrogram"/&gt;

==Growth rate==
The Wedderburn–Etherington numbers grow [[asymptotic analysis|asymptotically]] as
:&lt;math&gt;a_n \approx \sqrt{\frac{\rho+\rho^2B'(\rho^2)}{2\pi}} \frac{\rho^{-n}}{n^{3/2}},&lt;/math&gt;
where ''B'' is the [[generating function]] of the numbers and ''ρ'' is its [[radius of convergence]], approximately 0.4027 {{OEIS|A240943}}, and where the constant given by the part of the expression in the square root is approximately 0.3188 {{OEIS|A245651}}.&lt;ref&gt;{{citation
 | last = Landau | first = B. V.
 | issue = 2
 | journal = Mathematika
 | mr = 0498168
 | pages = 262–265
 | title = An asymptotic expansion for the Wedderburn-Etherington sequence
 | volume = 24
 | year = 1977
 | doi=10.1112/s0025579300009177}}.&lt;/ref&gt;

==Applications==
{{harvtxt|Young|Yung|2003}} use the Wedderburn–Etherington numbers as part of a design for an [[encryption]] system containing a hidden [[Backdoor (computing)|backdoor]]. When an input to be encrypted by their system can be sufficiently [[data compression|compressed]] by [[Huffman coding]], it is replaced by the compressed form together with additional information that leaks key data to the attacker. In this system, the shape of the Huffman coding tree is described as an Otter tree and encoded as a binary number in the interval from 0 to the  Wedderburn–Etherington number for the number of symbols in the code. In this way, the encoding uses a very small number of bits, the base-2 logarithm of the Wedderburn–Etherington number.&lt;ref&gt;{{citation
 | last1 = Young | first1 = Adam
 | last2 = Yung | first2 = Moti | author2-link = Moti Yung
 | contribution = Backdoor attacks on black-box ciphers exploiting low-entropy plaintexts
 | doi = 10.1007/3-540-45067-X_26
 | isbn = 3-540-40515-1
 | pages = 297–311
 | publisher = Springer
 | series = [[Lecture Notes in Computer Science]]
 | title = Proceedings of the 8th Australasian Conference on Information Security and Privacy (ACISP'03)
 | volume = 2727
 | year = 2003}}.&lt;/ref&gt;

{{harvtxt|Farzan|Munro|2008}} describe a similar encoding technique for rooted unordered binary trees, based on partitioning the trees into small subtrees and encoding each subtree as a number bounded by the Wedderburn–Etherington number for its size. Their scheme allows these trees to be encoded in a number of bits that is close to the information-theoretic lower bound (the base-2 logarithm of the Wedderburn–Etherington number) while still allowing constant-time navigation operations within the tree.&lt;ref&gt;{{citation
 | last1 = Farzan | first1 = Arash
 | last2 = Munro | first2 = J. Ian | author2-link = Ian Munro (computer scientist)
 | contribution = A uniform approach towards succinct representation of trees
 | doi = 10.1007/978-3-540-69903-3_17
 | mr = 2497008
 | pages = 173–184
 | publisher = Springer
 | series = Lecture Notes in Computer Science
 | title = Algorithm theory—SWAT 2008
 | volume = 5124
 | year = 2008}}.&lt;/ref&gt;

{{harvtxt|Iserles|Nørsett|1999}} use unordered binary trees, and the fact that the Wedderburn–Etherington numbers are significantly smaller than the numbers that count ordered binary trees, to significantly reduce the number of terms in a series representation of the solution to certain [[differential equation]]s.&lt;ref&gt;{{citation
 | last1 = Iserles | first1 = A.
 | last2 = Nørsett | first2 = S. P.
 | doi = 10.1098/rsta.1999.0362
 | issue = 1754
 | journal = The Royal Society of London
 | mr = 1694700
 | pages = 983–1019
 | title = On the solution of linear differential equations in Lie groups
 | volume = 357
 | year = 1999| bibcode = 1999RSPTA.357..983I
 }}.&lt;/ref&gt;

==See also==
* [[Catalan number]]

==References==
{{reflist}}

== Further reading==
*{{citation
 | last = Finch | first = Steven R.
 | doi = 10.1017/CBO9780511550447
 | isbn = 0-521-81805-2
 | location = Cambridge
 | mr = 2003519
 | pages = 295–316
 | publisher = Cambridge University Press
 | series = Encyclopedia of Mathematics and its Applications
 | title = Mathematical constants
 | volume = 94
 | year = 2003}}.

{{DEFAULTSORT:Wedderburn-Etherington number}}
[[Category:Integer sequences]]
[[Category:Trees (graph theory)]]
[[Category:Graph enumeration]]</text>
      <sha1>8iw3bs6jtbgawsca0k9q9oly5gp72bp</sha1>
    </revision>
  </page>
</mediawiki>
