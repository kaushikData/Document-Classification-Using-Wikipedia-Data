<mediawiki xmlns="http://www.mediawiki.org/xml/export-0.10/" xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" xsi:schemaLocation="http://www.mediawiki.org/xml/export-0.10/ http://www.mediawiki.org/xml/export-0.10.xsd" version="0.10" xml:lang="en">
  <siteinfo>
    <sitename>Wikipedia</sitename>
    <dbname>enwiki</dbname>
    <base>https://en.wikipedia.org/wiki/Main_Page</base>
    <generator>MediaWiki 1.33.0-wmf.6</generator>
    <case>first-letter</case>
    <namespaces>
      <namespace key="-2" case="first-letter">Media</namespace>
      <namespace key="-1" case="first-letter">Special</namespace>
      <namespace key="0" case="first-letter" />
      <namespace key="1" case="first-letter">Talk</namespace>
      <namespace key="2" case="first-letter">User</namespace>
      <namespace key="3" case="first-letter">User talk</namespace>
      <namespace key="4" case="first-letter">Wikipedia</namespace>
      <namespace key="5" case="first-letter">Wikipedia talk</namespace>
      <namespace key="6" case="first-letter">File</namespace>
      <namespace key="7" case="first-letter">File talk</namespace>
      <namespace key="8" case="first-letter">MediaWiki</namespace>
      <namespace key="9" case="first-letter">MediaWiki talk</namespace>
      <namespace key="10" case="first-letter">Template</namespace>
      <namespace key="11" case="first-letter">Template talk</namespace>
      <namespace key="12" case="first-letter">Help</namespace>
      <namespace key="13" case="first-letter">Help talk</namespace>
      <namespace key="14" case="first-letter">Category</namespace>
      <namespace key="15" case="first-letter">Category talk</namespace>
      <namespace key="100" case="first-letter">Portal</namespace>
      <namespace key="101" case="first-letter">Portal talk</namespace>
      <namespace key="108" case="first-letter">Book</namespace>
      <namespace key="109" case="first-letter">Book talk</namespace>
      <namespace key="118" case="first-letter">Draft</namespace>
      <namespace key="119" case="first-letter">Draft talk</namespace>
      <namespace key="710" case="first-letter">TimedText</namespace>
      <namespace key="711" case="first-letter">TimedText talk</namespace>
      <namespace key="828" case="first-letter">Module</namespace>
      <namespace key="829" case="first-letter">Module talk</namespace>
      <namespace key="2300" case="first-letter">Gadget</namespace>
      <namespace key="2301" case="first-letter">Gadget talk</namespace>
      <namespace key="2302" case="case-sensitive">Gadget definition</namespace>
      <namespace key="2303" case="case-sensitive">Gadget definition talk</namespace>
    </namespaces>
  </siteinfo>
  <page>
    <title>136 (number)</title>
    <ns>0</ns>
    <id>1549922</id>
    <revision>
      <id>859261248</id>
      <parentid>810337944</parentid>
      <timestamp>2018-09-12T21:21:02Z</timestamp>
      <contributor>
        <username>Timrollpickering</username>
        <id>32005</id>
      </contributor>
      <minor/>
      <comment>/* top */date tag template, replaced: {{example farm}} → {{example farm|date=September 2018}}</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4516">{{example farm|date=September 2018}}
{{Infobox number
| number = 136
| divisor = 1, 2, 4, 8, 17, 34, 68, 136
}}
'''136''' ('''one hundred [and] thirty six''') is the [[natural number]] following [[135 (number)|135]] and preceding [[137 (number)|137]].

==In mathematics==
136 is itself a factor of the [[Eddington number]]. With a total of 8 divisors, 8 among them, 136 is a [[refactorable number]]. It is a [[composite number]].

136 is a [[triangular number]], a [[centered triangular number]]&lt;ref&gt;{{Cite web|url=https://oeis.org/A005448|title=Sloane's A005448 : Centered triangular numbers|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-27}}&lt;/ref&gt; and a [[centered nonagonal number]].&lt;ref&gt;{{Cite web|url=https://oeis.org/A060544|title=Sloane's A060544 : Centered 9-gonal numbers|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-27}}&lt;/ref&gt;

The sum of the ninth row of [[Lozanić's triangle]] is 136.

136 is a [[self-descriptive number]] in base 4,&lt;ref&gt;{{Cite web|url=https://oeis.org/A108551|title=Sloane's A108551 : Self-descriptive numbers in various bases|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-27}}&lt;/ref&gt; and a [[repdigit]] in [[hexadecimal|base 16]]. In base 10, the sum of the cubes of its digits is &lt;math&gt;1^3 + 3^3 + 6^3 = 244&lt;/math&gt;. The sum of the cubes of the digits of 244 is &lt;math&gt;2^3 + 4^3 + 4^3 = 136&lt;/math&gt;.

136 is the sum of the first 16 positive integers.

==In the military==
* [[Force 136]] branch of the [[United Kingdom|British]] organization, the [[Special Operations Executive]] (SOE), in the [[South-East Asian Theatre of World War II]]
* {{USNS|Mission Soledad|T-AO-136}} was a [[United States Navy]] ''Mission Buenaventura''-class fleet oiler during [[World War II]]
* {{USS|Admirable|AM-136}} was a United States Navy [[Admirable-class minesweeper|her class]] of [[minesweeper]]
* {{USS|Ara|AK-136}} was a United States Navy ''Crater''-class cargo ship during World War II
* {{USS|Boggs|DD-136}} was a United States Navy {{sclass-|Wickes|destroyer}} during World War II
* {{USS|Botetourt|APA-136}} was a United States Navy {{sclass-|Haskell|attack transport}} during World War II and the [[Korean War]]
* {{USS|Carondelet|IX-136}} was a United States Navy [[Tanker (ship)|tanker]] during World War II
* {{USS|Carpellotti|APD-136}} was a United States Navy {{sclass-|Crosley|high-speed transport}} during World War II
* {{USS|Chicago|CA-136}} was a United States Navy [[heavy cruiser]] during World War II
* {{USS|Frederick C. Davis|DE-136}} was a United States Navy {{sclass-|Edsall|destroyer escort}} during World War II
* {{USS|General H. L. Scott|AP-136}} was a United States Navy ''General G. O. Squier''-class transport ship during World War II
* [[VAQ-136|Electronic Attack Squadron 136 (VAQ-136)]] also known as "The Gauntlets" is a United States Navy attack squadron at [[Naval Air Station Atsugi]], [[Japan]]
* [[VFA-136|Strike Fighter Squadron 136 (VFA-136)]] is a United States Navy strike fighter squadron based at [[Naval Air Station Oceana]], [[Virginia]]

==In transportation==
* [[London Buses route 136]] is a [[Transport for London]] contracted bus route in [[London]]

==In TV and radio==
* [[136 kHz]] band is the lowest frequency band [[amateur radio]] operators are allowed to transmit

==In other fields==
* The year [[136|AD 136]] or [[136 BC]]
* 136 AH is a year in the [[Islamic calendar]] that corresponds to [[753]] &amp;ndash; [[754]] [[Common Era|CE]]
* [[136 Austria]] is a [[main belt]] [[asteroid]] discovered in 1874
* [[WR 136]] is a [[Wolf-Rayet star|Wolf-Rayet]] [[red supergiant]] star
* [[136P/Mueller]], or Mueller 3, is a periodic [[comet]] in our [[solar system]]
*[[Mental Health Act 1983#Sections 135 and 136|Section 136]] of the [[Mental Health Act 1983]] (UK law) details removing a [[mentally ill]] person from a public place to a place of safety. It details [[police]] powers and the rights of someone in this position.
* [[Sonnet 136]] by [[William Shakespeare]]

== See also ==
* [[List of highways numbered 136]]
* [[United Nations Security Council Resolution 136]]

==External links==
{{Commons category|136 (number)}}
* [http://www.anthonymcg.com/archives/2008/02/03/one-hundred-and-thirty-six-cats 136 cats] (video)

== References ==
{{Reflist}}
{{Integers|1}}

{{DEFAULTSORT:136 (Number)}}
[[Category:Integers]]</text>
      <sha1>9ydsjqewkmzlpfeyv816hqjwjiss2su</sha1>
    </revision>
  </page>
  <page>
    <title>28 (number)</title>
    <ns>0</ns>
    <id>362213</id>
    <revision>
      <id>871034146</id>
      <parentid>871034113</parentid>
      <timestamp>2018-11-28T14:26:11Z</timestamp>
      <contributor>
        <username>Serols</username>
        <id>9929111</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/5.30.192.185|5.30.192.185]] ([[User talk:5.30.192.185|talk]]) ([[WP:HG|HG]]) (3.4.4)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10924">{{for|the card game|Twenty-eight (card game)}}
{{Infobox number
| number =  28
| divisor = 1, 2, 4, 7, 14, 28
}}
'''28''' ('''twenty-eight''') is the [[natural number]] following [[27 (number)|27]] and preceding [[29 (number)|29]].

==In mathematics==
[[File:Die_Gartenlaube_(1887)_b_320_3.jpg|thumb|7. triangular number]]
It is a [[composite number]], its proper [[divisor]]s being [[1 (number)|1]], [[2 (number)|2]], [[4 (number)|4]], [[7 (number)|7]], and [[14 (number)|14]].

Twenty-eight is the second [[perfect number]]. As a perfect number, it is related to the [[Mersenne prime]] 7, since 2&lt;sup&gt;(3 − 1)&lt;/sup&gt;(2&lt;sup&gt;3&lt;/sup&gt; − 1) = 28. The next perfect number is [[496 (number)|496]], the previous being [[6 (number)|6]].&lt;ref&gt;{{Cite web|url=https://oeis.org/A000396|title=Sloane's A000396 : Perfect numbers|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-31}}&lt;/ref&gt;

Twenty-eight is the sum of the [[totient function]] for the first nine integers.&lt;ref&gt;{{Cite web|url=https://oeis.org/A002088|title=Sloane's A002088 : Sum of totient function|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-31}}&lt;/ref&gt;

Since the greatest [[prime factor]] of 28&lt;sup&gt;2&lt;/sup&gt; + 1 = 785 is 157, which is more than 28 twice, 28 is a [[Størmer number]].&lt;ref&gt;{{Cite web|url=https://oeis.org/A005528|title=Sloane's A005528 : Størmer numbers|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-31}}&lt;/ref&gt;

Twenty-eight is a [[harmonic divisor number]],&lt;ref&gt;{{Cite web|url=https://oeis.org/A001599|title=Sloane's A001599 : Harmonic or Ore numbers|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-31}}&lt;/ref&gt; a [[happy number]],&lt;ref&gt;{{Cite web|url=https://oeis.org/A007770|title=Sloane's A007770 : Happy numbers|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-31}}&lt;/ref&gt; a [[triangular number]],&lt;ref&gt;{{Cite web|url=https://oeis.org/A000217|title=Sloane's A000217 : Triangular numbers|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-31}}&lt;/ref&gt; a [[hexagonal number]],&lt;ref&gt;{{Cite web|url=https://oeis.org/A000384|title=Sloane's A000384 : Hexagonal numbers|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-31}}&lt;/ref&gt; and a [[centered nonagonal number]].&lt;ref&gt;{{Cite web|url=https://oeis.org/A060544|title=Sloane's A060544 : Centered 9-gonal (also known as nonagonal or enneagonal) numbers|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-31}}&lt;/ref&gt;

It appears in the [[Padovan sequence]], preceded by the terms 12, 16, 21 (it is the sum of the first two of these).&lt;ref&gt;{{Cite web|url=https://oeis.org/A000931|title=Sloane's A000931 : Padovan sequence|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-31}}&lt;/ref&gt;

It is also a [[Keith number]], because it recurs in a [[Leonardo of Pisa|Fibonacci]]-like sequence started from its base 10 digits: 2, 8, 10, 18, 28...&lt;ref&gt;{{Cite web|url=https://oeis.org/A007629|title=Sloane's A007629 : Repfigit (REPetitive FIbonacci-like diGIT) numbers (or Keith numbers)|last=|first=|date=|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-05-31}}&lt;/ref&gt;

Twenty-eight is the third positive integer with a prime factorization of the form 2{{sup|2}}''q'' where ''q'' is an [[odd prime]].

Twenty-eight is the ninth and last number in early [[India]]n [[magic square]] of order 3.

There are twenty-eight [[convex uniform honeycomb]]s.

Twenty-eight is the only positive integer that has a unique [[Kayles]] [[nim-value]].

Twenty-eight is the only known number which can be expressed as a sum of the first non negative integers (1 + 2 + 3 + 4 + 5 + 6 + 7), a sum of the first primes (2 + 3 + 5 + 7 + 11) and a sum of the first non primes (1 + 4 + 6 + 8 + 9) and there is probably no other number with this property.&lt;ref&gt;{{cite web|url=http://mathoverflow.net/questions/212985|title=Intersection between the sums of the first positive integers, primes and non primes|author=|date=|website=mathoverflow.net|accessdate=2 April 2018}}&lt;/ref&gt;

There are 28 oriented [[diffeomorphism]] classes of manifolds homeomorphic to the 7-sphere.{{source?|date=September 2016}}

==In science==
* The [[atomic mass]] of [[silicon]].
* The [[atomic number]] of [[nickel]].
* The fourth [[Magic number (physics)|magic number]] in physics.
* The [[concrete#Curing|curing time of concrete]] is classically considered 28 days.
* The average human [[menstrual cycle]] is 28 days although no link has been established with the [[Menstrual cycle#Nightlighting and the moon|nightlighting and the Moon]].

==Astronomy==

* The apparent rotation time of the surface of the [[Sun]] at its equator as viewed from Earth is about 28 days while its core revolves in 33 days.&lt;ref&gt;[http://news.stanford.edu/news/2010/august/sun-082310.html Stober D. (2010) The strange case of solar flares and radioactive elements].&lt;/ref&gt;
* [[Messier object]] [[Messier 28|M28]], a [[visual magnitude|magnitude]] 8.5 [[globular cluster]] in the [[constellation]] [[Sagittarius (constellation)|Sagittarius]].
* The [[New General Catalogue]] [http://www.ngcic.org/ object] [[NGC 28]], an [[elliptical galaxy]] in the constellation [[Phoenix (constellation)|Phoenix]].

==In sports==
* The number of players on the active roster of teams in [[Nippon Professional Baseball]]. However, each team is limited to using 25 players in a given game; before every game, the manager must designate three players who will be ineligible for that game.

==In other fields==
'''Twenty-eight''' is:
* An abbreviation for such years as 1928 and 2028.
* In Hebrew [[Gematria]], ''koakh'' meaning "power", "energy" is a word that corresponds to the number 28.
* The number of Hebrew letters in ''Genesis 1:1'', the first verse of the Bible.
* The number of wheels on a [[Lockheed C-5 Galaxy]].
* In the [[code for international direct dial]] phone calls, +28 is unassigned.
* 028 is the [[ISO 3166-1]] numeric three-digit country code for [[Antigua and Barbuda]].
* The number of days in the shortest [[month]] of the [[Gregorian calendar]], [[February]] (except in [[leap year]]s, when there are twenty-nine). All twelve months of the Gregorian calendar have at least 28 days, regardless of the year.
* The [[Gregorian calendar]] follows a 28-year cycle for the most part, since there are seven days in a week and leap year generally occurs every four years; usually, a calendar from any year is the same as that from 28 years earlier (e.g., 2008 and 1980 or 2009 and 2037). However, that rule holds only when there have been exactly seven leap days in a 28-year interval; years divisible by 100 but not by 400 are common years. Indeed, 1900 (as well as 2100, 2200, etc.) does not use the same calendar as 1872 (2072, 2172, etc., respectively) for the simple reason that 1900 is a common year. In 28 years, any day-of-the-week and date combination occurs exactly four times. February 29 will fall on each day of the week once.
* In Jewish tradition there is a 28-year [[Solar cycle (calendar)|solar cycle]] in which the sun returns to its place in [[Genesis creation narrative|Creation]] every 28 solar years. This is commemorated in April every 28 years with the recitation of ''[[Birkat Hachama]]'', the blessing of the sun. 
* The common name for the [[parrot]] ''[[Australian ringneck|Barnardius zonarius semitorquatus]]'', widely distributed in [[Western Australia]] and [[South Australia]]. Its call sounds like "wenniate".
* The number of letters in the [[Danish alphabet|Danish]] and [[Swedish alphabet|Swedish]] [[alphabet]]s (not counting W), and also in the [[Arabic alphabet|Arabic]] and [[Esperanto orthography|Esperanto]] [[alphabet]]s.
* In neo-Nazi circles, twenty-eight indicates [[Blood and Honour]] (28 = BH - B - second letter of the alphabet and H - the eight letter).
* The number of [[Chinese constellations]], "Xiu" or "mansions" (a literal translation), equivalent to the 12 western [[zodiac]] constellations.
* The number of [[dominoes]] in standard domino sets.
* Deriving from the 29.46 year period of [[Saturn]]'s revolution around the [[Sun]], the 28-year cycle as well as its subdivisions by 14 and 7 are supposed in [[Astrology]] to mark significant turning points or sections in the course of a persons development in life. Thus, the number 28 has special significance in the culture of religious sects such as the [[Kadiri]] and the [[Mevlevi]] dervishes. The 28-beat metric pattern often used in the music compositions accompanying the main part of the [[Mevlevi]] [[sema]] ritual is called the "Devri kebir", meaning the "Big Circle" and is a reference to above astronomical facts about the year and the Saturn year.
* In [[Quebec]], [[François Pérusse]], in one of his best-selling [[Album du peuple]] made a parody of [[Wheel of Fortune (U.S. game show)|Wheel of Fortune]] in which all of the letters picked by the contestant were present 28 times. As a result, 28 became an almost [[Mythical number]] used by many Quebec youths, the phrase "Y'en a 28" (There are 28 [Letters]) became a [[running gag]] still used and recognized more than 15 years later.
* The [[Preludes Op. 28 (Chopin)|Preludes, Opus 28]] consists of  [[Frédéric Chopin]]'s 24 preludes for piano, ordinarily but not necessarily played together in concert.
* The [[postal code]] of the [[Community of Madrid|province of Madrid]], in [[Spain]].
* [[Twenty-eight (card game)|Twenty Eight]] is a popular game played in Kerala, India.
* The number of the French department [[Eure-et-Loir]].
* Approximately the number of grams in an ounce, and used as such in the illegal drug trade.
* The [[UIC Country Code]] for [[Georgia (country)|Georgia]] identifying member countries of the [[International Union of Railways]] (UIC).
* The letter ''[[Q]]'' when encoding the serial number for [[intermodal containers|intermodal (shipping) containers]] as defined by [[ISO 6346]].
* The name of a single on the [[Trilogy (The Weeknd album)|''Trilogy'' by The Weeknd]].
* The number of [[Panfilov's_Twenty-Eight_Guardsmen|Panfilov's Guardsmen]], said to have heroically fallen in combat on 16 November 1941 during the [[Battle of Moscow]], and venerated as Soviet's national heroes.

==See also==
* [[List of highways numbered 28]]

==References==
&lt;references/&gt;

==External links==
* [http://primes.utm.edu/curios/page.php/28.html Prime Curios! 28] from the [[Prime Pages]]

{{Integers|zero}}

[[Category:Integers]]</text>
      <sha1>fcg9k7un8bwq0nwwouw3ih03xvs75c3</sha1>
    </revision>
  </page>
  <page>
    <title>313 (number)</title>
    <ns>0</ns>
    <id>2121038</id>
    <revision>
      <id>843528335</id>
      <parentid>843401221</parentid>
      <timestamp>2018-05-29T18:43:44Z</timestamp>
      <contributor>
        <username>Arthur Rubin</username>
        <id>374195</id>
      </contributor>
      <comment>Reverted [[WP:AGF|good faith]] edits by [[Special:Contributions/2602:306:35AA:710:2DEA:20EE:ADB:6382|2602:306:35AA:710:2DEA:20EE:ADB:6382]] ([[User talk:2602:306:35AA:710:2DEA:20EE:ADB:6382|talk]]): Nonsense. ([[WP:TW|TW]])</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2810">{{Refimprove|date=December 2009}}
{{Infobox number
| number = 313
| prime = twin prime, palindromic prime
}}
'''313''' ('''three hundred [and] thirteen''') is the [[natural number]] following [[312 (number)|312]] and preceding [[314 (number)|314]].

==In mathematics==
'''313''' is:

*a [[prime number]]
*a [[twin prime]] with [[311 (number)|311]]
*a [[centered square number]]&lt;ref&gt;{{Cite web|url=https://oeis.org/A001844|title=Sloane's A001844 : Centered square numbers|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-06-02}}&lt;/ref&gt;
*a [[full reptend prime]] (smallest prime which is full reptend prime in base 10 but not in base 2 to 9)
*a [[pythagorean prime]]
*a [[regular prime]]&lt;ref&gt;{{Cite web|url=https://oeis.org/A007703|title=Sloane's A007703 : Regular primes|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-06-02}}&lt;/ref&gt;
*a [[palindromic prime]] in both [[decimal]] and [[Binary numeral system|binary]].
*a [[truncatable prime]]
*a [[weakly prime]] in base 5
*a [[happy number]]&lt;ref&gt;{{Cite web|url=https://oeis.org/A007770|title=Sloane's A007770 : Happy numbers]|website=The On-Line Encyclopedia of Integer Sequences|publisher=OEIS Foundation|access-date=2016-06-02}}&lt;/ref&gt;
*an [[Narcissistic number|Armstrong number]]  - in base 4 ( 3×4&lt;sup&gt;2&lt;/sup&gt; + 1×4&lt;sup&gt;1&lt;/sup&gt; + 3×4&lt;sup&gt;0&lt;/sup&gt; = 3&lt;sup&gt;3&lt;/sup&gt; + 1&lt;sup&gt;3&lt;/sup&gt; + 3&lt;sup&gt;3&lt;/sup&gt; )

==Religious significance ==
In [[Twelver]] Shia Islam mysticism, 313 is the number of soldiers in the army of the 12th "hidden Imam" ([[Mahdi]]).

==In popular culture==
*The number 313 is the U.S. [[Area code 313|telephone area code]] for the city of [[Detroit, Michigan|Detroit]] and nearby locales.&lt;ref&gt;{{Cite web |url=http://www.nanpa.com/area_code_maps/display.html?mi |title=NANPA Area Codes Map: Michigan |author=North American Numbering Plan Administration |authorlink=North American Numbering Plan |year=2012 |publisher=[[NeuStar]] |work=Nanpa.com |accessdate=19 February 2012 }}&lt;/ref&gt;
*Frame 313 of the [[Zapruder film]] shows the moment of impact for the bullet that killed President John F. Kennedy.&lt;ref&gt;{{Cite book |title=Reclaiming History: The Assassination of President John F. Kennedy |last=Bugliosi |first=Vincent |authorlink=Vincent Bugliosi |year=2007 |publisher=W.W. Norton |location=New York |isbn=978-0-393-04525-3 |page=505 }}&lt;/ref&gt;
*[[Donald Duck]]'s car plate bears the number 313.
*The sixth track of the album [[Get Some (album)]] by [[Snot (band)]] is called 313.
*The fifth track of the album [[Infinite (Eminem album)|Infinite (album)]] by [[Eminem]] is called 313.

==See also==
*The year [[313]] AD
*The year [[313 BC]]

==References==
{{Reflist}}

{{Integers|3}}

{{DEFAULTSORT:313 (Number)}}
[[Category:Integers]]


{{Num-stub}}</text>
      <sha1>6x1i9qmv0yb8w8m76houk65y32i09e6</sha1>
    </revision>
  </page>
  <page>
    <title>Arditi–Ginzburg equations</title>
    <ns>0</ns>
    <id>47950379</id>
    <revision>
      <id>840126346</id>
      <parentid>763850071</parentid>
      <timestamp>2018-05-07T22:10:18Z</timestamp>
      <contributor>
        <username>JCW-CleanerBot</username>
        <id>31737083</id>
      </contributor>
      <minor/>
      <comment>/* top */[[User:JCW-CleanerBot#Logic|task]], replaced: Trends in Ecology → Trends in Ecology &amp; Evolution using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2836">The '''Arditi–Ginzburg equations''' describe [[ratio]] dependent [[Predation|predator–prey]] dynamics. Where ''N'' is the population of a prey species and ''P'' that of a predator, the population dynamics are described by the following two equations:&lt;ref name=":0"&gt;{{Cite web|title = Coupling in predator-prey dynamics: Ratio-Dependence|url = http://ac.els-cdn.com/S0022519389802115/1-s2.0-S0022519389802115-main.pdf?_tid=1565d0da-6588-11e5-a81d-00000aab0f6b&amp;acdnat=1443407265_75f5fff588be891df9d652f2e3d694ff|website = ac.els-cdn.com|accessdate = 2015-09-28}}&lt;/ref&gt;

: &lt;math&gt;
\begin{align}
\frac{dN}{dt} &amp; = f(N)\,N-g{\left(\!\tfrac N P \!\right)}P \\[4pt]
\frac{dP}{dt} &amp; = e \,g{\left(\!\tfrac N P\! \right)}P-uP
\end{align}
&lt;/math&gt;

Here ''f''(''N'') captures any change in the prey population not due to predator activity including inherent [[Birth rate|birth]] and [[Mortality rate|death rates]]. The per capita effect of predators on the prey population (the harvest rate) is modeled by a function ''g'' which is a function of the [[ratio]] ''N''/''P'' of prey to predators. Predators receive a reproductive payoff, ''e,'' for consuming prey, and die at rate ''u''. Making predation pressure a function of the ratio of prey to predators contrasts with the prey dependent [[Lotka–Volterra equations]], where the effect of predators on the prey population is simply a function of the magnitude of the prey population ''g''(''N''). Because the number of prey harvested by each predator decreases as predators become more [[Population density|dense]], ratio dependent predation represents an example of a [[trophic function]]. Ratio dependent predation may account for [[Homogeneity and heterogeneity|heterogeneity]]  in large-scale natural systems in which predator efficiency decreases when prey is scarce.&lt;ref name=":0" /&gt; The merit of ratio dependent versus prey dependent models of predation has been the subject of much controversy, especially between the biologists [[Lev R. Ginzburg]] and Peter A. Abrams.&lt;ref name=":1"&gt;{{cite journal|title = The nature of predation: prey dependent, ratio dependent or neither?|url = http://www.sciencedirect.com/science/article/pii/S016953470001908X||accessdate = 2015-09-28|doi=10.1016/S0169-5347(00)01908-X|volume=15|journal=Trends in Ecology &amp; Evolution|pages=337–341}}&lt;/ref&gt; Ginzburg purports that ratio dependent models more accurately depict predator-prey interactions while Abrams maintains that these models make unwarranted complicating assumptions.&lt;ref name=":1"/&gt;

==See also==
* [[Lotka–Volterra equation]]
* [[Population dynamics]]

==References==
{{reflist}}

{{DEFAULTSORT:Arditi-Ginzburg equations}}
[[Category:Predation]]
[[Category:Ordinary differential equations]]
[[Category:Population models]]
[[Category:Mathematical modeling]]
[[Category:Community ecology]]</text>
      <sha1>jk6kimhw5kl9diuwi5d2ma7547bh480</sha1>
    </revision>
  </page>
  <page>
    <title>Arithmetica</title>
    <ns>0</ns>
    <id>1777555</id>
    <revision>
      <id>861125831</id>
      <parentid>836094509</parentid>
      <timestamp>2018-09-25T08:24:15Z</timestamp>
      <contributor>
        <ip>185.128.125.68</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5480">{{refimprove|date=July 2010}}
{{italic title}}
[[Image:Diophantus-cover.jpg|right|thumb|200px|Cover of the 1621 edition,  into [[Latin]] from [[Greek language|Greek]] by [[Claude Gaspard Bachet de Méziriac]].]]
'''''Arithmetica''''' ({{lang-grc-gre|Ἀριθμητικά}}) is an [[Ancient Greek]] text on [[mathematics]] written by the [[mathematician]] [[Diophantus]] in the 3rd century AD.&lt;ref&gt;{{cite web|title=Diophantus of Alexandria (Greek mathematician)|url=http://www.britannica.com/EBchecked/topic/164347/Diophantus-of-Alexandria#ref704023|publisher=Encyclopædia Britannica|accessdate=11 April 2013}}&lt;/ref&gt;  It is a collection of 130 [[algebra]]ic problems giving numerical solutions of determinate [[equations]] (those with a unique solution) and [[indeterminate equation]]s.

==Summary==
Equations in the book are presently called [[Diophantine equation]]s. The method for solving these equations is known as [[Diophantine analysis]].  Most of the ''Arithmetica'' problems lead to [[quadratic equation]]s.  

In Book 3, Diophantus solves problems of finding values which make two linear expressions simultaneously into squares or cubes. In book 4, he finds rational powers between given numbers. He also noticed that numbers of the form &lt;math&gt;4n + 3&lt;/math&gt; cannot be the sum of two squares. Diophantus also appears to know that every number can be written as the sum of four squares. If he did know this result (in the sense of having proved it as opposed to merely conjectured it), his doing so would be truly remarkable:  even Fermat, who stated the result, failed to provide a proof of it and it was not settled until [[Joseph Louis Lagrange]] proved it using results due to [[Leonhard Euler]].

''Arithmetica'' was originally written in thirteen books, but the Greek manuscripts that survived to the present contain no more than six books.&lt;ref name=magill&gt;{{cite book |year= 1998 |editor-last= Magill |editor-first= Frank N. |title= Dictionary of World Biography |url= https://books.google.com/books?id=_CMl8ziTbKYC&amp;pg=PA362 |volume= 1 |publisher= Salem Press |page= 362 |isbn= 9781135457396 |quote= }}&lt;/ref&gt; In 1968, [[Fuat Sezgin]] found four previously unknown books of ''Arithmetica'' at the shrine of Imam Rezā in the holy Islamic city of [[Mashhad]] in northeastern Iran.&lt;ref&gt;{{cite web|title= Review of J. Sesiano, Books IV to VII of Diophantus' Arithmetica |last= Hogendijk |first= Jan P. |authorlink= Jan Hogendijk |year= 1985 |url= http://www.jphogendijk.nl/reviews/sesiano.html |accessdate= 2014-07-06 |quote= Only six of the thirteen books of the ''Arithmetica'' of Diophantus (ca. A.D. 250) are extant in Greek. The remaining books were believed to be lost, until the recent discovery of a medieval Arabic translation of four of the remaining books in a manuscript in the Shrine Library in Meshed in Iran (see the catalogue [Gulchin-i Ma'ani 1971-1972, pp. 235-236]. The manuscript was discovered in 1968 by F. Sezgin).}}&lt;/ref&gt; The four books are thought to have been translated from Greek to Arabic by [[Qusta ibn Luqa]] (820–912).&lt;ref name=magill/&gt; Norbert Schappacher has written:
&lt;blockquote&gt;
[The four missing books] resurfaced around 1971 in the [[Central Library of Astan Quds Razavi|Astan Quds Library]] in Meshed (Iran) in a copy from 1198 AD.  It was not catalogued under the name of Diophantus (but under that of [[Qusta ibn Luqa]]) because the librarian was apparently not able to read the main line of the cover page where Diophantus’s name appears in geometric [[Kufic|Kufi calligraphy]].&lt;ref&gt;{{cite web|title= Diophantus of Alexandria : a Text and its History |last= Schappacher |first= Norbert |url= http://www-irma.u-strasbg.fr/~schappa/NSch/Publications_files/1998cBis_Dioph.pdf
 |date= April 2005 |page= 18 |accessdate= 2015-10-09}}&lt;/ref&gt;
&lt;/blockquote&gt;

''Arithmetica'' became known to [[Mathematics in medieval Islam|mathematicians in the Islamic world]] in the tenth century&lt;ref&gt;{{cite book|first=Carl B.|last=Boyer|authorlink=Carl Benjamin Boyer|title=A History of Mathematics|edition=Second|publisher=John Wiley &amp; Sons, Inc.|year=1991|chapter=The Arabic Hegemony|isbn=0-471-54397-7|quote=Note the omission of Diophantus and Pappus, authors who evidently were not at first known in Arabia, although the Diophantine ''Arithmetica'' became familiar before the end of the tenth century.|page=234}}&lt;/ref&gt; when [[Abūl Wafā' Būzjānī|Abu'l-Wefa]] translated it into Arabic.&lt;ref&gt;{{cite book|first=Carl B.|last=Boyer|authorlink=Carl Benjamin Boyer|title=A History of Mathematics|edition=Second|publisher=John Wiley &amp; Sons, Inc.|year=1991|chapter=The Arabic Hegemony|isbn=0-471-54397-7|quote=Abu'l-Wefa was a capable algebraist as well as a trigonometer. He commented on al-Khwarizmi's ''Algebra'' and translated from Greek one of the last great classics, the ''Arithmetica'' of Diophantus.|page=239}}&lt;/ref&gt;

==See also==
* [[Muhammad ibn Mūsā al-Khwārizmī]]

==References==
{{reflist|2}}

== External links ==
Diophantus Alexandrinus, Pierre de Fermat, Claude Gaspard Bachet de Meziriac, ''Diophanti Alexandrini Arithmeticorum libri 6, et De numeris multangulis liber unus''. Cum comm. C(laude) G(aspar) Bacheti et observationibus P(ierre) de Fermat. Acc. doctrinae analyticae inventum novum, coll. ex variis eiu. Tolosae 1670, {{DOI|10.3931/e-rara-9423}}.

{{Greek mathematics}}
{{Use dmy dates|date=April 2017}}

[[Category:3rd-century books]]
[[Category:Ancient Greek mathematical works]]
[[Category:History of algebra]]</text>
      <sha1>dmke0qsq61spziv7b6oa2hc3sy93v9v</sha1>
    </revision>
  </page>
  <page>
    <title>Association for Women in Mathematics</title>
    <ns>0</ns>
    <id>9858698</id>
    <revision>
      <id>867354301</id>
      <parentid>864098458</parentid>
      <timestamp>2018-11-05T04:43:32Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>/* See also */ per [[WP:SEEALSO]], rm links already in main text</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8940">{{Infobox organization
| name         = Association for Women in Mathematics
| image        = Association for Women in Mathematics (logo).gif
| image_border =
| size         =
| caption      =
| formation    = 1971
| type         = [[Professional organization]]
| headquarters = [[Baltimore, Maryland]]
| location     =
| membership   = 5200
| language     =
| leader_title = President
| leader_name  = [[Ami Radunskaya]]
| key_people   =
| num_staff    =
| budget       =
| website      = {{url|sites.google.com/site/awmmath/home}}
}}
The '''Association for Women in Mathematics (AWM)''' is a professional society whose mission is to encourage women and girls to study and to have active careers in the mathematical sciences, and to promote equal opportunity for and the equal treatment of women and girls in the mathematical sciences. The AWM was founded in 1971 and incorporated in the state of Massachusetts.  AWM has approximately 5200 members, including over 250 institutional members, such as colleges, universities, institutes, and mathematical societies.  It offers numerous programs and workshops to mentor women and girls in the mathematical sciences.  Much of AWM’s work is supported through federal grants.

==History==
The Association was founded in 1971 as the Association of Women Mathematicians, but the name was changed almost immediately.  As reported in "A Brief History of the Association for Women in Mathematics: The Presidents' Perspectives", by [[Lenore Blum]], "As Judy Green remembers (and [[Chandler Davis]], early AWM friend, concurs): 'The formal idea of women getting together and forming a caucus was first made publicly at a MAG [Mathematics Action Group] meeting in 1971 ... in Atlantic City.  Joanne Darken, then an instructor at Temple University and now at the Community College of Philadelphia, stood up at the meeting and suggested that the women present remain and form a caucus.  I have been able to document six women who remained: me (I was a graduate student at Maryland at the time), Joanne Darken, Mary [W.] Gray (she was already at American University), Diane Laison (then an instructor at Temple), Gloria Olive (a Senior Lecturer at the University of Otago, New Zealand who was visiting the U.S. at the time) and [[Annie Selden]]... It's not absolutely clear what happened next, except that I've personally always thought that Mary was responsible for getting the whole thing organized ....'"&lt;ref&gt;{{cite web|url=http://www.awm-math.org/articles/notices/199107/blum/node2.html#SECTION02010000000000000000 |title=A Brief History of the Association for Women in Mathematics (from Notices): How it was |publisher=Awm-math.org |date= |accessdate=2015-05-28}}&lt;/ref&gt;  [[Mary W. Gray]] was the early organizer, placing an advertisement in the February 1971 Notices of the AMS, and writing the first issue of the ''AWM Newsletter'' that May.  Early goals of the association focused on equal pay for equal work, as well as equal consideration for admission to graduate school and support while there; for faculty appointments at all levels; for promotion and for tenure; for administrative appointments; and for government grants, positions on review and advisory panels and positions in professional organizations. The AWM holds an annual meeting at the [[Joint Mathematics Meetings]].  In 2011 the association initiated a biennial Research Symposium during its 40th anniversary celebration 40 Years and Counting.&lt;ref&gt;{{cite web|title=40 Years and Counting: 2011 is AWM’s 40th Anniversary Year!|url=https://sites.google.com/site/awmmath/awm40events|website=Association for Women in Mathematics|publisher=Association for Women in Mathematics|accessdate=1 January 2017}}&lt;/ref&gt;

==Lectures==
The AWM sponsors three honorary lecture series.&lt;ref name=AWMPrograms&gt;{{cite web |title=AWM Programs |url=https://sites.google.com/site/awmmath/programs |publisher=Association for Women in Mathematics |accessdate=21 August 2015}}&lt;/ref&gt;

* The [[Noether Lecture]]s – honor women who "have made fundamental and sustained contributions to the mathematical sciences". Presented in association with the [[American Mathematical Society]], the lecture is given at the annual [[Joint Mathematics Meetings]].
* The [[AWM/MAA Falconer Lecturer|Falconer Lectures]] – honor women who "have made distinguished contributions to the mathematical sciences or mathematics education. Presented in association with the [[Mathematical Association of America]], the lecture is given at the annual [[MathFest]].
* The Kovalevsky Lectures – honor women who have "made distinguished contributions in applied or computational mathematics". Presented in association with the [[Society for Industrial and Applied Mathematics]] (SIAM), the lecture is given at the SIAM Annual Meeting. The lecture series is named for the mathematician [[Sonia Kovalevsky]].

==Awards==
The AWM sponsors several awards and prizes.&lt;ref name=AWMPrograms /&gt;

* [[Alice T. Schafer]] Prize – given each year "to an undergraduate woman for excellence in mathematics".
* [[Louise Hay Award]] – given each year for "outstanding achievements of a woman in mathematics education".
* [[M. Gweneth Humphreys Award]] – given each year for "outstanding mentorship activities of a woman in the mathematical sciences".
* [[Ruth I. Michler Memorial Prize]] – given each year to a woman recently tenured in mathematics.  The prize funds a semester in residence at [[Cornell University]] without teaching obligations.

Three recently created prizes for early-career women are also sponsored by the AWM.&lt;ref name=AWMPrograms /&gt;

* AWM-Birman Research Prize – given every other year beginning in 2015 for "exceptional research in topology/geometry".
* AWM-Microsoft Research Prize – given every other year beginning in 2014 for "exceptional research in algebra/number theory".
* [[Sadosky Prize|AWM-Sadosky Research Prize]] – given every other year beginning in 2014 for "exceptional research in analysis".

The AWM Fellows program recognizes "individuals who have demonstrated a sustained commitment to the support and advancement of women in the mathematical sciences".&lt;ref&gt;{{cite web|title=Launch of the AWM Fellows Program|url=https://sites.google.com/site/awmmath/awm-fellows|publisher=Association for Women in Mathematics|accessdate=12 January 2018}}&lt;/ref&gt;

==Past presidents==
* [[Mary W. Gray]], 1971&amp;ndash;1973
* [[Alice T. Schafer]], 1973&amp;ndash;1975
* [[Lenore Blum]], 1975&amp;ndash;1979
* [[Judith Roitman]], 1979&amp;ndash;1981
* [[Bhama Srinivasan]], 1981&amp;ndash;1983
* [[Linda Preiss Rothschild]], 1983&amp;ndash;1985
* [[Linda Keen]], 1985&amp;ndash;1987
* [[Rhonda Hughes]], 1987&amp;ndash;1989
* [[Jill P. Mesirov]], 1989&amp;ndash;1991
* [[Carol S. Wood]], 1991&amp;ndash;1993
* [[Cora Sadosky]], 1993&amp;ndash;1995
* [[Chuu-Lian Terng]], 1995&amp;ndash;1997
* [[Sylvia Wiegand|Sylvia M. Wiegand]], 1997&amp;ndash;1999
* [[Jean Taylor|Jean E. Taylor]], 1999&amp;ndash;2001
* [[Suzanne Lenhart]], 2001&amp;ndash;2003
* [[Carolyn S. Gordon]], 2003&amp;ndash;2005
* [[Barbara Keyfitz]], 2005&amp;ndash;2007
* [[Cathy Kessel]], 2007&amp;ndash;2009
* [[Georgia Benkart]], 2009&amp;ndash;2011
* [[Jill Pipher]], 2011&amp;ndash;2013
* [[Ruth Charney]], 2013&amp;ndash;2015
* [[Kristin Lauter]], 2015&amp;ndash;2017
* [[Ami Radunskaya]], 2017&amp;ndash;2019
* [[Ruth Haas]], 2019&amp;ndash;2021

==See also==
* [[List of women in mathematics]]

==References==
{{Reflist}}

==Further reading==
* {{cite journal |last=Blum |first=Leonore |title= A Brief History of the Association for Women in Mathematics: The Presidents’ Perspectives |url=http://www.awm-math.org/articles/notices/199107/blum/ |journal=Notices of the American Mathematical Society |date=September 1991 |volume=38 |issue=7 |pages=738–774}}
* {{cite journal |last1=Taylor |first1=Jean E. |author2=Sylvia M. Wiegand |title=AWM in the 1990s: A Recent History of the Association for Women in Mathematics |url=http://www.ams.org/notices/199901/awm.pdf |journal=Notices of the American Mathematical Society |date=January 1999 |volume=46 |issue=1 |pages=27–38}} An expanded version appeared [http://www.awm-math.org/articles/199812/awm1990s/index.html in parts in the ''AWM Newsletter'']
* {{cite journal |last1=Greenwald |first1=Sarah J. |author2=Anne M. Leggett, and Jill E. Thomley |title=The Association for Women in Mathematics: How and Why It Was Founded, and Why It’s Still Needed in the 21st Century |journal=[[The Mathematical Intelligencer]] |date=July 2015 |pages=1–11 |doi=10.1007/s00283-015-9539-8}}

==External links==
* {{official website|sites.google.com/site/awmmath/home}}

[[Category:Mathematical societies]]
[[Category:Organizations established in 1971]]
[[Category:Women in mathematics]]
[[Category:Organizations for women in science and technology]]
[[Category:Association for Women in Mathematics]]
[[Category:Awards and prizes of the Association for Women in Mathematics]]
[[Category:Fellows of the Association for Women in Mathematics]]</text>
      <sha1>ql8mxhrs3au29pe26b0vj63pa8eyuxv</sha1>
    </revision>
  </page>
  <page>
    <title>Bigram</title>
    <ns>0</ns>
    <id>1064587</id>
    <revision>
      <id>863240303</id>
      <parentid>849830170</parentid>
      <timestamp>2018-10-09T15:32:48Z</timestamp>
      <contributor>
        <username>Yangfl</username>
        <id>12172223</id>
      </contributor>
      <minor/>
      <comment>/* Applications */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4079">A '''bigram''' or '''digram''' is a sequence of two adjacent elements from a [[string (computer science)|string]] of [[Token (parser)|tokens]], which are typically letters, syllables, or words. A bigram is an [[n-gram|''n''-gram]] for ''n''=2. The frequency distribution of every bigram in a string is commonly used for simple statistical analysis of text in many applications, including in computational linguistics, cryptography, speech recognition, and so on.

''Gappy bigrams'' or ''skipping bigrams'' are word pairs which allow gaps (perhaps avoiding connecting words, or allowing some simulation of dependencies, as in a [[dependency grammar]]).

''Head word bigrams'' are gappy bigrams with an explicit dependency relationship.

Bigrams help provide the conditional probability of a token given the preceding token, when the relation of the [[conditional probability]] is applied:

&lt;math&gt; P(W_n|W_{n-1}) = { P(W_{n-1},W_n) \over P(W_{n-1}) } &lt;/math&gt;

That is, the probability &lt;math&gt; P() &lt;/math&gt; of a token &lt;math&gt;W_n&lt;/math&gt; given the preceding token &lt;math&gt;W_{n-1}&lt;/math&gt; is equal to the probability of their bigram, or the co-occurrence of the two tokens &lt;math&gt;P(W_{n-1},W_n)&lt;/math&gt;, divided by the probability of the preceding token.

==Applications==

Bigrams are used in most successful [[language model]]s for [[speech recognition]].&lt;ref&gt;{{cite journal |last1=Collins |first1=Michael John |title=A new statistical parser based on bigram lexical dependencies |date=1996-06-24 |pages=184–191 |doi=10.3115/981863.981888 |url=http://www.aclweb.org/anthology/P96-1025 |accessdate=2018-10-09 |publisher=Association for Computational Linguistics}}&lt;/ref&gt; They are a special case of [[N-gram]].

Bigram frequency attacks can be used in [[cryptography]] to solve [[cryptograms]]. See [[Frequency analysis (cryptanalysis)|frequency analysis]].

Bigram frequency is one approach to [[Language detection#Statistical approaches|statistical language identification]].

Some activities in [[logology]] or recreational linguistics involve bigrams. These include attempts to find English words beginning with every possible bigram,&lt;ref&gt;{{cite journal|last=Cohen|first=Philip M.|year=1975|title=Initial Bigrams|journal=Word Ways|volume=8|issue=2|url=http://digitalcommons.butler.edu/wordways/vol8/iss2/8 |accessdate=11 September 2016}}&lt;/ref&gt; or words containing a string of repeated bigrams, such as ''logogogue''.&lt;ref&gt;{{cite journal|last=Corbin|first=Kyle|year=1989|title=Double, Triple, and Quadruple Bigrams|journal=Word Ways|volume=22|issue=3|url=http://digitalcommons.butler.edu/wordways/vol22/iss3/8 |accessdate=11 September 2016}}&lt;/ref&gt;

==Bigram frequency in the English language==
The frequency of the most common letter bigrams in a small English corpus is:&lt;ref&gt;[http://www.math.cornell.edu/~mec/2003-2004/cryptography/subs/digraphs.html Cornell Math Explorer's Project &amp;ndash; Substitution Ciphers]&lt;/ref&gt;

 th 1.52       en 0.55       ng 0.18
 he 1.28       ed 0.53       of 0.16
 in 0.94       to 0.52       al 0.09
 er 0.94       it 0.50       de 0.09
 an 0.82       ou 0.50       se 0.08
 re 0.68       ea 0.47       le 0.08
 nd 0.63       hi 0.46       sa 0.06
 at 0.59       is 0.46       si 0.05
 on 0.57       or 0.43       ar 0.04
 nt 0.56       ti 0.34       ve 0.04
 ha 0.56       as 0.33       ra 0.04
 es 0.56       te 0.27       ld 0.02
 st 0.55       et 0.19       ur 0.02
Complete bigram frequencies for a larger corpus are available.&lt;ref&gt;{{Cite journal | issn = 0743-3808 | volume = 36 | issue = 3 | pages = 388–396| last = Jones| first = Michael N|author2=D J K Mewhort| title = Case-sensitive letter and bigram frequency counts from large-scale English corpora| journal = Behavior Research Methods, Instruments, and Computers | date = August 2004 | pmid=15641428}}&lt;/ref&gt;


==See also==
* [[Digraph (orthography)]]
* [[N-gram]]
* [[Letter frequency]]
* [[Sørensen–Dice coefficient]]

==References==
&lt;references/&gt;

{{Natural Language Processing}}

[[Category:Formal languages]]
[[Category:Classical cryptography]]
[[Category:Natural language processing]]</text>
      <sha1>8ygc5fuwmik4z8zrn11ycmxoispri72</sha1>
    </revision>
  </page>
  <page>
    <title>Bracket polynomial</title>
    <ns>0</ns>
    <id>3774360</id>
    <revision>
      <id>607500646</id>
      <parentid>556238198</parentid>
      <timestamp>2014-05-07T16:44:04Z</timestamp>
      <contributor>
        <username>Mabulak</username>
        <id>20760836</id>
      </contributor>
      <minor/>
      <comment>added some &lt;math&gt;</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1760">In the [[mathematics|mathematical]] field of [[knot theory]], the '''bracket polynomial''' (also known as the '''Kauffman bracket''') is a [[polynomial]] invariant of [[framed link]]s.  Although it is not an invariant of knots or links (as it is not invariant under type I [[Reidemeister move]]s), a suitably "normalized" version yields the famous [[knot invariant]] called the [[Jones polynomial]].  The bracket polynomial plays an important role in unifying the Jones polynomial with other [[quantum invariant]]s.  In particular, Kauffman's interpretation of the Jones polynomial allows generalization to invariants of [[3-manifold]]s.  

The bracket polynomial was discovered by [[Louis Kauffman]] in 1987.

==Definition==
The bracket polynomial of any (unoriented) link diagram &lt;math&gt;L&lt;/math&gt;, denoted &lt;math&gt; \langle L \rangle &lt;/math&gt;, is a polynomial in the variable &lt;math&gt;A&lt;/math&gt;, characterized by the three rules:

*  &lt;math&gt; \langle O \rangle = 1 &lt;/math&gt;, where &lt;math&gt; O &lt;/math&gt; is the standard diagram of the unknot
*  [[Image:kauffman_bracket2.png|275px]]
* &lt;math&gt; \langle O \cup L \rangle = (-A^2 - A^{-2}) \langle L \rangle &lt;/math&gt;

The pictures in the second rule represent brackets of the link diagrams which differ inside a disc as shown but are identical outside.  The third rule means that adding a circle disjoint from the rest of the diagram multiplies the bracket of the remaining diagram by &lt;math&gt; -A^2 - A^{-2} &lt;/math&gt;.

==Further reading==
*Louis H. Kauffman, ''State models and the Jones polynomial.'' Topology 26 (1987), no. 3, 395--407.  (introduces the bracket polynomial)

==External links==
*{{MathWorld|BracketPolynomial|Bracket Polynomial}}

{{Knot theory}}

[[Category:Knot theory]]
[[Category:Polynomials]]

{{knottheory-stub}}</text>
      <sha1>99gnptxavd17oew8rond99e6ew47r11</sha1>
    </revision>
  </page>
  <page>
    <title>Cayley–Dickson construction</title>
    <ns>0</ns>
    <id>175609</id>
    <revision>
      <id>870511313</id>
      <parentid>853055110</parentid>
      <timestamp>2018-11-25T08:37:17Z</timestamp>
      <contributor>
        <username>AndriusKulikauskas</username>
        <id>933997</id>
      </contributor>
      <comment>/* Quaternions */  nonzero element</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="14173">In [[mathematics]], the '''Cayley–Dickson construction''', named after [[Arthur Cayley]] and [[Leonard Eugene Dickson]], produces a sequence of [[algebra over a field|algebras]] over the [[field (mathematics)|field]] of [[real number]]s, each with twice the [[dimension of a vector space|dimension]] of the previous one. The algebras produced by this process are known as '''Cayley–Dickson algebras''', for example [[complex number]]s, [[quaternion]]s, and [[octonion]]s. These examples are useful [[composition algebra]]s frequently applied in [[mathematical physics]].

The Cayley–Dickson construction defines a new algebra similar to the [[Direct sum of algebras|direct sum]] of an algebra with itself, with [[multiplication]] defined in a specific way (different from the multiplication provided by the genuine direct sum) and an [[involution (mathematics)|involution]] known as '''conjugation'''.  The product of an element and its [[complex conjugate|conjugate]] (or sometimes the square root of this product) is called the [[norm (mathematics)|norm]].

The symmetries of the real field disappear as the Cayley–Dickson construction is repeatedly applied: first losing [[ordered field|order]], then [[commutativity]] of multiplication, [[associativity]] of multiplication, and next [[alternativity]].

More generally, the Cayley–Dickson construction takes any algebra with involution to another algebra with involution of twice the dimension.&lt;ref name=Sch66&gt;{{citation | first=Richard D. | last=Schafer | year=1995 | origyear=1966 | zbl=0145.25601 | title=An introduction to non-associative algebras | publisher=[[Dover Publications]] | isbn=0-486-68813-5 }}&lt;/ref&gt;{{rp|45}}

{| class="wikitable floatright"
|+ Cayley–Dickson algebras properties
|-
! rowspan=2|[[Algebra over a field|Algebra]]
! rowspan=2|[[Dimension of a vector space|Dimen‐&lt;br&gt;sion]] !! rowspan=2|[[Ordered field|Ordered]] !! colspan=4|[[Multiplication]] properties !! rowspan=2|Nontriv.&lt;br&gt;[[zero divisor|zero&lt;br&gt;divisors]]
|-
! [[Commutative|Commu‐&lt;br&gt;tative]] !! [[Associative|Associ‐&lt;br&gt;ative]] !! [[Alternative algebra|Alter‐&lt;br&gt;native]] !! [[Power associativity|Power-&lt;br&gt;assoc.]]
|-
! [[Real number|Real&amp;nbsp;numbers]]
| style="text-align:center" | 1 || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{No}}
|-
! [[Complex number|Complex&amp;nbsp;num.]]
| style="text-align:center" | 2 || {{No}} || {{Yes}} || {{Yes}} || {{Yes}} || {{Yes}} || {{No}}
|-
! [[Quaternions]]
| style="text-align:center" | 4 || {{No}} || {{No}} || {{Yes}} || {{Yes}} || {{Yes}} || {{No}}
|-
! [[Octonions]]
| style="text-align:center" | 8 || {{No}} || {{No}} || {{No}} || {{Yes}} || {{Yes}} || {{No}}
|-
! [[Sedenions]]
| style="text-align:center" | 16 || rowspan=2 {{No}} || rowspan=2 {{No}} || rowspan=2 {{No}} || rowspan=2 {{No}} || rowspan=2 {{Yes}} || rowspan=2 {{Yes}}
|-
!
| style="text-align:center" | &amp;gt; 16
|}

{{TOC left}}
{{Clear}}

== Complex numbers as ordered pairs ==
{{main|Complex number}}
The [[complex numbers]] can be written as [[ordered pair]]s (''a'',&amp;nbsp;''b'') of [[real number]]s ''a'' and ''b'', with the addition operator being component-by-component and with multiplication defined by

: &lt;math&gt;(a, b) (c, d) = (a c - b d, a d + b c).\,&lt;/math&gt;

A complex number whose second component is zero is associated with a real number: the complex number (''a'',&amp;nbsp;0) is the real number&amp;nbsp;''a''.

The [[complex conjugate]] (''a'',&amp;nbsp;''b'')* of (''a'',&amp;nbsp;''b'') is given by

: &lt;math&gt;(a, b)^* = (a^*, -b) = (a, -b) &lt;/math&gt; since ''a'' is a real number and its conjugate is just ''a''.

The conjugate has the property that

: &lt;math&gt;(a, b)^* (a, b)
  = (a a + b b, a b - b a) = (a^2 + b^2, 0),\,&lt;/math&gt;

which is a non-negative real number.  In this way, conjugation defines a ''[[norm (mathematics)|norm]]'', making the complex numbers a [[normed vector space]] over the real numbers:  the norm of a complex number&amp;nbsp;''z'' is

: &lt;math&gt;|z| = (z^* z)^{1/2}.\,&lt;/math&gt;

Furthermore, for any non-zero complex number&amp;nbsp;''z'', conjugation gives a [[inverse element|multiplicative inverse]],

: &lt;math&gt;z^{-1} = {z^* / |z|^2}.\,&lt;/math&gt;

As a complex number consists of two independent real numbers, they form a 2-dimensional [[vector space]] over the real numbers.

Besides being of higher dimension, the complex numbers can be said to lack one algebraic property of the real numbers: a real number is its own conjugate.

== Quaternions ==
{{main|Quaternion}}
The next step in the construction is to generalize the multiplication and conjugation operations.

Form ordered pairs &lt;math&gt;(a, b)&lt;/math&gt; of complex numbers &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt;, with multiplication defined by

: &lt;math&gt;(a, b) (c, d)
  = (a c - d^* b, d a + b c^*).\,&lt;/math&gt;

Slight variations on this formula are possible; the resulting constructions will yield structures identical up to the signs of bases.

The order of the factors seems odd now, but will be important in the next step.

Define the conjugate &lt;math&gt;(a, b)^*\,&lt;/math&gt; of &lt;math&gt;(a, b)&lt;/math&gt; by

: &lt;math&gt;(a, b)^* = (a^*, -b).\,&lt;/math&gt;

These operators are direct extensions of their complex analogs:  if &lt;math&gt;a&lt;/math&gt; and &lt;math&gt;b&lt;/math&gt; are taken from the real subset of complex numbers, the appearance of the conjugate in the formulas has no effect, so the operators are the same as those for the complex numbers.

The product of a nonzero element with its conjugate is a non-negative real number:

: &lt;math&gt;(a, b)^* (a, b)
  = (a^*, -b) (a, b)
  = (a^* a + b^* b, b a^* - b a^*)
  = (|a|^2 + |b|^2, 0 ).\,&lt;/math&gt;

As before, the conjugate thus yields a norm and an inverse for any such ordered pair.  So in the sense we explained above, these pairs constitute an algebra something like the real numbers.  They are the [[quaternions]], named by [[William Rowan Hamilton|Hamilton]] in 1843.

As a quaternion consists of two independent complex numbers, they form a 4-dimensional vector space over the real numbers.

The multiplication of quaternions is not quite like the multiplication of real numbers, though. It is not [[commutative]], that is, if &lt;math&gt;p&lt;/math&gt; and &lt;math&gt;q&lt;/math&gt; are quaternions, it is not always true that &lt;math&gt;p q = q p&lt;/math&gt;, but it is true that &lt;math&gt;p q = (q p)'&lt;/math&gt;, where &lt;math&gt;(a, b)' = (a, -b)&lt;/math&gt;.

== Octonions ==
{{main|Octonion}}
All the steps to create further algebras are the same from octonions on.

This time, form ordered pairs &lt;math&gt;(p, q)&lt;/math&gt; of
quaternions &lt;math&gt;p&lt;/math&gt; and &lt;math&gt;q&lt;/math&gt;, with multiplication and conjugation defined exactly as for the quaternions:
: &lt;math&gt;(p, q) (r, s)
  = (p r - s^* q, s p + q r^*).\,&lt;/math&gt;

Note, however, that because the quaternions are not commutative, the order of the factors in the multiplication formula becomes important—if the last factor in the multiplication formula were &lt;math&gt;r^*q&lt;/math&gt; rather than
&lt;math&gt;qr^*&lt;/math&gt;, the formula for multiplication of an element by its conjugate would not yield a real number.

For exactly the same reasons as before, the conjugation operator yields a norm and a multiplicative inverse of any nonzero element.

This algebra was discovered by [[John T. Graves]] in 1843, and is called the [[octonions]] or the "[[Arthur Cayley|Cayley]] numbers".

As an octonion consists of two independent quaternions, they form an 8-dimensional vector space over the real numbers.

The multiplication of octonions is even stranger than that of quaternions.  Besides being non-commutative, it is not [[associative]]: that is, if &lt;math&gt;p&lt;/math&gt;, &lt;math&gt;q&lt;/math&gt;, and &lt;math&gt;r&lt;/math&gt; are octonions, it is not always true that
:&lt;math&gt;(p q) r = p (q r).\ &lt;/math&gt;

For the reason of this non-associativity, octonions have [[Octonion#Properties|no matrix representation]].

== Further algebras ==

The algebra immediately following the octonions is called the [[sedenion]]s.  It retains an algebraic property called [[power associativity]], meaning that if &lt;math&gt;s&lt;/math&gt; is a sedenion, &lt;math&gt;s^n s^m = s^{n + m}&lt;/math&gt;, but loses the property of being an [[alternative algebra]] and hence cannot be a [[composition algebra]].

The Cayley–Dickson construction can be carried on ''[[ad infinitum]]'', at each step producing a power-associative algebra whose dimension is double that of the algebra of the preceding step.  All the algebras generated in this way over a field are ''quadratic'': that is, each element satisfies a quadratic equation with coefficients from the field.&lt;ref name=Sch66/&gt;{{rp|50}}

In 1954 R. D. Schafer examined the algebras generated by the Cayley-Dickson process over a field ''F'' and showed they satisfy the [[flexible identity]].&lt;ref&gt;Richard D. Schafer (1954) "On the algebras formed by the Cayley-Dickson process", [[American Journal of Mathematics]] 76: 435–46 {{doi|10.2307/2372583}}&lt;/ref&gt; He also proved that any [[derivation algebra]] of a Cayley-Dickson algebra is isomorphic to the derivation algebra of Cayley numbers, a 14-dimensional [[Lie algebra]] over ''F''.{{Citation needed|date=August 2018}}

== Modified Cayley–Dickson construction ==
The Cayley–Dickson construction, starting from the real numbers ℝ, generates [[division algebra|division]] composition algebras. There are also composition algebras with [[isotropic quadratic form]]s that are obtained through a slight modification, by replacing the minus sign in the definition of the product of ordered pairs with a plus sign, as follows:

: &lt;math&gt;(a, b) (c, d)
  = (a c + d^* b, d a + b c^*).\,&lt;/math&gt;

When this modified construction is applied to ℝ, one obtains the [[split-complex number]]s, which are [[ring isomorphism|ring-isomorphic]] to the direct sum ℝ ⊕ ℝ (also written &lt;sup&gt;2&lt;/sup&gt;ℝ); following that, one obtains the [[split-quaternion]]s, isomorphic to '''M'''&lt;sub&gt;2&lt;/sub&gt;(ℝ); and the [[split-octonion]]s, which are isomorphic to Zorn(ℝ). Applying the original Cayley–Dickson construction to the split-complexes also results in the split-quaternions and then the split-octonions.&lt;ref&gt;Kevin McCrimmon (2004) ''A Taste of Jordan Algebras'', pp 64, Universitext, Springer {{ISBN|0-387-95447-3}} {{mr|id=2014924}}&lt;/ref&gt;

== General Cayley–Dickson construction ==
{{harvtxt|Albert|1942|p= 171}} gave a slight generalization, defining the product and involution on ''B''=''A''⊕''A'' for ''A'' an [[*-algebra|algebra with involution]] (with (''xy'')&lt;sup&gt;*&lt;/sup&gt; = ''y''&lt;sup&gt;*&lt;/sup&gt;''x''&lt;sup&gt;*&lt;/sup&gt;) to be 
: &lt;math&gt;(p, q) (r, s)
  = (p r - \gamma s^* q, s p + q r^*)\,&lt;/math&gt;
:&lt;math&gt;(p, q)^* = (p^*, -q)\ &lt;/math&gt;
for  γ an additive map that commutes with * and left and right multiplication by any element. (Over the reals all choices of γ are equivalent to &amp;minus;1, 0 or 1.) In this construction, ''A'' is an algebra with involution, meaning:
*''A'' is an abelian group under +
*''A'' has a product that is left and right distributive over +
*''A'' has an involution *, with ''x''** = ''x'', (''x''&amp;nbsp;+&amp;nbsp;''y'')* = ''x''*&amp;nbsp;+&amp;nbsp;''y''*,  (''xy'')* &amp;nbsp;=&amp;nbsp;''y''*''x''*.
The algebra ''B''=''A''⊕''A'' produced by the Cayley–Dickson construction is also an algebra with involution.

''B'' inherits properties from ''A'' unchanged as follows. 
*If ''A'' has an identity 1&lt;sub&gt;''A''&lt;/sub&gt;, then ''B'' has an identity (1&lt;sub&gt;''A''&lt;/sub&gt;, 0).
*If ''A'' has the property that ''x''&amp;nbsp;+&amp;nbsp;''x''&lt;sup&gt;*&lt;/sup&gt;, ''xx''&lt;sup&gt;*&lt;/sup&gt; associate and commute with all elements, then so does ''B''. This property implies that any element generates a commutative associative *-algebra, so in particular the algebra is power associative.
 
Other properties of ''A'' only induce weaker properties of ''B'':
*If ''A'' is commutative and has trivial involution, then ''B'' is commutative.
*If ''A'' is commutative and associative then ''B'' is associative.
*If ''A'' is associative and ''x''&amp;nbsp;+&amp;nbsp;''x''&lt;sup&gt;*&lt;/sup&gt;, ''xx''&lt;sup&gt;*&lt;/sup&gt; associate and commute with everything, then ''B'' is an [[alternative algebra]].

== Notes ==
{{reflist}}

== References ==
*{{Citation | last1=Albert | first1=A. A. | author1-link=Abraham Adrian Albert | title=Quadratic forms permitting composition | jstor=1968887 | mr=0006140  | year=1942 | journal=[[Annals of Mathematics]] |series=Second Series | issn=0003-486X | volume=43 | pages=161–177 | doi=10.2307/1968887 | issue=1}} (see p.&amp;nbsp;171)
* {{Citation | last1=Baez | first1=John | author1-link=John Baez | title=The Octonions | url=http://math.ucr.edu/home/baez/octonions/octonions.html | year=2002 | journal=[[Bulletin of the American Mathematical Society]] | issn=0002-9904 | volume=39 | pages=145–205 | doi=10.1090/S0273-0979-01-00934-X | issue=2| arxiv=math/0105155 }}. ''(See "[http://math.ucr.edu/home/baez/octonions/node5.html Section 2.2, The Cayley-Dickson Construction]")''
*{{Citation | last1=Dickson | first1=L. E. | author1-link=Leonard Dickson | title=On Quaternions and Their Generalization and the History of the Eight Square Theorem | jstor=1967865 | publisher=Annals of Mathematics | series=Second Series | year=1919 | journal=[[Annals of Mathematics]] | issn=0003-486X | volume=20 | issue=3 | pages=155–171 | doi=10.2307/1967865}}
* {{Citation | last1=Kantor | first1=I. L. | last2=Solodownikow | first2=A. S. | title=Hyperkomplexe Zahlen | publisher=B.G. Teubner | location=Leipzig | year=1978}}
* {{Citation | last1=Hamilton | first1=William Rowan | author1-link=William Rowan Hamilton | title=On Quaternions | url=http://www.maths.tcd.ie/pub/HistMath/People/Hamilton/Quatern2/Quatern2.html | year=1847 | journal=Proceedings of the Royal Irish Academy | issn=1393-7197 | volume=3 | pages=1–16}}
* Guy Roos (2008) "Exceptional symmetric domains", §1: Cayley algebras, in ''Symmetries in Complex Analysis'' by Bruce Gilligan &amp; Guy Roos, volume 468 of ''Contemporary Mathematics'', [[American Mathematical Society]], {{ISBN|978-0-8218-4459-5}}.

==Further reading==
* Jamil Daboul &amp; Robert Delbourgo (1999) "Matrix representations of octonions and generalizations", [[Journal of Mathematical Physics]] 40: 3144 {{doi|10.1063/1.532950}}


{{Number systems}}

{{DEFAULTSORT:Cayley-Dickson construction}}
[[Category:Composition algebras]]
[[Category:Historical treatment of quaternions]]</text>
      <sha1>r611h9cxna4bg2ur9h1pnn5y6qvzrnd</sha1>
    </revision>
  </page>
  <page>
    <title>Chinese Whispers (clustering method)</title>
    <ns>0</ns>
    <id>46877898</id>
    <revision>
      <id>869516516</id>
      <parentid>851784722</parentid>
      <timestamp>2018-11-19T03:10:52Z</timestamp>
      <contributor>
        <username>BD2412</username>
        <id>196446</id>
      </contributor>
      <minor/>
      <comment>/* Applications */Fixing [[Wikipedia:Disambiguation pages with links|links to disambiguation pages]], replaced: [[open source]] program → [[Open-source software|open source]] program</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4554">{{Orphan|date=June 2015}}

'''Chinese Whispers''' is a clustering method used in network science named after the famous [[Chinese whispers|whispering game]].&lt;ref name= "Biemann2006"&gt;Chris Biemann,[http://wortschatz.uni-leipzig.de/~cbiemann/pub/2006/BiemannTextGraph06.pdf "Chinese Whispers- an Efficient Graph Clustering Algorithm and its Applications to Natural Language Processing Problems"], 2006&lt;/ref&gt; Clustering methods are basically used to identify communities of nodes or links in a given network. This algorithm was designed by [[Chris Biemann]] and [[Sven Teresniak]] in 2005.&lt;ref name="Biemann2006"/&gt; The name comes from the fact that the process can be modeled as a separation of communities where the nodes send the same type of information to each other.&lt;ref name="Biemann2006"/&gt;

Chinese Whispers is a hard partitioning, randomized, flat clustering (no [[hierarchical clustering|hierarchical relations between clusters]]) method.&lt;ref name="Biemann2006"/&gt; The random property means that running the process on the same network several times can lead to different results, while because of hard partitioning one node can only belong to one cluster at a given moment.  The original algorithm is applicable to undirected, weighted and unweighted graphs. Chinese Whispers is time linear which means that it is extremely fast even if the number of nodes and links are very high in the network.&lt;ref name="Biemann2006"/&gt;

==Algorithm==
[[File:Chinese Whispers example cluster.png|thumb|right|An example of how Chinese Whispers works in action. The different colors represent different classes.]]

The algorithm works in the following way in an undirected unweighted graph:&lt;ref name="Biemann2006"/&gt;
# All nodes are assigned to a random class. The number of initial classes equals the number of nodes.
# Then all of the network nodes are selected one by one in a random order. Every node moves to the class which the given node connects with the most links. In the case of equality the cluster is randomly chosen from the equally linked classes.
# Step two repeats itself until a predetermined number of iteration or until the process converges. In the end the emerging classes represent the clusters of the network.

The predetermined threshold for the number of the iterations is needed because it is possible that process does not converge. On the other hand in a network with approximately 10000 nodes the clusters does not change significantly after 40-50 iterations even if there is no convergence.&lt;ref name="Biemann2006"/&gt;

==Strengths and Weaknesses==

The main strength of Chinese Whispers lies in its time linear property. Because of the processing time increases linearly with the number of nodes, the algorithm is capable of identifying communities in a network very fast. For this reason Chinese Whispers is a good tool to analyze community structures in graph with a very high number of nodes. The effectiveness of the method increases further if the network has the [[Small-world experiment|small world property]].&lt;ref name="Biemann2006"/&gt;

On the other hand because the algorithm is not deterministic in the case of small node number the resulting clusters often significantly differ from each other. The reason for this is that in the case of a small network it matters more from which node the iteration process starts while in large networks the relevance of starting points disappears.&lt;ref name="Biemann2006"/&gt; For this reason for small graphs other clustering methods are recommended.

==Applications==

Chinese Whispers is used in many subfield of network science. Most frequently it is mentioned in the context of [[Natural Language Processing]] problems.&lt;ref&gt;Antonio Di Marco - Roberto Navigili,[http://www.mitpressjournals.org/doi/full/10.1162/COLI_a_00148#.VW4sDc_tmko "Clustering and Diversifying Web Search Results with Graph Based Word Sense Induction"], 2013&lt;/ref&gt;&lt;ref&gt;Ioannis Korkontzelos - Suresh Manandhar,[http://www.aclweb.org/anthology/P09-2017 "Detecting Compositionality in Multi-Word Expressions"], 2009&lt;/ref&gt; On the other hand the algorithm is applicable to any kind of community identification problem which is related to a network framework. Chinese Whispers is available for personal use as an extension package for [[Gephi]]&lt;ref&gt;[https://marketplace.gephi.org/plugin/chinese-whispers-clustering/ "Gephi Marketplace" ]&lt;/ref&gt; which is an [[Open-source software|open source]] program designed for network analysis.

==References==
{{reflist}}

[[Category:Algorithms]]
[[Category:Cluster analysis algorithms]]</text>
      <sha1>17ajjmqo183n6s9r2zuev27i3eh0ihe</sha1>
    </revision>
  </page>
  <page>
    <title>Chiral algebra</title>
    <ns>0</ns>
    <id>31824847</id>
    <revision>
      <id>825922165</id>
      <parentid>822482914</parentid>
      <timestamp>2018-02-16T04:37:34Z</timestamp>
      <contributor>
        <username>Turgidson</username>
        <id>1747755</id>
      </contributor>
      <minor/>
      <comment>links</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="862">In mathematics, a '''chiral algebra''' is an algebraic structure introduced by {{harvtxt|Beilinson|Drinfeld|2004}} as a rigorous version of the rather vague concept of a chiral algebra in physics.

== See also ==
*[[Chiral homology]]
*[[Chiral Lie algebra]]

==References==

*{{Citation | last1=Beilinson|first1=Alexander|authorlink1=Alexander Beilinson|last2=Drinfeld|first2=Vladimir|authorlink2=Vladimir Drinfeld | title=Chiral algebras | url=https://books.google.com/books?id=yHZh3p-kFqQC | publisher=[[American Mathematical Society]] | location=Providence, R.I. | series=American Mathematical Society Colloquium Publications | isbn=978-0-8218-3528-9 | mr=2058353 | year=2004 | volume=51}}

== Further reading ==
*[https://arxiv.org/abs/1103.5803 Chiral Koszul Duality]

[[Category:Conformal field theory]]
[[Category:Representation theory]]

{{algebra-stub}}</text>
      <sha1>3qs3fbt3e14cfumaxn35juhagxkr6f6</sha1>
    </revision>
  </page>
  <page>
    <title>Covariance and contravariance of vectors</title>
    <ns>0</ns>
    <id>202886</id>
    <revision>
      <id>869226782</id>
      <parentid>869226547</parentid>
      <timestamp>2018-11-17T06:29:27Z</timestamp>
      <contributor>
        <username>Anita5192</username>
        <id>13220696</id>
      </contributor>
      <comment>/* See also */ Removed redundant links, [[Change of basis]], [[Covariant transformation]]; reordered.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="33585">{{About||use of "covariance" in the context of special relativity|Lorentz covariance|other uses of "covariant" or "contravariant"|covariance and contravariance (disambiguation)}}

[[File:Vector 1-form.svg|400px|thumb| A {{color box|#CC0000}} vector, '''v''', represented in terms of
{{glossary}}
 {{term|tangent basis}}
   {{defn|{{color box|orange}} '''e'''{{sub|1}}, '''e'''{{sub|2}}, '''e'''{{sub|3}} to the {{color box|black}} coordinate curves (''left''),}}
 {{term|dual basis, covector basis, or reciprocal basis}}
   {{defn|{{color box|blue}} '''e'''{{sup|1}}, '''e'''{{sup|2}}, '''e'''{{sup|3}} to {{color box|#4B545B}} coordinate surfaces (''right''),}}
{{glossary end}}
in [[three-dimensional|3-d]] general ''[[curvilinear coordinates]]'' {{nowrap|(''q''{{sup|1}}, ''q''{{sup|2}}, ''q''{{sup|3}})}}, a [[tuple]] of numbers to define a point in a [[position space]]. Note the basis and cobasis coincide only when the basis is [[orthogonal basis|orthogonal]].&lt;ref&gt;
 {{cite book
  | title=Gravitation
  | author3=J.A. Wheeler
  | author1=C. Misner
  | author2=K.S. Thorne
  | publisher=W.H. Freeman &amp; Co
  | year=1973
  | isbn=0-7167-0344-0
 }}&lt;/ref&gt;  
]]

In [[multilinear algebra]] and [[tensor analysis]], '''covariance''' and '''contravariance''' describe how the quantitative description of certain geometric or physical entities changes with a [[change of basis]]. 

In physics, a basis is sometimes thought of as a set of reference axes.  A change of scale on the reference axes corresponds to a change of units in the problem.  For instance, by changing scale from meters to centimeters (that is, ''dividing'' the scale of the reference axes by 100), the components of a measured [[velocity]] [[Vector (mathematics and physics)|vector]] are ''multiplied'' by 100.  Vectors exhibit this behavior of changing scale ''inversely'' to changes in scale to the reference axes and consequently are called ''contravariant''.  As a result, vectors often have units of distance or distance with other units (as, for example, velocity has units of distance divided by time).

In contrast, [[covector]]s (also called ''dual vectors'') typically have units of the inverse of distance or the inverse of distance with other units.  An example of a covector is the [[gradient]], which has units of a spatial [[derivative]], or distance&lt;sup&gt;−1&lt;/sup&gt;.  The components of covectors change in the ''same way'' as changes to scale of the reference axes and consequently are called ''covariant''.

A third concept related to covariance and contravariance is [[Invariant (physics)|invariance]].  An example of a physical [[observable]] that does not change with a change of scale on the reference axes is the [[mass]] of a particle, which has units of mass (that is, no units of distance).  The single, [[Scalar (mathematics)|scalar]] value of mass is independent of changes to the scale of the reference axes and consequently is called ''invariant''.

Under more general changes in basis:
* For a [[Euclidean vector|vector]] (such as a [[direction vector]] or velocity vector) to be [[Invariant (mathematics)|basis-independent]], the components of the vector must ''contra-vary'' with a change of basis to compensate. That is, the matrix that transforms the vector components must be the inverse of the matrix that transforms the basis vectors. The components of vectors (as opposed to those of covectors) are said to be '''contravariant'''.  Examples of vectors with ''contravariant components'' include the position of an object relative to an observer, or any derivative of position with respect to time, including velocity, [[acceleration]], and [[jerk (physics)|jerk]]. In [[Einstein notation]], contravariant components are denoted with ''upper indices'' as in
*:&lt;math&gt;\mathbf{v} = v^i \mathbf{e}_i .&lt;/math&gt;
* For a covector to be basis-independent, its components must ''co-vary'' with a change of basis to remain representing the same covector. That is, the components must be transformed by the same matrix as the change of basis matrix. The components of covectors (as opposed to those of vectors) are said to be '''covariant'''. Examples of ''covariant'' vectors generally appear when taking a [[gradient]] of a function. In [[Einstein notation]], covariant components are denoted with ''lower indices'' as in
*:&lt;math&gt;\mathbf{v} = v_i \mathbf{e}^i .&lt;/math&gt;

Curvilinear coordinate systems, such as cylindrical or spherical coordinates, are often used in physical and geometric problems.  Associated with any coordinate system is a natural choice of coordinate basis for vectors based at each point of the space, and covariance and contravariance are particularly important for understanding how the coordinate description of a vector changes by passing from one coordinate system to another.

The terms ''covariant'' and ''contravariant'' were introduced by [[James Joseph Sylvester]] in 1851&lt;ref&gt;Sylvester, James Joseph. "On the general theory of associated algebraical forms." ''Cambridge and Dublin Math. Journal, VI'' (1851): 289-293.&lt;/ref&gt;&lt;ref&gt;{{Cite book|url=https://www.worldcat.org/oclc/758983870|title=The collected mathematical papers of James Joseph Sylvester. Volume 3, 1870-1883|last=1814-1897.|first=Sylvester, James Joseph,|date=2012|publisher=Cambridge University Press|isbn=1107661439|location=Cambridge|oclc=758983870}}&lt;/ref&gt; in the context of associated algebraic forms theory.  In the lexicon of [[category theory]], [[covariance and contravariance of functors|covariance and contravariance]] are properties of [[functor]]s; unfortunately, it is the lower-index objects (covectors) that generically have [[pullback (differential geometry)|pullback]]s, which are contravariant, while the upper-index objects (vectors) instead have [[pushforward (differential)|pushforward]]s, which are covariant.  This terminological conflict may be avoided by calling contravariant functors "cofunctors"—in accord with the "covector" terminology, and continuing the tradition of treating vectors as the concept and covectors as the coconcept.

[[Tensor]]s are objects in [[multilinear algebra]] that can have aspects of both covariance ''and'' contravariance.

==Introduction==
In physics, a vector typically arises as the outcome of a measurement or series of measurements, and is represented as a list (or [[tuple]]) of numbers such as

:&lt;math&gt;(v_1,v_2,v_3) .&lt;/math&gt;

The numbers in the list depend on the choice of [[coordinate system]].  For instance, if the vector represents position with respect to an observer ([[position vector]]), then the coordinate system may be obtained from a system of rigid rods, or reference axes, along which the components ''v''&lt;sub&gt;1&lt;/sub&gt;, ''v''&lt;sub&gt;2&lt;/sub&gt;, and ''v''&lt;sub&gt;3&lt;/sub&gt; are measured.  For a vector to represent a geometric object, it must be possible to describe how it looks in any other coordinate system.  That is to say, the components of the vectors will ''transform'' in a certain way in passing from one coordinate system to another.

A ''contravariant vector'' has components that "transform as the coordinates do" under changes of coordinates (and so inversely to the transformation of the reference axes), including [[rotation (mathematics)|rotation]] and dilation. [[Active and passive transformation#Passive transformation|The vector itself does not change under these operations]]; instead, the components of the vector change in a way that cancels the change in the spatial axes, in the same way that coordinates change.  In other words, if the reference axes were rotated in one direction, the component representation of the vector would rotate in exactly the opposite way. Similarly, if the reference axes were stretched in one direction, the components of the vector, like the coordinates, would reduce in an exactly compensating way.  Mathematically, if the coordinate system undergoes a transformation described by an [[invertible matrix]] ''M'', so that a [[coordinate vector]] '''x''' is transformed to &lt;math&gt;\mathbf{x}'=M\mathbf{x}&lt;/math&gt;, then a contravariant vector '''v''' must be similarly transformed via &lt;math&gt;\mathbf{v}'=M\mathbf{v}&lt;/math&gt;. This important requirement is what distinguishes a contravariant vector from any other triple of physically meaningful quantities.  For example, if ''v'' consists of the ''x''-, ''y''-, and ''z''-components of [[velocity]], then ''v'' is a contravariant vector: if the coordinates of space are stretched, rotated, or twisted, then the components of the velocity transform in the same way.  Examples of contravariant vectors include [[displacement (vector)|displacement]], [[velocity]] and [[acceleration]].  On the other hand, for instance, a triple consisting of the length, width, and height of a rectangular box could make up the three components of an abstract [[vector space|vector]], but this vector would not be contravariant, since a change in coordinates on the space does not change the box's length, width, and height: instead these are [[scalar (physics)|scalars]].

By contrast, a ''covariant vector'' has components that change oppositely to the coordinates or, equivalently, transform like the reference axes.  For instance, the components of the [[gradient]] vector of a function
:&lt;math&gt;\nabla f = \frac{\partial f}{\partial x^1}\widehat{x}^1+\frac{\partial f}{\partial x^2}\widehat{x}^2+\frac{\partial f}{\partial x^3}\widehat{x}^3&lt;/math&gt;
transform like the reference axes themselves.

==Definition==
The general formulation of covariance and contravariance refer to how the components of a coordinate vector transform under a [[change of basis]] ([[Active and passive transformation|passive transformation]]).  Thus let ''V'' be a [[vector space]] of dimension ''n'' over the field of [[scalar (mathematics)|scalars]] ''S'', and let each of {{nowrap|1='''f''' = (''X''&lt;sub&gt;1&lt;/sub&gt;, ..., ''X''&lt;sub&gt;''n''&lt;/sub&gt;)}} and {{nowrap|1='''f'''′ = (''Y''&lt;sub&gt;1&lt;/sub&gt;, ..., ''Y''&lt;sub&gt;''n''&lt;/sub&gt;)}} be a [[basis of a vector space|basis]] of ''V''.&lt;ref group="note"&gt;A basis '''f''' may here profitably be viewed as a [[linear isomorphism]] from '''R'''&lt;sup&gt;''n''&lt;/sup&gt; to ''V''.  Regarding '''f''' as a row vector whose entries are the elements of the basis, the associated linear isomorphism is then &lt;math&gt;\mathbf{x}\mapsto \mathbf{f}\mathbf{x}.&lt;/math&gt;&lt;/ref&gt;  Also, let the [[change of basis]] from '''f''' to '''f'''′ be given by

{{NumBlk|:|&lt;math&gt;\mathbf{f}\mapsto \mathbf{f}' = \left(\sum_i a^i_1X_i,\dots,\sum_i a^i_nX_i\right) = \mathbf{f}A&lt;/math&gt;|{{EquationRef|1}}}}

for some [[invertible matrix|invertible]] ''n''×''n'' matrix ''A'' with entries &lt;math&gt;a^i_j&lt;/math&gt;.
Here, each vector ''Y''&lt;sub&gt;''j''&lt;/sub&gt; of the '''f'''′ basis is a linear combination of the vectors ''X''&lt;sub&gt;''i''&lt;/sub&gt; of the '''f''' basis, so that

:&lt;math&gt;Y_j=\sum_i a^i_jX_i.&lt;/math&gt;

===Contravariant transformation===
A vector &lt;math&gt;v&lt;/math&gt; in ''V'' is expressed uniquely as a [[linear combination]] of the elements of the '''f''' basis as

{{NumBlk|:|&lt;math&gt;v=\sum_i v^i[\mathbf{f}]X_i,&lt;/math&gt;|{{EquationRef|2}}}}

where ''v''{{i sup|''i''}}['''f'''] are [[scalar (mathematics)|scalars]] in ''S'' known as the '''components''' of ''v'' in the '''f''' basis. Denote the [[column vector]] of components of ''v'' by '''v'''['''f''']:

:&lt;math&gt;\mathbf{v}[\mathbf{f}] = \begin{bmatrix}v^1[\mathbf{f}]\\v^2[\mathbf{f}]\\\vdots\\v^n[\mathbf{f}]\end{bmatrix}&lt;/math&gt;

so that ({{EquationNote|2}}) can be rewritten as a matrix product

:&lt;math&gt;v = \mathbf{f}\, \mathbf{v}[\mathbf{f}].&lt;/math&gt;

The vector ''v'' may also be expressed in terms of the '''f'''′ basis, so that

:&lt;math&gt;v = \mathbf{f'}\, \mathbf{v}[\mathbf{f'}].&lt;/math&gt;

However, since the vector ''v'' itself is invariant under the choice of basis,

:&lt;math&gt;\mathbf{f}\, \mathbf{v}[\mathbf{f}] = v = \mathbf{f'}\, \mathbf{v}[\mathbf{f'}].&lt;/math&gt;

The invariance of ''v'' combined with the relationship ({{EquationNote|1}}) between '''f''' and '''f'''′ implies that

:&lt;math&gt;\mathbf{f}\, \mathbf{v}[\mathbf{f}] = \mathbf{f}A\, \mathbf{v}[\mathbf{f}A],&lt;/math&gt;

giving the transformation rule

:&lt;math&gt;\mathbf{v}[\mathbf{f}A] = A^{-1}\mathbf{v}[\mathbf{f}].&lt;/math&gt;

In terms of components,

:&lt;math&gt;v^i[\mathbf{f}A] = \sum_j \tilde{a}^i_jv^j[\mathbf{f}]&lt;/math&gt;

where the coefficients &lt;math&gt;\tilde{a}^i_j&lt;/math&gt; are the entries of the [[inverse matrix]] of ''A''.

Because the components of the vector ''v'' transform with the ''inverse'' of the matrix ''A'', these components are said to '''transform contravariantly''' under a change of basis.

The way ''A'' relates the two  pairs is depicted in the following informal diagram using an arrow.  The reversal of the arrow indicates a contravariant change:
:&lt;math&gt;\begin{align}
     \mathbf{f} &amp;\longrightarrow \mathbf{f'} \\
  v[\mathbf{f}] &amp;\longleftarrow v[\mathbf{f'}]
\end{align}&lt;/math&gt;

===Covariant transformation===
{{main|Covariant transformation}}
A [[linear functional]] ''α'' on ''V'' is expressed uniquely in terms of its '''components''' (scalars in ''S'') in the '''f''' basis as

:&lt;math&gt;\alpha(X_i) = \alpha_i[\mathbf{f}] , \quad i=1,2,\dots,n.&lt;/math&gt;

These components are the action of ''α'' on the basis vectors ''X''&lt;sub&gt;''i''&lt;/sub&gt; of the '''f''' basis.

Under the change of basis from '''f''' to '''f'''′ ({{EquationNote|1}}), the components transform so that

{{NumBlk|:|&lt;math&gt;\begin{align}
  \alpha_i[\mathbf{f}A] &amp;= \alpha(Y_i) \\
                        &amp;= \alpha\left(\sum_j a^j_i X_j\right) \\
                        &amp;= \sum_j a^j_i \alpha(X_j) \\
                        &amp;= \sum_j a^j_i \alpha_j[\mathbf{f}].
\end{align}&lt;/math&gt;|{{EquationRef|3}}}}

Denote the [[row vector]] of components of ''α'' by ''α''['''f''']:

:&lt;math&gt;\mathbf{\alpha}[\mathbf{f}] = \begin{bmatrix}\alpha_1[\mathbf{f}],\alpha_2[\mathbf{f}],\dots,\alpha_n[\mathbf{f}]\end{bmatrix}&lt;/math&gt;

so that ({{EquationNote|3}}) can be rewritten as the matrix product

:&lt;math&gt;\alpha[\mathbf{f}A] = \alpha[\mathbf{f}]A.&lt;/math&gt;

Because the components of the linear functional α transform with the matrix ''A'', these components are said to '''transform covariantly''' under a change of basis.

The way ''A'' relates the two  pairs is depicted in the following informal diagram using an arrow.  A covariant relationship is indicated since the arrows travel in the same direction:
:&lt;math&gt;\begin{align}
          \mathbf{f} &amp;\longrightarrow \mathbf{f'} \\
  \alpha[\mathbf{f}] &amp;\longrightarrow \alpha[\mathbf{f'}]
\end{align}&lt;/math&gt;

Had a column vector representation been used instead, the transformation law would be the [[matrix transpose|transpose]]
:&lt;math&gt;\alpha^\mathrm{T}[\mathbf{f}A] = A^\mathrm{T}\alpha^\mathrm{T}[\mathbf{f}].&lt;/math&gt;

==Coordinates==
The choice of basis '''f''' on the vector space ''V'' defines uniquely a set of coordinate functions on ''V'', by means of
:&lt;math&gt;x^i[\mathbf{f}](v) = v^i[\mathbf{f}].&lt;/math&gt;
The coordinates on ''V'' are therefore contravariant in the sense that
:&lt;math&gt;x^i[\mathbf{f}A] = \sum_{k=1}^n \tilde{a}^i_kx^k[\mathbf{f}].&lt;/math&gt;
Conversely, a system of ''n'' quantities ''v''&lt;sup&gt;''i''&lt;/sup&gt; that transform like the coordinates ''x''&lt;sup&gt;''i''&lt;/sup&gt; on ''V'' defines a contravariant vector.  A system of ''n'' quantities that transform oppositely to the coordinates is then a covariant vector.

This formulation of contravariance and covariance is often more natural in applications in which there is a coordinate space (a [[manifold]]) on which vectors live as [[tangent vector]]s or [[cotangent vector]]s.  Given a local coordinate system ''x''&lt;sup&gt;''i''&lt;/sup&gt; on the manifold, the reference axes for the coordinate system are the [[vector field]]s
:&lt;math&gt;X_1 = \frac{\partial}{\partial x^1},\dots,X_n=\frac{\partial}{\partial x^n}.&lt;/math&gt;
This gives rise to the frame {{nowrap|1='''f''' = (''X''&lt;sub&gt;1&lt;/sub&gt;, ..., ''X''&lt;sub&gt;''n''&lt;/sub&gt;)}} at every point of the coordinate patch.

If ''y''&lt;sup&gt;''i''&lt;/sup&gt; is a different coordinate system and
:&lt;math&gt;Y_1=\frac{\partial}{\partial y^1},\dots,Y_n = \frac{\partial}{\partial y^n},&lt;/math&gt;
then the frame '''f'''' is related to the frame '''f''' by the inverse of the [[Jacobian matrix]] of the coordinate transition:
:&lt;math&gt;\mathbf{f}' = \mathbf{f}J^{-1},\quad J = \left(\frac{\partial y^i}{\partial x^j}\right)_{i,j=1}^n.&lt;/math&gt;
Or, in indices,
:&lt;math&gt;\frac{\partial}{\partial y^i} = \sum_{j=1}^n\frac{\partial x^j}{\partial y^i}\frac{\partial}{\partial x^j}.&lt;/math&gt;

A tangent vector is by definition a vector that is a linear combination of the coordinate partials &lt;math&gt;\partial/\partial x^i&lt;/math&gt;.  Thus a tangent vector is defined by
:&lt;math&gt;v = \sum_{i=1}^n v^i[\mathbf{f}] X_i = \mathbf{f}\ \mathbf{v}[\mathbf{f}].&lt;/math&gt;

Such a vector is contravariant with respect to change of frame.  Under changes in the coordinate system, one has
:&lt;math&gt;\mathbf{v}\left[\mathbf{f}'\right] = \mathbf{v}\left[\mathbf{f}J^{-1}\right] = J\, \mathbf{v}[\mathbf{f}].&lt;/math&gt;

Therefore, the components of a tangent vector transform via
:&lt;math&gt;v^i\left[\mathbf{f}'\right] = \sum_{j=1}^n \frac{\partial y^i}{\partial x^j}v^j[\mathbf{f}].&lt;/math&gt;

Accordingly, a system of ''n'' quantities ''v''&lt;sup&gt;''i''&lt;/sup&gt; depending on the coordinates that transform in this way on passing from one coordinate system to another is called a contravariant vector.

==Covariant and contravariant components of a vector with a metric==
In a finite-dimensional [[vector space]] ''V'' over a field ''K'' with a symmetric [[bilinear form]] {{nowrap|''g'' : ''V'' × ''V'' → ''K''}} (which may be referred to as the [[metric tensor]]), there is little distinction between covariant and contravariant vectors, because the [[bilinear form]] allows covectors to be identified with vectors.  That is, a vector ''v'' uniquely determines a covector ''α'' via
:&lt;math&gt;\alpha(w) = g(v, w) &lt;/math&gt;
for all vectors ''w''.  Conversely, each covector ''α'' determines a unique vector ''v'' by this equation.  Because of this identification of vectors with covectors, one may speak of the '''covariant components''' or '''contravariant components''' of a vector, that is, they are just representations of the same vector using the [[Dual basis|reciprocal basis]].

Given a basis {{nowrap|1='''f''' = (''X''&lt;sub&gt;1&lt;/sub&gt;, ..., ''X''&lt;sub&gt;''n''&lt;/sub&gt;)}} of ''V'', there is a unique reciprocal basis {{nowrap|1='''f'''&lt;sup&gt;#&lt;/sup&gt; = (''Y''&lt;sup&gt;1&lt;/sup&gt;, ..., ''Y''&lt;sup&gt;''n''&lt;/sup&gt;)}} of ''V'' determined by requiring that
:&lt;math&gt;Y^i(X_j) = \delta^i_j,&lt;/math&gt;
the [[Kronecker delta]].  In terms of these bases, any vector ''v'' can be written in two ways:
:&lt;math&gt;\begin{align}
v &amp;= \sum_i v^i[\mathbf{f}]X_i = \mathbf{f}\,\mathbf{v}[\mathbf{f}]\\
&amp;=\sum_i v_i[\mathbf{f}]Y^i = \mathbf{f}^\sharp\mathbf{v}^\sharp[\mathbf{f}].
\end{align}
&lt;/math&gt;
The components ''v''&lt;sup&gt;''i''&lt;/sup&gt;['''f'''] are the '''contravariant components''' of the vector ''v'' in the basis '''f''', and the components ''v''&lt;sub&gt;''i''&lt;/sub&gt;['''f'''] are the '''covariant components''' of ''v'' in the basis '''f'''.  The terminology is justified because under a change of basis,

:&lt;math&gt;\mathbf{v}[\mathbf{f}A] = A^{-1}\mathbf{v}[\mathbf{f}],\quad \mathbf{v}^\sharp[\mathbf{f}A] = A^T\mathbf{v}^\sharp[\mathbf{f}].&lt;/math&gt;

[[File:Basis.svg|frame|350px|left|The {{color box|darkred}} contravariant components of a vector are obtained by [[scalar projection|projecting]] onto the {{color box|orange}} coordinate axes. The covariant components are obtained by projecting onto the normal lines to the {{color box|blue}} coordinate hyperplanes.]]
{{clear}}

===Euclidean plane===
In the Euclidean plane, the [[dot product]] allows for vectors to be identified with covectors.  If &lt;math&gt;\mathbf{e}_1,\mathbf{e}_2&lt;/math&gt; is a basis, then the dual basis &lt;math&gt;\mathbf{e}^1,\mathbf{e}^2&lt;/math&gt; satisfies
:&lt;math&gt;\begin{align}
  \mathbf{e}^1\cdot\mathbf{e}_1 = 1, &amp;\quad \mathbf{e}^1\cdot\mathbf{e}_2 = 0 \\
  \mathbf{e}^2\cdot\mathbf{e}_1 = 0, &amp;\quad \mathbf{e}^2\cdot\mathbf{e}_2 = 1.
\end{align}&lt;/math&gt;

Thus, '''e'''&lt;sup&gt;1&lt;/sup&gt; and '''e'''&lt;sub&gt;2&lt;/sub&gt; are perpendicular to each other, as are '''e'''&lt;sup&gt;2&lt;/sup&gt; and '''e'''&lt;sub&gt;1&lt;/sub&gt;, and the lengths of '''e'''&lt;sup&gt;1&lt;/sup&gt; and '''e'''&lt;sup&gt;2&lt;/sup&gt; normalized against '''e'''&lt;sub&gt;1&lt;/sub&gt; and '''e'''&lt;sub&gt;2&lt;/sub&gt;, respectively.

====Example====
For example,&lt;ref&gt;{{cite web |last=Bowen |first=Ray |title=Introduction to Vectors and Tensors |year=2008 |publisher=Dover |pages=78, 79, 81 |url=http://repositories.tamu.edu/bitstream/handle/1969.1/2502/IntroductionToVectorsAndTensorsVol1.pdf?sequence=12 }}{{dead link|date=August 2017 |bot=InternetArchiveBot |fix-attempted=yes }}&lt;/ref&gt; suppose that we are given a basis '''e'''&lt;sub&gt;1&lt;/sub&gt;, '''e'''&lt;sub&gt;2&lt;/sub&gt; consisting of a pair of vectors making a 45° angle with one another, such that '''e'''&lt;sub&gt;1&lt;/sub&gt; has length 2 and '''e'''&lt;sub&gt;2&lt;/sub&gt; has length 1.  Then the dual basis vectors are given as follows:
* '''e'''&lt;sup&gt;2&lt;/sup&gt; is the result of rotating '''e'''&lt;sub&gt;1&lt;/sub&gt; through an angle of 90° (where the sense is measured by assuming the pair '''e'''&lt;sub&gt;1&lt;/sub&gt;, '''e'''&lt;sub&gt;2&lt;/sub&gt; to be positively oriented), and then rescaling so that {{nowrap|'''e'''&lt;sup&gt;2&lt;/sup&gt; ⋅ '''e'''&lt;sub&gt;2&lt;/sub&gt; {{=}} 1}} holds.
* '''e'''&lt;sup&gt;1&lt;/sup&gt; is the result of rotating '''e'''&lt;sub&gt;2&lt;/sub&gt; through an angle of 90°, and then rescaling so that {{nowrap|'''e'''&lt;sup&gt;1&lt;/sup&gt; ⋅ '''e'''&lt;sub&gt;1&lt;/sub&gt; {{=}} 1}} holds.
Applying these rules, we find
:&lt;math&gt;\mathbf{e}^1 = \frac{1}{2}\mathbf{e}_1 - \frac{1}{\sqrt{2}}\mathbf{e}_2&lt;/math&gt;
and
:&lt;math&gt;\mathbf{e}^2 = -\frac{1}{\sqrt{2}}\mathbf{e}_1 + 2\mathbf{e}_2.&lt;/math&gt;

Thus the change of basis matrix in going from the original basis to the reciprocal basis is
:&lt;math&gt;R = \begin{bmatrix}
   \frac{1}{2}         &amp; -\frac{1}{\sqrt{2}} \\
  -\frac{1}{\sqrt{2}}  &amp;  2
\end{bmatrix},&lt;/math&gt;
since
:&lt;math&gt;[\mathbf{e}^1\ \mathbf{e}^2] = [\mathbf{e}_1\ \mathbf{e}_2]\begin{bmatrix}
   \frac{1}{2}        &amp; -\frac{1}{\sqrt{2}} \\
  -\frac{1}{\sqrt{2}} &amp;  2
\end{bmatrix}.&lt;/math&gt;

For instance, the vector
:&lt;math&gt;v = \frac{3}{2}\mathbf{e}_1 + 2\mathbf{e}_2&lt;/math&gt;
is a vector with contravariant components
:&lt;math&gt;v^1 = \frac{3}{2},\quad v^2 = 2.&lt;/math&gt;

The covariant components are obtained by equating the two expressions for the vector ''v'':
:&lt;math&gt;v = v_1\mathbf{e}^1 + v_2\mathbf{e}^2 = v^1\mathbf{e}_1 + v^2\mathbf{e}_2&lt;/math&gt;
so
:&lt;math&gt;\begin{align}
  \begin{bmatrix}v_1\\ v_2\end{bmatrix} &amp;= R^{-1}\begin{bmatrix}v^1 \\ v^2\end{bmatrix} \\
                                        &amp;= \begin{bmatrix}4 &amp; \sqrt{2} \\ \sqrt{2} &amp; 1\end{bmatrix}
                                             \begin{bmatrix}v^1 \\ v^2\end{bmatrix} \\
                                        &amp;= \begin{bmatrix}6 + 2\sqrt{2} \\ 2 + \frac{3}{\sqrt{2}}\end{bmatrix}
\end{align}.&lt;/math&gt;

===Three-dimensional Euclidean space===

In the three-dimensional [[Euclidean space]], one can also determine explicitly the dual basis to a given set of [[basis vector]]s '''e'''&lt;sub&gt;1&lt;/sub&gt;, '''e'''&lt;sub&gt;2&lt;/sub&gt;, '''e'''&lt;sub&gt;3&lt;/sub&gt; of ''E''&lt;sub&gt;3&lt;/sub&gt; that are not necessarily assumed to be orthogonal nor of unit norm. The dual basis vectors are:

:&lt;math&gt; \mathbf{e}^1 = \frac{\mathbf{e}_2 \times \mathbf{e}_3}{\mathbf{e}_1 \cdot (\mathbf{e}_2 \times \mathbf{e}_3)} ; \qquad \mathbf{e}^2 = \frac{\mathbf{e}_3 \times \mathbf{e}_1}{\mathbf{e}_2 \cdot (\mathbf{e}_3 \times \mathbf{e}_1)}; \qquad \mathbf{e}^3 = \frac{\mathbf{e}_1 \times \mathbf{e}_2}{\mathbf{e}_3 \cdot (\mathbf{e}_1 \times \mathbf{e}_2)}.
&lt;/math&gt;

Even when the '''e'''&lt;sub&gt;i&lt;/sub&gt; and '''e'''&lt;sup&gt;i&lt;/sup&gt; are not [[Orthonormality|orthonormal]], they are still mutually reciprocal:
:&lt;math&gt;\mathbf{e}^i \cdot \mathbf{e}_j = \delta^i_j,&lt;/math&gt;

Then the contravariant components of any vector '''v''' can be obtained by the [[dot product]] of '''v''' with the dual basis vectors:

:&lt;math&gt; q^1 = \mathbf{v} \cdot \mathbf{e}^1; \qquad q^2 = \mathbf{v} \cdot \mathbf{e}^2; \qquad q^3 = \mathbf{v} \cdot \mathbf{e}^3 .&lt;/math&gt;

Likewise, the covariant components of '''v''' can be obtained from the dot product of '''v''' with basis vectors, viz.

:&lt;math&gt; q_1 = \mathbf{v} \cdot \mathbf{e}_1; \qquad q_2 = \mathbf{v} \cdot \mathbf{e}_2; \qquad q_3 = \mathbf{v} \cdot \mathbf{e}_3 .&lt;/math&gt;

Then '''v''' can be expressed in two (reciprocal) ways, viz.
:&lt;math&gt; \mathbf{v} = q^i \mathbf{e}_i = q^1 \mathbf{e}_1 + q^2 \mathbf{e}_2 + q^3 \mathbf{e}_3 .&lt;/math&gt;
or
:&lt;math&gt; \mathbf{v} = q_i \mathbf{e}^i = q_1 \mathbf{e}^1 + q_2 \mathbf{e}^2 + q_3 \mathbf{e}^3 &lt;/math&gt;
Combining the above relations, we have
:&lt;math&gt; \mathbf{v} = (\mathbf{v} \cdot \mathbf{e}^i) \mathbf{e}_i = (\mathbf{v} \cdot \mathbf{e}_i) \mathbf{e}^i &lt;/math&gt;
and we can convert between the basis and dual basis with
:&lt;math&gt;q_i = \mathbf{v}\cdot \mathbf{e}_i = (q^j \mathbf{e}_j)\cdot \mathbf{e}_i = (\mathbf{e}_j\cdot\mathbf{e}_i) q^j &lt;/math&gt;
and
:&lt;math&gt;q^i = \mathbf{v}\cdot \mathbf{e}^i = (q_j \mathbf{e}^j)\cdot \mathbf{e}^i = (\mathbf{e}^j\cdot\mathbf{e}^i) q_j .&lt;/math&gt;

If the basis vectors are [[orthonormal]], then they are the same as the dual basis vectors. So there is no need to distinguish between contravariant components and covariant components which are also equal.

===General Euclidean spaces===
More generally, in an ''n''-dimensional Euclidean space ''V'', if a basis is
:&lt;math&gt;\mathbf{e}_1,\dots,\mathbf{e}_n,&lt;/math&gt;
the reciprocal basis is given by (double indices are summed over),
:&lt;math&gt;\mathbf{e}^i=g^{ij}\mathbf{e}_j&lt;/math&gt;
where the coefficients ''g''&lt;sup&gt;''ij''&lt;/sup&gt; are the entries of the inverse matrix of
:&lt;math&gt;g_{ij} = \mathbf{e}_i\cdot\mathbf{e}_j .&lt;/math&gt;
Indeed, we then have
:&lt;math&gt;\mathbf{e}^i\cdot\mathbf{e}_k=g^{ij}\mathbf{e}_j\cdot\mathbf{e}_k=g^{ij}g_{jk} = \delta^i_k .&lt;/math&gt;

The covariant and contravariant components of any vector

:&lt;math&gt; \mathbf{v} = q_i \mathbf{e}^i = q^i \mathbf{e}_i \, &lt;/math&gt;

are related as above by
:&lt;math&gt;q_i = \mathbf{v}\cdot \mathbf{e}_i = (q^j \mathbf{e}_j)\cdot \mathbf{e}_i = q^jg_{ji}&lt;/math&gt;
and
:&lt;math&gt;q^i = \mathbf{v}\cdot \mathbf{e}^i = (q_j\mathbf{e}^j)\cdot \mathbf{e}^i = q_jg^{ji} .&lt;/math&gt;

==Informal usage==
In the field of [[physics]], the [[adjective]] '''covariant''' is often used informally as a synonym for invariant. For example, the [[Schrödinger equation]] does not keep its written form under the coordinate transformations of [[special relativity]]. Thus, a physicist might say that the Schrödinger equation is ''not covariant''. In contrast, the [[Klein–Gordon equation]] and the [[Dirac equation]] do keep their written form under these coordinate transformations. Thus, a physicist might say that these equations are ''covariant''.

Despite this usage of "covariant", it is more accurate to say that the Klein–Gordon and Dirac equations are invariant, and that the Schrödinger equation is not invariant. Additionally, to remove ambiguity, the transformation by which the invariance is evaluated should be indicated.

Because the components of vectors are contravariant and those of covectors are covariant, the vectors themselves are often referred to as being contravariant and the covectors as covariant.

==Use in tensor analysis==

The distinction between covariance and contravariance is particularly important for computations with [[tensor]]s, which often have '''mixed variance'''. This means that they have both covariant and contravariant components, or both vector and covector components. The valence of a tensor is the number of variant and covariant terms, and in [[Einstein notation]], covariant components have lower indices, while contravariant components have upper indices.  The duality between covariance and contravariance intervenes whenever a vector or tensor quantity is represented by its components, although modern [[differential geometry]] uses more sophisticated [[Tensor (intrinsic definition)|index-free methods to represent tensors]].

In [[tensor analysis]], a '''covariant''' vector varies more or less reciprocally to a corresponding contravariant vector.  Expressions for lengths, areas and volumes of objects in the vector space can then be given in terms of tensors with covariant and contravariant indices. Under simple expansions and contractions of the coordinates, the reciprocity is exact; under affine transformations the components of a vector intermingle on going between covariant and contravariant expression.

On a [[manifold]], a [[tensor field]] will typically have multiple, upper and lower indices, where Einstein notation is widely used. When the manifold is equipped with a [[metric tensor|metric]], covariant and contravariant indices become very closely related to one another. Contravariant indices can be turned into covariant indices by [[contraction of a tensor|contracting]] with the metric tensor. The reverse is possible by contracting with the (matrix) inverse of the metric tensor. Note that in general, no such relation exists in spaces not endowed with a metric tensor. Furthermore, from a more abstract standpoint, a tensor is simply "there" and its components of either kind are only calculational artifacts whose values depend on the chosen coordinates.

The explanation in geometric terms is that a general tensor will have contravariant indices as well as covariant indices, because it has parts that live in the [[tangent bundle]] as well as the [[cotangent bundle]].

A contravariant vector is one which transforms like &lt;math&gt;\frac{dx^{\mu}}{d\tau}&lt;/math&gt;, where &lt;math&gt;x^{\mu} \!&lt;/math&gt; are the coordinates of a particle at its [[proper time]] &lt;math&gt;\tau&lt;/math&gt;. A covariant vector is one which transforms like &lt;math&gt;\frac{\partial \varphi}{\partial x^{\mu}}&lt;/math&gt;, where &lt;math&gt;\varphi&lt;/math&gt; is a scalar field.

==Algebra and geometry==

In [[category theory]], there are [[covariant functor]]s and [[contravariant functor]]s. The assignment of the [[dual space]] to a vector space is a standard example of a contravariant functor. Some constructions of [[multilinear algebra]] are of 'mixed' variance, which prevents them from being functors.

In [[differential geometry]], the components of a vector relative to a basis of the [[tangent bundle]] are covariant if they change with the same linear transformation as a change of basis.  They are contravariant if they change by the inverse transformation.  This is sometimes a source of confusion for two distinct but related reasons.  The first is that vectors whose components are covariant (called covectors or [[1-form]]s) actually [[pullback (differential geometry)|pull back]] under smooth functions, meaning that the operation assigning the space of covectors to a smooth manifold is actually a ''contravariant'' functor.  Likewise, vectors whose components are contravariant [[pushforward (differential)|push forward]] under smooth mappings, so the operation assigning the space of (contravariant) vectors to a smooth manifold is a ''covariant'' functor.  Secondly, in the classical approach to differential geometry, it is not bases of the tangent bundle that are the most primitive object, but rather changes in the coordinate system.  Vectors with contravariant components transform in the same way as changes in the coordinates (because these actually change oppositely to the induced change of basis).  Likewise, vectors with covariant components transform in the opposite way as changes in the coordinates.

==See also==
* [[Active and passive transformation]]
* [[Mixed tensor]]
* [[Two-point tensor]], a generalization allowing indices to reference multiple vector bases

==Notes==
{{Reflist|group="note"}}

==Citations==
{{Reflist}}

==References==
* {{citation |last1=Arfken |first1=George B.|authorlink1=George B. Arfken |first2=Hans J. |last2=Weber |title=Mathematical Methods for Physicists |edition=6th |publisher=Harcourt |publication-place=San Diego |year=2005 |isbn=0-12-059876-0}}.
* {{Citation | last1=Dodson | first1=C. T. J. | last2=Poston | first2=T. | title=Tensor geometry | publisher=[[Springer-Verlag]] | location=Berlin, New York | edition=2nd | series=Graduate Texts in Mathematics | isbn=978-3-540-52018-4 | mr=1223091 | year=1991 | volume=130}}.
* {{Citation | last1=Greub | first1=Werner Hildbert | title=Multilinear algebra | publisher=Springer-Verlag New York, Inc., New York | series=Die Grundlehren der Mathematischen Wissenschaften, Band 136 | mr=0224623 | year=1967}}.
* {{Citation | last1=Sternberg | first1=Shlomo | author1-link=Shlomo Sternberg | title=Lectures on differential geometry | publisher=Chelsea | location=New York | isbn=978-0-8284-0316-0 | year=1983}}.
* {{citation |doi=10.1098/rstl.1853.0018 |first=J.J. |last=Sylvester |jstor=108572 |title=On a Theory of the Syzygetic Relations of Two Rational Integral Functions, Comprising an Application to the Theory of Sturm's Functions, and That of the Greatest Algebraical Common Measure |year=1853 |journal=Philosophical Transactions of the Royal Society of London |volume=143 |pages=407–548 |publisher=The Royal Society}}.

==External links==
* {{springer|title=Covariant tensor|id=p/c026880}}
* {{springer|title=Contravariant tensor|id=p/c025970}}
* {{MathWorld | urlname=CovariantTensor | title=Covariant Tensor}}
* {{MathWorld | urlname=ContravariantTensor | title=Contravariant Tensor}}
* [http://www.mathpages.com/home/kmath398/kmath398.htm Invariance, Contravariance, and Covariance]
* [http://www.ita.uni-heidelberg.de/~dullemond/lectures/tensor/tensor.pdf Introduction to tensor calculus----Kees Dullemond &amp; Kasper Peeters]
{{tensors}}

[[Category:Tensors]]
[[Category:Differential geometry]]
[[Category:Riemannian geometry]]
[[Category:Vectors (mathematics and physics)]]</text>
      <sha1>3khtunejaxsee8vcyfzoeah2lzo4jfm</sha1>
    </revision>
  </page>
  <page>
    <title>De Moivre's law</title>
    <ns>0</ns>
    <id>30389604</id>
    <revision>
      <id>704012836</id>
      <parentid>697050099</parentid>
      <timestamp>2016-02-09T00:15:07Z</timestamp>
      <contributor>
        <username>NewYorkActuary</username>
        <id>26033934</id>
      </contributor>
      <comment>/* History */ add ref</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="9150">{{lowercase title}}
{{for|the identity connecting complex numbers and trigonometric functions |de Moivre's formula}}

'''De Moivre's Law''' is a [[survival model]] applied in [[actuarial science]], named for [[Abraham de Moivre]].&lt;ref&gt;Abraham de Moivre (1725) ''Annuities upon Lives''. The second edition of ''Annuities upon Lives'' was published in 1743.&lt;/ref&gt;&lt;ref&gt;Abraham de Moivre (1752) ''A Treatise of Annuities on Lives''.&lt;/ref&gt;&lt;ref&gt;
{{cite book
|title = Pioneers of Financial Economics: Volume I, Contributions Prior to Irving Fisher
|editor = Geoffrey Poitras
|author = Geoffrey Poitras
|chapter = Life annuity valuation: from de Witt and Halley to de Moivre and Simpson
|year = 2006
|isbn = 978-1-84542-381-0
}}&lt;/ref&gt; It is a simple law of mortality based on a linear [[survival function]]. 

==Definition==
De Moivre's law has a single
parameter &lt;math&gt;\omega&lt;/math&gt; called the ''ultimate age''. Under de Moivre's
law, a newborn has probability of surviving at least ''x'' years given by the
[[survival function]]&lt;ref&gt;Bowers, N.L., Gerber, H.U., Hickman, J.C., Jones, D.A. and Nesbitt, C.J. (1997). ''Actuarial Mathematics (Second Edition)'', Schaumburg, Illinois, Society of Actuaries.&lt;/ref&gt;
:&lt;math&gt;
 S(x) = 1 - \frac{x}{\omega}, \qquad 0 \leq x &lt; \omega.
 &lt;/math&gt;
In [[actuarial notation]] ''(x)'' denotes a status or life that has survived to age ''x'', and ''T''(''x'') is the future lifetime of ''(x)'' (''T''(''x'') is a random variable). The conditional probability that ''(x)'' survives to age ''x+t'' is ''Pr[T(0) ≥ x+t | T(0) ≥ x] = S(x+t) / S(x),''
which is denoted by &lt;math&gt;{}_t p_x&lt;/math&gt;.&lt;ref&gt;Bowers, et al. (1977). Also see [[Actuarial_notation#Life_tables | Actuarial notation:Life tables]] for explanation of the notation &lt;math&gt;{}_t p_x&lt;/math&gt; for conditional probability of survival.&lt;/ref&gt;
Under de Moivre's law, the conditional probability that a life aged ''x'' years survives
at least ''t'' more years is
:&lt;math&gt;
 {}_t p_x = \frac{S(x+t)}{S(x)} = \frac{\omega-(x+t)}{\omega-x},
 \qquad 0 \leq t &lt; \omega-x,
&lt;/math&gt;
and the future lifetime random variable ''T''(''x'') therefore follows a [[Uniform distribution (continuous)|uniform distribution]] on 
&lt;math&gt;(0, \, \omega-x)&lt;/math&gt;. 

The [[Actuarial_notation#Life_tables |actuarial notation]] for conditional probability of failure is &lt;math&gt;{}_t q_x&lt;/math&gt; ''= Pr[0 ≤ T(x) ≤ t|T(0) ≥ x]''. Under de Moivre's law, the probability that ''(x)'' fails to survive to age ''x+t'' is 
:&lt;math&gt;
{}_t q_x = \frac{S(x)-S(x+t)}{S(x)} = \frac{t}{\omega-x}.
&lt;/math&gt;

The [[force of mortality]] ([[hazard rate]] or [[failure rate]]) is &lt;math&gt;\mu(x)=-S'(x)/S(x)=f(x)/S(x),&lt;/math&gt; where ''f(x)'' is the probability density function. Under de Moivre's law, the force of mortality for a life aged ''x'' is
:&lt;math&gt;
\mu(x+t) = \frac{1}{\omega - (x+t)}, \qquad 0 \leq t &lt; \omega-x,
&lt;/math&gt;
which has the property of an increasing failure rate with respect to age.

De Moivre's law is applied as a simple analytical law of mortality and the linear assumption is also applied as a model for interpolation for discrete survival models such as [[life table]]s.

==History==

[[File:De_Moivre_1725_page_25.jpg|thumb|de Moivre's illustration of his piecewise linear approximation]]
De Moivre's law first appeared in his 1725 ''Annuities upon Lives'', the earliest known example of an actuarial textbook.&lt;ref&gt;{{cite book|editor1-last=Haberman|editor1-first=Stephen|editor2-last=Sibbett|editor2-first=Trevor A.|title=History of Actuarial Science (Volume 1)|date=1995|publisher=William Pickering|location=London|isbn=1-85196-160-7|page=xxx|chapter=The History of Actuarial Science}}&lt;/ref&gt;  Despite the name now given to it, de Moivre himself did not consider his law (he called it a "hypothesis") to be a true description of the pattern of human mortality.  Instead, he introduced it as a useful approximation when calculating the cost of annuities.  In his text, de Moivre noted that " ... although the Notion of an equable Decrement of Life ... [does] not exactly agree with the ''Tables'', yet that Notion may successfully be employed in constructing a ''Table'' of the Values of ''Annuities'' for ''Ages'' not inferiour to ''Twelve'' ... ".&lt;ref&gt;page 20 of ''Annuities upon Lives''.  Italics and capitalization per original.&lt;/ref&gt;  Furthermore, although his text contained an algebraic demonstration that applied to the entire expected future life span, de Moivre also supplied an algebraic demonstration that applied only to a limited number of years.  It was this latter result that was used in his subsequent numerical examples.   These examples showed de Moivre using his hypothesis in a piecewise fashion, wherein he assumed that the overall pattern of human mortality could be approximated by several straight-line segments (see his illustration to the right).  He wrote that "since the Decrements of Life may without any sensible Error be supposed equal, for any short Interval of Time, it follows that if the whole Extent of Life be divided into several shorter Intervals, ... , the Values of ''Annuities'' for Life ... may easily be calculated ... conformably to any ''Table of Observations'', and for any ''Rate of Interest''".&lt;ref&gt;page 24 of ''Annuities upon Lives''.  Italics and capitalization per original.&lt;/ref&gt;  For both of the quotes, de Moivre's references to "tables" were to [[life table|actuarial life tables]].  

Modern authors are not consistent in their treatment of de Moivre's role in the history of mortality laws.  On the one hand, Dick London describes de Moivre's law as "the first [[continuous probability distribution]] to be suggested" for use as a model of human survival.&lt;ref&gt;{{cite book|last1=London|first1=Dick|title=Survival Models and their Estimation|date=1988|publisher=ACTEX Publications|location=Winsted, Connecticut|isbn=0-936031-02-6|page=17|edition=2nd}}&lt;/ref&gt;  Robert Batten takes a similar view, adding that "[de Moivre's] hypothesis .. has of course been found unrealistic".&lt;ref&gt;{{cite book|last1=Batten|first1=Robert W.|title=Mortality Table Construction|date=1978|publisher=Prentice-Hall|location=Englewood Cliffs, New Jersey|isbn=0-13-601302-3|page=3}}&lt;/ref&gt;  In contrast, the surveys of analytical human survival models by Spiegelman&lt;ref&gt;{{cite book|last1=Spiegelman|first1=Mortimer|title=Introduction to Demography|date=1968|publisher=Harvard University Press|location=Cambridge, Massachusetts|isbn=0-674-46100-2|pages=163-170|edition=Revised}}&lt;/ref&gt; and Benjamin&lt;ref&gt;{{cite journal | title=Demographic and Actuarial Aspects of Ageing, with Special Reference to England and Wales | author=Benjamin, B. | journal=Journal of the Institute of Actuaries | year=1964 | volume=90 | issue=Part III (No. 386) | pages=211-238}}  The survey starts at page 229.&lt;/ref&gt; do not mention de Moivre at all (in both cases, the surveys start with the work of [[Benjamin Gompertz]]).  In his essay on the history of actuarial science, Stephen Haberman does mention de Moivre, but in the section on "Life Insurance Mathematics" and not the one on "Life Tables and Survival Models".&lt;ref&gt;{{cite book|editor1-last=Haberman|editor1-first=Stephen|editor2-last=Sibbett|editor2-first=Trevor A.|title=History of Actuarial Science (Volume 1)|date=1995|publisher=William Pickering|location=London|isbn=1-85196-160-7|chapter=The History of Actuarial Science}}  The section on Life Tables and Survival Models appears at pages xxi through xxx; the section on Life Insurance Mathermatics appears at pages xxx through xl.&lt;/ref&gt;  A middle ground of sorts was taken by C. W. Jordan in his ''Life Contingencies'', where he included de Moivre in his section on "Some famous laws of mortality", but added that "de Moivre recognized that this was a very rough approximation [whose objective was] the practical one of simplifying the calculation of life annuity values, which in those days was an arduous task".&lt;ref&gt;{{cite book|last1=Jordan|first1=C. W.|title=Life Contingencies|date=1967|publisher=Society of Actuaries|pages=20-21|edition=2nd}}&lt;/ref&gt;

Another indication that de Moivre himself did not consider his "hypothesis" to be a true reflection of human mortality is the fact that he offered two distinct hypotheses in the ''Annuities upon Lives''.  When he turned his attention to the question of valuing annuities payable on more than one life, de Moivre found it convenient to drop his assumption of an equal number of deaths (per year) in favor of an assumption of equal probabilities of death at each year of age (i.e., what is now called the "constant [[force of mortality]]" assumption).  Although the constant-force assumption is also recognized today as a simple analytical law of mortality, it has never been known as "de Moivre's second law" or any other such name.&lt;ref&gt;De Moivre's introduction of his constant-force assumption starts at page 28 of ''Annuities upon Lives'' and is used extensively through page 49.&lt;/ref&gt;

==Notes==
{{reflist}}

==External links==
*[https://books.google.com/books?id=ed5bAAAAQAAJ&amp;printsec=frontcover#v=onepage&amp;q&amp;f=false 1725 edition of ''Annuities upon Lives'']

[[Category:Actuarial science]]
[[Category:Survival analysis]]</text>
      <sha1>g7ju5ca2e4zefb396vwgi3fo7wftlxy</sha1>
    </revision>
  </page>
  <page>
    <title>Defeasible logic</title>
    <ns>0</ns>
    <id>2634860</id>
    <revision>
      <id>710627843</id>
      <parentid>682486911</parentid>
      <timestamp>2016-03-18T02:33:14Z</timestamp>
      <contributor>
        <username>BG19bot</username>
        <id>14508071</id>
      </contributor>
      <minor/>
      <comment>/* References */Remove blank line(s) between list items per [[WP:LISTGAP]] to fix an accessibility issue for users of [[screen reader]]s. Do [[WP:GENFIXES]] and cleanup if needed. Discuss this at [[Wikipedia talk:WikiProject Accessibility#LISTGAP]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1276">'''Defeasible logic''' is a [[non-monotonic logic]] proposed by [[Donald Nute]] to formalize [[defeasible reasoning]]. In defeasible logic, there are three different types of propositions:

; strict rules : specify that a fact is always a consequence of another;
; defeasible rules : specify that a fact is typically a consequence of another;
; undercutting defeaters : specify exceptions to defeasible rules.

A priority ordering over the defeasible rules and the defeaters can be given. During the process of deduction, the strict rules are always applied, while a defeasible rule can be applied only if no defeater of a higher priority specifies that it should not.

==See also==

* [[Common sense]]
* [[Non-monotonic logic]]
* [[Default logic]]
* [[Defeasible reasoning]]

==References==

* D. Nute (1994). Defeasible logic. In ''Handbook of logic in artificial intelligence and logic programming'', volume 3: Nonmonotonic reasoning and uncertain reasoning, pages 353–395. Oxford University Press.
* G. Antoniou, D. Billington, G. Governatori, and M. Maher (2001). Representation results for defeasible logic. ''ACM Transactions on Computational Logic'', 2(2):255–287.

[[Category:Logic programming]]
[[Category:Non-classical logic]]


{{logic-stub}}
{{compu-AI-stub}}</text>
      <sha1>nsg26vclbcdzttulkli7g0bq3qdvp2x</sha1>
    </revision>
  </page>
  <page>
    <title>Direct sum</title>
    <ns>0</ns>
    <id>2084687</id>
    <revision>
      <id>855763832</id>
      <parentid>855763782</parentid>
      <timestamp>2018-08-20T17:32:42Z</timestamp>
      <contributor>
        <ip>2A06:BE00:1E6:0:38FA:B5F6:EE52:8936</ip>
      </contributor>
      <comment>/* Internal and external direct sums */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="11034">{{refimprove|date=December 2013}}
The '''direct sum''' is an operation from [[abstract algebra]], a branch of [[mathematics]].  For example, the direct sum &lt;math&gt; \mathbf{R} \oplus \mathbf{R} &lt;/math&gt;, where &lt;math&gt; \mathbf{R} &lt;/math&gt; is [[real coordinate space]], is the [[Cartesian plane]], &lt;math&gt; \mathbf{R} ^2 &lt;/math&gt;. To see how direct sum is used in abstract algebra, consider a more elementary structure in abstract algebra, the [[abelian group]].  The direct sum of two [[abelian group]]s &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; is another abelian group &lt;math&gt;A\oplus B&lt;/math&gt; consisting of the ordered pairs &lt;math&gt;(a,b)&lt;/math&gt; where &lt;math&gt;a \in A&lt;/math&gt; and &lt;math&gt;b \in B&lt;/math&gt;.  (Confusingly this ordered pair is also called the [[cartesian product]] of the two groups.) To add ordered pairs, we define the sum &lt;math&gt;(a, b) + (c, d)&lt;/math&gt; to be &lt;math&gt;(a + c, b + d)&lt;/math&gt;; in other words addition is defined coordinate-wise.  A similar process can be used to form the direct sum of any two algebraic structures, such as [[ring (mathematics)|rings]], [[module (mathematics)|modules]], and [[vector space]]s.

We can also form direct sums with any number of summands, for example &lt;math&gt;A \oplus B \oplus C&lt;/math&gt;, provided &lt;math&gt;A, B,&lt;/math&gt; and &lt;math&gt;C&lt;/math&gt; are the same kinds of algebraic structures, that is, all groups, rings, vector spaces, etc.

In the case of two summands, or any finite number of summands, the direct sum is the same as the [[direct product]].  If the arithmetic operation is written as +, as it usually is in abelian groups, then we use the direct sum.  If the arithmetic operation is written as × or ⋅ or using juxtaposition (as in the expression &lt;math&gt;xy&lt;/math&gt;) we use direct product.

In the case where infinitely many objects are combined, most authors make a distinction between direct sum and direct product.  As an example, consider the direct sum and direct product of infinitely many real lines.  An element in the direct product is an infinite sequence, such as (1,2,3,...) but in the direct sum, there would be a requirement that all but finitely many coordinates be zero, so the sequence (1,2,3,...) would be an element of the direct product but not of the direct sum, while (1,2,0,0,0,...) would be an element of both. More generally, if a + sign is used, all but finitely many coordinates must be zero, while if some form of multiplication is used, all but finitely many coordinates must be 1.  In more technical language, if the summands are &lt;math&gt;(A_i)_{i \in I}&lt;/math&gt;, the direct sum &lt;math&gt;\bigoplus_{i \in I} A_i&lt;/math&gt; is defined to be the set of tuples &lt;math&gt;(a_i)_{i \in I}&lt;/math&gt; with &lt;math&gt;a_i \in A_i&lt;/math&gt; such that &lt;math&gt;a_i=0&lt;/math&gt; for all but finitely many ''i''.  The direct sum &lt;math&gt;\bigoplus_{i \in I} A_i&lt;/math&gt; is contained in the [[direct product]] &lt;math&gt;\prod_{i \in I} A_i&lt;/math&gt;, but is usually strictly smaller when the [[index set]] &lt;math&gt;I&lt;/math&gt; is infinite, because direct products do not have the restriction that all but finitely many coordinates must be zero.&lt;ref&gt;[[Thomas W. Hungerford]], ''Algebra'', p.60, Springer, 1974, {{ISBN|0387905189}}&lt;/ref&gt;

==Examples==
For example, the ''xy''-plane, a two-dimensional [[vector space]], can be thought of as the direct sum of two one-dimensional vector spaces, namely the ''x'' and ''y'' axes. In this direct sum, the ''x'' and ''y'' axes intersect only at the origin (the zero vector).  Addition is defined coordinate-wise, that is &lt;math&gt;(x_1,y_1) + (x_2,y_2) = (x_1+x_2, y_1 + y_2)&lt;/math&gt;, which is the same as vector addition.

Given two objects &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt;, their direct sum is written as &lt;math&gt;A\oplus B&lt;/math&gt;.  Given an [[indexed family]] of objects &lt;math&gt;A_i&lt;/math&gt;, indexed with &lt;math&gt;i \in I&lt;/math&gt;, the direct sum may be written &lt;math&gt;\textstyle A=\bigoplus_{i\in I}A_i&lt;/math&gt;.  Each ''A&lt;sub&gt;i&lt;/sub&gt;'' is called a '''direct summand''' of ''A''.  If the index set is finite, the direct sum is the same as the direct product.  In the case of groups, if the group operation is written as &lt;math&gt;+&lt;/math&gt; the phrase "direct sum" is used, while if the group operation is written &lt;math&gt;*&lt;/math&gt; the phrase "direct product" is used.  When the index set is infinite, the direct sum is not the same as the direct product.  In the direct sum, all but finitely many coordinates must be zero.

===Internal and external direct sums===

A distinction is made between internal and external direct sums, though the two are [[isomorphic]].  If the factors are defined first, and then the direct sum is defined in terms of the factors, we have an external direct sum.  For example, if we define the real numbers &lt;math&gt;\mathbf{R}&lt;/math&gt; and then define &lt;math&gt;\mathbf{R} \oplus \mathbf{R}&lt;/math&gt; the direct sum is said to be external.

If, on the other hand, we first define some algebraic object, &lt;math&gt;S&lt;/math&gt; and then write &lt;math&gt;S&lt;/math&gt; as the direct sum of two of its subsets, &lt;math&gt;V&lt;/math&gt; and &lt;math&gt;W&lt;/math&gt;, then the direct sum is said to be internal.  In this case, each element of &lt;math&gt;S&lt;/math&gt; is expressible uniquely as an algebraic combination of an element of &lt;math&gt;V&lt;/math&gt; and an element of &lt;math&gt;W&lt;/math&gt;.  For an example of an internal direct sum, consider &lt;math&gt;Z_6&lt;/math&gt;, the integers modulo six, whose elements are &lt;math&gt;\{0, 1, 2, 3, 4, 5\}&lt;/math&gt;.  This is expressible as an internal direct sum &lt;math&gt; Z_6 =\{1, 3, 5\} \oplus \{0, 2, 4\}&lt;/math&gt;.

==Types of direct sum==

===Direct sum of abelian groups===
The '''direct sum of abelian groups''' is a prototypical example of a direct sum. Given two [[abelian groups]] &lt;math&gt;(A, \circ)&lt;/math&gt; and &lt;math&gt;(B, \bullet)&lt;/math&gt;, their direct sum &lt;math&gt;A \oplus B&lt;/math&gt; is the same as their [[direct product of groups|direct product]], that is the underlying set is the Cartesian product &lt;math&gt;A \times B&lt;/math&gt; and the group operation &lt;math&gt;\cdot&lt;/math&gt; is defined component-wise:
:&lt;math&gt;(a_1, b_1) \cdot (a_2, b_2) = (a_1 \circ a_2, b_1 \bullet b_2)&lt;/math&gt;.
This definition generalizes to direct sums of finitely many abelian groups.

For an infinite family of abelian groups ''A&lt;sub&gt;i&lt;/sub&gt;'' for ''i'' ∈ ''I'', the direct sum
:&lt;math&gt;\bigoplus_{i \in I}A_i&lt;/math&gt;
is a [[proper subgroup]] of the direct product. It consists of the elements &lt;math&gt;\textstyle (a_i)\in\prod_{j \in I}A_j&lt;/math&gt; such that ''a&lt;sub&gt;i&lt;/sub&gt;'' is the identity element of ''A&lt;sub&gt;i&lt;/sub&gt;'' for all but finitely many ''i''.&lt;ref&gt;Joseph J. Rotman, ''The Theory of Groups: an Introduction'', p. 177, Allyn and Bacon, 1965&lt;/ref&gt;

===Direct sum of modules===
{{main|Direct sum of modules}}
The ''direct sum of modules'' is a construction which combines several [[module (mathematics)|modules]] into a new module.

The most familiar examples of this construction occur when considering [[vector space]]s, which are modules over a [[field (mathematics)|field]].  The construction may also be extended to [[Banach space]]s and [[Hilbert space]]s.

===Direct sum of group representations===
{{See also|Representation theory of finite groups#Direct sum of representations}}

The '''direct sum of group representations''' generalizes the [[direct sum of modules|direct sum]] of the underlying [[module (mathematics)|modules]], adding a [[group action]] to it. Specifically, given a [[group (mathematics)|group]] ''G'' and two [[group representation|representations]] ''V'' and ''W'' of ''G'' (or, more generally, two [[G-module|''G''-modules]]), the direct sum of the representations is ''V'' ⊕ ''W'' with the action of ''g'' ∈ ''G'' given component-wise, i.e.
:''g''·(''v'', ''w'') = (''g''·''v'', ''g''·''w'').

===Direct sum of rings===
{{main|Product of rings}}
Some authors will speak of the direct sum &lt;math&gt;R \oplus S&lt;/math&gt; of two rings when they mean the [[direct product]] &lt;math&gt;R \times S&lt;/math&gt;, but this should be avoided&lt;ref&gt;[http://math.stackexchange.com/questions/345501/is-a-times-b-the-same-as-a-oplus-b Math StackExchange] on direct sum of rings vs. direct product of rings.&lt;/ref&gt; since &lt;math&gt;R \times S&lt;/math&gt; does not receive natural ring homomorphisms from ''R'' and ''S'': in particular, the map &lt;math&gt;R \to R \times S&lt;/math&gt; sending ''r'' to (''r'',0) is not a ring homomorphism since it fails to send 1 to (1,1) (assuming that 0≠1 in ''S'').  Thus &lt;math&gt;R \times S&lt;/math&gt; is not a coproduct in the [[category of rings]], and should not be written as a direct sum.  (The coproduct in the [[category of commutative rings]] is the [[tensor product of rings]].&lt;ref&gt;{{harvnb|Lang|2002}}, section I.11&lt;/ref&gt; In the category of rings, the coproduct is given by a construction similar to the [[free product]] of groups.)

Use of direct sum terminology and notation is especially problematic when dealing with infinite families of rings: If &lt;math&gt;(R_i)_{i \in I}&lt;/math&gt; is an infinite collection of nontrivial rings, then the direct sum of the underlying additive groups can be equipped with termwise multiplication, but this produces a [[rng (algebra)|rng]], i.e., a ring without a multiplicative identity.

===Direct sum in categories===
An [[additive category]] is an abstraction of the properties of the category of modules.&lt;ref&gt;[http://www.math.jussieu.fr/~schapira/lectnotes/HomAl.pdf "p.45"]&lt;/ref&gt;
&lt;ref&gt;[http://www.princeton.edu/~hhalvors/aqft.pdf "appendix"]&lt;/ref&gt;

In such a category finite products and coproducts agree and the direct sum is either of them, cf. [[biproduct]].

General case : &lt;ref&gt;[http://ncatlab.org/nlab/show/direct+sum]&lt;/ref&gt;
In [[category theory]] the direct sum is often, but not always, the [[coproduct]] in the [[Category (mathematics)|category]] of the mathematical objects in question.  For example, in the category of abelian groups, direct sum is a coproduct.  This is also true in the category of modules.

==Homomorphisms==
{{clarify|date=February 2015}}&lt;!--the context is unclear. --&gt;
The direct sum &lt;math&gt;\bigoplus_{i \in I} A_i&lt;/math&gt; comes equipped with a ''[[Projection (mathematics)|projection]]'' [[homomorphism]] &lt;math&gt;\pi_j \colon \bigoplus_{i \in I} A_i \to A_j&lt;/math&gt; for each ''j''  and a ''coprojection'' &lt;math&gt;\alpha_j \colon A_j \to \bigoplus_{i \in I} A_i&lt;/math&gt; for each ''j''.&lt;ref name=Heu26&gt;{{cite book | title=Categorical Quantum Models and Logics | series=Pallas Proefschriften | first=Chris | last=Heunen | publisher=Amsterdam University Press | year=2009 | isbn=9085550246 | page=26 }}&lt;/ref&gt;  Given another algebraic object ''B'' (with the same additional structure) and homomorphisms &lt;math&gt;g_j \colon A_j \to B&lt;/math&gt; for every ''j'', there is a unique homomorphism &lt;math&gt;g \colon \bigoplus_{i \in I} A_i \to B&lt;/math&gt; (called the sum of the ''g''&lt;sub&gt;''j''&lt;/sub&gt;) such that &lt;math&gt;g \alpha_j =g_j&lt;/math&gt; for all ''j''.  Thus the direct sum is the [[coproduct]] in the appropriate [[category (mathematics)|category]].

==See also==
*[[Direct sum of groups]]
*[[Direct sum of permutations]]
*[[Direct sum of topological groups]]
*[[Restricted product]]
*[[Whitney sum]]

==Notes==
{{reflist}}

==References==
*{{Lang Algebra|edition=3r}}

[[Category:Abstract algebra]]</text>
      <sha1>oo6qcsuy636idm2wo8d99mwsn9yeu01</sha1>
    </revision>
  </page>
  <page>
    <title>Electrocardiography</title>
    <ns>0</ns>
    <id>76988</id>
    <revision>
      <id>870507898</id>
      <parentid>870507816</parentid>
      <timestamp>2018-11-25T07:53:08Z</timestamp>
      <contributor>
        <ip>202.71.14.105</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="62062">{{Redirect2|ECG|EKG|other uses|ECG (disambiguation)|and|EKG (disambiguation)}}
{{distinguish|text=[[electrography (disambiguation)|other types of electrography]] or with [[echocardiography]]}}
{{Use dmy dates|date=May 2014}}
{{Infobox medical intervention
 | Name        = Electrocardiography
 | Image       = SinusRhythmLabels.svg
 | Caption     = ECG of a heart in normal [[sinus rhythm]]
 | ICD10       = R94.31
 | ICD9        = 89.52
 | MeshID      = D004562
 | MedlinePlus = 003868
 | OtherCodes  =
}}

'''Electrocardiography''' ('''ECG''' or '''EKG'''{{efn|The version with '-K-', more commonly used in [[American English]] than in [[British English]], is an early-20th-century [[loanword]] from the German acronym EKG for {{lang|de|Elektrokardiogramm}} (electrocardiogram),&lt;ref&gt;[http://www.oxforddictionaries.com/us/definition/english/EKG EKG]. Oxford Online Dictionaries&lt;/ref&gt; which reflects that German physicians were pioneers in the field at the time. Today [[AMA Manual of Style|AMA style]] and – under its stylistic influence – most American medical publications use ECG instead of EKG.&lt;ref name="AMA_MOS"&gt;{{Citation |publisher=[[American Medical Association]] |title=AMA Manual of Style |url=http://www.amamanualofstyle.com/ |section=15.3.1 Electrocardiographic Terms}}&lt;/ref&gt; The German term {{lang|de|Elektrokardiogramm}} as well as the English equivalent, electrocardiogram, consist of the [[New Latin]]/[[international scientific vocabulary]] elements {{lang|de|[[wikt:elektro-#German|elektro-]]}} (cognate {{lang|de|[[wikt:electro-#English|electro-]]}}) and {{lang|de|kardi-}} (cognate 'cardi-'), the latter from Greek {{lang|el|kardia}} (heart).&lt;ref name="MW_Collegiate"&gt;{{Cite web |title=Merriam-Webster's Collegiate Dictionary |publisher=[[Merriam-Webster]] |url=http://unabridged.merriam-webster.com/collegiate/ |url-access=subscription}}&lt;/ref&gt; The '-K-' version is more often retained under circumstances where there may be verbal confusion between ECG and EEG ([[electroencephalography]]) due to similar pronunciation.}}) is the process of recording the electrical activity of the [[heart]]&lt;ref&gt;{{cite book |editor1-last=Lilly |editor1-first=Leonard S |title=Pathophysiology of Heart Disease: A Collaborative Project of Medical Students and Faculty |edition=sixth |publisher=Lippincott Williams &amp; Wilkins |page=74 |date=2016 |isbn=978-1451192759 |ref=harv}}&lt;/ref&gt; over a period of time using [[electrode]]s placed over the skin. These electrodes detect the tiny electrical changes on the skin that arise from the [[cardiac muscle|heart muscle]]'s [[electrophysiology|electrophysiologic]] pattern of [[depolarization|depolarizing]] and [[repolarization|repolarizing]] during each [[cardiac cycle|heartbeat]]. It is very commonly performed to detect any cardiac problems.

In a conventional 12-lead ECG, ten electrodes are placed on the patient's limbs and on the surface of the chest. The overall [[Magnitude (mathematics)|magnitude]] of the heart's [[electrical potential]] is then measured from twelve different angles ("leads") and is recorded over a period of time (usually ten seconds). In this way, the overall magnitude and direction of the heart's electrical depolarization is captured at each moment throughout the [[cardiac cycle]].&lt;ref name="LHC"&gt;{{cite web | url=http://www.lifehugger.com/doc/120/ecg-100-steps | title=ECG- simplified|author=Aswini Kumar MD | publisher=LifeHugger | accessdate=11 February 2010}}&lt;/ref&gt; The graph of [[voltage]] versus time produced by this [[Non-invasive (medical)|noninvasive]] medical procedure is an '''electrocardiogram'''.

There are three main components to an ECG: the [[P wave (electrocardiography)|P wave]], which represents the depolarization of the atria; the [[QRS complex]], which represents the depolarization of the ventricles; and the [[T wave]], which represents the repolarization of the ventricles.{{sfn|Lilly|2016|pp=80}} It can also be further broken down into the following:

* O is the origin or datum point preceding the cycle 
* P is the atrial systole contraction pulse
* Q is a downward deflection immediately preceding the ventricular contraction
* R is the peak of the ventricular contraction
* S is the downward deflection immediately after the ventricular contraction
* T is the recovery of the ventricles
* U is the successor of the T wave but it is small and not always observed

During each heartbeat, a healthy heart has an orderly progression of depolarization that starts with [[pacemaker cells]] in the [[sinoatrial node]], spreads throughout the [[Atrium (heart)|atrium]], passes through the [[atrioventricular node]] down into the [[bundle of His]] and into the [[Purkinje fibers]], spreading down and to the left throughout the [[Ventricle (heart)|ventricle]]s.{{sfn|Lilly|2016|pp=80}} This orderly pattern of depolarization gives rise to the characteristic ECG tracing. To the trained [[clinician]], an ECG conveys a large amount of information about the structure of the heart and the function of its electrical conduction system.&lt;ref&gt;Walraven, G. (2011). ''Basic arrhythmias'' (7th ed.), pp. 1–11&lt;/ref&gt; Among other things, an ECG can be used to measure the rate and rhythm of heartbeats, the size and position of the [[heart chambers]], the presence of any damage to the heart's muscle cells or conduction system, the effects of heart drugs, and the function of implanted [[Artificial pacemaker|pacemakers]].&lt;ref&gt;Braunwald E. (ed) (1997), ''Heart Disease: A Textbook of Cardiovascular Medicine, Fifth Edition'', p. 108, Philadelphia, W.B. Saunders Co.. {{ISBN|0-7216-5666-8}}.&lt;/ref&gt;

==Medical uses ==
[[File:12leadECG.jpg|thumb|325px|A 12-lead ECG of a 26-year-old male with an incomplete [[RBBB]]]]

The overall goal of performing an ECG is to obtain information about the structure and function of the heart. Medical uses for this information are varied and generally need knowledge of the structure and/or function of the heart to be interpreted. Some [[Indication (medicine)|indications]] for performing an ECG include:

* Suspected [[myocardial infarction]] (heart attack) or [[chest pain]]
**ST elevated myocardial infarction (STEMI)&lt;ref&gt;{{Cite news|url=https://www.ecgmedicaltraining.com/what-is-a-stemi/|title=What is a STEMI? - ECG Medical Training|date=2015-06-24|work=ECG Medical Training|access-date=2018-06-24|language=en-US}}&lt;/ref&gt;
**non-ST elevated myocardial infarction (NSTEMI)&lt;ref&gt;{{Cite news|url=https://myheart.net/articles/nstemi/|title=What is NSTEMI? What You NEED to Know • MyHeart|date=2015-04-30|work=MyHeart|access-date=2018-06-24|language=en-US}}&lt;/ref&gt;
* Suspected [[pulmonary embolism]] or [[shortness of breath]]
* A [[third heart sound]], [[fourth heart sound]], a [[cardiac murmur]]&lt;ref name=masters&gt;{{cite book |author1=Masters, Jo |author2=Bowden, Carole |author3=Martin, Carole |title=Textbook of veterinary medical nursing |publisher=Butterworth-Heinemann |location=Oxford |year=2003 |page=244 |isbn=978-0-7506-5171-4}}&lt;/ref&gt; or other findings suggestive of a structural heart disease
* Perceived [[cardiac dysrhythmia|arrhythmia]] either by pulse or [[palpitations]]
* Monitoring of known cardiac arrhythmias
* [[Syncope (medicine)|Fainting]] or collapse&lt;ref name=masters/&gt;
* [[Seizure]]s&lt;ref name=masters/&gt;
* Monitoring the effects of a medication on the heart (e.g. [[drug-induced QT prolongation]])
* Assessing severity of [[Electrolyte imbalance|electrolyte abnormalities]], such as [[hyperkalemia]]
* [[Hypertrophic cardiomyopathy]] screening in adolescents as part of a [[sports physical]] out of concern for [[sudden cardiac death]] (varies by country)
* [[Perioperative]] monitoring in which any form of [[anesthesia]] is involved (e.g. [[Intraoperative neurophysiological monitoring|monitored anesthesia care]], [[general anesthesia]]); typically both intraoperative and postoperative
* As a part of a [[Preoperative care|preoperative assessment]] some time before a surgical procedure (especially for those with known cardiovascular disease or who are undergoing invasive, cardiac, vascular or pulmonary procedures, or who will receive [[General anaesthesia|general anesthesia]])
* [[Cardiac stress test]]ing
* [[Computed tomography angiography]] (CTA) and [[magnetic resonance angiography]] (MRA) of the heart (ECG is used to "gate" the scanning so that the anatomical position of the heart is steady)
* [[Biotelemetry]] of patients for any of the above reasons and such monitoring can include internal and external [[defibrillator]]s and [[pacemaker]]s

The [[United States Preventive Services Task Force]] does not recommend an ECG for routine screening in patients without symptoms and those at low risk for [[coronary heart disease|coronary artery disease]].&lt;ref name=Annals2012&gt;{{cite journal | author = Moyer VA | title = Screening for coronary heart disease with electrocardiography: U.S. Preventive Services Task Force recommendation statement | journal = Annals of Internal Medicine | volume = 157 | issue = 7 | pages = 512–18 | date = 2 October 2012 | pmid = 22847227 | doi = 10.7326/0003-4819-157-7-201210020-00514 }}&lt;/ref&gt;&lt;ref name="whenyouneedEKGs"&gt;{{Citation | author1 = [[Consumer Reports]]| author2 = [[American Academy of Family Physicians]] |author3=[[ABIM Foundation]]| date = April 2012| title = EKGs and exercise stress tests: When you need them for heart disease — and when you don't|publisher = [[Consumer Reports]]| work = Choosing Wisely |page = |url = http://consumerhealthchoices.org/wp-content/uploads/2012/04/ChoosingWiselyEKGAAFP2.pdf| accessdate = 14 August 2012}}&lt;/ref&gt; This is because an ECG may falsely indicate the existence of a problem, leading to [[misdiagnosis]], the recommendation of invasive procedures, or [[overtreatment]]. However, persons employed in certain critical occupations, such as aircraft pilots,&lt;ref name="FAA Medical Standards 2006"&gt;{{cite web|url=http://www.faa.gov/about/office_org/headquarters_offices/avs/offices/aam/ame/guide/media/synopsis.pdf|title=Summary of Medical Standards|year=2006|accessdate=27 December 2013|publisher=U.S. Federal Aviation Administration}}&lt;/ref&gt; may be required to have an ECG as part of their routine health evaluations.

''Continuous'' ECG monitoring is used to monitor critically ill patients, patients undergoing general anesthesia,&lt;ref name=masters/&gt; and patients who have an infrequently occurring cardiac arrhythmia that would unlikely be seen on a conventional ten-second ECG.

In the United States, a 12-lead ECG is commonly performed by specialized technicians that may be certified as [[electrocardiogram technician]]s.
ECG interpretation is a component of many healthcare fields (nurses and physicians and [[cardiac surgeons]] being the most obvious), but anyone trained to interpret an ECG is free to do so.
However, "official" interpretation is performed by a [[cardiologist]].
Certain fields such as anesthesia utilize continuous ECG monitoring, and knowledge of interpreting ECGs is crucial to their jobs.

One additional form of ECG is used in [[clinical cardiac electrophysiology]] in which a [[catheter]] is used to measure the electrical activity.
The catheter is inserted through the [[femoral vein]] and can have several electrodes along its length to record the direction of electrical activity from within the heart.

Evidence does not support the use of ECGs among those without symptoms or at low risk of [[cardiovascular disease]] as an effort for prevention.&lt;ref&gt;{{cite journal |last1=US Preventive Services Task |first1=Force. |last2=Curry |first2=SJ |last3=Krist |first3=AH |last4=Owens |first4=DK |last5=Barry |first5=MJ |last6=Caughey |first6=AB |last7=Davidson |first7=KW |last8=Doubeni |first8=CA |last9=Epling JW |first9=Jr |last10=Kemper |first10=AR |last11=Kubik |first11=M |last12=Landefeld |first12=CS |last13=Mangione |first13=CM |last14=Silverstein |first14=M |last15=Simon |first15=MA |last16=Tseng |first16=CW |last17=Wong |first17=JB |title=Screening for Cardiovascular Disease Risk With Electrocardiography: US Preventive Services Task Force Recommendation Statement |journal=JAMA |date=12 June 2018 |volume=319 |issue=22 |pages=2308–2314 |doi=10.1001/jama.2018.6848 |pmid=29896632}}&lt;/ref&gt;

==Electrocardiographs==
[[File:De-Modern ecg (CardioNetworks ECGpedia).jpg|thumb|300px|An electrocardiograph with integrated display and keyboard on a wheeled cart]]

An electrocardiograph is a machine that is used to perform electrocardiography, and produces the electrocardiogram.
The first electrocardiographs are discussed later and are electrically primitive compared to today's machines.

The fundamental component to an ECG is the [[instrumentation amplifier]], which is responsible for taking the [[voltage difference]] between leads (see below) and amplifying the signal.
ECG voltages measured across the body are on the order of hundreds of micro[[Volt|volts]] up to 1 millivolt (the small square on a standard ECG is 100 microvolts).
This low voltage necessitates a low [[Noise (electronics)|noise]] circuit and instrumentation amplifiers.

Early ECGs were constructed with [[analog electronics]] and the signal could drive a motor to print the signal on paper.
Today, electrocardiographs use [[analog-to-digital converter]]s to convert to a [[digital signal]] that can then be manipulated with [[digital electronics]].
This permits digital recording of ECGs and use on computers.

There are other components to the ECG:&lt;ref&gt;{{cite web|title=Mitigation Strategies for ECG Design Challenges|url=http://www.analog.com/media/en/technical-documentation/technical-articles/MS-2160.pdf|website=Analog Devices|accessdate=24 April 2016}}&lt;/ref&gt;
* Safety features that include voltage protection for the patient and operator. Since the machines are powered by [[mains power]], it is conceivable that either person could be subjected to voltage capable of causing death. Additionally, the heart is sensitive to the [[Alternating current|AC]] frequencies typically used for mains power (50 or 60&amp;nbsp;[[Hertz|Hz]]).
* [[Defibrillation]] protection: any ECG used in healthcare may be attached to a person who requires defibrillation and the ECG needs to protect itself from this source of energy.
* [[Electrostatic discharge]] is similar to defibrillation discharge and requires voltage protection up to 18,000 volts.
* Additionally circuitry called the [[Driven right leg circuit|right leg driver]] can be used to reduce [[common-mode interference]] (typically the 50 or 60&amp;nbsp;Hz mains power).

The typical design for a portable ECG is a combined unit that includes a screen, keyboard, and printer on a small wheeled cart.
The unit connects to a long cable that branches to each lead and attaches to a conductive pad on the patient.

The ECG may include a rhythm analysis [[algorithm]] that produces a computerized interpretation of the ECG.
The results from these algorithms are considered "preliminary" until verified and/or modified by someone trained in interpreting ECGs.
Included in this analysis is computation of common parameters that include [[PR interval]], [[QT interval]], corrected QT (QTc) interval, PR axis, QRS axis, and more.
Earlier designs recorded each lead sequentially but current designs employ circuits that can record all leads simultaneously.
The former introduces problems in interpretation since there may be beat-to-beat changes in the rhythm, which makes it unwise to compare across beats.

More recent advancements in electrocardiography include work in diminishing the size of the unit to make it more portable and therefore more accessible to larger groups of patients. To achieve this, these smaller devices rely on only two electrodes which together deliver "lead I" of the standard ECG.&lt;ref&gt;{{Cite news|url=https://techcrunch.com/2012/12/04/mobile-health-moves-forward-fda-approves-alivecors-heart-monitor-for-the-iphone/|title=FDA approves AliveCor heart monitor|work=Techcrunch|access-date=2018-08-25|language=en-US}}&lt;/ref&gt;

==Electrodes and leads==
[[File:Limb leads.svg|thumb|Proper placement of the limb electrodes. The limb electrodes can be far down on the limbs or close to the hips/shoulders as long as they are placed symmetrically.&lt;ref&gt;{{Cite web|url=http://www.scst.org.uk/resources/RESTING_12.pdf|title=Resting 12-Lead Electrode|last=Macfarlane|first=P.W.|last2=Coleman|date=1995|website=Society for Cardiological Science and Technology|archive-url=|archive-date=|dead-url=|access-date=21 October 2017}}&lt;/ref&gt;]]
[[File:Precordial leads in ECG.png|thumb|Placement of the precordial electrodes]]
Electrodes are the actual conductive pads attached to the body surface.  Any pair of electrodes can measure the [[electrical potential difference]] between the two corresponding locations of attachment.  Such a pair forms ''a lead''.  However, "leads" can also be formed between a physical electrode and a ''virtual electrode,'' known as ''the Wilson's central terminal'', whose potential is defined as the average potential measured by three limb electrodes that are attached to the right arm, the left arm, and the left foot, respectively.  

Commonly, 10 electrodes attached to the body are used to form 12 ECG leads, with each lead measuring a specific electrical potential difference (as listed in the table below).{{citation needed|date=April 2017}}

Leads are broken down into three types: limb; augmented limb; and precordial or chest. The 12-lead ECG has a total of three ''limb leads'' and three ''augmented limb leads'' arranged like spokes of a wheel in the [[coronal plane]] (vertical), and six ''precordial leads'' or ''chest leads'' that lie on the perpendicular [[transverse plane]] (horizontal).{{citation needed|date=April 2017}}

In medical settings, the term ''leads'' is also sometimes used to refer to the electrodes themselves, although this is technically incorrect.  This misuse of terminology can be the source of confusion.{{citation needed|date=April 2017}}

The 10 electrodes in a 12-lead ECG are listed below.&lt;ref name="CablesAndSensors"&gt;{{cite web|title=12-Lead ECG Placement Guide with Illustrations|url=https://www.cablesandsensors.eu/pages/12-lead-ecg-placement-guide-with-illustrations|website=Cables and Sensors|accessdate=11 July 2017}}&lt;/ref&gt;

{| class="wikitable"
|-
! Electrode name
! Electrode placement
|-
| RA
| On the right arm, avoiding thick [[muscle]].
|-
| LA
| In the same location where RA was placed, but on the left arm.
|-
| RL
| On the right leg, lower end of inner aspect of [[Triceps surae muscle|calf muscle]]. (Avoid bony prominences)
|-
| LL
| In the same location where RL was placed, but on the left leg.
|-
| V&lt;sub&gt;1&lt;/sub&gt;
| In the fourth [[intercostal space]] (between ribs 4 and 5) just to the right of the [[sternum]] (breastbone).
|-
| V&lt;sub&gt;2&lt;/sub&gt;
| In the fourth intercostal space (between ribs 4 and 5) just to the left of the sternum.
|-
| V&lt;sub&gt;3&lt;/sub&gt;
| Between leads V&lt;sub&gt;2&lt;/sub&gt; and V&lt;sub&gt;4&lt;/sub&gt;.
|-
| V&lt;sub&gt;4&lt;/sub&gt;
| In the fifth intercostal space (between ribs 5 and 6) in the [[mid-clavicular line]].
|-
| V&lt;sub&gt;5&lt;/sub&gt;
| Horizontally even with V&lt;sub&gt;4&lt;/sub&gt;, in the left [[anterior axillary line]].
|-
| V&lt;sub&gt;6&lt;/sub&gt;
| Horizontally even with V&lt;sub&gt;4&lt;/sub&gt; and V&lt;sub&gt;5&lt;/sub&gt; in the [[midaxillary line|mid-axillary line]].
|}

Two types of electrodes in common use are a flat paper-thin sticker and a self-adhesive circular pad.
The former are typically used in a single ECG recording while the latter are for continuous recordings as they stick longer.
Each electrode consists of an [[electrically conductive]] electrolyte gel and a [[Silver chloride electrode#Biological electrode systems|silver/silver chloride]] conductor.&lt;ref&gt;{{Cite book|last1=Kavuru|first1=Madhav S.|last2=Vesselle|first2=Hubert|last3=Thomas|first3=Cecil W.|title=Advances in Body Surface Potential Mapping (BSPM) Instrumentation|journal=Pediatric and Fundamental Electrocardiography|volume=56|date=1987|pages=315–327|doi=10.1007/978-1-4613-2323-5_15|series=Developments in Cardiovascular Medicine|issn=0166-9842|isbn=978-1-4612-9428-3}}&lt;/ref&gt;
The gel typically contains [[potassium chloride]] – sometimes [[silver chloride]] as well – to permit [[electron]] conduction from the skin to the wire and to the electrocardiogram.

The common virtual electrode, known as the Wilson's central terminal (V&lt;sub&gt;W&lt;/sub&gt;), is produced by averaging the measurements from the electrodes RA, LA, and LL to give an average potential of the body:
:&lt;math&gt;
V_W = \frac{1}{3}(RA+LA+LL)
&lt;/math&gt;
In a 12-lead ECG, all leads except the limb leads are unipolar (aVR, aVL, aVF, V&lt;sub&gt;1&lt;/sub&gt;, V&lt;sub&gt;2&lt;/sub&gt;, V&lt;sub&gt;3&lt;/sub&gt;, V&lt;sub&gt;4&lt;/sub&gt;, V&lt;sub&gt;5&lt;/sub&gt;, and V&lt;sub&gt;6&lt;/sub&gt;).
The measurement of a voltage requires two contacts and so, electrically, the unipolar leads are measured from the common lead (negative) and the unipolar lead (positive).
This averaging for the common lead and the abstract unipolar lead concept makes for a more challenging understanding and is complicated by sloppy usage of "lead" and "electrode."

===Limb leads===
[[File:Limb leads of EKG.png|600px|thumbnail|right|The limb leads and augmented limb leads (Wilson's central terminal is used as the negative pole for the latter in this representation)]]
[[File:EKG leads.png|300px|thumb]]
Leads I, II and III are called the ''limb leads''. The electrodes that form these signals are located on the limbs – one on each arm and one on the left leg.&lt;ref&gt;{{Cite web|url=https://www.cablesandsensors.com/pages/12-lead-ecg-placement-guide-with-illustrations|title=12-Lead ECG Placement Guide with Illustrations {{!}} Cables and Sensors|last=Sensors|first=Cables and|website=Cables and Sensors|access-date=2017-10-21}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://www.nottingham.ac.uk/nursing/practice/resources/cardiology/function/limb_leads.php |title=Limb Leads – ECG Lead Placement – Normal Function of the Heart – Cardiology Teaching Package – Practice Learning – Division of Nursing – The University of Nottingham |publisher=Nottingham.ac.uk |accessdate=15 August 2009}}&lt;/ref&gt;&lt;ref&gt;{{cite web|url=http://library.med.utah.edu/kw/ecg/ecg_outline/Lesson1/index.html#orientation |title=Lesson 1: The Standard 12 Lead ECG |publisher=Library.med.utah.edu |accessdate=15 August 2009 |deadurl=yes |archiveurl=https://web.archive.org/web/20090322042804/http://library.med.utah.edu/kw/ecg/ecg_outline/Lesson1/index.html |archivedate=22 March 2009 }}&lt;/ref&gt;  The limb leads form the points of what is known as [[Einthoven's triangle]].&lt;ref&gt;{{Cite journal|last=Jin|first=Benjamin E.|last2=Wulff|first2=Heike|last3=Widdicombe|first3=Jonathan H.|last4=Zheng|first4=Jie|last5=Bers|first5=Donald M.|last6=Puglisi|first6=Jose L.|date=December 2012|title=A simple device to illustrate the Einthoven triangle|journal=Advances in Physiology Education|volume=36|issue=4|pages=319–24|doi=10.1152/advan.00029.2012|issn=1043-4046|pmc=3776430|pmid=23209014|via=}}&lt;/ref&gt;
*Lead I is the voltage between the (positive) left arm (LA) electrode and right arm (RA) electrode:
:&lt;math&gt;
I = LA - RA
&lt;/math&gt;

*Lead II is the voltage between the (positive) left leg (LL) electrode and the right arm (RA) electrode:
:&lt;math&gt;
II = LL - RA
&lt;/math&gt;

*Lead III is the voltage between the (positive) left leg (LL) electrode and the left arm (LA) electrode:
:&lt;math&gt;
III = LL - LA
&lt;/math&gt;

===Augmented limb leads===
Leads aVR, aVL, and aVF are the ''augmented limb leads''. They are derived from the same three electrodes as leads I, II, and III, but they use Goldberger's central terminal as their negative pole. Goldberger's central terminal is a combination of inputs from two limb electrodes, with a different combination for each augmented lead. It is referred to immediately below as "the negative pole."

*Lead ''augmented vector right'' (aVR) has the positive electrode on the right arm. The negative pole is a combination of the left arm electrode and the left leg electrode:
:&lt;math&gt;
aVR = RA - \frac{1}{2} (LA + LL) = \frac 32 (RA - V_W)
&lt;/math&gt;

*Lead ''augmented vector left'' (aVL) has the positive electrode on the left arm. The negative pole is a combination of the right arm electrode and the left leg electrode:
:&lt;math&gt;
aVL = LA - \frac{1}{2} (RA + LL) = \frac 32 (LA - V_W)
&lt;/math&gt;

*Lead ''augmented vector foot'' (aVF) has the positive electrode on the left leg. The negative pole is a combination of the right arm electrode and the left arm electrode:
:&lt;math&gt;
aVF = LL - \frac{1}{2} (RA + LA) = \frac 32 (LL - V_W)
&lt;/math&gt;

Together with leads I, II, and III, augmented limb leads aVR, aVL, and aVF form the basis of the [[hexaxial reference system]], which is used to calculate the heart's electrical axis in the frontal plane.

===Precordial leads===
The ''precordial leads'' lie in the transverse (horizontal) plane, perpendicular to the other six leads. The six precordial electrodes act as the positive poles for the six corresponding precordial leads: (V&lt;sub&gt;1&lt;/sub&gt;, V&lt;sub&gt;2&lt;/sub&gt;, V&lt;sub&gt;3&lt;/sub&gt;, V&lt;sub&gt;4&lt;/sub&gt;, V&lt;sub&gt;5&lt;/sub&gt; and V&lt;sub&gt;6&lt;/sub&gt;). Wilson's central terminal is used as the negative pole.

===Specialized leads===
Additional electrodes may rarely be placed to generate other leads for specific diagnostic purposes. ''Right-sided'' precordial leads may be used to better study pathology of the right ventricle or for [[dextrocardia]] (and are denoted with an R (e.g., V&lt;sub&gt;5R&lt;/sub&gt;). ''Posterior leads'' (V&lt;sub&gt;7&lt;/sub&gt; to V&lt;sub&gt;9&lt;/sub&gt;) may be used to demonstrate the presence of a posterior myocardial infarction. A ''[[Lewis lead]]'' (requiring an electrode at the right sternal border in the second intercostal space) can be used to study pathological rhythms arising in the right atrium.

An ''esophogeal lead'' can be inserted to a part of the [[esophagus]] where the distance to the posterior wall of the [[left atrium]] is only approximately 5–6&amp;nbsp;mm (remaining constant in people of different age and weight).&lt;ref name="MeigasKaik2008"&gt;{{cite journal|last1=Meigas|first1=K|last2=Kaik|first2=J|last3=Anier|first3=A|title=Device and methods for performing transesophageal stimulation at reduced pacing current threshold|journal=Estonian Journal of Engineering|volume=57|issue=2|year=2008|page=154|doi=10.3176/eng.2008.2.05}}&lt;/ref&gt; An esophageal lead avails for a more accurate differentiation between certain cardiac arrhythmias, particularly [[atrial flutter]], [[AV nodal reentrant tachycardia]] and orthodromic [[atrioventricular reentrant tachycardia]].&lt;ref name="PehrsonBlomströ-LUNDQVIST1994"/&gt; It can also evaluate the risk in people with [[Wolff-Parkinson-White syndrome]], as well as terminate [[supraventricular tachycardia]] caused by [[Cardiac arrhythmia#Re-entry|re-entry]].&lt;ref name="PehrsonBlomströ-LUNDQVIST1994"&gt;{{cite journal|last1=Pehrson|first1=Steen M.|last2=Blomströ-Lundqvist|first2=Carina|last3=Ljungströ|first3=Erik|last4=Blomströ|first4=Per|title=Clinical value of transesophageal atrial stimulation and recording in patients with arrhythmia-related symptoms or documented supraventricular tachycardia-correlation to clinical history and invasive studies|journal=Clinical Cardiology|volume=17|issue=10|year=1994|pages=528–34|doi=10.1002/clc.4960171004|pmid=8001299}}&lt;/ref&gt;

An intracardiac electrogram (ICEG) is essentially an ECG with some added ''intracardiac leads'' (that is, inside the heart). The standard ECG leads (external leads) are I, II, III, aVL, V&lt;sub&gt;1&lt;/sub&gt;, and V&lt;sub&gt;6&lt;/sub&gt;. Two to four intracardiac leads are added via cardiac catheterization. The word "electrogram" (EGM) without further specification usually means an intracardiac electrogram.

===Lead locations on an ECG report===
A standard 12-lead ECG report (an electrocardiograph) shows a 2.5 second tracing of each of the twelve leads. The tracings are most commonly arranged in a grid of four columns and three rows. the first column is the limb leads (I, II, and III), the second column is the augmented limb leads (aVR, aVL, and aVF), and the last two columns are the precordial leads (V&lt;sub&gt;1&lt;/sub&gt; to V&lt;sub&gt;6&lt;/sub&gt;).
Additionally, a rhythm strip may be included as a fourth or fifth row.

The timing across the page is continuous and not tracings of the 12 leads for the same time period.
In other words, if the output were traced by needles on paper, each row would switch which leads as the paper is pulled under the needle.
For example, the top row would first trace lead I, then switch to lead aVR, then switch to V&lt;sub&gt;1&lt;/sub&gt;, and then switch to V&lt;sub&gt;4&lt;/sub&gt; and so none of these four tracings of the leads are from the same time period as they are traced in sequence through time.

==={{anchor|Lead groups}}Contiguity of leads===
[[File:Contiguous leads.svg|300px|thumb|Diagram showing the contiguous leads in the same color in the standard 12-lead layout]]
Each of the 12 ECG leads records the electrical activity of the heart from a different angle, and therefore align with different anatomical areas of the heart. Two leads that look at neighboring anatomical areas are said to be ''contiguous''.

{| class="wikitable"
|-
! Category
! Leads
! Activity
|-
| Inferior leads
| Leads II, III and aVF
| Look at electrical activity from the vantage point of the [[wikt:inferior|inferior]] surface ([[diaphragmatic surface of heart]])
|-
| Lateral leads
| I, aVL, V&lt;sub&gt;5&lt;/sub&gt; and V&lt;sub&gt;6&lt;/sub&gt;
| Look at the electrical activity from the vantage point of the [[lateral (anatomy)|lateral]] wall of left [[ventricle (heart)|ventricle]]
|-
| Septal leads
| V&lt;sub&gt;1&lt;/sub&gt; and V&lt;sub&gt;2&lt;/sub&gt;
| Look at electrical activity from the vantage point of the [[septal]] surface of the heart ([[interventricular septum]])
|-
| Anterior leads
| V&lt;sub&gt;3&lt;/sub&gt; and V&lt;sub&gt;4&lt;/sub&gt;
| Look at electrical activity from the vantage point of the [[anterior]] wall of the right and left ventricles ([[Sternocostal surface of heart]])
|}

In addition, any two precordial leads next to one another are considered to be contiguous. For example, though V&lt;sub&gt;4&lt;/sub&gt; is an anterior lead and V&lt;sub&gt;5&lt;/sub&gt; is a lateral lead, they are contiguous because they are next to one another.

==Electrophysiology==
{{main|Cardiac electrophysiology}}

The formal study of the electrical conduction system of the heart is called cardiac electrophysiology (EP).
An [[electrophysiology study]] involves a formal study of the conduction system and can be done for various reasons.
During such a study, [[catheter]]s are used to access the heart and some of these catheters include electrodes that can be placed anywhere in the heart to record the electrical activity from within the heart.
Some catheters contain several electrodes and can record the propagation of electrical activity.

==Interpretation==
Interpretation of the ECG is fundamentally about understanding the [[electrical conduction system of the heart]].
Normal conduction starts and propagates in a predictable pattern, and deviation from this pattern can be a normal variation or be [[pathological]].
An ECG does not equate with mechanical pumping activity of the heart, for example, [[pulseless electrical activity]] produces an ECG that should pump blood but no pulses are felt (and constitutes a [[medical emergency]] and [[Cardiopulmonary resuscitation|CPR]] should be performed).
[[Ventricular fibrillation]] produces an ECG but is too dysfunctional to produce a life-sustaining cardiac output. Certain rhythms are known to have good cardiac output and some are known to have bad cardiac output.
Ultimately, an [[echocardiogram]] or other anatomical imaging modality is useful in assessing the mechanical function of the heart.

Like all medical tests, what constitutes "normal" is based on [[population studies]]. The heartrate range of between 60 and 100 beats per minute (bpm) is considered normal since data shows this to be the usual resting heart rate.

===Theory===
[[File:ECG Vector.svg|230px|thumb|right|QRS is upright in a lead when its axis is aligned with that lead's [[Euclidean vector|vector]]]]
[[File:EKG Complex en.svg|280px|thumb|Schematic representation of a normal ECG]]

Interpretation of the ECG is ultimately that of pattern recognition.
In order to understand the patterns found, it is helpful to understand the theory of what ECGs represent.
The theory is rooted in [[electromagnetics]] and boils down to the four following points:
* depolarization of the heart ''toward'' the positive electrode produces a positive deflection
* depolarization of the heart ''away'' from the positive electrode produces a negative deflection
* repolarization of the heart ''toward'' the positive electrode produces a negative deflection
* repolarization of the heart ''away'' from the positive electrode produces a positive deflection

Thus, the overall direction of depolarization and repolarization produces a vector that produces positive or negative deflection on the ECG depending on which lead it points to.
For example, depolarizing from right to left would produce a positive deflection in lead I because the two vectors point in the same direction.
In contrast, that same depolarization would produce minimal deflection in V&lt;sub&gt;1&lt;/sub&gt; and V&lt;sub&gt;2&lt;/sub&gt; because the vectors are perpendicular and this phenomenon is called isoelectric.

Normal rhythm produces four entities – a P wave, a QRS complex, a T wave, and a U wave – that each have a fairly unique pattern.
* The P wave represents atrial depolarization.
* The QRS complex represents ventricular depolarization.
* The T wave represents ventricular repolarization.
* The U wave represents papillary muscle repolarization.

However, the U wave is not typically seen and its absence is generally ignored.
Changes in the structure of the heart and its surroundings (including blood composition) change the patterns of these four entities.

===Electrocardiogram grid===
ECGs are normally printed on a grid.
The horizontal axis represents time and the vertical axis represents voltage.
The standard values on this grid are shown in the adjacent image:
* A small box is 1&amp;nbsp;mm ×  1&amp;nbsp;mm and represents 0.1 mV × 0.04 seconds.
* A large box is 5&amp;nbsp;mm ×  5 mm and represents 0.5 mV ×  0.20 seconds.

The "large" box is represented by a heavier [[Font weight|line weight]] than the small boxes.

[[File:ECG Paper v2.svg|500px|left|Measuring time and voltage with ECG graph paper]]
{{clear}}

Not all aspects of an ECG rely on precise recordings or having a known scaling of amplitude or time.
For example, determining if the tracing is a sinus rhythm only requires feature recognition and matching, and not measurement of amplitudes or times (i.e., the scale of the grids are irrelevant).
An example to the contrary, the voltage requirements of [[left ventricular hypertrophy]] require knowing the grid scale.

===Rate and rhythm===
In a normal heart, the heart rate is the rate in which the [[sinoatrial node]] depolarizes as it is the source of depolarization of the heart.
Heart rate, like other [[vital signs]] like blood pressure and respiratory rate, change with age.
In adults, a normal heart rate is between 60 and 100 bpm (normocardic) where in children it is higher.
A heart rate less than normal is called [[bradycardia]] (&lt;60 in adults) and higher than normal is [[tachycardia]] (&gt;100 in adults).
A complication of this is when the atria and ventricles are not in synchrony and the "heart rate" must be specified as atrial or ventricular (e.g., the ventricular rate in [[ventricular fibrillation]] is 300–600 bpm, whereas the atrial rate can be normal [60–100] or faster [100–150]).

In normal resting hearts, the physiologic rhythm of the heart is [[normal sinus rhythm]] (NSR).
Normal sinus rhythm produces the prototypical pattern of P wave, QRS complex, and T wave.
Generally, deviation from normal sinus rhythm is considered a [[cardiac arrhythmia]].
Thus, the first question in interpreting an ECG is whether or not there is a sinus rhythm.
A criterion for sinus rhythm is that P waves and QRS complexes appear 1-to-1, thus implying that the P wave causes the QRS complex.

Once sinus rhythm is established, or not, the second question is the rate.
For a sinus rhythm this is either the rate of P waves or QRS complexes since they are 1-to-1.
If the rate is too fast then it is [[sinus tachycardia]] and if it is too slow then it is [[sinus bradycardia]].

If it is not a sinus rhythm, then determining the rhythm is necessary before proceeding with further interpretation.
Some arrhythmias with characteristic findings:
* Absent P waves with "irregularly irregular" QRS complexes is the hallmark of [[atrial fibrillation]]
* A "saw tooth" pattern with QRS complexes is the hallmark of [[atrial flutter]]
*[[Sine wave]] pattern is the hallmark of [[ventricular flutter]]
* Absent P waves with wide QRS complexes and a fast heart rate is [[ventricular tachycardia]]

Determination of rate and rhythm is necessary in order to make sense of further interpretation.

===Axis===
The heart has several axes, but the most common by far is the axis of the QRS complex (references to "the axis" imply the QRS axis).
Each axis can be computationally determined to result in a number representing degrees of deviation from zero, or it can be categorized into a few types.

The QRS axis is the general direction of the ventricular depolarization wavefront (or mean electrical vector) in the frontal plane.
It is often sufficient to classify the axis as one of three types: normal, left deviated, or right deviated.
Population data shows that a normal QRS axis is from −30° to 105°, with 0° being along lead I and positive being inferior and negative being superior (best understood graphically as the [[hexaxial reference system]]).&lt;ref&gt;{{cite book|last1=Surawicz|first1=Borys|last2=Knillans|first2=Timothy|title=Chou's electrocardiography in clinical practice : adult and pediatric|date=2008|publisher=Saunders/Elsevier|location=Philadelphia, PA|isbn=978-1416037743|page=12|edition=6th}}&lt;/ref&gt;
Beyond +105° is [[right axis deviation]] and beyond −30° is [[left axis deviation]] (the third quadrant of −90° to −180° is very rare and is an indeterminate axis).
A shortcut for determining if the QRS axis is normal is if the QRS complex is mostly positive in lead I and lead II (or lead I and aVF if +90° is the upper limit of normal).

The normal QRS axis is generally ''down and to the left'', following the anatomical orientation of the heart within the chest. An abnormal axis suggests a change in the physical shape and orientation of the heart or a defect in its conduction system that causes the ventricles to depolarize in an abnormal way.

{| class="wikitable"
|-
! Classification
! Angle
! Notes
|-
| Normal
| −30° to 105°
| Normal
|-
| [[Left axis deviation]]
| −30° to −90°
| May indicate [[left ventricular hypertrophy]], [[left anterior fascicular block]], or an old inferior STEMI
|-
| [[Right axis deviation]]
| +105° to +180°
| May indicate [[right ventricular hypertrophy]], [[left posterior fascicular block]], or an old lateral STEMI
|-
| ''Indeterminate axis''
| +180° to −90°
| Rarely seen; considered an 'electrical no-man's land'
|}

The extent of a normal axis can be +90° or 105° depending on the source.

===Amplitudes and intervals===
[[File:ECG principle slow.gif|thumb|Animation of a normal ECG wave]]
All of the waves on an ECG tracing and the intervals between them have a predictable time duration, a range of acceptable amplitudes (voltages), and a typical morphology. Any deviation from the normal tracing is potentially pathological and therefore of clinical significance.

For ease of measuring the amplitudes and intervals, an ECG is printed on graph paper at a standard scale: each 1&amp;nbsp;mm (one small box on the standard ECG paper) represents 40 milliseconds of time on the x-axis, and 0.1 millivolts on the y-axis.
{| class="wikitable"
|-
!Feature
!Description
!Pathology
!Duration
|-
|[[P wave (electrocardiography)|P wave]]
|The P wave represents depolarization of the atria. Atrial depolarization spreads from the SA node towards the AV node, and from the right [[atrium (anatomy)|atrium]] to the left [[atrium (anatomy)|atrium]].
|The P wave is typically upright in most leads except for aVR; an unusual P wave axis (inverted in other leads) can indicate an [[Ectopic pacemaker|ectopic atrial pacemaker]]. If the P wave is of unusually long duration, it may represent atrial enlargement. Typically a large ''right atrium'' gives a tall, peaked P wave while a large ''left atrium'' gives a two-humped bifid P wave.
|&lt;80 ms
|-
|[[PR interval]]
|The PR interval is measured from the beginning of the P wave to the beginning of the QRS complex. This interval reflects the time the electrical impulse takes to travel from the sinus node through the AV node.
|A PR interval shorter than 120 ms suggests that the electrical impulse is bypassing the AV node, as in [[Wolf-Parkinson-White syndrome]]. A PR interval consistently longer than 200 ms diagnoses [[first degree atrioventricular block]]. The PR segment (the portion of the tracing after the P wave and before the QRS complex) is typically completely flat, but may be depressed in [[pericarditis]].
|120 to 200 ms
|-
|[[QRS complex]]
|The QRS complex represents the rapid depolarization of the right and left ventricles. The ventricles have a large muscle mass compared to the atria, so the QRS complex usually has a much larger amplitude than the P wave.
|If the QRS complex is wide (longer than 120 ms) it suggests disruption of the heart's conduction system, such as in [[LBBB]], [[RBBB]], or ventricular rhythms such as [[ventricular tachycardia]]. Metabolic issues such as severe [[hyperkalemia]], or [[tricyclic antidepressant overdose]] can also widen the QRS complex. An unusually tall QRS complex may represent [[left ventricular hypertrophy]] while a very low-amplitude QRS complex may represent a [[pericardial effusion]] or [[restrictive cardiomyopathy|infiltrative myocardial disease]].
|80 to 100 ms
|-
|[[J-point]]
|The J-point is the point at which the QRS complex finishes and the ST segment begins.
|The J-point may be elevated as a normal variant. The appearance of a separate ''[[Osborn wave|J wave]]'' or ''Osborn wave'' at the J-point is [[pathognomonic]] of [[hypothermia]] or [[hypercalcemia]].&lt;ref&gt;{{cite journal | pmc = 101092 | pmid=11093425 | volume=27 | issue=3 | title=The "normothermic" Osborn wave induced by severe hypercalcemia | author=Otero J, Lenihan DJ | journal=Tex Heart Inst J | pages=316–17| year=2000 }}&lt;/ref&gt;
|
|-
|[[ST segment]]
|The ST segment connects the QRS complex and the T wave; it represents the period when the ventricles are depolarized.
|It is usually isoelectric, but may be depressed or elevated with [[myocardial infarction]] or ischemia. [[ST depression]] can also be caused by [[LVH]] or [[digoxin]]. [[ST elevation]] can also be caused by [[pericarditis]], [[Brugada syndrome]], or can be a normal variant (J-point elevation).
|
|-
|[[T wave]]
|The T wave represents the repolarization of the ventricles. It is generally upright in all leads except aVR and lead V1.
|Inverted T waves can be a sign of myocardial ischemia, [[LVH|left ventricular hypertrophy]], high [[intracranial pressure]], or metabolic abnormalities. Peaked T waves can be a sign of [[hyperkalemia]] or very early [[myocardial infarction]].
|160 ms
|-
|[[Corrected QT interval]] (QTc)
|The QT interval is measured from the beginning of the QRS complex to the end of the T wave. Acceptable ranges vary with heart rate, so it must be ''corrected'' to the QTc by dividing by the square root of the RR interval.
|A prolonged QTc interval is a risk factor for ventricular tachyarrhythmias and sudden death. Long QT can arise as a [[long QT syndrome|genetic syndrome]], or as a side effect of certain medications. An unusually short QTc can be seen in severe hypercalcemia.
|&lt;440 ms
|-
|[[U wave]]
|The U wave is hypothesized to be caused by the repolarization of the interventricular septum. It normally has a low amplitude, and even more often is completely absent.
|If the U wave is very prominent, suspect hypokalemia, hypercalcemia or hyperthyroidism.&lt;ref&gt;{{cite book|author1=Houghton, Andrew R |author2=Gray, David |title=Making Sense of the ECG, Third Edition|url=https://books.google.com/books?id=8s4TQ6yYHRkC|date=2012|publisher=Hodder Education|isbn=978-1-4441-6654-5|page=214}}&lt;/ref&gt;
|
|-
|}

===Ischemia and infarction===
{{Main|Electrocardiography in myocardial infarction}}
Ischemia or [[NSTEMI|non-ST elevation myocardial infarctions]] (non-STEMIs) may manifest as [[ST depression]] or inversion of [[T wave]]s. It may also affect the [[HFQRS|high frequency band of the QRS]].

[[STEMI|ST elevation myocardial infarctions]] (STEMIs) have different characteristic ECG findings based on the amount of time elapsed since the MI first occurred. The earliest sign is ''hyperacute T waves,'' peaked T waves due to local [[hyperkalemia]] in ischemic myocardium. This then progresses over a period of minutes to elevations of the [[ST segment]] by at least 1&amp;nbsp;mm. Over a period of hours, a pathologic [[QRS complex#Q wave|Q wave]] may appear and the T wave will invert. Over a period of days the ST elevation will resolve. Pathologic Q waves generally will remain permanently.&lt;ref name="Alpert-2000"&gt;{{cite journal |vauthors=Alpert JS, Thygesen K, Antman E, Bassand JP | title = Myocardial infarction redefined – a consensus document of The Joint European Society of Cardiology/American College of Cardiology Committee for the redefinition of myocardial infarction | journal = J Am Coll Cardiol | volume = 36 | issue = 3 | pages = 959–69 | year = 2000 | pmid = 10987628 | doi = 10.1016/S0735-1097(00)00804-4 }}&lt;/ref&gt;

The [[coronary artery]] that has been occluded can be identified in an STEMI based on the location of ST elevation. The [[Anterior interventricular branch of left coronary artery|left anterior descending]] (LAD) artery supplies the anterior wall of the heart, and therefore causes ST elevations in anterior leads (V&lt;sub&gt;1&lt;/sub&gt; and V&lt;sub&gt;2&lt;/sub&gt;). The [[Circumflex branch of left coronary artery|LCx]] supplies the lateral aspect of the heart and therefore causes ST elevations in lateral leads (I, aVL and V&lt;sub&gt;6&lt;/sub&gt;). The [[right coronary artery]] (RCA) usually supplies the inferior aspect of the heart, and therefore causes ST elevations in inferior leads (II, III and aVF).

===Artifacts===
An ECG tracing is affected by patient motion. Some rhythmic motions (such as shivering or [[Tremor|tremors]]) can create the illusion of cardiac arrhythmia.&lt;ref&gt;{{cite journal|doi=10.1016/j.ijcard.2014.10.076|pmid=25464416|title=Atrial flutter EKG can be useless without the proper electrophysiological basis|journal=International Journal of Cardiology|volume=179|pages=68–69|year=2015|last1=Segura-Sampedro|first1=Juan José|last2=Parra-López|first2=Loreto|last3=Sampedro-Abascal|first3=Consuelo|last4=Muñoz-Rodríguez|first4=Juan Carlos}}&lt;/ref&gt; Artifacts are distorted signals caused by a secondary internal or external sources, such as muscle movement or interference from an electrical device.&lt;ref name="Takla"&gt;{{cite journal|doi= 10.1213/01.ane.0000247964.47706.5d|pmid= 17056954|title= The Problem of Artifacts in Patient Monitor Data During Surgery: A Clinical and Methodological Review|journal= Anesthesia &amp; Analgesia|volume= 103|issue= 5|pages= 1196–204|year= 2006|last1= Takla|first1= George|last2= Petre|first2= John H.|last3= Doyle|first3= D John|last4= Horibe|first4= Mayumi|last5= Gopakumaran|first5= Bala}}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|title = Recommendations for the standardization and interpretation of the electrocardiogram: part I: The electrocardiogram and its technology: a scientific statement from the American Heart Association Electrocardiography and Arrhythmias Committee, Council on Clinical Cardiology; the American College of Cardiology Foundation; and the Heart Rhythm Society: endorsed by the International Society for Computerized Electrocardiology|journal = Circulation|date = 2007-03-13|pmid = 17322457|pages = 1306–24|volume = 115|issue = 10|doi = 10.1161/CIRCULATIONAHA.106.180200|first = Paul|last = Kligfield|first2 = Leonard S.|last2 = Gettes|first3 = James J.|last3 = Bailey|first4 = Rory|last4 = Childers|first5 = Barbara J.|last5 = Deal|first6 = E. William|last6 = Hancock|first7 = Gerard|last7 = van Herpen|first8 = Jan A.|last8 = Kors|first9 = Peter|last9 = Macfarlane}}&lt;/ref&gt;

Distortion poses significant challenges to healthcare providers,&lt;ref name="Takla"/&gt; who employ various techniques&lt;ref&gt;{{Cite web|url=https://www.physio-control.com/WorkArea/DownloadAsset.aspx?id=2147489452|title=Minimizing ECG Artifact|last=|first=|date=2015|website=Physio-Control|publisher=Physio-Control, Inc., Redmond WA|format=PDF|archive-url=|archive-date=|dead-url=|access-date=21 October 2017}}&lt;/ref&gt; and strategies to safely recognize&lt;ref name="Fahim H Jafary" &gt;{{cite journal|doi=10.1186/1752-1947-1-72|pmid=17760955|pmc=2000884|title=The "incidental" episode of ventricular fibrillation: A case report|journal=Journal of Medical Case Reports|volume=1|pages=72|year=2007|last1=Jafary|first1=Fahim H}}&lt;/ref&gt; these false signals.{{medcite|date=August 2015}} Accurately separating the ECG artifact from the true ECG signal can have a significant impact on patient outcomes and [[Legal liability|legal liabilities]].&lt;ref&gt;{{cite journal|doi=10.1016/j.ahj.2014.02.007|pmid=24766979|title=Medical professional liability risk among US cardiologists|journal=American Heart Journal|volume=167|issue=5|pages=690–96|year=2014|last1=Mangalmurti|first1=Sandeep|last2=Seabury|first2=Seth A.|last3=Chandra|first3=Amitabh|last4=Lakdawalla|first4=Darius|last5=Oetgen|first5=William J.|last6=Jena|first6=Anupam B.|pmc=4153384}}
&lt;/ref&gt;{{UNMED|date=August 2015}}

Improper lead placement (for example, reversing two of the limb leads) has been estimated to occur in 0.4% to 4% of all ECG recordings,&lt;ref&gt;Incorrect electrode cable connection during electrocardiographic recording (2007) Velislav N. Batchvarov, Marek Malik, A. John Camm, Europace, Oct 2007&lt;/ref&gt; and has resulted in improper diagnosis and treatment including unnecessary use of [[thrombolytic]] therapy.&lt;ref&gt;Chanarin, N., Caplin, J., &amp; Peacock, A. (1990). "Pseudo reinfarction": a consequence of electrocardiogram lead transposition following myocardial infarction. Clinical cardiology, 13(9), 668–69.&lt;/ref&gt;&lt;ref&gt;{{cite journal | author = Guijarro-Morales A., Gil-Extremera B., Maldonado-Martín A. | year = 1991 | title = ECG diagnostic errors due to improper connection of the right arm and leg cables | url = | journal = International Journal of Cardiology | volume = 30 | issue = 2| pages = 233–35 | doi=10.1016/0167-5273(91)90103-v}}&lt;/ref&gt;

==Diagnosis==
Numerous diagnoses and findings can be made based upon electrocardiography, and many are discussed above.  Overall, the diagnoses are made based on the patterns.  For example, an "irregularly irregular" QRS complex without P waves is the hallmark of [[atrial fibrillation]]; however, other findings can be present as well, such as a [[bundle branch block]] that alters the shape of the QRS complexes.  ECGs can be interpreted in isolation but should be applied – like all [[diagnostic tests]] – in the context of the patient.  For example, an observation of peaked T waves is not sufficient to diagnose hyperkalemia; such a diagnosis should be verified by measuring the blood potassium level.  Conversely, a discovery of hyperkalemia should be followed by an ECG for manifestations such as peaked T waves, widened QRS complexes, and loss of P waves.  The following is an organized list of possible ECG-based diagnoses.

Rhythm disturbances or arrhythmias:
* [[Atrial fibrillation]] and [[atrial flutter]] without rapid ventricular response
* [[Premature atrial contraction]] (PACs) and [[premature ventricular contraction]] (PVCs)
* [[Sinus arrhythmia]]
* [[Sinus bradycardia]] and [[sinus tachycardia]]
* [[Sinus pause]] and [[sinoatrial arrest]]
* [[Sick sinus syndrome]]: [[bradycardia-tachycardia syndrome]]
* [[Supraventricular tachycardia]]
**[[Atrial fibrillation]] with rapid ventricular response
** [[Atrial flutter]] with rapid ventricular response
** [[AV nodal reentrant tachycardia]]
** [[Atrioventricular reentrant tachycardia]]
** [[Junctional ectopic tachycardia]]
** [[Atrial tachycardia]]
*** [[Ectopic atrial tachycardia]] (unicentric)
*** [[Multifocal atrial tachycardia]]
*** [[Paroxysmal atrial tachycardia]]
** [[Sinoatrial nodal reentrant tachycardia]]
* [[Torsades de pointes]] (polymorphic ventricular tachycardia)
* [[Wide complex tachycardia]]
** [[Ventricular flutter]]
** [[Ventricular fibrillation]]
** [[Ventricular tachycardia]] (monomorphic ventricular tachycardia)
* [[Pre-excitation syndrome]]
** [[Lown–Ganong–Levine syndrome]]
** [[Wolff–Parkinson–White syndrome]]
* [[J wave]] (Osborn wave)

[[Heart block]] and conduction problems:
* [[Cardiac aberrancy|Aberration]]
* [[Sinoatrial block]]: first, second, and third-degree
* AV node
**[[First-degree AV block]]
** [[Second-degree AV block]] (Mobitz [Wenckebach] I and II)
** [[Third-degree AV block]] or complete AV block
* Right bundle
** [[Incomplete right bundle branch block]]
** Complete [[right bundle branch block]] (RBBB)
* Left bundle
** Complete [[left bundle branch block]] (LBBB)
** [[Incomplete left bundle branch block]] 
** [[Left anterior fascicular block]] (LAFB)
** [[Left posterior fascicular block]] (LPFB)
** [[Bifascicular block]] (LAFB plus LPFB)
** [[Trifascicular block]] (LAFP plus FPFB plus RBBB)
* QT syndromes
** [[Brugada syndrome]]
** [[Short QT syndrome]]
** [[Long QT syndrome]]s, genetic and drug-induced
* [[Right atrial abnormality|Right]] and [[left atrial abnormality]]

Electrolytes disturbances and intoxication:
* [[Digoxin poisoning|Digitalis intoxication]]
* Calcium: [[hypocalcemia]] and [[hypercalcemia]]
* Potassium: [[hypokalemia]] and [[hyperkalemia]]

Ischemia and infarction:
* [[Wellens' syndrome]] (LAD occlusion)
* [[de Winter T waves]] (LAD occlusion) &lt;ref&gt;{{cite journal |last1=de Winter |first1=Robert |title=A New ECG Sign of Proximal LAD Occlusion |journal=NEJM |date=6 Nov 2008 |volume=359 |issue=19 |pages=2071–3 |doi=10.1056/NEJMc0804737 |pmid=18987380 }}&lt;/ref&gt;
* [[ST elevation]] and [[ST depression]]
* [[High Frequency QRS]] changes
* [[Myocardial infarction]] (heart attack)
** [[Non-Q wave myocardial infarction]]
** [[NSTEMI]]
** [[STEMI]]
** [[Sgarbossa's criteria]] for ischemia with a [[left bundle branch block|LBBB]]

Structural:
* [[Acute pericarditis]]
* [[Right ventricular hypertrophy|Right]] and [[left ventricular hypertrophy]]
* [[Right ventricular strain]] or S1Q3T3 (can be seen in [[pulmonary embolism]])

== History ==
[[File:Willem Einthoven ECG.jpg|thumb|An early commercial ECG device (1911)]]
[[File:BASA-532K-1-2-15-Ran Bosilek.jpg|thumb|ECG from 1957]]
The etymology of the word is derived from the [[Greek language|Greek]] ''electro'', because it is related to electrical activity, ''kardia'', [[Greek language|Greek]] for heart, and ''graph'', a [[Greek language|Greek]] root meaning "to write".

[[Alexander Muirhead]] is reported to have attached wires to a feverish patient's wrist to obtain a record of the patient's heartbeat in 1872 at [[St Bartholomew's Hospital]].&lt;ref&gt;Ronald M. Birse,[http://www.oxforddnb.com/view/article/37794 rev. Patricia E. Knowlden] [[Oxford Dictionary of National Biography]] 2004 (Subscription required) – (original source is his biography written by his wife&amp;nbsp;– Elizabeth Muirhead. Alexandernn Muirhead 1848–1920. Oxford, Blackwell: privately printed 1926.)&lt;/ref&gt;  Another early pioneer was [[Augustus Desiré Waller|Augustus Waller]], of [[St Mary's Hospital (London)|St Mary's Hospital]] in [[London]].&lt;ref name=Waller_1887&gt;{{cite journal | author = Waller AD | title = A demonstration on man of electromotive changes accompanying the heart's beat | journal = J Physiol | volume = 8 | issue = 5 | pages = 229–34 | year = 1887 | pmid = 16991463 | pmc = 1485094 | doi=10.1113/jphysiol.1887.sp000257}}&lt;/ref&gt; His electrocardiograph machine consisted of a [[lippmann electrometer|Lippmann capillary electrometer]] fixed to a projector. The trace from the heartbeat was projected onto a photographic plate that was itself fixed to a toy train. This allowed a heartbeat to be recorded in real time.

An initial breakthrough came when [[Willem Einthoven]], working in [[Leiden]], the [[Netherlands]], used the [[string galvanometer]] (the first practical electrocardiograph) he invented in 1901.&lt;ref&gt;{{cite journal |vauthors=Rivera-Ruiz M, Cajavilca C, Varon J | title = Einthoven's String Galvanometer: The First Electrocardiograph | journal = Texas Heart Institute Journal / from the Texas Heart Institute of St. Luke's Episcopal Hospital, Texas Children's Hospital | volume = 35 | issue = 2 | pages = 174–78 | date = 29 September 1927 | pmid = 18612490 | pmc = 2435435 }}&lt;/ref&gt; This device was much more sensitive than both the capillary electrometer Waller used and the string galvanometer that had been invented separately in 1897 by the French engineer [[Clément Ader]].&lt;ref&gt;{{cite journal|author=Interwoven W|title=Un nouveau galvanometre|journal= Arch Neerl Sc Ex Nat |year=1901|volume= 6|page=625}}&lt;/ref&gt; Einthoven had previously, in 1895, assigned the letters P, Q, R, S, and T to the deflections in the theoretical waveform he created using equations which corrected the actual waveform obtained by the capillary electrometer to compensate for the imprecision of that instrument. Using letters different from A, B, C, and D (the letters used for the capillary electrometer's waveform) facilitated comparison when the uncorrected and corrected lines were drawn on the same graph.&lt;ref name=naming/&gt; Einthoven probably chose the initial letter P to follow the example set by [[Descartes]] in [[geometry]].&lt;ref name=naming/&gt; When a more  precise waveform was obtained using the string galvanometer, which matched the corrected capillary electrometer waveform, he continued to use the letters P, Q, R, S, and T,&lt;ref name=naming&gt;{{cite journal | author = Hurst JW | title = Naming of the Waves in the ECG, With a Brief Account of Their Genesis | journal = Circulation | volume = 98 | issue = 18 | pages = 1937–42 | date = 3 November 1998 | pmid = 9799216 | doi = 10.1161/01.CIR.98.18.1937 }}&lt;/ref&gt; and these letters are still in use today. Einthoven also described the electrocardiographic features of a number of cardiovascular disorders. In 1924, he was awarded the [[Nobel Prize in Medicine]] for his discovery.&lt;ref name=Cooper_1986&gt;{{cite journal | author = Cooper JK | title = Electrocardiography 100 years ago. Origins, pioneers, and contributors | journal = N Engl J Med | volume = 315 | issue = 7 | pages = 461–64 | year = 1986 | pmid = 3526152 | doi = 10.1056/NEJM198608143150721 }}&lt;/ref&gt;

By 1927, General Electric had developed a portable apparatus that could produce electrocardiograms without the use of the string galvanometer. This device instead combined amplifier tubes similar to those used in a radio with an internal lamp and a moving mirror that directed the tracing of the electric pulses onto film.&lt;ref&gt;{{cite journal|author=Blackford, John M., MD| title=Electrocardiography: A Short Talk Before the Staff of the Hospital| journal= Clinics of the Virginia Mason Hospital| date=1 May 1927| volume=6| issue=1 |pages=28–34}}&lt;/ref&gt;

In 1937, [[Taro Takemi]] invented a new portable electrocardiograph machine.&lt;ref&gt;{{Cite news|url=https://www.hsph.harvard.edu/takemi/about-the-program/dr-taro-takemi/|title=Dr. Taro Takemi|date=2012-08-27|work=Takemi Program in International Health|access-date=2017-10-21|language=en-US}}&lt;/ref&gt;

Though the basic principles of that era are still in use today, many advances in electrocardiography have been made over the years. Instrumentation has evolved from a cumbersome laboratory apparatus to compact electronic systems that often include computerized interpretation of the electrocardiogram.&lt;ref&gt;{{cite book |last=Mark|first=Jonathan B. |title=Atlas of cardiovascular monitoring |year=1998|publisher=Churchill Livingstone |location=New York |isbn=978-0-443-08891-9}}&lt;/ref&gt;

In September 2018, [[Apple Inc.]], introduced the [[Apple Watch Series 4]], with a built-in titanium electrode in the digital crown and the sapphire crystal electronic heart sensor, which allows the watch to give a single lead electrocardiogram using only the watch interface.&lt;ref&gt;{{Cite web|url=https://www.apple.com/apple-watch-series-4/health/|title=Apple Watch Series 4 - Health|website=Apple|language=en-US|access-date=2018-09-27}}&lt;/ref&gt;

==See also==
* [[Electrical conduction system of the heart]]
* [[Electrogastrogram]]
* [[Electropalatography]]
* [[Electroretinography]]
* [[Heart rate]]
* [[Heart rate monitor]]
* [[Emergency medicine]]

==Notes==
{{notelist}}

==References==
{{Reflist|30em}}

==External links==
{{Commons category|ECG}}
* [http://www.ecgpedia.org/A4/ECGpedia_on_1_A4En.pdf The whole ECG course on 1 A4 paper] from [http://en.ecgpedia.org/wiki/Main_Page ECGpedia], a wiki encyclopedia for  [http://en.ecgpedia.org/wiki/ECG_course a course on interpretation of ECG]
* [http://ecg.bidmc.harvard.edu/maven/mavenmain.asp Wave Maven – a large database of practice ECG questions] provided by [[Beth Israel Deaconess Medical Center]]
* [http://www.physionet.org/physiobank/database/#ecg PysioBank – a free scientific database with physiologic signals (here ecg) ]
* [http://ekg.academy EKG Academy – free EKG lectures, drills and quizzes]
* [https://ecg.utah.edu/lesson/1 ECG Learning Center] created by Eccles Health Sciences Library at [[University of Utah]]

{{Emergency medicine}}
{{Cardiac procedures}}
{{Electrodiagnosis}}
{{Cardiovascular physiology}}

{{Authority control}}

[[Category:Cardiac electrophysiology]]
[[Category:Cardiac procedures]]
[[Category:Electrodiagnosis]]
[[Category:Electrophysiology]]
[[Category:Mathematics in medicine]]
[[Category:Medical tests]]
[[Category:Dutch inventions]]</text>
      <sha1>m8rrccas0jppk03wnvme2f5jlfzrjml</sha1>
    </revision>
  </page>
  <page>
    <title>Euclid's Optics</title>
    <ns>0</ns>
    <id>24877937</id>
    <revision>
      <id>846317225</id>
      <parentid>680337775</parentid>
      <timestamp>2018-06-17T23:16:08Z</timestamp>
      <contributor>
        <username>Allforrous</username>
        <id>12120664</id>
      </contributor>
      <comment>new key for [[Category:Euclidean geometry]]: "*" using [[WP:HC|HotCat]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6569">{{DISPLAYTITLE:Euclid's ''Optics''}}
[[File:Drawing Square in Perspective 2.svg|thumb|Euclid postulated that visual rays proceed from the eyes onto objects, and that the different visual properties of the objects were determined by how the visual rays struck them. Here the red square is an actual object, while the yellow plane shows how the object is perceived.]]

'''Euclid's ''Optics''''' ({{lang-grc-gre|Ὀπτικά}}), is a work on the [[geometry]] of vision written by the Greek mathematician [[Euclid]] around 300 BC. The earliest surviving manuscript of ''Optics'' is in Greek and dates from the 10th century AD.
 
The work deals almost entirely with the geometry of vision, with little reference to either the physical or psychological aspects of sight. No Western scientist had previously given such mathematical attention to vision. Euclid's ''Optics'' influenced the work of later Greek, Islamic, and Western European Renaissance scientists and artists.

== Historical significance==
{{See also|History of optics}}
Writers before Euclid had developed theories of vision.  However, their works were mostly philosophical in nature and lacked the mathematics that Euclid introduced in his ''Optics''.&lt;ref&gt;Lindberg, D. C. (1976). ''Theories of Vision from Al-Kindi to Kepler''. Chicago: University of Chicago Press. p.&amp;nbsp;12.&lt;/ref&gt; Efforts by the Greeks prior to Euclid were concerned primarily with the physical dimension of vision.  Whereas [[Plato]] and [[Empedocles]] thought of the visual ray as "luminous and ethereal emanation",&lt;ref&gt;Zajonc, A. (1993). ''Catching the Light: The Entwined History of Light and Mind''. Oxford: Oxford University Press. p.&amp;nbsp;25.&lt;/ref&gt; Euclid’s treatment of vision in a mathematical way was part of the larger Hellenistic trend to quantify a whole range of scientific fields.

Because ''Optics'' contributed a new dimension to the study of vision, it influenced later scientists.  In particular, [[Ptolemy]] used Euclid's mathematical treatment of vision and his idea of a visual cone in combination with physical theories in Ptolemy's ''Optics'', which has been called "one of the most important works on optics written before Newton".&lt;ref&gt;Lindberg, D. C. (2007). ''The Beginnings of Western Science: The European Scientific Traditions in Philosophical, Religious, and Institutional Context, Prehistory to A.D. 1450''. 2nd ed. Chicago: University of Chicago Press, p.&amp;nbsp;106.&lt;/ref&gt; Renaissance artists such as [[Filippo Brunelleschi|Brunelleschi]], [[Leon Battista Alberti|Alberti]], and [[Albrecht Dürer|Dürer]] used Euclid's ''Optics''  in their own work on linear [[perspective (graphical)|perspective]].&lt;ref&gt;Zajonc (1993), p.&amp;nbsp;25.&lt;/ref&gt;

== Structure and method ==

Similar to Euclid's much more famous work on geometry, ''[[Euclid's Elements|Elements]]'', ''Optics'' begins with a small number of definitions and [[Axiom|postulates]], which are then used to [[mathematical proof|prove]], by [[deductive reasoning]], a body of geometric propositions ([[theorem]]s in modern terminology) about vision.

The postulates in ''Optics'' are:

&lt;blockquote&gt;Let it be assumed&lt;br /&gt;
1. That rectilinear rays proceeding from the eye diverge indefinitely;&lt;br /&gt;
2. That the figure contained by a set of visual rays is a cone of which the vertex is at the eye and the base at the surface of the objects seen;&lt;br /&gt;
3. That those things are seen upon which visuals rays fall and those things are not seen upon which visual rays do not fall;&lt;br /&gt;
4. That things seen under a larger angle appear larger, those under a smaller angle appear smaller, and those under equal angles appear equal;&lt;br /&gt;
5. That things seen by higher visual rays appear higher, and things seen by lower visual rays appear lower;&lt;br /&gt;
6. That, similarly, things seen by rays further to the right appear further to the right, and things seen by rays further to the left appear further to the left;&lt;br /&gt;
7. That things seen under more angles are seen more clearly.&lt;ref&gt;Lindberg (1976), p.&amp;nbsp;12.&lt;/ref&gt;&lt;/blockquote&gt;

The geometric treatment of the subject follows the same methodology as the ''Elements''.

== Content ==

According to Euclid, the eye sees objects that are within its visual cone. The visual cone is made up of straight lines, or visual rays, extending outward from the eye. These visual rays are discrete, but we perceive a continuous image because our eyes, and thus our visual rays, move very quickly.&lt;ref&gt;Russo, L. (2004). ''The Forgotten Revolution: How Science Was Born in 300 BC and Why It Had to Be Reborn''. S. Levy, transl. Berlin: Springer-Verlag p.&amp;nbsp;149.&lt;/ref&gt; Because visual rays are discrete, however, it is possible for small objects to lie unseen between them.  This accounts for the difficulty in searching for a dropped needle.  Although the needle may be within one's field of view, until the eye's visual rays fall upon the needle, it will not be seen.&lt;ref&gt;Zajonc (1993), p.&amp;nbsp;25.&lt;/ref&gt; Discrete visual rays also explain the sharp or blurred appearance of objects.  According to postulate 7, the closer an object, the more visual rays fall upon it and the more detailed or sharp it appears. This is an early attempt to describe the phenomenon of [[optical resolution]].

Much of the work considers perspective, how an object appears in space relative to the eye. For example, in proposition 8, Euclid argues that the perceived size of an object is not related to its distance from the eye by a simple proportion.&lt;ref&gt;Smith, M. A.  (1999). ''Ptolemy and the Foundations of Ancient Mathematical Optics: A Source Based Guided Study''. Philadelphia: American Philosophical Society. p.&amp;nbsp;57.&lt;/ref&gt;

== Notes ==
{{reflist}}

==References==
*{{cite book |last=Smith |first=M.A.  |title=Ptolemy's Theory of Visual Perception: An English Translation of the Optics with Introduction and Commentary |year=1996 |publisher=The American Philosophical Society |location=Philadelphia}}
*{{cite book |last=Smith |first=M.A.  |title=From Sight to Light. The Passage from Ancient to Modern Optics |year=2014 |publisher=The University of Chicago Press |location=Chicago &amp; London}}
* [http://philomatica.org/wp-content/uploads/2013/01/Optics-of-Euclid.pdf English translation of Euclid's ''Optics'']
* [http://www.wilbourhall.org/pdfs/Euclid_VOL_VII.pdf Greek text of Euclid's ''Optics'' from ''Euclidis Opera Omnia'', ed. J.L. Heiberg, vol. VII]
{{Ancient Greek mathematics}}

[[Category:Euclidean geometry|*]]
[[Category:Mathematics books]]
[[Category:Ancient Greek mathematical works]]
[[Category:Works by Euclid]]</text>
      <sha1>2kbtw0ppxqx7hggsejvb2tgnk6nuf63</sha1>
    </revision>
  </page>
  <page>
    <title>European Symposium on Algorithms</title>
    <ns>0</ns>
    <id>20994389</id>
    <revision>
      <id>803372945</id>
      <parentid>803372844</parentid>
      <timestamp>2017-10-02T03:26:43Z</timestamp>
      <contributor>
        <ip>2406:F00:1:64:3:0:0:9E</ip>
      </contributor>
      <comment>/* ESA locations */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3962">{{Infobox Academic Conference
 | history = 1993–
 | discipline = [[Algorithms]]
 | abbreviation = ESA
 | publisher = [[Springer Science+Business Media|Springer]] [[Lecture Notes in Computer Science|LNCS]]
 | country= International
 | frequency = annual
}}

The '''European Symposium on Algorithms''' ('''ESA''') is an international conference covering the field of [[algorithms]]. It has been held annually since 1993, typically in early Autumn in a different European location each year. Like most theoretical computer science conferences its contributions are strongly peer-reviewed; the articles appear in proceedings published in [[Springer Science+Business Media|Springer]] [[Lecture Notes in Computer Science]]. Acceptance rate of ESA is 24% in 2012 in both ''Design and Analysis'' and ''Engineering and Applications'' tracks.&lt;ref&gt;{{ cite web | url=http://www.springerlink.com/content/vvh2rq7353w0/front-matter.pdf | year=2012 | title=Algorithms – ESA 2012 (Lecture Notes in Computer Science) | accessdate=2012-09-17 }}&lt;/ref&gt;

==History==

The first ESA was held in 1993 and contained 35 papers. The intended scope was all research in algorithms, theoretical as well as applied, carried out in the fields of [[computer science]] and [[discrete mathematics]]. An explicit aim was to intensify the exchange between these two research communities.

In 2002, ESA incorporated the conference '''Workshop on Algorithms Engineering''' ('''WAE'''). In its current format, ESA contains two distinct tracks with their own programme committees: a track on the design an [[analysis of algorithms]], and a track on engineering and applications, together accepting around 70 contributions.

==ALGO conferences==

Since 2001, ESA is co-located with other algorithms conferences and workshops in a combined meeting called '''ALGO'''. This is the largest European event devoted to algorithms, attracting hundreds of researchers.

Other events in the ALGO conferences include the following.
* '''WABI''', the '''Workshop on Algorithms in Bioinformatics''', was part of ALGO in 2001–2006 and 2008.
* '''WAOA''', the '''Workshop on Approximation and Online Algorithms''', has been part of ALGO since 2003.
* '''ATMOS''', the '''Workshop on Algorithmic Approaches for Transportation Modeling, Optimization and Systems''', formerly the ''Workshop on Algorithmic Methods and Models for Optimization of Railways'', has been part of ALGO in 2003–2006 and 2008–2009.
* '''IPEC''', the International Symposium on Parameterized and Exact Computation, founded in 2004 and formerly the International Workshop on Parameterized and Exact Computation (IWPEC), is part of ALGO since 2011

ATMOS was co-located with the [[International Colloquium on Automata, Languages and Programming]] (ICALP) in 2001–2002.

==ESA locations==

[[Image:ESA 2009 coffee break.JPG|thumb|right|200px|ESA 2009 at [[IT University of Copenhagen]]]]
* 1993 – Bad Honnef, Germany
* 1994 – Utrecht, Netherlands
* 1995 – Corfu, Greece
* 1996 – Barcelona, Spain
* 1997 – Graz, Austria
* 1998 – Venice, Italy
* 1999 – Prague, Czech Republic
* 2000 – Saarbrücken, Germany
* 2001 – Århus, Denmark
* 2002 – Rome, Italy
* 2003 – Budapest, Hungary
* 2004 – Bergen, Norway
* 2005 – Palma de Mallorca, Spain
* 2006 – Zürich, Switzerland
* 2007 – Eilat, Israel
* 2008 – Karlsruhe, Germany
* 2009 – Copenhagen, Denmark
* 2010 – Liverpool, United Kingdom
* 2011 – Saarbrücken, Germany
* 2012 – Ljubljana, Slovenia
* 2013 – Sophia Antipolis, France
* 2014 – Wrocław, Poland
* 2015 – Patras, Greece
* 2016 – Aarhus, Denmark
* 2017 – Vienna, Austria

==External links==
* [http://esa-symposium.org/ Web site of ESA].
* [http://algo2009.itu.dk/history ALGO History] on ALGO 2009 web site.
* [http://www.informatik.uni-trier.de/~ley/db/conf/esa/index.html Index of ESA proceedings].

==References==

{{Reflist}}

[[Category:Theoretical computer science conferences]]</text>
      <sha1>owu3bq32z19v63wj7p86urz12gpxtfa</sha1>
    </revision>
  </page>
  <page>
    <title>Flexagon</title>
    <ns>0</ns>
    <id>98316</id>
    <revision>
      <id>869869814</id>
      <parentid>869844756</parentid>
      <timestamp>2018-11-20T22:15:26Z</timestamp>
      <contributor>
        <username>Loadmaster</username>
        <id>842485</id>
      </contributor>
      <comment>/* See also */ merged sects</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="19634">{{Use mdy dates|date=October 2018}}
[[File:Hexahexaflexagon - two sides - 01.jpg|thumb|right|250px|alt=A hexaflexagon, shown with the same face in two configurations|A hexaflexagon, shown with the same face in two configurations]]
In [[geometry]], '''flexagons''' are [[Plane (geometry)|flat]] models, usually constructed by folding strips of paper, that can be ''flexed'' or folded in certain ways to reveal faces besides the two that were originally on the back and front.

Flexagons are usually square or rectangular ('''tetraflexagons''') or [[hexagon]]al ('''hexaflexagons'''). A prefix can be added to the name to indicate the number of faces that the model can display, including the two faces (back and front) that are visible before flexing. For example, a hexaflexagon with a total of six faces is called a '''hexahexaflexagon'''.

In hexaflexagon theory (that is, concerning flexagons with six sides), flexagons are usually defined in terms of ''pats''.&lt;ref name="Oakley"&gt;{{cite journal |title=Flexagons |journal=The American Mathematical Monthly |publisher=Mathematical Association of America |first1=C. O. |last1=Oakley |first2=R. J. |last2=Wisner |volume=64 |issue=3 |date=March 1957 |pages=143–154 |jstor=2310544 |doi=10.2307/2310544}}&lt;/ref&gt;&lt;ref&gt;{{cite journal |url=http://www.sciencedirect.com/science/article/B6WDY-4W5VD4F-1/2/e1d94639a2f71f509b049f8ab6480cb7 |title=The combinatorics of all regular flexagons |journal=European Journal of Combinatorics |first1=Thomas |last1=Anderson |first2=T. Bruce |last2=McLean |first3=Homeira |last3=Pajoohesh |first4=Chasen |last4=Smith |volume=31 |issue=1 |date=January 2010 |pages=72–80 |doi=10.1016/j.ejc.2009.01.005}}&lt;/ref&gt;

Two flexagons are equivalent if one can be transformed to the other by a series of pinches and rotations. Flexagon equivalence is an [[equivalence relation]].&lt;ref name="Oakley" /&gt;

== History ==

=== Discovery and introduction ===

The discovery of the first flexagon, a trihexaflexagon, is credited to the British student [[Arthur Harold Stone|Arthur H. Stone]], who was studying at [[Princeton University]] in the United States in 1939. His new American paper would not fit in his English binder so he cut off the ends of the paper and began folding them into different shapes.&lt;ref name="Gardner1956"&gt;{{cite magazine |title=Flexagons |magazine=Scientific American |first=Martin |last=Gardner |volume=195 |issue=6 |pages=162–168 |date=December 1956 |doi=10.1038/scientificamerican1256-162 |oclc=4657622161}}&lt;/ref&gt; One of these formed a trihexaflexagon. Stone's colleagues [[Bryant Tuckerman]], [[Richard Feynman]], and [[John Tukey]] became interested in the idea and formed the Princeton Flexagon Committee. Tuckerman worked out a [[topology|topological]] method, called the Tuckerman traverse, for revealing all the faces of a flexagon.&lt;ref name="Gardner1988"&gt;{{cite book |title=Hexaflexagons and Other Mathematical Diversions: The First Scientific American Book of Puzzles and Games |publisher=University of Chicago Press |first=Martin |last=Gardner |year=1988 |isbn=0-226-28254-6}}&lt;/ref&gt;

Flexagons were introduced to the general public by the [[Recreational mathematics|recreational mathematician]] [[Martin Gardner]] in 1956 in the first ''[[Mathematical Games (column)|Mathematical Games]]'' column which he wrote for ''[[Scientific American]]'' magazine.&lt;ref name="Gardner1956"/&gt; In 1974, the magician [[Doug Henning]] included a construct-your-own hexaflexagon with the original cast recording of his Broadway show ''[[The Magic Show]]''.

=== Attempted commercial development ===

In 1955, Russell Rogers and Leonard D'Andrea of [[Homestead, Pennsylvania|Homestead Park, Pennsylvania]] applied for a patent, and in 1959 they were granted U.S. Patent number 2,883,195 for the hexahexaflexagon, under the title "Changeable Amusement Devices and the Like."

Their patent imagined possible applications of the device "as a toy, as an advertising display device, or as an educational geometric device."&lt;ref&gt;{{cite web |url=http://www.freepatentsonline.com/2883195.pdf |title=Changeable amusement devices and the like |work=Freepatentsonline.com |first1=Russell E. |last1=Rogers |first2=Leonard D. L. |last2=Andrea |id=U.S. Patent 2883195 |date=April 21, 1959 |accessdate=January 13, 2011}}&lt;/ref&gt; A few such novelties were produced by the [[Herbick &amp; Held Printing Company]], the printing company in [[Pittsburgh]] where Rogers worked, but the device, marketed as the "Hexmo", failed to catch on.

== Varieties ==

=== Tetraflexagons ===

[[File:Tritetraflexagon-net.PNG|thumb|alt=Diagram for folding a tritetraflexagon|A tritetraflexagon can be folded from a strip of paper as shown.]]

The tritetraflexagon is the simplest tetraflexagon (flexagon with [[square (geometry)|square]] sides). The "tri" in the name means it has three faces, two of which are visible at any given time if the flexagon is pressed flat. The construction of the tritetraflexagon is similar to the mechanism used in the traditional [[Jacob's ladder (toy)|Jacob's Ladder]] children's toy, in [[Rubik's Magic]]
and in the [[magic wallet]] trick or the [[Richard Himber|Himber]] wallet.

[[File:Tritetraflexagon-flexing.PNG|thumb|alt=Sides of a tritetraflexagon|This figure has two faces visible, built of squares marked with ''A''s and ''B''s. The face of ''C''s is hidden inside the flexagon.]]

A more complicated cyclic hexatetraflexagon requires no gluing. A cyclic hexatetraflexagon does not have any "dead ends", but the person making it can keep folding it until they reach the starting position. If the sides are colored in the process, the states can be seen more clearly.

=== Hexaflexagons ===

Hexaflexagons come in great variety, distinguished by the number of faces that can be achieved by flexing the assembled figure. (Note that the word ''hexaflexagons'' (with no prefixes) can sometimes refer to an ordinary hexahexaflexagon, with six sides instead of other numbers.)

==== Trihexaflexagon ====

[[File:Trihexaflexagon_example.png|thumb|This trihexaflexagon template shows 3 colors of 9 triangles, printed on one side, and folded to be colored on both sides. The two yellow triangles on the ends will end up taped together. The red and blue arcs are seen as full circles on the inside of one side or the other when folded.]]

A hexaflexagon with three faces. This is the simplest of the hexaflexagons to make and to manage, and is made from a single strip of paper, divided into nine equilateral triangles. (Some patterns provide ten triangles, two of which are glued together in the final assembly.)

To assemble, the strip is folded every third triangle, connecting back to itself after three inversions in the manner of the international [[recycling symbol]]. This makes a [[Möbius strip]] whose single edge forms a [[trefoil knot]].

==== Hexahexaflexagon ====

This hexaflexagon has six faces. It is made up of nineteen triangles folded from a strip of paper.
[[File:Hexahexaflexagon template.svg|thumb|center|600px|alt=A strip of paper, divided into triangles, which can be folded into a hexaflexagon.]]

Photos 1-6 below show the construction of a hexaflexagon made out of cardboard triangles on a backing made from a strip of cloth. It has been decorated in six colors; orange, blue, and red in figure 1 correspond to 1, 2, and 3 in the diagram above. The opposite side, figure 2, is decorated with purple, gray, and yellow. Note the different patterns used for the colors on the two sides. Figure 3 shows the first fold, and figure 4 the result of the first nine folds, which form a spiral. Figures 5-6 show the final folding of the spiral to make a hexagon; in 5, two red faces have been hidden by a valley fold, and in 6, two red faces on the bottom side have been hidden by a mountain fold. After figure 6, the final loose triangle is folded over and attached to the other end of the original strip so that one side is all blue, and the other all orange.

[[File:hexaflexagon-construction-and-use.jpg|thumb|center|400px|alt=A series of photos detailing construction and "flexing" of a hexaflexagon]]

Photos 7 and 8 show the process of everting the hexaflexagon to show the formerly hidden red triangles. By further manipulations, all six colors can be exposed. Faces 1, 2, and 3 are easier to find while faces 4, 5, and 6 are more difficult to find. An easy way to expose all six faces is using the Tuckerman traverse. It's named after Bryant Tuckerman, one of the first to investigate the properties of hexaflexagons. The Tuckerman traverse involves the repeated flexing by pinching one corner and flex from exactly the same corner every time. If the corner refuses to open, move to an adjacent corner and keep flexing. This procedure brings you to a 12-face cycle. During this procedure, however, 1, 2, and 3 show up three times as frequently as 4, 5, and 6. The cycle proceeds as follows:

1-3-6-1-3-2-4-3-2-1-5-2

And then back to 1 again.

Each color/face can also be exposed in more than one way. In figure 6, for example, each blue triangle has at the center its corner decorated with a wedge, but it is also possible, for example, to make the ones decorated with Y's come to the center. There are 18 such possible configurations for triangles with different colors, and they can be seen by flexing the hexahexaflexagon in all possible ways in theory, but only 15 can be flexed by the ordinary hexahexaflexagon. The 3 extra configurations are impossible due to the arrangement of the 4, 5, and 6 tiles at the back flap. (The 60-degree angles in the rhombi formed by the adjacent 4, 5, or 6 tiles will only appear on the sides and never will appear at the center because it would require one to cut the strip, which is topologically forbidden.)

Hexahexaflexagons can be constructed from different shaped nets of eighteen equilateral triangles. One hexahexaflexagon, constructed from an irregular paper strip, is almost identical to the one shown above, except that all 18 configurations can be flexed on this version.

==== Other hexaflexagons ====

While the most commonly seen hexaflexagons have either three or six faces, variations exist with four, five, seven, twelve, twenty-four, and forty-eight faces.

=== Higher order flexagons ===

==== Right octaflexagon and right dodecaflexagon ====
In these more recently discovered flexagons, each square or equilateral triangular face of a conventional flexagon is further divided into two right triangles, permitting additional flexing modes.&lt;ref&gt;{{cite web |url=http://www.eighthsquare.com/12-gon.html |title=Flexagon Discovery: The Shape-Shifting 12-Gon |publisher=Eighthsquare.com |first=Ann |last=Schwartz |year=2005 |accessdate=October 26, 2012}}&lt;/ref&gt; The division of the square faces of tetraflexagons into right isosceles triangles yields the octaflexagons,&lt;ref&gt;{{cite web |url=http://loki3.com/flex/octa.html |title=Octaflexagon |work=Loki3.com |first=Scott |last=Sherman |year=2007 |accessdate=October 26, 2012}}&lt;/ref&gt; and the division of the triangular faces of the hexaflexagons into 30-60-90 right triangles yields the dodecaflexagons.&lt;ref&gt;{{cite web |url=http://loki3.com/flex/dodeca.html |title=Dodecaflexagon |work=Loki3.com |first=Scott |last=Sherman |year=2007 |accessdate=October 26, 2012}}&lt;/ref&gt;

==== Pentaflexagon and right decaflexagon ====
In its flat state, the pentaflexagon looks much like the [[Chrysler]] logo: a regular [[pentagon]] divided from the center into five [[isosceles triangle]]s, with angles 72-54-54. Because of its fivefold symmetry, the pentaflexagon cannot be folded in half. However, a complex series of flexes results in its transformation from displaying sides one and two on the front and back, to displaying its previously hidden sides three and four.&lt;ref&gt;{{cite web |url=http://loki3.com/flex/penta.html |title=Pentaflexagon |work=Loki3.com |first=Scott |last=Sherman |year=2007 |accessdate=October 26, 2012}}&lt;/ref&gt;

By further dividing the 72-54-54 triangles of the pentaflexagon into 36-54-90 right triangles produces one variation of the 10-sided decaflexagon.&lt;ref&gt;{{cite web |url=http://loki3.com/flex/deca.html |title=Decaflexagon |work=Loki3.com |first=Scott |last=Sherman |year=2007 |accessdate=October 26, 2012}}&lt;/ref&gt;

==== Generalized isosceles n-flexagon ====
The pentaflexagon is one of an infinite sequence of flexagons based on dividing a regular ''n''-gon into ''n'' isosceles triangles. Other flexagons include the heptaflexagon,&lt;ref&gt;{{cite web |url=http://loki3.com/flex/hepta.html |title=Heptaflexagon |work=Loki3.com |first=Scott |last=Sherman |year=2007 |accessdate=October 26, 2012}}&lt;/ref&gt; the isosceles octaflexagon,&lt;ref&gt;{{cite web |url=http://loki3.com/flex/octa.html#iso |title=Octaflexagon: Isosceles Octaflexagon |work=Loki3.com |first=Scott |last=Sherman |year=2007 |accessdate=October 26, 2012}}&lt;/ref&gt; the enneaflexagon,&lt;ref&gt;{{cite web |url=http://loki3.com/flex/ennea.html#iso |title=Enneaflexagon: Isosceles Enneaflexagon |work=Loki3.com |first=Scott |last=Sherman |year=2007 |accessdate=October 26, 2012}}&lt;/ref&gt; and others.

==== Nonplanar pentaflexagon and nonplanar heptaflexagon ====

Harold V. McIntosh also describes "nonplanar" flexagons (i.e., ones which cannot be flexed so they lie flat); ones folded from [[pentagon]]s called ''pentaflexagons'',&lt;ref&gt;{{cite web |url=http://delta.cs.cinvestav.mx/~mcintosh/comun/pentags/pentags.html |title=Pentagonal Flexagons |publisher=Universidad Autónoma de Puebla ''via'' Cinvestav.mx |first=Harold V. |last=McIntosh |date=August 24, 2000 |accessdate=October 26, 2012}}&lt;/ref&gt; and from [[heptagon]]s called ''heptaflexagons''.&lt;ref&gt;{{cite web |url=http://delta.cs.cinvestav.mx/~mcintosh/comun/heptagon/heptagon.html |title=Heptagonal Flexagons |publisher=Universidad Autónoma de Puebla ''via'' Cinvestav.mx |first=Harold V. |last=McIntosh |date=March 11, 2000 |accessdate=October 26, 2012}}&lt;/ref&gt; These should be distinguished from the "ordinary" pentaflexagons and heptaflexagons described above, which are made out of [[isosceles triangle]]s, and they ''can'' be made to lie flat.

== In popular culture ==
Flexagons are also a popular book structure used by [[artist's book]] creators such as [[Julie Chen (book artist)|Julie Chen]] (''Life Cycle'') and [[Edward H. Hutchins]] (''Album'' and ''Voces de México''). Instructions for making tetra-tetra-flexagon and cross-flexagons are included in ''Making Handmade Books: 100+ Bindings, Structures and Forms'' by Alisa Golden.&lt;ref&gt;{{cite book |title=Making Handmade Books: 100+ Bindings, Structures &amp; Forms |publisher=Lark Crafts |first=Alisa J. |last=Golden |date=2011 |pages=130, 132-133 |isbn=978-1-60059-587-5}}&lt;/ref&gt;

A high-order hexaflexagon was used as a plot element in [[Piers Anthony]]'s novel ''[[Of Man and Manta|{{underline|{{overline|0Ⅹ}}}}]]'', in which a flex was analogous to the travel between alternate universes.&lt;ref name="Collings1984"&gt;{{cite book |url=https://books.google.com/books?id=wcomJEpp08oC&amp;pg=PA47&amp;lpg=PA47 |title=Piers Anthony |series=Starmont Reader's Guide #20 |publisher=Borgo Press |first=Michael R. |last=Collings |pages=47-48 |date=1984 |isbn=0-89370-058-4}}&lt;/ref&gt;

== Bibliography ==
*{{cite book |last=Mitchell |first=David |title=The Magic of Flexagons – Paper curiosities to cut out and make |year=2000 |publisher=Tarquin |isbn=1-899618-28-7}}
*{{cite book |last=Pook |first=Les |title=Serious Fun with Flexagons, A Compendium and Guide |year=2009 |publisher=Springer |isbn=90-481-2502-2}}
*{{cite book |last=Pook |first=Les |title=Flexagons Inside Out |year=2006 |publisher=Cambridge University Press |isbn=0-521-81970-9}}
* [[Martin Gardner]] wrote an excellent introduction to hexaflexagons in the December 1956 ''Mathematical Games'' column in ''Scientific American''. It also appears in:
**{{cite book |title=The "Scientific American" Book of Mathematical Puzzles and Diversions |publisher=Simon &amp; Schuster |year=1959}}
**{{cite book |title=Hexaflexagons and Other Mathematical Diversions: The First "Scientific American" Book of Puzzles and Games |publisher=University of Chicago Press |year=1988 |isbn=0-226-28254-6}}
**{{cite book |title=The Colossal Book of Mathematics |publisher=W. W. Norton &amp; Co. |year=2001 |isbn=0-393-02023-1}}
**{{cite book |title=Hexaflexagons, Probability Paradoxes, and the Tower of Hanoi: Martin Gardner's First Book of Mathematical Puzzles and Games |publisher=Cambridge University Press |year=2008 |isbn=0-521-73525-4}}
**{{cite journal |title=Hexaflexagons |journal=[[The College Mathematics Journal]] |volume=43 |issue=1 |pages=2-5 |date=January 2012 |doi=10.4169/college.math.j.43.1.002 |jstor=10.4169/college.math.j.43.1.002}} The issue also contains another article by Pook, and one by Iacob, McLean, and Hua.

== See also ==
*[[Cayley tree]]
*[[Geometric group theory]]
*[[Kaleidocycle]]

== References ==
{{reflist|2}}

== External links ==
{{linkfarm|date=January 2017}}
{{Commons category|Flexagons}}
'''Flexagons:'''
*[http://delta.cs.cinvestav.mx/~mcintosh/comun/fxgonw/fxgon.html My Flexagon Experiences] by Harold V. McIntosh – contains historical information and theory
*[http://www.flexagon.net/ The Flexagon Portal]{{spaced ndash}}Robin Moseley's site has patterns for a large variety of flexagons.
*[http://www.mathematische-basteleien.de/flexagons.htm Flexagons]
*[http://loki3.com/flex/ Flexagons]{{spaced ndash}}Scott Sherman's site, with variety of flexagons of different shapes.

'''Tetraflexagons:'''
*[[MathWorld]]'s page on [http://mathworld.wolfram.com/Tetraflexagon.html tetraflexagons], including three nets
* [http://droppingmadscience.blogspot.com/2007/02/nalu.html Folding User Interfaces] – A mobile phone design concept based on a tetraflexagon;  Folding the design gives access to different user interfaces.
* [http://lab.satyr.nl/flex Flexifier] – a simple online tetraflexagon generator
* [http://www.metacafe.com/watch/1007181/the_best_of_paper_toy_just_one_piece_of_paper_no_glue_6_face/ Instructions for making cyclic hexa-tetraflexagon from one piece of paper.]

'''Hexaflexagons:'''
* [http://theory.lcs.mit.edu/~edemaine/flexagons/Conrad-Hartline-1962/flexagon.html Flexagons]{{spaced ndash}}1962 paper by Antony S. Conrad and Daniel K. Hartline (RIAS)
* [http://www.puzzles.com/hexaflexagon/index.html Celebration of Mind - Hexaflexagon Site]
* [http://mathworld.wolfram.com/Hexaflexagon.html MathWorld entry on Hexaflexagons]
* [http://hexaflexagon.sourceforge.net Hexaflexagon Toolkit] software for printing flexagons from one's own pictures
* [http://www.coe.ufrj.br/~acmq/hexaflexagons/ Hexaflexagons]{{spaced ndash}}a catalog compiled by Antonio Carlos M. de Queiroz (c.1973).&lt;br&gt;Includes a program named HexaFind that finds all the possible Tuckerman traverses for given orders of hexaflexagons.
*[http://www.woollythoughts.com/foldingcushions.html Crochet hexaflexagon cushion]
* [[Yutaka Nishiyama]] (2010). [http://www.ijpam.eu/contents/2010-58-1/11/11.pdf "General Solution for Multiple Foldings of Hexaflexagons"] IJPAM, Vol. 58, No. 1, 113-124. [http://www.osaka-ue.ac.jp/zemi/nishiyama/mathpuzzle/hexflex19.pdf "19 faces of Flexagons"]
* [[Vi Hart]] video on Hexaflexagons [https://www.youtube.com/watch?v=VIVIegSt81k part 1] [https://www.youtube.com/watch?v=paQ10POrZh8 part 2]
* [https://github.com/michibo/hexaflexa Hexaflexa]{{spaced ndash}}an Open Source program to create hexaflexagon printouts.
* [http://youdomaths.com/2018/03/26/trihexaflexagons/ PHP TriHexaFlexagon Generator]

[[Category:Recreational mathematics]]
[[Category:Mechanical puzzles]]
[[Category:Paper folding]]
[[Category:Geometric group theory]]
[[Category:Paper toys]]</text>
      <sha1>kw2rxx3pebvlkezv2sldtq1nzs0pl5a</sha1>
    </revision>
  </page>
  <page>
    <title>FreeFlyer</title>
    <ns>0</ns>
    <id>27506488</id>
    <revision>
      <id>830747742</id>
      <parentid>822531954</parentid>
      <timestamp>2018-03-16T18:10:55Z</timestamp>
      <contributor>
        <username>Tom.Reding</username>
        <id>9784415</id>
      </contributor>
      <minor/>
      <comment>Fix [[:Category:Pages using deprecated image syntax]]; [[WP:GenFixes]] on, using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10626">{{orphan|date=June 2010}}

{{Infobox software
| name = FreeFlyer
| screenshot             = FreeFlyer v721 GUI Screenshot.png
| screenshot size        = 300px
| caption                = The FreeFlyer GUI with 2D and 3D output
| developer              = [[a.i. solutions, Inc.]]
| latest release version = 7.3.1
| latest release date    = {{start date and age|2018|1}}
| status                 = Active
| operating system       = [[Microsoft Windows|Windows]], [[Linux]]
| genre                  = [[Technical computing]]
| platform               = [[x86]], [[x86-64]]
| license                = [[Proprietary software|Proprietary]] [[commercial software]]
| website = {{URL|https://ai-solutions.com/freeflyer/}}
}}

'''FreeFlyer''' is a commercial off-the-shelf software application for use in [[satellite]] mission analysis, design and operations.  FreeFlyer's architecture centers on its native scripting language, known as FreeForm script.  As a mission planning tool, it encompasses several capabilities, including precise [[orbit]] modeling, 2D and [[Visualization (computer graphics)|3D visualization]], sensor modeling, maneuver modeling, maneuver estimation, plotting, orbit determination, tracking data simulation, and space environment modeling.

FreeFlyer implements standard astrodynamics models such as the JGM-2, [[EGM96|EGM-96]], LP-165 gravity potential models; the [[Jacchia Reference Atmosphere|Jacchia-Roberts]], Harris-Priester, and [[NRLMSISE-00|NRL-MSIS]] atmospheric density models; the [[International Reference Ionosphere]] model; and the [[International Geomagnetic Reference Field]] magnetic field model.&lt;ref name="FF"&gt;{{cite web | url = http://www.ai-solutions.com/freeflyer/ | title = FreeFlyer | accessdate = March 26, 2010 |date=January 2010}}&lt;/ref&gt;

==Background==
a.i. solutions, Inc. is the owner and developer of FreeFlyer which has been in use since 1997. FreeFlyer is utilized by [[NASA]], NOAA, and the [[USAF]] for space mission operations, mission assurance, and analysis support.&lt;ref name="spaceNews"&gt;{{cite web | url = http://www.spacenews.com/contracts/solutions-lead-goddard-support-teams.html | title = a.i. solutions To Lead Goddard Support Teams | accessdate = December 15, 2009 |date=October 2009}}&lt;/ref&gt;&lt;ref name="spaceRef"&gt;{{cite web | url = http://www.spaceref.com/news/viewpr.html?pid=29308 | title = NASA Selects a.i. solutions for $95M Flight Dynamics Support Services (FDSS) Contract | accessdate = January 24, 2010 |date=October 2009}}&lt;/ref&gt;&lt;ref name="spaceRef2"&gt;{{cite web | url = http://www.spaceref.com/news/viewpr.html?pid=8477 | title = Hadron/Analex Awarded $160 Million NASA Contract | accessdate = January 20, 2010 |date=May 2002}}&lt;/ref&gt;&lt;ref name="bizJournal"&gt;{{cite news | url = http://www.bizjournals.com/washington/stories/2005/10/03/daily2.html | title = Analex wins $65M Elvis contract extension | accessdate = January 20, 2010 |date=October 2005 | first=Jeff | last=Clabaugh}}&lt;/ref&gt;&lt;ref name="fboArchive"&gt;{{cite web | url = https://www.fbodaily.com/archive/2010/01-January/24-Jan-2010/FBO-02048435.htm | title = Missile Defense Agency Engineering and Support Services (MiDAESS) Quality Safety and Mission Assurance (QSMA) Functional Capability Group | accessdate = March 20, 2010 |date=January 2010}}&lt;/ref&gt;&lt;ref name="ERBISpatent"&gt;{{cite web | url = http://www.freepatentsonline.com/y2008/0005122.html | title = ENGINEERING REVIEW INFORMATION SYSTEM US Patent 20080005122 a.i. solutions, Inc. | accessdate = March 26, 2010 |date=November 2008}}&lt;/ref&gt;

===Operational and analysis support===
FreeFlyer has been used to support many spacecraft missions, for mission planning analysis, operational analysis, or both.  Specific mission examples include the [[International Space Station]] (ISS),&lt;ref name="CTS"&gt;{{cite web | url = http://www.satnews.com/cgi-bin/story.cgi?number=2117338379 | title = Johnson Space Center Core Trajectory Subsystem Contract Award | accessdate = December 12, 2011 |date=December 2011}}&lt;/ref&gt; the [[Joint Space Operations Center|JSpOC]] Mission System,&lt;ref name="JMS"&gt;{{cite web | url = http://www.marketwired.com/press-release/ai-solutions-awarded-contract-provide-freeflyerr-software-dods-joint-space-operations-1746494.htm | title = JMS Contract Award | accessdate = January 30, 2013 | date=January 2013}}&lt;/ref&gt; the [[Earth Observing System]],&lt;ref name="EOS"&gt;{{cite paper | citeseerx = 10.1.1.119.5197 | title = Automated Flight Dynamics Product Generation for the EOS AM-1 Spacecraft | first1 = Carla | last1 = Matusow | first2 = Robert | last2 = Wiegand }}&lt;/ref&gt;&lt;ref name="EOSCluster"&gt;[http://api.ning.com/files/ELzdJ8RcXavUJ6yVo3aoQEhKB3RTksd2sfNN3jl2gzMi0dnpGXr3bXuEXhzCtowqN5jxVMLGPQaX6Wf9XMGuZKpkdN*Tx41N/CloseApproachPredictionAnalysisOfTheEarthScienceConstellationWithTheFengyun1CDebris.pdf Close Approach Prediction Analysis of the Earth Science Constellation with the Fengyun-1C Debris, by Matthew Duncan and David Rand]&lt;/ref&gt; [[Solar Dynamics Observatory]] (SDO),&lt;ref name="SDO"&gt;[https://www.fbo.gov/index?s=opportunity&amp;mode=form&amp;id=ce9571f9c0fa0d430c5bf31f730ea62b&amp;tab=core&amp;_cview=0 FedBizOpps]&lt;/ref&gt; and [[Magnetospheric Multiscale Mission]] (MMS).&lt;ref name="MMS Apogee Raising"&gt;[http://www.ai-solutions.com/file.asp?F=A47ACEE6AC8845AF970F4B86643ADB38.pdf&amp;N=Apogee+Raising+Technique+for+the+MMS+Formation+Flying+Mission.pdf&amp;C=library Apogee Raising Technique for the MMS Formation Flying Mission, by Craig Roberts, Jason Tichy, and Cheryl Gramling]&lt;/ref&gt;
&lt;!-- reference to a DLR Paper with FreeFlyer&lt;ref name="DLR"&gt;[http://elib.dlr.de/19850/01/IAA-B5-1010P_OD_FullPaper.pdf Operational Aspects of Orbit Determination with GPS for Small Satellites with a SAR Payload, by Sergio De Florio, Tino Zehetbauer, and Dr. Thomas Neff]&lt;/ref&gt; --&gt;
&lt;!-- Article that mentions and describes FreeFlyer http://military-training-technology.com/msmf-archives/63-msmf-2008-volume1-issue-1/463-beyond-computer-instructions-and-data.html --&gt;

FreeFlyer has also been successfully used to conduct analysis in both the [[high-performance computing]] (HPC) and [[service-oriented architecture]] (SOA) environments.&lt;ref name="HPC"&gt;{{cite web | url = http://www.microsoft.com/casestudies/Case_Study_Detail.aspx?casestudyid=4000003011 | title = Microsoft HPC Case Study | accessdate = March 26, 2010 |date=June 2001}}&lt;/ref&gt;&lt;ref name="Crosslink"&gt;{{cite web | url = http://www.aero.org/publications/crosslink/summer2009/04.html | title = A Flexible Satellite Command and Control Framework | accessdate = March 26, 2010 |date=September 2009}}&lt;/ref&gt;

==Software tiers==
FreeFlyer is one stand-alone product, with no added modules.  However, it has two tiers of rising functionality.

{| border="2" class="wikitable"
|-
! style="background:#ABE" | '''Engineer'''
! style="background:#ABE" | '''Mission'''
|-
| valign = "top" | The Engineer tier includes:
* Full-featured scripting language and IDE
* Integrated 2D and 3D visualizations with customizable layouts
* Multiple spacecraft and hardware modeling including sensors, antennas, tanks, thrusters, and solar arrays
* Coverage analysis including sensors, antennas, ground stations, and other spacecraft
* Impulsive and finite maneuver modeling, including targeting system
* Spacecraft attitude modeling including attitude matrix, [[quaternion]]s, [[Euler angles]], RA/DEC, spin rates, and attitude history files
* Complex sensor and antenna modeling, such as custom patterns and obscuration masking
* Complex maneuver modeling including finite burns and supporting chemical and electrical (low-thrust) propulsion systems
* Complex coverage analysis, visibility and access calculations
* Added coordinate system types including custom/user-defined systems
* [[MATLAB]] Interface&lt;ref name="Matlab"&gt;{{cite web | url = http://www.mathworks.com/products/connections/product_detail/product_35478.html | title = Mathworks.com | accessdate = March 26, 2010 | year = 2008 }}&lt;/ref&gt;
* Automation of flight dynamics tasks
* [[Monte Carlo method|Monte Carlo]] analysis
* Collision Avoidance/Conjunction Analysis
* Formation Flying
* Mission Plan performance profiling
| valign = "top" | The Mission tier includes all Engineer functionality, plus:
* [[TCP/IP]] socket and [[SQL]] database interface
* Orbit determination and error analysis
** Estimation in Cartesian or Equinoctial element sets
** [[Weighted least squares#Weighted least squares|Batch Least Squares]]
** [[Extended Kalman Filter]]
** [[Kalman filter#Unscented Kalman filter|Unscented Kalman Filter]]
** Square Root Information Filter
* Generation of [[Two-line element set|NORAD Two-Line Element]] states from ephemeris or observational data
* Access to the FreeFlyer Runtime [[Application Programming Interface|API]] from [[C (programming language)|C]], [[C Sharp (programming language)|C#]], Python, or [[Java (programming language)|Java]]
* Ability to include custom objects and force models via [[C Sharp (programming language)|C#]] extensions: [[Component Object Model|COM]] objects
* Easily customizable graphic user interface
* Generic [[ASCII]]/[[binary file|binary]] file read write
* Automatic e-mail notification
|-
|}

==FreeFlyer scripting==
The FreeFlyer Engineer and Mission tiers contain an integrated scripting language and development environment.  The scripting language is an [[object-oriented programming|object-oriented]] script with objects and commands. Objects include properties and methods.

An example of FreeFlyer scripting is this:

 &lt;span style="color:green;"&gt;// Create a spacecraft object&lt;/span&gt;
 &lt;span style="color:#5f9ea0;"&gt;Spacecraft&lt;/span&gt; sc1;
 &lt;span style="color:green;"&gt;// Propagate and view the spacecraft for one day&lt;/span&gt;
 &lt;span style="color:blue;"&gt;While&lt;/span&gt; (sc1.ElapsedDays &amp;lt; 1);
 &lt;span style="color:blue;"&gt;    Step&lt;/span&gt; sc1;
 &lt;span style="color:blue;"&gt;    View&lt;/span&gt; sc1;
 &lt;span style="color:blue;"&gt;End;&lt;/span&gt;

==References==
{{clear}}
{{Reflist|colwidth=30em}}

==External links==
* {{official website|https://ai-solutions.com/freeflyer/freeflyer/}}
* [https://ai-solutions.com/request-evaluation/ Request an Evaluation Copy of FreeFlyer.]
* [https://ai-solutions.com/freeflyer-login/ FreeFlyer Technical Support Resources.]
* [https://ai-solutions.com/about-us/news-multimedia/?post_category=freeflyer-blog FreeFlyer Blog.]
* [https://docs.com/michael-barton/5675/freeflyer-analysis-software-for-spacecraft-mission FreeFlyer Capabilities.]

{{DEFAULTSORT:Freeflyer}}
[[Category:3D graphics software]]
[[Category:Aerospace engineering software]]
[[Category:Astronomy software]]
[[Category:Mathematical software]]
[[Category:Physics software]]
[[Category:Science software for Windows]]</text>
      <sha1>l34k0en7oqfmsn6tbv1fd9pzreo7u9p</sha1>
    </revision>
  </page>
  <page>
    <title>Fréchet mean</title>
    <ns>0</ns>
    <id>25021971</id>
    <revision>
      <id>869771122</id>
      <parentid>852623373</parentid>
      <timestamp>2018-11-20T07:17:57Z</timestamp>
      <contributor>
        <username>Citation bot</username>
        <id>7903804</id>
      </contributor>
      <minor/>
      <comment>Add: doi, pages, issue, volume, journal. Removed parameters. Formatted [[WP:ENDASH|dashes]]. You can [[WP:UCB|use this bot]] yourself. [[WP:DBUG|Report bugs here]]. | [[WP:UCB|User-activated]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4289">In [[mathematics]] and [[statistics]], the '''Fréchet mean''' is a generalization of [[centroid]]s to [[metric space]]s, giving a single representative point or [[central tendency]] for a cluster of points. It is named after [[Maurice Fréchet]]. '''Karcher mean''' is the renaming of the Riemannian Center of Mass construction developed by Karsten Grove and Hermann Karcher.&lt;ref name="gk73"&gt;{{citation|title=How to conjugate C1-close group actions, Math.Z. 132|journal=Mathematische Zeitschrift|volume=132|issue=1|pages=11–20|first1=Karsten|last1=Grove|first2=Hermann|last2=Karcher|year=1973|isbn=|url=https://link.springer.com/content/pdf/10.1007/BF01214029.pdf|doi=10.1007/BF01214029}}.&lt;/ref&gt;&lt;ref name="nb12"&gt;{{citation|title=Matrix Information Geometry|first1=Frank|last1=Nielsen|first2=Rajendra|last2=Bhatia|publisher=Springer|year=2012|isbn=9783642302329|page=171|url=https://books.google.com/books?id=MAhygTspBU8C&amp;pg=PA171}}.&lt;/ref&gt; On the real numbers, the [[arithmetic mean]], [[median]], [[geometric mean]], and [[harmonic mean]] can all be interpreted as Fréchet means for different distance functions.

==Definition==

Let (''M'', ''d'') be a complete metric space. Let ''x''&lt;sub&gt;1&lt;/sub&gt;, ''x''&lt;sub&gt;2&lt;/sub&gt;, …, ''x''&lt;sub&gt;''N''&lt;/sub&gt; be random points in ''M''. For any point ''p'' in ''M'', define the '''Fréchet variance''' to be the sum of squared distances from ''p'' to the ''x''&lt;sub&gt;''i''&lt;/sub&gt;:
:&lt;math&gt;\Psi(p) = \sum_{i=1}^N d^2\left(p, x_i\right)&lt;/math&gt;

The '''Karcher means''' are then those points, ''m'' of ''M'', which [[local minimum|locally minimise]] Ψ:&lt;ref name="nb12"/&gt;
:&lt;math&gt;m = \mathop{\text{arg min}}_{p \in M} \sum_{i=1}^N d^2\left(p, x_i\right)&lt;/math&gt;

If there is an ''m'' of ''M'' that globally minimises Ψ, then it is '''Fréchet mean'''.

Sometimes, the ''x''&lt;sub&gt;''i''&lt;/sub&gt; are assigned weights ''w''&lt;sub&gt;''i''&lt;/sub&gt;, the Fréchet variance is calculated at a weighted sum,
:&lt;math&gt;\Psi(p) = \sum_{i=1}^N w_i d^2\left(p, x_i\right), \;\;\;\; m = \mathop{\text{arg min}}_{p \in M} \sum_{i=1}^N w_i d^2\left(p, x_i\right).&lt;/math&gt;

==Examples of Fréchet means==

=== Arithmetic mean and median ===

For real numbers, the [[arithmetic mean]] is a Fréchet mean, using the usual Euclidean distance as the distance function. The [[median]] is also a Fréchet mean, using the square root of the distance.&lt;ref name="rb12-136"&gt;{{harvtxt|Nielsen|Bhatia|2012}}, [https://books.google.com/books?id=MAhygTspBU8C&amp;pg=PA136 p.&amp;nbsp;136].&lt;/ref&gt;

===Geometric mean===
On the positive real numbers, the (hyperbolic) distance function &lt;math&gt; d(x,y)= | \log(x) - \log(y) |&lt;/math&gt; can be defined. The [[geometric mean]] is the corresponding Fréchet mean. Indeed  &lt;math&gt; f:x\mapsto e^x&lt;/math&gt; is then an isometry from the euclidean space to this "hyperbolic" space and must respect the Fréchet mean: the Fréchet mean of the &lt;math&gt;x_i&lt;/math&gt; is the image by &lt;math&gt;f&lt;/math&gt; of the Fréchet mean (in the Euclidean sense) of the &lt;math&gt; f^{-1}(x_i)&lt;/math&gt;, i.e. it must be:

:&lt;math&gt; f\left( \frac{1}{n}\sum_{i=1}^n f^{-1}\left(x_i\right)\right) = \exp \left( \frac{1}{n} \sum_{i=1}^n\log x_i \right) = \sqrt[n]{x_1 \cdots x_n}&lt;/math&gt;.

===Harmonic mean===
On the [[positive real numbers]], the [[Metric (mathematics)|metric]] (distance function):
: &lt;math&gt;d_\operatorname{H}(x,y) = \left| \frac{1}{x} - \frac{1}{y} \right|&lt;/math&gt;

can be defined. The [[harmonic mean]] is the corresponding Fréchet mean.{{Citation needed|date=October 2010}}

===Power means===
Given a non-zero real number &lt;math&gt;m&lt;/math&gt;, the [[power mean]] can be obtained as a Fréchet mean by introducing the metric{{Citation needed|date=October 2010}}
: &lt;math&gt;d_m\left(x, y\right) = \left| x^m - y^m \right|&lt;/math&gt;

===f-mean===
Given an invertible function &lt;math&gt;f&lt;/math&gt;, the f-mean can be defined as the Fréchet mean obtained by using the metric:{{Citation needed|date=October 2010}}
: &lt;math&gt;d_f(x,y) = \left|f(x) - f(y)\right|&lt;/math&gt;

This is sometimes called the [[generalised f-mean]] or [[quasi-arithmetic mean]].

===Weighted means===
The general definition of the Fréchet mean that includes the possibility of weighting observations can be used to derive weighted versions for all of the above types of means.

==References==
{{reflist}}

{{DEFAULTSORT:Frechet Mean}}
[[Category:Means]]</text>
      <sha1>mjlyim8oje0fzr71vm5sdx7mo6chq4y</sha1>
    </revision>
  </page>
  <page>
    <title>Future value</title>
    <ns>0</ns>
    <id>578327</id>
    <revision>
      <id>840454888</id>
      <parentid>840453967</parentid>
      <timestamp>2018-05-10T00:17:48Z</timestamp>
      <contributor>
        <username>CASSIOPEIA</username>
        <id>31051948</id>
      </contributor>
      <minor/>
      <comment>Reverted edits by [[Special:Contributions/216.228.112.22|216.228.112.22]] ([[User talk:216.228.112.22|talk]]): unexplained content removal ([[WP:HG|HG]]) (3.3.5)</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8978">{{for|the computer science concept|Futures and promises}}
{{Refimprove|date=January 2010}}
'''Future value''' is the [[value (economics)|value]] of an [[asset]] at a specific date.&lt;ref&gt;[http://student.education2020.com/activities/vocab.aspx?keystr=112610&amp;order=070605&amp;stbl=2420646] - Education 2020 Homeschool Console, class "Economic Math"; definition of FUTURE VALUE: ''"Future value is the value of an asset at a specific date."''&lt;/ref&gt; It measures the nominal future sum of money that a given sum of money is "worth" at a specified time in the future assuming a certain [[interest rate]], or more generally, [[rate of return]]; it is the [[present value]] multiplied by the [[accumulation function]].&lt;ref&gt;EDUCATION 2020 HOMESCHOOL CONSOLE. FORMULA FOR CALCULATING THE FUTURE VALUE OF AN ANNUITY. URL:http://magic.education2020.com/vocImages/105937-future-value.jpg. Accessed: 2011-04-14. (Archived by WebCite® at https://www.webcitation.org/5xwWgqCpT)&lt;/ref&gt;
The value does not include corrections for inflation or other factors that affect the true value of money in the future.  This is used in [[time value of money]] calculations.

==Overview==
Money value [[inflation|fluctuates]] over time: $100 today has a different value than $100 in five years. This is because one can invest $100 today in an interest-bearing bank account or any other investment, and that money will grow/shrink due to the rate of return. Also, if $100 today allows the purchase of an item, it is possible that $100 will not be enough to purchase the same item in five years, because of [[inflation]] (increase in purchase price).

An investor who has some money has two options: to spend it right now or to invest it. The financial compensation for saving it (and not spending it) is that the money value will accrue through the interests that he will receive from a borrower (the bank account on which he has the money deposited).

Therefore, to evaluate the real worthiness of an amount of money today after a given period of time, economic agents compound the amount of money at a given interest rate. Most [[actuarial]] calculations use the [[risk-free interest rate]] which corresponds the minimum guaranteed rate provided the bank's saving account, for example. If one wants to compare their change in [[purchasing power]], then they should use the [[real interest rate]] ([[nominal interest rate]] minus [[inflation]] rate).

The operation of evaluating a present value into the future value is called capitalization (how much will $100 today be worth in 5 years?). The reverse operation which consists in evaluating the present value of a future amount of money is called a [[discounting]] (how much $100 that will be received in 5 years- at a [[lottery]], for example -are worth today?).

It follows that if one has to choose between receiving $100 today and $100 in one year, the rational decision is to cash the $100 today. If the money is to be received in one year and assuming the savings account interest rate is 5%, the person has to be offered at least $105 in one year so that two options are equivalent (either receiving $100 today or receiving $105 in one year). This is because if you have cash of $100 today and deposit in your savings account, you will have $105 in one year.

==Simple interest==
To determine future value (FV) using [[simple interest]] (i.e., without compounding):

:&lt;math&gt;FV = PV(1+rt)&lt;/math&gt;

where ''PV'' is the [[present value]] or principal, ''t'' is the time in years (or a fraction of year), and ''r'' stands for the per annum [[interest]] rate. [[Simple interest]] is rarely used, as compounding is considered more meaningful {{Citation needed|date=January 2010}}. Indeed, the Future Value in this case grows linearly (it's a [[linear function]] of the initial investment): it doesn't take into account the fact that the interest earned might be compounded itself and produce further interest (which corresponds to an [[exponential growth]] of the initial investment -see below-).
{{Expand section|date=January 2010}}

==Compound interest==
To determine '''future value''' using [[compound interest]]:

:&lt;math&gt;FV = PV(1+i)^t&lt;/math&gt;&lt;ref name="isbn0-324-65114-7"&gt;{{cite book |author1=Francis, Jennifer Yvonne |author2=Stickney, Clyde P. |author3=Weil, Roman L. |author4=Schipper, Katherine |title=Financial accounting: an introduction to concepts, methods, and uses |publisher=South-Western Cengage Learning |location= |year=2010 |page=806 |isbn=0-324-65114-7 }}&lt;/ref&gt;

where ''PV'' is the [[present value]], ''t'' is the number of compounding periods  (not necessarily an integer), and ''i'' is the interest rate for that period. Thus the future value [[Exponential growth|increases exponentially]] with time when ''i'' is positive. The [[Compound annual growth rate|growth rate]] is given by the period, and ''i'', the interest rate for that period. Alternatively the growth rate is expressed by the interest per unit time based on [[Compound interest#Continuous compounding|continuous compounding]]. For example, the following all represent the same growth rate:
*3 % per half year
*6.09 % per year ([[effective annual rate]], [[rate of return|annual rate of return]], the standard way of expressing the growth rate, for easy comparisons)
*2.95588022 %  per half year based on continuous compounding (because ln 1.03 = 0.0295588022)
*5.91176045 %  per year based on continuous compounding (simply twice the previous percentage)

Also the growth rate may be expressed in a percentage per period ([[nominal interest rate|nominal rate]]), with another period as compounding basis; for the same growth rate we have:
*6% per year with half a year as compounding basis

To convert an interest rate from one compounding basis to another compounding basis (between different periodic interest rates), the following formula applies:

:&lt;math&gt;i_2=\left[\left(1+\frac{i_1}{n_1}\right)^\frac{n_1}{n_2}-1\right]{\times}n_2&lt;/math&gt;

where
''i''&lt;sub&gt;1&lt;/sub&gt; is the periodic interest rate with compounding frequency ''n''&lt;sub&gt;1&lt;/sub&gt; and
''i''&lt;sub&gt;2&lt;/sub&gt; is the periodic interest rate with compounding frequency ''n''&lt;sub&gt;2&lt;/sub&gt;.

If the compounding frequency is annual, ''n''&lt;sub&gt;2&lt;/sub&gt; will be 1, and to get the annual interest rate (which may be referred to as the [[effective interest rate]], or the [[annual percentage rate]]), the formula can be simplified to:

:&lt;math&gt;r = \left( 1 + { i \over n } \right)^n - 1 &lt;/math&gt;

where ''r'' is the annual rate, ''i'' the periodic rate, and ''n'' the number of compounding periods per year.

Problems become more complex as you account for more variables.  For example, when accounting for [[Annuity (finance theory)|annuities]] (annual payments), there is no simple ''PV'' to plug into the equation. Either the ''PV'' must be calculated first, or a more complex annuity equation must be used. Another complication is when the interest rate is applied multiple times per period. For example, suppose the 10% interest rate in the earlier example is compounded twice a year (semi-annually).  Compounding means that each successive application of the interest rate applies to all of the previously accumulated amount, so instead of getting 0.05 each 6 months, one must figure out the true annual interest rate, which in this case would be 1.1025 (one would divide the 10% by two to get 5%, then apply it twice: 1.05&lt;sup&gt;2&lt;/sup&gt;.)  This 1.1025 represents the original amount 1.00 plus 0.05 in 6 months to make a total of 1.05, and get the same rate of interest on that 1.05 for the remaining 6 months of the year.  The second six-month period returns more than the first six months because the interest rate applies to the accumulated interest as well as the original amount.

This formula gives the future value (FV) of an ordinary [[Annuity (finance theory)|annuity]] (assuming compound interest):&lt;ref name="isbn0-07-140665-4"&gt;{{cite book |author=Vance, David |title=Financial analysis and decision making: tools and techniques to solve financial problems and make effective business decisions |publisher=McGraw-Hill |location=New York |year=2003 |page=99 |isbn=0-07-140665-4 }}&lt;/ref&gt;

:&lt;math&gt;FV_\mathrm{annuity} = {(1+r)^n - 1 \over r} \cdot \mathrm{(payment\ amount)}&lt;/math&gt;

where ''r'' = interest rate; ''n'' = number of periods.  The simplest way to understand the above formula is to cognitively split the right side of the equation into two parts, the payment amount, and the ratio of compounding over basic interest.  The ratio of compounding is composed of the aforementioned effective interest rate over the basic (nominal) interest rate.  This provides a ratio that increases the payment amount in terms present value.

==See also==
*[[Lifetime value]]
*[[Present value]]
*[[Time value of money]]

==References==
{{reflist}}

==External links==
*[http://formularium.org/?go=62 calculate the different FV's with one's own values]

[[Category:Theory of value (economics)]]
[[Category:Mathematical finance]]</text>
      <sha1>r7s7w17h8hu399smr84d768x61q47s7</sha1>
    </revision>
  </page>
  <page>
    <title>Generalized Ozaki cost function</title>
    <ns>0</ns>
    <id>14394227</id>
    <revision>
      <id>843461031</id>
      <parentid>805721279</parentid>
      <timestamp>2018-05-29T09:00:38Z</timestamp>
      <contributor>
        <username>Tony1</username>
        <id>332841</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3053">{{Orphan|date=October 2017}}

{{Expert needed|economics|date=November 2008}}
In [[economics]] the '''generalized-Ozaki cost''' is a general description of [[cost]] described by Shuichi Nakamura.&lt;ref&gt;{{cite journal |author=Shinichiro Nakamura |title=A Nonhomothetic Generalized Leontief Cost Function Based on Pooled Data |journal=The Review of Economics and Statistics |volume=72 |issue=4 |year=1990 |pages=649–656 |doi=10.2307/2109605 |jstor=2109605 |publisher=The MIT Press}}&lt;/ref&gt;

For output ''y'', at date ''t'' and a vector of ''m'' input prices ''p'', the generalized-Ozaki cost, ''c'', is

: &lt;math&gt;c(p,y,t) = \sum_i b_{ii} \left( y^{b_{yi}}e^{b_{ti}t} p_i + \sum_{j\,:\,j\neq i} b_{ij} \sqrt{p_ip_j} y^{b_y} e^{b_tt}\right).&lt;/math&gt;

==Discussion==

In [[econometrics]] it is often desirable to have a model of the [[cost]] of [[Manufacturing|production]] of a given output with given inputs&amp;mdash;or in common terms, what it will cost to produce some number of goods at prevailing prices, or given prevailing prices and a budget, how much can be made.  Generally there are two parts to a cost function, the [[fixed costs|fixed]] and [[variable costs]] involved in production.

The [[marginal cost]] is the change in the cost of production for a single unit. Most cost functions then take the price of the inputs and adjust for different factors of production, typically, technology, economies of scale, and elasticities of inputs.

Traditional cost functions include [[Cobb–Douglas]] and the [[constant elasticity of substitution]] models. These are still used because for a wide variety of activities, effects such as varying ability to substitute materials does not change. For example, for people running a bake sale, the ability to substitute one kind of chocolate chip for another will not vary over the number of cookies they can bake. However, as economies of scale and changes in substitution become important models that handle these effects become more useful, such as the [[transcendental log cost function]].

The traditional forms are economically [[Homothetic transformation|homothetic]]. This means they can be expressed as a function, and that function can be broken into an outer part and an inner part. The inner part will appear once as a term in the outer part, and the inner part will be [[monotonically increasing]], or to say it another way, it never goes down. However, empirically in the areas of trade and production, homoethetic and monolithic functional models do not accurately predict results. One example is in the gravity equation for trade, or how much will two countries trade with each other based on GDP and distance. This led researchers to explore non-homothetic models of production, to fit with a [[cross-sectional data|cross section]] analysis of producer behavior, for example, when producers would begin to minimize costs by switching inputs, or investing in increased production.

==References==
{{reflist}}

[[Category:Functions and mappings]]
[[Category:Production economics]]


{{economy-stub}}</text>
      <sha1>5z7gj3f44czdfj4zp9g1fy98ikp98gs</sha1>
    </revision>
  </page>
  <page>
    <title>Handshaking lemma</title>
    <ns>0</ns>
    <id>9607933</id>
    <revision>
      <id>832829373</id>
      <parentid>832812402</parentid>
      <timestamp>2018-03-28T06:39:48Z</timestamp>
      <contributor>
        <username>Materialscientist</username>
        <id>7852030</id>
      </contributor>
      <comment>Reverted 1 [[WP:AGF|good faith]] edit by [[Special:Contributions/207.151.35.4|207.151.35.4]] using [[WP:STiki|STiki]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8635">[[File:6n-graf.svg|thumb|250px|In this graph, an even number of vertices (the four vertices numbered 2, 4, 5, and 6) have odd degrees. The sum of the degrees of the vertices is 2&amp;nbsp;+&amp;nbsp;3&amp;nbsp;+&amp;nbsp;2&amp;nbsp;+&amp;nbsp;3&amp;nbsp;+&amp;nbsp;3&amp;nbsp;+&amp;nbsp;1&amp;nbsp;=&amp;nbsp;14, twice the number of edges.]]
In [[graph theory]], a branch of mathematics, the '''handshaking lemma''' is the statement that every finite [[undirected graph]] has an even number of vertices with odd [[degree (graph theory)|degree]] (the number of edges touching the vertex). In more colloquial terms, in a party of people some of whom shake hands, an even number of people must have shaken an odd number of other people's hands.

The handshaking lemma is a consequence of the '''degree sum formula''' (also sometimes called the '''handshaking lemma'''),
:&lt;math&gt;\sum_{v\in V} \deg v = 2|E|&lt;/math&gt;
for a graph with [[vertex (graph theory)|vertex set]] ''V'' and [[edge (graph theory)|edge set]] ''E''. Both results were proven by {{harvs|first=Leonhard|last=Euler|authorlink=Leonhard Euler|year=1736|txt}} in his famous paper on the [[Seven Bridges of Königsberg]] that began the study of graph theory.

The vertices of odd degree in a graph are sometimes called '''odd nodes''' or '''odd vertices'''; in this terminology, the handshaking lemma can be restated as the statement that every graph has an even number of odd nodes.

==Proof==
Euler's proof of the degree sum formula uses the technique of [[Double counting (proof technique)|double counting]]: he counts the number of incident pairs (''v'',''e'') where ''e'' is an edge and vertex ''v'' is one of its endpoints, in two different ways. Vertex ''v'' belongs to deg(''v'') pairs, where deg(''v'') (the [[degree (graph theory)|degree]] of ''v'') is the number of edges incident to it. Therefore, the number of incident pairs is the sum of the degrees. However, each edge in the graph belongs to exactly two incident pairs, one for each of its endpoints; therefore, the number of incident pairs is 2|''E''|. Since these two formulas count the same set of objects, they must have equal values.

In a sum of integers, the [[Parity (mathematics)|parity]] of the sum is not affected by the even terms in the sum; the overall sum is even when there is an even number of odd terms, and odd when there is an odd number of odd terms. Since one side of the degree sum formula is the even number 2|''E''|, the sum on the other side must have an even number of odd terms; that is, there must be an even number of odd-degree vertices.

Alternatively, it is possible to use [[mathematical induction]] to prove that the number of odd-degree vertices is even, by removing one edge at a time from a given graph and using a [[Proof by cases|case analysis]] on the degrees of its endpoints to determine the effect of this removal on the parity of the number of odd-degree vertices.

==Regular graphs==
The degree sum formula implies that every ''r''-[[regular graph]] with ''n'' vertices has ''nr''/2 edges.&lt;ref&gt;{{citation |title = Graphs and Applications: an Introductory Approach | series = Undergraduate Mathematics Series, The Open University | first1 = Joan M. | last1 = Aldous | first2 = Robin J. | last2 = Wilson | publisher = Springer-Verlag | year = 2000 | isbn = 978-1-85233-259-4 |page=44 |chapter=Theorem 2.2}}&lt;/ref&gt; In particular, if ''r'' is odd then the number of edges must be divisible by ''r''.

==Infinite graphs==
[[File:Infinite graph one direction.svg|thumb|150px|An infinite graph that does not obey the handshaking lemma]]
The handshaking lemma does not apply to infinite graphs, even when they have only a finite number of odd-degree vertices. For instance, an infinite [[path graph]] with one endpoint has only a single odd-degree vertex rather than having an even number of such vertices.

==Exchange graphs==
Several combinatorial structures listed by {{harvtxt|Cameron|Edmonds|1999}} may be shown to be even in number by relating them to the odd vertices in an appropriate "exchange graph".

For instance, as [[Cedric Smith (statistician)|C. A. B. Smith]] proved, in any [[cubic graph]] ''G'' there must be an even number of [[Hamiltonian cycle]]s through any fixed edge ''uv''; {{harvtxt|Thomason|1978}} used a proof based on the handshaking lemma to extend this result to graphs ''G'' in which all vertices have odd degree. Thomason defines an exchange graph ''H'', the vertices of which are in one-to-one correspondence with the Hamiltonian paths beginning at ''u'' and continuing through ''v''. Two such paths ''p''&lt;sub&gt;1&lt;/sub&gt; and ''p''&lt;sub&gt;2&lt;/sub&gt; are connected by an edge in ''H'' if one may obtain ''p''&lt;sub&gt;2&lt;/sub&gt; by adding a new edge to the end of ''p''&lt;sub&gt;1&lt;/sub&gt; and removing another edge from the middle of ''p''&lt;sub&gt;1&lt;/sub&gt;; this is a [[symmetric relation]], so ''H'' is an undirected graph. If path ''p'' ends at vertex ''w'', then the vertex corresponding to ''p'' in ''H'' has degree equal to the number of ways that ''p'' may be extended by an edge that does not connect back to ''u''; that is, the degree of this vertex in ''H'' is either deg(''w'')&amp;nbsp;&amp;minus;&amp;nbsp;1 (an even number) if ''p'' does not form part of a Hamiltonian cycle through ''uv'', or deg(''w'')&amp;nbsp;&amp;minus;&amp;nbsp;2 (an odd number) if ''p'' is part of a Hamiltonian cycle through ''uv''. Since ''H'' has an even number of odd vertices, ''G'' must have an even number of Hamiltonian cycles through ''uv''.

==Computational complexity==
In connection with the exchange graph method for proving the existence of combinatorial structures, it is of interest to ask how efficiently these structures may be found. For instance, suppose one is given as input a Hamiltonian cycle in a cubic graph; it follows from Smith's theorem that there exists a second cycle. How quickly can this second cycle be found?
{{harvtxt|Papadimitriou|1994}} investigated the [[Computational complexity theory|computational complexity]] of questions such as this, or more generally of finding a second odd-degree vertex when one is given a single odd vertex in a large [[implicitly-defined graph]]. He defined the [[complexity class]] [[PPA (complexity)|PPA]] to encapsulate problems such as this one; a closely related class defined on directed graphs, [[PPAD (complexity)|PPAD]], has attracted significant attention in [[algorithmic game theory]] because computing a [[Nash equilibrium]] is computationally equivalent to the hardest problems in this class.&lt;ref&gt;{{citation|first1=Xi|last1=Chen|first2=Xiaotie|last2=Deng|contribution=Settling the complexity of two-player Nash equilibrium|title=[[Symposium on Foundations of Computer Science|Proc. 47th Symp. Foundations of Computer Science]]|year=2006|pages=261–271|doi=10.1109/FOCS.2006.69|id={{ECCC|2005|05|140}}}}&lt;/ref&gt;

==Other applications==
The handshaking lemma is also used in proofs of [[Sperner's lemma]] and of the piecewise linear case of the [[mountain climbing problem]].

==Notes==
{{reflist}}

==References==
*{{citation | last1 = Cameron| first1 = Kathie | last2 = Edmonds| first2 = Jack| author2-link = Jack Edmonds
| issue = 3
| journal = [[Annales de l'Institut Fourier]]
| pages = 815–827
| title = Some graphic uses of an even number of odd nodes
| url = http://www.numdam.org/item?id=AIF_1999__49_3_815_0
| volume = 49
| year = 1999 | mr = 1703426 | doi=10.5802/aif.1694}}.
*{{citation | last = Euler| first = L. | journal = Commentarii Academiae Scientiarum Imperialis Petropolitanae | pages = 128–140 | title = Solutio problematis ad geometriam situs pertinentis | url = http://math.dartmouth.edu/~euler/docs/originals/E053.pdf | volume = 8 | year = 1736}}. Reprinted and translated in {{citation | last1 = Biggs| first1 = N. L. | last2 = Lloyd| first2 = E. K. | last3 = Wilson| first3 = R. J. | publisher = Oxford University Press | title = Graph Theory 1736–1936 | year = 1976}}.
*{{citation | last = Papadimitriou| first = Christos H.| author-link = Christos Papadimitriou | doi = 10.1016/S0022-0000(05)80063-7
| issue = 3
| journal = Journal of Computer and System Sciences
| pages = 498–532
| title = On the complexity of the parity argument and other inefficient proofs of existence
| volume = 48
| year = 1994 | mr = 1279412}}.
*{{citation
 | last = Thomason | first = A. G.
 | contribution = Hamiltonian cycles and uniquely edge colourable graphs
 | doi = 10.1016/S0167-5060(08)70511-9
 | mr = 499124
 | pages = 259–268
 | series = Annals of Discrete Mathematics
 | title = Advances in Graph Theory (Cambridge Combinatorial Conf., Trinity College, Cambridge, 1977)
 | volume = 3
 | year = 1978}}.

[[Category:Graph theory]]
[[Category:Lemmas]]</text>
      <sha1>dib1wormo34ppb435jxu3gsjwow0fug</sha1>
    </revision>
  </page>
  <page>
    <title>Indicator function</title>
    <ns>0</ns>
    <id>240790</id>
    <revision>
      <id>864618613</id>
      <parentid>864581655</parentid>
      <timestamp>2018-10-18T11:48:41Z</timestamp>
      <contributor>
        <ip>138.4.209.223</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="15184">{{More footnotes|date=December 2009}}

[[Image:Indicator function illustration.png|right|thumb|A three-dimensional plot of an indicator function, shown over a square two-dimensional domain (set X): the 'raised' portion overlays those two-dimensional points which are members of the 'indicated' subset (A).]]
In [[mathematics]], an '''indicator function''' or a '''characteristic function''' is a [[Function (mathematics)|function]] defined on a [[Set (mathematics)|set]] ''X'' that indicates membership of an [[Element (mathematics)|element]] in a [[subset]] ''A'' of ''X'', having the value 1 for all elements of ''A'' and the value 0 for all elements of ''X'' not in ''A''. It is usually denoted by a symbol 1 or ''I'', sometimes in boldface or  [[blackboard bold]]face, with a subscript specifying the subset.

In other contexts, such as [[computer science]], this would more often be described as a '''boolean [[Predicate (mathematical logic)|predicate]] function''' (to test set inclusion).

==Definition==
The indicator function of a subset ''A'' of a set ''X'' is a function

:&lt;math&gt;\mathbf{1}_A \colon X \to \{ 0,1 \} &lt;/math&gt;

defined as

:&lt;math&gt;\mathbf{1}_A(x) :=
\begin{cases}
1 &amp;\text{if } x \in A, \\
0 &amp;\text{if } x \notin A.
\end{cases}
&lt;/math&gt;

The [[Iverson bracket]] allows the equivalent notation, &lt;math&gt;[x\in A]&lt;/math&gt;, to be used instead of &lt;math&gt;\mathbf{1}_A(x)&lt;/math&gt;.

The function &lt;math&gt;\mathbf{1}_A&lt;/math&gt; is sometimes denoted &lt;math&gt;I_A&lt;/math&gt;, &lt;math&gt;\chi_A&lt;/math&gt;, ''K&lt;sub&gt;A&lt;/sub&gt;'' or even just &lt;math&gt;A&lt;/math&gt;. (The [[Greek alphabet|Greek letter]] &lt;math&gt;\chi&lt;/math&gt; appears because it is the initial letter of the Greek word χαρακτήρ, which is the ultimate origin of the word ''characteristic''.)

The set of all indicator functions on &lt;math&gt;X&lt;/math&gt; can be identified with &lt;math&gt;\mathcal{P}(X)&lt;/math&gt;, the [[power set]] of &lt;math&gt;X&lt;/math&gt;.  Consequently, both sets are sometimes denoted by &lt;math&gt;2^X&lt;/math&gt;. This is a special case (&lt;math&gt;Y =\{0,1\}=2&lt;/math&gt;) of the notation &lt;math&gt;Y^X&lt;/math&gt; for the set of all functions &lt;math&gt;f:X\to Y &lt;/math&gt;.

==Remark on notation and terminology==
* The notation &lt;math&gt;1_A&lt;/math&gt; is also used to denote the [[identity function]] of ''A''.{{clarify|date=July 2012}}
* The notation &lt;math&gt;\chi_A&lt;/math&gt; is also used to denote the [[Characteristic function (convex analysis)|characteristic function]] in [[convex analysis]].{{clarify|date=July 2012}}

A related concept in [[statistics]] is that of a [[dummy variable (statistics)|dummy variable]]. (This must not be confused with "dummy variables" as that term is usually used in mathematics, also called a [[free variables and bound variables|bound variable]].)

The term "[[characteristic function (probability theory)|characteristic function]]" has an unrelated meaning in [[probability theory|classic probability theory]]. For this reason, [[List of probabilists|traditional probabilists]] use the term '''indicator function''' for the function defined here almost exclusively, while mathematicians in other fields are more likely to use the term ''characteristic function'' to describe the function that indicates membership in a set.

In [[fuzzy logic]] and [[Many-valued logic|modern many-valued logic]], predicates are the [[characteristic function (probability theory)|characteristic functions]] of a [[probability distribution]]. That is, the strict true/false valuation of the predicate is replaced by a quantity interpreted as the degree of truth.

==Basic properties==
The ''indicator'' or ''characteristic'' [[function (mathematics)|function]] of a subset ''A'' of some set ''X'', [[Map (mathematics)|maps]] elements of ''X'' to the [[Range (mathematics)|range]] {0,1}.

This mapping is [[surjective]] only when ''A'' is a non-empty [[proper subset]] of ''X''. If ''A'' ≡ ''X'', then
'''1'''&lt;sub&gt;''A''&lt;/sub&gt; = 1. By a similar argument, if ''A'' ≡ Ø then '''1'''&lt;sub&gt;''A''&lt;/sub&gt; = 0.

In the following, the dot represents multiplication, 1·1 = 1, 1·0 = 0 etc. "+" and "−" represent addition and subtraction. "&lt;math&gt;\cap &lt;/math&gt;" and "&lt;math&gt;\cup &lt;/math&gt;" is intersection and union, respectively.

If &lt;math&gt;A&lt;/math&gt; and &lt;math&gt;B&lt;/math&gt; are two subsets of &lt;math&gt;X&lt;/math&gt;, then
:&lt;math&gt;\mathbf{1}_{A\cap B} = \min\{\mathbf{1}_A,\mathbf{1}_B\} = \mathbf{1}_A \cdot\mathbf{1}_B,&lt;/math&gt;
:&lt;math&gt;\mathbf{1}_{A\cup B} = \max\{{\mathbf{1}_A,\mathbf{1}_B}\} = \mathbf{1}_A + \mathbf{1}_B - \mathbf{1}_A \cdot\mathbf{1}_B,&lt;/math&gt;
and the indicator function of the [[Complement (set theory)|complement]] of &lt;math&gt;A&lt;/math&gt; i.e. &lt;math&gt;A^C&lt;/math&gt; is:
:&lt;math&gt;\mathbf{1}_{A^\complement} = 1-\mathbf{1}_A&lt;/math&gt;.

More generally, suppose &lt;math&gt;A_1, \dotsc, A_n&lt;/math&gt; is a collection of subsets of ''X''.  For any
''x'' ∈ ''X'':

:&lt;math&gt; \prod_{k \in I} ( 1 - \mathbf{1}_{A_k}(x))&lt;/math&gt;

is clearly a product of 0s and 1s.  This product has the value 1 at
precisely those ''x'' ∈ ''X'' that belong to none of the sets ''A&lt;sub&gt;k&lt;/sub&gt;'' and
is 0 otherwise. That is

:&lt;math&gt; \prod_{k \in I} ( 1 - \mathbf{1}_{A_k}) = \mathbf{1}_{X - \bigcup_{k} A_k} = 1 - \mathbf{1}_{\bigcup_{k} A_k}.&lt;/math&gt;

Expanding the product on the left hand side,

: &lt;math&gt; \mathbf{1}_{\bigcup_{k} A_k}= 1 - \sum_{F \subseteq \{1, 2, \dotsc, n\}} (-1)^{|F|} \mathbf{1}_{\bigcap_F A_k} = \sum_{\emptyset \neq F \subseteq \{1, 2, \dotsc, n\}} (-1)^{|F|+1} \mathbf{1}_{\bigcap_F A_k} &lt;/math&gt;

where |''F''| is the cardinality of ''F''. This is one form of the principle of [[inclusion-exclusion]].

As suggested by the previous example, the indicator function is a useful notational device in [[combinatorics]].  The notation is used in other places as well, for instance in [[probability theory]]: if &lt;math&gt;X&lt;/math&gt; is a [[probability space]] with probability measure &lt;math&gt;\mathbb{P}&lt;/math&gt; and &lt;math&gt;A&lt;/math&gt; is a [[Measure (mathematics)|measurable set]], then &lt;math&gt;\mathbf{1}_A&lt;/math&gt; becomes a [[random variable]] whose [[expected value]] is equal to the probability of &lt;math&gt;A&lt;/math&gt;:

:&lt;math&gt;\operatorname{E}(\mathbf{1}_A)= \int_{X} \mathbf{1}_A(x)\,d\mathbb{P} = \int_{A} d\mathbb{P} = \operatorname{P}(A)&lt;/math&gt;.

This identity is used in a simple proof of [[Markov's inequality]].

In many cases, such as [[order theory]], the inverse of the indicator function may be defined. This is commonly called the [[generalized Möbius function]], as a generalization of the inverse of the indicator function in elementary [[number theory]], the [[Möbius function]]. (See paragraph below about the use of the inverse in classical recursion theory.)

==Mean, variance and covariance==
Given a [[probability space]] &lt;math&gt;\textstyle (\Omega, \mathcal F, \mathbb P)&lt;/math&gt; with &lt;math&gt;A \in \mathcal F&lt;/math&gt;, the indicator random variable &lt;math&gt;\mathbf{1}_A \colon \Omega \rightarrow \Bbb{R}&lt;/math&gt; is defined by &lt;math&gt;\mathbf{1}_A (\omega) = 1 &lt;/math&gt; if &lt;math&gt; \omega \in A,&lt;/math&gt; otherwise &lt;math&gt;\mathbf{1}_A (\omega) = 0.&lt;/math&gt;

;[[Mean]]: &lt;math&gt;\operatorname{E}(\mathbf{1}_A (\omega)) = \operatorname{P}(A) &lt;/math&gt;

;[[Variance]]: &lt;math&gt;\operatorname{Var}(\mathbf{1}_A (\omega)) = \operatorname{P}(A)(1 - \operatorname{P}(A)) &lt;/math&gt;

;[[Covariance]]: &lt;math&gt; \operatorname{Cov}(\mathbf{1}_A (\omega), \mathbf{1}_B (\omega)) = \operatorname{P}(A \cap B) - \operatorname{P}(A)\operatorname{P}(B) &lt;/math&gt;

==Characteristic function in recursion theory, Gödel's and Kleene's ''representing function''==
[[Kurt Gödel]] described the ''representing function'' in his 1934 paper "On Undecidable Propositions of Formal Mathematical Systems". (The paper appears on pp.&amp;nbsp;41–74 in [[Martin Davis]] ed. ''The Undecidable''):
:"There shall correspond to each class or relation R a representing function φ(x&lt;sub&gt;1&lt;/sub&gt;, . . ., x&lt;sub&gt;n&lt;/sub&gt;) = 0 if R(x&lt;sub&gt;1&lt;/sub&gt;, . . ., x&lt;sub&gt;n&lt;/sub&gt;) and φ(x&lt;sub&gt;1&lt;/sub&gt;, . . ., x&lt;sub&gt;n&lt;/sub&gt;) = 1 if ~R(x&lt;sub&gt;1&lt;/sub&gt;, . . ., x&lt;sub&gt;n&lt;/sub&gt;)." (p. 42; the "~" indicates logical inversion i.e. "NOT")

[[Stephen Kleene]] (1952) (p.&amp;nbsp;227) offers up the same definition in the context of the [[primitive recursive function]]s as a function φ of a predicate P takes on values 0 if the predicate is true and 1 if the predicate is false.

For example, because the product of characteristic functions φ&lt;sub&gt;1&lt;/sub&gt;*φ&lt;sub&gt;2&lt;/sub&gt;* . . . *φ&lt;sub&gt;n&lt;/sub&gt; = 0 whenever any one of the functions equals 0, it plays the role of logical OR: IF φ&lt;sub&gt;1&lt;/sub&gt; = 0 OR φ&lt;sub&gt;2&lt;/sub&gt; = 0 OR . . . OR φ&lt;sub&gt;n&lt;/sub&gt; = 0 THEN their product is 0. What appears to the modern reader as the representing function's logical inversion, i.e. the representing function is 0 when the function R is "true" or satisfied", plays a useful role in Kleene's definition of the logical  functions OR, AND, and IMPLY (p.&amp;nbsp;228), the bounded- (p.&amp;nbsp;228) and unbounded- (p.&amp;nbsp;279ff) [[mu operator]]s (Kleene (1952)) and the CASE function (p.&amp;nbsp;229).

==Characteristic function in fuzzy set theory==
In classical mathematics, characteristic functions of sets only take values 1 (members) or 0 (non-members). In [[fuzzy set theory]], characteristic functions are generalized to take value in the real unit interval [0,&amp;nbsp;1], or more generally, in some [[universal algebra|algebra]] or [[structure (mathematical logic)|structure]] (usually required to be at least a [[partially ordered set|poset]] or [[lattice (order)|lattice]]). Such generalized characteristic functions are more usually called [[membership function (mathematics)|membership function]]s, and the corresponding "sets" are called ''fuzzy'' sets. Fuzzy sets model the gradual change in the membership [[degree of truth|degree]] seen in many real-world [[predicate (mathematics)|predicate]]s like "tall", "warm", etc.

==Derivatives of the indicator function==
A particular indicator function is the [[Heaviside step function]]. The Heaviside step function  ''H''(''x'') is the indicator function of the one-dimensional positive half-line, i.e. the domain [0, ∞). The [[distributional derivative]] of the Heaviside step function is equal to the [[Dirac delta function]], i.e.

:&lt;math&gt;
\delta(x)=\tfrac{d H(x)}{dx},
&lt;/math&gt;

with the following property:

:&lt;math&gt;
\int_{-\infty}^\infty f(x) \, \delta(x) dx =  f(0).
&lt;/math&gt;

The derivative of the Heaviside step function can be seen as the 'inward normal derivative' at the 'boundary' of the domain given by the positive half-line. In higher dimensions, the derivative naturally generalises to the inward normal derivative, while the Heaviside step function naturally generalises to the indicator function of some domain ''D''. The surface of ''D'' will be denoted by ''S''. Proceeding, it can be derived that the [[Laplacian of the indicator#Dirac surface delta function|inward normal derivative of the indicator]] gives rise to a 'surface delta function', which can be indicated by δ&lt;sub&gt;''S''&lt;/sub&gt;('''x'''):

:&lt;math&gt;\delta_S(\mathbf{x})=-\mathbf{n}_x\cdot\nabla_x\mathbf{1}_{\mathbf{x}\in D}&lt;/math&gt;

where ''n'' is the outward [[Normal (geometry)|normal]] of the surface ''S''. This 'surface delta function' has the following property:&lt;ref&gt;{{citation|last=Lange|first=Rutger-Jan|year=2012|publisher=Springer|title=Potential theory, path integrals and the Laplacian of the indicator|journal=Journal of High Energy Physics|volume=2012|pages=29–30|url=https://link.springer.com/article/10.1007%2FJHEP11(2012)032|issue=11|bibcode=2012JHEP...11..032L|doi=10.1007/JHEP11(2012)032|arxiv = 1302.0864 }}&lt;/ref&gt;

:&lt;math&gt;
-\int_{\mathbf{R}^n}f(\mathbf{x})\,\mathbf{n}_x\cdot\nabla_x\mathbf{1}_{\mathbf{x}\in D}\;d^{n}\mathbf{x}=\oint_{S}\,f(\mathbf{\beta})\;d^{n-1}\mathbf{\beta}.
&lt;/math&gt;

By setting the function ''f'' equal to one, it follows that the [[Laplacian of the indicator#Dirac surface delta function|inward normal derivative of the indicator]] integrates to the numerical value of the [[surface area]] ''S''.

==See also==
{{Div col|colwidth=40em}}
* [[Dirac measure]]
* [[Laplacian of the indicator]]
* [[Dirac delta]]
* [[Extension (predicate logic)]]
* [[Free variables and bound variables]]
* [[Heaviside step function]]
* [[Iverson bracket]]
* [[Kronecker delta]], a function that can be viewed as an indicator for the [[Equality (mathematics)|identity relation]]
* [[Macaulay brackets]]
* [[Multiset]]
* [[Membership function (mathematics)|Membership function]]
* [[Simple function]]
* [[Dummy variable (statistics)]]
* [[Statistical classification]]
* [[Zero-one loss function]]
{{div col end}}

==Notes==
{{Reflist}}

==References==
* {{cite book | last = Folland | first = G.B. | title = Real Analysis: Modern Techniques and Their Applications | edition = Second | publisher = John Wiley &amp; Sons, Inc. | year = 1999 | isbn=978-0-471-31716-6}}
* {{cite book
 | first1 = Thomas H. | last1 = Cormen | authorlink1 = Thomas H. Cormen
 | first2 = Charles E. | last2 = Leiserson | authorlink2 = Charles E. Leiserson
 | first3 = Ronald L. | last3 = Rivest | authorlink3 = Ronald L. Rivest
 | first4 = Clifford | last4 = Stein | authorlink4 = Clifford Stein
 | title = [[Introduction to Algorithms]]
 | edition = Second
 | publisher = MIT Press and McGraw-Hill
 | year = 2001
 | isbn = 0-262-03293-7
 | chapter = Section 5.2: Indicator random variables
 | pages = 94–99
 }}
* {{cite book
 | editor1-first = Martin | editor1-last = Davis | editor1-link = Martin Davis
 | year = 1965
 | title = The Undecidable
 | publisher = Raven Press Books, Ltd. | location = New York
 }}
* {{cite book
 | first = Stephen | last = Kleene | authorlink = Stephen Kleene
 | origyear = 1952 | title = Introduction to Metamathematics
 | publisher = Wolters-Noordhoff Publishing and North Holland Publishing Company | location = Netherlands
 | type = Sixth Reprint with corrections
 | year = 1971
 }}
* {{cite book
 | first1 = George | last1 = Boolos | authorlink1 = George Boolos
 | first2 = John P. | last2 = Burgess | authorlink2 = John P. Burgess
 | first3 = Richard C. | last3 = Jeffrey | authorlink3 = Richard C. Jeffrey
 | year = 2002
 | title = Computability and Logic
 | publisher =  Cambridge University Press | location = Cambridge UK
 | isbn = 0-521-00758-5
 }}
* {{cite journal
 |first       = Lotfi A.
 |last        = Zadeh
 |authorlink  = Lotfi A. Zadeh
 |date        = June 1965
 |title       = Fuzzy sets
 |journal     = [[Information and Control]]
 |volume      = 8
 |issue       = 3
 |pages       = 338–353
 |url         = http://www-bisc.cs.berkeley.edu/zadeh/papers/Fuzzy%20Sets-1965.pdf
 |format      = PDF
 |doi         = 10.1016/S0019-9958(65)90241-X
 |deadurl     = yes
 |archiveurl  = https://web.archive.org/web/20070622151801/http://www-bisc.cs.berkeley.edu/zadeh/papers/Fuzzy%20Sets-1965.pdf
 |archivedate = 2007-06-22
 |df          = 
}}
* {{cite journal
 | first = Joseph | last = Goguen | authorlink = Joseph Goguen
 | year = 1967
 | title = ''L''-fuzzy sets
 | journal = Journal of Mathematical Analysis and Applications
 | volume = 18 | issue = 1 | pages = 145–174
 | doi = 10.1016/0022-247X(67)90189-8
 }}

[[Category:Measure theory]]
[[Category:Integral calculus]]
[[Category:Real analysis]]
[[Category:Mathematical logic]]
[[Category:Basic concepts in set theory]]
[[Category:Probability theory]]
[[Category:Types of functions]]</text>
      <sha1>altegv5a8eumhhdvfsfjraluh8g4yin</sha1>
    </revision>
  </page>
  <page>
    <title>Jiří Matoušek (mathematician)</title>
    <ns>0</ns>
    <id>20938051</id>
    <revision>
      <id>842833489</id>
      <parentid>842833157</parentid>
      <timestamp>2018-05-25T00:10:45Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>/* Books */ also wik</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="8144">{{Other people|Jiří Matoušek}}

{{Infobox scientist
| name       = Jiří Matoušek
| image       = Jiri Matousek.jpg
| caption     = Jiří Matoušek at the [[Mathematical Research Institute of Oberwolfach]], 2005
| birth_place  = [[Prague]]
| birth_date  = {{Birth date|df=yes|1963|3|10}}
| death_date  = {{Death date and age|df=yes|2015|3|9|1963|3|10}}
| fields    = [[Mathematics]]
| alma_mater = [[Charles University in Prague]]
| workplaces = {{Plainlist|
* [[Charles University in Prague]]
* [[ETH Zurich]]
}}
}}

'''Jiří (Jirka) Matoušek''' (10 March 1963 – 9 March 2015) was a [[Czech people|Czech]] [[mathematician]] working in [[computational geometry]] and [[algebraic topology]]. He was a professor at [[Charles University in Prague]] and the author of several textbooks and research monographs.

Matoušek was born in [[Prague]]. In 1986, he received his [[Master's degree]] at Charles University under [[Miroslav Katětov]].&lt;ref&gt;{{cite thesis |last=Matoušek |first=Jiří |date=1986 |title=Vlastnosti R-stromů |degree=M.Sc. |language=Czech|publisher=Charles University in Prague}}&lt;/ref&gt;  From 1986 until his death he was employed at the Department of Applied Mathematics of [[Charles University in Prague]], holding a professor position since 2000. He was also a visiting and later full professor at [[ETH Zurich]].&lt;ref name="rememberingJM" /&gt;

In 1996, he won the [[European Mathematical Society]] prize&lt;ref&gt;[http://www.euro-math-soc.eu/ems_prize_winners.html EMS Prize Winners].&lt;/ref&gt; and in 2000 he won the Scientist award of the [[Learned Society of the Czech Republic]].&lt;ref&gt;[http://www.learned.cz/main.php?id=02.03.01.2000 2000 Awards], Learned Society of the Czech Republic.&lt;/ref&gt;
He became a fellow of the Learned Society of the Czech Republic in 2005.&lt;ref&gt;[http://www.learned.cz/en/fellows-of-the-learned-society-of-the-czech-republic/fellows-of-the-learned-society/matousek-jiri.html Member profile], [[Learned Society of the Czech Republic]], retrieved 2015-03-10.&lt;/ref&gt;

Matoušek's paper on computational aspects of [[algebraic topology]] won the Best Paper award at the 2012 ACM Symposium on Discrete Algorithms.&lt;ref&gt;{{cite web |url=http://www.siam.org/meetings/da12/paper.php |title=SIAM: Best Paper Awards |access-date=18 March 2015}}&lt;/ref&gt;

Aside from his own academic writing, he has translated the popularization book ''Mathematics: A Very Short Introduction'' by [[Timothy Gowers]] into Czech.&lt;ref&gt;{{cite book|last=Gowers|first=Timothy| title=Mathematics: A Very Short Introduction|trans-title=Matematika: Průvodce pro každého|others=Translated by Jiří Matoušek| language=Czech |publisher=Dokořán|date=2006|isbn=80-7363-083-4 }}&lt;/ref&gt;

He was a supporter and signatory of the [[Cost of Knowledge]] protest.&lt;ref&gt;{{cite web|url=http://thecostofknowledge.com/ |quote= I've been practicing this for many years, with some exceptions (journals with few good alternatives, requests for reports from people I didn't want to disappoint). I hope this initiative will not die out.|title=The Cost Of Knowledge|access-date=18 March 2015}}&lt;/ref&gt; He died in 2015, aged 51.&lt;ref name="rememberingJM"&gt;{{cite web |url= http://kam.mff.cuni.cz/Matousek-obituary.html |title= Prof. Jiří Matoušek |access-date= 2015-03-18 |author1=Jan Kratochvíl |author2=Martin Loebl |author3=Jarik Nešetřil |author4=Pavel Valtr |quote= In the name of Jiří Matoušek's colleagues and friends from Charles University }}, obituary&lt;/ref&gt;&lt;ref&gt;{{cite web | url=http://www.mff.cuni.cz/to.en/verejnost/konalo-se/2015-03-matousek/ | title=Obituary of Jiří Matoušek | website = Faculty of Mathematics and Physics, Charles University | date = 12 March 2015 | access-date = 18 March 2015 }}&lt;/ref&gt;

==Books==
*''Invitation to Discrete Mathematics'' (with [[Jaroslav Nešetřil]]). [[Oxford University Press]], 1998. {{isbn|978-0-19-850207-4}}. Translated into French by Delphine Hachez as ''Introduction Aux Mathématiques Discrètes'', [[Springer-Verlag]], 2004, {{isbn|978-2-287-20010-6}}.
*''Geometric Discrepancy: An Illustrated Guide''. Springer-Verlag, [[Algorithms and Combinatorics]] 18, 1999, {{isbn|978-3-540-65528-2}}.&lt;ref&gt;Review of ''Geometric Discrepancy'' by Allen D. Rogers, {{MR|1697825}}&lt;/ref&gt;
*''Lectures on Discrete Geometry''. Springer-Verlag, [[Graduate Texts in Mathematics]], 2002, {{isbn|978-0-387-95373-1}}.&lt;ref&gt;Review of ''Lectures on Discrete Geometry'' by E. Hertel, {{MR|1899299}}&lt;/ref&gt;&lt;ref&gt;{{citation|title= Matoušek, Jiří, ''Lectures on Discrete Geometry''|department=Book Reviews|first=Paul A.|last=Blaga|pages=119–120|url=http://www.cs.ubbcluj.ro/~studia-m/2003-1/rec.pdf|journal=Studia Univ. Babeș-Bolyai, Mathematica|volume=48|issue=1|date=March 2003}}.&lt;/ref&gt;&lt;ref&gt;{{citation|title=''Lectures on Discrete Geometry''|first=I.|last=Bárány|authorlink=Imre Bárány|journal=Bulletin of the London Mathematical Society|volume=35|issue=5|date=September 2003|pages=719–720|doi=10.1112/S0024609303269332|department=Book Reviews}}.&lt;/ref&gt;
*''Using the Borsuk-Ulam Theorem: Lectures on Topological Methods in Combinatorics and Geometry''. Springer-Verlag, 2003. {{isbn|978-3-540-00362-5}}.&lt;ref&gt;Review of ''Using the Borsuk-Ulam theorem'' by Zdzisław Dzedzej, {{MR|1988723}}&lt;/ref&gt;&lt;ref&gt;{{citation|title=''Using the Borsuk-Ulam Theorem: Lectures on Topological Methods in Combinatorics and Geometry''|journal=Combinatorics, Probability and Computing|volume=13|issue=2|date=March 2004|pages=281–282|department=Book Review|first=Imre|last=Bárány|authorlink=Imre Bárány|doi=10.1017/S096354830400608X}}.&lt;/ref&gt;
*''Topics in Discrete Mathematics: Dedicated to [[Jaroslav Nešetřil|Jarik Nešetřil]] on the Occasion of His 60th Birthday'' (with Martin Klazar, [[Jan Kratochvíl]], Martin Loebl, [[Robin Thomas (mathematician)|Robin Thomas]], and Pavel Valtr). Springer-Verlag, [[Algorithms and Combinatorics]] 26, 2006. {{isbn|978-3-540-33698-3}}.
*''Understanding and Using Linear Programming'' (with B. Gärtner). Springer-Verlag, Universitext, 2007, {{isbn|978-3-540-30697-9}}.&lt;ref&gt;{{citation|title=Reviewed Works: ''Understanding and Using Linear Programming'' by Jiří Matoušek, Bernd Gärtner; ''Introduction to Optimization'' by Pablo Pedregal|department=Review|journal=[[American Mathematical Monthly]]|volume=116|issue=5|pages=471–476|first=Allen|last=Holder|jstor= 40391139}}.&lt;/ref&gt;
*''Thirty-three miniatures — Mathematical and algorithmic applications of linear algebra''. [[American Mathematical Society|AMS]], 2010, {{isbn|978-0-8218-4977-4}}.&lt;ref&gt;Review of ''Thirty-three miniatures'' by Torsten Sander, {{MR|2656313}}&lt;/ref&gt;&lt;ref&gt;{{citation|title=Thirty-three miniatures : mathematical and algorithmic applications of linear algebra [book review]|url=http://www.euro-math-soc.eu/review/thirty-three-miniatures|publisher=European Mathematical Society|last=Díaz Sánchez|first=Raquel|year=2011}}.&lt;/ref&gt;&lt;ref&gt;{{citation|url=https://sashakolpakov.files.wordpress.com/2013/09/review33miniatures.pdf|title=Thirty-three Miniatures: Mathematical and Algorithmic Applications of Linear Algebra, by Jiri Matousek|first=Alexander|last=Kolpakov|journal=Elemente der Mathematik}}.&lt;/ref&gt;
*''Approximation Algorithms and Semidefinite Programming'' (with B. Gärtner). Springer Berlin Heidelberg, 2012, {{isbn|978-3-642-22014-2}}.&lt;ref&gt;Review of ''Approximation Algorithms and Semidefinite Programming'' by Jane Juan-Juan Ye, {{MR|3015090}}&lt;/ref&gt;
*''Mathematics++: Selected Topics Beyond the Basic Courses'' (with Ida Kantor and Robert Šámal). American Mathematical Society, 2015, {{isbn|978-1-4704-2261-5}}.

==See also==
*[[Ham sandwich theorem]]
*[[Discrepancy theory]]
*[[Kneser graph]]

==References==
{{reflist}}

==External links==
* [http://kam.mff.cuni.cz/~matousek/ Jiri Matousek home page]
* {{AcademicSearch|598156}}

{{Authority control}}

{{DEFAULTSORT:Matousek, Jiri}}
[[Category:1963 births]]
[[Category:2015 deaths]]
[[Category:Mathematicians from Prague]]
[[Category:Czech mathematicians]]
[[Category:Researchers in geometric algorithms]]
[[Category:Charles University in Prague faculty]]
[[Category:ETH Zurich faculty]]
[[Category:Combinatorialists]]
[[Category:Topologists]]</text>
      <sha1>4bh0skxmqd4bwslgky94j3opvat1fef</sha1>
    </revision>
  </page>
  <page>
    <title>Joseph R. Shoenfield</title>
    <ns>0</ns>
    <id>48884267</id>
    <revision>
      <id>839499704</id>
      <parentid>837581868</parentid>
      <timestamp>2018-05-03T20:57:32Z</timestamp>
      <contributor>
        <username>PeterStJohn</username>
        <id>1934778</id>
      </contributor>
      <comment>Adding Go to hobbies, as his contributions were significant in those early days of the sport in the U.S.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="3258">{{Use mdy dates|date=September 2016}}
{{Infobox scientist
|name=Joseph Robert Shoenfield
|field=[[Mathematical logic]]
|birth_place=Detroit, Michigan, US
|death_date={{death date and age|2000|11|15 |1927|1}}
|death_place=[[Durham, North Carolina|Durham]], North Carolina, US
|alma_mater=[[University of Michigan]]
|doctoral_advisor=[[Raymond Louis Wilder]]&lt;ref&gt;{{MathGenealogy|id=5126}}&lt;/ref&gt;
|thesis_year=1953
|thesis_title=Models of Formal Systems
|residence=United States
|workplaces=[[Duke University]]
|awards={{nowrap|Gödel Lecturer (1992)}}
|known_for={{nowrap|[[absoluteness|Shoenfield absoluteness theorem]]}}
}}

'''Joseph Robert Shoenfield''' (1927, Detroit – November 15, 2000, [[Durham, North Carolina]]) was an American mathematical logician.

==Education==
Shoenfield obtained his PhD in 1953 with [[Raymond Louis Wilder]] at the [[University of Michigan]] (Models of formal systems).

==Career==
From 1952, he lectured at [[Duke University]], where he remained until becoming Emeritus in 1992. From 1970 to 1973 he was President of the Mathematics Faculty. In 1956/57 he was at the [[Institute for Advanced Study]]. Shoenfield worked on [[recursion theory]], [[model theory]] and [[Set theory|axiomatic set theory]]. His textbook on mathematical logic has become a classic.&lt;ref&gt;{{harvnb|Jockusch|2001|page=393}}.&lt;/ref&gt;

==Honors==
From 1972 to 1976 he was president of the [[Association for Symbolic Logic]]. He delivered the Gödel Lecture at the 1992 meeting of the ASL.&lt;ref&gt;[http://www.aslonline.org/Goedel_lecturers.html Gödel Lectures, Association for Symbolic Logic]&lt;/ref&gt;

==Hobbies==
Already in his student days, he was a passionate and strong [[contract bridge]] player.
He was an early member [http://www.usgo.org/ratings-lookup-name?NamesLike=shoenfieldMember Number 694] of the [[American Go Association]] and the [http://www.trianglegoclub.org/tmm.htm Memorial Tournament] in North Carolina was founded in his memory. (The link includes a photograph of him.)

== Selected publications ==
* Mathematical Logic, Addison Wesley 1967, 2nd edition, Association for Symbolic Logic, 2001&lt;ref&gt;{{harvnb|Shoenfield|2001}}.&lt;/ref&gt;
* Degrees of unsolvability, North Holland Mathematical Studies 1971
* Recursion theory, Springer 1993&lt;ref&gt;{{harvnb|Shoenfield|2000}}.&lt;/ref&gt;

== Notes ==
&lt;references /&gt;

== References ==
* {{cite journal|ref=harv|last1=Jockusch|first1=Carl G.|title=In Memoriam: Joseph R. Shoenfield 1927–2000|journal=The Bulletin of Symbolic Logic|volume=7|number=3|year=2001|pages=393–396|url=http://www.math.ucla.edu/~asl/bsl/0703/0703-007.ps}}
* {{cite book|ref=harv| last1=Shoenfield
  | first1=Joseph R.
  | author1link=Joseph R. Shoenfield
  | title=Mathematical Logic
  | origyear=1967
  | publisher=[[A K Peters]]
  | edition=2nd
  | isbn=978-1-56881-135-2
  | year=2001
}}
* {{cite book |ref=harv| last1=Shoenfield|first1=Joseph R. |author1-link=Joseph R. Shoenfield| title=Recursion Theory | publisher=A K Peters Ltd | year=2000 | isbn=1-56881-149-7 }}

{{Authority control}}

{{DEFAULTSORT:Shoenfield, Joseph R}}
[[Category:20th-century mathematicians]]
[[Category:Duke University faculty]]
[[Category:Mathematical logicians]]
[[Category:American logicians]]
[[Category:1927 births]]
[[Category:2000 deaths]]</text>
      <sha1>1ejl5v10n9usj21scrfeipp08zk9i9x</sha1>
    </revision>
  </page>
  <page>
    <title>K-vertex-connected graph</title>
    <ns>0</ns>
    <id>7566175</id>
    <revision>
      <id>820650910</id>
      <parentid>813785479</parentid>
      <timestamp>2018-01-15T21:03:43Z</timestamp>
      <contributor>
        <username>Krishnavedala</username>
        <id>261228</id>
      </contributor>
      <minor/>
      <comment>/* Definitions */ vector image</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4330">{{DISPLAYTITLE:''k''-vertex-connected graph}}
In [[graph theory]], a [[Connectivity (graph theory)|connected graph]] ''G'' is said to be '''''k''-vertex-connected''' (or '''''k''-connected''') if it has more than ''k'' [[Vertex (graph theory)|vertices]] and remains [[Connectivity (graph theory)|connected]] whenever fewer than ''k'' vertices are removed.

The '''vertex-connectivity''', or just '''connectivity''', of a graph is the largest ''k'' for which the graph is ''k''-vertex-connected.

== Definitions ==
[[File:4-connected graph.svg|thumb|A graph with connectivity 4.]]
A graph (other than a [[complete graph]]) has connectivity ''k'' if ''k'' is the size of the smallest subset of vertices such that the graph becomes disconnected if you delete them.&lt;ref name="Schrijver"&gt;{{Citation|author=Schrijver|title=Combinatorial Optimization|publisher=Springer}}&lt;/ref&gt; Complete graphs are not included in this version of the definition since they cannot be disconnected by deleting vertices. The complete graph with ''n'' vertices has connectivity ''n''&amp;nbsp;&amp;minus;&amp;nbsp;1, as implied by the first definition.

An equivalent definition is that a graph with at least two vertices is ''k''-connected if, for every pair of its vertices, it is possible to find ''k'' vertex-independent [[Path (graph theory)|paths]] connecting these vertices; see [[Menger's theorem]] {{Harv|Diestel|2005| p=55}}. This definition produces the same answer, ''n''&amp;nbsp;&amp;minus;&amp;nbsp;1, for the connectivity of the complete graph ''K''&lt;sub&gt;''n''&lt;/sub&gt;.&lt;ref name="Schrijver"/&gt;

A 1-connected graph is called [[Connected graph|connected]]; a 2-connected graph is called [[Biconnected graph|biconnected]]. A 3-connected graph is called triconnected.

== Applications ==

=== Polyhedral combinatorics ===
The 1-[[Skeleton (topology)|skeleton]] of any ''k''-dimensional convex [[polytope]] forms a ''k''-vertex-connected graph ([[Balinski's theorem]], {{harvnb|Balinski|1961}}). As a partial converse, [[Steinitz's theorem]] states that any 3-vertex-connected [[planar graph]] forms the skeleton of a convex [[polyhedron]]. 

More generally, the ''3-sphere regular cellulation conjecture''  claims that every 2-connected graph is the 
one-dimensional skeleton of a regular [[CW-complex]]  on the three-dimensional sphere (http://twiki.di.uniroma1.it/pub/Users/SergioDeAgostino/DeAgostino2016.pdf).

== Computational complexity ==

The vertex-connectivity of an input graph ''G'' can be computed in polynomial time in the following way&lt;ref&gt;''The algorithm design manual'', p 506, and ''Computational discrete mathematics: combinatorics and graph theory with Mathematica'', p. 290-291&lt;/ref&gt; consider all possible pairs &lt;math&gt;(s, t)&lt;/math&gt; of nonadjacent nodes to disconnect, using [[Menger's theorem]] to justify that the minimal-size separator for &lt;math&gt;(s, t)&lt;/math&gt; is the number of pairwise vertex-independent paths between them, encode the input by doubling each vertex as an edge to reduce to a computation of the number of pairwise edge-independent paths, and compute the maximum number of such paths by computing the [[maximum flow]] in the graph between &lt;math&gt;s&lt;/math&gt; and &lt;math&gt;t&lt;/math&gt; with capacity 1 to each edge, noting that a flow of &lt;math&gt;k&lt;/math&gt; in this graph corresponds, by the [[integral flow theorem]], to &lt;math&gt;k&lt;/math&gt; pairwise edge-independent paths from &lt;math&gt;s&lt;/math&gt; to &lt;math&gt;t&lt;/math&gt;.

==See also==
* [[k-edge-connected graph|''k''-edge-connected graph]]
* [[Connectivity (graph theory)]]
* [[Menger's theorem]]
* [[Structural cohesion]]
* [[Tutte embedding]]
* [[Vertex separator]]

==Notes==
{{Reflist}}

==References==
*{{citation
 | last = Balinski | first = M. L. | authorlink = Michel Balinski
 | issue = 2
 | journal = [[Pacific Journal of Mathematics]]
 | pages = 431–434
 | title = On the graph structure of convex polyhedra in ''n''-space
 | url = http://www.projecteuclid.org/Dienst/UI/1.0/Summarize/euclid.pjm/1103037323
 | volume = 11
 | year = 1961
 | doi=10.2140/pjm.1961.11.431}}.
*{{citation
 | last = Diestel | first = Reinhard
 | edition = 3rd
 | isbn = 978-3-540-26183-4
 | location = Berlin, New York
 | publisher = Springer-Verlag
 | title = Graph Theory
 | url = http://www.math.uni-hamburg.de/home/diestel/books/graph.theory/
 | year = 2005}}.

[[Category:Graph connectivity]]
[[Category:Graph families]]</text>
      <sha1>5v68bqb1znvtwy5kkj10b05ursza102</sha1>
    </revision>
  </page>
  <page>
    <title>Kakeya set</title>
    <ns>0</ns>
    <id>588260</id>
    <revision>
      <id>859102353</id>
      <parentid>859101842</parentid>
      <timestamp>2018-09-11T18:43:07Z</timestamp>
      <contributor>
        <username>Bender235</username>
        <id>88026</id>
      </contributor>
      <comment>/* Kakeya sets in vector spaces over finite fields */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="24768">[[Image:Kakeya needle.gif|right|thumb|Needle shown rotating inside a [[deltoid curve|deltoid]]. At every stage of its rotation (except when an endpoint is at a cusp of the deltoid), the needle is in contact with the deltoid at three points: two endpoints (blue) and one tangent point (black).
The needle's midpoint (red) describes a circle with diameter equal to half the length of the needle.|208px]]

In [[mathematics]], a '''Kakeya set''', or '''Besicovitch set''', is a set of points in [[Euclidean space]] which contains a unit [[line segment]] in every direction. For instance, a [[Disk (mathematics)|disk]] of radius 1/2 in the [[Euclidean plane]], or a ball of radius 1/2 in three-dimensional space, forms a Kakeya set. Much of the research in this area has studied the problem of how small such sets can be. [[Abram Samoilovitch Besicovitch|Besicovitch]] showed that there are Besicovitch sets of [[measure zero]].

A '''Kakeya needle set''' (sometimes also known as a Kakeya set) is a (Besicovitch) set in the plane with a stronger property, that a unit line segment can be rotated continuously through 180 degrees within it, returning to its original position with reversed orientation. Again, the disk of radius 1/2 is an example of a Kakeya needle set.

==Kakeya needle problem==
The '''Kakeya needle problem''' asks whether there is a minimum area of a region ''D'' in the plane, in which a needle of unit length can be turned through 360°. This question was first posed, for [[Convex set|convex]] regions, by {{harvs|txt|authorlink=Sōichi Kakeya|first=Sōichi|last= Kakeya|year=1917}}. The minimum area for convex sets is achieved by an [[equilateral triangle]] of height&amp;nbsp;1 and area 1/{{radic|3}}, as [[Gyula Pál|Pál]] showed.&lt;ref&gt;{{cite journal| last = Pal | first = Julius  | title = Ueber ein elementares variationsproblem  | journal = Kongelige Danske Videnskabernes Selskab Math.-Fys. Medd.  | volume = 2 | pages = 1&amp;ndash;35 | year = 1920 }}&lt;/ref&gt;

Kakeya seems to have suggested that the Kakeya set ''D'' of minimum area, without the convexity restriction, would be a three-pointed [[deltoid curve|deltoid]] shape. However, this is false; there are smaller non-convex Kakeya sets.

==Besicovitch sets==
[[Image:Perron tree.svg|thumb|A "sprouting" method for constructing a Kakeya set of small measure. Shown here are two possible ways of dividing our triangle and overlapping the pieces to get a smaller set, the first if we just use two triangles, and the second if we use eight. Notice how small the sizes of the final figures are in comparison to the original starting figure.]]

[[Abram Samoilovitch Besicovitch|Besicovitch]] was able to show that there is no lower bound &gt; 0 for the area of such a region ''D'', in which a needle of unit length can be turned round.&lt;ref&gt;{{cite journal | last=Besicovitch | first=Abram | authorlink=Abram Samoilovitch Besicovitch | title=Sur deux questions d'integrabilite des fonctions | journal=J. Soc. Phys. Math. | pages=105–123 | volume=2 | year=1919}}&lt;br&gt;{{cite journal | last=Besicovitch | first=Abram | authorlink=Abram Samoilovitch Besicovitch | title=On Kakeya's problem and a similar one | journal=Mathematische Zeitschrift | volume=27 | pages=312–320 | year=1928 | doi=10.1007/BF01171101}}&lt;/ref&gt; This built on earlier work of his, on plane sets which contain a unit segment in each orientation. Such a set is now called a '''Besicovitch set'''. Besicovitch's work showing such a set could have arbitrarily small  [[measure (mathematics)|measure]] was from 1919. The problem may have been considered by analysts before that.

One method of constructing a Besicovitch set  (see figure for corresponding illustrations) is known as a "Perron tree" after O. Perron who was able to simplify Besicovitch's original construction:&lt;ref&gt;{{cite journal | last=Perron | first=O. | title=Über eine Satz von Besicovitch | journal=Mathematische Zeitschrift | volume=28 | pages=383–386 | year=1928 | doi=10.1007/BF01181172}}&lt;br&gt;{{cite book | last=Falconer | first=K. J. | title=The Geometry of Fractal Sets | publisher=Cambridge University Press | year=1985 | pages=96–99}}&lt;/ref&gt; take a triangle with height 1, divide it in two, and translate both pieces over each other so that their bases overlap on some small interval. Then this new figure will have a reduced total area.

Now, suppose we divide our triangle into eight subtriangles. For each consecutive pair of triangles, perform the same overlapping operation we described before to get four new shapes, each consisting of two overlapping triangles. Next, overlap consecutive pairs of these new shapes by shifting their bases over each other partially, so we're left with two shapes, and finally overlap these two in the same way. In the end, we get a shape looking somewhat like a tree, but with an area much smaller than our original triangle.

To construct an even smaller set, subdivide your triangle into, say, 2&lt;sup&gt;''n''&lt;/sup&gt; triangles each of base length 2&lt;sup&gt;−''n''&lt;/sup&gt;, and perform the same operations as we did before when we divided our triangle twice and eight times. If the amount of overlap we do on each triangle is small enough and the size ''n'' of the subdivision of our triangle is large enough, we can form a tree of area as small as we like. A Besicovitch set can be created by combining three rotations of a Perron tree created from an equilateral triangle.

Adapting this method further, we can construct a sequence of sets whose intersection is a Besicovitch set of measure zero. One way of doing this is to observe that if we have any parallelogram two of whose sides are on the lines ''x'' = 0 and ''x'' = 1 then we can find a union of parallelograms also with sides on these lines, whose total area is arbitrarily small and which contain translates of all lines joining a point on ''x'' = 0 to a point on ''x'' = 1  that are in the original parallelogram. This follows from a slight variation of Besicovich's construction above.  By repeating this we can find a sequence of sets
:&lt;math&gt; K_0\supseteq K_1 \supseteq K_2 \cdots&lt;/math&gt;
each a finite union of parallelograms between the lines ''x'' = 0 and ''x'' = 1, whose areas tend to zero and each of which contains translates of all lines joining ''x'' = 0 and ''x'' = 1 in a unit square. The intersection of these sets is then a measure 0 set containing translates of all these lines, so a union of two copies of this intersection is a measure 0 Besicovich set.

There are other methods for constructing Besicovitch sets of measure zero aside from the 'sprouting' method. For example, [[Jean-Pierre Kahane|Kahane]] uses [[Cantor set]]s to construct a Besicovitch set of measure zero in the two-dimensional plane.&lt;ref&gt;{{cite journal  | last = Kahane | first = Jean-Pierre | authorlink = Jean-Pierre Kahane  | title = Trois notes sur les ensembles parfaits linéaires  | journal = Enseignement Math. | volume = 15 | pages = 185–192 | year = 1969
}}&lt;/ref&gt;

[[Image:KakeyaNeedleSet3.GIF|thumb|A Kakeya needle set constructed from Perron trees.]]

==Kakeya needle sets==
By using a trick of [[Gyula Pál|Pál]], known as '''Pál joins''' (given two parallel lines, any unit line segment can be moved continuously from one to the other on a set of arbitrary small measure), a set in which a unit line segment can be rotated continuously through 180 degrees can be created from a Besicovitch set consisting of Perron trees.&lt;ref&gt;[http://www.mathematik.uni-muenchen.de/~lerdos/Stud/furtner.pdf The Kakeya Problem] by Markus Furtner&lt;/ref&gt;

In 1941, H. J. Van Alphen&lt;ref&gt;{{cite journal  | last = Alphen | first = H. J. | title = Uitbreiding van een stelling von Besicovitch  | journal = Mathematica Zutphen B | volume = 10 | pages = 144–157 | year = 1942}}&lt;/ref&gt; showed that there are arbitrary small Kakeya needle sets inside a circle with radius 2 + ε (arbitrary ε &gt; 0). [[Simply connected]] Kakeya needle sets with smaller area than the deltoid were found in 1965. Melvin Bloom and I. J. Schoenberg independently presented Kakeya needle sets with areas approaching to &lt;math&gt;\tfrac{\pi}{24}(5 - 2\sqrt{2})&lt;/math&gt;, the '''Bloom-Schoenberg number'''. Schoenberg conjectured that this number is the lower bound for the area of simply connected Kakeya needle sets. However, in 1971, F. Cunningham&lt;ref&gt;{{cite journal  | last = Cunningham | first = F. | title = The Kakeya problem for simply connected and for star-shaped sets  | journal = American Mathematical Monthly | volume = 78 | pages = 114–129 | year = 1971 | url = https://www.maa.org/sites/default/files/pdf/upload_library/22/Ford/Cunningham.pdf  | doi = 10.2307/2317619  | issue = 2  | jstor = 2317619 | publisher = The American Mathematical Monthly, Vol. 78, No. 2}}&lt;/ref&gt; showed that, given ε &gt; 0, there is a simply connected Kakeya needle set of area less than ε contained in a circle of radius 1.

Although there are Kakeya needle sets of arbitrarily small positive measure and Besicovich sets of measure 0, there are no Kakeya needle sets of measure 0.

==Kakeya conjecture==

===Statement===
The same question of how small these Besicovitch sets could be was then posed in higher dimensions, giving rise to a number of conjectures known collectively as the ''Kakeya conjectures'', and have helped initiate the field of mathematics known as [[geometric measure theory]].  In particular, if there exist Besicovitch sets of measure zero, could they also have s-dimensional [[Hausdorff measure]] zero for some dimension s less than the dimension of the space in which they lie? This question gives rise to the following conjecture:

:'''Kakeya set conjecture''': Define a ''Besicovitch set'' in '''R'''&lt;sup&gt;''n''&lt;/sup&gt; to be a set which contains a unit line segment in every direction. Is it true that such sets necessarily have [[Hausdorff dimension]] and [[Minkowski dimension]] equal to ''n''?

This is known to be true for ''n'' = 1, 2 but only partial results are known in higher dimensions.

===Kakeya maximal function===
A modern way of approaching this problem is to consider a particular type of [[maximal function]], which we construct as follows: Denote '''S'''&lt;sup&gt;''n''−1&lt;/sup&gt; ⊂ '''R'''&lt;sup&gt;''n''&lt;/sup&gt; to be the unit sphere in ''n''-dimensional space. Define &lt;math&gt;T_{e}^{\delta}(a)&lt;/math&gt; to be the cylinder of length 1, radius δ &gt; 0, centered at the point ''a'' ∈ '''R'''&lt;sup&gt;''n''&lt;/sup&gt;, and whose long side is parallel to the direction of the unit vector ''e'' ∈ '''S'''&lt;sup&gt;''n''−1&lt;/sup&gt;. Then for a [[locally integrable]] function ''f'', we define the '''Kakeya maximal function''' of ''f'' to be

:&lt;math&gt; f_{*}^{\delta}(e)=\sup_{a\in\mathbf{R}^{n}}\frac{1}{m(T_{e}^{\delta}(a))}\int_{T_{e}^{\delta}(a)}|f(y)|dm(y)&lt;/math&gt;

where ''m'' denotes the n-dimensional [[Lebesgue measure]]. Notice that &lt;math&gt;f_{*}^{\delta}&lt;/math&gt; is defined for vectors ''e'' in the sphere '''S'''&lt;sup&gt;''n''−1&lt;/sup&gt;.

Then there is a conjecture for these functions that, if true, will imply the Kakeya set conjecture for higher dimensions:

:'''Kakeya maximal function conjecture''': For all ε &gt; 0, there exists a constant ''C&lt;sub&gt;ε&lt;/sub&gt;'' &gt; 0 such that for any function ''f'' and all δ &gt; 0, (see [[lp space]] for notation)
::&lt;math&gt; \left \|f_{*}^{\delta} \right \|_{L^n(\mathbf{S}^{n-1})} \leqslant C_{\epsilon} \delta^{-\epsilon}\|f\|_{L^n(\mathbf{R}^{n})}. &lt;/math&gt;

=== Results===
Some results toward proving the Kakeya conjecture are the following:
* The Kakeya conjecture is true for ''n'' = 1 (trivially) and ''n'' = 2 (Davies&lt;ref&gt;{{cite journal| last = Davies | first = Roy  | title =Some remarks on the Kakeya problem | journal =Proc. Cambridge Philos. Soc. | volume = 69 | pages = 417&amp;ndash;421 | year = 1971  | doi = 10.1017/S0305004100046867 | issue = 3| bibcode = 1971PCPS...69..417D}}&lt;/ref&gt;).
* In any n-dimensional space, Wolff&lt;ref&gt;{{cite journal  | last = Wolff | first = Thomas | authorlink = Thomas Wolff| title = An improved bound for Kakeya type maximal functions  | journal = Rev. Mat. Iberoamericana | volume = 11 | pages = 651&amp;ndash;674 | year = 1995 | doi=10.4171/rmi/188}}&lt;/ref&gt; showed that the dimension of a Kakeya set must be at least (''n''+2)/2.
* In 2002, [[Nets Hawk Katz|Katz]] and [[Terence Tao|Tao]]&lt;ref&gt;{{cite journal | last=Katz | first=Nets Hawk|author2=Tao, Terence  | title = New bounds for Kakeya problems  | journal = J. Anal. Math. | volume = 87 | pages = 231&amp;ndash;263 | year = 2002 | doi=10.1007/BF02868476| arxiv=math/0102135}}&lt;/ref&gt; improved Wolff's bound to &lt;math&gt;(2-\sqrt{2})(n-4)+3&lt;/math&gt;, which is better for ''n'' &gt; 4.
* In 2000, [[Nets Hawk Katz|Katz]],  [[Izabella Łaba|Łaba]], and Tao&lt;ref&gt;{{cite journal|last1=Katz|first1=Nets Hawk|last2=Laba|first2=Izabella|last3=Tao|first3=Terence|title=An Improved Bound on the Minkowski Dimension of Besicovitch Sets in ℝ 3|journal=The Annals of Mathematics|date=September 2000|volume=152|issue=2|pages=383|doi=10.2307/2661389}}&lt;/ref&gt; proved that the [[Minkowski–Bouligand dimension|Minkowski dimension]] of Kakeya sets in 3 dimensions is strictly greater than 5/2.
* In 2000, [[Jean Bourgain]] connected the Kakeya problem to [[arithmetic combinatorics]]&lt;ref&gt;J. BOURGAIN, Harmonic analysis and combinatorics: How much may they contribute to each other?, Mathematics: Frontiers and Perspectives, IMU/Amer. Math. Soc., 2000, pp. 13–32.&lt;/ref&gt;&lt;ref&gt;{{cite journal | last = Tao | first = Terence | authorlink = Terence Tao  | title = From Rotating Needles to Stability of Waves: Emerging Connections between Combinatorics, Analysis and PDE| url=http://www.ams.org/notices/200103/fea-tao.pdf | journal = Notices of the AMS | volume = 48 | issue = 3 | pages = 297&amp;ndash;303 |date=March 2001}}&lt;/ref&gt; which involves [[harmonic analysis]] and [[additive number theory]].
* In 2017, [[Nets Hawk Katz|Katz]] and Zahl&lt;ref&gt;{{cite arxiv|last=Katz|first=Nets Hawk|last2=Zahl|first2=Joshua|date=2017|title=An improved bound on the Hausdorff dimension of Besicovitch sets in ℝ3|eprint=1704.07210|class=math.CA}}&lt;/ref&gt; improved the lower bound on the [[Hausdorff dimension]] of Besicovitch sets in 3 dimensions to &lt;math&gt;5/2+\epsilon&lt;/math&gt; for an absolute constant &lt;math&gt;\epsilon&gt;0&lt;/math&gt;.

==Applications to analysis==
Somewhat surprisingly, these conjectures have been shown to be connected to a number of questions in other fields, notably in [[harmonic analysis]].  For instance, in 1971, [[Charles Fefferman]]&lt;ref&gt;{{cite journal
  | last = Fefferman | first = Charles | authorlink = Charles Fefferman
  | title = The multiplier problem for the ball
  | journal = Annals of Mathematics | volume = 94 | pages = 330&amp;ndash;336 | year = 1971
  | doi = 10.2307/1970864
  | issue = 2
  | jstor = 1970864
}}&lt;/ref&gt; was able to use the Besicovitch set construction to show that in dimensions greater than 1, truncated Fourier integrals taken over balls centered at the origin with radii tending to infinity need not converge in [[Lp space|''L''&lt;sup&gt;''p''&lt;/sup&gt; norm]] when ''p'' ≠ 2 (this is in contrast to the one-dimensional case where such truncated integrals do converge).

==Analogues and generalizations of the Kakeya problem==

===Sets containing circles and spheres===
Analogues of the Kakeya problem include considering sets containing more general shapes than lines, such as circles.

* In 1997&lt;ref&gt;{{cite journal
  | last = Wolff | first = Thomas
  | title = A Kakeya problem for circles
  | journal = American Journal of Mathematics | volume = 119 | pages = 985&amp;ndash;1026 | year = 1997
  | doi = 10.1353/ajm.1997.0034|authorlink =Thomas Wolff
  | issue = 5
}}&lt;/ref&gt; and 1999,&lt;ref&gt;{{cite journal
  | last = Wolff | first = Thomas
  | title = On some variants of the Kakeya problem
  | journal = Pacific Journal of Mathematics| volume = 190 | pages = 111&amp;ndash;154 | year = 1999
  | doi = 10.2140/pjm.1999.190.111|authorlink =Thomas Wolff
  | last2 = Wolff
  | first2 = Thomas
}}&lt;/ref&gt; Wolff proved that sets containing a sphere of every radius must have full dimension, that is, the dimension is equal to the dimension of the space it is lying in, and proved this by proving bounds on a circular maximal function analogous to the Kakeya maximal function.

* It was conjectured that there existed sets containing a sphere around every point of measure zero. Results of [[Elias Stein]]&lt;ref&gt;{{cite journal  | last = Stein | first = Elias
 | title = Maximal functions: Spherical means
 | journal =Proc. Natl. Acad. Sci. U.S.A. | volume = 73 | pages = 2174&amp;ndash;2175 | year = 1976
 | doi = 10.1073/pnas.73.7.2174
 | pmid = 16592329  | issue = 7  | pmc = 430482|authorlink =Elias Stein
| bibcode = 1976PNAS...73.2174S}}&lt;/ref&gt; proved all such sets must have positive measure when ''n'' ≥ 3, and Marstrand&lt;ref&gt;{{cite journal
 | last = Marstrand | first = J. M.
| title = Packing circles in the plane | volume = 55 | pages = 37&amp;ndash;58 | year = 1987
| journal = Proceedings of the London Mathematical Society
| doi = 10.1112/plms/s3-55.1.37|authorlink =J. M. Marstrand
}}&lt;/ref&gt; proved the same for the case ''n=2''.

===Sets containing k-dimensional disks===
A generalization of the Kakeya conjecture is to consider sets that contain, instead of segments of lines in every direction, but, say, portions of ''k''-dimensional subspaces. Define an '''(''n'', ''k'')-Besicovitch set''' ''K'' to be a compact set in '''R'''&lt;sup&gt;''n''&lt;/sup&gt; containing a translate of every ''k''-dimensional unit disk which has Lebesgue measure zero. That is, if ''B'' denotes the unit ball centered at zero, for every ''k''-dimensional subspace ''P'', there exists ''x'' ∈ '''R'''&lt;sup&gt;''n''&lt;/sup&gt; such that (''P'' ∩ ''B'') + ''x'' ⊆ ''K''. Hence, a (''n'', 1)-Besicovitch set is the standard Besicovitch set described earlier.

:'''The (''n'', ''k'')-Besicovitch conjecture:''' There are no (''n'', ''k'')-Besicovitch sets for ''k'' &gt; 1.

In 1979, Marstrand&lt;ref&gt;{{cite journal | last = Marstrand | first = J. M.| title = Packing Planes in R&lt;sup&gt;3&lt;/sup&gt;  | journal = Mathematika | volume = 26 | pages = 180&amp;ndash;183 | year = 1979|authorlink =J. M. Marstrand| doi = 10.1112/S0025579300009748
| issue = 2}}&lt;/ref&gt;  proved that there were no (3, 2)-Besicovitch sets. At around the same time, however, [[Kenneth Falconer (mathematician)|Falconer]]&lt;ref&gt;{{cite journal | last = Falconer | first = K. J. | authorlink=Kenneth Falconer (mathematician)| title = Continuity properties of k-plane integrals and Besicovitch sets | journal = Math. Proc. Cambridge Philos. Soc. | volume = 87 | pages = 221&amp;ndash;226 | year = 1980 | doi = 10.1017/S0305004100056681| issue = 2| bibcode = 1980MPCPS..87..221F}}&lt;/ref&gt;  proved that there were no (''n'', ''k'')-Besicovitch sets for 2''k'' &gt; ''n''. The best bound to date is by Bourgain,&lt;ref&gt;{{cite journal
 | last = Bourgain | first = Jean | title = Besicovitch type maximal operators and applications to Fourier analysis | journal = Geom. Funct. Anal. | volume = 1 | pages = 147&amp;ndash;187 | year = 1997 | doi = 10.1007/BF01896376|authorlink =Jean Bourgain
 | issue = 2}}&lt;/ref&gt; who proved in that no such sets exist when 2&lt;sup&gt;''k''−1&lt;/sup&gt; + ''k'' &gt; ''n''.

===Kakeya sets in vector spaces over finite fields===
In 1999, Wolff posed the [[finite field]] analogue to the Kakeya problem, in hopes that the techniques for solving this conjecture could be carried over to the Euclidean case.

:'''Finite Field Kakeya Conjecture''': Let '''F''' be a finite field, let ''K'' ⊆ '''F'''&lt;sup&gt;n&lt;/sup&gt; be a Kakeya set, i.e. for each vector ''y'' ∈ '''F'''&lt;sup&gt;''n''&lt;/sup&gt; there exists ''x'' ∈ '''F'''&lt;sup&gt;''n''&lt;/sup&gt; such that ''K'' contains a line {''x'' + ''ty'' : ''t'' ∈ '''F'''}. Then the set ''K'' has size at least ''c&lt;sub&gt;n&lt;/sub&gt;''|'''F'''|&lt;sup&gt;''n''&lt;/sup&gt; where ''c&lt;sub&gt;n&lt;/sub&gt;''&gt;0 is a constant that only depends on ''n''.

Zeev Dvir proved this conjecture in 2008, showing that the statement holds for ''c&lt;sub&gt;n&lt;/sub&gt;'' = 1/''n''!.&lt;ref&gt;{{cite journal |first=Z. |last=Dvir |title=On the size of Kakeya sets in finite fields |journal=J. Amer. Math. Soc. |volume=22 |issue= |pages=1093–1097 |year=2009 |doi=10.1090/S0894-0347-08-00607-3 }}&lt;/ref&gt;&lt;ref&gt;{{cite web |url=http://terrytao.wordpress.com/2008/03/24/dvirs-proof-of-the-finite-field-kakeya-conjecture/ |title=Dvir’s proof of the finite field Kakeya conjecture |accessdate=2008-04-08 |author=Terence Tao |author-link=Terence Tao |date=2008-03-24|work=What's New}}&lt;/ref&gt; In his proof, he observed that any polynomial in ''n'' variables of degree less than |'''F'''| vanishing on a Kakeya set must be identically zero. On the other hand, the polynomials in ''n'' variables of degree less than |'''F'''| form a vector space of dimension

:&lt;math&gt;{|\mathbf{F}|+n-1\choose n}\ge \frac{|\mathbf{F}|^n}{n!}.&lt;/math&gt;

Therefore, there is at least one non-trivial polynomial of degree less than |'''F'''| that vanishes on any given set with less than this number of points. Combining these two observations shows that Kakeya sets must have at least |'''F'''|&lt;sup&gt;''n''&lt;/sup&gt;/''n''! points.

It's not clear whether the techniques will extend to proving the original Kakeya conjecture but this proof does lend credence to the original conjecture by making essentially algebraic counterexamples unlikely. Dvir has written a survey article on {{As of|2009|alt=recent}} progress on the finite field Kakeya problem and its relationship to [[randomness extractor]]s.&lt;ref&gt;{{Cite journal
|id={{ECCC|2009|09|077}} |title=From Randomness Extraction to Rotating Needles |year=2009|first=Zeev|last=Dvir}}.&lt;/ref&gt;

==See also==
* [[Nikodym set]]

==Notes==
{{reflist|colwidth=30em}}

==References==
*{{cite journal
  | last = Besicovitch | first = Abram | authorlink = Abram Samoilovitch Besicovitch
  | title = The Kakeya Problem
  | journal = American Mathematical Monthly | volume = 70 | pages = 697&amp;ndash;706 | year = 1963
  | doi = 10.2307/2312249
  | issue = 7
  | jstor = 2312249
  | mr=0157266
}}
*{{Cite journal | last1=Dvir | first1=Zeev | title=On the size of Kakeya sets in finite fields | arxiv=0803.2336  | doi=10.1090/S0894-0347-08-00607-3 | mr=2525780 | year=2009 | journal=[[Journal of the American Mathematical Society]] | volume=22 | issue=4 | pages=1093–1097| bibcode=2009JAMS...22.1093D }}
* {{cite book
  | last = Falconer | first = Kenneth J. 
  | title = The Geometry of Fractal Sets
  | publisher = Cambridge University Press | year = 1985
  | mr=0867284
  | location = Cambridge
  | series=Cambridge Tracts in Mathematics
  | volume=85
  | isbn=0-521-25694-1}}
*{{Cite journal | last1=Kakeya | first1=Soichi | title=Some problems on maximum and minimum regarding ovals | year=1917 | journal=Tohoku science reports | volume=6 | pages=71–88 | ref=harv}}
*{{cite journal
  | last1 = Katz | first1 = Nets Hawk | author1-link = Nets Katz
  | last2 = Łaba | first2 = Izabella | author2-link = Izabella Łaba
  | last3 = Tao | first3 = Terence | author3-link = Terence Tao
  | title = An improved bound on the Minkowski dimension of Besicovitch sets in &lt;math&gt;\mathbf{R}^3&lt;/math&gt;
  | journal = Annals of Mathematics | volume = 152 | pages = 383–446 | year = 2000
  | url = http://www.emis.de/journals/Annals/152_2/laba.pdf
  | doi = 10.2307/2661389
  | issue = 2
  | jstor = 2661389
  | mr=1804528
 }}

* {{cite book
  | last = Wolff | first = Thomas | authorlink = Thomas Wolff
  | chapter = Recent work connected with the Kakeya problem
  | pages=129–162
  | title = Prospects in Mathematics: Invited Talks on the Occasion of the 250th Anniversary of Princeton University | editor-first = Hugo | editor-last = Rossi
  | publisher = American Mathematical Society | location=Providence, RI | year = 1999 | isbn=978-0-8218-0975-4
  | mr=1660476}}
* {{cite book
  | last = Wolff | first = Thomas | authorlink = Thomas Wolff
  | title = Lectures on Harmonic Analysis
  | others = With a foreword by Charles Fefferman and preface by Izabella Łaba
  | editor1-last = Łaba | editor1-first = Izabella
  | editor2-last = Shubin | editor2-first = Carol
  | series = University Lecture Series
  | volume = 29
  | publisher = American Mathematical Society | location=Providence, RI| year = 2003
  | mr=2003254
  | isbn=0-8218-3449-5
  | doi=10.1090/ulect/029}}

==External links==
*[http://www.math.ubc.ca/~ilaba/kakeya.html Kakeya at University of British Columbia ]
*[http://www.math.ucla.edu/~tao/java/Besicovitch.html Besicovitch at UCLA]
*[http://mathworld.wolfram.com/KakeyaNeedleProblem.html Kakeya needle problem at mathworld]
*[http://terrytao.wordpress.com/2008/03/24/dvirs-proof-of-the-finite-field-kakeya-conjecture/ Dvir’s proof of the finite field Kakeya conjecture at Terence Tao's blog]
*[https://www.math.stonybrook.edu/~bishop/lectures/UW.pdf An Introduction to Besicovitch-Kakeya Sets]

{{DEFAULTSORT:Kakeya Set}}
[[Category:Harmonic analysis]]
[[Category:Real analysis]]
[[Category:Discrete geometry]]</text>
      <sha1>1jh394q8xn1d0rpy4mlmw2utq7m2v0s</sha1>
    </revision>
  </page>
  <page>
    <title>Lebesgue integration</title>
    <ns>0</ns>
    <id>26064288</id>
    <revision>
      <id>871750314</id>
      <parentid>871750262</parentid>
      <timestamp>2018-12-03T05:05:20Z</timestamp>
      <contributor>
        <ip>68.80.80.184</ip>
      </contributor>
      <comment>/* Simple functions */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="40419">[[File:Integral-area-under-curve.svg|thumb|The integral of a positive function can be interpreted as the area under a curve.]]
{{Calculus |Integral}}

In [[mathematics]], the [[integral]] of a non-negative [[Function (mathematics)|function]] of a single variable can be regarded, in the simplest case, as the [[area]] between the [[Graph of a function|graph]] of that function and the {{math|''x''}}-axis. The '''Lebesgue integral''' extends the integral to a larger class of functions. It also extends the [[Domain (mathematics)|domain]]s on which these functions can be defined.

Long before the advent of the 20th century, mathematicians already understood that for non-negative functions with a [[Smooth function|smooth]] enough graph—such as [[continuous function]]s on [[Closed set|closed]] [[Bounded set|bounded]] [[Interval (mathematics)|interval]]s—the ''area under the curve'' could be defined as the integral, and computed using approximation techniques on the region by [[polygon]]s.  However, as the need to consider more irregular functions arose—e.g., as a result of the [[Limit of a function|limiting]] processes of [[mathematical analysis]] and the mathematical [[theory of probability]]—it became clear that more careful approximation techniques were needed to define a suitable integral.  Also, one might wish to integrate on spaces more general than the real line. The Lebesgue integral provides the right abstractions needed to do this important job.

The Lebesgue integral plays an important role in probability theory, [[real analysis]], and many other fields in the mathematical sciences. It is named after [[Henri Lebesgue]] (1875–1941), who introduced the integral {{harv|Lebesgue|1904}}. It is also a pivotal part of the [[axiomatic theory of probability]].

The term ''Lebesgue integration'' can mean either the general theory of integration of a function with respect to a general [[measure (mathematics)|measure]], as introduced by Lebesgue, or the specific case of integration of a function defined on a sub-domain of the [[real line]] with respect to [[Lebesgue measure]].

== Introduction ==
The integral of a positive function {{math|''f''}} between limits {{math|''a''}} and {{math|''b''}} can be interpreted as the area under the graph of {{math|''f''}}. This is easy to understand for familiar functions such as [[polynomials]], but what does it mean for more exotic functions? In general, for which class of functions does "area under the curve" make sense? The answer to this question has great theoretical and practical importance.

As part of a general movement toward [[Mathematical rigor|rigor]] in mathematics in the nineteenth century, mathematicians attempted to put integral calculus on a firm foundation. The [[Riemann integral]]—proposed by [[Bernhard Riemann]] (1826–1866)—is a broadly successful attempt to provide such a foundation. Riemann's definition starts with the construction of a sequence of easily calculated areas that converge to the integral of a given function. This definition is successful in the sense that it gives the expected answer for many already-solved problems, and gives useful results for many other problems.

However, Riemann integration does not interact well with taking limits of sequences of functions, making such limiting processes difficult to analyze. This is important, for instance, in the study of [[Fourier series]], [[Fourier transform]]s, and other topics. The Lebesgue integral is better able to describe how and when it is possible to take limits under the integral sign (via the powerful [[monotone convergence theorem]] and [[dominated convergence theorem]]).

While the Riemann integral considers the area under a curve as made out of vertical rectangles, the Lebesgue definition considers horizontal slabs that are not necessarily just rectangles, and so it is more flexible. For this reason, the Lebesgue definition makes it possible to calculate integrals for a broader class of functions. For example, the [[Dirichlet function]], which is 0 where its argument is [[irrational number|irrational]] and 1 otherwise, has a Lebesgue integral, but does not have a Riemann integral.  Furthermore, the Lebesgue integral of this function is zero, which agrees with the intuition that when picking a real number uniformly at random from the unit interval, the probability of picking a rational number should be zero.

Lebesgue summarized his approach to integration in a letter to [[Paul Montel]]:
{{quote|I have to pay a certain sum, which I have collected in my pocket.  I take the bills and coins out of my pocket and give them to the creditor in the order I find them until I have reached the total sum. This is the Riemann integral. But I can proceed differently. After I have taken all the money out of my pocket I order the bills and coins according to identical values and then I pay the several heaps one after the other to the creditor. This is my integral.|sign=&lt;small&gt;''Source'': {{harv|Siegmund-Schultze|2008}}&lt;/small&gt;}}

The insight is that one should be able to rearrange the values of a function freely, while preserving the value of the integral.  This process of rearrangement can convert a very [[Pathological (mathematics)|pathological function]] into one that is "nice" from the point of view of integration, and thus let such pathological functions be integrated.

=== Intuitive interpretation ===
[[Image:RandLintegrals.png|thumb|250px|Riemann-Darboux's integration (in blue) and Lebesgue integration (in red).]]
To get some intuition about the different approaches to integration, let us imagine that we want to find a mountain's volume (above sea level).

;The Riemann–Darboux approach: Divide the base of the mountain into a grid of 1 meter squares. Measure the altitude of the mountain at the center of each square. The volume on a single grid square is approximately 1&amp;nbsp;m&lt;sup&gt;2&lt;/sup&gt; × (that square's altitude), so the total volume is 1&amp;nbsp;m&lt;sup&gt;2&lt;/sup&gt; times the sum of the altitudes.

;The Lebesgue approach: Draw a [[contour map]] of the mountain, where adjacent contours are 1&amp;nbsp;meter of altitude apart. The volume of earth a single contour contains is approximately 1&amp;nbsp;m × (that contour's area), so the total volume is the sum of these areas times 1&amp;nbsp;m.

Folland summarizes the difference between the Riemann and Lebesgue approaches thus: "to compute the Riemann integral of {{math|''f''}}, one partitions the domain {{math|[''a'', ''b'']}} into subintervals", while in the Lebesgue integral, "one is in effect partitioning the range of {{math|''f''}} ."&lt;ref name="Folland"&gt;{{cite book |first=Gerald B. |last=Folland |title=Real Analysis: Modern Techniques and Their Applications |location= |publisher=Wiley |year=1984 |page=56 |url=https://books.google.com/books?id=AnIPAQAAMAAJ&amp;pg=PA56 }}&lt;/ref&gt;

=== Towards a formal definition ===
[[File:Horizontal slice for Lebesgue.svg|thumb|right|A measurable function is shown, together with the set &lt;math&gt;\{x|f(x)&gt;t\}&lt;/math&gt; (on the x-axis).  The Lebesgue integral is gotten by slicing along the y-axis, using the 1-dimensional Lebesgue measure to measure the "width" of the slices.
]]
To define the Lebesgue integral requires the formal notion of a [[measure (mathematics)|measure]] that, roughly, associates to each set {{math|''A''}} of real numbers a nonnegative number {{math|μ(''A'')}} representing the "size" of {{math|''A''}}.  This notion of "size" should agree with the usual length of an interval or disjoint union of intervals.  Suppose that {{math|''f'' : ℝ → ℝ&lt;sup&gt;+&lt;/sup&gt;}} is a non-negative real-valued function.  Using the "partitioning the range of {{math|''f''}} " philosophy, the integral of {{math|''f''}} should be the sum over {{math|''t''}} of the elementary area contained in the thin horizontal strip between {{math|''y'' {{=}} ''t'' and ''y'' {{=}} ''t'' &amp;minus; ''dt''}}.  This elementary area is just
:&lt;math&gt;\mu \left (\{x\mid f(x)&gt;t\} \right ) \,dt.&lt;/math&gt;
Let
:&lt;math&gt;f^*(t)=\mu \left (\{x\mid f(x)&gt;t\} \right ).&lt;/math&gt;
The Lebesgue integral of {{math|''f''}} is then defined by&lt;ref&gt;{{harvnb|Lieb|Loss|2001}}&lt;/ref&gt;
:&lt;math&gt;\int f\,d\mu = \int_0^\infty f^*(t)\,dt&lt;/math&gt;
where the integral on the right is an ordinary [[improper Riemann integral]].  Note that {{math|''f''&lt;sup&gt;∗&lt;/sup&gt;}} is a non-negative decreasing function, and therefore has a well-defined improper Riemann integral with value in the interval {{math|[0,&amp;infin;]}}.  For a suitable class of functions (the [[measurable function]]s), this defines the Lebesgue integral.

A general (not necessarily positive) measurable function {{math|''f''}} is Lebesgue integrable if the area between the graph of {{math|''f''}} and the {{math|''x''}}-axis is finite:
:&lt;math&gt;\int |f|\,d\mu &lt; + \infty.&lt;/math&gt;
In that case, as in the Riemannian case, the integral is the difference between the area above the {{math|''x''}}-axis and the area below the {{math|''x''}}-axis:
:&lt;math&gt;\int f \,d\mu = \int f^+ \,d\mu - \int f^- \,d\mu&lt;/math&gt;
where &lt;math&gt;f=f^+ - f^-&lt;/math&gt; is the decomposition of ''f'' into the difference of two non-negative functions given by
:&lt;math&gt;\begin{align}
 f^+(x)&amp;=\max\{f(x),0\} &amp;=&amp;\begin{cases}
               f(x), &amp; \text{if } f(x) &gt; 0, \\
               0, &amp; \text{otherwise}
             \end{cases}\\
 f^-(x)&amp;=\max\{-f(x),0\} &amp;=&amp;\begin{cases}
               -f(x), &amp; \text{if } f(x) &lt; 0, \\
               0, &amp; \text{otherwise.}
             \end{cases}
\end{align}&lt;/math&gt;

== Construction ==
The theory of the Lebesgue integral requires a theory of measurable sets and measures on these sets, as well as a theory of measurable functions and integrals on these functions.

=== Measure theory ===
{{further|Measure (mathematics)}}
[[Measure theory]] was initially created to provide a useful abstraction of the notion of length of subsets of the real line—and, more generally, area and volume of subsets of Euclidean spaces. In particular, it provided a systematic answer to the question of which subsets of {{math|ℝ}} have a length. As later [[set theory]] developments showed (see [[non-measurable set]]), it is actually impossible to assign a length to all subsets of {{math|ℝ}} in a way that preserves some natural additivity and translation invariance properties.  This suggests that picking out a suitable class of ''measurable'' subsets is an essential prerequisite.

The Riemann integral uses the notion of length explicitly. Indeed, the element of calculation for the Riemann integral is the rectangle {{math|[''a'', ''b''] × [''c'', ''d'']}}, whose area is calculated to be {{math|(''b'' − ''a'')(''d'' − ''c'')}}. The quantity {{math|''b'' − ''a''}} is the length of the base of the rectangle and {{math|''d'' − ''c''}}  is the height of the rectangle.  Riemann could only use planar rectangles to approximate the area under the curve, because there was no adequate theory for measuring more general sets.

In the development of the theory in most modern textbooks (after 1950), the approach to measure and integration is ''axiomatic''.  This means that a measure is any function μ defined on a certain class {{math|''X'' }} of subsets of a set {{math|''E''}}, which satisfies a certain list of properties. These properties can be shown to hold in many different cases.

=== Measurable functions ===
We start with a [[measure space]] {{math|(''E'', ''X'', μ)}} where {{math|''E''}} is a [[Set (mathematics)|set]], {{math|''X''}} is a [[sigma-algebra|σ-algebra]] of subsets of {{math|''E''}}, and μ is a (non-[[Signed measure|negative]]) [[measure (mathematics)|measure]] on {{math|''E''}} defined on the sets of {{math|''X''}}.

For example,  {{math|''E''}} can be [[Euclidean space|Euclidean {{math|''n''}}-space]] {{math|ℝ&lt;sup&gt;''n''&lt;/sup&gt;}} or some [[Lebesgue measure|Lebesgue measurable]] subset of it, {{math|''X''}} is the [[σ-algebra]] of all Lebesgue measurable subsets of {{math|''E''}}, and μ is the Lebesgue measure. In the mathematical theory of probability, we confine our study to a [[probability]] measure&amp;nbsp;{{math|μ}}, which satisfies {{math|μ(''E'') {{=}} 1}}.

Lebesgue's theory defines integrals for a class of functions called [[measurable function]]s. A real-valued function {{math|''f''}} on {{math|''E''}} is measurable if the [[pre-image]] of every interval of the form {{math|(''t'', ∞)}} is in {{math|''X''}}:

:&lt;math&gt; \{x\,\mid\,f(x) &gt; t\} \in X\quad \forall t\in\mathbb{R}. &lt;/math&gt;

We can show that this is equivalent to requiring that the pre-image of any [[Borel algebra|Borel]] subset of ℝ be in {{math|''X''}}. The set of measurable functions is closed under algebraic operations, but more importantly it is closed under various kinds of [[Limit superior and limit inferior|point-wise sequential limits]]:

: &lt;math&gt; \sup_{k \in \mathbb{N}} f_k, \quad \liminf_{k \in \mathbb{N}} f_k, \quad \limsup_{k \in \mathbb{N}} f_k &lt;/math&gt;

are measurable if the original sequence {{math|(''f''&lt;sub&gt;''k''&lt;/sub&gt;)&lt;sub&gt;''k''&lt;/sub&gt;}}, where {{math|''k'' ∈  ℕ}}, consists of measurable functions.

There are several approaches for defining an integral:

: &lt;math&gt; \int_E f \, d \mu = \int_E f\left(x\right)\, d\mu\left(x\right)&lt;/math&gt;

for measurable real-valued functions {{math|''f''}} defined on {{math|''E''}}.  

===Constructing the integral===
[[File:Lebesgueintegralsimplefunctions.svg|right|thumb|Approximating a function by simple functions.]]
One approach to constructing the Lebesgue integral is to make use of so-called ''simple functions'': finite real-linear combinations of ''indicator functions''.  Simple functions can be used to approximate a measurable function, by partitioning the range into layers.  The integral of a simple function is equal to the measure of a given layer, times the height of that layer.  The integral of a non-negative general measurable function is then defined as an appropriate [[supremum]] of approximations by simple functions, and the integral of a (not necessarily positive) measurable function is the difference of two integrals of non-negative measurable functions, as mentioned [[#Towards a formal definition|earlier]].

==== Indicator functions ==== 
To assign a value to the integral of the [[indicator function]] {{math|1&lt;sub&gt;''S''&lt;/sub&gt;}} of a measurable set {{math|''S''}} consistent with the given measure μ, the only reasonable choice is to set:

:&lt;math&gt;\int 1_S  \,  d\mu = \mu (S).&lt;/math&gt;

Notice that the result may be equal to {{math|+∞}}, unless {{math|μ}} is a ''finite'' measure.

==== Simple functions ====
A finite [[linear combination]] of indicator functions

:&lt;math&gt;\sum_k a_k 1_{S_k}&lt;/math&gt;

where the coefficients {{math|''a''&lt;sub&gt;''k''&lt;/sub&gt;}} are real numbers and the sets {{math|''S&lt;sub&gt;k&lt;/sub&gt;''}} are measurable, is called a measurable [[simple function]]. We extend the integral by linearity to ''non-negative'' measurable simple functions.  When the coefficients {{math|''a&lt;sub&gt;k&lt;/sub&gt;''}} are non-negative, we set

:&lt;math&gt;\int \left(\sum_k a_k 1_{S_k}\right) \, d \mu = \sum_k a_k \int 1_{S_k} \, d \mu = \sum_k a_k \, \mu(S_k). &lt;/math&gt;

The convention {{math|0 × ∞ {{=}} 0}} must be used, and the result may be infinite. Even if a simple function can be written in many ways as a linear combination of indicator functions, the integral is always the same. This can be shown using the additivity property of measures.

Some care is needed when defining the integral of a ''real-valued'' simple function, to avoid the undefined expression {{math|∞ − ∞}}: one assumes that the representation

:&lt;math&gt; f = \sum_k a_k 1_{S_k}&lt;/math&gt;

is such that {{math|μ(''S''&lt;sub&gt;''k''&lt;/sub&gt;) &lt; ∞}} whenever {{math|''a''&lt;sub&gt;''k''&lt;/sub&gt; ≠ 0}}. Then the above formula for the integral of ''f'' makes sense, and the result does not depend upon the particular representation of {{math|''f''}} satisfying the assumptions.

If {{math|''B''}} is a measurable subset of {{math|''E''}} and {{math|''s''}} is a measurable simple function one defines

:&lt;math&gt; \int_B s \, d\mu = \int 1_B \, s \, d\mu = \sum_k a_k \, \mu(S_k \cap B). &lt;/math&gt;

==== Non-negative functions ====
Let {{math|''f''}} be a non-negative measurable function on {{math|''E''}}, which we allow to attain the value {{math|+∞}}, in other words, {{math|''f''}} takes non-negative values in the [[extended real number line]].  We define

:&lt;math&gt;\int_E f \, d\mu = \sup\left\{\,\int_E s\, d\mu : 0 \le s \le f,\ s\ \text{simple}\,\right\}.&lt;/math&gt;

We need to show this integral coincides with the preceding one, defined on the set of simple functions, when ''E''  is a segment [''a'',&amp;nbsp;''b'']. There is also the question of whether this corresponds in any way to a Riemann notion of integration. It is possible to prove that the answer to both questions is yes.

We have defined the integral of ''f'' for any non-negative extended real-valued measurable function on&amp;nbsp;''E''. For some functions, this integral  ∫&lt;sub&gt;''E''&lt;/sub&gt;&amp;nbsp;''f''&amp;nbsp;dμ  is infinite.

It is often useful to have a particular sequence of simple functions that approximates the Lebesgue integral well (analogously to a Riemann sum).  For a non-negative measurable function {{math|''f''}}, let &lt;math&gt;s_n(x)&lt;/math&gt; be the simple function whose value is &lt;math&gt;k/2^n&lt;/math&gt; whenever &lt;math&gt;k/2^n\le f(x)&lt;(k+1)/2^n&lt;/math&gt;, for ''k'' a non-negative integer less than (say) &lt;math&gt;4^n&lt;/math&gt;.  Then it can be proven directly that
:&lt;math&gt;\int f\,d\mu = \lim_{n\to\infty} \int s_n\,d\mu&lt;/math&gt;
and that the limit on the right hand side exists as an extended real number.  This bridges the connection between the approach to the Lebesgue integral using simple functions, and the motivation for the Lebesgue integral using a partition of the range.

==== Signed functions ====
To handle signed functions, we need a few more definitions. If {{math|''f''}} is a measurable function of the set {{math|''E''}} to the reals (including {{math|±∞}}), then we can write

:&lt;math&gt; f = f^+ - f^-, \quad &lt;/math&gt;

where

:&lt;math&gt; f^+(x) = \left\{\begin{matrix} f(x) &amp; \text{if } f(x) &gt; 0 \\ 0 &amp; \text{otherwise} \end{matrix}\right. &lt;/math&gt;
:&lt;math&gt; f^-(x) = \left\{\begin{matrix} -f(x) &amp; \text{if }  f(x) &lt; 0 \\ 0 &amp; \text{otherwise} \end{matrix}\right. &lt;/math&gt;

Note that both {{math|''f''&lt;sup&gt;+&lt;/sup&gt;}} and {{math|''f''&lt;sup&gt;−&lt;/sup&gt;}} are non-negative measurable functions. Also note that

:&lt;math&gt; |f| = f^+ + f^-. \quad &lt;/math&gt;

We say that the Lebesgue integral of the measurable function {{math|''f''}} ''exists'', or ''is defined'' if at least one of &lt;math&gt; \int f^+ \, d\mu &lt;/math&gt; and &lt;math&gt; \int f^- \, d\mu &lt;/math&gt; is finite:

:&lt;math&gt; \min\left(\int f^+ \, d \mu, \int f^- \, d \mu\right) &lt; \infty. &lt;/math&gt;

In this case we ''define''

:&lt;math&gt; \int f \, d \mu  =  \int f^+ \, d\mu - \int f^- \, d\mu. &lt;/math&gt;

If

:&lt;math&gt; \int |f| \, d \mu &lt; \infty, &lt;/math&gt;

we say that {{math|''f''}} is ''Lebesgue integrable''.

It turns out that this definition gives the desirable properties of the integral.

==== Complex valued functions ==== 
[[Complex number|Complex]] valued functions can be similarly integrated, by considering the real part and the imaginary part separately.

If ''h''=''f''+''ig'' for real-valued integrable functions ''f'', ''g'', then the integral of ''h'' is defined by

:&lt;math&gt; \int h \, d \mu = \int f \, d \mu + i \int g \, d \mu.&lt;/math&gt;

The function is Lebesgue integrable if and only if its [[absolute value]] is Lebesgue integrable (see [[Absolutely integrable function]]).

=== Example ===
Consider the [[indicator function]] of the rational numbers, {{math|1&lt;sub&gt;'''Q'''&lt;/sub&gt;}}.  This function is [[nowhere continuous]].

* &lt;math&gt;1_{\mathbf Q}&lt;/math&gt; '''is not Riemann-integrable on'''  {{math|[0, 1]}}:  No matter how the set {{math|[0, 1]}} is partitioned into subintervals, each partition contains at least one rational and at least one irrational number, because rationals and irrationals are both dense in the reals.  Thus the upper [[Darboux sum]]s are all one, and the lower Darboux sums are all zero.
* &lt;math&gt;1_{\mathbf Q}&lt;/math&gt; '''is Lebesgue-integrable on ''' {{math|[0, 1]}} using the [[Lebesgue measure]]:  Indeed, it is the indicator function of the rationals so by definition

::&lt;math&gt; \int_{[0,1]} 1_{\mathbf Q} \, d \mu = \mu(\mathbf{Q} \cap [0,1]) = 0,&lt;/math&gt;

:because {{math|'''Q'''}} is [[countable]].

=== Domain of integration ===
A technical issue in Lebesgue integration is that the domain of integration is defined as a ''set'' (a subset of a measure space), with no notion of orientation. In elementary calculus, one defines integration with respect to an [[orientation (manifold)|orientation]]:
:&lt;math&gt;\int_b^a f := - \int_a^b f.&lt;/math&gt;
Generalizing this to higher dimensions yields integration of [[differential form]]s. By contrast, Lebesgue integration provides an alternative generalization, integrating over subsets with respect to a measure; this can be notated as
:&lt;math&gt;\int_A f\,d\mu = \int_{[a,b]} f\,d\mu&lt;/math&gt;
to indicate integration over a subset {{math|''A''}}. For details on the relation between these generalizations, see {{section link|Differential form|Relation with measures}}.

== Limitations of the Riemann integral ==
Here we discuss the limitations of the Riemann integral and the greater scope offered by the Lebesgue integral. This discussion presumes a working understanding of the [[Riemann integral]].

With the advent of [[Fourier series]], many analytical problems involving integrals came up whose satisfactory solution required interchanging limit processes and integral signs. However, the conditions under which the integrals

: &lt;math&gt; \sum_k \int f_k(x) dx,  \quad \int \left [\sum_k f_k(x) \right ] dx  &lt;/math&gt;

are equal proved quite elusive in the Riemann framework. There are some other technical difficulties with the Riemann integral. These are linked with the limit-taking difficulty discussed above.

'''Failure of monotone convergence'''. As shown above, the [[indicator function]] {{math|1&lt;sub&gt;'''Q'''&lt;/sub&gt;}} on the rationals is not Riemann integrable.  In particular, the [[Monotone convergence theorem]] fails. To see why, let {{math|{''a''&lt;sub&gt;''k''&lt;/sub&gt;}}} be an enumeration of all the rational numbers in {{math|[0, 1]}} (they are [[countable]] so this can be done.) Then let
:&lt;math&gt; g_k(x) = \left\{\begin{matrix} 1 &amp; \text{if }  x = a_j, j\leq k \\ 0 &amp; \text{otherwise} \end{matrix} \right. &lt;/math&gt;

The function {{math|''g''&lt;sub&gt;''k''&lt;/sub&gt;}} is zero everywhere, except on a finite set of points. Hence its Riemann integral is zero. Each {{math|''g''&lt;sub&gt;''k''&lt;/sub&gt;}} is non-negative, and this sequence of functions is monotonically increasing, but its limit as {{math|''k'' → ∞}} is {{math|1&lt;sub&gt;'''Q'''&lt;/sub&gt;}}, which is not Riemann integrable.

'''Unsuitability for unbounded intervals'''. The Riemann integral can only integrate functions on a bounded interval. It can however be extended to unbounded intervals by taking limits, so long as this doesn't yield an answer such as {{math|∞ − ∞}}.

'''Integrating on structures other than Euclidean space'''.  The Riemann integral is inextricably linked to the order structure of the real line.

== Basic theorems of the Lebesgue integral ==
The Lebesgue integral does not distinguish between functions that differ only on a set of μ-measure zero. To make this precise, functions {{math|''f''}} and {{math|''g''}} are said to be equal [[almost everywhere]] (a.e.) if

:&lt;math&gt; \mu(\{x \in E: f(x) \neq g(x)\}) = 0. &lt;/math&gt;

* If {{math|''f'', ''g''}} are non-negative measurable functions (possibly assuming the value {{math|+∞}}) such that {{math|''f'' {{=}} ''g''}} almost everywhere, then

:&lt;math&gt; \int f \, d \mu =  \int g \, d \mu. &lt;/math&gt;

To wit, the integral respects the equivalence relation of almost-everywhere equality.

* If {{math|''f'', ''g''}} are functions such that {{math|''f'' {{=}} ''g''}} almost everywhere, then {{math|''f''}} is Lebesgue integrable if and only if {{math|''g''}} is Lebesgue integrable, and the integrals of {{math|''f''}} and {{math|''g''}} are the same if they exist.

The Lebesgue integral has the following properties:

[[linear transformation|Linearity]]: If {{math|''f''}} and {{math|''g''}} are Lebesgue integrable functions and {{math|''a''}} and {{math|''b''}} are real numbers, then {{math|''af'' + ''bg''}} is Lebesgue integrable and

:&lt;math&gt; \int (a f + bg) \, d \mu = a \int f \, d\mu + b \int g \, d\mu. &lt;/math&gt;

[[Monotonic]]ity: If {{math|''f'' ≤ ''g''}}, then

:&lt;math&gt; \int f \, d \mu \leq  \int g \, d \mu. &lt;/math&gt;

[[Lebesgue's monotone convergence theorem|Monotone convergence theorem]]: Suppose {{math|{ ''f''&lt;sub&gt;''k''&lt;/sub&gt;}&lt;sub&gt;''k'' ∈ ℕ&lt;/sub&gt;}}  is a sequence of non-negative measurable functions such that

:&lt;math&gt;  f_k(x) \leq f_{k+1}(x) \quad \forall k\in \mathbb{N}, \, \forall x \in E. &lt;/math&gt;

Then, the pointwise limit {{math|''f''}} of {{math|''f''&lt;sub&gt;''k''&lt;/sub&gt;}} is Lebesgue measurable and

:&lt;math&gt; \lim_k \int f_k \, d\mu = \int f \, d \mu. &lt;/math&gt;

The value of any of the integrals is allowed to be infinite.

[[Fatou's lemma]]: If {{math|{ ''f''&lt;sub&gt;''k''&lt;/sub&gt;}&lt;sub&gt;''k'' ∈ '''N'''&lt;/sub&gt;}} is a sequence of non-negative measurable functions, then

:&lt;math&gt; \int \liminf_k f_k \, d \mu  \leq  \liminf_k \int f_k \, d \mu.&lt;/math&gt;

Again, the value of any of the integrals may be infinite.

[[Dominated convergence theorem]]: Suppose {{math|{ ''f''&lt;sub&gt;''k''&lt;/sub&gt;}&lt;sub&gt;''k'' ∈ '''N'''&lt;/sub&gt;}} is a sequence of complex measurable functions with pointwise limit {{math|''f''}}, and there is a Lebesgue integrable function {{math|''g''}} (i.e., {{math|''g''}} belongs to the {{math|[[Lp space|space ''L''&lt;sup&gt;1&lt;/sup&gt;]])}} such that {{math|{{!}} ''f''&lt;sub&gt;''k''&lt;/sub&gt; {{!}} ≤ ''g''}} for all {{math|''k''}}.

Then, {{math|''f''}} is Lebesgue integrable and

:&lt;math&gt; \lim_k \int f_k \, d \mu = \int f \, d \mu. &lt;/math&gt;

== Proof techniques ==
To illustrate some of the proof techniques used in Lebesgue integration theory, we sketch a proof of the above-mentioned Lebesgue monotone convergence theorem.  Let {{math|{ ''f''&lt;sub&gt;''k''&lt;/sub&gt;}&lt;sub&gt;''k'' ∈ '''N'''&lt;/sub&gt;}} be a non-decreasing sequence of non-negative measurable functions and put

:&lt;math&gt; f = \sup_{k \in \mathbb{N}} f_k = \lim_{k \in \mathbb{N}} f_k. &lt;/math&gt;

By the monotonicity property of the integral, it is immediate that:

: &lt;math&gt; \int f \, d\mu \geq \lim_k \int f_k \, d\mu &lt;/math&gt;

and the limit on the right exists, because the sequence is monotonic. We now prove the inequality in the other direction.  It follows from the definition of integral that there is a non-decreasing sequence {{math|(''g''&lt;sub&gt;''n''&lt;/sub&gt;)}} of non-negative simple functions such that {{math|''g''&lt;sub&gt;''n''&lt;/sub&gt; ≤ ''f'' }} and

:&lt;math&gt; \lim_n \int g_n \, d\mu = \int f \, d\mu. &lt;/math&gt;

Therefore,  it suffices to prove that for each {{math|''n'' ∈  ℕ}},

:&lt;math&gt;  \int g_n \, d\mu \leq \lim_k \int f_k \, d\mu. &lt;/math&gt;

We will show that if {{math|''g''}} is a simple function and

:&lt;math&gt; \lim_k f_k(x) \geq g(x) &lt;/math&gt;

almost everywhere, then

:&lt;math&gt; \lim_k \int f_k \, d\mu \geq \int g \, d\mu.&lt;/math&gt;

By breaking up the function {{math|''g''}} into its constant value parts, this reduces to the case in which {{math|''g''}} is the indicator function of a set. The result we have to prove is then

&lt;blockquote&gt;Suppose {{math|''A''}} is a measurable set and {{math|{ ''f''&lt;sub&gt;''k''&lt;/sub&gt;}&lt;sub&gt;''k'' ∈  ℕ&lt;/sub&gt;}} is a nondecreasing sequence of non-negative measurable functions on {{math|''E''}}  such that

:&lt;math&gt; \lim_k f_k (x) \geq 1 &lt;/math&gt;

for almost all {{math|''x'' ∈ ''A''}}. Then

:&lt;math&gt; \lim_k \int f_k \, d\mu \geq \mu(A). &lt;/math&gt;&lt;/blockquote&gt;

To prove this result, fix {{math|''ε'' &gt; 0}} and define the sequence of measurable sets

:&lt;math&gt; B_k = \{x \in A: f_k(x) \geq 1 - \varepsilon \}. &lt;/math&gt;

By monotonicity of the integral, it follows that for any {{math|''k'' ∈  ℕ}},

:&lt;math&gt; (1 - \varepsilon) \mu(B_k) = \int (1 - \varepsilon) 1_{B_k} \, d\mu \leq \int f_k \, d\mu &lt;/math&gt;

Because almost every {{math|''x''}} is in {{math|''B&lt;sub&gt;k&lt;/sub&gt;''}} for large enough {{math|''k''}}, we have

:&lt;math&gt; \bigcup_k B_k = A, &lt;/math&gt;

up to a set of measure {{math|0}}. Thus by countable additivity of {{math|''μ''}}, and because {{math|''B&lt;sub&gt;k&lt;/sub&gt;''}} increases with&amp;nbsp;{{math|''k''}},

:&lt;math&gt; \mu(A) = \lim_k \mu(B_k) \leq \lim_k (1 - \varepsilon)^{-1} \int f_k \, d\mu. &lt;/math&gt;

As this is true for any positive {{math|''ε''}} the result follows.

For another Proof of the Monotone Convergence Theorem, we follow:&lt;ref name="Folland" /&gt;

Let {{math|(''X'', ''M'', ''μ'')}} be a measure space.

{{math|{ ''f''&lt;sub&gt;''n''&lt;/sub&gt;}{{null}}}} is an increasing sequence of numbers, therefore its limit exists, even if it is equal to {{math|∞}}. We know that

:&lt;math&gt; \int f_n \leq \int f&lt;/math&gt;

for all {{math|''n''}}, so that

:&lt;math&gt; \lim \limits_{n \rightarrow \infty} \int f_n \leq \int f&lt;/math&gt;.

Now we need to establish the reverse inequality. Fix {{math|''α'' ∈ (0, 1)}}, let {{math|''ϕ''}} be a simple function with {{math|0 ≤ ''ϕ'' ≤ ''f''}} and let

:&lt;math&gt; E_n = \{x : f_n (x) \geq \alpha \phi(x)\} &lt;/math&gt;.

Then {{math|{E&lt;sub&gt;n&lt;/sub&gt;}{{null}}}} is an increasing sequence of measurable sets with &lt;math&gt; \bigcup \limits^\infty E_n = X&lt;/math&gt;. We know that

:&lt;math&gt; \int f_n \geq \int \limits_{E_n} f_n \geq \alpha \int \limits_{E_n} \phi &lt;/math&gt;.

This is true for all {{math|''n''}}, including the limit:

:&lt;math&gt; \lim \int \limits_{E_n} \phi = \int \phi&lt;/math&gt;.

Hence,

:&lt;math&gt; \lim \int f_n \geq \alpha \int \phi &lt;/math&gt;.

This was true for all {{math|''α'' ∈ (0, 1)}}, so it remains true for {{math|1=''α'' = 1}}, and taking the supremum over simple {{math|''ϕ'' ≤ ''f''}} by the definition of integration in {{math|''L''&lt;sup&gt;+&lt;/sup&gt;}},

:&lt;math&gt; \lim \int f_n \geq \int f&lt;/math&gt;.

Now we have both inequalities, so we've shown the Monotone Convergence theorem:

:&lt;math&gt; \lim \int f_n = \int f &lt;/math&gt;

for {{math|''f''&lt;sub&gt;{''n''+1}&lt;/sub&gt; ≥ ''f''&lt;sub&gt;''n''&lt;/sub&gt;}}, and {{math|''f''&lt;sub&gt;n&lt;/sub&gt; → ''f''}} pointwise, {{math|{''f''&lt;sub&gt;n&lt;/sub&gt;} ∈ ''L''&lt;sup&gt;+&lt;/sup&gt;}}, the set of positive measurable functions from {{math|''X'' → [0, ∞]}}.

== Alternative formulations ==

It is possible to develop the integral with respect to the Lebesgue measure without relying on the full machinery of measure theory. One such approach is provided by the [[Daniell integral]].

There is also an alternative approach to developing the theory of integration via methods of [[functional analysis]].  The Riemann integral exists for any continuous function {{math|''f''}} of [[compact space|compact]] [[support (mathematics)|support]] defined on {{math|ℝ&lt;sup&gt;''n''&lt;/sup&gt;}} (or a fixed open subset).  Integrals of more general functions can be built starting from these integrals.

Let {{math|''C&lt;sub&gt;c&lt;/sub&gt;''}} be the space of all real-valued compactly supported continuous functions of ℝ. Define a norm on {{math|''C&lt;sub&gt;c&lt;/sub&gt;''}} by

: &lt;math&gt; \left\| f \right\| = \int |f(x)| \, dx .&lt;/math&gt;

Then {{math|''C&lt;sub&gt;c&lt;/sub&gt;''}} is a normed vector space (and in particular, it is a metric space.) All metric spaces have [[complete space|Hausdorff completions]], so let {{math|''L''&lt;sup&gt;1&lt;/sup&gt;}} be its completion. This space is isomorphic to the space of Lebesgue integrable functions modulo the subspace of functions with integral zero. Furthermore, the Riemann integral {{math|∫}} is a [[uniformly continuous]] functional with respect to the norm on {{math|''C&lt;sub&gt;c&lt;/sub&gt;''}}, which is dense in {{math|''L''&lt;sup&gt;1&lt;/sup&gt;}}. Hence {{math|∫}} has a unique extension to all of {{math|''L''&lt;sup&gt;1&lt;/sup&gt;}}. This integral is precisely the Lebesgue integral.

More generally, when the measure space on which the functions are defined is also a [[Locally compact space|locally compact]] [[topological space]] (as is the case with the real numbers ℝ), measures compatible with the topology in a suitable sense ([[Radon measure]]s, of which the Lebesgue measure is an example) an integral with respect to them can be defined in the same manner, starting from the integrals of [[continuous function]]s with [[compact support]]. More precisely, the compactly supported functions form a [[vector space]] that carries a natural [[topological space|topology]], and a (Radon) measure is defined as a continuous [[linear map|linear]] functional on this space.  The value of a measure at a compactly supported function is then also by definition the integral of the function. One then proceeds to expand the measure (the integral) to more general functions by continuity, and defines the measure of a set as the integral of its indicator function. This is the approach taken by {{Harvtxt|Bourbaki|2004}} and a certain number of other authors. For details see [[Radon measure#Radon measures on locally compact spaces|Radon measures]].

== Limitations of Lebesgue integral ==
The main purpose of Lebesgue integral is to provide an integral notion where limits of integrals hold under mild assumptions. There is no guarantee that every function is Lebesgue integrable. But it may happen that [[improper integral]]s exist for functions that are not Lebesgue integrable. One example would be
:&lt;math&gt;\frac{\sin(x)}{x}&lt;/math&gt;
over the entire real line. This function is not Lebesgue integrable, as
:&lt;math&gt; \int_{-\infty}^\infty \left|\frac{\sin(x)}{x}\right| dx =\infty.&lt;/math&gt;
On the other hand, &lt;math&gt; \int_{-\infty}^\infty\frac{\sin(x)}{x} dx&lt;/math&gt; exists as an improper integral and can be computed to be finite; it is twice the [[Dirichlet integral]].

== See also ==
* [[Henri Lebesgue#Lebesgue's theory of integration|Henri Lebesgue]], for a non-technical description of Lebesgue integration
* [[Null set]]
* [[integral|Integration]]
* [[measure (mathematics)|Measure]]
* [[Sigma-algebra]]
* [[Lebesgue space (disambiguation)|Lebesgue space]]
* [[Lebesgue–Stieltjes integration]]
* [[Henstock–Kurzweil integral]]

== Notes ==
{{reflist}}

== References ==

* {{cite book
| last = Bartle
| first = Robert G.
| title = The elements of integration and Lebesgue measure
| series = Wiley Classics Library
| publisher = John Wiley &amp;amp; Sons Inc.
| location = New York
| year = 1995
| pages = xii+179
| isbn = 0-471-04222-6
| nopp = true
| mr = 1312157}}

* {{cite book
| last = Bauer
| first = Heinz
| title = Measure and Integration Theory
| series = De Gruyter Studies in Mathematics 26
| publisher = De Gruyter
| location = Berlin
| year = 2001
| page = 236
| isbn = 978-3-11-016719-1
| nopp = true}}

* {{cite book
| last = Bourbaki
| first = Nicolas
| authorlink = Nicolas Bourbaki
| title = Integration. I. Chapters 1–6. Translated from the 1959, 1965 and 1967 French originals by Sterling K. Berberian
| series = Elements of Mathematics (Berlin)
| publisher= Springer-Verlag
| location = Berlin
| year = 2004
| pages = xvi+472
| isbn = 3-540-41129-1
| nopp = true
| mr = 2018901}}

* {{cite book
| last = Dudley
| first = Richard M.
| title = Real analysis and probability
| series = The Wadsworth &amp;amp; Brooks/Cole Mathematics Series
| publisher = Wadsworth &amp;amp; Brooks/Cole Advanced Books &amp;amp; Software
| location = Pacific Grove, CA
| year = 1989
| pages = xii+436
| isbn = 0-534-10050-3
| nopp = true
| mr = 982264}} Very thorough treatment, particularly for probabilists with good notes and historical references.

* {{cite book
| last = Folland
| first = Gerald B.
| title = Real analysis: Modern techniques and their applications
| series = Pure and Applied Mathematics (New York)
| edition = Second
| publisher = John Wiley &amp;amp; Sons Inc.
| location = New York
| year = 1999
| pages = xvi+386
| isbn = 0-471-31716-0
| nopp = true
| mr = 1681462}}

* {{cite book
| last = Halmos
| first = Paul R.
| authorlink = Paul Halmos
| title = Measure Theory
| publisher = D. Van Nostrand Company, Inc.
| location = New York, N. Y.
| year = 1950
| pages = xi+304
| mr = 0033869}} A classic, though somewhat dated presentation.

* {{springer|title=Lebesgue integral|id=p/l057860}}
* {{Cite journal
| last = Lebesgue
| first = Henri
| authorlink = Henri Lebesgue
| title = Leçons sur l'intégration et la recherche des fonctions primitives
| publisher = Gauthier-Villars
| year = 1904
| publication-place = Paris
| postscript = &lt;!--None--&gt;
| ref = harv}}

* {{cite book
| last = Lebesgue
| first = Henri
| authorlink = Henri Lebesgue
| title = Oeuvres scientifiques (en cinq volumes)
| publisher = Institut de Mathématiques de l'Université de Genève
| location = Geneva
| year = 1972
| page = 405
| language = French
| mr = 0389523}}

* {{cite book|last1=Lieb|first1=Elliott|authorlink1=Elliott H. Lieb|last2=Loss|first2=Michael|author2-link=Michael Loss|title=Analysis|year=2001|edition=2nd|publisher=[[American Mathematical Society]]|series=[[Graduate Studies in Mathematics]]|volume=14|isbn=978-0821827833}}
* {{cite book
| last = Loomis
| first = Lynn H.
| title = An introduction to abstract harmonic analysis
| publisher = D. Van Nostrand Company, Inc.
| location = Toronto-New York-London
| year = 1953
| pages = x+190
| mr = 0054173}} Includes a presentation of the Daniell integral.

* {{cite book
| last = Munroe
| first =  M. E.
| title = Introduction to measure and integration
| publisher = Addison-Wesley Publishing Company Inc.
| location = Cambridge, Mass.
| year = 1953
| pages = x+310
| mr = 0053186}} Good treatment of the theory of outer measures.

* {{cite book
| last = Royden
| first = H. L.
| title = Real analysis
| edition = Third
| publisher = Macmillan Publishing Company
| location = New York
| year = 1988
| pages = xx+444
| isbn = 0-02-404151-3
| mr = 1013117}}

* {{cite book
| last = Rudin
| first = Walter
| authorlink = Walter Rudin
| title = Principles of mathematical analysis
| edition = Third
| series = International Series in Pure and Applied Mathematics
| publisher = McGraw-Hill Book Co.
| location = New York
| year = 1976
| pages = x+342
| mr = 0385023}}  Known as ''Little Rudin'', contains the basics of the Lebesgue theory, but does not treat material such as [[Fubini's theorem]].

* {{cite book
| last = Rudin
| first = Walter
| title = Real and complex analysis
| publisher = McGraw-Hill Book Co.
| location = New York
| year = 1966
| pages = xi+412
| mr = 0210528}} Known as ''Big Rudin''. A complete and careful presentation of the theory.  Good presentation of the Riesz extension theorems. However, there is a minor flaw (in the first edition) in the proof of one of the extension theorems, the discovery of which constitutes exercise 21 of Chapter 2.
* {{Cite journal
| last = Saks
| first = Stanisław
| author-link = Stanislaw Saks
| title = Theory of the Integral
| place = [[Warszawa]]-[[Lwów]]
| publisher = G.E. Stechert &amp; Co.
| year = 1937
| series = [http://matwbn.icm.edu.pl/ksspis.php?wyd=10&amp;jez=pl Monografie Matematyczne]
| volume = 7
| edition = 2nd
| pages = VI+347
| url = https://archive.org/details/theoryoftheinteg032192mbp
| jfm = 63.0183.05 | zbl = 0017.30004
| postscript = &lt;!--None--&gt;}}. English translation by [[Laurence Chisholm Young]], with two additional notes by [[Stefan Banach]].

* {{cite book
| last = Shilov
| first = G. E.
| last2 = Gurevich
| first2 = B. L.
| title = Integral, measure and derivative: a unified approach. Translated from the Russian and edited by Richard A. Silverman
| series = Dover Books on Advanced Mathematics
| publisher = Dover Publications Inc.
| location = New York
| year = 1977
| pages = xiv+233
| isbn = 0-486-63519-8
| nopp = true
| mr = 0466463}} Emphasizes the [[Daniell integral]].

* {{citation|last=Siegmund-Schultze|first=Reinhard|chapter=Henri Lebesgue|title=Princeton Companion to Mathematics|editors=Timothy Gowers, June Barrow-Green, Imre Leader|year=2008|publisher=Princeton University Press}}.
* {{cite book
| last = Teschl
| first = Gerald
| authorlink = Gerald Teschl
| title = Topics in Real and Functional Analysis
| publisher = (lecture notes)
| url = http://www.mat.univie.ac.at/~gerald/ftp/book-fa/index.html}}

* {{cite book
| last = Yeh
| first = James
| title = Real Analysis: Theory of Measure and Integral 2nd. Edition Paperback
| publisher = World Scientific Publishing Company Pte. Ltd.
| location = Singapore
| year =2006
| page = 760
| isbn = 978-981-256-6}}

{{Integral}}

{{Authority control}}

[[Category:Definitions of mathematical integration]]
[[Category:Measure theory]]</text>
      <sha1>526wzekhme1j5j0bvxx0gk6dcb40hrv</sha1>
    </revision>
  </page>
  <page>
    <title>Lecture Notes in Mathematics</title>
    <ns>0</ns>
    <id>47931235</id>
    <revision>
      <id>849477760</id>
      <parentid>842811573</parentid>
      <timestamp>2018-07-09T08:45:37Z</timestamp>
      <contributor>
        <ip>126.114.83.232</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1895">{{italic title}}
'''''Lecture Notes in Mathematics''''' ('''LNM''', {{ISSN|0075-8434}}) is a [[book series]] in the field of [[mathematics]], including articles related to both [[research]] and [[teaching]].  It was established in 1964 and was edited by A. Dold, Heidelberg and B. Eckmann, Zürich. Its publisher is [[Springer Science+Business Media]] (formerly Springer-Verlag).

The intent of the series is to publish not only lecture notes, but results from seminars and conferences, more quickly than the several-years-long process of publishing polished journal papers in mathematics. In order to speed the publication process, early volumes of the series (before [[electronic publishing]]) were reproduced photographically from [[typewriter|typewritten]] manuscripts. According to Earl Taft it has been "enormously successful" and "is considered a very valuable service to the mathematical community".{{r|taft}}

{{as of|2018}} there have been 2232 volumes in this series.&lt;ref&gt;{{Official|https://link.springer.com/bookseries/304}}&lt;/ref&gt;

==See also==
* ''[[Lecture Notes in Physics]]''
* ''[[Lecture Notes in Computer Science]]''

==References==
{{reflist|refs=

&lt;ref name=taft&gt;{{citation
 | last = Taft | first = Earl J.
 | editor-last = Balaban | editor-first = Miriam | editor-link = Miriam Balaban
 | contribution = Editing a photographically reproduced journal
 | doi = 10.1007/978-94-009-9863-6_4
 | pages = 25–27
 | publisher = Springer
 | title = Scientific Information Transfer: The Editor's Role (Proceedings of the First International Conference of Scientific Editors, April 24–29, 1977, Jerusalem)
 | year = 1978}}. See in particular [https://books.google.com/books?id=7_bwCAAAQBAJ&amp;pg=PA26 p. 26].&lt;/ref&gt;

}}

[[Category:Publications established in 1964]]
[[Category:Series of mathematics books]]
[[Category:Springer Science+Business Media books]]

{{mathematics-book-stub}}</text>
      <sha1>t9qp4i1yy5bebb5m5045tm1xw27ifrn</sha1>
    </revision>
  </page>
  <page>
    <title>Leslie Fox Prize for Numerical Analysis</title>
    <ns>0</ns>
    <id>15366214</id>
    <revision>
      <id>841513145</id>
      <parentid>841512914</parentid>
      <timestamp>2018-05-16T09:10:13Z</timestamp>
      <contributor>
        <username>S k wilson</username>
        <id>33179669</id>
      </contributor>
      <minor/>
      <comment>Very minor formatting edits</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="4741">The '''Leslie Fox Prize for Numerical Analysis''' of the [[Institute of Mathematics and its Applications]] (IMA) is a biennial prize established in 1985 by the IMA in honour of mathematician [[Leslie Fox]] (1918-1992). The prize honours "young numerical analysts worldwide" (any person who is less than 31 years old), and applicants submit papers for review. A committee reviews the papers, invites shortlisted candidates to give lectures at the Leslie Fox Prize meeting, and then awards First Prize and Second Prizes based on "mathematical and algorithmic brilliance in tandem with presentational skills."&lt;ref name="LFP-2005"&gt;[http://202.38.126.65/mirror/www.ima.org.uk/mathematics/Report12LeslieFoxPrizeMeet.htm Report on the 12th Leslie Fox Prize Meeting] {{webarchive|url=https://web.archive.org/web/20090614150902/http://202.38.126.65/mirror/www.ima.org.uk/mathematics/Report12LeslieFoxPrizeMeet.htm |date=2009-06-14 }}, University of Dundee, 27 June 2005.&lt;/ref&gt;

==Prize winners list==
Source: [http://www.ima.org.uk/about_us/awards_and_medals/ima_leslie_fox_prize/previous_winners.html Institute of Mathematics and its Applications]
* 1985 - [[Lloyd N. Trefethen]] (inaugural prize winner)
* 1986 - [[J. W. Demmel]] and [[N. I. M. Gould]]&lt;ref name="Fox-13"&gt;[http://users.comlab.ox.ac.uk/endre.suli/fox/ 13th Leslie Fox Prize in Numerical Analysis] {{webarchive|url=https://web.archive.org/web/20110522115314/http://users.comlab.ox.ac.uk/endre.suli/fox/ |date=2011-05-22 }}, Oxford, 2007.&lt;/ref&gt;
* 1988 - [[Nicholas J. Higham]]&lt;ref name="Fox-13" /&gt;
* 1989 - 3 first prizes: [[Martin Buhmann]] ("Multivariable cardinal interpolation with radial basis functions"), [[Bart De Moor]] ("The restrictricted singular value decomposition: properties and applications"), [[Andrew M. Stuart]] ("Linear instability implies spurious periodic solutions")&lt;ref&gt;[http://www.netlib.org/na-digest-html/89/v89n39.html "1989 Fox Prize Meeting"], NA Digest, Oct. 8, 1989, v.89, n.39.&lt;/ref&gt;
* 1991 - [[Christopher Budd (mathematician)|Christopher Budd]] and [[J. F. B. M. Kraaijevanger]]&lt;ref name="Fox-13" /&gt;
* 1993 - [[Yuying Li]]&lt;ref name="Fox-13" /&gt;&lt;ref&gt;[http://www.cs.cornell.edu/Info/People/yuying/RESUME/cv.ps Yuying Li, Resume on Cornell website confirming given name]&lt;/ref&gt;
* 1995 - Adrian Hill&lt;ref name="Fox-13" /&gt;&lt;ref&gt;[http://www.bath.ac.uk/pip/directory/profile/1970 Hill's profile at Bath] {{webarchive|url=https://web.archive.org/web/20080119122005/http://www.bath.ac.uk/pip/directory/profile/1970 |date=2008-01-19 }}&lt;/ref&gt;
* 1997 - [[Wim Sweldens]], ("The Lifting Scheme: A Construction of Second Generation Wavelets")&lt;ref&gt;[http://www.netlib.org/na-digest-html/97/v97n26.html "Announcement of Fox Prize Winners"], NA Digest, v.97, n.65 (1997, 8th prize)&lt;/ref&gt;
* 1999 - [[Niles Pierce]] and [[Reha Tütüncü]]&lt;ref name="Fox-13" /&gt;
* 2001 - [[Anna-Karin Tornberg]]&lt;ref name="Fox-13" /&gt;
* 2003 - [[Jared Tanner]]&lt;ref name="Fox-13" /&gt;
* 2005 - [[Roland Opfer]] and [[Paul Tupper]]&lt;ref name="LFP-2005" /&gt;
* 2007 - [[Yoichiro Mori]]  and [[Ioana Dumitriu]]&lt;ref name="Fox-13" /&gt;
* 2009 - [[Brian Sutton (mathematician)|Brian Sutton]]&lt;ref&gt;[http://www.rmc.edu/News/09-06-30%20-%20Sutton%20Wins%20Fox%20Prize.aspx Professor Awarded the 14th Leslie Fox Prize in Numerical Analysis, 2008 Randolph-Macon College] {{webarchive|url=https://archive.is/20120805144948/http://www.rmc.edu/News/09-06-30%20-%20Sutton%20Wins%20Fox%20Prize.aspx |date=2012-08-05 }}&lt;/ref&gt;&lt;ref&gt;[http://www.warwick.ac.uk/~masdr/fox/ 14th Leslie Fox Prize, 2009 University of Warwick] {{webarchive|url=https://web.archive.org/web/20091214051658/http://www.warwick.ac.uk/~masdr/fox/ |date=2009-12-14 }}&lt;/ref&gt;
* 2011 - [[Yuji Nakatsukasa (mathematician)|Yuji Nakatsukasa]] &lt;ref&gt;[http://www.mims.manchester.ac.uk/events/workshops/FOX2011/winners.php Past Leslie Fox Prize Winners]&lt;/ref&gt;
* 2013 - [[ Michael Neilan]] &lt;ref&gt; {{cite web|url = http://www.ima.org.uk/about_us/awards_and_medals/ima_leslie_fox_prize/previous_winners.html|title= IMA Leslie Fox Prize for Numerical Analysis Winners|publisher= IMA|accessdate = 28 November 2014}} &lt;/ref&gt;
* 2015 - [[Iain Smears]] and [[Alex Townsend]] &lt;ref&gt; {{cite web|url = http://www.numerical.rl.ac.uk/people/nimg/fox/ | title = 17th IMA Leslie Fox Prize for Numerical Analysis 2015}} &lt;/ref&gt;
* 2017 - [[Nicole Spillane]] &lt;ref&gt; {{cite web|url = http://people.maths.ox.ac.uk/wathen/fox/prize.php | title = 18th IMA Leslie Fox Prize for Numerical Analysis 2017}} &lt;/ref&gt;

==References==
{{reflist}}

{{DEFAULTSORT:Fox Prize For Numerical Analysis}}
[[Category:Mathematics awards]]
[[Category:Early career awards]]
[[Category:Awards established in 1985]]
[[Category:British science and technology awards]]
[[Category:Awards for scholarly publications]]</text>
      <sha1>hx2kkag8xlo8kmudnyrnumn0dh4x86g</sha1>
    </revision>
  </page>
  <page>
    <title>List of space groups</title>
    <ns>0</ns>
    <id>11022628</id>
    <revision>
      <id>871673662</id>
      <parentid>871242367</parentid>
      <timestamp>2018-12-02T18:28:54Z</timestamp>
      <contributor>
        <username>Bor75</username>
        <id>11857645</id>
      </contributor>
      <comment>Based on International Tables for Crystallography, symbols Cs should be used. S1 is not used. Only S4 and S6</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="56066">There are 230 [[Space group#Table of space groups in 3 dimensions|space groups]] in three dimensions, given by a number index, and a full name in [[Hermann–Mauguin notation]], and a short name (international short symbol). The long names are given with spaces for readability. The groups each have a [[point groups]] of the unit cell.

== Symbols==
In [[Hermann–Mauguin notation]], space groups are named by a symbol combining the [[point group]] identifier with the uppercase letters describing the [[Bravais lattice#Bravais lattices in 3 dimensions|lattice type]]. Translations within the lattice in the form of [[screw axes]] and [[glide planes]] are also noted, giving a complete crystallographic space group.

These are the [[Bravais lattices]] in three dimensions:
*'''P''' primitive
*'''I''' body centered (from the German "Innenzentriert")
*'''F''' face centered (from the German "Flächenzentriert")
*'''A''' centered on A faces only
*'''B''' centered on B faces only
*'''C''' centered on C faces only
*'''R''' rhombohedral

A reflection plane '''m''' within the point groups can be replaced by a [[glide plane]], labeled as '''a''', '''b''', or '''c''' depending on which axis the glide is along. There is also the '''n''' glide, which is a glide along the half of a diagonal of '''a''' face, and the '''d''' glide, which is along a quarter of either a face or space diagonal of the unit cell. The '''d''' glide is often called the diamond glide plane as it features in the [[diamond]] structure.
*&lt;math&gt;a&lt;/math&gt;, &lt;math&gt;b&lt;/math&gt;, or &lt;math&gt;c&lt;/math&gt; glide translation along half the lattice vector of this face
*&lt;math&gt;n&lt;/math&gt; glide translation along with half a face diagonal
*&lt;math&gt;d&lt;/math&gt; glide planes with translation along a quarter of a face diagonal.
*&lt;math&gt;e&lt;/math&gt; two glides with the same glide plane and translation along two (different) half-lattice vectors.

A gyration point can be replaced by a [[screw axis]] denoted by a number, ''n'', where the angle of rotation is  &lt;math&gt;\color{Black}\tfrac{360^\circ}{n}&lt;/math&gt;. The degree of translation is then added as a subscript showing how far along the axis the translation is, as a portion of the parallel lattice vector.  For example, 2&lt;sub&gt;1&lt;/sub&gt; is a 180° (twofold) rotation followed by a translation of ½ of the lattice vector. 3&lt;sub&gt;1&lt;/sub&gt; is a 120° (threefold) rotation followed by a translation of ⅓ of the lattice vector.

The possible screw axes are: 2&lt;sub&gt;1&lt;/sub&gt;, 3&lt;sub&gt;1&lt;/sub&gt;, 3&lt;sub&gt;2&lt;/sub&gt;, 4&lt;sub&gt;1&lt;/sub&gt;, 4&lt;sub&gt;2&lt;/sub&gt;, 4&lt;sub&gt;3&lt;/sub&gt;, 6&lt;sub&gt;1&lt;/sub&gt;, 6&lt;sub&gt;2&lt;/sub&gt;, 6&lt;sub&gt;3&lt;/sub&gt;, 6&lt;sub&gt;4&lt;/sub&gt;, and 6&lt;sub&gt;5&lt;/sub&gt;.

In '''Schoenflies notation''', the symbol of a space group is represented by the symbol of corresponding point group with additional superscript. The superscript doesn't give any additional information about symmetry elements of the space group, but is instead related to the order in which Schoenflies derived the space groups.

In '''Fedorov symbol''', the type of space group is denoted as ''s'' (''symmorphic'' ), ''h'' (''hemisymmorphic''), or ''a'' (''asymmorphic''). The number is related to the order in which Fedorov derived space groups. There are 73 symmorphic, 54 hemisymmorphic, and 103 asymmorphic space groups. Symmorphic space groups can be obtained as combination of Bravais lattices with corresponding point group. These groups contain the same symmetry elements as the corresponding point groups. Hemisymmorphic space groups contain only axial combination of symmetry elements from the corresponding point groups. All the other space groups are asymmorphic. Example for point group 4/mmm (&lt;math&gt;\tfrac{4}{m}\tfrac{2}{m}\tfrac{2}{m}&lt;/math&gt;): the symmorphic space groups are P4/mmm (&lt;math&gt;P\tfrac{4}{m}\tfrac{2}{m}\tfrac{2}{m}&lt;/math&gt;, ''36s'') and I4/mmm (&lt;math&gt;I\tfrac{4}{m}\tfrac{2}{m}\tfrac{2}{m}&lt;/math&gt;, ''37s''); hemisymmorphic space groups should contain axial combination 422, these are P4/mcc (&lt;math&gt;P\tfrac{4}{m}\tfrac{2}{c}\tfrac{2}{c}&lt;/math&gt;, ''35h''), P4/nbm (&lt;math&gt;P\tfrac{4}{n}\tfrac{2}{b}\tfrac{2}{m}&lt;/math&gt;, ''36h''), P4/nnc (&lt;math&gt;P\tfrac{4}{n}\tfrac{2}{n}\tfrac{2}{c}&lt;/math&gt;, ''37h''), and I4/mcm (&lt;math&gt;I\tfrac{4}{m}\tfrac{2}{c}\tfrac{2}{m}&lt;/math&gt;, ''38h'').

==List of Triclinic==
{| class=wikitable align=right
|+ style="width: 150pt;"| Triclinic Bravais lattice
|- style="text-align: center;width: 150pt;"
|[[image:Triclinic.svg|80px]]
|}

{| class=wikitable
|+ [[Triclinic crystal system]]
!Number
![[Point group]]
![[Orbifold notation|Orbifold]]
!Short name
!Full name
![[Schoenflies notation|Schoenflies]]
![[Evgraf Fedorov|Fedorov]]
!Shubnikov
![[Fibrifold notation|Fibrifold]]
|- align=center
|1||1||&lt;math&gt;1&lt;/math&gt;||P1|| P 1|| &lt;math&gt;C_1^1&lt;/math&gt; || ''1s''||&lt;math&gt;(a/b/c)\cdot 1&lt;/math&gt; || &lt;math&gt;(\circ)&lt;/math&gt;
|- align=center
|2||{{overline|1}}||&lt;math&gt;\times&lt;/math&gt;||P{{overline|1}}|| P {{overline|1}}|| &lt;math&gt;C_i^1&lt;/math&gt; || ''2s''||&lt;math&gt;(a/b/c)\cdot \tilde 2&lt;/math&gt; || &lt;math&gt;(2222)&lt;/math&gt;
|}

==List of Monoclinic==
{| class=wikitable align=right
|+ Monoclinic Bravais lattice
|-
!Simple&lt;BR&gt;(P)
!Base&lt;BR&gt;(C)
|-
|[[image:Monoclinic.svg|80px]]
|[[image:Monoclinic-base-centered.svg|80px]]
|}

{| class=wikitable
|+ [[Monoclinic crystal system]]
!Number
![[Point group]]
![[Orbifold notation|Orbifold]]
!Short name
!colspan=2|Full name(s)
![[Schoenflies notation|Schoenflies]]
![[Evgraf Fedorov|Fedorov]]
!Shubnikov
![[Fibrifold notation|Fibrifold]] (primary)
![[Fibrifold notation|Fibrifold]] (secondary)
|- align=center
|3||rowspan=3|2||rowspan=3|&lt;math&gt;22&lt;/math&gt;||P2|| P 1 2 1||P 1 1 2 || &lt;math&gt;C_2^1&lt;/math&gt; || ''3s'' || &lt;math&gt;(b:(c/a)):2&lt;/math&gt; || &lt;math&gt;(2_02_02_02_0)&lt;/math&gt; || &lt;math&gt;({*}_0{*}_0)&lt;/math&gt;
|- align=center
|4||P2&lt;sub&gt;1&lt;/sub&gt;||P 1 2&lt;sub&gt;1&lt;/sub&gt; 1||P 1 1 2&lt;sub&gt;1&lt;/sub&gt; || &lt;math&gt;C_2^2&lt;/math&gt; || ''1a'' || &lt;math&gt;(b:(c/a)):2_1&lt;/math&gt; || &lt;math&gt;(2_12_12_12_1)&lt;/math&gt; || &lt;math&gt;(\bar{\times}\bar{\times})&lt;/math&gt;
|- align=center
|5||C2|| C 1 2 1||B 1 1 2 || &lt;math&gt;C_2^3&lt;/math&gt; || ''4s'' || &lt;math&gt;\left ( \tfrac{a+b}{2}/b:(c/a)\right ) :2&lt;/math&gt; || &lt;math&gt;(2_02_02_12_1)&lt;/math&gt; || &lt;math&gt;({*}_1{*}_1)&lt;/math&gt;, &lt;math&gt;({*}\bar{\times})&lt;/math&gt;
|- align=center
|6||rowspan=4|m||rowspan=4|&lt;math&gt;*&lt;/math&gt;||Pm|| P 1 m 1||P 1 1 m || &lt;math&gt;C_s^1&lt;/math&gt; || ''5s'' || &lt;math&gt;(b:(c/a))\cdot m&lt;/math&gt; || &lt;math&gt;[\circ_0]&lt;/math&gt; || &lt;math&gt;({*}{\cdot}{*}{\cdot})&lt;/math&gt;
|- align=center
|7||Pc|| P 1 c 1||P 1 1 b || &lt;math&gt;C_s^2&lt;/math&gt; || ''1h'' || &lt;math&gt;(b:(c/a))\cdot \tilde c&lt;/math&gt; || &lt;math&gt;(\bar\circ_0)&lt;/math&gt; || &lt;math&gt;({*}{:}{*}{:})&lt;/math&gt;, &lt;math&gt;({\times}{\times}_0)&lt;/math&gt;
|- align=center
|8||Cm|| C 1 m 1||B 1 1 m || &lt;math&gt;C_s^3&lt;/math&gt; || ''6s'' || &lt;math&gt;\left ( \tfrac{a+b}{2}/b:(c/a)\right ) \cdot m&lt;/math&gt; || &lt;math&gt;[\circ_1]&lt;/math&gt; || &lt;math&gt;({*}{\cdot}{*}{:})&lt;/math&gt;, &lt;math&gt;({*}{\cdot}{\times})&lt;/math&gt;
|- align=center
|9||Cc|| C 1 c 1||B 1 1 b || &lt;math&gt;C_s^4&lt;/math&gt; || ''2h'' || &lt;math&gt;\left ( \tfrac{a+b}{2}/b:(c/a)\right ) \cdot \tilde c&lt;/math&gt; || &lt;math&gt;(\bar\circ_1)&lt;/math&gt; || &lt;math&gt;({*}{:}{\times})&lt;/math&gt;, &lt;math&gt;({\times}{\times}_1)&lt;/math&gt;
|- align=center
|10||rowspan=6|2/m||rowspan=6|&lt;math&gt;2*&lt;/math&gt;||P2/m||P 1 2/m 1||P 1 1 2/m || &lt;math&gt;C_{2h}^1&lt;/math&gt; || ''7s'' || &lt;math&gt;(b:(c/a))\cdot m:2&lt;/math&gt; || &lt;math&gt;[2_02_02_02_0]&lt;/math&gt; || &lt;math&gt;[*2{\cdot}22{\cdot}2)&lt;/math&gt;
|- align=center
|11||P2&lt;sub&gt;1&lt;/sub&gt;/m||P 1 2&lt;sub&gt;1&lt;/sub&gt;/m 1||P 1 1 2&lt;sub&gt;1&lt;/sub&gt;/m || &lt;math&gt;C_{2h}^2&lt;/math&gt; || ''2a'' || &lt;math&gt;(b:(c/a))\cdot m:2_1&lt;/math&gt; || &lt;math&gt;[2_12_12_12_1]&lt;/math&gt; || &lt;math&gt;(22{*}{\cdot})&lt;/math&gt;
|- align=center
|12||C2/m||C 1 2/m 1||B 1 1 2/m || &lt;math&gt;C_{2h}^3&lt;/math&gt; || ''8s'' || &lt;math&gt;\left ( \tfrac{a+b}{2}/b:(c/a)\right ) \cdot m:2&lt;/math&gt; || &lt;math&gt;[2_02_02_12_1]&lt;/math&gt; || &lt;math&gt;(*2{\cdot}22{:}2)&lt;/math&gt;, &lt;math&gt;(2\bar{*}2{\cdot}2)&lt;/math&gt;
|- align=center
|13||P2/c||P 1 2/c 1||P 1 1 2/b || &lt;math&gt;C_{2h}^4&lt;/math&gt; || ''3h'' || &lt;math&gt;(b:(c/a))\cdot \tilde c:2&lt;/math&gt; || &lt;math&gt;(2_02_022)&lt;/math&gt; || &lt;math&gt;(*2{:}22{:}2)&lt;/math&gt;, &lt;math&gt;(22{*}_0)&lt;/math&gt;
|- align=center
|14||P2&lt;sub&gt;1&lt;/sub&gt;/c||P 1 2&lt;sub&gt;1&lt;/sub&gt;/c 1||P 1 1 2&lt;sub&gt;1&lt;/sub&gt;/b || &lt;math&gt;C_{2h}^5&lt;/math&gt; || ''3a'' || &lt;math&gt;(b:(c/a))\cdot \tilde c:2_1&lt;/math&gt; || &lt;math&gt;(2_12_122)&lt;/math&gt; || &lt;math&gt;(22{*}{:})&lt;/math&gt;, &lt;math&gt;(22{\times})&lt;/math&gt;
|- align=center
|15||C2/c||C 1 2/c 1||B 1 1 2/b || &lt;math&gt;C_{2h}^6&lt;/math&gt; || ''4h'' || &lt;math&gt;\left ( \tfrac{a+b}{2}/b:(c/a)\right ) \cdot \tilde c:2&lt;/math&gt; || &lt;math&gt;(2_02_122)&lt;/math&gt; || &lt;math&gt;(2\bar{*}2{:}2)&lt;/math&gt;, &lt;math&gt;(22{*}_1)&lt;/math&gt;
|}

==List of Orthorhombic==
{| class=wikitable
|+ [[Orthorhombic crystal system]]
!Number
![[Point group]]
![[Orbifold notation|Orbifold]]
!Short name
!Full name
![[Schoenflies notation|Schoenflies]]
![[Evgraf Fedorov|Fedorov]]
!Shubnikov
![[Fibrifold notation|Fibrifold]] (primary)
![[Fibrifold notation|Fibrifold]] (secondary)
|- align=center
|16||rowspan=9|222||rowspan=9|&lt;math&gt;222&lt;/math&gt;||P222||P 2 2 2|| &lt;math&gt;D_2^1&lt;/math&gt; || ''9s'' || &lt;math&gt;(c:a:b):2:2&lt;/math&gt; || &lt;math&gt;(*2_02_02_02_0)&lt;/math&gt; ||
|- align=center
|17||P222&lt;sub&gt;1&lt;/sub&gt;||P 2 2 2&lt;sub&gt;1&lt;/sub&gt;|| &lt;math&gt;D_2^2&lt;/math&gt; || ''4a'' || &lt;math&gt;(c:a:b):2_1:2&lt;/math&gt; || &lt;math&gt;(*2_12_12_12_1)&lt;/math&gt; || &lt;math&gt;(2_02_0{*})&lt;/math&gt;
|- align=center
|18||P2&lt;sub&gt;1&lt;/sub&gt;2&lt;sub&gt;1&lt;/sub&gt;2||P 2&lt;sub&gt;1&lt;/sub&gt; 2&lt;sub&gt;1&lt;/sub&gt; 2|| &lt;math&gt;D_2^3&lt;/math&gt; || ''7a'' ||&lt;math&gt;(c:a:b):2&lt;/math&gt; [[Image:Circled_colon.png|16px]] &lt;math&gt;2_1&lt;/math&gt; || &lt;math&gt;(2_02_0\bar{\times})&lt;/math&gt; || &lt;math&gt;(2_12_1{*})&lt;/math&gt;
|- align=center
|19||P2&lt;sub&gt;1&lt;/sub&gt;2&lt;sub&gt;1&lt;/sub&gt;2&lt;sub&gt;1&lt;/sub&gt;||P 2&lt;sub&gt;1&lt;/sub&gt; 2&lt;sub&gt;1&lt;/sub&gt; 2&lt;sub&gt;1&lt;/sub&gt;|| &lt;math&gt;D_2^4&lt;/math&gt; || ''8a'' || &lt;math&gt;(c:a:b):2_1&lt;/math&gt; [[Image:Circled_colon.png|16px]] &lt;math&gt;2_1&lt;/math&gt; || &lt;math&gt;(2_12_1\bar{\times})&lt;/math&gt; ||
|- align=center
|20||C222&lt;sub&gt;1&lt;/sub&gt;||C 2 2 2&lt;sub&gt;1&lt;/sub&gt;|| &lt;math&gt;D_2^5&lt;/math&gt; || ''5a'' || &lt;math&gt;\left ( \tfrac{a+b}{2}:c:a:b\right ) :2_1:2&lt;/math&gt; || &lt;math&gt;(2_1{*}2_12_1)&lt;/math&gt; || &lt;math&gt;(2_02_1{*})&lt;/math&gt;
|- align=center
|21||C222||C 2 2 2|| &lt;math&gt;D_2^6&lt;/math&gt; || ''10s'' || &lt;math&gt;\left ( \tfrac{a+b}{2}:c:a:b\right ) :2:2&lt;/math&gt; || &lt;math&gt;(2_0{*}2_02_0)&lt;/math&gt; || &lt;math&gt;(*2_02_02_12_1)&lt;/math&gt;
|- align=center
|22||F222||F 2 2 2|| &lt;math&gt;D_2^7&lt;/math&gt; || ''12s'' || &lt;math&gt;\left ( \tfrac{a+c}{2}/\tfrac{b+c}{2}/\tfrac{a+b}{2}:c:a:b\right ) :2:2&lt;/math&gt; || &lt;math&gt;(*2_02_12_02_1)&lt;/math&gt; ||
|- align=center
|23||I222||I 2 2 2|| &lt;math&gt;D_2^8&lt;/math&gt; || ''11s'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:b\right ) :2:2&lt;/math&gt; || &lt;math&gt;(2_1{*}2_02_0)&lt;/math&gt; ||
|- align=center
|24||I2&lt;sub&gt;1&lt;/sub&gt;2&lt;sub&gt;1&lt;/sub&gt;2&lt;sub&gt;1&lt;/sub&gt;||I 2&lt;sub&gt;1&lt;/sub&gt; 2&lt;sub&gt;1&lt;/sub&gt; 2&lt;sub&gt;1&lt;/sub&gt;|| &lt;math&gt;D_2^9&lt;/math&gt; || ''6a'' ||&lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:b \right ) :2:2_1&lt;/math&gt; || &lt;math&gt;(2_0{*}2_12_1)&lt;/math&gt; ||
|- align=center
|25||rowspan=22|mm2||rowspan=22|&lt;math&gt;*22&lt;/math&gt;||Pmm2||P m m 2|| &lt;math&gt;C_{2v}^1&lt;/math&gt; || ''13s'' || &lt;math&gt;(c:a:b):m \cdot 2&lt;/math&gt; || &lt;math&gt;(*{\cdot}2{\cdot}2{\cdot}2{\cdot}2)&lt;/math&gt; || &lt;math&gt;[{*}_0{\cdot}{*}_0{\cdot}]&lt;/math&gt;
|- align=center
|26||Pmc&lt;!-- Not a PMCID --&gt;2&lt;sub&gt;1&lt;/sub&gt;||P m c 2&lt;sub&gt;1&lt;/sub&gt;|| &lt;math&gt;C_{2v}^2&lt;/math&gt; || ''9a'' || &lt;math&gt;(c:a:b): \tilde c \cdot 2_1&lt;/math&gt; || &lt;math&gt;(*{\cdot}2{:}2{\cdot}2{:}2)&lt;/math&gt; || &lt;math&gt;(\bar{*}{\cdot}\bar{*}{\cdot})&lt;/math&gt;, &lt;math&gt;[{\times_0}{\times_0}]&lt;/math&gt;
|- align=center
|27||Pcc2||P c c 2 || &lt;math&gt;C_{2v}^3&lt;/math&gt; || ''5h'' || &lt;math&gt;(c:a:b): \tilde c \cdot 2&lt;/math&gt; || &lt;math&gt;(*{:}2{:}2{:}2{:}2)&lt;/math&gt; || &lt;math&gt;(\bar{*}_0\bar{*}_0)&lt;/math&gt;
|- align=center
|28||Pma2||P m a 2 || &lt;math&gt;C_{2v}^4&lt;/math&gt; || ''6h'' || &lt;math&gt;(c:a:b): \tilde a \cdot 2&lt;/math&gt; || &lt;math&gt;(2_02_0{*}{\cdot})&lt;/math&gt; || &lt;math&gt;[{*}_0{:}{*}_0{:}]&lt;/math&gt;, &lt;math&gt;(*{\cdot}{*}_0)&lt;/math&gt;
|- align=center
|29||Pca2&lt;sub&gt;1&lt;/sub&gt;||P c a 2&lt;sub&gt;1&lt;/sub&gt; || &lt;math&gt;C_{2v}^5&lt;/math&gt; || ''11a'' || &lt;math&gt;(c:a:b): \tilde a \cdot 2_1&lt;/math&gt; || &lt;math&gt;(2_12_1{*}{:})&lt;/math&gt; || &lt;math&gt;(\bar{*}{:}\bar{*}{:})&lt;/math&gt;
|- align=center
|30||Pnc2||P n c 2 || &lt;math&gt;C_{2v}^6&lt;/math&gt; || ''7h'' || &lt;math&gt;(c:a:b): \tilde c \odot 2&lt;/math&gt; || &lt;math&gt;(2_02_0{*}{:})&lt;/math&gt; || &lt;math&gt;(\bar{*}_1\bar{*}_1)&lt;/math&gt;, &lt;math&gt;({*}_0{\times}_0)&lt;/math&gt;
|- align=center
|31||Pmn2&lt;sub&gt;1&lt;/sub&gt;||P m n 2&lt;sub&gt;1&lt;/sub&gt; || &lt;math&gt;C_{2v}^7&lt;/math&gt; || ''10a'' || &lt;math&gt;(c:a:b): \widetilde{ac} \cdot 2_1&lt;/math&gt; || &lt;math&gt;(2_12_1{*}{\cdot})&lt;/math&gt; || &lt;math&gt;(*{\cdot}\bar{\times})&lt;/math&gt;, &lt;math&gt;[{\times}_0{\times}_1]&lt;/math&gt;
|- align=center
|32||Pba2||P b a 2 || &lt;math&gt;C_{2v}^8&lt;/math&gt; || ''9h'' || &lt;math&gt;(c:a:b): \tilde a \odot 2&lt;/math&gt; || &lt;math&gt;(2_02_0{\times}_0)&lt;/math&gt; || &lt;math&gt;(*{:}{*}_0)&lt;/math&gt;
|- align=center
|33||Pna2&lt;sub&gt;1&lt;/sub&gt;||P n a 2&lt;sub&gt;1&lt;/sub&gt; || &lt;math&gt;C_{2v}^9&lt;/math&gt; || ''12a'' || &lt;math&gt;(c:a:b): \tilde a \odot 2_1&lt;/math&gt; || &lt;math&gt;(2_12_1{\times})&lt;/math&gt; || &lt;math&gt;(*{:}{\times})&lt;/math&gt;, &lt;math&gt;({\times}{\times}_1)&lt;/math&gt;
|- align=center
|34||Pnn2||P n n 2 || &lt;math&gt;C_{2v}^{10}&lt;/math&gt; || ''8h'' || &lt;math&gt;(c:a:b): \widetilde{ac} \odot 2&lt;/math&gt; || &lt;math&gt;(2_02_0{\times}_1)&lt;/math&gt; || &lt;math&gt;(*_0{\times}_1)&lt;/math&gt;
|- align=center
|35||Cmm2||C m m 2|| &lt;math&gt;C_{2v}^{11}&lt;/math&gt; || ''14s'' || &lt;math&gt;\left ( \tfrac{a+b}{2}:c:a:b\right ) :m \cdot 2&lt;/math&gt; || &lt;math&gt;(2_0{*}{\cdot}2{\cdot}2)&lt;/math&gt; || &lt;math&gt;[*_0{\cdot}{*}_0{:}]&lt;/math&gt;
|- align=center
|36||Cmc2&lt;sub&gt;1&lt;/sub&gt;||C m c 2&lt;sub&gt;1&lt;/sub&gt; || &lt;math&gt;C_{2v}^{12}&lt;/math&gt; || ''13a'' || &lt;math&gt;\left ( \tfrac{a+b}{2}:c:a:b\right ) :\tilde c \cdot 2_1&lt;/math&gt; || &lt;math&gt;(2_1{*}{\cdot}2{:}2)&lt;/math&gt; || &lt;math&gt;(\bar{*}{\cdot}\bar{*}{:})&lt;/math&gt;, &lt;math&gt;[{\times}_1{\times}_1]&lt;/math&gt;
|- align=center
|37||Ccc2||C c c 2 || &lt;math&gt;C_{2v}^{13}&lt;/math&gt; || ''10h'' || &lt;math&gt;\left ( \tfrac{a+b}{2}:c:a:b\right ) : \tilde c \cdot 2&lt;/math&gt; || &lt;math&gt;(2_0{*}{:}2{:}2)&lt;/math&gt; || &lt;math&gt;(\bar{*}_0\bar{*}_1)&lt;/math&gt;
|- align=center
|38||Amm2||A m m 2 || &lt;math&gt;C_{2v}^{14}&lt;/math&gt; || ''15s'' || &lt;math&gt;\left ( \tfrac{b+c}{2}/c:a:b\right ):m \cdot 2&lt;/math&gt; || &lt;math&gt;(*{\cdot}2{\cdot}2{\cdot}2{:}2)&lt;/math&gt; || &lt;math&gt;[{*}_1{\cdot}{*}_1{\cdot}]&lt;/math&gt;, &lt;math&gt;[*{\cdot}{\times}_0]&lt;/math&gt;
|- align=center
|39||Aem2||A b m 2 || &lt;math&gt;C_{2v}^{15}&lt;/math&gt; || ''11h'' || &lt;math&gt;\left ( \tfrac{b+c}{2}/c:a:b\right ) :m \cdot 2_1&lt;/math&gt; || &lt;math&gt;(*{\cdot}2{:}2{:}2{:}2)&lt;/math&gt; || &lt;math&gt;[{*}_1{:}{*}_1{:}]&lt;/math&gt;, &lt;math&gt;(\bar{*}{\cdot}\bar{*}_0)&lt;/math&gt;
|- align=center
|40||Ama2||A m a 2 || &lt;math&gt;C_{2v}^{16}&lt;/math&gt; || ''12h'' || &lt;math&gt;\left ( \tfrac{b+c}{2}/c:a:b\right ) : \tilde a \cdot 2&lt;/math&gt; || &lt;math&gt;(2_02_1{*}{\cdot})&lt;/math&gt; || &lt;math&gt;(*{\cdot}{*}_1)&lt;/math&gt;, &lt;math&gt;[*{:}{\times}_1]&lt;/math&gt;
|- align=center
|41||Aea2||A b a 2 || &lt;math&gt;C_{2v}^{17}&lt;/math&gt; || ''13h'' || &lt;math&gt;\left ( \tfrac{b+c}{2}/c:a:b\right ) : \tilde a \cdot 2_1&lt;/math&gt; || &lt;math&gt;(2_02_1{*}{:})&lt;/math&gt; || &lt;math&gt;(*{:}{*}_1)&lt;/math&gt;, &lt;math&gt;(\bar{*}{:}\bar{*}_1)&lt;/math&gt;
|- align=center
|42||Fmm2||F m m 2 || &lt;math&gt;C_{2v}^{18}&lt;/math&gt; || ''17s'' || &lt;math&gt;\left ( \tfrac{a+c}{2}/\tfrac{b+c}{2}/\tfrac{a+b}{2}:c:a:b\right ) :m \cdot 2&lt;/math&gt; || &lt;math&gt;(*{\cdot}2{\cdot}2{:}2{:}2)&lt;/math&gt; || &lt;math&gt;[{*}_1{\cdot}{*}_1{:}]&lt;/math&gt;
|- align=center
|43||Fdd2||F dd2 || &lt;math&gt;C_{2v}^{19}&lt;/math&gt; || ''16h'' || &lt;math&gt;\left ( \tfrac{a+c}{2}/\tfrac{b+c}{2}/\tfrac{a+b}{2}:c:a:b \right ) : \tfrac{1}{2} \widetilde{ac} \odot 2&lt;/math&gt; || &lt;math&gt;(2_02_1{\times})&lt;/math&gt; || &lt;math&gt;({*}_1{\times})&lt;/math&gt;
|- align=center
|44||Imm2||I m m 2 || &lt;math&gt;C_{2v}^{20}&lt;/math&gt; || ''16s'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:b \right ) :m \cdot 2&lt;/math&gt; || &lt;math&gt;(2_1{*}{\cdot}2{\cdot}2)&lt;/math&gt; || &lt;math&gt;[*{\cdot}{\times}_1]&lt;/math&gt;
|- align=center
|45||Iba2||I b a 2 || &lt;math&gt;C_{2v}^{21}&lt;/math&gt; || ''15h'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:b \right ) : \tilde c \cdot 2&lt;/math&gt; || &lt;math&gt;(2_1{*}{:}2{:}2)&lt;/math&gt; || &lt;math&gt;(\bar{*}{:}\bar{*}_0)&lt;/math&gt;
|- align=center
|46||Ima2||I m a 2 || &lt;math&gt;C_{2v}^{22}&lt;/math&gt; || ''14h'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:b \right ) : \tilde a \cdot 2&lt;/math&gt; || &lt;math&gt;(2_0{*}{\cdot}2{:}2)&lt;/math&gt; || &lt;math&gt;(\bar{*}{\cdot}\bar{*}_1)&lt;/math&gt;, &lt;math&gt;[*{:}{\times}_0]&lt;/math&gt;
|- align=center
|47||rowspan=28|&lt;math&gt;\tfrac{2}{m}\tfrac{2}{m}\tfrac{2}{m}&lt;/math&gt;||rowspan=28|&lt;math&gt;*222&lt;/math&gt;||Pmmm||P 2/m 2/m 2/m || &lt;math&gt;D_{2h}^1&lt;/math&gt; || ''18s'' || &lt;math&gt;\left ( c:a:b \right ) \cdot m:2 \cdot m&lt;/math&gt; || &lt;math&gt;[*{\cdot}2{\cdot}2{\cdot}2{\cdot}2]&lt;/math&gt; ||
|- align=center
|48||Pnnn||P 2/n 2/n 2/n || &lt;math&gt;D_{2h}^2&lt;/math&gt; || ''19h'' || &lt;math&gt;\left ( c:a:b \right ) \cdot \widetilde{ab}:2 \odot \widetilde{ac}&lt;/math&gt; || &lt;math&gt;(2\bar{*}_12_02_0&lt;/math&gt; ||
|- align=center
|49||Pccm||P 2/c 2/c 2/m || &lt;math&gt;D_{2h}^3&lt;/math&gt; || ''17h'' || &lt;math&gt;\left ( c:a:b \right ) \cdot m:2 \cdot \tilde c&lt;/math&gt; || &lt;math&gt;[*{:}2{:}2{:}2{:}2]&lt;/math&gt; || &lt;math&gt;(*2_02_02{\cdot}2)&lt;/math&gt;
|- align=center
|50||Pban||P 2/b 2/a 2/n || &lt;math&gt;D_{2h}^4&lt;/math&gt; || ''18h'' || &lt;math&gt;\left ( c:a:b \right ) \cdot \widetilde{ab}:2 \odot \tilde a&lt;/math&gt; || &lt;math&gt;(2\bar{*}_02_02_0)&lt;/math&gt; || &lt;math&gt;(*2_02_02{:}2)&lt;/math&gt;
|- align=center
|51||Pmma||P 2&lt;sub&gt;1&lt;/sub&gt;/m 2/m 2/a || &lt;math&gt;D_{2h}^5&lt;/math&gt; || ''14a'' || &lt;math&gt;\left ( c:a:b \right ) \cdot \tilde a :2 \cdot m&lt;/math&gt; || &lt;math&gt;[2_02_0{*}{\cdot}]&lt;/math&gt; || &lt;math&gt;[*{\cdot}2{:}2{\cdot}2{:}2]&lt;/math&gt;, &lt;math&gt;[*2{\cdot}2{\cdot}2{\cdot}2]&lt;/math&gt;
|- align=center
|52||Pnna||P 2/n 2&lt;sub&gt;1&lt;/sub&gt;/n 2/a || &lt;math&gt;D_{2h}^6&lt;/math&gt; || ''17a'' || &lt;math&gt;\left ( c:a:b \right ) \cdot \tilde a:2 \odot \widetilde{ac}&lt;/math&gt; || &lt;math&gt;(2_02\bar{*}_1)&lt;/math&gt; || &lt;math&gt;(2_0{*}2{:}2)&lt;/math&gt;, &lt;math&gt;(2\bar{*}2_12_1)&lt;/math&gt;
|- align=center
|53||Pmna||P 2/m 2/n 2&lt;sub&gt;1&lt;/sub&gt;/a || &lt;math&gt;D_{2h}^7&lt;/math&gt; || ''15a'' || &lt;math&gt;\left ( c:a:b \right ) \cdot \tilde a:2_1 \cdot \widetilde{ac}&lt;/math&gt; || &lt;math&gt;[2_02_0{*}{:}]&lt;/math&gt; || &lt;math&gt;(*2_12_12{\cdot}2)&lt;/math&gt;, &lt;math&gt;(2_0{*}2{\cdot}2)&lt;/math&gt;
|- align=center
|54||Pcca||P 2&lt;sub&gt;1&lt;/sub&gt;/c 2/c 2/a || &lt;math&gt;D_{2h}^8&lt;/math&gt; || ''16a'' || &lt;math&gt;\left ( c:a:b \right ) \cdot \tilde a:2 \cdot \tilde c&lt;/math&gt; || &lt;math&gt;(2_02\bar{*}_0)&lt;/math&gt; || &lt;math&gt;(*2{:}2{:}2{:}2)&lt;/math&gt;, &lt;math&gt;(*2_12_12{:}2)&lt;/math&gt;
|- align=center
|55||Pbam||P 2&lt;sub&gt;1&lt;/sub&gt;/b 2&lt;sub&gt;1&lt;/sub&gt;/a 2/m || &lt;math&gt;D_{2h}^9&lt;/math&gt; || ''22a'' || &lt;math&gt;\left ( c:a:b \right ) \cdot m:2 \odot \tilde a&lt;/math&gt; || &lt;math&gt;[2_02_0{\times}_0]&lt;/math&gt; || &lt;math&gt;(*2{\cdot}2{:}2{\cdot}2)&lt;/math&gt;
|- align=center
|56||Pccn||P 2&lt;sub&gt;1&lt;/sub&gt;/c 2&lt;sub&gt;1&lt;/sub&gt;/c 2/n || &lt;math&gt;D_{2h}^{10}&lt;/math&gt; || ''27a'' || &lt;math&gt;\left ( c:a:b \right ) \cdot \widetilde{ab}:2 \cdot \tilde c&lt;/math&gt; || &lt;math&gt;(2\bar{*}{:}2{:}2)&lt;/math&gt; || &lt;math&gt;(2_12\bar{*}_0)&lt;/math&gt;
|- align=center
|57||Pbcm||P 2/b 2&lt;sub&gt;1&lt;/sub&gt;/c 2&lt;sub&gt;1&lt;/sub&gt;/m || &lt;math&gt;D_{2h}^{11}&lt;/math&gt; || ''23a'' || &lt;math&gt;\left ( c:a:b \right ) \cdot m:2_1 \odot \tilde c&lt;/math&gt; || &lt;math&gt;(2_02\bar{*}{\cdot})&lt;/math&gt; || &lt;math&gt;(*2{:}2{\cdot}2{:}2)&lt;/math&gt;, &lt;math&gt;[2_12_1{*}{:}]&lt;/math&gt;
|- align=center
|58||Pnnm||P 2&lt;sub&gt;1&lt;/sub&gt;/n 2&lt;sub&gt;1&lt;/sub&gt;/n 2/m || &lt;math&gt;D_{2h}^{12}&lt;/math&gt; || ''25a'' || &lt;math&gt;\left ( c:a:b \right ) \cdot m:2 \odot \widetilde{ac}&lt;/math&gt; || &lt;math&gt;[2_02_0{\times}_1]&lt;/math&gt; || &lt;math&gt;(2_1{*}2{\cdot}2)&lt;/math&gt;
|- align=center
|59||Pmmn||P 2&lt;sub&gt;1&lt;/sub&gt;/m 2&lt;sub&gt;1&lt;/sub&gt;/m 2/n || &lt;math&gt;D_{2h}^{13}&lt;/math&gt; || ''24a'' || &lt;math&gt;\left ( c:a:b \right ) \cdot \widetilde{ab}:2 \cdot m&lt;/math&gt; || &lt;math&gt;(2\bar{*}{\cdot}2{\cdot}2)&lt;/math&gt; || &lt;math&gt;[2_12_1{*}{\cdot}]&lt;/math&gt;
|- align=center
|60||Pbcn||P 2&lt;sub&gt;1&lt;/sub&gt;/b 2/c 2&lt;sub&gt;1&lt;/sub&gt;/n || &lt;math&gt;D_{2h}^{14}&lt;/math&gt; || ''26a'' || &lt;math&gt;\left ( c:a:b \right ) \cdot \widetilde{ab}:2_1 \odot \tilde c&lt;/math&gt; || &lt;math&gt;(2_02\bar{*}{:})&lt;/math&gt; || &lt;math&gt;(2_1{*}2{:}2)&lt;/math&gt;, &lt;math&gt;(2_12\bar{*}_1)&lt;/math&gt;
|- align=center
|61||Pbca||P 2&lt;sub&gt;1&lt;/sub&gt;/b 2&lt;sub&gt;1&lt;/sub&gt;/c 2&lt;sub&gt;1&lt;/sub&gt;/a || &lt;math&gt;D_{2h}^{15}&lt;/math&gt; || ''29a'' || &lt;math&gt;\left ( c:a:b \right ) \cdot \tilde a:2_1 \odot \tilde c&lt;/math&gt; || &lt;math&gt;(2_12\bar{*}{:})&lt;/math&gt; ||
|- align=center
|62||Pnma||P 2&lt;sub&gt;1&lt;/sub&gt;/n 2&lt;sub&gt;1&lt;/sub&gt;/m 2&lt;sub&gt;1&lt;/sub&gt;/a || &lt;math&gt;D_{2h}^{16}&lt;/math&gt; || ''28a'' || &lt;math&gt;\left ( c:a:b \right ) \cdot \tilde a:2_1 \odot m&lt;/math&gt; || &lt;math&gt;(2_12\bar{*}{\cdot})&lt;/math&gt; || &lt;math&gt;(2\bar{*}{\cdot}2{:}2)&lt;/math&gt;, &lt;math&gt;[2_12_1{\times}]&lt;/math&gt;
|- align=center
|63||Cmcm||C 2/m 2/c 2&lt;sub&gt;1&lt;/sub&gt;/m || &lt;math&gt;D_{2h}^{17}&lt;/math&gt; || ''18a'' ||&lt;math&gt;\left ( \tfrac{a+b}{2}:c:a:b\right ) \cdot m:2_1 \cdot \tilde c&lt;/math&gt; || &lt;math&gt;[2_02_1{*}{\cdot}]&lt;/math&gt; || &lt;math&gt;(*2{\cdot}2{\cdot}2{:}2)&lt;/math&gt;, &lt;math&gt;[2_1{*}{\cdot}2{:}2]&lt;/math&gt;
|- align=center
|64||Cmca||C 2/m 2/c 2&lt;sub&gt;1&lt;/sub&gt;/a || &lt;math&gt;D_{2h}^{18}&lt;/math&gt; || ''19a''||&lt;math&gt;\left ( \tfrac{a+b}{2}:c:a:b\right ) \cdot \tilde a :2_1 \cdot \tilde c&lt;/math&gt; || &lt;math&gt;[2_02_1{*}{:}]&lt;/math&gt; || &lt;math&gt;(*2{\cdot}2{:}2{:}2)&lt;/math&gt;, &lt;math&gt;(*2_12{\cdot}2{:}2)&lt;/math&gt;
|- align=center
|65||Cmmm||C 2/m 2/m 2/m || &lt;math&gt;D_{2h}^{19}&lt;/math&gt; || ''19s''||&lt;math&gt;\left ( \tfrac{a+b}{2}:c:a:b\right ) \cdot m:2 \cdot m&lt;/math&gt; || &lt;math&gt;[2_0{*}{\cdot}2{\cdot}2]&lt;/math&gt; || &lt;math&gt;[*{\cdot}2{\cdot}2{\cdot}2{:}2]&lt;/math&gt;
|- align=center
|66||Cccm||C 2/c 2/c 2/m || &lt;math&gt;D_{2h}^{20}&lt;/math&gt; || ''20h''||&lt;math&gt;\left ( \tfrac{a+b}{2}:c:a:b\right ) \cdot m:2 \cdot \tilde c&lt;/math&gt; || &lt;math&gt;[2_0{*}{:}2{:}2]&lt;/math&gt; || &lt;math&gt;(*2_02_12{\cdot}2)&lt;/math&gt;
|- align=center
|67||Cmme||C 2/m 2/m 2/e || &lt;math&gt;D_{2h}^{21}&lt;/math&gt; || ''21h''||&lt;math&gt;\left ( \tfrac{a+b}{2}:c:a:b\right ) \cdot \tilde a :2 \cdot m&lt;/math&gt; || &lt;math&gt;(*2_02{\cdot}2{\cdot}2)&lt;/math&gt; || &lt;math&gt;[*{\cdot}2{:}2{:}2{:}2]&lt;/math&gt;
|- align=center
|68||Ccce||C 2/c 2/c 2/e || &lt;math&gt;D_{2h}^{22}&lt;/math&gt; || ''22h''||&lt;math&gt;\left ( \tfrac{a+b}{2}:c:a:b\right ) \cdot \tilde a :2 \cdot \tilde c&lt;/math&gt; || &lt;math&gt;(*2_02{:}2{:}2)&lt;/math&gt; || &lt;math&gt;(*2_02_12{:}2)&lt;/math&gt;
|- align=center
|69||Fmmm||F 2/m 2/m 2/m || &lt;math&gt;D_{2h}^{23}&lt;/math&gt; || ''21s''|| &lt;math&gt;\left ( \tfrac{a+c}{2}/\tfrac{b+c}{2}/\tfrac{a+b}{2}:c:a:b\right ) \cdot m:2 \cdot m&lt;/math&gt; || &lt;math&gt;[*{\cdot}2{\cdot}2{:}2{:}2]&lt;/math&gt; ||
|- align=center
|70||Fddd||F 2/d 2/d 2/d || &lt;math&gt;D_{2h}^{24}&lt;/math&gt; || ''24h''|| &lt;math&gt;\left ( \tfrac{a+c}{2}/\tfrac{b+c}{2}/\tfrac{a+b}{2}:c:a:b\right ) \cdot \tfrac{1}{2}\widetilde{ab}:2 \odot \tfrac{1}{2}\widetilde{ac}&lt;/math&gt; || &lt;math&gt;(2\bar{*}2_02_1)&lt;/math&gt; ||
|- align=center
|71||Immm||I 2/m 2/m 2/m || &lt;math&gt;D_{2h}^{25}&lt;/math&gt; || ''20s''|| &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:b\right ) \cdot m:2 \cdot m&lt;/math&gt; || &lt;math&gt;[2_1{*}{\cdot}2{\cdot}2]&lt;/math&gt; ||
|- align=center
|72||Ibam||I 2/b 2/a 2/m || &lt;math&gt;D_{2h}^{26}&lt;/math&gt; || ''23h''|| &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:b\right ) \cdot m:2 \cdot \tilde c&lt;/math&gt; || &lt;math&gt;[2_1{*}{:}2{:}2]&lt;/math&gt; || &lt;math&gt;(*2_02{\cdot}2{:}2)&lt;/math&gt;
|- align=center
|73||Ibca||I 2/b 2/c 2/a || &lt;math&gt;D_{2h}^{27}&lt;/math&gt; || ''21a''|| &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:b\right ) \cdot \tilde a :2 \cdot \tilde c&lt;/math&gt; || &lt;math&gt;(*2_12{:}2{:}2)&lt;/math&gt; ||
|- align=center
|74||Imma||I 2/m 2/m 2/a || &lt;math&gt;D_{2h}^{28}&lt;/math&gt; || ''20a''|| &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:b\right ) \cdot \tilde a :2 \cdot m&lt;/math&gt; || &lt;math&gt;(*2_12{\cdot}2{\cdot}2)&lt;/math&gt; || &lt;math&gt;[2_0{*}{\cdot}2{:}2]&lt;/math&gt;
|}

==List of Tetragonal==
{| class=wikitable
|+ [[Tetragonal crystal system]]
!Number
![[Point group]]
![[Orbifold notation|Orbifold]]
!Short name
!Full name
![[Schoenflies notation|Schoenflies]]
![[Evgraf Fedorov|Fedorov]]
!Shubnikov
![[Fibrifold notation|Fibrifold]]
|- align=center
|75||rowspan=6|4||rowspan=6|&lt;math&gt;44&lt;/math&gt;||P4||P 4 || &lt;math&gt;C_4^1&lt;/math&gt; || ''22s'' || &lt;math&gt;(c:a:a):4&lt;/math&gt; || &lt;math&gt;(4_04_02_0)&lt;/math&gt;
|- align=center
|76||P4&lt;sub&gt;1&lt;/sub&gt;||P 4&lt;sub&gt;1&lt;/sub&gt; || &lt;math&gt;C_4^2&lt;/math&gt; || ''30a'' || &lt;math&gt;(c:a:a) :4_1&lt;/math&gt; || &lt;math&gt;(4_14_12_1)&lt;/math&gt;
|- align=center
|77||P4&lt;sub&gt;2&lt;/sub&gt;||P 4&lt;sub&gt;2&lt;/sub&gt; || &lt;math&gt;C_4^3&lt;/math&gt; || ''33a'' || &lt;math&gt;(c:a:a) :4_2&lt;/math&gt; || &lt;math&gt;(4_24_22_0)&lt;/math&gt;
|- align=center
|78||P4&lt;sub&gt;3&lt;/sub&gt;||P 4&lt;sub&gt;3&lt;/sub&gt; || &lt;math&gt;C_4^4&lt;/math&gt; || ''31a'' || &lt;math&gt;(c:a:a) :4_3&lt;/math&gt; || &lt;math&gt;(4_14_12_1)&lt;/math&gt;
|- align=center
|79||I4||I 4 || &lt;math&gt;C_4^5&lt;/math&gt; || ''23s'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) :4&lt;/math&gt; || &lt;math&gt;(4_24_02_1)&lt;/math&gt;
|- align=center
|80||I4&lt;sub&gt;1&lt;/sub&gt;||I 4&lt;sub&gt;1&lt;/sub&gt; || &lt;math&gt;C_4^6&lt;/math&gt; || ''32a'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) :4_1&lt;/math&gt;  || &lt;math&gt;(4_34_12_0)&lt;/math&gt;
|- align=center
|81||rowspan=2|{{overline|4}}||rowspan=2|&lt;math&gt;2\times&lt;/math&gt;||P{{overline|4}}||P {{overline|4}} || &lt;math&gt;S_4^1&lt;/math&gt; || ''26s'' || &lt;math&gt;(c:a:a):\tilde 4&lt;/math&gt; || &lt;math&gt;(442_0)&lt;/math&gt;
|- align=center
|82||I{{overline|4}}||I {{overline|4}} || &lt;math&gt;S_4^2&lt;/math&gt; || ''27s'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) :\tilde 4&lt;/math&gt; || &lt;math&gt;(442_1)&lt;/math&gt;
|- align=center
|83||rowspan=6|4/m||rowspan=6|&lt;math&gt;4*&lt;/math&gt;||P4/m||P 4/m|| &lt;math&gt;C_{4h}^1&lt;/math&gt; || ''28s'' || &lt;math&gt;(c:a:a)\cdot m:4&lt;/math&gt; || &lt;math&gt;[4_04_02_0]&lt;/math&gt;
|- align=center
|84||P4&lt;sub&gt;2&lt;/sub&gt;/m||P 4&lt;sub&gt;2&lt;/sub&gt;/m|| &lt;math&gt;C_{4h}^2&lt;/math&gt; || ''41a'' || &lt;math&gt;(c:a:a)\cdot m:4_2&lt;/math&gt; || &lt;math&gt;[4_24_22_0]&lt;/math&gt;
|- align=center
|85||P4/n||P 4/n|| &lt;math&gt;C_{4h}^3&lt;/math&gt; || 29h || &lt;math&gt;(c:a:a)\cdot \widetilde{ab}:4&lt;/math&gt; || &lt;math&gt;(44_02)&lt;/math&gt;
|- align=center
|86||P4&lt;sub&gt;2&lt;/sub&gt;/n||P 4&lt;sub&gt;2&lt;/sub&gt;/n|| &lt;math&gt;C_{4h}^4&lt;/math&gt; || ''42a'' || &lt;math&gt;(c:a:a)\cdot \widetilde{ab}:4_2&lt;/math&gt; || &lt;math&gt;(44_22)&lt;/math&gt;
|- align=center
|87||I4/m||I 4/m|| &lt;math&gt;C_{4h}^5&lt;/math&gt; || ''29s'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) \cdot m:4&lt;/math&gt; || &lt;math&gt;[4_24_02_1]&lt;/math&gt;
|- align=center
|88||I4&lt;sub&gt;1&lt;/sub&gt;/a||I 4&lt;sub&gt;1&lt;/sub&gt;/a|| &lt;math&gt;C_{4h}^6&lt;/math&gt; || ''40a'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) \cdot \tilde a :4_1&lt;/math&gt; || &lt;math&gt;(44_12)&lt;/math&gt;
|- align=center
|89||rowspan=10|422||rowspan=10|&lt;math&gt;224&lt;/math&gt;||P422||P 4 2 2 || &lt;math&gt;D_4^1&lt;/math&gt; || ''30s'' || &lt;math&gt;(c:a:a):4:2&lt;/math&gt; || &lt;math&gt;(*4_04_02_0)&lt;/math&gt;
|- align=center
|90||P42&lt;sub&gt;1&lt;/sub&gt;2||P42&lt;sub&gt;1&lt;/sub&gt;2 || &lt;math&gt;D_4^2&lt;/math&gt; || ''43a'' || &lt;math&gt;(c:a:a):4&lt;/math&gt; [[image:circled_colon.png|16px]] &lt;math&gt;2_1&lt;/math&gt; || &lt;math&gt;(4_0{*}2_0)&lt;/math&gt;
|- align=center
|91||P4&lt;sub&gt;1&lt;/sub&gt;22||P 4&lt;sub&gt;1&lt;/sub&gt; 2 2 || &lt;math&gt;D_4^3&lt;/math&gt; || ''44a'' || &lt;math&gt;(c:a:a):4_1:2&lt;/math&gt; || &lt;math&gt;(*4_14_12_1)&lt;/math&gt;
|- align=center
|92||P4&lt;sub&gt;1&lt;/sub&gt;2&lt;sub&gt;1&lt;/sub&gt;2||P 4&lt;sub&gt;1&lt;/sub&gt; 2&lt;sub&gt;1&lt;/sub&gt; 2 || &lt;math&gt;D_4^4&lt;/math&gt; || ''48a'' || &lt;math&gt;(c:a:a):4_1&lt;/math&gt; [[image:circled_colon.png|16px]] &lt;math&gt;2_1&lt;/math&gt; || &lt;math&gt;(4_1{*}2_1)&lt;/math&gt;
|- align=center
|93||P4&lt;sub&gt;2&lt;/sub&gt;22||P 4&lt;sub&gt;2&lt;/sub&gt; 2 2 || &lt;math&gt;D_4^5&lt;/math&gt; || ''47a'' || &lt;math&gt;(c:a:a):4_2:2&lt;/math&gt; || &lt;math&gt;(*4_24_22_0)&lt;/math&gt;
|- align=center
|94||P4&lt;sub&gt;2&lt;/sub&gt;2&lt;sub&gt;1&lt;/sub&gt;2||P 4&lt;sub&gt;2&lt;/sub&gt; 2&lt;sub&gt;1&lt;/sub&gt; 2 || &lt;math&gt;D_4^6&lt;/math&gt; || ''50a'' || &lt;math&gt;(c:a:a):4_2&lt;/math&gt; [[image:circled_colon.png|16px]] &lt;math&gt;2_1&lt;/math&gt; || &lt;math&gt;(4_2{*}2_0)&lt;/math&gt;
|- align=center
|95||P4&lt;sub&gt;3&lt;/sub&gt;22||P 4&lt;sub&gt;3&lt;/sub&gt; 2 2 || &lt;math&gt;D_4^7&lt;/math&gt; || ''45a'' || &lt;math&gt;(c:a:a):4_3:2&lt;/math&gt; || &lt;math&gt;(*4_14_12_1)&lt;/math&gt;
|- align=center
|96||P4&lt;sub&gt;3&lt;/sub&gt;2&lt;sub&gt;1&lt;/sub&gt;2||P 4&lt;sub&gt;3&lt;/sub&gt; 2&lt;sub&gt;1&lt;/sub&gt; 2 || &lt;math&gt;D_4^8&lt;/math&gt; || ''49a'' || &lt;math&gt;(c:a:a):4_3&lt;/math&gt; [[image:circled_colon.png|16px]] &lt;math&gt;2_1&lt;/math&gt; || &lt;math&gt;(4_1{*}2_1)&lt;/math&gt;
|- align=center
|97||I422||I 4 2 2 || &lt;math&gt;D_4^9&lt;/math&gt; || ''31s'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) :4:2&lt;/math&gt; || &lt;math&gt;(*4_24_02_1)&lt;/math&gt;
|- align=center
|98||I4&lt;sub&gt;1&lt;/sub&gt;22||I 4&lt;sub&gt;1&lt;/sub&gt; 2 2 || &lt;math&gt;D_4^{10}&lt;/math&gt; || ''46a'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) :4:2_1&lt;/math&gt; || &lt;math&gt;(*4_34_12_0)&lt;/math&gt;
|- align=center
|99||rowspan=12|4mm||rowspan=12|&lt;math&gt;*44&lt;/math&gt;||P4mm||P 4 m m || &lt;math&gt;C_{4v}^1&lt;/math&gt; || ''24s'' || &lt;math&gt;(c:a:a):4\cdot m&lt;/math&gt; || &lt;math&gt;(*{\cdot}4{\cdot}4{\cdot}2)&lt;/math&gt;
|- align=center
|100||P4bm|| P 4 b m || &lt;math&gt;C_{4v}^2&lt;/math&gt; || ''26h'' || &lt;math&gt;(c:a:a):4\odot \tilde a&lt;/math&gt; || &lt;math&gt;(4_0{*}{\cdot}2)&lt;/math&gt;
|- align=center
|101||P4&lt;sub&gt;2&lt;/sub&gt;cm|| P 4&lt;sub&gt;2&lt;/sub&gt; c m || &lt;math&gt;C_{4v}^3&lt;/math&gt; || ''37a'' || &lt;math&gt;(c:a:a):4_2\cdot \tilde c&lt;/math&gt; || &lt;math&gt;(*{:}4{\cdot}4{:}2)&lt;/math&gt;
|- align=center
|102||P4&lt;sub&gt;2&lt;/sub&gt;nm|| P 4&lt;sub&gt;2&lt;/sub&gt; n m || &lt;math&gt;C_{4v}^4&lt;/math&gt; || ''38a'' || &lt;math&gt;(c:a:a):4_2\odot \widetilde{ac}&lt;/math&gt; || &lt;math&gt;(4_2{*}{\cdot}2)&lt;/math&gt;
|- align=center
|103||P4cc|| P 4 c c || &lt;math&gt;C_{4v}^5&lt;/math&gt; || ''25h'' || &lt;math&gt;(c:a:a):4\cdot \tilde c&lt;/math&gt; || &lt;math&gt;(*{:}4{:}4{:}2)&lt;/math&gt;
|- align=center
|104||P4nc|| P 4 n c || &lt;math&gt;C_{4v}^6&lt;/math&gt; || ''27h'' || &lt;math&gt;(c:a:a):4\odot \widetilde{ac}&lt;/math&gt; || &lt;math&gt;(4_0{*}{:}2)&lt;/math&gt;
|- align=center
|105||P4&lt;sub&gt;2&lt;/sub&gt;mc|| P 4&lt;sub&gt;2&lt;/sub&gt; m c || &lt;math&gt;C_{4v}^7&lt;/math&gt; || ''36a'' || &lt;math&gt;(c:a:a):4_2\cdot m&lt;/math&gt; || &lt;math&gt;(*{\cdot}4{:}4{\cdot}2)&lt;/math&gt;
|- align=center
|106||P4&lt;sub&gt;2&lt;/sub&gt;bc|| P 4&lt;sub&gt;2&lt;/sub&gt; b c || &lt;math&gt;C_{4v}^8&lt;/math&gt; || ''39a'' || &lt;math&gt;(c:a:a):4\odot \tilde a&lt;/math&gt; || &lt;math&gt;(4_2{*}{:}2)&lt;/math&gt;
|- align=center
|107||I4mm|| I 4 m m || &lt;math&gt;C_{4v}^9&lt;/math&gt; || ''25s'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) :4\cdot m&lt;/math&gt; || &lt;math&gt;(*{\cdot}4{\cdot}4{:}2)&lt;/math&gt;
|- align=center
|108||I4cm|| I 4 c m || &lt;math&gt;C_{4v}^{10}&lt;/math&gt; || ''28h'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) :4\cdot \tilde c&lt;/math&gt; || &lt;math&gt;(*{\cdot}4{:}4{:}2)&lt;/math&gt;
|- align=center
|109||I4&lt;sub&gt;1&lt;/sub&gt;md|| I 4&lt;sub&gt;1&lt;/sub&gt; m d || &lt;math&gt;C_{4v}^{11}&lt;/math&gt; || ''34a'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) :4_1\odot m&lt;/math&gt; || &lt;math&gt;(4_1{*}{\cdot}2)&lt;/math&gt;
|- align=center
|110||I4&lt;sub&gt;1&lt;/sub&gt;cd|| I 4&lt;sub&gt;1&lt;/sub&gt; c d || &lt;math&gt;C_{4v}^{12}&lt;/math&gt; || ''35a'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) :4_1\odot \tilde c&lt;/math&gt; || &lt;math&gt;(4_1{*}{:}2)&lt;/math&gt;
|- align=center
|111||rowspan=12|{{overline|4}}2m||rowspan=12|&lt;math&gt;2{*}2&lt;/math&gt;||P{{overline|4}}2m|| P {{overline|4}} 2 m || &lt;math&gt;D_{2d}^1&lt;/math&gt; || ''32s'' || &lt;math&gt;(c:a:a):\tilde 4 :2&lt;/math&gt; || &lt;math&gt;(*4{\cdot}42_0)&lt;/math&gt;
|- align=center
|112||P{{overline|4}}2c|| P {{overline|4}} 2 c || &lt;math&gt;D_{2d}^2&lt;/math&gt; || ''30h'' || &lt;math&gt;(c:a:a):\tilde 4 &lt;/math&gt; [[image:circled_colon.png|16px]] &lt;math&gt;2&lt;/math&gt; || &lt;math&gt;(*4{:}42_0)&lt;/math&gt;
|- align=center
|113||P{{overline|4}}2&lt;sub&gt;1&lt;/sub&gt;m|| P {{overline|4}} 2&lt;sub&gt;1&lt;/sub&gt; m || &lt;math&gt;D_{2d}^3&lt;/math&gt; || ''52a'' || &lt;math&gt;(c:a:a):\tilde 4 \cdot \widetilde{ab}&lt;/math&gt; || &lt;math&gt;(4\bar{*}{\cdot}2)&lt;/math&gt;
|- align=center
|114||P{{overline|4}}2&lt;sub&gt;1&lt;/sub&gt;c|| P {{overline|4}} 2&lt;sub&gt;1&lt;/sub&gt; c || &lt;math&gt;D_{2d}^4&lt;/math&gt; || ''53a'' || &lt;math&gt;(c:a:a):\tilde 4 \cdot \widetilde{abc}&lt;/math&gt; || &lt;math&gt;(4\bar{*}{:}2)&lt;/math&gt;
|- align=center
|115||P{{overline|4}}m2|| P {{overline|4}} m 2 || &lt;math&gt;D_{2d}^5&lt;/math&gt; || ''33s'' || &lt;math&gt;(c:a:a):\tilde 4 \cdot m&lt;/math&gt; || &lt;math&gt;(*{\cdot}44{\cdot}2)&lt;/math&gt;
|- align=center
|116||P{{overline|4}}c2|| P {{overline|4}} c 2 || &lt;math&gt;D_{2d}^6&lt;/math&gt; || ''31h'' || &lt;math&gt;(c:a:a):\tilde 4 \cdot \tilde c&lt;/math&gt; || &lt;math&gt;(*{:}44{:}2)&lt;/math&gt;
|- align=center
|117||P{{overline|4}}b2|| P {{overline|4}} b 2 || &lt;math&gt;D_{2d}^7&lt;/math&gt; || ''32h'' || &lt;math&gt;(c:a:a):\tilde 4 \odot \tilde a&lt;/math&gt; || &lt;math&gt;(4\bar{*}_02_0)&lt;/math&gt;
|- align=center
|118||P{{overline|4}}n2|| P {{overline|4}} n 2 || &lt;math&gt;D_{2d}^8&lt;/math&gt; || ''33h'' || &lt;math&gt;(c:a:a):\tilde 4 \cdot \widetilde{ac}&lt;/math&gt; || &lt;math&gt;(4\bar{*}_12_0)&lt;/math&gt;
|- align=center
|119||I{{overline|4}}m2|| I {{overline|4}} m 2 || &lt;math&gt;D_{2d}^9&lt;/math&gt; || ''35s'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) :\tilde 4 \cdot m&lt;/math&gt; || &lt;math&gt;(*4{\cdot}42_1)&lt;/math&gt;
|- align=center
|120||I{{overline|4}}c2|| I {{overline|4}} c 2 || &lt;math&gt;D_{2d}^{10}&lt;/math&gt; || ''34h'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) :\tilde 4 \cdot \tilde c&lt;/math&gt; || &lt;math&gt;(*4{:}42_1)&lt;/math&gt;
|- align=center
|121||I{{overline|4}}2m|| I {{overline|4}} 2 m || &lt;math&gt;D_{2d}^{11}&lt;/math&gt; || ''34s'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) :\tilde 4 :2&lt;/math&gt; || &lt;math&gt;(*{\cdot}44{:}2)&lt;/math&gt;
|- align=center
|122||I{{overline|4}}2d|| I {{overline|4}} 2 d || &lt;math&gt;D_{2d}^{12}&lt;/math&gt; || ''51a'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) :\tilde 4 \odot \tfrac{1}{2}\widetilde{abc}&lt;/math&gt; || &lt;math&gt;(4\bar{*}2_1)&lt;/math&gt;
|- align=center
|123||rowspan=20|4/m 2/m 2/m||rowspan=20|&lt;math&gt;*224&lt;/math&gt;||P4/mmm|| P 4/m 2/m 2/m || &lt;math&gt;D_{4h}^1&lt;/math&gt; || ''36s'' || &lt;math&gt;(c:a:a)\cdot m:4\cdot m&lt;/math&gt; || &lt;math&gt;[*{\cdot}4{\cdot}4{\cdot}2]&lt;/math&gt;
|- align=center
|124||P4/mcc|| P 4/m 2/c 2/c || &lt;math&gt;D_{4h}^2&lt;/math&gt; || ''35h'' || &lt;math&gt;(c:a:a)\cdot m:4\cdot \tilde c&lt;/math&gt; || &lt;math&gt;[*{:}4{:}4{:}2]&lt;/math&gt;
|- align=center
|125||P4/nbm|| P 4/n 2/b 2/m|| &lt;math&gt;D_{4h}^3&lt;/math&gt; || ''36h'' || &lt;math&gt;(c:a:a)\cdot \widetilde{ab}:4\odot \tilde a&lt;/math&gt; || &lt;math&gt;(*4_04{\cdot}2)&lt;/math&gt;
|- align=center
|126||P4/nnc|| P 4/n 2/n 2/c || &lt;math&gt;D_{4h}^4&lt;/math&gt; || ''37h'' || &lt;math&gt;(c:a:a)\cdot \widetilde{ab}:4\odot \widetilde{ac}&lt;/math&gt; || &lt;math&gt;(*4_04{:}2)&lt;/math&gt;
|- align=center
|127||P4/mbm|| P 4/m 2&lt;sub&gt;1&lt;/sub&gt;/b 2/m || &lt;math&gt;D_{4h}^5&lt;/math&gt; || ''54a'' || &lt;math&gt;(c:a:a)\cdot m:4\odot \tilde a&lt;/math&gt; || &lt;math&gt;[4_0{*}{\cdot}2]&lt;/math&gt;
|- align=center
|128||P4/mnc|| P 4/m 2&lt;sub&gt;1&lt;/sub&gt;/n 2/c || &lt;math&gt;D_{4h}^6&lt;/math&gt; || ''56a'' || &lt;math&gt;(c:a:a)\cdot m:4\odot \widetilde{ac}&lt;/math&gt; || &lt;math&gt;[4_0{*}{:}2]&lt;/math&gt;
|- align=center
|129||P4/nmm|| P 4/n 2&lt;sub&gt;1&lt;/sub&gt;/m 2/m || &lt;math&gt;D_{4h}^7&lt;/math&gt; || ''55a'' || &lt;math&gt;(c:a:a)\cdot \widetilde{ab}:4\cdot m&lt;/math&gt; || &lt;math&gt;(*4{\cdot}4{\cdot}2)&lt;/math&gt;
|- align=center
|130||P4/ncc|| P 4/n 2&lt;sub&gt;1&lt;/sub&gt;/c 2/c || &lt;math&gt;D_{4h}^8&lt;/math&gt; || ''57a'' || &lt;math&gt;(c:a:a)\cdot \widetilde{ab}:4\cdot \tilde c&lt;/math&gt; || &lt;math&gt;(*4{:}4{:}2)&lt;/math&gt;
|- align=center
|131||P4&lt;sub&gt;2&lt;/sub&gt;/mmc|| P 4&lt;sub&gt;2&lt;/sub&gt;/m 2/m 2/c || &lt;math&gt;D_{4h}^9&lt;/math&gt; || ''60a'' || &lt;math&gt;(c:a:a)\cdot m:4_2\cdot m&lt;/math&gt; || &lt;math&gt;[*{\cdot}4{:}4{\cdot}2]&lt;/math&gt;
|- align=center
|132||P4&lt;sub&gt;2&lt;/sub&gt;/mcm|| P 4&lt;sub&gt;2&lt;/sub&gt;/m 2/c 2/m || &lt;math&gt;D_{4h}^{10}&lt;/math&gt; || ''61a'' || &lt;math&gt;(c:a:a)\cdot m:4_2\cdot \tilde c&lt;/math&gt; || &lt;math&gt;[*{:}4{\cdot}4{:}2]&lt;/math&gt;
|- align=center
|133||P4&lt;sub&gt;2&lt;/sub&gt;/nbc|| P 4&lt;sub&gt;2&lt;/sub&gt;/n 2/b 2/c || &lt;math&gt;D_{4h}^{11}&lt;/math&gt; || ''63a'' || &lt;math&gt;(c:a:a)\cdot \widetilde{ab}:4_2\odot \tilde a&lt;/math&gt; || &lt;math&gt;(*4_24{:}2)&lt;/math&gt;
|- align=center
|134||P4&lt;sub&gt;2&lt;/sub&gt;/nnm|| P 4&lt;sub&gt;2&lt;/sub&gt;/n 2/n 2/m || &lt;math&gt;D_{4h}^{12}&lt;/math&gt; || ''62a'' || &lt;math&gt;(c:a:a)\cdot \widetilde{ab}:4_2\odot \widetilde{ac}&lt;/math&gt; || &lt;math&gt;(*4_24{\cdot}2)&lt;/math&gt;
|- align=center
|135||P4&lt;sub&gt;2&lt;/sub&gt;/mbc|| P 4&lt;sub&gt;2&lt;/sub&gt;/m 2&lt;sub&gt;1&lt;/sub&gt;/b 2/c || &lt;math&gt;D_{4h}^{13}&lt;/math&gt; || ''66a'' || &lt;math&gt;(c:a:a)\cdot m:4_2\odot \tilde a&lt;/math&gt; || &lt;math&gt;[4_2{*}{:}2]&lt;/math&gt;
|- align=center
|136||P4&lt;sub&gt;2&lt;/sub&gt;/mnm|| P 4&lt;sub&gt;2&lt;/sub&gt;/m 2&lt;sub&gt;1&lt;/sub&gt;/n 2/m || &lt;math&gt;D_{4h}^{14}&lt;/math&gt; || ''65a'' || &lt;math&gt;(c:a:a)\cdot m:4_2\odot \widetilde{ac}&lt;/math&gt; || &lt;math&gt;[4_2{*}{\cdot}2]&lt;/math&gt;
|- align=center
|137||P4&lt;sub&gt;2&lt;/sub&gt;/nmc|| P 4&lt;sub&gt;2&lt;/sub&gt;/n 2&lt;sub&gt;1&lt;/sub&gt;/m 2/c || &lt;math&gt;D_{4h}^{15}&lt;/math&gt; || ''67a'' || &lt;math&gt;(c:a:a)\cdot \widetilde{ab}:4_2\cdot m&lt;/math&gt; || &lt;math&gt;(*4{\cdot}4{:}2)&lt;/math&gt;
|- align=center
|138||P4&lt;sub&gt;2&lt;/sub&gt;/ncm|| P 4&lt;sub&gt;2&lt;/sub&gt;/n 2&lt;sub&gt;1&lt;/sub&gt;/c 2/m || &lt;math&gt;D_{4h}^{16}&lt;/math&gt; || ''65a'' || &lt;math&gt;(c:a:a)\cdot \widetilde{ab}:4_2\cdot \tilde c&lt;/math&gt; || &lt;math&gt;(*4{:}4{\cdot}2)&lt;/math&gt;
|- align=center
|139||I4/mmm|| I 4/m 2/m 2/m || &lt;math&gt;D_{4h}^{17}&lt;/math&gt; || ''37s'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) \cdot m:4\cdot m&lt;/math&gt; || &lt;math&gt;[*{\cdot}4{\cdot}4{:}2]&lt;/math&gt;
|- align=center
|140||I4/mcm|| I 4/m 2/c 2/m || &lt;math&gt;D_{4h}^{18}&lt;/math&gt; || ''38h'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) \cdot m:4\cdot \tilde c&lt;/math&gt; || &lt;math&gt;[*{\cdot}4{:}4{:}2]&lt;/math&gt;
|- align=center
|141||I4&lt;sub&gt;1&lt;/sub&gt;/amd|| I 4&lt;sub&gt;1&lt;/sub&gt;/a 2/m 2/d || &lt;math&gt;D_{4h}^{19}&lt;/math&gt; || ''59a'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) \cdot \tilde a :4_1\odot m&lt;/math&gt; || &lt;math&gt;(*4_14{\cdot}2)&lt;/math&gt;
|- align=center
|142||I4&lt;sub&gt;1&lt;/sub&gt;/acd|| I 4&lt;sub&gt;1&lt;/sub&gt;/a 2/c 2/d || &lt;math&gt;D_{4h}^{20}&lt;/math&gt; || ''58a'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/c:a:a\right ) \cdot \tilde a :4_1\odot \tilde c&lt;/math&gt; || &lt;math&gt;(*4_14{:}2)&lt;/math&gt;
|}

==List of Trigonal==
{| class=wikitable align=right
|+ Unit cells for trigonal crystal system
|-
!Rhombohedral&lt;BR&gt;(R)
!Hexagonal&lt;BR&gt;(P)
|- valign=top
|[[File:Hexagonal latticeR.svg|100px]]
|[[File:Hexagonal latticeFRONT.svg|100px]]
|}

{| class=wikitable
|+ [[Trigonal crystal system]]
!Number
![[Point group]]
![[Orbifold notation|Orbifold]]
!Short name
!Full name
![[Schoenflies notation|Schoenflies]]
![[Evgraf Fedorov|Fedorov]]
!Shubnikov
![[Fibrifold notation|Fibrifold]]
|- align=center
|143||rowspan=4|3||rowspan=4|&lt;math&gt;33&lt;/math&gt;||P3|| P 3 || &lt;math&gt;C_3^1&lt;/math&gt; || ''38s'' || &lt;math&gt;(c:(a/a)):3&lt;/math&gt; || &lt;math&gt;(3_03_03_0)&lt;/math&gt;
|- align=center
|144||P3&lt;sub&gt;1&lt;/sub&gt;|| P 3&lt;sub&gt;1&lt;/sub&gt; || &lt;math&gt;C_3^2&lt;/math&gt; || ''68a'' || &lt;math&gt;(c:(a/a)):3_1&lt;/math&gt; || &lt;math&gt;(3_13_13_1)&lt;/math&gt;
|- align=center
|145||P3&lt;sub&gt;2&lt;/sub&gt;|| P 3&lt;sub&gt;2&lt;/sub&gt; || &lt;math&gt;C_3^3&lt;/math&gt; || ''69a'' || &lt;math&gt;(c:(a/a)):3_2&lt;/math&gt; || &lt;math&gt;(3_13_13_1)&lt;/math&gt;
|- align=center
|146||R3|| R 3 || &lt;math&gt;C_3^4&lt;/math&gt; || ''39s'' || &lt;math&gt;(a/a/a)/3&lt;/math&gt; || &lt;math&gt;(3_03_13_2)&lt;/math&gt;
|- align=center
|147||rowspan=2|{{overline|3}}||rowspan=2|&lt;math&gt;3\times&lt;/math&gt;||P{{overline|3}}|| P {{overline|3}} || &lt;math&gt;C_{3i}^1&lt;/math&gt; || ''51s'' || &lt;math&gt;(c:(a/a)):\tilde 6&lt;/math&gt; || &lt;math&gt;(63_02)&lt;/math&gt;
|- align=center
|148||R{{overline|3}}|| R {{overline|3}} || &lt;math&gt;C_{3i}^2&lt;/math&gt; || ''52s'' || &lt;math&gt;(a/a/a)/\tilde 6&lt;/math&gt; || &lt;math&gt;(63_12)&lt;/math&gt;
|- align=center
|149||rowspan=7|32||rowspan=7|&lt;math&gt;223&lt;/math&gt;||P312|| P 3 1 2 || &lt;math&gt;D_3^1&lt;/math&gt; || ''45s'' || &lt;math&gt;(c:(a/a)):2:3&lt;/math&gt; || &lt;math&gt;(*3_03_03_0)&lt;/math&gt;
|- align=center
|150||P321|| P 3 2 1 || &lt;math&gt;D_3^2&lt;/math&gt; || ''44s'' || &lt;math&gt;(c:(a/a))\cdot 2:3&lt;/math&gt; || &lt;math&gt;(3_0{*}3_0)&lt;/math&gt;
|- align=center
|151||P3&lt;sub&gt;1&lt;/sub&gt;12|| P 3&lt;sub&gt;1&lt;/sub&gt; 1 2 || &lt;math&gt;D_3^3&lt;/math&gt; || ''72a'' || &lt;math&gt;(c:(a/a)):2:3_1&lt;/math&gt; || &lt;math&gt;(*3_13_13_1)&lt;/math&gt;
|- align=center
|152||P3&lt;sub&gt;1&lt;/sub&gt;21|| P 3&lt;sub&gt;1&lt;/sub&gt; 2 1 || &lt;math&gt;D_3^4&lt;/math&gt; || ''70a'' || &lt;math&gt;(c:(a/a))\cdot 2:3_1&lt;/math&gt; || &lt;math&gt;(3_1{*}3_1)&lt;/math&gt;
|- align=center
|153||P3&lt;sub&gt;2&lt;/sub&gt;12|| P 3&lt;sub&gt;2&lt;/sub&gt; 1 2 || &lt;math&gt;D_3^5&lt;/math&gt; || ''73a'' || &lt;math&gt;(c:(a/a)):2:3_2&lt;/math&gt; || &lt;math&gt;(*3_13_13_1)&lt;/math&gt;
|- align=center
|154||P3&lt;sub&gt;2&lt;/sub&gt;21|| P 3&lt;sub&gt;2&lt;/sub&gt; 2 1 || &lt;math&gt;D_3^6&lt;/math&gt; || ''71a'' || &lt;math&gt;(c:(a/a))\cdot 2:3_2&lt;/math&gt; || &lt;math&gt;(3_1{*}3_1)&lt;/math&gt;
|- align=center
|155||R32|| R 3 2 || &lt;math&gt;D_3^7&lt;/math&gt; || ''46s'' || &lt;math&gt;(a/a/a)/3:2&lt;/math&gt; || &lt;math&gt;(*3_03_13_2)&lt;/math&gt;
|- align=center
|156||rowspan=6|3m||rowspan=6|&lt;math&gt;*33&lt;/math&gt;||P3m1|| P 3 m 1 || &lt;math&gt;C_{3v}^1&lt;/math&gt; || ''40s'' || &lt;math&gt;(c:(a/a)):m\cdot 3&lt;/math&gt; || &lt;math&gt;(*{\cdot}3{\cdot}3{\cdot}3)&lt;/math&gt;
|- align=center
|157||P31m|| P 3 1 m || &lt;math&gt;C_{3v}^2&lt;/math&gt; || ''41s'' || &lt;math&gt;(c:(a/a))\cdot m\cdot 3&lt;/math&gt; || &lt;math&gt;(3_0{*}{\cdot}3)&lt;/math&gt;
|- align=center
|158||P3c1|| P 3 c 1 || &lt;math&gt;C_{3v}^3&lt;/math&gt; || ''39h'' || &lt;math&gt;(c:(a/a)):\tilde c:3&lt;/math&gt; || &lt;math&gt;(*{:}3{:}3{:}3)&lt;/math&gt;
|- align=center
|159||P31c|| P 3 1 c || &lt;math&gt;C_{3v}^4&lt;/math&gt; || ''40h'' || &lt;math&gt;(c:(a/a))\cdot\tilde c :3&lt;/math&gt; || &lt;math&gt;(3_0{*}{:}3)&lt;/math&gt;
|- align=center
|160||R3m|| R 3 m || &lt;math&gt;C_{3v}^5&lt;/math&gt; || ''42s'' || &lt;math&gt;(a/a/a)/3\cdot m&lt;/math&gt; || &lt;math&gt;(3_1{*}{\cdot}3)&lt;/math&gt;
|- align=center
|161||R3c|| R 3 c || &lt;math&gt;C_{3v}^6&lt;/math&gt; || ''41h'' || &lt;math&gt;(a/a/a)/3\cdot\tilde c&lt;/math&gt; || &lt;math&gt;(3_1{*}{:}3)&lt;/math&gt;
|- align=center
|162||rowspan=6|{{overline|3}} 2/m||rowspan=6|&lt;math&gt;2{*}3&lt;/math&gt;||P{{overline|3}}1m|| P {{overline|3}} 1 2/m || &lt;math&gt;D_{3d}^1&lt;/math&gt; || ''56s'' || &lt;math&gt;(c:(a/a))\cdot m\cdot\tilde 6&lt;/math&gt; || &lt;math&gt;(*{\cdot}63_02)&lt;/math&gt;
|- align=center
|163||P{{overline|3}}1c|| P {{overline|3}} 1 2/c || &lt;math&gt;D_{3d}^2&lt;/math&gt; || ''46h'' || &lt;math&gt;(c:(a/a))\cdot\tilde c \cdot\tilde 6&lt;/math&gt; || &lt;math&gt;(*{:}63_02)&lt;/math&gt;
|- align=center
|164||P{{overline|3}}m1|| P {{overline|3}} 2/m 1 || &lt;math&gt;D_{3d}^3&lt;/math&gt; || ''55s'' || &lt;math&gt;(c:(a/a)):m\cdot\tilde 6&lt;/math&gt; || &lt;math&gt;(*6{\cdot}3{\cdot}2)&lt;/math&gt;
|- align=center
|165||P{{overline|3}}c1|| P {{overline|3}} 2/c 1 || &lt;math&gt;D_{3d}^4&lt;/math&gt; || ''45h'' || &lt;math&gt;(c:(a/a)):\tilde c \cdot\tilde 6&lt;/math&gt; || &lt;math&gt;(*6{:}3{:}2)&lt;/math&gt;
|- align=center
|166||R{{overline|3}}m|| R {{overline|3}} 2/m || &lt;math&gt;D_{3d}^5&lt;/math&gt; || ''57s'' || &lt;math&gt;(a/a/a)/\tilde 6 \cdot m&lt;/math&gt; || &lt;math&gt;(*{\cdot}63_12)&lt;/math&gt;
|- align=center
|167||R{{overline|3}}c|| R {{overline|3}} 2/c || &lt;math&gt;D_{3d}^6&lt;/math&gt; || ''47h'' || &lt;math&gt;(a/a/a)/\tilde 6 \cdot\tilde c&lt;/math&gt; || &lt;math&gt;(*{:}63_12)&lt;/math&gt;
|}

==List of Hexagonal==
[[File:Hexagonal latticeFRONT.svg|100px|thumb|Hexagonal lattice cell&lt;BR&gt;(P)]]
{| class=wikitable
|+ [[Hexagonal crystal system]]
!Number
![[Point group]]
![[Orbifold notation|Orbifold]]
!Short name
!Full name
![[Schoenflies notation|Schoenflies]]
![[Evgraf Fedorov|Fedorov]]
! Shubnikov
![[Fibrifold notation|Fibrifold]]
|- align=center
|168||rowspan=6|6||rowspan=6|&lt;math&gt;66&lt;/math&gt;||P6|| P 6 || &lt;math&gt;C_6^1&lt;/math&gt; || ''49s'' || &lt;math&gt;(c:(a/a)):6&lt;/math&gt; || &lt;math&gt;(6_03_02_0)&lt;/math&gt;
|- align=center
|169||P6&lt;sub&gt;1&lt;/sub&gt;|| P 6&lt;sub&gt;1&lt;/sub&gt; || &lt;math&gt;C_6^2&lt;/math&gt; || ''74a'' || &lt;math&gt;(c:(a/a)):6_1&lt;/math&gt; || &lt;math&gt;(6_13_12_1)&lt;/math&gt;
|- align=center
|170||P6&lt;sub&gt;5&lt;/sub&gt;|| P 6&lt;sub&gt;5&lt;/sub&gt; || &lt;math&gt;C_6^3&lt;/math&gt; || ''75a'' || &lt;math&gt;(c:(a/a)):6_5&lt;/math&gt; || &lt;math&gt;(6_13_12_1)&lt;/math&gt;
|- align=center
|171||P6&lt;sub&gt;2&lt;/sub&gt;|| P 6&lt;sub&gt;2&lt;/sub&gt; || &lt;math&gt;C_6^4&lt;/math&gt; || ''76a'' || &lt;math&gt;(c:(a/a)):6_2&lt;/math&gt; || &lt;math&gt;(6_23_22_0)&lt;/math&gt;
|- align=center
|172||P6&lt;sub&gt;4&lt;/sub&gt;|| P 6&lt;sub&gt;4&lt;/sub&gt; || &lt;math&gt;C_6^5&lt;/math&gt; || ''77a'' || &lt;math&gt;(c:(a/a)):6_4&lt;/math&gt; || &lt;math&gt;(6_23_22_0)&lt;/math&gt;
|- align=center
|173||P6&lt;sub&gt;3&lt;/sub&gt;|| P 6&lt;sub&gt;3&lt;/sub&gt; || &lt;math&gt;C_6^6&lt;/math&gt; || ''78a'' || &lt;math&gt;(c:(a/a)):6_3&lt;/math&gt; || &lt;math&gt;(6_33_02_1)&lt;/math&gt;
|- align=center
|174||{{overline|6}}||&lt;math&gt;3*&lt;/math&gt;||P{{overline|6}}|| P {{overline|6}} || &lt;math&gt;C_{3h}^1&lt;/math&gt; || ''43s'' || &lt;math&gt;(c:(a/a)):3:m&lt;/math&gt; || &lt;math&gt;[3_03_03_0]&lt;/math&gt;
|- align=center
|175||rowspan=2|6/m||rowspan=2|&lt;math&gt;6*&lt;/math&gt;||P6/m|| P 6/m || &lt;math&gt;C_{6h}^1&lt;/math&gt; || ''53s'' || &lt;math&gt;(c:(a/a))\cdot m :6&lt;/math&gt; || &lt;math&gt;[6_03_02_0]&lt;/math&gt;
|- align=center
|176||P6&lt;sub&gt;3&lt;/sub&gt;/m|| P 6&lt;sub&gt;3&lt;/sub&gt;/m || &lt;math&gt;C_{6h}^2&lt;/math&gt; || ''81a'' || &lt;math&gt;(c:(a/a))\cdot m :6_3&lt;/math&gt; || &lt;math&gt;[6_33_02_1]&lt;/math&gt;
|- align=center
|177||rowspan=6|622||rowspan=6|&lt;math&gt;226&lt;/math&gt;||P622|| P 6 2 2 || &lt;math&gt;D_6^1&lt;/math&gt; || ''54s'' || &lt;math&gt;(c:(a/a))\cdot 2 :6&lt;/math&gt; || &lt;math&gt;(*6_03_02_0)&lt;/math&gt;
|- align=center
|178||P6&lt;sub&gt;1&lt;/sub&gt;22|| P 6&lt;sub&gt;1&lt;/sub&gt; 2 2 || &lt;math&gt;D_6^2&lt;/math&gt; || ''82a'' || &lt;math&gt;(c:(a/a))\cdot 2 :6_1&lt;/math&gt; || &lt;math&gt;(*6_13_12_1)&lt;/math&gt;
|- align=center
|179||P6&lt;sub&gt;5&lt;/sub&gt;22|| P 6&lt;sub&gt;5&lt;/sub&gt; 2 2 || &lt;math&gt;D_6^3&lt;/math&gt; || ''83a'' || &lt;math&gt;(c:(a/a))\cdot 2 :6_5&lt;/math&gt; || &lt;math&gt;(*6_13_12_1)&lt;/math&gt;
|- align=center
|180||P6&lt;sub&gt;2&lt;/sub&gt;22|| P 6&lt;sub&gt;2&lt;/sub&gt; 2 2 || &lt;math&gt;D_6^4&lt;/math&gt; || ''84a'' || &lt;math&gt;(c:(a/a))\cdot 2 :6_2&lt;/math&gt; || &lt;math&gt;(*6_23_22_0)&lt;/math&gt;
|- align=center
|181||P6&lt;sub&gt;4&lt;/sub&gt;22|| P 6&lt;sub&gt;4&lt;/sub&gt; 2 2 || &lt;math&gt;D_6^5&lt;/math&gt; || ''85a'' || &lt;math&gt;(c:(a/a))\cdot 2 :6_4&lt;/math&gt; || &lt;math&gt;(*6_23_22_0)&lt;/math&gt;
|- align=center
|182||P6&lt;sub&gt;3&lt;/sub&gt;22|| P 6&lt;sub&gt;3&lt;/sub&gt; 2 2 || &lt;math&gt;D_6^6&lt;/math&gt; || ''86a'' || &lt;math&gt;(c:(a/a))\cdot 2 :6_3&lt;/math&gt; || &lt;math&gt;(*6_33_02_1)&lt;/math&gt;
|- align=center
|183||rowspan=4|6mm||rowspan=4|&lt;math&gt;*66&lt;/math&gt;||P6mm|| P 6 m m || &lt;math&gt;C_{6v}^1&lt;/math&gt; || ''50s'' || &lt;math&gt;(c:(a/a)):m\cdot 6&lt;/math&gt; || &lt;math&gt;(*{\cdot}6{\cdot}3{\cdot}2)&lt;/math&gt;
|- align=center
|184||P6cc|| P 6 c c || &lt;math&gt;C_{6v}^2&lt;/math&gt; || ''44h'' || &lt;math&gt;(c:(a/a)):\tilde c \cdot 6&lt;/math&gt; || &lt;math&gt;(*{:}6{:}3{:}2)&lt;/math&gt;
|- align=center
|185||P6&lt;sub&gt;3&lt;/sub&gt;cm|| P 6&lt;sub&gt;3&lt;/sub&gt; c m || &lt;math&gt;C_{6v}^3&lt;/math&gt; || ''80a'' || &lt;math&gt;(c:(a/a)):\tilde c \cdot 6_3&lt;/math&gt; || &lt;math&gt;(*{\cdot}6{:}3{:}2)&lt;/math&gt;
|- align=center
|186||P6&lt;sub&gt;3&lt;/sub&gt;mc|| P 6&lt;sub&gt;3&lt;/sub&gt; m c || &lt;math&gt;C_{6v}^4&lt;/math&gt; || ''79a'' || &lt;math&gt;(c:(a/a)):m\cdot 6_3&lt;/math&gt; || &lt;math&gt;(*{:}6{\cdot}3{\cdot}2)&lt;/math&gt;
|- align=center
|187||rowspan=4|{{overline|6}}m2||rowspan=4|&lt;math&gt;*223&lt;/math&gt;||P{{overline|6}}m2|| P {{overline|6}} m 2 || &lt;math&gt;D_{3h}^1&lt;/math&gt; || ''48s'' || &lt;math&gt;(c:(a/a)):m\cdot 3:m&lt;/math&gt; || &lt;math&gt;[*{\cdot}3{\cdot}3{\cdot}3]&lt;/math&gt;
|- align=center
|188||P{{overline|6}}c2|| P {{overline|6}} c 2 || &lt;math&gt;D_{3h}^2&lt;/math&gt; || ''43h'' || &lt;math&gt;(c:(a/a)):\tilde c \cdot 3:m&lt;/math&gt; || &lt;math&gt;[*{:}3{:}3{:}3]&lt;/math&gt;
|- align=center
|189||P{{overline|6}}2m|| P {{overline|6}} 2 m || &lt;math&gt;D_{3h}^3&lt;/math&gt; || ''47s'' || &lt;math&gt;(c:(a/a))\cdot m:3\cdot m&lt;/math&gt; || &lt;math&gt;[3_0{*}{\cdot}3]&lt;/math&gt;
|- align=center
|190||P{{overline|6}}2c|| P {{overline|6}} 2 c || &lt;math&gt;D_{3h}^4&lt;/math&gt; || ''42h'' || &lt;math&gt;(c:(a/a))\cdot m:3\cdot \tilde c&lt;/math&gt; || &lt;math&gt;[3_0{*}{:}3]&lt;/math&gt;
|- align=center
|191||rowspan=4|6/m 2/m 2/m||rowspan=4|&lt;math&gt;*226&lt;/math&gt;||P6/mmm|| P 6/m 2/m 2/m || &lt;math&gt;D_{6h}^1&lt;/math&gt; || ''58s'' || &lt;math&gt;(c:(a/a))\cdot m:6\cdot m&lt;/math&gt; || &lt;math&gt;[*{\cdot}6{\cdot}3{\cdot}2]&lt;/math&gt;
|- align=center
|192||P6/mcc|| P 6/m 2/c 2/c || &lt;math&gt;D_{6h}^2&lt;/math&gt; || ''48h'' || &lt;math&gt;(c:(a/a))\cdot m:6\cdot\tilde c&lt;/math&gt; || &lt;math&gt;[*{:}6{:}3{:}2]&lt;/math&gt;
|- align=center
|193||P6&lt;sub&gt;3&lt;/sub&gt;/mcm|| P 6&lt;sub&gt;3&lt;/sub&gt;/m 2/c 2/m || &lt;math&gt;D_{6h}^3&lt;/math&gt; || ''87a'' || &lt;math&gt;(c:(a/a))\cdot m:6_3\cdot\tilde c&lt;/math&gt; || &lt;math&gt;[*{\cdot}6{:}3{:}2]&lt;/math&gt;
|- align=center
|194||P6&lt;sub&gt;3&lt;/sub&gt;/mmc|| P 6&lt;sub&gt;3&lt;/sub&gt;/m 2/m 2/c || &lt;math&gt;D_{6h}^4&lt;/math&gt; || ''88a'' || &lt;math&gt;(c:(a/a))\cdot m:6_3\cdot m&lt;/math&gt; || &lt;math&gt;[*{:}6{\cdot}3{\cdot}2]&lt;/math&gt;
|}

==List of Cubic==
{| class=wikitable align=center
|+ Cubic Bravais lattice
|-
!Simple&lt;BR&gt;(P)
!Body centered&lt;BR&gt;(I)
!Face centered&lt;BR&gt;(F)
|-
|[[Image:Lattic_simple_cubic.svg|100px]]
|[[Image:Lattice_body_centered_cubic.svg|100px]]
|[[Image:Lattice_face_centered_cubic.svg|100px]]
|}
[[Image:CsCl crystal.png|125px|thumb|(221) [[Caesium chloride]]. Different colors for the two atom types.]]
[[Image:Sphalerite-unit-cell-depth-fade-3D-balls.png|150px|thumb|(216) [[Sphalerite]]]]
[[File:12-14-hedral honeycomb.png|150px|thumb|(223) [[Weaire–Phelan structure]]]]

{| class=wikitable
|+ [[Cubic crystal system]]
!Number
![[Point group]]
![[Orbifold notation|Orbifold]]
!Short name
!Full name
![[Schoenflies notation|Schoenflies]]
![[Evgraf Fedorov|Fedorov]]
! Shubnikov
! Conway
! Fibrifold (preserving &lt;math&gt;z&lt;/math&gt;)
! Fibrifold (preserving &lt;math&gt;x&lt;/math&gt;, &lt;math&gt;y&lt;/math&gt;, &lt;math&gt;z&lt;/math&gt;)
|- align=center
|195||rowspan=5|23||rowspan=5|&lt;math&gt;332&lt;/math&gt;||P23|| P 2 3 || &lt;math&gt;T^1&lt;/math&gt; || ''59s'' || &lt;math&gt;\left ( a:a:a\right ) :2/3&lt;/math&gt; || &lt;math&gt;2^\circ&lt;/math&gt; || &lt;math&gt;(*2_02_02_02_0){:}3&lt;/math&gt; || &lt;math&gt;(*2_02_02_02_0){:}3&lt;/math&gt;
|- align=center
|196||F23|| F 2 3 || &lt;math&gt;T^2&lt;/math&gt; || ''61s'' || &lt;math&gt;\left ( \tfrac{a+c}{2}/\tfrac{b+c}{2}/\tfrac{a+b}{2}:a:a:a\right ) :2/3&lt;/math&gt; || &lt;math&gt;1^\circ&lt;/math&gt; || &lt;math&gt;(*2_02_12_02_1){:}3&lt;/math&gt; || &lt;math&gt;(*2_02_12_02_1){:}3&lt;/math&gt;
|- align=center
|197||I23|| I 2 3 || &lt;math&gt;T^3&lt;/math&gt; || ''60s'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/a:a:a\right ) :2/3&lt;/math&gt;|| &lt;math&gt;4^{\circ\circ}&lt;/math&gt; || &lt;math&gt;(2_1{*}2_02_0){:}3&lt;/math&gt; || &lt;math&gt;(2_1{*}2_02_0){:}3&lt;/math&gt;
|- align=center
|198||P2&lt;sub&gt;1&lt;/sub&gt;3|| P 2&lt;sub&gt;1&lt;/sub&gt; 3 || &lt;math&gt;T^4&lt;/math&gt; || ''89a'' || &lt;math&gt;\left ( a:a:a\right ) :2_1/3&lt;/math&gt;|| &lt;math&gt;1^\circ/4&lt;/math&gt; || &lt;math&gt;(2_12_1\bar{\times}){:}3&lt;/math&gt; || &lt;math&gt;(2_12_1\bar{\times}){:}3&lt;/math&gt;
|- align=center
|199||I2&lt;sub&gt;1&lt;/sub&gt;3|| I 2&lt;sub&gt;1&lt;/sub&gt; 3 || &lt;math&gt;T^5&lt;/math&gt; || ''90a'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/a:a:a\right ) :2_1/3&lt;/math&gt;|| &lt;math&gt;2^\circ/4&lt;/math&gt; || &lt;math&gt;(2_0{*}2_12_1){:}3&lt;/math&gt; || &lt;math&gt;(2_0{*}2_12_1){:}3&lt;/math&gt;
|- align=center
|200||rowspan=7|2/m {{overline|3}}||rowspan=7|&lt;math&gt;3{*}2&lt;/math&gt;||Pm{{overline|3}}|| P 2/m {{overline|3}} || &lt;math&gt;T_h^1&lt;/math&gt; || ''62s'' || &lt;math&gt;\left ( a:a:a\right ) \cdot m/ \tilde 6&lt;/math&gt; || &lt;math&gt;4^-&lt;/math&gt; || &lt;math&gt;[*{\cdot}2{\cdot}2{\cdot}2{\cdot}2]{:}3&lt;/math&gt; || &lt;math&gt;[*{\cdot}2{\cdot}2{\cdot}2{\cdot}2]{:}3&lt;/math&gt;
|- align=center
|201||Pn{{overline|3}}|| P 2/n {{overline|3}} || &lt;math&gt;T_h^2&lt;/math&gt; || ''49h'' ||&lt;math&gt;\left ( a:a:a\right ) \cdot \widetilde{ab} / \tilde 6&lt;/math&gt; || &lt;math&gt;4^{\circ+}&lt;/math&gt; || &lt;math&gt;(2\bar{*}_12_02_0){:}3&lt;/math&gt; || &lt;math&gt;(2\bar{*}_12_02_0){:}3&lt;/math&gt;
|- align=center
|202||Fm{{overline|3}}|| F 2/m {{overline|3}} || &lt;math&gt;T_h^3&lt;/math&gt; || ''64s'' || &lt;math&gt;\left ( \tfrac{a+c}{2}/\tfrac{b+c}{2}/\tfrac{a+b}{2}:a:a:a\right ) \cdot m/ \tilde 6&lt;/math&gt; || &lt;math&gt;2^-&lt;/math&gt; || &lt;math&gt;[*{\cdot}2{\cdot}2{:}2{:}2]{:}3&lt;/math&gt; || &lt;math&gt;[*{\cdot}2{\cdot}2{:}2{:}2]{:}3&lt;/math&gt;
|- align=center
|203||Fd{{overline|3}}|| F 2/d {{overline|3}} || &lt;math&gt;T_h^4&lt;/math&gt; || ''50h'' || &lt;math&gt;\left ( \tfrac{a+c}{2}/\tfrac{b+c}{2}/\tfrac{a+b}{2}:a:a:a\right ) \cdot \tfrac{1}{2}\widetilde{ab} / \tilde 6&lt;/math&gt; || &lt;math&gt;2^{\circ+}&lt;/math&gt; || &lt;math&gt;(2\bar{*}2_02_1){:}3&lt;/math&gt; || &lt;math&gt;(2\bar{*}2_02_1){:}3&lt;/math&gt;
|- align=center
|204||Im{{overline|3}}|| I 2/m {{overline|3}} || &lt;math&gt;T_h^5&lt;/math&gt; || ''63s'' ||  &lt;math&gt;\left ( \tfrac{a+b+c}{2}/a:a:a\right ) \cdot m/\tilde 6&lt;/math&gt; || &lt;math&gt;8^{-\circ}&lt;/math&gt; || &lt;math&gt;[2_1{*}{\cdot}2{\cdot}2]{:}3&lt;/math&gt; || &lt;math&gt;[2_1{*}{\cdot}2{\cdot}2]{:}3&lt;/math&gt;
|- align=center
|205||Pa{{overline|3}}|| P 2&lt;sub&gt;1&lt;/sub&gt;/a {{overline|3}} || &lt;math&gt;T_h^6&lt;/math&gt; || ''91a'' || &lt;math&gt;\left ( a:a:a\right ) \cdot \tilde a /\tilde 6&lt;/math&gt; || &lt;math&gt;2^-/4&lt;/math&gt; || &lt;math&gt;(2_12\bar{*}{:}){:}3)&lt;/math&gt; || &lt;math&gt;(2_12\bar{*}{:}){:}3)&lt;/math&gt;
|- align=center
|206||Ia{{overline|3}}|| I 2&lt;sub&gt;1&lt;/sub&gt;/a {{overline|3}} || &lt;math&gt;T_h^7&lt;/math&gt;  || ''92a'' ||  &lt;math&gt;\left ( \tfrac{a+b+c}{2}/a:a:a\right ) \cdot \tilde a /\tilde 6&lt;/math&gt; || &lt;math&gt;4^-/4&lt;/math&gt; || &lt;math&gt;(*2_12{:}2{:}2){:}3&lt;/math&gt; || &lt;math&gt;(*2_12{:}2{:}2){:}3&lt;/math&gt;
|- align=center
|207||rowspan=8|432||rowspan=8|&lt;math&gt;432&lt;/math&gt;||P432|| P 4 3 2 || &lt;math&gt;O^1&lt;/math&gt; || ''68s'' || &lt;math&gt;\left ( a:a:a\right ) :4/3&lt;/math&gt; || &lt;math&gt;4^{\circ-}&lt;/math&gt; || &lt;math&gt;(*4_04_02_0){:}3&lt;/math&gt; || &lt;math&gt;(*2_02_02_02_0){:}6&lt;/math&gt;
|- align=center
|208||P4&lt;sub&gt;2&lt;/sub&gt;32|| P 4&lt;sub&gt;2&lt;/sub&gt; 3 2 || &lt;math&gt;O^2&lt;/math&gt; || ''98a'' || &lt;math&gt;\left ( a:a:a\right ) :4_2//3&lt;/math&gt; || &lt;math&gt;4^+&lt;/math&gt; || &lt;math&gt;(*4_24_22_0){:}3&lt;/math&gt; || &lt;math&gt;(*2_02_02_02_0){:}6&lt;/math&gt;
|- align=center
|209||F432|| F 4 3 2 || &lt;math&gt;O^3&lt;/math&gt; || ''70s'' || &lt;math&gt;\left ( \tfrac{a+c}{2}/\tfrac{b+c}{2}/\tfrac{a+b}{2}:a:a:a\right ) :4/3&lt;/math&gt; || &lt;math&gt;2^{\circ-}&lt;/math&gt; || &lt;math&gt;(*4_24_02_1){:}3&lt;/math&gt; || &lt;math&gt;(*2_02_12_02_1){:}6&lt;/math&gt;
|- align=center
|210||F4&lt;sub&gt;1&lt;/sub&gt;32|| F 4&lt;sub&gt;1&lt;/sub&gt; 3 2 || &lt;math&gt;O^4&lt;/math&gt; || ''97a'' || &lt;math&gt;\left ( \tfrac{a+c}{2}/\tfrac{b+c}{2}/\tfrac{a+b}{2}:a:a:a\right ) :4_1//3&lt;/math&gt; || &lt;math&gt;2^+&lt;/math&gt; || &lt;math&gt;(*4_34_12_0){:}3&lt;/math&gt; || &lt;math&gt;(*2_02_12_02_1){:}6&lt;/math&gt;
|- align=center
|211||I432|| I 4 3 2 || &lt;math&gt;O^5&lt;/math&gt; || ''69s'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/a:a:a\right ) :4/3&lt;/math&gt; || &lt;math&gt;8^{+\circ}&lt;/math&gt; || &lt;math&gt;(4_24_02_1){:3}&lt;/math&gt; || &lt;math&gt;(2_1{*}2_02_0){:}6&lt;/math&gt;
|- align=center
|212||P4&lt;sub&gt;3&lt;/sub&gt;32|| P 4&lt;sub&gt;3&lt;/sub&gt; 3 2 || &lt;math&gt;O^6&lt;/math&gt; || ''94a'' || &lt;math&gt;\left ( a:a:a\right ) :4_3//3&lt;/math&gt; || &lt;math&gt;2^+/4&lt;/math&gt; || &lt;math&gt;(4_1{*}2_1){:}3&lt;/math&gt; || &lt;math&gt;(2_12_1\bar{\times}){:}6&lt;/math&gt;
|- align=center
|213||P4&lt;sub&gt;1&lt;/sub&gt;32|| P 4&lt;sub&gt;1&lt;/sub&gt; 3 2 || &lt;math&gt;O^7&lt;/math&gt; || ''95a'' || &lt;math&gt;\left ( a:a:a\right ) :4_1//3&lt;/math&gt; || &lt;math&gt;2^+/4&lt;/math&gt; || &lt;math&gt;(4_1{*}2_1){:}3&lt;/math&gt; || &lt;math&gt;(2_12_1\bar{\times}){:}6&lt;/math&gt;
|- align=center
|214||I4&lt;sub&gt;1&lt;/sub&gt;32|| I 4&lt;sub&gt;1&lt;/sub&gt; 3 2 || &lt;math&gt;O^8&lt;/math&gt; ||''96a'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/:a:a:a\right ) :4_1//3&lt;/math&gt; || &lt;math&gt;4^+/4&lt;/math&gt; || &lt;math&gt;(*4_34_12_0){:}3&lt;/math&gt; || &lt;math&gt;(2_0{*}2_12_1){:}6&lt;/math&gt;
|- align=center
|215||rowspan=6|{{overline|4}}3m||rowspan=6|&lt;math&gt;*332&lt;/math&gt;||P{{overline|4}}3m|| P {{overline|4}} 3 m || &lt;math&gt;T_d^1&lt;/math&gt; || ''65s'' || &lt;math&gt;\left ( a:a:a\right ) :\tilde 4 /3&lt;/math&gt; || &lt;math&gt;2^\circ{:}2&lt;/math&gt; || &lt;math&gt;(*4{\cdot}42_0){:}3&lt;/math&gt; || &lt;math&gt;(*2_02_02_02_0){:}6&lt;/math&gt;
|- align=center
|216||F{{overline|4}}3m|| F {{overline|4}} 3 m || &lt;math&gt;T_d^2&lt;/math&gt; || ''67s'' || &lt;math&gt;\left ( \tfrac{a+c}{2}/\tfrac{b+c}{2}/\tfrac{a+b}{2}:a:a:a\right ) :\tilde 4 /3&lt;/math&gt; || &lt;math&gt;1^\circ{:}2&lt;/math&gt; || &lt;math&gt;(*4{\cdot}42_1){:}3&lt;/math&gt; || &lt;math&gt;(*2_02_12_02_1){:}6&lt;/math&gt;
|- align=center
|217||I{{overline|4}}3m|| I {{overline|4}} 3 m || &lt;math&gt;T_d^3&lt;/math&gt;  || ''66s'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/a:a:a\right ) :\tilde 4 /3&lt;/math&gt; || &lt;math&gt;4^\circ{:}2&lt;/math&gt; || &lt;math&gt;(*{\cdot}44{:}2){:}3&lt;/math&gt; || &lt;math&gt;(2_1{*}2_02_0){:}6&lt;/math&gt;
|- align=center
|218||P{{overline|4}}3n|| P {{overline|4}} 3 n || &lt;math&gt;T_d^4&lt;/math&gt; || ''51h'' || &lt;math&gt;\left ( a:a:a\right ) :\tilde 4 //3&lt;/math&gt; || &lt;math&gt;4^\circ&lt;/math&gt; || &lt;math&gt;(*4{:}42_0){:}3&lt;/math&gt; || &lt;math&gt;(*2_02_02_02_0){:}6&lt;/math&gt;
|- align=center
|219||F{{overline|4}}3c|| F {{overline|4}} 3 c || &lt;math&gt;T_d^5&lt;/math&gt; || ''52h'' || &lt;math&gt;\left ( \tfrac{a+c}{2}/\tfrac{b+c}{2}/\tfrac{a+b}{2}:a:a:a\right ) :\tilde 4 //3&lt;/math&gt; || &lt;math&gt;2^{\circ\circ}&lt;/math&gt; || &lt;math&gt;(*4{:}42_1){:}3&lt;/math&gt; || &lt;math&gt;(*2_02_12_02_1){:}6&lt;/math&gt;
|- align=center
|220||I{{overline|4}}3d|| I {{overline|4}} 3 d || &lt;math&gt;T_d^6&lt;/math&gt; || ''93a'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/a:a:a\right ) :\tilde 4 //3&lt;/math&gt; || &lt;math&gt;4^\circ/4&lt;/math&gt; || &lt;math&gt;(4\bar{*}2_1){:}3&lt;/math&gt; || &lt;math&gt;(2_0{*}2_12_1){:}6&lt;/math&gt;
|- align=center
|221||rowspan=10|4/m {{overline|3}} 2/m||rowspan=10|&lt;math&gt;*432&lt;/math&gt;||Pm{{overline|3}}m|| P 4/m {{overline|3}} 2/m || &lt;math&gt;O_h^1&lt;/math&gt; || ''71s'' || &lt;math&gt;\left ( a:a:a\right ) :4/\tilde 6 \cdot m&lt;/math&gt; || &lt;math&gt;4^-{:}2&lt;/math&gt; || &lt;math&gt;[*{\cdot}4{\cdot}4{\cdot}2]{:}3&lt;/math&gt; || &lt;math&gt;[*{\cdot}2{\cdot}2{\cdot}2{\cdot}2]{:}6&lt;/math&gt;
|- align=center
|222||Pn{{overline|3}}n|| P 4/n {{overline|3}} 2/n || &lt;math&gt;O_h^2&lt;/math&gt; || ''53h'' || &lt;math&gt;\left ( a:a:a\right ) :4/\tilde 6 \cdot \widetilde{abc}&lt;/math&gt; || &lt;math&gt;8^{\circ\circ}&lt;/math&gt; || &lt;math&gt;(*4_04{:}2){:}3&lt;/math&gt; || &lt;math&gt;(2\bar{*}_12_02_0){:}6&lt;/math&gt;
|- align=center
|223||Pm{{overline|3}}n|| P 4&lt;sub&gt;2&lt;/sub&gt;/m {{overline|3}} 2/n || &lt;math&gt;O_h^3&lt;/math&gt; || ''102a'' || &lt;math&gt;\left ( a:a:a\right ) :4_2//\tilde 6 \cdot \widetilde{abc}&lt;/math&gt; || &lt;math&gt;8^\circ&lt;/math&gt; || &lt;math&gt;[*{\cdot}4{:}4{\cdot}2]{:}3&lt;/math&gt; || &lt;math&gt;[*{\cdot}2{\cdot}2{\cdot}2{\cdot}2]{:}6&lt;/math&gt;
|- align=center
|224||Pn{{overline|3}}m|| P 4&lt;sub&gt;2&lt;/sub&gt;/n {{overline|3}} 2/m || &lt;math&gt;O_h^4&lt;/math&gt; || ''103a'' || &lt;math&gt;\left ( a:a:a\right ) :4_2//\tilde 6 \cdot m&lt;/math&gt; || &lt;math&gt;4^+{:}2&lt;/math&gt; || &lt;math&gt;(*4_24{\cdot}2){:}3&lt;/math&gt; || &lt;math&gt;(2\bar{*}_12_02_0){:}6&lt;/math&gt;
|- align=center
|225||Fm{{overline|3}}m|| F 4/m {{overline|3}} 2/m || &lt;math&gt;O_h^5&lt;/math&gt; || ''73s'' || &lt;math&gt;\left ( \tfrac{a+c}{2}/\tfrac{b+c}{2}/\tfrac{a+b}{2}:a:a:a\right ) :4/\tilde 6 \cdot m&lt;/math&gt; || &lt;math&gt;2^-{:}2&lt;/math&gt; || &lt;math&gt;[*{\cdot}4{\cdot}4{:}2]{:}3&lt;/math&gt; || &lt;math&gt;[*{\cdot}2{\cdot}2{:}2{:}2]{:}6&lt;/math&gt;
|- align=center
|226||Fm{{overline|3}}c|| F 4/m {{overline|3}} 2/c || &lt;math&gt;O_h^6&lt;/math&gt; || ''54h'' || &lt;math&gt;\left ( \tfrac{a+c}{2}/\tfrac{b+c}{2}/\tfrac{a+b}{2}:a:a:a\right ) :4/\tilde 6 \cdot \tilde c&lt;/math&gt; || &lt;math&gt;4^{--}&lt;/math&gt; || &lt;math&gt;[*{\cdot}4{:}4{:}2]{:}3&lt;/math&gt; || &lt;math&gt;[*{\cdot}2{\cdot}2{:}2{:}2]{:}6&lt;/math&gt;
|- align=center
|227||Fd{{overline|3}}m|| F 4&lt;sub&gt;1&lt;/sub&gt;/d {{overline|3}} 2/m || &lt;math&gt;O_h^7&lt;/math&gt; || ''100a'' || &lt;math&gt;\left ( \tfrac{a+c}{2}/\tfrac{b+c}{2}/\tfrac{a+b}{2}:a:a:a\right ) :4_1//\tilde 6 \cdot m&lt;/math&gt; || &lt;math&gt;2^+{:}2&lt;/math&gt; || &lt;math&gt;(*4_14{\cdot}2){:}3&lt;/math&gt; || &lt;math&gt;(2\bar{*}2_02_1){:}6&lt;/math&gt;
|- align=center
|228||Fd{{overline|3}}c|| F 4&lt;sub&gt;1&lt;/sub&gt;/d {{overline|3}} 2/c || &lt;math&gt;O_h^8&lt;/math&gt; || ''101a'' || &lt;math&gt;\left ( \tfrac{a+c}{2}/\tfrac{b+c}{2}/\tfrac{a+b}{2}:a:a:a\right ) :4_1//\tilde 6 \cdot \tilde c&lt;/math&gt; || &lt;math&gt;4^{++}&lt;/math&gt; || &lt;math&gt;(*4_14{:}2){:}3&lt;/math&gt; || &lt;math&gt;(2\bar{*}2_02_1){:}6&lt;/math&gt;
|- align=center
|229||Im{{overline|3}}m|| I 4/m {{overline|3}} 2/m || &lt;math&gt;O_h^9&lt;/math&gt; || ''72s'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/a:a:a\right ) :4/\tilde 6 \cdot m&lt;/math&gt; || &lt;math&gt;8^\circ{:}2&lt;/math&gt; || &lt;math&gt;[*{\cdot}4{\cdot}4{:}2]{:}3&lt;/math&gt; || &lt;math&gt;[2_1{*}{\cdot}2{\cdot}2]{:}6&lt;/math&gt;
|- align=center
|230||Ia{{overline|3}}d|| I 4&lt;sub&gt;1&lt;/sub&gt;/a {{overline|3}} 2/d || &lt;math&gt;O_h^{10}&lt;/math&gt; || ''99a'' || &lt;math&gt;\left ( \tfrac{a+b+c}{2}/a:a:a\right ) :4_1//\tilde 6 \cdot \tfrac{1}{2}\widetilde{abc}&lt;/math&gt; || &lt;math&gt;8^\circ/4&lt;/math&gt; || &lt;math&gt;(*4_14{:}2){:}3&lt;/math&gt; || &lt;math&gt;(*2_12{:}2{:}2){:}6&lt;/math&gt;
|}

==External links==
{{commons category|Space groups}}
* [http://www.iucr.org International Union of Crystallography]
* [http://neon.mems.cmu.edu/degraef/pointgroups/ Point Groups and Bravais Lattices]
* [http://img.chem.ucl.ac.uk/sgp/mainmenu.htm Full list of 230 crystallographic space groups]
* [https://www.emis.de/journals/BAG/vol.42/no.2/b42h2con.pdf Conway et al. on fibrifold notation]

[[Category:Symmetry]]
[[Category:Crystallography]]</text>
      <sha1>1iz8o0decw7ro0vir22w1zz16pmmj11</sha1>
    </revision>
  </page>
  <page>
    <title>Meta-Object Facility</title>
    <ns>0</ns>
    <id>145438</id>
    <revision>
      <id>841279619</id>
      <parentid>822460851</parentid>
      <timestamp>2018-05-14T22:27:35Z</timestamp>
      <contributor>
        <username>DeprecatedFixerBot</username>
        <id>33330201</id>
      </contributor>
      <minor/>
      <comment>Removed deprecated parameter(s) from [[Template:Div col]] using [[User:DeprecatedFixerBot| DeprecatedFixerBot]]. Questions? See [[Template:Div col#Usage of "cols" parameter]] or [[User talk:TheSandDoctor|msg TSD!]] (please mention that this is task #2!))</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6926">[[Image:M0-m3.png|thumb|320px|Illustration of the Meta-Object Facility.]]
The '''Meta-Object Facility''' ('''MOF''') is an [[Object Management Group]] (OMG) standard for [[model-driven engineering]]. Its purpose is to provide a [[type system]] for entities in the [[CORBA]] architecture and a set of interfaces through which those types can be created and manipulated. The official reference page may be found at OMG's website.&lt;ref&gt;[http://www.omg.org/mof/ OMG's MetaObject Facility]&lt;/ref&gt;

== Overview ==
MOF was developed to provide a [[type system]] for use in the [[CORBA]] architecture, a set of schemas by which the structure, meaning and behaviour of objects could be defined, and a set of CORBA interfaces through which these schemas could be created, stored and manipulated.&lt;ref&gt;{{cite web|title=Common Facilities RFP-5: Meta-Object Facility|url=http://www.omg.org/cgi-bin/doc?cf/96-05-02.pdf|publisher=Object Management Group|accessdate=14 January 2014|date=2 May 1996}}&lt;/ref&gt; 

MOF is designed as a four-layered architecture. It provides a meta-meta model at the top layer, called the M3 layer. This M3-model is the language used by MOF to build metamodels, called M2-models. The most prominent example of a Layer 2 MOF model is the UML metamodel, the model that describes the UML itself. These M2-models describe elements of the M1-layer, and thus M1-models. These would be, for example, models written in UML. The last layer is the M0-layer or data layer. It is used to describe real-world objects.

Beyond the M3-model, MOF describes the means to create and manipulate models and metamodels by defining [[CORBA]] interfaces that describe those operations. Because of the similarities between the MOF M3-model and UML structure models, MOF metamodels are usually modeled as UML class diagrams. A supporting standard of MOF is [[XML Metadata Interchange|XMI]], which defines an XML-based exchange format for models on the M3-, M2-, or M1-Layer.

== Metamodeling architecture ==
MOF is a ''closed'' metamodeling architecture; it defines an M3-model, which conforms to itself. MOF allows a ''strict'' meta-modeling architecture; every model element on every layer is strictly in correspondence with a model element of the layer above. MOF only provides a means to define the structure, or [[abstract syntax]] of a language or of data. For defining metamodels, MOF plays exactly the role that [[EBNF]] plays for defining programming language grammars. MOF is a [[Domain Specific Language]] (DSL) used to define metamodels, just as EBNF is a DSL for defining grammars. Similarly to EBNF, MOF could be defined in MOF.

In short MOF uses the notion of '''MOF::Classes''' (not to be confused with '''UML::Classes'''), as known from [[Object-oriented programming|object orientation]], to define concepts (model elements) on a metalayer. MOF may be used to define object-oriented metamodels (as [[Unified Modeling Language|UML]] for example) as well as non object-oriented metamodels (as a [[Petri net]] or a [[Web Service]] metamodel). 

As of May 2006, the [[Object Management Group|OMG]] has defined two compliance points for MOF:
*EMOF for Essential MOF&lt;ref name=mof241&gt;{{cite web|title=OMG Meta-Object Facility (MOF) Core Specification, Version 2.4.1|url=http://www.omg.org/spec/MOF/2.4.1/PDF|publisher=[[Object Management Group]]|accessdate=17 February 2014|page=1}}&lt;/ref&gt; 
*CMOF for Complete MOF&lt;ref name=mof241/&gt;

In June 2006, a ''request for proposal'' was issued by OMG for a third variant, SMOF (Semantic MOF).

The variant '''ECore''' that has been defined in the '''[[Eclipse Modeling Framework]]''' is more or less aligned on OMG's EMOF. 

Another related standard is [[Object Constraint Language|OCL]], which describes a formal language that can be used to define model constraints in terms of [[predicate logic]].

[[QVT]], which introduces means to query, view and transform MOF-based models, is a very important standard, approved in 2005.  See [[Model Transformation Language]] for further information.

== International standard ==
MOF is an international standard:
;MOF 2.4.2
:[[International Organization for Standardization|ISO]]/[[International Electrotechnical Commission|IEC]] 19508:2014 Information technology — Object Management Group Meta Object Facility (MOF) Core
;MOF 1.4.1
:[[International Organization for Standardization|ISO]]/[[International Electrotechnical Commission|IEC]] 19502:2005 Information technology — Meta Object Facility (MOF)

MOF can be viewed as a standard to write [[Metamodeling|metamodels]], for example in order to model the abstract syntax of [[Domain Specific Language]]s. [[Kermeta]] is an extension to MOF allowing executable actions to be attached to EMOF meta-models, hence making it possible to also model a DSL operational semantics and readily obtain an interpreter for it.

[[Java Metadata Interface|JMI]] defines a Java API for manipulating MOF models.

OMG's MOF is not to be confused with the Managed Object Format (MOF) defined by the [[Distributed Management Task Force]] (DMTF) in section 6 of the Common Information Model (CIM) Infrastructure Specification, version 2.5.0.&lt;ref&gt;[http://www.dmtf.org/standards/published_documents/DSP0004_2.5.0.pdf Common Interface Model (CIM) Interface Structure, version 2.5.0]&lt;/ref&gt;

== See also ==
{{div col|colwidth=22em}}
* [[Common Warehouse Metamodel]]
* [[Domain-specific language]]
* [[Kermeta]]
* [[KM3]]
* [[Metamodeling]]
* [[Metadata]]
* [[Model-driven architecture]]
* [[OGML]]
* [[Platform-independent model]]
* [[Platform-specific model]]
* [[QVT]]
* [[SPEM]]
* [[XML Metadata Interchange]]
{{div col end}}

==References==
{{reflist}}
{{Refimprove|date=February 2009}}

== Further reading ==
*[http://www.omg.org/spec/MOF/ Official MOF specification from OMG]
*Ralph Sobek, [https://web.archive.org/web/20050105221436/http://www.irit.fr/~Ralph.Sobek/neptune/mof_2_0.shtml MOF Specifications Documents]
*Johannes Ernst, [http://infogrid.org/wiki/Reference/WhatIsMetaModeling ''What is metamodeling?'']
*Woody Pidcock, [http://infogrid.org/wiki/Reference/PidcockArticle ''What are the differences between a vocabulary, a taxonomy, a thesaurus, an ontology, and a meta-model?'']
*Anna Gerber and Kerry Raymond, [http://portal.acm.org/ft_gateway.cfm?id=965673&amp;type=pdf MOF to EMF and Back Again.]
*[http://www.irisa.fr/triskell/publis/2005/Muller05a.pdf Weaving Executability into Object-Oriented Meta-Languages]
*[http://www.omg.org/cgi-bin/doc?ad/06-06-08 MOF Support for Semantic Structures RFP] Request For Proposal on SMOF

== External links ==
{{Commonscat|Meta-Object Facility}}
* [http://www.omg.org Object Management Group]
* [http://www.omg.org/mof/ OMG's MetaObject Facility]
* [http://www.metamodels.de M3Actions: MOF Operational Semantics]

{{ISO standards}}

[[Category:Specification languages]]
[[Category:Data modeling]]
[[Category:Unified Modeling Language]]
[[Category:ISO standards]]</text>
      <sha1>4j8r3k6b5781my8f4ox7dgum95p35hh</sha1>
    </revision>
  </page>
  <page>
    <title>No-broadcast theorem</title>
    <ns>0</ns>
    <id>6387477</id>
    <revision>
      <id>859027267</id>
      <parentid>822442284</parentid>
      <timestamp>2018-09-11T07:09:08Z</timestamp>
      <contributor>
        <username>Mikhail Ryazanov</username>
        <id>13263935</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2835">In [[physics]], the '''no-broadcast theorem''' is a result of [[quantum information science|quantum information theory]]. In the case of [[pure state|pure quantum states]], it is a [[theorem|corollary]] of the [[no-cloning theorem]]: since [[quantum state]]s cannot be copied in general, they cannot be broadcast.  Here, the word "broadcast" is used in the sense of conveying the state to two or more recipients.  For multiple recipients to each receive the state, there must be, in some sense, a way of duplicating the state.  The no-broadcast theorem generalizes the no-cloning theorem for [[mixed state (physics)|mixed states]].

The no-cloning theorem says that it is impossible to create two copies of an unknown state given a single copy of the state. 

The no-broadcast theorem says that, given a single copy of a state drawn from a restricted non-commuting set, it is impossible to create a state such that one part of it is the same as the original state and the other part is also the same as the original state.  That is, given an initial [[Mixed quantum state|state]] &lt;math&gt;\rho_1,&lt;/math&gt; it is impossible to create a state &lt;math&gt;\rho_{AB}&lt;/math&gt; in a [[Hilbert space]] &lt;math&gt;H_A \otimes H_B&lt;/math&gt; such that the [[partial trace]] &lt;math&gt;Tr_A\rho_{AB} = \rho_1&lt;/math&gt; and &lt;math&gt;Tr_B\rho_{AB} = \rho_1&lt;/math&gt;.  Remarkably, the theorem does not hold if more than one copy of the initial state are provided: for example, broadcasting six copies starting from four copies of the original state is allowed, even if the states are drawn from a non-commuting set. The purity of the state can even be increased in the process, a phenomenon known as [[superbroadcasting]]&lt;ref&gt;[https://journals.aps.org/prl/abstract/10.1103/PhysRevLett.95.060503 ''Superbroadcasting of Mixed States''], [[Giacomo Mauro D’Ariano|G. M. D'Ariano]], [[Chiara Macchiavello|C. Macchiavello]], and [[Paolo Perinotti|P. Perinotti]], ''Phys. Rev. Lett.'' '''95''', 060503 (2005).&lt;/ref&gt;.

==See also==
* [[No-communication theorem]]
* [[No-hiding theorem]]&lt;ref&gt;[http://phys.org/news/2011-03-quantum-no-hiding-theorem-experimentally.html Quantum no-hiding theorem experimentally confirmed for first time]. Mar 07, 2011 by Lisa Zyga.&lt;/ref&gt;
* [[Quantum teleportation]]
* [[Quantum entanglement]]
* [[Quantum information]]
* [[Uncertainty principle]]

==References==
{{Reflist}}

* ''Noncommuting Mixed States Cannot Be Broadcast'', [[Howard Barnum|H. Barnum]], [[Carlton M. Caves|C. M. Caves]], [[Christopher Fuchs|C. A. Fuchs]], [[Richard Jozsa|R. Jozsa]] and [[Benjamin Schumacher|B. Schumacher]], ''Phys. Rev. Lett.'' '''76''', 15, 2818–2821 (1996). ([http://prl.aps.org/abstract/PRL/v76/i15/p2818_1 prl.aps.org], [https://arxiv.org/abs/arxiv:quant-ph/9511010 ArXiv])

[[Category:Quantum information science]]
[[Category:Physics theorems]]

{{physics-stub}}</text>
      <sha1>d2rqeiozyfwgt4hhg2env6q7bs8q7k1</sha1>
    </revision>
  </page>
  <page>
    <title>PKIoverheid</title>
    <ns>0</ns>
    <id>40475757</id>
    <revision>
      <id>820840358</id>
      <parentid>751111155</parentid>
      <timestamp>2018-01-16T22:33:56Z</timestamp>
      <contributor>
        <username>Renamed user Y7tw0Ef0XR</username>
        <id>8097475</id>
      </contributor>
      <minor/>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="531">{{no sources|date=November 2016}}
[[Image:Staat-der-nederlanden-ca.png|thumb|175px|right|PKIoverheid certificate hierarchy]]
'''PKIoverheid''' is the [[Public-key infrastructure]] (PKI) from the [[Dutch government]].  Like any other PKI, the system issues and manages digital [[Public key certificate|certificates]] such that they can be realized.  PKIoverheid is run by [[:nl:Logius|Logius]].

== External links ==
*[http://www.pkioverheid.nl/ PKIoverheid.nl]
 
[[Category:Government of the Netherlands]]
[[Category:Cryptography]]</text>
      <sha1>iorjz1po7r1gvhgypouu6snfzuswg3j</sha1>
    </revision>
  </page>
  <page>
    <title>Path integral formulation</title>
    <ns>0</ns>
    <id>438476</id>
    <revision>
      <id>869833939</id>
      <parentid>858702049</parentid>
      <timestamp>2018-11-20T17:47:02Z</timestamp>
      <contributor>
        <username>Randy Kryn</username>
        <id>4796325</id>
      </contributor>
      <minor/>
      <comment>add template 'Richard Feynman', expanded existing template</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="83280">{{about|a formulation of quantum mechanics|integrals along a path, also known as line or contour integrals|line integral}}
{{Quantum mechanics|cTopic=Formulations}}
The '''path integral formulation''' of [[quantum mechanics]] is a description of quantum theory that generalizes the [[action (physics)|action principle]] of [[classical mechanics]].  It replaces the classical notion of a single, unique classical trajectory for a system with a sum, or [[functional integral]], over an infinity of quantum-mechanically possible trajectories to compute a [[probability amplitude|quantum amplitude]].

This formulation has proven crucial to the subsequent development of [[theoretical physics]], because manifest [[Lorentz covariance]] (time and space components of quantities enter equations in the same way) is easier to achieve than in the operator formalism of [[canonical quantization]]. Unlike previous methods, the path integral allows a physicist to easily change [[coordinates]] between very different [[canonical coordinates|canonical]] descriptions of the same quantum system. Another advantage is that it is in practice easier to guess the correct form of the [[Lagrangian (field theory)|Lagrangian]] of a theory, which naturally enters the path integrals (for interactions of a certain type, these are ''coordinate space'' or ''Feynman path integrals''), than the [[Hamiltonian (quantum mechanics)|Hamiltonian]]. Possible downsides of the approach include that [[unitarity]] (this is related to conservation of probability; the probabilities of all physically possible outcomes must add up to one) of the [[S-matrix]] is obscure in the formulation. The path-integral approach has been proved to be equivalent to the other formalisms of quantum mechanics and quantum field theory. Thus, by ''deriving'' either approach from the other, problems associated with one or the other approach (as exemplified by Lorentz covariance or unitarity) go away.&lt;ref&gt;{{harvnb|Weinberg|2002|loc=Chapter 9.}}&lt;/ref&gt;

The path integral also relates quantum and [[stochastic]] processes, and this provided the basis for the grand synthesis of the 1970s, which unified [[quantum field theory]] with the [[statistical mechanics|statistical field theory]] of a fluctuating field near a [[second-order phase transition]]. The [[Schrödinger equation]] is a [[diffusion equation]] with an imaginary diffusion constant, and the path integral is an [[analytic continuation]] of a method for summing up all possible [[random walk]]s.

The basic idea of the path integral formulation can be traced back to [[Norbert Wiener]], who introduced the [[Wiener integral]] for solving problems in diffusion and [[Brownian motion]].&lt;ref&gt;{{harvnb|Chaichian|Demichev|2001}}&lt;/ref&gt; This idea was extended to the use of the [[Lagrangian (field theory)|Lagrangian]] in quantum mechanics by [[P. A. M. Dirac]] in his 1933 article.&lt;ref&gt;{{harvnb|Dirac|1933}}&lt;br&gt;PDF:[http://www.hep.anl.gov/czachos/soysoy/Dirac33.pdf The Lagrangian in Quantum Mechanics]&lt;/ref&gt;&lt;ref&gt;{{harvnb|Van Vleck|1928}}&lt;/ref&gt; The complete method was developed in 1948 by [[Richard Feynman]]. Some preliminaries were worked out earlier in his doctoral work under the supervision of [[John Archibald Wheeler]]. The original motivation stemmed from the desire to obtain a quantum-mechanical formulation for the [[Wheeler–Feynman absorber theory]] using a [[Lagrangian (field theory)|Lagrangian]] (rather than a [[Hamiltonian (quantum mechanics)|Hamiltonian]]) as a starting point.

[[File:Three paths from A to B.png|thumbnail|250px|These are just three of the paths that contribute to the quantum amplitude for a particle moving from point A at some time {{math|''t''&lt;sub&gt;0&lt;/sub&gt;}} to point B at some other time {{math|''t''&lt;sub&gt;1&lt;/sub&gt;}}.]]

== Quantum action principle ==

In quantum mechanics, as in classical mechanics, the [[Hamiltonian (quantum mechanics)|Hamiltonian]] is the generator of time translations. This means that the state at a slightly later time differs from the state at the current time by the result of acting with the Hamiltonian operator (multiplied by the negative [[imaginary unit]], {{math|−''i''}}). For states with a definite energy, this is a statement of the [[de Broglie relation]] between frequency and energy, and the general relation is consistent with that plus the [[superposition principle]].

The Hamiltonian in classical mechanics is derived from a [[Lagrangian (field theory)|Lagrangian]], which is a more fundamental quantity relative to [[special relativity]]. The Hamiltonian indicates how to march forward in time, but the time is different in different [[Frame of reference|reference frames]]. The Lagrangian is a [[Lorentz scalar]], while the Hamiltonian is the time component of a [[four-vector]]. So the Hamiltonian is different in different frames, and this type of symmetry is not apparent in the original formulation of quantum mechanics.

The Hamiltonian is a function of the position and momentum at one time, and it determines the position and momentum a little later. The Lagrangian is a function of the position now and the position a little later (or, equivalently for infinitesimal time separations, it is a function of the position and velocity). The relation between the two is by a [[Legendre transformation]], and the condition that determines the classical equations of motion (the [[Euler–Lagrange equation]]s) is that the [[action (physics)|action]] has an extremum.

In quantum mechanics, the Legendre transform is hard to interpret, because the motion is not over a definite trajectory. In classical mechanics, with [[discretization]] in time, the Legendre transform becomes

:&lt;math&gt; \varepsilon H = p(t)\big(q(t + \varepsilon) - q(t)\big) - \varepsilon L&lt;/math&gt;
and
:&lt;math&gt; p = \frac{\partial L}{\partial \dot{q}},&lt;/math&gt;

where the partial derivative with respect to &lt;math&gt;\dot q&lt;/math&gt; holds {{math|''q''(''t'' + ''ε'')}} fixed. The inverse Legendre transform is

:&lt;math&gt; \varepsilon L = \varepsilon p \dot{q} - \varepsilon H,&lt;/math&gt;

where

:&lt;math&gt; \dot q = \frac{\partial H}{\partial p},&lt;/math&gt;

and the partial derivative now is with respect to {{mvar|p}} at fixed {{mvar|q}}.

In quantum mechanics, the state is a [[quantum superposition|superposition of different states]] with different values of {{mvar|q}}, or different values of {{mvar|p}}, and the quantities {{mvar|p}} and {{mvar|q}} can be interpreted as noncommuting operators. The operator {{mvar|p}} is only definite on states that are indefinite with respect to {{mvar|q}}. So consider two states separated in time and act with the operator corresponding to the Lagrangian:

:&lt;math&gt; e^{i\big[p \big(q(t + \varepsilon) - q(t)\big) - \varepsilon H(p, q) \big]}.&lt;/math&gt;

If the multiplications implicit in this formula are reinterpreted as ''matrix'' multiplications, the first factor is

:&lt;math&gt; e^{-ip q(t)},&lt;/math&gt;

and if this is also interpreted as a matrix multiplication, the sum over all states integrates over all {{math|''q''(''t'')}}, and so it takes the [[Fourier transform]] in {{math|''q''(''t'')}} to change basis to {{math|''p''(''t'')}}. That is the action on the Hilbert space – '''change basis to {{mvar|p}} at time {{mvar|t}}'''.

Next comes

:&lt;math&gt;e^{-i\varepsilon H(p,q)},&lt;/math&gt;

or '''evolve an infinitesimal time into the future'''.

Finally, the last factor in this interpretation is

:&lt;math&gt;e^{i p q(t + \varepsilon)},&lt;/math&gt;

which means '''change basis back to {{mvar|q}} at a later time'''.

This is not very different from just ordinary time evolution: the {{mvar|H}} factor contains all the dynamical information – it pushes the state forward in time. The first part and the last part are just Fourier transforms to change to a pure {{mvar|q}} basis from an intermediate {{mvar|p}} basis.
{{quote box|align=right|width=42em|quote=...we see that the integrand in (11) must be of the form {{math|''e''&lt;sup&gt;''iF''/''h''&lt;/sup&gt;}}, where {{mvar|F}} is a function of {{math|''q''&lt;sub&gt;''T''&lt;/sub&gt;, ''q''&lt;sub&gt;1&lt;/sub&gt;, ''q''&lt;sub&gt;2&lt;/sub&gt;, … ''q''&lt;sub&gt;''m''&lt;/sub&gt;, ''q''&lt;sub&gt;''t''&lt;/sub&gt;}}, which remains finite as {{mvar|h}} tends to zero. Let us now picture one of the intermediate {{mvar|q}}s, say {{mvar|q&lt;sub&gt;k&lt;/sub&gt;}}, as varying continuously while the other ones are fixed. Owing to the smallness of {{mvar|h}}, we shall then in general have ''F''/''h'' varying extremely rapidly. This means that {{math|''e''&lt;sup&gt;''iF''/''h''&lt;/sup&gt;}} will vary periodically with a very high frequency about the value zero, as a result of which its integral will be practically zero. The only important part in the domain of integration of {{mvar|q&lt;sub&gt;k&lt;/sub&gt;}} is thus that for which a comparatively large variation in {{mvar|q&lt;sub&gt;k&lt;/sub&gt;}} produces only a very small variation in {{mvar|F}}. This part is the neighbourhood of a point for which {{mvar|F}} is stationary with respect to small variations in {{mvar|q&lt;sub&gt;k&lt;/sub&gt;}}. We can apply this argument to each of the variables of integration ... and obtain the result that the only important part in the domain of integration is that for which {{mvar|F}} is stationary for small variations in all intermediate {{mvar|q}}s. ... We see that {{mvar|F}} has for its classical analogue {{math|{{intmath|int|''T''|''t''}} ''L dt''}}, which is just the action function, which classical mechanics requires to be stationary for small variations in all the intermediate {{mvar|q}}s. This shows the way in which equation (11) goes over into classical results when {{mvar|h}} becomes extremely small. |source=Dirac (1933), p. 69}}

Another way of saying this is that since the Hamiltonian is naturally a function of {{mvar|p}} and {{mvar|q}}, exponentiating this quantity and changing basis from {{mvar|p}} to {{mvar|q}} at each step allows the matrix element of {{mvar|H}} to be expressed as a simple function along each path. This function is the quantum analog of the classical action. This observation is due to [[Paul Dirac]].&lt;ref&gt;{{harvnb|Dirac|1933}}&lt;/ref&gt;

Dirac further noted that one could square the time-evolution operator in the {{mvar|S}} representation:

:&lt;math&gt; e^{i\varepsilon S},&lt;/math&gt;

and this gives the time-evolution operator between time {{mvar|t}} and time {{math|''t'' + 2''ε''}}. While in the {{mvar|H}} representation the quantity that is being summed over the intermediate states is an obscure matrix element, in the {{mvar|S}} representation it is reinterpreted as a quantity associated to the path. In the limit that one takes a large power of this operator, one reconstructs the full quantum evolution between two states, the early one with a fixed value of {{math|''q''(0)}} and the later one with a fixed value of {{math|''q''(''t'')}}. The result is a sum over paths with a phase, which is the quantum action. Crucially, Dirac identified in this article the deep quantum-mechanical reason for the [[principle of least action]] controlling the classical limit (see quotation box).

== Feynman's interpretation ==

Dirac's work did not provide a precise prescription to calculate the sum over paths, and he did not show that one could recover the Schrödinger equation or the [[canonical commutation relation]]s from this rule. This was done by Feynman.&lt;ref group=nb&gt;Both noted that in the limit of action that is large compared to the reduced [[Planck's constant]] {{mvar|ħ}} (using [[natural units]], {{math|''ħ'' {{=}} 1}}), or the classical limit, the path integral is dominated by solutions which are in the neighborhood of [[stationary point]]s of the action.&lt;/ref&gt; That is, the classical path arises naturally in the classical limit.

Feynman showed that Dirac's quantum action was, for most cases of interest, simply equal to the classical action, appropriately discretized. This means that the classical action is the phase acquired by quantum evolution between two fixed endpoints. He proposed to recover all of quantum mechanics from the following postulates:
# The [[probability]] for an event is given by the squared modulus of a complex number called the "probability amplitude".
# The [[probability amplitude]] is given by adding together the contributions of all paths in configuration space.
# The contribution of a path is proportional to {{math|''e''&lt;sup&gt;''iS''/''ħ''&lt;/sup&gt;}}, where {{mvar|S}} is the [[Action (physics)|action]] given by the [[time integral]] of the [[Lagrangian mechanics|Lagrangian]] along the path.

In order to find the overall probability amplitude for a given process, then, one adds up, or [[integral|integrates]], the amplitude of the 3rd postulate over the space of ''all'' possible paths of the system in between the initial and final states, including those that are absurd by classical standards. In calculating the probability amplitude for a single particle to go from one space-time coordinate to another, it is correct to include paths in which the particle describes elaborate [[curlicues]], curves in which the particle shoots off into outer space and flies back again, and so forth. The '''path integral''' assigns to all these amplitudes ''equal weight'' but varying [[phase (waves)|phase]], or argument of the [[complex number]]. Contributions from paths wildly different from the classical trajectory may be suppressed by [[Interference (wave propagation)|interference]] (see below).

Feynman showed that this formulation of quantum mechanics is equivalent to the [[Quantization (physics)|canonical approach to quantum mechanics]] when the Hamiltonian is at most quadratic in the momentum.  An amplitude computed according to Feynman's principles will also obey the [[Schrödinger equation]] for the [[Hamiltonian (quantum mechanics)|Hamiltonian]] corresponding to the given action.

The path integral formulation of quantum field theory represents the [[transition amplitude]] (corresponding to the classical [[correlation function]]) as a weighted sum of all possible histories of the system from the initial to the final state. A [[Feynman diagram]] is a graphical representation of a [[perturbative]] contribution to the transition amplitude.

== Path integral in quantum mechanics ==

=== Time-slicing derivation ===
{{main|Relation between Schrödinger%27s equation and the path integral formulation of quantum mechanics}}
One common approach to deriving the path integral formula is to divide the time interval into small pieces. Once this is done, the [[Lie product formula|Trotter product formula]] tells us that the noncommutativity of the kinetic and potential energy operators can be ignored.

For a particle in a smooth potential, the path integral is approximated by [[zigzag]] paths, which in one dimension is a product of ordinary integrals. For the motion of the particle from position {{mvar|x&lt;sub&gt;a&lt;/sub&gt;}} at time {{mvar|t&lt;sub&gt;a&lt;/sub&gt;}} to {{mvar|x&lt;sub&gt;b&lt;/sub&gt;}} at time {{mvar|t&lt;sub&gt;b&lt;/sub&gt;}}, the time sequence
:&lt;math&gt;t_a = t_0 &lt; t_1 &lt; \cdots &lt; t_{n-1} &lt; t_n &lt; t_{n+1} = t_b&lt;/math&gt;
can be divided up into {{math|''n'' + 1}} smaller segments {{math|''t&lt;sub&gt;j&lt;/sub&gt;'' − ''t''&lt;sub&gt;''j'' − 1&lt;/sub&gt;}}, where {{math|''j'' {{=}} 1, ..., ''n'' + 1}}, of fixed duration
:&lt;math&gt;\varepsilon = \Delta t = \frac{t_b - t_a}{n + 1}.&lt;/math&gt;

This process is called ''time-slicing''.

An approximation for the path integral can be computed as proportional to

:&lt;math&gt;\int\limits_{-\infty}^{+\infty} \cdots \int\limits_{-\infty}^{+\infty}
 \exp \left(\frac{i}{\hbar}\int_{t_a}^{t_b} L\big(x(t), v(t)\big) \,dt\right) \,dx_0 \, \cdots \, dx_n, &lt;/math&gt;

where {{math|''L''(''x'', ''v'')}} is the Lagrangian of the one-dimensional system with position variable {{math|''x''(''t'')}} and velocity {{math|''v'' {{=}} ''ẋ''(''t'')}} considered (see below), and {{mvar|dx&lt;sub&gt;j&lt;/sub&gt;}} corresponds to the position at the {{mvar|j}}th time step, if the time integral is approximated by a sum of {{mvar|n}} terms.&lt;ref group=nb&gt;For a simplified, step-by-step derivation of the above relation, see [http://www.quantumfieldtheory.info/website_Chap18.pdf Path Integrals in Quantum Theories: A Pedagogic 1st Step].&lt;/ref&gt;

In the limit {{mvar|''n'' → ∞}}, this becomes a [[functional integral]], which, apart from a nonessential factor, is directly the product of the probability amplitudes {{math|{{bra-ket|''x&lt;sub&gt;b&lt;/sub&gt;'', ''t&lt;sub&gt;b&lt;/sub&gt;''|''x&lt;sub&gt;a&lt;/sub&gt;'', ''t&lt;sub&gt;a&lt;/sub&gt;''}}}} (more precisely, since one must work with a continuous spectrum, the respective densities) to find the quantum mechanical particle at {{mvar|t&lt;sub&gt;a&lt;/sub&gt;}} in the initial state {{mvar|x&lt;sub&gt;a&lt;/sub&gt;}} and at {{mvar|t&lt;sub&gt;b&lt;/sub&gt;}} in the final state {{mvar|x&lt;sub&gt;b&lt;/sub&gt;}}.

Actually {{mvar|L}} is the classical [[Lagrangian mechanics|Lagrangian]] of the one-dimensional system considered,
:&lt;math&gt; L(x, \dot x) = T-V=\frac{1}{2}m|\dot{x}|^2-V(x)&lt;/math&gt;
and the abovementioned "zigzagging" corresponds to the appearance of the terms

:&lt;math&gt;\exp\left(\frac{i}{\hbar}\varepsilon \sum_{j=1}^{n+1} L \left(\tilde x_j, \frac{x_j - x_{j-1}}{\varepsilon}, j \right)\right)&lt;/math&gt;

in the [[Riemann sum]] approximating the time integral, which are finally integrated over {{math|''x''&lt;sub&gt;1&lt;/sub&gt;}} to {{mvar|x&lt;sub&gt;n&lt;/sub&gt;}} with the integration measure {{math|''dx''&lt;sub&gt;1&lt;/sub&gt;...''dx&lt;sub&gt;n&lt;/sub&gt;''}}, {{mvar|x̃&lt;sub&gt;j&lt;/sub&gt;}} is an arbitrary value of the interval corresponding to {{mvar|j}}, e.g. its center, {{math|{{sfrac|''x&lt;sub&gt;j&lt;/sub&gt;'' + ''x''&lt;sub&gt;''j''−1&lt;/sub&gt;|2}}}}.

Thus, in contrast to classical mechanics, not only does the stationary path contribute, but actually all virtual paths between the initial and the final point also contribute.

===Path integral formula===

In terms of the wave function in the position representation, the path integral formula reads as follows:
:&lt;math&gt;\psi(x,t)=\frac{1}{Z}\int_{\mathbf{x}(0)=x}\mathcal{D}\mathbf{x}\, e^{iS[\mathbf{x},\dot{\mathbf{x}}]}\psi_0(\mathbf{x}(t))\,&lt;/math&gt;
where &lt;math&gt;\mathcal{D}\mathbf{x}&lt;/math&gt; denotes integration over all paths &lt;math&gt;\mathbf{x}&lt;/math&gt; with &lt;math&gt;\mathbf{x}(0)=x&lt;/math&gt; and where &lt;math&gt;Z&lt;/math&gt; is a normalization factor. Here &lt;math&gt;S&lt;/math&gt; is the action, given by
:&lt;math&gt;S[\mathbf{x},\dot\mathbf{x}]=\int dt\, L(\mathbf{x}(t),\dot\mathbf{x}(t))&lt;/math&gt;

[[File:Path integral example.webm|thumb|The diagram shows the contribution to the path integral of a free particle for a set of paths.]]

=== Free particle ===

The path integral representation gives the quantum amplitude to go from point {{mvar|x}} to point {{mvar|y}} as an integral over all paths. For a free-particle action (for simplicity let {{math|''m'' {{=}} 1}}, {{math|''ħ'' {{=}} 1}})
:&lt;math&gt;S = \int \frac{\dot{x}^2}{2}\, dt,&lt;/math&gt;

the integral can be evaluated explicitly.

To do this, it is convenient to start without the factor {{mvar|i}} in the exponential, so that large deviations are suppressed by small numbers, not by cancelling oscillatory contributions:
:&lt;math&gt;K(x - y; T) = \int_{x(0) = x}^{x(T) = y} \exp\left(-\int_0^T \frac{\dot{x}^2}{2} \,dt\right) \,Dx.&lt;/math&gt;

Splitting the integral into time slices:
:&lt;math&gt;K(x, y; T) = \int_{x(0) = x}^{x(T) = y} \prod_t \exp\left(-\tfrac12 \left(\frac{x(t + \varepsilon) - x(t)}{\varepsilon}\right)^2 \varepsilon \right) \,Dx,&lt;/math&gt;

where the {{mvar|Dx}} is interpreted as a finite collection of integrations at each integer multiple of {{mvar|ε}}. Each factor in the product is a Gaussian as a function of {{math|''x''(''t'' + ''ε'')}} centered at {{math|''x''(''t'')}} with variance {{mvar|ε}}. The multiple integrals are a repeated [[convolution]] of this Gaussian {{mvar|G&lt;sub&gt;ε&lt;/sub&gt;}} with copies of itself at adjacent times:
:&lt;math&gt;K(x - y; T) = G_\varepsilon * G_\varepsilon * \cdots * G_\varepsilon,&lt;/math&gt;

where the number of convolutions is {{math|{{sfrac|''T''|''ε''}}}}. The result is easy to evaluate by taking the Fourier transform of both sides, so that the convolutions become multiplications:
:&lt;math&gt;\tilde{K}(p; T) = \tilde{G}_\varepsilon(p)^{T/\varepsilon}.&lt;/math&gt;

The Fourier transform of the Gaussian {{mvar|G}} is another Gaussian of reciprocal variance:

:&lt;math&gt;\tilde{G}_\varepsilon(p) = e^{-\frac{\varepsilon p^2}{2}},&lt;/math&gt;

and the result is
:&lt;math&gt;\tilde{K}(p; T) = e^{-\frac{T p^2}{2}}.&lt;/math&gt;

The Fourier transform gives {{mvar|K}}, and it is a Gaussian again with reciprocal variance:
:&lt;math&gt;K(x - y; T) \propto e^{ -\frac{(x - y)^2}{2T}}.&lt;/math&gt;

The proportionality constant is not really determined by the time-slicing approach, only the ratio of values for different endpoint choices is determined. The proportionality constant should be chosen to ensure that between each two time slices the time evolution is quantum-mechanically unitary, but a more illuminating way to fix the normalization is to consider the path integral as a description of a stochastic process.

The result has a probability interpretation. The sum over all paths of the exponential factor can be seen as the sum over each path of the probability of selecting that path. The probability is the product over each segment of the probability of selecting that segment, so that each segment is probabilistically independently chosen. The fact that the answer is a Gaussian spreading linearly in time is the [[central limit theorem]], which can be interpreted as the first historical evaluation of a statistical path integral.

The probability interpretation gives a natural normalization choice. The path integral should be defined so that
:&lt;math&gt;\int K(x - y; T) \,dy = 1.&lt;/math&gt;

This condition normalizes the Gaussian and produces a kernel that obeys the diffusion equation:
:&lt;math&gt;\frac{d}{dt} K(x; T) = \frac{\nabla^2}{2} K.&lt;/math&gt;

For oscillatory path integrals, ones with an {{mvar|i}} in the numerator, the time slicing produces convolved Gaussians, just as before. Now, however, the convolution product is marginally singular, since it requires careful limits to evaluate the oscillating integrals. To make the factors well defined, the easiest way is to add a small imaginary part to the time increment {{mvar|ε}}. This is closely related to [[Wick rotation]]. Then the same convolution argument as before gives the propagation kernel:
:&lt;math&gt;K(x - y; T) \propto e^\frac{i(x - y)^2}{2T},&lt;/math&gt;

which, with the same normalization as before (not the sum-squares normalization – this function has a divergent norm), obeys a free Schrödinger equation:
:&lt;math&gt;\frac{d}{dt} K(x; T) = i \frac{\nabla^2}{2} K.&lt;/math&gt;

This means that any superposition of {{mvar|K}}s will also obey the same equation, by linearity. Defining
:&lt;math&gt;\psi_t(y) = \int \psi_0(x) K(x - y; t) \,dx = \int \psi_0(x) \int_{x(0) = x}^{x(t) = y} e^{iS} \,Dx,&lt;/math&gt;

then {{mvar|ψ&lt;sub&gt;t&lt;/sub&gt;}} obeys the free Schrödinger equation just as {{mvar|K}} does:
:&lt;math&gt;i\frac{\partial}{\partial t} \psi_t = -\frac{\nabla^2}{2} \psi_t.&lt;/math&gt;

=== Simple harmonic oscillator ===
The Lagrangian for the simple harmonic oscillator is
:&lt;math&gt;\mathcal{L} = \tfrac12 m \dot{x}^2 - \tfrac12 m \omega^2 x^2.&lt;/math&gt;

Write its trajectory {{math|''x''(''t'')}} as the classical trajectory plus some perturbation, {{math|''x''(''t'') {{=}} ''x''&lt;sub&gt;c&lt;/sub&gt;(''t'') + ''δx''(''t'')}} and the action as {{math|''S'' {{=}} ''S''&lt;sub&gt;c&lt;/sub&gt; + ''δS''}}. The classical trajectory can be written as

:&lt;math&gt;x_\text{c}(t) = x_i \frac{\sin\omega(t_f - t)}{\sin\omega(t_f - t_i)} + x_f \frac{\sin\omega(t - t_i)}{\sin\omega(t_f - t_i)}.&lt;/math&gt;


This trajectory yields the classical action
: &lt;math&gt;
\begin{align}
S_\text{c} &amp; = \int_{t_i}^{t_f} \mathcal{L} \,dt = \int_{t_i}^{t_f} \left(\tfrac12 m\dot{x}^2 - \tfrac12 m\omega^2 x^2 \right) \,dt \\[6pt]
&amp; = \frac 1 2 m\omega \left( \frac{(x_i^2 + x_f^2) \cos\omega(t_f - t_i) - 2 x_i x_f}{\sin\omega(t_f - t_i)} \right)~.
\end{align}
&lt;/math&gt;

Next, expand the non-classical contribution to the action {{mvar|δS}} as a Fourier series, which gives
:&lt;math&gt;S = S_\text{c} + \sum_{n = 1}^\infty \tfrac12 a_n^2 \frac{m}{2} \left( \frac{(n \pi)^2}{t_f - t_i} - \omega^2(t_f - t_i) \right).&lt;/math&gt;

This means that the propagator is
: &lt;math&gt;
\begin{align}
K(x_f, t_f; x_i, t_i) &amp; = Q e^\frac{i S_\text{c}}{\hbar} \prod_{j=1}^\infty \frac{j \pi}{\sqrt{2}} \int da_j \exp{\left( \frac{i}{2\hbar}a_j^2 \frac{m}{2} \left( \frac{(j \pi)^2}{t_f - t_i} - \omega^2(t_f - t_i) \right) \right)} \\[6pt]
&amp; = e^\frac{i S_\text{c}}{\hbar} Q \prod_{j=1}^\infty \left( 1 - \left( \frac{\omega(t_f - t_i)}{j \pi} \right)^2 \right)^{-\frac12}
\end{align}
&lt;/math&gt;
for some normalization
:&lt;math&gt; Q = \sqrt{\frac{m}{2\pi i \hbar (t_f - t_i)}}~. &lt;/math&gt;

Using the infinite-product representation of the [[sinc function]],
:&lt;math&gt;\prod_{j=1}^\infty \left( 1 - \frac{x^2}{j^2} \right) = \frac{\sin\pi x}{\pi x}, &lt;/math&gt;
the propagator can be written as
:&lt;math&gt; K(x_f, t_f; x_i, t_i) = Q e^\frac{i S_\text{c}}{\hbar} \sqrt{ \frac{\omega(t_f - t_i)}{\sin\omega(t_f - t_i)} } = e^\frac{i S_c}{\hbar} \sqrt{ \frac{m\omega}{2\pi i \hbar \sin\omega(t_f - t_i)}}.&lt;/math&gt;

Let {{math|''T'' {{=}} ''t&lt;sub&gt;f&lt;/sub&gt;'' − ''t&lt;sub&gt;i&lt;/sub&gt;''}}.  One may write this propagator in terms of energy eigenstates as
: &lt;math&gt;
\begin{align}
K(x_f, t_f; x_i, t_i) &amp; = \left( \frac{m \omega}{2 \pi i \hbar \sin\omega T } \right)^\frac12 \exp{ \left( \frac{i}{\hbar} \tfrac12 m \omega  \frac{ (x_i^2 + x_f^2) \cos \omega T - 2 x_i x_f }{ \sin \omega T } \right) } \\[6pt]
&amp; = \sum_{n = 0}^\infty \exp{ \left( - \frac{i E_n T}{\hbar} \right) } \psi_n(x_f)^{*} \psi_n(x_i)~.
\end{align}
&lt;/math&gt;

Using the identities {{math|''i'' sin ''ωT'' {{=}} {{sfrac|1|2}}''e''&lt;sup&gt;''iωT''&lt;/sup&gt; (1 − ''e''&lt;sup&gt;−2''iωT''&lt;/sup&gt;)}} and {{math|cos ''ωT'' {{=}} {{sfrac|1|2}}''e''&lt;sup&gt;''iωT''&lt;/sup&gt; (1 + ''e''&lt;sup&gt;−2''iωT''&lt;/sup&gt;)}}, this amounts to
:&lt;math&gt;K(x_f, t_f; x_i, t_i) = \left( \frac{m \omega}{\pi \hbar} \right)^\frac12 e^\frac{-i \omega T} 2 \left( 1 - e^{-2 i \omega T} \right)^{-\frac12} \exp{ \left( - \frac{m \omega}{2 \hbar} \left( \left(x_i^2 + x_f^2\right) \frac{ 1 + e^{-2 i \omega T} }{ 1 - e^{- 2 i \omega T}} - \frac{4 x_i x_f e^{-i \omega T}}{1 - e^{ - 2 i \omega T} }\right) \right) }.&lt;/math&gt;

One may absorb all terms after the first {{math|''e''&lt;sup&gt;−''iωT''/2&lt;/sup&gt;}} into {{math|''R''(''T'')}}, thereby  obtaining
:&lt;math&gt; K(x_f, t_f; x_i, t_i) = \left( \frac{m \omega}{\pi \hbar} \right)^\frac12 e^\frac{-i \omega T } 2 \cdot R(T).&lt;/math&gt;

One may  finally expand {{math|''R''(''T'')}} in  powers of {{math|''e''&lt;sup&gt;−''iωT''&lt;/sup&gt;}}: All  terms in this expansion get multiplied by the {{math|''e''&lt;sup&gt;−''iωT''/2&lt;/sup&gt;}} factor in the front,  yielding terms  of the form
:&lt;math&gt;e^\frac{-i\omega T}{2} e^{-i n\omega T} = e^{-i \omega T \left( \frac12 + n\right) } \quad\text{for } n = 0, 1, 2, \ldots.&lt;/math&gt;
Comparison  to the above eigenstate expansion  yields the  standard energy spectrum for the simple harmonic oscillator,
:&lt;math&gt;E_n = \left( n + \tfrac12 \right) \hbar \omega~.&lt;/math&gt;

{{see also|Propagator#Basic examples: propagator of free particle and harmonic oscillator}}

===Coulomb potential===
Feynman's time-sliced approximation does not, however, exist for the most important quantum-mechanical path integrals of atoms, due to the singularity of the [[Coulomb potential]] {{math|{{sfrac|''e''&lt;sup&gt;2&lt;/sup&gt;|''r''}}}} at the origin. Only after replacing the time {{mvar|t}} by another path-dependent pseudo-time parameter
:&lt;math&gt;s = \int \frac{dt}{r(t)}&lt;/math&gt;
the singularity is removed and a time-sliced approximation exists, which is exactly integrable, since it can be made harmonic by a simple coordinate transformation, as discovered in 1979 by [[İsmail Hakkı Duru]] and [[Hagen Kleinert]].&lt;ref&gt;{{harvnb|Duru|Kleinert|1979|loc=Chapter 13.}}&lt;/ref&gt; The combination of a path-dependent time transformation and a coordinate transformation is an important tool to solve many path integrals and is called generically the [[Duru–Kleinert transformation]].

=== The Schrödinger equation ===

{{Main|Relation between Schrödinger's equation and the path integral formulation of quantum mechanics}}

The path integral reproduces the Schrödinger equation for the initial and final state even when a potential is present. This is easiest to see by taking a path-integral over infinitesimally separated times.
:&lt;math&gt;\psi(y;t+\varepsilon) = \int_{-\infty}^\infty \psi(x;t)\int_{x(t)=x}^{x(t+\varepsilon)=y} e^{i\int\limits_t^{t+\varepsilon} \left(\frac{\dot{x}^2}{2} - V(x)\right)\, dt}\, Dx(t) \, dx\qquad (1)&lt;/math&gt;

Since the time separation is infinitesimal and the cancelling oscillations become severe for large values of {{mvar|ẋ}}, the path integral has most weight for {{mvar|y}} close to {{mvar|x}}. In this case, to lowest order the potential energy is constant, and only the kinetic energy contribution is nontrivial. (This separation of the kinetic and potential energy terms in the exponent is essentially the [[Lie product formula|Trotter product formula]].) The exponential of the action is
:&lt;math&gt;e^{-i\varepsilon V(x)} e^{i\frac{\dot{x}^2}{2}\varepsilon}&lt;/math&gt;

The first term rotates the phase of {{math|''ψ''(''x'')}} locally by an amount proportional to the potential energy. The second term is the free particle propagator, corresponding to {{mvar|i}} times a diffusion process. To lowest order in {{mvar|ε}} they are additive; in any case one has with (1):

:&lt;math&gt;\psi(y;t+\varepsilon) \approx \int \psi(x;t) e^{-i\varepsilon V(x)} e^\frac{i(x-y)^2 }{ 2\varepsilon} \,dx\,.&lt;/math&gt;

As mentioned, the spread in {{mvar|ψ}} is diffusive from the free particle propagation, with an extra infinitesimal rotation in phase which slowly varies from point to point from the potential:
:&lt;math&gt;\frac{\partial\psi}{\partial t} = i\cdot \left(\tfrac12\nabla^2 - V(x)\right)\psi\,&lt;/math&gt;

and this is the Schrödinger equation. Note that the normalization of the path integral needs to be fixed in exactly the same way as in the free particle case. An arbitrary continuous potential does not affect the normalization, although singular potentials require careful treatment.

=== Equations of motion ===

Since the states obey the Schrödinger equation, the path integral must reproduce the Heisenberg equations of motion for the averages of {{mvar|x}} and  {{mvar|ẋ}} variables, but it is instructive to see this directly. The direct approach shows that the expectation values calculated from the path integral reproduce the usual ones of quantum mechanics.

Start by considering the path integral with some fixed initial state
:&lt;math&gt;\int \psi_0(x) \int_{x(0)=x} e^{iS(x,\dot{x})}\, Dx\,&lt;/math&gt;

Now note that {{mvar|''x''(''t'')}} at each separate time is a separate integration variable. So it is legitimate to change variables in the integral by shifting: {{math|''x''(''t'') {{=}} ''u''(''t'') + ''ε''(''t'')}} where {{math|''ε''(''t'')}} is a different shift at each time but {{math|''ε''(0) {{=}} ''ε''(''T'') {{=}} 0}}, since the endpoints are not integrated:
:&lt;math&gt;\int \psi_0(x) \int_{u(0)=x} e^{iS(u+\varepsilon,\dot{u}+\dot{\varepsilon})}\, Du\,&lt;/math&gt;

The change in the integral from the shift is, to first infinitesimal order in {{mvar|ε}}:
:&lt;math&gt;\int \psi_0(x) \int_{u(0)=x} \left( \int \frac{\partial S }{ \partial u } \varepsilon + \frac{ \partial S }{ \partial \dot{u} } \dot{\varepsilon}\, dt \right) e^{iS} \,Du\,&lt;/math&gt;

which, integrating by parts in {{mvar|t}}, gives:
:&lt;math&gt;\int \psi_0(x) \int_{u(0)=x} -\left( \int \left(\frac{d}{dt} \frac{\partial S}{\partial \dot{u}} - \frac{\partial S}{\partial u}\right)\varepsilon(t)\, dt \right) e^{iS}\, Du\,&lt;/math&gt;

But this was just a shift of integration variables, which doesn't change the value of the integral for any choice of {{mvar|''ε''(''t'')}}. The conclusion is that this first order variation is zero for an arbitrary initial state and at any arbitrary point in time:
:&lt;math&gt;\left\langle \psi_0\left| \frac{\delta S}{\delta x}(t) \right|\psi_0 \right\rangle = 0&lt;/math&gt;
this is the Heisenberg equation of motion.

If the action contains terms which multiply {{mvar|ẋ}} and {{mvar|x}}, at the same moment in time, the manipulations above are only heuristic, because the multiplication rules for these quantities is just as noncommuting in the path integral as it is in the operator formalism.

=== Stationary-phase approximation ===

If the variation in the action exceeds {{mvar|ħ}} by many orders of magnitude, we typically have destructive interference other than in the vicinity of those trajectories satisfying the [[Euler–Lagrange equation]], which is now reinterpreted as the condition for constructive interference. This can be shown using the method of stationary phase applied to the propagator. As {{mvar|ħ}} decreases, the exponential in the integral oscillates rapidly in the complex domain for any change in the action. Thus, in the limit that {{mvar|ħ}} goes to zero, only points where the classical action does not vary contribute to the propagator.

=== Canonical commutation relations ===

The formulation of the path integral does not make it clear at first sight that the quantities {{mvar|x}} and {{mvar|p}} do not commute. In the path integral, these are just integration variables and they have no obvious ordering. Feynman discovered that the non-commutativity is still present.&lt;ref&gt;{{harvnb|Feynman|1948}}&lt;/ref&gt;

To see this, consider the simplest path integral, the brownian walk. This is not yet quantum mechanics, so in the path-integral the action is not multiplied by {{mvar|i}}:

:&lt;math&gt;S=  \int \left( \frac{dx}{dt} \right)^2\, dt&lt;/math&gt;

The quantity {{mvar|''x''(''t'')}} is fluctuating, and the derivative is defined as the limit of a discrete difference.

:&lt;math&gt;\frac{dx}{dt} = \frac{x(t+\varepsilon) - x(t)} \varepsilon &lt;/math&gt;

Note that the distance that a random walk moves is proportional to {{math|{{sqrt|''t''}}}}, so that:
:&lt;math&gt;x(t+\varepsilon) - x(t) \approx \sqrt{\varepsilon}&lt;/math&gt;
This shows that the random walk is not differentiable, since the ratio that defines the derivative diverges with probability one.

The quantity {{mvar|xẋ}} is ambiguous, with two possible meanings:

:&lt;math&gt;[1] = x \frac{dx}{dt}  = x(t) \frac{x(t+\varepsilon) - x(t) }{\varepsilon } &lt;/math&gt;

:&lt;math&gt;[2] = x \frac{dx}{dt} = x(t+\varepsilon) \frac{x(t+\varepsilon) - x(t) }{\varepsilon} &lt;/math&gt;

In elementary calculus, the two are only different by an amount which goes to 0 as {{mvar|ε}} goes to 0. But in this case, the difference between the two is not 0:

:&lt;math&gt;[2] - [1] =  \frac{\big( x(t + \varepsilon) - x(t)\big )^2}{\varepsilon} \approx \frac \varepsilon \varepsilon&lt;/math&gt;

give a name to the value of the difference for any one random walk:
:&lt;math&gt;\frac{\big(x(t+\varepsilon)- x(t)\big)^2 }{\varepsilon} = f(t)&lt;/math&gt;

and note that {{math|''f''(''t'')}} is a rapidly fluctuating statistical quantity, whose average value is 1, i.e. a normalized "Gaussian process". The fluctuations of such a quantity can be described by a statistical Lagrangian
:&lt;math&gt;\mathcal L =  (f(t)-1)^2 \,,&lt;/math&gt;
and the equations of motion for {{mvar|f}} derived from extremizing the action {{mvar|S}} corresponding to {{mathcal|L}} just set it equal to 1. In physics, such a quantity is "equal to 1 as an operator identity". In mathematics, it "weakly converges to 1". In either case, it is 1 in any expectation value, or when averaged over any interval, or for all practical purpose.

Defining the time order to ''be'' the operator order:
:&lt;math&gt;[x, \dot x] = x \frac{dx}{dt} - \frac{dx}{dt} x = 1&lt;/math&gt;

This is called the [[Itō lemma]] in [[stochastic calculus]], and the (euclideanized) canonical commutation relations in physics.

For a general statistical action, a similar argument shows that
:&lt;math&gt;\left[x , \frac{\partial S }{ \partial \dot x} \right] = 1&lt;/math&gt;
and in quantum mechanics, the extra imaginary unit in the action converts this to the canonical commutation relation,
:&lt;math&gt;[x,p ] = i&lt;/math&gt;

=== Particle in curved space ===

For a particle in curved space the kinetic term depends on the position, and the above time slicing cannot be applied, this being a manifestation of the notorious [[operator ordering problem]] in Schrödinger quantum mechanics. One may, however, solve this problem by transforming the time-sliced flat-space path integral to curved space using a multivalued coordinate transformation ([[nonholonomic mapping]] explained [http://www.physik.fu-berlin.de/~kleinert/b5/psfiles/pthic10.pdf here]).

=== Measure-theoretic factors ===
Sometimes (e.g. a particle moving in curved space) we also have measure-theoretic factors in the functional integral:
:&lt;math&gt;\int \mu[x] e^{iS[x]} \,\mathcal{D}x.&lt;/math&gt;
This factor is needed to restore unitarity.

For instance, if
:&lt;math&gt;S = \int \left( \frac{m}{2} g_{ij} \dot{x}^i \dot{x}^j - V(x) \right) \,dt,&lt;/math&gt;
then it means that each spatial slice is multiplied by the measure {{math|{{sqrt|''g''}}}}. This measure cannot be expressed as a functional multiplying the {{math|{{mathcal|D}}''x''}} measure because they belong to entirely different classes.

==Euclidean path integrals==
It is very common in path integrals to perform a [[Wick rotation]] from real to imaginary times. In the setting of quantum field theory, the Wick rotation changes the geometry of space-time from Lorentzian to Euclidean; as a result, Wick-rotated path integrals are often called Euclidean path integrals.

===Wick rotation and the Feynman–Kac formula===
If we replace &lt;math&gt;t&lt;/math&gt; by &lt;math&gt;-it&lt;/math&gt;, the time-evolution operator &lt;math&gt;e^{-it\hat{H}/\hbar}&lt;/math&gt; is replaced by &lt;math&gt;e^{-t\hat{H}/\hbar}&lt;/math&gt;. (This change is known as a [[Wick rotation]].) If we repeat the derivation of the path-integral formula in this setting, we obtain&lt;ref&gt;{{harvnb|Hall|2013|loc=Section 20.3.}}&lt;/ref&gt;
:&lt;math&gt;\psi(x,t)=\frac{1}{Z}\int_{\mathbf{x}(0)=x} e^{-S_{\mathrm{Euclidean}}(\mathbf{x},\dot{\mathbf{x}})/\hbar}\psi_0(\mathbf{x}(t))\, \mathcal{D}\mathbf{x}\,&lt;/math&gt;,
where &lt;math&gt;S_{\mathrm{Euclidean}}&lt;/math&gt; is the Euclidean action, given by
:&lt;math&gt;S_{\mathrm{Euclidean}}(\mathbf{x},\dot{\mathbf{x}})=\int\left[ \frac{m}{2}|\dot\mathbf{x}(t)|^2+V(\mathbf{x}(t))\right] \,dt&lt;/math&gt;.
Note the sign change between this and the normal action, where the potential energy term is negative. (The term ''Euclidean'' is from the context of quantum field theory, where the change from real to imaginary time changes the space-time geometry from Lorentzian to Euclidean.)

Now, the contribution of the kinetic energy to the path integral is as follows:
:&lt;math&gt;\frac{1}{Z}\int_{\mathbf{x}(0)=x} f(\mathbf{x})e^{-\frac{m}{2}\int |\dot\mathbf{x}|^2dt}\, \mathcal{D}\mathbf{x}\,&lt;/math&gt;
where &lt;math&gt;f(\mathbf{x})&lt;/math&gt; includes all the remaining dependence of the integrand on the path. This integral has a rigorous mathematical interpretation as integration against the [[Wiener process|Wiener measure]], denoted &lt;math&gt;\mu_{x}&lt;/math&gt;. The Wiener measure, constructed by [[Norbert Wiener]] gives a rigorous foundation to [[Brownian motion#Einstein.27s theory|Einstein's mathematical model of Brownian motion]]. The subscript &lt;math&gt;x&lt;/math&gt; indicates that the measure &lt;math&gt;\mu_x&lt;/math&gt; is supported on paths &lt;math&gt;\mathbf{x}&lt;/math&gt; with &lt;math&gt;\mathbf{x}(0)=x&lt;/math&gt;.

We then have a rigorous version of the Feynman path integral, known as the [[Feynman–Kac formula]]:&lt;ref&gt;{{harvnb|Hall|2013|loc=Theorem 20.3.}}&lt;/ref&gt;

:&lt;math&gt;\psi(x,t)=\int e^{-\int V(\mathbf{x}(t)\,dt/\hbar}\,\psi_0(\mathbf{x}(t)) \,d\mu_x(\mathbf{x})&lt;/math&gt;,

where now &lt;math&gt;\psi(x,t)&lt;/math&gt; satisfies the Wick-rotated version of the Schrödinger equation,
:&lt;math&gt;\hbar \frac{\partial}{\partial t}\psi(x,t) = -\hat H \psi(x,t)&lt;/math&gt;.
Although the Wick-rotated Schrödinger equation does not have a direct physical meaning, interesting properties of the Schrödinger operator &lt;math&gt;\hat{H}&lt;/math&gt; can be extracted by studying it.&lt;ref&gt;{{harvnb|Simon|1979}}&lt;/ref&gt;

Much of the study of quantum field theories from the path-integral perspective, in both the mathematics and physics literatures, is done in the Euclidean setting, that is, after a Wick rotation. In particular, there are various results showing that if a Euclidean field theory with suitable properties can be constructed, one can then undo the Wick rotation to recover the physical, Lorentzian theory.&lt;ref&gt;{{harvnb|Glimm|Jaffe|1981|loc=Chapter 19.}}&lt;/ref&gt; On the other hand, it is much more difficult to give a meaning to path integrals (even Euclidean path integrals) in quantum field theory than in quantum mechanics.&lt;ref&gt;For a brief account of the origins of these difficulties, see {{harvtxt|Hall|2013|loc=Section 20.6.}}&lt;/ref&gt;

=== The path integral and the partition function ===

The path integral is just the generalization of the integral above to all quantum mechanical problems—
:&lt;math&gt;Z = \int  e^\frac{i\mathcal{S}[\mathbf{x}]}{\hbar}\, \mathcal{D}\mathbf{x} \quad\text{where }\mathcal{S}[\mathbf{x}]=\int_0^T L[\mathbf{x}(t),\dot\mathbf{x}(t)]\, dt&lt;/math&gt;
is the [[action (physics)|action]] of the classical problem in which one investigates the path starting at time {{math|''t'' {{=}} 0}} and ending at time {{math|''t'' {{=}} ''T''}}, and &lt;math&gt;\mathcal{D}\mathbf{x}&lt;/math&gt; denotes integration over all paths. In the classical limit, &lt;math&gt;\mathcal{S}[\mathbf{x}]\gg\hbar&lt;/math&gt;, the path of minimum action dominates the integral, because the phase of any path away from this fluctuates rapidly and different contributions cancel.&lt;ref name="Feynman-Hibbs"&gt;{{harvnb|Feynman|Hibbs|Styer|2010|pp=29–31}}&lt;/ref&gt;

The connection with [[statistical mechanics]] follows. Considering only paths which begin and end in the same configuration, perform the [[Wick rotation]] {{math|''it'' {{=}} ''τ''}}, i.e., make time imaginary, and integrate over all possible beginning-ending configurations. The Wick-rotated path integral—described in the previous subsection, with the ordinary action replaced by its "Euclidean" counterpart—now resembles the [[partition function (statistical mechanics)|partition function]] of statistical mechanics defined in a [[canonical ensemble]] with inverse temperature proportional to imaginary time, {{math|{{sfrac|1|''T''}} {{=}} {{sfrac|''k''&lt;sub&gt;B&lt;/sub&gt;''τ''|''ħ''}}}}. Strictly speaking, though, this is the partition function for a [[statistical field theory]].

Clearly, such a deep analogy between quantum mechanics and statistical mechanics cannot be dependent on the formulation. In the canonical formulation, one sees that the unitary evolution operator of a state is given by

:&lt;math&gt;|\alpha;t\rangle=e^{-\frac{iHt}{\hbar}}|\alpha;0\rangle&lt;/math&gt;

where the state {{mvar|α}} is evolved from time {{math|''t'' {{=}} 0}}. If one makes a Wick rotation here, and finds the amplitude to go from any state, back to the same state in (imaginary) time {{mvar|iT}} is given by

: &lt;math&gt;Z = \operatorname{Tr} \left[e^\frac{-HT}{\hbar}\right]&lt;/math&gt;

which is precisely the partition function of statistical mechanics for the same system at temperature quoted earlier. One aspect of this equivalence was also known to [[Erwin Schrödinger]] who remarked that the equation named after him looked like the [[diffusion equation]] after Wick rotation. Note, however, that the Euclidean path integral is actually in the form of a ''classical'' statistical mechanics model.

== Quantum field theory ==
{{Quantum field theory}}
Both the Schrödinger and Heisenberg approaches to quantum mechanics single out time and are not in the spirit of relativity. For example, the Heisenberg approach requires that scalar field operators obey the commutation relation

:&lt;math&gt;[\varphi(x), \partial_t \varphi(y)] = i \delta^3(x - y)&lt;/math&gt;

for two simultaneous spatial positions {{mvar|x}} and {{mvar|y}}, and this is not a relativistically invariant concept. The results of a calculation ''are'' covariant, but the symmetry is not apparent in intermediate stages. If naive field-theory calculations did not produce infinite answers in the continuum limit, this would not have been such a big problem – it would just have been a bad choice of coordinates. But the lack of symmetry means that the infinite quantities must be cut off, and the bad coordinates make it nearly impossible to cut off the theory without spoiling the symmetry. This makes it difficult to extract the physical predictions, which require a [[renormalization|careful limiting procedure]].

The problem of lost symmetry also appears in classical mechanics, where the Hamiltonian formulation also superficially singles out time. The Lagrangian formulation makes the relativistic invariance apparent. In the same way, the path integral is manifestly relativistic. It reproduces the Schrödinger equation, the Heisenberg equations of motion, and the canonical commutation relations and shows that they are compatible with relativity. It extends the Heisenberg-type operator algebra to [[operator product expansion|operator product rules]], which are new relations difficult to see in the old formalism.

Further, different choices of canonical variables lead to very different-seeming formulations of the same theory. The transformations between the variables can be very complicated, but the path integral makes them into reasonably straightforward changes of integration variables. For these reasons, the Feynman path integral has made earlier formalisms largely obsolete.

The price of a path integral representation is that the unitarity of a theory is no longer self-evident, but it can be proven by changing variables to some canonical representation. The path integral itself also deals with larger mathematical spaces than is usual, which requires more careful mathematics, not all of which has been fully worked out. The path integral historically was not immediately accepted, partly because it took many years to incorporate fermions properly. This required physicists to invent an entirely new mathematical object – the [[Grassmann variable]] – which also allowed changes of variables to be done naturally, as well as allowing [[Faddeev–Popov ghost|constrained quantization]].

The integration variables in the path integral are subtly non-commuting. The value of the product of two field operators at what looks like the same point depends on how the two points are ordered in space and time. This makes some naive identities [[Anomaly (physics)|fail]].

=== The propagator ===

In relativistic theories, there is both a particle and field representation for every theory. The field representation is a sum over all field configurations, and the particle representation is a sum over different particle paths.

The nonrelativistic formulation is traditionally given in terms of particle paths, not fields. There, the path integral in the usual variables, with fixed boundary conditions, gives the probability amplitude for a particle to go from point {{mvar|x}} to point {{mvar|y}} in time {{mvar|T}}:

:&lt;math&gt;K(x, y; T) = \langle y; T \mid x; 0 \rangle = \int_{x(0)=x}^{x(T)=y} e^{i S[x]} \,Dx.&lt;/math&gt;

This is called the [[propagator]]. Superposing different values of the initial position {{mvar|x}} with an arbitrary initial state {{math|''ψ''&lt;sub&gt;0&lt;/sub&gt;(''x'')}} constructs the final state:
:&lt;math&gt;\psi_T(y) = \int_x \psi_0(x) K(x, y; T) \,dx = \int^{x(T)=y} \psi_0(x(0)) e^{i S[x]} \,Dx.&lt;/math&gt;

For a spatially homogeneous system, where {{math|''K''(''x'', ''y'')}} is only a function of {{math|(''x'' − ''y'')}}, the integral is a [[convolution]], the final state is the initial state convolved with the propagator:
:&lt;math&gt;\psi_T = \psi_0 * K(;T).&lt;/math&gt;

For a free particle of mass {{mvar|m}}, the propagator can be evaluated either explicitly from the path integral or by noting that the Schrödinger equation is a diffusion equation in imaginary time, and the solution must be a normalized Gaussian:
:&lt;math&gt;K(x, y; T) \propto e^\frac{i m(x - y)^2}{2T}.&lt;/math&gt;

Taking the Fourier transform in {{math|(''x'' − ''y'')}} produces another Gaussian:

:&lt;math&gt;K(p; T) = e^\frac{i T p^2}{2m},&lt;/math&gt;

and in {{mvar|p}}-space the proportionality factor here is constant in time, as will be verified in a moment. The Fourier transform in time, extending {{math|''K''(''p''; ''T'')}} to be zero for negative times, gives Green's function, or the frequency-space propagator:
:&lt;math&gt;G_\text{F}(p, E) = \frac{-i}{E - \frac{\vec{p}^2}{2m} + i\varepsilon},&lt;/math&gt;

which is the reciprocal of the operator that annihilates the wavefunction in the Schrödinger equation, which wouldn't have come out right if the proportionality factor weren't constant in the {{mvar|p}}-space representation.

The infinitesimal term in the denominator is a small positive number, which guarantees that the inverse Fourier transform in {{mvar|E}} will be nonzero only for future times. For past times, the inverse Fourier transform contour closes toward values of {{mvar|E}} where there is no singularity. This guarantees that {{mvar|K}} propagates the particle into the future and is the reason for the subscript "F" on {{mvar|G}}. The infinitesimal term can be interpreted as an infinitesimal rotation toward imaginary time.

It is also possible to reexpress the nonrelativistic time evolution in terms of propagators going toward the past, since the Schrödinger equation is time-reversible. The past propagator is the same as the future propagator except for the obvious difference that it vanishes in the future, and in the Gaussian {{mvar|t}} is replaced by {{math|−''t''}}. In this case, the interpretation is that these are the quantities to convolve the final wavefunction so as to get the initial wavefunction:
:&lt;math&gt;G_\text{B}(p, E) = \frac{-i}{-E - \frac{i\vec{p}^2}{2m} + i\varepsilon}.&lt;/math&gt;
Given the nearly identical only change is the sign of {{mvar|E}} and {{mvar|ε}}, the parameter {{mvar|E}} in Green's function can either be the energy if the paths are going toward the future, or the negative of the energy if the paths are going toward the past.

For a nonrelativistic theory, the time as measured along the path of a moving particle and the time as measured by an outside observer are the same. In relativity, this is no longer true. For a relativistic theory the propagator should be defined as the sum over all paths that travel between two points in a fixed proper time, as measured along the path (these paths describe the trajectory of a particle in space and in time):

:&lt;math&gt;K(x - y, \Tau) = \int_{x(0)=x}^{x(\Tau)=y} e^{i \int_0^\Tau \sqrt{{\dot x}^2} - \alpha \,d\tau}.&lt;/math&gt;

The integral above is not trivial to interpret because of the square root. Fortunately, there is a heuristic trick. The sum is over the relativistic arc length of the path of an oscillating quantity, and like the nonrelativistic path integral should be interpreted as slightly rotated into imaginary time. The function {{math|''K''(''x'' − ''y'', ''τ'')}} can be evaluated when the sum is over paths in Euclidean space:
:&lt;math&gt;K(x - y, \Tau) = e^{-\alpha \Tau} \int_{x(0)=x}^{x(\Tau)=y} e^{-L}.&lt;/math&gt;

This describes a sum over all paths of length {{math|Τ}} of the exponential of minus the length. This can be given a probability interpretation. The sum over all paths is a probability average over a path constructed step by step. The total number of steps is proportional to {{math|Τ}}, and each step is less likely the longer it is. By the [[central limit theorem]], the result of many independent steps is a Gaussian of variance proportional to {{math|Τ}}:
:&lt;math&gt;K(x - y,\Tau) = e^{-\alpha \Tau} e^{-\frac{(x - y)^2}{\Tau}}.&lt;/math&gt;

The usual definition of the relativistic propagator only asks for the amplitude is to travel from {{mvar|x}} to {{mvar|y}}, after summing over all the possible proper times it could take:
:&lt;math&gt;K(x - y) = \int_0^\infty K(x - y, \Tau) W(\Tau) \,d\Tau,&lt;/math&gt;
where {{math|''W''(Τ)}} is a weight factor, the relative importance of paths of different proper time. By the translation symmetry in proper time, this weight can only be an exponential factor and can be absorbed into the constant {{mvar|α}}:
:&lt;math&gt;K(x - y) = \int_0^\infty e^{-\frac{(x - y)^2}{\Tau} -\alpha \Tau} \,d\Tau.&lt;/math&gt;

This is the [[Feynman diagram#Schwinger representation|Schwinger representation]]. Taking a Fourier transform over the variable {{math|(''x'' − ''y'')}} can be done for each value of {{math|Τ}} separately, and because each separate {{math|Τ}} contribution is a Gaussian, gives whose Fourier transform is another Gaussian with reciprocal width. So in {{mvar|p}}-space, the propagator can be reexpressed simply:
:&lt;math&gt;K(p) = \int_0^\infty e^{-\Tau p^2 - \Tau \alpha} \,d\Tau = \frac{1}{p^2 + \alpha},&lt;/math&gt;

which is the Euclidean propagator for a scalar particle. Rotating {{math|''p''&lt;sub&gt;0&lt;/sub&gt;}} to be imaginary gives the usual relativistic propagator, up to a factor of {{math|−''i''}} and an ambiguity, which will be clarified below:
:&lt;math&gt;K(p) = \frac{i}{p_0^2 - \vec{p}^2 - m^2}.&lt;/math&gt;

This expression can be interpreted in the nonrelativistic limit, where it is convenient to split it by [[partial fractions]]:

:&lt;math&gt;2 p_0 K(p) = \frac{i}{p_0 - \sqrt{\vec{p}^2 + m^2}} + \frac{i}{p_0 + \sqrt{\vec{p}^2 + m^2}}.&lt;/math&gt;

For states where one nonrelativistic particle is present, the initial wavefunction has a frequency distribution concentrated near {{math|''p''&lt;sub&gt;0&lt;/sub&gt; {{=}} ''m''}}. When convolving with the propagator, which in {{mvar|p}} space just means multiplying by the propagator, the second term is suppressed and the first term is enhanced. For frequencies near {{math|''p''&lt;sub&gt;0&lt;/sub&gt; {{=}} ''m''}}, the dominant first term has the form

:&lt;math&gt;2m K_\text{NR}(p) = \frac{i}{(p_0 - m) - \frac{\vec{p}^2}{2m}}.&lt;/math&gt;

This is the expression for the nonrelativistic [[Green's function]] of a free Schrödinger particle.

The second term has a nonrelativistic limit also, but this limit is concentrated on frequencies that are negative. The second pole is dominated by contributions from paths where the proper time and the coordinate time are ticking in an opposite sense, which means that the second term is to be interpreted as the antiparticle. The nonrelativistic analysis shows that with this form the antiparticle still has positive energy.

The proper way to express this mathematically is that, adding a small suppression factor in proper time, the limit where {{math|''t'' → −∞}} of the first term must vanish, while the {{math|''t'' → +∞}} limit of the second term must vanish. In the Fourier transform, this means shifting the pole in {{math|''p''&lt;sub&gt;0&lt;/sub&gt;}} slightly, so that the inverse Fourier transform will pick up a small decay factor in one of the time directions:

:&lt;math&gt;K(p) = \frac{i}{p_0 - \sqrt{\vec{p}^2 + m^2} + i\varepsilon} + \frac{i}{p_0 - \sqrt{\vec{p}^2+m^2} - i\varepsilon}.&lt;/math&gt;

Without these terms, the pole contribution could not be unambiguously evaluated when taking the inverse Fourier transform of {{math|''p''&lt;sub&gt;0&lt;/sub&gt;}}. The terms can be recombined:

:&lt;math&gt;K(p) = \frac{i}{p^2 - m^2 + i\varepsilon},&lt;/math&gt;

which when factored, produces opposite-sign infinitesimal terms in each factor. This is the mathematically precise form of the relativistic particle propagator, free of any ambiguities. The {{mvar|ε}} term introduces a small imaginary part to the {{math|''α'' {{=}} ''m''&lt;sup&gt;2&lt;/sup&gt;}}, which in the Minkowski version is a small exponential suppression of long paths.

So in the relativistic case, the Feynman path-integral representation of the propagator includes paths going backwards in time, which describe antiparticles. The paths that contribute to the relativistic propagator go forward and backwards in time, and the [[Feynman–Stueckelberg interpretation|interpretation]] of this is that the amplitude for a free particle to travel between two points includes amplitudes for the particle to fluctuate into an antiparticle, travel back in time, then forward again.

Unlike the nonrelativistic case, it is impossible to produce a relativistic theory of local particle propagation without including antiparticles. All local differential operators have inverses that are nonzero outside the light cone, meaning that it is impossible to keep a particle from travelling faster than light. Such a particle cannot have a Green's function which is only nonzero in the future in a relativistically invariant theory.

=== Functionals of fields ===

However, the path integral formulation is also extremely important in ''direct'' application to quantum field theory, in which the "paths" or histories being considered are not the motions of a single particle, but the possible time evolutions of a [[field (physics)|field]] over all space.  The action is referred to technically as a [[functional (mathematics)|functional]] of the field: {{math|''S''[''ϕ'']}}, where the field {{math|''ϕ''(''x&lt;sup&gt;μ&lt;/sup&gt;'')}} is itself a function of space and time, and the square brackets are a reminder that the action depends on all the field's values everywhere, not just some particular value. ''One'' such given function {{math|''ϕ''(''x&lt;sup&gt;μ&lt;/sup&gt;'')}} of [[spacetime]] is called a ''field configuration''. In principle, one integrates Feynman's amplitude over the class of all possible field configurations.

Much of the formal study of QFT is devoted to the properties of the resulting functional integral, and much effort (not yet entirely successful) has been made toward making these [[functional integral]]s mathematically precise.

Such a functional integral is extremely similar to the [[partition function (statistical mechanics)|partition function]] in [[statistical mechanics]].  Indeed, it is sometimes ''called'' a [[partition function (quantum field theory)|partition function]], and the two are essentially mathematically identical except for the factor of {{mvar|i}} in the exponent in Feynman's postulate 3. [[Analytic continuation|Analytically continuing]] the integral to an imaginary time variable (called a [[Wick rotation]]) makes the functional integral even more like a statistical partition function and also tames some of the mathematical difficulties of working with these integrals.

=== Expectation values ===

In [[quantum field theory]], if the [[action (physics)|action]] is given by the [[functional (mathematics)|functional]] {{mathcal|S}} of field configurations (which only depends locally on the fields), then the [[time-ordered]] [[vacuum expectation value]] of [[polynomially bounded]] functional {{mvar|F}}, {{math|{{angbr|''F''}}}}, is given by

:&lt;math&gt;\langle F \rangle = \frac{\int\mathcal{D}\varphi F[\varphi]e^{i\mathcal{S}[\varphi]}}{\int\mathcal{D}\varphi e^{i\mathcal{S}[\varphi]}}.&lt;/math&gt;

The symbol {{math|∫{{mathcal|D}}''ϕ''}} here is a concise way to represent the infinite-dimensional integral over all possible field configurations on all of space-time. As stated above, the unadorned path integral in the denominator ensures proper normalization.

=== As a probability ===
Strictly speaking, the only question that can be asked in physics is: ''What fraction of states satisfying condition {{mvar|A}} also satisfy condition {{mvar|B}}?'' The answer to this is a number between 0 and 1, which can be interpreted as a [[conditional probability]], written as {{math|P(''B''{{!}}''A'')}}. In terms of path integration, since {{math|P(''B''{{!}}''A'') {{=}} {{sfrac|P(''A''∩''B'')&amp;nbsp;|&amp;nbsp;P(''A'')}}}}, this means

:&lt;math&gt;\operatorname{P}(B\mid A) = \frac
{\sum_{F \subset A \cap B}\left| \int\mathcal{D}\varphi O_\text{in}[\varphi]e^{i\mathcal{S}[\varphi]}  F[\varphi]\right|^2}
{\sum_{F \subset A} \left|\int\mathcal{D}\varphi O_\text{in}[\varphi] e^{i\mathcal{S}[\varphi]} F[\varphi]\right|^2},&lt;/math&gt;

where the functional {{math|''O''&lt;sub&gt;in&lt;/sub&gt;[''ϕ'']}} is the superposition of all incoming states that could lead to the states we are interested in. In particular, this could be a state corresponding to the state of the Universe just after the [[Big Bang]], although for actual calculation this can be simplified using heuristic methods. Since this expression is a quotient of path integrals, it is naturally normalised.

=== Schwinger–Dyson equations ===
{{Main|Schwinger–Dyson equation}}

Since this formulation of quantum mechanics is analogous to classical action principle, one might expect that identities concerning the action in classical mechanics would have quantum counterparts derivable from a functional integral. This is often the case.

In the language of functional analysis, we can write the [[Euler–Lagrange equation]]s as
:&lt;math&gt;\frac{\delta \mathcal{S}[\varphi]}{\delta \varphi} = 0&lt;/math&gt;
(the left-hand side is a [[functional derivative]]; the equation means that the action is stationary under small changes in the field configuration).  The quantum analogues of these equations are called the [[Schwinger–Dyson equation]]s.

If the [[functional measure]] {{math|{{mathcal|D}}''ϕ''}} turns out to be [[Translational symmetry|translationally invariant]] (we'll assume this for the rest of this article, although this does not hold for, let's say [[nonlinear sigma model]]s), and if we assume that after a [[Wick rotation]]

:&lt;math&gt;e^{i\mathcal{S}[\varphi]},&lt;/math&gt;

which now becomes
:&lt;math&gt;e^{-H[\varphi]}&lt;/math&gt;
for some {{mvar|H}}, it goes to zero faster than a [[Multiplicative inverse|reciprocal]] of any [[polynomial]] for large values of {{mvar|φ}}, then we can [[integration by parts|integrate by parts]] (after a Wick rotation, followed by a Wick rotation back) to get the following Schwinger–Dyson equations for the expectation:

:&lt;math&gt;\left\langle \frac{\delta F[\varphi]}{\delta \varphi} \right\rangle = -i \left\langle F[\varphi]\frac{\delta \mathcal{S}[\varphi]}{\delta\varphi} \right\rangle&lt;/math&gt;

for any polynomially-bounded functional {{mvar|F}}. In the [[deWitt notation]] this looks like&lt;ref&gt;[http://www.scholarpedia.org/Path_integral Jean Zinn-Justin (2009), ''Scholarpedia'' '''4'''(2):8674 ].&lt;/ref&gt;
:&lt;math&gt;\left\langle F_{,i} \right\rangle = -i \left\langle F \mathcal{S}_{,i} \right\rangle.&lt;/math&gt;

These equations are the analog of the [[on-shell]] EL equations. The time ordering is taken before the time derivatives inside the {{math|{{mathcal|S}}&lt;sub&gt;,''i''&lt;/sub&gt;}}.

If {{mvar|J}} (called the [[source field]]) is an element of the [[dual space]] of the field configurations (which has at least an [[affine structure]] because of the assumption of the [[translational invariance]] for the functional measure), then the [[generating functional]] {{mvar|Z}} of the source fields is '''defined''' to be

:&lt;math&gt;Z[J] = \int \mathcal{D}\varphi e^{i\left(\mathcal{S}[\varphi] + \langle J,\varphi \rangle\right)}.&lt;/math&gt;

Note that

:&lt;math&gt;\frac{\delta^n Z}{\delta J(x_1) \cdots \delta J(x_n)}[J] = i^n \, Z[J] \, \left\langle \varphi(x_1)\cdots \varphi(x_n)\right\rangle_J,&lt;/math&gt;

or

:&lt;math&gt;Z^{,i_1\cdots i_n}[J] = i^n Z[J] \left \langle \varphi^{i_1}\cdots \varphi^{i_n}\right\rangle_J,&lt;/math&gt;

where

:&lt;math&gt;\langle F \rangle_J = \frac{\int \mathcal{D}\varphi F[\varphi]e^{i\left(\mathcal{S}[\varphi] + \langle J,\varphi \rangle\right)}}{\int\mathcal{D}\varphi e^{i\left(\mathcal{S}[\varphi] + \langle J,\varphi \rangle\right)}}.&lt;/math&gt;

Basically, if {{math|{{mathcal|D}}''φ'' ''e''&lt;sup&gt;''i''{{mathcal|S}}[''φ'']&lt;/sup&gt;}} is viewed as a functional distribution (this shouldn't be taken too literally as an interpretation of [[Quantum field theory|QFT]], unlike its Wick-rotated [[statistical mechanics]] analogue, because we have [[time ordering]] complications here!), then {{math|{{angbr|''φ''(''x''&lt;sub&gt;1&lt;/sub&gt;) ... ''φ''(''x&lt;sub&gt;n&lt;/sub&gt;'')}}}} are its [[moment (mathematics)|moments]], and {{mvar|Z}} is its [[Fourier transform]].

If {{mvar|F}} is a functional of {{mvar|φ}}, then for an [[Operator (mathematics)|operator]] {{mvar|K}}, {{math|''F''[''K'']}} is defined to be the operator that substitutes {{mvar|K}} for {{mvar|φ}}. For example, if

:&lt;math&gt;F[\varphi] = \frac{\partial^{k_1}}{\partial x_1^{k_1}}\varphi(x_1)\cdots \frac{\partial^{k_n}}{\partial x_n^{k_n}}\varphi(x_n),&lt;/math&gt;

and {{mvar|G}} is a functional of {{mvar|J}}, then

:&lt;math&gt;F\left[-i\frac{\delta}{\delta J}\right] G[J] = (-i)^n \frac{\partial^{k_1}}{\partial x_1^{k_1}}\frac{\delta}{\delta J(x_1)} \cdots \frac{\partial^{k_n}}{\partial x_n^{k_n}}\frac{\delta}{\delta J(x_n)} G[J].&lt;/math&gt;

Then, from the properties of the [[functional integral]]s

:&lt;math&gt;\left \langle \frac{\delta \mathcal{S}}{\delta \varphi(x)} [\varphi] + J(x)\right\rangle_J = 0&lt;/math&gt;

we get the "master" Schwinger–Dyson equation:

:&lt;math&gt;\frac{\delta \mathcal{S}}{\delta \varphi(x)}\left[-i \frac{\delta}{\delta J}\right]Z[J] + J(x)Z[J] = 0,&lt;/math&gt;

or

:&lt;math&gt;\mathcal{S}_{,i}[-i\partial]Z + J_i Z = 0.&lt;/math&gt;

If the functional measure is not translationally invariant, it might be possible to express it as the product {{math|''M''[''φ''] {{mathcal|D}}''φ''}}, where {{mvar|M}} is a functional and {{math|{{mathcal|D}}''φ''}} is a translationally invariant measure. This is true, for example, for nonlinear sigma models where the [[target space]] is diffeomorphic to {{math|'''R'''&lt;sup&gt;''n''&lt;/sup&gt;}}. However, if the [[target manifold]] is some topologically nontrivial space, the concept of a translation does not even make any sense.

In that case, we would have to replace the {{mathcal|S}} in this equation by another functional
:&lt;math&gt;\hat{\mathcal{S}} = \mathcal{S} - i\ln M.&lt;/math&gt;

If we expand this equation as a [[Taylor series]] about ''J'' {{=}} 0, we get the entire set of Schwinger–Dyson equations.

== Localization ==
The path integrals are usually thought of as being the sum of all paths through an infinite space–time. However, in [[local quantum field theory]] we would restrict everything to lie within a finite ''causally complete'' region, for example inside a double light-cone. This gives a more mathematically precise and physically rigorous definition of quantum field theory.

=== Ward–Takahashi identities ===
{{main|Ward–Takahashi identity}}

Now how about the [[on shell]] [[Noether's theorem]] for the classical case? Does it have a quantum analog as well? Yes, but with a caveat. The functional measure would have to be invariant under the one parameter group of symmetry transformation as well.

Let's just assume for simplicity here that the symmetry in question is local (not local in the sense of a [[gauge symmetry]], but in the sense that the transformed value of the field at any given point under an infinitesimal transformation would only depend on the field configuration over an arbitrarily small neighborhood of the point in question). Let's also assume that the action is local in the sense that it is the integral over spacetime of a [[Lagrangian (field theory)|Lagrangian]], and that
:&lt;math&gt;Q[\mathcal{L}(x)]=\partial_\mu f^\mu (x)&lt;/math&gt;
for some function {{mvar|f}} where {{mvar|f}} only depends locally on {{mvar|φ}} (and possibly the spacetime position).

If we don't assume any special boundary conditions, this would not be a "true" symmetry in the true sense of the term in general unless {{math|''f'' {{=}} 0}} or something. Here, {{mvar|Q}} is a [[Derivation (abstract algebra)|derivation]] which generates the one parameter group in question. We could have [[antiderivation]]s as well, such as [[BRST quantization|BRST]] and [[supersymmetry]].

Let's also assume
:&lt;math&gt;\int \mathcal{D}\varphi\, Q[F][\varphi]=0&lt;/math&gt;
for any polynomially-bounded functional {{mvar|F}}. This property is called the invariance of the measure. And this does not hold in general. See [[anomaly (physics)]] for more details.

Then,
:&lt;math&gt;\int \mathcal{D}\varphi\, Q\left[F e^{iS}\right][\varphi]=0,&lt;/math&gt;

which implies
:&lt;math&gt;\langle Q[F]\rangle +i\left\langle F\int_{\partial V} f^\mu\, ds_\mu\right\rangle=0&lt;/math&gt;

where the integral is over the boundary. This is the quantum analog of Noether's theorem.

Now, let's assume even further that {{mvar|Q}} is a local integral

:&lt;math&gt;Q=\int d^dx\, q(x)&lt;/math&gt;

where

:&lt;math&gt;q(x)[\varphi(y)] = \delta^{(d)}(X-y)Q[\varphi(y)] \,&lt;/math&gt;

so that

:&lt;math&gt;q(x)[S]=\partial_\mu j^\mu (x) \,&lt;/math&gt;

where

:&lt;math&gt;j^{\mu}(x)=f^\mu(x)-\frac{\partial}{\partial (\partial_\mu \varphi)}\mathcal{L}(x) Q[\varphi] \,&lt;/math&gt;

(this is assuming the Lagrangian only depends on {{mvar|φ}} and its first partial derivatives! More general Lagrangians would require a modification to this definition!). Note that we're NOT insisting that {{math|''q''(''x'')}} is the generator of a symmetry (i.e. we are ''not'' insisting upon the [[gauge principle]]), but just that {{mvar|Q}} is.  And we also assume the even stronger assumption that the functional measure is locally invariant:

:&lt;math&gt;\int \mathcal{D}\varphi\, q(x)[F][\varphi]=0.&lt;/math&gt;

Then, we would have

:&lt;math&gt;\langle q(x)[F] \rangle +i\langle F q(x)[S]\rangle=\langle q(x)[F]\rangle +i\left\langle F\partial_\mu j^\mu(x)\right\rangle=0.&lt;/math&gt;

Alternatively,

:&lt;math&gt;q(x)[S]\left[-i \frac{\delta}{\delta J}\right]Z[J]+J(x)Q[\varphi(x)]\left[-i \frac{\delta}{\delta J}\right]Z[J]=\partial_\mu j^\mu(x)\left[-i \frac{\delta}{\delta J}\right]Z[J]+J(x)Q[\varphi(x)]\left[-i \frac{\delta}{\delta J}\right]Z[J]=0.&lt;/math&gt;

The above two equations are the '''Ward–Takahashi identities'''.

Now for the case where {{math|''f'' {{=}} 0}}, we can forget about all the boundary conditions and locality assumptions. We'd simply have

:&lt;math&gt;\left\langle Q[F]\right\rangle =0.&lt;/math&gt;

Alternatively,

:&lt;math&gt;\int d^dx\, J(x)Q[\varphi(x)]\left[-i \frac{\delta}{\delta J}\right]Z[J]=0.&lt;/math&gt;

== The need for regulators and renormalization ==

Path integrals as they are defined here require the introduction of [[Regularization (physics)|regulators]]. Changing the scale of the regulator leads to the [[renormalization group]]. In fact, renormalization is the major obstruction to making path integrals well-defined.

== The path integral in quantum-mechanical interpretation ==

In one [[interpretation of quantum mechanics]], the "sum over histories" interpretation, the path integral is taken to be fundamental, and reality is viewed as a single indistinguishable "class" of paths that all share the same events.  For this interpretation, it is crucial to understand what exactly an event is. The sum-over-histories method gives identical results to canonical quantum mechanics, and Sinha and Sorkin&lt;ref&gt;{{harvnb|Sinha|Sorkin|1991}}&lt;/ref&gt; claim the interpretation explains the [[Einstein–Podolsky–Rosen paradox]] without resorting to [[action at a distance|nonlocality]].

Some{{who|date=August 2014}} advocates of interpretations of quantum mechanics emphasizing [[decoherence]] have attempted to make more rigorous the notion of extracting a classical-like "coarse-grained" history from the space of all possible histories.

== Quantum gravity ==
Whereas in quantum mechanics the path integral formulation is fully equivalent to other formulations, it may be that it can be extended to quantum gravity, which would make it different from the [[Hilbert space]] model. Feynman had some success in this direction, and his work has been extended by [[Stephen Hawking|Hawking]] and others.&lt;ref&gt;{{harvnb|Gell-Mann|1993}}&lt;/ref&gt; Approaches that use this method include [[causal dynamical triangulation]]s and [[spinfoam]] models.

== Quantum tunneling ==
[[Quantum tunnelling]] can be modeled by using the path integral formation to determine the action of the trajectory through a potential barrier. Using the [[WKB approximation]], the tunneling rate ({{math|Γ}}) can be determined to be of the form

:&lt;math&gt; \Gamma = A_\mathrm{o} \exp \left(-\frac{S_\mathrm{eff}}{\hbar}\right) &lt;/math&gt;

with the effective action {{math|''S''&lt;sub&gt;eff&lt;/sub&gt;}} and pre-exponential factor {{math|''A''&lt;sub&gt;o&lt;/sub&gt;}}. This form is specifically useful in a [[dissipative system]], in which the systems and surroundings must be modeled together. Using the [[Langevin equation]] to model [[Brownian motion]], the path integral formation can be used to determine an effective action and pre-exponential model to see the effect of dissipation on tunnelling.&lt;ref&gt;{{harvnb|Caldeira|Leggett|1983}}&lt;/ref&gt; From this model, tunneling rates of macroscopic systems (at finite temperatures) can be predicted.

== See also ==
* [[Theoretical and experimental justification for the Schrödinger equation]]
* [[Static forces and virtual-particle exchange]]
* [[Feynman checkerboard]]
* [[Berezin integral]]
* [[Propagator]]s
* [[Wheeler–Feynman absorber theory]]
* [[Feynman–Kac formula]]

== Remarks ==
{{Reflist|group=nb}}

== Notes ==
{{Reflist|2}}

== References ==
* {{cite book|ref=harv|last=Ahmad|first= Ishfaq |authorlink=Ishfaq Ahmad |title=Mathematical Integrals in Quantum Nature |series=The Nucleus |year=1971 |pages=189–209}}
* {{cite book|ref=harv|last1=Albeverio|first1= S.|last2=Hoegh-Krohn.|first2=R.|last3=Mazzucchi|first3=S|lastauthoramp=yes |title=Mathematical Theory of Feynman Path Integral |series=Lecture Notes in Mathematics 523 |publisher=Springer-Verlag |year=2008 |isbn=9783540769569}}
*{{cite journal|ref=harv|first=A. O.|last=Caldeira|authorlink1=Amir Caldeira|first2=A. J.|last2=Leggett|authorlink2=Anthony James Leggett|title=Quantum tunnelling in a dissipative system|journal=Annals of Physics|volume=149|year=1983|issue=2|pages=374–456 |doi=10.1016/0003-4916(83)90202-6 |url=http://www.sciencedirect.com/science/article/pii/0003491683902026 |bibcode=1983AnPhy.149..374C}}
* {{cite journal|ref=harv|last=Cartier |first=P |authorlink=Pierre Cartier (mathematician)|last2=DeWitt-Morette |first2=Cécile |title=A new perspective on Functional Integration |journal=Journal of Mathematical Physics |volume=36 |year=1995 |issue=5 |pages=2137–2340 |doi=10.1063/1.531039 |arxiv=funct-an/9602005 |bibcode = 1995JMP....36.2237C }}
*{{cite book |ref=harv|title=Path Integrals in Physics Volume 1: Stochastic Process &amp; Quantum Mechanics |url=https://books.google.com/books?id=-XDP-8mrmQYC&amp;pg=PA1 |chapter=Introduction |page=1ff |isbn=0-7503-0801-X |year=2001 |publisher=Taylor &amp; Francis |first1=M.|last1= Chaichian |first2=A. P.|last2= Demichev }}
* {{cite journal|ref=harv|authorlink=Cécile DeWitt-Morette |last=DeWitt-Morette |first=C.|title=Feynman's path integral: Definition without limiting procedure |journal=Communications in Mathematical Physics |volume=28 |issue=1 |year=1972 |pages=47–67 |mr=0309456 |doi=10.1007/BF02099371 |bibcode = 1972CMaPh..28...47D}}
*{{cite journal|ref=harv|last=Dirac |first=Paul A. M. |authorlink=Paul Dirac |year=1933 |title=The Lagrangian in Quantum Mechanics |journal=Physikalische Zeitschrift der Sowjetunion |volume=3 |pages=64–72 |url=http://www.hep.anl.gov/czachos/soysoy/Dirac33.pdf}}
*{{cite journal|ref=harv|first1=İ. H.|last1=Duru|authorlink1=İsmail Hakkı Duru|first2=Hagen|last2=Kleinert|authorlink2=Hagen Kleinert|title=Solution of the path integral for the H-atom |year=1979 |journal=Physics Letters |volume=84B |issue=2 |pages=185–188 |url=http://www.physik.fu-berlin.de/~kleinert/kleiner_re65/65.pdf|accessdate=2007-11-25|bibcode=1979PhLB...84..185D |doi=10.1016/0370-2693(79)90280-6}}
* {{cite web|ref=harv|last=Etingof|first=P|authorlink=Pavel Etingof |title=Geometry and Quantum Field Theory |publisher=MIT OpenCourseWare |year=2002 |url=http://ocw.mit.edu/courses/mathematics/18-238-geometry-and-quantum-field-theory-fall-2002/index.htm}} &lt;small&gt;This course, designed for mathematicians, is a rigorous introduction to perturbative quantum field theory, using the language of functional integrals.&lt;/small&gt;
*{{cite book|ref=harv|last=Feynman|first=R. P.|authorlink=Richard Feynman|editor-last=Brown|editor-first=L. M|title=Feynman's Thesis — A New Approach to Quantum Theory|year=2005|origyear=1942/1948|isbn=978-981-256-366-8|publisher=World Scientific|url=http://www.worldscientific.com/worldscibooks/10.1142/5852}} &lt;small&gt;The 1942 thesis. Also includes Dirac's 1933 paper and Feynman's 1948 publication.&lt;/small&gt;
*{{cite journal|ref=harv|last=Feynman|first=R. P. | title=Space-Time Approach to Non-Relativistic Quantum Mechanics |journal=Reviews of Modern Physics|volume= 20| pages=367–387|year=1948|doi= 10.1103/RevModPhys.20.367 |bibcode = 1948RvMP...20..367F|issue= 2 }}
* {{cite book |last1=Feynman|first1=R. P.|last2=Hibbs |first2=A. R. |year=1965 |title=Quantum Mechanics and Path Integrals |place=New York |publisher=McGraw-Hill |isbn=0-07-020650-3}} &lt;small&gt;The historical reference, written by the inventor of the path integral formulation himself and one of his students.&lt;/small&gt;
*{{cite book|ref=harv|last1=Feynman|first1=R. P.|last2=Hibbs|first2=A. R.|authorlink2=Albert Hibbs|last3=Styer|first3=D. F.|authorlink3=Daniel F. Styer|title = Quantum Mechanics and Path Integrals|year=2010|publisher=Dover Publications|location=Mineola, NY|isbn=0-486-47722-3| pages = 29–31 }}
*{{cite book|ref=harv|contribution=Most of the Good Stuff|title=Memories Of Richard Feynman|editor1-first=Laurie M.|editor1-last=Brown|editor2-first=John S.|editor2-last=Rigden|publisher=American Institute of Physics|first=Murray|last=Gell-Mann|authorlink=Murray Gell-Mann|isbn=978-0883188705|year=1993}}
*{{cite book|ref=harv|last1=Glimm|first1=J.|last2=Jaffe|first2=A|lastauthoramp=yes |title=Quantum Physics: A Functional Integral Point of View |place=New York |publisher=Springer-Verlag |year=1981 |isbn=0-387-90562-6}}
* {{cite book|ref=harv|last1=Glimm|first1=J.|last2=Jaffe|first2=A.|lastauthoramp=yes|title=Quantum Physics: A Functional Integral Point of View |place=New York |publisher=Springer-Verlag |year=1981 |isbn=0-387-90562-6}}
* {{cite book|ref=harv|last1=Grosche|first= Christian  |author2=Steiner|first2= Frank  |lastauthoramp=yes |year=1998 |title=Handbook of Feynman Path Integrals |series=Springer Tracts in Modern Physics 145 |publisher=Springer-Verlag |isbn=3-540-57135-3}}
* {{cite arXiv|ref=harv|last=Grosche |first=Christian |title=An Introduction into the Feynman Path Integral  |year=1992 |eprint=hep-th/9302097}}
*{{cite book|ref=harv|last=Hall |first=Brian C. |year=2013 |title=Quantum Theory for Mathematicians|series=Graduate Texts in Mathematics|volume=267 |publisher=Springer|isbn=978-1-4614-7115-8|doi=10.1007/978-1-4614-7116-5}}
*{{cite book|ref=harv|last=Inomata|first= Akira|last2= Kuratsuji|first2= Hiroshi|last3= Gerry|first3= Christopher  |title=Path Integrals and Coherent States of SU(2) and SU(1,1) |place=Singapore |publisher=World Scientific |year=1992 |isbn=981-02-0656-9}}
*{{cite book|ref=harv|editor-last1=Janke|editor-first1=W.|editor-last2=Pelster|editor-first2=Axel|title=Path Integrals--New Trends And Perspectives|year=2008|series=Proceedings Of The 9Th International Conference|publisher=World Scientific Publishing|isbn=978-981-283-726-4}}
* {{cite book|ref=harv|first1=Gerald W. |last1=Johnson  |first2=Michel L.|last2= Lapidus  |title=The Feynman Integral and Feynman's Operational Calculus |series=Oxford Mathematical Monographs |publisher=Oxford University Press |year=2002 |isbn=0-19-851572-3}}
* {{cite book|ref=harv|authorlink=John R. Klauder |last=Klauder|first=John R.|title=A Modern Approach to Functional Integration |place=New York |publisher=Birkhäuser |year=2010 |isbn=978-0-8176-4790-2}}
* {{cite book|ref=harv|authorlink=Hagen Kleinert |last=Kleinert |first=Hagen |year=2004 |title=Path Integrals in Quantum Mechanics, Statistics, Polymer Physics, and Financial Markets |edition=4th |place=Singapore |publisher=World Scientific |isbn=981-238-107-4 |url=http://www.physik.fu-berlin.de/~kleinert/b5}}
* {{cite arXiv|ref=harv|last=MacKenzie |first=Richard |year=2000 |title=Path Integral Methods and Applications |eprint=quant-ph/0004090}}
* {{cite book|ref=harv|author= Mazzucchi|first= S. |title=Mathematical Feynman path integrals and their applications|publisher=World Scientific |year=2009 |isbn=978-981-283-690-8}}
* {{cite book|ref=harv|first=Harald J. W.|last=Müller-Kirsten|year=2012|title=Introduction to Quantum Mechanics: Schrödinger Equation and Path Integral| edition=2nd|place=Singapore|publisher=World Scientific}}
* {{cite book|ref=harv|last=Rivers|first= R. J. |title=Path Integrals Methods in Quantum Field Theory |publisher=Cambridge University Press |year=1987 |isbn=0-521-25979-7}}
* {{cite book|ref=harv|last=Ryder|first= Lewis H. |title=Quantum Field Theory |publisher=Cambridge University Press |year=1985 |isbn=0-521-33859-X}} Highly readable textbook; introduction to relativistic QFT for particle physics.
* {{cite book|ref=harv|last=Schulman|first= L S. |year=1981 |title=Techniques &amp; Applications of Path Integration |place=New York |publisher=John Wiley &amp; Sons |isbn=0-486-44528-3}}
* {{cite book|ref=harv|authorlink=Barry Simon|last=Simon|first=B.|title=Functional Integration and Quantum Physics |place=New York |publisher=Academic Press |year=1979 |isbn=0-8218-6941-8}}
*{{cite journal |ref=harv|first=Sukanya |last=Sinha |first2=Rafael D. |last2=Sorkin |title=A Sum-over-histories Account of an EPR(B) Experiment |journal=Foundations of Physics Letters |volume=4 |issue=4 |pages=303–335 |year=1991 |doi=10.1007/BF00665892 |url=https://www.perimeterinstitute.ca/personal/rsorkin/some.papers/63.eprb.pdf |bibcode=1991FoPhL...4..303S }}
*{{cite book|ref=harv|last=Tomé|first=W. A.|authorlink= Wolfgang A. Tomé |year=1998 |title=Path Integrals on Group Manifolds |place=Singapore|publisher=World Scientific |isbn=981-02-3355-8}} Discusses the definition of Path Integrals for systems whose kinematical variables are the generators of a real separable, connected Lie group with irreducible, square integrable representations.
*{{cite journal|ref=harv|last=Van Vleck |first=J. H.|authorlink=John Hasbrouck Van Vleck|title=The correspondence principle in the statistical interpretation of quantum mechanics |journal=Proceedings of the National Academy of Sciences of the United States of America|volume=14| issue=2 | pages=178–188|year=1928|doi=10.1073/pnas.14.2.178 | pmid=16577107 | pmc=1085402 | bibcode=1928PNAS...14..178V }}
*{{citation|last=Weinberg|first=S.|year=2002|orig-year=1995|title=Foundations|series=The Quantum Theory of Fields|volume=1|isbn=0-521-55001-7|authorlink=Steven Weinberg|location=Cambridge|publisher=[[Cambridge University Press]]}}
* {{cite book|ref=harv|last=Zee |first=A. |authorlink=Anthony Zee |title=Quantum Field Theory in a Nutshell |edition=Second |publisher=Princeton University Press |location= |isbn=978-0-691-14034-6 }} A great introduction to Path Integrals (Chapter 1) and QFT in general.
*{{cite book|ref=harv|author=Zinn Justin|first= J.|authorlink=Jean Zinn-Justin|year=2004 |title=Path Integrals in Quantum Mechanics |publisher=Oxford University Press |isbn=0-19-856674-3}}

== External links ==
* [http://www.scholarpedia.org/article/Path_integral Path integral on Scholarpedia]
* [http://www.quantumfieldtheory.info/website_Chap18.pdf Path Integrals in Quantum Theories: A Pedagogic 1st Step]
* [https://www.youtube.com/watch?v=QTjmLBzAdAA A mathematically rigorous approach to perturbative path integrals]  via animation on YouTube

{{Quantum mechanics topics|state=expanded}}
{{Richard Feynman|state=collapsed}}


[[Category:Concepts in physics]]
[[Category:Statistical mechanics]]
[[Category:Quantum mechanics]]
[[Category:Quantum field theory]]
[[Category:Differential equations]]
[[Category:Articles containing video clips]]
[[Category:Mathematical physics]]
[[Category:Integrals]]</text>
      <sha1>a84k79faetbtr8l6690ic7ysf12qkfr</sha1>
    </revision>
  </page>
  <page>
    <title>Paul Chernoff</title>
    <ns>0</ns>
    <id>56424327</id>
    <revision>
      <id>826336452</id>
      <parentid>823908073</parentid>
      <timestamp>2018-02-18T15:39:31Z</timestamp>
      <contributor>
        <username>Baroc</username>
        <id>12983230</id>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6295">[[File:Paul Chernoff.jpg|thumb|Paul Chernoff]]
'''Paul Robert Chernoff''' (21 June 1942,  [[Philadelphia]] – 17 January 2017)&lt;ref&gt;biographical information from ''American Men and Women of Science'', Thomson Gale 2004&lt;/ref&gt;  was an American mathematician, specializing in functional analysis and the mathematical foundations of quantum mechanics.&lt;ref name=ObitSFC&gt;{{cite newspaper|title=Obituary. Paul Chernoff|newspaper=San Francisco Chronicle|date=2 April 2017|url=http://www.legacy.com/obituaries/sfgate/obituary.aspx?pid=184707211}}&lt;/ref&gt; He is known for Chernoff's Theorem, a mathematical result in the Feynman path integral formulation of quantum mechanics.&lt;ref&gt;{{cite journal|author=Butko, Yana A.|title=Chernoff approximation of subordinate semigroups and applications|journal=Stochastics and Dynamics|pages=1850021|year=2015|doi=10.1142/S0219493718500211}}&lt;/ref&gt;

==Education and career==
Chernoff graduated from [[Central High School (Philadelphia)|Central High School in Philadelphia]]. He matriculated at [[Harvard University]], where he received bachelor's degree ''summa cum laude'' in 1963, master's degree in 1965, and Ph.D. in 1968 under [[George Mackey]] with thesis ''Semigroup Product Formulas and Addition of Unbounded Operators''.&lt;ref&gt;{{MathGenealogy|id=30697|name=Paul Robert Chernoff}}&lt;/ref&gt;

At the [[University of California, Berkeley]] he became in 1969 a lecturer, in 1971 an assistant professor, and in 1980 a full professor. U. C. Berkeley awarded him multiple Distinguished Teaching Awards and the Lili Fabilli and Eric Hoffer Essay Prize.&lt;ref name=ObitSFC/&gt; In 1986 he was a visiting professor at the [[University of Pennsylvania]].

Chernoff was elected in 1984 a Fellow of the [[American Association for the Advancement of Science]]&lt;ref&gt;{{cite newspaper|newspaper=University of California Bulletin, week of August 6–10, 1984|volume=33|issue=3|title=American Association for the Advancement of Science Elects University Members|page=12|url=https://books.google.com/books?id=2vM2AQAAMAAJ&amp;pg=PA12}}&lt;/ref&gt; and in 2012 a Fellow of the [[American Mathematical Society]].

He gave in 1981 a simplified proof of the [[Hilbrand J. Groenewold|Groenewold]]-[[Léon Van Hove|Van Hove]] theorem,&lt;ref&gt;Chernoff, Mathematical obstructions to quantization, Hadronic J., vol. 4, 1981, pp. 879–898&lt;/ref&gt;&lt;ref&gt;{{cite book|author=Sternberg, Shlomo|authorlink=Shlomo Sternberg|author2=Guillemin, Victor|authorlink2=Victor Guillemin|title=Symplectic Techniques in Physics|publisher=Cambridge University Press|year=1990|pages=101–102|url=https://books.google.com/books?id=O7Rbx4ptxqsC&amp;pg=PA101|isbn=9780521389907}}&lt;/ref&gt;&lt;ref&gt;{{cite book|author=Berndt, Rolf|title=Einführung in die Symplektische Geometrie|publisher=Vieweg|year=1998|pages=119–120|url=https://books.google.com/books?id=BXOMdQ-jFUsC&amp;pg=PA119|isbn=9783322802156}}&lt;/ref&gt; which is a [[no-go theorem]] that relates classical mechanics to quantum mechanics.&lt;ref name=ObitSFC/&gt;

==Selected publications==
*Note on product formulas for operator semigroups, J. Funct. Analysis, vol. 2, 1968, pp.&amp;nbsp;238–242 {{doi|10.1016/0022-1236(68)90020-7}}
*with Richard Anthony Rasala and [[William C. Waterhouse]]: [https://msp.org/pjm/1968/27-2/pjm-v27-n2-p03-p.pdf The Stone-Weierstrass theorem for valuable fields] Pacific Journal of Mathematics vol. 27, no. 2, 1968, pp.&amp;nbsp;233–240
*Some remarks on quasi-analytic vectors, Trans. Amer. Math. Soc. vol. 167, 1972, pp.&amp;nbsp;105–113 {{doi|10.1090/S0002-9947-1972-0295125-5}}
*Representations, automorphisms, and derivations of some operator algebras, J. Funct. Analysis, vol. 12, 1973, pp.&amp;nbsp;275–289 {{doi|10.1016/0022-1236(73)90080-3}}
*Essential self-adjointness of powers of generators of hyperbolic equations, J. Funct. Analysis, vol. 12, 1973, pp.&amp;nbsp;401–414 {{doi|10.1016/0022-1236(73)90003-7}}
*[https://books.google.com/books?id=XeHTCQAAQBAJ Product formulas, nonlinear semigroups, and addition of unbounded operators], American Mathematical Society 1974.
*with [[Jerrold Marsden]]: [https://books.google.com/books/?id=F1x7CwAAQBAJ Properties of infinite dimensional Hamiltonian systems], Springer 1974
*[http://science.sciencemag.org/content/193/4250/276.1 Understanding mathematical proofs: Conceptual barriers]. Science vol. 193, no. 4250, 1976, p.&amp;nbsp;276
*[https://msp.org/pjm/1977/70-1/pjm-v70-n1-p11-s.pdf The quantum ''n''-body problem and a theorem of Littlewood], Pacific J. Math., vol. 70, 1977, pp.&amp;nbsp;117–123
*Irreducible representations of infinite-dimensional transformation groups and Lie algebras, I. J. Funct. Anal., vol. 130, 1995, pp.&amp;nbsp;255–282 {{doi|10.1006/jfan.1995.1069}}
*with [[Rhonda Hughes]]: "A new class of point interactions in one dimension." Journal of functional analysis, vol. 111, no. 1, 1993, pp.&amp;nbsp;97–117 {{doi|10.1006/jfan.1993.1006}}
*with R. Hughes: Some examples related to Kato's conjecture. J. Austral. Math. Soc. Ser. A, vol. 60, 1996, pp.&amp;nbsp;274–286. {{doi|10.1017/S1446788700037666}}
*[http://emis.ams.org/journals/EJDE/conf-proc/04/c2/chernoff.pdf Quantization and irreducible representations of infinite-dimensional transformation groups and Lie algebras]. In: Proceedings of the Symposium on Mathematical Physics and Quantum Field Theory (Berkeley, CA, 1999), Eletron. J. Differ. Equ. Conf., vol. 4, 2000, pp.&amp;nbsp;17–22
*A pseudo zeta function and the distribution of primes, Proc. Natl. Acad. Sci. USA, vol.  97, 2000, pp.&amp;nbsp;7697–7699 {{pmc|16606}} (There is a typographical error: "One can show that C(s) may be analytically continued at least into the half-plane Re s &gt; 0 except for an isolated singularity (presumably a simple pole) at s = 0." This should be "at s = 1" according to the mathematical argument given.)

==References==
{{reflist}}

{{authority control}}

{{DEFAULTSORT:Chernoff, Paul}}
[[Category:1942 births]]
[[Category:2017 deaths]]
[[Category:20th-century American mathematicians]]
[[Category:21st-century American mathematicians]]
[[Category:Mathematical analysts]]
[[Category:Central High School (Philadelphia) alumni]]
[[Category:Harvard University alumni]]
[[Category:University of California, Berkeley faculty]]
[[Category:Fellows of the American Association for the Advancement of Science]]
[[Category:Fellows of the American Mathematical Society]]</text>
      <sha1>0qsdbz1agm1pah6t3uvrv4iuaa0snra</sha1>
    </revision>
  </page>
  <page>
    <title>Probabilistic method</title>
    <ns>0</ns>
    <id>173525</id>
    <revision>
      <id>864332284</id>
      <parentid>861279855</parentid>
      <timestamp>2018-10-16T15:02:11Z</timestamp>
      <contributor>
        <ip>137.81.133.127</ip>
      </contributor>
      <comment>/* First example */</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="10667">{{hatnote|This article is not about [[interactive proof system]]s which use probability to convince a verifier that a proof is correct, nor about [[probabilistic algorithm]]s, which give the right answer with high probability but not with certainty, nor about [[Monte Carlo method]]s, which are algorithms involving repeated random sampling.}}

The '''probabilistic method''' is a [[nonconstructive proof|nonconstructive]] method, primarily used in [[combinatorics]] and pioneered by [[Paul Erdős]], for proving the existence of a prescribed kind of mathematical object. It works by showing that if one randomly chooses objects from a specified class, the [[probability]] that the result is of the prescribed kind is strictly greater than zero. Although the proof uses probability, the final conclusion is determined for ''certain'', without any possible error.

This method has now been applied to other areas of [[mathematics]] such as [[number theory]], [[linear algebra]], and [[real analysis]], as well as in [[computer science]] (e.g. [[randomized rounding]]), and [[information theory]].

==Introduction==
If every object in a collection of objects fails to have a certain property, then the probability that a random object chosen from the collection has that property is zero.

Similarly, showing that the probability is (strictly) less than 1 can be used to prove the existence of an object that does ''not'' satisfy the prescribed properties.

Another way to use the probabilistic method is by calculating the [[expected value]] of some [[random variable]]. If it can be shown that the random variable can take on a value less than the expected value, this proves that the random variable can also take on some value greater than the expected value.

Common tools used in the probabilistic method include [[Markov's inequality]], the [[Chernoff bound]], and the [[Lovász local lemma]].

==Two examples due to Erdős==
Although others before him proved theorems via the probabilistic method (for example, Szele's 1943 result that there exist [[tournament (graph theory)|tournaments]] containing a large number of [[Hamiltonian cycle]]s), many of the most well known proofs using this method are due to Erdős. Indeed, the Alon-Spencer textbook on the subject has his picture on the cover to highlight the method's association with Erdős. The first example below describes one such result from 1947 that gives a proof of a lower bound for the [[Ramsey's theorem|Ramsey number]] {{math|''R''(''r'', ''r'')}}.

===First example===
Suppose we have a [[complete graph]] on {{mvar|n}} vertices. We wish to show (for small enough values of {{mvar|n}}) that it is possible to [[Edge coloring|color the edges]] of the graph in two colors (say red and blue) so that there is no complete subgraph on {{mvar|r}} vertices which is monochromatic (every edge colored the same color).

To do so, we color the graph randomly. Color each edge independently with probability {{math|1/2}} of being red and {{math|1/2}} of being blue. We calculate the expected number of monochromatic subgraphs on {{mvar|r}} vertices as follows:

For any set {{mvar|S}} of {{mvar|r}} vertices from our graph, define the variable {{math|''X''(''S'')}} to be {{math|1}} if every edge amongst the {{mvar|r}} vertices is the same color, and {{math|0}} otherwise. Note that the number of monochromatic {{mvar|r}}-subgraphs is the sum of {{math|''X''(''S'')}} over all possible subsets. For any {{mvar|S}}, the [[expected value]] of {{math|''X''(''S'')}} is simply the probability that all of the

:&lt;math&gt;{r \choose 2}&lt;/math&gt;

edges in {{mvar|S}} are the same color,

:&lt;math&gt;2 \cdot 2^{-{r \choose 2}}&lt;/math&gt;

(the factor of {{math|2}} comes because there are two possible colors).

This holds true for any of the {{math|''C''(''n'', ''r'')}} possible subsets we could have chosen, so we have that the sum of {{math|''E''[''X''(''S'')]}} over all {{mvar|S}} is

:&lt;math&gt;{n \choose r}2^{1-{r \choose 2}}.&lt;/math&gt;

The sum of an expectation is the expectation of the sum (''regardless'' of whether the variables are [[statistical independence|independent]]), so the expectation of the sum (the expected number of monochromatic {{mvar|r}}-subgraphs) is

:&lt;math&gt;{n \choose r}2^{1-{r \choose 2}}.&lt;/math&gt;

Consider what happens if this value is less than {{math|1}}. Since the expected number of monochromatic {{mvar|r}}-subgraphs is strictly less than 1, it must be that a specific random coloring satisfies that the number of monochromatic {{mvar|r}}-subgraphs is strictly less than 1.  The number of monochromatic {{mvar|r}}-subgraphs in this random coloring is a non-negative integer, hence it must be 0 (0 is the only non-negative integer less than 1).  It follows that if :&lt;math&gt;{n \choose r}2^{1-{r \choose 2}} &lt; 1 ,&lt;/math&gt;, (which holds, for example, for {{mvar|n}}=5 and {{mvar|r}}=4)  there must exist a coloring in which there are no monochromatic {{mvar|r}}-subgraphs.   &lt;ref&gt;The same fact can be proved without probability, using a simple counting argument:
* The total number of ''r''-subgraphs is &lt;math&gt;{n \choose r}&lt;/math&gt;.
* Each ''r''-subgraphs has &lt;math&gt;{r \choose 2}&lt;/math&gt; edges and thus can be colored in &lt;math&gt;2^{r \choose 2}&lt;/math&gt; different ways.
* Of these colorings, only 2 colorings are 'bad' for that subgraph (the colorings in which all vertices are red or all vertices are blue).
* Hence, the total number of colorings that are bad for ''all'' subgraphs is at most &lt;math&gt;2 {n \choose r}&lt;/math&gt;.
* Hence, if &lt;math&gt;2^{r \choose 2} &gt; 2 {n \choose r}&lt;/math&gt;, there must be at least one coloring which is not 'bad' for any subgraph.
&lt;/ref&gt;

By definition of the [[Ramsey number]], this implies that {{math|''R''(''r'', ''r'')}} must be bigger than {{mvar|n}}.  In particular, {{math|''R''(''r'', ''r'')}} must [[exponential growth|grow at least exponentially]] with {{mvar|r}}.

A peculiarity of this argument is that it is entirely [[nonconstructive proof|nonconstructive]].  Even though it proves (for example) that almost every coloring of the complete graph on {{math|(1.1)&lt;sup&gt;''r''&lt;/sup&gt;}} vertices contains no monochromatic {{mvar|r}}-subgraph, it gives no explicit example of such a coloring. The problem of finding such a coloring has been open for more than 50 years.

===Second example===
A 1959 paper of Erdős (see reference cited below) addressed the following problem in [[graph theory]]: given positive integers {{mvar|g}} and {{mvar|k}}, does there exist a graph {{mvar|G}} containing  only [[cycle (graph theory)|cycles]] of length at least {{mvar|g}}, such that the [[chromatic number]] of {{mvar|G}} is at least {{mvar|k}}?

It can be shown that such a graph exists for any {{mvar|g}} and {{mvar|k}}, and the proof is reasonably simple.  Let {{mvar|n}} be very large and consider a random graph {{mvar|G}} on {{mvar|n}} vertices, where every edge in {{mvar|G}} exists with probability {{math|''p'' {{=}} ''n''&lt;sup&gt;1/''g''−1&lt;/sup&gt;}}.  We show that with positive probability, a graph satisfies the following two properties:

:'''Property 1.''' {{mvar|G}} contains at most {{math|''n''/2}} cycles of length less than {{mvar|g}}.

'''Proof.''' Let {{mvar|X}} be the number cycles of length less than {{mvar|g}}. Number of cycles of length {{mvar|i}} in the complete graph on {{mvar|n}} vertices is

:&lt;math&gt;\frac{n!}{2\cdot i \cdot (n-i)!} \le \frac{n^i}{2}&lt;/math&gt;

and each of them is present in {{mvar|G}} with probability {{math|''p&lt;sup&gt;i&lt;/sup&gt;''}}. Hence by [[Markov's inequality]] we have

:&lt;math&gt;\Pr \left (X&gt; \tfrac{n}{2} \right )\le \frac{2}{n} E[X] \le \frac{1}{n} \sum_{i=3}^{g-1} p^i n^i = \frac{1}{n} \sum_{i=3}^{g-1} n^{\frac{i}{g}} \le \frac{g}{n} n^{\frac{g-1}{g}} =  gn^{-\frac{1}{g}} = o(1).&lt;/math&gt;
: Thus for sufficiently large {{mvar|n}}, property 1 holds with a probability of more than {{math|1/2}}.

:'''Property 2.''' {{mvar|G}} contains no [[Independent set (graph theory)|independent set]] of size &lt;math&gt;\lceil \tfrac{n}{2k} \rceil&lt;/math&gt;.

'''Proof.''' Let {{mvar|Y}} be the size of the largest independent set in {{mvar|G}}. Clearly, we have

:&lt;math&gt;\Pr (Y\ge y) \le {n \choose y}(1-p)^{\frac{y(y-1)}{2}} \le n^y e^{-\frac{py(y-1)}{2}} = e^{- \frac{y}{2} \cdot (py -2\ln n - p)} = o(1),&lt;/math&gt;
when

:&lt;math&gt;y = \left \lceil \frac{n}{2k} \right \rceil.&lt;/math&gt; Thus, for sufficiently large {{mvar|n}}, property 2 holds with a probability of more than {{math|1/2}}.

For sufficiently large {{mvar|n}}, the probability that a graph from the distribution has both properties is positive, as the events for these properties cannot be disjoint (if they were, their probabilities would sum up to more than 1).

Here comes the trick: since {{mvar|G}} has these two properties, we can remove at most {{math|''n''/2}} vertices from {{mvar|G}} to obtain a new graph {{math|''G′''}} on &lt;math&gt;n'\geq n/2&lt;/math&gt; vertices that contains only cycles of length at least {{mvar|g}}. We can see that this new graph has no independent set of size &lt;math&gt;\lceil \frac{n'}{k} \rceil&lt;/math&gt;. {{math|''G′''}} can only be partitioned into at least {{mvar|k}} independent sets, and, hence, has chromatic number at least {{mvar|k}}.

This result gives a hint as to why the computation of the [[Graph coloring|chromatic number]] of a graph is so difficult: even when there are no local reasons (such as small cycles) for a graph to require many colors the chromatic number can still be arbitrarily large.

==See also==
{{Portal|Mathematics}}
*[[Interactive proof system]]
*[[Method of conditional probabilities]]
*[[Probabilistic proofs of non-probabilistic theorems]]
*[[Random graph]]

==References==
* Alon, Noga; Spencer, Joel H. (2000).  ''The probabilistic method'' (2ed).  New York: Wiley-Interscience.  {{isbn|0-471-37046-0}}.
* {{cite journal |doi=10.4153/CJM-1959-003-9 |author=Erdős, P. |year=1959 |title=Graph theory and probability |journal=Can. J. Math. |volume=11 |issue=0 |pages=34–38 |mr=0102081 |url=http://www.math-inst.hu/~p_erdos/1959-06.pdf}}
* {{cite journal |doi=10.4153/CJM-1961-029-9 |author=Erdős, P. |year=1961 |title=Graph theory and probability, II |journal=Can. J. Math. |volume=13 |issue=0 |pages=346–352 |mr=0120168 |url=http://www.math-inst.hu/~p_erdos/1961-06.pdf}}
* [[Jiří Matoušek (mathematician)|J. Matoušek]], J. Vondrak. [https://web.archive.org/web/20120205002452/http://kam.mff.cuni.cz/~matousek/prob-ln-2pp.ps.gz The Probabilistic Method]. Lecture notes.
* Alon, N and Krivelevich, M (2006). [http://www.math.tau.ac.il/~nogaa/PDFS/epc7.pdf Extremal and Probabilistic Combinatorics]

==Footnotes==
&lt;references/&gt;

[[Category:Combinatorics]]
[[Category:Mathematical proofs]]
[[Category:Probabilistic arguments]]</text>
      <sha1>obxd9b9im5rmmeptdpjs3ph225ze46y</sha1>
    </revision>
  </page>
  <page>
    <title>Provable prime</title>
    <ns>0</ns>
    <id>2549312</id>
    <revision>
      <id>747620780</id>
      <parentid>635592174</parentid>
      <timestamp>2016-11-03T11:43:40Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor/>
      <comment>/* top */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1031">{{distinguish|Probable prime}}
In [[number theory]], a '''provable prime''' is an [[integer]] that has been calculated to be [[prime number|prime]] using a primality-proving [[algorithm]]. Contrast with [[probable prime]], which is likely (but not certain) to be prime, based on the output of a [[probabilistic algorithm|probabilistic]] [[primality test]]. In principle, every prime number can be proved to be prime in [[polynomial time]] by using the [[AKS primality test]]. In practice, other methods which guarantee that their result is prime, but which do not work for all primes, are useful for the random generation of provable primes.&lt;ref&gt;{{citation|title=RSA and Public-Key Cryptography|series=Discrete Mathematics and Its Applications|first=Richard A.|last=Mollin|publisher=CRC Press|year=2002|isbn=9781420035247|pages=124–125|url=https://books.google.com/books?id=mZUYNy0UGNUC&amp;pg=PA124}}.&lt;/ref&gt;

==See also==
*[[Primality test]]
*[[Probable prime]]

==References==
{{reflist}}

{{num-stub}}
[[Category:Primality tests]]</text>
      <sha1>q5yytto17diftenis8lwszguoa3x5k6</sha1>
    </revision>
  </page>
  <page>
    <title>Rank (type theory)</title>
    <ns>0</ns>
    <id>31654716</id>
    <redirect title="Parametric polymorphism" />
    <revision>
      <id>427056317</id>
      <timestamp>2011-05-02T12:34:29Z</timestamp>
      <contributor>
        <username>Ruud Koot</username>
        <id>170083</id>
      </contributor>
      <comment>[[WP:AES|←]]Redirected page to [[Parametric polymorphism#Higher-ranked polymorphism]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="90">#REDIRECT [[Parametric polymorphism#Higher-ranked polymorphism]]

[[Category:Type theory]]</text>
      <sha1>t77z1pul7sks654v9ipb8q8o00b3whf</sha1>
    </revision>
  </page>
  <page>
    <title>Roger W. Brockett</title>
    <ns>0</ns>
    <id>11689633</id>
    <revision>
      <id>868138871</id>
      <parentid>843710505</parentid>
      <timestamp>2018-11-10T07:02:20Z</timestamp>
      <contributor>
        <username>Ser Amantio di Nicolao</username>
        <id>753665</id>
      </contributor>
      <minor/>
      <comment>/* External links */add category</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="6640">{{Infobox scientist
| name             = Roger W. Brockett
| image            =
| birth_date       = {{Birth date and age|1938|10|22}}
| birth_place      = [[Seville, Ohio]]
| death_date       =
| death_place      =
| residence        = [[United States]]
| nationality      = [[United States|American]]
| field            = [[Robotics]]&lt;br/&gt;[[Control theory]]
| work_institution = [[Harvard University]]
| alma_mater       = [[Case Western Reserve University]].
| doctoral_advisor = [[Mihajlo D. Mesarovic]]
| doctoral_students = [[Jan Camiel Willems|Jan C. Willems]]&lt;br&gt;[[David P. Dobkin]]
}}
'''Roger Ware Brockett''' (born October 22, 1938 in [[Seville, Ohio]]) is an American [[control theorist]] and the [[An Wang]] Professor of Computer Science and Electrical Engineering at [[Harvard University]], who founded the [http://hrl.harvard.edu/ Harvard Robotics Laboratory] in 1983.&lt;ref&gt;{{cite book | last = Baillieul | first = J. | title = Mathematical Control Theory | publisher = Springer | location = Berlin | year = 1999 | isbn = 0-387-98317-1 }}&lt;/ref&gt;&lt;ref&gt;{{Cite journal|url=http://news.nationalgeographic.com/news/2003/05/0519_030519_robots.html|title=Robots May Be Built as Companions, Expert Says|journal=[[National Geographic (magazine)|National Geographic]] News|date=May 19, 2003|last=Roach|first=John|postscript=&lt;!--None--&gt;}}.&lt;/ref&gt;

==Biography==
Brockett received his [[Bachelor of Science|B.S.]] in 1960, his [[Master of Science|M.S.]] in 1962, and his [[Ph.D.]] in 1964 (under the supervision of [[Mihajlo D. Mesarovic]]), all from [[Case Western Reserve University]].

After teaching at the [[Massachusetts Institute of Technology]] from 1963 to 1969, he joined the faculty at [[Harvard University]] where he became the Gordon McKay Professor of [[Applied Mathematics]] and in 1989 the [[An Wang]] Professor of Computer Science and Electrical Engineering.

==Awards and honors==
Brockett received several awards and honors, including:
* [[Fellow]] of the [[Institute of Electrical and Electronics Engineers]] (IEEE) since 1974&lt;ref&gt;{{cite web |url=http://www.ieee.org/membership_services/membership/fellows/chronology/fellows_1974.html |title=Fellow Class of 1974 |publisher=[[IEEE]] |accessdate={{Format date|2011|1|11}} |deadurl=yes |archiveurl=https://web.archive.org/web/20110629165241/http://www.ieee.org/membership_services/membership/fellows/chronology/fellows_1974.html |archivedate=2011-06-29 |df= }}&lt;/ref&gt;
* Elected to the [[National Academy of Engineering]] in 1991&lt;ref name="NAE"&gt;{{cite web|url=http://www.nae.edu/MembersSection/Directory20412/27944.aspx |title=NAE Members Directory - Dr. Roger W. Brockett |publisher=[[National Academy of Engineering|NAE]] |accessdate={{Format date|2011|1|11}}}}&lt;/ref&gt;
* In 1989 the [[Richard E. Bellman Control Heritage Award]] from the [[American Automatic Control Council]]&lt;ref&gt;{{cite web|url=http://a2c2.org/awards/richard-e-bellman-control-heritage-award |title=Richard E. Bellman Control Heritage Award |publisher=[[American Automatic Control Council]] |accessdate={{Format date|2013|2|10}}}}&lt;/ref&gt;
* In 1991 the [[IEEE Control Systems Science and Engineering Award]]&lt;ref name="book_Mathematical_control_theory"&gt;{{cite book|url=https://books.google.com/books?id=ovpcKtOKEL4C&amp;pg=PR19 |title=Mathematical control theory |editor1-first=John B. |editor1-last=Baillieul |editor2-first=Jan C. |editor2-last=Willems |year=1999 |page=xix |publisher=[[Springer-Verlag]] |isbn=0-387-98317-1 |accessdate={{Format date|2011|3|29}}}}&lt;/ref&gt;&lt;ref name="IEEE-ControlSystems-Award-Recipients"&gt;{{cite web|url=http://www.ieee.org/documents/control_sys_rl.pdf |title=IEEE Control Systems Award Recipients |publisher=[[IEEE]] |accessdate={{Format date|2011|1|13}}}}&lt;/ref&gt;&lt;ref name="IEEE-CSS-ControlSystems-Award"&gt;{{cite web |url=http://www.ieeecss.org/main/awards/control-systems-field-award |title=IEEE Control Systems Award |publisher=[[IEEE Control Systems Society]] |accessdate={{Format date|2011|1|13}} |deadurl=yes |archiveurl=https://web.archive.org/web/20101229173014/http://www.ieeecss.org/main/awards/control-systems-field-award |archivedate=2010-12-29 |df= }}&lt;/ref&gt;
* In 1996 the "W.T. and Idalia Reid Prize in Mathematics" from the [[Society for Industrial and Applied Mathematics]]&lt;ref&gt;{{cite web|url=http://www.siam.org/prizes/sponsored/reid.php |title=W.T. and Idalia Reid Prize in Mathematics |publisher=[[Society for Industrial and Applied Mathematics]] |accessdate={{Format date|2011|1|14}}}}&lt;/ref&gt;
* In 2005 the [[Rufus Oldenburger Medal]] from the [[American Society of Mechanical Engineers]]&lt;ref&gt;{{cite web|url=http://www.asme.org/about-asme/honors-awards/achievement-awards/rufus-oldenburger-medal |title=Rufus Oldenburger Medal |publisher=[[American Society of Mechanical Engineers]] |accessdate={{Format date|2013|2|12}}}}&lt;/ref&gt;
* In 2009 the [[IEEE Leon K. Kirchmayer Graduate Teaching Award]]&lt;ref name="IEEE-Kirchmayer-Award-Recipients"&gt;{{cite web|url=http://www.ieee.org/documents/graduate_rl.pdf |title=IEEE Leon K. Kirchmayer Graduate Teaching Award Recipients |publisher=[[IEEE]] |accessdate={{Format date|2011|1|10}}}}&lt;/ref&gt;
* In 2012 he became a fellow of the [[American Mathematical Society]].&lt;ref&gt;{{cite web|url=http://www.ams.org/profession/fellows-list |title=List of Fellows of the American Mathematical Society |publisher=[[American Mathematical Society]] |accessdate={{Format date|2012|11|10}}}}&lt;/ref&gt;

==Trivia==
* Roger Brockett and [[Donald Knuth]] were classmates at Case Western

==References==
{{reflist|2}}

==External links==
*[http://people.seas.harvard.edu/~brockett/brockett.html Brockett's web page] at Harvard
*[http://www.isr.umd.edu/People/faculty/Brockett.html Biography] at the [[University of Maryland, College Park|University of Maryland]].
{{Richard E. Bellman Control Heritage Award 1979-2000 Laureates}}

{{Authority control}}

{{DEFAULTSORT:Brockett, Roger W.}}
[[Category:American computer scientists]]
[[Category:Case Western Reserve University alumni]]
[[Category:Control theorists]]
[[Category:Fellow Members of the IEEE]]
[[Category:John A. Paulson School of Engineering and Applied Sciences faculty]]
[[Category:Massachusetts Institute of Technology faculty]]
[[Category:American roboticists]]
[[Category:Richard E. Bellman Control Heritage Award recipients]]
[[Category:1938 births]]
[[Category:Living people]]
[[Category:Fellows of the Society for Industrial and Applied Mathematics]]
[[Category:Fellows of the American Mathematical Society]]
[[Category:Members of the United States National Academy of Engineering]]
[[Category:Guggenheim Fellows]]
[[Category:People from Seville, Ohio]]
[[Category:Engineers from Ohio]]</text>
      <sha1>sccbvg8nd4r4jw18brqueokksxrhzdj</sha1>
    </revision>
  </page>
  <page>
    <title>Semi-simple operator</title>
    <ns>0</ns>
    <id>7252030</id>
    <revision>
      <id>644372405</id>
      <parentid>635799233</parentid>
      <timestamp>2015-01-27T07:50:11Z</timestamp>
      <contributor>
        <username>David Eppstein</username>
        <id>2051880</id>
      </contributor>
      <comment>/* References */ more bibdata</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="1698">In [[mathematics]], a [[linear operator]] ''T'' on a finite-dimensional [[vector space]] is '''semi-simple''' if every ''T''-[[invariant subspace]] has a [[Direct sum of modules#Internal direct sum| complementary]] ''T''-invariant subspace.&lt;ref name="Lam-2001-p39"&gt;Lam (2001), [{{Google books|plainurl=y|id=f15FyZuZ3-4C|page=39|text=linear operator}} p. 39]&lt;/ref&gt;

An important result regarding semi-simple operators is that, a linear operator on a finite dimensional vector space over an [[algebraically closed]] field is semi-simple if and only if it is [[diagonalizable]].&lt;ref name="Lam-2001-p39"/&gt; This is because such an operator always has an eigenvector; if it is, in addition, semi-simple, then it has a complementary invariant [[hyperplane]], which itself has an eigenvector, and thus by induction is diagonalizable. Conversely, diagonalizable operators are easily seen to be semi-simple, as invariant subspaces are direct sums of eigenspaces, and any basis for this space can be extended to an eigenbasis.

==Notes==
&lt;references/&gt;

==References==
*{{cite book
 | last1 = Hoffman | first1 = Kenneth
 | last2 = Kunze | first2 = Ray | author2-link = Ray Kunze
 | chapter = Semi-Simple operators
 | edition = 2nd
 | location = Englewood Cliffs, N.J.
 | mr = 0276251
 | publisher = Prentice-Hall, Inc.
 | title = Linear algebra
 | year = 1971}}
* {{cite book |last1=Lam |first1=Tsit-Yuen |authorlink1= |last2= |first2= |authorlink2= |title=A first course in noncommutative rings |url= |edition=2 |series=Graduate texts in mathematics |volume=131 |year=2001 |publisher=Springer |location= |isbn=0-387-95183-0 |id= }}

[[Category:Linear algebra]]
[[Category:Invariant subspaces]]


{{math-stub}}</text>
      <sha1>892tq48bjp9n989woaoggaxqy9wgsp3</sha1>
    </revision>
  </page>
  <page>
    <title>Series expansion</title>
    <ns>0</ns>
    <id>1575813</id>
    <revision>
      <id>827277688</id>
      <parentid>723410325</parentid>
      <timestamp>2018-02-23T20:05:01Z</timestamp>
      <contributor>
        <username>Dpleibovitz</username>
        <id>3747202</id>
      </contributor>
      <minor/>
      <comment>series approximate non-analytic functions</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2195">In [[mathematics]], a '''series expansion''' is a method for calculating a [[Function (mathematics)|function]] that cannot be expressed by just elementary operators (addition, subtraction, multiplication and division).

The resulting so-called ''[[Series (mathematics)|series]]'' often can be limited to a finite number of terms, thus yielding an [[approximation]] of the function. The  fewer terms of the sequence are used, the simpler this approximation will be. Often, the resulting inaccuracy (i.e., the [[partial sum]] of the omitted terms) can be described by an equation involving [[Big O notation]] (see also [[asymptotic expansion]]). The series expansion on an [[open interval]] will also be an approximation for non-[[analytic functions]].

There are several kinds of series expansions, such as:

* [[Taylor series]]: A [[power series]] based on a function’s [[derivative]]s at a single point.
* [[Maclaurin series]]: A special case of a Taylor series, centred at zero.
* [[Laurent series]]: An extension of the Taylor series, allowing negative exponent values.
* [[Dirichlet series]]: Used in [[number theory]].
* [[Fourier series]]: Describes periodical functions as a series of [[Trigonometric functions#Sine, cosine, and tangent|sine]] and [[Trigonometric functions#Sine, cosine, and tangent|cosine]] functions. In [[acoustics]], e.g., the [[Fundamental frequency|fundamental tone]] and the [[overtone]]s together form an example of a Fourier series.
* [[Table of Newtonian series|Newtonian series]]
* [[Legendre polynomials]]: Used in [[physics]] to describe an arbitrary electrical field as a [[superposition principle|superposition]] of a [[dipole]] field, a [[quadrupole]] field, an [[Multipole expansion|octupole]] field, etc.
* [[Zernike polynomials]]: Used in [[optics]] to calculate [[Optical aberration|aberration]]s of optical systems. Each term in the series describes a particular type of aberration.
* [[Stirling series]]: Used as an approximation for [[factorial]]s.

For more details, refer to the articles mentioned.

[[Category:Algebra]]
[[Category:Polynomials]]
[[Category:Mathematical analysis]]
[[Category:Mathematical series]]
[[Category:Series expansions]]</text>
      <sha1>7vwxa05cz41ld09efm8vkjibdsgu72t</sha1>
    </revision>
  </page>
  <page>
    <title>Sharp-P-completeness of 01-permanent</title>
    <ns>0</ns>
    <id>20768719</id>
    <revision>
      <id>837319984</id>
      <parentid>819600816</parentid>
      <timestamp>2018-04-20T02:31:06Z</timestamp>
      <contributor>
        <username>Onel5969</username>
        <id>10951369</id>
      </contributor>
      <minor/>
      <comment>Disambiguating links to [[Permanent]] (link changed to [[Permanent (mathematics)]]; link changed to [[Permanent (mathematics)]]; link changed to [[Permanent (mathematics)]]) using [[User:Qwertyytrewqqwerty/DisamAssist|DisamAssist]].</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="24336">{{MOS|article|Manual of Style (mathematics)|date=September 2010}}
{{Correct title|title=#P-completeness of 01-permanent|reason=hash}}

The '''#P-completeness of 01-permanent''', sometimes known as '''Valiant's theorem''',&lt;ref name="P"&gt;[[Christos Papadimitriou|Christos H. Papadimitriou]]. ''Computational Complexity.''  [[Addison-Wesley]], 1994. {{isbn|0-201-53082-1}}. Page 443&lt;/ref&gt; is a [[mathematical proof]] about the [[Permanent (mathematics)|permanent]] of [[matrix (mathematics)|matrices]], considered a seminal result in [[computational complexity theory]].&lt;ref&gt;[[Allen Kent]], James G. Williams, Rosalind Kent and Carolyn M. Hall (editors). [https://books.google.com/books?id=uDegDR4ikTQC&amp;pg=PA34&amp;dq=%22permanent+of+a+matrix%22+valiant&amp;lr=&amp;as_brr=3&amp;ei=M8NKSa7LH4G4M8mVyNoI ''Encyclopedia of microcomputers''.][[Marcel Dekker]], 1999. {{isbn|978-0-8247-2722-2}}; p. 34&lt;/ref&gt;&lt;ref&gt;Jin-Yi Cai, A. Pavan and D. Sivakumar, [https://books.google.com/books?id=fIAMFv4doooC&amp;pg=PA90&amp;dq=%22permanent+of+a+matrix%22+valiant&amp;as_brr=3&amp;ei=h8BKScClJYOUMtTP6LEO ''On the Hardness of Permanent.''] In: STACS, '99: 16th Annual Symposium on Theoretical Aspects of Computer Science, Trier, Germany, March 4–6, 1999 Proceedings. pp. 90–99. [[Springer-Verlag]], New York, LLC
Pub. Date: October 2007. {{isbn|978-3-540-65691-3}}; p. 90.&lt;/ref&gt; In a 1979 [[scholarly paper]], [[Leslie Valiant]] proved&lt;ref&gt;{{cite journal
  | author = Leslie G. Valiant
  | title = The Complexity of Computing the Permanent
  | journal = Theoretical Computer Science
  | volume = 8
  | issue = 2
  | pages = 189–201
  | publisher = Elsevier
  | location =
  | date = 1979
  | doi = 10.1016/0304-3975(79)90044-6
  | ref = harv}}&lt;/ref&gt; that the [[computational problem]] of computing the permanent of a matrix is [[Sharp-P-complete|#P-hard]], even if the matrix is restricted to have entries that are all 0 or 1. In this restricted case, computing the [[Permanent (mathematics)|permanent]] is even [[Sharp-P-complete|#P-complete]], because it corresponds to the [[Sharp-P|#P problem]] of counting the number of permutation matrices one can get by changing ones into zeroes.

Valiant's 1979 paper also introduced   [[Sharp-P|#P]] as a [[complexity class]].&lt;ref&gt;[[Lance Fortnow]]. [https://books.google.com/books?id=JWOA9M9CcX8C&amp;pg=PA265&amp;dq=permanent+Valiant ''My Favorite Ten Complexity Theorems of the Past Decade.''] Foundations of Software Technology and Theoretical Computer Science: Proceedings of the 14th Conference, Madras, India, December 15–17, 1994.  P. S. Thiagarajan (editor), pp. 256–275, [[Springer-Verlag]], New York, 2007. {{isbn|978-3-540-58715-6}}; p. 265&lt;/ref&gt;

==Significance==
One reason for interest in the computational complexity of the permanent is that it provides an example of a problem where constructing a single solution can be done efficiently but where counting all solutions is hard.&lt;ref&gt;{{cite book | last=Bürgisser | first=Peter | title=Completeness and reduction in algebraic complexity theory | zbl=0948.68082 | series=Algorithms and Computation in Mathematics | volume=7 | location=Berlin | publisher=[[Springer-Verlag]] | year=2000 | isbn=3-540-66752-0 | page=2 | url=https://books.google.com/books?id=XBlnjSW1VekC&amp;pg=PA2&amp;dq=permanent+Valiant#PPA2 }}&lt;/ref&gt;  As Papadimitriou writes in his book ''Computational Complexity'':
{{cquote|The most impressive and interesting #P-complete problems ''are those for which the corresponding search problem can be solved in polynomial time.'' The PERMANENT problem for 0–1 matrices, which is equivalent to the problem of counting perfect matchings in a bipartite graph [...] is the classic example here.&lt;ref name="P"/&gt;}}
Specifically, [[Computation of the permanent of a matrix|computing the permanent]] (shown to be difficult by Valiant's results) is closely connected with finding a [[perfect matching]] in a [[bipartite graph]], which is solvable in polynomial time by the [[Hopcroft–Karp algorithm]].&lt;ref&gt;[[John Hopcroft|John E. Hopcroft]], [[Richard Karp|Richard M. Karp]]: ''An &lt;math&gt;n^{5/2}&lt;/math&gt; Algorithm for Maximum Matchings in Bipartite Graphs.'' SIAM J. Comput. 2(4), 225–231 (1973)&lt;/ref&gt;&lt;ref name=clrs&gt;{{Introduction to Algorithms|2|chapter=26.5: The relabel-to-front algorithm|pages=696–697}}&lt;/ref&gt; For a bipartite graph with ''2n'' vertices partitioned into two parts with ''n'' vertices each, the number of perfect matchings equals the permanent of its [[biadjacency matrix]] and the square of the number of perfect matchings is equal to the permanent of its [[adjacency matrix]].&lt;ref name="Kozen"/&gt; Since any 0–1 matrix is the biadjacency matrix of some bipartite graph, Valiant's theorem implies&lt;ref name="Kozen"&gt;[[Dexter Kozen]]. [https://books.google.com/books?id=L_AMnf9UF9QC&amp;pg=PA141&amp;dq=%22permanent+of+a+matrix%22+valiant&amp;as_brr=3&amp;ei=h8BKScClJYOUMtTP6LEO#PPA142,M1 ''The Design and Analysis of Algorithms.''] [[Springer-Verlag]], New York, 1991.
{{isbn|978-0-387-97687-7}};  pp. 141–142&lt;/ref&gt; that the problem of counting the number of perfect matchings in a bipartite graph is [[Sharp-P-complete|#P-complete]], and in conjunction with [[Toda's theorem]] this implies that it is hard for the entire [[polynomial hierarchy]].&lt;ref&gt;[[Seinosuke Toda]]. [http://siamdl.aip.org/getabs/servlet/GetabsServlet?prog=normal&amp;id=SMJCAT000020000005000865000001&amp;idtype=cvips&amp;gifs=Yes PP is as Hard as the Polynomial-Time Hierarchy.] [[SIAM Journal on Computing]], Volume 20 (1991), Issue 5, pp. 865&amp;ndash;877.&lt;/ref&gt;&lt;ref&gt;[http://www.sigact.org/Prizes/Godel/1998.html 1998 Gödel Prize. Seinosuke Toda]&lt;/ref&gt;

The computational complexity of the permanent also has some significance in other aspects of complexity theory: it is not known whether [[NC (complexity)|NC]] equals P (informally, whether every polynomially-solvable problem can be solved by a polylogarithmic-time [[parallel algorithm]]) and [[Ketan Mulmuley]] has suggested an approach to resolving this question that relies on writing the permanent as the determinant of a matrix.&lt;ref&gt;[[Ketan Mulmuley]]. [https://dx.doi.org/10.1137/S0097539794282930 Lower Bounds in a Parallel Model without Bit Operations.] [[SIAM Journal on Computing]], Volume 28 (1999), Issue 4, pp. 1460&amp;ndash;1509.&lt;/ref&gt;

Hartmann &lt;ref&gt;W. Hartmann. [http://www.informaworld.com/smpp/content~content=a778402740~db=all On the complexity of immanants.] Linear and Multilinear Algebra 18 (1985), no. 2, pp. 127–140.&lt;/ref&gt; proved a generalization of Valiant's theorem concerning the complexity of computing [[immanant of a matrix|immanants of matrices]] that generalize both the determinant and the permanent.

==Ben-Dor and Halevi's proof==
Below, the proof that computing the permanent of a 01-matrix is [[Sharp-P-complete|#P-complete]] is described. It mainly follows the proof by {{harvtxt|Ben-Dor |Halevi|1993}}.&lt;ref&gt;{{Cite book |last1=Ben-Dor |first1=Amir |last2= Halevi |first2=Shai |year=1993 |url= http://people.csail.mit.edu/shaih/pubs/01perm.pdf |contribution= Zero-one permanent is ''#P''-complete, a simpler proof |title= Proceedings of the 2nd Israel Symposium on the Theory and Computing Systems |pages= 108–117 |ref=harv}}.&lt;/ref&gt;

===Overview===
Any square matrix &lt;math&gt;A = (a_{ij})&lt;/math&gt; can be viewed as the [[adjacency matrix]] of a directed graph, with &lt;math&gt;a_{ij}&lt;/math&gt; representing the weight of the edge from vertex ''i'' to vertex ''j''. Then, the permanent of A is equal to the sum of the weights of all cycle-covers of the graph; this is a [[Permanent (mathematics)|graph-theoretic interpretation of the permanent]].

[[Sharp-SAT|#SAT]], a [[function problem]] related to the [[Boolean satisfiability problem]], is the problem of counting the number of satisfying assignments of a given Boolean formula. It is a [[Sharp-P-complete|#P-complete]] problem (by definition), as any NP machine can be encoded into a Boolean formula by a process similar to that in [[Cook's theorem]], such that the number of satisfying assignments of the Boolean formula is equal to the number of accepting paths of the NP machine. Any formula in SAT [[3SAT|can be rewritten]] as a formula in 3-[[conjunctive normal form|CNF]] form preserving the number of satisfying assignments, and so #SAT and #3SAT are equivalent and #3SAT is [[Sharp-P-complete|#P-complete]] as well.

In order to prove that 01-Permanent is [[Sharp-P-complete|#P-hard]], it is therefore sufficient to show that the number of satisfying assignments for a 3-CNF formula can be expressed succinctly as a function of the permanent of a matrix that contains only the values 0 and 1. This is usually accomplished in two steps:
# Given a 3-CNF formula φ, construct a directed integer-weighted graph &lt;math&gt;G_\phi&lt;/math&gt;, such that the sum of the weights of cycle covers of &lt;math&gt;G_\phi&lt;/math&gt; (or equivalently, the permanent of its adjacency matrix) is equal to the number of satisfying assignments of φ. This establishes that Permanent is #P-hard.
# Through a series of reductions, reduce Permanent to 01-Permanent, the problem of computing the permanent of a matrix all entries 0 or 1. This establishes that 01-permanent is #P-hard as well.

===Constructing the integer graph===

Given a 3CNF-formula &lt;math&gt;\phi&lt;/math&gt; with ''m'' clauses and ''n'' variables, one can construct a weighted, directed graph &lt;math&gt;G_\phi&lt;/math&gt; such that
# each satisfying assignment for &lt;math&gt;\phi&lt;/math&gt; will have a corresponding set of cycle covers in &lt;math&gt;G_\phi&lt;/math&gt; where the sum of the weights of cycle covers in this set will be &lt;math&gt;12^m&lt;/math&gt; ; and
# all other cycle covers in &lt;math&gt;G_\phi&lt;/math&gt; will have weights summing to 0.
Thus if &lt;math&gt;(\#\phi)&lt;/math&gt; is the number of satisfying assignments for &lt;math&gt;\phi&lt;/math&gt;, the permanent of this graph will be &lt;math&gt;12^m \cdot (\#\phi)&lt;/math&gt;.
(Valiant's original proof constructs a graph with entries in &lt;math&gt;\{-1,0,1,2,3\}&lt;/math&gt; whose permanent is &lt;math&gt;4^{t(\phi)}\cdot(\#\phi)&lt;/math&gt; where &lt;math&gt;t(\phi)&lt;/math&gt; is "twice the number of occurrences of literals in &lt;math&gt;\phi&lt;/math&gt;" – &lt;math&gt;m&lt;/math&gt;.)

The graph construction makes use of a component that is treated as a "black box."  To keep the explanation simple, the properties of this component are given without actually defining the structure of the component.

To specify ''G''&lt;sub&gt;''φ''&lt;/sub&gt;, one first constructs a variable node in ''G''&lt;sub&gt;''φ''&lt;/sub&gt; for each of the ''n'' variables in ''φ''. Additionally, for each of the ''m'' clauses in ''φ'', one constructs a clause component ''C''&lt;sub&gt;''j''&lt;/sub&gt; in ''G''&lt;sub&gt;''φ''&lt;/sub&gt; that functions as a sort of "black box." All that needs to be noted about ''C''&lt;sub&gt;''j''&lt;/sub&gt; is that it has three input edges and three output edges. The input edges come either from variable nodes or from previous clause components (e.g., ''C''&lt;sub&gt;''o''&lt;/sub&gt; for some ''o''&amp;nbsp;&lt;&amp;nbsp;''j'') and the output edges go either to variable nodes or to later clause components (e.g., ''C''&lt;sub&gt;''o''&lt;/sub&gt; for some &lt;math&gt;o&gt;j&lt;/math&gt;). The first input and output edges correspond with the first variable of the clause ''j'', and so on. Thus far, all of the nodes that will appear in the graph ''G''&lt;sub&gt;''φ''&lt;/sub&gt; have been specified.

Next, one would consider the edges. For each variable &lt;math&gt;x_i&lt;/math&gt; of &lt;math&gt;\phi&lt;/math&gt;, one makes a true cycle (T-cycle) and a false cycle (F-cycle) in &lt;math&gt;G_\phi&lt;/math&gt;. To create the T-cycle, one starts at the variable node for &lt;math&gt;x_i&lt;/math&gt; and draw an edge to the clause component &lt;math&gt;C_j&lt;/math&gt; that corresponds to the first clause in which &lt;math&gt;x_i&lt;/math&gt; appears. If &lt;math&gt;x_i&lt;/math&gt; is the first variable in the clause of &lt;math&gt;\phi&lt;/math&gt; corresponding to &lt;math&gt;C_j&lt;/math&gt;, this edge will be the first input edge of &lt;math&gt;C_j&lt;/math&gt;, and so on. Thence, draw an edge to the next clause component corresponding to the next clause of &lt;math&gt;\phi&lt;/math&gt; in which &lt;math&gt;x_i&lt;/math&gt; appears, connecting it from the appropriate output edge of &lt;math&gt;C_j&lt;/math&gt; to the appropriate input edge of the next clause component, and so on. After the last clause in which &lt;math&gt;x_i&lt;/math&gt; appears, we connect the appropriate output edge of the corresponding clause component back to &lt;math&gt;x_i&lt;/math&gt;'s variable node. Of course, this completes the cycle. To create the F-cycle, one would follow the same procedure, but connect &lt;math&gt;x_i&lt;/math&gt;'s variable node to those clause components in which ~&lt;math&gt;x_i&lt;/math&gt; appears, and finally back to &lt;math&gt;x_i&lt;/math&gt;'s variable node. All of these edges outside the clause components are termed ''external edges'', all of which have weight 1. Inside the clause components, the edges are termed ''internal edges''. Every external edge is part of a T-cycle or an F-cycle (but not both—that would force inconsistency).

Note that the graph &lt;math&gt;G_\phi&lt;/math&gt; is of size linear in &lt;math&gt;|\phi|&lt;/math&gt;, so the construction can be done in polytime (assuming that the clause components do not cause trouble).

====Notable properties of the graph{{Anchor|A very nice property of the graph}}====

A useful property of &lt;math&gt;G_\phi&lt;/math&gt; is that its cycle covers correspond to variable assignments for &lt;math&gt;\phi&lt;/math&gt;. For a cycle cover '''Z''' of &lt;math&gt;G_\phi&lt;/math&gt;, one can say that '''Z''' induces an assignment of values for the variables in &lt;math&gt;\phi&lt;/math&gt; just in case '''Z''' contains all of the external edges in &lt;math&gt;x_i&lt;/math&gt;'s T-cycle and none of the external edges in &lt;math&gt;x_i&lt;/math&gt;'s F-cycle for all variables &lt;math&gt;x_i&lt;/math&gt; that the assignment makes true, and vice versa for all variables &lt;math&gt;x_i&lt;/math&gt; that the assignment makes false. Although any given cycle cover '''Z''' need not induce an assignment for &lt;math&gt;\phi&lt;/math&gt;, any one that does induces exactly one assignment, and the same assignment induced depends only on the external edges of '''Z'''. The term '''Z''' is considered an incomplete cycle cover at this stage, because one talks only about its external edges, M. In the section below, one considers M-completions to show that one has a set of cycle covers corresponding to each M that have the necessary properties.

The sort of '''Z''''s that don't induce assignments are the ones with cycles that "jump" inside the clause components. That is, if for every &lt;math&gt;C_j&lt;/math&gt;, at least one of &lt;math&gt;C_j&lt;/math&gt;'s input edges is in '''Z''', and every output edge of the clause components is in '''Z''' when the corresponding input edge is in '''Z''', then '''Z''' is proper with respect to each clause component, and '''Z''' will produce a satisfying assignment for &lt;math&gt;\phi&lt;/math&gt;. This is because proper '''Z''''s contain either the complete T-cycle or the complete F-cycle of every variable &lt;math&gt;x_i&lt;/math&gt; in &lt;math&gt;\phi&lt;/math&gt; as well as each including edges going into and coming out of each clause component. Thus, these '''Z''''s assign either true or false (but never both) to each &lt;math&gt;x_i&lt;/math&gt; and ensure that each clause is satisfied. Further, the sets of cycle covers corresponding to all such '''Z''''s have weight &lt;math&gt;12^m&lt;/math&gt;, and any other '''Z''''s have weight &lt;math&gt;0&lt;/math&gt;. The reasons for this depend on the construction of the clause components, and are outlined below.

====The clause component====

To understand the relevant properties of the clause components &lt;math&gt;C_j&lt;/math&gt;, one needs the notion of an M-completion. A cycle cover '''Z''' induces a satisfying assignment just in case its external edges satisfy certain properties. For any cycle cover of &lt;math&gt;G_\phi&lt;/math&gt;, consider only its external edges, the subset M. Let M be a set of external edges. A set of internal edges L is an M-completion just in case &lt;math&gt;M\cup L&lt;/math&gt; is a cycle cover of &lt;math&gt;G_\phi&lt;/math&gt;. Further, denote the set of all M-completions by &lt;math&gt;L^M&lt;/math&gt; and the set of all resulting cycle covers of &lt;math&gt;G_\phi&lt;/math&gt; by &lt;math&gt;Z^M&lt;/math&gt;.

Recall that construction of &lt;math&gt;G_\phi&lt;/math&gt; was such that each external edge had weight 1, so the weight of &lt;math&gt;Z^M&lt;/math&gt;, the cycle covers resulting from any M, depends only on the internal edges involved. We add here the premise that the construction of the clause components is such that the sum over possible M-completions of the weight of the internal edges in each clause component, where M is proper relative to the clause component, is 12. Otherwise the weight of the internal edges is 0.  Since there are ''m'' clause components, and the selection of sets of internal edges, L, within each clause component is independent of the selection of sets of internal edges in other clause components, so one can multiply everything to get the weight of &lt;math&gt;Z^M&lt;/math&gt;. So, the weight of each &lt;math&gt;Z^M&lt;/math&gt;, where M induces a satisfying assignment, is &lt;math&gt;12^m&lt;/math&gt;. Further, where M does not induce a satisfying assignment, M is not proper with respect to some &lt;math&gt;C_j&lt;/math&gt;, so the product of the weights of internal edges in &lt;math&gt;Z^M&lt;/math&gt; will be &lt;math&gt;0&lt;/math&gt;.

The clause component is a weighted, directed graph with 7 nodes with edges weighted and nodes arranged to yield the properties specified above, and is given in Appendix A of Ben-Dor and Halevi (1993). Note that the internal edges here have weights drawn from the set &lt;math&gt;\{-1,0,1,2,3\}&lt;/math&gt;; not all edges have 0–1 weights.

Finally, since the sum of weights of all the sets of cycle covers inducing any particular satisfying assignment is 12&lt;sup&gt;''m''&lt;/sup&gt;, and the sum of weights of all other sets of cycle covers is 0, one has Perm(''G''&lt;sub&gt;''φ''&lt;/sub&gt;)&amp;nbsp;=&amp;nbsp;12&lt;sup&gt;''m''&lt;/sup&gt;·(''#φ''). The following section reduces computing Perm(&lt;math&gt;G_\phi&lt;/math&gt;) to the permanent of a 01 matrix.

===01-Matrix===

The above section has shown that Permanent is #P-hard. Through a series of reductions, any permanent can be reduced to the permanent of a matrix with entries only 0 or 1. This will prove that 01-Permanent is #P-hard as well.

====Reduction to a non-negative matrix====

Using [[modular arithmetic]], convert an integer matrix ''A'' into an equivalent non-negative matrix &lt;math&gt;A'&lt;/math&gt; so that the permanent of &lt;math&gt;A&lt;/math&gt; can be computed easily from the permanent of &lt;math&gt;A'&lt;/math&gt;, as follows:

Let &lt;math&gt;A&lt;/math&gt; be an &lt;math&gt;n \times n&lt;/math&gt; integer matrix where no entry has a magnitude larger than &lt;math&gt;\mu&lt;/math&gt;.
* Compute &lt;math&gt;Q = 2 n! \cdot \mu^n + 1&lt;/math&gt;. The choice of Q is due to the fact that &lt;math&gt;|\operatorname{Perm}(A)| \le n! \cdot \mu^n&lt;/math&gt;
* Compute &lt;math&gt;A' = A\,\bmod\,Q&lt;/math&gt;
* Compute &lt;math&gt;P = \operatorname{Perm}(A')\,\bmod\,Q&lt;/math&gt;
* If &lt;math&gt;P &lt; Q/2&lt;/math&gt; then Perm(''A'') = ''P''. Otherwise &lt;math&gt;\operatorname{Perm}(A) = P - Q&lt;/math&gt;

The transformation of &lt;math&gt;A&lt;/math&gt; into &lt;math&gt;A'&lt;/math&gt; is polynomial in &lt;math&gt;n&lt;/math&gt; and &lt;math&gt;\log (\mu)&lt;/math&gt;, since the number of bits required to represent &lt;math&gt;Q&lt;/math&gt; is polynomial in &lt;math&gt;n&lt;/math&gt; and &lt;math&gt;\log (\mu)&lt;/math&gt;

An example of the transformation and why it works is given below.
:&lt;math&gt;A = \begin{bmatrix}2 &amp; -2 \\ -2 &amp; 1\end{bmatrix}&lt;/math&gt;
:&lt;math&gt;\operatorname{Perm}(A) = 2 \cdot 1 + (-2) \cdot (-2) = 6&lt;/math&gt;.

Here, &lt;math&gt;n = 2&lt;/math&gt;, &lt;math&gt;\mu = 2&lt;/math&gt;, and &lt;math&gt;\mu^n = 4&lt;/math&gt;, so &lt;math&gt;Q = 17&lt;/math&gt;. Thus
:&lt;math&gt;A' = A\,\bmod\,17 = \begin{bmatrix}2 &amp; 15 \\ 15 &amp; 1\end{bmatrix}.&lt;/math&gt;

Note how the elements are non-negative because of the modular arithmetic. It is simple to compute the permanent
:&lt;math&gt;\operatorname{Perm}(A') = 2 \cdot 1 + 15 \cdot 15 = 227&lt;/math&gt;

so &lt;math&gt;P = 227\,\bmod\,17 = 6&lt;/math&gt;. Then &lt;math&gt;P &lt; Q/2&lt;/math&gt;, so &lt;math&gt;\operatorname{Perm}(A) = P = 6.&lt;/math&gt;

====Reduction to powers of 2====
[[Image:Permanent-Nonneg2Powers.png|400px|right|Figure 1: Construction of 2Power from NonNeg]]

Note that any number can be [[Binary numeral system|decomposed into a sum of powers of 2]]. For example,
:&lt;math&gt;13 = 2^3 + 2^2 + 2^0&lt;/math&gt;

This fact is used to convert a non-negative matrix into an equivalent matrix whose entries are all powers of 2. The reduction can be expressed in terms of graphs equivalent to the matrices.

Let &lt;math&gt;G&lt;/math&gt; be a &lt;math&gt;n&lt;/math&gt;-node weighted directed graph with non-negative weights, where largest weight is &lt;math&gt;W&lt;/math&gt;. Every edge &lt;math&gt;e&lt;/math&gt; with weight &lt;math&gt;w&lt;/math&gt; is converted into an equivalent edge with weights in powers of 2 as follows:
:&lt;math&gt;w = 2^{x_1} + 2^{x_2} + \cdots + 2^{x_r}&lt;/math&gt;, &lt;math&gt;0 \le x_1 \le x_2 \le \cdots \le x_r \le \log (w)&lt;/math&gt;

This can be seen graphically in the Figure 1. The subgraph that replaces the existing edge contains &lt;math&gt;r&lt;/math&gt; nodes and &lt;math&gt;3r&lt;/math&gt; edges.

To prove that this produces an equivalent graph &lt;math&gt;G'&lt;/math&gt; that has the same permanent as the original, one must show the correspondence between the cycle covers of &lt;math&gt;G&lt;/math&gt; and &lt;math&gt;G'&lt;/math&gt;.

Consider some cycle-cover &lt;math&gt;R&lt;/math&gt; in &lt;math&gt;G&lt;/math&gt;.
* If an edge &lt;math&gt;e&lt;/math&gt; is not in &lt;math&gt;R&lt;/math&gt;, then to cover all the nodes in the new sub graph, one must use the self-loops. Since all self-loops have a weight of 1, the weight of cycle-covers in &lt;math&gt;R&lt;/math&gt; and &lt;math&gt;R'&lt;/math&gt; match.
* If &lt;math&gt;e&lt;/math&gt; is in &lt;math&gt;R&lt;/math&gt;, then in all the corresponding cycle-covers in &lt;math&gt;G'&lt;/math&gt;, there must be a path from &lt;math&gt;u&lt;/math&gt; to &lt;math&gt;v&lt;/math&gt;, where ''u'' and ''v'' are the nodes of edge ''e''. From the construction, one can see that there are &lt;math&gt;r&lt;/math&gt; different paths and sum of all these paths equal to the weight of the edge in the original graph &lt;math&gt;G&lt;/math&gt;. So the weight of corresponding cycle-covers in &lt;math&gt;G&lt;/math&gt; and &lt;math&gt;G'&lt;/math&gt; match.

Note that the size of &lt;math&gt;G'&lt;/math&gt; is polynomial in &lt;math&gt;n&lt;/math&gt; and &lt;math&gt;\log W&lt;/math&gt;.

====Reduction to 0–1====

&lt;!-- Image with unknown copyright status removed: [[Image:Permanent-Proof-01matrix.gif|275px|thumb|right|Figure 2: Construction of a 01-matrix from 2Power]] --&gt;
[[Image:Permanent-2powers01.png|right|Figure 2: Construction of a 01-matrix from 2Power]]

The objective here is to reduce a matrix whose entries are powers of 2 into an equivalent matrix containing only zeros and ones (i.e. a directed graph where each edge has a weight of 1).

Let G be a &lt;math&gt;n&lt;/math&gt;-node directed graph where all the weights on edges are powers of two.  Construct a graph, &lt;math&gt;G'&lt;/math&gt;, where the weight of each edge is 1 and Perm(G) = Perm(G').  The size of this new graph, G', is polynomial in &lt;math&gt;n&lt;/math&gt; and &lt;math&gt;p&lt;/math&gt; where the maximal weight of any edge in graph G is &lt;math&gt;2^p&lt;/math&gt;.

This reduction is done locally at each edge in G that has a weight larger than 1. Let &lt;math&gt;e = (u, v)&lt;/math&gt; be an edge in G with a weight &lt;math&gt;w = 2^r &gt; 1&lt;/math&gt;.  It is replaced by a subgraph &lt;math&gt;J_e&lt;/math&gt; that is made up of &lt;math&gt;2r&lt;/math&gt; nodes and &lt;math&gt;6r&lt;/math&gt; edges as seen in Figure 2. Each edge in &lt;math&gt;J_e&lt;/math&gt; has a weight of 1.  Thus, the resulting graph G' contains only edges with a weight of 1.

Consider some cycle-cover &lt;math&gt;R&lt;/math&gt; in &lt;math&gt;G&lt;/math&gt;.
* If an original edge &lt;math&gt;e&lt;/math&gt; from graph G is not in &lt;math&gt;R&lt;/math&gt;, one cannot create a path through the new subgraph &lt;math&gt;J_e&lt;/math&gt;.  The only way to form a cycle cover over &lt;math&gt;J_e&lt;/math&gt; in such a case is for each node in the subgraph to take its self-loop.  As each edge has a weight of one, the weight of the resulting cycle cover is equal to that of the original cycle cover.
* However, if the edge in G is a part of the cycle cover then in any cycle cover of &lt;math&gt;G'&lt;/math&gt; there must be a path from &lt;math&gt;u&lt;/math&gt; to &lt;math&gt;v&lt;/math&gt; in the subgraph. At each step down the subgraph there are two choices one can make to form such a path.  One must make this choice &lt;math&gt;r&lt;/math&gt; times, resulting in &lt;math&gt;2^r&lt;/math&gt; possible paths from &lt;math&gt;u&lt;/math&gt; to &lt;math&gt;v&lt;/math&gt;.  Thus, there are &lt;math&gt;2^r&lt;/math&gt; possible cycle covers and since each path has a weight of 1, the sum of the weights of all these cycle covers equals the weight of the original cycle cover.

==Aaronson's proof==
Quantum computer scientist Scott Aaronson&lt;ref&gt;[[Scott Aaronson|S. Aaronson]], [http://eccc.hpi-web.de/report/2011/043/download A Linear-Optical Proof that the Permanent is #P-Hard]&lt;/ref&gt; has proved #P-hardness of permanent using quantum methods.

==References==
{{Reflist}}

{{DEFAULTSORT:Permanent Is Sharp-P-Complete}}
[[Category:Computational problems]]
[[Category:Combinatorics]]
[[Category:Article proofs]]</text>
      <sha1>t2qzboch9wzqwqqgowvn8mfsskc0pca</sha1>
    </revision>
  </page>
  <page>
    <title>Simplicial polytope</title>
    <ns>0</ns>
    <id>8853373</id>
    <revision>
      <id>745802362</id>
      <parentid>627015281</parentid>
      <timestamp>2016-10-23T11:25:28Z</timestamp>
      <contributor>
        <username>Bender the Bot</username>
        <id>28903366</id>
      </contributor>
      <minor/>
      <comment>/* References */http&amp;rarr;https for [[Google Books]] and [[Google News]] using [[Project:AWB|AWB]]</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="2167">In [[geometry]], a '''simplicial polytope''' is a [[polytope]] whose [[facet_(mathematics)|facets]] are all [[Simplex|simplices]].

For example, a ''simplicial polyhedron'' in 3 dimensions contains only triangular faces&lt;ref&gt;Polyhedra, Peter R. Cromwell, 1997. (p.341)&lt;/ref&gt; and corresponds via [[Steinitz's theorem]] to a [[maximal planar graph]].

They are topologically [[Dual polytope|dual]] to [[simple polytope]]s. Polytopes which are both
simple and simplicial are either [[simplices]] or two-dimensional [[polygons]].

== Examples ==
Simplicial [[polyhedra]] include:
* [[Bipyramid]]s
* [[Gyroelongated dipyramid]]s
*[[Deltahedron|Deltahedra]] (equilateral triangles)
** [[Platonic solid|Platonic]] 
*** [[tetrahedron]], [[octahedron]], [[icosahedron]]
** [[Johnson solid]]s:
***[[triangular bipyramid]], [[pentagonal bipyramid]], [[snub disphenoid]], [[triaugmented triangular prism]], [[gyroelongated square dipyramid]]
* [[Catalan solid]]s:
** [[triakis tetrahedron]], [[triakis octahedron]], [[tetrakis hexahedron]], [[disdyakis dodecahedron]], [[triakis icosahedron]], [[pentakis dodecahedron]], [[disdyakis triacontahedron]]

Simplicial tilings:
* Regular:
** [[triangular tiling]]
*[[Laves tiling]]s:
** [[tetrakis square tiling]],  [[triakis triangular tiling]], [[bisected hexagonal tiling]]

Simplicial [[4-polytope]]s include:
*[[convex regular 4-polytope]]
** [[Pentachoron|4-simplex]], [[16-cell]], [[600-cell]]
* Dual [[convex uniform honeycomb]]s:
**[[Disphenoid tetrahedral honeycomb]]
**Dual of [[cantitruncated cubic honeycomb]]
**Dual of [[omnitruncated cubic honeycomb]]
**Dual of [[cantitruncated alternated cubic honeycomb]]

Simplicial higher polytope families:
*[[simplex]]
*[[cross-polytope]] (Orthoplex)

== See also ==
* [[Simplicial complex]]
* [[Delaunay triangulation]]

== Notes ==
{{reflist}}

== References ==
*{{cite book
  | last = Cromwell
  | first = Peter R.
  | title = Polyhedra
  | publisher = Cambridge University Press
  | date = 1997
  | isbn = 0-521-66405-5
  | url=https://books.google.com/books?id=OJowej1QWpoC&amp;lpg=PP1&amp;dq=Polyhedra&amp;pg=PP1#v=onepage&amp;q=&amp;f=false}}


{{Geometry-stub}}
[[Category:Euclidean geometry]]</text>
      <sha1>f4w70pm6gdnv8mh5p3nuaq0kgqu8vwy</sha1>
    </revision>
  </page>
  <page>
    <title>Super-recursive algorithm</title>
    <ns>0</ns>
    <id>15641067</id>
    <revision>
      <id>856210386</id>
      <parentid>847195298</parentid>
      <timestamp>2018-08-23T16:51:06Z</timestamp>
      <contributor>
        <username>JCW-CleanerBot</username>
        <id>31737083</id>
      </contributor>
      <minor/>
      <comment>[[User:JCW-CleanerBot#Logic|task]], replaced: J. Symb. Logic → J. Symb. Log.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="16146">In [[computability theory]], '''super-recursive algorithms''' are a generalization of ordinary [[algorithm]]s that are more powerful, that is, compute more than [[Turing machines]]. The term was introduced by Mark Burgin, whose book "Super-recursive algorithms" develops their theory and presents several mathematical models. Turing machines and other mathematical models of conventional algorithms allow researchers to find properties of recursive algorithms and their computations. In a similar way, mathematical models of super-recursive algorithms, such as [[#Inductive Turing machines|inductive Turing machines]], allow researchers to find properties of super-recursive algorithms and their computations.

Burgin, as well as other researchers (including [[Selim Akl]], Eugene Eberbach, Peter Kugel, [[Jan van Leeuwen]], [[Hava Siegelmann]], Peter Wegner, and Jiří Wiedermann) who studied different kinds of super-recursive algorithms and contributed to the theory of super-recursive algorithms, have argued that super-recursive algorithms can be used to disprove the [[Church-Turing thesis]], but this point of view has been criticized within the mathematical community and is not widely accepted.

== Definition ==

Burgin (2005: 13) uses the term '''recursive algorithms''' for [[algorithm]]s that can be implemented on Turing machines, and uses the word ''algorithm'' in a more general sense. Then a '''super-recursive class of algorithms''' is "a class of algorithms in which it is possible to compute functions not computable by any [[Turing machine]]" (Burgin 2005: 107).

Super-recursive algorithms are closely related to [[hypercomputation]] 
in a way similar to the relationship between ordinary computation and ordinary algorithms. Computation is a process, while an algorithm is a finite constructive description of such a process. Thus a super-recursive algorithm defines a "computational process (including processes of input and output) that cannot be realized by recursive algorithms." (Burgin 2005: 108). A more restricted definition demands that  [[hypercomputation]] solves a [[supertask]] (see Copeland 2002; Hagar and Korolev 2007).

Super-recursive algorithms are also related to '''algorithmic schemes''', which are more general than super-recursive algorithms. Burgin argues (2005: 115) that it is necessary to make a clear distinction between super-recursive algorithms and those algorithmic schemes that are not algorithms. Under this distinction, some types of hypercomputation are obtained by super-recursive algorithms, e.g., inductive Turing machines, while other types of hypercomputation are directed by algorithmic schemas, e.g., infinite time Turing machines. This explains how works on super-recursive algorithms are related to hypercomputation and vice versa. According to this argument, super-recursive algorithms are just one way of defining a hypercomputational process.

== Examples ==

Examples of super-recursive algorithms include (Burgin 2005: 132):
* '''limiting recursive functions''' and '''limiting partial recursive functions''' (E.M. Gold 1965)
* '''trial and error predicates''' (Hilary Putnam 1965) 
* '''inductive inference machines''' (Carl Smith)
* '''inductive Turing machines''', which perform computations similar to computations of [[Turing machines]] and produce their results after a finite number of steps (Mark Burgin)
* '''limit Turing machines''', which perform computations similar to computations of Turing machines but their final results are limits of their intermediate results (Mark Burgin)
*  '''trial-and-error machines''' (Ja. Hintikka and A. Mutanen 1998)
* '''[[#Schmidhuber's generalized Turing machines|general Turing machines]]''' (J. Schmidhuber)
* '''Internet machines''' ([[Jan van Leeuwen|van Leeuwen, J.]] and Wiedermann, J.)
* '''evolutionary computers''', which use DNA to produce the value of a function (Darko Roglic)
* '''fuzzy computation''' (Jirí Wiedermann 2004)
* '''evolutionary Turing machines''' (Eugene Eberbach 2005)

Examples of algorithmic schemes include:

* '''Turing machines with arbitrary oracles''' (Alan Turing)
* '''Transrecursive operators''' (Borodyanskii and Burgin)
* '''machines that compute with real numbers''' (L. Blum, F. Cucker, M. Shub, and S. Smale 1998)
* '''neural networks based on real numbers''' (Hava Siegelmann 1999)

For examples of practical '''super-recursive algorithms''', see the book of Burgin.

== Inductive Turing machines ==

'''Inductive Turing machines''' implement an important class of super-recursive algorithms. An inductive Turing machine is a definite list of well-defined instructions for completing a task which, when given an initial state, will proceed through a well-defined series of successive states, eventually giving the final result. The difference between an inductive Turing machine and an ordinary [[Turing machine]] is that an ordinary Turing machine must stop when it has obtained its result, while in some cases an inductive Turing machine can continue to compute after obtaining the result, without stopping. Kleene called procedures that could run forever without stopping by the name ''calculation procedure or algorithm'' (Kleene 1952:137). Kleene also demanded that such an algorithm must eventually exhibit "some object" (Kleene 1952:137). Burgin argues that this condition is satisfied by inductive Turing machines, as their results are exhibited after a finite number of steps. The reason that inductive Turing machines cannot be instructed to halt when their final output is produced is that in some cases inductive Turing machines may not be able to tell at which step the result has been obtained.

Simple inductive Turing machines are equivalent to other models of computation such as general Turing machines of Schmidhuber, trial and error predicates of Hilary Putnam, limiting partial recursive functions of Gold, and trial-and-error machines of Hintikka and Mutanen (1998). More advanced inductive Turing machines are much more powerful. There are hierarchies of inductive Turing machines that can decide membership in arbitrary sets of the [[arithmetical hierarchy]] (Burgin 2005). In comparison with other equivalent models of computation, simple inductive Turing machines and general Turing machines give direct constructions of computing automata that are thoroughly grounded in physical machines. In contrast, trial-and-error predicates, limiting recursive functions, and limiting partial recursive functions present only syntactic systems of symbols with formal rules for their manipulation. Simple inductive Turing machines and general Turing machines are related to limiting partial recursive functions and trial-and-error predicates as Turing machines are related to partial recursive functions and lambda calculus.

The non-halting computations of inductive Turing machines should not be confused with infinite-time computations (see, for example, Potgieter 2006). First, some computations of inductive Turing machines do halt. As in the case of conventional Turing machines, some halting computations give the result, while others do not. Even if it does not halt, an inductive Turing machine produces output from time to time. If this output stops changing, it is then considered the result of the computation.

There are two main distinctions between ordinary Turing machines and simple inductive Turing machines. The first distinction is that even simple inductive Turing machines can do much more than conventional Turing machines. The second distinction is that a conventional Turing machine will always determine (by coming to a final state) when the result is obtained, while a simple inductive Turing machine, in some cases (such as when "computing" something that cannot be computed by an ordinary Turing machine), will not be able to make this determination.

== Schmidhuber's generalized Turing machines ==
A symbol sequence is '''computable in the limit''' if there is a finite, possibly non-halting program on a [[universal Turing machine]] that incrementally outputs every symbol of the sequence. This includes the dyadic expansion of &amp;pi; but still excludes most of the real numbers, because most cannot be described by a finite program. Traditional [[Turing machine]]s with a write-only output tape cannot edit their previous outputs; generalized [[Turing machines]], according to [[Jürgen Schmidhuber]], can edit their output tape as well as their work tape. He defines the constructively describable symbol sequences as those that have a finite, non-halting program running on a generalized Turing machine, such that any output symbol eventually converges, that is, it does not change any more after some finite initial time interval.  Schmidhuber (2000, 2002) uses this approach to define the set of formally describable or constructively computable universes or constructive [[theory of everything|theories of everything]]. Generalized Turing machines and simple inductive Turing machines are two classes of super-recursive algorithms that are the closest to recursive algorithms (Schmidhuber 2000).

== Relation to the Church–Turing thesis ==

The Church–Turing thesis in recursion theory relies on a particular definition of the term ''algorithm''. Based on definitions that are more general than the one commonly used in recursion theory, Burgin argues that super-recursive algorithms, such as '''inductive Turing machines''' disprove the [[Church–Turing thesis]]. He proves furthermore that super-recursive algorithms could theoretically provide even greater efficiency gains than using [[quantum algorithms]].

Burgin's interpretation of super-recursive algorithms has encountered opposition in the mathematical community. One critic is logician [[Martin Davis]], who argues that Burgin's claims have been well understood "for decades". Davis states, 
:"The present criticism is not about the mathematical discussion of these matters but only about the misleading claims regarding physical systems of the present and future."(Davis 2006: 128)
Davis disputes Burgin's claims that sets at level &lt;math&gt;\Delta^0_2&lt;/math&gt; of the [[arithmetical hierarchy]] can be called computable, saying
:"It is generally understood that for a computational result to be useful one must be able to at least recognize that it is indeed the result sought." (Davis 2006: 128)

== See also ==
* [[Interactive computation]]

==References==

* Blum, L.,  F. Cucker, M. Shub, and S. Smale, ''Complexity and real computation'', [[Springer Publishing]] 1998
* Burgin, Mark (2005), ''Super-recursive algorithms'', Monographs in computer science, Springer. {{isbn|0-387-95569-0}}
**José Félix Costa, [http://www.ams.org/mathscinet/search/publdoc.html?pg1=IID&amp;s1=193826&amp;r=3&amp;mx-pid=2246430 MR2246430 Review] in [[MathSciNet]].
** Harvey Cohn (2005), [http://www.computingreviews.net/browse/browse_topics4.cfm?ccs_id=2376 CR131542 (0606-0574) Review] in [[Computing Reviews]]
**Martin Davis (2007),[http://www.math.ucla.edu/~asl/bsl/1302/1302-004.ps Review ] in ''[[Bulletin of Symbolic Logic]]'', v. 13 n. 2. 
** Marc L. Smith (2006), [http://comjnl.oxfordjournals.org/cgi/reprint/49/6/762-a.pdf Review] in ''[[The Computer Journal]]'', Vol. 49 No. 6 
**Review, Vilmar Trevisan (2005), [[Zentralblatt MATH]], Vol. 1070. Review [http://siba-sinmdb.unile.it/cgi-bin/zmen/ZMATH/en/quick.html?first=1&amp;maxdocs=3&amp;type=html&amp;an=1070.68038&amp;format=complete 1070.68038] 
* Copeland, J. (2002) Hypercomputation, ''[[Minds and Machines]]'', v. 12, pp.&amp;nbsp;461–502
* Davis, Martin (2006), "[https://wayback.archive-it.org/all/20080221162316/http://people.cs.uchicago.edu/~simon/TEACH/28000/DavisUniversal.pdf The Church–Turing Thesis: Consensus and opposition]". Proceedings, Computability in Europe 2006.  Lecture notes in computer science, 3988 pp.&amp;nbsp;125–132
* Eberbach, E. (2005) "Toward a theory of evolutionary computation", ''[[BioSystems]]'' 82, 1-19
* Gold, E.M. Limiting recursion. ''[[J. Symb. Log.]]'' 10 (1965), 28-48.
* {{Citation
  | last=Gold | first=E. Mark
  | title=Language Identification in the Limit
  | volume=10
  | pages=447–474
  | url=http://web.mit.edu/~6.863/www/spring2009/readings/gold67limit.pdf
  | publisher= [[Information and Control]] | year=1967}}
* Hagar, A. and Korolev, A. (2007) [http://philsci-archive.pitt.edu/archive/00003180/ "Quantum Hypercomputation – Hype or Computation?"] 
* Hintikka, Ja. and Mutanen, A. An Alternative Concept of Computability, in “Language, Truth, and Logic in Mathematics”, Dordrecht, pp.&amp;nbsp;174–188, 1998
* {{Citation | last1=Kleene | first1=Stephen C. | author1-link=Stephen C. Kleene| title=Introduction to Metamathematics | publisher=[[North-Holland Publishing Company]] | location=Amsterdam | year=1952|edition=First}}.
* Peter Kugel,  "It's time to think outside the computational box", ''Communications of the ACM'', Volume 48, Issue 11, November 2005 
* Petrus H. Potgieter, "Zeno machines and hypercomputation", ''Theoretical Computer Science'', Volume 358,  Issue 1  (July 2006) pp.&amp;nbsp;23 – 33   
* Hilary Putnam, "Trial and Error Predicates and the Solution to a Problem of Mostowski". ''[[Journal of Symbolic Logic]]'', Volume 30, Issue 1 (1965), 49-57
* Darko Roglic, "[https://arxiv.org/abs/0708.2686 The universal evolutionary computer based on super-recursive algorithms of evolvability]"
* Hava Siegelmann, ''Neural Networks and Analog Computation: Beyond the Turing Limit'', [[Birkhäuser]], 1999, {{isbn|0817639497}}
* Turing, A. (1939) Systems of Logic Based on Ordinals, ''[[Proc. Lond. Math. Soc.]]'', Ser.2, v. 45: 161-228 
* [[Jan van Leeuwen|van Leeuwen, J.]] and Wiedermann, J. (2000a) ''Breaking the Turing Barrier: The case of the Internet'', Techn. Report, Inst. of Computer Science, [[Academy of Sciences of the Czech Republic]], Prague
* Jiří Wiedermann, Characterizing the super-Turing computing power and efficiency of classical fuzzy Turing machines, ''Theoretical Computer Science'', Volume 317, Issue 1-3, June 2004
* Jiří Wiedermann and [[Jan van Leeuwen]], "The emergent computational potential of evolving artificial living systems", ''AI Communications'', v. 15, No. 4, 2002

== Further reading ==

* Akl, S.G., Three counterexamples to dispel the myth of the universal computer, ''Parallel Processing Letters'', Vol. 16, No. 3, September 2006, pp.&amp;nbsp;381 – 403.
* Akl, S.G., The myth of universal computation, in: Parallel Numerics, Trobec, R., Zinterhof, P., Vajtersic, M., and Uhl, A., Eds., Part 2, ''Systems and Simulation'', University of Salzburg, Salzburg, Austria and Jozef Stefan Institute, Ljubljana, Slovenia, 2005, pp.&amp;nbsp;211 – 236
* Angluin, D., and Smith, C. H. (1983) Inductive Inference: Theory and Methods, ''Comput. Surveys'', v. 15, no. 3, pp.&amp;nbsp;237–269
* Apsïtis, K, Arikawa, S, Freivalds, R., Hirowatari, E., and Smith, C. H. (1999) On the inductive inference of recursive real-valued functions, ''[[Theoretical Computer Science (journal)|Theoretical Computer Science]]'', 219(1-2): 3—17
* Boddy, M, Dean, T.  1989.  "Solving Time-Dependent Planning Problems". Technical Report: CS-89-03, [[Brown University]]
* Burgin, M. "Algorithmic Complexity of Recursive and Inductive Algorithms", ''Theoretical Computer Science'', v. 317, No. 1/3, 2004, pp.&amp;nbsp;31–60                   
* Burgin, M. and Klinger, A. Experience, Generations, and Limits in Machine Learning, ''[[Theoretical Computer Science (journal)|Theoretical Computer Science]]'', v. 317, No. 1/3, 2004, pp.&amp;nbsp;71–91    
* Eberbach, E., and Wegner, P., "Beyond Turing Machines", ''Bulletin of the [[European Association for Theoretical Computer Science]]'' (EATCS Bulletin), 81, Oct. 2003, 279-304
* S. Zilberstein, Using Anytime Algorithms in Intelligent Systems, "AI Magazine", 17(3):73-83, 1996

== External links ==
* [http://www.la-acm.org/Archives/laacm9912.html A New Paradigm for Computation]. Los Angeles ACM Chapter Meeting, December 1, 1999.
* ''[http://foldoc.org/?anytime+algorithm Anytime algorithm]'' from [[FOLDOC]]

[[Category:Algorithms]]
[[Category:Hypercomputation]]
[[Category:Theory of computation]]</text>
      <sha1>fwjiz3mn9ubzjmp4xmxho2jpgxu6o2b</sha1>
    </revision>
  </page>
  <page>
    <title>TORQUE</title>
    <ns>0</ns>
    <id>3234990</id>
    <revision>
      <id>852799150</id>
      <parentid>852798244</parentid>
      <timestamp>2018-07-31T12:53:48Z</timestamp>
      <contributor>
        <ip>199.64.7.57</ip>
      </contributor>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="5412">{{Cleanup|reason=the article needs work badly|date=February 2012}} 
{{Infobox software
| name                   = TORQUE
| developer              = Adaptive Computing
| released               = {{start date|2003}}
| latest release version = 6.1.2
| latest release date    = {{start date and age|2018|02|14|df=yes}}
| latest preview version =
| latest preview date    = 
| programming language   = [[ANSI C]]
| operating system       = [[Unix-like]]
| size                   = 5 MB
| language               = [[English language|English]]
| genre                  = [[Distributed resource manager]]
| license                = Proprietary License (As of June 2018) &lt;ref name="proprietary_license"&gt;{{cite web| last =| first =| authorlink =| coauthors =| title =Closed Source Software License | work =| publisher =Adaptive Computing, Inc.| year=2018| url =http://www.adaptivecomputing.com/products/torque/ |format =| doi =| accessdate =2018-07-31 |archiveurl= |archivedate=|deadurl=no }}&lt;/ref&gt;, OpenPBS version 2.3&lt;ref name="openpbsv2.3_licence"&gt;{{cite web| last =Veridian Information Solutions, Inc.| first =| authorlink =| coauthors =| title =OpenPBS (Portable Batch System) v2.3 Software License | work =| publisher =Cluster Resources, Inc.| year=2000| url =http://www.clusterresources.com/products/torque/docs10/a.hlicense.txt |format =| doi =| accessdate =2011-07-31 |archiveurl=https://www.webcitation.org/60b6KmP55 |archivedate=2011-07-31  |deadurl=no }}&lt;/ref&gt;&lt;ref name="torque_openpbsv2.3"&gt;{{cite web| last =| first =| authorlink =| coauthors =| title =Torque resource manager| work =| publisher =Cluster Resources, Inc. | year =2011| url =http://www.adaptivecomputing.com/products/open-source/torque/ |format =| doi =| accessdate =2011-07-31 |archiveurl=https://www.webcitation.org/60b6XX1To |archivedate=2011-07-31  |deadurl=no }}&lt;/ref&gt; (non-[[free software license|free]] in [[Debian Free Software Guidelines|DFSG]]&lt;ref name="DFSG_Torque_non-free" /&gt;), or TORQUE v2.5+ Software License v1.1{{citation needed|date=August 2011}} 
| website                = {{URL|adaptivecomputing.com/products/torque/}}
}}
The '''Terascale Open-source Resource and QUEue Manager''' ('''TORQUE''')&lt;ref name="TORQUE paper"&gt;[http://doi.acm.org/10.1145/1188455.1188464  TORQUE resource manager, Garrick Staples, SC '06: Proceedings of the 2006 ACM/IEEE conference on Supercomputing], {{ISBN|0-7695-2700-0}}&lt;/ref&gt; is a [[distributed resource manager]] providing control over batch jobs and distributed compute nodes. TORQUE can integrate with the non-commercial [[Maui Cluster Scheduler]] or the commercial [[Moab Cluster Suite|Moab Workload Manager]] to improve overall utilization, scheduling and administration on a cluster. 

The TORQUE community has extended the original [[Portable Batch System|PBS]] to extend scalability, fault tolerance, and functionality. Contributors include [[National Center for Supercomputing Applications|NCSA]], [[Ohio Supercomputer Center|OSC]], [[University of Southern California|USC]], the [[United States Department of Energy|US DOE]], [[Sandia National Laboratories|Sandia]], [[Pacific Northwest National Laboratory|PNNL]], [[University at Buffalo, The State University of New York|UB]], [[TeraGrid]], and other [[High-performance computing|HPC]] organizations. As of June 2018, TORQUE is no longer open-source even though previously it was described by its developers as [[open-source software]],&lt;ref name="proprietary_license" /&gt; using the OpenPBS version 2.3 license&lt;ref name="openpbsv2.3_licence" /&gt; and as non-[[free software license|free software]] by the [[Debian Free Software Guidelines]]&lt;ref name="DFSG_Torque_non-free"&gt;{{cite web| last =| first =| authorlink =| coauthors =| title = The DFSG and Software Licenses - Licenses that are DFSG-incompatible| work =| publisher =[[Debian]] | date =2011-03-27 | url =http://wiki.debian.org/DFSGLicenses#Licenses_that_are_DFSG-incompatible |format =| doi =| accessdate =2011-07-31 |archiveurl=https://www.webcitation.org/60b7SqSsP |archivedate=2011-07-31  |deadurl=no }}&lt;/ref&gt; due to license issues.

==Feature set==
TORQUE provides enhancements over standard OpenPBS in the following areas:

* Fault Tolerance
** Additional failure conditions checked/handled
** Node health check script support
* Scheduling Interface
** Extended query interface providing the scheduler with additional and more accurate information
** Extended control interface allowing the scheduler increased control over job behavior and attributes
** Allows the collection of statistics for completed jobs
* Scalability
** Significantly improved server to MOM communication model
** Ability to handle larger clusters (over 15 TF/2,500 processors)
** Ability to handle larger jobs (over 2000 processors)
** Ability to support larger server messages
* Usability
** Extensive logging additions
** More human readable logging (i.e. no more 'error 15038 on command 42')

==See also==
* {{section link|Job scheduler|Batch queuing for HPC clusters}}
&lt;!-- sort alphabetically: --&gt;
* [[Beowulf cluster]]
* [[Maui Cluster Scheduler]]
* [[Open Source Cluster Application Resources]] (OSCAR)
* [[Slurm Workload Manager]]
* [[Univa Grid Engine]]

==References==
{{reflist}}

==External links==
* {{Official website|adaptivecomputing.com/products/open-source/torque/}}
* [http://www.pbsworks.com/ OpenPBS home page]

[[Category:Computational science]]
[[Category:Job scheduling]]</text>
      <sha1>az33m5x34dd1gt3g5stybd1ej637xst</sha1>
    </revision>
  </page>
  <page>
    <title>Tarjan's strongly connected components algorithm</title>
    <ns>0</ns>
    <id>8244667</id>
    <revision>
      <id>863419246</id>
      <parentid>863386690</parentid>
      <timestamp>2018-10-10T17:11:01Z</timestamp>
      <contributor>
        <username>Aalok.sathe</username>
        <id>14758482</id>
      </contributor>
      <minor/>
      <comment>/* External links */ reworded to make sense in context</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="12031">{{Infobox algorithm
|class=
|image= [[File:Tarjan's Algorithm Animation.gif|250px]]
|caption = Tarjan's algorithm  animation
|data=[[Graph (data structure)|Graph]]
|time= &lt;math&gt;O(|V|+|E|)&lt;/math&gt;
|best-time=
|average-time=
|space=
|optimal=
|complete=
}}
'''Tarjan's algorithm'''  is an [[algorithm]] in [[graph theory]] for finding the [[strongly connected component]]s of a [[directed graph]]. It runs in [[linear time]], matching the time bound for alternative methods including [[Kosaraju's algorithm]] and the [[path-based strong component algorithm]]. Tarjan's algorithm is named for its inventor, [[Robert Tarjan]].&lt;ref&gt;{{citation|first=R. E.|last=Tarjan|authorlink=Robert Tarjan|title=Depth-first search and linear graph algorithms|journal=[[SIAM Journal on Computing]]|volume=1|year=1972|issue=2|pages=146–160|doi=10.1137/0201010}}&lt;/ref&gt;

== Overview ==

The algorithm takes a [[directed graph]] as input, and produces a [[Partition of a set|partition]] of the graph's [[Vertex (graph theory)|vertices]] into the graph's strongly connected components. Each vertex of the graph appears in exactly one of the strongly connected components.  Any vertex that is not on a directed cycle forms a strongly connected component all by itself: for example, a vertex whose in-degree or out-degree is 0, or any vertex of an acyclic graph.

The basic idea of the algorithm is this: a depth-first search begins from an arbitrary start node (and subsequent depth-first searches are conducted on any nodes that have not yet been found).  As usual with depth-first search, the search visits every node of the graph exactly once, declining to revisit any node that has already been visited. Thus, the collection of search trees is a [[Spanning forest#Spanning forests|spanning forest]] of the graph.  The strongly connected components will be recovered as certain subtrees of this forest.  The roots of these subtrees are called the "roots" of the strongly connected components.  Any node of a strongly connected component might serve as the root, if it happens to be the first node of the component that is discovered by the search.

=== Stack invariant ===

Nodes are placed on a [[Stack (data structure)|stack]] in the order in which they are visited.  When the depth-first search recursively visits a node &lt;tt&gt;v&lt;/tt&gt; and its descendants, those nodes are not all necessarily popped from the stack when this recursive call returns.  The crucial [[Invariant (computer science)|invariant property]] is that a node remains on the stack after it has been visited if and only if there exists a path in the input graph from it to some node earlier on the stack.

At the end of the call that visits &lt;tt&gt;v&lt;/tt&gt; and its descendants, we know whether &lt;tt&gt;v&lt;/tt&gt; itself has a path to any node earlier on the stack.  If so, the call returns, leaving &lt;tt&gt;v&lt;/tt&gt; on the stack to preserve the invariant.  If not, then &lt;tt&gt;v&lt;/tt&gt; must be the root of its strongly connected component, which consists of &lt;tt&gt;v&lt;/tt&gt; together with any nodes later on the stack than &lt;tt&gt;v&lt;/tt&gt; (such nodes all have paths back to &lt;tt&gt;v&lt;/tt&gt; but not to any earlier node, because if they had paths to earlier nodes then &lt;tt&gt;v&lt;/tt&gt; would also have paths to earlier nodes which is false). The connected component rooted at &lt;tt&gt;v&lt;/tt&gt; is then popped from the stack and returned, again preserving the invariant.

=== Bookkeeping ===

Each node &lt;tt&gt;v&lt;/tt&gt; is assigned a unique integer &lt;tt&gt;v.index&lt;/tt&gt;, which numbers the nodes consecutively in the order in which they are discovered.  It also maintains a value &lt;tt&gt;v.lowlink&lt;/tt&gt; that represents the smallest index of any node known to be reachable from &lt;tt&gt;v&lt;/tt&gt; through &lt;tt&gt;v&lt;/tt&gt;'s  DFS subtree, including &lt;tt&gt;v&lt;/tt&gt; itself. Therefore &lt;tt&gt;v&lt;/tt&gt; must be left on the stack if &lt;tt&gt;v.lowlink &lt; v.index&lt;/tt&gt;, whereas v must be removed as the root of a strongly connected component if &lt;tt&gt;v.lowlink == v.index&lt;/tt&gt;.  The value &lt;tt&gt;v.lowlink&lt;/tt&gt; is computed during the depth-first search from &lt;tt&gt;v&lt;/tt&gt;, as this finds the nodes that are reachable from &lt;tt&gt;v&lt;/tt&gt;.

== The algorithm in pseudocode ==
 
  '''algorithm''' tarjan '''is'''
   '''input:''' graph ''G'' = (''V'', ''E'')
   '''output:''' set of strongly connected components (sets of vertices)
 
   ''index'' := 0
   ''S'' := empty array
   '''for each''' ''v'' '''in''' ''V'' '''do'''
     '''if''' (''v''.index is undefined) '''then'''
       strongconnect(''v'')
     '''end if'''
   '''end for'''
 
   '''function''' strongconnect(''v'')
     ''// Set the depth index for v to the smallest unused index''
     ''v''.index := ''index''
     ''v''.lowlink := ''index''
     ''index'' := ''index'' + 1
     ''S''.push(''v'')
     ''v''.onStack := true
 
     ''// Consider successors of v''
     '''for each''' (''v'', ''w'') '''in''' ''E'' '''do'''
       '''if''' (''w''.index is undefined) '''then'''
         ''// Successor w has not yet been visited; recurse on it''
         strongconnect(''w'')
         ''v''.lowlink  := min(''v''.lowlink, ''w''.lowlink)
       '''else if''' (''w''.onStack) '''then'''
         ''// Successor w is in stack S and hence in the current SCC''
         ''// If ''w'' is not on stack, then (''v'', ''w'') is a cross-edge in the DFS tree and must be ignored
         ''// Note: The next line may look odd - but is correct.''
         ''// It says w.index not w.lowlink; that is deliberate and from the original paper''
         ''v''.lowlink  := min(''v''.lowlink, ''w''.index)
       '''end if'''
     '''end for'''
 
     ''// If v is a root node, pop the stack and generate an SCC''
     '''if''' (''v''.lowlink = ''v''.index) '''then'''
       start a new strongly connected component
       '''repeat'''
         ''w'' := ''S''.pop()
         ''w''.onStack := false
         add ''w'' to current strongly connected component
       '''while''' (''w'' != ''v'')
       output the current strongly connected component
     '''end if'''
   '''end function'''

The &lt;tt&gt;index&lt;/tt&gt; variable is the depth-first search node number counter. &lt;tt&gt;S&lt;/tt&gt; is the node stack, which starts out empty and stores the history of nodes explored but not yet committed to a strongly connected component. Note that this is not the normal depth-first search stack, as nodes are not popped as the search returns up the tree; they are only popped when an entire strongly connected component has been found.

The outermost loop searches each node that has not yet been visited, ensuring that nodes which are not reachable from the first node are still eventually traversed. The function &lt;tt&gt;strongconnect&lt;/tt&gt; performs a single depth-first search of the graph, finding all successors from the node &lt;tt&gt;v&lt;/tt&gt;, and reporting all strongly connected components of that subgraph.

When each node finishes recursing, if its lowlink is still set to its index, then it is the root node of a strongly connected component, formed by all of the nodes above it on the stack. The algorithm pops the stack up to and including the current node, and presents all of these nodes as a strongly connected component.

Note that &lt;tt&gt;''v''.lowlink  := min(''v''.lowlink, ''w''.index) &lt;/tt&gt; is the correct way to update ''v.lowlink'' if ''w'' is on stack. Because ''w'' is on the stack already, ''(v, w)'' is a back-edge in the DFS tree and therefore ''w'' is not in the subtree of ''v''. Because ''v.lowlink'' takes into account nodes reachable only through the nodes in the subtree of ''v'' we must stop at ''w'' and use ''w.index'' instead of ''w.lowlink''. 

== Complexity ==
''Time Complexity'': The Tarjan procedure is called once for each node; the forall statement considers each edge at most once.  The algorithm's running time is therefore linear in the number of edges and nodes in G, i.e. &lt;math&gt;O(|V|+|E|)&lt;/math&gt;.

In order to achieve this complexity, the test for whether &lt;tt&gt;w&lt;/tt&gt; is on the stack should be done in constant time.
This may be done, for example, by storing a flag on each node that indicates whether it is on the stack, and performing this test by examining the flag.

''Space Complexity'': The Tarjan procedure requires two words of supplementary data per vertex for the &lt;tt&gt;index&lt;/tt&gt; and &lt;tt&gt;lowlink&lt;/tt&gt; fields, along with one bit for &lt;tt&gt;onStack&lt;/tt&gt; and another for determining when &lt;tt&gt;index&lt;/tt&gt; is undefined.  In addition, one word is required on each stack frame to hold &lt;tt&gt;v&lt;/tt&gt; and another for the current position in the edge list.  Finally, the worst-case size of the stack &lt;tt&gt;S&lt;/tt&gt; must be &lt;math&gt;|V|&lt;/math&gt; (i.e. when the graph is one giant component).  This gives a final analysis of &lt;math&gt;O(|V|\cdot(2+5w))&lt;/math&gt; where &lt;math&gt;w&lt;/math&gt; is the machine word size.  The variation of Nuutila and Soisalon-Soininen reduced this to &lt;math&gt;O(|V|\cdot(1+4w))&lt;/math&gt; and, subsequently, that of Pearce requires only &lt;math&gt;O(|V|\cdot(1+3w))&lt;/math&gt;.&lt;ref&gt;{{cite web|last=Nuutila|first=Esko|title=On Finding the Strongly Connected Components in a Directed Graph|url=https://doi.org/10.1016/0020-0190(94)90047-7|journal=Information Processing Letters|pages=9-14|volume=49|number=1|accessdate=13 December 2017}}&lt;/ref&gt;&lt;ref&gt;{{cite web|last=Pearce|first=David|title=A Space Efficient Algorithm for Detecting Strongly Connected Components|url=https://doi.org/10.1016/j.ipl.2015.08.010|journal=Information Processing Letters|pages=47-52|number=1|volume=116|accessdate=13 December 2017}}&lt;/ref&gt;

==Additional remarks==
While there is nothing special about the order of the nodes within each strongly connected component, one useful property of the algorithm is that no strongly connected component will be identified before any of its successors. Therefore, the order in which the strongly connected components are identified constitutes a reverse [[Topological sorting|topological sort]] of the [[Directed acyclic graph|DAG]] formed by the strongly connected components.&lt;ref&gt;{{cite web|last=Harrison|first=Paul|title=Robust topological sorting and Tarjan's algorithm in Python|url=http://www.logarithmic.net/pfh/blog/01208083168|accessdate=9 February 2011}}&lt;/ref&gt;

[[Donald Knuth]] described Tarjan's algorithm as one of Knuth's favorite implementations in his book ''The Stanford GraphBase''.&lt;ref&gt;Knuth, ''The Stanford GraphBase'', pages 512–519.&lt;/ref&gt;
He also wrote:&lt;ref&gt;{{cite web|last=Knuth|first=Donald|title=Twenty Questions for Donald Knuth|url=http://www.informit.com/articles/article.aspx?p=2213858&amp;WT.mc_id=Author_Knuth_20Questions}}&lt;/ref&gt; {{quote|The data structures that he devised for this problem fit together in an amazingly beautiful way, so that the quantities you need to look at while exploring a directed graph are always magically at your fingertips. And his algorithm also does topological sorting as a byproduct.}}

== References ==
&lt;references /&gt;

== External links ==
*[https://stackoverflow.com/questions/6643076/tarjan-cycle-detection-help-c#sca Implementation of Tarjan's Algorithm in .NET]
*[https://github.com/danielrbradley/CycleDetection Implementation of Tarjan's Algorithm in .NET (GitHub)]
*[http://www.vacilando.org/article/php-implementation-tarjans-cycle-detection-algorithm Implementation of Tarjan's Algorithm in PHP]
*[https://github.com/bwesterb/py-tarjan/ Implementation of Tarjan's Algorithm in Python]
*[https://gist.github.com/1440602 Implementation of Tarjan's Algorithm in Javascript]
*[http://clj-me.cgrand.net/2013/03/18/tarjans-strongly-connected-components-algorithm/ Implementation of Tarjan's Algorithm in Clojure]
*[http://www.geeksforgeeks.org/tarjan-algorithm-find-strongly-connected-components/ Implementation of Tarjan's Algorithm in C++]
*[https://github.com/pallas/libite/blob/master/tarjan.cc Iterative version of Tarjan's Algorithm in C++]
*[https://github.com/t1/graph/blob/master/src/main/java/com/github/t1/graph/StronglyConnectedComponentsFinder.java Implementation of Tarjan's Algorithm in Java]
*[https://github.com/looplab/tarjan Implementation of Tarjan's Algorithm in Go]

[[Category:Graph algorithms]]
[[Category:Graph connectivity]]
[[Category:Articles with example pseudocode]]</text>
      <sha1>2j8n03bsphn23nor8v5c6m676apxm29</sha1>
    </revision>
  </page>
  <page>
    <title>Van der Waerden's theorem</title>
    <ns>0</ns>
    <id>157178</id>
    <revision>
      <id>867746706</id>
      <parentid>866018534</parentid>
      <timestamp>2018-11-07T19:11:12Z</timestamp>
      <contributor>
        <username>JCW-CleanerBot</username>
        <id>31737083</id>
      </contributor>
      <minor/>
      <comment>clean up, replaced: Proc. American Math. Soc. → Proc. Amer. Math. Soc.</comment>
      <model>wikitext</model>
      <format>text/x-wiki</format>
      <text xml:space="preserve" bytes="22876">'''Van der Waerden's theorem''' is a theorem in the branch of [[mathematics]] called [[Ramsey theory]].  Van der Waerden's theorem states that for any given positive [[integer]]s ''r'' and ''k'', there is some number ''N'' such that if the integers {1, 2, ..., ''N''} are colored, each with one of ''r'' different colors, then there are at least ''k'' integers in [[arithmetic progression]] all of the same color. The least such ''N'' is the [[Van der Waerden number]] ''W''(''r'',&amp;nbsp;''k''), named after the Dutch mathematician [[Bartel Leendert van der Waerden|B. L. van der Waerden]].&lt;ref&gt;{{cite journal |authorlink=Bartel Leendert van der Waerden |first=B. L. |last=van der Waerden |title=Beweis einer Baudetschen Vermutung|language=de |journal=Nieuw. Arch. Wisk. |volume=15 |year=1927 |issue= |pages=212–216 }}&lt;/ref&gt;

==Example==
For example, when ''r'' = 2, you have two colors, say &lt;span style="color:red;"&gt;red&lt;/span&gt; and &lt;span style="color:blue;"&gt;blue&lt;/span&gt;. ''W''(2, 3) is bigger than 8, because you can color the integers from {1, ..., 8} like this:
{|class="wikitable"
|&amp;nbsp;1&amp;nbsp;
|&amp;nbsp;2&amp;nbsp;
|&amp;nbsp;3&amp;nbsp;
|&amp;nbsp;4&amp;nbsp;
|&amp;nbsp;5&amp;nbsp;
|&amp;nbsp;6&amp;nbsp;
|&amp;nbsp;7&amp;nbsp;
|&amp;nbsp;8&amp;nbsp;
|-
|&amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp;
|}

and no three integers of the same color form an [[arithmetic progression]].  But you can't add a ninth integer to the end without creating such a progression.  If you add a &lt;span style="color:red;"&gt;red 9&lt;/span&gt;, then the &lt;span style="color:red;"&gt;red 3&lt;/span&gt;, &lt;span style="color:red;"&gt;6&lt;/span&gt;, and &lt;span style="color:red;"&gt;9&lt;/span&gt; are in arithmetic progression.  Alternatively, if you add a &lt;span style="color:blue;"&gt;blue 9&lt;/span&gt;, then the &lt;span style="color:blue;"&gt;blue 1&lt;/span&gt;, &lt;span style="color:blue;"&gt;5&lt;/span&gt;, and &lt;span style="color:blue;"&gt;9&lt;/span&gt; are in arithmetic progression.

In fact, there is no way of coloring 1 through 9 without creating such a progression (it can be proved by considering examples).  Therefore, ''W''(2, 3) is 9.

==Open problem==
It is an open problem to determine the values  of ''W''(''r'', ''k'') for most values of ''r'' and ''k''. The proof of the theorem provides only an upper bound.  For the case of ''r'' = 2 and ''k'' = 3, for example, the argument given below shows that it is sufficient to color the integers {1, ..., 325} with two colors to guarantee there will be a single-colored arithmetic progression of length 3. But in fact, the bound of 325 is very loose; the minimum required number of integers is only 9.  Any coloring of the integers {1, ..., 9} will have three evenly spaced integers of one color.

For ''r'' = 3 and ''k'' = 3, the bound given by the theorem is 7(2·3&lt;sup&gt;7&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;1)(2·3&lt;sup&gt;7·(2·3&lt;sup&gt;7&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;1)&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;1), or approximately 4.22·10&lt;sup&gt;14616&lt;/sup&gt;. But actually, you don't need that many integers to guarantee a single-colored progression of length 3; you only need 27. (And it is possible to color {1, ..., 26} with three colors so that there is no single-colored arithmetic progression of length 3; for example:
{|class="wikitable"
|&amp;nbsp;1&amp;nbsp;
|&amp;nbsp;2&amp;nbsp;
|&amp;nbsp;3&amp;nbsp;
|&amp;nbsp;4&amp;nbsp;
|&amp;nbsp;5&amp;nbsp;
|&amp;nbsp;6&amp;nbsp;
|&amp;nbsp;7&amp;nbsp;
|&amp;nbsp;8&amp;nbsp;
|&amp;nbsp;9&amp;nbsp;
|&amp;nbsp;10&amp;nbsp;
|&amp;nbsp;11&amp;nbsp;
|&amp;nbsp;12&amp;nbsp;
|&amp;nbsp;13&amp;nbsp;
|&amp;nbsp;14&amp;nbsp;
|&amp;nbsp;15&amp;nbsp;
|&amp;nbsp;16&amp;nbsp;
|&amp;nbsp;17&amp;nbsp;
|&amp;nbsp;18&amp;nbsp;
|&amp;nbsp;19&amp;nbsp;
|&amp;nbsp;20&amp;nbsp;
|&amp;nbsp;21&amp;nbsp;
|&amp;nbsp;22&amp;nbsp;
|&amp;nbsp;23&amp;nbsp;
|&amp;nbsp;24&amp;nbsp;
|&amp;nbsp;25&amp;nbsp;
|&amp;nbsp;26&amp;nbsp;
|-
|&amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:yellow;"&gt;Y&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:yellow;"&gt;Y&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:yellow;"&gt;Y&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:yellow;"&gt;Y&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:yellow;"&gt;Y&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:yellow;"&gt;Y&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:yellow;"&gt;Y&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:yellow;"&gt;Y&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp;
|&amp;nbsp;'''&lt;span style="color:yellow;"&gt;Y&lt;/span&gt;'''&amp;nbsp;
|}
.)

Anyone who can reduce the general upper bound to any 'reasonable' function can win a large cash prize. [[Ronald Graham]] has offered a prize of [[US$]]1000 for showing ''W''(2,''k'')&amp;lt;2&lt;sup&gt;''k''&lt;sup&gt;2&lt;/sup&gt;&lt;/sup&gt;.&lt;ref&gt;{{cite journal |authorlink=Ronald Graham |first=Ron |last=Graham |title=Some of My Favorite Problems in Ramsey Theory |journal=INTEGERS (The Electronic Journal of Combinatorial Number Theory |url=http://www.integers-ejcnt.org/vol7-2.html |volume=7 |issue=2 |year=2007 |pages=#A15 }}&lt;/ref&gt; The best upper bound currently known is due to [[Timothy Gowers]],&lt;ref&gt;{{cite journal |authorlink=Timothy Gowers |first=Timothy |last=Gowers |title=A new proof of Szemerédi's theorem |journal=Geom. Funct. Anal. |volume=11 |issue=3 |pages=465–588 |year=2001 |url=http://www.dpmms.cam.ac.uk/~wtg10/papers.html |doi=10.1007/s00039-001-0332-9 }}&lt;/ref&gt; who establishes

: &lt;math&gt;W(r,k) \leq 2^{2^{r^{2^{2^{k + 9}}}}},&lt;/math&gt;

by first establishing a similar result for [[Szemerédi's theorem]], which is a stronger version of Van der Waerden's theorem.  The previously best-known bound was due to [[Saharon Shelah]] and proceeded via first proving a result for the [[Hales&amp;ndash;Jewett theorem]], which is another strengthening of Van der Waerden's theorem.

The best lower bound currently known for &lt;math&gt;W(2, k)&lt;/math&gt; is that for all positive &lt;math&gt;\varepsilon&lt;/math&gt; we have &lt;math&gt;W(2, k) &gt; 2^k/k^\varepsilon&lt;/math&gt;, for all sufficiently large &lt;math&gt;k&lt;/math&gt;.&lt;ref&gt;{{cite journal |authorlink=Zoltán Szabó (mathematician)|first=Zoltán |last=Szabó |title=An application of Lovász' local lemma -- a new lower bound for the van der Waerden number |journal=Random Struct. Algorithms |volume=1 | issue = 3 |pages=343–360 |year=1990 }}&lt;/ref&gt;

== Proof of Van der Waerden's theorem (in a special case) ==

The following proof is due to [[Ronald Graham|Ron Graham]] and B.L. Rothschild.&lt;ref name="Graham1974"&gt;{{cite journal |authorlink=Ronald Graham |first=R. L. |last=Graham |first2=B. L. |last2=Rothschild |title=A short proof of van der Waerden's theorem on arithmetic progressions |journal=Proc. Amer. Math. Soc. |volume=42 |issue=2 |year=1974 |pages=385–386 |doi=10.1090/S0002-9939-1974-0329917-8 }}&lt;/ref&gt; [[A. Ya. Khinchin|Khinchin]]&lt;ref&gt;{{Harvtxt|Khinchin|1998|pp=11–17|loc=chapter 1}}&lt;/ref&gt; gives a fairly simple proof of the theorem without estimating ''W''(''r'',&amp;nbsp;''k'').

=== Proof in the case of W(2, 3) ===
{| class="wikitable floatright" style="text-align:right
|+ W(2, 3) table
! ''b'' !! colspan="5" | ''c''(''n''): color of integers
|-
! rowspan="2" | 0
| 1 || 2 || 3 || 4 || 5
|-
| &amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp;
|-
! rowspan="2" | 1
| 6 || 7 || 8 || 9 || 10
|-
| &amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp;
|-
! …
| colspan="5" | …
|-
! rowspan="2" | 64
| 321 || 322 || 323 || 324 || 325
|-
| &amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp;
|}
We will prove the special case mentioned above, that ''W''(2, 3) ≤ 325. Let ''c''(''n'') be a coloring of the integers {1, ..., 325}.  We will find three elements of {1, ..., 325} in arithmetic progression that are the same color.

Divide {1, ..., 325} into the 65 blocks {1, ..., 5}, {6, ..., 10}, ... {321, ..., 325}, thus each block is of the form {5''b'' + 1, ..., 5''b'' + 5} for some ''b'' in {0, ..., 64}. Since each integer is colored either &lt;span style="color:red;"&gt;red&lt;/span&gt; or &lt;span style="color:blue;"&gt;blue&lt;/span&gt;, each block is colored in one of 32 different ways.  By the [[pigeonhole principle]], there are two blocks among the first 33 blocks that are colored identically. That is, there are  two integers ''b''&lt;sub&gt;1&lt;/sub&gt; and ''b''&lt;sub&gt;2&lt;/sub&gt;, both in {0,...,32}, such that

: ''c''(5''b''&lt;sub&gt;1&lt;/sub&gt; + ''k'') = ''c''(5''b''&lt;sub&gt;2&lt;/sub&gt; + ''k'')

for all ''k'' in {1, ..., 5}.  Among the three integers 5''b''&lt;sub&gt;1&lt;/sub&gt; + 1, 5''b''&lt;sub&gt;1&lt;/sub&gt; + 2, 5''b''&lt;sub&gt;1&lt;/sub&gt; + 3, there must be at least two that are of the same color. (The [[pigeonhole principle]] again.)  Call these 5''b''&lt;sub&gt;1&lt;/sub&gt; + ''a''&lt;sub&gt;1&lt;/sub&gt; and 5''b''&lt;sub&gt;1&lt;/sub&gt; + ''a''&lt;sub&gt;2&lt;/sub&gt;, where the ''a''&lt;sub&gt;''i''&lt;/sub&gt; are in {1,2,3} and ''a''&lt;sub&gt;1&lt;/sub&gt; &amp;lt; ''a''&lt;sub&gt;2&lt;/sub&gt;.  Suppose (without loss of generality) that these two integers are both &lt;span style="color:red;"&gt;red&lt;/span&gt;.  (If they are both &lt;span style="color:blue;"&gt;blue&lt;/span&gt;, just exchange '&lt;span style="color:red;"&gt;red&lt;/span&gt;' and '&lt;span style="color:blue;"&gt;blue&lt;/span&gt;' in what follows.)

Let ''a''&lt;sub&gt;3&lt;/sub&gt; = 2''a''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;&amp;minus;&amp;nbsp;''a''&lt;sub&gt;1&lt;/sub&gt;. If 5''b''&lt;sub&gt;1&lt;/sub&gt; + ''a''&lt;sub&gt;3&lt;/sub&gt; is &lt;span style="color:red;"&gt;red&lt;/span&gt;, then we have found our arithmetic progression: 5''b''&lt;sub&gt;1&lt;/sub&gt;&amp;nbsp;+&amp;nbsp;''a''&lt;sub&gt;''i''&lt;/sub&gt; are all &lt;span style="color:red;"&gt;red&lt;/span&gt;.	

Otherwise, 5''b''&lt;sub&gt;1&lt;/sub&gt; + ''a''&lt;sub&gt;3&lt;/sub&gt; is &lt;span style="color:blue;"&gt;blue&lt;/span&gt;. Since ''a''&lt;sub&gt;3&lt;/sub&gt; ≤ 5,  5''b''&lt;sub&gt;1&lt;/sub&gt; + ''a''&lt;sub&gt;3&lt;/sub&gt; is in the ''b''&lt;sub&gt;1&lt;/sub&gt; block, and since the ''b''&lt;sub&gt;2&lt;/sub&gt; block is colored identically, 5''b''&lt;sub&gt;2&lt;/sub&gt; + ''a''&lt;sub&gt;3&lt;/sub&gt; is also &lt;span style="color:blue;"&gt;blue&lt;/span&gt;.

Now let ''b''&lt;sub&gt;3&lt;/sub&gt; = 2''b''&lt;sub&gt;2&lt;/sub&gt;&amp;nbsp;&amp;minus;&amp;nbsp;''b''&lt;sub&gt;1&lt;/sub&gt;. Then ''b''&lt;sub&gt;3&lt;/sub&gt; ≤ 64. Consider the integer  5''b''&lt;sub&gt;3&lt;/sub&gt; + ''a''&lt;sub&gt;3&lt;/sub&gt;, which must be ≤ 325. What color is it?

If it is &lt;span style="color:red;"&gt;red&lt;/span&gt;, then 5''b''&lt;sub&gt;1&lt;/sub&gt; + ''a''&lt;sub&gt;1&lt;/sub&gt;, 5''b''&lt;sub&gt;2&lt;/sub&gt; + ''a''&lt;sub&gt;2&lt;/sub&gt;, and 5''b''&lt;sub&gt;3&lt;/sub&gt; + ''a''&lt;sub&gt;3&lt;/sub&gt; form a &lt;span style="color:red;"&gt;red&lt;/span&gt; arithmetic progression. But if it is &lt;span style="color:blue;"&gt;blue&lt;/span&gt;, then 5''b''&lt;sub&gt;1&lt;/sub&gt; + ''a''&lt;sub&gt;3&lt;/sub&gt;, 5''b''&lt;sub&gt;2&lt;/sub&gt; + ''a''&lt;sub&gt;3&lt;/sub&gt;, and 5''b''&lt;sub&gt;3&lt;/sub&gt; + ''a''&lt;sub&gt;3&lt;/sub&gt; form a &lt;span style="color:blue;"&gt;blue&lt;/span&gt; arithmetic progression. Either way, we are done.

=== Proof in the case of W(3, 3) ===
{| class="wikitable floatright" style="text-align:right
|+ W(3, 3) table&lt;br /&gt;''g''=2·3&lt;sup&gt;7·(2·3&lt;sup&gt;7&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;1)&lt;/sup&gt;&amp;nbsp;,&lt;br /&gt;''m''=7(2·3&lt;sup&gt;7&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;1)
! ''b'' !! colspan="5" | ''c''(''n''): color of integers
|-
! rowspan="2" | 0
| 1 || 2 || 3 || … || ''m''
|-
| &amp;nbsp;'''&lt;span style="color:yellow;"&gt;Y&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp; || … || &amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp;
|-
! rowspan="2" | 1
| ''m''+1 || ''m''+2 || ''m''+3 || … || ''2m''
|-
| &amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:yellow;"&gt;Y&lt;/span&gt;'''&amp;nbsp; || … || &amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp;
|-
! …
| colspan="5" | …
|-
! rowspan="2" | ''g''
| ''gm+1'' || ''gm+2'' || ''gm+3'' || … || ''(g+1)m''
|-
| &amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:red;"&gt;R&lt;/span&gt;'''&amp;nbsp; || &amp;nbsp;'''&lt;span style="color:blue;"&gt;B&lt;/span&gt;'''&amp;nbsp; || … || &amp;nbsp;'''&lt;span style="color:yellow;"&gt;Y&lt;/span&gt;'''&amp;nbsp;
|}
A similar argument can be advanced to show that ''W''(3, 3) ≤ 7(2·3&lt;sup&gt;7&lt;/sup&gt;+1)(2·3&lt;sup&gt;7·(2·3&lt;sup&gt;7&lt;/sup&gt;+1)&lt;/sup&gt;+1). One begins by dividing the integers into  2·3&lt;sup&gt;7·(2·3&lt;sup&gt;7&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;1)&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;1 groups of 7(2·3&lt;sup&gt;7&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;1) integers each; of the first 3&lt;sup&gt;7·(2·3&lt;sup&gt;7&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;1)&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;1 groups, two must be colored identically.

Divide each of these two groups into 2·3&lt;sup&gt;7&lt;/sup&gt;+1 subgroups of 7 integers each; of the first 3&lt;sup&gt;7&lt;/sup&gt;&amp;nbsp;+&amp;nbsp;1 subgroups in each group, two of the subgroups must be colored identically.  Within each of these identical subgroups, two of the first four integers must be the same color, say &lt;span style="color:red;"&gt;red&lt;/span&gt;; this implies either a &lt;span style="color:red;"&gt;red&lt;/span&gt; progression or an element of a different color, say &lt;span style="color:blue;"&gt;blue&lt;/span&gt;, in the same subgroup.

Since we have two identically-colored subgroups, there is a third subgroup, still in the same group that contains an element which, if either &lt;span style="color:red;"&gt;red&lt;/span&gt; or &lt;span style="color:blue;"&gt;blue&lt;/span&gt;, would complete a &lt;span style="color:red;"&gt;red&lt;/span&gt; or &lt;span style="color:blue;"&gt;blue&lt;/span&gt; progression, by a construction analogous to the one for ''W''(2, 3). Suppose that this element is &lt;span style="color:yellow;"&gt;yellow&lt;/span&gt;. Since there is a group that is colored identically, it must contain copies of the &lt;span style="color:red;"&gt;red&lt;/span&gt;, &lt;span style="color:blue;"&gt;blue&lt;/span&gt;, and &lt;span style="color:yellow;"&gt;yellow&lt;/span&gt; elements we have identified; we can now find a pair of &lt;span style="color:red;"&gt;red&lt;/span&gt; elements, a pair of &lt;span style="color:blue;"&gt;blue&lt;/span&gt; elements, and a pair of &lt;span style="color:yellow;"&gt;yellow&lt;/span&gt; elements that 'focus' on the same integer, so that whatever color it is, it must complete a progression.

=== Proof in general case ===
The proof for ''W''(2, 3) depends essentially on proving that ''W''(32, 2) ≤ 33.  We divide the integers {1,...,325} into 65 'blocks', each of which can be colored in 32 different ways, and then show that two blocks of the first 33 must be the same color, and there is a block colored the opposite way.  Similarly, the proof for ''W''(3, 3) depends on proving that

: &lt;math&gt;W(3^{7(2 \cdot 3^7+1)},2) \leq 3^{7(2 \cdot 3^7+1)}+1.&lt;/math&gt;

By a double [[mathematical induction|induction]] on the number of colors and the length of the progression, the theorem is proved in general.

== Proof ==

A [[Generalized arithmetic progression|''D-dimensional arithmetic progression'' (AP)]] consists of
numbers of the form:
: &lt;math&gt; a + i_1 s_1 + i_2 s_2 + \cdots + i_D s_D &lt;/math&gt;
where {{mvar|a}} is the basepoint, the {{mvar|s}}'s are positive step-sizes, and the {{mvar|i}}'s range from 0 to {{math|L-1}}. A {{mvar|d}}-dimensional AP is ''homogeneous'' for some coloring when it is all the same color.

A ''{{mvar|D}}-dimensional arithmetic progression with benefits'' is all numbers of the form above, but where you add on some of the "boundary" of the arithmetic progression, i.e. some of the indices {{mvar|i}}'s can be equal to {{mvar|L}}. The sides you tack on are ones where the first {{mvar|k}} {{mvar|i}}'s are equal to {{mvar|L}}, and the remaining {{mvar|i}}'s are less than {{mvar|L}}.

The boundaries of a {{mvar|D}}-dimensional AP with benefits are these additional arithmetic progressions of dimension &lt;math&gt;d-1, d-2, d-3, d-4&lt;/math&gt;, down to 0. The 0-dimensional arithmetic progression is the single point at index value &lt;math&gt;(L, L, L, L, \cdots, L)&lt;/math&gt;. A {{mvar|D}}-dimensional AP with benefits is ''homogeneous'' when each of the boundaries are individually homogeneous, but different boundaries do not have to necessarily have the same color.

Next define the quantity {{math|MinN(L, D, N)}} to be the least integer so
that any assignment of {{mvar|N}} colors to an interval of length {{math|MinN}} or more necessarily contains a homogeneous {{mvar|D}}-dimensional arithmetical progression with benefits.

The goal is to bound the size of {{math|MinN}}. Note that {{math|MinN(L,1,N)}} is an upper bound for Van der Waerden's number. There are two inductions steps, as follows:

{{Math_theorem|name=Lemma 1|math_statement=Assume {{math|MinN}} is known for a given lengths {{mvar|L}} for all dimensions of arithmetic progressions with benefits up to {{mvar|D}}. This formula gives a bound on {{math|MinN}} when you increase the dimension to {{math|D+1}}:

let &lt;math&gt; M = {\mathrm MinN}(L,D,n)&lt;/math&gt;, then

: &lt;math&gt; {\mathrm MinN}(L, D+1 , n) \le  M \cdot {\mathrm MinN}(L,1,n^M)&lt;/math&gt;
}}
{{Math_proof|First, if you have an {{mvar|n}}-coloring of the interval 1...{{mvar|I}}, you can define a ''block coloring'' of {{mvar|k}}-size blocks. Just consider each sequence of {{mvar|k}} colors in each {{mvar|k}} block to define a unique color. Call this ''{{mvar|k}}-blocking'' an {{mvar|n}}-coloring. {{mvar|k}}-blocking an {{mvar|n}} coloring of length {{mvar|l}} produces an {{math|n{{sup|k}}}} coloring of length {{math|l/k}}.

So given a {{mvar|n}}-coloring of an interval {{mvar|I}} of size &lt;math&gt;M \cdot MinN(L,1,n^M))&lt;/math&gt; you can {{mvar|M}}-block it into an {{math|n{{sup|M}}}} coloring of length &lt;math&gt;MinN(L,1,n^M)&lt;/math&gt;. But that means, by the definition of {{math|MinN}}, that you can find a 1-dimensional arithmetic sequence (with benefits) of length {{mvar|L}} in the block coloring, which is a sequence of blocks equally spaced, which are all the same block-color, i.e. you have a bunch of blocks of length {{mvar|M}} in the original sequence, which are equally spaced, which have exactly the same sequence of colors inside.

Now, by the definition of {{mvar|M}}, you can find a {{mvar|d}}-dimensional arithmetic sequence with benefits in any one of these blocks, and since all of the blocks have the same sequence of colors, the same {{mvar|d}}-dimensional AP with benefits appears in all of the blocks, just by translating it from block to block. This is the definition of a {{math|d+1}} dimensional arithmetic progression, so you have a homogeneous {{math|d+1}} dimensional AP. The new stride parameter {{math|s{{sub|D+1}}}} is defined to be the distance between the blocks.

But you need benefits. The boundaries you get now are all old boundaries, plus their translations into identically colored blocks, because {{math|i{{sub|D+1}}}} is always less than {{mvar|L}}. The only boundary which is not like this is the 0-dimensional point when &lt;math&gt;i_1=i_2=\cdots=i_{D+1}=L&lt;/math&gt;. This is a single point, and is automatically homogeneous.
}}
{{Math_theorem|name=Lemma 2|math_statement=Assume {{math|MinN}} is known for one value of {{mvar|L}} and all possible dimensions {{mvar|D}}. Then you can bound MinN for length {{math|L+1}}.

: &lt;math&gt;{\mathrm MinN}(L+1,1,n) \le 2{\mathrm MinN}(L,n,n)&lt;/math&gt;
}}
{{Math_proof|Given an {{mvar|n}}-coloring of an interval of size {{math|MinN(L,n,n)}}, by definition, you can find an arithmetic sequence with benefits of dimension {{mvar|n}} of length {{mvar|L}}. But now, the number of "benefit" boundaries is equal to the number of colors, so one of the homogeneous boundaries, say of dimension {{mvar|k}}, has to have the same color as another one of the homogeneous benefit boundaries, say the one of dimension {{math|p&lt;k}}. This allows a length {{math|L+1}} arithmetic sequence (of dimension 1) to be constructed, by going along a line inside the {{mvar|k}}-dimensional boundary which ends right on the {{mvar|p}}-dimensional boundary, and including the terminal point in the {{mvar|p}}-dimensional boundary. In formulas:

if
: &lt;math&gt; a+ L s_1 + L s_2 + \cdots + L s_{D-k}&lt;/math&gt; has the same color as
: &lt;math&gt; a + L s_1 + L s_2 + \cdots +L s_{D-p}&lt;/math&gt;
then
: &lt;math&gt; a + L \cdot (s_1 + \cdots +s_{D-k}) + u \cdot (s_{D-k+1} + \cdots +s_p) &lt;/math&gt; have the same color
: &lt;math&gt; u = 0,1,2,\cdots,L-1,L &lt;/math&gt; i.e. {{mvar|u}} makes a sequence of length {{mvar|L}}+1.

This constructs a sequence of dimension 1, and the "benefits" are automatic, just add on another point of whatever color. To include this boundary point, one has to make the interval longer by the maximum possible value of the stride, which is certainly less than the interval size. So doubling the interval size will definitely work, and this is the reason for the factor of two. This completes the induction on {{mvar|L}}.
}}
Base case: {{math|MinN(1,d,n){{=}}1}}, i.e. if you want a length 1 homogeneous {{mvar|d}}-dimensional arithmetic sequence, with or without benefits, you have nothing to do. So this forms the base of the induction. The Van der Waerden theorem itself is the assertion that {{math|MinN(L,1,N)}} is finite, and it follows from the base case and the induction steps.&lt;ref name="Graham1974" /&gt;

==See also==
* [[Van der Waerden number]]s for all known values for ''W''(''n'',''r'') and the best-known bounds for unknown values
*[[Van der waerden game]] - a game where the player pick integers from the set 1,...,N and try to collect an arithmetic progression of length n.

==Notes==
{{reflist}}

==References==
*{{Citation
  | last1 = Khinchin   | first1 = A. Ya.
  | title = Three Pearls of Number Theory
  | publisher = Dover
  | location = Mineola, NY
  | date = 1998
  | isbn = 978-0-486-40026-6
  | url = {{Google books|sqtlwPI-iWcC|Three Pearls of Number Theory|page=11|plainurl=yes}}
  | pages = 11–17}}

==External links==
*{{MathWorld|title=van der Waerden's Theorem|urlname=vanderWaerdensTheorem|author=[[Kevin O'Bryant|O'Bryant, Kevin]]}}
*{{MathWorld|title=Van der Waerden Number|urlname=vanderWaerdenNumber|author=O'Bryant, Kevin and [[Eric W. Weisstein|Weisstein, Eric W.]]}}

[[Category:Articles containing proofs]]
[[Category:Ramsey theory]]
[[Category:Theorems in discrete mathematics]]</text>
      <sha1>hcp7anaadhrao90yj8ynpbb5gxoq1j9</sha1>
    </revision>
  </page>
</mediawiki>
