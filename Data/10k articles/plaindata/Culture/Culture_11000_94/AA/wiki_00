{"id": "31607734", "url": "https://en.wikipedia.org/wiki?curid=31607734", "title": "Alden Shoe Company", "text": "Alden Shoe Company\n\nThe Alden Shoe Company is a shoe company founded in 1884 by Charles H. Alden in Middleborough, Massachusetts. Alden specializes in handcrafted men's leather boots and dress shoes, such as Oxfords, Blüchers, loafers, and Chukka boots.\n\nIn the 19th century, there were once hundreds of shoemakers in New England, but now Alden is one of only a few factories. Alden is considered a heritage, family-owned brand. Many of the company's roughly 100 workers at its factory in Middleborough are second or third generation, and it sources its leathers mostly from small tanneries in Europe and the U.S. – its shell cordovan comes from the last such tannery in America, Horween Leather Company. Alden has used Horween as their leather supplier since 1930, and is their largest cordovan customer.\n\nAlong with other brands of Americana, Alden has experienced something of a resurgence in 21st century men's fashion. Despite a recession in the late-2000s and the relatively high prices of their products, Alden has grown again because of a renewed interest in more traditional men's shoes and boots, which can last decades. It is this reliance on high-end shoes, especially by those interested in business attire, that has allowed the company to avoid going under despite the decline of American shoe manufacturing.\n\nIn popular culture, Alden model 405 boots (commonly referred to as the Alden \"Indy\" boot) were the shoe of choice for Harrison Ford's Indiana Jones character in the film versions of the franchise.\n\n"}
{"id": "12769533", "url": "https://en.wikipedia.org/wiki?curid=12769533", "title": "Arte Público Press", "text": "Arte Público Press\n\nArte Público Press, in Houston, Texas, is the largest US publisher of contemporary and recovered literature by US Hispanic authors, part of the University of Houston. It publishes approximately 30 titles per year.\n\nArte Público was founded in 1979 by its current director, Nicolás Kanellos, Ph.D. Dr. Kanellos also founded and edited the \"Revista Chicana-Riqueña\" from 1972 to 1999. In 1980, Arte Público became a part of the University of Houston, where it is housed today.\n\nIn 1992, Arte Público launched the \"Recovering the U.S. Hispanic Literary Heritage\" project in order to recover, index and publish lost Latino writings dating from the American colonial period to 1960. In 1994, they created Piñata Books, their children's and young adult literature imprint.\n\nArte Público has published Lamberto Alvarez Victor Villaseñor, Nicholasa Mohr, Luis Valdez, Miguel Piñero, Sandra Cisneros, Julia Alvarez, Helena Maria Viramontes, Sergio Troncoso, Miguel Algarín, Graciela Limón, Gwendolyn Zepeda, Daniel Olivas, Daniel Chacón, Pat Mora, and José Ángel Gutiérrez.\n\n"}
{"id": "55867971", "url": "https://en.wikipedia.org/wiki?curid=55867971", "title": "Being a Man Festival", "text": "Being a Man Festival\n\nBeing a Man Festival (BAM) is a UK-based festival which addresses the challenges and pressures of masculine identity in the 21st century. The festival was founded in 2014 by Jude Kelly.\n\nSpeakers at the inaugural festival included: Grayson Perry (artist), Ziauddin Yousafzai (father of the Nobel laureate Malala Yousafzai), Michael Kaufman (co-founder of the White Ribbon Campaign) Jon Snow (journalist), Billy Bragg (singer), Nick Hornby (writer and lyricist), Charlie Condou (actor) and Hardeep Singh Kohli (broadcaster and writer).\n\nSpeakers at the second festival included: Sheldon Thomas (imam and former extremist), Gemma Cairney (BBC Radio 1 presenter and documentary filmmaker), Akala (rapper) and Frankie Boyle (comedian).\n\nSpeakers at the 2016 festival included: Professor Green (rapper) and Roger Moore.\n\nSpeakers included: Simon Amstell (comedian), Robert Webb (comedian), Kevin Powell (American political activist), Alan Hollinghurst (writer and winner of the 2004 Booker Prize) and Antonythasan Jesuthasan (author and actor).\n\n\n"}
{"id": "4775999", "url": "https://en.wikipedia.org/wiki?curid=4775999", "title": "BioSteel", "text": "BioSteel\n\nBioSteel was a trademark name for a high-strength fiber-based material made of the recombinant spider silk-like protein extracted from the milk of transgenic goats, made by Montreal-based company Nexia Biotechnologies, and later by the Randy Lewis lab of the University of Wyoming and Utah State University. It is reportedly 7-10 times as strong as steel if compared for the same weight, and can stretch up to 20 times its unaltered size without losing its strength properties. It also has very high resistance to extreme temperatures, not losing any of its properties within .\n\nThe company had created lines of goats to produce recombinant versions of either the MaSpI (Major ampullate spidroin I) or dragline I (for its superior elasticity, flexibility and strength) from \"Nephila clavipes\", the golden orb weaver) or MaSpII (Major ampullate spidroin 2 or dragline 2 from \"Nephila clavipes\") dragline proteins in their milk. When the female goats lactate, the milk, containing the recombinant DNA silk, was to be harvested and subjected to chromatographic techniques to purify the recombinant silk proteins.\n\nThe purified silk proteins could be dried, dissolved using solvents (DOPE formation) and transformed into microfibers using wet-spinning fiber production methods. The spun fibers were reported to have tenacities in the range of 2 - 3 grams/denier and elongation range of 25-45%. The \"Biosteel biopolymer\" had been transformed into nanofibers and nanomeshes using the electrospinning technique.\n\nNexia is the only company which has successfully produced fibres from spider silk expressed in goat's milk. The Lewis lab has produced fibers from recombinant spider silk protein and synthetic spider silk proteins and genetic chimeras produced in both recombinant E. coli and the milk of recombinant goats, however, no one has been able to produce the silk in commercial quantities thus far. The company was founded in 1993 by Dr. Jeffrey Turner and Paul Ballard, and was sold in 2005 to Pharmathene.\n\nIn 2018, two transgenic goats were sold to the Canada Agriculture Museum after Nexia Biotechnologies went bankrupt.\n\nResearch has since continued with the help of Randy Lewis, a professor formerly at the University of Wyoming and now at Utah State University. He was also able to successfully breed monkey goats in order to create artificial silk. There are now about 30 spider goats at a university-run farm.\n\nApplications of artificial spider silk biopolymers include using it for the coating of all kinds of implants and medical products as well as for artificial ligaments and tendons due to its elastic tendencies and also since it is a natural product which will synthesize well with the body. Furthermore, artificial silk biopolymers can be applicated in personal care products as well as in textile products.\n"}
{"id": "24189956", "url": "https://en.wikipedia.org/wiki?curid=24189956", "title": "Careers advisor", "text": "Careers advisor\n\nA careers advisor gives information about education and work opportunities to children and youths. Careers advisors give information about opportunities, as well as helping with competencies and help with CVs and application forms.\n\nCareer advising in the US began in the early 20th century, before World War I, when the main need was to help farmworkers transition into industrial jobs.\n\n"}
{"id": "3337426", "url": "https://en.wikipedia.org/wiki?curid=3337426", "title": "Centro de Educación Artística", "text": "Centro de Educación Artística\n\nThe Centro de Educación Artística (CEA) is an entertainment educational institution in Mexico run by that country's major media conglomerate, Televisa. Located at the company's facilities in the San Ángel neighborhood of Mexico City, the school was founded September 26, 1978 and is an incubator for talent working for the network's famed telenovelas, dramas and comedies, along with the Mexican film industry and beyond.\n\nApplicants must be between 17 and 23 years of age, with a very selective process, as only 35 to 40 applicants out of around 5,000 per year are accepted to the three-year program. Tuition for the program is covered by Televisa, and consists of 45 hours per week of coursework and performances. A secondary program is available for younger actors, known as \"CEA Infantil\".\n\n"}
{"id": "25304413", "url": "https://en.wikipedia.org/wiki?curid=25304413", "title": "Clothing in the Ragtime Era", "text": "Clothing in the Ragtime Era\n\nOften, when people think of the ragtime era, they think of the music that defined the generation, and although that is certainly a major factor, the era also helped to define a culture in clothing as well.\n\nBecause the ragtime era began in the late 19th century and transitioned into the early 20th century (approximately 1897-1918), it also took place during what is known as \"the turn of the century\". This was also one of the first observations of class divisions. The difference between upper class and lower class citizens could clearly be seen. Much of this had to do with their clothing and how they carried themselves in public. \n\nThe upper class could afford to dress well. Women often appeared in long white gowns, which were in fashion at the time. On these dresses were white collars and usually a brooch that kept the collar closed. Women often wore large white hats and carried parasols as accessories. Bustles were commonplace for women as well. Men often wore light, form-fitting suits and wore bow ties as accessories. It was not rare to see men wearing handlebar mustaches.\n\nWhereas the upper class were put together and proper, and everything was crisp, clean, and uniformed, the immigrant population was quite the opposite. This was the very beginning of immigrants being allowed to enter the United States via Ellis Island. Because they were able to carry very little over to the New World, they had little clothing and little money to buy more when their clothes started to wear out. They would therefore mismatch their clothing or reuse the fabric from an old article of clothing so that they could get multiple uses out of each garment.\n\nMany African Americans enjoyed the popular music from the ragtime era. Their sense of style in clothing seemed to reflect this. Whereas the mostly Caucasian American upper class dressed in clean and light colored fabric (often cream or white), the African Americans dressed in bright and vibrant clothing when they attended clubs that played ragtime music. Whereas the upper class clothing was form-fitting, their clothes were loose, which allowed for free and easy movement. Women often adorned their hair with colorful flowers as accessories, and men often wore vests and bowler hats as their accessories.\n"}
{"id": "28964679", "url": "https://en.wikipedia.org/wiki?curid=28964679", "title": "Collaborative e-democracy", "text": "Collaborative e-democracy\n\nCollaborative e-democracy or super-democracy is a democratic conception that combines key features of direct democracy, representative democracy, and e-democracy (i.e. the use of ICTs for democratic processes). The concept was first published at two international academic conferences in 2009 (see below).\n\nCollaborative e-democracy refers to a political system in which governmental stakeholders (politicians/parties, ministers, parliamentarians etc.) and non-governmental stakeholders (NGOs, political lobbies, local communities, individual citizens, etc.) collaborate on the development of public laws and policies. This collaborative policymaking process is conducted on a governmental social networking site in which all citizens are members (collaborative e-policy-making).\n\nWhile directly elected government officials (i.e. ‘proxy representatives’) would conduct the vast majority of law and policy-making processes (representative democracy), the citizens would retain their final voting power on each issue (direct democracy). Additionally each citizen would be empowered to propose their own policies to the electorate and thus initiate new policy processes where applicable (initiative). Collaboratively generated policies would consider the opinion of a larger proportion of the citizenry; therefore they may be more just, more sustainable, and thus easier to implement.\n\nCollaborative e-democracy involves following theory components:\n\nCollaborative e-policymaking is a process where public laws & policies are generated in collaboration of multiple stakeholders (e.g. affected people; domain experts; parties who can help to implement a solution). Each new policy cycle starts with the identification of a collective problem or goal by the collective of participants (i.e. citizens, experts, proxy representatives).\n\nTo be clear, CPM is automated as a software process that is conducted on the governmental social networking site.\n\nCollaborative e-democracy is based on following core principles:\n\nThe concept of collaborative e-democracy intends to achieve following benefits:\n\nOn the contrary the concept has several limitations:\n\nIn 2009 the two conceptions, \"collaborative e-democracy\" and \"collaborative e-policy-making\", were first published at two academic conferences on e-governance and e-democracy:\n\nCurrent research is conducted at the Queensland University of Technology in Brisbane, Australia where a prototype software for collaborative e-policymaking is being developed. The software will be featured on an independent web 2.0 platform with open participation; the volunteer participants will be facilitated to collaboratively develop Australian public laws and policies by proceeding through the phases of the CPM process. The start of the trialling period is probably 2011.\n\n\n"}
{"id": "54250364", "url": "https://en.wikipedia.org/wiki?curid=54250364", "title": "Cover Your Tracks (band)", "text": "Cover Your Tracks (band)\n\nCover Your Tracks is an American metalcore band from Atlanta, Georgia.\n\n\n"}
{"id": "14898898", "url": "https://en.wikipedia.org/wiki?curid=14898898", "title": "Cow blowing", "text": "Cow blowing\n\nCow blowing, Kuhblasen, phooka, or doom dev, is a process used in many countries according to ethnographers, in which forceful blowing of air into a cow's vagina (or sometimes anus) is applied to induce her to produce more milk.\n\nCow blowing was the reason why Gandhi abjured cow milk, saying that \"since I had come to know that the cow and the buffalo were subjected to the process of phooka, I had conceived a strong disgust for milk.\"\n\n\n"}
{"id": "56043336", "url": "https://en.wikipedia.org/wiki?curid=56043336", "title": "Diane Tuft", "text": "Diane Tuft\n\nDiane Tuft (born 1947) is an American photographer focusing on nature and landscape photography, documenting the effects of the environment on the Earth's landscape. She is based in New York City.\n\nTuft was born and raised in East Hartford, Connecticut. She graduated from the University of Connecticut with a degree in mathematics. After graduating, she moved to New York City to work as an actuarial assistant. She later held jobs with the Burroughs Corporation and Computer Design Corporation. During this time, she studied photography at The New School and the International Center of Photography. Tuft married in 1971 and while raising a family studied art at Pratt Institute in Brooklyn from 1981 to 1989.\n\nTuft began her work in 1998 with images of snow and ice in Aspen, Colorado. There she first experimented with infrared film, where the photos could capture the infrared light waves that were reflected and refracted on the landscape, which are beyond the human visible spectrum. These photographs would become platinum prints, and resulted in her first solo exhibition, \"Distillations\", at Hollis Taggart Galleries in New York City in 1999. She continued to photograph outside the visible spectrum, focusing on the visual effects of ultraviolet light waves on the Earth's landscape. Tuft began traveling to ozone-depleted areas where larger amounts of ultraviolet light reach the Earth. This led to an interest in climate change and other environmental issues. She began photographing the Arctic landscape in 2001, and has said that an aim of her work is to demonstrate the realities of global warming and its effect on the Earth. She often documents icy landscapes through aerial photography in order to capture \"the sculptural qualities of frozen water.\" She typically zooms in to the landscape to the point of abstraction, framing shots without a sense of scale. In 2008, Tuft published her first monograph, \"Unseen: Beyond the Visible Spectrum\", a retrospective of her photographs between the years 1998 and 2007, featuring the American West, Nepal, North Africa, Iceland and Greenland. The foreword was written by William Fox, director of the Center for Art and Environment at the Nevada Museum of Art.\n\nIn 2006, Tuft created a room-size installation, \"Internal Reflection\", which combined sculpture, light, sound and photography. It was exhibited at the Katonah Museum of Art in New York City and at Art LA in Santa Monica, California. Her 2008 series \"Salt Lake Reconsidered\", exhibited at Ameringer & Yohe Fine Art in New York City and the Kimball Art Center in Park City, Utah, featured aerial photographs of the Great Salt Lake. Tuft's photographic series \"Icelandic Glaciers\" in 2001 and \"Icelandic Sagas\" in 2008 document the change of Iceland's glaciers due to climate change. In 2010, Tuft revisited Iceland, creating her series \"Aftermath\", a collection of aerial photographs of the center of Iceland and Eyjafjallajökull's eruption.\n\nTuft received a 2012 National Science Foundation grant to explore the visual effects of ozone depletion on Antarctica's landscape. In October 2012, she traveled to Antarctica, living at McMurdo Station for six weeks. The resulting images collected in her 2014 book \"Gondwana: Images of an Ancient Land\" focused on the effects that the harsh environment of Antarctica had on shaping its landscape. These images include the meromictic lakes in the dry valleys of Antarctica, where millions of years of gasses have been trapped in the ice, volcanic gas formations, glacial striations that record millions of years of snow accumulation, and ventifacts formed by ongoing intense winds. The book's foreword was written by Elisabeth Sussman, curator of photography at the Whitney Museum of American Art.\n\nDuring the summers of 2015 and 2016, Tuft explored the Arctic to document the severe melt that was occurring throughout the region. Her journey included the mountain glaciers and surrounding waters of Svalbard, Norway, the sea ice in the Arctic Ocean including the North Pole, and the icebergs and ice sheet of Greenland. Tuft's series, \"The Arctic Melt: Images of a Disappearing Landscape\", has resulted in several exhibitions worldwide, as well as a three-minute film and book. Climate scientist Joe Romm wrote the book's introduction. The film was presented on Earth Day at the March for Science at the National Mall in Washington, D.C., and \"The Arctic Melt\" exhibition at Marlborough Gallery was nominated for a Global Fine Art Award in the Global Planet category in October 2017.\n\nTuft has work included in the permanent collections of the Whitney Museum of American Art in Manhattan, International Center of Photography in Manhattan, and the Parrish Art Museum in Water Mill, New York.\n\nTuft lives with her husband Tom Tuft in New York City. They have three children.\n\n\n"}
{"id": "39120509", "url": "https://en.wikipedia.org/wiki?curid=39120509", "title": "Epigenetic theories of homosexuality", "text": "Epigenetic theories of homosexuality\n\nEpigenetic theories of homosexuality concern the studies of changes in gene expression or cellular phenotype caused by mechanisms other than changes in the underlying DNA sequence, and their role in the development of homosexuality.\nEpigenetics examines the set of chemical reactions that switch parts of the genome on and off at strategic times and locations in the organism's life cycle. However, epigenetic theories tangle a multiplicity of initiating causes and of resulting final effects and will never lead to a single cause or a single result. Hence, any interpretation of such theories may not focus just one isolated reason of a multiplicity of causes or of effects.\n\nInstead of affecting the organism's DNA sequence, non-genetic factors may cause the organism’s genes to express themselves differently. DNA in the human body is wrapped around histones, which are proteins that package and order DNA into structural units. DNA and histone are covered with chemical tags known as the epigenome, which shapes the physical structure of the genome. It tightly wraps inactive genes on the DNA sequence making those genes unreadable while loosely wrapping active genes making them more expressive. The more tightly wrapped the gene, the less it will be expressed in the organism. These epigenetic tags react to stimuli presented from the outside world. It adjusts specific genes in the genome to respond to humans' rapidly changing environments. \nThe idea of epigenetics and gene expression has been a theory applied to the origins of homosexuality in humans. One team of researchers examined the effects of epi-marks buffering XX fetuses and XY fetuses from certain androgen exposure and used published data on fetal androgen signaling and gene regulation through non-genetic changes in DNA packaging to develop a new model for homosexuality. The researchers found that stronger than average epi-marks, epigenomes that are wrapped tightly around the DNA sequence, convert sexual preference in individuals without altering genitalia or sexual identity.\nThis research gives support to the hypothesis that homosexuality stems from the under expression of certain genes on the DNA sequence involved with sexual preferences. This theory as well as other concepts involved with epi-marks, twin studies, and fetal androgen signaling will be explored here.\n\nEpigenetic marks (epi-marks) are temporary \"switches\" that control how our genes are expressed during gestation and after birth. Moreover, epi-marks are modifications of histone proteins. Epigenetic marks are modifications of the methyl and acetyl groups that bind to DNA histones thereby changing how the proteins function and as a result, alter gene expression. Epi-marks change how the histones function and as a result, influence the way genes are expressed. Epigenetic marks promote normal sexual development during fetal development. However, they can be passed on to offspring through the process of mitosis. When they are transferred from one parent to an offspring of the opposite sex, it can contribute to an altered sexual development, thus leading to masculinization of female offspring and feminization of male offspring. However, these epi-marks hold no consistency between individuals in regard to strength and variability.\n\nIdentical twins have identical DNA, which leads to the perceived conclusion that all identical twins are either heterosexual or homosexual. However, it is evident that this is not the case, consequently leaving a gap in the explanation for homosexuality. A \"gay\" gene does not produce homosexuality. Rather, epigenetic modifications act as temporary \"switches\" that regulate how the genes are expressed. Of the pairs of identical twins in which one twin is homosexual, only twenty percent of the other twins are homosexual, which leads to the hypothesis that even though identical twins share the same DNA, homosexuality is created by something else rather than the genes. Epigenetic transformation allows the on and off switching of certain genes, subsequently shaping how cells respond to androgen signaling, which is critical in sexual development.\nAnother example of epigenetic consequences is evident in multiple sclerosis in monozygotic (identical) twins. There are pairs of twins that are discordant with multiple sclerosis and do not both show the trait. After gene testing, it was suggested that DNA was identical and that epigenetic differences contributed to the gene difference between identical twins.\n\nWhile in the fetal stages, hormonal influences of androgen, specifically testosterone, cause feminine qualities in regard to sexual development in females and masculine qualities in males. In typical sexual development, females are exposed to minimal amounts of testosterone, thus feminizing their sexual development, while males are typically exposed to high levels of testosterone, which masculinize their development. Epi-marks play a critical role in this development by acting as a buffer between the fetus and androgen exposure. Moreover, they predominantly protect XY fetuses from androgen underexposure while protecting XX fetuses from androgen overexposure. However, when androgen overexposure happens in XX fetuses, research suggests they can show masculinized behavior in comparison to females who undergo normal levels of androgen exposure. The research also suggests that excess androgen exposure in females led to reduced heterosexual interest in adulthood than did females with normal levels of androgen.\n\nNew epi-marks are usually produced with each generation, but these marks sometimes carry over between generations. Sex-specific epi-marks are produced in early fetal development that protect each sex from the natural disparity in testosterone that occurs during later stages of fetal development. Different epi-marks protect different sex-specific traits from being masculinized or feminized—some affect the genitals, others affect sexual identity, and yet others affect sexual preference. However, when these epi-marks are transmitted across generations from fathers to daughters or mothers to sons, they may cause reversed effects, such as the feminization of some traits in sons and similarly a partial masculinization of daughters. Furthermore, the reversed effects of feminization and masculinization can lead to a reversed sexual preference. For example, sex-specific epi-marks normally prevent female fetuses from being masculinized through exposure of atypically high testosterone, and vice versa for male fetuses. Sex-specific epi-marks are normally erased and not passed between generations. However, they can sometimes escape erasure and are then transferred from a father's genes to a daughter or from a mother's genes to a son. When this happens, this may lead to an altered sexual preference. Epi-marks normally protect parents from variation in sex hormone levels during fetal development, but can carry over across generations and subsequently lead to homosexuality in opposite-sex offspring. This demonstrates that gene coding for these epi-marks can spread in the population because they benefit the development and fitness of the parent but only rarely escape erasure, leading to same-sex sexual preference in offspring.\n"}
{"id": "8752", "url": "https://en.wikipedia.org/wiki?curid=8752", "title": "Flag of Denmark", "text": "Flag of Denmark\n\nThe flag of Denmark (, ) is red with a white Scandinavian cross that extends to the edges of the flag; the vertical part of the cross is shifted to the hoist side.\n\nA banner with a white-on-red cross is attested as having been used by the kings of Denmark since the 14th century.\nAn origin legend with considerable impact on Danish national historiography connects the introduction of the flag to the Battle of Lindanise of 1219. \nThe elongated Nordic cross reflects the use as maritime flag in the 18th century.\nThe flag became popular as national flag in the early 19th century. Its private use was outlawed in 1834, and again permitted in a regulation of 1854. The flag holds the world record of being the oldest continuously used national flag.\n\nA red field charged with a white cross extending to the edges; the vertical part of the cross is shifted to the hoist side. \nIn 1748, a regulation defined the correct lengths of the two last fields in the flag as .\nIn May 1893 a new regulation to all chiefs of police, stated that the police should not intervene, if the two last fields in the flag were longer than as long as these did not exceed , and provided that this was the only rule violated.\nThis regulation is still in effect today and thus the legal proportions of the National flag is today 3:1:3 in width and anywhere between 3:1:4.5 and 3:1:5.25 in length.\n\nNo official nuance definition of \"Dannebrog rød\" exists. The private company \"Dansk Standard\", regulation number 359 (2005), defines the red colour of the flag as Pantone 186c.\n\nThe white-on-red cross emblem originates in the age of the Crusades. In the 12th century, it was also used as war flag by the Holy Roman Empire.\n\nIn the \"Gelre Armorial\", dated 1340–1370, such a banner is shown alongside the coat of arms of the king of Denmark. (\"The King of Denmark\"). This is the earliest known undisputed colour rendering of the Dannebrog.\nAt about the same time, Valdemar IV of Denmark displays a cross in his coat of arms on his \"Danælog\" seal (\"Rettertingsseglet\", dated 1356).\nThe image from the Armorial Gelre is nearly identical to an image found in a 15th-century coats of arms book now located in the National Archives of Sweden, (\"Riksarkivet\")\nThe seal of Eric of Pomerania (1398) as king of the Kalmar union displays the arms of Denmark chief dexter, three lions. In this version, the lions are holding a Dannebrog banner.\n\nThe reason why the kings of Denmark in the 14th century begin displaying the cross banner in their coats of arms is unknown. \nCaspar Paludan-Müller (1873) suggested that it may reflect a banner sent by the pope to the Danish king in support of the Baltic countries.\nAdolf Ditlev Jørgensen (1875) identifies the banner as that of the Knights Hospitaller, which order had a presence in Denmark from the later 12th century.\n\nSeveral coins, seals and images exist, both foreign and domestic, from the 13th to 15th centuries and even earlier, showing heraldic designs similar to Dannebrog, alongside the royal coat of arms (three blue lions on a golden shield.)\n\nThere is a record suggesting that the Danish army had a \"chief banner\" (\"hoffuitbanner\") in the early 16th century.\nSuch a banner is mentioned in 1570 by Niels Hemmingsøn in the context of a 1520 battle between Danes and Swedes near Uppsala as nearly captured by the Swedes but saved by the heroic actions of the banner-carrier Mogens Gyldenstierne and Peder Skram.\nThe legend attributing the miraculous origin of the flag to the campaigns of Valdemar II of Denmark (r. 1202–1241) were recorded by Christiern Pedersen and Petrus Olai in the 1520s.\n\nHans Svaning's \"History of King Hans\" from 1558–1559 and Johan Rantzau's \"History about the Last Dithmarschen War\", from 1569, record the further fate of the Danish \"hoffuitbanner\":\nAccording to this tradition, the original flag from the Battle of Lindanise was used in the small campaign of 1500 when King Hans tried to conquer Dithmarschen (in western Holstein in north Germany). The flag was lost in a devastating defeat at the Battle of Hemmingstedt on 17 February 1500. \nIn 1559, King Frederik II recaptured it during his own Dithmarschen campaign.\nIn 1576, the son of \"Johan Rantzau\", Henrik Rantzau, also writes about the war and the fate of the flag. He notes that the flag was in a poor condition when returned. \nContemporary records describing the battle of Hemmingstedt make no reference to the loss of the original Dannebrog, although the capitulation state that all Danish banners lost in 1500 were to be returned.\nIn a letter dated 22 February 1500 to Oluf Stigsøn, King John describes the battle, but does not mention the loss of an important flag. In fact, the entire letter gives the impression that the lost battle was of limited importance.\nIn 1598, Neocorus wrote that the banner captured in 1500 was brought to the church in Wöhrden and hung there for the next 59 years, until it was returned to the Danes as part of the peace settlement in 1559.\nHenrik Rantzau in 1576 records that the flag after its return to Denmark was placed in the cathedral in Slesvig. Slesvig historian Ulrik Petersen (1656–1735) confirms the presence of such a banner in the cathedral in the early 17th century, and records that it had crumbled away by about 1660.\n\nThe size and shape of the civil ensign (\"\"Koffardiflaget\") for merchant ships is given in the regulation of 11 June 1748, which says: \"A red flag with a white cross with no split end. The white cross must be of the flag's height. The two first fields must be square in form and the two outer fields must be lengths of those\".\nThe proportions are thus: 3:1:3 vertically and 3:1:4.5 horizontally. This definition are the absolute proportions for the Danish national flag to this day, for both the civil version of the flag (\"Stutflaget\"), as well as the merchant flag (\"Handelsflaget\"\"). Both flags are identical.\n\nA regulation passed in 1758 required Danish ships sailing in the Mediterranean to carry the \nroyal cypher in the center of the flag in order to distinguish them from Maltese ships, due to the similarity of the flag of the Sovereign Military Order of Malta.\n\nAccording to the regulation of 11 June 1748 the colour was simply red, which is common known today as \"Dannebrog rød\" (\"\"Dannebrog red\"). The only available red fabric dye in 1748 was made of madder root, which can be processed to produce a brilliant red dye (used historically for British soldiers' jackets). \nA regulation of 4 May 1927 once again states that Danish merchant ships have to fly flags according to the regulation of 1748.\n\nThe first regulation regarding the \"Splitflag\" dates from 27 March 1630, in which King Christian IV orders that Norwegian \"Defensionskibe\" (armed merchants ships) may only use the \"Splitflag\" if they are in Danish war service.\nIn 1685 an order, distributed to a number of cities in Slesvig, states that all ships must carry the Danish flag, and in 1690 all merchant ships are forbidden to use the \"Splitflag\", with the exception of ships sailing in the East Indies, West Indies and along the coast of Africa.\nIn 1741 it is confirmed that the regulation of 1690 is still very much in effect; that merchant ships may not use the \"Splitflag\". At the same time the Danish East India Company is allowed to fly the \"Splitflag\" when past the equator.\n\nSome confusion must have existed regarding the \"Splitflag\". In 1696 the Admiralty presented the King with a proposal for a standard regulating both size and shape of the \"Splitflag\". In the same year a royal resolution defines the proportions of the \"Splitflag\", which in this resolution is called \"Kongeflaget\" (the King's flag), as follows: \"The cross must be of the flags height. The two first fields must be square in form with the sides three times the cross width. The two outer fields are rectangular and the length of the square fields. The tails are the length of the flag\".\n\nThese numbers are the basic for the \"Splitflag\", or \"Orlogsflag\", today, though the numbers have been slightly altered. The term \"Orlogsflag\" dates from 1806 and denotes use in the Danish Navy.\n\nFrom about 1750 to early 19th century a number of ships and companies which the government has interests in, received approval to use the \"Splitflag\".\n\nIn the royal resolution of 25 October 1939 for the Danish Navy, it is stated that the \"Orlogsflag\" is a \"Splitflag\" with a deep red (\"dybrød\") or madder red (\"kraprød\"\") colour. Like the National flag, no nuance is given, but in modern days this is given as 195U.\nFurthermore, the size and shape is corrected in this resolution to be: \"The cross must be of the flag's height. The two first fields must be square in form with the height of of the flag's height. The two outer fields are rectangular and the length of the square fields. The tails are the length of the rectangular fields\".\nThus, if compared to the standard of 1696, both the rectangular fields and the tails have decreased in size.\n\nUsed as maritime flag since the 16th century, the Dannebrog was introduced as regimental flag in the Danish army in 1785, and\nfor the militia (landeværn) in 1801. From 1842, it was used as the flag of the entire army.\n\nIn parallel to the development of Romantic nationalism in other European countries, the military flag increasingly came to be seen as representing the nation itself during the first half of the 19th century. Poems of this period invoking the \"Dannebrog\" were written by B.S. Ingemann, N.F.S. Grundtvig, Oehlenschläger, Chr. Winther and H.C. Andersen.\nBy the 1830s, the military flag had become so popular as unofficial national flag, and its use by private citizens was outlawed in a circular enacted on 7 January 1834. In the national enthusiasm sparked by the First Schleswig War during 1848–1850, the flag was still very widely displayed, and the prohibition of private use was again repealed in a regulation of 7 July 1854, for the first time allowing Danish citizens to display the Dannebrog (but not the swallow-tailed \"Splitflag\" variant).\nSpecial permission to use the \"Splitflag\" was given to individual institutions and private companies, especially after 1870.\nOn 10 April 1915, the hoisting of any \"other\" flag on Danish soil was prohibited.\nIn 1886, the war ministry introduced a regulation indicating that the flag should be flown from military buildings on thirteen specified days, including royal birthdays, the date of the signing of the Constitution of 5 June 1849 and on days of remembrance for military battles. \nIn 1913, the naval ministry issued its own list of flag days. From 1939, the yearbook \"Hvem-Hvad-Hvor\" included a list of flag days.\n\nA tradition recorded in the 16th century traces the origin of the flag to the campaigns of \nValdemar II of Denmark (r. 1202–1241).\nThe oldest of them is in Christiern Pedersen's \"\"Danske Krønike\", which is a sequel to Saxo’s Gesta Danorum, written 1520–23. Here, the flag falls from the sky during a Russian campaign of Valdemar's. Pedersen also states that the very same flag was taken into exile by Eric of Pomerania in 1440.\n\nThe second source is the writing of the Franciscan monk Petrus Olai (Peder Olsen) of Roskilde (died c. 1570). This record describes a battle in 1208 near a place called \"Felin\"\" during the Estonia campaign of King Valdemar II. The Danes were all but defeated when a lamb-skin banner depicting a white cross fell from the sky and miraculously led to a Danish victory. \nIn a third account, also by Petrus Olai, in \"Danmarks Tolv Herligheder\" (\"Twelve Splendours of Denmark\"), in splendour number nine, the same story is re-told almost verbatim, with a paragraph inserted correcting the year to 1219.\nNow, the flag is falling from the sky in the Battle of Lindanise, also known as the Battle of Valdemar (Danish: \"Volmerslaget\"), near Lindanise (Tallinn) in Estonia, of 15 June 1219.\n\nIt is this third account that has been the most influential, and some historians have treated it as the primary account taken from a (lost) source dating to the first half of the 15th century.\n\nIn Olai's account, the battle was going badly, and defeat seemed imminent. However the Danish Bishop Anders Sunesen on top of a hill overlooking the battle prayed to God with his arms raised, which meant that the Danes moved closer to victory the more he prayed. When he raised his arms the Danes surged forward and when his arms grew tired and he let them fall, the Estonians turned the Danes back. Attendants rushed forward to raise his arms once again and the Danes surged forward again. At a second he was so tired in his arms that he dropped them and the Danes then lost the advantage and were moving closer to defeat. He needed two soldiers to keep his hands up and when the Danes were about to lose, 'Dannebrog' miraculously fell from the sky and the King took it, showed it to the troops and their hearts were filled with courage and the Danes won the battle.\n\nThe possible historical nucleus behind this origin legend was extensively discussed by Danish historians in the 19th to 20th centuries.\nJørgensen (1875) argues that Bishop Theoderich was the original instigator of the 1218 inquiry from Bishop Albert of Buxhoeveden to King Valdemar II which led to the Danish participation in the Baltic crusades.\nJørgensen speculates that Bishop Theoderich might have carried the Knight Hospitaller's banner in the 1219 battle and that \"the enemy thought this was the King's symbol and mistakenly stormed Bishop Theoderich tent. He claims that the origin of the legend of the falling flag comes from this confusion in the battle.\" \nThe Danish church-historian L. P. Fabricius (1934) ascribes the origin to the 1208 Battle of Fellin, not the Battle of Lindanise in 1219, based on the earliest source available about the story.\nFabricius speculated that it might have been Archbishop Andreas Sunesøn's personal ecclesiastical banner or perhaps even the flag of Archbishop Absalon, under whose initiative and supervision several smaller crusades had already been conducted in Estonia. The banner would then already be known in Estonia. \nFabricius repeats Jørgensen's idea about the flag being planted in front of Bishop Theodorik's tent, which the enemy mistakenly attacks believing it to be the tent of the King.\nA different theory is briefly discussed by Fabricius and elaborated more by Helge Bruhn (1949). Bruhn interprets the story in the context of the widespread tradition of the miraculous appearance of crosses in the sky in Christian legend, specifically comparing such an event attributed to a battle of 10 September 1217 near Alcazar, where it is said that a golden cross on white appeared in the sky, to bring victory to the Christians.\n\nIn Swedish national historiography of the 18th century, there is a tale paralleling the Danish legend, in which \na golden cross appears in the blue sky during a Swedish battle in Finland in 1157.\n\nThe \"Splitflag\" and \"Orlogsflag\" have similar shapes but different sizes and shades of red. Legally, they are two different flags. The \"Splitflag\" is a Danish flag ending in a swallow-tail, it is \"Dannebrog red\", and is used on land. The \"Orlogsflag\" is an elongated \"Splitflag\" with a deeper red colour and is only used on sea.\nThe \"Orlogsflag\" with no markings, may only be used by the Royal Danish Navy. There are though a few exceptions to this. A few institutions have been allowed to fly the clean \"Orlogsflag\". The same flag with markings has been approved for a few dozen companies and institutions over the years.\n\nFurthermore, the \"Orlogsflag\" is only described as such if it has no additional markings. Any swallow-tail flag, no matter the color, is called a \"Splitflag\" provided it bears additional markings.\n\nThe current version of the royal standard was introduced on 16 November 1972 when the Queen adopted a new version of her personal coat of arms. The royal standard is the flag of Denmark with a swallow-tail and charged with the monarch’s coat of arms set in a white square. The centre square is 32 parts in a flag with the ratio 56:107.\nGreenland and the Faroe Islands are additional autonomous countries within the Kingdom of Denmark. These countries have their own official flags.\n\nSome areas in Denmark have unofficial flags, listed below. The regional flags of Bornholm and Ærø are known to be in active use. The flags of Vendsyssel (Vendelbrog) and the Jutlandic flag (\"Den jyske fane\") are obscure. None of these flags have legal recognition in Denmark, and are officially considered to be \"fantasy flags\". Denmark reserves official recognition to official flags and regional flags (\"områdeflag\") from other jurisdictions.\n\n\n\n"}
{"id": "52835887", "url": "https://en.wikipedia.org/wiki?curid=52835887", "title": "Geet-Gawai", "text": "Geet-Gawai\n\nGeet-Gawai (Bhojpuri:गीत गवाई) also called Geet Gawai is a pre-wedding ceremony of Bhojpuri speaking people of Indian descent in Mauritius. In 2016, this ritual was added to the UNESCO's Intangible Cultural Heritage List. This pre-wedding ceremony is a combination of certain traditional rituals, prayers to the gods and the goddesses, songs, music and dance. The ceremony may be done at the residence of bride or the groom, and is carried out by female family members and neighbours. This ceremony represents collective cultural memory. By breaking barriers of caste and class, it contributes to building of smooth and cohesive community identity. Presently, Geet-Gawai has come out of the confines of being a family function, and public performances also take place. Now-a-days, apart from women, men are also participating in these events. \n"}
{"id": "784781", "url": "https://en.wikipedia.org/wiki?curid=784781", "title": "Global city", "text": "Global city\n\nA global city, also called world city or sometimes alpha city or world center, is a city which is a primary node in the global economic network. The concept comes from geography and urban studies, and the idea that globalization is created, facilitated, and enacted in strategic geographic locales according to a hierarchy of importance to the operation of the global system of finance and trade.\n\nThe most complex node is the \"global city\", with links binding it to other cities having a direct and tangible effect on global socio-economic affairs. The term \"global city\", rather than \"megacity\", was popularized by sociologist Saskia Sassen in her 1991 work, \"The Global City: New York, London, Tokyo\". \"World city\", meaning a city heavily involved in global trade, appeared in the May 1886 description of Liverpool, by \"The Illustrated London News\". Patrick Geddes later used the term \"world city\" in 1915. More recently, the term has focused on a city's financial power and high technology infrastructure, with other factors becoming less relevant.\n\nGlobal city status is considered beneficial and desirable. Competing groups have developed multiple alternative methods to classify and rank \"world cities\" and to distinguish them from \"non-world cities\". Although there is a consensus upon leading world cities, the chosen criteria affect which other cities are included. Selection criteria may be based on a \"yardstick value\" (e.g., if the producer-service sector is the largest sector then city X is a world city) or on an \"imminent determination\" (if the producer-service sector of city X is greater than the combined producer-service sectors of N other cities then city X is a world city.)\n\nCities can fall from ranking, as in the case of cities that have become less cosmopolitan and less internationally renowned in the current era.\n\nAlthough criteria are variable and fluid, typical characteristics of world cities are:\n\nIn 2015, the second \"Global Economic Power Index\", a\nmeta list compiled by Richard Florida, was published by \"The Atlantic\" (distinct from a namesake list published by the \"Martin Prosperity Institute\"), with city composite rank based on five other lists.\n\nThe Institute for Urban Strategies at The Mori Memorial Foundation in Tokyo issued a comprehensive study of global cities in 2017. They are ranked based on six categories: economy, research & development, cultural interaction, livability, environment, and accessibility, with 70 individual indicators among them. The top ten world cities are also ranked by subjective categories including manager, researcher, artist, visitor and resident.\n\n\nJon Beaverstock, Richard G. Smith and Peter J. Taylor established the Globalization and World Cities Research Network (GaWC). A roster of world cities in the \"GaWC Research Bulletin 5\" is ranked by their connectivity through four \"advanced producer services\": accountancy, advertising, banking/finance, and law. The GaWC inventory identifies three levels of global cities and several sub-ranks, although the authors caution that \"concern for city rankings operates against \"the spirit of the GaWC project\"\" (emphasis in original).\n\nThe 2004 rankings added several new indicators while continuing to rank city economics more heavily than political or cultural factors. The 2008 roster, similar to the 1998 version, is sorted into categories of Alpha world cities (with four sub-categories), Beta world cities (three sub-categories), Gamma world cities (three sub-categories) and additional cities with High sufficiency or Sufficiency presence. The cities in the top two classification in the 2018 edition are:\n\nIn 2008, the American journal \"Foreign Policy\", in conjunction with the consulting firm A.T. Kearney and the Chicago Council on Global Affairs, published a ranking of global cities, based on consultation with Saskia Sassen, Witold Rybczynski, and others. \"Foreign Policy\" noted that \"the world’s biggest, most interconnected cities help set global agendas, weather transnational dangers, and serve as the hubs of global integration. They are the engines of growth for their countries and the gateways to the resources of their regions.\" The ranking is based on 27 metrics across five dimensions: business activity, human capital, information exchange, cultural experience, and political engagement and was updated in 2010, 2012, 2014, 2015 2016 and 2017. Since 2015 it is published together with a separate index called, the Global Cities Outlook: a projection of a city’s potential based on rate of change in 13 indicators across four dimensions: personal well-being, economics, innovation, and governance.\n\n\"The Wealth Report\" (a global perspective on prime property and wealth) is made by the London-based estate agent Knight Frank LLP together with the Citi Private Bank. The report includes a \"Global Cities Survey\", evaluating which cities are considered the most important to the world’s HNWIs (high-net-worth individuals, having over $25 million of investable assets). For the Global Cities Survey, Citi Private Bank’s wealth advisors, and Knight Frank’s luxury property specialists were asked to name the cities that they felt were the most important to HNWIs, in regard to: \"economic activity\", \"political power\", \"knowledge and influence\" and \"quality of life\".\n\nIn 2012, the Economist Intelligence Unit (The Economist Group), ranked the competitiveness of global cities according to their demonstrated ability to attract capital, businesses, talent and visitors.\n\n\n"}
{"id": "407480", "url": "https://en.wikipedia.org/wiki?curid=407480", "title": "Grout", "text": "Grout\n\nGrout is a dense fluid which is used to fill gaps or used as reinforcement in existing structures. Grout is generally a mixture of water, cement, and sand and is employed in pressure grouting, embedding rebar in masonry walls, connecting sections of pre-cast concrete, filling voids, and sealing joints such as those between tiles. Common uses for grout in the household include filling in tiles of shower floors and kitchen tiles. It is often color tinted when it has to be kept visible and sometimes includes fine gravel when being used to fill large spaces (such as the cores of concrete blocks). Unlike other structural pastes such as plaster or joint compound, correctly mixed and applied grout forms a waterproof seal.\n\nAlthough both grout and its close relative mortar are applied as a thick emulsion and harden over time, grout is distinguished by its low viscosity and lack of lime (added to mortar for pliability); grout is thin so it flows readily into gaps, while mortar is thick enough to support not only its own weight, but also that of masonry placed above it.\n\nGrout varieties include tiling grout, flooring grout, resin grout, non-shrink grout, structural grout and thixotropic grout.\n\nTiling grout is often used to fill the spaces between tiles or mosaics and to secure tile to its base. Although ungrouted mosaics do exist, most have grout between the tesserae. Tiling grout is also cement-based, and comes in sanded as well as unsanded varieties. The sanded variety contains finely ground silica sand; unsanded is finer and produces a non-gritty final surface. They are often enhanced with polymers and/or latex.\n\nStructural grout is often used in reinforced masonry to fill voids in masonry housing reinforcing steel, securing the steel in place and bonding it to the masonry. Non-shrink grout is used beneath metal bearing plates to ensure a consistent bearing surface between the plate and its substrate.\n\nPortland cement is the most common cementing agent in grout, but thermoset polymer matrix grouts based on thermosets such as urethanes and epoxies are also popular.\n\nPortland cement-based grouts come in different varieties depending on the particle size of the ground clinker used to make the cement, with a standard size of around 15 microns, microfine at around 6–10 microns, and ultrafine below 5 microns. Finer particle sizes let the grout penetrate more deeply into a fissure. Because these grouts depend on the presence of sand for their basic strength, they are often somewhat gritty when finally cured and hardened. \n\nFrom the different types of grout, a suitable one has to be chosen depending on the load. For example, a load of up to 7.5 tons can be expected for a garage access (2-component pavement joint mortar (traffic load)), whereas a cobbled garden path is only designed for a pedestrian load (1-component pavement joint mortar (pedestrian load)). Furthermore, various substructures determine whether the type of grout should be permanently permeable to water or waterproof for example by concrete subfloors.\n\nTools associated with groutwork include:\n\n"}
{"id": "34459484", "url": "https://en.wikipedia.org/wiki?curid=34459484", "title": "Hackerspace.gr", "text": "Hackerspace.gr\n\nHackerspace.gr ('hsgr') is a hackerspace in Athens, Greece, established in 2011. It operates as a cultural center, computer laboratory and meeting place (with free wireless access). Hackerspace.gr promotes creative coding and hardware hacking through its variety of activities. According to its website: \"Hackerspace.gr is a physical space dedicated to creative code and hardware hacking, in Athens\".\n\nHackerspace.gr vision is inspired by the Open Source philosophy. The main values, according to its vision page, are Excellence, Sharing, Consensus, and Do-ocracy. It is a self-funded community, through a membership fee, individual donations and supporters. Every year Hackerspace.gr publishes its annual financial balance titled \"The cost of Hacking\".\n\nIt organizes workshops, lectures, and entertainment and informational events. The events calendar lists several events weekly. Furthermore, hackerspace.gr is open for visitors as long as any of the administrators is in the premises.\n\nHackerspace.gr is an incubator place for many projects. Currently there is an OpenROV Taskforce on Hackerspace.gr. Verese community, a project participating on Mozilla WebFWD, is hosting its regular meetings at Hackerspace.gr. Ardupad was also incubated at Hackerspace.gr. A USB drop is located in the central area of the hackerspace. A custom open hardware delta 3D printer design, Anadelta is developed to cover its members need for a large 3D printer.\n\nHackerspace provides several online services to its members, visitors, and the general public. In particular some of its members are running an instance of the etherpad lite collaborative editor, a diaspora pod and a Jabber/XMPP service.\nHackerspace.gr usually deploys its geodesic dome in order to establish a mobile hackerspace when a large number of its members participate in events and venues that are away from its physical location providing tools, equipment and free of charge services for attendees. \nhackerspace.gr is utilized as the headquarters of Libre Space Foundation, an open space technologies non-profit, as its laboratory and main working space. Libre Space Foundation shares its testing and manufacturing equipment with hackerspace.gr's users and visitors.\n\nLibre Space Foundation has deployed its first SatNOGS ground station on the rooftop of hackerspace.gr and has used its machine, and electronics facilities for the manufacture, integration and initial testing of UPSat the first open source satellite, and also the first satellite made in Greece.\n"}
{"id": "17581476", "url": "https://en.wikipedia.org/wiki?curid=17581476", "title": "Harnack Medal", "text": "Harnack Medal\n\nThe highest award which is presented by the Max Planck Society for services to society is the Harnack Medal, first awarded in 1925. Past recipients of the Harnack Medal are\n\n\n"}
{"id": "1917500", "url": "https://en.wikipedia.org/wiki?curid=1917500", "title": "Instytut Rozbitek", "text": "Instytut Rozbitek\n\nInstytut Rozbitek is a center near Poznań, Poland meant for development of new work in the areas of film, theatre, music and new media. The institute was founded by the Oscar - Winning Polish composer Jan A.P. Kaczmarek in 2004. The Institute consists of the 19th-century castle and surrounding buildings, which are currently under renovation.\n\nIt is a place of workshops. In 2011 and 2012 it was one of the locations of the Master Classes workshops, during the Transatlantyk - Poznan International Film and Music Festival.\n"}
{"id": "13458509", "url": "https://en.wikipedia.org/wiki?curid=13458509", "title": "Internet linguistics", "text": "Internet linguistics\n\nInternet linguistics is a domain of linguistics advocated by the English linguist David Crystal. It studies new language styles and forms that have arisen under the influence of the Internet and of other new media, such as Short Message Service (SMS) text messaging. Since the beginning of human-computer interaction (HCI) leading to computer-mediated communication (CMC) and Internet-mediated communication (IMC), experts have acknowledged that linguistics has a contributing role in it, in terms of web interface and usability. Studying the emerging language on the Internet can help improve conceptual organization, translation and web usability. Such study aims to benefit both linguists and web users.\n\nThe study of Internet linguistics can take place through four main perspectives: sociolinguistics, education, stylistics and applied linguistics. Further dimensions have developed as a result of further technological advances - which include the development of the Web as corpus and the spread and influence of the stylistic variations brought forth by the spread of the Internet, through the mass media and through literary works. In view of the increasing number of users connected to the Internet, the linguistics future of the Internet remains to be determined, as new computer-mediated technologies continue to emerge and people adapt their languages to suit these new media. The Internet continues to play a significant role both in encouraging as well as in diverting attention away from the usage of languages.\n\nDavid Crystal has identified four main perspectives for further investigation – the sociolinguistic perspective, the educational perspective, the stylistic perspective and the applied perspective. The four perspectives are effectively interlinked and affect one another.\n\nThis perspective deals with how society views the impact of Internet development on languages. The advent of the Internet has revolutionized communication in many ways; it changed the way people communicate and created new platforms with far-reaching social impact. Significant avenues include but are not limited to SMS text messaging, e-mails, chatgroups, virtual worlds and the Web.\n\nThe evolution of these new mediums of communications has raised much concern with regards to the way language is being used. According to Crystal (2005), these concerns are neither without grounds nor unseen in history – it surfaces almost always when a new technology breakthrough influences languages; as seen in the 15th century when printing was introduced, the 19th century when the telephone was invented and the 20th century when broadcasting began to penetrate our society.\n\nAt a personal level, CMC such as SMS Text Messaging and mobile e-mailing (push mail) has greatly enhanced instantaneous communication. Some examples include the iPhone and the BlackBerry.\n\nIn schools, it is not uncommon for educators and students to be given personalized school e-mail accounts for communication and interaction purposes. Classroom discussions are increasingly being brought onto the Internet in the form of discussion forums. For instance, at Nanyang Technological University, students engage in collaborative learning at the university’s portal – edveNTUre, where they participate in discussions on forums and online quizzes and view streaming podcasts prepared by their course instructors among others. iTunes U in 2008 began to collaborate with universities as they converted the Apple music service into a store that makes available academic lectures and scholastic materials for free – they have partnered more than 600 institutions in 18 countries including Oxford, Cambridge and Yale Universities.\n\nThese forms of academic social networking and media are slated to rise as educators from all over the world continue to seek new ways to better engage students. It is commonplace for students in New York University to interact with “guest speakers weighing in via Skype, library staffs providing support via instant messaging, and students accessing library resources from off campus.” This will affect the way language is used as students and teachers begin to use more of these CMC platforms.\n\nAt a professional level, it is a common sight for companies to have their computers and laptops hooked up onto the Internet (via wired and wireless Internet connection), and for employees to have individual e-mail accounts. This greatly facilitates internal (among staffs of the company) and external (with other parties outside of one’s organization) communication. Mobile communications such as smart phones are increasingly making their way into the corporate world. For instance, in 2008, Apple announced their intention to actively step up their efforts to help companies incorporate the iPhone into their enterprise environment, facilitated by technological developments in streamlining integrated features (push e-mail, calendar and contact management) using ActiveSync.\n\nIn general, these new CMCs that are made possible by the Internet have altered the way people use language – there is heightened informality and consequently a growing fear of its deterioration. However, as David Crystal puts it, these should be seen positively as it reflects the power of the creativity of a language.\n\nThe sociolinguistics of the Internet may also be examined through five interconnected themes.\n\n\nThe educational perspective of internet linguistics examines the Internet's impact on formal language use, specifically on Standard English, which in turn affects language education. The rise and rapid spread of Internet use has brought about new linguistic features specific only to the Internet platform. These include, but are not limited to, an increase in the use of informal written language, inconsistency in written styles and stylistics and the use of new abbreviations in Internet chats and SMS text messaging, where constraints of technology on word count contributed to the rise of new abbreviations. Such acronyms exist primarily for practical reasons — to reduce the time and effort required to communicate through these mediums apart from technological limitations. Examples of common acronyms include \"lol\" (for laughing out loud; a general expression of laughter), \"omg\" (oh my god) and \"gtg\" (got to go).\n\nThe educational perspective has been considerably established in the research on the Internet's impact on language education. It is an important and crucial aspect as it affects and involves the education of current and future student generations in the appropriate and timely use of informal language that arises from Internet usage. There are concerns for the growing infiltration of informal language use and incorrect word use into academic or formal situations, such as the usage of casual words like \"guy\" or the choice of the word \"preclude\" in place of \"precede\" in academic papers by students. There are also issues with spellings and grammar occurring at a higher frequency among students' academic works as noted by educators, with the use of abbreviations such as \"u\" for \"you\" and \"2\" for \"to\" being the most common.\n\nLinguists and professors like Eleanor Johnson suspect that widespread mistakes in writing are strongly connected to Internet usage, where educators have similarly reported new kinds of spelling and grammar mistakes in student works. There is, however, no scientific evidence to confirm the proposed connection. Though there are valid concerns about Internet usage and its impact on students' academic and formal writing, its severity is however enlarged by the informal nature of the new media platforms. Naomi S. Baron (2008) argues in \"Always On\" that student writings suffer little impact from the use of Internet-mediated communication (IMC) such as internet chat, SMS text messaging and e-mail. A study in 2009 published by the British Journal of Developmental Psychology found that students who regularly texted (sent messages via SMS using a mobile phone) displayed a wider range of vocabulary and this may lead to a positive impact on their reading development.\n\nThough the use of the Internet resulted in stylistics that are not deemed appropriate in academic and formal language use, it is to be noted that Internet use may not hinder language education but instead aid it. The Internet has proven in different ways that it can provide potential benefits in enhancing language learning, especially in second or foreign language learning. Language education through the Internet in relation to Internet linguisitics is, most significantly, applied through the communication aspect (use of e-mails, discussion forums, chat messengers, blogs, etc.).\nIMC allows for greater interaction between language learners and native speakers of the language, providing for greater error corrections and better learning opportunities of standard language, in the process allowing the picking up of specific skills such as negotiation and persuasion.\n\nThis perspective examines how the Internet and its related technologies have encouraged new and different forms of creativity in language, especially in literature. It looks at the Internet as a medium through which new language phenomena have arisen. This new mode of language is interesting to study because it is an amalgam of both spoken and written languages. For example, traditional writing is static compared to the dynamic nature of the new language on the Internet where words can appear in different colors and font sizes on the computer screen. Yet, this new mode of language also contains other elements not found in natural languages. One example is the concept of framing found in e-mails and discussion forums. In replying to e-mails, people generally use the sender’s e-mail message as a frame to write their own messages. They can choose to respond to certain parts of an e-mail message while leaving other bits out. In discussion forums, one can start a new thread and anyone regardless of their physical location can respond to the idea or thought that was set down through the Internet. This is something that is usually not found in written language.\n\nFuture research also includes new varieties of expressions that the Internet and its various technologies are constantly producing and their effects not only on written languages but also their spoken forms. The communicative style of Internet language is best observed in the CMC channels below, as there are often attempts to overcome technological restraints such as transmission time lags and to re-establish social cues that are often vague in written text.\n\nMobile phones (also called \"cell phones\") have an expressive potential beyond their basic communicative functions. This can be seen in text-messaging poetry competitions such as the one held by The Guardian. The 160-character limit imposed by the cell phone has motivated users to exercise their linguistic creativity to overcome them. A similar example of new technology with character constraints is Twitter, which has a 280-character limit. There have been debates as to whether these new abbreviated forms introduced in users’ Tweets are \"lazy\" or whether they are creative fragments of communication. Despite the ongoing debate, there is no doubt that Twitter has contributed to the linguistic landscape with new lingoes and also brought about a new dimension of communication.\n\nThe cell phone has also created a new literary genre – cell phone novels. A typical cell phone novel consists of several chapters which readers download in short installments. These novels are in their \"raw\" form as they do not go through editing processes like traditional novels. They are written in short sentences, similar to text-messaging.\nAuthors of such novels are also able to receive feedbacks and new ideas from their readers through e-mails or online feedback channels. Unlike traditional novel writing, readers’ ideas sometimes get incorporated into the storyline or authors may also decide to change their story’s plot according to the demand and popularity of their novel (typically gauged by the number of download hits).\nDespite their popularity, there has also been criticism regarding the novels’ \"lack of diverse vocabulary\" and poor grammar.\n\nBlogging has brought about new ways of writing diaries and from a linguistic perspective, the language used in blogs is \"in its most 'naked' form\", published for the world to see without undergoing the formal editing process. This is what makes blogs stand out because almost all other forms of printed language have gone through some form of editing and standardization. David Crystal stated that blogs were \"the beginning of a new stage in the evolution of the written language\". Blogs have become so popular that they have expanded beyond written blogs, with the emergence of photoblog, videoblog, audioblog and moblog. These developments in interactive blogging have created new linguistic conventions and styles, with more expected to arise in the future.\n\nVirtual worlds provide insights into how users are adapting the usage of natural language for communication within these new mediums. The Internet language that has arisen through user interactions in text-based chatrooms and computer-simulated worlds has led to the development of slangs within digital communities. Examples of these include pwn and noob. Emoticons are further examples of how users have adapted different expressions to suit the limitations of cyberspace communication, one of which is the \"loss of emotivity\".\n\nCommunication in niches such as role-playing games (RPG) of Multi-User domains (MUDs) and virtual worlds is highly interactive, with emphasis on speed, brevity and spontaneity. As a result, CMC is generally more vibrant, volatile, unstructured and open. There are often complex organization of sequences and exchange structures evident in the connection of conversational strands and short turns. Some of the CMC strategies used include capitalization for words such as \"EMPHASIS\", usage of symbols such as the asterisk to enclose words as seen in \"*stress*\" and the creative use of punctuation like \"???!?!?!?\". Symbols are also used for discourse functions, such as the asterisk as a conversational repair marker and arrows and carats as deixis and referent markers. Besides contributing to these new forms in language, virtual worlds are also being used to teach languages. Virtual world language learning provides students with simulations of real-life environments, allowing them to find creative ways to improve their language skills. Virtual worlds are good tools for language learning among the younger learners because they already see such places as a \"natural place to learn and play\".\n\nOne of the most popular Internet-related technologies to be studied under this perspective is e-mail, which has expanded the stylistics of languages in many ways. A study done on the linguistic profile of e-mails has shown that there is a hybrid of speech and writing styles in terms of format, grammar and style. E-mail is rapidly replacing traditional letter-writing because of its convenience, speed and spontaneity. It is often related to informality as it feels temporary and can be deleted easily. However, as this medium of communication matures, e-mail is no longer confined to sending informal messages between friends and relatives. Instead, business correspondences are increasingly being carried out through e-mails. Job seekers are also using e-mails to send their resumes to potential employers. The result of a move towards more formal usages will be a medium representing a range of formal and informal stylistics.\n\nWhile e-mail has been blamed for students’ increased usage of informal language in their written work, David Crystal argues that e-mail is \"not a threat, for language education\" because e-mail with its array of stylistic expressiveness can act as a domain for language learners to make their own linguistic choices responsibly. Furthermore, the younger generation’s high propensity for using e-mail may improve their writing and communication skills because of the efforts they are making to formulate their thoughts and ideas, albeit through a digital medium.\n\nLike other forms of online communication, instant messaging has also developed its own acronyms and short forms. However, instant messaging is quite different from e-mail and chatgroups because it allows participants to interact with one another in real-time while conversing in private. With instant messaging, there is an added dimension of familiarity among participants. This increased degree of intimacy allows greater informality in language and \"typographical idiosyncrasies\". There are also greater occurrences of stylistic variation because there can be a very wide age gap between participants. For example, a granddaughter can catch up with her grandmother through instant messaging. Unlike chatgroups where participants come together with shared interests, there is no pressure to conform in language here.\n\nThe applied perspective views the linguistic exploitation of the Internet in terms of its communicative capabilities – the good and the bad. The Internet provides a platform where users can experience multilingualism. Although English is still the dominant language used on the Internet, other languages are gradually increasing in their number of users. The Global Internet usage page provides some information on the number of users of the Internet by language, nationality and geography. This multilingual environment continues to increase in diversity as more language communities become connected to the Internet. The Internet is thus a platform where minority and endangered languages can seek to revive their language use and/or create awareness. This can be seen in two instances where it provides these languages opportunities for progress in two important regards - language documentation and language revitalization.\n\nFirstly, the Internet facilitates language documentation. Digital archives of media such as audio and video recordings not only help to preserve language documentation, but also allows for global dissemination through the Internet. Publicity about endangered languages, such as Webster (2003) has helped to spur a worldwide interest in linguistic documentation.\n\nFoundations such as the Hans Rausing Endangered Languages Project (HRELP), funded by Arcadia also help to develop the interest in linguistic documentation. The HRELP is a project that seeks to document endangered languages, preserve and disseminate documentation materials among others. The materials gathered are made available online under its Endangered Languages Archive (ELAR) program.\n\nOther online materials that support language documentation include the Language Archive Newsletter which provides news and articles about topics in endangered languages. The web version of Ethnologue also provides brief information of all of the world’s known living languages. By making resources and information of endangered languages and language documentation available on the Internet, it allows researchers to build on these materials and hence preserve endangered languages.\n\nSecondly, the Internet facilitates language revitalization. Throughout the years, the digital environment has developed in various sophisticated ways that allow for virtual contact. From e-mails, chats to instant messaging, these virtual environments have helped to bridge the spatial distance between communicators. The use of e-mails has been adopted in language courses to encourage students to communicate in various styles such as conference-type formats and also to generate discussions. Similarly, the use of e-mails facilitates language revitalization in the sense that speakers of a minority language who moved to a location where their native language is not being spoken can take advantage of the Internet to communicate with their family and friends, thus maintaining the use of their native language. With the development and increasing use of telephone broadband communication such as Skype, language revitalization through the internet is no longer restricted to literate users.\n\nHawaiian educators have been taking advantage of the Internet in their language revitalization programs. The graphical bulletin board system, Leoki (Powerful Voice), was established in 1994. The content, interface and menus of the system are entirely in the Hawaiian language. It is installed throughout the immersion school system and includes components for e-mails, chat, dictionary and online newspaper among others. In higher institutions such as colleges and universities where the Leoki system is not yet installed, the educators make use of other software and Internet tools such as Daedalus Interchange, e-mails and the Web to connect students of Hawaiian language with the broader community.\n\nAnother use of the Internet includes having students of minority languages write about their native cultures in their native languages for distant audiences. Also, in an attempt to preserve their language and culture, Occitan speakers have been taking advantage of the Internet to reach out to other Occitan speakers from around the world. These methods provide reasons for using the minority languages by communicating in it. In addition, the use of digital technologies, which the young generation think of as ‘cool’, will appeal to them and in turn maintain their interest and usage of their native languages.\n\nThe Internet can also be exploited for activities such as terrorism, internet fraud and pedophilia. In recent years, there has been an increase in crimes that involved the use of the Internet such as e-mails and Internet Relay Chat (IRC), as it is relatively easy to remain anonymous. These conspiracies carry concerns for security and protection. From a forensic linguistic point of view, there are many potential areas to explore. While developing a chat room child protection procedure based on search terms filtering is effective, there is still minimal linguistically orientated literature to facilitate the task. In other areas, it is observed that the Semantic Web has been involved in tasks such as personal data protection, which helps to prevent fraud.\n\nThe dimensions covered in this section include looking at the Web as a corpus and issues of language identification and normalization. The impacts of internet linguistics on everyday life are examined under the spread and influence of Internet stylistics, trends of language change on the Internet and conversation discourse.\n\nWith the Web being a huge reservoir of data and resources, language scientists and technologists are increasingly turning to the web for language data. Corpora were first formally mentioned in the field of computational linguistics at the 1989 ACL meeting in Vancouver. It was met with much controversy as they lacked theoretical integrity leading to much skepticism of their role in the field, until the publication of the journal ‘Using Large Corpora’ in 1993 that the relationship between computational linguistics and corpora became widely accepted.\n\nTo establish whether the Web is a corpus, it is worthwhile to turn to the definition established by McEnery and Wilson (1996, pp 21).\n\nRelating closer to the Web as a Corpus, Manning and Schütze (1999, pp 120) further streamlines the definition:\n\nHit counts were used for carefully constructed search engine queries to identify rank orders for word sense frequencies, as an input to a word sense disambiguation engine. This method was further explored with the introduction of the concept of a parallel corpora where the existing Web pages that exist in parallel in local and major languages be brought together. It was demonstrated that it is possible to build a language-specific corpus from a single document in that specific language.\n\nThere has been much discussion about the possible developments in the arena of the Web as a corpus. The development of using the web as a data source for word sense disambiguation was brought forward in The EU MEANING project in 2002. It used the assumption that within a domain, words often have a single meaning, and that domains are identifiable on the Web. This was further explored by using Web technology to gather manual word sense annotations on the Word Expert Web site.\n\nIn areas of language modeling, the Web has been used to address data sparseness. Lexical statistics have been gathered for resolving prepositional phrase attachments, while Web document were used to seek a balance in the corpus.\n\nIn areas of information retrieval, a Web track was integrated as a component in the community’s TREC evaluation initiative. The sample of the Web used for this exercise amount to around 100GB, compromising of largely documents in the .gov top level domain.\n\nThe British National Corpus contains ample information on the dominant meanings and usage patterns for the 10,000 words that forms the core of English.\n\nThe number of words in the British National Corpus (ca 100 million) is sufficient for many empirical strategies for learning about language for linguists and lexicographers, and is satisfactory for technologies that utilize quantitative information about the behavior of words as input (parsing).\n\nHowever, for some other purposes, it is insufficient, as an outcome of the Zipfian nature of word frequencies. Because the bulk of the lexical stock occurs less than 50 times in the British National Corpus, it is insufficient for statistically stable conclusions about such words. Furthermore, for some rarer words, rare meanings of common words, and combinations of words, no data has been found. Researchers find that probabilistic models of language based on very large quantities of data are better than ones based on estimates from smaller, cleaner data sets.\n\nThe Web is clearly a multilingual corpus. It is estimated that 71% of the pages (453 million out of 634 million Web pages indexed by the Excite engine) were written in English, followed by Japanese (6.8%), German (5.1%), French (1.8%), Chinese (1.5%), Spanish (1.1%), Italian (0.9%), and Swedish (0.7%).\n\nA test to find contiguous words like ‘deep breath’ revealed 868,631 Web pages containing the terms in AlltheWeb. The number found through the search engines are more than three times the counts generated by the British National Corpus, indicating the significant size of the English corpus available on the Web.\n\nThe massive size of text available on the Web can be seen in the analysis of controlled data in which corpora of different languages were mixed in various proportions. The estimated Web size in words by AltaVista saw English at the top of the list with 76,598,718,000 words. The next is German, with 7,035,850,000 words along with 6 other languages with over a billion hits. Even languages with fewer hits on the Web such as Slovenian, Croatian, Malay, and Turkish have more than one hundred million words on the Web. This reveals the potential strength and accuracy of using the Web as a Corpus given its significant size, which warrants much additional research such as the project currently being carried out by the British National Corpus to exploit its scale.\n\nIn areas of language modeling, there are limitations on the applicability of any language model as the statistics for different types of text will be different. When a language technology application is put into use (applied to a new text type), it is not certain that the language model will fare in the same way as how it would when applied to the training corpus. It is found that there are substantial variations in model performance when the training corpus changes. This lack of theory types limits the assessment of the usefulness of language-modeling work.\n\nAs Web texts are easily produced (in terms of cost and time) and with many different authors working on them, it often results in little concern for accuracy. Grammatical and typographical errors are regarded as “erroneous” forms that cause the Web to be a dirty corpus. Nonetheless, it may still be useful even with some noise.\n\nThe issue of whether sublanguages should be included remains unsettled. Proponents of it argue that with all sublanguages removed, it will result in an impoverished view of language. Since language is made up of lexicons, grammar and a wide array of different sublanguages, they should be included. However, it is not until recently that it became a viable option. Striking a middle ground by including some sublanguages is contentious because it’s an arbitrary issue of which to include and which not.\n\nThe decision of what to include in a corpus lies with corpus developers, and it has been done so with pragmatism. The desiderata and criteria used for the British National Corpus serves as a good model for a general-purpose, general-language corpus with the focus of being representative replaced with being balanced.\n\nSearch engines such as Google serves as a default means of access to the Web and its wide array of linguistics resources. However, for linguists working in the field of corpora, there presents a number of challenges. This includes the limited instances that are presented by the search engines (1,000 or 5,000 maximum); insufficient context for each instance (Google provides a fragment of around ten words); results selected according to criteria that are distorted (from a linguistic point of view) as search term in titles and headings often occupy the top results slots; inability to allow searches to be specified according to linguistic criteria, such as the citation form for a word, or word class; unreliability of statistics, with results varying according to search engine load and many other factors. At present, in view of the conflicts of priorities among the different stakeholders, the best solution is for linguists to attempt to correct these problems by themselves. This will then lead to a large number of possibilities opening in the area of harnessing the rich potential of the Web.\n\nDespite the sheer size of the Web, it may still not be representative of all the languages and domains in the world, and neither are other corpora. However, the huge quantities of text, in numerous languages and language types on a huge range of topics makes it a good starting point that opens up to a large number of possibilities in the study of corpora.\n\nStylistics arising from Internet usage has spread beyond the new media into other areas and platforms, including but not limited to, films, music and literary works. The infiltration of Internet stylistics is important as mass audiences are exposed to the works, reinforcing certain Internet specific language styles which may not be acceptable in standard or more formal forms of language.\n\nApart from internet slang, grammatical errors and typographical errors are features of writing on the Internet and other CMC channels. As users of the Internet gets accustomed to these errors, it progressively infiltrates into everyday language use, in both written and spoken forms. It is also common to witness such errors in mass media works, from typographical errors in news articles to grammatical errors in advertisements and even internet slang in drama dialogues.\n\nThe more the internet is incorporated into daily life, the greater the impact it has on formal language. This is especially true in modern Language Arts classes through the use of smart phones, tablets, and social media. Students are exposed to the language of the internet more than ever, and as such, the grammatical structure and slang of the internet are bleeding into their formal writing. Full immersion into a language is always the best way to learn it. Mark Lester in his book \"Teaching Grammar and Usage\" states, “The biggest single problem that basic writers have in developing successful strategies for coping with errors is simply their lack of exposure to formal written English...We would think it absurd to expect a student to master a foreign language without extensive exposure to it.” Since students are immersed in internet language, that is the form and structure they are mirroring.\n\nThere has been instances of television advertisements using Internet slang, reinforcing the penetration of Internet stylistics in everyday language use. For example, in the Cingular commercial in the United States, acronyms such as \"BFF Jill\" (which means \"Best Friend Forever, Jill\") were used. More businesses have adopted the use of Internet slang in their advertisements as the more people are growing up using the Internet and other CMC platforms, in an attempt to relate and connect to them better. Such commercials have received relatively enthusiastic feedback from its audiences.\n\nThe use of Internet lingo has also spread into the arena of music, significantly seen in popular music. A recent example is Trey Songz's lyrics for \"LOL :-)\", which incorporated many Internet lingo and mentions of Twitter and texting.\n\nThe spread of Internet linguistics is also present in films made by both commercial and independent filmmakers. Though primarily screened at film festivals, DVDs of independent films are often available for purchase over the internet including paid-live-streamings, making access to films more easily available for the public. The very nature of commercial films being screened at public cinemas allows for the wide exposure to the mainstream mass audience, resulting in a faster and wider spread of Internet slangs. The latest commercial film is titled \"LOL\" (acronym for \"Laugh Out Loud\" or \"Laughing Out Loud\"), starring Miley Cyrus and Demi Moore. This movie is a 2011 remake of the Lisa Azuelos' 2008 popular French film similarly titled \"LOL (Laughing Out Loud)\".\n\nThe use of internet slangs is not limited to the English language but extends to other languages as well. The Korean language has incorporated the English alphabet in the formation of its slang, while others were formed from common misspellings arising from fast typing. The new Korean slang is further reinforced and brought into everyday language use by television shows such as soap operas or comedy dramas like “High Kick Through the Roof” released in 2009.\n\nWith the emergence of greater computer/Internet mediated communication systems, coupled with the readiness with which people adapt to meet the new demands of a more technologically sophisticated world, it is expected that users will continue to remain under pressure to alter their language use to suit the new dimensions of communication.\n\nAs the number of Internet users increase rapidly around the world, the cultural background, linguistic habits and language differences among users are brought into the Web at a much faster pace. These individual differences among Internet users are predicted to significantly impact the future of Internet linguistics, notably in the aspect of the multilingual web. As seen from 2000 to 2010, Internet penetration has experienced its greatest growth in non-English speaking countries such as China, India and Africa, resulting in more languages apart from English penetrating the Web.\n\nAlso, the interaction between English and other languages is predicted to be an important area of study. As global users interact with each other, possible references to different languages may continue to increase, resulting in formation of new Internet stylistics that spans across languages. Chinese and Korean languages have already experienced English language's infiltration leading to the formation of their multilingual Internet lingo.\n\nAt current state, the Internet provides a form of education and promotion for minority languages. However, similar to how cross-language interaction has resulted in English language's infiltration into Chinese and Korean languages to form new slangs, minority languages are also affected by the more common languages used on the Internet (such as English and Spanish). While language interaction can cause a loss in the authentic standard of minority languages, familiarity of the majority language can also affect the minority languages in adverse ways. For example, users attempting to learn the minority language may opt to read and understand about it in a majority language and stop there, resulting in a loss instead of gain in the potential speakers of the minority language. Also, speakers of minority languages may be encouraged to learn the more common languages that are being used on the Web in order to gain access to more resources, and in turn leading to a decline in their usage of their own language. The future of endangered minority languages in view of the spread of Internet remains to be observed.\n\n\n"}
{"id": "28750400", "url": "https://en.wikipedia.org/wiki?curid=28750400", "title": "John Whitney Hall Book Prize", "text": "John Whitney Hall Book Prize\n\nThe John Whitney Hall Book Prize has been awarded annually since 1994 by the Association for Asian Studies (AAS). Pioneer Japanese studies scholar John Whitney Hall is commemorated in the name of this prize.\n\nThe Hall Prize acknowledges an outstanding English language book published on Japan or Korea; and the prize honors the author of the book.\n\nAAS is a scholarly, non-political, non-profit professional association open to all persons interested in Asia. The association was founded in 1941 as publisher of the \"Far Eastern Quarterly\" (now the \"Journal of Asian Studies\"). The organization has gone through a series of reorganizations since those early days; but its continuing function serves to further an exchange of information among scholars to increase understanding about East, South, and Southeast Asia.\n\nThe Northeast Asia Council (NEAC) of the AAS oversees the John Whitney Hall Book Prize.\n\n\n"}
{"id": "300543", "url": "https://en.wikipedia.org/wiki?curid=300543", "title": "Knowledge gap hypothesis", "text": "Knowledge gap hypothesis\n\nThe knowledge gap hypothesis explains that knowledge, like other forms of wealth, is often differentially distributed throughout a social system. Specifically, the hypothesis predicts that \"as the infusion of mass media information into a social system increases, segments of the population with higher socioeconomic status tend to acquire this information at a faster rate than the lower status segments, so that the gap in knowledge between these segments tends to increase rather than decrease\". Phillip J. Tichenor, then Associate Professor of Journalism and Mass Communication, George A. Donohue, Professor of Sociology, and Clarice N. Olien, Instructor in Sociology – three University of Minnesota researchers – first proposed the knowledge gap hypothesis in 1970.\n\nAlthough first formally articulated in 1970, Tichenor, Donohue, and Olien note that the knowledge gap hypothesis has been implicit throughout the mass communication literature.\n\nIndeed, research published as early as the 1920s had already begun to examine the influence of individual characteristics on people's media content preferences. For example, Gray and Munroe identified education – still used today as an operationalization of socioeconomic status in knowledge gap research (see, e.g., Hwang and Jeong, 2009) – as a significant and positive correlate of a person's tendency to prefer \"serious\" (rather than non-serious) print content.\n\nPopular belief, however, held that such differences in preferences might be diminished by the advent of radio, which required neither the special skill nor the exertion of reading (Lazarsfeld, 1940). Guglielmo Marconi, inventor of the wireless telegraph, even believed that the radio would \"make war impossible, because it will make war ridiculous\" (Narodny, 1912, p. 145). Interested in whether radio had attenuated these individual differences in content preferences, Paul Lazarsfeld, head of the Office of Radio Research at Columbia University, set out to examine whether (1) the total amount of time that people listened to the radio and (2) the type of content they listened to correlated with their socioeconomic status. Not only did Lazarsfeld's data indicate people of lower socioeconomic status tended to listen to more radio programming, but also they were simultaneously less likely to listen to \"serious\" radio content. Contrary to popular belief at the time, then, the widespread adoption of the radio seems to have had little, if any, effect on a person's tendency to prefer specific types of content.\n\nFurther evidence supporting the knowledge gap hypothesis came from Star and Hughes (1950) analysis of efforts to inform Cincinnati adults about the United Nations. Like Gray and Munroe (1929) and Lazarsfeld (1940) before them, Star and Hughes found that while the campaign was successful in reaching better-educated people, those with less education virtually ignored the campaign. Additionally, after realizing that the highly educated people reached by the campaign also tended to be more interested in the topic, Star and Hughes suggested that knowledge, education, and interest may be interdependent.\n\nBased on observations implicit in mass communication research, Tichenor, Donohue, and Olien (1970) define the knowledge gap hypothesis as follows:\n\nAdditionally, Tichenor, Donohue, and Olien suggest 5 reasons why the knowledge gap should exist: \nailor their conte\n\nGiven the preceding information, the knowledge gap hypothesis can be expressed using the following set of related propositions:\n\nThe knowledge gap hypothesis can be operationalized both for cross-sectional and time-series appropriate research. For cross-sectional research, the knowledge gap hypothesis expects that \"\"at any given time\", there should be a higher correlation between acquisition of knowledge and education for topics highly publicized in the media than for topics less highly publicized. Tichenor, Donohue, and Olien (1970) tested this hypothesis using an experiment in which participants were asked to read and discuss two news stories of varying publicity. The results of the experiment support the hypothesis because correlations between education and understanding were significant for high publicity stories but not significant for low publicity stories.\n\nFor time-series research, the knowledge gap hypothesis expects that \"\"over time\", acquisition of knowledge of a heavily publicized topic will proceed at a faster rate among better educated persons than among those with less education.\" Tichenor, Donohue, and Olien (1970) tested this hypothesis using public opinion surveys gathered between 1949 and 1965 measuring whether participants believed humans would reach the Moon in the foreseeable future. During the 15-year span, belief among grade-school educated people increased only about 25 percentage points while belief among college educated people increased more than 60 percentage points, a trend consistent with the hypothesis.\n\nAlthough by the mid-1970s extensive data supported the existence of a knowledge gap among low and high socioeconomic status individuals, Donohue, Tichenor, and Olien (1975) sought to refine the hypothesis to determine under what conditions the knowledge gap might be attenuated or even eliminated. To this end, they examined survey data on national and local issues from probability samples of 16 Minnesota communities gathered between 1969 and 1975. Donohue and colleagues identified three variables that weakened the knowledge gap:\n\nAt least two narrative reviews and one meta-analysis of knowledge gap hypothesis research exist. Gaziano conducted two narrative reviews, one of 58 articles with relevant data in 1983 and the other of 39 additional studies in 1997. Gaziano writes, \"the most consistent result is the presence of knowledge differentials, regardless of topic, methodological, or theoretical variations, study excellence, or other variables and conditions\" (1997, p. 240). Evidence from several decades, Gaziano concludes, underscores the enduring character of knowledge gaps and indicates that they transcend topics and research settings.\n\nBecause narrative reviews examine significance tests rather than effect sizes, Hwang and Jeong (2009) conducted a meta-analysis of 46 knowledge gap studies. Consistent with Gaziano's results, however, Hwang and Jeong found constant knowledge gaps across time.\n\nIn 2010 Elizabeth Corley and Dietram Scheufele conducted a study to investigate the widening knowledge gap with the example of nanotechnology. On the whole, public opinion research has shown that respondents with higher socioeconomic status (SES) acquire new information at a higher rate than low SES respondents. Their previous analyses of two large national surveys conducted in 2004 and 2007 found that respondents with at least a college degree displayed an increase in knowledge levels between 2004 and 2007 while respondents with education levels of less than a high school diploma had a significant decrease in nanotechnology knowledge levels. These results stress that the group that is most in need of help, low SES bracket, have not been helped through communication efforts and their nanotechnology knowledge levels have decreased over time.\n\nCorley and Scheufele investigated a wide range of factors that may help to close knowledge gaps, including mass media. The researchers found that the number of days a week that respondents spent online was significantly correlated to knowledge levels about nanotechnology. Therefore, internet use helped those with less formal education to catch up to their counterparts.\n\nThe emergence of the Internet, and more specifically Web 2.0, may be playing a role in closing the knowledge gap. In fact, Corley and Scheufele explain that \"the internet may finally live up to the hype … as a tool for creating a more informed citizenry by serving as a \"leveler\" of knowledge gaps.\" (2010, p. 2) This is widely due to the fact that information on Web 2.0 is written in layman's terms. The content is created by those individuals who have an understanding of the information, but who are also able to tailor the articles towards a more general audience.\n\nStill, the knowledge gap may still exist even with the emergence of Web 2.0. The disenfranchised group, in this situation, the group with lower SES, must still be motivated to get the information to close the gap. Also, information about a given subject must be given. Without the content being provided, Web 2.0 will not be much of a help. However, if the content is provided, Web 2.0 has allowed the readers to be more interactive and talk with others online, through discussion boards, forums and blogs. The results of the research conducted by Corley and Scheufele are a clear call to action for researchers to investigate non-traditional ways of connecting with lay audiences about emerging technologies.\n\nOverall, studies show the introduction of Web 2.0 may help in closing the knowledge gap because the content that traditionally those with lower SES could not reach, can now be understood because it is written in layman's terms. Web 2.0 has helped because:\n\n\nThere are now three existing competing hypotheses: 1) Media Malaise hypothesis (that predicts a general negative effect), 2) the Virtuous Circle hypothesis (that predicts a general positive effect), and 3) the Differential Effect hypothesis (that predicts a positive effect from newspapers, and a null or negative effect from television)\" (Fraile, 2011). Three types of media outlets have been used to examine the media effects on knowledge gap: 1) Television – knowledge gap between lower and higher education groups are greater among light television users compared to heavy television users (Eveland, 2000), 2) Newspaper – the exposure to newspaper can potentially reinforce the knowledge gap in politics for different SES groups since reading newspaper requires literacy ability to effectively understand the information (Jerit \"et al.\", 2006), while other studies suggest that exposure to newspaper actually slightly decreases the knowledge gap rather than increasing it (Eveland, 2000), and 3) Internet - internet exposure increases public's general knowledge in health issues (Shim, 2008).\n\n"}
{"id": "51363053", "url": "https://en.wikipedia.org/wiki?curid=51363053", "title": "Kringle (book)", "text": "Kringle (book)\n\nKringle is an American children's novel by author Tony Abbott. The story chronicles the origins of Kris Kringle, also known as Santa Claus. The book was released in 2005.\n\n\"Summary is coming from the Scholastic website:\"\n\n\"Unlike the traditional Santa Claus myth, Kringle is a coming-of-age story about an orphan who becomes a force for good in a dark and violent time. It is a tale of fantasy, of goblins, elves, and flying reindeer — and of a boy from the humblest beginnings who fulfills his destiny. Our tale begins in 500 A.D., when goblins kidnapped human children and set them to work in underground mines. Kringle is one such child... until he discovers his mission — to free children from enslavement. His legend lives on today, as he travels the earth every Christmas Eve to quell the goblins once more.\" \n\n\"Kringle\" has been called \"deft writing\" and \"a credible blend of Anglo-Saxon legend and Christian myth\". Booklist wrote \"On first glance, this story of how Kris Kringle came to live at the North Pole, surrounded by toy-making elves, sounds like the premise for a lighthearted cartoon. It is anything but. ... Told in a come-nearer voice, this epic could have used some tightening, but the enticing premise, appealing young hero, and nonstop action will appeal to many fantasy lovers.\"\n\nIn 2007, Paramount Pictures optioned the film rights. The idea was pitched by film director Mark Dindal based on a script written by Dindal and Jason Richman, with Lorenzo di Bonaventura producing and Dindal directing. , no new updates have been announced regarding the status of the production.\n\n"}
{"id": "11192987", "url": "https://en.wikipedia.org/wiki?curid=11192987", "title": "List of Skinny Puppy side projects", "text": "List of Skinny Puppy side projects\n\nThis is a list of side projects of the prominent industrial band Skinny Puppy, who have released twelve albums and toured extensively since 1982. This list includes other projects begun by cEvin Key and Nivek Ogre—the only constant members of Skinny Puppy—since its inception. Other members have included Dwayne Goettel, Mark Walk, Dave \"Rave\" Ogilvie, and many other guests/session musicians; this list does not include other projects of those musicians.\n\n\nNotable guest appearances of Skinny Puppy members with other bands include:\n\n\n"}
{"id": "4028743", "url": "https://en.wikipedia.org/wiki?curid=4028743", "title": "Man of Miracles (comics)", "text": "Man of Miracles (comics)\n\nThe Man of Miracles (Mother of Existence or M.O.M. for short) is a fictional, ageless, mysterious, gender-less, super-being, featured in the \"Spawn\" comic book series.\n\nThe Mother (under the alias Man of Miracles) has appeared to offer her/his guidance and wisdom to The Hellspawn so that he might play his part in Armageddon. She/he is an ageless being of practically inconceivable power who has been the architect behind most of the events in the Spawn universe, holding significant knowledge of Al Simmons and his role as Spawn, knowledge that not even Mammon possesses and possessing powers beyond that of God or Satan. Her/his true form is as the \"Mother\" of Existence, though she/he is neither female nor male, it is able to cast an illusion to make her/his look male.\n\nIt is revealed that Mother/Man of Miracles is in fact the one who walked among mortals as \"Jesus Christ\", and thus was actually independent of God in the Spawn universe. The twelve heavenly warriors known as the Disciples, formed from the souls of the Twelve Apostles, actually follow Man of Miracles above God.\n\nMother gave each of her/his infinite children a world to run as they wish; God and Satan were both given Earth. God and Satan constantly bickered and fought to the point where they declared war on one another. Mankind, being created by God (from The Mother's energy) but given free will by Satan, became unique and Mother instantly fell in love with them and decided to act on their behalf, rather than let her/his children use them as cannon fodder. Mother stripped both of her/his children of their kingdoms and made them sleep in a forgotten corner of the universe. Mother then came to Earth as Jesus Christ, spreading a message of love and tolerance. This message being corrupted by mankind, Mother saw that Armageddon was inevitable. Giving mankind a chance to survive, Mother preserved all the souls that had died in the same hour as Al Simmons and placed them inside of Spawn. Spawn represents the potential of mankind and must prove that humanity is worth saving from God and Satan's feud. Mother brought back God and Satan as the human children of Terry and Wanda Fitzgerald in order to give them an appreciation for humanity and change their ways. This plan failed, as the twins simply became more insane than before and wreaked further chaos on Earth. They have since regained their memories, powers, and kingdoms and are bolstering their armies for the final push that will begin Armageddon.\n\nMother has also been revealed as being The Keeper of Greenworld, the voice of the Emerald Parliament and the one who originally summoned The Heap.\n\nM.O.M.'s appearance changes depending upon who is perceiving her/him. She/he has appeared as an anime-inspired hero, as Miracleman, as a mysterious woman covered in ivy who was presumably Gaia, Jesus Christ and, in her/his true form, as the Mother of Creation: a Caucasian skinned woman. When cloaked in her/his illusion, people see her/his as they want to, and she/he subsequently explains that this is because reality is far more malleable than humanity believes. In her/his anime guise, M.O.M.'s appearance changes consistently from panel to panel. The tattoos on her/his face are different each time, the logo on her/his chest appears and reappears and sometimes her/his shirt disappears but her/his logo remains visible on her/his skin.\n\nMan of Miracles was first introduced in \"Spawn\" #150. This character was very similar to the hero known as Miracleman (also known as Marvelman), popularized in the Eclipse Comics series written by Alan Moore and Neil Gaiman. In his original appearance he is named Mike Moran, the same human identity of Miracleman, with basically the same costume. Todd McFarlane bought the rights to Eclipse Comics believing Miracleman was also a part of the deal. However, the rights to the character of Miracleman have been heavily contested. In court a judge determined that McFarlane did not hold the rights to Miracleman. After this the two characters of Mother of Existence and Man of Miracles were combined into one where they were supposed to always be the same being.\n\n"}
{"id": "1970317", "url": "https://en.wikipedia.org/wiki?curid=1970317", "title": "Metal Slug 4", "text": "Metal Slug 4\n\n\"Metal Slug 4\" retains the same game-play as previous titles, with the addition of some new enemies, bosses, weapons, several new vehicles and a new bonus combo system. It was later ported to Microsoft Xbox (not compatible with Xbox 360) and Sony PlayStation 2 as a stand-alone game in Japan and Europe, and along with \"Metal Slug 5\" as a compilation in North America and South Korea. Nintendo Switch version was released in 2018.\n\nA bonus scoring system was added that allows the player to be rewarded depending on how many enemies are killed in the time allotted. The time allotted is determined from the type of emblem that is picked up. A time meter will appear on the top of the screen, and if the player lives through the end of the level, they will be awarded bonus points for badges that represent feats accomplished. Eri and Tarma were replaced with Nadia and Trevor.\n\nOne year after the events of Metal Slug 3 (timeline wise, also after Metal Slug 6, which takes place after 3) the world is trembling under the new threat of a mysterious but deadly cyber virus that threatens to attack and destroy any nation's military computer system. With Tarma and Eri unable to help due to their own assignments in the matter, Marco Rossi and Fiolina Germi are called in to investigate the situation and are joined by two newcomers, Nadia Cassel and Trevor Spacey. In their investigation, the group discovers that a rich terrorist organization known as the Amadeus Syndicate is behind the nefarious plot and has allied with General Morden's Rebel Army. They head into battle against Amadeus' forces, hoping to destroy the cyber virus before it gets the chance to wipe out the entire world's military computer system.\n\nHalfway through the game's story mode, the player is confronted by who they presume to be General Morden, however in the final stage they find an underground facility where android doubles of Morden are being manufactured. Allen O' Neil (who has appeared numerous times throughout the series) fights the player in this stage for the very last time, and is also revealed to be a machine replica. Finally, the player confronts the leader of the syndicate, Dr. Amadeus himself, who attacks with a series of powerful robots, but he is defeated and is trapped in his own devices as the base self-destructs, presumably killing him. If the player safely escapes the base's bonus explosion stage, the credits will show the main cast eating a feast of food, but if the player gets caught in the explosion, the player character(s) will appear in the hospital, bandaged and bed-ridden, being brought get-well gifts of food from Eri Kasamoto and Tarma Roving. After the credits, a single computer monitor is seen transmitting data to an unknown location before shutting down.\n\n\n\n\"Metal Slug 4\" was mixed to positive received by players with users scores of 7.8 for PS2, 7.3 for Xbox, 8.1 for Neo Geo and 8.3 for the arcade versions. While Metacritic and Gamrankings are given with 70.47% and 70 alongside with Metal Slug 5 as compilation for both PS2 and Xbox score.\n"}
{"id": "7174542", "url": "https://en.wikipedia.org/wiki?curid=7174542", "title": "Motive (law)", "text": "Motive (law)\n\nA motive, in law, especially criminal law, is the cause that moves people to induce a certain action. Motive, in itself, is not an element of any given crime; however, the legal system typically allows motive to be proven in order to make plausible the accused's reasons for committing a crime, at least when those motives may be obscure or hard to identify with. However, a motive is not required to reach a verdict. Motives are also used in other aspects of a specific case, for instance, when police are initially investigating.\n\nThe law technically distinguishes between motive and intent. \"Intent\" in criminal law is synonymous with \"Mens rea\", which means the mental state shows liability which is enforced by law as an element of a crime. \"Motive\" describes instead the reasons in the accused's background and station in life that are supposed to have induced the crime. Motives are often broken down into three categories; biological, social and personal.\n\nThere are two objections to motive when considering punishment. The first is volitional objection, which is the argument that the person cannot manage his or her own motives and therefore cannot be punished for them. The second objection is neutrality objection. This is based on the idea that our society has contrasting political opinions and therefore a government’s preference should be limited.\n\nThere are four different ways a defendants motive can be pertinent to his or her criminal liability. Motive can be fully inculpatory or exculpatory or only partially inculpatory or exculpatory. When one has acted with a specific motive, lawful behavior becomes illegal, and this is when motive is fully inculpatory. If illegal activity with a particular motive does not hold a defendant responsible then that motive is fully exculpatory. When a motive supplies inadequate defense to a crime, the motive is partially exculpatory. When a motive says the kind of infraction for which the defendant is responsible, the motive is partially inculpatory.\n\n"}
{"id": "21813", "url": "https://en.wikipedia.org/wiki?curid=21813", "title": "Naked News", "text": "Naked News\n\nNaked News is a Canadian news and entertainment program owned by Naked Broadcasting Network. It features naked women reading news bulletins derived from news wires. The show's production studio is located in Toronto, Canada. There are 6 new daily programs a week, that run approximately 22-minute in length. The female cast members read the news fully nude or disrobe as they present their various segments, including entertainment, sports, movies, food, sex and relationships. \"Naked News TV!\" is an offshoot of the web program and is broadcast on pay TV in various countries around the world.\n\nThe show often recruits women from around the world, to either appear as guest reporters, or to appear on a regular basis. Their audition feature, where amateur women try out for the program, is one of their most popular segments and generates the most feedback from its viewers. Another segment that generates a lot of interest is called Naked In The Streets. This is where a reporter will go topless in the street and speak to the general public about various topics.\n\nThe female announcers have been featured in almost every medium including television (\"CBS Sunday Morning\", \"The Today Show\", \"The View\", \"Sally Jessy Raphaël\", and numerous appearances on \"Entertainment Tonight\" and \"ET Insider\"), newspapers and magazines (\"TV Guide\", \"Playboy\"), and as guests on multiple radio shows, including Howard Stern.\n\n\"Naked News\" was conceived by Fernando Pereira and Kirby Stasyna and debuted in December 1999 as a web-based news service featuring an all-female cast. It began with only one anchor, Victoria Sinclair, who worked for the program until 2015. As the show grew the number of female anchors increased. Roxanne West joined Sinclair as a lead anchor, and other cast members included Holly Weston, Lily Kwan, Sandrine Renard, Erin Sherwood, Athena King, Brooke Roberts, Michelle Pantoliano, Erica Stevens, Samantha Page, Christine Kerr and Valentina Taylor, plus guest anchors. The website was popularized entirely by word of mouth, and quickly became a popular web destination. During the height of its popularity, the website was receiving over six million unique visitors per month. In the site's early days the entire newscast could be viewed for free online. The site was initially supported by advertising but this changed after the collapse of Internet advertising that occurred with the dot-com crash. By 2002 only one news segment could be viewed freely, and by 2004, no free content remained on the website. Beginning in 2005, a nudity-free version of \"Naked News\" was available to non-subscribers. Beginning in June 2008, two news segments could be viewed freely. However, this ended in December 2009.\n\nFollowing the success of \"The Naked Truth\", a similar show on Russian television, \"Naked News\" launched \"Naked News TV!\", the first Internet news program to successfully transfer from website to cable television. It was initially broadcast on Viewers Choice in Canada in 2001, and was first broadcast in the United States a few months later by the iN DEMAND cable TV service on its Too Much for TV pay-per-view network that also included Girls Gone Wild. In 2002 it was broadcast in Australia on The Comedy Channel via cable and satellite television platforms Foxtel and Austar. The British channel Sumo TV briefly showed episodes of \"Naked News\", while the free-to-view Playboy One broadcast the show at 9:30pm Mondays-Fridays until its closure in 2008.\n\n\"Naked News\" launched a Japanese version of the show in 2006. Japanese broadcasting regulations prohibited the presenters from being fully naked, allowing them only to strip to their underwear. In 2007 the Japanese government changed broadcasting guidelines to prevent the show receiving a subsidy for the section delivered in sign language.\n\nA male version of the show was created in 2001 to parallel the female version, but has ceased production as it did not enjoy the female version's popularity and fame. Although it was originally targeted towards female viewers (at one point said to be 30% of the website's audience), the male show later promoted itself as news from a queer perspective. The male version closed in 2007.\n\nIn the late 1990s, British cable television channel L!VE TV broadcast \"Tiffani's Big City Tips\", in which model Tiffani Banister gave the financial news while stripping to her underwear.\n\n\n\n\n"}
{"id": "32862313", "url": "https://en.wikipedia.org/wiki?curid=32862313", "title": "National Archives of Benin", "text": "National Archives of Benin\n\nThe National Archives of Benin are in Porto-Novo. The archives were established in 1914.\n\n\n"}
{"id": "27155424", "url": "https://en.wikipedia.org/wiki?curid=27155424", "title": "New Genres", "text": "New Genres\n\nNew Genres is an artistic movement begun in the early 20th century. The movement is marked by many notable artists who work with a variety of media such as video art, body art, installation, performance, and sound art.\n\nNew Genres art is commonly identified as installation art, video, film, sound, performance, Digital media, internet art, hybrid and emerging art forms. New Genres is a practice which begins with ideas and then moves to the appropriate form or media for that particular idea, sometimes inventing entirely new sites of cultural production, new methodologies, technologies, or genres in the process. New Genres gives emphasis to questioning preconceived notions of the role of art in culture and its relationship to a specific form or medium.\n\nWhile no recognized official date marks the beginning of the New Genre period, many believe that the early 20th-century work of the Dadaists and Futurists initiated this movement. Their work laid a foundation for the experimental practice of New Genres. Duchamp's \"Fountain\" (1917) introduced the idea of the readymade object: a non-art object which becomes viewed as art due to the intention and designation of the artist. This new use of artistic power and questioning of the art object opened up the conceptual sphere of New Genres. These discoveries illuminated the idea that art was possible outside the field of the classical genres like painting and sculpture.\n\nTo understand the movement of New Genres, it is important to look at the events occurring simultaneously in the 20th century. In the first half of the century, World War I broke out and caused devastation to much of Eastern Europe. In America, the Great Depression caused a major economic crises that affected many parts of the world. World War II quickly followed, splitting the world into two alliances: the Allies and the Axis. This war caused tremendous damage and introduced new tragedies into the world, such as the Holocaust and the first nuclear weapons.\n\n"}
{"id": "27171589", "url": "https://en.wikipedia.org/wiki?curid=27171589", "title": "Night Doctors", "text": "Night Doctors\n\nNight Doctors, also known as Night Riders, Night Witches, Ku Klux Doctors, and Student Doctors are bogeymen of African American folklore, with some factual basis. Emerging from the realities of grave robbing, enforced and punitive medical experimentation, and intimidation rumours spread maliciously by many Southern whites, the Night Doctors purpose was to further prevent slaves, Free Men, and black workers leaving for the North of the United States, in a prescient foreshadowing of the inevitable events during the early to mid 20th century, now known as The Great Migration.\n\nAfrican American folklore told of white doctors who would abduct, kill, and dissect, performing a plethora of experiments, referred to as \"Night Doctors\". These tales are difficult to verify. White slave owners disseminated scare stories of putative tortures performed by \"Night Doctors\" to prevent freed slaves from moving to the North. In order to augment rumours, white slave owners dressed in white sheets to represent kidnappers. Wandering the African American communities, perpetuating beliefs slaves would be abducted, taken to medical facilities and killed. \nTo many African Americans these \"Night Doctors\" weren't just fictions used as scare tactics; they were real life.\n\nIn the early 19th century, most states legislated against grave robbery. African Americans were powerless, voiceless, and unable to resist grave robberies to any meaningful extent. Dissection of white cadavers carried far greater risks for doctors. Nevertheless, poor whites were used, especially when available in abundance . Body snatching increased during the post-Revolutionary period in New York. Reflecting the impact of medical students performing dissections, rather than simply observing professors. Following outraged newspaper reporting, laws were passed attempting to circumvent the rapidly increasing incidence of grave robbery.\nFor example, Pennsylvania legislature required unclaimed bodies to be given to the state anatomy board, by public officials.\n\nAfrican Americans were the main source of dissection cadavers to the end of the civil war. Newly formed laws were openly flouted by medical schools as demonstrated in an 1824 advertisement for the Medical College of South Carolina:\n\nSome advantages of a peculiar character are connected with this institution, which it may be proper to point out. No place in the United States offers as great opportunities for the acquisition of anatomical knowledge. Subjects being obtained from the colored population in sufficient numbers for every purpose, and proper dissection carried out without offending any individuals in the community.\n\nExcavations at the Medical College of Georgia in 1989 yielded more than 9,000 bones, mainly from working class individuals. Approximately 80% of those were from African Americans. In addition to being the majority of cadavers, many Southern teaching hospitals would only perform new live surgical techniques and demonstrations on African-American patients.\n\nDon't you know white men taught them all that about ghosts? That was a way of keeping them down, keeping them under control.\n\nThere is a long history of whites using the supernatural to intimidate African Americans. During the slavery era, slave masters dressed as ghosts, rode around black communities on horseback. Later there were patrols called patterollers. After the Civil War, the Ku Klux Klan continued the \"Night Rider\" tradition. It was finally taken up by the Night Doctors.\n\nBetween the Civil War and the 1930s, labour agents were sent by Northern employers to recruit African Americans from the South. In addition to restrictive laws, Southern employers used rumour to intimidate the workforce into staying in the South. One of the most popular rumours concerned doctors roaming Northern streets nightly, killing African Americans for dissection purposes. White Southern employers would also maraude around black communities in white gowns, further spreading fear, anxiety and unease throughout neighbourhoods already overburdened with anguish.\n\nNew Orleans had an interesting variation on the Night Doctors called the \"Needle Men\". Thought to be medical students from Charity Hospital (now the Medical Center of Louisiana at New Orleans), the eponymous Needle Men, would poke unsuspecting individuals in the arm, resulting in death. Several explanations have been suggested, such as epilepsy and the 1924 case of a man who would poke women with a bayonet.\n\nI sure don't go out much at this time of year. You takes a chance just walkin' on the streets. Them Needle Mens is everywhere. They always comes 'round in the fall, and they's 'round to about March. You see, them Needle Mens is medical students from the Charity Hospital tryin' to git your body to work on. That's 'cause stiffs is very scarce at this time of the year.\nStudents at Charity Hospital were also referred to as \"Black Bottle Men\". The black bottle would be a poison given upon entrance to Charity Hospital, and the resulting death would allow dissection. It is now thought that the black bottle referred to cascara (Rhamnus purshiana) mixed with milk of magnesia, a diuretic commonly given to admitted patients of the era.\n\nCharity Hospital did not have sole responsibility for \"Needle Men\" or \"Black Bottle Men.\" Johns Hopkins Hospital was believed to be another source. These legendary figures were thought to kidnap African Americans from the street. A woman from the book \"The Immortal Life of Henrietta Lacks\" states that: \"You'd be surprised how many people disappeared in East Baltimore when I was a girl. I'm telling you, I lived here in the fifties when they got Henrietta, and we weren't allowed to go anywhere near Hopkins. When it got dark and we were young, we had to be \"on the steps\", or Hopkins might get us.\"\n\nIn 1979, when 25 African-American young men and boys disappeared in Georgia, the Night Doctors were blamed. In this incarnation, the Night Doctor removed internal organs for aphrodisiacs.\n\nThe discovery of the Tuskegee experiment in which doctors withheld treatment from 399 African-American men also revived the tale of the night doctors.\n\nA poem was created from the fears of the African Americans that applied more realism to the Night Doctor myths:\n<poem>\nTHE DISSECTING HALL\n\nYuh see dat house? Dat great brick house?\nWay yonder down de street?\nDey used to take dead folks een dar\nWrapped een a long white sheet.\n\nAn' sometimes we'en a nigger' d stop,\nA-wondering who was dead,\nDem stujent men would take a club\nAn' bat 'im on de head.\n\nAn' drag dat poor dead nigger chile\nRight een dat 'sectin hall\nTo vestigate 'is liver-lights-\nHis gizzardan' 'is gall.\n\nTek off dat nigger's han's an' feet-\nHis eyes, his head, an' all,\nAn' w'en dem stujent finish\nDey was nothin' left at all.\n</poem>\n\n"}
{"id": "566253", "url": "https://en.wikipedia.org/wiki?curid=566253", "title": "Open relationship", "text": "Open relationship\n\nAn open relationship is an intimate relationship which is consensually non-monogamous. This term may sometimes refer to polyamory, but it is often used to signify a primary emotional and intimate relationship between two partners who agree to have sexual relationships but not romantic relationships with other people. The nature of the openness in the relationship, including what outside sexual contact is permissible, varies widely.\nOpen relationships include any type of romantic relationship (dating, marriage, etc.) that is open. The concept of an open relationship has been recognized since the 1970s.\n\nTo a large degree, open relationships are a generalization of the concept of a relationship beyond monogamous relationships. A form of open relationship is the open marriage, in which the participants in a marriage have an open relationship.\n\nThere are several different styles of open relationships. Some examples include:\n\n\nThe term open relationship is sometimes used interchangeably with the closely related term polyamory, but the two concepts are not identical. \nThe main unifying element to open relationship styles is non-exclusivity of romantic or sexual relationships.\n\nSome believe that open relationships occur more frequently in certain demographics, such as the young rather than the old in America, including, more specifically, the college-educated middle-class, rather than the uneducated working-class, or people of certain ethnic and/or other racial minorities. Open relationships may also be more common among females rather than males, especially those in the same categories, such as college-educated, middle-class, white, younger Americans. This may be because women have more to gain by stressing this idea of equal rights, and that the women’s rights movement supports the idea of open relationships.\n\nA 1974 study showed that male students who either cohabit or live in a communal group are more likely to become involved in open relationships than females, and are still more interested in the concept than females even if not participating in open relationships. A survey taken by gay men's \"health and life magazine\", \"FS Magazine\", 41% of gay men interviewed have been in an open relationship and of the men who have been in open relationships, 27% believe that it is a good thing.\n\nMany couples within open relationships are dual-career, meaning that both primary partners have a stable job and/or a career. Both men and women in these, especially in closed groups, are also more likely to be in managerial jobs. Most also are either childfree, or post child-rearing.\n\nAn open relationship may form for various reasons. These include:\n\nMany couples consider open relationships, but choose not to follow through with the idea. If a person attempts to approach their committed monogamous partner about transitioning to an open relationship, the monogamous partner may convince or coerce them to either stay monogamous or pursue a new partner. There may also be concern that when beginning an open relationship, a partner may become only concerned in their personal development and pay less attention to their partner.\n\nJealousy is often present in monogamous relationships, and adding one or more partners to the relationship may cause it to increase. Results of some studies have suggested that jealousy remains a problem in open relationships because the actual involvement of a third party is seen as a trigger. In Constantine & Constantine (1971), the researchers found that 80% of participants in open marriages had experienced jealousy at one point or another.\n\nCultural pressure may also dissuade initiating or switching to an open relationship. There is a commonly held societal stereotype that those involved in open relationships are less committed or mature than those who are in monogamous relationships; and films, media, and self-help books present the message that to desire more than one partner means not having a \"true\" relationship. In the post-WWII 1950s-1970s, it was traditional to \"date around\" (with guidelines such not going out with one particular suitor twice in a row) until ready to start \"going steady\" (the onset of exclusivity and sexual exploration); since then, non-exclusive dating around has lost favour and going directly to steady (now known simply as exclusive dating) has been elevated instead. Desiring an open relationship is these days often claimed to be a phase that a person is passing through before being ready to \"settle down\". The logistics of an open relationship may be difficult to cope with, especially if the partners reside together, split finances, own property, or parent children.\n\nAny sexual contact outside of a strictly monogamous or polyfidelitous relationship increases the possibility that one member of the group will contract a sexually transmitted infection and pass it into the group.\n\nNeither barrier device use (such as condoms) nor more vigilant STI testing and vaccination can eliminate such risk, but can reduce the statistical increase attributable to nonmonogamy.\n\nOne of the most significant factors that aids a relationship in being successful is that it is about making the relationship fit the needs of all parties involved. No two open relationships will be the same, and the relationship will change due to the current circumstances at each specific moment. The style of the open relationship will mirror the parties' involved values, goals, desires, needs and philosophies.\n\nThe most successful relationships have been those that take longer to establish. By taking the time to develop a clear idea of what both partners want out of the openness of a relationship, it allows the parties involved to self-reflect, process their emotions, deal with possible conflicts, and (for those transitioning from monogamy to nonmonogamy) find ways to cope with the change.\n\nNegotiating the details of the open relationship is important throughout the communication process. Topics that are commonly found in negotiations between couples include honesty, the level of maintenance, trust, boundaries and time management.\n\nOther tools that couples utilize in the negotiation process include allowing partners to veto new relationships, prior permission, and interaction between partners. This helps to reassure each partner in the relationship that their opinion is important and matters. However, although ability to veto can be a useful tool in negotiation, a successful negotiation and open relationship can still occur without it. Some reject veto power because they believe it limits their partner from experiencing a new relationship and limits their freedom.\n\nTypes of boundaries include physical, which is along the lines of not touching someone without permission being given; sexual boundaries; and emotional boundaries, which is avoiding the discussion of specific emotions. Boundaries help to set out rules for what is and is not acceptable to the members of the relationship. They also help people to feel safe and that they are just as important in the open relationship as their partners.\n\nExamples of boundaries that are set could include:\n\nSome couples create a physical relationship contract. These can be useful in not only negotiating, but also clearly articulating the needs, wants, limits, expectations, and commitments that are expected of the parties involved.\n\nAdequate time management can contribute to the success of an open relationship. Even though having a serious commitment with one partner is common, negotiating the time spent among all partners is still important. Although the desire to give an unlimited amount of love, energy, and emotion to others is common, the limited amount of time in a day limits the actual time spent with each partner. Some find that if they cannot evenly distribute their time, they forego a partner. Time management can also be related to equity theory, which stresses the importance of fairness in relationships.\n\nSwinging is a form of open relationship in which the partners in a committed relationship engage in sexual activities with others at the same time. Swingers may regard the practice as a recreational or social activity that adds variety or excitement into their otherwise conventional sex lives or for curiosity. Swingers who engage in casual sex maintain that sex among swingers is often more frank and deliberative and therefore more honest than infidelity. Some couples see swinging as a healthy outlet and means to strengthen their relationship. Swinging can take place in various contexts, including spontaneous sexual activity involving partner swapping at an informal social gathering of friends, a formal swinger party or partner-swapping party, and a regular gathering in a sex club (or swinger club) or residence.\n\nPolyamory is the practice, desire, or acceptance of having more than one intimate relationship at a time with the knowledge and consent of everyone involved. It is often described as consensual, ethical, or responsible nonmonogamy. The word is sometimes used in a broader sense to refer to sexual or romantic relationships that are not sexually exclusive, though there is disagreement on how broadly it applies; an emphasis on ethics, honesty, and transparency all around is widely regarded as the crucial defining characteristic.\n\nWhile \"open relationship\" is sometimes used as a synonym for \"polyamory\" or \"polyamorous relationship\", the terms are not synonymous. The \"open\" in \"open relationship\" usually refers to the sexual aspect of a nonclosed relationship, whereas \"polyamory\" refers to the extension of a relationship by allowing bonds to form (which may be sexual or otherwise) as additional long-term relationships.\n\nThe terms \"polyamory\" and \"friends with benefits\" are fairly recent, having come about within the past few decades though non-monogamy has existed since prehistoric times.\n\nA subset of polyamory is group marriage or polyfidelity. This type of relationship functions as an expanded marriage, where no member is sexually or romantically involved with anyone other than the group's members.\n\n"}
{"id": "25490326", "url": "https://en.wikipedia.org/wiki?curid=25490326", "title": "Parochial political culture", "text": "Parochial political culture\n\nA parochial political culture is a political culture where citizens have only limited awareness of the existence of central government. \n"}
{"id": "13187140", "url": "https://en.wikipedia.org/wiki?curid=13187140", "title": "Psychic distance", "text": "Psychic distance\n\nPsychic distance is a perceived difference or distance between objects. The concept is used in aesthetics, international business and marketing, and computer science.\n\nPsychic distance is made up of the Greek word \"psychikos – ψυχικός\", an adjective referring to an individual's mind and soul, and \"distance\" which implies differences between two subjects or objects. Some therefore argue that the concept exists in the mind's eye of the individual and it is their subjective perception that uniquely determines \"psychic distance\". As a result it is often viewed as a humanistic reflection of individual acuity and not a collective, organisational or societal perspective. However, in the international business context, psychic distance is frequently measured in terms of national averages or in terms of the national-level differences that influence those perceptions.\n\nIn his book, King refers to his preference to use the term \"aesthetic distance\" rather than psychic distance, as he feels the latter term has misleading connotations in current usage.\n\nIn 1912 Cambridge's Edward Bullough wrote of it in a long paper entitled, \"Psychical Distance as a factor in Art and an Aesthetic Principle\" which appeared in the British Journal of Psychology. In this he set down in a reasonably complete manner the concept as it applied to the arts.\n\nEvidently, he successfully influenced thinkers 50 years later. Donald Sherburne, for example, says, \"Edward Bullough's psychical distance has become \"a classic doctrine of aesthetic thinking.\" And James L. Jarrett writes of Bullough's ideas, \"Perhaps no more influential idea has been introduced into modern aesthetics than that of psychical distance.\"\n\nThe psychical distance construct has been used as an intercultural theme by the arts in the study of creative detachment between East and West. Despite such cameo appearances in other fields, the concept has been essentially \"operationalised\" by business with the marketing function acting as the chief curator.\n\nIn international business (IB) and marketing settings, psychic distance is based on \"perceived\" differences between a home country and a \"foreign\" country regardless of physical time and space factors which differ across diverse cultures. It is a subjective type of distance (\"perceived differences\") unlike the distances forming the CAGE framework, for instance . This makes psychic distance very difficult to measure, and often times fallacious proxies are used to estimate it (e.g., Kogut & Singh (1988) index using Hofstede's (1980) cultural dimensions). More accurate approaches rely on asking decision-makers their perceptions towards different host countries (e.g., Hakanson & Ambos, 2010), or using its antecedents to estimate it (psychic distance stimuli: Dow & Karunaratna, 2006). \n\nThe business origins of the \"psychic distance\" idiom can be traced back to research conducted by Beckerman (1956) and Linnemann (1966). As a fully formed concept Vahlne and Wiedersheim-Paul (1973) as cited by Nordstrom and Vahlne (1992) described psychic distance as \"factors preventing or disturbing the flow of information between potential or actual suppliers and customers.\" These factors are associated with country-based diversities and dissimilarities and can be grouped into four clear areas:\n\n\nAlthough the concept was fully formed by the early 1970s, it was the study of Nordic multinationals by Johanson and Vahlne (1977) which followed on from two earlier studies in 1975 and 1976, that is generally accepted as the concept's real genesis. The studies concluded that a firm's international activities relate directly to psychic distance and that further international expansion progresses into markets with successively greater psychic distance.\n\nIn summary, companies tend to initially export to countries that they understand; then build on their acquired experience to explore opportunities further afield. In other words, firms enter new markets where they are able to identify opportunities with low market uncertainty then enter markets at successively greater psychic distance. As a consequence, contemporary literature on the internationalisation process cites psychic distance as a key variable and determinant for expansion into foreign markets.\n\nThe psychic distance has also on occasion been measured through formative indicators such as strong commercial relations, close political relations, historic ties, geographic ties, social ties, country information stock, level of development. These indicators are deeply analyzed for the international market selection. The measurement of these indicators is best accomplished through an index construction. However, the formative indicators are very context dependent. If they were applied to different industries, countries and entry modes, the indicators would have shifted dramatically.\n\nMore recently, Gairola and Chong incorporated psychic distance to simulate more realistic noise models in spatial games, leading to interesting results, including a manifestation of the psychic distance paradox.\n\n\n"}
{"id": "26950856", "url": "https://en.wikipedia.org/wiki?curid=26950856", "title": "Registration of architects in the United Kingdom", "text": "Registration of architects in the United Kingdom\n\nIn the United Kingdom, the Architects Act 1997 imposes restrictions on the use of the name, style or title \"architect\" in connection with a business or a professional practice, and for that purpose requires a statutory Register of Architects to be maintained. The Architects Registration Board constituted under the Act is responsible for Architects Registration in the United Kingdom and is required to publish the current version of the Register annually. Every person who is entitled to be registered under the Act has the right to be entered in the Register. The Act consolidated previous enactments originating with the Architects (Registration) Act, 1931 as amended by the Architects Registration Act 1938. It applies to England, Wales, Scotland and Northern Ireland.\n\nSection 2 of the Act prescribes that the Board shall appoint and regulate the functions ascribed to the Registrar. The Act refers to the Registrar by the masculine pronoun in the singular, but by the usual rules of statutory interpretation, this is not limited to an individual male person.\n\nAn under the European Communities Act 1972 came into force on 20 June 2008.\n\nThe recurring controversy about whether statutory protection of title serves useful purposes has been intensified by the legislative impact of the EU Directive on Unfair Commercial Practices implemented in May 2008 by two Statutory Instruments under the European Communities Act 1972, namely No 1276 (Trade Descriptions) and No 1277 (Consumer Protection).\n\nFor the purposes of the Legislative and Regulatory Reform Act 2006, \"regulatory function\" is defined in subsection 32(2).\n\nUnder subsection 20(1) of the Architects Act 1997, a person in the United Kingdom may only practise or carry on business under any name, style or title containing the word \"architect\" if registered. There is no restriction on its use in any other circumstance. The words in the current Act follow those of the 1938 Architects Registration Act under which it was decided that the use of the suffix \"FRIBA\" in business notepaper constituted an infringement.\n\nBy subsection 20(3) corporate bodies, firms or partnerships can carry on business under a name, style or title containing the word \"architect\" provided that (in broad terms) the architectural business is run by a registered person. However the statutory registration Board may (by rules made under subsection 20(4) – see General Rules, Rule 25) effectively limit the application of subsection 20(3) to those corporate bodies firms or partnerships who have supplied information necessary for determining whether the architectural business is run by a registered person.\n\nThe rule-making power under subsection 20(4) appears to be limited to prescribing particular information to be provided to the Board viz. \"such information necessary for statutory registration determining whether [subsection 20(3)] applies\". The subsection makes no provision for levying any fee.\n\nOpinions had been divided for well over a century about the merits of statutory registration of architects in the United Kingdom. The result was that Parliament, as the legislator and guided by the government of the day, has had to maintain a state of benevolent neutrality among the holders of these contending views, consistent with more general public policies for business competition, employment, professional education and so on. In relation to statutory protection of title, three aspects of the field in which architects practise invite examination. In summary:\n\n\nIn the light of experience since the inception of the Register under the 1931 Act, and more particularly under the Architects Registration Board’s regime from 1997, the recurring question has been whether protection of title serves useful purposes in respect of the three aspects mentioned above. The question of obsolescence has been further intensified by the EU Unfair Commercial Practices Directive, effective from 2007.\n\nStatutory registration had its origin within the architectural profession in the latter part of the nineteenth century. It was then (as now) a matter of controversy. However, by 1905 the RIBA had established a policy to secure satisfactory training of architects by statutory means.\n\nThe basis of the policy (on registration) had always been that the profession was governed by voluntary associations of practising architects and that the profession would retain control of registration. This was reflected in the composition of the registration body (the Architects' Registration Council of the United Kingdom - ARCUK) established by the 1931 Act. Shortly after, in the book published on occasion of the Institute's centenary celebration in 1934, in the concluding paragraphs of the chapter on statutory registration., Harry Barnes FRIBA., Chairman of the Registration Committee, wrote -\n\nAfter more than half a century times have changed and a regime of quite another kind has been installed under the 1997 Act.\n\nBy the 1990s it was almost universally accepted that the time had come to bring the statutory Architects' Registration Council as it then was to an end. Opinion within the profession was divided among those who held that statutory registration should be discontinued altogether and those who held that the registration body should be reconstituted.\n\nIn the event the body was reconstituted as the Architects Registration Board (1996/1997 Acts). But it was only after the event that many in the profession came to appreciate the effect of the new requirement that the majority of the Board should be non-architects and appointed by the government, as the following quotation shows:\n\n1834 - The body which was to become the Royal Institute of British Architects (originally known as the Institute of British Architects in London) was formed by Thomas de Grey, 2nd Earl de Grey and several prominent architects. (After the grant of the charter it had become known as the Royal Institute of British Architects in London, eventually dropping the reference to London in 1892.)\n\n1837 - It was granted its Royal Charter under William IV.\n\n1884 - Society of Architects formed, after a campaign by a group of ARIBA to be allowed to vote on RIBA affairs had been resisted by FRIBA.\n\n1887 - Architects and Engineers Registration Act Committee formed as an independent committee to promote a bill for registration of architects, engineers and surveyors. The bill was withdrawn after chief bodies representing engineers petitioned against it.\n\n1889 & 1891 - Architects Registration Bill Committee put forward bills for registration of architects, which were strongly supported by the Society of Architects but opposed by an independent group of prominent architects and artists.\n\n1892 - Papers published, defining the profession of architecture:\n\n1902 - Architects Registration Bill Committee amalgamated with the Society of Architects as a joint Registration Committee.\n\n1905 - RIBA Education Policy was adopted for statutory powers to secure satisfactory training for architects by way of registration of title, by and through the RIBA.\n\n1908 - RIBA Licentiate Class formed, for architects who could show evidence of competence, without exams. On closure in 1913, over 2000 had been accepted.\n\n1924-1959 - RIBA Standing Registration Committee.\n\n1925 - Amalgamation of RIBA and Society of Architects: most of the Society of Architects members transferred to Licentiate class, which was reopened.\n\n1927 - RIBA Registration Committee has draft bill introduced in Parliament, but opposed by the Incorporated Association of Architects and Surveyors and the Faculty of Architects and Surveyors.\n\n1931 - Bill recast and enacted as the Architects (Registration) Act 1931, enabling the Register of Architects to be established under a statutory body called the Architects' Registration Council of the United Kingdom (ARCUK). The Council was to be made up of representatives of all architectural bodies in the United Kingdom in proportion to the numbers of their memberships on the Register, and representatives from government departments and related professional bodies. Under ARCUK, the RIBA system of exams etc. was accepted for registration. (The provisions of the Act constituting the Board of Architectural Education were repealed when ARCUK was reconstituted as ARB in 1996/7.)\n\n1937 - A letter is sent by \"the president of the council\" [sic] to the Institute of Chartered Surveyors recognizing the fact that there was nothing in the Bill which the council was then promoting (and which subsequently became the Architects Registration Act, 1938) to interfere with the activities of registered architects. The letter is mentioned by\nLord Goddard,Lord Chief Justice, in the course of his judgment in the Queen's Bench Divisional Court in 1957 allowing the appeal of an architect (Hughes) against a professional miscondct decision of the Discipline Committee of ARCUK (later renamed Architects Registration Board), a case which becomes cited in later cases and in legal text-books as a judicial precedent.\n\n1938 - The Architects Registration Act, 1938 changed the protected title from \"Registered Architect\" to \"Architect\".\n\n1957 - Resulting from the 1938 legislation, an appeal by an architect against ARCUK is allowed by the High Court in a case which becomes a judicial precedent, \"Hughes v. Architects' Registration Council of the United Kingdom\": \"It is not of itself disgraceful to disagree with a majority view and to act accordingly. It is only if a man has bound himself in honour to accept that view and to act according to the code that a deliberate breach of the code for his own profit can be called disgraceful.\"—Devlin J. (Patrick Devlin, Baron Devlin)\n\n1992 - Government, in response to a request from ARCUK, commissioned review of the Architects Registration Acts by an independent assessor (John Warne).\n\n1993 - Warne Report published - principal recommendation: abolition of protection of title \"architect\" and disbanding of ARCUK. RIBA Council initially supported this recommendation, but this was resisted by the RIBA membership. As a result RIBA campaigned for the retention of protection of title with a \"stream-lined\" registration board.\n\n1996 - Part III of Housing Grants, Construction and Regeneration Act 1996, among other things, reconstituted the registration body as the Architects Registration Board (ARB).\n\n1997 - Architects Act 1997, a consolidating act, brought together the provisions of Part III of the 1996 Act and previous registration legislation. The Architects Registration Board then established with a majority of appointed lay members and a minority of elected Architect members.\n\n2008 - made in June 2008 by Statutory Instrument established rules for the recognition of professional qualifications enabling migrants from the European Economic Area or Switzerland to register as architects in the United Kingdom. It also set out provisions for facilitating temporary and occasional professional services cross-border.\n\nThe following analysis of the operative and other parts of the Architects Act 1997 as it was before the pays attention to details which sometimes go unnoticed.\n\nThe Act is fairly short. That is partly due to its conciseness, but this quality makes it all the more necessary to remember that the Act must be read as a whole to ascertain the meaning and effect of its various parts. Care is needed not to read into it what is not there (whatever conventional wisdom may have supposed or desired), and not to fail to notice what actually is there. In particular, like many such Acts, it can be better understood by looking at its beginning (Arrangement of Sections and long title) and its end (derivations), as well as the operative part in between.\n\nIts long title is \"An Act to consolidate the enactments relating to architects\". The Table of Derivations printed at the end lists the enactments which it consolidated; and Schedule 3 lists the originating and two amending Acts which it repealed, namely: the Architects (Registration) Act 1931, the Architects Registration Act 1938 and parts of the Housing Grants, Construction and Regeneration Act 1996.\n\nThe unbroken continuity of these enactments is shown by paragraph 19(2)(a) of Schedule 2:\n\nThe changes made by the 1996 Act to the originating Act as amended can be deduced from the Table of Derivations. This also shows that, for the purpose of the consolidation, certain definitions were inserted in the \"Interpretation\" section. These included one to make clear that where there is a reference to \"unacceptable professional conduct\", it has the same meaning as it has in section 14 (not vice versa). In subsection 14(1) the phrase is expanded as: \"conduct which falls short of the standard required of a registered person\".\n\nThe scheme of the consolidation Act of 1997 is identical with that of the originating Act of 1931, as amended. It is as follows:\n\nHere, the first is described as the \"general burden\"; the other two as the \"voluntary burdens\"; and the freedom to choose the \"statutory choices\".\n\nThe general burden is a prohibition imposed on all persons, firms, partnerships and bodies corporate carrying on business in the United Kingdom, including architects and Chartered Architects. The prohibition is against practising or carrying on business under the title of \"architect\", with two exceptions, viz.:\n\nHere, the first of these is described as a \"practice volunteer\", and the second as a \"business volunteer\".\n\nThe statutory choice is exercisable:\n\nThe voluntary burdens which result from the exercise of the statutory choice are as follows:\n\nIn consequence:\n\nA side effect of the Act is the imposition of burdens on third parties under Part II, namely, Schools of Architecture, but the effect of the changes made by the 1996 Act is that Schools of Architecture have disappeared from the legislation without trace. The result has been a certain amount of wrangling between the Schools, the ARB and the RIBA which is the principal professional body, whose concerns inevitably include architectural education.\n\nAnother side effect has been a claim by the ARB to be able to impose on registered persons certain requirements about Professional Indemnity Insurance.\n\nThe membership of the Board is the result of one of the changes made by the 1996 Act to the registration body's previous constitution when its name had been the Architects' Registration Council of the United Kingdom. The Act abolished the Board of Architectural Education, renamed the Council as the Board, and made this Board consist predominantly of persons appointed by the Privy Council.\n\nSubsection 3(4) states \"The Board shall publish the current version of the Register annually...\". Provisions of Part II of the Act prescribe how the Register shall be kept up to date, and who shall be entitled to be registered; other provisions of Parts II and III prescribe for the Board the circumstances, events or conditions when a person's name shall be removed from the Register; and other provisions prescribe for the Board certain ancillary, or derivative and secondary, duties in connection with the Board's primary responsibility for the maintenance and regular publication of the Register of Architects.\n\nApart from officers, employees and agents of the Board, the Act creates no duties or obligations towards the Board which fall on any one else at all; and nothing in the Act itself creates any obligation which an architect owes to the Board.\n\nSection 20 of the Architects Act 1997 mentions \"architecture\" (subsection (3)(b)) and the \"services\" of a person enrolled under the Act (subsection (5)). These are not defined by the Act, and it is quite obvious that Parliament has not delegated to the Board the power to define them. In practice, as technology and the building process continue to evolve new specialisms, the concept of architecture can be seen as becoming ever more fluid, extensive and comprehensive, and at the same time becoming narrower while ever more ancillary specialities are identified.\n\nSanctions are available to the Board under Part III of the Act against any person on the register who is found guilty under section 15 of unacceptable professional conduct or serious professional incompetence, or else has been convicted with a criminal offence which has relevance to fitness to practise.\n\nAlthough a criminal conviction is an objective criterion, no statutory definition is given that defines the level of professional conduct or incompetence that will attract a sanction, judgment in the matter being given to the Board's Professional Conduct Committee, subject to commonsense, reason and judicial review.\n\nThe PCC is constituted under the Act, Schedule 1 Part II, as amended. The Committee includes members of the Board, both elected and appointed, as well as persons appointed by the Board and nominees of the President of the Law Society. A bare quorum of the Committee meets for a disciplinary hearing, comprising a nominee of the President of the Law Society (invariably a solicitor), a person from the Register and another not on the Register. PCC members are paid for their service.\n\nThe Architects Registration Board is admittedly not a professional body or society in the sense explained in Wikipedia article \"British Professional Bodies\". The question whether or not it is a \"regulatory agency\" was publicly considered in 2003. The June 2003 issue of the \"RIBA Journal\" included the following:\n\nIn November 2003 the Architects Registration Board published a summary of a barrister's opinion which included the following:\n\nThe Royal Institute of British Architects, which is a professional body (see Wikipedia category list British Professional Bodies), operates a voluntary \"Chartered Practice\" scheme. The use of the title \"Chartered Practice\" is authorised under Article 4.7 of the RIBA Charter (see reference below). The Institute's website explains that the scheme requires all architectural work of a Chartered Practice to be under the supervision of a Chartered Architect. A directory listing Chartered Practices has been published annually by the Institute from 2007.\n\nThe standard of qualification is not equal for all persons applying to be included in the Register of Architects.\n\nThe Registrar is bound, under section 4 of the Architects Act, to register any person who applies for registration if that person has \"such qualifications and has gained such practical experience as may be prescribed\".\n\nAn alternative route to registration is to satisfy the Board that an equivalent standard of competence has been achieved. A matrix can be applied as follows:\n\nThe disparity arises from the European Directive on Mutual Recognition of Qualifications in Architecture 85/384/EC. (Go to Article 4 of the main text for the required duration of training. The original Directive has been updated.) It is clearly stated in that Directive that \"the total length of education and training shall consist of a minimum of either four years of full-time studies at a university or comparable educational establishment or at least six years of study at a university or comparable educational establishment of which at least three must be full time\".\n\nAs this is a minimum requirement there is nothing to stop a country applying higher standards to those obtaining qualifications and experience within its own jurisdiction. However it is widely held (and expressed in the report of Michael Highton to the RIBA Council) that any challenge to this disparity is likely to succeed on the grounds of irrationality. The report stated:\n\nIf the Architects Registration Board reduced the registration requirement to four years full-time study, there is no reason why the RIBA should lower its entry standard.\n\nAt a time when there had occurred a hiatus in production under the legislation of printed copies of the Register in the annual series from 1933, the Architects Registration Board made a new departure and in 2012 announced an online version of the Register which registered persons were invited to use for demonstrating their professionalism to members of the public and to non-registered competitors.\n\n"}
{"id": "49423318", "url": "https://en.wikipedia.org/wiki?curid=49423318", "title": "Religious tourism in India", "text": "Religious tourism in India\n\nReligious tourism in India is focus of Narendra Modi's national tourism policy. Uttarakhand has been popular as a religious and adventure tourism hub.\n\nMahesh Sharma said, \"In view of religious tourism, Ramdevera and Tanot would also be linked to a tourism circuit in western Rajasthan.\"\n\nReligious tourism has been one of the reason of developing India. India is widely known for its wonderful religious places which are nowhere seen in world. Many places like Kashi Vishwanath, Omkareshwar, Mahakaleshwar, Gangotri, Yamunotri, Badrinath, Kedarnath of lord Shiva are most visited places in India. Even people from USA and UK come to India to visit these places.\n\n"}
{"id": "324741", "url": "https://en.wikipedia.org/wiki?curid=324741", "title": "Replicator (Star Trek)", "text": "Replicator (Star Trek)\n\nIn \"Star Trek\" a replicator is a machine capable of creating (and recycling) objects. Replicators were originally seen used to synthesize meals on demand, but in later series they took on many other uses.\n\nAlthough previous sci-fi writers had speculated about the development of \"replicating\" or \"duplicating\" technology, the term \"replicator\" was not itself used until \"\". In simple terms, it was described as a 24th century advancement from the 23rd century \"food synthesizer\" seen in \"\". In Star Trek the original series food was created in various colored cubes. In the animated series (1974), various types of realistic looking food could be requested as in the episode entitled \"The Practical Joker\". The mechanics of these devices were never clearly explained on that show. The subsequent prequel series, \"\", set in the 22nd century, featured a \"protein resequencer\" that could only \"replicate certain foods,\" so an actual chef served on board who used \"a hydroponic greenhouse\" where fruits and vegetables were grown. Additionally, that ship had a \"bio-matter resequencer\" which was used to recycle waste product into usable material.\n\nAccording to an academic thesis: \"The so-called 'replicators' can reconstitute matter and produce everything that is needed out of pure energy, no matter whether food, medicaments, or spare parts are required.\" A replicator can create any inanimate matter, as long as the desired molecular structure is on file, but it cannot create antimatter, dilithium, latinum, or a living organism of any kind; in the case of living organisms, non-canon works such as the \"Star Trek: the Next Generation Technical Manual\" state that, though the replicators use a form of transporter technology, it's at such a low resolution that creating living tissue is a physical impossibility.\n\nIn its theory it seems to work similarly to a universal assembler.\n\nA replicator works by rearranging subatomic particles, which are abundant everywhere in the universe, to form molecules and arrange those molecules to form the object. For example, to create a pork chop, the replicator would first form atoms of carbon, hydrogen, nitrogen, etc., then arrange them into amino acids, proteins, and cells, and assemble the particles into the form of a pork chop.\n\nThis process requires the destructive conversion of bulk matter into energy and its subsequent reformation into a pre-scanned matter pattern. In principle, this is similar to the transporter, but on a smaller scale. However, unlike transporters, which duplicate matter at the quantum level, replicators must be capable of a large number of different materials on demand. If patterns were to be stored at the quantum level, an impossible amount of data storage (or a set of original copies of the materials) would be required. To resolve this, patterns are stored in memory at the molecular level.\n\nThe drawback of doing so is that it is impossible to replicate objects with complicated quantum structures, such as living beings, dilithium, or latinum. In reality, living beings and/or cited elements are not necessarily more complex on a quantum level; the putative 'extra complexity' is used an in-universe function to head off questions such as 'why can't Starfleet replicate people?' In the TNG episode \"\", aliens used their version of replicators to create a Picard impostor. Additionally, read/write errors cause a number of single-bit errors to occur in replicated materials. Though usually undetectable to human senses, computer scanning can be used to reveal these discrepancies, and they may explain the frequent complaint (by some gourmets and connoisseurs) that replicated food and beverages suffer from substandard taste. These errors also may cause a nontoxic material to become toxic when replicated, or create strains of deadly viruses and bacteria from previously harmless ones.\n\nOne of the most important pieces of technology in the \"Star Trek\" universe, the replicator is used primarily to provide food and water on board starships, thus eliminating the need to stock most provisions. (Starships, starbases, and other installations stock some provisions for emergency use, in case of replicator failure or an energy crisis.) On \"\", it was established that as long as there is an energy source to power life support, replication is used to provide breathable air on ships and starbases (and to disassemble the carbon dioxide exhaled by the crew), thus providing a seemingly endless supply of oxygen and eliminating the need to carry air tanks.\n\nThe technology is also used for producing spare parts, which makes it possible to repair most ship damage without having to return to a starbase. Other applications include replication of Starfleet uniforms, as well as everyday objects such as toys and souvenirs. Replication is also used by the Holodeck program to allow food, clothes, and other objects belonging within a simulation to be used or consumed by the participants.\n\nStarfleet's safety protocols prevent unauthorized replication of dangerous objects, such as weapons and poisonous substances.\n\nReplicators can also convert matter into energy. Following that principle, the device can dismantle any object into subatomic particles. The ensuing energy can then be stored for future use or immediately applied in a subsequent replication. This process is referred to as \"recycling\", and is applied to everything from dirty dishes to outgrown children's clothes.\n\nReplicator technology, even if produced on a larger scale, cannot be used to create complex objects such as shuttlecraft or starships (the production staff felt that being able to replicate entire starships \"at the push of a button\" would severely impact dramatic potential). However, in the \"Star Trek: Deep Space Nine\" episode , industrial replicators are used to replicate large components of ships, shuttlecraft, and other pieces of this sort, which are later used in shipyards to construct such vessels. In this manner, as few as 15 industrial replicators are enough to replicate the components needed to build a fleet of starships or to help a civilization recover from a planet-wide natural disaster.\n\nBy virtually eliminating material scarcity, replicator technology plays an important role in the moneyless human economy within the \"Star Trek\" universe.\n\nWhen the USS \"Voyager\" was pulled to the Delta Quadrant, it became clear that replicator technology was unknown to some of the indigenous peoples of that region.\nThroughout the first seasons, the Kazon and other races tried repeatedly to obtain the technology.\n\nCaptain Janeway feared that if this technology were acquired by a civilization before they were ready, disastrous consequences could ensue. For this reason, and because of the Prime Directive, Janeway refused to give away the technology at any price.\n\nAlso on \"Voyager\", the ship's energy constraints on the journey back to the Alpha Quadrant meant that replicator supplies had to be strictly controlled, leading to \"replicator rations\" becoming an unofficial ship currency. This is also the reason Neelix (aside from providing the crew with a morale boost through the preparation of fresh food) became employed as the ship's chef. Some ingredients came from the ship's hydroponics laboratory.\n\nIn 2014, researchers at Nestlé were reported as working on technology comparable to the replicator, with the goal of providing food tailored to an individual's nutritional requirements.\n\nIn 2015, a \"Star Trek\"-inspired Replicator-Emulator is proposed to robotically grow, print or assemble not only food, but also shelter, energy, transportation and even whole towns. #WPProjects assigned 250 renewable automation projects - one project to every country in the world - and it also laid out the important social programs needed to protect incomes and bolster economies as societies completed their 2-year Renewable Automations projects. \n\nIn comparison, 3D printers, which are now a mainstream technology and have a range of impressive and important capabilities (including shaping prostheses or making organs) are decidedly different, in that they do not create material \"ex nihilo\" (out of nothing), or perhaps more accurately, out of nuclei or atoms or programmed patterns of information, but instead, like regular printers, must use already pre-existing corporal material.\n\nAlso, 3D printers are limited in the materials that they can print. Currently only materials that can be easily fused together via extrusion or sintering processes can be used by 3D printer technology — generally plastics, metals, and clays. However food, concrete, and a few other materials have been successfully printed on a limited scale.\n\nImperial College London physicists have discovered how to create matter from light - a feat thought impossible when the idea was first theorised 80 years ago. In just one day in Imperial's Blackett Physics Laboratory, three physicists worked out a relatively simple way to physically prove a theory first devised by scientists Breit and Wheeler in 1934.\n\nBeeHex, an Ohio startup company, received a grant in 2013 from NASA intended for developing long-spaceflight food 3D printing technology. They now build food printing robots for eventual public use.\n\nCemvita Factory Inc., a biotech startup based in Houston, TX, is also developing a photobioreactor that converts Carbon Dioxide that's captured from air along with Hydrogen from hydrolyzing water to nutrients and pharmaceutics.. . \n\n\n"}
{"id": "177882", "url": "https://en.wikipedia.org/wiki?curid=177882", "title": "Sealab 2021", "text": "Sealab 2021\n\nSealab 2021 is an American adult animated television series created by Adam Reed and Matt Thompson. It was shown on Cartoon Network's late-night programming block, Adult Swim. Cartoon Network aired the show's first three episodes in December 2000 before the official inception of the Adult Swim block on September 2, 2001, with the final episode airing on April 24, 2005. \"Sealab 2021\" is one of the four original Williams Street series that premiered in 2000 before Adult Swim officially launched, the others being \"Aqua Teen Hunger Force\", \"The Brak Show\" and \"Harvey Birdman, Attorney at Law\".\n\nMuch like Adult Swim's \"Space Ghost Coast to Coast\", the animation used stock footage from a 1970s Hanna-Barbera cartoon, in this case the short-lived, environmentally-themed \"Sealab 2020\", along with original animation. The show was a satirical parody of the original \"Sealab\" series, and the conventions of 1970s animated children's series generally. While there was initial resistance from several of the original series' creators to the reuse of their characters, production moved forward on the series. \"Sealab 2021\" was produced by 70/30 Productions, which eventually closed on January 9, 2009.\n\nAdam Reed and Matt Thompson, the creators and writers of \"Sealab 2021\", came up with the idea for the show in 1995 while they were production assistants for Cartoon Network. They stumbled on a tape of the show \"Sealab 2020\", and wrote new dialogue. Cartoon Network passed on the show because they did not believe it was funny. Five years after quitting Cartoon Network, the two went back to the original tape, this time making the characters do what they wanted. Cartoon Network bought the show, coincidentally around the same time that Adult Swim was created. The original \"pitch pilot\" is available on the Season 1 DVD as a special feature.\n\nVery few of the episodes of the series share any continuity or ongoing plot. For instance, the entire installation is destroyed at the end of many episodes, and crew members are often killed in horrible ways, only to return in the following episode. There are occasional running gags, such as the \"Grizzlebee's\" restaurant chain, the character of Sharko, and Prescott, the half-man, half-tentacle monster \"from the network.\" It contains many references to the pop culture of the 1980s–2000s, and makes use of other cartoons from the 1970s besides that on which it is based, such as 1973's \"Butch Cassidy\" for the on-screen appearances of the Sealab writers, and various one-off appearances of other characters.\nPaul Di Filippo of the website \"Sci Fi Weekly\", in his review of the Season 3 DVD, felt that general fan opinion of the show declined sharply following the death of Harry Goz (the voice of Captain Hazel Murphy) during Season 3. After four seasons, the final episode aired on April 25, 2005.\n\n\n\n\n\n\n\n\n\n\n\nIn January 2009, IGN listed \"Sealab 2021\" as the 79th best in the \"Top 100 Animated Series\". In 2013 IGN placed \"Sealab 2021\" as number 22 on their list of Top 25 Animated series for adults.\n\n\n"}
{"id": "8334092", "url": "https://en.wikipedia.org/wiki?curid=8334092", "title": "Sense about Science", "text": "Sense about Science\n\nSense about Science is a UK charity that promotes the public understanding of science. Sense about Science was founded in 2002 by Lord Taverne, Bridget Ogilvie and others to promote respect for scientific evidence and good science. Sense about Science was established as a charitable trust in 2003, with 14 trustees, an advisory council and a small office staff. Tracey Brown has been the director since 2002.\n\nSense about Science aims to work with scientists, journalists and others to ensure that scientific evidence is at the forefront of public discussions about science, and to correct unscientific misinformation. They encourage and assist scientists to engage in public debates about their area of expertise, to respond to scientifically inaccurate claims in the media, to help people contact scientists with appropriate expertise, and to prepare briefings about the scientific background to issues of public concern.\n\nSense about Science publishes guides to different areas of science in partnership with experts. These include: Making Sense of Uncertainty, Making Sense of Allergies, Making Sense of Drug Safety Science, Making Sense of Crime, Making Sense of Statistics, Making Sense of Screening and Making Sense of GM.\n\nSense about Science maintains database of over 6,000 UK scientists willing to use their expertise to help inform public debate. It also runs the \"Voice of Young\" Science programme to help early career scientists engage in public debates. Sense About Science hosts an annual lecture.\n\nSince its founding, Sense about Science has contributed to UK public debates about such subjects as alternative medicine, \"detoxification\" products and detox diets, genetically modified food, avian influenza, chemicals and health, \"electrosmog\", vaccination, weather and climate, nuclear power, and the use and utility of peer review. Sense about Science encourages scientists to explain to the public the value of peer review in determining which reports should be taken seriously. Director Tracey Brown describes such critical thinking as crucial to preventing public health scares based on unpublished information.\n\nSense about Science hosts an annual lecture. Past speakers have been:\n\nSense about Science also promotes the \"John Maddox Prize\":\n\nThe John Maddox Prize for standing up for science rewards an individual who has promoted sound science and evidence on a matter of public interest. Its emphasis is on those who have faced difficulty or hostility in doing so. Nominations of active researchers who have yet to receive recognition for their public-interest work are particularly welcomed. \n\nPast winners are: \n\nThe AllTrials campaign calls for all past and present clinical trials to be registered and their full methods and summary results reported.\n\nAllTrials is an international initiative of Bad Science, \"BMJ\", Centre for Evidence-based Medicine, Cochrane Collaboration, James Lind Initiative, \"PLOS\" and Sense About Science and is being led in the US by Sense About Science USA, Dartmouth’s Geisel School of Medicine and the Dartmouth Institute for Health Policy & Clinical Practice.\n\nAs of January 2018, the AllTrials petition has been signed by 91,989 people and 737 organisations.\n\nAsk for Evidence was launched by Sense About Science in 2011. It is a campaign that helps people request for themselves the evidence behind news stories, marketing claims and policies. When challenged in this way, organisations may withdraw their claims or send evidence to support them. The campaign is supported by more than 6000 volunteer scientists who are available to review the evidence provided and determine whether it supports the original claim or story. The campaign has received funding from The Wellcome Trust and is endorsed by figures such as Dara Ó Briain and Derren Brown.\n\nChris Peters, Scientific Affairs Manager at Sense About Science, has said, 'If you ask people for evidence then they expect to be asked for evidence in the future'. This can lead to a cultural shift in the organisation; they make sure they have the evidence from the start and are more evidence-based.\n\nSense About Science launched the Keep Libel Laws out of Science campaign in June 2009 in defence of a member of its board of trustees, author and journalist Simon Singh, who has been sued for libel by the British Chiropractic Association. They issued a statement entitled \"The law has no place in scientific disputes\", which was signed by many people representing science, medicine, journalism, publishing, arts, humanities, entertainment, sceptics, campaign groups and law. In April 2010, the BCA lost this case with the court accepting that criticism of the BCA concerning its promotion of bogus treatments was fair comment.\n\nIn December 2009, Sense About Science, Index on Censorship and English PEN launched the Libel Reform Campaign. The Defamation Act 2013 received Royal Assent on 25 April 2013 and came into force on 1 January 2014.\n\nThe Trust actively campaigns in support of various causes. It has issued a statement signed by over 35 scientists asking the WHO to condemn homeopathy for diseases such as HIV.\n\nSense about Science and their publications have been cited a number of times in the popular press,\nmost notably for encouraging celebrities and the public to think critically about scientific claims,\ncriticizing marketing unsupported by research,\ndecrying the unsubstantiated claims of homeopathy,\nsupporting genetically modified crops,\ncriticising 'do-it-yourself' health testing,\ndenouncing detox products,\nwarning against 'miracle cures',\nand promoting public understanding of peer review.\nThey have received positive coverage in publications from the Royal Society\nand the U.S. National Science Foundation,\nand in the writings of scientists such as Ben Goldacre and Steven Novella.\n\nLord Taverne, chairman of Sense About Science, has criticised campaigns to ban plastic bags as counter-productive and being based on \"bad science\".\nAnti-genetic-modification campaigners and academics have criticised Sense About Science for what they view as a failure to disclose industry connections of some advisers,\nand \"Private Eye\" reported that it had seen a draft of the \"Making Sense of GM\" guide that included Monsanto Company's former director of scientific affairs as an author. \nTracey Brown, managing director of Sense About Science, rebutted these claims on the SAS website.\n\nJournalist George Monbiot has commented on the connections Tracey Brown, Dr Michael FitzPatrick, assistant director Ellen Raphael and others working with Sense About Science have with the former Revolutionary Communist Party and \"Living Marxism\" magazine. \nClaims of a Living Marxist 'network' have been denied.\n\nHomeopath Peter Fisher criticised Sense About Science, who have been working closely with NHS primary care trusts on the issue of funding for homeopathy, for being funded by the pharmaceutical industry; SAS responded in a statement to Channel 4 News that \"Peter Fisher's desperate comments show about as much grasp of reality as the homeopathic medicine he sells.\"\n\nSense about Science's principal funding is through grants and donations in respect of its core aims. In its funding policy it states: \"Donations do not entitle any individual or organisation to decision-making authority. External funding will not divert Sense About Science from its agreed aims and values\".\n\nFunding for the trust has been increasing. Some is derived from industrial organizations engaged in scientific dispute, clinical trials and research for which SAS is supportive (e.g. genetically modified crops) as well as major publishing houses. For example, for the fiscal year ending 5 April 2008, the trust received £145,902 in donations. Disclosed corporate donations comprised £88,000 with pharmaceutical company Astra Zeneca donating £35,000. Previous donations included other pharmaceutical industries such as Pfizer. This dependency has now been diminished since for the fiscal year ending April 2010, the trust received £183,971 in donations of which £17,500 was derived from the pharmaceutical industry (Unilever and G E Healthcare). In 2011 the amount diminished further to less than 6% funding derived from industry sources (the trust received £268,184 in donations with £15,000 from industry) with the rest derived from Science Bodies and individuals.\n\nSense about Science discloses the names of its funders and the purpose of each donation received in its annual reports. Its largest donor during 2015-16 was the Laura and John Arnold Foundation, which donated between £40,001-50,000 to support the AllTrials campaign.\n\nAs of November 2015, the trustees of Sense About Science are:\n\nThe charity has an advisory board, which includes: Professor John Adams, Mr Richard Ayre, Professor Peter Atkins, Professor Sir Colin Berry FMedSci, Professor Colin Blakemore FMedSci FRS, Professor Gustav Born FRS, Professor Sir Robert Boyd FMedSci, Professor John Coggins, Professor Phil Dale OBE, Professor Adrian Dixon FMedSci, Dr Simon Festing, Dr Ron Fraser, Mr David Allen Green, Dr Irene Hames, Dr Evan Harris, Lord Hunt of Chesterton FRS, Lord Jenkin of Roding, Professor Trevor M Jones CBE, Sir David King FRS, Professor Sir Peter Lachmann FRS FMedSci, Dr Stephen Ladyman, Ms Prue Leith OBE, Dr Robin Lovell-Badge FRS, Professor Julian Ma, Professor Alan Malcolm, Professor Vivian Moses, Professor Sir Keith Peters FRS PMedSci, Lord Plumb of Coleshill DL, Dr Ian Ragan, Dr Matt Ridley FMedSci, Professor Raymond Tallis FMedSci, Professor Anthony Trewavas FRS, Lord Turnberg of Cheadle FMedSci, Dr Roger Turner, Professor Simon Wessely FMedSci and Professor Michael Wilson.\n\n"}
{"id": "2157582", "url": "https://en.wikipedia.org/wiki?curid=2157582", "title": "Smock-frock", "text": "Smock-frock\n\nA smock-frock or smock is an outer garment traditionally worn by rural workers, especially shepherds and waggoners, in parts of England and Wales from throughout the 18th century. Today, the word smock refers to a loose overgarment worn to protect one's clothing, for instance by a painter.\n\nThe traditional smock-frock is made of heavy linen or wool and varies from thigh-length to mid-calf length. Characteristic features of the smock-frock are fullness across the back, breast, and sleeves folded into \"tubes\" (narrow unpressed pleats) held in place and decorated by smocking, a type of surface embroidery in a honeycomb pattern across the pleats that controls the fullness while allowing a degree of stretch.\n\n\nIt is uncertain whether smock-frocks are \"frocks made like smocks\" or \"smocks made like frocks\"—that is, whether the garment evolved from the smock, the shirt or underdress of the medieval period, or from the frock, an overgarment of equally ancient origin. What is certain is that the fully developed smock-frock resembles a melding of the two older garments.\n\nFrom the earlier 18th century, the smock-frock was worn by waggoners and carters; by the end of that century, it had become the common outer garment of agricultural labourers of all sorts throughout the Midlands and Southern England. The spread of the smock-frock matches a general decrease in agricultural wages and living standards in these areas in the second half of the 18th century. The smocks were cheaper than other forms of outer garments, and were both durable and washable.\n\nEmbroidery styles for smock-frocks varied by region, and a number of motifs became traditional for various occupations: wheel-shapes for carters and wagoners, sheep and crooks for shepherds, and so on. Most of this embroidery was done in heavy linen thread, often in the same color as the smock.\n\nBy the mid-19th century, wearing of traditional smock-frocks by country laborers was dying out, although Gertrude Jekyll noticed them in Sussex during her youth, and smocks were still worn by some people in rural Buckinghamshire into the 1920s. As the authentic tradition was fading away, a romantic nostalgia for England's rural past, as epitomized by the illustrations of Kate Greenaway, led to a fashion for women's and children's dresses and blouses loosely styled after smock-frocks. These garments are generally of very fine linen or cotton and feature delicate smocking embroidery done in cotton floss in contrasting colors; smocked garments with pastel-colored embroidery remain popular for babies.\n\nDuring World War II, military parachutists wore wind proof jump smocks primarily to cover equipment that may have caused the parachutist to be stuck in a narrow doorway. German parachutists wore the Knochensack, British parachutists wore the Denison smock whilst US Marine paramarines wore a jump smock as well. Today the name \"smock\" is still used for military combat jackets, particularly in the UK; in the Belgian army the borrowed English term has been corrupted to smoke-vest.\n\nExamples include DPM Parachute Smock, that replaced the Denison Smock, the Canadian Para Smock and Smock Windproof DPM.\n\nThe Walloon bleu sårot, is a dark blue smock worn by men in parts of Belgium as part of National dress.\n\nThe Lèine bhàn was a type of smock worn to church by Scottish men who had broken the law.\n\n\n"}
{"id": "20071599", "url": "https://en.wikipedia.org/wiki?curid=20071599", "title": "Swishing", "text": "Swishing\n\nSwishing refers to swapping an item or items of clothing or shoes or an accessory with friends or acquaintances. Parties must willingly give an item to participate in the transaction, once they have given an item they are free to choose something of interest from what others have offered. Value does not come into the equation, as swappers do not necessarily get an item of equal value and are free to choose anything that the other person if offering (without having to pay). Swishing is now being more widely practiced throughout the world and has evolved to include other items such as books and furniture. Not only do people hold 'swishing parties' either as a charity event or simply for the enjoyment of it (while recycling and saving money at the same time), but there are several websites where online swishes take place.\n\nSwishing (deriving from the dictionary definition of 'to rustle, as silk' - which in the eyes of the Swishing team means 'to rustle from friends') began in 2000 when Lucy Shea, founder of green PR firm Futerra and her colleagues wanted to come up with a way to combine a love of retail shopping without contributing to increased consumption. It has since been imbursed by fashion model Twiggy in Twiggy's frock exchange, and as a result has become one of the most popular ways of swapping clothes.\n\nA Swishing Party usually has a Swishing Host who lays out guests' items at the venue after the guests have arrived. The Host sends one group of guests to look at the items while other groups wait or do an activity. When an allocated amount of time has passed, the groups rotate. When all guests have seen the items, the Host places everyone's name into a hat or bowl and draws them one at a time; when a guest's name is called, he or she claims an item. The Host waits for only a few seconds before calling out the next name. The remainder of the items are them donated to charity by the Host.\n\nThere are several types of variations. If the Swish is for charity, attendees have their names placed into the bowl along with an extra amount for a donation; Swishes can be focused on specific items like clothes, crockery, shoes, handbags, and accessories; they can be held for family, friends, or workmates; they can be less orderly, as a free-for-all instead of a drawing by name.\n\n"}
{"id": "2732718", "url": "https://en.wikipedia.org/wiki?curid=2732718", "title": "Technology integration", "text": "Technology integration\n\nTechnology integration is the use of technology tools in general content areas in education in order to allow students to apply computer and technology skills to learning and problem-solving. Generally speaking, the curriculum drives the use of technology and not vice versa. Technology integration is defined as the use of technology to enhance and support the educational environment. Technology integration in the classroom can also support classroom instruction by creating opportunities for students to complete assignments on the computer rather than with normal pencil and paper.\n\"Curriculum integration with the use of technology involves the infusion of technology as a tool to enhance the learning in a content area or multidisciplinary setting... Effective integration of technology is achieved when students are able to select technology tools to help them obtain information in a timely manner, analyze and synthesize the information, and present it professionally to an authentic audience. The technology should become an integral part of how the classroom functions—as accessible as all other classroom tools. The focus in each lesson or unit is the curriculum outcome, not the technology.\"\n\nIntegrating technology with standard curriculum can not only give students a sense of power, but also allows for more advanced learning among broad topics. However, these technologies require infrastructure, continual maintenance and repair – one determining element, among many, in how these technologies can be used for curricula purposes and whether or not they will be successful. Examples of the infrastructure required to operate and support technology integration in schools include at the basic level electricity, Internet service providers, routers, modems, and personnel to maintain the network, beyond the initial cost of the hardware and software.\n\nStandard education curriculum with an integration of technology can provide tools for advanced learning among a broad range of topics. Integration of information and communication technology is often closely monitored and evaluated due to the current climate of accountability, outcome based education, and standardization in assessment.\n\nTechnology integration can in some instances be problematic. A high ratio of students to technological device has been shown to impede or slow learning and task completion. In some, instances dyadic peer interaction centered on integrated technology has proven to develop a more cooperative sense of social relations. Success or failure of technology integration is largely dependent on factors beyond the technology. The availability of appropriate software for the technology being integrated is also problematic in terms of software accessibility to students and educators. Another issue identified with technology integration is the lack of long-range planning for these tools within the educative districts they are being used.\n\nTechnology contributes to global development and diversity in classrooms while helping to develop upon the fundamental building blocks needed for students to achieve more complex ideas. In order for technology to make an impact within the educational system, teachers and students must access to technology in a contextual matter that is culturally relevant, responsive and meaningful to their educational practice and that promotes quality teaching and active student learning.\n\nThe term 'educational technology' was used during the post World War II era in the United States for the integration of implements such as film strips, slide projectors, language laboratories, audio tapes, and television. Presently, the computers, tablets, and mobile devices integrated into classroom settings for educational purposes are most often referred to as 'current' educational technologies. It is important to note that educational technologies continually change, and once referred to slate chalkboards used by students in early schoolhouses in the late nineteenth and early twentieth centuries. The phrase 'educational technology', a composite meaning of technology + education, is used to refer to the most advanced technologies that are available for both teaching and learning in a particular era.\n\nIn 1994 federal legislation for both the Educate America Act and the Improving America's School's Act (IASA) authorized funds for state and federal educational technology planning. One of the principal goals listed in the Educate America Act is to promote the research, consensus building, and systemic changes needed to ensure equitable educational opportunities and high levels of educational achievement for all students (Public Law 103-227). In 1996 the Telecommunications Act provided a systematic change necessary to ensure equitable educational opportunities of bringing new technology into the education sector. The Telecomm Act requires affordable access and service to advanced telecom services for public schools and libraries. Many of the computers, tablets, and mobile devices currently used in classrooms operate through Internet connectivity; particularly those that are application based such as tablets. Schools in high-cost areas and disadvantaged schools were to receive higher discounts in telecom services such as Internet, cable, satellite television, and the management component.\n\nA chart of \"Technology Penetration in U.S. Public Schools\" report states 98% percent of schools reported having computers in the 1995–1996 school year, with 64% Internet access, and 38% working via networked systems. The ratio of students to computers in the United States in 1984 stood at 15 students per 1 computer, it now stands at an average all-time low of 10 students to computer. From the 1980s on into the 2000s, the most substantial issue to examine in educational technology was school access to technologies according to the 1997 Policy Information Report for Computers and Classrooms: The Status of Technology in U.S. Schools. These technologies included computers, multimedia computers, the Internet, networks, cable TV, and satellite technology amongst other technology-based resources.\n\nMore recently ubiquitous computing devices, such as computers and tablets, are being used as networked collaborative technologies in the classroom. Computers, tablets and mobile devices may be used in educational settings within groups, between people and for collaborative tasks. These devices provide teachers and students access to the World Wide Web in addition to a variety of software applications.\n\nNational Educational Technology Standards (NETS) served as a roadmap since 1998 for improved teaching and learning by educators. As stated above, these standards are used by teachers, students, and administrators to measure competency and set higher goals to be skillful.\n\nThe Partnership for 21st Century Skills is a national organization that advocates for 21st century readiness for every student. Their most recent Technology Plan was released in 2010, \"Transforming American Education: Learning Powered by Technology\". This plan outlines a vision \"to leverage the learning sciences and modern technology to create engaging, relevant, and personalized learning experiences for all learners that mirror students' daily lives and the reality of their futures. In contrast to traditional classroom instruction, this requires that students be put at the center and encouraged to take control of their own learning by providing flexibility on several dimensions.\"\nAlthough tools have changed dramatically since the beginnings of educational technology, this vision of using technology for empowered, self-directed learning has remained consistent.\n\nThe integration of electronic devices into classrooms has been cited as a possible solution to bridge access for students, to close achievement gaps, that are subject to the digital divide, based on social class, economic inequality, or gender where and a potential user does not have enough cultural capital required to have access to information and communication technologies. Several motivations or arguments have been cited for integrating high-tech hardware and software into school, such as (1) making schools more efficient and productive than they currently are, (2) if this goal is achieved, teaching and learning will be transformed into an engaging and active process connected to real life, and (3) is to prepare the current generation of young people for the future workplace. The computer has access to graphics and other functions students can use to express their creativity. Technology integration does not always have to do with the computer. It can be the use of the overhead projector, student response clickers, etc. Enhancing how the student learns is very important in technology integration. Technology will always help students to learn and explore more.\n\nMost research in technology integration has been criticized for being atheoretical and ad hoc driven more by the affordances of the technology rather than the demands of pedagogy and subject matter. Armstrong (2012) argued that multimedia transmission turns to limit the learning into simple content, because it is difficult to deliver complicated content through multimedia.\n\nOne approach that attempts to address this concern is a framework aimed at describing the nature of teacher knowledge for successful technology integration. The technological pedagogical content knowledge or TPACK framework has recently received some positive attention.\n\nAnother model that has been used to analyze tech integration is the SAMR framework, developed by Ruben Puentedura. This model attempts to measure the level of tech integration with 4 the levels that go from Enhancement to Transformation: Substitution, Augmentation, Modification, Redefinition.\n\nConstructivism is a crucial component of technology integration. It is a learning theory that describes the process of students constructing their own knowledge through collaboration and inquiry-based learning. According to this theory, students learn more deeply and retain information longer when they have a say in what and how they will learn. Inquiry-based learning, thus, is researching a question that is personally relevant and purposeful because of its direct correlation to the one investigating the knowledge.\nAs stated by Jean Piaget, constructivist learning is based on four stages of cognitive development. In these stages, children must take an active role in their own learning and produce meaningful works in order to develop a clear understanding. These works are a reflection of the knowledge that has been achieved through active self-guided learning. Students are active leaders in their learning and the learning is student-led rather than teacher–directed.\n\nMany teachers use a constructivist approach in their classrooms assuming one or more of the following roles: facilitator, collaborator, curriculum developer, team member, community builder, educational leader, or information producer.\n\nIs technology in the classroom needed, or does it hinder students' social development? We've all seen a table of teenagers on their phones, all texting, not really socializing or talking to each other. How do they develop social and communication skills? Neil Postman (1993) concludes: The role of the school is to help students learn how to ignore and discard information so that they can achieve a sense of coherence in their lives; to help students cultivate a sense of social responsibility; to help students think critically, historically, and humanely; to help students understand the ways in which technology shapes their consciousness; to help students learn that their own needs sometimes are subordinate to the needs of the group. I could go on for another three pages in this vein without any reference to how machinery can give students access to information. Instead, let me summarize in two ways what I mean. First, I'll cite a remark made repeatedly by my friend Alan Kay, who is sometimes called \"the father of the personal computer.\" Alan likes to remind us that any problems the schools cannot solve without machines, they cannot solve with them. Second, and with this I shall come to a close: If a nuclear holocaust should occur some place in the world, it will not happen because of insufficient information; if children are starving in Somalia, it's not because of insufficient information; if crime terrorizes our cities, marriages are breaking up, mental disorders are increasing, and children are being abused, none of this happens because of a lack of information. These things happen because we lack something else. It is the \"something else\" that is now the business of schools.\n\nInteractive whiteboards are used in many schools as replacements for standard whiteboards and provide a way to allow students to interact with material on the computer. In addition, some interactive whiteboards software allow teachers to record their instruction.\n\nInteractive Whiteboards are another way that technology is expanding in schools. By assisting the teacher to helping students more kinestically as well as finding different ways to process there information throughout the entire classroom.\n\nStudent response systems consist of handheld remote control units, or response pads, which are operated by individual students. An infrared or radio frequency receiver attached to the teacher's computer collects the data submitted by students. The CPS (Classroom Performance System), once set, allows the teacher to pose a question to students in several formats. Students then use the response pad to send their answer to the infrared sensor. Data collected from these systems is available to the teacher in real time and can be presented to the students in a graph form on an LCD projector. The teacher can also access a variety of reports to collect and analyze student data. These systems have been used in higher education science courses since the 1970s and have become popular in K-12 classrooms beginning in the early 21st century.\n\nAudience response systems (ARS) can help teachers analyze, and act upon student feedback more efficiently. For example, with polleverywhere.com, students text in answers via mobile devices to warm-up or quiz questions. The class can quickly view collective responses to the multiple-choice questions electronically, allowing the teacher to differentiate instruction and learn where students need help most.\n\nCombining ARS with peer learning via collaborative discussions has also been proven to be particularly effective. When students answer an in-class conceptual question individually, then discuss it with their neighbors, and then vote again on the same or a conceptually similar question, the percentage of correct student responses usually increases, even in groups where no student had given the correct answer previously.\n\nAmong other tools that have been noted as being effective as a way of technology integration are podcasts, digital cameras, smart phones, tablets, digital media, and blogs.\n\nMobile learning is defined as \"learning across multiple contexts, through social and content interactions, using personal electronic devices\". A mobile device is essentially any device that is portable and has internet access and includes tablets, smart phones, cell phones, e-book readers, and MP3 players. As mobile devices become increasingly common personal devices of K-12 students, some educators seek to utilize downloadable applications and interactive games to help facilitate learning. This practice can be controversial because many parents and educators are concerned that students would be off-task because teachers cannot monitor their activity. This is currently being troubleshooted by forms of mobile learning that require a log-in, acting as a way to track engagement of students.\n\nAccording to findings from four meta analyses, blending technology with face-to-face teacher time generally produces better outcomes than face-to-face or online learning alone. Research is currently limited on the specific features of technology integration that improve learning. Meanwhile, the marketplace of learning technologies continues to grow and vary widely in content, quality, implementation, and context of use.\n\nResearch shows that adding technology to K-12 environments, alone, does not necessarily improve learning. What matters most to implementing mobile learning is how students and teachers use technology to develop knowledge and skills and that requires training. Successful technology integration for learning goes hand in hand with changes in teacher training, curricula, and assessment practices.\n\nAn example of teacher professional development is profiled in Edutopia's Schools That Work series on eMints, a program that offers teachers 200 hours of coaching and training in technology integration over a two-year span. In these workshops teachers are trained in practices such as using interactive whiteboards and the latest web tools to facilitate active learning. In a 2010 publication of Learning Point Associates, statistics showed that students of teachers who had participated in eMints had significantly higher standardized test scores than those attained by their peers.\n\nIt can keep students focused for longer periods of time. The use of computers to look up information/ data is a tremendous time saver, especially when used to access a comprehensive resource like the Internet to conduct research. This time-saving aspect can keep students focused on a project much longer than they would with books and paper resources, and it helps them develop better learning through exploration and research.\n\nDefinition: Project Based Learning is a teaching method in which students gain knowledge and skills by working for an extended period of time to investigate and respond to an authentic, engaging and complex question, problem, or challenge.\n\nProject Based Activities is a method of teaching where the students gain knowledge and skills by involving themselves for the more period of time to research and respond to the engaging and complex questions, problems, or challenges. the students will work in groups to solve the problems which are challenging.The students will work in groups to solve the problems which are challenging, real, curriculum based and frequently relating to more than one branch of knowledge. Therefore, a well designed project based learning activity is one which addresses different student learning styles and which does not assume that all students can demonstrate their knowledge in a single standard way.\n\nThe project based learning activities involves four basic elements.\n\nThe term \"hunt\" refers to finding or searching for something. \"CyberHunt\" means an online activity which learners use the internet as tool to find answers to the question's based upon the topics which are assigned by someone else. Hence learners also can design the CyberHunt on some specific topics. a CyberHunt, or internet scavenger hunt, is a project-based activity which helps students gain experience in exploring and browsing the internet. A CyberHunt may ask students to interact with the site (e.g.: play a game or watch a video), record short answers to teacher questions, as well as read and write about a topic in depth. There are basically two types of CyberHunt: \n\nIt is an inquiry oriented activity in which most or all of the information used by the learners which are drawn out by the internet/web. It is designed to use learner 'time well', to focus on using information rather than on looking for it and to support the learners to think at the level of analysis, synthesis, and evaluation. It is the wonderful way of capturing student's imagination and allowing them to explore in a guided, meaningful manner. It allow the students to explore issues and find their own answers.\n\nThere are six building blocks of webQuests:\nWebQuests are student-centered, web-based curricular units that are interactive and use Internet resources. The purpose of a webQuest is to use information on the web to support the instruction taught in the classroom. A webQuest consists of an introduction, a task (or final project that students complete at the end of the webQuest), processes (or instructional activities), web-based resources, evaluation of learning, reflection about learning, and a conclusion.\n\nThe Web-based Inquiry Science Environment (WISE) provides a platform for creating inquiry science projects for middle school and high school students using evidence and resources from the Web. Funded by the U.S. National Science Foundation, WISE has been developed at the University of California, Berkeley from 1996 until the present. WISE inquiry projects include diverse elements such as online discussions, data collection, drawing, argument creation, resource sharing, concept mapping and other built-in tools, as well as links to relevant web resources.It is the research-focused, open-source inquiry-based learning management system that includes the student- learning environment project authoring environment, grading tool, and tool and user/ course/ content management tools.\n\nA virtual field trip is a website that allows the students to experience places, ideas, or objects beyond the constraints of the classroom.\nA virtual field trip is a great way to allow the students to explore and experience new information. This format is especially helpful and beneficial in allowing schools to keep the cost down. Virtual field trips may also be more practical for children in the younger grades, due to the fact that there is not a demand for chaperones and supervision. Although, a virtual field trip does not allow the children to have the hands on experiences and the social interactions that can and do take place on an actual field trip. An educator should incorporate the use of hands on material to further their understanding of the material that is presented and experienced in a virtual field trip.It is a guided exploration through the www that organizes a collection of pre- screened, its thematically based web pages into a structure online learning experience\n\nAn ePortfolio is a collection of student work that exhibits the student's achievements in one or more areas over time. Components in a typical student ePortfolio might contain creative writings, paintings, photography, math explorations, music, and videos. And it is a collection of work developed across varied contexts over time. The portfolio can advance learning by providing students and/or faculty with a way to organize, archive and display pieces of work.\n"}
{"id": "461325", "url": "https://en.wikipedia.org/wiki?curid=461325", "title": "Thomas Fisher Rare Book Library", "text": "Thomas Fisher Rare Book Library\n\nThe Thomas Fisher Rare Book Library is a library in the University of Toronto, constituting the largest repository of publicly accessible rare books and manuscripts in Canada. The library is also home to the university archives which, in addition to institutional records, also contains the papers of many important Canadian literary figures including Margaret Atwood and Leonard Cohen.\n\nRichard Landon, the director until his death in 2011, organized two or three exhibitions of rare books and other materials annually.\n\nThe Department of Rare Books and Special Collections was founded in November 1955 by the Chief Librarian, Robert H. Blackburn. Blackburn hired Marion E. Brown who was working in the special collections department at Brown University. Brown's first responsibility was to deal with the items that had been accumulating since 1890. Some of these items in the collection included medieval manuscripts, early printed books, and special volumes of later periods that had been presented by Queen Victoria to the University. Between the accumulated items and items found in the stacks of the main library, there was enough to open up the Rare Book Room in 1957.\n\nThe Department of Rare Books and Special Collections and the University Archives didn't have a permanent home until 1973 when the Thomas Fisher Rare Book library was opened. The library is named in honour of Thomas Fisher (1792–1874), who immigrated from Yorkshire, settled along the Humber River in 1822, and became a successful merchant–miller. In 1973 his great-grandsons, Sidney and Charles Fisher, donated to the library their own collections of Shakespeare, various twentieth-century authors, and etchings of Wenceslaus Hollar. Since the opening of the library, it has grown to approximately 740,000 volumes and 4,000 meters of manuscript holdings.\n\nThe Fisher building was designed by Mathers and Haldenby, Toronto with design consultant Warner, Burns, Toan and Lunde, New York. It forms part of a complex with the John P. Robarts Research Library for the Humanities and Social Sciences, and the Claude Bissel Building which houses the Faculty of Information.\n\nAmong the collection's items are the \"Nuremberg Chronicle\" (1493), Shakespeare's First Folio (1623), and Newton's \"Principia\" (1687). Contrary to widespread internet claims, \nthe library does not have Darwin's proof copy with annotations of \"On the Origin of Species\" (1859); the library does however have annotated proof sheets of: \"The Power of Movement in Plants\"\n, \"The Expression of the Emotions in Man and Animals\"\n, and \"The Effects of Cross and Self Fertilisation in the Vegetable Kingdom\" \n. Other collections include Babylonian cuneiform tablet from Ur (1789 BC), 36 Egyptian papyrus manuscript fragments (245 BC), and Catholicon (1460).\n\nThe Robert S. Kenny Collection resides in the library. This immense collection of books, documents, and other materials pertaining to the radical and labour movements, particularly in Canada, contains approximately 25,000 items collected by Robert S. Kenny, who was a member of the Communist Party of Canada. The Canadian section, which has 382 books and 768 pamphlets, was acquired by the library from Kenny in 1977. The international section of the collection was donated by Kenny in 1993.\n\nIn addition, there is an outstanding collection of etchings by Wenceslaus Hollar (1607–1677) from the collection of Sidney Thomson Fisher. The collection has been digitized and is a remarkable historical resource.\n\nIn April 2018, it was announced that the library had acquired the oldest English-language book in Canada, and its 15 millionth item, known as the \"Caxton Cicero\", which was printed in 1481 by the Englishman William Caxton.\n\n"}
{"id": "96449", "url": "https://en.wikipedia.org/wiki?curid=96449", "title": "Tukoio", "text": "Tukoio\n\nTukoio is the name of a legendary or mythological chief in Māori mythology.\n\nIn a story from the Māori tradition of the Whanganui area, Tukoio, a mortal man, came across a Maero or Mohoao, a wild person or monster much feared in Maori legend. The creature instantly attacked him, fighting fiercely until Tukoio cut off its limbs and head, which he took as a trophy back to his village. However, when the severed head cried out for help from its clan, Tukoio instantly dropped the head and ran, fearing retaliation. When he and some of his villagers returned, they found the Mohoao gone, having reassembled itself and returned to the forest. \n\n"}
{"id": "5207309", "url": "https://en.wikipedia.org/wiki?curid=5207309", "title": "Vitralism", "text": "Vitralism\n\nMultiple artists have used the Vitralism art movement in the last 10 years, see Flickr group.\n\nVitralism is 21st-century art movement that uses broken color (similar to Impressionism) and line extension to achieve a \"stained glass\" look and effect. Light and dark colors are painted in many layers with round sponges (the kind used for a stencil) giving the broken color a transparent quality that simulates the light through a stained glass window. \nsee example painting. The background is done entirely with sponges. The subject may or may not be painted with sponges; brushes are used to paint the subject to create texture contrast.\n\nLine extensions contribute to integrate the painted subject with its background. Acrylic paints are the preferred medium for its fast-drying quality, which prevents the mixing of colors on the canvas and keeps the colors almost pure and separated.\n\nHere are several photos of paintings created using this method\n\nThe name Vitralism comes from the Spanish word which means stained-glass. The word vitral comes from the Latin root , which means glass.\n\nSome of the characteristics of Vitralism painting include: \n\nWhile experimenting with acrylic colors for the first time, artist Mele Flórez Avellán started testing the paints on a masonite board. With a base of semi-gloss house paint, she started to apply color to her subject. To her discontent, the brush-applied strokes didn’t stick on the support the way she expected. It was not an opaque covering but semi-transparent. Since Mele already took the time to design her painting, she decided to look around her work area to see what could be used to make this a successful painting. Being a crafty person throughout her whole life, her work area was filled with all sorts of gadgets. She took a stencil sponge, dipped it in paint and lightly touched the support with the sponge. She liked what she saw. The addition of white, silver and gold paints gave the painting more light. The line extensions were then darkened to create a strong contrast.\n\n\nVitralism uses mainly subjects from nature, like birds, leaves and butterflies.\n\n"}
{"id": "15723651", "url": "https://en.wikipedia.org/wiki?curid=15723651", "title": "Vote trading", "text": "Vote trading\n\nVote trading is the practice of voting in the manner another person wishes on a bill, position on a more general issue, or favored candidate in exchange for the other person's vote in the manner one wishes on another position, proposal, or candidate. Nearly all voting systems do not make vote trading a formal process, so vote trading is very often informal and thus not binding.\nOne form of vote trading that is formal is one that involves the trading of proxy voting rights - party A gets Party B's voting right formally, eg as a filled in proxy form with signature, perhaps authenticated by secretariats, and in this case party A may use B's vote on issue 1, and B uses A's vote on issue 2... votes traded.\n\nVote trading frequently occurs between and among members of legislative bodies. For example, Congressman A might vote for a dam in Congressman B's district in exchange for Congressman B's vote for farm subsidies in Congressman A's district. One of the first examples of vote trading to occur in the United States was the Compromise of 1790, in which Thomas Jefferson made a deal with James Madison and Alexander Hamilton to move the capital from New York to a site along the Potomac (after a lengthy stay in Philadelphia) in exchange for federal assumption of debts incurred by the states in the Revolutionary War. Hindrances to vote trading in the U.S. Congress include its bicameral structure and the geographic representation basis of its members. Vote trading is encouraged, however, by Congress's relatively loose party discipline which facilitates policy cross-overs by individual congressmen, in sharp contrast to European countries. In any case, vote trading is effectively a binding contract in the house, as both participants can actually see each other at the time of voting. If one party breaks their promise the other might change their vote on the issues involved in the trade, and be rather unfriendly with the other party in future.\n\nVote trading occasionally occurs between United States citizens domiciled in different states (and therefore citizens of those respective states) to demonstrate support for third-party candidates while minimizing the risk that their more favored (or less disfavored) major-party candidate will lose electoral votes in the nationwide election (\"i.e.\", the \"spoiler effect\"). For example:\nIn either case, both candidates and both voters receive a net benefit at minimal (if any) cost:\nVote trading thereby improves the outcome as measured by both candidates' preference orders and according to both \"maximax\" and \"maximin\" evaluation standards, at least given the constraints on the set of possible outcomes imposed by the \"bottleneck\" effect of the winner-take-all electoral-vote allocation procedure.\n\nPresidential vote trading between citizens has increased in popularity since the development of the Internet and World Wide Web facilitated interstate communications between individuals not personally known to each other but identifiable by user account names.\n\nCorporate vote trading has been proposed as a way of improving corporate governance. In this context, vote trading refers to borrowing shares of a stock in time to be the shareholder of record on the day of an important vote.\n\nA variant called vote pairing refers to voters on opposite sides in a single vote agreeing to abstain from voting or otherwise changing their vote. This technique is often used by legislators who do not wish to take time to come to the floor for a vote. A legislator will find a member on the opposite side of the issue who also desires to save time, and they will both agree to skip the vote, maintaining the balance of votes on each side.\n\n\"The Limits of Public Choice: A Sociological Critique of the Economic Theory\" notes that vote trading is often considered immoral, since votes should be determined on the basis of the merits of the question. It is viewed as being less serious an offense than bribery, although in some countries it is still unlawful. However, vote-trading can also be viewed as beneficial to democracy in that it makes it possible for minorities to exert some influence and thus alleviate the tyranny of the majority. In this way, vote-trading is similar to coalition-building, which also involves an exchange of policies and bargaining over cabinet positions in order to gain the parliamentary majority needed for approval of the entire program.\n\nThere have been academic proposals to streamline the legislative vote trading process by creating a market brokered by party leaders in which members buy and sell votes at prices set by supply and demand.\n\n"}
{"id": "22442477", "url": "https://en.wikipedia.org/wiki?curid=22442477", "title": "World Bodypainting Association", "text": "World Bodypainting Association\n\nThe World Bodypainting Association (WBA) is an association for body painters. It represents and looks after artists (whether professional or hobbyists) mediates models, sponsors and workshops under the WB Academy. It organizes under the WB Production: the World Bodypainting Festival (WBF) in Pörtschach, Austria, the European Bodypainting Festival (EBF) in Lisse, Netherlands and global side events. The WB Production also partners to bring such events as the Asia Tour and Living Art America.\n\nThe association was put in place as a result of the festivals success with the focus of creating a network of artists and a platform for companies and individuals interested in the bodypainting community, industry and art. \n\nThe World Bodypainting Association is dedicated -\n\nAlex Barendregt founded the association on October 12, 2001 in Austria as the European Bodypainting Association (EBPA). In 2004 due to the increased participation of international artists and supporting companies it was renamed the World Bodypainting Association also known as the WBA. In 2015 the association reached 600 members and continues to move strong. \n\n"}
