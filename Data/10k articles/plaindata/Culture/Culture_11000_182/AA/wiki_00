{"id": "34909859", "url": "https://en.wikipedia.org/wiki?curid=34909859", "title": "American Laboratory Theatre", "text": "American Laboratory Theatre\n\nThe American Laboratory Theatre was an American drama school and theatrical company located in New York City that existed during the 1920s and 1930s. It was a publicly subsidized, student-subscription organization that held fund-raising campaigns to support itself. \n\nThe school itself was known as the Theatre Arts Institute. It was founded in 1923 by former Moscow Art Theatre members Richard Boleslavsky and Maria Ouspenskaya and stressed Stanislavski's system as its teaching method. Students were taught to be uninhibited, with exercises such as acting as a fish under water, a melting ice cream cone, or (for women) the mother of a sick child praying to the Madonna. Both actors and directors were trained, and Boleslavsky and Ouspenskaya became known as the leading promulgators of Stanislavski's ideas in America. \n\nSome five hundred students were trained at the school during its years of existence. These included Lee Strasberg, Harold Clurman, and Stella Adler, all of whom would go on to exert a great influence on American acting. Future critic Francis Fergusson was also a student. Lenore LaFount started an acting career after her time at the school, but would soon marry George W. Romney and later become First Lady of Michigan and a political candidate in her own right (and the mother of businessman and politician Mitt Romney). \n\nThe school gave theatrical productions from 1925 to 1930. Relatively small in size, it aimed for a high-brow audience. Its productions included well-known works by William Shakespeare, Henrik Ibsen, and Anton Chekhov, but also included well-regarded instantiations of modern and avant-garde works such as Jean-Jacques Bernard's \"Martine\" and Arthur Schnitzler's \"The Bridal Veil\". Talent scouts for radio and film attended the school's performances.\n\nIn 1929, Maria Germanova succeeded Boleslavsky as director of the theatre; she was also from the Moscow Art Theatre. The Lab, as it was sometimes known, disbanded in 1933, but was an important and influential link between Stanislavski's Moscow company and the even more influential Group Theatre of New York that followed it during the 1930s.\n"}
{"id": "13281058", "url": "https://en.wikipedia.org/wiki?curid=13281058", "title": "Amish furniture", "text": "Amish furniture\n\nAmish furniture is furniture manufactured by the Amish, primarily of Pennsylvania, Indiana, and Ohio. It is generally known as being made completely out of wood, usually without particle board or laminate. The styles most often used by the Amish woodworkers are generally more traditional in nature.\n\nAmish furniture first gained attention in the 1920s, when early American folk art was \"discovered\", and dealers and historians placed great value upon the beauty and quality of the pieces. Many different styles of Amish furniture emerged. The Jonestown School began in the late 18th century in Lebanon County, Pennsylvania. The Jonestown School is most widely known for painted blanket chests decorated with flowers on three panels. Examples of these chests are on display at both the Smithsonian Museum and the Metropolitan Museum of Art in New York City.\nAnother distinctive style of Amish furniture is the Soap Hollow School, developed in Soap Hollow, Pennsylvania. These pieces are often brightly painted in red, gold, and black. Henry Lapp was a furniture maker based in Lancaster County, Pennsylvania, and it is his designs that most closely resemble the furniture we think of today as Amish-made. He was one of the first to abandon the painted, Germanic-style influence in his furniture and opted for an undecorated, plain style, following more the styles of Welsh furniture making of the time. The order book he offered to his customers contained watercolor paintings of his pieces and is now in the Philadelphia Museum of Art. The record price for American folk-painted furniture was sold at Sotheby's in 1986. It was a tall case clock made in 1801 by Johannes Spitler that sold for $203,500.\n\nBecause Amish beliefs prevent the use of electricity, many woodworking tools in Amish shops are powered by hydraulic and pneumatic power that is run on diesel compressors. Most communities permit some technology, and allowances can be made in the case of woodworking, as the craft often supports multiple families within the community.\n\nGreat attention is paid to the details of the wood in the furniture-making process. Each piece of wood is hand-selected to match the specific furniture in mind. Attention is paid to the grain of the wood, both in gluing pieces together and in achieving the desired look of the finished piece. Amish furniture is also valued for its sustainability and is considered a \"green\" product. The Amish woodworkers pride themselves in their work and view their products as both pieces of art and furnishings to be used and lived in for generations.\n\nAmish furniture is made in many different styles. The Mission and Shaker styles share a few characteristics. Mission is characterized by straight lines and exposed joinery. It is often considered to be clean and modern in design. The Shaker style is plain, yet elegant and has a very simple and basic design aimed at functionality and durability. The Queen Anne style is in direct contrast to the Mission and Shaker styles. It is considered traditional, with ornate moldings, unique foot details, and carved ornamentation. Other styles available include Southwestern, Rustic, Cottage, Country, Quaker, and Beachfront.\nAmish furniture-making is often a skill passed through many generations. Most Amish children rarely attend school beyond eighth grade, often to help out at home, or in the shops. Many families become known for their specific design details and niches. Some woodworkers focus only on outdoor furniture, others on pieces for the living room or bedroom. No piece of furniture is ever identical to another because of the care taken to select the wood. The grain is different on every piece of wood, and the craftsmen often try to highlight the features of each individual piece.\n\nDuring the second half of the 19th century, Grand Rapids became a major lumbering center, processing timber harvested in the region. By the end of the century, it was established as the premier furniture-manufacturing city of the United States. For this reason it was nicknamed “Furniture City”. After an international exhibition in Philadelphia in 1876, Grand Rapids became recognized worldwide as a leader in the production of fine furniture.\n\nA furniture-makers’ guild was established in 1931 to improve the design and craftsmanship of Grand Rapids furniture. National home furnishing markets were held in Grand Rapids for about 75 years, concluding in the 1960s. By that time, the furniture-making industry shifted to North Carolina.\n\nMuch of Grand Rapids furniture has now been outsourced to Asia. In that time, Amish builders have acquired much of the old machinery once used. However, because Amish beliefs prevent the use of electricity, many woodworking tools in Amish shops are powered by hydraulic and pneumatic power that is run on diesel compressors.\n\nIn recent years, the Amish furniture market has expanded to include online sales. The Amish craftsmen, because of their beliefs, are prohibited from running the websites. Non-Amish retailers often attend Amish furniture expositions in Ohio and Indiana to see Amish furniture on display and meet the craftsmen behind the pieces. Relationships are often developed, and the retailer becomes the middleman between the simple life of the Amish woodworker and the modern buyer.\n\nAmish furniture is now available to a wider market and to those who may not be in close proximity to an Amish woodworking shop. It is no longer necessary to visit a retail location to select the unique wood and stain combination desired; this can all be done on the Internet, and there are dozens of different wood, stain, and upholstery options to choose from. The finished furniture is shipped directly from the stain shop to the consumer.\n\nAmish furniture is made with a variety of quality hardwoods including northern red oak, quarter-sawn white oak, cherry, maple, beech, elm, mahogany, walnut, hickory, cedar, and pine. Northern red oak is a very popular choice for American consumers for its warmth, color, and durability. It is typically grown in Eastern U.S., particularly in the Appalachian Mountains. White oak is slightly harder than red oak and can be cut to show more ray fleck. The antique look of white oak makes it ideal for Mission and Shaker styles. Cherry has a light reddish-brown color that will darken with light exposure. Maple wood offers a spectrum of beauty from different angles. It is significantly harder than oak and is growing in popularity because of its beauty. American beech is white with a red tinge and bends readily when steamed. Elm ranges in color from nearly white to brown with a red tinge and is fairly stiff and heavy. Mahogany is typically used in high-class furnishings because of its attractive finish. As mahogany matures its color varies from yellowish or pinkish to deep red or brown. Walnut is heavy, hard, and stiff and ranges in color from nearly white in the sapwood to dark brown in the heartwood. Walnut holds stain, paint, and polish well. Hickory is harder than oak and distinguished by extreme contrasts of light and dark colors. Hickory’s sapwood is a creamy white while hickory’s heartwood is a red, pink or reddish-brown color and often referred to as red hickory. Cedar has a deep rosy glow and stripes of light golden sapwood. Eastern white pine is a soft wood. It tends to have more knots than a hardwood and can yellow with age.\n"}
{"id": "1408426", "url": "https://en.wikipedia.org/wiki?curid=1408426", "title": "Candaulism", "text": "Candaulism\n\nCandaulism is a sexual practice or fantasy in which a man exposes his female partner, or images of her, to other people for their voyeuristic pleasure.\n\nThe term may also be applied to the practice of undressing or otherwise exposing a female partner to others, or urging or forcing her to engage in sexual relations with a third person, such as during a swinging activity. Similarly, the term may also be applied to the posting of personal images of a female partner on the internet or urging or forcing her to wear clothing which reveals her physical attractiveness to others, such as by wearing very brief clothing, such as a microskirt, tight-fitting or see-through clothing or a low-cut top.\n\nThe term is derived from ancient King Candaules who conceived a plot to show his unaware naked wife to his servant Gyges of Lydia. After discovering Gyges while he was watching her naked, Candaules' wife ordered him to choose between killing himself or killing her husband in order to repair the vicious mischief.\n\nThe term was first defined by Richard von Krafft-Ebing in his book: \"Psychopathia sexualis. Eine klinisch-forensische Studie\" (Stuttgart: Enke 1886).\n\nIsidor Sadger hypothesized that the candaulist completely identifies with his partner's body, and deep in his mind is showing himself. Candaulism is also associated with voyeurism and exhibitionism. An alternative definition proposes it as a practice involving one person observing, often from concealment, two others having sexual relations.\n\nThe case of Sir Richard Worsley, against George Bissett for \"criminal conversation\"—that is, adultery with Lady Worsley—revealed an incident in which Sir Richard had assisted Bissett to spy on Lady Worsley taking a bath.\n\nThe art collector and connoisseur Charles Saatchi has considered the influence of candaulism upon the work of Salvador Dali, citing episodes recorded by the artist's biographers in which Dali's wife Gala was displayed to other men.\n\nThe notorious American FBI agent caught for having spied for the Soviet Union as well as the Russian Federation, Robert Hanssen, took explicit photographs of his wife and sent them to a friend. Later Hanssen invited his friend to clandestinely observe Hanssen having sex with Hanssen's wife during the friend's occasional visits to the Hanssen household. Initially, his friend watched through a window from outside the house. Later, Hanssen appropriated video equipment from the FBI to set up closed-circuit television to allow his friend to watch from his guest bedroom. Hanssen also posted sexually explicit stories to the Internet crafted to allow readers who knew the Hanssens to identify them, also without his wife's knowledge.\n\nCandaulism is a theme of \"A Dance to the Music of Time\", the cycle of novels by Anthony Powell. A key scene in the penultimate volume, \"Temporary Kings\", is set in a Venetian palazzo under a ceiling on which Tiepolo has depicted King Candaules allowing his wife to be seen naked by Gyges. The theme of voyeurism runs through the sequence of novels including a scene in which the recurring character Kenneth Widmerpool watches his wife with a lover.\n\nIn the \"Book of Esther\" the King orders Queen Vashti to appear before his guests wearing her crown and she refuses. Some commentators have taken this to mean her crown \"and nothing else\", which if accurate would place this story as an example.\n\n\n"}
{"id": "944850", "url": "https://en.wikipedia.org/wiki?curid=944850", "title": "Capitoline Triad", "text": "Capitoline Triad\n\nThe Capitoline Triad was a group of three deities who were worshipped in ancient Roman religion in an elaborate temple on Rome's Capitoline Hill (Latin \"Capitolium\"). It comprised Jupiter, Juno and Minerva. The triad held a central place in the public religion of Rome.\n\nThe three deities who are most commonly referred to as the \"Capitoline Triad\" are Jupiter, the king of the gods; Juno (in her aspect as \"Iuno Regina\", \"Queen Juno\"), his wife and sister; and Jupiter's daughter Minerva, the goddess of wisdom. This grouping of a male god and two goddesses was highly unusual in ancient Indo-European religions, and is almost certainly derived from the Etruscan trio of Tinia, the supreme deity, Uni, his wife, and Menrva, their daughter and the goddess of wisdom. In some interpretations, this group replaced an original Archaic Triad.\n\nJupiter, Juno and Minerva were honored in temples known as \"Capitolia\", which were built on hills and other prominent areas in many cities in Italy and the provinces, particularly during the Augustan and Julio-Claudian periods. Most had a triple \"cella\". The earliest known example of a Capitolium outside of Italy was at Emporion (now Empúries, Spain). According to Ovid, Terminus also had a place there, since he had a shrine there before it was built and, as the god of boundary stones, refused to yield.\n\nAlthough the word \"Capitolium\" (pl. \"Capitolia\") could be used to refer to any temple dedicated to the Capitoline Triad, it referred especially to the temple on the Capitoline Hill in Rome known as \"aedes Iovis Optimi Maximi Capitolini\" (\"Temple of Jupiter Best and Greatest on the Capitoline\"). The temple was built under the reign of Lucius Tarquinius Superbus, the last King of Rome prior to the establishment of the Roman Republic. Although the temple was shared by Jupiter, Juno and Minerva, each deity had a separate \"cella\", with Juno Regina on the left, Minerva on the right, and Jupiter Optimus Maximus in the middle. It included a podium and a tetrastyle (four column) pronaos (porch).\n\nAnother shrine (\"sacellum\") dedicated to Jupiter, Juno Regina and Minerva was the Capitolium Vetus on the Quirinal Hill. It was thought to be older than the more famous temple of Jupiter Optimus Maximus on the Capitoline Hill, and was still a landmark in Martial's time, in the late 1st century.\n"}
{"id": "28433310", "url": "https://en.wikipedia.org/wiki?curid=28433310", "title": "Career Transition For Dancers", "text": "Career Transition For Dancers\n\nCareer Transition For Dancers is a nonprofit organization with offices in New York, Los Angeles and Chicago helping dancers establish new careers when they have retired from their performing careers. Founded in 1985, the organization has provided over 46,000 hours of individual and group career counseling, with a dollar value of over $4 million, and awarded over $3 million in educational and entrepreneurial support. The organization has 4,600 active dancer-clients and has helped dancers in 47 states with their transitions through mobile National Outreach Projects.\n\nIn 1982, several foundations and unions including the National Endowment for the Arts, the AFL-CIO Labor Institute for Human Enrichment and the Actors' Equity Association, led by Agnes de Mille, granted funding for a conference to discuss the need to assist dancers both during and at the end of their careers.\n\nThe conference took place on June 8, 1982 and was led by Project Director Edward Weston, Chairman Richard E. LeBlond, Jr., and Honorary Chairman Agnes de Mille. Exploring the trauma professional dancers face both during and at the end of their careers, the main goal of the conference was to consider how to better assist dancers with moving into new professions that would make use of their backgrounds, talents and skills. By the end of the conference it had been agreed that it was critical for dancers to begin exploring alternative careers at the beginning of their dance training and performing careers.\n\nIn 1985, nonprofit organization Career Transition For Dancers, Inc. was established in New York City with money from Actors' Equity Association, the American Federation of Television and Radio Artists, American Guild of Musical Artists, and the Screen Actors Guild. Career Transition For Dancers is the only organization in the United States dedicated solely to the enrichment of dancers' post-performing years.\n\nCaroline Newhouse (1910–2003), an artist and arts philanthropist, served on the Board of Career Transition For Dancers as Director Emerita, establishing The Caroline H. Newhouse Scholarship Fund as well as The Caroline & Theodore Newhouse Center for Dancers with an endowed gift of $1 million during her tenure. Ms. Newhouse, an artist herself, always felt a special bond with dancers, who often served as models for her artwork. \"While you sculpt, you talk, and they told me how hard they have to work in order to perform,\" she said. \"There is no time for them to do anything except dance, dance, dance. And at 29, the body doesn't react anymore as it did when you were 19.\" Ms. Newhouse's love for dance was also shared by her husband, Theodore Newhouse, who helped establish the Newhouse communications company. The company owns Condé-Nast Publications, which has been a long-time supporter of the organization and is currently one of the organization's annual gala underwriters, along with the Samuel I. Newhouse Foundation.\n\nPrima ballerina Cynthia Gregory was the initial chairman of the board of the organization through 2015 and now serves as its chairman emeritus.\n\nThe mission of Career Transition For Dancers is to enable dancers to define their career possibilities and develop the skills necessary to excel in a variety of disciplines.\n\nThe organization is based in New York City, where the first office opened in 1985. A second branch opened in Los Angeles in 1995, and a third was established in Chicago in 2008. In addition, the organization maintains a national reach through annual National Outreach Projects in various cities throughout the United States.\n\nCareer Transition For Dancers is a member of the International Organization for the Transition of Professional Dancers(IOTPD). Other organizations in the IOTPD include: Dancer Transition Resource Centre (Canada), Dancers' Career Development (UK), Association Suisse Pour la Reconversion des Danseurs Professionnels (Switzerland).\n\nCareer counseling is offered in-person through the organization's three offices, over the internet and by phone. These methods have provided more than 4,600 dancers in 47 states with approximately 46,000 hours of career counseling.\n\nThe organization offers one-on-one career counseling, informative Career Conversation seminars on relevant topics of interest, and focus and support groups such as the Business Group (for dancer-entrepreneurs) and the Diamond Group (for mature dancers).\n\nNational Outreach Projects (NOPs) provide counseling and seminars for two days in various locations throughout the nation each year. Workshops examine the key components of career transition, from defining interests and skills to understanding the emotional aspects of change. Individual career counseling sessions are also offered after the workshops. To date, the organization has toured over 30 locations around the country.\n\nCareer Transition For Dancers provides grants and educational scholarships that allow dancers to go back to school or start new businesses. Over $4 million has been awarded for educational scholarships towards tuition, books, and related expenses since 1985. Over $400,000 has been awarded to entrepreneurial dancers.\n\nEach of the organization's offices have a computer lab and resource center that are open to all dancers. Resource center materials include self-help manuals, college guides, a job bank, and various publications relevant to working dancers pursuing career development.\n\nJock Soto, former principal dancer with NYCB, received a grant from the organization to attend culinary school after retiring, and wrote a cookbook with his former dance partner, Heather Watts. He now teaches at the School of American Ballet and manages a catering events company called Lucky Basset Events.\n\nOn September 21, 2015, the organization announced that it would merge with The Actors Fund of America. The former activities of the organization will thereafter be carried on as an operation of The Actors Fund.\n"}
{"id": "43848058", "url": "https://en.wikipedia.org/wiki?curid=43848058", "title": "Charlotte Medal", "text": "Charlotte Medal\n\nThe Charlotte Medal is a silver medallion wide, depicting the voyage of the \"Charlotte\", with the First Fleet, to Botany Bay, Australia. Its obverse depicts a scene of the ship and its reverse is inscribed with a description of the journey. The medal is said to be the first work of Australian colonial art.\n\nDuring the journey the Charlotte visited Rio de Janeiro. Whilst at anchor, one of the ship's convicts, a forger and mutineer by the name of Thomas Barrett was caught giving locals fake coins made from buckles, buttons and spoons. The Surgeon-general of the Fleet, John White was impressed with his skill in making these forgeries, without having the apparent means to do so. This led him to commission Barrett to make the medal, to commemorate the journey, possibly from the surgeon's silver kidney dish. It is created in the style of a Touch piece.\n\nThe obverse of the medal depicts the \"Charlotte\" at anchor at night in Botany Bay. The inscription reads \n\nThe reverse of the medal is inscribed with a journal of the voyage. It reads \n\nIt is unknown who owned the medal after White. It is possible that he presented it with his voyage findings, or it stayed with his family after his death, but at some point before 1919 it came into the possession of Princess Victoria and Prince Louis.\n\nIn 1919 it was sold via Sotheby, Wilkinson & Hodge to a British numismatist, Henry Baldwin.\n\nIn 1967 it was sold to an American numismatist, John J Ford.\n\nIn 1981 it was sold to a Melbourne dentist, Dr John Chapman, for $15,000 at Spink Sydney Auction. Dr Chapman donated a medal containing a reproduction of the Charlotte Medal to Museum Victoria to mark its bicentennial, in 1988.\n\nIn 2008 the Australian National Maritime Museum, with funds from the National Cultural Heritage Account, authorised through the Australian Government, won an auction for the medal with a bid of $750,000. The final price was $873,750, with $200,000 of NCH funding\n\nThis makes it possible that, despite its age, the medal has only been sold four times.\n\nA smaller copper medallion, with a diameter of , was created at the same time. The medal was made for White's personal servant, William Broughton and omits the ship scene, being inscribed simply with an abridged version of the journey. The medallion was uncovered during house restoration in the early 1940s. It has been suggested that this medal should \"rank as equal in rarity and significance\" as its silver counterpart.\n"}
{"id": "633853", "url": "https://en.wikipedia.org/wiki?curid=633853", "title": "Collective behavior", "text": "Collective behavior\n\nThe expression collective behavior was first used by Franklin Henry Giddings (1908) and employed later by Robert E. Park (1921), Herbert Blumer (1939), Ralph Turner and Lewis Killian (1957), and Neil Smelser (1962) to refer to social processes and events which do not reflect existing social structure (laws, conventions, and institutions), but which emerge in a \"spontaneous\" way. Use of the term has been expanded to include reference to cells, social animals like birds and fish, and insects including ants. Collective behavior takes many forms but generally violates societal norms (Miller 2000, Locher 2002). Collective behavior can be tremendously destructive, as with riots or mob violence, silly, as with fads, or anywhere in between. Collective behavior is always driven by group dynamics, encouraging people to engage in acts they might consider unthinkable under typical social circumstances (Locher 2002).\n\nTurner and Killian (1957) were the first sociologists to back their theoretical propositions with visual evidence in the form of photographs and motion pictures of collective behavior in action. Prior to that sociologists relied heavily upon eyewitness accounts, which turned out to be far less reliable than one would hope.\n\nTurner and Killian's approach is based largely upon the arguments of Blumer, who argued that social \"forces\" are not really forces. The actor is active: He creates an interpretation of the acts of others, and acts on the basis of this interpretation.\n\nHere are some instances of collective behavior: the Los Angeles riot of 1992, the hula-hoop fad of 1958, the stock market crashes of 1929, and the \"phantom gasser\" episodes in Virginia in 1933-34 and Mattoon, IL in 1944 (Locher 2002, Miller 2000). The claim that such diverse episodes all belong to a single field of inquiry is a theoretical assertion, and not all sociologists would agree with it. But Blumer and Neil Smelser did agree, as did others, indicating that the formulation has satisfied some leading sociological thinkers.\n\nAlthough there are several other schema that may be used to classify forms of collective behavior the following four categories from Blumer (1939) are generally considered useful by most sociologists.\n\nScholars differ about what classes of social events fall under the rubric of collective behavior. In fact, the only class of events which all authors include is crowds. Clark McPhail is one of those who treats crowds and collective behavior as synonyms. Although some consider McPhail's work overly simplistic (Locher 2002), his important contribution is to have gone beyond the speculations of others to carry out pioneering empirical studies of crowds. He finds them to form an elaborate set of types.\n\nThe classic treatment of crowds is Gustave LeBon, \"The Crowd: A Study of the Popular Mind\" (1896), in which the author interpreted the crowds of the French Revolution as irrational reversions to animal emotion, and inferred from this that such reversion is characteristic of crowds in general. LeBon believed that crowds somehow induced people to lose their ability to think rationally and to somehow recover this ability once they had left the crowd. He speculated, but could not explain how this might occur. Freud expressed a similar view in \"Group Psychology and the Analysis of the Ego\" (1922). Such authors have thought that their ideas were confirmed by various kinds of crowds, one of these being the economic bubble. In Holland, during the tulip mania (1637), the prices of tulip bulbs rose to astronomical heights. An array of such crazes and other historical oddities is narrated in Charles MacKay's \"Extraordinary Popular Delusions and the Madness of Crowds\" (1841).\n\nAt the University of Chicago, Robert Park and Herbert Blumer agreed with the speculations of LeBon and other that crowds are indeed emotional. But to them a crowd is capable of any emotion, not only the negative ones of anger and fear.\n\nA number of authors modify the common-sense notion of the crowd to include episodes during which the participants are not assembled in one place but are dispersed over a large area. Turner and Killian refer to such episodes as \"diffuse\" crowds, examples being Billy Graham's revivals, panics about sexual perils, witch hunts and Red scares. Their expanded definition of the crowd is justified if propositions which hold true among compact crowds do so for diffuse crowds as well.\n\nSome psychologists have claimed that there are three fundamental human emotions: fear, joy, and anger. Neil Smelser, John Lofland, and others have proposed three corresponding forms of the crowd: the panic (an expression of fear), the craze (an expression of joy), and the hostile outburst (an expression of anger). Each of the three emotions can characterize either a \"compact\" or a \"diffuse\" crowd, the result being a scheme of six types of crowds. Lofland has offered the most explicit discussion of these types.\n\nBoom distinguishes the crowd, which expresses a \"common emotion\", from a public, which discusses a \"single issue\". Thus, a public is not equivalent to all of the members of a society. Obviously, this is not the usual use of the word, \"public.\" To Park and Blumer, there are as many publics as there are issues. A public comes into being when discussion of an issue begins, and ceases to be when it reaches a decision on it.\n\nTo the crowd and the public Blumer adds a third form of collective behavior, the mass. It differs from both the crowd and the public in that it is defined not by a form of interaction but by the efforts of those who use the mass media to address an audience. The first mass medium was printing.\n\nWe change intellectual gears when we confront Blumer's final form of collective behavior, the social movement. He identifies several types of these, among which are \"active\" social movements such as the French Revolution and \"expressive\" ones such as Alcoholics Anonymous. An active movement tries to change society; an expressive one tries to change its own members.\n\nThe social movement is the form of collective behavior which satisfies least well the first definition of it which was offered at the beginning of this article. These episodes are less fluid than the other forms, and do not change as often as other forms do. Furthermore, as can be seen in the history of the labor movement and many religious sects, a social movement may begin as collective behavior but over time become firmly established as a social institution.\n\nFor this reason, social movements are often considered a separate field of sociology. The books and articles about them are far more numerous than the sum of studies of all the other forms of collective behavior put together. Social movements are considered in many Wikipedia articles, and an article on the field of social movements as a whole would be much longer than this essay.\n\nThe study of collective behavior spun its wheels for many years, but began to make progress with the appearance of Turner and Killian's \"Collective Behavior\" (1957) and Smelser's \"Theory of Collective Behavior\" (1962). Both books pushed the topic of collective behavior back into the consciousness of American sociologists and both theories contributed immensely to our understanding of collective behavior (Locher 2002, Miller 2000). Social disturbances in the U. S. and elsewhere in the late '60s and early '70s inspired another surge of interest in crowds and social movements. These studies presented a number of challenges to the armchair sociology of earlier students of collective behavior.\n\nSocial scientists have developed theories to explain crowd behavior.\n\n\n\n"}
{"id": "14599154", "url": "https://en.wikipedia.org/wiki?curid=14599154", "title": "Contemporary commercial music", "text": "Contemporary commercial music\n\nContemporary commercial music or CCM is a term used by some vocal pedagogists in the United States of America to refer to non-classical music. This term encompasses jazz, pop, blues, soul, country, folk, and rock styles.\n\n"}
{"id": "7104", "url": "https://en.wikipedia.org/wiki?curid=7104", "title": "Cotton Mather", "text": "Cotton Mather\n\nCotton Mather (February 12, 1663 – February 13, 1728; A.B. 1678, Harvard College; A.M. 1681, honorary doctorate 1710, University of Glasgow) was a socially and politically influential New England Puritan minister, prolific author, and pamphleteer. He left a scientific legacy due to his hybridization experiments and his promotion of inoculation for disease prevention, though he is most frequently remembered today for his involvement in the Salem witch trials. He was subsequently denied the presidency of Harvard College which his father, Increase Mather, had held.\n\nMather was born in Boston, Massachusetts Bay Colony, the son of Maria (née Cotton) and Increase Mather, and grandson of both John Cotton and Richard Mather, all also prominent Puritan ministers. Mather was named after his maternal grandfather John Cotton. He attended Boston Latin School, where his name was posthumously added to its Hall of Fame, and graduated from Harvard in 1678 at age 15. After completing his post-graduate work, he joined his father as assistant pastor of Boston's original North Church (not to be confused with the Anglican/Episcopal Old North Church of Paul Revere fame). In 1685, Mather assumed full responsibilities as pastor of the church.\nMather wrote more than 450 books and pamphlets, and his ubiquitous literary works made him one of the most influential religious leaders in America. He set the moral tone in the colonies, and sounded the call for second- and third-generation Puritans to return to the theological roots of Puritanism, whose parents had left England for the New England colonies of North America. The most important of these was \"Magnalia Christi Americana\" (1702) which comprises seven distinct books, many of which depict biographical and historical narratives.\nMather influenced early American science. In 1716, he conducted one of the first recorded experiments with plant hybridization because of observations of corn varieties. This observation was memorialized in a letter to his friend James Petiver:\nIn November 1713, Mather's wife, newborn twins, and two-year-old daughter all succumbed during a measles epidemic. He was twice widowed, and only two of his 15 children survived him; he died on the day after his 65th birthday and was buried on Copp's Hill, near Old North Church.\n\nRobert Boyle was a huge influence throughout Mather's career. He read Boyle's \"The Usefulness of Experimental Natural Philosophy\" closely throughout the 1680s, and his own early works on science and religion borrowed greatly from it, using almost identical language to Boyle.\n\nMather's relationship with his father Increase Mather is thought by some to have been strained and difficult. Increase was a pastor of the North Square Church and president of Harvard College; he led an accomplished life. Despite Cotton's efforts, he never became quite as well known and successful in politics as his father. He did surpass his father's output as a writer, writing more than 400 books. One of the most public displays of their strained relationship emerged during the witch trials, which Increase Mather reportedly did not support.\n\nCotton Mather helped convince Elihu Yale to make a donation to a new college in New Haven which became Yale College.\n\nIn 1689, Mather published \"Memorable Providences\" detailing the supposed afflictions of several children in the Goodwin family in Boston. Catholic washerwoman Goody Glover was convicted of witchcraft and executed in this case. Mather had a prominent role in this case. Besides praying for the children, which also included fasting and meditation, he would also observe and record their activities. The children were subject to hysterical fits, which he detailed in \"Memorable Providences\". In his book, Mather argued that since there are witches and devils, there are \"immortal souls.\" He also claimed that witches appear spectrally as themselves. He opposed any natural explanations for the fits, he believed that people who confessed to using witchcraft were sane, he warned against performing magic due to its connection with the devil and he argued that spectral evidence should not be used as evidence for witchcraft.\nRobert Calef was a contemporary of Mather and critical of him, and he considered this book responsible for laying the groundwork for the Salem witch trials three years later: \n\nNineteenth-century historian Charles Wentworth Upham shared the view that the afflicted in Salem were imitating the Goodwin children, but he put the blame on both Cotton and his father Increase Mather: \n\nMather was influential in the construction of the court for the trials from the beginning. Sir William Phips, governor of the newly chartered Province of Massachusetts Bay, appointed his lieutenant governor, William Stoughton, as head of a special witchcraft tribunal and then as chief justice of the colonial courts, where he presided over the witch trials. According to George Bancroft, Mather had been influential in gaining the politically unpopular Stoughton his appointment as lieutenant governor under Phips through the intervention of Mather's own politically powerful father, Increase. \"Intercession had been made by Cotton Mather for the advancement of Stoughton, a man of cold affections, proud, self-willed and covetous of distinction.\" Apparently Mather saw in Stoughton, a bachelor who had never wed, an ally for church-related matters. Bancroft quotes Mather's reaction to Stoughton's appointment as follows:\n\nMather claimed not to have attended the trials in Salem (although his father attended the trial of George Burroughs). His contemporaries Calef and Thomas Brattle place him at the executions (see below). Mather began to publicize and celebrate the trials well before they were put to an end: \"If in the midst of the many Dissatisfaction among us, the publication of these Trials may promote such a pious Thankfulness unto God, for Justice being so far executed among us, I shall Re-joyce that God is Glorified.\" Mather called himself a historian not an advocate but, according to one modern writer, his writing largely presumes the guilt of the accused and includes such comments as calling Martha Carrier \"a rampant hag\". Mather referred to George Burroughs as a \"very puny man\" whose \"tergiversations, contradictions, and falsehoods\" made his testimony not \"worth considering\".\n\nThe afflicted girls claimed that the semblance of a defendant, invisible to any but themselves, was tormenting them; this was considered evidence of witchcraft, despite the defendant's denial and profession of strongly held Christian beliefs. On May 31, 1692, Mather wrote to one of the judges, John Richards, a member of his congregation, expressing his support of the prosecutions, but cautioning; \"do not lay more stress on pure spectral evidence than it will bear … It is very certain that the Devils have sometimes represented the Shapes of persons not only innocent, but also very virtuous. Though I believe that the just God then ordinarily provides a way for the speedy vindication of the persons thus abused.\"\n\nAn opinion on the matter was sought from the ministers of the area and a response was submitted June 15, 1692. Cotton Mather seems to take credit for the varied responses when anonymously celebrating himself years later: \"drawn up at their desire, by Cotton Mather the younger, as I have been informed.\" The \"Return of the Several Ministers\" ambivalently discussed whether or not to allow spectral evidence. The original full version of the letter was reprinted in late 1692 in the final two pages of Increase Mather's \"Cases of Conscience\". It is a curious document and remains a source of confusion and argument. Calef calls it \"perfectly Ambidexter, giving as great as greater Encouragement to proceed in those dark methods, then cautions against them… indeed the Advice then given, looks most like a thing of his Composing, as carrying both Fire to increase and Water to quench the Conflagration.\" It seems likely that the \"Several\" ministers consulted did not agree, and thus Cotton Mather's construction and presentation of the advice could have been crucial to its interpretation.\n\nThomas Hutchinson summarized the Return, \"The two first and the last sections of this advice took away the force of all the others, and the prosecutions went on with more vigor than before.\" Reprinting the Return five years later in his anonymously published \"Life of Phips\" (1697), Cotton Mather omitted the fateful \"two first and the last\" sections, though they were the ones he had already given most attention in his \"Wonders of the Invisible World\" rushed into publication in the summer and early autumn of 1692.\n\nOn August 19, 1692, Mather attended the execution of George Burroughs (and four others who were executed after Mather spoke) and Robert Calef presents him as playing a direct and influential role:\n\nOn September 2, 1692, after eleven of the accused had been executed, Cotton Mather wrote a letter to Chief Justice William Stoughton congratulating him on \"extinguishing of as wonderful a piece of devilism as has been seen in the world\" and claiming that \"one half of my endeavors to serve you have not been told or seen.\"\n\nRegarding spectral evidence, Upham concludes that \"Cotton Mather never in any public writing 'denounced the admission' of it, never advised its absolute exclusion; but on the contrary recognized it as a ground of 'presumption' … [and once admitted] nothing could stand against it. Character, reason, common sense, were swept away.\" In a letter to an English clergyman in 1692, Boston intellectual Thomas Brattle, criticizing the trials, said of the judges' use of spectral evidence:\n\nThe later exclusion of spectral evidence from trials by Governor Phips, around the same time his own wife's (Lady Mary Phips) name coincidentally started being bandied about in connection with witchcraft, began in January 1693. This immediately brought about a sharp decrease in convictions. Due to a reprieve by Phips, there were no further executions. Phips's actions were vigorously opposed by William Stoughton.\n\nBancroft notes that Mather considered witches \"among the poor, and vile, and ragged beggars upon Earth\", and Bancroft asserts that Mather considered the people against the witch trials to be witch advocates.\n\nIn the years after the trials, of the principal actors in the trial, whose lives are recorded after, neither he nor Stoughton admitted strong misgivings. For several years after the trials, Cotton Mather continued to defend them and seemed to hold out a hope for their return.\n\n\"Wonders of the Invisible World\" contained a few of Mather's sermons, the conditions of the colony and a description of witch trials in Europe. He somewhat clarified the contradictory advice he had given in \"Return of the Several Ministers\", by defending the use of spectral evidence. \"Wonders of the Invisible World\" appeared around the same time as Increase Mather's \"Cases of Conscience.\"\"\nMather did not sign his name or support his father's book initially:\n\nThe last major events in Mather's involvement with witchcraft were his interactions with Mercy Short in December 1692 and Margaret Rule in September 1693. The latter brought a five year campaign by Boston merchant Robert Calef against the influential and powerful Mathers. Calef's book \"More Wonders of the Invisible World\" was inspired by the fear that Mather would succeed in once again stirring up new witchcraft trials, and the need to bear witness to the horrible experiences of 1692. He quotes the public apologies of the men on the jury and one of the judges. Increase Mather was said to have publicly burned Calef's book in Harvard Yard around the time he was removed from the head of the college and replaced by Samuel Willard.\n\nIn 1869, William Frederick Poole quoted from various school textbooks of the time demonstrating they were in agreement on Cotton Mather's role in the Witch Trials:\n\n<poem>\nIf anyone imagines that we are stating the case too strongly, let him try an experiment with the first bright boy he meets by asking...\n'Who got up Salem Witchcraft?'... he will reply, 'Cotton Mather'. Let him try another boy...\n'Who was Cotton Mather?' and the answer will come, 'The man who was on horseback, and hung witches.'</poem>\n\nPoole was a librarian, and a lover of literature, including Mather's \"Magnalia\" \"and other books and tracts, numbering nearly 400 [which] were never so prized by collectors as today.\" Poole announced his intention to redeem Mather's name, using as a springboard a harsh critique of a recently published tome by Charles Wentworth Upham, \"Salem Witchcraft Volumes I and II With an Account of Salem Village and a History of Opinions on Witchcraft and Kindred Subjects\", which runs to almost 1,000 pages, and a quick search of the name Mather (referring to either father, son, or ancestors) shows that it occurs 96 times. Poole's critique, in book form, runs less than 70 pages but the name \"Mather\" occurs many more times than the other book, which is more than ten times as long. Upham shows a balanced and complicated view of Cotton Mather, such as this first mention: \"One of Cotton Mather's most characteristic productions is the tribute to his venerated master. It flows from a heart warm with gratitude.\"\n\nUpham's book refers to Robert Calef no fewer than 25 times with the majority of these regarding documents compiled by Calef in the mid-1690s and stating: \"Although zealously devoted to the work of exposing the enormities connected with the witchcraft prosecutions, there is no ground to dispute the veracity of Calef as to matters of fact.\" He goes on to say that Calef's collection of writings \"gave a shock to Mather's influence, from which it never recovered.\"\n\nCalef produced only the one book; he is self-effacing and apologetic for his limitations, and on the title page he is listed not as author but \"collector\". Poole, champion of literature, cannot accept Calef whose \"faculties, as indicated by his writings appear to us to have been of an inferior order;…\", and his book \"in our opinion, has a reputation much beyond its merits.\" Poole refers to Calef as Mather's \"personal enemy\" and opens a line, \"Without discussing the character and motives of Calef…\" but does not follow up on this suggestive comment to discuss any actual or purported motive or reason to impugn Calef. Upham responded to Poole (referring to Poole as \"the Reviewer\") in a book running five times as long and sharing the same title but with the clauses reversed: Salem Witchcraft and Cotton Mather.Many of Poole's arguments were addressed, but both authors emphasize the importance of Cotton Mather's difficult and contradictory view on spectral evidence, as copied in the final pages, called \"The Return of Several Ministers\", of Increase Mather's \"Cases of Conscience\".\n\nEvidenced by the published opinion in the years that followed the Poole vs Upham debate, it would seem Upham was considered the clear winner (see Sibley, GH Moore, WC Ford, and GH Burr below.). In 1891, Harvard English professor Barrett Wendall wrote \"Cotton Mather, The Puritan Priest\". His book often expresses agreement with Upham but also announces an intention to show Cotton Mather in a more positive light. \"[Cotton Mather] gave utterance to many hasty things not always consistent with fact or with each other…\" And some pages later: \"[Robert] Calef’s temper was that of the rational Eighteenth century; the Mathers belonged rather to the Sixteenth, the age of passionate religious enthusiasm.\"\n\nIn 1907, George Lyman Kittredge published an essay that would become foundational to a major change in the 20th-century view of witchcraft and Mather culpability therein. Kittredge is dismissive of Robert Calef, and sarcastic toward Upham, but shows a fondness for Poole and a similar soft touch toward Cotton Mather. Responding to Kittredge in 1911, George Lincoln Burr, a historian at Cornell, published an essay that begins in a professional and friendly fashion toward both Poole and Kittredge, but quickly becomes a passionate and direct criticism, stating that Kittredge in the \"zeal of his apology… reached results so startlingly new, so contradictory of what my own lifelong study in this field has seemed to teach, so unconfirmed by further research… and withal so much more generous to our ancestors than I can find it in my conscience to deem fair, that I should be less than honest did I not seize this earliest opportunity share with you the reasons for my doubts…\" (In referring to \"ancestors\" Burr primarily means the Mathers, as is made clear in the substance of the essay.) The final paragraph of Burr's 1911 essay pushes these men's debate into the realm of a progressive creed\n\n… I fear that they who begin by excusing their ancestors may end by excusing themselves.\n\nPerhaps as a continuation of his argument, in 1914, George Lincoln Burr published a large compilation \"Narratives\". This book arguably continues to be the single most cited reference on the subject. Unlike Poole and Upham, Burr avoids forwarding his previous debate with Kittredge directly into his book and mentions Kittredge only once, briefly in a footnote citing both of their essays from 1907 and 1911, but without further comment. But in addition to the viewpoint displayed by Burr's selections, he weighs in on the Poole vs Upham debate at various times, including siding with Upham in a note on Thomas Brattle's letter, \"The strange suggestion of W. F. Poole that Brattle here means Cotton Mather himself, is adequately answered by Upham…\" Burr's \"Narratives\" reprint a lengthy but abridged portion of Calef's book and introducing it he digs deep into the historical record for information on Calef and concludes \"…that he had else any grievance against the Mathers or their colleagues there is no reason to think.\" Burr finds that a comparison between Calef's work and original documents in the historical record collections \"testify to the care and exactness…\"\n\n1920–3 Kenneth B. Murdock wrote a doctoral dissertation on Increase Mather advised by Chester Noyes Greenough and Kittredge. Murdock's father was a banker hired in 1920 to run the Harvard Press and he published his son's dissertation as a handsome volume in 1925: \"Increase Mather, The Foremost American Puritan\" (Harvard University Press). Kittredge was right hand man to the elder Murdock at the Press. This work focuses on Increase Mather and is more critical of the son, but the following year he published a selection of Cotton Mather's writings with an introduction that claims Cotton Mather was \"not less but more humane than his contemporaries. Scholars have demonstrated that his advice to the witch judges was always that they should be more cautious in accepting evidence\" against the accused. Murdock's statement seems to claim a majority view. But one wonders who Murdock would have meant by \"scholars\" at this time other than Poole, Kittredge, and TJ Holmes (below) and Murdock's obituary calls him a pioneer \"in the reversal of a movement among historians of American culture to discredit the Puritan and colonial period…\"\n\n1924 Thomas J. Holmes was an Englishman with no college education, but he apprenticed in bookbinding and emigrated to the U.S. and become the librarian at the William G. Mather Library in Ohio where he likely met Murdock. In 1924, Holmes wrote an essay for the Bibliographical Society of America identifying himself as part of the Poole-Kittredge lineage and citing Kenneth B. Murdock's still unpublished dissertation. In 1932 Holmes published a bibliography of Increase Mather followed by \"Cotton Mather, A Bibliography\" (1940). Holmes often cites Murdock and Kittredge and is highly knowledgeable about the construction of books. Holmes' work also includes Cotton Mather’s October 20, 1692 letter (see above) to his uncle opposing an end to the trials.\n\n1953 Perry Miller published \"The New England Mind: From Colony to Province\" (Belknap Press of Harvard University Press). Miller worked from the Harvard English Department and his expansive prose contains few citations, but the \"Bibliographical Notes\" for Chapter XIII \"The Judgement of the Witches\" references the bibliographies of TJ Holmes (above) calling Holmes portrayal of Cotton Mather's composition of \"Wonders\" \"an epoch in the study of Salem Witchcraft.\" However, following the discovery of the authentic holograph of the September 2, 1692 letter, in 1985, David Levin writes that the letter demonstrates that the timeline employed by TJ Holmes and Perry Miller, is off by \"three weeks.\" Contrary to the evidence in the later arriving letter, Miller portrays Phips and Stoughton as pressuring Cotton Mather to write the book (p.201): \"If ever there was a false book produced by a man whose heart was not in it, it is \"The Wonders\"….he was insecure, frightened, sick at heart…\" The book \"has ever since scarred his reputation,\" Perry Miller writes. Miller seems to imagine Cotton Mather as sensitive, tender, and a good vehicle for his jeremiad thesis: \"His mind was bubbling with every sentence of the jeremiads, for he was heart and soul in the effort to reorganize them.\n\n1969 Chadwick Hansen \"Witchcraft at Salem\". Hansen states a purpose to \"set the record straight\" and reverse the \"traditional interpretation of what happened at Salem…\" and names Poole and Kittredge as like-minded influences. (Hansen reluctantly keys his footnotes to Burr's anthology for the reader's convenience, \"in spite of [Burr's] anti-Puritan bias…\") Hansen presents Mather as a positive influence on the Salem Trials and considers Mather's handling of the Goodwin children sane and temperate. Hansen posits that Mather was a moderating influence by opposing the death penalty for those who confessed—or feigned confession—such as Tituba and Dorcas Good, and that most negative impressions of him stem from his \"defense\" of the ongoing trials in \"Wonders of the Invisible World\". Writing an introduction to a facsimile of Robert Calef's book in 1972, Hansen compares Robert Calef to Joseph Goebbels, and also explains that, in Hansen's opinion, women \"are more subject to hysteria than men.\"\n\n1971 \"The Admirable Cotton Mather\" by James Playsted Wood. A young adult book. In the preface, Wood discusses the Harvard-based revision and writes that Kittredge and Murdock \"added to a better understanding of a vital and courageous man…\" \n1985 David Hall writes, \"With [Kittredge] one great phase of interpretation came to a dead end.\" Hall writes that whether the old interpretation favored by \"antiquarians\" had begun with the \"malice of Robert Calef or deep hostility to Puritanism,\" either way \"such notions are no longer… the concern of the historian.\" But David Hall notes \"one minor exception. Debate continues on the attitude and role of Cotton Mather…\" \n\nToward the later half of the twentieth century, a number of historians at universities far from New England seemed to find inspiration in the Kittredge lineage. In \"Selected Letters of Cotton Mather\" Ken Silverman writes, \"Actually, Mather had very little to do with the trials.\" Twelve pages later Silverman publishes, for the first time, a letter to chief judge William Stoughton on September 2, 1692, in which Cotton Mather writes \"… I hope I can may say that one half of my endeavors to serve you have not been told or seen … I have labored to divert the thoughts of my readers with something of a designed contrivance…\"\nWriting in the early 1980s, historian John Demos imputed to Mather a purportedly moderating influence on the trials.\n\nCoinciding with the tercentary of the trials in 1992, there was a flurry of publications.\n\nHistorian Larry Gregg highlights Mather's cloudy thinking and confusion between sympathy for the possessed, and the boundlessness of spectral evidence when Mather stated, \"the devil have sometimes represented the shapes of persons not only innocent, but also the very virtuous.\"\n\nThe practice of smallpox inoculation (as opposed to the later practice of vaccination) was developed possibly in 8th-century India or 10th-century China. Spreading its reach in seventeenth-century Turkey, inoculation or, rather, variolation, involved infecting a person via a cut in the skin with exudate from a patient with a relatively mild case of smallpox (variola), to bring about a manageable and recoverable infection that would provide later immunity. By the beginning of the 18th century, the Royal Society in England was discussing the practice of inoculation, and the smallpox epidemic in 1713 spurred further interest. It was not until 1721, however, that England recorded its first case of inoculation.\n\nSmallpox was a serious threat in colonial America, most devastating to Native Americans, but also to Anglo-American settlers. New England suffered smallpox epidemics in 1677, 1689–90, and 1702. It was highly contagious, and mortality could reach as high as 30 percent. Boston had been plagued by smallpox outbreaks in 1690 and 1702. During this era, public authorities in Massachusetts dealt with the threat primarily by means of quarantine. Incoming ships were quarantined in Boston harbor, and any smallpox patients in town were held under guard or in a \"pesthouse\".\n\nIn 1706, Mather's slave, Onesimus, explained to Mather how he had been inoculated as a child in Africa. Mather was fascinated by the idea. By July 1716, he had read an endorsement of inoculation by Dr Emanuel Timonius of Constantinople in the \"Philosophical Transactions\". Mather then declared, in a letter to Dr John Woodward of Gresham College in London, that he planned to press Boston's doctors to adopt the practice of inoculation should smallpox reach the colony again.\n\nBy 1721, a whole generation of young Bostonians was vulnerable and memories of the last epidemic's horrors had by and large disappeared. On April 22 of that year, HMS \"Seahorse\" arrived from the West Indies carrying smallpox on board. Despite attempts to protect the town through quarantine, eight known cases of smallpox appeared in Boston by May 27, and by mid-June, the disease was spreading at an alarming rate. As a new wave of smallpox hit the area and continued to spread, many residents fled to outlying rural settlements. The combination of exodus, quarantine, and outside traders' fears disrupted business in the capital of the Bay Colony for weeks. Guards were stationed at the House of Representatives to keep Bostonians from entering without special permission. The death toll reached 101 in September, and the Selectmen, powerless to stop it, \"severely limited the length of time funeral bells could toll.\" As one response, legislators delegated a thousand pounds from the treasury to help the people who, under these conditions, could no longer support their families.\n\nOn June 6, 1721, Mather sent an abstract of reports on inoculation by Timonius and Jacobus Pylarinus to local physicians, urging them to consult about the matter. He received no response. Next, Mather pleaded his case to Dr. Zabdiel Boylston, who tried the procedure on his youngest son and two slaves—one grown and one a boy. All recovered in about a week. Boylston inoculated seven more people by mid-July. The epidemic peaked in October 1721, with 411 deaths; by February 26, 1722, Boston was again free from smallpox. The total number of cases since April 1721 came to 5,889, with 844 deaths—more than three-quarters of all the deaths in Boston during 1721. Meanwhile, Boylston had inoculated 287 people, with six resulting deaths.\n\nBoylston and Mather's inoculation crusade \"raised a horrid Clamour\" among the people of Boston. Both Boylston and Mather were \"Object[s] of their Fury; their furious Obloquies and Invectives\", which Mather acknowledges in his diary. Boston's Selectmen, consulting a doctor who claimed that the practice caused many deaths and only spread the infection, forbade Boylston from performing it again.\n\n\"The New-England Courant\" published writers who opposed the practice. The editorial stance was that the Boston populace feared that inoculation spread, rather than prevented, the disease; however, some historians, notably H. W. Brands, have argued that this position was a result of the contrarian positions of editor-in-chief James Franklin (a brother of Benjamin Franklin). Public discourse ranged in tone from organized arguments by John Williams from Boston, who posted that \"several arguments proving that inoculating the smallpox is not contained in the law of Physick, either natural or divine, and therefore unlawful\", to those put forth in a pamphlet by Dr. William Douglass of Boston, entitled \"The Abuses and Scandals of Some Late Pamphlets in Favour of Inoculation of the Small Pox\" (1721), on the qualifications of inoculation's proponents. (Douglass was exceptional at the time for holding a medical degree from Europe.) At the extreme, in November 1721, someone hurled a lighted grenade into Mather's home.\n\nSeveral opponents of smallpox inoculation, among them John Williams, stated that there were only two laws of physick (medicine): sympathy and antipathy. In his estimation, inoculation was neither a sympathy toward a wound or a disease, or an antipathy toward one, but the creation of one. For this reason, its practice violated the natural laws of medicine, transforming health care practitioners into those who harm rather than heal.\n\nAs with most colonists, Williams' Puritan beliefs were enmeshed in every aspect of his life, and he used the Bible to state his case. He quoted , when Jesus said: \"It is not the healthy who need a doctor, but the sick.\" Dr. William Douglass proposed a more secular argument against inoculation, stressing the importance of reason over passion and urging the public to be pragmatic in their choices. In addition, he demanded that ministers leave the practice of medicine to physicians, and not meddle in areas where they lacked expertise. According to Douglass, smallpox inoculation was \"a medical experiment of consequence,\" one not to be undertaken lightly. He believed that not all learned individuals were qualified to doctor others, and while ministers took on several roles in the early years of the colony, including that of caring for the sick, they were now expected to stay out of state and civil affairs. Douglass felt that inoculation caused more deaths than it prevented. The only reason Mather had had success in it, he said, was because Mather had used it on children, who are naturally more resilient. Douglass vowed to always speak out against \"the wickedness of spreading infection\". Speak out he did: \"The battle between these two prestigious adversaries [Douglass and Mather] lasted far longer than the epidemic itself, and the literature accompanying the controversy was both vast and venomous.\"\n\nGenerally, Puritan pastors favored the inoculation experiments. Increase Mather, Cotton's father, was joined by prominent pastors Benjamin Colman and William Cooper in openly propagating the use of inoculations. \"One of the classic assumptions of the Puritan mind was that the will of God was to be discerned in nature as well as in revelation.\" Nevertheless, Williams questioned whether the smallpox \"is not one of the strange works of God; and whether inoculation of it be not a fighting with the most High.\" He also asked his readers if the smallpox epidemic may have been given to them by God as \"punishment for sin,\" and warned that attempting to shield themselves from God's fury (via inoculation), would only serve to \"provoke him more\".\n\nPuritans found meaning in affliction, and they did not yet know why God was showing them disfavor through smallpox. Not to address their errant ways before attempting a cure could set them back in their \"errand\". Many Puritans believed that creating a wound and inserting poison was doing violence and therefore was antithetical to the healing art. They grappled with adhering to the Ten Commandments, with being proper church members and good caring neighbors. The apparent contradiction between harming or murdering a neighbor through inoculation and the Sixth Commandment—\"thou shalt not kill\"—seemed insoluble and hence stood as one of the main objections against the procedure. Williams maintained that because the subject of inoculation could not be found in the Bible, it was not the will of God, and therefore \"unlawful.\" He explained that inoculation violated The Golden Rule, because if one neighbor voluntarily infected another with disease, he was not doing unto others as he would have done to him. With the Bible as the Puritans' source for all decision-making, lack of scriptural evidence concerned many, and Williams vocally scorned Mather for not being able to reference an inoculation edict directly from the Bible.\n\nWith the smallpox epidemic catching speed and racking up a staggering death toll, a solution to the crisis was becoming more urgently needed by the day. The use of quarantine and various other efforts, such as balancing the body's humors, did not slow the spread of the disease. As news rolled in from town to town and correspondence arrived from overseas, reports of horrific stories of suffering and loss due to smallpox stirred mass panic among the people. \"By circa 1700, smallpox had become among the most devastating of epidemic diseases circulating in the Atlantic world.\"\n\nMather strongly challenged the perception that inoculation was against the will of God and argued the procedure was not outside of Puritan principles. He wrote that \"whether a Christian may not employ this Medicine (let the matter of it be what it will) and humbly give Thanks to God's good Providence in discovering of it to a miserable World; and humbly look up to His Good Providence (as we do in the use of any other Medicine) It may seem strange, that any wise Christian cannot answer it. And how strangely do Men that call themselves Physicians betray their Anatomy, and their Philosophy, as well as their Divinity in their invectives against this Practice?\" The Puritan minister began to embrace the sentiment that smallpox was an inevitability for anyone, both the good and the wicked, yet God had provided them with the means to save themselves. Mather reported that, from his view, \"none that have used it ever died of the Small Pox, tho at the same time, it were so malignant, that at least half the People died, that were infected With it in the Common way.\"\n\nWhile Mather was experimenting with the procedure, prominent Puritan pastors Benjamin Colman and William Cooper expressed public and theological support for them. The practice of smallpox inoculation was eventually accepted by the general population due to first-hand experiences and personal relationships. Although many were initially wary of the concept, it was because people were able to witness the procedure's consistently positive results, within their own community of ordinary citizens, that it became widely utilized and supported. One important change in the practice after 1721 was regulated quarantine of innoculees.\n\nAlthough Mather and Boylston were able to demonstrate the efficacy of the practice, the debate over inoculation would continue even beyond the epidemic of 1721–22. After overcoming considerable difficulty and achieving notable success, Boylston traveled to London in 1725, where he published his results and was elected to the Royal Society in 1726, with Mather formally receiving the honor two years prior.\n\nThroughout his career Mather was also keen to minister to convicted pirates. He produced a number of pamphlets and sermons concerning piracy, including “Faithful Warnings to prevent Fearful Judgments,” “Instructions to the Living, from the Condition of the Dead,” “The Converted Sinner ... A Sermon Preached in Boston, May 31, 1724, In the Hearing and at the Desire of certain Pirates,” “A Brief Discourse occasioned by a Tragical Spectacle of a Number of Miserables under Sentence of Death for Piracy,” “Useful Remarks. An Essay upon Remarkables in the Way of Wicked Men,” and “The Vial Poured Out Upon the Sea”. His father Increase had preached at the trial of Dutch pirate Peter Roderigo; Cotton Mather in turn preached at the trials and sometimes executions of pirate Captains (or the crews of) William Fly, John Quelch, Samuel Bellamy, William Kidd, Charles Harris, and John Phillips. He also ministered to Thomas Hawkins, Thomas Pound, and William Coward; having been convicted of piracy, they were jailed alongside “Mary Glover the Irish Catholic witch,” daughter of witch “Goody” Ann Glover at whose trial Mather had also preached.\n\nIn his conversations with William Fly and his crew Mather scolded them: “You have something within you, that will compell you to confess, That the Things which you have done, are most Unreasonable and Abominable. The Robberies and Piracies, you have committed, you can say nothing to Justify them. … It is a most hideous Article in the Heap of Guilt lying on you, that an Horrible Murder is charged upon you; There is a cry of Blood going up to Heaven against you.”\n\n\nThe Boston Ephemeris was an almanac written by Mather in 1686. The content was similar to what is known today as the \"Farmer's Almanac\". This was particularly important because it shows that Cotton Mather had influence in mathematics during the time of Puritan New England. This almanac contained a significant amount of astronomy, celestial within the text of the almanac the positions and motions of these celestial bodies, which he must have calculated by hand. \n\nWhen Mather died, he left behind an abundance of unfinished writings, including one entitled \"The Biblia Americana\". Mather believed that \"Biblia Americana\" was the best thing he had ever written; his masterwork. \"Biblia Americana\" contained Mather's thoughts and opinions on the Bible and how he interpreted it. \"Biblia Americana\" is incredibly large, and Mather worked on it from 1693 until 1728, when he died. Mather tried to convince others that philosophy and science could work together with religion instead of against it. People did not have to choose one or the other. In \"Biblia Americana\", Mather looked at the Bible through a scientific perspective, completely opposite to his perspective in \"The Christian Philosopher\", in which he approached science in a religious manner.\n\nMather's first published sermon, printed in 1686, concerned the execution of James Morgan, convicted of murder. Thirteen years later, Mather published the sermon in a compilation, along with other similar works, called \"Pillars of Salt\".\n\n\"Magnalia Christi Americana\", considered Mather's greatest work, was published in 1702, when he was 39. The book includes several biographies of saints and describes the process of the New England settlement. In this context \"saints\" does not refer to the canonized saints of the Catholic church, but to those Puritan divines about whom Mather is writing. It comprises seven total books, including \"Pietas in Patriam: The life of His Excellency Sir William Phips\", originally published anonymously in London in 1697. Despite being one of Mather's best-known works, some have openly criticized it, labeling it as hard to follow and understand, and poorly paced and organized. However, other critics have praised Mather's work, citing it as one of the best efforts at properly documenting the establishment of America and growth of the people.\n\nIn 1721, Mather published \"The Christian Philosopher\", the first systematic book on science published in America. Mather attempted to show how Newtonian science and religion were in harmony. It was in part based on Robert Boyle's \"The Christian Virtuoso\" (1690). Mather reportedly took inspiration from \"Hayy ibn Yaqdhan\", by the 12th-century Islamic philosopher Abu Bakr Ibn Tufail.\n\nDespite condemning the \"Mahometans\" as infidels, Mather viewed the novel's protagonist, Hayy, as a model for his ideal Christian philosopher and monotheistic scientist. Mather viewed Hayy as a noble savage and applied this in the context of attempting to understand the Native American Indians, in order to convert them to Puritan Christianity. Mather's short treatise on the Lord's Supper was later translated by his nephew Josiah Cotton.\n\n\n\n\n"}
{"id": "18932338", "url": "https://en.wikipedia.org/wiki?curid=18932338", "title": "Czech Centres", "text": "Czech Centres\n\nCzech Centres () is an organization of the Ministry of Foreign Affairs of the Czech Republic consisting of offices in 22 countries throughout three continents. It was established for the promotion of the Czech Republic’s history, culture, language, tourism and trade abroad. It is considered an active instrument of foreign policy of the Czech Republic through public diplomacy.\n\nThe organisation dates back to its opening in 1949 as the Cultural and Information Centres (CIS) in Sofia and Warsaw. In the Eastern Bloc, further CIS offices were opened in Budapest (1953), Berlin (1955) and Bucharest (1981). In 1993, the organisation's name was changed from the Cultural and Information Centres to Czech Centres and the range of operations was expanded to encompass exports and tourism with new offices opened outside of Central and Eastern Europe. \n\nIn 2006, Czech Centres opened its first office in Asia in Tokyo, Japan. In the same year, Czech Centres became a member of EUNIC.The most recent office to open was in Seoul, South Korea in 2013. In April 2017, the organisation's Director General Jan Závěšický was removed by the Foreign Minister Lubomir Zaoralek due to 'serious managerial errors'. \n\nAccording to its 2012-2015 Strategic Report, the Czech Centres are tasked with the following activities:\nIn 2015, Czech Centres enrolled 2,063 students in Czech language courses. \n\nWithin these activities, the Czech Centres organises cultural and educational events in the Czech Republic and abroad, such as: \nIn addition to this, it organises several awards and curatorial internships for areas of Czech culture.\n\nThe Czech Centres has branches in 22 major cities: \n\nIn April 2006, Czech Centres opened its new headquarters in Wenceslas Square, Prague. \n\nCzech Centres owns and curates its own galleries in Prague that are open to the general public at Rytířská 539/31.\n\nCzech Centres opened its Czech House in Moscow in 2002. Its primary ambition is to serve as a contact for Russian and Czech businesses. Czech House in Moscow is currently the largest complex owned by the Czech Republic abroad, containing 122 offices, 132 apartments, 87 hotel rooms in addition to conference rooms, a business centre, restaurant, bar, gym fitness centre and sauna, as well as other facilities.\n\n"}
{"id": "16926229", "url": "https://en.wikipedia.org/wiki?curid=16926229", "title": "Defence-in-depth (Roman military)", "text": "Defence-in-depth (Roman military)\n\nDefence-in-depth is the term used by American political analyst Edward Luttwak (born 1942) to describe his theory of the defensive strategy employed by the Late Roman army in the third and fourth centuries AD.\n\nLuttwak's \"Grand Strategy of the Roman Empire\" (1976) launched the thesis that in the third and early fourth centuries, the Imperial Roman army's defence strategy mutated from \"forward defence\" (or \"preclusive defence\") during the Principate era (30 BC-AD 284) to \"defence-in-depth\" in the fourth century. \"Forward-\" or \"preclusive\" defence aimed to neutralise external threats before they breached the Roman borders: the barbarian regions neighbouring the borders were envisaged as the theatres of operations. In contrast, \"defence-in-depth\" would not attempt to prevent incursions into Roman territory, but aimed to neutralise them on Roman soil - in effect turning border provinces into combat zones.\n\nScholarly opinion generally accepts \"forward-defence\" as a valid description of the Roman Empire's defensive posture during the Principate. But many specialists in Roman military history (which Luttwak is not) contest that this posture changed to Luttwak's \"defence-in-depth\" from 284 onwards. Described as \"manifestly wrong\" by the expert on Roman borders, C. R. Whittaker, \"defence-in-depth\" has been criticised as incompatible with fourth-century Roman imperialist ideology (which remained expansionist), Roman strategic planning capabilities, with the evidence of fourth-century Roman historian Ammianus Marcellinus and with the vast corpus of excavation evidence from the Roman border regions.\n\nAccording to this view, the Imperial Roman army had relied on neutralizing imminent barbarian incursions before they reached the imperial borders. This was achieved by stationing units (both legions and auxilia) right on the border and establishing and garrisoning strategic salients beyond the borders (such as the Agri Decumates in SW Germany). The response to any threat would thus be a pincer movement into barbarian territory: large infantry and cavalry forces from the border bases would immediately cross the border to intercept the coalescing enemy army; simultaneously the enemy would be attacked from behind by crack Roman cavalry (\"alae\") advancing from the strategic salient(s). This system obviously required first-rate intelligence of events in the barbarian borderlands, which was provided by a system of watch towers in the strategic salients and by continuous cross-border scouting operations (\"explorationes\").\n\nAccording to Luttwak, the forward defence system was always vulnerable to unusually large barbarian concentrations of forces, as the Roman army was too thinly spread along the enormous borders to deal with such threats. In addition, the lack of any reserves to the rear of the border meant that a barbarian force that successfully penetrated the perimeter defenses would have unchallenged ability to rampage deep into the empire before Roman reinforcements could arrive to intercept them. The first major challenge to forward defense was the great invasion of Germanic tribes (esp. Quadi and Marcomanni) across the Danube in 166-7, which began the Marcomannic Wars. The barbarians reached as far as Aquileia in northeastern Italy and were not finally expelled from the empire until 175. But the response of the imperial high command was not to change the forward defence strategy, but to reinforce it (by the founding of 2 new legions under Marcus Aurelius and 3 more under Septimius Severus and probably matching auxiliary forces). It was only after the catastrophic military crises of 251-71 that the Roman command under Diocletian turned to defence-in-depth: but only out of necessity, not conviction, as there were attempts to return to forward defence as late as Valentinian I (ruled 364-75) Forward defence had become simply too costly to maintain, especially with the emergence of a more powerful and expansionist Persian empire (the Sassanids) which required greater deployments in the East.\n\nThe essential feature of defence-in-depth, according to Luttwak, was the acceptance that the Roman frontier provinces themselves would become the main combat zone in operations against barbarian threats, rather than the barbarian lands across the border. Under this strategy, border forces would not attempt to repel a large incursion. Instead, they would retreat into fortified strongholds and wait for mobile forces (\"comitatenses\") to arrive and intercept the invaders. Border forces would be substantially weaker than under forward defence, but their reduction in numbers (and quality) would be compensated by the establishment of much stronger fortifications to protect themselves: hence the abandonment of the old \"playing-card\" rectangular design of Roman fort. The new forts were so designed that they could only be taken with the use of siege engines (which barbarians generally lacked): square or even circular layout, much higher and thicker walls, wider perimeter berms and deeper ditches; projecting towers to allow enfilading fire; and location in more defensible points, such as hilltops. At the same time, many more small forts were established in the hinterland, especially along roads, to impose delays on the invaders. Also, fortified granaries were built to store food safely and deny supplies to the invaders. Finally, the civilian population of the province was protected by providing walls for all towns, many villages and even some villas (large country houses); some pre-Roman hillforts, long since abandoned, were re-occupied in the form of new Roman walled settlements. The invading force would thus find itself in a region peppered with strongholds in enemy hands and where it could not easily get access to sufficient supplies. If the invaders ignored the strongholds and advanced, they risked sorties and attacks in the rear. If they attempted to besiege the strongholds, they would give the mobile troops valuable time to arrive. Overall, the aim of defence-in-depth was to provide an effective defence system at a sustainable cost, since defence-in-depth required much lower troop deployments than forward defence. To be more precise, the cost was transferred from general taxpayers to the people of the frontier provinces, especially the rural peasantry, who, for all the fortifications, would often see their family members killed or abducted, homes destroyed, livestock seized and crops burnt by marauding barbarians.\n\nLuttwak's work has been praised for its lucid analysis of, and insights into, issues regarding Roman military dispositions, and for stimulating much scholarly debate about these issues. But the validity of his basic thesis has been strongly disputed by a number of scholars, especially in a powerful critique by B. Isaac, the author of the fundamental study of the Roman army in the East (1992). The objections fall under two broad headings: (1) The Roman empire did not have the intelligence and planning capacity to sustain a \"grand strategy\" and in any case was not defensive in ideology or policy.(2) Defence-in-depth is not, in the main, consistent with the literary and archaeological evidence.\n\nLuttwak's thesis of an imperial grand strategy rests on a number of assumptions: (a) that the empire's strategic posture was basically defensive; (b) that Roman expansion and choice of borders were systematic and rational, with the main objective of securing defensible borders; (c) that the Roman government's primary concern was to ensure the security of its provincial subjects. But Isaac demonstrates that these assumptions are probably false and result from inappropriate application of modern concepts of international relations and military strategy to the ancient world. Isaac suggests that the empire was fundamentally aggressive both in ideology and military posture, up to and including the fourth century. This was demonstrated by the continued military operations and siting of fortifications well beyond the imperial borders. The empire's expansion was determined mostly by the ambitions of emperors; and that the choice of borders, to the extent that they were planned at all, was more influenced by logistical considerations (e.g. rivers, that were critical conduits for supplies) rather than defensibility. Finally, the imperial government probably was far less concerned with the security of its subjects than would be a modern government. Isaac shows that the empire did not develop the centralised military planning, or even accurate enough cartography, necessary to support grand strategy. Rome did not develop the equivalent of the centralised general staff of a modern army (and even less strategic studies institutes of the kind frequented by Luttwak). Emperors depended on the theatre military commanders (the provincial governors, later the \"magistri militum\" and \"duces\") for all military intelligence.\n\nThere is also little unequivocal archaeological and literary evidence to support defence-in-depth.\nLuttwak's defence-in-depth hypothesis appears to rely on two basic features: (a) deepened fortified border zones: \"It became necessary to build forts capable of sustained resistance, and these fortifications had to be built in depth, in order to protect internal lines of communication. Instead of a thin perimeter line on the edges of provincial territory, broad zones of military control had to be created...\" \"The thin line of auxiliary 'forts' and legionary 'fortresses' was gradually replaced by a much broader network of small fortified hard-points (in the hands of) scattered groups of static \"limitanei\"...\" The hypothesis thus predicts the establishment of fortifications well into the interior of border provinces, rather than just a string of bases right on the border line; (b) the use of the \"comitatus praesentales\" (imperial escort armies) as interception forces to deal with incursions. In this regard, it is to be noted that Luttwak terminates his analysis in 350, before the establishment of the regional \"comitatus\". The interception forces were thus the single large \"comitatus\" of Constantine, and, later, the 3 \"comitatus\" known from Ammianus to exist in 350 of Gaul, Illyricum and the East. But there are serious difficulties with both propositions.\n(a) J.C. Mann points out that there is no evidence, either in the \"Notitia Dignitatum\" or in the archaeological record, that units along the Rhine or Danube were stationed in the border hinterlands. On the contrary, virtually all forts identified as built or occupied in the fourth century on the Danube lay on, very near or even beyond the river, strikingly similar to the second-century distribution.\n\nLuttwak seizes on the situation in Palaestina Salutaris (mainly the former Arabia Petraea) province, which was dotted with forts all over, as an example of defence-in-depth. But here it cannot be proven that the defence system developed only in the fourth century. It may have dated from as early as the second century. In any case, Isaac shows that these \"in-depth\" forts were probably used for the purposes of internal security against rebels and brigands rather than defence against external threat. Indeed, such material as can be dated to Diocletian suggests that his reorganisation resulted in a massive reinforcement of linear defence along his newly built desert highway, the \"Strata Diocletiana\".\n\nIn Britain, the configuration of a large number of fourth-century units stationed between Hadrian's Wall and the legionary fortresses at \"Deva\" (Chester) and \"Eboracum\" (York), superficially resembles defence-in-depth. But the same configuration existed in the second century, and was due to the short length of the frontier, forcing a \"vertical\" rather than horizontal deployment, as well as the need to protect the coastlines from seaborne attack. It was not defence-in-depth in the Luttwak sense.\n\nSo strong is the evidence for forward defence under Diocletian that Luttwak himself struggles to avoid that conclusion. At one point, he describes it as \"shallow defence-in-depth\", a contradiction in terms. At another, he admits that Diocletian's policy was a \"sustained attempt to provide a preclusive (i.e. forward) defence of the imperial territory\". Indeed, the unfavourable evidence forces Luttwak to adopt a self-contradicting thesis. While claiming that the basic strategy of the fourth century was defence-in-depth, he admits that there were repeated attempts by the stronger emperors (up to and including Valentinian I) to revert to forward defence. This obviously casts doubt on whether a defence-in-depth strategy was ever contemplated or implemented in reality.\n\nThe Romans continued to assist the client tribes to defend themselves in the fourth century e.g. the construction by Constantine's army of two massive lines of defensive earthworks (the Devil's Dykes in Hungary and the Brazda lui Novac de Nord in Romania) well beyond the Danube (100–200 miles forward) to protect the client tribes of the Banat and the Wallachian plain against Gothic incursions. This system of a series of buffer zones of \"client tribes\" clearly represents an efficient and economical form of \"forward defence\". It contradicts the proposition that the border provinces of the empire were themselves envisaged as buffer zones.\n\nIn the absence of any evidence of \"defensive depth\" in the stationing of border forces, the only \"depth\" left were the \"comitatus praesentales\" (imperial escort armies) stationed in the interior of the empire. But Luttwak himself admits that these were too distant from the frontier to be of much value in intercepting barbarian incursions: their arrival in theatre could take weeks, if not months. Although they are often described as \"mobile field armies\", in this context \"immobile\" would be a more accurate description. Luttwak terminates his analysis in mid-fourth century, just before the establishment of the regional \"comitatus\". But the positioning of the latter, right on the borders or within 60 miles (100 km) of them, seems strikingly similar to that of the legions in the second century. It could be argued that the deployment of the regional \"comitatus\" was simply an admission that Zosimus' criticism of Constantine's policy was valid and that effective forward defence required reinforcement of the \"limitanei\" troops.\n\nA further powerful objection to defence-in-depth is that it is clear from Ammianus that Rome continued major offensive operations across the imperial borders in the fourth century. These were strikingly similar to the pincer movements described by Luttwak as being characteristic of forward defence in the early Principate. For example, Valentinian I's campaign against the Quadi in 375. The barbarian tribe that were the target of the operation rarely resisted the Romans in pitched battle and more often took refuge in forests and hills. The Romans would then systematically ravage their crops and burn their hamlets until starvation forced the barbarians to surrender. They would then be forced to conclude treaties of alliance with the Romans, often involving the client status described below. But there was no aspect of this activity that was peculiar to the fourth century.\n\nOne \"defence strategy\" the empire certainly employed was a system of treaties of mutual assistance with tribes living on the imperial frontiers, but this was not unique to the fourth century, but a long-standing practice dating to the days of the late Republic. The Romans would promise to defend the ally from attack by its neighbours. In return, the ally would promise to refrain from raiding imperial territory, and prevent neighbouring tribes from doing the same. In many cases, the loyalty of the ally would need to be further secured by gifts or regular subsidies. In some cases, the Romans would assume a loose suzerainty over the tribe, in effect dictating the choice of new chiefs. This practice was applied on all the frontiers: Germans along the Rhine, Sarmatians along the Danube, Armenian kings and Caucasian and Saracen tribes on the Eastern frontier and Mauri in North Africa. On the desert frontier of Syria, the Romans would appoint a Saracen sheikh (called a \"phylarchos\" in Greek), according him an official rank in the Roman hierarchy, to \"shadow\" each \"dux limitis\" in the sector. In return for food subsidies, the phylarchs would defend the desert frontier against raiders.\n\nAs regards imperial ideology and central defence planning, Adrian Goldsworthy argues that both sides of the debate, which continues vigorously, have made valid points. Some degree of central planning is implied by the disposition, frequently altered, of legions and auxiliary forces in the various provinces. In addition, although the empire's ideology may have been offensive in nature, border fortification such as Hadrian's Wall was clearly defensive. It is a fact that the empire ceased to expand its territory after the rule of emperor Trajan (98-117). Thereafter, the borders remained largely static, with indeed a few losses of territory: the immediate evacuation of Trajan's conquests in Mesopotamia by his successor Hadrian (r. 117-38) and of the Agri Decumates in Germany and of Dacia in the third century. Thus, even if the empire's ideology and propaganda was expansionist (the slogan \"imperium sine fine\"- \"empire without limits\" - was common), its policy was in reality generally non-expansionist.\n\nAs regards Luttwak's defence-in-depth theory itself, there appears to be insufficient clearcut evidence to support it and massive evidence against it. Mann's critique was written in 1979, so does not take account of the substantial corpus of archaeological data accumulated since. But the latter overwhelmingly contradicts a defence-in-depth strategy. Virtually all identified forts built in the fourth century lay on, very near or even beyond the border. Some evidence of fortifications in the hinterland has come to light that could be consistent with defence-in-depth. But such features cannot be unequivocally linked with military units. Furthermore, the fourth-century army's \"defence\" posture shares many features with the earlier forward-defence policy. The undoubted enhanced fortification of forts and other buildings, as well as cities in the border provinces (and deep in the interior of the empire including Rome itself) may therefore be interpreted as simply an admission that forward defence was not working as well as in the earlier centuries. Either barbarian pressure was much greater and/or the Roman border forces were less effective than before in containing it.\n\n\n\n"}
{"id": "57663720", "url": "https://en.wikipedia.org/wiki?curid=57663720", "title": "Deknong Kemalawati", "text": "Deknong Kemalawati\n\nDeknong Kemalawati (2 April, 1965, Meulaboh, Aceh) is one of the leading poetess of modern Indonesia, chairman of the Art Council Banda Aceh, winner of the Literary Prize of the Government of Aceh.\n\nShe graduated from the Pedagogical Faculty of Syiah Kuala University, workrd as a mathematics teacher in high school. One of the founders of the Institute of Culture and Society (Lapena) (1998), Chairman of the Art Council of Banda Aceh . She used to take part in the poetry workshops organizes by Dimas Arika Mihardja.\n\nShe writes poetry from the school bench. The first publications were published in newspapers and magazines during the student's period. The first author's collection \"Letter from a no-man country\" was published in 2006. The novel “Seoulusah” about the events of the 2004 Indian Ocean earthquake and tsunami was published in 2007 and received good reviews from critics. To date, on the poet's account are more than ten collections of poems, including some in collaboration with other poets.\n\nIn addition to literary activity, he takes part in theatrical productions and performances of national dances. In November 2012, she successfully participated in the Days of Indonesian Poetry (Pekan Baru). In October 27-28, 2017, she visited Kazan as part of the Aceh cultural group, where she recited her poems and represented the national dances. Many times she represented Indonesia in international forums: in 2016 in Kuala Lumpur on the World Poetry Readings of the Great Malay Nusantara organization (Numera) and the meeting of Nusantara poets, and in 2018 in Sabah during the discussion of the Indonesian and Malaysian poets on the role of poetry in the development of bilateral relations.\n\n\n...  The sincere voice of D. Kemalavati, coming from the bottom of her heart, is heard in the sounds of rebana as the unfading syair Teungku Sik Pante Kullu from Hikayat about the holy war. In the poems of D. Kemalavati we see the beautiful face of her homeland, the long-suffering Aceh ... Metaphors, diction, imbued with symbolism, leave in our souls the trace that makes us love the beauty ... Her poems are an integral part of Nusantara poetry and the whole world. - Ahmad Kamal Abdullah, Malaysian National Laureate, S.E.A. Write Award Laureate \n\n\n"}
{"id": "19171912", "url": "https://en.wikipedia.org/wiki?curid=19171912", "title": "Devonshire Hunting Tapestries", "text": "Devonshire Hunting Tapestries\n\nThe Devonshire Hunting Tapestries are a group of four magnificent Flemish tapestries dating from the mid-fifteenth century. These enormous works, each over 3 metres wide, depict men and women in fashionable dress of the early fifteenth century hunting in a forest. The tapestries formerly belonged to the Dukes of Devonshire, but in 1957 were accepted by HM Government in lieu of tax payable on the estate of the 10th Duke of Devonshire and allocated to the Victoria and Albert Museum, where they remain. \n\nThe 6th Duke described using his 'spare' tapestry to insulate the Long Gallery at Hardwick Hall in the 1840s, a practice which saved these rare Gothic hangings from being discarded. The tapestries depict a \"Deer Hunt\", \"Falconry\", a \"Swan and Otter Hunt\" and a \"Boar and Bear Hunt\". The hunt was a particularly powerful theme and would have been a familiar pastime to many of the high-born individuals and families who owned tapestries. Hunting was both a stylized sport and an important source of the only meats considered noble.\n\nThis detail is from the \"Boar and Bear Hunt\" Tapestry, made in the 1420s, and shows men carrying special boar-spears, which have cross-bars designed to stop the charge of the boar and keep its deadly tusks at arms length. Much of the charm of these scenes lies in the elaborate costume detail. The lady crossing the stream on the lower right has 'Monte le Desire' inscribed on her flowing sleeve. This is the opening line of a popular song of the period. The practice of embellishing one's clothes with such words was a medieval equivalent of having a stylish slogan printed on a T-shirt except that, in the tapestry, weaving on a horizontal loom has reversed the letters.\n\nThe dress of the participants is of the type worn at court, particularly that of Burgundy, which had control of the tapestry-weaving areas in the southern Netherlands. It is unlikely that any serious hunting took place in such restricting and exotic clothes.\n\n"}
{"id": "113193", "url": "https://en.wikipedia.org/wiki?curid=113193", "title": "Earthworm Jim (video game)", "text": "Earthworm Jim (video game)\n\nEarthworm Jim is a 1994 run and gun platform game developed by Shiny Entertainment, featuring an earthworm named Jim, who wears a robotic suit and battles evil. The game was released for the Sega Genesis, and subsequently ported to a number of other video game consoles.\n\nIt was well received by critics, and received a sequel, \"Earthworm Jim 2\", in 1995. In 2010, Gameloft developed and released a high definition remake for the PlayStation 3 and Xbox 360, titled \"Earthworm Jim HD\".\nIn February 2018, Gameloft’s contract with Interplay ended, so Earthworm Jim HD as well as the Nintendo DSI , Wii and Windows Phone ports were removed from digital stores.\n\nThe game plays as a 2D sidescrolling platformer with elements of a run and gun game as well. The player controls Jim and must maneuver him through the level while avoiding obstacles and enemies. The player can use Jim's gun as a method of shooting enemies, or his head as a whip for whipping enemies. The whip move also allows the player to grab hold of, and swing from, certain hooks in the game. Some levels have additional requirements beyond merely getting to the end of the level. For example, the level \"For Pete's Sake\", involves making sure the computer-controlled Peter Puppy character gets through the level unharmed, which is accomplished by whipping him to make him jump over pits, and defeating enemies before they can damage him. Failure to do so results in Peter lashing out at Jim, taking away from his health.\n\nLevels commonly culminate with a boss battle. The game incorporates a large variety of villains in the boss battles, including Psy-Crow, Queen Slug-for-a-Butt, Evil the Cat, Bob the Killer Goldfish, Major Mucus, and Professor Monkey-For-A-Head. Two villains made their only appearance in this game, Chuck, a junkyard man with a tendency to vomit bizarre objects, and Doc Duodenum, a crazed organ of a giant alien.\n\nIn-between most levels, a racing level called \"Andy Asteroids\" is played. Unlike the rest of the game, it places the viewpoint behind Jim. The player must direct Jim on his rocket, in a race against Psycrow, through a tube-like structure while collecting items and boosts and avoiding asteroids. If the player wins, the next level is started instantly. If the player loses, a special boss fight against Psy-Crow must be won in order to progress to the next level.\n\nOther variations in gameplay occur over the course of the game as well, such as a competitive bungie-jumping and fighting level, and an underwater maze that must be traversed both within a timelimit and without crashing too many times.\n\nJim is a normal earthworm, until a special \"super suit\" falls from the sky and allows him to operate much like a human, with his \"worm-part\" acting as a head and the suit acting as arms, body, and legs. Jim's task is two-fold, he must evade the game's many antagonists, who are after him because they want the suit back, and also rescue and protect Princess What's-Her-Name from them. The game plays out with Jim eluding and defeating all enemies, and saving Princess What's-Her-Name. However, not only does she not return Jim's affection, but she is also crushed by the flying cow that was launched at the beginning of the game by Jim himself.\n\nPlaymates Toys, finding success with the license for Teenage Mutant Ninja Turtles, wanted to start their own franchise. Inspired by the success of the \"Sonic the Hedgehog\" series with \"Sonic the Hedgehog\" and \"Sonic the Hedgehog 2\", they decided that they wanted to start the franchise as a video game, a rare approach at the time. From there, the game's design actually started with Douglas TenNapel's simple sketch of an earthworm that he presented to Shiny Entertainment. Impressed, programmer David Perry and the rest of Shiny bought the rights to Earthworm Jim from TenNapel, and started developing the game. From there, TenNapel would work on doing the game design, creating level ideas, and voicing Jim's character, while Perry and the other programmers created other characters and game mechanics. Perry recounted that the giant hamster \"was drawn by one of our guys at three o'clock one morning\".\n\nThe game's crazy atmosphere, world, and characters was due to the fact that the company had previously always been restricted to doing licensed games, like 7up's \"Cool Spot\", where they had to conform to the other company's preset limitations. In that respect, the game was actually created as a satire of platform video games at the time; for instance, \"Princess-What's-Her-Name\" was a parody of how so many video games had throw-away female characters to be saved.\n\nThe original version was released for the Genesis in 1994. A version for the Super NES was released shortly after the original and is largely the same as the Genesis version. The Super NES version has altered graphics, with alternate backgrounds and special effects, but lacked some sound effects and one of the levels from the Genesis version (titled \"Intestinal Distress\"). The stated reason for the Genesis version having the extra level was that the \"Genesis version was more easily compressed and had the room for the bonus level\". Subsequently, Nick Bruty has stated in an interview that Sega asked Shiny Entertainment to add a level exclusively to the Sega version in exchange for reduced cartridge cost. Nick states that they designed the level overnight, and completed coding and testing the level in a single day–the day the game was sent to be printed to the consoles. The Japanese Mega Drive version was available exclusively via the Sega Channel service.\n\nThe game's Genesis release was promoted with a television commercial in which an elderly woman tells a bedtime story about Earthworm Jim while eating live earthworms (actually plastic props). The networks airing the commercial received so many complaints from nauseated viewers that the commercial was pulled in some markets, including stations in Portland, Spokane, and Sacramento.\n\nThe game's Special Edition was released for the Sega Genesis add-on, the Sega CD, and Windows 95. It was based off the Genesis version, contained all of its levels, plus some extended section to the levels and a single completely new level, titled \"Big Bruty\", a new remixed CD audio soundtrack, as well as around 1,000 more frames of in-game animation. These versions were also the only ones to contain alternate endings when winning on the \"Practice\" or \"Difficult\" difficulties, in which a narrator rambles on about many (false) facts about worms or congratulates the player in a similar absurd manner respectively.\n\nAnother special edition of the game was released exclusively through the Sega Channel for a contest dubbed \"The Great Earthworm Jim Race\". This version included a secret room which, when reached by the first 200 players, would display a password and a toll-free telephone number. Those that called the number were awarded prizes.\n\nEurocom ported a compressed and scaled down version for the Game Boy. It was hindered by the lack of color, lack of graphical detail due to both processor and small screen size, choppy animations, and a lack of buttons, which made it hard to control. This version was also ported to the Game Gear, which included color graphics, but still suffered from all of the other problems of the Game Boy version. A direct port of the Game Gear version was also brought to the Sega Master System, but only in Brazil. However, it only has 4 levels, and the boss in What The Heck? is missing.\n\nA conversion of the game was also being developed by German studio Softgold and planned to be published by Atari Corporation for both the Atari Jaguar and the Atari Jaguar CD, but it was never finished due to the departure of Normen B. Kowalewski from Atari, who was the lead developer of the port, sometime between or at the end of 1995. The alpha prototype, which consisted of basic character animations, and the source code of the conversion are currently lost.\n\nThe game also had a MS-DOS port released in a package titled \"\" (along with the MS-DOS port of \"Earthworm Jim 2\") with redrawn graphics and missing the level \"Intestinal Distress\". The game was ported by Rainbow Arts.\n\nIn 2001 Game Titan ported the Super NES version to the Game Boy Advance. Despite the extra power of the Game Boy Advance, this version still ran very poorly, with poor animation, missing details, and was widely criticized.\n\nThe game was re-released digitally on a number of platforms in the late 2000s as well. The original Genesis version was released through Wii's Virtual Console service in Europe on October 3, 2008, and in North America on October 27, 2008. The MS-DOS version was re-released through DOSbox emulation on GOG.com and Steam.\n\nIn 2009 Gameloft released digitally an updated remake of the game on a number of mobile/handheld platforms. The remake was made entirely from scratch, without using the original game's code, and featured overhauled and smoothed graphics, a remixed soundtrack, re-recorded voice of Jim and touchscreen controls. Completely new, computer-themed levels were added, however, some previous features were lost. While extended version of the \"New Junk City\" level from the \"Special Edition\" is included, the \"Big Bruty\" (\"Special Edition\" new level) and \"Who Turned out the Lights?\" (secret level from the original release) are not present. It was later released as a download for the Nintendo DSi as DSiWare, which is also downloadable on the Nintendo 3DS system. The only new addition for the DSiWare version was an extra minigame that involved the player using the system's camera on their own face, in order to mimic the same faces Jim would make on-screen. The Gameloft remake was also later released digitally on Xbox Live Arcade and PlayStation Network as \"Earthworm Jim HD\". It featured a comic book-like introduction, three new computer-themed bonus levels, and a 4 player multiplayer mode with special levels based on already existing ones.\n\nReception for the game was very positive. \"Earthworm Jim\" was awarded Best Genesis Game of 1994 by \"Electronic Gaming Monthly\", \"GamePro\" gave the Genesis version a perfect score, and \"Famicom Tsūshin\" scored the Super Famicom version of the game a 30 out of 40. \"Earthworm Jim\" was rated the 114th-best game made on a Nintendo System in \"Nintendo Power\"<nowiki>'</nowiki>s Top 200 Games list.\n\nThe game has been noted for its fluid animation, featuring a hand-drawn style that was unusual for 16-bit releases. \"GamePro\" argued that the game has \"the most innovative game play since Sonic first raced onto the Genesis\", backing up the point by noting \"In the first level, New Junk City, Jim leaps off old tires, climbs strange crevices and cliffs, swings from chains, and creeps through a maze of garbage - and that's the most traditional level in the game!\" \"Electronic Gaming Monthly\" gave rave reviews for both the Genesis and SNES versions, praising animations, long levels, and warped sense of humor. One of their reviewers summarized that \"This game was made by a gamer, and it shows.\" With regard to the game's overall appeal, a review from GameZone stated \"Back when platformers were the king of genres, \"Earthworm Jim\" made its presence known as the 'cool kid on the block' by appealing to many demographics. Obtaining a moderate difficulty level and establishing itself with stylish humor, \"Earthworm Jim\" was a financial and critical success for Interplay and Shiny Entertainment. Even though I feel the sequel is the best of the series, the original still is able to stand out on its own.\" The review also went on to praise the soundtrack from Mark Miller as well. \"Next Generation\" reviewed the Genesis version of the game, and stated that \"Sure, it's only a slick 16bit platform game. And anyone looking for anything revolutionary will be disappointed - there's nothing strictly \"new\" here. But it's a whole load of fun, and \"that\"'s what counts.\"\n\nThe Sega CD enhanced port was also well received. \"GamePro\" and \"Electronic Gaming Monthly\" remarked that whereas most Sega CD ports simply add enhanced music, \"Earthworm Jim\" included a number of worthwhile additions such as new animations, new levels, and the new homing missile weapon. \"EGM\" gave it their \"Game of the Month\" award, and \"GamePro\" later awarded it Best Sega CD Game of 1995.\n\n\"Electronic Gaming Monthly\" reviewed that the Game Gear version has impressive graphics by portable standards but is crippled by the Game Gear's limited two-button control, frequent screen blurring, and frustrating difficulty. \"GamePro\" also felt the two-button control to be a serious problem, but concluded the Game Gear version to be \"Overall ... fine for fans who want to take their EWJ show on the road.\"\n\nReviewing the Windows 95 version, \"Maximum\" claimed \"it's not only a damn fine platform game, it's probably the best the PC has seen to date.\" They particularly praised the non-frustrating challenge and the strong personality of the graphics. \"Computer Games Strategy Plus\" named the computer release of \"Earthworm Jim\" the best arcade title of 1995. \n\n\"Next Generation\" reviewed the Sega CD version of \"Earthworm Jim: Special Edition\", and stated that \"While a prepackaged, presold character, who's as cynically calculated for hip success as Earthworm Jim does rub us the wrong way, we admit there was a cracking good game to back him up, and this new CD version is even better.\" \"Entertainment Weekly\" gave the game an A and wrote that the game is too similar to the Genesis and SNES versions to warrant a new purchase.\n\nHowever, later Gameloft remakes of the games received mixed reviews. Reception for the 2010 remake, \"Earthworm Jim HD\", less positive. IGN and GameSpot both felt that the surreal art style and animation stood the test of time, but felt that some gameplay aspects and controls felt dated in comparison to modern platformers. Similarly, the iPhone version of the game was criticized for its sloppy controls, mostly due to being touchscreen only.\n\nA sequel, \"Earthworm Jim 2\", was released in 1995. It was released in the same manner as the original; first on the Sega Genesis, and then ported to many other systems. It too was generally well received. Two further games, \"Earthworm Jim 3D\" for the Nintendo 64 and PC, and \"\" for the Game Boy Color, were produced in 1999. However, they were developed without the involvement of Shiny Entertainment and were mostly met with negative reviews. An enhanced remake, \"Earthworm Jim (PSP)\" by Atari was planned for a 2007 release for the PlayStation Portable, but was ultimately cancelled.\n\nThe game also inspired non-video game products, such as the \"Earthworm Jim\" television series, a comic book series, and a line of action figures. Earthworm Jim is playable as a standard character in \"ClayFighter 63⅓\", and as a secret character in the \"Sculptor's Cut\" version of that game.\n\n"}
{"id": "31615812", "url": "https://en.wikipedia.org/wiki?curid=31615812", "title": "Education NGOs", "text": "Education NGOs\n\nThe UNESCO stated “education for sustainable development is a broad task that calls for the full involvement of multiple educational organizations and groups in bureaucracies and civil societies. These include Non-Governmental Organizations or NGOs.\n\nThe language of education used by nation-states as well as international, intergovernmental organizations, non-governmental organizations, also known as NGOs, (both transnational and national), and agents of civil society (many of which belong to the aforementioned categories) contributes heavily to the self-identification of individuals. NGOs, can be defined as \"formal organizations, and as such, they emerge when a group of people organize themselves into a social unit “that was established with the explicit objective of achieving certain ends and formulating rules to govern the relations among the members of the organization and the duties of each member” (Blau and Scott, 1970)\".\n\nBy understanding the language of each, one can reach a greater understanding of the multiple, conflicting, and overlapping educational ideologies employed across the globe. The issue of education on an international scale is also embedded in a complex framework of international relations which alters the effectiveness of those who employ the ideologies in a practical manner. Education NGOs differ in practice and ideology based on the previously mentioned factors. However, in the age of globalization, travel and communication have contributed to new ideas about individual identities in relation to the a global - rather than national - community.\n\nAn article published in The Nation (Pakistan newspaper) on July 2, 2018 described education as crucial to the development of all individuals. Education nurtures a person’s way of thinking, inculcates values, and teaches skills needed to succeed in life. At the same time, it helps boost the social, economic, and political progress of nations. It said governments that establish an efficient education system reaps benefits in the long-term.\n\nPhilosophy is both the product of, and basis for, the educational ideologies that shape members of our society. Furthermore, the language utilized to describe the actors within society is key to understanding the outcomes of differing educational ideologies. Jean Jacques Rousseau is one of the philosophers who recognized the organizational utility of language in shaping nation-states. Although this language would only matter in the mind of single individuals, the proliferation of these concepts and their subsequent adoption was and is vital to their identification as members of a nation-state, as well as in distinguishing them from others. The following words; \"people\", \"nation\", \"citizen\", \"body politic\", \"popular sovereignty\", \"other\", and \"foreigner,\" were present throughout Jean Jacques Rousseau's social contract theory, which resulted in the creation of imagined boundaries that bind individuals together for mutual benefit. Education was the tool that would construct a common understanding of these terms and bind people together. Jacques Rousseau proposed this nationalistic form of education; \"It is education,\" he said,\"that should put the national stamp on men's minds and give the direction to their opinions and tastes which will make them patriots... National education is the privilege of free men who share common interests and are united under law\". Rousseau's philosophy shaped and reflects the current system of nation-states in the international system. It also exposes the political nature of education; public education under this philosophy is therefore a tool of the nation-state which is used to consolidate a certain identity and will.\n\nEducation is rapidly becoming essential to attaining social mobility and economic stability, especially in an increasingly globalized world where technological skills and knowledge are necessary to participate in the economy. Considering this reality, more educational institutions are seeking to incorporate more STEM courses and career training into their curriculums. However, these educational opportunities are not widely offered, which many scholars have argued has led to greater global inequality.\n\nGlobalization has also generated a greater push for common, global educational standards such as the United Nations' Millennium Development Goals and Sustainable Development Goals. Yet, some critics argue that these standards pushes a Westernized concept of quality education and focuses on economic benefits rather than the goals of sustainable development and global equality.\n\nWhere education has been the role of the nation state, globalization has created new institutions including global regulatory organizations, global mass media and the aforementioned global flow of populations, which have contributed to the weakening of the nation state in education. Global regulatory organizations include the intergovernmental organizations , such as membership organizations like the World Bank and World Trade Organization that regulate the world economy, as well as other international organizations such as the United Nations. These organizations operate within a context of global norms that are established, and laws that are passed, with the influence of non-state actors, or non-governmental organizations. While global regulatory organizations focus on the establishment and enforcement of policies by exerting influence over the conditions of monetary loans, NGOs attempt to establish and enforce norms through exerting a certain sense of moral authority.\n\nThe governments of some African countries do not fully welcome the idea of NGO contributions towards education; thus such governments tend to be constantly involved with activities of NGOs in the education sector. However, governments regulate the activities of NGO's based on their economic standing, \"administrative standing\" and \"historical relationship\" with NGO's. Instead of direct funding, government relationship between with NGOs and non state actors in education is focused on promotion of non state actors.\n\nIn 2000, the United Nations formed the Millennium Development Goals (MDGs) - eight goals that sought to address global poverty, educational disparities, gender inequality and health crises. The 191 member states of the UN committed to achieving the Millennium Development Goals (MDGs) by 2015.\n\nGoal 2 specifically focused on providing \"universal primary education,\" for children in every nation. The UN Education and Scientific Council (UNESCO) determined primary education to be \"the beginning of systematic apprenticeship of reading, writing and mathematics.\" In order to measure the progress towards achieving Goal 2, UNESCO utilized three gauges; the literacy rate of the 15 - 24 year old demographic, the net total enrollment in the primary education system and the number of students who start first grade and reach the fifth grade.\n\nIn a 2003 Education for All report, data revealed that the progress towards Goal 2 in Sub-Saharan Africa had been unsuccessful. The report disclosed that 58% of students in the area were enrolled in a primary school and that there was a 15% rate of repeating school grades in half of the countries within the region. However, research also indicates that the MDGs were in fact attained if measured on the overall, global scale. The rate of children of primary school age not receiving an education was reduced from 100 million to 57 million between the years 2000 to 2015 During this same period of time, the global literacy rate of people between 15–24 years old increased from 83% to 91%.\n\nIn 2016, the United Nations enacted the Sustainable Development Goals (SDGs) - a set of 17 global goals to be accomplished by 2030. Through Goal 4 (SDG 4), the United Nations seeks to \"ensure inclusive and equitable quality education and promote lifelong learning opportunities for all.\" \n\nAs of 2017, the UN reported that despite that more children in the world are attending school than ever before, millions of children still do not meet standard levels in math and reading. According to the current reports, less than half of students in 9 out of 24 sub-Saharan African nations and 6 of 15 Latin American countries had achieved proficient standards in math by the time they finished primary school. Additionally less than half of the children in a quarter of sub-Saharan African countries, who completed a primary education had reached proficient, reading standards. Due to the low achievement rates in these areas, the UN stressed that efforts in educational attainment must be increased in sub-Saharan Africa and Southern Asia as well as \"for vulnerable populations, including persons with disabilities, indigenous people, refugee children and poor children in rural areas.\" The UN claimed that the main factors that have contributed to these disparities were the deficient states of schools in developing areas, inadequate training for teachers and the lack of access to electricity and potable water in a majority of schools in the regions. To address these issues, in 2015, nations of the UN contributed $1 billion to the official development assistance (ODA) for scholarships.\n\nThe table below provides some historic and modern roles of NGOs in education:\n\nActors in education that operate outside and across national boundaries are categorized as members of a global community. Although this does not greatly distinguish between international/transnational actors, other scholars have developed the term \"global civil society\", including a network of NGOs that function in contrast to and sometimes competing with, the actions of nation-states. \"The emergence and growth of Civil Society over the past two decades has been one of the most significant trends in international development. The World Bank recognizes that civil society plays an especially critical role in helping to amplify the voices of the poorest people in the decisions that affect their lives, improve development effectiveness and sustainability, and hold governments and policymakers publicly accountable. The purpose of this web site is to provide Civil Service Organizations (CSOs) with information, links, and materials on the World Bank's evolving relationship with civil society in Washington and throughout the world\". Furthermore, the World Bank is the single largest external financeir of education in the world, with \"Education...at the heart of the World Bank's mission to reduce global poverty\" (according to their own reports). The World Bank also adopts educational standards established by other international intergovernmental organizations, such as their implementation of the Organization for Economic Cooperation and Development (OECD)'s \"Educational Sector Strategy\", which establishes specific international targets for \"primary education, adult literacy, and gender parity in basic education with the \"Education for All\" initiative\" as well as the OECD's \"Development Assistance Committee goals\".\"\n\nAlthough many scholars, activists, and NGOs agree with targeting specific populations, they contest the motives of the World Bank based on their measurements of success. These members of civil society see the actions of the World Bank as emphasizing economic growth based on the pursuit of profits as well as the increase of consumption in developing areas, as favoring representative governments, and as supporting the privatization of educational systems (including private schools, charging tuition for government schools, and privatizing textbook production). Critics, such as members of the antiglobalization movement, have argued that the economic and environmental growth policies of the World Bank actually hinder educational development in many countries, and are, in fact, undemocratic based on the perpetuation of inequality.\n\nThe \"U.S. Network for Global Economic Justice\", an antiglobalization platform, called for \"the immediate suspension of the policies and practices of the International Monetary Fund (IMF) and World Bank Group which have caused widespread poverty, inequality, and suffering among the world's peoples and damage to the world's environment...These institutions are anti-democratic... and their policies have benefitted international private sector financiers, transnational corporations, and corrupt officials and politicians...We demand that the World Bank Group immediately cease providing advice and resources to advance the goals associated with corporate globalization, such as privatization and liberalization\". These critics believe that the type of education that is promoted by international institutions and intergovernmental monetary institutions such as the IMF and World Bank represent the interest of the member countries, and do not benefit the human capital within the recipient countries. This is an example of policy implementation from above.\n\nOn the other hand, members of civil society and the antiglobalization movements alike view their own actions as policy implementation from below, emphasizing \"relations among people and between people and the environment\", as opposed to privatization of government services, and as focusing on the direct action and participation of citizens in decision-making. Because of this, education has shifted from nation-state oriented learning to an emphasis on world citizenship, civic education, service learning, human rights, and environmental education.\n\nGlobalization has had positive benefits on the concepts of global/world citizenship and civic society. The adoption of the concept of a global society, by educators, has led to a growth in the non-governmental (NGO) and non-profit sector. The idea of a global citizen arose with the following characteristics:\n\n\nWorld citizenship has exploded because of recent advances in new information technologies such as the internet and its social networking sites; these provide \"powerful, new opportunities to advocates to disseminate information and mobilize support\". The increase in population movements and a surge in travel to world conferences (i.e. 1992 Rio Earth Summit, 1995 Beijing UN Conference on Women) has helped the operationalist relations between NGOs, further solidifying transnational networks of communication and influence.\n\nEdward M. Kennedy Serve America Act of 2009\n\n\nNonprofit Capacity Building Program\n\n\nMany education nonprofits and programs in the United States seek to address the growing, academic disparities between racial and socioeconomic groups. In the U.S, exam scores of poor students, who are often Latino or black, are lower than those of white, middle-class students. Students who attend schools with 50% or more children in poverty greater concentration are more likely to fall below the U.S standards in mathematics and reading. Not only that, but as access to a higher education becomes increasingly crucial to earning a stable income, it has also become more costly in the nation. This issue has exacerbated the problem of equal access to college, as the typical black or Latino family do not possess a tenth of the wealth of the typical white family.\n\nThe academic disparities in the United States are also caused by other factors. One is that qualified, effective teachers are often concentrated in school districts that offer higher salaries Yet these districts are inhabited mainly by wealthier families and less by students of color. Another cause is that impoverished circumstances prevents children from learning in school, increasing the achievement gap. Studies have also revealed that the trend of school choice is a hindrance to social mobility, as more affluent families have access to schools with greater resources that are often private. This tendency has generated greater school segregation not only amongst students of various races but also of different socioeconomic classes.\n\nEducation nonprofit organizations generally aim to achieve the following goals that address the main concerns with the U.S education system:\n\n\nHuman rights NGOs, the largest group of NGOs in global civil society, shift focus away from an allegiance to the nation-state, as proposed by Rousseau, to an allegiance to humanity. The 1996 Report of the United Nations High Commissioner for Human Rights stressed the training, dissemination, and information efforts aimed at the building of a universal culture of human rights...through the imparting of knowledge and skills and the molding of attitudes and directed to: a)The strengthening of respect for human rights and fundamental freedoms; b)The full development of the human personality and the sense of its dignity; c) The promotion of understanding, tolerance, gender equality and friendship among all nations, indigenous peoples, and racial, national, ethnic religious and linguistic groups; d)The enabling of all persons to participate effectively in a free society; e)The furtherance of the activities of the United Nations for the maintenance of peace\". However, this report fails to address socio-economic inequalities, and is more ideological.\n\nTeaches about:\n\n\n\n"}
{"id": "300214", "url": "https://en.wikipedia.org/wiki?curid=300214", "title": "Eight Principles of Yong", "text": "Eight Principles of Yong\n\nThe Eight Principles of \"Yong\" (; , \"eiji happō\"; , \"Yeongjapalbeop\") explain how to write eight common strokes in regular script which are found all in the one character, (, \"forever\", \"permanence\"). It was traditionally believed that the frequent practice of these principles as a beginning calligrapher could ensure beauty in one's writing.\n\nThe Eight Principles are influenced by the earlier Seven Powers () by Lady Wei Shuo () of Eastern Jin. Publications on the Principles include:\n\nNote: － \"Xié\" 斜 is sometimes added to the 永's strokes. It is a concave \"Shù\" falling right, always ended by a \"Gōu\", visible on this image.\n\nIn addition to these eight common strokes in 永, there are at least two dozen strokes of combinations which enter in the composition of CJK strokes and by inclusion the CJK characters themselves.\n\n\n"}
{"id": "24748175", "url": "https://en.wikipedia.org/wiki?curid=24748175", "title": "Encyclopedia of Albanian Art", "text": "Encyclopedia of Albanian Art\n\nThe Encyclopedia of Albanian Art or Enciklopedia e Artit Shqiptar is an encyclopedia that specialises in coverage in Albania, Kosovo, Macedonia, Montenegro and in the Albanian diaspora. It combines knowledge on a wide range of Albanian art, from painting and sculpture to cinema and theatre.\n\nA number of authors have contributed to the encyclopedia including Josef Papagjoni, Zana Shuteriqi, Ferid Hudhri, Moikom Zeqo, Jakup Mato and Kristaq Dhamo in Albania. The encyclopedia is the first of its kind in Albania, an ambitious project which will eventually be published in six to eight volumes.\n\nThe publication is a product of the Center of Studies of Art, part of the Albanian Academy of Sciences. The encyclopedia has a board of 10 people, including some of the most prominent scholars in their fields. The board are also the most active contributors to the encyclopedia\n\nIn October 2007 about 1000 or so pages had been made ready for publication.\n\n"}
{"id": "11655", "url": "https://en.wikipedia.org/wiki?curid=11655", "title": "Factoid", "text": "Factoid\n\nA factoid is either a false statement presented as a fact or a true, but brief or trivial item of news or information.\n\nThe term was coined in 1973 by American writer Norman Mailer to mean a piece of information that becomes accepted as a fact even though it is not actually true, or an invented fact believed to be true because it appears in print. Since its creation in 1973, the term has evolved, now often being used to describe a brief or trivial item of news or information.\n\nThe \"Oxford English Dictionary\" defines a factoid as a brief or trivial item of news or information and as an item of unreliable information that is repeated so often that it becomes accepted as fact.\n\nThe term was coined by American writer Norman Mailer in his 1973 biography of Marilyn Monroe. Mailer described factoids as \"facts which have no existence before appearing in a magazine or newspaper\", and created the word by combining the word \"fact\" and the ending \"-oid\" to mean \"similar but not the same\". The \"Washington Times\" described Mailer's new word as referring to \"something that looks like a fact, could be a fact, but in fact is not a fact\".\n\nAccordingly, factoids may give rise to, or arise from, common misconceptions and urban legends. Several decades after the term was coined by Mailer, it grew to have several meanings, some of which are quite different from each other. In 1993, William Safire identified several contrasting senses of \"factoid\":\n\n\nThis new sense of a factoid as a trivial but interesting fact was popularized by the CNN Headline News TV channel, which, during the 1980s and 1990s, often included such a fact under the heading \"factoid\" during newscasts. BBC Radio 2 presenter Steve Wright uses factoids extensively on his show.\n\nHistorian Dion Smythe defines factoids to be assertions about the truth, as documented in primary sources of historical research. In this indirect meaning, the truthfulness of factoids comes from objectively observable existence of such assertions themselves, and not from the truthfulness of what they claim about the world.\n\nAs a result of confusion over the meaning of factoid, some English-language style and usage guides recommend against its use. William Safire in his \"On Language\" column advocated the use of the word \"factlet\" instead of \"factoid\" to express a brief interesting fact as well as a \"little bit of arcana\" but did not explain how adopting this new term would alleviate the ongoing confusion over the existing contradictory common use meanings of \"factoid\".\n\nSafire suggested that \"factlet\" be used to designate a small or trivial bit of information that is nonetheless true or accurate. A report in \"The Guardian\" identified Safire as the writer who coined the term \"factlet\", although Safire's 1993 column suggested \"factlet\" was already in use at that time. \"The Atlantic\" magazine agreed with Safire, and recommended \"factlet\" instead of \"factoid\", such that \"factlet\" would signify a \"small probably unimportant but interesting fact\", and that the term be used in place of \"factoid\", which they saw as often having negative connotations. The term \"factlet\" has been used in publications such as \"Mother Jones\", the \"San Jose Mercury News\", and in the \"Reno Gazette Journal\".\n\n"}
{"id": "29568406", "url": "https://en.wikipedia.org/wiki?curid=29568406", "title": "Flat Classroom Project", "text": "Flat Classroom Project\n\nFlat Classroom Project is an award winning global collaborative project for students in Grades 3 -12, inspired by Thomas Friedman's book, \"The World is Flat\", and leverages Web 2.0 tools to foster communication and interaction as well as collaboration and creation between students and teachers from classrooms around the world.\n\nCo-founded in 2006 by educators Vicki Davis (U.S.) and Julie Lindsay (Australia), the Flat Classroom® Project is a global collaborative project designed for students, typically in Grade 3 - 12, using Web 2.0 tools to support communication and collaboration between students and teachers from classrooms around the world. The project was inspired by topics included in Thomas Friedman's international bestselling book \"The World Is Flat: The World Is Flat: A Brief History of the Twenty-First Century\" and was featured in the 2007 update \"Release 3.0\" of that book. The original project served as a foundation which has been recreated and expanded. Currently the project runs three times a year.\n\nThe project has gained international recognition and awards from the International Society of Technology Educators(ISTE), Taking IT Global, World Innovation Summit for Education, and Edublog Awards and has fostered four similar projects based on the same holistic and constructivist educational approach, including the Digiteen, Eracism, NetGenEd, and A Week in the Life projects. \n\nThe first spin-off the NetGenEd Project, originally called the Horizon Project, examines the book \"Growing Up Digital\", by Don Tapscott, in conjunction with the New Media Consortium's annual \"Horizon Report\".\n\nAnother spin-off effort is the Flat Classroom™ Conference which brings together geographically distant students that may have participated in one of the affiliated projects for a face-to-face meeting. At the conference students share ideas and take part in theme-based workshops to take back to their home schools. Themes are designed to explore global social issues and inspire unity and action, while advancing continued student-to-student and educator-to-educator connections.\n\nThe goal of the Flat Classroom Project is to create global collaborative projects and maintain workspaces for students (at all school levels) and educators around the world. The aim is to provide bridging between students, educators, trainee teachers, and post-secondary education institutions.\n\nBy \"flattening\" the walls of the traditional classroom, participating classes essentially become one large virtual classroom, co-taught by participating teachers, via the Internet and a combination of synchronous and asynchronous communication tools. The project is designed to: \n\nThe core pedagogical approach includes the development of two primary products. The first product involves groups of students working collaboratively to compose a wiki web page, based on topic research, and using Wikipedia as a model. The second product involves individual students creating a multimedia artefact, again based on topic research, which should also include a portion requested from another student in a different part of the world. Student work is assessed with common criteria-based rubrics and reviewed by a panel of international judges. \n\nIt is designed as a multi-modal, inter-disciplinary learning experience that highlights digital citizenship. Additionally, participating students are encouraged to develop Personal Learning Environments and Networks, using a variety of Internet tools, while conducting their research.\n\nSince 2009, students and teachers from the following schools have participated in the Flat Classroom Project\nAcademy of Allied Health and Science, Neptune, New Jersey, USA\nAnglo-American School of Sofia, Sofia, Bulgaria\nAlta Vista School, Los Gatos, CA, USA\nBawlf School, Bawlf, Alberta, Canada\nBeijing BISS International School, Beijing, China\nThe Bullis School, Potomac, Maryland, USA\nCarrabassett Valley, Carrabassett, Maine, USA\nChoithram International World School, Indore, India\nCitrus High School, Inverness, Florida, USA\nCampo Verde High School, Gilbert, Arizona, USA\nDon Mills Collegiate Institute, Toronto, Ontario, Canada\nGilbert High School, Gilbert, Arizona, USA\nGoodland High School, Goodland, Kansas, USA\nGranville College, South West Sydney Institute of Technical and Further Education, Australia\nGrup Scolar Agricol Holboca, Sos. Iasi-Ungheni, Romania\nHawkesdale P12 College, Hawkesdale, Victoria, Australia\nHighland High School (Gilbert, Arizona), USA\nHopkinton High School, Hopkinton, Massachusetts, USA\nThe Illawarra Grammar School, Wollongong, New South Wales, Australia\nInternational School of the Americas, San Antonio, Texas, USA\nInternational School of Düsseldorf, Düsseldorf, Germany\nJordan Road School, Somers Point, New Jersey, USA\nKorea International School, South Korea\nManitou Springs High School, Manitou Springs, Colorado, USA\nMesquite High School (Gilbert, Arizona), USA\nMidpark High School, Middleburg Heights, Ohio, USA\nMount Carmel Area High School, Mount Carmel, Pennsylvania, USA\nNexus International School, Putrajaya, Malaysia\nNorth Rockland High School, Thiells, New York, USA\nOsama Bin Zaid School, Oman\nQatar Academy, Doha, State of Qatar\nRavenscroft School, Raleigh, North Carolina, USA\nRiver East Collegiate, Winnipeg, Manitoba, Canada\nRockford Lutheran School, Rockford, Illinois, USA\nRoyal Masonic School, Rickmansworth, Herts, England\nSanta Ana Valley High School, Santa Ana, California, USA\nSotogrande International School, Spain\nSpring Woods High School, Houston, Texas, USA\nSt. Alcuin Montessori School, Dallas, Texas, USA\nSt. Paul the Apostle, Los Angeles, California, USA\nTok School, Tok, Alaska, USA\nTrinity Lutheran School, Joppa, Maryland, USA\nUnion Intermediate High School, Broken Arrow, Oklahoma, USA\nWakefield School, The Plains, Virginia, USA\nWestdale Secondary School, Hamilton, Ontario, Canada\nWestside High School (Houston), Texas, USA\nWestwood Schools, Camilla, Georgia, USA\nWichern-Schule, Hamburg, Germany\nWindsor High School, Windsor, Vermont, USA\nWissahickon High School, Ambler, Pennsylvania, USA\n\nWith the success of the other projects it became apparent that many teachers of elementary students were wanting to also take part in the Flat Classroom experience. Beginning in October 2010 through to February 2011 eight international schools joined together to plan, work through and initiate the project.\n\nSchools Involved\n\n\nFive different categories were chosen for the students to investigate their lives and compare them to other students in the project. The categories chosen were Food and Celebrations, Clothing, Transportation, School Life and Housing. Within each of these categories teams were created with members from each school participating. A teacher mentor was in charge of each category to keep the teams focused. Synchronous and asynchornous methods were used to allow for authentic global collaboration among the students. At the end of the project each team combined their information together into one artifact representing their category sharing their lives from around the world. Teachers as well as students greatly benefited from the collaboration and helped to building relationships that are continuing on.\n\nThe pilot project was deemed a success and it has developed into a regular project under the Flat Classroom umbrella.\n\n"}
{"id": "41954154", "url": "https://en.wikipedia.org/wiki?curid=41954154", "title": "For-Profit Online University", "text": "For-Profit Online University\n\nFor-Profit Online University is a television special written and directed by Sam West for Adult Swim. The special is presented as an infomercial parodying for-profit universities. The special is created by Wild, Aggressive Dog, a conglomerate of former \"Onion\" writers, consisting of Geoff Haggerty, Dan E. Klein, Matthew Klinman, Michael Pielocik, Chris Sartinsky and Sam West.\n\nThe special aired on December 17, 2013 at 4 a.m. on Cartoon Network's late-night programming block, Adult Swim. The special was viewed by 871,000 viewers and received a 0.6 rating in the 18–49 demographic Nielsen household rating.\n\nThe infomercial consists of various students giving testimonies of their success, owing it to the school, literally named For-Profit Online University. Students pay for knowledge instead of attending traditional classes, and are offered jobs as 'Digital Gardeners,' solving CAPTCHA for digital currency. The later half of the special introduces the threat of an evil rogue spambot named Howard, who was bribed with a large sum of ThoughtCoins, a digital currency used through the school, in an attempt to placate him.\n\n\nThe special presents itself as a spoof of for-profit universities infomercials, a source of frequent criticism and congressional investigation in the United States. The special is written and directed by Sam West, and is co-written by the creative writing team Wild Aggressive Dog, a conglomerate of former \"Onion\" writers, consisting of Geoff Haggerty, Dan Klein, Matthew Klinman, Michael Pielocik, Chris Sartinsky and Sam West.\n\n\"For-Profit Online University\" aired on December 17, 2013, at 4 a.m. on Cartoon Network's late-night programming block, Adult Swim. The special was published onto Adult Swim's YouTube channel the same day. Initially made private on their channel, the special became viral online after being spotted by comedy blogs. The special was viewed by 871,000 viewers and received a 0.6 rating in the 18–49 demographic Nielsen household rating.\n\nDan Simmons of the \"Wisconsin State Journal\" praised the special as \"hilariously\" parodying \"what some in academia fear is a growing movement toward commodifying college.\" Andy Thomason of \"The Chronicle of Higher Education\" called the presentation of the special \"brutal\" in its criticism of both for-profit and traditional universities. Gaby Dunn of \"The Daily Dot\" compared the fictional college to the University of Phoenix in her review of the special. Laughing Squid's EDW Lynch called the infomercial \"hilarous\", while Splitsider stated of all of Adult Swim's faux-infomericals, the special was \"the best one yet\".\n\n\n"}
{"id": "25174711", "url": "https://en.wikipedia.org/wiki?curid=25174711", "title": "Genetically modified insect", "text": "Genetically modified insect\n\nA genetically modified (GM) insect is an insect that has been genetically modified, either through mutagenesis, or more precise processes of transgenesis, or cisgenesis. Motivations for using GM insects include biological research purposes and genetic pest management. Genetic pest management capitalizes on recent advances in biotechnology and the growing repertoire of sequenced genomes in order to control pest populations, including insects. Insect genomes can be found in genetic databases such as NCBI, and databases more specific to insects such as FlyBase, VectorBase, and BeetleBase. There is an ongoing initiative started in 2011 to sequence the genomes of 5,000 insects and other arthropods called the i5k. Some Lepidoptera (e.g. monarch butterflies and silkworms) have been genetically modified in nature by the wasp bracovirus.\n\nThe sterile insect technique (SIT) was developed conceptually in the 1930s and 1940s and first used in the environment in the 1950s. SIT is a control strategy where male insects are sterilized, usually by irradiation, then released to mate with wild females. If enough males are released, the females will mate with mostly sterile males and lay non-viable eggs. This causes the population of insects to crash (the abundance of insects is extremely diminished), and in some cases can lead to local eradication. Irradiation is a form of mutagenesis which causes random mutations in DNA.\n\nRelease of Insects carrying Dominant Lethals (RIDL) is a control strategy using genetically engineered insects that have (carry) a lethal gene in their genome (an organism's DNA). Lethal genes cause death in an organism, and RIDl genes only kill young insects, usually larvae or pupae. Similar to how inheritance of brown eyes is dominant to blue eyes, this lethal gene is dominant so that all offspring of the RIDL insect will also inherit the lethal gene. This lethal gene has a molecular on and off switch, allowing these RIDL insects to be reared. The lethal gene is turned off when the RIDL insects are mass reared in an insectory, and turned on when they are released into the environment. RIDL males and females are released to mate with wild males and their offspring die when they reach the larval or pupal stage because of the lethal gene. This causes the population of insects to crash. This technique is being developed for some insects and for other insects has been tested in the field. It has been used in the Grand Cayman Islands, Panama, and Brazil to control the mosquito vector of dengue, \"Ae. aegypti.\" It is being developed for use in diamondback moth (\"Plutella xylostella\"), medfly and olive fly.\n\nIncompatible Insect Technique (IIT) – \"Wolbachia\"\n\nMaternal Effect Dominant Embryonic Arrest (Medea)\n\nX-Shredder\n\nThere are concerns about using tetracycline on a routine basis for controlling the expression of lethal genes. There are plausible routes for resistance genes to develop in the bacteria within the guts of GM-insects fed on tetracycline and from there, to circulate widely in the environment. For example, antibiotic resistant genes could be spread to E.coli bacteria and into fruit by GM-Mediterranean fruit flies (\"Ceratitis capitata\").\n\nIn January 2016 it was announced that in response to the Zika virus outbreak, Brazil's National Biosafety Committee approved the releases of more genetically modified Aedes aegypti mosquitos throughout their country. Previously in July 2015, Oxitec released results of a test in the Juazeiro region of Brazil, of so-called \"self-limiting\" mosquitoes, to fight dengue, Chikungunya and Zika viruses. They concluded that mosquito populations were reduced by about 95%.\n\n\n\nThe diamondback moth's caterpillars gorge on cruciferous vegetables such as cabbage, broccoli, cauliflower and kale, globally costing farmers an estimated $5 billion (£3.2 billion) a year worldwide. In 2015, Oxitec developed GM-diamondback moths which produce non-viable female larvae to control populations able to develop resistance to insecticides. The GM-insects were initially placed in cages for field trials. Earlier, the moth was the first crop pest to evolve resistance to DDT and eventually became resistant to 45 other insecticides. In Malaysia, the moth has become immune to all synthetic sprays. The gene is a combination of DNA from a virus and a bacterium. In an earlier study, captive males carrying the gene eradicated communities of non-GM moths. Brood sizes were similar, but female offspring died before reproducing. The gene itself disappears after a few generations, requiring ongoing introductions of GM cultivated males. Modified moths can be identified by their red glow under ultraviolet light, caused by a coral transgene.\n\nOpponents claim that the protein made by the synthetic gene could harm non-target organisms that eat the moths. The creators claim to have tested the gene's protein on mosquitoes, fish, beetles, spiders and parasitoids without observing problems. Farmers near the test site claim that moths could endanger nearby farms' organic certification. Legal experts say that national organic standards penalize only deliberate GMO use. The creators claim that the moth does not migrate if sufficient food is available, nor can it survive winter weather.\n\nThe Mediterranean fruit fly is a global agricultural pest. They infest a wide range of crops (over 300) including wild fruit, vegetables and nuts, and in the process, cause substantial damage. The company Oxitec has developed GM-males which have a lethal gene that interrupts female development and kills them in a process called \"pre-pupal female lethality\". After several generations, the fly population diminishes as the males can no longer find mates. To breed the flies in the laboratory, the lethal gene can be \"silenced\" using the antibiotic tetracycline.\n\nOpponents argue that the long-term effects of releasing millions of GM-flies are impossible to predict. Dead fly larvae could be left inside crops. Helen Wallace from Genewatch, an organisation that monitors the use of genetic technology, stated \"Fruit grown using Oxitec's GM flies will be contaminated with GM maggots which are genetically programmed to die inside the fruit they are supposed to be protecting\". She added that the mechanism of lethality was likely to fail in the longer term as the GM flies evolve resistance or breed in sites contaminated with tetracycline which is widely used in agriculture.\n\nIn July 2015, the House of Lords (U.K.) Science and Technology Committee launched an inquiry into the possible uses of GM-insects and their associated technologies. The scope of the inquiry is to include questions such as \"Would farmers benefit if insects were modified in order to reduce crop pests? What are the safety and ethical concerns over the release of genetically modified insects? How should this emerging technology be regulated?\"\n\n\n"}
{"id": "23436893", "url": "https://en.wikipedia.org/wiki?curid=23436893", "title": "Gulfscapes Magazine", "text": "Gulfscapes Magazine\n\nGulfscapes Magazine was a lifestyle magazine for those who live or vacation along the Gulf coast. The magazine emphasized home design and travel. Articles offered information on home interiors; coastal recreation; food; travel destinations; and style. The magazine was created in 2001 in Port Aransas, TX by Victoria Munt Rogers.\n\nPublication editorial and coverage included the five states that border the Gulf of Mexico: Alabama, Florida, Louisiana, Mississippi and Texas.\n\nStandard sized, 8.375 x 10.875, the magazine was printed using a web offset process in full colour, and used 'perfect-binding'.\n\nDistribution included a pre-paid subscription base, targeted mail-outs and retail sales in more than 600 locations within HEB, Super Wal-Marts, Rouses, Publix, Barnes&Noble, Hastings and IGA. The magazine was also sold on Amazon.com. \n\nMagazines were sold in 11 states including: Texas, Alabama, Oklahoma, Louisiana, Arkansas, Florida, Mississippi, Georgia, New Mexico, Illinois and Tennessee.\n\n, the magazine was no longer published.\n\n\n"}
{"id": "699597", "url": "https://en.wikipedia.org/wiki?curid=699597", "title": "Holy Piby", "text": "Holy Piby\n\nThe Holy Piby is a proto-Rastafari text written by an Anguillan, Robert Athlyi Rogers (d. 1931), for the use of an Afrocentric religion in the West Indies founded by Rogers in the 1920s, known as the Afro-Athlican Constructive Gaathly. The theology outlined in this work saw Ethiopians (in the classical sense of all Africans) as the chosen people of God. The church preached self-reliance and self-determination for Africans, using the Piby as its guiding document.\n\nThe \"Holy Piby\" is made up of four books. The first, entitled \"The First Book of Athlyi Called Athlyi\", has only two chapters. The next, \"The Second Book of Athlyi Called Aggregation\", is the largest, with fifteen chapters, the seventh of which identifies Marcus Garvey as one of three apostles of God. The \"Third Book of Athlyi Named The Facts of the Apostles\" presents two prominent members of the UNIA-ACL, Robert Lincoln Poston and Henrietta Vinton Davis, as the other apostles in the Holy Trinity. The title of the last book is \"The Fourth Book of Athlyi Called Precaution\". That book is followed by a series of catechism-style questions and answers wherein Garvey, Davis and Poston are proclaimed to be the saviors of the \"down trodden children of Ethiopia\".\n\nTogether with the \"Royal Parchment Scroll of Black Supremacy\" by Fitz Balintine Pettersburg and Leonard P. Howell's \"The Promise Key\", the \"Holy Piby\" is today recognized as a root document of Rastafari thought. While not strictly speaking a \"Rastafari text\", it was certainly a primary source of influence to many in the Rastafari movement, who see His Imperial Majesty, Emperor Haile Selassie I of Ethiopia, as Christ (or rather, his second coming). Some Rastafari see Emperor Haile Selassie I as Christ in His Kingly Character as written in the Book of Revelation, Chapter 5 and Marcus Mosia Garvey, as a prophet.\n\nThe original edition of the \"Holy Piby\" was published in the United States, at Newark, New Jersey, in 1924. It is very rare. There are no copies listed in either the Library of Congress nor the University of California catalogs. \n\nThe \"Holy Piby\" was banned in Jamaica and other Caribbean Islands in the middle and late 1920s.\n\n"}
{"id": "3268213", "url": "https://en.wikipedia.org/wiki?curid=3268213", "title": "Hwangap", "text": "Hwangap\n\nHwangap () in Korean, in Japanese or Jiazi () in Chinese, is a traditional way of celebrating one's 60th birthday in East Asia. The number 60 means accomplishing one big 60-year cycle and starting another one in one's life following the traditional 60-year calendar cycle of the lunar calendar. In the traditional way of counting ages, one began a new 60-year cycle on New Year's Day, when everyone became a year older. Thus people who were 60 and had completed their first 60-year cycle entered their second cycle on the New Year's Day when they turned 61 and returned to the same combination of zodiacal symbols that governed the year of their birth. Under the currently popular western method of counting ages, however, one enters one's second cycle on one's 60th birthday. The traditional cycles still remain, but the way of counting ages has changed by one year. \n\nIn the past, a person's average life expectancy was much lower than 60, so Jiazi or Hwangap or Kanreki also meant a celebration of longevity. The celebration party is also a wish for an even longer and more prosperous life. This party is customarily thrown by the children of the person who is turning 60, unless that person does not have any children, in which case there's no party at all. On one's Hwangap family and relatives prepare a big birthday celebration with lots of food.\n\nWith the increasingly longer life spans of people these days, the Hwangap celebration has been given a lesser significance than before; typically, only close family members get together to have a big meal. Many Koreans now take trips with their families instead of having a big party to celebrate their 60th birthdays. Parties are also thrown when a person reaches 70 (called Gohi or Chilsun) or 80 (Palsun) years of age.\n\n"}
{"id": "1132716", "url": "https://en.wikipedia.org/wiki?curid=1132716", "title": "James Maybrick", "text": "James Maybrick\n\nJames Maybrick (24 October 1838 – 11 May 1889) was a Liverpool cotton merchant. After his death, his wife, Florence Maybrick, was convicted of murdering him by poisoning in a sensational trial. The \"Aigburth Poisoning\" case was widely reported in the press on both sides of the Atlantic. \n\nMore than a century after his death, Maybrick was accused of being the notorious serial killer Jack the Ripper, through his own words in a diary, but critics countered that the diary and confession are a hoax. Forensic tests were inconclusive. In 2017, Bruce Robinson and a team of researchers, who believe the diary to be genuine, found new evidence to support their claim, but it is still (currently) inconclusive. \nA serial killer, who became known as the Servant Girl Annihilator, preyed upon the city of Austin, Texas, during 1884 and 1885, and there have also been attempts to link Maybrick to those murders.\n\nMaybrick was born in Liverpool, Merseyside, the son of William Maybrick, an engraver, and his wife, Susanna. He was christened on 12 November 1838, at St Peter's Church in the city. He was named after a brother who had died the year before and was the Maybricks' third of seven sons.\n\nMaybrick's cotton trading business required him to travel regularly to the United States and in 1871 he settled in Norfolk, Virginia, to establish a branch office of his company. While there in 1874 he contracted malaria, which was then treated with a medication containing arsenic; he became addicted to the drug for the rest of his life.\nIn 1880, Maybrick returned to the company's offices in Britain. Sailing from New York on 12 March 1880, he arrived in Liverpool six days later. During the journey he was introduced to Florence (Florie) Elizabeth Chandler, the daughter of a banker from Mobile, Alabama, and their relationship quickly blossomed. Despite the difference in their ageshe was 42 to her 18they began to plan their wedding immediately.\n\nThe wedding was delayed until 27 July 1881, when it took place at St James' Church, Piccadilly, London. The couple returned to Liverpool to live at Maybrick's home, \"Battlecrease House\" in Aigburth, a suburb in the south of the city.\n\nThey had two children: a son, James Chandler (\"Bobo\") born in 1882 and a daughter, Gladys Evelyn, born in 1886.\n\nMaybrick continued to divide his time between the American and the British offices of his company and this may have caused difficulties within his marriage. He also resumed his relationships with his many mistresses, while his wife conducted an affair with an Alfred Brierley, a cotton broker. It is possible Florence embarked upon this on learning of her husband's infidelity.\n\nIn Maybrick's case a common-law wife, Sarah Ann Robertson, was identified. Sarah Ann is mentioned in her stepfather's will as \"Sarah Ann Maybrick, wife of James\".\n\nMaybrick's health deteriorated suddenly on 27 April 1889, and he died fifteen days later at his home in Aigburth. The circumstances of his death were deemed suspicious by his brothers and an inquest, held in a local hotel, came to the verdict that arsenic poisoning was the most likely cause, administered by persons unknown.\n\nSuspicion immediately fell on Florence and she was arrested some days later. She stood trial at Liverpool Crown Court and, after lengthy proceedings, the fairness of which was the subject of some debate in later years, she was convicted of murder and sentenced to death. The way in which the judge conducted her trial was questioned and this was probably the reason her sentence was commuted to life imprisonment, some of which she served in a prison in Woking, Surrey, and then at the \"House of Detention\" at Aylesbury, Buckinghamshire.\n\nA re-examination of her case resulted in her release in 1904. She supported herself through various occupations until her death on 23 October 1941. From her initial incarceration until her death, she never saw her children again.\n\nAfter their mother's conviction and imprisonment, Maybrick's children James and Gladys Evelyn were taken in by a Dr. Charles Chinner Fuller and his wife Gertrude, and the younger James changed his name to Fuller.\n\nJames Fuller became a mining engineer in British Columbia. In 1911, at the age of 29, while working at the Le Roi Gold Mine in Canada, he died after drinking cyanide, apparently thinking it was just a glass of water. His sister Gladys went to live in Ryde, Isle of Wight, with her uncle and aunt Michael and Laura (née Withers) Maybrick before marrying Frederick James Corbyn in Hampstead, London, in 1912. She died in South Wales in 1971, where the couple lived in their later years.\n\nJames Maybrick’s brother, Michael Maybrick, was a composer who published many pieces and songs under the name Stephen Adams. \"Good Company\" is one such example, but by far his best known work was the hymn “The Holy City”.\n\nIn 1992, a document presented as James Maybrick's diary surfaced, which claimed that he was Jack the Ripper. The diary's author does not mention his own name, but offers enough hints and references consistent with Maybrick's established life and habits that it is obvious readers are expected to believe it is him. The author of the document details alleged actions and crimes over a period of several months, taking credit for slaying the five victims most commonly credited to Jack the Ripper as well as for two other murders which have to date not been historically identified.\n\nThe diary was first introduced to the world by Michael Barrett, an unemployed former Liverpool scrap metal dealer, who claimed at the time that it had been given to him by a friend, Tony Devereux, in a pub. When this was queried, the story changed. Barrett's wife Ann, formerly Graham, said that the diary had been in her family for as long as she could remember. She had asked Devereux to give it to her husband because he had literary aspirations and she thought he might write a book about it. She had not wanted to tell him her family owned it because she thought he would ask her father about it and relations between the two men were strained. It was published as \"The Diary of Jack the Ripper\" in 1993 to great controversy. Few experts gave it any credence from the outset, and most immediately dismissed it as a hoax, though some were open to the possibility it might be genuine. Debate was often heated, and one writer notes that the \"saga of the Maybrick diary is confusing, complicated and inescapably tortuous.\"\n\nTests carried out on the ink used in the diary produced contradictory findings. The first test, using thin layer chromatography (TLC) revealed the ink contained no iron, and was based on a synthetic dye called nigrosine, patented and commercially available in 1867, and in general use in writing inks by the 1870s. The second TLC test found nothing in the ink inconsistent with the date of 1888, and that the ink contained iron and sodium, but no nigrosine. The third TLC test found nothing inconsistent with the Victorian period. A fourth TLC test was attempted, but could not be carried out.\n\nSeveral tests were carried out to find out whether the ink contained chloroacetamide, a preservative, in an effort to definitively date the ink. According to one source, chloroacetamide was introduced into the Merck Index in 1857, but not used commercially in ink until 1972. In 1995, Dr Earl Morris of the Dow Chemical Company stated that chloroacetamide has been found in preparations as early as 1857. A fourth test, this time using gas chromatography, found chloroacetamide present, at 6.5 parts per million. A fifth TLC test found traces of chloroacetamide, but this was attributed to contamination from the control. The test was carried out again, and no chloroacetamide was found.\n\nAmong the investigators were sceptic Joe Nickell and document expert Kenneth W. Rendell. In Rendell's analysis, he was struck that the handwriting style seemed more 20th century than Victorian. He also noted factual contradictions and handwriting inconsistencies. Written in a genuine Victorian scrapbook, but with 20 pages at the front end torn out, he also found this suspect as there was no logical explanation for the purported author to use such a book.\n\nIn January 1995, Michael Barrett swore in two separate affidavits that he was \"the author of the Manuscript written by my wife Anne Barrett at my dictation which is known as The Jack the Ripper Diary.\" Adding to the confusion, however, was Barrett's solicitor's subsequent repudiation of his affidavit, then Barrett's withdrawal of the repudiation.\n\nSome people, including Robert Smith, the present owner of the diary and original publisher of the associated book by Shirley Harrison, insist it may be genuine. They argue that scientific dating methods have established that the book and ink used to write in it are from the 19th century; that the symptoms of arsenic addiction, claimed to be described accurately in the book, are known to very few persons; that some details of the murders provided in it were known only to police and the Ripper himself before the book's publication; and that one of the original crime scene photographs shows the initials \"F. M.\" written on a wall behind the victim's body in what appears to be blood. These, they claim, refer to Florence Maybrick, James's wife, whose possible infidelities were the purported motivation for the murders. These claims are dismissed by the majority of experts.\n\nIn June 1993, a gentleman's pocket watch, made by William Verity of Rothwell (near Leeds) \nin 1847 or '48, was presented by Albert Johnson of Wallasey. The watch has \"J. Maybrick\" scratched on the inside cover, along with the words \"I am Jack\", as well as the initials of the five canonical Ripper victims. The watch was examined in 1993 by Dr Stephen Turgoose of the Corrosion and Protection Centre at the University of Manchester Institute of Science and Technology, using an electron microscope. He stated:\n\"On the basis of the evidence...especially the order in which the markings were made, it is clear that the engravings pre-date the vast majority of superficial surface scratch marks...the wear apparent on the engravings, evidenced by the rounded edges of the markings and 'polishing out' in places, would indicate a substantial age...whilst there is no evidence which would indicate a recent (last few years) origin...it must be emphasised that there are no features observed which conclusively prove the age of the engravings. They could have been produced recently, and deliberately artificially aged by polishing, but this would have been a complex multi-stage process...many of the features are only resolved by the scanning electron microscope, not being readily apparent in optical microscopy, and so, if they were of recent origin, the engraver would have to be aware of the potential evidence available from this technique, indicating a considerable skill and scientific awareness\".\n\nIn 1994, the watch was taken to the Interface Analysis Centre at Bristol University and studied by Dr Robert Wild using an electron microscope and Auger electron spectroscopy. Dr Wild found that:\n\"Provided the watch has remained in a normal environment, it would seem likely that the engravings were at least several tens of years age...in my opinion it is unlikely that anyone would have sufficient expertise to implant aged, brass particles into the base of the engravings\".\n\n"}
{"id": "40456042", "url": "https://en.wikipedia.org/wiki?curid=40456042", "title": "James McLachlan (scholar)", "text": "James McLachlan (scholar)\n\nJames McLachlan is a Mormon studies scholar and theologian. In 2005 he became the inaugural co-editor, along with Carrie McLachlan, of \"Element: a Journal of Mormon Philosophy and Theology,\" the flagship peer-reviewed journal of Mormon theology. McLachlan is also Professor of Philosophy and Religion at Western Carolina University. , McLachlan co-chaired the American Academy of Religion’s Mormon Studies Group and was a board member of the Society for Mormon Philosophy and Theology.\n\n"}
{"id": "58260806", "url": "https://en.wikipedia.org/wiki?curid=58260806", "title": "John Paul's Rock", "text": "John Paul's Rock\n\nJohn Paul's Rock is a novel published in 1932 by Canadian writer Frank Parker Day, about a Mi'kmaq guide who fled into Nova Scotia to escape white man's law.\n\nMinton, Balch & Company, New York, 1932. Hardcover.\n\nJim Charles, who was the inspiration for the novel, was perhaps the most noted Indian guide in both fact and fiction. His notoriety comes not from his guiding expertise but from his discovery of gold and a subsequent brush with the law which led to his fleeing as a fugitive into the wilds of the interior. In the 1860s he was living in a cabin and tending a few heads of cattle and a horse on the point which today beats his name within the boundaries of Kejimkujik National Park.\n"}
{"id": "34585251", "url": "https://en.wikipedia.org/wiki?curid=34585251", "title": "Juggling in ancient China", "text": "Juggling in ancient China\n\nAlthough juggling in its western form involving props such as balls, rings, and clubs is rarely performed in modern China, at certain periods in Chinese history it was much more popular. In fact, some of the world's earliest known jugglers were Chinese warriors and entertainers who lived during the time of the Spring and Autumn period of Chinese history. References to these artists in ancient Chinese literature have preserved records of their incredible achievements. From such references, it appears that juggling was a well-regarded and highly developed form of ancient Chinese art.\n\nXiong Yiliao (), was a famous Chu warrior who fought under King Zhuang of Chu (ruled 613-591 BC) during the Spring and Autumn period of Chinese history. Ancient Chinese annals state that he practiced \"nongwan\" (, \"throwing multiple objects up and down without dropping\"), and he is often cited as one of the world’s earliest known jugglers. During a battle in about 603 BC between the states of Chu () and Song (), Xiong Yiliao stepped out between the armies and juggled nine balls, which so amazed the Song troops that all five hundred of them turned and fled, allowing the Chu army to win a complete victory. As Xu Wugui () recounts in Chapter 24 of the Zhuangzi (), “Yiliao of Shinan juggled balls, and the conflict between the two states was ended.”\n\nLanzi (), another juggler from the Spring and Autumn period who is mentioned in the Chinese annals, lived during the reign of Duke Yuan of Song (, 531-517 BC). Roughly translated, Chapter 8 of the Liezi (), an ancient collection of Daoist sayings, reads as follows:\n\nThe passage states that Lanzi juggled the jian (), a straight, double-edged sword which was used during the Spring and Autumn period. According to Jian Zhao in \"The Early Warrior and the Birth of the Xia\", Lanzi was a general term for itinerant entertainers in pre-Qin and Han times.\n\nDiabolos evolved from the ancient Chinese yo-yo, which was originally standardized in the 12th century.\n\n"}
{"id": "2431422", "url": "https://en.wikipedia.org/wiki?curid=2431422", "title": "Karzer", "text": "Karzer\n\nA Karzer was a designated lock-up or detention room to incarcerate students as a punishment, within the jurisdiction of some institutions of learning in Germany and German-language universities abroad. Karzers existed both at universities and at gymnasiums (similar to a grammar school) in Germany until the beginning of the 20th century. Marburg's last Karzer inmate, for example, was registered as late as 1931. \nResponsible for the administration of the \"Karzer\" was the so-called \"Pedell\" (English: bedel), or during later times \"Karzerwärter\" (a warden). While Karzer arrest was originally a severe punishment, the respect for this punishment diminished with time, particularly in the 19th century, as it became a matter of honour to have been incarcerated at least once during one's time at university. At the end of the 19th century, as the students in the cell became responsible for their own food and drink and the receiving of visitors became permitted, the \"punishment\" would often turn into a social occasion with excessive consumption of alcohol.\n\nKarzers have been preserved at the universities of Heidelberg, Jena, Marburg, Freiburg, Tübingen, Freiberg (School of Mines), Greifswald, Göttingen, Friedrich-Alexander-Universität Erlangen-Nürnberg in Erlangen, and at Tartu, Estonia. The Karzer in Göttingen was known, after the Pedell Brühbach, as \"Hotel de Brühbach\"; it was moved in the 19th century, because of the extension of the university library, to the \"Aula\" building; a cell door, preserved from the old Karzer, shows graffiti by Otto von Bismarck. Bearing witness to how the students spent the time in the cell are the many memorable wall, table and door graffiti left by students in the cells and today shown as tourist attractions in the older German universities.\n\n\nThe final two lock-ups had been established in the manner and tradition of German campus prisons.\n"}
{"id": "18469774", "url": "https://en.wikipedia.org/wiki?curid=18469774", "title": "La última rumba de Papá Montero", "text": "La última rumba de Papá Montero\n\nLa última rumba de Papá Montero is a 1992 Cuban film directed by Octavio Cortázar. The film focuses on a contemporary documentary production company that is attempting to create a non-fiction feature on Papá Montero, a 1930s \"rumbero\" who was murdered during Havana’s carnival celebrations. The film uses flashback sequences to recreate Papá Montero's celebrated dancing and his disastrous involvement in a fatal love triangle.\n\nThe film features rumba-inspired dance sequences performed on the streets of Havana by El Conjunto Folklorico Nacional de Cuba.\n\n\"La última rumba de Papá Montero\" had a U.S. theatrical and home video release in 2001.\n\n"}
{"id": "52311270", "url": "https://en.wikipedia.org/wiki?curid=52311270", "title": "Learning to Labour", "text": "Learning to Labour\n\nLearning to Labour: How Working Class Kids Get Working Class Jobs is a 1977 book on education, written by British social scientist and cultural theorist Paul Willis. A Columbia University Press edition, titled the \"Morningside Edition,\" was published in the United States shortly after its reception.\n\nWillis's first major book, \"Learning to Labour\" relates the findings of his ethnographic study of working-class boys at a secondary school in England. In it, Willis attempts to explain the role of youths' culture and socialization as mediums by which schools route working-class students into working-class jobs. Stanley Aronowitz, in the preface to the Morningside edition, hails the book as a key text in Marxist social reproduction theory about education, advancing previous work in education studies by Samuel Bowles and Herbert Gintis's , as well as work by Michael Apple and John Dewey.\n\n\"Learning to Labour\" has been recognized by sociologists, critical pedagogues, and researchers in education studies as a landmark study of schooling and culture, and is one of the most cited sociological texts in education studies.\n\n\"Learning to Labour\" represents Paul Willis's ethnographic fieldwork with twelve working-class British male students, attending their second-to-last year of schooling at \"Hammertown Boys,\" a modern, boys-only school in a town in the British Midlands. Beginning in 1972, Willis followed the boys for about six months, observing their social behavior with each other and their school and interviewing them periodically. He also studied them at later points up until 1976. The makeup of Hammertown Boys, and Hammertown, is largely working-class, with some immigrants from South Asia and the West Indies. At the time of the study, the local school system was expanding its infrastructure and exploring new pedagogical methods, thanks to the implementation of the Raising of School Leaving Age policies in September 1972 that were in line with education reforms that sought to keep youth in schools for a greater span of time, as well as offer them opportunities for gainful employment and socioeconomic mobility.\n\nWillis's research was made possible by funding through the Social Science Research Council. Willis acknowledged the advice and support of members of the Centre for Contemporary Cultural Studies at the University of Birmingham, including cultural theorist Stuart Hall, in writing in the book.\n\n\"Learning to Labour\" is organized into two sections: ethnography and analysis. In Part One, Willis describes and analyzes the nonconformist counter-school culture produced by Hammertown Boys' White, working-class boys (called \"lads\"). In this section, he applies thick description and ethnographic analysis to the counter-school culture of the lads, recognizing the legitimacy and reality of the students' own interpretive accounts of schooling. In Part Two, Willis analyzes his own ethnography to produce a theoretical account of how counter-school culture plays a vital role in leading working-class students into subordinate, low-wage labor positions in adult life, fulfilling what he calls their \"self-damnation.\" Working-class youths' recognition of, and reaction against, the dominating, disciplinary mechanisms of school help seal their future outcomes as workers, in turn enabling the social reproduction of class positions.\n\nWillis uses the qualitative research methods of participant observation and group interviews to study an informal (but socially cohesive) group of twelve lads at Hammertown Boys. He distinguishes between two distinct, informal groups of the working-class students at Hammertown Boys: lads and 'ear'oles. Whereas the lads are those nonconformist students who defy authority, 'ear'oles are those conformist students who invest themselves in formal learning and academic achievement, and therefore in the authority of teachers and the school system. The term \"'ear'ole\" recalls the earhole or earlobe, signifying conformist students' passive reception of learning. This contrasts with lads' rowdy, active behaviour in school. The lads perceive themselves as more manly, more authentic, and more independent than the 'ear'oles.\n\nThe lads informally socialise and organise themselves against the 'ear'oles and the school as an institution, producing a culture of nonconformity, rebellion, and opposition to their school's authority figures and strictures. It is not only important that the lads smoke and have sex with girls, but are seen to smoke and recognized to have had sexual liaisons. Behaviours that define the forms of this culture, such as playing pranks on teachers, harassing conformist students, and refusing to inform teachers of each other's behaviour, also build a sense of solidarity and identity among their group. The lads' culture is also patriarchal and racist, as girls and non-Whites are excluded from their informal group. It also strongly identifies with the actual, working-class environment from which it originates. In terms of working-class identity, their culture has much in common with the culture of working-class shop floors. This includes the active search for producing moments of excitement, disorder, and enjoyment in what is an otherwise boring, routine, and meaningless span of time of work for adult workers, and school discipline for students.\n\nOver the course of their time as Hammertown Boys, the lads were recognised by school authorities as a distinct \"anti-school group.\" However, when they were of age to legally leave school, in their fifth year in secondary school, few of them did. By this point, as the lads took career-preparation lessons in school, they rejected the legitimacy of formal credentials and qualifications, prizing instead manual labour as superior to and more authentic than mental labour. This inverted the lessons' insinuation of mental labour’s being more desirable than manual labour by dint of its higher socioeconomic status. By the end of the ethnography, the lads were easily able to enter working-class jobs, including plumbing, bricklaying, and trainee machine work. However, half of them left their job for another after one year of work, and one was unable to find work at all. Willis ended the ethnographic study in autumn 1976, with the lads routed into working-class labour with little hope of rising into the middle class, even as they subjectively experienced manual labour and income as empowering. Willis writes:There is also a sense in which, despite the ravages -- fairly well contained at this point anyway -- manual work stands for something and is a way of contributing to and substantiating a certain view of life which criticises, scorns and devalues others as well as putting the self, as they feel it, in some elusive way ahead of the game. These feelings arise precisely from a sense of their own labour power which has been learnt and truly appropriated as insight and self-advance within the depths of the counter-school culture as it develops specific class forms in the institutional context. It is difficult to think how attitudes of such strength and informal and personal validity could have been formed in any other way. It is they, not formal schooling, which carry 'the lads' over into a certain application to the productive process. In a sense, therefore, there is an element of self-domination in the acceptance of subordinate roles in western capitalism. However, this damnation is experienced, paradoxically, as a form of true learning, appropriation and as a kind of resistance.\n\nIn the second half of \"Learning to Labour\", Willis synthesises his observations of the lads' counter-school culture at Hammertown Boys in order to produce a theory of social reproduction that integrates culture as a key element alongside education. He proposes that the working-class lads enter working-class jobs of their own apparent volition, but this is not to be understood as merely a psychological inclination toward these jobs, nor as merely the deterministic effect of capitalist ideology persuading them to select them. Rather, it is in school that the lads acquire a distorted class consciousness through their counter-school culture, in which they end up embracing working-class, manual labor as more affirming and authentic. A rebellious culture can successfully oppose the norms of capitalism transmitted in school, but the success is, in Willis's terms, a \"pyrrhic victory,\" for they end up taking on working-class jobs as adults.\n\nIn his analysis, Willis defines and uses the following concepts: \nIn \"Learning to Labour\", the informal, creative, counter-school culture of the lads is vital to understanding the reproduction of class structure. Willis notes that working-class cultures are distinct in that they have no stake in subscribing to the dominant capitalist ideology, and therefore have the potential to subvert it. Yet it is this subversion that routes the lads into working-class labour, seemingly of their own volition. They use culture to explain and interpret the structures of schooling and work that enfold them, but doing so also steers them into social reproduction.\n\nWillis warns against an overtly deterministic mode of social reproduction, encouraging the consideration of culture in a mediating role. He also warns against policy that would focus strictly on changing culture as a means to change material outcomes in education and labour. \"Learning to Labour\" ends with several practical suggestions for changing education accordingly, including:\nIn his afterword to the Morningside Edition, Willis reflected that \"Learning to Labour\" contributed to the academic literature of education by advancing social reproduction theory and by asserting the complicity of both liberal educational policies and student in causing educational and socioeconomic inequality. While researchers must be skeptical of schools' purported role in improving social mobility, schools are not all-powerful in reproducing class:There may be a justified skepticism about liberal claims in education, but the \"Reproduction\" perspective moves too quickly to a simple version of their opposite. Apparently, education unproblematically does the bidding of the capitalist economy by inserting working class agents into unequal futures ... The actually varied, complex, and creative field of human consciousness, culture, and capacity is reduced to the dry abstraction of structural determination. Capital requires it, therefore schools do it! Humans become dummies, dupes, or zombies. Their innermost sensibilities are freely drawn on. The school is even the main site for this cosmic drawing; for all we are told of how this actually happens, schools may as well be \"black boxes.\" This will not do theoretically. It certainly will not do politically. Pessimism reigns supreme in this, the most spectacular of secular relations of pre-determination. \n\n\"Learning to Labour\" was received with wide acclaim. In the years after its original release, Willis discussed his research with a variety of educators and community groups, who provided both support and criticism. An anthology of essays, titled \"Learning to Labour in New Times\", was published in 2004, growing from a 2002 American Educational Research Association meeting to recognise \"Learning to Labour\"'s 25th anniversary. Jean Anyon, Michael Apple, Peter McLaren, and other academics contributed essays to the anthology, applying Willis's ethnography to contemporary issues of gender, race, neoliberalism, work precarity, globalisation, media, and mass incarceration in the United States. For instance, Black youth in United States schools develop cultures of oppositionality, collective identity, and \"tough fronts,\" similar to the Hammertown lads, but are led to incarceration instead of working-class jobs. \"Learning to Labour\" has also been cited in later ethnographies of poor youth and economic inequality, such as Annette Lareau's \"Unequal Childhoods\" and Jay MacLeod's \"Ain't No Makin' It\".\n\nIn the wider field of cultural studies, \"Learning to Labour\" was recognized as an important text in youth studies, as well as working-class leisure and culture, whereas other contemporaneous leftist research in the social sciences tended to foreground employment, labour unions, and political organizations.\n\nWillis acknowledged that shortly after its first release, some right-wing policymakers and politicians sought to appropriate its findings to justify tracking and legitimate educational inequality. While Willis repudiated this use of his work, he even more strongly criticised well-intending, liberal policies that sought to extirpate counter-school cultures: Besides, even in the worst case of interpretation and action taken on the book-the \"oiling\" paradigm-a cynical recognition of actual cultures is preferable to their attempted destruction as \"pathological\" cases, or their chimerical projection into shocking Satanic forms visited upon us from nowhere. \"Solutions\" based on such myths are likely to be cruel because their recipients were never seen as real people.Willis further advanced concepts of profane, working-class youth culture and symbolic labour in his 1990 book \"Common Culture\".\n\nResearchers in education and cultural studies, including Angela McRobbie, criticized \"Learning to Labour\" for neglecting girls and conformist male students in its study. McRobbie wrote that Willis's study took little concern with the overt and violent sexism of the lads falling into a wider pattern of cultural studies' failure to prioritise gender. In reply, Willis acknowledged this sexism, but replied that he had indeed incorporated a construction of working-class masculinity as \"self-entrapment.\"\n\nWillis's ethnography was also criticised for an unclear methodology, inviting questions of reliability and generalization as a \"fish 'n' chips ethnography.'\" Teachers also replied to \"Learning to Labour\" that cultures of resistance were absent in their own classrooms. In turn, Willis argued that such cultures are not immediately obvious, and may be interpreted as individualised behaviours. Moreover, studies of student culture require extensive fieldwork to generate validity, and quantitative methods such as surveys, which may produce greater reliability, cannot satisfactorily report on cultural forms.\n"}
{"id": "48013587", "url": "https://en.wikipedia.org/wiki?curid=48013587", "title": "List of universities in Venezuela by enrollment", "text": "List of universities in Venezuela by enrollment\n\nThis is a list of universities and other higher education institutions in Venezuela by size of student population, it only reflects the institutions with a source of enrollment, those with no information of the enrollment, were not shown.\n\n"}
{"id": "6533810", "url": "https://en.wikipedia.org/wiki?curid=6533810", "title": "Merkur (magazine)", "text": "Merkur (magazine)\n\nMerkur, subtitled Deutsche Zeitschrift für europäisches Denken, is Germany's leading intellectual review, published monthly in Stuttgart by Klett Cotta. \n\n\"Merkur\" has been published since 1947 and had an edition of approximately 4,800 copies as of July 2011. The magazine was formerly headquartered in Munich.\n\nIn the course of its history, many influential scholars and public intellectuals have written for \"Merkur\". Among them were philosophers such as Hannah Arendt, Theodor W. Adorno, Ernst Bloch, Martin Heidegger, Hans-Georg Gadamer, Jürgen Habermas, and Axel Honneth, or sociologists such as Arnold Gehlen, Niklas Luhmann, Hans Joas, and Dirk Baecker, but also writers such as Ingeborg Bachmann, Ilse Aichinger, Alfred Andersch, Hans Magnus Enzensberger, and Kathrin Röggla. \"Merkur\" has been able to gather voices from both the left and the right, which is unusual in the divisive German intellectual landscape. \n\nIn January 2016, the 800th issue was published.\n\nList of magazines in Germany\n"}
{"id": "6992172", "url": "https://en.wikipedia.org/wiki?curid=6992172", "title": "Modern furniture", "text": "Modern furniture\n\nModern furniture refers to furniture produced from the late 19th century through the present that is influenced by modernism. Post-World War II ideals of cutting excess, commodification, and practicality of materials in design heavily influenced the aesthetic of the furniture. It was a tremendous departure from all furniture design that had gone before it. There was an opposition to the decorative arts, which included Art Nouveau, Neoclassical, and Victorian styles. Dark or gilded carved wood and richly patterned fabrics gave way to the glittering simplicity and geometry of polished metal. The forms of furniture evolved from visually heavy to visually light. This shift from decorative to minimalist principles of design can be attributed to the introduction of new technology, changes in philosophy, and the influences of the principles of architecture. As Philip Johnson, the founder of the Department of Architecture and Design at the Museum of Modern Art articulates: \n\"Today industrial design is functionally motivated and follows the same principles as modern architecture: machine-like simplicity, smoothness of surface, avoidance of ornament ... It is perhaps the most fundamental contrast between the two periods of design that in 1900 the Decorative Arts possessed ...\"\nWith the machine aesthetic, modern furniture easily came to promote factory modules, which emphasized the time-managing, efficient ideals of the period. Modernist design was able to strip down decorative elements and focus on the design of the object in order to save time, money, material, and labour. The goal of modern design was to capture timeless beauty in spare precision.\n\nPrior to the modernist design movement, there was an emphasis on furniture as an ornament. The length of time a piece took to create was often a measure of its value and desirability. The origins of design can be traced back to the Industrial Revolution and the birth of mechanized production. With new resources and advancements, a new philosophy emerged, one that shifted the emphasis of objects being created for decorative purposes to being designs that promote functionality, accessibility, and production.\n\nThe idea of accessible, mass-produced design that is affordable to anyone was not only applied to industrial mechanics, but also to the aesthetics of architecture and furniture. This philosophy of practicality came to be called Functionalism. It became a popular \"catchword\" and played a large role in theories of modern design. Functionalism rejected the imitation of stylistic and historical forms and sought an establishment of functionality in a piece. Functionalist designers would consider the interaction of the design with its user and how many of the features, such as shape, colour, and size, would conform to the human posture. Western design generally, whether architectural or design of furniture, had for millennia sought to convey an idea of lineage, a connection with tradition and history. \nHowever, the modern movement sought newness, originality, technical innovation, and ultimately the message that it conveyed spoke of the present and the future, rather than of what had gone before it.\nThe modernist design seems to have evolved out of a combination of influences: technically innovative materials and new manufacturing methods. Following the Second Industrial Revolution, new philosophies and artists emerged from the De Stijl movement in the Netherlands, the Deutscher Werkbund and the Bauhaus school, both located in Germany.\n\nThe De Stijl (The Style) movement, was founded in 1917 by Theo Van Doesburg in Amsterdam. The movement was based on the principles of promoting abstraction and universality by reducing excessive elements down to the essentials of form and colour. Dutch design generally has shown a preference for simple materials and construction, but De Stijl artists, architects, and designers strove to combine these elements to create a new visual culture. Characteristics of furniture from this movement include simplified geometry of vertical and horizontal compositions and pure primary colours and black and white. It was the rejection of the decorative excesses from Art Nouveau and promoted logicality through construction and function. Influential artists from this movement include Gerrit Rietveld, Piet Mondrian, and Mies van der Rohe, who continued to evolve the ideas of modernist design.\n\nFounded in 1907 in Munich, Germany, the Deutscher Werkbund was an organization of artists, designers, and manufacturers that pushed to create a cultural utopia achieved through a design and new ideas in the early twentieth century. They shared the Modern thought of \"form follows function\" as well as the \"ethnically pure\" design principles such as quality, material honestly, functionality, and sustainability. The DWB played a key role in advocating these ideas to other German artists and designers, which inspired the development of many Modern design institutions. Among the most notable architects and designers from the DWB are: Hermann Muthesius, Peter Behrens, and Ludwig Mies van der Rohe.\n\nThe Bauhaus school, founded in 1919 in Weimar, Germany, by architect Walter Gropius, was an art school that combined all aspects of art. It eventually was forced to move to Dessau, Germany, in 1925 due to political tensions, then Berlin, in 1932 until the doors of the school were closed from the pressure of the Nazi regime. With the change of location came a change of direction in the institution. The Bauhaus adopted an emphasis on production in Dessau, but maintained its intellectual concerns in design. Throughout the years, the goal of the institution was to combine intellectual, practical, commercial, and aesthetic concerns through art and technology. The Bauhaus promoted the unity of all areas of art and design: from typography to tableware, clothing, performance, furniture, art, and architecture. Prominent artists and designers from the Bauhaus include: Marcel Breur, Marianne Brandt, Hannes Meyer (who was Gropius's successor, only to be replaced by Mies van der Rohe).\n\nAn aesthetic preference for the baroque and the complex was challenged not only by new materials and the courage and creativity of a few Europeans, but also by the growing access to African and Asian design. In particular the influence of Japanese design is legend: in the last years of the 19th century the Edo period in Japan, Japanese isolationist policy began to soften, and trade with the west began in ernest. The artifacts that emerged were striking in their simplicity, their use of solid planes of color without ornament, and contrasting use of pattern. A tremendous fashion for all things Japanese – Japonism – swept Europe. Some say that the western Art Nouveau movement emerged from this influence directly. Designers such as Charles Rennie MacIntosh and Eileen Gray are known for both their modern and Art Deco work, and they and others like Frank Lloyd Wright are notable for a certain elegant blending of the two styles.\n\nThe use of new materials, such as steel in its many forms; glass, used by Walter Gropius; molded plywood, such as that used by Charles and Ray Eames; and of course plastics, were formative in the creation of these new designs. They would have been considered pioneering, even shocking in contrast to what came before. This interest in new and innovative materials and methods - produced a certain blending of the disciplines of technology and art. And this became a working philosophy among the members of the Deutscher Werkbund. The Werkbund was a government-sponsored organization to promote German art and design around the world. Many of those involved with it including Mies van der Rohe, Lilly Reich and others, were later involved in the Bauhaus School, and so it is not surprising perhaps that the Bauhaus School took on the mantle of this philosophy. They evolved a particular interest in using these new materials in such a way that they might be mass-produced and therefore make good design more accessible to the masses.\n\nThe first versions of Gerrit Rietveld's Red-Blue Armchair were created around 1917. However, they were originally stained black - the colour was eventually added to give characteristics of De Stijl in 1923. Rietveld's intent was to design a piece of furniture that could be cheaply mass-produced. He uses standard beechwood laths and pine planks that intersect and are fixed by wooden pegs. The functions of construction, the seat, the back and armrests are explicitly separated from one another visually. In fact, Rietveld saw the chair as the skeleton of an overstuffed armchair with all the excessive components removed. \nThis modernist creation is perhaps one of the most iconic furniture designs of all time. The Wassily Chair, also known as the Model B3 chair, was designed by Marcel Breuer in 1925-26 while he was the head of the cabinet-making workshop at the Bauhaus, in Dessau, Germany.\n\nThis piece is particularly influential because it introduces a simple, yet elegant and light-weight industrial material to be used in structures within the domestic space: chrome plated tubular steel. The design of the chair is revolutionary with its use of symmetrical, geometric planes framed by the tubular steel. Breuer uses simple straps on canvas for the seat, back and armrests to support the seated figure. The concept of the use of tubular steel, a never before seen the material in the domestic space was inspired by the handles of Breuer's bicycle. He reasoned that if such a material was light-weight yet strong enough to support the body in motion, it is likely to be able to support the body at rest. He applies uncomplicated essentials (the canvas strips) to create a functional aesthetic as well. Nonetheless, the Model B3 Chair (dubbed the Wassily Chair by the manufacturing company, Gavina after learning of the anecdote involving the painter Wassily Kandinsky) inspired many artists and designers to include the use of chrome plated steel, including Le Corbusier, who includes it as a structure for his Chaise Longue.\n\nInspired by Marcel Breuer's use of chrome plated tubular steel in his Wassily Chair, in 1928, Le Corbusier creates a sleek steel support for the back and seat of his Chaise Longue. The Chaise Longue features a movable seat section and an adjustable headrest, which is an early example of ergonomic design. With the tubular steel frames and leather or skin upholstery, the sleek Chaise Longue was initially manufactured for private French house commissions including the Villa Savoye, Poissy (1929–31) and the Ville-d'Avray. This piece epitomizes the mass production of the industrial age through the use of materials and structure. However, unlike the Wassily Chair, the complex design made reproduction expensive.\n\nDesigned in 1927 as a bedside table for the guest room in E-1027, the home Eileen Gray designed for herself (and Jean Badovici) in Cap Martin, France, the asymmetry of this piece is characteristic of her \"non-conformist\" design style in her architectural projects and furniture. Eileen Gray had always been influenced by Japanese lacquer and furniture, and the minimalist lines and elegant structure found normally in traditional Japanese works are found in most of Gray's objects. The name, E-1027, can be seen in a somewhat romantic reading: The \"E\" stands for \"Eileen\" and the numbers, corresponding to their sequence in the alphabet, stand for \"J\", \"B\", and \"G\". The second and tenth letter allude to her friend and mentor, Jean Badovici.Gray's emphasis on functionalist design is apparent in her use of tubular steel and the concentric symmetry of the surface. Notably, this piece also has specific utility, as it can be adjusted such that one can eat breakfast in bed on it. Gray's sister had requested such accommodation during her visits to E-1027. \nThe Barcelona chair has come to represent the Bauhaus design movement. Many consider it to be functional art, rather than just furniture. Designed by Mies Van Der Rohe and Lilly Reich in 1929 for the German Pavilion at the international design fair, the 1929 Barcelona International Exposition, it is said to have been inspired by both the folding chairs of the Pharaohs, and the 'X' shaped footstools of the Romans, and dedicated to the Spanish royal families. Like other designers following Breuer's example, he incorporates the use of chrome-plated flat steel bars to create a single 'S'-shaped curve. The front legs cross the 'S' curve of the bars forming the seat and the back legs. It creates a sleek and intentionally simple aesthetic to the piece.\n\nIn 1963 Robin Day designed the \"Polyprop\" chair for the British furniture design house Hille. Made of moulded polypropylene, the \"Polyprop\" sold in millions and became the world's best-selling chair. Today it is regarded as a modern design classic, and has been celebrated by Royal Mail with a commemorative postage stamp.\n\nNoguchi table was designed by Isamu Noguchi (1904–1988), a sculptor, draftsman, potter, architect, landscape architect, product, furniture and stage designer. Half American, half Japanese, he is famous for his organic modern forms. He often stated, \"Everything is sculpture, any materials, any idea without hindrance born into space, I consider sculpture.\" The Noguchi table – has become famous for its unique and unmistakable simplicity. It is refined and at the same time natural, it is one of the most sought-after pieces associated with the modern classic furniture movement.\n\nChronologically the design movement that produced modern furniture design, began earlier than one might imagine. Many of its most recognizable personalities were born of the 19th or the very beginning of the 20th centuries.\n\n\nThey were teaching and studying in Germany and elsewhere in the 1920s and 30s. At among other places the Bauhaus school of art and architecture. The furniture that was produced during this era is today known as \"Modern Classic Furniture\" or \"Mid Century Modern\".\n\nBoth the Bauhaus School and the Deutscher Werkbund had as their specific creative emphasis the blending of technology, new materials and art.\n\nObviously not all furniture produced since this time is modern, for there is still a tremendous amount of traditional design being reproduced for today's market and then, of course, there is also an entire breed of design which sits between the two, and is referred to as transitional design. Neither entirely modern or traditional, it seeks to blend elements of multiple styles. It often includes both modern and traditional as well as making visual reference to classical Greek form and/or other non-western styles (for example Tribal African pattern, Asian scroll work etc.).\n\nToday contemporary furniture designers and manufacturers continue to evolve the design. Still seeking new materials, with which to produce unique forms, still employing simplicity and lightness of form, in preference to a heavy ornament. And most of all they are still endeavouring to step beyond what has gone before to create entirely new visual experiences for us.\n\nThe designs that prompted this paradigm shift were produced in the middle of the 20th century, most of them well before 1960. And yet they are still regarded internationally as symbols of the modern age, the present and perhaps even the future. Modern Classic Furniture became an icon of elegance and sophistication.\n\n\n\n"}
{"id": "20224", "url": "https://en.wikipedia.org/wiki?curid=20224", "title": "Mummy", "text": "Mummy\n\nA mummy is a deceased human or an animal whose skin and organs have been preserved by either intentional or accidental exposure to chemicals, extreme cold, very low humidity, or lack of air, so that the recovered body does not decay further if kept in cool and dry conditions. Some authorities restrict the use of the term to bodies deliberately embalmed with chemicals, but the use of the word to cover accidentally desiccated bodies goes back to at least 1615 AD (See the section Etymology and meaning).\n\nMummies of humans and other animals have been found on every continent, both as a result of natural preservation through unusual conditions, and as cultural artifacts. Over one million animal mummies have been found in Egypt, many of which are cats. Many of the Egyptian animal mummies are sacred ibis, and radiocarbon dating suggests the Egyptian Ibis mummies that have been analyzed were from time frame that falls between approximately 450 and 250 BC.\n\nIn addition to the well-known mummies of ancient Egypt, deliberate mummification was a feature of several ancient cultures in areas of America and Asia with very dry climates. The Spirit Cave mummies of Fallon, Nevada in North America were accurately dated at more than 9,400 years old. Before this discovery, the oldest known deliberate mummy was a child, one of the Chinchorro mummies found in the Camarones Valley, Chile, which dates around 5050 BC. The oldest known naturally mummified human corpse is a severed head dated as 6,000 years old, found in 1936 AD at the site named Inca Cueva No. 4 in South America. \n\nThe English word \"mummy\" is derived from medieval Latin \"mumia\", a borrowing of the medieval Arabic word \"mūmiya\" (مومياء) and from a Persian word \"mūm\" (wax), which meant an embalmed corpse, and as well as the bituminous embalming substance, and also meant \"bitumen\". The Medieval English term \"mummy\" was defined as \"medical preparation of the substance of mummies\", rather than the entire corpse, with Richard Hakluyt in 1599 AD complaining that \"these dead bodies are the Mummy which the Phisistians and Apothecaries doe against our willes make us to swallow\". These substances were defined as mummia.\n\nThe OED defines a mummy as \"the body of a human being or animal embalmed (according to the ancient Egyptian or some analogous method) as a preparation for burial\", citing sources from 1615 AD onward. However, Chamber's \"Cyclopædia\" and the Victorian zoologist Francis Trevelyan Buckland define a mummy as follows: \"A human or animal body desiccated by exposure to sun or air. Also applied to the frozen carcase of an animal imbedded in prehistoric snow\".\n\nWasps of the genus \"Aleiodes\" are known as \"mummy wasps\" because they wrap their caterpillar prey as \"mummies\".\n\nWhile interest in the study of mummies dates as far back as Ptolemaic Greece, most structured scientific study began at the beginning of the 20th century. Prior to this, many rediscovered mummies were sold as curiosities or for use in pseudoscientific novelties such as mummia. The first modern scientific examinations of mummies began in 1901, conducted by professors at the English-language Government School of Medicine in Cairo, Egypt. The first X-ray of a mummy came in 1903, when professors Grafton Elliot Smith and Howard Carter used the only X-ray machine in Cairo at the time to examine the mummified body of Thutmose IV. British chemist Alfred Lucas applied chemical analyses to Egyptian mummies during this same period, which returned many results about the types of substances used in embalming. Lucas also made significant contributions to the analysis of Tutankhamun in 1922.\n\nPathological study of mummies saw varying levels of popularity throughout the 20th century. In 1992, the First World Congress on Mummy Studies was held in Puerto de la Cruz on Tenerife in the Canary Islands. More than 300 scientists attended the Congress to share nearly 100 years of collected data on mummies. The information presented at the meeting triggered a new surge of interest in the subject, with one of the major results being integration of biomedical and bioarchaeological information on mummies with existing databases. This was not possible prior to the Congress due to the unique and highly specialized techniques required to gather such data.\n\nIn more recent years, CT scanning has become an invaluable tool in the study of mummification by allowing researchers to digitally \"unwrap\" mummies without risking damage to the body. The level of detail in such scans is so intricate that small linens used in tiny areas such as the nostrils can be digitally reconstructed in 3-D. Such modelling has been utilized to perform digital autopsies on mummies to determine cause of death and lifestyle, such as in the case of Tutankhamun.\n\nMummies are typically divided into one of two distinct categories: anthropogenic or spontaneous. Anthropogenic mummies were deliberately created by the living for any number of reasons, the most common being for religious purposes. Spontaneous mummies, such as Ötzi, were created unintentionally due to natural conditions such as extremely dry heat or cold, or anaerobic conditions such as those found in bogs. While most individual mummies exclusively belong to one category or the other, there are examples of both types being connected to a single culture, such as those from the ancient Egyptian culture and the Andean cultures of South America.\n\nThe earliest ancient Egyptian mummies were created naturally due to the environment in which they were buried. In the era prior to 3500 BC, Egyptians buried the dead in pit graves, without regard to social status. Pit graves were often shallow. This characteristic allowed for the hot, dry sand of the desert to dehydrate the bodies, leading to natural mummification.\n\nThe natural preservation of the dead had a profound effect on ancient Egyptian religion. Deliberate mummification became an integral part of the rituals for the dead beginning as early as the 2nd dynasty (about 2800 BC). New research of an 11-year study by University of York, Macquarie University and University of Oxford suggests mummification occurred 1,500 years earlier than first thought. Egyptians saw the preservation of the body after death as an important step to living well in the afterlife. As Egypt gained more prosperity, burial practices became a status symbol for the wealthy as well. This cultural hierarchy lead to the creation of elaborate tombs, and more sophisticated methods of embalming.\nBy the 4th dynasty (about 2600 BC) Egyptian embalmers began to achieve \"true mummification\" through a process of evisceration, followed by preserving the body in various minerals and oils. Much of this early experimentation with mummification in Egypt is unknown.\n\nThe few documents that directly describe the mummification process date to the Greco-Roman period. The majority of the papyri that have survived only describe the ceremonial rituals involved in embalming, not the actual surgical processes involved. A text known as \"The Ritual of Embalming\" does describe some of the practical logistics of embalming, however, there are only two known copies and each is incomplete. With regards to mummification shown in images, there are apparently also very few. The tomb of Tjay designated TT23, is one of only two known which show the wrapping of a mummy (Riggs 2014).\n\nAnother text that describes the processes being used in latter periods is Herodotus' Histories. Written in Book 2 of the \"Histories\" is one of the most detailed descriptions of the Egyptian mummification process, including the mention of using natron in order to dehydrate corpses for preservation. However, these descriptions are short and fairly vague, leaving scholars to infer the majority of the techniques that were used by studying mummies that have been unearthed.\n\nBy utilizing current advancements in technology, scientists have been able to uncover a plethora of new information about the techniques used in mummification. A series of CT scans performed on a 2,400-year-old mummy in 2008 revealed a tool that was left inside the cranial cavity of the skull. The tool was a rod, made of an organic material, that was used to break apart the brain to allow it to drain out of the nose. This discovery helped to dispel the claim within Herodotus' works that the rod had been a hook made of iron. Earlier experimentation in 1994 by researchers Bob Brier and Ronald Wade supported these findings. While attempting to replicate Egyptian mummification, Brier and Wade discovered that removal of the brain was much easier when the brain was liquefied and allowed to drain with the help of gravity, as opposed to trying to pull the organ out piece-by-piece with a hook.\n\nThrough various methods of study over many decades, modern Egyptologists now have an accurate understanding of how mummification was achieved in ancient Egypt. The first and most important step was to halt the process of decomposition, by removing the internal organs and washing out the body with a mix of spices and palm wine. The only organ left behind was the heart, as tradition held the heart was the seat of thought and feeling and would therefore still be needed in the afterlife. After cleansing, the body was then dried out with natron inside the empty body cavity as well as outside on the skin. The internal organs were also dried and either sealed in individual jars, or wrapped to be replaced within the body. This process typically took forty days.\n\nAfter dehydration, the mummy was wrapped in many layers of linen cloth. Within the layers, Egyptian priests placed small amulets to guard the decedent from evil. Once the mummy was completely wrapped, it was coated in a resin in order to keep the threat of moist air away. Resin was also applied to the coffin in order to seal it. The mummy was then sealed within its tomb, alongside the worldly goods that were believed to help aid it in the afterlife.\n\nAspergillus niger has been found in the mummies of ancient Egyptian tombs and can be inhaled when they are disturbed.\n\nMummification is one of the defining customs in ancient Egyptian society for people today. The practice of preserving the human body is believed to be a quintessential feature of Egyptian life. Yet even mummification has a history of development and was accessible to different ranks of society in different ways during different periods. There were at least three different processes of mummification according to Herodotus. They range from \"the most perfect\" to the method employed by the \"poorer classes\".\n\nThe most expensive process was to preserve the body by dehydration and protect against pests, such as insects. Almost all of the actions Herodotus described serve one of these two functions.\n\nFirst, the brain was removed from the cranium through the nose; the gray matter was discarded. Modern mummy excavations have shown that instead of an iron hook inserted through the nose as Herodotus claims, a rod was used to liquefy the brain via the cranium, which then drained out the nose by gravity. The embalmers then rinsed the skull with certain drugs that mostly cleared any residue of brain tissue and also had the effect of killing bacteria. Next, the embalmers made an incision along the flank with a sharp blade fashioned from an Ethiopian stone and removed the contents of the abdomen. Herodotus does not discuss the separate preservation of these organs and their placement either in special jars or back in the cavity, a process that was part of the most expensive embalming, according to archaeological evidence.\n\nThe abdominal cavity was then rinsed with palm wine and an infusion of crushed, fragrant herbs and spices; the cavity was then filled with spices including myrrh, cassia, and, Herodotus notes, \"every other sort of spice except frankincense\", also to preserve the person.\n\nThe body was further dehydrated by placing it in natron, a naturally occurring salt, for seventy days. Herodotus insists that the body did not stay in the natron longer than seventy days. Any shorter time and the body is not completely dehydrated; any longer, and the body is too stiff to move into position for wrapping. The embalmers then wash the body again and wrapped it with linen bandages. The bandages were covered with a gum that modern research has shown is both waterproofing agent and an antimicrobial agent.\n\nAt this point, the body was given back to the family. These \"perfect\" mummies were then placed in wooden cases that were human-shaped. Richer people placed these wooden cases in stone sarcophagi that provided further protection. The family placed the sarcophagus in the tomb upright against the wall, according to Herodotus.\n\nThe second process that Herodotus describes was used by middle-class people or people who \"wish to avoid expense\". In this method, an oil derived from cedar trees was injected with a syringe into the abdomen. A rectal plug prevented the oil from escaping. This oil probably had the dual purpose of liquefying the internal organs but also of disinfecting the abdominal cavity. (By liquefying the organs, the family avoided the expense of canopic jars and separate preservation.) The body was then placed in natron for seventy days. At the end of this time, the body was removed and the cedar oil, now containing the liquefied organs, was drained through the rectum. With the body dehydrated, it could be returned to the family. Herodotus does not describe the process of burial of such mummies, but they were perhaps placed in a shaft tomb. Poorer people used coffins fashioned from terracotta.\n\nThe third and least-expensive method the embalmers offered was to clear the intestines with an unnamed liquid, injected as an enema. The body was then placed in natron for seventy days and returned to the family. Herodotus gives no further details.\n\nIn Christian tradition, some bodies of saints are naturally conserved and venerated.\n\nIn addition to the mummies of Egypt, there have been instances of mummies being discovered in other areas of the African continent. The bodies show a mix of anthropogenic and spontaneous mummification, with some being thousands of years old.\n\nThe mummified remains of an infant were discovered during an expedition by archaeologist Fabrizio Mori to Libya during the winter of 1958–1959 in the natural cave structure of Uan Muhuggiag. After curious deposits and cave paintings were discovered on the surfaces of the cave, expedition leaders decided to excavate. Uncovered alongside fragmented animal bone tools was the mummified body of an infant, wrapped in animal skin and wearing a necklace made of ostrich egg shell beads. Professor Tongiorgi of the University of Pisa radiocarbon-dated the infant to between 5,000–8,000 years old. A long incision located on the right abdominal wall, and the absence of internal organs, indicated that the body had been eviscerated post-mortem, possibly in an effort to preserve the remains. A bundle of herbs found within the body cavity also supported this conclusion. Further research revealed that the child had been around 30 months old at the time of death, though gender could not be determined due to poor preservation of the sex organs.\n\nThe first mummy to be discovered in South Africa was found in the Baviaanskloof Wilderness Area by Dr. Johan Binneman in 1999. Nicknamed Moses, the mummy was estimated to be around 2,000 years old. After being linked to the indigenous Khoi culture of the region, the National Council of Khoi Chiefs of South Africa began to make legal demands that the mummy be returned shortly after the body was moved to the Albany Museum in Grahamstown.\n\nThe mummies of Asia are usually considered to be accidental. The decedents were buried in just the right place where the environment could act as an agent for preservation. This is particularly common in the desert areas of the Tarim Basin and Iran. Mummies have been discovered in more humid Asian climates, however these are subject to rapid decay after being removed from the grave.\n\nMummies from various dynasties throughout China's history have been discovered in several locations across the country. They are almost exclusively considered to be unintentional mummifications. Many areas in which mummies have been uncovered are difficult for preservation, due to their warm, moist climates. This makes the recovery of mummies a challenge, as exposure to the outside world can cause the bodies to decay in a matter of hours.\n\nAn example of a Chinese mummy that was preserved despite being buried in an environment not conducive to mummification is Xin Zhui. Also known as Lady Dai, she was discovered in the early 1970s at the Mawangdui archaeological site in Changsha. She was the wife of the marquis of Dai during the Han dynasty, who was also buried with her alongside another young man often considered to be a very close relative. However, Xin Zhui's body was the only one of the three to be mummified. Her corpse was so well-preserved that surgeons from the Hunan Provincial Medical Institute were able to perform an autopsy. The exact reason why her body was so completely preserved has yet to be determined.\n\nSome of the more infamous mummies to be discovered in China are those termed Tarim mummies because of their discovery in the Tarim Basin. The dry desert climate of the basin proved to be an excellent agent for desiccation. For this reason, over 200 Tarim mummies, which are over 4,000 years old, were excavated from a cemetery in the present-day Xinjiang region. The mummies were found buried in upside-down boats with hundreds of 13-foot long wooden poles in the place of tombstones. DNA sequence data shows that the mummies had Haplogroup R1a (Y-DNA) characteristic of western Eurasia in the area of East-Central Europe, Central Asia and Indus Valley. This has created a stir in the Turkic-speaking Uighur population of the region, who claim the area has always belonged to their culture, while it was not until the 10th century when the Uighurs are said by scholars to have moved to the region from Central Asia. American Sinologist Victor H. Mair claims that \"the earliest mummies in the Tarim Basin were exclusively Caucasoid, or Europoid\" with \"east Asian migrants arriving in the eastern portions of the Tarim Basin around 3,000 years ago\", while Mair also notes that it was not until 842 that the Uighur peoples settled in the area. Other mummified remains have been recovered from around the Tarim Basin at sites including Qäwrighul, Yanghai, Shengjindian, Shanpula, Zaghunluq, and Qizilchoqa.\n\nAs of 2012, at least eight mummified human remains have been recovered from the Douzlakh Salt Mine at Chehr Abad in northwestern Iran. Due to their salt preservation, these bodies are collectively known as Saltmen. Carbon-14 testing conducted in 2008 dated three of the bodies to around 400 BC. Later isotopic research on the other mummies returned similar dates, however, many of these individuals were found to be from a region that is not closely associated with the mine. It was during this time that researchers determined the mine suffered a major collapse, which likely caused the death of the miners. Since there is significant archaeological data that indicates the area was not actively inhabited during this time period, current consensus holds that the accident occurred during a brief period of temporary mining activity.\n\nIn 1993, a team of Russian archaeologists led by Dr. Natalia Polosmak discovered the Siberian Ice Maiden, a Scytho-Siberian woman, on the Ukok Plateau in the Altai Mountains near the Mongolian border. The mummy was naturally frozen due to the severe climatic conditions of the Siberian steppe. Also known as Princess Ukok, the mummy was dressed in finely detailed clothing and wore an elaborate headdress and jewelry. Alongside her body were buried six decorated horses and a symbolic meal for her last journey. Her left arm and hand were tattooed with animal style figures, including a highly stylized deer.\n\nThe Ice Maiden has been a source of some recent controversy. The mummy's skin has suffered some slight decay, and the tattoos have faded since the excavation. Some residents of the Altai Republic, formed after the breakup of the Soviet Union, have requested the return of the Ice Maiden, who is currently stored in Novosibirsk in Siberia.\n\nAnother Siberian mummy, a man, was discovered much earlier in 1929. His skin was also marked with tattoos of two monsters resembling griffins, which decorated his chest, and three partially obliterated images which seem to represent two deer and a mountain goat on his left arm.\n\nPhilippine mummies are called Kabayan Mummies. They are common in Igorot culture and their heritage. The mummies are found in some areas named Kabayan, Sagada and among others. The mummies are dated between the 14th and 19th centuries.\n\nThe European continent is home to a diverse spectrum of spontaneous and anthropogenic mummies. Some of the best-preserved mummies have come from bogs located across the region. The Capuchin monks that inhabited the area left behind hundreds of intentionally-preserved bodies that have provided insight into the customs and cultures of people from various eras. One of the oldest, and most infamous, mummies (nicknamed Ötzi) was discovered on this continent. New mummies continue to be uncovered in Europe well into the 21st Century.\n\nThe United Kingdom, the Republic of Ireland, Germany, the Netherlands, Sweden, and Denmark have produced a number of bog bodies, mummies of people deposited in sphagnum bogs, apparently as a result of murder or ritual sacrifices. In such cases, the acidity of the water, low temperature and lack of oxygen combined to tan the body's skin and soft tissues. The skeleton typically disintegrates over time. Such mummies are remarkably well preserved on emerging from the bog, with skin and internal organs intact; it is even possible to determine the decedent's last meal by examining stomach contents. A famous case is that of the Haraldskær Woman, who was discovered by labourers in a bog in Jutland in 1835. She was erroneously identified as an early medieval Danish queen, and for that reason was placed in a royal sarcophagus at the Saint Nicolai Church, Vejle, where she currently remains. Another famous bog body, also from Denmark, known as the Tollund Man was discovered in 1950. The corpse was noted for its excellent preservation of the face and feet, which appeared as if the man had recently died. To this day, only the head of Tollund Man remains, due to the decomposition of the rest of his body, which was not preserved along with the head.\n\nThe mummies of the Canary Islands belong to the indigenous Guanche people and date to the time before 14th Century Spanish explorers settled in the area. All deceased people within the Guanche culture were mummified during this time, though the level of care taken with embalming and burial varied depending on individual social status. Embalming was carried out by specialized groups, organized according to gender, who were considered unclean by the rest of the community. The techniques for embalming were similar to those of the ancient Egyptians; involving evisceration, preservation, and stuffing of the evacuated bodily cavities, then wrapping of the body in animal skins. Despite the successful techniques utilized by the Guanche, very few mummies remain due to looting and desecration.\n\nThe majority of mummies recovered in the Czech Republic come from underground crypts. While there is some evidence of deliberate mummification, most sources state that desiccation occurred naturally due to unique conditions within the crypts.\n\nThe Capuchin Crypt in Brno contains three hundred years of mummified remains directly below the main altar. Beginning in the 18th Century when the crypt was opened, and continuing until the practice was discontinued in 1787, the Capuchin monks of the monastery would lay the deceased on a pillow of bricks on the ground. The unique air quality and topsoil within the crypt naturally preserved the bodies over time.\n\nApproximately fifty mummies were discovered in an abandoned crypt beneath the Church of St. Procopius of Sázava in Vamberk in the mid-1980s. Workers digging a trench accidentally broke into the crypt, which began to fill with waste water. The mummies quickly began to deteriorate, though thirty-four were able to be rescued and stored temporarily at the District Museum of the Orlické Mountains until they could be returned to the monastery in 2000. The mummies range in age and social status at time of death, with at least two children and one priest. The majority of the Vamberk mummies date from the 18th century.\n\nThe Klatovy catacombs currently house an exhibition of Jesuit mummies, alongside some aristocrats, that were originally interred between 1674–1783. In the early 1930s, the mummies were accidentally damaged during repairs, resulting in the loss of 140 bodies. The newly updated airing system preserves the thirty-eight bodies that are currently on display.\n\nApart from several bog bodies, Denmark has also yielded several other mummies, such as the three Borum Eshøj mummies, the Skrydstrup Woman and the Egtved Girl, who were all found inside burial mounds, or tumuli.\n\nIn 1875, the Borum Eshøj grave mound was uncovered, which had been built around three coffins, which belonged to a middle aged man and woman as well as a man in his early twenties. Through examination, the woman was discovered to be around 50–60 years old. She was found with several artifacts made of bronze, consisting of buttons, a belt plate, and rings, showing she was of higher class. All of the hair had been removed from the skull later when farmers had dug through the casket. Her original hairstyle is unknown. The two men wore kilts, and the younger man wore a sheath which contained a bronze dagger. All three mummies were dated to 1351–1345 BC.\n\nThe Skrydstrup Woman was unearthed from a tumulus in Southern Jutland, in 1935. Carbon-14 dating showed that she had died around 1300 BC; examination also revealed that she was around 18–19 years old at the time of death, and that she had been buried in the summertime. Her hair had been drawn up in an elaborate hairstyle, which was then covered by a horse hair hairnet made by the sprang technique. She was wearing a blouse and a necklace as well as two golden earrings, showing she was of higher class.\n\nThe Egtved Girl, dated to 1370 BC, was also found inside a sealed coffin within a tumulus, in 1921. She was wearing a bodice and a skirt, including a belt and bronze bracelets. Found with the girl, at her feet, were the cremated remains of a child and, by her head, a box containing some bronze pins, a hairnet, and an awl.\n\nIn 1994, 265 mummified bodies were found in the crypt of a Dominican church in Vác, Hungary from the 1729–1838 period. The discovery proved to be scientifically important, and by 2006 an exhibition was established in the Museum of Natural History in Budapest. Unique to the Hungarian mummies are their elaborately decorated coffins, with no two being exactly alike.\n\nThe varied geography and climatology of Italy has led to many cases of spontaneous mummification. Italian mummies display the same diversity, with a conglomeration of natural and intentional mummification spread across many centuries and cultures.\n\nThe oldest natural mummy in Europe was discovered in 1991 in the Ötztal Alps on the Austrian-Italian border. Nicknamed Ötzi, the mummy is a 5,300-year-old male believed to be a member of the Tamins-Carasso-Isera cultural group of South Tyrol. Despite his age, a recent DNA study conducted by Walther Parson of Innsbruck Medical University revealed Ötzi has 19 living genetic relatives.\n\nThe Capuchin Catacombs of Palermo were built in the 16th century by the monks of Palermo’s Capuchin monastery. Originally intended to hold the deliberately mummified remains of dead friars, interment in the catacombs became a status symbol for the local population in the following centuries. Burials continued until the 1920s, with one of the most famous final burials being that of Rosalia Lombardo. In all, the catacombs host nearly 8000 mummies. (See: Catacombe dei Cappuccini)\n\nThe most recent discovery of mummies in Italy came in 2010, when sixty mummified human remains were found in the crypt of the Conversion of St Paul church in Roccapelago di Pievepelago, Italy. Built in the 15th century as a cannon hold and later converted in the 16th century, the crypt had been sealed once it had reached capacity, leaving the bodies to be protected and preserved. The crypt was reopened during restoration work on the church, revealing the diverse array of mummies inside. The bodies were quickly moved to a museum for further study.\n\nThe mummies of North America are often steeped in controversy, as many of these bodies have been linked to still-existing native cultures. While the mummies provide a wealth of historically-significant data, native cultures and tradition often demands the remains be returned to their original resting places. This has led to many legal actions by Native American councils, leading to most museums keeping mummified remains out of the public eye.\n\nKwäday Dän Ts'ìnchi (\"Long ago person found\" in the Southern Tutchone language of the Champagne and Aishihik First Nations), was found in August 1999 by three First Nations hunters at the edge of a glacier in Tatshenshini-Alsek Provincial Park, British Columbia, Canada. According to the Kwäday Dän Ts'ìnchi Project, the remains are the oldest well preserved mummy discovered in North America. (It should be noted that the Spirit Cave mummy although not well preserved, is much older.) Initial radiocarbon tests date the mummy to around 550 years-old.\n\nIn 1972, eight remarkably preserved mummies were discovered at an abandoned Inuit settlement called Qilakitsoq, in Greenland. The \"Greenland Mummies\" consisted of a six-month-old baby, a four-year-old boy, and six women of various ages, who died around 500 years ago. Their bodies were naturally mummified by the sub-zero temperatures and dry winds in the cave in which they were found.\n\nIntentional mummification in pre-Columbian Mexico was practiced by the Aztec culture. These bodies are collectively known as Aztec mummies. Genuine Aztec mummies were \"bundled\" in a woven wrap and often had their faces covered by a ceremonial mask. Public knowledge of Aztec mummies increased due to traveling exhibits and museums in the 19th and 20th centuries, though these bodies were typically naturally desiccated remains and not actually the mummies associated with Aztec culture. (See: Aztec mummy)\n\nNatural mummification has been known to occur in several places in Mexico, though the most famous are the mummies of Guanajuato. A collection of these mummies, most of which date to the late 19th century, have been on display at \"El Museo de las Momias\" in the city of Guanajuato since 1970. The museum claims to have the smallest mummy in the world on display (a mummified fetus). It was thought that minerals in the soil had the preserving effect, however it may rather be due to the warm, arid climate. Mexican mummies are also on display in the small town of Encarnación de Díaz, Jalisco.\n\nSpirit Cave Man was discovered in 1940 during salvage work prior to guano mining activity that was scheduled to begin in the area. The mummy is a middle-aged male, found completely dressed and lying on a blanket made of animal skin. Radiocarbon tests in the 1990s dated the mummy to being nearly 9,000 years old. The remains are currently held at the Nevada State Museum. There has been some controversy within the local Native American community, who began petitioning to have the remains returned and reburied in 1995.\n\nMummies from the Oceania are not limited only to Australia. Discoveries of mummified remains have also been located in New Zealand, and the Torres Strait, though these mummies have been historically harder to examine and classify. Prior to the 20th Century, most literature on mummification in the region was either silent or anecdotal. However, the boom of interest generated by the scientific study of Egyptian mummification lead to more concentrated study of mummies in other cultures, including those of Oceania.\n\nThe aboriginal mummification traditions found in Australia are thought be related to those found in the Torres Strait islands, the inhabitants of which achieved a high level of sophisticated mummification techniques (See:Torres Strait). Australian mummies lack some of the technical ability of the Torres Strait mummies, however much of the ritual aspects of the mummification process are similar. Full-body mummification was achieved by these cultures, but not the level of artistic preservation as found on smaller islands. The reason for this seems to be for easier transport of bodies by more nomadic tribes.\n\nThe mummies of the Torres Strait have a considerably higher level of preservation technique as well as creativity compared to those found on Australia. The process began with removal of viscera, after which the bodies were set in a seated position on a platform and either left to dry in the sun or smoked over a fire in order to aid in desiccation. In the case of smoking, some tribes would collect the fat that drained from the body to mix with ocher to create red paint that would then be smeared back on the skin of the mummy. The mummies remained on the platforms, decorated with the clothing and jewelry they wore in life, before being buried.\n\nSome Māori tribes from New Zealand would keep mummified heads as trophies from tribal warfare. They are also known as Mokomokai. In the 19th Century, many of the trophies were acquired by Europeans who found the tattooed skin to be a phenomenal curiosity. Westerners began to offer valuable commodities in exchange for the uniquely tattooed mummified heads. The heads were later put on display in museums, 16 of which being housed across France alone. In 2010, the Rouen City Hall of France returned one of the heads to New Zealand, despite earlier protests by the Culture Ministry of France.\n\nThere is also evidence that some Maori tribes may have practiced full-body mummification, though the practice is not thought to have been widespread. The discussion of Maori mummification has been historically controversial, with some experts in past decades claiming that such mummies have never existed. Contemporary science does now acknowledge the existence of full-body mummification in the culture. There is still controversy, however, as to the nature of the mummification process. Some bodies appear to be spontaneously created by the natural environment, while others exhibit signs of deliberate practices. General modern consensus tends to agree that there could be a mixture of both types of mummification, similar to that of the ancient Egyptian mummies.\n\nThe South American continent contains some of the oldest mummies in the world, both deliberate and accidental. The bodies were preserved by the best agent for mummification: the environment. The Pacific coastal desert in Peru and Chile is one of the driest areas in the world and the dryness facilitated mummification. Rather than developing elaborate processes such as later-dynasty ancient Egyptians, the early South Americans often left their dead in naturally dry or frozen areas, though some did perform surgical preparation when mummification was intentional. Some of the reasons for intentional mummification in South America include memorialization, immortalization, and religious offerings. A large number of mummified bodies have been found in pre-Columbian cemeteries scattered around Peru. The bodies had often been wrapped for burial in finely-woven textiles.\n\nThe Chinchorro mummies are the oldest intentionally prepared mummified bodies ever found. Beginning in 5th millennium BC and continuing for an estimated 3,500 years, all human burials within the Chinchorro culture were prepared for mummification. The bodies were carefully prepared, beginning with removal of the internal organs and skin, before being left in the hot, dry climate of the Atacama Desert, which aided in desiccation. A large number of Chinchorro mummies were also prepared by skilled artisans to be preserved in a more artistic fashion, though the purpose of this practice is widely debated.\n\nSeveral naturally-preserved, unintentional mummies dating from the Incan period (1438–1532 AD) have been found in the colder regions of Argentina, Chile, and Peru. These are collectively known as \"ice mummies\". The first Incan ice mummy was discovered in 1954 atop El Plomo Peak in Chile, after an eruption of the nearby volcano Sabancaya melted away ice that covered the body. The Mummy of El Plomo was a male child who was presumed to be wealthy due to his well-fed bodily characteristics. He was considered to be the most well-preserved ice mummy in the world until the discovery of Mummy Juanita in 1995.\n\nMummy Juanita was discovered near the summit of Ampato in the Peruvian section of the Andes mountains by archaeologist Johan Reinhard. Her body had been so thoroughly frozen that it had not been desiccated; much of her skin, muscle tissue, and internal organs retained their original structure. She is believed to be a ritual sacrifice, due to the close proximity of her body to the Incan capital of Cusco, as well as the fact she was wearing highly intricate clothing to indicate her special social status. Several Incan ceremonial artifacts and temporary shelters uncovered in the surrounding area seem to support this theory.\n\nMore evidence that the Inca left sacrificial victims to die in the elements, and later be unintentionally preserved, came in 1999 with the discovery of the Llullaillaco mummies on the border of Argentina and Chile. The three mummies are children, two girls and one boy, who are thought to be sacrifices associated with the ancient ritual of \"qhapaq hucha\". Recent biochemical analysis of the mummies has revealed that the victims had consumed increasing quantities of alcohol and coca, possibly in the form of chicha, in the months leading up to sacrifice. The dominant theory for the drugging reasons that, alongside ritual uses, the substances probably made the children more docile. Chewed coca leaves found inside the eldest child's mouth upon her discovery in 1999 supports this theory.\n\nInca emperors. The bodies of Inca emperors and wives were mummified after death. In 1533, the Spanish conquistadors of the Inca Empire viewed the mummies in the Inca capital of Cuzco. The mummies were displayed, often in lifelike positions, in the palaces of the deceased emperors and had a retinue of servants to care for them. The Spanish were impressed with the quality of the mummification which involved removal of the organs, embalming, and freeze-drying.\n\nThe population revered the mummies of the Inca emperors. This reverence seemed idolatry to the Roman Catholic Spanish and in 1550 they confiscated the mummies. The mummies were taken to Lima where they were displayed in the San Andres Hospital. The mummies deteriorated in the humid climate of Lima and eventually they were either buried or destroyed by the Spanish.\n\nAn attempt to find the mummies of the Inca emperors beneath the San Andres hospital in 2001 was unsuccessful. The archaeologists found a crypt, but it was empty. Possibly the mummies had been removed when the building was repaired after an earthquake.\n\nMonks whose bodies remain incorrupt without any traces of deliberate mummification are venerated by some Buddhists who believe they successfully were able to mortify their flesh to death. Self-mummification was practiced until the late 1800s in Japan and has been outlawed since the early 1900s.\n\nMany Mahayana Buddhist monks were reported to know their time of death and left their last testaments and their students accordingly buried them sitting in lotus position, put into a vessel with drying agents (such as wood, paper, or lime) and surrounded by bricks, to be exhumed later, usually after three years. The preserved bodies would then be decorated with paint and adorned with gold.\n\nBodies purported to be those of self-mummified monks are exhibited in several Japanese shrines, and it has been claimed that the monks, prior to their death, stuck to a sparse diet made up of salt, nuts, seeds, roots, pine bark, and \"urushi\" tea.\n\nIn the 1830s, Jeremy Bentham, the founder of utilitarianism, left instructions to be followed upon his death which led to the creation of a sort of modern-day mummy. He asked that his body be displayed to illustrate how the \"horror at dissection originates in ignorance\"; once so displayed and lectured about, he asked that his body parts be preserved, including his skeleton (minus his skull, which despite being mis-preserved, was displayed beneath his feet until theft required it to be stored elsewhere), which were to be dressed in the clothes he usually wore and \"seated in a Chair usually occupied by me when living in the attitude in which I am sitting when engaged in thought\". His body, outfitted with a wax head created because of problems preparing it as Bentham requested, is on open display in the University College London.\n\nDuring the early 20th century, the Russian movement of Cosmism, as represented by Nikolai Fyodorovich Fyodorov, envisioned scientific resurrection of dead people. The idea was so popular that, after Vladimir Lenin's death, Leonid Krasin and Alexander Bogdanov suggested to cryonically preserve his body and brain in order to revive him in the future. Necessary equipment was purchased abroad, but for a variety of reasons the plan was not realized. Instead his body was embalmed and placed on permanent exhibition in the Lenin Mausoleum in Moscow, where it is displayed to this day. The mausoleum itself was modeled by Alexey Shchusev on the Pyramid of Djoser and the Tomb of Cyrus.\n\nIn late 19th-century Venezuela, a German-born doctor named Gottfried Knoche conducted experiments in mummification at his laboratory in the forest near La Guaira. He developed an embalming fluid (based on an aluminum chloride compound) that mummified corpses without having to remove the internal organs. The formula for his fluid was never revealed and has not been discovered. Most of the several dozen mummies created with the fluid (including himself and his immediate family) have been lost or were severely damaged by vandals and looters.\n\nIn 1975, an esoteric organization by the name of Summum introduced \"Modern Mummification\", a service that utilizes modern techniques along with aspects of ancient methods of mummification. The first person to formally undergo Summum's process of modern mummification was the founder of Summum, Summum Bonum Amen Ra, who died in January 2008. Summum is currently considered to be the only \"commercial mummification business\" in the world.\n\nIn 2010, a team led by forensic archaeologist Stephen Buckley mummified Alan Billis using techniques based on 19 years of research of 18th-dynasty Egyptian mummification. The process was filmed for television, for the documentary \"Mummifying Alan: Egypt's Last Secret\". Billis made the decision to allow his body to be mummified after being diagnosed with terminal cancer in 2009. His body currently resides at London's Gordon Museum.\n\nPlastination is a technique used in anatomy to conserve bodies or body parts. The water and fat are replaced by certain plastics, yielding specimens that can be touched, do not smell or decay, and even retain most microscopic properties of the original sample.\n\nThe technique was invented by Gunther von Hagens when working at the anatomical institute of the Heidelberg University in 1978. Von Hagens has patented the technique in several countries and is heavily involved in its promotion, especially as the creator and director of the Body Worlds traveling exhibitions, exhibiting plastinated human bodies internationally. He also founded and directs the Institute for Plastination in Heidelberg.\n\nMore than 40 institutions worldwide have facilities for plastination, mainly for medical research and study, and most affiliated to the International Society for Plastination.\n\nIn the Middle Ages, based on a mistranslation from the Arabic term for bitumen, it was thought that mummies possessed healing properties. As a result, it became common practice to grind Egyptian mummies into a powder to be sold and used as medicine. When actual mummies became unavailable, the sun-desiccated corpses of criminals, slaves and suicidal people were substituted by mendacious merchants. The practice developed into a wide-scale business that flourished until the late 16th century. Two centuries ago, mummies were still believed to have medicinal properties to stop bleeding, and were sold as pharmaceuticals in powdered form as in mellified man. Artists also made use of Egyptian mummies; a brownish pigment known as mummy brown, based on \"mummia\" (sometimes called alternatively \"caput mortuum\", Latin for \"death's head\"), which was originally obtained by grinding human and animal Egyptian mummies. It was most popular in the 17th century, but was discontinued in the early 19th century when its composition became generally known to artists who replaced the said pigment by a totally different blend -but keeping the original name, mummia or mummy brown-yielding a similar tint and based on ground minerals (oxides and fired earths) and or blends of powdered gums and oleoresins (such as myrrh and frankincense) as well as ground bitumen. These blends appeared on the market as forgeries of powdered mummy pigment but were ultimately considered as acceptable replacements, once antique mummies were no longer permitted to be destroyed. Many thousands of mummified cats were also sent from Egypt to England to be processed for use in fertilizer.\n\nDuring the 19th century, following the discovery of the first tombs and artifacts in Egypt, Egyptology was a huge fad in Europe, especially in Victorian England. European aristocrats would occasionally entertain themselves by purchasing mummies, having them unwrapped, and holding observation sessions. These sessions destroyed hundreds of mummies, because the exposure to the air caused them to disintegrate.\n\nThe use of mummies as fuel for locomotives was documented by Mark Twain (likely as a joke or humor), but the truth of the story remains debatable. During the American Civil War, mummy-wrapping linens were said to have been used to manufacture paper. Evidence for the reality of these claims is still equivocal.\n\nBibliography\n\nBooks\n\nOnline\n\nVideo\n"}
{"id": "2025792", "url": "https://en.wikipedia.org/wiki?curid=2025792", "title": "Music education", "text": "Music education\n\nMusic education is a field of study associated with the teaching and learning of music. It touches on all learning domains, including the psychomotor domain (the development of skills), the cognitive domain (the acquisition of knowledge), and, in particular and significant ways, the affective domain (the learner's willingness to receive, internalize, and share what is learned), including music appreciation and sensitivity. Music training from preschool through post-secondary education is common in most nations because involvement with music is considered a fundamental component of human culture and behavior. Cultures from around the world have different approaches to music education, largely due to the varying histories and politics. Studies show that teaching music from other cultures can help students perceive unfamiliar sounds more comfortably, and they also show that musical preference is related to the language spoken by the listener and the other sounds they are exposed to within their own culture.\n\nDuring the 20th century, many distinctive approaches were developed or further refined for the teaching of music, some of which have had widespread impact. The Dalcroze method (eurhythmics) was developed in the early 20th century by Swiss musician and educator Émile Jaques-Dalcroze. The Kodály Method emphasizes the benefits of physical instruction and response to music. The Orff Schulwerk \"approach\" to music education leads students to develop their music abilities in a way that parallels the development of western music.\n\nThe Suzuki method creates the same environment for learning music that a person has for learning their native language. Gordon Music Learning Theory provides the music teacher with a method for teaching musicianship through \"audiation\", Gordon's term for hearing music in the mind \"with understanding\". Conversational Solfège immerses students in the musical literature of their own culture, in this case American. The Carabo-Cone Method involves using props, costumes, and toys for children to learn basic musical concepts of staff, note duration, and the piano keyboard. The concrete environment of the specially planned classroom allows the child to learn the fundamentals of music by exploring through touch. Popular music pedagogy is the systematic teaching and learning of rock music and other forms of popular music both inside and outside formal classroom settings. Some have suggested that certain musical activities can help to improve breath, body and voice control of a child.\n\nThe MMCP (Manhattanville Music Curriculum Project) aims to shape attitudes, helping students see music not as static content to be mastered, but as personal, current, and evolving. \n\nIn primary schools in European countries, children often learn to play instruments such as keyboards or recorders, sing in small choirs, and learn about the elements of music and history of music. In countries such as India, the harmonium is used in schools, but instruments like keyboards and violin are also common. Students are normally taught basics of Indian Raga music. In primary and secondary schools, students may often have the opportunity to perform in some type of musical ensemble, such as a choir, orchestra, or school band: concert band, marching band, or jazz band. In some secondary schools, additional music classes may also be available. In junior high school or its equivalent, music usually continues to be a required part of the curriculum.\n\nAt the university level, students in most arts and humanities programs receive academic credit for music courses such as music history, typically of Western art music, or music appreciation, which focuses on listening and learning about different musical styles. In addition, most North American and European universities offer music ensembles – such as choir, concert band, marching band, or orchestra – that are open to students from various fields of study. Most universities also offer degree programs in music education, certifying students as primary and secondary music educators. Advanced degrees such as the D.M.A. or the Ph.D can lead to university employment. These degrees are awarded upon completion of music theory, music history, technique classes, private instruction with a specific instrument, ensemble participation, and in depth observations of experienced educators. Music education departments in North American and European universities also support interdisciplinary research in such areas as music psychology, music education historiography, educational ethnomusicology, sociomusicology, and philosophy of education.\n\nThe study of western art music is increasingly common in music education outside of North America and Europe, including Asian nations such as South Korea, Japan, and China. At the same time, Western universities and colleges are widening their curriculum to include music of outside the Western art music canon, including music of West Africa, of Indonesia (e.g. Gamelan music), Mexico (e.g., mariachi music), Zimbabwe (marimba music), as well as popular music.\n\nMusic education also takes place in individualized, lifelong learning, and in community contexts. Both amateur and professional musicians typically take music lessons, short private sessions with an individual teacher.\n\nWhile instructional strategies are determined by the music teacher and the music curriculum in his or her area, many teachers rely heavily on one of many instructional methodologies that emerged in recent generations and developed rapidly during the latter half of the 20th Century.\n\nThe Dalcroze method was developed in the early 20th century by Swiss musician and educator Émile Jaques-Dalcroze. The method is divided into three fundamental concepts − the use of solfège, improvisation, and eurhythmics. Sometimes referred to as \"rhythmic gymnastics,\" eurhythmics teaches concepts of rhythm, structure, and musical expression using movement, and is the concept for which Dalcroze is best known. It focuses on allowing the student to gain physical awareness and experience of music through training that engages all of the senses, particularly kinesthetic. According to the Dalcroze method, music is the fundamental language of the human brain and therefore deeply connected to who we are. American proponents of the Dalcroze method include Ruth Alperson, Ann Farber, Herb Henke, Virginia Mead, Lisa Parker, Martha Sanchez, and Julia Schnebly-Black. Many active teachers of Dalcroze method were trained by Dr. Hilda Schuster who was one of the students of Dalcroze.\n\nZoltán Kodály (1882–1967) was a prominent Hungarian music educator and composer who stressed the benefits of physical instruction and response to music. Although not really an educational method, his teachings reside within a fun, educational framework built on a solid grasp of basic music theory and music notation in various verbal and written forms. Kodály's primary goal was to instill a lifelong love of music in his students and felt that it was the duty of the child's school to provide this vital element of education. Some of Kodály's trademark teaching methods include the use of solfège hand signs, musical shorthand notation (stick notation), and rhythm solmization (verbalization). Most countries have used their own folk music traditions to construct their own instruction sequence, but the United States primarily uses the Hungarian sequence. The work of Denise Bacon, Katinka S. Daniel, John Feierabend, Jean Sinor, Jill Trinka, and others brought Kodaly's ideas to the forefront of music education in the United States.\n\nCarl Orff was a prominent German composer. Orff Schulwerk is considered an \"approach\" to music education. It begins with a student's innate abilities to engage in rudimentary forms of music, using basic rhythms and melodies. Orff considers the whole body a percussive instrument and students are led to develop their music abilities in a way that parallels the development of western music. The approach fosters student self-discovery, encourages improvisation, and discourages adult pressures and mechanical drill. Carl Orff developed a special group of instruments, including modifications of the glockenspiel, xylophone, metallophone, drum, and other percussion instruments to accommodate the requirements of the Schulwerk courses. Experts in shaping an American-style Orff approach include Jane Frazee, Arvida Steen, and Judith Thomas.\n\nThe Suzuki method was developed by Shinichi Suzuki in Japan shortly after World War II, and uses music education to enrich the lives and moral character of its students. The movement rests on the double premise that \"all children can be well educated\" in music, and that learning to play music at a high level also involves learning certain character traits or virtues which make a person's soul more beautiful. The primary method for achieving this is centered around creating the same environment for learning music that a person has for learning their native language. This 'ideal' environment includes love, high-quality examples, praise, rote training and repetition, and a time-table set by the student's developmental readiness for learning a particular technique. While the Suzuki Method is quite popular internationally, within Japan its influence is less significant than the Yamaha Method, founded by Genichi Kawakami in association with the Yamaha Music Foundation.\n\nIn addition to the four major international methods described above, other approaches have been influential. Lesser-known methods are described below:\n\nEdwin Gordon's Music Learning Theory is based on an extensive body of research and field testing by Edwin E. Gordon and others in the larger field of Music Learning Theory. It provides music teachers with a comprehensive framework for teaching musicianship through audiation, Gordon's term for hearing music in the mind with understanding and comprehension when the sound is not physically present. The sequence of instructions is Discrimination Learning and Inference Learning. Discrimination Learning, the ability to determine whether two elements are the same or not the same using aural/oral, verbal association, partial synthesis, symbolic association, and composite synthesis. Inference Learning, students take an active role in their own education and learn to identify, create, and improvise unfamiliar patterns. The skills and content sequences within the Audiation theory help music teachers establish sequential curricular objectives in accord with their own teaching styles and beliefs. There also is a Learning Theory for Newborns and Young Children in which the Types and Stages of Preparatory Audiation are outlined.\n\nThe growth of cultural diversity within school-age populations prompted music educators from the 1960s onward to diversify the music curriculum, and to work with ethnomusicologists and artist-musicians to establish instructional practices rooted in musical traditions. 'World music pedagogy' was coined by Patricia Shehan Campbell to describe world music content and practice in elementary and secondary school music programs. Pioneers of the movement, especially Barbara Reeder Lundquist, William M. Anderson, and Will Schmid, influenced a second generation of music educators (including J. Bryan Burton, Mary Goetze, Ellen McCullough-Brabson, and Mary Shamrock) to design and deliver curricular models to music teachers of various levels and specializations. The pedagogy advocates the use of human resources, i.e., \"culture-bearers,\" as well as deep and continued listening to archived resources such as those of Smithsonian Folkways Recordings.\n\nInfluenced by both the Kodály method and Gordon's Music Learning Theory, Conversational Solfège was developed by Dr. John M. Feierabend, former chair of music education at the Hartt School, University of Hartford. The program begins by immersing students in the musical literature of their own culture, in this case American. Music is seen as separate from, and more fundamental than, notation. In twelve learning stages, students move from hearing and singing music to decoding and then creating music using spoken syllables and then standard written notation. Rather than implementing the Kodály method directly, this method follows Kodály's original instructions and builds on America's own folk songs instead of on Hungarian folk songs.\n\nThis early-childhood approach, sometimes referred to as the Sensory-Motor Approach to Music, was developed by the violinist Madeleine Carabo-Cone. This approach involves using props, costumes, and toys for children to learn basic musical concepts of staff, note duration, and the piano keyboard. The concrete environment of the specially planned classroom allows the child to learn the fundamentals of music by exploring through touch.\n\n'Popular music pedagogy' — alternatively called rock music pedagogy, modern band, popular music education, or rock music education — is a recent development in music education consisting of the systematic teaching and learning of rock music and other forms of popular music both inside and outside formal classroom settings. Popular music pedagogy tends to emphasize group improvisation, and is more commonly associated with community music activities than fully institutionalized school music ensembles.\n\nThe Manhattanville Music Curriculum Project was developed in 1965 as a response to declining student interest in school music. This creative approach aims to shape attitudes, helping students see music not as static content to be mastered, but as personal, current, and evolving. Rather than imparting factual knowledge, this method centers around the student, who learns through investigation, experimentation, and discovery. The teacher gives a group of students a specific problem to solve together and allows freedom to create, perform, improvise, conduct, research, and investigate different facets of music in a spiral curriculum. MMCP is viewed as the forerunner to projects in creative music composition and improvisation activities in schools.\n\nAfter the preaching of Reverend Thomas Symmes, the first singing school was created in 1717 in Boston for the purposes of improving singing and music reading in the church. These singing schools gradually spread throughout the colonies. Music education continued to flourish with the creation of the Academy of Music in Boston. Reverend John Tufts published \"An Introduction to the Singing of Psalm Tunes Using Non-Traditional Notation\" which is regarded as the first music textbook in the colonies. Between 1700 and 1820, more than 375 tune books would be published by such authors as Samuel Holyoke, Francis Hopkinson, William Billings, and Oliver Holden.\n\nMusic began to spread as a curricular subject into other school districts. Soon after music expanded to all grade levels and the teaching of music reading was improved until the music curriculum grew to include several activities in addition to music reading. By the end of 1864 public school music had spread throughout the country.\n\nIn 1832, Lowell Mason and George Webb formed the Boston Academy of Music with the purposes of teaching singing and theory as well as methods of teaching music. Mason published his \"Manuel of Instruction\" in 1834 which was based upon the music education works of Pestalozzian System of Education founded by Swiss educator Johann Heinrich Pestalozzi. This handbook gradually became used by many singing school teachers. From 1837–1838, the Boston School Committee allowed Lowell Mason to teach music in the Hawes School as a demonstration. This is regarded as the first time music education was introduced to public schools in the United States. In 1838 the Boston School Committee approved the inclusion of music in the curriculum and Lowell Mason became the first recognized supervisor of elementary music. In later years Luther Whiting Mason became the Supervisor of Music in Boston and spread music education into all levels of public education (grammar, primary, and high school). During the middle of the 19th century, Boston became the model to which many other cities across the United States included and shaped their public school music education programs. Music methodology for teachers as a course was first introduced in the Normal School in Potsdam. The concept of classroom teachers in a school that taught music under the direction of a music supervisor was the standard model for public school music education during this century. (See also: \"Music education in the United States\") While women were discouraged from composing in the 19th century, \"later, it was accepted that women would have a role in music education, and they became involved in this field...to such a degree that women dominated music education during the later half of the 19th century and well into the 20th century.\"\n\nIn the United States, teaching colleges with four-year degree programs developed from the Normal Schools and included music. Oberlin Conservatory first offered the Bachelor of Music Education degree. Osbourne G. McCarthy, an American music educator, introduced details for studying music for credit in Chelsea High School. Notable events in the history of music education in the early 20th century also include:\n\nThe following table illustrates some notable developments from this period:\n\nMusic course offerings and even entire degree programs in online music education developed in the first decade of the 21st century at various institutions, and the fields of world music pedagogy and popular music pedagogy have also seen notable expansion.\n\nIn the late 20th and early 21st centuries, social aspects of teaching and learning music came to the fore. This emerged as praxial music education, critical theory, and feminist theory. Of importance are the colloquia and journals of the MayDay Group, \"an international think tank of music educators that aims to identify, critique, and change taken-for-granted patterns of professional activity, polemical approaches to method and philosophy, and educational politics and public pressures that threaten effective practice and critical communication in music education.\" With a new focus on social aspects of music education, scholars have analyzed critical aspects such as music and race, gender, class, institutional belonging, and sustainability.\n\nInstitutional music education was started in colonial India by Rabindranath Tagore after he founded the Visva-Bharati University. At present, most universities have a faculty of music with some universities specially dedicated to fine arts such as Indira Kala Sangeet University, Swathi Thirunal College of Music or Rabindra Bharati University.Indian classical music is based on the gurushyshyaparampara system. The teacher, known as Guru, transmit the musical knowledge to the student, or shyshya. This is still the main system used in India to transmit musical knowledge. Although European art music became popularized in schools throughout much of the world during the twentieth century (East Asia, Latin America, Oceania, Africa), India remains one of the few highly populated nations in which non-European indigenous music traditions have consistently received relatively greater emphasis. That said, there is certainly much western influence in the popular music associated with Bollywood film scores.\n\nThe South African Department of Education and the ILAM Music Heritage Project SA teach African music using western musical framework. ILAM's \"Listen and Learn\" for students 11–14 is \"unique\" in teaching curriculum requirements for western music using recordings of traditional African music.\n\nFrom the time that Africa was colonized up to 1994, indigenous music and arts being taught in schools was a rare occurrence. The African National Congress (ANC) attempted to repair the neglect of indigenous knowledge and the overwhelming emphasis on written musical literacy in schools. It is not well known that the learning of indigenous music actually has a philosophy and teaching procedure that is different from western “formal” training. It involves the whole community because indigenous songs are about the history of its people. After the colonization of Africa, music became more centered on Christian beliefs and European folk songs, rather than the more improvised and fluid indigenous music. Before the major changes education went through from 1994 to 2004, during the first decade of the democratic government, teachers were trained as classroom teachers and told that they would have to incorporate music into other subject areas. The few colleges with teaching programs that included instrumental programs held a greater emphasis on music theory, history of western music, western music notation, and less on making music. Up until 1999, most college syllabi did not include training in indigenous South African Music.\n\nIn African cultures music is seen as a community experience and is used for social and religious occasions. As soon as children show some sign of being able to handle music or a musical instrument they are allowed to participate with the adults of the community in musical events. Traditional songs are more important to many people because they are stories about the histories of the indigenous peoples.\n\nAmong the Aztecs, Mayans, and Incas, music was used in ceremonies and rituals to teach the history of their civilizations and was also used for worship. The Aztec people were mainly educated by their priests. Music remained an important way to teach religion and history and was taught by priests for many centuries. When Spain and Portugal colonized parts of South America, music started to be influenced by European ideas and qualities. Several priests of European descent, such as Antonio Sepp, taught European systems of music notation and theory based on their knowledge of playing instruments throughout the 1700-1800s. Since music was taught to the general public by rote, very few knew how to read music other than those who played instruments until the nineteenth and twentieth centuries. The development of music in South America mainly followed that of European development. Choirs were formed to sing masses, chants, psalms, but secular music also became more prevalent in the seventeenth and eighteenth centuries and beyond.\n\nMusic education in Latin America today has large emphasis on folk music, masses, and orchestral music. Many classrooms teach their choirs to sing in their native language as well as in English. Several Latin American Schools, specifically in Puerto Rico and Haiti, believe music to be an important subject and are working on expanding their programs. Outside of school, many communities form their own musical groups and organizations. Community performances are very popular with the local audiences. There are a few well-known Latin American choral groups, such as \"El Coro de Madrigalistas\" from Mexico. This famous choral group tours around Mexico, showing students around the country what a professional choral ensemble sounds like.\n\nThe music, languages, and sounds we are exposed to within our own cultures determine our tastes in music and affect the way we perceive the music of other cultures. Many studies have shown distinct differences in the preferences and abilities of musicians from around the world. One study attempted to view the distinctions between the musical preferences of English and Japanese speakers, providing both groups of people with the same series of tones and rhythms. The same type of study was done for English and French speakers. Both studies suggested that the language spoken by the listener determined which groupings of tones and rhythms were more appealing, based on the inflections and natural rhythm groupings of their language.\n\nAnother study had Europeans and Africans try to tap along with certain rhythms. European rhythms are regular and built on simple ratios, while African rhythms are typically based on irregular ratios. While both groups of people could perform the rhythms with European qualities, the European group struggled with the African rhythms. This has to do with the ubiquity of complex polyrhythm in African culture and their familiarity with this type of sound.\n\nWhile each culture has its own musical qualities and appeals, incorporating cross-cultural curricula in our music classrooms can help teach students how to better perceive music from other cultures. Studies show that learning to sing folk songs or popular music of other cultures is an effective way to understand a culture as opposed to merely learning about it. If music classrooms discuss the musical qualities and incorporate styles from other cultures, such as the Brazilian roots of the Bossa Nova, the Afro-Cuban clave, and African drumming, it will expose students to new sounds and teach them how to compare their cultures’ music to the different music and start to make them more comfortable with exploring sounds.\n\nAchievement standards are curricular statements used to guide educators in determining objectives for their teaching. Use of standards became a common practice in many nations during the 20th century. For much of its existence, the curriculum for music education in the United States was determined locally or by individual teachers. In recent decades there has been a significant move toward adoption of regional and/or national standards. , created nine voluntary content standards, called the \"National Standards for Music Education\". These standards call for:\n\nMany states and school districts have adopted their own standards for music education.\n\nSome schools and organizations promote integration of arts classes, such as music, with other subjects, such as math, science, or English, believing that integrating the different curricula will help each subject to build off of one another, enhancing the overall quality of education.\n\nOne example is the Kennedy Center's \"Changing Education Through the Arts\" program. CETA defines arts integration as finding a natural connection(s) between one or more art forms (dance, drama/theater, music, visual arts, storytelling, puppetry, and/or creative writing) and one or more other curricular areas (science, social studies, English language arts, mathematics, and others) in order to teach and assess objectives in both the art form and the other subject area. This allows a simultaneous focus on creating, performing, and/or responding to the arts while still addressing content in other subject areas.\n\nThe European Union Lifelong Learning Programme 2007–2013 has funded three projects that use music to support language learning. Lullabies of Europe (for pre-school and early learners), FolkDC (for primary), and the recent PopuLLar (for secondary). In addition, the ARTinED project is also using music for all subject areas.\n\nA number of researchers and music education advocates have argued that studying music enhances academic achievement, such as William Earhart, former president of the Music Educators National Conference, who claimed that \"Music enhances knowledge in the areas of mathematics, science, geography, history, foreign language, physical education, and vocational training.\" Researchers at the University of Wisconsin suggested that students with piano or keyboard experience performed 34% higher on tests that measure spatial-temporal lobe activity, which is the part of the brain that is used when doing mathematics, science, and engineering.\n\nAn experiment by Wanda T. Wallace setting text to melody suggested that some music may aid in text recall. She created a three verse song with a non-repetitive melody; each verse with different music. A second experiment created a three verse song with a repetitive melody; each verse had exactly the same music. A third experiment studied text recall without music. She found the repetitive music produced the highest amount of text recall, suggesting music can serve as a mnemonic device.\n\nSmith (1985) studied background music with word lists. One experiment involved memorizing a word list with background music; participants recalled the words 48 hours later. Another experiment involved memorizing a word list with no background music; participants also recalled the words 48 hours later. Participants who memorized word lists with background music recalled more words demonstrating music provides contextual cues.\n\nCiting studies that support music education's involvement in intellectual development and academic achievement, the United States Congress passed a resolution declaring that: \"Music education enhances intellectual development and enriches the academic environment for children of all ages; and Music educators greatly contribute to the artistic, intellectual and social development of American children and play a key role in helping children to succeed in school.\"\n\nBobbett (1990) suggests that most public school music programs have not changed since their inception at the turn of the last century. \"…the educational climate is not conducive to their continuance as historically conceived and the social needs and habits of people require a completely different kind of band program.\" A 2011 study conducted by Kathleen M. Kerstetter for the Journal of Band Research found that increased non-musical graduation requirements, block scheduling, increased number of non-traditional programs such as magnet schools, and the testing emphases created by the No Child Left Behind Act are only some of the concerns facing music educators. Both teachers and students are under increased time restrictions\"\n\nDr. Patricia Powers states, \"It is not unusual to see program cuts in the area of music and arts when economic issues surface. It is indeed unfortunate to lose support in this area especially since music and the art programs contribute to society in many positive ways.\" Comprehensive music education programs average $187 per pupil, according to a 2011 study funded by the national Association of Music Merchants (NAMM) Foundation The Texas Commission on Drugs and Alcohol Abuse Report noted that students who participated in band or orchestra reported the lowest lifetime and current use of all substances including alcohol, tobacco, and illicit drugs.\n\nStudies have shown that music education can be used to enhance cognitive achievement in students. In the United States an estimated 30% of students struggle with reading, while 17% are reported as having a specific learning disability linked to reading. Using intensive music curriculum as an intervention paired alongside regular classroom activities, research shows that students involved with the music curriculum show increases in reading comprehension, word knowledge, vocabulary recall, and word decoding. When a student is singing a melody with text, they are using multiple areas of their brain to multitask. Music effects language development, increases IQ, spatial-temporal skills, and improves test scores. Music education has also shown to improve the skills of dyslexic children in similar areas as mentioned earlier by focusing on visual auditory and fine motor skills as strategies to combat their disability. Since research in this area is sparse, we cannot convincingly conclude these findings to be true, however the results from research done do show a positive impact on both students with learning difficulties and those who are not diagnosed. Further research will need to be done, but the positive engaging way of bringing music into the classroom cannot be forgotten, and the students generally show a positive reaction to this form of instruction.\n\nMusic education has also been noted to have the ability to increase someones overall IQ, especially in children during peak development years. Spacial ability, verbal memory, reading and mathematic ability are seen to be increased alongside music education (primarily through the learning of an instrument). Researchers also note that a correlation between general attendance and IQ increases is evident, and due to students involvement in music education, general attendance rates increase along with their IQ.\n\nFine motor skills, social behaviours, and emotional well being can also be increased through music and music education. The learning of an instrument increases fine motor skills in students with physical disabilities. Emotional well being can be increased as students find meaning in songs and connect them to their everyday life. Through social interactions of playing in groups like jazz and concert bands, students learn to socialize and this can be linked to emotional and mental well being.\n\nIn some communities – and even entire national education systems – music is provided little support as an academic subject area, and music teachers feel that they must actively seek greater public endorsement for music education as a legitimate subject of study. This perceived need to change public opinion has resulted in the development of a variety of approaches commonly called \"music advocacy\". Music advocacy comes in many forms, some of which are based upon legitimate scholarly arguments and scientific findings, while other examples controversially rely on emotion, anecdotes, or unconvincing data.\n\nRecent high-profile music advocacy projects include the \"Mozart Effect\", the National Anthem Project, and the movement in World Music Pedagogy (also known as Cultural Diversity in Music Education) which seeks out means of equitable pedagogy across students regardless of their race, ethnicity, or socioeconomic circumstance. The Mozart effect is particularly controversial as while the initial study suggested listening to Mozart positively impacts spatial-temporal reasoning, later studies either failed to replicate the results, suggested no effect on IQ or spatial ability, or suggested the music of Mozart could be substituted for any music children enjoy in a term called \"enjoyment arousal.\" Another study suggested that even if listening to Mozart may temporarily enhance a student's spatial-temporal abilities, learning to play an instrument is much more likely to improve student performance and achievement. Educators similarly criticized the National Anthem Project not only for promoting the educational use of music as a tool for non-musical goals, but also for its links to nationalism and militarism.\n\nContemporary music scholars assert that effective music advocacy uses empirically sound arguments that transcend political motivations and personal agendas. Music education philosophers such as Bennett Reimer, Estelle Jorgensen, David J. Elliott, John Paynter, and Keith Swanwick support this view, yet many music teachers and music organizations and schools do not apply this line of reasoning into their music advocacy arguments. Researchers such as Ellen Winner conclude that arts advocates have made bogus claims to the detriment of defending the study of music, her research debunking claims that music education improves math, for example. Researchers Glenn Schellenberg and Eugenia Costa-Giomi also criticize advocates incorrectly associating correlation with causation, Giomi pointing out that while there is a \"strong relationship between music participation and academic achievement, the \"causal\" nature of the relationship is questionable.\" Philosophers David Elliott and Marissa Silverman suggest that more effective advocacy involves shying away from \"dumbing down\" values and aims through slogans and misleading data, energy being better focused into engaging potential supporters in active music-making and musical-affective experiences, these actions recognizing that music and music-making are inherent to human culture and behavior, distinguishing humans from other species.\n\nWhile music critics argued in the 1880s that \"...women [composers] lacked the innate creativity to compose good music\" due to \"biological predisposition\", later, it was accepted that women would have a role in music education, and they became involved in this field \"...to such a degree that women dominated music education during the later half of the 19th century and well into the 20th century.\"\"Traditional accounts of the history of music education [in the US] have often neglected the contributions of women, because these texts have emphasized bands and the top leaders in hierarchical music organizations.\" When looking beyond these bandleaders and top leaders, women had many music education roles in the \"...home, community, churches, public schools, and teacher-training institutions\" and \"...as writers, patrons, and through their volunteer work in organizations.\" \n\nDespite the limitations imposed on women's roles in music education in the 19th century, women were accepted as kindergarten teachers, because this was deemed to be a \"private sphere\". Women also taught music privately, in girl's schools, Sunday schools, and they trained musicians in school music programs. By the turn of the 20th century, women began to be employed as music supervisors in elementary schools, teachers in normal schools and professors of music in universities. Women also became more active in professional organizations in music education, and women presented papers at conferences. A woman, Frances Clarke (1860-1958) founded the Music Supervisors National Conference in 1907. While a small number of women served as President of the Music Supervisors National Conference (and the following renamed versions of the organization over the next century) in the early 20th century, there were only two female Presidents between 1952 and 1992, which \"[p]ossibly reflects discrimination.\"\n\nAfter 1990, however, leadership roles for women in the organization opened up. From 1990 to 2010, there were five female Presidents of this organization. Women music educators \"outnumber men two-to-one\" in teaching general music, choir, private lessons, and keyboard instruction . More men tend to be hired as for band education, administration and jazz jobs, and more men work in colleges and universities. According to Dr. Sandra Wieland Howe, there is still a \"glass ceiling\" for women in music education careers, as there is \"stigma\" associated with women in leadership positions and \"men outnumber women as administrators.\"\n\n\n\n\n"}
{"id": "16033266", "url": "https://en.wikipedia.org/wiki?curid=16033266", "title": "Old Aramaic language", "text": "Old Aramaic language\n\nOld Aramaic (code: oar) refers to the earliest stage of the Aramaic language, considered to give way to Middle Aramaic by the 3rd century (a conventional date is the rise of the Sasanian Empire in 224 AD).\n\nEmerging as the language of the city-states of the Arameans in the Levant in the Early Iron Age, Old Aramaic was adopted as a \"lingua franca\", and in this role was inherited for official use by the Achaemenid Empire during classical antiquity. After the fall of the Achaemenid Empire, local vernaculars became increasingly prominent, fanning the divergence of an Aramaic dialect continuum and the development of differing written standards.\n\n\"Ancient Aramaic\" refers to the earliest known period of the language, from its origin until it becomes the \"lingua franca\" of the Fertile Crescent and Bahrain. It was the language of the Aramaean city-states of Damascus, Hamath and Arpad.\nThere are inscriptions that evidence the earliest use of the language, dating from the 10th century BCE. The inscriptions are mostly diplomatic documents between Aramaean city-states. The alphabet of Aramaic then seems to be based on the Phoenician alphabet, and there is a unity in the written language. It seems that in time, a more refined alphabet, suited to the needs of the language, began to develop from this in the eastern regions of Aram. The dominance of the Neo-Assyrian Empire under Tiglath-Pileser III over Aram-Damascus in the middle of the 8th century led to the establishment of Aramaic as a lingua franca of the empire, rather than it being eclipsed by Akkadian. Distinctive royal inscriptions at Sam'al have led to some scholars suggesting a distinctive \"Sam'alian\" or \"Ya'udic\" variant of Old Aramaic.\n\nFrom 700 BCE, the language began to spread in all directions but lost much of its homogeneity. Different dialects emerged in Assyria, Babylonia, the Levant and Egypt. However, the Akkadian-influenced Aramaic of Assyria, and then Babylon, started to come to the fore. As described in 2 Kings 18:26, Hezekiah, king of Judah, negotiates with Assyrian ambassadors in Aramaic so that the common people would not understand. Around 600 BCE, Adon, a Canaanite king, used Aramaic to write to the Egyptian Pharaoh.\n\n\"Chaldee\" or \"Chaldean Aramaic\" used to be common terms for the Aramaic of the Neo-Babylonian Empire. It was used to describe Biblical Aramaic, which was, however, written in a later style. It is not to be confused with the modern language Chaldean Neo-Aramaic.\n\nThe first old Aramaic inscription to be found in Europe was the Carpentras stele, published by Rigord in 1704.\n\nAfter 539 BCE, following the Achaemenid conquest of Mesopotamia under Darius I, the Achaemenids adopted the local use of Aramaic. In 1955, Richard Frye questioned the classification of Imperial Aramaic as an \"official language\", noting that no surviving edict expressly and unambiguously accorded that status to any particular language. Frye reclassifies Imperial Aramaic as the \"lingua franca\" of Achaemenid territories, suggesting then that the Achaemenid-era use of Aramaic was more pervasive than generally thought.\n\nImperial Aramaic was highly standardised; its orthography was based more on historical roots than any spoken dialect, and the inevitable influence of Old Persian gave the language a new clarity and robust flexibility. For centuries after the fall of the Achaemenid Empire (in 331 BCE), Imperial Aramaic or a similar dialect would remain an influence on the various native Iranian languages. Aramaic script and, as ideograms, Aramaic vocabulary would survive as the essential characteristics of the Pahlavi scripts.\n\nOne of the largest collections of Imperial Aramaic texts is that of the Persepolis fortification tablets, which number about five hundred. Many of the extant documents witnessing to this form of Aramaic come from Egypt, Elephantine in particular (see: Elephantine papyri). Of them, the best known is the \"Wisdom of Ahiqar\", a book of instructive aphorisms quite similar in style to the biblical Book of Proverbs.\n\nAchaemenid Aramaic is sufficiently uniform that it is often difficult to know where any particular example of the language was written. Only careful examination reveals the occasional loanword from a local language.\n\nA group of thirty Aramaic documents from Bactria has been discovered, and an analysis was published in November 2006. The texts, which were rendered on leather, reflect the use of Aramaic in the fourth century BCE Achaemenid administration of Bactria and Sogdia.\n\nOld Aramaic and Biblical Hebrew both form part of the group of Northwest Semitic languages, and during antiquity, there may still have been substantial mutual intelligibility. In Pesahim, Tractate 87b, Hanina bar Hama said that God sent the exiled Jews to Babylon because \"[the Babylonian] language is akin to the \"Leshon Hakodesh\"\".\n\nThe conquest by Alexander the Great did not destroy the unity of Aramaic language and literature immediately. Aramaic that bears a relatively close resemblance to that of the fifth century BCE can be found right up to the early second century BCE. The Seleucids imposed Koine Greek in the administration of Syria and Mesopotamia from the start of their rule. In the third century BCE, Koine Greek overtook Aramaic as the common language in Egypt and Syria. However, a post-Achaemenid Aramaic continued to flourish from Judea, Assyria, Mesopotamia, through the Syrian Desert and into northern Arabia and Parthia.\n\nBiblical Aramaic is the term for the Aramaic passages interspersed in the Hebrew Bible. These passages make for a small fraction of the entire text (of the order of 1%), and most of it is due to the Aramaic parts of the Book of Daniel:\nBiblical Aramaic is a somewhat hybrid dialect. It is theorized that some Biblical Aramaic material originated in both Babylonia and Judaea before the fall of the Achaemenid dynasty. According to historical criticism, defiant Jewish propaganda shaped the Aramaic Book of Daniel during Seleucid rule. These stories might have existed as oral traditions at their earliest stage. This might be one factor that led to differing collections of Daniel in the Greek Septuagint and the Masoretic Text, which presents a lightly Hebrew-influenced Aramaic.\n\nUnder the category of post-Achaemenid is Hasmonaean Aramaic, the official language of the Hasmonean dynasty of Judaea (142–37 BCE). It influenced the Biblical Aramaic of the Qumran texts, and was the main language of non-biblical theological texts of that community. The major Targums, translations of the Hebrew Bible into Aramaic, were originally composed in Hasmonaean. Hasmonaean also appears in quotations in the Mishnah and Tosefta, although smoothed into its later context. It is written quite differently from Achaemenid Aramaic; there is an emphasis on writing as words are pronounced rather than using etymological forms.\n\nBabylonian Targumic is the later post-Achaemenid dialect found in the Targum Onqelos and Targum Jonathan, the \"official\" targums. The original, Hasmonaean targums had reached Babylon sometime in the 2nd or 3rd century CE. They were then reworked according to the contemporary dialect of Babylon to create the language of the standard targums. This combination formed the basis of Babylonian Jewish literature for centuries to follow.\n\nGalilean Targumic is similar to Babylonian Targumic. It is the mixing of literary Hasmonaean with the dialect of Galilee. The Hasmonaean targums reached Galilee in the 2nd century, and were reworked into this Galilean dialect for local use. The Galilean Targum was not considered an authoritative work by other communities, and documentary evidence shows that its text was amended. From the 11th century onwards, once the Babylonian Targum had become normative, the Galilean version became heavily influenced by it.\n\nBabylonian Documentary Aramaic is a dialect in use from the 3rd century onwards. It is the dialect of Babylonian private documents, and, from the 12th century, all Jewish private documents are in Aramaic. It is based on Hasmonaean with very few changes. This was perhaps because many of the documents in BDA are legal documents, the language in them had to be sensible throughout the Jewish community from the start, and Hasmonaean was the old standard.\n\nThe Nabataean language was the Western Aramaic variety used by the Nabateans of the Negev, including the kingdom of Petra. The kingdom (\"c.\" 200 BCE–106 CE) covered the east bank of the Jordan River, the Sinai Peninsula and northern Arabia. Perhaps because of the importance of the caravan trade, the Nabataeans began to use Aramaic in preference to Ancient North Arabian. The dialect is based on Achaemenid with a little influence from Arabic: \"l\" is often turned into \"n\", and there are a few Arabic loan words. Some Nabataean Aramaic inscriptions exist from the early days of the kingdom, but most are from the first four centuries CE. The language is written in a cursive script that is the precursor to the modern Arabic alphabet. The number of Arabic loan words increases through the centuries, until, in the 4th century, Nabataean merges seamlessly with Arabic.\n\nPalmyrene Aramaic is the dialect that was in use in the city state of Palmyra in the Syrian Desert from 44 BC to 274 CE. It was written in a rounded script, which later gave way to cursive Estrangela. Like Nabataean, Palmyrene was influenced by Arabic, but to a much lesser degree.\n\nThe use of written Aramaic in the Achaemenid bureaucracy also precipitated the adoption of Aramaic-derived scripts to render a number of Middle Iranian languages. Moreover, many common words, including even pronouns, particles, numerals, and auxiliaries, continued to written as Aramaic \"words\" even when writing Middle Iranian languages. In time, in Iranian usage, these Aramaic \"words\" became disassociated from the Aramaic language and came to be understood as \"signs\" (i.e. logograms), much like the sign is read as \"and\" in English and the original Latin \"et\" is now no longer obvious. Under the early third-century BCE Parthian Empire, whose government used Koine Greek but whose native language was Parthian, the Parthian language and the Aramaic-derived writing system used for Parthian both gained prestige. This in turn influenced the adoption of the name \"pahlavi\" (< \"parthawi\", \"of the Parthians\") for their use of Aramaic script with logograms. The Sasanian Empire, which succeeded the Parthian Arsacids in the mid-3rd century CE, subsequently inherited/adopted the Parthian-mediated Aramaic-derived writing system for their own Middle Iranian ethnolect as well. That particular Middle Iranian dialect, Middle Persian, i.e. the language of Persia proper, subsequently also became a prestige language. Following the Muslim conquest of Persia by the Arabs in the seventh-century, the Aramaic-derived writing system was replaced by the Arabic script in all but Zoroastrian usage, which continued to use the name \"pahlavi\" for the Aramaic-derived writing system and went on to create the bulk of all Middle Iranian literature in that writing system.\n\nThe dialects mentioned in the last section were all descended from Achaemenid Imperial Aramaic. However, the diverse regional dialects of Late Ancient Aramaic continued alongside them, often as simple, spoken languages. Early evidence for these spoken dialects is known only through their influence on words and names in a more standard dialect. However, the regional dialects became written languages in the 2nd century BCE and reflect a stream of Aramaic that is not dependent on Imperial Aramaic. They show a clear division between the regions of Mesopotamia, Babylon and the east, and Judah, Syria, and the west.\n\nIn the East, the dialects of Palmyrene and Arsacid Aramaic merged with the regional languages to create languages with a foot in Imperial and a foot in regional Aramaic. The written form of Mandaic, the language of the Mandaean religion, was descended from the Arsacid chancery script.\n\nIn the kingdom of Osroene, centred on Edessa and founded in 132 BCE, the regional dialect became the official language: Old Syriac. On the upper reaches of the Tigris, East Mesopotamian Aramaic flourished, with evidence from Hatra, Assur and the Tur Abdin. Tatian, the author of the gospel harmony known as the \"Diatessaron\", came from the Seleucid Empire and perhaps wrote his work (172 CE) in East Mesopotamian rather than Syriac or Greek. In Babylonia, the regional dialect was used by the Jewish community, Jewish Old Babylonian (\"c.\" 70 CE). The everyday language increasingly came under the influence of Biblical Aramaic and Babylonian Targumic.\n\nThe western regional dialects of Aramaic followed a similar course to those of the east. They are quite distinct from the eastern dialects and Imperial Aramaic. Aramaic came to coexist with Canaanite dialects, eventually completely displacing Phoenician in the 1st century BC and Hebrew around the turn of the 4th century CE.\n\nThe form of Late Old Western Aramaic used by the Jewish community is best attested, and is usually referred to as Jewish Old Palestinian. Its oldest form is Old East Jordanian, which probably comes from the region of Caesarea Philippi. This is the dialect of the oldest manuscript of the Book of Enoch (\"c.\" 170 BCE). The next distinct phase of the language is called Old Judaean into the 2nd century CE. Old Judaean literature can be found in various inscriptions and personal letters, preserved quotations in the Talmud and receipts from Qumran. Josephus' first, non-extant edition of his \"The Jewish War\" was written in Old Judaean.\n\nThe Old East Jordanian dialect continued to be used into the 1st century CE by pagan communities living to the east of the Jordan. Their dialect is often then called Pagan Old Palestinian, and it was written in a cursive script somewhat similar to that used for Old Syriac. A Christian Old Palestinian dialect may have arisen from the pagan one, and this dialect may be behind some of the Western Aramaic tendencies found in the otherwise eastern Old Syriac gospels (see Peshitta).\n\nIn the 1st century CE, Jews in Roman Judaea primarily spoke Aramaic (besides Koine Greek as the international language of the Roman administration and trade). \nIn addition to the formal, literary dialects of Aramaic based on Hasmonaean and Babylonian there were a number of colloquial Aramaic dialects. Seven dialects of Western Aramaic were spoken in the vicinity of Judaea in Jesus' time. They were probably distinctive yet mutually intelligible. Old Judaean was the prominent dialect of Jerusalem and Judaea. The region of Ein Gedi had the Southeastern Judaean dialect. Samaria had its distinctive Samaritan Aramaic, where the consonants he, and ayin all became pronounced the same as aleph, presumably a glottal stop. Galilean Aramaic, the dialect of Jesus' home region, is only known from a few place names, the influences on Galilean Targumic, some rabbinic literature and a few private letters. It seems to have a number of distinctive features: diphthongs are never simplified into monophthongs. East of the Jordan, the various dialects of East Jordanian were spoken. In the region of Damascus and the Anti-Lebanon Mountains, Damascene Aramaic was spoken (deduced mostly from Modern Western Aramaic). Finally, as far north as Aleppo, the western dialect of Orontes Aramaic was spoken.\n\nThe three languages influenced one another, especially Hebrew and Aramaic. Hebrew words entered Jewish Aramaic (mostly technical religious words but also everyday words like ' \"wood\"). Vice versa, Aramaic words entered Hebrew (not only Aramaic words like \"māmmôn\" \"wealth\" but Aramaic ways of using words like making Hebrew \"rā’ûi\", \"seen\" mean \"worthy\" in the sense of \"seemly\", which is a loan translation of Aramaic ' meaning \"seen\" and \"worthy\").\n\nThe Greek of the New Testament often preserves non-Greek \"semiticisms\", including transliterations of Semitic words:\n\n"}
{"id": "6291880", "url": "https://en.wikipedia.org/wiki?curid=6291880", "title": "Privy Seal of Japan", "text": "Privy Seal of Japan\n\nThe Privy Seal of Japan is one of the national seals and is the Emperor of Japan's official seal. It is cubic, and its inscription 天皇御璽 (\"The Emperor's Imperial Seal\") is written in . It has two lines of vertical writing, with the right-hand side containing the characters 天皇 (\"Tenno\", emperor), and on the left-hand side containing the characters 御璽 (\"Gyoji\", imperial seal). The seal is printed on Imperial rescripts, proclamation of sentences of laws, cabinet orders, treaties, instruments of ratification, ambassadors' credentials and their dismissal documents, documents of general power of attorney, consular commissions, letters authorizing foreign consuls, letters of appointment or dismissal of government officials, whose appointment requires the Emperor's attestation, and appointment documents and documents of the Prime Minister and Chief Justice, and their respective dismissals.\n\nThe history of the Privy Seal of Japan dates back to the Nara period. Although it was originally made from copper, it was manufactured from stone in 1868 (Meiji) and later, was made from pure gold. The present Privy Seal is pure gold and is about 3 sun (about 9 cm) in size and weighs 4.5 kg. The master-hand of the seal was Abei Rekido (安部井 櫟堂; 1805-1883), of Kyoto. He was commissioned to manufacture the State Seal of Japan within one year, in 1874 (Meiji 7). When not in use, the seal is kept in a leather bag. The seal is used with special cinnabar seal ink specially made by the National Printing Bureau.\n\nIf the State Seal or the Privy Seal are illegally reproduced, the penalty is at least two years or more of terminable penal servitude according to the first clause of Article 164 of the Criminal Code of Japan.\n\n\n"}
{"id": "25714032", "url": "https://en.wikipedia.org/wiki?curid=25714032", "title": "Realia (translation)", "text": "Realia (translation)\n\nIn translation, Realia (plural noun) are words and expressions for culture-specific material elements. The word \"realia\" comes from medieval Latin, in which it originally meant “the real things”, i.e. material things, as opposed to abstract ones. The Bulgarian translators Vlahov and Florin, who were the first to carry out an in-depth study of realia, coined the modern sense of the word. They indicate that since realia carry a very local overtone, they often pose a challenge for translation. Realia must not be confused with terminology: the latter is primarily used in the scientific literature to designate things that pertain to the scientific sphere, and usually only appears in other kinds of texts to serve a very specific stylistic purpose. Realia, on the other hand, are born in popular culture, and are increasingly found in very diverse kinds of texts. Fiction, in particular, is fond of realia for the exotic touch they bring.\n\nVlahov and Florin classify them into various categories:\n\n\n\n\nTo translate realia, various strategies exist : they range from phonetic transcription to translation of the overall meaning. Israeli scholar Gideon Toury offers one way of defining such solutions. According to his characterization, each of these can be placed between two extremes: adequacy (closeness to the original) and acceptability (making the word entirely consistent with the target culture). Here are the various possibilities at hand for translating realia:\n\n\nHow suitable each of these solutions is depends on various factors. One of them is the type of text that is being translated. Adequate translations (in Toury's sense) of realia add some exoticism, a quality that is often desirable in fiction. For non-fiction, nowadays adequacy is usually preferred to acceptability, so as to avoid the ambiguity that can arise from the use of more culturally neutral translations – though the opposite preference has prevailed in the past. One must also consider how the element of realia relates to the source culture in terms of importance and familiarity. If, for instance, it is rather common in the source culture, then providing an adequate translation creates an exotic note that wasn’t there in the first place (though this can be justified by the fact that, after all, one is not dealing with an original, but a translation). If, on the contrary, the source culture perceives the element of realia as unusual, unless the translator renders such an element more culturally neutral, readers of the translation will most likely also perceive it as unusual. Another thing to keep in mind when establishing a translation strategy is that not all languages are equally open to “foreignisms”, and how familiar speakers of that language may be with the realia one introduces. Some languages, such as Italian, welcome such words and frequently integrate them into their vocabulary. Other languages, on the contrary, have the opposite tendency: they are wary of foreign words and are very impermeable to them. French is a good example of such protectionism. Lastly, the expected readership (which may or may not be similar to that of the original) influences the choice of a suitable translation strategy. For example, the name of a chemical compound will be translated differently depending on whether one expects the text to be read by chemists or schoolchildren.\n\n"}
{"id": "48125", "url": "https://en.wikipedia.org/wiki?curid=48125", "title": "Roman naming conventions", "text": "Roman naming conventions\n\nOver the course of some fourteen centuries, the Romans and other peoples of Italy employed a system of nomenclature that differed from that used by other cultures of Europe and the Mediterranean, consisting of a combination of personal and family names. Although conventionally referred to as the \"tria nomina\", the combination of praenomen, nomen, and cognomen that have come to be regarded as the basic elements of the Roman name in fact represent a continuous process of development, from at least the seventh century BC to the end of the seventh century AD. The names developed as part of this system became a defining characteristic of Roman civilization, and although the system itself vanished during the early Middle Ages, the names themselves exerted a profound influence on the development of European naming practices, and many continue to survive in modern languages.\n\nThe distinguishing feature of Roman nomenclature was the use of both personal names and regular surnames. Throughout Europe and the Mediterranean, other ancient civilizations distinguished individuals through the use of single personal names, usually dithematic in nature. Consisting of two distinct elements, or \"themes\", these names allowed for hundreds or even thousands of possible combinations. But a markedly different system of nomenclature arose in Italy, where the personal name was joined by a hereditary surname. Over time, this binomial system expanded to include additional names and designations.\nThe most important of these names was the \"nomen gentilicium\", or simply \"nomen\", a hereditary surname that identified a person as a member of a distinct gens. This was preceded by the praenomen, or \"forename\", a personal name that served to distinguish between the different members of a family. The origin of this binomial system is lost in prehistory, but it appears to have been established in Latium and Etruria by at least 650 BC. In written form, the nomen was usually followed by a filiation, indicating the personal name of an individual's father, and sometimes the name of the mother or other antecedents. Toward the end of the Roman Republic, this was followed by the name of a citizen's voting tribe. Lastly, these elements could be followed by additional surnames, or cognomina, which could be either personal or hereditary, or a combination of both.\n\nThe Roman grammarians came to regard the combination of praenomen, nomen, and cognomen as a defining characteristic of Roman citizenship, known as the \"tria nomina\". However, although all three elements of the Roman name existed throughout most of Roman history, the concept of the \"tria nomina\" can be misleading, because not all of these names were required or used throughout the whole of Roman history. During the period of the Roman Republic, the praenomen and nomen represented the essential elements of the name; the cognomen first appeared among the Roman aristocracy at the inception of the Republic, but was not widely used among the plebeians, who made up the majority of the Roman people, until the second century BC. Even then, not all Roman citizens bore cognomina, and until the end of the Republic the cognomen was regarded as somewhat less than an official name. By contrast, in imperial times the cognomen became the principal distinguishing element of the Roman name, and although praenomina never completely vanished, the essential elements of the Roman name from the second century onward were the nomen and cognomen.\n\nNaming conventions for women also varied from the classical concept of the \"tria nomina\". Originally Roman women shared the binomial nomenclature of men; but over time the praenomen became less useful as a distinguishing element, and women's praenomina were gradually discarded, or replaced by informal names. By the end of the Republic, the majority of Roman women either did not have or did not use praenomina. Most women were called by their nomen alone, or by a combination of nomen and cognomen. Praenomina could still be given when necessary, and as with men's praenomina the practice survived well into imperial times, but the proliferation of personal cognomina eventually rendered women's praenomina obsolete.\n\nIn the later empire, members of the Roman aristocracy used several different schemes of assuming and inheriting nomina and cognomina, both to signify their rank, and to indicate their family and social connections. Some Romans came to be known by alternative names, or \"signia\", and due to the lack of surviving epigraphic evidence, the full nomenclature of most Romans, even among the aristocracy, is seldom recorded.\n\nThus, although the three types of names referred to as the \"tria nomina\" existed throughout Roman history, the period during which the majority of citizens possessed exactly three names was relatively brief. Nevertheless, because most of the important individuals during the best-recorded periods of Roman history possessed all three names, the \"tria nomina\" remains the most familiar conception of the Roman name.\n\nFor a variety of reasons, the Roman nomenclature system broke down in the centuries following the collapse of imperial authority in the west. The praenomen had already become scarce in written sources during the fourth century, and by the fifth century it was retained only by the most conservative elements of the old Roman aristocracy. Over the course of the sixth century, as Roman institutions and social structures gradually fell away, the need to distinguish between nomina and cognomina likewise vanished. By the end of the seventh century, the people of Italy and western Europe had reverted to single names. But many of the names that had originated as part of the \"tria nomina\" were adapted to this usage, and survived into modern times.\n\nAs in other cultures, the early peoples of Italy probably used a single name, which later developed into the praenomen. Marcus Terentius Varro wrote that the earliest Italians used simple names. Names of this type could be honorific or aspirational, or might refer to deities, physical peculiarities, or circumstances of birth. In this early period, the number of personal names must have been quite large; but with the development of additional names the number in widespread use dwindled. By the early Republic, about three dozen Latin praenomina remained in use, some of which were already rare; about eighteen were used by the patricians. Barely a dozen praenomina remained in general use under the Empire, although aristocratic families sometimes revived older praenomina, or created new ones from cognomina.\n\nThe development of the nomen as the second element of the Italic name cannot be attributed to a specific period or culture. From the earliest period it was common to both the Indo-European speaking Italic peoples and the Etruscans. The historian Livy relates the adoption of \"Silvius\" as a nomen by the kings of Alba Longa in honour of their ancestor, Silvius. As part of Rome's foundation myth, this statement cannot be regarded as historical fact, but it does indicate the antiquity of the period to which the Romans themselves ascribed the adoption of hereditary surnames.\n\nIn Latin, most nomina were formed by adding an adjectival suffix, usually \"-ius\", to the stem of an existing word or name. Frequently this required a joining element, such as \"-e-, -id-, -il-\", or \"-on-\". Many common nomina arose as patronymic surnames; for instance, the nomen \"Marcius\" was derived from the praenomen \"Marcus\", and originally signified \"Marci filius\", \"son of Marcus\". In the same way, \"Sextius\", \"Publilius\", and \"Lucilius\" arose from the praenomina \"Sextus\", \"Publius\", and \"Lucius\". This demonstrates that, much like later European surnames, the earliest nomina were not necessarily hereditary, but might be adopted and discarded at will, and changed from one generation to the next. The practice from which these patronymics arose also gave rise to the filiation, which in later times, once the nomen had become fixed, nearly always followed the nomen. Other nomina were derived from names that later came to be regarded as cognomina, such as \"Plancius\" from \"Plancus\" or \"Flavius\" from \"Flavus\"; or from place-names, such as \"Norbanus\" from \"Norba\".\n\nThe binomial name consisting of \"praenomen\" and \"nomen\" eventually spread throughout Italy. Nomina from different languages and regions often have distinctive characteristics; Latin nomina tended to end in \"-ius, -us, -aius, -eius, -eus\", or \"aeus\", while Oscan names frequently ended in \"-is\" or \"-iis\"; Umbrian names in \"-as, -anas, -enas\", or \"-inas\", and Etruscan names in \"-arna, -erna, -ena, -enna, -ina\", or \"-inna\". Oscan and Umbrian forms tend to be found in inscriptions; in Roman literature these names are often Latinized.\n\nMany individuals added an additional surname, or \"cognomen\", which helped to distinguish between members of larger families. Originally these were simply personal names, which might be derived from a person's physical features, personal qualities, occupation, place of origin, or even an object with which a person was associated. Some cognomina were derived from the circumstance of a person's adoption from one family into another, or were derived from foreign names, such as when a freedman received a Roman praenomen and nomen. Other cognomina commemorated important events associated with a person; a battle in which a man had fought (Regillensis), a town captured (Coriolanus); or a miraculous occurrence (Corvus). The late grammarians distinguished certain cognomina as \"agnomina\".\n\nAlthough originally a personal name, the cognomen frequently became hereditary, especially in large families, or \"gentes\", in which they served to identify distinct branches, known as \"stirpes\". Some Romans had more than one cognomen, and in aristocratic families it was not unheard of for individuals to have as many as three, of which some might be hereditary and some personal. These surnames were initially characteristic of patrician families, but over time cognomina were also acquired by the plebeians. However, a number of distinguished plebeian gentes, such as the Antonii and the Marii, were never divided into different branches, and in these families cognomina were the exception rather than the rule.\n\nCognomina are known from the beginning of the Republic, but were long regarded as informal names, and omitted from most official records before the second century BC. Later inscriptions commemorating the early centuries of the Republic supply these missing surnames, although the authenticity of some of them has been disputed. Under the Empire, however, the cognomen acquired great importance, and the number of cognomina assumed by the Roman aristocracy multiplied exponentially.\n\nAdding to the complexity of aristocratic names was the practice of combining the full nomenclature of both one's paternal and maternal ancestors, resulting in some individuals appearing to have two or more complete names. Duplicative or politically undesirable names might be omitted, while the order of names might be rearranged to emphasize those giving the bearer the greatest prestige.\n\nFollowing the promulgation of the \"Constitutio Antoniniana\" in AD 212, granting Roman citizenship to all free men living within the Roman Empire, the praenomen and nomen lost much of their distinguishing function, as all of the newly enfranchised citizens shared the name of \"Marcus Aurelius\". The praenomen and sometimes the nomen gradually disappeared from view, crowded out by other names indicating the bearer's rank and social connections. Surviving inscriptions from the fifth century rarely provide a citizen's full nomenclature.\n\nIn the final centuries of the Empire, the traditional nomenclature was sometimes replaced by alternate names, known as \"signa\". In the course of the sixth century, as central authority collapsed and Roman institutions disappeared, the complex forms of Roman nomenclature were abandoned altogether, and the people of Italy and western Europe reverted to single names. Modern European nomenclature developed independently of the Roman model during the Middle Ages and the Renaissance. However, many modern names are derived from Roman originals.\n\nThe three types of names that have come to be regarded as quintessentially Roman were the \"praenomen, nomen\", and \"cognomen\". Together, these were referred to as the \"tria nomina\". Although not all Romans possessed three names, the practice of using multiple names having different functions was a defining characteristic of Roman culture that distinguished citizens from foreigners.\n\nThe praenomen was a true personal name, chosen by a child's parents, and bestowed on the \"dies lustricius\", or \"day of lustration\", a ritual purification performed on the eighth day after the birth of a girl, or the ninth day after the birth of a boy. Normally all of the children in a family would have different praenomina. Although there was no law restricting the use of specific praenomina, the choice of the parents was usually governed by custom and family tradition. An eldest son was usually named after his father, and younger sons were named after their father's brothers or other male ancestors. In this way, the same praenomina were passed down in a family from one generation to the next. Not only did this serve to emphasize the continuity of a family across many generations, but the selection of praenomina also distinguished the customs of one gens from another. The patrician gentes in particular tended to limit the number of praenomina that they used far more than the plebeians, which was a way of reinforcing the exclusiveness of their social status.\n\nOf course, there were many exceptions to these general practices. A son might be named in honour of one of his maternal relatives, thus bringing a new name into the gens. Because some gentes made regular use of only three or four praenomina, new names might appear whenever there were several younger sons. Furthermore, a number of the oldest and most influential patrician families made a habit of choosing unusual names; in particular the Fabii, Aemilii, Furii, Claudii, Cornelii, and Valerii all used praenomina that were uncommon amongst the patricians, or which had fallen out of general use. In the last two centuries of the Republic, and under the early Empire, it was fashionable for aristocratic families to revive older praenomina.\n\nAbout three dozen Latin praenomina were in use at the beginning of the Republic, although only about eighteen were common. This number fell gradually, until by the first century AD, about a dozen praenomina remained in widespread use, with a handful of others used by particular families. The origin and use of praenomina was a matter of curiosity to the Romans themselves; in \"De Praenominibus\", Probus discusses a number of older praenomina and their meanings. Most praenomina were regularly abbreviated, and rarely written in full. Other praenomina were used by the Oscan, Umbrian, and Etruscan-speaking peoples of Italy, and many of these also had regular abbreviations. Lists of praenomina used by the various people of Italy, together with their usual abbreviations, can be found at praenomen.\n\nRoman men were usually known by their praenomina to members of their family and household, \"clientes\" and close friends; but outside of this circle, they might be called by their nomen, cognomen, or any combination of praenomen, nomen, and cognomen that was sufficient to distinguish them from other men with similar names. In the literature of the Republic, and on all formal occasions, such as when a senator was called upon to speak, it was customary to address a citizen by praenomen and nomen; or, if this were insufficient to distinguish him from other members of the gens, by praenomen and cognomen.\n\nIn imperial times, the praenomen became increasingly confused by the practices of the aristocracy. The emperors usually prefixed \"Imperator\" to their names as a praenomen, while at the same time retaining their own praenomina; but because most of the early emperors were legally adopted by their predecessors, and formally assumed new names, even these were subject to change. Several members of the Julio-Claudian dynasty exchanged their original praenomina for cognomina, or received cognomina in place of praenomina at birth. An emperor might emancipate or enfranchise large groups of people at once, all of whom would automatically receive the emperor's praenomen and nomen. Yet another common practice beginning in the first century AD was to give multiple sons the same praenomen, and distinguish them using different cognomina; by the second century this was becoming the rule, rather than the exception. Another confusing practice was the addition of the full nomenclature of maternal ancestors to the basic \"tria nomina\", so that a man might appear to have two praenomina, one occurring in the middle of his name.\n\nUnder the weight of these practices and others, the utility of the praenomen to distinguish between men continued to decline, until only the force of tradition prevented its utter abandonment. Over the course of the third century, praenomina become increasingly scarce in written records, and from the fourth century onward their appearance becomes exceptional. The descendants of those who had been granted citizenship by the \"Constitutio Antoniniana\" seem to have dispensed with praenomina altogether, and by the end of the western empire, only the oldest Roman families continued to use them.\n\nThe \"nomen gentilicium\", or \"gentile name\", designated a Roman citizen as a member of a gens. A gens, which may be translated as \"race\", \"family\", or \"clan\", constituted an extended Roman family, all of whom shared the same nomen, and claimed descent from a common ancestor. Particularly in the early Republic, the gens functioned as a state within the state, observing its own sacred rites, and establishing private laws, which were binding on its members, although not on the community as a whole.\n\nThe cognomen, the third element of the \"tria nomina\", began as an additional personal name. It was not unique to Rome, but Rome was where the cognomen flourished, as the development of the gens and the gradual decline of the praenomen as a useful means of distinguishing between individuals made the cognomen a useful means of identifying both individuals and whole branches of Rome's leading families. In the early years of the Republic, some aristocratic Romans had as many as three cognomina, some of which were hereditary, while others were personal.\n\nLike the nomen, cognomina could arise from any number of factors: personal characteristics, habits, occupations, places of origin, heroic exploits, and so forth. One class of cognomina consisted largely of archaic praenomina that were seldom used by the later Republic, although as cognomina these names persisted throughout Imperial times. Many cognomina had unusual terminations for Latin names, ending in \"-a, -o\", or \"-io\", and their meanings were frequently obscure, even in antiquity; this seems to emphasize the manner in which many cognomina originally arose from nicknames. The \"-ius\" termination typical of Latin nomina was generally not used for cognomina until the fourth century AD, making it easier to distinguish between nomina and cognomina until the final centuries of the western empire.\n\nUnlike the nomen, which was passed down unchanged from father to son, cognomina could appear and disappear almost at will. They were not normally chosen by the persons who bore them, but were earned or bestowed by others, which may account for the wide variety of unflattering names that were used as cognomina. Doubtless some cognomina were used ironically, while others continued in use largely because, whatever their origin, they were useful for distinguishing among individuals and between branches of large families. New cognomina were coined and came into fashion throughout Roman history.\n\nUnder the Empire, the number of cognomina increased dramatically. Where once only the most noble patrician houses used multiple surnames, Romans of all backgrounds and social standing might bear several cognomina. By the third century, this had become the norm amongst freeborn Roman citizens. The question of how to classify different cognomina led the grammarians of the fourth and fifth centuries to designate some of them as \"agnomina\".\n\nFor most of the Republic, the usual manner of distinguishing individuals was through the binomial form of praenomen and nomen. But as the praenomen lost its value as a distinguishing name, and gradually faded into obscurity, its former role was assumed by the versatile cognomen, and the typical manner of identifying individuals came to be by nomen and cognomen; essentially one form of binomial nomenclature was replaced by another, over the course of several centuries. The very lack of regularity that allowed the cognomen to be used as either a personal or a hereditary surname became its strength in imperial times; as a hereditary surname, a cognomen could be used to identify an individual's connection with other noble families, either by descent, or later by association. Individual cognomina could also be used to distinguish between members of the same family; even as siblings came to share the same praenomen, they bore different cognomina, some from the paternal line, and others from their maternal ancestors.\n\nAlthough the nomen was a required element of Roman nomenclature down to the end of the western empire, its usefulness as a distinguishing name declined throughout imperial times, as an increasingly large portion of the population bore nomina such as \"Flavius\" or \"Aurelius\", which had been granted \"en masse\" to newly enfranchised citizens. As a result, by the third century the cognomen became the most important element of the Roman name, and frequently the only one that was useful for distinguishing between individuals. In the later empire, the proliferation of cognomina was such that the full nomenclature of most individuals was not recorded, and in many cases the only names surviving in extant records are cognomina.\n\nBy the sixth century, traditional Roman cognomina were frequently prefixed by a series of names with Christian religious significance. As Roman institutions vanished, and the distinction between nomen and cognomen ceased to have any practical importance, the complex system of cognomina that developed under the later empire faded away. The people of the western empire reverted to single names, which were indistinguishable from the cognomina that they replaced; many former praenomina and nomina also survived in this way.\n\nThe proliferation of cognomina in the later centuries of the Empire led some grammarians to classify certain types as \"agnomina\". This class included two main types of cognomen: the \"cognomen ex virtute\", and cognomina that were derived from nomina, to indicate the parentage of Romans who had been adopted from one gens into another. Although these names had existed throughout Roman history, it was only in this late period that they were distinguished from other cognomina.\n\nThe \"cognomen ex virtute\" was a surname derived from some virtuous or heroic episode attributed to the bearer. Roman history is filled with individuals who obtained cognomina as a result of their exploits: Aulus Postumius Albus Regillensis, who commanded the Roman army at the Battle of Lake Regillus; Gnaeus Marcius Coriolanus, who captured the city of Corioli; Marcus Valerius Corvus, who defeated a giant Gaul in single combat, aided by a raven; Titus Manlius Torquatus, who likewise defeated a Gaulish giant, and took his name from the torque that he claimed as a prize; Publius Cornelius Scipio Africanus, who carried the Second Punic War to Africa, and defeated Hannibal. Ironically, the most famous examples of this class of cognomen come from the period of the Republic, centuries before the concept of the \"agnomen\" was formulated.\n\nAdoption was a common and formal process in Roman culture. Its chief purpose had nothing to do with providing homes for children; it was about ensuring the continuity of family lines that might otherwise become extinct. In early Rome, this was especially important for the patricians, who enjoyed tremendous status and privilege compared with the plebeians. Because few families were admitted to the patriciate after the expulsion of the kings, while the number of plebeians continually grew, the patricians continually struggled to preserve their wealth and influence. A man who had no sons to inherit his property and preserve his family name would adopt one of the younger sons from another family. In time, as the plebeians also acquired wealth and gained access to the offices of the Roman state, they too came to participate in the Roman system of adoption.\n\nSince the primary purpose of adoption was to preserve the name and status of the adopter, an adopted son would usually assume both the praenomen and nomen of his adoptive father, together with any hereditary cognomina, just as an eldest son would have done. However, adoption did not result in the complete abandonment of the adopted son's birth name. The son's original nomen (or occasionally cognomen) would become the basis of a new surname, formed by adding the derivative suffix \"-anus\" or \"-inus\" to the stem. Thus, when a son of Lucius Aemilius Paullus was adopted by Publius Cornelius Scipio, he became Publius Cornelius Scipio Aemilianus; in his will, the dictator Gaius Julius Caesar adopted his grandnephew, Gaius Octavius, who became known as Gaius Julius Caesar Octavianus.\n\nApart from the praenomen, the \"filiation\" was the oldest element of the Roman name. Even before the development of the nomen as a hereditary surname, it was customary to use the name of a person's father as a means of distinguishing him or her from others with the same personal name; thus Lucius, the son of Marcus would be \"Lucius, Marci filius\"; Paulla, the daughter of Quintus, would be \"Paulla, Quinti filia\". Many nomina were derived in the same way, and most praenomina have at least one corresponding nomen, such as Lucilius, Marcius, Publilius, Quinctius, or Servilius. These are known as patronymic surnames, because they are derived from the name of the original bearer's father. Even after the development of the nomen and cognomen, filiation remained a useful means of distinguishing between members of a large family.\nFiliations were normally written between the nomen and any cognomina, and abbreviated using the typical abbreviations for praenomina, followed by for \"filius\" or \"filia\", and sometimes for \"nepos\" (grandson) or \"neptis\" (granddaughter). Thus, the inscription means \"Spurius Postumius Albus Regillensis, of Aulus the son, of Publius the grandson\". \"Tiberius Aemilius Mamercinus, the son of Lucius and grandson of Mamercus\" would be written . The more formal the writing, the more generations might be included; a great-grandchild would be or for \"pronepos\" or \"proneptis\", a great-great-grandchild or for \"abnepos\" or \"abneptis\", and a great-great-great-grandchild \"adnepos\" or \"adneptis\". However, these forms are rarely included as part of a name, except on the grandest of monumental inscriptions.\n\nThe filiation sometimes included the name of the mother, in which case \"gnatus\" would follow the mother's name, instead of \"filius\" or \"filia\". This is especially common in families of Etruscan origin. The names of married women were sometimes followed by the husband's name and \"uxor\" for \"wife\". means \"Numerius Fabius Maximus, son of Quintus, grandson of Marcus, born of Furia\", while would be \"Claudia, wife of Lucius Valerius\".\n\nSlaves and freedmen also possessed filiations, although in this case the person referred to is usually the slave's owner, rather than his or her father. The abbreviations here include for \"servus\" or \"serva\" and for \"libertus\" or \"liberta\". A slave might have more than one owner, in which case the names could be given serially. In some cases the owner's nomen or cognomen was used instead of or in addition to the praenomen. The \"liberti\" of women sometimes used an inverted \"C\", signifying the feminine praenomen \"Gaia\", here used generically to mean any woman; and there are a few examples of an inverted \"M\", although it is not clear whether this was used generically, or specifically for the feminine praenomen \"Marca\" or \"Marcia\".\n\nAn example of the filiation of slaves and freedmen would be: , \"Alexander, slave of Lucius Cornelius\", who upon his emancipation would probably become , \"Lucius Cornelius Alexander, freedman of Lucius\"; it was customary for a freedman to take the praenomen of his former owner, if he did not already have one, and to use his original personal name as a cognomen. Another example might be , \"Salvia Pompeia, freedwoman of Gnaeus (Pompeius) and Gaia\"; here \"Gaia\" is used generically, irrespective of whether Pompeius' wife was actually named \"Gaia\". A freedman of the emperor might have the filiation , \"Augusti libertus\".\n\nAlthough filiation was common throughout the history of the Republic and well into imperial times, no law governed its use or inclusion in writing. It was used by custom and for convenience, but could be ignored or discarded, as it suited the needs of the writer.\n\nFrom the beginning of the Roman Republic, all citizens were enumerated in one of the tribes making up the \"comitia tributa\", or \"tribal assembly\". This was the most democratic of Rome's three main legislative assemblies of the Roman Republic, in that all citizens could participate on an equal basis, without regard to wealth or social status. Over time, its decrees, known as \"plebi scita\", or \"plebiscites\" became binding on the whole Roman people. Although much of the assembly's authority was usurped by the emperors, membership in a tribe remained an important part of Roman citizenship, so that the name of the tribe came to be incorporated into a citizen's full nomenclature.\n\nThe number of tribes varied over time; tradition ascribed the institution of thirty tribes to Servius Tullius, the sixth King of Rome, but ten of these were destroyed at the beginning of the Republic. Several tribes were added between 387 and 241 BC, as large swaths of Italy came under Roman control, bringing the total number of tribes to thirty-five; except for a brief experiment at the end of the Social War in 88 BC, this number remained fixed. The nature of the tribes was mainly geographic, rather than ethnic; inhabitants of Rome were, in theory, assigned to one of the four \"urban\" tribes, while the territory beyond the city was allocated to the \"rural\" or \"rustic\" tribes.\n\nGeography was not the sole determining factor in one's \"tribus\"; at times efforts were made to assign freedmen to the four urban tribes, thus concentrating their votes and limiting their influence on the \"comitia tributa\". Perhaps for similar reasons, when large numbers of provincials gained the franchise, certain rural tribes were preferred for their enrollment. Citizens did not normally change tribes when they moved from one region to another; but the censors had the power to punish a citizen by expelling him from one of the rural tribes and assigning him to one of the urban tribes. In later periods, most citizens were enrolled in tribes without respect to geography.\n\nPrecisely when it became common to include the name of a citizen's \"tribus\" as part of his full nomenclature is uncertain. The name of the tribe normally follows the filiation and precedes any cognomina, suggesting that it occurred before the cognomen was recognized as a formal part of the Roman name; so probably no later than the second century BC. However, in both writing and inscriptions, the \"tribus\" is found with much less frequency than other parts of the name; so the custom of including it does not seem to have been deeply ingrained in Roman practice. As with the filiation, it was common to abbreviate the name of the tribe. For the names of the thirty-five tribes and their abbreviations, see Roman tribe.\n\nIn the earliest period, the binomial nomenclature of praenomen and nomen that developed throughout Italy was shared by both men and women. Most praenomina had both masculine and feminine forms, and a number of praenomina common to women were seldom or never used by men. Just as men's praenomina, women's names were regularly abbreviated instead of being written in full. A list of women's praenomina can be found at praenomen.\n\nBut for a variety of reasons, women's praenomina became neglected over the course of Roman history, and by the end of the Republic, most women did not have or did not use praenomina. They did not disappear entirely, nor were Roman women bereft of personal names; but for most of Roman history women were known chiefly by their nomina or cognomina.\n\nThe first of these reasons is probably that the praenomen itself lost much of its original utility following the adoption of hereditary surnames. The number of praenomina commonly used by both men and women declined throughout Roman history. For men, who might hold public office or serve in the military, the praenomen remained an important part of the legal name. But, as in other ancient societies, Roman women played little role in public life, so the factors that resulted in the continuation of men's praenomina did not exist for women.\n\nAnother factor was probably that the praenomen was not usually necessary to distinguish between women within the family. Because a Roman woman did not change her nomen when she married, her nomen alone was usually sufficient to distinguish her from every other member of the family. As Latin names had distinctive masculine and feminine forms, the nomen was sufficient to distinguish a daughter from both of her parents and all of her brothers. Thus, there was no need for a personal name unless there were multiple sisters in the same household.\n\nWhen this occurred, praenomina could be and frequently were used to distinguish between sisters. However, it was also common to identify sisters using a variety of names, some of which could be used as either praenomina or cognomina. For example, if Publius Servilius had two daughters, they would typically be referred to as \"Servilia Major\" and \"Servilia Minor\". If there were more daughters, the eldest might be called \"Servilia Prima\" or \"Servilia Maxima\"; younger daughters as \"Servilia Secunda, Tertia, Quarta\", etc. All of these names could be used as praenomina, preceding the nomen, but common usage from the later Republic onward was to treat them as personal cognomina; when these names appear in either position, it is frequently impossible to determine whether they were intended as praenomina or cognomina.\n\nAlthough women's praenomina were infrequently used in the later Republic, they continued to be used, when needed, into imperial times. Among the other peoples of Italy, women's praenomina continued to be used regularly until the populace was thoroughly Romanized. In the Etruscan culture, where women held a markedly higher social status than at Rome or in other ancient societies, inscriptions referring to women nearly always include praenomina.\nMost Roman women were known by their nomina, with such distinction as described above for older and younger siblings. If further distinction were needed, she could be identified as a particular citizen's daughter or wife. For instance, Cicero refers to a woman as \"Annia P. Anni senatoris filia\", which means \"Annia, daughter of Publius Annius, the senator\". However, toward the end of the Republic, as hereditary cognomina came to be regarded as proper names, a woman might be referred to by her cognomen instead, or by a combination of nomen and cognomen; the daughter of Lucius Caecilius Metellus was usually referred to as \"Caecilia Metella\". Sometimes these cognomina were given diminutive forms, such as \"Agrippina\" from the masculine \"Agrippa\", or \"Drusilla\" from \"Drusus\".\n\nIn imperial times, other, less formal names were sometimes used to distinguish between women with similar names. Still later, Roman women, like men, adopted \"signa\", or alternative names, in place of their Roman names.\n\nWith the fall of the western empire in the fifth century, the last traces of the distinctive Italic nomenclature system began to disappear, and women too reverted to single names.\n\nAs Roman territory expanded beyond Italy, many foreigners obtained Roman citizenship, and adopted Roman names. Often these were discharged auxiliary soldiers, or the leaders of annexed towns and peoples. Customarily a newly enfranchised citizen would adopt the praenomen and nomen of his patron; that is, the person who had adopted or manumitted him, or otherwise procured his citizenship. But many such individuals retained a portion of their original names, usually in the form of cognomina. This was especially true for citizens of Greek origin. A name such as or would be typical of such persons, although in form these names are not distinguishable from those of freedmen.\n\nThe \"Constitutio Antoniniana\" promulgated by Caracalla in AD 212 was perhaps the most far-reaching of many imperial decrees enfranchising large numbers of non-citizens living throughout the empire. It extended citizenship to all free inhabitants of the empire, all of whom thus received the name \"Marcus Aurelius\", after the emperor's praenomen and nomen. The result was that vast numbers of individuals who had never possessed praenomina or nomina formally shared the same names. In turn, many of the \"new Romans\" promptly discarded their praenomina, and ignored their nomina except when required by formality. As a result, the cognomina adopted by these citizens, often including their original non-Latin names, became the most important part of their nomenclature.\n\nDuring the Republic, a person's names were usually static and predictable, unless he were adopted into a new family or obtained a new surname. In imperial times, however, names became highly variable and subject to change. Perhaps no names were more variable than those of the emperors.\n\nFor example, the first emperor, known conventionally as Augustus, began life as , or Gaius Octavius, the son of Gaius Octavius. His ancestors had borne the same name for at least four generations. Although the Octavii were an old and distinguished plebeian family, the gens was not divided into \"stirpes\" and had no hereditary cognomina; Octavius' father had put down a slave revolt at Thurii and was sometimes given the surname \"Thurinus\" (a cognomen \"ex virtute\"), but this name was not passed down to the son.\n\nAt the age of eighteen in 44 BC, Octavius was nominated \"magister equitum\" by his granduncle, Gaius Julius Caesar, who held the office of dictator. On the Ides of March, Caesar was assassinated, without legitimate children; but in his will he adopted his nephew, who then became , \"Gaius Julius Caesar Octavianus, son of Gaius\". Thus far, his name follows the Republican model, becoming that of his adoptive father, followed by his original nomen in the form of an agnomen.\n\nTwo years later, Caesar was deified by the Roman Senate, and Octavian, as he was then known, was styled , \"son of the divine (Caesar)\", instead of . Still later, after having been acclaimed \"Imperator\" by the troops under his command, Octavian assumed this title as an additional praenomen, becoming ; in some inscriptions his original praenomen is discarded altogether. In 27 BC, the Senate granted him the title of \"Augustus\", which would ever after be affixed as a cognomen to the names of the Roman emperors.\n\nA similar pattern was followed by Augustus' heirs. The emperor's stepson and eventual successor was born \"Tiberius Claudius Nero;\" after his adoption by the emperor, he became \"Tiberius Julius Caesar\" (retaining his original praenomen). His brother, born \"Decimus Claudius Nero\", subsequently became \"Nero Claudius Drusus\", exchanging his original praenomen for his paternal cognomen, and assuming a new cognomen from his maternal grandfather. Other members of the Julio-Claudian dynasty used praenomina such as \"Drusus\" and \"Germanicus\".\n\nIn subsequent generations, all reigning emperors assumed \"Imperator\" as an additional praenomen (usually without foregoing their original praenomina), and \"Augustus\" as a cognomen. \"Caesar\" came to be used as a cognomen designating an heir apparent; and for the first two centuries of the empire, most emperors were adopted by their predecessors. The result was that each emperor bore a series of names that had more to do with the previous emperor than the names with which he had been born. They added new cognomina as they fought and conquered enemies and new lands, and their filiations recorded their descent from a series of gods. As the names of the emperors themselves changed, so did the names of the members of their families.\n\nDuring the Empire, superficially the naming conventions appear to dissolve into anarchy. In fact, this was not the case: new conventions developed, which were themselves internally coherent.\n\nUnder the \"High Empire\", the new aristocracy began adopting two or more nomina – a practice which has been termed 'binary nomenclature'. This arose out of a desire to incorporate distinguished maternal ancestry in a name or, in order to inherit property, an heir was required by a will to incorporate the testator's name into his own name. For example, the suffect consul of AD 118/9, Gaius Bruttius Praesens Lucius Fulvius Rusticus, has a name which is composed of two standard sets of tria nomina: he was the natural son of a Lucius Bruttius, and added the nomina of his maternal grandfather, Lucius Fulvius Rusticus, to his paternal nomina.\n\nIn order to reflect an illustrious pedigree or other connections, the aristocracy expanded the binary nomenclature concept to include other nomina from an individual's paternal and maternal ancestry. There was no limit to the number of names which could be added in this way (known as polyonomy), and, for example, the consul of 169 AD, (usually called Q. Sosius Priscus) had thirty-eight names comprising fourteen sets of nomina reflecting a complex pedigree stretching back three generations.\n\nThe praenomen, even under the classic system, had never been particularly distinctive because of the limited number of praenomina available. Between the late Republic and the second century AD, the praenomen gradually became less used and eventually disappeared altogether. Even among the senatorial aristocracy it became a rarity by about 300 AD. In part this came about through a tendency for the same praenomen to be given to all males of a family, thereby fossilizing a particular preaenomen/nomen combination and making the praenomen even less distinctive e.g. all males in the emperor Vespasian's family (including all his sons) had the praenomen/nomen combination Titus Flavius:\nThe cognomen, as in Vespasian's family, then assumed the distinguishing function for individuals; where this happened, the cognomen replaced the praenomen in intimate address. The result was that two names remained in use for formal public address but instead of praenomen + nomen, it became nomen + cognomen.\n\nWith the Constitutio Antoniniana in 212, the emperor Caracalla granted Roman citizenship to all free inhabitants of the empire. It had long been the expectation that when a non-Roman acquired citizenship he, as part of his enfranchisement, took on a Roman name. With the mass enfranchisement of 212, the new citizens adopted the nomen \"Aurelius\" in recognition of Caracalla’s beneficence (the emperor's full name was Marcus Aurelius Severus Antoninus Augustus, with Aurelius as the nomen). \"Aurelius\" quickly became the most common nomen in the east and the second most common (after \"Julius\") in the west. The change in the origins of the new governing elite that assumed control of the empire from the end of the third century can be seen in their names: seven of the thirteen emperors between Gallienus and Diocletian bore the name \"Marcus Aurelius\"\n\nAlthough praenomina were not adopted by the new citizens, reflecting the pre-existing decline amongst \"old\" Romans, in the west the new names were formulated on the same basis as the existing Roman practices. In the east, however, the new citizens formulated their names by placing \"Aurelius\" before versions of their non-Roman given name and a patronymic. Ultimately, the ubiquity of \"Aurelius\" meant that it could not function as a true distinguishing nomen, and became primarily just a badge of citizenship added to any name.\n\nAlthough a nomen would long be required for official purposes, and, in isolated corners of the empire and in parts of Italy, its usage would persist into the seventh century, the nomen was generally omitted from the name (even of emperors) by the third century.\n\nTwo factors encouraged its frequent non-use. Firstly, the cognomen increasingly became the distinguishing name and general name of address. As a result, \"New Romans\" and, under their influence, \"old Romans\" too, either dropped the nomen from their name or, in some cases, treated the nomen as a praenomen.\n\nSecondly, with the nomen becoming an increasingly fossilized formality, non-Italian families, even those who had acquired citizenship and a nomen prior to 212, began to ignore their nomen. When a nomen was required for official purposes they would simply put the default nomen of \"Aurelius\" in front of their name, rather than use their actual nomen.\n\n\n"}
{"id": "5933633", "url": "https://en.wikipedia.org/wiki?curid=5933633", "title": "Samaritan Aramaic language", "text": "Samaritan Aramaic language\n\nSamaritan Aramaic, or \"Samaritan\", was the dialect of Aramaic used by the Samaritans in their sacred and scholarly literature. This should not be confused with the Samaritan Hebrew language of the Scriptures. Samaritan Aramaic ceased to be a spoken language some time between the 10th and the 12th centuries.\n\nIn form it resembles the Aramaic of the Targumim, and is written in the Samaritan alphabet.\n\nImportant works written in Samaritan include the translation of the Samaritan Pentateuch in the form of the targum paraphrased version. There are also legal, exegetical and liturgical texts, though later works of the same kind were often written in Arabic.\n\nExodus XX.1-6:\n\n\nNotice the similarities with Judeo-Aramaic as found in Targum Onqelos to this same passage (some expressions below are paraphrased, not literally translated):\n\n\n"}
{"id": "4332784", "url": "https://en.wikipedia.org/wiki?curid=4332784", "title": "Sawney", "text": "Sawney\n\nSawney (sometimes Sandie/y, or Sanders, or Sannock) was an English nickname for a Scotsman, now obsolete, and playing much the same linguistic role that \"Jock\" does now. The name is a Lowland Scots diminutive of the favourite Scottish first name Alexander (also Alasdair in Scottish Gaelic form, anglicised into Alistair) from the last two syllables. The English commonly abbreviate the first two syllables into \"Alec\". \n\nFrom the days after the accession of James VI to the English throne under the title of James I, to the time of George III and the Bute administration, when Scotsmen were exceedingly unpopular and Dr. Samuel Johnson - the great Scotophobe, and son of a Scottish bookseller at Lichfield - thought it prudent to disguise his origin, and overdid his prudence by maligning his father's countrymen, it was customary to designate a Scotsman a \"Sawney\". This vulgar epithet, however, was dying out fast by the 1880s, and was obsolete by the 20th century.\n\nSawney was a common figure of fun in English cartoons. A particularly racist example, \"Sawney in the Bog House\", shows a stereotypical Scots Highlander using a communal bench toilet by sticking one of his legs down each of the holes. This was originally published in London in June 1745, just over a month before \"Bonnie Prince Charlie\" landed in Scotland to begin the Jacobite rising of 1745. In this version Sawney's excreta emerge from below his kilt and flow across the bench. The idea was revived in a different and slightly more decorous version of 1779, which is attributed to the young James Gillray. An inscription reads: \n\nIt has also been suggested that the Galloway cannibal Sawney Bean may have been a fabrication to emphasise the alleged savagery of the Scots.\n\nSometimes also used in the term \"Sawney Ha'peth\", meaning \"Scots halfpennyworth\" implying \"Scottish fool\". At the time of the political union of Scotland and England in 1707, the Pound Scots was worth 1/12 of the Pound Sterling, thus a \"Scots halfpennyworth\" implies worthlessness.\n\nThe word \"sawney\" survives in the current Official Scrabble Players Dictionary (OSPD), which validates the word in Scrabble tournament play, and is defined as \"a foolish person\".\n\n\nThe main text of this article is derived from -\nWith additions from -\n"}
{"id": "540098", "url": "https://en.wikipedia.org/wiki?curid=540098", "title": "Screentone", "text": "Screentone\n\nScreentone is a technique for applying textures and shades to drawings, used as an alternative to hatching. In the conventional process, patterns are transferred to paper from preprinted sheets, but the technique is also simulated in computer graphics. It is also known by the common brand names Zip-A-Tone (1937, now defunct), Chart-Pak (1949), and Letratone (1966, from Letraset).\n\nA traditional screentone sheet consists of a flexible transparent backing, the printed texture, and a wax adhesive layer. The sheet is applied to the paper, adhesive down, and rubbed with a stylus on the backing side. The backing is then peeled off, leaving the ink adhered to the paper where pressure was applied.\n\nA screentone saves an artist's time by allowing quick application of textures to line art where a hand-shaded area would not be reproduced in a timely or acceptable manner. Much like halftone, the size and spacing of black dots, lines, or hatches determine how light or dark an area will appear. Visual artists need to take into account how much an image will be reduced when prepared for publication when choosing the pitch of a screentone. Screentones can also be layered to produce interference patterns such as moire effects, or to simulate multiple sources of shadow in an image.\n\nDifferent styles of screentone exist, variously intended to depict clothing, clouds, emotions, backgrounds, gradients and even objects such as trees. While the sheets are most commonly produced with black ink, there are also varieties in solid and patterned colors. Screentones can also be modified by lightly scratching the backing with an X-Acto blade to produce starbursts and other special effects.\n\nScreentones are widely used by illustrators and artists, especially for cartoons and advertising. Use of the original medium has been declining since the advent of graphics software and desktop publishing, but it is still used, for example in manga.\n\nWhile computer graphics software provides a variety of alternatives to screentone, its appearance is still frequently simulated, to achieve consistency with earlier work or avoid the appearance of computer-generated images. It is sometimes accomplished by scanning actual screentone sheets, but original vector or bitmap screen patterns are also used.\n\n"}
{"id": "4735888", "url": "https://en.wikipedia.org/wiki?curid=4735888", "title": "Social welfare model", "text": "Social welfare model\n\nA social welfare model is a system of social welfare provision and its accompanying value system. It usually involves social policies that affect the welfare of a country's citizens within the framework of a market or mixed economy.\n\nTaxation is concerned with how the state taxes the people, whether by a flat tax, regressive tax or a progressive tax system. The most common guiding rule of taxation is to levy taxes by the ability to pay.\n\nSocial insurance is concerned with how the state implements benefits for the unemployed, pensions, maternity and paternity leave and disabilities.\n\nServices such as health care can be almost entirely state funded, private insurance-based, or somewhere in-between. For example, the United Kingdom has an almost entirely publicly funded health service, the National Health Service (NHS), and Canada offers public health care offered at a provincial level. Conversely, in the United States, individuals have to rely on health insurance policies in the event of hospitalization, and a minimal amount of state support for the poorer people exists. Another element can be public transport, as some countries have nationalized rapid transit systems, while others have privatized them (in the UK for example, public transport has been privatised in Great Britain but not in Northern Ireland).\n\nEconomies with a more laissez-faire approach to employment will do nothing to provide job security. Other countries will rely on some degree of regulation to protect workers from arbitrary firings. A high degree of regulation such as expensive severance fees is often cited reason for making employers reluctant to hire and causing unemployment.\n\nUsed by the UK, Ireland, Canada, New Zealand, Australia, and the South Asian countries the British model tends to have a welfare state of roughly average size, relative to high-income OECD countries, but less comprehensive than those in Scandinavia and much of continental Europe. They have somewhat more poverty and higher inequality. Despite having a smaller welfare state than most Western European countries, the UK, Ireland and Canada do provide, among other things, universal single payer health care, redistribute income and guarantee an income at subsistence level.\n\nUsed by Austria, France, Germany, Belgium and Luxembourg, the Continental model has strict rules on job protection and a large amount of regulation in industry. However, the labour market has proven to be inflexible and slow to react to globalization. Generous insurance-based unemployment benefits and a well funded welfare state are used to reduce poverty and provide high quality health care. This model can generally be seen as middle ground between the British and Nordic models.\n\nUsed by Italy, Spain, Greece, Portugal, the Mediterranean model is similar to the Continental model, but focuses welfare on generous state-pensions. The labour market is inflexible with the same job protectionism as in the Continental model, but is not good at reducing poverty within the lower end of society.\n\nThe Nordic Model, mainly refers to Nordic countries Norway, Sweden, Denmark, Iceland, Finland but some include the Netherlands, also called 'Nordic corporatist' model because of strong influence of the corporatist elements such as labor unions and employers' organizations, advocates a highly developed and government-funded welfare state which provides generous unemployment benefits among other resources for the general public. Labor markets are kept mobile with easy firing and hiring, and government taking care of those laid off with unemployment benefits and retraining. The equality of the Nordic model is achieved through progressive taxation. As a result of the policy, Sweden, Denmark and Norway have the lowest income disparities in the world. Nordic countries have been enjoying high economic and productivity growth, but most remarkably they consistently conquer top spots in world happiness surveys.\n\nBefore the Great Depression, the United States adhered to a social model that could be summarized by the term \"rugged individualism\": the understanding that because most people are capable of taking care of themselves, each person should be left to succeed or fail on their own, only fettered by the bounds of the law, and the government should be limited to protection of civil liberties. The United States had very little in the way of a social safety net for its citizens, with most people depending on their families and private social organizations if they were unable to provide for themselves; this partially explains the enduring greater emphasis on family and religion in American society and politics today than in other comparably developed countries in Western Europe.\n\nAs a result of increased modernization in the late 19th century, this view changed in the emergence of the Progressive Movement, which held that the government can and should have a greater role in regulating the economy, so as to promote a better life for all of its citizens. The biggest change came with President Franklin Delano Roosevelt's New Deal, during which time the American government intervened extensively in the economy, guided often by Keynesian economics. New programs included relief for the poor, unemployed, and those who cannot work due to youth, old age, or disability.\n\nHowever, since the Great Depression, the United States has not followed other developed democracies in the establishment of a more comprehensive model for assuring its citizens' well being. One possible explanation for this is that the U.S was not affected in the same way by World War II as Europe was: while Europeans relied on strong centralized governments to help rebuild their economies after two world wars, the United States was enjoying a period of unprecedented economic growth due to its being one of the few industrialized countries on the planet whose productive capacity had not been destroyed by enemy nations. But now, with the rise of industrial and geo-political competition in Europe and Asia, growing income inequality, high energy prices, and mounting public debt, there is renewed debate over the role of government in modern society.\n\nFor more comprehensive information, see \"Social programs in the United States\"\n\n\n"}
{"id": "33257235", "url": "https://en.wikipedia.org/wiki?curid=33257235", "title": "Swapnasandhani", "text": "Swapnasandhani\n\nSwapnasandhani (Bengali: ) is a Bengali theater group from Kolkata. The group was founded on 29 May 1992. Swapnasandhani has been marked by the acting and direction of Kaushik Sen. In November 2017, Sen stopped directing the group in order to bring change and \"explore the creativity of the group\".\n\nFrom 2006, Swapnasandhani began performing on a regular basis at the Sujata Sadan every Saturday. This eliminated their need to wait in queue for free slots at the major theatres like the Academy of Fine Arts, Madhusudan Mancha etc.\n\n \nTill April, 2011 Swapnasandhani has staged 12 full-length plays and 14 short ones. Some of those are:\n\n\n"}
{"id": "223338", "url": "https://en.wikipedia.org/wiki?curid=223338", "title": "Swimsuit", "text": "Swimsuit\n\nSwimwear is clothing designed to be worn by people engaging in a water-based activity or water sports, such as swimming, diving and surfing, or sun-orientated activities, such as sun bathing. Different types may be worn by men, women, and children. Swimwear is described by a number of names, some of which are used only in particular locations, including swimsuit, bathing suit, swimming costume, bathing costume, swimming suit, swimmers, swimming togs, bathers, cossie (short for \"costume\"), or swimming trunks for men, besides others.\n\nA swimsuit can be worn as an undergarment in sports that require a wetsuit such as water skiing, scuba diving, surfing, and wakeboarding. Swimsuits may also be worn to display the wearer's physical attributes, as in the case of beauty pageants or bodybuilding contests, and glamour photography and magazines like the annual \"Sports Illustrated Swimsuit Issue\" feature models and sports personalities in swimsuits.\n\nThere is a very wide range of styles of modern swimsuits available, which vary as to body coverage and materials. The choice of style may depend on community standards of modesty, as well as current fashions, and personal preferences. The choice will also consider the occasion, for example whether it is to be worn for a passive occasion such as sunbathing or for an activity such as surfing or swimsuit competition. Swimwear for men usually exposes the chest, while suits for women usually cover at least the breasts.\n\nRayon was used in the 1920s in the manufacture of tight-fitting swimsuits, but its durability, especially when wet, proved problematic, with jersey and silk also sometimes being used.\n\nIn the 1930s, new materials were being developed and use in swimwear, particularly latex and nylon, and swimsuits gradually began hugging the body, especially women's swimsuits.\n\nIn western culture, men's swimsuit styles include boardshorts, jammers, swim trunks, briefs or \"speedos\", thongs, and g-strings, in order of decreasing lower body coverage, and Women's swimsuits include one-piece, bikinis, or thongs. While they go through many trends in pattern, length and cut there is not much modification to the original variety of suit. A recent innovation is the burqini, favored by some Muslim women, which covers the whole body and head (but not face) in a manner similar to a diver's wetsuit. These are an updated version of full-body swimwear, which has been available for centuries, but conforms with Islam's traditional emphasis on modest dress. In Egypt, the term \"Sharia swimsuit\" is used to describe full-body swimwear.\n\nSwimsuits can be skin-tight or loose-fitting. They are often lined with another layer of fabric if the outer fabric becomes transparent when wet.\n\nSwimsuits range from designs that almost completely cover the body to designs that expose almost all of the body. The choice of swimsuit will depend on personal and community standards of modesty and on considerations such as how much or how little sun protection is desired, and prevailing fashions. Almost all swimsuits cover the genitals and pubic hair, while most except thongs or G-string cover much or all of the buttocks.\n\nMost swimsuits in western culture leave at least the head, shoulders, arms, and lower part of the leg (below the knee) exposed. Women's swimsuits generally cover at least the areola and bottom half of the breasts, but some are designed for the top part of the swimsuit to be removed. In many countries, young girls and sometimes women choose not to wear a swimsuit top, and this can vary with the occasion, location, age, etc.\n\nBoth men and women may sometimes wear swimsuits covering more of the body when swimming in cold water (see also wetsuit and dry suit). In colder temperatures, the swimwear is needed to conserve body heat and protect the body core from hypothermia.\n\nGerms, bacteria, and mold can grow very quickly on wet bathing suits. Medical professionals warn that wearing damp swimwear for long periods of time can cause a number of infections and rashes in children and adults, and warn against sharing bathing suits with others. They suggest that changing out of a wet bathing suit right away can help prevent vaginal infections, itching and/or jock itch. \n\nIn public swimming pools in France for reasons of hygiene, it is only permitted to wear closer fitting styles of swimwear. Men, for instance, must wear \"Speedo\" style bathing suits and not baggy shorts or trunks.\n\nIn classical antiquity swimming and bathing were done naked. There are Roman murals which show women playing sports and exercising wearing two-piece suits covering the areas around their breasts and hips in a fashion remarkably similar to the present-day bikini. However, there is no evidence that they were used for swimming. All classical pictures of swimming show nude swimmers.\n\nIn various cultural traditions one swims, if not in the nude, in a version in suitable material of a garment or undergarment commonly worn on land, e.g. a loincloth such as the Japanese man's fundoshi.\n\nIn the United Kingdom until the mid-19th century there was no law against nude swimming, and each town was free to make its own laws. For example, the Bath Corporation official bathing dress code of 1737 prescribed, for men:\n\nIt is Ordered Established and Decreed by this Corporation that no Male person above the age of ten years shall at any time hereafter go into any Bath or Baths within this City by day or by night without a Pair of Drawers and a Waistcoat on their bodies.\n\nIn rivers, lakes, streams and the sea men swam in the nude, where the practice was common. Those who didn't swim in the nude, stripped to their underwear. The English practice of men swimming in the nude was banned in the United Kingdom in 1860. Drawers, or caleçons as they were called, came into use in the 1860s. Even then there were many who protested against them and wanted to remain in the nude. Francis Kilvert described men's bathing suits coming into use in the 1870s as \"a pair of very short red and white striped drawers\".\nFemale bathing costumes were derived from those worn at Bath and other spas. It would appear that until the 1670s nude female bathing in the spas was the norm and that after that time women bathed clothed. Celia Fiennes gave a detailed description of the standard ladies' bathing costume in 1687:\n\nThe Ladyes go into the bath with Garments made of a fine yellow canvas, which is stiff and made large with great sleeves like a parson’s gown; the water fills it up so that it is borne off that your shape is not seen, it does not cling close as other linning, which Lookes sadly in the poorer sort that go in their own linning. The Gentlemen have drawers and wastcoates of the same sort of canvas, this is the best linning, for the bath water will Change any other yellow.\n\nThe Bath Corporation official bathing dress code of 1737 prescribed, for women:\n\nNo Female person shall at any time hereafter go into a Bath or Baths within this City by day or by night without a decent Shift on their bodies.\n\n\"The Expedition of Humphry Clinker\" was published in 1771 and its description of ladies’ bathing costume is different from that of Celia Fiennes a hundred years earlier:\n\nThe ladies wear jackets and petticoats of brown linen, with chip hats, in which they fix their handkerchiefs to wipe the sweat from their faces; but, truly, whether it is owing to the steam that surrounds them, or the heat of the water, or the nature of the dress, or to all these causes together, they look so flushed, and so frightful, that I always turn my eyes another way.\n\nPenelope Byrde points out that Smollett’s description may not be accurate, for he describes a two-piece costume, not the one piece shift or smock that most people describe and is depicted in contemporary prints. His description does, however, tally with Elizabeth Grant’s description of the guide’s costume at Ramsgate in 1811. The only difference is in the fabric the costumes are made of. Flannel, however, was a common fabric for sea bathing costumes as many believed the warmer fabric was necessary in cold water.\n\nIn the 18th century women wore \"bathing gowns\" in the water; these were long dresses of fabrics that would not become transparent when wet, with weights sewn into the hems so that they would not rise up in the water. The men's swim suit, a rather form-fitting wool garment with long sleeves and legs similar to long underwear, was developed and would change little for a century.\n\nIn the 19th century, the woman's double suit was common, comprising a gown from shoulder to knees plus a set of trousers with leggings going down to the ankles.\n\nIn the Victorian era, popular beach resorts were commonly equipped with bathing machines designed to avoid the exposure of people in swimsuits, especially to people of the opposite sex.\n\nIn the United States, beauty pageants of women in bathing costumes became popular from the 1880s. However, such events were not regarded as respectable. Beauty contests became more respectable with the first modern \"Miss America\" contest held in 1921, though less respectable beauty contests continued to be held.\n\nIn 1907, the swimmer Annette Kellerman from Australia visited the United States as an \"underwater ballerina\", a version of synchronized swimming involving diving into glass tanks. She was arrested for indecent exposure because her swimsuit showed arms, legs and the neck. Kellerman changed the suit to have long arms and legs and a collar, still keeping the close fit that revealed the shapes underneath. She later starred in several movies, including one about her life. She marketed a line of bathing suits and her style of one-piece suits came to be known as \"the Annette Kellerman\". The Annette Kellerman was considered the most offensive style of swimsuit in the 1920s and became the focus of censorship efforts.\n\nDespite opposition from some groups, the form-fitting style proved popular. It was not long before swimwear started to shrink further. At first arms were exposed and then legs up to mid-thigh. Necklines receded from around the neck down to around the top of the bosom. The development of new fabrics allowed for new varieties of more comfortable and practical swimwear.\n\nDue to the figure-hugging nature of these garments, glamour photography since the 1940s and 1950s has often featured people wearing swimsuits. This type of glamour photography eventually evolved into swimsuit photography exemplified by the annual \"Sports Illustrated Swimsuit Issue\". Beauty contests also required contestants to wear form-fitting swimsuits.\n\nThe first bikinis appeared just after World War II. Early examples were not very different from the women's two pieces common since the 1920s, except that they had a gap below the breast line allowing for a section of bare midriff. They were named after Bikini Atoll, the site of several nuclear weapons tests, for their supposed explosive effect on the viewer.\n\nThrough the 1950s, it was thought proper for the lower part of the bikini to come up high enough to cover the navel. From the 1960s on, the bikini shrank in all directions until it sometimes covered little more than the nipples and genitalia, although less revealing models giving more support to the breasts remained popular. At the same time, fashion designer Rudi Gernreich introduced the monokini, a topless suit for women consisting of a modest bottom supported by two thin straps. Although not a commercial success, the suit opened eyes to new design possibilities. In the 1980s the thong or \"tanga\" came out of Brazil, said to have been inspired by traditional garments of native tribes in the Amazon. However, the one-piece suit continued to be popular for its more modest approach.\n\nMen's swimsuits developed roughly in parallel to women's during this period, with the shorts covering progressively less. Eventually racing-style \"speedo\" suits became popular—and not just for their speed advantages. Thongs, G-strings, and bikini style suits are also worn. Typically these are more popular in more tropical regions; however, they may also be worn at public swimming pools and inland lakes. But in the 1990s, longer and baggier shorts became popular, with the hems often reaching to the knees. Often called boardshorts and swim trunks, these were often worn lower on the hips than regular shorts.\n\nSince the early twentieth century a naturist movement has developed in western countries that seeks a return to non-sexual nakedness when swimming and during other appropriate activities. Some women prefer to engage in water or sun activities with their torso uncovered. The practice is often described as \"toplessness\" or \"topfreedom\". In some places around the world, nude beaches have been set aside for people who choose to engage in normal beach activities in the nude.\n\nAs an alternative to a swimsuit, some people wear trousers, underpants or a T-shirt either as a makeshift swimsuit or because they prefer regular clothes over swimsuits. Using a T-shirt can also provide extra protection against sunburn. In some countries, such as Thailand and Philippines, swimming in regular clothes is the norm while swimsuits are rare. At beaches, this may be more accepted than at swimming pools, which tend not to permit the use of underwear as swimwear because underwear is unlined, may become translucent, and may be perceived as unclean.\n"}
{"id": "57960893", "url": "https://en.wikipedia.org/wiki?curid=57960893", "title": "The Dragon Prince", "text": "The Dragon Prince\n\nThe Dragon Prince is an American epic fantasy adventure computer-animated television series created for Netflix by Aaron Ehasz and Justin Richmond and produced by Wonderstorm. It premiered on September 14, 2018.\n\nIn October 2018, Netflix announced a renewal of \"The Dragon Prince\" for a second season.\n\nThe series is set in a fantasy world where humans and elves are in conflict. Long ago, the humans used dark magic, and were driven off by the dragons and elves to one end of the continent. Now, after humans killed the dragon king and his egg, war is imminent. As forces gather, the elves attempt to assassinate the human king Harrow and his heir, the young prince Ezran. One among the elves, the young Rayla, along with Ezran and his older half-brother Callum, discover that the dragon king's egg wasn't in fact destroyed, but stolen. Together they undertake to return the egg to the dragons to prevent war. But the mage Viren, King Harrow's advisor, is intent on war. He seizes power after the king dies in the assassination attempt, and sends his children Claudia and Soren after the fugitives.\n\n\nAaron Ehasz was the head writer and co-executive producer of the animated series \"\", and a longtime writer and story editor for \"Futurama\". Richmond co-directed the video game \"\". The two co-founded the multimedia production studio Wonderstorm in 2017, along with Justin Santistevan, to work both on \"The Dragon Prince\" and a related video game. Ehasz and Richmond are co-creators and writers on the series, while Giancarlo Volpe, a former director for \"Avatar\", is an executive producer.\n\n\"The Dragon Prince\" is created using three-dimensional computer animation. A reduced frame rate is applied to offset \"floatiness\".\n\nThe series was announced on July 10, 2018. A trailer was released in July 2018 at the San Diego Comic-Con. The first season of \"The Dragon Prince\" was released on Netflix on September 14, 2018. In October 2018, a second season was confirmed for a 2019 release.\n\nAn advance review of the first episode by IGN concluded that the series \"surprises by comfortably exploring dark story elements while giving audiences an assortment of lovable characters to engage with. In short, it's a worthwhile animated series for audiences of all ages.\" On Rotten Tomatoes, the first season has a score of 100% based on 9 reviews, with an average rating of 7.8/10.\n\n"}
{"id": "25602603", "url": "https://en.wikipedia.org/wiki?curid=25602603", "title": "The Gladiator (Scarrow novel)", "text": "The Gladiator (Scarrow novel)\n\nThe Gladiator is a 2009 novel () by Simon Scarrow, the ninth book in the Eagle series, where we see the return of Macro and Cato, this time up against a ruthless gladiator in the Island of Crete after their ship is damaged by a tidal wave on their way to Rome\n"}
{"id": "2643943", "url": "https://en.wikipedia.org/wiki?curid=2643943", "title": "The Shrinking of Treehorn", "text": "The Shrinking of Treehorn\n\nThe Shrinking of Treehorn is a children's book by Florence Parry Heide, illustrated by Edward Gorey and first published in 1971. The main character in the book is Treehorn, whose parents barely notice when he shrinks.\n\nTreehorn is a young boy who begins shrinking after playing a strange board game. Treehorn discovers that he is getting smaller, when he cannot reach the gumballs and candy bars he has hidden on a previously accessible shelf. When his parents comment on it, they say, \"Maybe he's doing it on purpose, just to be different.\" In the end, Treehorn returns to his normal size.\n\nThe sequel to this book is \"Treehorn's Treasure\" (1981), followed by \"Treehorn's Wish\" (1984). The three books were collected in an omnibus edition, \"The Treehorn Trilogy\", in 2006.\n"}
{"id": "47888764", "url": "https://en.wikipedia.org/wiki?curid=47888764", "title": "Where My Country Gone?", "text": "Where My Country Gone?\n\n\"Where My Country Gone?\" is the second episode of the nineteenth season of the animated television series \"South Park\", and the 259th episode overall, written and directed by series co-creator Trey Parker. The episode premiered on Comedy Central on September 23, 2015. It parodies illegal immigration to the United States and the 2016 presidential candidacy of Donald Trump, along with Caitlyn Jenner and political correctness.\n\nKyle is unwillingly honored at the White House by Barack Obama for his acceptance of Caitlyn Jenner (in the previous episode). His reward is to be driven back home by Jenner, who runs over a pedestrian while leaving the White House (in reference to Jenner's real life collision).\n\nMr. Garrison observes that many Canadians have entered the United States illegally, placing many Canadian students in South Park Elementary. When he reacts intolerantly to a group of them disrupting his class, PC Principal forces the entire faculty to take \"Canadian-language\" night classes to better help their Canadian students. Garrison reacts to this by composing a song, \"Where My Country Gone?\" lamenting how immigration has ruined his country. He uses the song to rally the townspeople to his cause. When Garrison interrupts a school presentation on Canadian history he is fired, stirring tension between American and Canadian students. Cartman and his friends decide that the only way to bring peace is to encourage a romantic relationship between the factions: they appoint Butters to ask out a Canadian girl.\n\nGarrison gets the entire town behind his proposal to literally \"fuck the Canadians to death\", then build a wall to prevent further illegal immigration from Canada. However, it is discovered that Canada has already built an immense border wall of its own, to (according to a Canada border guard) prevent Americans from raping Canadian women and preserve Canada's \"cool shit\". To find out what \"cool shit\" they are hiding, Garrison sneaks into Canada by going over Niagara Falls in a barrel.\n\nMeanwhile, Butters begins dating a Canadian girl named Charlotte. The two find themselves falling in love. At dinner with her family, he learns that they and other Canadians left Canada unwillingly. Charlotte's father explains that during the last Canadian election there was a brash candidate (a parody of Donald Trump and his positions on immigration) that said outrageous things without offering realistic solutions to problems. Everyone thought it was funny and, thus, did not take him seriously as a candidate. He was elected, to their chagrin, because they let the joke go on for too long and neglected to vote against him. After dinner, Butters reveals his romantic feelings for Charlotte, who reciprocates.\n\nBarely surviving his jaunt across the border, Garrison finds that Canada has become a deserted wasteland and heads to the President's office. There, a physical altercation ensues between the two, which ends when Garrison gains the upper hand and rapes the Canadian President to death. Upon hearing the news that the Canadian President is dead, the emigrants - including Charlotte's family - return to Canada. Back in the United States, Garrison attributes the \"win\" to his policy. Kyle starts to point out that the conflict actually stemmed from such hostile policies but suddenly goes silent because \"no one wants another speech\". Garrison tells the people of South Park that he will join the 2016 campaign for President of the United States with his running mate, Caitlyn Jenner. The two leave for Washington in a car driven by Jenner, who immediately runs over another pedestrian.\n\n\"Where My Country Gone?\" received positive reviews. IGN's Max Nicholson gave the episode a 8.0 out of 10, concluding \"This week's South Park improved on the first, offering both a scathing satire and a clear target: Donald Trump. With the help of both Mr. Garrison and Butters (and Canadian Donald Trump), 'Where My Country Gone?' proved to be a surprisingly enjoyable entry\". Chris Longo from Den of Geek rated it 4 out of 5 stars and said in his review that \"this was an episode that should be satisfying even for those who have been put off by the constant flavor of the week references and faux cameos.\" Writing for \"The A.V. Club\", Dan Caffrey gave an A- rating to the episode, enjoying the way that it had built on the previous episode and the parody of Donald Trump, although finding Caitlyn Jenner's physical appearance \"more than a little nasty\". \"Rolling Stone\" writer Jon Blistein praised Mr. Garrison's campaign to rid America of Canadians as \"hilariously ignorant\".\n\n"}
